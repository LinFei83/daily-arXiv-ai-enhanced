{"id": "2509.03543", "categories": ["eess.IV", "physics.optics"], "pdf": "https://arxiv.org/pdf/2509.03543", "abs": "https://arxiv.org/abs/2509.03543", "authors": ["Chenyu Yuan"], "title": "Latent Space Single-Pixel Imaging Under Low-Sampling Conditions", "comment": null, "summary": "In recent years, the introduction of deep learning into the field of\nsingle-pixel imaging has garnered significant attention. However, traditional\nnetworks often operate within the pixel space. To address this, we innovatively\nmigrate single-pixel imaging to the latent space, naming this framework LSSPI\n(Latent Space Single-Pixel Imaging). Within the latent space, we conduct\nin-depth explorations into both reconstruction and generation tasks for\nsingle-pixel imaging. Notably, this approach significantly enhances imaging\ncapabilities even under low sampling rate conditions. Compared to conventional\ndeep learning networks, LSSPI not only reconstructs images with higher\nsignal-to-noise ratios (SNR) and richer details under equivalent sampling rates\nbut also enables blind denoising and effective recovery of high-frequency\ninformation. Furthermore, by migrating single-pixel imaging to the latent\nspace, LSSPI achieves superior advantages in terms of model parameter\nefficiency and reconstruction speed. Its excellent computational efficiency\nfurther positions it as an ideal solution for low-sampling single-pixel imaging\napplications, effectively driving the practical implementation of single-pixel\nimaging technology.", "AI": {"tldr": "本文提出LSSPI框架，将单像素成像从像素空间迁移到潜在空间，显著提升了低采样率下的成像能力、重建质量和计算效率，并支持盲去噪和高频信息恢复。", "motivation": "传统的深度学习单像素成像网络在像素空间操作，可能限制其性能，尤其是在低采样率条件下。", "method": "创新性地将单像素成像迁移到潜在空间，并命名为LSSPI（潜在空间单像素成像）框架。在该潜在空间中，深入探索了单像素成像的重建和生成任务。", "result": "LSSPI显著增强了低采样率下的成像能力。与传统深度学习网络相比，LSSPI在同等采样率下能重建出更高信噪比和更丰富细节的图像，并实现了盲去噪和高频信息有效恢复。此外，LSSPI在模型参数效率和重建速度方面也具有显著优势，计算效率极高。", "conclusion": "LSSPI作为一种理想的低采样单像素成像解决方案，其卓越的计算效率和性能优势，有效推动了单像素成像技术的实际应用。"}}
{"id": "2509.04051", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04051", "abs": "https://arxiv.org/abs/2509.04051", "authors": ["Yaojun Wu", "Chaoyi Lin", "Yiming Wang", "Semih Esenlik", "Zhaobin Zhang", "Kai Zhang", "Li Zhang"], "title": "Neural Video Compression with In-Loop Contextual Filtering and Out-of-Loop Reconstruction Enhancement", "comment": "9 pages, 8 figures, Accepted to ACMMM 2025", "summary": "This paper explores the application of enhancement filtering techniques in\nneural video compression. Specifically, we categorize these techniques into\nin-loop contextual filtering and out-of-loop reconstruction enhancement based\non whether the enhanced representation affects the subsequent coding loop.\nIn-loop contextual filtering refines the temporal context by mitigating error\npropagation during frame-by-frame encoding. However, its influence on both the\ncurrent and subsequent frames poses challenges in adaptively applying filtering\nthroughout the sequence. To address this, we introduce an adaptive coding\ndecision strategy that dynamically determines filtering application during\nencoding. Additionally, out-of-loop reconstruction enhancement is employed to\nrefine the quality of reconstructed frames, providing a simple yet effective\nimprovement in coding efficiency. To the best of our knowledge, this work\npresents the first systematic study of enhancement filtering in the context of\nconditional-based neural video compression. Extensive experiments demonstrate a\n7.71% reduction in bit rate compared to state-of-the-art neural video codecs,\nvalidating the effectiveness of the proposed approach.", "AI": {"tldr": "本文系统性地研究了增强滤波技术在神经视频压缩中的应用，将其分为环内上下文滤波和环外重建增强，并提出自适应编码决策策略，实现了比特率的显著降低。", "motivation": "在逐帧编码过程中，误差传播会影响时间上下文的细化，且自适应应用滤波具有挑战性；同时，需要简单有效的方法来提升重建帧的质量和编码效率。", "method": "作者将增强滤波技术分为环内上下文滤波（用于减轻误差传播）和环外重建增强（用于提升重建质量）。针对环内滤波的挑战，引入了一种自适应编码决策策略来动态决定滤波的应用。这是首次在基于条件的神经视频压缩背景下对增强滤波进行系统研究。", "result": "实验结果表明，与现有最先进的神经视频编解码器相比，所提出的方法实现了7.71%的比特率降低。", "conclusion": "增强滤波技术在神经视频压缩中是有效的，特别是通过结合环内上下文滤波和环外重建增强，并辅以自适应编码决策策略，能够显著提升编码效率和重建质量。"}}
{"id": "2509.04118", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04118", "abs": "https://arxiv.org/abs/2509.04118", "authors": ["Junqi Liao", "Yaojun Wu", "Chaoyi Lin", "Zhipin Deng", "Li Li", "Dong Liu", "Xiaoyan Sun"], "title": "EHVC: Efficient Hierarchical Reference and Quality Structure for Neural Video Coding", "comment": "9 pages, 8 figures, Accepted to ACMMM 2025", "summary": "Neural video codecs (NVCs), leveraging the power of end-to-end learning, have\ndemonstrated remarkable coding efficiency improvements over traditional video\ncodecs. Recent research has begun to pay attention to the quality structures in\nNVCs, optimizing them by introducing explicit hierarchical designs. However,\nless attention has been paid to the reference structure design, which\nfundamentally should be aligned with the hierarchical quality structure. In\naddition, there is still significant room for further optimization of the\nhierarchical quality structure. To address these challenges in NVCs, we propose\nEHVC, an efficient hierarchical neural video codec featuring three key\ninnovations: (1) a hierarchical multi-reference scheme that draws on\ntraditional video codec design to align reference and quality structures,\nthereby addressing the reference-quality mismatch; (2) a lookahead strategy to\nutilize an encoder-side context from future frames to enhance the quality\nstructure; (3) a layer-wise quality scale with random quality training strategy\nto stabilize quality structures during inference. With these improvements, EHVC\nachieves significantly superior performance to the state-of-the-art NVCs. Code\nwill be released in: https://github.com/bytedance/NEVC.", "AI": {"tldr": "本文提出了一种高效分层神经视频编码器（EHVC），通过引入分层多参考方案、前瞻策略和分层质量尺度与随机质量训练，解决了现有神经视频编码器中参考与质量结构不匹配以及质量结构优化不足的问题，显著提升了编码性能。", "motivation": "尽管神经视频编码器（NVCs）在编码效率上表现出色，但现有研究对NVC中的参考结构设计关注较少，导致其与分层质量结构不匹配。此外，分层质量结构本身仍有很大的优化空间。", "method": "本文提出了EHVC，包含三项关键创新：1) 分层多参考方案，借鉴传统视频编码器设计，使参考结构与质量结构对齐，解决参考-质量不匹配问题；2) 前瞻策略，利用未来帧的编码器侧上下文信息增强质量结构；3) 分层质量尺度与随机质量训练策略，以在推理时稳定质量结构。", "result": "EHVC在性能上显著优于现有最先进的神经视频编码器。", "conclusion": "EHVC通过其创新的分层多参考、前瞻策略和分层质量尺度设计，有效解决了神经视频编码器中的参考-质量不匹配和质量结构优化不足问题，实现了卓越的编码性能。"}}
{"id": "2509.03609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03609", "abs": "https://arxiv.org/abs/2509.03609", "authors": ["Shengkai Sun", "Zefan Zhang", "Jianfeng Dong", "Zhiyong Cheng", "Xiaojun Chang", "Meng Wang"], "title": "Towards Efficient General Feature Prediction in Masked Skeleton Modeling", "comment": "Accepted by ICCV 2025", "summary": "Recent advances in the masked autoencoder (MAE) paradigm have significantly\npropelled self-supervised skeleton-based action recognition. However, most\nexisting approaches limit reconstruction targets to raw joint coordinates or\ntheir simple variants, resulting in computational redundancy and limited\nsemantic representation. To address this, we propose a novel General Feature\nPrediction framework (GFP) for efficient mask skeleton modeling. Our key\ninnovation is replacing conventional low-level reconstruction with high-level\nfeature prediction that spans from local motion patterns to global semantic\nrepresentations. Specifically, we introduce a collaborative learning framework\nwhere a lightweight target generation network dynamically produces diversified\nsupervision signals across spatial-temporal hierarchies, avoiding reliance on\npre-computed offline features. The framework incorporates constrained\noptimization to ensure feature diversity while preventing model collapse.\nExperiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits\nof our approach: Computational efficiency (with 6.2$\\times$ faster training\nthan standard masked skeleton modeling methods) and superior representation\nquality, achieving state-of-the-art performance in various downstream tasks.", "AI": {"tldr": "本文提出了一种名为通用特征预测（GFP）的新框架，用于高效的掩码骨架建模，通过预测高层特征而非低层坐标，显著提升了骨架动作识别的计算效率和表示质量。", "motivation": "现有的掩码自编码器（MAE）范式在骨架动作识别中，通常将重建目标限制为原始关节坐标或其简单变体，导致计算冗余和语义表示能力有限。", "method": "本文提出通用特征预测（GFP）框架，用从局部运动模式到全局语义表示的高层特征预测取代传统的低层重建。它引入了一个协同学习框架，其中轻量级目标生成网络动态产生跨时空层次的多样化监督信号，避免依赖预计算的离线特征。此外，该框架还结合了约束优化以确保特征多样性并防止模型崩溃。", "result": "GFP方法在NTU RGB+D 60、NTU RGB+D 120和PKU-MMD数据集上进行了验证，展示了其优势：计算效率显著提升（训练速度比标准掩码骨架建模方法快6.2倍），并获得了卓越的表示质量，在各种下游任务中实现了最先进的性能。", "conclusion": "通过预测高层语义特征而非低层关节坐标，GFP框架有效解决了现有MAE在骨架动作识别中的局限性，实现了计算效率和表示质量的双重提升，达到了最先进的性能。"}}
{"id": "2509.03614", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03614", "abs": "https://arxiv.org/abs/2509.03614", "authors": ["Seungho Choe", "Xiaoli Qin", "Abubakr Shafique", "Amanda Dy", "Dimitri Androutsos", "Susan Done", "April Khademi"], "title": "Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge", "comment": "4 pages, 1 figures, final submission for MIDOG 2025 challenge", "summary": "Counting mitotic figures is time-intensive for pathologists and leads to\ninter-observer variability. Artificial intelligence (AI) promises a solution by\nautomatically detecting mitotic figures while maintaining decision consistency.\nHowever, AI tools are susceptible to domain shift, where a significant drop in\nperformance can occur due to differences in the training and testing sets,\nincluding morphological diversity between organs, species, and variations in\nstaining protocols. Furthermore, the number of mitoses is much less than the\ncount of normal nuclei, which introduces severely imbalanced data for the\ndetection task. In this work, we formulate mitosis detection as a pixel-level\nsegmentation and propose a teacher-student model that simultaneously addresses\nmitosis detection (Track 1) and atypical mitosis classification (Track 2). Our\nmethod is based on a UNet segmentation backbone that integrates domain\ngeneralization modules, namely contrastive representation learning and\ndomain-adversarial training. A teacher-student strategy is employed to generate\npixel-level pseudo-masks not only for annotated mitoses and hard negatives but\nalso for normal nuclei, thereby enhancing feature discrimination and improving\nrobustness against domain shift. For the classification task, we introduce a\nmulti-scale CNN classifier that leverages feature maps from the segmentation\nmodel within a multi-task learning paradigm. On the preliminary test set, the\nalgorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of\n0.8414 in Track 2, demonstrating the effectiveness of integrating\nsegmentation-based detection and classification into a unified framework for\nrobust mitosis analysis.", "AI": {"tldr": "本研究提出了一种结合像素级分割和教师-学生模型的统一AI框架，用于自动检测有丝分裂并分类异常有丝分裂，有效解决了病理学中的域偏移和数据不平衡问题，并取得了良好的性能。", "motivation": "病理学家计数有丝分裂耗时且存在观察者间差异。现有AI工具易受域偏移（训练和测试集差异，如形态、物种、染色协议）影响，且有丝分裂数量远少于正常细胞核，导致数据严重不平衡。", "method": "将有丝分裂检测公式化为像素级分割任务，提出一个教师-学生模型。该模型基于UNet分割骨干，并集成了域泛化模块（对比表示学习和域对抗训练）。教师-学生策略用于生成有丝分裂、难负样本和正常细胞核的像素级伪掩码，以增强特征辨别力并提高对抗域偏移的鲁棒性。分类任务引入了一个多尺度CNN分类器，利用分割模型的特征图进行多任务学习。", "result": "在初步测试集上，算法在有丝分裂检测（Track 1）中实现了0.7660的F1分数，在异常有丝分裂分类（Track 2）中实现了0.8414的平衡准确率。", "conclusion": "该研究证明了将基于分割的检测和分类整合到一个统一框架中，能有效实现鲁棒的有丝分裂分析。"}}
{"id": "2509.03563", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03563", "abs": "https://arxiv.org/abs/2509.03563", "authors": ["Quan Quan", "Jiwen Xu", "Runxiao Liu", "Yi Ding", "Jiaxing Che", "Kai-Yuan Cai"], "title": "Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation : A Table-Mechanics-Inspired Approach", "comment": null, "summary": "In comparison with existing approaches, which struggle with scalability,\ncommunication dependency, and robustness against dynamic failures, cooperative\naerial transportation via robot swarms holds transformative potential for\nlogistics and disaster response. Here, we present a physics-inspired\ncooperative transportation approach for flying robot swarms that imitates the\ndissipative mechanics of table-leg load distribution. By developing a\ndecentralized dissipative force model, our approach enables autonomous\nformation stabilization and adaptive load allocation without the requirement of\nexplicit communication. Based on local neighbor robots and the suspended\npayload, each robot dynamically adjusts its position. This is similar to\nenergy-dissipating table leg reactions. The stability of the resultant control\nsystem is rigorously proved. Simulations demonstrate that the tracking errors\nof the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches\nunder the cases of capability variation, cable uncertainty, limited vision, and\npayload variation, respectively. In real-world experiments with six flying\nrobots, the cooperative aerial transportation system achieved a 94% success\nrate under single-robot failure, disconnection events, 25% payload variation,\nand 40% cable length uncertainty, demonstrating strong robustness under outdoor\nwinds up to Beaufort scale 4. Overall, this physics-inspired approach bridges\nswarm intelligence and mechanical stability principles, offering a scalable\nframework for heterogeneous aerial systems to collectively handle complex\ntransportation tasks in communication-constrained environments.", "AI": {"tldr": "本文提出了一种受物理学启发的无人机蜂群协同运输方法，该方法模仿桌腿的耗散力学，实现了去中心化、无通信的编队稳定和自适应载荷分配，并在仿真和实际实验中展现出卓越的鲁棒性和性能。", "motivation": "现有协同运输方法在可扩展性、通信依赖性和动态故障鲁棒性方面存在局限性，而机器人蜂群的协同空中运输在物流和灾难响应中具有变革性潜力。", "method": "该研究开发了一种受物理学启发的去中心化耗散力模型，模仿桌腿载荷分配的耗散力学。每个机器人根据局部邻居和悬挂载荷动态调整位置，无需明确通信即可实现自主编队稳定和自适应载荷分配。该控制系统的稳定性得到了严格证明。", "result": "仿真结果表明，在能力变化、电缆不确定性、有限视野和载荷变化情况下，所提出方法的跟踪误差分别比现有方法低20%、68%、55.5%和21.9%。在六个飞行机器人的实际实验中，该系统在单机器人故障、断开事件、25%载荷变化和40%电缆长度不确定性下实现了94%的成功率，并在高达蒲福氏风级4级的室外风中表现出强大的鲁棒性。", "conclusion": "这种受物理学启发的协同运输方法将蜂群智能与机械稳定性原理相结合，为异构空中系统在通信受限环境中集体处理复杂运输任务提供了一个可扩展的框架。"}}
{"id": "2509.03525", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03525", "abs": "https://arxiv.org/abs/2509.03525", "authors": ["Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sepehr Karimi", "Sina Rashidi", "Ali Zolnour", "Maryam Dadkhah", "Yasaman Haghbin", "Hossein AzadMaleki", "Maryam Zolnoori"], "title": "Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies", "comment": null, "summary": "Over half of US adults with Alzheimer disease and related dementias remain\nundiagnosed, and speech-based screening offers a scalable detection approach.\nWe compared large language model adaptation strategies for dementia detection\nusing the DementiaBank speech corpus, evaluating nine text-only models and\nthree multimodal audio-text models on recordings from DementiaBank speech\ncorpus. Adaptations included in-context learning with different demonstration\nselection policies, reasoning-augmented prompting, parameter-efficient\nfine-tuning, and multimodal integration. Results showed that class-centroid\ndemonstrations achieved the highest in-context learning performance, reasoning\nimproved smaller models, and token-level fine-tuning generally produced the\nbest scores. Adding a classification head substantially improved\nunderperforming models. Among multimodal models, fine-tuned audio-text systems\nperformed well but did not surpass the top text-only models. These findings\nhighlight that model adaptation strategies, including demonstration selection,\nreasoning design, and tuning method, critically influence speech-based dementia\ndetection, and that properly adapted open-weight models can match or exceed\ncommercial systems.", "AI": {"tldr": "该研究比较了大型语言模型（LLMs）在痴呆症语音检测中的多种适应策略，发现模型适应方法对性能至关重要，且经过适当适应的开源模型可媲美甚至超越商业系统。", "motivation": "美国超过一半的阿尔茨海默病及相关痴呆症患者未被诊断，语音筛查提供了一种可扩展的检测方法。", "method": "研究评估了九种纯文本模型和三种多模态（音频-文本）模型，使用了DementiaBank语音语料库的录音。适应策略包括：不同演示选择策略的上下文学习、推理增强提示、参数高效微调（PEFT）以及多模态整合。", "result": "结果显示，类质心演示在上下文学习中表现最佳；推理改善了较小的模型；令牌级微调通常产生最佳分数。添加分类头显著改善了表现不佳的模型。在多模态模型中，微调的音频-文本系统表现良好，但未能超越顶级的纯文本模型。", "conclusion": "研究强调了模型适应策略（包括演示选择、推理设计和微调方法）对基于语音的痴呆症检测至关重要，并且经过适当适应的开源模型可以达到或超越商业系统的性能。"}}
{"id": "2509.03536", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.03536", "abs": "https://arxiv.org/abs/2509.03536", "authors": ["Weizhi Chen", "Ziwei Wang", "Leyang Yang", "Sheng Zhou", "Xiaoxuan Tang", "Jiajun Bu", "Yong Li", "Wei Jiang"], "title": "PG-Agent: An Agent Powered by Page Graph", "comment": "Paper accepted to ACM MM 2025", "summary": "Graphical User Interface (GUI) agents possess significant commercial and\nsocial value, and GUI agents powered by advanced multimodal large language\nmodels (MLLMs) have demonstrated remarkable potential. Currently, existing GUI\nagents usually utilize sequential episodes of multi-step operations across\npages as the prior GUI knowledge, which fails to capture the complex transition\nrelationship between pages, making it challenging for the agents to deeply\nperceive the GUI environment and generalize to new scenarios. Therefore, we\ndesign an automated pipeline to transform the sequential episodes into page\ngraphs, which explicitly model the graph structure of the pages that are\nnaturally connected by actions. To fully utilize the page graphs, we further\nintroduce Retrieval-Augmented Generation (RAG) technology to effectively\nretrieve reliable perception guidelines of GUI from them, and a tailored\nmulti-agent framework PG-Agent with task decomposition strategy is proposed to\nbe injected with the guidelines so that it can generalize to unseen scenarios.\nExtensive experiments on various benchmarks demonstrate the effectiveness of\nPG-Agent, even with limited episodes for page graph construction.", "AI": {"tldr": "本文提出PG-Agent，一个多智能体框架，通过将GUI操作序列转换为页面图并结合RAG技术，有效捕捉页面间的复杂转换关系，显著提升GUI智能体在未知场景下的泛化能力。", "motivation": "现有GUI智能体依赖于多步操作的顺序片段作为先验知识，未能捕捉页面间复杂的转换关系，导致智能体难以深入感知GUI环境并泛化到新场景。", "method": "研究设计了一个自动化流程，将顺序操作片段转换为页面图，显式建模页面及其通过动作连接的图结构。进一步引入检索增强生成（RAG）技术，从页面图中有效检索可靠的GUI感知指导。在此基础上，提出了一个定制化的多智能体框架PG-Agent，结合任务分解策略并注入这些指导，以实现对未见场景的泛化。", "result": "在各种基准测试上的大量实验表明，PG-Agent即使在用于页面图构建的片段有限的情况下，也表现出显著的有效性。", "conclusion": "PG-Agent通过利用页面图和RAG技术，成功解决了现有GUI智能体在感知复杂页面转换关系和泛化能力方面的不足，为GUI智能体在未知场景下的应用提供了有效方案。"}}
{"id": "2509.03685", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.03685", "abs": "https://arxiv.org/abs/2509.03685", "authors": ["Zhongjun Ni"], "title": "Data-Driven Smart Maintenance of Historic Buildings", "comment": "Doctoral thesis, Link\\\"oping Studies in Science and Technology.\n  Dissertations, ISSN 0345-7524 ; 2444", "summary": "Digital transformation in the built environment offers new opportunities to\nimprove building maintenance through data-driven approaches. Smart monitoring,\npredictive modeling, and artificial intelligence can enhance decision-making\nand enable proactive strategies. The preservation of historic buildings is an\nimportant scenario where preventive maintenance is essential to ensure\nlong-term sustainability while protecting heritage values. This thesis presents\na comprehensive solution for data-driven smart maintenance of historic\nbuildings, integrating Internet of Things (IoT), cloud computing, edge\ncomputing, ontology-based data modeling, and machine learning to improve indoor\nclimate management, energy efficiency, and conservation practices.\n  This thesis advances data-driven conservation of historic buildings by\ncombining smart monitoring, digital twins, and artificial intelligence. The\nproposed methods enable preventive maintenance and pave the way for the next\ngeneration of heritage conservation strategies.", "AI": {"tldr": "论文提出了一种针对历史建筑的综合性数据驱动智能维护解决方案，整合物联网、云计算、边缘计算、本体建模、机器学习和数字孪生，以优化室内气候管理、能源效率和保护实践，从而实现预防性维护并推进遗产保护策略。", "motivation": "建筑环境的数字化转型为通过数据驱动方法改进建筑维护提供了新机遇。在历史建筑的保护中，预防性维护对于确保长期可持续性及保护遗产价值至关重要。", "method": "整合物联网（IoT）、云计算、边缘计算、基于本体的数据建模和机器学习；结合智能监测、数字孪生和人工智能。", "result": "该方案改善了室内气候管理、能源效率和保护实践；实现了预防性维护；为下一代遗产保护策略奠定了基础。", "conclusion": "本论文提出了一种针对历史建筑的综合性数据驱动智能维护解决方案，通过结合智能监测、数字孪生和人工智能，推动了历史建筑的数据驱动保护和预防性维护策略。"}}
{"id": "2509.03616", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03616", "abs": "https://arxiv.org/abs/2509.03616", "authors": ["Rajeev Ranjan Dwivedi", "Ankur Kumar", "Vinod K Kurmi"], "title": "Multi Attribute Bias Mitigation via Representation Learning", "comment": "ECAI 2025 (28th European Conference on Artificial Intelligence)", "summary": "Real world images frequently exhibit multiple overlapping biases, including\ntextures, watermarks, gendered makeup, scene object pairings, etc. These biases\ncollectively impair the performance of modern vision models, undermining both\ntheir robustness and fairness. Addressing these biases individually proves\ninadequate, as mitigating one bias often permits or intensifies others. We\ntackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a\nlean two stage framework that needs group labels only while training and\nminimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)\ndeliberately identifies the influence of known shortcuts by training encoders\nfor each attribute and integrating them with the main backbone, compelling the\nclassifier to explicitly recognize these biases. Then Gradient Suppression Fine\nTuning prunes those very bias directions from the backbone's gradients, leaving\na single compact network that ignores all the shortcuts it just learned to\nrecognize. Moreover we find that existing bias metrics break under subgroup\nimbalance and train test distribution shifts, so we introduce Scaled Bias\nAmplification (SBA): a test time measure that disentangles model induced bias\namplification from distributional differences. We validate GMBM on FB CMNIST,\nCelebA, and COCO, where we boost worst group accuracy, halve multi attribute\nbias amplification, and set a new low in SBA even as bias complexity and\ndistribution shifts intensify, making GMBM the first practical, end to end\nmultibias solution for visual recognition. Project page:\nhttp://visdomlab.github.io/GMBM/", "AI": {"tldr": "该论文提出了广义多偏置缓解（GMBM）框架，一个两阶段的端到端解决方案，用于解决图像中多重重叠偏置问题。它通过自适应偏置集成学习和梯度抑制微调来减轻偏置，并引入了Scaled Bias Amplification（SBA）作为新的偏置度量指标，显著提高了视觉识别模型的公平性和鲁棒性。", "motivation": "现实世界的图像经常表现出多种重叠的偏置（如纹理、水印、性别妆容、场景物体配对等）。这些偏置共同损害了现代视觉模型的性能，削弱了它们的鲁棒性和公平性。单独处理这些偏置不足以解决问题，因为缓解一个偏置可能会允许或加剧其他偏置。", "method": "该研究提出了广义多偏置缓解（GMBM），一个精简的两阶段框架，仅在训练时需要组标签，并在测试时最小化偏置。第一阶段是自适应偏置集成学习（ABIL），通过为每个属性训练编码器并将其与主干网络集成，使分类器明确识别这些偏置的影响。第二阶段是梯度抑制微调，从主干网络的梯度中修剪掉这些偏置方向，留下一个紧凑的网络来忽略它刚刚学会识别的所有捷径。此外，为了解决现有偏置度量在子组不平衡和训练-测试分布偏移下的失效问题，该研究引入了Scaled Bias Amplification（SBA），一种在测试时解耦模型引起的偏置放大与分布差异的度量。", "result": "GMBM在FB CMNIST、CelebA和COCO数据集上进行了验证，结果显示它提高了最差组的准确性，将多属性偏置放大减半，并在偏置复杂性和分布偏移加剧的情况下，在SBA上创造了新低。这使得GMBM成为首个用于视觉识别的实用、端到端的多偏置解决方案。", "conclusion": "GMBM是一个有效且实用的端到端解决方案，能够应对视觉识别中多重重叠的偏置问题，显著提升模型的鲁棒性和公平性。同时，引入的SBA提供了一种在复杂条件下更准确的偏置评估方法。"}}
{"id": "2509.03638", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.03638", "abs": "https://arxiv.org/abs/2509.03638", "authors": ["David Alvear", "George Turkiyyah", "Shinkyu Park"], "title": "Cooperative Grasping for Collective Object Transport in Constrained Environments", "comment": null, "summary": "We propose a novel framework for decision-making in cooperative grasping for\ntwo-robot object transport in constrained environments. The core of the\nframework is a Conditional Embedding (CE) model consisting of two neural\nnetworks that map grasp configuration information into an embedding space. The\nresulting embedding vectors are then used to identify feasible grasp\nconfigurations that allow two robots to collaboratively transport an object. To\nensure generalizability across diverse environments and object geometries, the\nneural networks are trained on a dataset comprising a range of environment maps\nand object shapes. We employ a supervised learning approach with negative\nsampling to ensure that the learned embeddings effectively distinguish between\nfeasible and infeasible grasp configurations. Evaluation results across a wide\nrange of environments and objects in simulations demonstrate the model's\nability to reliably identify feasible grasp configurations. We further validate\nthe framework through experiments on a physical robotic platform, confirming\nits practical applicability.", "AI": {"tldr": "本文提出了一种新颖的框架，用于在受限环境中进行双机器人协作抓取和物体运输。该框架核心是一个条件嵌入（CE）模型，能够识别可行的抓取配置。", "motivation": "在受限环境中，双机器人协作抓取和物体运输的决策是一个挑战。需要一种通用的方法来识别可行的抓取配置，以确保机器人能够协同运输物体，并适用于各种环境和物体几何形状。", "method": "该方法的核心是一个条件嵌入（CE）模型，由两个神经网络组成。这些网络将抓取配置信息映射到一个嵌入空间中。通过监督学习和负采样，在包含多种环境地图和物体形状的数据集上训练这些网络，以有效区分可行和不可行的抓取配置。", "result": "在广泛的模拟环境和物体中进行的评估表明，该模型能够可靠地识别可行的抓取配置。通过在物理机器人平台上的实验进一步验证了该框架的实际适用性。", "conclusion": "所提出的基于条件嵌入模型的框架，能够有效地识别双机器人在受限环境中进行协作抓取的可行配置，并在模拟和物理实验中均表现出可靠性和实用性。"}}
{"id": "2509.03526", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03526", "abs": "https://arxiv.org/abs/2509.03526", "authors": ["Yansong Liu", "Jiateng Li", "Yuan Liu"], "title": "Enhancing Speech Large Language Models through Reinforced Behavior Alignment", "comment": null, "summary": "The recent advancements of Large Language Models (LLMs) have spurred\nconsiderable research interest in extending their linguistic capabilities\nbeyond text to other modalities, which leads to emergence of speech-based LLMs\n(SpeechLMs) with capability of processing user request in either speech or\ntextual formats. However, owing to inter-modal discrepancies, these SpeechLMs\nstill exhibit a significant performance gap compared to their text-based LLM\ncounterparts in instruction-following, particularly when confronted with the\ndynamic and variable nature of user speech. To address this challenge, this\npaper introduces a framework termed Reinforced Behavior Alignment (RBA),\ndesigned to bolster the language generation proficiency of SpeechLMs. Instead\nof relying on supervised fine-tuning from human annotations, RBA employs a\nself-synthesis methodology to generate extensive, high-fidelity alignment data\nby a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of\na teacher using a reinforcement learning-based approach. Experimental results\ndemonstrate that this method effectively enhances the instruction-following\ncapabilities of SpeechLMs that outperform conventional distillation baselines.\nCrucially, we demonstrate that RBA can be seamlessly extended to tasks such\nincluding spoken question answering and speech-to-text translation, attaining\nstate-of-the-art performance on open benchmarks with only self-generated data.", "AI": {"tldr": "本文提出了一种名为强化行为对齐（RBA）的框架，通过自合成高质量对齐数据和基于强化学习的方法，显著提升了语音大模型（SpeechLMs）的指令遵循能力，并使其在语音问答和语音到文本翻译等任务上达到最先进水平。", "motivation": "尽管语音大模型（SpeechLMs）能够处理语音和文本请求，但由于模态间差异，它们在指令遵循方面与纯文本大模型（LLMs）相比仍存在显著性能差距，尤其是在面对动态多变的语音输入时。", "method": "RBA框架不依赖于人工标注的监督微调。它采用自合成方法，利用一个强大的教师大模型生成大量高保真对齐数据。随后，通过基于强化学习的方法，将SpeechLMs的行为与教师大模型的行为进行对齐。", "result": "实验结果表明，RBA有效增强了SpeechLMs的指令遵循能力，优于传统的蒸馏基线方法。此外，RBA可以无缝扩展到语音问答和语音到文本翻译等任务，仅使用自生成数据就在开放基准测试中取得了最先进的性能。", "conclusion": "RBA框架通过结合自合成数据生成和强化学习的行为对齐，成功弥合了SpeechLMs与纯文本LLMs在指令遵循上的性能差距，并展示了其在多模态任务上的泛化能力和卓越表现。"}}
{"id": "2509.03548", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03548", "abs": "https://arxiv.org/abs/2509.03548", "authors": ["João P. Arroyo", "João G. Rodrigues", "Daniel Lawand", "Denis D. Mauá", "Junkyu Lee", "Radu Marinescu", "Alex Gray", "Eduardo R. Laurentino", "Fabio G. Cozman"], "title": "Multilinear and Linear Programs for Partially Identifiable Queries in Quasi-Markovian Structural Causal Models", "comment": "Accepted at the Causal Abstractions and Representations (CAR)\n  workshop of the 41st Conference on Uncertainty in Artificial Intelligence\n  (UAI 2025)", "summary": "We investigate partially identifiable queries in a class of causal models. We\nfocus on acyclic Structural Causal Models that are quasi-Markovian (that is,\neach endogenous variable is connected with at most one exogenous confounder).\nWe look into scenarios where endogenous variables are observed (and a\ndistribution over them is known), while exogenous variables are not fully\nspecified. This leads to a representation that is in essence a Bayesian network\nwhere the distribution of root variables is not uniquely determined. In such\ncircumstances, it may not be possible to precisely compute a probability value\nof interest. We thus study the computation of tight probability bounds, a\nproblem that has been solved by multilinear programming in general, and by\nlinear programming when a single confounded component is intervened upon. We\npresent a new algorithm to simplify the construction of such programs by\nexploiting input probabilities over endogenous variables. For scenarios with a\nsingle intervention, we apply column generation to compute a probability bound\nthrough a sequence of auxiliary linear integer programs, thus showing that a\nrepresentation with polynomial cardinality for exogenous variables is possible.\nExperiments show column generation techniques to be superior to existing\nmethods.", "AI": {"tldr": "本研究针对准马尔可夫因果模型中的部分可识别查询，提出了一种新算法来简化紧密概率边界的计算，并证明了在单一干预场景下，列生成技术优于现有方法。", "motivation": "在因果模型中，当外生变量未完全指定时，无法精确计算感兴趣的概率值。因此，需要计算紧密概率边界。现有方法（如多线性规划和线性规划）在程序构建上可能复杂，需要更高效的解决方案。", "method": "研究准马尔可夫非循环结构因果模型，将其表示为根变量分布不唯一确定的贝叶斯网络。提出一种新算法，利用内生变量的输入概率来简化程序构建。对于单一干预场景，应用列生成技术通过一系列辅助线性整数规划来计算概率边界，实现了外生变量的多项式基数表示。", "result": "该方法实现了外生变量的多项式基数表示。实验结果表明，列生成技术在计算概率边界方面优于现有方法。", "conclusion": "本研究提出了一种有效的新算法，特别是在单一干预场景下使用列生成技术，能够更高效地计算准马尔可夫因果模型中部分可识别查询的紧密概率边界，并优于现有方法。"}}
{"id": "2509.03694", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.03694", "abs": "https://arxiv.org/abs/2509.03694", "authors": ["Leon Greiser", "Christian Rathgeber", "Vladislav Nenchev", "Sören Hohmann"], "title": "Parameter Tuning Under Uncertain Road Perception in Driver Assistance Systems", "comment": null, "summary": "Advanced driver assistance systems have improved comfort, safety, and\nefficiency of modern vehicles. However, sensor limitations lead to noisy lane\nestimates that pose a significant challenge in developing performant control\narchitectures. Lateral trajectory planning often employs an optimal control\nformulation to maintain lane position and minimize steering effort. The\nparameters are often tuned manually, which is a time-intensive procedure. This\npaper presents an automatic parameter tuning method for lateral planning in\nlane-keeping scenarios based on recorded data, while taking into account noisy\nroad estimates. By simulating the lateral vehicle behavior along a reference\ncurve, our approach efficiently optimizes planner parameters for automated\ndriving and demonstrates improved performance on previously unseen test data.", "AI": {"tldr": "本文提出了一种自动参数调整方法，用于在考虑噪声的车道估计下，基于记录数据优化车道保持场景中的横向规划参数，并通过仿真验证了其在未知数据上的性能提升。", "motivation": "高级驾驶辅助系统面临传感器限制导致的噪声车道估计挑战，这影响了控制架构的性能。此外，横向轨迹规划的参数通常需要耗时的人工手动调整。", "method": "本研究提出了一种自动参数调整方法，用于车道保持场景中的横向规划。该方法基于记录数据，并考虑了道路估计中的噪声。通过沿参考曲线模拟车辆的横向行为，该方法能够有效地优化规划器参数。", "result": "该方法能够高效地优化自动驾驶规划器参数，并在以前未见的测试数据上展示了改进的性能。", "conclusion": "所提出的自动参数调整方法有效解决了噪声道路估计和手动参数调整的问题，提高了自动驾驶横向规划的效率和在实际场景中的性能。"}}
{"id": "2509.03631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03631", "abs": "https://arxiv.org/abs/2509.03631", "authors": ["Anders Kjelsrud", "Lasse Løvstakken", "Erik Smistad", "Håvard Dalen", "Gilles Van De Vyver"], "title": "Lightweight image segmentation for echocardiography", "comment": "4 pages, 6 figures, The 2025 IEEE International Ultrasonics Symposium", "summary": "Accurate segmentation of the left ventricle in echocardiography can enable\nfully automatic extraction of clinical measurements such as volumes and\nejection fraction. While models configured by nnU-Net perform well, they are\nlarge and slow, thus limiting real-time use. We identified the most effective\ncomponents of nnU-Net for cardiac segmentation through an ablation study,\nincrementally evaluating data augmentation schemes, architectural\nmodifications, loss functions, and post-processing techniques. Our analysis\nrevealed that simple affine augmentations and deep supervision drive\nperformance, while complex augmentations and large model capacity offer\ndiminishing returns. Based on these insights, we developed a lightweight U-Net\n(2M vs 33M parameters) that achieves statistically equivalent performance to\nnnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89\nfor LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster\n(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.\nCross-dataset evaluation on an internal dataset (N=311) confirms comparable\ngeneralization.", "AI": {"tldr": "本研究开发了一种轻量级U-Net模型，在左心室分割任务上实现了与nnU-Net统计学上等效的性能，但模型更小、速度更快，适用于实时应用。", "motivation": "nnU-Net在超声心动图左心室分割中表现良好，但其模型庞大且速度慢，限制了实时应用。准确的左心室分割对于自动提取临床测量值（如容积和射血分数）至关重要。", "method": "通过消融研究，逐步评估了数据增强方案、架构修改、损失函数和后处理技术在心脏分割中的有效组件。基于这些发现，开发了一种轻量级U-Net模型。", "result": "研究发现简单仿射增强和深度监督是性能的关键驱动因素，而复杂增强和大型模型容量的回报递减。开发的轻量级U-Net（2M参数）在CAMUS数据集上与nnU-Net（33M参数）达到了统计学上等效的性能（LV/MYO/LA的Dice分数分别为0.93/0.85/0.89 vs 0.93/0.86/0.89，$p>0.05$），同时模型小16倍，速度快4倍（每帧1.35ms vs 5.40ms）。跨数据集评估也证实了其可比的泛化能力。", "conclusion": "成功开发了一种轻量级U-Net，在保持与nnU-Net相当的性能的同时，显著减小了模型大小并提高了处理速度，使其更适用于实时应用。"}}
{"id": "2509.03658", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03658", "abs": "https://arxiv.org/abs/2509.03658", "authors": ["Antonio Guillen-Perez"], "title": "Efficient Virtuoso: A Latent Diffusion Transformer Model for Goal-Conditioned Trajectory Planning", "comment": null, "summary": "The ability to generate a diverse and plausible distribution of future\ntrajectories is a critical capability for autonomous vehicle planning systems.\nWhile recent generative models have shown promise, achieving high fidelity,\ncomputational efficiency, and precise control remains a significant challenge.\nIn this paper, we present the \\textbf{Efficient Virtuoso}, a conditional latent\ndiffusion model for goal-conditioned trajectory planning. Our approach\nintroduces a novel two-stage normalization pipeline that first scales\ntrajectories to preserve their geometric aspect ratio and then normalizes the\nresulting PCA latent space to ensure a stable training target. The denoising\nprocess is performed efficiently in this low-dimensional latent space by a\nsimple MLP denoiser, which is conditioned on a rich scene context fused by a\npowerful Transformer-based StateEncoder. We demonstrate that our method\nachieves state-of-the-art performance on the Waymo Open Motion Dataset,\nreaching a \\textbf{minADE of 0.25}. Furthermore, through a rigorous ablation\nstudy on goal representation, we provide a key insight: while a single endpoint\ngoal can resolve strategic ambiguity, a richer, multi-step sparse route is\nessential for enabling the precise, high-fidelity tactical execution that\nmirrors nuanced human driving behavior.", "AI": {"tldr": "本文提出了Efficient Virtuoso，一个用于目标条件轨迹规划的条件潜在扩散模型，通过新颖的两阶段归一化和高效去噪，在Waymo数据集上实现了最先进的性能，并强调了多步稀疏路径对于高保真战术执行的重要性。", "motivation": "自动驾驶规划系统需要生成多样且合理的未来轨迹。尽管现有生成模型有前景，但在实现高保真度、计算效率和精确控制方面仍面临挑战。", "method": "本文提出了Efficient Virtuoso，一个用于目标条件轨迹规划的条件潜在扩散模型。其核心方法包括：1) 新颖的两阶段归一化流程，首先缩放轨迹以保留几何纵横比，然后归一化PCA潜在空间以确保稳定的训练目标。2) 去噪过程在一个低维潜在空间中通过一个简单的MLP去噪器高效完成，该去噪器以Transformer-based StateEncoder融合的丰富场景上下文为条件。3) 对目标表示进行了严格的消融研究。", "result": "该方法在Waymo Open Motion Dataset上实现了最先进的性能，minADE达到0.25。通过对目标表示的消融研究发现：单一终点目标可以解决战略模糊性，但更丰富的多步稀疏路径对于实现模仿人类驾驶行为的精确、高保真战术执行至关重要。", "conclusion": "Efficient Virtuoso通过其创新的归一化和高效的潜在空间去噪，为自动驾驶轨迹规划提供了一个高性能解决方案。研究还揭示了使用多步稀疏路径作为目标表示对于实现精细、高保真战术执行的关键作用，这对于模拟人类驾驶行为至关重要。"}}
{"id": "2509.03527", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03527", "abs": "https://arxiv.org/abs/2509.03527", "authors": ["Bohdan M. Pavlyshenko"], "title": "Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model", "comment": null, "summary": "In the paper, we consider multilevel multitask analysis of cryptocurrency\nnews using a fine-tuned Mistral 7B large language model with\nretrieval-augmented generation (RAG).\n  On the first level of analytics, the fine-tuned model generates graph and\ntext summaries with sentiment scores as well as JSON representations of\nsummaries. Higher levels perform hierarchical stacking that consolidates sets\nof graph-based and text-based summaries as well as summaries of summaries into\ncomprehensive reports. The combination of graph and text summaries provides\ncomplementary views of cryptocurrency news. The model is fine-tuned with 4-bit\nquantization using the PEFT/LoRA approach. The representation of cryptocurrency\nnews as knowledge graph can essentially eliminate problems with large language\nmodel hallucinations.\n  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM\nmodels for multilevel cryptocurrency news analysis can conduct informative\nqualitative and quantitative analytics, providing important insights.", "AI": {"tldr": "本文提出了一种使用经过微调的Mistral 7B大型语言模型（LLM）结合检索增强生成（RAG）技术，对加密货币新闻进行多层次、多任务分析的方法，通过生成图谱和文本摘要，并进行分层堆叠，以提供全面的洞察。", "motivation": "研究旨在对加密货币新闻进行信息丰富的定性及定量分析，并提供重要见解。同时，通过将新闻表示为知识图谱，旨在消除大型语言模型幻觉的问题。", "method": "研究采用经过4位量化并使用PEFT/LoRA方法微调的Mistral 7B LLM，结合RAG技术。在第一层分析中，模型生成带有情感评分的图谱和文本摘要，以及摘要的JSON表示。更高层次进行分层堆叠，将图谱和文本摘要以及摘要的摘要整合为全面的报告。将加密货币新闻表示为知识图谱。", "result": "结果表明，使用经过微调的Mistral 7B LLM进行多层次加密货币新闻分析，能够进行信息丰富的定性及定量分析，并提供重要的洞察。", "conclusion": "该研究证实，结合微调的Mistral 7B LLM和RAG技术进行多层次加密货币新闻分析是有效的，能够提供有价值的见解，并且通过知识图谱表示可以有效解决LLM幻觉问题。"}}
{"id": "2509.03550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03550", "abs": "https://arxiv.org/abs/2509.03550", "authors": ["Tonghe Li", "Jixin Liu", "Weili Zeng", "Hao Jiang"], "title": "Diffusion-RL Based Air Traffic Conflict Detection and Resolution Method", "comment": "59 pages,13 figures, 3 tables", "summary": "In the context of continuously rising global air traffic, efficient and safe\nConflict Detection and Resolution (CD&R) is paramount for air traffic\nmanagement. Although Deep Reinforcement Learning (DRL) offers a promising\npathway for CD&R automation, existing approaches commonly suffer from a\n\"unimodal bias\" in their policies. This leads to a critical lack of\ndecision-making flexibility when confronted with complex and dynamic\nconstraints, often resulting in \"decision deadlocks.\" To overcome this\nlimitation, this paper pioneers the integration of diffusion probabilistic\nmodels into the safety-critical task of CD&R, proposing a novel autonomous\nconflict resolution framework named Diffusion-AC. Diverging from conventional\nmethods that converge to a single optimal solution, our framework models its\npolicy as a reverse denoising process guided by a value function, enabling it\nto generate a rich, high-quality, and multimodal action distribution. This core\narchitecture is complemented by a Density-Progressive Safety Curriculum (DPSC),\na training mechanism that ensures stable and efficient learning as the agent\nprogresses from sparse to high-density traffic environments. Extensive\nsimulation experiments demonstrate that the proposed method significantly\noutperforms a suite of state-of-the-art DRL benchmarks. Most critically, in the\nmost challenging high-density scenarios, Diffusion-AC not only maintains a high\nsuccess rate of 94.1% but also reduces the incidence of Near Mid-Air Collisions\n(NMACs) by approximately 59% compared to the next-best-performing baseline,\nsignificantly enhancing the system's safety margin. This performance leap stems\nfrom its unique multimodal decision-making capability, which allows the agent\nto flexibly switch to effective alternative maneuvers.", "AI": {"tldr": "该论文提出了一种名为Diffusion-AC的新型自主冲突解决框架，通过将扩散概率模型集成到深度强化学习中，解决了现有方法在空域冲突检测与解决（CD&R）中决策灵活性不足的问题。该方法能够生成多模态动作分布，在复杂高密度交通场景下显著提高了成功率并降低了近距空中相撞（NMACs）的发生率。", "motivation": "随着全球航空交通量的持续增长，高效安全的冲突检测与解决（CD&R）至关重要。尽管深度强化学习（DRL）为CD&R自动化提供了前景，但现有方法通常存在“单峰偏差”，导致在面对复杂动态约束时决策灵活性不足，常陷入“决策僵局”。", "method": "本文将扩散概率模型引入到CD&R任务中，提出了Diffusion-AC框架。该框架将策略建模为由价值函数引导的逆向去噪过程，以生成丰富、高质量和多模态的动作分布，而非单一最优解。此外，还引入了“密度渐进安全课程（Density-Progressive Safety Curriculum, DPSC）”训练机制，以确保智能体在从稀疏到高密度交通环境中稳定高效地学习。", "result": "广泛的仿真实验表明，所提出的方法显著优于一系列最先进的DRL基准。在最具挑战性的高密度场景中，Diffusion-AC不仅保持了94.1%的高成功率，而且与次优基线相比，将近距空中相撞（NMACs）的发生率降低了约59%，显著提升了系统安全裕度。", "conclusion": "该性能的显著提升源于其独特的多模态决策能力，这使得智能体能够灵活地切换到有效的替代机动方案。通过解决现有DRL方法的单峰偏差问题，Diffusion-AC在空域冲突解决中实现了更高的安全性与效率。"}}
{"id": "2509.03721", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.03721", "abs": "https://arxiv.org/abs/2509.03721", "authors": ["Cédric Join", "Michel Fliess"], "title": "Avoidance of an unexpected obstacle without reinforcement learning: Why not using advanced control-theoretic tools?", "comment": "IEEE 2025 - 13th International Conference on Systems and Control\n  (ICSC) - October 22-24, 2025 - Marrakesh, Morocco", "summary": "This communication on collision avoidance with unexpected obstacles is\nmotivated by some critical appraisals on reinforcement learning (RL) which\n\"requires ridiculously large numbers of trials to learn any new task\" (Yann\nLeCun). We use the classic Dubins' car in order to replace RL with\nflatness-based control, combined with the HEOL feedback setting, and the latest\nmodel-free predictive control approach. The two approaches lead to convincing\ncomputer experiments where the results with the model-based one are only\nslightly better. They exhibit a satisfactory robustness with respect to\nrandomly generated mismatches/disturbances, which become excellent in the\nmodel-free case. Those properties would have been perhaps difficult to obtain\nwith today's popular machine learning techniques in AI. Finally, we should\nemphasize that our two methods require a low computational burden.", "AI": {"tldr": "本文提出了一种基于平坦度控制和无模型预测控制的方法，用于Dubins汽车的意外障碍物避碰，以替代强化学习，展示了良好的鲁棒性和低计算负担。", "motivation": "由于强化学习（RL）在学习新任务时需要“荒谬的大量试验”（Yann LeCun），本研究旨在寻找一种更高效的避碰控制方法。", "method": "研究使用了经典的Dubins汽车模型，并采用了两种方法：1) 基于平坦度控制结合HEOL反馈设置；2) 最新的无模型预测控制方法。", "result": "两种方法都在计算机实验中取得了令人信服的结果。基于模型的方法（平坦度控制）结果略优。两种方法对随机生成的不匹配/干扰都表现出令人满意的鲁棒性，其中无模型方法表现出色。此外，这两种方法所需的计算负担较低。", "conclusion": "基于平坦度控制和无模型预测控制的方法为意外障碍物避碰提供了一种有效的替代方案，相较于强化学习，其具有更高的鲁棒性和更低的计算成本，这些特性在当前的流行AI机器学习技术中可能难以实现。"}}
{"id": "2509.03633", "categories": ["cs.CV", "cs.AI", "I.4.6; I.5.2; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.03633", "abs": "https://arxiv.org/abs/2509.03633", "authors": ["Josafat-Mattias Burmeister", "Andreas Tockner", "Stefan Reder", "Markus Engel", "Rico Richter", "Jan-Peter Mund", "Jürgen Döllner"], "title": "treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds", "comment": null, "summary": "Close-range laser scanning provides detailed 3D captures of forest stands but\nrequires efficient software for processing 3D point cloud data and extracting\nindividual trees. Although recent studies have introduced deep learning methods\nfor tree instance segmentation, these approaches require large annotated\ndatasets and substantial computational resources. As a resource-efficient\nalternative, we present a revised version of the treeX algorithm, an\nunsupervised method that combines clustering-based stem detection with region\ngrowing for crown delineation. While the original treeX algorithm was developed\nfor personal laser scanning (PLS) data, we provide two parameter presets, one\nfor ground-based laser scanning (stationary terrestrial - TLS and PLS), and one\nfor UAV-borne laser scanning (ULS). We evaluated the method on six public\ndatasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham\nWoods) and compared it to six open-source methods (original treeX, treeiso,\nRayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original\ntreeX algorithm, our revision reduces runtime and improves accuracy, with\ninstance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.\nFor ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original\nalgorithm fails to segment any correct instances. For TLS and PLS data, our\nalgorithm achieves accuracy similar to recent open-source methods, including\ndeep learning. Given its algorithmic design, we see two main applications for\nour method: (1) as a resource-efficient alternative to deep learning approaches\nin scenarios where the data characteristics align with the method design\n(sufficient stem visibility and point density), and (2) for the semi-automatic\ngeneration of labels for deep learning models. To enable broader adoption, we\nprovide an open-source Python implementation in the pointtree package.", "AI": {"tldr": "本文提出了一种修订版的无监督treeX算法，用于从地面和无人机激光扫描数据中高效地进行单木分割。该算法在准确性和运行时间上均优于原版，并可与深度学习方法媲美，且资源消耗更少，也可用于深度学习模型的数据标注。", "motivation": "近距离激光扫描能提供详细的森林三维数据，但需要高效软件处理点云并提取单木。现有深度学习方法虽有效，但需要大量标注数据和计算资源。因此，研究的动机是开发一种资源高效的替代方案。", "method": "研究采用并修订了无监督的treeX算法，该算法结合了基于聚类的树干检测和区域增长的树冠描绘。为适应不同数据源，提供了针对地面激光扫描（TLS和PLS）和无人机激光扫描（ULS）的两种参数预设。该方法在六个公开数据集上进行了评估，并与六种开源方法（包括原版treeX和深度学习方法）进行了比较。", "result": "与原版treeX算法相比，修订版减少了运行时间并提高了准确性，地面数据实例检测的F1分数提高了+0.11至+0.49。对于ULS数据，新预设实现了0.58的F1分数，而原版算法未能分割出任何正确实例。对于TLS和PLS数据，该算法的准确性与包括深度学习在内的最新开源方法相似。", "conclusion": "该修订版算法是一种资源高效的替代方案，适用于数据特性（足够的树干可见度和点密度）符合方法设计的场景。此外，它还可用于深度学习模型的半自动标签生成。为促进广泛应用，已在pointtree包中提供了开源Python实现。"}}
{"id": "2509.03690", "categories": ["cs.RO", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.03690", "abs": "https://arxiv.org/abs/2509.03690", "authors": ["Kelvin Daniel Gonzalez Amador"], "title": "Low-Cost Open-Source Ambidextrous Robotic Hand with 23 Direct-Drive servos for American Sign Language Alphabet", "comment": "9 pages, 8 figures, 4 tables. Submitted as preprint", "summary": "Accessible communication through sign language is vital for deaf communities,\n1 yet robotic solutions are often costly and limited. This study presents\nVulcanV3, a low- 2 cost, open-source, 3D-printed ambidextrous robotic hand\ncapable of reproducing the full 3 American Sign Language (ASL) alphabet (52\nsigns for right- and left-hand configurations). 4 The system employs 23\ndirect-drive servo actuators for precise finger and wrist movements, 5\ncontrolled by an Arduino Mega with dual PCA9685 modules. Unlike most humanoid\nupper- 6 limb systems, which rarely employ direct-drive actuation, VulcanV3\nachieves complete ASL 7 coverage with a reversible design. All CAD files and\ncode are released under permissive 8 open-source licenses to enable\nreplication. Empirical tests confirmed accurate reproduction 9 of all 52 ASL\nhandshapes, while a participant study (n = 33) achieved 96.97% recognition 10\naccuracy, improving to 98.78% after video demonstration. VulcanV3 advances\nassistive 11 robotics by combining affordability, full ASL coverage, and\nambidexterity in an openly 12 shared platform, contributing to accessible\ncommunication technologies and inclusive 13 innovation.", "AI": {"tldr": "VulcanV3是一种低成本、开源、3D打印的灵巧型机器人手，能够以高识别准确率再现完整的美国手语（ASL）字母表。", "motivation": "手语对聋哑社区的交流至关重要，但现有的机器人解决方案往往成本高昂且功能有限，阻碍了无障碍沟通的普及。", "method": "本研究提出了VulcanV3，一种低成本、开源、3D打印的灵巧型机器人手。它采用23个直驱伺服执行器进行精确的手指和手腕运动，由Arduino Mega和双PCA9685模块控制。该系统具有可逆设计，能够重现完整的美国手语字母表（左右手共52个手势）。所有CAD文件和代码均以开放源代码许可发布。", "result": "经验测试证实VulcanV3能够准确再现所有52种ASL手形。一项参与者研究（n=33）显示，其识别准确率达到96.97%，在视频演示后提高到98.78%。", "conclusion": "VulcanV3通过在一个开放共享的平台上结合了经济性、完整的ASL覆盖和灵巧性，推动了辅助机器人技术的发展，为无障碍通信技术和包容性创新做出了贡献。"}}
{"id": "2509.03528", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03528", "abs": "https://arxiv.org/abs/2509.03528", "authors": ["Matilde Contestabile", "Chiara Ferrara", "Alberto Giovannetti", "Giovanni Parrillo", "Andrea Vandin"], "title": "The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process", "comment": null, "summary": "Process Mining (PM), initially developed for industrial and business\ncontexts, has recently been applied to social systems, including legal ones.\nHowever, PM's efficacy in the legal domain is limited by the accessibility and\nquality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in\nItalian Chambers), a comprehensive event log of the Italian lawmaking process\nfrom 1987 to 2022. Created from unstructured data from the Normattiva portal\nand structured using large language models (LLMs), ProLiFIC aligns with recent\nefforts in integrating PM with LLMs. We exemplify preliminary analyses and\npropose ProLiFIC as a benchmark for legal PM, fostering new developments.", "AI": {"tldr": "本文介绍了ProLiFIC，一个利用大型语言模型（LLMs）从非结构化数据构建的意大利立法过程事件日志，旨在解决法律领域流程挖掘（PM）中数据集可访问性和质量的限制，并作为该领域的基准。", "motivation": "流程挖掘（PM）在法律领域的应用受到数据集可访问性和质量的限制，这阻碍了其在该领域的有效性。", "method": "研究者引入了ProLiFIC（意大利议会立法流程），这是一个包含1987年至2022年意大利立法过程的综合事件日志。该日志通过大型语言模型（LLMs）将来自Normattiva门户的非结构化数据进行结构化处理，实现了流程挖掘与LLMs的整合。", "result": "成功创建了ProLiFIC事件日志，并展示了初步分析结果。ProLiFIC被提议作为法律流程挖掘的基准。", "conclusion": "ProLiFIC通过提供高质量、结构化的法律流程数据集，解决了法律领域流程挖掘的数据瓶颈，并有望通过作为基准来促进该领域的新发展。"}}
{"id": "2509.03581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03581", "abs": "https://arxiv.org/abs/2509.03581", "authors": ["Davide Paglieri", "Bartłomiej Cupiał", "Jonathan Cook", "Ulyana Piterbarg", "Jens Tuyls", "Edward Grefenstette", "Jakob Nicolaus Foerster", "Jack Parker-Holder", "Tim Rocktäschel"], "title": "Learning When to Plan: Efficiently Allocating Test-Time Compute for LLM Agents", "comment": null, "summary": "Training large language models (LLMs) to reason via reinforcement learning\n(RL) significantly improves their problem-solving capabilities. In agentic\nsettings, existing methods like ReAct prompt LLMs to explicitly plan before\nevery action; however, we demonstrate that always planning is computationally\nexpensive and degrades performance on long-horizon tasks, while never planning\nfurther limits performance. To address this, we introduce a conceptual\nframework formalizing dynamic planning for LLM agents, enabling them to\nflexibly decide when to allocate test-time compute for planning. We propose a\nsimple two-stage training pipeline: (1) supervised fine-tuning on diverse\nsynthetic data to prime models for dynamic planning, and (2) RL to refine this\ncapability in long-horizon environments. Experiments on the Crafter environment\nshow that dynamic planning agents trained with this approach are more\nsample-efficient and consistently achieve more complex objectives.\nAdditionally, we demonstrate that these agents can be effectively steered by\nhuman-written plans, surpassing their independent capabilities. To our\nknowledge, this work is the first to explore training LLM agents for dynamic\ntest-time compute allocation in sequential decision-making tasks, paving the\nway for more efficient, adaptive, and controllable agentic systems.", "AI": {"tldr": "本文提出了一种针对大型语言模型（LLM）代理的动态规划框架和两阶段训练方法，使其能够灵活决定何时进行规划，从而在长周期任务中实现更高效、自适应和可控的问题解决能力。", "motivation": "现有的LLM代理（如ReAct）在每次行动前进行显式规划，这在计算上成本高昂，并可能在长周期任务中降低性能；而从不规划则会进一步限制性能。因此，研究的动机是需要一种机制，使LLM代理能够灵活地决定何时分配计算资源进行规划。", "method": "研究引入了一个形式化动态规划的概念框架，并提出了一个简单的两阶段训练流程：1) 在多样化的合成数据上进行监督微调（SFT），以初步培养模型的动态规划能力；2) 使用强化学习（RL）在长周期环境中进一步完善和优化这种能力。", "result": "在Crafter环境中的实验表明，通过这种方法训练的动态规划代理具有更高的样本效率，并能持续完成更复杂的任务目标。此外，这些代理可以被人类编写的计划有效引导，其性能甚至超越了独立执行时的能力。", "conclusion": "这项工作首次探索了在顺序决策任务中训练LLM代理进行动态测试时计算分配，为开发更高效、自适应和可控的代理系统铺平了道路。"}}
{"id": "2509.03789", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.03789", "abs": "https://arxiv.org/abs/2509.03789", "authors": ["Muratkhan Abdirash", "Xiaofan Cui"], "title": "Decentralized Safety-Critical Control of Resilient DC Microgrids with Large-Signal Stability Guarantees", "comment": null, "summary": "The increasing penetration of distributed energy resources and\npower-electronics interfaces in DC microgrids, coupled with rising cyber\nthreats, necessitates primary controllers that are provably safe,\ncyber-resilient, and practical. The increasing penetration of distributed\nenergy resources and power-electronics interfaces in DC microgrids, coupled\nwith rising cyber threats, necessitates primary controllers that are provably\nsafe, cyber-resilient, and practical. Conventional droop-based methods remain\nprevalent due to their simplicity, yet their design is largely empirical and\nconservative, lacking rigorous guarantees. Advanced strategies improve certain\naspects, but often sacrifice scalability, robustness, or formal safety. In this\nwork, we propose a Distributed Safety-Critical Controller (DSCC) that\nsystematically integrates global stabilization with formal safety guarantees in\na fully decentralized manner. Leveraging control barrier functions and the\nport-Hamiltonian system theory, the DSCC achieves scalable safe stabilization\nwhile preserving real-time implementability. High-fidelity switched-circuit\nsimulations validate the controller's advantages under various contingencies.\nThis framework paves the way for resilient, safety-critical, and scalable\ncontrol in next-generation DC microgrids.", "AI": {"tldr": "本文提出了一种分布式安全关键控制器（DSCC），它通过结合控制障碍函数和端口-哈密顿系统理论，为直流微电网实现了可扩展、安全且实用的稳定控制，以应对日益增长的分布式能源渗透和网络威胁。", "motivation": "随着分布式能源和电力电子接口在直流微电网中的普及以及网络威胁的增加，需要具有可证明的安全性、网络弹性和实用性的初级控制器。传统的下垂控制方法虽然简单但缺乏严格保证，设计经验性且保守；而先进策略常牺牲可扩展性、鲁棒性或形式安全性。", "method": "本文提出了一种分布式安全关键控制器（DSCC），该控制器以完全去中心化的方式系统地集成了全局稳定性和形式安全保证。它利用控制障碍函数（CBF）和端口-哈密顿系统理论（PHS），实现了可扩展的安全稳定控制。", "result": "DSCC实现了可扩展的安全稳定控制，同时保留了实时可实现性。高保真开关电路仿真验证了该控制器在各种突发情况下的优势。", "conclusion": "该框架为下一代直流微电网中弹性、安全关键和可扩展的控制铺平了道路。"}}
{"id": "2509.03635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03635", "abs": "https://arxiv.org/abs/2509.03635", "authors": ["Hongpei Zheng", "Lintao Xiang", "Qijun Yang", "Qian Lin", "Hujun Yin"], "title": "Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding", "comment": "16 pages, 6 figures", "summary": "The rapid development of Large Multimodal Models (LMMs) has led to remarkable\nprogress in 2D visual understanding; however, extending these capabilities to\n3D scene understanding remains a significant challenge. Existing approaches\npredominantly rely on text-only supervision, which fails to provide the\ngeometric constraints required for learning robust 3D spatial representations.\nIn this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction\nTuning framework that addresses this limitation by incorporating geometry-aware\nsupervision directly into the training process. Our key insight is that\neffective 3D understanding necessitates reconstructing underlying geometric\nstructures rather than merely describing them. Unlike existing methods that\ninject 3D information solely at the input level, Reg3D adopts a\ndual-supervision paradigm that leverages 3D geometric information both as input\nand as explicit learning targets. Specifically, we design complementary\nobject-level and frame-level reconstruction tasks within a dual-encoder\narchitecture, enforcing geometric consistency to encourage the development of\nspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,\nScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance\nimprovements, establishing a new training paradigm for spatially aware\nmultimodal models.", "AI": {"tldr": "Reg3D是一个新颖的框架，通过在训练过程中直接整合几何感知监督，解决了大型多模态模型在3D场景理解中缺乏几何约束的问题，显著提升了空间感知多模态模型的性能。", "motivation": "现有的大型多模态模型在2D视觉理解方面进展显著，但在3D场景理解方面仍面临挑战。主要问题在于现有方法大多依赖纯文本监督，无法提供学习鲁棒3D空间表示所需的几何约束。", "method": "本文提出了Reg3D，一个重建几何指令微调框架。其核心思想是3D理解需要重建而非仅仅描述几何结构。Reg3D采用双重监督范式，将3D几何信息作为输入和显式学习目标。具体而言，它在一个双编码器架构中设计了互补的对象级和帧级重建任务，以强制执行几何一致性，从而培养空间推理能力。", "result": "在ScanQA、Scan2Cap、ScanRefer和SQA3D等数据集上的大量实验表明，Reg3D带来了显著的性能提升，为空间感知多模态模型建立了一种新的训练范式。", "conclusion": "Reg3D通过引入几何感知监督和重建任务，有效解决了3D场景理解中几何约束不足的问题，显著提升了多模态模型对3D空间的理解能力，并开创了新的训练范式。"}}
{"id": "2509.03804", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.03804", "abs": "https://arxiv.org/abs/2509.03804", "authors": ["Ad-Deen Mahbub", "Md Ragib Shaharear"], "title": "Real-Time Buoyancy Estimation for AUV Simulations Using Convex Hull-Based Submerged Volume Calculation", "comment": "7 pages, 10 figures", "summary": "Accurate real-time buoyancy modeling is essential for high-fidelity\nAutonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a\nnative buoyancy system, requiring external solutions for precise underwater\nphysics. This paper presents a novel convex hull-based approach to dynamically\ncompute the submerged volume of an AUV in real time. By extracting mesh\ngeometry from the simulation environment and calculating the hull portion\nintersecting the water level along the z-axis, our method enhances accuracy\nover traditional geometric approximations. A cross-sectional area extension\nreduces computational overhead, enabling efficient buoyant force updates that\nadapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested\non a custom AUV design for SAUVC 2025, this approach delivers real-time\nperformance and scalability, improving simulation fidelity for underwater\nrobotics research without precomputed hydrodynamic models.", "AI": {"tldr": "本文提出了一种基于凸包的新方法，用于在NVIDIA Isaac Sim中实时、准确地计算AUV的浮力，解决了平台缺乏原生浮力系统的问题。", "motivation": "高精度AUV仿真需要准确的实时浮力建模，但NVIDIA Isaac Sim缺乏原生的浮力系统，需要外部解决方案来实现精确的水下物理模拟。", "method": "该方法通过从仿真环境中提取网格几何体，并沿Z轴计算与水面相交的凸包部分，动态计算AUV的浸没体积。此外，通过横截面积扩展技术降低了计算开销，实现了高效的浮力更新，能适应姿态、深度和正弦波波动。", "result": "该方法在为SAUVC 2025设计的定制AUV上进行了测试，实现了实时性能和可扩展性，提高了水下机器人研究的仿真保真度，且无需预计算流体动力学模型。", "conclusion": "所提出的基于凸包的浮力计算方法为Isaac Sim中的AUV仿真提供了准确、实时的浮力模型，显著提升了仿真精度和效率，且能够适应复杂的环境变化。"}}
{"id": "2509.03529", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03529", "abs": "https://arxiv.org/abs/2509.03529", "authors": ["Alejandro Álvarez Castro", "Joaquín Ordieres-Meré"], "title": "Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages", "comment": "Presented at NLMLT2025 (https://airccse.org/csit/V15N16.html), 15\n  pages, 5 figures", "summary": "Earnings calls represent a uniquely rich and semi-structured source of\nfinancial communication, blending scripted managerial commentary with\nunscripted analyst dialogue. Although recent advances in financial sentiment\nanalysis have integrated multi-modal signals, such as textual content and vocal\ntone, most systems rely on flat document-level or sentence-level models,\nfailing to capture the layered discourse structure of these interactions. This\npaper introduces a novel multi-modal framework designed to generate\nsemantically rich and structurally aware embeddings of earnings calls, by\nencoding them as hierarchical discourse trees. Each node, comprising either a\nmonologue or a question-answer pair, is enriched with emotional signals derived\nfrom text, audio, and video, as well as structured metadata including coherence\nscores, topic labels, and answer coverage assessments. A two-stage transformer\narchitecture is proposed: the first encodes multi-modal content and discourse\nmetadata at the node level using contrastive learning, while the second\nsynthesizes a global embedding for the entire conference. Experimental results\nreveal that the resulting embeddings form stable, semantically meaningful\nrepresentations that reflect affective tone, structural logic, and thematic\nalignment. Beyond financial reporting, the proposed system generalizes to other\nhigh-stakes unscripted communicative domains such as tele-medicine, education,\nand political discourse, offering a robust and explainable approach to\nmulti-modal discourse representation. This approach offers practical utility\nfor downstream tasks such as financial forecasting and discourse evaluation,\nwhile also providing a generalizable method applicable to other domains\ninvolving high-stakes communication.", "AI": {"tldr": "本文提出了一种新颖的多模态框架，通过将财报电话会议编码为分层话语树，并使用两阶段Transformer架构生成语义丰富、结构感知的嵌入，以克服现有扁平化模型的局限性。", "motivation": "现有的金融情感分析系统虽然整合了文本和语音等多模态信号，但大多依赖于扁平的文档级或句子级模型，未能捕捉财报电话会议中分层的话语结构，而财报电话会议本身是金融沟通中独特且半结构化的丰富来源。", "method": "该框架将财报电话会议编码为分层话语树。每个节点（独白或问答对）都通过文本、音频和视频中的情感信号以及连贯性得分、主题标签和回答覆盖率评估等结构化元数据进行丰富。采用两阶段Transformer架构：第一阶段使用对比学习编码节点级别的多模态内容和话语元数据；第二阶段综合生成整个会议的全局嵌入。", "result": "实验结果表明，生成的嵌入形成了稳定、语义有意义的表示，能够反映情感基调、结构逻辑和主题一致性。此外，该系统可推广到远程医疗、教育和政治话语等其他高风险非脚本式沟通领域。", "conclusion": "该方法为多模态话语表示提供了一种鲁棒且可解释的方法，对金融预测和话语评估等下游任务具有实际效用，并可推广应用于其他涉及高风险沟通的领域。"}}
{"id": "2509.03626", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03626", "abs": "https://arxiv.org/abs/2509.03626", "authors": ["Zahra Zehtabi Sabeti Moghaddam", "Zeinab Dehghani", "Maneeha Rani", "Koorosh Aslansefat", "Bhupesh Kumar Mishra", "Rameez Raja Kureshi", "Dhavalkumar Thakker"], "title": "Explainable Knowledge Graph Retrieval-Augmented Generation (KG-RAG) with KG-SMILE", "comment": null, "summary": "Generative AI, such as Large Language Models (LLMs), has achieved impressive\nprogress but still produces hallucinations and unverifiable claims, limiting\nreliability in sensitive domains. Retrieval-Augmented Generation (RAG) improves\naccuracy by grounding outputs in external knowledge, especially in domains like\nhealthcare, where precision is vital. However, RAG remains opaque and\nessentially a black box, heavily dependent on data quality. We developed a\nmethod-agnostic, perturbation-based framework that provides token and\ncomponent-level interoperability for Graph RAG using SMILE and named it as\nKnowledge-Graph (KG)-SMILE. By applying controlled perturbations, computing\nsimilarities, and training weighted linear surrogates, KG-SMILE identifies the\ngraph entities and relations most influential to generated outputs, thereby\nmaking RAG more transparent. We evaluate KG-SMILE using comprehensive\nattribution metrics, including fidelity, faithfulness, consistency, stability,\nand accuracy. Our findings show that KG-SMILE produces stable, human-aligned\nexplanations, demonstrating its capacity to balance model effectiveness with\ninterpretability and thereby fostering greater transparency and trust in\nmachine learning technologies.", "AI": {"tldr": "本文提出KG-SMILE，一个基于扰动的框架，为图RAG提供可解释性，通过识别对生成输出最具影响力的图实体和关系，从而提高透明度和信任。", "motivation": "生成式AI（如LLMs）存在幻觉和不可验证声明，限制了其在敏感领域的可靠性。检索增强生成（RAG）通过外部知识提高了准确性，但在医疗等领域仍缺乏透明度，且高度依赖数据质量，本质上是一个黑箱。", "method": "开发了一种与方法无关的、基于扰动的框架KG-SMILE，为使用SMILE的图RAG提供token和组件级别的互操作性。通过应用受控扰动、计算相似性并训练加权线性替代模型，KG-SMILE识别出对生成输出最有影响的图实体和关系。", "result": "通过保真度、忠实度、一致性、稳定性、准确性等归因指标评估，KG-SMILE产生了稳定且与人类对齐的解释。这表明它能够平衡模型有效性和可解释性。", "conclusion": "KG-SMILE通过提高RAG的透明度和可解释性，增强了机器学习技术的信任度，平衡了模型效果与可解释性。"}}
{"id": "2509.03836", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.03836", "abs": "https://arxiv.org/abs/2509.03836", "authors": ["Bingxin Zhang", "Han Zhang", "Kun Yang", "Yizhe Zhao", "Kezhi Wang"], "title": "On the Performance Analysis of Pinching-Antenna-Enabled SWIPT Systems", "comment": null, "summary": "In this paper, we studies the performance of a novel simultaneous wireless\ninformation and power transfer (SWIPT) system enabled by a flexible\npinching-antenna. To support flexible deployment and optimize energy-rate\nperformance, we propose three practical pinching antenna placement-schemes: the\nedge deployment scheme (EDS), the center deployment scheme (CDS), and the\ndiagonal deployment scheme (DDS). Moreover, a hybrid time-switching (TS) and\npower-splitting (PS) protocol is introduced, allowing dynamic adjustment\nbetween energy harvesting and information decoding. Under each deployment\nstrategy and the transmission protocol, closed-form expressions for the average\nharvested energy and average achievable rate of a randomly located user\nequipment (UE) are derived based on the optimal positioning of the\npinching-antenna. Numerical simulations confirm the accuracy of the theoretical\nanalysis and illustrate the trade-off between rate and energy harvesting under\ndifferent schemes.", "AI": {"tldr": "本文研究了一种基于柔性收缩天线的同步无线信息与功率传输（SWIPT）系统性能，通过提出天线部署方案和混合协议来优化能量-速率性能。", "motivation": "为了支持灵活部署并优化能量收集和信息传输的性能，研究人员旨在开发一种能够动态调整能量收集和信息解码的SWIPT系统。", "method": "研究提出了三种实用的收缩天线部署方案（边缘部署、中心部署和对角部署），并引入了混合时分切换（TS）和功率分割（PS）协议。在此基础上，推导了随机用户设备的平均收集能量和平均可达速率的闭式表达式，并通过数值模拟验证了理论分析的准确性。", "result": "研究成功推导了在不同部署策略和传输协议下，随机用户设备的平均收集能量和平均可达速率的闭式表达式。数值模拟结果证实了理论分析的准确性，并揭示了不同方案下速率和能量收集之间的权衡关系。", "conclusion": "该研究通过引入柔性收缩天线、优化部署方案和混合TS/PS协议，为SWIPT系统提供了一种有效提升能量-速率性能的方法，并量化了其在不同配置下的性能权衡。"}}
{"id": "2509.03704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03704", "abs": "https://arxiv.org/abs/2509.03704", "authors": ["Seth Z. Zhao", "Huizhi Zhang", "Zhaowei Li", "Juntong Peng", "Anthony Chui", "Zewei Zhou", "Zonglin Meng", "Hao Xiang", "Zhiyu Huang", "Fujia Wang", "Ran Tian", "Chenfeng Xu", "Bolei Zhou", "Jiaqi Ma"], "title": "QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception", "comment": null, "summary": "Cooperative perception through Vehicle-to-Everything (V2X) communication\noffers significant potential for enhancing vehicle perception by mitigating\nocclusions and expanding the field of view. However, past research has\npredominantly focused on improving accuracy metrics without addressing the\ncrucial system-level considerations of efficiency, latency, and real-world\ndeployability. Noticeably, most existing systems rely on full-precision models,\nwhich incur high computational and transmission costs, making them impractical\nfor real-time operation in resource-constrained environments. In this paper, we\nintroduce \\textbf{QuantV2X}, the first fully quantized multi-agent system\ndesigned specifically for efficient and scalable deployment of multi-modal,\nmulti-agent V2X cooperative perception. QuantV2X introduces a unified\nend-to-end quantization strategy across both neural network models and\ntransmitted message representations that simultaneously reduces computational\nload and transmission bandwidth. Remarkably, despite operating under low-bit\nconstraints, QuantV2X achieves accuracy comparable to full-precision systems.\nMore importantly, when evaluated under deployment-oriented metrics, QuantV2X\nreduces system-level latency by 3.2$\\times$ and achieves a +9.5 improvement in\nmAP30 over full-precision baselines. Furthermore, QuantV2X scales more\neffectively, enabling larger and more capable models to fit within strict\nmemory budgets. These results highlight the viability of a fully quantized\nmulti-agent intermediate fusion system for real-world deployment. The system\nwill be publicly released to promote research in this field:\nhttps://github.com/ucla-mobility/QuantV2X.", "AI": {"tldr": "本文提出了QuantV2X，这是首个全量化多智能体系统，专为高效、可扩展的多模态、多智能体V2X协作感知设计，通过统一的端到端量化策略显著降低了计算和传输成本，同时保持了与全精度系统相当的感知准确性，并大幅提升了部署效率。", "motivation": "V2X协作感知在增强车辆感知方面潜力巨大，但现有研究主要关注准确性，忽视了效率、延迟和实际部署性等系统级关键因素。大多数现有系统依赖全精度模型，导致高昂的计算和传输成本，使其在资源受限环境中难以实时运行。", "method": "本文引入了QuantV2X，这是第一个完全量化的多智能体系统，专为高效和可扩展的V2X协作感知部署而设计。它在神经网络模型和传输消息表示中引入了统一的端到端量化策略，同时减少了计算负载和传输带宽。", "result": "QuantV2X在低比特约束下仍能达到与全精度系统相当的准确性。在面向部署的指标评估中，QuantV2X将系统级延迟降低了3.2倍，并在mAP30上比全精度基线提高了9.5。此外，QuantV2X扩展性更强，允许在严格的内存预算内适应更大、功能更强的模型。", "conclusion": "研究结果强调了QuantV2X这种完全量化的多智能体中间融合系统在实际部署中的可行性。"}}
{"id": "2509.03842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03842", "abs": "https://arxiv.org/abs/2509.03842", "authors": ["Guanglu Jia", "Ceng Zhang", "Gregory S. Chirikjian"], "title": "INGRID: Intelligent Generative Robotic Design Using Large Language Models", "comment": "15 pages, 6 figures", "summary": "The integration of large language models (LLMs) into robotic systems has\naccelerated progress in embodied artificial intelligence, yet current\napproaches remain constrained by existing robotic architectures, particularly\nserial mechanisms. This hardware dependency fundamentally limits the scope of\nrobotic intelligence. Here, we present INGRID (Intelligent Generative Robotic\nDesign), a framework that enables the automated design of parallel robotic\nmechanisms through deep integration with reciprocal screw theory and kinematic\nsynthesis methods. We decompose the design challenge into four progressive\ntasks: constraint analysis, kinematic joint generation, chain construction, and\ncomplete mechanism design. INGRID demonstrates the ability to generate novel\nparallel mechanisms with both fixed and variable mobility, discovering\nkinematic configurations not previously documented in the literature. We\nvalidate our approach through three case studies demonstrating how INGRID\nassists users in designing task-specific parallel robots based on desired\nmobility requirements. By bridging the gap between mechanism theory and machine\nlearning, INGRID enables researchers without specialized robotics training to\ncreate custom parallel mechanisms, thereby decoupling advances in robotic\nintelligence from hardware constraints. This work establishes a foundation for\nmechanism intelligence, where AI systems actively design robotic hardware,\npotentially transforming the development of embodied AI systems.", "AI": {"tldr": "INGRID是一个将大语言模型（LLMs）与倒数螺旋理论和运动学综合方法深度结合的框架，旨在实现并联机器人机构的自动化设计，从而突破现有硬件对机器人智能的限制。", "motivation": "将LLMs集成到机器人系统中加速了具身人工智能的进展，但现有方法受限于串联机构等硬件，这种硬件依赖性从根本上限制了机器人智能的范围。研究的动机是解耦机器人智能的进步与硬件约束。", "method": "本研究提出了INGRID（智能生成机器人设计）框架，通过与倒数螺旋理论和运动学综合方法的深度集成，实现并联机器人机构的自动化设计。设计挑战被分解为四个渐进任务：约束分析、运动关节生成、链构造和完整机构设计。", "result": "INGRID展示了生成具有固定和可变移动性的新型并联机构的能力，并发现了文献中未曾记载的运动学配置。通过三个案例研究验证了该方法，展示了INGRID如何根据所需的移动性要求协助用户设计特定任务的并联机器人。它使得没有专业机器人学培训的研究人员也能创建定制的并联机构。", "conclusion": "INGRID通过弥合机构理论和机器学习之间的鸿沟，使机器人智能的进步不再受硬件限制。这项工作为“机构智能”奠定了基础，即AI系统主动设计机器人硬件，这可能彻底改变具身AI系统的发展。"}}
{"id": "2509.03530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03530", "abs": "https://arxiv.org/abs/2509.03530", "authors": ["Paul Blum", "Enrico Liscio", "Ruixuan Zhang", "Caroline Figueroa", "Pradeep K. Murukannaiah"], "title": "Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts", "comment": null, "summary": "Suicide is a leading cause of death among adolescents (12-18), yet predicting\nit remains a significant challenge. Many cases go undetected due to a lack of\ncontact with mental health services. Social media, however, offers a unique\nopportunity, as young people often share their thoughts and struggles online in\nreal time. In this work, we propose a novel task and method to approach it:\npredicting suicidal ideation and behavior (SIB) from forum posts before an\nadolescent explicitly expresses suicidal ideation on an online forum. This\npredictive framing, where no self-disclosure is used as input at any stage,\nremains largely unexplored in the suicide prediction literature. To this end,\nwe introduce Early-SIB, a transformer-based model that sequentially processes\nthe posts a user writes and engages with to predict whether they will write a\nSIB post. Our model achieves a balanced accuracy of 0.73 for predicting future\nSIB on a Dutch youth forum, demonstrating that such tools can offer a\nmeaningful addition to traditional methods.", "AI": {"tldr": "本文提出了一种新颖的任务和基于Transformer的模型Early-SIB，用于在青少年在线论坛上，在其明确表达自杀意念之前，预测其未来的自杀意念和行为（SIB）。", "motivation": "自杀是青少年死亡的主要原因，但预测自杀仍面临巨大挑战，许多案例因缺乏心理健康服务接触而未被发现。社交媒体为早期干预提供了独特机会，因为青少年常在线上实时分享想法和困扰。", "method": "提出了一种新颖的预测框架，即在用户尚未明确表达自杀意念时进行预测。引入了Early-SIB模型，这是一个基于Transformer的模型，它顺序处理用户撰写和参与的帖子，以预测他们是否会发布SIB帖子。", "result": "Early-SIB模型在荷兰青少年论坛上预测未来SIB的平衡准确率达到0.73，表明此类工具可以有效补充传统预测方法。", "conclusion": "该研究证明，通过分析青少年在线论坛上的帖子，可以在他们明确表达自杀意念之前，有效地预测其未来的自杀意念和行为，为早期干预提供了有意义的工具。"}}
{"id": "2509.03636", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03636", "abs": "https://arxiv.org/abs/2509.03636", "authors": ["Jacqueline Maasch", "John Kalantari", "Kia Khezeli"], "title": "CausalARC: Abstract Reasoning with Causal World Models", "comment": null, "summary": "Reasoning requires adaptation to novel problem settings under limited data\nand distribution shift. This work introduces CausalARC: an experimental testbed\nfor AI reasoning in low-data and out-of-distribution regimes, modeled after the\nAbstraction and Reasoning Corpus (ARC). Each CausalARC reasoning task is\nsampled from a fully specified causal world model, formally expressed as a\nstructural causal model. Principled data augmentations provide observational,\ninterventional, and counterfactual feedback about the world model in the form\nof few-shot, in-context learning demonstrations. As a proof-of-concept, we\nillustrate the use of CausalARC for four language model evaluation settings:\n(1) abstract reasoning with test-time training, (2) counterfactual reasoning\nwith in-context learning, (3) program synthesis, and (4) causal discovery with\nlogical reasoning.", "AI": {"tldr": "CausalARC是一个受ARC启发的实验性测试平台，用于在低数据和分布外条件下评估AI推理能力，其任务基于结构化因果模型，并通过因果反馈进行数据增强。", "motivation": "AI推理需要适应新问题、有限数据和分布偏移，这促使研究者开发一个能够在这种挑战性环境下测试AI推理能力的平台。", "method": "引入了CausalARC测试平台，其推理任务来源于完全指定的因果世界模型（结构化因果模型）。通过原则性的数据增强，提供观测、干预和反事实反馈，形成少样本、上下文学习的演示。作为概念验证，该平台被用于语言模型的四种评估场景。", "result": "CausalARC被成功应用于概念验证，评估了语言模型在以下四个场景中的表现：(1) 带测试时训练的抽象推理，(2) 带上下文学习的反事实推理，(3) 程序合成，以及(4) 带逻辑推理的因果发现。", "conclusion": "CausalARC提供了一个有效的实验测试平台，用于在低数据和分布外条件下，通过因果世界模型和多样化的因果反馈，评估和理解AI（特别是语言模型）的推理能力。"}}
{"id": "2509.03839", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2509.03839", "abs": "https://arxiv.org/abs/2509.03839", "authors": ["Daisuke Inoue", "Tadayoshi Matsumori", "Gouhei Tanaka", "Yuji Ito"], "title": "Reservoir Predictive Path Integral Control for Unknown Nonlinear Dynamics", "comment": "Submitted to IEEE for possible publication, 13 pages, 7 figures", "summary": "Neural networks capable of approximating complex nonlinearities have found\nextensive application in data-driven control of nonlinear dynamical systems.\nHowever, fast online identification and control of unknown dynamics remain\ncentral challenges. This paper integrates echo-state networks (ESNs) --\nreservoir computing models implemented with recurrent neural networks -- and\nmodel predictive path integral (MPPI) control -- sampling-based variants of\nmodel predictive control -- to meet these challenges. The proposed reservoir\npredictive path integral (RPPI) enables fast learning of nonlinear dynamics\nwith ESN and exploits the learned nonlinearities directly in parallelized MPPI\ncontrol computation without linearization approximations. The framework is\nfurther extended to uncertainty-aware RPPI (URPPI), which leverages ESN\nuncertainty to balance exploration and exploitation: exploratory inputs\ndominate during early learning, while exploitative inputs prevail as model\nconfidence grows. Experiments on controlling the Duffing oscillator and\nfour-tank systems demonstrate that URPPI improves control performance, reducing\ncontrol costs by up to 60% compared to traditional quadratic programming-based\nmodel predictive control methods.", "AI": {"tldr": "本文提出了一种结合回声状态网络（ESN）和模型预测路径积分（MPPI）的新型控制框架RPPI（及其不确定性感知版本URPPI），旨在实现非线性动力系统的快速在线学习和控制，并显著提高了控制性能。", "motivation": "非线性动力系统的快速在线识别和控制是当前面临的核心挑战。", "method": "该研究将回声状态网络（ESN）与模型预测路径积分（MPPI）控制相结合，提出了储层预测路径积分（RPPI）框架。ESN用于快速学习非线性动力学，MPPI则直接利用学习到的非线性特性进行并行化控制计算，无需线性化近似。此外，该框架进一步扩展为不确定性感知RPPI（URPPI），利用ESN的不确定性来平衡探索和利用，在早期学习阶段侧重探索，模型置信度提高后侧重利用。", "result": "在控制Duffing振子和四罐系统的实验中，URPPI显著提升了控制性能，与传统的基于二次规划的模型预测控制方法相比，控制成本降低了高达60%。", "conclusion": "RPPI和URPPI框架有效解决了非线性系统的快速在线学习和控制问题，并通过不确定性感知机制优化了探索与利用的平衡，从而显著提高了控制性能。"}}
{"id": "2509.03729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03729", "abs": "https://arxiv.org/abs/2509.03729", "authors": ["Bandita Bharadwaj", "Ankur Mishra", "Saurav Bharadwaj"], "title": "Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns", "comment": null, "summary": "This study evaluates the efficacy of three deep learning architectures:\nResNet50, MobileNetV2, and EfficientNetB0 for automated plant species\nclassification based on leaf venation patterns, a critical morphological\nfeature with high taxonomic relevance. Using the Swedish Leaf Dataset\ncomprising images from 15 distinct species (75 images per species, totalling\n1,125 images), the models were demonstrated using standard performance metrics\nduring training and testing phases. ResNet50 achieved a training accuracy of\n94.11% but exhibited overfitting, reflected by a reduced testing accuracy of\n88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better\ngeneralization capabilities, attaining a testing accuracy of 93.34% and an F1\nscore of 93.23%, indicating its suitability for lightweight, real-time\napplications. EfficientNetB0 outperformed both models, achieving a testing\naccuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,\nhighlighting its robustness in venation-based classification. The findings\nunderscore the potential of deep learning, particularly EfficientNetB0, in\ndeveloping scalable and accurate tools for automated plant taxonomy using\nvenation traits.", "AI": {"tldr": "本研究评估了ResNet50、MobileNetV2和EfficientNetB0三种深度学习架构在基于叶脉模式的植物物种自动分类中的性能，发现EfficientNetB0表现最佳，测试准确率达到94.67%。", "motivation": "叶脉模式是具有高度分类学相关性的关键形态特征。研究旨在利用深度学习开发可扩展且准确的自动化工具，进行基于叶脉特征的植物分类。", "method": "研究评估了ResNet50、MobileNetV2和EfficientNetB0三种深度学习模型。使用包含15种植物物种（每种75张图像，共1,125张图像）的瑞典叶片数据集，以叶脉模式作为分类特征。模型性能通过训练和测试阶段的标准性能指标（如准确率、F1分数、精确率、召回率）进行评估。", "result": "ResNet50训练准确率94.11%，测试准确率88.45%，F1分数87.82%，存在过拟合。MobileNetV2表现出更好的泛化能力，测试准确率93.34%，F1分数93.23%，适用于轻量级实时应用。EfficientNetB0表现最优，测试准确率94.67%，精确率、召回率和F1分数均超过94.6%。", "conclusion": "研究结果强调了深度学习，特别是EfficientNetB0，在利用叶脉特征开发可扩展且准确的植物物种自动分类工具方面的巨大潜力。"}}
{"id": "2509.03859", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.03859", "abs": "https://arxiv.org/abs/2509.03859", "authors": ["Haichao Zhang", "Haonan Yu", "Le Zhao", "Andrew Choi", "Qinxun Bai", "Yiqing Yang", "Wei Xu"], "title": "Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator", "comment": "Project: https://horizonrobotics.github.io/gail/SLIM", "summary": "Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM.", "AI": {"tldr": "本文提出了一种在模拟环境中训练视觉-运动策略的方法，用于四足机器人移动操作的抓取-放置任务，并在真实世界中实现了近80%的成功率，展示了高效的仿真到现实迁移。", "motivation": "四足机器人移动操作面临技能多样性、任务周期长和部分可观察性等重大挑战，需要一种能够有效应对这些复杂性的方法。", "method": "研究者提出了一种完全在模拟环境中训练视觉-运动策略的方法，用于多阶段的抓取-放置任务。该策略能够高效执行搜索、接近、抓取、运输和放置等动作，并涌现出重新抓取和任务链等行为。通过广泛的真实世界实验和消融研究来验证关键技术。", "result": "所提出的策略在真实世界中实现了近80%的成功率。实验证明了高效训练和有效的仿真到现实迁移的关键技术。此外，该方法在多种室内外环境中均表现出良好的部署能力。", "conclusion": "通过在模拟环境中训练的视觉-运动策略，四足机器人能够成功执行复杂的移动操作任务，并在真实世界中实现高成功率和鲁棒的仿真到现实迁移，同时展现出复杂的自主行为。"}}
{"id": "2509.03531", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03531", "abs": "https://arxiv.org/abs/2509.03531", "authors": ["Oscar Obeso", "Andy Arditi", "Javier Ferrando", "Joshua Freeman", "Cameron Holmes", "Neel Nanda"], "title": "Real-Time Detection of Hallucinated Entities in Long-Form Generation", "comment": null, "summary": "Large language models are now routinely used in high-stakes applications\nwhere hallucinations can cause serious harm, such as medical consultations or\nlegal advice. Existing hallucination detection methods, however, are\nimpractical for real-world use, as they are either limited to short factual\nqueries or require costly external verification. We present a cheap, scalable\nmethod for real-time identification of hallucinated tokens in long-form\ngenerations, and scale it effectively to 70B parameter models. Our approach\ntargets \\emph{entity-level hallucinations} -- e.g., fabricated names, dates,\ncitations -- rather than claim-level, thereby naturally mapping to token-level\nlabels and enabling streaming detection. We develop an annotation methodology\nthat leverages web search to annotate model responses with grounded labels\nindicating which tokens correspond to fabricated entities. This dataset enables\nus to train effective hallucination classifiers with simple and efficient\nmethods such as linear probes. Evaluating across four model families, our\nclassifiers consistently outperform baselines on long-form responses, including\nmore expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for\nLlama-3.3-70B), and are also an improvement in short-form question-answering\nsettings. Moreover, despite being trained only with entity-level labels, our\nprobes effectively detect incorrect answers in mathematical reasoning tasks,\nindicating generalization beyond entities. While our annotation methodology is\nexpensive, we find that annotated responses from one model can be used to train\neffective classifiers on other models; accordingly, we publicly release our\ndatasets to facilitate reuse. Overall, our work suggests a promising new\napproach for scalable, real-world hallucination detection.", "AI": {"tldr": "本文提出了一种经济、可扩展的方法，用于实时识别大型语言模型长文本生成中的实体级幻觉（如虚构的名称、日期、引用），并将其成功应用于70B参数模型，性能优于现有基线方法。", "motivation": "大型语言模型在医疗咨询、法律建议等高风险应用中被广泛使用，但其产生的幻觉可能导致严重危害。现有的幻觉检测方法要么局限于简短的事实查询，要么需要昂贵的外部验证，不适用于实际应用。", "method": "该方法针对实体级幻觉（如虚构的名称、日期、引用），将其映射到token级别的标签，从而实现流式检测。研究人员开发了一种利用网络搜索进行标注的方法，创建了一个带有接地标签的数据集，用于训练高效的幻觉分类器（例如线性探针）。", "result": "在四种模型家族上的评估显示，该分类器在长文本回复上始终优于基线方法（包括语义熵），并且在短文本问答设置中也有所改进（例如，Llama-3.3-70B的AUC从0.71提高到0.90）。尽管仅使用实体级标签进行训练，这些探针也能有效检测数学推理任务中的错误答案，表明其泛化能力。此外，一个模型标注的响应数据可用于训练其他模型的有效分类器。", "conclusion": "这项工作为可扩展、实际应用的幻觉检测提供了一种有前景的新方法。研究人员还公开了数据集以促进重用。"}}
{"id": "2509.03644", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03644", "abs": "https://arxiv.org/abs/2509.03644", "authors": ["François Olivier", "Zied Bouraoui"], "title": "Towards a Neurosymbolic Reasoning System Grounded in Schematic Representations", "comment": "To appear in Proceedings of Machine Learning Research, 19th\n  Conference on Neurosymbolic Learning and Reasoning, 2025", "summary": "Despite significant progress in natural language understanding, Large\nLanguage Models (LLMs) remain error-prone when performing logical reasoning,\noften lacking the robust mental representations that enable human-like\ncomprehension. We introduce a prototype neurosymbolic system, Embodied-LM, that\ngrounds understanding and logical reasoning in schematic representations based\non image schemas-recurring patterns derived from sensorimotor experience that\nstructure human cognition. Our system operationalizes the spatial foundations\nof these cognitive structures using declarative spatial reasoning within Answer\nSet Programming. Through evaluation on logical deduction problems, we\ndemonstrate that LLMs can be guided to interpret scenarios through embodied\ncognitive structures, that these structures can be formalized as executable\nprograms, and that the resulting representations support effective logical\nreasoning with enhanced interpretability. While our current implementation\nfocuses on spatial primitives, it establishes the computational foundation for\nincorporating more complex and dynamic representations.", "AI": {"tldr": "本文介绍了一种神经符号系统Embodied-LM，它通过基于图像图式的示意性表征来增强大型语言模型（LLMs）的理解和逻辑推理能力，从而提高了可解释性和鲁棒性。", "motivation": "尽管自然语言理解取得了显著进展，但大型语言模型（LLMs）在执行逻辑推理时仍然容易出错，常常缺乏像人类那样强大的心理表征能力。", "method": "该系统Embodied-LM将理解和逻辑推理基于图像图式（源自感觉运动经验并构建人类认知的重复模式）的示意性表征。它利用Answer Set Programming中的声明性空间推理来操作这些认知结构的空间基础。", "result": "研究表明，LLMs可以被引导通过具身认知结构来解释场景，这些结构可以被形式化为可执行程序，并且由此产生的表征支持有效的逻辑推理，并增强了可解释性。", "conclusion": "尽管当前实现侧重于空间原语，但Embodied-LM为整合更复杂和动态的表征奠定了计算基础，提升了LLMs的逻辑推理能力和可解释性。"}}
{"id": "2509.03899", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.03899", "abs": "https://arxiv.org/abs/2509.03899", "authors": ["Sampath Kumar Mulagaleti", "Andrea Del Prete"], "title": "Sample Efficient Certification of Discrete-Time Control Barrier Functions", "comment": "8 pages, accepted for publication in proceedings of IEEE CDC 2025", "summary": "Control Invariant (CI) sets are instrumental in certifying the safety of\ndynamical systems. Control Barrier Functions (CBFs) are effective tools to\ncompute such sets, since the zero sublevel sets of CBFs are CI sets. However,\ncomputing CBFs generally involves addressing a complex robust optimization\nproblem, which can be intractable. Scenario-based methods have been proposed to\nsimplify this computation. Then, one needs to verify if the CBF actually\nsatisfies the robust constraints. We present an approach to perform this\nverification that relies on Lipschitz arguments, and forms the basis of a\ncertification algorithm designed for sample efficiency. Through a numerical\nexample, we validated the efficiency of the proposed procedure.", "AI": {"tldr": "本文提出了一种基于Lipschitz论证的控制障碍函数（CBF）验证方法，旨在提高样本效率，以认证动态系统的安全性。", "motivation": "控制不变（CI）集对系统安全认证至关重要，而CBF是计算CI集的有效工具。然而，计算CBF通常涉及复杂且难以处理的鲁棒优化问题。情景方法虽能简化计算，但仍需验证CBF是否真正满足鲁棒约束。", "method": "本文提出了一种利用Lipschitz论证进行CBF验证的方法，并以此为基础设计了一个样本高效的认证算法。", "result": "通过数值示例验证了所提出验证程序的效率。", "conclusion": "该方法提供了一种高效验证CBF的手段，可作为样本高效认证算法的基础，有助于动态系统的安全认证。"}}
{"id": "2509.03737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03737", "abs": "https://arxiv.org/abs/2509.03737", "authors": ["Casper van Engelenburg", "Jan van Gemert", "Seyran Khademi"], "title": "LayoutGKN: Graph Similarity Learning of Floor Plans", "comment": "BMVC (2025)", "summary": "Floor plans depict building layouts and are often represented as graphs to\ncapture the underlying spatial relationships. Comparison of these graphs is\ncritical for applications like search, clustering, and data visualization. The\nmost successful methods to compare graphs \\ie, graph matching networks, rely on\ncostly intermediate cross-graph node-level interactions, therefore being slow\nin inference time. We introduce \\textbf{LayoutGKN}, a more efficient approach\nthat postpones the cross-graph node-level interactions to the end of the joint\nembedding architecture. We do so by using a differentiable graph kernel as a\ndistance function on the final learned node-level embeddings. We show that\nLayoutGKN computes similarity comparably or better than graph matching networks\nwhile significantly increasing the speed.\n\\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are\nopen.", "AI": {"tldr": "LayoutGKN是一种更高效的楼层平面图图比较方法，通过延迟跨图节点交互和使用可微分图核，显著提高了速度，同时保持或提升了相似性计算性能。", "motivation": "楼层平面图的图比较对于搜索、聚类和数据可视化等应用至关重要。然而，现有的图匹配网络方法依赖于耗时的跨图节点级交互，导致推理速度慢。", "method": "本文提出了LayoutGKN，一种更高效的方法，它将跨图节点级交互推迟到联合嵌入架构的末端。具体而言，它在最终学习到的节点级嵌入上使用可微分图核作为距离函数。", "result": "LayoutGKN在计算相似性方面与图匹配网络相当或更好，同时显著提高了速度。", "conclusion": "LayoutGKN提供了一种更高效且有效的楼层平面图图比较方法，解决了现有图匹配网络速度慢的问题。"}}
{"id": "2509.03889", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03889", "abs": "https://arxiv.org/abs/2509.03889", "authors": ["Neha Sunil", "Megha Tippur", "Arnau Saumell", "Edward Adelson", "Alberto Rodriguez"], "title": "Reactive In-Air Clothing Manipulation with Confidence-Aware Dense Correspondence and Visuotactile Affordance", "comment": "Accepted at CoRL 2025. Project website:\n  https://mhtippur.github.io/inairclothmanipulation/", "summary": "Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.", "AI": {"tldr": "本文提出一个双臂视觉触觉框架，结合置信度感知的密集视觉对应和触觉监督的抓取能力，直接操作褶皱和悬挂衣物，并能处理高度遮挡情况，实现鲁棒的服装操作。", "motivation": "服装操作因复杂构型、多变材料动力学和频繁自遮挡而充满挑战。现有系统通常要求展平衣物或假设关键特征可见，限制了其在实际场景中的应用。", "method": "该框架包含两个核心组件：1) 置信度感知的密集视觉对应模型，通过在自定义高保真模拟数据集上训练，利用分布损失捕获布料对称性并生成置信度估计，这些估计指导一个响应式状态机调整折叠策略。2) 触觉监督的抓取能力网络，通过高分辨率触觉反馈进行自监督，确定物理上可抓取的区域，并在执行期间使用同一触觉分类器进行实时抓取验证。系统通过在低置信度状态下推迟行动，以处理高度遮挡的桌面和空中配置。", "result": "该系统能够直接操作褶皱和悬挂的衣物，并有效处理高度遮挡的桌面和空中配置。研究者在折叠和悬挂任务中展示了其任务无关的抓取选择模块。此外，提出的密集描述符提供了一种可重用的中间表示。", "conclusion": "该框架通过结合视觉对应和触觉监督的抓取能力，克服了服装操作中的多项挑战。其密集描述符作为可重用的中间表示，有望应用于其他规划模式（如从人类视频演示中提取抓取目标），为更通用和可扩展的服装操作铺平了道路。"}}
{"id": "2509.03533", "categories": ["cs.CL", "cs.LG", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2509.03533", "abs": "https://arxiv.org/abs/2509.03533", "authors": ["Igor Halperin"], "title": "Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck", "comment": "26 pages, 4 figures", "summary": "Large Language Models (LLMs) are prone to critical failure modes, including\n\\textit{intrinsic faithfulness hallucinations} (also known as confabulations),\nwhere a response deviates semantically from the provided context. Frameworks\ndesigned to detect this, such as Semantic Divergence Metrics (SDM), rely on\nidentifying latent topics shared between prompts and responses, typically by\napplying geometric clustering to their sentence embeddings. This creates a\ndisconnect, as the topics are optimized for spatial proximity, not for the\ndownstream information-theoretic analysis. In this paper, we bridge this gap by\ndeveloping a principled topic identification method grounded in the\nDeterministic Information Bottleneck (DIB) for geometric clustering. Our key\ncontribution is to transform the DIB method into a practical algorithm for\nhigh-dimensional data by substituting its intractable KL divergence term with a\ncomputationally efficient upper bound. The resulting method, which we dub UDIB,\ncan be interpreted as an entropy-regularized and robustified version of K-means\nthat inherently favors a parsimonious number of informative clusters. By\napplying UDIB to the joint clustering of LLM prompt and response embeddings, we\ngenerate a shared topic representation that is not merely spatially coherent\nbut is fundamentally structured to be maximally informative about the\nprompt-response relationship. This provides a superior foundation for the SDM\nframework and offers a novel, more sensitive tool for detecting confabulations.", "AI": {"tldr": "本文提出了一种名为UDIB的新型主题识别方法，它基于确定性信息瓶颈（DIB）并解决了现有语义差异度量（SDM）在检测大型语言模型（LLM）幻觉时主题识别的局限性，从而提供了一个更敏感的幻觉检测工具。", "motivation": "检测LLM的内在忠实性幻觉（即胡言乱语）的现有框架（如SDM）依赖于通过对句子嵌入进行几何聚类来识别提示和响应之间的潜在主题。然而，这些主题是为空间接近度优化的，而非为下游的信息理论分析优化，这导致了方法上的脱节。", "method": "本文通过将确定性信息瓶颈（DIB）方法转换为高维数据的实用算法，弥合了这一差距。具体来说，他们用一个计算效率高的上限替换了DIB中难以处理的KL散度项，从而得到了UDIB方法。UDIB可以被解释为K-means的一种熵正则化和鲁棒化版本，它本质上偏爱数量适中但信息量丰富的聚类。", "result": "通过将UDIB应用于LLM提示和响应嵌入的联合聚类，该方法生成了一个共享的主题表示，该表示不仅在空间上连贯，而且在根本上被构建为关于提示-响应关系的最大信息量。这为SDM框架提供了一个卓越的基础。", "conclusion": "UDIB提供了一种新颖、更敏感的工具，用于检测LLM中的胡言乱语（幻觉），因为它能够生成最大化信息量的提示-响应关系主题表示。"}}
{"id": "2509.03646", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03646", "abs": "https://arxiv.org/abs/2509.03646", "authors": ["Haozhe Wang", "Qixin Xu", "Che Liu", "Junhong Wu", "Fangzhen Lin", "Wenhu Chen"], "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning", "comment": "Preprint", "summary": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.", "AI": {"tldr": "本文揭示了强化学习（RL）提升大型语言模型（LLM）推理能力的两阶段动态：先是低级技能，后是高级战略规划。针对现有RL算法效率低下的问题，提出HICRA算法，将优化重点放在高影响的规划令牌上，显著优于基线模型，并指出战略瓶颈是解锁高级推理的关键。", "motivation": "强化学习在增强大型语言模型复杂推理能力方面表现出色，但其成功背后的机制尚不明确，如“顿悟时刻”、“长度缩放”和熵动态等现象未被理解。现有RL算法（如GRPO）普适性地施加优化压力，稀释了学习信号，导致效率低下。", "method": "本文通过分析“顿悟时刻”、“长度缩放”和熵动态，揭示了LLM中出现的推理层级和两阶段学习动态（先低级技能，后高级战略规划）。在此基础上，提出层级感知信用分配（HIerarchy-Aware Credit Assignment, HICRA）算法，该算法将优化精力集中在高影响的规划令牌上。此外，研究还验证了语义熵作为衡量战略探索指标的优越性。", "result": "研究揭示了一个两阶段动态：模型首先受限于程序正确性，然后性能提升主要由高级战略规划的探索和掌握驱动。HICRA算法显著优于强大的基线模型，表明关注战略瓶颈是解锁高级推理的关键。同时，语义熵被证实是衡量战略探索的优越指标，而非令牌级熵。", "conclusion": "RL在提升LLM推理能力方面的成功源于一个出现的两阶段学习层级。通过识别并集中优化高级战略规划这一瓶颈，如HICRA算法所做，可以更有效地解锁高级推理能力。语义熵是衡量这一战略探索过程的更有效指标。"}}
{"id": "2509.04060", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04060", "abs": "https://arxiv.org/abs/2509.04060", "authors": ["Alejandro Penacho Riveiros", "Nicola Bastianello", "Karl H. Johansson", "Matthieu Barreau"], "title": "Physics-Informed Detection of Friction Anomalies in Satellite Reaction Wheels", "comment": null, "summary": "As the number of satellites in orbit has increased exponentially in recent\nyears, ensuring their correct functionality has started to require automated\nmethods to decrease human workload. In this work, we present an algorithm that\nanalyzes the on-board data related to friction from the Reaction Wheel\nAssemblies (RWA) of a satellite and determines their operating status,\ndistinguishing between nominal status and several possible anomalies that\nrequire preventive measures to be taken. The algorithm first uses a model based\non hybrid systems theory to extract the information relevant to the problem.\nThe extraction process combines techniques in changepoint detection, dynamic\nprogramming, and maximum likelihood in a structured way. A classifier then uses\nthe extracted information to determine the status of the RWA. This last\nclassifier has been previously trained with a labelled dataset produced by a\nhigh-fidelity simulator, comprised for the most part of nominal data. The final\nalgorithm combines model-based and data-based approaches to obtain satisfactory\nresults with an accuracy around 95%.", "AI": {"tldr": "该研究提出了一种自动化算法，用于分析卫星反作用轮组件（RWA）的摩擦数据，并确定其运行状态，区分正常和多种异常情况，以减少人工工作量并实现预防性维护。", "motivation": "近年来在轨卫星数量呈指数级增长，需要自动化方法来确保其功能正常并减少人工工作量。", "method": "该算法首先利用基于混合系统理论的模型提取相关信息。提取过程结合了变点检测、动态规划和最大似然技术。然后，一个分类器使用提取的信息来确定RWA的状态，该分类器已使用高保真模拟器生成（主要由正常数据组成）的标记数据集进行训练。最终算法结合了基于模型和基于数据的方法。", "result": "该算法能够区分RWA的正常运行状态和几种需要采取预防措施的可能异常情况。最终算法取得了令人满意的结果，准确率约为95%。", "conclusion": "该研究成功开发了一种结合模型驱动和数据驱动方法的自动化算法，能够高效准确地监测卫星RWA的运行状态，为预防性维护提供了有效手段。"}}
{"id": "2509.03740", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03740", "abs": "https://arxiv.org/abs/2509.03740", "authors": ["Taha Koleilat", "Hassan Rivaz", "Yiming Xiao"], "title": "Singular Value Few-shot Adaptation of Vision-Language Models", "comment": "10 pages, 2 figures, 8 tables", "summary": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and\n\\textit{parameter-efficient} adaptation technique that leverages Singular Value\nDecomposition (SVD) to modify the internal parameter space of CLIP without\ninjecting additional modules. Specifically, we fine-tune only the singular\nvalues of the CLIP parameter matrices to rescale the basis vectors for domain\nadaptation while retaining the pretrained model. This design enables enhanced\nadaptation performance using only \\textbf{0.04\\%} of the model's total\nparameters and better preservation of its generalization ability. CLIP-SVD\nachieves state-of-the-art classification results on 11 natural and 10\nbiomedical datasets, outperforming previous methods in both accuracy and\ngeneralization under few-shot settings. Additionally, we leverage a natural\nlanguage-based approach to analyze the effectiveness and dynamics of the CLIP\nadaptation to allow interpretability of CLIP-SVD. The code is publicly\navailable at https://github.com/HealthX-Lab/CLIP-SVD.", "AI": {"tldr": "CLIP-SVD是一种新颖的多模态、参数高效的CLIP微调技术，通过调整奇异值实现领域自适应，无需额外模块，在少样本设置下表现出色并保持泛化能力。", "motivation": "CLIP等视觉-语言模型（VLM）在适应新的细粒度领域时面临挑战，现有方法依赖提示工程、全模型微调成本高昂，或通过增加组件限制适应质量、破坏预训练知识。", "method": "本文提出了CLIP-SVD，一种多模态、参数高效的自适应技术，它利用奇异值分解（SVD）来修改CLIP的内部参数空间，而无需注入额外模块。具体而言，仅微调CLIP参数矩阵的奇异值以重新缩放基向量，从而实现领域自适应并保留预训练模型。", "result": "CLIP-SVD仅使用模型总参数的0.04%，就实现了增强的自适应性能，并更好地保留了其泛化能力。在11个自然数据集和10个生物医学数据集上取得了最先进的分类结果，在少样本设置下，其准确性和泛化能力均优于现有方法。此外，还利用基于自然语言的方法分析了CLIP自适应的有效性和动态性，以提高可解释性。", "conclusion": "CLIP-SVD通过创新的SVD方法，为CLIP等VLM在细粒度领域的自适应提供了一种高效、高性能的解决方案，显著减少了微调参数量，同时提高了模型准确性和泛化能力。"}}
{"id": "2509.04016", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04016", "abs": "https://arxiv.org/abs/2509.04016", "authors": ["Branimir Ćaran", "Vladimir Milić", "Marko Švaco", "Bojan Jerbić"], "title": "Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall Climbing Robot", "comment": "ACCEPTED FOR IEEE EUROPEAN CONFERENCE ON MOBILE ROBOTS 2025. PREPRINT\n  VERSION. ACCEPTED JUNE, 2025 AND PRESENTED SEPTEMBER, 2025", "summary": "This paper presents the design of a pose estimator for a four wheel\nindependent steer four wheel independent drive (4WIS4WID) wall climbing mobile\nrobot, based on the fusion of multimodal measurements, including wheel\nodometry, visual odometry, and an inertial measurement unit (IMU) data using\nExtended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose\nestimator is a critical component of wall climbing mobile robots, as their\noperational environment involves carrying precise measurement equipment and\nmaintenance tools in construction, requiring information about pose on the\nbuilding at the time of measurement. Due to the complex geometry and material\nproperties of building facades, the use of traditional localization sensors\nsuch as laser, ultrasonic, or radar is often infeasible for wall-climbing\nrobots. Moreover, GPS-based localization is generally unreliable in these\nenvironments because of signal degradation caused by reinforced concrete and\nelectromagnetic interference. Consequently, robot odometry remains the primary\nsource of velocity and position information, despite being susceptible to drift\ncaused by both systematic and non-systematic errors. The calibrations of the\nrobot's systematic parameters were conducted using nonlinear optimization and\nLevenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting\nmethods, while Genetic algorithm and Particle swarm were used as\nstochastic-based methods for kinematic parameter calibration. Performance and\nresults of the calibration methods and pose estimators were validated in detail\nwith experiments on the experimental mobile wall climbing robot.", "AI": {"tldr": "本文提出了一种基于多模态传感器融合（轮式里程计、视觉里程计、IMU）的四轮独立转向四轮独立驱动 (4WIS4WID) 爬墙移动机器人位姿估计器设计，并使用扩展卡尔曼滤波 (EKF) 和无迹卡尔曼滤波 (UKF) 进行融合，同时对系统参数和运动学参数进行了校准和实验验证。", "motivation": "爬墙机器人需要在建筑立面上携带精密测量设备和维护工具，因此精确的位姿信息至关重要。传统定位传感器（激光、超声波、雷达）因建筑立面复杂几何和材料特性而不可行；GPS 因钢筋混凝土和电磁干扰导致信号衰减而不可靠；机器人里程计虽是主要速度和位置来源，但易受系统性和非系统性误差影响而产生漂移。", "method": "位姿估计器设计：融合轮式里程计、视觉里程计和惯性测量单元 (IMU) 数据。融合算法：扩展卡尔曼滤波 (EKF) 和无迹卡尔曼滤波 (UKF)。系统参数校准：采用非线性优化、Levenberg-Marquardt 方法（牛顿-高斯和基于梯度的模型拟合）。运动学参数校准：采用遗传算法和粒子群算法（基于随机的方法）。性能验证：在实验性爬墙移动机器人上进行了详细的实验验证。", "result": "校准方法和位姿估计器的性能和结果均通过在实验性爬墙移动机器人上的实验得到了详细验证。", "conclusion": "本文成功设计并验证了一种针对4WIS4WID爬墙移动机器人的多模态融合位姿估计器，并开发了相应的系统和运动学参数校准方法，有效解决了复杂作业环境下的定位挑战。"}}
{"id": "2509.03535", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03535", "abs": "https://arxiv.org/abs/2509.03535", "authors": ["Ahmed Mubarak", "Amna Ahmed", "Amira Nasser", "Aya Mohamed", "Fares El-Sadek", "Mohammed Ahmed", "Ahmed Salah", "Youssef Sobhy"], "title": "QuesGenie: Intelligent Multimodal Question Generation", "comment": "7 pages, 8 figures, 12 tables. Supervised by Dr. Ahmed Salah and TA\n  Youssef Sobhy", "summary": "In today's information-rich era, learners have access to abundant educational\nresources, but the lack of practice materials tailored to these resources\npresents a significant challenge. This project addresses that gap by developing\na multi-modal question generation system that can automatically generate\ndiverse question types from various content formats. The system features four\nmajor components: multi-modal input handling, question generation,\nreinforcement learning from human feedback (RLHF), and an end-to-end\ninteractive interface. This project lays the foundation for automated,\nscalable, and intelligent question generation, carefully balancing resource\nefficiency, robust functionality and a smooth user experience.", "AI": {"tldr": "该项目开发了一个多模态问题生成系统，能够从各种内容格式中自动生成多样化的问题，以弥补学习资源与配套练习材料之间的差距。", "motivation": "在信息丰富的时代，学习者拥有大量教育资源，但缺乏与这些资源相匹配的实践材料是一个重大挑战。", "method": "该系统包含四个主要组件：多模态输入处理、问题生成、基于人类反馈的强化学习（RLHF）以及一个端到端交互式界面。", "result": "该项目为自动化、可扩展和智能的问题生成奠定了基础，并兼顾了资源效率、鲁棒功能和流畅的用户体验。", "conclusion": "该系统能够实现自动化、可扩展和智能的问题生成，有效解决教育资源与练习材料不匹配的问题，并提供良好的用户体验。"}}
{"id": "2509.03649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03649", "abs": "https://arxiv.org/abs/2509.03649", "authors": ["Davide Italo Serramazza", "Nikos Papadeas", "Zahraa Abdallah", "Georgiana Ifrim"], "title": "An Empirical Evaluation of Factors Affecting SHAP Explanation of Time Series Classification", "comment": null, "summary": "Explainable AI (XAI) has become an increasingly important topic for\nunderstanding and attributing the predictions made by complex Time Series\nClassification (TSC) models. Among attribution methods, SHapley Additive\nexPlanations (SHAP) is widely regarded as an excellent attribution method; but\nits computational complexity, which scales exponentially with the number of\nfeatures, limits its practicality for long time series. To address this, recent\nstudies have shown that aggregating features via segmentation, to compute a\nsingle attribution value for a group of consecutive time points, drastically\nreduces SHAP running time. However, the choice of the optimal segmentation\nstrategy remains an open question. In this work, we investigated eight\ndifferent Time Series Segmentation algorithms to understand how segment\ncompositions affect the explanation quality. We evaluate these approaches using\ntwo established XAI evaluation methodologies: InterpretTime and AUC Difference.\nThrough experiments on both Multivariate (MTS) and Univariate Time Series\n(UTS), we find that the number of segments has a greater impact on explanation\nquality than the specific segmentation method. Notably, equal-length\nsegmentation consistently outperforms most of the custom time series\nsegmentation algorithms. Furthermore, we introduce a novel attribution\nnormalisation technique that weights segments by their length and we show that\nit consistently improves attribution quality.", "AI": {"tldr": "为解决SHAP在长时序数据解释中的计算效率问题，本文研究了八种时间序列分割算法对解释质量的影响。结果表明，分割数量比具体分割方法更重要，等长分割表现良好，且提出的长度加权归一化技术能持续提升解释质量。", "motivation": "SHAP是一种优秀的归因方法，但其计算复杂性使其不适用于长时序数据。虽然通过分割聚合特征可以显著减少SHAP运行时间，但如何选择最优的分割策略仍是一个未解决的问题。", "method": "本文研究了八种不同的时间序列分割算法，以理解段组成如何影响解释质量。评估方法采用InterpretTime和AUC Difference两种已建立的XAI评估方法，并在多元和单变量时间序列上进行实验。此外，本文还引入了一种新颖的归因归一化技术，根据段的长度进行加权。", "result": "实验发现，分割的数量对解释质量的影响大于具体的分割方法。值得注意的是，等长分割的表现持续优于大多数自定义时间序列分割算法。此外，本文引入的长度加权归因归一化技术持续改善了归因质量。", "conclusion": "在时间序列分类模型的解释性AI中，分割数量是影响解释质量的关键因素，简单的等长分割策略表现出色。同时，提出的长度加权归因归一化技术能够有效提升解释质量。"}}
{"id": "2509.04096", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04096", "abs": "https://arxiv.org/abs/2509.04096", "authors": ["Lyssa Ramaut", "Chesney Buyle", "Jona Cappelle", "Liesbet Van der Perre"], "title": "Low-Power Impact Detection and Localization on Forklifts Using Wireless IMU Sensors", "comment": "This paper is accepted in IEEE Sensors 2025", "summary": "Forklifts are essential for transporting goods in industrial environments.\nThese machines face wear and tear during field operations, along with rough\nterrain, tight spaces and complex handling scenarios. This increases the\nlikelihood of unintended impacts, such as collisions with goods,\ninfrastructure, or other machinery. In addition, deliberate misuse has been\nstated, compromising safety and equipment integrity. This paper presents a\nlow-cost and low-power impact detection system based on multiple wireless\nsensor nodes measuring 3D accelerations. These were deployed in a measurement\ncampaign covering realworld operational scenarios. An algorithm was developed,\nbased on this collected data, to differentiate high-impact events from normal\nusage and to localize detected collisions on the forklift. The solution\nsuccessfully detects and localizes impacts, while maintaining low power\nconsumption, enabling reliable forklift monitoring with multi-year sensor\nautonomy.", "AI": {"tldr": "本文提出了一种基于多无线传感器节点的低成本、低功耗冲击检测系统，用于叉车碰撞的检测与定位，并能区分高强度冲击与正常使用，实现多年传感器自主运行。", "motivation": "叉车在工业环境中面临磨损、崎岖地形、狭窄空间和复杂操作，导致意外碰撞（与货物、基础设施或其他机械）的风险增加。此外，故意滥用也会损害安全和设备完整性。", "method": "该研究开发了一个基于多个无线传感器节点的低成本、低功耗冲击检测系统，测量3D加速度。这些节点部署在真实操作场景中进行测量活动。基于收集到的数据，开发了一种算法来区分高强度冲击事件和正常使用，并定位叉车上检测到的碰撞。", "result": "该解决方案成功地检测并定位了冲击，同时保持了低功耗，实现了可靠的叉车监控和多年的传感器自主运行。", "conclusion": "所提出的系统能有效检测和定位叉车冲击，同时具有低功耗和长续航能力，为叉车的安全和设备完整性提供了可靠的监控解决方案。"}}
{"id": "2509.03754", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03754", "abs": "https://arxiv.org/abs/2509.03754", "authors": ["Zongsen Qiu"], "title": "STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification", "comment": null, "summary": "Responding to rising global food security needs, precision agriculture and\ndeep learning-based plant disease diagnosis have become crucial. Yet, deploying\nhigh-precision models on edge devices is challenging. Most lightweight networks\nuse attention mechanisms designed for generic object recognition, which poorly\ncapture subtle pathological features like irregular lesion shapes and complex\ntextures. To overcome this, we propose a twofold solution: first, using a\ntraining-free neural architecture search method (DeepMAD) to create an\nefficient network backbone for edge devices; second, introducing the\nShape-Texture Attention Module (STAM). STAM splits attention into two branches\n-- one using deformable convolutions (DCNv4) for shape awareness and the other\nusing a Gabor filter bank for texture awareness. On the public CCMT plant\ndisease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)\nreached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm\nSTAM significantly improves performance over baseline and standard attention\nmodels. Integrating domain knowledge via decoupled attention thus presents a\npromising path for edge-deployed precision agriculture AI. The source code is\navailable at https://github.com/RzMY/STA-Net.", "AI": {"tldr": "本文提出了一种名为STA-Net的轻量级模型，通过结合无训练神经架构搜索和形状-纹理注意力模块（STAM），显著提升了边缘设备上植物病害诊断的精度，特别是在捕捉病变细微特征方面表现出色。", "motivation": "全球粮食安全需求日益增长，精准农业和基于深度学习的植物病害诊断至关重要。然而，在边缘设备上部署高精度模型面临挑战。大多数轻量级网络使用的通用注意力机制难以有效捕捉不规则病变形状和复杂纹理等细微病理特征。", "method": "本文提出了双重解决方案：1) 使用无训练神经架构搜索方法（DeepMAD）构建高效的边缘设备网络骨干；2) 引入形状-纹理注意力模块（STAM）。STAM将注意力分为两个分支：一个利用可变形卷积（DCNv4）感知形状，另一个利用Gabor滤波器组感知纹理。", "result": "在公共CCMT植物病害数据集上，STA-Net模型（401K参数，51.1M FLOPs）达到了89.00%的准确率和88.96%的F1分数。消融研究证实STAM相比基线和标准注意力模型显著提升了性能。", "conclusion": "通过解耦注意力（如STAM）集成领域知识，为边缘部署的精准农业AI提供了一条有前景的路径。"}}
{"id": "2509.04018", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04018", "abs": "https://arxiv.org/abs/2509.04018", "authors": ["Yifan Yang", "Zhixiang Duan", "Tianshi Xie", "Fuyu Cao", "Pinxi Shen", "Peili Song", "Piaopiao Jin", "Guokang Sun", "Shaoqing Xu", "Yangwei You", "Jingtai Liu"], "title": "FPC-VLA: A Vision-Language-Action Framework with a Supervisor for Failure Prediction and Correction", "comment": null, "summary": "Robotic manipulation is a fundamental component of automation. However,\ntraditional perception-planning pipelines often fall short in open-ended tasks\ndue to limited flexibility, while the architecture of a single end-to-end\nVision-Language-Action (VLA) offers promising capabilities but lacks crucial\nmechanisms for anticipating and recovering from failure. To address these\nchallenges, we propose FPC-VLA, a dual-model framework that integrates VLA with\na supervisor for failure prediction and correction. The supervisor evaluates\naction viability through vision-language queries and generates corrective\nstrategies when risks arise, trained efficiently without manual labeling. A\nsimilarity-guided fusion module further refines actions by leveraging past\npredictions. Evaluation results on multiple simulation platforms (SIMPLER and\nLIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA\noutperforms state-of-the-art models in both zero-shot and fine-tuned settings.\nBy activating the supervisor only at keyframes, our approach significantly\nincreases task success rates with minimal impact on execution time. Successful\nreal-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong\ngeneralization and practical utility for building more reliable autonomous\nsystems.", "AI": {"tldr": "FPC-VLA 是一种双模型框架，将视觉-语言-动作 (VLA) 模型与一个用于故障预测和纠正的监督器集成，在机器人操作任务中显著提高了成功率，且对执行时间影响极小。", "motivation": "传统的感知-规划流程在开放式任务中缺乏灵活性，而单一的端到端 VLA 架构虽然有前景，但缺乏预测和从故障中恢复的关键机制。", "method": "FPC-VLA 框架包含一个 VLA 模型和一个监督器。监督器通过视觉-语言查询评估动作可行性，并在风险出现时生成纠正策略，且无需手动标记即可高效训练。一个相似性引导的融合模块通过利用过去的预测来进一步优化动作。监督器仅在关键帧激活。", "result": "FPC-VLA 在多个模拟平台（SIMPLER 和 LIBERO）和机器人（WidowX、Google Robot、Franka）上的零样本和微调设置中均优于最先进的模型。它显著提高了任务成功率，同时对执行时间影响最小。在各种长期真实世界任务中的成功部署也证实了其强大的泛化能力和实用性。", "conclusion": "FPC-VLA 展现出强大的泛化能力和实用性，为构建更可靠的自主系统提供了有效途径。"}}
{"id": "2509.03537", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03537", "abs": "https://arxiv.org/abs/2509.03537", "authors": ["Cheng-Kai Yeh", "Hsing-Wang Lee", "Chung-Hung Kuo", "Hen-Hsen Huang"], "title": "AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models", "comment": "7 pages, accepted by CIKM 2025 as a short paper", "summary": "Abstraction--the ability to recognize and distill essential computational\npatterns from complex problem statements--is a foundational skill in computer\nscience, critical both for human problem-solvers and coding-oriented large\nlanguage models (LLMs). Despite recent advances in training LLMs for code\ngeneration using reinforcement learning (RL), most existing approaches focus\nprimarily on superficial pattern recognition, overlooking explicit training for\nabstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement\nLearning for Abstract Reasoning), a novel framework explicitly designed to\nenhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to\ntransform kernel problems into narrative-rich, challenging descriptions without\nchanging their fundamental logic. Simultaneously, a student coding model is\ntrained to solve these complex narrative problems by extracting their\nunderlying computational kernels. Experimental results demonstrate that AR$^2$\nsubstantially improves the student model's accuracy on previously unseen,\nchallenging programming tasks, underscoring abstraction as a key skill for\nenhancing LLM generalization.", "AI": {"tldr": "本研究提出AR$^2$（Adversarial Reinforcement Learning for Abstract Reasoning）框架，通过对抗性强化学习显式训练大型语言模型（LLMs）的抽象推理能力，以解决其在代码生成中对抽象能力缺乏训练的问题，从而提高其在复杂编程任务上的泛化能力。", "motivation": "抽象能力是计算机科学的基础技能，对人类和LLMs都至关重要。尽管LLMs在代码生成方面取得了进展，但现有方法主要关注表面模式识别，忽视了对抽象能力的显式训练。", "method": "AR$^2$框架包含一个教师模型和一个学生编码模型。教师模型将核心问题转换为叙述丰富但逻辑不变的复杂描述。学生编码模型则通过提取这些复杂叙述问题背后的计算核心来解决它们，从而在对抗性强化学习中提升抽象能力。", "result": "实验结果表明，AR$^2$显著提高了学生模型在以前未见的、具有挑战性的编程任务上的准确性。", "conclusion": "抽象能力是增强LLM泛化能力的关键技能。"}}
{"id": "2509.03728", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.03728", "abs": "https://arxiv.org/abs/2509.03728", "authors": ["Wesley Hanwen Deng", "Sunnie S. Y. Kim", "Akshita Jha", "Ken Holstein", "Motahhare Eslami", "Lauren Wilcox", "Leon A Gatys"], "title": "PersonaTeaming: Exploring How Introducing Personas Can Improve Automated AI Red-Teaming", "comment": null, "summary": "Recent developments in AI governance and safety research have called for\nred-teaming methods that can effectively surface potential risks posed by AI\nmodels. Many of these calls have emphasized how the identities and backgrounds\nof red-teamers can shape their red-teaming strategies, and thus the kinds of\nrisks they are likely to uncover. While automated red-teaming approaches\npromise to complement human red-teaming by enabling larger-scale exploration of\nmodel behavior, current approaches do not consider the role of identity. As an\ninitial step towards incorporating people's background and identities in\nautomated red-teaming, we develop and evaluate a novel method, PersonaTeaming,\nthat introduces personas in the adversarial prompt generation process to\nexplore a wider spectrum of adversarial strategies. In particular, we first\nintroduce a methodology for mutating prompts based on either \"red-teaming\nexpert\" personas or \"regular AI user\" personas. We then develop a dynamic\npersona-generating algorithm that automatically generates various persona types\nadaptive to different seed prompts. In addition, we develop a set of new\nmetrics to explicitly measure the \"mutation distance\" to complement existing\ndiversity measurements of adversarial prompts. Our experiments show promising\nimprovements (up to 144.1%) in the attack success rates of adversarial prompts\nthrough persona mutation, while maintaining prompt diversity, compared to\nRainbowPlus, a state-of-the-art automated red-teaming method. We discuss the\nstrengths and limitations of different persona types and mutation methods,\nshedding light on future opportunities to explore complementarities between\nautomated and human red-teaming approaches.", "AI": {"tldr": "本文提出了一种名为PersonaTeaming的自动化红队方法，通过在对抗性提示生成过程中引入不同身份（persona），显著提高了AI模型攻击成功率，并保持了提示多样性。", "motivation": "AI治理和安全研究需要有效的红队方法来发现潜在风险。人类红队成员的身份和背景会影响其策略及发现的风险类型。然而，现有自动化红队方法未考虑身份因素，限制了对抗策略的探索。", "method": "本文开发了PersonaTeaming方法：1. 引入“红队专家”或“普通AI用户”等身份来变异提示。2. 开发动态身份生成算法，根据初始提示自动生成不同身份类型。3. 提出新的“变异距离”指标，以补充现有对抗性提示的多样性测量。", "result": "实验结果显示，通过身份变异，对抗性提示的攻击成功率相比现有最先进方法RainbowPlus有显著提升（最高达144.1%），同时保持了提示的多样性。", "conclusion": "PersonaTeaming是自动化红队方法中融入人员背景和身份的初步尝试。研究讨论了不同身份类型和变异方法的优缺点，为未来探索自动化与人类红队方法的互补性提供了方向。"}}
{"id": "2509.04116", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04116", "abs": "https://arxiv.org/abs/2509.04116", "authors": ["Ioannis Tzortzis", "Themistoklis Charalambous", "Charalambos D. Charalambous"], "title": "Remote Estimation for Markov Jump Linear Systems: A Distributionally Robust Approach", "comment": null, "summary": "This paper considers the problem of remote state estimation for Markov jump\nlinear systems in the presence of uncertainty in the posterior mode\nprobabilities. Such uncertainty may arise when the estimator receives noisy or\nincomplete measurements over an unreliable communication network. To address\nthis challenge, the estimation problem is formulated within a distributionally\nrobust framework, where the true posterior is assumed to lie within a total\nvariation distance ball centered at the nominal posterior. The resulting\nminimax formulation yields an estimator that extends the classical MMSE\nsolution with additional terms that account for mode uncertainty. A tractable\nimplementation is developed using a distributionally robust variant of the\nfirst-order generalized pseudo-Bayesian algorithm. A numerical example is\nprovided to illustrate the applicability and effectiveness of the approach.", "AI": {"tldr": "本文提出了一种在后验模式概率存在不确定性时，针对马尔可夫跳跃线性系统进行远程状态估计的分布鲁棒方法。", "motivation": "当估计器通过不可靠的通信网络接收到有噪声或不完整的测量数据时，后验模式概率可能会出现不确定性，这促使研究人员寻求一种能够在这种不确定性下进行鲁健壮估计的方法。", "method": "研究将估计问题建模在一个分布鲁棒框架内，假设真实的后验分布位于以名义后验为中心的总变差距离球内。由此产生的极小极大公式扩展了经典的MMSE解决方案，并增加了考虑模式不确定性的项。通过一阶广义伪贝叶斯算法的分布鲁棒变体，开发了一种可行的实现。", "result": "该方法产生了一个扩展了经典MMSE解决方案的估计器，其中包含用于处理模式不确定性的附加项。通过数值示例证明了该方法的适用性和有效性。", "conclusion": "所提出的分布鲁棒方法能有效处理马尔可夫跳跃线性系统在后验模式概率存在不确定性时的远程状态估计问题，并提供了一个可行的实现方案。"}}
{"id": "2509.03786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03786", "abs": "https://arxiv.org/abs/2509.03786", "authors": ["Xinxin Wang", "Han Sun", "Ningzhong Liu", "Huiyu Zhou", "Yinan Yao"], "title": "SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection", "comment": "14pages, accepted by PRCV2025", "summary": "Underwater Camouflaged Object Detection (UCOD) aims to identify objects that\nblend seamlessly into underwater environments. This task is critically\nimportant to marine ecology. However, it remains largely underexplored and\naccurate identification is severely hindered by optical distortions, water\nturbidity, and the complex traits of marine organisms. To address these\nchallenges, we introduce the UCOD task and present DeepCamo, a benchmark\ndataset designed for this domain. We also propose Semantic Localization and\nEnhancement Network (SLENet), a novel framework for UCOD. We first benchmark\nstate-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet\nis built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)\nmodule and a Localization Guidance Branch (LGB) to enhance multi-scale feature\nrepresentation while generating a location map enriched with global semantic\ninformation. This map guides the Multi-Scale Supervised Decoder (MSSD) to\nproduce more accurate predictions. Experiments on our DeepCamo dataset and\nthree benchmark COD datasets confirm SLENet's superior performance over SOTA\nmethods, and underscore its high generality for the broader COD task.", "AI": {"tldr": "本文提出了水下伪装目标检测（UCOD）任务，并发布了首个UCOD基准数据集DeepCamo。为解决水下环境挑战，提出了一种新颖的UCOD框架SLENet，通过特征增强和定位引导显著提升了检测精度和泛化能力。", "motivation": "水下伪装目标检测（UCOD）对海洋生态至关重要，但由于光学畸变、水体浑浊和海洋生物复杂特征，该任务仍未被充分探索且准确识别面临巨大挑战。", "method": "本文首先引入了UCOD任务并构建了基准数据集DeepCamo。在此基础上，提出了语义定位与增强网络（SLENet），包含Gamma-非对称增强（GAE）模块用于增强多尺度特征表示，以及定位引导分支（LGB）生成富含全局语义信息的定位图，该图指导多尺度监督解码器（MSSD）生成更准确的预测。", "result": "在DeepCamo数据集和三个现有伪装目标检测（COD）基准数据集上的实验结果表明，SLENet的性能优于现有最先进的方法，并证明了其在更广泛的COD任务中具有很高的泛化能力。", "conclusion": "本文成功定义了UCOD任务，提供了专用数据集DeepCamo，并开发了高效的SLENet框架。SLENet通过其创新的特征增强和定位引导机制，显著提升了水下伪装目标检测的准确性，并展现出强大的跨领域泛化能力。"}}
{"id": "2509.04061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04061", "abs": "https://arxiv.org/abs/2509.04061", "authors": ["Ventseslav Yordanov", "Simon Schäfer", "Alexander Mann", "Stefan Kowalewski", "Bassam Alrifaee", "Lutz Eckstein"], "title": "Integrated Wheel Sensor Communication using ESP32 -- A Contribution towards a Digital Twin of the Road System", "comment": "6 pages, 2 figures, this work was submitted to and accepted by IEEE\n  International Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "While current onboard state estimation methods are adequate for most driving\nand safety-related applications, they do not provide insights into the\ninteraction between tires and road surfaces. This paper explores a novel\ncommunication concept for efficiently transmitting integrated wheel sensor data\nfrom an ESP32 microcontroller. Our proposed approach utilizes a\npublish-subscribe system, surpassing comparable solutions in the literature\nregarding data transmission volume. We tested this approach on a drum tire test\nrig with our prototype sensors system utilizing a diverse selection of sample\nfrequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our\ncommunication concept. The implemented prototype sensor showcases minimal data\nloss, approximately 0.1 % of the sampled data, validating the reliability of\nour developed communication system. This work contributes to advancing\nreal-time data acquisition, providing insights into optimizing integrated wheel\nsensor communication.", "AI": {"tldr": "本文提出了一种基于ESP32微控制器和发布-订阅系统的新型通信概念，用于高效传输集成轮毂传感器数据。在滚筒轮胎试验台上进行测试，结果表明该系统在数据传输量上优于现有方案，并实现了极低的数据丢失率。", "motivation": "当前的车载状态估计方法无法深入了解轮胎与路面之间的相互作用，因此需要一种新的方法来高效获取轮毂传感器数据。", "method": "研究了一种利用ESP32微控制器和发布-订阅系统的高效集成轮毂传感器数据传输通信概念。该方法在滚筒轮胎试验台上使用原型传感器系统进行了测试，采样频率范围在1 Hz到32000 Hz之间。", "result": "所提出的方法在数据传输量方面超越了现有文献中的可比解决方案。实现的通信系统数据丢失率极低，约为采样数据的0.1%。", "conclusion": "该研究验证了所开发通信系统的可靠性和有效性，并为推进实时数据采集以及优化集成轮毂传感器通信提供了有价值的见解。"}}
{"id": "2509.03540", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03540", "abs": "https://arxiv.org/abs/2509.03540", "authors": ["Shanglin Wu", "Lihui Liu", "Jinho D. Choi", "Kai Shu"], "title": "Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction", "comment": null, "summary": "Large Language Models (LLMs) often struggle with producing factually\nconsistent answers due to limitations in their parametric memory.\nRetrieval-Augmented Generation (RAG) methods address this issue by\nincorporating external knowledge from trusted sources at inference time.\nHowever, such methods typically treat knowledge as unstructured text, which\nlimits their ability to support compositional reasoning and identify factual\ninconsistencies. To overcome these limitations, we propose a novel framework\nthat dynamically constructs and expands knowledge graphs (KGs) during\ninference, integrating both internal knowledge extracted from LLMs and external\ninformation retrieved from external sources. Our method begins by extracting a\nseed KG from the question via prompting, followed by iterative expansion using\nthe LLM's latent knowledge. The graph is then selectively refined through\nexternal retrieval, enhancing factual coverage and correcting inaccuracies. We\nevaluate our approach on three diverse factual QA benchmarks, demonstrating\nconsistent improvements in factual accuracy, answer precision, and\ninterpretability over baseline prompting and static KG-augmented methods. Our\nfindings suggest that inference-time KG construction is a promising direction\nfor enhancing LLM factuality in a structured, interpretable, and scalable\nmanner.", "AI": {"tldr": "本文提出了一种新颖的框架，通过在推理时动态构建和扩展知识图谱（KG），整合LLM的内部知识和外部检索信息，以提高大型语言模型的事实准确性、答案精确性和可解释性，克服了现有RAG方法将知识视为非结构化文本的局限性。", "motivation": "大型语言模型（LLMs）由于参数记忆的限制，常难以生成事实一致的答案。现有的检索增强生成（RAG）方法虽然引入外部知识，但通常将知识视为非结构化文本，这限制了它们支持组合推理和识别事实不一致的能力。", "method": "该方法在推理时动态构建和扩展知识图谱（KG），整合LLM内部知识和外部检索信息。具体步骤包括：首先通过提示从问题中提取一个种子KG，然后利用LLM的潜在知识进行迭代扩展，最后通过外部检索选择性地完善图谱，以增强事实覆盖并纠正不准确之处。", "result": "在三个不同的事实问答基准测试中，该方法在事实准确性、答案精确性和可解释性方面，均比基线提示和静态KG增强方法有显著改进。", "conclusion": "推理时构建知识图谱是增强LLM事实性一个有前景的方向，它能够以结构化、可解释和可扩展的方式实现这一目标。"}}
{"id": "2509.03730", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.03730", "abs": "https://arxiv.org/abs/2509.03730", "authors": ["Pengrui Han", "Rafal Kocielnik", "Peiyang Song", "Ramit Debnath", "Dean Mobbs", "Anima Anandkumar", "R. Michael Alvarez"], "title": "The Personality Illusion: Revealing Dissociation Between Self-Reports & Behavior in LLMs", "comment": "We make public all code and source data at\n  https://github.com/psychology-of-AI/Personality-Illusion", "summary": "Personality traits have long been studied as predictors of human\nbehavior.Recent advances in Large Language Models (LLMs) suggest similar\npatterns may emerge in artificial systems, with advanced LLMs displaying\nconsistent behavioral tendencies resembling human traits like agreeableness and\nself-regulation. Understanding these patterns is crucial, yet prior work\nprimarily relied on simplified self-reports and heuristic prompting, with\nlittle behavioral validation. In this study, we systematically characterize LLM\npersonality across three dimensions: (1) the dynamic emergence and evolution of\ntrait profiles throughout training stages; (2) the predictive validity of\nself-reported traits in behavioral tasks; and (3) the impact of targeted\ninterventions, such as persona injection, on both self-reports and behavior.\nOur findings reveal that instructional alignment (e.g., RLHF, instruction\ntuning) significantly stabilizes trait expression and strengthens trait\ncorrelations in ways that mirror human data. However, these self-reported\ntraits do not reliably predict behavior, and observed associations often\ndiverge from human patterns. While persona injection successfully steers\nself-reports in the intended direction, it exerts little or inconsistent effect\non actual behavior. By distinguishing surface-level trait expression from\nbehavioral consistency, our findings challenge assumptions about LLM\npersonality and underscore the need for deeper evaluation in alignment and\ninterpretability.", "AI": {"tldr": "大语言模型(LLM)的指令对齐稳定了其性格特质的表达，但这些自报告的特质并不能可靠地预测行为，且与人类模式存在差异；人格注入也难以一致地影响实际行为，表明LLM的表面特质表达与行为一致性之间存在脱节。", "motivation": "LLM展现出类似人类的性格特质，理解这些模式至关重要。然而，现有研究主要依赖简化自报告和启发式提示，缺乏行为验证，因此需要更系统地刻画LLM的性格。", "method": "本研究从三个维度系统性地刻画了LLM的性格：1) 训练阶段特质概貌的动态出现和演变；2) 自报告特质在行为任务中的预测有效性；3) 目标干预（如人格注入）对自报告和行为的影响。", "result": "指令对齐（如RLHF、指令微调）显著稳定了特质表达并强化了特质相关性，与人类数据相似。然而，这些自报告的特质不能可靠地预测行为，且观察到的关联常与人类模式不同。人格注入虽然能成功引导自报告，但对实际行为的影响甚微或不一致。", "conclusion": "本研究区分了LLM的表面特质表达和行为一致性，挑战了关于LLM性格的假设，并强调了在对齐和可解释性方面进行更深入评估的必要性。"}}
{"id": "2509.04196", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04196", "abs": "https://arxiv.org/abs/2509.04196", "authors": ["Aditi Saxena", "Twinkle Tripathy", "Rajasekhar Anguluri"], "title": "Laplacian Flows in Complex-valued Directed Networks: Analysis, Design, and Consensus", "comment": null, "summary": "In the interdisciplinary field of network science, a complex-valued network,\nwith edges assigned complex weights, provides a more nuanced representation of\nrelationships by capturing both the magnitude and phase of interactions.\nAdditionally, an important application of this setting arises in distribution\npower grids. Motivated by the richer framework, we study the necessary and\nsufficient conditions for achieving consensus in both strongly and weakly\nconnected digraphs. The paper establishes that complex-valued Laplacian flows\nconverge to consensus subject to an additional constraint termed as real\ndominance which relies on the phase angles of the edge weights. Our approach\nbuilds on the complex Perron-Frobenius properties to study the spectral\nproperties of the Laplacian and its relation to graphical conditions. Finally,\nwe propose modified flows that guarantee consensus even if the original network\ndoes not converge to consensus. Additionally, we explore diffusion in\ncomplex-valued networks as a dual process of consensus and simulate our results\non synthetic and real-world networks.", "AI": {"tldr": "本研究探讨了复值网络中达成共识的必要和充分条件，引入了“实数主导”约束，并提出了改进的流方法以确保共识。", "motivation": "复值网络能够通过捕获交互的幅度和相位来提供更细致的关系表示，并在配电网等领域有重要应用。这种更丰富的框架促使作者研究其共识机制。", "method": "研究了强连接和弱连接有向图中的共识条件；利用复Perron-Frobenius性质研究拉普拉斯算子的谱特性及其与图条件的关系；提出了即使原始网络不收敛也能保证共识的改进流；探讨了复值网络中的扩散作为共识的对偶过程；在合成和真实网络上进行了模拟。", "result": "复值拉普拉斯流在满足“实数主导”（依赖于边权重的相角）的额外约束下收敛到共识。提出的改进流即使在原始网络不收敛的情况下也能保证共识。扩散作为共识的对偶过程得到了探索和模拟验证。", "conclusion": "本研究确立了复值网络中达成共识的必要和充分条件，特别是引入了“实数主导”这一关键约束。同时，提出了有效的方法来确保共识的实现，并探索了扩散作为共识的对偶过程。"}}
{"id": "2509.03794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03794", "abs": "https://arxiv.org/abs/2509.03794", "authors": ["Juhun Lee", "Simon S. Woo"], "title": "Fitting Image Diffusion Models on Video Datasets", "comment": "ICCV25 Workshop", "summary": "Image diffusion models are trained on independently sampled static images.\nWhile this is the bedrock task protocol in generative modeling, capturing the\ntemporal world through the lens of static snapshots is information-deficient by\ndesign. This limitation leads to slower convergence, limited distributional\ncoverage, and reduced generalization. In this work, we propose a simple and\neffective training strategy that leverages the temporal inductive bias present\nin continuous video frames to improve diffusion training. Notably, the proposed\nmethod requires no architectural modification and can be seamlessly integrated\ninto standard diffusion training pipelines. We evaluate our method on the\nHandCo dataset, where hand-object interactions exhibit dense temporal coherence\nand subtle variations in finger articulation often result in semantically\ndistinct motions. Empirically, our method accelerates convergence by over\n2$\\text{x}$ faster and achieves lower FID on both training and validation\ndistributions. It also improves generative diversity by encouraging the model\nto capture meaningful temporal variations. We further provide an optimization\nanalysis showing that our regularization reduces the gradient variance, which\ncontributes to faster convergence.", "AI": {"tldr": "该研究提出一种利用连续视频帧中时间归纳偏差的简单有效训练策略，以改进图像扩散模型的训练，实现更快的收敛和更高的生成质量，无需修改模型架构。", "motivation": "图像扩散模型通常基于独立采样的静态图像进行训练，但这种方式在捕捉时间世界时存在信息不足的问题，导致收敛缓慢、分布覆盖有限和泛化能力下降。", "method": "提出一种训练策略，利用连续视频帧中存在的时间归纳偏差来改进扩散模型训练。该方法无需修改模型架构，可无缝集成到标准扩散训练流程中。通过优化分析，该正则化策略能减少梯度方差。", "result": "在HandCo数据集上，该方法将收敛速度提高了2倍以上，并在训练和验证分布上实现了更低的FID。它还通过鼓励模型捕捉有意义的时间变化来提高生成多样性。优化分析表明，该正则化减少了梯度方差，有助于加速收敛。", "conclusion": "利用连续视频帧中的时间归纳偏差，可以显著加速图像扩散模型的训练收敛速度，提高生成质量和多样性，且无需修改现有模型架构，是一种简单有效的改进策略。"}}
{"id": "2509.04063", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04063", "abs": "https://arxiv.org/abs/2509.04063", "authors": ["Hongyin Zhang", "Shiyuan Zhang", "Junxi Jin", "Qixin Zeng", "Yifan Qiao", "Hongchao Lu", "Donglin Wang"], "title": "Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA Flow Models", "comment": null, "summary": "Vision-Language-Action (VLA) models based on flow matching have shown\nexcellent performance in general-purpose robotic manipulation tasks. However,\nthe action accuracy of these models on complex downstream tasks is\nunsatisfactory. One important reason is that these models rely solely on the\npost-training paradigm of imitation learning, which makes it difficult to have\na deeper understanding of the distribution properties of data quality, which is\nexactly what Reinforcement Learning (RL) excels at. In this paper, we\ntheoretically propose an offline RL post-training objective for VLA flow models\nand induce an efficient and feasible offline RL fine-tuning algorithm --\nAdaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted\nscaling factor in the VLA flow model loss, we construct a principled\nbias-variance trade-off objective function to optimally control the impact of\nRL signal on flow loss. ARFM adaptively balances RL advantage preservation and\nflow loss gradient variance control, resulting in a more stable and efficient\nfine-tuning process. Extensive simulation and real-world experimental results\nshow that ARFM exhibits excellent generalization, robustness, few-shot\nlearning, and continuous learning performance.", "AI": {"tldr": "针对基于流匹配的VLA模型在复杂任务上动作精度不足的问题，本文提出了ARFM算法，通过引入自适应调整的离线强化学习后训练目标，有效提升了模型的泛化、鲁棒性、少样本和持续学习性能。", "motivation": "基于流匹配的视觉-语言-动作（VLA）模型在通用机器人操作任务中表现出色，但在复杂下游任务上的动作精度不尽如人意。主要原因是这些模型仅依赖模仿学习的后训练范式，难以深入理解数据质量的分布特性，而这正是强化学习（RL）的优势所在。", "method": "本文理论上提出了VLA流模型的离线RL后训练目标，并由此推导出一个高效可行的离线RL微调算法——自适应强化流匹配（ARFM）。通过在VLA流模型损失中引入一个自适应调整的缩放因子，构建了一个有原则的偏差-方差权衡目标函数，以优化控制RL信号对流损失的影响。ARFM自适应地平衡了RL优势保留和流损失梯度方差控制。", "result": "大量的模拟和真实世界实验结果表明，ARFM展现出卓越的泛化能力、鲁棒性、少样本学习和持续学习性能。", "conclusion": "ARFM通过引入自适应调整的离线强化学习信号，为VLA流模型提供了一种更稳定和高效的微调过程，显著提升了模型在复杂任务上的动作精度和整体性能。"}}
{"id": "2509.03565", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.03565", "abs": "https://arxiv.org/abs/2509.03565", "authors": ["Qi Chen", "Jingxuan Wei", "Zhuoya Yao", "Haiguang Wang", "Gaowei Wu", "Bihui Yu", "Siyuan Li", "Cheng Tan"], "title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference", "comment": "Accepted to ACM MM 2025", "summary": "Understanding how scientific ideas evolve requires more than summarizing\nindividual papers-it demands structured, cross-document reasoning over\nthematically related research. In this work, we formalize multi-document\nscientific inference, a new task that extracts and aligns motivation,\nmethodology, and experimental results across related papers to reconstruct\nresearch development chains. This task introduces key challenges, including\ntemporally aligning loosely structured methods and standardizing heterogeneous\nexperimental tables. We present ResearchPulse, an agent-based framework that\nintegrates instruction planning, scientific content extraction, and structured\nvisualization. It consists of three coordinated agents: a Plan Agent for task\ndecomposition, a Mmap-Agent that constructs motivation-method mind maps, and a\nLchart-Agent that synthesizes experimental line charts. To support this task,\nwe introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper\nclusters. Experiments show that our system, despite using 7B-scale agents,\nconsistently outperforms strong baselines like GPT-4o in semantic alignment,\nstructural consistency, and visual fidelity. The dataset are available in\nhttps://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.", "AI": {"tldr": "本文提出了一种名为“多文档科学推理”的新任务，旨在通过提取和对齐相关论文中的动机、方法和结果来重建研究发展链。为此，作者开发了ResearchPulse，一个基于代理的框架，并创建了ResearchPulse-Bench基准。实验表明，ResearchPulse在语义对齐、结构一致性和视觉保真度方面优于GPT-4o等强大基线。", "motivation": "理解科学思想的演变需要对主题相关的研究进行结构化、跨文档的推理，而不仅仅是总结单篇论文。现有方法难以实现跨论文的动机、方法和实验结果的提取与对齐，特别是松散结构化方法的时序对齐和异构实验表格的标准化。", "method": "本文将“多文档科学推理”形式化为一个新任务。提出ResearchPulse，一个基于代理的框架，整合了指令规划、科学内容提取和结构化可视化。它包含三个协调代理：Plan Agent（任务分解）、Mmap-Agent（构建动机-方法思维导图）和Lchart-Agent（合成实验折线图）。为支持此任务，还引入了ResearchPulse-Bench，一个包含标注论文集群的引用感知基准。", "result": "实验结果表明，ResearchPulse系统（尽管使用7B规模的代理）在语义对齐、结构一致性和视觉保真度方面，持续优于GPT-4o等强大的基线模型。", "conclusion": "ResearchPulse框架及其基于代理的方法有效地解决了多文档科学推理任务中的关键挑战，能够重建科学研究的发展链条，并且在性能上超越了大型语言模型，证明了其在理解科学思想演变方面的优越性。"}}
{"id": "2509.03736", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03736", "abs": "https://arxiv.org/abs/2509.03736", "authors": ["James Mooney", "Josef Woldense", "Zheng Robert Jia", "Shirley Anugrah Hayati", "My Ha Nguyen", "Vipul Raheja", "Dongyeop Kang"], "title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation", "comment": "25 pages, 9 figures, 7 tables", "summary": "The impressive capabilities of Large Language Models (LLMs) have fueled the\nnotion that synthetic agents can serve as substitutes for real participants in\nhuman-subject research. In an effort to evaluate the merits of this claim,\nsocial science researchers have largely focused on whether LLM-generated survey\ndata corresponds to that of a human counterpart whom the LLM is prompted to\nrepresent. In contrast, we address a more fundamental question: Do agents\nmaintain internal consistency, retaining similar behaviors when examined under\ndifferent experimental settings? To this end, we develop a study designed to\n(a) reveal the agent's internal state and (b) examine agent behavior in a basic\ndialogue setting. This design enables us to explore a set of behavioral\nhypotheses to assess whether an agent's conversation behavior is consistent\nwith what we would expect from their revealed internal state. Our findings on\nthese hypotheses show significant internal inconsistencies in LLMs across model\nfamilies and at differing model sizes. Most importantly, we find that, although\nagents may generate responses matching those of their human counterparts, they\nfail to be internally consistent, representing a critical gap in their\ncapabilities to accurately substitute for real participants in human-subject\nresearch. Our simulation code and data are publicly accessible.", "AI": {"tldr": "研究发现大型语言模型（LLMs）作为人类受试者替代品时存在显著的内部不一致性，尽管它们可能生成与人类相似的回答。", "motivation": "LLMs的强大能力引发了合成代理可以替代真实人类参与者进行人类受试者研究的观点。现有研究主要关注LLM生成的调查数据是否与人类对应，但本研究旨在解决一个更根本的问题：代理在不同实验设置下能否保持内部一致性，即行为是否相似。", "method": "开发了一项研究，旨在(a)揭示代理的内部状态，以及(b)在基本对话设置中检查代理行为。该设计使得研究能够探索一系列行为假设，以评估代理的对话行为是否与其揭示的内部状态一致。", "result": "研究发现，在不同模型家族和不同模型大小的LLMs中存在显著的内部不一致性。最重要的是，尽管代理可能生成与人类对应的响应，但它们未能保持内部一致性。", "conclusion": "LLMs缺乏内部一致性是其准确替代人类受试者进行人类研究的一个关键缺陷，这限制了它们在此类研究中的应用能力。"}}
{"id": "2509.04199", "categories": ["eess.SY", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04199", "abs": "https://arxiv.org/abs/2509.04199", "authors": ["Dieter Schwarzmann", "Simon Käser"], "title": "On the Effect of Sampling-Time Jitter", "comment": "Submitted for review as letter in IEEE Journal for Transactions on\n  Control Systems Technology", "summary": "This brief, aimed at practitioners, offers an analysis of the effect of\nsampling-time jitter, i. e., the error produced by execution-time inaccuracies.\nWe propose reinterpreting jitter-afflicted linear time-invariant systems\nthrough equivalent jitter-free analogs. By constructing a perceived system that\nabsorbs the effects of timing perturbations into its dynamics, we find an\naffine scaling of jitter. We examine both measurement and implementation\nscenarios, demonstrating that the presence of jitter effectively scales the\nsystem matrices. Moreover, we observe that, in the Laplace domain, jitter can\nbe interpreted as a frequency scaling.", "AI": {"tldr": "本文分析了采样时间抖动对线性时不变系统的影响，提出通过无抖动模拟系统重新解释抖动系统，发现抖动会导致系统矩阵的仿射缩放和频率缩放。", "motivation": "研究旨在为实践者分析采样时间抖动（由执行时间不准确产生的误差）对线性时不变（LTI）系统的影响。", "method": "提出通过等效的无抖动模拟系统重新解释受抖动影响的线性时不变系统。通过构建一个将时序扰动吸收到其动态中的“感知系统”来实现。研究涵盖了测量和实现两种场景。", "result": "主要结果包括发现抖动的仿射缩放。证明了抖动会有效地缩放系统矩阵。此外，在拉普拉斯域中，抖动可以被解释为频率缩放。", "conclusion": "论文得出结论，采样时间抖动对线性时不变系统的影响可以通过将其重新解释为无抖动模拟系统进行系统分析，从而揭示抖动的仿射缩放、系统矩阵的缩放以及拉普拉斯域中的频率缩放。"}}
{"id": "2509.03800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03800", "abs": "https://arxiv.org/abs/2509.03800", "authors": ["Yuheng Li", "Yenho Chen", "Yuxiang Lai", "Jike Zhong", "Vanessa Wildman", "Xiaofeng Yang"], "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting", "comment": null, "summary": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.", "AI": {"tldr": "MedVista3D是一个多尺度、语义增强的3D CT视觉-语言预训练框架，通过局部和全局图像-文本对齐以及语义感知的报告处理，解决了放射学诊断中的错误和报告不一致问题，并在多项任务中实现了最先进的性能。", "motivation": "放射学诊断错误（如漏诊、注意力盲区、沟通失败）普遍存在，尤其在3D成像中因局部异常遗漏、全局上下文受限和报告语言可变性而加剧。现有3D视觉-语言模型无法同时满足精确局部检测、全局体积推理和语义一致的自然语言报告这三项需求，因为它们缺乏局部-全局理解，并且难以处理未经整理的放射学报告的变异性和噪声。", "method": "本文提出了MedVista3D，一个用于3D CT分析的多尺度语义增强视觉-语言预训练框架。为实现疾病检测和整体解释，MedVista3D在全体积背景下进行局部和全局图像-文本对齐，以学习细粒度表示。为解决报告变异性，该方法应用了语言模型重写，并引入了一个放射学语义匹配库（Radiology Semantic Matching Bank）以实现语义感知的对齐。", "result": "MedVista3D在零样本疾病分类、报告检索和医学视觉问答方面取得了最先进的性能，并能很好地迁移到器官分割和预后预测任务中。", "conclusion": "MedVista3D通过其多尺度语义增强的视觉-语言预训练框架，有效解决了3D CT分析中局部-全局理解和报告一致性的关键挑战，显著提高了诊断准确性和解释能力，并展现了在多种医学任务上的强大泛化能力。"}}
{"id": "2509.04069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04069", "abs": "https://arxiv.org/abs/2509.04069", "authors": ["Chengyandan Shen", "Christoffer Sloth"], "title": "Solving Robotics Tasks with Prior Demonstration via Exploration-Efficient Deep Reinforcement Learning", "comment": null, "summary": "This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework.", "AI": {"tldr": "本文提出了DRLR框架，一种探索高效的深度强化学习方法，结合演示用于机器人任务。它通过改进IBRL的动作选择模块（提供校准Q值）和使用SAC，有效缓解了引导误差并防止收敛到次优策略。该方法在模拟和真实世界机器人任务中均得到验证，展现出鲁棒性。", "motivation": "在机器人学习任务中，现有的深度强化学习方法常面临探索效率低下（由引导误差引起）和策略可能收敛到次优解的问题。因此，需要一个能够有效利用演示数据并克服这些挑战的框架。", "method": "DRLR（Deep Reinforcement Learning with Reference policy）框架基于IBRL算法开发。主要改进包括：1. 修改了动作选择模块，提供校准的Q值以缓解引导误差。2. 使用SAC（Soft Actor-Critic）作为RL策略而非TD3，以防止策略收敛到次优解。该框架整合了演示数据。通过在铲斗装载和抽屉开启等机器人任务中进行模拟和真实世界部署（sim2real），对方法进行了经验验证。", "result": "DRLR框架成功缓解了引导误差并有效防止了过拟合（即收敛到次优策略）。在需要大量环境交互的机器人任务中（如铲斗装载和抽屉开启），其有效性得到了经验验证。仿真结果表明，DRLR框架在低维和高维状态-动作空间以及不同演示质量的任务中均展现出鲁棒性。此外，铲斗装载任务在真实轮式装载机上的sim2real部署也取得了成功。", "conclusion": "DRLR框架是一个探索高效且鲁棒的深度强化学习方法，能够有效利用演示数据来学习机器人任务。它通过改进动作选择和策略选择，成功缓解了引导误差并防止了次优策略，并在模拟和真实世界的工业机器人任务中得到了成功验证。"}}
{"id": "2509.03610", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03610", "abs": "https://arxiv.org/abs/2509.03610", "authors": ["Josh Wisoff", "Yao Tang", "Zhengyu Fang", "Jordan Guzman", "YuTang Wang", "Alex Yu"], "title": "NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management", "comment": null, "summary": "Note-taking is a critical practice for capturing, organizing, and reflecting\non information in both academic and professional settings. The recent success\nof large language models has accelerated the development of AI-assisted tools,\nyet existing solutions often struggle with efficiency. We present NoteBar, an\nAI-assisted note-taking tool that leverages persona information and efficient\nlanguage models to automatically organize notes into multiple categories and\nbetter support user workflows. To support research and evaluation in this\nspace, we further introduce a novel persona-conditioned dataset of 3,173 notes\nand 8,494 annotated concepts across 16 MBTI personas, offering both diversity\nand semantic richness for downstream tasks. Finally, we demonstrate that\nNoteBar can be deployed in a practical and cost-effective manner, enabling\ninteractive use without reliance on heavy infrastructure. Together, NoteBar and\nits accompanying dataset provide a scalable and extensible foundation for\nadvancing AI-assisted personal knowledge management.", "AI": {"tldr": "本文介绍了NoteBar，一个利用人格信息和高效语言模型自动分类笔记的AI辅助笔记工具，并发布了一个包含3,173条笔记和8,494个概念的MBTI人格条件数据集，旨在提升AI辅助个人知识管理的效率和可扩展性。", "motivation": "笔记记录在学术和专业环境中至关重要，但现有AI辅助工具在效率方面存在不足，无法有效支持用户工作流程。", "method": "开发了NoteBar，一个AI辅助笔记工具，它利用人格信息和高效语言模型自动将笔记组织到多个类别中。同时，引入了一个包含3,173条笔记和8,494个概念的、基于16种MBTI人格的新型数据集。此外，NoteBar的设计注重实用性和成本效益，无需重型基础设施即可部署。", "result": "NoteBar能够高效地自动组织笔记，更好地支持用户工作流程。所创建的人格条件数据集为下游任务提供了多样性和语义丰富性。NoteBar的部署方式实用且经济高效，能够实现交互式使用。", "conclusion": "NoteBar及其配套数据集为推进AI辅助个人知识管理提供了一个可扩展和可扩展的基础。"}}
{"id": "2509.03768", "categories": ["cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.03768", "abs": "https://arxiv.org/abs/2509.03768", "authors": ["Connor Walker", "Koorosh Aslansefat", "Mohammad Naveed Akram", "Yiannis Papadopoulos"], "title": "RAGuard: A Novel Approach for in-context Safe Retrieval Augmented Generation for LLMs", "comment": null, "summary": "Accuracy and safety are paramount in Offshore Wind (OSW) maintenance, yet\nconventional Large Language Models (LLMs) often fail when confronted with\nhighly specialised or unexpected scenarios. We introduce RAGuard, an enhanced\nRetrieval-Augmented Generation (RAG) framework that explicitly integrates\nsafety-critical documents alongside technical manuals.By issuing parallel\nqueries to two indices and allocating separate retrieval budgets for knowledge\nand safety, RAGuard guarantees both technical depth and safety coverage. We\nfurther develop a SafetyClamp extension that fetches a larger candidate pool,\n\"hard-clamping\" exact slot guarantees to safety. We evaluate across sparse\n(BM25), dense (Dense Passage Retrieval) and hybrid retrieval paradigms,\nmeasuring Technical Recall@K and Safety Recall@K. Both proposed extensions of\nRAG show an increase in Safety Recall@K from almost 0\\% in RAG to more than\n50\\% in RAGuard, while maintaining Technical Recall above 60\\%. These results\ndemonstrate that RAGuard and SafetyClamp have the potential to establish a new\nstandard for integrating safety assurance into LLM-powered decision support in\ncritical maintenance contexts.", "AI": {"tldr": "RAGuard是一个增强型检索增强生成（RAG）框架，通过并行查询技术和安全文档集成，显著提升了离岸风电（OSW）维护中大型语言模型（LLM）的安全性召回率，同时保持了技术准确性。", "motivation": "传统的LLM在面对高度专业化或意外场景时，尤其是在离岸风电维护等安全关键领域，往往表现不佳，无法提供足够的准确性和安全性。", "method": "本文提出了RAGuard框架，它显式整合了安全关键文档和技术手册。通过向两个索引（知识和安全）并行查询，并为知识和安全分配独立的检索预算，确保了技术深度和安全覆盖。此外，还开发了SafetyClamp扩展，获取更大的候选池，并对安全进行“硬性限制”以保证精确槽位。评估涵盖了稀疏（BM25）、密集（DPR）和混合检索范式，测量了技术召回率@K和安全召回率@K。", "result": "RAGuard及其SafetyClamp扩展将安全召回率@K从RAG的几乎0%提高到50%以上，同时将技术召回率保持在60%以上。", "conclusion": "RAGuard和SafetyClamp有潜力为LLM驱动的决策支持在关键维护场景中整合安全保障树立新标准。"}}
{"id": "2509.04213", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04213", "abs": "https://arxiv.org/abs/2509.04213", "authors": ["Tobin Holtmann", "David Stenger", "Andres Posada-Moreno", "Friedrich Solowjow", "Sebastian Trimpe"], "title": "Sailing Towards Zero-Shot State Estimation using Foundation Models Combined with a UKF", "comment": "Accepted for publication at CDC2025", "summary": "State estimation in control and systems engineering traditionally requires\nextensive manual system identification or data-collection effort. However,\ntransformer-based foundation models in other domains have reduced data\nrequirements by leveraging pre-trained generalist models. Ultimately,\ndeveloping zero-shot foundation models of system dynamics could drastically\nreduce manual deployment effort. While recent work shows that transformer-based\nend-to-end approaches can achieve zero-shot performance on unseen systems, they\nare limited to sensor models seen during training. We introduce the foundation\nmodel unscented Kalman filter (FM-UKF), which combines a transformer-based\nmodel of system dynamics with analytically known sensor models via an UKF,\nenabling generalization across varying dynamics without retraining for new\nsensor configurations. We evaluate FM-UKF on a new benchmark of container ship\nmodels with complex dynamics, demonstrating a competitive accuracy, effort, and\nrobustness trade-off compared to classical methods with approximate system\nknowledge and to an end-to-end approach. The benchmark and dataset are open\nsourced to further support future research in zero-shot state estimation via\nfoundation models.", "AI": {"tldr": "本文提出了一种名为FM-UKF的零样本状态估计方法，它结合了基于Transformer的系统动力学模型和卡尔曼滤波（UKF）的分析传感器模型，实现了在不重新训练的情况下对不同传感器配置的泛化能力，并在集装箱船模型上表现出竞争力。", "motivation": "传统的控制和系统工程中的状态估计需要大量的系统识别或数据收集工作。虽然基于Transformer的预训练基础模型在其他领域减少了数据需求，但现有的基于Transformer的端到端方法在零样本性能上仍受限于训练期间见过的传感器模型。因此，需要开发一种能够泛化到新传感器配置的零样本基础模型。", "method": "本文引入了基础模型无迹卡尔曼滤波（FM-UKF）。该方法通过UKF将基于Transformer的系统动力学模型与分析已知的传感器模型相结合，从而在不为新的传感器配置重新训练的情况下，实现对不同动力学的泛化。", "result": "FM-UKF能够在不重新训练新传感器配置的情况下，实现对不同动力学的泛化。在新的复杂动力学集装箱船模型基准测试中，FM-UKF与具有近似系统知识的经典方法和端到端方法相比，在精度、工作量和鲁棒性之间取得了有竞争力的权衡。此外，该基准测试和数据集已开源以支持未来的研究。", "conclusion": "FM-UKF通过结合Transformer模型和UKF，有效解决了零样本状态估计中传感器模型泛化的问题，提供了一种在复杂系统（如集装箱船）中实现高效、鲁棒状态估计的新范式，并为该领域的研究提供了宝贵的开源资源。"}}
{"id": "2509.03803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03803", "abs": "https://arxiv.org/abs/2509.03803", "authors": ["Mengyu Gao", "Qiulei Dong"], "title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation", "comment": "ICCV 2025 Accepted", "summary": "Prompt learning has recently attracted much attention for adapting\npre-trained vision-language models (e.g., CLIP) to downstream recognition\ntasks. However, most of the existing CLIP-based prompt learning methods only\nshow a limited ability for handling fine-grained datasets. To address this\nissue, we propose a causality-guided text prompt learning method via visual\ngranulation for CLIP, called CaPL, where the explored visual granulation\ntechnique could construct sets of visual granules for the text prompt to\ncapture subtle discrepancies among different fine-grained classes through\ncasual inference. The CaPL method contains the following two modules: (1) An\nattribute disentanglement module is proposed to decompose visual features into\nnon-individualized attributes (shared by some classes) and individualized\nattributes (specific to single classes) using a Brownian Bridge Diffusion\nModel; (2) A granule learning module is proposed to construct visual granules\nby integrating the aforementioned attributes for recognition under two causal\ninference strategies. Thanks to the learned visual granules, more\ndiscriminative text prompt is expected to be learned. Extensive experimental\nresults on 15 datasets demonstrate that our CaPL method significantly\noutperforms the state-of-the-art prompt learning methods, especially on\nfine-grained datasets.", "AI": {"tldr": "本文提出CaPL，一种通过视觉粒化和因果推理的文本提示学习方法，以提高CLIP在细粒度数据集上的识别能力。", "motivation": "现有的基于CLIP的提示学习方法在处理细粒度数据集时表现有限，难以捕捉类别间的细微差异。", "method": "CaPL方法包含两个模块：1) 属性解耦模块，使用布朗桥扩散模型将视觉特征分解为非个体化和个体化属性；2) 粒化学习模块，通过整合上述属性，并在两种因果推理策略下构建视觉粒化，以学习更具判别力的文本提示。", "result": "在15个数据集上的大量实验结果表明，CaPL方法显著优于最先进的提示学习方法，尤其是在细粒度数据集上。", "conclusion": "CaPL通过探索视觉粒化技术和因果推理，有效地构建了视觉粒化，从而学习到更具判别力的文本提示，显著提升了CLIP在细粒度识别任务中的性能。"}}
{"id": "2509.04076", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04076", "abs": "https://arxiv.org/abs/2509.04076", "authors": ["Lennart Clasmeier", "Jan-Gerrit Habekost", "Connor Gäde", "Philipp Allgeuer", "Stefan Wermter"], "title": "Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot", "comment": "Submitted to ICANN 20255 Special Session on Neural Robotics", "summary": "We propose a novel diffusion-based action model for robotic motion planning.\nCommonly, established numerical planning approaches are used to solve general\nmotion planning problems, but have significant runtime requirements. By\nleveraging the power of deep learning, we are able to achieve good results in a\nmuch smaller runtime by learning from a dataset generated by these planners.\nWhile our initial model uses point cloud embeddings in the input to predict\nkeypoint-based joint sequences in its output, we observed in our ablation study\nthat it remained challenging to condition the network on the point cloud\nembeddings. We identified some biases in our dataset and refined it, which\nimproved the model's performance. Our model, even without the use of the point\ncloud encodings, outperforms numerical models by an order of magnitude\nregarding the runtime, while reaching a success rate of up to 90% of collision\nfree solutions on the test set.", "AI": {"tldr": "该研究提出了一种基于扩散的深度学习模型用于机器人运动规划，通过学习传统规划器生成的数据集，显著缩短了运行时间，并取得了较高的无碰撞解决方案成功率。", "motivation": "传统的数值运动规划方法虽然能解决通用运动规划问题，但运行时间要求高，效率低下。", "method": "提出了一种新颖的基于扩散的机器人动作模型。该模型利用深度学习，从传统规划器生成的数据集中学习。最初模型尝试使用点云嵌入作为输入来预测基于关键点的关节序列，但在消融研究中发现点云嵌入的条件化存在挑战。通过识别并修正数据集中的偏差，提高了模型性能。最终模型在不使用点云编码的情况下也表现出色。", "result": "即使不使用点云编码，该模型在运行时间上比数值模型快一个数量级。在测试集上，实现了高达90%的无碰撞解决方案成功率。", "conclusion": "所提出的基于扩散的深度学习模型能够显著提高机器人运动规划的运行效率，同时保持高成功率，超越了传统的数值规划方法。"}}
{"id": "2509.03615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03615", "abs": "https://arxiv.org/abs/2509.03615", "authors": ["Aryan Gupta", "Anupam Purwar"], "title": "E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition", "comment": "Sprinklr OCR provides a fast and compute light way of performing OCR", "summary": "Optical Character Recognition (OCR) in multilingual, noisy, and diverse\nreal-world images remains a significant challenge for optical character\nrecognition systems. With the rise of Large Vision-Language Models (LVLMs),\nthere is growing interest in their ability to generalize and reason beyond\nfixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR\nsystem built specifically optimized for edge deployment in resource-constrained\nenvironments. We present a large-scale comparative evaluation of five\nstate-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two\ntraditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly\nhand annotated dataset of multilingual (54 languages) images. Our benchmark\ncovers a broad range of metrics including accuracy, semantic consistency,\nlanguage coverage, computational efficiency (latency, memory, GPU usage), and\ndeployment cost. To better reflect real-world applicability, we also conducted\nedge case deployment analysis, evaluating model performance on CPU only\nenvironments. Among the results, Qwen achieved the highest precision (0.54),\nwhile Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and\noutperformed others in efficiency, processing images 35 faster (0.17 seconds\nper image on average) and at less than 0.01 of the cost (0.006 USD per 1,000\nimages) compared to LVLM. Our findings demonstrate that the most optimal OCR\nsystems for edge deployment are the traditional ones even in the era of LLMs\ndue to their low compute requirements, low latency, and very high\naffordability.", "AI": {"tldr": "本文介绍了一种名为Sprinklr-Edge-OCR的边缘优化传统OCR系统，并将其与五种先进的大型视觉-语言模型（LVLMs）和另一种传统OCR系统在一个多语言数据集上进行了大规模比较评估。研究发现，尽管LVLMs在某些指标上表现出色，但传统OCR系统在边缘部署场景下，尤其是在效率和成本方面，表现更为优越。", "motivation": "多语言、嘈杂和多样化的真实世界图像中的光学字符识别（OCR）仍然是一个重大挑战。随着大型视觉-语言模型（LVLMs）的兴起，人们对其超越固定OCR管道的泛化和推理能力越来越感兴趣。然而，在资源受限的边缘环境中部署这些模型的适用性尚未明确。", "method": "本文引入了Sprinklr-Edge-OCR，一个专为资源受限边缘部署优化的新型OCR系统。研究在一个专有的、经过双重人工标注的54种语言的多语言图像数据集上，对五种最先进的LVLMs（InternVL, Qwen, GOT OCR, LLaMA, MiniCPM）和两种传统OCR系统（Sprinklr-Edge-OCR, SuryaOCR）进行了大规模比较评估。评估涵盖了准确性、语义一致性、语言覆盖率、计算效率（延迟、内存、GPU使用）和部署成本等一系列指标。此外，还进行了仅限CPU环境下的边缘部署分析。", "result": "在评估结果中，Qwen取得了最高的精度（0.54），而Sprinklr-Edge-OCR则获得了最佳的整体F1分数（0.46）。与LVLM相比，Sprinklr-Edge-OCR在效率上表现更佳，处理图像速度快35倍（平均每张图像0.17秒），成本也低了不到0.01倍（每1000张图像0.006美元）。", "conclusion": "研究结果表明，即使在大型语言模型时代，由于传统OCR系统具有低计算需求、低延迟和极高的经济性，它们仍然是边缘部署最理想的OCR系统。"}}
{"id": "2509.03811", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03811", "abs": "https://arxiv.org/abs/2509.03811", "authors": ["Yongzhi Qi", "Jiaheng Yin", "Jianshen Zhang", "Dongyang Geng", "Zhengyu Chen", "Hao Hu", "Wei Qi", "Zuo-Jun Max Shen"], "title": "Leveraging LLM-Based Agents for Intelligent Supply Chain Planning", "comment": null, "summary": "In supply chain management, planning is a critical concept. The movement of\nphysical products across different categories, from suppliers to warehouse\nmanagement, to sales, and logistics transporting them to customers, entails the\ninvolvement of many entities. It covers various aspects such as demand\nforecasting, inventory management, sales operations, and replenishment. How to\ncollect relevant data from an e-commerce platform's perspective, formulate\nlong-term plans, and dynamically adjust them based on environmental changes,\nwhile ensuring interpretability, efficiency, and reliability, is a practical\nand challenging problem. In recent years, the development of AI technologies,\nespecially the rapid progress of large language models, has provided new tools\nto address real-world issues. In this work, we construct a Supply Chain\nPlanning Agent (SCPA) framework that can understand domain knowledge,\ncomprehend the operator's needs, decompose tasks, leverage or create new tools,\nand return evidence-based planning reports. We deploy this framework in\nJD.com's real-world scenario, demonstrating the feasibility of LLM-agent\napplications in the supply chain. It effectively reduced labor and improved\naccuracy, stock availability, and other key metrics.", "AI": {"tldr": "本文提出并部署了一个基于大语言模型代理（LLM-agent）的供应链规划代理（SCPA）框架，用于解决电子商务平台复杂的供应链规划问题，并在京东的真实场景中取得了显著的效率和指标提升。", "motivation": "供应链管理中的规划是一个复杂且关键的概念，涉及需求预测、库存管理、销售运营和补货等多个方面。从电商平台角度，如何收集数据、制定长期计划、动态调整计划，同时确保可解释性、效率和可靠性，是一个具有挑战性的实际问题。近年来AI技术，特别是大语言模型的快速发展，为解决这些问题提供了新工具。", "method": "研究人员构建了一个供应链规划代理（SCPA）框架。该框架能够理解领域知识、领会操作员需求、分解任务、利用或创建新工具，并返回基于证据的规划报告。该框架被部署在京东的真实场景中进行验证。", "result": "SCPA框架在京东的真实场景中成功部署，证明了LLM-agent在供应链中应用的可行性。它有效地减少了劳动力，并提高了准确性、库存可用性以及其他关键指标。", "conclusion": "LLM-agent在供应链规划中具有实际应用的可行性和显著效益，能够有效应对复杂的规划挑战，并通过减少劳动力和提高关键指标来提升效率和性能。"}}
{"id": "2509.04220", "categories": ["eess.SY", "cs.RO", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.04220", "abs": "https://arxiv.org/abs/2509.04220", "authors": ["Max H. Cohen", "Eugene Lavretsky", "Aaron D. Ames"], "title": "Compatibility of Multiple Control Barrier Functions for Constrained Nonlinear Systems", "comment": "To appear at IEEE CDC 2025", "summary": "Control barrier functions (CBFs) are a powerful tool for the constrained\ncontrol of nonlinear systems; however, the majority of results in the\nliterature focus on systems subject to a single CBF constraint, making it\nchallenging to synthesize provably safe controllers that handle multiple state\nconstraints. This paper presents a framework for constrained control of\nnonlinear systems subject to box constraints on the systems' vector-valued\noutputs using multiple CBFs. Our results illustrate that when the output has a\nvector relative degree, the CBF constraints encoding these box constraints are\ncompatible, and the resulting optimization-based controller is locally\nLipschitz continuous and admits a closed-form expression. Additional results\nare presented to characterize the degradation of nominal tracking objectives in\nthe presence of safety constraints. Simulations of a planar quadrotor are\npresented to demonstrate the efficacy of the proposed framework.", "AI": {"tldr": "本文提出了一种利用多个控制障碍函数（CBFs）来处理非线性系统向量值输出上的箱式约束的框架，实现了可证明安全的控制器，并在特定条件下具有闭式解。", "motivation": "现有文献中，控制障碍函数（CBFs）主要关注单个约束，难以综合处理多个状态约束以实现可证明安全的控制器。", "method": "本文提出了一个框架，使用多个CBFs来处理非线性系统向量值输出上的箱式约束。当输出具有向量相对度时，编码这些箱式约束的CBF约束是兼容的，由此产生的基于优化的控制器是局部Lipschitz连续的，并具有闭式表达式。此外，还分析了在安全约束存在下，标称跟踪目标的性能下降情况。", "result": "研究结果表明，当输出具有向量相对度时，CBF约束是兼容的，并且所得到的基于优化的控制器是局部Lipschitz连续的，并具有闭式表达式。通过对平面四旋翼飞行器的仿真，验证了所提出框架的有效性，能够处理多个输出箱式约束下的安全控制问题。", "conclusion": "该框架为具有多个箱式输出约束的非线性系统提供了一种有效的、可证明安全的控制方法，解决了现有CBF方法在处理多约束方面的挑战，并提供了具有良好理论性质的控制器。"}}
{"id": "2509.03808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03808", "abs": "https://arxiv.org/abs/2509.03808", "authors": ["Huanan Li", "Rui Fan", "Juntao Guan", "Weidong Hao", "Lai Rui", "Tong Wu", "Yikai Wang", "Lin Gu"], "title": "EGTM: Event-guided Efficient Turbulence Mitigation", "comment": null, "summary": "Turbulence mitigation (TM) aims to remove the stochastic distortions and\nblurs introduced by atmospheric turbulence into frame cameras. Existing\nstate-of-the-art deep-learning TM methods extract turbulence cues from multiple\ndegraded frames to find the so-called \"lucky'', not distorted patch, for \"lucky\nfusion''. However, it requires high-capacity network to learn from\ncoarse-grained turbulence dynamics between synchronous frames with limited\nframe-rate, thus fall short in computational and storage efficiency. Event\ncameras, with microsecond-level temporal resolution, have the potential to\nfundamentally address this bottleneck with efficient sparse and asynchronous\nimaging mechanism. In light of this, we (i) present the fundamental\n\\textbf{``event-lucky insight''} to reveal the correlation between turbulence\ndistortions and inverse spatiotemporal distribution of event streams. Then,\nbuild upon this insight, we (ii) propose a novel EGTM framework that extracts\npixel-level reliable turbulence-free guidance from the explicit but noisy\nturbulent events for temporal lucky fusion. Moreover, we (iii) build the first\nturbulence data acquisition system to contribute the first real-world\nevent-driven TM dataset. Extensive experimental results demonstrate that our\napproach significantly surpass the existing SOTA TM method by 710 times, 214\ntimes and 224 times in model size, inference latency and model complexity\nrespectively, while achieving the state-of-the-art in restoration quality\n(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating\nthe great efficiency merit of introducing event modality into TM task. Demo\ncode and data have been uploaded in supplementary material and will be released\nonce accepted.", "AI": {"tldr": "本文提出了一种基于事件相机的湍流抑制（TM）方法，利用事件相机的微秒级时间分辨率和稀疏异步成像机制，显著提高了计算和存储效率，并在图像恢复质量上超越了现有最先进的方法。", "motivation": "现有深度学习TM方法需要高容量网络从有限帧率的同步帧中提取湍流线索进行“幸运融合”，导致计算和存储效率低下。事件相机具有微秒级时间分辨率和高效的稀疏异步成像机制，有望从根本上解决这一瓶颈。", "method": ["提出了“事件幸运洞察”（event-lucky insight），揭示了湍流畸变与事件流逆时空分布之间的相关性。", "基于该洞察，提出了一种新颖的EGTM框架，从显式但嘈杂的湍流事件中提取像素级可靠的无湍流引导信息，用于时间幸运融合。", "构建了第一个湍流数据采集系统，并贡献了首个真实世界事件驱动的TM数据集。"], "result": "在模型大小、推理延迟和模型复杂度方面，本文方法分别比现有SOTA TM方法快710倍、214倍和224倍。同时，在真实世界EGTM数据集上，恢复质量达到SOTA水平（+0.94 PSNR和+0.08 SSIM）。", "conclusion": "将事件模态引入湍流抑制任务，在实现最先进恢复质量的同时，展示了巨大的效率优势。"}}
{"id": "2509.04094", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04094", "abs": "https://arxiv.org/abs/2509.04094", "authors": ["Fatih Dursun", "Bruno Vilhena Adorno", "Simon Watson", "Wei Pan"], "title": "Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators", "comment": "14 pages, 13 figures, 3 tables. Under Review for the IEEE\n  Transactions on Robotics (T-RO)", "summary": "Object reconstruction and inspection tasks play a crucial role in various\nrobotics applications. Identifying paths that reveal the most unknown areas of\nthe object becomes paramount in this context, as it directly affects\nefficiency, and this problem is known as the view path planning problem.\nCurrent methods often use sampling-based path planning techniques, evaluating\npotential views along the path to enhance reconstruction performance. However,\nthese methods are computationally expensive as they require evaluating several\ncandidate views on the path. To this end, we propose a computationally\nefficient solution that relies on calculating a focus point in the most\ninformative (unknown) region and having the robot maintain this point in the\ncamera field of view along the path. We incorporated this strategy into the\nwhole-body control of a mobile manipulator employing a visibility constraint\nwithout the need for an additional path planner. We conducted comprehensive and\nrealistic simulations using a large dataset of 114 diverse objects of varying\nsizes from 57 categories to compare our method with a sampling-based planning\nstrategy using Bayesian data analysis. Furthermore, we performed real-world\nexperiments with an 8-DoF mobile manipulator to demonstrate the proposed\nmethod's performance in practice. Our results suggest that there is no\nsignificant difference in object coverage and entropy. In contrast, our method\nis approximately nine times faster than the baseline sampling-based method in\nterms of the average time the robot spends between views.", "AI": {"tldr": "本文提出了一种计算高效的机器人视角路径规划方法，通过计算并维持一个“焦点”在相机视野中，无需额外的路径规划器，实现了与采样基线方法相似的物体覆盖率和信息熵，但速度快了约九倍。", "motivation": "在机器人物体重建和检查任务中，识别能够揭示物体最多未知区域的路径至关重要，这被称为视角路径规划问题。现有方法常使用基于采样的路径规划技术，评估路径上的潜在视图以提高重建性能，但这些方法计算成本高昂，因为需要评估大量候选视图。", "method": "我们提出了一种计算高效的解决方案：计算物体最信息丰富（未知）区域的“焦点”，并让机器人在沿路径移动时将此焦点保持在相机视野中。我们将此策略整合到移动机械臂的全身控制中，采用可见性约束，无需额外的路径规划器。通过对114个多样化物体进行综合模拟，并与基于采样的规划策略进行贝叶斯数据分析比较。此外，还使用8自由度移动机械臂进行了真实世界实验。", "result": "研究结果表明，与基于采样的基线方法相比，我们提出的方法在物体覆盖率和信息熵方面没有显著差异。然而，在机器人两次视图之间花费的平均时间方面，我们的方法比基线方法快了大约九倍。", "conclusion": "我们提出的基于焦点的方法为视角路径规划提供了一个计算高效的解决方案，在不牺牲重建质量（覆盖率和信息熵）的前提下，显著提高了效率，尤其在机器人两次视图之间的时间消耗上表现出巨大优势。"}}
{"id": "2509.03647", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03647", "abs": "https://arxiv.org/abs/2509.03647", "authors": ["Dani Roytburg", "Matthew Bozoukov", "Matthew Nguyen", "Jou Barzdukas", "Simon Fu", "Narmeen Oozeer"], "title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators", "comment": null, "summary": "Large language models (LLMs) increasingly serve as automated evaluators, yet\nthey suffer from \"self-preference bias\": a tendency to favor their own outputs\nover those of other models. This bias undermines fairness and reliability in\nevaluation pipelines, particularly for tasks like preference tuning and model\nrouting. We investigate whether lightweight steering vectors can mitigate this\nproblem at inference time without retraining. We introduce a curated dataset\nthat distinguishes self-preference bias into justified examples of\nself-preference and unjustified examples of self-preference, and we construct\nsteering vectors using two methods: Contrastive Activation Addition (CAA) and\nan optimization-based approach. Our results show that steering vectors can\nreduce unjustified self-preference bias by up to 97\\%, substantially\noutperforming prompting and direct preference optimization baselines. Yet\nsteering vectors are unstable on legitimate self-preference and unbiased\nagreement, implying self-preference spans multiple or nonlinear directions.\nThis underscores both their promise and limits as safeguards for LLM-as-judges\nand motivates more robust interventions.", "AI": {"tldr": "大型语言模型（LLMs）作为评估器存在“自我偏好偏差”，本研究发现轻量级转向向量可以在推理时显著减少高达97%的“无理由自我偏好偏差”，但对“合理自我偏好”和“无偏见一致性”不稳定，表明该偏差复杂且需要更强大的干预措施。", "motivation": "LLMs作为自动评估器越来越普遍，但其“自我偏好偏差”——倾向于偏爱自己的输出——损害了评估流程的公平性和可靠性，尤其是在偏好调整和模型路由等任务中。", "method": "研究人员调查了在推理时使用轻量级转向向量（无需重新训练）来缓解偏差。他们构建了一个专门的数据集，将自我偏好偏差分为“合理自我偏好”和“无理由自我偏好”。转向向量通过两种方法构建：对比激活添加（CAA）和一种基于优化的方法。", "result": "结果显示，转向向量可以将“无理由自我偏好偏差”减少高达97%，显著优于提示工程和直接偏好优化基线。然而，转向向量在处理“合理自我偏好”和“无偏见一致性”时表现不稳定。", "conclusion": "转向向量作为LLM评估器的保护措施，既有潜力也有局限性，这表明自我偏好偏差涉及多重或非线性方向。这促使需要更强大的干预措施来解决这一问题。"}}
{"id": "2509.03817", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.03817", "abs": "https://arxiv.org/abs/2509.03817", "authors": ["Wei Yang", "Jesse Thomason"], "title": "Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with Multi-agent Reinforcement Learning", "comment": null, "summary": "Multi-agent systems of large language models (LLMs) show promise for complex\nreasoning, but their effectiveness is often limited by fixed collaboration\nprotocols. These frameworks typically focus on macro-level orchestration while\noverlooking agents' internal deliberative capabilities. This critical\nmeta-cognitive blindspot treats agents as passive executors unable to adapt\ntheir strategy based on internal cognitive states like uncertainty or\nconfidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where\nagents learn a decentralized policy over a set of high-level meta-cognitive\nactions: Persist, Refine, and Concede. To overcome the instability of\ntraditional policy gradients in this setting, we develop SoftRankPO, a novel\nreinforcement learning algorithm. SoftRankPO stabilizes training by shaping\nadvantages based on the rank of rewards mapped through smooth normal quantiles,\nmaking the learning process robust to reward variance. Experiments show that\nMPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across\nfive mathematical and general reasoning benchmarks compared to six\nstate-of-the-art heuristic and learning-based multi-agent reasoning algorithms.\nOur work presents a paradigm for learning adaptive, meta-cognitive policies for\nmulti-agent LLM systems, shifting the focus from designing fixed protocols to\nlearning dynamic, deliberative strategies.", "AI": {"tldr": "本文提出了一种元策略审议框架（MPDF），通过让多智能体大型语言模型（LLMs）学习去中心化的元认知动作策略（坚持、精炼、让步），并结合新型强化学习算法SoftRankPO，显著提高了复杂推理任务的准确性。", "motivation": "现有的大型语言模型多智能体系统受限于固定的协作协议，仅关注宏观编排，忽视了智能体基于不确定性或信心等内部认知状态进行内部审议的能力，将智能体视为被动执行者，限制了其有效性。", "method": "引入了元策略审议框架（MPDF），其中智能体学习一组高级元认知动作（坚持、精炼、让步）的去中心化策略。为解决传统策略梯度在此设置下的不稳定性，开发了新型强化学习算法SoftRankPO。SoftRankPO通过基于奖励排名（通过平滑正态分位数映射）来塑造优势，从而稳定训练，使其对奖励方差具有鲁棒性。", "result": "实验结果表明，与六种最先进的启发式和基于学习的多智能体推理算法相比，采用SoftRankPO的MPDF在五个数学和通用推理基准测试中，平均准确率绝对提高了4-5%。", "conclusion": "本研究为多智能体LLM系统学习自适应、元认知策略提供了一种新范式，将重点从设计固定协议转向学习动态、审议性的策略。"}}
{"id": "2509.04288", "categories": ["eess.SY", "cs.AI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04288", "abs": "https://arxiv.org/abs/2509.04288", "authors": ["Rudi Coppola", "Hovsep Touloujian", "Pierfrancesco Ombrini", "Manuel Mazo Jr"], "title": "Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery Systems with Data-Driven Formal Verification", "comment": null, "summary": "Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of\nmodern technology. In the last decades, the production and design of such\nbatteries and their adjacent embedded charging and safety protocols, denoted by\nBattery Management Systems (BMS), has taken central stage. A fundamental\nchallenge to be addressed is the trade-off between the speed of charging and\nthe ageing behavior, resulting in the loss of capacity in the battery cell. We\nrely on a high-fidelity physics-based battery model and propose an approach to\ndata-driven charging and safety protocol design. Following a\nCounterexample-Guided Inductive Synthesis scheme, we combine Reinforcement\nLearning (RL) with recent developments in data-driven formal methods to obtain\na hybrid control strategy: RL is used to synthesise the individual controllers,\nand a data-driven abstraction guides their partitioning into a switched\nstructure, depending on the initial output measurements of the battery. The\nresulting discrete selection among RL-based controllers, coupled with the\ncontinuous battery dynamics, realises a hybrid system. When a design meets the\ndesired criteria, the abstraction provides probabilistic guarantees on the\nclosed-loop performance of the cell.", "AI": {"tldr": "本文提出了一种数据驱动的混合控制策略，结合强化学习和形式化方法，用于优化锂离子电池的充电和安全协议，以平衡充电速度和电池老化，并提供性能的概率保证。", "motivation": "锂离子电池在现代技术中无处不在，但电池管理系统（BMS）面临一个核心挑战：如何在充电速度和电池容量损失（老化）之间取得平衡。现有协议需要改进以优化这一权衡。", "method": "研究采用高保真物理电池模型，并提出一种数据驱动的充电和安全协议设计方法。该方法遵循反例引导归纳合成（CEGIS）方案，将强化学习（RL）用于合成独立的控制器，并利用数据驱动的形式化方法进行抽象。抽象根据电池的初始输出测量结果，将控制器划分成一个切换结构，从而实现一个混合控制系统（RL控制器离散选择与电池连续动态结合）。", "result": "该方法产生了一种混合控制策略，其中RL用于合成控制器，而数据驱动的抽象指导了控制器的切换。当设计满足预期标准时，该抽象能够为电池的闭环性能提供概率性保证。", "conclusion": "通过结合强化学习和数据驱动的形式化方法，可以设计出一种混合控制策略，有效解决锂离子电池充电速度与老化之间的权衡问题，并为电池的性能提供可靠的概率保证。"}}
{"id": "2509.03872", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03872", "abs": "https://arxiv.org/abs/2509.03872", "authors": ["Nan Yang", "Yang Wang", "Zhanwen Liu", "Yuchao Dai", "Yang Liu", "Xiangmo Zhao"], "title": "Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection", "comment": null, "summary": "Existing RGB-Event detection methods process the low-information regions of\nboth modalities (background in images and non-event regions in event data)\nuniformly during feature extraction and fusion, resulting in high computational\ncosts and suboptimal performance. To mitigate the computational redundancy\nduring feature extraction, researchers have respectively proposed token\nsparsification methods for the image and event modalities. However, these\nmethods employ a fixed number or threshold for token selection, hindering the\nretention of informative tokens for samples with varying complexity. To achieve\na better balance between accuracy and efficiency, we propose FocusMamba, which\nperforms adaptive collaborative sparsification of multimodal features and\nefficiently integrates complementary information. Specifically, an Event-Guided\nMultimodal Sparsification (EGMS) strategy is designed to identify and\nadaptively discard low-information regions within each modality by leveraging\nscene content changes perceived by the event camera. Based on the\nsparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed\nto effectively capture and integrate complementary features from both\nmodalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate\nthat the proposed method achieves superior performance in both accuracy and\nefficiency compared to existing methods. The code will be available at\nhttps://github.com/Zizzzzzzz/FocusMamba.", "AI": {"tldr": "FocusMamba通过事件引导的多模态稀疏化和跨模态聚焦融合，解决了RGB-事件检测中低信息区域处理导致的计算冗余和性能问题，实现了准确性和效率的平衡。", "motivation": "现有RGB-事件检测方法对低信息区域（图像背景、事件数据非事件区域）进行统一处理，导致计算成本高昂且性能不佳。此外，现有token稀疏化方法采用固定数量或阈值，无法根据样本复杂性自适应保留信息丰富的token。", "method": "提出FocusMamba模型，该模型通过以下方式实现自适应协同稀疏化和高效信息融合：1. 事件引导的多模态稀疏化（EGMS）策略：利用事件相机感知的场景内容变化，自适应识别并丢弃各模态中的低信息区域。2. 跨模态聚焦融合（CMFF）模块：基于稀疏化结果，有效捕获并整合来自两种模态的互补特征。", "result": "在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，所提出的方法在准确性和效率方面均优于现有方法。", "conclusion": "FocusMamba通过自适应协同稀疏化和高效融合互补信息，有效解决了RGB-事件检测中的计算冗余和性能瓶颈，实现了准确性和效率的显著提升。"}}
{"id": "2509.04095", "categories": ["cs.RO", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.04095", "abs": "https://arxiv.org/abs/2509.04095", "authors": ["Achilleas Santi Seisa", "Viswa Narayanan Sankaranarayanan", "Gerasimos Damigos", "Sumeet Gajanan Satpute", "George Nikolakopoulos"], "title": "Cloud-Assisted Remote Control for Aerial Robots: From Theory to Proof-of-Concept Implementation", "comment": "6 pages, 7 figures, CCGridW 2025", "summary": "Cloud robotics has emerged as a promising technology for robotics\napplications due to its advantages of offloading computationally intensive\ntasks, facilitating data sharing, and enhancing robot coordination. However,\nintegrating cloud computing with robotics remains a complex challenge due to\nnetwork latency, security concerns, and the need for efficient resource\nmanagement. In this work, we present a scalable and intuitive framework for\ntesting cloud and edge robotic systems. The framework consists of two main\ncomponents enabled by containerized technology: (a) a containerized cloud\ncluster and (b) the containerized robot simulation environment. The system\nincorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling\nbidirectional communication between the cloud cluster container and the robot\nsimulation environment, while simulating realistic network conditions. To\nachieve this, we consider the use case of cloud-assisted remote control for\naerial robots, while utilizing Linux-based traffic control to introduce\nartificial delay and jitter, replicating variable network conditions\nencountered in practical cloud-robot deployments.", "AI": {"tldr": "本文提出一个可扩展且直观的框架，用于测试云和边缘机器人系统，该框架利用容器化技术构建云集群和机器人仿真环境，并通过UDP隧道和Linux流量控制模拟真实的双向网络条件。", "motivation": "云机器人技术因其卸载计算密集型任务、促进数据共享和增强机器人协调的优势而前景广阔。然而，由于网络延迟、安全问题和高效资源管理的需求，将云计算与机器人技术集成仍然是一个复杂的挑战。", "method": "该框架包含两个主要的容器化组件：(a) 容器化云集群和 (b) 容器化机器人仿真环境。系统通过用户数据报协议（UDP）隧道实现云集群容器与机器人仿真环境之间的双向通信，并利用基于Linux的流量控制引入人工延迟和抖动，以模拟实际云机器人部署中遇到的可变网络条件。具体应用案例是云辅助的空中机器人远程控制。", "result": "本文成功构建了一个可扩展且直观的框架，能够通过容器化技术和UDP隧道实现云和边缘机器人系统的集成测试，并能有效模拟实际网络条件，为云辅助远程控制等应用提供测试环境。", "conclusion": "该框架提供了一个可扩展且直观的解决方案，用于在模拟真实网络条件的情况下测试云和边缘机器人系统，从而应对云计算与机器人技术集成所面临的复杂挑战。"}}
{"id": "2509.03662", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03662", "abs": "https://arxiv.org/abs/2509.03662", "authors": ["Ali Noori", "Somya Mohanty", "Prashanti Manda"], "title": "Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV", "comment": null, "summary": "Clinical notes contain rich clinical narratives but their unstructured format\nposes challenges for large-scale analysis. Standardized terminologies such as\nSNOMED CT improve interoperability, yet understanding how concepts relate\nthrough co-occurrence and semantic similarity remains underexplored. In this\nstudy, we leverage the MIMIC-IV database to investigate the relationship\nbetween SNOMED CT concept co-occurrence patterns and embedding-based semantic\nsimilarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained\nembeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently\nco-occurring concepts are also semantically close, whether embeddings can\nsuggest missing concepts, and how these relationships evolve temporally and\nacross specialties. Our analyses reveal that while co-occurrence and semantic\nsimilarity are weakly correlated, embeddings capture clinically meaningful\nassociations not always reflected in documentation frequency. Embedding-based\nsuggestions frequently matched concepts later documented, supporting their\nutility for augmenting clinical annotations. Clustering of concept embeddings\nyielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular\nconditions) that map to patient phenotypes and care patterns. Finally,\nco-occurrence patterns linked to outcomes such as mortality and readmission\ndemonstrate the practical utility of this approach. Collectively, our findings\nhighlight the complementary value of co-occurrence statistics and semantic\nembeddings in improving documentation completeness, uncovering latent clinical\nrelationships, and informing decision support and phenotyping applications.", "AI": {"tldr": "本研究利用MIMIC-IV数据库，探索SNOMED CT概念的共现模式与基于嵌入的语义相似性之间的关系，发现两者虽弱相关，但嵌入能捕捉临床上有意义的关联，并可用于补充临床注释、揭示潜在关系和支持决策。", "motivation": "临床笔记包含丰富的叙述但格式非结构化，难以大规模分析。标准化术语如SNOMED CT虽能提高互操作性，但概念间的共现和语义相似性关系尚未被充分探索。", "method": "研究利用MIMIC-IV数据库，结合SNOMED CT概念。采用归一化点互信息（NPMI）分析共现模式，并使用预训练嵌入（如ClinicalBERT、BioBERT）计算语义相似性。分析内容包括共现与语义相似性的相关性、嵌入对缺失概念的建议能力、以及这些关系随时间和专业的变化。", "result": "分析显示，共现和语义相似性之间存在弱相关性，但嵌入能捕捉到临床上有意义的关联，这些关联不总是体现在文档频率中。基于嵌入的建议经常与后来记录的概念相符。概念嵌入的聚类产生了连贯的临床主题（症状、实验室检查、诊断、心血管疾病），这些主题与患者表型和护理模式相对应。此外，与死亡率和再入院等结果相关的共现模式也证明了该方法的实用性。", "conclusion": "研究结果强调了共现统计和语义嵌入在提高文档完整性、揭示潜在临床关系以及为决策支持和表型分析应用提供信息方面的互补价值。"}}
{"id": "2509.03827", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03827", "abs": "https://arxiv.org/abs/2509.03827", "authors": ["Pierre Le Coz", "Jia An Liu", "Debarun Bhattacharjya", "Georgina Curto", "Serge Stinckwich"], "title": "What Would an LLM Do? Evaluating Policymaking Capabilities of Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly being adopted in high-stakes\ndomains. Their capacity to process vast amounts of unstructured data, explore\nflexible scenarios, and handle a diversity of contextual factors can make them\nuniquely suited to provide new insights for the complexity of social\npolicymaking. This article evaluates whether LLMs' are aligned with domain\nexperts (and among themselves) to inform social policymaking on the subject of\nhomelessness alleviation - a challenge affecting over 150 million people\nworldwide. We develop a novel benchmark comprised of decision scenarios with\npolicy choices across four geographies (South Bend, USA; Barcelona, Spain;\nJohannesburg, South Africa; Macau SAR, China). The policies in scope are\ngrounded in the conceptual framework of the Capability Approach for human\ndevelopment. We also present an automated pipeline that connects the\nbenchmarked policies to an agent-based model, and we explore the social impact\nof the recommended policies through simulated social scenarios. The paper\nresults reveal promising potential to leverage LLMs for social policy making.\nIf responsible guardrails and contextual calibrations are introduced in\ncollaboration with local domain experts, LLMs can provide humans with valuable\ninsights, in the form of alternative policies at scale.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在解决无家可归问题等社会政策制定方面的潜力，通过基准测试和基于代理的模型模拟，发现LLMs在专家协作和适当防护下可提供有价值的政策见解。", "motivation": "大型语言模型（LLMs）因其处理非结构化数据、探索灵活场景和处理多样化情境的能力，在社会政策制定中具有提供新见解的潜力。本研究旨在评估LLMs在解决全球无家可归问题这一复杂社会挑战中，是否能与领域专家保持一致，并有效为社会政策制定提供信息。", "method": "研究开发了一个包含决策场景和政策选择的新型基准，涵盖四个不同地理区域（美国南本德、西班牙巴塞罗那、南非约翰内斯堡、中国澳门特区）。所选政策基于人类发展的能力方法框架。此外，研究还提出了一个自动化流程，将基准政策连接到基于代理的模型，并通过模拟社会场景探索推荐政策的社会影响。", "result": "研究结果显示，利用大型语言模型进行社会政策制定具有巨大的前景。", "conclusion": "如果能与当地领域专家合作引入负责任的防护措施和情境校准，大型语言模型可以为人类提供有价值的见解，以大规模替代政策的形式，从而辅助社会政策制定。"}}
{"id": "2509.04308", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04308", "abs": "https://arxiv.org/abs/2509.04308", "authors": ["Farshad Amani", "Faezeh Ardali", "Amin Kargarian"], "title": "Learning Optimal Crew Dispatch for Grid Restoration Following an Earthquake", "comment": null, "summary": "Post-disaster crew dispatch is a critical but computationally intensive task.\nTraditional mixed-integer linear programming methods often require minutes to\nseveral hours to compute solutions, leading to delays that hinder timely\ndecision-making in highly dynamic restoration environments. To address this\nchallenge, we propose a novel learning-based framework that integrates\ntransformer architectures with deep reinforcement learning (DRL) to deliver\nnear real-time decision support without compromising solution quality. Crew\ndispatch is formulated as a sequential decision-making problem under\nuncertainty, where transformers capture high-dimensional system states and\ntemporal dependencies, while DRL enables adaptive and scalable decision-making.\nEarthquake-induced distribution network damage is first characterized using\nestablished seismic standards, followed by a scenario generation and reduction\npipeline that aggregates probable outcomes into a single geospatial impact map.\nConditioned on this map, the proposed framework generates second-level dispatch\nstrategies, trained offline on simulated and historical events and deployed\nonline for rapid response. In addition to substantial runtime improvements, the\nproposed method enhances system resilience by enabling faster and more\neffective recovery and restoration. Case studies, particularly on the 2869-bus\nEuropean gas and power network, demonstrate that the method substantially\naccelerates restoration while maintaining high-quality solutions, underscoring\nits potential for practical deployment in large-scale disaster response.", "AI": {"tldr": "该研究提出了一种结合Transformer和深度强化学习（DRL）的新型学习框架，用于灾后人员调度，旨在提供近实时决策支持，同时保持解决方案质量，以加速恢复并增强系统弹性。", "motivation": "传统的混合整数线性规划方法在计算灾后人员调度方案时耗时过长（数分钟至数小时），导致决策延迟，影响动态恢复环境中的及时响应。", "method": "将人员调度建模为不确定性下的序列决策问题。框架整合了Transformer架构（捕捉高维系统状态和时间依赖性）和DRL（实现自适应和可扩展决策）。首先依据地震标准表征地震引起的配电网络损坏，然后通过情景生成和缩减流程聚合结果为地理空间影响图。基于此图，离线训练（模拟和历史事件）并在线部署该框架，以生成二级调度策略。", "result": "该方法实现了近实时的决策支持，显著缩短了运行时间，同时保持了高质量的解决方案。通过实现更快、更有效的恢复，增强了系统弹性。在2869总线欧洲燃气和电力网络的案例研究中，证明了该方法能大幅加速恢复并保持高解决方案质量。", "conclusion": "所提出的方法能够显著加速灾后恢复过程，同时保持高解决方案质量，这凸显了其在大规模灾害响应中实际部署的巨大潜力。"}}
{"id": "2509.03873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03873", "abs": "https://arxiv.org/abs/2509.03873", "authors": ["Jiajun Song", "Xiaoou Liu"], "title": "SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition", "comment": "34th International Conference on Artificial Neural Networks - ICANN\n  2025", "summary": "Food recognition has gained significant attention, but the rapid emergence of\nnew dishes requires methods for recognizing unseen food categories, motivating\nZero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot\nFood Recognition (CZSFR), where cuisines and ingredients naturally align with\nattributes and objects in Compositional Zero-Shot learning (CZSL). However,\nCZSFR faces three challenges: (1) Redundant background information distracts\nmodels from learning meaningful food features, (2) Role confusion between\nstaple and side dishes leads to misclassification, and (3) Semantic bias in a\nsingle attribute can lead to confusion of understanding. Therefore, we propose\nSalientFusion, a context-aware CZSFR method with two components: SalientFormer,\nwhich removes background redundancy and uses depth features to resolve role\nconfusion; DebiasAT, which reduces the semantic bias by aligning prompts with\nvisual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we\nshow that SalientFusion achieves state-of-the-art results on these benchmarks\nand the most popular general datasets for the general CZSL. The code is\navaliable at https://github.com/Jiajun-RUC/SalientFusion.", "AI": {"tldr": "本文提出了组合零样本食物识别 (CZSFR) 任务，并针对其面临的背景冗余、角色混淆和语义偏差三大挑战，提出了 SalientFusion 方法。该方法通过 SalientFormer 移除背景并解决角色混淆，通过 DebiasAT 减少语义偏差，并在新提出的 CZSFood-90、CZSFood-164 数据集及现有通用数据集上实现了最先进的性能。", "motivation": "食物识别领域面临新菜品不断涌现的问题，需要识别未见过的食物类别，这催生了零样本食物学习 (ZSFL)。作者发现菜系和食材与组合零样本学习 (CZSL) 中的属性和对象天然契合，因此提出了组合零样本食物识别 (CZSFR) 任务。然而，CZSFR 面临三个挑战：1) 冗余背景信息干扰模型学习有意义的食物特征；2) 主食和配菜之间的角色混淆导致错误分类；3) 单一属性中的语义偏差导致理解混淆。", "method": "本文提出了 SalientFusion，一种上下文感知的 CZSFR 方法，包含两个主要组件：1) SalientFormer，用于移除背景冗余并利用深度特征解决角色混淆；2) DebiasAT，通过将提示词与视觉特征对齐来减少语义偏差。", "result": "通过使用作者提出的 CZSFood-90 和 CZSFood-164 基准数据集，SalientFusion 在这些数据集以及最流行的通用 CZSL 数据集上均取得了最先进的（state-of-the-art）结果。", "conclusion": "SalientFusion 方法有效解决了组合零样本食物识别 (CZSFR) 中的关键挑战，包括背景冗余、角色混淆和语义偏差，并在多个基准测试中展现出卓越的性能，推动了该领域的发展。"}}
{"id": "2509.04119", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04119", "abs": "https://arxiv.org/abs/2509.04119", "authors": ["Ke Wu", "Yuhao Wang", "Kevin Henry", "Cesare Stefanini", "Gang Zheng"], "title": "Lightweight Kinematic and Static Modeling of Cable-Driven Continuum Robots via Actuation-Space Energy Formulation", "comment": "Journal", "summary": "Continuum robots, inspired by octopus arms and elephant trunks, combine\ndexterity with intrinsic compliance, making them well suited for unstructured\nand confined environments. Yet their continuously deformable morphology poses\nchallenges for motion planning and control, calling for accurate but\nlightweight models. We propose the Lightweight Actuation Space Energy Modeling\n(LASEM) framework for cable driven continuum robots, which formulates actuation\npotential energy directly in actuation space. LASEM yields an analytical\nforward model derived from geometrically nonlinear beam and rod theories via\nHamilton's principle, while avoiding explicit modeling of cable backbone\ncontact. It accepts both force and displacement inputs, thereby unifying\nkinematic and static formulations. Assuming the friction is neglected, the\nframework generalizes to nonuniform geometries, arbitrary cable routings,\ndistributed loading and axial extensibility, while remaining computationally\nefficient for real-time use. Numerical simulations validate its accuracy, and a\nsemi-analytical iterative scheme is developed for inverse kinematics. To\naddress discretization in practical robots, LASEM further reformulates the\nfunctional minimization as a numerical optimization, which also naturally\nincorporates cable potential energy without explicit contact modeling.", "AI": {"tldr": "本文提出了一种名为LASEM的轻量级驱动空间能量建模框架，用于索驱动连续体机器人。该框架通过在驱动空间中直接构建驱动势能，提供了一个精确且计算高效的分析正向模型，并能处理逆运动学和实际离散化问题。", "motivation": "连续体机器人因其灵巧性和固有的柔顺性，非常适合非结构化和受限环境。然而，其连续可变形的形态给运动规划和控制带来了挑战，需要精确但轻量级的模型。", "method": "LASEM框架直接在驱动空间中构建驱动势能。它利用几何非线性梁和杆理论，通过哈密顿原理推导出一个分析正向模型，避免了显式建模缆索与骨架的接触。该模型接受力和位移输入，统一了运动学和静力学公式。在忽略摩擦的情况下，该框架可推广到非均匀几何形状、任意缆索布线、分布式载荷和轴向可伸缩性。为逆运动学开发了一种半分析迭代方案。对于实际机器人的离散化问题，LASEM将泛函最小化重新表述为数值优化问题。", "result": "LASEM框架提供了一个精确且计算高效的分析正向模型，适用于实时使用。数值模拟验证了其准确性。该框架能够推广到多种复杂情况，并为逆运动学提供了有效的解决方案。通过数值优化，它还能自然地处理实际机器人的离散化问题。", "conclusion": "LASEM框架成功解决了索驱动连续体机器人的建模挑战，提供了一个准确、轻量级且多功能的模型，适用于实时运动规划和控制，并能有效处理复杂的几何形状和实际应用中的离散化问题。"}}
{"id": "2509.03725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03725", "abs": "https://arxiv.org/abs/2509.03725", "authors": ["Parush Gera", "Tempestt Neal"], "title": "MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection", "comment": null, "summary": "We present the novel approach for stance detection across domains and\ntargets, Metric Learning-Based Few-Shot Learning for Cross-Target and\nCross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with\ntriplet loss to capture semantic similarities and differences between stance\ntargets, enhancing domain adaptation. By constructing a discriminative\nembedding space, MLSD allows a cross-target or cross-domain stance detection\nmodel to acquire useful examples from new target domains. We evaluate MLSD in\nmultiple cross-target and cross-domain scenarios across two datasets, showing\nstatistically significant improvement in stance detection performance across\nsix widely used stance detection models.", "AI": {"tldr": "本文提出MLSD（Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection），一种基于度量学习的少样本方法，用于跨领域和跨目标立场检测，显著提升了现有模型的性能。", "motivation": "解决跨领域和跨目标立场检测的挑战，提升其在不同目标和领域间的泛化能力和检测性能。", "method": "MLSD方法利用度量学习（Metric Learning）结合三重态损失（Triplet Loss）来捕获立场目标之间的语义相似性和差异。通过构建一个判别性嵌入空间，MLSD使跨目标或跨领域立场检测模型能够从新的目标领域获取有用的样本，从而增强领域适应性。", "result": "在两个数据集的多个跨目标和跨领域场景中对MLSD进行了评估。结果表明，MLSD在六种广泛使用的立场检测模型上均实现了统计学上显著的性能提升。", "conclusion": "MLSD是一种新颖且有效的跨目标和跨领域立场检测方法，能够通过度量学习显著提升现有立场检测模型的性能。"}}
{"id": "2509.03828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03828", "abs": "https://arxiv.org/abs/2509.03828", "authors": ["Jaerong Ahn", "Andrew Wen", "Nan Wang", "Heling Jia", "Zhiyi Yue", "Sunyang Fu", "Hongfang Liu"], "title": "An Agentic Model Context Protocol Framework for Medical Concept Standardization", "comment": null, "summary": "The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)\nprovides a standardized representation of heterogeneous health data to support\nlarge-scale, multi-institutional research. One critical step in data\nstandardization using OMOP CDM is the mapping of source medical terms to OMOP\nstandard concepts, a procedure that is resource-intensive and error-prone.\nWhile large language models (LLMs) have the potential to facilitate this\nprocess, their tendency toward hallucination makes them unsuitable for clinical\ndeployment without training and expert validation. Here, we developed a\nzero-training, hallucination-preventive mapping system based on the Model\nContext Protocol (MCP), a standardized and secure framework allowing LLMs to\ninteract with external resources and tools. The system enables explainable\nmapping and significantly improves efficiency and accuracy with minimal effort.\nIt provides real-time vocabulary lookups and structured reasoning outputs\nsuitable for immediate use in both exploratory and production environments.", "AI": {"tldr": "本文开发了一个基于模型上下文协议（MCP）的零训练、防幻觉映射系统，用于将源医疗术语高效准确地映射到OMOP CDM标准概念，解决了传统方法资源密集且易错的问题。", "motivation": "将异构健康数据标准化为OMOP CDM时，将源医疗术语映射到OMOP标准概念是一个资源密集且易错的关键步骤。虽然大型语言模型（LLMs）有潜力简化此过程，但其固有的“幻觉”倾向使其不适合未经训练和专家验证的临床部署。", "method": "研究开发了一个零训练、防幻觉的映射系统。该系统基于模型上下文协议（MCP），这是一个标准化且安全的框架，允许LLMs与外部资源和工具进行交互。", "result": "该系统实现了可解释的映射，显著提高了效率和准确性，且只需极少的努力。它提供了实时词汇查找和结构化推理输出，适用于探索性和生产环境的即时使用。", "conclusion": "该零训练、防幻觉的映射系统能够有效利用LLMs的优势，同时避免其缺点，为OMOP CDM数据标准化提供了一个高效、准确且可解释的解决方案，适用于实际部署。"}}
{"id": "2509.04388", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04388", "abs": "https://arxiv.org/abs/2509.04388", "authors": ["Regulo E. Avila-Martinez", "Xavier Guillaud", "Javier Renedo", "Luis Rouco", "Aurelio Garcia-Cerrada", "Lukas Sigrist"], "title": "Impact on transient stability of self-synchronisation control strategies in grid-forming VSC-based generators", "comment": "36 pages, 18 figures, 6 tables", "summary": "Grid-forming voltage source converters (GFM-VSCs) are emerging as a solution\nfor integrating renewable energy resources (RERs) into power systems. GFM-VSCs\nneed a self-synchronisation strategy to ensure that all converters and\ngenerators in the power system are in synchronism and they reach the same\nfrequency in steady state. The self-synchronisation strategy in GFM-VSCs that\nhas received most attention in previous research is virtual synchronous machine\n(VSM) control. However, no systematic study of the effects on transient\nstability of different variants of this strategy has been carried out in\nprevious work. This paper analyses and compares transient stability of four\nself-synchronisation strategies for GFM-VSCs: VSM without phase-locked loop\n(PLL), VSM with PLL, VSM without PLL using wash-out filter and\nintegral-proportional (IP) controller. The paper also analyses two different\nmethods that can \\color{black} be applied to GFM-VSC self-synchronisation\nstrategies to improve transient stability: the concept of virtual unsaturated\nactive-power controller (VAPC), proposed in previous work, and an algorithm for\nfrequency limitation in the GFM-VSC (FLC), which is proposed in this paper.", "AI": {"tldr": "本文分析并比较了四种GFM-VSC自同步策略（VSM变体）的暂态稳定性，并提出/分析了两种提高暂态稳定性的方法（VAPC和FLC）。", "motivation": "并网型电压源变换器（GFM-VSCs）是可再生能源并网的关键解决方案，需要自同步策略。尽管虚拟同步机（VSM）控制是流行的自同步策略，但目前缺乏对其不同变体对暂态稳定性影响的系统性研究。", "method": "本文分析并比较了四种GFM-VSC自同步策略的暂态稳定性：无锁相环（PLL）的VSM、带PLL的VSM、使用冲刷滤波器（wash-out filter）的无PLL VSM，以及积分-比例（IP）控制器。此外，本文还分析了两种可用于提高暂态稳定性的方法：虚拟不饱和有功功率控制器（VAPC）和本文提出的GFM-VSC限频算法（FLC）。", "result": "本文分析并比较了四种GFM-VSC自同步策略（VSM无PLL、VSM带PLL、VSM无PLL带冲刷滤波器、IP控制器）的暂态稳定性。同时，分析了VAPC概念和新提出的GFM-VSC限频算法（FLC）对提高暂态稳定性的效果。", "conclusion": "通过对不同GFM-VSC自同步策略及其改进方法的分析和比较，本文旨在深入理解和提升GFM-VSC在可再生能源并网中的暂态稳定性。"}}
{"id": "2509.03883", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.03883", "abs": "https://arxiv.org/abs/2509.03883", "authors": ["Haiwei Xue", "Xiangyang Luo", "Zhanghao Hu", "Xin Zhang", "Xunzhi Xiang", "Yuqin Dai", "Jianzhuang Liu", "Zhensong Zhang", "Minglei Li", "Jian Yang", "Fei Ma", "Zhiyong Wu", "Changpeng Yang", "Zonghong Dai", "Fei Richard Yu"], "title": "Human Motion Video Generation: A Survey", "comment": "Accepted by TPAMI. Github Repo:\n  https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:\n  https://ieeexplore.ieee.org/document/11106267", "summary": "Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "AI": {"tldr": "这篇综述对人类运动视频生成领域进行了全面深入的分析，涵盖了其生成过程的五个关键阶段、十多个子任务、三种主要模态（视觉、文本、音频），并首次探讨了大型语言模型在该领域的应用潜力。", "motivation": "现有综述侧重于单一方法，缺乏对整个生成过程的全面概述，且未讨论大型语言模型在增强人类运动视频生成方面的潜力，因此本研究旨在填补这一空白。", "method": "通过对二百多篇论文进行深入审查，本综述详细阐述了人类运动视频生成的五个关键阶段（输入、运动规划、运动视频生成、精炼、输出）和十多个子任务，并按视觉、文本和音频三种主要模态回顾了最新进展和技术趋势。", "result": "本综述提供了该领域的全面概览，突出了具有里程碑意义的技术突破性工作，并首次讨论了大型语言模型在增强人类运动视频生成方面的潜力。", "conclusion": "本综述旨在揭示人类运动视频生成的前景，并为推动数字人综合应用提供宝贵资源。"}}
{"id": "2509.04324", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04324", "abs": "https://arxiv.org/abs/2509.04324", "authors": ["Chen Hu", "Shan Luo", "Letizia Gionfrida"], "title": "OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection", "comment": null, "summary": "Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.", "AI": {"tldr": "OVGrasp是一个分层控制框架，通过集成RGB-D视觉、开放词汇提示和语音命令，为软骨骼外衣提供抓握辅助，实现在非结构化环境中对未知物体的零样本检测和高抓握能力，并优于现有技术。", "motivation": "恢复运动障碍个体在非结构化环境中的自主性至关重要，尤其是在物体类别和用户意图多样且不可预测的情况下。", "method": "OVGrasp是一个分层控制框架，用于基于软骨骼外衣的抓握辅助。它集成了RGB-D视觉、开放词汇提示和语音命令，以实现鲁棒的多模态交互。该框架采用视觉-语言基础模型和开放词汇机制，实现对未见物体的零样本检测。多模态决策器融合空间和语言线索，在多物体场景中推断用户意图。整个框架部署在定制的以自我为中心的穿戴式外骨骼上，并对15个物体和三种抓握类型进行了系统评估。", "result": "OVGrasp在十名参与者的实验中，抓握能力得分（GAS）达到87.00%，优于现有基线，并实现了与自然手部运动更佳的运动学对齐。", "conclusion": "OVGrasp提供了一种强大且可泛化的抓握辅助解决方案，能够帮助运动障碍个体在复杂多变的开放环境中实现自主性，并展现出优异的性能和自然度。"}}
{"id": "2509.03791", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03791", "abs": "https://arxiv.org/abs/2509.03791", "authors": ["Saki Imai", "Mert İnan", "Anthony Sicilia", "Malihe Alikhani"], "title": "SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation", "comment": null, "summary": "Evaluating sign language generation is often done through back-translation,\nwhere generated signs are first recognized back to text and then compared to a\nreference using text-based metrics. However, this two-step evaluation pipeline\nintroduces ambiguity: it not only fails to capture the multimodal nature of\nsign language-such as facial expressions, spatial grammar, and prosody-but also\nmakes it hard to pinpoint whether evaluation errors come from sign generation\nmodel or the translation system used to assess it. In this work, we propose\nSiLVERScore, a novel semantically-aware embedding-based evaluation metric that\nassesses sign language generation in a joint embedding space. Our contributions\ninclude: (1) identifying limitations of existing metrics, (2) introducing\nSiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness\nto semantic and prosodic variations, and (4) exploring generalization\nchallenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore\nachieves near-perfect discrimination between correct and random pairs (ROC AUC\n= 0.99, overlap < 7%), substantially outperforming traditional metrics.", "AI": {"tldr": "本文提出了一种名为SiLVERScore的新型语义感知嵌入式评估指标，用于手语生成，解决了传统回译方法的局限性。", "motivation": "现有手语生成评估方法（回译）存在歧义：它无法捕捉手语的多模态特性（如面部表情、空间语法、韵律），也难以区分评估错误是来自生成模型还是翻译系统。", "method": "本文提出SiLVERScore，这是一种新颖的、基于语义感知的嵌入式评估指标，用于在联合嵌入空间中评估手语生成。研究包括识别现有指标的局限性，引入SiLVERScore，证明其对语义和韵律变化的鲁棒性，并探索其在不同数据集上的泛化挑战。", "result": "在PHOENIX-14T和CSL-Daily数据集上，SiLVERScore在正确和随机配对之间实现了近乎完美的区分（ROC AUC = 0.99，重叠 < 7%），显著优于传统指标。", "conclusion": "SiLVERScore为手语生成提供了一种更鲁棒、语义感知更强的评估方法，有效克服了传统回译评估的不足，并在区分生成手语方面表现出卓越性能。"}}
{"id": "2509.03830", "categories": ["cs.AI", "cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.03830", "abs": "https://arxiv.org/abs/2509.03830", "authors": ["Kaizhen Tan", "Yufan Wu", "Yuxuan Liu", "Haoran Zeng"], "title": "A Multidimensional AI-powered Framework for Analyzing Tourist Perception in Historic Urban Quarters: A Case Study in Shanghai", "comment": null, "summary": "Historic urban quarters play a vital role in preserving cultural heritage\nwhile serving as vibrant spaces for tourism and everyday life. Understanding\nhow tourists perceive these environments is essential for sustainable,\nhuman-centered urban planning. This study proposes a multidimensional\nAI-powered framework for analyzing tourist perception in historic urban\nquarters using multimodal data from social media. Applied to twelve historic\nquarters in central Shanghai, the framework integrates focal point extraction,\ncolor theme analysis, and sentiment mining. Visual focus areas are identified\nfrom tourist-shared photos using a fine-tuned semantic segmentation model. To\nassess aesthetic preferences, dominant colors are extracted using a clustering\nmethod, and their spatial distribution across quarters is analyzed. Color\nthemes are further compared between social media photos and real-world street\nviews, revealing notable shifts. This divergence highlights potential gaps\nbetween visual expectations and the built environment, reflecting both\nstylistic preferences and perceptual bias. Tourist reviews are evaluated\nthrough a hybrid sentiment analysis approach combining a rule-based method and\na multi-task BERT model. Satisfaction is assessed across four dimensions:\ntourist activities, built environment, service facilities, and business\nformats. The results reveal spatial variations in aesthetic appeal and\nemotional response. Rather than focusing on a single technical innovation, this\nframework offers an integrated, data-driven approach to decoding tourist\nperception and contributes to informed decision-making in tourism, heritage\nconservation, and the design of aesthetically engaging public spaces.", "AI": {"tldr": "本研究提出一个多维AI框架，利用社交媒体的多模态数据（照片和评论）分析游客对历史城区的感知，以支持可持续的城市规划。", "motivation": "历史城区在文化遗产保护和旅游生活中扮演重要角色。理解游客对这些环境的感知对于可持续、以人为本的城市规划至关重要。", "method": "该研究提出了一个多维AI框架，应用于上海的十二个历史街区。框架整合了焦点区域提取、色彩主题分析和情感挖掘。通过微调的语义分割模型识别照片中的视觉焦点区域；利用聚类方法提取主导颜色并分析其空间分布，同时比较社交媒体照片与实际街景的色彩主题；通过结合基于规则的方法和多任务BERT模型的混合情感分析方法评估游客评论，并从旅游活动、建成环境、服务设施和商业形式四个维度评估满意度。", "result": "研究发现社交媒体照片与实际街景的色彩主题存在显著差异，表明视觉预期与建成环境之间可能存在差距，反映了风格偏好和感知偏差。结果还揭示了美学吸引力和情感反应上的空间差异。", "conclusion": "该框架提供了一种整合的、数据驱动的方法来解读游客感知，有助于旅游、遗产保护和美学公共空间设计中的知情决策。"}}
{"id": "2509.04399", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04399", "abs": "https://arxiv.org/abs/2509.04399", "authors": ["Adrian Wiltz", "Dimos V. Dimarogonas"], "title": "Leveraging Equivariances and Symmetries in the Control Barrier Function Synthesis", "comment": "15 pages", "summary": "The synthesis of Control Barrier Functions (CBFs) often involves demanding\ncomputations or a meticulous construction. However, structural properties of\nthe system dynamics and constraints have the potential to mitigate these\nchallenges. In this paper, we explore how equivariances in the dynamics,\nloosely speaking a form of symmetry, can be leveraged in the CBF synthesis.\nAlthough CBFs are generally not inherently symmetric, we show how equivariances\nin the dynamics and symmetries in the constraints induce symmetries in CBFs\nderived through reachability analysis. This insight allows us to infer their\nCBF values across the entire domain from their values on a subset, leading to\nsignificant computational savings. Interestingly, equivariances can be even\nleveraged to the CBF synthesis for non-symmetric constraints. Specifically, we\nshow how a partially known CBF can be leveraged together with equivariances to\nconstruct a CBF for various new constraints. Throughout the paper, we provide\nexamples illustrating the theoretical findings. Furthermore, a numerical study\ninvestigates the computational gains from invoking equivariances into the CBF\nsynthesis.", "AI": {"tldr": "本文探讨了如何利用系统动力学中的等变性（一种对称形式）来简化控制障碍函数（CBF）的合成和计算，从而显著节省计算资源。", "motivation": "控制障碍函数（CBF）的合成通常计算量大或需要精心构建。研究人员希望利用系统动力学和约束的结构特性（如对称性/等变性）来缓解这些挑战。", "method": "本文研究了动力学中的等变性如何应用于CBF合成。具体方法包括：1) 证明动力学中的等变性以及约束中的对称性如何通过可达性分析诱导CBF的对称性；2) 展示如何利用部分已知的CBF和等变性来为各种新约束（包括非对称约束）构建CBF；3) 通过理论示例和数值研究来验证计算增益。", "result": "研究发现，动力学中的等变性以及约束中的对称性可以诱导CBF的对称性。这一洞察使得研究者可以从CBF在子集上的值推断其在整个域上的值，从而实现显著的计算节省。此外，即使对于非对称约束，等变性也可以与部分已知的CBF结合使用，以构建新的CBF。数值研究也证实了利用等变性进行CBF合成的计算收益。", "conclusion": "利用系统动力学中的等变性可以有效简化控制障碍函数（CBF）的合成过程，减少计算量，并且能够将CBF应用于更广泛的约束条件（包括非对称约束）。"}}
{"id": "2509.03887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03887", "abs": "https://arxiv.org/abs/2509.03887", "authors": ["Bu Jin", "Songen Gu", "Xiaotao Hu", "Yupeng Zheng", "Xiaoyang Guo", "Qian Zhang", "Xiaoxiao Long", "Wei Yin"], "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction", "comment": null, "summary": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.", "AI": {"tldr": "本文提出了OccTENS，一种生成式占据世界模型，能够实现可控、高保真、高效的长期占据生成，同时解决了现有方法的效率低、长期生成退化和缺乏可控性等问题。", "motivation": "现有的基于自回归（AR）的占据世界模型在长期生成中存在效率低下、时间退化和缺乏可控性等问题。此外，3D场景的精细几何和动态演化对生成模型提出了巨大挑战。", "method": "OccTENS将占据世界模型重新定义为时间下一尺度预测（TENS）任务，将时间序列建模分解为空间逐尺度生成和时间逐场景预测。它使用一个名为TensFormer的模型来灵活高效地管理占据序列的时间因果关系和空间关系。为增强姿态可控性，还提出了一个整体姿态聚合策略，统一了占据和自我运动的序列建模。", "result": "实验表明，OccTENS在占据质量和推理速度方面均优于现有最先进的方法。", "conclusion": "OccTENS通过创新的TENS任务重构和TensFormer架构，成功解决了3D占据世界模型在长期生成中的效率、质量和可控性挑战，实现了卓越的性能。"}}
{"id": "2509.04441", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04441", "abs": "https://arxiv.org/abs/2509.04441", "authors": ["Hao-Shu Fang", "Branden Romero", "Yichen Xie", "Arthur Hu", "Bo-Ruei Huang", "Juan Alvarez", "Matthew Kim", "Gabriel Margolis", "Kavya Anbarasu", "Masayoshi Tomizuka", "Edward Adelson", "Pulkit Agrawal"], "title": "DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation", "comment": "project page: https://dex-op.github.io", "summary": "We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.", "AI": {"tldr": "本文提出了一种名为“围手术期操作”（perioperation）的机器人数据采集范式，并实现于被动式手部外骨骼DEXOP中。DEXOP通过连接人手与机器人手指，提供直接接触反馈和姿态镜像，以最大化数据可迁移性，实现高效、高质量的灵巧操作数据采集。", "motivation": "现有机器人数据采集方法（如远程操控）在灵巧操作任务中不够自然，限制了人类采集丰富感官数据的能力，导致数据质量和效率不高。研究旨在开发一种能最大化数据可迁移性，并使人类能自然、高效地采集高质量灵巧操作数据的方法。", "method": "引入“围手术期操作”（perioperation）范式。设计并实现了DEXOP，一个被动式手部外骨骼。DEXOP机械连接人手与机器人手指，为用户提供直接接触反馈（通过本体感受），并将人手姿态镜像到被动机器人手上，以最大化演示技能向机器人的迁移。", "result": "DEXOP能大规模收集高质量的灵巧、接触密集型任务演示数据。与远程操控相比，使用DEXOP数据学习的策略显著提高了单位数据采集时间的任务性能，并增加了演示的速度和准确性。", "conclusion": "DEXOP是一种强大的工具，通过提供更自然、高效且高质量的数据采集方式，显著推动了机器人灵巧性的发展。"}}
{"id": "2509.03805", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03805", "abs": "https://arxiv.org/abs/2509.03805", "authors": ["Saki Imai", "Mert İnan", "Anthony Sicilia", "Malihe Alikhani"], "title": "Measuring How (Not Just Whether) VLMs Build Common Ground", "comment": null, "summary": "Large vision language models (VLMs) increasingly claim reasoning skills, yet\ncurrent benchmarks evaluate them in single-turn or question answering settings.\nHowever, grounding is an interactive process in which people gradually develop\nshared understanding through ongoing communication. We introduce a four-metric\nsuite (grounding efficiency, content alignment, lexical adaptation, and\nhuman-likeness) to systematically evaluate VLM performance in interactive\ngrounding contexts. We deploy the suite on 150 self-play sessions of\ninteractive referential games between three proprietary VLMs and compare them\nwith human dyads. All three models diverge from human patterns on at least\nthree metrics, while GPT4o-mini is the closest overall. We find that (i) task\nsuccess scores do not indicate successful grounding and (ii) high\nimage-utterance alignment does not necessarily predict task success. Our metric\nsuite and findings offer a framework for future research on VLM grounding.", "AI": {"tldr": "本研究引入了一个四度量标准套件，用于评估大型视觉语言模型（VLMs）在交互式接地（grounding）上下文中的表现，发现现有模型与人类模式存在显著差异，且任务成功与接地质量并非直接关联。", "motivation": "当前基准测试主要在单轮或问答设置中评估VLMs的推理能力，但接地是一个互动过程，涉及通过持续沟通逐步建立共享理解。现有评估方法未能充分捕捉VLM在交互式接地中的表现。", "method": "研究引入了一个包含四个度量标准（接地效率、内容对齐、词汇适应和类人性）的套件。该套件被应用于三个专有VLM之间的150次交互式指称游戏自玩会话，并与人类配对进行比较。", "result": "所有三个模型在至少三个度量标准上都与人类模式存在差异，其中GPT4o-mini总体上最接近人类。研究发现，任务成功分数并不能表明成功的接地，并且高的图像-话语对齐也并不一定预示着任务成功。", "conclusion": "本研究提出的度量标准套件和发现为未来VLM接地能力的研究提供了一个框架，并强调了在交互式语境中评估VLM时需要更细致的视角。"}}
{"id": "2509.03857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03857", "abs": "https://arxiv.org/abs/2509.03857", "authors": ["Kishor Datta Gupta", "Mohd Ariful Haque", "Hasmot Ali", "Marufa Kamal", "Syed Bahauddin Alam", "Mohammad Ashiqur Rahman"], "title": "Continuous Monitoring of Large-Scale Generative AI via Deterministic Knowledge Graph Structures", "comment": null, "summary": "Generative AI (GEN AI) models have revolutionized diverse application domains\nbut present substantial challenges due to reliability concerns, including\nhallucinations, semantic drift, and inherent biases. These models typically\noperate as black-boxes, complicating transparent and objective evaluation.\nCurrent evaluation methods primarily depend on subjective human assessment,\nlimiting scalability, transparency, and effectiveness. This research proposes a\nsystematic methodology using deterministic and Large Language Model\n(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN\nAI reliability. We construct two parallel KGs: (i) a deterministic KG built\nusing explicit rule-based methods, predefined ontologies, domain-specific\ndictionaries, and structured entity-relation extraction rules, and (ii) an\nLLM-generated KG dynamically derived from real-time textual data streams such\nas live news articles. Utilizing real-time news streams ensures authenticity,\nmitigates biases from repetitive training, and prevents adaptive LLMs from\nbypassing predefined benchmarks through feedback memorization. To quantify\nstructural deviations and semantic discrepancies, we employ several established\nKG metrics, including Instantiated Class Ratio (ICR), Instantiated Property\nRatio (IPR), and Class Instantiation (CI). An automated real-time monitoring\nframework continuously computes deviations between deterministic and\nLLM-generated KGs. By establishing dynamic anomaly thresholds based on\nhistorical structural metric distributions, our method proactively identifies\nand flags significant deviations, thus promptly detecting semantic anomalies or\nhallucinations. This structured, metric-driven comparison between deterministic\nand dynamically generated KGs delivers a robust and scalable evaluation\nframework.", "AI": {"tldr": "本研究提出了一种使用确定性知识图谱（KG）和LLM生成知识图谱的系统方法，通过持续比较两者来实时监测和评估生成式AI（GEN AI）的可靠性，以检测幻觉和语义偏差。", "motivation": "生成式AI模型存在可靠性问题，如幻觉、语义漂移和固有偏见，且通常是黑箱操作，难以透明评估。现有评估方法主要依赖主观人工评估，缺乏可扩展性、透明度和效率。", "method": "研究构建了两个并行的知识图谱：1) 确定性KG，通过显式规则、预定义本体和结构化实体关系提取构建；2) LLM生成KG，从实时文本数据流（如实时新闻文章）动态生成。通过使用Instantiated Class Ratio (ICR)、Instantiated Property Ratio (IPR) 和 Class Instantiation (CI) 等KG指标，量化两个KG之间的结构偏差和语义差异。一个自动化实时监控框架持续计算这些偏差，并基于历史结构指标分布建立动态异常阈值，以主动识别和标记显著偏差。", "result": "该方法能够持续计算确定性KG与LLM生成KG之间的偏差，并通过动态异常阈值主动识别并标记显著偏差，从而及时检测语义异常或幻觉。", "conclusion": "通过确定性KG与动态生成KG之间的结构化、指标驱动比较，本研究提供了一个鲁棒且可扩展的生成式AI评估框架。"}}
{"id": "2509.04413", "categories": ["eess.SY", "cs.LG", "cs.MA", "cs.RO", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.04413", "abs": "https://arxiv.org/abs/2509.04413", "authors": ["Babak Esmaeili", "Hamidreza Modares"], "title": "SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety Certificates", "comment": "Submitted to IEEE Transactions on Automation Science and Engineering", "summary": "This paper proposes a fully data-driven motion-planning framework for\nhomogeneous linear multi-agent systems that operate in shared, obstacle-filled\nworkspaces without access to explicit system models. Each agent independently\nlearns its closed-loop behavior from experimental data by solving convex\nsemidefinite programs that generate locally invariant ellipsoids and\ncorresponding state-feedback gains. These ellipsoids, centered along grid-based\nwaypoints, certify the dynamic feasibility of short-range transitions and\ndefine safe regions of operation. A sampling-based planner constructs a tree of\nsuch waypoints, where transitions are allowed only when adjacent ellipsoids\noverlap, ensuring invariant-to-invariant transitions and continuous safety. All\nagents expand their trees simultaneously and are coordinated through a\nspace-time reservation table that guarantees inter-agent safety by preventing\nsimultaneous occupancy and head-on collisions. Each successful edge in the tree\nis equipped with its own local controller, enabling execution without\nre-solving optimization problems at runtime. The resulting trajectories are not\nonly dynamically feasible but also provably safe with respect to both\nenvironmental constraints and inter-agent collisions. Simulation results\ndemonstrate the effectiveness of the approach in synthesizing synchronized,\nsafe trajectories for multiple agents under shared dynamics and constraints,\nusing only data and convex optimization tools.", "AI": {"tldr": "本文提出了一种完全数据驱动的多智能体运动规划框架，适用于无显式系统模型的同构线性多智能体系统。该框架通过凸半定规划从实验数据中学习局部不变椭球和状态反馈增益，利用这些椭球在基于网格的路径点之间规划动态可行且可证明安全的轨迹。通过采样式规划器和时空预留表，确保了智能体间的连续安全性和无碰撞协调。", "motivation": "研究动机是为在共享、充满障碍的工作空间中运行的同构线性多智能体系统提供运动规划解决方案，尤其是在缺乏明确系统模型的情况下。目标是生成既动态可行又对环境和智能体间碰撞都可证明安全的轨迹。", "method": "每个智能体独立地从实验数据中学习其闭环行为，通过求解凸半定规划生成局部不变椭球和相应的状态反馈增益。这些椭球以网格路径点为中心，验证短程转换的动态可行性并定义安全操作区域。一个基于采样的规划器构建路径点树，仅当相邻椭球重叠时才允许转换，确保不变到不变的转换和连续安全性。所有智能体同时扩展其树，并通过时空预留表进行协调，以防止同时占用和正面碰撞，从而保证智能体间安全。树中的每个成功边都配备了自己的局部控制器，无需在运行时重新解决优化问题。", "result": "该方法能够为多个智能体合成同步、安全的轨迹。生成的轨迹不仅动态可行，而且对环境约束和智能体间碰撞都可证明安全。仿真结果表明，该方法在使用纯数据和凸优化工具的情况下，在共享动态和约束条件下，能够有效地实现多智能体的同步安全轨迹。", "conclusion": "该研究成功地提出了一种完全数据驱动的多智能体运动规划框架，在没有显式系统模型的情况下，通过学习不变椭球和采用协调规划策略，实现了在障碍环境中动态可行且对环境和智能体间碰撞都可证明安全的轨迹。该方法通过预先计算的局部控制器，实现了高效的运行时执行。"}}
{"id": "2509.03893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03893", "abs": "https://arxiv.org/abs/2509.03893", "authors": ["Stefan Stojanov", "Linan Zhao", "Yunzhi Zhang", "Daniel L. K. Yamins", "Jiajun Wu"], "title": "Weakly-Supervised Learning of Dense Functional Correspondences", "comment": "Accepted at ICCV 2025. Project website:\n  https://dense-functional-correspondence.github.io/", "summary": "Establishing dense correspondences across image pairs is essential for tasks\nsuch as shape reconstruction and robot manipulation. In the challenging setting\nof matching across different categories, the function of an object, i.e., the\neffect that an object can cause on other objects, can guide how correspondences\nshould be established. This is because object parts that enable specific\nfunctions often share similarities in shape and appearance. We derive the\ndefinition of dense functional correspondence based on this observation and\npropose a weakly-supervised learning paradigm to tackle the prediction task.\nThe main insight behind our approach is that we can leverage vision-language\nmodels to pseudo-label multi-view images to obtain functional parts. We then\nintegrate this with dense contrastive learning from pixel correspondences to\ndistill both functional and spatial knowledge into a new model that can\nestablish dense functional correspondence. Further, we curate synthetic and\nreal evaluation datasets as task benchmarks. Our results demonstrate the\nadvantages of our approach over baseline solutions consisting of off-the-shelf\nself-supervised image representations and grounded vision language models.", "AI": {"tldr": "本文提出了一种弱监督学习范式，利用视觉-语言模型生成功能部件的伪标签，并结合像素级对比学习，以建立跨类别图像之间的密集功能对应关系，并在合成和真实数据集上取得了优于基线方法的性能。", "motivation": "在图像对之间建立密集对应关系对于形状重建和机器人操作等任务至关重要。在具有挑战性的跨类别匹配场景中，物体的功能（即物体对其他物体产生的影响）可以指导如何建立对应关系，因为实现特定功能的物体部件通常在形状和外观上具有相似性。", "method": "基于上述观察，作者推导了密集功能对应关系的定义，并提出了一个弱监督学习范式。该方法的核心思想是利用视觉-语言模型对多视图图像进行伪标注，以获取功能部件。随后，将此与来自像素对应关系的密集对比学习相结合，将功能和空间知识提炼到一个新模型中，从而建立密集功能对应关系。此外，还构建了合成和真实评估数据集作为任务基准。", "result": "实验结果表明，与由现成的自监督图像表示和接地视觉-语言模型组成的基线解决方案相比，本文提出的方法具有显著优势。", "conclusion": "本文提出了一种通过结合视觉-语言模型伪标签和密集对比学习来建立密集功能对应关系的新方法，该方法在跨类别匹配任务中表现出色，并优于现有基线解决方案。"}}
{"id": "2509.04443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04443", "abs": "https://arxiv.org/abs/2509.04443", "authors": ["Lawrence Y. Zhu", "Pranav Kuppili", "Ryan Punamiya", "Patcharapong Aphiwetsa", "Dhruv Patel", "Simar Kareer", "Sehoon Ha", "Danfei Xu"], "title": "EMMA: Scaling Mobile Manipulation via Egocentric Human Data", "comment": null, "summary": "Scaling mobile manipulation imitation learning is bottlenecked by expensive\nmobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),\nan end-to-end framework training mobile manipulation policies from human mobile\nmanipulation data with static robot data, sidestepping mobile teleoperation. To\naccomplish this, we co-train human full-body motion data with static robot\ndata. In our experiments across three real-world tasks, EMMA demonstrates\ncomparable performance to baselines trained on teleoperated mobile robot data\n(Mobile ALOHA), achieving higher or equivalent task performance in full task\nsuccess. We find that EMMA is able to generalize to new spatial configurations\nand scenes, and we observe positive performance scaling as we increase the\nhours of human data, opening new avenues for scalable robotic learning in\nreal-world environments. Details of this project can be found at\nhttps://ego-moma.github.io/.", "AI": {"tldr": "EMMA是一个端到端框架，通过联合训练人类全身运动数据和静态机器人数据，实现了可扩展的移动操纵模仿学习，避免了昂贵的移动机器人遥操作。", "motivation": "移动操纵模仿学习受限于移动机器人遥操作成本高昂这一瓶颈。", "method": "提出了Egocentric Mobile MAnipulation (EMMA)框架。它通过协同训练人类全身运动数据和静态机器人数据，来训练移动操纵策略，从而避免了移动遥操作。", "result": "在三个真实世界任务中，EMMA表现出与基于遥操作移动机器人数据训练的基线（如Mobile ALOHA）相当的性能，在完整任务成功率上达到更高或同等水平。EMMA能够泛化到新的空间配置和场景，并且随着人类数据量的增加，性能呈现正向扩展。", "conclusion": "EMMA为真实世界环境中的可扩展机器人学习开辟了新途径，通过利用人类数据有效解决了移动操纵模仿学习中遥操作成本高昂的问题。"}}
{"id": "2509.03809", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03809", "abs": "https://arxiv.org/abs/2509.03809", "authors": ["Jiaxin Guo", "Daimeng Wei", "Yuanchang Luo", "Xiaoyu Chen", "Zhanglin Wu", "Huan Yang", "Hengchao Shang", "Zongyao Li", "Zhiqiang Rao", "Jinlong Yang", "Hao Yang"], "title": "Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation", "comment": "under preview", "summary": "Large language models (LLMs) have ushered in a new era for document-level\nmachine translation (\\textit{doc}-mt), yet their whole-document outputs\nchallenge existing evaluation methods that assume sentence-by-sentence\nalignment. We introduce \\textit{\\textbf{Align-then-Slide}}, a complete\nevaluation framework for ultra-long doc-mt. In the Align stage, we\nautomatically infer sentence-level source-target correspondences and rebuild\nthe target to match the source sentence number, resolving omissions and\nmany-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we\ncalculate averaged metric scores under 1-, 2-, 3- and 4-chunk for\nmulti-granularity assessment. Experiments on the WMT benchmark show a Pearson\ncorrelation of 0.929 between our method with expert MQM rankings. On a newly\ncurated real-world test set, our method again aligns closely with human\njudgments. Furthermore, preference data produced by Align-then-Slide enables\neffective CPO training and its direct use as a reward model for GRPO, both\nyielding translations preferred over a vanilla SFT baseline. The results\nvalidate our framework as an accurate, robust, and actionable evaluation tool\nfor doc-mt systems.", "AI": {"tldr": "本文提出了一种名为“先对齐后滑动”（Align-then-Slide）的文档级机器翻译（doc-mt）评估框架，旨在解决大型语言模型（LLMs）生成整文档输出时现有评估方法面临的对齐挑战，并经验证其准确性和实用性。", "motivation": "大型语言模型（LLMs）在文档级机器翻译（doc-mt）方面开创了新纪元，但其整文档输出使得依赖逐句对齐的现有评估方法面临挑战，尤其是在处理句子遗漏、多对一或一对多映射时。", "method": "本文提出了“先对齐后滑动”（Align-then-Slide）评估框架：\n1.  **对齐阶段（Align stage）**：自动推断句子级别的源-目标对应关系，并重建目标文本以匹配源文本的句子数量，从而解决遗漏以及多对一/一对多映射问题。\n2.  **N块滑动评估阶段（n-Chunk Sliding Evaluate stage）**：在1、2、3和4块粒度下计算平均度量分数，以实现多粒度评估。", "result": "1.  在WMT基准测试上，该方法与专家MQM排名之间的皮尔逊相关系数高达0.929。\n2.  在一个新策展的真实世界测试集上，该方法的结果也与人类判断高度一致。\n3.  “先对齐后滑动”生成的偏好数据能够有效支持CPO训练，并可直接用作GRPO的奖励模型，两者生成的译文均优于香草SFT基线。", "conclusion": "研究结果验证了“先对齐后滑动”框架是文档级机器翻译系统一个准确、鲁棒且可操作的评估工具。"}}
{"id": "2509.03863", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03863", "abs": "https://arxiv.org/abs/2509.03863", "authors": ["Sina Khajehabdollahi", "Gautier Hamon", "Marko Cvjetko", "Pierre-Yves Oudeyer", "Clément Moulin-Frier", "Cédric Colas"], "title": "Expedition & Expansion: Leveraging Semantic Representations for Goal-Directed Exploration in Continuous Cellular Automata", "comment": null, "summary": "Discovering diverse visual patterns in continuous cellular automata (CA) is\nchallenging due to the vastness and redundancy of high-dimensional behavioral\nspaces. Traditional exploration methods like Novelty Search (NS) expand locally\nby mutating known novel solutions but often plateau when local novelty is\nexhausted, failing to reach distant, unexplored regions. We introduce\nExpedition and Expansion (E&E), a hybrid strategy where exploration alternates\nbetween local novelty-driven expansions and goal-directed expeditions. During\nexpeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic\ngoals--descriptions of interesting but hypothetical patterns that drive\nexploration toward uncharted regions. By operating in semantic spaces that\nalign with human perception, E&E both evaluates novelty and generates goals in\nconceptually meaningful ways, enhancing the interpretability and relevance of\ndiscovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,\nemergent behaviors, E&E consistently uncovers more diverse solutions than\nexisting exploration methods. A genealogical analysis further reveals that\nsolutions originating from expeditions disproportionately influence long-term\nexploration, unlocking new behavioral niches that serve as stepping stones for\nsubsequent search. These findings highlight E&E's capacity to break through\nlocal novelty boundaries and explore behavioral landscapes in human-aligned,\ninterpretable ways, offering a promising template for open-ended exploration in\nartificial life and beyond.", "AI": {"tldr": "本文提出了一种名为“远征与扩展”（E&E）的混合探索策略，用于发现连续元胞自动机中多样化的视觉模式。E&E结合了局部新颖性驱动的扩展和由视觉-语言模型（VLM）引导的目标导向远征，以克服传统方法陷入局部最优的问题，并在人类感知的语义空间中进行探索。", "motivation": "在连续元胞自动机（CA）中发现多样化的视觉模式具有挑战性，因为高维行为空间巨大且冗余。传统的探索方法（如新颖性搜索）通过变异已知新颖解进行局部扩展，但当局部新颖性耗尽时，往往会停滞不前，无法到达遥远、未探索的区域。", "method": "E&E是一种混合策略，探索在局部新颖性驱动的扩展和目标导向的远征之间交替进行。在远征期间，E&E利用视觉-语言模型（VLM）生成语言目标（即有趣但假设的模式描述），从而将探索引向未知的区域。通过在与人类感知对齐的语义空间中操作，E&E以概念上有意义的方式评估新颖性并生成目标。", "result": "在以其丰富的涌现行为而闻名的Flow Lenia（一种连续CA）上进行测试，E&E始终比现有探索方法发现更多样化的解决方案。家谱分析进一步揭示，源自远征的解决方案对长期探索产生了不成比例的影响，解锁了新的行为生态位，为后续搜索提供了垫脚石。", "conclusion": "这些发现突出了E&E突破局部新颖性界限并以与人类对齐、可解释的方式探索行为景观的能力。E&E为人工生命及其他领域的开放式探索提供了一个有前景的模板。"}}
{"id": "2509.03953", "categories": ["cs.AI", "cs.SC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.03953", "abs": "https://arxiv.org/abs/2509.03953", "authors": ["Ángel Aso-Mollar", "Diego Aineto", "Enrico Scala", "Eva Onaindia"], "title": "Handling Infinite Domain Parameters in Planning Through Best-First Search with Delayed Partial Expansions", "comment": "To appear in the Proceedings of the Thirty-Fourth International Joint\n  Conference on Artificial Intelligence (IJCAI 2025)", "summary": "In automated planning, control parameters extend standard action\nrepresentations through the introduction of continuous numeric decision\nvariables. Existing state-of-the-art approaches have primarily handled control\nparameters as embedded constraints alongside other temporal and numeric\nrestrictions, and thus have implicitly treated them as additional constraints\nrather than as decision points in the search space. In this paper, we propose\nan efficient alternative that explicitly handles control parameters as true\ndecision points within a systematic search scheme. We develop a best-first,\nheuristic search algorithm that operates over infinite decision spaces defined\nby control parameters and prove a notion of completeness in the limit under\ncertain conditions. Our algorithm leverages the concept of delayed partial\nexpansion, where a state is not fully expanded but instead incrementally\nexpands a subset of its successors. Our results demonstrate that this novel\nsearch algorithm is a competitive alternative to existing approaches for\nsolving planning problems involving control parameters.", "AI": {"tldr": "本文提出了一种新的启发式搜索算法，用于处理包含连续控制参数的自动化规划问题，该算法将控制参数明确视为决策点而非约束，并证明了其在特定条件下的完备性。", "motivation": "现有方法主要将控制参数作为嵌入式约束处理，隐式地将其视为额外的限制而非搜索空间中的决策点。这促使研究者开发一种更有效的方法，明确地将控制参数视为真正的决策点。", "method": "研究人员开发了一种最佳优先的启发式搜索算法，该算法在由控制参数定义的无限决策空间中操作。该算法利用“延迟部分扩展”的概念，即状态不是完全扩展，而是增量地扩展其一部分后继节点。同时，该方法在特定条件下证明了其在极限情况下的完备性。", "result": "研究结果表明，这种新颖的搜索算法在解决涉及控制参数的规划问题时，是现有方法的一个有竞争力的替代方案。", "conclusion": "通过将控制参数明确地视为决策点并采用延迟部分扩展的启发式搜索算法，本文提供了一种有效且具有竞争力的解决方案，用于处理包含连续数值决策变量的自动化规划问题。"}}
{"id": "2509.03895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03895", "abs": "https://arxiv.org/abs/2509.03895", "authors": ["Phuoc-Nguyen Bui", "Khanh-Binh Nguyen", "Hyunseung Choo"], "title": "Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model", "comment": "ICCV 2025 - LIMIT Workshop", "summary": "Contrastive vision-language models excel in zero-shot image recognition but\nface challenges in few-shot scenarios due to computationally intensive offline\nfine-tuning using prompt learning, which risks overfitting. To overcome these\nlimitations, we propose Attn-Adapter, a novel online few-shot learning\nframework that enhances CLIP's adaptability via a dual attention mechanism. Our\ndesign incorporates dataset-specific information through two components: the\nMemory Attn-Adapter, which refines category embeddings using support examples,\nand the Local-Global Attn-Adapter, which enriches image embeddings by\nintegrating local and global features. This architecture enables dynamic\nadaptation from a few labeled samples without retraining the base model.\nAttn-Adapter outperforms state-of-the-art methods in cross-category and\ncross-dataset generalization, maintaining efficient inference and scaling\nacross CLIP backbones.", "AI": {"tldr": "本文提出了Attn-Adapter，一个新颖的在线少样本学习框架，通过双注意力机制增强了CLIP模型的适应性，无需重新训练基础模型即可实现高效的少样本泛化。", "motivation": "对比视觉-语言模型在零样本图像识别中表现出色，但在少样本场景中面临挑战，因为传统的提示学习（prompt learning）需要计算密集型离线微调，且容易过拟合。", "method": "Attn-Adapter是一个在线少样本学习框架，通过双注意力机制提升CLIP的适应性。它包含两个主要组件：1) Memory Attn-Adapter：利用支持样本细化类别嵌入；2) Local-Global Attn-Adapter：通过整合局部和全局特征来丰富图像嵌入。该架构能够在不重新训练基础模型的情况下，仅凭少量标记样本实现动态适应。", "result": "Attn-Adapter在跨类别和跨数据集泛化方面均优于现有最先进的方法，同时保持了高效的推理，并能良好地扩展到不同的CLIP骨干网络。", "conclusion": "Attn-Adapter成功克服了CLIP在少样本学习中的局限性，提供了一种高效、无需重新训练的在线适应方案，显著提升了模型在复杂少样本场景下的泛化能力和效率。"}}
{"id": "2509.04156", "categories": ["cs.CV", "cs.AI", "cs.RO", "68T07, 68T45, 68U10, 68T40", "I.2.10; I.4.8; I.5.4; I.2.9"], "pdf": "https://arxiv.org/pdf/2509.04156", "abs": "https://arxiv.org/abs/2509.04156", "authors": ["Serhii Svystun", "Pavlo Radiuk", "Oleksandr Melnychenko", "Oleg Savenko", "Anatoliy Sachenko"], "title": "YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components", "comment": "The 13th IEEE International Conference on Intelligent Data\n  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6\n  September, 2025, Gliwice, Poland", "summary": "Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up\nnew opportunities for monitoring wind power plants, including blades, towers,\nand other critical components. However, reliable defect detection requires\nhigh-resolution data and efficient methods to process multispectral imagery. In\nthis research, we aim to enhance defect detection accuracy through the\ndevelopment of an ensemble of YOLO-based deep learning models that integrate\nboth visible and thermal channels. We propose an ensemble approach that\nintegrates a general-purpose YOLOv8 model with a specialized thermal model,\nusing a sophisticated bounding box fusion algorithm to combine their\npredictions. Our experiments show this approach achieves a mean Average\nPrecision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone\nYOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that\ncombining multiple YOLO architectures with fused multispectral data provides a\nmore reliable solution, improving the detection of both visual and thermal\ndefects.", "AI": {"tldr": "该研究开发了一种基于YOLO模型集成（结合可见光和热成像数据）的方法，以提高无人机在风力发电厂缺陷检测中的准确性，并取得了优于单一模型的性能。", "motivation": "无人机为风力发电厂（包括叶片、塔架和其他关键部件）的监测提供了新机遇。然而，可靠的缺陷检测需要高分辨率数据和有效的多光谱图像处理方法。本研究旨在通过开发基于YOLO深度学习模型的集成方案来提高缺陷检测的准确性。", "method": "研究提出了一种集成YOLOv8通用模型和专用热成像模型的集成方法。该方法整合了可见光和热成像通道，并使用复杂的边界框融合算法来合并两者的预测。", "result": "实验结果显示，该集成方法在mAP@.5上达到了0.93，F1-score为0.90，优于单独的YOLOv8模型（mAP@.5为0.91）。", "conclusion": "结合多个YOLO架构和融合多光谱数据提供了一种更可靠的解决方案，显著提高了对视觉和热缺陷的检测能力。"}}
{"id": "2509.03829", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03829", "abs": "https://arxiv.org/abs/2509.03829", "authors": ["Huhong Xian", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "title": "NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation", "comment": null, "summary": "Different from traditional sentence-level audio deepfake detection (ADD),\npartial audio deepfake detection (PADD) requires frame-level positioning of the\nlocation of fake speech. While some progress has been made in this area,\nleveraging semantic information from audio, especially named entities, remains\nan underexplored aspect. To this end, we propose NE-PADD, a novel method for\nPartial Audio Deepfake Detection (PADD) that leverages named entity knowledge\nthrough two parallel branches: Speech Name Entity Recognition (SpeechNER) and\nPADD. The approach incorporates two attention aggregation mechanisms: Attention\nFusion (AF) for combining attention weights and Attention Transfer (AT) for\nguiding PADD with named entity semantics using an auxiliary loss. Built on the\nPartialSpoof-NER dataset, experiments show our method outperforms existing\nbaselines, proving the effectiveness of integrating named entity knowledge in\nPADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.", "AI": {"tldr": "本文提出了一种名为NE-PADD的新方法，用于局部音频深度伪造检测（PADD），通过结合命名实体知识和两种注意力聚合机制，实现了对伪造语音的帧级定位，并超越了现有基线。", "motivation": "传统的音频深度伪造检测（ADD）是句子级别的，而局部音频深度伪造检测（PADD）需要帧级定位伪造语音。在PADD领域，利用音频中的语义信息，特别是命名实体，仍然是一个未被充分探索的方面。", "method": "NE-PADD方法通过两个并行分支（语音命名实体识别SpeechNER和PADD）来利用命名实体知识。它结合了两种注意力聚合机制：注意力融合（AF）用于组合注意力权重，以及注意力转移（AT）通过辅助损失利用命名实体语义指导PADD。该方法在PartialSpoof-NER数据集上进行构建和评估。", "result": "实验结果表明，NE-PADD方法优于现有基线，证明了将命名实体知识整合到PADD中的有效性。", "conclusion": "将命名实体知识有效地整合到局部音频深度伪造检测（PADD）中，可以显著提升检测性能，实现伪造语音的精确帧级定位。"}}
{"id": "2509.03890", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03890", "abs": "https://arxiv.org/abs/2509.03890", "authors": ["Yineng Yan", "Xidong Wang", "Jin Seng Cheng", "Ran Hu", "Wentao Guan", "Nahid Farahmand", "Hengte Lin", "Yue Li"], "title": "FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer Marketplace", "comment": null, "summary": "The emergence of agentic AI, powered by Large Language Models (LLMs), marks a\nparadigm shift from reactive generative systems to proactive, goal-oriented\nautonomous agents capable of sophisticated planning, memory, and tool use. This\nevolution presents a novel opportunity to address long-standing challenges in\ncomplex digital environments. Core tasks on Consumer-to-Consumer (C2C)\ne-commerce platforms often require users to navigate complex Graphical User\nInterfaces (GUIs), making the experience time-consuming for both buyers and\nsellers. This paper introduces a novel approach to simplify these interactions\nthrough an LLM-powered agentic assistant. This agent functions as a new,\nconversational entry point to the marketplace, shifting the primary interaction\nmodel from a complex GUI to an intuitive AI agent. By interpreting natural\nlanguage commands, the agent automates key high-friction workflows. For\nsellers, this includes simplified updating and renewal of listings, and the\nability to send bulk messages. For buyers, the agent facilitates a more\nefficient product discovery process through conversational search. We present\nthe architecture for Facebook Marketplace Assistant (FaMA), arguing that this\nagentic, conversational paradigm provides a lightweight and more accessible\nalternative to traditional app interfaces, allowing users to manage their\nmarketplace activities with greater efficiency. Experiments show FaMA achieves\na 98% task success rate on solving complex tasks on the marketplace and enables\nup to a 2x speedup on interaction time.", "AI": {"tldr": "本文介绍了一种基于LLM的智能代理助手FaMA，通过对话式交互简化C2C电商平台（如Facebook Marketplace）的复杂GUI操作，显著提高了任务成功率和交互效率。", "motivation": "C2C电商平台上的核心任务通常需要用户操作复杂的图形用户界面（GUI），这使得买家和卖家的体验都非常耗时。代理式AI的兴起为解决数字环境中的长期挑战提供了机会。", "method": "论文提出了一种新方法，通过LLM驱动的代理助手FaMA来简化交互。该助手作为市场的新型对话式入口，将主要交互模式从复杂GUI转变为直观的AI代理。它通过解释自然语言命令来自动化高摩擦工作流程，例如简化卖家列表更新、批量消息发送，以及为买家提供对话式搜索以实现高效产品发现。", "result": "实验表明，FaMA在解决市场上的复杂任务时达到了98%的任务成功率，并将交互时间加快了多达2倍。", "conclusion": "基于LLM的代理式对话范式为传统应用界面提供了一种轻量级且更易于访问的替代方案，使用户能够更高效地管理其市场活动。"}}
{"id": "2509.03897", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03897", "abs": "https://arxiv.org/abs/2509.03897", "authors": ["Xiaofu Chen", "Israfel Salazar", "Yova Kementchedjhieva"], "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation", "comment": null, "summary": "As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS.", "AI": {"tldr": "本文提出了SPECS（Specificity-Enhanced CLIPScore），一种针对长图像描述的无参考RS（表征相似性）评估指标。SPECS通过修改CLIP模型，强调描述的特异性，使其在与人类判断的相关性上与基于LLM的指标相当，但在效率上远超后者，适用于模型开发中的迭代评估。", "motivation": "随着生成长而详细图像描述的需求增加，现有评估指标面临挑战：N-gram指标缺乏语义准确性；传统RS指标计算成本高且与人类判断相关性低；基于LLM的指标虽然相关性强，但计算成本过高，不适用于模型开发中的迭代使用。因此，需要一种既能准确评估长描述，又高效的指标。", "method": "本文引入了SPECS，这是一种无参考的RS指标，专为长图像描述设计。SPECS通过修改CLIP模型，引入了一个新的目标函数，该函数强调描述的特异性：奖励正确的细节，并惩罚不正确的细节。", "result": "SPECS在与人类判断的相关性方面，与开源的基于LLM的指标表现相当，但效率更高。", "conclusion": "SPECS提供了一个实用且高效的替代方案，可用于图像描述模型开发过程中的迭代检查点评估。"}}
{"id": "2509.03867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03867", "abs": "https://arxiv.org/abs/2509.03867", "authors": ["Yang Wang", "Chenghao Xiao", "Chia-Yi Hsiao", "Zi Yan Chang", "Chi-Li Chen", "Tyler Loakman", "Chenghua Lin"], "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth", "comment": "Accepted for oral presentation at the EMNLP 2025 Main Conference", "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a small but diverse benchmark dataset of over 1,200 meticulously\ncurated examples, with select instances in English, Mandarin, Spanish, French,\nJapanese, and Korean. Annotation was especially challenging: each of the\nexamples required careful expert review to verify that it truly reflected\nDrivelological characteristics. The process involved multiple rounds of\ndiscussion and adjudication to address disagreements, highlighting the subtle\nand subjective nature of the Drivelology. We evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss the implied rhetorical function\naltogether. These findings highlight a deeper representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.", "AI": {"tldr": "本文引入了“Drivelology”这一语言现象，即“有深度的胡言乱语”，并发现大型语言模型（LLMs）无法理解其深层语义。研究构建了一个多语言基准数据集，评估了LLMs在分类、生成和推理任务上的表现，结果表明LLMs在语用理解上存在明显局限。", "motivation": "尽管LLMs在许多NLP任务中表现出色，但它们未能理解“Drivelology”这种表面上无意义但却包含隐式含义的语言现象。研究旨在探究LLMs在处理这种需要上下文推断、道德推理或情感解释的复杂语义时的局限性。", "method": "研究构建了一个包含1200多个精心策划的多语言（英、普、西、法、日、韩）Drivelology示例的基准数据集。数据集的标注经过专家多轮审查和裁定。随后，研究评估了一系列LLMs在Drivelology的分类、生成和推理任务上的表现。", "result": "评估结果揭示了LLMs的明显局限性：模型经常将Drivelology与浅层无意义内容混淆，给出不连贯的解释，或完全未能理解其隐含的修辞功能。", "conclusion": "这些发现表明LLMs在语用理解方面存在深层的表征差距，并挑战了“统计流畅性等同于认知理解”的假设。研究发布了数据集和代码，以促进对超越表面连贯性的语言深度建模的进一步研究。"}}
{"id": "2509.03906", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03906", "abs": "https://arxiv.org/abs/2509.03906", "authors": ["Qika Lin", "Yifan Zhu", "Bin Pu", "Ling Huang", "Haoran Luo", "Jingying Ma", "Zhen Peng", "Tianzhe Zhao", "Fangzhi Xu", "Jian Zhang", "Kai He", "Zhonghong Ou", "Swapnil Mishra", "Mengling Feng"], "title": "A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning", "comment": "15 pages", "summary": "Medical foundation models (FMs) have shown tremendous promise amid the rapid\nadvancements in artificial intelligence (AI) technologies. However, current\nmedical FMs typically generate answers in a black-box manner, lacking\ntransparent reasoning processes and locally grounded interpretability, which\nhinders their practical clinical deployments. To this end, we introduce\nDeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It\nleverages a sequential training pipeline: initially fine-tuned on curated CXR\ninstruction data to equip with fundamental CXR interpretation capabilities,\nthen exposed to high-quality synthetic reasoning samples to enable cold-start\nreasoning, and finally refined via online reinforcement learning to enhance\nboth grounded reasoning quality and generation performance. Thus, the model\nproduces both an answer and reasoning steps tied to the image's local regions\nfor each query. Quantitative evaluation demonstrates substantial improvements\nin report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and\nvisual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)\ntasks. To facilitate robust assessment, we propose Report Arena, a benchmarking\nframework using advanced language models to evaluate answer quality, further\nhighlighting the superiority of DeepMedix-R1. Expert review of generated\nreasoning steps reveals greater interpretability and clinical plausibility\ncompared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall\npreference). Collectively, our work advances medical FM development toward\nholistic, transparent, and clinically actionable modeling for CXR\ninterpretation.", "AI": {"tldr": "DeepMedix-R1是一个用于胸部X光（CXR）解释的全面医疗基础模型，通过顺序训练（指令微调、合成推理、在线强化学习）提供透明且局部关联的推理过程，显著提升了报告生成和视觉问答任务的性能，并增强了可解释性和临床合理性。", "motivation": "当前的医疗基础模型通常以“黑箱”方式生成答案，缺乏透明的推理过程和局部关联的可解释性，这阻碍了它们在临床中的实际部署。", "method": "DeepMedix-R1采用顺序训练流程：首先在精选的CXR指令数据上进行微调以获得基本解释能力；然后通过高质量的合成推理样本实现冷启动推理；最后通过在线强化学习进行优化，以提高局部关联推理质量和生成性能。该模型为每个查询生成答案和与图像局部区域相关的推理步骤。此外，提出Report Arena基准框架，使用高级语言模型评估答案质量。", "result": "定量评估显示，DeepMedix-R1在报告生成任务上比LLaVA-Rad和MedGemma分别提高了14.54%和31.32%；在视觉问答任务上比MedGemma和CheXagent分别提高了57.75%和23.06%。Report Arena基准测试进一步突出了其优越性。专家对生成推理步骤的审查表明，其可解释性和临床合理性优于Qwen2.5-VL-7B模型（总体偏好为0.7416 vs. 0.2584）。", "conclusion": "本工作推动了医疗基础模型的发展，使其在CXR解释方面实现全面、透明和具有临床可操作性的建模。"}}
{"id": "2509.03903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03903", "abs": "https://arxiv.org/abs/2509.03903", "authors": ["Yuanfeng Ji", "Dan Lin", "Xiyue Wang", "Lu Zhang", "Wenhui Zhou", "Chongjian Ge", "Ruihang Chu", "Xiaoli Yang", "Junhan Zhao", "Junsong Chen", "Xiangde Luo", "Sen Yang", "Jin Fang", "Ping Luo", "Ruijiang Li"], "title": "A Generative Foundation Model for Chest Radiography", "comment": null, "summary": "The scarcity of well-annotated diverse medical images is a major hurdle for\ndeveloping reliable AI models in healthcare. Substantial technical advances\nhave been made in generative foundation models for natural images. Here we\ndevelop `ChexGen', a generative vision-language foundation model that\nintroduces a unified framework for text-, mask-, and bounding box-guided\nsynthesis of chest radiographs. Built upon the latent diffusion transformer\narchitecture, ChexGen was pretrained on the largest curated chest X-ray dataset\nto date, consisting of 960,000 radiograph-report pairs. ChexGen achieves\naccurate synthesis of radiographs through expert evaluations and quantitative\nmetrics. We demonstrate the utility of ChexGen for training data augmentation\nand supervised pretraining, which led to performance improvements across\ndisease classification, detection, and segmentation tasks using a small\nfraction of training data. Further, our model enables the creation of diverse\npatient cohorts that enhance model fairness by detecting and mitigating\ndemographic biases. Our study supports the transformative role of generative\nfoundation models in building more accurate, data-efficient, and equitable\nmedical AI systems.", "AI": {"tldr": "ChexGen是一个生成式视觉-语言基础模型，用于胸部X光片的文本、掩码和边界框引导合成。它通过数据增强和监督预训练提高了医疗AI模型的性能和公平性。", "motivation": "高质量、多样化的医学图像标注数据稀缺，严重阻碍了医疗领域可靠AI模型的发展。", "method": "开发了名为“ChexGen”的生成式视觉-语言基础模型，采用统一框架实现文本、掩码和边界框引导的胸部X光片合成。该模型基于潜在扩散变换器架构，并使用迄今为止最大的胸部X光片数据集（包含960,000对放射报告）进行预训练。", "result": "ChexGen通过专家评估和定量指标实现了准确的X光片合成。通过训练数据增强和监督预训练，该模型在疾病分类、检测和分割任务上，即使使用少量训练数据，也能显著提升性能。此外，它能创建多样化的患者队列，通过检测和缓解人口偏见来增强模型公平性。", "conclusion": "生成式基础模型在构建更准确、数据高效和公平的医疗AI系统方面具有变革性作用。"}}
{"id": "2509.03871", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03871", "abs": "https://arxiv.org/abs/2509.03871", "authors": ["Yanbo Wang", "Yongcan Yu", "Jian Liang", "Ran He"], "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models", "comment": "38 pages. This survey considers papers published up to June 30, 2025.\n  Work in progress", "summary": "The development of Long-CoT reasoning has advanced LLM performance across\nvarious tasks, including language understanding, complex problem solving, and\ncode generation. This paradigm enables models to generate intermediate\nreasoning steps, thereby improving both accuracy and interpretability. However,\ndespite these advancements, a comprehensive understanding of how CoT-based\nreasoning affects the trustworthiness of language models remains\nunderdeveloped. In this paper, we survey recent work on reasoning models and\nCoT techniques, focusing on five core dimensions of trustworthy reasoning:\ntruthfulness, safety, robustness, fairness, and privacy. For each aspect, we\nprovide a clear and structured overview of recent studies in chronological\norder, along with detailed analyses of their methodologies, findings, and\nlimitations. Future research directions are also appended at the end for\nreference and discussion. Overall, while reasoning techniques hold promise for\nenhancing model trustworthiness through hallucination mitigation, harmful\ncontent detection, and robustness improvement, cutting-edge reasoning models\nthemselves often suffer from comparable or even greater vulnerabilities in\nsafety, robustness, and privacy. By synthesizing these insights, we hope this\nwork serves as a valuable and timely resource for the AI safety community to\nstay informed on the latest progress in reasoning trustworthiness. A full list\nof related papers can be found at\n\\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.", "AI": {"tldr": "本文综述了CoT（思维链）推理对大型语言模型（LLM）可信度的影响，重点关注真实性、安全性、鲁棒性、公平性和隐私性。", "motivation": "尽管CoT推理显著提升了LLM在多项任务上的性能和可解释性，但其对模型可信度的影响仍缺乏全面理解。", "method": "本文通过对近期关于推理模型和CoT技术的研究进行综述，从真实性、安全性、鲁棒性、公平性和隐私性五个核心维度进行分析。研究按时间顺序概述了相关工作，并详细分析了其方法、发现和局限性。", "result": "CoT推理技术有望通过缓解幻觉、检测有害内容和提高鲁棒性来增强模型可信度。然而，尖端推理模型本身在安全性、鲁棒性和隐私性方面往往存在相似甚至更大的漏洞。", "conclusion": "推理技术在增强LLM可信度方面具有潜力，但当前的推理模型在安全性、鲁棒性和隐私性方面也面临显著挑战。本研究旨在为AI安全社区提供一个及时且有价值的资源，以了解推理可信度领域的最新进展。"}}
{"id": "2509.03956", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03956", "abs": "https://arxiv.org/abs/2509.03956", "authors": ["Minjong Yoo", "Jinwoo Jang", "Sihyung Yoon", "Honguk Woo"], "title": "World Model Implanting for Test-time Adaptation of Embodied Agents", "comment": null, "summary": "In embodied AI, a persistent challenge is enabling agents to robustly adapt\nto novel domains without requiring extensive data collection or retraining. To\naddress this, we present a world model implanting framework (WorMI) that\ncombines the reasoning capabilities of large language models (LLMs) with\nindependently learned, domain-specific world models through test-time\ncomposition. By allowing seamless implantation and removal of the world models,\nthe embodied agent's policy achieves and maintains cross-domain adaptability.\nIn the WorMI framework, we employ a prototype-based world model retrieval\napproach, utilizing efficient trajectory-based abstract representation\nmatching, to incorporate relevant models into test-time composition. We also\ndevelop a world-wise compound attention method that not only integrates the\nknowledge from the retrieved world models but also aligns their intermediate\nrepresentations with the reasoning model's representation within the agent's\npolicy. This framework design effectively fuses domain-specific knowledge from\nmultiple world models, ensuring robust adaptation to unseen domains. We\nevaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating\nsuperior zero-shot and few-shot performance compared to several LLM-based\napproaches across a range of unseen domains. These results highlight the\nframeworks potential for scalable, real-world deployment in embodied agent\nscenarios where adaptability and data efficiency are essential.", "AI": {"tldr": "本文提出WorMI框架，通过在测试时组合大型语言模型（LLM）与领域特定的世界模型，使具身智能体在无需大量数据或再训练的情况下，实现跨领域的鲁棒适应。", "motivation": "具身AI面临的持续挑战是，如何在不进行大量数据收集或再训练的情况下，使智能体能够鲁棒地适应新领域。", "method": "WorMI框架结合了LLM的推理能力和独立学习的领域特定世界模型，通过测试时组合实现。该框架支持世界模型的无缝植入和移除。具体方法包括：1) 采用基于原型的世界模型检索方法，利用高效的基于轨迹的抽象表示匹配来整合相关模型；2) 开发了一种世界级复合注意力机制，不仅整合了检索到的世界模型的知识，还将其中间表示与推理模型在智能体策略中的表示对齐。", "result": "WorMI在VirtualHome和ALFWorld基准测试中进行了评估，在各种未见领域，与几种基于LLM的方法相比，展现出卓越的零样本和少样本性能。", "conclusion": "这些结果突出了该框架在具身智能体场景中可扩展、真实世界部署的潜力，特别是在适应性和数据效率至关重要的应用中。"}}
{"id": "2509.03922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03922", "abs": "https://arxiv.org/abs/2509.03922", "authors": ["Xihua Sheng", "Yingwen Zhang", "Long Xu", "Shiqi Wang"], "title": "LMVC: An End-to-End Learned Multiview Video Coding Framework", "comment": null, "summary": "Multiview video is a key data source for volumetric video, enabling immersive\n3D scene reconstruction but posing significant challenges in storage and\ntransmission due to its massive data volume. Recently, deep learning-based\nend-to-end video coding has achieved great success, yet most focus on\nsingle-view or stereo videos, leaving general multiview scenarios\nunderexplored. This paper proposes an end-to-end learned multiview video coding\n(LMVC) framework that ensures random access and backward compatibility while\nenhancing compression efficiency. Our key innovation lies in effectively\nleveraging independent-view motion and content information to enhance\ndependent-view compression. Specifically, to exploit the inter-view motion\ncorrelation, we propose a feature-based inter-view motion vector prediction\nmethod that conditions dependent-view motion encoding on decoded\nindependent-view motion features, along with an inter-view motion entropy model\nthat learns inter-view motion priors. To exploit the inter-view content\ncorrelation, we propose a disparity-free inter-view context prediction module\nthat predicts inter-view contexts from decoded independent-view content\nfeatures, combined with an inter-view contextual entropy model that captures\ninter-view context priors. Experimental results show that our proposed LMVC\nframework outperforms the reference software of the traditional MV-HEVC\nstandard by a large margin, establishing a strong baseline for future research\nin this field.", "AI": {"tldr": "本文提出了一种端到端学习的多视角视频编码（LMVC）框架，通过有效利用独立视角的运动和内容信息来增强依赖视角的压缩效率，显著优于传统MV-HEVC标准。", "motivation": "多视角视频是沉浸式3D场景重建的关键数据源，但其庞大的数据量给存储和传输带来了巨大挑战。尽管基于深度学习的端到端视频编码已取得成功，但多数集中于单视角或立体视频，通用多视角场景尚未充分探索。", "method": "本文提出了一种端到端学习的多视角视频编码（LMVC）框架，确保随机访问和向后兼容性，并提高压缩效率。主要创新在于：1) 利用独立视角运动信息，提出基于特征的视角间运动矢量预测方法，并结合视角间运动熵模型学习先验；2) 利用独立视角内容信息，提出无视差的视角间上下文预测模块，并结合视角间上下文熵模型捕捉先验。", "result": "实验结果表明，所提出的LMVC框架在性能上大幅超越了传统MV-HEVC标准的参考软件。", "conclusion": "所提出的LMVC框架为该领域的未来研究建立了一个强大的基线。"}}
{"id": "2509.03888", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03888", "abs": "https://arxiv.org/abs/2509.03888", "authors": ["Cheng Wang", "Zeming Wei", "Qin Liu", "Muhao Chen"], "title": "False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize", "comment": null, "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.", "AI": {"tldr": "研究发现，现有基于探测的LLM安全检测方法可能学习到表面模式而非真正的语义有害性，导致虚假安全感，需要重新设计模型和评估协议。", "motivation": "尽管大型语言模型（LLMs）能力强大，但它们能够遵循有害指令，引发严重安全担忧。现有研究利用基于探测的方法来研究恶意和良性输入在LLM内部表示中的可分离性，并提出用这些方法进行安全检测。然而，这些方法在域外（OOD）性能不佳，促使作者假设探测器学习的是表面模式而非语义有害性。", "method": "本研究系统性地重新审视了基于探测的范式。通过一系列受控实验，包括将简单n-gram方法的性能与探测器进行比较、在语义清理的数据集上进行实验以及详细分析模式依赖性，来确认探测器学习的特定模式（指令模式和触发词）并验证其假设。", "result": "实验证实了探测器学习的是表面的指令模式和触发词，而非语义有害性。简单n-gram方法能达到与探测器相当的性能。这些结果揭示了当前基于探测的方法所带来的虚假安全感。", "conclusion": "当前基于探测的LLM安全检测方法存在根本性缺陷，未能学习到语义上的有害性。因此，迫切需要重新设计LLM模型和评估协议，以实现更负责任和有效的安全研究。"}}
{"id": "2509.03990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03990", "abs": "https://arxiv.org/abs/2509.03990", "authors": ["Chunlong Wu", "Zhibo Qu"], "title": "Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility for Resource-Efficient LLM Agent", "comment": null, "summary": "Large language model (LLM) agents achieve impressive single-task performance\nbut commonly exhibit repeated failures, inefficient exploration, and limited\ncross-task adaptability. Existing reflective strategies (e.g., Reflexion,\nReAct) improve per-episode behavior but typically produce ephemeral,\ntask-specific traces that are not reused across tasks. Reinforcement-learning\nbased alternatives can produce transferable policies but require substantial\nparameter updates and compute. In this work we introduce Meta-Policy Reflexion\n(MPR): a hybrid framework that consolidates LLM-generated reflections into a\nstructured, predicate-like Meta-Policy Memory (MPM) and applies that memory at\ninference time through two complementary mechanisms soft memory-guided decoding\nand hard rule admissibility checks(HAC). MPR (i) externalizes reusable\ncorrective knowledge without model weight updates, (ii) enforces domain\nconstraints to reduce unsafe or invalid actions, and (iii) retains the\nadaptability of language-based reflection. We formalize the MPM representation,\npresent algorithms for update and decoding, and validate the approach in a\ntext-based agent environment following the experimental protocol described in\nthe provided implementation (AlfWorld-based). Empirical results reported in the\nsupplied material indicate consistent gains in execution accuracy and\nrobustness when compared to Reflexion baselines; rule admissibility further\nimproves stability. We analyze mechanisms that explain these gains, discuss\nscalability and failure modes, and outline future directions for multimodal and\nmulti?agent extensions.", "AI": {"tldr": "Meta-Policy Reflexion (MPR) 是一种混合框架，通过将大型语言模型（LLM）生成的反思整合到结构化的元策略记忆（MPM）中，并在推理时通过软记忆引导解码和硬规则可采纳性检查来应用，从而提高LLM代理的跨任务性能、探索效率和适应性。", "motivation": "LLM代理在单一任务上表现出色，但常出现重复失败、探索效率低下和跨任务适应性有限的问题。现有反思策略（如Reflexion, ReAct）仅改进单次行为，但其痕迹是短暂且任务特定的，无法跨任务重用。基于强化学习的方法可以生成可迁移策略，但需要大量的参数更新和计算资源。", "method": "本文提出了元策略反思（Meta-Policy Reflexion, MPR）。它将LLM生成的反思整合为结构化的、谓词式的元策略记忆（Meta-Policy Memory, MPM）。在推理时，通过两种互补机制应用该记忆：软记忆引导解码（soft memory-guided decoding）和硬规则可采纳性检查（hard rule admissibility checks, HAC）。MPR无需模型权重更新即可外化可重用的纠正性知识，强制执行领域约束以减少不安全或无效动作，并保留了基于语言的反思的适应性。文中形式化了MPM表示，并提供了更新和解码算法。", "result": "在基于AlfWorld的文本代理环境中进行的实证结果表明，与Reflexion基线相比，MPR在执行准确性和鲁棒性方面持续获得提升；规则可采纳性（HAC）进一步提高了稳定性。研究还分析了解释这些收益的机制。", "conclusion": "MPR通过将LLM生成的反思系统地整合到可重用的元策略记忆中，并在推理时有效应用，成功解决了LLM代理的重复失败、低效探索和跨任务适应性差的问题，显著提高了其性能和稳定性。该方法无需模型权重更新，具有较好的通用性和适应性。"}}
{"id": "2509.03938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03938", "abs": "https://arxiv.org/abs/2509.03938", "authors": ["Minghui Zhang", "Yaoyu Liu", "Junyang Wu", "Xin You", "Hanxiao Zhang", "Junjun He", "Yun Gu"], "title": "TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained Tubular Shapes", "comment": null, "summary": "Medical tubular anatomical structures are inherently three-dimensional\nconduits with lumens, enclosing walls, and complex branching topologies.\nAccurate reconstruction of their geometry and topology is crucial for\napplications such as bronchoscopic navigation and cerebral arterial\nconnectivity assessment. Existing methods often rely on voxel-wise overlap\nmeasures, which fail to capture topological correctness and completeness.\nAlthough topology-aware losses and persistent homology constraints have shown\npromise, they are usually applied patch-wise and cannot guarantee global\npreservation or correct geometric errors at inference. To address these\nlimitations, we propose a novel TopoSculpt, a framework for topological\nrefinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a\nholistic whole-region modeling strategy to capture full spatial context, (ii)\nfirst introduces a Topological Integrity Betti (TIB) constraint that jointly\nenforces Betti number priors and global integrity, and (iii) employs a\ncurriculum refinement scheme with persistent homology to progressively correct\nerrors from coarse to fine scales. Extensive experiments on challenging\npulmonary airway and Circle of Willis datasets demonstrate substantial\nimprovements in both geometry and topology. For instance, $\\beta_{0}$ errors\nare reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on\nthe CoW dataset, with Tree length detected and branch detected rates improving\nby nearly 10\\%. These results highlight the effectiveness of TopoSculpt in\ncorrecting critical topological errors and advancing the high-fidelity modeling\nof complex 3D tubular anatomy. The project homepage is available at:\nhttps://github.com/Puzzled-Hui/TopoSculpt.", "AI": {"tldr": "本文提出了一种名为TopoSculpt的新框架，用于对医学三维细粒度管状结构进行拓扑精修，显著提高了几何和拓扑的准确性，解决了现有方法在全局拓扑正确性方面的不足。", "motivation": "医学管状结构（如气道和脑动脉）的准确几何和拓扑重建对于导航和连接性评估至关重要。现有方法常依赖体素重叠度量，无法捕捉拓扑正确性和完整性，且拓扑感知损失和持久同调约束通常局部应用，不能保证全局拓扑保持或在推理时纠正几何错误。", "method": "TopoSculpt框架采用以下策略：(i) 整体区域建模策略以捕捉完整空间上下文；(ii) 首次引入拓扑完整性Betti (TIB) 约束，共同强制执行Betti数先验和全局完整性；(iii) 采用带有持久同调的课程精修方案，从粗到细逐步纠正错误。", "result": "在肺气道和Willis环数据集上的实验表明，TopoSculpt在几何和拓扑方面都有显著改进。例如，气道数据集上的$\\beta_0$错误从69.00降至3.40，Willis环数据集上从1.65降至0.30，同时树长检测率和分支检测率提高了近10%。", "conclusion": "TopoSculpt框架在纠正关键拓扑错误和推进复杂三维管状解剖结构的高保真建模方面非常有效，解决了现有方法在全局拓扑正确性方面的局限性。"}}
{"id": "2509.03891", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03891", "abs": "https://arxiv.org/abs/2509.03891", "authors": ["Gowen Loo", "Chang Liu", "Qinghong Yin", "Xiang Chen", "Jiawei Chen", "Jingyuan Zhang", "Yu Tian"], "title": "MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation", "comment": null, "summary": "Smartphones have become indispensable in people's daily lives, permeating\nnearly every aspect of modern society. With the continuous advancement of large\nlanguage models (LLMs), numerous LLM-based mobile agents have emerged. These\nagents are capable of accurately parsing diverse user queries and automatically\nassisting users in completing complex or repetitive operations. However,\ncurrent agents 1) heavily rely on the comprehension ability of LLMs, which can\nlead to errors caused by misoperations or omitted steps during tasks, 2) lack\ninteraction with the external environment, often terminating tasks when an app\ncannot fulfill user queries, and 3) lack memory capabilities, requiring each\ninstruction to reconstruct the interface and being unable to learn from and\ncorrect previous mistakes. To alleviate the above issues, we propose MobileRAG,\na mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),\nwhich includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly\nand accurately identify user queries and accomplish complex and long-sequence\nmobile tasks. Additionally, to more comprehensively assess the performance of\nMobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark\ncharacterized by numerous complex, real-world mobile tasks that require\nexternal knowledge assistance. Extensive experimental results on MobileRAG-Eval\ndemonstrate that MobileRAG can easily handle real-world mobile tasks, achieving\n10.3\\% improvement over state-of-the-art methods with fewer operational steps.\nOur code is publicly available at:\nhttps://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv", "AI": {"tldr": "本文提出了MobileRAG，一个基于检索增强生成（RAG）的移动智能体框架，旨在解决现有LLM驱动移动智能体在理解、环境交互和记忆方面的不足，并在复杂真实世界任务上取得了显著性能提升。", "motivation": "现有基于大型语言模型（LLM）的移动智能体存在以下问题：1) 过度依赖LLM的理解能力，易导致操作错误或遗漏步骤；2) 缺乏与外部环境的交互，当应用无法满足需求时任务终止；3) 缺乏记忆能力，每次指令都需要重建界面且无法从错误中学习和纠正。", "method": "本文提出MobileRAG，一个通过检索增强生成（RAG）强化的移动智能体框架，包含InterRAG、LocalRAG和MemRAG。它利用RAG更快速准确地识别用户查询并完成复杂长序列移动任务。此外，还引入了MobileRAG-Eval，一个包含大量复杂真实世界任务且需要外部知识辅助的更具挑战性的基准来评估MobileRAG的性能。", "result": "在MobileRAG-Eval上的广泛实验结果表明，MobileRAG能够轻松处理真实世界移动任务，相比最先进的方法实现了10.3%的性能提升，并且操作步骤更少。", "conclusion": "MobileRAG通过引入RAG有效缓解了现有LLM驱动移动智能体在理解、环境交互和记忆方面的不足，使其能够更高效、准确地完成复杂真实的移动任务。"}}
{"id": "2509.04007", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04007", "abs": "https://arxiv.org/abs/2509.04007", "authors": ["Jinyuan Li", "Yi Chu", "Yiwen Sun", "Mengchuan Zou", "Shaowei Cai"], "title": "AutoPBO: LLM-powered Optimization for Local Search PBO Solvers", "comment": null, "summary": "Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling\ncombinatorial problems through pseudo-Boolean (PB) constraints. Local search\nsolvers have shown excellent performance in PBO solving, and their efficiency\nis highly dependent on their internal heuristics to guide the search. Still,\ntheir design often requires significant expert effort and manual tuning in\npractice. While Large Language Models (LLMs) have demonstrated potential in\nautomating algorithm design, their application to optimizing PBO solvers\nremains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered\nframework to automatically enhance PBO local search solvers. We conduct\nexperiments on a broad range of four public benchmarks, including one\nreal-world benchmark, a benchmark from PB competition, an integer linear\nprogramming optimization benchmark, and a crafted combinatorial benchmark, to\nevaluate the performance improvement achieved by AutoPBO and compare it with\nsix state-of-the-art competitors, including two local search PBO solvers NuPBO\nand OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed\ninteger programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates\nsignificant improvements over previous local search approaches, while\nmaintaining competitive performance compared to state-of-the-art competitors.\nThe results suggest that AutoPBO offers a promising approach to automating\nlocal search solver design.", "AI": {"tldr": "AutoPBO是一个基于大型语言模型（LLM）的框架，用于自动增强伪布尔优化（PBO）局部搜索求解器，实验表明其显著优于现有局部搜索方法，并与最先进的求解器保持竞争力。", "motivation": "伪布尔优化（PBO）局部搜索求解器的效率高度依赖于内部启发式算法，其设计通常需要大量的专家努力和手动调整。尽管大型语言模型（LLM）在自动化算法设计方面表现出潜力，但它们在优化PBO求解器方面的应用尚未被探索。", "method": "本文提出了AutoPBO，一个新颖的LLM驱动框架，用于自动增强PBO局部搜索求解器。通过在四个广泛的公共基准（包括真实世界、PB竞赛、整数线性规划和组合基准）上进行实验，评估AutoPBO的性能改进，并与六个最先进的竞争对手进行比较。", "result": "AutoPBO在性能上显著优于以前的局部搜索方法，同时与最先进的竞争对手相比保持了竞争力。", "conclusion": "AutoPBO为自动化局部搜索求解器设计提供了一种有前景的方法。"}}
{"id": "2509.03950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03950", "abs": "https://arxiv.org/abs/2509.03950", "authors": ["Alvaro Aranibar Roque", "Helga Sebastian"], "title": "Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture", "comment": "10 page, 5 figures", "summary": "Pneumothorax, the abnormal accumulation of air in the pleural space, can be\nlife-threatening if undetected. Chest X-rays are the first-line diagnostic\ntool, but small cases may be subtle. We propose an automated deep-learning\npipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax\nregions. Trained on the SIIM-ACR dataset with data augmentation and a combined\nbinary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and\nDice score of 0.8241 on the independent PTX-498 dataset. These results\ndemonstrate that the model can accurately localize pneumothoraces and support\nradiologists.", "AI": {"tldr": "该研究提出了一种基于U-Net和EfficientNet-B4编码器的自动化深度学习模型，用于准确分割胸部X光片中的气胸区域，并在独立数据集上取得了高精度，可辅助放射科医生诊断。", "motivation": "气胸若未被发现可能危及生命，而胸部X光片作为首选诊断工具，对于细微的小气胸案例可能难以识别。", "method": "研究提出了一种自动化深度学习流程，采用U-Net架构并结合EfficientNet-B4作为编码器，用于分割气胸区域。模型在SIIM-ACR数据集上进行训练，并使用了数据增强技术以及二元交叉熵与Dice损失函数的组合。", "result": "在独立的PTX-498数据集上，该模型的气胸分割性能达到0.7008的IoU（交并比）和0.8241的Dice分数。", "conclusion": "研究结果表明，所提出的模型能够准确地定位气胸，并能有效辅助放射科医生进行诊断。"}}
{"id": "2509.03918", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03918", "abs": "https://arxiv.org/abs/2509.03918", "authors": ["Fengxiao Tang", "Yufeng Li", "Zongzong Wu", "Ming Zhao"], "title": "MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering", "comment": null, "summary": "Complex Question Answering (QA) is a fundamental and challenging task in NLP.\nWhile large language models (LLMs) exhibit impressive performance in QA, they\nsuffer from significant performance degradation when facing complex and\nabstract QA tasks due to insufficient reasoning capabilities. Works such as\nChain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning\nabilities, but they face issues such as in-layer redundancy in tree structures\nand single paths in chain structures. Although some studies utilize\nRetrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the\nchallenge of effectively utilizing large amounts of information involving\nmultiple entities and hops remains critical. To address this, we propose the\nMatrix of Thought (MoT), a novel and efficient LLM thought structure. MoT\nexplores the problem in both horizontal and vertical dimensions through the\n\"column-cell communication\" mechanism, enabling LLMs to actively engage in\nmulti-strategy and deep-level thinking, reducing redundancy within the column\ncells and enhancing reasoning capabilities. Furthermore, we develop a\nfact-correction mechanism by constructing knowledge units from retrieved\nknowledge graph triples and raw text to enhance the initial knowledge for LLM\nreasoning and correct erroneous answers. This leads to the development of an\nefficient and accurate QA framework (MTQA). Experimental results show that our\nframework outperforms state-of-the-art methods on four widely-used datasets in\nterms of F1 and EM scores, with reasoning time only 14.4\\% of the baseline\nmethods, demonstrating both its efficiency and accuracy. The code for this\nframework is available at https://github.com/lyfiter/mtqa.", "AI": {"tldr": "本文提出了Matrix of Thought (MoT) 和事实纠正机制，构建了一个高效准确的问答框架MTQA，显著提升了大型语言模型在复杂问答任务中的推理能力和效率。", "motivation": "大型语言模型在复杂抽象问答任务中因推理能力不足而表现不佳。现有的CoT和ToT方法存在冗余或路径单一的问题，而RAG方法难以有效利用大量涉及多实体和多跳的信息。", "method": "本文提出了Matrix of Thought (MoT)，这是一种新颖的LLM思维结构，通过“列-单元格通信”机制在水平和垂直维度上探索问题，实现多策略、深层次思考，减少冗余并增强推理能力。此外，还开发了一个事实纠正机制，通过从知识图谱三元组和原始文本构建知识单元来增强初始知识并纠正错误答案，最终形成高效准确的MTQA框架。", "result": "实验结果表明，MTQA框架在四个广泛使用的数据集上，F1和EM分数均优于最先进的方法。同时，其推理时间仅为基线方法的14.4%。", "conclusion": "MTQA框架通过结合Matrix of Thought和事实纠正机制，显著提高了大型语言模型在复杂问答任务中的准确性和推理效率，证明了其优越性。"}}
{"id": "2509.04027", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04027", "abs": "https://arxiv.org/abs/2509.04027", "authors": ["Zeyu Gan", "Hao Yi", "Yong Liu"], "title": "CoT-Space: A Theoretical Framework for Internal Slow-Thinking via Reinforcement Learning", "comment": "Preprint Edition", "summary": "Reinforcement Learning (RL) has become a pivotal approach for enhancing the\nreasoning capabilities of Large Language Models (LLMs). However, a significant\ntheoretical gap persists, as traditional token-level RL frameworks fail to\nalign with the reasoning-level nature of complex, multi-step thought processes\nlike Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,\na novel theoretical framework that recasts LLM reasoning from a discrete\ntoken-prediction task to an optimization process within a continuous,\nreasoning-level semantic space. By analyzing this process from both a noise\nperspective and a risk perspective, we demonstrate that the convergence to an\noptimal CoT length is a natural consequence of the fundamental trade-off\nbetween underfitting and overfitting. Furthermore, extensive experiments\nprovide strong empirical validation for our theoretical findings. Our framework\nnot only provides a coherent explanation for empirical phenomena such as\noverthinking but also offers a solid theoretical foundation to guide the future\ndevelopment of more effective and generalizable reasoning agents.", "AI": {"tldr": "针对大型语言模型（LLMs）推理能力的强化学习（RL），本文提出了CoT-Space框架，将推理视为连续语义空间中的优化过程，解释了最优思维链（CoT）长度是欠拟合与过拟合权衡的自然结果，并为未来推理代理的发展奠定了理论基础。", "motivation": "传统的基于token的强化学习框架未能与LLMs复杂的多步骤推理过程（如思维链CoT）的推理级别性质对齐，存在显著的理论空白。", "method": "引入了CoT-Space这一新颖的理论框架，将LLM推理从离散的token预测任务重新定义为连续、推理级别的语义空间中的优化过程。该过程从噪声和风险两个角度进行了分析。", "result": "证明了收敛到最优CoT长度是欠拟合和过拟合之间基本权衡的自然结果。广泛的实验为理论发现提供了强有力的实证验证。该框架解释了“过度思考”等经验现象。", "conclusion": "CoT-Space框架不仅为经验现象提供了连贯的解释，也为未来开发更有效、更通用的大模型推理代理提供了坚实的理论基础。"}}
{"id": "2509.03951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03951", "abs": "https://arxiv.org/abs/2509.03951", "authors": ["Zhu Wenjie", "Zhang Yabin", "Xin Jin", "Wenjun Zeng", "Lei Zhang"], "title": "ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection", "comment": null, "summary": "The introduction of negative labels (NLs) has proven effective in enhancing\nOut-of-Distribution (OOD) detection. However, existing methods often lack an\nunderstanding of OOD images, making it difficult to construct an accurate\nnegative space. In addition, the presence of false negative labels\nsignificantly degrades their near-OOD performance. To address these issues, we\npropose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the\nunderstanding and reasoning capabilities of multimodal large language models\n(MLLMs). Specifically, we identify images likely to be OOD samples as negative\nimages and prompt the MLLM to describe these images, generating expressive\nnegative sentences that precisely characterize the OOD distribution and enhance\nfar-OOD detection. For the near-OOD setting, where OOD samples resemble the\nin-distribution (ID) subset, we first identify the subset of ID classes that\nare visually similar to negative images and then leverage the reasoning\ncapability of MLLMs to generate visually similar negative labels tailored to\nthis subset, effectively reducing false negatives and improving near-OOD\ndetection. To balance these two types of negative textual spaces, we design an\nadaptive weighted score that enables the method to handle different OOD task\nsettings (near-OOD and far-OOD) without relying on task-specific prior\nknowledge, making it highly adaptable in open environments. On the ImageNet\nbenchmark, our ANTS significantly reduces the FPR95 by 4.2\\%, establishing a\nnew state-of-the-art. Furthermore, our method is training-free and zero-shot,\nenabling high scalability.", "AI": {"tldr": "本文提出了一种名为自适应负文本空间（ANTS）的方法，利用多模态大语言模型（MLLMs）的理解和推理能力，为分布外（OOD）检测构建准确的负空间，有效提升了远OOD和近OOD的检测性能。", "motivation": "现有OOD检测方法在构建负空间时缺乏对OOD图像的理解，导致负空间不准确。此外，假负标签的存在显著降低了近OOD场景下的性能。", "method": "本文通过以下方式构建ANTS：1. 对于远OOD，识别潜在的OOD图像，并提示MLLM生成描述这些图像的富有表达力的负面句子。2. 对于近OOD，首先识别与负图像视觉相似的ID类别子集，然后利用MLLM的推理能力为该子集生成量身定制的视觉相似负标签，以减少假负例。3. 设计了一个自适应加权分数来平衡这两种负文本空间，使其无需特定任务先验知识即可处理不同OOD任务设置。", "result": "在ImageNet基准测试中，ANTS显著降低了FPR95 4.2%，达到了新的SOTA。此外，该方法是免训练和零样本的，具有高度的可扩展性。", "conclusion": "通过利用MLLMs的理解和推理能力，ANTS能够构建一个自适应的负文本空间，有效解决了现有OOD检测方法中负空间不准确和假负标签的问题，显著提升了远OOD和近OOD的检测性能，并展现出高适应性和可扩展性。"}}
{"id": "2509.03932", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03932", "abs": "https://arxiv.org/abs/2509.03932", "authors": ["Iro Lim", "Haein Ji", "Byungjun Kim"], "title": "Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling", "comment": "30 pages, 13 tables, 2 figures, Digital Humanities and Social\n  Sciences Korea Conference, James Joo-Jin Kim Center for Korean Studies,\n  University of Pennsylvania, Philadelphia, USA", "summary": "This study introduces KPoEM (Korean Poetry Emotion Mapping) , a novel dataset\nfor computational emotion analysis in modern Korean poetry. Despite remarkable\nprogress in text-based emotion classification using large language models,\npoetry-particularly Korean poetry-remains underexplored due to its figurative\nlanguage and cultural specificity. We built a multi-label emotion dataset of\n7,662 entries, including 7,007 line-level entries from 483 poems and 615\nwork-level entries, annotated with 44 fine-grained emotion categories from five\ninfluential Korean poets. A state-of-the-art Korean language model fine-tuned\non this dataset significantly outperformed previous models, achieving 0.60\nF1-micro compared to 0.34 from models trained on general corpora. The KPoEM\nmodel, trained through sequential fine-tuning-first on general corpora and then\non the KPoEM dataset-demonstrates not only an enhanced ability to identify\ntemporally and culturally specific emotional expressions, but also a strong\ncapacity to preserve the core sentiments of modern Korean poetry. This study\nbridges computational methods and literary analysis, presenting new\npossibilities for the quantitative exploration of poetic emotions through\nstructured data that faithfully retains the emotional and cultural nuances of\nKorean literature.", "AI": {"tldr": "本研究引入了KPoEM（韩国诗歌情感映射）数据集，用于现代韩国诗歌的计算情感分析。该数据集包含7,662个带有44种细粒度情感标签的条目。在一个最先进的韩国语言模型上对KPoEM进行微调后，其性能显著优于现有模型，F1-micro得分从0.34提高到0.60。", "motivation": "尽管大型语言模型在文本情感分类方面取得了显著进展，但诗歌（尤其是韩国诗歌）由于其比喻性语言和文化特异性，在情感分析领域仍未得到充分探索。", "method": "研究构建了一个多标签情感数据集KPoEM，包含7,662个条目（7,007个行级和615个作品级），由五位有影响力的韩国诗人的作品中提取，并用44种细粒度情感类别进行标注。随后，通过顺序微调（先在通用语料库上，再在KPoEM数据集上）训练了一个最先进的韩国语言模型。", "result": "在KPoEM数据集上微调后的韩国语言模型显著优于在通用语料库上训练的模型，F1-micro得分从0.34提高到0.60。KPoEM模型不仅增强了识别时间和文化特定情感表达的能力，还展现出保留现代韩国诗歌核心情感的强大能力。", "conclusion": "本研究通过构建结构化数据，有效连接了计算方法与文学分析，为定量探索诗歌情感提供了新的可能性，并忠实地保留了韩国文学的情感和文化细微之处。"}}
{"id": "2509.04041", "categories": ["cs.AI", "cs.LO", "68T30, 68T27, 03B35", "I.2.4; I.2.3; F.4.1; F.4.3"], "pdf": "https://arxiv.org/pdf/2509.04041", "abs": "https://arxiv.org/abs/2509.04041", "authors": ["Daniel Raggi", "Gem Stapleton", "Mateja Jamnik", "Aaron Stockdill", "Grecia Garcia Garcia", "Peter C-H. Cheng"], "title": "Oruga: An Avatar of Representational Systems Theory", "comment": null, "summary": "Humans use representations flexibly. We draw diagrams, change representations\nand exploit creative analogies across different domains. We want to harness\nthis kind of power and endow machines with it to make them more compatible with\nhuman use. Previously we developed Representational Systems Theory (RST) to\nstudy the structure and transformations of representations. In this paper we\npresent Oruga (caterpillar in Spanish; a symbol of transformation), an\nimplementation of various aspects of RST. Oruga consists of a core of data\nstructures corresponding to concepts in RST, a language for communicating with\nthe core, and an engine for producing transformations using a method we call\nstructure transfer. In this paper we present an overview of the core and\nlanguage of Oruga, with a brief example of the kind of transformation that\nstructure transfer can execute.", "AI": {"tldr": "本文介绍了Oruga，一个实现了表征系统理论（RST）多种功能的系统，旨在赋予机器灵活的表征转换能力，使其更接近人类思维。", "motivation": "人类能够灵活地使用和转换表征（如绘制图表、跨领域类比），研究者希望机器也能拥有这种能力，以提高其与人类的兼容性。", "method": "研究者基于之前开发的表征系统理论（RST），实现了名为Oruga的系统。Oruga包含对应RST概念的核心数据结构、用于与核心通信的语言，以及一个使用“结构转移”方法进行转换的引擎。", "result": "本文概述了Oruga的核心和语言，并简要展示了结构转移能够执行的转换类型示例。", "conclusion": "Oruga作为RST的实现，展示了如何通过核心数据结构、专用语言和结构转移引擎，使机器具备灵活的表征转换能力。"}}
{"id": "2509.03961", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03961", "abs": "https://arxiv.org/abs/2509.03961", "authors": ["Yijun Zhou", "Yikui Zhai", "Zilu Ying", "Tingfeng Xian", "Wenlve Zhou", "Zhiheng Zhou", "Xiaolin Tian", "Xudong Jia", "Hongsheng Zhang", "C. L. Philip Chen"], "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection", "comment": null, "summary": "Although deep learning has advanced remote sensing change detection (RSCD),\nmost methods rely solely on image modality, limiting feature representation,\nchange pattern modeling, and generalization especially under illumination and\nnoise disturbances. To address this, we propose MMChange, a multimodal RSCD\nmethod that combines image and text modalities to enhance accuracy and\nrobustness. An Image Feature Refinement (IFR) module is introduced to highlight\nkey regions and suppress environmental noise. To overcome the semantic\nlimitations of image features, we employ a vision language model (VLM) to\ngenerate semantic descriptions of bitemporal images. A Textual Difference\nEnhancement (TDE) module then captures fine grained semantic shifts, guiding\nthe model toward meaningful changes. To bridge the heterogeneity between\nmodalities, we design an Image Text Feature Fusion (ITFF) module that enables\ndeep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and\nSYSUCD demonstrate that MMChange consistently surpasses state of the art\nmethods across multiple metrics, validating its effectiveness for multimodal\nRSCD. Code is available at: https://github.com/yikuizhai/MMChange.", "AI": {"tldr": "MMChange是一种多模态遥感变化检测方法，它结合图像和文本模态，通过图像特征细化、文本差异增强和图像文本特征融合模块，提高了在光照和噪声干扰下的准确性和鲁棒性。", "motivation": "现有的深度学习遥感变化检测方法大多仅依赖图像模态，这限制了特征表示、变化模式建模和泛化能力，尤其是在光照和噪声干扰下表现不佳。", "method": "MMChange方法结合了图像和文本模态。它引入了图像特征细化（IFR）模块以突出关键区域并抑制环境噪声；利用视觉语言模型（VLM）生成双时相图像的语义描述；设计了文本差异增强（TDE）模块来捕捉细粒度的语义变化；并构建了图像文本特征融合（ITFF）模块以实现跨模态的深度整合，弥合模态间的异构性。", "result": "在LEVIRCD、WHUCD和SYSUCD等数据集上进行的广泛实验表明，MMChange在多项指标上持续超越了最先进的方法。", "conclusion": "MMChange方法有效提升了多模态遥感变化检测的准确性和鲁棒性，验证了其在复杂环境下的有效性。"}}
{"id": "2509.03934", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03934", "abs": "https://arxiv.org/abs/2509.03934", "authors": ["Yuqing Huang", "Rongyang Zhang", "Qimeng Wang", "Chengqiang Lu", "Yan Gao", "Yi Wu", "Yao Hu", "Xuyang Zhi", "Guiquan Liu", "Xin Li", "Hao Wang", "Enhong Chen"], "title": "SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment", "comment": null, "summary": "Recent advancements in large language models (LLMs) have revolutionized\nnatural language processing through their remarkable capabilities in\nunderstanding and executing diverse tasks. While supervised fine-tuning,\nparticularly in Retrieval-Augmented Generation (RAG) scenarios, effectively\nenhances task-specific performance, it often leads to catastrophic forgetting,\nwhere models lose their previously acquired knowledge and general capabilities.\nExisting solutions either require access to general instruction data or face\nlimitations in preserving the model's original distribution. To overcome these\nlimitations, we propose SelfAug, a self-distribution alignment method that\naligns input sequence logits to preserve the model's semantic distribution,\nthereby mitigating catastrophic forgetting and improving downstream\nperformance. Extensive experiments demonstrate that SelfAug achieves a superior\nbalance between downstream learning and general capability retention. Our\ncomprehensive empirical analysis reveals a direct correlation between\ndistribution shifts and the severity of catastrophic forgetting in RAG\nscenarios, highlighting how the absence of RAG capabilities in general\ninstruction tuning leads to significant distribution shifts during fine-tuning.\nOur findings not only advance the understanding of catastrophic forgetting in\nRAG contexts but also provide a practical solution applicable across diverse\nfine-tuning scenarios. Our code is publicly available at\nhttps://github.com/USTC-StarTeam/SelfAug.", "AI": {"tldr": "本文提出SelfAug方法，通过自分布对齐输入序列logits，以减轻大型语言模型在RAG场景下微调时出现的灾难性遗忘问题，同时提高下游任务性能并保持通用能力。", "motivation": "尽管监督微调（尤其是在RAG场景中）能有效提升任务特异性性能，但它常导致灾难性遗忘，使模型丧失原有知识和通用能力。现有解决方案要么依赖通用指令数据，要么在保持模型原始分布方面存在局限性。", "method": "本文提出SelfAug，一种自分布对齐方法。该方法通过对齐输入序列的logits来保留模型的语义分布，从而减轻灾难性遗忘并提高下游性能。", "result": "实验证明，SelfAug在下游学习和通用能力保留之间取得了更好的平衡。综合实证分析揭示了分布偏移与RAG场景中灾难性遗忘严重程度之间的直接关联，并指出通用指令微调中RAG能力的缺失导致了微调过程中的显著分布偏移。", "conclusion": "SelfAug不仅加深了对RAG背景下灾难性遗忘的理解，还提供了一种适用于多种微调场景的实用解决方案。"}}
{"id": "2509.04083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04083", "abs": "https://arxiv.org/abs/2509.04083", "authors": ["Alexander Beiser", "David Penz", "Nysret Musliu"], "title": "Intermediate Languages Matter: Formal Languages and LLMs affect Neurosymbolic Reasoning", "comment": "To appear in the proceedings of The Second Workshop on Knowledge\n  Graphs and Neurosymbolic AI (KG-NeSy) Co-located with SEMANTiCS 2025\n  Conference, Vienna, Austria - September 3rd, 2025", "summary": "Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs.", "AI": {"tldr": "神经符号LLM推理的成功受形式语言选择的影响，这是一个先前被忽视的关键因素。", "motivation": "尽管大型语言模型（LLMs）在许多任务上表现出色，但其形式推理能力仍有不足。神经符号LLM推理是一个有前景的方法，但其成功因素尚不明确。", "method": "本文提出了“中间语言挑战”，即选择合适的神经符号推理形式语言。通过在三个数据集和七个LLM上比较四种形式语言，来评估形式语言选择的影响。", "result": "研究表明，形式语言的选择会影响LLM的句法和语义推理能力，且这种影响在不同的LLM之间存在差异。", "conclusion": "形式语言的选择是神经符号LLM推理成功的一个关键且先前被忽视的因素，它显著影响推理能力，并且其效果因LLM而异。"}}
{"id": "2509.03973", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03973", "abs": "https://arxiv.org/abs/2509.03973", "authors": ["Yu Bai", "Zitong Yu", "Haowen Tian", "Xijing Wang", "Shuo Yan", "Lin Wang", "Honglin Li", "Xitong Ling", "Bo Zhang", "Zheng Zhang", "Wufan Wang", "Hui Gao", "Xiangyang Gong", "Wendong Wang"], "title": "SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification", "comment": null, "summary": "We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for\nperforming WSI classification. SAC-MIL consists of a positional encoding module\nto encode position information and a SAC block to perform full instance\ncorrelations. The positional encoding module utilizes the instance coordinates\nwithin the slide to encode the spatial relationships instead of the instance\nindex in the input WSI sequence. The positional encoding module can also handle\nthe length extrapolation issue where the training and testing sequences have\ndifferent lengths. The SAC block is an MLP-based method that performs full\ninstance correlation in linear time complexity with respect to the sequence\nlength. Due to the simple structure of MLP, it is easy to deploy since it does\nnot require custom CUDA kernels, compared to Transformer-based methods for WSI\nclassification. SAC-MIL has achieved state-of-the-art performance on the\nCAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon\nacceptance.", "AI": {"tldr": "提出了一种名为SAC-MIL的WSI分类方法，它结合了空间感知的定位编码模块和基于MLP的全实例关联SAC块，实现了SOTA性能且易于部署。", "motivation": "在全玻片图像（WSI）分类中，需要有效利用实例间的空间关系和相关性，同时解决序列长度变化以及Transformer类方法部署复杂（需要自定义CUDA内核）的问题。", "method": "SAC-MIL包含两个主要组件：1. 定位编码模块：利用WSI中实例的坐标而非索引来编码空间关系，并能处理训练和测试序列长度不一致的问题。2. SAC块：一个基于MLP的方法，以线性时间复杂度实现全实例关联，结构简单，易于部署，无需自定义CUDA内核。", "result": "SAC-MIL在CAMELYON-16、TCGA-LUNG和TCGA-BRAC数据集上取得了最先进（state-of-the-art）的性能。", "conclusion": "SAC-MIL通过其空间感知定位编码和高效的MLP-based全实例关联SAC块，为WSI分类提供了一种高性能且易于部署的解决方案，并在多个基准数据集上超越了现有方法。"}}
{"id": "2509.03937", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03937", "abs": "https://arxiv.org/abs/2509.03937", "authors": ["Yuhao Zhang", "Shaoming Duan", "Jinhang Su", "Chuanyi Liu", "Peiyi Han"], "title": "SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning", "comment": "EMNLP 2025 Findings", "summary": "Despite the significant advancements of self-play fine-tuning (SPIN), which\ncan transform a weak large language model (LLM) into a strong one through\ncompetitive interactions between models of varying capabilities, it still faces\nchallenges in the Text-to-SQL task. SPIN does not generate new information, and\nthe large number of correct SQL queries produced by the opponent model during\nself-play reduces the main model's ability to generate accurate SQL queries. To\naddress this challenge, we propose a new self-play fine-tuning method tailored\nfor the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a\nverification-based iterative fine-tuning approach, which synthesizes\nhigh-quality fine-tuning data iteratively based on the database schema and\nvalidation feedback to enhance model performance, while building a model base\nwith varying capabilities. During the self-play fine-tuning phase, we propose\nan error-driven loss method that incentivizes incorrect outputs from the\nopponent model, enabling the main model to distinguish between correct SQL and\nerroneous SQL generated by the opponent model, thereby improving its ability to\ngenerate correct SQL. Extensive experiments and in-depth analyses on six\nopen-source LLMs and five widely used benchmarks demonstrate that our approach\noutperforms existing state-of-the-art (SOTA) methods.", "AI": {"tldr": "本文提出了一种名为SPFT-SQL的新型自博弈微调方法，专为Text-to-SQL任务设计，通过引入基于验证的迭代微调和错误驱动的损失函数，有效提升了大型语言模型生成准确SQL查询的能力。", "motivation": "尽管自博弈微调（SPIN）在将弱LLM转化为强LLM方面取得了显著进展，但在Text-to-SQL任务中仍面临挑战。具体而言，SPIN不生成新信息，且对手模型生成的大量正确SQL查询反而降低了主模型生成准确SQL查询的能力。", "method": "SPFT-SQL方法包含两个主要阶段：1. 自博弈前，引入基于验证的迭代微调方法，根据数据库schema和验证反馈迭代合成高质量微调数据，以增强模型性能并构建具有不同能力的基础模型。2. 在自博弈微调阶段，提出一种错误驱动的损失方法，激励对手模型产生不正确的输出，从而使主模型能够区分正确和错误的SQL，提高其生成正确SQL的能力。", "result": "在六个开源LLM和五个广泛使用的基准测试上进行的广泛实验和深入分析表明，SPFT-SQL方法优于现有的最先进（SOTA）方法。", "conclusion": "SPFT-SQL通过创新的预微调和自博弈阶段策略，成功解决了现有SPIN在Text-to-SQL任务中的局限性，显著提升了LLM在该任务上的性能，实现了超越SOTA的成果。"}}
{"id": "2509.04100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04100", "abs": "https://arxiv.org/abs/2509.04100", "authors": ["Alberto Luise", "Michele Lombardi", "Florent Teichteil Koenigsbuch"], "title": "Hybrid Reinforcement Learning and Search for Flight Trajectory Planning", "comment": null, "summary": "This paper explores the combination of Reinforcement Learning (RL) and\nsearch-based path planners to speed up the optimization of flight paths for\nairliners, where in case of emergency a fast route re-calculation can be\ncrucial. The fundamental idea is to train an RL Agent to pre-compute\nnear-optimal paths based on location and atmospheric data and use those at\nruntime to constrain the underlying path planning solver and find a solution\nwithin a certain distance from the initial guess. The approach effectively\nreduces the size of the solver's search space, significantly speeding up route\noptimization. Although global optimality is not guaranteed, empirical results\nconducted with Airbus aircraft's performance models show that fuel consumption\nremains nearly identical to that of an unconstrained solver, with deviations\ntypically within 1%. At the same time, computation speed can be improved by up\nto 50% as compared to using a conventional solver alone.", "AI": {"tldr": "本文结合强化学习（RL）和基于搜索的路径规划器，通过RL预计算近乎最优路径来约束搜索空间，从而显著加速航空公司航班路径优化，尤其适用于紧急情况下的快速路径重计算。", "motivation": "在航空公司航班遇到紧急情况时，快速重新计算航线至关重要，这促使研究人员寻求加速航线优化计算的方法。", "method": "该方法训练一个强化学习（RL）智能体，根据位置和大气数据预计算近乎最优的路径。在运行时，这些预计算的路径被用作约束，以限制底层路径规划求解器的搜索空间，从而在初始猜测的一定距离内找到解决方案。", "result": "实证结果表明，与单独使用传统求解器相比，燃油消耗几乎相同（通常偏差在1%以内），而计算速度提高了高达50%。尽管不保证全局最优性，但实际性能表现良好。", "conclusion": "结合强化学习和基于搜索的路径规划能够显著加速航空公司航班路径优化，同时保持与非约束求解器几乎相同的燃油效率，这对于紧急情况下的快速航线重计算具有重要意义。"}}
{"id": "2509.03975", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03975", "abs": "https://arxiv.org/abs/2509.03975", "authors": ["Daniel Sobotka", "Alexander Herold", "Matthias Perkonigg", "Lucian Beer", "Nina Bastati", "Alina Sablatnig", "Ahmed Ba-Ssalamah", "Georg Langs"], "title": "Improving Vessel Segmentation with Multi-Task Learning and Auxiliary Data Available Only During Model Training", "comment": null, "summary": "Liver vessel segmentation in magnetic resonance imaging data is important for\nthe computational analysis of vascular remodelling, associated with a wide\nspectrum of diffuse liver diseases. Existing approaches rely on contrast\nenhanced imaging data, but the necessary dedicated imaging sequences are not\nuniformly acquired. Images without contrast enhancement are acquired more\nfrequently, but vessel segmentation is challenging, and requires large-scale\nannotated data. We propose a multi-task learning framework to segment vessels\nin liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data\navailable only during training to reduce the need for annotated training\nexamples. Our approach draws on paired native and contrast enhanced data with\nand without vessel annotations for model training. Results show that auxiliary\ndata improves the accuracy of vessel segmentation, even if they are not\navailable during inference. The advantage is most pronounced if only few\nannotations are available for training, since the feature representation\nbenefits from the shared task structure. A validation of this approach to\naugment a model for brain tumor segmentation confirms its benefits across\ndifferent domains. An auxiliary informative imaging modality can augment expert\nannotations even if it is only available during training.", "AI": {"tldr": "该研究提出一种多任务学习框架，利用训练期间可用的辅助对比增强MRI数据，在非对比增强肝脏MRI中进行血管分割，以减少对大量标注数据的需求，并在有限标注下显著提高分割精度。", "motivation": "肝脏血管分割对于弥漫性肝脏疾病的计算分析至关重要。现有方法依赖于对比增强MRI，但这类序列并非总是可用。非对比增强MRI数据更常见，但血管分割难度大且需要大量标注。因此，如何在非对比增强MRI中高效准确地进行血管分割是一个挑战。", "method": "研究提出一个多任务学习框架，用于在非对比增强肝脏MRI中分割血管。该框架在训练阶段利用辅助的对比增强MRI数据（推理时不可用），以减少对标注训练样本的需求。模型训练使用配对的原始和对比增强数据，其中部分数据包含血管标注，部分不包含。此外，该方法还通过脑肿瘤分割任务进行了跨领域验证。", "result": "结果表明，即使辅助数据在推理时不可用，它们也能提高血管分割的准确性。当训练时只有少量标注数据可用时，这种优势最为显著，因为共享任务结构有助于特征表示。将该方法应用于脑肿瘤分割模型也证实了其在不同领域的益处。", "conclusion": "研究得出结论，辅助信息成像模态可以增强专家标注的效果，即使该模态仅在训练期间可用。这为在数据标注受限的情况下进行医学图像分割提供了一种有效策略。"}}
{"id": "2509.03940", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.03940", "abs": "https://arxiv.org/abs/2509.03940", "authors": ["Weihao Wu", "Liang Cao", "Xinyu Wu", "Zhiwei Lin", "Rui Niu", "Jingbei Li", "Zhiyong Wu"], "title": "VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents", "comment": null, "summary": "Recent significant advancements in Large Language Models (LLMs) have greatly\npropelled the development of Role-Playing Conversational Agents (RPCAs). These\nsystems aim to create immersive user experiences through consistent persona\nadoption. However, current RPCA research faces dual limitations. First,\nexisting work predominantly focuses on the textual modality, entirely\noverlooking critical paralinguistic features including intonation, prosody, and\nrhythm in speech, which are essential for conveying character emotions and\nshaping vivid identities. Second, the speech-based role-playing domain suffers\nfrom a long-standing lack of standardized evaluation benchmarks. Most current\nspoken dialogue datasets target only fundamental capability assessments,\nfeaturing thinly sketched or ill-defined character profiles. Consequently, they\nfail to effectively quantify model performance on core competencies like\nlong-term persona consistency. To address this critical gap, we introduce\nVoxRole, the first comprehensive benchmark specifically designed for the\nevaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn\ndialogues, totaling 65.6 hours of speech from 1228 unique characters across 261\nmovies. To construct this resource, we propose a novel two-stage automated\npipeline that first aligns movie audio with scripts and subsequently employs an\nLLM to systematically build multi-dimensional profiles for each character.\nLeveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary\nspoken dialogue models, revealing crucial insights into their respective\nstrengths and limitations in maintaining persona consistency.", "AI": {"tldr": "本文介绍了VoxRole，首个专门用于评估基于语音的角色扮演对话代理（RPCA）的综合基准数据集，旨在解决现有研究中对副语言特征的忽视以及标准化评估基准的缺失。", "motivation": "现有RPCA研究主要关注文本模态，忽略了语音中对表达情感和塑造身份至关重要的副语言特征（如语调、韵律和节奏）。此外，基于语音的角色扮演领域缺乏标准化评估基准，现有口语对话数据集无法有效量化模型在长期角色一致性等核心能力上的表现。", "method": "本文引入了VoxRole基准，包含13335个多轮对话，总计65.6小时的语音，来自261部电影中的1228个独特角色。为构建该资源，作者提出了一种新颖的两阶段自动化流程：首先将电影音频与剧本对齐，然后利用大型语言模型（LLM）系统地为每个角色构建多维档案。", "result": "VoxRole是首个全面的基于语音RPCA评估基准。利用VoxRole，作者对当代口语对话模型进行了多维评估，揭示了它们在维持角色一致性方面的优势和局限性。", "conclusion": "VoxRole基准填补了基于语音RPCA评估领域的一个关键空白，为未来研究提供了标准化工具，以更有效地量化模型在处理副语言特征和保持角色一致性方面的性能。"}}
{"id": "2509.04125", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04125", "abs": "https://arxiv.org/abs/2509.04125", "authors": ["Tarik Zaciragic", "Aske Plaat", "K. Joost Batenburg"], "title": "Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker", "comment": null, "summary": "In the game of poker, being unpredictable, or bluffing, is an essential\nskill. When humans play poker, they bluff. However, most works on\ncomputer-poker focus on performance metrics such as win rates, while bluffing\nis overlooked. In this paper we study whether two popular algorithms, DQN\n(based on reinforcement learning) and CFR (based on game theory), exhibit\nbluffing behavior in Leduc Hold'em, a simplified version of poker. We designed\nan experiment where we let the DQN and CFR agent play against each other while\nwe log their actions. We find that both DQN and CFR exhibit bluffing behavior,\nbut they do so in different ways. Although both attempt to perform bluffs at\ndifferent rates, the percentage of successful bluffs (where the opponent folds)\nis roughly the same. This suggests that bluffing is an essential aspect of the\ngame, not of the algorithm. Future work should look at different bluffing\nstyles and at the full game of poker. Code at\nhttps://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.", "AI": {"tldr": "研究了DQN和CFR两种算法在Leduc Hold'em扑克中是否表现出诈唬行为及其方式。", "motivation": "诈唬是扑克中一项基本技能，但在计算机扑克研究中常被忽视，研究多关注胜率而非行为模式。", "method": "设计实验让DQN和CFR代理在简化的Leduc Hold'em扑克中对战，并记录其行动以分析诈唬行为。", "result": "DQN和CFR都表现出诈唬行为，但方式不同。虽然诈唬频率不同，但成功率（对手弃牌）大致相同。这表明诈唬是游戏本身的重要组成部分，而非算法特性。", "conclusion": "诈唬是扑克游戏的一个基本要素，并非特定算法所独有。未来的研究应探索不同的诈唬风格以及在完整扑克游戏中的表现。"}}
{"id": "2509.03986", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03986", "abs": "https://arxiv.org/abs/2509.03986", "authors": ["Mohamed Insaf Ismithdeen", "Muhammad Uzair Khattak", "Salman Khan"], "title": "Promptception: How Sensitive Are Large Multimodal Models to Prompts?", "comment": "Accepted to EMNLP 2025", "summary": "Despite the success of Large Multimodal Models (LMMs) in recent years, prompt\ndesign for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly\nunderstood. We show that even minor variations in prompt phrasing and structure\ncan lead to accuracy deviations of up to 15% for certain prompts and models.\nThis variability poses a challenge for transparent and fair LMM evaluation, as\nmodels often report their best-case performance using carefully selected\nprompts. To address this, we introduce Promptception, a systematic framework\nfor evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,\nspanning 15 categories and 6 supercategories, each targeting specific aspects\nof prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight\nopen-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:\nMMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit\ngreater sensitivity to prompt phrasing, reflecting tighter alignment with\ninstruction semantics, while open-source models are steadier but struggle with\nnuanced and complex phrasing. Based on this analysis, we propose Prompting\nPrinciples tailored to proprietary and open-source LMMs, enabling more robust\nand fair model evaluation.", "AI": {"tldr": "本研究揭示了多模态大模型（LMMs）在多项选择问答（MCQA）中对提示词设计的敏感性，提出了Promptception框架来系统评估，发现专有模型更敏感，并据此提出了提示词原则以实现更公平的评估。", "motivation": "LMMs在MCQA中的提示词设计尚不明确，即使是微小的提示词变化也可能导致高达15%的准确率偏差。这种变异性使得LMMs的透明和公平评估面临挑战，因为模型通常报告其精心选择提示词下的最佳性能。", "method": "引入了Promptception，一个系统评估LMMs提示词敏感性的框架。该框架包含61种提示词类型，跨越15个类别和6个超级类别，旨在针对提示词制定的特定方面进行评估。研究使用该框架评估了10个LMMs（从轻量级开源模型到GPT-4o和Gemini 1.5 Pro），并在MMStar、MMMU-Pro、MVBench这3个MCQA基准上进行测试。", "result": "研究发现，专有模型对提示词措辞表现出更高的敏感性，反映出它们与指令语义更紧密的对齐。相比之下，开源模型虽然更稳定，但在处理细致和复杂的措辞时表现不佳。", "conclusion": "基于分析结果，本研究提出了针对专有和开源LMMs的“提示词原则”（Prompting Principles），以期实现更稳健和公平的模型评估。"}}
{"id": "2509.03957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03957", "abs": "https://arxiv.org/abs/2509.03957", "authors": ["Ruiling Guo", "Xinwei Yang", "Chen Huang", "Tong Zhang", "Yong Hu"], "title": "CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking", "comment": "Findings of EMNLP 2025", "summary": "The effectiveness of large language models (LLMs) to fact-check\nmisinformation remains uncertain, despite their growing use. To this end, we\npresent CANDY, a benchmark designed to systematically evaluate the capabilities\nand limitations of LLMs in fact-checking Chinese misinformation. Specifically,\nwe curate a carefully annotated dataset of ~20k instances. Our analysis shows\nthat current LLMs exhibit limitations in generating accurate fact-checking\nconclusions, even when enhanced with chain-of-thought reasoning and few-shot\nprompting. To understand these limitations, we develop a taxonomy to categorize\nflawed LLM-generated explanations for their conclusions and identify factual\nfabrication as the most common failure mode. Although LLMs alone are unreliable\nfor fact-checking, our findings indicate their considerable potential to\naugment human performance when deployed as assistive tools in scenarios. Our\ndataset and code can be accessed at https://github.com/SCUNLP/CANDY", "AI": {"tldr": "该研究提出了CANDY基准来评估大型语言模型（LLMs）在中文虚假信息事实核查方面的能力和局限性。结果显示LLMs在生成准确结论方面存在局限，但作为辅助工具可显著增强人类表现。", "motivation": "尽管大型语言模型（LLMs）在日益普及，但它们在事实核查虚假信息方面的有效性仍不确定。", "method": "研究构建了一个名为CANDY的基准，用于系统评估LLMs在中文虚假信息事实核查中的能力和局限性。该基准包含一个精心标注的约2万条实例的数据集。为了理解LLMs的局限性，研究开发了一个分类法来归类LLM生成结论中存在的缺陷解释。", "result": "分析表明，即使通过思维链推理和少量样本提示增强，当前LLMs在生成准确事实核查结论方面仍存在局限性。研究识别出“事实编造”是LLM生成解释中最常见的失败模式。尽管LLMs单独进行事实核查不可靠，但它们作为辅助工具在特定场景下具有显著增强人类表现的潜力。", "conclusion": "大型语言模型（LLMs）在独立进行事实核查时并不可靠，但在作为辅助工具部署时，它们在增强人类表现方面具有巨大的潜力。"}}
{"id": "2509.04130", "categories": ["cs.AI", "cs.CY", "I.2.0"], "pdf": "https://arxiv.org/pdf/2509.04130", "abs": "https://arxiv.org/abs/2509.04130", "authors": ["William Stewart"], "title": "The human biological advantage over AI", "comment": "12 pages", "summary": "Recent advances in AI raise the possibility that AI systems will one day be\nable to do anything humans can do, only better. If artificial general\nintelligence (AGI) is achieved, AI systems may be able to understand, reason,\nproblem solve, create, and evolve at a level and speed that humans will\nincreasingly be unable to match, or even understand. These possibilities raise\na natural question as to whether AI will eventually become superior to humans,\na successor \"digital species\", with a rightful claim to assume leadership of\nthe universe. However, a deeper consideration suggests the overlooked\ndifferentiator between human beings and AI is not the brain, but the central\nnervous system (CNS), providing us with an immersive integration with physical\nreality. It is our CNS that enables us to experience emotion including pain,\njoy, suffering, and love, and therefore to fully appreciate the consequences of\nour actions on the world around us. And that emotional understanding of the\nconsequences of our actions is what is required to be able to develop\nsustainable ethical systems, and so be fully qualified to be the leaders of the\nuniverse. A CNS cannot be manufactured or simulated; it must be grown as a\nbiological construct. And so, even the development of consciousness will not be\nsufficient to make AI systems superior to humans. AI systems may become more\ncapable than humans on almost every measure and transform our society. However,\nthe best foundation for leadership of our universe will always be DNA, not\nsilicon.", "AI": {"tldr": "尽管AI可能在多方面超越人类，但本文认为人类的中央神经系统（CNS）赋予的情感体验和伦理理解是领导力的关键，而这是AI无法模拟或制造的，因此DNA而非硅基智能才是宇宙领导者的最佳基础。", "motivation": "随着AI的快速发展和通用人工智能（AGI）的潜在实现，引发了AI是否会最终超越人类，成为“数字物种”并主导宇宙的疑问。", "method": "本文通过提出人类与AI之间被忽视的区别不在于大脑，而在于中央神经系统（CNS）。它认为CNS赋予人类情感体验（如痛苦、喜悦、爱），从而能充分理解行为后果，并在此基础上发展可持续的伦理系统，这才是成为宇宙领导者的必要条件。该文强调CNS必须作为生物构造生长，无法被制造或模拟。", "result": "AI系统可能在几乎所有方面都比人类更强大并改变社会。然而，情感理解（源于生物CNS）对于发展可持续的伦理系统至关重要，因此，人类（基于DNA）而非AI（基于硅）才是宇宙领导力的最佳基础。", "conclusion": "即使AI系统发展出意识，也无法在领导力方面超越人类。因为生物中央神经系统所带来的情感理解和伦理发展能力是真正领导力的不可替代基础，而这并非硅基智能所能拥有。"}}
{"id": "2509.03999", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03999", "abs": "https://arxiv.org/abs/2509.03999", "authors": ["Han Huang", "Han Sun", "Ningzhong Liu", "Huiyu Zhou", "Jiaquan Shen"], "title": "SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation", "comment": "14 pages, accepted by PRCV2025", "summary": "Driven by autonomous driving's demands for precise 3D perception, 3D semantic\noccupancy prediction has become a pivotal research topic. Unlike\nbird's-eye-view (BEV) methods, which restrict scene representation to a 2D\nplane, occupancy prediction leverages a complete 3D voxel grid to model spatial\nstructures in all dimensions, thereby capturing semantic variations along the\nvertical axis. However, most existing approaches overlook height-axis\ninformation when processing voxel features. And conventional SENet-style\nchannel attention assigns uniform weight across all height layers, limiting\ntheir ability to emphasize features at different heights. To address these\nlimitations, we propose SliceSemOcc, a novel vertical slice based multimodal\nframework for 3D semantic occupancy representation. Specifically, we extract\nvoxel features along the height-axis using both global and local vertical\nslices. Then, a global local fusion module adaptively reconciles fine-grained\nspatial details with holistic contextual information. Furthermore, we propose\nthe SEAttention3D module, which preserves height-wise resolution through\naverage pooling and assigns dynamic channel attention weights to each height\nlayer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy\ndatasets verify that our method significantly enhances mean IoU, achieving\nespecially pronounced gains on most small-object categories. Detailed ablation\nstudies further validate the effectiveness of the proposed SliceSemOcc\nframework.", "AI": {"tldr": "针对3D语义占用预测中忽略高度轴信息和通道注意力分配不均的问题，本文提出了SliceSemOcc框架，通过垂直切片、全局局部融合以及动态高度感知通道注意力（SEAttention3D）显著提升了3D感知性能，尤其在小物体类别上表现突出。", "motivation": "自动驾驶对精确3D感知有很高需求，3D语义占用预测是关键技术。现有方法在处理体素特征时，忽略了高度轴信息，且传统的SENet式通道注意力对所有高度层分配统一权重，限制了其强调不同高度特征的能力。", "method": "本文提出了SliceSemOcc，一个基于垂直切片的多模态3D语义占用表示框架。具体而言，它使用全局和局部垂直切片沿高度轴提取体素特征；然后，通过全局局部融合模块自适应地协调精细空间细节与整体上下文信息；此外，还提出了SEAttention3D模块，该模块通过平均池化保留高度分辨率，并为每个高度层分配动态通道注意力权重。", "result": "在nuScenes-SurroundOcc和nuScenes-OpenOccupancy数据集上进行了广泛实验，验证了所提方法显著提升了平均IoU，尤其在大多数小物体类别上取得了显著增益。详细的消融研究进一步证实了SliceSemOcc框架的有效性。", "conclusion": "SliceSemOcc框架通过有效利用高度轴信息和动态通道注意力，成功解决了现有3D语义占用预测方法的局限性，显著增强了3D感知能力，尤其对小物体识别有显著提升。"}}
{"id": "2509.03962", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03962", "abs": "https://arxiv.org/abs/2509.03962", "authors": ["Ulin Nuha", "Adam Jatowt"], "title": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting", "comment": null, "summary": "The effectiveness of Large Language Models (LLMs) diminishes for extremely\nlow-resource languages, such as indigenous languages, primarily due to the lack\nof labeled data. Despite growing interest, the availability of high-quality\nnatural language processing (NLP) datasets for these languages remains limited,\nmaking it difficult to develop robust language technologies. This paper\naddresses such gap by focusing on Ladin, an endangered Romance language,\nspecifically targeting the Val Badia variant. Leveraging a small set of\nparallel Ladin-Italian sentence pairs, we create synthetic datasets for\nsentiment analysis and multiple-choice question answering (MCQA) by translating\nmonolingual Italian data. To ensure linguistic quality and reliability, we\napply rigorous filtering and back-translation procedures in our method. We\nfurther demonstrate that incorporating these synthetic datasets into machine\ntranslation training leads to substantial improvements over existing\nItalian-Ladin translation baselines. Our contributions include the first\npublicly available sentiment analysis and MCQA datasets for Ladin, establishing\nfoundational resources that can support broader NLP research and downstream\napplications for this underrepresented language.", "AI": {"tldr": "本文通过利用少量平行语料并翻译意大利语数据，为濒危语言拉登语（Ladin）创建了合成情感分析和多项选择问答数据集，并显著提升了机器翻译性能。", "motivation": "大型语言模型（LLMs）在极低资源语言（如土著语言）上的效果不佳，主要原因是缺乏标注数据。尽管兴趣日益增长，但这些语言的高质量自然语言处理（NLP）数据集仍然有限，阻碍了强大语言技术的发展。", "method": "该研究针对濒危罗曼语拉登语（特别是Val Badia方言），利用少量拉登语-意大利语平行句对，通过翻译单语意大利语数据创建了情感分析和多项选择问答（MCQA）的合成数据集。为确保语言质量和可靠性，采用了严格的过滤和回译程序。此外，将这些合成数据集整合到机器翻译训练中。", "result": "将合成数据集整合到机器翻译训练中，显著提升了现有意大利语-拉登语翻译基线的性能。本文还贡献了首批公开可用的拉登语情感分析和MCQA数据集。", "conclusion": "本研究为拉登语建立了基础资源，可支持该代表性不足语言的更广泛NLP研究和下游应用。"}}
{"id": "2509.04159", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04159", "abs": "https://arxiv.org/abs/2509.04159", "authors": ["Aarush Kumbhakern", "Saransh Kumar Gupta", "Lipika Dey", "Partha Pratim Das"], "title": "Towards an Action-Centric Ontology for Cooking Procedures Using Temporal Graphs", "comment": "6 pages, 3 figures, 1 table, 11 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop", "summary": "Formalizing cooking procedures remains a challenging task due to their\ninherent complexity and ambiguity. We introduce an extensible domain-specific\nlanguage for representing recipes as directed action graphs, capturing\nprocesses, transfers, environments, concurrency, and compositional structure.\nOur approach enables precise, modular modeling of complex culinary workflows.\nInitial manual evaluation on a full English breakfast recipe demonstrates the\nDSL's expressiveness and suitability for future automated recipe analysis and\nexecution. This work represents initial steps towards an action-centric\nontology for cooking, using temporal graphs to enable structured machine\nunderstanding, precise interpretation, and scalable automation of culinary\nprocesses - both in home kitchens and professional culinary settings.", "AI": {"tldr": "本文提出了一种可扩展的领域特定语言（DSL），用于将食谱表示为有向动作图，以形式化复杂的烹饪流程，旨在实现自动化分析和执行。", "motivation": "烹饪过程固有的复杂性和模糊性使得其形式化成为一项挑战。", "method": "引入了一种可扩展的领域特定语言（DSL），将食谱表示为有向动作图，捕获过程、转移、环境、并发性和组合结构。", "result": "对一份完整的英式早餐食谱进行的初步手动评估证明了该DSL的表达能力，并显示其适用于未来的自动化食谱分析和执行。", "conclusion": "这项工作是构建以动作为中心的烹饪本体论的初步步骤，利用时间图实现机器对烹饪过程的结构化理解、精确解释和可扩展自动化。"}}
{"id": "2509.04009", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04009", "abs": "https://arxiv.org/abs/2509.04009", "authors": ["Solha Kang", "Esla Timothy Anzaku", "Wesley De Neve", "Arnout Van Messem", "Joris Vankerschaver", "Francois Rameau", "Utku Ozbulak"], "title": "Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding", "comment": null, "summary": "Due to their powerful feature association capabilities, neural network-based\ncomputer vision models have the ability to detect and exploit unintended\npatterns within the data, potentially leading to correct predictions based on\nincorrect or unintended but statistically relevant signals. These clues may\nvary from simple color aberrations to small texts within the image. In\nsituations where these unintended signals align with the predictive task,\nmodels can mistakenly link these features with the task and rely on them for\nmaking predictions. This phenomenon is referred to as spurious correlations,\nwhere patterns appear to be associated with the task but are actually\ncoincidental. As a result, detection and mitigation of spurious correlations\nhave become crucial tasks for building trustworthy, reliable, and generalizable\nmachine learning models. In this work, we present a novel method to detect\nspurious correlations in vision transformers, a type of neural network\narchitecture that gained significant popularity in recent years. Using both\nsupervised and self-supervised trained models, we present large-scale\nexperiments on the ImageNet dataset demonstrating the ability of the proposed\nmethod to identify spurious correlations. We also find that, even if the same\narchitecture is used, the training methodology has a significant impact on the\nmodel's reliance on spurious correlations. Furthermore, we show that certain\nclasses in the ImageNet dataset contain spurious signals that are easily\ndetected by the models and discuss the underlying reasons for those spurious\nsignals. In light of our findings, we provide an exhaustive list of the\naforementioned images and call for caution in their use in future research\nefforts. Lastly, we present a case study investigating spurious signals in\ninvasive breast mass classification, grounding our work in real-world\nscenarios.", "AI": {"tldr": "本文提出了一种检测视觉Transformer中虚假关联的新方法，通过大规模实验证明其有效性，并发现训练方法对模型依赖虚假关联有显著影响，同时识别出ImageNet数据集中存在虚假信号的特定类别。", "motivation": "神经网络，尤其是计算机视觉模型，能够检测并利用数据中意想不到的模式（即虚假关联），从而基于不正确或非预期的统计相关信号做出预测。这导致模型不可信、不可靠且泛化能力差，因此检测和缓解虚假关联对于构建值得信赖的模型至关重要。", "method": "本文提出了一种新颖的方法来检测视觉Transformer中的虚假关联。该方法在ImageNet数据集上对监督和自监督训练的模型进行了大规模实验。此外，还进行了一项关于侵入性乳腺肿块分类中虚假信号的案例研究。", "result": "所提出的方法能够成功识别视觉Transformer中的虚假关联。研究发现，即使使用相同的架构，训练方法也会显著影响模型对虚假关联的依赖。ImageNet数据集中某些类别包含容易被模型检测到的虚假信号，并且本文讨论了这些信号的潜在原因。作者还提供了一份包含上述图像的详尽列表。", "conclusion": "虚假关联是影响模型可靠性的重要问题。本文提出的方法能有效检测视觉Transformer中的虚假关联，且模型的训练方式对其依赖程度有显著影响。研究人员在使用ImageNet数据集中特定图像时应保持谨慎。研究工作还通过乳腺肿块分类案例研究与现实世界应用相结合。"}}
{"id": "2509.03972", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03972", "abs": "https://arxiv.org/abs/2509.03972", "authors": ["Junghwan Lim", "Gangwon Jo", "Sungmin Lee", "Jiyoung Park", "Dongseok Kim", "Jihwan Kim", "Junhyeok Lee", "Wai Ting Cheung", "Dahye Choi", "Kibong Choi", "Jaeyeon Huh", "Beomgyu Kim", "Jangwoong Kim", "Taehyun Kim", "Haesol Lee", "Jeesoo Lee", "Dongpin Oh", "Changseok Song", "Daewon Suh"], "title": "Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study", "comment": null, "summary": "We introduce Llama-3-Motif, a language model consisting of 102 billion\nparameters, specifically designed to enhance Korean capabilities while\nretaining strong performance in English. Developed on the Llama 3 architecture,\nLlama-3-Motif employs advanced training techniques, including LlamaPro and\nMasked Structure Growth, to effectively scale the model without altering its\ncore Transformer architecture. Using the MoAI platform for efficient training\nacross hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully\ncurated dataset that maintains a balanced ratio of Korean and English data.\nLlama-3-Motif shows decent performance on Korean-specific benchmarks,\noutperforming existing models and achieving results comparable to GPT-4.", "AI": {"tldr": "Llama-3-Motif是一个1020亿参数的语言模型，基于Llama 3架构，通过LlamaPro和Masked Structure Growth等技术，增强了韩语能力并保持了强大的英语性能，在韩语基准测试中表现出色，可与GPT-4媲美。", "motivation": "旨在提升语言模型在韩语方面的能力，同时不牺牲其在英语上的优秀表现。", "method": "模型基于Llama 3架构，拥有1020亿参数。采用LlamaPro和Masked Structure Growth等先进训练技术进行扩展，不改变核心Transformer架构。利用MoAI平台在超大规模GPU集群上进行高效训练，并使用精心策划的、韩语和英语数据比例平衡的数据集进行优化。", "result": "Llama-3-Motif在韩语特定基准测试中表现良好，超越了现有模型，并取得了与GPT-4相当的成果。", "conclusion": "Llama-3-Motif成功地在增强韩语能力的同时保持了强大的英语性能，并在相关基准测试中展现出卓越表现。"}}
{"id": "2509.04192", "categories": ["cs.AI", "cs.LO", "math.LO", "68T27, 68T30, 68T37, 03C13", "I.2; F.4; G.3"], "pdf": "https://arxiv.org/pdf/2509.04192", "abs": "https://arxiv.org/abs/2509.04192", "authors": ["Vera Koponen"], "title": "Domain size asymptotics for Markov logic networks", "comment": null, "summary": "A Markov logic network (MLN) determines a probability distribution on the set\nof structures, or ``possible worlds'', with an arbitrary finite domain. We\nstudy the properties of such distributions as the domain size tends to\ninfinity. Three types of concrete examples of MLNs will be considered, and the\nproperties of random structures with domain sizes tending to infinity will be\nstudied: (1) Arbitrary quantifier-free MLNs over a language with only one\nrelation symbol which has arity 1. In this case we give a pretty complete\ncharacterization of the possible limit behaviours of random structures. (2) An\nMLN that favours graphs with fewer triangles (or more generally, fewer\nk-cliques). As a corollary of the analysis a ``$\\delta$-approximate 0-1 law''\nfor first-order logic is obtained. (3) An MLN that favours graphs with fewer\nvertices with degree higher than a fixed (but arbitrary) number. The analysis\nshows that depending on which ``soft constraints'' an MLN uses the limit\nbehaviour of random structures can be quite different, and the weights of the\nsoft constraints may, or may not, have influence on the limit behaviour. It\nwill also be demonstrated, using (1), that quantifier-free MLNs and lifted\nBayesian networks (in a broad sense) are asymptotically incomparable, roughly\nmeaning that there is a sequence of distributions on possible worlds with\nincreasing domain sizes that can be defined by one of the formalisms but not\neven approximated by the other. In a rather general context it is also shown\nthat on large domains the distribution determined by an MLN concentrates almost\nall its probability mass on a totally different part of the space of possible\nworlds than the uniform distribution does.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.04023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04023", "abs": "https://arxiv.org/abs/2509.04023", "authors": ["Shiku Kaito", "Shinnosuke Matsuo", "Daiki Suehiro", "Ryoma Bise"], "title": "Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning", "comment": "35 pages, 9 figures, Accepted in Pattern recognition", "summary": "The paper proposes a novel multi-class Multiple-Instance Learning (MIL)\nproblem called Learning from Majority Label (LML). In LML, the majority class\nof instances in a bag is assigned as the bag-level label. The goal of LML is to\ntrain a classification model that estimates the class of each instance using\nthe majority label. This problem is valuable in a variety of applications,\nincluding pathology image segmentation, political voting prediction, customer\nsentiment analysis, and environmental monitoring. To solve LML, we propose a\nCounting Network trained to produce bag-level majority labels, estimated by\ncounting the number of instances in each class. Furthermore, analysis\nexperiments on the characteristics of LML revealed that bags with a high\nproportion of the majority class facilitate learning. Based on this result, we\ndeveloped a Majority Proportion Enhancement Module (MPEM) that increases the\nproportion of the majority class by removing minority class instances within\nthe bags. Experiments demonstrate the superiority of the proposed method on\nfour datasets compared to conventional MIL methods. Moreover, ablation studies\nconfirmed the effectiveness of each module. The code is available at\n\\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.", "AI": {"tldr": "本文提出了一种名为“从多数标签学习”（LML）的多类别多示例学习（MIL）新问题，其目标是根据包内多数实例的类别标签来训练实例级分类模型。为解决LML问题，作者提出了一个计数网络（Counting Network）和一个多数比例增强模块（MPEM），并在实验中证明了其优于传统MIL方法的性能。", "motivation": "LML问题在病理图像分割、政治投票预测、客户情感分析和环境监测等多种应用中具有重要价值，因此需要有效的方法来解决这一问题。", "method": "本文提出了一种计数网络（Counting Network），通过估计每个类别中实例的数量来生成包级的多数标签。此外，基于“高多数比例的包有利于学习”的发现，开发了一个多数比例增强模块（MPEM），通过移除包内的少数类别实例来增加多数类别的比例。", "result": "实验结果表明，所提出的方法在四个数据集上优于传统的MIL方法。消融研究证实了每个模块的有效性。分析实验还揭示了高多数比例的包能够促进学习。", "conclusion": "LML是一个有价值的多类别多示例学习问题。本文提出的结合计数网络和多数比例增强模块的方法能够有效解决LML问题，并在实践中展现出卓越的性能。"}}
{"id": "2509.03995", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03995", "abs": "https://arxiv.org/abs/2509.03995", "authors": ["Zhaoyan Gong", "Juan Li", "Zhiqiang Liu", "Lei Liang", "Huajun Chen", "Wen Zhang"], "title": "RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models", "comment": "EMNLP 2025", "summary": "Current temporal knowledge graph question answering (TKGQA) methods primarily\nfocus on implicit temporal constraints, lacking the capability of handling more\ncomplex temporal queries, and struggle with limited reasoning abilities and\nerror propagation in decomposition frameworks. We propose RTQA, a novel\nframework to address these challenges by enhancing reasoning over TKGs without\nrequiring training. Following recursive thinking, RTQA recursively decomposes\nquestions into sub-problems, solves them bottom-up using LLMs and TKG\nknowledge, and employs multi-path answer aggregation to improve fault\ntolerance. RTQA consists of three core components: the Temporal Question\nDecomposer, the Recursive Solver, and the Answer Aggregator. Experiments on\nMultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements\nin \"Multiple\" and \"Complex\" categories, outperforming state-of-the-art methods.\nOur code and data are available at https://github.com/zjukg/RTQA.", "AI": {"tldr": "RTQA是一个无需训练的新框架，通过递归分解、LLM和多路径聚合来增强时间知识图谱问答（TKGQA）中的推理能力，有效处理复杂时间查询并提高容错性。", "motivation": "现有TKGQA方法主要关注隐式时间约束，缺乏处理复杂时间查询的能力，并且在分解框架中存在推理能力有限和错误传播问题。", "method": "RTQA框架遵循递归思维，将问题递归分解为子问题，利用大型语言模型（LLM）和TKG知识自下而上地解决，并通过多路径答案聚合提高容错性。它包含三个核心组件：时间问题分解器、递归求解器和答案聚合器。", "result": "在MultiTQ和TimelineKGQA基准测试上，RTQA在“多重”和“复杂”类别中实现了显著的Hits@1改进，优于现有最先进的方法。", "conclusion": "RTQA通过增强推理和提高容错性，有效解决了TKGQA中处理复杂时间查询的挑战，且无需额外训练。"}}
{"id": "2509.04239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04239", "abs": "https://arxiv.org/abs/2509.04239", "authors": ["Arturo Valdivia", "Paolo Burelli"], "title": "Evaluating Quality of Gaming Narratives Co-created with AI", "comment": null, "summary": "This paper proposes a structured methodology to evaluate AI-generated game\nnarratives, leveraging the Delphi study structure with a panel of narrative\ndesign experts. Our approach synthesizes story quality dimensions from\nliterature and expert insights, mapping them into the Kano model framework to\nunderstand their impact on player satisfaction. The results can inform game\ndevelopers on prioritizing quality aspects when co-creating game narratives\nwith generative AI.", "AI": {"tldr": "本文提出一种结构化方法，利用专家小组和Kano模型评估AI生成的游戏叙事质量，以指导开发者。", "motivation": "帮助游戏开发者在与生成式AI共同创作游戏叙事时，了解并优先考虑影响玩家满意度的质量方面。", "method": "采用德尔菲研究结构，召集叙事设计专家小组，综合文献和专家见解的故事质量维度，并将其映射到Kano模型框架，以理解其对玩家满意度的影响。", "result": "研究结果能够为游戏开发者提供信息，指导他们在与生成式AI共同创作游戏叙事时，如何优先考虑不同的质量方面。", "conclusion": "本研究提供了一种评估AI生成游戏叙事的有效方法，其结果可用于指导游戏开发者优化AI辅助叙事创作的质量优先级，从而提升玩家满意度。"}}
{"id": "2509.04043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04043", "abs": "https://arxiv.org/abs/2509.04043", "authors": ["Yuchen Zhu", "Longxiang Yin", "Kai Zhao"], "title": "Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on \"Phytium + Cambricon\"", "comment": "16 pages,17 figures", "summary": "In the frontier research and application of current video surveillance\ntechnology, traditional camera systems exhibit significant limitations of\nresponse delay exceeding 200 ms in dynamic scenarios due to the insufficient\ndeep feature extraction capability of automatic recognition algorithms and the\nefficiency bottleneck of computing architectures, failing to meet the real-time\nrequirements in complex scenes. To address this issue, this study proposes a\nheterogeneous computing architecture based on Phytium processors and Cambricon\naccelerator cards, constructing a UAV tracking and gazing system with\nmillisecond-level response capability. At the hardware level, the system adopts\na collaborative computing architecture of Phytium FT-2000/4 processors and\nMLU220 accelerator cards, enhancing computing power through multi-card\nparallelism. At the software level, it innovatively integrates a lightweight\nYOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming\na closed-loop control chain of \"detection-tracking-feedback\". Experimental\nresults demonstrate that the system achieves a stable single-frame\ncomprehensive processing delay of 50-100 ms in 1920*1080 resolution video\nstream processing, with a multi-scale target recognition accuracy of over\n98.5%, featuring both low latency and high precision. This study provides an\ninnovative solution for UAV monitoring and the application of domestic chips.", "AI": {"tldr": "本研究提出了一种基于飞腾处理器和寒武纪加速卡的异构计算架构，用于无人机跟踪与凝视系统，实现了毫秒级响应能力，并在低延迟和高精度方面取得了显著提升。", "motivation": "传统视频监控系统在动态场景中存在超过200毫秒的响应延迟，原因在于自动识别算法的深度特征提取能力不足和计算架构的效率瓶颈，无法满足复杂场景的实时性要求。", "method": "该研究在硬件层面采用了飞腾FT-2000/4处理器和MLU220加速卡协同计算架构，并通过多卡并行增强计算能力；在软件层面，创新性地将轻量级YOLOv5s检测网络与DeepSORT级联跟踪算法集成，形成了“检测-跟踪-反馈”的闭环控制链。", "result": "实验结果表明，该系统在1920*1080分辨率视频流处理中实现了50-100毫秒的稳定单帧综合处理延迟，多尺度目标识别精度超过98.5%，兼具低延迟和高精度。", "conclusion": "本研究为无人机监测和国产芯片的应用提供了一种创新性解决方案。"}}
{"id": "2509.04013", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04013", "abs": "https://arxiv.org/abs/2509.04013", "authors": ["Riccardo Lunardi", "Vincenzo Della Mea", "Stefano Mizzaro", "Kevin Roitero"], "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs", "comment": "Accepted at ECAI 2025", "summary": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.", "AI": {"tldr": "本研究发现大型语言模型（LLMs）在处理改写后的基准问题时，虽然排名相对稳定，但绝对性能显著下降，表明其在语言变异性方面存在不足，并对当前基准评估的可靠性提出了质疑。", "motivation": "LLMs通常通过固定格式的基准进行评估，但实际应用中存在广泛的语言变异性。研究旨在系统评估LLMs对改写问题的鲁棒性，并探讨基于基准的评估是否能可靠衡量模型能力。", "method": "研究系统性地生成了六个常用基准中所有问题的各种改写版本，并测量了34个不同规模和性能的先进LLMs在这些改写输入上的有效性变化。", "result": "研究发现，LLM的排名在改写输入下保持相对稳定，但其绝对有效性分数显著下降。这表明LLMs在处理语言变异性方面存在困难，引发了对其泛化能力和评估方法的担忧。此外，观察到的性能下降挑战了基于基准评估的可靠性，暗示高基准分数可能无法完全捕捉模型对真实世界输入变化的鲁棒性。", "conclusion": "研究强调了LLM评估方法需要改进，需要开发更注重鲁棒性的基准，以更好地反映实际部署场景的需求。"}}
{"id": "2509.04310", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04310", "abs": "https://arxiv.org/abs/2509.04310", "authors": ["Yunbo Long", "Liming Xu", "Lukas Beckenbauer", "Yuhan Liu", "Alexandra Brintrup"], "title": "EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn Negotiation", "comment": null, "summary": "Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.", "AI": {"tldr": "本文提出EvoEmo，一个基于进化强化学习的框架，用于优化LLM代理在多轮谈判中的动态情感表达。实验证明EvoEmo显著优于基线策略，提高了谈判成功率、效率和买家节约。", "motivation": "现有的大型语言模型（LLM）代理在多轮谈判中忽视了情感的功能性作用，仅生成被动、偏好驱动的情感反应，使其容易受到操纵和战略性利用。本研究旨在填补这一空白。", "method": "EvoEmo框架采用进化强化学习来优化动态情感表达。它将情感状态转换建模为马尔可夫决策过程（MDP），并利用基于群体的遗传优化来演化高回报的情感策略。此外，本文提出了一个评估框架，包含两种基线策略（普通策略和固定情感策略）用于基准测试情感感知谈判。", "result": "广泛的实验和消融研究表明，EvoEmo始终优于两种基线策略，实现了更高的成功率、更高的效率和更多的买家节约。", "conclusion": "研究结果强调了自适应情感表达在使LLM代理在多轮谈判中更有效方面的重要性。"}}
{"id": "2509.04050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04050", "abs": "https://arxiv.org/abs/2509.04050", "authors": ["Quang-Huy Che", "Le-Chuong Nguyen", "Gia-Nghia Tran", "Dinh-Duy Phan", "Vinh-Tiep Nguyen"], "title": "A Re-ranking Method using K-nearest Weighted Fusion for Person Re-identification", "comment": "Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313", "summary": "In person re-identification, re-ranking is a crucial step to enhance the\noverall accuracy by refining the initial ranking of retrieved results. Previous\nstudies have mainly focused on features from single-view images, which can\ncause view bias and issues like pose variation, viewpoint changes, and\nocclusions. Using multi-view features to present a person can help reduce view\nbias. In this work, we present an efficient re-ranking method that generates\nmulti-view features by aggregating neighbors' features using K-nearest Weighted\nFusion (KWF) method. Specifically, we hypothesize that features extracted from\nre-identification models are highly similar when representing the same\nidentity. Thus, we select K neighboring features in an unsupervised manner to\ngenerate multi-view features. Additionally, this study explores the weight\nselection strategies during feature aggregation, allowing us to identify an\neffective strategy. Our re-ranking approach does not require model fine-tuning\nor extra annotations, making it applicable to large-scale datasets. We evaluate\nour method on the person re-identification datasets Market1501, MSMT17, and\nOccluded-DukeMTMC. The results show that our method significantly improves\nRank@1 and mAP when re-ranking the top M candidates from the initial ranking\nresults. Specifically, compared to the initial results, our re-ranking method\nachieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:\nMSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach\ndemonstrates substantial enhancements in computational efficiency compared to\nother re-ranking methods.", "AI": {"tldr": "本文提出了一种高效的、基于K-近邻加权融合（KWF）的重排序方法，通过无监督地聚合邻居特征来生成多视角特征，以减少视角偏差，显著提升行人重识别的准确性和计算效率。", "motivation": "行人重识别中，单视角特征易受视角偏差、姿态变化、视角改变和遮挡等问题影响。使用多视角特征可以有效减少这些偏差，从而提升重排序的整体准确性。", "method": "该研究提出了一种高效的重排序方法，通过K-近邻加权融合（KWF）方法聚合邻居特征来生成多视角特征。核心假设是相同身份的特征高度相似，因此以无监督方式选择K个邻居特征进行聚合。此外，研究还探索了特征聚合过程中的权重选择策略。该方法无需模型微调或额外标注。", "result": "在Market1501、MSMT17和Occluded-DukeMTMC数据集上的评估显示，该方法在对初始排名前M位候选进行重排序时，显著提升了Rank@1和mAP。特别是在挑战性数据集MSMT17和Occluded-DukeMTMC上，Rank@1分别提升了9.8%和22.0%。与现有重排序方法相比，该方法在计算效率上也有显著提升。", "conclusion": "所提出的无监督KWF重排序方法通过生成多视角特征，有效解决了单视角特征的局限性，显著提高了行人重识别的准确性（Rank@1和mAP）和计算效率，尤其在复杂数据集上表现出色，且无需额外标注或模型微调，具有很高的实用价值。"}}
{"id": "2509.04032", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04032", "abs": "https://arxiv.org/abs/2509.04032", "authors": ["Debangan Mishra", "Arihant Rastogi", "Agyeya Negi", "Shashwat Goel", "Ponnurangam Kumaraguru"], "title": "What if I ask in \\textit{alia lingua}? Measuring Functional Similarity Across Languages", "comment": "Preprint, 11 Pages", "summary": "How similar are model outputs across languages? In this work, we study this\nquestion using a recently proposed model similarity metric $\\kappa_p$ applied\nto 20 languages and 47 subjects in GlobalMMLU. Our analysis reveals that a\nmodel's responses become increasingly consistent across languages as its size\nand capability grow. Interestingly, models exhibit greater cross-lingual\nconsistency within themselves than agreement with other models prompted in the\nsame language. These results highlight not only the value of $\\kappa_p$ as a\npractical tool for evaluating multilingual reliability, but also its potential\nto guide the development of more consistent multilingual systems.", "AI": {"tldr": "研究发现，随着大语言模型规模和能力的增长，其跨语言输出的一致性也随之提高，且模型内部的跨语言一致性高于与其他模型的共识。", "motivation": "研究旨在探究大语言模型在不同语言间输出的相似程度。", "method": "本研究采用新提出的模型相似性度量指标 $\\kappa_p$，在包含20种语言和47个科目的GlobalMMLU数据集上进行分析。", "result": "分析结果表明，模型的输出一致性随其规模和能力的增长而在不同语言间逐渐提高。此外，模型内部的跨语言一致性高于其在相同语言下与其他模型的一致性。", "conclusion": "研究不仅突显了 $\\kappa_p$ 作为评估多语言可靠性的实用工具的价值，也揭示了其指导开发更一致的多语言系统的潜力。"}}
{"id": "2509.04317", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04317", "abs": "https://arxiv.org/abs/2509.04317", "authors": ["Isidoro Tamassia", "Wendelin Böhmer"], "title": "Improving Robustness of AlphaZero Algorithms to Test-Time Environment Changes", "comment": null, "summary": "The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub.", "AI": {"tldr": "本文探讨了在测试环境可能发生变化时，AlphaZero框架的部署问题，并提出简单修改即可显著提升其性能，即使在规划预算较低的情况下。", "motivation": "AlphaZero框架通常假设其训练环境在测试时不会改变，这限制了其适用性。研究动机在于解决AlphaZero在潜在变化测试环境中的部署问题。", "method": "通过对标准AlphaZero框架进行简单的修改，以适应可能发生变化的测试环境。", "result": "即使在规划预算较低的情况下，结合这些简单修改也能显著提升AlphaZero代理的性能。", "conclusion": "对标准AlphaZero框架进行简单修改，可以显著提高其在潜在变化测试环境中的表现，从而扩展其适用范围。"}}
{"id": "2509.04086", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.04086", "abs": "https://arxiv.org/abs/2509.04086", "authors": ["Yaru Chen", "Faegheh Sardari", "Peiliang Zhang", "Ruohao Guo", "Yang Xiang", "Zhenbo Li", "Wenwu Wang"], "title": "TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph", "comment": null, "summary": "Audio-Visual Video Parsing (AVVP) task aims to identify event categories and\ntheir occurrence times in a given video with weakly supervised labels. Existing\nmethods typically fall into two categories: (i) designing enhanced\narchitectures based on attention mechanism for better temporal modeling, and\n(ii) generating richer pseudo-labels to compensate for the absence of\nframe-level annotations. However, the first type methods treat noisy\nsegment-level pseudo labels as reliable supervision and the second type methods\nlet indiscriminate attention spread them across all frames, the initial errors\nare repeatedly amplified during training. To address this issue, we propose a\nmethod that combines the Bi-Directional Text Fusion (BiT) module and\nCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate the\nstrengths and complementarity of the two previous research directions. We first\nperform semantic injection and dynamic calibration on audio and visual modality\nfeatures through the BiT module, to locate and purify cleaner and richer\nsemantic cues. Then, we leverage the CATS module for semantic propagation and\nconnection to enable precise semantic information dissemination across time.\nExperimental results demonstrate that our proposed method achieves\nstate-of-the-art (SOTA) performance in multiple key indicators on two benchmark\ndatasets, LLP and UnAV-100.", "AI": {"tldr": "本文提出一种结合双向文本融合（BiT）和类别感知时间图（CATS）模块的方法，用于弱监督音视频解析（AVVP），以解决现有方法中伪标签噪声和注意力扩散导致的错误放大问题，并取得了最先进的性能。", "motivation": "现有AVVP方法在处理弱监督标签时存在问题：(i) 基于注意力机制的架构将噪声分段级伪标签视为可靠监督；(ii) 生成丰富伪标签的方法让无差别的注意力将错误扩散到所有帧，导致训练过程中初始错误被反复放大。", "method": "提出结合双向文本融合（BiT）模块和类别感知时间图（CATS）模块。具体而言，BiT模块对音视频模态特征进行语义注入和动态校准，以定位和提纯更清晰、更丰富的语义线索。随后，CATS模块用于语义传播和连接，以实现精确的语义信息跨时间传播。", "result": "在LLP和UnAV-100两个基准数据集上，本文提出的方法在多个关键指标上取得了最先进（SOTA）的性能。", "conclusion": "通过结合语义提纯（BiT）和精确时间传播（CATS），本文方法有效解决了弱监督AVVP中错误放大的问题，显著提升了事件类别识别和时间定位的准确性，达到了SOTA水平。"}}
{"id": "2509.04046", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.04046", "abs": "https://arxiv.org/abs/2509.04046", "authors": ["Han Xiaohui", "Zhang Yunlong", "Guo Yuxi"], "title": "A RoBERTa-Based Functional Syntax Annotation Model for Chinese Texts", "comment": "The paper includes 10 pages, 6 tables, and 4 figures. This project is\n  completed with the assistance of National Center for Language Technology and\n  Digital Economy Research (No. GJLX20250002), and is funded by Heilongjiang\n  Language Research Committee Project Construction of an Adaptive Intelligent\n  Chinese Learning Platform for International Students in China (No. G2025Y003)", "summary": "Systemic Functional Grammar and its branch, Cardiff Grammar, have been widely\napplied to discourse analysis, semantic function research, and other tasks\nacross various languages and texts. However, an automatic annotation system\nbased on this theory for Chinese texts has not yet been developed, which\nsignificantly constrains the application and promotion of relevant theories. To\nfill this gap, this research introduces a functional syntax annotation model\nfor Chinese based on RoBERTa (Robustly Optimized BERT Pretraining Approach).\nThe study randomly selected 4,100 sentences from the People's Daily 2014 corpus\nand annotated them according to functional syntax theory to establish a dataset\nfor training. The study then fine-tuned the RoBERTa-Chinese wwm-ext model based\non the dataset to implement the named entity recognition task, achieving an F1\nscore of 0.852 on the test set that significantly outperforms other comparative\nmodels. The model demonstrated excellent performance in identifying core\nsyntactic elements such as Subject (S), Main Verb (M), and Complement (C).\nNevertheless, there remains room for improvement in recognizing entities with\nimbalanced label samples. As the first integration of functional syntax with\nattention-based NLP models, this research provides a new method for automated\nChinese functional syntax analysis and lays a solid foundation for subsequent\nstudies.", "AI": {"tldr": "本研究基于RoBERTa开发了一个中文功能句法自动标注模型，在测试集上F1分数达到0.852，显著优于其他模型，为中文功能句法分析提供了新方法。", "motivation": "尽管系统功能语法及其分支卡迪夫语法在语篇分析等领域应用广泛，但中文文本的自动标注系统尚未开发，这严重限制了相关理论的应用与推广。", "method": "研究引入了基于RoBERTa的中文功能句法标注模型。首先，从《人民日报》2014年语料库中随机选取4100个句子并进行功能句法标注以构建训练数据集。然后，基于该数据集对RoBERTa-Chinese wwm-ext模型进行微调，以实现命名实体识别任务。", "result": "该模型在测试集上取得了0.852的F1分数，显著优于其他对比模型。模型在识别主语(S)、主要动词(M)和补语(C)等核心句法元素方面表现出色。然而，对于标签样本不平衡的实体识别仍有改进空间。", "conclusion": "本研究首次将功能句法与基于注意力的NLP模型相结合，为中文功能句法自动化分析提供了一种新方法，并为后续研究奠定了坚实基础。"}}
{"id": "2509.04343", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.04343", "abs": "https://arxiv.org/abs/2509.04343", "authors": ["Maciej Besta", "Shriram Chandran", "Robert Gerstenberger", "Mathis Lindner", "Marcin Chrapek", "Sebastian Hermann Martschat", "Taraneh Ghandi", "Patrick Iff", "Hubert Niewiadomski", "Piotr Nyczyk", "Jürgen Müller", "Torsten Hoefler"], "title": "Psychologically Enhanced AI Agents", "comment": null, "summary": "We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.", "AI": {"tldr": "该研究引入了“MBTI-in-Thoughts”框架，通过提示工程基于心理学（如MBTI）对大型语言模型（LLM）代理进行人格条件反射，以控制其行为，从而提高性能、改善多智能体合作和推理质量，且无需微调。", "motivation": "研究动机在于提升大型语言模型代理的有效性，并通过心理学原理赋予代理可控的行为模式，以实现行为偏向和在不同任务中的优化表现。", "method": "研究方法是“MBTI-in-Thoughts”框架，通过提示工程将基于MBTI的人格原型（如认知和情感轴）注入LLM代理。此外，该框架支持结构化多智能体通信协议，并引入了交互前的自我反思机制。为确保人格特质的持久性，集成了16Personalities测试进行自动化验证。该方法还被证明可推广到其他心理学框架，如大五人格、HEXACO和九型人格。", "result": "研究结果表明，人格启动产生了连贯且可解释的行为偏向：情感表达型代理在叙事生成方面表现出色，而分析型代理在博弈论设置中采用更稳定的策略。框架还揭示，交互前的自我反思能提高合作和推理质量。此外，该方法可无缝推广到其他心理学框架。", "conclusion": "该研究通过连接心理学理论和LLM行为设计，为在不进行任何微调的情况下创建心理增强型AI代理奠定了基础。"}}
{"id": "2509.04092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04092", "abs": "https://arxiv.org/abs/2509.04092", "authors": ["Quang-Huy Che", "Duc-Khai Lam"], "title": "TriLiteNet: Lightweight Model for Multi-Task Visual Perception", "comment": null, "summary": "Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet.", "AI": {"tldr": "本文提出TriLiteNet模型，这是一个轻量级多任务感知模型，专为ADAS设计，能在低计算成本下同时处理车辆检测、可行驶区域分割和车道线分割任务，并在嵌入式设备上表现出低延迟和合理功耗。", "motivation": "高级驾驶辅助系统（ADAS）需要高效的感知模型，以实现快速处理和响应，确保在真实世界环境中的安全性和有效性，从而满足实时执行的需求。", "method": "研究引入了TriLiteNet模型，该模型旨在同时管理全景驾驶感知相关的多项任务，并优化性能同时保持低计算成本。模型提供了不同配置，包括一个参数极少的微型版本。通过在BDD100k数据集上进行实验评估，并测试其在嵌入式设备上的延迟和功耗。", "result": "TriLiteNet_base模型在BDD100k数据集上实现了车辆检测召回率85.6%、可行驶区域分割mIoU 92.4%和车道线分割准确率82.3%，仅需2.35M参数和7.72 GFLOPs。此外，模型还提供了一个仅有0.14M参数的微型配置。两种配置的TriLiteNet模型在嵌入式设备上均表现出低延迟和合理的推理功耗。", "conclusion": "TriLiteNet通过平衡性能、计算效率和可扩展性，为真实世界的自动驾驶应用提供了一个实用且可部署的解决方案。"}}
{"id": "2509.04059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04059", "abs": "https://arxiv.org/abs/2509.04059", "authors": ["Zhilin Wang", "Zhe Yang", "Yun Luo", "Yafu Li", "Haoran Zhang", "Runzhe Zhan", "Derek F. Wong", "Jizhe Zhou", "Yu Cheng"], "title": "Synthesizing Sheet Music Problems for Evaluation and Reinforcement Learning", "comment": "11 pages", "summary": "Enhancing the ability of Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) to interpret sheet music is a crucial step toward\nbuilding AI musicians. However, current research lacks both evaluation\nbenchmarks and training data for sheet music reasoning. To address this, we\npropose the idea of synthesizing sheet music problems grounded in music theory,\nwhich can serve both as evaluation benchmarks and as training data for\nreinforcement learning with verifiable rewards (RLVR). We introduce a data\nsynthesis framework that generates verifiable sheet music questions in both\ntextual and visual modalities, leading to the Synthetic Sheet Music Reasoning\nBenchmark (SSMR-Bench) and a complementary training set. Evaluation results on\nSSMR-Bench show the importance of models' reasoning abilities in interpreting\nsheet music. At the same time, the poor performance of Gemini 2.5-Pro\nhighlights the challenges that MLLMs still face in interpreting sheet music in\na visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and\nQwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the\ntrained Qwen3-8B-Base surpasses GPT-4 in overall performance on\nMusicTheoryBench and achieves reasoning performance comparable to GPT-4 with\nthe strategies of Role play and Chain-of-Thought. Notably, its performance on\nmath problems also improves relative to the original Qwen3-8B-Base.\nFurthermore, our results show that the enhanced reasoning ability can also\nfacilitate music composition. In conclusion, we are the first to propose the\nidea of synthesizing sheet music problems based on music theory rules, and\ndemonstrate its effectiveness not only in advancing model reasoning for sheet\nmusic understanding but also in unlocking new possibilities for AI-assisted\nmusic creation.", "AI": {"tldr": "本文提出了一种基于乐理规则合成乐谱问题的方法，创建了SSMR-Bench基准和训练数据，并通过可验证奖励强化学习（RLVR）显著提升了大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的乐谱理解能力，甚至促进了音乐创作。", "motivation": "当前研究在乐谱推理方面缺乏评估基准和训练数据，而提升LLMs和MLLMs解释乐谱的能力对于构建AI音乐家至关重要。", "method": "研究人员提出并实现了一个数据合成框架，该框架基于乐理规则生成文本和视觉模态的可验证乐谱问题。这形成了合成乐谱推理基准（SSMR-Bench）及其配套训练集。他们利用这些合成数据进行可验证奖励强化学习（RLVR），以训练和改进现有模型，并评估了多种LLMs/MLLMs（如Gemini 2.5-Pro, Qwen3-8B-Base, Qwen2.5-VL-Instruct, GPT-4）的表现。", "result": "SSMR-Bench的评估结果表明了模型推理能力在乐谱解释中的重要性。Gemini 2.5-Pro表现不佳，凸显了MLLMs在视觉乐谱解释方面的挑战。通过RLVR使用合成数据，Qwen3-8B-Base和Qwen2.5-VL-Instruct在SSMR-Bench上取得了显著进步。经过训练的Qwen3-8B-Base在MusicTheoryBench上的整体性能超越了GPT-4，并在角色扮演和思维链策略下达到了与GPT-4相当的推理水平。此外，其在数学问题上的性能也得到提升，增强的推理能力还能促进音乐创作。", "conclusion": "该研究首次提出了基于乐理规则合成乐谱问题的方法，并证明了其在提升模型乐谱理解推理能力方面的有效性，同时也为AI辅助音乐创作开辟了新的可能性。"}}
{"id": "2509.04439", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04439", "abs": "https://arxiv.org/abs/2509.04439", "authors": ["Matthew Ho", "Chen Si", "Zhaoxiang Feng", "Fangxu Yu", "Zhijian Liu", "Zhiting Hu", "Lianhui Qin"], "title": "ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory", "comment": null, "summary": "While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo.", "AI": {"tldr": "该研究提出了一种“概念级记忆”方法，通过将大型语言模型（LLM）的推理过程抽象为可重用的自然语言概念，从而实现测试时期的持续学习和性能提升，特别是在ARC-AGI基准测试中表现出色。", "motivation": "LLM在推理过程中产生的洞察和模式在上下文窗口重置后即被丢弃，限制了这些发现的重用性和可扩展性。虽然外部记忆已被证明对推理密集型任务有益，但现有方法多为实例级记忆（如查询/响应对），缺乏更广泛的可重用性和可扩展性。", "method": "该方法通过从LLM的解决方案轨迹中提取“概念级记忆”：可重用、模块化的抽象，并以自然语言形式存储。在处理新查询时，选择性地检索相关概念并将其集成到提示中，从而在不更新模型权重的情况下实现测试时期的持续学习。设计中包含了新的策略，用于从推理过程中抽象出要点以及检索新查询的记忆条目。", "result": "在具有挑战性的ARC-AGI基准测试中，该方法比无记忆基线取得了7.5%的相对增益，并且性能随推理计算的增加而持续提升。研究发现，抽象概念是记忆设计中最稳定有效的方式，在所有测试的推理计算规模下都优于基线。此外，动态更新记忆在测试时期表现优于相同但固定的记忆设置，支持了通过解决更多问题和抽象更多模式到记忆中可以实现进一步解决方案的自我改进假设。", "conclusion": "通过引入概念级记忆，LLM能够将推理过程中的发现持久化为可重用的抽象概念，从而实现测试时期的持续学习和自我改进。这种方法在推理计算扩展时仍能保持性能提升，并且抽象概念被证明是最有效的记忆设计，为LLM的泛化和长期推理能力提供了新的途径。"}}
{"id": "2509.04117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04117", "abs": "https://arxiv.org/abs/2509.04117", "authors": ["Mustafa Sakhai", "Kaung Sithu", "Min Khant Soe Oke", "Maciej Wielgosz"], "title": "DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset", "comment": "12 pages, 8 figures, 3 tables; dataset descriptor paper introducing\n  DVS-PedX (synthetic-and-real event-based pedestrian dataset with baselines)\n  External URL: https://doi.org/10.5281/zenodo.17030898", "summary": "Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness\nchanges instead of full frames, offering low latency, high dynamic range, and\nmotion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a\nneuromorphic dataset designed for pedestrian detection and crossing-intention\nanalysis in normal and adverse weather conditions across two complementary\nsources: (1) synthetic event streams generated in the CARLA simulator for\ncontrolled \"approach-cross\" scenes under varied weather and lighting; and (2)\nreal-world JAAD dash-cam videos converted to event streams using the v2e tool,\npreserving natural behaviors and backgrounds. Each sequence includes paired RGB\nframes, per-frame DVS \"event frames\" (33 ms accumulations), and frame-level\nlabels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0\nevent files and AVI DVS video files and metadata for flexible re-processing.\nBaseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset\nusability and reveal a sim-to-real gap, motivating domain adaptation and\nmultimodal fusion. DVS-PedX aims to accelerate research in event-based\npedestrian safety, intention prediction, and neuromorphic perception.", "AI": {"tldr": "DVS-PedX是一个新颖的神经形态数据集，专为事件相机在正常和恶劣天气下进行行人检测和过街意图分析而设计，包含合成和真实世界事件流，并提供了基线SNN结果。", "motivation": "事件相机（如DVS）具有低延迟、高动态范围和运动鲁棒性等优势，但缺乏专门的神经形态数据集来研究行人安全和意图预测，尤其是在应对恶劣天气和模拟到真实场景差距方面。", "method": "该研究构建了DVS-PedX数据集，包含两个互补来源：1) 在CARLA模拟器中生成的合成事件流，用于受控的“接近-穿越”场景，涵盖不同天气和光照条件；2) 将真实的JAAD行车记录仪视频通过v2e工具转换为事件流，保留自然行为和背景。数据集提供配对的RGB帧、DVS“事件帧”（33毫秒累积）、帧级标签（过街与否）、原始AEDAT文件和元数据。使用SpikingJelly的基线脉冲神经网络（SNNs）来演示数据集的可用性。", "result": "基线SNNs验证了数据集的可用性，并揭示了明显的模拟到真实场景（sim-to-real）差距，这激励了领域适应和多模态融合的研究。", "conclusion": "DVS-PedX数据集旨在加速基于事件的行人安全、意图预测和神经形态感知领域的研究，并为解决模拟到真实场景的差距和多模态融合提供了研究方向。"}}
{"id": "2509.04066", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04066", "abs": "https://arxiv.org/abs/2509.04066", "authors": ["Hicham Bourhil", "Yacine El Younoussi"], "title": "Arabic Chatbot Technologies in Education: An Overview", "comment": "Published as a book chapter in: Transformaci\\'on Digital en la\n  Educaci\\'on: Innovaciones y Desaf\\'ios desde los Campus Virtuales (UA\n  Journals, 2024), pp. 11-14", "summary": "The recent advancements in Artificial Intelligence (AI) in general, and in\nNatural Language Processing (NLP) in particular, and some of its applications\nsuch as chatbots, have led to their implementation in different domains like\neducation, healthcare, tourism, and customer service. Since the COVID-19\npandemic, there has been an increasing interest in these digital technologies\nto allow and enhance remote access. In education, e-learning systems have been\nmassively adopted worldwide. The emergence of Large Language Models (LLM) such\nas BERT (Bidirectional Encoder Representations from Transformers) and GPT\n(Generative Pre-trained Transformers) made chatbots even more popular. In this\nstudy, we present a survey on existing Arabic chatbots in education and their\ndifferent characteristics such as the adopted approaches, language variety, and\nmetrics used to measure their performance. We were able to identified some\nresearch gaps when we discovered that, despite the success of chatbots in other\nlanguages such as English, only a few educational Arabic chatbots used modern\ntechniques. Finally, we discuss future directions of research in this field.", "AI": {"tldr": "本文对教育领域现有的阿拉伯语聊天机器人进行了调查，发现它们普遍缺乏使用现代AI技术，并指出了未来的研究方向。", "motivation": "人工智能和自然语言处理的进步，特别是聊天机器人在教育等领域的广泛应用，以及COVID-19疫情推动的远程学习需求，促使人们对教育领域数字技术，尤其是聊天机器人的兴趣日益增长。大型语言模型的出现进一步普及了聊天机器人。", "method": "本研究通过进行一项调查（survey），分析了现有教育领域阿拉伯语聊天机器人的不同特征，包括其采用的方法、语言种类以及用于衡量性能的指标。", "result": "研究发现了一些研究空白：尽管聊天机器人在英语等其他语言中取得了成功，但很少有教育领域的阿拉伯语聊天机器人使用了现代技术。", "conclusion": "文章讨论了该领域未来的研究方向。"}}
{"id": "2509.04123", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04123", "abs": "https://arxiv.org/abs/2509.04123", "authors": ["Ayan Banerjee", "Josep Lladós", "Umapada Pal", "Anjan Dutta"], "title": "TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering", "comment": null, "summary": "Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering.", "AI": {"tldr": "TaleDiffusion是一个新颖的框架，通过迭代过程、身份一致性自注意力机制和准确的对话分配，解决了多角色文本到故事可视化中角色一致性和对话渲染的挑战。", "motivation": "现有方法在多角色文本到故事可视化中难以保持角色一致性，导致图像伪影、对话渲染不准确和故事叙述脱节。", "method": "TaleDiffusion框架采用迭代过程：首先使用预训练LLM通过上下文学习生成每帧描述、角色细节和对话；接着，利用基于边界注意力机制的逐框掩码技术控制角色互动并减少伪影；然后，应用身份一致性自注意力机制确保跨帧角色一致性，并结合区域感知交叉注意力进行精确物体放置；最后，通过CLIPSeg将对话渲染为气泡并分配给相应角色。", "result": "实验结果表明，TaleDiffusion在一致性、噪声消除和对话渲染方面均优于现有方法。", "conclusion": "TaleDiffusion成功地解决了多角色故事可视化中的关键挑战，实现了跨帧的角色一致性、减少了伪影并准确渲染了对话，从而生成了更连贯的故事视觉内容。"}}
{"id": "2509.04077", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04077", "abs": "https://arxiv.org/abs/2509.04077", "authors": ["Rishit Tyagi", "Rahul Bouri", "Mohit Gupta"], "title": "Improving Narrative Classification and Explanation via Fine Tuned Language Models", "comment": null, "summary": "Understanding covert narratives and implicit messaging is essential for\nanalyzing bias and sentiment. Traditional NLP methods struggle with detecting\nsubtle phrasing and hidden agendas. This study tackles two key challenges: (1)\nmulti-label classification of narratives and sub-narratives in news articles,\nand (2) generating concise, evidence-based explanations for dominant\nnarratives. We fine-tune a BERT model with a recall-oriented approach for\ncomprehensive narrative detection, refining predictions using a GPT-4o pipeline\nfor consistency. For narrative explanation, we propose a ReACT (Reasoning +\nActing) framework with semantic retrieval-based few-shot prompting, ensuring\ngrounded and relevant justifications. To enhance factual accuracy and reduce\nhallucinations, we incorporate a structured taxonomy table as an auxiliary\nknowledge base. Our results show that integrating auxiliary knowledge in\nprompts improves classification accuracy and justification reliability, with\napplications in media analysis, education, and intelligence gathering.", "AI": {"tldr": "本研究通过微调BERT模型和GPT-4o管道进行多标签叙事分类，并提出ReACT框架结合辅助知识库生成证据性解释，以有效检测和解释新闻文章中的隐性叙事，提高了分类准确性和解释可靠性。", "motivation": "传统的自然语言处理方法难以检测细微的措辞和隐藏的意图，因此需要开发新的方法来理解隐性叙事和隐含信息，以分析偏见和情感，并生成简洁、基于证据的解释。", "method": "本研究采用多标签分类方法识别新闻文章中的叙事和子叙事。具体方法包括：1) 微调一个面向召回率的BERT模型进行叙事检测；2) 使用GPT-4o管道精炼BERT的预测以确保一致性；3) 提出一个ReACT（推理+行动）框架，结合基于语义检索的少样本提示，用于生成叙事解释；4) 引入结构化分类表作为辅助知识库，以提高事实准确性并减少幻觉。", "result": "研究结果表明，在提示中整合辅助知识显著提高了分类准确性和解释的可靠性。", "conclusion": "所提出的方法在检测和解释隐性叙事方面表现出色，并在媒体分析、教育和情报收集等领域具有潜在应用价值。"}}
{"id": "2509.04126", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04126", "abs": "https://arxiv.org/abs/2509.04126", "authors": ["Yuan Zhao", "Liu Lin"], "title": "MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation", "comment": null, "summary": "Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.", "AI": {"tldr": "本文提出了一种名为MEPG的多专家规划与生成框架，通过结合位置和风格感知的LLM与空间语义专家模块，显著提升了文本到图像扩散模型处理复杂多元素提示的能力以及风格多样性。", "motivation": "文本到图像扩散模型在处理复杂、多元素的提示时表现不佳，并且风格多样性有限。", "method": "该框架包含两个核心组件：1) 位置-风格感知（PSA）模块，利用经过监督微调的LLM将输入提示分解为精确的空间坐标和风格编码的语义指令；2) 多专家扩散（MED）模块，通过跨局部区域和全局区域的动态专家路由实现跨区域生成。在生成过程中，特定区域的专业模型（如写实专家、风格化专家）通过基于注意力的门控机制被选择性激活。此外，该架构支持专家模型的轻量级集成和替换，并提供一个交互式界面以进行实时空间布局编辑和按区域风格选择。", "result": "实验表明，MEPG在图像质量和风格多样性方面均显著优于使用相同骨干网络的基线模型。", "conclusion": "MEPG框架通过其独特的多专家规划与生成机制，有效解决了现有文本到图像模型在处理复杂提示和实现风格多样性方面的局限性，并展现出强大的可扩展性和交互性。"}}
{"id": "2509.04104", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04104", "abs": "https://arxiv.org/abs/2509.04104", "authors": ["Keara Schaaij", "Roel Boumans", "Tibor Bosse", "Iris Hendrickx"], "title": "Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue", "comment": "Accepted for TSD 2025", "summary": "Lexical alignment, where speakers start to use similar words across\nconversation, is known to contribute to successful communication. However, its\nimplementation in conversational agents remains underexplored, particularly\nconsidering the recent advancements in large language models (LLMs). As a first\nstep towards enabling lexical alignment in human-agent dialogue, this study\ndraws on strategies for personalising conversational agents and investigates\nthe construction of stable, personalised lexical profiles as a basis for\nlexical alignment. Specifically, we varied the amounts of transcribed spoken\ndata used for construction as well as the number of items included in the\nprofiles per part-of-speech (POS) category and evaluated profile performance\nacross time using recall, coverage, and cosine similarity metrics. It was shown\nthat smaller and more compact profiles, created after 10 min of transcribed\nspeech containing 5 items for adjectives, 5 items for conjunctions, and 10\nitems for adverbs, nouns, pronouns, and verbs each, offered the best balance in\nboth performance and data efficiency. In conclusion, this study offers\npractical insights into constructing stable, personalised lexical profiles,\ntaking into account minimal data requirements, serving as a foundational step\ntoward lexical alignment strategies in conversational agents.", "AI": {"tldr": "本研究旨在为会话代理构建稳定、个性化的词汇配置文件，以实现词汇对齐。通过调整数据量和词性项目数量，找到了在性能和数据效率之间取得最佳平衡的紧凑配置文件。", "motivation": "词汇对齐有助于成功的沟通，但在会话代理中（尤其是在大型语言模型背景下）其实现仍未得到充分探索。本研究旨在迈出实现人机对话中词汇对齐的第一步。", "method": "研究借鉴了会话代理个性化策略，通过改变用于构建配置文件的数据量（转录语音数据）以及每个词性类别中包含的项目数量，来构建稳定、个性化的词汇配置文件。使用召回率、覆盖率和余弦相似度指标随时间评估配置文件的性能。", "result": "结果表明，由10分钟转录语音数据创建的更小、更紧凑的配置文件（形容词和连词各5项，副词、名词、代词和动词各10项），在性能和数据效率方面提供了最佳平衡。", "conclusion": "本研究为构建稳定、个性化的词汇配置文件提供了实用见解，考虑了最小数据需求，是会话代理中实现词汇对齐策略的基础性一步。"}}
{"id": "2509.04180", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04180", "abs": "https://arxiv.org/abs/2509.04180", "authors": ["Safouane El Ghazouali", "Umberto Michelucci"], "title": "VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision", "comment": null, "summary": "AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.", "AI": {"tldr": "VisioFirm是一个开源的AI辅助图像标注Web应用，它结合了前沿的基础模型和交互式工具，旨在大幅减少人工标注工作量，提高效率和准确性。", "motivation": "AI模型依赖于大量的标注数据进行学习和预测，但传统标注工具需要大量手动输入，导致标注过程劳动密集且难以扩展到大型数据集。", "method": "VisioFirm采用混合AI辅助方法：集成CLIP、Ultralytics预训练检测器和Grounding DINO等零样本模型生成初始标注（采用低置信度阈值以最大化召回率）。它提供交互式工具（如边界框、定向边界框、多边形）供用户精修。此外，它利用WebGPU加速的Segment Anything实现浏览器端实时分割，并通过基于CLIP的组件聚类和IoU图抑制冗余检测来保持准确性。该工具支持YOLO、COCO、Pascal VOC、CSV等多种导出格式，并可在模型缓存后离线运行。", "result": "在COCO类型类别上，初始预测被证明大部分是正确的。VisioFirm在不同数据集的基准测试中，将人工工作量减少了高达90%，同时通过智能过滤和冗余抑制保持了高标注准确性。", "conclusion": "VisioFirm通过AI辅助自动化显著简化了图像标注过程，大幅减少了所需的人工工作量，并能保持高标注准确性，为AI数据标注提供了一个可扩展且高效的解决方案。"}}
{"id": "2509.04150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04150", "abs": "https://arxiv.org/abs/2509.04150", "authors": ["Orlando Castaneda", "Kevin So-Tang", "Kshitij Gurung"], "title": "Revisiting Simple Baselines for In-The-Wild Deepfake Detection", "comment": null, "summary": "The widespread adoption of synthetic media demands accessible deepfake\ndetectors and realistic benchmarks. While most existing research evaluates\ndeepfake detectors on highly controlled datasets, we focus on the recently\nreleased \"in-the-wild\" benchmark, Deepfake-Eval-2024. Initial reporting on\nDeepfake-Eval-2024 showed that three finetuned open-source models achieve\naccuracies between 61% and 69%, significantly lagging behind the leading\ncommercial deepfake detector with 82% accuracy. Our work revisits one of these\nbaseline approaches, originally introduced by Ojha et al., which adapts\nstandard pretrained vision backbones to produce generalizable deepfake\ndetectors. We demonstrate that with better-tuned hyperparameters, this simple\napproach actually yields much higher performance -- 81% accuracy on\nDeepfake-Eval-2024 -- surpassing the previously reported accuracy of this\nbaseline approach by 18% and competing with commercial deepfake detectors. We\ndiscuss tradeoffs in accuracy, computational costs, and interpretability,\nfocusing on how practical these deepfake detectors might be when deployed in\nreal-world settings. Our code can be found at\nhttps://github.com/Deepfake-Detection-KKO/deepfake-detection.", "AI": {"tldr": "研究表明，通过优化超参数，一个简单的开源深度伪造检测器在“in-the-wild”基准测试Deepfake-Eval-2024上的准确率可达81%，与领先的商业检测器相当，显著超越了其先前的报告性能。", "motivation": "合成媒体的广泛应用需要易于获取的深度伪造检测器和真实的基准测试。现有研究多在高度受控数据集上评估，而新发布的“in-the-wild”基准Deepfake-Eval-2024显示，开源模型表现远低于商业检测器（61%-69% vs 82%）。", "method": "重新审视了Ojha等人提出的基线方法，该方法通过调整标准预训练视觉骨干网络来生成可泛化的深度伪造检测器。核心改进在于通过更好地调整超参数来优化性能。", "result": "经过优化超参数后，该简单方法在Deepfake-Eval-2024上达到了81%的准确率，比之前报告的基线方法性能提高了18%，并与商业深度伪造检测器（82%）具有竞争力。论文还讨论了准确性、计算成本和可解释性方面的权衡。", "conclusion": "一个简单的、基于预训练视觉骨干网络的开源方法，通过精细的超参数调优，可以在实际部署场景中提供高度实用且有效的深度伪造检测能力，其性能足以与商业解决方案竞争。"}}
{"id": "2509.04111", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04111", "abs": "https://arxiv.org/abs/2509.04111", "authors": ["Dan Saattrup Smart"], "title": "MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages", "comment": null, "summary": "We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which\ncovers 306 languages. The context data comes from Wikipedia articles, with\nquestions generated by an LLM and the answers appearing verbatim in the\nWikipedia articles. We conduct a crowdsourced human evaluation of the fluency\nof the generated questions across 30 of the languages, providing evidence that\nthe questions are of good quality. We evaluate 6 different language models,\nboth decoder and encoder models of varying sizes, showing that the benchmark is\nsufficiently difficult and that there is a large performance discrepancy\namongst the languages. The dataset and survey evaluations are freely available.", "AI": {"tldr": "本文介绍了一个名为MultiWikiQA的全新阅读理解数据集，涵盖306种语言，其问题由LLM生成，答案直接取自维基百科文章。数据集经过人工评估，证实问题质量良好且具有挑战性，同时揭示了不同语言模型在不同语言上的性能差异。", "motivation": "需要一个覆盖大量语言的阅读理解数据集，以促进多语言LLM的开发和评估，并深入了解这些模型在不同语言上的表现。", "method": "通过以下步骤构建和评估数据集：1) 从维基百科文章中提取上下文。2) 使用大型语言模型（LLM）生成问题，确保答案在维基百科文章中逐字出现。3) 对30种语言的问题进行众包人工流畅性评估，以验证问题质量。4) 使用6种不同类型和规模的语言模型（包括解码器和编码器模型）进行基准测试，以评估数据集的难度和模型的性能。", "result": "研究结果表明：1) 众包人工评估证实生成的问题质量良好。2) 该基准测试对现有语言模型来说具有足够的难度。3) 在不同语言上，语言模型的性能存在显著差异。", "conclusion": "MultiWikiQA是一个高质量、具有挑战性且覆盖广泛语言的阅读理解数据集，可用于评估和推动多语言大型语言模型的发展，并揭示了模型在跨语言理解方面的表现差异。"}}
{"id": "2509.04183", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04183", "abs": "https://arxiv.org/abs/2509.04183", "authors": ["Aishik Mandal", "Tanmoy Chakraborty", "Iryna Gurevych"], "title": "MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions", "comment": "25 pages, 29 figures", "summary": "The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.", "AI": {"tldr": "本文提出MAGneT，一个多智能体框架，用于生成高质量的合成心理咨询会话数据，以解决现有数据稀缺问题，并显著提升了大型语言模型在心理咨询方面的性能。", "motivation": "可扩展心理咨询的需求日益增长，需要高质量、符合隐私规范的数据来微调开源大型语言模型（LLMs）。然而，此类数据非常稀缺，且现有单智能体方法未能充分捕捉真实咨询的复杂性。此外，现有评估协议存在不一致性。", "method": "本文引入了MAGneT，一个新颖的多智能体框架，用于生成合成心理咨询会话。它将咨询师响应生成分解为由专业LLM智能体处理的协调子任务，每个智能体模拟一种关键心理技术。与以往的单智能体方法不同，MAGneT能更好地捕捉真实咨询的结构和细微差别。此外，作者提出了一个统一的评估框架，整合了多样化的自动和专家指标，并将专家评估从之前的四个方面扩展到九个方面，以实现更全面、稳健的数据质量评估。", "result": "实验结果表明，MAGneT在生成咨询会话的质量、多样性和治疗一致性方面显著优于现有方法，平均将通用咨询技能提高3.2%，CBT特异性技能提高4.3%（基于认知疗法评级量表CTRS）。专家在77.2%的情况下更倾向于MAGneT生成的会话。更重要的是，使用MAGneT生成的会话微调开源模型，其性能比使用基线方法生成的会话微调的模型平均提升了6.3%的通用咨询技能和7.3%的CBT特异性技能（基于CTRS）。代码和数据已公开。", "conclusion": "MAGneT通过其多智能体框架成功生成了高质量、治疗对齐的合成心理咨询会话数据，有效解决了数据稀缺问题。这些数据能显著提高开源大型语言模型在心理咨询领域的性能，为可扩展心理咨询提供了有价值的资源。"}}
{"id": "2509.04193", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04193", "abs": "https://arxiv.org/abs/2509.04193", "authors": ["Ruohong Yang", "Peng Hu", "Yunfan Li", "Xi Peng"], "title": "DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval", "comment": null, "summary": "Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of\nthe same category across diverse domains without relying on annotations.\nExisting UCIR methods, which align cross-domain features for the entire image,\noften struggle with the domain gap, as the object features critical for\nretrieval are frequently entangled with domain-specific styles. To address this\nchallenge, we propose DUDE, a novel UCIR method building upon feature\ndisentanglement. In brief, DUDE leverages a text-to-image generative model to\ndisentangle object features from domain-specific styles, thus facilitating\nsemantical image retrieval. To further achieve reliable alignment of the\ndisentangled object features, DUDE aligns mutual neighbors from within domains\nto across domains in a progressive manner. Extensive experiments demonstrate\nthat DUDE achieves state-of-the-art performance across three benchmark datasets\nover 13 domains. The code will be released.", "AI": {"tldr": "DUDE是一种无监督跨域图像检索（UCIR）方法，通过利用文本到图像生成模型解耦对象特征与领域特定风格，并渐进式对齐域内和跨域的互近邻，实现了最先进的性能。", "motivation": "现有的UCIR方法在对齐整个图像的跨域特征时，对象特征常与领域特定风格纠缠，导致难以克服领域差距。", "method": "DUDE方法基于特征解耦。它利用文本到图像生成模型将对象特征从领域特定风格中解耦出来，以促进语义图像检索。为进一步实现解耦对象特征的可靠对齐，DUDE以渐进方式对齐域内到跨域的互近邻。", "result": "DUDE在三个基准数据集（涵盖13个领域）上取得了最先进的性能。", "conclusion": "DUDE通过特征解耦和渐进式互近邻对齐，有效解决了无监督跨域图像检索中的领域差距问题，并取得了显著的性能提升。"}}
{"id": "2509.04182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04182", "abs": "https://arxiv.org/abs/2509.04182", "authors": ["Wei Liu", "Michael Strube"], "title": "Joint Modeling of Entities and Discourse Relations for Coherence Assessment", "comment": "EMNLP 2025", "summary": "In linguistics, coherence can be achieved by different means, such as by\nmaintaining reference to the same set of entities across sentences and by\nestablishing discourse relations between them. However, most existing work on\ncoherence modeling focuses exclusively on either entity features or discourse\nrelation features, with little attention given to combining the two. In this\nstudy, we explore two methods for jointly modeling entities and discourse\nrelations for coherence assessment. Experiments on three benchmark datasets\nshow that integrating both types of features significantly enhances the\nperformance of coherence models, highlighting the benefits of modeling both\nsimultaneously for coherence evaluation.", "AI": {"tldr": "本研究探索了联合建模实体特征和语篇关系特征以评估连贯性的方法，实验证明结合这两种特征能显著提升连贯性模型的性能。", "motivation": "现有的连贯性建模工作大多只关注实体特征或语篇关系特征，很少有研究尝试将两者结合起来进行建模。", "method": "研究探索了两种联合建模实体和语篇关系的方法，用于连贯性评估。", "result": "在三个基准数据集上的实验表明，整合这两种类型的特征显著提高了连贯性模型的性能。", "conclusion": "同时建模实体和语篇关系对于连贯性评估具有显著优势。"}}
{"id": "2509.04243", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04243", "abs": "https://arxiv.org/abs/2509.04243", "authors": ["Wanfu Wang", "Qipeng Huang", "Guangquan Xue", "Xiaobo Liang", "Juntao Li"], "title": "Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding", "comment": null, "summary": "Vision Language Models (VLMs) have recently achieved significant progress in\nbridging visual perception and linguistic reasoning. Recently, OpenAI o3 model\nintroduced a zoom-in search strategy that effectively elicits active perception\ncapabilities in VLMs, improving downstream task performance. However, enabling\nVLMs to reason effectively over appropriate image regions remains a core\nchallenge in GUI grounding, particularly under high-resolution inputs and\ncomplex multi-element visual interactions. In this work, we propose LASER, a\nself-evolving framework that progressively endows VLMs with multi-step\nperception capabilities, enabling precise coordinate prediction. Specifically,\nour approach integrate Monte Carlo quality estimation with\nIntersection-over-Union (IoU)-based region quality evaluation to jointly\nencourage both accuracy and diversity in constructing high-quality preference\ndata. This combination explicitly guides the model to focus on\ninstruction-relevant key regions while adaptively allocating reasoning steps\nbased on task complexity. Comprehensive experiments on the ScreenSpot Pro and\nScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating\nthe effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER\nachieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new\nstate-of-the-art (SoTA) among 7B-scale models.", "AI": {"tldr": "本文提出了LASER，一个自演进框架，通过结合蒙特卡洛质量估计和IoU评估，赋予视觉语言模型（VLMs）多步感知能力，以实现精确的坐标预测，从而改进GUI接地任务中的区域推理，并在基准测试中达到了最先进的性能。", "motivation": "尽管最新的VLM（如OpenAI o3模型）引入了“放大搜索”策略以提升感知能力，但在高分辨率输入和复杂多元素视觉交互下，VLM在GUI接地任务中仍难以有效地对适当图像区域进行推理，精确的区域感知仍然是一个核心挑战。", "method": "本文提出了LASER框架，一个自演进的多步感知框架，旨在实现精确的坐标预测。具体而言，该方法将蒙特卡洛质量估计与基于IoU的区域质量评估相结合，共同鼓励在构建高质量偏好数据时兼顾准确性和多样性。这种组合明确指导模型关注指令相关的关键区域，并根据任务复杂性自适应地分配推理步骤。", "result": "在ScreenSpot Pro和ScreenSpot-v2基准测试上进行的综合实验表明，该方法取得了持续的性能提升，验证了其有效性。此外，当在GTA1-7B上进行微调时，LASER在ScreenSpot-Pro基准测试中取得了55.7分，在7B规模模型中建立了新的最先进（SoTA）水平。", "conclusion": "LASER框架通过其自演进的多步感知能力，显著提升了VLMs在GUI接地任务中对复杂高分辨率输入的区域推理和坐标预测精度，并在现有基准测试中取得了领先的性能，证明了其在增强VLM主动感知方面的有效性。"}}
{"id": "2509.04268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04268", "abs": "https://arxiv.org/abs/2509.04268", "authors": ["David Huangal", "J. Alex Hurt"], "title": "Differential Morphological Profile Neural Networks for Semantic Segmentation", "comment": "14 pages, 7 figures", "summary": "Semantic segmentation of overhead remote sensing imagery enables applications\nin mapping, urban planning, and disaster response. State-of-the-art\nsegmentation networks are typically developed and tuned on ground-perspective\nphotographs and do not directly address remote sensing challenges such as\nextreme scale variation, foreground-background imbalance, and large image\nsizes. We explore the incorporation of the differential morphological profile\n(DMP), a multi-scale shape extraction method based on grayscale morphology,\ninto modern segmentation networks. Prior studies have shown that the DMP can\nprovide critical shape information to Deep Neural Networks to enable superior\ndetection and classification performance in overhead imagery. In this work, we\nextend prior DMPNet work beyond classification and object detection by\nintegrating DMP features into three state-of-the-art convolutional and\ntransformer semantic segmentation architectures. We utilize both direct input,\nwhich adapts the input stem of feature extraction architectures to accept DMP\nchannels, and hybrid architectures, a dual-stream design that fuses RGB and DMP\nencoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP\ndifferentials and structuring element shapes to more effectively provide shape\ninformation to the model. Our results show that while non-DMP models generally\noutperform the direct-input variants, hybrid DMP consistently outperforms\ndirect-input and is capable of surpassing a non-DMP model on mIoU, F1, and\nRecall.", "AI": {"tldr": "本文探索将差分形态学剖面（DMP）整合到现代语义分割网络中，以解决遥感图像的特殊挑战，并发现混合DMP架构能有效提升分割性能。", "motivation": "现有最先进的分割网络主要针对地面视角照片开发，难以直接应对遥感图像中的极端尺度变化、前景-背景不平衡及大图像尺寸等挑战。DMP已被证明能为深度神经网络提供关键形状信息，从而提高航空图像的检测和分类性能。", "method": "将差分形态学剖面（DMP）特征整合到三种先进的卷积和Transformer语义分割架构中。采用两种整合方式：1) 直接输入，即调整特征提取架构的输入，使其接受DMP通道；2) 混合架构，采用双流设计融合RGB和DMP编码器。使用iSAID基准数据集，评估了不同DMP差分和结构元素形状的效果。", "result": "非DMP模型通常优于直接输入DMP的变体。然而，混合DMP架构持续优于直接输入DMP，并且在mIoU、F1和Recall指标上能够超越非DMP模型。", "conclusion": "将DMP特征集成到语义分割网络中，特别是通过混合双流架构，可以有效提升遥感图像的分割性能，克服了为地面视角图像设计的网络所面临的挑战。"}}
{"id": "2509.04202", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.04202", "abs": "https://arxiv.org/abs/2509.04202", "authors": ["Congbo Ma", "Yuxia Wang", "Jia Wu", "Jian Yang", "Jing Du", "Zitai Qiu", "Qing Li", "Hu Wang", "Preslav Nakov"], "title": "Explicit and Implicit Data Augmentation for Social Event Detection", "comment": null, "summary": "Social event detection involves identifying and categorizing important events\nfrom social media, which relies on labeled data, but annotation is costly and\nlabor-intensive. To address this problem, we propose Augmentation framework for\nSocial Event Detection (SED-Aug), a plug-and-play dual augmentation framework,\nwhich combines explicit text-based and implicit feature-space augmentation to\nenhance data diversity and model robustness. The explicit augmentation utilizes\nlarge language models to enhance textual information through five diverse\ngeneration strategies. For implicit augmentation, we design five novel\nperturbation techniques that operate in the feature space on structural fused\nembeddings. These perturbations are crafted to keep the semantic and relational\nproperties of the embeddings and make them more diverse. Specifically, SED-Aug\noutperforms the best baseline model by approximately 17.67% on the Twitter2012\ndataset and by about 15.57% on the Twitter2018 dataset in terms of the average\nF1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.", "AI": {"tldr": "本文提出了一种名为SED-Aug的即插即用双重数据增强框架，用于解决社交事件检测中标注数据稀缺的问题。该框架结合了显式文本增强和隐式特征空间增强，显著提升了模型性能。", "motivation": "社交事件检测依赖于标注数据，但数据标注成本高昂且耗时费力，限制了该领域的发展。", "method": "SED-Aug框架包含两部分：1) 显式增强：利用大型语言模型通过五种不同的生成策略增强文本信息；2) 隐式增强：设计了五种新颖的扰动技术，在结构融合嵌入的特征空间中操作，旨在保持嵌入的语义和关系特性同时增加多样性。", "result": "在Twitter2012数据集上，SED-Aug的平均F1分数比最佳基线模型提高了约17.67%；在Twitter2018数据集上，提高了约15.57%。", "conclusion": "SED-Aug通过结合显式文本和隐式特征空间增强，有效提升了数据多样性和模型鲁棒性，从而显著改善了社交事件检测的性能。"}}
{"id": "2509.04304", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04304", "abs": "https://arxiv.org/abs/2509.04304", "authors": ["Juraj Vladika", "Mahdi Dhaini", "Florian Matthes"], "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models", "comment": "Accepted to Findings of EMNLP 2025", "summary": "The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.", "AI": {"tldr": "LLMs在医疗领域的应用面临过时知识的风险。本研究通过构建新数据集评估LLMs，发现它们普遍依赖过时知识，并分析其原因，提出缓解方案。", "motivation": "大型语言模型（LLMs）在医疗保健领域具有巨大潜力，但它们依赖静态训练数据，这在医学建议不断演变的情况下构成重大风险。当LLMs记忆过时的医学知识时，可能提供有害建议或在临床推理任务中失败，因此有必要调查此问题。", "method": "研究引入了两个新的问答（QA）数据集，均来源于系统性综述：MedRevQA（包含16,501个通用生物医学知识QA对）和MedChangeQA（包含512个医学共识随时间变化的QA对）。研究者使用这些数据集评估了八个主流LLMs，并额外分析了过时预训练数据和训练策略的影响。", "result": "对八个主流LLMs在所构建数据集上的评估显示，所有模型都普遍依赖过时知识。", "conclusion": "LLMs在医疗领域普遍存在依赖过时知识的问题。通过分析过时预训练数据和训练策略的影响，本研究为开发更实时、可靠的医疗AI系统奠定了基础，并提出了未来的缓解方向。"}}
{"id": "2509.04269", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04269", "abs": "https://arxiv.org/abs/2509.04269", "authors": ["Yuxin Gong", "Se-in Jang", "Wei Shao", "Yi Su", "Kuang Gong"], "title": "TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D Diffusion Models", "comment": "9 pages, 4 figures, submitted to IEEE Transactions on Radiation and\n  Plasma Medical Sciences", "summary": "Accurate quantification of tau pathology via tau positron emission tomography\n(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).\nHowever, the high cost and limited availability of tau PET restrict its\nwidespread use. In contrast, structural magnetic resonance imaging (MRI) and\nplasma-based biomarkers provide non-invasive and widely available complementary\ninformation related to brain anatomy and disease progression. In this work, we\npropose a text-guided 3D diffusion model for 3D tau PET image synthesis,\nleveraging multimodal conditions from both structural MRI and plasma\nmeasurement. Specifically, the textual prompt is from the plasma p-tau217\nmeasurement, which is a key indicator of AD progression, while MRI provides\nanatomical structure constraints. The proposed framework is trained and\nevaluated using clinical AV1451 tau PET data from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. Experimental results demonstrate that\nour approach can generate realistic, clinically meaningful 3D tau PET across a\nrange of disease stages. The proposed framework can help perform tau PET data\naugmentation under different settings, provide a non-invasive, cost-effective\nalternative for visualizing tau pathology, and support the simulation of\ndisease progression under varying plasma biomarker levels and cognitive\nconditions.", "AI": {"tldr": "本文提出了一种文本引导的3D扩散模型，利用结构MRI和血浆p-tau217测量（作为文本提示）来合成3D tau PET图像，以应对tau PET成本高昂和可用性有限的问题。", "motivation": "Tau PET扫描在诊断和监测阿尔茨海默病(AD)方面至关重要，但其高成本和有限的可用性限制了广泛应用。相比之下，结构MRI和血浆生物标志物是非侵入性且广泛可用的，可以提供脑解剖和疾病进展信息。", "method": "研究者提出了一种文本引导的3D扩散模型，用于合成3D tau PET图像。该模型利用多模态条件：结构MRI提供解剖结构约束，血浆p-tau217测量（AD进展的关键指标）作为文本提示。该框架使用ADNI数据库中的临床AV1451 tau PET数据进行训练和评估。", "result": "实验结果表明，该方法能够生成逼真且具有临床意义的3D tau PET图像，涵盖了不同疾病阶段。", "conclusion": "所提出的框架可以帮助在不同设置下进行tau PET数据增强，提供一种非侵入性、经济高效的替代方案来可视化tau病理，并支持在不同血浆生物标志物水平和认知条件下模拟疾病进展。"}}
{"id": "2509.04292", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04292", "abs": "https://arxiv.org/abs/2509.04292", "authors": ["Qinyan Zhang", "Xinping Lei", "Ruijie Miao", "Yu Fu", "Haojie Fan", "Le Chang", "Jiafan Hou", "Dingling Zhang", "Zhongfei Hou", "Ziqiang Yang", "Changxin Pu", "Fei Hu", "Jingkai Liu", "Mengyun Liu", "Yang Liu", "Xiang Gao", "Jiaheng Liu", "Tong Yang", "Zaiyuan Wang", "Ge Zhang", "Wenhao Huang"], "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?", "comment": null, "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.", "AI": {"tldr": "本文提出了Inverse IFEval基准测试，用于评估大型语言模型（LLMs）在面对与训练模式冲突的指令时，克服“认知惯性”并遵循“反直觉”指令的能力，发现现有LLMs在此方面表现不足，并强调未来对齐工作应关注非常规情境下的适应性。", "motivation": "大型语言模型（LLMs）在遵循与其监督微调（SFT）中学到的标准化模式相冲突的指令时，常表现出“认知惯性”并难以执行。为了评估LLMs的这一局限性，研究者提出了Inverse IFEval。", "method": "研究者提出了Inverse IFEval基准测试，包含八种挑战类型，如问题修正、有意文本缺陷等。他们通过人机协作流程，构建了一个包含1012个高质量中英文问题的多领域数据集（涵盖23个领域），并采用优化的LLM-as-a-Judge框架进行评估。", "result": "对现有领先LLMs的实验表明，所提出的Inverse IFEval基准测试是必要的。研究结果强调，未来的对齐工作不仅应追求流畅性和事实准确性，还应考虑在非常规上下文中的适应性。", "conclusion": "Inverse IFEval可作为诊断工具和开发方法的基石，以缓解LLMs的认知惯性，减少对狭隘模式的过拟合，并最终提高LLMs在多样化和不可预测的现实场景中遵循指令的可靠性。"}}
{"id": "2509.04338", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04338", "abs": "https://arxiv.org/abs/2509.04338", "authors": ["JiYuan Wang", "Chunyu Lin", "Lei Sun", "Rongying Liu", "Lang Nie", "Mingxing Li", "Kang Liao", "Xiangxiang Chu", "Yao Zhao"], "title": "From Editor to Dense Geometry Estimator", "comment": "20pages", "summary": "Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce \\textbf{FE2E}, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100$\\times$ data.\nThe project page can be accessed \\href{https://amap-ml.github.io/FE2E/}{here}.", "AI": {"tldr": "该研究提出FE2E框架，开创性地将基于Diffusion Transformer (DiT) 架构的图像编辑模型应用于密集几何估计，通过改进的训练目标和精度处理，显著超越了基于文生图模型的方法，并在零样本深度和法线估计任务中取得了卓越性能。", "motivation": "当前密集预测任务常利用预训练的文生图（T2I）生成模型，但密集预测本质上是图生图任务。研究者认为图像编辑模型可能比文生图模型更适合作为微调的基础，因为它们可能具有固有的结构先验。", "method": "1. 系统性分析了编辑模型和生成模型在密集几何估计任务中的微调行为。2. 引入FE2E框架，首次将先进的基于Diffusion Transformer (DiT) 架构的编辑模型用于密集几何预测。3. 将编辑模型原始的流匹配损失重新表述为“一致速度”训练目标。4. 使用对数量化来解决编辑模型原生BFloat16格式与任务所需高精度之间的冲突。5. 利用DiT的全局注意力机制，实现深度和法线在单次前向传播中的无成本联合估计，使监督信号相互增强。", "result": "1. 研究发现编辑模型具有固有的结构先验，使其能够更稳定地收敛，并通过“提炼”其内在特征，最终比生成模型实现更高的性能。2. FE2E在不增加训练数据的情况下，在多个数据集上的零样本单目深度和法线估计中取得了显著的性能提升。3. 在ETH3D数据集上实现了超过35%的性能提升。4. 性能超越了训练数据量是其100倍的DepthAnything系列模型。", "conclusion": "图像编辑模型，尤其是通过FE2E框架进行适应性改造后，比文生图生成模型更适合密集几何估计任务，因为它们具有固有的结构先验。FE2E通过创新的损失函数、精度处理和联合估计策略，在零样本密集几何预测中取得了SOTA性能，证明了编辑模型在图生图任务中的巨大潜力。"}}
{"id": "2509.04273", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04273", "abs": "https://arxiv.org/abs/2509.04273", "authors": ["Junying Meng", "Gangxuan Zhou", "Jun Liu", "Weihong Guo"], "title": "Dual-Scale Volume Priors with Wasserstein-Based Consistency for Semi-Supervised Medical Image Segmentation", "comment": null, "summary": "Despite signi cant progress in semi-supervised medical image segmentation,\nmost existing segmentation networks overlook e ective methodological guidance\nfor feature extraction and important prior information from\n  datasets. In this paper, we develop a semi-supervised medical image\nsegmentation framework that e ectively integrates spatial regularization\nmethods and volume priors. Speci cally, our approach integrates a strong\nexplicit volume prior at the image scale and Threshold Dynamics spatial\nregularization, both derived from variational models, into the backbone\nsegmentation network. The target region volumes for each unlabeled image are\nestimated by a regression network, which e ectively regularizes the backbone\nsegmentation network through an image-scale Wasserstein distance constraint,\nensuring that the class ratios in the segmentation results for each unlabeled\nimage match those predicted by the regression network. Additionally, we design\na dataset-scale Wasserstein distance loss function based on a weak implicit\nvolume prior, which enforces that the volume distribution predicted for the\nunlabeled dataset is similar to that of labeled dataset. Experimental results\non the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset\nshow the superiority of the proposed method.", "AI": {"tldr": "本文提出了一种半监督医学图像分割框架，通过有效整合变分模型中的空间正则化方法和体素先验信息，显著提升了分割性能。", "motivation": "现有的半监督医学图像分割网络在特征提取的方法指导和利用数据集中的重要先验信息方面存在不足。", "method": "该方法将强显式图像尺度体素先验和阈值动力学空间正则化（均源于变分模型）整合到骨干分割网络中。通过回归网络估计未标记图像的目标区域体素，并使用图像尺度Wasserstein距离约束进行正则化。此外，还设计了一个基于弱隐式体素先验的数据集尺度Wasserstein距离损失函数，以确保未标记数据集的体素分布与已标记数据集相似。", "result": "在2017 ACDC数据集、PROMISE12数据集和股部肌肉MR图像数据集上的实验结果表明，所提出的方法具有优越性。", "conclusion": "该研究开发了一个有效的半监督医学图像分割框架，成功整合了空间正则化方法和体素先验信息，显著提升了分割性能。"}}
{"id": "2509.04357", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.04357", "abs": "https://arxiv.org/abs/2509.04357", "authors": ["Jiajun He", "Naoki Sawada", "Koichi Miyazaki", "Tomoki Toda"], "title": "PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation", "comment": "Accepted by ASRU 2025", "summary": "Automatic speech recognition (ASR) systems struggle with domain-specific\nnamed entities, especially homophones. Contextual ASR improves recognition but\noften fails to capture fine-grained phoneme variations due to limited entity\ndiversity. Moreover, prior methods treat entities as independent tokens,\nleading to incomplete multi-token biasing. To address these issues, we propose\nPhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation\n(PARCO), which integrates phoneme-aware encoding, contrastive entity\ndisambiguation, entity-level supervision, and hierarchical entity filtering.\nThese components enhance phonetic discrimination, ensure complete entity\nretrieval, and reduce false positives under uncertainty. Experiments show that\nPARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English\nDATA2 under 1,000 distractors, significantly outperforming baselines. PARCO\nalso demonstrates robust gains on out-of-domain datasets like THCHS-30 and\nLibriSpeech.", "AI": {"tldr": "PARCO通过音素感知编码、对比实体消歧和分层过滤，显著提升了上下文ASR系统对领域特定命名实体（尤其是同音词）的识别能力，并表现出强大的鲁棒性。", "motivation": "自动语音识别（ASR）系统在处理领域特定的命名实体时（特别是同音词）表现不佳。现有的上下文ASR虽然有所改进，但由于实体多样性有限，难以捕捉细微的音素差异。此外，先前方法将实体视为独立标记，导致多标记偏置不完整。", "method": "本文提出了Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation (PARCO)。该方法集成了音素感知编码、对比实体消歧、实体级监督和分层实体过滤。这些组件旨在增强语音辨别能力，确保完整的实体检索，并减少不确定性下的误报。", "result": "在包含1,000个干扰词的情况下，PARCO在中文AISHELL-1数据集上实现了4.22%的字符错误率（CER），在英文DATA2数据集上实现了11.14%的词错误率（WER），显著优于基线系统。PARCO还在域外数据集（如THCHS-30和LibriSpeech）上展现了稳健的性能提升。", "conclusion": "PARCO通过其创新的组件，有效解决了ASR系统在识别领域特定命名实体和同音词方面的挑战，显著提升了识别性能，并在不同数据集上表现出强大的鲁棒性。"}}
{"id": "2509.04379", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04379", "abs": "https://arxiv.org/abs/2509.04379", "authors": ["Jimin Xu", "Bosheng Qin", "Tao Jin", "Zhou Zhao", "Zhenhui Ye", "Jun Yu", "Fei Wu"], "title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer", "comment": null, "summary": "Recent advancements in neural representations, such as Neural Radiance Fields\nand 3D Gaussian Splatting, have increased interest in applying style transfer\nto 3D scenes. While existing methods can transfer style patterns onto\n3D-consistent neural representations, they struggle to effectively extract and\ntransfer high-level style semantics from the reference style image.\nAdditionally, the stylized results often lack structural clarity and\nseparation, making it difficult to distinguish between different instances or\nobjects within the 3D scene. To address these limitations, we propose a novel\n3D style transfer pipeline that effectively integrates prior knowledge from\npretrained 2D diffusion models. Our pipeline consists of two key stages: First,\nwe leverage diffusion priors to generate stylized renderings of key viewpoints.\nThen, we transfer the stylized key views onto the 3D representation. This\nprocess incorporates two innovative designs. The first is cross-view style\nalignment, which inserts cross-view attention into the last upsampling block of\nthe UNet, allowing feature interactions across multiple key views. This ensures\nthat the diffusion model generates stylized key views that maintain both style\nfidelity and instance-level consistency. The second is instance-level style\ntransfer, which effectively leverages instance-level consistency across\nstylized key views and transfers it onto the 3D representation. This results in\na more structured, visually coherent, and artistically enriched stylization.\nExtensive qualitative and quantitative experiments demonstrate that our 3D\nstyle transfer pipeline significantly outperforms state-of-the-art methods\nacross a wide range of scenes, from forward-facing to challenging 360-degree\nenvironments. Visit our project page https://jm-xu.github.io/SSGaussian for\nimmersive visualization.", "AI": {"tldr": "本文提出了一种新颖的3D风格迁移方法，通过整合预训练的2D扩散模型先验知识，解决了现有方法在高级风格语义提取和结构清晰度方面的不足，实现了更具结构化、视觉连贯性和艺术性的3D场景风格化。", "motivation": "现有的3D风格迁移方法（如基于神经辐射场和3D高斯泼溅）难以有效提取和迁移参考风格图像中的高级风格语义，并且风格化结果缺乏结构清晰度和实例分离性，难以区分3D场景中的不同对象。", "method": "该方法包含两个主要阶段：首先，利用扩散先验生成关键视角的风格化渲染图；然后，将这些风格化的关键视图迁移到3D表示上。其中包含两项创新设计：1) 跨视图风格对齐，在UNet的最后一个上采样块中引入跨视图注意力，确保生成风格保真和实例级别一致的关键视图；2) 实例级别风格迁移，利用风格化关键视图间的实例级别一致性，将其迁移到3D表示上。", "result": "广泛的定性和定量实验表明，该3D风格迁移管线在各种场景（从前向视图到具有挑战性的360度环境）中显著优于现有最先进的方法，产生了更具结构化、视觉连贯和艺术丰富性的风格化效果。", "conclusion": "本文提出的3D风格迁移管线通过有效整合2D扩散模型的先验知识，成功解决了现有方法在高级风格语义提取和结构清晰度方面的局限性，实现了卓越的3D场景风格化，提供了更结构化、视觉连贯且艺术性强的风格化结果。"}}
{"id": "2509.04276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04276", "abs": "https://arxiv.org/abs/2509.04276", "authors": ["Jianning Deng", "Kartic Subr", "Hakan Bilen"], "title": "PAOLI: Pose-free Articulated Object Learning from Sparse-view Images", "comment": null, "summary": "We present a novel self-supervised framework for learning articulated object\nrepresentations from sparse-view, unposed images. Unlike prior methods that\nrequire dense multi-view observations and ground-truth camera poses, our\napproach operates with as few as four views per articulation and no camera\nsupervision. To address the inherent challenges, we first reconstruct each\narticulation independently using recent advances in sparse-view 3D\nreconstruction, then learn a deformation field that establishes dense\ncorrespondences across poses. A progressive disentanglement strategy further\nseparates static from moving parts, enabling robust separation of camera and\nobject motion. Finally, we jointly optimize geometry, appearance, and\nkinematics with a self-supervised loss that enforces cross-view and cross-pose\nconsistency. Experiments on the standard benchmark and real-world examples\ndemonstrate that our method produces accurate and detailed articulated object\nrepresentations under significantly weaker input assumptions than existing\napproaches.", "AI": {"tldr": "本文提出了一种新颖的自监督框架，用于从稀疏、未摆姿态的图像中学习可变形物体的表示，显著降低了输入要求。", "motivation": "现有方法需要密集的视角观测和地面真实相机姿态，而本研究旨在仅使用少量（例如四个）视角且无需相机监督的情况下，学习可变形物体表示。", "method": "该方法首先利用稀疏视角3D重建技术独立重建每个姿态，然后学习一个变形场以建立跨姿态的密集对应关系。接着，采用渐进式解耦策略分离静态和运动部分（相机与物体运动），最后通过自监督损失（强制跨视角和跨姿态一致性）联合优化几何、外观和运动学。", "result": "实验表明，该方法在标准基准和真实世界示例上，能在比现有方法弱得多的输入假设下，生成准确且详细的可变形物体表示。", "conclusion": "该框架成功地在极少输入（稀疏、未摆姿态图像）的情况下，学习了精确的可变形物体表示，克服了现有方法的局限性。"}}
{"id": "2509.04373", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04373", "abs": "https://arxiv.org/abs/2509.04373", "authors": ["Bufan Gao", "Elisa Kreiss"], "title": "Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases", "comment": null, "summary": "As LLMs are increasingly applied in socially impactful settings, concerns\nabout gender bias have prompted growing efforts both to measure and mitigate\nsuch bias. These efforts often rely on evaluation tasks that differ from\nnatural language distributions, as they typically involve carefully constructed\ntask prompts that overtly or covertly signal the presence of gender\nbias-related content. In this paper, we examine how signaling the evaluative\npurpose of a task impacts measured gender bias in LLMs. Concretely, we test\nmodels under prompt conditions that (1) make the testing context salient, and\n(2) make gender-focused content salient. We then assess prompt sensitivity\nacross four task formats with both token-probability and discrete-choice\nmetrics. We find that even minor prompt changes can substantially alter bias\noutcomes, sometimes reversing their direction entirely. Discrete-choice metrics\nfurther tend to amplify bias relative to probabilistic measures. These findings\ndo not only highlight the brittleness of LLM gender bias evaluations but open a\nnew puzzle for the NLP benchmarking and development community: To what extent\ncan well-controlled testing designs trigger LLM ``testing mode'' performance,\nand what does this mean for the ecological validity of future benchmarks.", "AI": {"tldr": "本文研究了提示语中是否明确指出评估目的如何影响大型语言模型（LLMs）中测量的性别偏见，发现微小的提示语变化会显著改变偏见结果，并质疑现有评估方法的生态有效性。", "motivation": "随着LLMs在社会影响领域应用日益广泛，人们对性别偏见的担忧促使了对其测量和缓解的努力。然而，这些评估任务通常依赖于精心构造的提示语，与自然语言分布不同，可能无意中发出性别偏见相关内容的信号。因此，研究提示语中评估目的的信号如何影响LLMs中测量的性别偏见成为必要。", "method": "研究者在以下两种提示语条件下测试了模型：1) 使测试上下文显著；2) 使性别相关内容显著。然后，他们通过四种任务格式，使用词元概率和离散选择两种指标评估了提示语的敏感性。", "result": "研究发现，即使是微小的提示语变化也能显著改变偏见结果，有时甚至完全逆转偏见方向。此外，相对于概率度量，离散选择度量往往会放大偏见。", "conclusion": "这些发现不仅突出了LLM性别偏见评估的脆弱性，也为NLP基准测试和开发社区提出了新问题：精心控制的测试设计能在多大程度上触发LLM的“测试模式”表现，以及这对未来基准测试的生态有效性意味着什么。"}}
{"id": "2509.04298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04298", "abs": "https://arxiv.org/abs/2509.04298", "authors": ["Yingxuan Li", "Jiafeng Mao", "Yusuke Matsui"], "title": "Noisy Label Refinement with Semantically Reliable Synthetic Images", "comment": "Accepted to ICIP2025", "summary": "Semantic noise in image classification datasets, where visually similar\ncategories are frequently mislabeled, poses a significant challenge to\nconventional supervised learning approaches. In this paper, we explore the\npotential of using synthetic images generated by advanced text-to-image models\nto address this issue. Although these high-quality synthetic images come with\nreliable labels, their direct application in training is limited by domain gaps\nand diversity constraints. Unlike conventional approaches, we propose a novel\nmethod that leverages synthetic images as reliable reference points to identify\nand correct mislabeled samples in noisy datasets. Extensive experiments across\nmultiple benchmark datasets show that our approach significantly improves\nclassification accuracy under various noise conditions, especially in\nchallenging scenarios with semantic label noise. Additionally, since our method\nis orthogonal to existing noise-robust learning techniques, when combined with\nstate-of-the-art noise-robust training methods, it achieves superior\nperformance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100\nunder 70% semantic noise, and by 24% on ImageNet-100 under real-world noise\nconditions.", "AI": {"tldr": "本文提出了一种新颖的方法，利用文本到图像模型生成的合成图像作为可靠参考点，来识别和纠正真实数据集中语义噪声导致的错误标签，显著提高了图像分类的准确性。", "motivation": "图像分类数据集中存在的语义噪声（即视觉上相似但标签错误的类别）对传统的监督学习构成了严峻挑战。虽然文本到图像模型生成的合成图像带有可靠标签，但由于领域差距和多样性限制，其直接用于训练的效果有限。", "method": "本文提出一种不同于传统方法的方案，利用合成图像作为可靠的参考点，以识别和纠正噪声数据集中被错误标记的样本。该方法与现有的抗噪声学习技术正交，可以结合使用。", "result": "在多种基准数据集和噪声条件下，尤其是语义标签噪声场景下，该方法显著提高了分类准确率。与最先进的抗噪声训练方法结合后，在70%语义噪声下，CIFAR-10上准确率提高了30%，CIFAR-100上提高了11%；在真实世界噪声条件下，ImageNet-100上准确率提高了24%。", "conclusion": "利用合成图像作为可靠参考点可以有效解决图像分类数据集中语义噪声问题，显著提升分类性能，并且可以与现有抗噪声学习技术结合实现更优异的表现。"}}
{"id": "2509.04432", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04432", "abs": "https://arxiv.org/abs/2509.04432", "authors": ["Mutsumi Sasaki", "Go Kamoda", "Ryosuke Takahashi", "Kosuke Sato", "Kentaro Inui", "Keisuke Sakaguchi", "Benjamin Heinzerling"], "title": "Can Language Models Handle a Non-Gregorian Calendar?", "comment": null, "summary": "Temporal reasoning and knowledge are essential capabilities for language\nmodels (LMs). While much prior work has analyzed and improved temporal\nreasoning in LMs, most studies have focused solely on the Gregorian calendar.\nHowever, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrew\ncalendars, are in active use and reflect culturally grounded conceptions of\ntime. If and how well current LMs can accurately handle such non-Gregorian\ncalendars has not been evaluated so far. Here, we present a systematic\nevaluation of how well open-source LMs handle one such non-Gregorian system:\nthe Japanese calendar. For our evaluation, we create datasets for four tasks\nthat require both temporal knowledge and temporal reasoning. Evaluating a range\nof English-centric and Japanese-centric LMs, we find that some models can\nperform calendar conversions, but even Japanese-centric models struggle with\nJapanese-calendar arithmetic and with maintaining consistency across calendars.\nOur results highlight the importance of developing LMs that are better equipped\nfor culture-specific calendar understanding.", "AI": {"tldr": "本文系统评估了开源语言模型在处理非公历（特别是日本日历）方面的能力，发现即使是日文模型也难以进行算术运算和保持跨日历一致性。", "motivation": "语言模型的时间推理能力大多集中在公历上，但许多非公历系统（如日本、伊斯兰、希伯来日历）仍在广泛使用，反映了文化背景下的时间概念。目前尚未评估现有语言模型处理这些非公历系统的能力。", "method": "研究人员对开源语言模型处理日本日历的能力进行了系统评估。为此，他们创建了四个需要时间知识和时间推理的数据集，并评估了一系列以英语为中心和以日语为中心的语言模型。", "result": "评估结果显示，一些模型能够执行日历转换，但即使是以日语为中心的模型在处理日本日历的算术运算以及保持跨日历一致性方面也表现不佳。", "conclusion": "研究结果强调了开发更适合文化特定日历理解的语言模型的重要性。"}}
{"id": "2509.04326", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04326", "abs": "https://arxiv.org/abs/2509.04326", "authors": ["Silvio Chito", "Paolo Rabino", "Tatiana Tommasi"], "title": "Efficient Odd-One-Out Anomaly Detection", "comment": "Accepted at ICIAP 2025", "summary": "The recently introduced odd-one-out anomaly detection task involves\nidentifying the odd-looking instances within a multi-object scene. This problem\npresents several challenges for modern deep learning models, demanding spatial\nreasoning across multiple views and relational reasoning to understand context\nand generalize across varying object categories and layouts. We argue that\nthese challenges must be addressed with efficiency in mind. To this end, we\npropose a DINO-based model that reduces the number of parameters by one third\nand shortens training time by a factor of three compared to the current\nstate-of-the-art, while maintaining competitive performance. Our experimental\nevaluation also introduces a Multimodal Large Language Model baseline,\nproviding insights into its current limitations in structured visual reasoning\ntasks. The project page can be found at\nhttps://silviochito.github.io/EfficientOddOneOut/", "AI": {"tldr": "本文提出了一种基于DINO的高效模型，用于解决“格格不入”异常检测任务，该模型在保持竞争性能的同时，显著减少了参数量和训练时间。此外，还引入了多模态大语言模型作为基线，揭示了其在结构化视觉推理方面的局限性。", "motivation": "“格格不入”异常检测任务对现代深度学习模型提出了多项挑战，包括需要跨多个视图进行空间推理、理解上下文的关系推理，以及在不同物体类别和布局之间进行泛化。研究动机在于解决这些挑战时，必须兼顾效率。", "method": "研究人员提出了一种基于DINO的模型，旨在通过减少参数量来提高效率。与当前最先进的模型相比，该模型将参数量减少了三分之一，训练时间缩短了三倍。此外，实验评估还引入了一个多模态大语言模型（Multimodal Large Language Model）作为基线。", "result": "所提出的DINO模型在保持竞争性能的同时，成功将参数量减少了三分之一，训练时间缩短了三倍，优于当前最先进的模型。对多模态大语言模型的评估也揭示了其在结构化视觉推理任务中的当前局限性。", "conclusion": "研究表明，通过基于DINO的方法，可以高效地解决“格格不入”异常检测任务，显著提升效率而性能不减。同时，多模态大语言模型在处理此类需要复杂结构化视觉推理的任务时仍存在不足，提示了未来研究的方向。"}}
{"id": "2509.04403", "categories": ["cs.CV", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.04403", "abs": "https://arxiv.org/abs/2509.04403", "authors": ["Jingen Qu", "Lijun Li", "Bo Zhang", "Yichen Yan", "Jing Shao"], "title": "Self-adaptive Dataset Construction for Real-World Multimodal Safety Scenarios", "comment": "Accepted at EMNLP 2025 Findings", "summary": "Multimodal large language models (MLLMs) are rapidly evolving, presenting\nincreasingly complex safety challenges. However, current dataset construction\nmethods, which are risk-oriented, fail to cover the growing complexity of\nreal-world multimodal safety scenarios (RMS). And due to the lack of a unified\nevaluation metric, their overall effectiveness remains unproven. This paper\nintroduces a novel image-oriented self-adaptive dataset construction method for\nRMS, which starts with images and end constructing paired text and guidance\nresponses. Using the image-oriented method, we automatically generate an RMS\ndataset comprising 35k image-text pairs with guidance responses. Additionally,\nwe introduce a standardized safety dataset evaluation metric: fine-tuning a\nsafety judge model and evaluating its capabilities on other safety\ndatasets.Extensive experiments on various tasks demonstrate the effectiveness\nof the proposed image-oriented pipeline. The results confirm the scalability\nand effectiveness of the image-oriented approach, offering a new perspective\nfor the construction of real-world multimodal safety datasets.", "AI": {"tldr": "本文提出了一种新颖的图像导向自适应方法，用于构建真实世界多模态安全（RMS）数据集，并引入了统一的评估指标，以解决现有方法在覆盖复杂场景和评估效果方面的不足。", "motivation": "多模态大语言模型（MLLMs）的安全挑战日益复杂，但当前风险导向的数据集构建方法无法覆盖日益增长的真实世界多模态安全场景（RMS）。此外，缺乏统一的评估指标导致其整体有效性未经证实。", "method": "1. 提出了一种新颖的图像导向自适应数据集构建方法，该方法从图像开始，最终构建配对的文本和指导响应。2. 使用此方法自动生成了一个包含3.5万图像-文本对及指导响应的RMS数据集。3. 引入了一个标准化的安全数据集评估指标：微调一个安全判断模型，并在其他安全数据集上评估其能力。", "result": "1. 成功生成了一个包含3.5万图像-文本对及指导响应的RMS数据集。2. 在各种任务上的广泛实验证明了所提出的图像导向管道的有效性。3. 结果证实了图像导向方法的扩展性和有效性。", "conclusion": "该研究为真实世界多模态安全数据集的构建提供了一个新视角，证明了图像导向方法的有效性和可扩展性，并引入了统一的评估标准。"}}
{"id": "2509.04334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04334", "abs": "https://arxiv.org/abs/2509.04334", "authors": ["Pengyue Jia", "Yingyi Zhang", "Xiangyu Zhao", "Yixuan Li"], "title": "GeoArena: An Open Platform for Benchmarking Large Vision-language Models on WorldWide Image Geolocalization", "comment": null, "summary": "Image geolocalization aims to predict the geographic location of images\ncaptured anywhere on Earth, but its global nature presents significant\nchallenges. Current evaluation methodologies suffer from two major limitations.\nFirst, data leakage: advanced approaches often rely on large vision-language\nmodels (LVLMs) to predict image locations, yet these models are frequently\npretrained on the test datasets, compromising the accuracy of evaluating a\nmodel's actual geolocalization capability. Second, existing metrics primarily\nrely on exact geographic coordinates to assess predictions, which not only\nneglects the reasoning process but also raises privacy concerns when user-level\nlocation data is required. To address these issues, we propose GeoArena, a\nfirst open platform for evaluating LVLMs on worldwide image geolocalization\ntasks, offering true in-the-wild and human-centered benchmarking. GeoArena\nenables users to upload in-the-wild images for a more diverse evaluation\ncorpus, and it leverages pairwise human judgments to determine which model\noutput better aligns with human expectations. Our platform has been deployed\nonline for two months, during which we collected over thousands voting records.\nBased on this data, we conduct a detailed analysis and establish a leaderboard\nof different LVLMs on the image geolocalization task.", "AI": {"tldr": "本文提出了GeoArena，一个开放平台，用于解决图像地理定位任务中现有评估方法的数据泄露和指标局限性问题，通过人类判断对大型视觉-语言模型（LVLMs）进行真实世界的基准测试。", "motivation": "当前图像地理定位评估存在两大局限：一是数据泄露，即用于预测的LVLMs常在测试数据集上预训练，影响评估准确性；二是现有指标主要依赖精确地理坐标，忽略推理过程并引发隐私担忧。", "method": "GeoArena是一个开放平台，用于评估全球图像地理定位任务中的LVLMs。它允许用户上传真实世界图像以丰富评估语料库，并利用成对的人类判断来评估模型输出与人类期望的匹配程度。", "result": "GeoArena平台已在线部署两个月，收集了数千条投票记录。基于这些数据，作者进行了详细分析，并建立了不同LVLMs在图像地理定位任务上的排行榜。", "conclusion": "GeoArena提供了一个新颖的、以人为中心的、真实世界图像地理定位LVLM评估平台，有效解决了现有评估方法的局限性，并能更准确地衡量模型的实际能力。"}}
{"id": "2509.04438", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04438", "abs": "https://arxiv.org/abs/2509.04438", "authors": ["Sabbir Mollah", "Rohit Gupta", "Sirnam Swetha", "Qingyang Liu", "Ahnaf Munir", "Mubarak Shah"], "title": "The Telephone Game: Evaluating Semantic Drift in Unified Models", "comment": null, "summary": "Employing a single, unified model (UM) for both visual understanding\n(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened\na new direction in Visual Language Model (VLM) research. While UMs can also\nsupport broader unimodal tasks (e.g., text-to-text, image-to-image), we focus\non the core cross-modal pair T2I and I2T, as consistency between understanding\nand generation is critical for downstream use. Existing evaluations consider\nthese capabilities in isolation: FID and GenEval for T2I, and benchmarks such\nas MME, MMBench for I2T. These single-pass metrics do not reveal whether a\nmodel that understands a concept can also render it, nor whether meaning is\npreserved when cycling between image and text modalities. To address this, we\nintroduce the Unified Consistency Framework for Unified Models (UCF-UM), a\ncyclic evaluation protocol that alternates I2T and T2I over multiple\ngenerations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean\nCumulative Drift (MCD), an embedding-based measure of overall semantic loss;\n(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)\nMulti-Generation GenEval (MGG), an object-level compliance score extending\nGenEval. To assess generalization beyond COCO, which is widely used in\ntraining; we create a new benchmark ND400, sampled from NoCaps and DOCCI and\nevaluate on seven recent models. UCF-UM reveals substantial variation in\ncross-modal stability: some models like BAGEL maintain semantics over many\nalternations, whereas others like Vila-u drift quickly despite strong\nsingle-pass scores. Our results highlight cyclic consistency as a necessary\ncomplement to standard I2T and T2I evaluations, and provide practical metrics\nto consistently assess unified model's cross-modal stability and strength of\ntheir shared representations. Code:\nhttps://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models", "AI": {"tldr": "本文提出了一种名为UCF-UM的循环评估框架，用于衡量统一视觉语言模型在图像到文本(I2T)和文本到图像(T2I)任务之间语义漂移。通过多代交替评估和新指标（MCD、SDR、MGG）及新基准（ND400），研究发现不同模型的跨模态稳定性差异显著，揭示了循环一致性作为标准评估的必要补充。", "motivation": "现有评估方法（如FID、GenEval用于T2I；MME、MMBench用于I2T）孤立地衡量统一模型的理解和生成能力。它们无法揭示模型是否能同时理解和生成同一概念，也无法评估在图像和文本模态之间循环时语义是否保持一致。然而，理解与生成之间的一致性对于下游应用至关重要。", "method": "研究引入了“统一模型统一一致性框架”（UCF-UM），这是一种循环评估协议，通过多次交替进行I2T和T2I任务来量化语义漂移。UCF-UM提出了三个指标：(i) 平均累积漂移 (MCD)，基于嵌入的整体语义损失度量；(ii) 语义漂移率 (SDR)，总结语义衰减率；(iii) 多代GenEval (MGG)，一个扩展GenEval的对象级依从性得分。此外，为评估模型在COCO之外的泛化能力，创建了新的基准ND400，并对七个近期模型进行了评估。", "result": "UCF-UM揭示了跨模态稳定性在不同模型之间存在显著差异。例如，BAGEL等模型在多次交替后仍能保持语义，而Vila-u等模型尽管单次通过得分很高，但漂移速度很快。", "conclusion": "循环一致性是标准I2T和T2I评估的必要补充。UCF-UM提供了实用的指标，可以一致地评估统一模型的跨模态稳定性和其共享表示的强度。"}}
{"id": "2509.04344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04344", "abs": "https://arxiv.org/abs/2509.04344", "authors": ["Feng-Qi Cui", "Zhen Lin", "Xinlong Rao", "Anyang Tong", "Shiyao Li", "Fei Wang", "Changlin Chen", "Bin Liu"], "title": "MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition", "comment": "Accepted by IEEE ISPA2025", "summary": "Dynamic facial expression recognition (DFER) faces significant challenges due\nto long-tailed category distributions and complexity of spatio-temporal feature\nmodeling. While existing deep learning-based methods have improved DFER\nperformance, they often fail to address these issues, resulting in severe model\ninduction bias. To overcome these limitations, we propose a novel\nmulti-instance learning framework called MICACL, which integrates\nspatio-temporal dependency modeling and long-tailed contrastive learning\noptimization. Specifically, we design the Graph-Enhanced Instance Interaction\nModule (GEIIM) to capture intricate spatio-temporal between adjacent instances\nrelationships through adaptive adjacency matrices and multiscale convolutions.\nTo enhance instance-level feature aggregation, we develop the Weighted Instance\nAggregation Network (WIAN), which dynamically assigns weights based on instance\nimportance. Furthermore, we introduce a Multiscale Category-aware Contrastive\nLearning (MCCL) strategy to balance training between major and minor\ncategories. Extensive experiments on in-the-wild datasets (i.e., DFEW and\nFERV39k) demonstrate that MICACL achieves state-of-the-art performance with\nsuperior robustness and generalization.", "AI": {"tldr": "本文提出了一种名为MICACL的多实例学习框架，用于动态面部表情识别（DFER），通过整合时空依赖建模和长尾对比学习优化，解决了长尾分布和时空特征建模的挑战，并取得了最先进的性能。", "motivation": "动态面部表情识别（DFER）面临长尾类别分布和复杂的时空特征建模带来的显著挑战。现有的深度学习方法往往未能有效解决这些问题，导致模型存在严重的归纳偏差。", "method": "本文提出了一种新颖的多实例学习框架MICACL，它集成了时空依赖建模和长尾对比学习优化。具体方法包括：\n1.  **图增强实例交互模块（GEIIM）**：通过自适应邻接矩阵和多尺度卷积，捕获相邻实例之间复杂的时空关系。\n2.  **加权实例聚合网络（WIAN）**：根据实例重要性动态分配权重，增强实例级别的特征聚合。\n3.  **多尺度类别感知对比学习（MCCL）策略**：平衡主要类别和次要类别之间的训练。", "result": "在野外数据集（即DFEW和FERV39k）上的大量实验表明，MICACL实现了最先进的性能，并展现出卓越的鲁棒性和泛化能力。", "conclusion": "MICACL框架通过有效解决长尾类别分布和复杂时空特征建模问题，显著提升了动态面部表情识别的性能，达到了最先进水平。"}}
{"id": "2509.04370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04370", "abs": "https://arxiv.org/abs/2509.04370", "authors": ["Dor Cohen", "Inga Efrosman", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Stitching the Story: Creating Panoramic Incident Summaries from Body-Worn Footage", "comment": "5 pages, 3 figures", "summary": "First responders widely adopt body-worn cameras to document incident scenes\nand support post-event analysis. However, reviewing lengthy video footage is\nimpractical in time-critical situations. Effective situational awareness\ndemands a concise visual summary that can be quickly interpreted. This work\npresents a computer vision pipeline that transforms body-camera footage into\ninformative panoramic images summarizing the incident scene. Our method\nleverages monocular Simultaneous Localization and Mapping (SLAM) to estimate\ncamera trajectories and reconstruct the spatial layout of the environment. Key\nviewpoints are identified by clustering camera poses along the trajectory, and\nrepresentative frames from each cluster are selected. These frames are fused\ninto spatially coherent panoramic images using multi-frame stitching\ntechniques. The resulting summaries enable rapid understanding of complex\nenvironments and facilitate efficient decision-making and incident review.", "AI": {"tldr": "该研究提出一种计算机视觉流程，将执法记录仪视频转换为全景图像，以提供事件现场的简洁视觉摘要，从而提高态势感知能力。", "motivation": "急救人员广泛使用执法记录仪，但在时间紧迫的情况下，审查冗长的视频不切实际。有效态势感知需要能够快速解读的简洁视觉摘要。", "method": "该方法利用单目同步定位与建图（SLAM）估计相机轨迹并重建环境空间布局。通过聚类相机姿态识别关键视角，并从每个聚类中选择代表性帧。这些帧使用多帧拼接技术融合成空间连贯的全景图像。", "result": "该流程将执法记录仪视频转换为信息丰富的全景图像，概括了事件现场。生成的摘要能实现对复杂环境的快速理解。", "conclusion": "所产生的摘要通过快速理解复杂环境，促进了高效的决策制定和事件回顾。"}}
{"id": "2509.04376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04376", "abs": "https://arxiv.org/abs/2509.04376", "authors": ["Hao Ju", "Hu Zhang", "Zhedong Zheng"], "title": "AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval for Text-Based Person Anomaly Search", "comment": null, "summary": "With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research.", "AI": {"tldr": "AnomalyLMM是首个利用大型多模态模型（LMMs）进行基于文本的人员异常行为搜索的框架，通过粗到细的管道和免训练的适应策略，有效解决了细粒度跨模态对齐和稀疏样本下的异常识别挑战。", "motivation": "公共安全需求日益增长，催生了基于文本的人员异常搜索任务。该任务面临两大挑战：细粒度跨模态对齐（文本异常与视觉行为）和稀疏样本下的异常识别。尽管大型多模态模型（LMMs）在多模态理解方面表现出色，但其在细粒度异常检索中的潜力尚未充分挖掘，主要受限于生成性知识与判别性检索之间的领域差距，以及缺乏高效的部署适应策略。", "method": "本文提出了AnomalyLMM框架，首次将LMMs应用于基于文本的人员异常搜索。其核心方法包括：1) 一个新颖的粗到细管道，将LMM的生成性世界知识与以检索为中心的异常检测相结合；2) 一个免训练的适应策略“食谱”，包含掩码跨模态提示、行为显著性预测和知识感知重排序，实现了对细微异常线索的零样本聚焦。", "result": "在唯一的公开基准PAB数据集上进行了严格评估，结果表明AnomalyLMM的有效性，其Recall@1准确率超越了竞争基线0.96%。值得注意的是，该方法还通过定性分析揭示了文本异常与视觉行为之间可解释的对齐关系。", "conclusion": "AnomalyLMM成功地利用LMMs解决了基于文本的人员异常搜索任务中的关键挑战，实现了对异常行为的有效检索，并提供了可解释的跨模态对齐。该研究为LMMs在该领域的应用开辟了新方向，并提供了可供未来研究使用的代码和模型。"}}
{"id": "2509.04378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04378", "abs": "https://arxiv.org/abs/2509.04378", "authors": ["Yilin Tao", "Jiashui Huang", "Huaze Xu", "Ling Shao"], "title": "Aesthetic Image Captioning with Saliency Enhanced MLLMs", "comment": null, "summary": "Aesthetic Image Captioning (AIC) aims to generate textual descriptions of\nimage aesthetics, becoming a key research direction in the field of\ncomputational aesthetics. In recent years, pretrained Multimodal Large Language\nModels (MLLMs) have advanced rapidly, leading to a significant increase in\nimage aesthetics research that integrates both visual and textual modalities.\nHowever, most existing studies on image aesthetics primarily focus on\npredicting aesthetic ratings and have shown limited application in AIC.\nExisting AIC works leveraging MLLMs predominantly rely on fine-tuning methods\nwithout specifically adapting MLLMs to focus on target aesthetic content. To\naddress this limitation, we propose the Aesthetic Saliency Enhanced Multimodal\nLarge Language Model (ASE-MLLM), an end-to-end framework that explicitly\nincorporates aesthetic saliency into MLLMs. Within this framework, we introduce\nthe Image Aesthetic Saliency Module (IASM), which efficiently and effectively\nextracts aesthetic saliency features from images. Additionally, we design\nIAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency\nfeatures with original image features via a cross-attention mechanism. To the\nbest of our knowledge, ASE-MLLM is the first framework to integrate image\naesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments\ndemonstrated that our approach significantly outperformed traditional methods\nand generic MLLMs on current mainstream AIC benchmarks, achieving\nstate-of-the-art (SOTA) performance.", "AI": {"tldr": "本文提出了ASE-MLLM，一个端到端框架，首次将图像美学显著性明确整合到多模态大语言模型（MLLMs）中，专门用于美学图像描述（AIC）任务，并在主流基准测试中取得了最先进的性能。", "motivation": "现有的图像美学研究主要关注美学评分预测，在美学图像描述（AIC）中的应用有限。尽管预训练多模态大语言模型（MLLMs）发展迅速，但现有利用MLLMs的AIC工作主要依赖微调，未能专门使MLLMs关注目标美学内容。", "method": "本文提出了美学显著性增强多模态大语言模型（ASE-MLLM），这是一个端到端框架，明确将美学显著性融入MLLMs。该框架引入了图像美学显著性模块（IASM）来高效提取美学显著性特征。此外，设计了IAS-ViT作为MLLMs的图像编码器，通过交叉注意力机制将美学显著性特征与原始图像特征融合。", "result": "广泛的实验表明，ASE-MLLM在当前主流AIC基准测试中显著优于传统方法和通用MLLMs，达到了最先进（SOTA）的性能。", "conclusion": "ASE-MLLM成功地将图像美学显著性整合到多模态大语言模型中，解决了现有AIC方法的局限性，并在美学图像描述任务上取得了显著的性能提升，实现了最先进的成果。"}}
{"id": "2509.04402", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04402", "abs": "https://arxiv.org/abs/2509.04402", "authors": ["Tingyou Li", "Zixin Xu", "Zirui Gao", "Hanfei Yan", "Xiaojing Huang", "Jizhou Li"], "title": "Learning neural representations for X-ray ptychography reconstruction with unknown probes", "comment": null, "summary": "X-ray ptychography provides exceptional nanoscale resolution and is widely\napplied in materials science, biology, and nanotechnology. However, its full\npotential is constrained by the critical challenge of accurately reconstructing\nimages when the illuminating probe is unknown. Conventional iterative methods\nand deep learning approaches are often suboptimal, particularly under the\nlow-signal conditions inherent to low-dose and high-speed experiments. These\nlimitations compromise reconstruction fidelity and restrict the broader\nadoption of the technique. In this work, we introduce the Ptychographic\nImplicit Neural Representation (PtyINR), a self-supervised framework that\nsimultaneously addresses the object and probe recovery problem. By\nparameterizing both as continuous neural representations, PtyINR performs\nend-to-end reconstruction directly from raw diffraction patterns without\nrequiring any pre-characterization of the probe. Extensive evaluations\ndemonstrate that PtyINR achieves superior reconstruction quality on both\nsimulated and experimental data, with remarkable robustness under challenging\nlow-signal conditions. Furthermore, PtyINR offers a generalizable,\nphysics-informed framework for addressing probe-dependent inverse problems,\nmaking it applicable to a wide range of computational microscopy problems.", "AI": {"tldr": "本文提出PtyINR，一个自监督隐式神经表示框架，用于在X射线叠层成像中同时重建未知探针和物体，在低信号条件下表现出卓越的重建质量和鲁棒性。", "motivation": "X射线叠层成像在纳米尺度分辨率方面表现出色，但当照明探针未知时，准确图像重建面临关键挑战。传统迭代方法和深度学习方法在低剂量和高速实验固有的低信号条件下表现不佳，这限制了重建保真度并阻碍了该技术的广泛应用。", "method": "本文引入了叠层成像隐式神经表示（PtyINR），这是一个自监督框架，通过将物体和探针都参数化为连续的神经表示来解决物体和探针的恢复问题。PtyINR直接从原始衍射图样进行端到端重建，无需预先表征探针。", "result": "广泛评估表明，PtyINR在模拟和实验数据上均实现了卓越的重建质量，并在挑战性的低信号条件下表现出显著的鲁棒性。", "conclusion": "PtyINR提供了一个可泛化的、物理信息驱动的框架，用于解决依赖于探针的逆问题，使其适用于各种计算显微镜问题。"}}
{"id": "2509.04406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04406", "abs": "https://arxiv.org/abs/2509.04406", "authors": ["Zanwei Zhou", "Taoran Yi", "Jiemin Fang", "Chen Yang", "Lingxi Xie", "Xinggang Wang", "Wei Shen", "Qi Tian"], "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation", "comment": "Project page: https://github.com/Zanue/MDT-dist", "summary": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.", "AI": {"tldr": "本文提出MDT-dist框架，通过引入速度匹配（VM）和速度蒸馏（VD）优化目标，显著加速了基于流的3D生成模型（如TRELLIS）的推理过程，同时保持了高视觉和几何保真度。", "motivation": "基于流的3D生成模型通常需要数十个采样步骤进行推理，导致速度较慢。尽管一致性模型（CMs）等少数步骤蒸馏方法在加速2D扩散模型方面取得了显著进展，但它们在更复杂的3D生成任务中仍未得到充分探索。", "method": "本文提出了MDT-dist框架，其主要目标是蒸馏预训练模型以学习边际数据传输（Marginal-Data Transport）。由于直接学习该目标需要对速度场进行积分，而这在计算上是不可行的。因此，本文提出了两个可优化的目标：速度匹配（Velocity Matching, VM）和速度蒸馏（Velocity Distillation, VD）。VM旨在稳定地匹配学生模型和教师模型之间的速度场，但可能提供有偏的梯度估计。VD通过利用学习到的速度场进行概率密度蒸馏，进一步增强了优化过程。", "result": "在先驱3D生成框架TRELLIS上进行评估，MDT-dist将每个流转换器的采样步骤从25步减少到1或2步。在A800上实现了0.68秒（1步x2）和0.94秒（2步x2）的延迟，分别带来了9.0倍和6.5倍的加速，同时保持了高视觉和几何保真度。实验表明，该方法显著优于现有的一致性模型蒸馏方法，并使TRELLIS在少步3D生成中表现出卓越的性能。", "conclusion": "MDT-dist框架通过新颖的VM和VD优化目标，成功实现了基于流的3D生成模型的少步蒸馏，显著提高了推理速度，同时保持了生成质量，并在3D生成任务中超越了现有的一致性模型蒸馏方法。"}}
{"id": "2509.04434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04434", "abs": "https://arxiv.org/abs/2509.04434", "authors": ["Hyunsoo Cha", "Byungjun Kim", "Hanbyul Joo"], "title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer", "comment": "Project Page: https://hyunsoocha.github.io/durian", "summary": "We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.", "AI": {"tldr": "Durian是首个零样本人像动画生成方法，通过双参考网络实现面部属性从参考图像到目标人像的高保真和空间一致性迁移。", "motivation": "现有方法难以在零样本设置下，实现高保真且空间一致的人像动画生成及面部属性迁移。研究旨在解决这一挑战。", "method": "Durian引入了双参考网络，将人像和属性图像的空间特征注入扩散模型的去噪过程。模型采用自重建训练，通过从同一视频采样两帧（一帧作属性参考，一帧作目标人像）并重建其余帧。为支持不同空间范围的属性迁移，提出了基于关键点条件图像生成的掩码扩展策略。此外，通过空间和外观级变换增强属性和人像图像，以提高对位置不对齐的鲁棒性。", "result": "Durian在带属性迁移的人像动画生成方面达到了最先进的性能。其双参考设计使得在单次生成中无需额外训练即可实现多属性组合。", "conclusion": "Durian有效解决了零样本人像动画属性迁移问题，展现出强大的泛化能力和鲁棒性，并在多属性组合方面表现出色，为该领域树立了新标准。"}}
{"id": "2509.04437", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.04437", "abs": "https://arxiv.org/abs/2509.04437", "authors": ["Benjamin El-Zein", "Dominik Eckert", "Andreas Fieselmann", "Christopher Syben", "Ludwig Ritschl", "Steffen Kappler", "Sebastian Stober"], "title": "From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray Collimators via Hough Transform", "comment": null, "summary": "Collimation in X-ray imaging restricts exposure to the region-of-interest\n(ROI) and minimizes the radiation dose applied to the patient. The detection of\ncollimator shadows is an essential image-based preprocessing step in digital\nradiography posing a challenge when edges get obscured by scattered X-ray\nradiation. Regardless, the prior knowledge that collimation forms\npolygonal-shaped shadows is evident. For this reason, we introduce a deep\nlearning-based segmentation that is inherently constrained to its geometry. We\nachieve this by incorporating a differentiable Hough transform-based network to\ndetect the collimation borders and enhance its capability to extract the\ninformation about the ROI center. During inference, we combine the information\nof both tasks to enable the generation of refined, line-constrained\nsegmentation masks. We demonstrate robust reconstruction of collimated regions\nachieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real\nXray images. While this application involves at most four shadow borders, our\nmethod is not fundamentally limited by a specific number of edges.", "AI": {"tldr": "本文提出了一种基于深度学习的、几何约束的分割方法，用于在X射线图像中检测准直器阴影，即使在散射X射线导致边缘模糊的情况下也能实现鲁棒的重建。", "motivation": "X射线成像中的准直器阴影检测是限制曝光区域和最小化患者辐射剂量的关键预处理步骤。然而，散射X射线会导致边缘模糊，使检测变得困难。鉴于准直器阴影通常呈多边形，研究旨在利用这一先验知识来克服检测挑战。", "method": "该方法引入了一种深度学习分割模型，其几何形状受到内在约束。它集成了一个可微分的霍夫变换网络来检测准直器边界，并增强了提取ROI中心信息的能力。在推理阶段，结合这两个任务的信息来生成精细的、受线条约束的分割掩模。", "result": "该方法在多样化的真实X射线图像测试集上，实现了准直区域的鲁棒重建，中位数豪斯多夫距离为4.3-5.0毫米。尽管该应用涉及最多四个阴影边界，但该方法不限于特定数量的边缘。", "conclusion": "研究成功开发了一种鲁棒的、几何约束的深度学习方法，用于检测X射线图像中的准直器阴影，能够生成精细的、受线条约束的分割掩模，并在实际图像上表现出良好的准确性。"}}
{"id": "2509.04444", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04444", "abs": "https://arxiv.org/abs/2509.04444", "authors": ["Xin Lin", "Xian Ge", "Dizhe Zhang", "Zhaoliang Wan", "Xianshun Wang", "Xiangtai Li", "Wenjie Jiang", "Bo Du", "Dacheng Tao", "Ming-Hsuan Yang", "Lu Qi"], "title": "One Flight Over the Gap: A Survey from Perspective to Panoramic Vision", "comment": null, "summary": "Driven by the demand for spatial intelligence and holistic scene perception,\nomnidirectional images (ODIs), which provide a complete 360\\textdegree{} field\nof view, are receiving growing attention across diverse applications such as\nvirtual reality, autonomous driving, and embodied robotics. Despite their\nunique characteristics, ODIs exhibit remarkable differences from perspective\nimages in geometric projection, spatial distribution, and boundary continuity,\nmaking it challenging for direct domain adaption from perspective methods. This\nsurvey reviews recent panoramic vision techniques with a particular emphasis on\nthe perspective-to-panorama adaptation. We first revisit the panoramic imaging\npipeline and projection methods to build the prior knowledge required for\nanalyzing the structural disparities. Then, we summarize three challenges of\ndomain adaptation: severe geometric distortions near the poles, non-uniform\nsampling in Equirectangular Projection (ERP), and periodic boundary continuity.\nBuilding on this, we cover 20+ representative tasks drawn from more than 300\nresearch papers in two dimensions. On one hand, we present a cross-method\nanalysis of representative strategies for addressing panoramic specific\nchallenges across different tasks. On the other hand, we conduct a cross-task\ncomparison and classify panoramic vision into four major categories: visual\nquality enhancement and assessment, visual understanding, multimodal\nunderstanding, and visual generation. In addition, we discuss open challenges\nand future directions in data, models, and applications that will drive the\nadvancement of panoramic vision research. We hope that our work can provide new\ninsight and forward looking perspectives to advance the development of\npanoramic vision technologies. Our project page is\nhttps://insta360-research-team.github.io/Survey-of-Panorama", "AI": {"tldr": "本综述全面回顾了全景图像（ODIs）领域，重点关注从透视图像到全景图像的适应性问题。它总结了主要挑战、分析了20多个代表性任务及其解决方法，并将全景视觉分为四大类，并展望了未来的研究方向。", "motivation": "随着虚拟现实、自动驾驶和具身机器人等应用对空间智能和整体场景感知的需求增长，全景图像（提供完整360度视野）日益受到关注。然而，全景图像在几何投影、空间分布和边界连续性方面与透视图像存在显著差异，使得直接应用透视方法面临挑战，因此需要专门的适应性研究。", "method": "本研究通过以下方法进行：1) 回顾全景成像流程和投影方法，以理解结构差异。2) 总结透视到全景领域适应的三个主要挑战：极点附近的几何畸变、等距柱状投影（ERP）中的非均匀采样和周期性边界连续性。3) 从300多篇论文中选取20多个代表性任务，进行跨方法分析以探讨解决挑战的策略。4) 进行跨任务比较，将全景视觉分为四大类：视觉质量增强与评估、视觉理解、多模态理解和视觉生成。5) 讨论数据、模型和应用方面的开放挑战和未来方向。", "result": "本研究的结果包括：1) 明确了全景视觉领域适应的三个核心挑战：严重的极点几何畸变、等距柱状投影（ERP）中的非均匀采样以及周期性边界连续性。2) 总结了应对这些挑战的代表性策略。3) 将全景视觉任务划分为四大主要类别：视觉质量增强与评估、视觉理解、多模态理解和视觉生成。4) 提供了跨方法和跨任务的深入分析。", "conclusion": "本综述为全景视觉领域提供了新的见解和前瞻性视角，旨在推动全景视觉技术的发展。它不仅系统地梳理了现有技术和挑战，还指出了数据、模型和应用方面的开放挑战和未来研究方向，为研究人员提供了宝贵的参考。"}}
{"id": "2509.04446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04446", "abs": "https://arxiv.org/abs/2509.04446", "authors": ["Kiymet Akdemir", "Jing Shi", "Kushal Kafle", "Brian Price", "Pinar Yanardag"], "title": "Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing with Text-to-Image Diffusion Models", "comment": null, "summary": "Text-to-image diffusion models have demonstrated significant capabilities to\ngenerate diverse and detailed visuals in various domains, and story\nvisualization is emerging as a particularly promising application. However, as\ntheir use in real-world creative domains increases, the need for providing\nenhanced control, refinement, and the ability to modify images post-generation\nin a consistent manner becomes an important challenge. Existing methods often\nlack the flexibility to apply fine or coarse edits while maintaining visual and\nnarrative consistency across multiple frames, preventing creators from\nseamlessly crafting and refining their visual stories. To address these\nchallenges, we introduce Plot'n Polish, a zero-shot framework that enables\nconsistent story generation and provides fine-grained control over story\nvisualizations at various levels of detail.", "AI": {"tldr": "本文提出Plot'n Polish，一个零样本框架，用于实现文本到图像故事生成的一致性，并提供对故事视觉化细粒度控制。", "motivation": "文本到图像扩散模型在故事视觉化方面潜力巨大，但现有方法在生成后图像的控制、精修和一致性修改方面存在不足，尤其是在多帧视觉和叙事一致性上缺乏灵活性，阻碍了创作者的无缝故事创作和精修。", "method": "引入了一个名为“Plot'n Polish”的零样本框架。", "result": "该框架能够实现一致的故事生成，并提供对故事视觉化在不同细节层级的细粒度控制。", "conclusion": "Plot'n Polish框架有效解决了文本到图像故事生成中对一致性和细粒度控制的挑战。"}}
{"id": "2509.04448", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.04448", "abs": "https://arxiv.org/abs/2509.04448", "authors": ["Zehong Yan", "Peng Qi", "Wynne Hsu", "Mong Li Lee"], "title": "TRUST-VL: An Explainable News Assistant for General Multimodal Misinformation Detection", "comment": "EMNLP 2025; Project Homepage: https://yanzehong.github.io/trust-vl/", "summary": "Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.", "AI": {"tldr": "本文提出了TRUST-VL，一个统一且可解释的视觉-语言模型，用于通用的多模态错误信息检测，并通过一个大规模指令数据集TRUST-Instruct进行训练，实现了最先进的性能和强大的泛化能力。", "motivation": "多模态错误信息（包括文本、视觉和跨模态失真）日益增长，并被生成式AI放大，对社会构成威胁。现有方法通常只关注单一类型的失真，难以泛化到未见场景。", "method": "作者观察到不同失真类型共享共同的推理能力，同时也需要特定任务的技能。他们假设跨失真类型的联合训练有助于知识共享并增强模型的泛化能力。为此，他们引入了TRUST-VL，一个统一且可解释的视觉-语言模型，用于通用多模态错误信息检测。TRUST-VL包含一个新颖的“问题感知视觉放大器”模块，用于提取特定任务的视觉特征。此外，他们构建了TRUST-Instruct，一个包含19.8万个样本的大规模指令数据集，其结构化推理链与人类事实核查工作流程对齐。", "result": "在域内和零样本基准测试中，TRUST-VL都取得了最先进的性能，同时展现出强大的泛化能力和可解释性。", "conclusion": "TRUST-VL作为一个统一且可解释的视觉-语言模型，结合其新颖的模块设计和大规模指令数据集，有效解决了多模态错误信息检测中的泛化和解释性挑战，取得了显著的性能提升。"}}
{"id": "2509.04450", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04450", "abs": "https://arxiv.org/abs/2509.04450", "authors": ["Jun-Kun Chen", "Aayush Bansal", "Minh Phuoc Vo", "Yu-Xiong Wang"], "title": "Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual Try-On from a Single Image -- Technical Preview", "comment": "Project Page: https://immortalco.github.io/VirtualFittingRoom/", "summary": "We introduce the Virtual Fitting Room (VFR), a novel video generative model\nthat produces arbitrarily long virtual try-on videos. Our VFR models long video\ngeneration tasks as an auto-regressive, segment-by-segment generation process,\neliminating the need for resource-intensive generation and lengthy video data,\nwhile providing the flexibility to generate videos of arbitrary length. The key\nchallenges of this task are twofold: ensuring local smoothness between adjacent\nsegments and maintaining global temporal consistency across different segments.\nTo address these challenges, we propose our VFR framework, which ensures\nsmoothness through a prefix video condition and enforces consistency with the\nanchor video -- a 360-degree video that comprehensively captures the human's\nwholebody appearance. Our VFR generates minute-scale virtual try-on videos with\nboth local smoothness and global temporal consistency under various motions,\nmaking it a pioneering work in long virtual try-on video generation.", "AI": {"tldr": "本文提出了一种名为VFR（虚拟试衣间）的新型视频生成模型，能够自动回归、分段生成任意长度的虚拟试穿视频，解决了长视频生成中的局部平滑性和全局时间一致性问题。", "motivation": "生成任意长度的虚拟试穿视频面临资源密集型计算和缺乏灵活性的挑战。核心问题在于如何确保相邻视频片段间的局部平滑性以及不同片段间的全局时间一致性。", "method": "VFR模型采用自回归、分段生成的方式处理长视频任务。为确保局部平滑性，引入了前缀视频条件；为维持全局时间一致性，使用了一个全面捕捉人体全身外观的360度锚点视频。", "result": "VFR模型能够生成分钟级别的虚拟试穿视频，并在多种动作下同时实现局部平滑性和全局时间一致性。", "conclusion": "VFR是长虚拟试穿视频生成领域的开创性工作，成功解决了该任务的关键挑战。"}}
