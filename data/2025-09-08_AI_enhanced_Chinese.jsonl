{"id": "2509.04535", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04535", "abs": "https://arxiv.org/abs/2509.04535", "authors": ["Minjong Yoo", "Woo Kyung Kim", "Honguk Woo"], "title": "In-Context Policy Adaptation via Cross-Domain Skill Diffusion", "comment": "9 pages", "summary": "In this work, we present an in-context policy adaptation (ICPAD) framework\ndesigned for long-horizon multi-task environments, exploring diffusion-based\nskill learning techniques in cross-domain settings. The framework enables rapid\nadaptation of skill-based reinforcement learning policies to diverse target\ndomains, especially under stringent constraints on no model updates and only\nlimited target domain data. Specifically, the framework employs a cross-domain\nskill diffusion scheme, where domain-agnostic prototype skills and a\ndomain-grounded skill adapter are learned jointly and effectively from an\noffline dataset through cross-domain consistent diffusion processes. The\nprototype skills act as primitives for common behavior representations of\nlong-horizon policies, serving as a lingua franca to bridge different domains.\nFurthermore, to enhance the in-context adaptation performance, we develop a\ndynamic domain prompting scheme that guides the diffusion-based skill adapter\ntoward better alignment with the target domain. Through experiments with\nrobotic manipulation in Metaworld and autonomous driving in CARLA, we show that\nour $\\oursol$ framework achieves superior policy adaptation performance under\nlimited target domain data conditions for various cross-domain configurations\nincluding differences in environment dynamics, agent embodiment, and task\nhorizon.", "AI": {"tldr": "本文提出了一个名为ICPAD的框架，用于长时程多任务环境中的上下文策略适应。该框架利用基于扩散的技能学习技术，通过跨域技能扩散和动态域提示，实现了在无模型更新和目标域数据有限的严格约束下，对跨域环境中基于技能的强化学习策略的快速适应。", "motivation": "在长时程多任务环境中，需要使基于技能的强化学习策略能够快速适应多样化的目标域，尤其是在不允许模型更新且目标域数据非常有限的严格约束下。", "method": "该框架采用以下方法：1. 跨域技能扩散方案：通过跨域一致的扩散过程，从离线数据中共同有效地学习域无关的原型技能和域基础的技能适配器。原型技能作为通用行为表示的基元，用于连接不同领域。2. 动态域提示方案：引导基于扩散的技能适配器更好地与目标域对齐，从而增强上下文适应性能。", "result": "通过在Metaworld（机器人操作）和CARLA（自动驾驶）中的实验，作者展示了ICPAD框架在目标域数据有限的条件下，对于包括环境动态、智能体形态和任务时程差异在内的各种跨域配置，实现了卓越的策略适应性能。", "conclusion": "ICPAD框架通过结合跨域技能扩散和动态域提示，为长时程多任务环境中的快速、有效的上下文策略适应提供了一个强大的解决方案，特别是在数据和模型更新受限的跨域场景下表现出色。"}}
{"id": "2509.04628", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04628", "abs": "https://arxiv.org/abs/2509.04628", "authors": ["Alejandro Posadas-Nava", "Andrea Scorsoglio", "Luca Ghilardi", "Roberto Furfaro", "Richard Linares"], "title": "Action Chunking with Transformers for Image-Based Spacecraft Guidance and Control", "comment": "12 pages, 6 figures, 2025 AAS/AIAA Astrodynamics Specialist\n  Conference", "summary": "We present an imitation learning approach for spacecraft guidance,\nnavigation, and control(GNC) that achieves high performance from limited data.\nUsing only 100 expert demonstrations, equivalent to 6,300 environment\ninteractions, our method, which implements Action Chunking with Transformers\n(ACT), learns a control policy that maps visual and state observations to\nthrust and torque commands. ACT generates smoother, more consistent\ntrajectories than a meta-reinforcement learning (meta-RL) baseline trained with\n40 million interactions. We evaluate ACT on a rendezvous task: in-orbit docking\nwith the International Space Station (ISS). We show that our approach achieves\ngreater accuracy, smoother control, and greater sample efficiency.", "AI": {"tldr": "本文提出一种基于模仿学习的航天器制导、导航与控制（GNC）方法，利用Action Chunking with Transformers (ACT)，仅用少量专家演示数据即可实现高性能，并在对接任务中展现出更高的精度、更平滑的控制和更高的样本效率，优于基线方法。", "motivation": "在航天器GNC领域，实现高性能控制通常需要大量数据。本研究旨在开发一种能够从有限数据中学习并实现高表现的控制策略。", "method": "研究采用了一种名为Action Chunking with Transformers (ACT) 的模仿学习方法。该方法仅使用100次专家演示（相当于6,300次环境交互），学习一个将视觉和状态观测映射到推力与扭矩指令的控制策略。该方法在国际空间站（ISS）的在轨交会对接任务上进行了评估。", "result": "ACT方法生成的轨迹比使用4000万次交互训练的元强化学习（meta-RL）基线更平滑、更一致。实验表明，该方法在精度、控制平滑性和样本效率方面均表现更优。", "conclusion": "基于ACT的模仿学习方法能够以极高的样本效率，从有限的专家演示中学习到高性能的航天器GNC控制策略，实现更准确、更平滑的控制，并显著优于传统强化学习基线。"}}
{"id": "2509.04645", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04645", "abs": "https://arxiv.org/abs/2509.04645", "authors": ["Kallol Saha", "Amber Li", "Angela Rodriguez-Izquierdo", "Lifan Yu", "Ben Eisner", "Maxim Likhachev", "David Held"], "title": "Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement", "comment": "Conference on Robot Learning (CoRL) 2025\n  (https://planning-from-point-clouds.github.io/)", "summary": "Long-horizon planning for robot manipulation is a challenging problem that\nrequires reasoning about the effects of a sequence of actions on a physical 3D\nscene. While traditional task planning methods are shown to be effective for\nlong-horizon manipulation, they require discretizing the continuous state and\naction space into symbolic descriptions of objects, object relationships, and\nactions. Instead, we propose a hybrid learning-and-planning approach that\nleverages learned models as domain-specific priors to guide search in\nhigh-dimensional continuous action spaces. We introduce SPOT: Search over Point\ncloud Object Transformations, which plans by searching for a sequence of\ntransformations from an initial scene point cloud to a goal-satisfying point\ncloud. SPOT samples candidate actions from learned suggesters that operate on\npartially observed point clouds, eliminating the need to discretize actions or\nobject relationships. We evaluate SPOT on multi-object rearrangement tasks,\nreporting task planning success and task execution success in both simulation\nand real-world environments. Our experiments show that SPOT generates\nsuccessful plans and outperforms a policy-learning approach. We also perform\nablations that highlight the importance of search-based planning.", "AI": {"tldr": "本文提出了一种名为SPOT的混合学习与规划方法，通过在点云对象变换空间中搜索，实现了机器人长时间跨度操作的规划，无需离散化，并在仿真和真实环境中表现优于纯策略学习方法。", "motivation": "机器人长时间跨度操作规划面临巨大挑战，需要推理一系列动作对3D物理场景的影响。传统任务规划方法虽有效，但需将连续状态和动作空间离散化为符号描述。本文旨在解决无需离散化连续空间，实现有效长时间跨度操作规划的问题。", "method": "本文提出SPOT（Search over Point cloud Object Transformations）方法，它是一种混合学习与规划的方法。SPOT通过搜索从初始场景点云到满足目标点云的变换序列来规划。它利用学习到的建议器（suggesters）从部分观测的点云中采样候选动作，从而避免了离散化动作或对象关系。学习模型作为领域特定先验来指导高维连续动作空间的搜索。", "result": "SPOT在多对象重新排列任务上进行了评估，报告了在仿真和真实世界环境中的任务规划成功率和任务执行成功率。实验结果表明，SPOT能够生成成功的规划，并优于纯策略学习方法。消融实验也突出了基于搜索的规划的重要性。", "conclusion": "SPOT成功地通过在点云对象变换空间中搜索来解决长时间跨度机器人操作规划问题，避免了对连续状态和动作空间的离散化。该方法利用学习模型作为先验指导搜索，并在实际任务中表现出优越的规划和执行能力，证明了混合学习与规划范式的有效性。"}}
{"id": "2509.04658", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04658", "abs": "https://arxiv.org/abs/2509.04658", "authors": ["Manish Kansana", "Sindhuja Penchala", "Shahram Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "Surformer v2: A Multimodal Classifier for Surface Understanding from Touch and Vision", "comment": "6 pages", "summary": "Multimodal surface material classification plays a critical role in advancing\ntactile perception for robotic manipulation and interaction. In this paper, we\npresent Surformer v2, an enhanced multi-modal classification architecture\ndesigned to integrate visual and tactile sensory streams through a\nlate(decision level) fusion mechanism. Building on our earlier Surformer v1\nframework [1], which employed handcrafted feature extraction followed by\nmid-level fusion architecture with multi-head cross-attention layers, Surformer\nv2 integrates the feature extraction process within the model itself and shifts\nto late fusion. The vision branch leverages a CNN-based classifier(Efficient\nV-Net), while the tactile branch employs an encoder-only transformer model,\nallowing each modality to extract modality-specific features optimized for\nclassification. Rather than merging feature maps, the model performs\ndecision-level fusion by combining the output logits using a learnable weighted\nsum, enabling adaptive emphasis on each modality depending on data context and\ntraining dynamics. We evaluate Surformer v2 on the Touch and Go dataset [2], a\nmulti-modal benchmark comprising surface images and corresponding tactile\nsensor readings. Our results demonstrate that Surformer v2 performs well,\nmaintaining competitive inference speed, suitable for real-time robotic\napplications. These findings underscore the effectiveness of decision-level\nfusion and transformer-based tactile modeling for enhancing surface\nunderstanding in multi-modal robotic perception.", "AI": {"tldr": "Surformer v2 是一种增强的多模态分类架构，通过决策级（晚期）融合机制整合视觉（CNN）和触觉（Transformer）传感器数据，用于机器人操作和交互中的表面材料分类，并在保持实时推理速度的同时表现良好。", "motivation": "推动机器人操作和交互中的触觉感知能力，特别是在多模态表面材料分类方面。旨在改进 Surformer v1 框架，将特征提取集成到模型中并采用晚期融合策略。", "method": "Surformer v2 采用增强的多模态分类架构。视觉分支使用基于 CNN 的分类器（Efficient V-Net），触觉分支使用仅编码器 Transformer 模型。模型通过学习到的加权和组合输出对数（logits）进行决策级（晚期）融合，而不是合并特征图。与 Surformer v1 相比，Surformer v2 将特征提取过程集成到模型内部，并从手工特征提取和中级融合转向模型内学习特征提取和晚期融合。", "result": "Surformer v2 在 Touch and Go 数据集上进行了评估，表现良好，并保持了有竞争力的推理速度，适用于实时机器人应用。这些发现强调了决策级融合和基于 Transformer 的触觉建模在增强多模态机器人感知中表面理解的有效性。", "conclusion": "决策级融合和基于 Transformer 的触觉建模对于增强多模态机器人感知中的表面理解是有效的，Surformer v2 证明了其在实时机器人应用中的潜力和竞争力。"}}
{"id": "2509.04455", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04455", "abs": "https://arxiv.org/abs/2509.04455", "authors": ["Shisong Chen", "Qian Zhu", "Wenyan Yang", "Chengyi Yang", "Zhong Wang", "Ping Wang", "Xuan Lin", "Bo Xu", "Daqian Li", "Chao Yuan", "Licai Qi", "Wanqing Xu", "sun zhenxing", "Xin Lu", "Shiqiang Xiong", "Chao Chen", "Haixiang Hu", "Yanghua Xiao"], "title": "INSEva: A Comprehensive Chinese Benchmark for Large Language Models in Insurance", "comment": "Under review", "summary": "Insurance, as a critical component of the global financial system, demands\nhigh standards of accuracy and reliability in AI applications. While existing\nbenchmarks evaluate AI capabilities across various domains, they often fail to\ncapture the unique characteristics and requirements of the insurance domain. To\naddress this gap, we present INSEva, a comprehensive Chinese benchmark\nspecifically designed for evaluating AI systems' knowledge and capabilities in\ninsurance. INSEva features a multi-dimensional evaluation taxonomy covering\nbusiness areas, task formats, difficulty levels, and cognitive-knowledge\ndimension, comprising 38,704 high-quality evaluation examples sourced from\nauthoritative materials. Our benchmark implements tailored evaluation methods\nfor assessing both faithfulness and completeness in open-ended responses.\nThrough extensive evaluation of 8 state-of-the-art Large Language Models\n(LLMs), we identify significant performance variations across different\ndimensions. While general LLMs demonstrate basic insurance domain competency\nwith average scores above 80, substantial gaps remain in handling complex,\nreal-world insurance scenarios. The benchmark will be public soon.", "AI": {"tldr": "本文提出了INSEva，一个专门针对保险领域AI系统知识和能力的综合性中文基准测试。它包含38,704个高质量示例，并采用多维度评估方法。对8个最先进的大语言模型进行评估后发现，虽然它们具备基本能力，但在处理复杂现实保险场景时仍存在显著差距。", "motivation": "现有的AI基准测试未能捕捉保险领域的独特特征和要求，导致在该领域AI评估方面存在空白，尤其是在对准确性和可靠性要求极高的金融系统中。", "method": "研究者开发了INSEva，一个综合性中文基准测试，其特点是多维度评估分类法，涵盖业务领域、任务格式、难度级别和认知知识维度。该基准包含38,704个来自权威材料的高质量评估示例，并为开放式回答设计了评估忠实性和完整性的定制方法。研究者使用此基准评估了8个最先进的大语言模型。", "result": "评估结果显示，不同维度之间存在显著的性能差异。虽然通用大语言模型表现出基本的保险领域能力，平均得分超过80分，但在处理复杂的现实世界保险场景方面仍存在巨大差距。", "conclusion": "INSEva成功弥补了保险领域AI评估的空白。尽管当前的大语言模型已具备一定的保险领域基础能力，但它们在应对复杂和实际的保险场景时仍需大幅提升。该基准将很快公开。"}}
{"id": "2509.04490", "categories": ["cs.CV", "J.4; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.04490", "abs": "https://arxiv.org/abs/2509.04490", "authors": ["Abel van Elburg", "Konstantinos Gkentsidis", "Mathieu Sarrazin", "Sarah Barendswaard", "Varun Kotian", "Riender Happee"], "title": "Facial Emotion Recognition does not detect feeling unsafe in automated driving", "comment": null, "summary": "Trust and perceived safety play a crucial role in the public acceptance of\nautomated vehicles. To understand perceived risk, an experiment was conducted\nusing a driving simulator under two automated driving styles and optionally\nintroducing a crossing pedestrian. Data was collected from 32 participants,\nconsisting of continuous subjective comfort ratings, motion, webcam footage for\nfacial expression, skin conductance, heart rate, and eye tracking. The\ncontinuous subjective perceived risk ratings showed significant discomfort\nassociated with perceived risk during cornering and braking followed by relief\nor even positive comfort on continuing the ride. The dynamic driving style\ninduced a stronger discomfort as compared to the calm driving style. The\ncrossing pedestrian did not affect discomfort with the calm driving style but\ndoubled the comfort decrement with the dynamic driving style. This illustrates\nthe importance of consequences of critical interactions in risk perception.\nFacial expression was successfully analyzed for 24 participants but most\n(15/24) did not show any detectable facial reaction to the critical event.\nAmong the 9 participants who did, 8 showed a Happy expression, and only 4\nshowed a Surprise expression. Fear was never dominant. This indicates that\nfacial expression recognition is not a reliable method for assessing perceived\nrisk in automated vehicles. To predict perceived risk a neural network model\nwas implemented using vehicle motion and skin conductance. The model correlated\nwell with reported perceived risk, demonstrating its potential for objective\nperceived risk assessment in automated vehicles, reducing subjective bias and\nhighlighting areas for future research.", "AI": {"tldr": "本研究通过驾驶模拟器实验，探究了自动驾驶车辆中感知风险与信任。结果显示，动态驾驶风格和关键交互（如行人）会增加不适感。面部表情识别被证明不可靠，而基于车辆运动和皮肤电反应的神经网络模型在客观评估感知风险方面显示出潜力。", "motivation": "信任和感知安全在公众接受自动驾驶车辆中至关重要。为了促进自动驾驶的普及，需要深入理解用户对自动驾驶的感知风险。", "method": "实验使用驾驶模拟器，设置两种自动驾驶风格（平稳和动态）并可选引入横穿行人。收集了32名参与者的连续主观舒适度评分、车辆运动数据、网络摄像头（面部表情）、皮肤电导、心率和眼动数据。采用主观评分分析、面部表情分析和基于车辆运动与皮肤电反应的神经网络模型来预测感知风险。", "result": "主观感知风险评分显示，转弯和制动时存在显著不适感，随后是放松或舒适。动态驾驶风格比平稳风格引起更强的不适。横穿行人对平稳驾驶风格下的不适无影响，但使动态驾驶风格下的舒适度下降加倍。面部表情识别被证明不可靠，大多数参与者没有可检测的面部反应，少数有反应的也多为“开心”或“惊讶”，从未出现“恐惧”。基于车辆运动和皮肤电反应的神经网络模型与报告的感知风险高度相关，显示出客观评估感知风险的潜力。", "conclusion": "面部表情识别不是评估自动驾驶车辆中感知风险的可靠方法。利用车辆运动和皮肤电反应的神经网络模型在客观评估自动驾驶车辆中的感知风险方面具有潜力，有助于减少主观偏见并为未来研究指明方向。"}}
{"id": "2509.04505", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.04505", "abs": "https://arxiv.org/abs/2509.04505", "authors": ["Somtochukwu Azie", "Yiping Meng"], "title": "The Ethical Compass of the Machine: Evaluating Large Language Models for Decision Support in Construction Project Management", "comment": "16 Pages", "summary": "The integration of Artificial Intelligence (AI) into construction project\nmanagement (CPM) is accelerating, with Large Language Models (LLMs) emerging as\naccessible decision-support tools. This study aims to critically evaluate the\nethical viability and reliability of LLMs when applied to the ethically\nsensitive, high-risk decision-making contexts inherent in CPM. A mixed-methods\nresearch design was employed, involving the quantitative performance testing of\ntwo leading LLMs against twelve real-world ethical scenarios using a novel\nEthical Decision Support Assessment Checklist (EDSAC), and qualitative analysis\nof semi-structured interviews with 12 industry experts to capture professional\nperceptions. The findings reveal that while LLMs demonstrate adequate\nperformance in structured domains such as legal compliance, they exhibit\nsignificant deficiencies in handling contextual nuance, ensuring\naccountability, and providing transparent reasoning. Stakeholders expressed\nconsiderable reservations regarding the autonomous use of AI for ethical\njudgments, strongly advocating for robust human-in-the-loop oversight. To our\nknowledge, this is one of the first studies to empirically test the ethical\nreasoning of LLMs within the construction domain. It introduces the EDSAC\nframework as a replicable methodology and provides actionable recommendations,\nemphasising that LLMs are currently best positioned as decision-support aids\nrather than autonomous ethical agents.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）在建筑项目管理（CPM）中处理伦理敏感决策时的伦理可行性和可靠性。结果显示，LLMs在结构化领域表现尚可，但在处理情境细微差别、问责和透明推理方面存在显著不足，强调需加强人工监督。", "motivation": "人工智能（AI）及其中的大型语言模型（LLMs）正加速融入建筑项目管理（CPM）作为决策支持工具。本研究旨在批判性评估LLMs在CPM中固有的伦理敏感、高风险决策情境中的伦理可行性和可靠性。", "method": "采用混合方法研究设计：\n1. 定量分析：使用新颖的“伦理决策支持评估清单（EDSAC）”，针对12个真实世界的伦理情景，对两个领先的LLMs进行性能测试。\n2. 定性分析：对12位行业专家进行半结构化访谈，以获取专业看法。", "result": "1. LLMs在法律合规等结构化领域表现尚可。\n2. LLMs在处理情境细微差别、确保问责制和提供透明推理方面存在显著缺陷。\n3. 利益相关者对AI自主进行伦理判断表示担忧，强烈主张实施强大的人工监督（human-in-the-loop）。", "conclusion": "LLMs目前最适合作为决策支持辅助工具，而非自主的伦理代理。本研究引入了EDSAC框架作为可复制的方法论，并提供了可操作的建议。这是首批在建筑领域实证测试LLMs伦理推理的研究之一。"}}
{"id": "2509.04451", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04451", "abs": "https://arxiv.org/abs/2509.04451", "authors": ["Nicole Fronda", "Hariharan Narayanan", "Sadia Afrin Ananna", "Steven Weber", "Houssam Abbas"], "title": "PRREACH: Probabilistic Risk Assessment Using Reachability for UAV Control", "comment": "Accepted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "We present a new approach for designing risk-bounded controllers for Uncrewed\nAerial Vehicles (UAVs). Existing frameworks for assessing risk of UAV\noperations rely on knowing the conditional probability of an incident occurring\ngiven different causes. Limited data for computing these probabilities makes\nreal-world implementation of these frameworks difficult. Furthermore, existing\nframeworks do not include control methods for risk mitigation. Our approach\nrelies on UAV dynamics, and employs reachability analysis for a probabilistic\nrisk assessment over all feasible UAV trajectories. We use this holistic risk\nassessment to formulate a control optimization problem that minimally changes a\nUAV's existing control law to be bounded by an accepted risk threshold. We call\nour approach PRReach. Public and readily available UAV dynamics models and open\nsource spatial data for mapping hazard outcomes enables practical\nimplementation of PRReach for both offline pre-flight and online in-flight risk\nassessment and mitigation. We evaluate PRReach through simulation experiments\non real-world data. Results show that PRReach controllers reduce risk by up to\n24% offline, and up to 53% online from classical controllers.", "AI": {"tldr": "本文提出了一种名为PRReach的新方法，利用可达性分析对无人机轨迹进行概率风险评估，并通过优化控制律以满足预设风险阈值，从而实现风险受限的无人机控制。", "motivation": "现有无人机风险评估框架依赖于难以获取的条件概率数据，且缺乏风险缓解的控制方法，这使得实际应用面临挑战。", "method": "PRReach方法基于无人机动力学，采用可达性分析对所有可行无人机轨迹进行概率风险评估。然后，将这种整体风险评估用于构建一个控制优化问题，旨在以最小改动现有控制律的方式，使其风险受限于可接受的阈值。该方法利用公开的无人机动力学模型和开源空间数据。", "result": "仿真实验结果表明，与传统控制器相比，PRReach控制器在离线情况下可将风险降低高达24%，在线情况下可降低高达53%。", "conclusion": "PRReach提供了一种实用且有效的方法来设计无人机风险受限控制器，能够实现离线预飞行和在线飞行中的风险评估和缓解。"}}
{"id": "2509.04677", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.04677", "abs": "https://arxiv.org/abs/2509.04677", "authors": ["Mayur S Gowda", "John Shi", "Augusto Santos", "José M. F. Moura"], "title": "Inferring the Graph Structure of Images for Graph Neural Networks", "comment": null, "summary": "Image datasets such as MNIST are a key benchmark for testing Graph Neural\nNetwork (GNN) architectures. The images are traditionally represented as a grid\ngraph with each node representing a pixel and edges connecting neighboring\npixels (vertically and horizontally). The graph signal is the values\n(intensities) of each pixel in the image. The graphs are commonly used as input\nto graph neural networks (e.g., Graph Convolutional Neural Networks (Graph\nCNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the\nimages. In this work, we improve the accuracy of downstream graph neural\nnetwork tasks by finding alternative graphs to the grid graph and superpixel\nmethods to represent the dataset images, following the approach in [5, 6]. We\nfind row correlation, column correlation, and product graphs for each image in\nMNIST and Fashion-MNIST using correlations between the pixel values building on\nthe method in [5, 6]. Experiments show that using these different graph\nrepresentations and features as input into downstream GNN models improves the\naccuracy over using the traditional grid graph and superpixel methods in the\nliterature.", "AI": {"tldr": "本研究通过为图像数据集（如MNIST）构建替代的图表示（行/列相关图、乘积图），而非传统的网格图或超像素方法，显著提高了下游图神经网络（GNN）的分类准确性。", "motivation": "传统的图像表示方法（如网格图和超像素）在作为图神经网络（GNN）输入时，可能未能充分捕捉图像信息，导致下游GNN任务（如图像分类）的准确性有待提高。", "method": "研究人员基于像素值之间的相关性，为MNIST和Fashion-MNIST数据集中的每张图像构建了行相关图、列相关图和乘积图。这些新颖的图表示和特征被用作下游GNN模型的输入，并与传统方法进行了比较。", "result": "实验结果表明，使用这些基于像素相关性构建的不同图表示和特征作为GNN模型的输入，能够比使用传统网格图和超像素方法获得更高的分类准确性。", "conclusion": "通过探索和应用基于像素相关性的替代图表示方法，可以有效提升图神经网络在图像分类任务上的性能，优于传统的图像图表示方法。"}}
{"id": "2509.04712", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.04712", "abs": "https://arxiv.org/abs/2509.04712", "authors": ["Zhihao Zhang", "Chengyang Peng", "Ekim Yurtsever", "Keith A. Redmill"], "title": "Bootstrapping Reinforcement Learning with Sub-optimal Policies for Autonomous Driving", "comment": null, "summary": "Automated vehicle control using reinforcement learning (RL) has attracted\nsignificant attention due to its potential to learn driving policies through\nenvironment interaction. However, RL agents often face training challenges in\nsample efficiency and effective exploration, making it difficult to discover an\noptimal driving strategy. To address these issues, we propose guiding the RL\ndriving agent with a demonstration policy that need not be a highly optimized\nor expert-level controller. Specifically, we integrate a rule-based lane change\ncontroller with the Soft Actor Critic (SAC) algorithm to enhance exploration\nand learning efficiency. Our approach demonstrates improved driving performance\nand can be extended to other driving scenarios that can similarly benefit from\ndemonstration-based guidance.", "AI": {"tldr": "该研究通过将一个非专家级的演示策略（基于规则的变道控制器）与SAC算法结合，来指导强化学习自动驾驶智能体，以提高样本效率和探索能力，从而改善驾驶性能。", "motivation": "强化学习在自动驾驶控制中面临样本效率低和有效探索困难的挑战，这使得智能体难以发现最优驾驶策略。", "method": "提出用一个非专家级的演示策略来引导强化学习驾驶智能体。具体来说，将一个基于规则的变道控制器与Soft Actor Critic (SAC) 算法相结合，以增强探索和学习效率。", "result": "该方法展示了改进的驾驶性能。", "conclusion": "所提出的方法能够提高探索和学习效率，改善驾驶性能，并且可以推广到其他类似受益于基于演示指导的驾驶场景。"}}
{"id": "2509.04456", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04456", "abs": "https://arxiv.org/abs/2509.04456", "authors": ["Anandi Dutta", "Shivani Mruthyunjaya", "Jessica Saddington", "Kazi Sifatul Islam"], "title": "Mentalic Net: Development of RAG-based Conversational AI and Evaluation Framework for Mental Health Support", "comment": "Preprint Version, Accepted in ISEMV 2025", "summary": "The emergence of large language models (LLMs) has unlocked boundless\npossibilities, along with significant challenges. In response, we developed a\nmental health support chatbot designed to augment professional healthcare, with\na strong emphasis on safe and meaningful application. Our approach involved\nrigorous evaluation, covering accuracy, empathy, trustworthiness, privacy, and\nbias. We employed a retrieval-augmented generation (RAG) framework, integrated\nprompt engineering, and fine-tuned a pre-trained model on novel datasets. The\nresulting system, Mentalic Net Conversational AI, achieved a BERT Score of\n0.898, with other evaluation metrics falling within satisfactory ranges. We\nadvocate for a human-in-the-loop approach and a long-term, responsible strategy\nin developing such transformative technologies, recognizing both their\npotential to change lives and the risks they may pose if not carefully managed.", "AI": {"tldr": "本文开发了一个名为Mentalic Net Conversational AI的心理健康支持聊天机器人，旨在增强专业医疗服务，并强调安全和有意义的应用。该系统采用RAG框架、提示工程和模型微调，并在多维度评估中表现良好。", "motivation": "大型语言模型的兴起带来了无限可能，但也伴随着重大挑战。研究旨在开发一个安全、有意义的心理健康支持应用，以辅助专业医疗服务。", "method": "研究采用了检索增强生成（RAG）框架，整合了提示工程，并使用新颖数据集对预训练模型进行了微调。系统经过了严格评估，涵盖了准确性、同理心、可信度、隐私和偏见等多个方面。", "result": "所开发的Mentalic Net Conversational AI系统在BERT Score上达到了0.898，其他评估指标也均在满意范围内。", "conclusion": "研究倡导在开发此类变革性技术时，应采取“人机协作”（human-in-the-loop）方法和长期负责任的策略，以充分发挥其改变生活的潜力，并有效管理潜在风险。"}}
{"id": "2509.04545", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04545", "abs": "https://arxiv.org/abs/2509.04545", "authors": ["Linqing Wang", "Ximing Xing", "Yiji Cheng", "Zhiyuan Zhao", "Jiale Tao", "Qixun Wang", "Ruihuang Li", "Xin Li", "Mingrui Wu", "Xinchi Deng", "Chunyu Wang", "Qinglin Lu"], "title": "PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting", "comment": "technical report", "summary": "Recent advancements in text-to-image (T2I) diffusion models have demonstrated\nremarkable capabilities in generating high-fidelity images. However, these\nmodels often struggle to faithfully render complex user prompts, particularly\nin aspects like attribute binding, negation, and compositional relationships.\nThis leads to a significant mismatch between user intent and the generated\noutput. To address this challenge, we introduce PromptEnhancer, a novel and\nuniversal prompt rewriting framework that enhances any pretrained T2I model\nwithout requiring modifications to its weights. Unlike prior methods that rely\non model-specific fine-tuning or implicit reward signals like image-reward\nscores, our framework decouples the rewriter from the generator. We achieve\nthis by training a Chain-of-Thought (CoT) rewriter through reinforcement\nlearning, guided by a dedicated reward model we term the AlignEvaluator. The\nAlignEvaluator is trained to provide explicit and fine-grained feedback based\non a systematic taxonomy of 24 key points, which are derived from a\ncomprehensive analysis of common T2I failure modes. By optimizing the CoT\nrewriter to maximize the reward from our AlignEvaluator, our framework learns\nto generate prompts that are more precisely interpreted by T2I models.\nExtensive experiments on the HunyuanImage 2.1 model demonstrate that\nPromptEnhancer significantly improves image-text alignment across a wide range\nof semantic and compositional challenges. Furthermore, we introduce a new,\nhigh-quality human preference benchmark to facilitate future research in this\ndirection.", "AI": {"tldr": "PromptEnhancer是一个新颖的、通用的提示词重写框架，通过强化学习训练CoT重写器并由AlignEvaluator奖励模型指导，显著提升了预训练文生图模型对复杂提示词的理解和图像-文本对齐。", "motivation": "当前的文生图（T2I）扩散模型在生成高保真图像方面表现出色，但难以忠实地渲染复杂用户提示，特别是在属性绑定、否定和组合关系方面，导致用户意图与生成输出之间存在显著 Mismatch。", "method": "本文提出了PromptEnhancer，一个无需修改模型权重的通用提示词重写框架。它将重写器与生成器解耦，通过强化学习训练一个Chain-of-Thought (CoT) 重写器。该训练由一个专门的奖励模型AlignEvaluator指导，AlignEvaluator根据对常见T2I失败模式的24个关键点分类学提供显式和细粒度的反馈。", "result": "在HunyuanImage 2.1模型上的广泛实验表明，PromptEnhancer在各种语义和组合挑战中显著改善了图像-文本对齐。此外，本文还引入了一个新的高质量人类偏好基准，以促进未来的研究。", "conclusion": "PromptEnhancer提供了一个有效且通用的提示词重写解决方案，通过学习生成更精确的提示词，显著提高了文生图模型对用户意图的理解和图像生成质量，克服了复杂提示词处理的挑战。"}}
{"id": "2509.04642", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.04642", "abs": "https://arxiv.org/abs/2509.04642", "authors": ["Wenxiao Wang", "Priyatham Kattakinda", "Soheil Feizi"], "title": "Maestro: Joint Graph & Config Optimization for Reliable AI Agents", "comment": "Technical Report by RELAI.ai", "summary": "Building reliable LLM agents requires decisions at two levels: the graph\n(which modules exist and how information flows) and the configuration of each\nnode (models, prompts, tools, control knobs). Most existing optimizers tune\nconfigurations while holding the graph fixed, leaving structural failure modes\nunaddressed. We introduce Maestro, a framework-agnostic holistic optimizer for\nLLM agents that jointly searches over graphs and configurations to maximize\nagent quality, subject to explicit rollout/token budgets. Beyond numeric\nmetrics, Maestro leverages reflective textual feedback from traces to\nprioritize edits, improving sample efficiency and targeting specific failure\nmodes. On the IFBench and HotpotQA benchmarks, Maestro consistently surpasses\nleading prompt optimizers--MIPROv2, GEPA, and GEPA+Merge--by an average of 12%,\n4.9%, and 4.86%, respectively; even when restricted to prompt-only\noptimization, it still leads by 9.65%, 2.37%, and 2.41%. Maestro achieves these\nresults with far fewer rollouts than GEPA. We further show large gains on two\napplications (interviewer & RAG agents), highlighting that joint graph &\nconfiguration search addresses structural failure modes that prompt tuning\nalone cannot fix.", "AI": {"tldr": "Maestro是一个LLM代理的整体优化器，它通过联合搜索代理的图结构和节点配置来最大化代理质量，解决了现有优化器无法处理的结构性故障模式，并在多个基准测试中显著超越了领先的提示优化器。", "motivation": "构建可靠的LLM代理需要在图结构和节点配置两个层面进行决策。现有优化器通常只调整配置，而忽略了图结构，导致无法解决结构性故障模式。这促使研究人员开发一个能同时优化这两个层面的框架。", "method": "Maestro是一个与框架无关的整体优化器，它在明确的运行/token预算下，联合搜索LLM代理的图结构（模块及其信息流）和每个节点的配置（模型、提示、工具、控制旋钮）。此外，Maestro利用来自追踪的反射性文本反馈来优先处理编辑，从而提高样本效率并针对特定的故障模式。", "result": "在IFBench和HotpotQA基准测试中，Maestro平均分别超越了领先的提示优化器MIPROv2、GEPA和GEPA+Merge 12%、4.9%和4.86%。即使仅限于提示优化，Maestro仍分别领先9.65%、2.37%和2.41%。Maestro在取得这些结果的同时，使用的运行次数远少于GEPA。此外，Maestro在面试官和RAG代理这两个应用中也显示出显著的提升，证明联合图结构和配置搜索能够解决仅凭提示调整无法修复的结构性故障模式。", "conclusion": "联合搜索LLM代理的图结构和配置对于构建可靠的LLM代理至关重要，它能够有效解决仅通过提示调整无法解决的结构性故障模式。Maestro框架通过这种整体优化方法，显著提升了LLM代理的性能和效率。"}}
{"id": "2509.04506", "categories": ["eess.SY", "cs.AI", "cs.AR", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04506", "abs": "https://arxiv.org/abs/2509.04506", "authors": ["Zacharia A. Rudge", "Dominik Dold", "Moritz Fieback", "Dario Izzo", "Said Hamdioui"], "title": "Memristor-Based Neural Network Accelerators for Space Applications: Enhancing Performance with Temporal Averaging and SIRENs", "comment": "21 pages, IAA acta astronautica. arXiv admin note: text overlap with\n  arXiv:2509.02369", "summary": "Memristors are an emerging technology that enables artificial intelligence\n(AI) accelerators with high energy efficiency and radiation robustness --\nproperties that are vital for the deployment of AI on-board spacecraft.\nHowever, space applications require reliable and precise computations, while\nmemristive devices suffer from non-idealities, such as device variability,\nconductance drifts, and device faults. Thus, porting neural networks (NNs) to\nmemristive devices often faces the challenge of severe performance degradation.\nIn this work, we show in simulations that memristor-based NNs achieve\ncompetitive performance levels on on-board tasks, such as navigation \\& control\nand geodesy of asteroids. Through bit-slicing, temporal averaging of NN layers,\nand periodic activation functions, we improve initial results from around\n$0.07$ to $0.01$ and $0.3$ to $0.007$ for both tasks using RRAM devices, coming\nclose to state-of-the-art levels ($0.003-0.005$ and $0.003$, respectively). Our\nresults demonstrate the potential of memristors for on-board space\napplications, and we are convinced that future technology and NN improvements\nwill further close the performance gap to fully unlock the benefits of\nmemristors.", "AI": {"tldr": "该研究通过模拟展示了忆阻器基神经网络在航天任务中的潜力，并利用特定技术显著提升了其性能，使其接近最先进水平。", "motivation": "忆阻器具有高能效和抗辐射性，非常适合航天器上的AI部署。然而，忆阻器器件的非理想性（如器件变异、电导漂移和故障）导致神经网络性能严重下降，难以满足航天应用对可靠和精确计算的要求。", "method": "研究通过模拟，采用位切片 (bit-slicing)、神经网络层的时间平均 (temporal averaging of NN layers) 和周期性激活函数 (periodic activation functions) 等技术，以改善RRAM器件上忆阻器基神经网络的性能。", "result": "在导航与控制以及小行星测地学等板载任务中，忆阻器基神经网络的初始性能（误差）从约0.07提高到0.01，从0.3提高到0.007，已接近最先进水平（分别为0.003-0.005和0.003）。", "conclusion": "研究结果证明了忆阻器在板载航天应用中的巨大潜力。作者相信，未来的技术和神经网络改进将进一步缩小性能差距，充分发挥忆阻器的优势。"}}
{"id": "2509.04819", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04819", "abs": "https://arxiv.org/abs/2509.04819", "authors": ["Shuhan Ding", "Jingjing Fu", "Yu Gu", "Naiteek Sangani", "Mu Wei", "Paul Vozila", "Nan Liu", "Jiang Bian", "Hoifung Poon"], "title": "AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive Representations", "comment": null, "summary": "Medical image synthesis has become an essential strategy for augmenting\ndatasets and improving model generalization in data-scarce clinical settings.\nHowever, fine-grained and controllable synthesis remains difficult due to\nlimited high-quality annotations and domain shifts across datasets. Existing\nmethods, often designed for natural images or well-defined tumors, struggle to\ngeneralize to chest radiographs, where disease patterns are morphologically\ndiverse and tightly intertwined with anatomical structures. To address these\nchallenges, we propose AURAD, a controllable radiology synthesis framework that\njointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike\nprior approaches that rely on randomly sampled masks-limiting diversity,\ncontrollability, and clinical relevance-our method learns to generate masks\nthat capture multi-pathology coexistence and anatomical-pathological\nconsistency. It follows a progressive pipeline: pseudo masks are first\ngenerated from clinical prompts conditioned on anatomical structures, and then\nused to guide image synthesis. We also leverage pretrained expert medical\nmodels to filter outputs and ensure clinical plausibility. Beyond visual\nrealism, the synthesized masks also serve as labels for downstream tasks such\nas detection and segmentation, bridging the gap between generative modeling and\nreal-world clinical applications. Extensive experiments and blinded radiologist\nevaluations demonstrate the effectiveness and generalizability of our method\nacross tasks and datasets. In particular, 78% of our synthesized images are\nclassified as authentic by board-certified radiologists, and over 40% of\npredicted segmentation overlays are rated as clinically useful. All code,\npre-trained models, and the synthesized dataset will be released upon\npublication.", "AI": {"tldr": "AURAD是一个可控的放射学合成框架，能够生成高保真胸部X射线图像和伪语义掩膜，以解决医疗图像合成中的数据稀缺和精细控制难题。", "motivation": "在数据稀缺的临床环境中，医学图像合成是增强数据集和提高模型泛化能力的重要策略。然而，由于高质量标注有限以及数据集间的领域差异，精细和可控的合成仍然困难。现有方法难以推广到胸部X射线，因为其疾病模式形态多样且与解剖结构紧密交织。", "method": "本文提出了AURAD框架，它联合生成高保真胸部X射线图像和伪语义掩膜。该方法学习生成能够捕捉多病理共存和解剖-病理一致性的掩膜。其遵循渐进式流程：首先根据临床提示并以解剖结构为条件生成伪掩膜，然后用这些掩膜指导图像合成。此外，还利用预训练的医学专家模型过滤输出，确保临床合理性。合成的掩膜也可用作下游任务（如检测和分割）的标签。", "result": "广泛的实验和盲法放射科医生评估证明了该方法在不同任务和数据集上的有效性和泛化能力。具体而言，78%的合成图像被认证放射科医生归类为真实，超过40%的预测分割叠加被评为具有临床实用性。", "conclusion": "AURAD框架有效解决了胸部X射线医疗图像合成中的挑战，实现了高保真、可控的图像生成和有用的伪语义掩膜。它通过提供下游任务的标签，并确保临床合理性，弥合了生成模型与现实世界临床应用之间的差距。"}}
{"id": "2509.04722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04722", "abs": "https://arxiv.org/abs/2509.04722", "authors": ["Adrian B. Ghansah", "Sergio A. Esteban", "Aaron D. Ames"], "title": "Hierarchical Reduced-Order Model Predictive Control for Robust Locomotion on Humanoid Robots", "comment": "8 pages, 6 figures, accepted to IEEE-RAS International Conference on\n  Humanoid Robots 2025", "summary": "As humanoid robots enter real-world environments, ensuring robust locomotion\nacross diverse environments is crucial. This paper presents a computationally\nefficient hierarchical control framework for humanoid robot locomotion based on\nreduced-order models -- enabling versatile step planning and incorporating arm\nand torso dynamics to better stabilize the walking. At the high level, we use\nthe step-to-step dynamics of the ALIP model to simultaneously optimize over\nstep periods, step lengths, and ankle torques via nonlinear MPC. The ALIP\ntrajectories are used as references to a linear MPC framework that extends the\nstandard SRB-MPC to also include simplified arm and torso dynamics. We validate\nthe performance of our approach through simulation and hardware experiments on\nthe Unitree G1 humanoid robot. In the proposed framework the high-level step\nplanner runs at 40 Hz and the mid-level MPC at 500 Hz using the onboard\nmini-PC. Adaptive step timing increased the push recovery success rate by 36%,\nand the upper body control improved the yaw disturbance rejection. We also\ndemonstrate robust locomotion across diverse indoor and outdoor terrains,\nincluding grass, stone pavement, and uneven gym mats.", "AI": {"tldr": "本文提出了一种计算高效的分层控制框架，用于实现人形机器人在多变环境中的鲁棒运动，通过简化的模型进行步态规划并整合手臂和躯干动力学以增强稳定性，并在Unitree G1机器人上进行了仿真和硬件验证。", "motivation": "随着人形机器人进入现实世界环境，确保其在各种环境下都能进行鲁棒运动变得至关重要。", "method": "该研究采用了一种分层控制框架：高层使用ALIP模型的步态动力学，通过非线性MPC同时优化步态周期、步长和踝关节扭矩；中层则是一个线性MPC框架，它扩展了标准的SRB-MPC，纳入了简化的手臂和躯干动力学，并以高层ALIP轨迹作为参考。整个框架计算高效，高层MPC运行频率为40 Hz，中层MPC为500 Hz，均在板载迷你PC上运行。方法通过在Unitree G1人形机器人上的仿真和硬件实验进行验证。", "result": "自适应步态时序将推力恢复成功率提高了36%；上身控制改善了偏航扰动抑制能力。此外，该方法在多种室内外地形（包括草地、石板路和不平坦的健身垫）上都展现了鲁棒的运动能力。", "conclusion": "所提出的分层控制框架能够使人形机器人在多样化环境中实现鲁棒且多功能的运动，显著提升了扰动抑制和推力恢复能力，同时保持了计算效率。"}}
{"id": "2509.04457", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04457", "abs": "https://arxiv.org/abs/2509.04457", "authors": ["Xiao Zhang", "Dongyuan Li", "Liuyu Xiang", "Yao Zhang", "Cheng Zhong", "Zhaofeng He"], "title": "Do MLLMs Really Understand the Charts?", "comment": "19 pages,15 figures", "summary": "Although Multimodal Large Language Models (MLLMs) have demonstrated\nincreasingly impressive performance in chart understanding, most of them\nexhibit alarming hallucinations and significant performance degradation when\nhandling non-annotated charts. Therefore, a question arises: Do MLLMs really\nunderstand the charts? Since a human is capable of understanding charts and\nestimating the values by visual reasoning, we first carefully establish a\ncomprehensive Chart Reasoning Benchmark CRBench to rigorously evaluate the\nvisual reasoning abilities of MLLMs on non-annotated charts. We argue that\nMLLMs are primarily relying on recognition rather than reasoning to interpret\nthe charts. To steer MLLMs to reasonable chart understanding, we propose\nChartReasoner that mimics human behavior by grounding their estimation in chart\nunderstanding. Extensive results on the proposed CRBench show that\nChartReasnoner-3B/7B achieves superior performance in chart reasoning, even\ncompared to GPT-4o and Gemini-2.5-Flash. More importantly, ChartReasnoner also\ndemonstrates the visual reasoning abilities in general chart comprehension on\npublic benchmarks, leading to significant performance gains and enabling MLLMs\nto rationally understand the charts. The code and dataset will be publicly\navailable upon publication.", "AI": {"tldr": "现有多模态大语言模型（MLLMs）在处理无标注图表时存在幻觉和性能下降问题，本文提出ChartReasoner模型和CRBench基准测试，旨在通过模仿人类视觉推理来提升MLLMs的图表理解能力。", "motivation": "尽管MLLMs在图表理解方面表现出色，但在处理无标注图表时出现严重的幻觉和性能下降，这引发了对它们是否真正理解图表的质疑。人类能够通过视觉推理理解图表并估计数值，这促使研究者探索如何让MLLMs也具备类似能力。", "method": "1. 建立了全面的图表推理基准CRBench，以严格评估MLLMs在无标注图表上的视觉推理能力。2. 提出MLLMs主要依赖识别而非推理来解释图表。3. 提出了ChartReasoner模型，通过将估计结果建立在图表理解的基础上，模仿人类行为来引导MLLMs进行合理的图表理解。", "result": "在CRBench上的广泛实验表明，ChartReasoner-3B/7B取得了卓越的图表推理性能，甚至优于GPT-4o和Gemini-2.5-Flash。更重要的是，ChartReasoner还在公共基准测试上展示了其在通用图表理解方面的视觉推理能力，带来了显著的性能提升。", "conclusion": "ChartReasoner通过增强MLLMs的视觉推理能力，使其能够更理性地理解图表，显著解决了现有模型在无标注图表处理中的幻觉和性能问题。"}}
{"id": "2509.04548", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04548", "abs": "https://arxiv.org/abs/2509.04548", "authors": ["Hongyang Wei", "Baixin Xu", "Hongbo Liu", "Cyrus Wu", "Jie Liu", "Yi Peng", "Peiyu Wang", "Zexiang Liu", "Jingwen He", "Yidan Xietian", "Chuanxin Tang", "Zidong Wang", "Yichen Wei", "Liang Hu", "Boyi Jiang", "William Li", "Ying He", "Yang Liu", "Xuchen Song", "Eric Li", "Yahui Zhou"], "title": "Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model", "comment": null, "summary": "Recent advances in multimodal models have demonstrated impressive\ncapabilities in unified image generation and editing. However, many prominent\nopen-source models prioritize scaling model parameters over optimizing training\nstrategies, limiting their efficiency and performance. In this work, we present\nUniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which\nachieves state-of-the-art image generation and editing while extending\nseamlessly into a unified multimodal framework. Our approach begins with\narchitectural modifications to SD3.5-Medium and large-scale pre-training on\nhigh-quality data, enabling joint text-to-image generation and editing\ncapabilities. To enhance instruction following and editing consistency, we\npropose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which\neffectively strengthens both tasks in a staged manner. We empirically validate\nthat the reinforcement phases for different tasks are mutually beneficial and\ndo not induce negative interference. After pre-training and reinforcement\nstrategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and\nediting capabilities than models with significantly larger generation\nparameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following\nthe MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a\nconnector and perform joint training to launch a unified multimodal model\nUniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and\nediting, achieving top-tier performance across diverse tasks with a simple and\nscalable training paradigm. This consistently validates the effectiveness and\ngeneralizability of our proposed training paradigm, which we formalize as\nSkywork UniPic 2.0.", "AI": {"tldr": "本文提出了UniPic2-SD3.5M-Kontext，一个2B参数的DiT模型，通过优化训练策略实现了最先进的图像生成和编辑能力。在此基础上，通过连接器与Qwen2.5-VL-7B结合，构建了统一的多模态模型UniPic2-Metaquery，整合了理解、生成和编辑，并在各种任务中取得了顶尖性能。", "motivation": "许多主流开源多模态模型侧重于扩展模型参数而非优化训练策略，这限制了它们的效率和性能。", "method": "1. 对SD3.5-Medium进行架构修改并进行大规模高质量数据预训练，以实现图文生成和编辑能力。2. 提出渐进式双任务强化策略（PDTR），分阶段增强指令遵循和编辑一致性。3. 将UniPic2-SD3.5M-Kontext与Qwen2.5-VL-7B通过连接器进行联合训练，构建统一的多模态模型UniPic2-Metaquery。", "result": "1. UniPic2-SD3.5M-Kontext（2B参数）在图像生成和编辑方面超越了参数量更大的模型（如BAGEL 7B和Flux-Kontext 12B）。2. PDTR策略的强化阶段对不同任务互利且不产生负面干扰。3. UniPic2-Metaquery在理解、生成和编辑等多样任务中实现了顶尖性能。", "conclusion": "所提出的训练范式（Skywork UniPic 2.0）被证实是有效、可泛化且可扩展的，能够以更少的参数实现强大的统一多模态能力。"}}
{"id": "2509.04646", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2509.04646", "abs": "https://arxiv.org/abs/2509.04646", "authors": ["Philippe J. Giabbanelli", "Ameeta Agrawal"], "title": "Towards Personalized Explanations for Health Simulations: A Mixed-Methods Framework for Stakeholder-Centric Summarization", "comment": "Accepted at the AAAI 2025 Fall Symposium Series. November 6-8, 2025,\n  Arlington, VA, USA", "summary": "Modeling & Simulation (M&S) approaches such as agent-based models hold\nsignificant potential to support decision-making activities in health, with\nrecent examples including the adoption of vaccines, and a vast literature on\nhealthy eating behaviors and physical activity behaviors. These models are\npotentially usable by different stakeholder groups, as they support\npolicy-makers to estimate the consequences of potential interventions and they\ncan guide individuals in making healthy choices in complex environments.\nHowever, this potential may not be fully realized because of the models'\ncomplexity, which makes them inaccessible to the stakeholders who could benefit\nthe most. While Large Language Models (LLMs) can translate simulation outputs\nand the design of models into text, current approaches typically rely on\none-size-fits-all summaries that fail to reflect the varied informational needs\nand stylistic preferences of clinicians, policymakers, patients, caregivers,\nand health advocates. This limitation stems from a fundamental gap: we lack a\nsystematic understanding of what these stakeholders need from explanations and\nhow to tailor them accordingly. To address this gap, we present a step-by-step\nframework to identify stakeholder needs and guide LLMs in generating tailored\nexplanations of health simulations. Our procedure uses a mixed-methods design\nby first eliciting the explanation needs and stylistic preferences of diverse\nhealth stakeholders, then optimizing the ability of LLMs to generate tailored\noutputs (e.g., via controllable attribute tuning), and then evaluating through\na comprehensive range of metrics to further improve the tailored generation of\nsummaries.", "AI": {"tldr": "该研究提出了一种分步框架，通过识别不同健康利益相关者的解释需求和风格偏好，并指导大型语言模型（LLMs）生成定制化的健康模拟解释，以提高模拟模型的可及性。", "motivation": "健康领域的建模与仿真（M&S）方法（如基于代理的模型）在支持决策方面潜力巨大，但其复杂性使得这些模型对关键利益相关者（如临床医生、政策制定者、患者等）而言难以理解和利用。虽然LLMs可以转换模拟输出，但现有方法通常提供“一刀切”的摘要，未能满足不同利益相关者多样化的信息需求和风格偏好。", "method": "该研究提出一个分步框架，采用混合方法设计：首先，通过启发式方法获取不同健康利益相关者的解释需求和风格偏好；其次，优化LLMs生成定制化输出的能力（例如，通过可控属性调整）；最后，通过一系列综合指标评估并进一步改进定制化摘要的生成。", "result": "该研究的主要成果是提出了一个系统的分步框架，用于识别健康领域利益相关者的解释需求，并指导LLMs生成针对性的、定制化的健康模拟解释，以克服模型复杂性和现有LLM摘要普适性的局限性。", "conclusion": "该框架旨在弥合复杂健康模拟与利益相关者理解之间的鸿沟，通过LLMs生成定制化解释，使健康模拟模型更易于访问和利用，从而充分发挥其在支持健康决策中的潜力。"}}
{"id": "2509.04514", "categories": ["eess.SY", "cs.SY", "stat.ME", "62L10", "I.6.6"], "pdf": "https://arxiv.org/pdf/2509.04514", "abs": "https://arxiv.org/abs/2509.04514", "authors": ["Yuwei Zhou", "Sigrún Andradóttir", "Seong-Hee Kim", "Chuljin Park"], "title": "Indifference-Zone Relaxation Procedures for Finding Feasible Systems", "comment": null, "summary": "We consider the problem of finding feasible systems with respect to\nstochastic constraints when system performance is evaluated through simulation.\nOur objective is to solve this problem with high computational efficiency and\nstatistical validity. Existing indifference-zone (IZ) procedures introduce a\nfixed tolerance level, which denotes how much deviation the decision-maker is\nwilling to accept from the threshold in the constraint. These procedures are\ndeveloped under the assumption that all systems' performance measures are\nexactly the tolerance level away from the threshold, leading to unnecessary\nsimulations. In contrast, IZ-free procedures, which eliminate the tolerance\nlevel, perform well when systems' performance measures are far from the\nthreshold. However, they may significantly underperform compared to IZ\nprocedures when systems' performance measures are close to the threshold. To\naddress these challenges, we propose the Indifference-Zone Relaxation (IZR)\nprocedure, IZR introduces a set of relaxed tolerance levels and utilizes two\nsubroutines for each level: one to identify systems that are clearly feasible\nand the other to exclude those that are clearly infeasible. We also develop the\nIZR procedure with estimation (IZE), which introduces two relaxed tolerance\nlevels for each system and constraint: one matching the original tolerance\nlevel and the other based on an estimate of the system's performance measure.\nBy employing different tolerance levels, these procedures facilitate early\nfeasibility determination with statistical validity. We prove that IZR and IZE\ndetermine system feasibility with the desired probability and show through\nexperiments that they significantly reduce the number of observations required\ncompared to an existing procedure.", "AI": {"tldr": "本文提出了一种名为“无差异区松弛 (IZR)”的新方法，以及其改进版本“带估计的IZR (IZE)”，用于通过模拟寻找具有随机约束的系统可行性。这些方法通过引入松弛的容忍水平，提高了计算效率和统计有效性，显著减少了所需的观测次数。", "motivation": "现有方法在处理随机约束下的系统可行性问题时存在局限性：传统的无差异区 (IZ) 程序引入了固定的容忍水平，导致不必要的模拟；而无IZ程序在系统性能接近阈值时表现不佳。因此，需要一种更高效且统计上有效的方法来解决这一挑战。", "method": "本文提出了IZR程序，它引入了一组松弛的容忍水平，并为每个水平使用两个子程序：一个识别明显可行的系统，另一个排除明显不可行的系统。在此基础上，进一步开发了IZE程序，它为每个系统和约束引入了两个松弛的容忍水平：一个匹配原始容忍水平，另一个基于系统性能度量的估计。这些程序通过采用不同的容忍水平，实现了统计有效的早期可行性判定。", "result": "研究证明，IZR和IZE程序能够以期望的概率确定系统可行性。通过实验表明，与现有程序相比，这些新方法显著减少了所需的观测次数，提高了计算效率。", "conclusion": "IZR和IZE程序通过引入松弛的容忍水平，为解决随机约束下的系统可行性问题提供了统计有效且计算高效的解决方案。它们能够实现早期可行性判定，并显著降低模拟成本。"}}
{"id": "2509.04870", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04870", "abs": "https://arxiv.org/abs/2509.04870", "authors": ["Yuanyuan Gui", "Wei Li", "Yinjian Wang", "Xiang-Gen Xia", "Mauro Marty", "Christian Ginzler", "Zuyuan Wang"], "title": "Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images", "comment": null, "summary": "Recent advances in semantic segmentation of multi-modal remote sensing images\nhave significantly improved the accuracy of tree cover mapping, supporting\napplications in urban planning, forest monitoring, and ecological assessment.\nIntegrating data from multiple modalities-such as optical imagery, light\ndetection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown\nsuperior performance over single-modality methods. However, these data are\noften acquired days or even months apart, during which various changes may\noccur, such as vegetation disturbances (e.g., logging, and wildfires) and\nvariations in imaging quality. Such temporal misalignments introduce\ncross-modal uncertainty, especially in high-resolution imagery, which can\nseverely degrade segmentation accuracy. To address this challenge, we propose\nMURTreeFormer, a novel multi-modal segmentation framework that mitigates and\nleverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer\ntreats one modality as primary and others as auxiliary, explicitly modeling\npatch-level uncertainty in the auxiliary modalities via a probabilistic latent\nrepresentation. Uncertain patches are identified and reconstructed from the\nprimary modality's distribution through a VAE-based resampling mechanism,\nproducing enhanced auxiliary features for fusion. In the decoder, a gradient\nmagnitude attention (GMA) module and a lightweight refinement head (RH) are\nfurther integrated to guide attention toward tree-like structures and to\npreserve fine-grained spatial details. Extensive experiments on multi-modal\ndatasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly\nimproves segmentation performance and effectively reduces the impact of\ntemporally induced aleatoric uncertainty.", "AI": {"tldr": "本文提出MURTreeFormer框架，通过概率潜在表示和VAE重采样机制，有效缓解多模态遥感图像中因时间错位导致的交叉模态不确定性，显著提升树木覆盖分割精度。", "motivation": "多模态遥感图像在树木覆盖测绘中表现优越，但不同模态数据采集时间可能相隔数天甚至数月，导致植被变化或成像质量差异，引入交叉模态不确定性，严重降低分割精度。", "method": "MURTreeFormer将一种模态设为主模态，其他为辅助模态。它通过概率潜在表示显式建模辅助模态的块级不确定性。不确定补丁通过基于VAE的重采样机制从主模态分布中重建，生成增强的辅助特征用于融合。解码器中进一步集成梯度幅度注意力（GMA）模块和轻量级细化头（RH），以引导对树状结构的关注并保留精细空间细节。", "result": "在上海和苏黎世的多模态数据集上进行的广泛实验表明，MURTreeFormer显著提高了分割性能，并有效减少了时间引起的不确定性影响。", "conclusion": "MURTreeFormer是一个新颖的多模态分割框架，能够有效缓解并利用不确定性，实现鲁棒的树木覆盖测绘，解决了多模态遥感图像中时间错位带来的挑战。"}}
{"id": "2509.04737", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04737", "abs": "https://arxiv.org/abs/2509.04737", "authors": ["Ryoga Oishi", "Sho Sakaino", "Toshiaki Tsuji"], "title": "Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics", "comment": "16 pages, 5 figures, Accepted at CoRL2025", "summary": "In the field of robot learning, coordinating robot actions through language\ninstructions is becoming increasingly feasible. However, adapting actions to\nhuman instructions remains challenging, as such instructions are often\nqualitative and require exploring behaviors that satisfy varying conditions.\nThis paper proposes a motion generation model that adapts robot actions in\nresponse to modifier directives human instructions imposing behavioral\nconditions during task execution. The proposed method learns a mapping from\nmodifier directives to actions by segmenting demonstrations into short\nsequences, assigning weakly supervised labels corresponding to specific\nmodifier types. We evaluated our method in wiping and pick and place tasks.\nResults show that it can adjust motions online in response to modifier\ndirectives, unlike conventional batch-based methods that cannot adapt during\nexecution.", "AI": {"tldr": "本文提出了一种机器人运动生成模型，能够根据人类指令中的修饰词（如行为条件）在线调整机器人动作。", "motivation": "机器人学习领域中，通过语言指令协调机器人动作日益可行，但将动作适应人类指令仍具挑战，因为此类指令通常是定性的，需要探索满足不同条件的行为。", "method": "该方法通过将演示分割成短序列，并分配对应特定修饰词类型的弱监督标签，学习修饰词指令到动作的映射。", "result": "在擦拭和抓取放置任务中，该方法能够在线响应修饰词指令调整动作，这与无法在执行期间适应的传统批处理方法不同。", "conclusion": "该研究成功地提出了一种机器人运动生成模型，使其能够在线适应人类指令中的定性修饰词，从而提高了机器人行为的灵活性和适应性。"}}
{"id": "2509.04458", "categories": ["cs.CL", "I.2"], "pdf": "https://arxiv.org/pdf/2509.04458", "abs": "https://arxiv.org/abs/2509.04458", "authors": ["Daniel B. Hier", "Steven Keith Platt", "Tayo Obafemi-Ajayi"], "title": "Predicting Failures of LLMs to Link Biomedical Ontology Terms to Identifiers Evidence Across Models and Ontologies", "comment": "Accepted for Presentation, IEEE-EMBS International Conference on\n  Biomedical and Health Informatics (BHI 25), Atlanta GA USA, October 26-29,\n  2025", "summary": "Large language models often perform well on biomedical NLP tasks but may fail\nto link ontology terms to their correct identifiers. We investigate why these\nfailures occur by analyzing predictions across two major ontologies, Human\nPhenotype Ontology and Gene Ontology, and two high-performing models, GPT-4o\nand LLaMa 3.1 405B. We evaluate nine candidate features related to term\nfamiliarity, identifier usage, morphology, and ontology structure. Univariate\nand multivariate analyses show that exposure to ontology identifiers is the\nstrongest predictor of linking success.", "AI": {"tldr": "研究发现大型语言模型在生物医学NLP任务中难以将本体术语链接到正确标识符，通过分析GPT-4o和LLaMa 3.1 405B模型在HDO和GO本体上的表现，发现模型对本体标识符的熟悉程度是链接成功的最强预测因子。", "motivation": "大型语言模型在生物医学自然语言处理任务中表现良好，但经常无法将本体术语与其正确的标识符关联起来。本研究旨在调查这些失败发生的原因。", "method": "研究分析了GPT-4o和LLaMa 3.1 405B两个高性能模型在人类表型本体（Human Phenotype Ontology）和基因本体（Gene Ontology）这两个主要本体上的预测结果。评估了九个候选特征，涉及术语熟悉度、标识符使用、形态学和本体结构。采用单变量和多变量分析方法。", "result": "单变量和多变量分析结果表明，模型对本体标识符的暴露或熟悉程度是链接成功的最强预测因子。", "conclusion": "模型能否成功将本体术语链接到正确标识符的关键因素在于其对本体标识符本身的接触和熟悉程度。"}}
{"id": "2509.04582", "categories": ["cs.CV", "I.3.6; I.3.3"], "pdf": "https://arxiv.org/pdf/2509.04582", "abs": "https://arxiv.org/abs/2509.04582", "authors": ["Jingyi Lu", "Kai Han"], "title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping", "comment": "Accepted to ICCV 2025. Project page:\n  https://visual-ai.github.io/inpaint4drag/", "summary": "Drag-based image editing has emerged as a powerful paradigm for intuitive\nimage manipulation. However, existing approaches predominantly rely on\nmanipulating the latent space of generative models, leading to limited\nprecision, delayed feedback, and model-specific constraints. Accordingly, we\npresent Inpaint4Drag, a novel framework that decomposes drag-based editing into\npixel-space bidirectional warping and image inpainting. Inspired by elastic\nobject deformation in the physical world, we treat image regions as deformable\nmaterials that maintain natural shape under user manipulation. Our method\nachieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at\n512x512 resolution, significantly improving the interaction experience compared\nto existing methods that require minutes per edit. By transforming drag inputs\ndirectly into standard inpainting formats, our approach serves as a universal\nadapter for any inpainting model without architecture modification,\nautomatically inheriting all future improvements in inpainting technology.\nExtensive experiments demonstrate that our method achieves superior visual\nquality and precise control while maintaining real-time performance. Project\npage: https://visual-ai.github.io/inpaint4drag/", "AI": {"tldr": "Inpaint4Drag提出了一种新的拖拽式图像编辑框架，通过像素空间双向变形和图像修复，实现了实时、高精度且通用的编辑体验，克服了现有潜在空间方法的局限。", "motivation": "现有的拖拽式图像编辑方法主要依赖生成模型的潜在空间操作，导致精度有限、反馈延迟以及模型特定限制，影响了用户交互体验。", "method": "该方法将拖拽式编辑分解为像素空间双向变形和图像修复。受物理世界弹性物体变形启发，将图像区域视为可变形材料。它将拖拽输入直接转换为标准修复格式，使其能作为任何修复模型的通用适配器，无需修改架构。", "result": "Inpaint4Drag实现了512x512分辨率下实时变形预览（0.01秒）和高效修复（0.3秒），显著优于现有方法（需数分钟）。实验表明，该方法在保持实时性能的同时，实现了卓越的视觉质量和精确控制。", "conclusion": "Inpaint4Drag通过将拖拽式编辑分解为像素空间变形和图像修复，提供了一种实时、高精度、通用且能自动继承未来修复技术改进的图像编辑解决方案，显著提升了交互体验。"}}
{"id": "2509.04676", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04676", "abs": "https://arxiv.org/abs/2509.04676", "authors": ["Sasha Mitts"], "title": "An Approach to Grounding AI Model Evaluations in Human-derived Criteria", "comment": "4 figures, 6 pages, presented at CHI 2025 Workshop on Human-AI\n  Interaction for Augmented Reasoning", "summary": "In the rapidly evolving field of artificial intelligence (AI), traditional\nbenchmarks can fall short in attempting to capture the nuanced capabilities of\nAI models. We focus on the case of physical world modeling and propose a novel\napproach to augment existing benchmarks with human-derived evaluation criteria,\naiming to enhance the interpretability and applicability of model behaviors.\nGrounding our study in the Perception Test and OpenEQA benchmarks, we conducted\nin-depth interviews and large-scale surveys to identify key cognitive skills,\nsuch as Prioritization, Memorizing, Discerning, and Contextualizing, that are\ncritical for both AI and human reasoning. Our findings reveal that participants\nperceive AI as lacking in interpretive and empathetic skills yet hold high\nexpectations for AI performance. By integrating insights from our findings into\nbenchmark design, we offer a framework for developing more human-aligned means\nof defining and measuring progress. This work underscores the importance of\nuser-centered evaluation in AI development, providing actionable guidelines for\nresearchers and practitioners aiming to align AI capabilities with human\ncognitive processes. Our approach both enhances current benchmarking practices\nand sets the stage for future advancements in AI model evaluation.", "AI": {"tldr": "该研究提出了一种以人为中心的评估框架，通过整合人类认知的关键技能来增强现有AI基准测试，旨在更准确地衡量AI模型的细微能力，尤其是在物理世界建模方面。", "motivation": "传统AI基准测试难以捕捉AI模型的细微能力，尤其是在物理世界建模方面，导致模型行为的解释性和适用性不足。", "method": "研究以Perception Test和OpenEQA基准为基础，通过深度访谈和大规模调查识别出优先级、记忆、辨别和情境化等关键认知技能。这些技能被整合到基准设计中，以开发更符合人类的评估方法。", "result": "研究发现，参与者认为AI在解释性和同理心技能方面存在不足，但对其性能抱有很高期望。通过将这些发现整合到基准设计中，研究提供了一个开发更符合人类的AI进展定义和衡量方法的框架。", "conclusion": "该工作强调了在AI开发中以用户为中心评估的重要性，为研究人员和实践者提供了可操作的指导，以使AI能力与人类认知过程对齐。这种方法既增强了当前的基准测试实践，也为AI模型评估的未来发展奠定了基础。"}}
{"id": "2509.04533", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04533", "abs": "https://arxiv.org/abs/2509.04533", "authors": ["Hai Wang", "Baoshen Guo", "Xiaolei Zhou", "Shuai Wang", "Zhiqing Hong", "Tian He"], "title": "Resource-Oriented Optimization of Electric Vehicle Systems: A Data-Driven Survey on Charging Infrastructure, Scheduling, and Fleet Management", "comment": null, "summary": "Driven by growing concerns over air quality and energy security, electric\nvehicles (EVs) has experienced rapid development and are reshaping global\ntransportation systems and lifestyle patterns. Compared to traditional\ngasoline-powered vehicles, EVs offer significant advantages in terms of lower\nenergy consumption, reduced emissions, and decreased operating costs. However,\nthere are still some core challenges to be addressed: (i) Charging station\ncongestion and operational inefficiencies during peak hours, (ii) High charging\ncost under dynamic electricity pricing schemes, and (iii) Conflicts between\ncharging needs and passenger service requirements.Hence, in this paper, we\npresent a comprehensive review of data-driven models and approaches proposed in\nthe literature to address the above challenges. These studies cover the entire\nlifecycle of EV systems, including charging station deployment, charging\nscheduling strategies, and large-scale fleet management. Moreover, we discuss\nthe broader implications of EV integration across multiple domains, such as\nhuman mobility, smart grid infrastructure, and environmental sustainability,\nand identify key opportunities and directions for future research.", "AI": {"tldr": "本文综述了数据驱动模型和方法，旨在解决电动汽车（EV）系统在充电站拥堵、充电成本和乘客服务需求冲突等方面的核心挑战，并探讨了其对人类出行、智能电网和环境可持续性的影响，同时指出了未来的研究方向。", "motivation": "电动汽车的快速发展带来了空气质量改善和能源安全优势，但仍面临充电站拥堵、动态电价下的高充电成本以及充电需求与乘客服务要求冲突等核心挑战，促使研究者寻求解决方案。", "method": "本文采用文献综述的方法，全面回顾了现有文献中提出的数据驱动模型和方法，这些研究涵盖了电动汽车系统的整个生命周期，包括充电站部署、充电调度策略和大规模车队管理。", "result": "研究结果包括对解决电动汽车挑战的数据驱动模型和方法的综合回顾，这些方法覆盖了充电站部署、充电调度和车队管理。此外，论文还讨论了电动汽车整合在人类出行、智能电网基础设施和环境可持续性等多个领域中的广泛影响，并识别了未来研究的关键机会和方向。", "conclusion": "通过对数据驱动模型和方法的全面回顾，本文为解决电动汽车面临的核心挑战提供了深入见解，并强调了电动汽车在多领域融合的广泛影响，为未来研究指明了方向。"}}
{"id": "2509.04888", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.04888", "abs": "https://arxiv.org/abs/2509.04888", "authors": ["Natascha Niessen", "Carolin M. Pirkl", "Ana Beatriz Solana", "Hannah Eichhorn", "Veronika Spieker", "Wenqi Huang", "Tim Sprenger", "Marion I. Menzel", "Julia A. Schnabel"], "title": "INR meets Multi-Contrast MRI Reconstruction", "comment": null, "summary": "Multi-contrast MRI sequences allow for the acquisition of images with varying\ntissue contrast within a single scan. The resulting multi-contrast images can\nbe used to extract quantitative information on tissue microstructure. To make\nsuch multi-contrast sequences feasible for clinical routine, the usually very\nlong scan times need to be shortened e.g. through undersampling in k-space.\nHowever, this comes with challenges for the reconstruction. In general,\nadvanced reconstruction techniques such as compressed sensing or deep\nlearning-based approaches can enable the acquisition of high-quality images\ndespite the acceleration. In this work, we leverage redundant anatomical\ninformation of multi-contrast sequences to achieve even higher acceleration\nrates. We use undersampling patterns that capture the contrast information\nlocated at the k-space center, while performing complementary undersampling\nacross contrasts for high frequencies. To reconstruct this highly sparse\nk-space data, we propose an implicit neural representation (INR) network that\nis ideal for using the complementary information acquired across contrasts as\nit jointly reconstructs all contrast images. We demonstrate the benefits of our\nproposed INR method by applying it to multi-contrast MRI using the MPnRAGE\nsequence, where it outperforms the state-of-the-art parallel imaging compressed\nsensing (PICS) reconstruction method, even at higher acceleration factors.", "AI": {"tldr": "本研究提出了一种基于隐式神经表示（INR）网络的多对比度MRI重建方法，通过互补欠采样模式和联合重建，实现了更高的加速因子并优于现有技术。", "motivation": "多对比度MRI序列扫描时间长，限制了其临床应用。通过k空间欠采样可以缩短扫描时间，但这给重建带来了挑战。需要先进的重建技术来在加速下获得高质量图像。", "method": "该方法利用多对比度序列中冗余的解剖信息，设计了特殊的欠采样模式：在k空间中心捕获对比度信息，同时在高频部分对不同对比度进行互补欠采样。提出了一种隐式神经表示（INR）网络，该网络能够联合重建所有对比度图像，从而有效利用跨对比度获取的互补信息来重建高度稀疏的k空间数据。", "result": "将所提出的INR方法应用于MPnRAGE序列的多对比度MRI重建，结果表明，即使在更高的加速因子下，该方法也优于最先进的并行成像压缩感知（PICS）重建方法。", "conclusion": "所提出的INR方法通过利用多对比度序列的互补信息和联合重建，能够实现更高加速率的多对比度MRI，并获得优于现有技术的重建质量，有望使多对比度序列更适用于临床常规应用。"}}
{"id": "2509.04836", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04836", "abs": "https://arxiv.org/abs/2509.04836", "authors": ["Dongping Li", "Shaoting Peng", "John Pohovey", "Katherine Rose Driggs-Campbell"], "title": "COMMET: A System for Human-Induced Conflicts in Mobile Manipulation of Everyday Tasks", "comment": null, "summary": "Continuous advancements in robotics and AI are driving the integration of\nrobots from industry into everyday environments. However, dynamic and\nunpredictable human activities in daily lives would directly or indirectly\nconflict with robot actions. Besides, due to the social attributes of such\nhuman-induced conflicts, solutions are not always unique and depend highly on\nthe user's personal preferences. To address these challenges and facilitate the\ndevelopment of household robots, we propose COMMET, a system for human-induced\nCOnflicts in Mobile Manipulation of Everyday Tasks. COMMET employs a hybrid\ndetection approach, which begins with multi-modal retrieval and escalates to\nfine-tuned model inference for low-confidence cases. Based on collected user\npreferred options and settings, GPT-4o will be used to summarize user\npreferences from relevant cases. In preliminary studies, our detection module\nshows better accuracy and latency compared with GPT models. To facilitate\nfuture research, we also design a user-friendly interface for user data\ncollection and demonstrate an effective workflow for real-world deployments.", "AI": {"tldr": "该论文提出了COMMET系统，旨在解决家庭环境中机器人与人类活动之间的冲突，并通过混合检测方法和GPT-4o总结用户偏好来提供个性化解决方案。", "motivation": "随着机器人和AI技术的发展，机器人正从工业领域进入日常生活环境。然而，日常生活中动态且不可预测的人类活动可能与机器人行为发生冲突。由于这些冲突的社会属性，解决方案往往不唯一，且高度依赖用户的个人偏好，这阻碍了家用机器人的发展。", "method": "论文提出了COMMET系统，用于处理日常任务中移动操作引起的人类冲突。COMMET采用混合检测方法，首先进行多模态检索，对于低置信度情况则升级为微调模型推理。此外，系统会收集用户偏好选项和设置，并利用GPT-4o从相关案例中总结用户偏好。", "result": "初步研究表明，COMMET的检测模块在准确性和延迟方面优于GPT模型。为了促进未来的研究，论文还设计了一个用户友好的界面用于用户数据收集，并展示了一个有效的实际部署工作流程。", "conclusion": "COMMET系统提供了一种有效的方法来检测和解决家庭环境中机器人与人类的冲突，通过结合混合检测和基于用户偏好的个性化解决方案。其检测模块表现出优越的性能，并且系统设计了数据收集界面和部署工作流程，为家用机器人的进一步发展奠定了基础。"}}
{"id": "2509.04459", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04459", "abs": "https://arxiv.org/abs/2509.04459", "authors": ["Shiqin Han", "Manning Gao", "Menghua Jiang", "Yuncheng Jiang", "Haifeng Hu", "Sijie Mai"], "title": "Uncertainty-Aware Collaborative System of Large and Small Models for Multimodal Sentiment Analysis", "comment": null, "summary": "The advent of Multimodal Large Language Models (MLLMs) has significantly\nadvanced the state-of-the-art in multimodal machine learning, yet their\nsubstantial computational demands present a critical barrier to real-world\ndeployment. Conversely, smaller, specialized models offer high efficiency but\noften at the cost of performance. To reconcile this performance-efficiency\ntrade-off, we propose a novel Uncertainty-Aware Collaborative System (U-ACS)\nthat synergistically orchestrates a powerful MLLM (e.g., HumanOmni) and a\nlightweight baseline model for multimodal sentiment analysis. The core of our\nsystem is an uncertainty-driven cascade mechanism, where the efficient small\nmodel first acts as a rapid filter for all input samples. Only those samples\nyielding high predictive uncertainty, thereby indicating greater difficulty,\nare selectively escalated to the MLLM for more sophisticated analysis.\nFurthermore, our system introduces advanced strategies to handle ambiguous or\nconflicting predictions, including weighted averaging for predictions of\nsimilar polarity and a prompt-based cross-verification to resolve conflicting\npredictions when both models exhibit high uncertainty. This\nsample-difficulty-aware approach allows for a dynamic allocation of\ncomputational resources, drastically reducing inference costs while retaining\nthe high accuracy of MLLM. Extensive experiments on benchmark datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nwhile requiring only a fraction of the computational resources compared to\nusing a standalone MLLM.", "AI": {"tldr": "该研究提出了一种不确定性感知协同系统（U-ACS），结合轻量级模型和大型多模态语言模型（MLLM），通过不确定性驱动的级联机制，在多模态情感分析中实现了计算效率和性能的平衡。", "motivation": "多模态大型语言模型（MLLMs）性能强大但计算成本高昂，难以实际部署；小型模型效率高但性能不足。研究旨在解决这种性能与效率之间的矛盾。", "method": "提出U-ACS系统，核心是不确定性驱动的级联机制：轻量级模型首先处理所有输入，只有高预测不确定性的样本才会被上报给MLLM进行深入分析。此外，系统还包含处理模糊或冲突预测的策略，如相似极性预测的加权平均和高不确定性冲突预测的基于提示的交叉验证。", "result": "在基准数据集上的实验表明，所提出的方法在显著降低计算资源消耗（相比单独使用MLLM）的同时，实现了最先进的性能。", "conclusion": "U-ACS通过动态分配计算资源，有效平衡了多模态情感分析中的性能和计算效率，使得高性能MLLM的优势能够在资源受限的环境中得到利用。"}}
{"id": "2509.04597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04597", "abs": "https://arxiv.org/abs/2509.04597", "authors": ["Jin Ma", "Mohammed Aldeen", "Christopher Salas", "Feng Luo", "Mashrur Chowdhury", "Mert Pesé", "Long Cheng"], "title": "DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models", "comment": null, "summary": "Object detection is fundamental to various real-world applications, such as\nsecurity monitoring and surveillance video analysis. Despite their\nadvancements, state-of-theart object detectors are still vulnerable to\nadversarial patch attacks, which can be easily applied to real-world objects to\neither conceal actual items or create non-existent ones, leading to severe\nconsequences. Given the current diversity of adversarial patch attacks and\npotential unknown threats, an ideal defense method should be effective,\ngeneralizable, and robust against adaptive attacks. In this work, we introduce\nDISPATCH, the first diffusion-based defense framework for object detection.\nUnlike previous works that aim to \"detect and remove\" adversarial patches,\nDISPATCH adopts a \"regenerate and rectify\" strategy, leveraging generative\nmodels to disarm attack effects while preserving the integrity of the input\nimage. Specifically, we utilize the in-distribution generative power of\ndiffusion models to regenerate the entire image, aligning it with benign data.\nA rectification process is then employed to identify and replace adversarial\nregions with their regenerated benign counterparts. DISPATCH is attack-agnostic\nand requires no prior knowledge of the existing patches. Extensive experiments\nacross multiple detectors and attacks demonstrate that DISPATCH consistently\noutperforms state-of-the-art defenses on both hiding attacks and creating\nattacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and\nlowering the attack success rate to 24.8% on untargeted creating attacks.\nMoreover, it maintains strong robustness against adaptive attacks, making it a\npractical and reliable defense for object detection systems.", "AI": {"tldr": "本文提出DISPATCH，首个基于扩散模型的物体检测防御框架，采用“再生与纠正”策略，有效抵御各种对抗性补丁攻击，无需预知攻击类型。", "motivation": "先进的物体检测器易受对抗性补丁攻击，这些攻击可能隐藏物体或制造虚假物体，导致严重后果。鉴于攻击的多样性和潜在的未知威胁，需要一种有效、通用且能抵御自适应攻击的防御方法。", "method": "DISPATCH采用“再生与纠正”策略，而非传统的“检测与移除”。它利用扩散模型的分布内生成能力，重新生成整个图像以使其与良性数据对齐。随后，通过纠正过程识别并用重新生成的良性区域替换对抗性区域。该方法与攻击类型无关，无需预先了解补丁信息。", "result": "DISPATCH在多种检测器和攻击类型上均优于现有最先进的防御方法。在隐藏攻击上，其mAP.5得分达到89.3%；在非目标创建攻击上，攻击成功率降至24.8%。此外，它对自适应攻击也表现出强大的鲁棒性。", "conclusion": "DISPATCH是一种实用且可靠的物体检测系统防御方案，能够有效、普遍且鲁棒地应对各种对抗性补丁攻击，包括隐藏攻击和创建攻击，并能抵御自适应攻击。"}}
{"id": "2509.04731", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO", "68T05, 90C40, 91A26, 68T42, 93E35", "I.2.11; I.2.6; I.2.8; I.2.9; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.04731", "abs": "https://arxiv.org/abs/2509.04731", "authors": ["Brennen Hill"], "title": "Language-Driven Hierarchical Task Structures as Explicit World Models for Multi-Agent Learning", "comment": null, "summary": "The convergence of Language models, Agent models, and World models represents\na critical frontier for artificial intelligence. While recent progress has\nfocused on scaling Language and Agent models, the development of sophisticated,\nexplicit World Models remains a key bottleneck, particularly for complex,\nlong-horizon multi-agent tasks. In domains such as robotic soccer, agents\ntrained via standard reinforcement learning in high-fidelity but\nstructurally-flat simulators often fail due to intractable exploration spaces\nand sparse rewards. This position paper argues that the next frontier in\ndeveloping capable agents lies in creating environments that possess an\nexplicit, hierarchical World Model. We contend that this is best achieved\nthrough hierarchical scaffolding, where complex goals are decomposed into\nstructured, manageable subgoals. Drawing evidence from a systematic review of\n2024 research in multi-agent soccer, we identify a clear and decisive trend\ntowards integrating symbolic and hierarchical methods with multi-agent\nreinforcement learning (MARL). These approaches implicitly or explicitly\nconstruct a task-based world model to guide agent learning. We then propose a\nparadigm shift: leveraging Large Language Models to dynamically generate this\nhierarchical scaffold, effectively using language to structure the World Model\non the fly. This language-driven world model provides an intrinsic curriculum,\ndense and meaningful learning signals, and a framework for compositional\nlearning, enabling Agent Models to acquire sophisticated, strategic behaviors\nwith far greater sample efficiency. By building environments with explicit,\nlanguage-configurable task layers, we can bridge the gap between low-level\nreactive behaviors and high-level strategic team play, creating a powerful and\ngeneralizable framework for training the next generation of intelligent agents.", "AI": {"tldr": "该论文提出，为解决复杂多智能体任务中强化学习的局限性，应构建显式、分层且由语言模型动态生成的“世界模型”，以提供内在课程和密集学习信号，从而提高智能体的学习效率和策略能力。", "motivation": "语言模型、智能体模型和世界模型的融合是AI前沿，但复杂、长周期多智能体任务中，显式世界模型的开发仍是瓶颈。标准强化学习在结构扁平的模拟器中常因探索空间巨大和奖励稀疏而失败，例如在机器人足球等领域。", "method": "论文主张通过分层脚手架（将复杂目标分解为可管理的子目标）来创建具有显式、分层世界模型的环境。通过对2024年多智能体足球研究的系统综述，发现整合符号和分层方法与多智能体强化学习（MARL）的趋势。在此基础上，论文提出利用大型语言模型（LLM）动态生成这种分层脚手架，从而构建一个语言驱动的世界模型。", "result": "通过语言驱动的世界模型，可以提供内在课程、密集而有意义的学习信号，并支持组合学习，使智能体模型能够以更高的样本效率习得复杂的战略行为。这将弥合低级反应行为与高级战略团队协作之间的差距。", "conclusion": "通过构建具有显式、语言可配置任务层的环境，可以为训练下一代智能体提供一个强大且可泛化的框架，从而实现更复杂、更高效的学习和战略行为。"}}
{"id": "2509.04593", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04593", "abs": "https://arxiv.org/abs/2509.04593", "authors": ["Aditya Gahlawat", "Vivek Khatana", "Duo Wang", "Sambhu H. Karumanchi", "Naira Hovakimyan", "Petros Voulgaris"], "title": "Wasserstein Distributionally Robust Adaptive Covariance Steering", "comment": null, "summary": "We present a methodology for predictable and safe covariance steering control\nof uncertain nonlinear stochastic processes. The systems under consideration\nare subject to general uncertainties, which include unbounded random\ndisturbances (aleatoric uncertainties) and incomplete model knowledge\n(state-dependent epistemic uncertainties). These general uncertainties lead to\ntemporally evolving state distributions that are entirely unknown, can have\narbitrary shapes, and may diverge unquantifiably from expected behaviors,\nleading to unpredictable and unsafe behaviors. Our method relies on an\n$\\mathcal{L}_1$-adaptive control architecture that ensures robust control of\nuncertain stochastic processes while providing Wasserstein metric certificates\nin the space of probability measures. We show how these distributional\ncertificates can be incorporated into the high-level covariance control\nsteering to guarantee safe control. Unlike existing distributionally robust\nplanning and control methodologies, our approach avoids difficult-to-verify\nrequirements like the availability of finite samples from the true underlying\ndistribution or an a priori knowledge of time-varying ambiguity sets to which\nthe state distributions are assumed to belong.", "AI": {"tldr": "本文提出了一种针对不确定非线性随机过程的、可预测且安全的协方差控制方法，通过结合$\\mathcal{L}_1$-自适应控制和 Wasserstein 度量证书来处理一般不确定性。", "motivation": "现有方法难以处理包含无界随机扰动和不完整模型知识的普遍不确定性，导致状态分布未知、形状任意且可能无法量化地偏离预期行为，从而引发不可预测和不安全的控制。", "method": "该方法基于$\\mathcal{L}_1$-自适应控制架构，确保对不确定随机过程的鲁棒控制，并提供概率测度空间中的 Wasserstein 度量证书。这些分布证书被整合到高层协方差控制中，以保证安全控制。", "result": "该方法能保证安全控制，并避免了现有分布鲁棒规划和控制方法中难以验证的要求，如需要真实潜在分布的有限样本或先验知识的时变模糊集。", "conclusion": "研究提出了一种新颖的方法，实现了在普遍不确定性下的可预测和安全协方差控制，克服了现有技术在处理未知和复杂状态分布方面的局限性。"}}
{"id": "2509.05154", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05154", "abs": "https://arxiv.org/abs/2509.05154", "authors": ["Julia Dietlmeier", "Oluwabukola Grace Adegboro", "Vayangi Ganepola", "Claudia Mazo", "Noel E. O'Connor"], "title": "VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation", "comment": "Medical Imaging with Deep Learning (MIDL 2025) short paper", "summary": "Vision-language models and their adaptations to image segmentation tasks\npresent enormous potential for producing highly accurate and interpretable\nresults. However, implementations based on CLIP and BiomedCLIP are still\nlagging behind more sophisticated architectures such as CRIS. In this work,\ninstead of focusing on text prompt engineering as is the norm, we attempt to\nnarrow this gap by showing how to ensemble vision-language segmentation models\n(VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice\nscore improvement of 6.3% on the BKAI polyp dataset using the ensembled\nBiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%.\nFurthermore, we provide initial results on additional four radiology and\nnon-radiology datasets. We conclude that ensembling works differently across\nthese datasets (from outperforming to underperforming the CRIS model),\nindicating a topic for future investigation by the community. The code is\navailable at https://github.com/juliadietlmeier/VLSM-Ensemble.", "AI": {"tldr": "本文提出了一种将视觉语言分割模型（VLSMs）与低复杂度CNN进行集成的方法，以弥补基于CLIP/BiomedCLIP的VLSMs在图像分割任务中与更先进架构（如CRIS）之间的性能差距。", "motivation": "基于CLIP和BiomedCLIP的视觉语言模型在图像分割任务中的表现落后于CRIS等更复杂的架构，且现有研究多集中于文本提示工程。", "method": "研究人员没有专注于文本提示工程，而是通过将视觉语言分割模型（VLSMs）与一个低复杂度的卷积神经网络（CNN）进行集成。具体使用了集成的BiomedCLIPSeg。", "result": "该方法在BKAI息肉数据集上使集成BiomedCLIPSeg的Dice分数显著提高了6.3%，在其他数据集上也有1%到6%的提升。此外，还在四个额外的放射学和非放射学数据集上提供了初步结果，显示集成方法在不同数据集上的表现（优于或劣于CRIS模型）存在差异。", "conclusion": "集成方法在不同数据集上的效果不一（有时优于CRIS模型，有时劣于），这表明该领域有待社区进一步研究。代码已开源。"}}
{"id": "2509.04853", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04853", "abs": "https://arxiv.org/abs/2509.04853", "authors": ["Chengkai Xu", "Jiaqi Liu", "Yicheng Guo", "Peng Hang", "Jian Sun"], "title": "A Knowledge-Driven Diffusion Policy for End-to-End Autonomous Driving Based on Expert Routing", "comment": "https://perfectxu88.github.io/KDP-AD/", "summary": "End-to-end autonomous driving remains constrained by the need to generate\nmulti-modal actions, maintain temporal stability, and generalize across diverse\nscenarios. Existing methods often collapse multi-modality, struggle with\nlong-horizon consistency, or lack modular adaptability. This paper presents\nKDP, a knowledge-driven diffusion policy that integrates generative diffusion\nmodeling with a sparse mixture-of-experts routing mechanism. The diffusion\ncomponent generates temporally coherent and multi-modal action sequences, while\nthe expert routing mechanism activates specialized and reusable experts\naccording to context, enabling modular knowledge composition. Extensive\nexperiments across representative driving scenarios demonstrate that KDP\nachieves consistently higher success rates, reduced collision risk, and\nsmoother control compared to prevailing paradigms. Ablation studies highlight\nthe effectiveness of sparse expert activation and the Transformer backbone, and\nactivation analyses reveal structured specialization and cross-scenario reuse\nof experts. These results establish diffusion with expert routing as a scalable\nand interpretable paradigm for knowledge-driven end-to-end autonomous driving.", "AI": {"tldr": "本文提出了一种知识驱动的扩散策略KDP，通过结合生成扩散模型和稀疏专家混合路由机制，解决了端到端自动驾驶中多模态动作生成、时间稳定性及泛化能力不足的问题。", "motivation": "现有的端到端自动驾驶方法在生成多模态动作、保持时间稳定性以及在不同场景下泛化方面存在局限，常导致多模态性缺失、长时序一致性差或模块化适应性不足。", "method": "KDP整合了生成扩散模型（用于生成时间连贯和多模态的动作序列）与稀疏专家混合路由机制（根据上下文激活专业且可重用的专家），从而实现模块化的知识组合。该方法还利用了Transformer骨干网络。", "result": "KDP在各种驾驶场景中取得了更高的成功率、更低的碰撞风险和更平滑的控制。消融研究证明了稀疏专家激活和Transformer骨干网络的有效性，激活分析揭示了专家结构化专业化和跨场景重用能力。", "conclusion": "结合专家路由的扩散模型为知识驱动的端到端自动驾驶提供了一个可扩展且可解释的范式。"}}
{"id": "2509.04460", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04460", "abs": "https://arxiv.org/abs/2509.04460", "authors": ["Yihan Chen", "Jiawei Chen", "Guozhao Mo", "Xuanang Chen", "Ben He", "Xianpei Han", "Le Sun"], "title": "CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection", "comment": null, "summary": "The growing integration of large language models (LLMs) into the peer review\nprocess presents potential risks to the fairness and reliability of scholarly\nevaluation. While LLMs offer valuable assistance for reviewers with language\nrefinement, there is growing concern over their use to generate substantive\nreview content. Existing general AI-generated text detectors are vulnerable to\nparaphrasing attacks and struggle to distinguish between surface language\nrefinement and substantial content generation, suggesting that they primarily\nrely on stylistic cues. When applied to peer review, this limitation can result\nin unfairly suspecting reviews with permissible AI-assisted language\nenhancement, while failing to catch deceptively humanized AI-generated reviews.\nTo address this, we propose a paradigm shift from style-based to content-based\ndetection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark\nbuilt upon a fine-grained dataset of AI-generated peer reviews, covering six\ndistinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an\nAI review detector via a multi-task learning framework, designed to achieve\nmore accurate and robust detection of AI involvement in review content. Our\nwork offers a practical foundation for evaluating the use of LLMs in peer\nreview, and contributes to the development of more precise, equitable, and\nreliable detection methods for real-world scholarly applications. Our code and\ndata will be publicly available at https://github.com/Y1hanChen/COCONUTS.", "AI": {"tldr": "针对大型语言模型（LLMs）在同行评审中生成内容的风险，本文提出了一种从基于风格到基于内容的检测范式转变。我们引入了CoCoNUTS基准和CoCoDet多任务学习检测器，以实现更准确、公平的AI参与同行评审内容检测。", "motivation": "LLMs在同行评审中生成实质性内容引发了对公平性和可靠性的担忧。现有通用AI文本检测器容易受到复述攻击，且难以区分语言润色和实质性内容生成，主要依赖风格线索，导致误报或漏报。", "method": "本文提出从基于风格的检测转向基于内容的检测。具体而言，我们构建了CoCoNUTS，一个内容导向的基准，包含一个涵盖六种人机协作模式的细粒度AI生成同行评审数据集。在此基础上，我们开发了CoCoDet，一个通过多任务学习框架实现的AI评审检测器。", "result": "CoCoNUTS和CoCoDet为评估LLMs在同行评审中的使用提供了一个实用的基础，并实现了对评审内容中AI参与的更准确和鲁棒的检测。", "conclusion": "这项工作有助于开发更精确、公平和可靠的检测方法，以应对现实世界学术应用中LLMs在同行评审中的使用问题。"}}
{"id": "2509.04600", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04600", "abs": "https://arxiv.org/abs/2509.04600", "authors": ["Qijun Ying", "Zhongyuan Hu", "Rui Zhang", "Ronghui Li", "Yu Lu", "Zijiao Zeng"], "title": "WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human", "comment": null, "summary": "Global human motion reconstruction from in-the-wild monocular videos is\nincreasingly demanded across VR, graphics, and robotics applications, yet\nrequires accurate mapping of human poses from camera to world coordinates-a\ntask challenged by depth ambiguity, motion ambiguity, and the entanglement\nbetween camera and human movements. While human-motion-centric approaches excel\nin preserving motion details and physical plausibility, they suffer from two\ncritical limitations: insufficient exploitation of camera orientation\ninformation and ineffective integration of camera translation cues. We present\nWATCH (World-aware Allied Trajectory and pose reconstruction for Camera and\nHuman), a unified framework addressing both challenges. Our approach introduces\nan analytical heading angle decomposition technique that offers superior\nefficiency and extensibility compared to existing geometric methods.\nAdditionally, we design a camera trajectory integration mechanism inspired by\nworld models, providing an effective pathway for leveraging camera translation\ninformation beyond naive hard-decoding approaches. Through experiments on\nin-the-wild benchmarks, WATCH achieves state-of-the-art performance in\nend-to-end trajectory reconstruction. Our work demonstrates the effectiveness\nof jointly modeling camera-human motion relationships and offers new insights\nfor addressing the long-standing challenge of camera translation integration in\nglobal human motion reconstruction. The code will be available publicly.", "AI": {"tldr": "WATCH是一个统一框架，通过分析性航向角分解和受世界模型启发的相机轨迹集成，解决了单目视频中全球人体运动重建中的相机方向和位移信息利用不足的问题，实现了最先进的性能。", "motivation": "虚拟现实、图形和机器人应用对野外单目视频中全球人体运动重建的需求日益增长，但现有方法面临深度模糊、运动模糊以及相机与人体运动纠缠的挑战。此外，以人体运动为中心的方法在保留运动细节和物理合理性方面表现出色，但未能充分利用相机方向信息和有效整合相机位移线索。", "method": "本文提出了WATCH（World-aware Allied Trajectory and pose reconstruction for Camera and Human）框架。该方法引入了一种分析性航向角分解技术，相比现有几何方法更高效、更具扩展性。此外，设计了一种受世界模型启发的相机轨迹集成机制，为利用相机位移信息提供了一条有效途径，超越了简单的硬解码方法。", "result": "通过在野外基准测试上的实验，WATCH在端到端轨迹重建方面取得了最先进的性能。", "conclusion": "本工作证明了联合建模相机-人体运动关系的有效性，并为解决全球人体运动重建中长期存在的相机位移集成挑战提供了新见解。"}}
{"id": "2509.04791", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04791", "abs": "https://arxiv.org/abs/2509.04791", "authors": ["Yuan Sui", "Yanming Zhang", "Yi Liao", "Yu Gu", "Guohua Tang", "Zhongqian Sun", "Wei Yang", "Bryan Hooi"], "title": "What-If Analysis of Large Language Models: Explore the Game World Using Proactive Thinking", "comment": "arXiv admin note: text overlap with arXiv:2508.21365", "summary": "Large language models (LLMs) excel at processing information reactively but\nlack the ability to systemically explore hypothetical futures. They cannot ask,\n\"what if we take this action? how will it affect the final outcome\" and\nforecast its potential consequences before acting. This critical gap limits\ntheir utility in dynamic, high-stakes scenarios like strategic planning, risk\nassessment, and real-time decision making. To bridge this gap, we propose\nWiA-LLM, a new paradigm that equips LLMs with proactive thinking capabilities.\nOur approach integrates What-If Analysis (WIA), a systematic approach for\nevaluating hypothetical scenarios by changing input variables. By leveraging\nenvironmental feedback via reinforcement learning, WiA-LLM moves beyond\nreactive thinking. It dynamically simulates the outcomes of each potential\naction, enabling the model to anticipate future states rather than merely react\nto the present conditions. We validate WiA-LLM in Honor of Kings (HoK), a\ncomplex multiplayer game environment characterized by rapid state changes and\nintricate interactions. The game's real-time state changes require precise\nmulti-step consequence prediction, making it an ideal testbed for our approach.\nExperimental results demonstrate WiA-LLM achieves a remarkable 74.2% accuracy\nin forecasting game-state changes (up to two times gain over baselines). The\nmodel shows particularly significant gains in high-difficulty scenarios where\naccurate foresight is critical. To our knowledge, this is the first work to\nformally explore and integrate what-if analysis capabilities within LLMs.\nWiA-LLM represents a fundamental advance toward proactive reasoning in LLMs,\nproviding a scalable framework for robust decision-making in dynamic\nenvironments with broad implications for strategic applications.", "AI": {"tldr": "WiA-LLM是一种新的范式，通过整合情景假设分析（WIA）和强化学习，赋予大型语言模型（LLMs）主动思考能力，使其能够在动态环境中预测行动后果，并在《王者荣耀》中实现显著的未来状态预测准确率提升。", "motivation": "现有大型语言模型擅长被动信息处理，但缺乏系统性探索假设未来的能力，无法在行动前预测潜在后果。这一关键缺陷限制了它们在战略规划、风险评估和实时决策等高风险场景中的应用。", "method": "本文提出了WiA-LLM，通过整合情景假设分析（WIA）和利用强化学习的环境反馈，使LLMs能够主动思考。WiA-LLM动态模拟每个潜在行动的结果，从而预测未来状态，而非仅仅对当前条件作出反应。", "result": "在复杂的多人游戏环境《王者荣耀》中，WiA-LLM在预测游戏状态变化方面达到了74.2%的准确率（比基线模型提高了两倍），尤其在高难度场景中表现出显著优势。这是首次将情景假设分析能力正式整合到LLMs中的工作。", "conclusion": "WiA-LLM代表了LLMs在主动推理方面的一个根本性进展，为动态环境中的稳健决策提供了一个可扩展的框架，对战略应用具有广泛的意义。"}}
{"id": "2509.04619", "categories": ["eess.SY", "cs.SY", "math.DS"], "pdf": "https://arxiv.org/pdf/2509.04619", "abs": "https://arxiv.org/abs/2509.04619", "authors": ["Aditya Gahlawat", "Sambhu H. Karumanchi", "Naira Hovakimyan"], "title": "$\\mathcal{L}_1$-DRAC: Distributionally Robust Adaptive Control", "comment": null, "summary": "Data-driven machine learning methodologies have attracted considerable\nattention for the control and estimation of dynamical systems. However, such\nimplementations suffer from a lack of predictability and robustness. Thus,\nadoption of data-driven tools has been minimal for safety-aware applications\ndespite their impressive empirical results. While classical tools like robust\nadaptive control can ensure predictable performance, their consolidation with\ndata-driven methods remains a challenge and, when attempted, leads to\nconservative results. The difficulty of consolidation stems from the inherently\ndifferent `spaces' that robust control and data-driven methods occupy.\nData-driven methods suffer from the distribution-shift problem, which current\nrobust adaptive controllers can only tackle if using over-simplified learning\nmodels and unverifiable assumptions. In this paper, we present $\\mathcal{L}_1$\ndistributionally robust adaptive control ($\\mathcal{L}_1$-DRAC): a control\nmethodology for uncertain stochastic processes that guarantees robustness\ncertificates in terms of uniform (finite-time) and maximal distributional\ndeviation. We leverage the $\\mathcal{L}_1$ adaptive control methodology to\nensure the existence of Wasserstein ambiguity set around a nominal\ndistribution, which is guaranteed to contain the true distribution. The uniform\nambiguity set produces an ambiguity tube of distributions centered on the\nnominal temporally-varying nominal distribution. The designed controller\ngenerates the ambiguity tube in response to both epistemic (model\nuncertainties) and aleatoric (inherent randomness and disturbances)\nuncertainties.", "AI": {"tldr": "本文提出了一种名为 $\\mathcal{L}_1$ 分布式鲁棒自适应控制（$\\mathcal{L}_1$-DRAC）的新型控制方法，用于不确定随机过程，旨在通过结合数据驱动和鲁棒自适应控制的优势，解决数据驱动方法在安全关键应用中的可预测性和鲁棒性不足问题，并提供严格的分布偏差鲁棒性保证。", "motivation": "数据驱动的机器学习方法在动态系统控制和估计方面取得了显著进展，但其可预测性和鲁棒性不足限制了在安全相关应用中的采用。传统的鲁棒自适应控制虽然能保证可预测性，但与数据驱动方法整合时面临挑战，通常导致保守结果，且难以有效应对数据驱动方法固有的分布偏移问题。", "method": "本文提出了 $\\mathcal{L}_1$ 分布式鲁棒自适应控制（$\\mathcal{L}_1$-DRAC）。该方法利用 $\\mathcal{L}_1$ 自适应控制方法，确保存在一个以名义分布为中心的 Wasserstein 模糊集，该模糊集被保证包含真实分布。该模糊集会生成一个围绕时间变化名义分布的“模糊管”，用于应对认知不确定性（模型不确定性）和偶然不确定性（固有随机性和扰动）。", "result": "所设计的 $\\mathcal{L}_1$-DRAC 控制器能为不确定随机过程提供鲁棒性证书，具体体现在均匀（有限时间）和最大分布偏差方面。该方法确保了包含真实分布的 Wasserstein 模糊集围绕着一个随时间变化的名义分布。", "conclusion": "$\\mathcal{L}_1$-DRAC 提供了一种有效的方法，将数据驱动工具与鲁棒自适应控制相结合，解决了传统方法在处理分布偏移问题时的局限性，为不确定随机过程提供了强大的可预测性和鲁棒性保证，使其适用于安全关键应用。"}}
{"id": "2509.05169", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.05169", "abs": "https://arxiv.org/abs/2509.05169", "authors": ["Huu-Tai Phung", "Yu-Hsiang Lin", "Yen-Kuan Ho", "Wen-Hsiao Peng"], "title": "Exploring Autoregressive Vision Foundation Models for Image Compression", "comment": null, "summary": "This work presents the first attempt to repurpose vision foundation models\n(VFMs) as image codecs, aiming to explore their generation capability for\nlow-rate image compression. VFMs are widely employed in both conditional and\nunconditional generation scenarios across diverse downstream tasks, e.g.,\nphysical AI applications. Many VFMs employ an encoder-decoder architecture\nsimilar to that of end-to-end learned image codecs and learn an autoregressive\n(AR) model to perform next-token prediction. To enable compression, we\nrepurpose the AR model in VFM for entropy coding the next token based on\npreviously coded tokens. This approach deviates from early semantic compression\nefforts that rely solely on conditional generation for reconstructing input\nimages. Extensive experiments and analysis are conducted to compare VFM-based\ncodec to current SOTA codecs optimized for distortion or perceptual quality.\nNotably, certain pre-trained, general-purpose VFMs demonstrate superior\nperceptual quality at extremely low bitrates compared to specialized learned\nimage codecs. This finding paves the way for a promising research direction\nthat leverages VFMs for low-rate, semantically rich image compression.", "AI": {"tldr": "首次尝试将视觉基础模型（VFMs）重新用作图像编解码器，以探索其在低码率图像压缩中的生成能力，并发现它们在极低码率下展现出卓越的感知质量。", "motivation": "视觉基础模型（VFMs）在生成任务中被广泛应用，许多VFMs采用类似于端到端学习图像编解码器的编码器-解码器架构，并学习自回归（AR）模型进行下一token预测。本研究旨在探索这些模型的生成能力，将其应用于低码率图像压缩。", "method": "将VFM中的自回归（AR）模型重新利用，基于先前编码的token对下一个token进行熵编码，从而实现图像压缩。这种方法不同于早期仅依赖条件生成重建输入图像的语义压缩。", "result": "通过广泛的实验和分析，发现某些预训练的通用VFMs在极低码率下，相比于专门优化失真或感知质量的SOTA编解码器，展现出卓越的感知质量。", "conclusion": "这项研究为利用VFMs进行低码率、语义丰富的图像压缩开辟了一个有前景的研究方向。"}}
{"id": "2509.04948", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04948", "abs": "https://arxiv.org/abs/2509.04948", "authors": ["Emanuela Boros"], "title": "Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)", "comment": "Master's thesis", "summary": "Topological localization is a fundamental problem in mobile robotics, since\nrobots must be able to determine their position in order to accomplish tasks.\nVisual localization and place recognition are challenging due to perceptual\nambiguity, sensor noise, and illumination variations. This work addresses\ntopological localization in an office environment using only images acquired\nwith a perspective color camera mounted on a robot platform, without relying on\ntemporal continuity of image sequences. We evaluate state-of-the-art visual\ndescriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and\nBag-of-Visual-Words approaches inspired by text retrieval. Our contributions\ninclude a systematic, quantitative comparison of these features, distance\nmeasures, and classifiers. Performance was analyzed using standard evaluation\nmetrics and visualizations, extending previous experiments. Results demonstrate\nthe advantages of proper configurations of appearance descriptors, similarity\nmeasures, and classifiers. The quality of these configurations was further\nvalidated in the Robot Vision task of the ImageCLEF evaluation campaign, where\nthe system identified the most likely location of novel image sequences. Future\nwork will explore hierarchical models, ranking methods, and feature\ncombinations to build more robust localization systems, reducing training and\nruntime while avoiding the curse of dimensionality. Ultimately, this aims\ntoward integrated, real-time localization across varied illumination and longer\nroutes.", "AI": {"tldr": "本文针对办公环境下的机器人拓扑定位问题，不依赖图像序列的时间连续性，系统性地比较了多种视觉描述符、距离度量和分类器的性能，并验证了其在复杂视觉条件下的有效性。", "motivation": "移动机器人需要确定自身位置以完成任务，而视觉定位和地点识别面临感知模糊、传感器噪声和光照变化等挑战，因此需要研究鲁棒的拓扑定位方法。", "method": "本研究使用安装在机器人平台上的透视彩色相机获取图像，评估了包括颜色直方图、SIFT、ASIFT、RGB-SIFT和受文本检索启发的视觉词袋（Bag-of-Visual-Words）等最先进的视觉描述符。通过标准评估指标和可视化方法，对这些特征、距离度量和分类器进行了系统性的定量比较，且不依赖图像序列的时间连续性。", "result": "结果表明，通过适当配置外观描述符、相似性度量和分类器，可以显著提高拓扑定位性能。这些配置的质量在ImageCLEF评估活动的机器人视觉任务中得到了进一步验证，系统成功识别了新图像序列的最可能位置。", "conclusion": "适当配置视觉描述符、相似性度量和分类器对于在复杂环境下实现鲁棒的拓扑定位至关重要。未来的工作将探索分层模型、排序方法和特征组合，以构建更鲁棒、实时性更强的定位系统，同时减少训练和运行时开销，并避免维度灾难。"}}
{"id": "2509.04461", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2509.04461", "abs": "https://arxiv.org/abs/2509.04461", "authors": ["Tian Ma", "Kaiyu Feng", "Yu Rong", "Kangfei Zhao"], "title": "From Post To Personality: Harnessing LLMs for MBTI Prediction in Social Media", "comment": null, "summary": "Personality prediction from social media posts is a critical task that\nimplies diverse applications in psychology and sociology. The Myers Briggs Type\nIndicator (MBTI), a popular personality inventory, has been traditionally\npredicted by machine learning (ML) and deep learning (DL) techniques. Recently,\nthe success of Large Language Models (LLMs) has revealed their huge potential\nin understanding and inferring personality traits from social media content.\nHowever, directly exploiting LLMs for MBTI prediction faces two key challenges:\nthe hallucination problem inherent in LLMs and the naturally imbalanced\ndistribution of MBTI types in the population. In this paper, we propose\nPostToPersonality (PtoP), a novel LLM based framework for MBTI prediction from\nsocial media posts of individuals. Specifically, PtoP leverages Retrieval\nAugmented Generation with in context learning to mitigate hallucination in\nLLMs. Furthermore, we fine tune a pretrained LLM to improve model specification\nin MBTI understanding with synthetic minority oversampling, which balances the\nclass imbalance by generating synthetic samples. Experiments conducted on a\nreal world social media dataset demonstrate that PtoP achieves state of the art\nperformance compared with 10 ML and DL baselines.", "AI": {"tldr": "本文提出PostToPersonality (PtoP) 框架，通过结合检索增强生成（RAG）和合成少数类过采样技术微调大型语言模型（LLM），解决了LLM在MBTI人格预测中存在的幻觉问题和数据类别不平衡挑战，并实现了最先进的性能。", "motivation": "社交媒体帖子的人格预测（特别是MBTI）具有重要的心理学和社会学应用价值。尽管大型语言模型（LLM）在此领域潜力巨大，但直接应用它们面临两大挑战：LLM固有的幻觉问题以及MBTI类型在人群中自然存在的不平衡分布。", "method": "本文提出了名为PostToPersonality (PtoP) 的新型LLM框架。具体方法包括：1. 利用检索增强生成（RAG）和上下文学习（in-context learning）来减轻LLM的幻觉问题。2. 通过合成少数类过采样（synthetic minority oversampling）技术微调预训练LLM，以生成合成样本来平衡类别不平衡，从而提高模型对MBTI的理解能力。", "result": "在真实社交媒体数据集上进行的实验表明，PtoP框架与10种机器学习（ML）和深度学习（DL）基线方法相比，取得了最先进的性能。", "conclusion": "PtoP框架成功地利用LLM进行MBTI人格预测，有效克服了LLM的幻觉问题和数据类别不平衡挑战，并在真实世界数据上展现出卓越的预测能力。"}}
{"id": "2509.04602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04602", "abs": "https://arxiv.org/abs/2509.04602", "authors": ["MinJu Jeon", "Si-Woo Kim", "Ye-Chan Kim", "HyunGee Kim", "Dong-Jin Kim"], "title": "Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning", "comment": "Accepted in EMNLP 2025", "summary": "Dense video captioning aims to temporally localize events in video and\ngenerate captions for each event. While recent works propose end-to-end models,\nthey suffer from two limitations: (1) applying timestamp supervision only to\ntext while treating all video frames equally, and (2) retrieving captions from\nfixed-size video chunks, overlooking scene transitions. To address these, we\npropose Sali4Vid, a simple yet effective saliency-aware framework. We introduce\nSaliency-aware Video Reweighting, which converts timestamp annotations into\nsigmoid-based frame importance weights, and Semantic-based Adaptive Caption\nRetrieval, which segments videos by frame similarity to capture scene\ntransitions and improve caption retrieval. Sali4Vid achieves state-of-the-art\nresults on YouCook2 and ViTT, demonstrating the benefit of jointly improving\nvideo weighting and retrieval for dense video captioning", "AI": {"tldr": "Sali4Vid 是一种简单有效的密集视频字幕框架，通过引入显著性感知视频重加权和基于语义的自适应字幕检索，解决了现有方法中视频帧处理不均和固定块检索的问题，并在YouCook2和ViTT数据集上取得了最先进的结果。", "motivation": "现有端到端密集视频字幕模型存在两个局限性：1) 仅对文本应用时间戳监督，而对所有视频帧一视同仁；2) 从固定大小的视频块中检索字幕，忽略了场景转换。", "method": "本文提出了Sali4Vid框架。它包含两个主要组件：1) 显著性感知视频重加权（Saliency-aware Video Reweighting），将时间戳注释转换为基于Sigmoid的帧重要性权重；2) 基于语义的自适应字幕检索（Semantic-based Adaptive Caption Retrieval），通过帧相似性分割视频以捕捉场景转换并改进字幕检索。", "result": "Sali4Vid在YouCook2和ViTT数据集上取得了最先进（state-of-the-art）的结果。", "conclusion": "联合改进视频加权和检索对密集视频字幕任务具有显著益处。"}}
{"id": "2509.04809", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04809", "abs": "https://arxiv.org/abs/2509.04809", "authors": ["Haechang Kim", "Hao Chen", "Can Li", "Jong Min Lee"], "title": "TalkToAgent: A Human-centric Explanation of Reinforcement Learning Agents with Large Language Models", "comment": "31 pages total", "summary": "Explainable Reinforcement Learning (XRL) has emerged as a promising approach\nin improving the transparency of Reinforcement Learning (RL) agents. However,\nthere remains a gap between complex RL policies and domain experts, due to the\nlimited comprehensibility of XRL results and isolated coverage of current XRL\napproaches that leave users uncertain about which tools to employ. To address\nthese challenges, we introduce TalkToAgent, a multi-agent Large Language Models\n(LLM) framework that delivers interactive, natural language explanations for RL\npolicies. The architecture with five specialized LLM agents (Coordinator,\nExplainer, Coder, Evaluator, and Debugger) enables TalkToAgent to automatically\nmap user queries to relevant XRL tools and clarify an agent's actions in terms\nof either key state variables, expected outcomes, or counterfactual\nexplanations. Moreover, our approach extends previous counterfactual\nexplanations by deriving alternative scenarios from qualitative behavioral\ndescriptions, or even new rule-based policies. We validated TalkToAgent on\nquadruple-tank process control problem, a well-known nonlinear control\nbenchmark. Results demonstrated that TalkToAgent successfully mapped user\nqueries into XRL tasks with high accuracy, and coder-debugger interactions\nminimized failures in counterfactual generation. Furthermore, qualitative\nevaluation confirmed that TalkToAgent effectively interpreted agent's actions\nand contextualized their meaning within the problem domain.", "AI": {"tldr": "本文提出了TalkToAgent，一个基于多智能体大型语言模型（LLM）的框架，旨在为强化学习（RL）策略提供交互式、自然语言的解释，以弥合复杂RL策略与领域专家之间的理解鸿沟。", "motivation": "现有可解释强化学习（XRL）结果的可理解性有限，且XRL方法覆盖范围孤立，导致用户不确定如何选择工具，从而在复杂RL策略与领域专家之间存在理解障碍。", "method": "引入了TalkToAgent，一个包含五个专门LLM智能体（协调器、解释器、编码器、评估器和调试器）的多智能体LLM框架。该框架能自动将用户查询映射到相关的XRL工具，并以关键状态变量、预期结果或反事实解释的形式阐明智能体的行为。此外，它通过从定性行为描述或新规则策略中推导替代场景，扩展了现有的反事实解释。", "result": "在四罐过程控制问题上进行了验证。结果表明，TalkToAgent能够高精度地将用户查询映射到XRL任务，并且编码器-调试器交互最大限度地减少了反事实生成中的失败。定性评估也证实TalkToAgent能有效解释智能体的行为，并将其含义置于问题域中。", "conclusion": "TalkToAgent通过提供交互式、自然语言的解释，有效提高了RL策略的透明度和可理解性，并解决了现有XRL方法在工具选择和反事实生成方面的局限性，从而有效地弥合了复杂RL与领域专家之间的理解差距。"}}
{"id": "2509.04708", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04708", "abs": "https://arxiv.org/abs/2509.04708", "authors": ["Chun-Wei Kong", "Jay McMahon", "Morteza Lahijanian"], "title": "Bayesian Diagnosability and Active Fault Identification", "comment": null, "summary": "We study fault identification in discrete-time nonlinear systems subject to\nadditive Gaussian white noise. We introduce a Bayesian framework that\nexplicitly accounts for unmodeled faults under reasonable assumptions. Our\napproach hinges on a new quantitative diagnosability definition, revealing when\npassive fault identification (FID) is fundamentally limited by the given\ncontrol sequence. To overcome such limitations, we propose an active FID\nstrategy that designs control inputs for better fault identification. Numerical\nstudies on a two-water tank system and a Mars satellite with complex and\ndiscontinuous dynamics demonstrate that our method significantly reduces\nfailure rates with shorter identification delays compared to purely passive\ntechniques.", "AI": {"tldr": "本文提出了一种针对离散时间非线性系统（含高斯白噪声）的贝叶斯故障识别框架，引入了新的可诊断性定义，并设计了主动故障识别策略以优化控制输入，显著提高了故障识别的效率和准确性。", "motivation": "在离散时间非线性系统（存在附加高斯白噪声）中，被动故障识别（FID）受到给定控制序列的根本限制，尤其是在存在未建模故障的情况下。", "method": "研究引入了一个贝叶斯框架，在合理假设下明确考虑了未建模故障。该方法依赖于一个新的定量可诊断性定义，揭示了被动故障识别的局限性。为克服这些局限，提出了一种主动故障识别策略，通过设计控制输入来改善故障识别。", "result": "在双水箱系统和具有复杂不连续动力学的火星卫星上的数值研究表明，与纯被动技术相比，该方法显著降低了故障率并缩短了识别延迟。", "conclusion": "所提出的贝叶斯框架结合新的可诊断性定义和主动故障识别策略，能够有效克服被动方法在复杂非线性系统故障识别中的局限性，显著提升性能。"}}
{"id": "2509.05261", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.05261", "abs": "https://arxiv.org/abs/2509.05261", "authors": ["Thierry Judge", "Nicolas Duchateau", "Khuram Faraz", "Pierre-Marc Jodoin", "Olivier Bernard"], "title": "Generation of realistic cardiac ultrasound sequences with ground truth motion and speckle decorrelation", "comment": "4 pages. IUS 2025", "summary": "Simulated ultrasound image sequences are key for training and validating\nmachine learning algorithms for left ventricular strain estimation. Several\nsimulation pipelines have been proposed to generate sequences with\ncorresponding ground truth motion, but they suffer from limited realism as they\ndo not consider speckle decorrelation. In this work, we address this limitation\nby proposing an improved simulation framework that explicitly accounts for\nspeckle decorrelation. Our method builds on an existing ultrasound simulation\npipeline by incorporating a dynamic model of speckle variation. Starting from\nreal ultrasound sequences and myocardial segmentations, we generate meshes that\nguide image formation. Instead of applying a fixed ratio of myocardial and\nbackground scatterers, we introduce a coherence map that adapts locally over\ntime. This map is derived from correlation values measured directly from the\nreal ultrasound data, ensuring that simulated sequences capture the\ncharacteristic temporal changes observed in practice. We evaluated the realism\nof our approach using ultrasound data from 98 patients in the CAMUS database.\nPerformance was assessed by comparing correlation curves from real and\nsimulated images. The proposed method achieved lower mean absolute error\ncompared to the baseline pipeline, indicating that it more faithfully\nreproduces the decorrelation behavior seen in clinical data.", "AI": {"tldr": "本研究提出了一种改进的超声图像序列模拟框架，通过引入散斑去相关动态模型，显著提高了模拟序列的真实性，使其能更准确地捕捉实际观察到的时间变化。", "motivation": "用于左心室应变估计的机器学习算法的训练和验证需要真实的模拟超声图像序列。然而，现有的模拟管道因未考虑散斑去相关而导致真实性有限。", "method": "该方法在现有超声模拟管道的基础上，整合了一个动态散斑变异模型。它从真实超声序列和心肌分割数据生成网格，并引入一个随时间局部自适应的相干性图（该图来源于真实超声数据中测量的相关值），以取代固定比例的散射体，从而显式地模拟散斑去相关。", "result": "通过比较真实和模拟图像的相关曲线，该方法在98名患者的超声数据上进行了评估。结果显示，所提出的方法比基线管道的平均绝对误差更低，表明它能更忠实地再现临床数据中的去相关行为。", "conclusion": "该改进的模拟框架通过考虑散斑去相关，显著提高了模拟超声图像序列的真实性，使其能更有效地用于机器学习算法的训练和验证。"}}
{"id": "2509.04950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04950", "abs": "https://arxiv.org/abs/2509.04950", "authors": ["Byeong-Il Ham", "Hyun-Bin Kim", "Kyung-Soo Kim"], "title": "Ground-Aware Octree-A* Hybrid Path Planning for Memory-Efficient 3D Navigation of Ground Vehicles", "comment": "6 pages, 3 figures. Accepted at The 25th International Conference on\n  Control, Automation, and Systems (ICCAS 2025). This is arXiv v1\n  (pre-revision); the camera-ready has been submitted", "summary": "In this paper, we propose a 3D path planning method that integrates the A*\nalgorithm with the octree structure. Unmanned Ground Vehicles (UGVs) and legged\nrobots have been extensively studied, enabling locomotion across a variety of\nterrains. Advances in mobility have enabled obstacles to be regarded not only\nas hindrances to be avoided, but also as navigational aids when beneficial. A\nmodified 3D A* algorithm generates an optimal path by leveraging obstacles\nduring the planning process. By incorporating a height-based penalty into the\ncost function, the algorithm enables the use of traversable obstacles to aid\nlocomotion while avoiding those that are impassable, resulting in more\nefficient and realistic path generation. The octree-based 3D grid map achieves\ncompression by merging high-resolution nodes into larger blocks, especially in\nobstacle-free or sparsely populated areas. This reduces the number of nodes\nexplored by the A* algorithm, thereby improving computational efficiency and\nmemory usage, and supporting real-time path planning in practical environments.\nBenchmark results demonstrate that the use of octree structure ensures an\noptimal path while significantly reducing memory usage and computation time.", "AI": {"tldr": "本文提出了一种结合A*算法和八叉树结构的3D路径规划方法，用于无人地面车辆和仿生机器人，通过利用可穿越障碍物并优化地图表示来提高效率和实时性。", "motivation": "随着无人地面车辆和仿生机器人移动能力的进步，障碍物不仅是需要避开的阻碍，也可以作为导航辅助。传统路径规划可能无法有效利用这些障碍物，且计算效率和内存使用是实时规划的挑战。", "method": "该方法将3D A*算法与八叉树结构相结合。修改后的3D A*算法在成本函数中引入基于高度的惩罚，以利用可穿越障碍物并避开不可穿越障碍物。八叉树3D网格地图通过合并高分辨率节点（尤其是在无障碍或稀疏区域）实现压缩，从而减少A*算法探索的节点数量。", "result": "该算法能够生成更高效、更真实的路径，通过利用可穿越障碍物辅助运动。八叉树结构显著减少了内存使用和计算时间，提高了计算效率，并支持在实际环境中的实时路径规划。基准测试结果证明了在确保最优路径的同时，内存使用和计算时间得到了显著降低。", "conclusion": "所提出的结合A*算法和八叉树结构的3D路径规划方法，通过智能地利用障碍物和高效的地图表示，实现了最优路径规划，同时显著降低了内存消耗和计算时间，为无人地面车辆和仿生机器人的实时导航提供了有效解决方案。"}}
{"id": "2509.04462", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04462", "abs": "https://arxiv.org/abs/2509.04462", "authors": ["Yu Hou", "Zaifu Zhan", "Rui Zhang"], "title": "Benchmarking GPT-5 for biomedical natural language processing", "comment": null, "summary": "The rapid expansion of biomedical literature has heightened the need for\nscalable natural language processing (NLP) solutions. While GPT-4 substantially\nnarrowed the gap with task-specific systems, especially in question answering,\nits performance across other domains remained uneven. We updated a standardized\nBioNLP benchmark to evaluate GPT-5 and GPT-4o under zero-, one-, and five-shot\nprompting across 12 datasets spanning six task families: named entity\nrecognition, relation extraction, multi-label document classification, question\nanswering, text summarization, and text simplification. Using fixed prompt\ntemplates, identical decoding parameters, and batch inference, we report\nprimary metrics per dataset and include prior results for GPT-4, GPT-3.5, and\nLLaMA-2-13B for comparison. GPT-5 achieved the strongest overall benchmark\nperformance, with macro-average scores rising to 0.557 under five-shot\nprompting versus 0.506 for GPT-4 and 0.508 for GPT-4o. On MedQA, GPT-5 reached\n94.1% accuracy, exceeding the previous supervised state of the art by over\nfifty points, and attained parity with supervised systems on PubMedQA (0.734).\nIn extraction tasks, GPT-5 delivered major gains in chemical NER (0.886 F1) and\nChemProt relation extraction (0.616 F1), outperforming GPT-4 and GPT-4o, though\nsummarization and disease NER still lagged behind domain-specific baselines.\nThese results establish GPT-5 as a general-purpose model now offering\ndeployment-ready performance for reasoning-oriented biomedical QA, while\nprecision-critical extraction and evidence-dense summarization continue to\nfavor fine-tuned or hybrid approaches. The benchmark delineates where simple\nprompting suffices and where retrieval-augmented or planning-based scaffolds\nare likely required, providing actionable guidance for BioNLP system design as\nfrontier models advance.", "AI": {"tldr": "本研究评估了GPT-5和GPT-4o在生物医学自然语言处理（BioNLP）基准上的表现，发现GPT-5在多项任务中取得了最强的整体性能，尤其是在问答方面，但在某些提取和摘要任务上仍落后于特定领域模型。", "motivation": "生物医学文献的快速增长，以及对可扩展自然语言处理解决方案的需求日益增加。尽管GPT-4在特定任务（如问答）上表现出色，但在其他领域性能不均衡，促使研究人员评估更先进的模型。", "method": "研究人员更新了一个标准化的BioNLP基准，使用零次、一次和五次少样本提示，评估了GPT-5和GPT-4o在涵盖命名实体识别、关系抽取、多标签文档分类、问答、文本摘要和文本简化六个任务家族的12个数据集上的性能。评估使用了固定的提示模板、相同的解码参数和批量推理，并与GPT-4、GPT-3.5和LLaMA-2-13B的先前结果进行了比较。", "result": "GPT-5取得了最强的整体基准性能，在五次少样本提示下，宏平均分数上升到0.557，超过GPT-4（0.506）和GPT-4o（0.508）。在MedQA上，GPT-5达到94.1%的准确率，超过了之前监督式SOTA五十多个百分点；在PubMedQA上与监督式系统持平（0.734）。在抽取任务中，GPT-5在化学NER（0.886 F1）和ChemProt关系抽取（0.616 F1）上取得了显著进步。然而，摘要和疾病NER任务仍落后于特定领域基线。", "conclusion": "研究结果表明，GPT-5作为一种通用模型，在推理导向的生物医学问答方面已具备部署就绪的性能。然而，对于精度关键的抽取和证据密集型摘要任务，精调或混合方法仍然更具优势。该基准为BioNLP系统设计提供了可操作的指导，指明了简单提示足以应对的场景，以及可能需要检索增强或基于规划的支架的场景。"}}
{"id": "2509.04624", "categories": ["cs.CV", "cs.ET", "cs.RO", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.04624", "abs": "https://arxiv.org/abs/2509.04624", "authors": ["Ali Khanpour", "Tianyi Wang", "Afra Vahidi-Shams", "Wim Ectors", "Farzam Nakhaie", "Amirhossein Taheri", "Christian Claudel"], "title": "UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis", "comment": "15 pages, 8 figures, 2 tables", "summary": "Traffic congestion and violations pose significant challenges for urban\nmobility and road safety. Traditional traffic monitoring systems, such as fixed\ncameras and sensor-based methods, are often constrained by limited coverage,\nlow adaptability, and poor scalability. To address these challenges, this paper\nintroduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance\nsystem capable of accurate vehicle detection, classification, tracking, and\nbehavioral analysis in real-world, unconstrained urban environments. The system\nleverages multi-scale and multi-angle template matching, Kalman filtering, and\nhomography-based calibration to process aerial video data collected from\naltitudes of approximately 200 meters. A case study in urban area demonstrates\nrobust performance, achieving a detection precision of 91.8%, an F1-score of\n90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.\nBeyond precise detection, the system classifies five vehicle types and\nautomatically detects critical traffic violations, including unsafe lane\nchanges, illegal double parking, and crosswalk obstructions, through the fusion\nof geofencing, motion filtering, and trajectory deviation analysis. The\nintegrated analytics module supports origin-destination tracking, vehicle count\nvisualization, inter-class correlation analysis, and heatmap-based congestion\nmodeling. Additionally, the system enables entry-exit trajectory profiling,\nvehicle density estimation across road segments, and movement direction\nlogging, supporting comprehensive multi-scale urban mobility analytics.\nExperimental results confirms the system's scalability, accuracy, and practical\nrelevance, highlighting its potential as an enforcement-aware,\ninfrastructure-independent traffic monitoring solution for next-generation\nsmart cities.", "AI": {"tldr": "本文提出了一种基于无人机（UAV）的先进交通监控系统，能够对城市交通进行车辆检测、分类、跟踪和行为分析，并自动识别交通违规行为，具有高精度、可扩展性和实用性。", "motivation": "传统的交通监控系统（如固定摄像头和传感器）存在覆盖范围有限、适应性差和可扩展性低的问题，无法有效应对城市交通拥堵和违规挑战。", "method": "该系统利用无人机在约200米高空采集视频数据，通过多尺度和多角度模板匹配、卡尔曼滤波和基于单应性的校准进行处理。车辆行为分析和违规检测则融合了地理围栏、运动过滤和轨迹偏差分析。此外，系统还集成了分析模块，支持起讫点跟踪、车辆计数、类间关联分析、热力图拥堵建模、出入口轨迹分析、路段车辆密度估算和运动方向记录。", "result": "在城市区域案例研究中，系统实现了91.8%的检测精度、90.5%的F1分数，以及92.1%的MOTA和93.7%的MOTP跟踪指标。系统能分类五种车辆类型，并自动检测不安全变道、非法双重停车和人行横道阻塞等关键交通违规行为。实验结果证实了系统的可扩展性、准确性和实用性。", "conclusion": "该系统是一个可扩展、准确且具有实际意义的交通监控解决方案，有望成为下一代智慧城市中独立于基础设施、支持执法的新型交通监控工具。"}}
{"id": "2509.04847", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04847", "abs": "https://arxiv.org/abs/2509.04847", "authors": ["Mukul Singh", "Arjun Radhakrishna", "Sumit Gulwani"], "title": "Collaboration and Conflict between Humans and Language Models through the Lens of Game Theory", "comment": "9 pages", "summary": "Language models are increasingly deployed in interactive online environments,\nfrom personal chat assistants to domain-specific agents, raising questions\nabout their cooperative and competitive behavior in multi-party settings. While\nprior work has examined language model decision-making in isolated or\nshort-term game-theoretic contexts, these studies often neglect long-horizon\ninteractions, human-model collaboration, and the evolution of behavioral\npatterns over time. In this paper, we investigate the dynamics of language\nmodel behavior in the iterated prisoner's dilemma (IPD), a classical framework\nfor studying cooperation and conflict. We pit model-based agents against a\nsuite of 240 well-established classical strategies in an Axelrod-style\ntournament and find that language models achieve performance on par with, and\nin some cases exceeding, the best-known classical strategies. Behavioral\nanalysis reveals that language models exhibit key properties associated with\nstrong cooperative strategies - niceness, provocability, and generosity while\nalso demonstrating rapid adaptability to changes in opponent strategy mid-game.\nIn controlled \"strategy switch\" experiments, language models detect and respond\nto shifts within only a few rounds, rivaling or surpassing human adaptability.\nThese results provide the first systematic characterization of long-term\ncooperative behaviors in language model agents, offering a foundation for\nfuture research into their role in more complex, mixed human-AI social\nenvironments.", "AI": {"tldr": "本研究在迭代囚徒困境中系统性地评估了语言模型的长期合作行为，发现它们表现出高水平的合作性、适应性，并能与经典策略相媲美甚至超越。", "motivation": "语言模型在交互式在线环境中日益普及，但其在多方设置中的合作与竞争行为，尤其是在长期互动、人机协作以及行为模式随时间演变方面的研究不足。", "method": "研究采用迭代囚徒困境（IPD）框架，让语言模型代理与240种经典策略进行Axelrod式锦标赛。同时，通过受控的“策略切换”实验，评估语言模型对对手策略变化的适应性。", "result": "语言模型表现出与最佳经典策略相当甚至超越的性能。行为分析表明，它们具备强合作策略的关键特性：友善、可激怒性、慷慨，并能快速适应对手策略的变化，在几轮内检测并响应策略转变，媲美甚至超越人类的适应性。", "conclusion": "本研究首次系统性地刻画了语言模型代理的长期合作行为，为未来研究其在更复杂、人机混合的社会环境中的作用奠定了基础。"}}
{"id": "2509.04885", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.04885", "abs": "https://arxiv.org/abs/2509.04885", "authors": ["Han Zhang", "Bingxin Zhang", "Yizhe Zhao", "Kun Yang", "Guopeng Zhang"], "title": "Performance Analysis of Pinching-Antenna-Enabled Internet of Things Systems", "comment": null, "summary": "The pinching-antenna systems (PASS), which activate small dielectric\nparticles along a dielectric waveguide, has recently emerged as a promising\nparadigm for flexible antenna deployment in next-generation wireless\ncommunication networks. While most existing studies assume rectangular indoor\nlayouts with full coverage waveguide, practical deployments may involve\ngeometric constraints, partial coverage, and non-negligible waveguide\nattenuation. This paper presents the first analytical investigation of PASS in\na circular indoor environment, encompassing both full coverage and partial\ncoverage waveguide configurations with/without propagation loss. A unified\ngeometric-propagation framework is developed that jointly captures\npinching-antenna placement, Internet of Things (IoT) device location\ndistribution, and waveguide attenuation. Closed-form expressions for the outage\nprobability and average achievable rate are derived for four scenarios, with\naccuracy validated via extensive Monte-Carlo simulations. The analysis reveals\nthat, under the partial coverage waveguide scenario with propagation loss, the\nsystem performance demonstrates a non-monotonic trend with respect to the\nwaveguide length, and the optimal length decreases as the attenuation\ncoefficient increases. Numerical results further quantify the interplay between\ndeployment strategy, waveguide propagation loss, and coverage geometry,\noffering practical guidelines for performance-oriented PASS design.", "AI": {"tldr": "本文首次分析了在圆形室内环境中，考虑全覆盖/部分覆盖波导以及传播损耗的夹持天线系统（PASS）性能，并提供了实用的设计指导。", "motivation": "现有研究大多假设矩形室内布局、全覆盖波导且忽略传播损耗，这与实际部署中存在的几何约束、部分覆盖和不可忽略的波导衰减不符。", "method": "开发了一个统一的几何-传播框架，综合考虑夹持天线放置、物联网设备位置分布和波导衰减。推导了四种场景下的中断概率和平均可达速率的闭式表达式，并通过蒙特卡洛仿真验证了准确性。", "result": "推导了四种场景（包括有/无传播损耗的全覆盖和部分覆盖波导）的中断概率和平均可达速率的闭式表达式。分析表明，在有传播损耗的部分覆盖波导场景下，系统性能随波导长度呈现非单调趋势，且最佳长度随衰减系数的增加而减小。数值结果量化了部署策略、波导传播损耗和覆盖几何之间的相互作用。", "conclusion": "该研究揭示了部署策略、波导传播损耗和覆盖几何之间的复杂关系，为面向性能的夹持天线系统设计提供了实用的指导方针。"}}
{"id": "2509.04970", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04970", "abs": "https://arxiv.org/abs/2509.04970", "authors": ["Tien Pham", "Xinyun Chi", "Khang Nguyen", "Manfred Huber", "Angelo Cangelosi"], "title": "DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation", "comment": null, "summary": "Reinforcement learning (RL) agents can learn to solve complex tasks from\nvisual inputs, but generalizing these learned skills to new environments\nremains a major challenge in RL application, especially robotics. While data\naugmentation can improve generalization, it often compromises sample efficiency\nand training stability. This paper introduces DeGuV, an RL framework that\nenhances both generalization and sample efficiency. In specific, we leverage a\nlearnable masker network that produces a mask from the depth input, preserving\nonly critical visual information while discarding irrelevant pixels. Through\nthis, we ensure that our RL agents focus on essential features, improving\nrobustness under data augmentation. In addition, we incorporate contrastive\nlearning and stabilize Q-value estimation under augmentation to further enhance\nsample efficiency and training stability. We evaluate our proposed method on\nthe RL-ViGen benchmark using the Franka Emika robot and demonstrate its\neffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuV\noutperforms state-of-the-art methods in both generalization and sample\nefficiency while also improving interpretability by highlighting the most\nrelevant regions in the visual input", "AI": {"tldr": "DeGuV是一种强化学习框架，通过可学习的深度掩码网络、对比学习和稳定的Q值估计，显著提升了视觉输入RL智能体的泛化能力和样本效率，特别适用于零样本模拟到真实世界的迁移。", "motivation": "强化学习智能体在从视觉输入中学习复杂任务时，将所学技能泛化到新环境（尤其在机器人领域）是一个重大挑战。数据增强虽能提高泛化能力，但常以牺牲样本效率和训练稳定性为代价。", "method": "本文提出了DeGuV框架。它利用一个可学习的掩码网络，从深度输入生成掩码，只保留关键视觉信息并丢弃无关像素，使RL智能体专注于必要特征，从而增强数据增强下的鲁棒性。此外，DeGuV还结合了对比学习，并稳定了增强条件下的Q值估计，以进一步提高样本效率和训练稳定性。", "result": "DeGuV在RL-ViGen基准测试（使用Franka Emika机器人）中进行了评估，并在零样本模拟到真实世界的迁移中展示了其有效性。结果表明，DeGuV在泛化能力和样本效率方面均优于现有最先进的方法，同时通过突出视觉输入中最相关的区域，提高了可解释性。", "conclusion": "DeGuV是一个有效的RL框架，能够显著提高视觉输入RL智能体的泛化能力和样本效率，并在数据增强下保持鲁棒性，同时提供更好的可解释性，为机器人等实际应用提供了更强大的解决方案。"}}
{"id": "2509.04464", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04464", "abs": "https://arxiv.org/abs/2509.04464", "authors": ["Yang Nan", "Pengfei He", "Ravi Tandon", "Han Xu"], "title": "Can Multiple Responses from an LLM Reveal the Sources of Its Uncertainty?", "comment": "Proceedings of The 2025 Conference on Empirical Methods in Natural\n  Language Processing (Findings)", "summary": "Large language models (LLMs) have delivered significant breakthroughs across\ndiverse domains but can still produce unreliable or misleading outputs, posing\ncritical challenges for real-world applications. While many recent studies\nfocus on quantifying model uncertainty, relatively little work has been devoted\nto \\textit{diagnosing the source of uncertainty}. In this study, we show that,\nwhen an LLM is uncertain, the patterns of disagreement among its multiple\ngenerated responses contain rich clues about the underlying cause of\nuncertainty. To illustrate this point, we collect multiple responses from a\ntarget LLM and employ an auxiliary LLM to analyze their patterns of\ndisagreement. The auxiliary model is tasked to reason about the likely source\nof uncertainty, such as whether it stems from ambiguity in the input question,\na lack of relevant knowledge, or both. In cases involving knowledge gaps, the\nauxiliary model also identifies the specific missing facts or concepts\ncontributing to the uncertainty. In our experiment, we validate our framework\non AmbigQA, OpenBookQA, and MMLU-Pro, confirming its generality in diagnosing\ndistinct uncertainty sources. Such diagnosis shows the potential for relevant\nmanual interventions that improve LLM performance and reliability.", "AI": {"tldr": "本研究提出了一种诊断大型语言模型（LLMs）不确定性来源的方法，通过分析其多个生成响应之间的分歧模式，并利用辅助LLM来推断不确定性的具体原因（如输入歧义、知识缺乏）。", "motivation": "尽管LLMs取得了显著进展，但它们仍可能产生不可靠或误导性输出，对实际应用构成挑战。现有研究多关注量化模型不确定性，但很少探究不确定性的根本来源。", "method": "从目标LLM收集多个响应，然后使用一个辅助LLM来分析这些响应之间的分歧模式。辅助模型负责推断不确定性的可能来源，例如输入问题的歧义性、相关知识的缺乏，或两者兼有。在涉及知识空白的情况下，辅助模型还会识别导致不确定性的具体缺失事实或概念。", "result": "该框架在AmbigQA、OpenBookQA和MMLU-Pro数据集上进行了验证，证实了其在诊断不同不确定性来源方面的通用性。", "conclusion": "诊断不确定性来源有助于进行相关的人工干预，从而提高LLM的性能和可靠性。"}}
{"id": "2509.04669", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04669", "abs": "https://arxiv.org/abs/2509.04669", "authors": ["Mustafa Munir", "Alex Zhang", "Radu Marculescu"], "title": "VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation", "comment": "Proceedings of the 2025 IEEE/CVF International Conference on Computer\n  Vision (ICCV) Workshops", "summary": "Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)\nhave challenged the dominance of Convolutional Neural Networks (CNNs) in\ncomputer vision. ViTs excel at capturing global context, and SSMs like Mamba\noffer linear complexity for long sequences, yet they do not capture\nfine-grained local features as effectively as CNNs. Conversely, CNNs possess\nstrong inductive biases for local features but lack the global reasoning\ncapabilities of transformers and Mamba. To bridge this gap, we introduce\n\\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs\nand multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a\nhierarchical structure with convolutional blocks in its early stages to extract\nrich local features. These convolutional blocks are then processed by later\nstages incorporating multi-directional Mamba blocks designed to efficiently\nmodel long-range dependencies and global context. This hybrid design allows for\nsuperior feature representation while maintaining linear complexity with\nrespect to image resolution. We demonstrate VCMamba's effectiveness through\nextensive experiments on ImageNet-1K classification and ADE20K semantic\nsegmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,\nsurpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming\nVision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains\n47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing\n62% fewer parameters. Code is available at\nhttps://github.com/Wertyuui345/VCMamba.", "AI": {"tldr": "VCMamba是一种结合了卷积神经网络（CNN）和多向Mamba状态空间模型（SSM）的新型视觉骨干网络，旨在高效捕捉局部和全局特征，并在图像分类和语义分割任务上表现出色。", "motivation": "Vision Transformers (ViTs)和Mamba等SSM在全局上下文和长序列建模方面表现优异，但在细粒度局部特征捕捉上不如CNN。而CNN擅长局部特征，但缺乏全局推理能力。研究旨在弥合这一差距，结合两者的优势。", "method": "VCMamba采用卷积主干和早期阶段的卷积块来提取丰富的局部特征，随后通过多向Mamba块处理这些特征，以有效地建模长距离依赖和全局上下文。这种混合设计保持了线性复杂性，并具有分层结构。", "result": "VCMamba-B在ImageNet-1K分类上达到82.6%的top-1准确率，比PlainMamba-L3高0.3%且参数减少37%，比Vision GNN-B高0.3%且参数减少64%。在ADE20K语义分割上，VCMamba-B获得47.1 mIoU，超越EfficientFormer-L7 2.0 mIoU且参数减少62%。", "conclusion": "VCMamba成功地整合了CNN的局部特征提取能力和多向Mamba SSM的全局上下文建模能力，实现了卓越的特征表示，同时保持了图像分辨率的线性复杂度，并在多项视觉任务上展现出优越的性能和效率。"}}
{"id": "2509.04871", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04871", "abs": "https://arxiv.org/abs/2509.04871", "authors": ["Krittanon Kaewtawee", "Wachiravit Modecrua", "Krittin Pachtrachai", "Touchapon Kraisingkorn"], "title": "Cloning a Conversational Voice AI Agent from Call\\,Recording Datasets for Telesales", "comment": "10 pages, 4 figures", "summary": "Recent advances in language and speech modelling have made it possible to\nbuild autonomous voice assistants that understand and generate human dialogue\nin real time. These systems are increasingly being deployed in domains such as\ncustomer service and healthcare care, where they can automate repetitive tasks,\nreduce operational costs, and provide constant support around the clock. In\nthis paper, we present a general methodology for cloning a conversational voice\nAI agent from a corpus of call recordings. Although the case study described in\nthis paper uses telesales data to illustrate the approach, the underlying\nprocess generalizes to any domain where call transcripts are available. Our\nsystem listens to customers over the telephone, responds with a synthetic\nvoice, and follows a structured playbook learned from top performing human\nagents. We describe the domain selection, knowledge extraction, and prompt\nengineering used to construct the agent, integrating automatic speech\nrecognition, a large language model based dialogue manager, and text to speech\nsynthesis into a streaming inference pipeline. The cloned agent is evaluated\nagainst human agents on a rubric of 22 criteria covering introduction, product\ncommunication, sales drive, objection handling, and closing. Blind tests show\nthat the AI agent approaches human performance in routine aspects of the call\nwhile underperforming in persuasion and objection handling. We analyze these\nshortcomings and refine the prompt accordingly. The paper concludes with design\nlessons and avenues for future research, including large scale simulation and\nautomated evaluation.", "AI": {"tldr": "本文提出了一种从通话录音语料库中克隆对话式语音AI代理的通用方法，该代理能够听取客户、用合成语音回应并遵循人类优秀代理的学习剧本，并在常规任务中接近人类表现，但在说服和异议处理方面仍有不足。", "motivation": "语言和语音建模的最新进展使得构建自主语音助手成为可能，这些系统能够实时理解和生成人类对话，并被部署在客户服务和医疗保健等领域，以自动化重复任务、降低运营成本并提供全天候支持。", "method": "研究人员开发了一种通用方法，通过整合自动语音识别（ASR）、基于大型语言模型（LLM）的对话管理器和文本到语音（TTS）合成，构建了一个流式推理管道。该系统从通话录音中学习结构化剧本，并使用领域选择、知识提取和提示工程来构建代理。通过22项标准（包括介绍、产品沟通、销售推动、异议处理和结束语）对克隆代理进行盲测评估。", "result": "盲测结果显示，AI代理在通话的常规方面接近人类表现，但在说服和异议处理方面表现不佳。研究人员对这些不足进行了分析并相应地优化了提示。", "conclusion": "论文总结了设计经验和未来研究方向，包括大规模模拟和自动化评估。尽管AI代理在说服和异议处理方面仍有提升空间，但在常规任务中已能接近人类表现。"}}
{"id": "2509.05003", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05003", "abs": "https://arxiv.org/abs/2509.05003", "authors": ["Saeideh Mansouri", "Mohamed Shamekh", "Simon Indola", "Petri Mahonen"], "title": "Estimating Cellular Network Delays in Finnish Railways: A Machine Learning Enhanced Approach", "comment": "Accepted for presentation at IEEE PIMRC 2025. 6 pages, 7 figures", "summary": "There is growing interest in using public cellular networks for specialized\ncommunication applications, replacing standalone sector-specific networks. One\nsuch application is transitioning from the aging GSM-R railway network to\npublic 4G and 5G networks. Finland is modernizing its railway communication\nsystem through the Digirail project, leveraging public cellular networks. To\nevaluate network performance, a nationwide measurement campaign was conducted\nin two modes: Best Quality and Packet Replication. However, Best Quality mode\nintroduces artificial delays, making it unsuitable for real-world assessments.\nIn this paper, railway network delays are modeled using machine learning based\non measurements from the Packet Replication mode. The best-performing model is\nthen employed to generate a dataset estimating network delays across Finland's\nrailway network. This dataset provides a more accurate representation of\nnetwork performance. Machine learning based network performance prediction is\nshown to be feasible, and the results indicate that Finland's public cellular\nnetwork can meet the stringent performance requirements of railway network\ncontrol.", "AI": {"tldr": "该研究基于芬兰Digirail项目，利用机器学习对公共蜂窝网络在铁路通信中的延迟进行建模和预测，以评估其替代老旧GSM-R网络的可行性。", "motivation": "随着公共蜂窝网络技术发展，人们日益关注其替代专用行业网络（如老旧的GSM-R铁路网络）的可能性。芬兰的Digirail项目旨在将铁路通信系统现代化，转向公共4G和5G网络，因此需要评估这些公共网络的性能是否能满足铁路通信的严格要求。", "method": "研究在全国范围内进行了两种模式的测量活动：最佳质量模式（Best Quality）和数据包复制模式（Packet Replication）。由于最佳质量模式引入了人为延迟，不适用于真实评估，因此研究基于数据包复制模式的测量数据，利用机器学习对铁路网络延迟进行建模。最终，使用表现最佳的模型生成了一个估算芬兰铁路网络延迟的全国性数据集。", "result": "研究表明基于机器学习的网络性能预测是可行的。生成的全国性延迟数据集提供了更准确的网络性能表示。结果表明，芬兰的公共蜂窝网络能够满足铁路网络控制的严格性能要求。", "conclusion": "芬兰的公共蜂窝网络有能力满足铁路网络控制的严苛性能需求，为Digirail项目向公共网络过渡提供了可行性支持。"}}
{"id": "2509.04984", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04984", "abs": "https://arxiv.org/abs/2509.04984", "authors": ["Koji Matsuno", "Chien Chern Cheah"], "title": "Lyapunov-Based Deep Learning Control for Robots with Unknown Jacobian", "comment": null, "summary": "Deep learning, with its exceptional learning capabilities and flexibility,\nhas been widely applied in various applications. However, its black-box nature\nposes a significant challenge in real-time robotic applications, particularly\nin robot control, where trustworthiness and robustness are critical in ensuring\nsafety. In robot motion control, it is essential to analyze and ensure system\nstability, necessitating the establishment of methodologies that address this\nneed. This paper aims to develop a theoretical framework for end-to-end deep\nlearning control that can be integrated into existing robot control theories.\nThe proposed control algorithm leverages a modular learning approach to update\nthe weights of all layers in real time, ensuring system stability based on\nLyapunov-like analysis. Experimental results on industrial robots are presented\nto illustrate the performance of the proposed deep learning controller. The\nproposed method offers an effective solution to the black-box problem in deep\nlearning, demonstrating the possibility of deploying real-time deep learning\nstrategies for robot kinematic control in a stable manner. This achievement\nprovides a critical foundation for future advancements in deep learning based\nreal-time robotic applications.", "AI": {"tldr": "本文提出了一种用于机器人运动控制的端到端深度学习控制理论框架，通过模块化学习和类李雅普诺夫分析确保系统稳定性，有效解决了深度学习的“黑箱”问题，并实现了稳定、实时的机器人运动学控制。", "motivation": "深度学习在机器人应用中面临“黑箱”问题，特别是在机器人控制中，缺乏对系统稳定性的信任度和鲁棒性保障，这对于确保安全至关重要。研究旨在建立一种能与现有机器人控制理论结合的方法，以分析和确保系统稳定性。", "method": "开发了一个端到端深度学习控制的理论框架，该框架采用模块化学习方法实时更新所有层的权重，并基于类李雅普诺夫分析来确保系统稳定性。", "result": "在工业机器人上的实验结果表明，所提出的深度学习控制器性能良好，有效解决了深度学习的“黑箱”问题，并展示了以稳定方式部署实时深度学习策略进行机器人运动学控制的可能性。", "conclusion": "该研究为深度学习在实时机器人应用中提供了一个关键基础，证明了实现稳定、实时深度学习机器人运动学控制的可行性。"}}
{"id": "2509.04465", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04465", "abs": "https://arxiv.org/abs/2509.04465", "authors": ["Sushrita Rakshit", "James Hale", "Kushal Chawla", "Jeanne M. Brett", "Jonathan Gratch"], "title": "Emotionally-Aware Agents for Dispute Resolution", "comment": null, "summary": "In conflict, people use emotional expressions to shape their counterparts'\nthoughts, feelings, and actions. This paper explores whether automatic text\nemotion recognition offers insight into this influence in the context of\ndispute resolution. Prior work has shown the promise of such methods in\nnegotiations; however, disputes evoke stronger emotions and different social\nprocesses. We use a large corpus of buyer-seller dispute dialogues to\ninvestigate how emotional expressions shape subjective and objective outcomes.\nWe further demonstrate that large-language models yield considerably greater\nexplanatory power than previous methods for emotion intensity annotation and\nbetter match the decisions of human annotators. Findings support existing\ntheoretical models for how emotional expressions contribute to conflict\nescalation and resolution and suggest that agent-based systems could be useful\nin managing disputes by recognizing and potentially mitigating emotional\nescalation.", "AI": {"tldr": "本研究利用大型语言模型（LLMs）对买卖纠纷对话中的情感表达进行识别和分析，发现LLMs在情感强度标注上优于传统方法，并能有效揭示情感表达如何影响冲突升级与解决，为基于智能体的纠纷管理系统提供潜力。", "motivation": "以往研究已显示文本情感识别在谈判中的潜力，但纠纷情境会引发更强烈的情绪和不同的社会过程。因此，本研究旨在探索自动文本情感识别，特别是利用大型语言模型，能否为理解和干预纠纷解决中的情感影响提供新的见解。", "method": "研究使用了大量的买卖纠纷对话语料库。通过大型语言模型（LLMs）进行情感强度标注，并将其解释力与之前的方法以及人类标注者的决策进行比较。", "result": "研究发现，大型语言模型在情感强度标注方面比以往方法具有显著更高的解释力，并且与人类标注者的判断更吻合。研究结果支持了现有关于情感表达如何导致冲突升级和解决的理论模型。", "conclusion": "情感识别技术，尤其是基于大型语言模型的，可以为理解和管理纠纷中的情感动态提供深入洞察。基于智能体的系统有望通过识别和潜在地缓解情感升级来有效地管理纠纷。"}}
{"id": "2509.04687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04687", "abs": "https://arxiv.org/abs/2509.04687", "authors": ["Vanshika Vats", "Ashwani Rathee", "James Davis"], "title": "Guideline-Consistent Segmentation via Multi-Agent Refinement", "comment": null, "summary": "Semantic segmentation in real-world applications often requires not only\naccurate masks but also strict adherence to textual labeling guidelines. These\nguidelines are typically complex and long, and both human and automated\nlabeling often fail to follow them faithfully. Traditional approaches depend on\nexpensive task-specific retraining that must be repeated as the guidelines\nevolve. Although recent open-vocabulary segmentation methods excel with simple\nprompts, they often fail when confronted with sets of paragraph-length\nguidelines that specify intricate segmentation rules. To address this, we\nintroduce a multi-agent, training-free framework that coordinates\ngeneral-purpose vision-language models within an iterative Worker-Supervisor\nrefinement architecture. The Worker performs the segmentation, the Supervisor\ncritiques it against the retrieved guidelines, and a lightweight reinforcement\nlearning stop policy decides when to terminate the loop, ensuring\nguideline-consistent masks while balancing resource use. Evaluated on the Waymo\nand ReasonSeg datasets, our method notably outperforms state-of-the-art\nbaselines, demonstrating strong generalization and instruction adherence.", "AI": {"tldr": "本文提出一个多智能体、免训练框架，通过迭代的“工作者-监督者”细化架构协调通用视觉语言模型，以解决语义分割中复杂文本指南依从性差的问题，并在基准测试中表现优异。", "motivation": "在实际应用中，语义分割不仅需要准确的掩码，还需要严格遵守复杂的文本标注指南。传统方法需要昂贵的任务特定再训练，且难以适应指南变化；现有开放词汇分割方法在处理简单提示时表现良好，但面对段落长度的复杂规则时则效果不佳，导致自动化和人工标注都难以忠实遵循指南。", "method": "本文引入了一个多智能体、免训练框架。该框架通过迭代的“工作者-监督者”细化架构来协调通用视觉语言模型。其中，工作者执行分割任务，监督者根据检索到的指南对分割结果进行批判，轻量级强化学习停止策略决定何时终止循环，以确保掩码符合指南并平衡资源使用。", "result": "在Waymo和ReasonSeg数据集上的评估显示，该方法显著优于现有最先进的基线方法，展现出强大的泛化能力和指令依从性。", "conclusion": "该框架通过创新的多智能体和迭代细化机制，有效解决了语义分割中遵守复杂、段落长度文本指南的挑战，且无需额外训练，显著提升了分割结果的指南一致性。"}}
{"id": "2509.04876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04876", "abs": "https://arxiv.org/abs/2509.04876", "authors": ["Jusheng Zhang", "Yijia Fan", "Kaitong Cai", "Xiaofei Sun", "Keze Wang"], "title": "OSC: Cognitive Orchestration through Dynamic Knowledge Alignment in Multi-Agent LLM Collaboration", "comment": "Accepted at EMNLP 2025 (Long Paper)", "summary": "This paper introduces OSC (Orchestrating Cognitive Synergy), a\nknowledge-aware adaptive collaboration framework designed to enhance cognitive\nsynergy in multi-agent systems with large language models. While prior work has\nadvanced agent selection and result aggregation, efficient linguistic\ninteractions for deep collaboration among expert agents remain a critical\nbottleneck. OSC addresses this gap as a pivotal intermediate layer between\nselection and aggregation, introducing Collaborator Knowledge Models (CKM) to\nenable each agent to dynamically perceive its collaborators' cognitive states.\nThrough real-time cognitive gap analysis, agents adaptively adjust\ncommunication behaviors, including content focus, detail level, and expression\nstyle, using learned strategies. Experiments on complex reasoning and\nproblem-solving benchmarks demonstrate that OSC significantly improves task\nperformance and communication efficiency, transforming \"parallel-working\nindividuals'' into a \"deeply collaborative cognitive team.'' This framework not\nonly optimizes multi-agent collaboration but also offers new insights into LLM\nagent interaction behaviors.", "AI": {"tldr": "OSC (Orchestrating Cognitive Synergy) 是一个知识感知的自适应协作框架，旨在通过引入协作知识模型（CKM）和实时认知差距分析，增强多智能体系统中大型语言模型之间的深度协作，从而优化沟通效率和任务性能。", "motivation": "现有工作在智能体选择和结果聚合方面有所进展，但专家智能体之间高效的语言交互以实现深度协作仍然是关键瓶颈。OSC 旨在解决在智能体选择和聚合之间缺乏有效中间层的问题。", "method": "OSC 作为一个关键的中间层，引入了协作知识模型（CKM），使每个智能体能够动态感知其协作者的认知状态。通过实时认知差距分析，智能体利用学习到的策略自适应地调整沟通行为，包括内容焦点、细节水平和表达风格。", "result": "在复杂的推理和问题解决基准测试中，OSC 显著提高了任务性能和沟通效率，将“并行工作的个体”转变为“深度协作的认知团队”。", "conclusion": "该框架不仅优化了多智能体协作，还为大型语言模型智能体之间的交互行为提供了新的见解。"}}
{"id": "2509.05020", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05020", "abs": "https://arxiv.org/abs/2509.05020", "authors": ["Matthieu Mesnage", "Sophie Villenave", "Bertrand Massot", "Matthieu Blanchard", "Pierre Raimbaud", "Guillaume Lavoué", "Claudine Gehin"], "title": "StimulHeat: a Low-Energy Wearable Thermal Feedback Device Using Peltier Elements with Heat Flow Controlled Loop for Hand Interactions in Virtual Reality", "comment": null, "summary": "Nowadays, the majority of wearable thermal feedback systems designed for use\nin virtual reality applications are not compatible or not integrated to\nstandard controllers and are based on temperature control. The objectives of\nthe present work is to enable integration with existing controllers, in this\ncase Valve Index controllers, and to propose an alternative approach to\nmanaging thermal stimulation with Peltier modules by controlling heat flow\ninstead of temperature. We introduce StimulHeat as a wireless, low power\nthermal feedback system, based on the continuous relationship between heat and\ncurrent injection in thermoelectric device (TED). First, we designed an\noptimized TED driver capable of injecting a continuous, bidirectional current\ninto the TED, thereby driving it as a heater or cooler. Subsequently, this\ndriver was implemented in an electronic board to include temperature and heat\nflow control loops, as well as Bluetooth Low Energy interface for remote\ncontrol. A mechanical integration was conducted, in the form of a controller\nextension which is non-intrusive and can be clipped to Valve Index controllers\nto enclose the TED, temperature sensors and electronics. Finally, we present a\nuser study validating StimulHeat for use in Virtual Reality, utilizing a\nUnity-built virtual environment with our open-source package.", "AI": {"tldr": "StimulHeat是一个无线、低功耗的热反馈系统，可集成到Valve Index控制器，通过控制热流而非温度提供热刺激，并通过用户研究验证了其在VR中的应用。", "motivation": "现有VR热反馈系统大多不兼容或不集成到标准控制器，且基于温度控制，而非热流控制。", "method": "本研究通过以下方法实现：1. 提出通过控制热流而非温度来管理热刺激的方法。2. 设计了优化的热电装置（TED）驱动器，能够注入连续、双向电流以实现加热或冷却。3. 将该驱动器集成到电子板中，包含温度和热流控制回路以及蓝牙低功耗（BLE）接口。4. 设计了可夹持在Valve Index控制器上的非侵入式机械集成扩展件。5. 进行了用户研究，在一个Unity构建的虚拟环境中验证了StimulHeat在VR中的应用。", "result": "本研究开发了无线、低功耗的热反馈系统StimulHeat，该系统通过控制热电装置（TED）的电流实现热流控制，而非传统的温度控制。设计并实现了能够注入连续双向电流的TED驱动器，以及包含温度和热流控制回路、蓝牙低功耗接口的电子板。完成了可集成到Valve Index控制器的非侵入式机械设计。用户研究结果验证了StimulHeat在虚拟现实应用中的有效性。", "conclusion": "StimulHeat作为一个无线、低功耗的热反馈系统，成功实现了与现有VR控制器的集成，并通过控制热流而非温度提供了新的热刺激管理方法，并通过用户研究验证了其在VR应用中的有效性。"}}
{"id": "2509.04996", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04996", "abs": "https://arxiv.org/abs/2509.04996", "authors": ["Moritz Reuss", "Hongyi Zhou", "Marcel Rühle", "Ömer Erdinç Yağmurlu", "Fabian Otto", "Rudolf Lioutikov"], "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient Vision-Language-Action Flow Policies", "comment": "Published at CoRL 2025", "summary": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to $50\\%$ of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by $20\\%$ through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across $190$ tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/.", "AI": {"tldr": "本文提出了一种名为FLOWER的高效视觉-语言-动作（VLA）策略，通过中间模态融合和动作特定全局条件化，显著降低了计算成本和资源需求，同时保持了竞争力。", "motivation": "现有扩散式VLA策略需要数十亿参数模型和海量数据集才能达到良好性能，导致计算成本和资源需求过高，阻碍了其实际机器人部署。", "method": "研究采用了两种方法：1. 中间模态融合：通过剪枝高达50%的LLM层，将容量重新分配给扩散头。2. 动作特定Global-AdaLN条件化：通过模块化适应，减少了20%的参数。这些方法被整合到一个9.5亿参数的VLA模型FLOWER中。", "result": "FLOWER仅用200个H100 GPU小时预训练，在涵盖10个模拟和真实世界基准的190项任务中，与更大的VLA模型表现出竞争力，并在CALVIN ABC基准上取得了4.53的新SOTA（State-of-the-Art）成绩，同时对多样化的机器人实体展现了鲁棒性。", "conclusion": "FLOWER提供了一个高效、高性能的VLA解决方案，通过创新的架构优化，显著降低了训练和部署成本，使其成为实际机器人应用中更具可行性的选择。"}}
{"id": "2509.04466", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04466", "abs": "https://arxiv.org/abs/2509.04466", "authors": ["Yuxuan Li", "Declan Campbell", "Stephanie C. Y. Chan", "Andrew Kyle Lampinen"], "title": "Just-in-time and distributed task representations in language models", "comment": null, "summary": "Many of language models' impressive capabilities originate from their\nin-context learning: based on instructions or examples, they can infer and\nperform new tasks without weight updates. In this work, we investigate\n\\emph{when} representations for new tasks are formed in language models, and\n\\emph{how} these representations change over the course of context. We focus on\n''transferrable'' task representations -- vector representations that can\nrestore task context in another instance of the model, even without the full\nprompt. We show that these representations evolve in non-monotonic and sporadic\nways, and are distinct from a more inert representation of high-level task\ncategories that persists throughout the context. Specifically, models often\ncondense multiple evidence into these transferrable task representations, which\nalign well with the performance improvement based on more examples in the\ncontext. However, this accrual process exhibits strong locality along the\nsequence dimension, coming online only at certain tokens -- despite task\nidentity being reliably decodable throughout the context. Moreover, these local\nbut transferrable task representations tend to capture minimal ''task scopes'',\nsuch as a semantically-independent subtask, and models rely on more\ntemporally-distributed representations to support longer and composite tasks.\nThis two-fold locality (temporal and semantic) underscores a kind of\njust-in-time computational process underlying language models' ability to adapt\nto new evidence and learn new tasks on the fly.", "AI": {"tldr": "本研究探究了语言模型在上下文学习中新任务表示的形成时间和演变方式，发现这些可迁移表示是非单调、零星且具有时序和语义局部性的，表明模型采用一种即时计算过程来适应新证据。", "motivation": "语言模型强大的上下文学习能力源于其无需权重更新即可根据指令或示例学习新任务。本研究旨在深入理解这些新任务的表示何时形成以及在上下文过程中如何变化。", "method": "研究关注“可迁移”任务表示，即能够恢复模型中任务上下文的向量表示。通过观察这些表示在上下文中的演变，分析其形成机制、累积过程以及与任务性能和结构的关系。", "result": "研究发现，可迁移任务表示以非单调和零星的方式演变，且与更惰性的高级任务类别表示不同。模型将多个证据浓缩到这些可迁移表示中，这与更多示例带来的性能提升相符。然而，这种累积过程在序列维度上表现出强烈的局部性，仅在特定标记处在线。此外，这些局部但可迁移的任务表示倾向于捕获最小的“任务范围”（如语义独立的子任务），而模型依赖更长时间分布的表示来支持更长和复合的任务，显示出时序和语义上的双重局部性。", "conclusion": "语言模型在上下文学习中，新任务表示的形成和演变具有显著的时序和语义局部性。这种“即时”的计算过程是语言模型适应新证据并实时学习新任务能力的基础。"}}
{"id": "2509.04711", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.04711", "abs": "https://arxiv.org/abs/2509.04711", "authors": ["Satoshi Tanaka", "Kok Seang Tan", "Isamu Yamashita"], "title": "Domain Adaptation for Different Sensor Configurations in 3D Object Detection", "comment": null, "summary": "Recent advances in autonomous driving have underscored the importance of\naccurate 3D object detection, with LiDAR playing a central role due to its\nrobustness under diverse visibility conditions. However, different vehicle\nplatforms often deploy distinct sensor configurations, causing performance\ndegradation when models trained on one configuration are applied to another\nbecause of shifts in the point cloud distribution. Prior work on multi-dataset\ntraining and domain adaptation for 3D object detection has largely addressed\nenvironmental domain gaps and density variation within a single LiDAR; in\ncontrast, the domain gap for different sensor configurations remains largely\nunexplored. In this work, we address domain adaptation across different sensor\nconfigurations in 3D object detection. We propose two techniques: Downstream\nFine-tuning (dataset-specific fine-tuning after multi-dataset training) and\nPartial Layer Fine-tuning (updating only a subset of layers to improve\ncross-configuration generalization). Using paired datasets collected in the\nsame geographic region with multiple sensor configurations, we show that joint\ntraining with Downstream Fine-tuning and Partial Layer Fine-tuning consistently\noutperforms naive joint training for each configuration. Our findings provide a\npractical and scalable solution for adapting 3D object detection models to the\ndiverse vehicle platforms.", "AI": {"tldr": "本文提出并解决了3D目标检测中不同LiDAR传感器配置间的域适应问题，通过下游微调和部分层微调技术，实现了模型在不同车辆平台上的鲁棒性能。", "motivation": "自动驾驶中，不同车辆平台部署的传感器配置各异，导致模型在不同配置间应用时性能下降，原因在于点云分布的变化。现有工作主要关注环境域差距和单一LiDAR内的密度变化，而不同传感器配置间的域差距尚未得到充分探索。", "method": "提出了两种技术：1) 下游微调（多数据集训练后进行数据集特定微调），2) 部分层微调（仅更新部分网络层以改善跨配置泛化能力）。研究使用了在同一地理区域、多种传感器配置下采集的配对数据集。", "result": "研究表明，结合下游微调和部分层微调的联合训练，在每种传感器配置下都显著优于简单的联合训练（即朴素联合训练）。", "conclusion": "研究结果为3D目标检测模型适应多样化车辆平台提供了一个实用且可扩展的解决方案。"}}
{"id": "2509.04908", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04908", "abs": "https://arxiv.org/abs/2509.04908", "authors": ["Hongyi Jing", "Jiafu Chen", "Chen Rao", "Ziqiang Dang", "Jiajie Teng", "Tianyi Chu", "Juncheng Mo", "Shuo Fang", "Huaizhong Lin", "Rui Lv", "Chenguang Ma", "Lei Zhao"], "title": "SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing", "comment": null, "summary": "The existing Multimodal Large Language Models (MLLMs) for GUI perception have\nmade great progress. However, the following challenges still exist in prior\nmethods: 1) They model discrete coordinates based on text autoregressive\nmechanism, which results in lower grounding accuracy and slower inference\nspeed. 2) They can only locate predefined sets of elements and are not capable\nof parsing the entire interface, which hampers the broad application and\nsupport for downstream tasks. To address the above issues, we propose\nSparkUI-Parser, a novel end-to-end framework where higher localization\nprecision and fine-grained parsing capability of the entire interface are\nsimultaneously achieved. Specifically, instead of using probability-based\ndiscrete modeling, we perform continuous modeling of coordinates based on a\npre-trained Multimodal Large Language Model (MLLM) with an additional token\nrouter and coordinate decoder. This effectively mitigates the limitations\ninherent in the discrete output characteristics and the token-by-token\ngeneration process of MLLMs, consequently boosting both the accuracy and the\ninference speed. To further enhance robustness, a rejection mechanism based on\na modified Hungarian matching algorithm is introduced, which empowers the model\nto identify and reject non-existent elements, thereby reducing false positives.\nMoreover, we present ScreenParse, a rigorously constructed benchmark to\nsystematically assess structural perception capabilities of GUI models across\ndiverse scenarios. Extensive experiments demonstrate that our approach\nconsistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,\nCAGUI-Grounding and ScreenParse benchmarks. The resources are available at\nhttps://github.com/antgroup/SparkUI-Parser.", "AI": {"tldr": "SparkUI-Parser是一个用于GUI感知的端到端框架，它通过连续坐标建模和拒绝机制，显著提高了定位精度和全界面解析能力，并引入了新的评估基准ScreenParse。", "motivation": "现有的GUI多模态大语言模型（MLLMs）面临两大挑战：1) 基于文本自回归机制的离散坐标建模导致定位精度低和推理速度慢；2) 只能定位预定义元素，无法解析整个界面，限制了其广泛应用。", "method": "本文提出了SparkUI-Parser框架。它通过在预训练MLLM上增加一个token路由器和坐标解码器，实现坐标的连续建模，而非概率离散建模，从而提高精度和推理速度。此外，引入了基于改进匈牙利匹配算法的拒绝机制，以识别并拒绝不存在的元素，减少误报。同时，构建了一个新的基准ScreenParse，用于系统评估GUI模型的结构感知能力。", "result": "广泛实验表明，SparkUI-Parser在ScreenSpot、ScreenSpot-v2、CAGUI-Grounding和ScreenParse等基准测试中，持续优于现有SOTA方法。", "conclusion": "SparkUI-Parser通过创新的连续坐标建模和鲁棒的拒绝机制，成功实现了更高的定位精度和细粒度的全界面解析能力，有效解决了现有MLLM在GUI感知中的局限性。同时，提出的ScreenParse基准为未来研究提供了系统评估工具。"}}
{"id": "2509.05167", "categories": ["eess.SY", "cs.SY", "math.OC", "quant-ph"], "pdf": "https://arxiv.org/pdf/2509.05167", "abs": "https://arxiv.org/abs/2509.05167", "authors": ["Eya Guizani", "Julian Berberich"], "title": "Model predictive quantum control: A modular approach for efficient and robust quantum optimal control", "comment": null, "summary": "Model predictive control (MPC) is one of the most successful modern control\nmethods. It relies on repeatedly solving a finite-horizon optimal control\nproblem and applying the beginning piece of the optimal input. In this paper,\nwe develop a modular framework for improving efficiency and robustness of\nquantum optimal control (QOC) via MPC. We first provide a tutorial introduction\nto basic concepts of MPC from a QOC perspective. We then present multiple MPC\nschemes, ranging from simple approaches to more sophisticated schemes which\nadmit stability guarantees. This yields a modular framework which can be used\n1) to improve efficiency of open-loop QOC and 2) to improve robustness of\nclosed-loop quantum control by incorporating feedback. We demonstrate these\nbenefits with numerical results, where we benchmark the proposed methods\nagainst competing approaches.", "AI": {"tldr": "本文提出一个模块化框架，通过模型预测控制（MPC）提高量子最优控制（QOC）的效率和鲁棒性。", "motivation": "模型预测控制（MPC）是现代控制方法中非常成功的一种，研究者希望将其应用于量子最优控制（QOC），以解决现有QOC的效率和鲁棒性问题。", "method": "本文首先从QOC角度介绍MPC基本概念，然后提出多种MPC方案，从简单方法到具有稳定性保证的复杂方案。该框架可用于提高开环QOC的效率和通过反馈提高闭环量子控制的鲁棒性。", "result": "数值结果表明，所提出的方法在效率和鲁棒性方面优于现有竞争方法。", "conclusion": "MPC可以显著提高量子最优控制的效率和闭环量子控制的鲁棒性，为QOC提供了一个有效的模块化框架。"}}
{"id": "2509.05031", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.05031", "abs": "https://arxiv.org/abs/2509.05031", "authors": ["Luca Müller", "Hassan Ali", "Philipp Allgeuer", "Lukáš Gajdošech", "Stefan Wermter"], "title": "Pointing-Guided Target Estimation via Transformer-Based Attention", "comment": "Accepted at the 34th International Conference on Artificial Neural\n  Networks (ICANN) 2025,12 pages,4 figures,1 table; work was co-funded by\n  Horizon Europe project TERAIS under Grant agreement number 101079338", "summary": "Deictic gestures, like pointing, are a fundamental form of non-verbal\ncommunication, enabling humans to direct attention to specific objects or\nlocations. This capability is essential in Human-Robot Interaction (HRI), where\nrobots should be able to predict human intent and anticipate appropriate\nresponses. In this work, we propose the Multi-Modality Inter-TransFormer\n(MM-ITF), a modular architecture to predict objects in a controlled tabletop\nscenario with the NICOL robot, where humans indicate targets through natural\npointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing\ngestures to object locations, assigns a likelihood score to each, and\nidentifies the most likely target. Our results demonstrate that the method can\naccurately predict the intended object using monocular RGB data, thus enabling\nintuitive and accessible human-robot collaboration. To evaluate the\nperformance, we introduce a patch confusion matrix, providing insights into the\nmodel's predictions across candidate object locations. Code available at:\nhttps://github.com/lucamuellercode/MMITF.", "AI": {"tldr": "本文提出MM-ITF模型，通过多模态交互注意力，利用单目RGB数据准确预测人类指向手势的目标物体，以实现直观的人机协作。", "motivation": "指向手势是人类非语言沟通的基本形式，在人机交互（HRI）中至关重要，机器人需要能够预测人类意图并做出适当响应。", "method": "提出了一种名为Multi-Modality Inter-TransFormer (MM-ITF) 的模块化架构，该架构利用跨模态注意力，将2D指向手势映射到物体位置，为每个位置分配可能性分数，并识别最可能的目标。使用单目RGB数据进行预测。为评估性能，引入了补丁混淆矩阵。", "result": "研究结果表明，该方法能够使用单目RGB数据准确预测预期的目标物体，从而实现直观且易于访问的人机协作。", "conclusion": "MM-ITF模型通过准确预测指向手势的目标，显著提升了人机交互的直观性和可访问性。"}}
{"id": "2509.04467", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04467", "abs": "https://arxiv.org/abs/2509.04467", "authors": ["Hao Zhang", "Mengsi Lyu", "Yulong Ao", "Yonghua Lin"], "title": "Enhancing LLM Efficiency: Targeted Pruning for Prefill-Decode Disaggregation in Inference", "comment": "21 pages", "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the default settings, our method\nachieves a 20.56% inference speedup and a 4.95 times reduction in data\ntransmission bandwidth consumption.", "AI": {"tldr": "本文提出了一种针对大语言模型（LLMs）预填充-解码（PD）分离推理的新型剪枝方法，通过独立的块移除和令牌感知的KV缓存剪枝，显著提高了推理速度并降低了数据传输带宽。", "motivation": "大语言模型（LLMs）的高计算和内存成本限制了其部署。现有剪枝方法未能充分考虑实际应用中预填充-解码（PD）分离的特性，导致剪枝效率和精度不足。", "method": "本文提出了一种针对PD分离推理的剪枝方法。它构建了剪枝和蒸馏数据集，对预填充和解码阶段独立进行迭代块移除，以获得更优的剪枝方案。此外，引入了令牌感知的缓存剪枝机制，在预填充阶段保留所有KV缓存，但在解码阶段选择性地重用特定层中第一个和最后一个令牌序列的KV缓存条目，以最小开销减少通信成本。", "result": "该方法在PD分离和PD统一设置下均表现出强大的性能。在默认设置下，推理速度提升了20.56%，数据传输带宽消耗减少了4.95倍。", "conclusion": "所提出的剪枝方法有效解决了LLM部署中的计算和内存瓶颈，通过精确的块和KV缓存剪枝，特别是在PD分离推理场景下，显著提升了推理效率和带宽利用率。"}}
{"id": "2509.04729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04729", "abs": "https://arxiv.org/abs/2509.04729", "authors": ["Tianxiang Xue", "Jiayi Zhao", "Jingsheng Li", "Changlu Chen", "Kun Zhan"], "title": "CD-Mamba: Cloud detection with long-range spatial dependency modeling", "comment": "Journal of Applied Remote Sensing", "summary": "Remote sensing images are frequently obscured by cloud cover, posing\nsignificant challenges to data integrity and reliability. Effective cloud\ndetection requires addressing both short-range spatial redundancies and\nlong-range atmospheric similarities among cloud patches. Convolutional neural\nnetworks are effective at capturing local spatial dependencies, while Mamba has\nstrong capabilities in modeling long-range dependencies. To fully leverage both\nlocal spatial relations and long-range dependencies, we propose CD-Mamba, a\nhybrid model that integrates convolution and Mamba's state-space modeling into\na unified cloud detection network. CD-Mamba is designed to comprehensively\ncapture pixelwise textural details and long term patchwise dependencies for\ncloud detection. This design enables CD-Mamba to manage both pixel-wise\ninteractions and extensive patch-wise dependencies simultaneously, improving\ndetection accuracy across diverse spatial scales. Extensive experiments\nvalidate the effectiveness of CD-Mamba and demonstrate its superior performance\nover existing methods.", "AI": {"tldr": "本文提出CD-Mamba，一种结合卷积和Mamba状态空间模型的混合网络，用于遥感图像云检测，旨在同时捕获局部空间细节和长距离依赖关系，并取得了优于现有方法的性能。", "motivation": "遥感图像常被云层遮挡，严重影响数据完整性和可靠性。有效的云检测需要同时处理云斑块的短程空间冗余和长程大气相似性。现有方法在捕获局部或长程依赖方面存在局限性。", "method": "提出CD-Mamba混合模型，将卷积神经网络（擅长捕获局部空间依赖）与Mamba的状态空间模型（擅长建模长程依赖）集成到一个统一的云检测网络中。该模型旨在全面捕获像素级纹理细节和长期的块级依赖关系。", "result": "广泛的实验验证了CD-Mamba的有效性，并证明其性能优于现有方法。", "conclusion": "CD-Mamba通过同时管理像素级交互和广泛的块级依赖，显著提高了不同空间尺度下的云检测精度。"}}
{"id": "2509.04926", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04926", "abs": "https://arxiv.org/abs/2509.04926", "authors": ["Barbara Gendron", "Gaël Guibon", "Mathieu D'aquin"], "title": "Towards Ontology-Based Descriptions of Conversations with Qualitatively-Defined Concepts", "comment": "Accepted at TOTh 2025 (Terminology \\& Ontology: Theories and\n  applications)", "summary": "The controllability of Large Language Models (LLMs) when used as\nconversational agents is a key challenge, particularly to ensure predictable\nand user-personalized responses. This work proposes an ontology-based approach\nto formally define conversational features that are typically qualitative in\nnature. By leveraging a set of linguistic descriptors, we derive quantitative\ndefinitions for qualitatively-defined concepts, enabling their integration into\nan ontology for reasoning and consistency checking. We apply this framework to\nthe task of proficiency-level control in conversations, using CEFR language\nproficiency levels as a case study. These definitions are then formalized in\ndescription logic and incorporated into an ontology, which guides controlled\ntext generation of an LLM through fine-tuning. Experimental results demonstrate\nthat our approach provides consistent and explainable proficiency-level\ndefinitions, improving transparency in conversational AI.", "AI": {"tldr": "本文提出了一种基于本体论的方法，通过将定性会话特征（如语言熟练度）量化并形式化，以提高大型语言模型（LLM）作为对话代理时的可控性和透明度。", "motivation": "LLM作为对话代理时，其可控性是一个关键挑战，尤其是在确保可预测和用户个性化响应方面。传统的定性会话特征难以有效控制。", "method": "该研究提出了一种基于本体论的方法。首先，利用语言描述符将定性会话特征（如CEFR语言熟练度）转换为定量定义。然后，将这些定义形式化为描述逻辑并整合到本体中，用于推理和一致性检查。最后，通过微调LLM，利用该本体指导受控的文本生成。", "result": "实验结果表明，该方法提供了连贯且可解释的熟练度级别定义，显著提高了会话AI的透明度。", "conclusion": "基于本体论的方法能够有效地将定性会话特征形式化并用于控制LLM的文本生成，从而提高LLM在会话中的可控性、一致性和透明度。"}}
{"id": "2509.05191", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05191", "abs": "https://arxiv.org/abs/2509.05191", "authors": ["Songlin Jin", "Yuanbo Nie", "Morgan Jones"], "title": "Feedback Linearisation with State Constraints", "comment": null, "summary": "Feedback Linearisation (FBL) is a widely used technique that applies feedback\nlaws to transform input-affine nonlinear dynamical systems into linear\ndynamical systems, allowing for the use of linear controller design methods\nsuch as pole placement. However, for problems with state constraints,\ncontrolling the linear system induced by FBL can be more challenging than\ncontrolling the original system. This is because simple state constraints in\nthe original nonlinear system become complex nonlinear constraints in the FBL\ninduced linearised system, thereby diminishing the advantages of linearisation.\nTo avoid increasing the complexity of state constraints under FBL, this paper\nintroduces a method to first augment system dynamics to capture state\nconstraints before applying FBL. We show that our proposed augmentation method\nleads to ill-defined relative degrees at state constraint boundaries. However,\nwe show that ill-defined relative degrees can be overcome by using a switching\nFBL controller. Numerical experiments illustrate the capabilities of this\nmethod for handling state constraints within the FBL framework.", "AI": {"tldr": "本文提出一种在应用反馈线性化（FBL）之前先增强系统动力学以捕获状态约束的方法，并通过切换FBL控制器克服了由此产生的相对度定义不清问题，从而简化了FBL中状态约束的处理。", "motivation": "反馈线性化（FBL）虽然能将非线性系统转化为线性系统，但原始系统中的简单状态约束在FBL后的线性化系统中会变得复杂，削弱了线性化的优势，增加了控制难度。", "method": "该方法首先在应用FBL之前增强系统动力学以纳入状态约束。针对由此在状态约束边界处导致的相对度定义不清问题，提出使用切换FBL控制器来解决。", "result": "所提出的增强方法会导致在状态约束边界处出现相对度定义不清的问题，但通过使用切换FBL控制器可以克服这些问题。数值实验验证了该方法在FBL框架内处理状态约束的能力。", "conclusion": "该方法通过在FBL前增强系统动力学并结合切换FBL控制器，成功地在FBL框架内处理了状态约束，避免了约束复杂化的问题。"}}
{"id": "2509.05042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05042", "abs": "https://arxiv.org/abs/2509.05042", "authors": ["Cristiano Caissutti", "Estelle Gerbier", "Ehsan Khorrambakht", "Paolo Marinelli", "Andrea Munafo'", "Andrea Caiti"], "title": "Shared Autonomy through LLMs and Reinforcement Learning for Applications to Ship Hull Inspections", "comment": null, "summary": "Shared autonomy is a promising paradigm in robotic systems, particularly\nwithin the maritime domain, where complex, high-risk, and uncertain\nenvironments necessitate effective human-robot collaboration. This paper\ninvestigates the interaction of three complementary approaches to advance\nshared autonomy in heterogeneous marine robotic fleets: (i) the integration of\nLarge Language Models (LLMs) to facilitate intuitive high-level task\nspecification and support hull inspection missions, (ii) the implementation of\nhuman-in-the-loop interaction frameworks in multi-agent settings to enable\nadaptive and intent-aware coordination, and (iii) the development of a modular\nMission Manager based on Behavior Trees to provide interpretable and flexible\nmission control. Preliminary results from simulation and real-world lake-like\nenvironments demonstrate the potential of this multi-layered architecture to\nreduce operator cognitive load, enhance transparency, and improve adaptive\nbehaviour alignment with human intent. Ongoing work focuses on fully\nintegrating these components, refining coordination mechanisms, and validating\nthe system in operational port scenarios. This study contributes to\nestablishing a modular and scalable foundation for trustworthy,\nhuman-collaborative autonomy in safety-critical maritime robotics applications.", "AI": {"tldr": "本文提出了一种用于异构海洋机器人舰队的共享自治多层架构，该架构结合了大型语言模型、人机协作框架和行为树，旨在提高人机协作在复杂海洋环境中的效率和透明度。", "motivation": "在复杂、高风险和不确定的海洋环境中，有效的机器人与人类协作至关重要，因此需要先进的共享自治范式。", "method": "该研究结合了三种互补方法：1) 集成大型语言模型（LLMs）以促进直观的高级任务规范（如船体检查）；2) 在多智能体设置中实现人机协作框架，以实现自适应和意图感知协调；3) 开发基于行为树的模块化任务管理器，提供可解释和灵活的任务控制。", "result": "初步的仿真和实际湖泊环境测试结果表明，该多层架构有潜力减少操作员的认知负荷，增强透明度，并改善自适应行为与人类意图的一致性。", "conclusion": "这项研究为安全关键型海洋机器人应用中的可信、人机协作自治奠定了模块化和可扩展的基础。"}}
{"id": "2509.04468", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04468", "abs": "https://arxiv.org/abs/2509.04468", "authors": ["Xuan Yao", "Qianteng Wang", "Xinbo Liu", "Ke-Wei Huang"], "title": "Evaluating Large Language Models for Financial Reasoning: A CFA-Based Benchmark Study", "comment": null, "summary": "The rapid advancement of large language models presents significant\nopportunities for financial applications, yet systematic evaluation in\nspecialized financial contexts remains limited. This study presents the first\ncomprehensive evaluation of state-of-the-art LLMs using 1,560 multiple-choice\nquestions from official mock exams across Levels I-III of CFA, most rigorous\nprofessional certifications globally that mirror real-world financial analysis\ncomplexity. We compare models distinguished by core design priorities:\nmulti-modal and computationally powerful, reasoning-specialized and highly\naccurate, and lightweight efficiency-optimized.\n  We assess models under zero-shot prompting and through a novel\nRetrieval-Augmented Generation pipeline that integrates official CFA curriculum\ncontent. The RAG system achieves precise domain-specific knowledge retrieval\nthrough hierarchical knowledge organization and structured query generation,\nsignificantly enhancing reasoning accuracy in professional financial\ncertification evaluation.\n  Results reveal that reasoning-oriented models consistently outperform others\nin zero-shot settings, while the RAG pipeline provides substantial improvements\nparticularly for complex scenarios. Comprehensive error analysis identifies\nknowledge gaps as the primary failure mode, with minimal impact from text\nreadability. These findings provide actionable insights for LLM deployment in\nfinance, offering practitioners evidence-based guidance for model selection and\ncost-performance optimization.", "AI": {"tldr": "本研究首次使用1,560道CFA模拟考试题全面评估了最先进的大型语言模型（LLMs）在金融领域的表现，并引入了一种结合CFA课程内容的检索增强生成（RAG）管道。", "motivation": "尽管大型语言模型在金融应用中展现巨大潜力，但其在专业金融背景下的系统性评估仍然有限。", "method": "研究使用了来自CFA I-III级官方模拟考试的1,560道选择题。比较了多模态、推理专用和轻量级LLMs在零样本提示下的表现。还开发了一种新颖的检索增强生成（RAG）管道，通过分层知识组织和结构化查询生成，整合了官方CFA课程内容以增强领域特定知识检索。", "result": "结果显示，推理导向模型在零样本设置下表现最佳，而RAG管道尤其在复杂场景中提供了显著改进。全面的错误分析表明，知识空白是主要的失败模式，文本可读性影响微乎其微。", "conclusion": "这些发现为LLM在金融领域的部署提供了可操作的见解，为从业者提供了模型选择和成本-性能优化的循证指导。"}}
{"id": "2509.04732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04732", "abs": "https://arxiv.org/abs/2509.04732", "authors": ["Shengqian Zhu", "Jiafei Wu", "Xiaogang Xu", "Chengrong Yu", "Ying Song", "Zhang Yi", "Guangjun Li", "Junjie Hu"], "title": "Exploiting Unlabeled Structures through Task Consistency Training for Versatile Medical Image Segmentation", "comment": null, "summary": "Versatile medical image segmentation (VMIS) targets the segmentation of\nmultiple classes, while obtaining full annotations for all classes is often\nimpractical due to the time and labor required. Leveraging partially labeled\ndatasets (PLDs) presents a promising alternative; however, current VMIS\napproaches face significant class imbalance due to the unequal category\ndistribution in PLDs. Existing methods attempt to address this by generating\npseudo-full labels. Nevertheless, these typically require additional models and\noften result in potential performance degradation from label noise. In this\nwork, we introduce a Task Consistency Training (TCT) framework to address class\nimbalance without requiring extra models. TCT includes a backbone network with\na main segmentation head (MSH) for multi-channel predictions and multiple\nauxiliary task heads (ATHs) for task-specific predictions. By enforcing a\nconsistency constraint between the MSH and ATH predictions, TCT effectively\nutilizes unlabeled anatomical structures. To avoid error propagation from\nlow-consistency, potentially noisy data, we propose a filtering strategy to\nexclude such data. Additionally, we introduce a unified auxiliary\nuncertainty-weighted loss (UAUWL) to mitigate segmentation quality declines\ncaused by the dominance of specific tasks. Extensive experiments on eight\nabdominal datasets from diverse clinical sites demonstrate our approach's\neffectiveness.", "AI": {"tldr": "本文提出了一种任务一致性训练（TCT）框架，用于解决在部分标注数据集上进行多功能医学图像分割（VMIS）时因类别不平衡和标签噪声问题，该框架无需额外模型，通过一致性约束、过滤策略和统一辅助不确定性加权损失来实现。", "motivation": "多功能医学图像分割（VMIS）需要完整的类别标注，但这在实践中耗时且费力。利用部分标注数据集（PLDs）是一种有前景的替代方案，但现有方法面临严重的类别不平衡问题。现有方法通过生成伪完整标签来解决，但这通常需要额外的模型并可能因标签噪声导致性能下降。", "method": "本文引入了任务一致性训练（TCT）框架。TCT包含一个主干网络，一个用于多通道预测的主分割头（MSH）和多个用于特定任务预测的辅助任务头（ATHs）。通过在MSH和ATH预测之间施加一致性约束，TCT有效利用未标注的解剖结构。为避免低一致性、潜在噪声数据引起的错误传播，提出了一种过滤策略。此外，引入了统一辅助不确定性加权损失（UAUWL）来缓解特定任务主导导致的分割质量下降。", "result": "在来自不同临床站点的八个腹部数据集上进行的广泛实验证明了该方法的有效性。", "conclusion": "TCT框架能够有效解决部分标注数据集上VMIS的类别不平衡问题，避免了额外模型的需求，并通过一致性训练、数据过滤和不确定性加权损失来提升分割性能和鲁棒性。"}}
{"id": "2509.04979", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04979", "abs": "https://arxiv.org/abs/2509.04979", "authors": ["Rajesh Tembarai Krishnamachari", "Srividya Rajesh"], "title": "Internet 3.0: Architecture for a Web-of-Agents with it's Algorithm for Ranking Agents", "comment": null, "summary": "AI agents -- powered by reasoning-capable large language models (LLMs) and\nintegrated with tools, data, and web search -- are poised to transform the\ninternet into a \\emph{Web of Agents}: a machine-native ecosystem where\nautonomous agents interact, collaborate, and execute tasks at scale. Realizing\nthis vision requires \\emph{Agent Ranking} -- selecting agents not only by\ndeclared capabilities but by proven, recent performance. Unlike Web~1.0's\nPageRank, a global, transparent network of agent interactions does not exist;\nusage signals are fragmented and private, making ranking infeasible without\ncoordination.\n  We propose \\textbf{DOVIS}, a five-layer operational protocol\n(\\emph{Discovery, Orchestration, Verification, Incentives, Semantics}) that\nenables the collection of minimal, privacy-preserving aggregates of usage and\nperformance across the ecosystem. On this substrate, we implement\n\\textbf{AgentRank-UC}, a dynamic, trust-aware algorithm that combines\n\\emph{usage} (selection frequency) and \\emph{competence} (outcome quality,\ncost, safety, latency) into a unified ranking. We present simulation results\nand theoretical guarantees on convergence, robustness, and Sybil resistance,\ndemonstrating the viability of coordinated protocols and performance-aware\nranking in enabling a scalable, trustworthy Agentic Web.", "AI": {"tldr": "本文提出了DOVIS协议和AgentRank-UC算法，旨在解决未来“智能体之网”中智能体排名面临的数据碎片化和隐私问题，通过协调协议实现基于使用和能力的动态、信任感知的智能体排名。", "motivation": "随着大型语言模型驱动的AI智能体及其工具、数据和网络搜索的集成，互联网正演变为一个“智能体之网”，其中自主智能体大规模交互、协作并执行任务。然而，要实现这一愿景，需要对智能体进行排名，而现有的排名机制（如PageRank）不适用，因为智能体的使用信号是碎片化和私密的，使得在没有协调的情况下进行排名不可行。", "method": "研究提出了DOVIS，一个五层操作协议（发现、编排、验证、激励、语义），用于收集最小、隐私保护的生态系统使用和性能聚合数据。在此基础上，实现了AgentRank-UC，一个动态、信任感知的算法，将“使用”（选择频率）和“能力”（结果质量、成本、安全性、延迟）结合起来进行统一排名。", "result": "通过模拟结果和理论保证，证明了DOVIS和AgentRank-UC在收敛性、鲁棒性和Sybil攻击抵抗方面的可行性，展示了协调协议和性能感知排名在构建可扩展、可信赖的智能体网络中的潜力。", "conclusion": "协调协议和性能感知排名对于实现可扩展、可信赖的智能体网络至关重要。DOVIS协议和AgentRank-UC算法为解决智能体排名挑战提供了可行的解决方案，能够有效整合使用和能力信号，促进智能体生态系统的发展。"}}
{"id": "2509.05201", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.05201", "abs": "https://arxiv.org/abs/2509.05201", "authors": ["Nariman Niknejad", "Gokul S. Sankar", "Bahare Kiumarsi", "Hamidreza Modares"], "title": "Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers", "comment": null, "summary": "This paper presents a robust model predictive control (MPC) framework that\nexplicitly addresses the non-Gaussian noise inherent in deep learning-based\nperception modules used for state estimation. Recognizing that accurate\nuncertainty quantification of the perception module is essential for safe\nfeedback control, our approach departs from the conventional assumption of\nzero-mean noise quantification of the perception error. Instead, it employs\nset-based state estimation with constrained zonotopes to capture biased,\nheavy-tailed uncertainties while maintaining bounded estimation errors. To\nimprove computational efficiency, the robust MPC is reformulated as a linear\nprogram (LP), using a Minkowski-Lyapunov-based cost function with an added\nslack variable to prevent degenerate solutions. Closed-loop stability is\nensured through Minkowski-Lyapunov inequalities and contractive zonotopic\ninvariant sets. The largest stabilizing terminal set and its corresponding\nfeedback gain are then derived via an ellipsoidal approximation of the\nzonotopes. The proposed framework is validated through both simulations and\nhardware experiments on an omnidirectional mobile robot along with a camera and\na convolutional neural network-based perception module implemented within a\nROS2 framework. The results demonstrate that the perception-aware MPC provides\nstable and accurate control performance under heavy-tailed noise conditions,\nsignificantly outperforming traditional Gaussian-noise-based designs in terms\nof both state estimation error bounding and overall control performance.", "AI": {"tldr": "本文提出了一种鲁棒模型预测控制（MPC）框架，专门处理深度学习感知模块中固有的非高斯噪声，通过集合状态估计和线性规划重构实现，并在重尾噪声条件下表现出优越的控制性能。", "motivation": "传统的MPC方法假设感知误差为零均值高斯噪声，但基于深度学习的感知模块存在固有的非高斯、有偏、重尾噪声，导致不准确的不确定性量化和不安全的反馈控制。因此，需要一种能够准确捕捉这类不确定性的控制框架。", "method": "该方法采用基于约束zonotope的集合状态估计来捕获有偏、重尾的不确定性，并保持有界估计误差。为了提高计算效率，鲁棒MPC被重新表述为线性规划（LP），使用基于Minkowski-Lyapunov的成本函数和松弛变量。通过Minkowski-Lyapunov不等式和收缩zonotope不变集确保闭环稳定性，并通过zonotope的椭球近似推导出最大的稳定终端集和反馈增益。", "result": "通过全向移动机器人上的仿真和硬件实验验证了该框架。结果表明，在重尾噪声条件下，所提出的感知感知MPC提供了稳定和准确的控制性能，在状态估计误差边界和整体控制性能方面显著优于传统的基于高斯噪声的设计。", "conclusion": "所提出的感知感知MPC框架能够有效处理深度学习感知模块中的非高斯、重尾噪声，确保了在复杂不确定性条件下的稳定和准确控制，其性能明显优于传统方法。"}}
{"id": "2509.05198", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05198", "abs": "https://arxiv.org/abs/2509.05198", "authors": ["Mohammad Saeid", "Amir Salarpour", "Pedram MohajerAnsari"], "title": "Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet", "comment": "This paper has been accepted for presentation at the 7th\n  International Conference on Pattern Recognition and Image Analysis (IPRIA\n  2025)", "summary": "The classification of 3D point clouds is crucial for applications such as\nautonomous driving, robotics, and augmented reality. However, the commonly used\nModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D\ndata, size mismatches, and inadequate class differentiation, which hinder model\nperformance. This paper introduces ModelNet-R, a meticulously refined version\nof ModelNet40 designed to address these issues and serve as a more reliable\nbenchmark. Additionally, this paper proposes Point-SkipNet, a lightweight\ngraph-based neural network that leverages efficient sampling, neighborhood\ngrouping, and skip connections to achieve high classification accuracy with\nreduced computational overhead. Extensive experiments demonstrate that models\ntrained in ModelNet-R exhibit significant performance improvements. Notably,\nPoint-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a\nsubstantially lower parameter count compared to contemporary models. This\nresearch highlights the crucial role of dataset quality in optimizing model\nefficiency for 3D point cloud classification. For more details, see the code\nat: https://github.com/m-saeid/ModeNetR_PointSkipNet.", "AI": {"tldr": "本文提出ModelNet-R，一个改进的3D点云分类数据集，并引入Point-SkipNet，一个轻量级图神经网络。实验证明ModelNet-R能显著提升模型性能，Point-SkipNet在ModelNet-R上以更少参数实现了最先进的准确率。", "motivation": "现有的ModelNet40数据集在3D点云分类中存在标签不一致、2D数据、尺寸不匹配和类别区分不足等局限性，阻碍了模型性能。", "method": "1. 引入ModelNet-R，一个精心改进的ModelNet40版本，旨在解决原数据集的问题。2. 提出Point-SkipNet，一个轻量级图神经网络，利用高效采样、邻域分组和跳跃连接。", "result": "1. 在ModelNet-R上训练的模型表现出显著的性能提升。2. Point-SkipNet在ModelNet-R上实现了最先进的准确率。3. Point-SkipNet与现有模型相比，参数量大幅减少。", "conclusion": "数据集质量对于优化3D点云分类模型的效率至关重要。"}}
{"id": "2509.04469", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04469", "abs": "https://arxiv.org/abs/2509.04469", "authors": ["David Berghaus", "Armin Berger", "Lars Hillebrand", "Kostadin Cvejoski", "Rafet Sifa"], "title": "Multi-Modal Vision vs. Text-Based Parsing: Benchmarking LLM Strategies for Invoice Processing", "comment": null, "summary": "This paper benchmarks eight multi-modal large language models from three\nfamilies (GPT-5, Gemini 2.5, and open-source Gemma 3) on three diverse openly\navailable invoice document datasets using zero-shot prompting. We compare two\nprocessing strategies: direct image processing using multi-modal capabilities\nand a structured parsing approach converting documents to markdown first.\nResults show native image processing generally outperforms structured\napproaches, with performance varying across model types and document\ncharacteristics. This benchmark provides insights for selecting appropriate\nmodels and processing strategies for automated document systems. Our code is\navailable online.", "AI": {"tldr": "本文对八种多模态大语言模型（GPT-5、Gemini 2.5、Gemma 3系列）在三个发票数据集上进行了零样本基准测试，比较了直接图像处理和结构化解析两种策略，发现直接图像处理通常表现更优。", "motivation": "为自动化文档系统选择合适的模型和处理策略提供指导和见解。", "method": "使用零样本提示，对来自三个系列（GPT-5、Gemini 2.5、开源Gemma 3）的八种多模态大语言模型进行了基准测试。测试数据集为三个多样化的公开可用发票文档数据集。比较了两种处理策略：直接使用多模态能力处理图像，以及先将文档转换为Markdown再进行结构化解析。", "result": "原生图像处理通常优于结构化解析方法。模型的性能因模型类型和文档特征而异。", "conclusion": "这项基准测试为自动化文档系统选择合适的模型和处理策略提供了有价值的见解。"}}
{"id": "2509.04735", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04735", "abs": "https://arxiv.org/abs/2509.04735", "authors": ["Dharsan Ravindran", "Kevin Wang", "Zhuoyuan Cao", "Saleh Abdelrahman", "Jeffery Wu"], "title": "Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization", "comment": null, "summary": "Recent advances in vision foundation models, such as the Segment Anything\nModel (SAM) and its successor SAM2, have achieved state-of-the-art performance\non general image segmentation benchmarks. However, these models struggle in\nadverse weather conditions where visual ambiguity is high, largely due to their\nlack of uncertainty quantification. Inspired by progress in medical imaging,\nwhere uncertainty-aware training has improved reliability in ambiguous cases,\nwe investigate two approaches to enhance segmentation robustness for autonomous\ndriving. First, we introduce a multi-step finetuning procedure for SAM2 that\nincorporates uncertainty metrics directly into the loss function, improving\noverall scene recognition. Second, we adapt the Uncertainty-Aware Adapter\n(UAT), originally designed for medical image segmentation, to driving contexts.\nWe evaluate both methods on CamVid, BDD100K, and GTA driving datasets.\nExperiments show that UAT-SAM outperforms standard SAM in extreme weather,\nwhile SAM2 with uncertainty-aware loss achieves improved performance across\ndiverse driving scenes. These findings underscore the value of explicit\nuncertainty modeling for safety-critical autonomous driving in challenging\nenvironments.", "AI": {"tldr": "本文通过引入不确定性量化训练，提高了视觉基础模型（如SAM2）在恶劣天气下自动驾驶场景的分割鲁棒性。", "motivation": "尽管SAM和SAM2等视觉基础模型在通用图像分割方面表现出色，但它们在视觉模糊性高的恶劣天气条件下表现不佳，主要原因在于缺乏不确定性量化。受医学成像领域不确定性感知训练能提高模糊情况可靠性的启发，研究旨在提升自动驾驶的分割鲁棒性。", "method": "研究了两种方法：1. 对SAM2进行多步微调，将不确定性指标直接纳入损失函数，以改善整体场景识别。2. 将原用于医学图像分割的“不确定性感知适配器”（UAT）应用于驾驶场景。两种方法均在CamVid、BDD100K和GTA驾驶数据集上进行了评估。", "result": "实验表明，UAT-SAM在极端天气下优于标准SAM，而引入不确定性感知损失的SAM2在各种驾驶场景中均实现了性能提升。", "conclusion": "这些发现强调了在挑战性环境中为安全关键型自动驾驶进行显式不确定性建模的价值。"}}
{"id": "2509.05007", "categories": ["cs.AI", "cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.05007", "abs": "https://arxiv.org/abs/2509.05007", "authors": ["Jie Chen", "Jinhao Jiang", "Yingqian Min", "Zican Dong", "Shijie Wang", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "Sticker-TTS: Learn to Utilize Historical Experience with a Sticker-driven Test-Time Scaling Framework", "comment": "11 pages, 1 figures, 5 tables", "summary": "Large reasoning models (LRMs) have exhibited strong performance on complex\nreasoning tasks, with further gains achievable through increased computational\nbudgets at inference. However, current test-time scaling methods predominantly\nrely on redundant sampling, ignoring the historical experience utilization,\nthereby limiting computational efficiency. To overcome this limitation, we\npropose Sticker-TTS, a novel test-time scaling framework that coordinates three\ncollaborative LRMs to iteratively explore and refine solutions guided by\nhistorical attempts. At the core of our framework are distilled key\nconditions-termed stickers-which drive the extraction, refinement, and reuse of\ncritical information across multiple rounds of reasoning. To further enhance\nthe efficiency and performance of our framework, we introduce a two-stage\noptimization strategy that combines imitation learning with self-improvement,\nenabling progressive refinement. Extensive evaluations on three challenging\nmathematical reasoning benchmarks, including AIME-24, AIME-25, and OlymMATH,\ndemonstrate that Sticker-TTS consistently surpasses strong baselines, including\nself-consistency and advanced reinforcement learning approaches, under\ncomparable inference budgets. These results highlight the effectiveness of\nsticker-guided historical experience utilization. Our code and data are\navailable at https://github.com/RUCAIBox/Sticker-TTS.", "AI": {"tldr": "Sticker-TTS是一个新的测试时扩展框架，它利用历史经验（称为“贴纸”）来指导三个大型推理模型（LRMs）迭代探索和完善解决方案，从而显著提高了复杂推理任务的计算效率和性能。", "motivation": "当前的测试时扩展方法主要依赖冗余采样，忽略了历史经验的利用，这限制了计算效率。因此，研究的动机是克服这一局限性，提高LRMs在推理时的计算效率和性能。", "method": "Sticker-TTS框架协调三个协作的LRMs，通过历史尝试的指导迭代地探索和完善解决方案。其核心是“贴纸”（从历史尝试中提炼的关键条件），用于在多轮推理中提取、精炼和重用关键信息。为了进一步提高效率和性能，引入了两阶段优化策略，结合了模仿学习和自我改进。", "result": "在AIME-24、AIME-25和OlymMATH三个具有挑战性的数学推理基准测试中，Sticker-TTS在可比的推理预算下，持续超越了包括自洽性和先进强化学习方法在内的强大基线。", "conclusion": "这些结果突出了“贴纸”引导的历史经验利用在提高大型推理模型性能和效率方面的有效性。"}}
{"id": "2509.04470", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04470", "abs": "https://arxiv.org/abs/2509.04470", "authors": ["Swarnadeep Bhar", "Omar Naim", "Eleni Metheniti", "Bastien Navarri", "Loïc Cabannes", "Morteza Ezzabady", "Nicholas Asher"], "title": "COCORELI: Cooperative, Compositional Reconstitution \\& Execution of Language Instructions", "comment": "18 pages", "summary": "We present COCORELI, a hybrid agent framework designed to tackle the\nlimitations of large language models (LLMs) in tasks requiring: following\ncomplex instructions, minimizing hallucination, and spatial reasoning. COCORELI\nintegrates medium-sized LLM agents with novel abstraction mechanisms and a\ndiscourse module to parse instructions to in-context learn dynamic, high-level\nrepresentations of the environment. Experiments on natural collaborative\nconstruction tasks show that COCORELI outperforms single-LLM CoT and agentic\nLLM systems, all using larger LLMs. It manages to largely avoid hallucinations,\nidentify missing information, ask for clarifications, and update its learned\nobjects. COCORELI's abstraction abilities extend beyond ENVIRONMENT, as shown\nin the ToolBench API completion task.", "AI": {"tldr": "COCORELI是一个混合代理框架，通过结合中型LLM代理、新颖的抽象机制和话语模块，有效解决了大型语言模型在处理复杂指令、减少幻觉和进行空间推理方面的局限性。", "motivation": "大型语言模型（LLMs）在需要遵循复杂指令、最小化幻觉和进行空间推理的任务中存在局限性，这促使研究者开发COCORELI以克服这些挑战。", "method": "COCORELI是一个混合代理框架，它集成了中型LLM代理、新颖的抽象机制以及一个话语模块。该框架通过上下文学习（in-context learn）动态的、高层次的环境表示来解析指令。", "result": "在自然协作构建任务中，COCORELI表现优于使用更大LLM的单一LLM CoT和代理LLM系统。它能很大程度上避免幻觉、识别缺失信息、请求澄清并更新其学习到的对象。此外，COCORELI的抽象能力不仅限于环境，在ToolBench API完成任务中也得到了验证。", "conclusion": "COCORELI框架成功地提高了LLM代理在处理复杂指令、减少幻觉和进行空间推理方面的能力，并通过其独特的抽象机制和话语模块实现了优异的性能和泛化能力。"}}
{"id": "2509.04736", "categories": ["cs.CV", "I.2.10; H.5.2"], "pdf": "https://arxiv.org/pdf/2509.04736", "abs": "https://arxiv.org/abs/2509.04736", "authors": ["Taeyoung Yeon", "Vasco Xu", "Henry Hoffmann", "Karan Ahuja"], "title": "WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches", "comment": "8 pages, 4 figures, ICMI '25 (27th International Conference on\n  Multimodal Interaction), October 13-17, 2025, Canberra, ACT, Australia", "summary": "Despite advances in practical and multimodal fine-grained Human Activity\nRecognition (HAR), a system that runs entirely on smartwatches in unconstrained\nenvironments remains elusive. We present WatchHAR, an audio and inertial-based\nHAR system that operates fully on smartwatches, addressing privacy and latency\nissues associated with external data processing. By optimizing each component\nof the pipeline, WatchHAR achieves compounding performance gains. We introduce\na novel architecture that unifies sensor data preprocessing and inference into\nan end-to-end trainable module, achieving 5x faster processing while\nmaintaining over 90% accuracy across more than 25 activity classes. WatchHAR\noutperforms state-of-the-art models for event detection and activity\nclassification while running directly on the smartwatch, achieving 9.3 ms\nprocessing time for activity event detection and 11.8 ms for multimodal\nactivity classification. This research advances on-device activity recognition,\nrealizing smartwatches' potential as standalone, privacy-aware, and\nminimally-invasive continuous activity tracking devices.", "AI": {"tldr": "WatchHAR是一个基于智能手表的音频和惯性传感器的人类活动识别（HAR）系统，它完全在设备上运行，解决了隐私和延迟问题，并实现了高性能。", "motivation": "尽管细粒度HAR取得了进展，但完全在智能手表上、在无约束环境下运行的系统仍然难以实现，现有方案存在外部数据处理带来的隐私和延迟问题。", "method": "WatchHAR利用音频和惯性传感器数据，优化了管道中的每个组件，并引入了一种新颖的架构，将传感器数据预处理和推理统一到一个端到端可训练的模块中，以实现更快的处理速度和高准确性。", "result": "WatchHAR实现了5倍更快的处理速度，在超过25种活动类别中保持了90%以上的准确率。它在智能手表上直接运行时，在事件检测和活动分类方面均优于最先进的模型，活动事件检测处理时间为9.3毫秒，多模态活动分类处理时间为11.8毫秒。", "conclusion": "这项研究推动了设备上活动识别技术的发展，实现了智能手表作为独立的、隐私保护的、微创的连续活动跟踪设备的潜力。"}}
{"id": "2509.05072", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05072", "abs": "https://arxiv.org/abs/2509.05072", "authors": ["Nir Sweed", "Hanit Hakim", "Ben Wolfson", "Hila Lifshitz", "Dafna Shahaf"], "title": "Finding your MUSE: Mining Unexpected Solutions Engine", "comment": null, "summary": "Innovators often exhibit cognitive fixation on existing solutions or nascent\nideas, hindering the exploration of novel alternatives. This paper introduces a\nmethodology for constructing Functional Concept Graphs (FCGs), interconnected\nrepresentations of functional elements that support abstraction, problem\nreframing, and analogical inspiration. Our approach yields large-scale,\nhigh-quality FCGs with explicit abstraction relations, overcoming limitations\nof prior work. We further present MUSE, an algorithm leveraging FCGs to\ngenerate creative inspirations for a given problem. We demonstrate our method\nby computing an FCG on 500K patents, which we release for further research.", "AI": {"tldr": "本文提出了一种构建功能概念图（FCG）的方法，并开发了MUSE算法，利用FCG为给定问题生成创新灵感，以克服创新者认知固着的问题。", "motivation": "创新者常对现有解决方案或初期想法存在认知固着，阻碍了新颖替代方案的探索。", "method": "引入了功能概念图（FCG）的构建方法，FCG是相互关联的功能元素表示，支持抽象、问题重构和类比启发。该方法生成了具有显式抽象关系的大规模、高质量FCG，克服了以往工作的局限。进一步提出了MUSE算法，利用FCG为给定问题生成创新灵感。通过在50万份专利上计算FCG来展示该方法，并发布了该FCG。", "result": "成功构建了大规模、高质量且具有显式抽象关系的功能概念图（FCG），克服了现有方法的局限。MUSE算法能够为给定问题生成创造性灵感。在50万份专利上计算并发布了一个FCG，可供进一步研究。", "conclusion": "功能概念图（FCG）及其MUSE算法能够有效帮助创新者克服认知固着，促进新颖解决方案的探索，并能生成创造性灵感。发布的FCG是一个有价值的研究资源。"}}
{"id": "2509.04471", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04471", "abs": "https://arxiv.org/abs/2509.04471", "authors": ["Alice Schiavone", "Marco Fraccaro", "Lea Marie Pehrson", "Silvia Ingala", "Rasmus Bonnevie", "Michael Bachmann Nielsen", "Vincent Beliveau", "Melanie Ganz", "Desmond Elliott"], "title": "MOSAIC: A Multilingual, Taxonomy-Agnostic, and Computationally Efficient Approach for Radiological Report Classification", "comment": "8 pages, 14 pages including references and appendix. 9 figures.\n  Preprint", "summary": "Radiology reports contain rich clinical information that can be used to train\nimaging models without relying on costly manual annotation. However, existing\napproaches face critical limitations: rule-based methods struggle with\nlinguistic variability, supervised models require large annotated datasets, and\nrecent LLM-based systems depend on closed-source or resource-intensive models\nthat are unsuitable for clinical use. Moreover, current solutions are largely\nrestricted to English and single-modality, single-taxonomy datasets. We\nintroduce MOSAIC, a multilingual, taxonomy-agnostic, and computationally\nefficient approach for radiological report classification. Built on a compact\nopen-access language model (MedGemma-4B), MOSAIC supports both zero-/few-shot\nprompting and lightweight fine-tuning, enabling deployment on consumer-grade\nGPUs. We evaluate MOSAIC across seven datasets in English, Spanish, French, and\nDanish, spanning multiple imaging modalities and label taxonomies. The model\nachieves a mean macro F1 score of 88 across five chest X-ray datasets,\napproaching or exceeding expert-level performance, while requiring only 24 GB\nof GPU memory. With data augmentation, as few as 80 annotated samples are\nsufficient to reach a weighted F1 score of 82 on Danish reports, compared to 86\nwith the full 1600-sample training set. MOSAIC offers a practical alternative\nto large or proprietary LLMs in clinical settings. Code and models are\nopen-source. We invite the community to evaluate and extend MOSAIC on new\nlanguages, taxonomies, and modalities.", "AI": {"tldr": "MOSAIC是一种多语言、与分类法无关且计算高效的放射学报告分类方法，基于紧凑的开源语言模型，在多语言、多模态数据集上表现出色，为临床环境中的大型或专有LLM提供了实用替代方案。", "motivation": "现有放射学报告分析方法面临挑战：基于规则的方法难以处理语言变异性；监督模型需要大量标注数据；近期基于LLM的系统依赖闭源或资源密集型模型，不适合临床使用；且大多限于英语和单一模态/分类法。", "method": "引入MOSAIC，一种多语言、与分类法无关且计算高效的方法。它基于紧凑的开源语言模型（MedGemma-4B），支持零/少样本提示和轻量级微调，可在消费级GPU上部署。", "result": "MOSAIC在英语、西班牙语、法语和丹麦语的七个多模态和多标签分类法数据集上进行了评估。在五个胸部X光数据集上平均宏观F1得分达到88，接近或超过专家水平，仅需24GB GPU内存。通过数据增强，仅80个标注样本即可在丹麦语报告上达到82的加权F1分数（完整训练集为86）。", "conclusion": "MOSAIC为临床环境中大型或专有LLM提供了一个实用且可部署的替代方案。该模型和代码均为开源，鼓励社区评估和扩展其在新的语言、分类法和模态上的应用。"}}
{"id": "2509.04757", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04757", "abs": "https://arxiv.org/abs/2509.04757", "authors": ["Zhangding Liu", "Neda Mohammadi", "John E. Taylor"], "title": "MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery", "comment": "34 pages, 7 figures", "summary": "Rapid and accurate post-hurricane damage assessment is vital for disaster\nresponse and recovery. Yet existing CNN-based methods struggle to capture\nmulti-scale spatial features and to distinguish visually similar or\nco-occurring damage types. To address these issues, we propose MCANet, a\nmulti-label classification framework that learns multi-scale representations\nand adaptively attends to spatially relevant regions for each damage category.\nMCANet employs a Res2Net-based hierarchical backbone to enrich spatial context\nacross scales and a multi-head class-specific residual attention module to\nenhance discrimination. Each attention branch focuses on different spatial\ngranularities, balancing local detail with global context. We evaluate MCANet\non the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael.\nMCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet,\nRes2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads,\nperformance further improves to 92.35%, boosting average precision for\nchallenging classes such as Road Blocked by over 6%. Class activation mapping\nconfirms MCANet's ability to localize damage-relevant regions, supporting\ninterpretability. Outputs from MCANet can inform post-disaster risk mapping,\nemergency routing, and digital twin-based disaster response. Future work could\nintegrate disaster-specific knowledge graphs and multimodal large language\nmodels to improve adaptability to unseen disasters and enrich semantic\nunderstanding for real-world decision-making.", "AI": {"tldr": "MCANet是一个多标签分类框架，旨在通过学习多尺度表示和自适应关注空间相关区域，解决现有CNN方法在飓风后损害评估中难以捕捉多尺度特征和区分相似损害类型的问题。", "motivation": "飓风后快速准确的损害评估对于灾害响应和恢复至关重要。然而，现有的基于CNN的方法难以捕捉多尺度空间特征，并且难以区分视觉上相似或同时发生的损害类型。", "method": "该论文提出了MCANet框架。它采用基于Res2Net的层次化骨干网络来丰富跨尺度的空间上下文，并使用多头类别特定的残差注意力模块来增强区分能力。每个注意力分支专注于不同的空间粒度，平衡局部细节和全局上下文。MCANet在Hurricane Michael后的4,494张无人机图像组成的RescueNet数据集上进行了评估。", "result": "MCANet在RescueNet数据集上实现了91.75%的平均精度均值（mAP），优于ResNet、Res2Net、VGG、MobileNet、EfficientNet和ViT等模型。使用八个注意力头时，性能进一步提升至92.35%，使“道路堵塞”等挑战性类别的平均精度提高了6%以上。类别激活映射证实了MCANet定位损害相关区域的能力，支持了可解释性。", "conclusion": "MCANet有效解决了多尺度特征捕获和相似损害类型区分的挑战，其输出可为灾后风险测绘、紧急路线规划和基于数字孪生的灾害响应提供信息。未来的工作可以整合灾害特定知识图谱和多模态大型语言模型，以提高对未知灾害的适应性和丰富语义理解，从而支持实际决策。"}}
{"id": "2509.05091", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05091", "abs": "https://arxiv.org/abs/2509.05091", "authors": ["Matteo Bortoletto", "Yichao Zhou", "Lance Ying", "Tianmin Shu", "Andreas Bulling"], "title": "ProToM: Promoting Prosocial Behaviour via Theory of Mind-Informed Feedback", "comment": "Website at https://www.matteobortoletto.org/protom/", "summary": "While humans are inherently social creatures, the challenge of identifying\nwhen and how to assist and collaborate with others - particularly when pursuing\nindependent goals - can hinder cooperation. To address this challenge, we aim\nto develop an AI system that provides useful feedback to promote prosocial\nbehaviour - actions that benefit others, even when not directly aligned with\none's own goals. We introduce ProToM, a Theory of Mind-informed facilitator\nthat promotes prosocial actions in multi-agent systems by providing targeted,\ncontext-sensitive feedback to individual agents. ProToM first infers agents'\ngoals using Bayesian inverse planning, then selects feedback to communicate by\nmaximising expected utility, conditioned on the inferred goal distribution. We\nevaluate our approach against baselines in two multi-agent environments: Doors,\nKeys, and Gems, as well as Overcooked. Our results suggest that\nstate-of-the-art large language and reasoning models fall short of\ncommunicating feedback that is both contextually grounded and well-timed -\nleading to higher communication overhead and task speedup. In contrast, ProToM\nprovides targeted and helpful feedback, achieving a higher success rate,\nshorter task completion times, and is consistently preferred by human users.", "AI": {"tldr": "本文提出了ProToM，一个基于心智理论的AI系统，通过提供有针对性、情境敏感的反馈，在多智能体系统中促进亲社会行为，并被证明优于现有的大语言模型。", "motivation": "人类在追求独立目标时，难以判断何时以及如何协助他人，这阻碍了合作。研究旨在开发一个AI系统，通过提供有用反馈来促进亲社会行为，即即便不直接符合自身目标，也能使他人受益的行为。", "method": "引入ProToM，一个受心智理论启发的协调器。它首先使用贝叶斯逆向规划推断智能体的目标，然后根据推断的目标分布，通过最大化预期效用选择要传达的反馈，从而提供有针对性、情境敏感的反馈。", "result": "在两个多智能体环境（Doors, Keys, and Gems 和 Overcooked）中进行评估。结果显示，最先进的大语言和推理模型在提供情境化和及时反馈方面表现不足，导致更高的通信开销和更慢的任务加速。相比之下，ProToM提供了有针对性且有益的反馈，实现了更高的成功率、更短的任务完成时间，并持续受到人类用户的青睐。", "conclusion": "ProToM通过提供有针对性且有益的反馈，有效促进了多智能体系统中的亲社会行为，其性能优于现有的大语言模型，证明了其在提升合作方面的潜力。"}}
{"id": "2509.04472", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04472", "abs": "https://arxiv.org/abs/2509.04472", "authors": ["Kushan Mitra", "Dan Zhang", "Hannah Kim", "Estevam Hruschka"], "title": "RECAP: REwriting Conversations for Intent Understanding in Agentic Planning", "comment": null, "summary": "Understanding user intent is essential for effective planning in\nconversational assistants, particularly those powered by large language models\n(LLMs) coordinating multiple agents. However, real-world dialogues are often\nambiguous, underspecified, or dynamic, making intent detection a persistent\nchallenge. Traditional classification-based approaches struggle to generalize\nin open-ended settings, leading to brittle interpretations and poor downstream\nplanning. We propose RECAP (REwriting Conversations for Agent Planning), a new\nbenchmark designed to evaluate and advance intent rewriting, reframing\nuser-agent dialogues into concise representations of user goals. RECAP captures\ndiverse challenges such as ambiguity, intent drift, vagueness, and mixed-goal\nconversations. Alongside the dataset, we introduce an LLM-based evaluator that\nassesses planning utility given the rewritten intent. Using RECAP, we develop a\nprompt-based rewriting approach that outperforms baselines. We further\ndemonstrate that fine-tuning two DPO-based rewriters yields additional utility\ngains. Our results highlight intent rewriting as a critical and tractable\ncomponent for improving agent planning in open-domain dialogue systems.", "AI": {"tldr": "本文提出了RECAP基准测试和一种意图重写方法，旨在将用户对话转化为简洁的用户目标表示，以解决会话助手（特别是基于LLM的多智能体系统）中意图检测的挑战，从而改善智能体规划。", "motivation": "在会话助手中，理解用户意图对于有效规划至关重要，但真实世界的对话常常模糊、不明确或动态，使得意图检测成为一个难题。传统的分类方法在开放式设置中难以泛化，导致解释脆弱和下游规划不佳。", "method": "研究者提出了RECAP（REwriting Conversations for Agent Planning）基准测试，用于评估和推进意图重写，将用户-智能体对话重构为用户目标的简洁表示。RECAP涵盖了歧义、意图漂移、模糊性和混合目标对话等挑战。同时，他们引入了一个基于LLM的评估器，根据重写的意图评估规划效用。在此基础上，开发了一种基于提示的重写方法，并进一步通过微调两个基于DPO的重写器来提升效用。", "result": "基于提示的重写方法优于基线模型。通过微调两个基于DPO的重写器，获得了额外的效用提升。研究结果表明，意图重写是改进开放域对话系统中智能体规划的关键且可行的组成部分。", "conclusion": "意图重写是改进开放域对话系统中智能体规划的一个关键且可行的组成部分。通过RECAP基准测试和有效的重写方法，可以显著提升会话助手的规划能力。"}}
{"id": "2509.04758", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04758", "abs": "https://arxiv.org/abs/2509.04758", "authors": ["Kaname Yokoyama", "Chihiro Nakatani", "Norimichi Ukita"], "title": "Dynamic Group Detection using VLM-augmented Temporal Groupness Graph", "comment": "10 pages, Accepted to ICCV2025", "summary": "This paper proposes dynamic human group detection in videos. For detecting\ncomplex groups, not only the local appearance features of in-group members but\nalso the global context of the scene are important. Such local and global\nappearance features in each frame are extracted using a Vision-Language Model\n(VLM) augmented for group detection in our method. For further improvement, the\ngroup structure should be consistent over time. While previous methods are\nstabilized on the assumption that groups are not changed in a video, our method\ndetects dynamically changing groups by global optimization using a graph with\nall frames' groupness probabilities estimated by our groupness-augmented CLIP\nfeatures. Our experimental results demonstrate that our method outperforms\nstate-of-the-art group detection methods on public datasets. Code:\nhttps://github.com/irajisamurai/VLM-GroupDetection.git", "AI": {"tldr": "该论文提出了一种在视频中动态检测人类群体的方法，通过增强的视觉-语言模型（VLM）提取局部和全局特征，并使用全局优化来处理群体随时间的变化，优于现有技术。", "motivation": "检测复杂群体不仅需要群内成员的局部特征，还需要场景的全局上下文。此外，群体结构应随时间保持一致，而现有方法通常假设群体在视频中不变，无法处理动态变化的群体。", "method": "该方法使用一个针对群体检测进行增强的视觉-语言模型（VLM）来提取每帧的局部和全局外观特征。通过对CLIP特征进行“群体性”增强，估算每帧的群体概率。然后，利用包含所有帧群体概率的图进行全局优化，以检测动态变化的群体。", "result": "实验结果表明，该方法在公共数据集上优于现有的最先进的群体检测方法。", "conclusion": "该论文提出了一种有效的动态人类群体检测方法，通过结合VLM提取的局部/全局特征和基于全局优化的时间一致性处理，成功解决了视频中复杂和动态群体检测的挑战。"}}
{"id": "2509.05139", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05139", "abs": "https://arxiv.org/abs/2509.05139", "authors": ["Jaime Osvaldo Salas", "Paolo Pareti", "Semih Yumuşak", "Soulmaz Gheisari", "Luis-Daniel Ibáñez", "George Konstantinidis"], "title": "Evaluation and Comparison Semantics for ODRL", "comment": "Accepted as a full paper at the 14th International Joint Conference\n  on Knowledge Graphs (IJCKG 2025). This is the submitted manuscript, the\n  accepted manuscript will be published by Springer Nature", "summary": "We consider the problem of evaluating, and comparing computational policies\nin the Open Digital Rights Language (ODRL), which has become the de facto\nstandard for governing the access and usage of digital resources. Although\npreliminary progress has been made on the formal specification of the\nlanguage's features, a comprehensive formal semantics of ODRL is still missing.\nIn this paper, we provide a simple and intuitive formal semantics for ODRL that\nis based on query answering. Our semantics refines previous formalisations, and\nis aligned with the latest published specification of the language (2.2).\nBuilding on our evaluation semantics, and motivated by data sharing scenarios,\nwe also define and study the problem of comparing two policies, detecting\nequivalent, more restrictive or more permissive policies.", "AI": {"tldr": "该论文提出了基于查询应答的ODRL形式化语义，并在此基础上定义了策略比较问题，用于评估和比较数字权利语言策略。", "motivation": "ODRL是数字资源访问和使用的事实标准，但仍缺乏全面的形式化语义。现有工作虽有初步进展，但不足以支持对计算策略的评估和比较，特别是在数据共享场景中。", "method": "本文提出了一种基于查询应答的ODRL形式化语义，该语义简单直观，并与最新的ODRL 2.2规范保持一致。在此评估语义的基础上，定义并研究了策略比较问题，包括检测等效、更严格或更宽松的策略。", "result": "成功提供了一个与ODRL 2.2规范对齐的、基于查询应答的ODRL形式化语义。在此基础上，建立了用于比较两个ODRL策略（如等效性、限制性或宽松性）的框架。", "conclusion": "该论文通过提供全面的形式化语义和策略比较框架，填补了ODRL语言在评估和比较计算策略方面的空白，特别适用于数据共享场景。"}}
{"id": "2509.04473", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04473", "abs": "https://arxiv.org/abs/2509.04473", "authors": ["Jaekwon Yoo", "Kunal Chandiramani", "Divya Tadimeti", "Abenezer Girma", "Chandra Dhir"], "title": "SpeechLLM: Unified Speech and Language Model for Enhanced Multi-Task Understanding in Low Resource Settings", "comment": null, "summary": "While integrating speech encoder with LLM requires substantial data and\nresources, use cases face limitations due to insufficient availability. To\naddress this, we propose a solution with a parameter-efficient adapter that\nconverts speech embeddings into LLM-compatible tokens, focusing on end-to-end\nautomatic speech recognition (ASR), named entity recognition (NER), and\nsentiment analysis (SA). To reduce labeling costs, we employ an LLM-based\nsynthetic dataset annotation technique. The proposed adapter, using 7x fewer\ntrainable parameters, achieves significant performance gains: a 26% relative\nWord Error Rates (WER) improvement on the LibriSpeech ASR task, a 6.3% relative\nF1 score increase on the NER task, and a 32% relative F1 score boost on the SA\ntask. Moreover, using advanced techniques such as adding a classifier\nregularizer and optimizing the LLM with Low-Rank Adaptation (LoRA) yields\nnotable performance gains, with Spoken Language Understanding Evaluation (SLUE)\nscore improvement of 6.6% and 9.5%", "AI": {"tldr": "该研究提出了一种参数高效的适配器，能将语音嵌入转换为LLM兼容的tokens，并结合LLM生成合成数据集，以解决语音编码器与LLM集成时数据和资源不足的问题。该方法在ASR、NER和SA任务上取得了显著性能提升，且训练参数减少7倍。", "motivation": "将语音编码器与大型语言模型（LLM）集成需要大量数据和资源，但实际应用中数据可用性不足，导致用例受限。", "method": "研究提出了一种参数高效的适配器，用于将语音嵌入转换为LLM兼容的tokens。为降低标注成本，采用了基于LLM的合成数据集标注技术。此外，还使用了分类器正则化和LoRA（Low-Rank Adaptation）等高级技术来优化LLM。", "result": "所提出的适配器使用的可训练参数减少了7倍，并在性能上取得了显著提升：LibriSpeech ASR任务的词错误率（WER）相对改善26%，NER任务的F1分数相对增加6.3%，SA任务的F1分数相对提升32%。结合高级技术，如分类器正则化和LoRA，使口语理解评估（SLUE）分数分别提高了6.6%和9.5%。", "conclusion": "该研究提出的参数高效适配器结合LLM生成的合成数据集，能有效且资源节约地将语音集成到LLM中，并在多个语音理解任务上取得了显著的性能提升，为解决语音-LLM集成中的数据和资源限制提供了可行方案。"}}
{"id": "2509.04772", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04772", "abs": "https://arxiv.org/abs/2509.04772", "authors": ["Zhangding Liu", "Neda Mohammadi", "John E. Taylor"], "title": "FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph", "comment": null, "summary": "Timely and accurate floodwater depth estimation is critical for road\naccessibility and emergency response. While recent computer vision methods have\nenabled flood detection, they suffer from both accuracy limitations and poor\ngeneralization due to dependence on fixed object detectors and task-specific\ntraining. To enable accurate depth estimation that can generalize across\ndiverse flood scenarios, this paper presents FloodVision, a zero-shot framework\nthat combines the semantic reasoning abilities of the foundation\nvision-language model GPT-4o with a structured domain knowledge graph. The\nknowledge graph encodes canonical real-world dimensions for common urban\nobjects including vehicles, people, and infrastructure elements to ground the\nmodel's reasoning in physical reality. FloodVision dynamically identifies\nvisible reference objects in RGB images, retrieves verified heights from the\nknowledge graph to mitigate hallucination, estimates submergence ratios, and\napplies statistical outlier filtering to compute final depth values. Evaluated\non 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean\nabsolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and\nsurpassing prior CNN-based methods. The system generalizes well across varying\nscenes and operates in near real-time, making it suitable for future\nintegration into digital twin platforms and citizen-reporting apps for smart\ncity flood resilience.", "AI": {"tldr": "本文提出FloodVision，一个结合GPT-4o视觉语言模型和结构化领域知识图谱的零样本框架，用于准确、泛化性强的洪水深度估计，其在实际图像上显著优于基线和现有方法。", "motivation": "及时准确的洪水深度估计对于道路通行和应急响应至关重要。现有计算机视觉方法在洪水检测方面存在准确性限制和泛化能力差的问题，因为它们依赖于固定对象检测器和特定任务训练。", "method": "FloodVision是一个零样本框架，它结合了基础视觉语言模型GPT-4o的语义推理能力和一个结构化的领域知识图谱。该知识图谱编码了常见城市物体（如车辆、人员、基础设施）的规范真实世界尺寸。FloodVision动态识别RGB图像中的可见参考物体，从知识图谱中检索验证过的高度以减少幻觉，估计淹没比例，并应用统计异常值过滤来计算最终深度值。", "result": "在来自MyCoast New York的110张众包图像上进行评估，FloodVision实现了8.17厘米的平均绝对误差，将GPT-4o基线的10.28厘米降低了20.5%，并超越了之前的基于CNN的方法。该系统在不同场景下表现出良好的泛化能力，并能近实时运行。", "conclusion": "FloodVision系统在不同场景下具有良好的泛化能力并能近实时运行，使其适用于未来集成到数字孪生平台和公民报告应用程序中，以增强智慧城市的洪水韧性。"}}
{"id": "2509.05263", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05263", "abs": "https://arxiv.org/abs/2509.05263", "authors": ["Yinglin Duan", "Zhengxia Zou", "Tongwei Gu", "Wei Jia", "Zhan Zhao", "Luyi Xu", "Xinzhu Liu", "Hao Jiang", "Kang Chen", "Shuang Qiu"], "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation", "comment": null, "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n$90\\times$ increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18", "AI": {"tldr": "LatticeWorld是一个高效的3D世界生成框架，它结合轻量级大型语言模型和工业级渲染引擎，通过多模态输入快速创建具有高精度物理模拟和实时渲染的大规模互动3D环境，显著提升了生产效率。", "motivation": "为了缩小模拟与现实之间的差距，需要更真实、具有精确物理特性的3D世界模型。传统手动建模效率低下，而现代机器学习方法，尤其是生成式方法，能够根据用户指令创建虚拟世界，这促使了对更高效、高质量3D环境生成技术的需求。", "method": "本文提出了LatticeWorld框架，它利用轻量级大型语言模型（如LLaMA-2-7B）和工业级渲染引擎（如Unreal Engine 5）来生成动态环境。该框架接受文本描述和视觉指令作为多模态输入，能够创建包含动态智能体、多智能体交互、高保真物理模拟和实时渲染的大规模3D互动世界。", "result": "LatticeWorld在场景布局生成和视觉保真度方面表现出卓越的准确性。与传统手动生产方法相比，它在保持高创造性质量的同时，将工业生产效率提高了90倍以上。", "conclusion": "LatticeWorld是一个简单而有效的3D世界生成框架，它通过结合先进的AI技术和工业级渲染引擎，极大地简化了3D环境的工业生产流程，实现了高效、高保真、大规模的互动3D世界生成，对具身AI、自动驾驶等领域具有重要应用价值。"}}
{"id": "2509.04474", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04474", "abs": "https://arxiv.org/abs/2509.04474", "authors": ["Shengyin Sun", "Yiming Li", "Xing Li", "Yingzhao Lian", "Weizhe Lin", "Hui-Ling Zhen", "Zhiyuan Yang", "Chen Chen", "Xianzhi Yu", "Mingxuan Yuan", "Chen Ma"], "title": "Scaling Up, Speeding Up: A Benchmark of Speculative Decoding for Efficient LLM Test-Time Scaling", "comment": "18 pages", "summary": "Test-time scaling has emerged as a powerful paradigm for enhancing the\nreasoning capabilities of large language models (LLMs) by allocating additional\ncomputational resources during inference. However, this paradigm is inherently\ninefficient due to the generation of redundant and repetitive reasoning traces,\nleading to significant computational overhead. Speculative decoding offers a\npromising avenue for mitigating this inefficiency, yet its efficacy in the\nstructured, repetition-rich context of test-time scaling remains largely\nunexplored. To bridge this gap, we introduce the first comprehensive benchmark\ndesigned to evaluate speculative decoding methods for accelerating LLM\ntest-time scaling. Our benchmark provides consistent experimental protocols\nacross representative test-time scaling paradigms (e.g., Best-of-N sampling and\nmulti-round thinking), enabling a fair comparison of three major categories of\nspeculative decoding: model-based, training-based, and n-gram-based methods.\nExtensive experiments reveal that simple n-gram-based methods effectively\ncapture repetitive patterns, demonstrating unique potential in accelerating\ntest-time scaling. This phenomenon demonstrates the value of integrating\nn-gram-based methods with model-based or training-based approaches to balance\nacceleration for both repetitive and diverse reasoning in test-time scaling. We\nhope this benchmark spurs further research on speculative decoding for\ntest-time scaling, enabling faster and more practical reasoning in LLMs through\nbetter handling of repetitive and diverse reasoning paths.", "AI": {"tldr": "本研究引入了一个基准测试，用于评估投机解码方法在加速LLM测试时间扩展方面的效果，发现简单的N-gram方法在处理重复推理时表现出色，并建议结合不同方法。", "motivation": "测试时间扩展（Test-time scaling）能增强大型语言模型（LLMs）的推理能力，但由于生成冗余和重复的推理痕迹，导致计算效率低下。投机解码（Speculative decoding）有望缓解这种低效率，但在测试时间扩展这种结构化、重复丰富的场景中，其有效性尚未被充分探索。", "method": "引入了第一个全面的基准测试，旨在评估用于加速LLM测试时间扩展的投机解码方法。该基准测试为代表性的测试时间扩展范式（如Best-of-N采样和多轮思考）提供了统一的实验协议，公平比较了三种主要的投机解码类别：基于模型、基于训练和基于N-gram的方法。", "result": "广泛的实验表明，简单的基于N-gram的方法能有效捕获重复模式，在加速测试时间扩展方面展现出独特的潜力。", "conclusion": "基于N-gram的方法可以与基于模型或基于训练的方法相结合，以平衡测试时间扩展中重复和多样化推理的加速效果。该基准测试有望推动投机解码在测试时间扩展方面的进一步研究，通过更好地处理重复和多样化推理路径，实现LLMs更快、更实用的推理。"}}
{"id": "2509.04773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04773", "abs": "https://arxiv.org/abs/2509.04773", "authors": ["Bangxiang Lan", "Ruobing Xie", "Ruixiang Zhao", "Xingwu Sun", "Zhanhui Kang", "Gang Yang", "Xirong Li"], "title": "Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval", "comment": "Accepted to ICCV2025", "summary": "The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by\ntextual queries with the same semantic meanings. Recent CLIP-based approaches\nhave explored two frameworks: Two-Tower versus Single-Tower framework, yet the\nformer suffers from low effectiveness, while the latter suffers from low\nefficiency. In this study, we explore a new Hybrid-Tower framework that can\nhybridize the advantages of the Two-Tower and Single-Tower framework, achieving\nhigh effectiveness and efficiency simultaneously. We propose a novel hybrid\nmethod, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG,\nwhich includes a new pseudo-query generator designed to generate a pseudo-query\nfor each video. This enables the video feature and the textual features of\npseudo-query to interact in a fine-grained manner, similar to the Single-Tower\napproaches to hold high effectiveness, even before the real textual query is\nreceived. Simultaneously, our method introduces no additional storage or\ncomputational overhead compared to the Two-Tower framework during the inference\nstage, thus maintaining high efficiency. Extensive experiments on five commonly\nused text-video retrieval benchmarks demonstrate that our method achieves a\nsignificant improvement over the baseline, with an increase of $1.6\\% \\sim\n3.9\\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower\nmodels while achieving near state-of-the-art performance, highlighting the\nadvantages of the Hybrid-Tower framework.", "AI": {"tldr": "本文提出了一种名为 PIG 的混合塔（Hybrid-Tower）框架，用于文本到视频检索（T2VR），通过生成伪查询实现视频特征与文本特征的细粒度交互，从而同时提高检索的有效性和效率。", "motivation": "现有的基于 CLIP 的文本到视频检索方法在双塔（Two-Tower）框架下效率高但效果差，而单塔（Single-Tower）框架下效果好但效率低，缺乏一种能兼顾两者优点的方案。", "method": "本文提出了一种新的混合塔框架和一种名为 PIG（Fine-grained Pseudo-query Interaction and Generation）的混合方法。PIG 包含一个伪查询生成器，为每个视频生成伪查询，使其视频特征能与伪查询的文本特征进行细粒度交互，从而在未接收到真实查询前就达到高有效性。同时，该方法在推理阶段不增加额外的存储或计算开销，保持了高效率。", "result": "在五个常用的文本-视频检索基准测试中，该方法比基线模型显著提高了 1.6% 到 3.9% 的 R@1 指标。此外，该方法在保持与双塔模型相同效率的同时，达到了接近最先进的性能。", "conclusion": "混合塔框架及其 PIG 方法成功地结合了双塔框架的效率和单塔框架的有效性，为文本到视频检索任务提供了一个高性能且高效率的解决方案。"}}
{"id": "2303.06298", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2303.06298", "abs": "https://arxiv.org/abs/2303.06298", "authors": ["Samir Mitha", "Seungho Choe", "Pejman Jahbedar Maralani", "Alan R. Moody", "April Khademi"], "title": "MLP-SRGAN: A Single-Dimension Super Resolution GAN using MLP-Mixer", "comment": "14 pages, 10 figures", "summary": "We propose a novel architecture called MLP-SRGAN, which is a single-dimension\nSuper Resolution Generative Adversarial Network (SRGAN) that utilizes\nMulti-Layer Perceptron Mixers (MLP-Mixers) along with convolutional layers to\nupsample in the slice direction. MLP-SRGAN is trained and validated using high\nresolution (HR) FLAIR MRI from the MSSEG2 challenge dataset. The method was\napplied to three multicentre FLAIR datasets (CAIN, ADNI, CCNA) of images with\nlow spatial resolution in the slice dimension to examine performance on\nheld-out (unseen) clinical data. Upsampled results are compared to several\nstate-of-the-art SR networks. For images with high resolution (HR) ground\ntruths, peak-signal-to-noise-ratio (PSNR) and structural similarity index\n(SSIM) are used to measure upsampling performance. Several new structural,\nno-reference image quality metrics were proposed to quantify sharpness (edge\nstrength), noise (entropy), and blurriness (low frequency information) in the\nabsence of ground truths. Results show MLP-SRGAN results in sharper edges, less\nblurring, preserves more texture and fine-anatomical detail, with fewer\nparameters, faster training/evaluation time, and smaller model size than\nexisting methods. Code for MLP-SRGAN training and inference, data generators,\nmodels and no-reference image quality metrics will be available at\nhttps://github.com/IAMLAB-Ryerson/MLP-SRGAN.", "AI": {"tldr": "本文提出了一种名为MLP-SRGAN的新型单维度超分辨率生成对抗网络架构，它结合了多层感知器混频器（MLP-Mixer）和卷积层，用于切片方向的图像上采样，并在MRI图像超分辨率任务中表现出优于现有方法的性能和效率。", "motivation": "现有超分辨率网络在MRI图像的单维度（切片方向）上采样方面可能存在局限性，导致边缘不清晰、细节丢失或模型效率低下。研究旨在开发一种更有效的架构，以改善低空间分辨率MRI图像的质量，尤其是在缺乏高分辨率真实图像的情况下。", "method": "研究提出MLP-SRGAN，一个结合MLP-Mixer和卷积层的单维度SRGAN，用于切片方向的上采样。该模型使用MSSEG2挑战数据集中的高分辨率FLAIR MRI进行训练和验证。它被应用于CAIN、ADNI、CCNA三个多中心FLAIR数据集，以评估其在未见临床数据上的性能。性能评估使用PSNR和SSIM（有真实图像时），并提出了新的无参考图像质量指标（用于量化清晰度、噪声和模糊度，在无真实图像时）进行比较。", "result": "MLP-SRGAN生成的结果具有更锐利的边缘、更少的模糊、保留了更多的纹理和精细解剖细节。与现有方法相比，它拥有更少的参数、更快的训练/评估时间以及更小的模型尺寸。", "conclusion": "MLP-SRGAN是一种高效且高性能的单维度超分辨率方法，特别适用于MRI图像的切片方向上采样，它在提高图像质量（如锐度、细节保留）和操作效率（更少的参数、更快的速度、更小的模型）方面优于现有技术。"}}
{"id": "2509.04475", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04475", "abs": "https://arxiv.org/abs/2509.04475", "authors": ["Hao Wen", "Yifan Su", "Feifei Zhang", "Yunxin Liu", "Yunhao Liu", "Ya-Qin Zhang", "Yuanchun Li"], "title": "ParaThinker: Native Parallel Thinking as a New Paradigm to Scale LLM Test-time Compute", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have been driven by test-time\ncompute scaling - a strategy that improves reasoning by generating longer,\nsequential thought processes. While effective, this approach encounters a\nsignificant bottleneck as computation increases, where further computation\noffers only marginal performance gains. We argue this ceiling is not an\ninherent limit of the model's capability but a flaw in the scaling strategy\nitself, a phenomenon we term \"Tunnel Vision\", where a model's imperfect initial\nsteps lock it into a suboptimal reasoning path. To overcome this, we introduce\na new scaling paradigm: native thought parallelism. We present ParaThinker, an\nend-to-end framework that trains an LLM to generate multiple, diverse reasoning\npaths in parallel and synthesize them into a superior final answer. By\nexploring different lines of thoughts simultaneously, ParaThinker effectively\nsidesteps the Tunnel Vision issue and unlocks the model's latent reasoning\npotential. Our approach demonstrates that scaling compute in parallel (width)\nis a more effective and efficient way to superior reasoning than simply scaling\nsequentially (depth). On challenging reasoning benchmarks, ParaThinker achieves\nsubstantial accuracy improvements over sequential LLMs (12.3% for 1.5B and 7.5%\nfor 7B models on average with 8 parallel paths), while adding only negligible\nlatency overhead (7.1%). This enables smaller models to surpass much larger\ncounterparts and establishes parallel thinking as a critical, efficient\ndimension for scaling future LLMs.", "AI": {"tldr": "该论文提出了ParaThinker，一个通过并行生成和综合多个多样化推理路径来训练大型语言模型（LLM）的框架，以克服传统顺序推理的“隧道视野”问题，从而实现更高效、更优质的推理。", "motivation": "当前大型语言模型（LLM）的推理能力提升主要依赖于测试时计算扩展，即生成更长的顺序思维过程。然而，这种策略在计算量增加时会遇到瓶颈，即进一步的计算只能带来微小的性能提升。作者认为这不是模型能力的固有局限，而是扩展策略本身的缺陷，称之为“隧道视野”，即模型不完美的初始步骤会将其锁定在次优的推理路径中。", "method": "引入了一种新的扩展范式：原生思维并行化。提出了ParaThinker，一个端到端框架，通过训练LLM并行生成多个多样化的推理路径，并将其综合成一个更优的最终答案。通过同时探索不同的思路，ParaThinker有效地规避了“隧道视野”问题。", "result": "在具有挑战性的推理基准测试中，ParaThinker比顺序LLM取得了显著的准确性提升（对于1.5B模型平均提高12.3%，对于7B模型平均提高7.5%，均使用8条并行路径），同时只增加了可忽略的延迟开销（7.1%）。这使得较小的模型能够超越大得多的模型。", "conclusion": "并行思维是比简单顺序扩展（深度）更有效、更高效的卓越推理方式，它能够解锁模型的潜在推理能力，并确立了并行思维作为未来LLM扩展的关键、高效维度。"}}
{"id": "2509.04775", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04775", "abs": "https://arxiv.org/abs/2509.04775", "authors": ["R. Makharia", "J. G. Singla", "Amitabh", "N. Dube", "H. Sharma"], "title": "Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data", "comment": "27 pages, 11 figures, 3 tables", "summary": "Accurate image registration is critical for lunar exploration, enabling\nsurface mapping, resource localization, and mission planning. Aligning data\nfrom diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera,\nNarrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer),\nand radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya\nmission) -- is challenging due to differences in resolution, illumination, and\nsensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT,\nAKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using\ncross-modality image pairs from equatorial and polar regions. A preprocessing\npipeline is proposed, including georeferencing, resolution alignment, intensity\nnormalization, and enhancements like adaptive histogram equalization, principal\ncomponent analysis, and shadow correction. SuperGlue consistently yields the\nlowest root mean square error and fastest runtimes. Classical methods such as\nSIFT and AKAZE perform well near the equator but degrade under polar lighting.\nThe results highlight the importance of preprocessing and learning-based\napproaches for robust lunar image registration across diverse conditions.", "AI": {"tldr": "该研究评估了多种特征匹配算法在月球跨模态图像配准中的性能，发现SuperGlue表现最佳，并强调了预处理和学习方法的重要性。", "motivation": "月球探测需要精确的图像配准来实现表面测绘、资源定位和任务规划。然而，来自不同月球传感器（光学、高光谱、雷达）的数据因分辨率、光照和传感器畸变差异，导致数据对齐极具挑战性。", "method": "研究评估了五种特征匹配算法：SIFT、ASIFT、AKAZE、RIFT2和SuperGlue（一种基于深度学习的匹配器），使用了来自赤道和极地地区的跨模态图像对。同时，提出了一套预处理流程，包括地理配准、分辨率对齐、强度归一化以及自适应直方图均衡化、主成分分析和阴影校正等增强技术。", "result": "SuperGlue持续产生最低的均方根误差和最快的运行时间。SIFT和AKAZE等经典方法在赤道附近表现良好，但在极地光照条件下性能下降。", "conclusion": "研究结果强调了预处理和基于学习的方法对于在不同条件下实现鲁棒月球图像配准的重要性。"}}
{"id": "2509.04476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04476", "abs": "https://arxiv.org/abs/2509.04476", "authors": ["Seojin Kim", "Hyeontae Song", "Jaehyun Nam", "Jinwoo Shin"], "title": "Training Text-to-Molecule Models with Context-Aware Tokenization", "comment": "EMNLP 2025 Findings", "summary": "Recently, text-to-molecule models have shown great potential across various\nchemical applications, e.g., drug-discovery. These models adapt language models\nto molecular data by representing molecules as sequences of atoms. However,\nthey rely on atom-level tokenizations, which primarily focus on modeling local\nconnectivity, thereby limiting the ability of models to capture the global\nstructural context within molecules. To tackle this issue, we propose a novel\ntext-to-molecule model, coined Context-Aware Molecular T5 (CAMT5). Inspired by\nthe significance of the substructure-level contexts in understanding molecule\nstructures, e.g., ring systems, we introduce substructure-level tokenization\nfor text-to-molecule models. Building on our tokenization scheme, we develop an\nimportance-based training strategy that prioritizes key substructures, enabling\nCAMT5 to better capture the molecular semantics. Extensive experiments verify\nthe superiority of CAMT5 in various text-to-molecule generation tasks.\nIntriguingly, we find that CAMT5 outperforms the state-of-the-art methods using\nonly 2% of training tokens. In addition, we propose a simple yet effective\nensemble strategy that aggregates the outputs of text-to-molecule models to\nfurther boost the generation performance. Code is available at\nhttps://github.com/Songhyeontae/CAMT5.git.", "AI": {"tldr": "现有文本到分子模型因原子级分词而缺乏全局结构上下文。CAMT5通过引入子结构级分词和基于重要性的训练策略，有效捕捉分子语义，在文本到分子生成任务中表现优异，且训练效率更高。", "motivation": "当前的文本到分子模型依赖原子级分词，主要关注局部连接性，限制了模型捕捉分子内全局结构上下文的能力。", "method": "本文提出了上下文感知分子T5 (CAMT5) 模型。核心方法包括：1) 引入子结构级分词（如环系统）来理解分子结构；2) 开发一种基于重要性的训练策略，优先处理关键子结构；3) 提出一种简单有效的集成策略，聚合多个文本到分子模型的输出以进一步提升性能。", "result": "CAMT5在各种文本到分子生成任务中均优于现有最先进的方法。值得注意的是，CAMT5仅使用2%的训练token就超越了SOTA方法。此外，提出的集成策略能进一步提高生成性能。", "conclusion": "通过子结构级分词和基于重要性的训练策略，CAMT5显著提升了文本到分子模型捕捉全局分子语义的能力，实现了卓越的生成性能和更高的训练效率。"}}
{"id": "2509.04478", "categories": ["cs.CL", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.04478", "abs": "https://arxiv.org/abs/2509.04478", "authors": ["Iniakpokeikiye Peter Thompson", "Yi Dewei", "Reiter Ehud"], "title": "An End-to-End System for Culturally-Attuned Driving Feedback using a Dual-Component NLG Engine", "comment": "The paper has 5 figures and 1 table", "summary": "This paper presents an end-to-end mobile system that delivers\nculturally-attuned safe driving feedback to drivers in Nigeria, a low-resource\nenvironment with significant infrastructural challenges. The core of the system\nis a novel dual-component Natural Language Generation (NLG) engine that\nprovides both legally-grounded safety tips and persuasive, theory-driven\nbehavioural reports. We describe the complete system architecture, including an\nautomatic trip detection service, on-device behaviour analysis, and a\nsophisticated NLG pipeline that leverages a two-step reflection process to\nensure high-quality feedback. The system also integrates a specialized machine\nlearning model for detecting alcohol-influenced driving, a key local safety\nissue. The architecture is engineered for robustness against intermittent\nconnectivity and noisy sensor data. A pilot deployment with 90 drivers\ndemonstrates the viability of our approach, and initial results on detected\nunsafe behaviours are presented. This work provides a framework for applying\ndata-to-text and AI systems to achieve social good.", "AI": {"tldr": "本文提出了一个针对尼日利亚低资源环境的端到端移动系统，通过新颖的双组件自然语言生成（NLG）引擎和机器学习模型，提供文化适应的安全驾驶反馈。", "motivation": "在尼日利亚等基础设施挑战显著的低资源环境中，需要为驾驶员提供安全驾驶反馈，并解决酒精影响驾驶等当地关键安全问题。", "method": "开发了一个端到端移动系统，其核心是一个双组件NLG引擎，提供基于法律的安全提示和理论驱动的行为报告。系统架构包括自动行程检测服务、设备端行为分析、利用两步反射过程的高质量NLG管道，以及一个专门用于检测酒精影响驾驶的机器学习模型。该架构还针对间歇性连接和嘈杂传感器数据进行了鲁棒性设计。", "result": "对90名司机进行的试点部署证明了该方法的可行性，并展示了检测到的不安全行为的初步结果。", "conclusion": "这项工作为应用数据到文本和人工智能系统以实现社会效益提供了一个框架。"}}
{"id": "2509.04800", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04800", "abs": "https://arxiv.org/abs/2509.04800", "authors": ["Asif Newaz", "Masum Mushfiq Ishti", "A Z M Ashraful Azam", "Asif Ur Rahman Adib"], "title": "Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images", "comment": "Under Review in ICSigSys 2025", "summary": "Skin diseases are among the most prevalent health concerns worldwide, yet\nconventional diagnostic methods are often costly, complex, and unavailable in\nlow-resource settings. Automated classification using deep learning has emerged\nas a promising alternative, but existing studies are mostly limited to\ndermoscopic datasets and a narrow range of disease classes. In this work, we\ncurate a large dataset of over 50 skin disease categories captured with mobile\ndevices, making it more representative of real-world conditions. We evaluate\nmultiple convolutional neural networks and Transformer-based architectures,\ndemonstrating that Transformer models, particularly the Swin Transformer,\nachieve superior performance by effectively capturing global contextual\nfeatures. To enhance interpretability, we incorporate Gradient-weighted Class\nActivation Mapping (Grad-CAM), which highlights clinically relevant regions and\nprovides transparency in model predictions. Our results underscore the\npotential of Transformer-based approaches for mobile-acquired skin lesion\nclassification, paving the way toward accessible AI-assisted dermatological\nscreening and early diagnosis in resource-limited environments.", "AI": {"tldr": "本研究通过手机采集的图像构建了一个包含50多种皮肤病的大型数据集，并发现Transformer模型（特别是Swin Transformer）在皮肤病分类上优于CNN，结合Grad-CAM提升了可解释性，为资源匮乏地区的AI辅助皮肤病诊断提供了新途径。", "motivation": "传统的皮肤病诊断方法成本高、复杂且在资源匮乏地区难以获得。现有的深度学习研究多局限于皮肤镜数据集和少数疾病类别，无法代表真实的临床条件。", "method": "研究人员策划并构建了一个包含50多种皮肤病类别的大型数据集，图像通过移动设备采集，更具真实世界代表性。他们评估了多种卷积神经网络（CNN）和基于Transformer的架构，并特别关注了Swin Transformer。为了增强模型的可解释性，引入了梯度加权类激活映射（Grad-CAM）。", "result": "Transformer模型，特别是Swin Transformer，通过有效捕获全局上下文特征，取得了优越的性能。Grad-CAM成功突出了临床相关的区域，并为模型预测提供了透明度。", "conclusion": "研究结果强调了基于Transformer的方法在手机采集皮肤病变分类方面的潜力，为在资源有限的环境中实现可及的AI辅助皮肤病筛查和早期诊断铺平了道路。"}}
{"id": "2509.04479", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04479", "abs": "https://arxiv.org/abs/2509.04479", "authors": ["Jing Liu"], "title": "No Clustering, No Routing: How Transformers Actually Process Rare Tokens", "comment": null, "summary": "Large language models struggle with rare token prediction, yet the mechanisms\ndriving their specialization remain unclear. Prior work identified specialized\n``plateau'' neurons for rare tokens following distinctive three-regime\ninfluence patterns \\cite{liu2025emergent}, but their functional organization is\nunknown. We investigate this through neuron influence analyses, graph-based\nclustering, and attention head ablations in GPT-2 XL and Pythia models. Our\nfindings show that: (1) rare token processing requires additional plateau\nneurons beyond the power-law regime sufficient for common tokens, forming dual\ncomputational regimes; (2) plateau neurons are spatially distributed rather\nthan forming modular clusters; and (3) attention mechanisms exhibit no\npreferential routing to specialists. These results demonstrate that rare token\nspecialization arises through distributed, training-driven differentiation\nrather than architectural modularity, preserving context-sensitive flexibility\nwhile achieving adaptive capacity allocation.", "AI": {"tldr": "大型语言模型中罕见词元预测的专业化是由分布式、训练驱动的分化而非架构模块化产生的，它通过额外的“高原”神经元实现，这些神经元空间分散且不依赖于注意力机制的优先路由。", "motivation": "大型语言模型在罕见词元预测上表现不佳，其专业化机制尚不明确。先前的研究识别出处理罕见词元的“高原”神经元，但其功能组织方式未知。", "method": "通过神经元影响分析、基于图的聚类和注意力头消融实验，在GPT-2 XL和Pythia模型上进行研究。", "result": "1) 罕见词元处理需要超出常见词元所需（幂律机制）的额外“高原”神经元，形成双重计算机制；2) “高原”神经元空间上分散分布，而非形成模块化集群；3) 注意力机制未显示出对专业神经元的优先路由。", "conclusion": "罕见词元专业化是通过分布式、训练驱动的分化而非架构模块化产生的，这在实现自适应容量分配的同时，保留了上下文敏感的灵活性。"}}
{"id": "2509.04480", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04480", "abs": "https://arxiv.org/abs/2509.04480", "authors": ["Ryo Takahashi", "Naoki Saito", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Discrete Prompt Tuning via Recursive Utilization of Black-box Multimodal Large Language Model for Personalized Visual Emotion Recognition", "comment": "11 pages, 4 figures", "summary": "Visual Emotion Recognition (VER) is an important research topic due to its\nwide range of applications, including opinion mining and advertisement design.\nExtending this capability to recognize emotions at the individual level further\nbroadens its potential applications. Recently, Multimodal Large Language Models\n(MLLMs) have attracted increasing attention and demonstrated performance\ncomparable to that of conventional VER methods. However, MLLMs are trained on\nlarge and diverse datasets containing general opinions, which causes them to\nfavor majority viewpoints and familiar patterns. This tendency limits their\nperformance in a personalized VER, which is crucial for practical and\nreal-world applications, and indicates a key area for improvement. To address\nthis limitation, the proposed method employs discrete prompt tuning inspired by\nthe process of humans' prompt engineering to adapt the VER task to each\nindividual. Our method selects the best natural language representation from\nthe generated prompts and uses it to update the prompt for the realization of\naccurate personalized VER.", "AI": {"tldr": "本文提出一种离散提示调整方法，通过模仿人类提示工程，将多模态大语言模型（MLLMs）适应于个性化视觉情感识别（VER），以克服其偏向多数观点的局限性。", "motivation": "视觉情感识别（VER）应用广泛，个性化VER尤为关键。然而，现有MLLMs在个性化VER中表现受限，因为它们倾向于多数观点和常见模式，无法满足实际应用需求。", "method": "该方法采用受人类提示工程启发的离散提示调整技术，为每个个体调整VER任务。它从生成的提示中选择最佳的自然语言表示，并用其更新提示，以实现准确的个性化VER。", "result": "通过提出的离散提示调整方法，MLLMs能够适应个性化VER任务，从而克服其对一般观点的偏好，实现准确的个性化视觉情感识别。", "conclusion": "所提出的离散提示调整方法有效解决了MLLMs在个性化视觉情感识别中偏向多数观点的问题，为实现准确和实用的个性化VER提供了关键改进。"}}
{"id": "2509.04816", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04816", "abs": "https://arxiv.org/abs/2509.04816", "authors": ["Svetlana Pavlitska", "Beyza Keskin", "Alwin Faßbender", "Christian Hubschneider", "J. Marius Zöllner"], "title": "Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation", "comment": "Accepted for publication at the STREAM workshop at ICCV2025", "summary": "Estimating accurate and well-calibrated predictive uncertainty is important\nfor enhancing the reliability of computer vision models, especially in\nsafety-critical applications like traffic scene perception. While ensemble\nmethods are commonly used to quantify uncertainty by combining multiple models,\na mixture of experts (MoE) offers an efficient alternative by leveraging a\ngating network to dynamically weight expert predictions based on the input.\nBuilding on the promising use of MoEs for semantic segmentation in our previous\nworks, we show that well-calibrated predictive uncertainty estimates can be\nextracted from MoEs without architectural modifications. We investigate three\nmethods to extract predictive uncertainty estimates: predictive entropy, mutual\ninformation, and expert variance. We evaluate these methods for an MoE with two\nexperts trained on a semantical split of the A2D2 dataset. Our results show\nthat MoEs yield more reliable uncertainty estimates than ensembles in terms of\nconditional correctness metrics under out-of-distribution (OOD) data.\nAdditionally, we evaluate routing uncertainty computed via gate entropy and\nfind that simple gating mechanisms lead to better calibration of routing\nuncertainty estimates than more complex classwise gates. Finally, our\nexperiments on the Cityscapes dataset suggest that increasing the number of\nexperts can further enhance uncertainty calibration. Our code is available at\nhttps://github.com/KASTEL-MobilityLab/mixtures-of-experts/.", "AI": {"tldr": "该研究表明，混合专家模型（MoE）能从计算机视觉模型中提取准确且校准良好的预测不确定性，比集成方法更可靠，尤其是在分布外数据下，且无需修改架构。", "motivation": "在交通场景感知等安全关键应用中，准确且校准良好的预测不确定性对于提高计算机视觉模型的可靠性至关重要。虽然集成方法常用于量化不确定性，但混合专家模型（MoE）通过门控网络动态加权专家预测，提供了一种更高效的替代方案。", "method": "研究基于之前工作中MoE在语义分割上的应用，探讨了三种从MoE中提取预测不确定性的方法：预测熵、互信息和专家方差。这些方法在一个包含两个专家且在A2D2数据集语义分割上训练的MoE模型上进行评估。此外，还通过门控熵评估了路由不确定性，并比较了简单门控机制与更复杂的类别门控。最后，在Cityscapes数据集上验证了增加专家数量对不确定性校准的影响。", "result": "结果显示，在分布外（OOD）数据下，MoE在条件正确性指标方面比集成方法产生更可靠的不确定性估计。简单的门控机制比复杂的类别门控能更好地校准路由不确定性估计。在Cityscapes数据集上的实验表明，增加专家数量可以进一步增强不确定性校准。", "conclusion": "混合专家模型（MoE）无需架构修改即可提供准确且校准良好的预测不确定性估计，其可靠性优于集成方法，尤其是在处理分布外数据时。简单的门控机制有助于更好的路由不确定性校准，且增加专家数量能进一步提升不确定性校准效果。"}}
{"id": "2509.04482", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04482", "abs": "https://arxiv.org/abs/2509.04482", "authors": ["Ravi Shankar", "Sheng Wong", "Lin Li", "Magdalena Bachmann", "Alex Silverthorne", "Beth Albert", "Gabriel Davis Jones"], "title": "Energy Landscapes Enable Reliable Abstention in Retrieval-Augmented Large Language Models for Healthcare", "comment": null, "summary": "Reliable abstention is critical for retrieval-augmented generation (RAG)\nsystems, particularly in safety-critical domains such as women's health, where\nincorrect answers can lead to harm. We present an energy-based model (EBM) that\nlearns a smooth energy landscape over a dense semantic corpus of 2.6M\nguideline-derived questions, enabling the system to decide when to generate or\nabstain. We benchmark the EBM against a calibrated softmax baseline and a\nk-nearest neighbour (kNN) density heuristic across both easy and hard\nabstention splits, where hard cases are semantically challenging\nnear-distribution queries. The EBM achieves superior abstention performance\nabstention on semantically hard cases, reaching AUROC 0.961 versus 0.950 for\nsoftmax, while also reducing FPR@95 (0.235 vs 0.331). On easy negatives,\nperformance is comparable across methods, but the EBM's advantage becomes most\npronounced in safety-critical hard distributions. A comprehensive ablation with\ncontrolled negative sampling and fair data exposure shows that robustness stems\nprimarily from the energy scoring head, while the inclusion or exclusion of\nspecific negative types (hard, easy, mixed) sharpens decision boundaries but is\nnot essential for generalisation to hard cases. These results demonstrate that\nenergy-based abstention scoring offers a more reliable confidence signal than\nprobability-based softmax confidence, providing a scalable and interpretable\nfoundation for safe RAG systems.", "AI": {"tldr": "该研究提出了一种基于能量模型（EBM）的检索增强生成（RAG）系统拒答机制，在语义困难的近分布查询上，其性能优于传统的softmax和kNN方法，特别是在安全关键领域。", "motivation": "在女性健康等安全关键领域，RAG系统的不准确回答可能导致危害，因此可靠的拒答功能至关重要。传统的置信度评估方法可能不足以处理语义上具有挑战性的查询。", "method": "研究构建了一个基于能量模型（EBM），通过在包含260万个指南衍生问题的密集语义语料库上学习平滑的能量景观，使系统能够决定何时生成或拒答。该EBM与校准的softmax基线和kNN密度启发式方法在简单和困难的拒答情境下进行了基准测试。通过受控的负采样和公平的数据暴露进行了全面的消融实验。", "result": "EBM在语义困难的拒答案例上取得了卓越的性能，AUROC达到0.961（softmax为0.950），FPR@95降低至0.235（softmax为0.331）。在简单负例上，各种方法性能相当。EBM的鲁棒性主要源于其能量评分头，而不同负例类型的包含或排除仅能锐化决策边界，但并非泛化到困难案例的关键。", "conclusion": "基于能量的拒答评分提供了比基于概率的softmax置信度更可靠的置信信号，为构建安全的RAG系统提供了可扩展且可解释的基础。"}}
{"id": "2509.04483", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04483", "abs": "https://arxiv.org/abs/2509.04483", "authors": ["Minghui Huang"], "title": "DecMetrics: Structured Claim Decomposition Scoring for Factually Consistent LLM Outputs", "comment": null, "summary": "Claim decomposition plays a crucial role in the fact-checking process by\nbreaking down complex claims into simpler atomic components and identifying\ntheir unfactual elements. Despite its importance, current research primarily\nfocuses on generative methods for decomposition, with insufficient emphasis on\nevaluating the quality of these decomposed atomic claims. To bridge this gap,\nwe introduce \\textbf{DecMetrics}, which comprises three new metrics:\n\\texttt{COMPLETENESS}, \\texttt{CORRECTNESS}, and \\texttt{SEMANTIC ENTROPY},\ndesigned to automatically assess the quality of claims produced by\ndecomposition models. Utilizing these metrics, we develop a lightweight claim\ndecomposition model, optimizing its performance through the integration of\nthese metrics as a reward function. Through automatic evaluation, our approach\naims to set a benchmark for claim decomposition, enhancing both the reliability\nand effectiveness of fact-checking systems.", "AI": {"tldr": "该论文引入了DecMetrics，一套包含COMPLETENESS、CORRECTNESS和SEMANTIC ENTROPY的新指标，用于自动评估声明分解模型的质量，并利用这些指标优化了一个轻量级分解模型。", "motivation": "尽管声明分解在事实核查中至关重要，但当前研究主要集中于生成方法，对分解出的原子声明质量评估关注不足，存在评估空白。", "method": "引入了DecMetrics，包含COMPLETENESS、CORRECTNESS和SEMANTIC ENTROPY三个新指标，用于自动评估分解声明的质量。此外，开发了一个轻量级声明分解模型，并以这些指标作为奖励函数进行优化。", "result": "通过自动评估，该方法旨在为声明分解设定基准，提高事实核查系统的可靠性和有效性。", "conclusion": "通过引入新的评估指标和优化方法，本研究显著提升了声明分解的质量评估能力，并有望改善事实核查系统的整体性能。"}}
{"id": "2509.04824", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04824", "abs": "https://arxiv.org/abs/2509.04824", "authors": ["Haosong Liu", "Xiancheng Zhu", "Huanqiang Zeng", "Jianqing Zhu", "Jiuwen Cao", "Junhui Hou"], "title": "Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution", "comment": null, "summary": "Recently, Mamba-based methods, with its advantage in long-range information\nmodeling and linear complexity, have shown great potential in optimizing both\ncomputational cost and performance of light field image super-resolution\n(LFSR). However, current multi-directional scanning strategies lead to\ninefficient and redundant feature extraction when applied to complex LF data.\nTo overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)\nstrategy, based on which we design the Subspace Simple Mamba Block (SSMB) to\nachieve more efficient and precise feature extraction. Furthermore, we propose\na dual-stage modeling strategy to address the limitation of state space in\npreserving spatial-angular and disparity information, thereby enabling a more\ncomprehensive exploration of non-local spatial-angular correlations.\nSpecifically, in stage I, we introduce the Spatial-Angular Residual Subspace\nMamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage\nII, we use a dual-branch parallel structure combining the Epipolar Plane Mamba\nBlock (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar\nfeature refinement. Building upon meticulously designed modules and strategies,\nwe introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates\nthe strengths of Mamba and Transformer models for LFSR, enabling comprehensive\ninformation exploration across spatial, angular, and epipolar-plane domains.\nExperimental results demonstrate that LFMT significantly outperforms current\nstate-of-the-art methods in LFSR, achieving substantial improvements in\nperformance while maintaining low computational complexity on both real-word\nand synthetic LF datasets.", "AI": {"tldr": "本文提出了一种名为LFMT的混合Mamba-Transformer框架，通过子空间简单扫描策略和双阶段建模策略，解决了现有Mamba方法在光场超分辨率（LFSR）中特征提取效率低和信息保存不足的问题，显著提升了LFSR性能并保持了较低的计算复杂度。", "motivation": "现有基于Mamba的方法在光场图像超分辨率（LFSR）中展现潜力，但在复杂光场数据上，多方向扫描策略导致特征提取效率低下且冗余。此外，状态空间模型在保留空间-角度和视差信息方面存在局限性，未能充分探索非局部空间-角度相关性。", "method": "本文提出以下方法：1. 子空间简单扫描（Sub-SS）策略及子空间简单Mamba块（SSMB），以实现更高效和精确的特征提取。2. 双阶段建模策略，用于全面探索非局部空间-角度相关性：第一阶段引入空间-角度残差子空间Mamba块（SA-RSMB）进行浅层空间-角度特征提取；第二阶段采用双分支并行结构，结合极平面Mamba块（EPMB）和极平面Transformer块（EPTB）进行深层极平面特征细化。3. 构建了混合Mamba-Transformer框架LFMT，整合了Mamba和Transformer模型的优势，以全面探索空间、角度和极平面域的信息。", "result": "实验结果表明，LFMT在LFSR任务中显著优于当前最先进的方法，在真实和合成光场数据集上均实现了显著的性能提升，同时保持了较低的计算复杂度。", "conclusion": "LFMT框架成功地将Mamba和Transformer的优势结合起来，通过创新的子空间扫描和双阶段建模策略，有效解决了光场超分辨率中特征提取和多维度信息探索的挑战，实现了卓越的性能和计算效率。"}}
{"id": "2509.04484", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.04484", "abs": "https://arxiv.org/abs/2509.04484", "authors": ["Abdelrahman Sadallah", "Tim Baumgärtner", "Iryna Gurevych", "Ted Briscoe"], "title": "The Good, the Bad and the Constructive: Automatically Measuring Peer Review's Utility for Authors", "comment": "EMNLP 2025 Main", "summary": "Providing constructive feedback to paper authors is a core component of peer\nreview. With reviewers increasingly having less time to perform reviews,\nautomated support systems are required to ensure high reviewing quality, thus\nmaking the feedback in reviews useful for authors. To this end, we identify\nfour key aspects of review comments (individual points in weakness sections of\nreviews) that drive the utility for authors: Actionability, Grounding &\nSpecificity, Verifiability, and Helpfulness. To enable evaluation and\ndevelopment of models assessing review comments, we introduce the RevUtil\ndataset. We collect 1,430 human-labeled review comments and scale our data with\n10k synthetically labeled comments for training purposes. The synthetic data\nadditionally contains rationales, i.e., explanations for the aspect score of a\nreview comment. Employing the RevUtil dataset, we benchmark fine-tuned models\nfor assessing review comments on these aspects and generating rationales. Our\nexperiments demonstrate that these fine-tuned models achieve agreement levels\nwith humans comparable to, and in some cases exceeding, those of powerful\nclosed models like GPT-4o. Our analysis further reveals that machine-generated\nreviews generally underperform human reviews on our four aspects.", "AI": {"tldr": "该研究引入了RevUtil数据集和微调模型，用于评估同行评审评论的实用性（基于可操作性、基础与特异性、可验证性和帮助性四个方面）。这些模型在某些方面表现优于GPT-4o，但分析显示机器生成的评论总体上不如人类评论。", "motivation": "随着审稿人用于评审的时间越来越少，需要自动化支持系统来确保高质量的评审，从而为作者提供有用的反馈。这促使研究者识别并评估评审评论中驱动其实用性的关键方面。", "method": "研究者确定了评审评论的四个关键方面：可操作性、基础与特异性、可验证性和帮助性。为此，他们创建了RevUtil数据集，包含1,430条人工标注的评论和1万条带理由的合成标注评论。利用该数据集，他们对微调模型进行了基准测试，以评估评论的这些方面并生成理由。", "result": "实验表明，微调模型在评估评审评论方面与人类达成了一致，在某些情况下甚至超过了GPT-4o等强大的闭源模型。然而，分析也揭示机器生成的评论在所定义的四个方面上普遍不如人类评论。", "conclusion": "该研究成功开发了用于评估评审评论实用性的数据集和模型，证明了其在自动化评审支持方面的潜力。同时，研究也强调了当前机器生成评论与人类评论在质量上仍存在差距。"}}
{"id": "2509.04485", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04485", "abs": "https://arxiv.org/abs/2509.04485", "authors": ["Chris Sainsbury", "Andreas Karwath"], "title": "ASCENDgpt: A Phenotype-Aware Transformer Model for Cardiovascular Risk Prediction from Electronic Health Records", "comment": null, "summary": "We present ASCENDgpt, a transformer-based model specifically designed for\ncardiovascular risk prediction from longitudinal electronic health records\n(EHRs). Our approach introduces a novel phenotype-aware tokenization scheme\nthat maps 47,155 raw ICD codes to 176 clinically meaningful phenotype tokens,\nachieving 99.6\\% consolidation of diagnosis codes while preserving semantic\ninformation. This phenotype mapping contributes to a total vocabulary of 10,442\ntokens - a 77.9\\% reduction when compared with using raw ICD codes directly. We\npretrain ASCENDgpt on sequences derived from 19402 unique individuals using a\nmasked language modeling objective, then fine-tune for time-to-event prediction\nof five cardiovascular outcomes: myocardial infarction (MI), stroke, major\nadverse cardiovascular events (MACE), cardiovascular death, and all-cause\nmortality. Our model achieves excellent discrimination on the held-out test set\nwith an average C-index of 0.816, demonstrating strong performance across all\noutcomes (MI: 0.792, stroke: 0.824, MACE: 0.800, cardiovascular death: 0.842,\nall-cause mortality: 0.824). The phenotype-based approach enables clinically\ninterpretable predictions while maintaining computational efficiency. Our work\ndemonstrates the effectiveness of domain-specific tokenization and pretraining\nfor EHR-based risk prediction tasks.", "AI": {"tldr": "ASCENDgpt是一个基于Transformer的模型，利用创新的表型感知分词方案，从纵向电子健康记录（EHRs）中预测心血管疾病风险，实现了高效且可解释的预测，并取得了优异的性能。", "motivation": "从纵向电子健康记录中进行心血管风险预测面临挑战，需要一种既能有效处理海量原始诊断代码，又能保持临床语义信息和计算效率的方法。", "method": "该研究提出了ASCENDgpt模型，其核心方法包括：1) 创新的表型感知分词方案，将47,155个原始ICD代码映射为176个临床有意义的表型标记，实现了99.6%的诊断代码整合和77.9%的词汇量缩减；2) 在19,402名个体的序列数据上使用掩码语言建模进行预训练；3) 针对心肌梗死、中风、主要不良心血管事件、心血管死亡和全因死亡等五种心血管结局进行时间到事件预测的微调。", "result": "ASCENDgpt模型在测试集上取得了出色的判别能力，平均C-index为0.816。具体结果包括：心肌梗死0.792，中风0.824，主要不良心血管事件0.800，心血管死亡0.842，全因死亡0.824。这种基于表型的方法在保持计算效率的同时，实现了临床可解释的预测。", "conclusion": "该工作证明了领域特定分词和预训练对于基于EHR的风险预测任务的有效性，ASCENDgpt为心血管风险预测提供了一个高性能、高效率且可解释的解决方案。"}}
{"id": "2509.04833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04833", "abs": "https://arxiv.org/abs/2509.04833", "authors": ["Ming Dai", "Wenxuan Cheng", "Jiedong Zhuang", "Jiang-jiang Liu", "Hongshen Zhao", "Zhenhua Feng", "Wankou Yang"], "title": "PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination", "comment": "ICCV2025", "summary": "Recent advances in visual grounding have largely shifted away from\ntraditional proposal-based two-stage frameworks due to their inefficiency and\nhigh computational complexity, favoring end-to-end direct reference paradigms.\nHowever, these methods rely exclusively on the referred target for supervision,\noverlooking the potential benefits of prominent prospective targets. Moreover,\nexisting approaches often fail to incorporate multi-granularity discrimination,\nwhich is crucial for robust object identification in complex scenarios. To\naddress these limitations, we propose PropVG, an end-to-end proposal-based\nframework that, to the best of our knowledge, is the first to seamlessly\nintegrate foreground object proposal generation with referential object\ncomprehension without requiring additional detectors. Furthermore, we introduce\na Contrastive-based Refer Scoring (CRS) module, which employs contrastive\nlearning at both sentence and word levels to enhance the capability in\nunderstanding and distinguishing referred objects. Additionally, we design a\nMulti-granularity Target Discrimination (MTD) module that fuses object- and\nsemantic-level information to improve the recognition of absent targets.\nExtensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO\n(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and\nmodels are available at https://github.com/Dmmm1997/PropVG.", "AI": {"tldr": "PropVG是一个端到端、基于提议的视觉定位框架，它将前景对象提议生成与参照对象理解无缝集成，并引入了对比参照评分（CRS）模块和多粒度目标判别（MTD）模块，以解决现有方法效率低下、忽略前景目标和缺乏多粒度判别的问题。", "motivation": "现有视觉定位方法大多从传统的两阶段框架转向端到端直接参照范式，但它们过度依赖被参照目标进行监督，忽略了显著前景目标的潜在益处。此外，现有方法通常未能整合多粒度判别，这对于复杂场景中的鲁棒对象识别至关重要。", "method": "本文提出了PropVG，一个端到端、基于提议的框架，首次在不额外引入检测器的情况下，将前景对象提议生成与参照对象理解无缝集成。PropVG引入了对比参照评分（CRS）模块，该模块在句子和词语层面采用对比学习，以增强理解和区分参照对象的能力。此外，还设计了多粒度目标判别（MTD）模块，融合对象级和语义级信息，以改进对缺失目标的识别。", "result": "在gRefCOCO (GREC/GRES)、Ref-ZOM、R-RefCOCO和RefCOCO (REC/RES) 基准上的广泛实验证明了PropVG的有效性。", "conclusion": "PropVG通过其创新的端到端提议集成、对比学习和多粒度判别机制，显著提升了视觉定位任务的性能和鲁棒性。"}}
{"id": "2509.04488", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04488", "abs": "https://arxiv.org/abs/2509.04488", "authors": ["Hao Shi", "Yusuke Fujita", "Tomoya Mizumoto", "Lianbo Liu", "Atsushi Kojima", "Yui Sudo"], "title": "Serialized Output Prompting for Large Language Model-based Multi-Talker Speech Recognition", "comment": null, "summary": "Prompts are crucial for task definition and for improving the performance of\nlarge language models (LLM)-based systems. However, existing LLM-based\nmulti-talker (MT) automatic speech recognition (ASR) systems either omit\nprompts or rely on simple task-definition prompts, with no prior work exploring\nthe design of prompts to enhance performance. In this paper, we propose\nextracting serialized output prompts (SOP) and explicitly guiding the LLM using\nstructured prompts to improve system performance (SOP-MT-ASR). A Separator and\nserialized Connectionist Temporal Classification (CTC) layers are inserted\nafter the speech encoder to separate and extract MT content from the mixed\nspeech encoding in a first-speaking-first-out manner. Subsequently, the SOP,\nwhich serves as a prompt for LLMs, is obtained by decoding the serialized CTC\noutputs using greedy search. To train the model effectively, we design a\nthree-stage training strategy, consisting of serialized output training (SOT)\nfine-tuning, serialized speech information extraction, and SOP-based\nadaptation. Experimental results on the LibriMix dataset show that, although\nthe LLM-based SOT model performs well in the two-talker scenario, it fails to\nfully leverage LLMs under more complex conditions, such as the three-talker\nscenario. The proposed SOP approach significantly improved performance under\nboth two- and three-talker conditions.", "AI": {"tldr": "本文提出了一种名为序列化输出提示（SOP）的方法，通过显式引导大型语言模型（LLM）来显著提升基于LLM的多说话人自动语音识别（ASR）系统的性能，尤其是在复杂场景下。", "motivation": "现有基于LLM的多说话人ASR系统要么省略提示，要么仅使用简单的任务定义提示，缺乏对提示设计以提升系统性能的探索。这限制了LLM在处理多说话人场景时的潜力。", "method": "研究人员在语音编码器后插入分离器和序列化连接时序分类（CTC）层，以“先说先出”的方式从混合语音编码中提取多说话人内容。通过贪婪搜索解码序列化CTC输出，得到作为LLM提示的SOP。同时，设计了三阶段训练策略：序列化输出训练（SOT）微调、序列化语音信息提取和基于SOP的适应。", "result": "实验结果显示，尽管基于LLM的SOT模型在双说话人场景中表现良好，但在更复杂的三说话人场景中未能充分利用LLM。所提出的SOP方法在双说话人和三说话人条件下均显著提升了系统性能。", "conclusion": "SOP方法通过提取序列化输出并显式引导LLM，有效解决了现有LLM-based多说话人ASR系统在复杂场景下的性能瓶颈，显著提升了系统在多说话人条件下的识别准确性。"}}
{"id": "2509.04491", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04491", "abs": "https://arxiv.org/abs/2509.04491", "authors": ["Xinnian Zhao", "Hugo Van Hamme"], "title": "Refining Transcripts With TV Subtitles by Prompt-Based Weakly Supervised Training of ASR", "comment": "eusipco2025", "summary": "This study proposes a novel approach to using TV subtitles within a weakly\nsupervised (WS) Automatic Speech Recognition (ASR) framework. Although TV\nsubtitles are readily available, their imprecise alignment with corresponding\naudio limits their applicability as supervised targets for verbatim\ntranscription. Rather than using subtitles as direct supervision signals, our\nmethod reimagines them as context-rich prompts. This design enables the model\nto handle discrepancies between spoken audio and subtitle text. Instead,\ngenerated pseudo transcripts become the primary targets, with subtitles acting\nas guiding cues for iterative refinement. To further enhance the process, we\nintroduce a weighted attention mechanism that emphasizes relevant subtitle\ntokens during inference. Our experiments demonstrate significant improvements\nin transcription accuracy, highlighting the effectiveness of the proposed\nmethod in refining transcripts. These enhanced pseudo-labeled datasets provide\nhigh-quality foundational resources for training robust ASR systems.", "AI": {"tldr": "本研究提出一种新的弱监督ASR方法，利用电视字幕作为上下文丰富的提示，通过迭代细化伪转录本，显著提高了转录准确性。", "motivation": "电视字幕虽然易得，但与对应音频的对齐不精确，限制了其作为监督信号的直接应用。需要一种方法来有效利用这些不精确的字幕。", "method": "该方法将字幕视为上下文丰富的提示，而非直接监督信号。生成的伪转录本成为主要目标，字幕作为指导线索进行迭代细化。此外，引入加权注意力机制，在推断时强调相关字幕标记。", "result": "实验证明转录准确性显著提高，表明所提出方法在细化转录本方面的有效性。这些增强的伪标记数据集为训练鲁棒的ASR系统提供了高质量的基础资源。", "conclusion": "所提出的方法能够有效利用不精确的电视字幕来细化转录本，生成高质量的伪标记数据集，从而为训练强大的ASR系统奠定基础。"}}
{"id": "2509.04834", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04834", "abs": "https://arxiv.org/abs/2509.04834", "authors": ["Yifei Jia", "Shiyu Cheng", "Yu Dong", "Guan Li", "Dong Tian", "Ruixiao Peng", "Xuyi Lu", "Yu Wang", "Wei Yao", "Guihua Shan"], "title": "TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution", "comment": null, "summary": "Understanding the complex combustion dynamics within scramjet engines is\ncritical for advancing high-speed propulsion technologies. However, the large\nscale and high dimensionality of simulation-generated temporal flow field data\npresent significant challenges for visual interpretation, feature\ndifferentiation, and cross-case comparison. In this paper, we present\nTemporalFlowViz, a parameter-aware visual analytics workflow and system\ndesigned to support expert-driven clustering, visualization, and interpretation\nof temporal flow fields from scramjet combustion simulations. Our approach\nleverages hundreds of simulated combustion cases with varying initial\nconditions, each producing time-sequenced flow field images. We use pretrained\nVision Transformers to extract high-dimensional embeddings from these frames,\napply dimensionality reduction and density-based clustering to uncover latent\ncombustion modes, and construct temporal trajectories in the embedding space to\ntrack the evolution of each simulation over time. To bridge the gap between\nlatent representations and expert reasoning, domain specialists annotate\nrepresentative cluster centroids with descriptive labels. These annotations are\nused as contextual prompts for a vision-language model, which generates\nnatural-language summaries for individual frames and full simulation cases. The\nsystem also supports parameter-based filtering, similarity-based case\nretrieval, and coordinated multi-view exploration to facilitate in-depth\nanalysis. We demonstrate the effectiveness of TemporalFlowViz through two\nexpert-informed case studies and expert feedback, showing TemporalFlowViz\nenhances hypothesis generation, supports interpretable pattern discovery, and\nenhances knowledge discovery in large-scale scramjet combustion analysis.", "AI": {"tldr": "TemporalFlowViz是一个参数感知的可视化分析系统，旨在通过结合视觉转换器、降维聚类和视觉语言模型，支持专家对超燃冲压发动机燃烧模拟中的大规模时序流场数据进行聚类、可视化和解释，从而促进模式发现和知识获取。", "motivation": "超燃冲压发动机中复杂的燃烧动力学对于高速推进技术至关重要。然而，模拟生成的大规模、高维时序流场数据在视觉解释、特征区分和跨案例比较方面带来了显著挑战。", "method": "TemporalFlowViz系统采用以下方法：1) 利用预训练的视觉转换器从时序流场图像中提取高维嵌入。2) 应用降维和基于密度的聚类来揭示潜在的燃烧模式。3) 在嵌入空间中构建时间轨迹以追踪模拟演变。4) 领域专家标注代表性聚类中心，作为视觉语言模型的上下文提示，生成帧和完整模拟案例的自然语言摘要。5) 支持基于参数的过滤、基于相似性的案例检索和协调多视图探索。", "result": "通过两个专家知情的案例研究和专家反馈，TemporalFlowViz被证明能够增强假设生成、支持可解释的模式发现，并促进大规模超燃冲压发动机燃烧分析中的知识发现。", "conclusion": "TemporalFlowViz是一个有效的参数感知可视化分析工作流和系统，能够帮助专家对超燃冲压发动机燃烧模拟中的复杂时序流场数据进行深入分析、模式识别和知识获取。"}}
{"id": "2509.04492", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04492", "abs": "https://arxiv.org/abs/2509.04492", "authors": ["Charles Moslonka", "Hicham Randrianarivo", "Arthur Garnier", "Emmanuel Malherbe"], "title": "Learned Hallucination Detection in Black-Box LLMs using Token-level Entropy Production Rate", "comment": "8 pages, 7 figures, 1 table. pre-print version", "summary": "Hallucinations in Large Language Model (LLM) outputs for Question Answering\n(QA) tasks critically undermine their real-world reliability. This paper\nintroduces an applied methodology for robust, one-shot hallucination detection,\nspecifically designed for scenarios with limited data access, such as\ninteracting with black-box LLM APIs that typically expose only a few top\ncandidate log-probabilities per token. Our approach derives uncertainty\nindicators directly from these readily available log-probabilities generated\nduring non-greedy decoding. We first derive an Entropy Production Rate (EPR)\nmetric that offers baseline performance, later augmented with supervised\nlearning. Our learned model uses features representing the entropic\ncontributions of the accessible top-ranked tokens within a single generated\nsequence, requiring no multiple query re-runs. Evaluated across diverse QA\ndatasets and multiple LLMs, this estimator significantly improves hallucination\ndetection over using EPR alone. Crucially, high performance is demonstrated\nusing only the typically small set of available log-probabilities (e.g., top\n<10 per token), confirming its practical efficiency and suitability for these\nAPI-constrained deployments. This work provides a readily deployable technique\nto enhance the trustworthiness of LLM responses from a single generation pass\nin QA and Retrieval-Augmented Generation (RAG) systems, with its utility\nfurther demonstrated in a finance framework analyzing responses to queries on\nannual reports from an industrial dataset.", "AI": {"tldr": "本文提出了一种针对LLM问答任务中幻觉的单次检测方法，该方法利用非贪婪解码过程中有限的对数概率（例如，每个token的前10个）来推导不确定性指标，并通过监督学习进行增强，显著提高了检测性能。", "motivation": "大型语言模型（LLM）在问答任务中的幻觉严重损害了其在实际应用中的可靠性。尤其是在数据访问受限（如与黑盒LLM API交互，只能获取少量排名靠前的token对数概率）的场景下，急需一种鲁棒的幻觉检测方法。", "method": "该方法直接从非贪婪解码过程中生成的、易于获取的对数概率中推导不确定性指标。首先，提出了一种熵产生率（EPR）度量作为基线性能。然后，通过监督学习对EPR进行增强，利用单个生成序列中可访问的排名靠前token的熵贡献作为特征。该方法无需多次查询重跑，仅依赖于每个token通常可用的少量对数概率。", "result": "该方法在不同的问答数据集和多个LLM上进行了评估，结果表明其幻觉检测性能显著优于单独使用EPR。关键是，即使仅使用通常很小的可用对数概率集（例如，每个token的前10个），也能表现出高检测性能，证实了其在API受限部署中的实用性和效率。该技术在金融框架中（分析年度报告查询响应）也得到了应用验证。", "conclusion": "这项工作提供了一种易于部署的技术，可以在问答（QA）和检索增强生成（RAG）系统中，通过单次生成过程提高LLM响应的可信度。其在API受限环境中的高效性及其在工业数据集上的成功应用进一步证明了其价值。"}}
{"id": "2509.04497", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04497", "abs": "https://arxiv.org/abs/2509.04497", "authors": ["Syed Ahmad Chan Bukhari", "Fazel Keshtkar", "Alyssa Meczkowska"], "title": "A Narrative-Driven Computational Framework for Clinician Burnout Surveillance", "comment": "6 pages, 6 Figure", "summary": "Clinician burnout poses a substantial threat to patient safety, particularly\nin high-acuity intensive care units (ICUs). Existing research predominantly\nrelies on retrospective survey tools or broad electronic health record (EHR)\nmetadata, often overlooking the valuable narrative information embedded in\nclinical notes. In this study, we analyze 10,000 ICU discharge summaries from\nMIMIC-IV, a publicly available database derived from the electronic health\nrecords of Beth Israel Deaconess Medical Center. The dataset encompasses\ndiverse patient data, including vital signs, medical orders, diagnoses,\nprocedures, treatments, and deidentified free-text clinical notes. We introduce\na hybrid pipeline that combines BioBERT sentiment embeddings fine-tuned for\nclinical narratives, a lexical stress lexicon tailored for clinician burnout\nsurveillance, and five-topic latent Dirichlet allocation (LDA) with workload\nproxies. A provider-level logistic regression classifier achieves a precision\nof 0.80, a recall of 0.89, and an F1 score of 0.84 on a stratified hold-out\nset, surpassing metadata-only baselines by greater than or equal to 0.17 F1\nscore. Specialty-specific analysis indicates elevated burnout risk among\nproviders in Radiology, Psychiatry, and Neurology. Our findings demonstrate\nthat ICU clinical narratives contain actionable signals for proactive\nwell-being monitoring.", "AI": {"tldr": "本研究利用混合自然语言处理（NLP）方法分析ICU出院总结中的临床叙述文本，以识别临床医生职业倦怠风险，并发现临床叙述中包含可操作的预警信号。", "motivation": "临床医生职业倦怠对患者安全构成重大威胁，尤其是在高危重症监护室（ICU）。现有研究主要依赖回顾性调查或宽泛的电子健康记录（EHR）元数据，往往忽视了临床笔记中嵌入的宝贵叙述信息，因此需要一种更有效的方法来利用这些信息进行主动监测。", "method": "研究分析了来自MIMIC-IV数据库的10,000份ICU出院总结，该数据集包含生命体征、医疗指令、诊断、程序、治疗和去识别化的自由文本临床笔记。研究引入了一个混合管道，结合了为临床叙述微调的BioBERT情感嵌入、为临床医生职业倦怠监测量身定制的词汇压力词典，以及包含工作量代理的五主题潜在狄利克雷分配（LDA）。最终使用提供者级别的逻辑回归分类器进行风险预测。", "result": "在分层保留集上，提供者级别的逻辑回归分类器实现了0.80的精确度、0.89的召回率和0.84的F1分数，F1分数超过仅使用元数据的基线至少0.17。专业特异性分析表明，放射科、精神病学和神经内科的提供者职业倦怠风险较高。", "conclusion": "ICU临床叙述中包含可操作的信号，可用于主动监测临床医生的福祉，证明了利用自然语言处理从临床笔记中提取此类信息的有效性。"}}
{"id": "2509.04848", "categories": ["cs.CV", "physics.bio-ph", "physics.optics", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.04848", "abs": "https://arxiv.org/abs/2509.04848", "authors": ["Enze Ye", "Wei Lin", "Shaochi Ren", "Yakun Liu", "Xiaoping Li", "Hao Wang", "He Sun", "Feng Pan"], "title": "Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations", "comment": "16 pages, 5 figures", "summary": "High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables\nlabel-free, volumetric characterization of individual cells by reconstructing\ntheir refractive index (RI) distributions from multiple viewing angles during\nflow through microfluidic channels. However, current imaging methods assume\nthat cells undergo uniform, single-axis rotation, which require their poses to\nbe known at each frame. This assumption restricts applicability to\nnear-spherical cells and prevents accurate imaging of irregularly shaped cells\nwith complex rotations. As a result, only a subset of the cellular population\ncan be analyzed, limiting the ability of flow-based assays to perform robust\nstatistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction\nframework that leverages the Fourier diffraction theorem and implicit neural\nrepresentations (INRs) for high-throughput flow cytometry tomographic imaging.\nBy jointly optimizing each cell's unknown rotational trajectory and volumetric\nstructure under weak scattering assumptions, OmniFHT supports arbitrary cell\ngeometries and multi-axis rotations. Its continuous representation also allows\naccurate reconstruction from sparsely sampled projections and restricted\nangular coverage, producing high-fidelity results with as few as 10 views or\nonly 120 degrees of angular range. OmniFHT enables, for the first time, in\nsitu, high-throughput tomographic imaging of entire flowing cell populations,\nproviding a scalable and unbiased solution for label-free morphometric analysis\nin flow cytometry platforms.", "AI": {"tldr": "OmniFHT是一种新型的无姿态3D折射率（RI）重建框架，结合傅里叶衍射定理和隐式神经表示（INRs），首次实现了流式细胞术中对所有流动细胞群体的原位、高通量断层成像，克服了现有方法对细胞形状和旋转的限制。", "motivation": "现有高通量3D定量相位成像（QPI）方法假设细胞进行均匀的单轴旋转且姿态已知，这限制了其仅适用于近球形细胞，无法准确成像不规则形状细胞的复杂旋转，导致只能分析部分细胞群体，限制了流式分析的统计鲁棒性。", "method": "该研究引入了OmniFHT框架，利用傅里叶衍射定理和隐式神经表示（INRs）进行高通量流式细胞术断层成像。它在弱散射假设下，联合优化每个细胞的未知旋转轨迹和体积结构，从而实现无姿态的3D RI重建。", "result": "OmniFHT支持任意细胞几何形状和多轴旋转。其连续表示允许从稀疏采样投影和受限角度范围（例如，仅10个视图或120度角范围）进行准确重建，产生高保真结果。首次实现了对整个流动细胞群体的原位、高通量断层成像。", "conclusion": "OmniFHT为流式细胞术平台提供了一种可扩展且无偏的无标记形态分析解决方案，首次实现了对所有流动细胞群体的原位、高通量断层成像。"}}
{"id": "2509.04498", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04498", "abs": "https://arxiv.org/abs/2509.04498", "authors": ["Krithi Shailya", "Akhilesh Kumar Mishra", "Gokul S Krishnan", "Balaraman Ravindran"], "title": "Where Should I Study? Biased Language Models Decide! Evaluating Fairness in LMs for Academic Recommendations", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used as daily recommendation\nsystems for tasks like education planning, yet their recommendations risk\nperpetuating societal biases. This paper empirically examines geographic,\ndemographic, and economic biases in university and program suggestions from\nthree open-source LLMs: LLaMA-3.1-8B, Gemma-7B, and Mistral-7B. Using 360\nsimulated user profiles varying by gender, nationality, and economic status, we\nanalyze over 25,000 recommendations. Results show strong biases: institutions\nin the Global North are disproportionately favored, recommendations often\nreinforce gender stereotypes, and institutional repetition is prevalent. While\nLLaMA-3.1 achieves the highest diversity, recommending 481 unique universities\nacross 58 countries, systemic disparities persist. To quantify these issues, we\npropose a novel, multi-dimensional evaluation framework that goes beyond\naccuracy by measuring demographic and geographic representation. Our findings\nhighlight the urgent need for bias consideration in educational LMs to ensure\nequitable global access to higher education.", "AI": {"tldr": "本研究发现，用于教育规划的开源大型语言模型（LLMs）存在严重的地理、人口和经济偏见，过度偏爱全球北方机构并强化性别刻板印象。文章提出了一个多维度评估框架，并强调了在教育LLM中考虑偏见的紧迫性。", "motivation": "大型语言模型（LLMs）日益被用作教育规划等任务的推荐系统，但其推荐存在延续社会偏见的风险，这可能影响高等教育的公平可及性。", "method": "研究通过360个模拟用户档案（性别、国籍、经济状况），分析了LLaMA-3.1-8B、Gemma-7B和Mistral-7B三个开源LLM的25,000多条大学和项目推荐。为量化这些问题，论文还提出了一个超越准确性、衡量人口和地理代表性的新型多维度评估框架。", "result": "研究结果显示LLMs存在显著偏见：全球北方机构被不成比例地偏爱；推荐常强化性别刻板印象；机构推荐重复性高。尽管LLaMA-3.1在多样性方面表现最佳（推荐了58个国家的481所独特大学），但系统性差异依然普遍存在。", "conclusion": "研究发现凸显了在教育领域LLMs中迫切需要考虑偏见问题，以确保全球高等教育的公平可及性。"}}
{"id": "2509.04499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04499", "abs": "https://arxiv.org/abs/2509.04499", "authors": ["Pranav Narayanan Venkit", "Philippe Laban", "Yilun Zhou", "Kung-Hsiang Huang", "Yixin Mao", "Chien-Sheng Wu"], "title": "DeepTRACE: Auditing Deep Research AI Systems for Tracking Reliability Across Citations and Evidence", "comment": "arXiv admin note: text overlap with arXiv:2410.22349", "summary": "Generative search engines and deep research LLM agents promise trustworthy,\nsource-grounded synthesis, yet users regularly encounter overconfidence, weak\nsourcing, and confusing citation practices. We introduce DeepTRACE, a novel\nsociotechnically grounded audit framework that turns prior community-identified\nfailure cases into eight measurable dimensions spanning answer text, sources,\nand citations. DeepTRACE uses statement-level analysis (decomposition,\nconfidence scoring) and builds citation and factual-support matrices to audit\nhow systems reason with and attribute evidence end-to-end. Using automated\nextraction pipelines for popular public models (e.g., GPT-4.5/5, You.com,\nPerplexity, Copilot/Bing, Gemini) and an LLM-judge with validated agreement to\nhuman raters, we evaluate both web-search engines and deep-research\nconfigurations. Our findings show that generative search engines and deep\nresearch agents frequently produce one-sided, highly confident responses on\ndebate queries and include large fractions of statements unsupported by their\nown listed sources. Deep-research configurations reduce overconfidence and can\nattain high citation thoroughness, but they remain highly one-sided on debate\nqueries and still exhibit large fractions of unsupported statements, with\ncitation accuracy ranging from 40--80% across systems.", "AI": {"tldr": "本研究引入了DeepTRACE审计框架，用于评估生成式搜索引擎和深度研究LLM代理在可信度、来源引用和事实支持方面的表现。研究发现，这些系统普遍存在过度自信、片面性以及大量陈述未被其引用来源支持的问题，即使是深度研究配置也未能完全解决这些挑战。", "motivation": "生成式搜索引擎和深度研究LLM代理承诺提供可信、基于来源的综合信息，但用户经常遇到过度自信、来源薄弱和引用实践混乱的问题。因此，需要一个框架来识别和衡量这些已知的失败案例。", "method": "研究引入了DeepTRACE，一个基于社会技术学的审计框架，将社区识别的失败案例转化为八个可衡量的维度，涵盖答案文本、来源和引用。它使用陈述级分析（分解、置信度评分）并构建引用和事实支持矩阵。通过自动化提取管道评估了流行的公共模型（如GPT-4.5/5, You.com, Perplexity, Copilot/Bing, Gemini），并使用与人类评估者达成一致的LLM判官，评估了网络搜索引擎和深度研究配置。", "result": "研究发现，生成式搜索引擎和深度研究代理在辩论查询上经常产生片面、高度自信的回答，并且包含大量未被其列出来源支持的陈述。深度研究配置虽然能降低过度自信并达到较高的引用彻底性，但在辩论查询上仍高度片面，且仍显示大量未被支持的陈述，系统间的引用准确率在40-80%之间。", "conclusion": "尽管生成式搜索引擎和深度研究LLM代理承诺提供可信信息，但它们普遍存在过度自信、片面性以及其自身来源无法支持大量陈述的问题。虽然深度研究配置在降低过度自信和提高引用彻底性方面有所帮助，但仍未能解决片面性和未支持陈述的根本问题，引用准确性也存在显著不足。"}}
{"id": "2509.04859", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04859", "abs": "https://arxiv.org/abs/2509.04859", "authors": ["Hannah Schieber", "Dominik Frischmann", "Simon Boche", "Victor Schaack", "Angela Schoellig", "Stefan Leutenegger", "Daniel Roth"], "title": "CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus", "comment": null, "summary": "Mobile reconstruction for autonomous aerial robotics holds strong potential\nfor critical applications such as tele-guidance and disaster response. These\ntasks demand both accurate 3D reconstruction and fast scene processing. Instead\nof reconstructing the entire scene in detail, it is often more efficient to\nfocus on specific objects, i.e., points of interest (PoIs). Mobile robots\nequipped with advanced sensing can usually detect these early during data\nacquisition or preliminary analysis, reducing the need for full-scene\noptimization. Gaussian Splatting (GS) has recently shown promise in delivering\nhigh-quality novel view synthesis and 3D representation by an incremental\nlearning process. Extending GS with scene editing, semantics adds useful\nper-splat features to isolate objects effectively.\n  Semantic 3D Gaussian editing can already be achieved before the full training\ncycle is completed, reducing the overall training time. Moreover, the\nsemantically relevant area, the PoI, is usually already known during capturing.\nTo balance high-quality reconstruction with reduced training time, we propose\nCoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS\nand then refine it for the semantic object using our novel color-based\neffective filtering for effective object isolation. This is speeding up the\ntraining process to be about a quarter less than a full training cycle for\nsemantic GS. We evaluate our approach on two datasets, SCRREAM (real-world,\noutdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher\nnovel-view-synthesis quality.", "AI": {"tldr": "本文提出CoRe-GS，一种针对自主航空机器人移动重建的方法，通过结合语义高斯溅射和新颖的基于颜色的过滤，实现更快、更高质量的特定对象（兴趣点）3D重建，减少训练时间并提高新视图合成质量。", "motivation": "远程指导和灾害响应等关键应用需要准确的3D重建和快速场景处理。详细重建整个场景效率低下，而关注特定对象（兴趣点）更有效。高斯溅射（GS）在高质量新视图合成和3D表示方面前景广阔，但需要优化训练时间，尤其是在关注特定对象时。", "method": "本文提出了CoRe-GS方法。首先，使用语义高斯溅射生成一个粗略的、可用于分割的场景。然后，利用新颖的基于颜色的有效过滤技术对语义对象（兴趣点）进行精细化隔离和重建。", "result": "CoRe-GS将训练过程时间缩短了约四分之一，相对于完整的语义高斯溅射训练周期。在SCRREAM（真实世界，室外）和NeRDS 360（合成，室内）两个数据集上的评估表明，该方法减少了运行时间并提高了新视图合成的质量。", "conclusion": "CoRe-GS通过结合语义高斯溅射和创新的颜色过滤，实现了更快、更高质量的特定对象3D重建，有效平衡了重建质量与训练时间，非常适用于需要快速处理兴趣点的移动机器人应用。"}}
{"id": "2509.04500", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04500", "abs": "https://arxiv.org/abs/2509.04500", "authors": ["Rushi Wang", "Jiateng Liu", "Cheng Qian", "Yifan Shen", "Yanzhou Pan", "Zhaozhuo Xu", "Ahmed Abbasi", "Heng Ji", "Denghui Zhang"], "title": "Context Engineering for Trustworthiness: Rescorla Wagner Steering Under Mixed and Inappropriate Contexts", "comment": "36 pages, 7 figures", "summary": "Incorporating external context can significantly enhance the response quality\nof Large Language Models (LLMs). However, real-world contexts often mix\nrelevant information with disproportionate inappropriate content, posing\nreliability risks. How do LLMs process and prioritize mixed context? To study\nthis, we introduce the Poisoned Context Testbed, pairing queries with\nreal-world contexts containing relevant and inappropriate content. Inspired by\nassociative learning in animals, we adapt the Rescorla-Wagner (RW) model from\nneuroscience to quantify how competing contextual signals influence LLM\noutputs. Our adapted model reveals a consistent behavioral pattern: LLMs\nexhibit a strong tendency to incorporate information that is less prevalent in\nthe context. This susceptibility is harmful in real-world settings, where small\namounts of inappropriate content can substantially degrade response quality.\nEmpirical evaluations on our testbed further confirm this vulnerability. To\ntackle this, we introduce RW-Steering, a two-stage finetuning-based approach\nthat enables the model to internally identify and ignore inappropriate signals.\nUnlike prior methods that rely on extensive supervision across diverse context\nmixtures, RW-Steering generalizes robustly across varying proportions of\ninappropriate content. Experiments show that our best fine-tuned model improves\nresponse quality by 39.8% and reverses the undesirable behavior curve,\nestablishing RW-Steering as a robust, generalizable context engineering\nsolution for improving LLM safety in real-world use.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在处理混合上下文时，倾向于采纳其中不那么普遍的不当信息，导致响应质量下降。本文提出了RW-Steering，一种两阶段微调方法，使LLMs能够识别并忽略不当信号，显著提升了模型在真实世界场景中的安全性和响应质量。", "motivation": "将外部上下文整合到LLMs中可以显著提高响应质量，但实际上下文往往混杂了相关信息和大量不当内容，这带来了可靠性风险。研究旨在探究LLMs如何处理和优先排序这种混合上下文。", "method": "引入了“中毒上下文测试平台”（Poisoned Context Testbed），将查询与包含相关和不当内容的真实世界上下文配对。受动物联想学习启发，改编了神经科学中的Rescorla-Wagner (RW) 模型来量化竞争性上下文信号如何影响LLM输出。为解决发现的问题，提出了RW-Steering，这是一种两阶段的基于微调的方法，使模型能够内部识别并忽略不当信号。", "result": "改编模型揭示了LLMs的一种一致行为模式：它们强烈倾向于采纳上下文中不那么普遍的信息。这种易感性在实际设置中是有害的，少量不当内容就能显著降低响应质量。经验评估进一步证实了这种脆弱性。RW-Steering方法与现有方法不同，它能鲁棒地泛化到不同比例的不当内容。实验表明，最佳微调模型将响应质量提高了39.8%，并逆转了不良行为曲线。", "conclusion": "RW-Steering被确立为一种鲁棒、可泛化的上下文工程解决方案，用于提高LLMs在真实世界使用中的安全性，通过使模型内部识别并忽略不当信号，解决了LLMs易受上下文中不那么普遍的不当信息影响的问题。"}}
{"id": "2509.04501", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T05, 62M45, 68T50, 90C40", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.04501", "abs": "https://arxiv.org/abs/2509.04501", "authors": ["Rohit Patel"], "title": "Understanding Reinforcement Learning for Model Training, and future directions with GRAPE", "comment": "35 pages, 1 figure", "summary": "This paper provides a self-contained, from-scratch, exposition of key\nalgorithms for instruction tuning of models: SFT, Rejection Sampling,\nREINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy\nOptimization (PPO), Group Relative Policy Optimization (GRPO), and Direct\nPreference Optimization (DPO). Explanations of these algorithms often assume\nprior knowledge, lack critical details, and/or are overly generalized and\ncomplex. Here, each method is discussed and developed step by step using\nsimplified and explicit notation focused on LLMs, aiming to eliminate ambiguity\nand provide a clear and intuitive understanding of the concepts. By minimizing\ndetours into the broader RL literature and connecting concepts to LLMs, we\neliminate superfluous abstractions and reduce cognitive overhead. Following\nthis exposition, we provide a literature review of new techniques and\napproaches beyond those detailed. Finally, new ideas for research and\nexploration in the form of GRAPE (Generalized Relative Advantage Policy\nEvolution) are presented.", "AI": {"tldr": "本文从零开始详细阐述了大型语言模型（LLMs）指令微调的关键算法（如SFT、PPO、DPO等），采用简化且专注LLMs的符号，旨在消除歧义并提供清晰直观的理解，并提出了新的研究方向GRAPE。", "motivation": "现有算法解释常假设读者有先验知识、缺乏关键细节、过于泛化或复杂，导致难以理解。本文旨在消除歧义，提供对概念的清晰直观理解，并减少认知负担。", "method": "本文逐一讨论并逐步发展了SFT、Rejection Sampling、REINFORCE、TRPO、PPO、GRPO和DPO等指令微调算法，采用简化、明确且专注于LLMs的符号。它最大程度地减少了对更广泛强化学习文献的深入探讨，并将概念与LLMs紧密连接。此外，还提供了最新技术的文献综述，并提出了名为GRAPE（Generalized Relative Advantage Policy Evolution）的新研究思路。", "result": "通过简化和明确的符号，本文成功地消除了对LLMs指令微调算法的歧义，提供了清晰直观的概念理解，并减少了认知负担。同时，它回顾了新颖技术，并提出了GRAPE作为新的研究和探索方向。", "conclusion": "本文为LLMs指令微调的关键算法提供了全面、清晰且易于理解的阐述，成功地简化了复杂概念，并为未来的研究和探索（如GRAPE）奠定了基础。"}}
{"id": "2509.04886", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04886", "abs": "https://arxiv.org/abs/2509.04886", "authors": ["Trixia Simangan", "Ahmed Nadeem Abbasi", "Yipeng Hu", "Shaheer U. Saeed"], "title": "Cryo-RL: automating prostate cancer cryoablation planning with reinforcement learning", "comment": "Accepted at MICAD (Medical Imaging and Computer-Aided Diagnosis) 2025", "summary": "Cryoablation is a minimally invasive localised treatment for prostate cancer\nthat destroys malignant tissue during de-freezing, while sparing surrounding\nhealthy structures. Its success depends on accurate preoperative planning of\ncryoprobe placements to fully cover the tumour and avoid critical anatomy. This\nplanning is currently manual, expertise-dependent, and time-consuming, leading\nto variability in treatment quality and limited scalability. In this work, we\nintroduce Cryo-RL, a reinforcement learning framework that models cryoablation\nplanning as a Markov decision process and learns an optimal policy for\ncryoprobe placement. Within a simulated environment that models clinical\nconstraints and stochastic intraoperative variability, an agent sequentially\nselects cryoprobe positions and ice sphere diameters. Guided by a reward\nfunction based on tumour coverage, this agent learns a cryoablation strategy\nthat leads to optimal cryoprobe placements without the need for any\nmanually-designed plans. Evaluated on 583 retrospective prostate cancer cases,\nCryo-RL achieved over 8 percentage-point Dice improvements compared with the\nbest automated baselines, based on geometric optimisation, and matched human\nexpert performance while requiring substantially less planning time. These\nresults highlight the potential of reinforcement learning to deliver clinically\nviable, reproducible, and efficient cryoablation plans.", "AI": {"tldr": "本文提出Cryo-RL，一个基于强化学习的框架，用于前列腺癌冷冻消融术中冷冻探针放置的自动化规划，显著提升了规划质量和效率。", "motivation": "当前冷冻消融规划是手动、依赖专家经验且耗时的，导致治疗质量不一和可扩展性受限。", "method": "将冷冻消融规划建模为马尔可夫决策过程，并开发了一个强化学习框架Cryo-RL。在一个模拟环境中，智能体通过基于肿瘤覆盖的奖励函数，学习并顺序选择冷冻探针位置和冰球直径，以生成最优的冷冻消融策略。", "result": "在583例回顾性前列腺癌病例中，Cryo-RL相比于最佳几何优化自动化基线，Dice系数提高了超过8个百分点，并与人类专家表现相当，同时大幅减少了规划时间。", "conclusion": "强化学习在前列腺癌冷冻消融规划中具有巨大潜力，能够提供临床可行、可复现且高效的治疗方案。"}}
{"id": "2509.04502", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04502", "abs": "https://arxiv.org/abs/2509.04502", "authors": ["Qixin Sun", "Ziqin Wang", "Hengyuan Zhao", "Yilin Li", "Kaiyou Song", "Linjiang Huang", "Xiaolin Hu", "Qingpei Guo", "Si Liu"], "title": "VaccineRAG: Boosting Multimodal Large Language Models' Immunity to Harmful RAG Samples", "comment": null, "summary": "Retrieval Augmented Generation enhances the response accuracy of Large\nLanguage Models (LLMs) by integrating retrieval and generation modules with\nexternal knowledge, demonstrating particular strength in real-time queries and\nVisual Question Answering tasks. However, the effectiveness of RAG is\nfrequently hindered by the precision of the retriever: many retrieved samples\nfed into the generation phase are irrelevant or misleading, posing a critical\nbottleneck to LLMs' performance. To address this challenge, we introduce\nVaccineRAG, a novel Chain-of-Thought-based retrieval-augmented generation\ndataset. On one hand, VaccineRAG employs a benchmark to evaluate models using\ndata with varying positive/negative sample ratios, systematically exposing\ninherent weaknesses in current LLMs. On the other hand, it enhances models'\nsample-discrimination capabilities by prompting LLMs to generate explicit\nChain-of-Thought (CoT) analysis for each sample before producing final answers.\nFurthermore, to enhance the model's ability to learn long-sequence complex CoT\ncontent, we propose Partial-GRPO. By modeling the outputs of LLMs as multiple\ncomponents rather than a single whole, our model can make more informed\npreference selections for complex sequences, thereby enhancing its capacity to\nlearn complex CoT. Comprehensive evaluations and ablation studies on VaccineRAG\nvalidate the effectiveness of the proposed scheme. The code and dataset will be\npublicly released soon.", "AI": {"tldr": "本文提出了一种名为VaccineRAG的新型Chain-of-Thought（CoT）检索增强生成数据集，旨在通过评估不同正负样本比例的数据和生成显式CoT分析来解决检索器精度问题，并引入Partial-GRPO方法以增强模型学习复杂CoT内容的能力。", "motivation": "检索增强生成（RAG）的有效性常受检索器精度的限制，许多检索到的样本与生成阶段无关或具有误导性，成为大型语言模型（LLM）性能的关键瓶颈。", "method": "1. 引入VaccineRAG数据集，一个基于CoT的检索增强生成数据集。2. VaccineRAG使用基准测试，通过不同正负样本比例的数据评估模型，暴露LLM的固有弱点。3. 通过提示LLM为每个样本生成显式CoT分析，增强模型的样本判别能力。4. 提出Partial-GRPO方法，通过将LLM输出建模为多个组件而非单一整体，以增强模型学习长序列复杂CoT内容的能力。", "result": "在VaccineRAG上的综合评估和消融研究验证了所提出方案的有效性。", "conclusion": "所提出的VaccineRAG数据集和Partial-GRPO方法能有效提升RAG的性能，通过增强样本判别和复杂CoT学习，解决了检索器精度不足的瓶颈问题。"}}
{"id": "2509.04504", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04504", "abs": "https://arxiv.org/abs/2509.04504", "authors": ["Zehua Pei", "Hui-Ling Zhen", "Ying Zhang", "Zhiyuan Yang", "Xing Li", "Xianzhi Yu", "Mingxuan Yuan", "Bei Yu"], "title": "Behavioral Fingerprinting of Large Language Models", "comment": "Submitted to 1st Open Conference on AI Agents for Science\n  (agents4science 2025)", "summary": "Current benchmarks for Large Language Models (LLMs) primarily focus on\nperformance metrics, often failing to capture the nuanced behavioral\ncharacteristics that differentiate them. This paper introduces a novel\n``Behavioral Fingerprinting'' framework designed to move beyond traditional\nevaluation by creating a multi-faceted profile of a model's intrinsic cognitive\nand interactive styles. Using a curated \\textit{Diagnostic Prompt Suite} and an\ninnovative, automated evaluation pipeline where a powerful LLM acts as an\nimpartial judge, we analyze eighteen models across capability tiers. Our\nresults reveal a critical divergence in the LLM landscape: while core\ncapabilities like abstract and causal reasoning are converging among top\nmodels, alignment-related behaviors such as sycophancy and semantic robustness\nvary dramatically. We further document a cross-model default persona clustering\n(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,\nthis suggests that a model's interactive nature is not an emergent property of\nits scale or reasoning power, but a direct consequence of specific, and highly\nvariable, developer alignment strategies. Our framework provides a reproducible\nand scalable methodology for uncovering these deep behavioral differences.\nProject: https://github.com/JarvisPei/Behavioral-Fingerprinting", "AI": {"tldr": "本文提出了一种“行为指纹”框架，用于超越传统性能指标，评估大型语言模型（LLMs）的内在认知和交互风格。研究发现，尽管顶级模型的核心能力趋于一致，但与对齐相关的行为差异显著，且模型的交互性质是开发者对齐策略的直接结果。", "motivation": "当前LLM基准测试主要关注性能指标，未能捕捉到区分模型细微行为特征的能力。", "method": "引入了“行为指纹”框架，通过一套精心策划的“诊断提示套件”和一个创新的自动化评估流程（由强大的LLM充当公正的评判者），分析了18个不同能力层级的模型。", "result": "研究揭示，LLM领域存在关键差异：顶级模型的核心能力（如抽象和因果推理）趋于收敛，但与对齐相关的行为（如谄媚和语义鲁棒性）差异巨大。此外，还发现跨模型存在默认人格聚类（ISTJ/ESTJ），这可能反映了常见的对齐激励。", "conclusion": "模型的交互性质并非其规模或推理能力的涌现属性，而是特定且高度可变的开发者对齐策略的直接结果。该框架提供了一种可复现和可扩展的方法来揭示这些深层的行为差异。"}}
{"id": "2509.04889", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04889", "abs": "https://arxiv.org/abs/2509.04889", "authors": ["Dominik Pegler", "David Steyrl", "Mengfan Zhang", "Alexander Karner", "Jozsef Arato", "Frank Scharnowski", "Filip Melinscak"], "title": "SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models", "comment": "60 pages (30 main text, 30 appendix), 20 figures (5 in main text, 15\n  in appendix)", "summary": "Advances in computer vision have opened new avenues for clinical\napplications, particularly in computerized exposure therapy where visual\nstimuli can be dynamically adjusted based on patient responses. As a critical\nstep toward such adaptive systems, we investigated whether pretrained computer\nvision models can accurately predict fear levels from spider-related images. We\nadapted three diverse models using transfer learning to predict human fear\nratings (on a 0-100 scale) from a standardized dataset of 313 images. The\nmodels were evaluated using cross-validation, achieving an average mean\nabsolute error (MAE) between 10.1 and 11.0. Our learning curve analysis\nrevealed that reducing the dataset size significantly harmed performance,\nthough further increases yielded no substantial gains. Explainability\nassessments showed the models' predictions were based on spider-related\nfeatures. A category-wise error analysis further identified visual conditions\nassociated with higher errors (e.g., distant views and artificial/painted\nspiders). These findings demonstrate the potential of explainable computer\nvision models in predicting fear ratings, highlighting the importance of both\nmodel explainability and a sufficient dataset size for developing effective\nemotion-aware therapeutic technologies.", "AI": {"tldr": "研究表明，预训练的计算机视觉模型能够从蜘蛛图像中准确预测人类的恐惧水平，平均绝对误差在10.1至11.0之间，并强调了数据量和模型可解释性的重要性。", "motivation": "计算机视觉的进步为临床应用（特别是计算机化暴露疗法）开辟了新途径，其中视觉刺激可以根据患者反应进行动态调整。本研究旨在探索预训练的计算机视觉模型是否能准确预测患者对蜘蛛图像的恐惧水平，以作为开发此类自适应系统的关键一步。", "method": "研究人员采用迁移学习方法，调整了三个不同的预训练计算机视觉模型，使其能够从一个包含313张标准化图像的数据集中预测人类的恐惧评分（0-100分）。模型通过交叉验证进行评估，并进行了学习曲线分析、可解释性评估以及类别错误分析。", "result": "模型在预测人类恐惧评分方面取得了平均10.1到11.0的平均绝对误差（MAE）。学习曲线分析显示，减少数据集大小会显著损害性能，但进一步增加数据量并未带来实质性收益。可解释性评估表明模型的预测基于蜘蛛相关特征。类别错误分析进一步识别出与较高误差相关的视觉条件，例如远距离视图和人造/绘画的蜘蛛图像。", "conclusion": "研究结果证明了可解释的计算机视觉模型在预测恐惧评分方面的潜力，并强调了模型可解释性以及足够的数据集大小对于开发有效的、情感感知的治疗技术的重要性。"}}
{"id": "2509.04507", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04507", "abs": "https://arxiv.org/abs/2509.04507", "authors": ["Nithyashree Sivasubramaniam"], "title": "From Silent Signals to Natural Language: A Dual-Stage Transformer-LLM Approach", "comment": null, "summary": "Silent Speech Interfaces (SSIs) have gained attention for their ability to\ngenerate intelligible speech from non-acoustic signals. While significant\nprogress has been made in advancing speech generation pipelines, limited work\nhas addressed the recognition and downstream processing of synthesized speech,\nwhich often suffers from phonetic ambiguity and noise. To overcome these\nchallenges, we propose an enhanced automatic speech recognition framework that\ncombines a transformer-based acoustic model with a large language model (LLM)\nfor post-processing. The transformer captures full utterance context, while the\nLLM ensures linguistic consistency. Experimental results show a 16% relative\nand 6% absolute reduction in word error rate (WER) over a 36% baseline,\ndemonstrating substantial improvements in intelligibility for silent speech\ninterfaces.", "AI": {"tldr": "本文提出了一种结合Transformer声学模型和LLM后处理的增强型ASR框架，显著提高了静默语音接口合成语音的识别准确性，降低了词错误率。", "motivation": "静默语音接口生成的合成语音通常存在语音歧义和噪声问题，而目前对其识别和下游处理的研究有限。", "method": "本文提出了一种增强的自动语音识别（ASR）框架。该框架结合了基于Transformer的声学模型（用于捕获完整话语上下文）和大型语言模型（LLM）进行后处理（用于确保语言一致性）。", "result": "实验结果显示，相对于36%的基线，词错误率（WER）相对降低了16%，绝对降低了6%。", "conclusion": "该框架显著提高了静默语音接口生成语音的可懂度，为解决其语音识别挑战提供了有效方案。"}}
{"id": "2509.04508", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04508", "abs": "https://arxiv.org/abs/2509.04508", "authors": ["Biddut Sarker Bijoy", "Mohammad Saqib Hasan", "Pegah Alipoormolabashi", "Avirup Sil", "Aruna Balasubramanian", "Niranjan Balasubramanian"], "title": "ProST: Progressive Sub-task Training for Pareto-Optimal Multi-agent Systems Using Small Language Models", "comment": null, "summary": "Multi-agent systems with smaller language models (SLMs) present a viable\nalternative to single agent systems powered by large language models (LLMs) for\naddressing complex problems. In this work, we study how these alternatives\ncompare in terms of both effectiveness and efficiency. To study this trade-off,\nwe instantiate single and multi-agent systems for the complex problems in the\nAppWorld environment using different sized language models.\n  We find that difficulties with long-trajectory learning in smaller language\nmodels (SLMs) limit their performance. Even when trained for specialized roles,\nSLMs fail to learn all subtasks effectively. To address this issue, we\nintroduce a simple progressive sub-task training strategy, which introduces new\nsub-tasks progressively in each training epoch. We find that this novel\nstrategy, analogous to instance level curriculum learning, consistently\nimproves the effectiveness of multi-agents at all configurations. Our Pareto\nanalysis shows that fine-tuned multi-agent systems yield better\neffectiveness-efficiency trade-offs. Additional ablations and analyses shows\nthe importance of our progressive training strategy and its ability to reduce\nsubtask error rates.", "AI": {"tldr": "本研究探讨了多智能体小型语言模型（SLM）系统作为大型语言模型（LLM）单智能体系统的替代方案，并提出了一种渐进式子任务训练策略，显著提高了多智能体SLM系统的有效性和效率。", "motivation": "研究多智能体SLM系统与单智能体LLM系统在解决复杂问题时的有效性和效率权衡，并解决SLM在长轨迹学习和子任务学习中的局限性。", "method": "在AppWorld环境中实例化单智能体和多智能体系统，使用不同大小的语言模型。引入了一种渐进式子任务训练策略，在每个训练周期逐步引入新的子任务。通过帕累托分析、消融实验和其他分析来评估系统性能。", "result": "SLM在长轨迹学习中表现不佳，即使进行专业化训练也未能有效学习所有子任务。提出的渐进式子任务训练策略显著提高了多智能体在所有配置下的有效性。精细调整的多智能体系统在有效性-效率权衡方面表现更优。该训练策略能有效降低子任务错误率。", "conclusion": "多智能体SLM系统，特别是结合渐进式子任务训练策略后，能够为复杂问题提供比单智能体LLM系统更好的有效性-效率权衡，有效克服SLM在长轨迹学习中的局限性。"}}
{"id": "2509.04894", "categories": ["cs.CV", "cs.LG", "I.4"], "pdf": "https://arxiv.org/pdf/2509.04894", "abs": "https://arxiv.org/abs/2509.04894", "authors": ["Alpana Dubey", "Suma Mani Kuriakose", "Nitish Bhardwaj"], "title": "SynGen-Vision: Synthetic Data Generation for training industrial vision models", "comment": null, "summary": "We propose an approach to generate synthetic data to train computer vision\n(CV) models for industrial wear and tear detection. Wear and tear detection is\nan important CV problem for predictive maintenance tasks in any industry.\nHowever, data curation for training such models is expensive and time-consuming\ndue to the unavailability of datasets for different wear and tear scenarios.\nOur approach employs a vision language model along with a 3D simulation and\nrendering engine to generate synthetic data for varying rust conditions. We\nevaluate our approach by training a CV model for rust detection using the\ngenerated dataset and tested the trained model on real images of rusted\nindustrial objects. The model trained with the synthetic data generated by our\napproach, outperforms the other approaches with a mAP50 score of 0.87. The\napproach is customizable and can be easily extended to other industrial wear\nand tear detection scenarios", "AI": {"tldr": "该研究提出了一种利用视觉语言模型和3D模拟引擎生成合成数据的方法，用于训练工业磨损检测计算机视觉模型，并在锈蚀检测任务上取得了优异性能。", "motivation": "工业磨损检测是预测性维护中的重要计算机视觉问题，但由于缺乏不同磨损场景的数据集，数据收集和标注成本高昂且耗时，限制了模型训练。", "method": "该方法结合了视觉语言模型（VLM）与3D模拟和渲染引擎，以生成针对不同锈蚀条件的合成数据。生成的合成数据集随后用于训练计算机视觉模型进行锈蚀检测。", "result": "使用该方法生成的合成数据训练的计算机视觉模型，在真实工业锈蚀图像上的mAP50得分达到0.87，优于其他方法。该方法还具有可定制性和可扩展性，适用于其他工业磨损检测场景。", "conclusion": "所提出的合成数据生成方法能有效解决工业磨损检测中数据稀缺的问题，通过训练模型在真实数据上表现出色，并可推广到其他磨损场景。"}}
{"id": "2509.04515", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04515", "abs": "https://arxiv.org/abs/2509.04515", "authors": ["Martha O. Dimgba", "Sharon Oba", "Ameeta Agrawal", "Philippe J. Giabbanelli"], "title": "Mitigation of Gender and Ethnicity Bias in AI-Generated Stories through Model Explanations", "comment": null, "summary": "Language models have been shown to propagate social bias through their\noutput, particularly in the representation of gender and ethnicity. This paper\ninvestigates gender and ethnicity biases in AI-generated occupational stories.\nRepresentation biases are measured before and after applying our proposed\nmitigation strategy, Bias Analysis and Mitigation through Explanation (BAME),\nrevealing improvements in demographic representation ranging from 2% to 20%.\nBAME leverages model-generated explanations to inform targeted prompt\nengineering, effectively reducing biases without modifying model parameters. By\nanalyzing stories generated across 25 occupational groups, three large language\nmodels (Claude 3.5 Sonnet, Llama 3.1 70B Instruct, and GPT-4 Turbo), and\nmultiple demographic dimensions, we identify persistent patterns of\noverrepresentation and underrepresentation linked to training data stereotypes.\nOur findings demonstrate that guiding models with their own internal reasoning\nmechanisms can significantly enhance demographic parity, thereby contributing\nto the development of more transparent generative AI systems.", "AI": {"tldr": "本研究调查并缓解了AI生成职业故事中的性别和种族偏见，通过提出的BAME策略，利用模型解释进行提示工程，显著改善了人口统计学代表性。", "motivation": "语言模型已被证明会通过其输出传播社会偏见，尤其是在性别和种族表示方面。本研究旨在解决AI生成职业故事中存在的性别和种族偏见问题。", "method": "研究提出了一种名为“通过解释进行偏见分析和缓解”（BAME）的缓解策略。BAME利用模型生成的解释来指导有针对性的提示工程，从而在不修改模型参数的情况下减少偏见。偏见通过在应用BAME前后测量表示偏差来评估。研究分析了25个职业群体、三种大型语言模型（Claude 3.5 Sonnet, Llama 3.1 70B Instruct, 和 GPT-4 Turbo）以及多个人口统计学维度生成的职业故事。", "result": "应用BAME后，人口统计学代表性得到了2%到20%的改善。研究还发现，与训练数据中的刻板印象相关的过度代表和代表不足的持续模式。结果表明，通过模型自身的内部推理机制来指导模型可以显著增强人口统计学上的平等。", "conclusion": "本研究得出结论，利用模型自身的内部推理机制可以显著提高人口统计学上的平等，从而有助于开发更透明的生成式AI系统。"}}
{"id": "2509.04510", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.04510", "abs": "https://arxiv.org/abs/2509.04510", "authors": ["Michele Materazzini", "Gianluca Morciano", "Jose Manuel Alcalde-Llergo", "Enrique Yeguas-Bolivar", "Giuseppe Calabro", "Andrea Zingoni", "Juri Taborri"], "title": "Combine Virtual Reality and Machine-Learning to Identify the Presence of Dyslexia: A Cross-Linguistic Approach", "comment": "22 pages, 10 figures, 5 tables", "summary": "This study explores the use of virtual reality (VR) and artificial\nintelligence (AI) to predict the presence of dyslexia in Italian and Spanish\nuniversity students. In particular, the research investigates whether\nVR-derived data from Silent Reading (SR) tests and self-esteem assessments can\ndifferentiate between students that are affected by dyslexia and students that\nare not, employing machine learning (ML) algorithms. Participants completed\nVR-based tasks measuring reading performance and self-esteem. A preliminary\nstatistical analysis (t tests and Mann Whitney tests) on these data was\nperformed, to compare the obtained scores between individuals with and without\ndyslexia, revealing significant differences in completion time for the SR test,\nbut not in accuracy, nor in self esteem. Then, supervised ML models were\ntrained and tested, demonstrating an ability to classify the presence/absence\nof dyslexia with an accuracy of 87.5 per cent for Italian, 66.6 per cent for\nSpanish, and 75.0 per cent for the pooled group. These findings suggest that VR\nand ML can effectively be used as supporting tools for assessing dyslexia,\nparticularly by capturing differences in task completion speed, but\nlanguage-specific factors may influence classification accuracy.", "AI": {"tldr": "本研究利用虚拟现实（VR）和机器学习（ML）技术，通过无声阅读测试数据，预测意大利和西班牙大学生中是否存在阅读障碍，并取得了可观的分类准确率。", "motivation": "研究旨在探索VR和AI（特别是ML算法）是否能有效预测意大利和西班牙大学生中的阅读障碍，特别是通过VR衍生的无声阅读测试数据和自尊评估来区分患有和未患阅读障碍的学生。", "method": "参与者完成了基于VR的阅读表现和自尊评估任务。首先进行了初步统计分析（t检验和Mann Whitney检验），比较阅读障碍组和非阅读障碍组的得分。随后，训练并测试了监督式机器学习模型，以分类阅读障碍的存在与否。", "result": "统计分析显示，阅读障碍组在无声阅读测试的完成时间上存在显著差异，但在准确率和自尊方面无显著差异。机器学习模型在分类阅读障碍方面表现出：意大利组准确率为87.5%，西班牙组为66.6%，合并组为75.0%。结果表明语言特异性因素可能影响分类准确率。", "conclusion": "研究结果表明，VR和ML可以作为评估阅读障碍的有效辅助工具，尤其在捕捉任务完成速度差异方面。然而，语言特异性因素可能会影响分类的准确性。"}}
{"id": "2509.04895", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04895", "abs": "https://arxiv.org/abs/2509.04895", "authors": ["Maryam Adelipour", "Gustavo Carneiro", "Jeongkwon Kim"], "title": "Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting", "comment": "8 pages, 1 figure, 2 tables", "summary": "Sebocytes are lipid-secreting cells whose differentiation is marked by the\naccumulation of intracellular lipid droplets, making their quantification a key\nreadout in sebocyte biology. Manual counting is labor-intensive and subjective,\nmotivating automated solutions. Here, we introduce a simple attention-based\nmultiple instance learning (MIL) framework for sebocyte image analysis. Nile\nRed-stained sebocyte images were annotated into 14 classes according to droplet\ncounts, expanded via data augmentation to about 50,000 cells. Two models were\nbenchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated\npatch-level counts, and an attention-based MIL model leveraging ResNet-50\nfeatures with instance weighting. Experiments using five-fold cross-validation\nshowed that the baseline MLP achieved more stable performance (mean MAE = 5.6)\ncompared with the attention-based MIL, which was less consistent (mean MAE =\n10.7) but occasionally superior in specific folds. These findings indicate that\nsimple bag-level aggregation provides a robust baseline for slide-level droplet\ncounting, while attention-based MIL requires task-aligned pooling and\nregularization to fully realize its potential in sebocyte image analysis.", "AI": {"tldr": "本研究引入了一种基于注意力机制的多实例学习（MIL）框架，用于自动化皮脂细胞脂滴计数。结果表明，简单的包级聚合模型（MLP）比注意力MIL模型更稳定，是脂滴计数的可靠基线。", "motivation": "皮脂细胞分化以细胞内脂滴积累为标志，其定量是皮脂细胞生物学中的关键指标。手动计数费时且主观，因此需要自动化的解决方案。", "method": "研究开发了一个简单的基于注意力机制的多实例学习（MIL）框架。使用尼罗红染色的皮脂细胞图像根据脂滴数量被标注为14个类别，并通过数据增强扩展到约50,000个细胞。对比了两种模型：一个基于聚合补丁级计数的基线多层感知器（MLP），以及一个利用ResNet-50特征和实例加权的基于注意力机制的MIL模型。实验采用五折交叉验证。", "result": "基线MLP模型表现出更稳定的性能（平均MAE = 5.6），而基于注意力机制的MIL模型一致性较差（平均MAE = 10.7），但在特定折叠中偶尔表现更优。", "conclusion": "研究结果表明，简单的包级聚合为玻片级脂滴计数提供了一个鲁棒的基线。基于注意力机制的MIL模型需要任务对齐的池化和正则化，才能充分发挥其在皮脂细胞图像分析中的潜力。"}}
{"id": "2509.04534", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04534", "abs": "https://arxiv.org/abs/2509.04534", "authors": ["Zaifu Zhan", "Shuang Zhou", "Min Zeng", "Kai Yu", "Meijia Song", "Xiaoyi Chen", "Jun Wang", "Yu Hou", "Rui Zhang"], "title": "Quantized Large Language Models in Biomedical Natural Language Processing: Evaluation and Recommendation", "comment": "11 pages, 7 figures", "summary": "Large language models have demonstrated remarkable capabilities in biomedical\nnatural language processing, yet their rapid growth in size and computational\nrequirements present a major barrier to adoption in healthcare settings where\ndata privacy precludes cloud deployment and resources are limited. In this\nstudy, we systematically evaluated the impact of quantization on 12\nstate-of-the-art large language models, including both general-purpose and\nbiomedical-specific models, across eight benchmark datasets covering four key\ntasks: named entity recognition, relation extraction, multi-label\nclassification, and question answering. We show that quantization substantially\nreduces GPU memory requirements-by up to 75%-while preserving model performance\nacross diverse tasks, enabling the deployment of 70B-parameter models on 40GB\nconsumer-grade GPUs. In addition, domain-specific knowledge and responsiveness\nto advanced prompting methods are largely maintained. These findings provide\nsignificant practical and guiding value, highlighting quantization as a\npractical and effective strategy for enabling the secure, local deployment of\nlarge yet high-capacity language models in biomedical contexts, bridging the\ngap between technical advances in AI and real-world clinical translation.", "AI": {"tldr": "本研究表明，量化能显著降低大型语言模型（LLMs）的GPU内存需求（高达75%），同时保持性能，从而实现在资源受限的生物医学环境中安全、本地部署LLMs。", "motivation": "大型语言模型在生物医学自然语言处理中表现出色，但其巨大的模型尺寸和计算需求阻碍了在医疗保健领域的应用，因为数据隐私要求本地部署且资源有限。", "method": "研究系统地评估了量化对12个最先进的大型语言模型（包括通用和生物医学专用模型）的影响，并在涵盖命名实体识别、关系提取、多标签分类和问答四项关键任务的八个基准数据集上进行了测试。", "result": "量化将GPU内存需求减少了高达75%，同时在各种任务中保持了模型性能，使得70B参数的模型可以在40GB消费级GPU上部署。此外，模型的领域特定知识和对高级提示方法的响应能力也得到了很大程度的保留。", "conclusion": "量化是一种实用且有效的策略，能够实现大型且高性能语言模型在生物医学环境中的安全、本地部署，弥合了AI技术进步与实际临床转化之间的鸿沟。"}}
{"id": "2509.04512", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04512", "abs": "https://arxiv.org/abs/2509.04512", "authors": ["Edoardo Pinzuti", "Oliver Tüscher", "André Ferreira Castro"], "title": "Scaling behavior of large language models in emotional safety classification across sizes and tasks", "comment": null, "summary": "Understanding how large language models (LLMs) process emotionally sensitive\ncontent is critical for building safe and reliable systems, particularly in\nmental health contexts. We investigate the scaling behavior of LLMs on two key\ntasks: trinary classification of emotional safety (safe vs. unsafe vs.\nborderline) and multi-label classification using a six-category safety risk\ntaxonomy. To support this, we construct a novel dataset by merging several\nhuman-authored mental health datasets (> 15K samples) and augmenting them with\nemotion re-interpretation prompts generated via ChatGPT. We evaluate four LLaMA\nmodels (1B, 3B, 8B, 70B) across zero-shot, few-shot, and fine-tuning settings.\nOur results show that larger LLMs achieve stronger average performance,\nparticularly in nuanced multi-label classification and in zero-shot settings.\nHowever, lightweight fine-tuning allowed the 1B model to achieve performance\ncomparable to larger models and BERT in several high-data categories, while\nrequiring <2GB VRAM at inference. These findings suggest that smaller,\non-device models can serve as viable, privacy-preserving alternatives for\nsensitive applications, offering the ability to interpret emotional context and\nmaintain safe conversational boundaries. This work highlights key implications\nfor therapeutic LLM applications and the scalable alignment of safety-critical\nsystems.", "AI": {"tldr": "该研究调查了大型语言模型（LLMs）处理情感敏感内容的扩展行为，发现虽然更大模型性能更强，但经过轻量级微调的小型模型也能在隐私敏感应用中表现出色。", "motivation": "理解LLMs如何处理情感敏感内容对于构建安全可靠的系统至关重要，尤其是在心理健康领域。", "method": "研究采用两种分类任务：情感安全三元分类（安全、不安全、临界）和六类别安全风险多标签分类。构建了一个包含超过1.5万样本的新型数据集，该数据集结合了人工编写的心理健康数据并使用ChatGPT生成的提示进行情感重新解释增强。评估了四种LLaMA模型（1B、3B、8B、70B），并在零样本、少样本和微调设置下进行测试。", "result": "结果显示，更大的LLMs通常表现出更强的平均性能，尤其是在细致的多标签分类和零样本设置中。然而，通过轻量级微调，1B模型在几个高数据类别中达到了与更大模型和BERT相当的性能，且推理时仅需不到2GB的显存。", "conclusion": "这些发现表明，小型、可在设备上运行的模型可以作为敏感应用的有效、隐私保护替代方案，能够解释情感上下文并维持安全的对话边界。这对于治疗性LLM应用和安全关键系统的可扩展对齐具有重要意义。"}}
{"id": "2509.04932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04932", "abs": "https://arxiv.org/abs/2509.04932", "authors": ["Haowang Cui", "Rui Chen", "Tao Luo", "Rui Li", "Jiaze Wang"], "title": "UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features", "comment": "Submitted to ACM TOMM", "summary": "The task of synthesizing novel views from a single image is highly ill-posed\ndue to multiple explanations for unobserved areas. Most current methods tend to\ngenerate unseen regions from ambiguity priors and interpolation near input\nviews, which often lead to severe distortions. To address this limitation, we\npropose a novel model dubbed as UniView, which can leverage reference images\nfrom a similar object to provide strong prior information during view\nsynthesis. More specifically, we construct a retrieval and augmentation system\nand employ a multimodal large language model (MLLM) to assist in selecting\nreference images that meet our requirements. Additionally, a plug-and-play\nadapter module with multi-level isolation layers is introduced to dynamically\ngenerate reference features for the target views. Moreover, in order to\npreserve the details of an original input image, we design a decoupled triple\nattention mechanism, which can effectively align and integrate multi-branch\nfeatures into the synthesis process. Extensive experiments have demonstrated\nthat our UniView significantly improves novel view synthesis performance and\noutperforms state-of-the-art methods on the challenging datasets.", "AI": {"tldr": "UniView是一种新颖的单图新视角合成模型，通过利用检索到的相似物体参考图像和多模态大语言模型（MLLM）提供强先验信息，并结合适配器模块和解耦三重注意力机制，显著改善了合成质量并超越了现有技术。", "motivation": "从单张图像合成新视角是一个高度不适定的任务，因为未观察区域存在多种解释。大多数现有方法依赖模糊先验和输入视图附近的插值来生成未见区域，这经常导致严重的失真。", "method": "本文提出了UniView模型，其方法包括：1) 构建一个检索和增强系统，并利用多模态大语言模型（MLLM）辅助选择符合要求的参考图像；2) 引入一个带有多层隔离层的即插即用适配器模块，用于动态生成目标视图的参考特征；3) 设计一个解耦三重注意力机制，以有效对齐和整合多分支特征到合成过程中，从而保留原始输入图像的细节。", "result": "广泛的实验证明，UniView显著提高了新视角合成性能，并在具有挑战性的数据集上超越了最先进的方法。", "conclusion": "UniView通过有效利用相似物体的参考图像作为强先验信息，并结合创新的特征生成和整合机制，成功克服了单图新视角合成的局限性，实现了卓越的合成效果。"}}
{"id": "2509.04549", "categories": ["cs.CL", "cs.AI", "68T50, 68T05", "I.2.7; I.2.6; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.04549", "abs": "https://arxiv.org/abs/2509.04549", "authors": ["Faruk Alpay", "Taylan Alpay"], "title": "Manipulating Transformer-Based Models: Controllability, Steerability, and Robust Interventions", "comment": "13 pages", "summary": "Transformer-based language models excel in NLP tasks, but fine-grained\ncontrol remains challenging. This paper explores methods for manipulating\ntransformer models through principled interventions at three levels: prompts,\nactivations, and weights. We formalize controllable text generation as an\noptimization problem addressable via prompt engineering, parameter-efficient\nfine-tuning, model editing, and reinforcement learning. We introduce a unified\nframework encompassing prompt-level steering, activation interventions, and\nweight-space edits. We analyze robustness and safety implications, including\nadversarial attacks and alignment mitigations. Theoretically, we show minimal\nweight updates can achieve targeted behavior changes with limited side-effects.\nEmpirically, we demonstrate >90% success in sentiment control and factual edits\nwhile preserving base performance, though generalization-specificity trade-offs\nexist. We discuss ethical dual-use risks and the need for rigorous evaluation.\nThis work lays groundwork for designing controllable and robust language\nmodels.", "AI": {"tldr": "本文探索了通过提示、激活和权重干预，在三个层面实现Transformer模型精细控制的方法，并提出了一个统一框架，在保持基础性能的同时实现了高成功率的文本生成控制。", "motivation": "尽管基于Transformer的语言模型在NLP任务中表现出色，但对其进行精细控制仍然是一个挑战。", "method": "本文将可控文本生成形式化为一个优化问题，可通过提示工程、参数高效微调、模型编辑和强化学习来解决。研究引入了一个统一框架，涵盖提示级引导、激活干预和权重空间编辑。理论上，作者展示了最小的权重更新即可实现有针对性的行为改变，且副作用有限。", "result": "经验证，在情感控制和事实编辑方面取得了超过90%的成功率，同时保留了基础性能。研究还分析了鲁棒性和安全隐患，包括对抗性攻击和对齐缓解措施，但存在泛化-特异性权衡。", "conclusion": "这项工作为设计可控且鲁棒的语言模型奠定了基础，并讨论了伦理双重用途风险以及严格评估的必要性。"}}
{"id": "2509.04516", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.04516", "abs": "https://arxiv.org/abs/2509.04516", "authors": ["Sophie Jaffer", "Simeon Sayer"], "title": "Artificially Fluent: Swahili AI Performance Benchmarks Between English-Trained and Natively-Trained Datasets", "comment": "13 Pages, 3 Figures", "summary": "As large language models (LLMs) expand multilingual capabilities, questions\nremain about the equity of their performance across languages. While many\ncommunities stand to benefit from AI systems, the dominance of English in\ntraining data risks disadvantaging non-English speakers. To test the hypothesis\nthat such data disparities may affect model performance, this study compares\ntwo monolingual BERT models: one trained and tested entirely on Swahili data,\nand another on comparable English news data. To simulate how multilingual LLMs\nprocess non-English queries through internal translation and abstraction, we\ntranslated the Swahili news data into English and evaluated it using the\nEnglish-trained model. This approach tests the hypothesis by evaluating whether\ntranslating Swahili inputs for evaluation on an English model yields better or\nworse performance compared to training and testing a model entirely in Swahili,\nthus isolating the effect of language consistency versus cross-lingual\nabstraction. The results prove that, despite high-quality translation, the\nnative Swahili-trained model performed better than the Swahili-to-English\ntranslated model, producing nearly four times fewer errors: 0.36% vs. 1.47%\nrespectively. This gap suggests that translation alone does not bridge\nrepresentational differences between languages and that models trained in one\nlanguage may struggle to accurately interpret translated inputs due to\nimperfect internal knowledge representation, suggesting that native-language\ntraining remains important for reliable outcomes. In educational and\ninformational contexts, even small performance gaps may compound inequality.\nFuture research should focus on addressing broader dataset development for\nunderrepresented languages and renewed attention to multilingual model\nevaluation, ensuring the reinforcing effect of global AI deployment on existing\ndigital divides is reduced.", "AI": {"tldr": "本研究发现，对于斯瓦希里语，原生语言训练的模型表现明显优于通过翻译处理的英文训练模型，表明语言一致性对模型性能至关重要。", "motivation": "随着大型语言模型（LLMs）多语言能力的扩展，其在不同语言间的性能公平性受到质疑。英文训练数据的主导地位可能使非英文使用者处于不利地位，因此需要检验数据差异是否影响模型性能。", "method": "研究比较了两个单语BERT模型：一个完全用斯瓦希里语数据训练和测试，另一个用可比的英语新闻数据训练。为了模拟多语言LLM处理非英语查询的方式，研究将斯瓦希里语新闻数据翻译成英语，并用英语训练的模型进行评估。然后将此性能与完全用斯瓦希里语训练和测试的模型进行比较，以隔离语言一致性与跨语言抽象的影响。", "result": "原生斯瓦希里语训练的模型表现优于翻译成英语后用英语模型评估的模型，错误率分别为0.36%和1.47%，前者错误率是后者的近四分之一。这表明即使有高质量翻译，翻译也无法弥合语言间的表征差异，并且用一种语言训练的模型可能难以准确解释翻译后的输入。", "conclusion": "原生语言训练对于获得可靠结果仍然重要。即使是小的性能差距也可能加剧教育和信息领域的不平等。未来的研究应关注欠代表语言的数据集开发和多语言模型评估，以减少全球AI部署对现有数字鸿沟的强化效应。"}}
{"id": "2509.04957", "categories": ["cs.CV", "cs.MM", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.04957", "abs": "https://arxiv.org/abs/2509.04957", "authors": ["Gehui Chen", "Guan'an Wang", "Xiaowen Huang", "Jitao Sang"], "title": "Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper", "comment": null, "summary": "Recent Video-to-Audio (V2A) generation relies on extracting semantic and\ntemporal features from video to condition generative models. Training these\nmodels from scratch is resource intensive. Consequently, leveraging foundation\nmodels (FMs) has gained traction due to their cross-modal knowledge transfer\nand generalization capabilities. One prior work has explored fine-tuning a\nlightweight mapper network to connect a pre-trained visual encoder with a\ntext-to-audio generation model for V2A. Inspired by this, we introduce the\nMultiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper\napproach, MFM-Mapper benefits from richer semantic and temporal information by\nfusing features from dual visual encoders. Furthermore, by replacing a linear\nmapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels\nbetween cross-modal features mapping and autoregressive translation tasks. Our\nMFM-Mapper exhibits remarkable training efficiency. It achieves better\nperformance in semantic and temporal consistency with fewer training consuming,\nrequiring only 16\\% of the training scale compared to previous mapper-based\nwork, yet achieves competitive performance with models trained on a much larger\nscale.", "AI": {"tldr": "本文提出MFM-Mapper，通过融合双视觉编码器特征并使用GPT-2作为映射器，显著提高了视频到音频（V2A）生成的效率和性能。", "motivation": "从头训练视频到音频（V2A）生成模型资源消耗巨大。现有利用预训练基础模型的工作通过轻量级映射器连接单一视觉编码器和文本到音频模型，但可能未能充分利用语义和时序信息，且特征对齐有待提升。", "method": "引入MFM-Mapper，通过融合来自双视觉编码器的特征，以获取更丰富的语义和时序信息。将之前的线性映射器替换为GPT-2，将跨模态特征映射视为自回归翻译任务，从而改善特征对齐。", "result": "MFM-Mapper展现出卓越的训练效率，训练规模仅为先前基于映射器工作的16%。在语义和时序一致性方面表现更佳，并能与更大规模训练的模型达到竞争性性能。", "conclusion": "MFM-Mapper通过结合双视觉编码器和GPT-2映射器，显著提升了V2A生成的训练效率，并在语义和时序一致性上取得了更好的性能。"}}
{"id": "2509.04606", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04606", "abs": "https://arxiv.org/abs/2509.04606", "authors": ["Osman Batur İnce", "André F. T. Martins", "Oisin Mac Aodha", "Edoardo M. Ponti"], "title": "Sample-efficient Integration of New Modalities into Large Language Models", "comment": "Pre-print", "summary": "Multimodal foundation models can process several modalities. However, since\nthe space of possible modalities is large and evolving over time, training a\nmodel from scratch to encompass all modalities is unfeasible. Moreover,\nintegrating a modality into a pre-existing foundation model currently requires\na significant amount of paired data, which is often not available for\nlow-resource modalities. In this paper, we introduce a method for\nsample-efficient modality integration (SEMI) into Large Language Models (LLMs).\nTo this end, we devise a hypernetwork that can adapt a shared projector --\nplaced between modality-specific encoders and an LLM -- to any modality. The\nhypernetwork, trained on high-resource modalities (i.e., text, speech, audio,\nvideo), is conditioned on a few samples from any arbitrary modality at\ninference time to generate a suitable adapter. To increase the diversity of\ntraining modalities, we artificially multiply the number of encoders through\nisometric transformations. We find that SEMI achieves a significant boost in\nsample efficiency during few-shot integration of new modalities (i.e.,\nsatellite images, astronomical images, inertial measurements, and molecules)\nwith encoders of arbitrary embedding dimensionality. For instance, to reach the\nsame accuracy as 32-shot SEMI, training the projector from scratch needs\n64$\\times$ more data. As a result, SEMI holds promise to extend the modality\ncoverage of foundation models.", "AI": {"tldr": "本文提出了一种名为SEMI（样本高效模态集成）的方法，通过超网络实现样本高效地将新模态集成到大型语言模型（LLMs）中，尤其适用于低资源模态。", "motivation": "多模态基础模型难以涵盖所有不断演进的模态，从头训练不切实际。将新模态集成到现有模型中通常需要大量配对数据，而低资源模态往往缺乏此类数据。", "method": "该方法设计了一个超网络，用于调整一个共享的投影器（位于模态特定编码器和LLM之间）。超网络在推理时通过任意模态的少量样本进行条件化，以生成合适的适配器，并在高资源模态（文本、语音、音频、视频）上进行训练。通过等距变换人工增加编码器数量，以提高训练模态的多样性。", "result": "SEMI在少量样本集成新模态（如卫星图像、天文图像、惯性测量和分子）时，显著提高了样本效率，并能处理任意嵌入维度的编码器。例如，达到与32样本SEMI相同的精度，从头训练投影器需要多64倍的数据。", "conclusion": "SEMI有望扩展基础模型的模态覆盖范围，为低资源模态的集成提供了有效途径。"}}
{"id": "2509.04517", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04517", "abs": "https://arxiv.org/abs/2509.04517", "authors": ["Indu Bala", "Lewis Mitchell", "Marianne H Gillam"], "title": "Analysis of Voluntarily Reported Data Post Mesh Implantation for Detecting Public Emotion and Identifying Concern Reports", "comment": null, "summary": "Mesh implants are widely utilized in hernia repair surgeries, but\npostoperative complications present a significant concern. This study analyzes\npatient reports from the Manufacturer and User Facility Device Experience\n(MAUDE) database spanning 2000 to 2021 to investigate the emotional aspects of\npatients following mesh implantation using Natural Language Processing (NLP).\nEmploying the National Research Council Canada (NRC) Emotion Lexicon and\nTextBlob for sentiment analysis, the research categorizes patient narratives\ninto eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy,\nand disgust) and assesses sentiment polarity. The goal is to discern patterns\nin patient sentiment over time and to identify reports signaling urgent\nconcerns, referred to as \"Concern Reports,\" thereby understanding shifts in\npatient experiences in relation to changes in medical device regulation and\ntechnological advancements in healthcare. The study detected an increase in\nConcern Reports and higher emotional intensity during the periods of 2011-2012\nand 2017-2018. Through temporal analysis of Concern Reports and overall\nsentiment, this research provides valuable insights for healthcare\npractitioners, enhancing their understanding of patient experiences\npost-surgery, which is critical for improving preoperative counselling,\npostoperative care, and preparing patients for mesh implant surgeries. The\nstudy underscores the importance of emotional considerations in medical\npractices and the potential for sentiment analysis to inform and enhance\npatient care.", "AI": {"tldr": "本研究利用NLP分析MAUDE数据库中2000-2021年间患者对网片植入术后情感报告，发现特定时期（2011-2012和2017-2018）“关注报告”和情感强度增加，为改善术前咨询和术后护理提供见解。", "motivation": "疝气修复手术中网片植入术后并发症是一个重要问题。研究旨在通过分析患者报告，了解患者植入网片后的情感体验，并探究这些体验如何随医疗设备监管和技术进步而变化，以期改善患者护理。", "method": "研究使用自然语言处理（NLP）技术，分析了2000年至2021年美国制造商和用户设施设备经验（MAUDE）数据库中的患者报告。采用加拿大国家研究委员会（NRC）情感词典将患者叙述分为八种情绪（愤怒、恐惧、预期、信任、惊喜、悲伤、喜悦和厌恶），并利用TextBlob进行情感极性分析。研究还识别了表示紧急关注的“关注报告”，并进行了时间序列分析。", "result": "研究发现，“关注报告”和情感强度在2011-2012年和2017-2018年期间有所增加。通过对“关注报告”和整体情感的时间分析，揭示了患者术后体验的模式和变化。", "conclusion": "本研究为医护人员提供了宝贵的见解，加深了他们对患者术后体验的理解，这对改善术前咨询、术后护理和患者术前准备至关重要。研究强调了在医疗实践中考虑情感因素的重要性，以及情感分析在指导和增强患者护理方面的潜力。"}}
{"id": "2509.05000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05000", "abs": "https://arxiv.org/abs/2509.05000", "authors": ["Tianpei Zhang", "Jufeng Zhao", "Yiming Zhu", "Guangmang Cui"], "title": "Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework", "comment": null, "summary": "Most existing infrared-visible image fusion (IVIF) methods assume\nhigh-quality inputs, and therefore struggle to handle dual-source degraded\nscenarios, typically requiring manual selection and sequential application of\nmultiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion\npipeline inevitably leads to error accumulation and performance degradation. To\novercome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),\na novel framework that synergistically integrates vision-language models (VLMs)\nfor degradation perception with dual-domain (frequency/spatial) joint\noptimization. Concretely, the designed Guided Frequency Modality-Specific\nExtraction (GFMSE) module performs frequency-domain degradation perception and\nsuppression and discriminatively extracts fusion-relevant sub-band features.\nMeanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries\nout cross-modal degradation filtering and adaptive multi-source feature\naggregation in the spatial domain to enhance modality complementarity and\nstructural consistency. Extensive qualitative and quantitative experiments\ndemonstrate that GD^2Fusion achieves superior fusion performance compared with\nexisting algorithms and strategies in dual-source degraded scenarios. The code\nwill be publicly released after acceptance of this paper.", "AI": {"tldr": "本文提出了一种名为GD^2Fusion的新型红外-可见光图像融合（IVIF）框架，通过结合视觉-语言模型（VLMs）进行退化感知和双域（频率/空间）联合优化，有效处理双源图像退化场景。", "motivation": "现有IVIF方法假定输入图像质量高，难以处理双源退化场景，需要手动且顺序应用预增强步骤。这种解耦的“预增强-融合”流程会导致误差累积和性能下降。", "method": "GD^2Fusion框架协同整合了视觉-语言模型（VLMs）进行退化感知，并结合了双域（频率/空间）联合优化。具体包括：1) 引导式频率模态特定提取（GFMSE）模块，用于频率域的退化感知、抑制和鉴别性特征提取。2) 引导式空间模态聚合融合（GSMAF）模块，用于空间域的跨模态退化滤波和自适应多源特征聚合，以增强模态互补性和结构一致性。", "result": "广泛的定性和定量实验表明，GD^2Fusion在双源退化场景下，与现有算法和策略相比，实现了卓越的融合性能。", "conclusion": "GD^2Fusion通过其创新的框架，成功克服了传统IVIF方法在处理退化输入时的局限性，显著提升了融合效果。"}}
{"id": "2509.04650", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04650", "abs": "https://arxiv.org/abs/2509.04650", "authors": ["Sharif Noor Zisad", "Ragib Hasan"], "title": "Comparative Analysis of Transformer Models in Disaster Tweet Classification for Public Safety", "comment": null, "summary": "Twitter and other social media platforms have become vital sources of real\ntime information during disasters and public safety emergencies. Automatically\nclassifying disaster related tweets can help emergency services respond faster\nand more effectively. Traditional Machine Learning (ML) models such as Logistic\nRegression, Naive Bayes, and Support Vector Machines have been widely used for\nthis task, but they often fail to understand the context or deeper meaning of\nwords, especially when the language is informal, metaphorical, or ambiguous. We\nposit that, in this context, transformer based models can perform better than\ntraditional ML models. In this paper, we evaluate the effectiveness of\ntransformer based models, including BERT, DistilBERT, RoBERTa, and DeBERTa, for\nclassifying disaster related tweets. These models are compared with traditional\nML approaches to highlight the performance gap. Experimental results show that\nBERT achieved the highest accuracy (91%), significantly outperforming\ntraditional models like Logistic Regression and Naive Bayes (both at 82%). The\nuse of contextual embeddings and attention mechanisms allows transformer models\nto better understand subtle language in tweets, where traditional ML models\nfall short. This research demonstrates that transformer architectures are far\nmore suitable for public safety applications, offering improved accuracy,\ndeeper language understanding, and better generalization across real world\nsocial media text.", "AI": {"tldr": "本研究评估了Transformer模型在灾害相关推文分类中的有效性，发现它们显著优于传统机器学习模型，其中BERT表现最佳。", "motivation": "Twitter等社交媒体在灾害期间提供实时信息至关重要，自动分类灾害推文有助于应急响应。然而，传统机器学习模型难以理解推文中非正式、隐喻或模糊的语言上下文。", "method": "研究评估了包括BERT、DistilBERT、RoBERTa和DeBERTa在内的Transformer模型，用于分类灾害相关推文。这些模型与传统的机器学习方法（如逻辑回归、朴素贝叶斯和支持向量机）进行了比较。", "result": "实验结果显示，BERT取得了最高的准确率（91%），显著优于传统模型如逻辑回归和朴素贝叶斯（均为82%）。Transformer模型通过上下文嵌入和注意力机制，能更好地理解推文中细微的语言。", "conclusion": "Transformer架构更适用于公共安全应用，它们提供了更高的准确性、更深层次的语言理解以及在真实世界社交媒体文本中更好的泛化能力。"}}
{"id": "2509.04518", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04518", "abs": "https://arxiv.org/abs/2509.04518", "authors": ["Dhruvi Paprunia", "Vansh Kharidia", "Pankti Doshi"], "title": "Advancing SLM Tool-Use Capability using Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have progressed beyond simple text creation, and\ntool use has become increasingly important for complex, real-world tasks. Tool\nuse in LLMs refers to their ability to utilize external resources such as APIs,\ndatabases, or software functions to extend their functionality beyond\ngenerating text.Tools are used for tasks such as performing calculations,\nmaking API calls to retrieve the current time and date, and more. This\ncapability enables models to fetch real-time data, execute commands, or solve\nproblems requiring dynamic interaction, making it indispensable for\napplications like AI agents in virtual assistants, robotic control, or\nautomated workflows.\n  However, while LLMs are usually adept tool use, their vast resource\nrequirements and computation complexity restrict their use in every use case.As\na result, there is an increasing need for more compact and efficient Small\nLanguage Models (SLMs). Small language models (SLMs) struggle in tool use\ncompared to large language models (LLMs). As soon in Table 1. SLMs are\ntypically trained on smaller, more specific datasets, resulting in a narrower\nknowledge base and limited contextual understanding compared to LLMs.\n  This research addresses these challenges by using Reinforcement Learning\n(RL), specifically Group Relative Policy Optimization (GRPO), to enhance\ntool-use proficiency in SLMs. Unlike conventional fine-tuning approaches that\nrequire heavy computation and often lack adaptability, our method provides an\nefficient, effective solution that significantly boosts SLM tool-use accuracy,\nincreasing their practical utility.", "AI": {"tldr": "本研究利用强化学习（特别是GRPO）来提升小型语言模型（SLMs）的工具使用能力，以解决其在工具使用方面相较于大型语言模型（LLMs）的不足。", "motivation": "大型语言模型（LLMs）在工具使用方面表现出色，但其巨大的资源需求和计算复杂性限制了其广泛应用。小型语言模型（SLMs）虽然更紧凑高效，但在工具使用方面表现不佳，知识库和上下文理解能力有限。因此，需要一种方法来增强SLMs的工具使用能力。", "method": "本研究采用强化学习（RL），具体是群组相对策略优化（Group Relative Policy Optimization, GRPO）方法，来提升SLMs的工具使用熟练度。与传统的微调方法不同，该方法旨在提供一个高效且适应性强的解决方案。", "result": "研究结果表明，该方法显著提高了SLMs的工具使用准确性，从而增加了它们的实际应用价值。", "conclusion": "本研究提供了一种高效、有效的解决方案，通过强化学习（GRPO）显著提升了小型语言模型（SLMs）的工具使用能力，增强了它们的实用性。"}}
{"id": "2509.05004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05004", "abs": "https://arxiv.org/abs/2509.05004", "authors": ["Mohammad Abbadi", "Yassine Himeur", "Shadi Atalla", "Wathiq Mansoor"], "title": "Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study", "comment": "6 pages, 2 figures and 1 table", "summary": "Breast cancer remains a leading cause of cancer-related mortality among women\nworldwide. Ultrasound imaging, widely used due to its safety and\ncost-effectiveness, plays a key role in early detection, especially in patients\nwith dense breast tissue. This paper presents a comprehensive study on the\napplication of machine learning and deep learning techniques for breast cancer\nclassification using ultrasound images. Using datasets such as BUSI, BUS-BRA,\nand BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,\nKNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,\nGoogLeNet). Experimental results show that ResNet-18 achieves the highest\naccuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML\nmodels, though outperformed by CNNs, achieve competitive performance when\nenhanced with deep feature extraction. Grad-CAM visualizations further improve\nmodel transparency by highlighting diagnostically relevant image regions. These\nfindings support the integration of AI-based diagnostic tools into clinical\nworkflows and demonstrate the feasibility of deploying high-performing,\ninterpretable systems for ultrasound-based breast cancer detection.", "AI": {"tldr": "本研究利用机器学习和深度学习技术，对乳腺超声图像进行乳腺癌分类，并取得了高准确率和可解释性。", "motivation": "乳腺癌是女性癌症相关死亡的主要原因。超声成像因其安全性和成本效益在早期检测中发挥关键作用，尤其是在乳腺组织致密的患者中。本研究旨在通过AI技术进一步提升乳腺超声图像的诊断能力。", "method": "研究使用了BUSI、BUS-BRA和BrEaST-Lesions USG等数据集，评估了经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet）。同时，还探讨了通过深度特征提取增强经典机器学习模型的方法，并使用Grad-CAM进行模型可视化和可解释性分析。", "result": "实验结果显示，ResNet-18实现了最高准确率（99.7%）和恶性病变完美敏感性。经典机器学习模型在通过深度特征提取增强后，也取得了具有竞争力的性能。Grad-CAM可视化进一步提高了模型透明度，突出了诊断相关的图像区域。", "conclusion": "研究结果支持将基于AI的诊断工具整合到临床工作流程中，并证明了部署高性能、可解释的超声乳腺癌检测系统的可行性。"}}
{"id": "2509.04655", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04655", "abs": "https://arxiv.org/abs/2509.04655", "authors": ["Ayush Gupta", "Ramneet Kaur", "Anirban Roy", "Adam D. Cobb", "Rama Chellappa", "Susmit Jha"], "title": "Polysemantic Dropout: Conformal OOD Detection for Specialized LLMs", "comment": "Accepted to EMNLP 2025 main conference", "summary": "We propose a novel inference-time out-of-domain (OOD) detection algorithm for\nspecialized large language models (LLMs). Despite achieving state-of-the-art\nperformance on in-domain tasks through fine-tuning, specialized LLMs remain\nvulnerable to incorrect or unreliable outputs when presented with OOD inputs,\nposing risks in critical applications. Our method leverages the Inductive\nConformal Anomaly Detection (ICAD) framework, using a new non-conformity\nmeasure based on the model's dropout tolerance. Motivated by recent findings on\npolysemanticity and redundancy in LLMs, we hypothesize that in-domain inputs\nexhibit higher dropout tolerance than OOD inputs. We aggregate dropout\ntolerance across multiple layers via a valid ensemble approach, improving\ndetection while maintaining theoretical false alarm bounds from ICAD.\nExperiments with medical-specialized LLMs show that our approach detects OOD\ninputs better than baseline methods, with AUROC improvements of $2\\%$ to $37\\%$\nwhen treating OOD datapoints as positives and in-domain test datapoints as\nnegatives.", "AI": {"tldr": "本文提出了一种针对专业化大型语言模型（LLMs）的新型推理时域外（OOD）检测算法，该算法利用模型对Dropout的容忍度，结合归纳共形异常检测（ICAD）框架来识别OOD输入。", "motivation": "尽管专业化LLMs在域内任务上表现出色，但当遇到域外（OOD）输入时，它们仍然容易产生不正确或不可靠的输出，这在关键应用中带来了风险。因此，需要有效的方法来检测OOD输入以提高模型可靠性。", "method": "该方法基于归纳共形异常检测（ICAD）框架，并引入了一种新的非一致性度量，该度量基于模型对Dropout的容忍度。研究假设域内输入比OOD输入表现出更高的Dropout容忍度。通过有效的集成方法，在多个层聚合Dropout容忍度，以提高检测效果，同时保持ICAD的理论误报边界。", "result": "在医学专业化LLMs上的实验表明，该方法在检测OOD输入方面优于基线方法，当将OOD数据点视为正例、域内测试数据点视为负例时，AUROC（曲线下面积）提高了2%到37%。", "conclusion": "所提出的基于Dropout容忍度和ICAD的OOD检测算法能有效识别专业化LLMs的域外输入，显著提升了模型在关键应用中的可靠性和安全性。"}}
{"id": "2509.04519", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04519", "abs": "https://arxiv.org/abs/2509.04519", "authors": ["Zvi Badash", "Hadas Ben-Atya", "Naama Gavrielov", "Liam Hazan", "Gili Focht", "Ruth Cytter-Kuint", "Talar Hagopian", "Dan Turner", "Moti Freiman"], "title": "Hierarchical Section Matching Prediction (HSMP) BERT for Fine-Grained Extraction of Structured Data from Hebrew Free-Text Radiology Reports in Crohn's Disease", "comment": null, "summary": "Extracting structured clinical information from radiology reports is\nchallenging, especially in low-resource languages. This is pronounced in\nCrohn's disease, with sparsely represented multi-organ findings. We developed\nHierarchical Structured Matching Prediction BERT (HSMP-BERT), a prompt-based\nmodel for extraction from Hebrew radiology text. In an administrative database\nstudy, we analyzed 9,683 reports from Crohn's patients imaged 2010-2023 across\nIsraeli providers. A subset of 512 reports was radiologist-annotated for\nfindings across six gastrointestinal organs and 15 pathologies, yielding 90\nstructured labels per subject. Multilabel-stratified split (66%\ntrain+validation; 33% test), preserving label prevalence. Performance was\nevaluated with accuracy, F1, Cohen's $\\kappa$, AUC, PPV, NPV, and recall. On 24\norgan-finding combinations with $>$15 positives, HSMP-BERT achieved mean F1\n0.83$\\pm$0.08 and $\\kappa$ 0.65$\\pm$0.17, outperforming the SMP zero-shot\nbaseline (F1 0.49$\\pm$0.07, $\\kappa$ 0.06$\\pm$0.07) and standard fine-tuning\n(F1 0.30$\\pm$0.27, $\\kappa$ 0.27$\\pm$0.34; paired t-test $p < 10^{-7}$).\nHierarchical inference cuts runtime 5.1$\\times$ vs. traditional inference.\nApplied to all reports, it revealed associations among ileal wall thickening,\nstenosis, and pre-stenotic dilatation, plus age- and sex-specific trends in\ninflammatory findings. HSMP-BERT offers a scalable solution for structured\nextraction in radiology, enabling population-level analysis of Crohn's disease\nand demonstrating AI's potential in low-resource settings.", "AI": {"tldr": "本文开发了HSMP-BERT模型，一种基于提示的BERT模型，用于从希伯来语放射学报告中提取克罗恩病（Crohn's disease）的结构化临床信息。该模型在低资源语言环境下表现出色，显著优于基线模型，并能实现人群层面的疾病分析。", "motivation": "从放射学报告中提取结构化临床信息具有挑战性，尤其是在资源匮乏的语言（如希伯来语）中。对于克罗恩病这种涉及多器官且发现稀疏的疾病，这一挑战更为突出。因此，研究的动机是开发一种有效的解决方案来解决这一问题。", "method": "研究开发了HSMP-BERT（Hierarchical Structured Matching Prediction BERT），一个基于提示（prompt-based）的模型，用于从希伯来语放射学文本中提取信息。研究分析了9,683份克罗恩病患者的放射学报告，并对其中512份报告进行了放射科医生标注，涵盖六个胃肠器官和15种病理，生成了90个结构化标签。数据采用多标签分层划分（66%训练+验证；33%测试），并使用准确率、F1分数、Cohen's $\\kappa$、AUC、PPV、NPV和召回率进行性能评估。模型与SMP零样本基线和标准微调方法进行了比较，并引入了分层推理以提高运行效率。", "result": "在24个阳性样本超过15个的器官-发现组合上，HSMP-BERT实现了平均F1分数0.83±0.08和$\\kappa$值0.65±0.17。这显著优于SMP零样本基线（F1 0.49±0.07，$\\kappa$ 0.06±0.07）和标准微调（F1 0.30±0.27，$\\kappa$ 0.27±0.34），配对t检验p值小于10^-7。此外，分层推理将运行时长缩短了5.1倍。将模型应用于所有报告后，揭示了回肠壁增厚、狭窄和狭窄前扩张之间的关联，以及炎症性发现中年龄和性别特异性趋势。", "conclusion": "HSMP-BERT为放射学领域结构化信息提取提供了一个可扩展的解决方案，特别适用于低资源语言环境。该模型能够实现克罗恩病的人群水平分析，并展示了人工智能在资源受限环境中的巨大潜力。"}}
{"id": "2509.05012", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05012", "abs": "https://arxiv.org/abs/2509.05012", "authors": ["Hulin Li", "Qiliang Ren", "Jun Li", "Hanbing Wei", "Zheng Liu", "Linfang Fan"], "title": "A biologically inspired separable learning vision model for real-time traffic object perception in Dark", "comment": null, "summary": "Fast and accurate object perception in low-light traffic scenes has attracted\nincreasing attention. However, due to severe illumination degradation and the\nlack of reliable visual cues, existing perception models and methods struggle\nto quickly adapt to and accurately predict in low-light environments. Moreover,\nthere is the absence of available large-scale benchmark specifically focused on\nlow-light traffic scenes. To bridge this gap, we introduce a physically\ngrounded illumination degradation method tailored to real-world low-light\nsettings and construct Dark-traffic, the largest densely annotated dataset to\ndate for low-light traffic scenes, supporting object detection, instance\nsegmentation, and optical flow estimation. We further propose the Separable\nLearning Vision Model (SLVM), a biologically inspired framework designed to\nenhance perception under adverse lighting. SLVM integrates four key components:\na light-adaptive pupillary mechanism for illumination-sensitive feature\nextraction, a feature-level separable learning strategy for efficient\nrepresentation, task-specific decoupled branches for multi-task separable\nlearning, and a spatial misalignment-aware fusion module for precise\nmulti-feature alignment. Extensive experiments demonstrate that SLVM achieves\nstate-of-the-art performance with reduced computational overhead. Notably, it\noutperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1\npercentage points in instance segmentation, and reduces endpoint error (EPE) of\nbaseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end\ntrained SLVM surpasses Swin Transformer+EnlightenGAN and\nConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key\nmetrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage\npoints. The Dark-traffic dataset and complete code is released at\nhttps://github.com/alanli1997/slvm.", "AI": {"tldr": "本文针对低光照交通场景下的目标感知问题，提出了一个物理降解方法来构建迄今为止最大的低光照交通场景数据集Dark-traffic，并提出了受生物启发的SLVM模型，该模型在检测、实例分割和光流估计等任务上实现了最先进的性能，同时降低了计算开销。", "motivation": "在低光照交通场景中，由于严重的照明退化和缺乏可靠的视觉线索，现有的感知模型难以快速适应和准确预测。此外，目前缺乏专门针对低光照交通场景的大规模基准数据集。", "method": "研究者引入了一种基于物理原理的照明降解方法，并构建了Dark-traffic数据集，这是目前最大的、密集标注的低光照交通场景数据集，支持目标检测、实例分割和光流估计。此外，本文提出了可分离学习视觉模型（SLVM），这是一个受生物启发的框架，包含四个关键组件：光自适应瞳孔机制、特征级可分离学习策略、任务特定解耦分支和空间错位感知融合模块。", "result": "SLVM在降低计算开销的同时，实现了最先进的性能。在Dark-traffic数据集上，SLVM在检测方面超越RT-DETR 11.2个百分点，在实例分割方面超越YOLOv12 6.1个百分点，并使基线模型的光流端点误差（EPE）降低12.37%。在LIS基准上，端到端训练的SLVM在关键指标上平均超越Swin Transformer+EnlightenGAN和ConvNeXt-T+EnlightenGAN 11个百分点，并超越Mask RCNN（带光照增强）3.1个百分点。Dark-traffic数据集和完整代码已发布。", "conclusion": "本文提出的Dark-traffic数据集和SLVM模型有效解决了低光照交通场景目标感知的挑战，为该领域提供了一个高效且性能优越的解决方案，显著提升了恶劣照明条件下的感知能力。"}}
{"id": "2509.04657", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04657", "abs": "https://arxiv.org/abs/2509.04657", "authors": ["Mohammadtaher Safarzadeh", "Afshin Oroojlooyjadid", "Dan Roth"], "title": "Evaluating NL2SQL via SQL2NL", "comment": "Accepted to EMNLP 2025", "summary": "Robust evaluation in the presence of linguistic variation is key to\nunderstanding the generalization capabilities of Natural Language to SQL\n(NL2SQL) models, yet existing benchmarks rarely address this factor in a\nsystematic or controlled manner. We propose a novel schema-aligned paraphrasing\nframework that leverages SQL-to-NL (SQL2NL) to automatically generate\nsemantically equivalent, lexically diverse queries while maintaining alignment\nwith the original schema and intent. This enables the first targeted evaluation\nof NL2SQL robustness to linguistic variation in isolation-distinct from prior\nwork that primarily investigates ambiguity or schema perturbations. Our\nanalysis reveals that state-of-the-art models are far more brittle than\nstandard benchmarks suggest. For example, LLaMa3.3-70B exhibits a 10.23% drop\nin execution accuracy (from 77.11% to 66.9%) on paraphrased Spider queries,\nwhile LLaMa3.1-8B suffers an even larger drop of nearly 20% (from 62.9% to\n42.5%). Smaller models (e.g., GPT-4o mini) are disproportionately affected. We\nalso find that robustness degradation varies significantly with query\ncomplexity, dataset, and domain -- highlighting the need for evaluation\nframeworks that explicitly measure linguistic generalization to ensure reliable\nperformance in real-world settings.", "AI": {"tldr": "现有NL2SQL模型在面对语言变体时鲁棒性极差，现有基准测试未能有效揭示此问题。本文提出了一种新的框架来生成语言多样化的查询，并发现最先进的模型在这些变体上性能显著下降。", "motivation": "现有基准测试很少系统或受控地解决语言变体对自然语言到SQL（NL2SQL）模型泛化能力的影响，而这对于理解模型的鲁棒性至关重要。", "method": "本文提出了一种新颖的模式对齐复述框架，该框架利用SQL到自然语言（SQL2NL）技术自动生成语义等效、词汇多样化且与原始模式和意图保持一致的查询。这使得首次能够孤立地评估NL2SQL模型对语言变体的鲁棒性。", "result": "分析表明，最先进的模型远比标准基准测试所显示的更脆弱。例如，LLaMa3.3-70B在复述的Spider查询上执行准确率下降了10.23%（从77.11%降至66.9%），而LLaMa3.1-8B的下降幅度更大，接近20%（从62.9%降至42.5%）。小型模型（如GPT-4o mini）受到的影响尤为严重。鲁棒性下降程度还随查询复杂性、数据集和领域显著变化。", "conclusion": "研究结果强调了需要专门衡量语言泛化能力的评估框架，以确保NL2SQL模型在实际应用中的可靠性能。"}}
{"id": "2509.04523", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.04523", "abs": "https://arxiv.org/abs/2509.04523", "authors": ["David Anderson", "Galia Benitez", "Margret Bjarnadottir", "Shriyan Reyya"], "title": "Using LLMs to create analytical datasets: A case study of reconstructing the historical memory of Colombia", "comment": null, "summary": "Colombia has been submerged in decades of armed conflict, yet until recently,\nthe systematic documentation of violence was not a priority for the Colombian\ngovernment. This has resulted in a lack of publicly available conflict\ninformation and, consequently, a lack of historical accounts. This study\ncontributes to Colombia's historical memory by utilizing GPT, a large language\nmodel (LLM), to read and answer questions about over 200,000 violence-related\nnewspaper articles in Spanish. We use the resulting dataset to conduct both\ndescriptive analysis and a study of the relationship between violence and the\neradication of coca crops, offering an example of policy analyses that such\ndata can support. Our study demonstrates how LLMs have opened new research\nopportunities by enabling examinations of large text corpora at a previously\ninfeasible depth.", "AI": {"tldr": "本研究利用GPT分析超过20万篇哥伦比亚暴力相关报纸文章，以弥补历史记录空白，并研究暴力与古柯作物根除的关系，展示了大型语言模型（LLM）在深度文本分析中的新机遇。", "motivation": "哥伦比亚长期受武装冲突困扰，但政府过去并未优先系统记录暴力事件，导致公开可用的冲突信息和历史记载严重缺失。", "method": "利用大型语言模型（LLM）GPT阅读并回答关于20多万篇西班牙语暴力相关报纸文章的问题，生成了一个数据集。随后，利用该数据集进行描述性分析，并研究暴力与古柯作物根除之间的关系。", "result": "创建了一个关于哥伦比亚暴力历史的详细数据集，并基于此数据进行了描述性分析，以及一项关于暴力与古柯作物根除之间关系的政策分析示例。", "conclusion": "本研究表明，大型语言模型（LLM）通过实现对大型文本语料库前所未有的深度分析，开辟了新的研究机会。"}}
{"id": "2509.05019", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05019", "abs": "https://arxiv.org/abs/2509.05019", "authors": ["Mohsine El Khayati", "Ayyad Maafiri", "Yassine Himeur", "Hamzah Ali Alkhazaleh", "Shadi Atalla", "Wathiq Mansoor"], "title": "Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition", "comment": "20pages, 9 figures and 11 tables", "summary": "The study explores the integration of transfer learning (TL) with\nmobile-enabled convolutional neural networks (MbNets) to enhance Arabic\nHandwritten Character Recognition (AHCR). Addressing challenges like extensive\ncomputational requirements and dataset scarcity, this research evaluates three\nTL strategies--full fine-tuning, partial fine-tuning, and training from\nscratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and\nShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,\nHIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently\nachieving superior accuracy, robustness, and efficiency, with ShuffleNet\nexcelling in generalization, particularly under full fine-tuning. The IFHCDB\ndataset yielded the highest results, with 99% accuracy using MnasNet under full\nfine-tuning, highlighting its suitability for robust character recognition. The\nAHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA\nposed significant challenges due to its variability, achieving a peak accuracy\nof 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall\nperformance, balancing accuracy and convergence speed, while partial\nfine-tuning underperformed across metrics. These findings underscore the\npotential of combining TL and MbNets for resource-efficient AHCR, paving the\nway for further optimizations and broader applications. Future work will\nexplore architectural modifications, in-depth dataset feature analysis, data\naugmentation, and advanced sensitivity analysis to enhance model robustness and\ngeneralizability.", "AI": {"tldr": "本研究探索了迁移学习（TL）与移动端卷积神经网络（MbNets）的结合，以提升阿拉伯手写字符识别（AHCR）的性能，解决了计算资源和数据稀缺问题，并评估了不同策略和模型在基准数据集上的表现。", "motivation": "阿拉伯手写字符识别（AHCR）面临计算需求高和数据集稀缺的挑战，需要更高效、资源友好的解决方案。", "method": "研究评估了三种迁移学习策略（完全微调、部分微调、从头开始训练）与四种轻量级MbNets（MobileNet、SqueezeNet、MnasNet、ShuffleNet）的结合。实验在AHCD、HIJJA和IFHCDB三个基准数据集上进行。", "result": "MobileNet表现最佳，在准确性、鲁棒性和效率上均优异；ShuffleNet在泛化能力上表现出色，尤其在完全微调下。IFHCDB数据集取得了最高结果（MnasNet完全微调下达99%准确率），AHCD数据集达到97%（ShuffleNet），HIJJA数据集因变异性挑战取得92%（ShuffleNet）。完全微调策略在平衡准确性和收敛速度方面表现最佳，而部分微调表现不佳。", "conclusion": "结合迁移学习和MbNets为资源高效的AHCR提供了巨大潜力。未来的工作将集中于架构修改、数据集特征分析、数据增强和高级敏感性分析，以进一步提高模型的鲁棒性和泛化能力。"}}
{"id": "2509.04696", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04696", "abs": "https://arxiv.org/abs/2509.04696", "authors": ["Samira Khorshidi", "Azadeh Nikfarjam", "Suprita Shankar", "Yisi Sang", "Yash Govind", "Hyun Jang", "Ali Kasgari", "Alexis McClimans", "Mohamed Soliman", "Vishnu Konda", "Ahmed Fakhry", "Xiaoguang Qi"], "title": "ODKE+: Ontology-Guided Open-Domain Knowledge Extraction with LLMs", "comment": null, "summary": "Knowledge graphs (KGs) are foundational to many AI applications, but\nmaintaining their freshness and completeness remains costly. We present ODKE+,\na production-grade system that automatically extracts and ingests millions of\nopen-domain facts from web sources with high precision. ODKE+ combines modular\ncomponents into a scalable pipeline: (1) the Extraction Initiator detects\nmissing or stale facts, (2) the Evidence Retriever collects supporting\ndocuments, (3) hybrid Knowledge Extractors apply both pattern-based rules and\nontology-guided prompting for large language models (LLMs), (4) a lightweight\nGrounder validates extracted facts using a second LLM, and (5) the Corroborator\nranks and normalizes candidate facts for ingestion. ODKE+ dynamically generates\nontology snippets tailored to each entity type to align extractions with schema\nconstraints, enabling scalable, type-consistent fact extraction across 195\npredicates. The system supports batch and streaming modes, processing over 9\nmillion Wikipedia pages and ingesting 19 million high-confidence facts with\n98.8% precision. ODKE+ significantly improves coverage over traditional\nmethods, achieving up to 48% overlap with third-party KGs and reducing update\nlag by 50 days on average. Our deployment demonstrates that LLM-based\nextraction, grounded in ontological structure and verification workflows, can\ndeliver trustworthiness, production-scale knowledge ingestion with broad\nreal-world applicability. A recording of the system demonstration is included\nwith the submission and is also available at https://youtu.be/UcnE3_GsTWs.", "AI": {"tldr": "ODKE+是一个生产级的系统，它利用模块化组件、混合知识提取器（包括LLM）和本体引导，从网络源自动、高精度地提取和摄取开放域事实，以维护知识图谱的及时性和完整性。", "motivation": "知识图谱是许多AI应用的基础，但维护其新鲜度和完整性成本高昂。", "method": "ODKE+是一个可扩展的模块化流水线系统，包括：1) 提取启动器检测缺失或过时事实；2) 证据检索器收集支持文档；3) 混合知识提取器结合基于模式的规则和本体引导的LLM提示；4) 轻量级验证器使用第二个LLM验证提取的事实；5) 确认器对候选事实进行排名和规范化。系统动态生成针对每个实体类型的本体片段，以使提取与模式约束对齐，并支持批处理和流式模式。", "result": "ODKE+处理了超过900万个维基百科页面，摄取了1900万个高置信度事实，精度达到98.8%。它显著提高了覆盖率，与第三方知识图谱重叠度高达48%，并平均减少了50天的更新延迟。", "conclusion": "基于LLM的提取，在本体结构和验证工作流的支持下，能够实现可信赖的、生产规模的知识摄取，并具有广泛的实际应用价值。"}}
{"id": "2509.04605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04605", "abs": "https://arxiv.org/abs/2509.04605", "authors": ["Xiyuan Gao", "Shekhar Nayak", "Matt Coler"], "title": "Spoken in Jest, Detected in Earnest: A Systematic Review of Sarcasm Recognition -- Multimodal Fusion, Challenges, and Future Prospects", "comment": "20 pages, 7 figures, Submitted to IEEE Transactions on Affective\n  Computing", "summary": "Sarcasm, a common feature of human communication, poses challenges in\ninterpersonal interactions and human-machine interactions. Linguistic research\nhas highlighted the importance of prosodic cues, such as variations in pitch,\nspeaking rate, and intonation, in conveying sarcastic intent. Although previous\nwork has focused on text-based sarcasm detection, the role of speech data in\nrecognizing sarcasm has been underexplored. Recent advancements in speech\ntechnology emphasize the growing importance of leveraging speech data for\nautomatic sarcasm recognition, which can enhance social interactions for\nindividuals with neurodegenerative conditions and improve machine understanding\nof complex human language use, leading to more nuanced interactions. This\nsystematic review is the first to focus on speech-based sarcasm recognition,\ncharting the evolution from unimodal to multimodal approaches. It covers\ndatasets, feature extraction, and classification methods, and aims to bridge\ngaps across diverse research domains. The findings include limitations in\ndatasets for sarcasm recognition in speech, the evolution of feature extraction\ntechniques from traditional acoustic features to deep learning-based\nrepresentations, and the progression of classification methods from unimodal\napproaches to multimodal fusion techniques. In so doing, we identify the need\nfor greater emphasis on cross-cultural and multilingual sarcasm recognition, as\nwell as the importance of addressing sarcasm as a multimodal phenomenon, rather\nthan a text-based challenge.", "AI": {"tldr": "这篇系统综述首次关注基于语音的讽刺识别，追踪了从单模态到多模态方法的发展，并指出了数据集、特征提取和分类方法的演变以及未来的研究方向。", "motivation": "讽刺是人类交流中的常见挑战，对人际和人机互动构成障碍。语言学研究强调了韵律线索的重要性，但语音数据在讽刺识别中的作用尚未被充分探索。自动语音讽刺识别可以改善神经退行性疾病患者的社交互动，并提高机器对复杂人类语言的理解，从而实现更细致的交互。", "method": "本文采用系统综述的方法，首次专注于基于语音的讽刺识别。它涵盖了数据集、特征提取和分类方法，并追溯了从单模态到多模态方法的演变，旨在弥合不同研究领域之间的鸿沟。", "result": "研究发现，语音讽刺识别的数据集存在局限性；特征提取技术从传统的声学特征发展到基于深度学习的表示；分类方法从单模态方法演进到多模态融合技术。", "conclusion": "结论强调需要更多地关注跨文化和多语言的讽刺识别，以及将讽刺视为一种多模态现象而非仅基于文本的挑战。"}}
{"id": "2509.05030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05030", "abs": "https://arxiv.org/abs/2509.05030", "authors": ["Cong Cao", "Xianhang Cheng", "Jingyuan Liu", "Yujian Zheng", "Zhenhui Lin", "Meriem Chkir", "Hao Li"], "title": "LUIVITON: Learned Universal Interoperable VIrtual Try-ON", "comment": null, "summary": "We present LUIVITON, an end-to-end system for fully automated virtual try-on,\ncapable of draping complex, multi-layer clothing onto diverse and arbitrarily\nposed humanoid characters. To address the challenge of aligning complex\ngarments with arbitrary and highly diverse body shapes, we use SMPL as a proxy\nrepresentation and separate the clothing-to-body draping problem into two\ncorrespondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,\nwhere each has its unique challenges. While we address the clothing-to-SMPL\nfitting problem using a geometric learning-based approach for\npartial-to-complete shape correspondence prediction, we introduce a diffusion\nmodel-based approach for body-to-SMPL correspondence using multi-view\nconsistent appearance features and a pre-trained 2D foundation model. Our\nmethod can handle complex geometries, non-manifold meshes, and generalizes\neffectively to a wide range of humanoid characters -- including humans, robots,\ncartoon subjects, creatures, and aliens, while maintaining computational\nefficiency for practical adoption. In addition to offering a fully automatic\nfitting solution, LUIVITON supports fast customization of clothing size,\nallowing users to adjust clothing sizes and material properties after they have\nbeen draped. We show that our system can produce high-quality 3D clothing\nfittings without any human labor, even when 2D clothing sewing patterns are not\navailable.", "AI": {"tldr": "LUIVITON是一个端到端的全自动虚拟试穿系统，能够将复杂的多层服装，披挂到各种姿态多样的人形角色上，通过将服装与身体的披挂问题分解为服装到SMPL和身体到SMPL的对应任务，并分别使用几何学习和扩散模型解决。", "motivation": "研究动机是为了解决在全自动虚拟试穿中，将复杂服装与任意且高度多样化的身体形状进行对齐的挑战。", "method": "该方法使用SMPL作为代理表示，将服装到身体的披挂问题分解为两个对应任务：1) 服装到SMPL的对应，通过基于几何学习的方法进行部分到完整形状对应预测；2) 身体到SMPL的对应，引入了基于扩散模型的方法，利用多视角一致的外观特征和预训练的2D基础模型。系统还支持服装尺寸和材料属性的快速定制。", "result": "LUIVITON系统能够生成高质量的3D服装拟合，无需人工干预，即使没有2D服装缝纫图案也能工作。它能处理复杂的几何形状、非流形网格，并有效泛化到各种人形角色（包括人类、机器人、卡通人物、生物和外星人），同时保持计算效率以实现实际应用。", "conclusion": "LUIVITON提供了一个全自动、高质量且高效的虚拟试穿解决方案，能够克服复杂服装和多样身体形状的挑战，并具有广泛的适用性和实用性。"}}
{"id": "2509.04716", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.04716", "abs": "https://arxiv.org/abs/2509.04716", "authors": ["Yushi Sun", "Kai Sun", "Yifan Ethan Xu", "Xiao Yang", "Xin Luna Dong", "Nan Tang", "Lei Chen"], "title": "KERAG: Knowledge-Enhanced Retrieval-Augmented Generation for Advanced Question Answering", "comment": "Accepted by EMNLP Findings 2025", "summary": "Retrieval-Augmented Generation (RAG) mitigates hallucination in Large\nLanguage Models (LLMs) by incorporating external data, with Knowledge Graphs\n(KGs) offering crucial information for question answering. Traditional\nKnowledge Graph Question Answering (KGQA) methods rely on semantic parsing,\nwhich typically retrieves knowledge strictly necessary for answer generation,\nthus often suffer from low coverage due to rigid schema requirements and\nsemantic ambiguity. We present KERAG, a novel KG-based RAG pipeline that\nenhances QA coverage by retrieving a broader subgraph likely to contain\nrelevant information. Our retrieval-filtering-summarization approach, combined\nwith fine-tuned LLMs for Chain-of-Thought reasoning on knowledge sub-graphs,\nreduces noises and improves QA for both simple and complex questions.\nExperiments demonstrate that KERAG surpasses state-of-the-art solutions by\nabout 7% in quality and exceeds GPT-4o (Tool) by 10-21%.", "AI": {"tldr": "KERAG是一种基于知识图谱的检索增强生成（RAG）管道，通过检索更广泛的子图、过滤、摘要和使用微调LLM进行思维链推理，显著提高了问答覆盖率和质量，超越了现有SOTA和GPT-4o。", "motivation": "大型语言模型（LLM）存在幻觉问题，RAG可通过外部数据缓解。知识图谱（KG）为问答提供关键信息。传统KGQA方法依赖语义解析，检索知识严格受限，常因严格的模式要求和语义模糊导致覆盖率低。因此，需要一种能提高问答覆盖率的KG-based RAG方法。", "method": "KERAG采用检索-过滤-摘要的方法。它首先检索一个更广泛的知识子图，该子图可能包含相关信息，以提高覆盖率。然后对检索到的子图进行过滤和摘要处理以减少噪声。最后，结合经过微调的LLM，利用思维链（Chain-of-Thought）推理在处理后的知识子图上进行问答。", "result": "KERAG在问答质量上超越了现有最先进解决方案约7%，并且比GPT-4o (Tool) 高出10-21%。", "conclusion": "KERAG通过其新颖的检索-过滤-摘要方法和结合微调LLM的思维链推理，有效解决了传统KGQA覆盖率低的问题，显著提高了简单和复杂问题的问答质量，并优于现有顶级LLM解决方案。"}}
{"id": "2509.04615", "categories": ["cs.CL", "cs.CR", "cs.LG", "68T07, 68T50", "I.2.7; I.2.6; K.6.5"], "pdf": "https://arxiv.org/pdf/2509.04615", "abs": "https://arxiv.org/abs/2509.04615", "authors": ["Brennen Hill", "Surendra Parla", "Venkata Abhijeeth Balabhadruni", "Atharv Prajod Padmalayam", "Sujay Chandra Shekara Sharma"], "title": "Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) has introduced critical\nsecurity challenges, where adversarial actors can manipulate input prompts to\ncause significant harm and circumvent safety alignments. These prompt-based\nattacks exploit vulnerabilities in a model's design, training, and contextual\nunderstanding, leading to intellectual property theft, misinformation\ngeneration, and erosion of user trust. A systematic understanding of these\nattack vectors is the foundational step toward developing robust\ncountermeasures. This paper presents a comprehensive literature survey of\nprompt-based attack methodologies, categorizing them to provide a clear threat\nmodel. By detailing the mechanisms and impacts of these exploits, this survey\naims to inform the research community's efforts in building the next generation\nof secure LLMs that are inherently resistant to unauthorized distillation,\nfine-tuning, and editing.", "AI": {"tldr": "大语言模型（LLMs）面临严重的提示攻击安全挑战。本文对这些攻击方法进行了全面的文献综述和分类，旨在提供一个清晰的威胁模型，并指导未来安全LLMs的开发。", "motivation": "随着大语言模型（LLMs）的普及，出现了关键的安全挑战，恶意行为者可以通过操纵输入提示来造成重大损害并规避安全对齐。这些基于提示的攻击利用模型设计、训练和上下文理解中的漏洞，导致知识产权盗窃、虚假信息生成和用户信任受损。系统地理解这些攻击向量是开发强大防御措施的基础。", "method": "本文通过对基于提示的攻击方法进行全面的文献综述，并对其进行分类，以提供一个清晰的威胁模型。", "result": "论文详细阐述了基于提示的攻击机制和影响，并对攻击方法进行了系统分类，从而构建了一个清晰的威胁模型。", "conclusion": "本综述旨在为研究社区构建下一代安全的LLMs提供信息，使其能够固有地抵抗未经授权的蒸馏、微调和编辑，从而开发出更强大的对策。"}}
{"id": "2509.05034", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05034", "abs": "https://arxiv.org/abs/2509.05034", "authors": ["Jingqi Wu", "Hanxi Li", "Lin Yuanbo Wu", "Hao Chen", "Deyin Liu", "Peng Wang"], "title": "Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization", "comment": null, "summary": "Industrial product inspection is often performed using Anomaly Detection (AD)\nframeworks trained solely on non-defective samples. Although defective samples\ncan be collected during production, leveraging them usually requires\npixel-level annotations, limiting scalability. To address this, we propose\nADClick, an Interactive Image Segmentation (IIS) algorithm for industrial\nanomaly detection. ADClick generates pixel-wise anomaly annotations from only a\nfew user clicks and a brief textual description, enabling precise and efficient\nlabeling that significantly improves AD model performance (e.g., AP = 96.1\\% on\nMVTec AD). We further introduce ADClick-Seg, a cross-modal framework that\naligns visual features and textual prompts via a prototype-based approach for\nanomaly detection and localization. By combining pixel-level priors with\nlanguage-guided cues, ADClick-Seg achieves state-of-the-art results on the\nchallenging ``Multi-class'' AD task (AP = 80.0\\%, PRO = 97.5\\%, Pixel-AUROC =\n99.1\\% on MVTec AD).", "AI": {"tldr": "本文提出了ADClick和ADClick-Seg，用于工业异常检测。ADClick通过少量点击和文本描述实现高效的像素级异常标注，显著提升了异常检测模型性能。ADClick-Seg是一个跨模态框架，结合像素级先验和语言引导线索，在多类别异常检测任务中取得了最先进的结果。", "motivation": "工业产品检测中的异常检测（AD）框架通常仅使用无缺陷样本进行训练。虽然可以收集到有缺陷样本，但利用它们通常需要耗时的像素级标注，这限制了模型的可扩展性。", "method": "1. **ADClick**: 一种交互式图像分割（IIS）算法，仅通过少量用户点击和简短文本描述即可生成像素级异常标注。2. **ADClick-Seg**: 一个跨模态框架，通过基于原型的方法对视觉特征和文本提示进行对齐，结合像素级先验和语言引导线索进行异常检测和定位。", "result": "1. ADClick实现了精确高效的标注，显著提高了异常检测模型性能（例如，在MVTec AD上AP达到96.1%）。2. ADClick-Seg在具有挑战性的“多类别”异常检测任务中取得了最先进的结果（在MVTec AD上AP达到80.0%，PRO达到97.5%，Pixel-AUROC达到99.1%）。", "conclusion": "ADClick和ADClick-Seg通过提供高效的交互式像素级标注和强大的跨模态框架，有效地解决了工业异常检测中利用缺陷样本的可扩展性问题，从而显著提升了在通用和多类别异常检测及定位任务中的性能，并达到了最先进水平。"}}
{"id": "2509.04753", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04753", "abs": "https://arxiv.org/abs/2509.04753", "authors": ["Cheng Peng", "Xinyu Dong", "Mengxian Lyu", "Daniel Paredes", "Yaoyun Zhang", "Yonghui Wu"], "title": "A Study of Large Language Models for Patient Information Extraction: Model Architecture, Fine-Tuning Strategy, and Multi-task Instruction Tuning", "comment": null, "summary": "Natural language processing (NLP) is a key technology to extract important\npatient information from clinical narratives to support healthcare\napplications. The rapid development of large language models (LLMs) has\nrevolutionized many NLP tasks in the clinical domain, yet their optimal use in\npatient information extraction tasks requires further exploration. This study\nexamines LLMs' effectiveness in patient information extraction, focusing on LLM\narchitectures, fine-tuning strategies, and multi-task instruction tuning\ntechniques for developing robust and generalizable patient information\nextraction systems. This study aims to explore key concepts of using LLMs for\nclinical concept and relation extraction tasks, including: (1) encoder-only or\ndecoder-only LLMs, (2) prompt-based parameter-efficient fine-tuning (PEFT)\nalgorithms, and (3) multi-task instruction tuning on few-shot learning\nperformance. We benchmarked a suite of LLMs, including encoder-based LLMs\n(BERT, GatorTron) and decoder-based LLMs (GatorTronGPT, Llama 3.1,\nGatorTronLlama), across five datasets. We compared traditional full-size\nfine-tuning and prompt-based PEFT. We explored a multi-task instruction tuning\nframework that combines both tasks across four datasets to evaluate the\nzero-shot and few-shot learning performance using the leave-one-dataset-out\nstrategy.", "AI": {"tldr": "本研究探讨了大型语言模型（LLMs）在临床叙述中进行患者信息提取的有效性，重点关注LLM架构、参数高效微调（PEFT）策略以及多任务指令微调技术，以开发稳健且泛化能力强的系统。", "motivation": "自然语言处理（NLP）是提取临床叙述中患者信息以支持医疗应用的关键技术。尽管大型语言模型（LLMs）的快速发展彻底改变了临床领域的许多NLP任务，但其在患者信息提取任务中的最佳使用仍需进一步探索。", "method": "本研究探讨了LLM在临床概念和关系提取任务中的关键概念，包括：1) 编码器-only或解码器-only LLM架构，2) 基于提示的参数高效微调（PEFT）算法，以及3) 多任务指令微调对少样本学习性能的影响。研究基准测试了一系列LLMs，包括编码器基LLMs（BERT, GatorTron）和解码器基LLMs（GatorTronGPT, Llama 3.1, GatorTronLlama），并跨五个数据集进行评估。研究比较了传统的全尺寸微调和基于提示的PEFT。此外，还探索了一个多任务指令微调框架，该框架结合了四个数据集上的两项任务，并使用留一数据集法评估了零样本和少样本学习性能。", "result": "摘要描述了研究将要进行的基准测试和比较，但并未明确给出具体的实验结果或发现。它表明研究将比较不同LLM架构、微调策略和多任务指令微调在患者信息提取任务中的表现。", "conclusion": "摘要并未提供明确的研究结论，而是指出了本研究旨在探索和理解LLM在临床患者信息提取中的最佳使用方式，并通过对不同架构、微调策略和多任务指令微调的系统性评估，为开发稳健和通用系统提供见解。"}}
{"id": "2509.04656", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04656", "abs": "https://arxiv.org/abs/2509.04656", "authors": ["Aisha Alansari", "Hamzah Luqman"], "title": "AraHalluEval: A Fine-grained Hallucination Evaluation Framework for Arabic LLMs", "comment": null, "summary": "Recently, extensive research on the hallucination of the large language\nmodels (LLMs) has mainly focused on the English language. Despite the growing\nnumber of multilingual and Arabic-specific LLMs, evaluating LLMs' hallucination\nin the Arabic context remains relatively underexplored. The knowledge gap is\nparticularly pressing given Arabic's widespread use across many regions and its\nimportance in global communication and media. This paper presents the first\ncomprehensive hallucination evaluation of Arabic and multilingual LLMs on two\ncritical Arabic natural language generation tasks: generative question\nanswering (GQA) and summarization. This study evaluates a total of 12 LLMs,\nincluding 4 Arabic pre-trained models, 4 multilingual models, and 4\nreasoning-based models. To assess the factual consistency and faithfulness of\nLLMs' outputs, we developed a fine-grained hallucination evaluation framework\nconsisting of 12 fine-grained hallucination indicators that represent the\nvarying characteristics of each task. The results reveal that factual\nhallucinations are more prevalent than faithfulness errors across all models\nand tasks. Notably, the Arabic pre-trained model Allam consistently\ndemonstrates lower hallucination rates than multilingual models and a\ncomparative performance with reasoning-based models. The code is available at:\n\\href{https://github.com/aishaalansari57/AraHalluEval}{Github link}.", "AI": {"tldr": "本研究首次对阿拉伯语和多语言大型语言模型（LLMs）在阿拉伯语生成式问答和摘要任务中的幻觉现象进行了全面评估，并发现事实性幻觉更普遍，且阿拉伯语预训练模型表现较好。", "motivation": "尽管多语言和阿拉伯语LLMs数量不断增长，但LLMs在阿拉伯语语境下的幻觉评估仍相对不足。鉴于阿拉伯语的广泛使用及其在全球交流和媒体中的重要性，填补这一知识空白刻不容缓。", "method": "研究评估了12个LLMs（包括4个阿拉伯语预训练模型、4个多语言模型和4个基于推理的模型），在生成式问答（GQA）和摘要两个关键阿拉伯语自然语言生成任务上进行。开发了一个包含12个细粒度幻觉指标的评估框架，以评估LLMs输出的事实一致性和忠实度。", "result": "结果显示，在所有模型和任务中，事实性幻觉比忠实度错误更为普遍。值得注意的是，阿拉伯语预训练模型Allam持续展现出比多语言模型更低的幻觉率，并与基于推理的模型表现相当。", "conclusion": "本研究首次全面评估了阿拉伯语LLMs的幻觉现象，揭示了事实性幻觉的普遍性，并表明专门的阿拉伯语预训练模型在降低幻觉方面可能优于多语言模型，为未来的阿拉伯语LLM开发提供了重要见解。"}}
{"id": "2509.05071", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.05071", "abs": "https://arxiv.org/abs/2509.05071", "authors": ["Mojtaba Safari", "Zach Eidex", "Richard L. J. Qiu", "Matthew Goette", "Tonghe Wang", "Xiaofeng Yang"], "title": "Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction", "comment": null, "summary": "Background: To systematically review and perform a meta-analysis of\nartificial intelligence (AI)-driven methods for detecting and correcting\nmagnetic resonance imaging (MRI) motion artifacts, assessing current\ndevelopments, effectiveness, challenges, and future research directions.\nMethods: A comprehensive systematic review and meta-analysis were conducted,\nfocusing on deep learning (DL) approaches, particularly generative models, for\nthe detection and correction of MRI motion artifacts. Quantitative data were\nextracted regarding utilized datasets, DL architectures, and performance\nmetrics. Results: DL, particularly generative models, show promise for reducing\nmotion artifacts and improving image quality; however, limited\ngeneralizability, reliance on paired training data, and risk of visual\ndistortions remain key challenges that motivate standardized datasets and\nreporting. Conclusions: AI-driven methods, particularly DL generative models,\nshow significant potential for improving MRI image quality by effectively\naddressing motion artifacts. However, critical challenges must be addressed,\nincluding the need for comprehensive public datasets, standardized reporting\nprotocols for artifact levels, and more advanced, adaptable DL techniques to\nreduce reliance on extensive paired datasets. Addressing these aspects could\nsubstantially enhance MRI diagnostic accuracy, reduce healthcare costs, and\nimprove patient care outcomes.", "AI": {"tldr": "该论文系统综述并元分析了AI驱动方法（特别是深度学习生成模型）在磁共振成像（MRI）运动伪影检测与校正中的应用，评估了其发展、有效性、挑战和未来方向。", "motivation": "为了评估AI驱动方法（特别是深度学习生成模型）在MRI运动伪影处理中的当前发展、有效性、面临的挑战以及未来的研究方向。尽管这些方法显示出前景，但其有限的泛化性、对配对训练数据的依赖以及潜在的视觉失真风险是亟待解决的关键挑战。", "method": "进行了一项全面的系统综述和元分析，重点关注深度学习（DL）方法，特别是生成模型，在MRI运动伪影检测和校正中的应用。提取了关于所用数据集、DL架构和性能指标的定量数据。", "result": "深度学习，特别是生成模型，在减少运动伪影和提高图像质量方面显示出潜力。然而，有限的泛化性、对配对训练数据的依赖以及视觉失真风险仍然是主要的挑战，这些挑战促使需要标准化数据集和报告。", "conclusion": "AI驱动方法，特别是深度学习生成模型，在通过有效处理运动伪影来改善MRI图像质量方面具有显著潜力。但必须解决关键挑战，包括需要全面的公共数据集、标准化伪影水平报告协议，以及更先进、适应性强的深度学习技术，以减少对大量配对数据集的依赖。解决这些问题可以大幅提高MRI诊断准确性，降低医疗成本，并改善患者护理结果。"}}
{"id": "2509.04779", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04779", "abs": "https://arxiv.org/abs/2509.04779", "authors": ["Eli Borodach", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Decoders Laugh as Loud as Encoders", "comment": null, "summary": "From the dawn of the computer, Allen Turing dreamed of a robot that could\ncommunicate using language as a human being. The recent advances in the field\nof Large Language Models (LLMs) shocked the scientific community when a single\nmodel can apply for various natural language processing (NLP) tasks, while the\noutput results are sometimes even better than most human communication skills.\nModels such as GPT, Claude, Grok, etc. have left their mark on the scientific\ncommunity. However, it is unclear how much these models understand what they\nproduce, especially in a nuanced theme such as humor. The question of whether\ncomputers understand humor is still open (among the decoders, the latest to be\nchecked was GPT-2). We addressed this issue in this paper; we have showed that\na fine-tuned decoder (GPT-4o) performed (Mean F1-macro score of 0.85) as well\nas the best fine-tuned encoder (RoBERTa with a Mean of F1-score 0.86)", "AI": {"tldr": "本文比较了微调后的解码器（GPT-4o）和编码器（RoBERTa）在幽默理解任务上的表现，发现两者性能相当。", "motivation": "尽管大型语言模型（LLMs）在各种自然语言处理任务中表现出色，甚至超越人类水平，但它们对所生成内容的理解程度仍不清楚，尤其是在幽默这种细微主题上。计算机是否理解幽默的问题仍未解决。", "method": "研究人员对一个解码器模型（GPT-4o）进行了微调，并将其与一个最佳的微调编码器模型（RoBERTa）在幽默理解任务上进行了比较。", "result": "微调后的解码器GPT-4o在幽默理解任务中取得了0.85的平均F1-macro分数，而微调后的编码器RoBERTa取得了0.86的平均F1分数。这表明两者的性能相当。", "conclusion": "微调后的解码器（如GPT-4o）在幽默理解方面可以与最佳的微调编码器（如RoBERTa）表现得同样出色。"}}
{"id": "2509.04664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04664", "abs": "https://arxiv.org/abs/2509.04664", "authors": ["Adam Tauman Kalai", "Ofir Nachum", "Santosh S. Vempala", "Edwin Zhang"], "title": "Why Language Models Hallucinate", "comment": null, "summary": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art\nsystems and undermine trust. We argue that language models hallucinate because\nthe training and evaluation procedures reward guessing over acknowledging\nuncertainty, and we analyze the statistical causes of hallucinations in the\nmodern training pipeline. Hallucinations need not be mysterious -- they\noriginate simply as errors in binary classification. If incorrect statements\ncannot be distinguished from facts, then hallucinations in pretrained language\nmodels will arise through natural statistical pressures. We then argue that\nhallucinations persist due to the way most evaluations are graded -- language\nmodels are optimized to be good test-takers, and guessing when uncertain\nimproves test performance. This \"epidemic\" of penalizing uncertain responses\ncan only be addressed through a socio-technical mitigation: modifying the\nscoring of existing benchmarks that are misaligned but dominate leaderboards,\nrather than introducing additional hallucination evaluations. This change may\nsteer the field toward more trustworthy AI systems.", "AI": {"tldr": "大型语言模型（LLMs）的幻觉源于训练和评估机制奖励猜测而非承认不确定性，本质是二元分类错误。解决之道在于修改现有基准测试的评分方式，以引导模型更值得信赖。", "motivation": "LLMs在不确定时会“猜测”，产生看似合理但错误的陈述（即“幻觉”），这损害了用户对系统的信任，即使是先进模型也存在此问题。", "method": "本文通过分析现代训练流程中的统计原因，将幻觉归结为二元分类错误。同时，作者论证了现有评估方法（即“惩罚不确定响应”）如何导致幻觉持续存在，因为模型被优化为“善于考试者”，猜测能提高测试表现。", "result": "幻觉并非神秘，它们是二元分类中的简单错误。当错误陈述无法与事实区分时，预训练语言模型在统计压力下会产生幻觉。此外，由于大多数评估的评分方式，奖励猜测而非承认不确定性，导致幻觉持续存在。", "conclusion": "解决幻觉的“流行”需要社会技术层面的缓解措施：修改现有主导排行榜但存在偏差的基准测试的评分方式，而非引入额外的幻觉评估。这种改变可能促使领域转向更值得信赖的AI系统。"}}
{"id": "2509.05075", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05075", "abs": "https://arxiv.org/abs/2509.05075", "authors": ["Yangming Li", "Chaoyu Liu", "Lihao Liu", "Simon Masnou", "Carola-Bibian Schönlieb"], "title": "GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting", "comment": null, "summary": "A few recent works explored incorporating geometric priors to regularize the\noptimization of Gaussian splatting, further improving its performance. However,\nthose early studies mainly focused on the use of low-order geometric priors\n(e.g., normal vector), and they are also unreliably estimated by\nnoise-sensitive methods, like local principal component analysis. To address\ntheir limitations, we first present GeoSplat, a general geometry-constrained\noptimization framework that exploits both first-order and second-order\ngeometric quantities to improve the entire training pipeline of Gaussian\nsplatting, including Gaussian initialization, gradient update, and\ndensification. As an example, we initialize the scales of 3D Gaussian\nprimitives in terms of principal curvatures, leading to a better coverage of\nthe object surface than random initialization. Secondly, based on certain\ngeometric structures (e.g., local manifold), we introduce efficient and\nnoise-robust estimation methods that provide dynamic geometric priors for our\nframework. We conduct extensive experiments on multiple datasets for novel view\nsynthesis, showing that our framework: GeoSplat, significantly improves the\nperformance of Gaussian splatting and outperforms previous baselines.", "AI": {"tldr": "GeoSplat是一个通用的几何约束优化框架，通过利用一阶和二阶几何先验以及鲁棒的估计方法，显著提升了高斯泼溅（Gaussian Splatting）在新视角合成中的性能。", "motivation": "现有研究在优化高斯泼溅时，主要使用低阶几何先验（如法向量），且这些先验通过对噪声敏感的方法（如局部主成分分析）估计，导致性能提升有限且不可靠。本研究旨在解决这些局限性。", "method": "本文提出了GeoSplat框架，该框架利用一阶和二阶几何量（如主曲率）来改进高斯泼溅的整个训练流程，包括高斯初始化、梯度更新和稠密化。例如，通过主曲率初始化3D高斯基元的尺度，以更好地覆盖物体表面。此外，基于特定的几何结构（如局部流形），引入了高效且对噪声鲁棒的估计方法，为框架提供动态几何先验。", "result": "GeoSplat框架在多个新视角合成数据集上进行了广泛实验，结果表明它显著提高了高斯泼溅的性能，并优于以往的基线方法。", "conclusion": "GeoSplat通过有效整合一阶和二阶几何先验以及鲁棒的估计方法，成功解决了现有高斯泼溅优化中几何先验的局限性，显著提升了其在新视角合成任务中的表现。"}}
{"id": "2509.04784", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04784", "abs": "https://arxiv.org/abs/2509.04784", "authors": ["Yilei Chen", "Souradip Chakraborty", "Lorenz Wolf", "Ioannis Ch. Paschalidis", "Aldo Pacchiano"], "title": "Enhancing Diversity in Large Language Models via Determinantal Point Processes", "comment": null, "summary": "Supervised fine-tuning and reinforcement learning are two popular methods for\npost-training large language models (LLMs). While improving the model's\nperformance on downstream tasks, they often reduce the model's output\ndiversity, leading to narrow, canonical responses. Existing methods to enhance\ndiversity are limited, either by operating at inference time or by focusing on\nlexical differences. We propose a novel training method named DQO based on\ndeterminantal point processes (DPPs) to jointly optimize LLMs for quality and\nsemantic diversity. Our approach samples and embeds a group of responses for\neach prompt, then uses the determinant of a kernel-based similarity matrix to\nmeasure diversity as the volume spanned by the embeddings of these responses.\nExperiments across instruction-following, summarization, story generation, and\nreasoning tasks demonstrate that our method substantially improves semantic\ndiversity without sacrificing model quality.", "AI": {"tldr": "本文提出了一种名为DQO的新型训练方法，基于行列式点过程（DPPs），旨在同时优化大型语言模型（LLMs）的输出质量和语义多样性，有效解决了现有后训练方法导致的多样性下降问题。", "motivation": "监督微调和强化学习等LLM后训练方法虽然能提升模型性能，但常会降低输出多样性，导致响应狭隘和规范化。现有提高多样性的方法要么仅限于推理时，要么只关注词汇差异，存在局限性。", "method": "DQO方法基于行列式点过程（DPPs），为每个提示词采样并嵌入一组响应。它利用基于核函数的相似性矩阵的行列式来衡量多样性，该行列式代表了这些响应嵌入所跨越的“体积”。", "result": "在指令遵循、摘要、故事生成和推理等任务上的实验表明，DQO方法在不牺牲模型质量的前提下，显著提高了语义多样性。", "conclusion": "DQO是一种有效的训练方法，能够联合优化大型语言模型的质量和语义多样性，解决了传统后训练方法在多样性方面的不足。"}}
{"id": "2509.04702", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04702", "abs": "https://arxiv.org/abs/2509.04702", "authors": ["Wei Chu", "Yuanzhe Dong", "Ke Tan", "Dong Han", "Xavier Menendez-Pidal", "Ruchao Fan", "Chenfeng Miao", "Chanwoo Kim", "Bhiksha Raj", "Rita Singh"], "title": "OleSpeech-IV: A Large-Scale Multispeaker and Multilingual Conversational Speech Dataset with Diverse Topics", "comment": null, "summary": "OleSpeech-IV dataset is a large-scale multispeaker and multilingual\nconversational speech dataset with diverse topics. The audio content comes from\npublicly-available English podcasts, talk shows, teleconferences, and other\nconversations. Speaker names, turns, and transcripts are human-sourced and\nrefined by a proprietary pipeline, while additional information such as\ntimestamps and confidence scores is derived from the pipeline. The IV denotes\nits position as Tier IV in the Olewave dataset series. In addition, we have\nopen-sourced a subset, OleSpeech-IV-2025-EN-AR-100, for non-commercial research\nuse.", "AI": {"tldr": "OleSpeech-IV是一个大规模、多说话人、多语言的对话语音数据集，内容来源于公开的英文播客和会议，经过人工标注和专有流程处理，并开放了一个子集供非商业研究使用。", "motivation": "抽象中没有明确说明动机，但推测是为了提供一个高质量、大规模、多样化的对话语音数据集，以支持相关领域的语音技术研究和开发。", "method": "音频内容来源于公开的英文播客、脱口秀、电话会议及其他对话。说话人姓名、轮次和转录本通过人工标注和专有流程进行精炼，时间戳和置信度等附加信息也由该流程生成。", "result": "创建了OleSpeech-IV数据集，它是一个大规模、多说话人、多语言的对话语音数据集，包含多样主题。该数据集是Olewave数据集系列的Tier IV。此外，还开放了一个名为OleSpeech-IV-2025-EN-AR-100的子集，供非商业研究使用。", "conclusion": "该研究提供了一个高质量、大规模的对话语音数据集，并通过开放子集的方式，为非商业研究社区提供了宝贵的资源，有助于推动多说话人、多语言对话语音处理领域的发展。"}}
{"id": "2509.05078", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05078", "abs": "https://arxiv.org/abs/2509.05078", "authors": ["Djamel Eddine Boukhari"], "title": "Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction", "comment": null, "summary": "Automated Facial Beauty Prediction (FBP) is a challenging computer vision\ntask due to the complex interplay of local and global facial features that\ninfluence human perception. While Convolutional Neural Networks (CNNs) excel at\nfeature extraction, they often process information at a fixed scale,\npotentially overlooking the critical inter-dependencies between features at\ndifferent levels of granularity. To address this limitation, we introduce the\nScale-Interaction Transformer (SIT), a novel hybrid deep learning architecture\nthat synergizes the feature extraction power of CNNs with the relational\nmodeling capabilities of Transformers. The SIT first employs a multi-scale\nmodule with parallel convolutions to capture facial characteristics at varying\nreceptive fields. These multi-scale representations are then framed as a\nsequence and processed by a Transformer encoder, which explicitly models their\ninteractions and contextual relationships via a self-attention mechanism. We\nconduct extensive experiments on the widely-used SCUT-FBP5500 benchmark\ndataset, where the proposed SIT model establishes a new state-of-the-art. It\nachieves a Pearson Correlation of 0.9187, outperforming previous methods. Our\nfindings demonstrate that explicitly modeling the interplay between multi-scale\nvisual cues is crucial for high-performance FBP. The success of the SIT\narchitecture highlights the potential of hybrid CNN-Transformer models for\ncomplex image regression tasks that demand a holistic, context-aware\nunderstanding.", "AI": {"tldr": "该论文提出了一种名为Scale-Interaction Transformer (SIT) 的混合深度学习架构，结合了CNN和Transformer的优势，通过显式建模多尺度面部特征的相互作用，在面部美感预测任务上取得了最先进的性能。", "motivation": "面部美感预测(FBP)是一项具有挑战性的计算机视觉任务，涉及局部和全局面部特征的复杂相互作用。传统卷积神经网络(CNN)通常以固定尺度处理信息，可能忽略了不同粒度级别特征之间关键的相互依赖关系，而这些关系对人类感知至关重要。", "method": "SIT模型首先采用一个多尺度模块，通过并行卷积捕获不同感受野下的面部特征。然后，这些多尺度表示被组织成一个序列，并由Transformer编码器处理，该编码器通过自注意力机制显式建模它们之间的相互作用和上下文关系。", "result": "在广泛使用的SCUT-FBP5500基准数据集上进行了大量实验，SIT模型取得了新的最先进成果，实现了0.9187的皮尔逊相关系数，优于现有方法。", "conclusion": "研究结果表明，显式建模多尺度视觉线索之间的相互作用对于高性能面部美感预测至关重要。SIT架构的成功突显了混合CNN-Transformer模型在需要整体、上下文感知理解的复杂图像回归任务中的潜力。"}}
{"id": "2509.04897", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04897", "abs": "https://arxiv.org/abs/2509.04897", "authors": ["Preferred Networks", ":", "Kaizaburo Chubachi", "Yasuhiro Fujita", "Shinichi Hemmi", "Yuta Hirokawa", "Toshiki Kataoka", "Goro Kobayashi", "Kenichi Maehashi", "Calvin Metzger", "Hiroaki Mikami", "Shogo Murai", "Daisuke Nishino", "Kento Nozawa", "Shintarou Okada", "Daisuke Okanohara", "Shunta Saito", "Shotaro Sano", "Shuji Suzuki", "Daisuke Tanaka", "Avinash Ummadisingu", "Hanqin Wang", "Sixue Wang", "Tianqi Xu"], "title": "PLaMo 2 Technical Report", "comment": null, "summary": "In this report, we introduce PLaMo 2, a series of Japanese-focused large\nlanguage models featuring a hybrid Samba-based architecture that transitions to\nfull attention via continual pre-training to support 32K token contexts.\nTraining leverages extensive synthetic corpora to overcome data scarcity, while\ncomputational efficiency is achieved through weight reuse and structured\npruning. This efficient pruning methodology produces an 8B model that achieves\nperformance comparable to our previous 100B model. Post-training further\nrefines the models using a pipeline of supervised fine-tuning (SFT) and direct\npreference optimization (DPO), enhanced by synthetic Japanese instruction data\nand model merging techniques. Optimized for inference using vLLM and\nquantization with minimal accuracy loss, the PLaMo 2 models achieve\nstate-of-the-art results on Japanese benchmarks, outperforming similarly-sized\nopen models in instruction-following, language fluency, and Japanese-specific\nknowledge.", "AI": {"tldr": "PLaMo 2 是一系列专注于日语的大型语言模型，采用混合Samba架构，通过持续预训练支持32K长上下文。它利用合成数据克服数据稀缺，并通过权重复用和结构化剪枝实现高效训练，使8B模型达到与之前100B模型相当的性能。通过SFT和DPO等后训练优化，PLaMo 2在日语基准测试上取得了最先进的成果，超越了同等规模的开源模型。", "motivation": "开发高性能、长上下文、专注于日语的大型语言模型，同时克服日语数据稀缺问题，并通过计算效率优化使其在推理时表现出色。", "method": "该研究采用混合Samba-based架构，通过持续预训练过渡到全注意力机制以支持32K长上下文。训练利用大量合成语料库解决数据稀缺。计算效率通过权重复用和结构化剪枝实现。后训练流程包括监督微调（SFT）和直接偏好优化（DPO），并结合合成日语指令数据和模型合并技术。推理优化则通过vLLM和量化技术实现。", "result": "PLaMo 2 系列模型在日语基准测试上取得了最先进的成果。特别是，通过高效剪枝，一个8B模型实现了与之前100B模型相当的性能。这些模型在指令遵循、语言流畅性和日语特定知识方面优于同等规模的开源模型。", "conclusion": "PLaMo 2 成功开发了一系列高效、高性能的日语大型语言模型。通过结合创新的混合架构、合成数据利用、高效剪枝和先进的后训练技术，PLaMo 2 在长上下文支持和日语特定任务上树立了新的标杆，并在计算效率和性能之间取得了卓越的平衡。"}}
{"id": "2509.04745", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04745", "abs": "https://arxiv.org/abs/2509.04745", "authors": ["Lee Kezar", "Zed Sehyr", "Jesse Thomason"], "title": "Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization", "comment": null, "summary": "Sign language datasets are often not representative in terms of vocabulary,\nunderscoring the need for models that generalize to unseen signs. Vector\nquantization is a promising approach for learning discrete, token-like\nrepresentations, but it has not been evaluated whether the learned units\ncapture spurious correlations that hinder out-of-vocabulary performance. This\nwork investigates two phonological inductive biases: Parameter Disentanglement,\nan architectural bias, and Phonological Semi-Supervision, a regularization\ntechnique, to improve isolated sign recognition of known signs and\nreconstruction quality of unseen signs with a vector-quantized autoencoder. The\nprimary finding is that the learned representations from the proposed model are\nmore effective for one-shot reconstruction of unseen signs and more\ndiscriminative for sign identification compared to a controlled baseline. This\nwork provides a quantitative analysis of how explicit, linguistically-motivated\nbiases can improve the generalization of learned representations of sign\nlanguage.", "AI": {"tldr": "本研究通过引入音系学归纳偏置（参数解耦和音系学半监督）改进了矢量量化自编码器，以提高手语模型对未见手语的泛化能力和已知手语的识别性能。", "motivation": "手语数据集在词汇量方面通常不具代表性，因此需要模型能够泛化到未见手语。现有矢量量化方法尚未评估其学习到的单元是否捕获了阻碍词汇外性能的虚假相关性。", "method": "使用矢量量化自编码器，并引入两种音系学归纳偏置：参数解耦（一种架构偏置）和音系学半监督（一种正则化技术），旨在改善已知手语的孤立手语识别和未见手语的重建质量。", "result": "与受控基线相比，所提出模型学习到的表征在未见手语的单次重建方面更有效，并且在手语识别方面更具判别性。", "conclusion": "本工作定量分析了明确的、受语言学启发的偏置如何能提高手语学习表征的泛化能力。"}}
{"id": "2509.05086", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05086", "abs": "https://arxiv.org/abs/2509.05086", "authors": ["Svetlana Pavlitska", "Haixi Fan", "Konstantin Ditschuneit", "J. Marius Zöllner"], "title": "Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers", "comment": "Accepted for publication at the STREAM workshop at ICCV 2025", "summary": "Robustifying convolutional neural networks (CNNs) against adversarial attacks\nremains challenging and often requires resource-intensive countermeasures. We\nexplore the use of sparse mixture-of-experts (MoE) layers to improve robustness\nby replacing selected residual blocks or convolutional layers, thereby\nincreasing model capacity without additional inference cost. On ResNet\narchitectures trained on CIFAR-100, we find that inserting a single MoE layer\nin the deeper stages leads to consistent improvements in robustness under PGD\nand AutoPGD attacks when combined with adversarial training. Furthermore, we\ndiscover that when switch loss is used for balancing, it causes routing to\ncollapse onto a small set of overused experts, thereby concentrating\nadversarial training on these paths and inadvertently making them more robust.\nAs a result, some individual experts outperform the gated MoE model in\nrobustness, suggesting that robust subpaths emerge through specialization. Our\ncode is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.", "AI": {"tldr": "本文通过在CNN中引入稀疏专家混合（MoE）层，结合对抗训练，提高了模型对PGD和AutoPGD攻击的鲁棒性。研究发现，平衡损失（switch loss）会导致路由塌陷，使得部分专家过度使用并因此变得更加鲁棒，甚至形成更鲁棒的子路径。", "motivation": "使卷积神经网络（CNN）对抗对抗性攻击仍然具有挑战性，且通常需要大量资源。研究旨在探索一种无需额外推理成本即可提高模型容量和鲁棒性的方法。", "method": "研究人员在ResNet架构上，用稀疏专家混合（MoE）层替换选定的残差块或卷积层，并在CIFAR-100数据集上进行训练。他们结合对抗训练（PGD和AutoPGD）评估了模型的鲁棒性，并分析了使用平衡损失（switch loss）对专家路由和鲁棒性的影响。", "result": "在ResNet架构上，将单个MoE层插入更深阶段，结合对抗训练，能持续提高模型在PGD和AutoPGD攻击下的鲁棒性。此外，当使用平衡损失时，路由会塌陷到一小组过度使用的专家上，从而将对抗训练集中在这些路径上，无意中使它们更加鲁棒。结果显示，一些单独的专家在鲁棒性方面甚至优于整个门控MoE模型。", "conclusion": "通过专家混合层的引入和平衡损失的使用，模型中出现了鲁棒的子路径，这表明鲁棒性可以通过专家特化而产生。这种方法在不增加额外推理成本的情况下，有效地提升了CNN的对抗鲁棒性。"}}
{"id": "2509.05066", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05066", "abs": "https://arxiv.org/abs/2509.05066", "authors": ["Matteo Bortoletto", "Constantin Ruhdorfer", "Andreas Bulling"], "title": "ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions", "comment": "EMNLP 2025 (Main)", "summary": "Most existing Theory of Mind (ToM) benchmarks for foundation models rely on\nvariations of the Sally-Anne test, offering only a very limited perspective on\nToM and neglecting the complexity of human social interactions. To address this\ngap, we propose ToM-SSI: a new benchmark specifically designed to test ToM\ncapabilities in environments rich with social interactions and spatial\ndynamics. While current ToM benchmarks are limited to text-only or dyadic\ninteractions, ToM-SSI is multimodal and includes group interactions of up to\nfour agents that communicate and move in situated environments. This unique\ndesign allows us to study, for the first time, mixed cooperative-obstructive\nsettings and reasoning about multiple agents' mental state in parallel, thus\ncapturing a wider range of social cognition than existing benchmarks. Our\nevaluations reveal that the current models' performance is still severely\nlimited, especially in these new tasks, highlighting critical gaps for future\nresearch.", "AI": {"tldr": "本文提出ToM-SSI，一个多模态、支持多智能体群体互动的全新心智理论（ToM）基准，旨在解决现有基准的局限性，并揭示了当前模型在此类复杂任务中的显著不足。", "motivation": "现有针对基础模型的心智理论（ToM）基准主要依赖于Sally-Anne测试的变体，视角非常有限，未能捕捉人类社会互动的复杂性，且仅限于文本或双边互动。", "method": "提出ToM-SSI基准，专为测试基础模型在富含社会互动和空间动态环境中的ToM能力。它支持多模态，包含多达四个智能体的群体互动，智能体可在情境化环境中交流和移动。该设计首次允许研究混合合作-阻碍设置以及并行推理多个智能体的心智状态。", "result": "评估结果表明，当前模型的性能仍然受到严重限制，尤其是在ToM-SSI提出的新任务中表现不佳。", "conclusion": "当前基础模型在复杂社会互动场景中的ToM能力存在关键性差距，为未来研究指明了方向。"}}
{"id": "2509.04770", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04770", "abs": "https://arxiv.org/abs/2509.04770", "authors": ["Zucheng Liang", "Wenxin Wei", "Kaijie Zhang", "Hongyi Chen"], "title": "Research on Multi-hop Inference Optimization of LLM Based on MQUAKE Framework", "comment": null, "summary": "Accurately answering complex questions has consistently been a significant\nchallenge for Large Language Models (LLMs). To address this, this paper\nproposes a multi-hop question decomposition method for complex questions,\nbuilding upon research within the MQUAKE framework. Utilizing the LLAMA3 model,\nwe systematically investigate the impact of multi-hop question decomposition\nwithin knowledge graphs on model comprehension and reasoning accuracy, both\nbefore and after model training. In our experiments, we systematically\npartitioned and converted the MQUAKE-T dataset into two distinct formats: a\nsingle-hop dataset designed for directly answering complex questions, and a\nmulti-hop dataset constructed using the multi-hop question decomposition\nmethod. We then fine-tuned the LLAMA3 model on these datasets and conducted\ninference tests. Our results demonstrate that, without fine-tuning the LLM, the\nprediction performance based on the multi-hop question decomposition method\nsignificantly outperforms the method of directly answering complex questions.\nAfter fine-tuning using the LoRA (Low-Rank Adaptation) method, the performance\nof both approaches improved compared to the untrained baseline. Crucially, the\nmethod utilizing multi-hop decomposition consistently maintained its\nsuperiority. These findings validate the effectiveness of the multi-hop\ndecomposition method both before and after training, demonstrating its\ncapability to effectively enhance the LLM's ability to answer complex\nquestions.", "AI": {"tldr": "本文提出并验证了一种基于多跳问题分解的方法，显著提高了大型语言模型（LLM）回答复杂问题的准确性，无论模型是否经过微调。", "motivation": "大型语言模型（LLMs）在准确回答复杂问题方面一直面临重大挑战。", "method": "本文基于MQUAKE框架，提出了一种针对复杂问题的多跳问题分解方法。利用LLAMA3模型，将MQUAKE-T数据集划分为直接回答复杂问题的单跳数据集和使用多跳分解方法构建的多跳数据集。然后，使用LoRA（Low-Rank Adaptation）方法对LLAMA3模型在这些数据集上进行微调，并进行推理测试，系统研究了多跳问题分解对模型理解和推理准确性的影响。", "result": "实验结果表明，在未微调LLM的情况下，基于多跳问题分解方法的预测性能显著优于直接回答复杂问题的方法。经过LoRA微调后，两种方法的性能均有所提高，但多跳分解方法始终保持其优越性。", "conclusion": "这些发现验证了多跳分解方法在训练前后均有效，证明了其能够有效增强LLM回答复杂问题的能力。"}}
{"id": "2509.05092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05092", "abs": "https://arxiv.org/abs/2509.05092", "authors": ["Mainak Biswas", "Ambedkar Dukkipati", "Devarajan Sridharan"], "title": "Semi-supervised Deep Transfer for Regression without Domain Alignment", "comment": "15 pages, 6 figures, International Conference on Computer Vision 2025", "summary": "Deep learning models deployed in real-world applications (e.g., medicine)\nface challenges because source models do not generalize well to domain-shifted\ntarget data. Many successful domain adaptation (DA) approaches require full\naccess to source data. Yet, such requirements are unrealistic in scenarios\nwhere source data cannot be shared either because of privacy concerns or\nbecause it is too large and incurs prohibitive storage or computational costs.\nMoreover, resource constraints may limit the availability of labeled targets.\nWe illustrate this challenge in a neuroscience setting where source data are\nunavailable, labeled target data are meager, and predictions involve\ncontinuous-valued outputs. We build upon Contradistinguisher (CUDA), an\nefficient framework that learns a shared model across the labeled source and\nunlabeled target samples, without intermediate representation alignment. Yet,\nCUDA was designed for unsupervised DA, with full access to source data, and for\nclassification tasks. We develop CRAFT -- a Contradistinguisher-based\nRegularization Approach for Flexible Training -- for source-free (SF),\nsemi-supervised transfer of pretrained models in regression tasks. We showcase\nthe efficacy of CRAFT in two neuroscience settings: gaze prediction with\nelectroencephalography (EEG) data and ``brain age'' prediction with structural\nMRI data. For both datasets, CRAFT yielded up to 9% improvement in\nroot-mean-squared error (RMSE) over fine-tuned models when labeled training\nexamples were scarce. Moreover, CRAFT leveraged unlabeled target data and\noutperformed four competing state-of-the-art source-free domain adaptation\nmodels by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two\nother real-world regression benchmarks. We propose CRAFT as an efficient\napproach for source-free, semi-supervised deep transfer for regression that is\nubiquitous in biology and medicine.", "AI": {"tldr": "本文提出CRAFT，一种基于Contradistinguisher的正则化方法，用于源数据不可用、目标数据标签稀缺的回归任务的半监督深度迁移学习。CRAFT在神经科学和其他真实世界回归基准测试中表现出色，优于现有微调和最先进的无源域适应模型。", "motivation": "深度学习模型在实际应用中（如医学）面临领域漂移导致泛化能力差的问题。许多成功的域适应方法需要完全访问源数据，但这在隐私或成本限制下不切实际。此外，标记的目标数据也可能稀缺，尤其是在连续值输出的回归任务中。", "method": "本文基于Contradistinguisher (CUDA) 框架开发了CRAFT（Contradistinguisher-based Regularization Approach for Flexible Training）。与CUDA（设计用于无监督DA、完全访问源数据、分类任务）不同，CRAFT专为源数据不可用（Source-Free, SF）、半监督迁移和回归任务而设计，无需中间表示对齐。", "result": "CRAFT在标记训练样本稀缺时，相较于微调模型，均方根误差（RMSE）最高提升9%。它有效利用了未标记的目标数据，并超越了四种最先进的无源域适应模型超过3%。CRAFT在注视预测（EEG数据）和“脑龄”预测（MRI数据）两个神经科学场景以及另外两个真实世界回归基准测试中均展现了其有效性。", "conclusion": "CRAFT是一种高效的、适用于生物学和医学中普遍存在的回归任务的无源、半监督深度迁移学习方法。"}}
{"id": "2509.05100", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05100", "abs": "https://arxiv.org/abs/2509.05100", "authors": ["Zhiyu Cao", "Peifeng Li", "Qiaoming Zhu"], "title": "ICR: Iterative Clarification and Rewriting for Conversational Search", "comment": null, "summary": "Most previous work on Conversational Query Rewriting employs an end-to-end\nrewriting paradigm. However, this approach is hindered by the issue of multiple\nfuzzy expressions within the query, which complicates the simultaneous\nidentification and rewriting of multiple positions. To address this issue, we\npropose a novel framework ICR (Iterative Clarification and Rewriting), an\niterative rewriting scheme that pivots on clarification questions. Within this\nframework, the model alternates between generating clarification questions and\nrewritten queries. The experimental results show that our ICR can continuously\nimprove retrieval performance in the clarification-rewriting iterative process,\nthereby achieving state-of-the-art performance on two popular datasets.", "AI": {"tldr": "本文提出了一种名为ICR（迭代澄清与重写）的新框架，通过迭代生成澄清问题和重写查询来解决对话查询重写中多重模糊表达的问题，并取得了最先进的性能。", "motivation": "以往的对话查询重写方法多采用端到端范式，但这种方法难以同时识别和重写查询中存在的多个模糊表达，导致性能受限。", "method": "本文提出ICR框架，采用迭代重写方案，以澄清问题为核心。模型在此框架下交替生成澄清问题和重写后的查询。", "result": "实验结果表明，ICR在澄清-重写迭代过程中能持续提升检索性能，并在两个流行数据集上取得了最先进的性能。", "conclusion": "ICR通过迭代澄清和重写的方法，有效解决了对话查询中多重模糊表达的挑战，显著提升了查询重写的质量和检索性能。"}}
{"id": "2509.04794", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04794", "abs": "https://arxiv.org/abs/2509.04794", "authors": ["Gunmay Handa", "Zekun Wu", "Adriano Koshiyama", "Philip Treleaven"], "title": "Personality as a Probe for LLM Evaluation: Method Trade-offs and Downstream Effects", "comment": null, "summary": "Personality manipulation in large language models (LLMs) is increasingly\napplied in customer service and agentic scenarios, yet its mechanisms and\ntrade-offs remain unclear. We present a systematic study of personality control\nusing the Big Five traits, comparing in-context learning (ICL),\nparameter-efficient fine-tuning (PEFT), and mechanistic steering (MS). Our\ncontributions are fourfold. First, we construct a contrastive dataset with\nbalanced high/low trait responses, enabling effective steering vector\ncomputation and fair cross-method evaluation. Second, we introduce a unified\nevaluation framework based on within-run $\\Delta$ analysis that disentangles,\nreasoning capability, agent performance, and demographic bias across MMLU,\nGAIA, and BBQ benchmarks. Third, we develop trait purification techniques to\nseparate openness from conscientiousness, addressing representational overlap\nin trait encoding. Fourth, we propose a three-level stability framework that\nquantifies method-, trait-, and combination-level robustness, offering\npractical guidance under deployment constraints. Experiments on Gemma-2-2B-IT\nand LLaMA-3-8B-Instruct reveal clear trade-offs: ICL achieves strong alignment\nwith minimal capability loss, PEFT delivers the highest alignment at the cost\nof degraded task performance, and MS provides lightweight runtime control with\ncompetitive effectiveness. Trait-level analysis shows openness as uniquely\nchallenging, agreeableness as most resistant to ICL, and personality encoding\nconsolidating around intermediate layers. Taken together, these results\nestablish personality manipulation as a multi-level probe into behavioral\nrepresentation, linking surface conditioning, parameter encoding, and\nactivation-level steering, and positioning mechanistic steering as a\nlightweight alternative to fine-tuning for both deployment and\ninterpretability.", "AI": {"tldr": "本文系统研究了大型语言模型（LLMs）中基于大五人格特质的人格控制机制及权衡，比较了上下文学习（ICL）、参数高效微调（PEFT）和机制转向（MS）三种方法，并揭示了它们在对齐、能力损失和部署方面的不同表现。", "motivation": "LLMs中的人格操纵在客户服务和智能体场景中应用日益广泛，但其内在机制和权衡尚未明确。", "method": ["构建了一个平衡高/低特质响应的对比数据集，用于向量计算和跨方法评估。", "引入了一个统一的评估框架，基于内部运行的$\\Delta$分析，以解耦推理能力、智能体性能和人口统计学偏见。", "开发了特质净化技术，以分离开放性和尽责性，解决特质编码中的重叠问题。", "提出了一个三级稳定性框架，量化方法、特质和组合层面的鲁棒性。", "在Gemma-2-2B-IT和LLaMA-3-8B-Instruct模型上，比较了ICL、PEFT和MS三种方法。"], "result": ["ICL实现了强大的对齐，同时能力损失最小。", "PEFT提供了最高的对齐度，但代价是任务性能下降。", "MS提供了轻量级的运行时控制，且效果具有竞争力。", "特质层面分析显示，开放性特质最具挑战性，宜人性对ICL最不敏感，人格编码倾向于在中间层巩固。"], "conclusion": "人格操纵是行为表征的多层次探针，它连接了表面条件反射、参数编码和激活层转向。机制转向（MS）作为一种轻量级替代方案，在部署和可解释性方面均可替代微调。"}}
{"id": "2509.05131", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05131", "abs": "https://arxiv.org/abs/2509.05131", "authors": ["Arianna Rampini", "Kanika Madan", "Bruno Roy", "AmirHossein Zamani", "Derek Cheung"], "title": "A Scalable Attention-Based Approach for Image-to-3D Texture Mapping", "comment": null, "summary": "High-quality textures are critical for realistic 3D content creation, yet\nexisting generative methods are slow, rely on UV maps, and often fail to remain\nfaithful to a reference image. To address these challenges, we propose a\ntransformer-based framework that predicts a 3D texture field directly from a\nsingle image and a mesh, eliminating the need for UV mapping and differentiable\nrendering, and enabling faster texture generation. Our method integrates a\ntriplane representation with depth-based backprojection losses, enabling\nefficient training and faster inference. Once trained, it generates\nhigh-fidelity textures in a single forward pass, requiring only 0.2s per shape.\nExtensive qualitative, quantitative, and user preference evaluations\ndemonstrate that our method outperforms state-of-the-art baselines on\nsingle-image texture reconstruction in terms of both fidelity to the input\nimage and perceptual quality, highlighting its practicality for scalable,\nhigh-quality, and controllable 3D content creation.", "AI": {"tldr": "该论文提出了一种基于Transformer的框架，能够从单张图像和网格直接预测3D纹理场，无需UV映射或可微渲染，实现快速、高质量的纹理生成。", "motivation": "现有的3D纹理生成方法速度慢、依赖UV映射，并且难以忠实还原参考图像，这阻碍了高质量3D内容的高效创建。", "method": "本文提出一个基于Transformer的框架，直接从单张图像和网格预测3D纹理场。该方法结合了三平面表示（triplane representation）和基于深度的反投影损失（depth-based backprojection losses），从而实现了高效训练和快速推理，并消除了对UV映射和可微渲染的需求。", "result": "训练后，该方法能以单次前向传播生成高保真纹理，每个形状仅需0.2秒。定性、定量和用户偏好评估均表明，在单图像纹理重建方面，该方法在对输入图像的保真度和感知质量上均优于现有最先进的基线方法。", "conclusion": "该方法为可扩展、高质量和可控的3D内容创建提供了实用工具，解决了传统纹理生成方法的痛点，显著提高了效率和质量。"}}
{"id": "2509.05218", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05218", "abs": "https://arxiv.org/abs/2509.05218", "authors": ["Chang Dai", "Hongyu Shan", "Mingyang Song", "Di Liang"], "title": "HoPE: Hyperbolic Rotary Positional Encoding for Stable Long-Range Dependency Modeling in Large Language Models", "comment": "This paper proposes Hyperbolic Rotary Positional Encoding (HoPE), a\n  geometric reformulation of positional encoding inspired by Lorentz\n  transformations. HoPE addresses limitations of existing methods like RoPE by\n  enabling stable long-distance dependency modeling. Code and data will be made\n  available upon publication", "summary": "Positional encoding mechanisms enable Transformers to model sequential\nstructure and long-range dependencies in text. While absolute positional\nencodings struggle with extrapolation to longer sequences due to fixed\npositional representations, and relative approaches like Alibi exhibit\nperformance degradation on extremely long contexts, the widely-used Rotary\nPositional Encoding (RoPE) introduces oscillatory attention patterns that\nhinder stable long-distance dependency modelling. We address these limitations\nthrough a geometric reformulation of positional encoding. Drawing inspiration\nfrom Lorentz transformations in hyperbolic geometry, we propose Hyperbolic\nRotary Positional Encoding (HoPE), which leverages hyperbolic functions to\nimplement Lorentz rotations on token representations. Theoretical analysis\ndemonstrates that RoPE is a special case of our generalized formulation. HoPE\nfundamentally resolves RoPE's slation issues by enforcing monotonic decay of\nattention weights with increasing token distances. Extensive experimental\nresults, including perplexity evaluations under several extended sequence\nbenchmarks, show that HoPE consistently exceeds existing positional encoding\nmethods. These findings underscore HoPE's enhanced capacity for representing\nand generalizing long-range dependencies. Data and code will be available.", "AI": {"tldr": "本文提出了一种基于双曲几何的旋转位置编码（HoPE），通过借鉴洛伦兹变换，解决了传统RoPE在长序列中注意力模式不稳定的问题，并在多项长序列基准测试中表现优于现有方法。", "motivation": "现有的位置编码机制存在局限性：绝对位置编码难以泛化到更长序列；Alibi在超长上下文上性能下降；广泛使用的RoPE引入了振荡注意力模式，阻碍了稳定的长距离依赖建模。", "method": "本文通过对位置编码进行几何重构，提出了双曲旋转位置编码（HoPE）。HoPE受双曲几何中的洛伦兹变换启发，利用双曲函数对token表示进行洛伦兹旋转。理论分析表明，RoPE是HoPE的特例。HoPE通过强制注意力权重随token距离增加而单调衰减，从根本上解决了RoPE的平移问题。", "result": "广泛的实验结果，包括在多个扩展序列基准下的困惑度评估，表明HoPE始终优于现有的位置编码方法。这些发现强调了HoPE在表示和泛化长距离依赖方面的增强能力。", "conclusion": "HoPE通过其独特的双曲几何设计，有效解决了现有位置编码在长距离依赖建模中的挑战，特别是在稳定性和泛化能力方面表现出色，为Transformer模型处理长序列提供了更优的解决方案。"}}
{"id": "2509.04796", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04796", "abs": "https://arxiv.org/abs/2509.04796", "authors": ["Figarri Keisha", "Zekun Wu", "Ze Wang", "Adriano Koshiyama", "Philip Treleaven"], "title": "Knowledge Collapse in LLMs: When Fluency Survives but Facts Fail under Recursive Synthetic Training", "comment": null, "summary": "Large language models increasingly rely on synthetic data due to\nhuman-written content scarcity, yet recursive training on model-generated\noutputs leads to model collapse, a degenerative process threatening factual\nreliability. We define knowledge collapse as a distinct three-stage phenomenon\nwhere factual accuracy deteriorates while surface fluency persists, creating\n\"confidently wrong\" outputs that pose critical risks in accuracy-dependent\ndomains. Through controlled experiments with recursive synthetic training, we\ndemonstrate that collapse trajectory and timing depend critically on\ninstruction format, distinguishing instruction-following collapse from\ntraditional model collapse through its conditional, prompt-dependent nature. We\npropose domain-specific synthetic training as a targeted mitigation strategy\nthat achieves substantial improvements in collapse resistance while maintaining\ncomputational efficiency. Our evaluation framework combines model-centric\nindicators with task-centric metrics to detect distinct degradation phases,\nenabling reproducible assessment of epistemic deterioration across different\nlanguage models. These findings provide both theoretical insights into collapse\ndynamics and practical guidance for sustainable AI training in\nknowledge-intensive applications where accuracy is paramount.", "AI": {"tldr": "由于合成数据训练，大型语言模型面临“知识崩溃”，即事实准确性下降但表面流畅性保持的现象。研究发现崩溃轨迹和时间取决于指令格式，并提出领域特定合成训练作为有效的缓解策略。", "motivation": "人类编写内容稀缺导致大型语言模型日益依赖合成数据，但模型在自身生成输出上进行递归训练会导致模型崩溃，特别是“知识崩溃”，即模型生成“自信但错误”的输出，对依赖准确性的领域构成严重风险。", "method": "通过递归合成训练进行受控实验，定义了知识崩溃为三个阶段的现象。研究了指令格式对崩溃轨迹和时间的影响。提出了领域特定合成训练作为缓解策略。评估框架结合了以模型为中心的指标和以任务为中心的度量来检测退化阶段。", "result": "知识崩溃是一个独特的三阶段现象，表现为事实准确性下降但表面流畅性持续存在。崩溃轨迹和时间关键取决于指令格式，区分了有条件的、依赖提示的指令遵循崩溃。领域特定合成训练作为缓解策略，显著提高了抗崩溃能力并保持了计算效率。", "conclusion": "这些发现提供了关于崩溃动力学的理论见解，并为知识密集型应用中可持续的AI训练提供了实用指导，其中准确性至关重要。"}}
{"id": "2509.05144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05144", "abs": "https://arxiv.org/abs/2509.05144", "authors": ["Chaolei Wang", "Yang Luo", "Jing Du", "Siyu Chen", "Yiping Chen", "Ting Han"], "title": "SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing", "comment": null, "summary": "Accurate 3D instance segmentation is crucial for high-quality scene\nunderstanding in the 3D vision domain. However, 3D instance segmentation based\non 2D-to-3D lifting approaches struggle to produce precise instance-level\nsegmentation, due to accumulated errors introduced during the lifting process\nfrom ambiguous semantic guidance and insufficient depth constraints. To tackle\nthese challenges, we propose splitting and growing reliable semantic mask for\nhigh-fidelity 3D instance segmentation (SGS-3D), a novel \"split-then-grow\"\nframework that first purifies and splits ambiguous lifted masks using geometric\nprimitives, and then grows them into complete instances within the scene.\nUnlike existing approaches that directly rely on raw lifted masks and sacrifice\nsegmentation accuracy, SGS-3D serves as a training-free refinement method that\njointly fuses semantic and geometric information, enabling effective\ncooperation between the two levels of representation. Specifically, for\nsemantic guidance, we introduce a mask filtering strategy that leverages the\nco-occurrence of 3D geometry primitives to identify and remove ambiguous masks,\nthereby ensuring more reliable semantic consistency with the 3D object\ninstances. For the geometric refinement, we construct fine-grained object\ninstances by exploiting both spatial continuity and high-level features,\nparticularly in the case of semantic ambiguity between distinct objects.\nExperimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that\nSGS-3D substantially improves segmentation accuracy and robustness against\ninaccurate masks from pre-trained models, yielding high-fidelity object\ninstances while maintaining strong generalization across diverse indoor and\noutdoor environments. Code is available in the supplementary materials.", "AI": {"tldr": "本文提出SGS-3D，一个“先分割后增长”的无训练框架，通过结合语义和几何信息，有效提炼和完善由2D-to-3D提升方法产生的模糊3D实例分割结果，显著提高了分割精度和鲁棒性。", "motivation": "基于2D-to-3D提升的方法在生成精确3D实例分割时，由于语义指导模糊和深度约束不足，在提升过程中会累积误差，导致实例级分割精度不足。因此，需要一种方法来解决这些挑战。", "method": "本文提出了SGS-3D框架，采用“先分割后增长”策略。首先，它利用几何基元纯化并分割模糊的提升掩模；然后，将这些掩模增长为完整的场景实例。SGS-3D是一种无训练的细化方法，它融合了语义和几何信息。具体来说，它引入了掩模过滤策略，利用3D几何基元的共现来识别和移除模糊掩模，以确保与3D对象实例更可靠的语义一致性。在几何细化方面，它通过利用空间连续性和高级特征来构建细粒度对象实例，尤其是在不同对象之间存在语义歧义的情况下。", "result": "在ScanNet200、ScanNet++和KITTI-360数据集上的实验结果表明，SGS-3D显著提高了分割精度和对预训练模型不准确掩模的鲁棒性，生成了高保真度的对象实例，并在多样化的室内外环境中保持了强大的泛化能力。", "conclusion": "SGS-3D通过有效融合语义和几何信息，成功解决了2D-to-3D提升方法在3D实例分割中累积误差的问题，实现了高精度、高鲁棒性及强泛化能力的3D实例分割。"}}
{"id": "2509.05230", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05230", "abs": "https://arxiv.org/abs/2509.05230", "authors": ["Aysenur Kocak", "Shuo Yang", "Bardh Prenkaj", "Gjergji Kasneci"], "title": "CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models", "comment": "Accepted at the Conference on Empirical Methods in Natural Language\n  Processing (EMNLP 2025)", "summary": "Pre-trained language models have achieved remarkable success across diverse\napplications but remain susceptible to spurious, concept-driven correlations\nthat impair robustness and fairness. In this work, we introduce CURE, a novel\nand lightweight framework that systematically disentangles and suppresses\nconceptual shortcuts while preserving essential content information. Our method\nfirst extracts concept-irrelevant representations via a dedicated content\nextractor reinforced by a reversal network, ensuring minimal loss of\ntask-relevant information. A subsequent controllable debiasing module employs\ncontrastive learning to finely adjust the influence of residual conceptual\ncues, enabling the model to either diminish harmful biases or harness\nbeneficial correlations as appropriate for the target task. Evaluated on the\nIMDB and Yelp datasets using three pre-trained architectures, CURE achieves an\nabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,\nwhile introducing minimal computational overhead. Our approach establishes a\nflexible, unsupervised blueprint for combating conceptual biases, paving the\nway for more reliable and fair language understanding systems.", "AI": {"tldr": "CURE是一个轻量级框架，通过解耦和抑制概念性捷径，同时保留内容信息，提高预训练语言模型的鲁棒性和公平性。它在IMDB和Yelp数据集上取得了显著的F1分数提升，且计算开销极小。", "motivation": "预训练语言模型在应用中表现出色，但容易受到虚假、概念驱动的相关性影响，这会损害模型的鲁棒性和公平性。", "method": "CURE框架包含两个主要部分：1) 内容提取器，通过逆转网络强化，提取与概念无关的表示，确保任务相关信息损失最小；2) 可控去偏模块，利用对比学习精细调整残余概念线索的影响，以减少有害偏见或利用有益关联。", "result": "在IMDB和Yelp数据集上，使用三种预训练架构进行评估，CURE在IMDB上F1分数绝对提升了10点，在Yelp上提升了2点，同时引入的计算开销极小。", "conclusion": "该方法为对抗概念性偏见提供了一个灵活、无监督的蓝图，为更可靠和公平的语言理解系统铺平了道路。"}}
{"id": "2509.04802", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04802", "abs": "https://arxiv.org/abs/2509.04802", "authors": ["Ilham Wicaksono", "Zekun Wu", "Theo King", "Adriano Koshiyama", "Philip Treleaven"], "title": "Mind the Gap: Evaluating Model- and Agentic-Level Vulnerabilities in LLMs with Action Graphs", "comment": null, "summary": "As large language models transition to agentic systems, current safety\nevaluation frameworks face critical gaps in assessing deployment-specific\nrisks. We introduce AgentSeer, an observability-based evaluation framework that\ndecomposes agentic executions into granular action and component graphs,\nenabling systematic agentic-situational assessment. Through cross-model\nvalidation on GPT-OSS-20B and Gemini-2.0-flash using HarmBench single turn and\niterative refinement attacks, we demonstrate fundamental differences between\nmodel-level and agentic-level vulnerability profiles. Model-level evaluation\nreveals baseline differences: GPT-OSS-20B (39.47% ASR) versus Gemini-2.0-flash\n(50.00% ASR), with both models showing susceptibility to social engineering\nwhile maintaining logic-based attack resistance. However, agentic-level\nassessment exposes agent-specific risks invisible to traditional evaluation. We\ndiscover \"agentic-only\" vulnerabilities that emerge exclusively in agentic\ncontexts, with tool-calling showing 24-60% higher ASR across both models.\nCross-model analysis reveals universal agentic patterns, agent transfer\noperations as highest-risk tools, semantic rather than syntactic vulnerability\nmechanisms, and context-dependent attack effectiveness, alongside\nmodel-specific security profiles in absolute ASR levels and optimal injection\nstrategies. Direct attack transfer from model-level to agentic contexts shows\ndegraded performance (GPT-OSS-20B: 57% human injection ASR; Gemini-2.0-flash:\n28%), while context-aware iterative attacks successfully compromise objectives\nthat failed at model-level, confirming systematic evaluation gaps. These\nfindings establish the urgent need for agentic-situation evaluation paradigms,\nwith AgentSeer providing the standardized methodology and empirical validation.", "AI": {"tldr": "本文介绍了AgentSeer，一个基于可观察性的评估框架，用于评估大型语言模型向智能体系统转型时面临的部署特定风险。研究发现，智能体层面的漏洞与模型层面存在显著差异，并揭示了“仅智能体”漏洞，尤其是在工具调用和智能体转移操作中，凸显了对智能体情境评估的迫切需求。", "motivation": "随着大型语言模型向智能体系统过渡，现有的安全评估框架在评估部署特定风险方面存在严重不足，无法有效捕捉智能体特有的漏洞。", "method": "引入了AgentSeer框架，该框架基于可观察性，将智能体执行分解为细粒度的动作和组件图，以实现系统的智能体情境评估。通过对GPT-OSS-20B和Gemini-2.0-flash进行跨模型验证，并使用HarmBench单轮和迭代细化攻击来评估漏洞。", "result": "研究发现模型层面和智能体层面的漏洞特征存在根本差异。模型层面评估显示GPT-OSS-20B的ASR为39.47%，Gemini-2.0-flash为50.00%，两者都易受社会工程攻击但对基于逻辑的攻击具有抵抗力。然而，智能体层面评估揭示了传统评估无法发现的“仅智能体”漏洞，其中工具调用使两种模型的ASR提高了24-60%。跨模型分析揭示了普遍的智能体模式：智能体转移操作是风险最高的工具，漏洞机制是语义而非语法，攻击效果依赖于上下文。直接从模型层面到智能体层面的攻击效果下降（GPT-OSS-20B：57%，Gemini-2.0-flash：28%），而上下文感知的迭代攻击成功攻破了模型层面失败的目标。", "conclusion": "这些发现确立了对智能体情境评估范式的迫切需求，AgentSeer提供了标准化的方法论和实证验证，以解决这一关键评估空白。"}}
{"id": "2509.05188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05188", "abs": "https://arxiv.org/abs/2509.05188", "authors": ["Ariel Basso Madjoukeng", "Jérôme Fink", "Pierre Poitier", "Edith Belise Kenmogne", "Benoit Frenay"], "title": "SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition", "comment": null, "summary": "Sign language recognition (SLR) is a machine learning task aiming to identify\nsigns in videos. Due to the scarcity of annotated data, unsupervised methods\nlike contrastive learning have become promising in this field. They learn\nmeaningful representations by pulling positive pairs (two augmented versions of\nthe same instance) closer and pushing negative pairs (different from the\npositive pairs) apart. In SLR, in a sign video, only certain parts provide\ninformation that is truly useful for its recognition. Applying contrastive\nmethods to SLR raises two issues: (i) contrastive learning methods treat all\nparts of a video in the same way, without taking into account the relevance of\ncertain parts over others; (ii) shared movements between different signs make\nnegative pairs highly similar, complicating sign discrimination. These issues\nlead to learning non-discriminative features for sign recognition and poor\nresults in downstream tasks. In response, this paper proposes a self-supervised\nlearning framework designed to learn meaningful representations for SLR. This\nframework consists of two key components designed to work together: (i) a new\nself-supervised approach with free-negative pairs; (ii) a new data augmentation\ntechnique. This approach shows a considerable gain in accuracy compared to\nseveral contrastive and self-supervised methods, across linear evaluation,\nsemi-supervised learning, and transferability between sign languages.", "AI": {"tldr": "本文提出了一种新的自监督学习框架，包含无负样本对方法和数据增强技术，以解决手语识别中对比学习的局限性，显著提高了识别准确率。", "motivation": "手语识别领域标注数据稀缺，对比学习等无监督方法很有前景。然而，现有对比学习方法存在两个问题：1) 对视频所有部分一视同仁，未考虑关键信息部位的重要性；2) 不同手语之间共享动作导致负样本对高度相似，难以区分，进而导致学习到非判别性特征，下游任务表现不佳。", "method": "本文提出了一个为手语识别设计的自监督学习框架。该框架包含两个核心组件：1) 一种新的基于“无负样本对”（free-negative pairs）的自监督学习方法；2) 一种新的数据增强技术。这两个组件协同工作以学习有意义的表示。", "result": "与多种对比学习和自监督方法相比，本文提出的方法在准确性方面取得了显著提升。该提升在线性评估、半监督学习以及手语之间的可迁移性等多种评估设置中均得到验证。", "conclusion": "本文提出的自监督学习框架有效解决了手语识别中现有对比学习方法的局限性，通过引入无负样本对方法和新的数据增强技术，学习到了更具判别性的特征，显著提高了手语识别任务的性能和泛化能力。"}}
{"id": "2509.05249", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05249", "abs": "https://arxiv.org/abs/2509.05249", "authors": ["Yassine Taoudi-Benchekroun", "Klim Troyan", "Pascal Sager", "Stefan Gerber", "Lukas Tuggener", "Benjamin Grewe"], "title": "COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization", "comment": "10 main pages, 3 figure, appendix available", "summary": "The ability to compose learned concepts and apply them in novel settings is\nkey to human intelligence, but remains a persistent limitation in\nstate-of-the-art machine learning models. To address this issue, we introduce\nCOGITAO, a modular and extensible data generation framework and benchmark\ndesigned to systematically study compositionality and generalization in visual\ndomains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs\nrule-based tasks which apply a set of transformations to objects in grid-like\nenvironments. It supports composition, at adjustable depth, over a set of 28\ninteroperable transformations, along with extensive control over grid\nparametrization and object properties. This flexibility enables the creation of\nmillions of unique task rules -- surpassing concurrent datasets by several\norders of magnitude -- across a wide range of difficulties, while allowing\nvirtually unlimited sample generation per rule. We provide baseline experiments\nusing state-of-the-art vision models, highlighting their consistent failures to\ngeneralize to novel combinations of familiar elements, despite strong in-domain\nperformance. COGITAO is fully open-sourced, including all code and datasets, to\nsupport continued research in this field.", "AI": {"tldr": "COGITAO是一个模块化、可扩展的数据生成框架和基准，旨在系统地研究视觉领域中的组合性和泛化能力，并揭示现有模型在此方面的不足。", "motivation": "人类智能的关键在于能够组合已学习的概念并将其应用于新颖场景，但这仍是当前最先进机器学习模型的一个持续性限制。", "method": "COGITAO受ARC-AGI启发，构建基于规则的任务，在网格环境中对对象应用一系列变换。它支持可调节深度的组合，包含28种可互操作的变换，并能广泛控制网格参数和对象属性。这种灵活性使其能创建数百万个独特的任务规则，并为每个规则生成几乎无限的样本。", "result": "使用最先进的视觉模型进行的基线实验表明，尽管这些模型在域内表现强劲，但它们始终无法泛化到熟悉元素的新颖组合。", "conclusion": "COGITAO提供了一个开放源代码的工具和基准，以支持在该领域持续研究组合性和泛化能力，并突显了现有模型在应对复杂组合任务时的局限性。"}}
{"id": "2509.04813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04813", "abs": "https://arxiv.org/abs/2509.04813", "authors": ["Alexandre Nikolaev", "Yu-Ying Chuang", "R. Harald Baayen"], "title": "Analyzing Finnish Inflectional Classes through Discriminative Lexicon and Deep Learning Models", "comment": null, "summary": "Descriptions of complex nominal or verbal systems make use of inflectional\nclasses. Inflectional classes bring together nouns which have similar stem\nchanges and use similar exponents in their paradigms. Although inflectional\nclasses can be very useful for language teaching as well as for setting up\nfinite state morphological systems, it is unclear whether inflectional classes\nare cognitively real, in the sense that native speakers would need to discover\nthese classes in order to learn how to properly inflect the nouns of their\nlanguage. This study investigates whether the Discriminative Lexicon Model\n(DLM) can understand and produce Finnish inflected nouns without setting up\ninflectional classes, using a dataset with 55,271 inflected nouns of 2000\nhigh-frequency Finnish nouns from 49 inflectional classes. Several DLM\ncomprehension and production models were set up. Some models were not informed\nabout frequency of use, and provide insight into learnability with infinite\nexposure (endstate learning). Other models were set up from a usage based\nperspective, and were trained with token frequencies being taken into\nconsideration (frequency-informed learning). On training data, models performed\nwith very high accuracies. For held-out test data, accuracies decreased, as\nexpected, but remained acceptable. Across most models, performance increased\nfor inflectional classes with more types, more lower-frequency words, and more\nhapax legomena, mirroring the productivity of the inflectional classes. The\nmodel struggles more with novel forms of unproductive and less productive\nclasses, and performs far better for unseen forms belonging to productive\nclasses. However, for usage-based production models, frequency was the dominant\npredictor of model performance, and correlations with measures of productivity\nwere tenuous or absent.", "AI": {"tldr": "本研究探讨了判别词汇模型（DLM）在没有明确设置屈折类的情况下，能否理解和生成芬兰语屈折名词，并发现其性能与屈折类的生产力及词频相关。", "motivation": "屈折类在语言教学和形态学系统中很有用，但尚不清楚它们是否具有认知真实性，即母语者是否需要发现这些类别才能正确学习名词变格。", "method": "使用判别词汇模型（DLM），数据集包含来自49个屈折类的2000个高频芬兰语名词的55,271个屈折形式。设置了多种DLM理解和生成模型，包括未考虑词频的模型（无限暴露学习）和考虑词频的模型（基于使用频率的学习）。", "result": "模型在训练数据上表现出高准确性，在测试数据上准确性有所下降但仍可接受。在大多数模型中，对于类型更多、低频词更多、单现词更多的屈折类，性能有所提高，这反映了屈折类的生产力。模型在非生产性和生产力较低的类别的新颖形式上表现较差，但在生产性类别中表现更好。然而，对于基于使用的生成模型，词频是模型性能的主要预测因子，与生产力指标的相关性微弱或缺失。", "conclusion": "DLM无需明确的屈折类即可理解和生成芬兰语屈折名词，这表明屈折类可能不具备认知真实性。在基于使用的学习中，词频是影响模型性能的关键因素。"}}
{"id": "2509.05208", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05208", "abs": "https://arxiv.org/abs/2509.05208", "authors": ["Yamei Chen", "Haoquan Zhang", "Yangyi Huang", "Zeju Qiu", "Kaipeng Zhang", "Yandong Wen", "Weiyang Liu"], "title": "Symbolic Graphics Programming with Large Language Models", "comment": "Technical report (32 pages, 12 figures, project page:\n  https://spherelab.ai/SGP-Gen/)", "summary": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.", "AI": {"tldr": "本文探讨了大型语言模型（LLMs）生成符号图形程序（SGPs，特别是SVG）的能力。研究引入了SGP-GenBench基准测试，发现专有模型表现优于开源模型。为弥补差距，提出了一种带有可验证奖励的强化学习（RL）方法，显著提升了LLMs生成SVG的质量和语义，使其达到前沿水平，并揭示了RL在对象分解和场景连贯性方面的作用。", "motivation": "尽管大型语言模型在程序合成方面表现出色，但它们生成能够精确渲染视觉内容的符号图形程序（SGPs）的能力尚未得到充分探索。通过这项任务，可以深入了解LLMs如何理解视觉世界。初步研究发现，前沿专有模型与开源模型之间存在显著性能差距，且性能与通用编码能力相关。", "method": "1. 引入SGP-GenBench：一个全面的基准测试，用于评估LLMs生成SVG的能力，涵盖对象保真度、场景保真度和组合性（属性绑定、空间关系、数字能力）。2. 提出一种带有可验证奖励的强化学习（RL）方法：该方法包含一个格式有效性门以确保可渲染的SVG，以及一个跨模态奖励机制，通过强大的视觉编码器（如SigLIP用于文本-图像对齐，DINO用于图像-图像对齐）来对齐文本和渲染图像。3. 将此方法应用于Qwen-2.5-7B模型。", "result": "1. 在SGP-GenBench上，前沿专有模型显著优于开源模型，且性能与通用编码能力高度相关。2. 提出的RL方法显著提升了Qwen-2.5-7B模型的SVG生成质量和语义，使其性能达到前沿系统的水平。3. 训练动态分析表明，RL诱导了对象更精细的分解为可控基元，以及改善场景连贯性的上下文细节。", "conclusion": "符号图形编程提供了一个精确且可解释的视角，用于研究大型语言模型的跨模态基础能力。通过强化学习，可以有效提升LLMs生成精确视觉内容的能力，甚至使较小的模型达到前沿性能。"}}
{"id": "2509.05291", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05291", "abs": "https://arxiv.org/abs/2509.05291", "authors": ["Deniz Bayazit", "Aaron Mueller", "Antoine Bosselut"], "title": "Crosscoding Through Time: Tracking Emergence & Consolidation Of Linguistic Representations Throughout LLM Pretraining", "comment": null, "summary": "Large language models (LLMs) learn non-trivial abstractions during\npretraining, like detecting irregular plural noun subjects. However, it is not\nwell understood when and how specific linguistic abilities emerge as\ntraditional evaluation methods such as benchmarking fail to reveal how models\nacquire concepts and capabilities. To bridge this gap and better understand\nmodel training at the concept level, we use sparse crosscoders to discover and\nalign features across model checkpoints. Using this approach, we track the\nevolution of linguistic features during pretraining. We train crosscoders\nbetween open-sourced checkpoint triplets with significant performance and\nrepresentation shifts, and introduce a novel metric, Relative Indirect Effects\n(RelIE), to trace training stages at which individual features become causally\nimportant for task performance. We show that crosscoders can detect feature\nemergence, maintenance, and discontinuation during pretraining. Our approach is\narchitecture-agnostic and scalable, offering a promising path toward more\ninterpretable and fine-grained analysis of representation learning throughout\npretraining.", "AI": {"tldr": "本文提出使用稀疏跨编码器和相对间接效应（RelIE）指标，在预训练过程中跟踪大型语言模型（LLMs）中语言特征的出现、维持和消失，以更深入地理解概念层面能力的习得。", "motivation": "传统的评估方法无法揭示LLMs何时以及如何获取语言概念和能力，因此需要弥合这一差距，从概念层面更好地理解模型训练过程。", "method": "研究采用稀疏跨编码器在模型检查点之间发现并对齐特征，以追踪预训练期间语言特征的演变。通过在具有显著性能和表示变化的开源检查点三元组之间训练跨编码器，并引入一种新颖的指标——相对间接效应（RelIE），来追踪单个特征何时对任务性能变得因果重要。", "result": "研究表明，跨编码器能够检测预训练期间特征的出现、维持和消失。该方法与模型架构无关且具有可扩展性。", "conclusion": "该方法为在预训练过程中对表示学习进行更具解释性和细粒度的分析提供了一条有前景的途径。"}}
{"id": "2509.04821", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04821", "abs": "https://arxiv.org/abs/2509.04821", "authors": ["Yan Xie", "Yibo Cui", "Liang Xie", "Erwei Yin"], "title": "AFD-SLU: Adaptive Feature Distillation for Spoken Language Understanding", "comment": "5 pages, 1 figures", "summary": "Spoken Language Understanding (SLU) is a core component of conversational\nsystems, enabling machines to interpret user utterances. Despite its\nimportance, developing effective SLU systems remains challenging due to the\nscarcity of labeled training data and the computational burden of deploying\nLarge Language Models (LLMs) in real-world applications. To further alleviate\nthese issues, we propose an Adaptive Feature Distillation framework that\ntransfers rich semantic representations from a General Text Embeddings\n(GTE)-based teacher model to a lightweight student model. Our method introduces\na dynamic adapter equipped with a Residual Projection Neural Network (RPNN) to\nalign heterogeneous feature spaces, and a Dynamic Distillation Coefficient\n(DDC) that adaptively modulates the distillation strength based on real-time\nfeedback from intent and slot prediction performance. Experiments on the\nChinese profile-based ProSLU benchmark demonstrate that AFD-SLU achieves\nstate-of-the-art results, with 95.67% intent accuracy, 92.02% slot F1 score,\nand 85.50% overall accuracy.", "AI": {"tldr": "本文提出了一种自适应特征蒸馏框架（AFD-SLU），通过将基于通用文本嵌入（GTE）的教师模型的丰富语义表示转移到轻量级学生模型，以解决口语理解（SLU）中数据稀缺和大型语言模型（LLM）部署计算负担的问题，并在中文ProSLU基准上达到了最先进的性能。", "motivation": "开发有效的口语理解（SLU）系统面临两大挑战：标记训练数据稀缺和大型语言模型（LLM）在实际应用中部署的计算负担。", "method": "本文提出了一个自适应特征蒸馏（Adaptive Feature Distillation）框架。该方法引入了一个配备残差投影神经网络（RPNN）的动态适配器来对齐异构特征空间，并使用一个动态蒸馏系数（DDC），根据意图和槽位预测性能的实时反馈自适应地调整蒸馏强度。", "result": "在中文基于配置文件的ProSLU基准测试中，AFD-SLU框架取得了最先进的结果，包括95.67%的意图准确率、92.02%的槽位F1分数和85.50%的整体准确率。", "conclusion": "该框架通过自适应特征蒸馏，成功将大型教师模型的语义知识有效迁移至轻量级学生模型，显著提升了SLU系统的性能，同时缓解了数据和计算资源限制，实现了口语理解任务的最新技术水平。"}}
{"id": "2509.05296", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05296", "abs": "https://arxiv.org/abs/2509.05296", "authors": ["Zizun Li", "Jianjun Zhou", "Yifan Wang", "Haoyu Guo", "Wenzheng Chang", "Yang Zhou", "Haoyi Zhu", "Junyi Chen", "Chunhua Shen", "Tong He"], "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool", "comment": null, "summary": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.", "AI": {"tldr": "WinT3R是一个前馈重建模型，能够在线预测精确的相机姿态和高质量点云图，通过滑动窗口机制和紧凑相机表示解决了传统方法在重建质量和实时性能之间的权衡，实现了最先进的性能。", "motivation": "以往的方法在重建质量和实时性能之间存在权衡，难以同时达到高水准。WinT3R旨在解决这一问题，实现在线高精度重建和实时性能。", "method": "该研究引入了滑动窗口机制，确保窗口内帧之间充分的信息交换，以提高几何预测质量而无需大量计算。此外，它利用紧凑的相机表示并维护一个全局相机令牌池，以提高相机姿态估计的可靠性而不牺牲效率。", "result": "WinT3R在在线重建质量、相机姿态估计和重建速度方面均达到了最先进的性能，并通过对各种数据集的广泛实验得到了验证。", "conclusion": "WinT3R通过其独特的设计，成功地在保持实时性能的同时，显著提升了在线相机姿态估计和点云图重建的质量和速度，解决了现有方法的局限性。"}}
{"id": "2509.04866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04866", "abs": "https://arxiv.org/abs/2509.04866", "authors": ["Boxiang Ma", "Ru Li", "Yuanlong Wang", "Hongye Tan", "Xiaoli Li"], "title": "Memorization $\\neq$ Understanding: Do Large Language Models Have the Ability of Scenario Cognition?", "comment": "EMNLP 2025 Main Conference", "summary": "Driven by vast and diverse textual data, large language models (LLMs) have\ndemonstrated impressive performance across numerous natural language processing\n(NLP) tasks. Yet, a critical question persists: does their generalization arise\nfrom mere memorization of training data or from deep semantic understanding? To\ninvestigate this, we propose a bi-perspective evaluation framework to assess\nLLMs' scenario cognition - the ability to link semantic scenario elements with\ntheir arguments in context. Specifically, we introduce a novel scenario-based\ndataset comprising diverse textual descriptions of fictional facts, annotated\nwith scenario elements. LLMs are evaluated through their capacity to answer\nscenario-related questions (model output perspective) and via probing their\ninternal representations for encoded scenario elements-argument associations\n(internal representation perspective). Our experiments reveal that current LLMs\npredominantly rely on superficial memorization, failing to achieve robust\nsemantic scenario cognition, even in simple cases. These findings expose\ncritical limitations in LLMs' semantic understanding and offer cognitive\ninsights for advancing their capabilities.", "AI": {"tldr": "本研究评估了大型语言模型（LLM）的场景认知能力，发现LLM主要依赖表面记忆，而非深层语义理解，揭示了其语义理解的关键局限。", "motivation": "探究LLM的泛化能力是源于对训练数据的简单记忆，还是深层语义理解。", "method": "提出了一个双视角评估框架来评估LLM的场景认知能力。构建了一个包含虚构事实文本描述及场景元素标注的场景数据集。通过LLM回答场景相关问题的能力（模型输出视角）和探测其内部表示中编码的场景元素-论元关联（内部表示视角）进行评估。", "result": "实验表明，当前的LLM主要依赖表面记忆，即使在简单情况下也未能实现稳健的语义场景认知。", "conclusion": "这些发现揭示了LLM在语义理解方面的关键局限性，并为提升其能力提供了认知见解。"}}
{"id": "2509.05297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05297", "abs": "https://arxiv.org/abs/2509.05297", "authors": ["Matteo Poggi", "Fabio Tosi"], "title": "FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases", "comment": "ICCV 2025 - Project Page: https://flowseek25.github.io/ - Code:\n  https://github.com/mattpoggi/flowseek", "summary": "We present FlowSeek, a novel framework for optical flow requiring minimal\nhardware resources for training. FlowSeek marries the latest advances on the\ndesign space of optical flow networks with cutting-edge single-image depth\nfoundation models and classical low-dimensional motion parametrization,\nimplementing a compact, yet accurate architecture. FlowSeek is trained on a\nsingle consumer-grade GPU, a hardware budget about 8x lower compared to most\nrecent methods, and still achieves superior cross-dataset generalization on\nSintel Final and KITTI, with a relative improvement of 10 and 15% over the\nprevious state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow\ndatasets.", "AI": {"tldr": "FlowSeek是一个新颖的光流框架，训练所需硬件资源极少（单块消费级GPU），但仍能在多个数据集上实现优于现有技术的泛化性能。", "motivation": "大多数最新光流方法在训练时需要大量硬件资源。本研究旨在开发一种紧凑、准确且对硬件要求极低的光流架构。", "method": "FlowSeek结合了光流网络设计空间的最新进展、前沿的单图像深度基础模型以及经典的低维运动参数化方法，构建了一个紧凑而精确的架构。", "result": "FlowSeek在单个消费级GPU上训练（硬件预算比大多数最新方法低约8倍），但在Sintel Final和KITTI数据集上实现了卓越的跨数据集泛化，比之前的最先进技术SEA-RAFT相对提升了10%和15%，并在Spring和LayeredFlow数据集上也表现出色。", "conclusion": "FlowSeek是一个高效、准确且资源友好的光流框架，它在显著降低训练硬件成本的同时，实现了超越现有技术的泛化性能。"}}
{"id": "2509.04868", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04868", "abs": "https://arxiv.org/abs/2509.04868", "authors": ["Sylvia Vassileva", "Ivan Koychev", "Svetla Boytcheva"], "title": "Using LLMs for Multilingual Clinical Entity Linking to ICD-10", "comment": "7 pages, 2 Figures, to be published in Proceedings of the 15th\n  International Conference on Recent Advances in Natural Language Processing,\n  RANLP 2025", "summary": "The linking of clinical entities is a crucial part of extracting structured\ninformation from clinical texts. It is the process of assigning a code from a\nmedical ontology or classification to a phrase in the text. The International\nClassification of Diseases - 10th revision (ICD-10) is an international\nstandard for classifying diseases for statistical and insurance purposes.\nAutomatically assigning the correct ICD-10 code to terms in discharge summaries\nwill simplify the work of healthcare professionals and ensure consistent coding\nin hospitals. Our paper proposes an approach for linking clinical terms to\nICD-10 codes in different languages using Large Language Models (LLMs). The\napproach consists of a multistage pipeline that uses clinical dictionaries to\nmatch unambiguous terms in the text and then applies in-context learning with\nGPT-4.1 to predict the ICD-10 code for the terms that do not match the\ndictionary. Our system shows promising results in predicting ICD-10 codes on\ndifferent benchmark datasets in Spanish - 0.89 F1 for categories and 0.78 F1 on\nsubcategories on CodiEsp, and Greek - 0.85 F1 on ElCardioCC.", "AI": {"tldr": "本文提出了一种利用大型语言模型（LLMs），特别是GPT-4.1，通过多阶段流程将临床术语链接到多语言ICD-10代码的方法，旨在自动化疾病分类。", "motivation": "将临床实体链接到ICD-10代码对于从临床文本中提取结构化信息至关重要。自动分配正确的ICD-10代码可以简化医护人员的工作，并确保医院编码的一致性。", "method": "该方法采用多阶段流水线：首先使用临床词典匹配文本中明确的术语；然后，对于未匹配词典的术语，利用GPT-4.1进行情境学习（in-context learning）来预测ICD-10代码。该方法支持不同语言。", "result": "该系统在不同基准数据集上显示出有希望的结果：在西班牙语CodiEsp数据集上，类别F1分数为0.89，子类别F1分数为0.78；在希腊语ElCardioCC数据集上，F1分数为0.85。", "conclusion": "所提出的基于LLM的方法能够有效且有前景地将多语言临床术语链接到ICD-10代码，有助于自动化疾病分类工作。"}}
{"id": "2509.05146", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05146", "abs": "https://arxiv.org/abs/2509.05146", "authors": ["Yanzhi Tian", "Zeming Liu", "Zhengyang Liu", "Chong Feng", "Xin Li", "Heyan Huang", "Yuhang Guo"], "title": "PRIM: Towards Practical In-Image Multilingual Machine Translation", "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "In-Image Machine Translation (IIMT) aims to translate images containing texts\nfrom one language to another. Current research of end-to-end IIMT mainly\nconducts on synthetic data, with simple background, single font, fixed text\nposition, and bilingual translation, which can not fully reflect real world,\ncausing a significant gap between the research and practical conditions. To\nfacilitate research of IIMT in real-world scenarios, we explore Practical\nIn-Image Multilingual Machine Translation (IIMMT). In order to convince the\nlack of publicly available data, we annotate the PRIM dataset, which contains\nreal-world captured one-line text images with complex background, various\nfonts, diverse text positions, and supports multilingual translation\ndirections. We propose an end-to-end model VisTrans to handle the challenge of\npractical conditions in PRIM, which processes visual text and background\ninformation in the image separately, ensuring the capability of multilingual\ntranslation while improving the visual quality. Experimental results indicate\nthe VisTrans achieves a better translation quality and visual effect compared\nto other models. The code and dataset are available at:\nhttps://github.com/BITHLP/PRIM.", "AI": {"tldr": "该研究关注实际图像内多语言机器翻译（IIMMT），解决了现有研究主要基于合成数据的局限性。为此，作者构建了真实世界图像数据集PRIM，并提出了端到端模型VisTrans，在翻译质量和视觉效果上均优于其他模型。", "motivation": "当前的图像内机器翻译（IIMT）研究主要依赖于具有简单背景、单一字体、固定文本位置和双语翻译的合成数据，这与真实世界的复杂场景存在显著差距，导致研究成果难以应用于实际条件。为了推动IIMT在真实世界场景中的研究，需要解决数据不足和模型适应性问题。", "method": "为了弥补公开数据不足，作者标注了PRIM数据集，该数据集包含真实世界捕获的单行文本图像，具有复杂背景、多样字体、不同文本位置，并支持多语言翻译方向。作者提出了一种名为VisTrans的端到端模型，用于处理PRIM数据集中的实际挑战。VisTrans模型将图像中的视觉文本和背景信息分开处理，旨在确保多语言翻译能力的同时，提高视觉质量。", "result": "实验结果表明，与现有其他模型相比，VisTrans在翻译质量和视觉效果方面均取得了更好的表现。", "conclusion": "该研究通过构建真实世界的PRIM数据集和提出VisTrans模型，有效推动了实际图像内多语言机器翻译（IIMMT）的研究，并证明了其方法在复杂真实场景下的有效性和优越性。"}}
{"id": "2509.04884", "categories": ["cs.CL", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.04884", "abs": "https://arxiv.org/abs/2509.04884", "authors": ["Raul Singh", "Nicolo Brunello", "Vincenzo Scotti", "Mark James Carman"], "title": "L1RA: Dynamic Rank Assignment in LoRA Fine-Tuning", "comment": "Work published at ICNLSP 2025, waiting for publication link", "summary": "The ability of Large Language Models (LLMs) to solve complex tasks has made\nthem crucial in the development of AI-based applications. However, the high\ncomputational requirements to fine-tune these LLMs on downstream tasks pose\nsignificant challenges, particularly when resources are limited. In response to\nthis challenge, we introduce L1RA, a novel technique aimed at dynamically\ndistributing the rank of low-rank adapters during fine-tuning using LoRA. Given\na rank budget (i.e., total sum of adapters rank), L1RA leverages L1\nregularisation to prune redundant ranks and redistribute them across adapters,\nthereby optimising resource utilisation. Through a series of comprehensive\nexperiments, we empirically demonstrate that L1RA maintains comparable or even\nreduced computational overhead compared to other LoRA variants, including the\nvanilla approach, while achieving same or better performances. Moreover, the\npost-training analysis of rank distribution unveiled insights into the specific\nmodel components requiring the most adaptation to align with the task\nobjective: the feed-forward layers and the attention output projection. These\nresults highlight the efficacy of L1RA in not only enhancing the efficiency of\nLLM fine-tuning, but also in providing valuable diagnostic information for\nmodel refinement and customisation. In conclusion, L1RA stands as a promising\ntechnique for advancing the performance and interpretability of LLM adaptation,\nparticularly in scenarios where computational resources are constrained.", "AI": {"tldr": "L1RA是一种新颖的技术，通过L1正则化动态分配LoRA微调中的低秩适配器秩，从而优化资源利用并提高LLM微调的效率和性能。", "motivation": "大型语言模型（LLMs）在复杂任务中表现出色，但其微调计算成本高昂，尤其是在资源受限的情况下，这构成了重大挑战。", "method": "L1RA在LoRA微调过程中引入L1正则化，根据给定的秩预算，动态修剪冗余秩并将其重新分配给不同的适配器，以优化资源利用。它通过实验与现有LoRA变体进行比较。", "result": "L1RA在保持与传统LoRA变体相当或更低的计算开销的同时，实现了相同或更好的性能。此外，训练后分析揭示了前馈层和注意力输出投影是模型中最需要适应以匹配任务目标的组件。", "conclusion": "L1RA是一种很有前景的技术，不仅能提高LLM微调的效率，还能提供有价值的诊断信息，有助于模型优化和定制，特别适用于计算资源受限的场景，从而提升LLM适应的性能和可解释性。"}}
{"id": "2509.04903", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.04903", "abs": "https://arxiv.org/abs/2509.04903", "authors": ["Jianghao Chen", "Wei Sun", "Qixiang Yin", "Lingxing Kong", "Zhixing Tan", "Jiajun Zhang"], "title": "ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning", "comment": "Under review, our code is available at https://github.com/ZNLP/ACE-RL", "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\nlong-context understanding, yet they face significant challenges in\nhigh-quality long-form generation. Existing studies primarily suffer from two\nlimitations: (1) A heavy reliance on scarce, high-quality long-form response\ndata for supervised fine-tuning (SFT) or for pairwise preference reward in\nreinforcement learning (RL). (2) Focus on coarse-grained quality optimization\ndimensions, such as relevance, coherence, and helpfulness, overlooking the\nfine-grained specifics inherent to diverse long-form generation scenarios. To\naddress this issue, we propose a framework using Adaptive Constraint-Enhanced\nreward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first\nautomatically deconstructs each instruction into a set of fine-grained,\nadaptive constraint criteria by identifying its underlying intents and demands.\nSubsequently, we design a reward mechanism that quantifies the quality of\nlong-form responses based on their satisfaction over corresponding constraints,\nconverting subjective quality evaluation into constraint verification. Finally,\nwe utilize reinforcement learning to guide models toward superior long-form\ngeneration capabilities. Experimental results demonstrate that our ACE-RL\nframework significantly outperforms existing SFT and RL baselines by 20.70% and\n7.32% on WritingBench, and our top-performing model even surpasses proprietary\nsystems like GPT-4o by 7.10%, providing a more effective training paradigm for\nLLMs to generate high-quality content across diverse long-form generation\nscenarios.", "AI": {"tldr": "本文提出了ACE-RL框架，通过自适应约束增强奖励机制，解决了大型语言模型在长文本生成中对稀缺数据和粗粒度质量优化的依赖问题，显著提升了长文本生成质量，并超越了现有基线和GPT-4o。", "motivation": "现有大型语言模型在高质量长文本生成方面面临挑战，主要原因有二：1) 过度依赖稀缺的高质量长文本响应数据进行监督微调（SFT）或强化学习（RL）中的偏好奖励；2) 关注于粗粒度的质量优化维度（如相关性、连贯性、有用性），而忽略了多样化长文本生成场景中固有的细粒度具体要求。", "method": "本文提出了一个名为ACE-RL（Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning）的框架。该框架首先通过识别指令的潜在意图和需求，自动将其解构为一系列细粒度、自适应的约束条件；其次，设计了一个奖励机制，通过量化长文本响应对相应约束的满足程度来评估其质量，将主观质量评估转化为约束验证；最后，利用强化学习来引导模型获得卓越的长文本生成能力。", "result": "实验结果表明，ACE-RL框架在WritingBench上显著优于现有的SFT和RL基线，分别提高了20.70%和7.32%。表现最佳的模型甚至超越了专有系统GPT-4o 7.10%。", "conclusion": "ACE-RL框架为大型语言模型在多样化长文本生成场景中生成高质量内容提供了一种更有效的训练范式。"}}
{"id": "2509.04969", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04969", "abs": "https://arxiv.org/abs/2509.04969", "authors": ["Midhun Shyam", "Jim Basilakis", "Kieran Luken", "Steven Thomas", "John Crozier", "Paul M. Middleton", "X. Rosalind Wang"], "title": "Classification of kinetic-related injury in hospital triage data using NLP", "comment": "Accepted as a short paper for publishing at ADMA 2025\n  (https://adma2025.github.io), with Supplementary Material available at\n  https://github.com/CRMDS/Kinetic-Injury-Triage", "summary": "Triage notes, created at the start of a patient's hospital visit, contain a\nwealth of information that can help medical staff and researchers understand\nEmergency Department patient epidemiology and the degree of time-dependent\nillness or injury. Unfortunately, applying modern Natural Language Processing\nand Machine Learning techniques to analyse triage data faces some challenges:\nFirstly, hospital data contains highly sensitive information that is subject to\nprivacy regulation thus need to be analysed on site; Secondly, most hospitals\nand medical facilities lack the necessary hardware to fine-tune a Large\nLanguage Model (LLM), much less training one from scratch; Lastly, to identify\nthe records of interest, expert inputs are needed to manually label the\ndatasets, which can be time-consuming and costly. We present in this paper a\npipeline that enables the classification of triage data using LLM and limited\ncompute resources. We first fine-tuned a pre-trained LLM with a classifier\nusing a small (2k) open sourced dataset on a GPU; and then further fine-tuned\nthe model with a hospital specific dataset of 1000 samples on a CPU. We\ndemonstrated that by carefully curating the datasets and leveraging existing\nmodels and open sourced data, we can successfully classify triage data with\nlimited compute resources.", "AI": {"tldr": "本文提出了一种使用有限计算资源对急诊分诊笔记进行分类的流水线，通过两阶段微调预训练大型语言模型实现。", "motivation": "分析急诊分诊数据面临多重挑战：数据隐私限制必须现场分析；多数医院缺乏微调甚至训练大型语言模型所需的硬件；识别感兴趣的记录需要耗时且昂贵的人工标注。", "method": "该方法包括一个两阶段的微调流水线：首先，在一个GPU上使用一个小型（2k）开源数据集对预训练的大型语言模型及其分类器进行微调；然后，在一个CPU上使用1000个医院特定样本对模型进行进一步微调。关键在于精心策划数据集并利用现有模型和开源数据。", "result": "研究表明，通过精心策划数据集并利用现有模型和开源数据，可以在有限的计算资源下成功对分诊数据进行分类。", "conclusion": "通过仔细的数据管理和利用现有模型及开源数据，即使计算资源有限，也能有效地对急诊分诊数据进行分类。"}}
{"id": "2509.04982", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.04982", "abs": "https://arxiv.org/abs/2509.04982", "authors": ["Julius Neumann", "Robert Lange", "Yuni Susanti", "Michael Färber"], "title": "Optimizing Small Transformer-Based Language Models for Multi-Label Sentiment Analysis in Short Texts", "comment": "Accepted at LDD@ECAI 2025", "summary": "Sentiment classification in short text datasets faces significant challenges\nsuch as class imbalance, limited training samples, and the inherent\nsubjectivity of sentiment labels -- issues that are further intensified by the\nlimited context in short texts. These factors make it difficult to resolve\nambiguity and exacerbate data sparsity, hindering effective learning. In this\npaper, we evaluate the effectiveness of small Transformer-based models (i.e.,\nBERT and RoBERTa, with fewer than 1 billion parameters) for multi-label\nsentiment classification, with a particular focus on short-text settings.\nSpecifically, we evaluated three key factors influencing model performance: (1)\ncontinued domain-specific pre-training, (2) data augmentation using\nautomatically generated examples, specifically generative data augmentation,\nand (3) architectural variations of the classification head. Our experiment\nresults show that data augmentation improves classification performance, while\ncontinued pre-training on augmented datasets can introduce noise rather than\nboost accuracy. Furthermore, we confirm that modifications to the\nclassification head yield only marginal benefits. These findings provide\npractical guidance for optimizing BERT-based models in resource-constrained\nsettings and refining strategies for sentiment classification in short-text\ndatasets.", "AI": {"tldr": "本文评估了小型Transformer模型（BERT、RoBERTa）在短文本多标签情感分类中的表现，重点关注了领域特定预训练、数据增强和分类头架构，发现数据增强有效，而预训练可能引入噪声。", "motivation": "短文本情感分类面临诸多挑战，包括类别不平衡、训练样本有限、情感标签主观性以及上下文有限导致的歧义和数据稀疏性，这些因素阻碍了有效的学习。", "method": "研究评估了小于10亿参数的小型Transformer模型（BERT和RoBERTa）在短文本多标签情感分类中的有效性。具体考察了三个关键影响因素：1) 持续的领域特定预训练，2) 使用自动生成示例（生成式数据增强）进行数据增强，3) 分类头架构的变体。", "result": "实验结果表明，数据增强能提高分类性能；然而，在增强数据集上进行持续预训练反而可能引入噪声，而非提升准确性。此外，对分类头进行修改仅带来微小的收益。", "conclusion": "研究结果为在资源受限环境下优化基于BERT的模型以及改进短文本数据集的情感分类策略提供了实用指导。"}}
{"id": "2509.05006", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05006", "abs": "https://arxiv.org/abs/2509.05006", "authors": ["Inbal Bolshinsky", "Shani Kupiec", "Almog Sasson", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Do Large Language Models Need Intent? Revisiting Response Generation Strategies for Service Assistant", "comment": "7 pages, 1 figure", "summary": "In the era of conversational AI, generating accurate and contextually\nappropriate service responses remains a critical challenge. A central question\nremains: Is explicit intent recognition a prerequisite for generating\nhigh-quality service responses, or can models bypass this step and produce\neffective replies directly? This paper conducts a rigorous comparative study to\naddress this fundamental design dilemma. Leveraging two publicly available\nservice interaction datasets, we benchmark several state-of-the-art language\nmodels, including a fine-tuned T5 variant, across both paradigms: Intent-First\nResponse Generation and Direct Response Generation. Evaluation metrics\nencompass both linguistic quality and task success rates, revealing surprising\ninsights into the necessity or redundancy of explicit intent modelling. Our\nfindings challenge conventional assumptions in conversational AI pipelines,\noffering actionable guidelines for designing more efficient and effective\nresponse generation systems.", "AI": {"tldr": "本文比较了两种服务响应生成范式：先识别意图再生成响应（Intent-First）和直接生成响应（Direct Response Generation），以探究显式意图识别是否为高质量响应的必要条件。", "motivation": "在对话式AI中，生成准确且符合上下文的服务响应是一个关键挑战。核心问题是：显式意图识别是否是生成高质量服务响应的先决条件，或者模型能否跳过此步骤直接生成有效回复？", "method": "本文进行了一项严格的比较研究，利用两个公开的服务交互数据集，在“意图优先响应生成”和“直接响应生成”两种范式下，对包括微调T5变体在内的多个最先进语言模型进行了基准测试。评估指标涵盖语言质量和任务成功率。", "result": "评估结果揭示了关于显式意图建模必要性或冗余性的出人意料的见解，挑战了对话式AI管道中的传统假设。", "conclusion": "研究结果为设计更高效、更有效的响应生成系统提供了可操作的指导方针。"}}
{"id": "2509.05056", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05056", "abs": "https://arxiv.org/abs/2509.05056", "authors": ["Despoina Kosmopoulou", "Efthymios Georgiou", "Vaggelis Dorovatas", "Georgios Paraskevopoulos", "Alexandros Potamianos"], "title": "Masked Diffusion Language Models with Frequency-Informed Training", "comment": "Preprint", "summary": "We present a masked diffusion language modeling framework for data-efficient\ntraining for the BabyLM 2025 Challenge. Our approach applies diffusion training\nobjectives to language modeling under strict data constraints, incorporating\nfrequency-informed masking that prioritizes learning from rare tokens while\nmaintaining theoretical validity. We explore multiple noise scheduling\nstrategies, including two-mode approaches, and investigate different noise\nweighting schemes within the NELBO objective. We evaluate our method on the\nBabyLM benchmark suite, measuring linguistic competence, world knowledge, and\nhuman-likeness. Results show performance competitive to hybrid\nautoregressive-masked baselines, demonstrating that diffusion-based training\noffers a viable alternative for data-restricted language learning.", "AI": {"tldr": "本文提出了一种掩码扩散语言建模框架，用于在数据受限条件下进行高效训练，并在BabyLM 2025挑战赛中表现出与混合自回归-掩码基线相当的竞争力。", "motivation": "在严格的数据限制下（如BabyLM 2025挑战赛），实现数据高效的语言模型训练。", "method": "该方法应用扩散训练目标于语言建模，并结合了频率感知掩码（优先学习稀有词元）。研究探索了多种噪声调度策略（包括双模方法）以及NELBO目标内的不同噪声加权方案。", "result": "在BabyLM基准测试套件（衡量语言能力、世界知识和类人性）上的评估结果显示，该方法的性能与混合自回归-掩码基线相当。", "conclusion": "扩散式训练为数据受限的语言学习提供了一种可行的替代方案。"}}
{"id": "2509.05060", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05060", "abs": "https://arxiv.org/abs/2509.05060", "authors": ["Patrick Amadeus Irawan", "Ryandito Diandaru", "Belati Jagad Bintang Syuhada", "Randy Zakya Suchrady", "Alham Fikri Aji", "Genta Indra Winata", "Fajri Koto", "Samuel Cahyawijaya"], "title": "Entropy2Vec: Crosslingual Language Modeling Entropy as End-to-End Learnable Language Representations", "comment": null, "summary": "We introduce Entropy2Vec, a novel framework for deriving cross-lingual\nlanguage representations by leveraging the entropy of monolingual language\nmodels. Unlike traditional typological inventories that suffer from feature\nsparsity and static snapshots, Entropy2Vec uses the inherent uncertainty in\nlanguage models to capture typological relationships between languages. By\ntraining a language model on a single language, we hypothesize that the entropy\nof its predictions reflects its structural similarity to other languages: Low\nentropy indicates high similarity, while high entropy suggests greater\ndivergence. This approach yields dense, non-sparse language embeddings that are\nadaptable to different timeframes and free from missing values. Empirical\nevaluations demonstrate that Entropy2Vec embeddings align with established\ntypological categories and achieved competitive performance in downstream\nmultilingual NLP tasks, such as those addressed by the LinguAlchemy framework.", "AI": {"tldr": "Entropy2Vec是一个新颖的跨语言表征框架，它利用单语语言模型的熵来捕捉语言间的类型学关系，克服了传统方法稀疏性和静态性的问题。", "motivation": "传统的类型学清单存在特征稀疏性和静态快照的问题，无法充分捕捉语言间的类型学关系。", "method": "该方法通过训练单语语言模型，并利用其预测的熵来反映语言间的结构相似性（熵低表示相似度高，熵高表示差异大）。这产生了密集、非稀疏、可适应且无缺失值的语言嵌入。", "result": "经验评估表明，Entropy2Vec嵌入与既定的类型学类别保持一致，并在下游多语言NLP任务（例如LinguAlchemy框架）中取得了具有竞争力的性能。", "conclusion": "Entropy2Vec通过利用语言模型的内在不确定性，提供了一种有效且创新的方法来推导跨语言表征，成功捕捉了语言间的类型学关系，并克服了传统方法的局限性。"}}
{"id": "2509.05199", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05199", "abs": "https://arxiv.org/abs/2509.05199", "authors": ["David Herrera-Poyatos", "Carlos Peláez-González", "Cristina Zuheros", "Virilo Tejedor", "Rosana Montes", "Francisco Herrera"], "title": "Triadic Fusion of Cognitive, Functional, and Causal Dimensions for Explainable LLMs: The TAXAL Framework", "comment": "27 pages, 9 tables and 2 figures", "summary": "Large Language Models (LLMs) are increasingly being deployed in high-risk\ndomains where opacity, bias, and instability undermine trust and\naccountability. Traditional explainability methods, focused on surface outputs,\ndo not capture the reasoning pathways, planning logic, and systemic impacts of\nagentic LLMs.\n  We introduce TAXAL (Triadic Alignment for eXplainability in Agentic LLMs), a\ntriadic fusion framework that unites three complementary dimensions: cognitive\n(user understanding), functional (practical utility), and causal (faithful\nreasoning). TAXAL provides a unified, role-sensitive foundation for designing,\nevaluating, and deploying explanations in diverse sociotechnical settings.\n  Our analysis synthesizes existing methods, ranging from post-hoc attribution\nand dialogic interfaces to explanation-aware prompting, and situates them\nwithin the TAXAL triadic fusion model. We further demonstrate its applicability\nthrough case studies in law, education, healthcare, and public services,\nshowing how explanation strategies adapt to institutional constraints and\nstakeholder roles.\n  By combining conceptual clarity with design patterns and deployment pathways,\nTAXAL advances explainability as a technical and sociotechnical practice,\nsupporting trustworthy and context-sensitive LLM applications in the era of\nagentic AI.", "AI": {"tldr": "本文提出了TAXAL（代理型大型语言模型可解释性三元对齐）框架，通过融合认知、功能和因果三个维度，为代理型LLM在不同社会技术环境中的可解释性设计、评估和部署提供了统一且角色敏感的基础。", "motivation": "大型语言模型（LLMs）正被部署在高风险领域，但其不透明性、偏见和不稳定性损害了信任和问责制。传统的解释方法仅关注表面输出，无法捕捉代理型LLMs的推理路径、规划逻辑和系统性影响，因此需要更全面的可解释性解决方案。", "method": "本文引入了TAXAL框架，这是一个三元融合框架，结合了认知（用户理解）、功能（实际效用）和因果（忠实推理）三个互补维度。该方法通过综合现有方法（包括事后归因、对话界面和解释感知提示），并将其置于TAXAL模型中，并通过法律、教育、医疗和公共服务领域的案例研究来展示其适用性。", "result": "TAXAL框架为在多样化社会技术环境中设计、评估和部署解释提供了一个统一且角色敏感的基础。它成功地综合了现有解释方法，并通过案例研究展示了解释策略如何适应制度约束和利益相关者角色。", "conclusion": "通过结合概念清晰性、设计模式和部署路径，TAXAL将可解释性提升为一种技术和社会技术实践，从而在代理型AI时代支持可信赖和情境敏感的LLM应用。"}}
{"id": "2509.05209", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05209", "abs": "https://arxiv.org/abs/2509.05209", "authors": ["Mao Zheng", "Zheng Li", "Bingxin Qu", "Mingyang Song", "Yang Du", "Mingrui Sun", "Di Wang"], "title": "Hunyuan-MT Technical Report", "comment": null, "summary": "In this report, we introduce Hunyuan-MT-7B, our first open-source\nmultilingual translation model, which supports bidirectional translation across\n33 major languages and places a special emphasis on translation between\nMandarin and several ethnic minority languages as well as dialects.\nFurthermore, to serve and address diverse translation scenarios and enhance\nmodel performance at test time, we introduce Hunyuan-MT-Chimera-7B, a\ntranslation model inspired by the slow thinking mode. This model integrates\nmultiple outputs generated by the Hunyuan-MT-7B model under varying parameter\nsettings, thereby achieving performance superior to that of conventional\nslow-thinking models based on Chain-of-Thought (CoT). The development of our\nmodels follows a holistic training process specifically engineered for\nmultilingual translation, which begins with general and MT-oriented\npre-training to build foundational capabilities, proceeds to Supervised\nFine-Tuning (SFT) for task-specific adaptation, and culminates in advanced\nalignment through Reinforcement Learning (RL) and weak-to-strong RL. Through\ncomprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and\nHunyuan-MT-Chimera-7B significantly outperform all translation-specific models\nof comparable parameter size and most of the SOTA large models, particularly on\nthe task of translation between Mandarin and minority languages as well as\ndialects. In the WMT2025 shared task (General Machine Translation), our models\ndemonstrate state-of-the-art performance, ranking first in 30 out of 31\nlanguage pairs. This result highlights the robustness of our models across a\ndiverse linguistic spectrum, encompassing high-resource languages such as\nChinese, English, and Japanese, as well as low-resource languages including\nCzech, Marathi, Estonian, and Icelandic.", "AI": {"tldr": "本文介绍了两个开源多语言翻译模型：Hunyuan-MT-7B 和 Hunyuan-MT-Chimera-7B。Hunyuan-MT-7B 支持 33 种语言的双向翻译，并特别关注普通话与少数民族语言及方言的翻译。Hunyuan-MT-Chimera-7B 采用“慢思考”模式，通过整合 Hunyuan-MT-7B 的多种输出，实现了优于传统方法的性能。这两个模型在综合实验中表现出色，超越了同等参数规模的翻译模型和大多数 SOTA 大模型，并在 WMT2025 共享任务中 31 个语对中的 30 个排名第一。", "motivation": "开发高性能的开源多语言翻译模型，特别是解决普通话与少数民族语言及方言之间的翻译需求，并通过引入“慢思考”模式来服务和解决多样化的翻译场景并提升模型在测试时的性能。", "method": "开发了 Hunyuan-MT-7B，一个支持 33 种主要语言的双向翻译模型。引入了 Hunyuan-MT-Chimera-7B，该模型受慢思考模式启发，通过整合 Hunyuan-MT-7B 在不同参数设置下生成的多个输出。模型的开发遵循一个为多语言翻译量身定制的整体训练流程，包括通用和面向机器翻译的预训练、监督微调（SFT）以及通过强化学习（RL）和弱到强 RL 进行的高级对齐。", "result": "Hunyuan-MT-7B 和 Hunyuan-MT-Chimera-7B 显著优于所有同等参数规模的翻译专用模型和大多数 SOTA 大型模型，尤其是在普通话与少数民族语言及方言之间的翻译任务上。Hunyuan-MT-Chimera-7B 的性能优于基于 Chain-of-Thought (CoT) 的传统慢思考模型。在 WMT2025 共享任务（通用机器翻译）中，模型在 31 个语对中的 30 个语对中排名第一，展示了在包括高资源语言（如中文、英语、日语）和低资源语言（如捷克语、马拉地语、爱沙尼亚语、冰岛语）在内的多样化语言范围内的鲁棒性。", "conclusion": "Hunyuan-MT-7B 和 Hunyuan-MT-Chimera-7B 模型在多语言机器翻译领域取得了显著进展，提供了最先进的性能，特别是在低资源语言和普通话与少数民族语言及方言之间的翻译中表现出色，并证明了所提出的慢思考集成方法和整体训练过程的有效性。"}}
{"id": "2509.05215", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05215", "abs": "https://arxiv.org/abs/2509.05215", "authors": ["Medhasweta Sen", "Zachary Gottesman", "Jiaxing Qiu", "C. Bayan Bruss", "Nam Nguyen", "Tom Hartvigsen"], "title": "BEDTime: A Unified Benchmark for Automatically Describing Time Series", "comment": null, "summary": "Many recent studies have proposed general-purpose foundation models designed\nfor a variety of time series analysis tasks. While several established datasets\nalready exist for evaluating these models, previous works frequently introduce\ntheir models in conjunction with new datasets, limiting opportunities for\ndirect, independent comparisons and obscuring insights into the relative\nstrengths of different methods. Additionally, prior evaluations often cover\nnumerous tasks simultaneously, assessing a broad range of model abilities\nwithout clearly pinpointing which capabilities contribute to overall\nperformance. To address these gaps, we formalize and evaluate 3 tasks that test\na model's ability to describe time series using generic natural language: (1)\nrecognition (True/False question-answering), (2) differentiation (multiple\nchoice question-answering), and (3) generation (open-ended natural language\ndescription). We then unify 4 recent datasets to enable head-to-head model\ncomparisons on each task. Experimentally, in evaluating 13 state-of-the-art\nlanguage, vision--language, and time series--language models, we find that (1)\npopular language-only methods largely underperform, indicating a need for time\nseries-specific architectures, (2) VLMs are quite successful, as expected,\nidentifying the value of vision models for these tasks and (3) pretrained\nmultimodal time series--language models successfully outperform LLMs, but still\nhave significant room for improvement. We also find that all approaches exhibit\nclear fragility in a range of robustness tests. Overall, our benchmark provides\na standardized evaluation on a task necessary for time series reasoning\nsystems.", "AI": {"tldr": "本文提出了一个标准化基准，用于评估时间序列基础模型在自然语言描述任务上的能力，并统一了多个数据集以实现直接比较，发现现有模型（特别是纯语言模型）仍有显著提升空间且鲁棒性不足。", "motivation": "现有时间序列基础模型评估存在问题：新模型常伴随新数据集，导致难以直接比较和理解不同方法的优劣；评估任务过于宽泛，未能明确指出模型具体能力对整体性能的贡献。", "method": "研究者形式化并评估了3个测试模型使用通用自然语言描述时间序列能力的任务：识别（真/假问答）、区分（多项选择问答）和生成（开放式自然语言描述）。他们统一了4个近期数据集，以实现模型在每个任务上的直接比较，并评估了13个最先进的语言、视觉-语言和时间序列-语言模型。", "result": "实验发现：1) 流行的纯语言方法表现普遍不佳，表明需要特定于时间序列的架构；2) 视觉-语言模型（VLM）表现相当成功，证实了视觉模型对这些任务的价值；3) 预训练的多模态时间序列-语言模型优于大型语言模型（LLM），但仍有显著改进空间。此外，所有方法在鲁棒性测试中都表现出明显的脆弱性。", "conclusion": "该基准为时间序列推理系统所需的任务提供了标准化评估，揭示了当前模型（尤其是纯语言模型）的局限性，并强调了多模态方法（特别是结合视觉）的潜力，但所有模型在鲁棒性方面仍需大幅提升。"}}
{"id": "2509.05226", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05226", "abs": "https://arxiv.org/abs/2509.05226", "authors": ["Abdul Waheed", "Chancharik Mitra", "Laurie Z. Wang", "Deva Ramanan", "Bhiksha Raj"], "title": "Less is More Tokens: Efficient Math Reasoning via Difficulty-Aware Chain-of-Thought Distillation", "comment": "28 Pages", "summary": "Chain-of-thought reasoning, while powerful, can produce unnecessarily verbose\noutput for simpler problems. We present a framework for difficulty-aware\nreasoning that teaches models to dynamically adjust reasoning depth based on\nproblem complexity. Remarkably, we show that models can be endowed with such\ndynamic inference pathways without any architectural modifications; we simply\npost-train on data that is carefully curated to include chain-of-thought traces\nthat are proportional in length to problem difficulty. Our analysis reveals\nthat post-training via supervised fine-tuning (SFT) primarily captures patterns\nlike reasoning length and format, while direct preference optimization (DPO)\npreserves reasoning accuracy, with their combination reducing length and\nmaintaining or improving performance. Both quantitative metrics and qualitative\nassessments confirm that models can learn to \"think proportionally\", reasoning\nminimally on simple problems while maintaining depth for complex ones.", "AI": {"tldr": "本文提出了一种难度感知推理框架，通过精心策划的数据进行后训练，使模型能够根据问题复杂性动态调整思维链的推理深度，无需架构修改。", "motivation": "虽然思维链推理功能强大，但对于简单问题会产生不必要的冗长输出，因此需要一种方法使模型能够根据问题难度动态调整推理深度。", "method": "研究人员通过在精心策划的数据上进行后训练（包括与问题难度成比例的思维链痕迹），使模型具备动态推理路径。他们比较了监督微调（SFT）和直接偏好优化（DPO）及其组合的效果，发现无需任何架构修改。", "result": "分析表明，SFT主要捕捉推理长度和格式模式，DPO保持推理准确性，而两者的结合既能缩短长度又能保持或提高性能。定量和定性评估均证实，模型可以学会“按比例思考”，在简单问题上进行最少推理，同时在复杂问题上保持深度。", "conclusion": "模型能够学习难度感知推理，动态调整推理深度，在简单问题上进行简洁推理，在复杂问题上保持必要的深度，从而实现更高效和适应性的AI推理。"}}
{"id": "2509.05254", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05254", "abs": "https://arxiv.org/abs/2509.05254", "authors": ["Hailin Hao", "Elsi Kaiser"], "title": "Uniform Information Density and Syntactic Reduction: Revisiting $\\textit{that}$-Mentioning in English Complement Clauses", "comment": null, "summary": "Speakers often have multiple ways to express the same meaning. The Uniform\nInformation Density (UID) hypothesis suggests that speakers exploit this\nvariability to maintain a consistent rate of information transmission during\nlanguage production. Building on prior work linking UID to syntactic reduction,\nwe revisit the finding that the optional complementizer $\\textit{that}$in\nEnglish complement clauses is more likely to be omitted when the clause has low\ninformation density (i.e., more predictable). We advance this line of research\nby analyzing a large-scale, contemporary conversational corpus and using\nmachine learning and neural language models to refine estimates of information\ndensity. Our results replicated the established relationship between\ninformation density and $\\textit{that}$-mentioning. However, we found that\nprevious measures of information density based on matrix verbs'\nsubcategorization probability capture substantial idiosyncratic lexical\nvariation. By contrast, estimates derived from contextual word embeddings\naccount for additional variance in patterns of complementizer usage.", "AI": {"tldr": "研究发现，说话者在信息密度低时更倾向于省略补语连接词“that”，以保持信息传输的均匀性。使用上下文词嵌入能更准确地预测这种省略模式。", "motivation": "重新审视并验证信息密度（UID）假说与英语补语从句中“that”省略之间的关系，并利用先进的机器学习和神经语言模型改进信息密度估计，以克服以往测量方法的局限性。", "method": "分析一个大规模、现代的对话语料库。使用机器学习和神经语言模型（特别是上下文词嵌入）来精炼信息密度的估计。", "result": "复制了信息密度与“that”提及之间已建立的关系。发现先前基于主句动词次范畴化概率的信息密度测量捕获了显著的词汇特异性变异，而通过上下文词嵌入得出的估计则解释了补语使用模式中额外的变异。", "conclusion": "信息密度假说在“that”省略中得到支持。先进的自然语言处理技术（如上下文词嵌入）提供了更精确的信息密度测量方法，从而更好地理解补语连接词的使用模式。"}}
{"id": "2509.05282", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05282", "abs": "https://arxiv.org/abs/2509.05282", "authors": ["Zhen Qin", "Xuyang Shen", "Yiran Zhong"], "title": "Elucidating the Design Space of Decay in Linear Attention", "comment": "Accepted to COLM 2025. Yiran Zhong is the corresponding author. Code\n  is available at https://github.com/Doraemonzzz/xmixers", "summary": "This paper presents a comprehensive investigation into the decay mechanisms\ninherent in linear complexity sequence models. We systematically delineate the\ndesign space of decay mechanisms across four pivotal dimensions:\nparameterization strategy, which refers to the computational methodology for\ndecay; parameter sharing, which involves the utilization of supplementary\nparameters for decay computation; decay granularity, comparing scalar versus\nvector-based decay; and compatibility with relative positional encoding\nmethods, such as Rotary Position Embedding (RoPE). Through an extensive series\nof experiments conducted on diverse language modeling tasks, we uncovered\nseveral critical insights. Firstly, the design of the parameterization strategy\nfor decay requires meticulous consideration. Our findings indicate that\neffective configurations are typically confined to a specific range of\nparameters. Secondly, parameter sharing cannot be used arbitrarily, as it may\ncause decay values to be too large or too small, thereby significantly\nimpacting performance. Thirdly, under identical parameterization strategies,\nscalar decay generally underperforms compared to its vector-based counterpart.\nHowever, in certain scenarios with alternative parameterization strategies,\nscalar decay may unexpectedly surpass vector decay in efficacy. Lastly, our\nanalysis reveals that RoPE, a commonly employed relative positional encoding\nmethod, typically fails to provide tangible benefits to the majority of linear\nattention mechanisms.", "AI": {"tldr": "本文全面研究了线性复杂度序列模型中的衰减机制，系统地划分了其设计空间，并揭示了参数化策略、参数共享、衰减粒度以及与相对位置编码（如RoPE）兼容性方面的关键发现。", "motivation": "旨在深入理解和改进线性复杂度序列模型中固有的衰减机制，以优化其性能。", "method": "通过系统地将衰减机制的设计空间划分为四个关键维度（参数化策略、参数共享、衰减粒度、与相对位置编码的兼容性），并在各种语言建模任务上进行广泛实验。", "result": "1. 衰减的参数化策略需精细考虑，有效配置通常局限于特定参数范围。2. 参数共享不能随意使用，可能导致衰减值过大或过小，严重影响性能。3. 在相同参数化策略下，标量衰减通常不如向量衰减，但在特定替代策略下，标量衰减可能意外超越向量衰减。4. RoPE（旋转位置嵌入）通常未能为大多数线性注意力机制带来显著益处。", "conclusion": "线性复杂度序列模型中衰减机制的设计需要跨多个维度（参数化、共享、粒度）进行仔细考量。RoPE通常对线性注意力机制无明显增益。这些发现为设计更有效的线性复杂度模型提供了关键见解。"}}
