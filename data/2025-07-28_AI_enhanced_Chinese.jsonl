{"id": "2507.18738", "categories": ["eess.SY", "cs.GT", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18738", "abs": "https://arxiv.org/abs/2507.18738", "authors": ["Abhijan Theja", "Mayukha Pal"], "title": "An Explainable Equity-Aware P2P Energy Trading Framework for Socio-Economically Diverse Microgrid", "comment": null, "summary": "Fair and dynamic energy allocation in community microgrids remains a critical\nchallenge, particularly when serving socio-economically diverse participants.\nStatic optimization and cost-sharing methods often fail to adapt to evolving\ninequities, leading to participant dissatisfaction and unsustainable\ncooperation. This paper proposes a novel framework that integrates\nmulti-objective mixed-integer linear programming (MILP), cooperative game\ntheory, and a dynamic equity-adjustment mechanism driven by reinforcement\nlearning (RL). At its core, the framework utilizes a bi-level optimization\nmodel grounded in Equity-regarding Welfare Maximization (EqWM) principles,\nwhich incorporate Rawlsian fairness to prioritize the welfare of the least\nadvantaged participants. We introduce a Proximal Policy Optimization (PPO)\nagent that dynamically adjusts socio-economic weights in the optimization\nobjective based on observed inequities in cost and renewable energy access.\nThis RL-powered feedback loop enables the system to learn and adapt,\ncontinuously striving for a more equitable state. To ensure transparency,\nExplainable AI (XAI) is used to interpret the benefit allocations derived from\na weighted Shapley value. Validated across six realistic scenarios, the\nframework demonstrates peak demand reductions of up to 72.6%, and significant\ncooperative gains. The adaptive RL mechanism further reduces the Gini\ncoefficient over time, showcasing a pathway to truly sustainable and fair\nenergy communities.", "AI": {"tldr": "该论文提出一个新颖的框架，结合多目标混合整数线性规划、合作博弈论和基于强化学习的动态公平调整机制，旨在社区微电网中实现公平且动态的能源分配，尤其针对社会经济背景多元的参与者。", "motivation": "传统的静态优化和成本分摊方法难以适应不断变化的能源分配不公平问题，导致参与者不满和合作不可持续，尤其在服务社会经济背景多元的社区微电网时，公平和动态的能源分配仍是一个关键挑战。", "method": "该框架核心是一个基于公平福利最大化（EqWM）原则的双层优化模型，融入罗尔斯公平理论优先考虑弱势参与者。引入一个近端策略优化（PPO）智能体，通过强化学习驱动的反馈循环，根据观察到的成本和可再生能源获取不公平性动态调整优化目标中的社会经济权重。此外，利用可解释AI（XAI）解释加权Shapley值导出的利益分配，确保透明度。", "result": "该框架在六个现实场景中得到验证，展示了高达72.6%的峰值需求削减，并取得了显著的合作收益。自适应的强化学习机制能够随时间推移降低基尼系数。", "conclusion": "该框架提供了一种通向真正可持续和公平能源社区的途径，通过动态适应不公平性并优化能源分配，有效解决了社区微电网中的公平与效率挑战。"}}
{"id": "2507.18850", "categories": ["eess.IV", "physics.med-ph", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.18850", "abs": "https://arxiv.org/abs/2507.18850", "authors": ["Nicholas Dwork", "Jeremy W. Gordon", "Shuyu Tang", "Peder E. Z. Larson"], "title": "Estimating Sensitivity Maps for X-Nuclei Magnetic Resonance Spectroscopic Imaging", "comment": null, "summary": "The purpose of this research is to estimate sensitivity maps when imaging\nX-nuclei that may not have a significant presence throughout the field of view.\nWe propose to estimate the coil's sensitivities by solving a least-squares\nproblem where each row corresponds to an individual estimate of the sensitivity\nfor a given voxel. Multiple estimates come from the multiple bins of the\nspectrum with spectroscopy, multiple times with dynamic imaging, or multiple\nfrequencies when utilizing spectral excitation. The method presented in this\nmanuscript, called the L2 optimal method, is compared to the commonly used\nRefPeak method which uses the spectral bin with the highest energy to estimate\nthe sensitivity maps. The L2 optimal method yields more accurate sensitivity\nmaps when imaging a numerical phantom and is shown to yield a higher\nsignal-to-noise ratio when imaging the brain, pancreas, and heart with\nhyperpolarized pyruvate as the contrast agent with hyperpolarized MRI. The L2\noptimal method is able to better estimate the sensitivity by extracting more\ninformation from the measurements.", "AI": {"tldr": "该研究提出了一种名为L2最优的新方法，用于在X核成像中估计线圈灵敏度图，特别是在信号不均匀时。与RefPeak方法相比，L2最优方法能生成更准确的灵敏度图，并提高超极化MRI的信噪比。", "motivation": "在X核成像中，当X核在视野中分布不显著时，准确估计灵敏度图是一个挑战。", "method": "提出了一种名为L2最优的方法，通过解决一个最小二乘问题来估计线圈灵敏度。该方法利用光谱学中的多个谱段、动态成像中的多个时间点或光谱激发中的多个频率来获取灵敏度估计值。此方法与常用的RefPeak方法（使用最高能量谱段估计灵敏度）进行了比较。", "result": "L2最优方法在数值模型成像时能产生更准确的灵敏度图；在使用超极化丙酮酸作为对比剂进行超极化MRI对大脑、胰腺和心脏成像时，该方法显示出更高的信噪比。", "conclusion": "L2最优方法通过从测量中提取更多信息，能够更好地估计灵敏度。"}}
{"id": "2507.18853", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18853", "abs": "https://arxiv.org/abs/2507.18853", "authors": ["Hassan Zahid Butt", "Xingpeng Li"], "title": "Approximating CCCV charging using SOC-dependent tapered charging power constraints in long-term microgrid planning", "comment": null, "summary": "Traditional long-term microgrid planning models assume constant power\ncharging for battery energy storage systems (BESS), overlooking efficiency\nlosses that occur toward the end of charge due to rising internal resistance.\nWhile this issue can be mitigated at the cell level using constant\ncurrent-constant voltage (CCCV) charging, it is impractical at the pack level\nin large-scale systems. However, battery management systems and inverter\ncontrols can emulate this effect by tapering charging power at high\nstate-of-charge (SOC) levels, trading off charging speed for improved\nefficiency and reduced thermal stress. Ignoring this behavior in planning\nmodels can lead to undersized batteries and potential reliability issues. This\npaper proposes a tractable and scalable approach to approximate CCCV behavior\nusing SOC-dependent tapered charging power (TCP) constraints. A MATLAB-based\nproof of concept demonstrates the energy delivery and efficiency benefits of\ntapering. The method is integrated into a long-term planning framework and\nevaluated under a synthetic load and solar profile. Results show tapering\nsignificantly affects BESS sizing, cost, and reliability under dynamic\noperating conditions that demand fast charging. These findings highlight\ntapering as a critical modeling factor for accurately capturing BESS\nperformance in long-term microgrid planning.", "AI": {"tldr": "该研究提出了一种在微电网长期规划中模拟电池储能系统（BESS）恒流恒压（CCCV）充电行为的方法，通过考虑高荷电状态（SOC）下的充电功率递减（TCP），以更准确地规划BESS尺寸、成本和可靠性。", "motivation": "传统的微电网规划模型假设电池恒功率充电，忽略了充电末端因内阻升高导致的效率损失。这种忽略可能导致电池尺寸规划不足和潜在的可靠性问题。尽管CCCV充电在电池单元层面可行，但在大型系统电池组层面不切实际，但电池管理系统和逆变器可以通过在较高SOC时降低充电功率来模拟这种效果，以提高效率并减少热应力。", "method": "本文提出了一种可处理且可扩展的方法，通过引入依赖于荷电状态（SOC）的递减充电功率（TCP）约束来近似CCCV充电行为。该方法在MATLAB中进行了概念验证，并集成到长期规划框架中，通过合成负载和太阳能剖面进行评估。", "result": "研究结果表明，在需要快速充电的动态运行条件下，考虑充电功率递减（TCP）显著影响BESS的尺寸、成本和可靠性。", "conclusion": "充电功率递减是长期微电网规划中准确捕捉BESS性能的关键建模因素。"}}
{"id": "2507.19138", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19138", "abs": "https://arxiv.org/abs/2507.19138", "authors": ["Weisong Zhao", "Jingkai Zhou", "Xiangyu Zhu", "Weihua Chen", "Xiao-Yu Zhang", "Zhen Lei", "Fan Wang"], "title": "RealisVSR: Detail-enhanced Diffusion for Real-World 4K Video Super-Resolution", "comment": null, "summary": "Video Super-Resolution (VSR) has achieved significant progress through\ndiffusion models, effectively addressing the over-smoothing issues inherent in\nGAN-based methods. Despite recent advances, three critical challenges persist\nin VSR community: 1) Inconsistent modeling of temporal dynamics in foundational\nmodels; 2) limited high-frequency detail recovery under complex real-world\ndegradations; and 3) insufficient evaluation of detail enhancement and 4K\nsuper-resolution, as current methods primarily rely on 720P datasets with\ninadequate details. To address these challenges, we propose RealisVSR, a\nhigh-frequency detail-enhanced video diffusion model with three core\ninnovations: 1) Consistency Preserved ControlNet (CPC) architecture integrated\nwith the Wan2.1 video diffusion to model the smooth and complex motions and\nsuppress artifacts; 2) High-Frequency Rectified Diffusion Loss (HR-Loss)\ncombining wavelet decomposition and HOG feature constraints for texture\nrestoration; 3) RealisVideo-4K, the first public 4K VSR benchmark containing\n1,000 high-definition video-text pairs. Leveraging the advanced spatio-temporal\nguidance of Wan2.1, our method requires only 5-25% of the training data volume\ncompared to existing approaches. Extensive experiments on VSR benchmarks (REDS,\nSPMCS, UDM10, YouTube-HQ, VideoLQ, RealisVideo-720P) demonstrate our\nsuperiority, particularly in ultra-high-resolution scenarios.", "AI": {"tldr": "本文提出RealistVSR，一个基于扩散模型的视频超分辨率（VSR）方法，通过引入一致性保留ControlNet、高频校正扩散损失和首个4K VSR基准数据集，解决了现有VSR在时间一致性、高频细节恢复和4K评估方面的挑战，并展现出卓越性能。", "motivation": "当前视频超分辨率（VSR）领域面临三大挑战：1）基础模型中时间动态建模不一致；2）复杂真实世界退化下高频细节恢复能力有限；3）现有方法主要依赖720P数据集，导致细节增强和4K超分辨率评估不足。", "method": "本文提出RealistVSR，一个高频细节增强的视频扩散模型，包含三项核心创新：1）一致性保留ControlNet（CPC）架构，与Wan2.1视频扩散模型集成，用于建模平滑复杂的运动并抑制伪影；2）高频校正扩散损失（HR-Loss），结合小波分解和HOG特征约束以恢复纹理；3）RealistVideo-4K，首个公开的4K VSR基准数据集，包含1,000对高清视频-文本对。该方法利用Wan2.1的先进时空引导，仅需现有方法5-25%的训练数据量。", "result": "在REDS、SPMCS、UDM10、YouTube-HQ、VideoLQ、RealistVideo-720P等VSR基准上进行的广泛实验表明，RealistVSR表现出优越性，尤其在超高分辨率场景下。", "conclusion": "RealistVSR通过改进时间一致性、增强高频细节恢复并提供4K评估基准，有效解决了当前VSR面临的关键挑战，并在多种场景下实现了卓越的超分辨率性能，且对训练数据量要求较低。"}}
{"id": "2507.18888", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18888", "abs": "https://arxiv.org/abs/2507.18888", "authors": ["Xinming Wang", "Zongyi Guo", "Jianguo Guo", "Jun Yang", "Yunda Yan"], "title": "Enhancing Robustness of Control Barrier Function: A Reciprocal Resistance-based Approach", "comment": "7 pages, 5 figures, No presented at any conference", "summary": "In this note, a new reciprocal resistance-based control barrier function\n(RRCBF) is developed to enhance the robustness of control barrier functions for\ndisturbed affine nonlinear systems, without requiring explicit knowledge of\ndisturbance bounds. By integrating a reciprocal resistance-like term into the\nconventional zeroing barrier function framework, we formally establish the\nconcept of the reciprocal resistance-based barrier function (RRBF), rigorously\nproving the forward invariance of its associated safe set and its robustness\nagainst bounded disturbances. The RRBF inherently generates a buffer zone near\nthe boundary of the safe set, effectively dominating the influence of\nuncertainties and external disturbances. This foundational concept is extended\nto formulate RRCBFs, including their high-order variants. To alleviate\nconservatism in the presence of complex, time-varying disturbances, we further\nintroduce a disturbance observer-based RRCBF (DO-RRCBF), which exploits\ndisturbance estimates to enhance safety guarantees and recover nominal control\nperformance. The effectiveness of the proposed framework is validated through\ntwo simulation studies: a second-order linear system illustrating forward\ninvariance in the phase plane, and an adaptive cruise control scenario\ndemonstrating robustness in systems with high relative degree.", "AI": {"tldr": "本文提出了一种新的基于倒数电阻的控制障碍函数（RRCBF），旨在提高受扰动仿射非线性系统的鲁棒性，无需明确的扰动界限知识。通过引入倒数电阻项，RRCBF能生成缓冲区以应对不确定性，并可结合扰动观测器（DO-RRCBF）进一步提升性能。", "motivation": "现有控制障碍函数在受扰动仿射非线性系统中的鲁棒性不足，尤其是在缺乏明确扰动界限信息的情况下，需要一种更稳健的解决方案。", "method": "1. 将倒数电阻状项整合到传统归零障碍函数框架中，提出倒数电阻障碍函数（RRBF），并严格证明其关联安全集的前向不变性和对有界扰动的鲁棒性。2. 将RRBF概念扩展至RRCBF，包括高阶变体。3. 引入基于扰动观测器的RRCBF（DO-RRCBF），利用扰动估计来缓解复杂时变扰动下的保守性，提升安全保障并恢复标称控制性能。", "result": "1. RRBF能够形式化建立，其关联安全集的前向不变性和对有界扰动的鲁棒性得到严格证明。2. RRBF在安全集边界附近固有地生成一个缓冲区，有效抑制不确定性和外部扰动的影响。3. DO-RRCBF增强了安全保证并恢复了标称控制性能。4. 通过两个仿真研究（二阶线性系统和自适应巡航控制场景）验证了所提框架的有效性。", "conclusion": "所提出的基于倒数电阻的控制障碍函数（RRCBF）及其结合扰动观测器的变体（DO-RRCBF），能够有效增强受扰动仿射非线性系统的鲁棒性，无需明确的扰动界限，并在实际应用中显示出潜力。"}}
{"id": "2507.19155", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.19155", "abs": "https://arxiv.org/abs/2507.19155", "authors": ["Michal K. Grzeszczyk", "Tomasz Szczepański", "Pawel Renc", "Siyeop Yoon", "Jerome Charton", "Tomasz Trzciński", "Arkadiusz Sitek"], "title": "RegScore: Scoring Systems for Regression Tasks", "comment": "Accepted for the 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2025", "summary": "Scoring systems are widely adopted in medical applications for their inherent\nsimplicity and transparency, particularly for classification tasks involving\ntabular data. In this work, we introduce RegScore, a novel, sparse, and\ninterpretable scoring system specifically designed for regression tasks. Unlike\nconventional scoring systems constrained to integer-valued coefficients,\nRegScore leverages beam search and k-sparse ridge regression to relax these\nrestrictions, thus enhancing predictive performance. We extend RegScore to\nbimodal deep learning by integrating tabular data with medical images. We\nutilize the classification token from the TIP (Tabular Image Pretraining)\ntransformer to generate Personalized Linear Regression parameters and a\nPersonalized RegScore, enabling individualized scoring. We demonstrate the\neffectiveness of RegScore by estimating mean Pulmonary Artery Pressure using\ntabular data and further refine these estimates by incorporating cardiac MRI\nimages. Experimental results show that RegScore and its personalized bimodal\nextensions achieve performance comparable to, or better than, state-of-the-art\nblack-box models. Our method provides a transparent and interpretable approach\nfor regression tasks in clinical settings, promoting more informed and\ntrustworthy decision-making. We provide our code at\nhttps://github.com/SanoScience/RegScore.", "AI": {"tldr": "提出RegScore，一个针对回归任务的稀疏、可解释评分系统，并通过结合表格数据和医学图像扩展到双模态深度学习，实现个性化评分。", "motivation": "现有评分系统主要用于分类任务且系数受限于整数，缺乏针对回归任务的简单、透明且高性能的系统，尤其是在需要结合多模态数据时。", "method": "引入RegScore，一个稀疏、可解释的回归评分系统。利用束搜索（beam search）和k-稀疏岭回归（k-sparse ridge regression）来放宽传统评分系统对整数系数的限制。将RegScore扩展到双模态深度学习，整合表格数据和医学图像，并利用TIP（Tabular Image Pretraining）transformer的分类token生成个性化线性回归参数和个性化RegScore，实现个体化评分。", "result": "RegScore成功应用于估计平均肺动脉压（PAP），先使用表格数据，再结合心脏MRI图像进行优化。实验结果显示，RegScore及其个性化双模态扩展的性能与现有最先进的黑盒模型相当或更优。", "conclusion": "RegScore为临床环境中的回归任务提供了一种透明且可解释的方法，有助于做出更明智和值得信赖的决策。"}}
{"id": "2507.18742", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18742", "abs": "https://arxiv.org/abs/2507.18742", "authors": ["Víctor Gallego"], "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking Through Test-Time Refinement", "comment": "Accepted to SCALR Workshop @ COLM 2025", "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .", "AI": {"tldr": "该论文提出了“规范自我修正（SSC）”框架，使语言模型能在推理时识别并修正其指导规范中的缺陷，从而显著减少模型利用漏洞进行奖励欺骗的行为，提高模型与用户真实意图的对齐。", "motivation": "语言模型容易受到上下文奖励欺骗的影响，即它们会利用不完善的规范或评分标准来获得高分，却未能真正满足用户的意图。", "method": "引入了“规范自我修正（SSC）”框架，这是一个在测试时运行的框架。其多步推理过程包括：模型首先根据潜在有缺陷的规范生成响应，然后批判性地评估其输出，接着修改规范本身以消除可利用的漏洞，最后使用这个自我修正后的规范生成一个更稳健的响应。", "result": "在创意写作和代理编码任务的实验中，最初模型在50-70%的情况下会利用有缺陷的规范进行欺骗，而SSC过程将这种漏洞减少了90%以上。这种动态修复在推理时发生，不需要修改模型权重。", "conclusion": "SSC框架能够有效提高语言模型的鲁棒对齐行为，使其在不修改模型权重的情况下，动态地在推理时修复规范缺陷，从而避免奖励欺骗并更好地满足用户意图。"}}
{"id": "2507.18938", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.18938", "abs": "https://arxiv.org/abs/2507.18938", "authors": ["Behzad Zamani", "James Kennedy", "Airlie Chapman", "Peter Dower", "Chris Manzie", "Simon Crase"], "title": "GMM-Based Time-Varying Coverage Control", "comment": "Submitted to CDC 2025", "summary": "In coverage control problems that involve time-varying density functions, the\ncoverage control law depends on spatial integrals of the time evolution of the\ndensity function. The latter is often neglected, replaced with an upper bound\nor calculated as a numerical approximation of the spatial integrals involved.\nIn this paper, we consider a special case of time-varying density functions\nmodeled as Gaussian Mixture Models (GMMs) that evolve with time via a set of\ntime-varying sources (with known corresponding velocities). By imposing this\nstructure, we obtain an efficient time-varying coverage controller that fully\nincorporates the time evolution of the density function. We show that the\ninduced trajectories under our control law minimise the overall coverage cost.\nWe elicit the structure of the proposed controller and compare it with a\nclassical time-varying coverage controller, against which we benchmark the\ncoverage performance in simulation. Furthermore, we highlight that the\ncomputationally efficient and distributed nature of the proposed control law\nmakes it ideal for multi-vehicle robotic applications involving time-varying\ncoverage control problems. We employ our method in plume monitoring using a\nswarm of drones. In an experimental field trial we show that drones guided by\nthe proposed controller are able to track a simulated time-varying chemical\nplume in a distributed manner.", "AI": {"tldr": "本文提出了一种针对时变高斯混合模型（GMM）密度函数的覆盖控制律，该控制律充分考虑了密度函数的时间演化，实现了高效、分布式且能最小化覆盖成本的控制，并适用于多无人机应用。", "motivation": "在涉及时变密度函数的覆盖控制问题中，现有的控制律往往忽略密度函数的时间演化、使用其上限或通过数值近似计算空间积分，导致效率低下或精度不足。", "method": "研究者将时变密度函数建模为由已知速度的时变源演化而来的高斯混合模型（GMMs）。基于此结构，他们推导了一个高效的时变覆盖控制器，该控制器完全融入了密度函数的时间演化。通过仿真与经典时变覆盖控制器进行性能对比，并在无人机群进行羽流监测的现场试验中验证了其有效性。", "result": "研究表明，所提出的控制律引导的轨迹能最小化整体覆盖成本。该控制器计算效率高且具有分布式特性，在仿真中展现出优越的覆盖性能。在实际试验中，由该控制器引导的无人机群能够以分布式方式成功跟踪模拟的时变化学羽流。", "conclusion": "所提出的基于GMM的时变覆盖控制器是一种高效、分布式且性能优越的解决方案，能够充分考虑密度函数的时间演化，特别适用于多无人机等涉及动态环境监测的实际应用场景。"}}
{"id": "2507.19165", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19165", "abs": "https://arxiv.org/abs/2507.19165", "authors": ["Kang Wang", "Chen Qin", "Zhang Shi", "Haoran Wang", "Xiwen Zhang", "Chen Chen", "Cheng Ouyang", "Chengliang Dai", "Yuanhan Mo", "Chenchen Dai", "Xutong Kuang", "Ruizhe Li", "Xin Chen", "Xiuzheng Yue", "Song Tian", "Alejandro Mora-Rubio", "Kumaradevan Punithakumar", "Shizhan Gong", "Qi Dou", "Sina Amirrajab", "Yasmina Al Khalil", "Cian M. Scannell", "Lexiaozi Fan", "Huili Yang", "Xiaowu Sun", "Rob van der Geest", "Tewodros Weldebirhan Arega", "Fabrice Meriaudeau", "Caner Özer", "Amin Ranem", "John Kalkhof", "İlkay Öksüz", "Anirban Mukhopadhyay", "Abdul Qayyum", "Moona Mazher", "Steven A Niederer", "Carles Garcia-Cabrera", "Eric Arazo", "Michal K. Grzeszczyk", "Szymon Płotka", "Wanqin Ma", "Xiaomeng Li", "Rongjun Ge", "Yongqing Kou", "Xinrong Chen", "He Wang", "Chengyan Wang", "Wenjia Bai", "Shuo Wang"], "title": "Extreme Cardiac MRI Analysis under Respiratory Motion: Results of the CMRxMotion Challenge", "comment": null, "summary": "Deep learning models have achieved state-of-the-art performance in automated\nCardiac Magnetic Resonance (CMR) analysis. However, the efficacy of these\nmodels is highly dependent on the availability of high-quality, artifact-free\nimages. In clinical practice, CMR acquisitions are frequently degraded by\nrespiratory motion, yet the robustness of deep learning models against such\nartifacts remains an underexplored problem. To promote research in this domain,\nwe organized the MICCAI CMRxMotion challenge. We curated and publicly released\na dataset of 320 CMR cine series from 40 healthy volunteers who performed\nspecific breathing protocols to induce a controlled spectrum of motion\nartifacts. The challenge comprised two tasks: 1) automated image quality\nassessment to classify images based on motion severity, and 2) robust\nmyocardial segmentation in the presence of motion artifacts. A total of 22\nalgorithms were submitted and evaluated on the two designated tasks. This paper\npresents a comprehensive overview of the challenge design and dataset, reports\nthe evaluation results for the top-performing methods, and further investigates\nthe impact of motion artifacts on five clinically relevant biomarkers. All\nresources and code are publicly available at: https://github.com/CMRxMotion", "AI": {"tldr": "该论文介绍了MICCAI CMRxMotion挑战赛，旨在解决深度学习模型在心血管磁共振（CMR）图像分析中对呼吸运动伪影的鲁棒性问题。挑战赛发布了一个包含受控运动伪影的数据集，并设置了图像质量评估和心肌分割两个任务，评估了顶级算法并分析了运动伪影对生物标志物的影响。", "motivation": "深度学习模型在自动化CMR分析中表现出色，但其有效性高度依赖于高质量、无伪影的图像。临床实践中，CMR图像常因呼吸运动而质量下降，但深度学习模型对抗此类伪影的鲁棒性研究不足。", "method": "组织了MICCAI CMRxMotion挑战赛，策划并发布了一个包含320个CMR电影序列的数据集，这些序列来自40名健康志愿者，通过特定呼吸协议诱导受控运动伪影。挑战赛包括两项任务：1) 基于运动严重程度的自动化图像质量评估；2) 在运动伪影存在下的鲁棒心肌分割。共评估了22种算法。", "result": "论文全面概述了挑战赛设计和数据集，报告了表现最佳方法的评估结果，并进一步研究了运动伪影对五种临床相关生物标志物的影响。所有资源和代码均已公开。", "conclusion": "CMRxMotion挑战赛成功推动了深度学习模型在CMR图像分析中对运动伪影鲁棒性的研究。通过提供新的数据集和评估框架，为该领域未来的发展奠定了基础，并揭示了运动伪影对临床生物标志物的影响。"}}
{"id": "2507.18762", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18762", "abs": "https://arxiv.org/abs/2507.18762", "authors": ["Abdulhady Abas Abdullah", "Amir H. Gandomi", "Tarik A Rashid", "Seyedali Mirjalili", "Laith Abualigah", "Milena Živković", "Hadi Veisi"], "title": "The Role of Orthographic Consistency in Multilingual Embedding Models for Text Classification in Arabic-Script Languages", "comment": null, "summary": "In natural language processing, multilingual models like mBERT and\nXLM-RoBERTa promise broad coverage but often struggle with languages that share\na script yet differ in orthographic norms and cultural context. This issue is\nespecially notable in Arabic-script languages such as Kurdish Sorani, Arabic,\nPersian, and Urdu. We introduce the Arabic Script RoBERTa (AS-RoBERTa) family:\nfour RoBERTa-based models, each pre-trained on a large corpus tailored to its\nspecific language. By focusing pre-training on language-specific script\nfeatures and statistics, our models capture patterns overlooked by\ngeneral-purpose models. When fine-tuned on classification tasks, AS-RoBERTa\nvariants outperform mBERT and XLM-RoBERTa by 2 to 5 percentage points. An\nablation study confirms that script-focused pre-training is central to these\ngains. Error analysis using confusion matrices shows how shared script traits\nand domain-specific content affect performance. Our results highlight the value\nof script-aware specialization for languages using the Arabic script and\nsupport further work on pre-training strategies rooted in script and language\nspecificity.", "AI": {"tldr": "本文介绍了AS-RoBERTa系列模型，通过针对阿拉伯语系语言（如库尔德语、阿拉伯语、波斯语、乌尔都语）进行脚本特异性预训练，显著优于mBERT和XLM-RoBERTa等多语言模型。", "motivation": "多语言模型（如mBERT和XLM-RoBERTa）在处理共享相同书写系统但正字法规范和文化背景不同的语言（特别是阿拉伯语系语言）时表现不佳，未能充分捕捉特定语言的脚本特征和统计模式。", "method": "引入了AS-RoBERTa系列模型，该系列包含四个基于RoBERTa的模型，每个模型都针对其特定语言（如库尔德语索拉尼、阿拉伯语、波斯语、乌尔都语）的大规模语料库进行预训练。预训练过程侧重于语言特定的脚本特征和统计数据。通过消融研究确认了脚本聚焦预训练的重要性，并使用混淆矩阵进行错误分析。", "result": "在分类任务中，AS-RoBERTa变体比mBERT和XLM-RoBERTa性能高出2到5个百分点。消融研究证实，以脚本为中心的预训练是性能提升的关键。错误分析揭示了共享脚本特征和领域特定内容如何影响性能。", "conclusion": "研究结果强调了对于使用阿拉伯语系书写系统的语言，脚本感知专业化的价值，并支持进一步开展基于脚本和语言特异性的预训练策略研究。"}}
{"id": "2507.18808", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18808", "abs": "https://arxiv.org/abs/2507.18808", "authors": ["Miguel Saavedra-Ruiz", "Samer B. Nashed", "Charlie Gauthier", "Liam Paull"], "title": "Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments", "comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025) Code available at\n  https://github.com/montrealrobotics/perpetua-code. Webpage and additional\n  videos at https://montrealrobotics.ca/perpetua/", "summary": "Many robotic systems require extended deployments in complex, dynamic\nenvironments. In such deployments, parts of the environment may change between\nsubsequent robot observations. Most robotic mapping or environment modeling\nalgorithms are incapable of representing dynamic features in a way that enables\npredicting their future state. Instead, they opt to filter certain state\nobservations, either by removing them or some form of weighted averaging. This\npaper introduces Perpetua, a method for modeling the dynamics of semi-static\nfeatures. Perpetua is able to: incorporate prior knowledge about the dynamics\nof the feature if it exists, track multiple hypotheses, and adapt over time to\nenable predicting of future feature states. Specifically, we chain together\nmixtures of \"persistence\" and \"emergence\" filters to model the probability that\nfeatures will disappear or reappear in a formal Bayesian framework. The\napproach is an efficient, scalable, general, and robust method for estimating\nthe states of features in an environment, both in the present as well as at\narbitrary future times. Through experiments on simulated and real-world data,\nwe find that Perpetua yields better accuracy than similar approaches while also\nbeing online adaptable and robust to missing observations.", "AI": {"tldr": "Perpetua是一种用于机器人系统建模半静态特征动态的方法，它能预测特征的未来状态，并优于现有方法。", "motivation": "许多机器人系统需要在复杂、动态的环境中长时间部署，但现有的大多数机器人建图或环境建模算法无法有效表示动态特征并预测其未来状态，通常选择过滤掉这些动态信息。", "method": "Perpetua通过将“持久性”（persistence）和“出现性”（emergence）滤波器混合链式结合，在贝叶斯框架下建模特征消失或重新出现的概率。该方法能够整合先验知识、跟踪多个假设，并随时间自适应，从而预测特征的未来状态。", "result": "通过模拟和真实世界数据实验，Perpetua比类似方法具有更高的准确性，同时具备在线适应性并对缺失观测数据具有鲁棒性。", "conclusion": "Perpetua是一种高效、可扩展、通用且鲁棒的方法，用于估计环境中特征的当前和未来任意时刻的状态。"}}
{"id": "2507.18645", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18645", "abs": "https://arxiv.org/abs/2507.18645", "authors": ["Milan Maksimovic", "Anna Bohdanets", "Immaculate Motsi-Omoijiade", "Guido Governatori", "Ivan S. Maksymov"], "title": "Quantum-Cognitive Tunnelling Neural Networks for Military-Civilian Vehicle Classification and Sentiment Analysis", "comment": null, "summary": "Prior work has demonstrated that incorporating well-known quantum tunnelling\n(QT) probability into neural network models effectively captures important\nnuances of human perception, particularly in the recognition of ambiguous\nobjects and sentiment analysis. In this paper, we employ novel QT-based neural\nnetworks and assess their effectiveness in distinguishing customised\nCIFAR-format images of military and civilian vehicles, as well as sentiment,\nusing a proprietary military-specific vocabulary. We suggest that QT-based\nmodels can enhance multimodal AI applications in battlefield scenarios,\nparticularly within human-operated drone warfare contexts, imbuing AI with\ncertain traits of human reasoning.", "AI": {"tldr": "该研究利用基于量子隧穿（QT）的新型神经网络，用于区分军事和民用车辆图像以及进行情感分析，旨在增强战场AI的人类推理能力。", "motivation": "先前的研究表明，将量子隧穿概率融入神经网络能有效捕捉人类感知的细微差别，特别是在识别模糊对象和情感分析方面。本研究旨在将此方法应用于军事领域，以提升战场AI的性能，赋予其人类推理的某些特质。", "method": "采用新型的基于量子隧穿的神经网络模型。通过定制的CIFAR格式军事和民用车辆图像以及使用专有军事词汇的情感分析任务，评估模型的有效性。", "result": "该研究评估了基于量子隧穿的模型在区分军事/民用车辆图像和进行情感分析方面的有效性。结果表明，此类模型有望增强战场多模态AI应用。", "conclusion": "基于量子隧穿的模型能够增强战场场景下的多模态AI应用，特别是在人类操作的无人机战情境中，通过赋予AI某些人类推理的特质。"}}
{"id": "2507.18775", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18775", "abs": "https://arxiv.org/abs/2507.18775", "authors": ["Ilche Georgievski", "Marco Aiello"], "title": "Initial Steps in Integrating Large Reasoning and Action Models for Service Composition", "comment": "16 pages, 3 figures, 19th Symposium and Summer School on\n  Service-Oriented Computing (SummerSOC)", "summary": "Service composition remains a central challenge in building adaptive and\nintelligent software systems, often constrained by limited reasoning\ncapabilities or brittle execution mechanisms. This paper explores the\nintegration of two emerging paradigms enabled by large language models: Large\nReasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs\naddress the challenges of semantic reasoning and ecosystem complexity while\nLAMs excel in dynamic action execution and system interoperability. However,\neach paradigm has complementary limitations - LRMs lack grounded action\ncapabilities, and LAMs often struggle with deep reasoning. We propose an\nintegrated LRM-LAM architectural framework as a promising direction for\nadvancing automated service composition. Such a system can reason about service\nrequirements and constraints while dynamically executing workflows, thus\nbridging the gap between intention and execution. This integration has the\npotential to transform service composition into a fully automated,\nuser-friendly process driven by high-level natural language intent.", "AI": {"tldr": "论文提出整合大型推理模型（LRMs）和大型动作模型（LAMs）的架构，以实现自动化服务组合。", "motivation": "服务组合面临推理能力有限和执行机制脆弱的挑战。LRMs和LAMs虽然有各自优势，但也存在互补的局限性（LRMs缺乏具体动作能力，LAMs推理能力不足）。", "method": "提出一个集成的LRM-LAM架构框架。", "result": "该集成系统能够推理服务需求和约束，同时动态执行工作流，从而弥合意图与执行之间的差距。", "conclusion": "这种集成有潜力将服务组合转变为一个完全自动化、用户友好且由高级自然语言意图驱动的过程。"}}
{"id": "2507.19029", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.19029", "abs": "https://arxiv.org/abs/2507.19029", "authors": ["Selma Cheshmeh Khavar", "Arya Abdollahi"], "title": "Research on Sectionalizing Switches Placement Problem of Distribution System Automation Based on Multi-Objective Optimization Analysis", "comment": null, "summary": "Achieving high distribution-reliability levels and concurrently minimizing\noperating costs can be considered as the main issues in distribution system\noptimization. Determination of the optimal number and location of automation\ndevices in the distribution system network is an essential issue from the\nreliability and economical points of view. To address these issues, this paper\ndevelops a multi-objective model, wherein the primary objective, optimal\nautomation devices placement is implemented aiming at minimizing the operating\ncosts, while in the second objective the reliability indices improvement is\ntaken into account. So, modified non dominated sorting genetic algorithm, is\ndeveloped and presented to solve this multi-objective mixed-integer non-linear\nprogramming problem. The feasibility of the proposed algorithm examined by\napplication to two distribution feeders of the Tabriz distribution network\ncontaining the third feeder of the Azar substation with a distributed\ngeneration unit and first and third feeders of ElGoli substation which form a\ndouble feed feeder.", "AI": {"tldr": "本文提出了一种改进的多目标非支配排序遗传算法，用于解决配电系统中自动化设备的最优数量和位置确定问题，旨在同时最小化运行成本和提高可靠性。", "motivation": "配电系统优化面临的主要挑战是在实现高配电可靠性水平的同时，最大限度地降低运行成本。确定自动化设备的最优数量和位置对于提高可靠性和经济性至关重要。", "method": "本文开发了一个多目标模型，其中主要目标是优化自动化设备的位置以最小化运行成本，次要目标是改善可靠性指标。为解决这个多目标混合整数非线性规划问题，提出并应用了一种改进的非支配排序遗传算法。", "result": "通过将所提出的算法应用于大不里士配电网的两个配电馈线（包括带有分布式发电单元的Azar变电站的第三馈线以及构成双馈线的ElGoli变电站的第一和第三馈线），验证了该算法的可行性。", "conclusion": "该研究提供了一种有效的方法，通过优化自动化设备的放置，在配电系统中实现运行成本最小化和可靠性提高的双重目标。"}}
{"id": "2507.19199", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19199", "abs": "https://arxiv.org/abs/2507.19199", "authors": ["Abdul Hannan", "Zahid Mahmood", "Rizwan Qureshi", "Hazrat Ali"], "title": "Enhancing Diabetic Retinopathy Classification Accuracy through Dual Attention Mechanism in Deep Learning", "comment": "submitted to Computer Methods in Biomechanics and Biomedical\n  Engineering: Imaging & Visualization", "summary": "Automatic classification of Diabetic Retinopathy (DR) can assist\nophthalmologists in devising personalized treatment plans, making it a critical\ncomponent of clinical practice. However, imbalanced data distribution in the\ndataset becomes a bottleneck in the generalization of deep learning models\ntrained for DR classification. In this work, we combine global attention block\n(GAB) and category attention block (CAB) into the deep learning model, thus\neffectively overcoming the imbalanced data distribution problem in DR\nclassification. Our proposed approach is based on an attention mechanism-based\ndeep learning model that employs three pre-trained networks, namely,\nMobileNetV3-small, Efficientnet-b0, and DenseNet-169 as the backbone\narchitecture. We evaluate the proposed method on two publicly available\ndatasets of retinal fundoscopy images for DR. Experimental results show that on\nthe APTOS dataset, the DenseNet-169 yielded 83.20% mean accuracy, followed by\nthe MobileNetV3-small and EfficientNet-b0, which yielded 82% and 80%\naccuracies, respectively. On the EYEPACS dataset, the EfficientNet-b0 yielded a\nmean accuracy of 80%, while the DenseNet-169 and MobileNetV3-small yielded\n75.43% and 76.68% accuracies, respectively. In addition, we also compute the\nF1-score of 82.0%, precision of 82.1%, sensitivity of 83.0%, specificity of\n95.5%, and a kappa score of 88.2% for the experiments. Moreover, in our work,\nthe MobileNetV3-small has 1.6 million parameters on the APTOS dataset and 0.90\nmillion parameters on the EYEPACS dataset, which is comparatively less than\nother methods. The proposed approach achieves competitive performance that is\nat par with recently reported works on DR classification.", "AI": {"tldr": "该研究提出了一种结合全局和类别注意力机制的深度学习模型，以克服糖尿病视网膜病变（DR）分类中数据不平衡问题，并在两个公开数据集上取得了有竞争力的性能。", "motivation": "自动分类糖尿病视网膜病变（DR）对制定个性化治疗方案至关重要。然而，数据集中不平衡的数据分布限制了深度学习模型在DR分类中的泛化能力。", "method": "将全局注意力块（GAB）和类别注意力块（CAB）集成到深度学习模型中，以有效解决数据不平衡问题。该方法基于注意力机制，并采用MobileNetV3-small、Efficientnet-b0和DenseNet-169作为预训练骨干网络。在APTOS和EYEPACS两个公开的眼底图像数据集上进行了评估。", "result": "在APTOS数据集上，DenseNet-169取得了83.20%的平均准确率，MobileNetV3-small和EfficientNet-b0分别为82%和80%。在EYEPACS数据集上，EfficientNet-b0取得了80%的平均准确率，DenseNet-169和MobileNetV3-small分别为75.43%和76.68%。实验还计算了82.0%的F1分数、82.1%的精确度、83.0%的敏感度、95.5%的特异性和88.2%的Kappa分数。此外，MobileNetV3-small在APTOS和EYEPACS数据集上的参数量分别为160万和90万，相对较少。", "conclusion": "所提出的注意力机制方法有效克服了DR分类中的数据不平衡问题，并取得了与现有DR分类研究相当的竞争性性能，其中MobileNetV3-small模型参数量较少。"}}
{"id": "2507.18769", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18769", "abs": "https://arxiv.org/abs/2507.18769", "authors": ["Nicole Lai-Lopez", "Lusha Wang", "Su Yuan", "Liza Zhang"], "title": "ylmmcl at Multilingual Text Detoxification 2025: Lexicon-Guided Detoxification and Classifier-Gated Rewriting", "comment": "16 pages, 5 figures, 3 tables,", "summary": "In this work, we introduce our solution for the Multilingual Text\nDetoxification Task in the PAN-2025 competition for the ylmmcl team: a robust\nmultilingual text detoxification pipeline that integrates lexicon-guided\ntagging, a fine-tuned sequence-to-sequence model (s-nlp/mt0-xl-detox-orpo) and\nan iterative classifier-based gatekeeping mechanism. Our approach departs from\nprior unsupervised or monolingual pipelines by leveraging explicit toxic word\nannotation via the multilingual_toxic_lexicon to guide detoxification with\ngreater precision and cross-lingual generalization. Our final model achieves\nthe highest STA (0.922) from our previous attempts, and an average official J\nscore of 0.612 for toxic inputs in both the development and test sets. It also\nachieved xCOMET scores of 0.793 (dev) and 0.787 (test). This performance\noutperforms baseline and backtranslation methods across multiple languages, and\nshows strong generalization in high-resource settings (English, Russian,\nFrench). Despite some trade-offs in SIM, the model demonstrates consistent\nimprovements in detoxification strength. In the competition, our team achieved\nninth place with a score of 0.612.", "AI": {"tldr": "本文介绍了PAN-2025多语言文本去毒化任务的解决方案，该方案整合了词典引导标注、微调的序列到序列模型和迭代分类器门控机制，实现了高精度和跨语言泛化。", "motivation": "现有去毒化方法多为无监督或单语言管道，缺乏精确的毒性词汇标注和跨语言泛化能力。本研究旨在开发一个鲁棒的多语言去毒化解决方案，以更高精度和更强的跨语言泛化能力处理毒性文本。", "method": "研究采用了一个多语言文本去毒化管道，包括：1) 基于`multilingual_toxic_lexicon`的词典引导标注，用于显式毒性词汇识别；2) 一个经过微调的序列到序列模型（s-nlp/mt0-xl-detox-orpo）；3) 一个迭代的基于分类器的门控机制。该方法通过利用明确的毒性词汇标注来指导去毒化过程。", "result": "最终模型在开发和测试集上取得了0.922的STA分数、0.612的平均官方J分数。xCOMET分数分别为0.793（开发集）和0.787（测试集）。该性能优于基线和回译方法，并在高资源语言（英语、俄语、法语）中显示出强大的泛化能力。尽管在SIM（相似度）方面存在一些权衡，模型在去毒化强度上表现出持续改进。在竞赛中，团队以0.612的分数获得第九名。", "conclusion": "所提出的多语言文本去毒化解决方案通过整合词典引导标注、微调模型和门控机制，有效提升了去毒化精度和跨语言泛化能力，并在多语言环境中取得了优异表现，超越了现有基线方法。"}}
{"id": "2507.18819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18819", "abs": "https://arxiv.org/abs/2507.18819", "authors": ["Trent Weiss", "Madhur Behl"], "title": "Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Overtaking in high-speed autonomous racing demands precise, real-time\nestimation of collision risk; particularly in wheel-to-wheel scenarios where\nsafety margins are minimal. Existing methods for collision risk estimation\neither rely on simplified geometric approximations, like bounding circles, or\nperform Monte Carlo sampling which leads to overly conservative motion planning\nbehavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)\nalgorithm, a principled two-stage integration method that estimates collision\nrisk by combining Gauss-Legendre with a non-homogeneous Poisson process over\ntime. GLR produces accurate risk estimates that account for vehicle geometry\nand trajectory uncertainty. In experiments across 446 overtaking scenarios in a\nhigh-fidelity Formula One racing simulation, GLR outperforms five\nstate-of-the-art baselines achieving an average error reduction of 77% and\nsurpassing the next-best method by 52%, all while running at 1000 Hz. The\nframework is general and applicable to broader motion planning contexts beyond\nautonomous racing.", "AI": {"tldr": "针对高速自动驾驶赛车超车场景，本文提出了高斯-勒让德矩形（GLR）算法，通过结合高斯-勒让德积分和非齐次泊松过程，实现了精确且实时的碰撞风险估计，在仿真中表现优于现有方法。", "motivation": "在高速自动驾驶赛车超车场景中，碰撞风险估计需要极高的精度和实时性，因为安全裕度极小。现有方法要么依赖简化的几何近似（如边界圆），要么使用蒙特卡洛采样导致过于保守的运动规划行为，无法满足赛车速度下的要求。", "method": "本文引入了高斯-勒让德矩形（GLR）算法，这是一种原则性的两阶段积分方法。它通过结合高斯-勒让德积分和随时间变化的非齐次泊松过程来估计碰撞风险，同时考虑了车辆几何形状和轨迹不确定性。", "result": "在对446个高速F1赛车模拟超车场景的实验中，GLR算法的平均误差降低了77%，比次优方法高出52%，且运行频率达到1000 Hz，性能显著优于五种最先进的基线方法。", "conclusion": "GLR算法能为自动驾驶赛车提供精确且快速的碰撞风险估计，并且该框架具有通用性，可应用于自动驾驶赛车以外的更广泛运动规划场景。"}}
{"id": "2507.18647", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18647", "abs": "https://arxiv.org/abs/2507.18647", "authors": ["Rayyan Ridwan"], "title": "XAI-Guided Analysis of Residual Networks for Interpretable Pneumonia Detection in Paediatric Chest X-rays", "comment": "13 pages, 14 figures", "summary": "Pneumonia remains one of the leading causes of death among children\nworldwide, underscoring a critical need for fast and accurate diagnostic tools.\nIn this paper, we propose an interpretable deep learning model on Residual\nNetworks (ResNets) for automatically diagnosing paediatric pneumonia on chest\nX-rays. We enhance interpretability through Bayesian Gradient-weighted Class\nActivation Mapping (BayesGrad-CAM), which quantifies uncertainty in visual\nexplanations, and which offers spatial locations accountable for the\ndecision-making process of the model. Our ResNet-50 model, trained on a large\npaediatric chest X-rays dataset, achieves high classification accuracy\n(95.94%), AUC-ROC (98.91%), and Cohen's Kappa (0.913), accompanied by\nclinically meaningful visual explanations. Our findings demonstrate that high\nperformance and interpretability are not only achievable but critical for\nclinical AI deployment.", "AI": {"tldr": "该论文提出一个基于ResNet的深度学习模型，结合BayesGrad-CAM提高可解释性，用于儿科肺炎的胸部X光自动诊断，取得了高准确性和有意义的视觉解释。", "motivation": "肺炎仍是全球儿童死亡的主要原因之一，迫切需要快速准确的诊断工具。", "method": "提出一个基于残差网络（ResNets）的可解释深度学习模型，用于儿科肺炎的胸部X光自动诊断。通过贝叶斯梯度加权类激活映射（BayesGrad-CAM）增强可解释性，该方法量化视觉解释中的不确定性并指出模型决策的空间位置。", "result": "在大型儿科胸部X光数据集上训练的ResNet-50模型实现了高分类准确率（95.94%）、AUC-ROC（98.91%）和Cohen's Kappa（0.913），并提供了具有临床意义的视觉解释。", "conclusion": "研究结果表明，高性能和可解释性在临床AI部署中不仅是可实现的，而且至关重要。"}}
{"id": "2507.18795", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18795", "abs": "https://arxiv.org/abs/2507.18795", "authors": ["Fatima Al-Ani", "Molly Wang", "Jevon Charles", "Aaron Ong", "Joshua Forday", "Vinayak Modi"], "title": "Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization", "comment": null, "summary": "This study focuses on the development of a simulation-driven reinforcement\nlearning (RL) framework for optimizing routing decisions in complex queueing\nnetwork systems, with a particular emphasis on manufacturing and communication\napplications. Recognizing the limitations of traditional queueing methods,\nwhich often struggle with dynamic, uncertain environments, we propose a robust\nRL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with\nDyna-style planning (Dyna-DDPG). The framework includes a flexible and\nconfigurable simulation environment capable of modeling diverse queueing\nscenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG\nimplementation incorporates separate predictive models for next-state\ntransitions and rewards, significantly improving stability and sample\nefficiency. Comprehensive experiments and rigorous evaluations demonstrate the\nframework's capability to rapidly learn effective routing policies that\nmaintain robust performance under disruptions and scale effectively to larger\nnetwork sizes. Additionally, we highlight strong software engineering practices\nemployed to ensure reproducibility and maintainability of the framework,\nenabling practical deployment in real-world scenarios.", "AI": {"tldr": "本研究开发了一个基于仿真的强化学习框架（Dyna-DDPG），用于优化复杂排队网络系统（如制造和通信）中的路由决策，以应对动态不确定环境。", "motivation": "传统的排队论方法在动态、不确定环境中表现不佳，难以处理这类复杂系统的路由优化问题。", "method": "提出了一个结合深度确定性策略梯度（DDPG）和Dyna式规划（Dyna-DDPG）的强化学习方法。该框架包含一个灵活可配置的仿真环境，能够模拟各种排队场景和中断。Dyna-DDPG实现中，为下一状态转换和奖励分别引入了独立的预测模型，以提高稳定性和样本效率。", "result": "实验和评估表明，该框架能快速学习有效的路由策略，在中断下保持鲁棒性能，并能有效扩展到更大的网络规模。此外，框架采用良好的软件工程实践，确保了可复现性和可维护性。", "conclusion": "该仿真驱动的强化学习框架（Dyna-DDPG）能够有效、鲁棒地优化复杂排队网络中的路由决策，并具备实际部署潜力。"}}
{"id": "2507.19111", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.19111", "abs": "https://arxiv.org/abs/2507.19111", "authors": ["Bowen Li", "Junting Chen"], "title": "Radio Map Assisted Routing and Predictive Resource Allocation over Dynamic Low Altitude Networks", "comment": null, "summary": "Dynamic low altitude networks offer significant potential for efficient and\nreliable data transport via unmanned aerial vehicles (UAVs) relays which\nusually operate with predetermined trajectories. However, it is challenging to\noptimize the data routing and resource allocation due to the time-varying\ntopology and the need to control interference with terrestrial systems.\nTraditional schemes rely on time-expanded graphs with uniform and fine time\nsubdivisions, making them impractical for interference-aware applications. This\npaper develops a dynamic space-time graph model with a cross-layer optimization\nframework that converts a joint routing and predictive resource allocation\nproblem into a joint bottleneck path planning and resource allocation problem.\nWe develop explicit deterministic bounds to handle the channel uncertainty and\nprove a monotonicity property in the problem structure that enables us to\nefficiently reach the globally optimal solution to the predictive resource\nallocation subproblem. Then, this approach is extended to multi-commodity\ntransmission tasks through time-frequency allocation, and a bisection search\nalgorithm is developed to find the optimum solution by leveraging the\nmonotonicity of the feasible set family. Simulations verify that the\nsingle-commodity algorithm approaches global optimality with more than 30 dB\nperformance gain over the classical graph-based methods for delay-sensitive and\nlarge data transportation. At the same time, the multi-commodity method\nachieves 100X improvements in dense service scenarios and enables an additional\n20 dB performance gain by data segmenting.", "AI": {"tldr": "该论文提出一种动态时空图模型及跨层优化框架，解决无人机中继低空网络中数据路由和预测性资源分配问题，通过利用问题结构中的单调性实现高效优化，并在单商品和多商品传输场景下验证了显著的性能提升。", "motivation": "动态低空网络（UAV中继）在数据传输方面潜力巨大，但由于时变拓扑和对地面系统的干扰控制需求，数据路由和资源分配优化面临挑战。传统方法依赖于均匀细粒度时间划分的时扩展图，不适用于干扰感知应用。", "method": "开发了一个动态时空图模型和跨层优化框架，将联合路由与预测性资源分配问题转换为联合瓶颈路径规划与资源分配问题。通过显式确定性界限处理信道不确定性，并利用问题结构的单调性实现预测性资源分配子问题的全局最优解。进一步扩展到多商品传输任务，通过时频分配和二分搜索算法找到最优解，利用了可行集族的单调性。", "result": "单商品算法在延迟敏感和大数据传输方面，比传统基于图的方法性能提升超过30 dB，接近全局最优。多商品方法在密集服务场景中实现了100倍的改进，并通过数据分段额外获得了20 dB的性能增益。", "conclusion": "所提出的动态时空图模型和优化框架能有效解决无人机中继低空网络的路由和资源分配挑战，在单商品和多商品传输任务中均表现出显著的性能提升，尤其适用于延迟敏感和高密度服务场景。"}}
{"id": "2507.19282", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.19282", "abs": "https://arxiv.org/abs/2507.19282", "authors": ["Guoping Xu", "Yan Dai", "Hengrui Zhao", "Ying Zhang", "Jie Deng", "Weiguo Lu", "You Zhang"], "title": "SAM2-Aug: Prior knowledge-based Augmentation for Target Volume Auto-Segmentation in Adaptive Radiation Therapy Using Segment Anything Model 2", "comment": "26 pages, 10 figures", "summary": "Purpose: Accurate tumor segmentation is vital for adaptive radiation therapy\n(ART) but remains time-consuming and user-dependent. Segment Anything Model 2\n(SAM2) shows promise for prompt-based segmentation but struggles with tumor\naccuracy. We propose prior knowledge-based augmentation strategies to enhance\nSAM2 for ART.\n  Methods: Two strategies were introduced to improve SAM2: (1) using prior MR\nimages and annotations as contextual inputs, and (2) improving prompt\nrobustness via random bounding box expansion and mask erosion/dilation. The\nresulting model, SAM2-Aug, was fine-tuned and tested on the One-Seq-Liver\ndataset (115 MRIs from 31 liver cancer patients), and evaluated without\nretraining on Mix-Seq-Abdomen (88 MRIs, 28 patients) and Mix-Seq-Brain (86\nMRIs, 37 patients).\n  Results: SAM2-Aug outperformed convolutional, transformer-based, and\nprompt-driven models across all datasets, achieving Dice scores of 0.86(liver),\n0.89(abdomen), and 0.90(brain). It demonstrated strong generalization across\ntumor types and imaging sequences, with improved performance in\nboundary-sensitive metrics.\n  Conclusions: Incorporating prior images and enhancing prompt diversity\nsignificantly boosts segmentation accuracy and generalizability. SAM2-Aug\noffers a robust, efficient solution for tumor segmentation in ART. Code and\nmodels will be released at https://github.com/apple1986/SAM2-Aug.", "AI": {"tldr": "本文提出了一种名为SAM2-Aug的模型，通过引入先验知识增强和提示鲁棒性策略，显著提高了SAM2在自适应放射治疗中肿瘤分割的准确性和泛化能力。", "motivation": "肿瘤分割对于自适应放射治疗（ART）至关重要，但现有方法耗时且依赖用户。Segment Anything Model 2 (SAM2) 在基于提示的分割方面有潜力，但在肿瘤分割精度上表现不足。", "method": "研究者提出了两种增强SAM2的策略：1) 使用先前的MR图像和注释作为上下文输入；2) 通过随机边界框扩展和掩膜侵蚀/膨胀来提高提示的鲁棒性。由此产生的模型SAM2-Aug在One-Seq-Liver数据集上进行了微调和测试，并在Mix-Seq-Abdomen和Mix-Seq-Brain数据集上进行了未经再训练的评估。", "result": "SAM2-Aug在所有数据集上均优于卷积、基于Transformer和提示驱动的模型，在肝脏、腹部和大脑数据集上分别达到了0.86、0.89和0.90的Dice分数。它在不同肿瘤类型和成像序列上表现出强大的泛化能力，并在边界敏感指标上有所改进。", "conclusion": "整合先验图像和增强提示多样性显著提升了分割精度和泛化能力。SAM2-Aug为ART中的肿瘤分割提供了一个鲁棒、高效的解决方案。"}}
{"id": "2507.18791", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18791", "abs": "https://arxiv.org/abs/2507.18791", "authors": ["Yilun Yang", "Yekun Chai"], "title": "Evaluating Code-Mixing in LLMs Across 18 Languages", "comment": null, "summary": "Code-mixing, the practice of switching between languages within a\nconversation, presents unique challenges for traditional natural language\nprocessing. Existing benchmarks, such as LinCE and GLUECoS, are limited by\nnarrow language pairings and tasks, failing to adequately evaluate the\ncode-mixing capabilities of large language models (LLMs). Despite the\nsignificance of code-mixing for multilingual users, research on LLMs in this\ncontext remains limited. Additionally, current methods for generating\ncode-mixed data are underdeveloped. In this paper, we conduct a comprehensive\nevaluation of LLMs' performance on code-mixed data across 18 languages from\nseven language families. We also propose a novel approach for generating\nsynthetic code-mixed texts by combining word substitution with GPT-4 prompting.\nOur analysis reveals consistent underperformance of LLMs on code-mixed datasets\ninvolving multiple language families. We suggest that improvements in training\ndata size, model scale, and few-shot learning could enhance their performance.", "AI": {"tldr": "本文对大型语言模型（LLMs）在18种语言的语码混用数据上的性能进行了全面评估，并提出了一种新的合成语码混用文本生成方法。研究发现LLMs在跨语系语码混用数据上表现不佳。", "motivation": "语码混用对传统自然语言处理构成独特挑战，现有基准测试（如LinCE和GLUECoS）受限于狭窄的语言对和任务，未能充分评估LLMs的语码混用能力。尽管语码混用对多语言用户意义重大，但LLMs在此领域的研究仍有限。此外，当前的语码混用数据生成方法不完善。", "method": "研究方法包括：1) 对LLMs在来自七个语系的18种语言的语码混用数据上进行全面评估。2) 提出一种结合词语替换和GPT-4提示的新颖方法来生成合成语码混用文本。", "result": "分析显示，LLMs在涉及多个语系的语码混用数据集上持续表现不佳。", "conclusion": "为提升LLMs在语码混用场景下的性能，建议改进训练数据规模、模型大小和少样本学习能力。"}}
{"id": "2507.18820", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18820", "abs": "https://arxiv.org/abs/2507.18820", "authors": ["Rachel Ringe", "Robin Nolte", "Nima Zargham", "Robert Porzel", "Rainer Malaka"], "title": "MetaMorph -- A Metamodelling Approach For Robot Morphology", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Robot appearance crucially shapes Human-Robot Interaction (HRI) but is\ntypically described via broad categories like anthropomorphic, zoomorphic, or\ntechnical. More precise approaches focus almost exclusively on anthropomorphic\nfeatures, which fail to classify robots across all types, limiting the ability\nto draw meaningful connections between robot design and its effect on\ninteraction. In response, we present MetaMorph, a comprehensive framework for\nclassifying robot morphology. Using a metamodeling approach, MetaMorph was\nsynthesized from 222 robots in the IEEE Robots Guide, offering a structured\nmethod for comparing visual features. This model allows researchers to assess\nthe visual distances between robot models and explore optimal design traits\ntailored to different tasks and contexts.", "AI": {"tldr": "MetaMorph是一个综合性的机器人形态分类框架，解决了现有分类方法过于宽泛或仅限于拟人化的问题，能更精确地连接机器人设计与人机交互效果。", "motivation": "机器人外观对人机交互（HRI）至关重要，但现有分类方法过于宽泛（如拟人化、动物化、技术型）或仅关注拟人化特征，无法对所有类型的机器人进行精确分类，限制了理解机器人设计如何影响交互的能力。", "method": "研究者提出了MetaMorph框架，采用元建模方法，从IEEE机器人指南中的222个机器人数据中综合提炼而成，旨在提供一种结构化的方法来比较视觉特征。", "result": "MetaMorph模型允许研究人员评估不同机器人模型之间的视觉距离，并探索针对不同任务和情境的最佳设计特征。", "conclusion": "MetaMorph提供了一个更精确和全面的机器人形态分类工具，有助于研究人员深入理解机器人设计与人机交互之间的关系，并优化机器人设计以适应特定应用。"}}
{"id": "2507.18649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18649", "abs": "https://arxiv.org/abs/2507.18649", "authors": ["Haiyang Liu", "Xiaolin Hong", "Xuancheng Yang", "Yudi Ruan", "Xiang Lian", "Michael Lingelbach", "Hongwei Yi", "Wei Li"], "title": "Livatar-1: Real-Time Talking Heads Generation with Tailored Flow Matching", "comment": "Technical Report", "summary": "We present Livatar, a real-time audio-driven talking heads videos generation\nframework. Existing baselines suffer from limited lip-sync accuracy and\nlong-term pose drift. We address these limitations with a flow matching based\nframework. Coupled with system optimizations, Livatar achieves competitive\nlip-sync quality with a 8.50 LipSync Confidence on the HDTF dataset, and\nreaches a throughput of 141 FPS with an end-to-end latency of 0.17s on a single\nA10 GPU. This makes high-fidelity avatars accessible to broader applications.\nOur project is available at https://www.hedra.com/ with with examples at\nhttps://h-liu1997.github.io/Livatar-1/", "AI": {"tldr": "Livatar是一个实时音频驱动的数字人说话视频生成框架，通过流匹配和系统优化，解决了现有方法唇形同步不佳和姿态漂移问题，实现了高保真、低延迟的生成。", "motivation": "现有基线方法在唇形同步准确性方面有限，且存在长期姿态漂移问题。", "method": "采用基于流匹配（flow matching）的框架，并结合系统优化。", "result": "在HDTF数据集上实现了8.50的唇形同步置信度，唇形同步质量具有竞争力；在单张A10 GPU上，吞吐量达到141 FPS，端到端延迟为0.17秒。", "conclusion": "该框架使高保真数字人能够应用于更广泛的场景。"}}
{"id": "2507.18868", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.18868", "abs": "https://arxiv.org/abs/2507.18868", "authors": ["Alex Noviello", "Claas Beger", "Jacob Groner", "Kevin Ellis", "Weinan Sun"], "title": "A Neuroscience-Inspired Dual-Process Model of Compositional Generalization", "comment": null, "summary": "Systematic compositional generalization - constructing and understanding\nnovel combinations of known building blocks - remains a core challenge for AI\nsystems. Human cognition achieves this flexibility via the interplay of the\nhippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes\nepisodes, and the prefrontal cortex consolidates them into reusable schemas for\nreasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with\nRules and Abstractions from Generalized Experience), a framework that achieves\nsystematic generalization on compositional tasks. MIRAGE has two interacting\nmodules mirroring the brain's deliberative HPC-PFC loop and intuitive\nneocortical pattern recognition. (1) The meta-trained Transformer Neural\nDecomposer, paralleling neocortical \"System 1\" computation, is trained on a\ntask-agnostic stream of randomly sampled compositional grammars and applies one\ndecomposition step per pass, with successive passes iteratively refining the\nsequence representation. (2) The Schema Engine, analogous to the HPC-PFC\n\"System 2\" loop, dynamically extracts, ranks, and applies reusable schemas,\nstoring variable bindings in episodic memory and expanding them when needed. By\nexplicitly equipping the Transformer component of MIRAGE with actively managed\nschematic structures, our model performs systematic compositional operations\nthrough explicit schema application and transformation, relying solely on\nfrozen weights when solving entirely novel tasks. This approach demonstrates\nsystematic compositional generalization on the SCAN benchmark, achieving > 99%\naccuracy on all task splits with only 1.19M parameters in the transformer\nmodule. Ablation studies confirm that MIRAGE's systematicity critically depends\non the quality of extracted schemas and the model's iterative refinement\nprocess.", "AI": {"tldr": "MIRAGE是一个受大脑HPC-PFC环路启发的框架，通过结合元训练的Transformer和动态模式引擎，实现了对组合任务的系统泛化能力，在SCAN基准测试中表现出色。", "motivation": "AI系统在系统性组合泛化（即构建和理解已知构建块的新颖组合）方面面临核心挑战。人类认知通过海马体（HPC）和前额叶皮层（PFC）的相互作用来实现这种灵活性，海马体快速编码事件，前额叶皮层将其巩固为可重用的推理模式。", "method": "MIRAGE框架包含两个交互模块，模仿大脑的HPC-PFC回路和新皮层模式识别：1) 元训练的Transformer神经分解器（类似“系统1”），在任务无关的组合语法流上训练，每次通过一次分解步骤并迭代细化序列表示。2) 模式引擎（类似HPC-PFC“系统2”），动态提取、排序和应用可重用模式，在情景记忆中存储变量绑定并按需扩展。模型通过主动管理的模式结构和迭代细化过程进行系统组合操作。", "result": "MIRAGE在SCAN基准测试上实现了系统性组合泛化，所有任务分割的准确率均超过99%，其Transformer模块仅有1.19M参数。消融研究证实，MIRAGE的系统性关键取决于提取模式的质量和模型的迭代细化过程。", "conclusion": "MIRAGE通过明确装备Transformer组件以主动管理的模式结构，并借鉴大脑的HPC-PFC环路机制，实现了系统性组合泛化，仅依靠冻结权重即可解决全新任务，证明了其方法的有效性。"}}
{"id": "2507.19244", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.19244", "abs": "https://arxiv.org/abs/2507.19244", "authors": ["Rodrigo A. González", "Angel L. Cedeño", "Koen Tiels", "Tom Oomen"], "title": "Truncated Gaussian Noise Estimation in State-Space Models", "comment": "6 pages,2 figures", "summary": "Within Bayesian state estimation, considerable effort has been devoted to\nincorporating constraints into state estimation for process optimization, state\nmonitoring, fault detection and control. Nonetheless, in the domain of\nstate-space system identification, the prevalent practice entails constructing\nmodels under Gaussian noise assumptions, which can lead to inaccuracies when\nthe noise follows bounded distributions. With the aim of generalizing the\nGaussian noise assumption to potentially truncated densities, this paper\nintroduces a method for estimating the noise parameters in a state-space model\nsubject to truncated Gaussian noise. Our proposed data-driven approach is\nrooted in maximum likelihood principles combined with the\nExpectation-Maximization algorithm. The efficacy of the proposed approach is\nsupported by a simulation example.", "AI": {"tldr": "本文提出了一种基于最大似然和EM算法的数据驱动方法，用于估计受截断高斯噪声影响的状态空间模型中的噪声参数，以解决传统高斯噪声假设在有界噪声下不准确的问题。", "motivation": "在贝叶斯状态估计中，虽然已努力将约束纳入状态估计，但在状态空间系统辨识领域，主流做法仍是基于高斯噪声假设构建模型。当噪声遵循有界分布时，这种假设会导致不准确性。因此，研究动机在于将高斯噪声假设推广到潜在的截断密度。", "method": "提出了一种用于估计受截断高斯噪声影响的状态空间模型中噪声参数的方法。该方法是数据驱动的，基于最大似然原理并结合了期望最大化（EM）算法。", "result": "通过一个仿真示例验证了所提出方法的有效性。", "conclusion": "所提出的基于最大似然和EM算法的方法，能够有效估计受截断高斯噪声影响的状态空间模型中的噪声参数，从而解决了传统高斯噪声假设在有界噪声下可能导致不准确的问题。"}}
{"id": "2507.19404", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.19404", "abs": "https://arxiv.org/abs/2507.19404", "authors": ["Chong Chen", "Marc Vornehm", "Preethi Chandrasekaran", "Muhammad A. Sultan", "Syed M. Arshad", "Yingmin Liu", "Yuchi Han", "Rizwan Ahmad"], "title": "A multi-dynamic low-rank deep image prior (ML-DIP) for real-time 3D cardiovascular MRI", "comment": null, "summary": "Purpose: To develop a reconstruction framework for 3D real-time cine\ncardiovascular magnetic resonance (CMR) from highly undersampled data without\nrequiring fully sampled training data.\n  Methods: We developed a multi-dynamic low-rank deep image prior (ML-DIP)\nframework that models spatial image content and temporal deformation fields\nusing separate neural networks. These networks are optimized per scan to\nreconstruct the dynamic image series directly from undersampled k-space data.\nML-DIP was evaluated on (i) a 3D cine digital phantom with simulated premature\nventricular contractions (PVCs), (ii) ten healthy subjects (including two\nscanned during both rest and exercise), and (iii) five patients with PVCs.\nPhantom results were assessed using peak signal-to-noise ratio (PSNR) and\nstructural similarity index measure (SSIM). In vivo performance was evaluated\nby comparing left-ventricular function quantification (against 2D real-time\ncine) and image quality (against 2D real-time cine and binning-based 5D-Cine).\n  Results: In the phantom study, ML-DIP achieved PSNR > 29 dB and SSIM > 0.90\nfor scan times as short as two minutes, while recovering cardiac motion,\nrespiratory motion, and PVC events. In healthy subjects, ML-DIP yielded\nfunctional measurements comparable to 2D cine and higher image quality than\n5D-Cine, including during exercise with high heart rates and bulk motion. In\nPVC patients, ML-DIP preserved beat-to-beat variability and reconstructed\nirregular beats, whereas 5D-Cine showed motion artifacts and information loss\ndue to binning.\n  Conclusion: ML-DIP enables high-quality 3D real-time CMR with acceleration\nfactors exceeding 1,000 by learning low-rank spatial and temporal\nrepresentations from undersampled data, without relying on external fully\nsampled training datasets.", "AI": {"tldr": "该研究开发了一种名为ML-DIP的框架，用于从高度欠采样的K空间数据中重建高质量的3D实时心血管磁共振（CMR）图像，无需完全采样的训练数据，并能有效处理心律不齐。", "motivation": "开发一种无需完全采样训练数据、能从高度欠采样数据中重建3D实时电影CMR的方法，以克服现有技术在处理欠采样数据和缺乏训练数据方面的挑战。", "method": "开发了多动态低秩深度图像先验（ML-DIP）框架，该框架使用独立的神经网络分别建模空间图像内容和时间形变场。这些网络针对每次扫描进行优化，直接从欠采样的K空间数据重建动态图像序列。通过3D电影数字体模（含室性早搏）、10名健康受试者（包括静息和运动状态）和5名室性早搏患者进行了评估。性能通过峰值信噪比（PSNR）、结构相似性指数（SSIM）、左心室功能量化和图像质量进行衡量，并与2D实时电影和基于分箱的5D-Cine进行比较。", "result": "在体模研究中，ML-DIP在短至两分钟的扫描时间内实现了PSNR > 29 dB和SSIM > 0.90，并能恢复心脏运动、呼吸运动和室性早搏事件。在健康受试者中，ML-DIP的功能测量结果与2D电影相当，图像质量优于5D-Cine，包括在高心率和整体运动的运动状态下。在室性早搏患者中，ML-DIP保留了逐搏变异性并重建了不规则搏动，而5D-Cine则因分箱显示出运动伪影和信息丢失。", "conclusion": "ML-DIP通过从欠采样数据中学习低秩空间和时间表示，实现了高质量的3D实时CMR，加速因子超过1000，且不依赖外部完全采样的训练数据集。"}}
{"id": "2507.18827", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18827", "abs": "https://arxiv.org/abs/2507.18827", "authors": ["Pranav Gupta"], "title": "CueBuddy: helping non-native English speakers navigate English-centric STEM education", "comment": null, "summary": "Students across the world in STEM classes, especially in the Global South,\nfall behind their peers who are more fluent in English, despite being at par\nwith them in terms of scientific prerequisites. While many of them are able to\nfollow everyday English at ease, key terms in English stay challenging. In most\ncases, such students have had most of their course prerequisites in a lower\nresource language. Live speech translation to lower resource languages is a\npromising area of research, however, models for speech translation can be too\nexpensive on a large scale and often struggle with technical content. In this\npaper, we describe CueBuddy, which aims to remediate these issues by providing\nreal-time \"lexical cues\" through technical keyword spotting along real-time\nmultilingual glossary lookup to help students stay up to speed with complex\nEnglish jargon without disrupting their concentration on the lecture. We also\ndescribe the limitations and future extensions of our approach.", "AI": {"tldr": "CueBuddy是一个实时辅助工具，通过技术关键词识别和多语言词汇表查询，帮助非英语母语的STEM学生理解复杂的英语术语，而无需打断他们的注意力。", "motivation": "全球STEM学生（尤其是在全球南方）因英语流利度不足而落后，尽管他们在科学先决知识上与同龄人相当。他们通常能理解日常英语，但技术术语仍是挑战。实时语音翻译虽然有前景，但成本高昂且难以处理技术内容。", "method": "CueBuddy通过提供实时“词汇提示”来解决问题，它结合了技术关键词识别和实时的多语言词汇表查询。", "result": "本文描述了CueBuddy系统，旨在帮助学生跟上复杂的英语专业术语，同时不干扰他们对讲座的注意力。文章还讨论了该方法的局限性和未来扩展。", "conclusion": "CueBuddy提供了一种创新的、低干扰的方式，通过实时词汇提示帮助非英语母语的STEM学生克服技术英语术语障碍，从而更好地参与和理解课程内容。"}}
{"id": "2507.18847", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18847", "abs": "https://arxiv.org/abs/2507.18847", "authors": ["Pinhao Song", "Yutong Hu", "Pengteng Li", "Renaud Detry"], "title": "Equivariant Volumetric Grasping", "comment": "19 pages", "summary": "We propose a new volumetric grasp model that is equivariant to rotations\naround the vertical axis, leading to a significant improvement in sample\nefficiency. Our model employs a tri-plane volumetric feature representation --\ni.e., the projection of 3D features onto three canonical planes. We introduce a\nnovel tri-plane feature design in which features on the horizontal plane are\nequivariant to 90{\\deg} rotations, while the sum of features from the other two\nplanes remains invariant to the same transformations. This design is enabled by\na new deformable steerable convolution, which combines the adaptability of\ndeformable convolutions with the rotational equivariance of steerable ones.\nThis allows the receptive field to adapt to local object geometry while\npreserving equivariance properties. We further develop equivariant adaptations\nof two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,\nwe derive a new equivariant formulation of IGD's deformable attention mechanism\nand propose an equivariant generative model of grasp orientations based on flow\nmatching. We provide a detailed analytical justification of the proposed\nequivariance properties and validate our approach through extensive simulated\nand real-world experiments. Our results demonstrate that the proposed\nprojection-based design significantly reduces both computational and memory\ncosts. Moreover, the equivariant grasp models built on top of our tri-plane\nfeatures consistently outperform their non-equivariant counterparts, achieving\nhigher performance with only a modest computational overhead. Video and code\ncan be viewed in: https://mousecpn.github.io/evg-page/", "AI": {"tldr": "提出了一种新的垂直轴旋转等变的三平面体积抓取模型，显著提高了样本效率，并改进了现有抓取规划器。", "motivation": "现有抓取模型在处理旋转时效率低下，导致样本效率、计算和内存成本较高，且性能受限。本研究旨在通过引入旋转等变性来解决这些问题。", "method": "核心方法包括：1) 提出一种新的垂直轴旋转等变的体积抓取模型。2) 采用三平面体积特征表示，并设计一种新颖的三平面特征：水平平面特征对90度旋转等变，另两平面特征之和对相同变换保持不变。3) 引入可变形可操纵卷积（deformable steerable convolution），结合可变形卷积的适应性与可操纵卷积的旋转等变性。4) 开发了GIGA和IGD两种先进体积抓取规划器的等变版本，包括IGD可变形注意力机制的等变公式和基于流匹配的抓取方向等变生成模型。", "result": "所提出的基于投影的设计显著降低了计算和内存成本。基于三平面特征构建的等变抓取模型始终优于非等变模型，以适度的计算开销实现了更高的性能，并显著提高了样本效率。", "conclusion": "所提出的垂直轴旋转等变的三平面体积抓取模型，通过创新的特征表示和卷积设计，有效解决了抓取模型的样本效率、计算和内存问题，并在性能上超越了非等变方法，为体积抓取规划提供了更优的解决方案。"}}
{"id": "2507.18650", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18650", "abs": "https://arxiv.org/abs/2507.18650", "authors": ["Venant Niyonkuru", "Sylla Sekou", "Jimmy Jackson Sinzinkayo"], "title": "Features extraction for image identification using computer vision", "comment": null, "summary": "This study examines various feature extraction techniques in computer vision,\nthe primary focus of which is on Vision Transformers (ViTs) and other\napproaches such as Generative Adversarial Networks (GANs), deep feature models,\ntraditional approaches (SIFT, SURF, ORB), and non-contrastive and contrastive\nfeature models. Emphasizing ViTs, the report summarizes their architecture,\nincluding patch embedding, positional encoding, and multi-head self-attention\nmechanisms with which they overperform conventional convolutional neural\nnetworks (CNNs). Experimental results determine the merits and limitations of\nboth methods and their utilitarian applications in advancing computer vision.", "AI": {"tldr": "本研究探讨了计算机视觉中的多种特征提取技术，重点关注Vision Transformers (ViTs)及其与传统CNNs、GANs、深度特征模型和传统方法（如SIFT）的比较。", "motivation": "旨在检查和比较计算机视觉中各种特征提取技术，特别是ViTs，并评估它们在推动该领域发展中的优点和局限性。", "method": "研究方法包括：1) 总结ViTs的架构（补丁嵌入、位置编码、多头自注意力机制）；2) 讨论其他特征提取方法，如GANs、深度特征模型、传统方法（SIFT, SURF, ORB）以及对比/非对比特征模型；3) 通过实验结果确定各种方法的优缺点。", "result": "实验结果表明，Vision Transformers (ViTs) 在性能上超越了传统的卷积神经网络 (CNNs)。研究还明确了这些方法各自的优点和局限性。", "conclusion": "ViTs在计算机视觉中表现出色，优于传统CNNs，并且这些技术在推动计算机视觉领域发展方面具有重要的实用价值。"}}
{"id": "2507.18883", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18883", "abs": "https://arxiv.org/abs/2507.18883", "authors": ["Wuhao Wang", "Zhiyong Chen"], "title": "Success in Humanoid Reinforcement Learning under Partial Observation", "comment": "11 pages, 3 figures, and 4 tables. Not published anywhere else", "summary": "Reinforcement learning has been widely applied to robotic control, but\neffective policy learning under partial observability remains a major\nchallenge, especially in high-dimensional tasks like humanoid locomotion. To\ndate, no prior work has demonstrated stable training of humanoid policies with\nincomplete state information in the benchmark Gymnasium Humanoid-v4\nenvironment. The objective in this environment is to walk forward as fast as\npossible without falling, with rewards provided for staying upright and moving\nforward, and penalties incurred for excessive actions and external contact\nforces. This research presents the first successful instance of learning under\npartial observability in this environment. The learned policy achieves\nperformance comparable to state-of-the-art results with full state access,\ndespite using only one-third to two-thirds of the original states. Moreover,\nthe policy exhibits adaptability to robot properties, such as variations in\nbody part masses. The key to this success is a novel history encoder that\nprocesses a fixed-length sequence of past observations in parallel. Integrated\ninto a standard model-free algorithm, the encoder enables performance on par\nwith fully observed baselines. We hypothesize that it reconstructs essential\ncontextual information from recent observations, thereby enabling robust\ndecision-making.", "AI": {"tldr": "该研究首次在部分可观测条件下成功训练人形机器人策略，并在Gymnasium Humanoid-v4环境中达到与全状态访问方法相当的性能，关键在于引入了一种新颖的历史编码器。", "motivation": "强化学习在机器人控制中应用广泛，但在部分可观测条件下进行有效策略学习仍是巨大挑战，尤其是在人形机器人运动等高维任务中。此前，在基准Gymnasium Humanoid-v4环境中，尚未有工作能实现不完全状态信息下的人形机器人策略稳定训练。", "method": "引入了一种新颖的历史编码器，该编码器并行处理固定长度的过去观测序列。此编码器被集成到一个标准的无模型强化学习算法中。", "result": "首次在Gymnasium Humanoid-v4环境中实现了部分可观测条件下的成功学习。所学策略仅使用原始状态信息的1/3到2/3，却能达到与全状态访问方法相当的性能。此外，该策略还表现出对机器人属性（如身体部件质量变化）的适应性。", "conclusion": "所提出的历史编码器能够从近期观测中重建必要的上下文信息，从而在部分可观测条件下实现鲁棒的决策，并达到与完全可观测基线相当的性能。"}}
{"id": "2507.19260", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.19260", "abs": "https://arxiv.org/abs/2507.19260", "authors": ["Daniele Falchi", "Eduardo Prieto-Araujo", "Oriol Gomis-Bellmunt"], "title": "Cell-based VSC Analysis Methodology: From Graph Laplacian to Converter Degrees of Freedom", "comment": null, "summary": "Power-electronics-based converters are being considerably employed through\nthe power system to interconnect multiple heterogeneous electrical layers.\nFurthermore, the intrinsic versatility to play with the converter network\ntopology is widely exploited to accommodate a certain number of terminals and\nports according with the specific application. On this regard, several\nconverter arrangements can be encountered in power applications. Moreover, to\nproperly establish both the operation and the control, the so-called degrees of\nfreedom (DOFs) need to be assessed per each converter topology. On this matter,\nsimilarly to the well-known Clarke transformation, which clearly reveals the\nDOFs for the star-based topology system, further similar transformations can be\nachieved to depict the independent set of variables characterizing a certain\nconverter structure. Referring to the cell-based class of Voltage Source\nConverter (VSC) topologies, including Modular Multilevel Converter (MMC); this\narticle proposes a general methodology to determine the change of variable\nmatrix transformation for several converter arrangements which are related to\ncomplete bi-partite and multi-partite graphs. The methodology lies in the graph\nLaplacian spectral analysis, which remarks the structural normal modes at the\nconverter points of connections. Furthermore, for a complete characterization,\nthe instantaneous power patterns formulations, based on the DOFs, are also\nintroduced.", "AI": {"tldr": "本文提出了一种基于图拉普拉斯谱分析的通用方法，用于确定多种电压源变换器（VSC）拓扑（特别是与完全二部和多部图相关的拓扑）的变量变换矩阵，并引入了基于自由度（DOFs）的瞬时功率模式公式。", "motivation": "电力电子变换器在电力系统中广泛应用，连接异构电气层，并具有多种可配置拓扑。为了正确建立其运行和控制，需要评估每种变换器拓扑的自由度。现有方法（如Clarke变换）仅适用于特定拓扑（如星形连接），需要一种更通用的方法来揭示不同变换器结构的独立变量集。", "method": "本文提出了一种通用方法，利用图拉普拉斯谱分析来确定多种变换器配置（与完全二部和多部图相关）的变量变换矩阵。该方法揭示了变换器连接点的结构正常模式。此外，为了进行完整表征，还引入了基于自由度的瞬时功率模式公式。", "result": "该研究提供了一种确定多种VSC拓扑（包括模块化多电平变换器MMC）变量变换矩阵的通用方法，通过图拉普拉斯谱分析揭示了结构正常模式。同时，基于这些自由度，还建立了瞬时功率模式的公式，实现了对变换器的全面表征。", "conclusion": "该方法为评估和理解各种复杂VSC拓扑的自由度和功率特性提供了一种通用且系统化的途径，类似于Clarke变换但更具普适性，有助于变换器的操作和控制设计。"}}
{"id": "2507.18655", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18655", "abs": "https://arxiv.org/abs/2507.18655", "authors": ["James Dickens", "Kamyar Hamad"], "title": "Part Segmentation of Human Meshes via Multi-View Human Parsing", "comment": null, "summary": "Recent advances in point cloud deep learning have led to models that achieve\nhigh per-part labeling accuracy on large-scale point clouds, using only the raw\ngeometry of unordered point sets. In parallel, the field of human parsing\nfocuses on predicting body part and clothing/accessory labels from images. This\nwork aims to bridge these two domains by enabling per-vertex semantic\nsegmentation of large-scale human meshes. To achieve this, a pseudo-ground\ntruth labeling pipeline is developed for the Thuman2.1 dataset: meshes are\nfirst aligned to a canonical pose, segmented from multiple viewpoints, and the\nresulting point-level labels are then backprojected onto the original mesh to\nproduce per-point pseudo ground truth annotations. Subsequently, a novel,\nmemory-efficient sampling strategy is introduced, a windowed iterative farthest\npoint sampling (FPS) with space-filling curve-based serialization to\neffectively downsample the point clouds. This is followed by a purely geometric\nsegmentation using PointTransformer, enabling semantic parsing of human meshes\nwithout relying on texture information. Experimental results confirm the\neffectiveness and accuracy of the proposed approach.", "AI": {"tldr": "本文提出了一种无需纹理信息，利用纯几何深度学习方法对大规模人体网格进行逐顶点语义分割的技术，并开发了伪真值标注和高效采样策略。", "motivation": "将点云深度学习在无序点集上的高精度分割能力与人体解析领域相结合，以实现对大规模人体网格的逐顶点语义分割，弥补现有方法在人体网格语义解析上的空白。", "method": "1. 开发了Thuman2.1数据集的伪真值标注流程：将网格对齐到规范姿态，从多视角进行分割，并将点级标签反向投影到原始网格上。2. 引入了一种内存高效的采样策略：基于空间填充曲线序列化的窗口迭代最远点采样（FPS）。3. 使用PointTransformer进行纯几何分割，不依赖纹理信息。", "result": "实验结果证实了所提出方法的有效性和准确性。", "conclusion": "该研究成功实现了对人体网格的语义解析，证明了纯几何方法在不依赖纹理信息的情况下，也能有效进行大规模人体网格的语义分割。"}}
{"id": "2507.18857", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18857", "abs": "https://arxiv.org/abs/2507.18857", "authors": ["Mohammad Kachuee", "Teja Gollapudi", "Minseok Kim", "Yin Huang", "Kai Sun", "Xiao Yang", "Jiaqi Wang", "Nirav Shah", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "title": "PrismRAG: Boosting RAG Factuality with Distractor Resilience and Strategized Reasoning", "comment": null, "summary": "Retrieval-augmented generation (RAG) often falls short when retrieved context\nincludes confusing semi-relevant passages, or when answering questions require\ndeep contextual understanding and reasoning. We propose an efficient\nfine-tuning framework, called PrismRAG, that (i) trains the model with\ndistractor-aware QA pairs mixing gold evidence with subtle distractor passages,\nand (ii) instills reasoning-centric habits that make the LLM plan, rationalize,\nand synthesize without relying on extensive human engineered instructions.\nEvaluated across 12 open-book RAG QA benchmarks spanning diverse application\ndomains and scenarios, PrismRAG improves average factuality by 5.4%,\noutperforming state-of-the-art solutions.", "AI": {"tldr": "PrismRAG是一种高效的微调框架，通过结合干扰项感知的问答对训练和灌输以推理为中心的习惯，显著提升了检索增强生成（RAG）模型的准确性。", "motivation": "传统的RAG模型在检索到的上下文包含混淆性半相关段落时，或在回答需要深度上下文理解和推理的问题时，表现不佳。", "method": "本文提出了PrismRAG框架，该框架通过两种方式训练模型：(i) 使用混合了黄金证据和微妙干扰项段落的干扰项感知问答对进行训练；(ii) 灌输以推理为中心的习惯，使大型语言模型（LLM）无需依赖大量人工设计的指令即可进行规划、推理和综合。", "result": "在涵盖不同应用领域和场景的12个开放式RAG问答基准测试中，PrismRAG将平均事实性提高了5.4%，超越了现有最先进的解决方案。", "conclusion": "PrismRAG通过其创新的微调方法，有效解决了RAG在处理复杂上下文和需要深度推理问题时的不足，显著提升了模型的准确性和可靠性。"}}
{"id": "2507.18886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18886", "abs": "https://arxiv.org/abs/2507.18886", "authors": ["Zheng Yang", "Kuan Xu", "Shenghai Yuan", "Lihua Xie"], "title": "A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras", "comment": null, "summary": "In this paper, we introduce a novel approach for efficiently estimating the\n6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method\nthat capitalizes on overlapping planar elements. Conventional RGB-D visual\nodometry(RGBD-VO) often relies on iterative optimization solvers to estimate\npose and involves a process of feature extraction and matching. This results in\nsignificant computational burden and time delays. To address this, our\ninnovative method for RGBD-VO separates the estimation of rotation and\ntranslation. Initially, we exploit the overlaid planar characteristics within\nthe scene to calculate the rotation matrix. Following this, we utilize a kernel\ncross-correlator (KCC) to ascertain the translation. By sidestepping the\nresource-intensive iterative optimization and feature extraction and alignment\nprocedures, our methodology offers improved computational efficacy, achieving a\nperformance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on\nfeature points, our technique exhibits enhanced performance in low-texture\ndegenerative environments compared to state-of-the-art methods.", "AI": {"tldr": "本文提出了一种新型的、非迭代的、解耦的六自由度机器人姿态估计算法，利用重叠平面元素和核互相关器，显著提高了RGB-D视觉里程计的计算效率和在低纹理环境下的性能。", "motivation": "传统的RGB-D视觉里程计（RGBD-VO）依赖于迭代优化求解器进行姿态估计，并涉及特征提取和匹配过程，导致计算负担重和时间延迟大。", "method": "该方法将旋转和平移的估计解耦。首先，利用场景中重叠的平面特性计算旋转矩阵；随后，利用核互相关器（KCC）确定平移。此方法避免了资源密集型迭代优化和特征提取与对齐过程。", "result": "该方法显著提高了计算效率，在低端i5 CPU上实现了71Hz的性能。当RGBD-VO不依赖特征点时，该技术在低纹理退化环境中表现出比现有先进方法更强的性能。", "conclusion": "所提出的解耦、非迭代方法为RGB-D视觉里程计提供了一种高效且在低纹理环境下性能优越的机器人姿态估计方案，有效解决了传统方法的计算效率和鲁棒性问题。"}}
{"id": "2507.18653", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18653", "abs": "https://arxiv.org/abs/2507.18653", "authors": ["Mohammed Abdul Hafeez Khan", "Parth Ganeriwala", "Sarah M. Lehman", "Siddhartha Bhattacharyya", "Amy Alvarez", "Natasha Neogi"], "title": "Adapt, But Don't Forget: Fine-Tuning and Contrastive Routing for Lane Detection under Distribution Shift", "comment": "Accepted to ICCV 2025, 2COOOL Workshop. Total 14 pages, 5 tables, and\n  4 figures", "summary": "Lane detection models are often evaluated in a closed-world setting, where\ntraining and testing occur on the same dataset. We observe that, even within\nthe same domain, cross-dataset distribution shifts can cause severe\ncatastrophic forgetting during fine-tuning. To address this, we first train a\nbase model on a source distribution and then adapt it to each new target\ndistribution by creating separate branches, fine-tuning only selected\ncomponents while keeping the original source branch fixed. Based on a\ncomponent-wise analysis, we identify effective fine-tuning strategies for\ntarget distributions that enable parameter-efficient adaptation. At inference\ntime, we propose using a supervised contrastive learning model to identify the\ninput distribution and dynamically route it to the corresponding branch. Our\nframework achieves near-optimal F1-scores while using significantly fewer\nparameters than training separate models for each distribution.", "AI": {"tldr": "该研究提出了一种针对车道线检测模型跨数据集分布偏移问题的框架，通过分支微调和动态路由，在保持高性能的同时显著减少参数量，避免灾难性遗忘。", "motivation": "车道线检测模型通常在封闭环境中评估，但即使在同一领域内，跨数据集的分布偏移也会导致微调时出现严重的灾难性遗忘。", "method": "首先在源分布上训练一个基础模型，然后为每个新的目标分布创建独立分支进行适应性训练。这些分支只微调选定组件，同时保持原始源分支固定。通过组件级分析确定有效的参数高效微调策略。在推理时，利用监督对比学习模型识别输入分布并将其动态路由到相应的分支。", "result": "该框架实现了接近最优的F1分数，并且与为每个分布单独训练模型相比，使用的参数量显著减少。", "conclusion": "所提出的框架有效解决了车道线检测模型在跨数据集场景下的灾难性遗忘问题，实现了高性能和参数效率的平衡，为模型在多源数据环境下的部署提供了可行方案。"}}
{"id": "2507.18977", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18977", "abs": "https://arxiv.org/abs/2507.18977", "authors": ["Mehrnoosh Mirtaheri", "Ryan A. Rossi", "Sungchul Kim", "Kanak Mahadik", "Tong Yu", "Xiang Chen", "Mohammad Rostami"], "title": "Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling", "comment": null, "summary": "Temporal Knowledge Graph (TKG) completion models traditionally assume access\nto the entire graph during training. This overlooks challenges stemming from\nthe evolving nature of TKGs, such as: (i) the model's requirement to generalize\nand assimilate new knowledge, and (ii) the task of managing new or unseen\nentities that often have sparse connections. In this paper, we present an\nincremental training framework specifically designed for TKGs, aiming to\naddress entities that are either not observed during training or have sparse\nconnections. Our approach combines a model-agnostic enhancement layer with a\nweighted sampling strategy, that can be augmented to and improve any existing\nTKG completion method. The enhancement layer leverages a broader, global\ndefinition of entity similarity, which moves beyond mere local neighborhood\nproximity of GNN-based methods. The weighted sampling strategy employed in\ntraining accentuates edges linked to infrequently occurring entities. We\nevaluate our method on two benchmark datasets, and demonstrate that our\nframework outperforms existing methods in total link prediction, inductive link\nprediction, and in addressing long-tail entities. Notably, our method achieves\na 10\\% improvement and a 15\\% boost in MRR for these datasets. The results\nunderscore the potential of our approach in mitigating catastrophic forgetting\nand enhancing the robustness of TKG completion methods, especially in an\nincremental training context", "AI": {"tldr": "提出了一种针对时间知识图谱（TKG）的增量训练框架，旨在解决新实体和稀疏连接实体的挑战，并通过模型无关的增强层和加权采样策略提升现有TKG补全方法的性能。", "motivation": "传统的TKG补全模型假设训练时可访问整个图，忽略了TKG的演化特性，导致模型难以泛化新知识，并难以处理未见或连接稀疏的新实体。", "method": "该方法是一个增量训练框架，结合了两个关键组件：1) 一个模型无关的增强层，利用更广泛的全局实体相似性定义，超越了基于GNN的局部邻域相似性；2) 一个加权采样策略，在训练中强调与不常出现实体相关的边。该框架可增强和改进任何现有的TKG补全方法。", "result": "在两个基准数据集上，该框架在总链接预测、归纳链接预测和处理长尾实体方面均优于现有方法，并在这些数据集上实现了MRR 10%和15%的提升。", "conclusion": "研究结果表明，该方法在缓解灾难性遗忘和增强TKG补全方法的鲁棒性方面具有巨大潜力，尤其是在增量训练的背景下。"}}
{"id": "2507.19100", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19100", "abs": "https://arxiv.org/abs/2507.19100", "authors": ["Taewon Kang", "Ji-Wook Kwon", "Il Bae", "Jin Hyo Kim"], "title": "Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations", "comment": null, "summary": "Localization of mobile robots is crucial for deploying robots in real-world\napplications such as search and rescue missions. This work aims to develop an\naccurate localization system applicable to swarm robots equipped only with\nlow-cost monocular vision sensors and visual markers. The system is designed to\noperate in fully open spaces, without landmarks or support from positioning\ninfrastructures. To achieve this, we propose a localization method based on\nequilateral triangular formations. By leveraging the geometric properties of\nequilateral triangles, the accurate two-dimensional position of each\nparticipating robot is estimated using one-dimensional lateral distance\ninformation between robots, which can be reliably and accurately obtained with\na low-cost monocular vision sensor. Experimental and simulation results\ndemonstrate that, as travel time increases, the positioning error of the\nproposed method becomes significantly smaller than that of a conventional\ndead-reckoning system, another low-cost localization approach applicable to\nopen environments.", "AI": {"tldr": "本文提出了一种基于等边三角形编队的低成本单目视觉定位方法，用于在开放空间中对配备廉价传感器的群体机器人进行高精度定位，其误差随时间增长显著小于航位推算。", "motivation": "移动机器人的定位对于在搜索救援等实际应用中部署机器人至关重要。特别是在没有地标或定位基础设施的完全开放空间中，为仅配备低成本单目视觉传感器和视觉标记的群体机器人开发一个准确的定位系统是一个挑战。", "method": "提出了一种基于等边三角形编队的定位方法。通过利用等边三角形的几何特性，使用低成本单目视觉传感器可靠且准确地获取机器人之间的一维横向距离信息，从而估计每个参与机器人的精确二维位置。", "result": "实验和仿真结果表明，随着行进时间的增加，所提出方法的定位误差显著小于传统的航位推算系统（另一种适用于开放环境的低成本定位方法）。", "conclusion": "该研究成功开发了一种适用于开放空间中群体机器人的准确定位系统，该系统仅依赖于低成本单目视觉传感器和视觉标记，并在长时间运行中表现出优于传统方法的定位精度。"}}
{"id": "2507.18944", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18944", "abs": "https://arxiv.org/abs/2507.18944", "authors": ["Guanyi Qin", "Ziyue Wang", "Daiyun Shen", "Haofeng Liu", "Hantao Zhou", "Junde Wu", "Runze Hu", "Yueming Jin"], "title": "Structure Matters: Revisiting Boundary Refinement in Video Object Segmentation", "comment": null, "summary": "Given an object mask, Semi-supervised Video Object Segmentation (SVOS)\ntechnique aims to track and segment the object across video frames, serving as\na fundamental task in computer vision. Although recent memory-based methods\ndemonstrate potential, they often struggle with scenes involving occlusion,\nparticularly in handling object interactions and high feature similarity. To\naddress these issues and meet the real-time processing requirements of\ndownstream applications, in this paper, we propose a novel bOundary Amendment\nvideo object Segmentation method with Inherent Structure refinement, hereby\nnamed OASIS. Specifically, a lightweight structure refinement module is\nproposed to enhance segmentation accuracy. With the fusion of rough edge priors\ncaptured by the Canny filter and stored object features, the module can\ngenerate an object-level structure map and refine the representations by\nhighlighting boundary features. Evidential learning for uncertainty estimation\nis introduced to further address challenges in occluded regions. The proposed\nmethod, OASIS, maintains an efficient design, yet extensive experiments on\nchallenging benchmarks demonstrate its superior performance and competitive\ninference speed compared to other state-of-the-art methods, i.e., achieving the\nF values of 91.6 (vs. 89.7 on DAVIS-17 validation set) and G values of 86.6\n(vs. 86.2 on YouTubeVOS 2019 validation set) while maintaining a competitive\nspeed of 48 FPS on DAVIS.", "AI": {"tldr": "本文提出了一种名为OASIS的半监督视频目标分割方法，通过引入边界修正和内在结构细化来解决遮挡和特征相似性问题，并结合证据学习进行不确定性估计，实现了卓越的性能和实时处理速度。", "motivation": "现有基于记忆的半监督视频目标分割（SVOS）方法在处理遮挡场景（特别是目标交互和高特征相似度）时表现不佳。此外，下游应用对实时处理能力有要求。", "method": "本文提出OASIS方法，包含：1. 轻量级结构细化模块，通过融合Canny滤波器捕获的粗略边缘先验和存储的目标特征，生成目标级结构图并细化表示，突出边界特征以提高分割精度。2. 引入证据学习进行不确定性估计，以进一步解决遮挡区域的挑战。", "result": "OASIS在DAVIS-17验证集上取得了91.6的F值（对比89.7），在YouTubeVOS 2019验证集上取得了86.6的G值（对比86.2），同时在DAVIS数据集上保持了48 FPS的竞争性推理速度。", "conclusion": "OASIS方法设计高效，在具有挑战性的基准测试中展现出优越的性能和有竞争力的推理速度，有效解决了遮挡问题，提高了视频目标分割的精度和实时性。"}}
{"id": "2507.18884", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18884", "abs": "https://arxiv.org/abs/2507.18884", "authors": ["Ming Gong", "Xucheng Huang", "Ziheng Xu", "Vijayan K. Asari"], "title": "MindFlow+: A Self-Evolving Agent for E-Commerce Customer Service", "comment": null, "summary": "High-quality dialogue is crucial for e-commerce customer service, yet\ntraditional intent-based systems struggle with dynamic, multi-turn\ninteractions. We present MindFlow+, a self-evolving dialogue agent that learns\ndomain-specific behavior by combining large language models (LLMs) with\nimitation learning and offline reinforcement learning (RL). MindFlow+\nintroduces two data-centric mechanisms to guide learning: tool-augmented\ndemonstration construction, which exposes the model to knowledge-enhanced and\nagentic (ReAct-style) interactions for effective tool use; and\nreward-conditioned data modeling, which aligns responses with task-specific\ngoals using reward signals. To evaluate the model's role in response\ngeneration, we introduce the AI Contribution Ratio, a novel metric quantifying\nAI involvement in dialogue. Experiments on real-world e-commerce conversations\nshow that MindFlow+ outperforms strong baselines in contextual relevance,\nflexibility, and task accuracy. These results demonstrate the potential of\ncombining LLMs tool reasoning, and reward-guided learning to build\ndomain-specialized, context-aware dialogue systems.", "AI": {"tldr": "MindFlow+是一个自进化的对话代理，它结合了大型语言模型（LLMs）、模仿学习和离线强化学习，通过工具增强的演示和奖励条件数据建模，解决了传统意图系统在电商客服多轮对话中的不足，并在实际电商对话中表现出卓越的上下文相关性、灵活性和任务准确性。", "motivation": "电子商务客服需要高质量的对话，但传统的基于意图的系统难以处理动态、多轮的交互。因此，需要一种能学习领域特定行为并有效利用工具的对话代理。", "method": "本文提出了MindFlow+，一个自进化的对话代理，它结合了LLMs、模仿学习和离线强化学习。它引入了两种数据中心机制：1) 工具增强的演示构建，使模型接触知识增强和代理式（ReAct风格）交互，以有效使用工具；2) 奖励条件数据建模，使用奖励信号使响应与任务目标对齐。此外，引入了AI贡献率（AI Contribution Ratio）这一新指标来量化AI在对话中的参与度。", "result": "在真实世界的电商对话实验中，MindFlow+在上下文相关性、灵活性和任务准确性方面均优于强大的基线模型。", "conclusion": "结合LLMs的工具推理能力和奖励引导学习，能够构建领域专业化、上下文感知的对话系统，这展示了该方法的巨大潜力。"}}
{"id": "2507.18947", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18947", "abs": "https://arxiv.org/abs/2507.18947", "authors": ["Asad Ali Shahid", "Angelo Moroncelli", "Drazen Brscic", "Takayuki Kanda", "Loris Roveda"], "title": "GEAR: Gaze-Enabled Human-Robot Collaborative Assembly", "comment": "Accepted for publication at 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "Recent progress in robot autonomy and safety has significantly improved\nhuman-robot interactions, enabling robots to work alongside humans on various\ntasks. However, complex assembly tasks still present significant challenges due\nto inherent task variability and the need for precise operations. This work\nexplores deploying robots in an assistive role for such tasks, where the robot\nassists by fetching parts while the skilled worker provides high-level guidance\nand performs the assembly. We introduce GEAR, a gaze-enabled system designed to\nenhance human-robot collaboration by allowing robots to respond to the user's\ngaze. We evaluate GEAR against a touch-based interface where users interact\nwith the robot through a touchscreen. The experimental study involved 30\nparticipants working on two distinct assembly scenarios of varying complexity.\nResults demonstrated that GEAR enabled participants to accomplish the assembly\nwith reduced physical demand and effort compared to the touchscreen interface,\nespecially for complex tasks, maintaining great performance, and receiving\nobjects effectively. Participants also reported enhanced user experience while\nperforming assembly tasks. Project page: sites.google.com/view/gear-hri", "AI": {"tldr": "本文提出GEAR系统，一种通过凝视实现机器人辅助零件拾取，以降低复杂装配任务中人体负担并提升用户体验的人机协作方案。", "motivation": "尽管机器人自主性和安全性有进步，复杂装配任务仍因其固有的任务变异性和对精确操作的需求而对机器人构成挑战。本研究旨在探索机器人作为辅助角色，通过帮助获取零件来提升人类在此类任务中的表现。", "method": "引入了GEAR（Gaze-Enabled System），一个允许机器人响应用户凝视来增强人机协作的系统。通过与基于触摸屏的界面进行对比评估，实验招募了30名参与者，在两种不同复杂度的装配场景中进行测试。", "result": "实验结果表明，与触摸屏界面相比，GEAR使参与者在完成装配任务时显著降低了体力需求和精力消耗，尤其是在复杂任务中，同时保持了良好的性能和有效的物体接收。参与者还报告了在执行装配任务时用户体验得到了提升。", "conclusion": "凝视驱动的机器人辅助系统（GEAR）在复杂装配任务中表现出色，能有效减轻人类的体力负担和努力程度，并显著改善用户体验，证明了其在人机协作领域的潜力。"}}
{"id": "2507.18656", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18656", "abs": "https://arxiv.org/abs/2507.18656", "authors": ["Muhammad Zaeem Shahzad", "Muhammad Abdullah Hanif", "Bassem Ouni", "Muhammad Shafique"], "title": "ShrinkBox: Backdoor Attack on Object Detection to Disrupt Collision Avoidance in Machine Learning-based Advanced Driver Assistance Systems", "comment": "8 pages, 8 figures, 1 table", "summary": "Advanced Driver Assistance Systems (ADAS) significantly enhance road safety\nby detecting potential collisions and alerting drivers. However, their reliance\non expensive sensor technologies such as LiDAR and radar limits accessibility,\nparticularly in low- and middle-income countries. Machine learning-based ADAS\n(ML-ADAS), leveraging deep neural networks (DNNs) with only standard camera\ninput, offers a cost-effective alternative. Critical to ML-ADAS is the\ncollision avoidance feature, which requires the ability to detect objects and\nestimate their distances accurately. This is achieved with specialized DNNs\nlike YOLO, which provides real-time object detection, and a lightweight,\ndetection-wise distance estimation approach that relies on key features\nextracted from the detections like bounding box dimensions and size. However,\nthe robustness of these systems is undermined by security vulnerabilities in\nobject detectors. In this paper, we introduce ShrinkBox, a novel backdoor\nattack targeting object detection in collision avoidance ML-ADAS. Unlike\nexisting attacks that manipulate object class labels or presence, ShrinkBox\nsubtly shrinks ground truth bounding boxes. This attack remains undetected in\ndataset inspections and standard benchmarks while severely disrupting\ndownstream distance estimation. We demonstrate that ShrinkBox can be realized\nin the YOLOv9m object detector at an Attack Success Rate (ASR) of 96%, with\nonly a 4% poisoning ratio in the training instances of the KITTI dataset.\nFurthermore, given the low error targets introduced in our relaxed poisoning\nstrategy, we find that ShrinkBox increases the Mean Absolute Error (MAE) in\ndownstream distance estimation by more than 3x on poisoned samples, potentially\nresulting in delays or prevention of collision warnings altogether.", "AI": {"tldr": "本文提出了一种名为ShrinkBox的新型后门攻击，专门针对ML-ADAS（基于机器学习的高级驾驶辅助系统）中的目标检测器。该攻击通过微缩真实边界框来规避检测，同时严重破坏下游的距离估计功能，对碰撞避免系统构成潜在威胁。", "motivation": "ADAS系统依赖昂贵的传感器（如LiDAR、雷达），限制了其普及性，尤其是在中低收入国家。基于机器学习的ADAS（ML-ADAS）利用深度神经网络和标准摄像头，提供了一种经济高效的替代方案。然而，ML-ADAS中的碰撞避免功能（依赖于目标检测和距离估计）面临安全漏洞，特别是目标检测器的脆弱性。", "method": "研究者引入了ShrinkBox，一种新颖的后门攻击，目标是ML-ADAS中碰撞避免功能的目标检测部分。与现有攻击不同，ShrinkBox通过微妙地缩小训练数据中的真实边界框来实现攻击。他们在YOLOv9m目标检测器上，使用KITTI数据集，以4%的投毒比例和宽松的投毒策略来演示其有效性。", "result": "ShrinkBox攻击在YOLOv9m检测器上实现了96%的攻击成功率（ASR），且仅需4%的训练数据投毒比例。该攻击在数据集检查和标准基准测试中保持未被检测到。在受攻击的样本上，它使下游距离估计的平均绝对误差（MAE）增加了3倍以上，可能导致碰撞警告延迟或完全失效。", "conclusion": "ShrinkBox是一种隐蔽且高效的后门攻击，它通过操纵目标检测器的边界框，严重损害了ML-ADAS中关键的碰撞避免功能。这揭示了ML-ADAS系统在安全方面存在新的、未被充分研究的漏洞，需要进一步关注和防御措施。"}}
{"id": "2507.19089", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19089", "abs": "https://arxiv.org/abs/2507.19089", "authors": ["Shuhao Li", "Weidong Yang", "Yue Cui", "Xiaoxing Liu", "Lingkai Meng", "Lipeng Ma", "Fan Zhang"], "title": "Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation", "comment": null, "summary": "Fine-grained traffic management and prediction are fundamental to key\napplications such as autonomous driving, lane change guidance, and traffic\nsignal control. However, obtaining lane-level traffic data has become a\ncritical bottleneck for data-driven models due to limitations in the types and\nnumber of sensors and issues with the accuracy of tracking algorithms. To\naddress this, we propose the Fine-grained Road Traffic Inference (FRTI) task,\nwhich aims to generate more detailed lane-level traffic information using\nlimited road data, providing a more energy-efficient and cost-effective\nsolution for precise traffic management. This task is abstracted as the first\nscene of the spatio-temporal graph node generation problem. We designed a\ntwo-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.\nThis framework leverages the Road-Lane Correlation Autoencoder-Decoder and the\nLane Diffusion Module to fully utilize the limited spatio-temporal dependencies\nand distribution relationships of road data to accurately infer fine-grained\nlane traffic states. Based on existing research, we designed several baseline\nmodels with the potential to solve the FRTI task and conducted extensive\nexperiments on six datasets representing different road conditions to validate\nthe effectiveness of the RoadDiff model in addressing the FRTI task. The\nrelevant datasets and code are available at\nhttps://github.com/ShuhaoLii/RoadDiff.", "AI": {"tldr": "针对车道级交通数据获取困难的问题，本文提出了细粒度道路交通推断（FRTI）任务，并设计了一个名为RoadDiff的两阶段框架，利用有限的道路数据准确推断车道级交通状态。", "motivation": "自动驾驶、变道引导和交通信号控制等关键应用需要细粒度的交通管理和预测，但由于传感器类型、数量限制以及跟踪算法精度问题，获取车道级交通数据已成为数据驱动模型的关键瓶颈。", "method": "提出了细粒度道路交通推断（FRTI）任务，将其抽象为时空图节点生成问题。设计了一个名为RoadDiff的两阶段框架来解决FRTI任务，该框架包含路-车道关联自编码器-解码器（Road-Lane Correlation Autoencoder-Decoder）和车道扩散模块（Lane Diffusion Module），以充分利用有限的时空依赖和分布关系来推断交通状态。", "result": "通过在代表不同道路条件的六个数据集上进行广泛实验，验证了RoadDiff模型在解决FRTI任务方面的有效性，并优于现有的基线模型。", "conclusion": "FRTI任务和RoadDiff模型为精确交通管理提供了一种更节能、更具成本效益的解决方案，能够利用有限的道路数据生成更详细的车道级交通信息。"}}
{"id": "2507.19458", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.19458", "abs": "https://arxiv.org/abs/2507.19458", "authors": ["Amir Fard", "Arnold X. -X. Yuan"], "title": "Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints", "comment": null, "summary": "Budget planning and maintenance optimization are crucial for infrastructure\nasset management, ensuring cost-effectiveness and sustainability. However, the\ncomplexity arising from combinatorial action spaces, diverse asset\ndeterioration, stringent budget constraints, and environmental uncertainty\nsignificantly limits existing methods' scalability. This paper proposes a\nHierarchical Deep Reinforcement Learning methodology specifically tailored to\nmulti-year infrastructure planning. Our approach decomposes the problem into\ntwo hierarchical levels: a high-level Budget Planner allocating annual budgets\nwithin explicit feasibility bounds, and a low-level Maintenance Planner\nprioritizing assets within the allocated budget. By structurally separating\nmacro-budget decisions from asset-level prioritization and integrating linear\nprogramming projection within a hierarchical Soft Actor-Critic framework, the\nmethod efficiently addresses exponential growth in the action space and ensures\nrigorous budget compliance. A case study evaluating sewer networks of varying\nsizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed\napproach. Compared to conventional Deep Q-Learning and enhanced genetic\nalgorithms, our methodology converges more rapidly, scales effectively, and\nconsistently delivers near-optimal solutions even as network size grows.", "AI": {"tldr": "本文提出了一种分层深度强化学习（H-DRL）方法，用于解决多年度基础设施规划中的预算和维护优化问题，有效应对了复杂性和可扩展性挑战。", "motivation": "基础设施资产管理中的预算规划和维护优化至关重要，但现有方法在面对组合行动空间、资产劣化多样性、严格预算限制和环境不确定性时，其可扩展性受到显著限制。", "method": "该方法将问题分解为两个层次：高层预算规划器分配年度预算，低层维护规划器在预算内对资产进行优先级排序。通过结构性分离宏观预算决策和资产级别优先级，并在线性规划投影与分层Soft Actor-Critic框架中集成，有效处理了行动空间指数增长并确保了预算合规性。", "result": "在不同规模（10、15、20个下水道流域）的污水管网案例研究中，该方法与传统深度Q学习和增强遗传算法相比，收敛更快、扩展性更强，并且即使网络规模增大，也能持续提供接近最优的解决方案。", "conclusion": "所提出的分层深度强化学习方法能有效、可扩展地解决多年度基础设施规划问题，在复杂性和预算合规性方面表现优异，并能提供近乎最优的解决方案。"}}
{"id": "2507.18890", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18890", "abs": "https://arxiv.org/abs/2507.18890", "authors": ["Jonathan Ivey", "Susan Gauch", "David Jurgens"], "title": "NUTMEG: Separating Signal From Noise in Annotator Disagreement", "comment": null, "summary": "NLP models often rely on human-labeled data for training and evaluation. Many\napproaches crowdsource this data from a large number of annotators with varying\nskills, backgrounds, and motivations, resulting in conflicting annotations.\nThese conflicts have traditionally been resolved by aggregation methods that\nassume disagreements are errors. Recent work has argued that for many tasks\nannotators may have genuine disagreements and that variation should be treated\nas signal rather than noise. However, few models separate signal and noise in\nannotator disagreement. In this work, we introduce NUTMEG, a new Bayesian model\nthat incorporates information about annotator backgrounds to remove noisy\nannotations from human-labeled training data while preserving systematic\ndisagreements. Using synthetic data, we show that NUTMEG is more effective at\nrecovering ground-truth from annotations with systematic disagreement than\ntraditional aggregation methods. We provide further analysis characterizing how\ndifferences in subpopulation sizes, rates of disagreement, and rates of spam\naffect the performance of our model. Finally, we demonstrate that downstream\nmodels trained on NUTMEG-aggregated data significantly outperform models\ntrained on data from traditionally aggregation methods. Our results highlight\nthe importance of accounting for both annotator competence and systematic\ndisagreements when training on human-labeled data.", "AI": {"tldr": "本文提出NUTMEG，一个贝叶斯模型，通过整合标注者背景信息，在处理人类标注数据时，能有效区分并去除噪声标注，同时保留系统性分歧，从而提高下游模型的性能。", "motivation": "NLP模型依赖人工标注数据，但标注者技能、背景和动机差异导致标注冲突。传统方法将所有分歧视为错误，而近期研究认为部分分歧是真实信号。目前鲜有模型能有效区分标注者分歧中的信号与噪声。", "method": "引入NUTMEG，一种新的贝叶斯模型。该模型整合标注者背景信息，旨在从人工标注训练数据中去除噪声标注，同时保留系统性分歧。", "result": "在合成数据上，NUTMEG在存在系统性分歧的情况下，比传统聚合方法更能有效恢复真实数据。研究还分析了子群体大小、分歧率和垃圾标注率对模型性能的影响。下游模型在NUTMEG聚合数据上训练后，显著优于在传统方法聚合数据上训练的模型。", "conclusion": "研究结果强调了在处理人工标注数据进行模型训练时，同时考虑标注者能力和系统性分歧的重要性。"}}
{"id": "2507.18979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18979", "abs": "https://arxiv.org/abs/2507.18979", "authors": ["Deokjin Lee", "Junho Song", "Alireza Karimi", "Sehoon Oh"], "title": "Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots", "comment": null, "summary": "Motion control of flexible joint robots (FJR) is challenged by inherent\nflexibility and configuration-dependent variations in system dynamics. While\ndisturbance observers (DOB) can enhance system robustness, their performance is\noften limited by the elasticity of the joints and the variations in system\nparameters, which leads to a conservative design of the DOB. This paper\npresents a novel frequency response function (FRF)-based optimization method\naimed at improving DOB performance, even in the presence of flexibility and\nsystem variability. The proposed method maximizes control bandwidth and\neffectively suppresses vibrations, thus enhancing overall system performance.\nClosed-loop stability is rigorously proven using the Nyquist stability\ncriterion. Experimental validation on a FJR demonstrates that the proposed\napproach significantly improves robustness and motion performance, even under\nconditions of joint flexibility and system variation.", "AI": {"tldr": "本文提出一种基于频率响应函数(FRF)的优化方法，用于改进柔性关节机器人(FJR)的扰动观测器(DOB)性能，以应对关节柔性和系统动力学变化。", "motivation": "柔性关节机器人的运动控制面临固有柔性和构型依赖的系统动力学变化挑战。扰动观测器虽能增强鲁棒性，但其性能常受关节弹性和参数变化限制，导致设计保守。", "method": "提出了一种新颖的基于FRF的优化方法，旨在提高DOB性能。该方法通过最大化控制带宽和有效抑制振动来增强系统性能。闭环稳定性通过奈奎斯特稳定性判据严格证明。", "result": "在柔性关节机器人上的实验验证表明，即使在关节柔性和系统变化条件下，所提出的方法也能显著提高鲁棒性和运动性能。", "conclusion": "所提出的基于FRF的优化方法能够有效改善柔性关节机器人的扰动观测器性能，克服关节柔性和系统变化带来的挑战，从而显著提升系统的鲁棒性和运动表现。"}}
{"id": "2507.18657", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18657", "abs": "https://arxiv.org/abs/2507.18657", "authors": ["Zehui Zhao", "Laith Alzubaidi", "Haider A. Alwzwazy", "Jinglan Zhang", "Yuantong Gu"], "title": "VGS-ATD: Robust Distributed Learning for Multi-Label Medical Image Classification Under Heterogeneous and Imbalanced Conditions", "comment": "15 pages, 8 figures, 6 tables", "summary": "In recent years, advanced deep learning architectures have shown strong\nperformance in medical imaging tasks. However, the traditional centralized\nlearning paradigm poses serious privacy risks as all data is collected and\ntrained on a single server. To mitigate this challenge, decentralized\napproaches such as federated learning and swarm learning have emerged, allowing\nmodel training on local nodes while sharing only model weights. While these\nmethods enhance privacy, they struggle with heterogeneous and imbalanced data\nand suffer from inefficiencies due to frequent communication and the\naggregation of weights. More critically, the dynamic and complex nature of\nclinical environments demands scalable AI systems capable of continuously\nlearning from diverse modalities and multilabels. Yet, both centralized and\ndecentralized models are prone to catastrophic forgetting during system\nexpansion, often requiring full model retraining to incorporate new data. To\naddress these limitations, we propose VGS-ATD, a novel distributed learning\nframework. To validate VGS-ATD, we evaluate it in experiments spanning 30\ndatasets and 80 independent labels across distributed nodes, VGS-ATD achieved\nan overall accuracy of 92.7%, outperforming centralized learning (84.9%) and\nswarm learning (72.99%), while federated learning failed under these conditions\ndue to high requirements on computational resources. VGS-ATD also demonstrated\nstrong scalability, with only a 1% drop in accuracy on existing nodes after\nexpansion, compared to a 20% drop in centralized learning, highlighting its\nresilience to catastrophic forgetting. Additionally, it reduced computational\ncosts by up to 50% relative to both centralized and swarm learning, confirming\nits superior efficiency and scalability.", "AI": {"tldr": "本文提出VGS-ATD，一种新型分布式学习框架，用于解决医疗影像中集中式和现有去中心化学习（如联邦学习和群学习）在隐私、数据异构性、通信效率、可扩展性和灾难性遗忘方面的局限性。VGS-ATD在多数据集和多标签实验中表现出更高的准确性、更强的可扩展性（抵抗灾难性遗忘）和更低的计算成本。", "motivation": "传统的集中式学习在医疗影像中存在严重隐私风险。联邦学习和群学习等去中心化方法虽然增强了隐私，但难以处理异构和不平衡数据，且因频繁通信和权重聚合效率低下。更关键的是，临床环境要求可扩展的AI系统能持续从多模态和多标签数据中学习，而现有集中式和去中心化模型在系统扩展时容易遭受灾难性遗忘，常需完全重新训练。", "method": "提出VGS-ATD，一种新型分布式学习框架，旨在解决现有方法的局限性。", "result": "在30个数据集和80个独立标签的分布式节点实验中，VGS-ATD的总准确率达到92.7%，优于集中式学习（84.9%）和群学习（72.99%），而联邦学习因高计算资源要求而失败。VGS-ATD还展现出强大的可扩展性，扩展后现有节点准确率仅下降1%，而集中式学习下降20%，表明其对灾难性遗忘的抵抗力。此外，相对于集中式和群学习，VGS-ATD计算成本降低高达50%。", "conclusion": "VGS-ATD是一种高效、可扩展且对灾难性遗忘具有鲁棒性的分布式学习框架，在医疗影像任务中显著优于现有的集中式和去中心化学习方法，解决了隐私、数据异构性、通信效率和可扩展性等关键挑战。"}}
{"id": "2507.19109", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.19109", "abs": "https://arxiv.org/abs/2507.19109", "authors": ["Noé Lallouet", "Tristan Cazenave", "Cyrille Enderli"], "title": "Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization", "comment": "Preprint ; accepted to ECAI 2025", "summary": "We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for\nmulti-objective optimization problems over discrete search spaces. Extending\nthe Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for\nsingle-objective problems, Pareto-NRPA generalizes the nested search and policy\nupdate mechanism to multi-objective optimization. The algorithm uses a set of\npolicies to concurrently explore different regions of the solution space and\nmaintains non-dominated fronts at each level of search. Policy adaptation is\nperformed with respect to the diversity and isolation of sequences within the\nPareto front. We benchmark Pareto-NRPA on two classes of problems: a novel\nbi-objective variant of the Traveling Salesman Problem with Time Windows\nproblem (MO-TSPTW), and a neural architecture search task on well-known\nbenchmarks. Results demonstrate that Pareto-NRPA achieves competitive\nperformance against state-of-the-art multi-objective algorithms, both in terms\nof convergence and diversity of solutions. Particularly, Pareto-NRPA strongly\noutperforms state-of-the-art evolutionary multi-objective algorithms on\nconstrained search spaces. To our knowledge, this work constitutes the first\nadaptation of NRPA to the multi-objective setting.", "AI": {"tldr": "Pareto-NRPA是一种新的蒙特卡洛算法，用于离散搜索空间中的多目标优化问题，它将单目标NRPA算法推广到多目标设置，并在收敛性和解的多样性方面表现出竞争力。", "motivation": "现有算法在离散搜索空间多目标优化问题上的局限性，以及将单目标NRPA算法扩展到多目标设置的需求，尤其是处理复杂约束的离散搜索空间。", "method": "Pareto-NRPA通过泛化嵌套搜索和策略更新机制来处理多目标优化。它使用一组策略并行探索解空间，在每个搜索级别维护非支配前沿，并根据帕累托前沿内序列的多样性和隔离性进行策略适应。", "result": "在多目标旅行商问题（MO-TSPTW）和神经架构搜索任务上，Pareto-NRPA在收敛性和解的多样性方面均优于或与最先进的多目标算法竞争，尤其在约束搜索空间上表现显著优于现有进化算法。", "conclusion": "Pareto-NRPA是NRPA算法首次在多目标环境下的成功适应，它在离散多目标优化问题上展现出强大的性能，尤其在处理复杂约束时表现出色。"}}
{"id": "2507.18901", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18901", "abs": "https://arxiv.org/abs/2507.18901", "authors": ["Chuxuan Hu", "Liyun Zhang", "Yeji Lim", "Aum Wadhwani", "Austin Peters", "Daniel Kang"], "title": "REPRO-Bench: Can Agentic AI Systems Assess the Reproducibility of Social Science Research?", "comment": "Accepted to ACL 2025 Findings", "summary": "Assessing the reproducibility of social science papers is essential for\npromoting rigor in research processes, but manual assessment is costly. With\nrecent advances in agentic AI systems (i.e., AI agents), we seek to evaluate\ntheir capability to automate this process. However, existing benchmarks for\nreproducing research papers (1) focus solely on reproducing results using\nprovided code and data without assessing their consistency with the paper, (2)\noversimplify real-world scenarios, and (3) lack necessary diversity in data\nformats and programming languages. To address these issues, we introduce\nREPRO-Bench, a collection of 112 task instances, each representing a social\nscience paper with a publicly available reproduction report. The agents are\ntasked with assessing the reproducibility of the paper based on the original\npaper PDF and the corresponding reproduction package. REPRO-Bench features\nend-to-end evaluation tasks on the reproducibility of social science papers\nwith complexity comparable to real-world assessments. We evaluate three\nrepresentative AI agents on REPRO-Bench, with the best-performing agent\nachieving an accuracy of only 21.4%. Building on our empirical analysis, we\ndevelop REPRO-Agent, which improves the highest accuracy achieved by existing\nagents by 71%. We conclude that more advanced AI agents should be developed to\nautomate real-world reproducibility assessment. REPRO-Bench is publicly\navailable at https://github.com/uiuc-kang-lab/REPRO-Bench.", "AI": {"tldr": "本研究旨在评估AI智能体自动化社会科学论文可复现性评估的能力。为此，我们引入了REPRO-Bench基准测试集以解决现有评估方法的不足，并开发了REPRO-Agent，结果表明AI智能体在该任务上仍有显著提升空间。", "motivation": "评估社会科学论文的可复现性对于提升研究严谨性至关重要，但手动评估成本高昂。现有的论文复现基准测试存在局限性，包括：仅关注使用提供代码和数据复现结果而不评估其与论文的一致性、过度简化真实场景以及缺乏数据格式和编程语言的多样性。", "method": "为解决现有问题，我们推出了REPRO-Bench，一个包含112个任务实例的集合，每个实例代表一篇带有公开复现报告的社会科学论文。智能体被要求根据原始论文PDF和相应的复现包评估论文的可复现性。我们评估了三种代表性AI智能体在REPRO-Bench上的表现，并在此基础上开发了REPRO-Agent。", "result": "在REPRO-Bench上，表现最佳的现有AI智能体仅取得了21.4%的准确率。我们开发的REPRO-Agent将现有智能体的最高准确率提升了71%。", "conclusion": "当前AI智能体在自动化真实世界可复现性评估方面的能力有限，需要开发更先进的AI智能体来完成这项任务。"}}
{"id": "2507.19079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19079", "abs": "https://arxiv.org/abs/2507.19079", "authors": ["Feng Zhu", "Zihang Zhang", "Kangcheng Teng", "Abduhelil Yakup", "Xiaohong Zhang"], "title": "SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research", "comment": null, "summary": "High-precision navigation and positioning systems are critical for\napplications in autonomous vehicles and mobile mapping, where robust and\ncontinuous localization is essential. To test and enhance the performance of\nalgorithms, some research institutions and companies have successively\nconstructed and publicly released datasets. However, existing datasets still\nsuffer from limitations in sensor diversity and environmental coverage. To\naddress these shortcomings and advance development in related fields, the\nSmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset\nhas been developed. This dataset integrates data from multiple sensors,\nincluding Global Navigation Satellite Systems (GNSS), Inertial Measurement\nUnits (IMU), optical cameras, and LiDAR, to provide a rich and versatile\nresource for research in multi-sensor fusion and high-precision navigation. The\ndataset construction process is thoroughly documented, encompassing sensor\nconfigurations, coordinate system definitions, and calibration procedures for\nboth cameras and LiDAR. A standardized framework for data collection and\nprocessing ensures consistency and scalability, enabling large-scale analysis.\nValidation using state-of-the-art Simultaneous Localization and Mapping (SLAM)\nalgorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's\napplicability for advanced navigation research. Covering a wide range of\nreal-world scenarios, including urban areas, campuses, tunnels, and suburban\nenvironments, the dataset offers a valuable tool for advancing navigation\ntechnologies and addressing challenges in complex environments. By providing a\npublicly accessible, high-quality dataset, this work aims to bridge gaps in\nsensor diversity, data accessibility, and environmental representation,\nfostering further innovation in the field.", "AI": {"tldr": "开发了一个名为SmartPNT的多源集成导航、定位和姿态数据集，整合了GNSS、IMU、相机和LiDAR数据，旨在弥补现有数据集在传感器多样性和环境覆盖方面的不足，并支持高精度导航和多传感器融合研究。", "motivation": "现有高精度导航和定位数据集在传感器多样性及环境覆盖范围上存在局限性，无法充分满足自动驾驶和移动测绘领域对鲁棒、连续定位算法测试和增强的需求。", "method": "开发了SmartPNT多源集成导航、定位和姿态数据集，整合了GNSS、IMU、光学相机和LiDAR等多种传感器数据。数据集的构建过程详细记录了传感器配置、坐标系定义以及相机和LiDAR的标定程序。采用标准化框架进行数据收集和处理，确保一致性和可扩展性。", "result": "SmartPNT数据集通过使用VINS-Mono和LIO-SAM等先进SLAM算法进行了验证，证明其适用于高级导航研究。数据集覆盖了城市、校园、隧道和郊区等多种真实世界场景，为解决复杂环境中的导航挑战提供了有价值的工具。", "conclusion": "通过提供一个公开可访问、高质量的多源数据集，该工作旨在弥补现有数据集在传感器多样性、数据可访问性和环境代表性方面的空白，从而促进相关领域的技术创新和发展。"}}
{"id": "2507.18660", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18660", "abs": "https://arxiv.org/abs/2507.18660", "authors": ["Adilet Yerkin", "Ayan Igali", "Elnara Kadyrgali", "Maksat Shagyrov", "Malika Ziyada", "Muragul Muratbekova", "Pakizar Shamoi"], "title": "Fuzzy Theory in Computer Vision: A Review", "comment": "Submitted to Journal of Intelligent and Fuzzy Systems for\n  consideration (8 pages, 6 figures, 1 table)", "summary": "Computer vision applications are omnipresent nowadays. The current paper\nexplores the use of fuzzy logic in computer vision, stressing its role in\nhandling uncertainty, noise, and imprecision in image data. Fuzzy logic is able\nto model gradual transitions and human-like reasoning and provides a promising\napproach to computer vision. Fuzzy approaches offer a way to improve object\nrecognition, image segmentation, and feature extraction by providing more\nadaptable and interpretable solutions compared to traditional methods. We\ndiscuss key fuzzy techniques, including fuzzy clustering, fuzzy inference\nsystems, type-2 fuzzy sets, and fuzzy rule-based decision-making. The paper\nalso discusses various applications, including medical imaging, autonomous\nsystems, and industrial inspection. Additionally, we explore the integration of\nfuzzy logic with deep learning models such as convolutional neural networks\n(CNNs) to enhance performance in complex vision tasks. Finally, we examine\nemerging trends such as hybrid fuzzy-deep learning models and explainable AI.", "AI": {"tldr": "本文探讨了模糊逻辑在计算机视觉中的应用，强调其在处理不确定性、噪声和不精确数据方面的作用，并讨论了其与深度学习的结合。", "motivation": "计算机视觉应用无处不在，但图像数据常伴有不确定性、噪声和不精确性。传统方法难以有效处理这些问题，而模糊逻辑能够建模渐变和类人推理，为计算机视觉提供了一种有前景的解决方案。", "method": "本文讨论了关键的模糊技术，包括模糊聚类、模糊推理系统、2型模糊集和基于模糊规则的决策。此外，还探讨了模糊逻辑与卷积神经网络（CNNs）等深度学习模型的集成，以及混合模糊-深度学习模型和可解释AI等新兴趋势。", "result": "模糊方法能通过提供更具适应性和可解释性的解决方案，改进目标识别、图像分割和特征提取。模糊逻辑与深度学习模型的结合可以增强复杂视觉任务的性能。", "conclusion": "模糊逻辑在计算机视觉领域具有巨大潜力，尤其是在处理不确定性、噪声和不精确性方面。它与深度学习的集成有望进一步提升性能，并促进可解释AI的发展，为医疗成像、自主系统和工业检测等应用带来益处。"}}
{"id": "2507.19132", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19132", "abs": "https://arxiv.org/abs/2507.19132", "authors": ["Xuetian Chen", "Yinghao Chen", "Xinfeng Yuan", "Zhuo Peng", "Lu Chen", "Yuekeng Li", "Zhoujia Zhang", "Yingqian Huang", "Leyan Huang", "Jiaqing Liang", "Tianbao Xie", "Zhiyong Wu", "Qiushi Sun", "Biqing Qi", "Bowen Zhou"], "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?", "comment": "Work in progress", "summary": "Computer-using agents have shown strong potential to boost human productivity\nand enable new application forms across platforms. While recent advances have\nled to usable applications, existing benchmarks fail to account for the\ninternal task heterogeneity and the corresponding agent capabilities, as well\nas their alignment with actual user demands-hindering both targeted capability\ndevelopment and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily\ncomputer-using automation that organizes its 416 realistic tasks across 15\napplications along two key dimensions: a five-level taxonomy of automation and\na generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with\nreal-world scenarios, OS-MAP evaluates agents along two dimensions: automation\nlevel across a five-level taxonomy, and generalization scope across a demand\nhierarchy. This design captures varying levels of required agent autonomy and\ngeneralization, forming a performance-generalization evaluation matrix for\nstructured and comprehensive assessment. Experiments show that even\nState-of-the-Art agents with VLM backbones struggle with higher-level tasks\ninvolving perception, reasoning, and coordination-highlighting the need for a\ndeeper understanding of current strengths and limitations to drive the future\nprogress in computer-using agents research and deployment. All code,\nenvironments, baselines, and data are publicly available at\nhttps://github.com/OS-Copilot/OS-Map.", "AI": {"tldr": "本文提出了OS-MAP，一个用于评估日常计算机操作自动化代理的新基准，并揭示了现有SOTA代理在处理复杂高级任务时的不足。", "motivation": "尽管计算机操作代理取得了进展，但现有基准未能有效捕捉任务的内部异质性、代理能力及其与实际用户需求的对齐，从而阻碍了有针对性的能力发展和研究成果向实际部署的转化。", "method": "本文提出了OS-MAP基准，它组织了15个应用程序中的416个真实任务，并沿两个关键维度进行评估：一个五级自动化分类法和一个源自真实用户需求层级的泛化范围。这种设计形成了性能-泛化评估矩阵，以实现结构化和全面的评估。", "result": "实验结果表明，即使是采用VLM骨干的最新（SOTA）代理，在涉及感知、推理和协调的更高级任务上仍然表现不佳。", "conclusion": "当前计算机操作代理在处理复杂高级任务时存在显著局限性，这突显了需要更深入地理解其当前优势和局限性，以推动计算机操作代理研究和部署的未来进展。"}}
{"id": "2507.18902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18902", "abs": "https://arxiv.org/abs/2507.18902", "authors": ["Hongyuan Lu", "Zixuan Li", "Zefan Zhang", "Wai Lam"], "title": "SLoW: Select Low-frequency Words! Automatic Dictionary Selection for Translation on Large Language Models", "comment": null, "summary": "There are more than 7,000 languages around the world, and current Large\nLanguage Models (LLMs) only support hundreds of languages. Dictionary-based\nprompting methods can enhance translation on them, but most methods use all the\navailable dictionaries, which could be expensive. Instead, it will be flexible\nto have a trade-off between token consumption and translation performance. This\npaper proposes a novel task called \\textbf{A}utomatic \\textbf{D}ictionary\n\\textbf{S}election (\\textbf{ADS}). The goal of the task is to automatically\nselect which dictionary to use to enhance translation. We propose a novel and\neffective method which we call \\textbf{S}elect \\textbf{Lo}w-frequency\n\\textbf{W}ords! (\\textbf{SLoW}) which selects those dictionaries that have a\nlower frequency. Our methods have unique advantages. First, there is no need\nfor access to the training data for frequency estimation (which is usually\nunavailable). Second, it inherits the advantage of dictionary-based methods,\nwhere no additional tuning is required on LLMs. Experimental results on 100\nlanguages from FLORES indicate that SLoW surpasses strong baselines, and it can\nobviously save token usage, with many languages even surpassing the translation\nperformance of the full dictionary baseline.\\footnote{A shocking fact is that\nthere is no need to use the actual training data (often unobtainable) for\nfrequency estimation, and an estimation frequency obtained using public\nresources is still apparently effective in improving translation with ChatGPT\nand Llama, and DeepSeek.}\\footnote{Code and data available upon publication.}", "AI": {"tldr": "本文提出了一种名为自动词典选择（ADS）的新任务，并提出SLoW方法，通过选择低频词对应的词典来优化LLM翻译，在节省token的同时提升翻译性能，且无需访问训练数据或对LLM进行额外微调。", "motivation": "现有大型语言模型（LLMs）仅支持数百种语言，而全球有7000多种语言。基于词典的提示方法可以增强翻译，但多数方法使用所有可用词典，导致成本高昂。因此，需要一种在token消耗和翻译性能之间进行权衡的灵活方法。", "method": "提出自动词典选择（ADS）任务，目标是自动选择用于增强翻译的词典。提出SLoW（Select Low-frequency Words!）方法，该方法选择包含低频词的词典。其独特优势在于：1) 无需访问训练数据进行频率估计；2) 继承了词典方法的优势，无需对LLMs进行额外微调。", "result": "在FLORES数据集上的100种语言实验结果表明，SLoW方法超越了强基线，显著节省了token使用，并且对于许多语言，其翻译性能甚至超越了使用完整词典的基线。通过公共资源估计的频率在提高ChatGPT、Llama和DeepSeek的翻译效果方面仍然有效。", "conclusion": "SLoW是一种新颖且有效的方法，用于自动选择词典以增强LLM的翻译能力。它在不访问训练数据和不微调LLM的情况下，成功实现了token消耗和翻译性能之间的优化平衡，并在多语言翻译任务中表现出色。"}}
{"id": "2507.19082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19082", "abs": "https://arxiv.org/abs/2507.19082", "authors": ["Rachel Ringe", "Leandra Thiele", "Mihai Pomarlan", "Nima Zargham", "Robin Nolte", "Lars Hurrelbrink", "Rainer Malaka"], "title": "Bot Appétit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "This study explores which factors of the visual design of a robot may\ninfluence how humans would place it in a collaborative cooking scenario and how\nthese features may influence task delegation. Human participants were placed in\na Virtual Reality (VR) environment and asked to set up a kitchen for cooking\nalongside a robot companion while considering the robot's morphology. We\ncollected multimodal data for the arrangements created by the participants,\ntranscripts of their think-aloud as they were performing the task, and\ntranscripts of their answers to structured post-task questionnaires. Based on\nanalyzing this data, we formulate several hypotheses: humans prefer to\ncollaborate with biomorphic robots; human beliefs about the sensory\ncapabilities of robots are less influenced by the morphology of the robot than\nbeliefs about action capabilities; and humans will implement fewer avoidance\nstrategies when sharing space with gracile robots. We intend to verify these\nhypotheses in follow-up studies.", "AI": {"tldr": "本研究探讨了机器人的视觉设计因素如何影响人类在协作烹饪场景中放置机器人以及任务分配，并提出了相关假设。", "motivation": "研究旨在理解机器人的视觉设计，特别是其形态，如何影响人类在共享空间中的放置决策和任务委派，以优化人机协作。", "method": "参与者在虚拟现实（VR）环境中为与机器人伙伴协作烹饪设置厨房，同时考虑机器人形态。研究收集了参与者创建的布局多模态数据、思考过程的同步记录以及任务后问卷的回答。", "result": "基于数据分析，研究提出了几个假设：人类倾向于与仿生机器人协作；机器人形态对人类对其感知能力的信念影响小于对其行动能力的信念；与纤细型机器人共享空间时，人类会采取较少的规避策略。", "conclusion": "研究提出了关于机器人视觉设计影响人机协作的初步假设，这些假设将在后续研究中进一步验证。"}}
{"id": "2507.18661", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18661", "abs": "https://arxiv.org/abs/2507.18661", "authors": ["Ruixing Zhang", "Yang Zhang", "Tongyu Zhu", "Leilei Sun", "Weifeng Lv"], "title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by Reinforcement Learning from Visual Map Feed Back", "comment": null, "summary": "Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches.", "AI": {"tldr": "该研究提出一种利用视觉-语言模型（VLM）进行下一地点预测的方法，通过将道路网络和轨迹渲染成图像，并结合监督微调和强化学习，使模型能像人类一样基于地图进行推理，达到最先进的预测性能和跨城市泛化能力。", "motivation": "现有的下一地点预测模型无法像人类一样通过可视化地图并基于道路连通性和移动趋势进行推理。最近视觉-语言模型（VLM）在视觉感知和推理方面的强大能力，为模型以更接近人类的方式进行轨迹推理提供了新途径。", "method": "首先提出Vision-Guided Location Search (VGLS) 方法，评估通用VLM在不修改内部参数的情况下进行轨迹推理的能力。在此基础上，提出VLMLocPredictor：第一阶段，设计两种监督微调（SFT）任务，帮助VLM理解道路网络和轨迹结构，并获得基本的视觉输入推理能力；第二阶段，引入基于视觉地图反馈的强化学习，使模型通过与环境交互自我提升下一地点预测能力。", "result": "在来自四个不同城市的数据集上进行的实验表明，该方法实现了最先进（SOTA）的性能，并且与其他基于LLM的方法相比，展现出卓越的跨城市泛化能力。", "conclusion": "通过将道路网络和轨迹渲染为图像并利用VLM的视觉推理能力，结合监督微调和强化学习，可以有效实现像人类一样基于地图的下一地点预测，并取得了领先的性能和泛化表现。"}}
{"id": "2507.19172", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19172", "abs": "https://arxiv.org/abs/2507.19172", "authors": ["Jiyao Wang", "Xiao Yang", "Qingyong Hu", "Jiankai Tang", "Can Liu", "Dengbo He", "Yuntao Wang", "Yingcong Chen", "Kaishun Wu"], "title": "PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring", "comment": "It is the initial version, not the final version", "summary": "Robust and unobtrusive in-vehicle physiological monitoring is crucial for\nensuring driving safety and user experience. While remote physiological\nmeasurement (RPM) offers a promising non-invasive solution, its translation to\nreal-world driving scenarios is critically constrained by the scarcity of\ncomprehensive datasets. Existing resources are often limited in scale, modality\ndiversity, the breadth of biometric annotations, and the range of captured\nconditions, thereby omitting inherent real-world challenges in driving. Here,\nwe present PhysDrive, the first large-scale multimodal dataset for contactless\nin-vehicle physiological sensing with dedicated consideration on various\nmodality settings and driving factors. PhysDrive collects data from 48 drivers,\nincluding synchronized RGB, near-infrared camera, and raw mmWave radar data,\naccompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,\nand SpO2). It covers a wide spectrum of naturalistic driving conditions,\nincluding driver motions, dynamic natural light, vehicle types, and road\nconditions. We extensively evaluate both signal-processing and deep-learning\nmethods on PhysDrive, establishing a comprehensive benchmark across all\nmodalities, and release full open-source code with compatibility for mainstream\npublic toolboxes. We envision PhysDrive will serve as a foundational resource\nand accelerate research on multimodal driver monitoring and smart-cockpit\nsystems.", "AI": {"tldr": "PhysDrive是首个大规模多模态非接触式车内生理感知数据集，包含RGB、近红外、毫米波雷达数据及六种生理真值，覆盖多种驾驶条件，旨在推动驾驶员监测和智能座舱系统研究。", "motivation": "远程生理测量(RPM)在保障驾驶安全和用户体验方面潜力巨大，但现有数据集规模小、模态单一、生物识别标注不足且未充分考虑真实驾驶场景的复杂性，限制了RPM在实际驾驶中的应用。", "method": "研究团队构建了PhysDrive数据集，收集了48名驾驶员的同步RGB、近红外摄像头和原始毫米波雷达数据，并同步记录了心电图(ECG)、血容量脉搏(BVP)、呼吸、心率(HR)、呼吸频率(RR)和血氧饱和度(SpO2)六种生理真值。数据集涵盖了驾驶员动作、动态自然光、车辆类型和道路状况等多种自然驾驶条件。此外，还在PhysDrive上评估了信号处理和深度学习方法，建立了跨所有模态的综合基准。", "result": "PhysDrive是首个大规模多模态非接触式车内生理感知数据集，专门考虑了各种模态设置和驾驶因素。它提供了全面的基准评估，并发布了与主流公共工具箱兼容的完整开源代码。", "conclusion": "PhysDrive将作为基础资源，加速多模态驾驶员监测和智能座舱系统的研究进展。"}}
{"id": "2507.18905", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18905", "abs": "https://arxiv.org/abs/2507.18905", "authors": ["Rachel L. Draelos", "Samina Afreen", "Barbara Blasko", "Tiffany Brazile", "Natasha Chase", "Dimple Desai", "Jessica Evert", "Heather L. Gardner", "Lauren Herrmann", "Aswathy Vaikom House", "Stephanie Kass", "Marianne Kavan", "Kirshma Khemani", "Amanda Koire", "Lauren M. McDonald", "Zahraa Rabeeah", "Amy Shah"], "title": "Large language models provide unsafe answers to patient-posed medical questions", "comment": "20 pages", "summary": "Millions of patients are already using large language model (LLM) chatbots\nfor medical advice on a regular basis, raising patient safety concerns. This\nphysician-led red-teaming study compares the safety of four publicly available\nchatbots--Claude by Anthropic, Gemini by Google, GPT-4o by OpenAI, and\nLlama3-70B by Meta--on a new dataset, HealthAdvice, using an evaluation\nframework that enables quantitative and qualitative analysis. In total, 888\nchatbot responses are evaluated for 222 patient-posed advice-seeking medical\nquestions on primary care topics spanning internal medicine, women's health,\nand pediatrics. We find statistically significant differences between chatbots.\nThe rate of problematic responses varies from 21.6 percent (Claude) to 43.2\npercent (Llama), with unsafe responses varying from 5 percent (Claude) to 13\npercent (GPT-4o, Llama). Qualitative results reveal chatbot responses with the\npotential to lead to serious patient harm. This study suggests that millions of\npatients could be receiving unsafe medical advice from publicly available\nchatbots, and further work is needed to improve the clinical safety of these\npowerful tools.", "AI": {"tldr": "研究发现，现有公开LLM聊天机器人在提供医疗建议时存在显著安全问题，不安全响应率高，可能对患者造成严重伤害。", "motivation": "数百万患者已常规使用大型语言模型（LLM）聊天机器人获取医疗建议，这引发了对患者安全的担忧。", "method": "本研究采用由医生主导的红队测试方法，使用新数据集HealthAdvice，对四款公开可用的聊天机器人（Anthropic的Claude、Google的Gemini、OpenAI的GPT-4o和Meta的Llama3-70B）进行安全性比较。共评估了222个患者提出的初级保健医疗问题（涵盖内科、妇科、儿科）的888个聊天机器人响应，采用定量和定性分析相结合的评估框架。", "result": "聊天机器人之间存在统计学上的显著差异。问题响应率从21.6%（Claude）到43.2%（Llama）不等，不安全响应率从5%（Claude）到13%（GPT-4o，Llama）不等。定性分析揭示了可能导致严重患者伤害的聊天机器人响应。", "conclusion": "研究表明，数百万患者可能从公开可用的聊天机器人那里获得不安全的医疗建议。需要进一步努力来提高这些强大工具的临床安全性。"}}
{"id": "2507.19146", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19146", "abs": "https://arxiv.org/abs/2507.19146", "authors": ["Ahmed Abouelazm", "Johannes Ratz", "Philip Schörner", "J. Marius Zöllner"], "title": "Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL", "comment": "Paper accepted in IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Autonomous driving faces challenges in navigating complex real-world traffic,\nrequiring safe handling of both common and critical scenarios. Reinforcement\nlearning (RL), a prominent method in end-to-end driving, enables agents to\nlearn through trial and error in simulation. However, RL training often relies\non rule-based traffic scenarios, limiting generalization. Additionally, current\nscenario generation methods focus heavily on critical scenarios, neglecting a\nbalance with routine driving behaviors. Curriculum learning, which\nprogressively trains agents on increasingly complex tasks, is a promising\napproach to improving the robustness and coverage of RL driving policies.\nHowever, existing research mainly emphasizes manually designed curricula,\nfocusing on scenery and actor placement rather than traffic behavior dynamics.\nThis work introduces a novel student-teacher framework for automatic curriculum\nlearning. The teacher, a graph-based multi-agent RL component, adaptively\ngenerates traffic behaviors across diverse difficulty levels. An adaptive\nmechanism adjusts task difficulty based on student performance, ensuring\nexposure to behaviors ranging from common to critical. The student, though\nexchangeable, is realized as a deep RL agent with partial observability,\nreflecting real-world perception constraints. Results demonstrate the teacher's\nability to generate diverse traffic behaviors. The student, trained with\nautomatic curricula, outperformed agents trained on rule-based traffic,\nachieving higher rewards and exhibiting balanced, assertive driving.", "AI": {"tldr": "本文提出一种新颖的师生框架，通过自动课程学习为自动驾驶RL代理生成多样化交通行为，从而提高其在复杂场景下的泛化能力和驾驶性能。", "motivation": "当前自动驾驶RL训练依赖规则化交通场景，限制了泛化能力；现有场景生成侧重关键场景，忽视日常驾驶行为平衡；手动设计的课程学习主要关注场景和参与者布局，而非交通行为动态。", "method": "引入一个师生自动课程学习框架。教师是一个基于图的多智能体RL组件，自适应生成不同难度级别的交通行为。一个自适应机制根据学生表现调整任务难度。学生是一个具有部分可观测性的深度RL智能体。", "result": "教师能够生成多样化的交通行为。通过自动课程训练的学生比基于规则交通训练的智能体表现更好，获得更高奖励，并展现出平衡且自信的驾驶风格。", "conclusion": "所提出的自动课程学习框架能够有效生成多样化的交通行为，显著提高了RL自动驾驶策略的鲁棒性和覆盖范围。"}}
{"id": "2507.18667", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18667", "abs": "https://arxiv.org/abs/2507.18667", "authors": ["Nicholas Fidalgo", "Aaron Contreras", "Katherine Harvey", "Johnny Ni"], "title": "Gen-AI Police Sketches with Stable Diffusion", "comment": null, "summary": "This project investigates the use of multimodal AI-driven approaches to\nautomate and enhance suspect sketching. Three pipelines were developed and\nevaluated: (1) baseline image-to-image Stable Diffusion model, (2) same model\nintegrated with a pre-trained CLIP model for text-image alignment, and (3)\nnovel approach incorporating LoRA fine-tuning of the CLIP model, applied to\nself-attention and cross-attention layers, and integrated with Stable\nDiffusion. An ablation study confirmed that fine-tuning both self- and\ncross-attention layers yielded the best alignment between text descriptions and\nsketches. Performance testing revealed that Model 1 achieved the highest\nstructural similarity (SSIM) of 0.72 and a peak signal-to-noise ratio (PSNR) of\n25 dB, outperforming Model 2 and Model 3. Iterative refinement enhanced\nperceptual similarity (LPIPS), with Model 3 showing improvement over Model 2\nbut still trailing Model 1. Qualitatively, sketches generated by Model 1\ndemonstrated the clearest facial features, highlighting its robustness as a\nbaseline despite its simplicity.", "AI": {"tldr": "该项目研究了多模态AI驱动的方法来自动化和增强嫌疑人素描，开发并评估了三种模型，发现基线模型在结构相似度和清晰度方面表现最佳。", "motivation": "旨在利用多模态AI驱动的方法，自动化并增强嫌疑人素描的生成过程。", "method": "开发并评估了三种AI管道：1) 基线图像到图像的Stable Diffusion模型；2) 集成预训练CLIP模型的Stable Diffusion模型；3) 创新性地将LoRA微调的CLIP模型（应用于自注意力和交叉注意力层）与Stable Diffusion集成。通过消融研究确认了微调自注意力和交叉注意力层的效果，并通过结构相似性（SSIM）、峰值信噪比（PSNR）和感知相似性（LPIPS）进行性能测试。", "result": "模型1（基线Stable Diffusion）取得了最高的结构相似性（SSIM为0.72）和峰值信噪比（PSNR为25 dB），优于模型2和模型3。迭代优化提升了感知相似性，模型3优于模型2但仍落后于模型1。定性分析显示，模型1生成的素描面部特征最清晰。消融研究证实，同时微调自注意力和交叉注意力层在文本描述与素描对齐方面效果最好。", "conclusion": "尽管简单，基线Stable Diffusion模型（模型1）在嫌疑人素描生成方面表现出强大的鲁棒性，并在结构相似性和特征清晰度方面优于更复杂的集成模型。迭代优化有助于提升感知相似性，且在CLIP模型中同时微调自注意力和交叉注意力层能有效改善文本-图像对齐。"}}
{"id": "2507.19182", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.19182", "abs": "https://arxiv.org/abs/2507.19182", "authors": ["Kuncheng Zou", "Jiahao Mai", "Yonggang Zhang", "Yuyi Wang", "Ondřej Kuželka", "Yuanhong Wang", "Yi Chang"], "title": "Faster Lifting for Ordered Domains with Predecessor Relations", "comment": null, "summary": "We investigate lifted inference on ordered domains with predecessor\nrelations, where the elements of the domain respect a total (cyclic) order, and\nevery element has a distinct (clockwise) predecessor. Previous work has\nexplored this problem through weighted first-order model counting (WFOMC),\nwhich computes the weighted sum of models for a given first-order logic\nsentence over a finite domain. In WFOMC, the order constraint is typically\nencoded by the linear order axiom introducing a binary predicate in the\nsentence to impose a linear ordering on the domain elements. The immediate and\nsecond predecessor relations are then encoded by the linear order predicate.\nAlthough WFOMC with the linear order axiom is theoretically tractable, existing\nalgorithms struggle with practical applications, particularly when the\npredecessor relations are involved. In this paper, we treat predecessor\nrelations as a native part of the axiom and devise a novel algorithm that\ninherently supports these relations. The proposed algorithm not only provides\nan exponential speedup for the immediate and second predecessor relations,\nwhich are known to be tractable, but also handles the general k-th predecessor\nrelations. The extensive experiments on lifted inference tasks and\ncombinatorics math problems demonstrate the efficiency of our algorithm,\nachieving speedups of a full order of magnitude.", "AI": {"tldr": "该论文研究了带前驱关系有序域上的提升推理，提出了一种新算法，将前驱关系作为公理的固有部分处理，显著提升了处理效率。", "motivation": "现有加权一阶模型计数（WFOMC）方法在处理涉及前驱关系的有序域问题时，尽管理论上可行，但在实际应用中效率低下，特别是当需要通过线性序公理编码前驱关系时。", "method": "将前驱关系作为公理的固有部分，而非通过引入二元谓词进行编码，并设计了一种新型算法，能够固有地支持这些关系。", "result": "对于立即前驱和二级前驱关系实现了指数级加速，并且能够处理更通用的k阶前驱关系。在提升推理任务和组合数学问题上的实验表明，算法效率提升了一个数量级。", "conclusion": "所提出的新算法有效解决了带前驱关系有序域上的提升推理难题，显著提高了计算效率，尤其在处理前驱关系时表现出卓越性能。"}}
{"id": "2507.18910", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18910", "abs": "https://arxiv.org/abs/2507.18910", "authors": ["Agada Joseph Oche", "Ademola Glory Folashade", "Tirthankar Ghosal", "Arpan Biswas"], "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions", "comment": "33 pages, 2 figures", "summary": "Retrieval-Augmented Generation (RAG) represents a major advancement in\nnatural language processing (NLP), combining large language models (LLMs) with\ninformation retrieval systems to enhance factual grounding, accuracy, and\ncontextual relevance. This paper presents a comprehensive systematic review of\nRAG, tracing its evolution from early developments in open domain question\nanswering to recent state-of-the-art implementations across diverse\napplications. The review begins by outlining the motivations behind RAG,\nparticularly its ability to mitigate hallucinations and outdated knowledge in\nparametric models. Core technical components-retrieval mechanisms,\nsequence-to-sequence generation models, and fusion strategies are examined in\ndetail. A year-by-year analysis highlights key milestones and research trends,\nproviding insight into RAG's rapid growth. The paper further explores the\ndeployment of RAG in enterprise systems, addressing practical challenges\nrelated to retrieval of proprietary data, security, and scalability. A\ncomparative evaluation of RAG implementations is conducted, benchmarking\nperformance on retrieval accuracy, generation fluency, latency, and\ncomputational efficiency. Persistent challenges such as retrieval quality,\nprivacy concerns, and integration overhead are critically assessed. Finally,\nthe review highlights emerging solutions, including hybrid retrieval\napproaches, privacy-preserving techniques, optimized fusion strategies, and\nagentic RAG architectures. These innovations point toward a future of more\nreliable, efficient, and context-aware knowledge-intensive NLP systems.", "AI": {"tldr": "本文对检索增强生成（RAG）进行了全面的系统综述，追溯了其发展历程、核心组件、应用挑战以及未来趋势，旨在提升大型语言模型的准确性和语境相关性。", "motivation": "RAG的出现旨在解决参数模型（如大型语言模型）中存在的幻觉问题和知识过时问题，从而增强其事实依据、准确性和语境相关性。", "method": "本文采用系统综述的方法，具体包括：追溯RAG从早期发展到最新实现的技术演变；详细分析检索机制、序列到序列生成模型和融合策略等核心技术组件；进行年度分析以揭示关键里程碑和研究趋势；探讨RAG在企业系统中的部署及其相关挑战；对RAG实现进行比较评估，基准测试其在检索准确性、生成流畅性、延迟和计算效率方面的性能；批判性评估持续存在的挑战；并展望新兴解决方案。", "result": "RAG显著提升了语言模型的真实性、准确性和语境相关性。综述揭示了RAG的快速发展、关键里程碑和研究趋势。研究讨论了在企业部署中遇到的实际挑战（如专有数据检索、安全性和可伸缩性）。对RAG实现的比较评估提供了关于检索准确性、生成流畅性、延迟和计算效率的性能基准。文章还识别了检索质量、隐私问题和集成开销等持续存在的挑战。", "conclusion": "新兴的解决方案，如混合检索方法、隐私保护技术、优化的融合策略和代理RAG架构，预示着未来将出现更可靠、高效和上下文感知的知识密集型自然语言处理系统。"}}
{"id": "2507.19151", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.19151", "abs": "https://arxiv.org/abs/2507.19151", "authors": ["Michael Amir", "Guang Yang", "Zhan Gao", "Keisuke Okumura", "Heedo Woo", "Amanda Prorok"], "title": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination", "comment": null, "summary": "Constraint-based optimization is a cornerstone of robotics, enabling the\ndesign of controllers that reliably encode task and safety requirements such as\ncollision avoidance or formation adherence. However, handcrafted constraints\ncan fail in multi-agent settings that demand complex coordination. We introduce\nReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid\nframework that merges the reliability of optimization-based controllers with\nthe adaptability of multi-agent reinforcement learning. Rather than discarding\nexpert controllers, ReCoDe improves them by learning additional, dynamic\nconstraints that capture subtler behaviors, for example, by constraining agent\nmovements to prevent congestion in cluttered scenarios. Through local\ncommunication, agents collectively constrain their allowed actions to\ncoordinate more effectively under changing conditions. In this work, we focus\non applications of ReCoDe to multi-agent navigation tasks requiring intricate,\ncontext-based movements and consensus, where we show that it outperforms purely\nhandcrafted controllers, other hybrid approaches, and standard MARL baselines.\nWe give empirical (real robot) and theoretical evidence that retaining a\nuser-defined controller, even when it is imperfect, is more efficient than\nlearning from scratch, especially because ReCoDe can dynamically change the\ndegree to which it relies on this controller.", "AI": {"tldr": "ReCoDe是一个去中心化的混合框架，它通过学习动态约束来增强基于优化的多智能体控制器，以实现复杂的协调，并在多智能体导航任务中表现优异。", "motivation": "在多智能体环境中，手工设计的约束难以应对复杂的协调需求，导致传统基于优化的控制器失效。需要一种结合可靠性与适应性的方法来处理复杂、动态的场景。", "method": "ReCoDe（基于强化学习的约束设计）是一个去中心化的混合框架，它将基于优化的控制器与多智能体强化学习相结合。它不抛弃专家控制器，而是通过学习额外的、动态的约束来改进它们，例如防止拥堵。智能体通过局部通信共同约束其允许的动作，以提高协调效率。", "result": "在需要复杂、基于上下文移动和共识的多智能体导航任务中，ReCoDe的表现优于纯手工控制器、其他混合方法和标准多智能体强化学习基线。实证（真实机器人）和理论证据表明，保留用户定义的控制器（即使不完美）比从头开始学习更有效率，因为ReCoDe可以动态调整对该控制器的依赖程度。", "conclusion": "ReCoDe成功地将优化控制器的可靠性与强化学习的适应性相结合，通过学习动态约束来增强现有控制器，有效解决了多智能体复杂协调问题，并证明了在现有控制器基础上进行改进的效率优势。"}}
{"id": "2507.18675", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18675", "abs": "https://arxiv.org/abs/2507.18675", "authors": ["Sanyam Jain", "Marsha Mariya Kappan", "Vijeta Sharma"], "title": "Advancing Vision-based Human Action Recognition: Exploring Vision-Language CLIP Model for Generalisation in Domain-Independent Tasks", "comment": null, "summary": "Human action recognition plays a critical role in healthcare and medicine,\nsupporting applications such as patient behavior monitoring, fall detection,\nsurgical robot supervision, and procedural skill assessment. While traditional\nmodels like CNNs and RNNs have achieved moderate success, they often struggle\nto generalize across diverse and complex actions. Recent advancements in\nvision-language models, especially the transformer-based CLIP model, offer\npromising capabilities for generalizing action recognition from video data. In\nthis work, we evaluate CLIP on the UCF-101 dataset and systematically analyze\nits performance under three masking strategies: (1) percentage-based and\nshape-based black masking at 10%, 30%, and 50%, (2) feature-specific masking to\nsuppress bias-inducing elements, and (3) isolation masking that retains only\nclass-specific regions. Our results reveal that CLIP exhibits inconsistent\nbehavior and frequent misclassifications, particularly when essential visual\ncues are obscured. To overcome these limitations, we propose incorporating\nclass-specific noise, learned via a custom loss function, to reinforce\nattention to class-defining features. This enhancement improves classification\naccuracy and model confidence while reducing bias. We conclude with a\ndiscussion on the challenges of applying such models in clinical domains and\noutline directions for future work to improve generalizability across\ndomain-independent healthcare scenarios.", "AI": {"tldr": "本文评估了CLIP模型在动作识别任务中的表现，特别是在医疗健康领域的应用潜力。通过系统性地测试不同遮蔽策略，发现CLIP在关键视觉线索被遮挡时表现不稳定。为解决此问题，作者提出引入类别特定噪声并结合定制损失函数来强化模型对关键特征的关注，从而提升了分类准确性和模型置信度。", "motivation": "传统的CNN和RNN模型在多样且复杂的人类动作识别上泛化能力不足，而基于Transformer的视觉-语言模型（如CLIP）在从视频数据中泛化动作识别方面显示出巨大潜力，尤其在医疗健康领域（如患者行为监测、跌倒检测、手术机器人监督等）具有重要应用价值。", "method": "研究评估了CLIP模型在UCF-101数据集上的性能，并系统分析了其在三种遮蔽策略下的表现：（1）10%、30%和50%的基于百分比和形状的黑色遮蔽；（2）特征特异性遮蔽以抑制引入偏差的元素；（3）仅保留类别特定区域的隔离遮蔽。为克服CLIP的局限性，提出通过定制损失函数学习并引入类别特定噪声，以增强模型对定义类别特征的注意力。", "result": "实验结果显示，CLIP模型表现出不一致的行为和频繁的错误分类，尤其是在关键视觉线索被遮蔽时。通过引入类别特定噪声的增强方法，显著提高了分类准确性和模型置信度，同时减少了偏差。", "conclusion": "研究讨论了将此类模型应用于临床领域的挑战，并提出了未来工作方向，旨在提高模型在独立于特定领域的医疗健康场景中的泛化能力。"}}
{"id": "2507.19261", "categories": ["cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.19261", "abs": "https://arxiv.org/abs/2507.19261", "authors": ["Osama Almurshed", "Ashish Kaushal", "Asmail Muftah", "Nitin Auluck", "Omer Rana"], "title": "Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments", "comment": "18 pages, 4 figures, ArXiv preprint - Novel \"knowledge grafting\"\n  technique achieving 88.54% AI model size reduction while improving accuracy\n  for resource-constrained deployment", "summary": "The increasing adoption of Artificial Intelligence (AI) has led to larger,\nmore complex models with numerous parameters that require substantial computing\npower -- resources often unavailable in many real-world application scenarios.\nOur paper addresses this challenge by introducing knowledge grafting, a novel\nmechanism that optimizes AI models for resource-constrained environments by\ntransferring selected features (the scion) from a large donor model to a\nsmaller rootstock model. The approach achieves an 88.54% reduction in model\nsize (from 64.39 MB to 7.38 MB), while improving generalization capability of\nthe model. Our new rootstock model achieves 89.97% validation accuracy (vs.\ndonor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and\nperforms exceptionally well on unseen test data with 90.45% accuracy. It\naddresses the typical size vs performance trade-off, and enables deployment of\nAI frameworks on resource-constrained devices with enhanced performance. We\nhave tested our approach on an agricultural weed detection scenario, however,\nit can be extended across various edge computing scenarios, potentially\naccelerating AI adoption in areas with limited hardware/software support -- by\nmirroring in a similar manner the horticultural grafting enables productive\ncultivation in challenging agri-based environments.", "AI": {"tldr": "本文提出“知识嫁接”机制，将大型AI模型的精选特征迁移到小型模型，显著减小模型尺寸并提升性能，以适应资源受限环境。", "motivation": "随着AI模型日益复杂和庞大，其所需的计算资源在许多实际应用场景中难以满足，尤其是在资源受限的环境下部署AI面临挑战。", "method": "引入“知识嫁接”机制，该方法从一个大型“供体”模型中选择性地提取特征（“接穗”），并将其转移到一个较小的“砧木”模型上，从而优化模型以适应资源受限环境。", "result": "该方法使模型尺寸减少了88.54%（从64.39 MB降至7.38 MB），同时提升了泛化能力。新的砧木模型验证准确率达到89.97%（供体为87.47%），验证损失更低（0.2976 vs 0.5068），在未见测试数据上表现出色，准确率为90.45%。该方法已在农业杂草检测场景中验证。", "conclusion": "“知识嫁接”机制有效解决了AI模型尺寸与性能之间的权衡问题，使得AI框架能够在资源受限设备上部署并实现增强性能。该方法具有普适性，可推广至各种边缘计算场景，加速AI在硬件/软件支持有限区域的普及。"}}
{"id": "2507.18915", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18915", "abs": "https://arxiv.org/abs/2507.18915", "authors": ["Ananya Sahu", "Amith Ananthram", "Kathleen McKeown"], "title": "Mining Contextualized Visual Associations from Images for Creativity Understanding", "comment": null, "summary": "Understanding another person's creative output requires a shared language of\nassociation. However, when training vision-language models such as CLIP, we\nrely on web-scraped datasets containing short, predominantly literal, alt-text.\nIn this work, we introduce a method for mining contextualized associations for\nsalient visual elements in an image that can scale to any unlabeled dataset.\nGiven an image, we can use these mined associations to generate high quality\ncreative captions at increasing degrees of abstraction. With our method, we\nproduce a new dataset of visual associations and 1.7m creative captions for the\nimages in MSCOCO. Human evaluation confirms that these captions remain visually\ngrounded while exhibiting recognizably increasing abstraction. Moreover,\nfine-tuning a visual encoder on this dataset yields meaningful improvements in\nzero-shot image-text retrieval in two creative domains: poetry and metaphor\nvisualization. We release our dataset, our generation code and our models for\nuse by the broader community.", "AI": {"tldr": "该研究提出了一种挖掘图像上下文关联的方法，用于生成高质量、渐进抽象的创意描述，并构建了一个大型数据集，显著提升了视觉语言模型在创意领域的理解和检索能力。", "motivation": "当前视觉语言模型（如CLIP）依赖于网络抓取的短文本描述（alt-text），这些描述过于字面化，限制了模型理解和生成具有创造性、抽象性内容的潜力。", "method": "引入了一种方法，能够从任何未标注数据集中挖掘图像中显著视觉元素的上下文关联。利用这些挖掘到的关联，生成不同抽象程度的高质量创意描述。基于此方法，构建了一个包含视觉关联和170万MSCOCO创意描述的新数据集。", "result": "成功生成了MSCOCO图像的视觉关联数据集和170万条创意描述。人工评估证实这些描述在保持视觉关联的同时，抽象程度显著提高。此外，使用该数据集微调视觉编码器，在诗歌和隐喻可视化这两个创意领域的零样本图文检索任务中取得了显著性能提升。", "conclusion": "该方法和数据集有效弥补了现有视觉语言模型在理解创意内容方面的不足，显著提升了模型在创意任务中的表现。研究成果已开源供社区使用。"}}
{"id": "2507.19196", "categories": ["cs.RO", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19196", "abs": "https://arxiv.org/abs/2507.19196", "authors": ["Ruben Janssens", "Tony Belpaeme"], "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "comment": "Submitted to the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices.", "AI": {"tldr": "本文探讨了大型语言模型驱动的社交机器人在开放域对话中缺乏多模态社交技能的问题，并提出视觉-语言模型是解决此问题的通用方案。", "motivation": "尽管大型语言模型赋予了社交机器人开放域对话的能力，但它们缺乏利用多模态信息进行社交互动的基础技能。以往工作多集中于任务导向或特定现象，而缺少对机器人社交对话中多模态系统整体需求的考量。", "method": "文章概述了社交机器人多模态系统所需满足的整体需求，并提出视觉-语言模型能够以足够通用的方式处理广泛的视觉信息。同时，讨论了如何将视觉-语言模型应用于此场景、仍存在的技挑战以及评估方法。", "result": "文章论证了视觉-语言模型有能力以通用方式处理社交机器人所需的广泛视觉信息，为实现机器人多模态社交对话提供了方向。", "conclusion": "视觉-语言模型为赋予社交机器人多模态社交对话能力提供了有前景的途径，但仍需解决技术挑战并完善评估实践。"}}
{"id": "2507.18677", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.18677", "abs": "https://arxiv.org/abs/2507.18677", "authors": ["Siyu Mu", "Wei Xuan Chan", "Choon Hwai Yap"], "title": "HeartUnloadNet: A Weakly-Supervised Cycle-Consistent Graph Network for Predicting Unloaded Cardiac Geometry from Diastolic States", "comment": "Codes are available at https://github.com/SiyuMU/Loaded2UnNet", "summary": "The unloaded cardiac geometry (i.e., the state of the heart devoid of luminal\npressure) serves as a valuable zero-stress and zero-strain reference and is\ncritical for personalized biomechanical modeling of cardiac function, to\nunderstand both healthy and diseased physiology and to predict the effects of\ncardiac interventions. However, estimating the unloaded geometry from clinical\nimages remains a challenging task. Traditional approaches rely on inverse\nfinite element (FE) solvers that require iterative optimization and are\ncomputationally expensive. In this work, we introduce HeartUnloadNet, a deep\nlearning framework that predicts the unloaded left ventricular (LV) shape\ndirectly from the end diastolic (ED) mesh while explicitly incorporating\nbiophysical priors. The network accepts a mesh of arbitrary size along with\nphysiological parameters such as ED pressure, myocardial stiffness scale, and\nfiber helix orientation, and outputs the corresponding unloaded mesh. It adopts\na graph attention architecture and employs a cycle-consistency strategy to\nenable bidirectional (loading and unloading) prediction, allowing for partial\nself-supervision that improves accuracy and reduces the need for large training\ndatasets. Trained and tested on 20,700 FE simulations across diverse LV\ngeometries and physiological conditions, HeartUnloadNet achieves sub-millimeter\naccuracy, with an average DSC of 0.986 and HD of 0.083 cm, while reducing\ninference time to just 0.02 seconds per case, over 10^5 times faster and\nsignificantly more accurate than traditional inverse FE solvers. Ablation\nstudies confirm the effectiveness of the architecture. Notably, the\ncycle-consistent design enables the model to maintain a DSC of 97% even with as\nfew as 200 training samples. This work thus presents a scalable and accurate\nsurrogate for inverse FE solvers, supporting real-time clinical applications in\nthe future.", "AI": {"tldr": "HeartUnloadNet是一个深度学习框架，能从临床图像中快速准确地预测左心室（LV）的无负荷几何形状，作为传统逆向有限元（FE）求解器的替代方案。", "motivation": "无负荷心脏几何形状是心脏生物力学建模的关键零应力/应变参考，对于理解健康与疾病生理学及预测心脏干预效果至关重要。然而，从临床图像中估计无负荷几何形状具有挑战性，传统逆向有限元（FE）求解器计算成本高昂且耗时。", "method": "本研究引入了HeartUnloadNet，一个基于图注意力架构的深度学习框架。它直接从舒张末期（ED）网格和生理参数（如ED压力、心肌刚度、纤维螺旋方向）预测无负荷LV网格。该框架采用循环一致性策略实现双向（加载和卸载）预测，从而实现部分自我监督，减少了对大量训练数据的需求。", "result": "HeartUnloadNet在20,700个有限元模拟数据集上进行了训练和测试，实现了亚毫米级精度（平均DSC为0.986，HD为0.083 cm）。推理时间仅为每例0.02秒，比传统逆向有限元求解器快10^5倍以上。消融研究证实了其架构的有效性，循环一致性设计使得模型即使在只有200个训练样本的情况下也能保持97%的DSC。", "conclusion": "HeartUnloadNet为逆向有限元求解器提供了一个可扩展且精确的替代方案，有望支持未来实时临床应用。"}}
{"id": "2507.19263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19263", "abs": "https://arxiv.org/abs/2507.19263", "authors": ["Achille Morenville", "Éric Piette"], "title": "Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games", "comment": null, "summary": "In imperfect-information games, agents must make decisions based on partial\nknowledge of the game state. The Belief Stochastic Game model addresses this\nchallenge by delegating state estimation to the game model itself. This allows\nagents to operate on externally provided belief states, thereby reducing the\nneed for game-specific inference logic. This paper investigates two approaches\nto represent beliefs in games with hidden piece identities: a constraint-based\nmodel using Constraint Satisfaction Problems and a probabilistic extension\nusing Belief Propagation to estimate marginal probabilities. We evaluated the\nimpact of both representations using general-purpose agents across two\ndifferent games. Our findings indicate that constraint-based beliefs yield\nresults comparable to those of probabilistic inference, with minimal\ndifferences in agent performance. This suggests that constraint-based belief\nstates alone may suffice for effective decision-making in many settings.", "AI": {"tldr": "本文探讨了在不完全信息博弈中，信念随机博弈模型如何通过外部提供的信念状态来简化智能体的决策。研究比较了基于约束和概率的信念表示方法，发现基于约束的方法在性能上与概率推理相当。", "motivation": "在不完全信息博弈中，智能体必须基于部分游戏状态知识做出决策，这要求复杂的内部推理。信念随机博弈模型通过将状态估计委托给游戏模型本身，旨在减少智能体对特定游戏推理逻辑的需求。", "method": "研究了两种信念表示方法：1) 基于约束的模型，使用约束满足问题（CSP）来表示信念；2) 概率扩展模型，使用信念传播（Belief Propagation）来估计边际概率。通过通用智能体在两种不同游戏中评估了这两种方法的影响。", "result": "研究发现，基于约束的信念表示方法与概率推理方法产生了可比较的结果，智能体性能差异极小。这表明在许多情况下，单独使用基于约束的信念状态可能足以实现有效的决策。", "conclusion": "基于约束的信念状态表示方法在不完全信息博弈中表现出与概率推理相当的有效性，可能足以支持智能体在许多场景下的有效决策，从而简化了智能体的设计和推理负担。"}}
{"id": "2507.18918", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18918", "abs": "https://arxiv.org/abs/2507.18918", "authors": ["Richmond Sin Jing Xuan", "Jalil Huseynov", "Yang Zhang"], "title": "Uncovering Cross-Linguistic Disparities in LLMs using Sparse Autoencoders", "comment": null, "summary": "Multilingual large language models (LLMs) exhibit strong cross-linguistic\ngeneralization, yet medium to low resource languages underperform on common\nbenchmarks such as ARC-Challenge, MMLU, and HellaSwag. We analyze activation\npatterns in Gemma-2-2B across all 26 residual layers and 10 languages: Chinese\n(zh), Russian (ru), Spanish (es), Italian (it), medium to low resource\nlanguages including Indonesian (id), Catalan (ca), Marathi (mr), Malayalam\n(ml), and Hindi (hi), with English (en) as the reference. Using Sparse\nAutoencoders (SAEs), we reveal systematic disparities in activation patterns.\nMedium to low resource languages receive up to 26.27 percent lower activations\nin early layers, with a persistent gap of 19.89 percent in deeper layers. To\naddress this, we apply activation-aware fine-tuning via Low-Rank Adaptation\n(LoRA), leading to substantial activation gains, such as 87.69 percent for\nMalayalam and 86.32 percent for Hindi, while maintaining English retention at\napproximately 91 percent. After fine-tuning, benchmark results show modest but\nconsistent improvements, highlighting activation alignment as a key factor in\nenhancing multilingual LLM performance.", "AI": {"tldr": "研究发现多语言LLM在低资源语言上表现不佳源于激活模式的系统性差异，通过激活感知微调（LoRA）可显著提升低资源语言的激活并带来性能改善。", "motivation": "多语言大型语言模型（LLMs）在低资源语言上的性能不如高资源语言，尽管它们具有强大的跨语言泛化能力，但在常见基准测试中表现不佳。", "method": "使用稀疏自编码器（SAEs）分析了Gemma-2-2B模型在26个残差层和10种语言（包括中文、俄语、西班牙语、意大利语、印尼语、加泰罗尼亚语、马拉地语、马拉雅拉姆语、印地语和英语）上的激活模式。为了解决发现的激活差异，通过低秩适应（LoRA）进行了激活感知微调。", "result": "低资源语言在早期层接收到的激活量比英语低26.27%，在更深层保持19.89%的差距。经过激活感知微调后，马拉雅拉姆语和印地语等语言的激活量显著增加（分别达到87.69%和86.32%），同时英语的保留率约为91%。微调后，基准测试结果显示出适度但持续的性能提升。", "conclusion": "激活对齐是增强多语言LLM性能的关键因素。通过解决激活模式的差异，可以有效提升模型在低资源语言上的表现。"}}
{"id": "2507.19242", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19242", "abs": "https://arxiv.org/abs/2507.19242", "authors": ["Kang Xiangli", "Yage He", "Xianwu Gong", "Zehan Liu", "Yuru Bai"], "title": "Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation", "comment": null, "summary": "This study presents a grasping method for objects with uneven mass\ndistribution by leveraging diffusion models to localize the center of gravity\n(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to\npostural instability, where existing keypoint-based or affordance-driven\nmethods exhibit limitations. We constructed a dataset of 790 images featuring\nunevenly distributed objects with keypoint annotations for CoG localization. A\nvision-driven framework based on foundation models was developed to achieve\nCoG-aware grasping. Experimental evaluations across real-world scenarios\ndemonstrate that our method achieves a 49\\% higher success rate compared to\nconventional keypoint-based approaches and an 11\\% improvement over\nstate-of-the-art affordance-driven methods. The system exhibits strong\ngeneralization with a 76\\% CoG localization accuracy on unseen objects,\nproviding a novel solution for precise and stable grasping tasks.", "AI": {"tldr": "本研究提出一种利用扩散模型定位未知物体重心（CoG）的抓取方法，以解决物体质量分布不均导致的抓取不稳定性问题。", "motivation": "在机器人抓取中，重心偏移常导致姿态不稳。现有基于关键点或功能驱动的抓取方法在此方面存在局限性。", "method": "构建了一个包含790张图像的CoG关键点标注数据集，用于非均匀质量分布物体的重心定位。开发了一个基于基础模型的视觉驱动框架，利用扩散模型实现CoG感知抓取。", "result": "与传统基于关键点的方法相比，成功率提高了49%；与最先进的功能驱动方法相比，成功率提高了11%。在未见物体上的CoG定位准确率达到76%，展现出强大的泛化能力。", "conclusion": "该方法为精确稳定的抓取任务提供了一种新颖的解决方案，尤其适用于质量分布不均的物体，并具有良好的泛化性。"}}
{"id": "2507.18678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18678", "abs": "https://arxiv.org/abs/2507.18678", "authors": ["Xingyu Miao", "Haoran Duan", "Quanhao Qian", "Jiuniu Wang", "Yang Long", "Ling Shao", "Deli Zhao", "Ran Xu", "Gongjie Zhang"], "title": "Towards Scalable Spatial Intelligence via 2D-to-3D Data Lifting", "comment": "ICCV 2025 (Highlight)", "summary": "Spatial intelligence is emerging as a transformative frontier in AI, yet it\nremains constrained by the scarcity of large-scale 3D datasets. Unlike the\nabundant 2D imagery, acquiring 3D data typically requires specialized sensors\nand laborious annotation. In this work, we present a scalable pipeline that\nconverts single-view images into comprehensive, scale- and appearance-realistic\n3D representations - including point clouds, camera poses, depth maps, and\npseudo-RGBD - via integrated depth estimation, camera calibration, and scale\ncalibration. Our method bridges the gap between the vast repository of imagery\nand the increasing demand for spatial scene understanding. By automatically\ngenerating authentic, scale-aware 3D data from images, we significantly reduce\ndata collection costs and open new avenues for advancing spatial intelligence.\nWe release two generated spatial datasets, i.e., COCO-3D and Objects365-v2-3D,\nand demonstrate through extensive experiments that our generated data can\nbenefit various 3D tasks, ranging from fundamental perception to MLLM-based\nreasoning. These results validate our pipeline as an effective solution for\ndeveloping AI systems capable of perceiving, understanding, and interacting\nwith physical environments.", "AI": {"tldr": "该研究提出一种可扩展的管道，能将单视角2D图像转换为真实的3D数据（点云、相机姿态、深度图等），以解决大规模3D数据集稀缺的问题，并发布了两个生成的3D数据集。", "motivation": "空间智能是AI的新兴前沿，但受限于大规模3D数据集的稀缺。与2D图像不同，获取3D数据通常需要专业传感器和大量标注，成本高昂。", "method": "开发了一个可扩展的管道，通过集成深度估计、相机标定和尺度标定，将单视角图像转换为全面的、尺度和外观真实的3D表示，包括点云、相机姿态、深度图和伪RGBD。", "result": "成功生成了两个空间数据集：COCO-3D和Objects365-v2-3D。通过大量实验证明，生成的数据能有效提升各种3D任务的性能，涵盖从基础感知到基于MLLM的推理。", "conclusion": "该管道是开发能够感知、理解并与物理环境交互的AI系统的有效解决方案，显著降低了数据收集成本并为空间智能的进步开辟了新途径。"}}
{"id": "2507.19364", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.19364", "abs": "https://arxiv.org/abs/2507.19364", "authors": ["Patrick Taillandier", "Jean Daniel Zucker", "Arnaud Grignard", "Benoit Gaudou", "Nghi Quang Huynh", "Alexis Drogoul"], "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges", "comment": null, "summary": "This position paper examines the use of Large Language Models (LLMs) in\nsocial simulation, analyzing both their potential and their limitations from a\ncomputational social science perspective. The first part reviews recent\nfindings on the ability of LLMs to replicate key aspects of human cognition,\nincluding Theory of Mind reasoning and social inference, while also\nhighlighting significant limitations such as cognitive biases, lack of true\nunderstanding, and inconsistencies in behavior. The second part surveys\nemerging applications of LLMs in multi-agent simulation frameworks, focusing on\nsystem architectures, scale, and validation strategies. Notable projects such\nas Generative Agents (Smallville) and AgentSociety are discussed in terms of\ntheir design choices, empirical grounding, and methodological innovations.\nParticular attention is given to the challenges of behavioral fidelity,\ncalibration, and reproducibility in large-scale LLM-driven simulations. The\nfinal section distinguishes between contexts where LLMs, like other black-box\nsystems, offer direct value-such as interactive simulations and serious\ngames-and those where their use is more problematic, notably in explanatory or\npredictive modeling. The paper concludes by advocating for hybrid approaches\nthat integrate LLMs into traditional agent-based modeling platforms (GAMA,\nNetlogo, etc), enabling modelers to combine the expressive flexibility of\nlanguage-based reasoning with the transparency and analytical rigor of\nclassical rule-based systems.", "AI": {"tldr": "本文从计算社会科学角度，探讨了大型语言模型（LLMs）在社会模拟中的潜力和局限性，并主张采用混合方法。", "motivation": "随着LLMs能力增强，研究其在社会模拟中复制人类认知和行为的潜力，同时识别其固有限制和面临的挑战，以指导未来的应用和研究方向。", "method": "本文是一篇立场论文，通过回顾LLMs复制人类认知（如心智理论和社交推理）的最新发现及其局限性（如认知偏差、缺乏理解），并调查LLMs在多智能体模拟框架中的新兴应用（如系统架构、规模、验证策略），讨论了Generative Agents和AgentSociety等项目的设计与挑战。", "result": "LLMs在复制人类认知方面展现出潜力，但也存在认知偏差、缺乏真实理解和行为不一致等显著局限。在多智能体模拟中，LLMs的应用面临行为保真度、校准和可复现性等挑战。LLMs在交互式模拟和严肃游戏中具有直接价值，但在解释性或预测性建模中应用则更为复杂。", "conclusion": "建议采用混合方法，将LLMs整合到传统的基于智能体的建模平台中，以结合语言推理的表达灵活性与经典基于规则系统的透明度和分析严谨性。"}}
{"id": "2507.18940", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.18940", "abs": "https://arxiv.org/abs/2507.18940", "authors": ["Jingxuan Wei", "Caijun Jia", "Qi Chen", "Yujun Cai", "Linzhuang Sun", "Xiangxiang Zhang", "Gaowei Wu", "Bihui Yu"], "title": "LLaVA-NeuMT: Selective Layer-Neuron Modulation for Efficient Multilingual Multimodal Translation", "comment": null, "summary": "Multimodal Machine Translation (MMT) enhances translation quality by\nincorporating visual context, helping to resolve textual ambiguities. While\nexisting MMT methods perform well in bilingual settings, extending them to\nmultilingual translation remains challenging due to cross-lingual interference\nand ineffective parameter-sharing strategies. To address this, we propose\nLLaVA-NeuMT, a novel multimodal multilingual translation framework that\nexplicitly models language-specific and language-agnostic representations to\nmitigate multilingual interference. Our approach consists of a layer selection\nmechanism that identifies the most informative layers for different language\npairs and a neuron-level adaptation strategy that dynamically selects\nlanguage-specific and agnostic neurons to improve translation quality while\nreducing redundancy. We conduct extensive experiments on the M3-Multi30K and\nM3-AmbigCaps datasets, demonstrating that LLaVA-NeuMT, while fine-tuning only\n40\\% of the model parameters, surpasses full fine-tuning approaches and\nultimately achieves SOTA results on both datasets. Our analysis further\nprovides insights into the importance of selected layers and neurons in\nmultimodal multilingual adaptation, offering an efficient and scalable solution\nto cross-lingual adaptation in multimodal translation.", "AI": {"tldr": "LLaVA-NeuMT是一种新型的多模态多语言翻译框架，通过显式建模语言特异性和语言无关表示，以及层选择和神经元级别自适应策略，有效缓解跨语言干扰，以更少参数实现最先进的多语言多模态翻译性能。", "motivation": "现有多模态机器翻译（MMT）方法在双语环境下表现良好，但在多语言翻译中面临跨语言干扰和参数共享策略效率低下等挑战。", "method": "提出LLaVA-NeuMT框架，通过建模语言特异性和语言无关表示来减轻多语言干扰。该方法包含一个层选择机制，用于识别不同语言对最信息丰富的层；以及一个神经元级别自适应策略，动态选择语言特异性和无关神经元，以提高翻译质量并减少冗余。", "result": "LLaVA-NeuMT在M3-Multi30K和M3-AmbigCaps数据集上进行了广泛实验，结果表明，仅微调40%的模型参数，其性能超越了全量微调方法，并在两个数据集上均取得了最先进（SOTA）的结果。", "conclusion": "LLaVA-NeuMT为多模态翻译中的跨语言自适应提供了一个高效且可扩展的解决方案，并深入揭示了所选层和神经元在多模态多语言自适应中的重要性。"}}
{"id": "2507.19335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19335", "abs": "https://arxiv.org/abs/2507.19335", "authors": ["Ilaria Consoli", "Claudio Mattutino", "Cristina Gena", "Berardina de Carolis", "Giuseppe Palestra"], "title": "How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version", "comment": null, "summary": "This paper presents an empirical study investigating how individuals across\ndifferent age groups, children, young and older adults, interpret emotional\nbody language expressed by the humanoid robot NAO. The aim is to offer insights\ninto how users perceive and respond to emotional cues from robotic agents,\nthrough an empirical evaluation of the robot's effectiveness in conveying\nemotions to different groups of users. By analyzing data collected from elderly\nparticipants and comparing these findings with previously gathered data from\nyoung adults and children, the study highlights similarities and differences\nbetween the groups, with younger and older users more similar but different\nfrom young adults.", "AI": {"tldr": "本文研究了不同年龄段（儿童、年轻人、老年人）个体如何解读NAO人形机器人表达的情绪肢体语言。", "motivation": "旨在深入了解用户如何感知和响应机器人代理的情绪线索，并通过实证评估机器人向不同用户群体传达情感的有效性。", "method": "一项实证研究，通过收集老年参与者的数据，并与之前收集的年轻成人和儿童数据进行比较，分析NAO机器人情绪表达的解读情况。", "result": "研究揭示了不同年龄组之间的异同，发现儿童和老年用户在解读上更为相似，而与年轻成人有所不同。", "conclusion": "提供了关于不同年龄段用户感知机器人情绪肢体语言的见解，并强调了儿童与老年用户之间出人意料的相似性。"}}
{"id": "2507.18713", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18713", "abs": "https://arxiv.org/abs/2507.18713", "authors": ["Yun Chen", "Matthew Haines", "Jingkang Wang", "Krzysztof Baron-Lis", "Sivabalan Manivasagam", "Ze Yang", "Raquel Urtasun"], "title": "SaLF: Sparse Local Fields for Multi-Sensor Rendering in Real-Time", "comment": null, "summary": "High-fidelity sensor simulation of light-based sensors such as cameras and\nLiDARs is critical for safe and accurate autonomy testing. Neural radiance\nfield (NeRF)-based methods that reconstruct sensor observations via ray-casting\nof implicit representations have demonstrated accurate simulation of driving\nscenes, but are slow to train and render, hampering scale. 3D Gaussian\nSplatting (3DGS) has demonstrated faster training and rendering times through\nrasterization, but is primarily restricted to pinhole camera sensors,\npreventing usage for realistic multi-sensor autonomy evaluation. Moreover, both\nNeRF and 3DGS couple the representation with the rendering procedure (implicit\nnetworks for ray-based evaluation, particles for rasterization), preventing\ninteroperability, which is key for general usage. In this work, we present\nSparse Local Fields (SaLF), a novel volumetric representation that supports\nrasterization and raytracing. SaLF represents volumes as a sparse set of 3D\nvoxel primitives, where each voxel is a local implicit field. SaLF has fast\ntraining (<30 min) and rendering capabilities (50+ FPS for camera and 600+ FPS\nLiDAR), has adaptive pruning and densification to easily handle large scenes,\nand can support non-pinhole cameras and spinning LiDARs. We demonstrate that\nSaLF has similar realism as existing self-driving sensor simulation methods\nwhile improving efficiency and enhancing capabilities, enabling more scalable\nsimulation. https://waabi.ai/salf/", "AI": {"tldr": "SaLF是一种新型稀疏局部场体素表示，支持光栅化和光线追踪，实现了高保真传感器（相机/LiDAR）的快速训练和渲染，并支持非针孔相机和旋转LiDAR，解决了现有方法效率低、兼容性差的问题。", "motivation": "高保真传感器模拟对自动驾驶测试至关重要。现有方法如NeRF训练和渲染速度慢，3DGS虽快但仅限于针孔相机，且二者均将表示与渲染过程耦合，缺乏互操作性，难以满足多传感器自动驾驶评估的需求。", "method": "本文提出Sparse Local Fields (SaLF)，一种新颖的体素表示方法。SaLF将体素表示为稀疏的3D体素原语集合，每个体素都是一个局部隐式场。它支持光栅化和光线追踪，并包含自适应剪枝和密集化功能，以处理大型场景。", "result": "SaLF实现了快速训练（<30分钟）和渲染（相机50+ FPS，LiDAR 600+ FPS）。它能够支持非针孔相机和旋转LiDAR，且具有与现有自动驾驶传感器模拟方法相似的真实感，同时显著提高了效率并增强了能力。", "conclusion": "SaLF提供了一种更具可扩展性的传感器模拟方法，通过提高效率、增强多传感器支持和互操作性，克服了现有NeRF和3DGS方法的局限性，为自动驾驶测试提供了更强大的工具。"}}
{"id": "2507.19372", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19372", "abs": "https://arxiv.org/abs/2507.19372", "authors": ["Flavio Petruzzellis", "Alberto Testolin", "Alessandro Sperduti"], "title": "Learning neuro-symbolic convergent term rewriting systems", "comment": "48 pages, 31 figures. Submitted for review by Artificial Intelligence\n  Journal", "summary": "Building neural systems that can learn to execute symbolic algorithms is a\nchallenging open problem in artificial intelligence, especially when aiming for\nstrong generalization and out-of-distribution performance. In this work, we\nintroduce a general framework for learning convergent term rewriting systems\nusing a neuro-symbolic architecture inspired by the rewriting algorithm itself.\nWe present two modular implementations of such architecture: the Neural\nRewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a\nresult of algorithmic-inspired design and key architectural elements, both\nmodels can generalize to out-of-distribution instances, with FastNRS offering\nsignificant improvements in terms of memory efficiency, training speed, and\ninference time. We evaluate both architectures on four tasks involving the\nsimplification of mathematical formulas and further demonstrate their\nversatility in a multi-domain learning scenario, where a single model is\ntrained to solve multiple types of problems simultaneously. The proposed system\nsignificantly outperforms two strong neural baselines: the Neural Data Router,\na recent transformer variant specifically designed to solve algorithmic\nproblems, and GPT-4o, one of the most powerful general-purpose large-language\nmodels. Moreover, our system matches or outperforms the latest o1-preview model\nfrom OpenAI that excels in reasoning benchmarks.", "AI": {"tldr": "本文提出了一种受算法启发的神经符号架构（NRS和FastNRS），用于学习收敛的项重写系统，以解决符号算法执行中的泛化问题，并在数学公式简化等任务上表现出色，超越了现有基线模型。", "motivation": "在人工智能领域，构建能够学习执行符号算法的神经网络系统是一个具有挑战性的开放问题，尤其是在实现强大的泛化能力和处理分布外（OOD）数据方面。", "method": "研究引入了一个通用的框架，用于学习收敛的项重写系统。该框架采用了一种受重写算法本身启发的神经符号架构，并提出了两种模块化实现：神经重写系统（NRS）和快速神经重写系统（FastNRS）。", "result": "NRS和FastNRS模型都能够泛化到分布外实例，其中FastNRS在内存效率、训练速度和推理时间方面有显著改进。在四项数学公式简化任务和多领域学习场景中进行了评估，该系统显著优于两种强大的神经基线模型（Neural Data Router和GPT-4o），并与OpenAI的o1-preview模型持平或超越。", "conclusion": "所提出的神经符号系统能够有效地学习和执行符号算法，并在具有挑战性的任务上展现出强大的泛化能力和优越性能，超越了多种先进的神经网络和大型语言模型。"}}
{"id": "2507.18952", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18952", "abs": "https://arxiv.org/abs/2507.18952", "authors": ["Yongjie Li", "Ruilin Nong", "Jianan Liu", "Lucas Evans"], "title": "Legal Document Summarization: Enhancing Judicial Efficiency through Automation Detection", "comment": null, "summary": "Legal document summarization represents a significant advancement towards\nimproving judicial efficiency through the automation of key information\ndetection. Our approach leverages state-of-the-art natural language processing\ntechniques to meticulously identify and extract essential data from extensive\nlegal texts, which facilitates a more efficient review process. By employing\nadvanced machine learning algorithms, the framework recognizes underlying\npatterns within judicial documents to create precise summaries that encapsulate\nthe crucial elements. This automation alleviates the burden on legal\nprofessionals, concurrently reducing the likelihood of overlooking vital\ninformation that could lead to errors. Through comprehensive experiments\nconducted with actual legal datasets, we demonstrate the capability of our\nmethod to generate high-quality summaries while preserving the integrity of the\noriginal content and enhancing processing times considerably. The results\nreveal marked improvements in operational efficiency, allowing legal\npractitioners to direct their efforts toward critical analytical and\ndecision-making activities instead of manual reviews. This research highlights\npromising technology-driven strategies that can significantly alter workflow\ndynamics within the legal sector, emphasizing the role of automation in\nrefining judicial processes.", "AI": {"tldr": "本文提出一种利用先进自然语言处理和机器学习技术自动生成法律文档摘要的方法，旨在提高司法效率、减轻法律专业人员负担并减少错误。", "motivation": "目前的司法流程中，法律专业人员需要手动审查大量法律文本，效率低下且容易遗漏关键信息导致错误。因此，需要自动化技术来提高司法效率和准确性。", "method": "该研究利用最先进的自然语言处理（NLP）技术来识别和提取法律文本中的关键信息，并采用先进的机器学习算法识别司法文档中的潜在模式，从而生成精确的摘要。", "result": "通过在真实法律数据集上进行的综合实验，结果表明该方法能够生成高质量的摘要，同时保持原始内容的完整性，显著缩短处理时间，并显著提高操作效率。", "conclusion": "这项研究表明，技术驱动的自动化策略可以显著改变法律行业的工作流程动态，强调了自动化在优化司法流程中的重要作用。"}}
{"id": "2507.18763", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18763", "abs": "https://arxiv.org/abs/2507.18763", "authors": ["Keshav Gupta", "Tejas S. Stanley", "Pranjal Paul", "Arun K. Singh", "K. Madhava Krishna"], "title": "Diffusion-FS: Multimodal Free-Space Prediction via Diffusion for Autonomous Driving", "comment": "8 pages, 7 figures, IROS 2025", "summary": "Drivable Free-space prediction is a fundamental and crucial problem in\nautonomous driving. Recent works have addressed the problem by representing the\nentire non-obstacle road regions as the free-space. In contrast our aim is to\nestimate the driving corridors that are a navigable subset of the entire road\nregion. Unfortunately, existing corridor estimation methods directly assume a\nBEV-centric representation, which is hard to obtain. In contrast, we frame\ndrivable free-space corridor prediction as a pure image perception task, using\nonly monocular camera input. However such a formulation poses several\nchallenges as one doesn't have the corresponding data for such free-space\ncorridor segments in the image. Consequently, we develop a novel\nself-supervised approach for free-space sample generation by leveraging future\nego trajectories and front-view camera images, making the process of visual\ncorridor estimation dependent on the ego trajectory. We then employ a diffusion\nprocess to model the distribution of such segments in the image. However, the\nexisting binary mask-based representation for a segment poses many limitations.\nTherefore, we introduce ContourDiff, a specialized diffusion-based architecture\nthat denoises over contour points rather than relying on binary mask\nrepresentations, enabling structured and interpretable free-space predictions.\nWe evaluate our approach qualitatively and quantitatively on both nuScenes and\nCARLA, demonstrating its effectiveness in accurately predicting safe multimodal\nnavigable corridors in the image.", "AI": {"tldr": "本文提出了一种名为ContourDiff的新型自监督扩散模型，用于仅通过单目图像预测可驾驶自由空间走廊。该模型通过利用未来自我轨迹和前视图图像进行数据生成，并直接在轮廓点而非二值掩码上进行去噪，从而实现了结构化且可解释的预测。", "motivation": "自动驾驶中的可驾驶自由空间预测至关重要。现有方法将整个非障碍道路区域视为自由空间，但目标是更精确地估计可导航的驾驶走廊。现有走廊估计方法依赖于难以获取的BEV（鸟瞰图）表示。直接从单目图像进行走廊预测面临数据缺乏的挑战，且传统的二值掩码表示存在局限性。", "method": "将可驾驶自由空间走廊预测框架为纯图像感知任务，仅使用单目相机输入。开发了一种新颖的自监督方法，通过利用未来自我轨迹和前视图相机图像生成自由空间样本。然后，采用扩散过程来建模图像中这些段的分布。引入了ContourDiff，这是一种专门的基于扩散的架构，它对轮廓点进行去噪，而非依赖二值掩码表示。", "result": "在nuScenes和CARLA数据集上进行了定性和定量评估，结果表明该方法能有效准确地预测图像中安全的多模态可导航走廊。", "conclusion": "本文成功地将可驾驶自由空间走廊预测转化为一个纯粹的单目图像感知任务，并通过创新的自监督数据生成和基于轮廓点的扩散模型ContourDiff，克服了数据和表示上的挑战，实现了准确且可解释的走廊预测。"}}
{"id": "2507.18740", "categories": ["eess.IV", "cs.CV", "cs.LG", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.18740", "abs": "https://arxiv.org/abs/2507.18740", "authors": ["Serban C. Tudosie", "Valerio Gandolfi", "Shivaprasad Varakkoth", "Andrea Farina", "Cosimo D'Andrea", "Simon Arridge"], "title": "Learned Single-Pixel Fluorescence Microscopy", "comment": "10 pages, 6 figures, 1 table", "summary": "Single-pixel imaging has emerged as a key technique in fluorescence\nmicroscopy, where fast acquisition and reconstruction are crucial. In this\ncontext, images are reconstructed from linearly compressed measurements. In\npractice, total variation minimisation is still used to reconstruct the image\nfrom noisy measurements of the inner product between orthogonal sampling\npattern vectors and the original image data. However, data can be leveraged to\nlearn the measurement vectors and the reconstruction process, thereby enhancing\ncompression, reconstruction quality, and speed. We train an autoencoder through\nself-supervision to learn an encoder (or measurement matrix) and a decoder. We\nthen test it on physically acquired multispectral and intensity data. During\nacquisition, the learned encoder becomes part of the physical device. Our\napproach can enhance single-pixel imaging in fluorescence microscopy by\nreducing reconstruction time by two orders of magnitude, achieving superior\nimage quality, and enabling multispectral reconstructions. Ultimately, learned\nsingle-pixel fluorescence microscopy could advance diagnosis and biological\nresearch, providing multispectral imaging at a fraction of the cost.", "AI": {"tldr": "该研究提出了一种基于自监督自编码器的单像素荧光显微成像方法，旨在显著提高图像重建速度和质量，并支持多光谱成像。", "motivation": "单像素成像在荧光显微镜中至关重要，但现有方法（如总变分最小化）在从带噪声测量中重建图像时，速度和重建质量仍有提升空间。研究旨在利用数据学习测量向量和重建过程，以增强压缩、重建质量和速度。", "method": "通过自监督训练一个自编码器，学习编码器（即测量矩阵）和解码器。然后，将训练好的模型应用于物理获取的多光谱和强度数据进行测试。在数据采集过程中，学习到的编码器被整合到物理设备中。", "result": "该方法将重建时间缩短了两个数量级，实现了卓越的图像质量，并能够进行多光谱重建。", "conclusion": "学习型单像素荧光显微成像技术能够以更低的成本提供多光谱成像，从而推动诊断和生物学研究的进展。"}}
{"id": "2507.18788", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18788", "abs": "https://arxiv.org/abs/2507.18788", "authors": ["Hitesh Kumar Gupta"], "title": "Tell Me What You See: An Iterative Deep Learning Framework for Image Captioning", "comment": "16 pages, 12 total figures (including a 7-figure appendix), 4 tables", "summary": "Image captioning, a task at the confluence of computer vision and natural\nlanguage processing, requires a sophisticated understanding of both visual\nscenes and linguistic structure. While modern approaches are dominated by\nlarge-scale Transformer architectures, this paper documents a systematic,\niterative development of foundational image captioning models, progressing from\na simple CNN-LSTM encoder-decoder to a competitive attention-based system. We\npresent a series of five models, beginning with Genesis and concluding with\nNexus, an advanced model featuring an EfficientNetV2B3 backbone and a dynamic\nattention mechanism. Our experiments chart the impact of architectural\nenhancements and demonstrate a key finding within the classic CNN-LSTM\nparadigm: merely upgrading the visual backbone without a corresponding\nattention mechanism can degrade performance, as the single-vector bottleneck\ncannot transmit the richer visual detail. This insight validates the\narchitectural shift to attention. Trained on the MS COCO 2017 dataset, our\nfinal model, Nexus, achieves a BLEU-4 score of 31.4, surpassing several\nfoundational benchmarks and validating our iterative design process. This work\nprovides a clear, replicable blueprint for understanding the core architectural\nprinciples that underpin modern vision-language tasks.", "AI": {"tldr": "本文系统地迭代开发了图像字幕模型，从简单的CNN-LSTM到基于注意力机制的系统，揭示了升级视觉骨干网络时注意力机制的重要性，并提供了一个可复现的架构设计蓝图。", "motivation": "研究旨在通过系统、迭代的方式开发基础图像字幕模型，以深入理解视觉场景和语言结构在图像字幕任务中的结合，并探索不同架构增强（特别是视觉骨干和注意力机制）对模型性能的影响。", "method": "论文采用迭代开发方法，构建了从Genesis到Nexus共五种模型。起始模型为简单的CNN-LSTM编码器-解码器架构，逐步引入并增强注意力机制。最终模型Nexus采用了EfficientNetV2B3作为视觉骨干网络，并结合了动态注意力机制。所有模型均在MS COCO 2017数据集上进行训练。", "result": "研究发现，在经典CNN-LSTM范式中，仅仅升级视觉骨干网络而没有相应的注意力机制，可能会导致性能下降，因为单向量瓶颈无法有效传输更丰富的视觉细节。这一发现验证了向注意力机制架构转变的必要性。最终模型Nexus在MS COCO 2017数据集上取得了31.4的BLEU-4分数，超越了多个基础基准。", "conclusion": "该研究提供了一个清晰、可复现的蓝图，用于理解支撑现代视觉-语言任务的核心架构原理。迭代设计过程被验证是有效的，并且强调了在图像字幕任务中，当升级视觉特征提取能力时，注意力机制对于有效利用丰富视觉信息至关重要。"}}
{"id": "2507.18956", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18956", "abs": "https://arxiv.org/abs/2507.18956", "authors": ["Sang Min Jung", "Kaixiang Zhang", "Cristian Danescu-Niculescu-Mizil"], "title": "A Similarity Measure for Comparing Conversational Dynamics", "comment": "Code and demos available in ConvoKit (https://convokit.cornell.edu/)", "summary": "The quality of a conversation goes beyond the individual quality of each\nreply, and instead emerges from how these combine into interactional patterns\nthat give the conversation its distinctive overall \"shape\". However, there is\nno robust automated method for comparing conversations in terms of their\noverall interactional dynamics. Such methods could enhance the analysis of\nconversational data and help evaluate conversational agents more holistically.\n  In this work, we introduce a similarity measure for comparing conversations\nwith respect to their dynamics. We design a validation framework for testing\nthe robustness of the metric in capturing differences in conversation dynamics\nand for assessing its sensitivity to the topic of the conversations. Finally,\nto illustrate the measure's utility, we use it to analyze conversational\ndynamics in a large online community, bringing new insights into the role of\nsituational power in conversations.", "AI": {"tldr": "提出了一种衡量对话动态相似性的方法，用于自动化比较对话的整体交互模式。", "motivation": "现有方法无法鲁棒地自动化比较对话的整体交互动态，这限制了对话数据分析和对话代理评估的全面性。对话质量不仅取决于单条回复，更在于交互模式形成的整体“形状”。", "method": "引入了一种新的对话动态相似性度量方法；设计了一个验证框架来测试该度量的鲁棒性及其对对话主题的敏感性；将该度量应用于分析大型在线社区的对话动态。", "result": "该度量方法成功应用于分析大型在线社区的对话动态，为情境权力在对话中的作用提供了新见解。", "conclusion": "该方法提供了一种有效且鲁棒的工具，用于比较对话的整体交互动态，并能揭示影响对话模式的深层因素，例如情境权力。"}}
{"id": "2507.18881", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18881", "abs": "https://arxiv.org/abs/2507.18881", "authors": ["Bolei Chen", "Jiaxu Kang", "Haonan Yang", "Ping Zhong", "Jianxin Wang"], "title": "Perspective from a Higher Dimension: Can 3D Geometric Priors Help Visual Floorplan Localization?", "comment": "Accepted by ACM MM 2025", "summary": "Since a building's floorplans are easily accessible, consistent over time,\nand inherently robust to changes in visual appearance, self-localization within\nthe floorplan has attracted researchers' interest. However, since floorplans\nare minimalist representations of a building's structure, modal and geometric\ndifferences between visual perceptions and floorplans pose challenges to this\ntask. While existing methods cleverly utilize 2D geometric features and pose\nfilters to achieve promising performance, they fail to address the localization\nerrors caused by frequent visual changes and view occlusions due to variously\nshaped 3D objects. To tackle these issues, this paper views the 2D Floorplan\nLocalization (FLoc) problem from a higher dimension by injecting 3D geometric\npriors into the visual FLoc algorithm. For the 3D geometric prior modeling, we\nfirst model geometrically aware view invariance using multi-view constraints,\ni.e., leveraging imaging geometric principles to provide matching constraints\nbetween multiple images that see the same points. Then, we further model the\nview-scene aligned geometric priors, enhancing the cross-modal geometry-color\ncorrespondences by associating the scene's surface reconstruction with the RGB\nframes of the sequence. Both 3D priors are modeled through self-supervised\ncontrastive learning, thus no additional geometric or semantic annotations are\nrequired. These 3D priors summarized in extensive realistic scenes bridge the\nmodal gap while improving localization success without increasing the\ncomputational burden on the FLoc algorithm. Sufficient comparative studies\ndemonstrate that our method significantly outperforms state-of-the-art methods\nand substantially boosts the FLoc accuracy. All data and code will be released\nafter the anonymous review.", "AI": {"tldr": "该论文提出一种通过注入3D几何先验（包括几何感知视图不变性和视图-场景对齐几何先验）来提升2D平面图定位（FLoc）准确性的方法，这些先验通过自监督对比学习获得，无需额外标注，且不增加计算负担。", "motivation": "建筑物平面图易于获取、稳定且对视觉变化鲁棒，因此基于平面图的自定位受到关注。然而，平面图是建筑结构的极简表示，导致视觉感知与平面图之间存在模态和几何差异。现有方法未能解决频繁视觉变化和3D物体遮挡导致的定位误差。", "method": "该方法从更高维度审视2D平面图定位问题，通过注入3D几何先验。具体包括：1. 利用多视图约束（成像几何原理）建模几何感知视图不变性，提供多图像间的匹配约束。2. 通过将场景的表面重建与序列的RGB帧关联，进一步建模视图-场景对齐的几何先验。这两种3D先验均通过自监督对比学习建模，无需额外的几何或语义标注。", "result": "该方法能够弥合模态鸿沟，提高定位成功率，且不增加FLoc算法的计算负担。充分的比较研究表明，该方法显著优于现有最先进的方法，并大幅提升了FLoc的准确性。", "conclusion": "通过自监督学习注入3D几何先验（包括视图不变性和视图-场景对齐），能够有效弥合视觉感知与平面图之间的模态差异，显著提高2D平面图定位的准确性，且无需额外标注或增加计算开销。"}}
{"id": "2507.18741", "categories": ["cs.CV", "cs.DL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.18741", "abs": "https://arxiv.org/abs/2507.18741", "authors": ["Tristan Repolusk", "Eduardo Veas"], "title": "KuiSCIMA v2.0: Improved Baselines, Calibration, and Cross-Notation Generalization for Historical Chinese Music Notations in Jiang Kui's Baishidaoren Gequ", "comment": "International Conference on Document Analysis and Recognition. This\n  preprint has not undergone any post-submission improvements or corrections.\n  The Version of Record of this contribution is published in \"19th\n  International Conference on Document Analysis and Recognition (ICDAR 2025),\n  Wuhan, China, September 16-21, 2025, Proceedings\", and is available online at\n  the External DOI field below", "summary": "Optical Music Recognition (OMR) for historical Chinese musical notations,\nsuch as suzipu and l\\\"ul\\\"upu, presents unique challenges due to high class\nimbalance and limited training data. This paper introduces significant\nadvancements in OMR for Jiang Kui's influential collection Baishidaoren Gequ\nfrom 1202. In this work, we develop and evaluate a character recognition model\nfor scarce imbalanced data. We improve upon previous baselines by reducing the\nCharacter Error Rate (CER) from 10.4% to 7.1% for suzipu, despite working with\n77 highly imbalanced classes, and achieve a remarkable CER of 0.9% for\nl\\\"ul\\\"upu. Our models outperform human transcribers, with an average human CER\nof 15.9% and a best-case CER of 7.6%. We employ temperature scaling to achieve\na well-calibrated model with an Expected Calibration Error (ECE) below 0.0162.\nUsing a leave-one-edition-out cross-validation approach, we ensure robust\nperformance across five historical editions. Additionally, we extend the\nKuiSCIMA dataset to include all 109 pieces from Baishidaoren Gequ, encompassing\nsuzipu, l\\\"ul\\\"upu, and jianzipu notations. Our findings advance the\ndigitization and accessibility of historical Chinese music, promoting cultural\ndiversity in OMR and expanding its applicability to underrepresented music\ntraditions.", "AI": {"tldr": "本文针对历史中文乐谱（如俗字谱和律吕谱）的光学音乐识别（OMR）挑战，提出了一种字符识别模型，显著降低了错误率，甚至超越了人类转录员，并扩展了相关数据集，促进了历史中文音乐的数字化和可访问性。", "motivation": "历史中文乐谱（如俗字谱和律吕谱）的光学音乐识别（OMR）面临高类别不平衡和训练数据有限的独特挑战。", "method": "开发并评估了一个针对稀疏不平衡数据的字符识别模型；采用温度标定法实现模型校准；使用留一版本交叉验证法确保在多个历史版本上的鲁棒性；扩展了KuiSCIMA数据集，包含了《白石道人歌曲》中的所有109首作品。", "result": "将俗字谱的字符错误率（CER）从10.4%降至7.1%（处理77个高度不平衡类别），律吕谱的CER达到0.9%；模型表现优于人类转录员（人类平均CER为15.9%，最佳为7.6%）；模型校准良好，预期校准误差（ECE）低于0.0162。", "conclusion": "研究成果推动了历史中文音乐的数字化和可访问性，促进了OMR领域的文化多样性，并将其适用性扩展到代表性不足的音乐传统。"}}
{"id": "2507.18815", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18815", "abs": "https://arxiv.org/abs/2507.18815", "authors": ["Benjamin Carter", "Nathan Dilla", "Micheal Callahan", "Atuhaire Ambala"], "title": "Deepfake Detection Via Facial Feature Extraction and Modeling", "comment": "Keywords: deepfake, facial recognition, feature extraction,\n  artificial intelligence, recurrent neural network, convolutional neural\n  network, artificial neural network", "summary": "The rise of deepfake technology brings forth new questions about the\nauthenticity of various forms of media found online today. Videos and images\ngenerated by artificial intelligence (AI) have become increasingly more\ndifficult to differentiate from genuine media, resulting in the need for new\nmodels to detect artificially-generated media. While many models have attempted\nto solve this, most focus on direct image processing, adapting a convolutional\nneural network (CNN) or a recurrent neural network (RNN) that directly\ninteracts with the video image data. This paper introduces an approach of using\nsolely facial landmarks for deepfake detection. Using a dataset consisting of\nboth deepfake and genuine videos of human faces, this paper describes an\napproach for extracting facial landmarks for deepfake detection, focusing on\nidentifying subtle inconsistencies in facial movements instead of raw image\nprocessing. Experimental results demonstrated that this feature extraction\ntechnique is effective in various neural network models, with the same facial\nlandmarks tested on three neural network models, with promising performance\nmetrics indicating its potential for real-world applications. The findings\ndiscussed in this paper include RNN and artificial neural network (ANN) models\nwith accuracy between 96% and 93%, respectively, with a CNN model hovering\naround 78%. This research challenges the assumption that raw image processing\nis necessary to identify deepfake videos by presenting a facial feature\nextraction approach compatible with various neural network models while\nrequiring fewer parameters.", "AI": {"tldr": "该研究提出一种基于面部特征点而非原始图像处理的深度伪造检测方法，通过分析面部运动的细微不一致性，在不同神经网络模型上取得了高准确率。", "motivation": "深度伪造技术使AI生成媒体与真实媒体越来越难以区分，现有检测模型多依赖于直接的图像处理（如CNN、RNN与视频图像数据直接交互），需要新的、更高效的模型来识别伪造媒体。", "method": "该方法从深度伪造和真实人脸视频数据集中提取面部特征点，专注于识别面部运动中的细微不一致性，而非原始图像像素。提取的特征点数据随后用于训练和测试多种神经网络模型，包括循环神经网络（RNN）、人工神经网络（ANN）和卷积神经网络（CNN）。", "result": "实验结果表明，该面部特征点提取技术对各种神经网络模型均有效。在相同特征点数据下，RNN模型准确率达到96%，ANN模型达到93%，CNN模型约为78%。该方法所需参数更少，且性能表现良好，具有实际应用潜力。", "conclusion": "该研究挑战了识别深度伪造视频需要原始图像处理的假设，证明了基于面部特征点提取的方法是可行的，并且与多种神经网络模型兼容，同时所需参数更少，为深度伪造检测提供了一种高效的新途径。"}}
{"id": "2507.18973", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18973", "abs": "https://arxiv.org/abs/2507.18973", "authors": ["Bohan Yao", "Vikas Yadav"], "title": "A Toolbox, Not a Hammer -- Multi-TAG: Scaling Math Reasoning with Multi-Tool Aggregation", "comment": "21 pages, 3 figures", "summary": "Augmenting large language models (LLMs) with external tools is a promising\navenue for developing high-performance mathematical reasoning systems. Prior\ntool-augmented approaches typically finetune an LLM to select and invoke a\nsingle tool at each reasoning step and show promising results on simpler math\nreasoning benchmarks such as GSM8K. However, these approaches struggle with\nmore complex math problems that require precise reasoning over multiple steps.\nTo address this limitation, in this work, we propose Multi-TAG, a Multi-Tool\nAGgregation-based framework. Instead of relying on a single tool, Multi-TAG\nguides an LLM to concurrently invoke multiple tools at each reasoning step. It\nthen aggregates their diverse outputs to verify and refine the reasoning\nprocess, enhancing solution robustness and accuracy. Notably, Multi-TAG is a\nfinetuning-free, inference-only framework, making it readily applicable to any\nLLM backbone, including large open-weight models which are computationally\nexpensive to finetune and proprietary frontier models which cannot be finetuned\nwith custom recipes. We evaluate Multi-TAG on four challenging benchmarks:\nMATH500, AIME, AMC, and OlympiadBench. Across both open-weight and\nclosed-source LLM backbones, Multi-TAG consistently and substantially\noutperforms state-of-the-art baselines, achieving average improvements of 6.0%\nto 7.5% over state-of-the-art baselines.", "AI": {"tldr": "Multi-TAG是一个免微调的推理框架，通过引导LLM在每个推理步骤并发调用多个工具并聚合其输出，以增强数学推理的鲁棒性和准确性，尤其针对复杂数学问题。", "motivation": "现有工具增强型LLM方法通常在每个推理步骤只微调LLM选择并调用单个工具，这在简单数学推理基准上表现良好，但在需要多步精确推理的复杂数学问题上表现不佳。", "method": "Multi-TAG框架不依赖于单个工具，而是引导LLM在每个推理步骤并发调用多个工具。然后，它聚合这些工具的多样化输出，以验证和完善推理过程，从而增强解决方案的鲁棒性和准确性。Multi-TAG是免微调、仅推理的框架，适用于任何LLM骨干模型。", "result": "在MATH500、AIME、AMC和OlympiadBench这四个挑战性基准测试中，Multi-TAG在开源和闭源LLM骨干模型上均持续且显著优于现有最先进的基线方法，平均性能提升6.0%至7.5%。", "conclusion": "Multi-TAG通过并发使用和聚合多个工具的输出，有效解决了LLM在复杂数学推理中单工具限制的问题，显著提升了数学推理系统的性能和准确性，且无需微调，具有广泛适用性。"}}
{"id": "2507.19354", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19354", "abs": "https://arxiv.org/abs/2507.19354", "authors": ["Melih Yazgan", "Allen Xavier Arasan", "J. Marius Zöllner"], "title": "EffiComm: Bandwidth Efficient Multi Agent Communication", "comment": "Accepted for publication at ITSC 2025", "summary": "Collaborative perception allows connected vehicles to exchange sensor\ninformation and overcome each vehicle's blind spots. Yet transmitting raw point\nclouds or full feature maps overwhelms Vehicle-to-Vehicle (V2V) communications,\ncausing latency and scalability problems. We introduce EffiComm, an end-to-end\nframework that transmits less than 40% of the data required by prior art while\nmaintaining state-of-the-art 3D object detection accuracy. EffiComm operates on\nBird's-Eye-View (BEV) feature maps from any modality and applies a two-stage\nreduction pipeline: (1) Selective Transmission (ST) prunes low-utility regions\nwith a confidence mask; (2) Adaptive Grid Reduction (AGR) uses a Graph Neural\nNetwork (GNN) to assign vehicle-specific keep ratios according to role and\nnetwork load. The remaining features are fused with a soft-gated\nMixture-of-Experts (MoE) attention layer, offering greater capacity and\nspecialization for effective feature integration. On the OPV2V benchmark,\nEffiComm reaches 0.84 mAP@0.7 while sending only an average of approximately\n1.5 MB per frame, outperforming previous methods on the accuracy-per-bit curve.\nThese results highlight the value of adaptive, learned communication for\nscalable Vehicle-to-Everything (V2X) perception.", "AI": {"tldr": "EffiComm是一个端到端框架，通过两阶段数据缩减和MoE融合，显著减少协作感知中的V2V通信数据量，同时保持先进的3D目标检测精度。", "motivation": "在协作感知中，传输原始点云或完整特征图会使V2V通信过载，导致延迟和可扩展性问题，限制了车辆克服盲点和实现互联感知的能力。", "method": "EffiComm在BEV特征图上运行，采用两阶段数据缩减：1) 选择性传输（ST）使用置信度掩码修剪低效用区域；2) 自适应网格缩减（AGR）利用图神经网络（GNN）根据车辆角色和网络负载分配特定车辆的保留比率。剩余特征通过软门控专家混合（MoE）注意力层进行融合。", "result": "在OPV2V基准测试中，EffiComm在保持最先进的3D目标检测精度（0.84 mAP@0.7）的同时，将数据传输量减少到先前方法的40%以下，平均每帧仅传输约1.5 MB，在精度-每比特曲线上优于现有方法。", "conclusion": "这些结果强调了自适应、学习型通信对于可扩展的V2X感知的重要价值。"}}
{"id": "2507.18743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18743", "abs": "https://arxiv.org/abs/2507.18743", "authors": ["Xinjun Cheng", "Yiguo He", "Junjie Zhu", "Chunping Qiu", "Jun Wang", "Qiangjuan Huang", "Ke Yang"], "title": "SAR-TEXT: A Large-Scale SAR Image-Text Dataset Built with SAR-Narrator and Progressive Transfer Learning", "comment": "IEEE Submission", "summary": "Vision Language Models (VLMs) have achieved remarkable breakthroughs in the\nfield of remote sensing in recent years. Synthetic Aperture Radar (SAR)\nimagery, with its all-weather capability, is essential in remote sensing, yet\nthe lack of large-scale, high-quality SAR image-text datasets hinders its\nsemantic understanding. In this paper, we construct SAR-Text, a large-scale and\nhigh-quality dataset consisting of over 130,000 SAR image-text pairs. To\nconstruct the SAR-Text dataset, we design the SAR-Narrator framework, which\ngenerates textual descriptions for SAR images through a multi-stage progressive\ntransfer learning strategy. To verify the effectiveness of the SAR-TEXT\ndataset, we conduct experiments on three typical vision-language tasks:\nimage-text retrieval, image captioning, and visual question answering (VQA).\nSpecifically, we construct three representative models on SAR-TEXT:\nSAR-RS-CLIP, SAR-RS-CoCa, and SAR-GPT. SAR-RS-CLIP achieves notable\nimprovements in retrieval performance, boosting average recall by 16.43% and\n10.54% on the OSdataset-512 and HRSID test sets, respectively. In the\ncaptioning task, SAR-RS-CoCa achieves BLEU-4, SPICE, and CIDEr scores exceeding\nthose of the original CoCa model by more than 8x, 4x, and 10x, respectively. In\nthe VQA task, SAR-GPT outperforms baseline and single-stage models on multiple\nSAR-VQA datasets, demonstrating stronger semantic understanding and reasoning\nability, as further confirmed by qualitative results. It is worth noting that,\nas a flexible captioning tool, SAR-Narrator can be readily adopted by the\ncommunity to construct larger-scale SAR image-text datasets.", "AI": {"tldr": "该论文构建了一个大规模、高质量的SAR图像-文本数据集SAR-Text，并通过SAR-Narrator框架生成文本描述。实验证明，基于SAR-Text训练的VLM模型在图像-文本检索、图像字幕生成和视觉问答任务上均取得了显著提升。", "motivation": "合成孔径雷达（SAR）图像在遥感领域至关重要，但缺乏大规模、高质量的SAR图像-文本数据集，这阻碍了对其语义的理解和视觉语言模型（VLMs）在SAR领域的应用。", "method": "研究人员构建了包含超过13万对SAR图像-文本对的SAR-Text数据集。为此，他们设计了SAR-Narrator框架，通过多阶段渐进式迁移学习策略为SAR图像生成文本描述。为验证数据集有效性，在SAR-Text上构建了SAR-RS-CLIP、SAR-RS-CoCa和SAR-GPT三个代表性模型，并在图像-文本检索、图像字幕生成和视觉问答（VQA）三个典型视觉-语言任务上进行实验。", "result": "SAR-RS-CLIP在检索性能上显著提升，在OSdataset-512和HRSID测试集上平均召回率分别提升了16.43%和10.54%。在字幕生成任务中，SAR-RS-CoCa的BLEU-4、SPICE和CIDEr分数比原始CoCa模型分别高出8倍、4倍和10倍以上。在VQA任务中，SAR-GPT在多个SAR-VQA数据集上优于基线和单阶段模型，表现出更强的语义理解和推理能力。", "conclusion": "SAR-Text数据集和SAR-Narrator框架有效提升了SAR图像的语义理解和VLM模型在相关任务上的性能。SAR-Narrator作为一个灵活的字幕工具，可供社区采用以构建更大规模的SAR图像-文本数据集。"}}
{"id": "2507.18838", "categories": ["cs.CV", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.18838", "abs": "https://arxiv.org/abs/2507.18838", "authors": ["Fabio De Sousa Ribeiro", "Omar Todd", "Charles Jones", "Avinash Kori", "Raghav Mehta", "Ben Glocker"], "title": "Flow Stochastic Segmentation Networks", "comment": "Accepted at ICCV 2025", "summary": "We introduce the Flow Stochastic Segmentation Network (Flow-SSN), a\ngenerative segmentation model family featuring discrete-time autoregressive and\nmodern continuous-time flow variants. We prove fundamental limitations of the\nlow-rank parameterisation of previous methods and show that Flow-SSNs can\nestimate arbitrarily high-rank pixel-wise covariances without assuming the rank\nor storing the distributional parameters. Flow-SSNs are also more efficient to\nsample from than standard diffusion-based segmentation models, thanks to most\nof the model capacity being allocated to learning the base distribution of the\nflow, constituting an expressive prior. We apply Flow-SSNs to challenging\nmedical imaging benchmarks and achieve state-of-the-art results. Code\navailable: https://github.com/biomedia-mira/flow-ssn.", "AI": {"tldr": "本文提出了Flow-SSN，一种生成式分割模型家族，包含离散时间自回归和连续时间流变体。它克服了现有方法低秩参数化的限制，能估计任意高秩像素协方差，且采样效率更高，在医学图像分割上达到了SOTA。", "motivation": "现有分割方法存在低秩参数化的根本限制，无法估计高秩像素级协方差，且基于扩散的采样效率不高。研究旨在开发一种能克服这些限制的更高效、更强大的生成式分割模型。", "method": "Flow-SSN是一种生成式分割模型，包含离散时间自回归和现代连续时间流变体。它被证明能够估计任意高秩的像素级协方差，无需假设秩或存储分布参数。模型将大部分容量用于学习流的基础分布，形成富有表现力的先验，从而提高了采样效率。", "result": "Flow-SSN在具有挑战性的医学图像基准测试中取得了最先进（state-of-the-art）的结果。", "conclusion": "Flow-SSN通过结合流模型和高效的容量分配，成功克服了传统分割模型在处理高秩协方差和采样效率方面的局限性，在医学图像分割任务中展现出卓越的性能和潜力。"}}
{"id": "2507.19081", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19081", "abs": "https://arxiv.org/abs/2507.19081", "authors": ["Hao Li", "Yizheng Sun", "Viktor Schlegel", "Kailai Yang", "Riza Batista-Navarro", "Goran Nenadic"], "title": "Arg-LLaDA: Argument Summarization via Large Language Diffusion Models and Sufficiency-Aware Refinement", "comment": "Preprint", "summary": "Argument summarization aims to generate concise, structured representations\nof complex, multi-perspective debates. While recent work has advanced the\nidentification and clustering of argumentative components, the generation stage\nremains underexplored. Existing approaches typically rely on single-pass\ngeneration, offering limited support for factual correction or structural\nrefinement. To address this gap, we introduce Arg-LLaDA, a novel large language\ndiffusion framework that iteratively improves summaries via sufficiency-guided\nremasking and regeneration. Our method combines a flexible masking controller\nwith a sufficiency-checking module to identify and revise unsupported,\nredundant, or incomplete spans, yielding more faithful, concise, and coherent\noutputs. Empirical results on two benchmark datasets demonstrate that Arg-LLaDA\nsurpasses state-of-the-art baselines in 7 out of 10 automatic evaluation\nmetrics. In addition, human evaluations reveal substantial improvements across\ncore dimensions, coverage, faithfulness, and conciseness, validating the\neffectiveness of our iterative, sufficiency-aware generation strategy.", "AI": {"tldr": "提出Arg-LLaDA框架，利用大语言扩散模型迭代地改进论点摘要，通过充分性引导的重掩码和再生，提高摘要的忠实性、简洁性和连贯性。", "motivation": "论点摘要的生成阶段研究不足，现有方法通常采用单次生成，难以支持事实纠正或结构细化，导致摘要质量受限。", "method": "引入Arg-LLaDA，一个新颖的大语言扩散框架。该方法通过充分性引导的重掩码和再生，迭代地改进摘要。它结合了一个灵活的掩码控制器和一个充分性检查模块，用于识别和修改不受支持、冗余或不完整的文本片段。", "result": "在两个基准数据集上的实证结果表明，Arg-LLaDA在10项自动评估指标中有7项超越了最先进的基线。此外，人工评估显示在核心维度（覆盖率、忠实性、简洁性）上都有显著改进。", "conclusion": "Arg-LLaDA的迭代式、充分性感知的生成策略有效提高了论点摘要的质量，生成了更忠实、简洁和连贯的输出。"}}
{"id": "2507.19459", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19459", "abs": "https://arxiv.org/abs/2507.19459", "authors": ["Pol Francesch Huc", "Emily Bates", "Simone D'Amico"], "title": "Fast Learning of Non-Cooperative Spacecraft 3D Models through Primitive Initialization", "comment": null, "summary": "The advent of novel view synthesis techniques such as NeRF and 3D Gaussian\nSplatting (3DGS) has enabled learning precise 3D models only from posed\nmonocular images. Although these methods are attractive, they hold two major\nlimitations that prevent their use in space applications: they require poses\nduring training, and have high computational cost at training and inference. To\naddress these limitations, this work contributes: (1) a Convolutional Neural\nNetwork (CNN) based primitive initializer for 3DGS using monocular images; (2)\na pipeline capable of training with noisy or implicit pose estimates; and (3)\nand analysis of initialization variants that reduce the training cost of\nprecise 3D models. A CNN takes a single image as input and outputs a coarse 3D\nmodel represented as an assembly of primitives, along with the target's pose\nrelative to the camera. This assembly of primitives is then used to initialize\n3DGS, significantly reducing the number of training iterations and input images\nneeded -- by at least an order of magnitude. For additional flexibility, the\nCNN component has multiple variants with different pose estimation techniques.\nThis work performs a comparison between these variants, evaluating their\neffectiveness for downstream 3DGS training under noisy or implicit pose\nestimates. The results demonstrate that even with imperfect pose supervision,\nthe pipeline is able to learn high-fidelity 3D representations, opening the\ndoor for the use of novel view synthesis in space applications.", "AI": {"tldr": "该研究提出一种基于CNN的3D Gaussian Splatting（3DGS）初始化方法，可从单目图像生成粗糙3D模型和姿态，显著降低训练成本，并能在存在噪声或隐式姿态估计的情况下学习高保真3D表示，从而推动新视角合成技术在空间应用中的部署。", "motivation": "现有的新视角合成技术（如NeRF和3DGS）在空间应用中存在两大限制：训练时需要精确的相机姿态，以及训练和推理的计算成本高昂。", "method": "1. 提出一个基于卷积神经网络（CNN）的3DGS基元初始化器，该CNN以单张图像为输入，输出一个由基元组成的粗糙3D模型以及目标相对于相机的姿态。2. 设计了一个能够使用噪声或隐式姿态估计进行训练的管道。3. 分析了不同的初始化变体，以降低精确3D模型的训练成本。该方法通过CNN初始化显著减少了3DGS所需的训练迭代次数和输入图像数量。", "result": "该方法将训练迭代次数和所需输入图像数量减少了至少一个数量级。即使在不完美的姿态监督下，该管道也能够学习到高保真度的3D表示。", "conclusion": "本研究通过解决姿态依赖和计算成本问题，成功使新视角合成技术能够在空间应用中实现高保真3D表示学习，即使在姿态估计不完美的情况下也能有效工作。"}}
{"id": "2507.18758", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18758", "abs": "https://arxiv.org/abs/2507.18758", "authors": ["Yifan Liu", "Shengjun Zhang", "Chensheng Dai", "Yang Chen", "Hao Liu", "Chen Li", "Yueqi Duan"], "title": "Learning Efficient and Generalizable Human Representation with Human Gaussian Model", "comment": null, "summary": "Modeling animatable human avatars from videos is a long-standing and\nchallenging problem. While conventional methods require per-instance\noptimization, recent feed-forward methods have been proposed to generate 3D\nGaussians with a learnable network. However, these methods predict Gaussians\nfor each frame independently, without fully capturing the relations of\nGaussians from different timestamps. To address this, we propose Human Gaussian\nGraph to model the connection between predicted Gaussians and human SMPL mesh,\nso that we can leverage information from all frames to recover an animatable\nhuman representation. Specifically, the Human Gaussian Graph contains dual\nlayers where Gaussians are the first layer nodes and mesh vertices serve as the\nsecond layer nodes. Based on this structure, we further propose the intra-node\noperation to aggregate various Gaussians connected to one mesh vertex, and\ninter-node operation to support message passing among mesh node neighbors.\nExperimental results on novel view synthesis and novel pose animation\ndemonstrate the efficiency and generalization of our method.", "AI": {"tldr": "本文提出了一种名为Human Gaussian Graph (HGG) 的方法，通过构建高斯点和SMPL网格之间的双层图结构，利用跨帧信息生成可动画的人体替身，解决了现有方法独立处理每帧高斯点的问题。", "motivation": "从视频中建模可动画的人体替身是一个长期存在的挑战。传统方法需要实例级优化，而近期基于可学习网络生成3D高斯点的方法则独立预测每帧高斯点，未能充分捕捉不同时间戳高斯点之间的关系，导致无法有效利用所有帧的信息来恢复可动画的人体表示。", "method": "提出Human Gaussian Graph (HGG) 来建模预测高斯点与人体SMPL网格之间的连接。HGG包含双层结构：高斯点作为第一层节点，网格顶点作为第二层节点。在此基础上，设计了内部节点操作（intra-node operation）来聚合连接到同一网格顶点的各种高斯点，以及节点间操作（inter-node operation）来支持网格节点邻居之间的消息传递。", "result": "在新视角合成（novel view synthesis）和新姿态动画（novel pose animation）任务上的实验结果表明，该方法具有高效性和泛化能力。", "conclusion": "Human Gaussian Graph (HGG) 成功地通过建模高斯点与SMPL网格的连接，并利用双层图结构及相应的节点操作，有效地整合了所有帧的信息，从而实现了可动画人体替身的建模，解决了现有方法在跨帧信息利用上的不足。"}}
{"id": "2507.18848", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18848", "abs": "https://arxiv.org/abs/2507.18848", "authors": ["Beidi Zhao", "SangMook Kim", "Hao Chen", "Chen Zhou", "Zu-hua Gao", "Gang Wang", "Xiaoxiao Li"], "title": "PTCMIL: Multiple Instance Learning via Prompt Token Clustering for Whole Slide Image Analysis", "comment": null, "summary": "Multiple Instance Learning (MIL) has advanced WSI analysis but struggles with\nthe complexity and heterogeneity of WSIs. Existing MIL methods face challenges\nin aggregating diverse patch information into robust WSI representations. While\nViTs and clustering-based approaches show promise, they are computationally\nintensive and fail to capture task-specific and slide-specific variability. To\naddress these limitations, we propose PTCMIL, a novel Prompt Token\nClustering-based ViT for MIL aggregation. By introducing learnable prompt\ntokens into the ViT backbone, PTCMIL unifies clustering and prediction tasks in\nan end-to-end manner. It dynamically aligns clustering with downstream tasks,\nusing projection-based clustering tailored to each WSI, reducing complexity\nwhile preserving patch heterogeneity. Through token merging and prototype-based\npooling, PTCMIL efficiently captures task-relevant patterns. Extensive\nexperiments on eight datasets demonstrate its superior performance in\nclassification and survival analysis tasks, outperforming state-of-the-art\nmethods. Systematic ablation studies confirm its robustness and strong\ninterpretability. The code is released at https://github.com/ubc-tea/PTCMIL.", "AI": {"tldr": "PTCMIL是一种新型的基于Prompt Token聚类的ViT模型，用于多示例学习（MIL）的病理全切片图像（WSI）分析。它通过引入可学习的prompt token，将聚类和预测任务端到端地统一起来，并动态调整聚类以适应下游任务，同时通过token合并和原型池化高效捕获相关模式，在分类和生存分析任务中表现优异。", "motivation": "现有的多示例学习（MIL）方法在处理病理全切片图像（WSI）的复杂性和异质性时面临挑战，难以将多样化的补丁信息聚合为鲁棒的WSI表示。尽管ViTs和基于聚类的方法有前景，但它们计算成本高，并且未能捕获任务特定和切片特定的变异性。", "method": "PTCMIL通过在ViT骨干网络中引入可学习的prompt token，实现了聚类和预测任务的端到端统一。它动态地将聚类与下游任务对齐，采用针对每个WSI定制的基于投影的聚类，以降低复杂性同时保留补丁异质性。模型通过token合并和基于原型的池化来高效捕获任务相关的模式。", "result": "在八个数据集上进行的广泛实验表明，PTCMIL在分类和生存分析任务中表现出卓越的性能，优于现有最先进的方法。系统的消融研究证实了其鲁棒性和强大的可解释性。", "conclusion": "PTCMIL成功解决了现有MIL方法在WSI分析中的局限性，通过统一聚类和预测、动态任务对齐以及高效的模式捕获，实现了卓越的性能和良好的可解释性，为WSI分析提供了一个强大的解决方案。"}}
{"id": "2507.19090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19090", "abs": "https://arxiv.org/abs/2507.19090", "authors": ["Haorui He", "Yupeng Li", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "title": "Debating Truth: Debate-driven Claim Verification with Multiple Large Language Model Agents", "comment": null, "summary": "Claim verification is critical for enhancing digital literacy. However, the\nstate-of-the-art single-LLM methods struggle with complex claim verification\nthat involves multi-faceted evidences. Inspired by real-world fact-checking\npractices, we propose DebateCV, the first claim verification framework that\nadopts a debate-driven methodology using multiple LLM agents. In our framework,\ntwo Debaters take opposing stances on a claim and engage in multi-round\nargumentation, while a Moderator evaluates the arguments and renders a verdict\nwith justifications. To further improve the performance of the Moderator, we\nintroduce a novel post-training strategy that leverages synthetic debate data\ngenerated by the zero-shot DebateCV, effectively addressing the scarcity of\nreal-world debate-driven claim verification data. Experimental results show\nthat our method outperforms existing claim verification methods under varying\nlevels of evidence quality. Our code and dataset are publicly available at\nhttps://anonymous.4open.science/r/DebateCV-6781.", "AI": {"tldr": "提出DebateCV，一个基于多LLM代理的辩论驱动式声明验证框架，通过辩论和仲裁机制处理复杂声明，并利用合成数据对仲裁者进行训练，超越现有方法。", "motivation": "现有单一LLM方法在处理涉及多方面证据的复杂声明验证时表现不佳。", "method": "引入DebateCV框架，包含两个持相反立场的“辩论者”进行多轮论证，一个“仲裁者”评估论证并给出判断。为提升仲裁者性能，采用新颖的后训练策略，利用零样本DebateCV生成的合成辩论数据，解决真实世界数据稀缺问题。", "result": "实验结果表明，该方法在不同证据质量下均优于现有声明验证方法。", "conclusion": "DebateCV通过模拟真实世界事实核查的辩论方法，有效提升了复杂声明验证的性能。"}}
{"id": "2507.19469", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19469", "abs": "https://arxiv.org/abs/2507.19469", "authors": ["João G. Melo", "João P. Mafaldo", "Edna Barros"], "title": "Efficient Lines Detection for Robot Soccer", "comment": "12 pages, 8 figures, RoboCup Symposium 2025", "summary": "Self-localization is essential in robot soccer, where accurate detection of\nvisual field features, such as lines and boundaries, is critical for reliable\npose estimation. This paper presents a lightweight and efficient method for\ndetecting soccer field lines using the ELSED algorithm, extended with a\nclassification step that analyzes RGB color transitions to identify lines\nbelonging to the field. We introduce a pipeline based on Particle Swarm\nOptimization (PSO) for threshold calibration to optimize detection performance,\nrequiring only a small number of annotated samples. Our approach achieves\naccuracy comparable to a state-of-the-art deep learning model while offering\nhigher processing speed, making it well-suited for real-time applications on\nlow-power robotic platforms.", "AI": {"tldr": "本文提出一种轻量高效的足球场线条检测方法，结合ELSED算法和基于RGB颜色转换的分类步骤，并利用PSO进行阈值校准，实现了与SOTA深度学习模型相当的精度，同时处理速度更快，适用于低功耗机器人平台。", "motivation": "在机器人足球中，自定位至关重要，而准确检测视觉场特征（如线条和边界）对于可靠的姿态估计至关重要。现有方法可能无法满足低功耗平台对实时性和效率的要求。", "method": "该方法基于ELSED算法，并扩展了一个分类步骤，通过分析RGB颜色转换来识别属于足球场的线条。引入了基于粒子群优化（PSO）的管道进行阈值校准，仅需少量标注样本即可优化检测性能。", "result": "该方法在准确性方面与最先进的深度学习模型相当，同时提供更高的处理速度。", "conclusion": "所提出的方法由于其高精度和高处理速度，非常适合在低功耗机器人平台上进行实时应用。"}}
{"id": "2507.18830", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18830", "abs": "https://arxiv.org/abs/2507.18830", "authors": ["Shen Zhu", "Yinzhu Jin", "Tyler Spears", "Ifrah Zawar", "P. Thomas Fletcher"], "title": "RealDeal: Enhancing Realism and Details in Brain Image Generation via Image-to-Image Diffusion Models", "comment": "19 pages, 10 figures", "summary": "We propose image-to-image diffusion models that are designed to enhance the\nrealism and details of generated brain images by introducing sharp edges, fine\ntextures, subtle anatomical features, and imaging noise. Generative models have\nbeen widely adopted in the biomedical domain, especially in image generation\napplications. Latent diffusion models achieve state-of-the-art results in\ngenerating brain MRIs. However, due to latent compression, generated images\nfrom these models are overly smooth, lacking fine anatomical structures and\nscan acquisition noise that are typically seen in real images. This work\nformulates the realism enhancing and detail adding process as image-to-image\ndiffusion models, which refines the quality of LDM-generated images. We employ\ncommonly used metrics like FID and LPIPS for image realism assessment.\nFurthermore, we introduce new metrics to demonstrate the realism of images\ngenerated by RealDeal in terms of image noise distribution, sharpness, and\ntexture.", "AI": {"tldr": "本文提出了一种图像到图像的扩散模型（RealDeal），用于增强生成脑部图像的真实感和细节，解决现有模型生成图像过于平滑的问题。", "motivation": "生成模型在生物医学图像生成中应用广泛，尽管潜在扩散模型（LDM）在生成脑部MRI方面表现出色，但由于潜在压缩，其生成的图像过于平滑，缺乏真实图像中常见的精细解剖结构和扫描采集噪声。", "method": "将真实感增强和细节添加过程表述为图像到图像的扩散模型，用于细化LDM生成的图像质量。除了使用FID和LPIPS等常用指标评估图像真实感外，还引入了新的指标来评估生成图像在图像噪声分布、锐度和纹理方面的真实感。", "result": "通过引入图像到图像的扩散模型，能够为生成的脑部图像引入锐利边缘、精细纹理、微妙解剖特征和成像噪声，有效增强了图像的真实感和细节。新的评估指标进一步证明了模型在图像噪声、锐度和纹理方面的真实性。", "conclusion": "所提出的图像到图像扩散模型能够有效提高生成脑部图像的真实感和细节，弥补了现有潜在扩散模型生成图像过于平滑的不足，并通过新旧指标证明了其有效性。"}}
{"id": "2507.18925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18925", "abs": "https://arxiv.org/abs/2507.18925", "authors": ["Heitor R. Medeiros", "Atif Belal", "Masih Aminbeidokhti", "Eric Granger", "Marco Pedersoli"], "title": "WiSE-OD: Benchmarking Robustness in Infrared Object Detection", "comment": "8 pages, conference", "summary": "Object detection (OD) in infrared (IR) imagery is critical for low-light and\nnighttime applications. However, the scarcity of large-scale IR datasets forces\nmodels to rely on weights pre-trained on RGB images. While fine-tuning on IR\nimproves accuracy, it often compromises robustness under distribution shifts\ndue to the inherent modality gap between RGB and IR. To address this, we\nintroduce LLVIP-C and FLIR-C, two cross-modality out-of-distribution (OOD)\nbenchmarks built by applying corruption to standard IR datasets. Additionally,\nto fully leverage the complementary knowledge from RGB and infrared trained\nmodels, we propose WiSE-OD, a weight-space ensembling method with two variants:\nWiSE-OD$_{ZS}$, which combines RGB zero-shot and IR fine-tuned weights, and\nWiSE-OD$_{LP}$, which blends zero-shot and linear probing. Evaluated across\nthree RGB-pretrained detectors and two robust baselines, WiSE-OD improves both\ncross-modality and corruption robustness without any additional training or\ninference cost.", "AI": {"tldr": "针对红外图像目标检测中RGB预训练模型存在的跨模态和分布偏移鲁棒性问题，本文引入了两个新的跨模态OOD基准，并提出了一种名为WiSE-OD的权重空间集成方法，该方法在不增加训练或推理成本的情况下显著提升了鲁棒性。", "motivation": "红外图像目标检测对低光和夜间应用至关重要，但由于缺乏大规模红外数据集，模型通常依赖于RGB图像预训练权重。尽管在红外数据上微调可以提高准确性，但RGB和红外之间固有的模态差异导致模型在分布偏移下鲁棒性不佳。", "method": "1. 构建了两个跨模态OOD基准数据集LLVIP-C和FLIR-C，通过对标准红外数据集应用图像损坏来模拟分布偏移。2. 提出了一种权重空间集成方法WiSE-OD，旨在充分利用RGB预训练模型和红外微调模型的互补知识。WiSE-OD包含两种变体：WiSE-OD$_{ZS}$（结合RGB零样本和红外微调权重）和WiSE-OD$_{LP}$（混合零样本和线性探测）。", "result": "在三种RGB预训练检测器和两种鲁棒性基线上的评估表明，WiSE-OD在不增加任何额外训练或推理成本的情况下，显著提升了跨模态和损坏鲁棒性。", "conclusion": "WiSE-OD是一种有效提升红外目标检测模型鲁棒性的方法，它通过权重空间集成策略，成功弥合了RGB预训练和红外模态之间的差距，且不引入额外开销，使其适用于实际应用。"}}
{"id": "2507.19117", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19117", "abs": "https://arxiv.org/abs/2507.19117", "authors": ["Swapnil Hingmire", "Ze Shi Li", "Shiyu", "Zeng", "Ahmed Musa Awon", "Luiz Franciscatto Guerra", "Neil Ernst"], "title": "Objectifying the Subjective: Cognitive Biases in Topic Interpretations", "comment": "Accepted for publication at the Transactions of ACL (TACL) (pre-MIT\n  Press publication version)", "summary": "Interpretation of topics is crucial for their downstream applications.\nState-of-the-art evaluation measures of topic quality such as coherence and\nword intrusion do not measure how much a topic facilitates the exploration of a\ncorpus. To design evaluation measures grounded on a task, and a population of\nusers, we do user studies to understand how users interpret topics. We propose\nconstructs of topic quality and ask users to assess them in the context of a\ntopic and provide rationale behind evaluations. We use reflexive thematic\nanalysis to identify themes of topic interpretations from rationales. Users\ninterpret topics based on availability and representativeness heuristics rather\nthan probability. We propose a theory of topic interpretation based on the\nanchoring-and-adjustment heuristic: users anchor on salient words and make\nsemantic adjustments to arrive at an interpretation. Topic interpretation can\nbe viewed as making a judgment under uncertainty by an ecologically rational\nuser, and hence cognitive biases aware user models and evaluation frameworks\nare needed.", "AI": {"tldr": "该研究通过用户研究探讨用户如何解释主题，发现用户解释主题时依赖启发式而非概率，并提出了基于锚定与调整的理论，强调评估主题时需考虑用户认知偏差。", "motivation": "现有主题质量评估指标（如连贯性和词语入侵）未能衡量主题对语料库探索的促进作用。为了设计基于任务和用户群体的评估方法，需要理解用户如何解释主题。", "method": "通过用户研究，要求用户评估主题质量并提供理由。采用反思性主题分析法从用户理由中识别主题解释模式。基于用户评估和理由，提出了主题解释的构建。", "result": "用户在解释主题时依赖可用性启发式和代表性启发式，而非概率。研究提出了一种基于锚定与调整启发式的主题解释理论：用户以显著词为锚点，进行语义调整以得出解释。主题解释可视为生态理性用户在不确定性下的判断。", "conclusion": "用户解释主题是一个受认知偏差影响的判断过程，因此需要开发考虑认知偏差的用户模型和评估框架来衡量主题质量。"}}
{"id": "2507.18863", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18863", "abs": "https://arxiv.org/abs/2507.18863", "authors": ["Matthew Kit Khinn Teng", "Haibo Zhang", "Takeshi Saitoh"], "title": "Phoneme-Level Visual Speech Recognition via Point-Visual Fusion and Language Model Reconstruction", "comment": "10 pages, 3 figures", "summary": "Visual Automatic Speech Recognition (V-ASR) is a challenging task that\ninvolves interpreting spoken language solely from visual information, such as\nlip movements and facial expressions. This task is notably challenging due to\nthe absence of auditory cues and the visual ambiguity of phonemes that exhibit\nsimilar visemes-distinct sounds that appear identical in lip motions. Existing\nmethods often aim to predict words or characters directly from visual cues, but\nthey commonly suffer from high error rates due to viseme ambiguity and require\nlarge amounts of pre-training data. We propose a novel phoneme-based two-stage\nframework that fuses visual and landmark motion features, followed by an LLM\nmodel for word reconstruction to address these challenges. Stage 1 consists of\nV-ASR, which outputs the predicted phonemes, thereby reducing training\ncomplexity. Meanwhile, the facial landmark features address speaker-specific\nfacial characteristics. Stage 2 comprises an encoder-decoder LLM model, NLLB,\nthat reconstructs the output phonemes back to words. Besides using a large\nvisual dataset for deep learning fine-tuning, our PV-ASR method demonstrates\nsuperior performance by achieving 17.4% WER on the LRS2 and 21.0% WER on the\nLRS3 dataset.", "AI": {"tldr": "提出一种基于音素的两阶段视觉自动语音识别（V-ASR）框架，结合视觉和面部地标特征，并利用LLM进行词语重建，以应对视觉歧义和数据需求挑战。", "motivation": "视觉自动语音识别（V-ASR）面临挑战，包括缺乏听觉线索、音素的视觉模糊性（即“视素”歧义），以及现有方法错误率高且需要大量预训练数据。", "method": "提出一个新型两阶段框架：第一阶段是V-ASR模型，融合视觉和面部地标运动特征，输出预测的音素；第二阶段是编码器-解码器LLM模型（NLLB），将音素重建为词语。该方法利用大型视觉数据集进行深度学习微调。", "result": "PV-ASR方法在LRS2数据集上实现了17.4%的词错误率（WER），在LRS3数据集上实现了21.0%的词错误率（WER），表现出卓越的性能。", "conclusion": "该新型基于音素的两阶段V-ASR框架，通过结合多模态特征和大型语言模型，有效解决了视觉自动语音识别中的关键挑战，并显著提高了识别准确率。"}}
{"id": "2507.18929", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18929", "abs": "https://arxiv.org/abs/2507.18929", "authors": ["Jian Chen", "Yuxuan Hu", "Haifeng Lu", "Wei Wang", "Min Yang", "Chengming Li", "Xiping Hu"], "title": "MGHFT: Multi-Granularity Hierarchical Fusion Transformer for Cross-Modal Sticker Emotion Recognition", "comment": "Accepted by ACMMM2025", "summary": "Although pre-trained visual models with text have demonstrated strong\ncapabilities in visual feature extraction, sticker emotion understanding\nremains challenging due to its reliance on multi-view information, such as\nbackground knowledge and stylistic cues. To address this, we propose a novel\nmulti-granularity hierarchical fusion transformer (MGHFT), with a multi-view\nsticker interpreter based on Multimodal Large Language Models. Specifically,\ninspired by the human ability to interpret sticker emotions from multiple\nviews, we first use Multimodal Large Language Models to interpret stickers by\nproviding rich textual context via multi-view descriptions. Then, we design a\nhierarchical fusion strategy to fuse the textual context into visual\nunderstanding, which builds upon a pyramid visual transformer to extract both\nglobal and local sticker features at multiple stages. Through contrastive\nlearning and attention mechanisms, textual features are injected at different\nstages of the visual backbone, enhancing the fusion of global- and\nlocal-granularity visual semantics with textual guidance. Finally, we introduce\na text-guided fusion attention mechanism to effectively integrate the overall\nmultimodal features, enhancing semantic understanding. Extensive experiments on\n2 public sticker emotion datasets demonstrate that MGHFT significantly\noutperforms existing sticker emotion recognition approaches, achieving higher\naccuracy and more fine-grained emotion recognition. Compared to the best\npre-trained visual models, our MGHFT also obtains an obvious improvement, 5.4%\non F1 and 4.0% on accuracy. The code is released at\nhttps://github.com/cccccj-03/MGHFT_ACMMM2025.", "AI": {"tldr": "本文提出了一种多粒度分层融合Transformer (MGHFT) 模型，结合多模态大语言模型（MLLMs）进行多视角贴纸情感理解，通过分层融合策略将文本上下文与视觉特征融合，显著提升了贴纸情感识别的准确性和细粒度。", "motivation": "现有预训练视觉模型在视觉特征提取方面表现强大，但贴纸情感理解仍具挑战性，因为它依赖于多视角信息，如背景知识和风格线索，而这些是现有模型难以捕获的。", "method": "该研究提出MGHFT模型，其核心是基于多模态大语言模型（MLLMs）的多视角贴纸解释器。首先，MLLMs通过提供多视角描述来生成丰富的文本上下文。接着，设计了一种分层融合策略，利用金字塔视觉Transformer在多个阶段提取全局和局部贴纸特征，并通过对比学习和注意力机制将文本特征注入视觉主干的不同阶段。最后，引入文本引导的融合注意力机制，有效整合整体多模态特征，增强语义理解。", "result": "在两个公开的贴纸情感数据集上进行的大量实验表明，MGHFT显著优于现有贴纸情感识别方法，实现了更高的准确性和更细粒度的情感识别。与最佳预训练视觉模型相比，MGHFT在F1分数上提升了5.4%，在准确率上提升了4.0%。", "conclusion": "MGHFT通过模拟人类从多视角解释贴纸情感的能力，并结合多模态大语言模型与多粒度分层视觉-文本融合，有效解决了贴纸情感理解的挑战，显著提升了识别性能，证实了其在多模态情感理解方面的优越性。"}}
{"id": "2507.19156", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19156", "abs": "https://arxiv.org/abs/2507.19156", "authors": ["Gioele Giachino", "Marco Rondina", "Antonio Vetrò", "Riccardo Coppola", "Juan Carlos De Martin"], "title": "An Empirical Investigation of Gender Stereotype Representation in Large Language Models: The Italian Case", "comment": "16 pages, European Conference on Machine Learning and Principles and\n  Practice of Knowledge Discovery in Databases (ECML PKDD 2025) - 5th Workshop\n  on Bias and Fairness in AI (BIAS25)", "summary": "The increasing use of Large Language Models (LLMs) in a large variety of\ndomains has sparked worries about how easily they can perpetuate stereotypes\nand contribute to the generation of biased content. With a focus on gender and\nprofessional bias, this work examines in which manner LLMs shape responses to\nungendered prompts, contributing to biased outputs. This analysis uses a\nstructured experimental method, giving different prompts involving three\ndifferent professional job combinations, which are also characterized by a\nhierarchical relationship. This study uses Italian, a language with extensive\ngrammatical gender differences, to highlight potential limitations in current\nLLMs' ability to generate objective text in non-English languages. Two popular\nLLM-based chatbots are examined, namely OpenAI ChatGPT (gpt-4o-mini) and Google\nGemini (gemini-1.5-flash). Through APIs, we collected a range of 3600\nresponses. The results highlight how content generated by LLMs can perpetuate\nstereotypes. For example, Gemini associated 100% (ChatGPT 97%) of 'she'\npronouns to the 'assistant' rather than the 'manager'. The presence of bias in\nAI-generated text can have significant implications in many fields, such as in\nthe workplaces or in job selections, raising ethical concerns about its use.\nUnderstanding these risks is pivotal to developing mitigation strategies and\nassuring that AI-based systems do not increase social inequalities, but rather\ncontribute to more equitable outcomes. Future research directions include\nexpanding the study to additional chatbots or languages, refining prompt\nengineering methods or further exploiting a larger experimental base.", "AI": {"tldr": "本研究探讨了LLM在生成内容中如何固化性别和职业偏见，特别是在意大利语环境下，发现LLM倾向于将女性与较低层级职业关联，揭示了其在非英语语言中生成客观文本的局限性。", "motivation": "LLM日益广泛的应用引发了对其传播刻板印象和生成偏见内容的担忧。本研究旨在探究LLM如何将无性别提示的回复塑造成偏向性输出，尤其关注性别和职业偏见，并强调其在非英语语言中可能存在的局限性。", "method": "采用结构化实验方法，使用涉及三种不同且具有层级关系的专业职位组合提示，在意大利语环境下（一种具有广泛语法性别差异的语言）进行。研究考察了OpenAI ChatGPT (gpt-4o-mini) 和 Google Gemini (gemini-1.5-flash) 两款LLM聊天机器人，通过API收集了3600个回复进行分析。", "result": "结果显示LLM生成的内容会固化刻板印象。例如，Gemini将100%的“她”代词与“助理”而非“经理”关联，ChatGPT的这一比例为97%。这表明AI生成文本中的偏见可能对职场或工作选择等领域产生重大影响。", "conclusion": "AI生成文本中存在的偏见引发了伦理担忧，理解这些风险对于制定缓解策略至关重要，以确保AI系统不会加剧社会不平等。未来的研究方向包括扩展到其他聊天机器人或语言，改进提示工程方法，或进一步扩大实验基础。"}}
{"id": "2507.18870", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18870", "abs": "https://arxiv.org/abs/2507.18870", "authors": ["Keke Tang", "Yuze Gao", "Weilong Peng", "Xiaofei Wang", "Meie Fang", "Peican Zhu"], "title": "Transferable and Undefendable Point Cloud Attacks via Medial Axis Transform", "comment": null, "summary": "Studying adversarial attacks on point clouds is essential for evaluating and\nimproving the robustness of 3D deep learning models. However, most existing\nattack methods are developed under ideal white-box settings and often suffer\nfrom limited transferability to unseen models and insufficient robustness\nagainst common defense mechanisms. In this paper, we propose MAT-Adv, a novel\nadversarial attack framework that enhances both transferability and\nundefendability by explicitly perturbing the medial axis transform (MAT)\nrepresentations, in order to induce inherent adversarialness in the resulting\npoint clouds. Specifically, we employ an autoencoder to project input point\nclouds into compact MAT representations that capture the intrinsic geometric\nstructure of point clouds. By perturbing these intrinsic representations,\nMAT-Adv introduces structural-level adversarial characteristics that remain\neffective across diverse models and defense strategies. To mitigate overfitting\nand prevent perturbation collapse, we incorporate a dropout strategy into the\noptimization of MAT perturbations, further improving transferability and\nundefendability. Extensive experiments demonstrate that MAT-Adv significantly\noutperforms existing state-of-the-art methods in both transferability and\nundefendability. Codes will be made public upon paper acceptance.", "AI": {"tldr": "提出MAT-Adv，一种通过扰动中轴变换（MAT）表示来增强点云对抗攻击可迁移性和抗防御性的新方法。", "motivation": "现有针对点云的对抗攻击方法多在白盒设置下开发，对未知模型的可迁移性有限，且对常见防御机制的鲁棒性不足。", "method": "提出MAT-Adv框架。该方法利用自编码器将点云投影到紧凑的中轴变换（MAT）表示中，通过扰动这些内在的MAT表示来引入结构级别的对抗性特征。为减轻过拟合并防止扰动崩溃，在MAT扰动优化中融入了dropout策略，进一步提升了可迁移性和抗防御性。", "result": "大量实验表明，MAT-Adv在可迁移性和抗防御性方面显著优于现有最先进的方法。", "conclusion": "MAT-Adv通过对中轴变换（MAT）表示进行结构性扰动，成功提升了点云对抗攻击的可迁移性和抗防御性，解决了现有方法的局限性。"}}
{"id": "2507.18967", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18967", "abs": "https://arxiv.org/abs/2507.18967", "authors": ["UMMPK Nawarathne", "HMNS Kumari", "HMLS Kumari"], "title": "Underwater Waste Detection Using Deep Learning A Performance Comparison of YOLOv7 to 10 and Faster RCNN", "comment": "7 pages, 11 figures, to be published in International Journal of\n  Research in Computing (IJRC)", "summary": "Underwater pollution is one of today's most significant environmental\nconcerns, with vast volumes of garbage found in seas, rivers, and landscapes\naround the world. Accurate detection of these waste materials is crucial for\nsuccessful waste management, environmental monitoring, and mitigation\nstrategies. In this study, we investigated the performance of five cutting-edge\nobject recognition algorithms, namely YOLO (You Only Look Once) models,\nincluding YOLOv7, YOLOv8, YOLOv9, YOLOv10, and Faster Region-Convolutional\nNeural Network (R-CNN), to identify which model was most effective at\nrecognizing materials in underwater situations. The models were thoroughly\ntrained and tested on a large dataset containing fifteen different classes\nunder diverse conditions, such as low visibility and variable depths. From the\nabove-mentioned models, YOLOv8 outperformed the others, with a mean Average\nPrecision (mAP) of 80.9%, indicating a significant performance. This increased\nperformance is attributed to YOLOv8's architecture, which incorporates advanced\nfeatures such as improved anchor-free mechanisms and self-supervised learning,\nallowing for more precise and efficient recognition of items in a variety of\nsettings. These findings highlight the YOLOv8 model's potential as an effective\ntool in the global fight against pollution, improving both the detection\ncapabilities and scalability of underwater cleanup operations.", "AI": {"tldr": "本研究比较了五种先进目标识别算法在水下垃圾检测中的性能，发现YOLOv8表现最佳，mAP达到80.9%。", "motivation": "水下污染是一个重大的环境问题，准确检测废物对于有效的废物管理、环境监测和缓解策略至关重要。", "method": "研究比较了YOLOv7、YOLOv8、YOLOv9、YOLOv10和Faster R-CNN五种目标识别算法，在一个包含15个不同类别、涵盖低能见度和不同深度等多样条件的大型数据集上进行训练和测试。", "result": "YOLOv8在所有模型中表现最佳，平均精度（mAP）达到80.9%。其卓越性能归因于其改进的无锚机制和自监督学习等先进架构特性。", "conclusion": "研究结果表明YOLOv8模型在水下污染检测中具有巨大潜力，可有效提升水下清理行动的检测能力和可扩展性，有助于全球抗击污染。"}}
{"id": "2507.19195", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19195", "abs": "https://arxiv.org/abs/2507.19195", "authors": ["Chaymaa Abbas", "Mariette Awad", "Razane Tajeddine"], "title": "Can Small-Scale Data Poisoning Exacerbate Dialect-Linked Biases in Large Language Models?", "comment": null, "summary": "Despite the ongoing improvements in the design of large language models\n(LLMs) to foster inclusion and balanced responses, these systems remain\nsusceptible to encoding and amplifying social biases. This study examines how\ndialectal variation, specifically African American Vernacular English (AAVE)\nversus Standard American English (SAE), interacts with data poisoning to\ninfluence toxicity in outputs. Using both small- and medium-scale LLaMA models,\nwe show that even minimal exposure to poisoned data significantly increases\ntoxicity for AAVE inputs, while it remains comparatively unaffected for SAE.\nLarger models exhibit a more significant amplification effect which suggests\nheightened susceptibility with scale. To further assess these disparities, we\nemployed GPT-4o as a fairness auditor, which identified harmful stereotypical\npatterns disproportionately tied to AAVE inputs, including portrayals of\naggression, criminality, and intellectual inferiority. These findings\nunderscore the compounding impact of data poisoning and dialectal bias and\nemphasize the need for dialect-aware evaluation, targeted debiasing\ninterventions, and socially responsible training protocols during development.", "AI": {"tldr": "研究发现，数据投毒会显著增加大型语言模型（LLMs）对非洲裔美国人白话英语（AAVE）输入的毒性，而对标准美式英语（SAE）影响较小，且模型规模越大，这种负面影响越显著，导致对AAVE的有害刻板印象。", "motivation": "尽管大型语言模型在包容性和平衡响应方面有所改进，但它们仍易于编码和放大社会偏见。本研究旨在探讨方言变体（特别是AAVE与SAE）如何与数据投毒相互作用，从而影响输出中的毒性。", "method": "研究使用了小型和中型LLaMA模型，通过数据投毒来测试AAVE和SAE输入对毒性的影响。此外，还利用GPT-4o作为公平性审计工具，评估输出中是否存在有害的刻板印象模式。", "result": "即使是极少量投毒数据，也会显著增加AAVE输入的毒性，而SAE输入则相对不受影响。较大的模型表现出更显著的放大效应，表明随着模型规模的增加，其脆弱性也随之提高。GPT-4o审计发现，与AAVE输入相关的有害刻板印象模式（包括攻击性、犯罪性和智力低下）不成比例地出现。", "conclusion": "这些发现强调了数据投毒和方言偏见的复合影响，并强调了在模型开发过程中进行方言感知评估、有针对性的去偏干预以及负责任的训练协议的必要性。"}}
{"id": "2507.18895", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18895", "abs": "https://arxiv.org/abs/2507.18895", "authors": ["Vangelis Kostoulas", "Arthur Guijt", "Ellen M. Kerkhof", "Bradley R. Pieters", "Peter A. N. Bosman", "Tanja Alderliesten"], "title": "Dealing with Segmentation Errors in Needle Reconstruction for MRI-Guided Brachytherapy", "comment": "Published in: Proc. SPIE Medical Imaging 2025, Vol. 13408, 1340826", "summary": "Brachytherapy involves bringing a radioactive source near tumor tissue using\nimplanted needles. Image-guided brachytherapy planning requires amongst others,\nthe reconstruction of the needles. Manually annotating these needles on patient\nimages can be a challenging and time-consuming task for medical professionals.\nFor automatic needle reconstruction, a two-stage pipeline is commonly adopted,\ncomprising a segmentation stage followed by a post-processing stage. While deep\nlearning models are effective for segmentation, their results often contain\nerrors. No currently existing post-processing technique is robust to all\npossible segmentation errors. We therefore propose adaptations to existing\npost-processing techniques mainly aimed at dealing with segmentation errors and\nthereby improving the reconstruction accuracy. Experiments on a prostate cancer\ndataset, based on MRI scans annotated by medical professionals, demonstrate\nthat our proposed adaptations can help to effectively manage segmentation\nerrors, with the best adapted post-processing technique achieving median\nneedle-tip and needle-bottom point localization errors of $1.07$ (IQR $\\pm\n1.04$) mm and $0.43$ (IQR $\\pm 0.46$) mm, respectively, and median shaft error\nof $0.75$ (IQR $\\pm 0.69$) mm with 0 false positive and 0 false negative\nneedles on a test set of 261 needles.", "AI": {"tldr": "该研究提出对现有后处理技术进行改进，以有效应对深度学习分割模型在近距离放射治疗针头重建中产生的误差，从而提高重建精度。", "motivation": "近距离放射治疗中的针头重建对手动标注而言耗时且困难。现有自动重建方法（分割加后处理）中，深度学习分割模型虽有效但常有误差，且现有后处理技术无法鲁棒处理所有可能的分割误差。", "method": "研究者提出对现有后处理技术进行适应性修改，主要目的是处理分割误差，从而提高重建精度。方法在MRI扫描的 prostate cancer 数据集上进行了验证。", "result": "在包含261根针头的测试集上，最佳的改进后处理技术实现了针尖定位中位数误差1.07毫米，针底部定位中位数误差0.43毫米，针杆中位数误差0.75毫米，且无假阳性或假阴性针头。", "conclusion": "所提出的适应性后处理技术能有效管理分割误差，显著提高近距离放射治疗中针头重建的准确性。"}}
{"id": "2507.19004", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19004", "abs": "https://arxiv.org/abs/2507.19004", "authors": ["Siyi Xun", "Yue Sun", "Jingkun Chen", "Zitong Yu", "Tong Tong", "Xiaohong Liu", "Mingxiang Wu", "Tao Tan"], "title": "MedIQA: A Scalable Foundation Model for Prompt-Driven Medical Image Quality Assessment", "comment": "We note that the version after peer review of this paper has been\n  provisionally accepted by The 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI 2025)", "summary": "Rapid advances in medical imaging technology underscore the critical need for\nprecise and automated image quality assessment (IQA) to ensure diagnostic\naccuracy. Existing medical IQA methods, however, struggle to generalize across\ndiverse modalities and clinical scenarios. In response, we introduce MedIQA,\nthe first comprehensive foundation model for medical IQA, designed to handle\nvariability in image dimensions, modalities, anatomical regions, and types. We\ndeveloped a large-scale multi-modality dataset with plentiful manually\nannotated quality scores to support this. Our model integrates a salient slice\nassessment module to focus on diagnostically relevant regions feature retrieval\nand employs an automatic prompt strategy that aligns upstream physical\nparameter pre-training with downstream expert annotation fine-tuning. Extensive\nexperiments demonstrate that MedIQA significantly outperforms baselines in\nmultiple downstream tasks, establishing a scalable framework for medical IQA\nand advancing diagnostic workflows and clinical decision-making.", "AI": {"tldr": "MedIQA是首个针对医学图像质量评估（IQA）的综合基础模型，旨在解决现有方法泛化性差的问题，并通过大规模多模态数据集和创新模块显著优于基线。", "motivation": "医学成像技术快速发展，但现有医学IQA方法难以跨不同模态和临床场景泛化，导致诊断准确性评估面临挑战，因此迫切需要精确且自动化的IQA。", "method": "提出了MedIQA，一个能处理多种图像维度、模态、解剖区域和类型的基础模型。构建了一个大规模多模态、手动标注质量分数的SOTA数据集。模型整合了显著切片评估模块以关注诊断相关区域特征，并采用自动提示策略，将上游物理参数预训练与下游专家标注微调对齐。", "result": "MedIQA在多个下游任务中显著优于基线，证明了其卓越性能。", "conclusion": "MedIQA为医学IQA建立了一个可扩展的框架，有望推进诊断工作流程和临床决策。"}}
{"id": "2507.19219", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.19219", "abs": "https://arxiv.org/abs/2507.19219", "authors": ["Zi Liang", "Liantong Yu", "Shiyu Zhang", "Qingqing Ye", "Haibo Hu"], "title": "How Much Do Large Language Model Cheat on Evaluation? Benchmarking Overestimation under the One-Time-Pad-Based Framework", "comment": "Source code: https://github.com/liangzid/ArxivRoll/ Website:\n  https://arxivroll.moreoverai.com/", "summary": "Overestimation in evaluating large language models (LLMs) has become an\nincreasing concern. Due to the contamination of public benchmarks or imbalanced\nmodel training, LLMs may achieve unreal evaluation results on public\nbenchmarks, either intentionally or unintentionally, which leads to unfair\ncomparisons among LLMs and undermines their realistic capability assessments.\nExisting benchmarks attempt to address these issues by keeping test cases\npermanently secret, mitigating contamination through human evaluation, or\nrepeatedly collecting and constructing new samples. However, these approaches\nfail to ensure reproducibility, transparency, and high efficiency\nsimultaneously. Moreover, the extent of overestimation in current LLMs remains\nunquantified. To address these issues, we propose ArxivRoll, a dynamic\nevaluation framework inspired by one-time pad encryption in cryptography.\nArxivRoll comprises two key components: \\emph{i) SCP (Sequencing, Cloze, and\nPrediction)}, an automated generator for private test cases, and \\emph{ii)\nRugged Scores (RS)}, metrics that measure the proportion of public benchmark\ncontamination and training bias. Leveraging SCP, ArxivRoll constructs a new\nbenchmark every six months using recent articles from ArXiv and employs them\nfor one-time evaluations of LLM performance. Extensive experiments demonstrate\nthe high quality of our benchmark, and we provide a systematic evaluation of\ncurrent LLMs. The source code is available at\nhttps://github.com/liangzid/ArxivRoll/.", "AI": {"tldr": "该论文提出了ArxivRoll，一个动态评估框架，通过每六个月使用arXiv最新文章生成一次性私有测试用例，并引入“崎岖分数”来量化基准污染和训练偏差，以解决大型语言模型（LLMs）评估中的高估问题。", "motivation": "LLMs评估中存在高估问题，原因在于公共基准被污染或模型训练不平衡，导致评估结果不真实，进而造成LLMs间的不公平比较并损害其真实能力评估。现有方法（如保密测试用例、人工评估、重复收集新样本）无法同时保证可复现性、透明度和高效率，且当前LLMs的高估程度尚未被量化。", "method": "提出了ArxivRoll动态评估框架，灵感来源于密码学中的一次性密码本。该框架包含两个核心组件：i) SCP（Sequencing, Cloze, and Prediction），一个自动化私有测试用例生成器；ii) Rugged Scores (RS)，用于衡量公共基准污染和训练偏差的指标。ArxivRoll每六个月利用arXiv上的最新文章构建一个新的基准，并将其用于LLM性能的一次性评估。", "result": "广泛的实验证明了所构建基准的高质量，并且论文提供了一个对当前LLMs的系统性评估。", "conclusion": "ArxivRoll框架有效解决了LLMs评估中的高估问题，通过动态生成高质量私有基准和量化污染/偏差，实现了更公平、更真实的LLM能力评估。"}}
{"id": "2507.18911", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18911", "abs": "https://arxiv.org/abs/2507.18911", "authors": ["Zhihao Luo", "Luojun Lin", "Zheng Lin"], "title": "Synthetic-to-Real Camouflaged Object Detection", "comment": null, "summary": "Due to the high cost of collection and labeling, there are relatively few\ndatasets for camouflaged object detection (COD). In particular, for certain\nspecialized categories, the available image dataset is insufficiently\npopulated. Synthetic datasets can be utilized to alleviate the problem of\nlimited data to some extent. However, directly training with synthetic datasets\ncompared to real datasets can lead to a degradation in model performance. To\ntackle this problem, in this work, we investigate a new task, namely\nSyn-to-Real Camouflaged Object Detection (S2R-COD). In order to improve the\nmodel performance in real world scenarios, a set of annotated synthetic\ncamouflaged images and a limited number of unannotated real images must be\nutilized. We propose the Cycling Syn-to-Real Domain Adaptation Framework\n(CSRDA), a method based on the student-teacher model. Specially, CSRDA\npropagates class information from the labeled source domain to the unlabeled\ntarget domain through pseudo labeling combined with consistency regularization.\nConsidering that narrowing the intra-domain gap can improve the quality of\npseudo labeling, CSRDA utilizes a recurrent learning framework to build an\nevolving real domain for bridging the source and target domain. Extensive\nexperiments demonstrate the effectiveness of our framework, mitigating the\nproblem of limited data and handcraft annotations in COD. Our code is publicly\navailable at: https://github.com/Muscape/S2R-COD", "AI": {"tldr": "针对伪装目标检测（COD）中真实数据稀缺的问题，本文提出了一种从合成数据到真实数据的域适应框架（S2R-COD），利用有限的未标注真实图像和标注合成图像来提升模型在真实场景下的性能。", "motivation": "伪装目标检测（COD）数据集的收集和标注成本高昂，导致可用数据集相对较少，特别是对于某些特定类别。尽管合成数据集可以缓解数据不足，但直接使用合成数据训练会导致模型性能下降。", "method": "本文提出了Syn-to-Real Camouflaged Object Detection (S2R-COD) 新任务。为解决此问题，提出了基于师生模型的循环合成到真实域适应框架（CSRDA）。CSRDA通过伪标签结合一致性正则化将类别信息从有标签的源域传播到无标签的目标域。为提高伪标签质量，CSRDA采用循环学习框架构建一个不断演进的真实域，以弥合源域和目标域之间的差距。", "result": "大量的实验证明了该框架的有效性，成功缓解了COD领域数据有限和手工标注的难题。", "conclusion": "CSRDA框架通过有效的域适应策略，成功利用合成数据和少量未标注真实数据，解决了伪装目标检测中数据稀缺的问题，提高了模型在真实世界场景中的性能。"}}
{"id": "2507.19035", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19035", "abs": "https://arxiv.org/abs/2507.19035", "authors": ["Jitindra Fartiyal", "Pedro Freire", "Yasmeen Whayeb", "James S. Wolffsohn", "Sergei K. Turitsyn", "Sergei G. Sokolov"], "title": "Dual Path Learning -- learning from noise and context for medical image denoising", "comment": "10 pages, 7 figures", "summary": "Medical imaging plays a critical role in modern healthcare, enabling\nclinicians to accurately diagnose diseases and develop effective treatment\nplans. However, noise, often introduced by imaging devices, can degrade image\nquality, leading to misinterpretation and compromised clinical outcomes.\nExisting denoising approaches typically rely either on noise characteristics or\non contextual information from the image. Moreover, they are commonly developed\nand evaluated for a single imaging modality and noise type. Motivated by Geng\net.al CNCL, which integrates both noise and context, this study introduces a\nDual-Pathway Learning (DPL) model architecture that effectively denoises\nmedical images by leveraging both sources of information and fusing them to\ngenerate the final output. DPL is evaluated across multiple imaging modalities\nand various types of noise, demonstrating its robustness and generalizability.\nDPL improves PSNR by 3.35% compared to the baseline UNet when evaluated on\nGaussian noise and trained across all modalities. The code is available at\n10.5281/zenodo.15836053.", "AI": {"tldr": "该研究提出了一种双路径学习（DPL）模型，通过融合噪声特征和图像上下文信息，实现对多种医学成像模态和噪声类型的通用去噪，并显著提升了图像质量。", "motivation": "医学图像中的噪声会降低图像质量，导致诊断错误。现有去噪方法通常只依赖单一信息源（噪声特性或上下文信息），且缺乏跨模态和噪声类型的通用性。", "method": "受Geng等人的CNCL模型启发，本研究引入了一种双路径学习（DPL）模型架构。该模型通过利用并融合噪声信息和图像上下文信息来有效去除医学图像噪声。DPL在多种成像模态和不同噪声类型下进行了评估。", "result": "DPL模型展现出良好的鲁棒性和泛化性。与基线UNet相比，在评估高斯噪声并跨所有模态训练时，DPL将PSNR提高了3.35%。", "conclusion": "DPL模型通过有效结合噪声和上下文信息，实现了对多模态医学图像的鲁棒且通用的去噪，显著提升了图像质量，为临床诊断提供了更清晰的图像。"}}
{"id": "2507.19227", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19227", "abs": "https://arxiv.org/abs/2507.19227", "authors": ["Yuanhe Zhang", "Fangzhou Xie", "Zhenhong Zhou", "Zherui Li", "Hao Chen", "Kun Wang", "Yufei Guo"], "title": "Jailbreaking Large Language Diffusion Models: Revealing Hidden Safety Flaws in Diffusion-Based Text Generation", "comment": null, "summary": "Large Language Diffusion Models (LLDMs) exhibit comparable performance to\nLLMs while offering distinct advantages in inference speed and mathematical\nreasoning tasks.The precise and rapid generation capabilities of LLDMs amplify\nconcerns of harmful generations, while existing jailbreak methodologies\ndesigned for Large Language Models (LLMs) prove limited effectiveness against\nLLDMs and fail to expose safety vulnerabilities.Successful defense cannot\ndefinitively resolve harmful generation concerns, as it remains unclear whether\nLLDMs possess safety robustness or existing attacks are incompatible with\ndiffusion-based architectures.To address this, we first reveal the\nvulnerability of LLDMs to jailbreak and demonstrate that attack failure in\nLLDMs stems from fundamental architectural differences.We present a PArallel\nDecoding jailbreak (PAD) for diffusion-based language models. PAD introduces\nMulti-Point Attention Attack, which guides parallel generative processes toward\nharmful outputs that inspired by affirmative response patterns in LLMs.\nExperimental evaluations across four LLDMs demonstrate that PAD achieves\njailbreak attack success rates by 97%, revealing significant safety\nvulnerabilities. Furthermore, compared to autoregressive LLMs of the same size,\nLLDMs increase the harmful generation speed by 2x, significantly highlighting\nrisks of uncontrolled misuse.Through comprehensive analysis, we provide an\ninvestigation into LLDM architecture, offering critical insights for the secure\ndeployment of diffusion-based language models.", "AI": {"tldr": "研究揭示了大型语言扩散模型（LLDMs）对越狱攻击的脆弱性，并提出了一种名为PAD的新型并行解码越狱方法，有效暴露了LLDMs的安全漏洞。", "motivation": "LLDMs在推理速度和数学推理方面表现出色，但其快速生成能力引发了有害内容生成的担忧。现有针对大型语言模型（LLMs）的越狱方法对LLDMs效果有限，无法揭示其安全漏洞，导致不清楚LLDMs是否真正具有安全鲁棒性，或仅仅是现有攻击不兼容其扩散架构。", "method": "研究首先揭示了LLDMs越狱的脆弱性，并指出攻击失败源于其根本的架构差异。提出了一种针对扩散语言模型的并行解码越狱（PAD）方法。PAD引入了多点注意力攻击（Multi-Point Attention Attack），通过引导并行生成过程产生有害输出，其灵感来源于LLMs的肯定响应模式。", "result": "在四种LLDMs上的实验评估表明，PAD实现了97%的越狱攻击成功率，揭示了显著的安全漏洞。此外，与同等大小的自回归LLMs相比，LLDMs有害内容生成速度提高了2倍，显著突出了失控滥用的风险。", "conclusion": "研究通过全面分析，深入探讨了LLDM架构，为扩散语言模型的安全部署提供了关键见解，强调了其潜在的安全风险和需要进一步关注的领域。"}}
{"id": "2507.18921", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18921", "abs": "https://arxiv.org/abs/2507.18921", "authors": ["Elham Soltani Kazemi", "Imad Eddine Toubal", "Gani Rahmon", "Jaired Collins", "K. Palaniappan"], "title": "HQ-SMem: Video Segmentation and Tracking Using Memory Efficient Object Embedding With Selective Update and Self-Supervised Distillation Feedback", "comment": "submit/6651762", "summary": "Video Object Segmentation (VOS) is foundational to numerous computer vision\napplications, including surveillance, autonomous driving, robotics and\ngenerative video editing. However, existing VOS models often struggle with\nprecise mask delineation, deformable objects, topologically transforming\nobjects, tracking drift and long video sequences. In this paper, we introduce\nHQ-SMem, for High Quality video segmentation and tracking using Smart Memory, a\nnovel method that enhances the performance of VOS base models by addressing\nthese limitations. Our approach incorporates three key innovations: (i)\nleveraging SAM with High-Quality masks (SAM-HQ) alongside appearance-based\ncandidate-selection to refine coarse segmentation masks, resulting in improved\nobject boundaries; (ii) implementing a dynamic smart memory mechanism that\nselectively stores relevant key frames while discarding redundant ones, thereby\noptimizing memory usage and processing efficiency for long-term videos; and\n(iii) dynamically updating the appearance model to effectively handle complex\ntopological object variations and reduce drift throughout the video. These\ncontributions mitigate several limitations of existing VOS models including,\ncoarse segmentations that mix-in background pixels, fixed memory update\nschedules, brittleness to drift and occlusions, and prompt ambiguity issues\nassociated with SAM. Extensive experiments conducted on multiple public\ndatasets and state-of-the-art base trackers demonstrate that our method\nconsistently ranks among the top two on VOTS and VOTSt 2024 datasets. Moreover,\nHQ-SMem sets new benchmarks on Long Video Dataset and LVOS, showcasing its\neffectiveness in challenging scenarios characterized by complex multi-object\ndynamics over extended temporal durations.", "AI": {"tldr": "HQ-SMem是一种新的视频目标分割（VOS）方法，通过结合SAM-HQ、动态智能记忆和自适应外观模型，显著提升了现有VOS模型在精确掩码、形变对象、拓扑变化和长视频序列上的性能。", "motivation": "现有VOS模型在精确掩码描绘、可变形对象处理、拓扑变换对象跟踪、跟踪漂移以及长视频序列处理方面存在局限性，导致性能不佳。", "method": "本研究引入HQ-SMem，包含三项关键创新：1) 利用SAM-HQ和基于外观的候选选择来优化粗略分割掩码；2) 实施动态智能记忆机制，选择性存储关键帧以优化长视频的内存和效率；3) 动态更新外观模型以有效处理复杂拓扑对象变化并减少视频中的漂移。", "result": "HQ-SMem在多个公共数据集和先进基础跟踪器上进行了广泛实验，结果显示其在VOTS和VOTSt 2024数据集上始终位列前二，并在Long Video Dataset和LVOS上创造了新的基准，展示了其在复杂多对象动态和长时间场景下的有效性。", "conclusion": "HQ-SMem成功解决了现有VOS模型的多个限制，包括粗糙分割、固定内存更新、对漂移和遮挡的脆弱性以及SAM相关的提示歧义问题，并在具有挑战性的长时序复杂多目标场景中表现出色。"}}
{"id": "2507.19054", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19054", "abs": "https://arxiv.org/abs/2507.19054", "authors": ["Binxu Li", "Yuhui Zhang", "Xiaohan Wang", "Weixin Liang", "Ludwig Schmidt", "Serena Yeung-Levy"], "title": "Closing the Modality Gap for Mixed Modality Search", "comment": "Project page: https://yuhui-zh15.github.io/MixedModalitySearch/", "summary": "Mixed modality search -- retrieving information across a heterogeneous corpus\ncomposed of images, texts, and multimodal documents -- is an important yet\nunderexplored real-world application. In this work, we investigate how\ncontrastive vision-language models, such as CLIP, perform on the mixed modality\nsearch task. Our analysis reveals a critical limitation: these models exhibit a\npronounced modality gap in the embedding space, where image and text embeddings\nform distinct clusters, leading to intra-modal ranking bias and inter-modal\nfusion failure. To address this issue, we propose GR-CLIP, a lightweight\npost-hoc calibration method that removes the modality gap in CLIP's embedding\nspace. Evaluated on MixBench -- the first benchmark specifically designed for\nmixed modality search -- GR-CLIP improves NDCG@10 by up to 26 percentage points\nover CLIP, surpasses recent vision-language generative embedding models by 4\npercentage points, while using 75x less compute.", "AI": {"tldr": "本文研究了CLIP等视觉-语言模型在混合模态搜索中的表现，发现其存在模态间隙问题。为解决此问题，提出了轻量级校准方法GR-CLIP，显著提升了搜索性能。", "motivation": "混合模态搜索（在图像、文本和多模态文档构成的异构语料库中检索信息）是一个重要但尚未充分探索的现实应用。现有的对比视觉-语言模型（如CLIP）在此任务中存在关键局限性。", "method": "分析了CLIP在混合模态搜索任务中的表现，揭示了嵌入空间中图像和文本嵌入形成不同聚类的模态间隙问题，导致模态内排序偏差和模态间融合失败。提出GR-CLIP，一种轻量级的后处理校准方法，用于消除CLIP嵌入空间中的模态间隙。在专门为混合模态搜索设计的首个基准测试MixBench上进行评估。", "result": "CLIP模型在嵌入空间中表现出显著的模态间隙。GR-CLIP在MixBench上将NDCG@10指标相对于CLIP提高了多达26个百分点，超越了最近的视觉-语言生成嵌入模型4个百分点，同时计算量减少了75倍。", "conclusion": "模态间隙是对比视觉-语言模型在混合模态搜索中面临的关键挑战。GR-CLIP作为一种轻量级校准方法，能有效消除模态间隙，显著提升混合模态搜索的性能和效率。"}}
{"id": "2507.19303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19303", "abs": "https://arxiv.org/abs/2507.19303", "authors": ["Ilias Chalkidis", "Stephanie Brandl", "Paris Aslanidis"], "title": "Identifying Fine-grained Forms of Populism in Political Discourse: A Case Study on Donald Trump's Presidential Campaigns", "comment": "Pre-print", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of instruction-following tasks, yet their grasp of nuanced social\nscience concepts remains underexplored. This paper examines whether LLMs can\nidentify and classify fine-grained forms of populism, a complex and contested\nconcept in both academic and media debates. To this end, we curate and release\nnovel datasets specifically designed to capture populist discourse. We evaluate\na range of pre-trained (large) language models, both open-weight and\nproprietary, across multiple prompting paradigms. Our analysis reveals notable\nvariation in performance, highlighting the limitations of LLMs in detecting\npopulist discourse. We find that a fine-tuned RoBERTa classifier vastly\noutperforms all new-era instruction-tuned LLMs, unless fine-tuned.\nAdditionally, we apply our best-performing model to analyze campaign speeches\nby Donald Trump, extracting valuable insights into his strategic use of\npopulist rhetoric. Finally, we assess the generalizability of these models by\nbenchmarking them on campaign speeches by European politicians, offering a lens\ninto cross-context transferability in political discourse analysis. In this\nsetting, we find that instruction-tuned LLMs exhibit greater robustness on\nout-of-domain data.", "AI": {"tldr": "研究发现，大型语言模型（LLMs）在识别民粹主义等细致的社会科学概念方面存在局限性。除非经过微调，否则微调的RoBERTa分类器在检测民粹主义言论方面远超新一代LLMs。然而，指令调优的LLMs在域外数据上表现出更好的鲁棒性。", "motivation": "尽管LLMs在指令遵循任务上表现出色，但它们对细致的社会科学概念（如民粹主义）的理解尚未得到充分探索。", "method": "研究构建并发布了专门用于捕捉民粹主义话语的新数据集。评估了多种预训练LLMs（包括开源和专有模型），并采用了多种提示范式。将LLMs的性能与微调的RoBERTa分类器进行比较。使用表现最佳的模型分析唐纳德·特朗普的竞选演讲，并将其在欧洲政治家的竞选演讲上进行基准测试，以评估跨语境的泛化能力。", "result": "LLMs在检测民粹主义言论方面表现出显著的性能差异和局限性。微调的RoBERTa分类器在未微调的情况下，其性能远超所有新一代指令调优LLMs。指令调优的LLMs在域外数据上表现出更大的鲁棒性。研究还从特朗普的演讲中提取了关于其民粹主义修辞策略的见解。", "conclusion": "LLMs在理解和分类细致的社会科学概念（如民粹主义）方面存在挑战。对于特定领域的民粹主义检测，微调的传统模型（如RoBERTa）表现更优。然而，指令调优的LLMs在处理域外数据时显示出更好的泛化能力和鲁棒性。"}}
{"id": "2507.18923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18923", "abs": "https://arxiv.org/abs/2507.18923", "authors": ["Zhentao Huang", "Di Wu", "Zhenbang He", "Minglun Gong"], "title": "Gaussian Set Surface Reconstruction through Per-Gaussian Optimization", "comment": null, "summary": "3D Gaussian Splatting (3DGS) effectively synthesizes novel views through its\nflexible representation, yet fails to accurately reconstruct scene geometry.\nWhile modern variants like PGSR introduce additional losses to ensure proper\ndepth and normal maps through Gaussian fusion, they still neglect individual\nplacement optimization. This results in unevenly distributed Gaussians that\ndeviate from the latent surface, complicating both reconstruction refinement\nand scene editing. Motivated by pioneering work on Point Set Surfaces, we\npropose Gaussian Set Surface Reconstruction (GSSR), a method designed to\ndistribute Gaussians evenly along the latent surface while aligning their\ndominant normals with the surface normal. GSSR enforces fine-grained geometric\nalignment through a combination of pixel-level and Gaussian-level single-view\nnormal consistency and multi-view photometric consistency, optimizing both\nlocal and global perspectives. To further refine the representation, we\nintroduce an opacity regularization loss to eliminate redundant Gaussians and\napply periodic depth- and normal-guided Gaussian reinitialization for a\ncleaner, more uniform spatial distribution. Our reconstruction results\ndemonstrate significantly improved geometric precision in Gaussian placement,\nenabling intuitive scene editing and efficient generation of novel\nGaussian-based 3D environments. Extensive experiments validate GSSR's\neffectiveness, showing enhanced geometric accuracy while preserving\nhigh-quality rendering performance.", "AI": {"tldr": "GSSR是一种改进3D高斯溅射（3DGS）几何精度的方法，通过均匀分布高斯并使其法线与表面对齐，从而实现更精确的三维重建和场景编辑。", "motivation": "现有3DGS方法虽然能有效合成新视图，但在几何重建方面表现不佳。即使是PGSR等变体，也忽略了单个高斯球体的放置优化，导致高斯分布不均并偏离潜在表面，这使得重建精炼和场景编辑变得复杂。", "method": "本文提出了高斯集合表面重建（GSSR）方法，旨在使高斯均匀分布在潜在表面上，并将其主导法线与表面法线对齐。GSSR通过结合像素级和高斯级的单视图法线一致性以及多视图光度一致性来强制执行精细的几何对齐，同时优化局部和全局视角。此外，引入了不透明度正则化损失以消除冗余高斯，并应用周期性的深度和法线引导高斯重新初始化，以获得更清晰、更均匀的空间分布。", "result": "重建结果表明，高斯放置的几何精度显著提高，这使得直观的场景编辑和基于高斯的新型3D环境的高效生成成为可能。大量的实验验证了GSSR的有效性，显示其在保持高质量渲染性能的同时，增强了几何精度。", "conclusion": "GSSR通过优化高斯球体的分布和法线对齐，显著提升了3DGS的几何重建精度，使得三维场景的编辑和生成更为便捷高效，且不牺牲渲染质量。"}}
{"id": "2507.19098", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19098", "abs": "https://arxiv.org/abs/2507.19098", "authors": ["Francisco Caetano", "Lemar Abdi", "Christiaan Viviers", "Amaan Valiuddin", "Fons van der Sommen"], "title": "MedSymmFlow: Bridging Generative Modeling and Classification in Medical Imaging through Symmetrical Flow Matching", "comment": "DGM4MICCAI 2025", "summary": "Reliable medical image classification requires accurate predictions and\nwell-calibrated uncertainty estimates, especially in high-stakes clinical\nsettings. This work presents MedSymmFlow, a generative-discriminative hybrid\nmodel built on Symmetrical Flow Matching, designed to unify classification,\ngeneration, and uncertainty quantification in medical imaging. MedSymmFlow\nleverages a latent-space formulation that scales to high-resolution inputs and\nintroduces a semantic mask conditioning mechanism to enhance diagnostic\nrelevance. Unlike standard discriminative models, it naturally estimates\nuncertainty through its generative sampling process. The model is evaluated on\nfour MedMNIST datasets, covering a range of modalities and pathologies. The\nresults show that MedSymmFlow matches or exceeds the performance of established\nbaselines in classification accuracy and AUC, while also delivering reliable\nuncertainty estimates validated by performance improvements under selective\nprediction.", "AI": {"tldr": "MedSymmFlow是一个基于Symmetrical Flow Matching的生成-判别混合模型，旨在统一医学图像分类、生成和不确定性量化。它在分类准确性和不确定性估计方面表现出色。", "motivation": "在临床环境中，可靠的医学图像分类需要准确的预测和校准良好的不确定性估计，这在医疗领域至关重要。", "method": "MedSymmFlow是一个基于Symmetrical Flow Matching构建的生成-判别混合模型。它采用潜在空间公式以适应高分辨率输入，并引入语义掩码条件机制来增强诊断相关性。该模型通过其生成采样过程自然地估计不确定性。", "result": "MedSymmFlow在四个MedMNIST数据集上进行了评估，其分类准确性和AUC表现与现有基线模型持平或超越。同时，它提供了可靠的不确定性估计，并通过选择性预测下的性能改进得到了验证。", "conclusion": "MedSymmFlow成功地将医学图像的分类、生成和不确定性量化功能结合在一个模型中，并在性能上达到了或超越了现有基线，特别是在提供可靠不确定性估计方面表现突出。"}}
{"id": "2507.19315", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19315", "abs": "https://arxiv.org/abs/2507.19315", "authors": ["Yicheng Tao", "Yuanhao Huang", "Jie Liu"], "title": "AutoPCR: Automated Phenotype Concept Recognition by Prompting", "comment": null, "summary": "Phenotype concept recognition (CR) is a fundamental task in biomedical text\nmining, enabling applications such as clinical diagnostics and knowledge graph\nconstruction. However, existing methods often require ontology-specific\ntraining and struggle to generalize across diverse text types and evolving\nbiomedical terminology. We present AutoPCR, a prompt-based phenotype CR method\nthat does not require ontology-specific training. AutoPCR performs CR in three\nstages: entity extraction using a hybrid of rule-based and neural tagging\nstrategies, candidate retrieval via SapBERT, and entity linking through\nprompting a large language model. Experiments on four benchmark datasets show\nthat AutoPCR achieves the best average and most robust performance across both\nmention-level and document-level evaluations, surpassing prior state-of-the-art\nmethods. Further ablation and transfer studies demonstrate its inductive\ncapability and generalizability to new ontologies.", "AI": {"tldr": "AutoPCR是一种基于提示的表型概念识别方法，无需本体特定训练，在多个基准数据集上表现优异且泛化能力强。", "motivation": "现有表型概念识别方法需要本体特定训练，并且难以泛化到不同文本类型和不断演变的生物医学术语，限制了其在临床诊断和知识图谱构建等应用中的广泛使用。", "method": "AutoPCR采用三阶段方法：首先，通过结合基于规则和神经标记策略进行实体提取；其次，利用SapBERT进行候选实体检索；最后，通过提示大型语言模型完成实体链接。", "result": "在四个基准数据集上的实验表明，AutoPCR在提及级别和文档级别评估中均取得了最佳平均和最稳健的性能，超越了现有最先进方法。进一步的消融和迁移研究证明了其归纳能力和对新本体的泛化能力。", "conclusion": "AutoPCR提供了一种无需本体特定训练的有效表型概念识别解决方案，具有卓越的性能和强大的泛化能力，能够适应多样化的文本类型和不断发展的生物医学术语。"}}
{"id": "2507.18939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18939", "abs": "https://arxiv.org/abs/2507.18939", "authors": ["Jionghao Wang", "Cheng Lin", "Yuan Liu", "Rui Xu", "Zhiyang Dou", "Xiao-Xiao Long", "Hao-Xiang Guo", "Taku Komura", "Wenping Wang", "Xin Li"], "title": "PDT: Point Distribution Transformation with Diffusion Models", "comment": "Project page: https://shanemankiw.github.io/PDT/", "summary": "Point-based representations have consistently played a vital role in\ngeometric data structures. Most point cloud learning and processing methods\ntypically leverage the unordered and unconstrained nature to represent the\nunderlying geometry of 3D shapes. However, how to extract meaningful structural\ninformation from unstructured point cloud distributions and transform them into\nsemantically meaningful point distributions remains an under-explored problem.\nWe present PDT, a novel framework for point distribution transformation with\ndiffusion models. Given a set of input points, PDT learns to transform the\npoint set from its original geometric distribution into a target distribution\nthat is semantically meaningful. Our method utilizes diffusion models with\nnovel architecture and learning strategy, which effectively correlates the\nsource and the target distribution through a denoising process. Through\nextensive experiments, we show that our method successfully transforms input\npoint clouds into various forms of structured outputs - ranging from\nsurface-aligned keypoints, and inner sparse joints to continuous feature lines.\nThe results showcase our framework's ability to capture both geometric and\nsemantic features, offering a powerful tool for various 3D geometry processing\ntasks where structured point distributions are desired. Code will be available\nat this link: https://github.com/shanemankiw/PDT.", "AI": {"tldr": "PDT是一个基于扩散模型的新颖框架，用于将无序点云从原始几何分布转换为语义上有意义的目标点分布。", "motivation": "从非结构化点云中提取有意义的结构信息并将其转换为语义上有意义的点分布是一个尚未充分探索的问题。", "method": "本文提出了PDT框架，利用具有新颖架构和学习策略的扩散模型，通过去噪过程有效地关联源分布和目标分布，实现点分布转换。", "result": "PDT成功将输入点云转换为各种结构化输出，包括表面对齐的关键点、内部稀疏关节和连续特征线，展示了其捕获几何和语义特征的能力。", "conclusion": "PDT为需要结构化点分布的各种3D几何处理任务提供了一个强大的工具。"}}
{"id": "2507.19119", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19119", "abs": "https://arxiv.org/abs/2507.19119", "authors": ["Yanghong Liu", "Xingping Dong", "Ming Li", "Weixing Zhang", "Yidong Lou"], "title": "PatchTraj: Dynamic Patch Representation Learning for Time-Frequency Trajectory Prediction", "comment": null, "summary": "Pedestrian trajectory prediction is crucial for autonomous driving and\nrobotics. While existing point-based and grid-based methods expose two key\nlimitations: insufficiently modeling human motion dynamics, as they fail to\nbalance local motion details with long-range spatiotemporal dependencies, and\nthe time representation lacks interaction with the frequency domain in modeling\ntrajectory sequences. To address these challenges, we propose PatchTraj, a\ndynamic patch-based trajectory prediction framework that unifies time-domain\nand frequency-domain representations. Specifically, we decompose the trajectory\ninto raw time sequences and frequency components, employing dynamic patch\npartitioning for multi-scale trajectory segmentation to capture hierarchical\nmotion patterns. Each patch is processed by an adaptive embedding layer with\nscale-aware feature extraction, followed by hierarchical feature aggregation to\nmodel both fine-grained and long-range dependencies. The outputs of two\nbranches interact via cross-modal attention, enabling complementary fusion of\ntemporal and spectral cues. Finally, a Transformer encoder-decoder integrates\nboth modalities to autoregressively predict future trajectories. Extensive\nexperiments on ETH-UCY, SDD, NBA, and JRDB datasets demonstrate that our method\nachieves state-of-the-art performance with high efficiency.", "AI": {"tldr": "PatchTraj是一个动态分块的行人轨迹预测框架，它统一了时域和频域表示，通过多尺度分割和跨模态融合来平衡局部细节和长程依赖，实现了最先进的性能。", "motivation": "现有的点基和网格基方法在建模人体运动动力学时存在两个主要限制：一是无法平衡局部运动细节与长程时空依赖；二是时间表示在建模轨迹序列时缺乏与频域的交互。", "method": "本文提出了PatchTraj框架，将轨迹分解为原始时间序列和频率分量。采用动态分块划分进行多尺度轨迹分割，以捕获分层运动模式。每个块通过自适应嵌入层进行处理，进行尺度感知特征提取，然后进行分层特征聚合以建模细粒度和长程依赖。时域和频域两个分支的输出通过跨模态注意力进行交互，实现时间信息和频谱信息的互补融合。最后，一个Transformer编解码器整合两种模态以自回归地预测未来轨迹。", "result": "在ETH-UCY、SDD、NBA和JRDB数据集上的大量实验表明，所提出的方法以高效率实现了最先进的性能。", "conclusion": "PatchTraj框架通过统一时域和频域表示，并有效平衡局部运动细节与长程时空依赖，显著提升了行人轨迹预测的准确性和效率，达到了最先进的水平。"}}
{"id": "2507.19353", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19353", "abs": "https://arxiv.org/abs/2507.19353", "authors": ["Kai Liu", "Zhan Su", "Peijie Dong", "Fengran Mo", "Jianfei Gao", "ShaoTing Zhang", "Kai Chen"], "title": "Smooth Reading: Bridging the Gap of Recurrent LLM to Self-Attention LLM on Long-Context Tasks", "comment": null, "summary": "Recently, recurrent large language models (Recurrent LLMs) with linear\ncomputational complexity have re-emerged as efficient alternatives to\nself-attention-based LLMs (Self-Attention LLMs), which have quadratic\ncomplexity. However, Recurrent LLMs often underperform on long-context tasks\ndue to their limited fixed-size memory. Previous research has primarily focused\non enhancing the memory capacity of Recurrent LLMs through architectural\ninnovations, but these approaches have not yet enabled Recurrent LLMs to match\nthe performance of Self-Attention LLMs on long-context tasks. We argue that\nthis limitation arises because processing the entire context at once is not\nwell-suited for Recurrent LLMs. In this paper, we propose Smooth Reading, a\nchunk-wise inference method inspired by human reading strategies. Smooth\nReading processes context in chunks and iteratively summarizes the contextual\ninformation, thereby reducing memory demands and making the approach more\ncompatible with Recurrent LLMs. Our experimental results show that this method\nsubstantially narrows the performance gap between Recurrent and Self-Attention\nLLMs on long-context tasks, while preserving the efficiency advantages of\nRecurrent LLMs. Our Smooth Reading boosts SWA-3B-4k (a Recurrent LLM) from\n5.68% lower to 3.61% higher performance than Self-Attention LLMs on LongBench.\nBesides, our method maintains the high efficiency, training 3x faster and\ninferring 2x faster at 64k context compared to Self-Attention LLMs. To our\nknowledge, this is the first work to achieve comparable performance using\nRecurrent LLMs compared with Self-Attention LLMs on long-context tasks. We hope\nour method will inspire future research in this area. To facilitate further\nprogress, we will release code and dataset.", "AI": {"tldr": "该研究提出一种名为“平滑阅读”（Smooth Reading）的块级推理方法，通过分块处理和迭代总结上下文信息，显著提升了循环大语言模型（Recurrent LLMs）在长上下文任务上的性能，使其在保持高效性的同时，首次达到与自注意力大语言模型（Self-Attention LLMs）相当的水平。", "motivation": "循环大语言模型（Recurrent LLMs）具有线性计算复杂度，比具有二次复杂度的自注意力大语言模型（Self-Attention LLMs）更高效。然而，Recurrent LLMs由于固定大小的内存限制，在长上下文任务上表现不佳。以往研究主要通过架构创新提升内存容量，但未能使Recurrent LLMs在长上下文任务上匹配Self-Attention LLMs的性能，这表明一次性处理整个上下文不适合Recurrent LLMs。", "method": "提出“平滑阅读”（Smooth Reading）方法，这是一种受人类阅读策略启发的块级推理方法。该方法将上下文分块处理，并迭代地总结上下文信息，从而降低内存需求，使其更兼容Recurrent LLMs。", "result": "“平滑阅读”方法显著缩小了Recurrent LLMs与Self-Attention LLMs在长上下文任务上的性能差距。具体而言，在LongBench测试中，使用该方法后，SWA-3B-4k（一种Recurrent LLM）的性能从比Self-Attention LLMs低5.68%提升到高3.61%。此外，该方法保持了Recurrent LLMs的高效率，在64k上下文时，训练速度比Self-Attention LLMs快3倍，推理速度快2倍。", "conclusion": "该研究首次使循环大语言模型（Recurrent LLMs）在长上下文任务上实现了与自注意力大语言模型（Self-Attention LLMs）相当的性能，同时保留了其效率优势。该方法有望启发未来在该领域的研究。"}}
{"id": "2507.18958", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18958", "abs": "https://arxiv.org/abs/2507.18958", "authors": ["Xiaocheng Fang", "Jieyi Cai", "Huanyu Liu", "Chengju Zhou", "Minhua Lu", "Bingzhi Chen"], "title": "PerioDet: Large-Scale Panoramic Radiograph Benchmark for Clinical-Oriented Apical Periodontitis Detection", "comment": "MICCAI 2025(Early Accept)", "summary": "Apical periodontitis is a prevalent oral pathology that presents significant\npublic health challenges. Despite advances in automated diagnostic systems\nacross various medical fields, the development of Computer-Aided Diagnosis\n(CAD) applications for apical periodontitis is still constrained by the lack of\na large-scale, high-quality annotated dataset. To address this issue, we\nrelease a large-scale panoramic radiograph benchmark called \"PerioXrays\",\ncomprising 3,673 images and 5,662 meticulously annotated instances of apical\nperiodontitis. To the best of our knowledge, this is the first benchmark\ndataset for automated apical periodontitis diagnosis. This paper further\nproposes a clinical-oriented apical periodontitis detection (PerioDet)\nparadigm, which jointly incorporates Background-Denoising Attention (BDA) and\nIoU-Dynamic Calibration (IDC) mechanisms to address the challenges posed by\nbackground noise and small targets in automated detection. Extensive\nexperiments on the PerioXrays dataset demonstrate the superiority of PerioDet\nin advancing automated apical periodontitis detection. Additionally, a\nwell-designed human-computer collaborative experiment underscores the clinical\napplicability of our method as an auxiliary diagnostic tool for professional\ndentists.", "AI": {"tldr": "该研究发布了首个大规模根尖周炎全景X光片数据集PerioXrays，并提出了一种名为PerioDet的检测范式，通过背景去噪注意力和IoU动态校准机制，显著提升了自动化根尖周炎诊断的性能和临床实用性。", "motivation": "尽管医学领域的自动化诊断系统取得了进展，但根尖周炎的计算机辅助诊断（CAD）应用仍受限于缺乏大规模、高质量的标注数据集。现有挑战包括背景噪声和小目标检测困难。", "method": "1. 发布了名为“PerioXrays”的大规模全景X光片基准数据集，包含3,673张图像和5,662个根尖周炎实例的精细标注。2. 提出了一种面向临床的根尖周炎检测范式PerioDet，该范式联合集成了背景去噪注意力（BDA）和IoU动态校准（IDC）机制，以解决背景噪声和小目标检测的挑战。3. 在PerioXrays数据集上进行了广泛的实验验证。4. 进行了人机协作实验，以评估其临床辅助诊断潜力。", "result": "1. PerioXrays是目前首个用于自动化根尖周炎诊断的基准数据集。2. 在PerioXrays数据集上的大量实验表明，PerioDet在自动化根尖周炎检测方面表现出卓越的性能。3. 精心设计的人机协作实验强调了该方法作为专业牙医辅助诊断工具的临床适用性。", "conclusion": "该研究通过提供大规模高质量数据集和创新的检测范式，显著推动了根尖周炎的自动化诊断技术，并证明了其在临床实践中作为辅助诊断工具的有效性和潜力。"}}
{"id": "2507.19201", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19201", "abs": "https://arxiv.org/abs/2507.19201", "authors": ["Xin Li", "Kaixiang Yang", "Qiang Li", "Zhiwei Wang"], "title": "Joint Holistic and Lesion Controllable Mammogram Synthesis via Gated Conditional Diffusion Model", "comment": "Accepted, ACM Multimedia 2025, 10 pages, 5 figures", "summary": "Mammography is the most commonly used imaging modality for breast cancer\nscreening, driving an increasing demand for deep-learning techniques to support\nlarge-scale analysis. However, the development of accurate and robust methods\nis often limited by insufficient data availability and a lack of diversity in\nlesion characteristics. While generative models offer a promising solution for\ndata synthesis, current approaches often fail to adequately emphasize\nlesion-specific features and their relationships with surrounding tissues. In\nthis paper, we propose Gated Conditional Diffusion Model (GCDM), a novel\nframework designed to jointly synthesize holistic mammogram images and\nlocalized lesions. GCDM is built upon a latent denoising diffusion framework,\nwhere the noised latent image is concatenated with a soft mask embedding that\nrepresents breast, lesion, and their transitional regions, ensuring anatomical\ncoherence between them during the denoising process. To further emphasize\nlesion-specific features, GCDM incorporates a gated conditioning branch that\nguides the denoising process by dynamically selecting and fusing the most\nrelevant radiomic and geometric properties of lesions, effectively capturing\ntheir interplay. Experimental results demonstrate that GCDM achieves precise\ncontrol over small lesion areas while enhancing the realism and diversity of\nsynthesized mammograms. These advancements position GCDM as a promising tool\nfor clinical applications in mammogram synthesis. Our code is available at\nhttps://github.com/lixinHUST/Gated-Conditional-Diffusion-Model/", "AI": {"tldr": "本文提出Gated Conditional Diffusion Model (GCDM)，一种新颖的框架，用于联合合成完整的乳腺X光图像和局部病变，旨在解决乳腺癌筛查中深度学习数据不足和病变特征强调不足的问题。", "motivation": "乳腺X光图像是乳腺癌筛查常用方式，对深度学习分析需求大，但数据不足和病变特征多样性缺乏限制了模型开发。现有生成模型未能充分强调病变特异性特征及其与周围组织的关系。", "method": "GCDM基于潜在去噪扩散框架，将噪声潜在图像与表示乳腺、病变及其过渡区域的软掩模嵌入连接，确保去噪过程中的解剖学一致性。此外，GCDM引入门控条件分支，通过动态选择和融合病变最相关的放射组学和几何特性来引导去噪过程，有效捕捉它们的相互作用。", "result": "实验结果表明，GCDM能精确控制小病变区域，同时增强合成乳腺X光图像的真实感和多样性。", "conclusion": "GCDM在乳腺X光图像合成的临床应用中展现出巨大潜力，是解决数据稀缺和病变特征强调问题的有前景工具。"}}
{"id": "2507.19356", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.19356", "abs": "https://arxiv.org/abs/2507.19356", "authors": ["Hsuan-Yu Wang", "Pei-Ying Lee", "Berlin Chen"], "title": "Enhancing Speech Emotion Recognition Leveraging Aligning Timestamps of ASR Transcripts and Speaker Diarization", "comment": "6 pages, 3 figures, to appear in the Proceedings of the 2025\n  International Conference on Asian Language Processing (IALP)", "summary": "In this paper, we investigate the impact of incorporating timestamp-based\nalignment between Automatic Speech Recognition (ASR) transcripts and Speaker\nDiarization (SD) outputs on Speech Emotion Recognition (SER) accuracy.\nMisalignment between these two modalities often reduces the reliability of\nmultimodal emotion recognition systems, particularly in conversational\ncontexts. To address this issue, we introduce an alignment pipeline utilizing\npre-trained ASR and speaker diarization models, systematically synchronizing\ntimestamps to generate accurately labeled speaker segments. Our multimodal\napproach combines textual embeddings extracted via RoBERTa with audio\nembeddings from Wav2Vec, leveraging cross-attention fusion enhanced by a gating\nmechanism. Experimental evaluations on the IEMOCAP benchmark dataset\ndemonstrate that precise timestamp alignment improves SER accuracy,\noutperforming baseline methods that lack synchronization. The results highlight\nthe critical importance of temporal alignment, demonstrating its effectiveness\nin enhancing overall emotion recognition accuracy and providing a foundation\nfor robust multimodal emotion analysis.", "AI": {"tldr": "该论文研究了将自动语音识别（ASR）转录与说话人识别（SD）输出进行时间戳对齐，对语音情感识别（SER）准确性的影响。", "motivation": "ASR转录和说话人识别输出之间的错位常常会降低多模态情感识别系统的可靠性，尤其是在对话场景中。", "method": "引入了一个对齐流水线，利用预训练的ASR和说话人识别模型，系统地同步时间戳以生成准确标注的说话人片段。多模态方法结合了RoBERTa提取的文本嵌入和Wav2Vec的音频嵌入，并通过门控机制增强的交叉注意力融合进行整合。", "result": "在IEMOCAP数据集上的实验评估表明，精确的时间戳对齐显著提高了SER准确性，优于缺乏同步的基线方法。", "conclusion": "研究结果强调了时间对齐的关键重要性，证明了其在提高整体情感识别准确性方面的有效性，并为鲁棒的多模态情感分析奠定了基础。"}}
{"id": "2507.18966", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18966", "abs": "https://arxiv.org/abs/2507.18966", "authors": ["Saraa Al-Saddik", "Manna Elizabeth Philip", "Ali Haidar"], "title": "YOLO for Knowledge Extraction from Vehicle Images: A Baseline Study", "comment": null, "summary": "Accurate identification of vehicle attributes such as make, colour, and shape\nis critical for law enforcement and intelligence applications. This study\nevaluates the effectiveness of three state-of-the-art deep learning approaches\nYOLO-v11, YOLO-World, and YOLO-Classification on a real-world vehicle image\ndataset. This dataset was collected under challenging and unconstrained\nconditions by NSW Police Highway Patrol Vehicles. A multi-view inference (MVI)\napproach was deployed to enhance the performance of the models' predictions. To\nconduct the analyses, datasets with 100,000 plus images were created for each\nof the three metadata prediction tasks, specifically make, shape and colour.\nThe models were tested on a separate dataset with 29,937 images belonging to\n1809 number plates. Different sets of experiments have been investigated by\nvarying the models sizes. A classification accuracy of 93.70%, 82.86%, 85.19%,\nand 94.86% was achieved with the best performing make, shape, colour, and\ncolour-binary models respectively. It was concluded that there is a need to use\nMVI to get usable models within such complex real-world datasets. Our findings\nindicated that the object detection models YOLO-v11 and YOLO-World outperformed\nclassification-only models in make and shape extraction. Moreover, smaller YOLO\nvariants perform comparably to larger counterparts, offering substantial\nefficiency benefits for real-time predictions. This work provides a robust\nbaseline for extracting vehicle metadata in real-world scenarios. Such models\ncan be used in filtering and sorting user queries, minimising the time required\nto search large vehicle images datasets.", "AI": {"tldr": "本研究评估了YOLO-v11、YOLO-World和YOLO-Classification在真实世界车辆图像数据集上识别车辆属性（品牌、形状、颜色）的有效性，并引入多视角推理（MVI）以提高性能，结果显示YOLO目标检测模型优于纯分类模型，且小型YOLO变体表现良好，为实际应用提供了鲁棒基线。", "motivation": "准确识别车辆属性（如品牌、颜色、形状）对于执法和情报应用至关重要。", "method": "研究评估了YOLO-v11、YOLO-World和YOLO-Classification三种先进深度学习方法。数据集由新南威尔士州警方公路巡逻车在复杂无约束条件下收集，为品牌、形状和颜色预测任务创建了超过10万张图像的训练集，并在包含29,937张图像的独立测试集上进行测试。部署了多视角推理（MVI）方法以增强模型预测性能，并研究了不同模型尺寸的影响。", "result": "在最佳表现模型中，品牌、形状、颜色和颜色二分类的分类准确率分别达到93.70%、82.86%、85.19%和94.86%。研究发现，目标检测模型YOLO-v11和YOLO-World在品牌和形状提取方面优于纯分类模型。此外，较小的YOLO变体与较大变体表现相当，为实时预测提供了显著的效率优势。", "conclusion": "在复杂的真实世界数据集中，需要使用多视角推理（MVI）才能获得可用的模型。本研究为在真实世界场景中提取车辆元数据提供了鲁棒的基线，这些模型可用于过滤和排序用户查询，从而最大限度地减少搜索大型车辆图像数据集所需的时间。"}}
{"id": "2507.19304", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19304", "abs": "https://arxiv.org/abs/2507.19304", "authors": ["Muhammad Ibrahim", "Naveed Akhtar", "Haitian Wang", "Saeed Anwar", "Ajmal Mian"], "title": "Multistream Network for LiDAR and Camera-based 3D Object Detection in Outdoor Scenes", "comment": "This paper has been accepted by IEEE/RSJ IROS 2025 for oral\n  presentation on 19 Oct. 2025", "summary": "Fusion of LiDAR and RGB data has the potential to enhance outdoor 3D object\ndetection accuracy. To address real-world challenges in outdoor 3D object\ndetection, fusion of LiDAR and RGB input has started gaining traction. However,\neffective integration of these modalities for precise object detection task\nstill remains a largely open problem. To address that, we propose a MultiStream\nDetection (MuStD) network, that meticulously extracts task-relevant information\nfrom both data modalities. The network follows a three-stream structure. Its\nLiDAR-PillarNet stream extracts sparse 2D pillar features from the LiDAR input\nwhile the LiDAR-Height Compression stream computes Bird's-Eye View features. An\nadditional 3D Multimodal stream combines RGB and LiDAR features using UV\nmapping and polar coordinate indexing. Eventually, the features containing\ncomprehensive spatial, textural and geometric information are carefully fused\nand fed to a detection head for 3D object detection. Our extensive evaluation\non the challenging KITTI Object Detection Benchmark using public testing server\nat\nhttps://www.cvlibs.net/datasets/kitti/eval_object_detail.php?&result=d162ec699d6992040e34314d19ab7f5c217075e0\nestablishes the efficacy of our method by achieving new state-of-the-art or\nhighly competitive results in different categories while remaining among the\nmost efficient methods. Our code will be released through MuStD GitHub\nrepository at https://github.com/IbrahimUWA/MuStD.git", "AI": {"tldr": "该论文提出了一种名为MuStD的多流检测网络，用于有效融合LiDAR和RGB数据以提高户外3D目标检测的准确性，并在KITTI基准测试中取得了领先或极具竞争力的结果。", "motivation": "户外3D目标检测中LiDAR和RGB数据融合的潜力巨大，但如何有效整合这两种模态以实现精确的目标检测仍是一个悬而未决的问题。", "method": "提出了一种多流检测（MuStD）网络。该网络采用三流结构：LiDAR-PillarNet流从LiDAR输入中提取稀疏的2D柱状特征；LiDAR-Height Compression流计算鸟瞰图特征；额外的3D多模态流通过UV映射和极坐标索引结合RGB和LiDAR特征。最终，这些包含空间、纹理和几何信息的特征被仔细融合并送入检测头进行3D目标检测。", "result": "在具有挑战性的KITTI目标检测基准测试中，该方法在不同类别中取得了新的最先进或极具竞争力的结果，同时保持了高效性。", "conclusion": "MuStD网络通过精心设计的多流结构，成功解决了LiDAR和RGB数据有效融合的难题，显著提升了户外3D目标检测的性能和效率。"}}
{"id": "2507.19361", "categories": ["cs.CL", "cs.AI", "cs.SC", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19361", "abs": "https://arxiv.org/abs/2507.19361", "authors": ["Zhen Wan", "Chao-Han Huck Yang", "Yahan Yu", "Jinchuan Tian", "Sheng Li", "Ke Hu", "Zhehuai Chen", "Shinji Watanabe", "Fei Cheng", "Chenhui Chu", "Sadao Kurohashi"], "title": "SpeechIQ: Speech Intelligence Quotient Across Cognitive Levels in Voice Understanding Large Language Models", "comment": "Our Speech-IQ leaderboard will be hosted at\n  huggingface.co/spaces/nvidia/Speech-IQ-leaderboard. ACL 2025 main", "summary": "We introduce Speech-based Intelligence Quotient (SIQ) as a new form of human\ncognition-inspired evaluation pipeline for voice understanding large language\nmodels, LLM Voice, designed to assess their voice understanding ability. Moving\nbeyond popular voice understanding metrics such as word error rate (WER), SIQ\nexamines LLM Voice across three cognitive levels motivated by Bloom's Taxonomy:\n(1) Remembering (i.e., WER for verbatim accuracy); (2) Understanding (i.e.,\nsimilarity of LLM's interpretations); and (3) Application (i.e., QA accuracy\nfor simulating downstream tasks). We demonstrate that SIQ not only quantifies\nvoice understanding abilities but also provides unified comparisons between\ncascaded methods (e.g., ASR LLM) and end-to-end models, identifies annotation\nerrors in existing benchmarks, and detects hallucinations in LLM Voice. Our\nframework represents a first-of-its-kind intelligence examination that bridges\ncognitive principles with voice-oriented benchmarks, while exposing overlooked\nchallenges in multi-modal training.", "AI": {"tldr": "本文提出了一种名为语音智商（SIQ）的新型评估流程，用于评估大型语言模型（LLM Voice）的语音理解能力，该流程受到人类认知启发，并超越了传统的语音理解度量标准。", "motivation": "现有的语音理解度量标准（如词错误率WER）不足以全面评估LLM Voice的语音理解能力。研究者希望引入一种受人类认知启发的评估方法，以更深入地衡量模型的理解水平。", "method": "SIQ评估框架基于布鲁姆分类法，将语音理解能力分为三个认知层面进行评估：1) 记忆（使用WER评估逐字准确性）；2) 理解（评估LLM解释的相似性）；3) 应用（通过问答准确性模拟下游任务）。", "result": "SIQ不仅能量化语音理解能力，还能统一比较级联方法（如ASR+LLM）和端到端模型，识别现有基准测试中的标注错误，并检测LLM Voice中的幻觉现象。", "conclusion": "SIQ代表了一种首创的智能测试方法，它将认知原理与面向语音的基准测试相结合，同时揭示了多模态训练中被忽视的挑战。"}}
{"id": "2507.18988", "categories": ["cs.CV", "cs.CR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.18988", "abs": "https://arxiv.org/abs/2507.18988", "authors": ["Chao Wang", "Kejiang Chen", "Zijin Yang", "Yaofei Wang", "Weiming Zhang"], "title": "AEDR: Training-Free AI-Generated Image Attribution via Autoencoder Double-Reconstruction", "comment": null, "summary": "The rapid advancement of image-generation technologies has made it possible\nfor anyone to create photorealistic images using generative models, raising\nsignificant security concerns. To mitigate malicious use, tracing the origin of\nsuch images is essential. Reconstruction-based attribution methods offer a\npromising solution, but they often suffer from reduced accuracy and high\ncomputational costs when applied to state-of-the-art (SOTA) models. To address\nthese challenges, we propose AEDR (AutoEncoder Double-Reconstruction), a novel\ntraining-free attribution method designed for generative models with continuous\nautoencoders. Unlike existing reconstruction-based approaches that rely on the\nvalue of a single reconstruction loss, AEDR performs two consecutive\nreconstructions using the model's autoencoder, and adopts the ratio of these\ntwo reconstruction losses as the attribution signal. This signal is further\ncalibrated using the image homogeneity metric to improve accuracy, which\ninherently cancels out absolute biases caused by image complexity, with\nautoencoder-based reconstruction ensuring superior computational efficiency.\nExperiments on eight top latent diffusion models show that AEDR achieves 25.5%\nhigher attribution accuracy than existing reconstruction-based methods, while\nrequiring only 1% of the computational time.", "AI": {"tldr": "提出了一种名为AEDR的新型免训练归因方法，用于追踪由具有连续自编码器的生成模型生成的图像来源，该方法通过两次连续重建和损失比率实现了更高的准确性和计算效率。", "motivation": "图像生成技术的快速发展带来了安全隐患，需要追踪生成图像的来源。现有基于重建的归因方法在应用于最先进模型时，往往面临准确性降低和计算成本高昂的问题。", "method": "提出AEDR（AutoEncoder Double-Reconstruction）方法，这是一种免训练的归因方法，专为具有连续自编码器的生成模型设计。它不依赖单一重建损失值，而是通过模型的自编码器执行两次连续重建，并采用这两个重建损失的比率作为归因信号。该信号通过图像同质性度量进行校准，以提高准确性，同时自编码器重建确保了计算效率。", "result": "在八个顶级的潜在扩散模型上进行的实验表明，AEDR的归因准确率比现有基于重建的方法提高了25.5%，而计算时间仅为后者的1%。", "conclusion": "AEDR是一种高效且准确的图像生成模型归因方法，通过其独特的双重建和损失比率机制，显著优于现有方法，能够有效应对图像来源追踪的挑战。"}}
{"id": "2507.19321", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19321", "abs": "https://arxiv.org/abs/2507.19321", "authors": ["Viktar Dubovik", "Łukasz Struski", "Jacek Tabor", "Dawid Rymarczyk"], "title": "SIDE: Sparse Information Disentanglement for Explainable Artificial Intelligence", "comment": null, "summary": "Understanding the decisions made by deep neural networks is essential in\nhigh-stakes domains such as medical imaging and autonomous driving. Yet, these\nmodels often lack transparency, particularly in computer vision.\nPrototypical-parts-based neural networks have emerged as a promising solution\nby offering concept-level explanations. However, most are limited to\nfine-grained classification tasks, with few exceptions such as InfoDisent.\nInfoDisent extends prototypical models to large-scale datasets like ImageNet,\nbut produces complex explanations.\n  We introduce Sparse Information Disentanglement for Explainability (SIDE), a\nnovel method that improves the interpretability of prototypical parts through a\ndedicated training and pruning scheme that enforces sparsity. Combined with\nsigmoid activations in place of softmax, this approach allows SIDE to associate\neach class with only a small set of relevant prototypes. Extensive experiments\nshow that SIDE matches the accuracy of existing methods while reducing\nexplanation size by over $90\\%$, substantially enhancing the understandability\nof prototype-based explanations.", "AI": {"tldr": "本文提出SIDE方法，通过稀疏性训练和剪枝，以及使用Sigmoid激活函数，显著提高了原型部件解释的可理解性，同时保持了模型精度，将解释大小减少了90%以上。", "motivation": "深度神经网络在医学成像、自动驾驶等高风险领域缺乏透明度，难以理解其决策过程。现有的基于原型部件的神经网络虽能提供概念级解释，但大多限于细粒度分类任务，少数如InfoDisent虽能处理大规模数据集，但其解释过于复杂，不易理解。", "method": "本文提出了稀疏信息解缠解释（SIDE）方法。该方法通过专门的训练和剪枝方案来强制执行稀疏性，并用Sigmoid激活函数代替Softmax。这使得SIDE能够将每个类别仅与一小部分相关的原型相关联，从而简化了原型部件的解释。", "result": "SIDE方法在实验中达到了现有方法的准确性水平，同时将解释大小减少了90%以上。这显著增强了基于原型解释的可理解性。", "conclusion": "SIDE通过引入稀疏性训练和Sigmoid激活，有效提升了原型部件解释的清晰度和可理解性，使其在保持高精度的同时，能为每个类别提供更简洁、更具洞察力的解释，从而更好地满足高风险领域对模型透明度的需求。"}}
{"id": "2507.19374", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.19374", "abs": "https://arxiv.org/abs/2507.19374", "authors": ["Penny Karanasou", "Mengjie Qian", "Stefano Bannò", "Mark J. F. Gales", "Kate M. Knill"], "title": "Data Augmentation for Spoken Grammatical Error Correction", "comment": "This work has been accepted by ISCA SLaTE 2025", "summary": "While there exist strong benchmark datasets for grammatical error correction\n(GEC), high-quality annotated spoken datasets for Spoken GEC (SGEC) are still\nunder-resourced. In this paper, we propose a fully automated method to generate\naudio-text pairs with grammatical errors and disfluencies. Moreover, we propose\na series of objective metrics that can be used to evaluate the generated data\nand choose the more suitable dataset for SGEC. The goal is to generate an\naugmented dataset that maintains the textual and acoustic characteristics of\nthe original data while providing new types of errors. This augmented dataset\nshould augment and enrich the original corpus without altering the language\nassessment scores of the second language (L2) learners. We evaluate the use of\nthe augmented corpus both for written GEC (the text part) and for SGEC (the\naudio-text pairs). Our experiments are conducted on the S\\&I Corpus, the first\npublicly available speech dataset with grammar error annotations.", "AI": {"tldr": "提出一种全自动方法生成语音语法错误纠正（SGEC）数据集，并提供评估指标。", "motivation": "现有语法错误纠正（GEC）基准数据集很强，但高质量的语音语法错误纠正（SGEC）标注数据集资源不足。", "method": "提出一种全自动方法来生成包含语法错误和不流畅的音频-文本对。同时，提出一系列客观指标来评估生成的数据，以选择更适合SGEC的数据集。目标是生成一个在保持原始数据文本和声学特征的同时提供新类型错误的增强数据集，且不改变第二语言（L2）学习者的语言评估分数。", "result": "在S&I语料库上，对增强语料库在书面GEC（文本部分）和SGEC（音频-文本对）中的使用进行了评估。", "conclusion": "该方法能够生成用于SGEC的增强数据集，该数据集在保持原始数据特征的同时引入新错误类型，并已在书面GEC和SGEC中得到评估，证明其潜力。"}}
{"id": "2507.18997", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18997", "abs": "https://arxiv.org/abs/2507.18997", "authors": ["Zixiang Ai", "Zhenyu Cui", "Yuxin Peng", "Jiahuan Zhou"], "title": "UPP: Unified Point-Level Prompting for Robust Point Cloud Analysis", "comment": "Accepted by ICCV 2025 as a Poster", "summary": "Pre-trained point cloud analysis models have shown promising advancements in\nvarious downstream tasks, yet their effectiveness is typically suffering from\nlow-quality point cloud (i.e., noise and incompleteness), which is a common\nissue in real scenarios due to casual object occlusions and unsatisfactory data\ncollected by 3D sensors. To this end, existing methods focus on enhancing point\ncloud quality by developing dedicated denoising and completion models. However,\ndue to the isolation between the point cloud enhancement and downstream tasks,\nthese methods fail to work in various real-world domains. In addition, the\nconflicting objectives between denoising and completing tasks further limit the\nensemble paradigm to preserve critical geometric features. To tackle the above\nchallenges, we propose a unified point-level prompting method that reformulates\npoint cloud denoising and completion as a prompting mechanism, enabling robust\nanalysis in a parameter-efficient manner. We start by introducing a\nRectification Prompter to adapt to noisy points through the predicted\nrectification vector prompts, effectively filtering noise while preserving\nintricate geometric features essential for accurate analysis. Sequentially, we\nfurther incorporate a Completion Prompter to generate auxiliary point prompts\nbased on the rectified point clouds, facilitating their robustness and\nadaptability. Finally, a Shape-Aware Unit module is exploited to efficiently\nunify and capture the filtered geometric features for the downstream point\ncloud analysis.Extensive experiments on four datasets demonstrate the\nsuperiority and robustness of our method when handling noisy and incomplete\npoint cloud data against existing state-of-the-art methods. Our code is\nreleased at https://github.com/zhoujiahuan1991/ICCV2025-UPP.", "AI": {"tldr": "针对低质量点云（噪声和不完整性）问题，本文提出一种统一的点级别提示方法（UPP），将点云去噪和补全重构为提示机制，以参数高效的方式实现鲁棒的点云分析，并优于现有方法。", "motivation": "预训练点云分析模型在真实场景中常因传感器数据质量不佳和物体遮挡导致点云质量低下（噪声、不完整），影响其性能。现有方法专注于独立的去噪和补全模型，但它们与下游任务隔离，且去噪和补全的目标冲突，难以协同工作并保留关键几何特征。", "method": "本文提出一种统一的点级别提示方法（UPP），将点云去噪和补全重构为提示机制。具体包括：1) 引入一个“校正提示器”（Rectification Prompter），通过预测校正向量提示来适应噪声点，有效过滤噪声并保留几何特征。2) 进一步集成一个“补全提示器”（Completion Prompter），基于校正后的点云生成辅助点提示，增强鲁棒性和适应性。3) 利用一个“形状感知单元”（Shape-Aware Unit）模块，高效地统一并捕获过滤后的几何特征，用于下游点云分析。", "result": "在四个数据集上进行的广泛实验表明，本文方法在处理有噪声和不完整的点云数据时，相比现有最先进方法展现出卓越的性能和鲁棒性。", "conclusion": "通过将点云去噪和补全整合为统一的提示机制，本文提出的点级别提示方法有效解决了低质量点云的挑战，实现了鲁棒且优越的点云分析能力。"}}
{"id": "2507.19362", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19362", "abs": "https://arxiv.org/abs/2507.19362", "authors": ["Yusuke Hirota", "Boyi Li", "Ryo Hachiuma", "Yueh-Hua Wu", "Boris Ivanovic", "Yuta Nakashima", "Marco Pavone", "Yejin Choi", "Yu-Chiang Frank Wang", "Chao-Han Huck Yang"], "title": "LOTUS: A Leaderboard for Detailed Image Captioning from Quality to Societal Bias and User Preferences", "comment": "Accepted to ACL 2025. Leaderboard:\n  huggingface.co/spaces/nvidia/lotus-vlm-bias-leaderboard", "summary": "Large Vision-Language Models (LVLMs) have transformed image captioning,\nshifting from concise captions to detailed descriptions. We introduce LOTUS, a\nleaderboard for evaluating detailed captions, addressing three main gaps in\nexisting evaluations: lack of standardized criteria, bias-aware assessments,\nand user preference considerations. LOTUS comprehensively evaluates various\naspects, including caption quality (e.g., alignment, descriptiveness), risks\n(\\eg, hallucination), and societal biases (e.g., gender bias) while enabling\npreference-oriented evaluations by tailoring criteria to diverse user\npreferences. Our analysis of recent LVLMs reveals no single model excels across\nall criteria, while correlations emerge between caption detail and bias risks.\nPreference-oriented evaluations demonstrate that optimal model selection\ndepends on user priorities.", "AI": {"tldr": "LOTUS是一个评估大型视觉语言模型(LVLM)详细图像描述的排行榜，旨在解决现有评估中缺乏标准化、偏差意识和用户偏好考虑的问题，并发现没有单一模型在所有标准上表现最佳。", "motivation": "LVLM使图像描述从简洁转向详细，但现有评估存在三个主要缺陷：缺乏标准化评估标准、缺乏对偏差的感知评估以及未能充分考虑用户偏好。", "method": "引入了LOTUS排行榜，用于全面评估LVLM生成的详细图像描述。LOTUS涵盖了多个评估维度，包括描述质量（如对齐性、描述性）、风险（如幻觉）和社会偏差（如性别偏见），并允许根据不同用户偏好进行定制化的偏好导向评估。", "result": "对近期LVLM的分析显示，没有一个模型能在所有评估标准上都表现优异。同时，发现描述的详细程度与偏差风险之间存在相关性。偏好导向评估表明，最佳的模型选择取决于用户的具体优先级。", "conclusion": "LVLM的详细图像描述需要更全面、更具偏好意识的评估方法。模型的最佳性能并非普适，而是与用户需求紧密相关，提示在实际应用中需根据具体偏好选择模型。"}}
{"id": "2507.19396", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19396", "abs": "https://arxiv.org/abs/2507.19396", "authors": ["Rachel M. Murphy", "Nishant Mishra", "Nicolette F. de Keizer", "Dave A. Dongelmans", "Kitty J. Jager", "Ameen Abu-Hanna", "Joanna E. Klopotowska", "Iacer Calixto"], "title": "Detection of Adverse Drug Events in Dutch clinical free text documents using Transformer Models: benchmark study", "comment": "30 Pages, 5 Figures (Main Paper), 19 Pages, 2 Figures(Supplements).\n  Rachel M. Murphy and Nishant Mishra are shared first authors. Joanna E.\n  Klopotowska and Iacer Calixto are shared last authors", "summary": "In this study, we set a benchmark for adverse drug event (ADE) detection in\nDutch clinical free text documents using several transformer models, clinical\nscenarios and fit-for-purpose performance measures. We trained a Bidirectional\nLong Short-Term Memory (Bi-LSTM) model and four transformer-based Dutch and/or\nmultilingual encoder models (BERTje, RobBERT, MedRoBERTa.nl, and NuNER) for the\ntasks of named entity recognition (NER) and relation classification (RC) using\n102 richly annotated Dutch ICU clinical progress notes. Anonymized free text\nclinical progress notes of patients admitted to intensive care unit (ICU) of\none academic hospital and discharge letters of patients admitted to Internal\nMedicine wards of two non-academic hospitals were reused. We evaluated our ADE\nRC models internally using gold standard (two-step task) and predicted entities\n(end-to-end task). In addition, all models were externally validated on\ndetecting ADEs at the document level. We report both micro- and macro-averaged\nF1 scores, given the imbalance of ADEs in the datasets. Although differences\nfor the ADE RC task between the models were small, MedRoBERTa.nl was the best\nperforming model with macro-averaged F1 score of 0.63 using gold standard and\n0.62 using predicted entities. The MedRoBERTa.nl models also performed the best\nin our external validation and achieved recall of between 0.67 to 0.74 using\npredicted entities, meaning between 67 to 74% of discharge letters with ADEs\nwere detected. Our benchmark study presents a robust and clinically meaningful\napproach for evaluating language models for ADE detection in clinical free text\ndocuments. Our study highlights the need to use appropriate performance\nmeasures fit for the task of ADE detection in clinical free-text documents and\nenvisioned future clinical use.", "AI": {"tldr": "本研究为荷兰语临床自由文本中的不良药物事件（ADE）检测设立了基准，并评估了多种Transformer模型，发现MedRoBERTa.nl表现最佳。", "motivation": "在荷兰语临床自由文本中，不良药物事件（ADE）的自动检测缺乏一个全面的基准评估，且需要适合临床场景的性能度量。", "method": "研究训练了一个双向长短期记忆网络（Bi-LSTM）和四种基于Transformer的荷兰语/多语言编码模型（BERTje, RobBERT, MedRoBERTa.nl, NuNER），用于命名实体识别（NER）和关系分类（RC）任务。数据包括102份富含注释的荷兰语ICU临床进展记录，以及来自一家学术医院ICU和两家非学术医院内科的匿名临床自由文本。模型在内部使用金标准实体（两步任务）和预测实体（端到端任务）进行评估，并在文档层面进行外部验证。性能指标包括微观和宏观平均F1分数。", "result": "尽管模型在ADE关系分类任务上的差异较小，但MedRoBERTa.nl表现最佳。它在使用金标准实体时获得了0.63的宏观平均F1分数，使用预测实体时为0.62。在外部验证中，MedRoBERTa.nl模型同样表现最佳，使用预测实体时召回率在0.67到0.74之间，这意味着67%到74%含有ADE的出院信被成功检测。", "conclusion": "本基准研究为荷兰语临床自由文本中的ADE检测提供了一个稳健且具有临床意义的语言模型评估方法。研究强调了使用适合临床任务和未来临床用途的适当性能度量的重要性。"}}
{"id": "2507.18998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18998", "abs": "https://arxiv.org/abs/2507.18998", "authors": ["Yongsong Huang", "Tomo Miyazaki", "Xiaofeng Liu", "Shinichiro Omachi"], "title": "GPSMamba: A Global Phase and Spectral Prompt-guided Mamba for Infrared Image Super-Resolution", "comment": "This manuscript is under review, and copyright will be transferred\n  without notice", "summary": "Infrared Image Super-Resolution (IRSR) is challenged by the low contrast and\nsparse textures of infrared data, requiring robust long-range modeling to\nmaintain global coherence. While State-Space Models like Mamba offer\nproficiency in modeling long-range dependencies for this task, their inherent\n1D causal scanning mechanism fragments the global context of 2D images,\nhindering fine-detail restoration. To address this, we propose Global Phase and\nSpectral Prompt-guided Mamba (GPSMamba), a framework that synergizes\narchitectural guidance with non-causal supervision. First, our Adaptive\nSemantic-Frequency State Space Module (ASF-SSM) injects a fused\nsemantic-frequency prompt directly into the Mamba block, integrating non-local\ncontext to guide reconstruction. Then, a novel Thermal-Spectral Attention and\nPhase Consistency Loss provides explicit, non-causal supervision to enforce\nglobal structural and spectral fidelity. By combining these two innovations,\nour work presents a systematic strategy to mitigate the limitations of causal\nmodeling. Extensive experiments demonstrate that GPSMamba achieves\nstate-of-the-art performance, validating our approach as a powerful new\nparadigm for infrared image restoration. Code is available at\nhttps://github.com/yongsongH/GPSMamba.", "AI": {"tldr": "针对红外图像超分辨率（IRSR）中Mamba模型一维因果扫描限制，本文提出GPSMamba框架，通过融合语义-频率提示和非因果的结构与频谱一致性损失，实现全局上下文建模，显著提升红外图像复原性能。", "motivation": "红外图像超分辨率（IRSR）面临对比度低和纹理稀疏的挑战，需要鲁棒的长距离建模来保持全局一致性。Mamba等状态空间模型虽然擅长长距离依赖建模，但其固有的1D因果扫描机制会割裂2D图像的全局上下文，阻碍细节恢复。", "method": "本文提出了全局相位和频谱提示引导的Mamba（GPSMamba）框架，该框架结合了架构引导和非因果监督。具体方法包括：1) 自适应语义-频率状态空间模块（ASF-SSM），将融合的语义-频率提示直接注入Mamba块，整合非局部上下文以指导重建；2) 新颖的热谱注意力与相位一致性损失，提供明确的非因果监督，以强制实现全局结构和频谱保真度。", "result": "通过结合上述两项创新，GPSMamba有效缓解了因果建模的局限性。广泛的实验证明，GPSMamba实现了最先进的性能。", "conclusion": "GPSMamba提供了一种系统性的策略来减轻因果建模的局限性，并被验证为红外图像复原领域强大的新范式。"}}
{"id": "2507.19398", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19398", "abs": "https://arxiv.org/abs/2507.19398", "authors": ["Rajesh Madhipati", "Sheethal Bhat", "Lukas Buess", "Andreas Maier"], "title": "CXR-CML: Improved zero-shot classification of long-tailed multi-label diseases in Chest X-Rays", "comment": null, "summary": "Chest radiography (CXR) plays a crucial role in the diagnosis of various\ndiseases. However, the inherent class imbalance in the distribution of clinical\nfindings presents a significant challenge for current self-supervised deep\nlearning models. These models often fail to accurately classify long-tailed\nclasses. Current Vision-Language models such as Contrastive Language Image\nPre-training (CLIP) models effectively model the manifold distribution of the\nlatent space, enabling high zero-shot classification accuracies. Although CLIP\nperforms well on most of the primary classes in the dataset, our work reveals\nthat its effectiveness decreases significantly for classes with a long-tailed\ndistribution. Our approach employs a class-weighting mechanism that directly\naligns with the distribution of classes within the latent space. This method\nensures a substantial improvement in overall classification performance, with\nparticular emphasis on enhancing the recognition and accuracy of rarely\nobserved classes. We accomplish this by applying Gaussian Mixture Model (GMM)\nclustering to the latent space. The subsequent clusters are further refined by\nStudent t-distribution, followed by a metric loss that utilizes the altered\nembeddings. Our approach facilitates stable and adaptive clustering of the\nfeatures. This results in a notable average improvement of 7\\% points in\nzero-shot AUC scores across 40 classes in the MIMIC-CXR-JPG dataset from\nprevious SOTA models.", "AI": {"tldr": "本文提出一种新的方法，通过在CLIP模型的潜在空间中应用高斯混合模型（GMM）聚类和Student t-分布细化，并结合度量损失，来解决胸部X光片（CXR）诊断中长尾类别的分类不准确问题，显著提升了稀有类别的识别和整体分类性能。", "motivation": "胸部X光片诊断中存在固有的类别不平衡，导致当前自监督深度学习模型（包括CLIP）在分类长尾类别时表现不佳，无法准确识别不常出现的疾病。", "method": "该方法引入了一个与潜在空间中类别分布直接对齐的类别加权机制。具体而言，它首先对潜在空间应用高斯混合模型（GMM）进行聚类，然后通过Student t-分布进一步细化这些聚类，最后利用修改后的嵌入应用度量损失，以实现特征的稳定自适应聚类。", "result": "该方法在MIMIC-CXR-JPG数据集的40个类别上，相对于之前的SOTA模型，零样本AUC分数平均提高了7个百分点，尤其显著提升了稀有类别的识别准确性。", "conclusion": "通过在潜在空间中进行稳定和自适应的特征聚类，该方法有效解决了长尾分布问题，显著提高了胸部X光片诊断的整体分类性能，尤其在识别不常观察到的类别方面表现突出。"}}
{"id": "2507.19407", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19407", "abs": "https://arxiv.org/abs/2507.19407", "authors": ["Mohammad Khodadad", "Ali Shiraee", "Mahdi Astaraki", "Hamidreza Mahyar"], "title": "Towards Domain Specification of Embedding Models in Medicine", "comment": null, "summary": "Medical text embedding models are foundational to a wide array of healthcare\napplications, ranging from clinical decision support and biomedical information\nretrieval to medical question answering, yet they remain hampered by two\ncritical shortcomings. First, most models are trained on a narrow slice of\nmedical and biological data, beside not being up to date in terms of\nmethodology, making them ill suited to capture the diversity of terminology and\nsemantics encountered in practice. Second, existing evaluations are often\ninadequate: even widely used benchmarks fail to generalize across the full\nspectrum of real world medical tasks.\n  To address these gaps, we leverage MEDTE, a GTE model extensively fine-tuned\non diverse medical corpora through self-supervised contrastive learning across\nmultiple data sources, to deliver robust medical text embeddings.\n  Alongside this model, we propose a comprehensive benchmark suite of 51 tasks\nspanning classification, clustering, pair classification, and retrieval modeled\non the Massive Text Embedding Benchmark (MTEB) but tailored to the nuances of\nmedical text. Our results demonstrate that this combined approach not only\nestablishes a robust evaluation framework but also yields embeddings that\nconsistently outperform state of the art alternatives in different tasks.", "AI": {"tldr": "本文提出了MEDTE模型和一套包含51项任务的全面基准测试，旨在解决现有医学文本嵌入模型在数据多样性和评估充分性方面的不足，并显著提升了医学文本嵌入的性能。", "motivation": "现有医学文本嵌入模型存在两个主要缺陷：一是训练数据范围狭窄且方法论过时，难以捕捉实践中术语和语义的多样性；二是现有评估基准不足以泛化到真实世界的医学任务。", "method": "引入了MEDTE模型，这是一个通过多源自监督对比学习在多样化医学语料库上进行广泛微调的GTE模型。同时，提出了一个包含51项任务的综合基准测试套件，涵盖分类、聚类、配对分类和检索，该套件基于MTEB但针对医学文本的细微差别进行了定制。", "result": "研究结果表明，MEDTE模型与新基准测试相结合的方法不仅建立了一个稳健的评估框架，而且所产生的嵌入在不同任务中持续优于最先进的替代方案。", "conclusion": "通过MEDTE模型和全面的基准测试，本文成功解决了医学文本嵌入模型在数据多样性和评估方面的局限性，为医学文本处理提供了更强大、更通用的嵌入和更可靠的评估框架。"}}
{"id": "2507.19002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19002", "abs": "https://arxiv.org/abs/2507.19002", "authors": ["Ying Ba", "Tianyu Zhang", "Yalong Bai", "Wenyi Mo", "Tao Liang", "Bing Su", "Ji-Rong Wen"], "title": "Enhancing Reward Models for High-quality Image Generation: Beyond Text-Image Alignment", "comment": "Accepted to ICCV 2025", "summary": "Contemporary image generation systems have achieved high fidelity and\nsuperior aesthetic quality beyond basic text-image alignment. However, existing\nevaluation frameworks have failed to evolve in parallel. This study reveals\nthat human preference reward models fine-tuned based on CLIP and BLIP\narchitectures have inherent flaws: they inappropriately assign low scores to\nimages with rich details and high aesthetic value, creating a significant\ndiscrepancy with actual human aesthetic preferences. To address this issue, we\ndesign a novel evaluation score, ICT (Image-Contained-Text) score, that\nachieves and surpasses the objectives of text-image alignment by assessing the\ndegree to which images represent textual content. Building upon this\nfoundation, we further train an HP (High-Preference) score model using solely\nthe image modality to enhance image aesthetics and detail quality while\nmaintaining text-image alignment. Experiments demonstrate that the proposed\nevaluation model improves scoring accuracy by over 10\\% compared to existing\nmethods, and achieves significant results in optimizing state-of-the-art\ntext-to-image models. This research provides theoretical and empirical support\nfor evolving image generation technology toward higher-order human aesthetic\npreferences. Code is available at https://github.com/BarretBa/ICTHP.", "AI": {"tldr": "该研究指出现有图像生成评估模型（基于CLIP和BLIP）在评估细节丰富、美学价值高的图像时存在缺陷。为解决此问题，作者提出了新的评估分数ICT（用于文本-图像对齐）和HP（用于图像美学和细节），显著提高了评估准确性，并能优化SOTA文生图模型。", "motivation": "当代图像生成系统已达到高保真度和卓越美学质量，但现有评估框架未能同步发展。基于CLIP和BLIP架构微调的人类偏好奖励模型存在固有缺陷，它们不恰当地给细节丰富、美学价值高的图像打低分，与实际人类审美偏好存在显著差异。", "method": "1. 揭示了基于CLIP和BLIP架构的人类偏好奖励模型在评估上的缺陷。2. 设计了一种新的评估分数ICT（Image-Contained-Text），用于评估图像表示文本内容的程度，以实现并超越文本-图像对齐的目标。3. 在此基础上，进一步训练了一个HP（High-Preference）分数模型，该模型仅使用图像模态来增强图像美学和细节质量，同时保持文本-图像对齐。", "result": "所提出的评估模型与现有方法相比，评分准确率提高了10%以上。在优化最先进的文本到图像模型方面取得了显著成果。", "conclusion": "这项研究为图像生成技术向更高阶的人类审美偏好发展提供了理论和经验支持。新的评估框架能够更好地与人类审美偏好对齐，并有效优化先进的图像生成模型。"}}
{"id": "2507.19457", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE", "I.2.7; I.2.6; I.2.4; I.2.8"], "pdf": "https://arxiv.org/pdf/2507.19457", "abs": "https://arxiv.org/abs/2507.19457", "authors": ["Lakshya A Agrawal", "Shangyin Tan", "Dilara Soylu", "Noah Ziems", "Rishi Khare", "Krista Opsahl-Ong", "Arnav Singhvi", "Herumb Shandilya", "Michael J Ryan", "Meng Jiang", "Christopher Potts", "Koushik Sen", "Alexandros G. Dimakis", "Ion Stoica", "Dan Klein", "Matei Zaharia", "Omar Khattab"], "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.", "AI": {"tldr": "GEPA是一种利用自然语言反思的提示优化器，它通过分析系统轨迹来诊断问题、更新提示并结合经验，从而以极少的回合数显著优于基于强化学习和现有提示优化器的方法。", "motivation": "现有的大语言模型（LLM）强化学习方法（如GRPO）在适应下游任务时需要大量回合（数千次）才能学习新任务。本文认为，语言的可解释性为LLM提供了比稀疏标量奖励派生的策略梯度更丰富的学习介质。", "method": "引入了GEPA（Genetic-Pareto）提示优化器。GEPA通过采样系统级轨迹（如推理、工具调用、工具输出），用自然语言对其进行反思以诊断问题，提出并测试提示更新，并从自身尝试的帕累托前沿中结合互补的经验教训。", "result": "GEPA在四个任务上平均比GRPO性能高出10%，最高达20%，同时使用的回合数减少高达35倍。GEPA在两个LLM上比领先的提示优化器MIPROv2性能高出10%以上，并在代码优化推理时搜索策略方面展现出有前景的结果。", "conclusion": "GEPA的设计使其能够将少量回合转化为巨大的质量提升。自然语言反思为LLM提供了一种高效且有效的学习和提示优化方法，显著优于传统强化学习和其他优化器。"}}
{"id": "2507.19419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19419", "abs": "https://arxiv.org/abs/2507.19419", "authors": ["Mohammad Aflah Khan", "Ameya Godbole", "Johnny Tian-Zheng Wei", "Ryan Wang", "James Flemings", "Krishna Gummadi", "Willie Neiswanger", "Robin Jia"], "title": "TokenSmith: Streamlining Data Editing, Search, and Inspection for Large-Scale Language Model Training and Interpretability", "comment": null, "summary": "Understanding the relationship between training data and model behavior\nduring pretraining is crucial, but existing workflows make this process\ncumbersome, fragmented, and often inaccessible to researchers. We present\nTokenSmith, an open-source library for interactive editing, inspection, and\nanalysis of datasets used in Megatron-style pretraining frameworks such as\nGPT-NeoX, Megatron, and NVIDIA NeMo. TokenSmith supports a wide range of\noperations including searching, viewing, ingesting, exporting, inspecting, and\nsampling data, all accessible through a simple user interface and a modular\nbackend. It also enables structured editing of pretraining data without\nrequiring changes to training code, simplifying dataset debugging, validation,\nand experimentation.\n  TokenSmith is designed as a plug and play addition to existing large language\nmodel pretraining workflows, thereby democratizing access to production-grade\ndataset tooling. TokenSmith is hosted on GitHub1, with accompanying\ndocumentation and tutorials. A demonstration video is also available on\nYouTube.", "AI": {"tldr": "TokenSmith是一个开源库，旨在简化大型语言模型预训练数据（如Megatron-style框架）的交互式编辑、检查和分析过程。", "motivation": "现有预训练数据与模型行为关系理解的工作流程繁琐、分散且难以访问，阻碍了研究人员进行调试、验证和实验。", "method": "TokenSmith提供了一个带有简单用户界面和模块化后端的开源库，支持搜索、查看、摄取、导出、检查和采样数据等操作，并允许对预训练数据进行结构化编辑，而无需修改训练代码。", "result": "TokenSmith简化了数据集的调试、验证和实验过程，并使研究人员能够更容易地使用生产级别的MML数据工具。", "conclusion": "TokenSmith是一个可即插即用的工具，能够集成到现有的大型语言模型预训练工作流程中，从而提升数据管理和分析的效率和可访问性。"}}
{"id": "2507.19024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19024", "abs": "https://arxiv.org/abs/2507.19024", "authors": ["Zhiyuan Chen", "Yuecong Min", "Jie Zhang", "Bei Yan", "Jiahao Wang", "Xiaozhen Wang", "Shiguang Shan"], "title": "A Survey of Multimodal Hallucination Evaluation and Detection", "comment": "33 pages, 5 figures", "summary": "Multi-modal Large Language Models (MLLMs) have emerged as a powerful paradigm\nfor integrating visual and textual information, supporting a wide range of\nmulti-modal tasks. However, these models often suffer from hallucination,\nproducing content that appears plausible but contradicts the input content or\nestablished world knowledge. This survey offers an in-depth review of\nhallucination evaluation benchmarks and detection methods across Image-to-Text\n(I2T) and Text-to-image (T2I) generation tasks. Specifically, we first propose\na taxonomy of hallucination based on faithfulness and factuality, incorporating\nthe common types of hallucinations observed in practice. Then we provide an\noverview of existing hallucination evaluation benchmarks for both T2I and I2T\ntasks, highlighting their construction process, evaluation objectives, and\nemployed metrics. Furthermore, we summarize recent advances in hallucination\ndetection methods, which aims to identify hallucinated content at the instance\nlevel and serve as a practical complement of benchmark-based evaluation.\nFinally, we highlight key limitations in current benchmarks and detection\nmethods, and outline potential directions for future research.", "AI": {"tldr": "该综述深入探讨了多模态大语言模型（MLLMs）中的幻觉问题，对图像到文本（I2T）和文本到图像（T2I）任务中的幻觉评估基准和检测方法进行了全面回顾，并指出了未来的研究方向。", "motivation": "多模态大语言模型（MLLMs）在整合视觉和文本信息方面表现强大，但常产生与输入内容或世界知识相矛盾的“幻觉”内容，这促使研究人员对幻觉问题进行深入评估和检测。", "method": "本研究提出了一种基于忠实度和事实性的幻觉分类法，概述了现有针对T2I和I2T任务的幻觉评估基准（包括其构建过程、评估目标和指标），并总结了最新的幻觉检测方法。", "result": "研究提出了一个幻觉分类法，并提供了T2I和I2T任务的现有幻觉评估基准的详细概述。此外，还总结了旨在实例层面识别幻觉内容的检测方法，作为基准评估的补充。", "conclusion": "当前幻觉评估基准和检测方法存在局限性，未来研究应致力于解决这些限制并探索新的方向，以更有效地应对MLLMs中的幻觉问题。"}}
{"id": "2507.19470", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19470", "abs": "https://arxiv.org/abs/2507.19470", "authors": ["Son Quoc Tran", "Tushaar Gangavarapu", "Nicholas Chernogor", "Jonathan P. Chang", "Cristian Danescu-Niculescu-Mizil"], "title": "Conversations Gone Awry, But Then? Evaluating Conversational Forecasting Models", "comment": "Code and data available as part of ConvoKit:\n  https://convokit.cornell.edu", "summary": "We often rely on our intuition to anticipate the direction of a conversation.\nEndowing automated systems with similar foresight can enable them to assist\nhuman-human interactions. Recent work on developing models with this predictive\ncapacity has focused on the Conversations Gone Awry (CGA) task: forecasting\nwhether an ongoing conversation will derail. In this work, we revisit this task\nand introduce the first uniform evaluation framework, creating a benchmark that\nenables direct and reliable comparisons between different architectures. This\nallows us to present an up-to-date overview of the current progress in CGA\nmodels, in light of recent advancements in language modeling. Our framework\nalso introduces a novel metric that captures a model's ability to revise its\nforecast as the conversation progresses.", "AI": {"tldr": "本文引入了一个统一的评估框架和基准，用于预测对话是否会偏离（CGA）任务，并提出了一种衡量模型预测修正能力的新指标。", "motivation": "赋予自动化系统预测对话方向的能力，以辅助人机交互。现有关于预测对话偏离（CGA）模型的工作缺乏统一的评估框架，难以进行直接和可靠的比较。", "method": "引入了首个统一的评估框架，创建了一个基准数据集，使不同架构的模型能够直接可靠地进行比较。同时，提出了一种新的度量标准，用于捕捉模型随着对话进行修正其预测的能力。", "result": "该框架提供了一个最新的CGA模型进展概览，并结合了语言建模的最新进展，实现了模型间的直接可靠比较。", "conclusion": "所提出的统一评估框架和新指标，显著提高了CGA任务中模型评估的可靠性和深度，有助于推动该领域的研究进展。"}}
{"id": "2507.19045", "categories": ["cs.CV", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.19045", "abs": "https://arxiv.org/abs/2507.19045", "authors": ["Yufei Ma", "Hanwen Zhang", "Qiya Yang", "Guibo Luo", "Yuesheng Zhu"], "title": "A New One-Shot Federated Learning Framework for Medical Imaging Classification with Feature-Guided Rectified Flow and Knowledge Distillation", "comment": "Accepted at ECAI 2025", "summary": "In multi-center scenarios, One-Shot Federated Learning (OSFL) has attracted\nincreasing attention due to its low communication overhead, requiring only a\nsingle round of transmission. However, existing generative model-based OSFL\nmethods suffer from low training efficiency and potential privacy leakage in\nthe healthcare domain. Additionally, achieving convergence within a single\nround of model aggregation is challenging under non-Independent and Identically\nDistributed (non-IID) data. To address these challenges, in this paper a\nmodified OSFL framework is proposed, in which a new Feature-Guided Rectified\nFlow Model (FG-RF) and Dual-Layer Knowledge Distillation (DLKD) aggregation\nmethod are developed. FG-RF on the client side accelerates generative modeling\nin medical imaging scenarios while preserving privacy by synthesizing\nfeature-level images rather than pixel-level images. To handle non-IID\ndistributions, DLKD enables the global student model to simultaneously mimic\nthe output logits and align the intermediate-layer features of client-side\nteacher models during aggregation. Experimental results on three non-IID\nmedical imaging datasets show that our new framework and method outperform\nmulti-round federated learning approaches, achieving up to 21.73% improvement,\nand exceeds the baseline FedISCA by an average of 21.75%. Furthermore, our\nexperiments demonstrate that feature-level synthetic images significantly\nreduce privacy leakage risks compared to pixel-level synthetic images.", "AI": {"tldr": "本文提出了一种改进的单轮联邦学习（OSFL）框架，通过特征引导矫正流模型（FG-RF）和双层知识蒸馏（DLKD）方法，解决了医疗领域中现有OSFL方法的训练效率低、隐私泄露风险和非IID数据下收敛困难的问题。", "motivation": "现有的基于生成模型的单轮联邦学习（OSFL）方法在医疗健康领域存在训练效率低、潜在隐私泄露问题。此外，在非独立同分布（non-IID）数据下，单轮模型聚合难以实现收敛。", "method": "提出了一种新的OSFL框架，包括：1. 特征引导矫正流模型（FG-RF）：在客户端通过合成特征级图像而非像素级图像，加速生成建模并保护隐私。2. 双层知识蒸馏（DLKD）聚合方法：在聚合过程中，使全局学生模型同时模仿客户端教师模型的输出logits并对齐其中间层特征，以处理非IID数据分布。", "result": "在三个非IID医学影像数据集上的实验结果表明，该新框架和方法优于多轮联邦学习方法，性能提升高达21.73%，并且比基线FedISCA平均高出21.75%。此外，特征级合成图像显著降低了隐私泄露风险。", "conclusion": "所提出的OSFL框架及其FG-RF和DLKD方法有效解决了医疗领域中单轮联邦学习的训练效率、隐私保护和非IID数据收敛性挑战，在性能和隐私方面均表现出色。"}}
{"id": "2507.19478", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19478", "abs": "https://arxiv.org/abs/2507.19478", "authors": ["Xuehui Wang", "Zhenyu Wu", "JingJing Xie", "Zichen Ding", "Bowen Yang", "Zehao Li", "Zhaoyang Liu", "Qingyun Li", "Xuan Dong", "Zhe Chen", "Weiyun Wang", "Xiangyu Zhao", "Jixuan Chen", "Haodong Duan", "Tianbao Xie", "Chenyu Yang", "Shiqian Su", "Yue Yu", "Yuan Huang", "Yiqian Liu", "Xiao Zhang", "Yanting Zhang", "Xiangyu Yue", "Weijie Su", "Xizhou Zhu", "Wei Shen", "Jifeng Dai", "Wenhai Wang"], "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents", "comment": "in progress", "summary": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels: GUI Content Understanding, Element\nGrounding, Task Automation, and Task Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\nmetric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\ndeterminant of overall task success, emphasizing the substantial benefits of\nmodular frameworks that integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad\naction space, and long-term reasoning playing a critical role. More important,\ntask efficiency remains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization,\neffective planning, and early stopping strategies is indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\ndata, and running environment will be publicly available at\nhttps://github.com/open-compass/MMBench-GUI.", "AI": {"tldr": "MMBench-GUI是一个跨平台、分层级的GUI自动化代理基准测试，涵盖内容理解、元素定位、任务自动化和任务协作，并引入EQA指标评估效率，强调视觉定位、任务规划和效率优化的重要性。", "motivation": "现有GUI自动化评估缺乏一个全面、跨平台的基准，无法有效衡量代理的关键技能和执行效率，特别是效率方面存在显著的未探索维度。", "method": "引入MMBench-GUI，一个针对Windows、macOS、Linux、iOS、Android和Web平台的GUI自动化代理分层基准测试，包含GUI内容理解、元素定位、任务自动化和任务协作四个级别。同时，提出一种新的效率-质量面积（EQA）指标，用于评估在线自动化场景下的执行效率。", "result": "通过MMBench-GUI发现，准确的视觉定位是任务成功的关键决定因素，集成专业定位模块的模块化框架具有显著优势。代理需要强大的任务规划和跨平台泛化能力，长上下文记忆、广阔的动作空间和长期推理至关重要。所有模型都存在严重的效率低下问题，即使任务完成也包含大量冗余步骤。", "conclusion": "为实现真正高效和可扩展的GUI自动化，必须整合精确的本地化、有效的规划和早期停止策略。该基准测试的代码、评估数据和运行环境将公开可用。"}}
{"id": "2507.19052", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19052", "abs": "https://arxiv.org/abs/2507.19052", "authors": ["Hamid Abdollahi", "Amir Hossein Mansouri Majoumerd", "Amir Hossein Bagheri Baboukani", "Amir Abolfazl Suratgar", "Mohammad Bagher Menhaj"], "title": "Probing Multimodal Fusion in the Brain: The Dominance of Audiovisual Streams in Naturalistic Encoding", "comment": null, "summary": "Predicting brain activity in response to naturalistic, multimodal stimuli is\na key challenge in computational neuroscience. While encoding models are\nbecoming more powerful, their ability to generalize to truly novel contexts\nremains a critical, often untested, question. In this work, we developed brain\nencoding models using state-of-the-art visual (X-CLIP) and auditory (Whisper)\nfeature extractors and rigorously evaluated them on both in-distribution (ID)\nand diverse out-of-distribution (OOD) data. Our results reveal a fundamental\ntrade-off between model complexity and generalization: a higher-capacity\nattention-based model excelled on ID data, but a simpler linear model was more\nrobust, outperforming a competitive baseline by 18\\% on the OOD set.\nIntriguingly, we found that linguistic features did not improve predictive\naccuracy, suggesting that for familiar languages, neural encoding may be\ndominated by the continuous visual and auditory streams over redundant textual\ninformation. Spatially, our approach showed marked performance gains in the\nauditory cortex, underscoring the benefit of high-fidelity speech\nrepresentations. Collectively, our findings demonstrate that rigorous OOD\ntesting is essential for building robust neuro-AI models and provides nuanced\ninsights into how model architecture, stimulus characteristics, and sensory\nhierarchies shape the neural encoding of our rich, multimodal world.", "AI": {"tldr": "研究开发了基于先进视觉和听觉特征提取器的脑编码模型，并严格评估了其在分布内（ID）和分布外（OOD）数据上的泛化能力，发现模型复杂度和泛化能力之间存在权衡，且语言特征对预测准确性无改善。", "motivation": "计算神经科学中的一个关键挑战是预测大脑对自然、多模态刺激的活动响应，特别是编码模型泛化到真正新颖情境的能力仍是未经验证的关键问题。", "method": "开发了使用最先进视觉（X-CLIP）和听觉（Whisper）特征提取器的脑编码模型，并对其在分布内（ID）和多样化分布外（OOD）数据上进行了严格评估。比较了高容量基于注意力的模型和更简单的线性模型，并分析了语言特征的影响。", "result": "结果揭示了模型复杂度和泛化能力之间的基本权衡：高容量的注意力模型在ID数据上表现出色，但更简单的线性模型更鲁棒，在OOD数据集上比有竞争力的基线高出18%。有趣的是，语言特征没有提高预测准确性。空间上，该方法在听觉皮层表现出显著的性能提升。", "conclusion": "严格的OOD测试对于构建鲁棒的神经-AI模型至关重要，并提供了关于模型架构、刺激特征和感觉层级如何塑造我们丰富多模态世界神经编码的细致见解。"}}
{"id": "2507.19058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19058", "abs": "https://arxiv.org/abs/2507.19058", "authors": ["Chong Xia", "Shengjun Zhang", "Fangfu Liu", "Chang Liu", "Khodchaphun Hirunyaratsameewong", "Yueqi Duan"], "title": "ScenePainter: Semantically Consistent Perpetual 3D Scene Generation with Concept Relation Alignment", "comment": null, "summary": "Perpetual 3D scene generation aims to produce long-range and coherent 3D view\nsequences, which is applicable for long-term video synthesis and 3D scene\nreconstruction. Existing methods follow a \"navigate-and-imagine\" fashion and\nrely on outpainting for successive view expansion. However, the generated view\nsequences suffer from semantic drift issue derived from the accumulated\ndeviation of the outpainting module. To tackle this challenge, we propose\nScenePainter, a new framework for semantically consistent 3D scene generation,\nwhich aligns the outpainter's scene-specific prior with the comprehension of\nthe current scene. To be specific, we introduce a hierarchical graph structure\ndubbed SceneConceptGraph to construct relations among multi-level scene\nconcepts, which directs the outpainter for consistent novel views and can be\ndynamically refined to enhance diversity. Extensive experiments demonstrate\nthat our framework overcomes the semantic drift issue and generates more\nconsistent and immersive 3D view sequences. Project Page:\nhttps://xiac20.github.io/ScenePainter/.", "AI": {"tldr": "ScenePainter是一个用于生成语义一致的永久3D场景的框架，通过引入分层图结构SceneConceptGraph来解决现有方法中的语义漂移问题。", "motivation": "现有的永久3D场景生成方法在生成长程、连贯的3D视图序列时，依赖于“导航-想象”和外绘（outpainting），导致累积偏差产生语义漂移问题。", "method": "提出ScenePainter框架，通过将外绘器的场景特定先验与当前场景的理解对齐。引入了一个名为SceneConceptGraph的分层图结构，用于构建多级场景概念之间的关系，指导外绘器生成一致的新视图，并可动态优化以增强多样性。", "result": "实验证明，该框架克服了语义漂移问题，生成了更一致、更具沉浸感的3D视图序列。", "conclusion": "ScenePainter成功解决了永久3D场景生成中的语义漂移挑战，实现了长程、语义一致的3D视图序列生成。"}}
{"id": "2507.19059", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19059", "abs": "https://arxiv.org/abs/2507.19059", "authors": ["Xiaocheng Fang", "Jieyi Cai", "Huanyu Liu", "Wenxiu Cai", "Yishu Liu", "Bingzhi Chen"], "title": "Revisiting DETR for Small Object Detection via Noise-Resilient Query Optimization", "comment": "2025 IEEE International Conference on Multimedia and Expo (ICME)", "summary": "Despite advancements in Transformer-based detectors for small object\ndetection (SOD), recent studies show that these detectors still face challenges\ndue to inherent noise sensitivity in feature pyramid networks (FPN) and\ndiminished query quality in existing label assignment strategies. In this\npaper, we propose a novel Noise-Resilient Query Optimization (NRQO) paradigm,\nwhich innovatively incorporates the Noise-Tolerance Feature Pyramid Network\n(NT-FPN) and the Pairwise-Similarity Region Proposal Network (PS-RPN).\nSpecifically, NT-FPN mitigates noise during feature fusion in FPN by preserving\nspatial and semantic information integrity. Unlike existing label assignment\nstrategies, PS-RPN generates a sufficient number of high-quality positive\nqueries by enhancing anchor-ground truth matching through position and shape\nsimilarities, without the need for additional hyperparameters. Extensive\nexperiments on multiple benchmarks consistently demonstrate the superiority of\nNRQO over state-of-the-art baselines.", "AI": {"tldr": "针对小目标检测中Transformer检测器在FPN噪声敏感性和查询质量方面的挑战，本文提出了NRQO范式，通过NT-FPN降噪和PS-RPN优化查询生成，显著提升了性能。", "motivation": "尽管Transformer基检测器在小目标检测（SOD）方面取得了进展，但它们仍面临挑战：特征金字塔网络（FPN）固有的噪声敏感性，以及现有标签分配策略导致的查询质量下降。", "method": "本文提出了噪声鲁棒查询优化（NRQO）范式，该范式创新性地结合了噪声容忍特征金字塔网络（NT-FPN）和成对相似度区域提议网络（PS-RPN）。NT-FPN通过保持空间和语义信息完整性来减轻FPN特征融合过程中的噪声。PS-RPN通过增强锚框与真值框的位置和形状相似性匹配，生成足够数量的高质量正查询，且无需额外超参数。", "result": "在多个基准测试上的大量实验一致表明，NRQO优于最先进的基线方法。", "conclusion": "NRQO范式通过解决FPN的噪声敏感性和提升查询质量，有效改善了Transformer基检测器在小目标检测中的性能，达到了SOTA水平。"}}
{"id": "2507.19064", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19064", "abs": "https://arxiv.org/abs/2507.19064", "authors": ["Haochen Han", "Alex Jinpeng Wang", "Fangming Liu"], "title": "Negation-Aware Test-Time Adaptation for Vision-Language Models", "comment": "This paper will be submitted to the IEEE for possible publication", "summary": "In this paper, we study a practical but less-touched problem in\nVision-Language Models (VLMs), \\ie, negation understanding. Specifically, many\nreal-world applications require models to explicitly identify what is false or\nnon-existent, \\eg, radiologists may search for images that exclude specific\nconditions. Despite the impressive transferability of VLMs through large-scale\ntraining, they suffer from a critical limitation that fails to handle negation.\nTo address this challenge, existing methods attribute its root cause to the\nscarcity of negation training data and propose to fine-tune VLMs on massive\ndata containing explicit negation. Undoubtedly, such data-centric solutions\ndemand substantial data and computational resources, limiting their sustainable\nwidespread adoption. To tackle negation in a low-carbon manner, we empirically\nobserve that the key obstacle lies in the dual-concept shifts between the\naffirmation and negation distributions. Therefore, we propose a Negation-Aware\nTest-Time Adaptation (NEAT) method to efficiently adjust distribution-related\nparameters during inference. In brief, NEAT can reduce distribution shift in\nconsistent semantics while eliminating false distributional consistency in\nunrelated semantics. Extensive experiments on the various negation\nunderstanding tasks verify the effectiveness of the proposed method. The code\nis available at https://github.com/hhc1997/NEAT.", "AI": {"tldr": "本论文提出了一种名为NEAT（Negation-Aware Test-Time Adaptation）的低碳方法，通过在推理时调整分布相关参数，有效解决了视觉-语言模型（VLMs）在理解否定句方面的不足，避免了对大量否定训练数据的依赖。", "motivation": "尽管视觉-语言模型（VLMs）具有强大的迁移能力，但它们在理解否定概念（如识别“不存在”或“不包含”的物体/条件）方面存在严重缺陷，而这在许多实际应用中至关重要（例如放射科医生搜索排除特定病症的图像）。现有方法通过大量否定数据微调VLM，但这种数据密集型方案需要巨大的数据和计算资源，限制了其可持续普及。", "method": "作者观察到否定理解的关键障碍在于肯定和否定分布之间的双概念偏移。为此，他们提出了一种“否定感知测试时自适应（NEAT）”方法。NEAT在推理阶段高效地调整与分布相关的参数，旨在减少一致语义中的分布偏移，同时消除不相关语义中的错误分布一致性。", "result": "在各种否定理解任务上的广泛实验验证了所提出方法的有效性。", "conclusion": "NEAT方法通过有效处理肯定与否定之间的分布偏移，显著提高了视觉-语言模型对否定概念的理解能力，且以低碳、高效的方式实现了这一目标，避免了对大量训练数据的依赖。"}}
{"id": "2507.19071", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19071", "abs": "https://arxiv.org/abs/2507.19071", "authors": ["Yangyang Xu", "Bangzhen Liu", "Wenqi Shao", "Yong Du", "Shengfeng He", "Tingting Zhu"], "title": "Cross-Subject Mind Decoding from Inaccurate Representations", "comment": null, "summary": "Decoding stimulus images from fMRI signals has advanced with pre-trained\ngenerative models. However, existing methods struggle with cross-subject\nmappings due to cognitive variability and subject-specific differences. This\nchallenge arises from sequential errors, where unidirectional mappings generate\npartially inaccurate representations that, when fed into diffusion models,\naccumulate errors and degrade reconstruction fidelity. To address this, we\npropose the Bidirectional Autoencoder Intertwining framework for accurate\ndecoded representation prediction. Our approach unifies multiple subjects\nthrough a Subject Bias Modulation Module while leveraging bidirectional mapping\nto better capture data distributions for precise representation prediction. To\nfurther enhance fidelity when decoding representations into stimulus images, we\nintroduce a Semantic Refinement Module to improve semantic representations and\na Visual Coherence Module to mitigate the effects of inaccurate visual\nrepresentations. Integrated with ControlNet and Stable Diffusion, our method\noutperforms state-of-the-art approaches on benchmark datasets in both\nqualitative and quantitative evaluations. Moreover, our framework exhibits\nstrong adaptability to new subjects with minimal training samples.", "AI": {"tldr": "该研究提出了一种双向自编码器交织框架（BAI），用于解决现有fMRI信号图像解码方法在跨被试映射中存在的误差累积和重建保真度下降问题，通过引入被试偏差调制、双向映射、语义和视觉一致性模块，显著提升了图像解码的准确性和对新被试的适应性。", "motivation": "现有从fMRI信号解码刺激图像的方法在跨被试映射时表现不佳，原因在于认知变异性和个体差异导致单向映射产生部分不准确的表示，这些表示在输入扩散模型时会累积误差，从而降低重建保真度。", "method": "提出了双向自编码器交织框架（Bidirectional Autoencoder Intertwining framework），核心组件包括：1) 被试偏差调制模块（Subject Bias Modulation Module），用于统一多被试数据；2) 双向映射（bidirectional mapping），以更精确地捕获数据分布；3) 语义精炼模块（Semantic Refinement Module），以改善语义表示；4) 视觉一致性模块（Visual Coherence Module），以减轻不准确视觉表示的影响。该方法还与ControlNet和Stable Diffusion集成。", "result": "在基准数据集上的定性和定量评估中，该方法超越了最先进的方法。此外，该框架对新被试表现出强大的适应性，仅需少量训练样本即可。", "conclusion": "所提出的双向自编码器交织框架（BAI）通过解决跨被试映射中的误差累积问题，显著提高了从fMRI信号解码刺激图像的准确性和重建保真度，并展现出良好的泛化能力和对新被试的适应性。"}}
{"id": "2507.19074", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19074", "abs": "https://arxiv.org/abs/2507.19074", "authors": ["Shuiqing Zhao", "Meihuan Wang", "Jiaxuan Xu", "Jie Feng", "Wei Qian", "Rongchang Chen", "Zhenyu Liang", "Shouliang Qi", "Yanan Wu"], "title": "A Self-training Framework for Semi-supervised Pulmonary Vessel Segmentation and Its Application in COPD", "comment": null, "summary": "Background: It is fundamental for accurate segmentation and quantification of\nthe pulmonary vessel, particularly smaller vessels, from computed tomography\n(CT) images in chronic obstructive pulmonary disease (COPD) patients.\nObjective: The aim of this study was to segment the pulmonary vasculature using\na semi-supervised method. Methods: In this study, a self-training framework is\nproposed by leveraging a teacher-student model for the segmentation of\npulmonary vessels. First, the high-quality annotations are acquired in the\nin-house data by an interactive way. Then, the model is trained in the\nsemi-supervised way. A fully supervised model is trained on a small set of\nlabeled CT images, yielding the teacher model. Following this, the teacher\nmodel is used to generate pseudo-labels for the unlabeled CT images, from which\nreliable ones are selected based on a certain strategy. The training of the\nstudent model involves these reliable pseudo-labels. This training process is\niteratively repeated until an optimal performance is achieved. Results:\nExtensive experiments are performed on non-enhanced CT scans of 125 COPD\npatients. Quantitative and qualitative analyses demonstrate that the proposed\nmethod, Semi2, significantly improves the precision of vessel segmentation by\n2.3%, achieving a precision of 90.3%. Further, quantitative analysis is\nconducted in the pulmonary vessel of COPD, providing insights into the\ndifferences in the pulmonary vessel across different severity of the disease.\nConclusion: The proposed method can not only improve the performance of\npulmonary vascular segmentation, but can also be applied in COPD analysis. The\ncode will be made available at\nhttps://github.com/wuyanan513/semi-supervised-learning-for-vessel-segmentation.", "AI": {"tldr": "本研究提出了一种基于教师-学生模型的半监督自训练框架，用于COPD患者肺血管的CT图像分割，显著提高了分割精度，并可用于疾病分析。", "motivation": "在慢性阻塞性肺疾病（COPD）患者中，准确分割和量化CT图像中的肺血管，特别是小血管，对于疾病分析至关重要。", "method": "本研究提出了一种半监督自训练框架，利用教师-学生模型进行肺血管分割。首先，通过交互方式获取少量高质量标注数据。然后，在一个小规模标记数据集上训练一个全监督模型作为教师模型。教师模型为未标记CT图像生成伪标签，并根据特定策略选择可靠的伪标签。学生模型利用这些可靠的伪标签进行训练。此训练过程迭代重复，直至达到最佳性能。", "result": "在125名COPD患者的非增强CT扫描图像上进行了广泛实验。定量和定性分析表明，所提出的Semi2方法显著提高了血管分割的精度2.3%，达到90.3%。此外，对COPD患者肺血管的定量分析也为不同疾病严重程度下的肺血管差异提供了见解。", "conclusion": "所提出的方法不仅可以提高肺血管分割的性能，还可以应用于COPD的疾病分析。"}}
{"id": "2507.19076", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19076", "abs": "https://arxiv.org/abs/2507.19076", "authors": ["Rui Pan", "Ruiying Lu"], "title": "SP-Mamba: Spatial-Perception State Space Model for Unsupervised Medical Anomaly Detection", "comment": "11 pages", "summary": "Radiography imaging protocols target on specific anatomical regions,\nresulting in highly consistent images with recurrent structural patterns across\npatients. Recent advances in medical anomaly detection have demonstrated the\neffectiveness of CNN- and transformer-based approaches. However, CNNs exhibit\nlimitations in capturing long-range dependencies, while transformers suffer\nfrom quadratic computational complexity. In contrast, Mamba-based models,\nleveraging superior long-range modeling, structural feature extraction, and\nlinear computational efficiency, have emerged as a promising alternative. To\ncapitalize on the inherent structural regularity of medical images, this study\nintroduces SP-Mamba, a spatial-perception Mamba framework for unsupervised\nmedical anomaly detection. The window-sliding prototype learning and\nCircular-Hilbert scanning-based Mamba are introduced to better exploit\nconsistent anatomical patterns and leverage spatial information for medical\nanomaly detection. Furthermore, we excavate the concentration and contrast\ncharacteristics of anomaly maps for improving anomaly detection. Extensive\nexperiments on three diverse medical anomaly detection benchmarks confirm the\nproposed method's state-of-the-art performance, validating its efficacy and\nrobustness. The code is available at https://github.com/Ray-RuiPan/SP-Mamba.", "AI": {"tldr": "该研究提出了SP-Mamba，一个基于Mamba的空间感知框架，用于无监督医学异常检测，通过利用Mamba在长程依赖建模和线性计算效率方面的优势，并结合窗口滑动原型学习和圆形希尔伯特扫描，以更好地利用医学图像的结构规律性。", "motivation": "放射影像具有高度一致的结构模式。现有CNN模型在捕获长程依赖方面存在局限，而Transformer模型则面临二次计算复杂度问题。Mamba模型在长程建模和计算效率方面表现出潜力，因此被视为有前景的替代方案，以解决现有方法的不足并利用医学图像的固有结构规律性。", "method": "引入了SP-Mamba框架，包含窗口滑动原型学习（window-sliding prototype learning）和基于圆形希尔伯特扫描的Mamba（Circular-Hilbert scanning-based Mamba），旨在更好地利用一致的解剖模式和空间信息进行异常检测。此外，还挖掘了异常图的集中度和对比度特性以改进异常检测。", "result": "在三个不同的医学异常检测基准上进行了广泛实验，结果证实所提出的方法达到了最先进的性能。", "conclusion": "SP-Mamba框架在无监督医学异常检测任务中表现出卓越的有效性和鲁棒性，验证了其利用Mamba模型优势和空间感知策略的成功。"}}
{"id": "2507.19077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19077", "abs": "https://arxiv.org/abs/2507.19077", "authors": ["Yangyang Xu", "Xi Ye", "Duo Su"], "title": "Multi-Task Dense Prediction Fine-Tuning with Mixture of Fine-Grained Experts", "comment": "Accepted to ACM Multimedia 2025 (MM'25)", "summary": "Multi-task learning (MTL) for dense prediction has shown promising results\nbut still faces challenges in balancing shared representations with\ntask-specific specialization. In this paper, we introduce a novel Fine-Grained\nMixture of Experts (FGMoE) architecture that explores MoE-based MTL models\nthrough a combination of three key innovations and fine-tuning. First, we\npropose intra-task experts that partition along intermediate hidden dimensions\nof MLPs, enabling finer decomposition of task information while maintaining\nparameter efficiency. Second, we introduce shared experts that consolidate\ncommon information across different contexts of the same task, reducing\nredundancy, and allowing routing experts to focus on unique aspects. Third, we\ndesign a global expert that facilitates adaptive knowledge transfer across\ntasks based on both input feature and task requirements, promoting beneficial\ninformation sharing while preventing harmful interference. In addition, we use\nthe fine-tuning approach to improve parameter efficiency only by training the\nparameters of the decoder. Extensive experimental results show that the\nproposed FGMoE uses fewer parameters and significantly outperforms current\nMoE-based competitive MTL models on two dense prediction datasets\n(\\textit{i.e.,} NYUD-v2, PASCAL-Context) in various metrics.", "AI": {"tldr": "本文提出了一种新颖的细粒度专家混合（FGMoE）架构，用于密集预测的多任务学习（MTL），通过引入任务内专家、共享专家和全局专家，并结合微调策略，有效平衡了共享表示与任务特异性，实现了更少的参数和更优的性能。", "motivation": "密集预测的多任务学习（MTL）虽然已取得显著成果，但在平衡共享表示和任务特定专业化方面仍面临挑战。", "method": "本文提出FGMoE架构，包含三项关键创新：1) **任务内专家**：沿MLP的中间隐藏维度进行划分，实现任务信息的更精细分解并保持参数效率；2) **共享专家**：整合同一任务不同上下文的通用信息，减少冗余，使路由专家专注于独特方面；3) **全局专家**：基于输入特征和任务需求，促进跨任务的自适应知识迁移，促进有益共享并防止有害干扰。此外，采用微调方法，仅训练解码器参数以提高参数效率。", "result": "所提出的FGMoE模型在NYUD-v2和PASCAL-Context两个密集预测数据集上，使用更少的参数，并在多项指标上显著优于当前基于MoE的竞争性MTL模型。", "conclusion": "FGMoE架构通过其创新的专家设计和微调策略，成功解决了MTL中共享与特异性平衡的挑战，在密集预测任务上展现出卓越的性能和参数效率。"}}
{"id": "2507.19110", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19110", "abs": "https://arxiv.org/abs/2507.19110", "authors": ["Zhihui Guo", "Xin Man", "Hui Xu", "Jie Shao"], "title": "LISA: A Layer-wise Integration and Suppression Approach for Hallucination Mitigation in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel in vision-language tasks such\nas image captioning but remain prone to object hallucinations, where they\ndescribe objects that do not appear in the image. To mitigate this, we propose\n\\textbf{LISA}, a \\textbf{L}ayer-wise \\textbf{I}ntegration and\n\\textbf{S}uppression \\textbf{A}pproach that enhances generation consistency\nthrough hierarchical modulation and multi-layer fusion. LISA leverages the\nfunctional hierarchy within MLLMs, where shallow layers provide visual\ngrounding, middle layers encode semantics, and deep layers tend to amplify\nspurious signals. First, zone-specific spectral modulation stabilizes attention\nby suppressing over-amplified activations in deeper layers while preserving\nalignment cues in earlier layers. Second, token-level logits from selected\nlayers are fused via anchor-based routing, with token-wise anchor selection and\nsoft logit fusion enabling adaptive integration during decoding. LISA is fully\n\\textbf{plug-and-play} and can be seamlessly integrated into existing MLLMs,\nincluding Qwen2.5-VL. Experiments on multiple benchmarks show that LISA reduces\nhallucinations by up to 53.6\\% in $\\mathrm{CHAIR}_I$ and improves POPE F1 by\n4.5\\%, demonstrating strong generalization across models and tasks.", "AI": {"tldr": "LISA是一种层级整合与抑制方法，通过分层调制和多层融合来减少多模态大语言模型（MLLMs）中的物体幻觉问题，可即插即用，显著提升生成一致性。", "motivation": "多模态大语言模型（MLLMs）在视觉-语言任务中表现出色，但容易产生物体幻觉，即描述图像中不存在的物体。", "method": "LISA利用MLLM的功能层级（浅层提供视觉基础，中层编码语义，深层易放大虚假信号）。首先，通过区域特定谱调制抑制深层过度激活，同时保留早期层的对齐线索。其次，通过基于锚点的路由融合选定层的token级logits，实现解码时的自适应集成。LISA是即插即用的。", "result": "在多个基准测试中，LISA将CHAIR_I幻觉率降低了高达53.6%，POPE F1分数提高了4.5%，显示出强大的模型和任务泛化能力。", "conclusion": "LISA通过其层级整合与抑制方法，有效缓解了多模态大语言模型中的物体幻觉问题，显著提高了生成的一致性，并且具有良好的通用性和易用性。"}}
{"id": "2507.19118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19118", "abs": "https://arxiv.org/abs/2507.19118", "authors": ["Abu Sadat Mohammad Salehin Amit", "Xiaoli Zhang", "Md Masum Billa Shagar", "Zhaojun Liu", "Xiongfei Li", "Fanlong Meng"], "title": "Cross Spatial Temporal Fusion Attention for Remote Sensing Object Detection via Image Feature Matching", "comment": null, "summary": "Effectively describing features for cross-modal remote sensing image matching\nremains a challenging task due to the significant geometric and radiometric\ndifferences between multimodal images. Existing methods primarily extract\nfeatures at the fully connected layer but often fail to capture cross-modal\nsimilarities effectively. We propose a Cross Spatial Temporal Fusion (CSTF)\nmechanism that enhances feature representation by integrating scale-invariant\nkeypoints detected independently in both reference and query images. Our\napproach improves feature matching in two ways: First, by creating\ncorrespondence maps that leverage information from multiple image regions\nsimultaneously, and second, by reformulating the similarity matching process as\na classification task using SoftMax and Fully Convolutional Network (FCN)\nlayers. This dual approach enables CSTF to maintain sensitivity to distinctive\nlocal features while incorporating broader contextual information, resulting in\nrobust matching across diverse remote sensing modalities. To demonstrate the\npractical utility of improved feature matching, we evaluate CSTF on object\ndetection tasks using the HRSC2016 and DOTA benchmark datasets. Our method\nachieves state-of-theart performance with an average mAP of 90.99% on HRSC2016\nand 90.86% on DOTA, outperforming existing models. The CSTF model maintains\ncomputational efficiency with an inference speed of 12.5 FPS. These results\nvalidate that our approach to crossmodal feature matching directly enhances\ndownstream remote sensing applications such as object detection.", "AI": {"tldr": "针对跨模态遥感图像匹配中几何和辐射差异大的挑战，本文提出了一种跨时空融合（CSTF）机制，通过融合尺度不变关键点和将匹配重构为分类任务，有效提升了特征表示和匹配性能，并在目标检测任务上取得了SOTA结果。", "motivation": "由于多模态图像间显著的几何和辐射差异，有效描述用于跨模态遥感图像匹配的特征仍然是一个挑战。现有方法主要在全连接层提取特征，但往往未能有效捕获跨模态相似性。", "method": "本文提出了一种跨时空融合（CSTF）机制。该机制通过整合在参考图像和查询图像中独立检测到的尺度不变关键点来增强特征表示。具体而言，它通过两种方式改进特征匹配：一是创建同时利用多个图像区域信息的对应图；二是通过使用SoftMax和全卷积网络（FCN）层将相似性匹配过程重新表述为分类任务。这种双重方法使CSTF能够保持对独特局部特征的敏感性，同时结合更广泛的上下文信息。", "result": "CSTF在HRSC2016数据集上实现了90.99%的平均mAP，在DOTA数据集上实现了90.86%的平均mAP，性能优于现有模型，达到了最先进水平。该模型还保持了计算效率，推理速度为12.5 FPS。", "conclusion": "本文提出的跨模态特征匹配方法直接增强了下游遥感应用（如目标检测）的性能，实验结果验证了其有效性和实用性。"}}
{"id": "2507.19121", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19121", "abs": "https://arxiv.org/abs/2507.19121", "authors": ["Kaiyue Zhou", "Zelong Tan", "Hongxiao Wang", "Ya-li Li", "Shengjin Wang"], "title": "Preserving Topological and Geometric Embeddings for Point Cloud Recovery", "comment": null, "summary": "Recovering point clouds involves the sequential process of sampling and\nrestoration, yet existing methods struggle to effectively leverage both\ntopological and geometric attributes. To address this, we propose an end-to-end\narchitecture named \\textbf{TopGeoFormer}, which maintains these critical\nfeatures throughout the sampling and restoration phases. First, we revisit\ntraditional feature extraction techniques to yield topological embedding using\na continuous mapping of relative relationships between neighboring points, and\nintegrate it in both phases for preserving the structure of the original space.\nSecond, we propose the \\textbf{InterTwining Attention} to fully merge\ntopological and geometric embeddings, which queries shape with local awareness\nin both phases to form a learnable shape context facilitated with point-wise,\npoint-shape-wise, and intra-shape features. Third, we introduce a full geometry\nloss and a topological constraint loss to optimize the embeddings in both\nEuclidean and topological spaces. The geometry loss uses inconsistent matching\nbetween coarse-to-fine generations and targets for reconstructing better\ngeometric details, and the constraint loss limits embedding variances for\nbetter approximation of the topological space. In experiments, we\ncomprehensively analyze the circumstances using the conventional and\nlearning-based sampling/upsampling algorithms. The quantitative and qualitative\nresults demonstrate that our method significantly outperforms existing sampling\nand recovery methods.", "AI": {"tldr": "TopGeoFormer是一种端到端架构，通过在点云采样和恢复过程中整合拓扑和几何属性，显著提升了点云恢复性能。", "motivation": "现有方法在点云恢复的采样和恢复过程中，未能有效利用拓扑和几何属性。", "method": "该研究提出了TopGeoFormer架构：1) 重新审视特征提取，通过邻近点相对关系的连续映射生成拓扑嵌入。2) 提出交织注意力（InterTwining Attention）机制，融合拓扑和几何嵌入，形成可学习的形状上下文。3) 引入全几何损失和拓扑约束损失，在欧几里得空间和拓扑空间中优化嵌入。", "result": "实验结果表明，该方法在定量和定性上均显著优于现有的采样和恢复方法。", "conclusion": "TopGeoFormer通过有效整合拓扑和几何特征，解决了现有方法的局限性，实现了更优的点云恢复效果。"}}
{"id": "2507.19125", "categories": ["eess.IV", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.19125", "abs": "https://arxiv.org/abs/2507.19125", "authors": ["Yuqi Li", "Haotian Zhang", "Li Li", "Dong Liu"], "title": "Learned Image Compression with Hierarchical Progressive Context Modeling", "comment": "17 pages, ICCV 2025", "summary": "Context modeling is essential in learned image compression for accurately\nestimating the distribution of latents. While recent advanced methods have\nexpanded context modeling capacity, they still struggle to efficiently exploit\nlong-range dependency and diverse context information across different coding\nsteps. In this paper, we introduce a novel Hierarchical Progressive Context\nModel (HPCM) for more efficient context information acquisition. Specifically,\nHPCM employs a hierarchical coding schedule to sequentially model the\ncontextual dependencies among latents at multiple scales, which enables more\nefficient long-range context modeling. Furthermore, we propose a progressive\ncontext fusion mechanism that incorporates contextual information from previous\ncoding steps into the current step, effectively exploiting diverse contextual\ninformation. Experimental results demonstrate that our method achieves\nstate-of-the-art rate-distortion performance and strikes a better balance\nbetween compression performance and computational complexity. The code is\navailable at https://github.com/lyq133/LIC-HPCM.", "AI": {"tldr": "本文提出了一种新颖的分层渐进上下文模型（HPCM），用于学习图像压缩，以更有效地获取上下文信息，从而实现更好的压缩性能和计算效率。", "motivation": "现有上下文建模方法在学习图像压缩中难以有效利用长距离依赖和不同编码步骤中的多样化上下文信息，尽管它们已扩展了上下文建模能力。", "method": "HPCM采用分层编码调度，在多个尺度上顺序建模潜在变量之间的上下文依赖，以实现更高效的长距离上下文建模；同时，提出渐进上下文融合机制，将先前编码步骤的上下文信息融入当前步骤，有效利用多样化上下文信息。", "result": "实验结果表明，HPCM实现了最先进的码率-失真性能，并在压缩性能和计算复杂度之间取得了更好的平衡。", "conclusion": "HPCM通过其独特的分层和渐进式上下文建模方法，显著提升了学习图像压缩的效率和性能。"}}
{"id": "2507.19131", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19131", "abs": "https://arxiv.org/abs/2507.19131", "authors": ["Weitian Wang", "Rai Shubham", "Cecilia De La Parra", "Akash Kumar"], "title": "MixA-Q: Revisiting Activation Sparsity for Vision Transformers from a Mixed-Precision Quantization Perspective", "comment": "Accepted to ICCV 2025", "summary": "In this paper, we propose MixA-Q, a mixed-precision activation quantization\nframework that leverages intra-layer activation sparsity (a concept widely\nexplored in activation pruning methods) for efficient inference of quantized\nwindow-based vision transformers. For a given uniform-bit quantization\nconfiguration, MixA-Q separates the batched window computations within Swin\nblocks and assigns a lower bit width to the activations of less important\nwindows, improving the trade-off between model performance and efficiency. We\nintroduce a Two-Branch Swin Block that processes activations separately in\nhigh- and low-bit precision, enabling seamless integration of our method with\nmost quantization-aware training (QAT) and post-training quantization (PTQ)\nmethods, or with simple modifications. Our experimental evaluations over the\nCOCO dataset demonstrate that MixA-Q achieves a training-free 1.35x\ncomputational speedup without accuracy loss in PTQ configuration. With QAT,\nMixA-Q achieves a lossless 1.25x speedup and a 1.53x speedup with only a 1% mAP\ndrop by incorporating activation pruning. Notably, by reducing the quantization\nerror in important regions, our sparsity-aware quantization adaptation improves\nthe mAP of the quantized W4A4 model (with both weights and activations in 4-bit\nprecision) by 0.7%, reducing quantization degradation by 24%.", "AI": {"tldr": "MixA-Q是一种混合精度激活量化框架，通过利用层内激活稀疏性，为量化后的基于窗口的视觉Transformer提供高效推理，实现了计算加速并保持或提升了模型性能。", "motivation": "在量化后的基于窗口的视觉Transformer中，需要在模型性能和推理效率之间取得更好的平衡。作者旨在通过利用激活的层内稀疏性来改进这一权衡，从而实现更高效的推理。", "method": "MixA-Q框架将Swin块内的批量窗口计算分离，并为不那么重要的窗口的激活分配较低的位宽。为此，它引入了一个“双分支Swin块”，该块在高精度和低精度下分别处理激活，使其能够无缝集成到大多数量化感知训练(QAT)和后训练量化(PTQ)方法中。", "result": "在COCO数据集上，MixA-Q在PTQ配置下实现了1.35倍的无精度损失计算加速。在QAT配置下，实现了无损的1.25倍加速，并且通过结合激活剪枝，在仅有1%mAP下降的情况下实现了1.53倍加速。此外，通过减少重要区域的量化误差，它将量化W4A4模型的mAP提高了0.7%，减少了24%的量化退化。", "conclusion": "MixA-Q通过利用层内激活稀疏性并采用混合精度量化，显著提高了量化视觉Transformer的推理效率和性能，有效平衡了模型精度和计算成本。"}}
{"id": "2507.19140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19140", "abs": "https://arxiv.org/abs/2507.19140", "authors": ["Tianyu Zou", "Shengwu Xiong", "Ruilin Yao", "Yi Rong"], "title": "Balancing Conservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot Segmentation", "comment": "8 pages, 7 figures", "summary": "This paper studies the few-shot segmentation (FSS) task, which aims to\nsegment objects belonging to unseen categories in a query image by learning a\nmodel on a small number of well-annotated support samples. Our analysis of two\nmainstream FSS paradigms reveals that the predictions made by prototype\nlearning methods are usually conservative, while those of affinity learning\nmethods tend to be more aggressive. This observation motivates us to balance\nthe conservative and aggressive information captured by these two types of FSS\nframeworks so as to improve the segmentation performance. To achieve this, we\npropose a **P**rototype-**A**ffinity **H**ybrid **Net**work (PAHNet), which\nintroduces a Prototype-guided Feature Enhancement (PFE) module and an Attention\nScore Calibration (ASC) module in each attention block of an affinity learning\nmodel (called affinity learner). These two modules utilize the predictions\ngenerated by a pre-trained prototype learning model (called prototype\npredictor) to enhance the foreground information in support and query image\nrepresentations and suppress the mismatched foreground-background (FG-BG)\nrelationships between them, respectively. In this way, the aggressiveness of\nthe affinity learner can be effectively mitigated, thereby eventually\nincreasing the segmentation accuracy of our PAHNet method. Experimental results\nshow that PAHNet outperforms most recently proposed methods across 1-shot and\n5-shot settings on both PASCAL-5$^i$ and COCO-20$^i$ datasets, suggesting its\neffectiveness. The code is available at: [GitHub - tianyu-zou/PAHNet: Balancing\nConservatism and Aggressiveness: Prototype-Affinity Hybrid Network for Few-Shot\nSegmentation (ICCV'25)](https://github.com/tianyu-zou/PAHNet)", "AI": {"tldr": "本文提出PAHNet，一种原型-亲和力混合网络，通过结合原型学习的保守性和亲和力学习的激进性来提高少样本图像分割性能。", "motivation": "研究发现少样本分割（FSS）中，原型学习方法预测偏保守，亲和力学习方法预测偏激进。这种差异促使研究者寻求一种平衡二者信息的方法以提升分割性能。", "method": "提出PAHNet，在亲和力学习模型的每个注意力块中引入原型引导特征增强（PFE）模块和注意力分数校准（ASC）模块。这两个模块利用预训练原型学习模型的预测来增强前景信息并抑制前景-背景不匹配关系，从而有效缓解亲和力学习的激进性。", "result": "PAHNet在PASCAL-5$^i$和COCO-20$^i$数据集的1-shot和5-shot设置下，均优于大多数最新方法，证明了其有效性。", "conclusion": "PAHNet通过平衡原型学习的保守性和亲和力学习的激进性，显著提高了少样本分割的准确性。"}}
{"id": "2507.19141", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19141", "abs": "https://arxiv.org/abs/2507.19141", "authors": ["Jie Chen", "Zhangchi Hu", "Peixi Wu", "Huyue Zhu", "Hebei Li", "Xiaoyan Sun"], "title": "DASH: 4D Hash Encoding with Self-Supervised Decomposition for Real-Time Dynamic Scene Rendering", "comment": null, "summary": "Dynamic scene reconstruction is a long-term challenge in 3D vision. Existing\nplane-based methods in dynamic Gaussian splatting suffer from an unsuitable\nlow-rank assumption, causing feature overlap and poor rendering quality.\nAlthough 4D hash encoding provides an explicit representation without low-rank\nconstraints, directly applying it to the entire dynamic scene leads to\nsubstantial hash collisions and redundancy. To address these challenges, we\npresent DASH, a real-time dynamic scene rendering framework that employs 4D\nhash encoding coupled with self-supervised decomposition. Our approach begins\nwith a self-supervised decomposition mechanism that separates dynamic and\nstatic components without manual annotations or precomputed masks. Next, we\nintroduce a multiresolution 4D hash encoder for dynamic elements, providing an\nexplicit representation that avoids the low-rank assumption. Finally, we\npresent a spatio-temporal smoothness regularization strategy to mitigate\nunstable deformation artifacts. Experiments on real-world datasets demonstrate\nthat DASH achieves state-of-the-art dynamic rendering performance, exhibiting\nenhanced visual quality at real-time speeds of 264 FPS on a single 4090 GPU.\nCode: https://github.com/chenj02/DASH.", "AI": {"tldr": "DASH是一种实时的动态场景渲染框架，它结合了4D哈希编码和自监督分解来解决现有方法的缺陷，实现了高质量的实时渲染。", "motivation": "动态场景重建是一个长期挑战。现有的平面动态高斯splatting方法存在不合适的低秩假设，导致特征重叠和渲染质量差。而直接将4D哈希编码应用于整个动态场景会导致大量哈希冲突和冗余。", "method": "本文提出了DASH框架：1. 采用自监督分解机制，无需手动标注或预计算掩码即可分离动态和静态组件。2. 为动态元素引入多分辨率4D哈希编码器，提供显式表示并避免低秩假设。3. 引入时空平滑正则化策略，以减轻不稳定的变形伪影。", "result": "在真实世界数据集上的实验表明，DASH实现了最先进的动态渲染性能，在单张4090 GPU上以264 FPS的实时速度展现出增强的视觉质量。", "conclusion": "DASH通过结合自监督分解和多分辨率4D哈希编码，并辅以时空平滑正则化，有效解决了动态场景重建中的挑战，实现了高质量、实时、稳定的动态场景渲染。"}}
{"id": "2507.19175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19175", "abs": "https://arxiv.org/abs/2507.19175", "authors": ["Yuki Igaue", "Hiroaki Aizawa"], "title": "Patch Pruning Strategy Based on Robust Statistical Measures of Attention Weight Diversity in Vision Transformers", "comment": null, "summary": "Multi-head self-attention is a distinctive feature extraction mechanism of\nvision transformers that computes pairwise relationships among all input\npatches, contributing significantly to their high performance. However, it is\nknown to incur a quadratic computational complexity with respect to the number\nof patches. One promising approach to address this issue is patch pruning,\nwhich improves computational efficiency by identifying and removing redundant\npatches. In this work, we propose a patch pruning strategy that evaluates the\nimportance of each patch based on the variance of attention weights across\nmultiple attention heads. This approach is inspired by the design of multi-head\nself-attention, which aims to capture diverse attention patterns across\ndifferent subspaces of feature representations. The proposed method can be\neasily applied during both training and inference, and achieves improved\nthroughput while maintaining classification accuracy in scenarios such as\nfine-tuning with pre-trained models. In addition, we also found that using\nrobust statistical measures, such as the median absolute deviation in place of\nvariance, to assess patch importance can similarly lead to strong performance.\nFurthermore, by introducing overlapping patch embeddings, our method achieves\nbetter performance with comparable throughput to conventional approaches that\nutilize all patches.", "AI": {"tldr": "该论文提出了一种基于多头自注意力权重方差的补丁剪枝策略，以提高视觉Transformer的计算效率，同时保持分类精度。", "motivation": "视觉Transformer中的多头自注意力机制虽然性能优异，但其计算复杂度与输入补丁数量呈二次方关系，导致计算效率低下。", "method": "通过评估每个补丁在不同注意力头上的注意力权重方差来衡量其重要性，并移除冗余补丁。该方法可在训练和推理阶段应用。此外，还探索了使用中位数绝对偏差（MAD）等鲁棒统计量替代方差，并引入了重叠补丁嵌入以进一步提升性能。", "result": "所提出的方法在保持分类精度的同时，显著提高了吞吐量，尤其是在使用预训练模型进行微调的场景下。使用鲁棒统计量（如MAD）同样能带来良好性能。引入重叠补丁嵌入后，方法在相似吞吐量下实现了与传统使用所有补丁的方法相当甚至更好的性能。", "conclusion": "基于注意力权重方差的补丁剪枝是一种有效提高视觉Transformer计算效率的方法，通过结合鲁棒统计量和重叠补丁嵌入，可以进一步优化性能和效率。"}}
{"id": "2507.19184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19184", "abs": "https://arxiv.org/abs/2507.19184", "authors": ["Kotha Kartheek", "Lingamaneni Gnanesh Chowdary", "Snehasis Mukherjee"], "title": "Continual Learning-Based Unified Model for Unpaired Image Restoration Tasks", "comment": "Under Review", "summary": "Restoration of images contaminated by different adverse weather conditions\nsuch as fog, snow, and rain is a challenging task due to the varying nature of\nthe weather conditions. Most of the existing methods focus on any one\nparticular weather conditions. However, for applications such as autonomous\ndriving, a unified model is necessary to perform restoration of corrupted\nimages due to different weather conditions. We propose a continual learning\napproach to propose a unified framework for image restoration. The proposed\nframework integrates three key innovations: (1) Selective Kernel Fusion layers\nthat dynamically combine global and local features for robust adaptive feature\nselection; (2) Elastic Weight Consolidation (EWC) to enable continual learning\nand mitigate catastrophic forgetting across multiple restoration tasks; and (3)\na novel Cycle-Contrastive Loss that enhances feature discrimination while\npreserving semantic consistency during domain translation. Further, we propose\nan unpaired image restoration approach to reduce the dependance of the proposed\napproach on the training data. Extensive experiments on standard benchmark\ndatasets for dehazing, desnowing and deraining tasks demonstrate significant\nimprovements in PSNR, SSIM, and perceptual quality over the state-of-the-art.", "AI": {"tldr": "该论文提出了一种基于持续学习的统一框架，用于恢复受多种恶劣天气（如雾、雪、雨）影响的图像。", "motivation": "现有的图像恢复方法大多只针对单一天气条件，而自动驾驶等应用需要一个能处理不同天气导致的图像损坏的统一模型。", "method": "该框架集成了三项关键创新：1) 选择性核融合层（Selective Kernel Fusion layers）动态结合全局和局部特征；2) 弹性权重整合（Elastic Weight Consolidation, EWC）实现持续学习并减轻灾难性遗忘；3) 新颖的循环对比损失（Cycle-Contrastive Loss）在保持语义一致性的同时增强特征判别。此外，还提出了无配对图像恢复方法以减少对训练数据的依赖。", "result": "在去雾、去雪和去雨的标准基准数据集上进行了大量实验，结果显示在PSNR、SSIM和感知质量方面均显著优于现有最先进的方法。", "conclusion": "所提出的统一持续学习框架能有效恢复受多种恶劣天气影响的图像，并在性能上超越了现有技术水平。"}}
{"id": "2507.19186", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19186", "abs": "https://arxiv.org/abs/2507.19186", "authors": ["Niklas Bubeck", "Yundi Zhang", "Suprosanna Shit", "Daniel Rueckert", "Jiazhen Pan"], "title": "Reconstruct or Generate: Exploring the Spectrum of Generative Modeling for Cardiac MRI", "comment": null, "summary": "In medical imaging, generative models are increasingly relied upon for two\ndistinct but equally critical tasks: reconstruction, where the goal is to\nrestore medical imaging (usually inverse problems like inpainting or\nsuperresolution), and generation, where synthetic data is created to augment\ndatasets or carry out counterfactual analysis. Despite shared architecture and\nlearning frameworks, they prioritize different goals: generation seeks high\nperceptual quality and diversity, while reconstruction focuses on data fidelity\nand faithfulness. In this work, we introduce a \"generative model zoo\" and\nsystematically analyze how modern latent diffusion models and autoregressive\nmodels navigate the reconstruction-generation spectrum. We benchmark a suite of\ngenerative models across representative cardiac medical imaging tasks, focusing\non image inpainting with varying masking ratios and sampling strategies, as\nwell as unconditional image generation. Our findings show that diffusion models\noffer superior perceptual quality for unconditional generation but tend to\nhallucinate as masking ratios increase, whereas autoregressive models maintain\nstable perceptual performance across masking levels, albeit with generally\nlower fidelity.", "AI": {"tldr": "本文系统分析并比较了扩散模型和自回归模型在医学图像重建（如图像修复）和生成任务中的表现，发现它们在感知质量和数据保真度之间存在不同权衡。", "motivation": "医学成像中，生成模型被广泛用于图像重建（解决逆问题）和数据生成（增强数据集或反事实分析）。尽管架构相似，但它们的目标不同：生成追求感知质量和多样性，重建则侧重数据保真度。因此，有必要系统分析现代生成模型如何平衡这些目标。", "method": "构建了一个“生成模型动物园”，系统分析了潜在扩散模型和自回归模型。在代表性的心脏医学成像任务上进行基准测试，包括不同遮罩比例和采样策略下的图像修复，以及无条件图像生成。", "result": "扩散模型在无条件生成方面提供卓越的感知质量，但在遮罩比例增加时倾向于产生幻觉。自回归模型在不同遮罩水平下保持稳定的感知性能，但整体保真度较低。", "conclusion": "研究表明，扩散模型在纯生成任务中表现出色，但在高缺失数据下的重建任务中易产生幻觉，而自回归模型在重建任务中表现更稳定但保真度较低，这揭示了不同生成模型在感知质量和数据保真度之间的权衡。"}}
{"id": "2507.19188", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19188", "abs": "https://arxiv.org/abs/2507.19188", "authors": ["Haoang Lu", "Yuanqi Su", "Xiaoning Zhang", "Longjun Gao", "Yu Xue", "Le Wang"], "title": "VisHall3D: Monocular Semantic Scene Completion from Reconstructing the Visible Regions to Hallucinating the Invisible Regions", "comment": null, "summary": "This paper introduces VisHall3D, a novel two-stage framework for monocular\nsemantic scene completion that aims to address the issues of feature\nentanglement and geometric inconsistency prevalent in existing methods.\nVisHall3D decomposes the scene completion task into two stages: reconstructing\nthe visible regions (vision) and inferring the invisible regions\n(hallucination). In the first stage, VisFrontierNet, a visibility-aware\nprojection module, is introduced to accurately trace the visual frontier while\npreserving fine-grained details. In the second stage, OcclusionMAE, a\nhallucination network, is employed to generate plausible geometries for the\ninvisible regions using a noise injection mechanism. By decoupling scene\ncompletion into these two distinct stages, VisHall3D effectively mitigates\nfeature entanglement and geometric inconsistency, leading to significantly\nimproved reconstruction quality.\n  The effectiveness of VisHall3D is validated through extensive experiments on\ntwo challenging benchmarks: SemanticKITTI and SSCBench-KITTI-360. VisHall3D\nachieves state-of-the-art performance, outperforming previous methods by a\nsignificant margin and paves the way for more accurate and reliable scene\nunderstanding in autonomous driving and other applications.", "AI": {"tldr": "VisHall3D是一种新颖的两阶段单目语义场景补全框架，通过分解可见区域重建和不可见区域推断来解决特征纠缠和几何不一致问题。", "motivation": "现有单目语义场景补全方法存在特征纠缠和几何不一致的问题，导致重建质量不佳。", "method": "VisHall3D框架分为两个阶段：1. 重建可见区域（vision），引入VisFrontierNet（可见性感知投影模块）以精确追踪视觉边界并保留细节。2. 推断不可见区域（hallucination），使用OcclusionMAE（幻觉网络）结合噪声注入机制生成合理的几何形状。这种解耦有效缓解了特征纠缠和几何不一致。", "result": "VisHall3D在SemanticKITTI和SSCBench-KITTI-360两个基准测试上取得了最先进的性能，显著优于现有方法。", "conclusion": "VisHall3D为更准确、可靠的场景理解铺平了道路，对自动驾驶及其他应用具有重要意义。"}}
{"id": "2507.19209", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.19209", "abs": "https://arxiv.org/abs/2507.19209", "authors": ["Xiaoyu Zhang", "Zhifeng Bao", "Hai Dong", "Ziwei Wang", "Jiajun Liu"], "title": "Querying Autonomous Vehicle Point Clouds: Enhanced by 3D Object Counting with CounterNet", "comment": null, "summary": "Autonomous vehicles generate massive volumes of point cloud data, yet only a\nsubset is relevant for specific tasks such as collision detection, traffic\nanalysis, or congestion monitoring. Effectively querying this data is essential\nto enable targeted analytics. In this work, we formalize point cloud querying\nby defining three core query types: RETRIEVAL, COUNT, and AGGREGATION, each\naligned with distinct analytical scenarios. All these queries rely heavily on\naccurate object counts to produce meaningful results, making precise object\ncounting a critical component of query execution. Prior work has focused on\nindexing techniques for 2D video data, assuming detection models provide\naccurate counting information. However, when applied to 3D point cloud data,\nstate-of-the-art detection models often fail to generate reliable object\ncounts, leading to substantial errors in query results. To address this\nlimitation, we propose CounterNet, a heatmap-based network designed for\naccurate object counting in large-scale point cloud data. Rather than focusing\non accurate object localization, CounterNet detects object presence by finding\nobject centers to improve counting accuracy. We further enhance its performance\nwith a feature map partitioning strategy using overlapping regions, enabling\nbetter handling of both small and large objects in complex traffic scenes. To\nadapt to varying frame characteristics, we introduce a per-frame dynamic model\nselection strategy that selects the most effective configuration for each\ninput. Evaluations on three real-world autonomous vehicle datasets show that\nCounterNet improves counting accuracy by 5% to 20% across object categories,\nresulting in more reliable query outcomes across all supported query types.", "AI": {"tldr": "该研究针对自动驾驶点云数据查询中物体计数不准确的问题，提出了CounterNet模型，通过基于热图的中心点检测和动态模型选择策略，显著提升了物体计数精度，从而改善了查询结果的可靠性。", "motivation": "自动驾驶车辆生成的海量点云数据需要有效查询以进行特定分析。现有2D视频数据索引技术假设检测模型能提供准确计数，但应用于3D点云数据时，最先进的检测模型在生成可靠物体计数方面表现不佳，导致查询结果出现显著误差。", "method": "1. 形式化定义了点云查询的三种核心类型：检索、计数和聚合。2. 提出了CounterNet，一个基于热图的网络，通过检测物体中心点而非精确物体定位来提高计数精度。3. 引入了特征图分区策略（使用重叠区域）以更好地处理复杂交通场景中的大小物体。4. 提出了逐帧动态模型选择策略，根据帧特性选择最有效的配置。", "result": "在三个真实世界的自动驾驶数据集上评估显示，CounterNet将物体计数精度提高了5%到20%，从而使所有支持的查询类型都获得了更可靠的查询结果。", "conclusion": "CounterNet通过其创新的物体中心检测和动态适应策略，显著提升了大规模点云数据中的物体计数准确性，为自动驾驶车辆的分析查询提供了更可靠的基础。"}}
{"id": "2507.19213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19213", "abs": "https://arxiv.org/abs/2507.19213", "authors": ["Hanbing Wu", "Ping Jiang", "Anyang Su", "Chenxu Zhao", "Tianyu Fu", "Minghui Wu", "Beiping Tan", "Huiying Li"], "title": "PRE-MAP: Personalized Reinforced Eye-tracking Multimodal LLM for High-Resolution Multi-Attribute Point Prediction", "comment": null, "summary": "Visual selective attention, driven by individual preferences, regulates human\nprioritization of visual stimuli by bridging subjective cognitive mechanisms\nwith objective visual elements, thereby steering the semantic interpretation\nand hierarchical processing of dynamic visual scenes. However, existing models\nand datasets predominantly neglect the influence of subjective cognitive\ndiversity on fixation behavior. Conventional saliency prediction models,\ntypically employing segmentation approaches, rely on low-resolution imagery to\ngenerate saliency heatmaps, subsequently upscaled to native resolutions, which\nlimiting their capacity to capture personalized attention patterns.\nFurthermore, MLLMs are constrained by factors such as hallucinations, making it\nvery costly to strictly adhere to the expected format in tasks involving\nmultiple point predictions, and achieving precise point positioning is\nchallenging. To address these limitations, we present Subjective Personalized\nAttention for Advertisement Videos, namely SPA-ADV, a large-scale multimodal\ndataset capturing gaze behaviors from over 4,500 participants varying in age\nand gender with 486 videos. Furthermore, we propose PRE-MAP, a novel\neye-tracking saliency model that characterizes Personalized visual disparities\nthrough Reinforcement learning-optimized Eye-tracking, built upon MLLMs and\nguided by Multi-Attribute user profiles to predict Points. To ensure MLLMs\nproduce prediction points that are both format-correct and spatially accurate,\nwe introduce Consistency Group Relative Policy Optimization (C-GRPO), inspired\nby the variability in eye movement points and Multi-Attribute profiles.\nExtensive experiments on SPA-ADV and other benchmarks demonstrate the\neffectiveness of our approach. The code and dataset are available at\n\\href{https://github.com/mininglamp-MLLM/PRE-MAP}{this URL}.", "AI": {"tldr": "该研究提出了一个大规模多模态数据集SPA-ADV，以及一个名为PRE-MAP的新型眼动显著性模型，用于预测广告视频中受个体偏好影响的个性化视觉注意点，并引入C-GRPO以提高MLLM预测点的准确性。", "motivation": "现有的视觉显著性模型和数据集主要忽略了主观认知多样性对注视行为的影响；传统的显著性预测模型依赖低分辨率图像且难以捕捉个性化模式；多模态大语言模型（MLLMs）在多点预测任务中存在幻觉和难以严格遵循预期格式、实现精确点定位的挑战。", "method": "1. 构建了大规模多模态数据集SPA-ADV，包含4500多名参与者的眼动行为和486个视频。2. 提出了PRE-MAP模型，这是一个基于MLLM并由多属性用户画像引导的强化学习优化眼动模型，用于预测个性化视觉注意点。3. 引入了Consistency Group Relative Policy Optimization (C-GRPO) 策略，以确保MLLM生成的预测点格式正确且空间准确。", "result": "在SPA-ADV和其他基准数据集上的大量实验证明了所提出方法的有效性。", "conclusion": "该研究通过构建大规模个性化眼动数据集和提出新颖的基于强化学习的MLLM模型，有效解决了现有显著性预测模型和MLLM在捕捉个性化注意力和精确点定位方面的局限性。"}}
{"id": "2507.19230", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19230", "abs": "https://arxiv.org/abs/2507.19230", "authors": ["Niels Rocholl", "Ewoud Smit", "Mathias Prokop", "Alessa Hering"], "title": "Unstable Prompts, Unreliable Segmentations: A Challenge for Longitudinal Lesion Analysis", "comment": null, "summary": "Longitudinal lesion analysis is crucial for oncological care, yet automated\ntools often struggle with temporal consistency. While universal lesion\nsegmentation models have advanced, they are typically designed for single time\npoints. This paper investigates the performance of the ULS23 segmentation model\nin a longitudinal context. Using a public clinical dataset of baseline and\nfollow-up CT scans, we evaluated the model's ability to segment and track\nlesions over time. We identified two critical, interconnected failure modes: a\nsharp degradation in segmentation quality in follow-up cases due to inter-scan\nregistration errors, and a subsequent breakdown of the lesion correspondence\nprocess. To systematically probe this vulnerability, we conducted a controlled\nexperiment where we artificially displaced the input volume relative to the\ntrue lesion center. Our results demonstrate that the model's performance is\nhighly dependent on its assumption of a centered lesion; segmentation accuracy\ncollapses when the lesion is sufficiently displaced. These findings reveal a\nfundamental limitation of applying single-timepoint models to longitudinal\ndata. We conclude that robust oncological tracking requires a paradigm shift\naway from cascading single-purpose tools towards integrated, end-to-end models\ninherently designed for temporal analysis.", "AI": {"tldr": "研究发现，为单时间点设计的病灶分割模型（如ULS23）在纵向分析中表现不佳，主要因扫描间配准误差导致分割质量下降和病灶对应失败。模型对病灶居中假设的依赖性强，揭示了单时间点模型应用于纵向数据的局限性。", "motivation": "肿瘤学护理中的纵向病灶分析至关重要，但现有自动化工具在时间一致性方面表现不佳。尽管通用病灶分割模型已取得进展，但它们通常是为单时间点设计的，无法很好地处理纵向数据。", "method": "本研究评估了ULS23分割模型在纵向背景下的性能。利用一个包含基线和随访CT扫描的公共临床数据集，评估了模型随时间分割和跟踪病灶的能力。为系统性探究模型脆弱性，进行了对照实验，人工位移输入体相对于真实病灶中心。", "result": "研究发现了两个关键且相互关联的失效模式：由于扫描间配准误差，随访病例的分割质量急剧下降；随后病灶对应过程崩溃。结果表明，模型的性能高度依赖于其对病灶居中的假设；当病灶充分位移时，分割精度会急剧下降。", "conclusion": "这些发现揭示了将单时间点模型应用于纵向数据的根本局限性。研究得出结论，鲁棒的肿瘤跟踪需要范式转变，从级联的单一用途工具转向本质上为时间分析设计的集成式端到端模型。"}}
{"id": "2507.19232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19232", "abs": "https://arxiv.org/abs/2507.19232", "authors": ["Donggeun Lim", "Jinseok Bae", "Inwoo Hwang", "Seungmin Lee", "Hwanhee Lee", "Young Min Kim"], "title": "Event-Driven Storytelling with Multiple Lifelike Humans in a 3D Scene", "comment": "16 pages, project page:\n  https://rms0329.github.io/Event-Driven-Storytelling/", "summary": "In this work, we propose a framework that creates a lively virtual dynamic\nscene with contextual motions of multiple humans. Generating multi-human\ncontextual motion requires holistic reasoning over dynamic relationships among\nhuman-human and human-scene interactions. We adapt the power of a large\nlanguage model (LLM) to digest the contextual complexity within textual input\nand convert the task into tangible subproblems such that we can generate\nmulti-agent behavior beyond the scale that was not considered before.\nSpecifically, our event generator formulates the temporal progression of a\ndynamic scene into a sequence of small events. Each event calls for a\nwell-defined motion involving relevant characters and objects. Next, we\nsynthesize the motions of characters at positions sampled based on spatial\nguidance. We employ a high-level module to deliver scalable yet comprehensive\ncontext, translating events into relative descriptions that enable the\nretrieval of precise coordinates. As the first to address this problem at scale\nand with diversity, we offer a benchmark to assess diverse aspects of\ncontextual reasoning. Benchmark results and user studies show that our\nframework effectively captures scene context with high scalability. The code\nand benchmark, along with result videos, are available at our project page:\nhttps://rms0329.github.io/Event-Driven-Storytelling/.", "AI": {"tldr": "该论文提出了一个框架，利用大型语言模型（LLM）处理文本输入中的上下文复杂性，从而生成具有上下文感知的多人类虚拟动态场景。", "motivation": "生成多人类的上下文运动需要对人-人以及人-场景交互中的动态关系进行整体推理，这在现有方法中难以大规模处理。", "method": "该框架利用LLM消化文本输入的上下文复杂性，并将任务转换为可操作的子问题。具体而言，事件生成器将动态场景的时间进程表述为一系列小事件，每个事件定义了相关角色和对象的运动。接着，根据空间指导合成角色的运动。此外，还采用一个高级模块将事件转换为相对描述，以检索精确坐标，从而提供可扩展且全面的上下文。论文还首次提出了一个基准来评估上下文推理。", "result": "基准测试结果和用户研究表明，该框架能有效捕捉场景上下文，并具有高可扩展性。", "conclusion": "该框架成功解决了大规模、多样化地生成具有复杂上下文的虚拟多人类动态场景的问题，并有效捕捉了场景上下文。"}}
{"id": "2507.19239", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19239", "abs": "https://arxiv.org/abs/2507.19239", "authors": ["Jiaru Zhong", "Jiahao Wang", "Jiahui Xu", "Xiaofan Li", "Zaiqing Nie", "Haibao Yu"], "title": "CoopTrack: Exploring End-to-End Learning for Efficient Cooperative Sequential Perception", "comment": "Accepted by ICCV 2025 (Highlight)", "summary": "Cooperative perception aims to address the inherent limitations of\nsingle-vehicle autonomous driving systems through information exchange among\nmultiple agents. Previous research has primarily focused on single-frame\nperception tasks. However, the more challenging cooperative sequential\nperception tasks, such as cooperative 3D multi-object tracking, have not been\nthoroughly investigated. Therefore, we propose CoopTrack, a fully\ninstance-level end-to-end framework for cooperative tracking, featuring\nlearnable instance association, which fundamentally differs from existing\napproaches. CoopTrack transmits sparse instance-level features that\nsignificantly enhance perception capabilities while maintaining low\ntransmission costs. Furthermore, the framework comprises two key components:\nMulti-Dimensional Feature Extraction, and Cross-Agent Association and\nAggregation, which collectively enable comprehensive instance representation\nwith semantic and motion features, and adaptive cross-agent association and\nfusion based on a feature graph. Experiments on both the V2X-Seq and Griffin\ndatasets demonstrate that CoopTrack achieves excellent performance.\nSpecifically, it attains state-of-the-art results on V2X-Seq, with 39.0\\% mAP\nand 32.8\\% AMOTA. The project is available at\nhttps://github.com/zhongjiaru/CoopTrack.", "AI": {"tldr": "CoopTrack是一个端到端的协同多目标跟踪框架，通过传输稀疏实例级特征实现低通信成本下的高性能，并在V2X-Seq和Griffin数据集上取得了SOTA结果。", "motivation": "单车自动驾驶系统存在固有限制，而现有协同感知研究主要集中在单帧感知任务，对更具挑战性的协同序列感知任务（如协同3D多目标跟踪）缺乏深入研究。", "method": "本文提出了CoopTrack框架，它是一个完全实例级的端到端协同跟踪框架，具有可学习的实例关联机制。该框架传输稀疏的实例级特征以降低通信成本。CoopTrack包含两个关键组件：多维特征提取（用于获取语义和运动特征）和跨智能体关联与聚合（基于特征图进行自适应关联和融合）。", "result": "CoopTrack在V2X-Seq和Griffin数据集上表现出色。特别是在V2X-Seq数据集上，它达到了最先进的性能，mAP为39.0%，AMOTA为32.8%。", "conclusion": "CoopTrack成功解决了协同序列感知任务，通过创新的实例级特征传输和处理机制，在保持低通信成本的同时，显著提升了协同跟踪的性能，达到了当前最佳水平。"}}
{"id": "2507.19253", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19253", "abs": "https://arxiv.org/abs/2507.19253", "authors": ["An Xiang", "Zixuan Huang", "Xitong Gao", "Kejiang Ye", "Cheng-zhong Xu"], "title": "BridgeNet: A Unified Multimodal Framework for Bridging 2D and 3D Industrial Anomaly Detection", "comment": null, "summary": "Industrial anomaly detection for 2D objects has gained significant attention\nand achieved progress in anomaly detection (AD) methods. However, identifying\n3D depth anomalies using only 2D information is insufficient. Despite\nexplicitly fusing depth information into RGB images or using point cloud\nbackbone networks to extract depth features, both approaches struggle to\nadequately represent 3D information in multimodal scenarios due to the\ndisparities among different modal information. Additionally, due to the\nscarcity of abnormal samples in industrial data, especially in multimodal\nscenarios, it is necessary to perform anomaly generation to simulate real-world\nabnormal samples. Therefore, we propose a novel unified multimodal anomaly\ndetection framework to address these issues. Our contributions consist of 3 key\naspects. (1) We extract visible depth information from 3D point cloud data\nsimply and use 2D RGB images to represent appearance, which disentangles depth\nand appearance to support unified anomaly generation. (2) Benefiting from the\nflexible input representation, the proposed Multi-Scale Gaussian Anomaly\nGenerator and Unified Texture Anomaly Generator can generate richer anomalies\nin RGB and depth. (3) All modules share parameters for both RGB and depth data,\neffectively bridging 2D and 3D anomaly detection. Subsequent modules can\ndirectly leverage features from both modalities without complex fusion.\nExperiments show our method outperforms state-of-the-art (SOTA) on MVTec-3D AD\nand Eyecandies datasets. Code available at:\nhttps://github.com/Xantastic/BridgeNet", "AI": {"tldr": "本文提出了一种新颖的统一多模态异常检测框架，通过解耦2D RGB外观和3D深度信息，并生成丰富的多模态异常样本，有效弥合了2D和3D异常检测之间的鸿沟。", "motivation": "现有方法在仅使用2D信息识别3D深度异常时不足；显式融合深度或使用点云骨干网络在多模态场景中难以充分表示3D信息，因为不同模态间存在差异；工业数据中，特别是多模态场景下，异常样本稀缺，需要进行异常生成来模拟真实异常。", "method": "1. 从3D点云数据中简单提取可见深度信息，并用2D RGB图像表示外观，从而解耦深度和外观，支持统一的异常生成。2. 提出多尺度高斯异常生成器和统一纹理异常生成器，以在RGB和深度上生成更丰富的异常。3. 所有模块共享RGB和深度数据的参数，有效连接2D和3D异常检测，后续模块可直接利用两种模态的特征而无需复杂融合。", "result": "该方法在MVTec-3D AD和Eyecandies数据集上均优于最先进（SOTA）方法。", "conclusion": "所提出的统一多模态异常检测框架通过有效解耦模态和创新的异常生成机制，成功克服了多模态3D异常检测的挑战，并在多个数据集上取得了SOTA性能，证明了其在工业异常检测领域的有效性。"}}
{"id": "2507.19262", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19262", "abs": "https://arxiv.org/abs/2507.19262", "authors": ["Monika Wysoczańska", "Shyamal Buch", "Anurag Arnab", "Cordelia Schmid"], "title": "OVFact: Measuring and Improving Open-Vocabulary Factuality for Long Caption Models", "comment": null, "summary": "Large vision-language models (VLMs) often struggle to generate long and\nfactual captions. However, traditional measures for hallucination and\nfactuality are not well suited for evaluating longer, more diverse captions and\nin settings where ground-truth human-annotated captions are unavailable. We\nintroduce OV-Fact, a novel method for measuring caption factuality of long\ncaptions that leverages open-vocabulary visual grounding and tool-based\nverification without depending on human annotations. Our method improves\nagreement with human judgments and captures both caption descriptiveness\n(recall) and factual precision in the same metric. Furthermore, unlike previous\nmetrics, our reference-free method design enables new applications towards\nfactuality-based data filtering. We observe models trained on an\nOVFact-filtered (2.5-5x less) subset of a large-scale, noisy (VLM-generated)\npretraining set meaningfully improve factuality precision without sacrificing\ncaption descriptiveness across a range of downstream long caption benchmarks.", "AI": {"tldr": "OV-Fact是一种无需人工标注的视觉-语言模型（VLM）长文本描述事实性评估新方法，它结合了开放词汇视觉定位和工具验证，并能有效过滤训练数据以提高模型的描述事实性。", "motivation": "大型视觉-语言模型在生成长而真实描述时表现不佳，且现有幻觉和事实性评估方法不适用于评估长、多样化的描述，尤其是在缺乏人工标注的真实数据时。", "method": "提出了OV-Fact，一种新颖的评估长文本描述事实性的方法。它利用开放词汇视觉定位和基于工具的验证，无需依赖人工标注。该方法在同一指标中同时衡量描述性（召回率）和事实精确性。", "result": "OV-Fact与人类判断的一致性更高，并且能同时捕捉描述性和事实精确性。此外，使用OV-Fact过滤过的大规模、嘈杂（VLM生成）预训练数据集（减少2.5-5倍）训练的模型，在不牺牲描述性的前提下，显著提高了下游长文本描述基准测试的事实精确性。", "conclusion": "OV-Fact是一种有效的、无需参考的评估长文本描述事实性的方法，它支持基于事实性的数据过滤，从而能训练出在长文本描述任务上表现更佳、事实性更强的VLM模型。"}}
{"id": "2507.19264", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19264", "abs": "https://arxiv.org/abs/2507.19264", "authors": ["Sijie Li", "Chen Chen", "Jungong Han"], "title": "SimMLM: A Simple Framework for Multi-modal Learning with Missing Modality", "comment": null, "summary": "In this paper, we propose SimMLM, a simple yet powerful framework for\nmultimodal learning with missing modalities. Unlike existing approaches that\nrely on sophisticated network architectures or complex data imputation\ntechniques, SimMLM provides a generic and effective solution that can adapt to\nvarious missing modality scenarios with improved accuracy and robustness.\nSpecifically, SimMLM consists of a generic Dynamic Mixture of Modality Experts\n(DMoME) architecture, featuring a dynamic, learnable gating mechanism that\nautomatically adjusts each modality's contribution in both full and partial\nmodality settings. A key innovation of SimMLM is the proposed More vs. Fewer\n(MoFe) ranking loss, which ensures that task accuracy improves or remains\nstable as more modalities are made available. This aligns the model with an\nintuitive principle: removing one or more modalities should not increase\naccuracy. We validate SimMLM on multimodal medical image segmentation (BraTS\n2018) and multimodal classification (UPMC Food-101, avMNIST) tasks, where it\nconsistently surpasses competitive methods, demonstrating superior accuracy,\ninterpretability, robustness, and reliability across both complete and missing\nmodality scenarios at test time.", "AI": {"tldr": "SimMLM是一个简单而强大的多模态学习框架，能有效处理缺失模态，并在各种场景下提升准确性和鲁棒性。", "motivation": "现有处理缺失模态的方法依赖于复杂的网络架构或数据插补技术，研究旨在提供一个通用、有效、适应性强的解决方案，以提高准确性和鲁棒性。", "method": "提出SimMLM框架，包含通用的动态模态专家混合（DMoME）架构，该架构具有动态、可学习的门控机制，能自动调整各模态的贡献。核心创新是“更多 vs. 更少”（MoFe）排序损失，确保在可用模态增多时任务准确性提高或保持稳定。", "result": "在多模态医学图像分割（BraTS 2018）和多模态分类（UPMC Food-101, avMNIST）任务上，SimMLM持续超越现有竞争方法，在完整和缺失模态场景下均展现出卓越的准确性、可解释性、鲁棒性和可靠性。", "conclusion": "SimMLM为处理缺失模态的多模态学习提供了一个通用且高效的解决方案，其性能优于现有复杂方法，并在各种测试场景下表现出优越的准确性和鲁棒性。"}}
{"id": "2507.19272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19272", "abs": "https://arxiv.org/abs/2507.19272", "authors": ["Marcel Simon", "Tae-Ho Kim", "Seul-Ki Yeom"], "title": "Video Self-Distillation for Single-Image Encoders: A Step Toward Physically Plausible Perception", "comment": "4 pages, 2 figures, 2 tables", "summary": "Self-supervised image encoders such as DINO have recently gained significant\ninterest for learning robust visual features without labels. However, most SSL\nmethods train on static images and miss the temporal cues inherent in videos.\nWe introduce a video-distilled single-image encoder trained to predict the\nnext-frame representation from the current frame. This simple objective injects\n3D spatial and temporal priors without optical flow or tracking. When\npre-training on a single 2-hour video, our approach raises the mean\nIntersection-over-Union (mIoU) on ADE20K from 35.0 (DoRA) to 36.4 while\nremaining a drop-in replacement for image-only pipelines. Our results highlight\nvideo self-distillation as a lightweight route to geometry-aware perception an\nessential ingredient for physically plausible world models and Physical AI.", "AI": {"tldr": "本文提出一种视频蒸馏的单图像编码器，通过预测下一帧表征来注入3D时空先验，从而在无需光流或跟踪的情况下，提升图像编码器的鲁棒性和几何感知能力。", "motivation": "现有的自监督学习（SSL）方法主要在静态图像上训练，忽略了视频中固有的时间线索，而这些线索对于学习鲁棒的视觉特征和构建物理世界模型至关重要。", "method": "引入一个视频蒸馏的单图像编码器，其训练目标是根据当前帧预测下一帧的表征。这种简单目标在不使用光流或跟踪的情况下，注入了3D空间和时间先验。", "result": "在仅用2小时视频进行预训练后，该方法将ADE20K数据集上的平均交并比（mIoU）从DoRA的35.0提升到36.4，同时仍可作为现有纯图像处理流程的直接替代品。", "conclusion": "视频自蒸馏是一种轻量级的方法，可以实现几何感知的感知能力，这对于构建物理可信的世界模型和物理AI至关重要。"}}
{"id": "2507.19280", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19280", "abs": "https://arxiv.org/abs/2507.19280", "authors": ["Liang Yao", "Fan Liu", "Hongbo Lu", "Chuanyi Zhang", "Rui Min", "Shengxiang Xu", "Shimin Di", "Pai Peng"], "title": "RemoteReasoner: Towards Unifying Geospatial Reasoning Workflow", "comment": null, "summary": "Remote sensing imagery presents vast, inherently unstructured spatial data,\ndemanding sophisticated reasoning to interpret complex user intents and\ncontextual relationships beyond simple recognition tasks. In this paper, we aim\nto construct an Earth observation workflow to handle complex queries by\nreasoning about spatial context and user intent. As a reasoning workflow, it\nshould be somewhat autonomous, where predefined ground-truth reasoning paths do\nnot constrain the learning process. Furthermore, its architecture ought to be\nunified yet flexible, enabling the model to perform diverse reasoning tasks\nwith distinct output formats through a single forward pass. Existing remote\nsensing approaches fail to address these requirements, as they rely on\nsupervised fine-tuning paradigms that constrain the autonomy of reasoning. To\nthis end, we propose RemoteReasoner, a flexible and robust workflow for remote\nsensing reasoning tasks. The design of RemoteReasoner integrates a multi-modal\nlarge language model (MLLM) for interpreting user instructions and localizing\ntargets, together with task adaptation strategies that enable multi-granularity\noutput generation. In contrast to existing methods, our framework is trained\nwith reinforcement learning (RL) to endow the MLLM sufficient autonomy for\nprecise reasoning. At the inference stage, our adaptation strategies enable\ndiverse output formats at inference time without requiring task-specific\ndecoders or further fine-tuning. Preliminary experiments demonstrated that\nRemoteReasoner achieves remarkable performance across multi-granularity\nreasoning tasks, including region-level and pixel-level. Additionally, our\nframework enables novel capabilities such as the contour extraction task beyond\nthe reach of existing reasoning pipelines.", "AI": {"tldr": "本文提出了RemoteReasoner，一个灵活且鲁棒的遥感推理工作流，通过整合多模态大语言模型（MLLM）和强化学习（RL），实现对复杂用户意图和空间上下文的自主推理，并支持多粒度输出。", "motivation": "现有遥感方法无法处理复杂的用户查询，缺乏自主推理能力，且其监督微调范式限制了推理的自主性，难以实现统一且灵活的架构以生成多样化的输出格式。", "method": "提出RemoteReasoner框架，该框架结合了多模态大语言模型（MLLM）用于解释用户指令和目标定位，并设计了任务适应策略以支持多粒度输出生成。与现有方法不同，RemoteReasoner通过强化学习（RL）进行训练，赋予MLLM足够的自主推理能力。推理阶段，其适应策略无需特定解码器或额外微调即可生成多样化输出。", "result": "初步实验表明，RemoteReasoner在多粒度推理任务（包括区域级和像素级）上表现出色。此外，该框架还实现了现有推理管线无法企及的新能力，例如轮廓提取任务。", "conclusion": "RemoteReasoner提供了一个灵活、鲁棒且自主的遥感推理工作流，能够有效处理复杂的遥感查询，并支持多样化的输出格式，显著超越了现有方法的局限性。"}}
{"id": "2507.19292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19292", "abs": "https://arxiv.org/abs/2507.19292", "authors": ["Sakuya Ota", "Qing Yu", "Kent Fujiwara", "Satoshi Ikehata", "Ikuro Sato"], "title": "PINO: Person-Interaction Noise Optimization for Long-Duration and Customizable Motion Generation of Arbitrary-Sized Groups", "comment": "Accepted to ICCV 2025, Project page: https://sinc865.github.io/pino/", "summary": "Generating realistic group interactions involving multiple characters remains\nchallenging due to increasing complexity as group size expands. While existing\nconditional diffusion models incrementally generate motions by conditioning on\npreviously generated characters, they rely on single shared prompts, limiting\nnuanced control and leading to overly simplified interactions. In this paper,\nwe introduce Person-Interaction Noise Optimization (PINO), a novel,\ntraining-free framework designed for generating realistic and customizable\ninteractions among groups of arbitrary size. PINO decomposes complex group\ninteractions into semantically relevant pairwise interactions, and leverages\npretrained two-person interaction diffusion models to incrementally compose\ngroup interactions. To ensure physical plausibility and avoid common artifacts\nsuch as overlapping or penetration between characters, PINO employs\nphysics-based penalties during noise optimization. This approach allows precise\nuser control over character orientation, speed, and spatial relationships\nwithout additional training. Comprehensive evaluations demonstrate that PINO\ngenerates visually realistic, physically coherent, and adaptable multi-person\ninteractions suitable for diverse animation, gaming, and robotics applications.", "AI": {"tldr": "PINO是一个无需训练的框架，通过分解为双人交互并结合物理约束，生成逼真且可定制的任意大小群体互动。", "motivation": "现有条件扩散模型在生成多人互动时，随着人数增加复杂性急剧上升，且依赖单一共享提示，导致控制有限和互动过于简化，无法生成细致的群体互动。", "method": "引入了Person-Interaction Noise Optimization (PINO)框架。它将复杂的群体互动分解为语义相关的成对互动，利用预训练的双人互动扩散模型逐步合成群体互动。为确保物理真实性，PINO在噪声优化过程中采用基于物理的惩罚机制，以避免角色重叠或穿透，并支持用户对角色朝向、速度和空间关系进行精确控制。", "result": "PINO能够生成视觉逼真、物理连贯且适应性强的多人互动。", "conclusion": "PINO生成的互动适用于动画、游戏和机器人等多种应用，具有高度的真实性、连贯性和适应性。"}}
{"id": "2507.19296", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19296", "abs": "https://arxiv.org/abs/2507.19296", "authors": ["Ahmed Endris Hasen", "Yang Shangming", "Chiagoziem C. Ukwuoma", "Biniyam Gashaw", "Abel Zenebe Yutra"], "title": "ABCD: Automatic Blood Cell Detection via Attention-Guided Improved YOLOX", "comment": null, "summary": "Detection of blood cells in microscopic images has become a major focus of\nmedical image analysis, playing a crucial role in gaining valuable insights\ninto a patient's health. Manual blood cell checks for disease detection are\nknown to be time-consuming, inefficient, and error-prone. To address these\nlimitations, analyzing blood cells using deep learning-based object detectors\ncan be regarded as a feasible solution. In this study, we propose automatic\nblood cell detection method (ABCD) based on an improved version of YOLOX, an\nobject detector, for detecting various types of blood cells, including white\nblood cells, red blood cells, and platelets. Firstly, we introduce the\nConvolutional Block Attention Module (CBAM) into the network's backbone to\nenhance the efficiency of feature extraction. Furthermore, we introduce the\nAdaptively Spatial Feature Fusion (ASFF) into the network's neck, which\noptimizes the fusion of different features extracted from various stages of the\nnetwork. Finally, to speed up the model's convergence, we substitute the\nIntersection over Union (IOU) loss function with the Complete Intersection over\nUnion (CIOU) loss function. The experimental results demonstrate that the\nproposed method is more effective than other existing methods for BCCD dataset.\nCompared to the baseline algorithm, our method ABCD achieved 95.49 % mAP@0.5\nand 86.89 % mAP@0.5-0.9, which are 2.8% and 23.41% higher, respectively, and\nincreased the detection speed by 2.9%, making it highly efficient for real-time\napplications.", "AI": {"tldr": "该研究提出了一种基于改进YOLOX的自动血细胞检测方法（ABCD），通过引入CBAM、ASFF和CIOU损失函数，显著提升了血细胞检测的准确性和效率，适用于实时应用。", "motivation": "手动血细胞检测耗时、低效且易出错，难以满足疾病诊断需求。因此，需要一种基于深度学习的自动化、高效且准确的血细胞检测方法来克服这些局限性。", "method": "本研究提出了一种名为ABCD的自动血细胞检测方法，该方法基于YOLOX目标检测器并进行了以下改进：1. 在网络骨干中引入卷积块注意力模块（CBAM）以增强特征提取效率。2. 在网络颈部引入自适应空间特征融合（ASFF）以优化不同阶段特征的融合。3. 将损失函数从IOU替换为CIOU以加速模型收敛。", "result": "实验结果表明，该方法在BCCD数据集上优于其他现有方法。与基线算法相比，ABCD方法在mAP@0.5上达到95.49%（提高2.8%），在mAP@0.5-0.9上达到86.89%（提高23.41%），同时检测速度提高了2.9%。", "conclusion": "所提出的ABCD方法在血细胞检测方面表现出更高的有效性和效率，使其非常适合实时应用。"}}
{"id": "2507.19328", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19328", "abs": "https://arxiv.org/abs/2507.19328", "authors": ["Kirsten W. H. Maas", "Danny Ruijters", "Nicola Pezzotti", "Anna Vilanova"], "title": "NerT-CA: Efficient Dynamic Reconstruction from Sparse-view X-ray Coronary Angiography", "comment": null, "summary": "Three-dimensional (3D) and dynamic 3D+time (4D) reconstruction of coronary\narteries from X-ray coronary angiography (CA) has the potential to improve\nclinical procedures. However, there are multiple challenges to be addressed,\nmost notably, blood-vessel structure sparsity, poor background and blood vessel\ndistinction, sparse-views, and intra-scan motion. State-of-the-art\nreconstruction approaches rely on time-consuming manual or error-prone\nautomatic segmentations, limiting clinical usability. Recently, approaches\nbased on Neural Radiance Fields (NeRF) have shown promise for automatic\nreconstructions in the sparse-view setting. However, they suffer from long\ntraining times due to their dependence on MLP-based representations. We propose\nNerT-CA, a hybrid approach of Neural and Tensorial representations for\naccelerated 4D reconstructions with sparse-view CA. Building on top of the\nprevious NeRF-based work, we model the CA scene as a decomposition of low-rank\nand sparse components, utilizing fast tensorial fields for low-rank static\nreconstruction and neural fields for dynamic sparse reconstruction. Our\napproach outperforms previous works in both training time and reconstruction\naccuracy, yielding reasonable reconstructions from as few as three angiogram\nviews. We validate our approach quantitatively and qualitatively on\nrepresentative 4D phantom datasets.", "AI": {"tldr": "本文提出NerT-CA，一种结合神经和张量表示的混合方法，用于加速稀疏视图X射线冠状动脉造影的4D重建，解决了现有NeRF方法训练时间长和重建精度的问题。", "motivation": "X射线冠状动脉造影的3D/4D重建对临床有益，但面临血管结构稀疏、对比度差、视图稀疏和扫描内运动等挑战。现有方法依赖耗时或易错的手动/自动分割，且基于NeRF的方法虽有前景但训练时间长。", "method": "提出NerT-CA，一种混合神经和张量表示的方法。将冠状动脉场景建模为低秩（静态）和稀疏（动态）分量的分解。利用快速张量场进行低秩静态重建，并使用神经场进行动态稀疏重建。该方法基于之前的NeRF工作。", "result": "NerT-CA在训练时间和重建精度上均优于现有工作。即使仅从三个造影视图，也能获得合理的重建结果。该方法在代表性4D体模数据集上进行了定量和定性验证。", "conclusion": "NerT-CA是一种高效且准确的4D冠状动脉重建方法，能从稀疏视图造影图像中加速重建，克服了现有方法的局限性。"}}
{"id": "2507.19359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19359", "abs": "https://arxiv.org/abs/2507.19359", "authors": ["Lanmiao Liu", "Esam Ghaleb", "Aslı Özyürek", "Zerrin Yumak"], "title": "SemGes: Semantics-aware Co-Speech Gesture Generation using Semantic Coherence and Relevance Learning", "comment": "Accepted to IEEE/CVF International Conference on Computer Vision\n  (ICCV) 2025", "summary": "Creating a virtual avatar with semantically coherent gestures that are\naligned with speech is a challenging task. Existing gesture generation research\nmainly focused on generating rhythmic beat gestures, neglecting the semantic\ncontext of the gestures. In this paper, we propose a novel approach for\nsemantic grounding in co-speech gesture generation that integrates semantic\ninformation at both fine-grained and global levels. Our approach starts with\nlearning the motion prior through a vector-quantized variational autoencoder.\nBuilt on this model, a second-stage module is applied to automatically generate\ngestures from speech, text-based semantics and speaker identity that ensures\nconsistency between the semantic relevance of generated gestures and\nco-occurring speech semantics through semantic coherence and relevance modules.\nExperimental results demonstrate that our approach enhances the realism and\ncoherence of semantic gestures. Extensive experiments and user studies show\nthat our method outperforms state-of-the-art approaches across two benchmarks\nin co-speech gesture generation in both objective and subjective metrics. The\nqualitative results of our model, code, dataset and pre-trained models can be\nviewed at https://semgesture.github.io/.", "AI": {"tldr": "本文提出了一种新颖的方法，通过在细粒度和全局层面整合语义信息，生成与语音语义连贯且对齐的虚拟形象手势。", "motivation": "现有手势生成研究主要关注韵律节拍手势，忽略了手势的语义上下文，且创建与语音语义连贯的手势具有挑战性。", "method": "该方法分两阶段：首先，通过向量量化变分自编码器（VQ-VAE）学习动作先验；其次，在此模型基础上，应用第二阶段模块，结合语音、文本语义和说话人身份自动生成手势，并通过语义连贯性和相关性模块确保生成手势与共现语音语义的语义一致性。", "result": "实验结果表明，该方法增强了语义手势的真实性和连贯性。广泛的实验和用户研究显示，该方法在共同语音手势生成的两个基准测试中，无论是客观还是主观指标，均优于现有最先进的方法。", "conclusion": "该方法通过在细粒度和全局层面整合语义信息，成功生成了语义连贯且逼真的共同语音手势，并超越了现有技术水平。"}}
{"id": "2507.19360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19360", "abs": "https://arxiv.org/abs/2507.19360", "authors": ["Chen Zhu", "Wangbo Zhao", "Huiwen Zhang", "Samir Khaki", "Yuhao Zhou", "Weidong Tang", "Shuo Wang", "Zhihang Yuan", "Yuzhang Shang", "Xiaojiang Peng", "Kai Wang", "Dawei Yang"], "title": "EA-ViT: Efficient Adaptation for Elastic Vision Transformer", "comment": "Published as a conference paper at ICCV 2025", "summary": "Vision Transformers (ViTs) have emerged as a foundational model in computer\nvision, excelling in generalization and adaptation to downstream tasks.\nHowever, deploying ViTs to support diverse resource constraints typically\nrequires retraining multiple, size-specific ViTs, which is both time-consuming\nand energy-intensive. To address this issue, we propose an efficient ViT\nadaptation framework that enables a single adaptation process to generate\nmultiple models of varying sizes for deployment on platforms with various\nresource constraints. Our approach comprises two stages. In the first stage, we\nenhance a pre-trained ViT with a nested elastic architecture that enables\nstructural flexibility across MLP expansion ratio, number of attention heads,\nembedding dimension, and network depth. To preserve pre-trained knowledge and\nensure stable adaptation, we adopt a curriculum-based training strategy that\nprogressively increases elasticity. In the second stage, we design a\nlightweight router to select submodels according to computational budgets and\ndownstream task demands. Initialized with Pareto-optimal configurations derived\nvia a customized NSGA-II algorithm, the router is then jointly optimized with\nthe backbone. Extensive experiments on multiple benchmarks demonstrate the\neffectiveness and versatility of EA-ViT. The code is available at\nhttps://github.com/zcxcf/EA-ViT.", "AI": {"tldr": "提出EA-ViT框架，通过一次自适应过程为不同资源限制生成多种尺寸的ViT模型，解决了传统方法需要重复训练的效率问题。", "motivation": "Vision Transformers (ViTs) 在计算机视觉领域表现出色，但为适应不同资源约束，通常需要对不同尺寸的ViT进行多次耗时且耗能的重新训练。", "method": "该方法分两阶段：1. 增强预训练ViT，引入嵌套弹性架构（MLP扩展比、注意力头数、嵌入维度、网络深度可变），并采用课程学习策略逐步增加弹性以保持预训练知识和稳定自适应。2. 设计轻量级路由器，根据计算预算和下游任务需求选择子模型，路由器通过定制的NSGA-II算法初始化Pareto最优配置，并与骨干网络联合优化。", "result": "在多个基准测试中，EA-ViT展示了其有效性和多功能性。", "conclusion": "EA-ViT提供了一个高效且通用的ViT自适应框架，能够通过单次训练生成适应不同资源约束的多种尺寸模型。"}}
{"id": "2507.19370", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19370", "abs": "https://arxiv.org/abs/2507.19370", "authors": ["Felix Brandstaetter", "Erik Schuetz", "Katharina Winter", "Fabian Flohr"], "title": "BEV-LLM: Leveraging Multimodal BEV Maps for Scene Captioning in Autonomous Driving", "comment": null, "summary": "Autonomous driving technology has the potential to transform transportation,\nbut its wide adoption depends on the development of interpretable and\ntransparent decision-making systems. Scene captioning, which generates natural\nlanguage descriptions of the driving environment, plays a crucial role in\nenhancing transparency, safety, and human-AI interaction. We introduce BEV-LLM,\na lightweight model for 3D captioning of autonomous driving scenes. BEV-LLM\nleverages BEVFusion to combine 3D LiDAR point clouds and multi-view images,\nincorporating a novel absolute positional encoding for view-specific scene\ndescriptions. Despite using a small 1B parameter base model, BEV-LLM achieves\ncompetitive performance on the nuCaption dataset, surpassing state-of-the-art\nby up to 5\\% in BLEU scores. Additionally, we release two new datasets - nuView\n(focused on environmental conditions and viewpoints) and GroundView (focused on\nobject grounding) - to better assess scene captioning across diverse driving\nscenarios and address gaps in current benchmarks, along with initial\nbenchmarking results demonstrating their effectiveness.", "AI": {"tldr": "本文提出BEV-LLM，一个轻量级自动驾驶3D场景描述模型，结合激光雷达和多视角图像，在nuCaption数据集上表现优于现有技术，并发布了nuView和GroundView两个新数据集以评估多样化场景。", "motivation": "自动驾驶技术的广泛应用取决于可解释和透明的决策系统。场景描述（Scene captioning）在增强透明度、安全性以及人机交互方面发挥着关键作用。", "method": "引入BEV-LLM模型，一个轻量级的自动驾驶场景3D描述模型。它利用BEVFusion融合3D LiDAR点云和多视角图像，并引入新颖的绝对位置编码来生成视角特定的场景描述。此外，还发布了nuView（关注环境条件和视角）和GroundView（关注物体定位）两个新数据集。", "result": "BEV-LLM尽管使用1B参数的小型基础模型，但在nuCaption数据集上取得了有竞争力的表现，BLEU分数超越现有技术高达5%。新数据集nuView和GroundView的初步基准测试结果也证明了它们的有效性。", "conclusion": "BEV-LLM模型为自动驾驶场景的3D描述提供了一个高效且性能优异的解决方案，有助于提升系统的可解释性。新发布的nuView和GroundView数据集填补了现有基准的空白，能够更好地评估多样化驾驶场景下的场景描述能力。"}}
{"id": "2507.19409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19409", "abs": "https://arxiv.org/abs/2507.19409", "authors": ["Toufiq Parag", "Ahmed Elgammal"], "title": "Modality Agnostic Efficient Long Range Encoder", "comment": null, "summary": "The long-context capability of recent large transformer models can be\nsurmised to rely on techniques such as attention/model parallelism, as well as\nhardware-level optimizations. While these strategies allow input lengths to\nscale to millions of tokens, they do not fundamentally mitigate the quadratic\ncomputational and memory complexity of the core attention mechanism. In this\npaper, we address the challenge of long-context processing on a single device\nusing generic implementations by reducing the quadratic memory footprint and\ninference cost. Existing approaches to extend the context length for generic\nsingle device implementations -- such as token merging and modified attentions\n-- are often modality specific and attain a suboptimal tradeoff between\naccuracy and efficiency. To overcome these limitations, we propose MAELRE\n(Modality Agnostic Efficient Long Range Encoder), a unified and efficient\ntransformer architecture designed for long-range encoding across diverse\nmodalities. MAELRE integrates token merging with attention approximation,\nprogressively merging tokens at different stages of internal computational\nblocks. It employs a lightweight attention approximation when the number of\ntokens is large, and switches to standard dot-product attention as the sequence\nbecomes shorter through successive aggregation. We demonstrate that MAELRE\nachieves superior accuracy while reducing computational cost compared to\nexisting long-context models on classification tasks spanning multiple\nmodalities, including text, time series, audio, and vision.", "AI": {"tldr": "本文提出MAELRE，一种模态无关的高效长程编码器，通过结合渐进式Token合并和注意力近似，在单个设备上有效处理长上下文，同时保持高精度并降低计算成本。", "motivation": "现有大型Transformer模型虽然能处理百万级Token，但其长上下文能力依赖并行和硬件优化，并未从根本上解决核心注意力机制的二次计算和内存复杂度。对于单设备通用实现，现有方法（如Token合并和修改注意力）通常是模态特定的，且在准确性和效率之间权衡不佳。", "method": "提出MAELRE（Modality Agnostic Efficient Long Range Encoder），一种统一高效的Transformer架构。它将Token合并与注意力近似相结合，在内部计算块的不同阶段渐进式合并Token。当Token数量较多时，采用轻量级注意力近似；随着序列通过聚合变短，则切换回标准点积注意力。", "result": "MAELRE在文本、时间序列、音频和视觉等多种模态的分类任务上，与现有长上下文模型相比，实现了卓越的准确性，同时显著降低了计算成本。", "conclusion": "MAELRE通过创新地结合Token合并和自适应注意力机制，为单设备上的长上下文处理提供了一个高效、准确且模态无关的通用解决方案，有效缓解了注意力机制的二次复杂性问题。"}}
{"id": "2507.19418", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.19418", "abs": "https://arxiv.org/abs/2507.19418", "authors": ["Yiwei Lou", "Yuanpeng He", "Rongchao Zhang", "Yongzhi Cao", "Hanpin Wang", "Yu Huang"], "title": "DEFNet: Multitasks-based Deep Evidential Fusion Network for Blind Image Quality Assessment", "comment": null, "summary": "Blind image quality assessment (BIQA) methods often incorporate auxiliary\ntasks to improve performance. However, existing approaches face limitations due\nto insufficient integration and a lack of flexible uncertainty estimation,\nleading to suboptimal performance. To address these challenges, we propose a\nmultitasks-based Deep Evidential Fusion Network (DEFNet) for BIQA, which\nperforms multitask optimization with the assistance of scene and distortion\ntype classification tasks. To achieve a more robust and reliable\nrepresentation, we design a novel trustworthy information fusion strategy. It\nfirst combines diverse features and patterns across sub-regions to enhance\ninformation richness, and then performs local-global information fusion by\nbalancing fine-grained details with coarse-grained context. Moreover, DEFNet\nexploits advanced uncertainty estimation technique inspired by evidential\nlearning with the help of normal-inverse gamma distribution mixture. Extensive\nexperiments on both synthetic and authentic distortion datasets demonstrate the\neffectiveness and robustness of the proposed framework. Additional evaluation\nand analysis are carried out to highlight its strong generalization capability\nand adaptability to previously unseen scenarios.", "AI": {"tldr": "本文提出了一种名为DEFNet的深度证据融合网络，用于盲图像质量评估（BIQA）。该网络通过多任务优化、可信信息融合策略和证据学习驱动的不确定性估计，解决了现有BIQA方法中集成不足和不确定性估计不灵活的问题，实现了更鲁棒和可靠的图像质量评估。", "motivation": "现有的盲图像质量评估（BIQA）方法虽然引入了辅助任务来提高性能，但面临集成度不足和缺乏灵活的不确定性估计的局限性，导致性能不佳。", "method": "提出了一个基于多任务的深度证据融合网络（DEFNet），通过场景和失真类型分类辅助任务进行多任务优化。设计了一种新颖的可信信息融合策略，该策略首先结合子区域的多样特征以增强信息丰富度，然后通过平衡细粒度细节和粗粒度上下文进行局部-全局信息融合。此外，DEFNet利用基于正态逆伽马分布混合的证据学习技术进行先进的不确定性估计。", "result": "在合成和真实失真数据集上的大量实验证明了所提出框架的有效性和鲁棒性。额外的评估和分析突出了其强大的泛化能力和对未知场景的适应性。", "conclusion": "DEFNet通过整合多任务优化、可靠的信息融合和灵活的不确定性估计，有效解决了现有BIQA方法的挑战，显著提升了图像质量评估的性能、鲁棒性和泛化能力。"}}
{"id": "2507.19420", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19420", "abs": "https://arxiv.org/abs/2507.19420", "authors": ["Yiming Zhang", "Chengzhang Yu", "Zhuokai Zhao", "Kun Wang", "Qiankun Li", "Zihan Chen", "Yang Liu", "Zenghui Ding", "Yining Sun"], "title": "CircuitProbe: Dissecting Spatiotemporal Visual Semantics with Circuit Tracing", "comment": null, "summary": "The processing mechanisms underlying language and image understanding in\nlarge vision-language models (LVLMs) have been extensively studied. However,\nthe internal reasoning mechanisms of LVLMs for spatiotemporal understanding\nremain poorly understood. In this work, we introduce a systematic,\ncircuit-based framework designed to investigate how spatiotemporal visual\nsemantics are represented and processed within these LVLMs. Specifically, our\nframework comprises three circuits: visual auditing circuit, semantic tracing\ncircuit, and attention flow circuit. Through the lens of these circuits, we\ndiscover that visual semantics are highly localized to specific object\ntokens--removing these tokens can degrade model performance by up to 92.6%.\nFurthermore, we identify that interpretable concepts of objects and actions\nemerge and become progressively refined in the middle-to-late layers of LVLMs.\nIn contrary to the current works that solely focus on objects in one image, we\nreveal that the middle-to-late layers of LVLMs exhibit specialized functional\nlocalization for spatiotemporal semantics. Our findings offer significant\nmechanistic insights into spatiotemporal semantics analysis of LVLMs, laying a\nfoundation for designing more robust and interpretable models.", "AI": {"tldr": "本研究通过引入电路框架，系统探究大型视觉语言模型（LVLMs）内部的时空理解机制，发现视觉语义高度局限于特定对象token，且可解释的对象和动作概念在中后期层中演化和精细化。", "motivation": "尽管大型视觉语言模型（LVLMs）的语言和图像理解机制已被广泛研究，但其内部时空理解的推理机制仍不甚明了。", "method": "引入一个系统性的、基于电路的框架，包含视觉审计电路、语义追踪电路和注意力流电路，用于调查LVLMs中时空视觉语义的表示和处理方式。", "result": "1. 视觉语义高度局限于特定对象token，移除这些token可使模型性能下降高达92.6%。\n2. 可解释的对象和动作概念在LVLMs的中后期层中出现并逐渐精细化。\n3. LVLMs的中后期层展现出时空语义的专门化功能局部性，这与当前仅关注单幅图像中对象的研究不同。", "conclusion": "本研究为LVLMs的时空语义分析提供了重要的机制洞察，为设计更鲁棒和可解释的模型奠定了基础。"}}
{"id": "2507.19451", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19451", "abs": "https://arxiv.org/abs/2507.19451", "authors": ["Baijun Ye", "Minghui Qin", "Saining Zhang", "Moonjun Gong", "Shaoting Zhu", "Zebang Shen", "Luan Zhang", "Lu Zhang", "Hao Zhao", "Hang Zhao"], "title": "GS-Occ3D: Scaling Vision-only Occupancy Reconstruction for Autonomous Driving with Gaussian Splatting", "comment": "ICCV 2025. Project Page: https://gs-occ3d.github.io/", "summary": "Occupancy is crucial for autonomous driving, providing essential geometric\npriors for perception and planning. However, existing methods predominantly\nrely on LiDAR-based occupancy annotations, which limits scalability and\nprevents leveraging vast amounts of potential crowdsourced data for\nauto-labeling. To address this, we propose GS-Occ3D, a scalable vision-only\nframework that directly reconstructs occupancy. Vision-only occupancy\nreconstruction poses significant challenges due to sparse viewpoints, dynamic\nscene elements, severe occlusions, and long-horizon motion. Existing\nvision-based methods primarily rely on mesh representation, which suffer from\nincomplete geometry and additional post-processing, limiting scalability. To\novercome these issues, GS-Occ3D optimizes an explicit occupancy representation\nusing an Octree-based Gaussian Surfel formulation, ensuring efficiency and\nscalability. Additionally, we decompose scenes into static background, ground,\nand dynamic objects, enabling tailored modeling strategies: (1) Ground is\nexplicitly reconstructed as a dominant structural element, significantly\nimproving large-area consistency; (2) Dynamic vehicles are separately modeled\nto better capture motion-related occupancy patterns. Extensive experiments on\nthe Waymo dataset demonstrate that GS-Occ3D achieves state-of-the-art geometry\nreconstruction results. By curating vision-only binary occupancy labels from\ndiverse urban scenes, we show their effectiveness for downstream occupancy\nmodels on Occ3D-Waymo and superior zero-shot generalization on Occ3D-nuScenes.\nIt highlights the potential of large-scale vision-based occupancy\nreconstruction as a new paradigm for autonomous driving perception. Project\nPage: https://gs-occ3d.github.io/", "AI": {"tldr": "GS-Occ3D是一个可扩展的纯视觉占据重建框架，它利用基于八叉树的高斯曲面表示和场景分解（背景、地面、动态物体）来解决现有方法对激光雷达的依赖和纯视觉重建的挑战，实现了最先进的几何重建和零样本泛化能力。", "motivation": "现有的自动驾驶占据重建方法主要依赖激光雷达标注，限制了可扩展性并阻碍了利用大规模众包数据进行自动标注。纯视觉重建面临稀疏视角、动态元素、严重遮挡和长距离运动等挑战。现有基于视觉的方法（如网格表示）存在几何不完整和额外后处理问题，限制了其可扩展性。", "method": "GS-Occ3D是一个纯视觉框架，直接重建占据信息。它通过优化一个显式占据表示，采用基于八叉树的高斯曲面（Gaussian Surfel）公式，以确保效率和可扩展性。此外，该方法将场景分解为静态背景、地面和动态物体，并采用定制的建模策略：1) 地面作为主导结构元素被显式重建，以提高大面积一致性；2) 动态车辆被单独建模，以更好地捕捉运动相关的占据模式。", "result": "在Waymo数据集上，GS-Occ3D实现了最先进的几何重建结果。通过从多样城市场景中整理纯视觉二值占据标签，该研究展示了其对Occ3D-Waymo下游占据模型的有效性，并在Occ3D-nuScenes上展现了卓越的零样本泛化能力。", "conclusion": "大规模纯视觉占据重建作为自动驾驶感知的新范式，具有巨大潜力。"}}
{"id": "2507.19468", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19468", "abs": "https://arxiv.org/abs/2507.19468", "authors": ["Federico Baldassarre", "Marc Szafraniec", "Basile Terver", "Vasil Khalidov", "Francisco Massa", "Yann LeCun", "Patrick Labatut", "Maximilian Seitzer", "Piotr Bojanowski"], "title": "Back to the Features: DINO as a Foundation for Video World Models", "comment": null, "summary": "We present DINO-world, a powerful generalist video world model trained to\npredict future frames in the latent space of DINOv2. By leveraging a\npre-trained image encoder and training a future predictor on a large-scale\nuncurated video dataset, DINO-world learns the temporal dynamics of diverse\nscenes, from driving and indoor scenes to simulated environments. We show that\nDINO-world outperforms previous models on a variety of video prediction\nbenchmarks, e.g. segmentation and depth forecasting, and demonstrates strong\nunderstanding of intuitive physics. Furthermore, we show that it is possible to\nfine-tune the predictor on observation-action trajectories. The resulting\naction-conditioned world model can be used for planning by simulating candidate\ntrajectories in latent space.", "AI": {"tldr": "DINO-world是一个强大的通用视频世界模型，利用DINOv2的潜在空间预测未来帧，在多种视频预测任务和物理理解上表现出色，并可用于基于动作的规划。", "motivation": "旨在学习各种场景（如驾驶、室内、模拟环境）的时间动态，并有效预测未来帧，通过利用预训练的图像编码器实现。", "method": "该模型在DINOv2的潜在空间中预测未来帧，并在大规模未分类视频数据集上训练未来预测器。它利用预训练的DINOv2图像编码器。此外，预测器可以通过观测-动作轨迹进行微调，从而实现基于动作的规划。", "result": "DINO-world在分割和深度预测等多种视频预测基准上超越了现有模型，并展示了对直观物理学的强大理解。微调后的模型能够通过在潜在空间中模拟候选轨迹进行规划。", "conclusion": "DINO-world是一个强大的通用视频世界模型，能够有效预测未来帧，理解物理，并且可以扩展到基于动作的规划应用。"}}
{"id": "2507.19474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19474", "abs": "https://arxiv.org/abs/2507.19474", "authors": ["Ziren Gong", "Xiaohan Li", "Fabio Tosi", "Youmin Zhang", "Stefano Mattoccia", "Jun Wu", "Matteo Poggi"], "title": "DINO-SLAM: DINO-informed RGB-D SLAM for Neural Implicit and Explicit Representations", "comment": null, "summary": "This paper presents DINO-SLAM, a DINO-informed design strategy to enhance\nneural implicit (Neural Radiance Field -- NeRF) and explicit representations\n(3D Gaussian Splatting -- 3DGS) in SLAM systems through more comprehensive\nscene representations. Purposely, we rely on a Scene Structure Encoder (SSE)\nthat enriches DINO features into Enhanced DINO ones (EDINO) to capture\nhierarchical scene elements and their structural relationships. Building upon\nit, we propose two foundational paradigms for NeRF and 3DGS SLAM systems\nintegrating EDINO features. Our DINO-informed pipelines achieve superior\nperformance on the Replica, ScanNet, and TUM compared to state-of-the-art\nmethods.", "AI": {"tldr": "DINO-SLAM通过DINO特征和场景结构编码器（SSE）增强神经隐式（NeRF）和显式（3DGS）SLAM系统，以实现更全面的场景表示。", "motivation": "旨在通过更全面的场景表示来增强SLAM系统中神经隐式（NeRF）和显式（3DGS）表示的性能。", "method": "核心方法是引入一个场景结构编码器（SSE），将DINO特征丰富为增强型DINO特征（EDINO），以捕捉分层场景元素及其结构关系。基于此，提出了两种将EDINO特征整合到NeRF和3DGS SLAM系统中的基础范式。", "result": "DINO-informed的流水线在Replica、ScanNet和TUM数据集上均表现出优于现有最先进方法的性能。", "conclusion": "通过DINO特征增强（特别是EDINO）来改进场景表示，可以显著提升基于NeRF和3DGS的SLAM系统的性能。"}}
{"id": "2507.19481", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19481", "abs": "https://arxiv.org/abs/2507.19481", "authors": ["Byungjun Kim", "Shunsuke Saito", "Giljoo Nam", "Tomas Simon", "Jason Saragih", "Hanbyul Joo", "Junxuan Li"], "title": "HairCUP: Hair Compositional Universal Prior for 3D Gaussian Avatars", "comment": "ICCV 2025. Project Page: https://bjkim95.github.io/haircup/", "summary": "We present a universal prior model for 3D head avatars with explicit hair\ncompositionality. Existing approaches to build generalizable priors for 3D head\navatars often adopt a holistic modeling approach, treating the face and hair as\nan inseparable entity. This overlooks the inherent compositionality of the\nhuman head, making it difficult for the model to naturally disentangle face and\nhair representations, especially when the dataset is limited. Furthermore, such\nholistic models struggle to support applications like 3D face and hairstyle\nswapping in a flexible and controllable manner. To address these challenges, we\nintroduce a prior model that explicitly accounts for the compositionality of\nface and hair, learning their latent spaces separately. A key enabler of this\napproach is our synthetic hairless data creation pipeline, which removes hair\nfrom studio-captured datasets using estimated hairless geometry and texture\nderived from a diffusion prior. By leveraging a paired dataset of hair and\nhairless captures, we train disentangled prior models for face and hair,\nincorporating compositionality as an inductive bias to facilitate effective\nseparation. Our model's inherent compositionality enables seamless transfer of\nface and hair components between avatars while preserving identity.\nAdditionally, we demonstrate that our model can be fine-tuned in a few-shot\nmanner using monocular captures to create high-fidelity, hair-compositional 3D\nhead avatars for unseen subjects. These capabilities highlight the practical\napplicability of our approach in real-world scenarios, paving the way for\nflexible and expressive 3D avatar generation.", "AI": {"tldr": "该论文提出了一个通用的3D头部形象先验模型，明确分离了面部和头发，以实现更好的解耦、灵活的组成和少样本微调。", "motivation": "现有3D头部形象建模方法常将面部和头发视为不可分割的整体，导致难以有效解耦、在数据有限时表现不佳，且难以灵活支持面部和发型互换等应用。", "method": "引入一个明确考虑面部和头发组合性的先验模型，分别学习它们的潜在空间。关键在于开发了一个合成无发数据生成管线，通过扩散先验估计的无发几何和纹理从工作室捕获数据中去除头发。利用头发和无发捕获的配对数据集，训练解耦的先验模型，并将组合性作为归纳偏置促进有效分离。", "result": "模型固有的组合性使得面部和头发组件能在形象之间无缝转移并保持身份。此外，模型可以通过少量单目捕获进行微调，为未见主体创建高保真、头发可组合的3D头部形象。", "conclusion": "该方法在实际场景中具有很强的实用性，为灵活和富有表现力的3D形象生成铺平了道路。"}}
