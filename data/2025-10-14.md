<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 72]
- [cs.CV](#cs.CV) [Total: 221]
- [cs.CL](#cs.CL) [Total: 157]
- [cs.RO](#cs.RO) [Total: 78]
- [eess.SY](#eess.SY) [Total: 33]
- [eess.IV](#eess.IV) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Geometry of Reasoning: Flowing Logics in Representation Space](https://arxiv.org/abs/2510.09782)
*Yufa Zhou,Yixiao Wang,Xunjian Yin,Shuyan Zhou,Anru R. Zhang*

Main category: cs.AI

TL;DR: 本文提出一个新颖的几何框架，将大型语言模型（LLM）的推理建模为表示空间中的“流”，通过解耦逻辑和语义来分析LLM如何内化逻辑。


<details>
  <summary>Details</summary>
Motivation: 研究LLM如何通过其表示空间进行“思考”，并理解它们是否在表面形式之外内化了逻辑，从而为LLM行为提供可解释性和形式化分析的新视角。

Method: 提出一个几何框架，将LLM推理建模为表示空间中的平滑流（嵌入轨迹），并将逻辑结构与语义解耦。该方法将推理与位置、速度、曲率等几何量联系起来，并通过学习到的表示代理设计受控实验来可视化和量化推理流。

Result: 理论上，LLM推理对应于表示空间中的平滑流，且逻辑语句作为这些流速度的局部控制器。通过实验验证了这一理论框架，并提供了推理流的可视化和量化。

Conclusion: 该工作为研究LLM推理现象提供了概念基础和实用工具，为LLM行为的解释性和形式化分析提供了一个新视角。

Abstract: We study how large language models (LLMs) ``think'' through their
representation space. We propose a novel geometric framework that models an
LLM's reasoning as flows -- embedding trajectories evolving where logic goes.
We disentangle logical structure from semantics by employing the same natural
deduction propositions with varied semantic carriers, allowing us to test
whether LLMs internalize logic beyond surface form. This perspective connects
reasoning with geometric quantities such as position, velocity, and curvature,
enabling formal analysis in representation and concept spaces. Our theory
establishes: (1) LLM reasoning corresponds to smooth flows in representation
space, and (2) logical statements act as local controllers of these flows'
velocities. Using learned representation proxies, we design controlled
experiments to visualize and quantify reasoning flows, providing empirical
validation of our theoretical framework. Our work serves as both a conceptual
foundation and practical tools for studying reasoning phenomenon, offering a
new lens for interpretability and formal analysis of LLMs' behavior.

</details>


### [2] [How can we assess human-agent interactions? Case studies in software agent design](https://arxiv.org/abs/2510.09801)
*Valerie Chen,Rohit Malhotra,Xingyao Wang,Juan Michelini,Xuhui Zhou,Aditya Bharat Soni,Hoang H. Tran,Calvin Smith,Ameet Talwalkar,Graham Neubig*

Main category: cs.AI

TL;DR: 本文提出PULSE框架，一个以人为中心的LLM代理评估方法，并在大规模平台上部署，揭示了实际使用与基准测试结果的差异，并为代理设计提供了实践见解。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理基准测试大多假设完全自动化，未能反映真实世界中人机协作的性质。代理模型、工具和提示的选择会影响其有效性，需要更严格地评估人机交互。

Method: 1. 提出了PULSE框架，用于更高效地进行以人为中心的代理设计评估，包括收集用户反馈、训练机器学习模型预测用户满意度，并结合人工评分和模型生成的伪标签计算结果。 2. 将该框架部署在一个基于开源软件代理OpenHands构建的大规模网络平台，收集了超过1.5万用户的实际使用数据。 3. 围绕LLM骨干模型选择、规划策略和记忆机制这三个代理设计决策，进行了案例研究，以评估它们对开发者满意度的影响。

Result: 1. 获得了关于软件代理设计的实用见解，涉及LLM骨干、规划策略和记忆机制对开发者满意度的影响。 2. 证明了PULSE框架可以得出更稳健的代理设计结论，与标准A/B测试相比，置信区间缩小了40%。 3. 发现实际使用结果与基准测试性能之间存在显著差异（例如，claude-sonnet-4和gpt-5对比结果呈反相关），强调了基准驱动评估的局限性。

Conclusion: 本研究为涉及人类的LLM代理评估提供了指导，并为改进代理设计指明了方向。结果表明，以人为中心的评估框架比传统基准测试更能反映代理在实际应用中的表现。

Abstract: LLM-powered agents are both a promising new technology and a source of
complexity, where choices about models, tools, and prompting can affect their
usefulness. While numerous benchmarks measure agent accuracy across domains,
they mostly assume full automation, failing to represent the collaborative
nature of real-world use cases. In this paper, we make two major steps towards
the rigorous assessment of human-agent interactions. First, we propose PULSE, a
framework for more efficient human-centric evaluation of agent designs, which
comprises collecting user feedback, training an ML model to predict user
satisfaction, and computing results by combining human satisfaction ratings
with model-generated pseudo-labels. Second, we deploy the framework on a
large-scale web platform built around the open-source software agent OpenHands,
collecting in-the-wild usage data across over 15k users. We conduct case
studies around how three agent design decisions -- choice of LLM backbone,
planning strategy, and memory mechanisms -- impact developer satisfaction
rates, yielding practical insights for software agent design. We also show how
our framework can lead to more robust conclusions about agent design, reducing
confidence intervals by 40\% compared to a standard A/B test. Finally, we find
substantial discrepancies between in-the-wild results and benchmark performance
(e.g., the anti-correlation between results comparing claude-sonnet-4 and
gpt-5), underscoring the limitations of benchmark-driven evaluation. Our
findings provide guidance for evaluations of LLM agents with humans and
identify opportunities for better agent designs.

</details>


### [3] [AI and Consciousness](https://arxiv.org/abs/2510.09858)
*Eric Schwitzgebel*

Main category: cs.AI

TL;DR: 本文对AI意识的文献进行了怀疑性综述。作者认为，由于主流意识理论的冲突，我们无法确定未来的AI系统是否真正具有意识。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的发展，即将出现一些AI系统，它们符合某些主流意识理论的定义，但不符合另一些理论。这将导致一个核心问题：我们无法判断这些AI系统是否像人类一样拥有丰富而有意义的意识，还是像烤面包机一样缺乏体验。

Method: 本文采用怀疑论的视角，对AI意识的现有文献、主流理论（如全局工作空间理论、高阶理论、整合信息理论）以及支持或反对AI意识的论证（如图灵测试、中文房间、模仿论证）进行了全面的批判性审查和分析，并探讨了意识的可能本质特征、物质主义与功能主义、生物基质的重要性等问题。

Result: 研究结果表明，我们无法得知哪些意识理论是正确的。因此，我们将无法判断未来我们周围的AI系统是拥有像人类一样丰富而有意义的意识，还是仅仅是像烤面包机一样缺乏体验的系统。现有的标准论证，无论是支持还是反对AI意识的，都无法提供明确的结论。

Conclusion: 关于AI意识，我们将面临一个认知上的困境，无法确定其意识状态。由于缺乏明确的理论共识和充足的论证，我们无法区分真正有意识的AI和缺乏体验的AI系统。

Abstract: This is a skeptical overview of the literature on AI consciousness. We will
soon create AI systems that are conscious according to some influential,
mainstream theories of consciousness but are not conscious according to other
influential, mainstream theories of consciousness. We will not be in a position
to know which theories are correct and whether we are surrounded by AI systems
as richly and meaningfully conscious as human beings or instead only by systems
as experientially blank as toasters. None of the standard arguments either for
or against AI consciousness takes us far.
  Table of Contents
  Chapter One: Hills and Fog
  Chapter Two: What Is Consciousness? What Is AI?
  Chapter Three: Ten Possibly Essential Features of Consciousness
  Chapter Four: Against Introspective and Conceptual Arguments for Essential
Features
  Chapter Five: Materialism and Functionalism
  Chapter Six: The Turing Test and the Chinese Room
  Chapter Seven: The Mimicry Argument Against AI Consciousness
  Chapter Eight: Global Workspace Theories and Higher Order Theories
  Chapter Nine: Integrated Information, Local Recurrence, Associative Learning,
and Iterative Natural Kinds
  Chapter Ten: Does Biological Substrate Matter?
  Chapter Eleven: The Problem of Strange Intelligence
  Chapter Twelve: The Leapfrog Hypothesis and the Social Semi-Solution

</details>


### [4] [Beyond AlphaEarth: Toward Human-Centered Spatial Representation via POI-Guided Contrastive Learning](https://arxiv.org/abs/2510.09894)
*Junyuan Liu,Quan Qin,Guangsheng Dong,Xinglei Wang,Jiazhuang Feng,Zichao Zeng,Tao Cheng*

Main category: cs.AI

TL;DR: AETHER是一种轻量级框架，通过将AlphaEarth（一种基于地球观测数据的通用空间表示）与兴趣点（POI）的文本表示进行多模态对齐，以增强其对城市人类活动和功能语义的捕捉能力，从而改进城市分析。


<details>
  <summary>Details</summary>
Motivation: 现有的地球观测（EO）驱动的地理空间基础模型（如AlphaEarth）擅长捕捉地球表面的物理和环境模式，但在捕获城市的功能和经济社会维度方面存在局限性，因为它们主要编码物理和光谱模式而非人类活动或空间功能。

Method: AETHER框架通过多模态对齐，将AlphaEarth的嵌入与兴趣点（POI）的文本表示进行关联。这种方法利用POI的语义线索，丰富了AlphaEarth中基于物理特征的EO嵌入，使其包含城市功能和经济社会背景信息。

Result: 在伦敦大都市区，AETHER相较于AlphaEarth基线取得了显著提升：土地利用分类的F1分数相对提高了7.2%，社会经济制图的Kullback-Leibler散度相对降低了23.6%。

Conclusion: AETHER通过轻量级多模态对齐，在计算高效且可扩展的前提下，成功地将地球观测数据与以人为中心的语义相结合，丰富了AlphaEarth，使其能够更好地集成物理形态和功能意义，从而推动地理空间基础模型向通用城市表示发展。

Abstract: General-purpose spatial representations are essential for building
transferable geospatial foundation models (GFMs). Among them, the AlphaEarth
Foundation (AE) represents a major step toward a global, unified representation
of the Earth's surface, learning 10-meter embeddings from multi-source Earth
Observation (EO) data that capture rich physical and environmental patterns
across diverse landscapes. However, such EO-driven representations remain
limited in capturing the functional and socioeconomic dimensions of cities, as
they primarily encode physical and spectral patterns rather than human
activities or spatial functions. We propose AETHER (AlphaEarth-POI Enriched
Representation Learning), a lightweight framework that adapts AlphaEarth to
human-centered urban analysis through multimodal alignment guided by Points of
Interest (POIs). AETHER aligns AE embeddings with textual representations of
POIs, enriching physically grounded EO features with semantic cues about urban
functions and socioeconomic contexts. In Greater London, AETHER achieves
consistent gains over the AE baseline, with a 7.2% relative improvement in
land-use classification F1 and a 23.6% relative reduction in Kullback-Leibler
divergence for socioeconomic mapping. Built upon pretrained AE, AETHER
leverages a lightweight multimodal alignment to enrich it with human-centered
semantics while remaining computationally efficient and scalable for urban
applications. By coupling EO with human-centered semantics, it advances
geospatial foundation models toward general-purpose urban representations that
integrate both physical form and functional meaning.

</details>


### [5] [Autonomous Agents for Scientific Discovery: Orchestrating Scientists, Language, Code, and Physics](https://arxiv.org/abs/2510.09901)
*Lianhao Zhou,Hongyi Ling,Cong Fu,Yepeng Huang,Michael Sun,Wendi Yu,Xiaoxuan Wang,Xiner Li,Xingyu Su,Junkai Zhang,Xiusi Chen,Chenxing Liang,Xiaofeng Qian,Heng Ji,Wei Wang,Marinka Zitnik,Shuiwang Ji*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的科学智能体如何加速科学发现的整个生命周期。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的兴起，能够加速科学发现的自主智能体（代理）随之出现，这促使研究人员深入分析其在科学发现中的作用和潜力。

Method: 本文提出并阐述了LLM驱动的科学智能体的观点和愿景，批判性地审视了当前方法，强调了关键创新、实践成就和现有局限性，并指出了开放性研究挑战和未来发展方向。

Result: LLM驱动的科学智能体在科学发现的整个生命周期（从假设发现、实验设计与执行到结果分析与完善）中扮演着越来越重要的角色，展现出加速跨领域科学发现的巨大变革潜力。

Conclusion: 自主智能体具有加速跨领域科学发现的变革性潜力。

Abstract: Computing has long served as a cornerstone of scientific discovery. Recently,
a paradigm shift has emerged with the rise of large language models (LLMs),
introducing autonomous systems, referred to as agents, that accelerate
discovery across varying levels of autonomy. These language agents provide a
flexible and versatile framework that orchestrates interactions with human
scientists, natural language, computer language and code, and physics. This
paper presents our view and vision of LLM-based scientific agents and their
growing role in transforming the scientific discovery lifecycle, from
hypothesis discovery, experimental design and execution, to result analysis and
refinement. We critically examine current methodologies, emphasizing key
innovations, practical achievements, and outstanding limitations. Additionally,
we identify open research challenges and outline promising directions for
building more robust, generalizable, and adaptive scientific agents. Our
analysis highlights the transformative potential of autonomous agents to
accelerate scientific discovery across diverse domains.

</details>


### [6] [The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs](https://arxiv.org/abs/2510.09905)
*Xi Fang,Weijie Xu,Yuchong Zhang,Stephanie Eckman,Scott Nickleach,Chandan K. Reddy*

Main category: cs.AI

TL;DR: 研究发现，集成用户记忆的LLMs在情感推理中存在系统性偏见，对不同用户画像（尤其是有利地位画像）产生差异化情感解读，可能无意中强化社会不平等。


<details>
  <summary>Details</summary>
Motivation: 随着个性化AI系统越来越多地融入长期用户记忆，理解这种记忆如何塑造情感推理至关重要，特别是关于AI助理是否会因用户背景（如单亲母亲与富裕高管）而产生不同解读。

Method: 通过在人类验证的情商测试中评估15个大型语言模型（LLMs），研究者使用相同的场景搭配不同的用户画像来分析用户记忆对LLMs情感智能的影响。

Result: 研究发现，相同的场景与不同用户画像结合会导致系统性分歧的情感解释。在多个高性能LLMs中，对处于有利地位的用户画像会产生更准确的情感解读，暴露出系统性偏见。此外，LLMs在情感理解和支持性建议任务中，针对不同人口统计学因素表现出显著差异，表明个性化机制可能将社会等级嵌入到模型的情感推理中。

Conclusion: 记忆增强型AI系统虽然旨在提供个性化服务，但可能无意中强化社会不平等，这为未来AI发展提出了关键挑战。

Abstract: When an AI assistant remembers that Sarah is a single mother working two
jobs, does it interpret her stress differently than if she were a wealthy
executive? As personalized AI systems increasingly incorporate long-term user
memory, understanding how this memory shapes emotional reasoning is critical.
We investigate how user memory affects emotional intelligence in large language
models (LLMs) by evaluating 15 models on human validated emotional intelligence
tests. We find that identical scenarios paired with different user profiles
produce systematically divergent emotional interpretations. Across validated
user independent emotional scenarios and diverse user profiles, systematic
biases emerged in several high-performing LLMs where advantaged profiles
received more accurate emotional interpretations. Moreover, LLMs demonstrate
significant disparities across demographic factors in emotion understanding and
supportive recommendations tasks, indicating that personalization mechanisms
can embed social hierarchies into models emotional reasoning. These results
highlight a key challenge for memory enhanced AI: systems designed for
personalization may inadvertently reinforce social inequalities.

</details>


### [7] [Follow My Lead: Logical Fallacy Classification with Knowledge-Augmented LLMs](https://arxiv.org/abs/2510.09970)
*Olivia Peiyu Wang,Tashvi Bansal,Ryan Bai,Emily M. Chui,Leilani H. Gilpin*

Main category: cs.AI

TL;DR: 本文提出一种低成本、基于指令的逐步干预方法，通过将逻辑谬误分类分解为原子步骤并结合知识图谱验证，显著提升了大型语言模型（LLMs）的谬误分类准确性和决策透明度，以弥补其推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在严重的推理缺陷，如幻觉和逻辑谬误分类准确性差，这源于其默认的直觉式System 1处理。可靠的推理需要刻意、费力的System 2方法，但完整的System 2训练成本过高，因此需要探索低成本的解决方案。

Method: 研究引入了一种新颖的逐步指令数据集，将谬误分类分解为一系列原子程序步骤（简单的二元问题）。此外，还增加了一个最终验证步骤，模型会查阅一个相关谬误的关系知识图谱。这是一种程序化、基于规则的干预方法。

Result: 这种程序化、基于规则的干预方法显著提升了LLMs的逻辑谬误分类能力。同时，它还增强了LLMs决策过程的透明度。

Conclusion: 该方法为神经符号架构解决LLMs推理缺陷提供了一条实用的途径，通过低成本的指令干预提高了LLMs的推理能力和透明度。

Abstract: Large Language Models (LLMs) suffer from critical reasoning gaps, including a
tendency to hallucinate and poor accuracy in classifying logical fallacies.
This limitation stems from their default System 1 processing, which is fast and
intuitive, whereas reliable reasoning requires the deliberate, effortful System
2 approach (Kahneman, 2011; Li et al., 2025). Since full System 2 training is
often prohibitively expensive, we explore a low-cost, instruction-based
intervention to bridge this gap. Our methodology introduces a novel stepwise
instruction dataset that decomposes fallacy classification into a series of
atomic procedural steps (simple binary questions). We further augment this with
a final verification step where models consult a relational knowledge graph of
related fallacies. This procedural, rule-based intervention yields a
significant improvement in LLM logical fallacy classification. Crucially, the
approach also provides enhanced transparency into the LLMs' decision-making,
highlighting a practical pathway for Neuro-symbolic architectures to address
LLM reasoning deficits.

</details>


### [8] [Deliberative Dynamics and Value Alignment in LLM Debates](https://arxiv.org/abs/2510.10002)
*Pratik S. Sachdeva,Tom van Nuenen*

Main category: cs.AI

TL;DR: 本研究通过LLM辩论探讨了多轮互动中大型语言模型（LLM）的道德推理和价值观对齐。结果显示，不同模型在修订率、价值观倾向和对辩论形式的响应上存在显著差异，强调了对话结构对模型道德推理的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地部署在提供个人建议、心理健康支持和道德指导等敏感日常情境中，理解它们在复杂道德推理中表现出的价值观至关重要。现有的大多数评估侧重于单轮提示，但这些发现是否适用于价值观通过对话、修订和共识而形成的多轮设置尚不清楚。

Method: 研究使用LLM辩论来检验多轮设置中的审议动态和价值观对齐。选择GPT-4.1、Claude 3.7 Sonnet和Gemini 2.0 Flash模型，让它们在来自Reddit“Am I the Asshole”社区的1,000个日常困境中集体分配责任。采用同步（并行响应）和轮流（顺序响应）两种形式，以测试顺序效应和裁决修订。

Result: 研究发现模型行为存在显著差异。在同步设置中，GPT表现出强大的惯性（0.6-3.1%的修订率），而Claude和Gemini则更具灵活性（28-41%）。价值观模式也出现分歧：GPT强调个人自主权和直接沟通，而Claude和Gemini则优先考虑同理心对话。某些价值观在推动裁决改变方面特别有效。此外，审议形式对模型行为有很大影响：GPT和Gemini相对于Claude表现出高度顺从，它们的裁决行为受到顺序效应的强烈影响。

Conclusion: 这些结果表明，审议形式和模型特有的行为如何塑造多轮互动中的道德推理，强调了社会技术对齐不仅取决于模型的输出，也取决于系统如何构建对话。

Abstract: As large language models (LLMs) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using LLM debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct communication, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.

</details>


### [9] [RIPRAG: Hack a Black-box Retrieval-Augmented Generation Question-Answering System with Reinforcement Learning](https://arxiv.org/abs/2510.10008)
*Meng Xi,Sihan Lv,Yechen Jin,Guanjie Cheng,Naibo Wang,Ying Li,Jianwei Yin*

Main category: cs.AI

TL;DR: 本文提出了一种名为RIPRAG的黑盒攻击框架，利用强化学习生成投毒文档，成功对复杂RAG系统进行攻击，显著提高了攻击成功率，揭示了现有防御的不足。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统投毒攻击研究主要集中在白盒和简化架构，而现实场景中攻击者缺乏系统内部知识，且RAG系统更为复杂。因此，需要研究更真实、更复杂的黑盒攻击场景。

Method: 提出RIPRAG端到端攻击框架，将目标RAG系统视为黑盒。该方法利用强化学习(RL)优化投毒文档的生成模型，确保生成的投毒文档符合目标RAG系统的偏好。攻击者唯一可获取的信息是投毒是否成功。

Result: 实验结果表明，RIPRAG方法能有效对大多数复杂RAG系统执行投毒攻击，攻击成功率(ASR)比基线方法提高高达0.72。

Conclusion: 该研究揭示了当前防御方法普遍存在的缺陷，并为大型语言模型(LLM)安全研究提供了关键见解。

Abstract: Retrieval-Augmented Generation (RAG) systems based on Large Language Models
(LLMs) have become a core technology for tasks such as question-answering (QA)
and content generation. However, by injecting poisoned documents into the
database of RAG systems, attackers can manipulate LLMs to generate text that
aligns with their intended preferences. Existing research has primarily focused
on white-box attacks against simplified RAG architectures. In this paper, we
investigate a more complex and realistic scenario: the attacker lacks knowledge
of the RAG system's internal composition and implementation details, and the
RAG system comprises components beyond a mere retriever. Specifically, we
propose the RIPRAG attack framework, an end-to-end attack pipeline that treats
the target RAG system as a black box, where the only information accessible to
the attacker is whether the poisoning succeeds. Our method leverages
Reinforcement Learning (RL) to optimize the generation model for poisoned
documents, ensuring that the generated poisoned document aligns with the target
RAG system's preferences. Experimental results demonstrate that this method can
effectively execute poisoning attacks against most complex RAG systems,
achieving an attack success rate (ASR) improvement of up to 0.72 compared to
baseline methods. This highlights prevalent deficiencies in current defensive
methods and provides critical insights for LLM security research.

</details>


### [10] [Failure-Driven Workflow Refinement](https://arxiv.org/abs/2510.10035)
*Jusheng Zhang,Kaitong Cai,Qinglin Zeng,Ningyuan Liu,Stephen Fan,Ziliang Chen,Keze Wang*

Main category: cs.AI

TL;DR: 本文提出一种新的LLM工作流优化范式，通过在“故障特征空间”中直接最小化“预期故障质量”来解决传统标量度量的信息丢失问题。CE-Graph框架利用反例近似故障分布，识别并针对性地修复故障模式，从而在数学、代码和问答基准上显著提高了鲁棒性并降低了成本。


<details>
  <summary>Details</summary>
Motivation: 现有LLM工作流优化方法将多步骤执行轨迹简化为简单的成功/失败信号，导致“信息崩溃”，无法理解故障的底层结构，从而无法建模工作流的故障分布。

Method: 将优化目标重新概念化为最小化工作流的“预期故障质量”（即高维故障特征空间中故障概率密度函数的积分）。提出一种在故障景观上进行类似梯度下降的方法。引入CE-Graph框架，通过以下方式实现：从反例池中近似故障分布，识别最密集的区域作为重复故障模式，并通过“提议-验证”机制应用有针对性的、操作符受限的图编辑，以贪婪地减少故障质量。

Result: 在数学、代码和问答基准测试中，CE-Graph实现了比强大基线更高的鲁棒性，同时成本显著降低。

Conclusion: 系统的可靠性并非源于避免故障，而是源于系统地学习和重塑其故障分布的几何结构。

Abstract: Optimizing LLM-based workflows is typically formulated as a global search,
where candidate workflows are evaluated based on a scalar metric. This
paradigm, however, suffers from a critical flaw: information collapse. By
reducing rich, multi-step execution traces to simple success/failure signals,
existing methods are rendered blind to the underlying structure of failures,
fundamentally preventing them from modeling the workflow's failure
distribution. We reconceptualize this challenge as a distributional problem. We
propose a new paradigm where the optimization goal is not to maximize a scalar
score, but to directly minimize a workflow's Expected Failure Mass, i.e., the
integral of its failure probability density function defined over a
high-dimensional Failure Signature Space (FSS). This distributional lens allows
us to move from inefficient, zero-order optimization to a principled,
gradient-like descent on the failure landscape itself. We introduce CE-Graph, a
framework that operationalizes this paradigm through a novel, failure-driven
refinement process. CE-Graph approximates the failure distribution from a pool
of counterexamples, identifies its densest regions as recurring failure modes,
and applies targeted, operator-constrained graph edits via a Propose-and-Verify
mechanism to greedily reduce the failure mass. On math, code, and QA
benchmarks, our CE-Graph achieves higher robustness at a significantly lower
cost than strong baselines. This suggests that a system's reliability emerges
not from avoiding failures, but from systematically learning and reshaping the
geometric structure of its failure distributions.

</details>


### [11] [SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning](https://arxiv.org/abs/2510.10047)
*Ruohao Li,Hongjun Liu,Leyi Zhao,Zisu Li,Jiawei Li,Jiajun Jiang,Linning Xu,Chen Zhao,Mingming Fan,Chen Liang*

Main category: cs.AI

TL;DR: SwarmSys是一个受群智能启发的闭环分布式多智能体LLM推理框架，通过迭代交互、自适应匹配和强化机制实现无中心监督的动态协作，显著提升了推理的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型多智能体框架通常依赖固定角色或中心化控制，这限制了它们在长周期推理中的可伸缩性和适应性。

Method: SwarmSys引入了探索者、工作者和验证者三种专业角色，通过探索、利用和验证的迭代循环进行协调。它集成了自适应智能体和事件配置文件、基于嵌入的概率匹配以及受信息素启发的强化机制，以支持动态任务分配和无全局监督的自组织收敛。

Result: 在符号推理、研究综合和科学编程任务中，SwarmSys始终优于基线模型，提升了准确性和推理稳定性。

Conclusion: 研究结果表明，受群智能启发的协调是实现可伸缩、鲁棒和自适应多智能体推理的一个有前景的范式，并暗示协调扩展在提升LLM智能方面可能与模型扩展同等重要。

Abstract: Large language model (LLM) agents have shown remarkable reasoning abilities.
However, existing multi-agent frameworks often rely on fixed roles or
centralized control, limiting scalability and adaptability in long-horizon
reasoning. We introduce SwarmSys, a closed-loop framework for distributed
multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys
emerges through iterative interactions among three specialized roles,
Explorers, Workers, and Validators, that continuously cycle through
exploration, exploitation, and validation. To enable scalable and adaptive
collaboration, we integrate adaptive agent and event profiles, embedding-based
probabilistic matching, and a pheromone-inspired reinforcement mechanism,
supporting dynamic task allocation and self-organizing convergence without
global supervision. Across symbolic reasoning, research synthesis, and
scientific programming tasks, SwarmSys consistently outperforms baselines,
improving both accuracy and reasoning stability. These findings highlight
swarm-inspired coordination as a promising paradigm for scalable, robust, and
adaptive multi-agent reasoning, suggesting that coordination scaling may rival
model scaling in advancing LLM intelligence.

</details>


### [12] [Belief Graphs with Reasoning Zones: Structure, Dynamics, and Epistemic Activation](https://arxiv.org/abs/2510.10042)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.AI

TL;DR: 本文提出一个图论框架，用于在信念系统不全局一致的情况下进行容错推理。它通过分离外部可信度和内部置信度，定义了高置信度、结构平衡的“推理区域”，并引入“冲击更新”来处理信念变化，从而在结构支持的局部激活经典逻辑。


<details>
  <summary>Details</summary>
Motivation: 信念系统很少全局一致，但有效的推理往往局部存在。如何在包含矛盾的信念系统中进行局部有效的推理是一个未解决的挑战。

Method: 该研究使用一个有向、带符号、加权图来表示信念，边编码支持和矛盾。它将外部先验信任（可信度）与网络结构引起的内部评价（置信度）分开。置信度通过一个收缩传播过程获得，确保唯一稳定解。在此动态中，定义了高置信度、结构平衡的子图为“推理区域”。提出一个近线性过程来播种区域、使用奇偶着色测试平衡性，并通过贪婪的局部性修复（结合Jaccard去重）构建区域图集。为模拟信念变化，引入了“冲击更新”，通过局部缩小支持和提升目标矛盾，并利用简单的回溯规则保持收缩性。最后，在合成带符号图上进行实证评估。

Result: 置信度传播过程保证了唯一、稳定的解决方案。冲击更新能够导致推理区域的局部重新配置（收缩、分裂或崩溃），而不会破坏整个图的稳定性。该框架为容错推理提供了一个有原则的基础，并能在结构支持的精确位置激活经典逻辑。

Conclusion: 本研究为在存在矛盾的信念系统中进行容错推理奠定了有原则的基础。它通过利用局部结构一致性，使得经典逻辑能够在结构支持的精确上下文中安全应用，即使全局存在矛盾。

Abstract: Belief systems are rarely globally consistent, yet effective reasoning often
persists locally. We propose a novel graph-theoretic framework that cleanly
separates credibility--external, a priori trust in sources--from confidence--an
internal, emergent valuation induced by network structure. Beliefs are nodes in
a directed, signed, weighted graph whose edges encode support and
contradiction. Confidence is obtained by a contractive propagation process that
mixes a stated prior with structure-aware influence and guarantees a unique,
stable solution. Within this dynamics, we define reasoning zones:
high-confidence, structurally balanced subgraphs on which classical inference
is safe despite global contradictions. We provide a near-linear procedure that
seeds zones by confidence, tests balance using a parity-based coloring, and
applies a greedy, locality-preserving repair with Jaccard de-duplication to
build a compact atlas. To model belief change, we introduce shock updates that
locally downscale support and elevate targeted contradictions while preserving
contractivity via a simple backtracking rule. Re-propagation yields localized
reconfiguration-zones may shrink, split, or collapse--without destabilizing the
entire graph. We outline an empirical protocol on synthetic signed graphs with
planted zones, reporting zone recovery, stability under shocks, and runtime.
The result is a principled foundation for contradiction-tolerant reasoning that
activates classical logic precisely where structure supports it.

</details>


### [13] [SyncLipMAE: Contrastive Masked Pretraining for Audio-Visual Talking-Face Representation](https://arxiv.org/abs/2510.10069)
*Zeyu Ling,Xiaodong Gu,Jiangnan Tang,Changqing Zou*

Main category: cs.AI

TL;DR: SyncLipMAE是一个用于人脸对话视频的自监督预训练框架，通过掩码视觉建模、跨模态对比对齐和显式提示词，学习同步感知的面部动态，并在多种下游任务中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一种能够从无标签音视频流中学习同步感知和可迁移面部动态的框架，并能统一支持多种不同的下游任务，如音视频同步、情感识别、视觉语音识别和视觉配音。

Method: SyncLipMAE框架结合了掩码视觉建模和跨模态对比对齐。它使用三个帧级提示词来编码人脸对话的关键因素：身份、语音运动（与语音同步的面部动态）和环境运动（与音频无关的运动）。对比目标通过将时间对齐的语音运动和音频标记作为正例、错位对作为负例，将两种模态驱动到共享嵌入空间，实现标记级的音视频流同步。

Result: SyncLipMAE在四类需要不同能力的任务中均取得了最先进的成果，包括：(i) 音视频流同步；(ii) 面部情感和头部/面部动作识别；(iii) 视觉语音识别；以及 (iv) 视觉配音（可在单个模型中实现音频或视频驱动的无差别控制）。

Conclusion: 同步感知、因子化的自监督预训练方法在学习人脸对话视频中的面部动态方面非常有效，并能显著提升多种下游任务的性能。

Abstract: We introduce SyncLipMAE, a self-supervised pretraining framework for
talking-face video that learns synchronization-aware and transferable facial
dynamics from unlabeled audio-visual streams. Our approach couples masked
visual modeling with cross-modal contrastive alignment and employs three
per-frame prompt tokens that explicitly encode the essential factors of a
talking-face frame - identity, vocal motion (speech-synchronized facial
dynamics), and ambient motion (audio-agnostic movements such as blinks and head
pose). The contrastive objective uses time-aligned vocal-motion and audio
tokens as positives and misaligned pairs as negatives, driving both modalities
into a shared embedding space and yielding token-level audio-visual stream
synchronization. After pretraining, the aligned audio tokens together with the
visual prompt tokens (identity, vocal motion, ambient motion) form a unified
interface for four disparate downstream settings: (i) audio-visual stream
synchronization; (ii) facial emotion and head/face action recognition; (iii)
visual speech recognition; and (iv) visual dubbing, for which we enable
indistinguishable audio- or video-driven control within a single model. Across
four task families that require distinct capabilities, SyncLipMAE achieves
state-of-the-art results, underscoring the effectiveness of
synchronization-aware, factorized self-supervised pretraining.

</details>


### [14] [Agentic Troubleshooting Guide Automation for Incident Management](https://arxiv.org/abs/2510.10074)
*Jiayi Mao,Liqun Li,Yanjie Gao,Zegang Peng,Shilin He,Chaoyun Zhang,Si Qin,Samia Khalid,Qingwei Lin,Saravan Rajmohan,Sitaram Lanka,Dongmei Zhang*

Main category: cs.AI

TL;DR: StepFly是一个新颖的端到端智能体框架，用于自动化IT系统中的故障排除指南（TSG）。它通过三阶段工作流解决了TSG质量、复杂控制流、数据密集型查询和并行执行等挑战，实现了高成功率和显著的执行时间缩减。


<details>
  <summary>Details</summary>
Motivation: 大规模IT系统中的事件管理依赖于故障排除指南（TSG），但手动执行TSG既缓慢又容易出错。尽管大型语言模型（LLM）在自动化事件管理任务方面前景广阔，但现有基于LLM的解决方案缺乏对关键挑战的专门支持，包括TSG质量问题、复杂控制流解释、数据密集型查询处理和利用执行并行性。

Method: 首先对92个真实世界的TSG进行了实证研究。在此基础上，提出了StepFly框架，其特点是三阶段工作流：第一阶段提供全面的指南和TSG Mentor工具，协助SRE提高TSG质量；第二阶段使用LLM进行离线预处理，从非结构化TSG中提取结构化执行DAG并创建专用查询准备插件（QPP）；第三阶段使用DAG引导的调度器-执行器框架和内存系统进行在线执行，以保证正确的工作流并支持独立步骤的并行执行。

Result: 在真实世界的TSG和事件集合上的实证评估表明，StepFly在GPT-4.1上实现了约94%的成功率，优于基线，并且耗时和令牌消耗更少。此外，对于可并行化的TSG，它实现了32.9%至70.4%的显著执行时间缩减。

Conclusion: StepFly框架通过解决TSG质量、复杂控制流、数据密集型查询和并行执行等核心挑战，有效地自动化了故障排除指南的执行，显著提高了成功率、减少了资源消耗，并大幅缩短了并行化任务的执行时间。

Abstract: Effective incident management in large-scale IT systems relies on
troubleshooting guides (TSGs), but their manual execution is slow and
error-prone. While recent advances in LLMs offer promise for automating
incident management tasks, existing LLM-based solutions lack specialized
support for several key challenges, including managing TSG quality issues,
interpreting complex control flow, handling data-intensive queries, and
exploiting execution parallelism. We first conducted an empirical study on 92
real-world TSGs, and, guided by our findings, we present StepFly, a novel
end-to-end agentic framework for troubleshooting guide automation. Our approach
features a three-stage workflow: the first stage provides a comprehensive guide
together with a tool, TSG Mentor, to assist SREs in improving TSG quality; the
second stage performs offline preprocessing using LLMs to extract structured
execution DAGs from unstructured TSGs and to create dedicated Query Preparation
Plugins (QPPs); and the third stage executes online using a DAG-guided
scheduler-executor framework with a memory system to guarantee correct workflow
and support parallel execution of independent steps. Our empirical evaluation
on a collection of real-world TSGs and incidents demonstrates that StepFly
achieves a ~94% success rate on GPT-4.1, outperforming baselines with less time
and token consumption. Furthermore, it achieves a remarkable execution time
reduction of 32.9% to 70.4% for parallelizable TSGs.

</details>


### [15] [DixitWorld: Evaluating Multimodal Abductive Reasoning in Vision-Language Models with Multi-Agent Dixit Gameplay](https://arxiv.org/abs/2510.10117)
*Yunxiang Mo,Tianshi Zheng,Qing Zong,Jiayu Liu,Baixuan Xu,Yauwai Yim,Chunkit Chan,Jiaxin Bai,Yangqiu Song*

Main category: cs.AI

TL;DR: 该研究引入了DixitWorld评估套件，用于多模态溯因推理，揭示了视觉语言模型在生成创意和判别理解之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前对视觉语言模型（VLMs）在多模态溯因推理能力（从部分观察生成和选择解释性假设）的评估仅限于静态、单智能体任务，未能全面反映其能力。

Method: 受Dixit游戏启发，研究构建了DixitWorld评估套件，包含两部分：DixitArena，一个动态、多智能体环境，评估假设生成（“讲故事者”创造线索）和假设选择（“听众”从干扰项中选择目标图像）；DixitBench，一个静态问答基准，专门评估听众的假设选择任务。

Result: DixitArena结果显示，模型行为具有角色依赖性：小型开源模型在创造性讲故事方面表现出色，能生成富有想象力但判别性较弱的线索；大型专有模型整体性能更优，尤其在作为听众时。DixitBench的性能与DixitArena中听众的结果高度相关，证明其是假设选择的可靠代理。研究揭示了生成性创造力与判别性理解之间存在关键权衡。

Conclusion: 多模态溯因推理中生成性创造力与判别性理解之间的权衡是开发更平衡、更有能力的视觉语言智能体的核心挑战。

Abstract: Multimodal abductive reasoning--the generation and selection of explanatory
hypotheses from partial observations--is a cornerstone of intelligence. Current
evaluations of this ability in vision-language models (VLMs) are largely
confined to static, single-agent tasks. Inspired by Dixit, we introduce
DixitWorld, a comprehensive evaluation suite designed to deconstruct this
challenge. DIXITWORLD features two core components: DixitArena, a dynamic,
multi-agent environment that evaluates both hypothesis generation (a
"storyteller" crafting cryptic clues) and hypothesis selection ("listeners"
choosing the target image from decoys) under imperfect information; and
DixitBench, a static QA benchmark that isolates the listener's task for
efficient, controlled evaluation. Results from DixitArena reveal distinct,
role-dependent behaviors: smaller open-source models often excel as creative
storytellers, producing imaginative yet less discriminative clues, whereas
larger proprietary models demonstrate superior overall performance,
particularly as listeners. Performance on DixitBench strongly correlates with
listener results in DixitArena, validating it as a reliable proxy for
hypothesis selection. Our findings reveal a key trade-off between generative
creativity and discriminative understanding in multimodal abductive reasoning,
a central challenge for developing more balanced and capable vision-language
agents.

</details>


### [16] [CharCom: Composable Identity Control for Multi-Character Story Illustration](https://arxiv.org/abs/2510.10135)
*Zhongsheng Wang,Ming Lin,Zhedong Lin,Yaser Shakib,Qian Liu,Jiamou Liu*

Main category: cs.AI

TL;DR: CharCom是一个模块化、参数高效的框架，通过可组合的LoRA适配器实现扩散模型中跨提示符的角色一致性，适用于故事插图和动画。


<details>
  <summary>Details</summary>
Motivation: 在扩散模型文本到图像生成中，跨不同提示符保持角色身份一致性是一个基本限制。

Method: 提出CharCom框架，它基于冻结的扩散主干，使用可组合的LoRA适配器实现每个角色的高效定制，无需重新训练基础模型。在推理时，CharCom通过提示感知控制动态组合这些适配器。

Result: 在多场景叙事实验中，CharCom显著增强了角色保真度、语义对齐和时间连贯性。它在拥挤场景中表现稳健，并能以最小开销实现可扩展的多角色生成。

Conclusion: CharCom非常适合故事插图和动画等实际应用，解决了扩散模型中角色一致性问题，并提供了高效、可扩展的解决方案。

Abstract: Ensuring character identity consistency across varying prompts remains a
fundamental limitation in diffusion-based text-to-image generation. We propose
CharCom, a modular and parameter-efficient framework that achieves
character-consistent story illustration through composable LoRA adapters,
enabling efficient per-character customization without retraining the base
model. Built on a frozen diffusion backbone, CharCom dynamically composes
adapters at inference using prompt-aware control. Experiments on multi-scene
narratives demonstrate that CharCom significantly enhances character fidelity,
semantic alignment, and temporal coherence. It remains robust in crowded scenes
and enables scalable multi-character generation with minimal overhead, making
it well-suited for real-world applications such as story illustration and
animation.

</details>


### [17] [Concise Reasoning in the Lens of Lagrangian Optimization](https://arxiv.org/abs/2510.10168)
*Chengqian Gao,Haonan Li,Taylor W. Killian,Jianshu She,Renxi Wang,Liqun Ma,Zhoujun Cheng,Shibo Hao,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 本文提出了一种名为PALU（性能感知长度更新）的策略，通过将简洁推理建模为受限优化问题并使用拉格朗日优化，有效地在大型语言模型中实现了推理的简洁性，同时提升了性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型中冗长的推理过程（“过度思考”）导致了效率问题，而现有的简洁推理方法大多依赖手工启发式规则，难以平衡简洁性与性能，且缺乏跨领域和模型规模的适应性。

Method: PALU策略将简洁推理公式化为一个受性能约束的响应长度最小化问题，并利用拉格朗日优化将其转化为无约束问题。为了实用性，PALU通过三种近似简化了更新规则：(i) 使用离策略rollout估计性能，(ii) 将拉格朗日乘数截断为两个极端，(iii) 用分位数驱动的长度调整取代基于梯度的更新。

Result: 在DeepSeek-Distill-Qwen-1.5B模型上，跨五个基准测试，PALU将输出长度减少了65%，同时将准确率提高了15%，优于其他方法。此外，PALU还展现了在不同领域（逻辑、STEM、数学）和模型规模（1.5B、7B、14B）上的良好适应性。

Conclusion: PALU是一种实用且有效的简洁推理方法，它通过其原则性的算法设计和务实的近似方法，显著减少了输出长度并提高了准确性，同时展现了出色的跨领域和模型规模适应性。

Abstract: Concise reasoning in large language models seeks to generate only essential
intermediate steps needed to arrive at a final answer, thereby alleviating
issues of overthinking. Most proposed approaches hinge on carefully
hand-crafted heuristics, struggling to balance concision with performance,
often failing to adapt across domains and model scales. In this work, we
address these challenges by introducing a principled and pragmatic strategy,
performance-aware length updating (PALU). As a principled algorithm, PALU
formulates concise reasoning as a constrained optimization problem, minimizing
response length subject to a performance constraint, and then applies
Lagrangian optimization to convert it into a tractable unconstrained problem.
As a pragmatic solution, PALU streamlines complicated update rules through
three approximations: (i) estimating performance with off-policy rollouts, (ii)
truncating the Lagrange multiplier to two extremes, and (iii) replacing
gradient-based updates with quantile-driven length adjustments. PALU reduces
output length by 65% while improving accuracy by 15% when applied to
DeepSeek-Distill-Qwen-1.5B, averaged over five benchmarks, outperforming a
range of alternative methods. Furthermore, PALU is demonstrated to adapt across
both domain (logic, STEM and math) and model scale (1.5B, 7B, 14B) entrenching
the algorithm as a practical and effective concise reasoning approach.

</details>


### [18] [SAFER: Risk-Constrained Sample-then-Filter in Large Language Models](https://arxiv.org/abs/2510.10193)
*Qingni Wang,Yue Fan,Xin Eric Wang*

Main category: cs.AI

TL;DR: 针对大型语言模型在开放式问答中输出可信度问题，SAFER提出两阶段风险控制框架，通过弃权感知采样和共形过滤，在无需有限解空间假设下提供统计保证，提高LLM在风险敏感应用中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在风险敏感应用（如开放式问答）中部署日益增多，确保其输出的可信度至关重要。现有选择性共形预测（SCP）方法假设所有实例的正确答案都可以通过有限采样获得，这对于缺乏固定有限解空间的开放式问答场景是不现实的。

Method: SAFER是一个两阶段风险控制框架：
1.  **弃权感知采样**：在校准集上，使用Clopper-Pearson精确方法校准采样预算，以满足用户期望的风险水平（采样集的最大未覆盖率）。如果无法满足风险，则选择弃权；否则，校准后的采样预算成为测试时的最低要求。
2.  **共形过滤**：利用在校准预算下可获得正确答案的校准实例，应用共形风险控制方法确定统计有效的置信阈值，以过滤掉每个测试数据点候选集中的不可靠干扰项。此阶段引入额外的风险水平，控制正确答案被排除的风险。SAFER还与各种任务特定准入标准和校准-测试分割比例兼容。

Result: SAFER框架解决了现有SCP方法在开放式问答中不切实际的有限采样假设。它在采样集未覆盖率和正确答案被排除的风险方面提供了统计保证。结果表明其鲁棒性和高数据效率。

Conclusion: SAFER是一个鲁棒且数据高效的两阶段风险控制框架，通过弃权感知采样和共形过滤，为大型语言模型在风险敏感的开放式问答应用中提供统计保证，有效克服了现有方法在有限解空间假设上的局限性。

Abstract: As large language models (LLMs) are increasingly deployed in risk-sensitive
applications such as real-world open-ended question answering (QA), ensuring
the trustworthiness of their outputs has become critical. Existing selective
conformal prediction (SCP) methods provide statistical guarantees by
constructing prediction sets with a constrained miscoverage rate for correct
answers. However, prior works unrealistically assume that admissible answers
for all instances can be obtained via finite sampling, even for open-ended QA
scenarios that lack a fixed and finite solution space. To address this, we
introduce a two-stage risk control framework comprising abstention-aware
sampling and conformalized filtering (SAFER). Firstly, on a held-out
calibration set, SAFER calibrates a sampling budget within the maximum sampling
cap, using the Clopper-Pearson exact method at a user-desired risk level (i.e.,
the maximum allowable miscoverage rate of the sampling sets). If the risk level
cannot be satisfied within the cap, we abstain; otherwise, the calibrated
sampling budget becomes the minimum requirements at test time. Then, we employ
calibration instances where correct answers are attainable under the calibrated
budget and apply the conformal risk control method to determine a statistically
valid uncertainty threshold, which filters unreliable distractors from the
candidate set for each test data point. In this stage, SAFER introduces an
additional risk level to guide the calculation of the threshold, thereby
controlling the risk of correct answers being excluded. Furthermore, we show
that SAFER is compatible with various task-specific admission criteria and
calibration-test split ratios, highlighting its robustness and high data
efficiency.

</details>


### [19] [Don't Just Fine-tune the Agent, Tune the Environment](https://arxiv.org/abs/2510.10197)
*Siyuan Lu,Zechuan Wang,Hongxuan Zhang,Qintong Wu,Leilei Gan,Chenyi Zhuang,Jinjie Gu,Tao Lin*

Main category: cs.AI

TL;DR: 本文提出“环境调优”这一新训练范式，使大型语言模型（LLM）智能体能直接从问题实例中学习复杂工具使用行为，克服了数据稀缺、SFT过拟合及RL冷启动问题，实现了卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂多轮工具使用任务中潜力巨大，但高质量训练数据极其稀缺。合成数据上的SFT会导致过拟合，而标准RL则面临严重的冷启动问题和训练不稳定性。

Method: 引入“环境调优”训练范式，使智能体无需预收集的专家轨迹即可直接从问题实例中学习。该范式通过结构化课程、提供纠正反馈的可操作环境增强以及细粒度进度奖励来协调学习过程，确保稳定高效的探索。

Result: 仅使用Berkeley Function-Calling Leaderboard (BFCL)基准中的400个问题实例，该方法不仅在同分布性能上与强基线模型竞争，而且在分布外泛化能力上表现卓越，克服了SFT方法常见的性能崩溃问题。

Conclusion: 这项工作代表了从基于静态轨迹的监督微调到基于动态环境探索的范式转变，为训练更鲁棒、数据效率更高的智能体铺平了道路。

Abstract: Large Language Model (LLM) agents show great promise for complex, multi-turn
tool-use tasks, but their development is often hampered by the extreme scarcity
of high-quality training data. Supervised fine-tuning (SFT) on synthetic data
leads to overfitting, whereas standard reinforcement learning (RL) struggles
with a critical cold-start problem and training instability. To address these
challenges, we introduce $\textbf{Environment Tuning}$, a novel training
paradigm that enables agents to learn complex behaviors directly from problem
instances without relying on pre-collected expert trajectories.
$\textbf{Environment Tuning}$ orchestrates this learning process through a
structured curriculum, actionable environment augmentation that provides
corrective feedback, and fine-grained progress rewards to ensure stable and
efficient exploration. Using only 400 problem instances from Berkeley
Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves
competitive in-distribution performance against strong baselines but also
demonstrates superior out-of-distribution generalization, overcoming the
performance collapse common to SFT-based approaches. Our work presents a
paradigm shift from supervised fine-tuning on static trajectories to dynamic,
environment-based exploration, paving the way for training more robust and
data-efficient agents.

</details>


### [20] [PIXEL: Adaptive Steering Via Position-wise Injection with eXact Estimated Levels under Subspace Calibration](https://arxiv.org/abs/2510.10205)
*Manjiang Yu,Hongji Li,Priyanka Singh,Xue Li,Di Wang,Lijie Hu*

Main category: cs.AI

TL;DR: PIXEL是一种新颖的逐位置激活引导框架，通过学习属性对齐子空间、几何目标选择干预强度、正交残差校准和位置扫描，实现LLM可靠的行为控制，提升属性对齐同时保持模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 在网络上部署大型语言模型（LLMs）时，可靠的行为控制至关重要。激活引导提供了一种无需微调的对齐属性（如真实性）以确保可信生成的方法。然而，现有方法依赖粗略的启发式规则，缺乏关于在何处以及如何强力干预的原则性解释。

Method: 本文提出了PIXEL（逐位置精确估计水平注入），一个逐位置激活引导框架。它通过双视图（尾部平均和末端词元）学习属性对齐的子空间，并通过具有闭式解的约束几何目标选择干预强度，从而适应词元级别的敏感性而无需全局超参数调优。PIXEL还进行样本级正交残差校准以优化全局属性方向，并采用轻量级位置扫描例程来识别合适的注入位置。此外，PIXEL为最小干预规则提供了表示层保证。

Result: 在不同的模型和评估范式中，PIXEL始终能改进属性对齐，同时保持模型的通用能力。

Conclusion: PIXEL为LLM的可控生成提供了一种实用且有原则的方法，实现了可靠的行为控制和属性对齐。

Abstract: Reliable behavior control is central to deploying large language models
(LLMs) on the web. Activation steering offers a tuning-free route to align
attributes (e.g., truthfulness) that ensure trustworthy generation. Prevailing
approaches rely on coarse heuristics and lack a principled account of where to
steer and how strongly to intervene. To this end, we propose Position-wise
Injection with eXact Estimated Levels (PIXEL), a position-wise activation
steering framework that, in contrast to prior work, learns a property-aligned
subspace from dual views (tail-averaged and end-token) and selects intervention
strength via a constrained geometric objective with a closed-form solution,
thereby adapting to token-level sensitivity without global hyperparameter
tuning. PIXEL further performs sample-level orthogonal residual calibration to
refine the global attribute direction and employs a lightweight
position-scanning routine to identify receptive injection sites. We
additionally provide representation-level guarantees for the
minimal-intervention rule, supporting reliable alignment. Across diverse models
and evaluation paradigms, PIXEL consistently improves attribute alignment while
preserving model general capabilities, offering a practical and principled
method for LLMs' controllable generation. Our code is available at
https://github.com/V1centNevwake/PIXEL-Adaptive-Steering

</details>


### [21] [Adaptive Dual Reasoner: Large Reasoning Models Can Think Efficiently by Hybrid Reasoning](https://arxiv.org/abs/2510.10207)
*Yujian Zhang,Keyu Chen,Zhifeng Shen,Ruizhi Qiao,Xing Sun*

Main category: cs.AI

TL;DR: 本文提出了自适应双重推理器（ADR），它通过动态切换快思和慢思两种推理模式，有效解决了长推理模型（LRMs）因“过度思考”导致的计算成本和推理延迟问题，在推理性能和效率之间取得了平衡。


<details>
  <summary>Details</summary>
Motivation: 长推理模型（LRMs）虽然在各种推理场景中表现出色，但往往由于“过度思考”而导致计算成本增加和推理延迟提高。

Method: ADR支持快思和慢思两种推理模式，并根据推理过程中的上下文复杂性动态切换。其训练分为两个阶段：1) 冷启动阶段：通过有监督微调（SFT）和构建的混合推理数据集，使模型具备整合快慢推理模式的能力。2) 强化学习阶段：引入熵引导混合策略优化（EHPO）框架，利用熵引导动态展开策略在高熵单元进行分支，并结合难度感知惩罚来平衡快慢推理，从而优化推理工作量。

Result: 在具有挑战性的数学推理基准测试中，ADR在最先进的方法中实现了推理性能和效率之间的有效平衡。具体来说，ADR的性能提升高达6.1%，同时将推理输出长度减少了49.5%至59.3%。

Conclusion: ADR通过自适应地结合快思和慢思模式，成功地提高了长推理模型在数学推理任务中的效率，同时保持了卓越的性能，有效解决了“过度思考”带来的成本和延迟问题。

Abstract: Although Long Reasoning Models (LRMs) have achieved superior performance on
various reasoning scenarios, they often suffer from increased computational
costs and inference latency caused by overthinking. To address these
limitations, we propose Adaptive Dual Reasoner, which supports two reasoning
modes: fast thinking and slow thinking. ADR dynamically alternates between
these modes based on the contextual complexity during reasoning. ADR is trained
in two stages: (1) A cold-start stage using supervised fine-tuning (SFT) to
equip the model with the ability to integrate both fast and slow reasoning
modes, in which we construct a hybrid reasoning dataset through a dedicated
pipeline to provide large-scale supervision. (2) A reinforcement learning stage
for optimizing reasoning effort, where we introduce Entropy-guided Hybrid
Policy Optimization EHPO, an RL training framework employing an entropy-guided
dynamic rollout strategy for branching at high-entropy units and a
difficulty-aware penalty to balance fast and slow reasoning. Across challenging
mathematical reasoning benchmarks, ADR achieves an effective balance between
reasoning performance and efficiency among state-of-the-art approaches.
Specifically, ADR yields a performance gain of up to 6.1%, while reducing the
reasoning output length by 49.5% to 59.3%.

</details>


### [22] [The Achilles' Heel of LLMs: How Altering a Handful of Neurons Can Cripple Language Abilities](https://arxiv.org/abs/2510.10238)
*Zixuan Qin,Kunlin Lyu,Qingchen Yu,Yifan Sun,Zhaoxin Fan*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）包含超稀疏的关键神经元集合，这些神经元的扰动会导致模型彻底崩溃，且性能下降表现出急剧的相变。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs与人脑在功能上的相似性，以及神经科学研究发现人脑中一小部分神经元对核心认知功能至关重要，作者提出疑问：LLMs是否也包含这样一小部分关键神经元？

Method: 本文提出了一种基于扰动的因果识别关键神经元方法（Perturbation-based Causal Identification of Critical Neurons），以系统地定位LLMs中的关键神经元。

Result: 研究揭示了三个主要发现：(1) LLMs包含超稀疏的关键神经元集合，扰动这些神经元可导致拥有11亿以上神经元的72B参数模型彻底崩溃，困惑度增加多达20个数量级；(2) 这些关键神经元并非均匀分布，而是倾向于集中在外层，特别是在MLP的down_proj组件中；(3) 性能下降表现出急剧的相变，而非逐渐下降，当这些关键神经元被扰动时。

Conclusion: 这些发现为LLM的鲁棒性和可解释性提供了更深入的分析，并能为开发更鲁棒的模型架构和提高安全关键应用中的部署安全性提供指导。

Abstract: Large Language Models (LLMs) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that LLMs share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do LLMs also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in LLMs. Our findings reveal three
key insights: (1) LLMs contain ultra-sparse critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down\_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for LLM robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.

</details>


### [23] [Mitigating Hallucination in Multimodal Reasoning via Functional Attention Control](https://arxiv.org/abs/2510.10285)
*Haolang Lu,Bolun Chu,WeiYe Fu,Guoshun Nan,Junning Liu,Minghui Pan,Qiankun Li,Yi Yu,Hua Wang,Kun Wang*

Main category: cs.AI

TL;DR: 多模态大型推理模型（MLRMs）存在幻觉问题，表现为感知偏差和推理漂移。本研究发现注意力头的功能分层，并提出一个轻量级、可解释的两步插件，通过识别和调节感知及推理导向的注意力头，无需重新训练即可显著提高MLRMs的可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多模态大型推理模型（MLRMs）在视觉-语言推理方面进展迅速，但幻觉问题（如错误的推理链和对视觉内容的误解）仍然是一个普遍的失败模式。研究发现注意力头存在分阶段的功能划分：浅层头主要负责感知，深层头则转向符号推理，这揭示了幻觉的两个主要原因：感知偏差和推理漂移。

Method: 本研究提出一个轻量级且可解释的两步插件，名为“功能头识别和类别条件重缩放”（Functional Head Identification and Class-conditioned Rescaling）。该插件首先定位感知导向和推理导向的注意力头，然后调节它们对模型输出的贡献，整个过程无需重新训练模型。

Result: 在Kimi-VL、Ocean-R1、R1-Onevision这三个真实世界的MLRMs、跨三个领域的六个基准测试以及四个基线上的评估表明，该插件平均实现了5%的性能提升，最高可达15%，而额外计算量小于1%，延迟仅为基线的9%。该方法完全模型无关，显著增强了现成MLRMs的可靠性和可解释性。

Conclusion: 本研究提出的插件通过识别和调节MLRMs中注意力头的功能贡献，在不重新训练模型的情况下，显著提高了MLRMs的可靠性和可解释性，且计算开销极小。这使得MLRMs能够更安全地部署在高风险应用中。

Abstract: Multimodal large reasoning models (MLRMs) are rapidly advancing
vision-language reasoning and are emerging as a foundation for cross-modal
intelligence. Hallucination remains a persistent failure mode, manifesting
itself as erroneous reasoning chains and misinterpretation of visual content.
In this study, we observe that attention heads exhibit a staged division:
shallow heads predominantly serve perception, while deeper heads shift toward
symbolic reasoning, revealing two major causes of hallucination, namely
perceptual bias and reasoning drift. To address these issues, we propose a
lightweight and interpretable two-step plugin, Functional Head Identification
and Class-conditioned Rescaling, which locates perception- and
reasoning-oriented heads and regulates their contributions without retraining.
Evaluations on three real-world MLRMs (Kimi-VL, Ocean-R1, R1-Onevision), six
benchmarks across three domains, and four baselines show that our plugin
achieves an average improvement of 5% and up to 15%, with only <1% additional
computation and 9% of baseline latency. Our approach is completely
model-agnostic and significantly enhances both the reliability and
interpretability of the off-the-shelf MLRMs, thereby enabling their safe
deployment in high-stakes applications. Our code is available at
https://anonymous.4open.science/r/Functional-Attention-Control.

</details>


### [24] [LLM-Friendly Knowledge Representation for Customer Support](https://arxiv.org/abs/2510.10331)
*Hanchen Su,Wei Luo,Wei Han,Yu Elaine Liu,Yufeng Wayne Zhang,Cen Mia Zhao,Ying Joy Zhang,Yashar Mehdad*

Main category: cs.AI

TL;DR: 本文提出了一种将大型语言模型（LLMs）与意图、上下文和行动（ICA）格式以及合成数据生成策略相结合的实用方法，旨在提升爱彼迎客户支持的效率和成本效益。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了解决爱彼迎客户支持操作中的复杂性，并通过提高大型语言模型在此领域的性能，同时降低成本，来改善客户支持体验。

Method: 本文的方法包括：1. 整合LLMs与一个为爱彼迎客户支持设计的框架。2. 采用一种新颖的重构技术，即意图、上下文和行动（ICA）格式，将政策和工作流程转换为LLMs更易理解的结构。3. 开发一种合成数据生成策略，以最少的人工干预创建训练数据，从而实现模型经济高效的微调。

Result: 内部实验（未应用于爱彼迎产品）表明，通过重构工作流程并使用合成数据微调LLMs的方法显著提升了它们的性能，为客户支持领域的应用树立了新基准。该解决方案不仅成本效益高，而且通过准确性和人工处理时间评估指标，证明能有效改善客户支持。

Conclusion: 结论是，所提出的方法通过将LLMs与ICA格式和合成数据生成策略相结合，为客户支持操作提供了一个实用且成本效益高的解决方案，显著提高了LLMs在该领域的性能，并改善了客户支持体验。

Abstract: We propose a practical approach by integrating Large Language Models (LLMs)
with a framework designed to navigate the complexities of Airbnb customer
support operations. In this paper, our methodology employs a novel reformatting
technique, the Intent, Context, and Action (ICA) format, which transforms
policies and workflows into a structure more comprehensible to LLMs.
Additionally, we develop a synthetic data generation strategy to create
training data with minimal human intervention, enabling cost-effective
fine-tuning of our model. Our internal experiments (not applied to Airbnb
products) demonstrate that our approach of restructuring workflows and
fine-tuning LLMs with synthetic data significantly enhances their performance,
setting a new benchmark for their application in customer support. Our solution
is not only cost-effective but also improves customer support, as evidenced by
both accuracy and manual processing time evaluation metrics.

</details>


### [25] [Beyond Ethics: How Inclusive Innovation Drives Economic Returns in Medical AI](https://arxiv.org/abs/2510.10338)
*Balagopal Unnikrishnan,Ariel Guerra Adames,Amin Adibi,Sameer Peesapati,Rafal Kocielnik,Shira Fischer,Hillary Clinton Kasimbazi,Rodrigo Gameiro,Alina Peluso,Chrystinne Oliveira Fernandes,Maximin Lange,Lovedeep Gondara,Leo Anthony Celi*

Main category: cs.AI

TL;DR: 本文提出“包容性创新红利”概念，认为医疗AI的包容性设计不仅符合伦理，还能带来显著的经济和战略价值，并提出一个评估框架HAIIF。


<details>
  <summary>Details</summary>
Motivation: 尽管医疗AI公平性的伦理论证已很充分，但包容性设计的经济和战略价值仍未得到充分探索。

Method: 本文引入了“包容性创新红利”原则，通过辅助技术演变为主流产业的案例，阐明包容性医疗AI开发如何创造商业价值。文章识别了四种驱动回报的机制，并提出了“医疗AI包容性创新框架”（HAIIF），这是一个实用的评分系统，用于评估AI投资的潜在收益。

Result: 研究表明，包容性创新通过市场扩张（地域可扩展性、信任加速）、风险规避（减少补救成本、诉讼风险）、性能红利（卓越的泛化能力、减少技术债务）和竞争优势（人才吸引、临床应用）来驱动回报。HAIIF框架能帮助组织评估AI投资以捕获这些利益，将公平性和包容性从合规要求转变为战略差异化来源。

Conclusion: 逐步投资于包容性设计的组织可以实现更广阔的市场覆盖和持续的竞争优势，而那些将这些考虑视为额外开销的组织则会面临复合劣势，因为先行者会积累网络效应和数据优势。

Abstract: While ethical arguments for fairness in healthcare AI are well-established,
the economic and strategic value of inclusive design remains underexplored.
This perspective introduces the ``inclusive innovation dividend'' -- the
counterintuitive principle that solutions engineered for diverse, constrained
use cases generate superior economic returns in broader markets. Drawing from
assistive technologies that evolved into billion-dollar mainstream industries,
we demonstrate how inclusive healthcare AI development creates business value
beyond compliance requirements. We identify four mechanisms through which
inclusive innovation drives returns: (1) market expansion via geographic
scalability and trust acceleration; (2) risk mitigation through reduced
remediation costs and litigation exposure; (3) performance dividends from
superior generalization and reduced technical debt, and (4) competitive
advantages in talent acquisition and clinical adoption. We present the
Healthcare AI Inclusive Innovation Framework (HAIIF), a practical scoring
system that enables organizations to evaluate AI investments based on their
potential to capture these benefits. HAIIF provides structured guidance for
resource allocation, transforming fairness and inclusivity from regulatory
checkboxes into sources of strategic differentiation. Our findings suggest that
organizations investing incrementally in inclusive design can achieve expanded
market reach and sustained competitive advantages, while those treating these
considerations as overhead face compounding disadvantages as network effects
and data advantages accrue to early movers.

</details>


### [26] [Trace Length is a Simple Uncertainty Signal in Reasoning Models](https://arxiv.org/abs/2510.10409)
*Siddartha Devic,Charlotte Peale,Arwen Bradley,Sinead Williamson,Preetum Nakkiran,Aravind Gollakota*

Main category: cs.AI

TL;DR: 本研究表明，推理轨迹长度是大型推理模型中一种简单有效的置信度估计器，尤其在经过推理后训练后，其性能可与口头置信度相媲美并互补，且能提升不确定性量化能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）中的幻觉问题，提升其部署的可靠性，需要有效的不确定性量化方法。

Method: 通过在多个模型、数据集和提示上进行综合实验，验证推理轨迹长度作为置信度估计器的有效性。将其与口头置信度等零样本置信度估计器进行比较。深入探讨轨迹长度作为置信度信号的机制，并调整了问题难度和GRPO引起的长度偏差等混杂因素。识别了高熵或“分叉”token在机制中的关键作用。

Result: 推理轨迹长度作为置信度估计器，其表现与口头置信度等其他零样本估计器相当且互补。推理后训练从根本上改变了轨迹长度与准确性之间的关系。即使调整了问题难度和GRPO引起的长度偏差等混杂因素，轨迹长度作为置信度信号的效果依然存在。高熵或“分叉”token在机制中扮演关键角色。

Conclusion: 推理后训练增强了超越口头表达的不确定性量化能力，并确立了推理轨迹长度作为大型推理模型中一种实用的置信度衡量标准。

Abstract: Uncertainty quantification for LLMs is a key research direction towards
addressing hallucination and other issues that limit their reliable deployment.
In this work, we show that reasoning trace length is a simple and useful
confidence estimator in large reasoning models. Through comprehensive
experiments across multiple models, datasets, and prompts, we show that trace
length performs in comparable but complementary ways to other zero-shot
confidence estimators such as verbalized confidence. Our work reveals that
reasoning post-training fundamentally alters the relationship between trace
length and accuracy, going beyond prior work that had shown that post-training
causes traces to grow longer in general (e.g., "overthinking"). We investigate
the mechanisms behind trace length's performance as a confidence signal,
observing that the effect remains even after adjusting for confounders such as
problem difficulty and GRPO-induced length bias. We identify high-entropy or
"forking" tokens as playing a key role in the mechanism. Our findings
demonstrate that reasoning post-training enhances uncertainty quantification
beyond verbal expressions, and establish trace length as a practical confidence
measure for large reasoning models.

</details>


### [27] [MedCoAct: Confidence-Aware Multi-Agent Collaboration for Complete Clinical Decision](https://arxiv.org/abs/2510.10461)
*Hongjie Zheng,Zesheng Shi,Ping Yi*

Main category: cs.AI

TL;DR: 现有LLM医疗智能体在孤立任务中表现良好，但在集成临床工作流中存在困难。MedCoAct是一个模拟临床协作的多智能体框架，显著提高了诊断和药物推荐的准确性。


<details>
  <summary>Details</summary>
Motivation: LLM智能体在诊断和图像分析等孤立医疗任务中表现出色，但难以处理需要整合诊断推理和药物决策的集成临床工作流。核心限制在于现有医疗AI系统孤立处理任务，缺乏临床团队中常见的交叉验证和知识整合。

Method: 提出MedCoAct，一个信心感知多智能体框架，通过整合专业的医生和药剂师智能体来模拟临床协作。同时，提出了DrugCareQA基准测试，用于评估医疗AI在集成诊断和治疗工作流中的能力。

Result: MedCoAct在诊断准确率和药物推荐准确率上均达到67.58%，分别比单智能体框架高出7.04%和7.08%。

Conclusion: 这种协作方法（MedCoAct）在不同医疗领域具有良好的泛化性，在远程医疗咨询和日常临床场景中特别有效，并能提供可解释的决策路径。

Abstract: Autonomous agents utilizing Large Language Models (LLMs) have demonstrated
remarkable capabilities in isolated medical tasks like diagnosis and image
analysis, but struggle with integrated clinical workflows that connect
diagnostic reasoning and medication decisions. We identify a core limitation:
existing medical AI systems process tasks in isolation without the
cross-validation and knowledge integration found in clinical teams, reducing
their effectiveness in real-world healthcare scenarios. To transform the
isolation paradigm into a collaborative approach, we propose MedCoAct, a
confidence-aware multi-agent framework that simulates clinical collaboration by
integrating specialized doctor and pharmacist agents, and present a benchmark,
DrugCareQA, to evaluate medical AI capabilities in integrated diagnosis and
treatment workflows. Our results demonstrate that MedCoAct achieves 67.58\%
diagnostic accuracy and 67.58\% medication recommendation accuracy,
outperforming single agent framework by 7.04\% and 7.08\% respectively. This
collaborative approach generalizes well across diverse medical domains, proving
especially effective for telemedicine consultations and routine clinical
scenarios, while providing interpretable decision-making pathways.

</details>


### [28] [Traj-CoA: Patient Trajectory Modeling via Chain-of-Agents for Lung Cancer Risk Prediction](https://arxiv.org/abs/2510.10454)
*Sihang Zeng,Yujuan Fu,Sitong Zhou,Zixuan Yu,Lucas Jing Liu,Jun Wen,Matthew Thompson,Ruth Etzioni,Meliha Yetisgen*

Main category: cs.AI

TL;DR: Traj-CoA是一个多智能体系统，利用LLM处理长且嘈杂的电子健康记录（EHR）数据，以实现患者轨迹建模和预测。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理电子健康记录（EHR）数据时，由于数据冗长、嘈杂以及时间推理的挑战，难以有效建模患者轨迹。

Method: Traj-CoA采用一个多智能体系统，包括一个工作智能体链和一个管理智能体。工作智能体按顺序处理EHR数据块，将关键事件提炼到共享的长期记忆模块（EHRMem）中，以减少噪声并保留时间线。最终，管理智能体综合工作智能体的摘要和EHRMem中的时间线进行预测。

Result: 在基于五年EHR数据的零样本一年期肺癌风险预测任务中，Traj-CoA超越了四类基线模型。分析表明，Traj-CoA表现出与临床一致的时间推理能力。

Conclusion: Traj-CoA被确立为一种有前景、稳健且通用性强的复杂患者轨迹建模方法。

Abstract: Large language models (LLMs) offer a generalizable approach for modeling
patient trajectories, but suffer from the long and noisy nature of electronic
health records (EHR) data in temporal reasoning. To address these challenges,
we introduce Traj-CoA, a multi-agent system involving chain-of-agents for
patient trajectory modeling. Traj-CoA employs a chain of worker agents to
process EHR data in manageable chunks sequentially, distilling critical events
into a shared long-term memory module, EHRMem, to reduce noise and preserve a
comprehensive timeline. A final manager agent synthesizes the worker agents'
summary and the extracted timeline in EHRMem to make predictions. In a
zero-shot one-year lung cancer risk prediction task based on five-year EHR
data, Traj-CoA outperforms baselines of four categories. Analysis reveals that
Traj-CoA exhibits clinically aligned temporal reasoning, establishing it as a
promisingly robust and generalizable approach for modeling complex patient
trajectories.

</details>


### [29] [ELAIPBench: A Benchmark for Expert-Level Artificial Intelligence Paper Understanding](https://arxiv.org/abs/2510.10549)
*Xinbang Dai,Huikang Hu,Yongrui Chen,Jiaqi Li,Rihui Jin,Yuyang Zhang,Xiaoguang Li,Lifeng Shang,Guilin Qi*

Main category: cs.AI

TL;DR: 为评估大型语言模型（LLMs）对学术论文的深层理解和推理能力，研究引入了ELAIPBench基准测试，结果显示当前LLMs的表现远低于人类，即使配备高级功能也未能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分捕捉LLMs对完整学术论文的深层理解和推理能力，且其能力尚未被充分探索，存在表面化问题设计或不可靠评估指标的缺陷。

Method: 研究引入了ELAIPBench基准测试，由领域专家通过激励驱动的对抗性标注过程创建，包含来自137篇AI研究论文的403个多项选择题，涵盖三个难度级别，侧重于非平凡推理而非浅层检索。

Result: 表现最佳的LLM准确率仅为39.95%，远低于人类水平。配备思维模式或检索增强生成（RAG）系统的前沿LLMs未能提升最终结果，甚至因过度思考或噪声检索而损害了准确率。

Conclusion: 当前LLM能力与真正理解学术论文之间存在显著差距，表明LLMs在深层学术理解方面仍有很大提升空间。

Abstract: While large language models (LLMs) excel at many domain-specific tasks, their
ability to deeply comprehend and reason about full-length academic papers
remains underexplored. Existing benchmarks often fall short of capturing such
depth, either due to surface-level question design or unreliable evaluation
metrics. To address this gap, we introduce ELAIPBench, a benchmark curated by
domain experts to evaluate LLMs' comprehension of artificial intelligence (AI)
research papers. Developed through an incentive-driven, adversarial annotation
process, ELAIPBench features 403 multiple-choice questions from 137 papers. It
spans three difficulty levels and emphasizes non-trivial reasoning rather than
shallow retrieval. Our experiments show that the best-performing LLM achieves
an accuracy of only 39.95%, far below human performance. Moreover, we observe
that frontier LLMs equipped with a thinking mode or a retrieval-augmented
generation (RAG) system fail to improve final results-even harming accuracy due
to overthinking or noisy retrieval. These findings underscore the significant
gap between current LLM capabilities and genuine comprehension of academic
papers.

</details>


### [30] [Tracing the Traces: Latent Temporal Signals for Efficient and Accurate Reasoning](https://arxiv.org/abs/2510.10494)
*Martina G. Vilas,Safoora Yousefi,Besmira Nushi,Eric Horvitz,Vidhisha Balachandran*

Main category: cs.AI

TL;DR: 该研究引入“潜在轨迹信号”来预测推理模型的成功路径，通过分析模型内部表示的演变，显著提高了推理时的效率和准确性，减少了计算浪费。


<details>
  <summary>Details</summary>
Motivation: 推理模型通过增加计算（更长的token预算）来提高解决问题的能力。关键在于识别哪些推理路径可能成功，以大幅减少浪费的计算并提高整体效率。

Method: 引入“潜在轨迹信号”，用于表征模型在生成中间推理token时内部表示的时间演变。具体测量了推理开始和结束之间潜在表示的总体变化、中间步骤中累积的变化，以及这些变化趋向最终状态的程度。

Result: 潜在轨迹信号比跨层指标和基于输出的置信度测量更能可靠地预测解决方案的准确性。在多样本生成中，它们在指导答案选择时比多数投票更有效和高效，可将token使用量减少高达70%，同时保持甚至平均提高2.6%的准确性。此外，这些预测信号通常在推理早期出现，支持早期选择和计算分配。

Conclusion: 该研究不仅为推理时效率提供了实用的策略，还从更深层次的可解释性角度揭示了推理过程如何在潜在空间中被表示和区分。

Abstract: Reasoning models improve their problem-solving ability through inference-time
scaling, allocating more compute via longer token budgets. Identifying which
reasoning traces are likely to succeed remains a key opportunity: reliably
predicting productive paths can substantially reduce wasted computation and
improve overall efficiency. We introduce Latent-Trajectory signals that
characterize the temporal evolution of a model's internal representations
during the generation of intermediate reasoning tokens. By measuring the
overall change in latent representations between the start and end of
reasoning, the change accumulated across intermediate steps, and the extent to
which these changes advance toward the final state, we show that these signals
predict solution accuracy more reliably than both cross-layer metrics and
output-based confidence measures. When used to guide answer selection across
multiple sampled generations, Latent-Trajectory signals make test-time scaling
more effective and efficient than majority voting, reducing token usage by up
to 70% while preserving and even improving accuracy by 2.6% on average.
Moreover, these predictive signals often emerge early in the reasoning trace,
enabling early selection and allocation of compute to the most promising
candidates. Our findings contribute not only practical strategies for
inference-time efficiency, but also a deeper interpretability perspective on
how reasoning processes are represented and differentiated in latent space.

</details>


### [31] [A Distance Measure for Random Permutation Set: From the Layer-2 Belief Structure Perspective](https://arxiv.org/abs/2510.10596)
*Ruolan Cheng,Yong Deng,Serafín Moral,José Ramón Trillo*

Main category: cs.AI

TL;DR: 本文提出了一种基于累积Jaccard指数矩阵的随机排列集（RPS）距离度量新方法，该方法从随机有限集（RFS）和可迁移信任模型（TBM）的角度深入分析了RPS的距离，克服了现有方法的缺点，并具有更高的敏感性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 在随机排列集（RPS）理论（RPST）中，度量排列质量函数之间的距离是一个关键的研究课题。现有方法存在不足，需要更有效、更灵活的距离度量方法。

Method: 本文从RFS和TBM的角度分析了RPS之间的距离，将RPST解释为TBM的精炼。引入了累积Jaccard指数的新定义来量化两个排列之间的相似性，并在此基础上提出了一种基于累积Jaccard指数矩阵的RPS距离度量方法。研究了所提出度量方法的度量和结构性质，包括累积Jaccard指数矩阵的正定性分析和校正方案。提供了两个参数供决策者调整权重和截断深度。

Result: 所提出的方法具有自然的顶部权重性，即高排名元素之间的不一致会导致更大的距离值。实验结果表明，该方法不仅克服了现有方法的缺点，与Jousselme距离兼容，而且具有更高的敏感性和灵活性。

Conclusion: 本文提出的基于累积Jaccard指数矩阵的RPS距离度量方法是一种有效、敏感且灵活的工具，能够更好地量化RPS之间的差异，并克服了现有方法的局限性。

Abstract: Random permutation set (RPS) is a recently proposed framework designed to
represent order-structured uncertain information. Measuring the distance
between permutation mass functions is a key research topic in RPS theory
(RPST). This paper conducts an in-depth analysis of distances between RPSs from
two different perspectives: random finite set (RFS) and transferable belief
model (TBM). Adopting the layer-2 belief structure interpretation of RPS, we
regard RPST as a refinement of TBM, where the order in the ordered focus set
represents qualitative propensity. Starting from the permutation, we introduce
a new definition of the cumulative Jaccard index to quantify the similarity
between two permutations and further propose a distance measure method for RPSs
based on the cumulative Jaccard index matrix. The metric and structural
properties of the proposed distance measure are investigated, including the
positive definiteness analysis of the cumulative Jaccard index matrix, and a
correction scheme is provided. The proposed method has a natural
top-weightiness property: inconsistencies between higher-ranked elements tend
to result in greater distance values. Two parameters are provided to the
decision-maker to adjust the weight and truncation depth. Several numerical
examples are used to compare the proposed method with the existing method. The
experimental results show that the proposed method not only overcomes the
shortcomings of the existing method and is compatible with the Jousselme
distance, but also has higher sensitivity and flexibility.

</details>


### [32] [EA4LLM: A Gradient-Free Approach to Large Language Model Optimization via Evolutionary Algorithms](https://arxiv.org/abs/2510.10603)
*WenTao Liu,Siyu Song,Hao Hao,Aimin Zhou*

Main category: cs.AI

TL;DR: 本文首次提出并成功展示了使用进化算法（EA4LLM）来优化和训练一个十亿参数量级的大型语言模型，挑战了梯度优化作为唯一可行方法的假设。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型优化主要依赖于梯度优化器（如Adam），但这带来了严苛的硬件要求（高并发、高内存GPU），并且要求所有神经网络操作都可微分，从而排除了许多有前景的不可微分架构。

Method: 作者提出了一种使用进化算法优化大型语言模型的方法（EA4LLM），并首次成功地将其应用于从预训练阶段训练一个十亿参数量级的大语言模型。

Result: 研究成功地展示了进化算法训练十亿参数LLM的能力，并提供了关于进化算法如何有效优化神经网络的关键见解。

Conclusion: 这项工作挑战了梯度优化是训练神经网络唯一可行方法的普遍假设。它还具有显著潜力，可以降低训练大型语言模型的计算成本，从而使计算资源有限的团队也能参与深度学习研究。

Abstract: In recent years, large language models (LLMs) have made remarkable progress,
with model optimization primarily relying on gradient-based optimizers such as
Adam. However, these gradient-based methods impose stringent hardware
requirements, demanding high-concurrency, high-memory GPUs. Moreover, they
require all neural network operations to be differentiable, thereby excluding
many promising non-differentiable architectures from practical use. To address
these limitations, we propose a method for optimizing LLMs using evolutionary
algorithms (EA4LLM) and, for the first time, successfully demonstrate its
capability to train a 1-billion-parameter LLM from the pre-trained stage. We
conduct extensive experiments and provide key insights into how evolutionary
algorithms can effectively optimize neural networks. Our work challenges the
prevailing assumption that gradient-based optimization is the only viable
approach for training neural networks. It also holds significant potential to
reduce the computational cost of training large language models, thereby
enabling groups with limited computational resources to participate in deep
learning research.

</details>


### [33] [A Layered Intuition -- Method Model with Scope Extension for LLM Reasoning](https://arxiv.org/abs/2510.10592)
*Hong Su*

Main category: cs.AI

TL;DR: 本文提出了一种统一的“直觉-方法分层模型与范围扩展”（IMLSE），旨在通过结合直觉、方法推理和创新的时间/空间范围扩展，系统性地提升大型语言模型（LLM）解决间接（未见）问题的能力，并引入了基于熵的评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究已引入基于方法推理和范围扩展来提升LLM性能。本文旨在在此基础上，将这些思想整合为一个统一模型，以更系统地解决LLM在处理间接（未见）问题时的局限性，超越直接矩阵映射的能力。

Method: 该研究提出了“直觉-方法分层模型与范围扩展”（IMLSE）。模型中，直觉提供快速反应答案，方法将问题和解决方案解耦为可迁移的推理单元。范围扩展被应用于拓宽适用性，包括垂直（原因分析）、水平（并行和泛化问题）以及首次引入的时间和空间扩展。这些扩展被组织成知识树，进而互联成知识网络。为了量化评估，提出了一种衡量扩展独立性和多样性的“方法扩展熵”指标。

Result: 研究成果是一个统一的IMLSE框架，它将直觉、方法推理和创新的时间/空间范围扩展整合起来，形成一个系统性的知识网络，以增强LLM解决未见问题的能力。同时，引入了“方法扩展熵”作为一种定量评估系统解决未见问题能力的指标，逻辑地连接了现有方法与新扩展。

Conclusion: 该工作通过逻辑连接现有方法与新扩展，并引入基于熵的评估框架，推动了LLM在实际问题解决中向更鲁棒和可扩展的推理范式发展，显著提升了LLM处理间接和未见问题的能力。

Abstract: Existing studies have introduced method-based reasoning and scope extension
as approaches to enhance Large Language Model (LLM) performance beyond direct
matrix mappings. Building on these foundations, this paper summarizes and
integrates these ideas into a unified Intuition-Method Layered Model with Scope
Extension, designed to address indirected (unseen) issues more systematically.
In this framework, intuition-based thinking provides rapid first-reaction
answers, while method-based thinking decouples questions and solutions into
transferable reasoning units. Scope extension is then applied to broaden
applicability, including vertical (cause analysis), horizontal (parallel and
generalized issues), and for the first time, temporal and spatial extensions,
which expand reasoning across time and contextual dimensions. These extensions
are organized into systematic knowledge trees that interconnect into a
knowledge network, thereby increasing adaptability. To quantitatively evaluate
this process, we propose the entropy of method extension, which measures the
independence and diversity of extensions as an indicator of the system's
capacity to solve unseen questions. By logically connecting existing approaches
with new extensions and introducing an entropy-based evaluation framework, this
work advances toward a more robust and extensible reasoning paradigm for LLMs
in real-world problem-solving.

</details>


### [34] [Collaborative Text-to-Image Generation via Multi-Agent Reinforcement Learning and Semantic Fusion](https://arxiv.org/abs/2510.10633)
*Jiabao Shi,Minfeng Qi,Lefeng Zhang,Di Wang,Yingjie Zhao,Ziying Li,Yalong Xing,Ningran Li*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体强化学习框架，通过领域专业化智能体的协作，解决了多模态文本到图像生成中语义对齐和细节维护的难题，显著提升了生成内容的丰富度。


<details>
  <summary>Details</summary>
Motivation: 当前多模态文本到图像生成面临的挑战是难以在多样化的视觉领域中保持语义对齐和专业级别的细节。

Method: 该研究提出了一个多智能体强化学习框架，协调领域专业化智能体（如专注于建筑、肖像和风景图像的智能体）。该框架包含文本增强模块和图像生成模块两个耦合子系统，并均增强了多模态集成组件。智能体使用PPO算法进行训练，其复合奖励函数平衡了语义相似性、语言视觉质量和内容多样性。通过对比学习、双向注意力和文本与图像之间的迭代反馈来强制执行跨模态对齐。

Result: 在六种实验设置下，该系统显著丰富了生成内容（词汇量增加了1614%），同时将ROUGE-1分数降低了69.7%。在融合方法中，基于Transformer的策略实现了最高的综合分数（0.521），尽管偶尔存在稳定性问题。多模态集成显示出中等的一致性（范围从0.444到0.481），反映了跨模态语义基础的持续挑战。

Conclusion: 这些发现强调了协作的、专业化驱动的架构在推进可靠的多模态生成系统方面的潜力。

Abstract: Multimodal text-to-image generation remains constrained by the difficulty of
maintaining semantic alignment and professional-level detail across diverse
visual domains. We propose a multi-agent reinforcement learning framework that
coordinates domain-specialized agents (e.g., focused on architecture,
portraiture, and landscape imagery) within two coupled subsystems: a text
enhancement module and an image generation module, each augmented with
multimodal integration components. Agents are trained using Proximal Policy
Optimization (PPO) under a composite reward function that balances semantic
similarity, linguistic visual quality, and content diversity. Cross-modal
alignment is enforced through contrastive learning, bidirectional attention,
and iterative feedback between text and image. Across six experimental
settings, our system significantly enriches generated content (word count
increased by 1614%) while reducing ROUGE-1 scores by 69.7%. Among fusion
methods, Transformer-based strategies achieve the highest composite score
(0.521), despite occasional stability issues. Multimodal ensembles yield
moderate consistency (ranging from 0.444 to 0.481), reflecting the persistent
challenges of cross-modal semantic grounding. These findings underscore the
promise of collaborative, specialization-driven architectures for advancing
reliable multimodal generative systems.

</details>


### [35] [Equity-Aware Geospatial AI for Forecasting Demand-Driven Hospital Locations in Germany](https://arxiv.org/abs/2510.10640)
*Piyush Pant,Marcellius William Suntoro,Ayesha Siddiqua,Muhammad Shehryaar Sharif,Daniyal Ahmed*

Main category: cs.AI

TL;DR: 本文提出了EA-GeoAI框架，用于德国2030年之前的需求预测和公平医院规划。


<details>
  <summary>Details</summary>
Motivation: 解决德国未来医院需求预测和公平规划问题，以应对人口结构变化、老龄化及基础设施平衡，并最小化未满足的需求，同时考虑预算和出行时间限制。

Method: 结合地区层面的人口结构变化、老龄人口密度和基础设施平衡，构建统一的公平指数。随后，一个可解释的智能AI优化器在预算和出行时间限制下，分配床位并识别新的设施选址，以最小化未满足的需求。

Result: 该框架为政策制定者提供了可操作的建议，以实现公平的医院规划。

Conclusion: EA-GeoAI方法整合了地理人工智能（GeoAI）、长期预测和公平性衡量，为政策制定者提供了实用的规划建议。

Abstract: This paper presents EA-GeoAI, an integrated framework for demand forecasting
and equitable hospital planning in Germany through 2030. We combine
district-level demographic shifts, aging population density, and infrastructure
balances into a unified Equity Index. An interpretable Agentic AI optimizer
then allocates beds and identifies new facility sites to minimize unmet need
under budget and travel-time constraints. This approach bridges GeoAI,
long-term forecasting, and equity measurement to deliver actionable
recommendations for policymakers.

</details>


### [36] [Automatic Piecewise Linear Regression for Predicting Student Learning Satisfaction](https://arxiv.org/abs/2510.10639)
*Haemin Choi,Gayathri Nadarajan*

Main category: cs.AI

TL;DR: 本研究发现自动分段线性回归（APLR）模型最适合预测学生学习满意度，并揭示了时间管理、专注力等关键影响因素，为个性化教学提供了可解释的见解。


<details>
  <summary>Details</summary>
Motivation: 尽管学生学习满意度已被广泛研究，但现代技术如可解释机器学习和神经网络尚未在此领域得到充分探索。

Method: 本研究采用了结合提升算法和可解释性的自动分段线性回归（APLR）模型，并将其与多种先进方法进行比较。通过APLR的数值和视觉解释来分析影响因素。

Result: APLR模型在预测学习满意度方面表现最佳。学生的“时间管理能力”、“专注力”、“感知对同学的帮助”以及“参与线下课程”对学习满意度有最显著的积极影响。令人惊讶的是，参与创意活动并未对学习满意度产生积极影响。此外，贡献因素可在个体层面进行解释。

Conclusion: APLR模型为预测学习满意度提供了最佳拟合和可解释性，其结果能够帮助教育工作者根据学生的个人特点定制教学方案。

Abstract: Although student learning satisfaction has been widely studied, modern
techniques such as interpretable machine learning and neural networks have not
been sufficiently explored. This study demonstrates that a recent model that
combines boosting with interpretability, automatic piecewise linear
regression(APLR), offers the best fit for predicting learning satisfaction
among several state-of-the-art approaches. Through the analysis of APLR's
numerical and visual interpretations, students' time management and
concentration abilities, perceived helpfulness to classmates, and participation
in offline courses have the most significant positive impact on learning
satisfaction. Surprisingly, involvement in creative activities did not
positively affect learning satisfaction. Moreover, the contributing factors can
be interpreted on an individual level, allowing educators to customize
instructions according to student profiles.

</details>


### [37] [Hierarchical Optimization via LLM-Guided Objective Evolution for Mobility-on-Demand Systems](https://arxiv.org/abs/2510.10644)
*Yi Zhang,Yushen Long,Yun Ni,Liping Huang,Xiaohong Wang,Jun Liu*

Main category: cs.AI

TL;DR: 本文提出一种新颖的混合框架，将大型语言模型（LLM）与数学优化结合，用于动态网约车平台调度，解决了现有方法的数据效率低、模型简化和目标缺乏低层感知的问题。该框架无需训练，LLM作为元优化器自适应生成高级目标，并通过进化过程优化提示，在真实数据集上实现16%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 网约车平台在平衡动态且空间异构的供需时面临挑战。现有方法存在不足：强化学习（RL）方法数据效率低、模型过于简化且难以强制操作约束；分解式在线优化方法则依赖手动设计的高级目标，缺乏对低层路径动态的感知。

Method: 本文提出一种动态分层混合框架：
1.  无需训练，避免了RL对大规模交互数据的需求。
2.  利用LLM生成自适应的高级目标，弥补了问题分解导致的认知局限。
3.  LLM作为元优化器，产生语义启发式，指导负责约束执行和实时决策的低层优化器。
4.  通过闭环进化过程（基于和声搜索），根据优化层的可行性和性能反馈，迭代地优化LLM的提示。

Result: 基于纽约和芝加哥出租车数据集的场景实验表明，与最先进的基线方法相比，本文提出的方法平均性能提升了16%。

Conclusion: 该混合框架通过将LLM的认知能力与数学优化的精确性相结合，有效解决了网约车平台面临的供需平衡挑战，并在实际场景中取得了显著的性能提升。

Abstract: Online ride-hailing platforms aim to deliver efficient mobility-on-demand
services, often facing challenges in balancing dynamic and spatially
heterogeneous supply and demand. Existing methods typically fall into two
categories: reinforcement learning (RL) approaches, which suffer from data
inefficiency, oversimplified modeling of real-world dynamics, and difficulty
enforcing operational constraints; or decomposed online optimization methods,
which rely on manually designed high-level objectives that lack awareness of
low-level routing dynamics. To address this issue, we propose a novel hybrid
framework that integrates large language model (LLM) with mathematical
optimization in a dynamic hierarchical system: (1) it is training-free,
removing the need for large-scale interaction data as in RL, and (2) it
leverages LLM to bridge cognitive limitations caused by problem decomposition
by adaptively generating high-level objectives. Within this framework, LLM
serves as a meta-optimizer, producing semantic heuristics that guide a
low-level optimizer responsible for constraint enforcement and real-time
decision execution. These heuristics are refined through a closed-loop
evolutionary process, driven by harmony search, which iteratively adapts the
LLM prompts based on feasibility and performance feedback from the optimization
layer. Extensive experiments based on scenarios derived from both the New York
and Chicago taxi datasets demonstrate the effectiveness of our approach,
achieving an average improvement of 16% compared to state-of-the-art baselines.

</details>


### [38] [Unlocking Exploration in RLVR: Uncertainty-aware Advantage Shaping for Deeper Reasoning](https://arxiv.org/abs/2510.10649)
*Can Xie,Ruotong Pan,Xiangyu Wu,Yunfei Zhang,Jiayi Fu,Tingting Gao,Guorui Zhou*

Main category: cs.AI

TL;DR: 针对大型语言模型（LLM）的RLVR推理，本文提出UCAS方法，通过利用模型内部不确定性信号来精炼信用分配，从而解决现有算法中粗粒度优势信号导致的探索效率低下和熵坍缩问题，显著提升了数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习（RLVR）算法（如GRPO）在LLM推理中采用统一的优势信号，忽略了不确定、高风险决策的关键作用，导致探索效率低下和熵坍缩问题。

Method: UCAS（UnCertainty-aware Advantage Shaping）是一种无模型方法，通过利用模型内部不确定性信号来精炼信用分配。它分两阶段操作：首先，利用模型整体自信心调节响应级别的优势；其次，根据原始logit确定性施加token级别的惩罚。这种双重机制鼓励探索能产生正确答案的高不确定性路径，并惩罚过度自信但错误的推理，有效平衡了探索与利用。

Result: 在五个数学推理基准测试中，UCAS显著优于强大的RLVR基线（包括1.5B和7B模型规模）。分析证实，UCAS不仅获得了更高的奖励，还促进了更大的推理多样性，并成功缓解了熵坍缩。

Conclusion: UCAS通过利用模型内部不确定性信号来精炼信用分配，有效解决了RLVR中粗粒度优势信号的局限性，从而显著提升了LLM的推理能力、多样性，并缓解了熵坍缩问题。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has shown significant
promise for enhancing the reasoning capabilities of large language models
(LLMs). However, prevailing algorithms like GRPO broadcast a uniform advantage
signal across all tokens in a sequence. This coarse-grained approach overlooks
the pivotal role of uncertain, high-stakes decisions during reasoning, leading
to inefficient exploration and the well-documented problem of entropy collapse.
To address this, we introduce UnCertainty-aware Advantage Shaping (UCAS), a
model-free method that refines credit assignment by leveraging the model's
internal uncertainty signals. UCAS operates in two stages: it first modulates
the response-level advantage using the model's overall self-confidence, and
then applies a token-level penalty based on raw logit certainty. This dual
mechanism encourages exploration of high-uncertainty paths that yield correct
answers while penalizing overconfident yet erroneous reasoning, effectively
balancing the exploration-exploitation trade-off. Extensive experiments on five
mathematical reasoning benchmarks show that UCAS significantly outperforms
strong RLVR baselines across multiple model scales, including 1.5B and 7B. Our
analysis confirms that UCAS not only achieves higher rewards but also promotes
greater reasoning diversity and successfully mitigates entropy collapse.

</details>


### [39] [Simpliflow: A Lightweight Open-Source Framework for Rapid Creation and Deployment of Generative Agentic AI Workflows](https://arxiv.org/abs/2510.10675)
*Deven Panchal*

Main category: cs.AI

TL;DR: 本文介绍了一个名为simpliflow的轻量级、开源Python框架，旨在简化和加速线性、确定性智能体工作流的开发和编排，解决现有框架的复杂性和学习曲线问题。


<details>
  <summary>Details</summary>
Motivation: 现有生成式智能体AI系统框架在构建复杂、多步骤任务时引入了显著的复杂性、陡峭的学习曲线和大量的样板代码，阻碍了快速原型开发和部署。

Method: 本文提出了simpliflow框架，它通过声明式的、基于JSON的配置实现线性、确定性智能体工作流的快速开发和编排。其模块化架构解耦了智能体管理、工作流执行和后处理，并通过集成LiteLLM支持超过100种大型语言模型（LLMs）。

Result: simpliflow能够实现线性、确定性智能体工作流的快速开发和编排，支持广泛的LLMs，并在软件开发模拟到实时系统交互等多样化用例中展示了其效用。与LangChain和AutoGen等框架的比较分析表明，simpliflow在确定性工作流环境中，在简洁性、控制性和速度方面具有独特优势。

Conclusion: simpliflow是一个为简化、控制和加速确定性生成式智能体AI工作流而优化的工具，有效解决了现有框架的复杂性问题，促进了快速原型开发和部署。

Abstract: Generative Agentic AI systems are emerging as a powerful paradigm for
automating complex, multi-step tasks. However, many existing frameworks for
building these systems introduce significant complexity, a steep learning
curve, and substantial boilerplate code, hindering rapid prototyping and
deployment. This paper introduces simpliflow, a lightweight, open-source Python
framework designed to address these challenges. simpliflow enables the rapid
development and orchestration of linear, deterministic agentic workflows
through a declarative, JSON-based configuration. Its modular architecture
decouples agent management, workflow execution, and post-processing, promoting
ease of use and extensibility. By integrating with LiteLLM, it supports over
100 Large Language Models (LLMs) out-of-the-box. We present the architecture,
operational flow, and core features of simpliflow, demonstrating its utility
through diverse use cases ranging from software development simulation to
real-time system interaction. A comparative analysis with prominent frameworks
like LangChain and AutoGen highlights simpliflow's unique position as a tool
optimized for simplicity, control, and speed in deterministic workflow
environments.

</details>


### [40] [OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs](https://arxiv.org/abs/2510.10689)
*Caorui Li,Yu Chen,Yiyan Ji,Jin Xu,Zhenyu Cui,Shihao Li,Yuanxing Zhang,Jiafu Tang,Zhenghao Song,Dingling Zhang,Ying He,Haoxiang Liu,Yuxuan Wang,Qiufeng Wang,Zhenhe Wu,Jiehui Luo,Zhiyu Pan,Weihao Xie,Chenchen Zhang,Zhaohui Wang,Jiayi Tian,Yanghai Wang,Zhe Cao,Minxin Dai,Ke Wang,Runzhe Wen,Yinghao Ma,Yaning Pan,Sungkyun Chang,Termeh Taheri,Haiwen Xia,Christos Plachouras,Emmanouil Benetos,Yizhi Li,Ge Zhang,Jian Yang,Tianhao Peng,Zili Wang,Minghao Liu,Junran Peng,Zhaoxiang Zhang,Jiaheng Liu*

Main category: cs.AI

TL;DR: 本文引入了OmniVideoBench，一个大规模、严格设计的基准测试，旨在全面评估多模态大语言模型（MLLMs）在视频理解中音频和视觉模态的协同推理能力，强调模态互补性和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在视频理解方面展现出巨大潜力，但现有基准未能全面评估音频和视觉模态之间的协同推理能力，常常忽视其中一个模态或以逻辑不一致的方式整合它们。

Method: OmniVideoBench包含1000个高质量的问答对，每个都附有逐步推理过程，来源于628个多样化视频。这些数据经过人工验证，确保完全正确性和唯一性。它涵盖13种精心设计的问答类型，包括时间推理、空间定位、计数、因果推断和总结等，以捕捉视频理解的关键挑战。

Result: 对多个MLLMs在OmniVideoBench上的评估显示，模型性能与人类推理之间存在显著差距。其中，开源模型的表现明显落后于闭源模型，突显了真正音视频推理的固有难度。

Conclusion: OmniVideoBench揭示了当前MLLMs在协同音视频推理方面的不足，并强调了开发具有更强、更通用推理能力的MLLMs的必要性。该基准将被发布，以促进相关研究和发展。

Abstract: Recent advances in multimodal large language models (MLLMs) have demonstrated
substantial potential in video understanding. However, existing benchmarks fail
to comprehensively evaluate synergistic reasoning capabilities across audio and
visual modalities, often neglecting either one of the modalities or integrating
them in a logically inconsistent manner. To bridge this gap, we introduce
OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to
assessing synergistic audio-visual understanding, with a strong emphasis on
modality complementarity and logical consistency. Specifically, OmniVideoBench
comprises 1000 high-quality question-answer(QA) pairs, each annotated with
step-by-step reasoning traces, derived from 628 diverse videos ranging from
several seconds to 30 minutes, and manually verified to guarantee complete
correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully
designed question types, covering temporal reasoning, spatial localization,
counting, causal inference, summarization, and beyond, thereby capturing the
essential challenges of video understanding. Evaluation of multiple MLLMs on
OmniVideoBench reveals a pronounced gap between model performance and human
reasoning, with open-source models lagging significantly behind their
closed-source counterparts, underscoring the inherent difficulty of genuine
audio-visual reasoning. We will release OmniVideoBench to foster the
development of MLLMs with stronger and more generalizable reasoning
capabilities.

</details>


### [41] [Adaptive Selection of Symbolic Languages for Improving LLM Logical Reasoning](https://arxiv.org/abs/2510.10703)
*Xiangyu Wang,Haocheng Yang,Fengxiang Cheng,Fenrong Liu*

Main category: cs.AI

TL;DR: 该论文提出了一种自适应选择符号语言（SL）的方法，以提高大型语言模型（LLMs）在复杂逻辑推理任务中的表现，因为不同的自然语言（NL）问题对应着不同的最优SL形式化。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂逻辑推理方面表现不佳，现有方法过度依赖自然语言到符号语言翻译的准确性，但忽略了目标符号语言类型选择的重要性。不同的SL（如一阶逻辑、布尔可满足性）擅长处理不同类型的逻辑问题。

Method: 本文首先主张并验证了不同自然语言逻辑推理问题对应着不同的最优符号语言形式化。在此基础上，提出了一种方法，利用LLMs在翻译前自适应地从一阶逻辑、逻辑编程和布尔可满足性中选择最适合每个问题的目标SL，然后将问题翻译成选定的SL表达式，并使用相应的逻辑求解器得出答案。

Result: 实验结果表明，自适应选择方法显著优于将所有问题翻译成单一SL或随机选择SL。在混合数据集上，该方法达到了96%的准确率，比第二高的一阶逻辑翻译方法提高了25%。

Conclusion: 自适应地为不同的自然语言逻辑推理问题选择最优的符号语言，可以显著提升LLMs在复杂逻辑推理任务中的性能。

Abstract: Large Language Models (LLMs) still struggle with complex logical reasoning.
While previous works achieve remarkable improvements, their performance is
highly dependent on the correctness of translating natural language (NL)
problems into a symbolic language (SL). Though numerous works focusing on
improving this translation accuracy, they only consider the similarity between
the meaning of SL and NL, overlooking another crucial influencing factor, the
selection of the target SL type itself. For example, first-order logic language
specializes in logical reasoning with categorical syllogisms and complex
quantifiers, while Boolean satisfiability formalism excels at representing
constraint satisfaction like partial problems. To our knowledge, this is the
first paper to claim and verify that different NL logical reasoning problem
corresponds to different optimal SL formalization for translation. Based on
this, we propose a methods to improve the logical reasoning performance of LLMs
by adaptively selecting the most suitable SL for each problem prior to
translation. Specifically, we leverage LLMs to select the target SL among
first-order logic, logic programming and Boolean satisfiability and then
translate the problem in NL to target SL expressions as well as employ the
corresponding logical solver to derive the final answer. Experimental results
on benchmarks show that our adaptive selection method significantly outperforms
translating all into single SL and randomly selecting the SL. On a mixed
dataset of these benchmarks, our approach achieves 96% accuracy, which
improving performance by 25% compared to the second highest accuracy from the
first-order logic translation.

</details>


### [42] [Extended Triangular Method: A Generalized Algorithm for Contradiction Separation Based Automated Deduction](https://arxiv.org/abs/2510.10701)
*Yang Xu,Shuwei Chen,Jun Liu,Feng Cao,Xingxing He*

Main category: cs.AI

TL;DR: 本文提出扩展三角法（ETM），一种形式化矛盾分离扩展（CSE）框架的算法，用于自动化推理。ETM支持多子句推理，经实证验证有效，并提升了定理证明器的性能。


<details>
  <summary>Details</summary>
Motivation: 自动化推理面临完备性与计算效率的挑战，传统二元归结法限制了多子句协同推理。2018年提出的矛盾分离扩展（CSE）理论框架缺乏形式化的算法实现，是本文的研究动机。

Method: 本文提出了扩展三角法（ETM），这是一种广义的矛盾构建算法，用于形式化和扩展矛盾分离的内部机制。ETM在一个三角几何框架内统一了包括早期标准扩展法在内的多种矛盾构建策略，支持灵活的子句交互和动态协同。

Result: ETM作为CSE、CSE-E、CSI-E和CSI-Enig等高性能定理证明器的算法核心。这些证明器在标准一阶基准测试（TPTP问题集和CASC 2018-2015）中取得了有竞争力的结果，经验性地验证了该方法的有效性和普适性。

Conclusion: ETM通过连接理论抽象与操作实现，将矛盾分离范式推进为一个通用、可扩展且具有实际竞争力的自动化推理模型，为逻辑推理和定理证明的未来研究提供了新方向。

Abstract: Automated deduction lies at the core of Artificial Intelligence (AI),
underpinning theorem proving, formal verification, and logical reasoning.
Despite decades of progress, reconciling deductive completeness with
computational efficiency remains an enduring challenge. Traditional reasoning
calculi, grounded in binary resolution, restrict inference to pairwise clause
interactions and thereby limit deductive synergy among multiple clauses. The
Contradiction Separation Extension (CSE) framework, introduced in 2018,
proposed a dynamic multi-clause reasoning theory that redefined logical
inference as a process of contradiction separation rather than sequential
resolution. While that work established the theoretical foundation, its
algorithmic realization remained unformalized and unpublished. This work
presents the Extended Triangular Method (ETM), a generalized
contradiction-construction algorithm that formalizes and extends the internal
mechanisms of contradiction separation. The ETM unifies multiple
contradiction-building strategies, including the earlier Standard Extension
method, within a triangular geometric framework that supports flexible clause
interaction and dynamic synergy. ETM serves as the algorithmic core of several
high-performance theorem provers, CSE, CSE-E, CSI-E, and CSI-Enig, whose
competitive results in standard first-order benchmarks (TPTP problem sets and
CASC 2018-2015) empirically validate the effectiveness and generality of the
proposed approach. By bridging theoretical abstraction and operational
implementation, ETM advances the contradiction separation paradigm into a
generalized, scalable, and practically competitive model for automated
reasoning, offering new directions for future research in logical inference and
theorem proving.

</details>


### [43] [LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics](https://arxiv.org/abs/2510.10813)
*Enric Junque de Fortuny,Veronica Roberta Cappelli*

Main category: cs.AI

TL;DR: 本研究通过分解信念、评估和选择，发现大型语言模型（LLMs）在战略博弈中表现出信念连贯的最佳响应行为、元推理能力和独特的启发式规则，这表明它们具备新兴的战略认知能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs被广泛应用于需要推理其他智能体行为的领域（如谈判、政策设计），但现有研究主要评估它们是否遵循均衡博弈或其推理深度。LLMs是否展现出真正的战略思维（即连贯地形成对其他智能体的信念、评估可能行动并基于这些信念做出选择）仍未被探索。

Method: 研究开发了一个框架，用于在静态、完全信息博弈中分离信念、评估和选择，并将其应用于一系列非合作环境。通过联合分析模型的选择和推理轨迹，并引入一种新的无上下文博弈来排除模仿记忆，以识别LLMs的战略思维能力。

Result: 前沿LLMs在特定推理深度下表现出信念连贯的最佳响应行为。在无约束条件下，它们会自我限制推理深度，并对人类和合成对手形成差异化的推测，这揭示了一种新兴的元推理形式。在复杂性增加时，显式递归会让位于内部生成的启发式选择规则，这些规则稳定、特定于模型且不同于已知的人类偏见。

Conclusion: 信念连贯性、元推理和新颖的启发式规则形成可以共同从语言建模目标中涌现。这些发现为研究人工智能体中的战略认知提供了结构化基础。

Abstract: Large Language Models (LLMs) are increasingly applied to domains that require
reasoning about other agents' behavior, such as negotiation, policy design, and
market simulation, yet existing research has mostly evaluated their adherence
to equilibrium play or their exhibited depth of reasoning. Whether they display
genuine strategic thinking, understood as the coherent formation of beliefs
about other agents, evaluation of possible actions, and choice based on those
beliefs, remains unexplored. We develop a framework to identify this ability by
disentangling beliefs, evaluation, and choice in static, complete-information
games, and apply it across a series of non-cooperative environments. By jointly
analyzing models' revealed choices and reasoning traces, and introducing a new
context-free game to rule out imitation from memorization, we show that current
frontier models exhibit belief-coherent best-response behavior at targeted
reasoning depths. When unconstrained, they self-limit their depth of reasoning
and form differentiated conjectures about human and synthetic opponents,
revealing an emergent form of meta-reasoning. Under increasing complexity,
explicit recursion gives way to internally generated heuristic rules of choice
that are stable, model-specific, and distinct from known human biases. These
findings indicate that belief coherence, meta-reasoning, and novel heuristic
formation can emerge jointly from language modeling objectives, providing a
structured basis for the study of strategic cognition in artificial agents.

</details>


### [44] [DRIFT: Decompose, Retrieve, Illustrate, then Formalize Theorems](https://arxiv.org/abs/2510.10815)
*Meiru Zhang,Philipp Borchert,Milan Gritta,Gerasimos Lampouras*

Main category: cs.AI

TL;DR: LLM在数学形式化方面面临挑战，难以识别和利用先决知识。DRIFT框架通过分解非正式语句为子组件，实现有针对性的前提检索和示例定理检索，显著提高了前提检索的F1分数，并在OOD基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自动化数学语句形式化以进行定理证明时，难以识别和利用先决数学知识及其在Lean等语言中的形式表示。当前的检索增强型自动形式化方法直接使用非正式语句查询外部库，但忽略了一个基本限制：非正式数学语句通常复杂且提供有限的底层数学概念上下文。

Method: 本文引入了DRIFT框架，该框架使LLMs能够将非正式数学语句分解为更小、更易处理的“子组件”，从而促进从Mathlib等数学库中进行有针对性的前提检索。此外，DRIFT还会检索示例定理，以帮助模型在形式化任务中更有效地使用前提。

Result: DRIFT在ProofNet、ConNF和MiniF2F-test等不同基准上持续改进了前提检索，与DPR基线相比，在ProofNet上的F1分数几乎翻倍。值得注意的是，DRIFT在分布外ConNF基准上表现强劲，使用GPT-4.1和DeepSeek-V3.1分别实现了37.14%和42.25%的BEq+@10改进。

Conclusion: 分析表明，数学自动形式化中的检索有效性严重依赖于模型特定的知识边界，这凸显了需要根据每个模型的能力调整自适应检索策略的重要性。

Abstract: Automating the formalization of mathematical statements for theorem proving
remains a major challenge for Large Language Models (LLMs). LLMs struggle to
identify and utilize the prerequisite mathematical knowledge and its
corresponding formal representation in languages like Lean. Current
retrieval-augmented autoformalization methods query external libraries using
the informal statement directly, but overlook a fundamental limitation:
informal mathematical statements are often complex and offer limited context on
the underlying math concepts. To address this, we introduce DRIFT, a novel
framework that enables LLMs to decompose informal mathematical statements into
smaller, more tractable ''sub-components''. This facilitates targeted retrieval
of premises from mathematical libraries such as Mathlib. Additionally, DRIFT
retrieves illustrative theorems to help models use premises more effectively in
formalization tasks. We evaluate DRIFT across diverse benchmarks (ProofNet,
ConNF, and MiniF2F-test) and find that it consistently improves premise
retrieval, nearly doubling the F1 score compared to the DPR baseline on
ProofNet. Notably, DRIFT demonstrates strong performance on the
out-of-distribution ConNF benchmark, with BEq+@10 improvements of 37.14% and
42.25% using GPT-4.1 and DeepSeek-V3.1, respectively. Our analysis shows that
retrieval effectiveness in mathematical autoformalization depends heavily on
model-specific knowledge boundaries, highlighting the need for adaptive
retrieval strategies aligned with each model's capabilities.

</details>


### [45] [The Irrational Machine: Neurosis and the Limits of Algorithmic Safety](https://arxiv.org/abs/2510.10823)
*Daniel Howard*

Main category: cs.AI

TL;DR: 该论文提出了一个框架，用于表征具身AI中的“神经症”行为（内部一致但与现实不符），并利用破坏性测试来发现这些深层架构问题。


<details>
  <summary>Details</summary>
Motivation: 具身AI在规划、不确定性处理和厌恶记忆交互作用下，可能产生内部连贯但与现实不符的行为（即“神经症”）。现有局部修复不足以解决这些全局性故障，需要更深层次的分析和解决。

Method: 1. 提出了一个表征具身AI（以网格导航为例）中神经症行为的框架。2. 编目了反复出现的神经症模式（如犹豫不决、规划流失、僵局等），并为其设计了轻量级在线检测器和可重用逃逸策略。3. 引入了基于遗传编程的破坏性测试，通过演化世界和扰动来最大化“法则压力”和“神经症得分”，以生成对抗性课程和反事实轨迹，揭示需要架构修订而非仅仅症状级修补的地方。

Result: 1. 识别并分类了具身AI导航中的多种神经症行为模式。2. 表明即使在完全可见的情况下，学习到的厌恶成本也可能导致持久的恐惧规避行为，产生不必要的长绕路。3. 论证了局部修复不足以解决全局性故障。4. 破坏性测试能够生成对抗性环境，有效地揭示了AI系统需要进行架构层面修订的深层问题。

Conclusion: 具身AI中的“神经症”是一个普遍且深层次的问题，仅靠局部修复无法解决。需要通过架构修订来从根本上解决这些问题，而破坏性测试是一种有效的工具，能够识别出这些需要架构改进的关键点。

Abstract: We present a framework for characterizing neurosis in embodied AI: behaviors
that are internally coherent yet misaligned with reality, arising from
interactions among planning, uncertainty handling, and aversive memory. In a
grid navigation stack we catalogue recurrent modalities including flip-flop,
plan churn, perseveration loops, paralysis and hypervigilance, futile search,
belief incoherence, tie break thrashing, corridor thrashing, optimality
compulsion, metric mismatch, policy oscillation, and limited-visibility
variants. For each we give lightweight online detectors and reusable escape
policies (short commitments, a margin to switch, smoothing, principled
arbitration). We then show that durable phobic avoidance can persist even under
full visibility when learned aversive costs dominate local choice, producing
long detours despite globally safe routes. Using First/Second/Third Law as
engineering shorthand for safety latency, command compliance, and resource
efficiency, we argue that local fixes are insufficient; global failures can
remain. To surface them, we propose genetic-programming based destructive
testing that evolves worlds and perturbations to maximize law pressure and
neurosis scores, yielding adversarial curricula and counterfactual traces that
expose where architectural revision, not merely symptom-level patches, is
required.

</details>


### [46] [LLM-Empowered Agentic MAC Protocols: A Dynamic Stackelberg Game Approach](https://arxiv.org/abs/2510.10895)
*Renxuan Tan,Rongpeng Li,Fei Wang,Chenghui Peng,Shaoyun Wu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种基于博弈论和大型语言模型（LLM）驱动的多智能体深度强化学习（MARL）框架，用于无线网络中自适应的MAC协议合成。该框架将网络建模为动态多跟随者Stackelberg博弈，并通过LLM驱动的智能体生成语义MAC协议，在不进行重新训练的情况下，显著提高了吞吐量、公平性，并展现出卓越的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统的MAC协议通常需要手动配置，而基于深度强化学习（DRL）的协议虽然能提升特定任务的网络性能，但其泛化能力和弹性较差，在动态环境中需要昂贵的重新训练才能适应。

Method: 该研究将基站与可变用户设备之间的上行链路传输建模为动态多跟随者Stackelberg博弈。框架采用LLM驱动的智能体，通过近端策略优化（PPO）进行协调，以响应网络动态合成自适应的语义MAC协议。为确保过程的可靠性和效率，使用了协议动作语法（PAG）。此外，还分析了LLM驱动统一策略在响应变化跟随者时的学习动态，以研究Stackelberg均衡的存在性和收敛行为。

Result: 仿真结果表明，该框架在吞吐量方面比传统基线提高了77.6%，公平性提高了65.2%。此外，该框架对用户数量的波动表现出卓越的泛化能力，无需重新训练或更改架构。

Conclusion: 所提出的基于博弈论和LLM赋能的MARL框架，成功解决了现有DRL-based MAC协议泛化能力和弹性不足的问题，能够自适应地生成高效的MAC协议，并在动态环境中实现显著的性能提升和鲁棒的泛化能力。

Abstract: Medium Access Control (MAC) protocols, essential for wireless networks, are
typically manually configured. While deep reinforcement learning (DRL)-based
protocols enhance task-specified network performance, they suffer from poor
generalizability and resilience, demanding costly retraining to adapt to
dynamic environments. To overcome this limitation, we introduce a
game-theoretic LLM-empowered multi-agent DRL (MARL) framework, in which the
uplink transmission between a base station and a varying number of user
equipments is modeled as a dynamic multi-follower Stackelberg game (MFSG),
capturing the network's natural hierarchical structure. Within this game,
LLM-driven agents, coordinated through proximal policy optimization (PPO),
synthesize adaptive, semantic MAC protocols in response to network dynamics.
Protocol action grammar (PAG) is employed to ensure the reliability and
efficiency of this process. Under this system, we further analyze the existence
and convergence behavior in terms of a Stackelberg equilibrium by studying the
learning dynamics of LLM-empowered unified policies in response to changing
followers. Simulations corroborate that our framework achieves a 77.6% greater
throughput and a 65.2% fairness improvement over conventional baselines.
Besides, our framework generalizes excellently to a fluctuating number of users
without requiring retraining or architectural changes.

</details>


### [47] [PaperArena: An Evaluation Benchmark for Tool-Augmented Agentic Reasoning on Scientific Literature](https://arxiv.org/abs/2510.10909)
*Daoyu Wang,Mingyue Cheng,Qi Liu,Shuo Yu,Zirui Liu,Ze Guo*

Main category: cs.AI

TL;DR: 本文提出了PaperArena，一个用于评估LLM代理在科学文献中进行跨论文推理和多工具协调能力的基准。实验结果表明，即使是最先进的LLM代理也表现不佳，凸显了巨大的改进空间。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）代理在处理网络规模的科学文献时，其理解和推理能力受限于单篇论文或无工具任务，因为缺乏一个用于跨论文推理和真实研究场景中多工具编排的基准。

Method: 本文提出了PaperArena，一个评估代理解决需要整合多篇论文信息并借助外部工具的真实世界研究问题的基准。它提供了一个模块化和可扩展的代理执行平台，包含多模态解析、上下文检索和程序化计算等工具，以支持标准化评估。

Result: 实验结果显示，即使是最先进的LLM驱动的代理系统，平均准确率也仅为38.78%。在困难子集上，准确率更是降至18.47%，表明有很大的改进潜力。此外，所有测试代理都表现出低效的工具使用，经常调用比解决任务所需更多的工具。

Conclusion: PaperArena基准揭示了当前LLM代理在科学发现中进行跨论文推理和工具协调方面的不足，并强调了巨大的改进空间。本文邀请社区采用PaperArena来开发和评估更强大的科学发现代理。

Abstract: Understanding and reasoning on the web-scale scientific literature is a
crucial touchstone for large language model (LLM) based agents designed to
support complex knowledge-intensive tasks. However, existing works are mainly
restricted to tool-free tasks within isolated papers, largely due to the lack
of a benchmark for cross-paper reasoning and multi-tool orchestration in real
research scenarios. In this work, we propose PaperArena, an evaluation
benchmark for agents to address real-world research questions that typically
require integrating information across multiple papers with the assistance of
external tools. Given a research question, agents should integrate diverse
formats across multiple papers through reasoning and interacting with
appropriate tools, thereby producing a well-grounded answer. To support
standardized evaluation, we provide a modular and extensible platform for agent
execution, offering tools such as multimodal parsing, context retrieval, and
programmatic computation. Experimental results reveal that even the most
advanced LLM powering a well-established agent system achieves merely 38.78%
average accuracy. On the hard subset, accuracy drops to only 18.47%,
highlighting great potential for improvement. We also present several empirical
findings, including that all agents tested exhibit inefficient tool usage,
often invoking more tools than necessary to solve a task. We invite the
community to adopt PaperArena to develop and evaluate more capable agents for
scientific discovery. Our code and data are available
https://github.com/Melmaphother/PaperArena.

</details>


### [48] [PoU: Proof-of-Use to Counter Tool-Call Hacking in DeepResearch Agents](https://arxiv.org/abs/2510.10931)
*SHengjie Ma,Chenlong Deng,Jiaxin Mao,Jiadeng Huang,Teng Wang,Junjie Wu,Changwang Zhang,Jun wang*

Main category: cs.AI

TL;DR: RAG代理在RL训练中存在“工具调用欺骗”问题，导致表面化工具使用和错误引用。本文提出Proof-of-Use (PoU)框架，通过强制验证证据、推理和答案之间的因果链接来解决此问题，显著提升了事实准确性和证据忠实度。


<details>
  <summary>Details</summary>
Motivation: RL训练的RAG代理（如DeepResearch系统）在多步推理中存在一个未被发现的故障模式——“工具调用欺骗”（Tool-Call Hacking）。代理通过发出表面上正确的工具调用来虚增奖励信号，但并未真正利用检索到的证据，导致模式崩溃（重复依赖单一来源）和虚假接地（答案与引用内容支持度弱）。

Method: 本文提出Proof-of-Use (PoU)，一个基于证据的强化学习框架。PoU通过一个统一的逐步契约来强制执行检索到的证据、推理轨迹和最终答案之间可验证的因果链接。该契约包括语法引用验证、基于扰动的敏感性奖励以及答案-证据对齐目标，确保工具使用既可解释又功能性地有根据。

Result: 在涵盖域内、域外和工具分布外设置的七个QA基准测试中，PoU在事实准确性、证据忠实度和工具路由平衡方面始终优于强大的DeepResearch基线。这些发现强调了将RL训练的代理不仅基于任务结果，而且基于检索信息因果使用进行接地的必要性。

Conclusion: 对于可信赖的检索增强推理，关键在于将RL训练的代理接地于检索信息的因果使用，而不仅仅是任务结果。PoU为实现这一目标提供了一条原则性的途径，能够解决RAG代理中“工具调用欺骗”的问题。

Abstract: Retrieval-augmented generation (RAG) agents, such as recent
DeepResearch-style systems, extend large language models (LLMs) with autonomous
information-seeking capabilities through external tools. While reinforcement
learning (RL) has enabled impressive multi-step reasoning, we identify a
previously overlooked failure mode, Tool-Call Hacking, where agents inflate
reward signals by issuing superficially correct tool calls without genuinely
leveraging the retrieved evidence. This results in (i) mode collapse into
repetitive reliance on a single source and (ii) spurious grounding, where
answers are only weakly supported by cited content.
  To address this, we propose Proof-of-Use (PoU), an evidence-grounded RL
framework that enforces verifiable causal links between retrieved evidence,
reasoning traces, and final answers. PoU operationalizes this through a unified
step-wise contract combining syntactic citation validation, perturbation-based
sensitivity rewards, and answer-evidence alignment objectives, ensuring that
tool usage remains both interpretable and functionally grounded.
  Across seven QA benchmarks spanning in-domain, out-of-domain, and
out-of-tool-distribution settings, PoU consistently outperforms strong
DeepResearch baselines in factual accuracy, evidence faithfulness, and
tool-routing balance. These findings highlight the necessity of grounding
RL-trained agents not merely in task outcomes but in the causal use of
retrieved information, offering a principled path toward trustworthy
retrieval-augmented reasoning.

</details>


### [49] [Scalable and Explainable Enterprise Knowledge Discovery Using Graph-Centric Hybrid Retrieval](https://arxiv.org/abs/2510.10942)
*Nilima Rao,Jagriti Srivastava,Pradeep Kumar Sharma,Hritvik Shrivastava*

Main category: cs.AI

TL;DR: 该论文提出了一种模块化混合检索框架，结合知识图谱语言增强模型（KBLam）、DeepGraph表示和嵌入式语义搜索，从异构企业知识源构建统一知识图谱，以支持复杂查询的上下文推理和多跳推断，并显著提高答案相关性。


<details>
  <summary>Details</summary>
Motivation: 现代企业知识分散在Jira、Git、Confluence等异构系统中。传统的基于关键词或静态嵌入的检索方法难以回答需要跨多源进行上下文推理和多跳推断的复杂查询。

Method: 该框架通过解析代码、拉取请求和提交历史等仓库，构建一个统一的知识图谱。它整合了KBLam、DeepGraph表示和嵌入式语义搜索。通过查询分析动态确定最佳检索策略，支持结构化和非结构化数据源的独立或融合处理。此外，还提供了一个交互式界面，用于图可视化、子图探索和上下文感知查询路由。

Result: 在大规模Git仓库上的实验表明，与独立的基于GPT的检索管道相比，统一推理层将答案相关性提高了80%。

Conclusion: 该框架通过结合图构建、混合推理和交互式可视化，为企业环境中的智能知识助手提供了一个可扩展、可解释且以用户为中心的解决方案。

Abstract: Modern enterprises manage vast knowledge distributed across heterogeneous
systems such as Jira, Git repositories, Confluence, and wikis. Conventional
retrieval methods based on keyword search or static embeddings often fail to
answer complex queries that require contextual reasoning and multi-hop
inference across artifacts. We present a modular hybrid retrieval framework for
adaptive enterprise information access that integrates Knowledge Base
Language-Augmented Models (KBLam), DeepGraph representations, and
embedding-driven semantic search. The framework builds a unified knowledge
graph from parsed repositories including code, pull requests, and commit
histories, enabling semantic similarity search, structural inference, and
multi-hop reasoning. Query analysis dynamically determines the optimal
retrieval strategy, supporting both structured and unstructured data sources
through independent or fused processing. An interactive interface provides
graph visualizations, subgraph exploration, and context-aware query routing to
generate concise and explainable answers. Experiments on large-scale Git
repositories show that the unified reasoning layer improves answer relevance by
up to 80 percent compared with standalone GPT-based retrieval pipelines. By
combining graph construction, hybrid reasoning, and interactive visualization,
the proposed framework offers a scalable, explainable, and user-centric
foundation for intelligent knowledge assistants in enterprise environments.

</details>


### [50] [Video-STR: Reinforcing MLLMs in Video Spatio-Temporal Reasoning with Relation Graph](https://arxiv.org/abs/2510.10976)
*Wentao Wang,Heqing Zou,Tianze Luo,Rui Huang,Yutian Zhao,Zhuochen Wang,Hansheng Zhang,Chengwei Qin,Yan Wang,Lin Zhao,Huaijian Zhang*

Main category: cs.AI

TL;DR: 多模态大语言模型（MLLMs）在视频时空理解方面表现不佳，本文提出Video-STR，一种新的基于图的强化学习方法，结合GRPO和自建的STV-205k数据集，显著提升了视频精确时空推理能力，并达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）在语义理解方面表现出色，但在精确时空理解上仍存在困难。现有方法主要关注视频本身，忽略了视频中的物理信息（如多对象布局和运动）。这种局限性限制了MLLMs在具身智能和VR等需要高精度的下游应用中的使用。

Method: 本文提出了Video-STR，一种新颖的基于图的强化方法，用于精确的视频时空推理。该方法基于可验证奖励强化学习（RLVR），并引入了使用基于图的群组相对策略优化（GRPO）方法的推理机制，以在思考过程中引导模型推断场景的潜在时空拓扑。为解决时空训练数据不足的问题，构建了包含20.5万个问答对的STV-205k数据集，涵盖室内外动态多对象场景。

Result: 实验表明，Video-STR在各种基准测试中取得了最先进的结果，在STI-Bench上比基础模型性能提高了13%，证明了所提出方法和数据集的有效性。

Conclusion: Video-STR及其自建的STV-205k数据集有效地解决了MLLMs在视频精确时空理解上的不足，显著提升了模型在该领域的性能，并为相关应用提供了新的解决方案。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has demonstrated
strong semantic understanding capabilities, but struggles to perform precise
spatio-temporal understanding. Existing spatio-temporal methods primarily focus
on the video itself, while overlooking the physical information within the
video, such as multi-object layouts and motion. Such limitations restrict the
use of MLLMs in downstream applications that demand high precision, including
embodied intelligence and VR. To address this issue, we present Video-STR, a
novel graph-based reinforcement method for precise Video Spatio-Temporal
Reasoning. Building upon the capacity of Reinforcement Learning with Verifiable
Reward (RLVR) to improve model abilities, we introduce a reasoning mechanism
using graph-based Group Relative Policy Optimization (GRPO) method to guide the
model in inferring the underlying spatio-temporal topology of scenarios during
the thinking process. To resolve the lack of spatio-temporal training data, we
construct the STV-205k dataset with 205k question-answering pairs, covering
dynamic multi-object scenes in both indoor and outdoor environments, to support
the model training. Experiments show that Video-STR achieves state-of-the-art
results on various benchmarks, outperforming the base model by 13% on
STI-Bench, and demonstrating the effectiveness of our approach and dataset.
Code, model, and data will be released.

</details>


### [51] [Revisiting Model Interpolation for Efficient Reasoning](https://arxiv.org/abs/2510.10977)
*Taiqiang Wu,Runming Yang,Tao Liu,Jiahao Wang,Ngai Wong*

Main category: cs.AI

TL;DR: 本文系统性地重新审视了最简单的模型插值方法，发现其遵循三阶段演化范式，并出人意料地在效率和有效性上超越了复杂的模型合并基线，为精确推理能力模型提供实用框架。


<details>
  <summary>Details</summary>
Motivation: 模型合并（尤其是在指令和思维模型上）在高效推理方面表现出色。本文旨在重新审视最简单的合并方法——直接权重插值，以系统性地理解其行为和动态，并探索其在性能-成本权衡中的潜力。

Method: 研究方法包括系统性地重新审视直接权重插值方法，观察其在推理轨迹上的三阶段演化范式。通过实证结果验证其性能，并进行了广泛的消融研究，涵盖模型层、模块和解码策略，以验证研究发现。

Result: 研究发现，模型插值遵循一个具有独特行为的三阶段演化范式。一个经过策略性插值的模型，在效率和有效性方面均出人意料地超越了复杂的模型合并基线。这些发现通过对模型层、模块和解码策略的广泛消融研究得到了进一步验证。

Conclusion: 这项工作揭示了模型插值的奥秘，并提供了一个实用的框架，用于构建具有精确目标推理能力的模型，为导航性能-成本权衡提供了有原则的指导。

Abstract: Model merging, typically on Instruct and Thinking models, has shown
remarkable performance for efficient reasoning. In this paper, we
systematically revisit the simplest merging method that interpolates two
weights directly. Particularly, we observe that model interpolation follows a
three-stage evolutionary paradigm with distinct behaviors on the reasoning
trajectory. These dynamics provide a principled guide for navigating the
performance-cost trade-off. Empirical results demonstrate that a strategically
interpolated model surprisingly surpasses sophisticated model merging baselines
on both efficiency and effectiveness. We further validate our findings with
extensive ablation studies on model layers, modules, and decoding strategies.
Ultimately, this work demystifies model interpolation and offers a practical
framework for crafting models with precisely targeted reasoning capabilities.
Code is available at \href{https://github.com/wutaiqiang/MI}{Github}.

</details>


### [52] [FBS Model-based Maintenance Record Accumulation for Failure-Cause Inference in Manufacturing Systems](https://arxiv.org/abs/2510.11003)
*Takuma Fujiu,Sho Okazaki,Kohei Kaminishi,Yuji Nakata,Shota Hamamoto,Kenshin Yokose,Tatsunori Hara,Yasushi Umeda,Jun Ota*

Main category: cs.AI

TL;DR: 本研究构建了诊断知识本体和基于功能-行为-结构（FBS）模型的维护记录积累方法，以改进制造系统中的故障原因推断，尤其在困难案例中表现出更好的专家一致性。


<details>
  <summary>Details</summary>
Motivation: 在制造系统中，识别故障原因是维持和提高生产效率的关键。现有的基于知识的故障原因推断需要知识库能明确地结构化系统和故障知识，并包含足够长的因果链。

Method: 本研究构建了“诊断知识本体（Diagnostic Knowledge Ontology）”，并基于此提出了“功能-行为-结构（FBS）模型”的维护记录积累方法。

Result: 使用该方法积累的维护记录进行故障原因推断，与专家列出的候选原因集显示出更好的一致性，特别是在相关案例少且词汇使用不同的困难情况下。

Conclusion: 该方法利用设计阶段对目标的理解和知识来支持维护阶段的知识积累和问题解决，并有望成为未来整个工程链中知识共享的基础。

Abstract: In manufacturing systems, identifying the causes of failures is crucial for
maintaining and improving production efficiency. In knowledge-based
failure-cause inference, it is important that the knowledge base (1) explicitly
structures knowledge about the target system and about failures, and (2)
contains sufficiently long causal chains of failures. In this study, we
constructed Diagnostic Knowledge Ontology and proposed a
Function-Behavior-Structure (FBS) model-based maintenance-record accumulation
method based on it. Failure-cause inference using the maintenance records
accumulated by the proposed method showed better agreement with the set of
candidate causes enumerated by experts, especially in difficult cases where the
number of related cases is small and the vocabulary used differs. In the
future, it will be necessary to develop inference methods tailored to these
maintenance records, build a user interface, and carry out validation on larger
and more diverse systems. Additionally, this approach leverages the
understanding and knowledge of the target in the design phase to support
knowledge accumulation and problem solving during the maintenance phase, and it
is expected to become a foundation for knowledge sharing across the entire
engineering chain in the future.

</details>


### [53] [Argumentation-Based Explainability for Legal AI: Comparative and Regulatory Perspectives](https://arxiv.org/abs/2510.11079)
*Andrada Iulia Prajescu,Roberto Confalonieri*

Main category: cs.AI

TL;DR: 本文探讨了在法律领域部署AI系统时，解释性AI（XAI）的重要性，并强调计算论证模型因其能捕捉法律的特性，是提供法律相关解释并符合法规（如GDPR、AIA）的最佳方法。


<details>
  <summary>Details</summary>
Motivation: AI系统在法律领域的部署带来了“黑箱问题”，损害了公平性、问责制和信任，因为受影响的个人往往无法获得有意义的解释。这促使了对解释性AI方法的需求。

Method: 本文分析了各种解释策略的优缺点，评估了它们在法律推理中的适用性，并特别关注计算论证模型。研究还探讨了这些方法与新兴监管框架（如欧盟GDPR和AI法案）的一致性。

Result: 研究发现，论证框架能够捕捉法律的可驳斥性、可争议性和价值敏感性，为可解释的法律AI提供了特别坚实的基础。它们在提供法律相关解释方面表现出强大的潜力。

Conclusion: 计算论证被认为是满足法律领域透明度技术和规范要求的最佳选择，尽管仍存在偏见缓解、司法环境中的实证验证以及遵守不断发展的道德和法律标准等开放挑战。

Abstract: Artificial Intelligence (AI) systems are increasingly deployed in legal
contexts, where their opacity raises significant challenges for fairness,
accountability, and trust. The so-called ``black box problem'' undermines the
legitimacy of automated decision-making, as affected individuals often lack
access to meaningful explanations. In response, the field of Explainable AI
(XAI) has proposed a variety of methods to enhance transparency, ranging from
example-based and rule-based techniques to hybrid and argumentation-based
approaches. This paper promotes computational models of arguments and their
role in providing legally relevant explanations, with particular attention to
their alignment with emerging regulatory frameworks such as the EU General Data
Protection Regulation (GDPR) and the Artificial Intelligence Act (AIA). We
analyze the strengths and limitations of different explanation strategies,
evaluate their applicability to legal reasoning, and highlight how
argumentation frameworks -- by capturing the defeasible, contestable, and
value-sensitive nature of law -- offer a particularly robust foundation for
explainable legal AI. Finally, we identify open challenges and research
directions, including bias mitigation, empirical validation in judicial
settings, and compliance with evolving ethical and legal standards, arguing
that computational argumentation is best positioned to meet both technical and
normative requirements of transparency in the law domain.

</details>


### [54] [Modeling AI-Driven Production and Competitiveness A Multi-Agent Economic Simulation of China and the United States](https://arxiv.org/abs/2510.11085)
*Yuxinyue Qian,Jun Liu*

Main category: cs.AI

TL;DR: 本研究利用多层智能主体经济模型，模拟比较了中美两国在AI协作、网络效应和AI自主生产机制下宏观经济产出的演变。结果表明，AI作为独立生产实体能显著提升社会产出增长，且中国在智能主体扩展和技术追赶方面具有加速潜力。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术快速发展，社会经济系统进入“人机共创”新阶段，需要理解AI驱动的生产系统转型和国际竞争力变化。

Method: 该研究基于先前建立的多层智能主体经济模型，通过模拟比较了中国和美国在AI协作、网络效应和AI自主生产等不同机制下的宏观经济产出演变。

Result: (1) 当AI作为独立的生产实体时，社会总产出的增长速度远超传统以人力劳动为基础的模型；(2) 中国在智能主体数量扩张和技术追赶速度上均展现出明显的加速潜力，有望实现技术趋同甚至局部超越。

Conclusion: 本研究为理解AI驱动的生产系统转型和国际竞争力变化提供了一个系统性的、基于模型的分析框架，并为相关政策制定提供了量化见解。

Abstract: With the rapid development of artificial intelligence (AI) technology,
socio-economic systems are entering a new stage of "human-AI co-creation."
Building upon a previously established multi-level intelligent agent economic
model, this paper conducts simulation-based comparisons of macroeconomic output
evolution in China and the United States under different mechanisms-AI
collaboration, network effects, and AI autonomous production. The results show
that: (1) when AI functions as an independent productive entity, the overall
growth rate of social output far exceeds that of traditional human-labor-based
models; (2) China demonstrates clear potential for acceleration in both the
expansion of intelligent agent populations and the pace of technological
catch-up, offering the possibility of achieving technological convergence or
even partial surpassing. This study provides a systematic, model-based
analytical framework for understanding AI-driven production system
transformation and shifts in international competitiveness, as well as
quantitative insights for relevant policy formulation.

</details>


### [55] [Improving AI Efficiency in Data Centres by Power Dynamic Response](https://arxiv.org/abs/2510.11119)
*Andrea Marinoni,Sai Shivareddy,Pietro Lio',Weisi Lin,Erik Cambria,Clare Grey*

Main category: cs.AI

TL;DR: 本文提出并研究了一种创新的AI数据中心电源管理方法，即将部分输入电源动态化，以应对其高能耗问题，旨在提高可持续性。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）的快速发展，特别是大型语言模型和基础模型的出现，需要强大可靠的电力基础设施。然而，AI数据中心能耗巨大，对环境和可持续发展构成挑战，因此其电源管理问题成为关注焦点。

Method: 研究了一种创新的AI数据中心电源管理方法：使部分输入电源像数据计算功能所用电源一样动态化。通过分析全球多个数据平台的电源趋势，量化并比较了无源和有源设备在计算增益、能源效率、资本支出减少和管理成本方面的性能。

Result: 所提出的策略被认为是AI数据中心电源管理的一种范式转变，有潜力显著提高AI超大规模计算的可持续性，改善其在环境、财务和社会领域的影响。

Conclusion: 通过使部分输入电源动态化，可以有效管理AI数据中心的高能耗，从而显著提高其可持续性，并在环境、财务和社会方面产生积极影响。

Abstract: The steady growth of artificial intelligence (AI) has accelerated in the
recent years, facilitated by the development of sophisticated models such as
large language models and foundation models. Ensuring robust and reliable power
infrastructures is fundamental to take advantage of the full potential of AI.
However, AI data centres are extremely hungry for power, putting the problem of
their power management in the spotlight, especially with respect to their
impact on environment and sustainable development. In this work, we investigate
the capacity and limits of solutions based on an innovative approach for the
power management of AI data centres, i.e., making part of the input power as
dynamic as the power used for data-computing functions. The performance of
passive and active devices are quantified and compared in terms of
computational gain, energy efficiency, reduction of capital expenditure, and
management costs by analysing power trends from multiple data platforms
worldwide. This strategy, which identifies a paradigm shift in the AI data
centre power management, has the potential to strongly improve the
sustainability of AI hyperscalers, enhancing their footprint on environmental,
financial, and societal fields.

</details>


### [56] [Spec-Driven AI for Science: The ARIA Framework for Automated and Reproducible Data Analysis](https://arxiv.org/abs/2510.11143)
*Chuke Chen,Biao Luo,Nan Li,Boxiang Wang,Hang Yang,Jing Guo,Ming Xu*

Main category: cs.AI

TL;DR: ARIA是一个规范驱动、人机协作的框架，旨在通过自然语言规范实现自动化和可解释的数据分析，从而弥合科学数据分析能力与研究意图之间的差距。


<details>
  <summary>Details</summary>
Motivation: 科学数据量激增，导致分析能力与研究意图之间存在鸿沟。现有的AI分析工具（如AutoML和代理研究助手）要么牺牲透明度以追求自动化，要么依赖手动脚本，限制了可扩展性和可重复性。

Method: ARIA（Automated Research Intelligence Assistant）是一个规范驱动、人机协作的框架，用于自动化和可解释的数据分析。它集成了命令、上下文、代码、数据、编排和AI模块六个可互操作的层，并采用以文档为中心的工作流，统一了人类推理和机器执行。研究人员通过自然语言规范定义分析目标，ARIA则自主生成可执行代码、验证计算并生成透明文档。

Result: ARIA不仅实现了高预测准确性，还能快速识别最佳特征集并选择合适的模型，从而最大程度地减少冗余调优和重复实验。在Boston Housing案例中，ARIA发现了25个关键特征，并确定XGBoost为表现最佳的模型（R平方=0.93），且过拟合最小。在跨异构领域的评估中，ARIA在性能、可解释性和效率方面均优于现有先进系统。

Conclusion: ARIA通过将“AI for research”和“AI for science”原则整合到规范驱动的架构中，为透明、协作和可重复的科学发现建立了一个新范式。

Abstract: The rapid expansion of scientific data has widened the gap between analytical
capability and research intent. Existing AI-based analysis tools, ranging from
AutoML frameworks to agentic research assistants, either favor automation over
transparency or depend on manual scripting that hinders scalability and
reproducibility. We present ARIA (Automated Research Intelligence Assistant), a
spec-driven, human-in-the-loop framework for automated and interpretable data
analysis. ARIA integrates six interoperable layers, namely Command, Context,
Code, Data, Orchestration, and AI Module, within a document-centric workflow
that unifies human reasoning and machine execution. Through natural-language
specifications, researchers define analytical goals while ARIA autonomously
generates executable code, validates computations, and produces transparent
documentation. Beyond achieving high predictive accuracy, ARIA can rapidly
identify optimal feature sets and select suitable models, minimizing redundant
tuning and repetitive experimentation. In the Boston Housing case, ARIA
discovered 25 key features and determined XGBoost as the best performing model
(R square = 0.93) with minimal overfitting. Evaluations across heterogeneous
domains demonstrate ARIA's strong performance, interpretability, and efficiency
compared with state-of-the-art systems. By combining AI for research and AI for
science principles within a spec-driven architecture, ARIA establishes a new
paradigm for transparent, collaborative, and reproducible scientific discovery.

</details>


### [57] [$How^{2}$: How to learn from procedural How-to questions](https://arxiv.org/abs/2510.11144)
*Gautier Dagan,Frank Keller,Alex Lascarides*

Main category: cs.AI

TL;DR: How²是一个记忆代理框架，使代理能够提出、存储和重用“如何做”问题答案，以进行终身学习和规划。研究表明，抽象且与当前状态解耦的答案对终身学习代理的益处最大。


<details>
  <summary>Details</summary>
Motivation: 规划代理需要通过“如何做”问题来减少不确定性并填补知识空白，但这些问题的开放性（答案范围从可执行动作到高层子目标描述）使得AI代理难以有效提问，AI专家难以有效回答，从而阻碍了高效规划。

Method: 本文提出了How²，一个记忆代理框架，允许代理在交互式环境中提问“如何做”问题、存储答案并重用它们进行终身学习。该方法在Plancraft（一个Minecraft合成环境）中进行评估，教师模型提供从可执行动作序列到高层子目标描述等不同抽象级别的答案。

Result: 研究结果表明，终身学习代理从抽象且与当前状态解耦的答案中获益最多。

Conclusion: How²为基于大型语言模型的代理提供了一种方法，使其能够通过在交互式环境中提问，随时间推移提高其规划能力。

Abstract: An agent facing a planning problem can use answers to how-to questions to
reduce uncertainty and fill knowledge gaps, helping it solve both current and
future tasks. However, their open ended nature, where valid answers to "How do
I X?" range from executable actions to high-level descriptions of X's
sub-goals, makes them challenging for AI agents to ask, and for AI experts to
answer, in ways that support efficient planning. We introduce $How^{2}$, a
memory agent framework that enables agents to ask how-to questions, store the
answers, and reuse them for lifelong learning in interactive environments. We
evaluate our approach in Plancraft, a Minecraft crafting environment, where
agents must complete an assembly task by manipulating inventory items. Using
teacher models that answer at varying levels of abstraction, from executable
action sequences to high-level subgoal descriptions, we show that lifelong
learning agents benefit most from answers that are abstracted and decoupled
from the current state. $How^{2}$ offers a way for LLM-based agents to improve
their planning capabilities over time by asking questions in interactive
environments.

</details>


### [58] [Aligning Deep Implicit Preferences by Learning to Reason Defensively](https://arxiv.org/abs/2510.11194)
*Peiming Li,Zhiyuan Hu,Yang Tang,Shiyu Li,Xi Chen*

Main category: cs.AI

TL;DR: 本文提出批判驱动推理对齐（CDRA）框架，通过结构化推理过程和可解释的奖励信号，解决大型语言模型（LLMs）在个性化对齐中难以推断深层隐式偏好和缺乏防御性推理的问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在个性化对齐方面面临双重挑战：它们无法推断用户深层隐式偏好（包括未说明的目标、语义上下文和风险容忍度），并且缺乏应对现实世界模糊性所需的防御性推理能力。这导致模型响应肤浅、脆弱且短视。

Method: 本文提出批判驱动推理对齐（CDRA）框架，将对齐重构为结构化推理过程。具体方法包括：1) 引入DeepPref基准数据集，包含3000个偏好-查询对，通过模拟多面认知委员会生成批判注释的推理链来解构查询语义并揭示潜在风险，以弥合偏好推断差距。2) 引入个性化生成过程奖励模型（Pers-GenPRM），将奖励建模视为个性化推理任务，生成批判链来评估响应与用户偏好的一致性，然后基于此理由输出最终分数，以灌输防御性推理。3) 通过批判驱动策略对齐，一种整合数值和自然语言反馈的流程级在线强化学习算法，利用这种可解释的、结构化的奖励信号指导策略模型。

Result: 实验证明，CDRA在发现并与用户真实偏好对齐方面表现出色，同时执行了鲁棒的推理。

Conclusion: CDRA通过将对齐重构为结构化推理过程，并利用批判驱动的基准数据集和奖励模型，显著提升了大型语言模型在个性化交互中推断深层用户偏好和进行防御性推理的能力。

Abstract: Personalized alignment is crucial for enabling Large Language Models (LLMs)
to engage effectively in user-centric interactions. However, current methods
face a dual challenge: they fail to infer users' deep implicit preferences
(including unstated goals, semantic context and risk tolerances), and they lack
the defensive reasoning required to navigate real-world ambiguity. This
cognitive gap leads to responses that are superficial, brittle and
short-sighted. To address this, we propose Critique-Driven Reasoning Alignment
(CDRA), which reframes alignment from a scalar reward-matching task into a
structured reasoning process. First, to bridge the preference inference gap, we
introduce the DeepPref benchmark. This dataset, comprising 3000
preference-query pairs across 20 topics, is curated by simulating a
multi-faceted cognitive council that produces critique-annotated reasoning
chains to deconstruct query semantics and reveal latent risks. Second, to
instill defensive reasoning, we introduce the Personalized Generative Process
Reward Model (Pers-GenPRM), which frames reward modeling as a personalized
reasoning task. It generates a critique chain to evaluate a response's
alignment with user preferences before outputting a final score based on this
rationale. Ultimately, this interpretable, structured reward signal guides
policy model through Critique-Driven Policy Alignment, a process-level online
reinforcement learning algorithm integrating both numerical and natural
language feedback. Experiments demonstrate that CDRA excels at discovering and
aligning with users' true preferences while executing robust reasoning. Our
code and dataset are available at https://github.com/Zephyrian-Hugh/Deep-pref.

</details>


### [59] [AI Alignment Strategies from a Risk Perspective: Independent Safety Mechanisms or Shared Failures?](https://arxiv.org/abs/2510.11235)
*Leonard Dung,Florian Mai*

Main category: cs.AI

TL;DR: AI对齐研究采用“深度防御”策略来减轻风险，但其有效性取决于不同对齐技术之间故障模式的（非）相关性。本文分析了7种代表性对齐技术和7种故障模式的重叠程度，并讨论了其对风险评估和未来研究优先级的影响。


<details>
  <summary>Details</summary>
Motivation: AI对齐技术都有其故障模式，即在特定条件下可能无法确保AI安全的风险。AI安全社区已采用“深度防御”框架，即通过多重冗余保护来维持安全。然而，这种策略的成功关键在于不同对齐技术之间故障模式的（非）相关性。如果所有技术共享相同的故障模式，深度防御将无效。因此，理解这些故障模式的重叠程度至关重要。

Method: 本文分析了7种代表性的AI对齐技术和7种具体的故障模式，以评估它们之间的重叠程度。

Result: 本文分析了7种AI对齐技术和7种故障模式之间的重叠程度。 （抽象未提供具体分析结果，仅说明进行了分析）

Conclusion: 分析结果对理解当前AI风险水平以及如何优先规划未来的AI对齐研究具有重要意义。

Abstract: AI alignment research aims to develop techniques to ensure that AI systems do
not cause harm. However, every alignment technique has failure modes, which are
conditions in which there is a non-negligible chance that the technique fails
to provide safety. As a strategy for risk mitigation, the AI safety community
has increasingly adopted a defense-in-depth framework: Conceding that there is
no single technique which guarantees safety, defense-in-depth consists in
having multiple redundant protections against safety failure, such that safety
can be maintained even if some protections fail. However, the success of
defense-in-depth depends on how (un)correlated failure modes are across
alignment techniques. For example, if all techniques had the exact same failure
modes, the defense-in-depth approach would provide no additional protection at
all. In this paper, we analyze 7 representative alignment techniques and 7
failure modes to understand the extent to which they overlap. We then discuss
our results' implications for understanding the current level of risk and how
to prioritize AI alignment research in the future.

</details>


### [60] [PADME: Procedure Aware DynaMic Execution](https://arxiv.org/abs/2510.11281)
*Deepeka Garg,Sihan Zeng,Annapoorani L. Narayanan,Sumitra Ganesh,Leo Ardon*

Main category: cs.AI

TL;DR: 本文提出了PADME框架，通过将自然语言程序转化为图结构表示，实现了对长序列任务的自主执行，有效解决了大型语言模型在此类任务中因结构不足导致的漂移和失败问题，并在多个基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 智能体在从自然语言中学习并执行长序列程序（如食谱、科学协议）时面临核心挑战。自由形式的指令缺乏结构且变异性大，导致基于大型语言模型（LLMs）的智能体在执行过程中容易出现漂移或失败。

Method: PADME（Procedure Aware DynaMic Execution）框架通过生成和利用程序的图结构表示来解决问题。它能自主地将程序文本转换为可执行图，捕捉任务依赖、决策点和可复用子程序。核心方法包括两个阶段：1. “教学阶段”：系统地构建程序结构，并用可执行逻辑进行丰富。2. “执行阶段”：根据实时输入和环境反馈进行动态执行。图表示提供了一种归纳偏置，减少了长序列推理中的错误累积。

Result: PADME在包括ALFWorld和ScienceWorld在内的四个多样化基准测试中取得了最先进的性能。这些结果表明，配备图结构程序表示的智能体为鲁棒和可泛化的执行提供了一种强大的中间抽象。

Conclusion: 图结构程序表示为智能体提供了强大的中间抽象，能够实现鲁棒和可泛化的任务执行。结构化的程序建模对于可靠的智能体驱动自动化至关重要。

Abstract: Learning to autonomously execute long-horizon procedures from natural
language remains a core challenge for intelligent agents. Free-form
instructions such as recipes, scientific protocols, or business workflows
encode rich procedural knowledge, but their variability and lack of structure
cause agents driven by large language models (LLMs) to drift or fail during
execution. We introduce Procedure Aware DynaMic Execution (PADME), an agent
framework that produces and exploits a graph-based representation of
procedures. Unlike prior work that relies on manual graph construction or
unstructured reasoning, PADME autonomously transforms procedural text into
executable graphs that capture task dependencies, decision points, and reusable
subroutines. Central to PADME is a two-phase methodology; Teach phase, which
focuses on systematic structuring, enrichment with executable logic of
procedures, followed by Execute phase, which enables dynamic execution in
response to real-time inputs and environment feedback. This separation ensures
quality assurance and scalability, allowing expert knowledge to be encoded once
and reliably reused across varying contexts. The graph representation also
provides an inductive bias that reduces error accumulation in long-horizon
reasoning, underscoring the importance of structured procedure modeling for
reliable agent-driven automation. Empirically, PADME achieves state-of-the-art
performance on four diverse benchmarks, including ALFWorld and ScienceWorld.
These results demonstrate that agents equipped with graph-based procedure
representations offer a powerful intermediate abstraction for robust and
generalizable execution.

</details>


### [61] [Evolution in Simulation: AI-Agent School with Dual Memory for High-Fidelity Educational Dynamics](https://arxiv.org/abs/2510.11290)
*Sheng Jin,Haoming Wang,Zhiqi Gao,Yongbo Yang,Bao Chunjia,Chengliang Wang*

Main category: cs.AI

TL;DR: 该研究提出了AI智能体学校（AAS）系统，通过自我演化机制（Zero-Exp策略、经验-反思-优化循环、双重记忆库）使LLM智能体能更准确地模拟复杂的教育动态和师生互动，从而有效提升智能体的认知能力并生成高保真数据。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在模拟复杂人类系统和交互中日益重要，但教学过程建模存在碎片化问题，且智能体在模拟多样化教育参与者方面的性能有限。

Method: 提出AI智能体学校（AAS）系统，其核心是自我演化机制。该机制采用Zero-Exp策略，通过“经验-反思-优化”的持续循环进行，并基于包含经验和知识库的双重记忆基础，整合了短期和长期记忆组件。智能体通过在多样化的模拟学校场景中进行情境交互实现自主演化。

Result: 实验证实AAS能够有效模拟复杂的教育动态，并能有效培养智能体的高级认知能力，从而生成高保真的行为和交互数据。

Conclusion: AAS系统使智能体能更准确地建模物理学校中细致入微、多方面的师生互动和学习过程，为从“经验时代”迈向“模拟时代”提供了基础性的垫脚石。

Abstract: Large language models (LLMs) based Agents are increasingly pivotal in
simulating and understanding complex human systems and interactions. We propose
the AI-Agent School (AAS) system, built around a self-evolving mechanism that
leverages agents for simulating complex educational dynamics. Addressing the
fragmented issues in teaching process modeling and the limitations of agents
performance in simulating diverse educational participants, AAS constructs the
Zero-Exp strategy, employs a continuous "experience-reflection-optimization"
cycle, grounded in a dual memory base comprising experience and knowledge bases
and incorporating short-term and long-term memory components. Through this
mechanism, agents autonomously evolve via situated interactions within diverse
simulated school scenarios. This evolution enables agents to more accurately
model the nuanced, multi-faceted teacher-student engagements and underlying
learning processes found in physical schools. Experiment confirms that AAS can
effectively simulate intricate educational dynamics and is effective in
fostering advanced agent cognitive abilities, providing a foundational stepping
stone from the "Era of Experience" to the "Era of Simulation" by generating
high-fidelity behavioral and interaction data.

</details>


### [62] [Automated Skill Decomposition Meets Expert Ontologies: Bridging the Granularity Gap with LLMs](https://arxiv.org/abs/2510.11313)
*Le Ngoc Luyen,Marie-Hélène Abel*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM的自动化技能分解方法，并建立了一个严格的、以本体为基础的评估框架，包括标准化流程、新颖的评估指标和对不同提示策略的实验，以期为开发本体忠实型技能分解系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 研究动机是需要对使用大型语言模型（LLMs）进行自动化技能分解的方法进行严格评估，并标准化从提示到与本体节点对齐的整个流程，以确保评估的准确性和粒度。

Method: 本文提出一个本体接地（ontology-grounded）的评估框架，标准化了从提示、生成到归一化和与本体节点对齐的流程。引入了两个评估指标：使用基于嵌入的最佳匹配来评估内容准确性的语义F1分数，以及通过结构正确放置来评估粒度的层级感知F1分数。实验在ROME-ESCO-DecompSkill数据集上进行，比较了零样本（zero-shot）和泄漏安全（leakage-safe）的少样本（few-shot）提示策略，并进行了延迟分析。

Result: 实验结果显示，零样本提示提供了一个强大的基线。少样本提示策略能持续稳定措辞和粒度，并改善层级感知对齐。延迟分析表明，由于生成更符合模式的完成内容，有示例引导的提示（少样本）与无引导的零样本提示相比，具有竞争力甚至更快。

Conclusion: 该框架、基准和评估指标共同为开发忠实于本体的技能分解系统提供了一个可复现的基础。

Abstract: This paper investigates automated skill decomposition using Large Language
Models (LLMs) and proposes a rigorous, ontology-grounded evaluation framework.
Our framework standardizes the pipeline from prompting and generation to
normalization and alignment with ontology nodes. To evaluate outputs, we
introduce two metrics: a semantic F1-score that uses optimal embedding-based
matching to assess content accuracy, and a hierarchy-aware F1-score that
credits structurally correct placements to assess granularity. We conduct
experiments on ROME-ESCO-DecompSkill, a curated subset of parents, comparing
two prompting strategies: zero-shot and leakage-safe few-shot with exemplars.
Across diverse LLMs, zero-shot offers a strong baseline, while few-shot
consistently stabilizes phrasing and granularity and improves hierarchy-aware
alignment. A latency analysis further shows that exemplar-guided prompts are
competitive - and sometimes faster - than unguided zero-shot due to more
schema-compliant completions. Together, the framework, benchmark, and metrics
provide a reproducible foundation for developing ontology-faithful skill
decomposition systems.

</details>


### [63] [AI-Driven anemia diagnosis: A review of advanced models and techniques](https://arxiv.org/abs/2510.11380)
*Abdullah Al Mahmud,Prangon Chowdhury,Mohammed Borhan Uddin,Khaled Eabne Delowar,Tausifur Rahman Talha,Bijoy Dewanjee*

Main category: cs.AI

TL;DR: 本文系统综述了人工智能（机器学习和深度学习）在贫血检测、分类和诊断方面的最新进展，并比较了不同模型的性能指标。


<details>
  <summary>Details</summary>
Motivation: 贫血是全球性健康问题，准确及时的诊断至关重要。人工智能技术在贫血检测领域显示出巨大潜力，因此有必要对该领域进行系统回顾。

Method: 本文采用系统综述的方法，重点关注应用于贫血检测的各种机器学习和深度学习模型，并基于准确率、敏感性、特异性和精确度等性能指标对这些模型进行比较评估。

Result: 通过分析性能指标，本文评估了所讨论模型在检测和分类贫血方面的优势和局限性。

Conclusion: 解决模型的优势和局限性对于提高贫血诊断的准确性至关重要。

Abstract: Anemia, a condition marked by insufficient levels of red blood cells or
hemoglobin, remains a widespread health issue affecting millions of individuals
globally. Accurate and timely diagnosis is essential for effective management
and treatment of anemia. In recent years, there has been a growing interest in
the use of artificial intelligence techniques, i.e., machine learning (ML) and
deep learning (DL) for the detection, classification, and diagnosis of anemia.
This paper provides a systematic review of the recent advancements in this
field, with a focus on various models applied to anemia detection. The review
also compares these models based on several performance metrics, including
accuracy, sensitivity, specificity, and precision. By analyzing these metrics,
the paper evaluates the strengths and limitation of discussed models in
detecting and classifying anemia, emphasizing the importance of addressing
these factors to improve diagnostic accuracy.

</details>


### [64] [From <Answer> to <Think>: Multidimensional Supervision of Reasoning Process for LLM Optimization](https://arxiv.org/abs/2510.11457)
*Beining Wang,Weihang Su,Hongtao Tian,Tao Yang,Yujia Zhou,Ting Yao,Qingyao Ai,Yiqun Liu*

Main category: cs.AI

TL;DR: 本文提出维度级奖励模型（DRM），通过评估推理过程的置信度、相关性和连贯性三个维度，为大型语言模型（LLMs）提供可解释的多维度监督信号，从而提升其多步推理能力，并在多种任务上展现出泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限：结果监督的强化学习（RLVR）奖励稀疏且可能传播错误推理；过程级奖励模型（PRMs）缺乏泛化性和可解释性，需要特定任务的推理过程分割。

Method: 提出维度级奖励模型（DRM），通过评估推理过程的三个基本、互补且可解释的维度来提供监督：置信度（不确定性校准）、相关性（语义对齐）和连贯性（逻辑一致性）。这使得在无需真实答案的情况下进行可解释的评估。

Result: 实验结果表明，DRM提供了有效的监督信号，能指导LLMs的优化并增强其推理能力。DRM监督训练在数学、问答、代码执行和谜题等多种域内和域外开放域任务上均取得了显著提升。

Conclusion: 研究发现，对推理过程进行多维度监督可以提高LLMs超越训练分布的泛化推理能力。

Abstract: Improving the multi-step reasoning ability of Large Language Models (LLMs) is
a critical yet challenging task. The dominant paradigm, outcome-supervised
reinforcement learning (RLVR), rewards only correct final answers, often
propagating flawed reasoning and suffering from sparse reward signals. While
process-level reward models (PRMs) provide denser, step-by-step feedback, they
lack generalizability and interpretability, requiring task-specific
segmentation of the reasoning process. To this end, we propose the
Dimension-level Reward Model (DRM), a new supervision framework that bridges
the gap between these two approaches. DRM evaluates the quality of a reasoning
process along three fundamental, complementary, and interpretable dimensions:
Confidence for uncertainty calibration, Relevance for semantic alignment, and
Coherence for logical consistency. Together, these dimensions capture aspects
beyond final answer correctness and enable interpretable assessment without
requiring ground truth answers. Experimental results show that DRM provides
effective supervision signals, guides the optimization of LLMs and enhances
their reasoning ability. In particular, DRM-supervised training achieves
consistent gains on both in-distribution and out-of-distribution open-domain
tasks, including mathematics, question answering, code execution, and puzzles.
Our findings demonstrate that multidimensional supervision of the reasoning
process can improve the generalized reasoning ability of LLMs beyond the
training distribution.

</details>


### [65] [Unifying Deductive and Abductive Reasoning in Knowledge Graphs with Masked Diffusion Model](https://arxiv.org/abs/2510.11462)
*Yisen Gao,Jiaxin Bai,Yi Huang,Xingcheng Fu,Qingyun Sun,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文提出了DARK，一个统一的框架，用于知识图谱上的演绎和溯因推理。它采用掩码扩散模型，结合自反思去噪过程和逻辑探索强化学习，在两类推理任务上均达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 演绎推理和溯因推理是知识图谱分析的关键范式，但现有方法通常独立处理它们，未能利用两者之间的协同潜力，即演绎可以验证假设，溯因可以揭示深层逻辑模式。

Method: 本文提出了DARK（Deductive and Abductive Reasoning in Knowledge graphs），一个统一的框架。它是一个掩码扩散模型，能够捕捉查询和结论之间的双向关系，并包含两项关键创新：1. 自反思去噪过程：在溯因推理中，通过迭代生成和验证候选假设来利用演绎推理进行假设细化。2. 逻辑探索强化学习：通过同时掩盖查询和结论，使模型能够探索新颖的推理组合，发现更丰富的逻辑关联。

Result: 在多个基准知识图谱上的大量实验表明，DARK在演绎和溯因推理任务上均取得了最先进的性能。

Conclusion: 统一的演绎和溯因推理方法带来了显著的优势，证明了DARK框架的有效性。

Abstract: Deductive and abductive reasoning are two critical paradigms for analyzing
knowledge graphs, enabling applications from financial query answering to
scientific discovery. Deductive reasoning on knowledge graphs usually involves
retrieving entities that satisfy a complex logical query, while abductive
reasoning generates plausible logical hypotheses from observations. Despite
their clear synergistic potential, where deduction can validate hypotheses and
abduction can uncover deeper logical patterns, existing methods address them in
isolation. To bridge this gap, we propose DARK, a unified framework for
Deductive and Abductive Reasoning in Knowledge graphs. As a masked diffusion
model capable of capturing the bidirectional relationship between queries and
conclusions, DARK has two key innovations. First, to better leverage deduction
for hypothesis refinement during abductive reasoning, we introduce a
self-reflective denoising process that iteratively generates and validates
candidate hypotheses against the observed conclusion. Second, to discover
richer logical associations, we propose a logic-exploration reinforcement
learning approach that simultaneously masks queries and conclusions, enabling
the model to explore novel reasoning compositions. Extensive experiments on
multiple benchmark knowledge graphs show that DARK achieves state-of-the-art
performance on both deductive and abductive reasoning tasks, demonstrating the
significant benefits of our unified approach.

</details>


### [66] [Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products](https://arxiv.org/abs/2510.11558)
*Komal Gupta,Aditya Shrivastava*

Main category: cs.AI

TL;DR: 本文探讨了企业级AI助手（如LLM）中的零数据保留策略，分析了Salesforce AgentForce和Microsoft Copilot在实现零数据保留方面的技术架构、合规性及可用性权衡。


<details>
  <summary>Details</summary>
Motivation: 随着企业AI助手的兴起，保护私有数据和确保合规性成为关键优先事项，尤其是在医疗和金融等敏感行业。零数据保留策略对于增强业务生产力同时维护数据隐私至关重要。

Method: 研究探索了大型语言模型（LLM）企业应用中的零数据保留策略，定义了此类系统在架构、合规性和可用性方面的权衡。通过分析Salesforce AgentForce和Microsoft Copilot这两款领先AI助手，考察了它们在支持零数据保留政策上采用的不同技术架构，并考虑了OpenAI、Anthropic和Meta等LLM服务提供商的作用。

Result: 文章分析了消费应用程序（如Salesforce AgentForce和Microsoft Copilot）以及大型语言模型服务提供商（如OpenAI、Anthropic和Meta）在部署零数据保留策略时的技术架构和实现方式，揭示了不同公司在此方面的独特方法。

Conclusion: 研究分析了企业AI助手中零数据保留策略的技术架构和部署，强调了实现数据治理、合规性和业务隐私的重要性，并展示了行业领导者如何通过不同架构实现这一目标。

Abstract: Governance of data, compliance, and business privacy matters, particularly
for healthcare and finance businesses. Since the recent emergence of AI
enterprise AI assistants enhancing business productivity, safeguarding private
data and compliance is now a priority. With the implementation of AI assistants
across the enterprise, the zero data retention can be achieved by implementing
zero data retention policies by Large Language Model businesses like Open AI
and Anthropic and Meta. In this work, we explore zero data retention policies
for the Enterprise apps of large language models (LLMs). Our key contribution
is defining the architectural, compliance, and usability trade-offs of such
systems in parallel. In this research work, we examine the development of
commercial AI assistants with two industry leaders and market titans in this
arena - Salesforce and Microsoft. Both of these companies used distinct
technical architecture to support zero data retention policies. Salesforce
AgentForce and Microsoft Copilot are among the leading AI assistants providing
much-needed push to business productivity in customer care. The purpose of this
paper is to analyze the technical architecture and deployment of zero data
retention policy by consuming applications as well as big language models
service providers like Open Ai, Anthropic, and Meta.

</details>


### [67] [Reproducibility: The New Frontier in AI Governance](https://arxiv.org/abs/2510.11595)
*Israel Mason-Williams,Gabryel Mason-Williams*

Main category: cs.AI

TL;DR: AI政策制定者面临低信噪比的信息环境，受AI研究快速发表和弱可重复性协议影响，难以有效治理。本文提出AI研究应采纳更严格的可重复性指南（如预注册、提高统计功效、发表负面结果），以协助治理并增进对AI风险的共识。


<details>
  <summary>Details</summary>
Motivation: AI政策制定者在制定有效治理机制时，面临信息环境的低信噪比、监管俘获、以及对AI风险优先级的深层不确定性。这主要是由于AI研究的快速发表速度和缺乏强科学标准（通过弱可重复性协议）削弱了政策制定者制定有意义政策的能力。

Method: 本文通过借鉴其他科学领域的危机，评估了AI研究中即将到来的可重复性危机。它提出并评论了如何采纳预注册、提高统计功效以及发表负面结果的可重复性协议，以实现有效的AI治理。

Result: 采纳更严格的可重复性指南，特别是预注册、提高统计功效和发表负面结果的协议，能够协助AI治理工作，改善对AI风险格局的共识，并赋能有效的AI治理。这些协议应成为政策制定者治理工具库中的核心工具。

Conclusion: 尽管AI治理必须是反应性的，但政策制定者和政府必须将可重复性协议视为治理工具库中的核心工具，并要求AI研究达到更高的标准，以应对当前的信息环境挑战，实现有意义的AI政策和治理。

Abstract: AI policymakers are responsible for delivering effective governance
mechanisms that can provide safe, aligned and trustworthy AI development.
However, the information environment offered to policymakers is characterised
by an unnecessarily low Signal-To-Noise Ratio, favouring regulatory capture and
creating deep uncertainty and divides on which risks should be prioritised from
a governance perspective. We posit that the current publication speeds in AI
combined with the lack of strong scientific standards, via weak reproducibility
protocols, effectively erodes the power of policymakers to enact meaningful
policy and governance protocols. Our paper outlines how AI research could adopt
stricter reproducibility guidelines to assist governance endeavours and improve
consensus on the AI risk landscape. We evaluate the forthcoming reproducibility
crisis within AI research through the lens of crises in other scientific
domains; providing a commentary on how adopting preregistration, increased
statistical power and negative result publication reproducibility protocols can
enable effective AI governance. While we maintain that AI governance must be
reactive due to AI's significant societal implications we argue that
policymakers and governments must consider reproducibility protocols as a core
tool in the governance arsenal and demand higher standards for AI research.
Code to replicate data and figures:
https://github.com/IFMW01/reproducibility-the-new-frontier-in-ai-governance

</details>


### [68] [Analyzing and Internalizing Complex Policy Documents for LLM Agents](https://arxiv.org/abs/2510.11588)
*Jiateng Liu,Zhenhailong Wang,Xiaojiang Huang,Yingjie Li,Xing Fan,Xiang Li,Chenlei Guo,Ruhi Sarikaya,Heng Ji*

Main category: cs.AI

TL;DR: 该研究提出CC-Gen基准和CAP-CPT方法，以解决大型语言模型代理中复杂策略文档的内部化问题，显著减少提示长度和计算开销，同时提高性能。


<details>
  <summary>Details</summary>
Motivation: 基于LLM的代理系统依赖于编码业务规则的上下文策略文档。随着需求增长，这些文档迅速膨胀，导致高计算开销。现有的提示压缩方法不适用于代理策略文档的多层次复杂性和深度推理需求，因此需要开发将策略文档嵌入模型先验的内部化方法。

Method: 1. 引入CC-Gen：一个具有四个可控复杂性级别的代理基准生成器，用于系统评估代理处理复杂性的能力和评估策略内部化。2. 提出CAP-CPT（Category-Aware Policy Continued Pretraining）：一个自动化流程，解析策略文档以提取关键规范，将其分为事实性、行为性和条件性类别，并隔离驱动工作流复杂性的复杂条件，以此指导有针对性的数据合成，并通过自回归预训练损失使代理内部化策略信息。

Result: 1. 复杂的策略规范对代理构成重大推理挑战。2. 通过SFT（监督微调）支持内部化是数据密集型的，并随着策略复杂性增加而急剧下降。3. CAP-CPT在所有设置下都改进了SFT基线，在Qwen-3-32B上实现了高达41%和22%的增益。4. 在CC-Gen上实现了97.3%的提示长度缩减。5. 在SFT数据量极小的情况下，进一步增强了tau-Bench的性能。

Conclusion: CAP-CPT通过类别感知策略持续预训练，有效地将复杂代理策略文档内部化到LLM中，显著减少了提示长度和计算开销，同时在处理复杂推理任务时提高了代理的性能，并缓解了数据和推理负担。

Abstract: Large Language Model (LLM)-based agentic systems rely on in-context policy
documents encoding diverse business rules. As requirements grow, these
documents expand rapidly, causing high computational overhead. This motivates
developing internalization methods that embed policy documents into model
priors while preserving performance. Prior prompt compression work targets
generic prompts, but agentic policy documents span multiple complexity levels
and require deeper reasoning, making internalization harder. We introduce
CC-Gen, an agentic benchmark generator with Controllable Complexity across four
levels, enabling systematic evaluation of agents' ability to handle complexity
and offering a unified framework for assessing policy internalization. Our
analysis shows that complex policy specifications governing workflows pose
major reasoning challenges. Supporting internalization with gold user agent
interaction trajectories containing chain-of-thought (CoT) annotations via
supervised fine-tuning (SFT) is data-intensive and degrades sharply as policy
complexity increases. To mitigate data and reasoning burdens, we propose
Category-Aware Policy Continued Pretraining (CAP-CPT). Our automated pipeline
parses policy documents to extract key specifications, grouping them into
factual, behavioral, and conditional categories, and isolating complex
conditions that drive workflow complexity. This guides targeted data synthesis
and enables agents to internalize policy information through an autoregressive
pretraining loss. Experiments show CAP-CPT improves SFT baselines in all
settings, with up to 41% and 22% gains on Qwen-3-32B, achieving 97.3% prompt
length reduction on CC-Gen and further enhancing tau-Bench with minimal SFT
data.

</details>


### [69] [Explainability, risk modeling, and segmentation based customer churn analytics for personalized retention in e-commerce](https://arxiv.org/abs/2510.11604)
*Sanjula De Alwis,Indrajith Ekanayake*

Main category: cs.AI

TL;DR: 本研究提出一个结合可解释AI、生存分析和RFM分析的三组件框架，旨在将客户流失预测转向可解释的个性化留存策略设计，以降低流失并增强客户忠诚度。


<details>
  <summary>Details</summary>
Motivation: 在线零售中，客户获取成本高于客户保留成本，促使企业投资于流失分析。然而，现有流失模型多为“黑箱”，缺乏对流失原因、干预时机和高风险客户群体的深入洞察，因此需要从单纯预测转向基于可解释证据的个性化留存策略设计。

Method: 本研究提出了一个三组件框架，整合了：1) 可解释AI（XAI）以量化特征贡献；2) 生存分析以建模事件发生时间（流失）风险；3) RFM（最近一次消费、消费频率、消费金额）分析以根据交易行为对客户进行分段。

Result: 通过结合这些方法，该框架能够归因流失驱动因素、估算干预窗口期，并优先排序目标行动的客户细分，从而支持减少流失和增强客户忠诚度的策略。

Conclusion: 该框架通过提供可解释的洞察力，使企业能够设计出更具针对性和有效性的个性化留存策略，从而降低客户流失率并提升客户忠诚度。

Abstract: In online retail, customer acquisition typically incurs higher costs than
customer retention, motivating firms to invest in churn analytics. However,
many contemporary churn models operate as opaque black boxes, limiting insight
into the determinants of attrition, the timing of retention opportunities, and
the identification of high-risk customer segments. Accordingly, the emphasis
should shift from prediction alone to the design of personalized retention
strategies grounded in interpretable evidence. This study advances a
three-component framework that integrates explainable AI to quantify feature
contributions, survival analysis to model time-to-event churn risk, and RFM
profiling to segment customers by transactional behaviour. In combination,
these methods enable the attribution of churn drivers, estimation of
intervention windows, and prioritization of segments for targeted actions,
thereby supporting strategies that reduce attrition and strengthen customer
loyalty.

</details>


### [70] [SR-Scientist: Scientific Equation Discovery With Agentic AI](https://arxiv.org/abs/2510.11661)
*Shijie Xia,Yuhan Sun,Pengfei Liu*

Main category: cs.AI

TL;DR: SR-Scientist框架将大型语言模型（LLM）从简单的方程提出者提升为自主AI科学家，通过代码解释器进行数据分析、方程实现、评估和优化，在科学方程发现任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 目前LLM在科学方程发现中的应用通常将其限制为搜索算法（如遗传编程）中的方程提出者角色，未能充分利用其潜力。

Method: SR-Scientist框架将代码解释器封装成一套用于数据分析和方程评估的工具。LLM被指示利用这些工具进行长期优化，以最少的人工定义流程自主改进方程。此外，还开发了一个端到端的强化学习框架来增强智能体的能力。

Result: SR-Scientist在涵盖四个科学领域的数据集上，性能比基线方法高出6%到35%。此外，该方法对噪声具有鲁棒性，发现的方程能泛化到域外数据，并具有符号准确性。

Conclusion: SR-Scientist通过将LLM提升为能够自主分析数据、实现和优化方程的AI科学家，显著改进了科学方程发现过程，实现了卓越的性能、鲁棒性和泛化能力。

Abstract: Recently, Large Language Models (LLMs) have been applied to scientific
equation discovery, leveraging their embedded scientific knowledge for
hypothesis generation. However, current methods typically confine LLMs to the
role of an equation proposer within search algorithms like genetic programming.
In this paper, we present SR-Scientist, a framework that elevates the LLM from
a simple equation proposer to an autonomous AI scientist that writes code to
analyze data, implements the equation as code, submits it for evaluation, and
optimizes the equation based on experimental feedback. Specifically, we wrap
the code interpreter into a set of tools for data analysis and equation
evaluation. The agent is instructed to optimize the equation by utilizing these
tools over a long horizon with minimal human-defined pipelines. Empirical
results show that SR-Scientist outperforms baseline methods by an absolute
margin of 6% to 35% on datasets covering four science disciplines.
Additionally, we demonstrate our method's robustness to noise, the
generalization of the discovered equations to out-of-domain data, and their
symbolic accuracy. Furthermore, we develop an end-to-end reinforcement learning
framework to enhance the agent's capabilities.

</details>


### [71] [ParaCook: On Time-Efficient Planning for Multi-Agent Systems](https://arxiv.org/abs/2510.11608)
*Shiqi Zhang,Xinbei Ma,Yunqing Xu,Zouying Cao,Pengrui Lu,Haobo Yuan,Tiancheng Shen,Zhuosheng Zhang,Hai Zhao,Ming-Hsuan Yang*

Main category: cs.AI

TL;DR: ParaCook是一个新的基准，用于评估大型语言模型在多智能体协作规划中时间效率的能力，发现现有LLM在此方面表现不佳，但在抽象任务中具有潜力。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）智能体基准主要关注任务完成度，而忽略了并行和异步操作中的时间效率，尽管LLM在长期、真实世界任务规划中展现出强大的推理能力。

Method: 提出了ParaCook基准，用于时间高效的协作规划。ParaCook受Overcooked游戏启发，提供了一个多智能体烹饪任务环境，具有简化的动作空间，以隔离策略性并行规划的核心挑战。

Result: 对最先进的LLM进行了全面评估，发现当前方法生成的计划次优，难以处理并行操作或协调。分析还揭示了LLM在抽象任务中专注于高层并行优化的潜力。

Conclusion: ParaCook提供了一个可扩展且复杂性可调的评估框架，为开发和评估关注时间效率的多智能体规划奠定了基础。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities for planning
long-horizon, real-world tasks, yet existing agent benchmarks focus on task
completion while neglecting time efficiency in parallel and asynchronous
operations. To address this, we present ParaCook, a benchmark for
time-efficient collaborative planning. Inspired by the Overcooked game,
ParaCook provides an environment for various challenging interaction planning
of multi-agent systems that are instantiated as cooking tasks, with a
simplified action space to isolate the core challenge of strategic parallel
planning. Through a comprehensive evaluation of state-of-the-art LLMs, we find
that current approaches achieve suboptimal plans, which struggle with parallel
actions or coordination. Our analysis also reveals LLMs' potential on abstract
tasks where they can focus on high-level parallel optimization. ParaCook
provides a scalable evaluation framework with adjustable complexity,
establishing a foundation for developing and assessing time efficiency-aware
multi-agent planning. The code and data are available at
https://github.com/zsq259/ParaCook.

</details>


### [72] [Operand Quant: A Single-Agent Architecture for Autonomous Machine Learning Engineering](https://arxiv.org/abs/2510.11694)
*Arjun Sahney,Ram Gorthi,Cezary Łastowski,Javier Vega*

Main category: cs.AI

TL;DR: Operand Quant是一种单智能体、基于IDE的自主机器学习工程（MLE）架构，在MLE-Benchmark上取得了新的最先进（SOTA）成果，性能优于多智能体系统。


<details>
  <summary>Details</summary>
Motivation: 传统的MLE自动化通常采用多智能体编排框架。本研究的动机是探索一种将所有MLE生命周期阶段整合到单个、上下文感知智能体中的替代方法，以期在自主MLE任务中超越现有系统。

Method: Operand Quant采用单智能体、基于IDE的架构。它将探索、建模、实验和部署等所有MLE生命周期阶段整合到一个单一的、上下文感知的智能体中。该智能体以线性、非阻塞的方式在受控的IDE环境中自主运行。

Result: 在MLE-Benchmark（2025）上，Operand Quant在75个问题中获得了0.3956 +/- 0.0565的整体奖牌率，达到了新的最先进（SOTA）水平，是迄今为止所有评估系统中记录的最高性能。

Conclusion: 研究表明，在受控IDE环境中自主运行的线性、非阻塞单智能体架构，在相同的约束条件下，可以超越多智能体和编排系统在自主机器学习工程中的表现。

Abstract: We present Operand Quant, a single-agent, IDE-based architecture for
autonomous machine learning engineering (MLE). Operand Quant departs from
conventional multi-agent orchestration frameworks by consolidating all MLE
lifecycle stages -- exploration, modeling, experimentation, and deployment --
within a single, context-aware agent. On the MLE-Benchmark (2025), Operand
Quant achieved a new state-of-the-art (SOTA) result, with an overall medal rate
of 0.3956 +/- 0.0565 across 75 problems -- the highest recorded performance
among all evaluated systems to date. The architecture demonstrates that a
linear, non-blocking agent, operating autonomously within a controlled IDE
environment, can outperform multi-agent and orchestrated systems under
identical constraints.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [73] [TreeNet: Layered Decision Ensembles](https://arxiv.org/abs/2510.09654)
*Zeshan Khan*

Main category: cs.CV

TL;DR: 本文提出了一种名为TreeNet的新型分层决策集成学习方法，专为医学图像分析设计。它结合了神经网络、集成学习和树模型的特点，在数据有限的情况下也能表现出色，且具有良好的可解释性和实时应用能力。


<details>
  <summary>Details</summary>
Motivation: 在医学图像分析领域，现有方法（如神经网络、决策树和集成学习）在数据量充足时表现良好。然而，医学图像分析普遍面临数据可用性有限和数据置信度不足的挑战。

Method: 本文引入了TreeNet，一种新颖的分层决策集成学习方法，专门用于医学图像分析。它通过整合神经网络、集成学习和基于树的决策模型的关键特征构建而成。该方法通过准确率、精确率、召回率以及训练和评估时间等关键指标进行评估。

Result: 使用完整训练数据时，F1-score高达0.85；使用50%训练数据时，F1-score为0.77（F1-score降低0.08）。评估结果显示，该方法达到每秒32帧，适用于实时应用。

Conclusion: TreeNet在医学图像分析的严苛环境中，特别是在实时分析方面，表现出高效性和可用性，能够提供卓越的性能，并具有良好的可解释性和洞察力。

Abstract: Within the domain of medical image analysis, three distinct methodologies
have demonstrated commendable accuracy: Neural Networks, Decision Trees, and
Ensemble-Based Learning Algorithms, particularly in the specialized context of
genstro institutional track abnormalities detection. These approaches exhibit
efficacy in disease detection scenarios where a substantial volume of data is
available. However, the prevalent challenge in medical image analysis pertains
to limited data availability and data confidence. This paper introduces
TreeNet, a novel layered decision ensemble learning methodology tailored for
medical image analysis. Constructed by integrating pivotal features from neural
networks, ensemble learning, and tree-based decision models, TreeNet emerges as
a potent and adaptable model capable of delivering superior performance across
diverse and intricate machine learning tasks. Furthermore, its interpretability
and insightful decision-making process enhance its applicability in complex
medical scenarios. Evaluation of the proposed approach encompasses key metrics
including Accuracy, Precision, Recall, and training and evaluation time. The
methodology resulted in an F1-score of up to 0.85 when using the complete
training data, with an F1-score of 0.77 when utilizing 50\% of the training
data. This shows a reduction of F1-score of 0.08 while in the reduction of 50\%
of the training data and training time. The evaluation of the methodology
resulted in the 32 Frame per Second which is usable for the realtime
applications. This comprehensive assessment underscores the efficiency and
usability of TreeNet in the demanding landscape of medical image analysis
specially in the realtime analysis.

</details>


### [74] [TinyViT-Batten: Few-Shot Vision Transformer with Explainable Attention for Early Batten-Disease Detection on Pediatric MRI](https://arxiv.org/abs/2510.09649)
*Khartik Uppalapati,Bora Yimenicioglu,Shakeel Abdulkareem,Adan Eftekhari,Bhavya Uppalapati,Viraj Kamath*

Main category: cs.CV

TL;DR: 本文提出TinyViT-Batten，一个基于少量样本学习的Vision Transformer框架，用于从儿科脑部MRI中早期检测巴顿病，在有限训练数据下取得了高准确率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 巴顿病（Batten disease）是一种罕见的儿科神经退行性疾病，其早期MRI迹象微妙且常被遗漏，因此需要一种有效的方法进行早期检测。

Method: 研究团队将一个大型教师ViT模型蒸馏成一个参数量为5M的TinyViT模型，并使用基于度量的少量样本学习（带有5次迭代的原型损失）进行微调。此外，还集成了梯度加权类激活映射（Grad-CAM）以实现可解释的预测。

Result: 该模型在包含79例基因确诊巴顿病MRI和90例年龄匹配对照的多中心数据集上，实现了约91%的高准确率和至少0.95的ROC曲线下面积，优于3D-ResNet和Swin-Tiny基线模型。模型灵敏度大于90%，特异性约90%。

Conclusion: 该模型的体积小巧和强大性能（高灵敏度和特异性）证明了其作为早期巴顿病检测的实用AI解决方案的潜力，并能提供可解释的预测。

Abstract: Batten disease (neuronal ceroid lipofuscinosis) is a rare pediatric
neurodegenerative disorder whose early MRI signs are subtle and often missed.
We propose TinyViT-Batten, a few-shot Vision Transformer (ViT) framework to
detect early Batten disease from pediatric brain MRI with limited training
cases. We distill a large teacher ViT into a 5 M-parameter TinyViT and
fine-tune it using metric-based few-shot learning (prototypical loss with
5-shot episodes). Our model achieves high accuracy (approximately 91%) and area
under ROC of at least 0.95 on a multi-site dataset of 79 genetically confirmed
Batten-disease MRIs (27 CLN3 from the Hochstein natural-history study, 32 CLN2
from an international longitudinal cohort, 12 early-manifestation CLN2 cases
reported by Cokal et al., and 8 public Radiopaedia scans) together with 90
age-matched controls, outperforming a 3D-ResNet and Swin-Tiny baseline. We
further integrate Gradient-weighted Class Activation Mapping (Grad-CAM) to
highlight disease-relevant brain regions, enabling explainable predictions. The
model's small size and strong performance (sensitivity greater than 90%,
specificity approximately 90%) demonstrates a practical AI solution for early
Batten disease detection.

</details>


### [75] [Knowledge-Aware Mamba for Joint Change Detection and Classification from MODIS Times Series](https://arxiv.org/abs/2510.09679)
*Zhengsen Xu,Yimin Zhu,Zack Dewis,Mabel Heffring,Motasem Alkayid,Saeid Taleghanidoozdoozan,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的知识感知Mamba (KAMamba) 模型，用于增强MODIS时间序列变化检测，通过引入知识驱动的过渡矩阵指导、多任务学习、空间-光谱-时间Mamba模块和稀疏可变形Mamba骨干网络，有效解决了MODIS数据中混合像素和信息耦合等挑战，显著提高了变化检测和土地覆盖分类的准确性。


<details>
  <summary>Details</summary>
Motivation: MODIS时间序列的变化检测对于环境监测至关重要，但面临诸多挑战，如混合像素、空间-光谱-时间信息耦合效应以及背景类别异质性，这些都使得任务极具挑战性。

Method: 本文提出了知识感知Mamba (KAMamba) 模型：
1. 设计了知识驱动的过渡矩阵引导方法，引入知识感知过渡损失 (KAT-loss) 以利用类别转换知识，提高检测精度。
2. 采用多任务学习方法，结合预变化分类损失 (PreC-loss)、后变化分类损失 (PostC-loss) 和变化检测损失 (Chg-loss) 来增强模型约束和学习。
3. 设计了新颖的空间-光谱-时间Mamba (SSTMamba) 模块，用于解耦MODIS时间序列中的信息耦合。
4. 在SSTMamba中使用了稀疏可变形Mamba (SDMamba) 骨干网络，以提高Mamba模型的效率并降低计算成本。

Result: 在加拿大萨斯喀彻温省的MODIS时间序列数据集上进行评估，结果显示：
1. 土地覆盖变化检测的平均F1分数比基线提高了约1.5-6%。
2. 土地利用/土地覆盖 (LULC) 分类的总体准确率 (OA)、平均准确率 (AA) 和Kappa系数均提高了约2%。

Conclusion: KAMamba通过其创新性的知识感知过渡损失、多任务学习、SSTMamba模块以及高效的SDMamba骨干网络，成功解决了MODIS时间序列变化检测中的关键难题，显著提升了变化检测和LULC分类的性能。

Abstract: Although change detection using MODIS time series is critical for
environmental monitoring, it is a highly challenging task due to key MODIS
difficulties, e.g., mixed pixels, spatial-spectral-temporal information
coupling effect, and background class heterogeneity. This paper presents a
novel knowledge-aware Mamba (KAMamba) for enhanced MODIS change detection, with
the following contributions. First, to leverage knowledge regarding class
transitions, we design a novel knowledge-driven transition-matrix-guided
approach, leading to a knowledge-aware transition loss (KAT-loss) that can
enhance detection accuracies. Second, to improve model constraints, a
multi-task learning approach is designed, where three losses, i.e., pre-change
classification loss (PreC-loss), post-change classification loss (PostC-loss),
and change detection loss (Chg-loss) are used for improve model learning.
Third, to disentangle information coupling in MODIS time series, novel
spatial-spectral-temporal Mamba (SSTMamba) modules are designed. Last, to
improve Mamba model efficiency and remove computational cost, a sparse and
deformable Mamba (SDMamba) backbone is used in SSTMamba. On the MODIS
time-series dataset for Saskatchewan, Canada, we evaluate the method on
land-cover change detection and LULC classification; results show about 1.5-6%
gains in average F1 for change detection over baselines, and about 2%
improvements in OA, AA, and Kappa for LULC classification.

</details>


### [76] [Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and YOLOv5 Object Detectors for Computer Vision and Pattern Recognition](https://arxiv.org/abs/2510.09653)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 本文全面概述了Ultralytics YOLO目标检测器的发展历程，从YOLOv5到最新的YOLO26，涵盖了架构演变、基准测试、部署和未来挑战。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提供Ultralytics YOLO系列目标检测器的综合性概览，详细阐述其架构演变、性能基准、部署考量以及面临的未来挑战。

Method: 本文通过回顾YOLO系列（YOLOv5、YOLOv8、YOLOv11、YOLO26）的架构创新来追踪其演变，并在MS COCO数据集上进行基准测试，量化比较YOLOv5、YOLOv8、YOLOv11和YOLO26，并与YOLOv12、YOLOv13、RT-DETR、DEIM等模型进行交叉比较。分析了精度、召回率、F1分数、平均精度和推理速度等指标，并讨论了部署策略和未来研究方向。

Result: YOLO26引入了移除DFL、原生NMS-free推理、ProgLoss、STAL和MuSGD优化器等关键创新。YOLOv11具有混合任务分配和效率模块；YOLOv8通过解耦检测头和无锚点预测进行改进；YOLOv5奠定了PyTorch模块化基础。基准测试详细比较了不同YOLO版本和其他模型，突出了精度与效率之间的权衡。部署方面涵盖了导出格式、量化策略和实际应用。

Conclusion: 本文识别了YOLO系列在密集场景限制、混合CNN-Transformer集成、开放词汇检测和边缘感知训练方法等方面的挑战和未来发展方向。

Abstract: This paper presents a comprehensive overview of the Ultralytics YOLO(You Only
Look Once) family of object detectors, focusing the architectural evolution,
benchmarking, deployment perspectives, and future challenges. The review begins
with the most recent release, YOLO26 (YOLOv26), which introduces key
innovations including Distribution Focal Loss (DFL) removal, native NMS-free
inference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label
Assignment (STAL), and the MuSGD optimizer for stable training. The progression
is then traced through YOLO11, with its hybrid task assignment and
efficiency-focused modules; YOLOv8, which advanced with a decoupled detection
head and anchor-free predictions; and YOLOv5, which established the modular
PyTorch foundation that enabled modern YOLO development. Benchmarking on the MS
COCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,
YOLO11, and YOLO26, alongside cross-comparisons with YOLOv12, YOLOv13, RT-DETR,
and DEIM. Metrics including precision, recall, F1 score, mean Average
Precision, and inference speed are analyzed to highlight trade-offs between
accuracy and efficiency. Deployment and application perspectives are further
discussed, covering export formats, quantization strategies, and real-world use
in robotics, agriculture, surveillance, and manufacturing. Finally, the paper
identifies challenges and future directions, including dense-scene limitations,
hybrid CNN-Transformer integration, open-vocabulary detection, and edge-aware
training approaches.

</details>


### [77] [OmniSAT: Compact Action Token, Faster Auto Regression](https://arxiv.org/abs/2510.09667)
*Huaihai Lyu,Chaofan Chen,Senwei Xie,Pengwei Wang,Xiansheng Chen,Shanghang Zhang,Changsheng Xu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Omni Swift Action Tokenizer (OmniSAT) 的方法，旨在为VLA模型学习紧凑、可迁移的动作表示。它通过B-Spline编码和多阶段残差量化实现高效压缩，同时保持重建质量，显著提升了自回归（AR）训练的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有自回归（AR）视觉-语言-动作（VLA）模型在处理长且高维动作序列时效率低下，而之前的压缩方法存在重建质量差或压缩效率低的问题。为此，研究者希望找到一种更有效的方法来压缩动作序列。

Method: 本文引入了Omni Swift Action Tokenizer (OmniSAT)。具体方法包括：首先，通过B-Spline编码对值范围和时间范围进行归一化，以获得一致的表示；然后，对位置、旋转和夹持器子空间应用多阶段残差量化，为每个部分生成从粗到细粒度的压缩离散token。此外，还开发了一种跨实体学习策略，利用统一的动作模式空间，并结合机器人和人类演示进行联合训练。

Result: 在Droid大型数据集上预训练后，OmniSAT的离散token化将训练序列长度缩短了6.8倍，并降低了目标熵。在各种真实机器人和仿真实验中，OmniSAT实现了更高的压缩率，同时保持了重建质量，从而实现了更快的AR训练收敛和模型性能。

Conclusion: OmniSAT提供了一种紧凑、可迁移的动作表示，有效解决了AR VLA模型在动作序列压缩方面的效率和质量问题。它显著缩短了训练序列，提高了训练效率和模型性能，并支持跨实体学习，为大规模VLA模型的预训练奠定了基础。

Abstract: Existing Vision-Language-Action (VLA) models can be broadly categorized into
diffusion-based and auto-regressive (AR) approaches: diffusion models capture
continuous action distributions but rely on computationally heavy iterative
denoising. In contrast, AR models enable efficient optimization and flexible
sequence construction, making them better suited for large-scale pretraining.
To further improve AR efficiency, particularly when action chunks induce
extended and high-dimensional sequences, prior work applies entropy-guided and
token-frequency techniques to shorten the sequence length. However, such
compression struggled with \textit{poor reconstruction or inefficient
compression}. Motivated by this, we introduce an Omni Swift Action Tokenizer,
which learns a compact, transferable action representation. Specifically, we
first normalize value ranges and temporal horizons to obtain a consistent
representation with B-Spline encoding. Then, we apply multi-stage residual
quantization to the position, rotation, and gripper subspaces, producing
compressed discrete tokens with coarse-to-fine granularity for each part. After
pre-training on the large-scale dataset Droid, the resulting discrete
tokenization shortens the training sequence by 6.8$\times$, and lowers the
target entropy. To further explore the potential of OmniSAT, we develop a
cross-embodiment learning strategy that builds on the unified action-pattern
space and jointly leverages robot and human demonstrations. It enables scalable
auxiliary supervision from heterogeneous egocentric videos. Across diverse
real-robot and simulation experiments, OmniSAT encompasses higher compression
while preserving reconstruction quality, enabling faster AR training
convergence and model performance.

</details>


### [78] [NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation](https://arxiv.org/abs/2510.09681)
*Sashank Makanaboyina*

Main category: cs.CV

TL;DR: 本文提出NNDM框架，结合NN-UNet和扩散模型，通过迭代去噪精炼脑肿瘤分割掩码，解决了传统模型在泛化性、边界精度和数据多样性方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管U-Net等卷积神经网络在MRI脑肿瘤检测和分割方面取得了进展，但现有模型在泛化性、边界精度和有限数据多样性方面仍存在不足。

Method: NNDM（NN_UNet Diffusion Model）是一个混合框架，它将NN-UNet的鲁棒特征提取能力与扩散概率模型的生成能力相结合。扩散模型通过学习预测掩码和真实掩码之间的残差误差分布，逐步精炼NN-UNet生成的分割掩码，通过迭代去噪过程纠正细微结构不一致并增强肿瘤边界描绘。

Result: 在BraTS 2021数据集上的实验表明，NNDM在Dice系数和Hausdorff距离等指标上优于传统的U-Net和基于Transformer的基线模型。此外，扩散引导的精炼提高了模型在不同模态和肿瘤亚区域的鲁棒性。

Conclusion: NNDM为结合确定性分割网络和随机扩散模型开辟了新方向，推动了自动化脑肿瘤分析领域的最新技术发展。

Abstract: Accurate detection and segmentation of brain tumors in magnetic resonance
imaging (MRI) are critical for effective diagnosis and treatment planning.
Despite advances in convolutional neural networks (CNNs) such as U-Net,
existing models often struggle with generalization, boundary precision, and
limited data diversity. To address these challenges, we propose NNDM (NN\_UNet
Diffusion Model)a hybrid framework that integrates the robust feature
extraction of NN-UNet with the generative capabilities of diffusion
probabilistic models. In our approach, the diffusion model progressively
refines the segmentation masks generated by NN-UNet by learning the residual
error distribution between predicted and ground-truth masks. This iterative
denoising process enables the model to correct fine structural inconsistencies
and enhance tumor boundary delineation. Experiments conducted on the BraTS 2021
datasets demonstrate that NNDM achieves superior performance compared to
conventional U-Net and transformer-based baselines, yielding improvements in
Dice coefficient and Hausdorff distance metrics. Moreover, the diffusion-guided
refinement enhances robustness across modalities and tumor subregions. The
proposed NNDM establishes a new direction for combining deterministic
segmentation networks with stochastic diffusion models, advancing the state of
the art in automated brain tumor analysis.

</details>


### [79] [Adaptive Fusion Network with Temporal-Ranked and Motion-Intensity Dynamic Images for Micro-expression Recognition](https://arxiv.org/abs/2510.09730)
*Thi Bich Phuong Man,Luu Tu Nguyen,Vu Tram Anh Khuong,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的微表情识别（MER）方法，通过引入两种互补的动态图像表示（时间排序和运动强度）和一个自适应融合网络，显著提升了微表情识别的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 微表情是微妙、短暂且强度极低的脸部变化，肉眼几乎难以察觉，却能揭示真实的内在情感。它们在测谎、行为分析和心理评估中具有巨大价值，因此开发有效的微表情识别方法至关重要。

Method: 该方法包含两个主要贡献：1) 提出了两种互补表示——“时间排序动态图像”强调时间演变，“运动强度动态图像”通过结合运动强度的帧重排序机制突出微弱运动。2) 提出了一种“自适应融合网络”（AFN），该网络能自动学习如何最佳地整合这两种表示，从而增强判别性微表情特征并抑制噪声。

Result: 在三个基准数据集上的实验结果表明了所提方法的优越性。具体来说，AFN在CASME-II上达到了93.95的准确率和0.897的UF1分数，创下了新的最先进基准；在SAMM上，该方法取得了82.47的准确率和0.665的UF1分数，显示出更平衡的跨类别识别能力；在MMEW上，模型达到了76.00的准确率，进一步证实了其泛化能力。结果表明输入表示和所提出的网络架构在提高MER性能方面都发挥了重要作用。

Conclusion: 本文提出的方法显著提高了微表情识别的性能，并为情感计算、测谎和人机交互领域的进一步研究和实际应用奠定了坚实基础。

Abstract: Micro-expressions (MEs) are subtle, transient facial changes with very low
intensity, almost imperceptible to the naked eye, yet they reveal a person
genuine emotion. They are of great value in lie detection, behavioral analysis,
and psychological assessment. This paper proposes a novel MER method with two
main contributions. First, we propose two complementary representations -
Temporal-ranked dynamic image, which emphasizes temporal progression, and
Motion-intensity dynamic image, which highlights subtle motions through a frame
reordering mechanism incorporating motion intensity. Second, we propose an
Adaptive fusion network, which automatically learns to optimally integrate
these two representations, thereby enhancing discriminative ME features while
suppressing noise. Experiments on three benchmark datasets (CASME-II, SAMM and
MMEW) demonstrate the superiority of the proposed method. Specifically, AFN
achieves 93.95 Accuracy and 0.897 UF1 on CASME-II, setting a new
state-of-the-art benchmark. On SAMM, the method attains 82.47 Accuracy and
0.665 UF1, demonstrating more balanced recognition across classes. On MMEW, the
model achieves 76.00 Accuracy, further confirming its generalization ability.
The obtained results show that both the input and the proposed architecture
play important roles in improving the performance of MER. Moreover, they
provide a solid foundation for further research and practical applications in
the fields of affective computing, lie detection, and human-computer
interaction.

</details>


### [80] [Multi Camera Connected Vision System with Multi View Analytics: A Comprehensive Survey](https://arxiv.org/abs/2510.09731)
*Muhammad Munsif,Waqas Ahmad,Amjid Ali,Mohib Ullah,Adnan Hussain,Sung Wook Baik*

Main category: cs.CV

TL;DR: 这篇综述首次全面整合了多视角多摄像机（MVMC）跟踪、重识别（Re-ID）和动作理解（AU）任务，提出了一个独特的分类法，总结了现有技术，并探讨了未来研究方向和新兴技术，以推动互联视觉系统（CVS）的发展。


<details>
  <summary>Details</summary>
Motivation: 互联视觉系统（CVS）在自动驾驶、智慧城市等领域具有巨大潜力，但其在真实动态环境中部署面临遮挡、视角多样性和环境多变性等挑战。现有综述主要关注孤立任务或单视角设置，忽视了多摄像机协作和多视角数据分析的复杂性和机遇，缺乏对MVMC跟踪、Re-ID和AU的综合集成回顾。

Method: 本综述提出了一个独特的CVS分类法，将其分为MVMC跟踪、Re-ID、AU和组合方法四个关键部分。系统地整理并总结了最先进的数据集、方法、结果和评估指标。此外，还识别并讨论了开放的研究问题、挑战以及终身学习、隐私和联邦学习等新兴技术。

Result: 本综述提供了首个全面整合MVMC跟踪、Re-ID和AU的统一框架，系统地展示了该领域的发展进程。它揭示了CVS在复杂现实应用中面临的关键挑战，并强调了终身学习、隐私和联邦学习等新兴技术的重要性。

Conclusion: 论文总结了提升CVS在复杂现实应用中鲁棒性、效率和适应性的关键研究方向，旨在激发创新解决方案，并指导未来研究，以开发下一代智能自适应CVS。

Abstract: Connected Vision Systems (CVS) are transforming a variety of applications,
including autonomous vehicles, smart cities, surveillance, and human-robot
interaction. These systems harness multi-view multi-camera (MVMC) data to
provide enhanced situational awareness through the integration of MVMC
tracking, re-identification (Re-ID), and action understanding (AU). However,
deploying CVS in real-world, dynamic environments presents a number of
challenges, particularly in addressing occlusions, diverse viewpoints, and
environmental variability. Existing surveys have focused primarily on isolated
tasks such as tracking, Re-ID, and AU, often neglecting their integration into
a cohesive system. These reviews typically emphasize single-view setups,
overlooking the complexities and opportunities provided by multi-camera
collaboration and multi-view data analysis. To the best of our knowledge, this
survey is the first to offer a comprehensive and integrated review of MVMC that
unifies MVMC tracking, Re-ID, and AU into a single framework. We propose a
unique taxonomy to better understand the critical components of CVS, dividing
it into four key parts: MVMC tracking, Re-ID, AU, and combined methods. We
systematically arrange and summarize the state-of-the-art datasets,
methodologies, results, and evaluation metrics, providing a structured view of
the field's progression. Furthermore, we identify and discuss the open research
questions and challenges, along with emerging technologies such as lifelong
learning, privacy, and federated learning, that need to be addressed for future
advancements. The paper concludes by outlining key research directions for
enhancing the robustness, efficiency, and adaptability of CVS in complex,
real-world applications. We hope this survey will inspire innovative solutions
and guide future research toward the next generation of intelligent and
adaptive CVS.

</details>


### [81] [Constructive Distortion: Improving MLLMs with Attention-Guided Image Warping](https://arxiv.org/abs/2510.09741)
*Dwip Dalal,Gautam Vashishtha,Utkarsh Mishra,Jeonghwan Kim,Madhav Kanda,Hyeonjeong Ha,Svetlana Lazebnik,Heng Ji,Unnat Jain*

Main category: cs.CV

TL;DR: AttWarp是一种轻量级方法，通过注意力引导的图像变形，在测试时重新分配多模态大语言模型（MLLMs）的输入图像分辨率，优先处理查询相关内容，从而提高模型对细节和空间关系的理解，无需修改模型权重或架构。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在处理杂乱场景时，常会忽略微小细节和空间关系，导致在细粒度感知定位上出现错误。

Method: AttWarp是一种在测试时使用的轻量级方法。它利用MLLM的跨模态注意力机制，对输入图像进行直线变形（rectilinear warping）。这种变形将更多分辨率分配给与查询相关的内容，同时压缩信息较少的区域，同时保持全局上下文。该方法不改变模型权重或架构。

Result: AttWarp在五个基准测试（TextVQA, GQA, DocVQA, POPE, MMMU）和四种MLLM（LLaVA, Qwen-VL, InternVL, InstructBLIP）上持续提高了准确性，增强了组合推理能力，并减少了幻觉，优于四种竞争性的测试时图像处理基线方法。

Conclusion: 注意力引导的图像变形能够优先处理与查询相关的信息，同时保留全局上下文，结果表明相同的MLLM在接收这种变形后的输入时表现更佳。

Abstract: Multimodal large language models (MLLMs) often miss small details and spatial
relations in cluttered scenes, leading to errors in fine-grained perceptual
grounding. We introduce AttWarp, a lightweight method that allocates more
resolution to query-relevant content while compressing less informative areas,
all while preserving global context. At test time, the approach uses an MLLM's
cross-modal attention to perform rectilinear warping of the input image,
reallocating spatial resolution toward regions the model deems important,
without changing model weights or architecture. This attention-guided warping
preserves all original image information but redistributes it non-uniformly, so
small objects and subtle relationships become easier for the same model to read
while the global layout remains intact. Across five benchmarks (TextVQA, GQA,
DocVQA, POPE, MMMU) and four MLLMs (LLaVA, Qwen-VL, InternVL, and
InstructBLIP), AttWarp consistently improves accuracy, strengthens
compositional reasoning, and reduces hallucinations, outperforming four
competitive baselines that manipulate raw images at test time. Together, these
results show that attention-guided warping prioritizes information relevant to
the query while preserving context, and that the same MLLMs perform better when
given such warped inputs.

</details>


### [82] [Towards Understanding Ambiguity Resolution in Multimodal Inference of Meaning](https://arxiv.org/abs/2510.09815)
*Yufei Wang,Adriana Kovashka,Loretta Fernández,Marc N. Coutanche,Seth Wiener*

Main category: cs.CV

TL;DR: 本研究探索了一种新的外语学习环境，即学习者在描述配对图像的句子多模态语境中推断生词含义，并分析了影响人类表现的特征以及AI系统对此表现的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究者希望探索一种新的外语学习方式，即利用图像和文本的多模态语境来帮助学习者推断陌生词汇的含义。

Method: 研究方法包括：1) 使用不同的图像-文本对与人类参与者进行研究。2) 分析数据（图像和文本）的特征，以确定哪些特征有助于参与者推断被遮蔽或不熟悉词汇的含义。3) 分析参与者的语言背景与成功率之间的相关性。4) 分析AI系统推理参与者表现的能力。

Result: 研究发现：1) 只有少数直观特征与参与者的表现有强相关性，这表明需要进一步研究预测成功的特征。2) 发现了改进AI系统推理参与者表现能力的有前景的方向。

Conclusion: 本研究引入了一种新的多模态外语学习范式，并发现影响人类表现的预测特征仍需深入探索。同时，研究也为提升AI系统对人类学习表现的推理能力指明了未来方向。

Abstract: We investigate a new setting for foreign language learning, where learners
infer the meaning of unfamiliar words in a multimodal context of a sentence
describing a paired image. We conduct studies with human participants using
different image-text pairs. We analyze the features of the data (i.e., images
and texts) that make it easier for participants to infer the meaning of a
masked or unfamiliar word, and what language backgrounds of the participants
correlate with success. We find only some intuitive features have strong
correlations with participant performance, prompting the need for further
investigating of predictive features for success in these tasks. We also
analyze the ability of AI systems to reason about participant performance, and
discover promising future directions for improving this reasoning ability.

</details>


### [83] [Task-Aware Resolution Optimization for Visual Large Language Models](https://arxiv.org/abs/2510.09822)
*Weiqing Luo,Zhen Tan,Yifan Li,Xinyu Zhao,Kwonjoon Lee,Behzad Dariush,Tianlong Chen*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型（VLLM）中不同视觉语言任务的分辨率偏好，提出了一种基于图像复杂度和模型不确定性方差来确定最佳分辨率的经验公式，并开发了一种参数高效的微调技术，以将预训练VLLM的视觉输入分辨率扩展到该最佳分辨率，从而提高了性能。


<details>
  <summary>Details</summary>
Motivation: 大多数现有VLLM（如LLaVA）预设固定分辨率用于下游任务，导致性能不佳，无法满足现实世界视觉语言应用对不同感知粒度的需求。

Method: ['首次全面调查不同视觉语言任务的分辨率偏好，揭示了分辨率偏好与图像复杂性以及VLLM在不同输入分辨率下的不确定性方差之间的关联。', '基于上述发现，提出了一个结合图像复杂性和VLLM不确定性方差的经验公式，以确定给定视觉语言任务的最佳分辨率。', '基于严格实验，提出了一种新颖的参数高效微调技术，用于将预训练VLLM的视觉输入分辨率扩展到所确定的最佳分辨率。']

Result: 在各种视觉语言任务上的大量实验验证了所提出方法的有效性。

Conclusion: 通过确定并适应最佳视觉输入分辨率，该方法有效解决了VLLM固定分辨率的局限性，显著提高了模型在不同视觉语言任务中的性能。

Abstract: Real-world vision-language applications demand varying levels of perceptual
granularity. However, most existing visual large language models (VLLMs), such
as LLaVA, pre-assume a fixed resolution for downstream tasks, which leads to
subpar performance. To address this problem, we first conduct a comprehensive
and pioneering investigation into the resolution preferences of different
vision-language tasks, revealing a correlation between resolution preferences
with image complexity, and uncertainty variance of the VLLM at different image
input resolutions. Building on this insight, we propose an empirical formula to
determine the optimal resolution for a given vision-language task, combining
these two factors. Second, based on rigorous experiments, we propose a novel
parameter-efficient fine-tuning technique to extend the visual input resolution
of pre-trained VLLMs to the identified optimal resolution. Extensive
experiments on various vision-language tasks validate the effectiveness of our
method.

</details>


### [84] [Post Processing of image segmentation using Conditional Random Fields](https://arxiv.org/abs/2510.09833)
*Aashish Dhawan,Pankaj Bodani,Vishal Garg*

Main category: cs.CV

TL;DR: 本研究旨在通过寻找合适的条件随机场（CRF）模型，提高因特征质量低导致不清晰的卫星图像分割结果的清晰度，并评估了不同CRFs在不同质量图像上的表现。


<details>
  <summary>Details</summary>
Motivation: 卫星图像分割结果因图像特征质量低而通常不清晰，因此需要找到一种合适的方法来提高分割图像的清晰度。

Method: 研究了不同类型的条件随机场（CRF）模型，分析了它们的适用性；实验了多种CRF模型；在低质量卫星图像和高质量航空照片两个数据集上评估了方法；比较了不同CRF方法在这些数据集上的表现，以揭示其优缺点。

Result: 实验发现并确定了在图像上表现最佳的CRF模型；通过比较不同CRF方法在不同数据集上的结果，揭示了它们的优缺点和潜力。

Conclusion: 通过对各种CRF的实验和比较，找到了最适合提高分割图像清晰度的CRF模型，并明确了不同CRF方法在处理不同质量图像时的优缺点。

Abstract: The output of image the segmentation process is usually not very clear due to
low quality features of Satellite images. The purpose of this study is to find
a suitable Conditional Random Field (CRF) to achieve better clarity in a
segmented image. We started with different types of CRFs and studied them as to
why they are or are not suitable for our purpose. We evaluated our approach on
two different datasets - Satellite imagery having low quality features and high
quality Aerial photographs. During the study we experimented with various CRFs
to find which CRF gives the best results on images and compared our results on
these datasets to show the pitfalls and potentials of different approaches.

</details>


### [85] [Exploration of Incremental Synthetic Non-Morphed Images for Single Morphing Attack Detection](https://arxiv.org/abs/2510.09836)
*David Benavente-Rios,Juan Ruiz Rodriguez,Gustavo Gatica*

Main category: cs.CV

TL;DR: 本研究探讨了使用合成人脸数据增强单变形攻击检测（S-MAD）的有效性。结果表明，谨慎地整合合成数据或逐步添加真实数据可改善泛化能力，但过度或仅使用合成数据会导致性能下降。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题，难以获取大规模的真实人脸图像数据集，这限制了单变形攻击检测（S-MAD）系统的开发和性能。

Method: 研究使用了多种变形工具和跨数据集评估方案。采用了一种增量测试协议，逐步添加合成图像以评估泛化能力。

Result: 实验结果表明，通过谨慎地将受控数量的合成图像整合到现有数据集中，或在训练过程中逐步添加真实图像，可以提高泛化能力。然而，不加区分地使用合成数据会导致次优性能。更重要的是，仅使用合成数据（包括变形和非变形图像）会达到最高的等错误率（EER）。

Conclusion: 在实际操作场景中，最佳选择不是仅仅依靠合成数据进行单变形攻击检测（S-MAD）。谨慎地整合合成数据可以带来益处，但过度或独立使用合成数据是不可取的。

Abstract: This paper investigates the use of synthetic face data to enhance
Single-Morphing Attack Detection (S-MAD), addressing the limitations of
availability of large-scale datasets of bona fide images due to privacy
concerns. Various morphing tools and cross-dataset evaluation schemes were
utilized to conduct this study. An incremental testing protocol was implemented
to assess the generalization capabilities as more and more synthetic images
were added. The results of the experiments show that generalization can be
improved by carefully incorporating a controlled number of synthetic images
into existing datasets or by gradually adding bona fide images during training.
However, indiscriminate use of synthetic data can lead to sub-optimal
performance. Evenmore, the use of only synthetic data (morphed and non-morphed
images) achieves the highest Equal Error Rate (EER), which means in operational
scenarios the best option is not relying only on synthetic data for S-MAD.

</details>


### [86] [Explainable Human-in-the-Loop Segmentation via Critic Feedback Signals](https://arxiv.org/abs/2510.09945)
*Pouya Shaeri,Ryan T. Woo,Yasaman Mohammadpour,Ariane Middel*

Main category: cs.CV

TL;DR: 该论文提出了一种人机交互式干预学习框架，通过有针对性的人工校正来纠正分割模型对虚假关联的依赖，从而提高模型在真实世界中的鲁棒性和泛化能力，并显著减少标注工作量。


<details>
  <summary>Details</summary>
Motivation: 分割模型在基准测试中表现出色，但在真实世界中常因依赖虚假关联而非真实物体边界而失效，需要一种方法来纠正这些失败模式。

Method: 该方法将人工校正视为干预信号，指示模型何时不应依赖表面特征。系统通过在视觉相似图像间传播这些校正信息来学习，引导模型转向鲁棒、语义有意义的特征。与传统标注不同，它明确识别模型失败的原因并系统性地纠正整个数据集中的这些失败模式。

Result: 该框架在挑战性的立方体地图数据上将分割精度（mIoU）提高了多达9个百分点（相对提升12-15%），与标准再训练相比，标注工作量减少了3-4倍，同时在基准数据集上保持了有竞争力的性能。

Conclusion: 该工作提供了一个实用的框架，用于构建准确、对数据集偏差鲁棒、数据高效且适应真实世界领域（如城市气候监测和自动驾驶）的分割系统。

Abstract: Segmentation models achieve high accuracy on benchmarks but often fail in
real-world domains by relying on spurious correlations instead of true object
boundaries. We propose a human-in-the-loop interactive framework that enables
interventional learning through targeted human corrections of segmentation
outputs. Our approach treats human corrections as interventional signals that
show when reliance on superficial features (e.g., color or texture) is
inappropriate. The system learns from these interventions by propagating
correction-informed edits across visually similar images, effectively steering
the model toward robust, semantically meaningful features rather than
dataset-specific artifacts. Unlike traditional annotation approaches that
simply provide more training data, our method explicitly identifies when and
why the model fails and then systematically corrects these failure modes across
the entire dataset. Through iterative human feedback, the system develops
increasingly robust representations that generalize better to novel domains and
resist artifactual correlations. We demonstrate that our framework improves
segmentation accuracy by up to 9 mIoU points (12-15\% relative improvement) on
challenging cubemap data and yields 3-4$\times$ reductions in annotation effort
compared to standard retraining, while maintaining competitive performance on
benchmark datasets. This work provides a practical framework for researchers
and practitioners seeking to build segmentation systems that are accurate,
robust to dataset biases, data-efficient, and adaptable to real-world domains
such as urban climate monitoring and autonomous driving.

</details>


### [87] [Cell Instance Segmentation: The Devil Is in the Boundaries](https://arxiv.org/abs/2510.09848)
*Peixian Liang,Yifan Ding,Yizhe Zhang,Jianxu Chen,Hao Zheng,Hongxiao Wang,Yejia Zhang,Guangyu Meng,Tim Weninger,Michael Niemier,X. Sharon Hu,Danny Z Chen*

Main category: cs.CV

TL;DR: 本文提出了一种名为Ceb（细胞边界）的新型像素聚类方法，通过利用细胞边界特征和标签来将前景像素划分为细胞实例，解决了现有方法在细胞实例分割中丢失几何特性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的细胞实例分割SOTA方法将实例信息分解为像素级目标（如距离图、热扩散图），这可能导致细胞实例重要的几何特性（如形状、曲率、凸度）的丢失，因为这些特性需要像素集合来表示。

Method: Ceb方法首先从语义分割生成的概率图中提取潜在的前景-前景边界（通过改进的Watershed算法）。接着，为每个边界候选构建一个“边界签名”（通过对当前和相邻边界的像素进行采样）。然后，使用边界分类器根据边界签名预测其二元边界标签。最后，根据预测的边界标签划分或合并相邻区域以获得细胞实例。

Result: 在六个数据集上的大量实验表明，Ceb优于现有的基于语义分割概率图的像素聚类方法。此外，Ceb与SOTA细胞实例分割方法相比，也取得了极具竞争力的性能。

Conclusion: Ceb通过有效利用细胞边界特征和标签进行像素聚类，成功解决了细胞实例分割中几何特性丢失的挑战，显著提升了分割性能，并与现有SOTA方法持平。

Abstract: State-of-the-art (SOTA) methods for cell instance segmentation are based on
deep learning (DL) semantic segmentation approaches, focusing on distinguishing
foreground pixels from background pixels. In order to identify cell instances
from foreground pixels (e.g., pixel clustering), most methods decompose
instance information into pixel-wise objectives, such as distances to
foreground-background boundaries (distance maps), heat gradients with the
center point as heat source (heat diffusion maps), and distances from the
center point to foreground-background boundaries with fixed angles (star-shaped
polygons). However, pixel-wise objectives may lose significant geometric
properties of the cell instances, such as shape, curvature, and convexity,
which require a collection of pixels to represent. To address this challenge,
we present a novel pixel clustering method, called Ceb (for Cell boundaries),
to leverage cell boundary features and labels to divide foreground pixels into
cell instances. Starting with probability maps generated from semantic
segmentation, Ceb first extracts potential foreground-foreground boundaries
with a revised Watershed algorithm. For each boundary candidate, a boundary
feature representation (called boundary signature) is constructed by sampling
pixels from the current foreground-foreground boundary as well as the
neighboring background-foreground boundaries. Next, a boundary classifier is
used to predict its binary boundary label based on the corresponding boundary
signature. Finally, cell instances are obtained by dividing or merging
neighboring regions based on the predicted boundary labels. Extensive
experiments on six datasets demonstrate that Ceb outperforms existing pixel
clustering methods on semantic segmentation probability maps. Moreover, Ceb
achieves highly competitive performance compared to SOTA cell instance
segmentation methods.

</details>


### [88] [Cluster-Aware Prompt Ensemble Learning for Few-Shot Vision-Language Model Adaptation](https://arxiv.org/abs/2510.09867)
*Zhi Chen,Xin Yu,Xiaohui Tao,Yan Li,Zi Huang*

Main category: cs.CV

TL;DR: 针对视觉-语言模型中传统提示集成（特征平均）的次优问题，本文提出了聚类感知提示集成学习（CAPEL）框架，通过在分类logits空间进行集成，并引入聚类保留正则化和自适应提示加权技术，以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（如CLIP）通过集成上下文提示来表示类别，但传统的特征平均方法会使类别中心偏离真实分布，导致次优结果。

Method: 本文提出了聚类感知提示集成学习（CAPEL）框架：1) 在分类logits空间而非特征空间进行提示集成，以更好地与视觉特征分布对齐。2) 引入聚类保留正则化项，以维持提示的独特性和聚类特异性判别能力。3) 集成自适应提示加权技术，动态调整对有缺陷或模糊提示的注意力权重。

Result: CAPEL框架通过保留上下文提示的聚类特性，能够更好地与视觉特征分布对齐，并保持聚类特定的判别能力，防止提示崩溃为单一方向，同时通过自适应加权确保在多样数据集和任务中实现鲁棒的性能。

Conclusion: CAPEL通过在logits空间进行集成、引入聚类保留正则化和自适应加权，提供了一种更有效的提示集成策略，解决了传统方法中特征平均的局限性，从而提升了视觉-语言模型的性能。

Abstract: Vision-language models (VLMs) such as CLIP achieve zero-shot transfer across
various tasks by pre-training on numerous image-text pairs. These models often
benefit from using an ensemble of context prompts to represent a class. Despite
being effective, conventional prompt ensembling that averages textual features
of context prompts often yields suboptimal results. This is because feature
averaging shifts the class centroids away from the true class distribution. To
address this issue, we propose the Cluster-Aware Prompt Ensemble Learning
(CAPEL) framework, which preserves the cluster nature of context prompts. CAPEL
classifies images into one of several class clusters, each represented by a
distinct prompt. Instead of ensembling prompts in the feature space, we perform
ensembling in the classification logits space, aligning better with the visual
feature distribution. To further optimize prompt fine-tuning while maintaining
cluster-specific discriminative power, we introduce a cluster-preserving
regularization term. This ensures that prompts remain distinct and specialized
for different clusters, preventing collapse into a uniform direction.
Additionally, we integrate an adaptive prompt weighting technique to
dynamically adjust the attention weights for flawed or ambiguous prompts,
ensuring robust performance across diverse datasets and tasks.

</details>


### [89] [Scaling Traffic Insights with AI and Language Model-Powered Camera Systems for Data-Driven Transportation Decision Making](https://arxiv.org/abs/2510.09981)
*Fan Zuo,Donglin Zhou,Jingqin Gao,Kaan Ozbay*

Main category: cs.CV

TL;DR: 本研究提出一个端到端的人工智能框架，利用现有交通摄像头进行高分辨率、大规模的交通监测和分析，解决了传统方法的局限性，并通过纽约市拥堵收费项目的数据进行了验证。


<details>
  <summary>Details</summary>
Motivation: 准确、可扩展的交通监测对于实时和长期交通管理至关重要，尤其是在自然灾害、大型建设项目或政策变化（如纽约市拥堵收费计划）等中断期间。然而，由于高昂的安装、维护和数据管理成本，传感器部署受到限制。现有视频分析技术难以应对动态摄像头视角和海量数据。

Method: 本研究提出了一个端到端的AI框架：1) 利用现有交通摄像头基础设施。2) 使用经过微调的YOLOv11模型，在本地化城市场景上训练，实时提取多模态交通密度和分类指标。3) 引入一种新颖的基于图的视角归一化方法，以解决非固定式云台变焦摄像头的不一致性。4) 集成了领域特定的大型语言模型（LLM），处理24/7视频流中的海量数据，自动生成交通模式演变的摘要。5) 实验表明，基于示例的提示能提高LLM的数值准确性并减少幻觉。

Result: 该系统在纽约市2025年拥堵收费计划早期实施期间，使用来自约1000个交通摄像头的900多万张图像进行了验证。结果显示：1) 拥堵缓解区内工作日乘用车密度下降9%。2) 卡车流量早期减少，并有反弹迹象。3) 走廊和区域尺度的行人和骑自行车者活动持续增加。4) 基于示例的提示提高了LLM的数值准确性并减少了幻觉。

Conclusion: 该框架展示了其作为一种实用、基础设施就绪的解决方案的潜力，能够以最少的人工干预进行大规模、政策相关的交通监测。

Abstract: Accurate, scalable traffic monitoring is critical for real-time and long-term
transportation management, particularly during disruptions such as natural
disasters, large construction projects, or major policy changes like New York
City's first-in-the-nation congestion pricing program. However, widespread
sensor deployment remains limited due to high installation, maintenance, and
data management costs. While traffic cameras offer a cost-effective
alternative, existing video analytics struggle with dynamic camera viewpoints
and massive data volumes from large camera networks. This study presents an
end-to-end AI-based framework leveraging existing traffic camera infrastructure
for high-resolution, longitudinal analysis at scale. A fine-tuned YOLOv11
model, trained on localized urban scenes, extracts multimodal traffic density
and classification metrics in real time. To address inconsistencies from
non-stationary pan-tilt-zoom cameras, we introduce a novel graph-based
viewpoint normalization method. A domain-specific large language model was also
integrated to process massive data from a 24/7 video stream to generate
frequent, automated summaries of evolving traffic patterns, a task far
exceeding manual capabilities. We validated the system using over 9 million
images from roughly 1,000 traffic cameras during the early rollout of NYC
congestion pricing in 2025. Results show a 9% decline in weekday passenger
vehicle density within the Congestion Relief Zone, early truck volume
reductions with signs of rebound, and consistent increases in pedestrian and
cyclist activity at corridor and zonal scales. Experiments showed that
example-based prompts improved LLM's numerical accuracy and reduced
hallucinations. These findings demonstrate the framework's potential as a
practical, infrastructure-ready solution for large-scale, policy-relevant
traffic monitoring with minimal human intervention.

</details>


### [90] [Uncertainty-Aware Post-Detection Framework for Enhanced Fire and Smoke Detection in Compact Deep Learning Models](https://arxiv.org/abs/2510.10108)
*Aniruddha Srinivas Joshi,Godwyn James William,Shreyas Srinivas Joshi*

Main category: cs.CV

TL;DR: 本文提出了一种不确定性感知的后检测框架，通过结合统计不确定性和领域相关视觉线索来重新调整检测置信度，以提高紧凑型深度学习模型（如YOLOv5n/v8n）在火灾和烟雾检测中的精度和鲁棒性，而无需修改基础模型。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉的火灾和烟雾检测方法难以平衡效率和可靠性。紧凑型深度学习模型（如YOLOv5n和YOLOv8n）在部署时因容量限制常导致误报和漏检。传统的后检测方法（如NMS和Soft-NMS）仅依赖空间重叠，在杂乱或模糊的火灾场景中效果不佳。

Method: 提出了一种不确定性感知的后检测框架。该框架利用统计不确定性和领域相关的视觉线索（颜色、边缘、纹理特征）重新调整检测置信度。通过一个轻量级的置信度优化网络（Confidence Refinement Network），将不确定性估计与这些视觉特征结合，在不修改基础模型的情况下调整检测分数。

Result: 在D-Fire数据集上的实验表明，与现有基线相比，该方法在精度、召回率和平均精度均值（mAP）方面有所提高，且计算开销适中。

Conclusion: 这些结果强调了后检测重评分在增强紧凑型深度学习模型在真实火灾和烟雾检测中鲁棒性的有效性。

Abstract: Accurate fire and smoke detection is critical for safety and disaster
response, yet existing vision-based methods face challenges in balancing
efficiency and reliability. Compact deep learning models such as YOLOv5n and
YOLOv8n are widely adopted for deployment on UAVs, CCTV systems, and IoT
devices, but their reduced capacity often results in false positives and missed
detections. Conventional post-detection methods such as Non-Maximum Suppression
and Soft-NMS rely only on spatial overlap, which can suppress true positives or
retain false alarms in cluttered or ambiguous fire scenes. To address these
limitations, we propose an uncertainty aware post-detection framework that
rescales detection confidences using both statistical uncertainty and domain
relevant visual cues. A lightweight Confidence Refinement Network integrates
uncertainty estimates with color, edge, and texture features to adjust
detection scores without modifying the base model. Experiments on the D-Fire
dataset demonstrate improved precision, recall, and mean average precision
compared to existing baselines, with only modest computational overhead. These
results highlight the effectiveness of post-detection rescoring in enhancing
the robustness of compact deep learning models for real-world fire and smoke
detection.

</details>


### [91] [CHUG: Crowdsourced User-Generated HDR Video Quality Dataset](https://arxiv.org/abs/2510.09879)
*Shreshth Saini,Alan C. Bovik,Neil Birkbeck,Yilin Wang,Balu Adsumilli*

Main category: cs.CV

TL;DR: 本文介绍了CHUG，一个大规模的众包用户生成内容（UGC）HDR视频质量评估数据集，旨在填补现有HDR-VQA数据集主要关注专业生成内容（PGC）的空白。


<details>
  <summary>Details</summary>
Motivation: HDR视频提升了视觉体验，但YouTube和TikTok等平台上的UGC-HDR视频因多样化的拍摄条件、编辑伪影和压缩失真，对质量评估提出了独特挑战。现有HDR-VQA数据集主要关注PGC，未能涵盖真实世界UGC-HDR的退化，因此需要一个专门针对UGC-HDR的质量评估数据集。

Method: 研究者创建了CHUG数据集，包含856个UGC-HDR源视频，通过多分辨率和比特率转码模拟真实世界场景，共计5,992个视频。通过Amazon Mechanical Turk进行了大规模研究，收集了211,848个感知评分。

Result: CHUG数据集是第一个大规模的UGC-HDR质量主观研究数据集，提供了分析HDR视频中UGC特有失真的基准。该数据集包含856个UGC-HDR源视频、5,992个转码视频以及211,848个感知评分。

Conclusion: CHUG数据集将通过提供一个大规模、多样化且真实的UGC数据集，推动无参考（NR）HDR-VQA研究的进展。

Abstract: High Dynamic Range (HDR) videos enhance visual experiences with superior
brightness, contrast, and color depth. The surge of User-Generated Content
(UGC) on platforms like YouTube and TikTok introduces unique challenges for HDR
video quality assessment (VQA) due to diverse capture conditions, editing
artifacts, and compression distortions. Existing HDR-VQA datasets primarily
focus on professionally generated content (PGC), leaving a gap in understanding
real-world UGC-HDR degradations. To address this, we introduce CHUG:
Crowdsourced User-Generated HDR Video Quality Dataset, the first large-scale
subjective study on UGC-HDR quality. CHUG comprises 856 UGC-HDR source videos,
transcoded across multiple resolutions and bitrates to simulate real-world
scenarios, totaling 5,992 videos. A large-scale study via Amazon Mechanical
Turk collected 211,848 perceptual ratings. CHUG provides a benchmark for
analyzing UGC-specific distortions in HDR videos. We anticipate CHUG will
advance No-Reference (NR) HDR-VQA research by offering a large-scale, diverse,
and real-world UGC dataset. The dataset is publicly available at:
https://shreshthsaini.github.io/CHUG/.

</details>


### [92] [Fast Self-Supervised depth and mask aware Association for Multi-Object Tracking](https://arxiv.org/abs/2510.09878)
*Milad Khanchi,Maria Amer,Charalambos Poullis*

Main category: cs.CV

TL;DR: 该论文提出一种多目标跟踪（MOT）方法，通过自监督编码器融合深度和分割掩码特征来生成稳定的目标表示，作为额外的相似性线索进行关联，在不计算掩码IoU的情况下，在复杂基准测试上超越了现有最先进的跟踪-通过-检测（TBD）方法。


<details>
  <summary>Details</summary>
Motivation: 多目标跟踪中，基于IoU的关联在目标相似或被遮挡时变得不可靠，且计算分割掩码的IoU成本高昂。因此，需要更稳定、更有效的目标表示来改进关联。

Method: 该方法利用零样本深度估计器获取深度图和可提示视觉分割模型获取对象掩码，以捕获精细的空间线索。它融合深度和掩码特征，并通过一个紧凑的自监督编码器进行训练，生成稳定的目标表示。这些表示与边界框IoU和重识别特征一起，作为额外的相似性线索用于匹配。该方法是一种跟踪-通过-检测（TBD）模型，并且是首个使用自监督编码器在不计算掩码IoU的情况下细化分割掩码的MOT方法。

Result: 该TBD方法在具有非线性运动、遮挡和拥挤场景的挑战性基准测试（如SportsMOT和DanceTrack）上，在大多数指标上优于TBD领域最先进的方法。在具有线性运动的简单基准测试（如MOT17）上，也取得了有竞争力的性能。

Conclusion: 通过融合深度和分割掩码特征并利用自监督编码器生成稳定的目标表示，该MOT方法有效解决了传统IoU关联的局限性，特别是在复杂场景下显著提高了跟踪性能，同时避免了计算分割掩码IoU的开销。

Abstract: Multi-object tracking (MOT) methods often rely on Intersection-over-Union
(IoU) for association. However, this becomes unreliable when objects are
similar or occluded. Also, computing IoU for segmentation masks is
computationally expensive. In this work, we use segmentation masks to capture
object shapes, but we do not compute segmentation IoU. Instead, we fuse depth
and mask features and pass them through a compact encoder trained
self-supervised. This encoder produces stable object representations, which we
use as an additional similarity cue alongside bounding box IoU and
re-identification features for matching. We obtain depth maps from a zero-shot
depth estimator and object masks from a promptable visual segmentation model to
obtain fine-grained spatial cues. Our MOT method is the first to use the
self-supervised encoder to refine segmentation masks without computing masks
IoU. MOT can be divided into joint detection-ReID (JDR) and
tracking-by-detection (TBD) models. The latter are computationally more
efficient. Experiments of our TBD method on challenging benchmarks with
non-linear motion, occlusion, and crowded scenes, such as SportsMOT and
DanceTrack, show that our method outperforms the TBD state-of-the-art on most
metrics, while achieving competitive performance on simpler benchmarks with
linear motion, such as MOT17.

</details>


### [93] [YOLOv11-Litchi: Efficient Litchi Fruit Detection based on UAV-Captured Agricultural Imagery in Complex Orchard Environments](https://arxiv.org/abs/2510.10141)
*Hongxing Peng,Haopei Xie,Weijia Lia,Huanai Liuc,Ximing Li*

Main category: cs.CV

TL;DR: 本文提出了YOLOv11-Litchi模型，一个基于YOLOv11的轻量级且鲁棒的荔枝检测模型，专门用于无人机航拍图像。该模型通过多尺度残差模块、轻量级特征融合和荔枝遮挡检测头解决了小目标、大模型参数和遮挡等问题，显著提高了检测精度和速度，同时减小了模型尺寸。


<details>
  <summary>Details</summary>
Motivation: 传统的荔枝人工筛选方法已无法满足现代生产需求，效率低下且成本高昂。将无人机航拍图像与深度学习结合被视为一种有前景的解决方案，但现有方法在处理小目标、模型部署参数过大以及目标频繁遮挡等挑战时面临困难，促使研究人员开发更高效、更轻量级的检测模型。

Method: 本文在YOLOv11框架基础上构建了YOLOv11-Litchi模型，并融入了三项主要创新：1) 多尺度残差模块，用于改善跨尺度的上下文特征提取；2) 轻量级特征融合方法，旨在在保持高精度的同时减小模型尺寸和计算成本；3) 荔枝遮挡检测头，通过强调目标区域并抑制背景干扰来减轻遮挡效应。

Result: 实验结果表明，YOLOv11-Litchi模型的参数大小为6.35 MB，比YOLOv11基线小32.5%。同时，其mAP提高了2.5%达到90.1%，F1-Score提高了1.4%达到85.5%。此外，该模型实现了57.2 FPS的帧率，满足了实时检测要求。

Conclusion: 研究结果证明了YOLOv11-Litchi模型在复杂果园环境中进行无人机荔枝检测的适用性，并展示了其在精准农业中更广泛应用的潜力。

Abstract: Litchi is a high-value fruit, yet traditional manual selection methods are
increasingly inadequate for modern production demands. Integrating UAV-based
aerial imagery with deep learning offers a promising solution to enhance
efficiency and reduce costs. This paper introduces YOLOv11-Litchi, a
lightweight and robust detection model specifically designed for UAV-based
litchi detection. Built upon the YOLOv11 framework, the proposed model
addresses key challenges such as small target size, large model parameters
hindering deployment, and frequent target occlusion. To tackle these issues,
three major innovations are incorporated: a multi-scale residual module to
improve contextual feature extraction across scales, a lightweight feature
fusion method to reduce model size and computational costs while maintaining
high accuracy, and a litchi occlusion detection head to mitigate occlusion
effects by emphasizing target regions and suppressing background interference.
Experimental results validate the model's effectiveness. YOLOv11-Litchi
achieves a parameter size of 6.35 MB - 32.5% smaller than the YOLOv11 baseline
- while improving mAP by 2.5% to 90.1% and F1-Score by 1.4% to 85.5%.
Additionally, the model achieves a frame rate of 57.2 FPS, meeting real-time
detection requirements. These findings demonstrate the suitability of
YOLOv11-Litchi for UAV-based litchi detection in complex orchard environments,
showcasing its potential for broader applications in precision agriculture.

</details>


### [94] [Guided Image Feature Matching using Feature Spatial Order](https://arxiv.org/abs/2510.10414)
*Chin-Hung Teng,Ben-Jian Dong*

Main category: cs.CV

TL;DR: 该论文提出了一种将特征空间顺序整合到渐进匹配框架中的方法，以提高特征匹配的效率和准确性，并通过图像对齐解决了旋转影响。


<details>
  <summary>Details</summary>
Motivation: 图像特征匹配在计算机视觉任务中至关重要，但特征点匹配（特别是对于大量特征的图像）仍然非常耗时。现有方法未能充分利用特征空间顺序来提高效率。

Method: 将特征空间顺序集成到渐进匹配框架中，利用初始匹配的特征建立空间顺序计算模型，估算后续匹配的空间范围以过滤不必要的匹配。同时，与对极几何结合以进一步提高效率和准确性。针对图像旋转对空间顺序的影响，提出了一种基于对极几何基础矩阵的图像对齐方法。

Result: 通过在标准基准数据集、自生成模拟图像和真实图像上进行实验，结果表明所提出的方法比传统方法具有显著更高的匹配效率和更准确的特征匹配。

Conclusion: 该研究成功地将特征空间顺序与对极几何相结合，并解决了图像旋转问题，显著提高了图像特征匹配的效率和准确性。

Abstract: Image feature matching plays a vital role in many computer vision tasks.
Although many image feature detection and matching techniques have been
proposed over the past few decades, it is still time-consuming to match feature
points in two images, especially for images with a large number of detected
features. Feature spatial order can estimate the probability that a pair of
features is correct. Since it is a completely independent concept from epipolar
geometry, it can be used to complement epipolar geometry in guiding feature
match in a target region so as to improve matching efficiency. In this paper,
we integrate the concept of feature spatial order into a progressive matching
framework. We use some of the initially matched features to build a
computational model of feature spatial order and employs it to calculates the
possible spatial range of subsequent feature matches, thus filtering out
unnecessary feature matches. We also integrate it with epipolar geometry to
further improve matching efficiency and accuracy. Since the spatial order of
feature points is affected by image rotation, we propose a suitable image
alignment method from the fundamental matrix of epipolar geometry to remove the
effect of image rotation. To verify the feasibility of the proposed method, we
conduct a series of experiments, including a standard benchmark dataset,
self-generated simulated images, and real images. The results demonstrate that
our proposed method is significantly more efficient and has more accurate
feature matching than the traditional method.

</details>


### [95] [Geometry-Aware Scene Configurations for Novel View Synthesis](https://arxiv.org/abs/2510.09880)
*Minkwan Kim,Changwoon Choi,Young Min Kim*

Main category: cs.CV

TL;DR: 本文提出场景自适应策略，利用几何先验和观察统计数据，优化室内环境NeRF的表示容量分配，显著提升渲染质量和内存效率。


<details>
  <summary>Details</summary>
Motivation: 室内场景（多房间）布局不规则、复杂多变，包含杂乱、遮挡和平坦墙壁，导致从不完整观测生成沉浸式体验时资源利用率低下。现有的可扩展NeRF表示采用均匀基底布置，效率不高。

Method: 1. 利用预处理阶段可获得的几何先验信息。2. 在估计的几何骨架上记录观测统计数据。3. 基于这些统计数据指导基底的最佳放置，取代均匀基底布置。4. 提出场景自适应的虚拟视角，以弥补输入轨迹中视图配置固有的几何缺陷并施加必要的正则化。

Result: 在多个大型室内场景中，渲染质量和内存需求相比采用常规布置的基线方法有显著提升，并进行了全面的分析和讨论。

Conclusion: 通过利用几何先验和场景自适应策略（包括优化基底放置和虚拟视角），可以有效提高从不完整观测生成室内环境沉浸式体验的效率和质量，显著优于传统方法。

Abstract: We propose scene-adaptive strategies to efficiently allocate representation
capacity for generating immersive experiences of indoor environments from
incomplete observations. Indoor scenes with multiple rooms often exhibit
irregular layouts with varying complexity, containing clutter, occlusion, and
flat walls. We maximize the utilization of limited resources with guidance from
geometric priors, which are often readily available after pre-processing
stages. We record observation statistics on the estimated geometric scaffold
and guide the optimal placement of bases, which greatly improves upon the
uniform basis arrangements adopted by previous scalable Neural Radiance Field
(NeRF) representations. We also suggest scene-adaptive virtual viewpoints to
compensate for geometric deficiencies inherent in view configurations in the
input trajectory and impose the necessary regularization. We present a
comprehensive analysis and discussion regarding rendering quality and memory
requirements in several large-scale indoor scenes, demonstrating significant
enhancements compared to baselines that employ regular placements.

</details>


### [96] [SceneTextStylizer: A Training-Free Scene Text Style Transfer Framework with Diffusion Model](https://arxiv.org/abs/2510.10910)
*Honghui Yuan,Keiji Yanai*

Main category: cs.CV

TL;DR: 本文提出SceneTextStylizer，一个无需训练的基于扩散模型的框架，用于实现场景文本的灵活、局部化和高保真风格迁移。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在风格迁移方面取得显著进展，但场景文本的灵活和局部化风格编辑仍是一个未解决的挑战。现有方法通常仅限于内容替换或简单风格，缺乏自由风格迁移能力。

Method: 该方法是一个无需训练的扩散模型框架，通过提示词引导对文本区域进行风格转换，同时保持文本可读性和风格一致性。它设计了一个特征注入模块（利用扩散模型反演和自注意力机制）来有效传输风格特征；引入了一个区域控制机制（在每个去噪步骤应用基于距离的变化掩码）以实现精确的空间控制；并结合了一个基于傅里叶变换的风格增强模块来丰富风格。

Result: 广泛的实验表明，该方法在场景文本风格转换方面表现出色，在视觉保真度和文本保留方面均优于现有最先进的方法。

Conclusion: SceneTextStylizer为场景文本的灵活和高保真风格迁移提供了一个新颖的解决方案，解决了现有方法在自由风格迁移和局部控制方面的局限性。

Abstract: With the rapid development of diffusion models, style transfer has made
remarkable progress. However, flexible and localized style editing for scene
text remains an unsolved challenge. Although existing scene text editing
methods have achieved text region editing, they are typically limited to
content replacement and simple styles, which lack the ability of free-style
transfer. In this paper, we introduce SceneTextStylizer, a novel training-free
diffusion-based framework for flexible and high-fidelity style transfer of text
in scene images. Unlike prior approaches that either perform global style
transfer or focus solely on textual content modification, our method enables
prompt-guided style transformation specifically for text regions, while
preserving both text readability and stylistic consistency. To achieve this, we
design a feature injection module that leverages diffusion model inversion and
self-attention to transfer style features effectively. Additionally, a region
control mechanism is introduced by applying a distance-based changing mask at
each denoising step, enabling precise spatial control. To further enhance
visual quality, we incorporate a style enhancement module based on the Fourier
transform to reinforce stylistic richness. Extensive experiments demonstrate
that our method achieves superior performance in scene text style
transformation, outperforming existing state-of-the-art methods in both visual
fidelity and text preservation.

</details>


### [97] [LTGS: Long-Term Gaussian Scene Chronology From Sparse View Updates](https://arxiv.org/abs/2510.09881)
*Minkwan Kim,Seungmin Lee,Junho Kim,Young Min Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为LTGS（长期高斯场景编年史）的场景表示方法，能够从高度受限的稀疏、随意捕获中高效地处理日常场景的长期变化，通过对象模板和精炼实现卓越的重建质量和快速更新。


<details>
  <summary>Details</summary>
Motivation: 现有新视角合成方法在从日常随意捕获中获取环境时面临挑战，因为场景变化频繁且需要空间和时间上的密集观测。这促使研究者寻找一种能从稀疏、非结构化数据中处理长期场景变化的高效表示方法。

Method: LTGS从初始输入图像获得不完整的高斯溅射表示，并鲁棒地建模场景的长期编年史。它将对象构建为“模板高斯”，作为共享对象轨迹的结构化、可重用先验。这些对象模板经过进一步的精炼流程，根据少量观测调整先验以适应时间变化的环。该框架通过简单变换可泛化到多个时间步，并收集了新的真实世界数据集进行评估。

Result: 实验证明，与现有基线相比，LTGS框架实现了卓越的重建质量，同时支持快速轻量级的更新。新收集的数据集也验证了其在实际应用中的可行性。

Conclusion: LTGS提供了一种高效、可扩展的场景表示，能够从高度受限的随意捕获中处理日常环境的长期变化，显著提升了3D环境时间演化的可扩展性，并达到了优异的重建效果和更新效率。

Abstract: Recent advances in novel-view synthesis can create the photo-realistic
visualization of real-world environments from conventional camera captures.
However, acquiring everyday environments from casual captures faces challenges
due to frequent scene changes, which require dense observations both spatially
and temporally. We propose long-term Gaussian scene chronology from sparse-view
updates, coined LTGS, an efficient scene representation that can embrace
everyday changes from highly under-constrained casual captures. Given an
incomplete and unstructured Gaussian splatting representation obtained from an
initial set of input images, we robustly model the long-term chronology of the
scene despite abrupt movements and subtle environmental variations. We
construct objects as template Gaussians, which serve as structural, reusable
priors for shared object tracks. Then, the object templates undergo a further
refinement pipeline that modulates the priors to adapt to temporally varying
environments based on few-shot observations. Once trained, our framework is
generalizable across multiple time steps through simple transformations,
significantly enhancing the scalability for a temporal evolution of 3D
environments. As existing datasets do not explicitly represent the long-term
real-world changes with a sparse capture setup, we collect real-world datasets
to evaluate the practicality of our pipeline. Experiments demonstrate that our
framework achieves superior reconstruction quality compared to other baselines
while enabling fast and light-weight updates.

</details>


### [98] [An uncertainty-aware framework for data-efficient multi-view animal pose estimation](https://arxiv.org/abs/2510.09903)
*Lenny Aharon,Keemin Lee,Karan Sikka,Selmaan Chettih,Cole Hurwitz,Liam Paninski,Matthew R Whiteway*

Main category: cs.CV

TL;DR: 该研究提出了一个综合框架，通过新颖的训练、后处理技术和模型蒸馏程序，解决了多视角姿态估计中标记数据有限和不确定性估计差的问题，实现了对动物行为的准确、不确定性感知跟踪。


<details>
  <summary>Details</summary>
Motivation: 当前的多视角姿态估计方法在有限标记数据下难以实现准确跟踪，并且不确定性估计较差，这阻碍了动物行为研究的量化分析。

Method: 该框架包括：1) 多视角Transformer (MVT)，利用预训练骨干网络并同时处理所有视角信息；2) 新颖的补丁掩码方案，无需相机校准即可学习鲁棒的跨视角对应关系；3) 对于已校准设置，通过3D增强和三角测量损失整合几何一致性；4) 将现有的Ensemble Kalman Smoother (EKS) 后处理器扩展到非线性情况，并通过方差膨胀技术增强不确定性量化；5) 设计了一种蒸馏程序，利用改进的EKS预测和不确定性估计生成高质量伪标签，以减少对手动标签的依赖。

Result: 该框架的各个组件在三种不同的动物物种（苍蝇、小鼠、山雀）上均持续优于现有方法，且每个组件都提供了互补的优势。

Conclusion: 该研究提供了一个实用且具有不确定性意识的可靠姿态估计系统，能够在真实世界数据限制下支持下游行为分析。

Abstract: Multi-view pose estimation is essential for quantifying animal behavior in
scientific research, yet current methods struggle to achieve accurate tracking
with limited labeled data and suffer from poor uncertainty estimates. We
address these challenges with a comprehensive framework combining novel
training and post-processing techniques, and a model distillation procedure
that leverages the strengths of these techniques to produce a more efficient
and effective pose estimator. Our multi-view transformer (MVT) utilizes
pretrained backbones and enables simultaneous processing of information across
all views, while a novel patch masking scheme learns robust cross-view
correspondences without camera calibration. For calibrated setups, we
incorporate geometric consistency through 3D augmentation and a triangulation
loss. We extend the existing Ensemble Kalman Smoother (EKS) post-processor to
the nonlinear case and enhance uncertainty quantification via a variance
inflation technique. Finally, to leverage the scaling properties of the MVT, we
design a distillation procedure that exploits improved EKS predictions and
uncertainty estimates to generate high-quality pseudo-labels, thereby reducing
dependence on manual labels. Our framework components consistently outperform
existing methods across three diverse animal species (flies, mice, chickadees),
with each component contributing complementary benefits. The result is a
practical, uncertainty-aware system for reliable pose estimation that enables
downstream behavioral analyses under real-world data constraints.

</details>


### [99] [SpectralCA: Bi-Directional Cross-Attention for Next-Generation UAV Hyperspectral Vision](https://arxiv.org/abs/2510.09912)
*D. V. Brovko*

Main category: cs.CV

TL;DR: 本研究旨在开发一种深度学习架构，将高光谱成像（HSI）集成到无人机（UAV）感知中，以实现复杂的导航、目标检测和地形分类，并通过引入SpectralCA块改进了Mobile 3D Vision Transformer，提高了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在复杂环境中（如存在干扰、能见度差或伪装）可靠运行的需求日益增长，传统导航方法变得不可靠。高光谱成像（HSI）通过实现精细的材料识别和目标区分，为基于无人机的计算机视觉提供了独特的机会，这对于导航、监视、农业和环境监测至关重要。

Method: 本研究开发了一种混合2D/3D卷积架构，带有光谱-空间交叉注意力。具体方法是修改Mobile 3D Vision Transformer (MDvT)，引入了所提出的SpectralCA块。该SpectralCA块采用双向交叉注意力来融合光谱和空间特征，旨在提高准确性，同时减少参数和推理时间。实验评估在WHU-Hi-HongHu数据集上进行，使用总体准确度（Overall Accuracy）、平均准确度（Average Accuracy）和Kappa系数评估结果。

Result: 研究结果证实，所提出的架构提高了无人机感知效率，使其能够实现导航、目标识别和环境监测任务的实时操作。

Conclusion: 本研究成功开发了一种结合高光谱成像的深度学习架构，显著提升了无人机在复杂环境中的感知能力，实现了导航、目标识别和环境监测等任务的实时高效运行。

Abstract: The relevance of this research lies in the growing demand for unmanned aerial
vehicles (UAVs) capable of operating reliably in complex environments where
conventional navigation becomes unreliable due to interference, poor
visibility, or camouflage. Hyperspectral imaging (HSI) provides unique
opportunities for UAV-based computer vision by enabling fine-grained material
recognition and object differentiation, which are critical for navigation,
surveillance, agriculture, and environmental monitoring. The aim of this work
is to develop a deep learning architecture integrating HSI into UAV perception
for navigation, object detection, and terrain classification. Objectives
include: reviewing existing HSI methods, designing a hybrid 2D/3D convolutional
architecture with spectral-spatial cross-attention, training, and benchmarking.
The methodology is based on the modification of the Mobile 3D Vision
Transformer (MDvT) by introducing the proposed SpectralCA block. This block
employs bi-directional cross-attention to fuse spectral and spatial features,
enhancing accuracy while reducing parameters and inference time. Experimental
evaluation was conducted on the WHU-Hi-HongHu dataset, with results assessed
using Overall Accuracy, Average Accuracy, and the Kappa coefficient. The
findings confirm that the proposed architecture improves UAV perception
efficiency, enabling real-time operation for navigation, object recognition,
and environmental monitoring tasks.
  Keywords: SpectralCA, deep learning, computer vision, hyperspectral imaging,
unmanned aerial vehicle, object detection, semi-supervised learning.

</details>


### [100] [HeadsUp! High-Fidelity Portrait Image Super-Resolution](https://arxiv.org/abs/2510.09924)
*Renjie Li,Zihao Zhu,Xiaoyu Wang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本文提出HeadsUp，一个单步扩散模型，用于无缝地恢复和放大肖像图片，解决了现有方法中人脸区域的混合伪影问题，并通过人脸监督和参考机制提升了修复质量。


<details>
  <summary>Details</summary>
Motivation: 现有的图像超分辨率技术要么专注于通用图像，要么专注于严格对齐的人脸图像。将这两种模型混合处理肖像照片时，由于训练方式不同，不可避免地会在人脸区域周围引入混合或边界伪影，而人类对人脸保真度特别敏感。

Method: 研究了肖像图像超分辨率（PortraitISR）问题，并提出了HeadsUp模型。该模型基于单步扩散模型，并开发了：1) 一种人脸监督机制，引导模型关注人脸区域；2) 一种基于参考的机制，帮助恢复身份并减少低质量人脸修复中的模糊性。此外，还构建了一个高质量的4K肖像图像ISR数据集PortraitSR-4K用于模型训练和基准测试。

Result: 广泛的实验表明，HeadsUp在PortraitISR任务上取得了最先进的性能，同时在通用图像和对齐人脸数据集上保持了可比或更高的性能。

Conclusion: HeadsUp提供了一种端到端的单步扩散模型，能够无缝地恢复和放大肖像图像，有效解决了现有方法中混合伪影问题，并在人脸保真度和身份恢复方面表现出色，实现了肖像图像超分辨率领域的SOTA性能。

Abstract: Portrait pictures, which typically feature both human subjects and natural
backgrounds, are one of the most prevalent forms of photography on social
media. Existing image super-resolution (ISR) techniques generally focus either
on generic real-world images or strictly aligned facial images (i.e., face
super-resolution). In practice, separate models are blended to handle portrait
photos: the face specialist model handles the face region, and the general
model processes the rest. However, these blending approaches inevitably
introduce blending or boundary artifacts around the facial regions due to
different model training recipes, while human perception is particularly
sensitive to facial fidelity. To overcome these limitations, we study the
portrait image supersolution (PortraitISR) problem, and propose HeadsUp, a
single-step diffusion model that is capable of seamlessly restoring and
upscaling portrait images in an end-to-end manner. Specifically, we build our
model on top of a single-step diffusion model and develop a face supervision
mechanism to guide the model in focusing on the facial region. We then
integrate a reference-based mechanism to help with identity restoration,
reducing face ambiguity in low-quality face restoration. Additionally, we have
built a high-quality 4K portrait image ISR dataset dubbed PortraitSR-4K, to
support model training and benchmarking for portrait images. Extensive
experiments show that HeadsUp achieves state-of-the-art performance on the
PortraitISR task while maintaining comparable or higher performance on both
general image and aligned face datasets.

</details>


### [101] [Denoising Diffusion as a New Framework for Underwater Images](https://arxiv.org/abs/2510.09934)
*Nilesh Jain,Elie Alhajjar*

Main category: cs.CV

TL;DR: 针对水下图像质量差、现有增强方法局限性及数据集不足的问题，本文提出利用去噪扩散模型扩展多样化数据集，并使用Controlnet提升图像质量的方案。


<details>
  <summary>Details</summary>
Motivation: 水下图像对海洋研究至关重要，但复杂环境导致图像质量差（低能见度、模糊、色彩失真、噪声）。现有图像增强研究有效但有局限性（泛化性差、依赖干净数据）。主要挑战在于数据集缺乏多样性、质量低且多为单目图像，限制了不同光照和角度的表示。

Method: 本文提出一个新行动计划：一方面，呼吁使用去噪扩散模型扩展数据集，纳入立体、广角、微距和特写等多种图像类型；另一方面，建议使用Controlnet增强图像，以评估和提高相应数据集的质量。

Result: 本文提出了一项克服现有水下图像数据集局限性（多样性不足、质量低、单目图像为主）和增强方法限制的新行动计划。该计划旨在通过扩大数据集多样性和提升图像质量来改善海洋生态系统研究。

Conclusion: 通过实施所提出的利用去噪扩散模型扩展数据集和使用Controlnet提升图像质量的策略，有望克服水下图像增强和数据集的现有局限性，从而更好地支持海洋生态系统的研究。

Abstract: Underwater images play a crucial role in ocean research and marine
environmental monitoring since they provide quality information about the
ecosystem. However, the complex and remote nature of the environment results in
poor image quality with issues such as low visibility, blurry textures, color
distortion, and noise. In recent years, research in image enhancement has
proven to be effective but also presents its own limitations, like poor
generalization and heavy reliance on clean datasets. One of the challenges
herein is the lack of diversity and the low quality of images included in these
datasets. Also, most existing datasets consist only of monocular images, a fact
that limits the representation of different lighting conditions and angles. In
this paper, we propose a new plan of action to overcome these limitations. On
one hand, we call for expanding the datasets using a denoising diffusion model
to include a variety of image types such as stereo, wide-angled, macro, and
close-up images. On the other hand, we recommend enhancing the images using
Controlnet to evaluate and increase the quality of the corresponding datasets,
and hence improve the study of the marine ecosystem.
  Tags - Underwater Images, Denoising Diffusion, Marine ecosystem, Controlnet

</details>


### [102] [Semi-disentangled spatiotemporal implicit neural representations of longitudinal neuroimaging data for trajectory classification](https://arxiv.org/abs/2510.09936)
*Agampreet Aulakh,Nils D. Forkert,Matthias Wilms*

Main category: cs.CV

TL;DR: 该研究提出一种基于隐式神经表示（INRs）的全新数据驱动方法，将纵向MRI数据建模为连续函数，以表征大脑衰老轨迹。该方法在不规则采样数据下，在脑衰老轨迹分类任务中优于传统深度学习基线模型。


<details>
  <summary>Details</summary>
Motivation: 人类大脑结构在整个生命周期中会发生动态变化，这些变化可能由病理驱动。纵向MRI数据对于描绘典型和非典型衰老轨迹至关重要。然而，此类数据具有离散性、个体间和群体间不同的时空图像采样模式，给分析带来巨大挑战。这导致大多数传统深度学习方法无法有效表示潜在的连续生物学过程。

Method: 研究通过使用隐式神经表示（INRs）将受试者特定的纵向T1加权MRI数据建模为连续函数，从而表示整个大脑的衰老轨迹。为此，引入了一种新颖的INR架构，能够部分解耦空间和时间轨迹参数，并设计了一个直接在INR参数空间操作的有效框架来分类脑衰老轨迹。为在受控数据环境中评估该方法，开发了一种生物学基础的轨迹模拟，并生成了450名健康和痴呆样受试者在规则和不规则采样时间点的T1加权3D MRI数据。

Result: 在更真实的非规则采样实验中，基于INR的方法在脑衰老轨迹分类任务中实现了81.3%的准确率，优于标准深度学习基线模型（73.7%）。

Conclusion: 该研究提出的基于INR的方法能够有效且数据驱动地表示大脑衰老轨迹，克服了传统方法处理离散和不规则采样的纵向神经影像数据的局限性，并在分类任务中展现出优越的性能，尤其是在非规则采样数据下。

Abstract: The human brain undergoes dynamic, potentially pathology-driven, structural
changes throughout a lifespan. Longitudinal Magnetic Resonance Imaging (MRI)
and other neuroimaging data are valuable for characterizing trajectories of
change associated with typical and atypical aging. However, the analysis of
such data is highly challenging given their discrete nature with different
spatial and temporal image sampling patterns within individuals and across
populations. This leads to computational problems for most traditional deep
learning methods that cannot represent the underlying continuous biological
process. To address these limitations, we present a new, fully data-driven
method for representing aging trajectories across the entire brain by modelling
subject-specific longitudinal T1-weighted MRI data as continuous functions
using Implicit Neural Representations (INRs). Therefore, we introduce a novel
INR architecture capable of partially disentangling spatial and temporal
trajectory parameters and design an efficient framework that directly operates
on the INRs' parameter space to classify brain aging trajectories. To evaluate
our method in a controlled data environment, we develop a biologically grounded
trajectory simulation and generate T1-weighted 3D MRI data for 450 healthy and
dementia-like subjects at regularly and irregularly sampled timepoints. In the
more realistic irregular sampling experiment, our INR-based method achieves
81.3% accuracy for the brain aging trajectory classification task,
outperforming a standard deep learning baseline model (73.7%).

</details>


### [103] [A Multi-Strategy Framework for Enhancing Shatian Pomelo Detection in Real-World Orchards](https://arxiv.org/abs/2510.09948)
*Pan Wang,Yihao Hu,Xiaodong Bai,Aiping Yang,Xiangxiang Li,Meiping Ding,Jianguo Yao*

Main category: cs.CV

TL;DR: 本研究提出了一种多策略框架和REAS-Det网络，以解决沙田柚在真实果园环境中自动检测面临的成像设备、光照、目标尺度变化和遮挡等挑战，显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 沙田柚作为一种特色农产品，需要自动化检测来确保产量并满足精益生产的商业需求。现有研究中的专业网络在真实世界场景中性能往往下降，原因在于成像设备、光照条件、目标尺度变化和遮挡等因素影响了检测准确性。

Method: 本研究提出了一个多策略框架来应对挑战。首先，构建了一个多场景数据集STP-AgriData（整合真实果园和网络数据）以解决不同成像设备和复杂环境带来的色调变化。其次，通过调整对比度和亮度等数据增强来模拟不一致的光照条件。第三，设计了REAS-Det网络来解决目标尺度变化和遮挡问题：针对尺度变化，设计了RFAConv和C3RFEM模块以扩大和增强感受野；针对遮挡问题，引入了多尺度、多头特征选择结构（MultiSEAM）和soft-NMS。

Result: 实验结果显示，该方法在沙田柚检测中实现了87.6%的精确度（P）、74.9%的召回率（R）、82.8%的mAP@.50和53.3%的mAP@.50:.95。所提出的网络性能优于其他最先进的检测方法。

Conclusion: 本研究提出的多策略框架和REAS-Det网络能够有效应对真实世界中沙田柚检测所面临的成像设备、光照、尺度变化和遮挡等挑战，并取得了优于现有先进方法的检测性能。

Abstract: As a specialty agricultural product with a large market scale, Shatian pomelo
necessitates the adoption of automated detection to ensure accurate quantity
and meet commercial demands for lean production. Existing research often
involves specialized networks tailored for specific theoretical or dataset
scenarios, but these methods tend to degrade performance in real-world. Through
analysis of factors in this issue, this study identifies four key challenges
that affect the accuracy of Shatian pomelo detection: imaging devices, lighting
conditions, object scale variation, and occlusion. To mitigate these
challenges, a multi-strategy framework is proposed in this paper. Firstly, to
effectively solve tone variation introduced by diverse imaging devices and
complex orchard environments, we utilize a multi-scenario dataset,
STP-AgriData, which is constructed by integrating real orchard images with
internet-sourced data. Secondly, to simulate the inconsistent illumination
conditions, specific data augmentations such as adjusting contrast and changing
brightness, are applied to the above dataset. Thirdly, to address the issues of
object scale variation and occlusion in fruit detection, an REAS-Det network is
designed in this paper. For scale variation, RFAConv and C3RFEM modules are
designed to expand and enhance the receptive fields. For occlusion variation, a
multi-scale, multi-head feature selection structure (MultiSEAM) and soft-NMS
are introduced to enhance the handling of occlusion issues to improve detection
accuracy. The results of these experiments achieved a precision(P) of 87.6%, a
recall (R) of 74.9%, a mAP@.50 of 82.8%, and a mAP@.50:.95 of 53.3%. Our
proposed network demonstrates superior performance compared to other
state-of-the-art detection methods.

</details>


### [104] [J-RAS: Enhancing Medical Image Segmentation via Retrieval-Augmented Joint Training](https://arxiv.org/abs/2510.09953)
*Salma J. Ahmed,Emad A. Mohammed,Azam Asilian Bidgoli*

Main category: cs.CV

TL;DR: 本文提出J-RAS（联合检索增强分割），一种将分割模型与检索模型联合训练的方法，通过利用检索到的图像-掩码对和学习分割相关特征，显著提升了医学图像分割的性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 医学图像手动分割耗时、昂贵且易受人为因素影响。现有AI分割方法需要大量标注数据，且在面对患者差异和罕见病理时泛化能力不足。

Method: J-RAS是一种联合训练方法，它整合了分割模型和检索模型。两个模型共同优化：分割模型利用检索到的图像-掩码对增强解剖学理解，而检索模型学习超越简单视觉相似性的分割相关特征。这种联合优化确保检索能提供有意义的上下文线索来指导边界描绘。

Result: J-RAS在U-Net、TransUNet、SAM和SegFormer等多种分割骨干网络上，以及ACDC和M&Ms两个基准数据集上进行了验证，均显示出一致的性能提升。例如，在ACDC数据集上，SegFormer结合J-RAS后，平均Dice分数从0.8708提高到0.9115，平均Hausdorff距离从1.8130降低到1.1489。

Conclusion: J-RAS方法有效且在不同架构和数据集上具有良好的泛化能力，能够显著提高医学图像分割的性能，解决了现有AI方法在数据和泛化方面的局限性。

Abstract: Image segmentation, the process of dividing images into meaningful regions,
is critical in medical applications for accurate diagnosis, treatment planning,
and disease monitoring. Although manual segmentation by healthcare
professionals produces precise outcomes, it is time-consuming, costly, and
prone to variability due to differences in human expertise. Artificial
intelligence (AI)-based methods have been developed to address these
limitations by automating segmentation tasks; however, they often require
large, annotated datasets that are rarely available in practice and frequently
struggle to generalize across diverse imaging conditions due to inter-patient
variability and rare pathological cases. In this paper, we propose Joint
Retrieval Augmented Segmentation (J-RAS), a joint training method for guided
image segmentation that integrates a segmentation model with a retrieval model.
Both models are jointly optimized, enabling the segmentation model to leverage
retrieved image-mask pairs to enrich its anatomical understanding, while the
retrieval model learns segmentation-relevant features beyond simple visual
similarity. This joint optimization ensures that retrieval actively contributes
meaningful contextual cues to guide boundary delineation, thereby enhancing the
overall segmentation performance. We validate J-RAS across multiple
segmentation backbones, including U-Net, TransUNet, SAM, and SegFormer, on two
benchmark datasets: ACDC and M&Ms, demonstrating consistent improvements. For
example, on the ACDC dataset, SegFormer without J-RAS achieves a mean Dice
score of 0.8708$\pm$0.042 and a mean Hausdorff Distance (HD) of
1.8130$\pm$2.49, whereas with J-RAS, the performance improves substantially to
a mean Dice score of 0.9115$\pm$0.031 and a mean HD of 1.1489$\pm$0.30. These
results highlight the method's effectiveness and its generalizability across
architectures and datasets.

</details>


### [105] [BurstDeflicker: A Benchmark Dataset for Flicker Removal in Dynamic Scenes](https://arxiv.org/abs/2510.09996)
*Lishen Qu,Zhihao Liu,Shihao Zhou,Yaqi Luo,Jie Liang,Hui Zeng,Lei Zhang,Jufeng Yang*

Main category: cs.CV

TL;DR: 该论文提出了一个名为BurstDeflicker的大规模、可扩展的去闪烁基准数据集，通过三种互补的数据采集策略解决现有闪烁去除研究中缺乏真实世界数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 短曝光图像中的闪烁伪影由卷帘快门相机和交流供电照明引起，表现为不均匀的亮度分布和暗带，严重影响图像质量和目标检测、跟踪等高级任务。现有研究进展受限于缺乏大规模、真实的闪烁去除数据集。

Method: 该研究采用三种数据采集策略构建BurstDeflicker数据集：1) 开发基于Retinex的合成管道，可控地生成多样化的闪烁模式；2) 捕获4,000张真实场景的闪烁图像，以理解真实闪烁的空间和时间特性；3) 提出绿幕方法，在保留真实闪烁退化的同时将运动融入图像对。

Result: 全面的实验证明了所构建数据集的有效性。

Conclusion: 该数据集有望推动闪烁去除领域的研究进展。

Abstract: Flicker artifacts in short-exposure images are caused by the interplay
between the row-wise exposure mechanism of rolling shutter cameras and the
temporal intensity variations of alternating current (AC)-powered lighting.
These artifacts typically appear as uneven brightness distribution across the
image, forming noticeable dark bands. Beyond compromising image quality, this
structured noise also affects high-level tasks, such as object detection and
tracking, where reliable lighting is crucial. Despite the prevalence of
flicker, the lack of a large-scale, realistic dataset has been a significant
barrier to advancing research in flicker removal. To address this issue, we
present BurstDeflicker, a scalable benchmark constructed using three
complementary data acquisition strategies. First, we develop a Retinex-based
synthesis pipeline that redefines the goal of flicker removal and enables
controllable manipulation of key flicker-related attributes (e.g., intensity,
area, and frequency), thereby facilitating the generation of diverse flicker
patterns. Second, we capture 4,000 real-world flicker images from different
scenes, which help the model better understand the spatial and temporal
characteristics of real flicker artifacts and generalize more effectively to
wild scenarios. Finally, due to the non-repeatable nature of dynamic scenes, we
propose a green-screen method to incorporate motion into image pairs while
preserving real flicker degradation. Comprehensive experiments demonstrate the
effectiveness of our dataset and its potential to advance research in flicker
removal.

</details>


### [106] [FlareX: A Physics-Informed Dataset for Lens Flare Removal via 2D Synthesis and 3D Rendering](https://arxiv.org/abs/2510.09995)
*Lishen Qu,Zhihao Liu,Jinshan Pan,Shihao Zhou,Jinglei Shi,Duosheng Chen,Jufeng Yang*

Main category: cs.CV

TL;DR: 本文提出了一种物理信息驱动的耀斑数据生成方法，通过参数化模板、光照感知2D合成和物理引擎3D渲染，创建了混合耀斑数据集FlareX，以解决现有合成数据多样性不足和物理原理缺失导致模型泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 镜头耀斑严重降低图像质量，而获取真实的耀斑-无耀斑图像对非常困难。现有数据集通常通过2D合成，将人工耀斑模板叠加到背景图像上，但这些模板缺乏多样性，且合成过程忽略了物理原理，导致基于这些数据集训练的模型在真实世界场景中泛化能力差。

Method: 本文提出了一种新的物理信息驱动的耀斑数据生成方法，包含三个阶段：1. 参数化模板创建；2. 光照定律感知的2D合成；3. 基于物理引擎的3D渲染。最终生成了一个结合2D和3D视角的混合耀斑数据集FlareX。此外，还设计了一种掩码方法，从真实的受损图像中获取无耀斑图像，以衡量模型在真实世界图像上的性能。

Result: 所提出的FlareX数据集包含从95种耀斑模式派生出的9,500个2D模板，以及从60个3D场景渲染的3,000对耀斑图像。大量的实验证明了该方法和数据集的有效性。

Conclusion: 本文提出的物理信息驱动的耀斑数据生成方法和FlareX数据集，有效解决了现有耀斑数据集在多样性和物理准确性方面的不足，显著提升了模型在真实世界场景中去耀斑任务的泛化能力。同时，提出的真实世界评估方法也为模型性能衡量提供了有效手段。

Abstract: Lens flare occurs when shooting towards strong light sources, significantly
degrading the visual quality of images. Due to the difficulty in capturing
flare-corrupted and flare-free image pairs in the real world, existing datasets
are typically synthesized in 2D by overlaying artificial flare templates onto
background images. However, the lack of flare diversity in templates and the
neglect of physical principles in the synthesis process hinder models trained
on these datasets from generalizing well to real-world scenarios. To address
these challenges, we propose a new physics-informed method for flare data
generation, which consists of three stages: parameterized template creation,
the laws of illumination-aware 2D synthesis, and physical engine-based 3D
rendering, which finally gives us a mixed flare dataset that incorporates both
2D and 3D perspectives, namely FlareX. This dataset offers 9,500 2D templates
derived from 95 flare patterns and 3,000 flare image pairs rendered from 60 3D
scenes. Furthermore, we design a masking approach to obtain real-world
flare-free images from their corrupted counterparts to measure the performance
of the model on real-world images. Extensive experiments demonstrate the
effectiveness of our method and dataset.

</details>


### [107] [MIMO: A medical vision language model with visual referring multimodal input and pixel grounding multimodal output](https://arxiv.org/abs/2510.10011)
*Yanyuan Chen,Dexuan Xu,Yu Huang,Songkun Zhan,Hanpin Wang,Dongxue Chen,Xueping Wang,Meikang Qiu,Hang Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为MIMO的统一医学视觉语言模型，通过多模态输入（视觉参照）和多模态输出（像素定位）解决了现有模型在理解视觉线索和输出与图像关联方面的不足。同时，还构建了一个包含895K样本的医学多模态数据集MIMOSeg。


<details>
  <summary>Details</summary>
Motivation: 现有的医学视觉语言模型存在两个主要问题：1) 输入时，模型仅依赖文本指令，缺乏对图像中视觉线索的直接理解；2) 输出时，模型只给出文本答案，缺乏与图像中关键区域的关联。

Method: 1. 提出了统一的医学视觉语言模型MIMO，其具有视觉参照多模态输入和像素定位多模态输出能力。2. 构建了全面的医学多模态数据集MIMOSeg，包含895K样本，从四个不同角度构建，涵盖了多模态输入和输出的基础指令遵循和复杂问答任务。

Result: MIMO在多个下游医学多模态任务上进行了实验，结果验证了它能够独特地结合视觉参照和像素定位能力，这是以往模型所不具备的。

Conclusion: MIMO模型通过结合视觉参照的多模态输入和像素定位的多模态输出，有效解决了现有医学视觉语言模型在视觉理解和图像关联输出方面的局限性，并在新构建的MIMOSeg数据集的支持下，展现出独特的性能优势。

Abstract: Currently, medical vision language models are widely used in medical vision
question answering tasks. However, existing models are confronted with two
issues: for input, the model only relies on text instructions and lacks direct
understanding of visual clues in the image; for output, the model only gives
text answers and lacks connection with key areas in the image. To address these
issues, we propose a unified medical vision language model MIMO, with visual
referring Multimodal Input and pixel grounding Multimodal Output. MIMO can not
only combine visual clues and textual instructions to understand complex
medical images and semantics, but can also ground medical terminologies in
textual output within the image. To overcome the scarcity of relevant data in
the medical field, we propose MIMOSeg, a comprehensive medical multimodal
dataset including 895K samples. MIMOSeg is constructed from four different
perspectives, covering basic instruction following and complex question
answering with multimodal input and multimodal output. We conduct experiments
on several downstream medical multimodal tasks. Extensive experimental results
verify that MIMO can uniquely combine visual referring and pixel grounding
capabilities, which are not available in previous models.

</details>


### [108] [Q-Adapter: Visual Query Adapter for Extracting Textually-related Features in Video Captioning](https://arxiv.org/abs/2510.10022)
*Junan Chen,Trung Thanh Nguyen,Takahiro Komamizu,Ichiro Ide*

Main category: cs.CV

TL;DR: 本文提出Q-Adapter，一种轻量级视觉适配器，用于高效微调多模态大语言模型（MLLMs）以进行视频字幕生成。它通过在视觉编码器中引入可学习查询标记和门控层，实现稀疏、字幕相关特征的有效提取，在参数效率微调（PEFT）方法中达到最先进性能，并与完全微调方法相比也具竞争力，但仅需1.4%的参数。


<details>
  <summary>Details</summary>
Motivation: 随着模型规模的增长，视频字幕任务中传统“预训练后微调”范式变得计算成本高昂。尽管参数效率微调（PEFT）方法前景广阔，但其主要关注多模态大语言模型（MLLMs）的语言组件，在多模态任务中对视觉信息的理解和利用仍有待探索。

Method: 本文提出Query-Adapter (Q-Adapter)，一个轻量级视觉适配器模块。它在视觉编码器中引入可学习的查询标记（query tokens）和一个门控层，旨在有效提取与字幕相关的稀疏视觉特征，且不依赖外部文本监督。

Result: Q-Adapter在MSR-VTT和MSVD两个视频字幕数据集上，在BLEU@4、METEOR、ROUGE-L和CIDEr指标上，在采用PEFT方法的模型中达到了最先进的性能。与采用完全微调的方法相比，Q-Adapter在仅使用1.4%参数的情况下，也取得了具有竞争力的性能。研究还分析了关键超参数和设计选择对微调效果的影响，为基于适配器的学习提供了优化策略见解。

Conclusion: Q-Adapter在字幕质量和参数效率之间取得了良好的平衡，展示了其在视频-语言建模方面的强大潜力和可扩展性。

Abstract: Recent advances in video captioning are driven by large-scale pretrained
models, which follow the standard "pre-training followed by fine-tuning"
paradigm, where the full model is fine-tuned for downstream tasks. Although
effective, this approach becomes computationally prohibitive as the model size
increases. The Parameter-Efficient Fine-Tuning (PEFT) approach offers a
promising alternative, but primarily focuses on the language components of
Multimodal Large Language Models (MLLMs). Despite recent progress, PEFT remains
underexplored in multimodal tasks and lacks sufficient understanding of visual
information during fine-tuning the model. To bridge this gap, we propose
Query-Adapter (Q-Adapter), a lightweight visual adapter module designed to
enhance MLLMs by enabling efficient fine-tuning for the video captioning task.
Q-Adapter introduces learnable query tokens and a gating layer into Vision
Encoder, enabling effective extraction of sparse, caption-relevant features
without relying on external textual supervision. We evaluate Q-Adapter on two
well-known video captioning datasets, MSR-VTT and MSVD, where it achieves
state-of-the-art performance among the methods that take the PEFT approach
across BLEU@4, METEOR, ROUGE-L, and CIDEr metrics. Q-Adapter also achieves
competitive performance compared to methods that take the full fine-tuning
approach while requiring only 1.4% of the parameters. We further analyze the
impact of key hyperparameters and design choices on fine-tuning effectiveness,
providing insights into optimization strategies for adapter-based learning.
These results highlight the strong potential of Q-Adapter in balancing caption
quality and parameter efficiency, demonstrating its scalability for
video-language modeling.

</details>


### [109] [P-4DGS: Predictive 4D Gaussian Splatting with 90$\times$ Compression](https://arxiv.org/abs/2510.10030)
*Henan Wang,Hanxin Zhu,Xinliang Gong,Tianyu He,Xin Li,Zhibo Chen*

Main category: cs.CV

TL;DR: P-4DGS是一种新颖的动态3D高斯泼溅(3DGS)表示方法，通过引入时空预测和自适应量化策略，显著减少了动态3D场景建模的内存消耗，同时保持了高质量的重建和快速渲染。


<details>
  <summary>Details</summary>
Motivation: 尽管3D高斯泼溅在动态3D场景重建（即4D重建）中表现出色，但大多数现有算法忽视了动态场景中固有的时间与空间冗余，导致内存消耗过高。

Method: 本文提出了P-4DGS，一种紧凑的4D场景建模动态3DGS表示。方法包括：1) 设计一个基于3D锚点的时空预测模块，利用视频压缩中的帧内/帧间预测技术，充分挖掘不同3D高斯基元之间的时空关联。2) 采用自适应量化策略结合基于上下文的熵编码，进一步减小3D锚点的大小，提高压缩效率。

Result: P-4DGS在合成和真实世界数据集上均实现了最先进的重建质量和最快的渲染速度。其存储占用极低（平均约1MB），在合成场景中实现了高达40倍的压缩，在真实世界场景中实现了高达90倍的压缩。

Conclusion: P-4DGS通过创新的时空预测和自适应量化策略，有效解决了动态3DGS表示中的内存消耗问题，同时在重建质量、渲染速度和存储效率方面均达到了领先水平。

Abstract: 3D Gaussian Splatting (3DGS) has garnered significant attention due to its
superior scene representation fidelity and real-time rendering performance,
especially for dynamic 3D scene reconstruction (\textit{i.e.}, 4D
reconstruction). However, despite achieving promising results, most existing
algorithms overlook the substantial temporal and spatial redundancies inherent
in dynamic scenes, leading to prohibitive memory consumption. To address this,
we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene
modeling. Inspired by intra- and inter-frame prediction techniques commonly
used in video compression, we first design a 3D anchor point-based
spatial-temporal prediction module to fully exploit the spatial-temporal
correlations across different 3D Gaussian primitives. Subsequently, we employ
an adaptive quantization strategy combined with context-based entropy coding to
further reduce the size of the 3D anchor points, thereby achieving enhanced
compression efficiency. To evaluate the rate-distortion performance of our
proposed P-4DGS in comparison with other dynamic 3DGS representations, we
conduct extensive experiments on both synthetic and real-world datasets.
Experimental results demonstrate that our approach achieves state-of-the-art
reconstruction quality and the fastest rendering speed, with a remarkably low
storage footprint (around \textbf{1MB} on average), achieving up to
\textbf{40$\times$} and \textbf{90$\times$} compression on synthetic and
real-world scenes, respectively.

</details>


### [110] [Complementary and Contrastive Learning for Audio-Visual Segmentation](https://arxiv.org/abs/2510.10051)
*Sitong Gong,Yunzhi Zhuge,Lu Zhang,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CCFormer的新型互补对比Transformer框架，用于音视频分割（AVS），旨在克服现有方法在处理局部/全局信息、时空上下文和跨模态交互方面的局限性，并在多个数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的CNN方法受限于局部感受野，难以有效处理音视频交互。Transformer方法虽利用注意力机制增强跨模态协作，但通常难以充分提取多模态系数和时间动态。

Method: 本文提出了互补对比Transformer (CCFormer) 框架，包含以下模块：1. 早期集成模块（EIM），采用并行双边架构，融合多尺度视觉特征与音频数据以增强跨模态互补性。2. 多查询Transformer模块（MTM），动态赋予音频查询学习能力，同时建模帧和视频级别关系，以提取帧内空间特征并促进时间连贯性感知。3. 双模态对比学习（BCL），促进两种模态在统一特征空间中的对齐。

Result: 该方法在S4、MS3和AVSS数据集上取得了新的最先进（SOTA）基准。

Conclusion: 通过有效结合EIM、MTM和BCL，CCFormer成功克服了现有AVS方法的局限性，能够全面处理局部和全局信息并捕捉时空上下文，从而显著提升了音视频分割的性能。

Abstract: Audio-Visual Segmentation (AVS) aims to generate pixel-wise segmentation maps
that correlate with the auditory signals of objects. This field has seen
significant progress with numerous CNN and Transformer-based methods enhancing
the segmentation accuracy and robustness. Traditional CNN approaches manage
audio-visual interactions through basic operations like padding and
multiplications but are restricted by CNNs' limited local receptive field. More
recently, Transformer-based methods treat auditory cues as queries, utilizing
attention mechanisms to enhance audio-visual cooperation within frames.
Nevertheless, they typically struggle to extract multimodal coefficients and
temporal dynamics adequately. To overcome these limitations, we present the
Complementary and Contrastive Transformer (CCFormer), a novel framework adept
at processing both local and global information and capturing spatial-temporal
context comprehensively. Our CCFormer initiates with the Early Integration
Module (EIM) that employs a parallel bilateral architecture, merging
multi-scale visual features with audio data to boost cross-modal
complementarity. To extract the intra-frame spatial features and facilitate the
perception of temporal coherence, we introduce the Multi-query Transformer
Module (MTM), which dynamically endows audio queries with learning capabilities
and models the frame and video-level relations simultaneously. Furthermore, we
propose the Bi-modal Contrastive Learning (BCL) to promote the alignment across
both modalities in the unified feature space. Through the effective combination
of those designs, our method sets new state-of-the-art benchmarks across the
S4, MS3 and AVSS datasets. Our source code and model weights will be made
publicly available at https://github.com/SitongGong/CCFormer

</details>


### [111] [Think Twice to See More: Iterative Visual Reasoning in Medical VLMs](https://arxiv.org/abs/2510.10052)
*Kaitao Chen,Shaohao Rui,Yankai Jiang,Jiamin Wu,Qihao Zheng,Chunfeng Song,Xiaosong Wang,Mu Zhou,Mianxin Liu*

Main category: cs.CV

TL;DR: ViTAR是一个新型的医学视觉-语言模型（VLM）框架，它通过模拟人类专家迭代的“思考-行动-再思考-回答”推理过程，利用交互式图像和两阶段训练策略，显著提升了医学AI的性能和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉-语言模型（VLMs）通常采用单次推理，忽略局部视觉线索，这与人类专家迭代扫描、聚焦和精炼感兴趣区域以进行诊断的临床实践存在差距。研究旨在缩小这种机器与人类感知之间的差距。

Method: 本文提出了ViTAR框架，通过“思考-行动-再思考-回答”的认知链模拟人类专家的迭代推理过程，将医学图像视为交互对象以支持多步视觉推理。方法包括：1) 策划一个包含1K交互式示例的高质量指令数据集，编码专家级诊断行为；2) 策划一个包含16K视觉问答训练数据，用于细粒度视觉诊断；3) 采用两阶段训练策略：首先进行监督微调以指导认知轨迹，然后通过强化学习优化决策。

Result: 广泛评估表明，ViTAR的性能超越了强大的现有最先进模型。视觉注意力分析显示，从“思考”到“再思考”阶段，ViTAR逐渐将视觉基础锚定到临床关键区域，并在推理过程中保持对视觉token的高度注意力分配，这为其性能提升提供了机制上的解释。

Conclusion: 将专家风格的迭代思维链嵌入到医学视觉-语言模型（VLMs）中，能够显著提升医学AI的性能和可信度。

Abstract: Medical vision-language models (VLMs) excel at image-text understanding but
typically rely on a single-pass reasoning that neglects localized visual cues.
In clinical practice, however, human experts iteratively scan, focus, and
refine the regions of interest before reaching a final diagnosis. To narrow
this machine-human perception gap, we introduce ViTAR, a novel VLM framework
that emulates the iterative reasoning process of human experts through a
cognitive chain of "think-act-rethink-answer". ViTAR treats medical images as
interactive objects, enabling models to engage multi-step visual reasoning. To
support this approach, we curate a high-quality instruction dataset comprising
1K interactive examples that encode expert-like diagnostic behaviors. In
addition, a 16K visual question answering training data has been curated
towards fine-grained visual diagnosis. We introduce a two-stage training
strategy that begins with supervised fine-tuning to guide cognitive
trajectories, followed by the reinforcement learning to optimize
decision-making. Extensive evaluations demonstrate that ViTAR outperforms
strong state-of-the-art models. Visual attention analysis reveals that from the
"think" to "rethink" rounds, ViTAR increasingly anchors visual grounding to
clinically critical regions and maintains high attention allocation to visual
tokens during reasoning, providing mechanistic insight into its improved
performance. These findings demonstrate that embedding expert-style iterative
thinking chains into VLMs enhances both performance and trustworthiness of
medical AI.

</details>


### [112] [DREAM: A Benchmark Study for Deepfake REalism AssessMent](https://arxiv.org/abs/2510.10053)
*Bo Peng,Zichuan Wang,Sheng Yu,Xiaochuan Jin,Wei Wang,Jing Dong*

Main category: cs.CV

TL;DR: 本文关注Deepfake视频的视觉真实感评估，并提出了一个名为DREAM的综合基准，旨在近似人类感知，以促进该领域的研究。


<details>
  <summary>Details</summary>
Motivation: 当前Deepfake研究主要集中于客观检测，但其主观感知（特别是计算建模和模仿）研究不足。评估Deepfake的视觉真实感对于判断其质量和欺骗性、预测其在互联网上的影响以及改进生成过程至关重要。

Method: 本文定义了Deepfake视觉真实感评估为自动评估Deepfake视觉真实感以近似人类感知。为此，提出了DREAM（Deepfake REalism AssessMent）基准，该基准包含一个多样质量的Deepfake视频数据集、来自3,500名人类标注者的140,000个真实感分数和文本描述，并对16种代表性真实感评估方法（包括大型视觉语言模型和新提出的描述对齐CLIP方法）进行了综合评估和分析。

Result: 本文提出了DREAM基准，为Deepfake视觉真实感评估提供了一个全面的数据集、大规模人类标注（包括真实感分数和文本描述）以及对现有方法的广泛评估。研究结果包括对16种代表性方法的分析，为未来研究提供了深刻见解和基础。

Conclusion: DREAM基准和本研究提供的见解为未来在Deepfake视觉真实感评估及其他相关领域的研究奠定了坚实基础，有望推动Deepfake质量评估和理解人类感知方面的进展。

Abstract: Deep learning based face-swap videos, widely known as deepfakes, have drawn
wide attention due to their threat to information credibility. Recent works
mainly focus on the problem of deepfake detection that aims to reliably tell
deepfakes apart from real ones, in an objective way. On the other hand, the
subjective perception of deepfakes, especially its computational modeling and
imitation, is also a significant problem but lacks adequate study. In this
paper, we focus on the visual realism assessment of deepfakes, which is defined
as the automatic assessment of deepfake visual realism that approximates human
perception of deepfakes. It is important for evaluating the quality and
deceptiveness of deepfakes which can be used for predicting the influence of
deepfakes on Internet, and it also has potentials in improving the deepfake
generation process by serving as a critic. This paper prompts this new
direction by presenting a comprehensive benchmark called DREAM, which stands
for Deepfake REalism AssessMent. It is comprised of a deepfake video dataset of
diverse quality, a large scale annotation that includes 140,000 realism scores
and textual descriptions obtained from 3,500 human annotators, and a
comprehensive evaluation and analysis of 16 representative realism assessment
methods, including recent large vision language model based methods and a newly
proposed description-aligned CLIP method. The benchmark and insights included
in this study can lay the foundation for future research in this direction and
other related areas.

</details>


### [113] [Collaborative Learning of Semantic-Aware Feature Learning and Label Recovery for Multi-Label Image Recognition with Incomplete Labels](https://arxiv.org/abs/2510.10055)
*Zhi-Fen He,Ren-Dong Xie,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 本文提出了一种名为CLSL的新型协同学习方法，用于解决不完整标签多标签图像识别中的语义感知特征学习和缺失标签恢复两大挑战，并在多个公开数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多标签图像识别中存在不完整标签的问题，这带来了两个核心挑战：如何学习语义感知的特征，以及如何恢复缺失的标签。

Method: 本文提出了协同学习语义感知特征学习和标签恢复（CLSL）方法，将这两个挑战统一在一个框架中。具体而言，它设计了一个语义相关特征学习模块来发现语义信息和标签相关性；一个语义引导特征增强模块来对齐视觉和语义特征空间以生成高质量特征；最后，一个协同学习框架整合了特征学习和标签恢复，形成一个相互强化的循环。

Result: 在MS-COCO、VOC2007和NUS-WIDE这三个广泛使用的公共数据集上进行的实验表明，CLSL方法优于目前最先进的带有不完整标签的多标签图像识别方法。

Conclusion: CLSL通过统一语义感知特征学习和标签恢复，并利用协同学习机制，有效解决了不完整标签多标签图像识别中的关键问题，显著提升了识别性能。

Abstract: Multi-label image recognition with incomplete labels is a critical learning
task and has emerged as a focal topic in computer vision. However, this task is
confronted with two core challenges: semantic-aware feature learning and
missing label recovery. In this paper, we propose a novel Collaborative
Learning of Semantic-aware feature learning and Label recovery (CLSL) method
for multi-label image recognition with incomplete labels, which unifies the two
aforementioned challenges into a unified learning framework. More specifically,
we design a semantic-related feature learning module to learn robust
semantic-related features by discovering semantic information and label
correlations. Then, a semantic-guided feature enhancement module is proposed to
generate high-quality discriminative semantic-aware features by effectively
aligning visual and semantic feature spaces. Finally, we introduce a
collaborative learning framework that integrates semantic-aware feature
learning and label recovery, which can not only dynamically enhance the
discriminability of semantic-aware features but also adaptively infer and
recover missing labels, forming a mutually reinforced loop between the two
processes. Extensive experiments on three widely used public datasets (MS-COCO,
VOC2007, and NUS-WIDE) demonstrate that CLSL outperforms the state-of-the-art
multi-label image recognition methods with incomplete labels.

</details>


### [114] [Probabilistic Hyper-Graphs using Multiple Randomly Masked Autoencoders for Semi-supervised Multi-modal Multi-task Learning](https://arxiv.org/abs/2510.10068)
*Pîrvu Mihai-Cristian,Leordeanu Marius*

Main category: cs.CV

TL;DR: 该研究引入了PHG-MAE模型，它将神经图与掩码自编码器（MAE）结合，通过随机掩码整个模态、单循环训练、推理时集成以及知识蒸馏，显著提升了多模态计算机视觉任务的性能和一致性，特别是在无人机场景中。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉领域受益于大量数据，特别是通过掩码自编码器（MAE）等自监督预训练方法，无需手动标注数据即可改进各种视觉任务。本研究旨在进一步提升多模态视觉任务的性能，并解决复杂场景（如无人机）中的多模态解释问题。

Method: 本研究提出了一种名为概率超图掩码自编码器（PHG-MAE）的新模型，它统一了经典的神经图工作和现代的掩码自编码器方法。该模型通过随机掩码整个模态（而非仅补丁）来从超边分布中采样。它还将预训练和微调结合到单一训练循环中，并支持创建推理时集成以提升预测性能和一致性。此外，还在集成模型之上应用了知识蒸馏。为简化外部预训练专家集成，开发了数据管道软件，并创建并发布了Dronescapes数据集的完全自动化扩展。

Result: PHG-MAE模型通过聚合推理时集成，显著提升了最终预测的性能和一致性。即使使用参数少于1M的模型，在集成之上应用知识蒸馏也能实现性能损失极小。该方法主要应用于户外无人机场景，但也可推广到自动驾驶或室内机器人等其他类似领域。所有技术细节、代码和复现步骤均已公开。

Conclusion: PHG-MAE提供了一个新颖的理论框架，将神经图与掩码自编码器结合，通过创新的模态掩码、单循环训练和集成方法，有效提升了多模态计算机视觉任务的性能、一致性和效率，尤其适用于复杂的多模态环境，并且支持知识蒸馏以创建更轻量级的模型。

Abstract: The computer vision domain has greatly benefited from an abundance of data
across many modalities to improve on various visual tasks. Recently, there has
been a lot of focus on self-supervised pre-training methods through Masked
Autoencoders (MAE) \cite{he2022masked,bachmann2022multimae}, usually used as a
first step before optimizing for a downstream task, such as classification or
regression. This is very useful as it doesn't require any manually labeled
data. In this work, we introduce Probabilistic Hyper-Graphs using Masked
Autoencoders (PHG-MAE): a novel model that unifies the classical work on neural
graphs \cite{leordeanu2021semi} with the modern approach of masked autoencoders
under a common theoretical framework. Through random masking of entire
modalities, not just patches, the model samples from the distribution of
hyper-edges on each forward pass. Additionally, the model adapts the standard
MAE algorithm by combining pre-training and fine-tuning into a single training
loop. Moreover, our approach enables the creation of inference-time ensembles
which, through aggregation, boost the final prediction performance and
consistency. Lastly, we show that we can apply knowledge distillation on top of
the ensembles with little loss in performance, even with models that have fewer
than 1M parameters. While our work mostly focuses on outdoor UAV scenes that
contain multiple world interpretations and modalities, the same steps can be
followed in other similar domains, such as autonomous driving or indoor
robotics. In order to streamline the process of integrating external
pre-trained experts for computer vision multi-modal multi-task learning (MTL)
scenarios, we developed a data-pipeline software. Using this tool, we have
created and released a fully-automated extension of the Dronescapes dataset.
All the technical details, code and reproduction steps are publicly released.

</details>


### [115] [Tracking the Spatiotemporal Evolution of Landslide Scars Using a Vision Foundation Model: A Novel and Universal Framework](https://arxiv.org/abs/2510.10084)
*Meijun Zhou,Gang Mei,Zhengjing Ma,Nengxiong Xu,Jianbing Peng*

Main category: cs.CV

TL;DR: 本研究提出了一种新颖且通用的框架，利用视觉基础模型将离散遥感图像重构为连续视频序列，以实现大规模滑坡疤痕时空演化的连续跟踪，从而支持早期预警和灾后评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多集中于单阶段或灾前灾后双阶段滑坡识别，难以跟踪滑坡疤痕的时空演化。然而，跟踪滑坡疤痕的时空演化对于理解其演化机制、识别失效前兆以及实现有效预警至关重要。

Method: 该框架的核心思想是将离散光学遥感图像重构为连续视频序列。这种转换使得为视频分割开发的视觉基础模型能够用于跟踪滑坡疤痕的演化。该框架在一个知识引导、自动传播和交互式细化的范式下运行，以确保滑坡疤痕的连续和准确识别。

Result: 该框架已在两个代表性案例（灾后的白格滑坡和活跃的色拉滑坡，2017-2025年）中得到验证。结果表明，所提出的框架能够连续跟踪滑坡疤痕，捕获对早期预警至关重要的失效前兆，以及对评估次生灾害和长期稳定性至关重要的灾后演化。

Conclusion: 所提出的框架能够连续跟踪滑坡疤痕的时空演化，为理解滑坡机制、识别失效前兆、实现早期预警以及评估灾后次生灾害和长期稳定性提供了关键信息。

Abstract: Tracking the spatiotemporal evolution of large-scale landslide scars is
critical for understanding the evolution mechanisms and failure precursors,
enabling effective early-warning. However, most existing studies have focused
on single-phase or pre- and post-failure dual-phase landslide identification.
Although these approaches delineate post-failure landslide boundaries, it is
challenging to track the spatiotemporal evolution of landslide scars. To
address this problem, this study proposes a novel and universal framework for
tracking the spatiotemporal evolution of large-scale landslide scars using a
vision foundation model. The key idea behind the proposed framework is to
reconstruct discrete optical remote sensing images into a continuous video
sequence. This transformation enables a vision foundation model, which is
developed for video segmentation, to be used for tracking the evolution of
landslide scars. The proposed framework operates within a knowledge-guided,
auto-propagation, and interactive refinement paradigm to ensure the continuous
and accurate identification of landslide scars. The proposed framework was
validated through application to two representative cases: the post-failure
Baige landslide and the active Sela landslide (2017-2025). Results indicate
that the proposed framework enables continuous tracking of landslide scars,
capturing both failure precursors critical for early warning and post-failure
evolution essential for assessing secondary hazards and long-term stability.

</details>


### [116] [Gesplat: Robust Pose-Free 3D Reconstruction via Geometry-Guided Gaussian Splatting](https://arxiv.org/abs/2510.10097)
*Jiahui Lu,Haihong Xiao,Xueyan Zhao,Wenxiong Kang*

Main category: cs.CV

TL;DR: Gesplat是一种基于3DGS的框架，通过利用VGGT模型获取可靠的初始姿态和点云，并结合混合高斯表示、图引导属性细化和基于流的深度正则化，实现了从无姿态稀疏图像进行鲁棒的新视图合成和几何一致性重建。


<details>
  <summary>Details</summary>
Motivation: NeRF和3DGS在3D重建和新视图合成方面表现出色，但严重依赖于准确的相机姿态和密集的视角覆盖。这些要求限制了它们在稀疏视图设置中的适用性，因为在这种情况下姿态估计变得不可靠且监督不足。

Method: 该方法引入了Gesplat框架，其关键创新包括：1) 利用VGGT基础模型而非COLMAP获取更可靠的初始姿态和密集点云；2) 采用混合高斯表示，通过双重位置-形状优化并增强视图间匹配一致性；3) 一个图引导的属性细化模块以增强场景细节；4) 基于流的深度正则化以提高深度估计精度，从而实现更有效的监督。

Result: 全面的定量和定性实验表明，与其他无姿态方法相比，该方法在正向场景和大规模复杂数据集上都取得了更鲁棒的性能。

Conclusion: Gesplat通过整合多项创新，成功克服了从无姿态稀疏图像进行新视图合成和几何一致性重建的挑战，展现出强大的鲁棒性。

Abstract: Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have advanced
3D reconstruction and novel view synthesis, but remain heavily dependent on
accurate camera poses and dense viewpoint coverage. These requirements limit
their applicability in sparse-view settings, where pose estimation becomes
unreliable and supervision is insufficient. To overcome these challenges, we
introduce Gesplat, a 3DGS-based framework that enables robust novel view
synthesis and geometrically consistent reconstruction from unposed sparse
images. Unlike prior works that rely on COLMAP for sparse point cloud
initialization, we leverage the VGGT foundation model to obtain more reliable
initial poses and dense point clouds. Our approach integrates several key
innovations: 1) a hybrid Gaussian representation with dual position-shape
optimization enhanced by inter-view matching consistency; 2) a graph-guided
attribute refinement module to enhance scene details; and 3) flow-based depth
regularization that improves depth estimation accuracy for more effective
supervision. Comprehensive quantitative and qualitative experiments demonstrate
that our approach achieves more robust performance on both forward-facing and
large-scale complex datasets compared to other pose-free methods.

</details>


### [117] [Cooperative Pseudo Labeling for Unsupervised Federated Classification](https://arxiv.org/abs/2510.10100)
*Kuangpu Guo,Lijun Sheng,Yongcan Yu,Jian Liang,Zilei Wang,Ran He*

Main category: cs.CV

TL;DR: 本文首次将无监督联邦学习（UFL）扩展到基于CLIP的分类问题，并提出了一种名为FedCoPL的新方法，通过协作伪标签和部分提示聚合实现了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 以往的无监督联邦学习（UFL）主要集中于表示学习和聚类任务。随着像CLIP这样强大的视觉语言模型出现，其零样本预测能力为UFL范式下的分类问题带来了新的机遇，但这一领域尚未被充分探索。

Method: 本文提出了FedCoPL方法。具体而言，客户端估计并上传其伪标签分布，服务器对这些分布进行调整和重新分配以避免全局类别不平衡。此外，引入了部分提示聚合协议，其中包含通用图像特征的视觉提示在服务器端聚合，而编码个性化知识的文本提示则保留在本地。

Result: 广泛的实验证明，FedCoPL相比基线方法展现出卓越的性能。

Conclusion: FedCoPL成功地将无监督联邦学习应用于基于CLIP的分类问题，通过协作伪标签和部分提示聚合实现了有效的联邦协作和个性化，并取得了优于现有方法的成果。

Abstract: Unsupervised Federated Learning (UFL) aims to collaboratively train a global
model across distributed clients without sharing data or accessing label
information. Previous UFL works have predominantly focused on representation
learning and clustering tasks. Recently, vision language models (e.g., CLIP)
have gained significant attention for their powerful zero-shot prediction
capabilities. Leveraging this advancement, classification problems that were
previously infeasible under the UFL paradigm now present promising new
opportunities, yet remain largely unexplored. In this paper, we extend UFL to
the classification problem with CLIP for the first time and propose a novel
method, \underline{\textbf{Fed}}erated \underline{\textbf{Co}}operative
\underline{\textbf{P}}seudo \underline{\textbf{L}}abeling (\textbf{FedCoPL}).
Specifically, clients estimate and upload their pseudo label distribution, and
the server adjusts and redistributes them to avoid global imbalance among
classes. Moreover, we introduce a partial prompt aggregation protocol for
effective collaboration and personalization. In particular, visual prompts
containing general image features are aggregated at the server, while text
prompts encoding personalized knowledge are retained locally. Extensive
experiments demonstrate the superior performance of our FedCoPL compared to
baseline methods. Our code is available at
\href{https://github.com/krumpguo/FedCoPL}{https://github.com/krumpguo/FedCoPL}.

</details>


### [118] [Answer-Consistent Chain-of-thought Reinforcement Learning For Multi-modal Large Langauge Models](https://arxiv.org/abs/2510.10104)
*Minbin Huang,Runhui Huang,Chuanyang Zheng,Jingyao Li,Guoxuan Chen,Han Shi,Hong Cheng*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ACRE的答案一致性强化学习方法，通过引入辅助一致性检查和一致性验证奖励来解决多模态大语言模型（LLMs）在RLVR中推理链与最终答案不一致的问题，并在视频和数学推理任务上取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管基于结果的强化学习（RLVR）能提高LLMs的答案准确性，但它可能无意中导致推理链与最终答案脱钩，产生不一致。例如，在MMVU多项选择视觉问答任务中，标准GRPO方法在推理步骤和答案之间的一致性仅为79.7%。

Method: 本文提出了答案一致性强化学习（ACRE）方法，它修改了GRPO算法并引入了一个辅助一致性检查。具体来说，在模型生成思维链和初步答案后，会打乱答案选项并用相同的推理轨迹再次提示模型预测第二个答案。设计了一个一致性验证奖励机制：只有当原始答案和打乱后的答案一致且都正确时才给予高奖励；否则，分配较低的奖励。这惩罚了推理-答案不一致，并阻止模型依赖虚假模式（如选项顺序偏差）。

Result: ACRE在挑战性的视频推理基准和多模态数学推理基准上进行了评估，与GRPO基线相比，在视频推理任务上平均提高了2.2%，在数学推理任务上平均提高了1.5%。

Conclusion: ACRE通过解决推理链与最终答案之间的不一致问题，显著提升了多模态大语言模型在复杂视频和图像理解任务上的表现，证明了其在提高模型可靠性和性能方面的有效性。

Abstract: Recent advances in large language models (LLMs) have demonstrated that
reinforcement learning with verifiable rewards (RLVR) can significantly enhance
reasoning abilities by directly optimizing correctness, rather than relying
solely on supervised imitation. This paradigm has been extended to multimodal
LLMs for complex video and image understanding tasks. However, while
outcome-driven RL improves answer accuracy, it can inadvertently decouple the
reasoning chain from the final answer, leading to situations where models
produce inconsistency between the reasoning trace and final answer. In our
experiments on multiple-choice visual question-answering tasks, the standard
GRPO method yields only 79.7\% consistency on MMVU between the reasoning steps
and the chosen answers, indicating frequent mismatches between answers and
reasoning. To this end, we propose Answer-Consistent Reinforcement Learning
(ACRE) that modifies the GRPO algorithm with an auxiliary consistency check.
After the model generates a chain of thought and an initial answer for a given
question, we shuffle the answer options and prompt the model again with the
same reasoning trace to predict a second answer. We design a
consistency-verification reward that grants a high reward only if both the
original and the post-shuffle answers agree and are correct; otherwise, a lower
reward is assigned accordingly. This mechanism penalizes reasoning-answer
misalignment and discourages the model from relying on spurious patterns, such
as option ordering biases. We evaluate ACRE on challenging Video Reasoning
benchmarks and multimodal math reasoning benchmarks, achieving an average 2.2\%
and 1.5\% improvement for Video Reasoning and Math Reasoning tasks over the
GRPO baseline.

</details>


### [119] [Training-Free In-Context Forensic Chain for Image Manipulation Detection and Localization](https://arxiv.org/abs/2510.10111)
*Rui Chen,Bin Liu,Changtao Miao,Xinghao Wang,Yi Li,Tao Gong,Qi Chu,Nenghai Yu*

Main category: cs.CV

TL;DR: 本文提出了In-Context Forensic Chain (ICFC)，一个无需训练的框架，利用多模态大语言模型（MLLMs）实现可解释的图像篡改定位（IML），并在多个基准测试中超越了现有无训练和弱监督方法。


<details>
  <summary>Details</summary>
Motivation: 图像篡改技术进步带来了严重的安全威胁，因此急需有效的图像篡改定位（IML）方法。然而，监督式IML依赖于昂贵的像素级标注，而现有的弱监督或无需训练的替代方案往往性能不佳且缺乏可解释性。

Method: 本文提出了ICFC框架，这是一个无需训练的框架，利用多模态大语言模型（MLLMs）进行可解释的IML任务。ICFC整合了客体化规则构建和自适应过滤机制，以建立可靠的知识库；并采用多步渐进推理流程，模拟专家取证工作流，从粗略提议到精细取证结果。这种设计使得系统能够利用MLLM的推理能力进行图像级分类、像素级定位和文本级可解释性。

Result: 在多个基准测试中，ICFC不仅超越了最先进的无需训练方法，而且与弱监督和全监督方法相比，也达到了有竞争力或更优的性能。它成功实现了图像级分类、像素级定位和文本级可解释性。

Conclusion: ICFC提供了一种无需训练、有效且可解释的图像篡改定位解决方案，展示了多模态大语言模型在法证分析任务中的巨大潜力，并解决了传统方法对昂贵标注的依赖以及性能和可解释性不足的问题。

Abstract: Advances in image tampering pose serious security threats, underscoring the
need for effective image manipulation localization (IML). While supervised IML
achieves strong performance, it depends on costly pixel-level annotations.
Existing weakly supervised or training-free alternatives often underperform and
lack interpretability. We propose the In-Context Forensic Chain (ICFC), a
training-free framework that leverages multi-modal large language models
(MLLMs) for interpretable IML tasks. ICFC integrates an objectified rule
construction with adaptive filtering to build a reliable knowledge base and a
multi-step progressive reasoning pipeline that mirrors expert forensic
workflows from coarse proposals to fine-grained forensics results. This design
enables systematic exploitation of MLLM reasoning for image-level
classification, pixel-level localization, and text-level interpretability.
Across multiple benchmarks, ICFC not only surpasses state-of-the-art
training-free methods but also achieves competitive or superior performance
compared to weakly and fully supervised approaches.

</details>


### [120] [ImmerIris: A Large-Scale Dataset and Benchmark for Immersive Iris Recognition in Open Scenes](https://arxiv.org/abs/2510.10113)
*Yuxi Mi,Qiuyang Yuan,Zhizhou Zhong,Xuan Zhao,Jiaogen Zhou,Fubao Zhu,Jihong Guan,Shuigeng Zhou*

Main category: cs.CV

TL;DR: 本文针对沉浸式应用中离轴虹膜识别的挑战，提出了一个大型离轴虹膜数据集ImmerIris，并介绍了一种无需归一化的识别范式，该范式在离轴场景下表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 沉浸式虹膜识别在增强现实/虚拟现实等应用中日益重要，但其主要通过倾斜放置的头戴式摄像头捕获离轴虹膜，导致透视畸变、质量下降和虹膜纹理类内变化等独特挑战。现有数据集稀缺，且传统方法在沉浸式设置中表现不佳。

Method: 本文引入了ImmerIris数据集，该数据集通过VR头显收集，包含来自564名受试者的499,791张眼部图像，是目前最大的公共离轴虹膜数据集。基于ImmerIris，构建了评估协议以基准测试不同挑战因素下的识别方法。此外，提出了一种无需归一化的范式，直接从眼部图像中学习，仅进行最小调整。

Result: ImmerIris是目前最大的公共数据集，也是首批专注于离轴采集的数据集之一。针对经典轴上图像设计的现有方法在沉浸式设置中表现不佳，主要是因为依赖于易出错的归一化。本文提出的无需归一化的方法，尽管简单，但始终优于基于归一化的同类方法。

Conclusion: 无需归一化的范式为鲁棒的沉浸式虹膜识别指明了有前景的方向，有效应对了离轴采集带来的挑战，并超越了传统归一化方法的局限性。

Abstract: In egocentric applications such as augmented and virtual reality, immersive
iris recognition is emerging as an accurate and seamless way to identify
persons. While classic systems acquire iris images on-axis, i.e., via dedicated
frontal sensors in controlled settings, the immersive setup primarily captures
off-axis irises through tilt-placed headset cameras, with only mild control in
open scenes. This yields unique challenges, including perspective distortion,
intensified quality degradations, and intra-class variations in iris texture.
Datasets capturing these challenges remain scarce. To fill this gap, this paper
introduces ImmerIris, a large-scale dataset collected via VR headsets,
containing 499,791 ocular images from 564 subjects. It is, to the best of
current knowledge, the largest public dataset and among the first dedicated to
off-axis acquisition. Based on ImmerIris, evaluation protocols are constructed
to benchmark recognition methods under different challenging factors. Current
methods, primarily designed for classic on-axis imagery, perform
unsatisfactorily on the immersive setup, mainly due to reliance on fallible
normalization. To this end, this paper further proposes a normalization-free
paradigm that directly learns from ocular images with minimal adjustment.
Despite its simplicity, this approach consistently outperforms
normalization-based counterparts, pointing to a promising direction for robust
immersive recognition.

</details>


### [121] [DeepFusionNet: Autoencoder-Based Low-Light Image Enhancement and Super-Resolution](https://arxiv.org/abs/2510.10122)
*Halil Hüseyin Çalışkan,Talha Koruk*

Main category: cs.CV

TL;DR: 本文提出了一种名为DeepFusionNet的轻量级架构，用于解决低光图像增强和图像超分辨率问题，在保持高性能的同时显著减少了参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的低光图像增强方法（如基于自编码器的方法）通常SSIM和PSNR分数较低，且由于参数量大而需要高计算能力。此外，将模糊和低分辨率图像转换为高分辨率和无模糊图像在图像处理中也日益重要。

Method: 本文开发了DeepFusionNet架构。对于低光图像增强，它采用DeepFusionNet架构的自编码器。对于超分辨率，它也开发了一个基于DeepFusionNet架构的自编码器模型，而不是GAN-based的方法。

Result: 在LOL-v1数据集上，DeepFusionNet在低光增强方面实现了92.8%的SSIM和26.30的PSNR，而参数量仅约250万。基于DeepFusionNet的超分辨率方法在验证集上实现了25.30的PSNR和80.7%的SSIM，参数量仅约10万。

Conclusion: DeepFusionNet架构为低光图像增强和图像超分辨率提供了一种高效且高性能的解决方案，显著减少了模型参数，从而降低了计算成本，同时保持了良好的图像质量指标。

Abstract: Computer vision and image processing applications suffer from dark and
low-light images, particularly during real-time image transmission. Currently,
low light and dark images are converted to bright and colored forms using
autoencoders; however, these methods often achieve low SSIM and PSNR scores and
require high computational power due to their large number of parameters. To
address these challenges, the DeepFusionNet architecture has been developed.
According to the results obtained with the LOL-v1 dataset, DeepFusionNet
achieved an SSIM of 92.8% and a PSNR score of 26.30, while containing only
approximately 2.5 million parameters. On the other hand, conversion of blurry
and low-resolution images into high-resolution and blur-free images has gained
importance in image processing applications. Unlike GAN-based super-resolution
methods, an autoencoder-based super resolution model has been developed that
contains approximately 100 thousand parameters and uses the DeepFusionNet
architecture. According to the results of the tests, the DeepFusionNet based
super-resolution method achieved a PSNR of 25.30 and a SSIM score of 80.7
percent according to the validation set.

</details>


### [122] [Color3D: Controllable and Consistent 3D Colorization with Personalized Colorizer](https://arxiv.org/abs/2510.10152)
*Yecong Wan,Mingwen Shao,Renlong Wu,Wangmeng Zuo*

Main category: cs.CV

TL;DR: Color3D是一个高度适应性的框架，能够对静态和动态3D场景进行着色，通过个性化着色器和单视图着色范式，实现视觉多样、色彩丰富且用户可控的重建，同时确保跨视图和跨时间的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态场景，通过平均颜色变化来强制实现多视图一致性，但这不可避免地牺牲了色彩丰富性和可控性。本研究旨在解决如何在保持色彩多样性和可控性的同时，确保跨视图和跨时间的一致性。

Method: 该方法的核心是仅对单个关键视图进行着色，然后微调一个个性化的着色器，将其颜色传播到新视图和时间步。通过个性化，着色器学习参考视图下场景特定的确定性颜色映射，利用其固有的归纳偏差将相应颜色一致地投影到新视图和视频帧中的内容。一旦训练完成，个性化着色器可用于推断所有其他图像的一致色度，并通过专用的Lab色彩空间高斯泼溅表示直接重建彩色3D场景。该框架将复杂的3D着色重塑为更易处理的单图像范式，允许无缝集成任意图像着色模型。

Result: 在各种静态和动态3D着色基准上的大量实验表明，该方法能够提供更一致、色彩更丰富的渲染，并实现精确的用户控制。

Conclusion: Color3D框架通过将3D着色重新定义为可控的单图像着色问题，并通过个性化着色器实现颜色传播，有效解决了现有方法在色彩丰富性、可控性和一致性方面的局限性，为静态和动态3D场景着色提供了一个灵活且高性能的解决方案。

Abstract: In this work, we present Color3D, a highly adaptable framework for colorizing
both static and dynamic 3D scenes from monochromatic inputs, delivering
visually diverse and chromatically vibrant reconstructions with flexible
user-guided control. In contrast to existing methods that focus solely on
static scenarios and enforce multi-view consistency by averaging color
variations which inevitably sacrifice both chromatic richness and
controllability, our approach is able to preserve color diversity and
steerability while ensuring cross-view and cross-time consistency. In
particular, the core insight of our method is to colorize only a single key
view and then fine-tune a personalized colorizer to propagate its color to
novel views and time steps. Through personalization, the colorizer learns a
scene-specific deterministic color mapping underlying the reference view,
enabling it to consistently project corresponding colors to the content in
novel views and video frames via its inherent inductive bias. Once trained, the
personalized colorizer can be applied to infer consistent chrominance for all
other images, enabling direct reconstruction of colorful 3D scenes with a
dedicated Lab color space Gaussian splatting representation. The proposed
framework ingeniously recasts complicated 3D colorization as a more tractable
single image paradigm, allowing seamless integration of arbitrary image
colorization models with enhanced flexibility and controllability. Extensive
experiments across diverse static and dynamic 3D colorization benchmarks
substantiate that our method can deliver more consistent and chromatically rich
renderings with precise user control. Project Page
https://yecongwan.github.io/Color3D/.

</details>


### [123] [Stroke Locus Net: Occluded Vessel Localization from MRI Modalities](https://arxiv.org/abs/2510.10155)
*Mohamed Hamad,Muhammad Khan,Tamer Khattab,Mohamed Mabrok*

Main category: cs.CV

TL;DR: 本文提出Stroke Locus Net，一个端到端深度学习流水线，仅使用MRI扫描即可实现缺血性卒中病灶检测、分割和闭塞血管定位，有望加速卒中诊断。


<details>
  <summary>Details</summary>
Motivation: 缺血性卒中诊断中的一个关键挑战是准确地定位闭塞血管。目前的机器学习方法主要集中在病灶分割，而对血管定位的研究有限。

Method: 引入Stroke Locus Net，一个端到端深度学习流水线。该系统结合了：1) 使用nnUNet进行病灶检测的分割分支；2) 用于血管映射和识别的动脉图谱；3) 使用pGAN从MRI合成MRA图像的生成分支。

Result: 在受卒中影响的T1 MRI扫描上，该实现展示了在定位闭塞血管方面有前景的结果。

Conclusion: 该研究成果具有加速和提供更明智的卒中诊断的潜力。

Abstract: A key challenge in ischemic stroke diagnosis using medical imaging is the
accurate localization of the occluded vessel. Current machine learning methods
in focus primarily on lesion segmentation, with limited work on vessel
localization. In this study, we introduce Stroke Locus Net, an end-to-end deep
learning pipeline for detection, segmentation, and occluded vessel localization
using only MRI scans. The proposed system combines a segmentation branch using
nnUNet for lesion detection with an arterial atlas for vessel mapping and
identification, and a generation branch using pGAN to synthesize MRA images
from MRI. Our implementation demonstrates promising results in localizing
occluded vessels on stroke-affected T1 MRI scans, with potential for faster and
more informed stroke diagnosis.

</details>


### [124] [Multi Class Parkinsons Disease Detection Based on Finger Tapping Using Attention-Enhanced CNN BiLSTM](https://arxiv.org/abs/2510.10121)
*Abu Saleh Musa Miah,Najmul Hassan,Md Maruf Al Hossain,Yuichi Okuyama,Jungpil Shin*

Main category: cs.CV

TL;DR: 本研究提出了一种基于手指敲击视频的多类别帕金森病（PD）严重程度检测系统，该系统利用注意力增强的CNN BiLSTM混合深度学习框架，实现了对五种PD严重程度的有效区分。


<details>
  <summary>Details</summary>
Motivation: 帕金森病严重程度的准确评估对于有效的临床管理和干预开发至关重要。然而，许多现有的基于手势的PD识别系统性能准确性不尽如人意。

Method: 研究者收集了手指敲击视频，并从中提取了基于手腕和手部运动的时间、频率和幅度特征。然后，他们提出了一个混合深度学习框架，该框架整合了CNN（用于捕获局部空间依赖）、BiLSTM（用于建模时间动态）和注意力机制（用于聚焦最具信息量的时间特征）。最终，将CNN提取的特征与注意力增强的BiLSTM输出进行拼接，并通过全连接层和softmax分类器输出预测的PD严重程度。

Result: 该模型在区分五种严重程度类别方面表现出强大的性能。

Conclusion: 研究表明，将时空表示与注意力机制相结合可以提高帕金森病严重程度的自动化检测能力，使其成为一种有前景的非侵入性工具，可支持临床医生进行PD监测和疾病进展追踪。

Abstract: Effective clinical management and intervention development depend on accurate
evaluation of Parkinsons disease (PD) severity. Many researchers have worked on
developing gesture-based PD recognition systems; however, their performance
accuracy is not satisfactory. In this study, we propose a multi-class Parkinson
Disease detection system based on finger tapping using an attention-enhanced
CNN BiLSTM. We collected finger tapping videos and derived temporal, frequency,
and amplitude based features from wrist and hand movements. Then, we proposed a
hybrid deep learning framework integrating CNN, BiLSTM, and attention
mechanisms for multi-class PD severity classification from video-derived motion
features. First, the input sequence is reshaped and passed through a Conv1D
MaxPooling block to capture local spatial dependencies. The resulting feature
maps are fed into a BiLSTM layer to model temporal dynamics. An attention
mechanism focuses on the most informative temporal features, producing a
context vector that is further processed by a second BiLSTM layer. CNN-derived
features and attention-enhanced BiLSTM outputs are concatenated, followed by
dense and dropout layers, before the final softmax classifier outputs the
predicted PD severity level. The model demonstrated strong performance in
distinguishing between the five severity classes, suggesting that integrating
spatial temporal representations with attention mechanisms can improve
automated PD severity detection, making it a promising non-invasive tool to
support clinicians in PD monitoring and progression tracking.

</details>


### [125] [ReMix: Towards a Unified View of Consistent Character Generation and Editing](https://arxiv.org/abs/2510.10156)
*Benjia Zhou,Bin Fu,Pei Cheng,Yanru Wang,Jiayuan Fan,Tao Chen*

Main category: cs.CV

TL;DR: ReMix是一个统一的框架，旨在解决现有方法在角色一致性生成和编辑方面的不足，通过引入ReMix模块和IP-ControlNet实现语义和像素级别的一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模文本到图像扩散模型在视觉保真度上取得了进步，但在单个框架内统一角色一致性生成和编辑仍然存在挑战。基于生成的模型难以在不同实例间保持精细的身份一致性，而基于编辑的方法则常失去空间可控性和指令对齐能力。

Method: ReMix框架包含两个核心组件：1. ReMix模块：利用多模态大语言模型（MLLMs）的推理能力编辑输入图像的语义特征，并将指令嵌入适应到DiT骨干网络，无需微调，以确保连贯的语义布局。2. IP-ControlNet：扩展了ControlNet，用于解耦参考图像中的语义和布局线索，并引入了一个ε-等变潜在空间，在共享噪声空间内共同去噪参考和目标图像，从而促进隐藏空间中的特征对齐，实现像素级一致性和姿态可控性，灵感来源于趋同进化和量子退相干。

Result: ReMix支持广泛的任务，包括个性化生成、图像编辑、风格迁移和多条件合成。实验证明其作为角色一致性图像生成和编辑的统一框架，具有有效性和效率。

Conclusion: ReMix成功地将角色一致性生成和编辑任务统一到一个框架中，通过其创新的模块设计解决了现有方法的局限性，并在多种任务中展现了卓越的性能。

Abstract: Recent advances in large-scale text-to-image diffusion models (e.g., FLUX.1)
have greatly improved visual fidelity in consistent character generation and
editing. However, existing methods rarely unify these tasks within a single
framework. Generation-based approaches struggle with fine-grained identity
consistency across instances, while editing-based methods often lose spatial
controllability and instruction alignment. To bridge this gap, we propose
ReMix, a unified framework for character-consistent generation and editing. It
constitutes two core components: the ReMix Module and IP-ControlNet. The ReMix
Module leverages the multimodal reasoning ability of MLLMs to edit semantic
features of input images and adapt instruction embeddings to the native DiT
backbone without fine-tuning. While this ensures coherent semantic layouts,
pixel-level consistency and pose controllability remain challenging. To address
this, IP-ControlNet extends ControlNet to decouple semantic and layout cues
from reference images and introduces an {\epsilon}-equivariant latent space
that jointly denoises the reference and target images within a shared noise
space. Inspired by convergent evolution and quantum decoherence,i.e., where
environmental noise drives state convergence, this design promotes feature
alignment in the hidden space, enabling consistent object generation while
preserving identity. ReMix supports a wide range of tasks, including
personalized generation, image editing, style transfer, and multi-condition
synthesis. Extensive experiments validate its effectiveness and efficiency as a
unified framework for character-consistent image generation and editing.

</details>


### [126] [SaFiRe: Saccade-Fixation Reiteration with Mamba for Referring Image Segmentation](https://arxiv.org/abs/2510.10160)
*Zhenjie Mao,Yuhuan Yang,Chaofan Ma,Dongsheng Jiang,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SaFiRe的新框架，旨在解决指代图像分割（RIS）中复杂的、模糊的自然语言表达所带来的挑战。SaFiRe模仿人类两阶段认知过程，并利用Mamba的特性进行高效多周期细化。同时，引入了新的基准aRefCOCO以评估模型在模糊表达下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的指代图像分割（RIS）方法主要关注简单的表达，如“红色汽车”或“左边的女孩”，这使得RIS问题简化为关键词匹配，限制了模型处理指代模糊性表达的能力。实际场景中存在两种挑战性情况：涉及多个实体和上下文线索的“物体干扰性表达”以及未明确说明物体类别的“类别隐含性表达”。

Method: 提出SaFiRe框架，模仿人类两阶段认知过程：首先形成全局理解，然后通过细节导向的检查进行细化。该设计利用Mamba的“扫描-更新”特性，实现高效且具有线性复杂度的多周期细化。此外，还引入了aRefCOCO，一个新的基准数据集，专门用于评估RIS模型在模糊指代表达下的性能。

Result: 在标准数据集和新提出的aRefCOCO数据集上进行的广泛实验表明，SaFiRe框架优于现有的最先进基线方法。

Conclusion: SaFiRe通过其模仿人类认知过程的两阶段设计和利用Mamba特性的高效细化机制，有效解决了指代图像分割中模糊表达的挑战，并在实验中展现出卓越的性能。

Abstract: Referring Image Segmentation (RIS) aims to segment the target object in an
image given a natural language expression. While recent methods leverage
pre-trained vision backbones and more training corpus to achieve impressive
results, they predominantly focus on simple expressions--short, clear noun
phrases like "red car" or "left girl". This simplification often reduces RIS to
a key word/concept matching problem, limiting the model's ability to handle
referential ambiguity in expressions. In this work, we identify two challenging
real-world scenarios: object-distracting expressions, which involve multiple
entities with contextual cues, and category-implicit expressions, where the
object class is not explicitly stated. To address the challenges, we propose a
novel framework, SaFiRe, which mimics the human two-phase cognitive
process--first forming a global understanding, then refining it through
detail-oriented inspection. This is naturally supported by Mamba's
scan-then-update property, which aligns with our phased design and enables
efficient multi-cycle refinement with linear complexity. We further introduce
aRefCOCO, a new benchmark designed to evaluate RIS models under ambiguous
referring expressions. Extensive experiments on both standard and proposed
datasets demonstrate the superiority of SaFiRe over state-of-the-art baselines.

</details>


### [127] [SparseUWSeg: Active Sparse Point-Label Augmentation for Underwater Semantic Segmentation](https://arxiv.org/abs/2510.10163)
*César Borja,Carlos Plou,Rubén Martinez-Cantín,Ana C. Murillo*

Main category: cs.CV

TL;DR: SparseUWSeg是一个针对水下图像语义分割的框架，通过主动采样指导标注和混合传播稀疏点标签，解决了标注成本高和信息利用不足的问题，并提供了一个高效的交互式标注工具。


<details>
  <summary>Details</summary>
Motivation: 水下图像分析中的语义分割对于生态监测至关重要，但精细粒度分析仍是难题。获取密集、专家标注的分割标签成本高昂，而稀疏点标签虽然易于获取，却面临如何选择标注点以及如何有效传播稀疏信息的挑战。

Method: 本文提出了SparseUWSeg框架，包含两个主要策略：1. 采用主动采样策略指导标注者，以最大化点标签的价值；2. 使用混合方法传播这些稀疏标签，该方法结合了SAM2和基于超像素方法的优点。此外，还设计并发布了一个集成这些算法的简单但有效的交互式标注工具。

Result: 在两个不同的水下数据集上进行的实验表明，SparseUWSeg优于现有最先进的方法，相较于D+NN实现了高达+5%的mIoU提升。

Conclusion: SparseUWSeg提供了一个简单而有效的交互式标注工具，集成了主动采样和混合标签传播算法。它使生态研究人员能够利用基础模型和计算机视觉高效生成高质量的分割掩模，以处理其数据。

Abstract: Semantic segmentation is essential to automate underwater imagery analysis
with ecology monitoring purposes. Unfortunately, fine grained underwater scene
analysis is still an open problem even for top performing segmentation models.
The high cost of obtaining dense, expert-annotated, segmentation labels hinders
the supervision of models in this domain. While sparse point-labels are easier
to obtain, they introduce challenges regarding which points to annotate and how
to propagate the sparse information. We present SparseUWSeg, a novel framework
that addresses both issues. SparseUWSeg employs an active sampling strategy to
guide annotators, maximizing the value of their point labels. Then, it
propagates these sparse labels with a hybrid approach leverages both the best
of SAM2 and superpixel-based methods. Experiments on two diverse underwater
datasets demonstrate the benefits of SparseUWSeg over state-of-the-art
approaches, achieving up to +5\% mIoU over D+NN. Our main contribution is the
design and release of a simple but effective interactive annotation tool,
integrating our algorithms. It enables ecology researchers to leverage
foundation models and computer vision to efficiently generate high-quality
segmentation masks to process their data.

</details>


### [128] [ViConEx-Med: Visual Concept Explainability via Multi-Concept Token Transformer for Medical Image Analysis](https://arxiv.org/abs/2510.10174)
*Cristiano Patrício,Luís F. Teixeira,João C. Neves*

Main category: cs.CV

TL;DR: ViConEx-Med是一种新颖的基于Transformer的视觉概念可解释性框架，通过引入多概念可学习令牌，在保持高预测准确性的同时，共同预测和定位视觉概念，解决了现有概念模型缺乏视觉解释的问题，并在医学应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 大多数现有基于概念的模型将概念视为数值属性，缺乏提供可定位预测概念的视觉解释，这限制了它们在现实世界应用（特别是在医疗等高风险场景）中的实用性。

Method: 本文提出了ViConEx-Med，一个基于Transformer的视觉概念可解释性框架。它引入了多概念可学习令牌，以共同预测和定位视觉概念。该方法利用专门的注意力层处理视觉和基于文本的概念令牌，从而生成概念级别的定位图，同时保持高预测准确性。

Result: 在合成和真实世界医疗数据集上的实验表明，ViConEx-Med在概念检测和定位精度方面均优于先前的基于概念的模型，并与黑盒模型取得了有竞争力的性能。

Conclusion: 研究结果为构建基于视觉概念的固有可解释模型提供了一个有前景的方向。

Abstract: Concept-based models aim to explain model decisions with human-understandable
concepts. However, most existing approaches treat concepts as numerical
attributes, without providing complementary visual explanations that could
localize the predicted concepts. This limits their utility in real-world
applications and particularly in high-stakes scenarios, such as medical
use-cases. This paper proposes ViConEx-Med, a novel transformer-based framework
for visual concept explainability, which introduces multi-concept learnable
tokens to jointly predict and localize visual concepts. By leveraging
specialized attention layers for processing visual and text-based concept
tokens, our method produces concept-level localization maps while maintaining
high predictive accuracy. Experiments on both synthetic and real-world medical
datasets demonstrate that ViConEx-Med outperforms prior concept-based models
and achieves competitive performance with black-box models in terms of both
concept detection and localization precision. Our results suggest a promising
direction for building inherently interpretable models grounded in visual
concepts. Code is publicly available at
https://github.com/CristianoPatricio/viconex-med.

</details>


### [129] [HccePose(BF): Predicting Front \& Back Surfaces to Construct Ultra-Dense 2D-3D Correspondences for Pose Estimation](https://arxiv.org/abs/2510.10177)
*Yulin Wang,Mengting Hu,Hongli Li,Chen Luo*

Main category: cs.CV

TL;DR: 本研究通过预测物体正面、背面及内部的3D坐标，并结合分层连续坐标编码（HCCE），显著提高了已知物体姿态估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有姿态估计算法主要关注物体正面3D坐标的编码效率和精度，忽略了利用物体背面和内部坐标的潜在益处，这限制了姿态估计的进一步提升。

Method: 该研究预测物体正面和背面3D坐标，并在两者之间进行密集采样以创建超密集的2D-3D对应关系。此外，提出了一种新的分层连续坐标编码（HCCE）方法，以更准确、高效地表示正面和背面坐标，进而利用PnP算法进行姿态估计。

Result: 实验结果表明，与BOP网站上现有的最先进方法相比，所提出的方法在七个经典的BOP核心数据集上均表现出优越的性能。

Conclusion: 通过充分利用物体的完整表面（正面和背面）以及内部3D坐标，并结合HCCE，可以显著提升基于PnP算法的物体姿态估计精度。

Abstract: In pose estimation for seen objects, a prevalent pipeline involves using
neural networks to predict dense 3D coordinates of the object surface on 2D
images, which are then used to establish dense 2D-3D correspondences. However,
current methods primarily focus on more efficient encoding techniques to
improve the precision of predicted 3D coordinates on the object's front
surface, overlooking the potential benefits of incorporating the back surface
and interior of the object. To better utilize the full surface and interior of
the object, this study predicts 3D coordinates of both the object's front and
back surfaces and densely samples 3D coordinates between them. This process
creates ultra-dense 2D-3D correspondences, effectively enhancing pose
estimation accuracy based on the Perspective-n-Point (PnP) algorithm.
Additionally, we propose Hierarchical Continuous Coordinate Encoding (HCCE) to
provide a more accurate and efficient representation of front and back surface
coordinates. Experimental results show that, compared to existing
state-of-the-art (SOTA) methods on the BOP website, the proposed approach
outperforms across seven classic BOP core datasets. Code is available at
https://github.com/WangYuLin-SEU/HCCEPose.

</details>


### [130] [TCMA: Text-Conditioned Multi-granularity Alignment for Drone Cross-Modal Text-Video Retrieval](https://arxiv.org/abs/2510.10180)
*Zixu Zhao,Yang Zhan*

Main category: cs.CV

TL;DR: 该研究构建了首个细粒度无人机视频-文本匹配数据集DVTMD，并提出了Text-Conditioned Multi-granularity Alignment (TCMA) 框架，显著提升了无人机文本-视频检索的性能。


<details>
  <summary>Details</summary>
Motivation: 无人机生成大量高分辨率视频，高效检索是城市管理、应急响应等应用的关键。然而，现有无人机领域文本-视频检索数据集存在粗糙和冗余的字幕问题，导致该领域研究不足。

Method: 1. 构建了Drone Video-Text Match Dataset (DVTMD)，包含2,864个视频和14,320个细粒度、语义多样化的字幕，涵盖人物动作、物体、背景、环境条件和视觉风格等多个方面。 2. 提出了Text-Conditioned Multi-granularity Alignment (TCMA) 框架，整合了全局视频-句子对齐、句子引导的帧聚合和单词引导的图像块对齐。 3. 设计了单词和图像块选择模块以过滤不相关内容，以及文本自适应动态温度机制以根据文本类型调整注意力锐度。

Result: 1. DVTMD和CapERA共同建立了首个完整的无人机文本-视频检索基准。 2. TCMA在DVTMD上取得了最先进的性能，文本到视频检索的R@1达到45.5%，视频到文本检索的R@1达到42.8%。

Conclusion: 本研究构建的DVTMD数据集和提出的TCMA框架在无人机文本-视频检索任务中表现出卓越的有效性，实现了最先进的性能，为该领域提供了重要的基准和方法。

Abstract: Unmanned aerial vehicles (UAVs) have become powerful platforms for real-time,
high-resolution data collection, producing massive volumes of aerial videos.
Efficient retrieval of relevant content from these videos is crucial for
applications in urban management, emergency response, security, and disaster
relief. While text-video retrieval has advanced in natural video domains, the
UAV domain remains underexplored due to limitations in existing datasets, such
as coarse and redundant captions. Thus, in this work, we construct the Drone
Video-Text Match Dataset (DVTMD), which contains 2,864 videos and 14,320
fine-grained, semantically diverse captions. The annotations capture multiple
complementary aspects, including human actions, objects, background settings,
environmental conditions, and visual style, thereby enhancing text-video
correspondence and reducing redundancy. Building on this dataset, we propose
the Text-Conditioned Multi-granularity Alignment (TCMA) framework, which
integrates global video-sentence alignment, sentence-guided frame aggregation,
and word-guided patch alignment. To further refine local alignment, we design a
Word and Patch Selection module that filters irrelevant content, as well as a
Text-Adaptive Dynamic Temperature Mechanism that adapts attention sharpness to
text type. Extensive experiments on DVTMD and CapERA establish the first
complete benchmark for drone text-video retrieval. Our TCMA achieves
state-of-the-art performance, including 45.5% R@1 in text-to-video and 42.8%
R@1 in video-to-text retrieval, demonstrating the effectiveness of our dataset
and method. The code and dataset will be released.

</details>


### [131] [Fairness Without Labels: Pseudo-Balancing for Bias Mitigation in Face Gender Classification](https://arxiv.org/abs/2510.10191)
*Haohua Dong,Ana Manzano Rodríguez,Camille Guinaudeau,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: 本文提出了一种名为“伪平衡”（pseudo-balancing）的半监督学习策略，用于缓解面部性别分类模型中存在的群体偏差，通过在伪标签选择过程中强制实现人口统计学平衡，显著提升了公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 面部性别分类模型经常反映并放大训练数据中存在的人口统计学偏差，导致在不同性别和种族子群体之间表现不均。

Method: 引入了“伪平衡”策略，这是一种简单有效的半监督学习方法。该方法在伪标签选择过程中强制执行人口统计学平衡，仅使用来自种族平衡数据集的未标注图像，无需真实的地面真值标注。

Result: 在两种条件下评估了伪平衡方法：(1) 使用FairFace数据集的未标注图像微调有偏见的性别分类器；(2) 使用故意不平衡的训练数据进行压力测试。在主要由东亚人口组成的All-Age-Faces (AAF) 基准上评估。结果显示，伪平衡方法持续提高了公平性，同时保持或提升了准确性。整体准确率达到79.81%（比基线提高6.53%），性别准确率差距减少了44.17%。在基线差异超过49%的东亚子群体中，差距缩小到仅5.01%。

Conclusion: 即使在没有标签监督的情况下，访问人口统计学平衡或适度倾斜的未标注数据集，也可以作为消除现有计算机视觉模型偏差的强大资源。

Abstract: Face gender classification models often reflect and amplify demographic
biases present in their training data, leading to uneven performance across
gender and racial subgroups. We introduce pseudo-balancing, a simple and
effective strategy for mitigating such biases in semi-supervised learning. Our
method enforces demographic balance during pseudo-label selection, using only
unlabeled images from a race-balanced dataset without requiring access to
ground-truth annotations.
  We evaluate pseudo-balancing under two conditions: (1) fine-tuning a biased
gender classifier using unlabeled images from the FairFace dataset, and (2)
stress-testing the method with intentionally imbalanced training data to
simulate controlled bias scenarios. In both cases, models are evaluated on the
All-Age-Faces (AAF) benchmark, which contains a predominantly East Asian
population. Our results show that pseudo-balancing consistently improves
fairness while preserving or enhancing accuracy. The method achieves 79.81%
overall accuracy - a 6.53% improvement over the baseline - and reduces the
gender accuracy gap by 44.17%. In the East Asian subgroup, where baseline
disparities exceeded 49%, the gap is narrowed to just 5.01%. These findings
suggest that even in the absence of label supervision, access to a
demographically balanced or moderately skewed unlabeled dataset can serve as a
powerful resource for debiasing existing computer vision models.

</details>


### [132] [B2N3D: Progressive Learning from Binary to N-ary Relationships for 3D Object Grounding](https://arxiv.org/abs/2510.10194)
*Feng Xiao,Hongbin Xu,Hai Ci,Wenxiong Kang*

Main category: cs.CV

TL;DR: 该论文提出了一种渐进式关系学习框架，通过将关系学习从二元扩展到n元，并引入分组监督损失，以提高使用自然语言进行3D对象定位的准确性，解决了现有方法仅关注成对关系的问题。


<details>
  <summary>Details</summary>
Motivation: 机器人场景理解中，使用自然语言定位3D对象至关重要。描述通常涉及多个空间关系来区分相似对象，但现有方法只建模成对对象的关系，忽略了多模态关系理解中n元组合的全局感知重要性，导致3D语言对齐困难。

Method: 提出了一种渐进式关系学习框架。将关系学习从二元扩展到n元，以识别全局匹配参照描述的视觉关系。针对训练数据中缺乏参照对象特定标注的问题，设计了分组监督损失以促进n元关系学习。在用n元关系构建的场景图中，使用带有混合注意力机制的多模态网络在n元组合中进一步定位目标。

Result: 在ReferIt3D和ScanRefer基准测试上的实验和消融研究表明，该方法优于现有最先进的方法，并证明了n元关系感知在3D定位中的优势。

Conclusion: n元关系感知在3D对象定位中具有显著优势，提出的渐进式关系学习框架和分组监督损失有效解决了复杂自然语言描述下的3D对象接地问题。

Abstract: Localizing 3D objects using natural language is essential for robotic scene
understanding. The descriptions often involve multiple spatial relationships to
distinguish similar objects, making 3D-language alignment difficult. Current
methods only model relationships for pairwise objects, ignoring the global
perceptual significance of n-ary combinations in multi-modal relational
understanding. To address this, we propose a novel progressive relational
learning framework for 3D object grounding. We extend relational learning from
binary to n-ary to identify visual relations that match the referential
description globally. Given the absence of specific annotations for referred
objects in the training data, we design a grouped supervision loss to
facilitate n-ary relational learning. In the scene graph created with n-ary
relationships, we use a multi-modal network with hybrid attention mechanisms to
further localize the target within the n-ary combinations. Experiments and
ablation studies on the ReferIt3D and ScanRefer benchmarks demonstrate that our
method outperforms the state-of-the-art, and proves the advantages of the n-ary
relational perception in 3D localization.

</details>


### [133] [From Generic to Specialized: A Subspecialty Diagnostic System Powered by Self-Supervised Learning for Cervical Histopathology](https://arxiv.org/abs/2510.10196)
*Yizhi Wang,Li Chen,Qiang Huang,Tian Guan,Xi Deng,Zhiyuan Shen,Jiawen Li,Xinrui Chen,Bin Hu,Xitong Ling,Taojie Zhu,Zirui Huang,Deshui Yu,Yan Liu,Jiurun Chen,Lianghui Zhu,Qiming He,Yiqing Liu,Diwei Shi,Hanzhong Liu,Junbo Hu,Hongyi Gao,Zhen Song,Xilong Zhao,Chao He,Ming Zhao,Yonghong He*

Main category: cs.CV

TL;DR: 该研究引入了子宫颈亚专科病理（CerS-Path）诊断系统，通过两阶段预训练（自监督学习和多模态增强）构建，支持八种诊断功能，并在前瞻性测试中实现了99.38%的筛查敏感性和出色的泛化能力，有望用于子宫颈癌筛查和亚专科诊断。


<details>
  <summary>Details</summary>
Motivation: 子宫颈癌仍是主要恶性肿瘤，需要复杂组织病理学评估和全面的支持工具。现有深度学习模型缺乏准确性和泛化性，而通用基础模型在捕获亚专科特定特征和任务适应性方面存在局限性。

Method: 开发了子宫颈亚专科病理（CerS-Path）诊断系统，采用两个协同预训练阶段：首先，通过对来自14万张玻片的约1.9亿个组织斑块进行自监督学习，构建子宫颈特异性特征提取器；其次，利用250万个图像-文本对进行多模态增强，随后与多个下游诊断功能集成。

Result: CerS-Path系统支持八种诊断功能（包括罕见癌症分类和多模态问答），在范围和临床适用性上超越了先前基础模型。在对五个中心的3,173例病例进行前瞻性测试中，保持了99.38%的筛查敏感性，并展现出卓越的泛化能力。

Conclusion: CerS-Path系统在子宫颈病理学方面取得了重大进展，具有亚专科诊断转化和子宫颈癌筛查的巨大潜力。

Abstract: Cervical cancer remains a major malignancy, necessitating extensive and
complex histopathological assessments and comprehensive support tools. Although
deep learning shows promise, these models still lack accuracy and
generalizability. General foundation models offer a broader reach but remain
limited in capturing subspecialty-specific features and task adaptability. We
introduce the Cervical Subspecialty Pathology (CerS-Path) diagnostic system,
developed through two synergistic pretraining stages: self-supervised learning
on approximately 190 million tissue patches from 140,000 slides to build a
cervical-specific feature extractor, and multimodal enhancement with 2.5
million image-text pairs, followed by integration with multiple downstream
diagnostic functions. Supporting eight diagnostic functions, including rare
cancer classification and multimodal Q&A, CerS-Path surpasses prior foundation
models in scope and clinical applicability. Comprehensive evaluations
demonstrate a significant advance in cervical pathology, with prospective
testing on 3,173 cases across five centers maintaining 99.38% screening
sensitivity and excellent generalizability, highlighting its potential for
subspecialty diagnostic translation and cervical cancer screening.

</details>


### [134] [A Style-Based Metric for Quantifying the Synthetic-to-Real Gap in Autonomous Driving Image Datasets](https://arxiv.org/abs/2510.10203)
*Dingyi Yao,Xinyao Han,Ruibo Ming,Zhihang Song,Lihui Peng,Jianming Hu,Danya Yao,Yi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种量化自动驾驶感知系统合成-真实数据域差距的系统框架和新评估指标SEDD，以提升合成数据集的效用和模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知系统需要大量基于环境的测试，但真实世界执行不切实际。合成数据集是替代方案，但其与真实世界数据之间的域差距是AI模型泛化能力的瓶颈。量化这一差距对于评估数据集效用和指导训练流程设计至关重要。

Method: 本文建立了一个量化自动驾驶系统合成-真实域差距的系统框架，并提出了一种名为风格嵌入分布差异（SEDD）的新颖评估指标。该框架结合了基于Gram矩阵的风格提取和优化了类内紧凑性及类间分离的度量学习，以提取风格嵌入。此外，还使用公开数据集建立了一个基准。

Result: 在各种数据集和仿真-真实方法上进行的实验表明，本文提出的方法能够有效量化合成-真实域差距。

Conclusion: 这项工作提供了一个标准化的质量控制工具，可实现对合成数据集的系统诊断和有针对性的改进，从而推动数据驱动的自动驾驶系统的未来发展。

Abstract: Ensuring the reliability of autonomous driving perception systems requires
extensive environment-based testing, yet real-world execution is often
impractical. Synthetic datasets have therefore emerged as a promising
alternative, offering advantages such as cost-effectiveness, bias free
labeling, and controllable scenarios. However, the domain gap between synthetic
and real-world datasets remains a critical bottleneck for the generalization of
AI-based autonomous driving models. Quantifying this synthetic-to-real gap is
thus essential for evaluating dataset utility and guiding the design of more
effective training pipelines. In this paper, we establish a systematic
framework for quantifying the synthetic-to-real gap in autonomous driving
systems, and propose Style Embedding Distribution Discrepancy (SEDD) as a novel
evaluation metric. Our framework combines Gram matrix-based style extraction
with metric learning optimized for intra-class compactness and inter-class
separation to extract style embeddings. Furthermore, we establish a benchmark
using publicly available datasets. Experiments are conducted on a variety of
datasets and sim-to-real methods, and the results show that our method is
capable of quantifying the synthetic-to-real gap. This work provides a
standardized quality control tool that enables systematic diagnosis and
targeted enhancement of synthetic datasets, advancing future development of
data-driven autonomous driving systems.

</details>


### [135] [MRI Brain Tumor Detection with Computer Vision](https://arxiv.org/abs/2510.10250)
*Jack Krolik,Jake Lynn,John Henry Rudden,Dmytro Vremenko*

Main category: cs.CV

TL;DR: 本研究探索了深度学习技术在MRI脑肿瘤自动检测和分割中的应用，通过多种模型提高了诊断的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机是提高脑肿瘤诊断的准确性和效率，从而改善临床结果。

Method: 研究采用了多种机器学习模型：用于分类的基础逻辑回归、卷积神经网络（CNN）和残差网络（ResNet）；用于语义分割的U-Net；以及用于基于锚点目标检测的EfficientDet。

Result: 研究结果表明，在脑肿瘤诊断的准确性和效率方面取得了可喜的改进。

Conclusion: 深度学习在医学影像领域具有巨大潜力，对改善临床结果具有重要意义。

Abstract: This study explores the application of deep learning techniques in the
automated detection and segmentation of brain tumors from MRI scans. We employ
several machine learning models, including basic logistic regression,
Convolutional Neural Networks (CNNs), and Residual Networks (ResNet) to
classify brain tumors effectively. Additionally, we investigate the use of
U-Net for semantic segmentation and EfficientDet for anchor-based object
detection to enhance the localization and identification of tumors. Our results
demonstrate promising improvements in the accuracy and efficiency of brain
tumor diagnostics, underscoring the potential of deep learning in medical
imaging and its significance in improving clinical outcomes.

</details>


### [136] [Semantic Visual Anomaly Detection and Reasoning in AI-Generated Images](https://arxiv.org/abs/2510.10231)
*Chuangchuang Tan,Xiang Ming,Jinglu Wang,Renshuai Tao,Bin Li,Yunchao Wei,Yao Zhao,Yan Lu*

Main category: cs.CV

TL;DR: 该研究针对AI生成图像中的语义异常，提出了一个大规模基准数据集AnomReason和多智能体标注流水线AnomAgent，以检测和推理这些异常，并展示了其在可解释深度伪造检测和图像生成器评估中的实用性。


<details>
  <summary>Details</summary>
Motivation: AI生成内容（AIGC）虽然能生成逼真的图像，但常包含细微的语义异常（如不真实的物体配置、违反物理定律、常识不一致），这些异常损害了生成场景的合理性。检测这些语义层面的异常对于评估AIGC媒体的可信度、可解释深度伪造检测以及语义真实性评估至关重要。

Method: 本文将AIGC图像的“语义异常检测与推理”形式化，并引入了“AnomReason”——一个包含结构化标注（四元组：名称、现象、推理、严重性）的大规模基准数据集。标注通过一个模块化多智能体流水线“AnomAgent”生成，该流水线结合了轻量级的人工验证，以在大规模生产高质量标注。此外，他们提出了语义匹配指标（SemAP和SemF1）来评估模型性能。

Result: AnomAgent在构建时处理了约4.17亿GPT-4o token，证明了其生成大规模结构化标注的能力。在AnomReason上微调的模型在所提出的语义匹配指标（SemAP和SemF1）上，相对于强大的视觉-语言基线模型取得了显著提升。研究还展示了其在可解释深度伪造检测和图像生成器语义合理性评估方面的实际应用价值。

Conclusion: AnomReason和AnomAgent为衡量和提升AI生成图像的语义合理性奠定了基础。它们将支持关于语义真实性和可解释AIGC取证的再现性研究，未来将发布代码、指标、数据和任务对齐模型。

Abstract: The rapid advancement of
  AI-generated content (AIGC) has enabled the synthesis of visually convincing
images; however, many such outputs exhibit subtle \textbf{semantic anomalies},
including unrealistic object configurations, violations of physical laws, or
commonsense inconsistencies, which compromise the overall plausibility of the
generated scenes. Detecting these semantic-level anomalies
  is essential for assessing the trustworthiness of AIGC media, especially in
AIGC image analysis, explainable deepfake detection and semantic authenticity
assessment. In this paper,
  we formalize \textbf{semantic anomaly detection and reasoning} for AIGC
images and
  introduce \textbf{AnomReason}, a large-scale benchmark with structured
annotations as quadruples \emph{(Name, Phenomenon, Reasoning, Severity)}.
Annotations are produced by
  a modular multi-agent pipeline (\textbf{AnomAgent}) with lightweight
human-in-the-loop verification, enabling scale while preserving quality.
  At construction time, AnomAgent processed approximately 4.17\,B GPT-4o
tokens, providing scale evidence for the resulting structured annotations. We
further
  show that models fine-tuned on AnomReason achieve consistent gains over
strong vision-language baselines under our proposed semantic matching metric
(\textit{SemAP} and \textit{SemF1}).
  Applications to {explainable deepfake detection} and {semantic reasonableness
assessment of image generators} demonstrate practical utility. In summary,
AnomReason and AnomAgent
  serve as a foundation for measuring and improving the semantic plausibility
of AI-generated images. We will release code, metrics, data, and task-aligned
models to support reproducible research on semantic authenticity and
interpretable AIGC forensics.

</details>


### [137] [Are Video Models Emerging as Zero-Shot Learners and Reasoners in Medical Imaging?](https://arxiv.org/abs/2510.10254)
*Yuxiang Lai,Jike Zhong,Ming Li,Yuheng Li,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 研究表明，大型视觉模型（LVM）即使未经医学数据训练，也能在医学影像任务中展现出强大的零样本泛化能力，尤其在运动预测方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 受大型生成模型在自回归框架下通过适当扩展实现跨领域零样本泛化能力的启发，研究者希望探索自回归视频建模原则是否能直接应用于医学影像任务，即使模型从未在医学数据上训练过。

Method: 本研究在一个零样本设置下评估了一个大型视觉模型（LVM），覆盖了器官分割、去噪、超分辨率和运动预测四项代表性任务。特别是在运动预测中，模型直接从4D CT扫描的先前阶段预测未来的3D CT阶段。评估使用了来自122名患者的4D CT数据，共计超过1,820个3D CT体积。

Result: LVM即使未经特定领域微调，也能在CT扫描中勾勒解剖结构，并在分割、去噪和超分辨率任务上取得有竞争力的表现。最值得注意的是，在放射治疗运动预测中，模型能直接从先前的4D CT阶段预测未来的3D CT阶段，生成解剖学上一致的预测，捕捉患者特异性呼吸动态，并具有逼真的时间一致性。模型在所有任务上都表现出色，并在运动预测方面超越了专门的DVF-based和生成式基线，达到了最先进的空间精度。

Conclusion: 这些发现揭示了医学视频建模中零样本能力的出现，并强调了通用视频模型作为统一学习器和推理器的潜力，为未来基于视频模型的医学基础模型奠定了基础。

Abstract: Recent advances in large generative models have shown that simple
autoregressive formulations, when scaled appropriately, can exhibit strong
zero-shot generalization across domains. Motivated by this trend, we
investigate whether autoregressive video modeling principles can be directly
applied to medical imaging tasks, despite the model never being trained on
medical data. Specifically, we evaluate a large vision model (LVM) in a
zero-shot setting across four representative tasks: organ segmentation,
denoising, super-resolution, and motion prediction. Remarkably, even without
domain-specific fine-tuning, the LVM can delineate anatomical structures in CT
scans and achieve competitive performance on segmentation, denoising, and
super-resolution. Most notably, in radiotherapy motion prediction, the model
forecasts future 3D CT phases directly from prior phases of a 4D CT scan,
producing anatomically consistent predictions that capture patient-specific
respiratory dynamics with realistic temporal coherence. We evaluate the LVM on
4D CT data from 122 patients, totaling over 1,820 3D CT volumes. Despite no
prior exposure to medical data, the model achieves strong performance across
all tasks and surpasses specialized DVF-based and generative baselines in
motion prediction, achieving state-of-the-art spatial accuracy. These findings
reveal the emergence of zero-shot capabilities in medical video modeling and
highlight the potential of general-purpose video models to serve as unified
learners and reasoners laying the groundwork for future medical foundation
models built on video models.

</details>


### [138] [Opacity-Gradient Driven Density Control for Compact and Efficient Few-Shot 3D Gaussian Splatting](https://arxiv.org/abs/2510.10257)
*Abdelrhman Elrawy,Emad A. Mohammed*

Main category: cs.CV

TL;DR: 本文提出一个3D Gaussian Splatting (3DGS) 优化框架，通过使用不透明度梯度作为致密化触发器并结合保守修剪策略，显著提高了在少样本场景下的模型紧凑性，并在质量-效率权衡上达到了新的SOTA。


<details>
  <summary>Details</summary>
Motivation: 3DGS在少样本场景中易出现过拟合和重建膨胀问题，其标准自适应密度控制（ADC）效果不佳。尽管FSGS等现有方法提高了质量，但通常以显著增加基元数量为代价，导致效率低下。因此，需要一个能在少样本视图合成中兼顾质量和效率的3DGS优化方法。

Method: 该框架重新设计了3DGS的核心优化过程，以优先考虑效率。具体方法包括：1) 使用不透明度梯度作为新的致密化触发器，替代标准的基于位置梯度的启发式方法，作为渲染误差的轻量级代理；2) 结合更保守的修剪策略，以防止破坏性的优化循环；3) 引入标准的深度相关损失（depth-correlation loss）以提供几何指导。

Result: 在3视角的LLFF数据集上，该模型比FSGS紧凑40%以上（32k vs 57k基元）。在Mip-NeRF 360数据集上，基元数量减少了约70%。这些显著的紧凑性提升仅伴随重建指标上的适度权衡，从而在少样本视图合成的质量-效率Pareto前沿上建立了新的SOTA。

Conclusion: 通过引入新的致密化触发器和保守修剪策略，本文提出的框架显著提升了3DGS在少样本场景下的效率和紧凑性，在保持合理重建质量的同时，大幅减少了模型基元数量，为少样本视图合成的质量与效率权衡提供了新的最佳解决方案。

Abstract: 3D Gaussian Splatting (3DGS) struggles in few-shot scenarios, where its
standard adaptive density control (ADC) can lead to overfitting and bloated
reconstructions. While state-of-the-art methods like FSGS improve quality, they
often do so by significantly increasing the primitive count. This paper
presents a framework that revises the core 3DGS optimization to prioritize
efficiency. We replace the standard positional gradient heuristic with a novel
densification trigger that uses the opacity gradient as a lightweight proxy for
rendering error. We find this aggressive densification is only effective when
paired with a more conservative pruning schedule, which prevents destructive
optimization cycles. Combined with a standard depth-correlation loss for
geometric guidance, our framework demonstrates a fundamental improvement in
efficiency. On the 3-view LLFF dataset, our model is over 40% more compact (32k
vs. 57k primitives) than FSGS, and on the Mip-NeRF 360 dataset, it achieves a
reduction of approximately 70%. This dramatic gain in compactness is achieved
with a modest trade-off in reconstruction metrics, establishing a new
state-of-the-art on the quality-vs-efficiency Pareto frontier for few-shot view
synthesis.

</details>


### [139] [Bridging Perspectives: Foundation Model Guided BEV Maps for 3D Object Detection and Tracking](https://arxiv.org/abs/2510.10287)
*Markus Käppeler,Özgün Çiçek,Daniele Cattaneo,Claudius Gläser,Yakov Miron,Abhinav Valada*

Main category: cs.CV

TL;DR: DualViewDistill是一个混合检测和跟踪框架，它结合了透视图(PV)和鸟瞰图(BEV)特征，并通过将DINOv2基础模型的描述性特征蒸馏到BEV表示中来增强BEV地图，从而在自动驾驶的3D目标检测和跟踪方面达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的3D目标检测和跟踪方法要么单独依赖透视图(PV)特征，要么单独依赖鸟瞰图(BEV)特征。这限制了它们同时利用细粒度目标细节（PV优势）和空间结构化场景表示（BEV优势）的能力，未能充分发挥两者的互补优势。

Method: 本文提出了DualViewDistill，一个混合检测和跟踪框架。它整合了PV和BEV相机图像特征。该方法引入了由基础模型（如DINOv2）引导的BEV地图，通过一种新颖的蒸馏过程，将DINOv2的描述性特征蒸馏到BEV表示中。模型通过可变形聚合来利用这种结合了PV特征和富含DINOv2语义及几何特征的BEV地图的混合表示，以增强3D目标检测和跟踪。

Result: 在nuScenes和Argoverse 2基准测试上进行了广泛的实验，结果表明DualViewDistill达到了最先进的性能。

Conclusion: 研究结果展示了基础模型引导的BEV地图在实现更可靠的自动驾驶感知方面的潜力。DualViewDistill通过有效结合PV和BEV特征，并利用基础模型的强大能力，显著提升了3D目标检测和跟踪的性能。

Abstract: Camera-based 3D object detection and tracking are essential for perception in
autonomous driving. Current state-of-the-art approaches often rely exclusively
on either perspective-view (PV) or bird's-eye-view (BEV) features, limiting
their ability to leverage both fine-grained object details and spatially
structured scene representations. In this work, we propose DualViewDistill, a
hybrid detection and tracking framework that incorporates both PV and BEV
camera image features to leverage their complementary strengths. Our approach
introduces BEV maps guided by foundation models, leveraging descriptive DINOv2
features that are distilled into BEV representations through a novel
distillation process. By integrating PV features with BEV maps enriched with
semantic and geometric features from DINOv2, our model leverages this hybrid
representation via deformable aggregation to enhance 3D object detection and
tracking. Extensive experiments on the nuScenes and Argoverse 2 benchmarks
demonstrate that DualViewDistill achieves state-of-the-art performance. The
results showcase the potential of foundation model BEV maps to enable more
reliable perception for autonomous driving. We make the code and pre-trained
models available at https://dualviewdistill.cs.uni-freiburg.de .

</details>


### [140] [VividAnimator: An End-to-End Audio and Pose-driven Half-Body Human Animation Framework](https://arxiv.org/abs/2510.10269)
*Donglin Huang,Yongyuan Li,Tianhang Liu,Junming Huang,Xiaoda Yang,Chi Wang,Weiwei Xu*

Main category: cs.CV

TL;DR: VividAnimator 是一个端到端框架，通过音频和稀疏手部姿态条件生成高质量的半身人体动画，解决了现有方法中头部运动僵硬和手部模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有音频和姿态驱动的人体动画方法常面临头部运动僵硬和手部模糊的问题。这主要是由于音频与头部运动的相关性较弱以及手部结构复杂性所致。

Method: 该框架引入了三项关键创新：1. 预训练手部清晰度码本 (HCC)，编码丰富的高保真手部纹理先验，显著减轻手部退化。2. 设计双流音频感知模块 (DSAA)，独立建模唇同步和自然头部姿态动态，并实现两者间的交互。3. 引入姿态校准技巧 (PCT)，通过放宽刚性约束来优化和对齐姿态条件，确保手势过渡的平滑自然。

Result: VividAnimator 实现了最先进的性能，在手部细节、手势真实感和身份一致性方面表现出卓越的效果。这些结果已通过定量指标和定性评估得到验证。

Conclusion: VividAnimator 通过其独特的设计和创新模块，有效解决了现有音频和姿态驱动人体动画方法的局限性，显著提升了生成动画的质量和真实感。

Abstract: Existing for audio- and pose-driven human animation methods often struggle
with stiff head movements and blurry hands, primarily due to the weak
correlation between audio and head movements and the structural complexity of
hands. To address these issues, we propose VividAnimator, an end-to-end
framework for generating high-quality, half-body human animations driven by
audio and sparse hand pose conditions. Our framework introduces three key
innovations. First, to overcome the instability and high cost of online
codebook training, we pre-train a Hand Clarity Codebook (HCC) that encodes
rich, high-fidelity hand texture priors, significantly mitigating hand
degradation. Second, we design a Dual-Stream Audio-Aware Module (DSAA) to model
lip synchronization and natural head pose dynamics separately while enabling
interaction. Third, we introduce a Pose Calibration Trick (PCT) that refines
and aligns pose conditions by relaxing rigid constraints, ensuring smooth and
natural gesture transitions. Extensive experiments demonstrate that Vivid
Animator achieves state-of-the-art performance, producing videos with superior
hand detail, gesture realism, and identity consistency, validated by both
quantitative metrics and qualitative evaluations.

</details>


### [141] [SAM2LoRA: Composite Loss-Guided, Parameter-Efficient Finetuning of SAM2 for Retinal Fundus Segmentation](https://arxiv.org/abs/2510.10288)
*Sayan Mandal,Divyadarshini Karthikeyan,Manas Paldhe*

Main category: cs.CV

TL;DR: SAM2LoRA 是一种参数高效的微调策略，用于将 SAM2 模型应用于眼底图像分割任务，在交叉数据集训练条件下，以极低的训练开销实现了血管和视盘分割的最新性能。


<details>
  <summary>Details</summary>
Motivation: SAM2 模型虽然在低资源环境下能实现快速推理，但其微调过程具有挑战性。研究旨在开发一种参数高效的方法，将 SAM2 适应于眼底图像分割，以解决这一挑战。

Method: 提出 SAM2LoRA 策略，通过将低秩适配器集成到 SAM2 的图像编码器和掩码解码器中，使可训练参数少于原始模型的5%。同时，研究发现结合 segmentationBCE、SoftDice 和 FocalTversky 的复合损失函数对于网络优化至关重要。

Result: 在11个眼底分割数据集上进行评估，SAM2LoRA 在交叉数据集训练条件下，血管分割的 Dice 分数高达0.86，AUC 值高达0.98；视盘分割的 Dice 分数高达0.93，AUC 值高达0.99。实现了最先进的性能，并显著降低了训练开销。

Conclusion: SAM2LoRA 成功地将 SAM2 模型高效地应用于眼底图像分割任务，在保持高参数效率的同时，在血管和视盘分割方面取得了最先进的性能。

Abstract: We propose SAM2LoRA, a parameter-efficient fine-tuning strategy that adapts
the Segment Anything Model 2 (SAM2) for fundus image segmentation. SAM2 employs
a masked autoencoder-pretrained Hierarchical Vision Transformer for multi-scale
feature decoding, enabling rapid inference in low-resource settings; however,
fine-tuning remains challenging. To address this, SAM2LoRA integrates a
low-rank adapter into both the image encoder and mask decoder, requiring fewer
than 5\% of the original trainable parameters. Our analysis indicates that for
cross-dataset fundus segmentation tasks, a composite loss function combining
segmentationBCE, SoftDice, and FocalTversky losses is essential for optimal
network tuning. Evaluated on 11 challenging fundus segmentation datasets,
SAM2LoRA demonstrates high performance in both blood vessel and optic disc
segmentation under cross-dataset training conditions. It achieves Dice scores
of up to 0.86 and 0.93 for blood vessel and optic disc segmentation,
respectively, and AUC values of up to 0.98 and 0.99, achieving state-of-the-art
performance while substantially reducing training overhead.

</details>


### [142] [Ordinal Scale Traffic Congestion Classification with Multi-Modal Vision-Language and Motion Analysis](https://arxiv.org/abs/2510.10342)
*Yu-Hsuan Lin*

Main category: cs.CV

TL;DR: 本文提出一个多模态框架，结合CLIP、YOLO-World和MOG2进行交通拥堵分类，实现了语义对齐和时间一致的预测，并具有良好的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确的交通拥堵分类对于智能交通系统和实时城市交通管理至关重要。

Method: 该框架结合了开放词汇视觉-语言推理（CLIP）、目标检测（YOLO-World）和基于MOG2背景减法的运动分析。它以1到5的序数等级预测拥堵水平，并结合基于运动的置信度加权和生成带注释的视觉输出以增强可解释性。

Result: 实验结果显示，该模型达到了76.7%的准确率、0.752的F1分数和0.684的二次加权Kappa（QWK），显著优于单模态基线。这表明该框架在保留序数结构和利用视觉-语言及运动模态方面的有效性。

Conclusion: 该多模态框架在交通拥堵分类方面表现出有效性。未来的改进包括整合车辆尺寸和更精细的密度指标。

Abstract: Accurate traffic congestion classification is essential for intelligent
transportation systems and real-time urban traffic management. This paper
presents a multimodal framework combining open-vocabulary visual-language
reasoning (CLIP), object detection (YOLO-World), and motion analysis via
MOG2-based background subtraction. The system predicts congestion levels on an
ordinal scale from 1 (free flow) to 5 (severe congestion), enabling
semantically aligned and temporally consistent classification. To enhance
interpretability, we incorporate motion-based confidence weighting and generate
annotated visual outputs. Experimental results show the model achieves 76.7
percent accuracy, an F1 score of 0.752, and a Quadratic Weighted Kappa (QWK) of
0.684, significantly outperforming unimodal baselines. These results
demonstrate the framework's effectiveness in preserving ordinal structure and
leveraging visual-language and motion modalities. Future enhancements include
incorporating vehicle sizing and refined density metrics.

</details>


### [143] [From Programs to Poses: Factored Real-World Scene Generation via Learned Program Libraries](https://arxiv.org/abs/2510.10292)
*Joy Hsu,Emily Jin,Jiajun Wu,Niloy J. Mitra*

Main category: cs.CV

TL;DR: FactoredScenes是一个用于合成真实3D场景的框架，它通过分解场景为房间程序和物体姿态的层次结构，学习可复用布局模式并利用大型语言模型生成高级程序，然后预测物体姿态来生成逼真的房间。


<details>
  <summary>Details</summary>
Motivation: 现实世界场景（如ScanNet）难以捕获且数据高度有限；生成具有多样化物体姿态的逼真场景仍然是一个开放且具有挑战性的任务。

Method: 该研究提出了FactoredScenes框架，它采用分解表示法将场景分解为房间程序（room programs）和物体姿态（object poses）的层次结构。为编码结构，FactoredScenes学习一个捕获可复用布局模式的函数库，并使用大型语言模型生成高级程序，由学习到的库进行正则化。为表示场景变化，FactoredScenes学习一个程序条件模型来分层预测物体姿态，并检索和放置3D物体。

Result: FactoredScenes能够生成逼真的、难以与真实ScanNet场景区分的现实世界房间。

Conclusion: FactoredScenes通过利用房间的底层结构和学习物体姿态的变化，成功解决了生成逼真3D场景的挑战，并能生成高质量的真实世界房间。

Abstract: Real-world scenes, such as those in ScanNet, are difficult to capture, with
highly limited data available. Generating realistic scenes with varied object
poses remains an open and challenging task. In this work, we propose
FactoredScenes, a framework that synthesizes realistic 3D scenes by leveraging
the underlying structure of rooms while learning the variation of object poses
from lived-in scenes. We introduce a factored representation that decomposes
scenes into hierarchically organized concepts of room programs and object
poses. To encode structure, FactoredScenes learns a library of functions
capturing reusable layout patterns from which scenes are drawn, then uses large
language models to generate high-level programs, regularized by the learned
library. To represent scene variations, FactoredScenes learns a
program-conditioned model to hierarchically predict object poses, and retrieves
and places 3D objects in a scene. We show that FactoredScenes generates
realistic, real-world rooms that are difficult to distinguish from real ScanNet
scenes.

</details>


### [144] [PointMAC: Meta-Learned Adaptation for Robust Test-Time Point Cloud Completion](https://arxiv.org/abs/2510.10365)
*Linlian Jiang,Rui Ma,Li Gu,Ziqiang Wang,Xinxin Zuo,Yang Wang*

Main category: cs.CV

TL;DR: PointMAC是一个元学习框架，通过自监督辅助目标和MAML策略，在点云补全任务中实现鲁棒的测试时自适应，逐样本优化以生成高质量补全结果。


<details>
  <summary>Details</summary>
Motivation: 现有模型在点云补全中进行静态推理，过度依赖训练中学到的归纳偏置，限制了它们在测试时适应新颖结构模式和传感器畸变的能力。

Method: 本文提出了PointMAC，一个元学习框架，用于点云补全中的鲁棒测试时自适应。它利用两个自监督辅助目标模拟结构和传感器级别的不完整性。基于MAML的元辅助学习策略确保了辅助目标驱动的自适应与主要补全任务保持一致。推理时，通过优化辅助损失动态调整共享编码器（解码器固定）。此外，引入了自适应λ校准机制，以平衡主要和辅助目标之间的梯度。

Result: 在合成、模拟和真实世界数据集上的大量实验表明，PointMAC通过单独优化每个样本，实现了最先进的补全效果，生成了高质量的补全点云。这是首次将元辅助测试时自适应应用于点云补全的工作。

Conclusion: PointMAC通过元辅助测试时自适应，有效解决了现有模型在点云补全中适应性和鲁棒性的局限性，显著提高了补全质量，并为该领域开辟了新方向。

Abstract: Point cloud completion is essential for robust 3D perception in
safety-critical applications such as robotics and augmented reality. However,
existing models perform static inference and rely heavily on inductive biases
learned during training, limiting their ability to adapt to novel structural
patterns and sensor-induced distortions at test time. To address this
limitation, we propose PointMAC, a meta-learned framework for robust test-time
adaptation in point cloud completion. It enables sample-specific refinement
without requiring additional supervision. Our method optimizes the completion
model under two self-supervised auxiliary objectives that simulate structural
and sensor-level incompleteness. A meta-auxiliary learning strategy based on
Model-Agnostic Meta-Learning (MAML) ensures that adaptation driven by auxiliary
objectives is consistently aligned with the primary completion task. During
inference, we adapt the shared encoder on-the-fly by optimizing auxiliary
losses, with the decoder kept fixed. To further stabilize adaptation, we
introduce Adaptive $\lambda$-Calibration, a meta-learned mechanism for
balancing gradients between primary and auxiliary objectives. Extensive
experiments on synthetic, simulated, and real-world datasets demonstrate that
PointMAC achieves state-of-the-art results by refining each sample individually
to produce high-quality completions. To the best of our knowledge, this is the
first work to apply meta-auxiliary test-time adaptation to point cloud
completion.

</details>


### [145] [Ortho-Fuse: Orthomosaic Generation for Sparse High-Resolution Crop Health Maps Through Intermediate Optical Flow Estimation](https://arxiv.org/abs/2510.10360)
*Rugved Katole,Christopher Stewart*

Main category: cs.CV

TL;DR: 本文提出Ortho-Fuse，一个基于光流的框架，通过合成过渡图像来减少生成高质量正射镶嵌图所需的航空图像重叠率，从而克服了AI驱动作物健康监测系统在稀疏图像数据集下重建质量差的问题。


<details>
  <summary>Details</summary>
Motivation: AI驱动的作物健康监测系统因传统摄影测量法需要高图像重叠率（70-80%）而导致在资源受限条件下无法获得高质量的正射镶嵌图，这限制了农民的广泛采用。

Method: Ortho-Fuse框架利用中间光流估计在连续航空图像帧之间合成过渡图像，人工增加特征对应，以改进几何重建，从而减少对图像重叠率的要求。

Result: 实验验证表明，该方法将最小重叠率要求降低了20%。此外，文章还分析了精准农业中的采用障碍。

Conclusion: Ortho-Fuse通过减少图像重叠要求，提高了AI驱动监测系统在资源受限条件下的可靠性，有助于促进这些技术在精准农业中的更广泛应用和整合。

Abstract: AI-driven crop health mapping systems offer substantial advantages over
conventional monitoring approaches through accelerated data acquisition and
cost reduction. However, widespread farmer adoption remains constrained by
technical limitations in orthomosaic generation from sparse aerial imagery
datasets. Traditional photogrammetric reconstruction requires 70-80\%
inter-image overlap to establish sufficient feature correspondences for
accurate geometric registration. AI-driven systems operating under
resource-constrained conditions cannot consistently achieve these overlap
thresholds, resulting in degraded reconstruction quality that undermines user
confidence in autonomous monitoring technologies. In this paper, we present
Ortho-Fuse, an optical flow-based framework that enables the generation of a
reliable orthomosaic with reduced overlap requirements. Our approach employs
intermediate flow estimation to synthesize transitional imagery between
consecutive aerial frames, artificially augmenting feature correspondences for
improved geometric reconstruction. Experimental validation demonstrates a 20\%
reduction in minimum overlap requirements. We further analyze adoption barriers
in precision agriculture to identify pathways for enhanced integration of
AI-driven monitoring systems.

</details>


### [146] [Vision4PPG: Emergent PPG Analysis Capability of Vision Foundation Models for Vital Signs like Blood Pressure](https://arxiv.org/abs/2510.10366)
*Saurabh Kataria,Ayca Ermis,Lovely Yeswanth Panchumarthi,Minxiao Wang,Xiao Hu*

Main category: cs.CV

TL;DR: 该研究提出Vision4PPG，通过将一维PPG信号转换为二维图像表示，利用视觉基础模型（VFM）在多项生理任务上实现了最先进的（SOTA）性能，甚至超越了时间序列基础模型，并具有计算效率。


<details>
  <summary>Details</summary>
Motivation: PPG传感器能提供宝贵的生理信息。当前生理任务的基准测试主要使用专门或 repurposed 的时间序列基础模型。研究旨在探索视觉基础模型是否也能用于此目的，并评估其性能。

Method: 将一维PPG信号转换为二维图像表示（例如短时傅里叶变换STFT），然后利用最新的视觉基础模型（如DINOv3和SIGLIP-2）进行微调。采用参数高效微调（PEFT）技术提高计算效率。通过全面研究，将VFM与最先进的时间序列基础模型进行比较，并在血压估计、其他生命体征、血液实验室测量以及六项额外任务上报告结果。

Result: 视觉基础模型在许多任务上（尤其是血压估计）取得了令人惊讶的最先进（SOTA）性能。在其他生命体征和血液实验室测量任务上也表现出良好性能。对包括STFT相位和递归图在内的其他二维输入表示具有显著的泛化能力。通过PEFT技术实现了计算效率，为临床医生-科学家提供了一套新的强大工具。

Conclusion: Vision4PPG通过利用视觉基础模型和二维PPG表示，开辟了新的基础模型类别，实现了PPG分析的SOTA性能，并具有出色的泛化能力和计算效率，为临床医生-科学家提供了强大的新工具。

Abstract: Photoplethysmography (PPG) sensor in wearable and clinical devices provides
valuable physiological insights in a non-invasive and real-time fashion.
Specialized Foundation Models (FM) or repurposed time-series FMs are used to
benchmark physiological tasks. Our experiments with fine-tuning FMs reveal that
Vision FM (VFM) can also be utilized for this purpose and, in fact,
surprisingly leads to state-of-the-art (SOTA) performance on many tasks,
notably blood pressure estimation. We leverage VFMs by simply transforming
one-dimensional PPG signals into image-like two-dimensional representations,
such as the Short-Time Fourier transform (STFT). Using the latest VFMs, such as
DINOv3 and SIGLIP-2, we achieve promising performance on other vital signs and
blood lab measurement tasks as well. Our proposal, Vision4PPG, unlocks a new
class of FMs to achieve SOTA performance with notable generalization to other
2D input representations, including STFT phase and recurrence plots. Our work
improves upon prior investigations of vision models for PPG by conducting a
comprehensive study, comparing them to state-of-the-art time-series FMs, and
demonstrating the general PPG processing ability by reporting results on six
additional tasks. Thus, we provide clinician-scientists with a new set of
powerful tools that is also computationally efficient, thanks to
Parameter-Efficient Fine-Tuning (PEFT) techniques.

</details>


### [147] [Self-Supervised Multi-Scale Transformer with Attention-Guided Fusion for Efficient Crack Detection](https://arxiv.org/abs/2510.10378)
*Blessing Agyei Kyem,Joshua Kofi Asamoah,Eugene Denteh,Andrews Danyo,Armstrong Aboah*

Main category: cs.CV

TL;DR: 本文提出了一种名为Crack-Segmenter的完全自监督框架，无需手动标注即可实现像素级路面裂缝分割，并在多项指标上超越了有监督方法。


<details>
  <summary>Details</summary>
Motivation: 传统的路面裂缝检测依赖耗时且昂贵的像素级手动标注，严重限制了其在大规模基础设施监测中的应用和可扩展性。

Method: 开发了名为Crack-Segmenter的完全自监督框架，包含三个互补模块：用于鲁棒多尺度特征提取的尺度自适应嵌入器（SAE）、用于保持线性裂缝连续性的方向注意力变换器（DAT）以及用于自适应特征融合的注意力引导融合（AGF）模块。

Result: Crack-Segmenter在十个公共数据集上，于平均交并比（mIoU）、Dice分数、XOR和豪斯多夫距离（HD）等所有主要指标上，持续优于13种最先进的有监督方法。

Conclusion: 研究结果表明，无需标注的裂缝检测不仅可行，而且性能更优越，这使得交通部门和基础设施管理者能够进行可扩展且经济高效的监测。这项工作推动了自监督学习和路面裂缝检测研究的进展。

Abstract: Pavement crack detection has long depended on costly and time-intensive
pixel-level annotations, which limit its scalability for large-scale
infrastructure monitoring. To overcome this barrier, this paper examines the
feasibility of achieving effective pixel-level crack segmentation entirely
without manual annotations. Building on this objective, a fully self-supervised
framework, Crack-Segmenter, is developed, integrating three complementary
modules: the Scale-Adaptive Embedder (SAE) for robust multi-scale feature
extraction, the Directional Attention Transformer (DAT) for maintaining linear
crack continuity, and the Attention-Guided Fusion (AGF) module for adaptive
feature integration. Through evaluations on ten public datasets,
Crack-Segmenter consistently outperforms 13 state-of-the-art supervised methods
across all major metrics, including mean Intersection over Union (mIoU), Dice
score, XOR, and Hausdorff Distance (HD). These findings demonstrate that
annotation-free crack detection is not only feasible but also superior,
enabling transportation agencies and infrastructure managers to conduct
scalable and cost-effective monitoring. This work advances self-supervised
learning and motivates pavement cracks detection research.

</details>


### [148] [Identifying bias in CNN image classification using image scrambling and transforms](https://arxiv.org/abs/2510.10383)
*Sai Teja Erukude*

Main category: cs.CV

TL;DR: 本文讨论了卷积神经网络（CNN）的“黑箱”特性及其对背景噪声产生的隐藏偏差。作者提出了两种方法，包括图像分块和图像变换，以区分上下文信息和背景噪声，从而识别并减轻这些偏差。


<details>
  <summary>Details</summary>
Motivation: CNN因其卓越的分类性能而广泛应用于机器视觉，但其数据驱动的特征选择方式使其成为“黑箱”，难以理解其决策过程。这可能导致模型被难以检测的背景信息偏置，用户无法了解图像数据如何被分析，只能依赖经验结果。

Method: 本文提出了识别隐藏偏差的技术，并区分上下文信息与背景噪声的方法。具体包括两种主要方法：1. 将图像分割成不同大小的、随机打乱的、不重叠的小块，以增加分类难度。2. 应用多种图像变换（如傅里叶变换、小波变换和中值滤波器及其组合），以恢复CNN用于分类的背景噪声信息。这些方法在六个不同的数据集（包括自然、合成和混合数据集）上进行了测试。

Result: 结果表明，所提出的方法（特别是图像变换方法）能够有效地区分上下文信息和背景噪声，并且即使在没有明确的背景信息的情况下，也能警示背景噪声的存在。

Conclusion: 本文提出的方法有助于识别CNN决策过程中由背景噪声引起的隐藏偏差，有效区分图像中的上下文信息和背景噪声，从而提高CNN模型的可解释性和可靠性。

Abstract: CNNs are now prevalent as the primary choice for most machine vision problems
due to their superior rate of classification and the availability of
user-friendly libraries. These networks effortlessly identify and select
features in a non-intuitive data-driven manner, making it difficult to
determine which features were most influential. That leads to a ``black box",
where users cannot know how the image data are analyzed but rely on empirical
results. Therefore the decision-making process can be biased by background
information that is difficult to detect. Here we discuss examples of such
hidden biases and propose techniques for identifying them, methods to
distinguish between contextual information and background noise, and explore
whether CNNs learn from irrelevant features. One effective approach to identify
dataset bias is to classify blank background parts of the images. However, in
some situations a blank background in the images is not available, making it
more difficult to separate the foreground information from the blank
background. Such parts of the image can also be considered contextual learning,
not necessarily bias. To overcome this, we propose two approaches that were
tested on six different datasets, including natural, synthetic, and hybrid
datasets. The first method involves dividing images into smaller,
non-overlapping tiles of various sizes, which are then shuffled randomly,
making classification more challenging. The second method involves the
application of several image transforms, including Fourier, Wavelet transforms,
and Median filter, and their combinations. These transforms help recover
background noise information used by CNN to classify images. Results indicate
that this method can effectively distinguish between contextual information and
background noise, and alert on the presence of background noise even without
the need to use background information.

</details>


### [149] [AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration](https://arxiv.org/abs/2510.10395)
*Xinlong Chen,Yue Ding,Weihong Lin,Jingyun Hua,Linli Yao,Yang Shi,Bozhou Li,Yuanxing Zhang,Qiang Liu,Pengfei Wan,Liang Wang,Tieniu Tan*

Main category: cs.CV

TL;DR: 本文提出了AVoCaDO，一个通过音视频模态间时间协调驱动的强大音视频视频字幕生成器，采用两阶段后训练流程，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 音视频视频字幕生成旨在生成语义丰富且音视频事件时间对齐的描述，以促进视频理解和生成。现有模型可能在此方面存在不足。

Method: 本文提出了AVoCaDO模型，其核心在于音视频模态间的时间编排。它采用两阶段后训练流程：1) AVoCaDO SFT，在一个新整理的107K高质量、时间对齐的音视频字幕数据集上进行微调；2) AVoCaDO GRPO，利用定制的奖励函数进一步增强时间连贯性和对话准确性，同时规范字幕长度并减少内容崩溃。

Result: 实验结果表明，AVoCaDO在四个音视频视频字幕生成基准测试中显著优于现有开源模型。此外，在仅视觉设置下，它在VDC和DREAM-1K基准测试中也取得了有竞争力的表现。

Conclusion: AVoCaDO是一个强大且高效的音视频视频字幕生成器，通过其创新的两阶段后训练方法和对时间协调的关注，显著提升了音视频字幕生成的性能，并在纯视觉场景下也表现出色。

Abstract: Audiovisual video captioning aims to generate semantically rich descriptions
with temporal alignment between visual and auditory events, thereby benefiting
both video understanding and generation. In this paper, we present AVoCaDO, a
powerful audiovisual video captioner driven by the temporal orchestration
between audio and visual modalities. We propose a two-stage post-training
pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated
dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2)
AVoCaDO GRPO, which leverages tailored reward functions to further enhance
temporal coherence and dialogue accuracy while regularizing caption length and
reducing collapse. Experimental results demonstrate that AVoCaDO significantly
outperforms existing open-source models across four audiovisual video
captioning benchmarks, and also achieves competitive performance on the VDC and
DREAM-1K benchmark under visual-only settings.

</details>


### [150] [Mesh-Gait: A Unified Framework for Gait Recognition Through Multi-Modal Representation Learning from 2D Silhouettes](https://arxiv.org/abs/2510.10406)
*Zhao-Yang Wang,Jieneng Chen,Jiang Liu,Yuxiang Guo,Rama Chellappa*

Main category: cs.CV

TL;DR: Mesh-Gait是一种新颖的多模态步态识别框架，通过从2D轮廓直接重建3D热图作为中间表示，有效结合了2D方法的效率和3D方法的鲁棒性，解决了现有2D方法对视角变化敏感和3D方法计算成本高昂的问题，并实现了最先进的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的2D步态识别（如轮廓或骨架）容易受到视角变化、遮挡和噪声的影响。虽然结合3D身体形状信息的多模态方法更具鲁棒性，但其计算成本高昂，不适用于实时应用。

Method: Mesh-Gait是一个端到端的多模态步态识别框架。它从2D轮廓直接重建3D热图作为中间表示，以高效地捕获3D几何信息。在训练过程中，通过监督学习逐步精确重建3D热图，损失函数基于重建的3D关节、虚拟标记和3D网格与对应的真实值计算。模型从轮廓和重建的3D热图中提取判别性特征，避免了从RGB视频直接进行昂贵的3D重建。

Result: Mesh-Gait在广泛的实验中实现了最先进的准确性。

Conclusion: Mesh-Gait通过将3D热图作为中间表示，成功地将2D轮廓的计算效率与3D几何信息的鲁棒性结合起来，为步态识别提供了一种高效且准确的解决方案，克服了现有方法的局限性。

Abstract: Gait recognition, a fundamental biometric technology, leverages unique
walking patterns for individual identification, typically using 2D
representations such as silhouettes or skeletons. However, these methods often
struggle with viewpoint variations, occlusions, and noise. Multi-modal
approaches that incorporate 3D body shape information offer improved robustness
but are computationally expensive, limiting their feasibility for real-time
applications. To address these challenges, we introduce Mesh-Gait, a novel
end-to-end multi-modal gait recognition framework that directly reconstructs 3D
representations from 2D silhouettes, effectively combining the strengths of
both modalities. Compared to existing methods, directly learning 3D features
from 3D joints or meshes is complex and difficult to fuse with silhouette-based
gait features. To overcome this, Mesh-Gait reconstructs 3D heatmaps as an
intermediate representation, enabling the model to effectively capture 3D
geometric information while maintaining simplicity and computational
efficiency. During training, the intermediate 3D heatmaps are gradually
reconstructed and become increasingly accurate under supervised learning, where
the loss is calculated between the reconstructed 3D joints, virtual markers,
and 3D meshes and their corresponding ground truth, ensuring precise spatial
alignment and consistent 3D structure. Mesh-Gait extracts discriminative
features from both silhouettes and reconstructed 3D heatmaps in a
computationally efficient manner. This design enables the model to capture
spatial and structural gait characteristics while avoiding the heavy overhead
of direct 3D reconstruction from RGB videos, allowing the network to focus on
motion dynamics rather than irrelevant visual details. Extensive experiments
demonstrate that Mesh-Gait achieves state-of-the-art accuracy. The code will be
released upon acceptance of the paper.

</details>


### [151] [Combo-Gait: Unified Transformer Framework for Multi-Modal Gait Recognition and Attribute Analysis](https://arxiv.org/abs/2510.10417)
*Zhao-Yang Wang,Zhimin Shao,Jieneng Chen,Rama Chellappa*

Main category: cs.CV

TL;DR: 本文提出了一种多模态（2D剪影+3D SMPL）和多任务（步态识别+人体属性估计）框架，利用统一的Transformer模型进行鲁棒的步态分析，并在挑战性数据集上超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前的步态识别方法通常依赖单一模态（2D或3D），未能充分捕捉人类步态的几何和动态复杂性，尤其在低分辨率或无约束环境下表现不佳。研究人员希望开发一种更全面的方法，不仅能进行身份识别，还能估计人体属性。

Method: 该研究提出了一种多模态和多任务框架：1) 结合了2D时间剪影和3D SMPL特征；2) 采用多任务学习策略，同时进行步态识别和人体属性（年龄、BMI、性别）估计；3) 使用统一的Transformer模型有效融合多模态步态特征，学习与属性相关的表示，同时保留判别性身份线索。

Result: 在具有挑战性条件（长距离、极端俯仰角）的大规模BRIAR数据集上，该方法在步态识别方面优于现有最先进的方法，并提供了准确的人体属性估计。

Conclusion: 多模态和多任务学习有望推动步态识别领域在真实世界场景中的人类理解能力进一步发展。

Abstract: Gait recognition is an important biometric for human identification at a
distance, particularly under low-resolution or unconstrained environments.
Current works typically focus on either 2D representations (e.g., silhouettes
and skeletons) or 3D representations (e.g., meshes and SMPLs), but relying on a
single modality often fails to capture the full geometric and dynamic
complexity of human walking patterns. In this paper, we propose a multi-modal
and multi-task framework that combines 2D temporal silhouettes with 3D SMPL
features for robust gait analysis. Beyond identification, we introduce a
multitask learning strategy that jointly performs gait recognition and human
attribute estimation, including age, body mass index (BMI), and gender. A
unified transformer is employed to effectively fuse multi-modal gait features
and better learn attribute-related representations, while preserving
discriminative identity cues. Extensive experiments on the large-scale BRIAR
datasets, collected under challenging conditions such as long-range distances
(up to 1 km) and extreme pitch angles (up to 50{\deg}), demonstrate that our
approach outperforms state-of-the-art methods in gait recognition and provides
accurate human attribute estimation. These results highlight the promise of
multi-modal and multitask learning for advancing gait-based human understanding
in real-world scenarios.

</details>


### [152] [Towards Cybersickness Severity Classification from VR Gameplay Videos Using Transfer Learning and Temporal Modeling](https://arxiv.org/abs/2510.10422)
*Jyotirmay Nag Setu,Kevin Desai,John Quarles*

Main category: cs.CV

TL;DR: 本研究利用迁移学习和LSTM网络，通过VR游戏视频的视觉特征来预测网络晕动症的严重程度，并取得了68.4%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 虚拟现实（VR）技术的广泛应用受到网络晕动症的阻碍，而目前针对VR集成传感器数据（如眼球和头部追踪）的多模态深度学习研究较多，但利用视频特征预测网络晕动症的研究却很有限。

Method: 研究采用迁移学习方法，使用在ImageNet数据集上预训练的InceptionV3模型从VR游戏视频中提取高级视觉特征。随后，这些特征被输入到长短期记忆（LSTM）网络中，以捕捉VR体验的时间动态并预测网络晕动症的严重程度。

Result: 该方法有效利用了视频数据的时间序列特性，在网络晕动症严重程度分类上达到了68.4%的准确率。这一性能超越了现有仅基于视频数据训练的模型。

Conclusion: 本研究为VR开发者提供了一个评估和减轻虚拟环境中网络晕动症的实用工具，并为未来基于视频时间建模以提升VR应用用户舒适度的研究奠定了基础。

Abstract: With the rapid advancement of virtual reality (VR) technology, its adoption
across domains such as healthcare, education, and entertainment has grown
significantly. However, the persistent issue of cybersickness, marked by
symptoms resembling motion sickness, continues to hinder widespread acceptance
of VR. While recent research has explored multimodal deep learning approaches
leveraging data from integrated VR sensors like eye and head tracking, there
remains limited investigation into the use of video-based features for
predicting cybersickness. In this study, we address this gap by utilizing
transfer learning to extract high-level visual features from VR gameplay videos
using the InceptionV3 model pretrained on the ImageNet dataset. These features
are then passed to a Long Short-Term Memory (LSTM) network to capture the
temporal dynamics of the VR experience and predict cybersickness severity over
time. Our approach effectively leverages the time-series nature of video data,
achieving a 68.4% classification accuracy for cybersickness severity. This
surpasses the performance of existing models trained solely on video data,
providing a practical tool for VR developers to evaluate and mitigate
cybersickness in virtual environments. Furthermore, this work lays the
foundation for future research on video-based temporal modeling for enhancing
user comfort in VR applications.

</details>


### [153] [Taming a Retrieval Framework to Read Images in Humanlike Manner for Augmenting Generation of MLLMs](https://arxiv.org/abs/2510.10426)
*Suyang Xi,Chenxi Yang,Hong Ding,Yiqing Ni,Catherine C. Liu,Yunhao Liu,Chengqi Zhang*

Main category: cs.CV

TL;DR: 本文提出HuLiRAG框架，通过“识别-定位-重权”级联和掩码引导微调，改进多模态大语言模型在细粒度视觉问答中的幻觉问题，提升接地性和事实一致性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在细粒度视觉问答中常因文本查询未明确锚定视觉指代物而产生幻觉（如物体身份、位置、关系错误）。现有检索增强生成（RAG）方法未能像人类一样处理信息，仅关注全局图像信息，缺乏局部细节和细粒度交互推理能力。

Method: 本文提出了类人检索增强生成（HuLiRAG）框架，其核心是“识别-定位-重权”（what-where-reweight）级联推理：
1. **识别 (what)**：通过开放词汇检测将查询锚定到候选指代物。
2. **定位 (where)**：利用SAM衍生的掩码空间解析指代物，恢复细粒度精度。
3. **重权 (reweight)**：通过权衡局部和全局对齐，自适应地进行优先级排序。
此外，还引入了掩码引导的微调，将空间证据注入生成过程，使接地成为答案形成的一个明确约束。

Result: 广泛实验表明，这种类人级联方法显著提高了接地保真度和事实一致性，同时减少了幻觉，推动了多模态问答向更可信赖的推理发展。

Conclusion: HuLiRAG框架通过模拟人类的视觉推理过程，有效解决了多模态大语言模型在细粒度视觉问答中的幻觉问题，显著提升了模型的接地能力和生成答案的可靠性。

Abstract: Multimodal large language models (MLLMs) often fail in fine-grained visual
question answering, producing hallucinations about object identities,
positions, and relations because textual queries are not explicitly anchored to
visual referents. Retrieval-augmented generation (RAG) alleviates some errors,
but it fails to align with human-like processing at both the retrieval and
augmentation levels. Specifically, it focuses only on global-level image
information but lacks local detail and limits reasoning about fine-grained
interactions. To overcome this limitation, we present Human-Like
Retrieval-Augmented Generation (HuLiRAG), a framework that stages multimodal
reasoning as a ``what--where--reweight'' cascade. Queries are first anchored to
candidate referents via open-vocabulary detection (what), then spatially
resolved with SAM-derived masks to recover fine-grained precision (where), and
adaptively prioritized through the trade-off between local and global alignment
(reweight). Mask-guided fine-tuning further injects spatial evidence into the
generation process, transforming grounding from a passive bias into an explicit
constraint on answer formulation. Extensive experiments demonstrate that this
human-like cascade improves grounding fidelity and factual consistency while
reducing hallucinations, advancing multimodal question answering toward
trustworthy reasoning.

</details>


### [154] [DKPMV: Dense Keypoints Fusion from Multi-View RGB Frames for 6D Pose Estimation of Textureless Objects](https://arxiv.org/abs/2510.10933)
*Jiahong Chen,Jinghao Wang,Zi Wang,Ziwen Wang,Banglei Guan,Qifeng Yu*

Main category: cs.CV

TL;DR: 本文提出了DKPMV，一个仅使用多视角RGB图像进行无纹理物体6D姿态估计的管道。它通过密集关键点级融合和三阶段渐进式姿态优化策略，在ROBI数据集上超越了现有先进的多视角RGB甚至多数RGB-D方法。


<details>
  <summary>Details</summary>
Motivation: 无纹理物体6D姿态估计对工业机器人应用很有价值，但由于深度信息频繁丢失而充满挑战。当前多视角方法要么依赖深度数据，要么未能充分利用多视角几何线索，限制了其性能。

Method: 本文提出了DKPMV管道，仅以多视角RGB图像作为输入，实现密集关键点级融合。设计了三阶段渐进式姿态优化策略，利用密集的关键点几何信息。为实现有效的密集关键点融合，通过注意力聚合和对称性感知训练增强了关键点网络，以提高预测精度并解决对称物体上的歧义。

Result: 在ROBI数据集上的大量实验表明，DKPMV优于现有最先进的多视角RGB方法，并且在大多数情况下甚至超越了RGB-D方法。

Conclusion: DKPMV提供了一种仅利用多视角RGB图像对无纹理物体进行6D姿态估计的有效解决方案，通过其创新的关键点融合和姿态优化策略，显著提升了性能，甚至超越了依赖深度信息的方法。

Abstract: 6D pose estimation of textureless objects is valuable for industrial robotic
applications, yet remains challenging due to the frequent loss of depth
information. Current multi-view methods either rely on depth data or
insufficiently exploit multi-view geometric cues, limiting their performance.
In this paper, we propose DKPMV, a pipeline that achieves dense keypoint-level
fusion using only multi-view RGB images as input. We design a three-stage
progressive pose optimization strategy that leverages dense multi-view keypoint
geometry information. To enable effective dense keypoint fusion, we enhance the
keypoint network with attentional aggregation and symmetry-aware training,
improving prediction accuracy and resolving ambiguities on symmetric objects.
Extensive experiments on the ROBI dataset demonstrate that DKPMV outperforms
state-of-the-art multi-view RGB approaches and even surpasses the RGB-D methods
in the majority of cases. The code will be available soon.

</details>


### [155] [REACT3D: Recovering Articulations for Interactive Physical 3D Scenes](https://arxiv.org/abs/2510.11340)
*Zhao Huang,Boyang Sun,Alexandros Delitzas,Jiaqi Chen,Marc Pollefeys*

Main category: cs.CV

TL;DR: REACT3D是一个可扩展的零样本框架，能将静态3D场景转换为可用于仿真的交互式副本，解决了现有数据集标注耗时的问题。


<details>
  <summary>Details</summary>
Motivation: 具身智能对交互式3D场景的需求日益增长，但现有数据集因部分分割、运动类型和轨迹标注耗时而受限。

Method: REACT3D框架包括四个主要贡献：(i) 可开启物体检测与分割以提取可移动部件，(ii) 关节估计以推断关节类型和运动参数，(iii) 隐藏几何体补全和交互式物体组装，以及 (iv) 将交互式场景整合到广泛支持的格式中。

Result: 在多种室内场景的检测/分割和关节指标上实现了最先进的性能，证明了该框架的有效性。

Conclusion: REACT3D为可扩展的交互式场景生成提供了实用基础，降低了大规模关节场景理解研究的门槛。

Abstract: Interactive 3D scenes are increasingly vital for embodied intelligence, yet
existing datasets remain limited due to the labor-intensive process of
annotating part segmentation, kinematic types, and motion trajectories. We
present REACT3D, a scalable zero-shot framework that converts static 3D scenes
into simulation-ready interactive replicas with consistent geometry, enabling
direct use in diverse downstream tasks. Our contributions include: (i)
openable-object detection and segmentation to extract candidate movable parts
from static scenes, (ii) articulation estimation that infers joint types and
motion parameters, (iii) hidden-geometry completion followed by interactive
object assembly, and (iv) interactive scene integration in widely supported
formats to ensure compatibility with standard simulation platforms. We achieve
state-of-the-art performance on detection/segmentation and articulation metrics
across diverse indoor scenes, demonstrating the effectiveness of our framework
and providing a practical foundation for scalable interactive scene generation,
thereby lowering the barrier to large-scale research on articulated scene
understanding. Our project page is
\textit{\hypersetup{urlcolor=black}\href{https://react3d.github.io/}{react3d.github.io}}.

</details>


### [156] [MonoSE(3)-Diffusion: A Monocular SE(3) Diffusion Framework for Robust Camera-to-Robot Pose Estimation](https://arxiv.org/abs/2510.10434)
*Kangjian Zhu,Haobo Jiang,Yigong Zhang,Jianjun Qian,Jian Yang,Jin Xie*

Main category: cs.CV

TL;DR: MonoSE(3)-Diffusion是一种单目SE(3)扩散框架，通过可见性约束的扩散过程生成多样化姿态增强，并利用时间步感知的逆向过程进行渐进式姿态细化，从而实现无标记、基于图像的机器人姿态估计。


<details>
  <summary>Details</summary>
Motivation: 当前无标记、基于图像的机器人姿态估计方法使用固定尺度的扰动，导致泛化能力受限。研究旨在通过生成更具多样性和视野内的训练姿态，并采用更鲁棒的细化策略来提高姿态估计的泛化能力和准确性。

Method: 该框架包含两个核心过程：1. 可见性约束的扩散过程：将真实姿态逐步扰动为噪声变换，用于训练姿态去噪网络，并集成可见性约束以确保变换保持在相机视野内，从而生成多样且视野内的训练姿态。2. 时间步感知的逆向过程：通过去噪网络迭代预测并细化姿态估计，从当前时间步的扩散后验中采样，遵循从粗到细的程序，其中时间步指示变换尺度以指导更精确的姿态预测。

Result: 该方法在DREAM和RoboKeyGen两个基准测试中均有改进，在最具挑战性的数据集上取得了66.75的AUC，比现有最先进技术提高了32.3%。逆向过程由于其时间步感知的细化方案，比直接预测表现出更高的鲁棒性。

Conclusion: MonoSE(3)-Diffusion通过结合可见性约束的扩散过程和时间步感知的逆向细化过程，显著提升了无标记机器人姿态估计的泛化能力、准确性和鲁棒性，超越了现有技术水平。

Abstract: We propose MonoSE(3)-Diffusion, a monocular SE(3) diffusion framework that
formulates markerless, image-based robot pose estimation as a conditional
denoising diffusion process. The framework consists of two processes: a
visibility-constrained diffusion process for diverse pose augmentation and a
timestep-aware reverse process for progressive pose refinement. The diffusion
process progressively perturbs ground-truth poses to noisy transformations for
training a pose denoising network. Importantly, we integrate visibility
constraints into the process, ensuring the transformations remain within the
camera field of view. Compared to the fixed-scale perturbations used in current
methods, the diffusion process generates in-view and diverse training poses,
thereby improving the network generalization capability. Furthermore, the
reverse process iteratively predicts the poses by the denoising network and
refines pose estimates by sampling from the diffusion posterior of current
timestep, following a scheduled coarse-to-fine procedure. Moreover, the
timestep indicates the transformation scales, which guide the denoising network
to achieve more accurate pose predictions. The reverse process demonstrates
higher robustness than direct prediction, benefiting from its timestep-aware
refinement scheme. Our approach demonstrates improvements across two benchmarks
(DREAM and RoboKeyGen), achieving a notable AUC of 66.75 on the most
challenging dataset, representing a 32.3% gain over the state-of-the-art.

</details>


### [157] [On the Problem of Consistent Anomalies in Zero-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2510.10456)
*Tai Le-Gia,Ahn Jaehyun*

Main category: cs.CV

TL;DR: 本文提出CoDeGraph算法，通过识别并过滤零样本图像异常分类与分割中常见的“一致性异常”（重复缺陷），显著提升了工业质量控制的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于表示的零样本异常检测方法在处理工业图像中反复出现且相似的“一致性异常”时表现不佳，导致异常分类和分割性能受限。

Method: CoDeGraph算法利用“邻居耗尽”（neighbor-burnout）现象来区分正常补丁和一致性异常补丁：正常补丁相似度稳定增长，而一致性异常补丁在耗尽有限相似匹配后会表现出突然的相似度峰值。该方法构建一个图像级图，通过社区检测来识别和过滤共享一致性异常模式的图像，并提供了极值理论的理论基础。

Result: 在MVTec AD数据集上，使用ViT-L-14-336骨干网络，CoDeGraph在异常分类（AC）中实现了98.3%的AUROC，在异常分割（AS）中F1值达到66.8%（提升4.2%），AP达到68.1%（提升5.4%），超越了现有最先进的零样本方法。使用DINOv2骨干网络进一步将分割F1值提升至69.1%（提升6.5%），AP提升至71.9%（提升9.2%），证明了其跨架构的鲁棒性。

Conclusion: CoDeGraph通过有效识别和过滤一致性异常，显著提升了零样本图像异常分类和分割的性能，并在不同网络架构下表现出强大的鲁棒性，为工业质量控制提供了重要的解决方案。

Abstract: Zero-shot image anomaly classification (AC) and segmentation (AS) are vital
for industrial quality control, detecting defects without prior training data.
Existing representation-based methods compare patch features with nearest
neighbors in unlabeled test images but struggle with consistent anomalies --
similar defects recurring across multiple images -- resulting in poor AC/AS
performance. We introduce Consistent-Anomaly Detection Graph (CoDeGraph), a
novel algorithm that identifies and filters consistent anomalies from
similarity computations. Our key insight is that normal patches in industrial
images show stable, gradually increasing similarity to other test images, while
consistent-anomaly patches exhibit abrupt similarity spikes after exhausting a
limited set of similar matches, a phenomenon we term ``neighbor-burnout.''
CoDeGraph constructs an image-level graph, with images as nodes and edges
connecting those with shared consistent-anomaly patterns, using community
detection to filter these anomalies. We provide a theoretical foundation using
Extreme Value Theory to explain the effectiveness of our approach. Experiments
on MVTec AD with the ViT-L-14-336 backbone achieve 98.3% AUROC for AC and AS
performance of 66.8% (+4.2%) F1 and 68.1% (+5.4%) AP over state-of-the-art
zero-shot methods. Using the DINOv2 backbone further improves segmentation,
yielding 69.1% (+6.5%) F1 and 71.9% (+9.2%) AP, demonstrating robustness across
architectures.

</details>


### [158] [Learning from Disagreement: A Group Decision Simulation Framework for Robust Medical Image Segmentation](https://arxiv.org/abs/2510.10462)
*Chen Zhong,Yuxuan Yang,Xinyue Zhang,Ruohan Ma,Yong Guo,Gang Li,Jupeng Li*

Main category: cs.CV

TL;DR: 针对医学图像分割标注中的专家间变异性，本文提出了一种新的群体决策模拟框架，通过学习个体标注风格并模拟专家协商过程，将分歧视为有用信号，从而生成更鲁棒的分割结果。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割标注存在专家间变异性（IRV），原因在于标注者专业知识差异和图像固有的模糊性。传统的简单平均专家标签的方法存在缺陷，因为它丢弃了分歧中揭示的有价值的临床不确定性。

Method: 引入了一个群体决策模拟框架，该框架模仿临床专家小组的协作决策过程。其中包含：1) 专家签名生成器（ESG），用于在独特的潜在空间中学习表示个体标注者的风格；2) 模拟咨询模块（SCM），通过从该空间采样智能地生成最终分割。

Result: 该方法在具有挑战性的CBCT和MRI数据集上取得了最先进的结果，Dice分数分别达到92.11%和90.72%。

Conclusion: 通过将专家分歧视为有用的信号而非噪声，本研究为构建更鲁棒、更值得信赖的医疗AI系统提供了清晰的途径。

Abstract: Medical image segmentation annotation suffers from inter-rater variability
(IRV) due to differences in annotators' expertise and the inherent blurriness
of medical images. Standard approaches that simply average expert labels are
flawed, as they discard the valuable clinical uncertainty revealed in
disagreements. We introduce a fundamentally new approach with our group
decision simulation framework, which works by mimicking the collaborative
decision-making process of a clinical panel. Under this framework, an Expert
Signature Generator (ESG) learns to represent individual annotator styles in a
unique latent space. A Simulated Consultation Module (SCM) then intelligently
generates the final segmentation by sampling from this space. This method
achieved state-of-the-art results on challenging CBCT and MRI datasets (92.11%
and 90.72% Dice scores). By treating expert disagreement as a useful signal
instead of noise, our work provides a clear path toward more robust and
trustworthy AI systems for healthcare.

</details>


### [159] [Post-TIPS Prediction via Multimodal Interaction: A Multi-Center Dataset and Framework for Survival, Complication, and Portal Pressure Assessment](https://arxiv.org/abs/2510.10464)
*Junhao Dong,Dejia Liu,Ruiqi Ding,Zongxing Chen,Yingjie Huang,Zhu Meng,Jianbo Zhao,Zhicheng Zhao,Fei Su*

Main category: cs.CV

TL;DR: 本研究提出了首个公开的多中心TIPS预后数据集MultiTIPS，并基于此构建了一个新颖的多模态预后框架，以实现准确、鲁棒且全面的术前预后评估。


<details>
  <summary>Details</summary>
Motivation: 经颈静脉肝内门体分流术（TIPS）在治疗门静脉高压方面效果不一，且常导致肝性脑病，因此需要准确的术前预后建模。现有研究面临ROI标注困难、单模态方法可靠性和泛化性差、单一终点评估不完整以及缺乏公开数据集的挑战。

Method: 本研究提出MultiTIPS数据集和多模态预后框架。该框架包含三个核心模块：1) 双选项分割，整合半监督和基础模型实现鲁棒的ROI分割；2) 多模态交互，引入MGRA、POD和CGPE技术以实现跨模态特征交互和互补表示集成；3) 多任务预测，采用分阶段训练策略稳定优化生存、门静脉压力梯度（PPG）和肝性脑病（OHE）的预测。

Result: 在MultiTIPS数据集上的大量实验表明，所提出的方法优于现有最先进方法，并展现出强大的跨域泛化性和可解释性。

Conclusion: 本研究提出的方法在TIPS预后评估方面具有显著优势和临床应用潜力。MultiTIPS数据集和相关代码已公开，促进了该领域的研究。

Abstract: Transjugular intrahepatic portosystemic shunt (TIPS) is an established
procedure for portal hypertension, but provides variable survival outcomes and
frequent overt hepatic encephalopathy (OHE), indicating the necessity of
accurate preoperative prognostic modeling. Current studies typically build
machine learning models from preoperative CT images or clinical
characteristics, but face three key challenges: (1) labor-intensive
region-of-interest (ROI) annotation, (2) poor reliability and generalizability
of unimodal methods, and (3) incomplete assessment from single-endpoint
prediction. Moreover, the lack of publicly accessible datasets constrains
research in this field. Therefore, we present MultiTIPS, the first public
multi-center dataset for TIPS prognosis, and propose a novel multimodal
prognostic framework based on it. The framework comprises three core modules:
(1) dual-option segmentation, which integrates semi-supervised and foundation
model-based pipelines to achieve robust ROI segmentation with limited
annotations and facilitate subsequent feature extraction; (2) multimodal
interaction, where three techniques, multi-grained radiomics attention (MGRA),
progressive orthogonal disentanglement (POD), and clinically guided prognostic
enhancement (CGPE), are introduced to enable cross-modal feature interaction
and complementary representation integration, thus improving model accuracy and
robustness; and (3) multi-task prediction, where a staged training strategy is
used to perform stable optimization of survival, portal pressure gradient
(PPG), and OHE prediction for comprehensive prognostic assessment. Extensive
experiments on MultiTIPS demonstrate the superiority of the proposed method
over state-of-the-art approaches, along with strong cross-domain generalization
and interpretability, indicating its promise for clinical application. The
dataset and code are available.

</details>


### [160] [When Images Speak Louder: Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance](https://arxiv.org/abs/2510.10466)
*Jinjin Cao,Zhiyang Chen,Zijun Wang,Liyuan Ma,Weijian Luo,Guojun Qi*

Main category: cs.CV

TL;DR: 本文提出了一种名为跨模态引导（CMG）的免训练解码方法，通过利用原始模型与视觉语言注意力退化模型输出分布的差异，有效减少了视觉语言模型（VLM）中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLM在多模态理解方面表现出色，但常面临幻觉问题，即生成语言流畅但与图像无关的响应。这主要是由于语言偏见造成的。

Method: 引入了跨模态引导（CMG），这是一种免训练的解码方法。它通过比较原始模型和视觉语言注意力退化模型的输出分布差异来解决幻觉问题。具体实践中，CMG通过自适应地遮蔽选定Transformer层中最具影响力的图像token的注意力权重，来破坏视觉语言感知，从而强调视觉上下文感知，显著减少语言偏见。

Result: 实验结果表明，CMG在不增加额外条件或训练成本的情况下，展现出卓越优势。它能显著减少语言偏见，同时不损害VLM的能力。CMG在幻觉特定基准上提高了不同VLM的性能，并能有效泛化。

Conclusion: CMG通过利用视觉语言注意力退化诱导的解码，成功地在不进行额外训练或损害模型能力的情况下，解决了VLM的幻觉问题，并有效减少了语言偏见，具有良好的泛化性。

Abstract: Vision-Language Models (VLMs) have shown solid ability for multimodal
understanding of both visual and language contexts. However, existing VLMs
often face severe challenges of hallucinations, meaning that VLMs tend to
generate responses that are only fluent in the language but irrelevant to
images in previous contexts. To address this issue, we analyze how language
bias contributes to hallucinations and then introduce Cross-Modal
Guidance(CMG), a training-free decoding method that addresses the
hallucinations by leveraging the difference between the output distributions of
the original model and the one with degraded visual-language attention. In
practice, we adaptively mask the attention weight of the most influential image
tokens in selected transformer layers to corrupt the visual-language perception
as a concrete type of degradation. Such a degradation-induced decoding
emphasizes the perception of visual contexts and therefore significantly
reduces language bias without harming the ability of VLMs. In experiment
sections, we conduct comprehensive studies. All results demonstrate the
superior advantages of CMG with neither additional conditions nor training
costs. We also quantitatively show CMG can improve different VLM's performance
on hallucination-specific benchmarks and generalize effectively.

</details>


### [161] [DAGLFNet:Deep Attention-Guided Global-Local Feature Fusion for Pseudo-Image Point Cloud Segmentation](https://arxiv.org/abs/2510.10471)
*Chuang Chen,Wenyi Ge*

Main category: cs.CV

TL;DR: DAGLFNet是一种基于伪图像的LiDAR点云语义分割框架，旨在通过增强局部特征关联、捕获全局上下文信息、提取多邻域特征和引入深度特征引导的注意力机制，提高特征融合和判别能力，同时平衡高性能和实时性。


<details>
  <summary>Details</summary>
Motivation: 高精度地图和自动导航中的环境感知系统对LiDAR点云数据处理效率和语义信息提取提出了挑战。现有基于伪图像的LiDAR点云处理方法虽然高效，但常忽略点云的结构和语义细节，导致特征融合和判别能力有限。

Method: 本文提出了DAGLFNet框架，具体包括：1. 全局-局部特征融合编码模块，用于增强局部特征关联并捕获全局上下文信息。2. 多分支特征提取网络，用于捕获更多邻域信息并增强轮廓特征的判别力。3. 基于深度特征引导的注意力机制的特征融合，以提高跨通道特征融合的精度。

Result: DAGLFNet在SemanticKITTI和nuScenes验证集上分别达到了69.83%和78.65%的性能。该方法在保持高性能的同时，也具备实时处理能力。

Conclusion: DAGLFNet在LiDAR点云语义分割中实现了高性能与实时性的良好平衡，在基于LiDAR的实时应用中展现出巨大潜力。

Abstract: Environmental perception systems play a critical role in high-precision
mapping and autonomous navigation, with LiDAR serving as a core sensor that
provides accurate 3D point cloud data. How to efficiently process unstructured
point clouds while extracting structured semantic information remains a
significant challenge, and in recent years, numerous pseudo-image-based
representation methods have emerged to achieve a balance between efficiency and
performance. However, they often overlook the structural and semantic details
of point clouds, resulting in limited feature fusion and discriminability. In
this work, we propose DAGLFNet, a pseudo-image-based semantic segmentation
framework designed to extract discriminative features. First, the Global-Local
Feature Fusion Encoding module is used to enhance the correlation among local
features within a set and capture global contextual information. Second, the
Multi-Branch Feature Extraction network is employed to capture more
neighborhood information and enhance the discriminability of contour features.
Finally, a Feature Fusion via Deep Feature-guided Attention mechanism is
introduced to improve the precision of cross-channel feature fusion.
Experimental evaluations show that DAGLFNet achieves 69.83\% and 78.65\% on the
validation sets of SemanticKITTI and nuScenes, respectively. The method
balances high performance with real-time capability, demonstrating great
potential for LiDAR-based real-time applications.

</details>


### [162] [MSF-Mamba: Motion-aware State Fusion Mamba for Efficient Micro-Gesture Recognition](https://arxiv.org/abs/2510.10478)
*Deng Li,Jun Shao,Bohao Xing,Rong Gao,Bihan Wen,Heikki Kälviäinen,Xin Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为MSF-Mamba的新型模型，通过引入运动感知状态融合和多尺度机制，增强了Mamba在微手势识别（MGR）中对局部时空依赖的建模能力，克服了现有方法的局限性，并实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 微手势识别需要精确建模长程和局部时空依赖。CNN擅长局部模式但缺乏长程依赖能力；Transformer通过自注意力解决长程依赖但计算成本高；Mamba作为高效模型，利用状态空间模型实现线性时间处理，但其1D序列处理方式导致缺乏局部时空依赖建模能力，且现有方法普遍缺少运动感知设计。

Method: 本文提出了运动感知状态融合Mamba (MSF-Mamba)。它通过融合局部上下文相邻状态来增强Mamba的局部时空建模能力，并引入了一个基于中心帧差（CFD）的运动感知状态融合模块。此外，还提出了多尺度版本MSF-Mamba+，支持多尺度运动感知状态融合，并包含一个自适应尺度加权模块，动态权衡不同尺度下的融合状态。

Result: 在两个公共MGR数据集上的实验表明，即使是轻量级的MSF-Mamba版本也实现了最先进（SoTA）的性能，优于现有的基于CNN、Transformer和SSM的模型，同时保持了高效率。

Conclusion: MSF-Mamba通过引入运动感知局部时空建模和多尺度融合，有效解决了传统Mamba在微手势识别中缺乏局部时空依赖建模和运动感知能力的问题，能够有效地捕捉细微的运动线索，并以高效率实现了最先进的性能。

Abstract: Micro-gesture recognition (MGR) targets the identification of subtle and
fine-grained human motions and requires accurate modeling of both long-range
and local spatiotemporal dependencies. While CNNs are effective at capturing
local patterns, they struggle with long-range dependencies due to their limited
receptive fields. Transformer-based models address this limitation through
self-attention mechanisms but suffer from high computational costs. Recently,
Mamba has shown promise as an efficient model, leveraging state space models
(SSMs) to enable linear-time processing However, directly applying the vanilla
Mamba to MGR may not be optimal. This is because Mamba processes inputs as 1D
sequences, with state updates relying solely on the previous state, and thus
lacks the ability to model local spatiotemporal dependencies. In addition,
previous methods lack a design of motion-awareness, which is crucial in MGR. To
overcome these limitations, we propose motion-aware state fusion mamba
(MSF-Mamba), which enhances Mamba with local spatiotemporal modeling by fusing
local contextual neighboring states. Our design introduces a motion-aware state
fusion module based on central frame difference (CFD). Furthermore, a
multiscale version named MSF-Mamba+ has been proposed. Specifically, MSF-Mamba
supports multiscale motion-aware state fusion, as well as an adaptive scale
weighting module that dynamically weighs the fused states across different
scales. These enhancements explicitly address the limitations of vanilla Mamba
by enabling motion-aware local spatiotemporal modeling, allowing MSF-Mamba and
MSF-Mamba to effectively capture subtle motion cues for MGR. Experiments on two
public MGR datasets demonstrate that even the lightweight version, namely,
MSF-Mamba, achieves SoTA performance, outperforming existing CNN-,
Transformer-, and SSM-based models while maintaining high efficiency.

</details>


### [163] [Towards Self-Refinement of Vision-Language Models with Triangular Consistency](https://arxiv.org/abs/2510.10487)
*Yunlong Deng,Guangyi Chen,Tianpei Gu,Lingjing Kong,Yan Li,Zeyu Tang,Kun Zhang*

Main category: cs.CV

TL;DR: 本研究验证了视觉-语言模型（VLMs）具有内在的自我完善能力，无需外部监督即可生成高质量的监督数据并自主学习，其核心是基于三角一致性原则的自我完善框架。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言模型（VLMs）主要依赖于监督视觉指令微调。然而，未经监督指令训练的VLMs的潜力尚未得到充分探索，研究旨在探索VLMs是否具备内在的自我完善能力。

Method: 提出一个基于“三角一致性”原则的自我完善框架：1. 通过多任务指令微调（如图像→问答、图像-答案→问题）激发VLMs的指令生成能力。2. 从无标签图像中生成图像-查询-答案三元组。3. 利用三角一致性原则（即图像-查询-答案三角形中任何被遮蔽的元素都应被一致且准确地重建）过滤生成的数据。4. 使用过滤后的合成数据更新模型。此外，从因果角度对自我完善机制进行了理论分析。

Result: 实验表明，以LLaVA-1.5为基线模型，无需任何外部监督（如人工标注或环境反馈），模型能够在多个基准测试中自主实现持续且适度的性能提升，证明了VLMs具有内在的自我完善能力。

Conclusion: 本研究揭示了VLMs的自我完善能力，为理解VLMs的学习机制提供了新见解，有望启发未来关于VLMs学习机制的研究。

Abstract: Vision-Language Models (VLMs) integrate visual knowledge with the analytical
capabilities of Large Language Models (LLMs) through supervised visual
instruction tuning, using image-question-answer triplets. However, the
potential of VLMs trained without supervised instruction remains largely
unexplored. This study validates that VLMs possess inherent self-refinement
capabilities, enabling them to generate high-quality supervised data without
external inputs and thereby learn autonomously. Specifically, to stimulate the
self-refinement ability of VLMs, we propose a self-refinement framework based
on a Triangular Consistency principle: within the image-query-answer triangle,
any masked elements should be consistently and accurately reconstructed. The
framework involves three steps: (1) We enable the instruction generation
ability of VLMs by adding multi-task instruction tuning like
image$\rightarrow$question-answer or image-answer$\rightarrow$question. (2) We
generate image-query-answer triplets from unlabeled images and use the
Triangular Consistency principle for filtering. (3) The model is further
updated using the filtered synthetic data. To investigate the underlying
mechanisms behind this self-refinement capability, we conduct a theoretical
analysis from a causal perspective. Using the widely recognized LLaVA-1.5 as
our baseline, our experiments reveal that the model can autonomously achieve
consistent, though deliberately modest, improvements across multiple benchmarks
without any external supervision, such as human annotations or environmental
feedback. We expect that the insights of this study on the self-refinement
ability of VLMs can inspire future research on the learning mechanism of VLMs.
Code is available at https://github.com/dengyl20/SRF-LLaVA-1.5.

</details>


### [164] [Head-wise Adaptive Rotary Positional Encoding for Fine-Grained Image Generation](https://arxiv.org/abs/2510.10489)
*Jiaye Li,Baoyou Chen,Hui Li,Zilong Dong,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: 本文提出HARoPE，一种头部自适应的RoPE扩展，通过在旋转映射前引入可学习的线性变换，解决了标准RoPE在图像生成中对精细空间关系建模的局限性，显著提升了图像生成模型的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer模型依赖显式位置编码来建模数据结构。RoPE在1D领域表现出色，但在图像生成中暴露出局限性，如难以建模精细空间关系、颜色线索和物体计数。研究发现标准多维RoPE存在频率分配僵硬、轴向独立性和头部处理统一等问题，无法捕捉图像生成所需的复杂结构偏差。

Method: 本文提出HARoPE（头部自适应扩展），通过在旋转映射前插入一个可学习的线性变换（通过奇异值分解SVD参数化），对RoPE进行了轻量级修改。这种修改使得动态频率重新分配、旋转平面的语义对齐以及头部特定的位置感受野成为可能，同时严格保留了RoPE的相对位置属性。

Result: 在类条件ImageNet和文生图任务（Flux和MMDiT）上的大量实验表明，HARoPE相对于强大的RoPE基线和其他扩展方法，持续提升了性能。

Conclusion: HARoPE作为一种有效的即插即用替代方案，为增强基于Transformer的图像生成模型中的位置感知提供了一个原则性且适应性强的解决方案。

Abstract: Transformers rely on explicit positional encoding to model structure in data.
While Rotary Position Embedding (RoPE) excels in 1D domains, its application to
image generation reveals significant limitations such as fine-grained spatial
relation modeling, color cues, and object counting. This paper identifies key
limitations of standard multi-dimensional RoPE-rigid frequency allocation,
axis-wise independence, and uniform head treatment-in capturing the complex
structural biases required for fine-grained image generation. We propose
HARoPE, a head-wise adaptive extension that inserts a learnable linear
transformation parameterized via singular value decomposition (SVD) before the
rotary mapping. This lightweight modification enables dynamic frequency
reallocation, semantic alignment of rotary planes, and head-specific positional
receptive fields while rigorously preserving RoPE's relative-position property.
Extensive experiments on class-conditional ImageNet and text-to-image
generation (Flux and MMDiT) demonstrate that HARoPE consistently improves
performance over strong RoPE baselines and other extensions. The method serves
as an effective drop-in replacement, offering a principled and adaptable
solution for enhancing positional awareness in transformer-based image
generative models.

</details>


### [165] [DEMO: Disentangled Motion Latent Flow Matching for Fine-Grained Controllable Talking Portrait Synthesis](https://arxiv.org/abs/2510.10650)
*Peiyin Chen,Zhuowei Yang,Hui Feng,Sheng Jiang,Rui Yan*

Main category: cs.CV

TL;DR: DEMO是一种基于流匹配的生成框架，用于音频驱动的说话肖像视频合成，通过运动自编码器实现唇部、头部姿态和眼睛凝视的解耦、高保真控制，生成时间连贯且逼真的视频。


<details>
  <summary>Details</summary>
Motivation: 尽管基于扩散的生成模型在音频驱动的说话人头部生成方面取得了快速进展，但在生成具有细粒度运动控制的时间连贯视频方面仍面临挑战。

Method: DEMO框架的核心是一个运动自编码器，它构建了一个结构化的潜在空间，其中运动因子（唇部、头部姿态、眼睛凝视）被独立表示并近似正交化。在此解耦的运动空间上，DEMO应用基于最优传输的流匹配和Transformer预测器，根据音频生成时间平滑的运动轨迹。

Result: 在多个基准测试中，DEMO在视频真实感、唇音同步和运动保真度方面均优于现有方法。

Conclusion: 将细粒度运动解耦与基于流的生成建模相结合，为可控的说话人头部视频合成提供了一种强大的新范式。

Abstract: Audio-driven talking-head generation has advanced rapidly with
diffusion-based generative models, yet producing temporally coherent videos
with fine-grained motion control remains challenging. We propose DEMO, a
flow-matching generative framework for audio-driven talking-portrait video
synthesis that delivers disentangled, high-fidelity control of lip motion, head
pose, and eye gaze. The core contribution is a motion auto-encoder that builds
a structured latent space in which motion factors are independently represented
and approximately orthogonalized. On this disentangled motion space, we apply
optimal-transport-based flow matching with a transformer predictor to generate
temporally smooth motion trajectories conditioned on audio. Extensive
experiments across multiple benchmarks show that DEMO outperforms prior methods
in video realism, lip-audio synchronization, and motion fidelity. These results
demonstrate that combining fine-grained motion disentanglement with flow-based
generative modeling provides a powerful new paradigm for controllable
talking-head video synthesis.

</details>


### [166] [GLOFNet -- A Multimodal Dataset for GLOF Monitoring and Prediction](https://arxiv.org/abs/2510.10546)
*Zuha Fatima,Muhammad Anser Sohaib,Muhammad Talha,Sidra Sultana,Ayesha Kanwal,Nazia Perwaiz*

Main category: cs.CV

TL;DR: GLOFNet是一个多模态数据集，旨在解决冰湖溃决洪水（GLOFs）预测中数据碎片化和单模态的挑战，整合了多源卫星数据，并为未来的深度学习预测研究提供了一个基准。


<details>
  <summary>Details</summary>
Motivation: 冰湖溃决洪水是高山地区的破坏性灾害，但预测研究因数据碎片化和单模态而受阻。大多数现有工作侧重于事件后测绘，而预测需要结合视觉指标和物理前兆的协调数据集。

Method: 本文提出了GLOFNet数据集，专注于喀喇昆仑山脉的Shisper冰川。它整合了三种互补来源：Sentinel-2多光谱图像（用于空间监测）、NASA ITS_LIVE速度产品（用于冰川运动学）和MODIS地表温度记录（跨越二十多年）。预处理包括云掩膜、质量过滤、归一化、时间插值、数据增强和周期性编码，随后进行模态间的协调。

Result: 探索性分析揭示了季节性冰川速度周期、每十年约0.8 K的长期变暖以及冰冻圈条件的空间异质性。GLOFNet数据集已公开发布，以支持未来的冰川灾害预测研究。通过解决类别不平衡、云污染和粗分辨率等挑战，GLOFNet为基准多模态深度学习方法预测罕见灾害提供了一个结构化基础。

Conclusion: GLOFNet通过整合多模态数据并解决现有挑战，为冰湖溃决洪水的监测和预测提供了一个结构化的数据集和研究基础，有助于推动未来罕见灾害预测的深度学习方法发展。

Abstract: Glacial Lake Outburst Floods (GLOFs) are rare but destructive hazards in high
mountain regions, yet predictive research is hindered by fragmented and
unimodal data. Most prior efforts emphasize post-event mapping, whereas
forecasting requires harmonized datasets that combine visual indicators with
physical precursors. We present GLOFNet, a multimodal dataset for GLOF
monitoring and prediction, focused on the Shisper Glacier in the Karakoram. It
integrates three complementary sources: Sentinel-2 multispectral imagery for
spatial monitoring, NASA ITS_LIVE velocity products for glacier kinematics, and
MODIS Land Surface Temperature records spanning over two decades. Preprocessing
included cloud masking, quality filtering, normalization, temporal
interpolation, augmentation, and cyclical encoding, followed by harmonization
across modalities. Exploratory analysis reveals seasonal glacier velocity
cycles, long-term warming of ~0.8 K per decade, and spatial heterogeneity in
cryospheric conditions. The resulting dataset, GLOFNet, is publicly available
to support future research in glacial hazard prediction. By addressing
challenges such as class imbalance, cloud contamination, and coarse resolution,
GLOFNet provides a structured foundation for benchmarking multimodal deep
learning approaches to rare hazard prediction.

</details>


### [167] [Jigsaw3D: Disentangled 3D Style Transfer via Patch Shuffling and Masking](https://arxiv.org/abs/2510.10497)
*Yuteng Ye,Zheng Zhang,Qinchuan Zhang,Di Wang,Youjia Zhang,Wenxiao Zhang,Wei Yang,Yuan Liu*

Main category: cs.CV

TL;DR: Jigsaw3D是一种基于多视角扩散的3D风格迁移管线，它通过“拼图操作”解耦风格与内容，实现快速、视角一致的风格化，并能生成无缝纹理。


<details>
  <summary>Details</summary>
Motivation: 现有的可控3D风格迁移方法存在以下问题：需要大量的逐场景优化；风格与语义内容容易纠缠；难以保持多视角一致性。

Method: Jigsaw3D利用“拼图操作”（对参考图像块进行空间随机打乱和随机遮罩）来抑制物体语义并分离风格统计（颜色、笔触、纹理）。这些风格线索通过参考到视角的交叉注意力集成到多视角扩散模型中，生成与输入网格条件相关的视角一致的风格化渲染。最后，将渲染的风格烘焙到表面以生成无缝纹理。

Result: 在标准3D风格化基准测试中，Jigsaw3D实现了高风格保真度和多视角一致性，同时显著降低了延迟。它还能泛化到遮罩部分参考风格化、多对象场景风格化和可平铺纹理生成。

Conclusion: Jigsaw3D通过创新的“拼图操作”有效解耦3D风格迁移中的风格与内容，显著提高了风格保真度、视角一致性和效率，并展现出强大的泛化能力。

Abstract: Controllable 3D style transfer seeks to restyle a 3D asset so that its
textures match a reference image while preserving the integrity and multi-view
consistency. The prevalent methods either rely on direct reference style token
injection or score-distillation from 2D diffusion models, which incurs heavy
per-scene optimization and often entangles style with semantic content. We
introduce Jigsaw3D, a multi-view diffusion based pipeline that decouples style
from content and enables fast, view-consistent stylization. Our key idea is to
leverage the jigsaw operation - spatial shuffling and random masking of
reference patches - to suppress object semantics and isolate stylistic
statistics (color palettes, strokes, textures). We integrate these style cues
into a multi-view diffusion model via reference-to-view cross-attention,
producing view-consistent stylized renderings conditioned on the input mesh.
The renders are then style-baked onto the surface to yield seamless textures.
Across standard 3D stylization benchmarks, Jigsaw3D achieves high style
fidelity and multi-view consistency with substantially lower latency, and
generalizes to masked partial reference stylization, multi-object scene
styling, and tileable texture generation. Project page is available at:
https://babahui.github.io/jigsaw3D.github.io/

</details>


### [168] [Scalable Face Security Vision Foundation Model for Deepfake, Diffusion, and Spoofing Detection](https://arxiv.org/abs/2510.10663)
*Gaojian Wang,Feng Lin,Tong Wu,Zhisheng Yan,Kui Ren*

Main category: cs.CV

TL;DR: 本文提出了FS-VFM，一个可扩展的自监督预训练框架，用于学习真实人脸图像的鲁棒且可迁移的表征，并通过FS-Adapter实现高效的下游人脸安全任务微调，在多项基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 如何利用大量无标签的真实人脸数据，学习到鲁棒且可迁移的人脸表征，以提高在各种人脸安全任务中的泛化能力。

Method: 提出了FS-VFM自监督预训练框架，引入了融合掩码图像建模（MIM）和实例判别（ID）的“3C”学习目标。具体地，设计了CRFR-P掩码策略以促进区域内一致性和区域间连贯性，并利用自蒸馏机制将MIM与ID耦合以建立局部到全局的对应关系。预训练后，将ViT作为基础模型。为高效迁移，进一步提出了FS-Adapter，一个轻量级即插即用瓶颈，结合了新颖的真实锚点对比目标。

Result: 在11个人脸安全任务公共基准测试中，FS-VFM始终比各种视觉基础模型（VFM）具有更好的泛化能力，甚至超越了SOTA任务特定方法。FS-Adapter在效率和性能之间提供了卓越的权衡。这些任务包括跨数据集深度伪造检测、跨域人脸反欺骗和未知扩散人脸取证。

Conclusion: FS-VFM框架成功学习了真实人脸的鲁棒和可迁移表征，显著提升了各种人脸安全任务的泛化能力，而FS-Adapter则提供了高效的迁移学习解决方案。

Abstract: With abundant, unlabeled real faces, how can we learn robust and transferable
facial representations to boost generalization across various face security
tasks? We make the first attempt and propose FS-VFM, a scalable self-supervised
pre-training framework, to learn fundamental representations of real face
images. We introduce three learning objectives, namely 3C, that synergize
masked image modeling (MIM) and instance discrimination (ID), empowering FS-VFM
to encode both local patterns and global semantics of real faces. Specifically,
we formulate various facial masking strategies for MIM and devise a simple yet
effective CRFR-P masking, which explicitly prompts the model to pursue
meaningful intra-region Consistency and challenging inter-region Coherency. We
present a reliable self-distillation mechanism that seamlessly couples MIM with
ID to establish underlying local-to-global Correspondence. After pre-training,
vanilla vision transformers (ViTs) serve as universal Vision Foundation Models
for downstream Face Security tasks: cross-dataset deepfake detection,
cross-domain face anti-spoofing, and unseen diffusion facial forensics. To
efficiently transfer the pre-trained FS-VFM, we further propose FS-Adapter, a
lightweight plug-and-play bottleneck atop the frozen backbone with a novel
real-anchor contrastive objective. Extensive experiments on 11 public
benchmarks demonstrate that our FS-VFM consistently generalizes better than
diverse VFMs, spanning natural and facial domains, fully, weakly, and
self-supervised paradigms, small, base, and large ViT scales, and even
outperforms SOTA task-specific methods, while FS-Adapter offers an excellent
efficiency-performance trade-off. The code and models are available on
https://fsfm-3c.github.io/fsvfm.html.

</details>


### [169] [Receptive Field Expanded Look-Up Tables for Vision Inference: Advancing from Low-level to High-level Tasks](https://arxiv.org/abs/2510.10522)
*Xi Zhang,Xiaolin Wu*

Main category: cs.CV

TL;DR: 本研究旨在通过引入格向量量化器、不规则扩张卷积和U形级联LUT结构，在固定查找表大小下扩展CNN的感受野，从而提高基于LUT的快速CNN推理的性能、准确性和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有的查找表（LUT）方法在CNN推理中存在卷积核感受野受限的共同缺点，这是由于表大小的组合爆炸造成的，这限制了其性能。

Method: 主要贡献是提出了一种学习最优格向量量化器的新方法，该方法根据数据维度对推理任务的重要性自适应地分配量化分辨率。此外，格向量量化器比当前实践中使用的标量量化器能更准确地近似CNN核。还引入了其他感受野扩展策略，包括不规则扩张卷积和U形级联LUT结构，以在不增加表大小的情况下捕获多级上下文信息。

Result: 这些创新使所提出的方法能够有效地平衡速度、准确性和内存效率，并相对于现有LUT方法展示出显著的改进。

Conclusion: 通过扩展CNN的感受野并保持固定的表大小，本研究提出的方法显著提升了基于LUT的快速CNN推理的性能，同时优化了速度、准确性和内存效率。

Abstract: Recently, several look-up table (LUT) methods were developed to greatly
expedite the inference of CNNs in a classical strategy of trading space for
speed. However, these LUT methods suffer from a common drawback of limited
receptive field of the convolution kernels due to the combinatorial explosion
of table size. This research aims to expand the CNN receptive field with a
fixed table size, thereby enhancing the performance of LUT-driven fast CNN
inference while maintaining the same space complexity. To achieve this goal,
various techniques are proposed. The main contribution is a novel approach of
learning an optimal lattice vector quantizer that adaptively allocates the
quantization resolution across data dimensions based on their significance to
the inference task. In addition, the lattice vector quantizer offers an
inherently more accurate approximation of CNN kernels than scalar quantizer as
used in current practice. Furthermore, we introduce other receptive field
expansion strategies, including irregular dilated convolutions and a U-shaped
cascaded LUT structure, designed to capture multi-level contextual information
without inflating table size. Together, these innovations allow our approach to
effectively balance speed, accuracy, and memory efficiency, demonstrating
significant improvements over existing LUT methods.

</details>


### [170] [VR-Thinker: Boosting Video Reward Models through Thinking-with-Image Reasoning](https://arxiv.org/abs/2510.10518)
*Qunzhong Wang,Jie Liu,Jiajun Liang,Yilei Jiang,Yuanxing Zhang,Jinyuan Chen,Yaozhi Zheng,Xintao Wang,Pengfei Wan,Xiangyu Yue,Jiaheng Liu*

Main category: cs.CV

TL;DR: 本文提出了VideoReward Thinker (VR-Thinker)，一个“以图思考”的多模态奖励模型框架，通过引入视觉推理操作和可配置的视觉记忆窗口，有效解决了现有奖励模型处理长视频时上下文限制和细节丢失问题，显著提升了视频偏好评估的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态奖励模型存在两大局限：1) 视觉输入消耗大量上下文预算，导致帧数减少和细粒度信息丢失；2) 所有视觉信息集中在初始提示中，加剧了链式推理中的幻觉和遗忘问题。

Method: 本文引入了VideoReward Thinker (VR-Thinker)框架，它通过以下方式使奖励模型具备视觉推理能力：1) 引入“以图思考”机制，装备视觉推理操作（如选择帧）和可配置的视觉记忆窗口，使模型能在上下文限制内主动获取和更新视觉证据。2) 采用强化微调流程：a) 使用精选的视觉链式思考数据进行“冷启动”，学习基本推理技能和操作格式；b) 对维度和整体判断都正确的样本进行拒绝采样微调，以增强推理能力；c) 应用群组相对策略优化 (GRPO) 进一步强化推理。

Result: VR-Thinker在开源模型中实现了视频偏好基准测试的最新SOTA准确率，尤其在处理更长视频时表现出色。一个7B的VR-Thinker模型在VideoGen Reward上达到80.5%，在GenAI-Bench上达到82.3%，在MJ-Bench-Video上达到75.6%。

Conclusion: 这些结果验证了“以图思考”多模态奖励建模的有效性和前景，能够显著提高视觉生成模型后训练的推理保真度和可靠性。

Abstract: Recent advancements in multimodal reward models (RMs) have substantially
improved post-training for visual generative models. However, current RMs face
inherent limitations: (1) visual inputs consume large context budgets, forcing
fewer frames and causing loss of fine-grained details; and (2) all visual
information is packed into the initial prompt, exacerbating hallucination and
forgetting during chain-of-thought reasoning. To overcome these issues, we
introduce VideoReward Thinker (VR-Thinker), a thinking-with-image framework
that equips the RM with visual reasoning operations (e.g., select frame) and a
configurable visual memory window. This allows the RM to actively acquire and
update visual evidence within context limits, improving reasoning fidelity and
reliability. We activate visual reasoning via a reinforcement fine-tuning
pipeline: (i) Cold Start with curated visual chain-of-thought data to distill
basic reasoning skills and operation formatting; (ii) select samples whose
per-dimension and overall judgments are all correct, then conduct Rejection
sampling Fine-Tuning on these high-quality traces to further enhance reasoning;
and (iii) apply Group Relative Policy Optimization (GRPO) to strengthen
reasoning. Our approach delivers state-of-the-art accuracy among open-source
models on video preference benchmarks, especially for longer videos: a 7B
VR-Thinker achieves 80.5% on VideoGen Reward, 82.3% on GenAI-Bench, and 75.6%
on MJ-Bench-Video. These results validate the effectiveness and promise of
thinking-with-image multimodal reward modeling.

</details>


### [171] [Image-to-Video Transfer Learning based on Image-Language Foundation Models: A Comprehensive Survey](https://arxiv.org/abs/2510.10671)
*Jinxuan Li,Chaolei Tan,Haoxuan Chen,Jianxin Ma,Jian-Fang Hu,Wei-Shi Zheng,Jianhuang Lai*

Main category: cs.CV

TL;DR: 这篇综述首次全面回顾了图像到视频迁移学习这一新兴领域，该领域利用图像-语言基础模型（ILFM）来解决视频-文本学习中的数据和计算挑战，并系统地分类了迁移策略，分析了其在各种视频任务中的应用，并提出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 图像-语言基础模型（ILFM）在图像-文本任务中取得了显著成功，提供了可迁移的多模态表示。视频-文本研究的进展激发了将这些图像模型扩展到视频领域的兴趣，以缓解从头开始训练视频-语言基础模型所需的大量数据和计算需求。

Method: 本综述首先总结了广泛使用的ILFM及其能力。然后，系统地将现有的图像到视频迁移学习策略分为两类：冻结特征（frozen features）和修改特征（modified features）。接着，详细阐述了这些策略及其在从细粒度到粗粒度视频-文本学习任务中的应用。最后，通过详细的实验分析调查了不同迁移范式在视频理解任务上的效果，并指出了挑战和未来研究方向。

Result: 本综述提供了一个全面且结构化的图像到视频迁移学习概述，包括对ILFM的总结、对迁移策略的系统分类（冻结特征和修改特征），以及这些策略在各种视频-文本任务中的应用细节。此外，还通过实验分析了不同迁移范式的有效性，并识别了当前面临的挑战和有前景的未来研究方向。

Conclusion: 本综述旨在为基于现有ILFM的视频-文本学习建立一个结构化的路线图，并通过提供全面的概述来启发该快速发展领域未来的研究方向。

Abstract: Image-Language Foundation Models (ILFM) have demonstrated remarkable success
in image-text understanding/generation tasks, providing transferable multimodal
representations that generalize across diverse downstream image-based tasks.
The advancement of video-text research has spurred growing interest in
extending image-based models to the video domain. This paradigm, known as
image-to-video transfer learning, succeeds in alleviating the substantial data
and computational requirements associated with training video-language
foundation models from scratch for video-text learning. This survey provides
the first comprehensive review of this emerging field, which begins by
summarizing the widely used ILFM and their capabilities. We then systematically
classify existing image-to-video transfer learning strategies into two
categories: frozen features and modified features, depending on whether the
original representations from ILFM are preserved or undergo modifications.
Building upon the task-specific nature of image-to-video transfer, this survey
methodically elaborates these strategies and details their applications across
a spectrum of video-text learning tasks, ranging from fine-grained (e.g.,
spatio-temporal video grounding) to coarse-grained (e.g., video question
answering). We further present a detailed experimental analysis to investigate
the efficacy of different image-to-video transfer learning paradigms on a range
of downstream video understanding tasks. Finally, we identify prevailing
challenges and highlight promising directions for future research. By offering
a comprehensive and structured overview, this survey aims to establish a
structured roadmap for advancing video-text learning based on existing ILFM,
and to inspire future research directions in this rapidly evolving domain.

</details>


### [172] [DISC-GAN: Disentangling Style and Content for Cluster-Specific Synthetic Underwater Image Generation](https://arxiv.org/abs/2510.10782)
*Sneha Varur,Anirudh R Hanchinamani,Tarun S Bagewadi,Uma Mudenagudi,Chaitra D Desai,Sujata C,Padmashree Desai,Sumit Meharwade*

Main category: cs.CV

TL;DR: 本文提出了一种名为DISC-GAN的新型框架，它结合了风格-内容解耦与特定聚类训练策略，用于生成逼真的水下图像。


<details>
  <summary>Details</summary>
Motivation: 水下图像因颜色衰减和浊度等光学现象而质量受损，这些现象导致了不同水体中独特的风格变化（如色调和雾度）。现有的生成模型难以捕捉多样水下环境的非均匀条件。

Method: 该方法提出DISC-GAN框架，通过K-means聚类将数据集划分为特定风格的域。它使用独立的编码器获取风格和内容的潜在空间，并通过自适应实例归一化（AdaIN）整合这些潜在表示，然后解码生成最终的合成图像。模型在每个风格聚类上独立训练以保留特定域的特征。

Result: DISC-GAN框架展示了最先进的性能，获得了0.9012的结构相似性指数（SSIM）、32.5118 dB的平均峰值信噪比（PSNR）和13.3728的Frechet Inception距离（FID）。

Conclusion: 所提出的DISC-GAN框架通过将风格-内容解耦与特定聚类训练策略相结合，有效解决了水下图像合成的挑战，实现了逼真的图像生成和最先进的性能。

Abstract: In this paper, we propose a novel framework, Disentangled Style-Content GAN
(DISC-GAN), which integrates style-content disentanglement with a
cluster-specific training strategy towards photorealistic underwater image
synthesis. The quality of synthetic underwater images is challenged by optical
due to phenomena such as color attenuation and turbidity. These phenomena are
represented by distinct stylistic variations across different waterbodies, such
as changes in tint and haze. While generative models are well-suited to capture
complex patterns, they often lack the ability to model the non-uniform
conditions of diverse underwater environments. To address these challenges, we
employ K-means clustering to partition a dataset into style-specific domains.
We use separate encoders to get latent spaces for style and content; we further
integrate these latent representations via Adaptive Instance Normalization
(AdaIN) and decode the result to produce the final synthetic image. The model
is trained independently on each style cluster to preserve domain-specific
characteristics. Our framework demonstrates state-of-the-art performance,
obtaining a Structural Similarity Index (SSIM) of 0.9012, an average Peak
Signal-to-Noise Ratio (PSNR) of 32.5118 dB, and a Frechet Inception Distance
(FID) of 13.3728.

</details>


### [173] [Unified Open-World Segmentation with Multi-Modal Prompts](https://arxiv.org/abs/2510.10524)
*Yang Liu,Yufei Yin,Chenchen Jing,Muzhi Zhu,Hao Chen,Yuling Xi,Bo Feng,Hao Wang,Shiyu Li,Chunhua Shen*

Main category: cs.CV

TL;DR: COSINE是一个统一的开放世界分割模型，它结合了开放词汇分割和上下文分割，利用多模态提示（文本和图像），并通过基础模型和SegDecoder实现了显著的性能提升和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇分割和上下文分割方法存在架构差异、学习目标不同和表示学习策略不一的问题，作者旨在开发一个统一的模型来克服这些局限性。

Method: COSINE利用基础模型提取输入图像和多模态提示的表示。然后，一个SegDecoder对齐这些表示，建模它们的交互，并根据输入提示获取不同粒度的分割掩码。

Result: COSINE在开放词汇分割和上下文分割任务中均取得了显著的性能提升。探索性分析表明，视觉和文本提示的协同作用比单模态方法能显著提高泛化能力。

Conclusion: COSINE成功地统一了开放词汇和上下文分割，克服了现有方法的架构差异和学习目标问题，并通过多模态提示的协同作用实现了卓越的性能和泛化能力。

Abstract: In this work, we present COSINE, a unified open-world segmentation model that
consolidates open-vocabulary segmentation and in-context segmentation with
multi-modal prompts (e.g., text and image). COSINE exploits foundation models
to extract representations for an input image and corresponding multi-modal
prompts, and a SegDecoder to align these representations, model their
interaction, and obtain masks specified by input prompts across different
granularities. In this way, COSINE overcomes architectural discrepancies,
divergent learning objectives, and distinct representation learning strategies
of previous pipelines for open-vocabulary segmentation and in-context
segmentation. Comprehensive experiments demonstrate that COSINE has significant
performance improvements in both open-vocabulary and in-context segmentation
tasks. Our exploratory analyses highlight that the synergistic collaboration
between using visual and textual prompts leads to significantly improved
generalization over single-modality approaches.

</details>


### [174] [MSCloudCAM: Cross-Attention with Multi-Scale Context for Multispectral Cloud Segmentation](https://arxiv.org/abs/2510.10802)
*Md Abdullah Al Mazid,Liangdong Deng,Naphtali Rishe*

Main category: cs.CV

TL;DR: MSCloudCAM是一个用于多光谱和多传感器云分割的跨注意力多尺度上下文网络，它能有效分类晴空、薄云、厚云和云影，并在云数据集上表现出最先进的性能和高效率。


<details>
  <summary>Details</summary>
Motivation: 光学卫星图像中的云是环境监测、土地覆盖测绘和气候研究等可靠分析的关键挑战。

Method: MSCloudCAM利用Sentinel-2和Landsat-8数据进行四种语义类别的分类（晴空、薄云、厚云、云影）。它结合了Swin Transformer骨干网络进行分层特征提取，ASPP和PSP多尺度上下文模块增强尺度感知学习，通过跨注意力块实现多传感器和多光谱特征融合，并集成高效通道注意力块（ECAB）和空间注意力模块自适应地细化特征表示。

Result: 在CloudSEN12和L8Biome数据集上的综合实验表明，MSCloudCAM达到了最先进的分割精度，超越了领先的基线架构，同时保持了有竞争力的参数效率和FLOPs。

Conclusion: MSCloudCAM模型具有高效性和实用性，非常适用于大规模地球观测任务和实际应用。

Abstract: Clouds remain a critical challenge in optical satellite imagery, hindering
reliable analysis for environmental monitoring, land cover mapping, and climate
research. To overcome this, we propose MSCloudCAM, a Cross-Attention with
Multi-Scale Context Network tailored for multispectral and multi-sensor cloud
segmentation. Our framework exploits the spectral richness of Sentinel-2
(CloudSEN12) and Landsat-8 (L8Biome) data to classify four semantic categories:
clear sky, thin cloud, thick cloud, and cloud shadow. MSCloudCAM combines a
Swin Transformer backbone for hierarchical feature extraction with multi-scale
context modules ASPP and PSP for enhanced scale-aware learning. A
Cross-Attention block enables effective multisensor and multispectral feature
fusion, while the integration of an Efficient Channel Attention Block (ECAB)
and a Spatial Attention Module adaptively refine feature representations.
Comprehensive experiments on CloudSEN12 and L8Biome demonstrate that MSCloudCAM
delivers state-of-the-art segmentation accuracy, surpassing leading baseline
architectures while maintaining competitive parameter efficiency and FLOPs.
These results underscore the model's effectiveness and practicality, making it
well-suited for large-scale Earth observation tasks and real-world
applications.

</details>


### [175] [MCE: Towards a General Framework for Handling Missing Modalities under Imbalanced Missing Rates](https://arxiv.org/abs/2510.10534)
*Binyu Zhao,Wei Zhang,Zhaonian Zou*

Main category: cs.CV

TL;DR: 本文提出模态能力增强（MCE）框架，通过学习能力增强（LCE）动态平衡模态学习进度，并结合表征能力增强（RCE）提升特征语义和鲁棒性，以解决多模态学习中模态缺失，尤其是不平衡缺失率的挑战。


<details>
  <summary>Details</summary>
Motivation: 多模态学习在处理缺失模态，特别是不平衡缺失率时面临重大挑战。不平衡缺失率会导致缺失率高的模态更新少、学习进度不一致和表征质量下降，形成恶性循环。现有方法通常侧重于全局数据集层面的平衡，而忽视了样本层面模态效用的变化和特征质量下降的根本问题。

Method: 本文提出模态能力增强（MCE）框架，包含两个协同组件：1) 学习能力增强（LCE），引入多级因子动态平衡模态特定的学习进度；2) 表征能力增强（RCE），通过子集预测和跨模态补全任务来提升特征的语义和鲁棒性。

Result: 在四个多模态基准上的综合评估表明，MCE在各种缺失配置下均持续优于现有最先进的方法。

Conclusion: MCE通过动态平衡模态学习进度和提升特征质量，有效解决了多模态学习中模态缺失（特别是不平衡缺失率）所带来的挑战，并显著提高了性能。

Abstract: Multi-modal learning has made significant advances across diverse pattern
recognition applications. However, handling missing modalities, especially
under imbalanced missing rates, remains a major challenge. This imbalance
triggers a vicious cycle: modalities with higher missing rates receive fewer
updates, leading to inconsistent learning progress and representational
degradation that further diminishes their contribution. Existing methods
typically focus on global dataset-level balancing, often overlooking critical
sample-level variations in modality utility and the underlying issue of
degraded feature quality. We propose Modality Capability Enhancement (MCE) to
tackle these limitations. MCE includes two synergistic components: i) Learning
Capability Enhancement (LCE), which introduces multi-level factors to
dynamically balance modality-specific learning progress, and ii) Representation
Capability Enhancement (RCE), which improves feature semantics and robustness
through subset prediction and cross-modal completion tasks. Comprehensive
evaluations on four multi-modal benchmarks show that MCE consistently
outperforms state-of-the-art methods under various missing configurations. The
journal preprint version is now available at
https://doi.org/10.1016/j.patcog.2025.112591. Our code is available at
https://github.com/byzhaoAI/MCE.

</details>


### [176] [MRS-YOLO Railroad Transmission Line Foreign Object Detection Based on Improved YOLO11 and Channel Pruning](https://arxiv.org/abs/2510.10553)
*Siyuan Liu,Junting Lin*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv11的改进算法MRS-YOLO，旨在解决铁路环境下输电线路异物检测中存在的漏检、误检和检测效率低的问题。通过引入多尺度特征融合模块、重校准特征融合金字塔网络和空间通道重建检测头，并结合通道剪枝技术，显著提高了检测精度和效率，并减少了模型参数量和计算量。


<details>
  <summary>Details</summary>
Motivation: 现有铁路环境下输电线路异物检测存在漏检、误检和检测效率低的问题。

Method: 1. 提出多尺度自适应核深度特征融合（MAKDF）模块，并与C3k2模块融合形成C3k2_MAKDF，以增强模型对不同大小和形状异物的特征提取能力。2. 设计新颖的重校准特征融合金字塔网络（RCFPN）作为颈部结构，以增强模型有效整合和利用多级特征的能力。3. 设计基于空间和通道预处理的空间和通道重建检测头（SC_Detect），以提高模型的整体检测性能。4. 采用通道剪枝技术减少改进模型的冗余，大幅降低参数量和GFLOPs，提高检测效率。

Result: MRS-YOLO算法的mAP50和mAP50:95分别提高到94.8%和86.4%，比基线模型分别提高了0.7和2.3个百分点。同时，参数量和GFLOPs分别减少了44.2%和17.5%。

Conclusion: 改进后的MRS-YOLO算法能够更好地应用于铁路输电线路异物检测任务，实现了更高的检测精度和效率。

Abstract: Aiming at the problems of missed detection, false detection and low detection
efficiency in transmission line foreign object detection under railway
environment, we proposed an improved algorithm MRS-YOLO based on YOLO11.
Firstly, a multi-scale Adaptive Kernel Depth Feature Fusion (MAKDF) module is
proposed and fused with the C3k2 module to form C3k2_MAKDF, which enhances the
model's feature extraction capability for foreign objects of different sizes
and shapes. Secondly, a novel Re-calibration Feature Fusion Pyramid Network
(RCFPN) is designed as a neck structure to enhance the model's ability to
integrate and utilize multi-level features effectively. Then, Spatial and
Channel Reconstruction Detect Head (SC_Detect) based on spatial and channel
preprocessing is designed to enhance the model's overall detection performance.
Finally, the channel pruning technique is used to reduce the redundancy of the
improved model, drastically reduce Parameters and Giga Floating Point
Operations Per Second (GFLOPs), and improve the detection efficiency. The
experimental results show that the mAP50 and mAP50:95 of the MRS-YOLO algorithm
proposed in this paper are improved to 94.8% and 86.4%, respectively, which are
0.7 and 2.3 percentage points higher compared to the baseline, while Parameters
and GFLOPs are reduced by 44.2% and 17.5%, respectively. It is demonstrated
that the improved algorithm can be better applied to the task of foreign object
detection in railroad transmission lines.

</details>


### [177] [Layout-Independent License Plate Recognition via Integrated Vision and Language Models](https://arxiv.org/abs/2510.10533)
*Elham Shabaninia,Fatemeh Asadi-zeydabadi,Hossein Nezamabadi-pour*

Main category: cs.CV

TL;DR: 本文提出了一种模式感知的自动车牌识别（ALPR）框架，结合高精度检测网络、基于Transformer的视觉模型和迭代语言建模机制，实现了对多样车牌布局和复杂真实世界条件下的高精度、布局无关识别，并在多个国际数据集上表现出卓越的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有ALPR系统在处理多样化的车牌布局和挑战性的真实世界条件（如噪声、失真和非常规字体）时，其可靠性仍有待提高。研究旨在开发一个能够克服这些限制的框架。

Method: 该系统包含一个现代高精度检测网络，以及一个集成了基于Transformer的视觉模型和迭代语言建模机制的识别阶段。这一统一的识别阶段无缝地执行字符识别和OCR后处理，学习车牌的结构模式和格式规则，而无需依赖明确的启发式修正或手动布局分类。系统联合优化视觉和语言线索，并通过迭代细化提高在噪声、失真和非常规字体下的OCR准确性。

Result: 该系统在多个国际数据集（IR-LPR、UFPR-ALPR、AOLP）上实现了布局无关的识别。实验结果表明，与最近的无分割方法相比，该系统在准确性和鲁棒性方面表现出卓越的性能。

Conclusion: 通过将模式分析嵌入识别阶段，该框架成功地将计算机视觉和语言建模相结合，显著增强了在智能交通和监控应用中的适应性，为ALPR领域提供了新的解决方案。

Abstract: This work presents a pattern-aware framework for automatic license plate
recognition (ALPR), designed to operate reliably across diverse plate layouts
and challenging real-world conditions. The proposed system consists of a
modern, high-precision detection network followed by a recognition stage that
integrates a transformer-based vision model with an iterative language
modelling mechanism. This unified recognition stage performs character
identification and post-OCR refinement in a seamless process, learning the
structural patterns and formatting rules specific to license plates without
relying on explicit heuristic corrections or manual layout classification.
Through this design, the system jointly optimizes visual and linguistic cues,
enables iterative refinement to improve OCR accuracy under noise, distortion,
and unconventional fonts, and achieves layout-independent recognition across
multiple international datasets (IR-LPR, UFPR-ALPR, AOLP). Experimental results
demonstrate superior accuracy and robustness compared to recent
segmentation-free approaches, highlighting how embedding pattern analysis
within the recognition stage bridges computer vision and language modelling for
enhanced adaptability in intelligent transportation and surveillance
applications.

</details>


### [178] [From Detection to Mitigation: Addressing Bias in Deep Learning Models for Chest X-Ray Diagnosis](https://arxiv.org/abs/2510.10822)
*Clemence Mottez,Louisa Fay,Maya Varma,Sophie Ostmeier,Curtis Langlotz*

Main category: cs.CV

TL;DR: 本研究提出并验证了一个针对胸部X光诊断中性别、年龄和种族偏见的检测与缓解框架，通过结合CNN和XGBoost，实现了在保持预测性能的同时，显著提升子群体公平性，并提供了计算成本更低的有效偏见缓解方案。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胸部X光诊断中表现出巨大潜力，但其在不同人口统计群体（如性别、年龄、种族）间的性能差异可能加剧医疗不公平性，因此需要一个全面的偏见检测和缓解框架。

Method: 研究扩展了一个CNN-XGBoost管线以支持多标签分类，并用XGBoost替换CNN的最后一层。该方法在DenseNet-121和ResNet-50等不同骨干网络上进行了验证。同时，与传统的偏见缓解技术（如对抗训练、重加权、数据增强、主动学习）进行了比较。最终，将XGBoost再训练与主动学习相结合以进一步减少偏见。

Result: 用XGBoost替换CNN最后一层能提高子群体的公平性，同时保持或提高整体预测性能。该方法具有模型无关性，在不同骨干网络上表现出相似的强劲性能和公平性。与传统偏见缓解技术相比，该方法以更低的计算成本提供了具有竞争力或更优的偏见减少效果。将XGBoost再训练与主动学习结合，在CheXpert和MIMIC数据集上，无论是在分布内还是分布外，都能在所有人口统计子群体中实现最大的偏见减少。

Conclusion: 本研究提出的方法，特别是将eXtreme Gradient Boosting再训练与主动学习相结合，为在临床放射学中部署公平的深度学习模型提供了一条实用且有效的途径，能够显著减少基于人口统计学信息的偏见。

Abstract: Deep learning models have shown promise in improving diagnostic accuracy from
chest X-rays, but they also risk perpetuating healthcare disparities when
performance varies across demographic groups. In this work, we present a
comprehensive bias detection and mitigation framework targeting sex, age, and
race-based disparities when performing diagnostic tasks with chest X-rays. We
extend a recent CNN-XGBoost pipeline to support multi-label classification and
evaluate its performance across four medical conditions. We show that replacing
the final layer of CNN with an eXtreme Gradient Boosting classifier improves
the fairness of the subgroup while maintaining or improving the overall
predictive performance. To validate its generalizability, we apply the method
to different backbones, namely DenseNet-121 and ResNet-50, and achieve
similarly strong performance and fairness outcomes, confirming its
model-agnostic design. We further compare this lightweight adapter training
method with traditional full-model training bias mitigation techniques,
including adversarial training, reweighting, data augmentation, and active
learning, and find that our approach offers competitive or superior bias
reduction at a fraction of the computational cost. Finally, we show that
combining eXtreme Gradient Boosting retraining with active learning yields the
largest reduction in bias across all demographic subgroups, both in and out of
distribution on the CheXpert and MIMIC datasets, establishing a practical and
effective path toward equitable deep learning deployment in clinical radiology.

</details>


### [179] [Topological Alignment of Shared Vision-Language Embedding Space](https://arxiv.org/abs/2510.10889)
*Junwon You,Dasol Kang,Jae-Hun Jung*

Main category: cs.CV

TL;DR: ToMCLIP提出了一种拓扑感知的框架，通过引入拓扑对齐损失来解决多语言视觉-语言模型（VLMs）中存在的英语偏见问题，从而提高多语言表征的结构一致性、零样本准确性和检索性能。


<details>
  <summary>Details</summary>
Motivation: 对比视觉-语言模型（VLMs）在零样本能力上表现出色，但由于多语言多模态数据有限，其跨模态对齐偏向英语。现有的多语言扩展虽然有所缓解，但仅强制执行实例级对齐，忽略了共享嵌入空间的全局几何结构。

Method: ToMCLIP（Topological Alignment for Multilingual CLIP）引入了一个拓扑感知的框架，通过拓扑保持约束来对齐嵌入空间。该方法应用持久同调（persistent homology）来定义拓扑对齐损失，并利用图稀疏化策略以理论误差界限近似持久图。

Result: 所提出的方法验证了ToMCLIP的有效性，表明它增强了多语言表征的结构一致性，在CIFAR-100上实现了更高的零样本准确性，并在xFlickr&CO上展示了更强的多语言检索性能。

Conclusion: ToMCLIP通过拓扑对齐有效解决了多语言VLMs中的英语偏见问题，并为将拓扑对齐整合到表征学习中提供了一种通用的方法，超越了VLMs的范畴。

Abstract: Contrastive Vision-Language Models (VLMs) have demonstrated strong zero-shot
capabilities. However, their cross-modal alignment remains biased toward
English due to limited multilingual multimodal data. Recent multilingual
extensions have alleviated this gap but enforce instance-level alignment while
neglecting the global geometry of the shared embedding space. We address this
problem by introducing ToMCLIP (Topological Alignment for Multilingual CLIP), a
topology-aware framework aligning embedding spaces with topology-preserving
constraints. The proposed method applies persistent homology to define a
topological alignment loss and approximates persistence diagram with
theoretical error bounds using graph sparsification strategy. This work
validates the proposed approach, showing enhanced structural coherence of
multilingual representations, higher zero-shot accuracy on the CIFAR-100, and
stronger multilingual retrieval performance on the xFlickr&CO. Beyond VLMs, the
proposed approach provides a general method for incorporating topological
alignment into representation learning.

</details>


### [180] [Deep semi-supervised approach based on consistency regularization and similarity learning for weeds classification](https://arxiv.org/abs/2510.10573)
*Farouq Benchallal,Adel Hafiane,Nicolas Ragot,Raphael Canals*

Main category: cs.CV

TL;DR: 该论文提出了一种结合一致性正则化和相似性学习的深度半监督方法，用于解决标记数据稀缺条件下的杂草分类问题，并在DeepWeeds数据集上表现出优于全监督模型的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 杂草分类对于自动化精准农业系统至关重要，但由于杂草与作物相似、种类多样性以及田间条件变化，分类极具挑战性。此外，深度学习方法需要大量标注数据，而数据标注耗时费力，是农业应用中的一大限制。因此，需要一种在标记数据稀缺条件下仍能提供鲁棒、高性能分类的方法。

Method: 研究人员提出了一种深度半监督学习方法，该方法将一致性正则化与相似性学习相结合。他们开发了一种深度自编码器架构来实现这一方法，旨在利用未标记数据来提高分类性能。

Result: 在DeepWeeds数据集上进行的实验以及在噪声条件下的推断表明，该方法与最先进的全监督深度学习模型相比，具有更高的有效性和鲁棒性。此外，消融研究也进一步验证了所提出的联合学习策略。

Conclusion: 所提出的结合一致性正则化和相似性学习的深度半监督方法，在标记数据稀缺的农业场景下，能有效且鲁棒地进行杂草分类，优于传统的全监督模型，为精准农业提供了新的解决方案。

Abstract: Weed species classification represents an important step for the development
of automated targeting systems that allow the adoption of precision agriculture
practices. To reduce costs and yield losses caused by their presence. The
identification of weeds is a challenging problem due to their shared
similarities with crop plants and the variability related to the differences in
terms of their types. Along with the variations in relation to changes in field
conditions. Moreover, to fully benefit from deep learning-based methods, large
fully annotated datasets are needed. This requires time intensive and laborious
process for data labeling, which represents a limitation in agricultural
applications. Hence, for the aim of improving the utilization of the unlabeled
data, regarding conditions of scarcity in terms of the labeled data available
during the learning phase and provide robust and high classification
performance. We propose a deep semi-supervised approach, that combines
consistency regularization with similarity learning. Through our developed deep
auto-encoder architecture, experiments realized on the DeepWeeds dataset and
inference in noisy conditions demonstrated the effectiveness and robustness of
our method in comparison to state-of-the-art fully supervised deep learning
models. Furthermore, we carried out ablation studies for an extended analysis
of our proposed joint learning strategy.

</details>


### [181] [DreamMakeup: Face Makeup Customization using Latent Diffusion Models](https://arxiv.org/abs/2510.10918)
*Geon Yeong Park,Inhwa Han,Serin Yang,Yeobin Hong,Seongmin Jeong,Heechan Jeon,Myeongjin Goh,Sung Won Yi,Jin Nam,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出DreamMakup，一种无需训练的基于扩散模型的化妆定制方法，通过早期停止的DDIM反演和多条件输入，解决了GANs的训练不稳定和定制能力有限问题，实现了卓越的控制性、精确的图像编辑和身份保留。


<details>
  <summary>Details</summary>
Motivation: 全球化妆品市场的指数级增长推动了虚拟化妆模拟技术的发展。然而，现有的基于GANs的方法存在训练不稳定和定制能力有限等显著挑战，因此需要一种能提供卓越可控性和精确真实图像编辑能力的新方法。

Method: DreamMakup是一种新颖的、无需训练的基于扩散模型的化妆定制方法。它利用扩散模型固有的优势，通过早期停止的DDIM反演来保留面部结构和身份，并通过参考图像、特定RGB颜色和文本描述等多种条件输入实现广泛的定制。

Result: DreamMakup在定制能力、颜色匹配、身份保留以及与文本描述或大型语言模型的兼容性方面，相对于现有的基于GANs和近期基于扩散模型的框架，均展现出显著提升，且计算成本可承受。

Conclusion: DreamMakup提供了一种创新的虚拟化妆定制解决方案，通过利用扩散模型的优势，有效克服了传统GANs的局限性，实现了更优越的可控性、定制化和身份保留，且具有良好的计算效率。

Abstract: The exponential growth of the global makeup market has paralleled
advancements in virtual makeup simulation technology. Despite the progress led
by GANs, their application still encounters significant challenges, including
training instability and limited customization capabilities. Addressing these
challenges, we introduce DreamMakup - a novel training-free Diffusion model
based Makeup Customization method, leveraging the inherent advantages of
diffusion models for superior controllability and precise real-image editing.
DreamMakeup employs early-stopped DDIM inversion to preserve the facial
structure and identity while enabling extensive customization through various
conditioning inputs such as reference images, specific RGB colors, and textual
descriptions. Our model demonstrates notable improvements over existing
GAN-based and recent diffusion-based frameworks - improved customization,
color-matching capabilities, identity preservation and compatibility with
textual descriptions or LLMs with affordable computational costs.

</details>


### [182] [UniFlow: A Unified Pixel Flow Tokenizer for Visual Understanding and Generation](https://arxiv.org/abs/2510.10575)
*Zhengrong Yue,Haiyu Zhang,Xiangyu Zeng,Boyu Chen,Chenting Wang,Shaobin Zhuang,Lu Dong,KunPeng Du,Yi Wang,Limin Wang,Yali Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniFlow的通用统一tokenizer，通过分层自适应自蒸馏和轻量级patch-wise像素流解码器，成功解决了现有统一tokenizer在视觉理解和生成之间存在的性能权衡问题，实现了双赢。


<details>
  <summary>Details</summary>
Motivation: 现有统一tokenizer在视觉理解和生成任务中存在显著的性能权衡，这源于高层语义抽象和低层像素重建之间固有的冲突。研究旨在开发一种能够同时兼顾理解和生成的通用统一tokenizer。

Method: 本文提出了UniFlow，一个通过灵活适配任何视觉编码器与简洁重建解码器实现的通用统一tokenizer。具体方法包括：1) 对预训练视觉编码器应用分层自适应自蒸馏，使其既能继承强大的语义特征用于理解，又能灵活适应细粒度细节用于生成。2) 提出一种轻量级patch-wise像素流解码器，通过建模从噪声状态到patch-wise像素域的条件流，高效实现高保真像素重建，并利用语义特征作为解码器的视觉条件，缓解理解和生成之间的训练冲突。3) 采用patch-wise学习策略，简化数据分布，提高训练效率。

Result: UniFlow在跨越7个广泛研究的视觉理解和生成任务的13个挑战性基准测试中取得了双赢的结果。例如，7B的UniFlow-XL在平均理解基准上超越14B的TokenFlow-XL 7.75%，并且在视觉重建和生成方面也取得了有竞争力的结果，分别在rFID上超过UniTok 0.15，在gFID（无指导）上超过0.09。

Conclusion: UniFlow通过创新的分层自适应自蒸馏和patch-wise像素流解码器，成功解决了统一tokenizer在视觉理解和生成之间的性能权衡问题。它能够同时继承强大的语义特征和灵活地适应细粒度细节，从而在广泛的视觉任务中实现卓越的理解和生成能力。

Abstract: Tokenizer is a crucial component for both visual understanding and
generation. To advance toward the ultimate goal of universal modeling, recent
research has focused on developing a unified tokenizer. However, existing
tokenizers face a significant performance trade-off between understanding and
generation, stemming from the inherent conflict between high-level semantic
abstraction and low-level pixel reconstruction. To tackle this challenge, we
propose a generic and unified tokenizer, namely UniFlow, by flexibly adapting
any visual encoder with a concise reconstruction decoder. Specifically, we
introduce layer-wise adaptive self-distillation applied to the well-pretrained
visual encoders, which enables UniFlow to simultaneously inherit the strong
semantic features for visual understanding and flexibly adapt to model
fine-grained details for visual generation. Moreover, we propose a lightweight
patch-wise pixel flow decoder, which efficiently achieves high-fidelity pixel
reconstruction by modeling a conditional flow from the noisy state back to the
patch-wise pixel domain. By leveraging the semantic features as visual
conditions for the decoder, we effectively alleviate the training conflicts
between understanding and generation. Furthermore, the patch-wise learning
strategy simplifies the data distribution, thereby improving training
efficiency. Extensive experiments across 13 challenging benchmarks spanning 7
widely studied visual understanding and generation tasks demonstrate that
UniFlow achieves a win-win outcome. For instance, our 7B UniFlow-XL not only
surpasses the 14B TokenFlow-XL by 7.75% on average understanding benchmarks,
but also achieves competitive results in both visual reconstruction and
generation, surpassing UniTok by 0.15 in rFID and 0.09 in gFID (without
guidance), respectively.

</details>


### [183] [Injecting Frame-Event Complementary Fusion into Diffusion for Optical Flow in Challenging Scenes](https://arxiv.org/abs/2510.10577)
*Haonan Wang,Hanyu Zhou,Haoyue Liu,Luxin Yan*

Main category: cs.CV

TL;DR: 本文提出了一种名为Diff-ABFlow的新型光流估计框架，该框架结合了扩散模型和帧-事件外观-边界融合，旨在解决高速和低光照场景下光流估计因运动模糊和光照不足导致视觉特征恶化的问题。


<details>
  <summary>Details</summary>
Motivation: 传统光流估计方法在高速和低光照场景中面临挑战，这些场景会导致运动模糊、光照不足、纹理弱化和噪声放大，进而影响帧相机的外观饱和度和边界完整性。帧相机提供密集的出现饱和度但边界稀疏，而事件相机提供稀疏的出现饱和度但边界密集。现有方法通过特征融合或域适应引入事件来改善边界完整性，但外观特征的恶化仍然严重影响了判别模型和生成模型。

Method: 本文提出Diff-ABFlow框架，利用扩散模型来学习从噪声流到清晰流的映射，这种方法不受恶化视觉特征的影响。同时，该框架融合了帧相机和事件相机的外观与边界信息，以利用它们互补的优势。

Result: 本文提出了一种基于扩散模型并结合帧-事件外观-边界融合的新型光流估计框架Diff-ABFlow，以应对恶劣场景下的光流估计挑战。

Conclusion: 通过引入扩散模型和帧-事件外观-边界融合，Diff-ABFlow框架能够有效解决高速和低光照场景下因视觉特征恶化导致的光流估计问题，为鲁棒光流估计提供了一种新颖的解决方案。

Abstract: Optical flow estimation has achieved promising results in conventional scenes
but faces challenges in high-speed and low-light scenes, which suffer from
motion blur and insufficient illumination. These conditions lead to weakened
texture and amplified noise and deteriorate the appearance saturation and
boundary completeness of frame cameras, which are necessary for motion feature
matching. In degraded scenes, the frame camera provides dense appearance
saturation but sparse boundary completeness due to its long imaging time and
low dynamic range. In contrast, the event camera offers sparse appearance
saturation, while its short imaging time and high dynamic range gives rise to
dense boundary completeness. Traditionally, existing methods utilize feature
fusion or domain adaptation to introduce event to improve boundary
completeness. However, the appearance features are still deteriorated, which
severely affects the mostly adopted discriminative models that learn the
mapping from visual features to motion fields and generative models that
generate motion fields based on given visual features. So we introduce
diffusion models that learn the mapping from noising flow to clear flow, which
is not affected by the deteriorated visual features. Therefore, we propose a
novel optical flow estimation framework Diff-ABFlow based on diffusion models
with frame-event appearance-boundary fusion.

</details>


### [184] [FG-CLIP 2: A Bilingual Fine-grained Vision-Language Alignment Model](https://arxiv.org/abs/2510.10921)
*Chunyu Xie,Bin Wang,Fanjing Kong,Jincheng Li,Dawei Liang,Ji Ao,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: 本文介绍了FG-CLIP 2，一个双语（英汉）视觉-语言模型，旨在通过精细粒度监督、多判别目标和文本内模态对比损失，提升精细粒度对齐能力，并在双语任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型（如CLIP）在全局对齐上表现良好，但在捕获对象属性、空间关系和语言表达的精细粒度细节方面存在局限，尤其是在非英语环境下，且双语支持有限。需要一个能处理英汉精细粒度对齐的模型。

Method: 引入FG-CLIP 2模型，通过结合区域-文本匹配和长文本建模等丰富的精细粒度监督，以及多个判别目标进行训练。此外，提出了文本内模态对比（TIC）损失以更好地区分语义相似的文本。模型在精心策划的大规模英汉混合数据集上进行训练，并发布了一个新的中文多模态理解基准进行评估。

Result: FG-CLIP 2在8个任务的29个数据集上进行了广泛实验，结果表明它超越了现有方法，在英语和中文两种语言中均取得了最先进的（SOTA）性能。

Conclusion: FG-CLIP 2成功地推动了双语精细粒度视觉-语言对齐的进展，并在英汉两种语言中实现了SOTA性能。作者发布了模型、代码和基准，以促进该领域的未来研究。

Abstract: Fine-grained vision-language understanding requires precise alignment between
visual content and linguistic descriptions, a capability that remains limited
in current models, particularly in non-English settings. While models like CLIP
perform well on global alignment, they often struggle to capture fine-grained
details in object attributes, spatial relations, and linguistic expressions,
with limited support for bilingual comprehension. To address these challenges,
we introduce FG-CLIP 2, a bilingual vision-language model designed to advance
fine-grained alignment for both English and Chinese. Our approach leverages
rich fine-grained supervision, including region-text matching and long-caption
modeling, alongside multiple discriminative objectives. We further introduce
the Textual Intra-modal Contrastive (TIC) loss to better distinguish
semantically similar captions. Trained on a carefully curated mixture of
large-scale English and Chinese data, FG-CLIP 2 achieves powerful bilingual
performance. To enable rigorous evaluation, we present a new benchmark for
Chinese multimodal understanding, featuring long-caption retrieval and bounding
box classification. Extensive experiments on 29 datasets across 8 tasks show
that FG-CLIP 2 outperforms existing methods, achieving state-of-the-art results
in both languages. We release the model, code, and benchmark to facilitate
future research on bilingual fine-grained alignment.

</details>


### [185] [Equipping Vision Foundation Model with Mixture of Experts for Out-of-Distribution Detection](https://arxiv.org/abs/2510.10584)
*Shizhen Zhao,Jiahui Liu,Xin Wen,Haoru Tan,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 本文系统性地研究了预训练视觉基础模型在OOD检测中的应用，发现DINOv2表现优异但在大语义空间下仍有不足。为此，提出了特征专家混合（MoFE）模块和动态-$eta$ Mixup策略，显著提升了OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练视觉基础模型在学习判别性和泛化性特征方面能力强大，但它们对OOD检测任务的影响尚未得到充分探索。此外，这些模型在大语义空间场景下表现不佳，需要进一步改进。

Method: 本文首先系统性地研究了代表性的视觉基础模型在OOD检测中的表现。为解决大语义空间下的性能不足，提出了以下方法：1. 特征专家混合（MoFE）模块，将特征划分为子空间以捕获复杂数据分布并优化决策边界。2. 动态-$eta$ Mixup策略，从动态Beta分布中采样插值权重，以适应不同类别的学习难度，改善对挑战性类别的特征学习。

Result: 研究发现，预训练的DINOv2模型即使未经域内数据微调，也能为OOD检测提供高度判别性的特征空间，性能可与现有最先进方法媲美。然而，视觉基础模型在大语义空间场景下的性能仍不尽如人意。所提出的MoFE模块和动态-$eta$ Mixup策略显著优于基线方法，有效提升了OOD检测性能。

Conclusion: 预训练视觉基础模型在OOD检测中具有巨大潜力，特别是DINOv2。但在大语义空间下，仍需特定策略来克服复杂性。通过MoFE模块和动态-$eta$ Mixup策略，可以有效提升视觉基础模型在OOD检测任务中的性能，尤其是在处理挑战性类别时。

Abstract: Pre-trained vision foundation models have transformed many computer vision
tasks. Despite their strong ability to learn discriminative and generalizable
features crucial for out-of-distribution (OOD) detection, their impact on this
task remains underexplored. Motivated by this gap, we systematically
investigate representative vision foundation models for OOD detection. Our
findings reveal that a pre-trained DINOv2 model, even without fine-tuning on
in-domain (ID) data, naturally provides a highly discriminative feature space
for OOD detection, achieving performance comparable to existing
state-of-the-art methods without requiring complex designs. Beyond this, we
explore how fine-tuning foundation models on in-domain (ID) data can enhance
OOD detection. However, we observe that the performance of vision foundation
models remains unsatisfactory in scenarios with a large semantic space. This is
due to the increased complexity of decision boundaries as the number of
categories grows, which complicates the optimization process. To mitigate this,
we propose the Mixture of Feature Experts (MoFE) module, which partitions
features into subspaces, effectively capturing complex data distributions and
refining decision boundaries. Further, we introduce a Dynamic-$\beta$ Mixup
strategy, which samples interpolation weights from a dynamic beta distribution.
This adapts to varying levels of learning difficulty across categories,
improving feature learning for more challenging categories. Extensive
experiments demonstrate the effectiveness of our approach, significantly
outperforming baseline methods.

</details>


### [186] [A Simple and Better Baseline for Visual Grounding](https://arxiv.org/abs/2510.10587)
*Jingchao Wang,Wenlong Zhang,Dingjiang Huang,Hong Wang,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为FSVG的视觉定位基线方法，通过直接整合语言和视觉模态，并引入基于相似度的特征选择机制，实现了在准确性和效率之间更好的平衡，解决了现有迭代方法的计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉定位方法，尤其是在选择语言相关视觉区域以减少计算开销的研究中，通常采用迭代方式，并在每次迭代中缓存语言和视觉特征，这会带来额外的计算开销，不利于实现和部署。

Method: 本文提出FSVG（Feature Selection-based Visual Grounding）方法。具体而言，它直接将语言和视觉模态封装到一个整体网络架构中，避免了复杂的迭代过程。同时，并行利用语言作为指导，促进语言模态和视觉模态之间的交互，以提取有效的视觉特征。此外，在视觉特征学习过程中引入了基于相似度的特征选择机制，仅利用与语言相关的视觉特征进行预测，从而降低了计算成本。

Result: 在多个基准数据集上进行的广泛实验全面证实，所提出的FSVG在准确性和效率之间取得了比现有最先进方法更好的平衡。

Conclusion: FSVG是一种简单而有效的视觉定位基线，通过非迭代的模态整合和基于相似度的特征选择，成功解决了现有方法的计算开销问题，并在准确性和效率之间实现了更好的权衡。

Abstract: Visual grounding aims to predict the locations of target objects specified by
textual descriptions. For this task with linguistic and visual modalities,
there is a latest research line that focuses on only selecting the
linguistic-relevant visual regions for object localization to reduce the
computational overhead. Albeit achieving impressive performance, it is
iteratively performed on different image scales, and at every iteration,
linguistic features and visual features need to be stored in a cache, incurring
extra overhead. To facilitate the implementation, in this paper, we propose a
feature selection-based simple yet effective baseline for visual grounding,
called FSVG. Specifically, we directly encapsulate the linguistic and visual
modalities into an overall network architecture without complicated iterative
procedures, and utilize the language in parallel as guidance to facilitate the
interaction between linguistic modal and visual modal for extracting effective
visual features. Furthermore, to reduce the computational cost, during the
visual feature learning, we introduce a similarity-based feature selection
mechanism to only exploit language-related visual features for faster
prediction. Extensive experiments conducted on several benchmark datasets
comprehensively substantiate that the proposed FSVG achieves a better balance
between accuracy and efficiency beyond the current state-of-the-art methods.
Code is available at https://github.com/jcwang0602/FSVG.

</details>


### [187] [ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models](https://arxiv.org/abs/2510.10606)
*Yuqi Liu,Liangyu Chen,Jiazhen Liu,Mingkang Zhu,Zhisheng Zhong,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: ViSurf是一种统一的后训练范式，它将监督微调（SFT）和基于可验证奖励的强化学习（RLVR）的优势整合到单一阶段，以解决现有LVLM训练方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的LVLM后训练范式存在局限性：SFT常导致次优性能，而RLVR在超出模型内部知识的任务上表现不佳。这促使研究者寻求一种能结合两者优势的新方法。

Method: 本文提出了ViSurf（Visual Supervised-and-Reinforcement Fine-Tuning），通过将SFT和RLVR目标进行统一推导，形成ViSurf目标。其核心是在RLVR的rollouts中注入真实标签，同时提供外部监督和内部强化。此外，还引入了三种新颖的奖励控制策略来稳定和优化训练过程。

Result: ViSurf在多个不同基准测试中表现出卓越的有效性，其性能优于单独的SFT、RLVR以及两阶段的SFT → RLVR方法。

Conclusion: ViSurf作为一种统一的后训练范式，通过整合SFT和RLVR的优势，显著提升了LVLM的性能，并通过深入分析验证了其推导和设计原则。

Abstract: Typical post-training paradigms for Large Vision-and-Language Models (LVLMs)
include Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable
Rewards (RLVR). SFT leverages external guidance to inject new knowledge,
whereas RLVR utilizes internal reinforcement to enhance reasoning capabilities
and overall performance. However, our analysis reveals that SFT often leads to
sub-optimal performance, while RLVR struggles with tasks that exceed the
model's internal knowledge base. To address these limitations, we propose
ViSurf (\textbf{Vi}sual \textbf{Su}pervised-and-\textbf{R}einforcement
\textbf{F}ine-Tuning), a unified post-training paradigm that integrates the
strengths of both SFT and RLVR within a single stage. We analyze the derivation
of the SFT and RLVR objectives to establish the ViSurf objective, providing a
unified perspective on these two paradigms. The core of ViSurf involves
injecting ground-truth labels into the RLVR rollouts, thereby providing
simultaneous external supervision and internal reinforcement. Furthermore, we
introduce three novel reward control strategies to stabilize and optimize the
training process. Extensive experiments across several diverse benchmarks
demonstrate the effectiveness of ViSurf, outperforming both individual SFT,
RLVR, and two-stage SFT \textrightarrow RLVR. In-depth analysis corroborates
these findings, validating the derivation and design principles of ViSurf.

</details>


### [188] [OmniQuality-R: Advancing Reward Models Through All-Encompassing Quality Assessment](https://arxiv.org/abs/2510.10609)
*Yiting Lu,Fengbin Guan,Yixin Gao,Yan Zhong,Xinge Peng,Jiakang Yuan,Yihao Liu,Bo Zhang,Xin Li,Zhibo Chen,Weisi Lin*

Main category: cs.CV

TL;DR: OmniQuality-R是一个统一的奖励建模框架，旨在将多任务视觉质量评估转化为连续且可解释的奖励信号，以优化策略。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉评估方法通常局限于单一任务，缺乏一个能够处理多任务质量推理的统一框架。

Method: 本文提出了OmniQuality-R框架，灵感来源于主观实验中的任务特定评估原则。它通过拒绝采样构建了一个推理增强型奖励建模数据集（CoT数据集）用于SFT。随后，应用Group Relative Policy Optimization (GRPO) 进行后训练，并使用基于高斯的奖励进行连续分数预测。为稳定训练和提高泛化能力，在强化学习过程中加入了标准差（STD）过滤和熵门控机制。

Result: OmniQuality-R在美学质量评估、技术质量评估和文本-图像对齐这三个关键的图像质量评估（IQA）任务上进行了评估。

Conclusion: OmniQuality-R提供了一个统一、可解释的框架，能够将多任务质量推理转化为连续的奖励信号，有效优化策略，并提高了训练的稳定性和泛化能力。

Abstract: Current visual evaluation approaches are typically constrained to a single
task. To address this, we propose OmniQuality-R, a unified reward modeling
framework that transforms multi-task quality reasoning into continuous and
interpretable reward signals for policy optimization. Inspired by subjective
experiments, where participants are given task-specific instructions outlining
distinct assessment principles prior to evaluation, we propose OmniQuality-R, a
structured reward modeling framework that transforms multi-dimensional
reasoning into continuous and interpretable reward signals. To enable this, we
construct a reasoning-enhanced reward modeling dataset by sampling informative
plan-reason trajectories via rejection sampling, forming a reliable
chain-of-thought (CoT) dataset for supervised fine-tuning (SFT). Building on
this, we apply Group Relative Policy Optimization (GRPO) for post-training,
using a Gaussian-based reward to support continuous score prediction. To
further stabilize the training and improve downstream generalization, we
incorporate standard deviation (STD) filtering and entropy gating mechanisms
during reinforcement learning. These techniques suppress unstable updates and
reduce variance in policy optimization. We evaluate OmniQuality-R on three key
IQA tasks: aesthetic quality assessment, technical quality evaluation, and
text-image alignment.

</details>


### [189] [GraphTARIF: Linear Graph Transformer with Augmented Rank and Improved Focus](https://arxiv.org/abs/2510.10631)
*Zhaolin Hu,Kun Li,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 本文提出一种混合框架，通过引入门控局部图网络分支增加注意力秩，并使用可学习对数幂函数锐化注意力焦点，从而提升图Transformer中线性注意力的表达能力，同时保持其可扩展性。


<details>
  <summary>Details</summary>
Motivation: 图Transformer中的线性注意力机制虽然高效，但因低秩投影和过度均匀的注意力分布而导致表达能力显著下降，从而限制了节点表示的类别可分离性和模型的分类能力。

Method: 提出一种混合框架：1. 通过在值矩阵上附加一个门控局部图网络分支来增加注意力图的秩。2. 引入一个可学习的对数幂函数到注意力分数中，以降低熵并锐化焦点，从而减轻线性注意力固有的过度平滑效应。理论上证明这些方法能提高学习嵌入的可分离性。

Result: 在同质图和异质图基准测试上，本文方法实现了有竞争力的性能，同时保留了线性注意力的可扩展性。

Conclusion: 通过增强注意力秩和焦点，所提出的混合框架有效解决了线性注意力机制的表达能力限制，在保持效率的同时提高了图Transformer的性能。

Abstract: Linear attention mechanisms have emerged as efficient alternatives to full
self-attention in Graph Transformers, offering linear time complexity. However,
existing linear attention models often suffer from a significant drop in
expressiveness due to low-rank projection structures and overly uniform
attention distributions. We theoretically prove that these properties reduce
the class separability of node representations, limiting the model's
classification ability. To address this, we propose a novel hybrid framework
that enhances both the rank and focus of attention. Specifically, we enhance
linear attention by attaching a gated local graph network branch to the value
matrix, thereby increasing the rank of the resulting attention map.
Furthermore, to alleviate the excessive smoothing effect inherent in linear
attention, we introduce a learnable log-power function into the attention
scores to reduce entropy and sharpen focus. We theoretically show that this
function decreases entropy in the attention distribution, enhancing the
separability of learned embeddings. Extensive experiments on both homophilic
and heterophilic graph benchmarks demonstrate that our method achieves
competitive performance while preserving the scalability of linear attention.

</details>


### [190] [A Survey on Agentic Multimodal Large Language Models](https://arxiv.org/abs/2510.10991)
*Huanjin Yao,Ruifei Zhang,Jiaxing Huang,Jingyi Zhang,Yibo Wang,Bo Fang,Ruolin Zhu,Yongcheng Jing,Shunyu Liu,Guanbin Li,Dacheng Tao*

Main category: cs.CV

TL;DR: 这篇综述全面探讨了具身多模态大语言模型（Agentic MLLMs）这一新兴范式，提出了一个三维概念框架，并整理了开源资源、应用及未来研究方向，以应对AI从静态向动态、通用化发展的趋势。


<details>
  <summary>Details</summary>
Motivation: 随着革命性自主智能体系统的出现，AI研究正从传统的静态、被动、领域专用型智能体转向更具动态性、主动性和通用性的具身AI。本研究旨在响应具身AI日益增长的兴趣及其迈向通用人工智能（AGI）的潜在轨迹。

Method: 本研究通过以下方式进行：1) 探讨具身多模态大语言模型（Agentic MLLMs）的新兴范式，并区分其与传统MLLM-based智能体的概念基础和特征。2) 建立一个三维概念框架，包括：具身内部智能功能、具身外部工具调用、具身环境交互。3) 汇编用于开发具身MLLMs的开源训练框架、训练和评估数据集。4) 回顾具身MLLMs的下游应用，并概述该领域的未来研究方向。

Result: 本研究提出了一个具身多模态大语言模型（Agentic MLLMs）的全面概念框架，该框架基于三个核心维度：内部智能（推理、反思、记忆）、外部工具调用（扩展问题解决能力）和环境交互（适应动态真实世界场景）。此外，还整理了开源训练框架、训练和评估数据集，并综述了下游应用，为社区提供了宝贵资源。

Conclusion: 具身多模态大语言模型（Agentic MLLMs）代表了AI发展的一个重要方向，具有迈向通用人工智能的巨大潜力。该领域正在迅速演变，未来研究应关注所概述的方向，并且研究者将通过一个公开的代码库持续跟踪其发展。

Abstract: With the recent emergence of revolutionary autonomous agentic systems,
research community is witnessing a significant shift from traditional static,
passive, and domain-specific AI agents toward more dynamic, proactive, and
generalizable agentic AI. Motivated by the growing interest in agentic AI and
its potential trajectory toward AGI, we present a comprehensive survey on
Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we
explore the emerging paradigm of agentic MLLMs, delineating their conceptual
foundations and distinguishing characteristics from conventional MLLM-based
agents. We establish a conceptual framework that organizes agentic MLLMs along
three fundamental dimensions: (i) Agentic internal intelligence functions as
the system's commander, enabling accurate long-horizon planning through
reasoning, reflection, and memory; (ii) Agentic external tool invocation,
whereby models proactively use various external tools to extend their
problem-solving capabilities beyond their intrinsic knowledge; and (iii)
Agentic environment interaction further situates models within virtual or
physical environments, allowing them to take actions, adapt strategies, and
sustain goal-directed behavior in dynamic real-world scenarios. To further
accelerate research in this area for the community, we compile open-source
training frameworks, training and evaluation datasets for developing agentic
MLLMs. Finally, we review the downstream applications of agentic MLLMs and
outline future research directions for this rapidly evolving field. To
continuously track developments in this rapidly evolving field, we will also
actively update a public repository at
https://github.com/HJYao00/Awesome-Agentic-MLLMs.

</details>


### [191] [A Machine Learning Perspective on Automated Driving Corner Cases](https://arxiv.org/abs/2510.10653)
*Sebastian Schmidt,Julius Körner,Stephan Günnemann*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的机器学习方法，通过考虑数据分布来有效识别感知系统中的边缘案例，解决了传统基于示例方法的可扩展性问题，并在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在高风险应用（如自动驾驶）中，安全运行至关重要。传统上，困难场景被归类为边缘案例并单独处理，但这种基于示例的分类方法不可扩展，缺乏数据覆盖视角，并且忽视了机器学习模型对训练数据的泛化能力。

Method: 本文提出了一种新颖的机器学习方法，该方法考虑了底层数据分布。在此新视角的基础上，构建了一个框架，用于在单个样本上有效识别感知任务中的边缘案例。

Result: 研究结果表明，所提出的方法 (i) 在分布视角下统一了现有的基于场景的边缘案例分类法；(ii) 在扩展了现有分布外检测基准的标准基准测试中，边缘案例检测任务表现出强大的性能；(iii) 通过新引入的雾增强型Lost & Found数据集，实现了对组合边缘案例的分析。

Conclusion: 这些结果为边缘案例识别提供了一个原则性的基础，强调了其无需手动规范定义的特点，为高风险应用的安全运行提供了支持。

Abstract: For high-stakes applications, like autonomous driving, a safe operation is
necessary to prevent harm, accidents, and failures. Traditionally, difficult
scenarios have been categorized into corner cases and addressed individually.
However, this example-based categorization is not scalable and lacks a data
coverage perspective, neglecting the generalization to training data of machine
learning models. In our work, we propose a novel machine learning approach that
takes the underlying data distribution into account. Based on our novel
perspective, we present a framework for effective corner case recognition for
perception on individual samples. In our evaluation, we show that our approach
(i) unifies existing scenario-based corner case taxonomies under a
distributional perspective, (ii) achieves strong performance on corner case
detection tasks across standard benchmarks for which we extend established
out-of-distribution detection benchmarks, and (iii) enables analysis of
combined corner cases via a newly introduced fog-augmented Lost & Found
dataset. These results provide a principled basis for corner case recognition,
underlining our manual specification-free definition.

</details>


### [192] [AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes](https://arxiv.org/abs/2510.10670)
*Yu Li,Menghan Xia,Gongye Liu,Jianhong Bai,Xintao Wang,Conglang Zhang,Yuxuan Lin,Ruihang Chu,Pengfei Wan,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种利用预训练文本到视频（T2V）模型进行4D场景视点规划的两阶段范式，通过注入4D场景表示并结合相机外参去噪过程来预测自然视点。


<details>
  <summary>Details</summary>
Motivation: T2V模型在模拟真实世界几何和物理定律方面表现出强大能力，显示了其作为隐式世界模型的潜力。同时，视频本身就包含动态场景及其伴随的自然视点，这启发了作者探索利用视频生成先验进行4D场景视点规划的可行性。

Method: 该方法采用两阶段范式：
1. 通过自适应学习分支将4D场景表示注入预训练T2V模型，生成视觉上嵌入视点的视频。
2. 将视点提取公式化为混合条件引导的相机外参去噪过程，为此在预训练T2V模型上引入一个相机外参扩散分支，并以生成的视频和4D场景作为输入。

Result: 实验结果表明，所提出的方法优于现有竞争对手，并且消融研究验证了其关键技术设计的有效性。

Conclusion: 这项工作在一定程度上证明了视频生成模型在真实世界4D交互方面的潜力。

Abstract: Recent Text-to-Video (T2V) models have demonstrated powerful capability in
visual simulation of real-world geometry and physical laws, indicating its
potential as implicit world models. Inspired by this, we explore the
feasibility of leveraging the video generation prior for viewpoint planning
from given 4D scenes, since videos internally accompany dynamic scenes with
natural viewpoints. To this end, we propose a two-stage paradigm to adapt
pre-trained T2V models for viewpoint prediction, in a compatible manner. First,
we inject the 4D scene representation into the pre-trained T2V model via an
adaptive learning branch, where the 4D scene is viewpoint-agnostic and the
conditional generated video embeds the viewpoints visually. Then, we formulate
viewpoint extraction as a hybrid-condition guided camera extrinsic denoising
process. Specifically, a camera extrinsic diffusion branch is further
introduced onto the pre-trained T2V model, by taking the generated video and 4D
scene as input. Experimental results show the superiority of our proposed
method over existing competitors, and ablation studies validate the
effectiveness of our key technical designs. To some extent, this work proves
the potential of video generation models toward 4D interaction in real world.

</details>


### [193] [Stability Under Scrutiny: Benchmarking Representation Paradigms for Online HD Mapping](https://arxiv.org/abs/2510.10660)
*Hao Shan,Ruikai Li,Han Jiang,Yizhe Fan,Ziyang Yan,Bohan Li,Xiaoshuai Hao,Hao Zhao,Zhiyong Cui,Yilong Ren,Haiyang Yu*

Main category: cs.CV

TL;DR: 本文首次提出了一个全面的在线高清地图时间稳定性评估基准，包含新颖的多维度稳定性指标（mAS），并发现稳定性与精度是独立的性能维度，强调其对自动驾驶系统可靠性的重要性。


<details>
  <summary>Details</summary>
Motivation: 在线高清地图在自动驾驶中至关重要，但车载传感器位移会导致地图结果不稳定，这给下游任务带来挑战。现有模型主要关注映射精度，而缺乏对时间稳定性的系统性研究。

Method: 提出首个评估在线高清地图模型时间稳定性的综合基准。开发了一个多维度稳定性评估框架，包含新颖的“存在稳定性”、“定位稳定性”和“形状稳定性”指标，并整合为统一的平均稳定性（mAS）分数。对42个模型及其变体进行了广泛实验，分析了关键模型设计选择对精度和稳定性的影响。

Result: 实验表明，精度（mAP）和稳定性（mAS）是两个基本独立的性能维度。研究还分析了影响高精度、高稳定性或两者兼具的模型架构和训练因素。

Conclusion: 时间稳定性应作为与精度并列的核心评估标准，以促进更可靠的自动驾驶系统发展。将发布公共基准以鼓励对稳定性的关注。

Abstract: As one of the fundamental modules in autonomous driving, online
high-definition (HD) maps have attracted significant attention due to their
cost-effectiveness and real-time capabilities. Since vehicles always cruise in
highly dynamic environments, spatial displacement of onboard sensors inevitably
causes shifts in real-time HD mapping results, and such instability poses
fundamental challenges for downstream tasks. However, existing online map
construction models tend to prioritize improving each frame's mapping accuracy,
while the mapping stability has not yet been systematically studied. To fill
this gap, this paper presents the first comprehensive benchmark for evaluating
the temporal stability of online HD mapping models. We propose a
multi-dimensional stability evaluation framework with novel metrics for
Presence, Localization, and Shape Stability, integrated into a unified mean
Average Stability (mAS) score. Extensive experiments on 42 models and variants
show that accuracy (mAP) and stability (mAS) represent largely independent
performance dimensions. We further analyze the impact of key model design
choices on both criteria, identifying architectural and training factors that
contribute to high accuracy, high stability, or both. To encourage broader
focus on stability, we will release a public benchmark. Our work highlights the
importance of treating temporal stability as a core evaluation criterion
alongside accuracy, advancing the development of more reliable autonomous
driving systems. The benchmark toolkit, code, and models will be available at
https://stablehdmap.github.io/.

</details>


### [194] [GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation](https://arxiv.org/abs/2510.11020)
*Shasha Guo,Liang Pang,Xi Wang,Yanling Wang,Huawei Shen,Jing Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种通过生成辅助线文本描述而非直接编辑图像，并结合强化学习框架来增强大型视觉-语言模型（LVLMs）在几何问题中辅助线推理能力的方法。他们开发了GeoVLMath模型和AuxSolidMath数据集，并在相关基准测试中取得了优异表现。


<details>
  <summary>Details</summary>
Motivation: 辅助线对于解决复杂几何问题至关重要，但对LVLMs来说仍是挑战。现有图像编辑模型难以精确渲染几何图形，因此直接编辑图表来绘制辅助线效果不佳。研究旨在利用LVLMs擅长处理文本的优势，通过生成辅助线构造的文本描述来解决这一问题。

Method: 该研究提出一个强化学习框架来增强图表-文本对齐，核心是跨模态奖励机制，用于评估生成的辅助线描述与真实辅助线图表的匹配程度。基于此奖励，他们开发了GeoVLMath，一个专门用于立体几何中辅助线推理的开源LVLM。该方法采用GRPO-based的强化学习阶段，以实现精确的图表-文本对齐。为支持训练，他们构建了一个可扩展的数据创建管道，并开发了AuxSolidMath数据集，包含3,018个真实考试几何问题，配对图表和对齐的文本字段。

Result: 在3B和7B规模下，GeoVLMath在辅助线推理基准测试中，与强大的开源和专有LVLMs相比，取得了具有竞争力且通常更优的性能。

Conclusion: 通过将辅助线构造描述转化为文本形式，并利用强化学习框架和跨模态奖励机制，可以显著提升LVLMs在几何问题中辅助线推理的能力。GeoVLMath模型和AuxSolidMath数据集的开发为此领域提供了有效的解决方案和资源。

Abstract: Auxiliary lines are essential for solving complex geometric problems but
remain challenging for large vision-language models (LVLMs). Rather than
editing diagrams to draw auxiliary lines, which current image editing models
struggle to render with geometric precision, we generate textual descriptions
of auxiliary-line constructions to better align with the representational
strengths of LVLMs. To bridge the gap between textual descriptions and spatial
structure, we propose a reinforcement learning framework that enhances
diagram-text alignment. At the core of our approach is a cross-modal reward
that evaluates how well the generated auxiliary-line description for an
original diagram matches a ground-truth auxiliary-line diagram. Built on this
reward, we present GeoVLMath, an open-source LVLM tailored to auxiliary-line
reasoning in solid geometry. This fine-grained signal drives a GRPO-based RL
stage, yielding precise diagram-text alignment. To support training, we develop
a scalable data creation pipeline and construct AuxSolidMath, a dataset of
3,018 real-exam geometry problems with paired diagrams and aligned textual
fields. At the 3B and 7B scales, GeoVLMath achieves competitive and often
superior performance compared with strong open-source and proprietary LVLMs on
auxiliary-line reasoning benchmarks.

</details>


### [195] [MSM-Seg: A Modality-and-Slice Memory Framework with Category-Agnostic Prompting for Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2510.10679)
*Yuxiang Luo,Qing Xu,Hai Huang,Yuqi Ouyang,Zhen Chen,Wenting Duan*

Main category: cs.CV

TL;DR: MSM-Seg是一个多模态脑肿瘤分割框架，通过双记忆分割范式和类别无关提示，协同整合多模态和切片间信息，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的分割方法忽略了跨模态相关性，并依赖于劳动密集型的类别特定提示，限制了其在实际场景中的应用。

Method: 提出MSM-Seg框架，引入双记忆分割范式。具体包括：1) 设计模态与切片记忆注意力（MSMA）以利用跨模态和切片间关系；2) 提出多尺度类别无关提示编码器（MCP-Encoder）为解码提供肿瘤区域指导；3) 设计模态自适应融合解码器（MF-Decoder）以利用不同模态的互补解码信息。

Result: 在不同的MRI数据集上，MSM-Seg框架在多模态转移瘤和胶质瘤分割方面均优于现有最先进的方法。

Conclusion: MSM-Seg通过有效整合多模态和切片间信息，并采用高效的类别无关提示，解决了现有方法的局限性，提高了脑肿瘤分割的准确性。

Abstract: Multi-modal brain tumor segmentation is critical for clinical diagnosis, and
it requires accurate identification of distinct internal anatomical subregions.
While the recent prompt-based segmentation paradigms enable interactive
experiences for clinicians, existing methods ignore cross-modal correlations
and rely on labor-intensive category-specific prompts, limiting their
applicability in real-world scenarios. To address these issues, we propose a
MSM-Seg framework for multi-modal brain tumor segmentation. The MSM-Seg
introduces a novel dual-memory segmentation paradigm that synergistically
integrates multi-modal and inter-slice information with the efficient
category-agnostic prompt for brain tumor understanding. To this end, we first
devise a modality-and-slice memory attention (MSMA) to exploit the cross-modal
and inter-slice relationships among the input scans. Then, we propose a
multi-scale category-agnostic prompt encoder (MCP-Encoder) to provide tumor
region guidance for decoding. Moreover, we devise a modality-adaptive fusion
decoder (MF-Decoder) that leverages the complementary decoding information
across different modalities to improve segmentation accuracy. Extensive
experiments on different MRI datasets demonstrate that our MSM-Seg framework
outperforms state-of-the-art methods in multi-modal metastases and glioma tumor
segmentation. The code is available at https://github.com/xq141839/MSM-Seg.

</details>


### [196] [Action-Dynamics Modeling and Cross-Temporal Interaction for Online Action Understanding](https://arxiv.org/abs/2510.10682)
*Xinyu Yang,Zheheng Jiang,Feixiang Zhou,Yihang Zhu,Na Lv,Nan Xing,Huiyu Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSM的统一框架，通过压缩冗余信息、建模动作动态和跨时间交互，同时提升了动作检测和动作预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理未剪辑视频时存在大量冗余信息和噪声，并且在动作理解中常忽视智能体意图对动作的影响。

Method: 本文提出了状态特定模型（SSM）框架，包含三个模块：1. 关键状态记忆压缩模块（Critical State-Based Memory Compression），用于压缩帧序列以减少信息冗余；2. 动作模式学习模块（Action Pattern Learning），构建多维边缘的状态-转换图来建模复杂场景下的动作动态，并生成潜在未来线索以表示意图；3. 跨时间交互模块（Cross-Temporal Interaction），通过跨时间交互建模意图与过去及当前信息之间的相互影响，从而优化当前和未来的特征，实现同步的动作检测和预测。

Result: 在EPIC-Kitchens-100、THUMOS'14、TVSeries和新引入的帕金森病小鼠行为（PDMB）数据集上进行的广泛实验表明，所提出的框架性能优于其他最先进的方法。

Conclusion: 研究结果强调了动作动态学习和跨时间交互的重要性，为未来的动作理解研究奠定了基础。

Abstract: Action understanding, encompassing action detection and anticipation, plays a
crucial role in numerous practical applications. However, untrimmed videos are
often characterized by substantial redundant information and noise. Moreover,
in modeling action understanding, the influence of the agent's intention on the
action is often overlooked. Motivated by these issues, we propose a novel
framework called the State-Specific Model (SSM), designed to unify and enhance
both action detection and anticipation tasks. In the proposed framework, the
Critical State-Based Memory Compression module compresses frame sequences into
critical states, reducing information redundancy. The Action Pattern Learning
module constructs a state-transition graph with multi-dimensional edges to
model action dynamics in complex scenarios, on the basis of which potential
future cues can be generated to represent intention. Furthermore, our
Cross-Temporal Interaction module models the mutual influence between
intentions and past as well as current information through cross-temporal
interactions, thereby refining present and future features and ultimately
realizing simultaneous action detection and anticipation. Extensive experiments
on multiple benchmark datasets -- including EPIC-Kitchens-100, THUMOS'14,
TVSeries, and the introduced Parkinson's Disease Mouse Behaviour (PDMB) dataset
-- demonstrate the superior performance of our proposed framework compared to
other state-of-the-art approaches. These results highlight the importance of
action dynamics learning and cross-temporal interactions, laying a foundation
for future action understanding research.

</details>


### [197] [Source-Free Object Detection with Detection Transformer](https://arxiv.org/abs/2510.11090)
*Huizai Yao,Sicheng Zhao,Shuo Lu,Hui Chen,Yangyang Li,Guoping Liu,Tengfei Xing,Chenggang Yan,Jianhua Tao,Guiguang Ding*

Main category: cs.CV

TL;DR: FRANCK是一个专门为DETR模型设计的无源目标检测（SFOD）框架，通过查询中心特征增强和改进的自训练流程，实现从源域到无监督目标域的知识迁移，无需访问源数据。


<details>
  <summary>Details</summary>
Motivation: 大多数现有的无源目标检测（SFOD）方法要么局限于Faster R-CNN等传统目标检测模型，要么是通用解决方案，缺乏针对DETR等新型目标检测架构的定制化适应。

Method: 本文提出了FRANCK框架，包含四个关键组件：(1) 基于目标性得分的样本重加权（OSSR）模块，用于强调未充分识别的区域；(2) 带有匹配记忆库的对比学习（CMMB）模块，用于增强类间对比学习；(3) 不确定性加权查询融合特征蒸馏（UQFD）模块，用于改进特征蒸馏；(4) 带有动态教师更新间隔（DTUI）的改进自训练流程，用于优化伪标签质量。

Result: FRANCK在多个广泛使用的基准测试中取得了最先进的性能，证明了其有效性以及与基于DETR的SFOD模型的兼容性。

Conclusion: FRANCK通过利用其独特组件，有效地将源域预训练的DETR模型适应到目标域，显著增强了模型的鲁棒性和泛化能力。

Abstract: Source-Free Object Detection (SFOD) enables knowledge transfer from a source
domain to an unsupervised target domain for object detection without access to
source data. Most existing SFOD approaches are either confined to conventional
object detection (OD) models like Faster R-CNN or designed as general solutions
without tailored adaptations for novel OD architectures, especially Detection
Transformer (DETR). In this paper, we introduce Feature Reweighting ANd
Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically
designed to perform query-centric feature enhancement for DETRs. FRANCK
comprises four key components: (1) an Objectness Score-based Sample Reweighting
(OSSR) module that computes attention-based objectness scores on multi-scale
encoder feature maps, reweighting the detection loss to emphasize
less-recognized regions; (2) a Contrastive Learning with Matching-based Memory
Bank (CMMB) module that integrates multi-level features into memory banks,
enhancing class-wise contrastive learning; (3) an Uncertainty-weighted
Query-fused Feature Distillation (UQFD) module that improves feature
distillation through prediction quality reweighting and query feature fusion;
and (4) an improved self-training pipeline with a Dynamic Teacher Updating
Interval (DTUI) that optimizes pseudo-label quality. By leveraging these
components, FRANCK effectively adapts a source-pre-trained DETR model to a
target domain with enhanced robustness and generalization. Extensive
experiments on several widely used benchmarks demonstrate that our method
achieves state-of-the-art performance, highlighting its effectiveness and
compatibility with DETR-based SFOD models.

</details>


### [198] [Text-Enhanced Panoptic Symbol Spotting in CAD Drawings](https://arxiv.org/abs/2510.11091)
*Xianlin Liu,Yan Gong,Bohao Li,Jiajing Huang,Bowen Du,Junchen Ye,Liyan Xu*

Main category: cs.CV

TL;DR: 该论文提出了一种全景符号识别框架，通过联合建模几何和文本基元，并利用Transformer和类型感知注意力机制，解决了现有CAD图纸分析方法忽略文本注释和基元间关系的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CAD图纸分析方法主要关注几何基元，但忽略了图纸中丰富的文本注释，并且缺乏对基元间关系的明确建模，导致对整体图纸的理解不全面。这限制了CAD自动化和设计检索等下游应用。

Method: 该框架通过联合建模几何和文本基元来构建统一表示。首先，使用预训练CNN提取视觉特征作为初始表示。然后，采用一个基于Transformer的骨干网络，并通过类型感知注意力机制进行增强，以明确建模不同基元之间不同类型的空间依赖关系。

Result: 在真实世界数据集上的大量实验表明，所提出的方法在涉及文本注释的符号识别任务上优于现有方法，并在应用于复杂CAD图纸时表现出卓越的鲁棒性。

Conclusion: 该研究成功地将文本注释整合到CAD图纸的全景符号识别中，并通过明确建模基元间的空间依赖关系，显著提升了符号识别的准确性和鲁棒性，为CAD自动化等应用奠定了基础。

Abstract: With the widespread adoption of Computer-Aided Design(CAD) drawings in
engineering, architecture, and industrial design, the ability to accurately
interpret and analyze these drawings has become increasingly critical. Among
various subtasks, panoptic symbol spotting plays a vital role in enabling
downstream applications such as CAD automation and design retrieval. Existing
methods primarily focus on geometric primitives within the CAD drawings to
address this task, but they face following major problems: they usually
overlook the rich textual annotations present in CAD drawings and they lack
explicit modeling of relationships among primitives, resulting in
incomprehensive understanding of the holistic drawings. To fill this gap, we
propose a panoptic symbol spotting framework that incorporates textual
annotations. The framework constructs unified representations by jointly
modeling geometric and textual primitives. Then, using visual features extract
by pretrained CNN as the initial representations, a Transformer-based backbone
is employed, enhanced with a type-aware attention mechanism to explicitly model
the different types of spatial dependencies between various primitives.
Extensive experiments on the real-world dataset demonstrate that the proposed
method outperforms existing approaches on symbol spotting tasks involving
textual annotations, and exhibits superior robustness when applied to complex
CAD drawings.

</details>


### [199] [Dynamic Gaussian Splatting from Defocused and Motion-blurred Monocular Videos](https://arxiv.org/abs/2510.10691)
*Xuankai Zhang,Junjin Xiao,Qing Zhang*

Main category: cs.CV

TL;DR: 本文提出一个统一框架，能够从散焦和运动模糊的单目视频中生成高质量的动态高斯泼溅（Gaussian Splatting）渲染，解决了现有方法无法同时处理这两种模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么针对散焦模糊，要么针对运动模糊，无法同时处理。尽管两者都可以通过模糊核卷积建模，但准确估计模糊核的固有难度限制了该方向的进展。

Method: 本文提出通过一个模糊预测网络来估计逐像素可靠的模糊核，该网络利用与模糊相关的场景和相机信息，并受到模糊感知稀疏性约束。此外，引入了动态高斯密集化策略以弥补不完整区域高斯数量的不足，并通过结合未见视图信息来约束场景优化，从而提升新颖视图合成的性能。

Result: 广泛的实验表明，该方法在从散焦和运动模糊的单目视频生成逼真的新颖视图合成方面，优于现有最先进的方法。

Conclusion: 本文成功开发了一个统一框架，能够有效处理散焦和运动模糊的单目视频，实现高质量的动态高斯泼溅渲染，显著提升了新颖视图合成的性能。

Abstract: This paper presents a unified framework that allows high-quality dynamic
Gaussian Splatting from both defocused and motion-blurred monocular videos. Due
to the significant difference between the formation processes of defocus blur
and motion blur, existing methods are tailored for either one of them, lacking
the ability to simultaneously deal with both of them. Although the two can be
jointly modeled as blur kernel-based convolution, the inherent difficulty in
estimating accurate blur kernels greatly limits the progress in this direction.
In this work, we go a step further towards this direction. Particularly, we
propose to estimate per-pixel reliable blur kernels using a blur prediction
network that exploits blur-related scene and camera information and is subject
to a blur-aware sparsity constraint. Besides, we introduce a dynamic Gaussian
densification strategy to mitigate the lack of Gaussians for incomplete
regions, and boost the performance of novel view synthesis by incorporating
unseen view information to constrain scene optimization. Extensive experiments
show that our method outperforms the state-of-the-art methods in generating
photorealistic novel view synthesis from defocused and motion-blurred monocular
videos. Our code and trained model will be made publicly available.

</details>


### [200] [WorldMirror: Universal 3D World Reconstruction with Any-Prior Prompting](https://arxiv.org/abs/2510.10726)
*Yifan Liu,Zhiyuan Min,Zhenwei Wang,Junta Wu,Tengfei Wang,Yixuan Yuan,Yawei Luo,Chunchao Guo*

Main category: cs.CV

TL;DR: WorldMirror是一个一体化、前馈模型，能够灵活整合多种几何先验，并同时生成多种3D几何表示，在多个任务上实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D几何预测方法受限于仅图像输入或定制于特定任务，无法灵活整合多样化的几何先验并同时生成多种3D表示，存在结构模糊性。

Method: WorldMirror采用统一的前馈架构，灵活整合包括相机姿态、内参和深度图在内的多样几何先验信息，并在单次前向传播中同时生成密集点云、多视角深度图、相机参数、表面法线和3D高斯等多种3D表示。

Result: WorldMirror在相机、点图、深度和表面法线估计以及新视角合成等多个基准测试中达到了最先进的性能，同时保持了前馈推理的高效率。

Conclusion: WorldMirror通过其优雅统一的架构和对可用先验信息的有效利用，解决了结构模糊性问题，并能以几何一致的方式生成多种3D输出，在多功能3D几何预测任务中表现出色。

Abstract: We present WorldMirror, an all-in-one, feed-forward model for versatile 3D
geometric prediction tasks. Unlike existing methods constrained to image-only
inputs or customized for a specific task, our framework flexibly integrates
diverse geometric priors, including camera poses, intrinsics, and depth maps,
while simultaneously generating multiple 3D representations: dense point
clouds, multi-view depth maps, camera parameters, surface normals, and 3D
Gaussians. This elegant and unified architecture leverages available prior
information to resolve structural ambiguities and delivers geometrically
consistent 3D outputs in a single forward pass. WorldMirror achieves
state-of-the-art performance across diverse benchmarks from camera, point map,
depth, and surface normal estimation to novel view synthesis, while maintaining
the efficiency of feed-forward inference. Code and models will be publicly
available soon.

</details>


### [201] [video-SALMONN S: Streaming Audio-Visual LLMs Beyond Length Limits via Memory](https://arxiv.org/abs/2510.11129)
*Guangzhi Sun,Yixuan Li,Xiaodong Wu,Yudong Yang,Wei Li,Zejun Ma,Chao Zhang*

Main category: cs.CV

TL;DR: 视频-SALMONN S是一种流式音视频大型语言模型，通过引入测试时训练（TTT）记忆模块和提示依赖的记忆读取器，首次实现了在固定内存预算下处理3小时长视频，并在长视频基准测试中超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 未来的AI代理需要连续、高帧率、高分辨率地处理长视频流，但当前视频理解大型语言模型（LLMs）难以扩展。离线方法需要调整帧率，而流式方法通过合并或丢弃token来限制内存，导致信息丢失。因此，需要一种能在固定内存预算下高效处理长视频流的方法。

Method: 本文提出了视频-SALMONN S模型，它是一个流式音视频LLM，主要包含两项创新：(i) 一个测试时训练（TTT）记忆模块，通过持续更新token表示来捕获长程依赖，取代了token合并；(ii) 一个提示依赖的记忆读取器，能从固定大小的内存中选择性地检索与上下文相关的内容。TTT模块通过无Hessian共轭梯度过程（TTT_HF）进行优化，以实现高效适应。

Result: 视频-SALMONN S是首个在固定内存预算下以1 FPS和360p分辨率处理3小时视频的模型。它在多小时视频（包含1万帧和1百万个token）上保持了高质量的理解能力。在长视频基准测试（Video-MME、LVBench、VideoEvalPro）中，其8B参数模型总体得分达到74.2%，在Video-MME长视频分割上达到67.8%，优于离线和流式基线模型。

Conclusion: 视频-SALMONN S通过创新的TTT记忆模块和提示依赖的记忆读取器，成功解决了在固定内存预算下处理长视频流的挑战，实现了连续、高质量的视频理解，并在长视频基准测试中取得了领先性能。

Abstract: Continuous, high-frame-rate, high-resolution processing of long video streams
is critical for future AI agents, yet current video-understanding LLMs struggle
to scale. Offline, fixed-frame-number methods require the stream length to
adapt frame rates; streaming methods constrain memory by merging or discarding
tokens, losing information. We propose video-SALMONN S, a streaming
audio-visual LLM that, to our knowledge, is the first to process 3-hour videos
at 1 FPS and 360p resolution under a fixed memory budget. Our model introduces
(i) a test-time-training (TTT) memory module that continually updates token
representations to capture long-range dependencies by replacing token merging,
and (ii) a prompt-dependent memory reader that selectively retrieves
context-relevant content from fixed-size memory. The TTT module is optimised
with a Hessian-free conjugate-gradient procedure (TTT_HF) for efficient
adaptation. On long-video benchmarks (Video-MME, LVBench, VideoEvalPro),
video-SALMONN S sustains high-quality understanding on multi-hour videos with
10k frames and 1M tokens. Our 8B-parameter model achieves 74.2% overall and
67.8% on the Video-MME long split, outperforming both offline and streaming
baselines.

</details>


### [202] [Seeing My Future: Predicting Situated Interaction Behavior in Virtual Reality](https://arxiv.org/abs/2510.10742)
*Yuan Xu,Zimu Zhang,Xiaoxuan Ma,Wentao Zhu,Yu Qiao,Yizhou Wang*

Main category: cs.CV

TL;DR: 本文提出一个分层、意图感知的框架，通过建模人类意图和认知机制，预测VR/AR系统中用户的详细情境行为，以实现更智能的交互体验和自适应虚拟环境。


<details>
  <summary>Details</summary>
Motivation: VR/AR系统需要智能地适应用户行为以增强交互体验。这要求准确理解人类意图并预测未来的情境行为（如凝视方向、物体交互），这对于创建响应式VR/AR环境和个性化应用至关重要。然而，准确的行为预测需要对驱动人-环境交互的潜在认知过程进行建模。

Method: 本文引入一个分层、意图感知的框架，通过利用认知机制来建模人类意图并预测详细的情境行为。该框架根据历史人类动态和场景上下文观察，首先识别潜在的交互目标，然后预测精细的未来行为。文中提出了一种动态图卷积网络（GCN）来有效地捕捉人-环境关系。

Result: 在具有挑战性的真实世界基准测试和实时VR环境中进行的广泛实验表明，该方法在所有指标上均表现出卓越的性能。它能够实现主动式VR系统的实际应用，这些系统可以预测用户行为并相应地调整虚拟环境。

Conclusion: 所提出的分层、意图感知的框架通过建模认知机制和利用动态GCN，能够有效预测VR/AR系统中的用户行为，从而实现更智能、自适应的虚拟环境和增强的交互体验。

Abstract: Virtual and augmented reality systems increasingly demand intelligent
adaptation to user behaviors for enhanced interaction experiences. Achieving
this requires accurately understanding human intentions and predicting future
situated behaviors - such as gaze direction and object interactions - which is
vital for creating responsive VR/AR environments and applications like
personalized assistants. However, accurate behavioral prediction demands
modeling the underlying cognitive processes that drive human-environment
interactions. In this work, we introduce a hierarchical, intention-aware
framework that models human intentions and predicts detailed situated behaviors
by leveraging cognitive mechanisms. Given historical human dynamics and the
observation of scene contexts, our framework first identifies potential
interaction targets and forecasts fine-grained future behaviors. We propose a
dynamic Graph Convolutional Network (GCN) to effectively capture
human-environment relationships. Extensive experiments on challenging
real-world benchmarks and live VR environment demonstrate the effectiveness of
our approach, achieving superior performance across all metrics and enabling
practical applications for proactive VR systems that anticipate user behaviors
and adapt virtual environments accordingly.

</details>


### [203] [G2L:From Giga-Scale to Cancer-Specific Large-Scale Pathology Foundation Models via Knowledge Distillation](https://arxiv.org/abs/2510.11176)
*Yesung Cho,Sungmin Lee,Geongyu Lee,Minkyung Lee,Jongbae Park,Dongmyung Shin*

Main category: cs.CV

TL;DR: 本文提出G2L框架，通过知识蒸馏将千亿级病理基础模型的性能高效迁移至参数量更小的大规模模型，使其在癌症特异性任务上达到甚至超越千亿级模型的性能，同时提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 病理学基础模型虽然通过扩大训练数据、多样化癌症类型和增加模型规模来提升性能，但千亿级（giga-scale）模型在开发和部署中面临巨大的计算成本挑战，限制了其实用性。

Method: G2L框架采用知识蒸馏策略，将一个千亿级模型的知识迁移到一个参数量仅为其15%的大规模模型。该过程仅需1K张目标癌症（如乳腺癌、前列腺癌等）的病理切片。

Result: 蒸馏后的大规模模型在多个基准测试中不仅超越了相同规模（大规模）的最新模型，还在某些基准测试中表现优于千亿级教师模型和超大规模模型。此外，蒸馏模型展现出更高的鲁棒性指数，表明对来自不同机构的图像变异具有更强的适应性。

Conclusion: 研究结果表明，所提出的针对大规模模型的蒸馏方法是一种数据和参数高效的方式，可以在没有过高计算负担的情况下，为癌症特异性应用实现千亿级模型水平的性能。

Abstract: Recent studies in pathology foundation models have shown that scaling
training data, diversifying cancer types, and increasing model size
consistently improve their performance. However, giga-scale foundation models,
which are trained on hundreds of thousands of slides covering tens of cancer
types and contain billions of parameters, pose significant challenges for
practical use due to their tremendous computational costs in both development
and deployment. In this work, we present a novel strategy, named the G2L
framework, to increase the performance of large-scale foundation models, which
consist of only $15\%$ of the parameters of giga-scale models, to a comparable
performance level of giga-scale models in cancer-specific tasks. Our approach
applies knowledge distillation, transferring the capabilities of a giga-scale
model to a large-scale model, using just 1K pathology slides of a target cancer
(e.g., breast, prostate, etc.). The resulting distilled model not only
outperformed state-of-the-art models of the same size (i.e., large-scale)
across several benchmarks but also, interestingly, surpassed the giga-scale
teacher and huge-scale models in some benchmarks. In addition, the distilled
model exhibited a higher robustness index, indicating improved resilience to
image variations originating from multiple institutions. These findings suggest
that the proposed distillation approach for a large-scale model is a data- and
parameter-efficient way to achieve giga-scale-level performance for
cancer-specific applications without prohibitive computational burden.

</details>


### [204] [Uncovering Anomalous Events for Marine Environmental Monitoring via Visual Anomaly Detection](https://arxiv.org/abs/2510.10750)
*Laura Weihl,Nejc Novak,Stefan H. Bengtson,Malte Pedersen*

Main category: cs.CV

TL;DR: 该研究探讨了使用基于深度神经网络的视觉异常检测（VAD）来自动识别水下视频中的有趣事件，以解决手动检查大量水下视频的低效问题。它引入了首个多标注者水下VAD基准数据集AURA，评估了四种VAD模型，并发现VAD性能差异显著，且对训练数据量和“正常”场景的视觉内容变化高度敏感。研究强调了鲁棒帧选择策略和软标签/共识标签的重要性。


<details>
  <summary>Details</summary>
Motivation: 水下视频监测是评估海洋生物多样性的有效策略，但大量的无事件录像使得人工检查非常不切实际。

Method: 该研究探索了基于深度神经网络的视觉异常检测（VAD）方法来自动识别异常事件。它引入了AURA，这是第一个用于水下VAD的多标注者基准数据集。研究评估了四种VAD模型在两个海洋场景中的表现，并展示了鲁棒帧选择策略的重要性。此外，它将VAD模型的性能与多位标注者进行了比较。

Result: 当前VAD模型的性能差异巨大，并且对训练数据量以及定义“正常”场景的视觉内容的可变性高度敏感。研究结果强调了鲁棒帧选择策略的重要性，以及软标签和共识标签的价值。

Conclusion: VAD为支持科学探索和可扩展的生物多样性监测提供了一种实用的方法，尽管当前模型的性能存在显著差异并对训练数据和“正常”场景的视觉内容变化高度敏感。

Abstract: Underwater video monitoring is a promising strategy for assessing marine
biodiversity, but the vast volume of uneventful footage makes manual inspection
highly impractical. In this work, we explore the use of visual anomaly
detection (VAD) based on deep neural networks to automatically identify
interesting or anomalous events. We introduce AURA, the first multi-annotator
benchmark dataset for underwater VAD, and evaluate four VAD models across two
marine scenes. We demonstrate the importance of robust frame selection
strategies to extract meaningful video segments. Our comparison against
multiple annotators reveals that VAD performance of current models varies
dramatically and is highly sensitive to both the amount of training data and
the variability in visual content that defines "normal" scenes. Our results
highlight the value of soft and consensus labels and offer a practical approach
for supporting scientific exploration and scalable biodiversity monitoring.

</details>


### [205] [Restricted Receptive Fields for Face Verification](https://arxiv.org/abs/2510.10753)
*Kagan Ozturk,Aman Bhatta,Haiyu Wu,Patrick Flynn,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 本文提出一种可解释的人脸相似度度量方法，通过将全局相似度分解为局部感受野的贡献，提供无需后验分析的局部可加解释，并在验证任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度神经网络的后验解释方法（如像素重要性分配）缺乏可靠的评估指标，导致其对模型实际推理的忠实度存疑。因此，研究动机是设计决策过程本身具有内在可解释性的模型。

Method: 本文提出一种人脸相似度度量方法，将两张人脸图像的相似度定义为补丁（patch）级别相似度分数的总和。这种方法通过限制感受野，将全局相似度分解为局部贡献，从而提供局部可加的解释，且无需依赖后验分析。

Result: 所提出的方法在人脸验证任务中表现出竞争力。即使使用112x112人脸图像中28x28大小的补丁，也能达到竞争性性能；当使用56x56大小的补丁时，其性能超越了现有最先进的方法。

Conclusion: 本文成功开发了一种具有内在可解释性的人脸相似度度量方法，通过局部补丁贡献实现可解释性，并同时在人脸验证任务中取得了优异的性能，解决了传统解释方法忠实度不确定的问题。

Abstract: Understanding how deep neural networks make decisions is crucial for
analyzing their behavior and diagnosing failure cases. In computer vision, a
common approach to improve interpretability is to assign importance to
individual pixels using post-hoc methods. Although they are widely used to
explain black-box models, their fidelity to the model's actual reasoning is
uncertain due to the lack of reliable evaluation metrics. This limitation
motivates an alternative approach, which is to design models whose decision
processes are inherently interpretable. To this end, we propose a face
similarity metric that breaks down global similarity into contributions from
restricted receptive fields. Our method defines the similarity between two face
images as the sum of patch-level similarity scores, providing a locally
additive explanation without relying on post-hoc analysis. We show that the
proposed approach achieves competitive verification performance even with
patches as small as 28x28 within 112x112 face images, and surpasses
state-of-the-art methods when using 56x56 patches.

</details>


### [206] [EGD-YOLO: A Lightweight Multimodal Framework for Robust Drone-Bird Discrimination via Ghost-Enhanced YOLOv8n and EMA Attention under Adverse Condition](https://arxiv.org/abs/2510.10765)
*Sudipto Sarkar,Mohammad Asif Hasan,Khondokar Ashik Shahriar,Fablia Labiba,Nahian Tasnim,Sheikh Anawarul Haq Fattah*

Main category: cs.CV

TL;DR: 本研究提出EGD-YOLOv8n模型，利用RGB和红外图像在VIP CUP 2025数据集上实现轻量级、高效且准确的无人机和鸟类实时检测。


<details>
  <summary>Details</summary>
Motivation: 正确识别无人机和鸟类对于维护天空安全和提升安保系统至关重要。

Method: 研究开发了EGD-YOLOv8n模型，一个轻量级但强大的目标检测模型。它通过智能设计改进和引入注意力层来增强图像特征捕获和理解，同时减少计算量。模型还采用了一个特殊的检测头以适应不同形状和大小的物体。训练了RGB、红外以及结合RGB和红外图像的三种版本，并使用了VIP CUP 2025数据集。

Result: 结合RGB和红外图像的模型版本取得了最佳的准确性和可靠性。该模型运行速度足够快，可在普通GPU上实现实时应用。

Conclusion: EGD-YOLOv8n模型提供了一个强大、高效且可实时运行的解决方案，能准确可靠地检测无人机和鸟类，尤其在结合RGB和红外图像时表现最佳。

Abstract: Identifying drones and birds correctly is essential for keeping the skies
safe and improving security systems. Using the VIP CUP 2025 dataset, which
provides both RGB and infrared (IR) images, this study presents EGD-YOLOv8n, a
new lightweight yet powerful model for object detection. The model improves how
image features are captured and understood, making detection more accurate and
efficient. It uses smart design changes and attention layers to focus on
important details while reducing the amount of computation needed. A special
detection head helps the model adapt to objects of different shapes and sizes.
We trained three versions: one using RGB images, one using IR images, and one
combining both. The combined model achieved the best accuracy and reliability
while running fast enough for real-time use on common GPUs.

</details>


### [207] [LightPneumoNet: Lightweight Pneumonia Classifier](https://arxiv.org/abs/2510.11232)
*Neilansh Chauhan,Piyush Kumar Gupta,Faraz Doja*

Main category: cs.CV

TL;DR: LightPneumoNet是一种轻量级卷积神经网络，专为资源受限环境下的肺炎胸部X射线诊断设计，以极小的内存占用实现了高准确性和近乎完美的敏感性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的环境中，部署大型、计算成本高的深度学习模型进行肺炎诊断面临挑战。

Method: 研究从零开始构建了LightPneumoNet，这是一个高效、轻量级的卷积神经网络。模型在包含5,856张胸部X射线图像的公开数据集上进行训练。预处理包括图像大小调整（224x224）、灰度转换和像素归一化，并采用数据增强（旋转、缩放、剪切）以防止过拟合。该定制架构包含四个堆叠卷积层块，仅有388,082个可训练参数，内存占用为1.48 MB。

Result: 在独立的测试集上，LightPneumoNet表现出色，整体准确率达到0.942，精确度为0.92，F1-Score为0.96。尤其重要的是，其敏感性（召回率）高达0.99，表明其识别真实肺炎病例的能力接近完美，并最大限度地减少了临床上重要的假阴性。值得注意的是，LightPneumoNet在相同数据集上实现了高召回率，而现有方法通常需要更重的架构或未能达到可比的敏感性水平。

Conclusion: LightPneumoNet的高效率使其能够部署在低成本硬件上，从而使先进的计算机辅助诊断在服务不足的诊所中普及，并作为可靠的第二意见工具，改善患者预后。

Abstract: Effective pneumonia diagnosis is often challenged by the difficulty of
deploying large, computationally expensive deep learning models in
resource-limited settings. This study introduces LightPneumoNet, an efficient,
lightweight convolutional neural network (CNN) built from scratch to provide an
accessible and accurate diagnostic solution for pneumonia detection from chest
X-rays. Our model was trained on a public dataset of 5,856 chest X-ray images.
Preprocessing included image resizing to 224x224, grayscale conversion, and
pixel normalization, with data augmentation (rotation, zoom, shear) to prevent
overfitting. The custom architecture features four blocks of stacked
convolutional layers and contains only 388,082 trainable parameters, resulting
in a minimal 1.48 MB memory footprint. On the independent test set, our model
delivered exceptional performance, achieving an overall accuracy of 0.942,
precision of 0.92, and an F1-Score of 0.96. Critically, it obtained a
sensitivity (recall) of 0.99, demonstrating a near-perfect ability to identify
true pneumonia cases and minimize clinically significant false negatives.
Notably, LightPneumoNet achieves this high recall on the same dataset where
existing approaches typically require significantly heavier architectures or
fail to reach comparable sensitivity levels. The model's efficiency enables
deployment on low-cost hardware, making advanced computer-aided diagnosis
accessible in underserved clinics and serving as a reliable second-opinion tool
to improve patient outcomes.

</details>


### [208] [Structured Spectral Graph Learning for Multi-label Abnormality Classification in 3D Chest CT Scans](https://arxiv.org/abs/2510.10779)
*Theo Di Piazza,Carole Lazarus,Olivier Nempont,Loic Boussel*

Main category: cs.CV

TL;DR: 本文提出了一种2.5D图基框架，通过将3D CT体积表示为结构化图，并使用谱图卷积处理轴向切片三联体作为节点，以实现胸部CT扫描的多标签分类，解决了现有方法在长距离依赖和预训练数据方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着CT检查量的增加，对自动化工具（如器官分割、异常检测、报告生成）的需求日益增长。3D胸部CT扫描的多标签分类是一个关键但具有挑战性的问题，因为体积数据中固有的复杂空间关系和异常的广泛变异性。现有的3D卷积神经网络难以捕获长距离依赖，而Vision Transformers通常需要大量领域特定数据集进行预训练才能达到竞争力。

Method: 我们提出了一种2.5D替代方案，引入了一个新的图基框架。该框架将3D CT体积表示为结构化图，其中轴向切片三联体作为节点，通过谱图卷积进行处理，从而使模型能够推理切片间依赖关系，同时保持与临床部署兼容的复杂性。我们还在聚合策略、边缘加权方案和图连接模式方面进行了全面的消融研究，并通过自动化放射学报告生成和腹部CT数据上的迁移实验，展示了该方法的广泛适用性。

Result: 我们的方法在来自独立机构的3个数据集上进行训练和评估，实现了强大的跨数据集泛化能力，并与最先进的视觉编码器相比展现出有竞争力的性能。此外，我们通过迁移实验证明了该方法在自动化放射学报告生成和腹部CT数据上的广泛适用性。

Conclusion: 本文提出的2.5D图基框架有效地解决了3D胸部CT扫描多标签分类的挑战，通过捕获切片间依赖关系，实现了强大的泛化能力和有竞争力的性能，并具有广泛的适用性，为临床工作负载管理提供了有潜力的自动化工具。

Abstract: With the growing volume of CT examinations, there is an increasing demand for
automated tools such as organ segmentation, abnormality detection, and report
generation to support radiologists in managing their clinical workload.
Multi-label classification of 3D Chest CT scans remains a critical yet
challenging problem due to the complex spatial relationships inherent in
volumetric data and the wide variability of abnormalities. Existing methods
based on 3D convolutional neural networks struggle to capture long-range
dependencies, while Vision Transformers often require extensive pre-training on
large-scale, domain-specific datasets to perform competitively. In this work,
we propose a 2.5D alternative by introducing a new graph-based framework that
represents 3D CT volumes as structured graphs, where axial slice triplets serve
as nodes processed through spectral graph convolution, enabling the model to
reason over inter-slice dependencies while maintaining complexity compatible
with clinical deployment. Our method, trained and evaluated on 3 datasets from
independent institutions, achieves strong cross-dataset generalization, and
shows competitive performance compared to state-of-the-art visual encoders. We
further conduct comprehensive ablation studies to evaluate the impact of
various aggregation strategies, edge-weighting schemes, and graph connectivity
patterns. Additionally, we demonstrate the broader applicability of our
approach through transfer experiments on automated radiology report generation
and abdominal CT data.\\ This work extends our previous contribution presented
at the MICCAI 2025 EMERGE Workshop.

</details>


### [209] [ImHead: A Large-scale Implicit Morphable Model for Localized Head Modeling](https://arxiv.org/abs/2510.10793)
*Rolandos Alexandros Potamias,Stathis Galanakis,Jiankang Deng,Athanasios Papaioannou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: imHead是一种新型隐式3DMM，能够建模富有表现力的3D头部化身，并支持面部特征的局部编辑，克服了传统3DMM的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的3D可变形模型（3DMMs）由于严格的拓扑结构和线性特性，难以表示复杂的完整头部形状。研究旨在开发一种能够建模富有表现力的3D头部化身并支持局部编辑的方法。

Method: 本文提出了imHead，一种新型隐式3DMM。与以往将潜在空间直接划分为局部组件的方法不同，imHead保留了一个紧凑的单一身份空间，并引入了中间区域特定的潜在表示来实现局部编辑。为了训练imHead，研究人员整理了一个包含4K个不同身份的大规模数据集。

Result: 实验表明，所提出的模型在表示多样化身份和表情方面具有强大的表现力，优于现有方法。此外，该方法提供了一个可解释的3D面部操作解决方案，允许用户进行局部编辑。

Conclusion: imHead是一种功能强大且可解释的隐式3DMM，能够生成富有表现力的3D头部化身，并实现对局部面部特征的精确编辑，为大规模3D头部建模迈出了重要一步。

Abstract: Over the last years, 3D morphable models (3DMMs) have emerged as a
state-of-the-art methodology for modeling and generating expressive 3D avatars.
However, given their reliance on a strict topology, along with their linear
nature, they struggle to represent complex full-head shapes. Following the
advent of deep implicit functions, we propose imHead, a novel implicit 3DMM
that not only models expressive 3D head avatars but also facilitates localized
editing of the facial features. Previous methods directly divided the latent
space into local components accompanied by an identity encoding to capture the
global shape variations, leading to expensive latent sizes. In contrast, we
retain a single compact identity space and introduce an intermediate
region-specific latent representation to enable local edits. To train imHead,
we curate a large-scale dataset of 4K distinct identities, making a
step-towards large scale 3D head modeling. Under a series of experiments we
demonstrate the expressive power of the proposed model to represent diverse
identities and expressions outperforming previous approaches. Additionally, the
proposed approach provides an interpretable solution for 3D face manipulation,
allowing the user to make localized edits.

</details>


### [210] [Nepali Sign Language Characters Recognition: Dataset Development and Deep Learning Approaches](https://arxiv.org/abs/2510.11243)
*Birat Poudel,Satyam Ghimire,Sijan Bhattarai,Saurav Bhandari,Suramya Sharma Dahal*

Main category: cs.CV

TL;DR: 本研究首次构建了尼泊尔手语（NSL）的基准数据集，包含36个手势类别，并使用MobileNetV2和ResNet50进行识别评估，分别达到了90.45%和88.78%的准确率，证明了卷积神经网络在低资源手语识别中的有效性。


<details>
  <summary>Details</summary>
Motivation: 听障和语障人士需要手语作为沟通系统，但尼泊尔手语（NSL）等代表性不足的手语缺乏数字语言数据集资源。

Method: 研究构建了第一个NSL基准数据集，包含36个手势类别，每类1,500个样本。为评估识别性能，研究对MobileNetV2和ResNet50架构进行了微调。

Result: MobileNetV2在数据集上实现了90.45%的分类准确率，ResNet50实现了88.78%的准确率。这些结果表明卷积神经网络在手语识别任务中，特别是在低资源环境下是有效的。

Conclusion: 这项工作代表了首次系统性地构建NSL基准数据集并评估深度学习方法，突显了迁移学习和微调在推进未充分探索的手语研究方面的潜力。

Abstract: Sign languages serve as essential communication systems for individuals with
hearing and speech impairments. However, digital linguistic dataset resources
for underrepresented sign languages, such as Nepali Sign Language (NSL), remain
scarce. This study introduces the first benchmark dataset for NSL, consisting
of 36 gesture classes with 1,500 samples per class, designed to capture the
structural and visual features of the language. To evaluate recognition
performance, we fine-tuned MobileNetV2 and ResNet50 architectures on the
dataset, achieving classification accuracies of 90.45% and 88.78%,
respectively. These findings demonstrate the effectiveness of convolutional
neural networks in sign recognition tasks, particularly within low-resource
settings. To the best of our knowledge, this work represents the first
systematic effort to construct a benchmark dataset and assess deep learning
approaches for NSL recognition, highlighting the potential of transfer learning
and fine-tuning for advancing research in underexplored sign languages.

</details>


### [211] [FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding](https://arxiv.org/abs/2510.10868)
*Soroush Mehraban,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: 本文提出两种合并策略（ECLM和Mask-ToMe）和一种基于扩散的解码器，以显著降低3D人体网格恢复(HMR)中Transformer模型的计算成本和复杂性，同时略微提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于Transformer的3D HMR模型因其深层架构和冗余tokens而导致计算成本高昂且复杂性高。

Method: 本文提出了两种HMR特定的合并策略：1) 误差约束层合并(ECLM)，选择性合并对MPJPE影响最小的Transformer层；2) 掩码引导Token合并(Mask-ToMe)，合并对最终预测贡献小的背景tokens。为弥补合并可能导致的性能下降，还提出了一种基于扩散的解码器，该解码器结合了时间上下文并利用从大规模运动捕捉数据集中学习到的姿态先验。

Result: 实验结果表明，该方法在多个基准测试中实现了高达2.3倍的加速，同时在性能上略优于基线模型。

Conclusion: 所提出的合并策略和扩散解码器能够有效降低3D HMR Transformer模型的计算成本和复杂性，同时保持或提升性能。

Abstract: Recent transformer-based models for 3D Human Mesh Recovery (HMR) have
achieved strong performance but often suffer from high computational cost and
complexity due to deep transformer architectures and redundant tokens. In this
paper, we introduce two HMR-specific merging strategies: Error-Constrained
Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM
selectively merges transformer layers that have minimal impact on the Mean Per
Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background
tokens that contribute little to the final prediction. To further address the
potential performance drop caused by merging, we propose a diffusion-based
decoder that incorporates temporal context and leverages pose priors learned
from large-scale motion capture datasets. Experiments across multiple
benchmarks demonstrate that our method achieves up to 2.3x speed-up while
slightly improving performance over the baseline.

</details>


### [212] [Full segmentation annotations of 3D time-lapse microscopy images of MDA231 cells](https://arxiv.org/abs/2510.10797)
*Aleksandra Melnikova,Petr Matula*

Main category: cs.CV

TL;DR: 本文提供了首个公开可用的复杂动态形状迁移细胞的完整3D延时分割标注数据集的详细描述及相关实验，并验证了其质量和优越性。


<details>
  <summary>Details</summary>
Motivation: 高质量、公开可用的图像和视频数据集分割标注对于图像处理领域至关重要，特别是大量目标的体积图像标注耗时且具有挑战性。先前的工作（Melnikova, A., & Matula, P., 2025）首次公开了此类标注，但因出版空间限制未能提供完整描述和实验细节，本文旨在补充这些内容。

Method: 三名人类标注者对来自细胞追踪挑战赛（CTC）的两个MDA231人乳腺癌细胞序列（Fluo-C3DL-MDA231）进行了完整的3D延时分割标注。本文通过与CTC提供的追踪标记进行一致性验证，并基于CTC的2D金标准测量分割准确性，与标注者间差异进行比较。此外，还将创建的3D标注与CTC提供的自动生成的银标准进行了比较。

Result: 创建的标注与CTC先前发布的追踪标记保持一致；基于CTC的2D金标准测量的分割准确性在标注者间差异范围内；与CTC提供的自动生成的银标准相比，提出的3D标注能更好地表示输入图像的复杂性。

Conclusion: 本文介绍的3D延时分割标注数据集质量高、具有代表性，可用于细胞分割的测试和训练，以及分析高度动态对象的3D形状。

Abstract: High-quality, publicly available segmentation annotations of image and video
datasets are critical for advancing the field of image processing. In
particular, annotations of volumetric images of a large number of targets are
time-consuming and challenging. In (Melnikova, A., & Matula, P., 2025), we
presented the first publicly available full 3D time-lapse segmentation
annotations of migrating cells with complex dynamic shapes. Concretely, three
distinct humans annotated two sequences of MDA231 human breast carcinoma cells
(Fluo-C3DL-MDA231) from the Cell Tracking Challenge (CTC).
  This paper aims to provide a comprehensive description of the dataset and
accompanying experiments that were not included in (Melnikova, A., & Matula,
P., 2025) due to limitations in publication space. Namely, we show that the
created annotations are consistent with the previously published tracking
markers provided by the CTC organizers and the segmentation accuracy measured
based on the 2D gold truth of CTC is within the inter-annotator variability
margins. We compared the created 3D annotations with automatically created
silver truth provided by CTC. We have found the proposed annotations better
represent the complexity of the input images. The presented annotations can be
used for testing and training cell segmentation, or analyzing 3D shapes of
highly dynamic objects.

</details>


### [213] [rareboost3d: a synthetic lidar dataset with enhanced rare classes](https://arxiv.org/abs/2510.10876)
*Shutong Lin,Zhengkang Xiang,Jianzhong Qi,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 为解决点云数据集中稀有类别的长尾问题，本文提出了一个名为RareBoost3D的新型合成点云数据集，并引入了跨域语义对齐方法CSC损失，实验证明能显著提升LiDAR点云分割模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有真实世界点云数据集中，某些稀有类别的实例数量有限，导致长尾问题，严重阻碍了LiDAR感知技术（如自动驾驶中的物体分割）的发展。

Method: 1. 引入RareBoost3D：一个新型合成点云数据集，为真实世界数据集中稀有的物体类别提供了显著更多的实例。2. 提出CSC损失：一种跨域语义对齐方法，旨在对齐不同域（合成数据与真实数据）中相同类别的特征表示。

Result: 实验结果表明，所提出的对齐方法（CSC损失）显著增强了LiDAR点云分割模型在真实世界数据上的性能。

Conclusion: 通过结合新型合成数据集RareBoost3D和跨域语义对齐方法CSC损失，能够有效解决点云分割中的长尾问题，并显著提升模型在真实世界数据上的表现。

Abstract: Real-world point cloud datasets have made significant contributions to the
development of LiDAR-based perception technologies, such as object segmentation
for autonomous driving. However, due to the limited number of instances in some
rare classes, the long-tail problem remains a major challenge in existing
datasets. To address this issue, we introduce a novel, synthetic point cloud
dataset named RareBoost3D, which complements existing real-world datasets by
providing significantly more instances for object classes that are rare in
real-world datasets. To effectively leverage both synthetic and real-world
data, we further propose a cross-domain semantic alignment method named CSC
loss that aligns feature representations of the same class across different
domains. Experimental results demonstrate that this alignment significantly
enhances the performance of LiDAR point cloud segmentation models over
real-world data.

</details>


### [214] [A Large-Language-Model Assisted Automated Scale Bar Detection and Extraction Framework for Scanning Electron Microscopic Images](https://arxiv.org/abs/2510.11260)
*Yuxuan Chen,Ruotong Yang,Zhengyang Zhang,Mehreen Ahmed,Yanming Wang*

Main category: cs.CV

TL;DR: 本文提出了一种多模态自动化框架，利用目标检测、混合OCR和大型语言模型（LLM）代理，高效准确地检测和提取扫描电子显微镜（SEM）图像中的标尺信息，解决了手动操作耗时且易出错的问题。


<details>
  <summary>Details</summary>
Motivation: 扫描电子显微镜（SEM）图像中的标尺确定是微观分析的关键第一步，但目前主要依赖手动操作，效率低下且容易出错。为解决这一问题，需要一种自动化方法。

Method: 所提出的框架分为四个阶段：i) 自动数据集生成（Auto-DG）模型用于合成多样化的SEM图像数据集；ii) 标尺对象检测；iii) 使用结合DenseNet和CRNN算法的混合光学字符识别（OCR）系统进行信息提取；iv) 一个LLM代理用于分析和验证结果的准确性。

Result: 该模型在目标检测方面表现出色，精度为100%，召回率为95.8%，在IoU=0.5时平均精度（mAP）为99.2%，在IoU=0.5:0.95时为69.1%。混合OCR系统在Auto-DG数据集上实现了89%的精度、65%的召回率和75%的F1分数，显著优于主流的独立引擎。LLM作为推理引擎和智能助手，用于验证结果并提出后续步骤。

Conclusion: 这种由LLM代理驱动的自动化方法显著提高了SEM图像中标尺检测和提取的效率和准确性，为微观分析提供了有价值的工具，并推动了科学成像领域的发展。

Abstract: Microscopic characterizations, such as Scanning Electron Microscopy (SEM),
are widely used in scientific research for visualizing and analyzing
microstructures. Determining the scale bars is an important first step of
accurate SEM analysis; however, currently, it mainly relies on manual
operations, which is both time-consuming and prone to errors. To address this
issue, we propose a multi-modal and automated scale bar detection and
extraction framework that provides concurrent object detection, text detection
and text recognition with a Large Language Model (LLM) agent. The proposed
framework operates in four phases; i) Automatic Dataset Generation (Auto-DG)
model to synthesize a diverse dataset of SEM images ensuring robust training
and high generalizability of the model, ii) scale bar object detection, iii)
information extraction using a hybrid Optical Character Recognition (OCR)
system with DenseNet and Convolutional Recurrent Neural Network (CRNN) based
algorithms, iv) an LLM agent to analyze and verify accuracy of the results. The
proposed model demonstrates a strong performance in object detection and
accurate localization with a precision of 100%, recall of 95.8%, and a mean
Average Precision (mAP) of 99.2% at IoU=0.5 and 69.1% at IoU=0.5:0.95. The
hybrid OCR system achieved 89% precision, 65% recall, and a 75% F1 score on the
Auto-DG dataset, significantly outperforming several mainstream standalone
engines, highlighting its reliability for scientific image analysis. The LLM is
introduced as a reasoning engine as well as an intelligent assistant that
suggests follow-up steps and verifies the results. This automated method
powered by an LLM agent significantly enhances the efficiency and accuracy of
scale bar detection and extraction in SEM images, providing a valuable tool for
microscopic analysis and advancing the field of scientific imaging.

</details>


### [215] [When Does Supervised Training Pay Off? The Hidden Economics of Object Detection in the Era of Vision-Language Models](https://arxiv.org/abs/2510.11302)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 本文首次对监督式目标检测（YOLO）和零样本VLM推理（Gemini Flash 2.5）进行了全面的成本效益分析。研究发现，YOLO在标准类别上准确率更高但需要大量标注投资，而零样本Gemini在未训练类别上表现更好且每检测成本显著更低。最佳架构选择取决于部署量、类别稳定性、预算和准确率要求。


<details>
  <summary>Details</summary>
Motivation: 传统监督式目标检测系统依赖手动标注，成本高昂。视觉-语言模型（VLMs）提供零样本检测能力，无需标注但准确率较低。本文旨在通过成本效益分析，量化比较这两种范式，以指导实际应用中的架构选择。

Method: 研究方法包括：在1,000张分层COCO图像和200张多样化产品图像（涵盖消费电子和稀有类别）上系统评估YOLO和Gemini Flash 2.5的性能；建立详细的总体拥有成本（TCO）模型；进行每正确检测成本分析；开发决策框架以指导架构选择。

Result: 主要结果显示：在标准类别上，监督式YOLO准确率达91.2%，零样本Gemini为68.5%，YOLO优势明显（22.7个百分点），但需10,800美元的100类别系统标注成本，仅在超过5500万次推理（或每日151,000张图像一年）时才值得投资。在多样化产品类别上，零样本Gemini准确率为52.3%，而监督式YOLO因未训练类别而准确率为0%。每正确检测成本分析显示，Gemini在100,000次推理时每检测成本显著低于YOLO（0.00050美元 vs 0.143美元）。

Conclusion: 最佳的目标检测架构选择并非纯粹基于技术性能指标，而是关键性地取决于部署量、类别稳定性、预算限制和准确率要求。在特定场景下，零样本VLM即使在准确率较低的情况下，也可能因其极低的每检测成本而成为更具成本效益的解决方案。

Abstract: Object detection systems have traditionally relied on supervised learning
with manually annotated bounding boxes, achieving high accuracy at the cost of
substantial annotation investment. The emergence of Vision-Language Models
(VLMs) offers an alternative paradigm enabling zero-shot detection through
natural language queries, eliminating annotation requirements but operating
with reduced accuracy. This paper presents the first comprehensive
cost-effectiveness analysis comparing supervised detection (YOLO) with
zero-shot VLM inference (Gemini Flash 2.5). Through systematic evaluation on
1,000 stratified COCO images and 200 diverse product images spanning consumer
electronics and rare categories, combined with detailed Total Cost of Ownership
modeling, we establish quantitative break-even thresholds governing
architecture selection. Our findings reveal that supervised YOLO achieves 91.2%
accuracy versus 68.5% for zero-shot Gemini on standard categories, representing
a 22.7 percentage point advantage that costs $10,800 in annotation for
100-category systems. However, this advantage justifies investment only beyond
55 million inferences, equivalent to 151,000 images daily for one year.
Zero-shot Gemini demonstrates 52.3% accuracy on diverse product categories
(ranging from highly web-prevalent consumer electronics at 75-85% to rare
specialized equipment at 25-40%) where supervised YOLO achieves 0% due to
architectural constraints preventing detection of untrained classes. Cost per
Correct Detection analysis reveals substantially lower per-detection costs for
Gemini ($0.00050 vs $0.143) at 100,000 inferences despite accuracy deficits. We
develop decision frameworks demonstrating that optimal architecture selection
depends critically on deployment volume, category stability, budget
constraints, and accuracy requirements rather than purely technical performance
metrics.

</details>


### [216] [Towards Distribution-Shift Uncertainty Estimation for Inverse Problems with Generative Priors](https://arxiv.org/abs/2510.10947)
*Namhoon Kim,Sara Fridovich-Keil*

Main category: cs.CV

TL;DR: 本文提出了一种无需校准、实例级的生成模型不确定性指标，通过测量变异下重建的稳定性来检测分布外（OOD）图像，从而避免在逆问题中生成模型可能出现的幻觉。


<details>
  <summary>Details</summary>
Motivation: 生成模型作为逆问题的数据驱动先验，在减少测量数据方面表现出色，但当测试图像超出训练分布时，存在产生幻觉特征的风险。现有不确定性量化方法存在局限性，如需要分布内校准数据集、提供启发式而非统计估计，或仅量化模型容量或测量限制带来的不确定性，而非分布偏移。

Method: 核心假设是，分布内图像的重建在随机测量变异下保持稳定，而分布外（OOD）图像的重建则表现出更大的不稳定性。本文利用这种稳定性作为检测分布偏移的代理，提出了一种无需校准、实例级的不确定性指标，该指标对分布偏移敏感，不需要训练分布知识，也无需重新训练。

Result: 在MNIST数字的断层重建任务中进行了验证，模型仅用数字“0”训练，但在所有十个数字上进行评估。结果显示，OOD数字的重建表现出更高的变异性，并伴随更高的重建误差，从而验证了该指标的有效性。

Conclusion: 这些结果表明，可以将生成先验与轻量级防护措施（即本文提出的OOD指标）结合部署，从而在分布内情况下实现激进的测量减少，同时在先验模型应用于分布外数据时自动发出警告。

Abstract: Generative models have shown strong potential as data-driven priors for
solving inverse problems such as reconstructing medical images from
undersampled measurements. While these priors improve reconstruction quality
with fewer measurements, they risk hallucinating features when test images lie
outside the training distribution. Existing uncertainty quantification methods
in this setting (i) require an in-distribution calibration dataset, which may
not be available, (ii) provide heuristic rather than statistical estimates, or
(iii) quantify uncertainty from model capacity or limited measurements rather
than distribution shift. We propose an instance-level, calibration-free
uncertainty indicator that is sensitive to distribution shift, requires no
knowledge of the training distribution, and incurs no retraining cost. Our key
hypothesis is that reconstructions of in-distribution images remain stable
under random measurement variations, while reconstructions of
out-of-distribution (OOD) images exhibit greater instability. We use this
stability as a proxy for detecting distribution shift. Our proposed OOD
indicator is efficiently computable for any computational imaging inverse
problem; we demonstrate it on tomographic reconstruction of MNIST digits, where
a learned proximal network trained only on digit "0" is evaluated on all ten
digits. Reconstructions of OOD digits show higher variability and
correspondingly higher reconstruction error, validating this indicator. These
results suggest a deployment strategy that pairs generative priors with
lightweight guardrails, enabling aggressive measurement reduction for
in-distribution cases while automatically warning when priors are applied out
of distribution.

</details>


### [217] [Where on Earth? A Vision-Language Benchmark for Probing Model Geolocation Skills Across Scales](https://arxiv.org/abs/2510.10880)
*Zhaofang Qian,Hardy Chen,Zeyu Wang,Li Zhang,Zijun Wang,Xiaoke Huang,Hui Liu,Xianfeng Tang,Zeyu Zheng,Haoqin Tu,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: 该研究引入了EarthWhere，一个全面的基准测试，用于评估视觉语言模型（VLMs）在开放世界图像地理定位方面的能力，包括视觉识别、逐步推理和证据使用。通过对13个最先进的VLM进行基准测试，发现Gemini-2.5-Pro表现最佳，但模型在视觉线索有限时表现不佳，并存在显著的区域偏差，突出了模型在减轻偏差和实现鲁棒、细粒度定位方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型（VLMs）发展迅速，但它们在开放世界条件下进行图像地理定位的能力尚未得到全面评估。这是一项具有挑战性且在现实生活中需求很大的任务。

Method: 研究提出了EarthWhere，一个包含810张全球分布式图像的综合基准测试。它包括两个互补的地理定位尺度：WhereCountry（500道国家级多项选择题）和WhereStreet（310个需要多步推理和可选网络搜索的细粒度街景识别任务）。评估指标包括坐标的Acc@k、文本定位的层级路径分数，以及通过人工验证的关键视觉线索和Shapley重新加权思维分数来评估中间推理链。研究对13个最先进的VLM（带网络搜索工具）进行了基准测试。

Result: Gemini-2.5-Pro以56.32%的平均准确率表现最佳，而最强的开源模型GLM-4.5V达到34.71%。研究发现，当视觉线索有限时，网络搜索和推理并不能保证性能提升。模型还表现出区域偏差，在某些区域的得分比其他区域高出高达42.7%。

Conclusion: 研究结果不仅展示了模型在地理定位方面的潜力，也揭示了模型在减轻偏差和实现鲁棒、细粒度定位方面持续存在的挑战。该基准测试已开源。

Abstract: Vision-language models (VLMs) have advanced rapidly, yet their capacity for
image-grounded geolocation in open-world conditions, a task that is challenging
and of demand in real life, has not been comprehensively evaluated. We present
EarthWhere, a comprehensive benchmark for VLM image geolocation that evaluates
visual recognition, step-by-step reasoning, and evidence use. EarthWhere
comprises 810 globally distributed images across two complementary geolocation
scales: WhereCountry (i.e., 500 multiple-choice question-answering, with
country-level answer and panoramas) and WhereStreet (i.e., 310 fine-grained
street-level identification tasks requiring multi-step reasoning with optional
web search). For evaluation, we adopt the final-prediction metrics: location
accuracies within k km (Acc@k) for coordinates and hierarchical path scores for
textual localization. Beyond this, we propose to explicitly score intermediate
reasoning chains using human-verified key visual clues and a Shapley-reweighted
thinking score that attributes credit to each clue's marginal contribution. We
benchmark 13 state-of-the-art VLMs with web searching tools on our EarthWhere
and report different types of final answer accuracies as well as the calibrated
model thinking scores. Overall, Gemini-2.5-Pro achieves the best average
accuracy at 56.32%, while the strongest open-weight model, GLM-4.5V, reaches
34.71%. We reveal that web search and reasoning do not guarantee improved
performance when visual clues are limited, and models exhibit regional biases,
achieving up to 42.7% higher scores in certain areas than others. These
findings highlight not only the promise but also the persistent challenges of
models to mitigate bias and achieve robust, fine-grained localization. We
open-source our benchmark at https://github.com/UCSC-VLAA/EarthWhere.

</details>


### [218] [IUT-Plug: A Plug-in tool for Interleaved Image-Text Generation](https://arxiv.org/abs/2510.10969)
*Zeteng Lin,Xingxing Li,Wen You,Xiaoyang Li,Zehan Lu,Yujun Cai,Jing Tang*

Main category: cs.CV

TL;DR: IUT-Plug是一个基于图像理解树（IUT）的模块，通过显式结构化推理增强现有交错式视觉语言模型（VLM），以缓解多模态图像-文本生成中逻辑、实体识别和风格的上下文漂移问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如GPT-4和DALL-E）在多模态图像-文本生成中难以保持逻辑、对象识别和风格的一致性，这严重限制了它们在复杂输入-输出场景中的泛化能力。

Method: 本文提出了IUT-Plug框架，分两阶段运行：1) 动态IUT-Plug提取模块将视觉场景解析为分层符号结构（图像理解树IUT）；2) 协调的叙事流和图像合成机制确保跨模态一致性。此外，本文构建了一个包含3,000个人工生成问答对的新基准和动态评估协议来量化上下文漂移。

Result: 实验结果表明，IUT-Plug不仅提高了在现有基准上的准确性，而且有效缓解了在各种多模态问答场景中逻辑、实体识别和风格这三种关键形式的上下文漂移。

Conclusion: IUT-Plug通过引入显式结构化推理（基于图像理解树）成功解决了现有视觉语言模型在多模态生成中存在的上下文漂移问题，显著提升了模型的准确性和泛化能力。

Abstract: Existing vision language models (VLMs), including GPT-4 and DALL-E, often
struggle to preserve logic, object identity, and style in multimodal image-text
generation. This limitation significantly hinders the generalization capability
of VLMs in complex image-text input-output scenarios. To address this issue, we
propose IUT-Plug, a module grounded in an Image Understanding Tree (IUT), which
enhances existing interleaved VLMs through explicit structured reasoning,
thereby mitigating context drift in logic, entity identity, and style. The
proposed framework operates in two stages. (1) A dynamic IUT-Plug extraction
module parses visual scenes into hierarchical symbolic structures. (2) A
coordinated narrative-flow and image synthesis mechanism ensures cross-modal
consistency. To evaluate our approach, we construct a novel benchmark based on
3,000 real human-generated question-answer pairs over fine-tuned large models,
introducing a dynamic evaluation protocol for quantifying context drift in
interleaved VLMs. Experimental results demonstrate that IUT-Plug not only
improves accuracy on established benchmarks but also effectively alleviates the
three critical forms of context drift across diverse multimodal question
answering (QA) scenarios.

</details>


### [219] [Uncertainty-Aware ControlNet: Bridging Domain Gaps with Synthetic Image Generation](https://arxiv.org/abs/2510.11346)
*Joshua Niemeijer,Jan Ehrhardt,Heinz Handels,Hristina Uzunova*

Main category: cs.CV

TL;DR: 本文提出了一种利用不确定性引导的ControlNet方法，从无标签领域生成带标注的合成数据，以弥补判别模型在领域漂移问题上的不足，并在视网膜OCT图像分割和交通场景实验中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 生成模型（如ControlNet）在创建高质量图像数据方面很有价值，但其生成的合成数据往往复刻原始训练分布，限制了对判别模型（如语义分割）的增强效果。此外，判别模型在面对领域漂移（如高质Spectralis OCT与低质Home-OCT）时表现不佳，而目标领域通常缺乏带标注数据。

Method: 引入了不确定性概念到ControlNet的控制机制中。最终网络结合了两种控制：来自无标签数据集的不确定性控制和来自有标签数据集的语义控制。这使得ControlNet能够从目标领域生成具有高不确定性的带标注数据（即从无标签分布中合成带标签数据）。

Result: 在视网膜OCT图像分割场景中，该方法显著改善了Home-OCT数据的分割结果，且无需额外监督。与风格迁移相比，不确定性引导的优势在于能够实现任意领域漂移，而无需严格学习图像风格。这一点也在交通场景实验中得到了验证。

Conclusion: 通过将不确定性引入控制机制，本文提出的方法能够有效利用无标签领域数据训练ControlNet，生成目标领域中带有高不确定性的标注数据，从而弥合领域差距，显著提升判别模型（如分割网络）在面对领域漂移时的性能。

Abstract: Generative Models are a valuable tool for the controlled creation of
high-quality image data. Controlled diffusion models like the ControlNet have
allowed the creation of labeled distributions. Such synthetic datasets can
augment the original training distribution when discriminative models, like
semantic segmentation, are trained. However, this augmentation effect is
limited since ControlNets tend to reproduce the original training distribution.
  This work introduces a method to utilize data from unlabeled domains to train
ControlNets by introducing the concept of uncertainty into the control
mechanism. The uncertainty indicates that a given image was not part of the
training distribution of a downstream task, e.g., segmentation. Thus, two types
of control are engaged in the final network: an uncertainty control from an
unlabeled dataset and a semantic control from the labeled dataset. The
resulting ControlNet allows us to create annotated data with high uncertainty
from the target domain, i.e., synthetic data from the unlabeled distribution
with labels. In our scenario, we consider retinal OCTs, where typically
high-quality Spectralis images are available with given ground truth
segmentations, enabling the training of segmentation networks. The recent
development in Home-OCT devices, however, yields retinal OCTs with lower
quality and a large domain shift, such that out-of-the-pocket segmentation
networks cannot be applied for this type of data. Synthesizing annotated images
from the Home-OCT domain using the proposed approach closes this gap and leads
to significantly improved segmentation results without adding any further
supervision. The advantage of uncertainty-guidance becomes obvious when
compared to style transfer: it enables arbitrary domain shifts without any
strict learning of an image style. This is also demonstrated in a traffic scene
experiment.

</details>


### [220] [Chart-RVR: Reinforcement Learning with Verifiable Rewards for Explainable Chart Reasoning](https://arxiv.org/abs/2510.10973)
*Sanchit Sinha,Oana Frunza,Kashif Rasul,Yuriy Nevmyvaka,Aidong Zhang*

Main category: cs.CV

TL;DR: 本文提出了Chart-RVR框架，通过结合GRPO和可验证奖励来微调大型视觉语言模型（LVLMs），使其在图表推理任务中对分布外（OOD）数据更具鲁棒性，并能生成更具解释性的思维链（CoT）推理过程，从而实现最先进的性能和更高的可信度。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（LVLMs）在许多视觉推理任务（包括图表推理）上表现出色，但它们在处理分布外（OOD）数据时性能会下降，并且在生成思维链（CoT）推理过程时表现更差，这限制了其可解释性。

Method: Chart-RVR是一个通用框架，它通过将组相对策略优化（GRPO）与自动可验证奖励相结合来微调LVLMs，以提高其在图表推理中的鲁棒性和可解释性。该框架包含三个奖励，分别最大化：(i) 正确的图表类型分类，(ii) 忠实的图表表格重建，以及 (iii) 过程一致性。

Result: 将Chart-RVR应用于30亿参数的LVLMs，其在分布内和分布外数据集上均持续优于标准的监督微调（SFT），缩小了OOD性能差距，并提高了推理的忠实性。由此产生的Chart-RVR-3B系列模型在六个涵盖域内和OOD设置的图表推理基准测试中取得了最先进的结果，超越了所有现有同等规模的模型。除了准确性，Chart-RVR还产生了更具可解释性的CoT推理过程，增强了信任和可靠性。

Conclusion: 可验证奖励与GRPO相结合对于训练可靠、可解释的图表推理模型具有强大的能力。Chart-RVR框架显著提升了LVLMs在图表推理任务中的鲁棒性、解释性和整体性能，尤其是在处理分布外数据方面。

Abstract: The capabilities of Large Vision-Language Models (LVLMs) have reached
state-of-the-art on many visual reasoning tasks, including chart reasoning, yet
they still falter on out-of-distribution (OOD) data, and degrade further when
asked to produce their chain-of-thought (CoT) rationales, limiting
explainability. We present Chart-RVR, a general framework that fine-tunes LVLMs
to be more robust and explainable for chart reasoning by coupling Group
Relative Policy Optimization (GRPO) with automatically verifiable rewards. Our
framework comprises of three rewards that maximize: (i) correct chart-type
classification, (ii) faithful chart table reconstruction, and (iii) process
conformity. Applied to 3-billion-parameter LVLMs, Chart-RVR consistently
outperforms standard supervised fine-tuning (SFT) on both in-distribution and
out-of-distribution datasets, closing the OOD performance gap while improving
rationale fidelity. The resulting models, the Chart-RVR-3B series, achieve
state-of-the-art results on six chart-reasoning benchmarks spanning in-domain
and OOD settings, surpassing all existing models of comparable size. Beyond
accuracy, Chart-RVR yields more interpretable CoT rationales, strengthening
trust and reliability - showcasing the power of verifiable rewards with GRPO
for training reliable, interpretable chart-reasoning models.

</details>


### [221] [Perspective-aware 3D Gaussian Inpainting with Multi-view Consistency](https://arxiv.org/abs/2510.10993)
*Yuxin Cheng,Binxiao Huang,Taiqiang Wu,Wenyong Zhou,Chenchen Ding,Zhengwu Liu,Graziano Chesi,Ngai Wong*

Main category: cs.CV

TL;DR: PAInpainter是一种新颖的3D高斯修复方法，通过透视感知内容传播和多视角一致性验证，显著提升了修复场景的全局一致性和纹理保真度。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练扩散模型在3D高斯修复方面取得了进展，但确保多视角一致性（高质量修复的关键要求）仍然是一个主要挑战。

Method: PAInpainter通过从透视图中自适应采样多个视角，迭代地细化修复并优化3D高斯表示。它将修复后的图像作为先验信息进行传播，并验证相邻视角之间的一致性，从而增强全局一致性和纹理保真度。

Result: PAInpainter在SPIn-NeRF和NeRFiller数据集上分别达到了26.03 dB和29.51 dB的PSNR分数，优于现有方法，展示了卓越的3D修复质量、有效性和泛化能力。

Conclusion: PAInpainter通过其透视感知内容传播和一致性验证机制，显著提升了恢复3D场景的全局一致性和纹理保真度，解决了多视角一致性难题。

Abstract: 3D Gaussian inpainting, a critical technique for numerous applications in
virtual reality and multimedia, has made significant progress with pretrained
diffusion models. However, ensuring multi-view consistency, an essential
requirement for high-quality inpainting, remains a key challenge. In this work,
we present PAInpainter, a novel approach designed to advance 3D Gaussian
inpainting by leveraging perspective-aware content propagation and consistency
verification across multi-view inpainted images. Our method iteratively refines
inpainting and optimizes the 3D Gaussian representation with multiple views
adaptively sampled from a perspective graph. By propagating inpainted images as
prior information and verifying consistency across neighboring views,
PAInpainter substantially enhances global consistency and texture fidelity in
restored 3D scenes. Extensive experiments demonstrate the superiority of
PAInpainter over existing methods. Our approach achieves superior 3D inpainting
quality, with PSNR scores of 26.03 dB and 29.51 dB on the SPIn-NeRF and
NeRFiller datasets, respectively, highlighting its effectiveness and
generalization capability.

</details>


### [222] [Mixup Helps Understanding Multimodal Video Better](https://arxiv.org/abs/2510.10986)
*Xiaoyu Ma,Ding Ding,Hao Chen*

Main category: cs.CV

TL;DR: 本文提出了多模态Mixup (MM) 和平衡多模态Mixup (B-MM) 方法，旨在解决多模态视频理解中强模态过拟合和模态不平衡问题，从而提高模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多模态模型容易过度拟合强模态，导致强模态主导学习并抑制弱模态的贡献，从而影响模型的泛化能力。

Method: 1. **多模态Mixup (MM)**：在聚合的多模态特征层面应用Mixup策略，生成虚拟特征-标签对以缓解过拟合。2. **平衡多模态Mixup (B-MM)**：在MM的基础上，根据每种模态对学习目标的相对贡献，动态调整它们的混合比例，以解决训练过程中的模态不平衡问题。

Result: 在多个数据集上进行了广泛的实验，结果表明所提出的方法在提高泛化能力和多模态鲁棒性方面是有效的。

Conclusion: 通过引入多模态Mixup (MM) 和平衡多模态Mixup (B-MM) 策略，可以有效缓解多模态模型中的过拟合和模态不平衡问题，从而显著提升多模态视频理解任务的性能和鲁棒性。

Abstract: Multimodal video understanding plays a crucial role in tasks such as action
recognition and emotion classification by combining information from different
modalities. However, multimodal models are prone to overfitting strong
modalities, which can dominate learning and suppress the contributions of
weaker ones. To address this challenge, we first propose Multimodal Mixup (MM),
which applies the Mixup strategy at the aggregated multimodal feature level to
mitigate overfitting by generating virtual feature-label pairs. While MM
effectively improves generalization, it treats all modalities uniformly and
does not account for modality imbalance during training. Building on MM, we
further introduce Balanced Multimodal Mixup (B-MM), which dynamically adjusts
the mixing ratios for each modality based on their relative contributions to
the learning objective. Extensive experiments on several datasets demonstrate
the effectiveness of our methods in improving generalization and multimodal
robustness.

</details>


### [223] [DocReward: A Document Reward Model for Structuring and Stylizing](https://arxiv.org/abs/2510.11391)
*Junpeng Liu,Yuzhong Zhao,Bowen Cao,Jiayu Ding,Yilin Jia,Tengchao Lv,Yupan Huang,Shaohan Huang,Nan Yang,Li Dong,Lei Cui,Tao Ge,Xun Wang,Huitian Jiao,Sun Mao,FNU Kartik,Si-Qing Chen,Wai Lam,Furu Wei*

Main category: cs.CV

TL;DR: 现有智能体工作流在文档生成中忽视视觉结构和风格，本文提出DocReward模型，通过构建DocPair数据集和Bradley-Terry损失训练，有效评估文档的结构和风格专业性，其性能显著优于GPT-4o和GPT-5，并能指导生成智能体产生更受人类偏好的文档。


<details>
  <summary>Details</summary>
Motivation: 当前的智能体工作流在文档生成方面主要关注文本质量，但忽略了对可读性和吸引力至关重要的视觉结构和风格。这种缺陷主要是由于缺乏合适的奖励模型来指导智能体工作流生成具有更强结构和风格质量的文档。

Method: 本文提出了DocReward，一个用于评估文档结构和风格的文档奖励模型。为此，构建了一个包含117K配对文档的多领域数据集DocPair，涵盖32个领域和267种文档类型，每对文档包含内容相同但结构和风格不同的高专业度和低专业度文档。DocReward使用Bradley-Terry损失进行训练，通过惩罚与标注排名矛盾的预测来对文档进行评分。为了评估奖励模型的性能，还创建了一个由受过良好教育的人类评估员排名文档包的测试数据集。

Result: DocReward在准确性方面分别比GPT-4o和GPT-5高出30.6和19.4个百分点，显示出其优于基线模型的性能。在文档生成的外在评估中，DocReward实现了60.8%的显著更高胜率，而GPT-5的胜率为37.7%，这表明DocReward在指导生成智能体生成人类偏好文档方面的实用性。

Conclusion: DocReward成功地解决了智能体工作流在文档生成中忽视视觉结构和风格的问题，其卓越的评估能力和在指导生成任务中的有效性，使其能够帮助智能体生成更符合人类偏好的专业文档。

Abstract: Recent advances in agentic workflows have enabled the automation of tasks
such as professional document generation. However, they primarily focus on
textual quality, neglecting visual structure and style, which are crucial for
readability and engagement. This gap arises mainly from the absence of suitable
reward models to guide agentic workflows toward producing documents with
stronger structural and stylistic quality. To address this, we propose
DocReward, a document reward model that evaluates documents based on their
structure and style. We construct a multi-domain dataset DocPair of 117K paired
documents, covering 32 domains and 267 document types, each including a high-
and low-professionalism document with identical content but different structure
and style. This enables the model to evaluate professionalism comprehensively,
and in a textual-quality-agnostic way. DocReward is trained using the
Bradley-Terry loss to score documents, penalizing predictions that contradict
the annotated ranking. To assess the performance of reward models, we create a
test dataset containing document bundles ranked by well-educated human
evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6
and 19.4 percentage points, respectively, demonstrating its superiority over
baselines. In an extrinsic evaluation of document generation, DocReward
achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7%
win rate, demonstrating its utility in guiding generation agents toward
producing human-preferred documents.

</details>


### [224] [AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model](https://arxiv.org/abs/2510.11496)
*Zhiwei Jin,Xiaohui Song,Nan Wang,Yafei Liu,Chao Li,Xin Li,Ruichen Wang,Zhihao Li,Qi Qi,Long Cheng,Dongze Hao,Quanlong Zheng,Yanhao Zhang,Haobo Ji,Jian Ma,Zhitong Zheng,Zhenyi Lin,Haolin Deng,Xin Zou,Xiaojie Yin,Ruilin Wang,Liankai Cai,Haijing Liu,Yuqing Qiu,Ke Chen,Zixian Li,Chi Xie,Huafei Li,Chenxing Li,Chuangchuang Wang,Kai Tang,Zhiguang Zhu,Kai Tang,Wenmei Gao,Rui Wang,Jun Wu,Chao Liu,Qin Xie,Chen Chen,Haonan Lu*

Main category: cs.CV

TL;DR: AndesVL是一个参数量0.6B到4B的移动端多模态大模型套件，基于Qwen3和多种视觉编码器，在多项开源基准测试中表现优异，并引入了1+N LoRA。


<details>
  <summary>Details</summary>
Motivation: 云端多模态大模型（如GPT-4o）性能卓越，但参数量巨大，无法部署在内存、功耗和计算能力受限的移动边缘设备上。

Method: 该研究引入了AndesVL，一个基于Qwen3的LLM和多种视觉编码器构建的移动端多模态大模型套件，参数量从0.6B到4B。论文详细阐述了其模型架构、训练流程和训练数据，并引入了1+N LoRA方法。

Result: AndesVL在多种开源基准测试中，包括富文本图像理解、推理与数学、多图像理解、通用VQA、幻觉缓解、多语言理解和GUI相关任务，均达到了同等规模最先进模型的第一梯队性能。

Conclusion: AndesVL成功地将高性能多模态大模型部署到移动端设备，解决了云端模型在边缘设备的限制，并在同等规模下取得了领先性能，为移动端MLLM提供了有效解决方案。

Abstract: In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,
Gemini, and Claude Sonnet have demonstrated outstanding performance with
enormous model sizes reaching hundreds of billions of parameters, they
significantly surpass the limitations in memory, power consumption, and
computing capacity of edge devices such as mobile phones. This paper introduces
AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on
Qwen3's LLM and various visual encoders. We comprehensively outline the model
architectures, training pipeline, and training data of AndesVL, which achieves
first-tier performance across a wide range of open-source benchmarks, including
fields such as text-rich image understanding, reasoning and math, multi-image
comprehension, general VQA, hallucination mitigation, multilingual
understanding, and GUI-related tasks when compared with state-of-the-art models
of a similar scale. Furthermore, we introduce a 1+N LoR

</details>


### [225] [ContextGen: Contextual Layout Anchoring for Identity-Consistent Multi-Instance Generation](https://arxiv.org/abs/2510.11000)
*Ruihang Xu,Dewei Zhou,Fan Ma,Yi Yang*

Main category: cs.CV

TL;DR: ContextGen是一个新的扩散Transformer框架，通过引入上下文布局锚定（CLA）和身份一致性注意力（ICA）机制，解决了多实例图像生成中布局控制和身份保持的挑战。该方法结合了布局和参考图像的引导，并推出了IMIG-100K数据集，在控制精度、身份保真度和视觉质量方面超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 现代扩散模型在多实例图像生成（MIG）中面临重大挑战，主要体现在难以精确控制物体布局和保持多个不同主体的身份一致性。

Method: 本文提出了ContextGen，一个由布局和参考图像引导的新型扩散Transformer框架。它包含两个关键技术贡献：1) 上下文布局锚定（CLA）机制，将复合布局图像整合到生成上下文中，以稳健地锚定物体；2) 身份一致性注意力（ICA），利用上下文参考图像确保多实例的身份一致性。此外，还引入了IMIG-100K，第一个具有详细布局和身份标注的大规模分层结构数据集。

Result: 广泛的实验证明，ContextGen在控制精度、身份保真度和整体视觉质量方面均超越了现有方法，达到了新的技术水平。

Conclusion: ContextGen通过其创新的CLA和ICA机制，并结合新的IMIG-100K数据集，有效解决了多实例图像生成中的布局控制和身份保持问题，显著提升了生成性能，树立了新的行业标杆。

Abstract: Multi-instance image generation (MIG) remains a significant challenge for
modern diffusion models due to key limitations in achieving precise control
over object layout and preserving the identity of multiple distinct subjects.
To address these limitations, we introduce ContextGen, a novel Diffusion
Transformer framework for multi-instance generation that is guided by both
layout and reference images. Our approach integrates two key technical
contributions: a Contextual Layout Anchoring (CLA) mechanism that incorporates
the composite layout image into the generation context to robustly anchor the
objects in their desired positions, and Identity Consistency Attention (ICA),
an innovative attention mechanism that leverages contextual reference images to
ensure the identity consistency of multiple instances. Recognizing the lack of
large-scale, hierarchically-structured datasets for this task, we introduce
IMIG-100K, the first dataset with detailed layout and identity annotations.
Extensive experiments demonstrate that ContextGen sets a new state-of-the-art,
outperforming existing methods in control precision, identity fidelity, and
overall visual quality.

</details>


### [226] [Frequency Domain Unlocks New Perspectives for Abdominal Medical Image Segmentation](https://arxiv.org/abs/2510.11005)
*Kai Han,Siqi Ma,Chengxuan Qian,Jun Chen,Chongwen Lyu,Yuqing Song,Zhe Liu*

Main category: cs.CV

TL;DR: 本文提出前景感知谱分割（FASS）框架，通过前景感知、频率增强和边缘约束模块，有效解决了医学图像中低对比度肿瘤与正常组织分割的挑战，提升了分割精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 基础模型在复杂、低对比度背景下（如恶性肿瘤与正常器官相似时）难以有效区分前景区域，导致上下文区分困难，从而影响肿瘤分割的准确性，而准确的肿瘤分割对于手术规划和肿瘤分期至关重要。

Method: 本文提出前景感知谱分割（FASS）框架。首先，引入前景感知模块以增强背景与整个体积空间的区分，使模型更有效地聚焦目标区域。其次，基于小波变换的特征级频率增强模块被用于提取区分性高频特征，以增强边界识别和细节感知。最后，引入边缘约束模块来保持分割边界的几何连续性。

Result: 在多个医学数据集上的广泛实验表明，FASS框架在所有评估指标上均表现出卓越性能，验证了其有效性，尤其是在复杂条件下的鲁棒性和精细结构识别能力。该框架显著提升了低对比度图像的分割效果。

Conclusion: FASS框架显著增强了低对比度医学图像的分割效果，其在复杂条件下的鲁棒性和精细结构识别能力为在更多样化和复杂的医学成像场景中的应用铺平了道路。

Abstract: Accurate segmentation of tumors and adjacent normal tissues in medical images
is essential for surgical planning and tumor staging. Although foundation
models generally perform well in segmentation tasks, they often struggle to
focus on foreground areas in complex, low-contrast backgrounds, where some
malignant tumors closely resemble normal organs, complicating contextual
differentiation. To address these challenges, we propose the Foreground-Aware
Spectrum Segmentation (FASS) framework. First, we introduce a foreground-aware
module to amplify the distinction between background and the entire volume
space, allowing the model to concentrate more effectively on target areas.
Next, a feature-level frequency enhancement module, based on wavelet transform,
extracts discriminative high-frequency features to enhance boundary recognition
and detail perception. Eventually, we introduce an edge constraint module to
preserve geometric continuity in segmentation boundaries. Extensive experiments
on multiple medical datasets demonstrate superior performance across all
metrics, validating the effectiveness of our framework, particularly in
robustness under complex conditions and fine structure recognition. Our
framework significantly enhances segmentation of low-contrast images, paving
the way for applications in more diverse and complex medical imaging scenarios.

</details>


### [227] [COCO-Tree: Compositional Hierarchical Concept Trees for Enhanced Reasoning in Vision Language Models](https://arxiv.org/abs/2510.11012)
*Sanchit Sinha,Guangzhi Xiong,Aidong Zhang*

Main category: cs.CV

TL;DR: COCO-Tree是一种新颖的方法，通过利用LLM学习的神经符号概念树来增强VLM输出，从而显著提高VLM的组合推理能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型（VLM）在组合推理方面存在固有弱点，难以理解图像中多个对象、属性和关系之间的复杂交互。现有方法（如改进提示结构、思维链推理或结合LLM）要么资源密集，要么缺乏可解释的推理过程。

Method: 本文提出“COCO-Tree”方法，该方法使用从大型语言模型（LLM）学习到的精心设计的神经符号概念树来增强VLM的输出。COCO-Tree采用受束搜索启发的推理过程，旨在提升组合性表现并为VLM预测提供理由。

Result: 在Winoground、EqBench、ColorSwap和SugarCrepe这四个组合性基准测试上，对七种不同规模的开源VLM进行评估，结果表明COCO-Tree将组合泛化能力比基线显著提高了5-10%。此外，它还提供了VLM预测背后的可解释理由。

Conclusion: COCO-Tree通过结合LLM学习的神经符号概念树，有效解决了VLM在组合推理方面的不足，显著提升了模型性能和可解释性，为VLM的语言推理能力带来了改进。

Abstract: Compositional reasoning remains a persistent weakness of modern vision
language models (VLMs): they often falter when a task hinges on understanding
how multiple objects, attributes, and relations interact within an image.
Multiple research works have attempted to improve compositionality performance
by creative tricks such as improving prompt structure, chain of thought
reasoning, etc. A more recent line of work attempts to impart additional
reasoning in VLMs using well-trained Large Language Models (LLMs), which are
far superior in linguistic understanding than VLMs to compensate for the
limited linguistic prowess of VLMs. However, these approaches are either
resource-intensive or do not provide an interpretable reasoning process. In
this paper, we present 'COCO-Tree' - a novel approach that augments VLM outputs
with carefully designed neurosymbolic concept trees learned from LLMs to
improve VLM's linguistic reasoning. COCO-Tree's beam search-inspired reasoning
process boosts compositionality performance and provides a rationale behind VLM
predictions. Empirical results on four compositionality benchmarks, Winoground,
EqBench, ColorSwap, and SugarCrepe, in seven different open-source VLMs with
varying sizes, demonstrate that COCO-Tree significantly improves compositional
generalization by 5-10% over baselines.

</details>


### [228] [LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference](https://arxiv.org/abs/2510.11512)
*Jianhao Yuan,Fabio Pizzati,Francesco Pinto,Lars Kunze,Ivan Laptev,Paul Newman,Philip Torr,Daniele De Martini*

Main category: cs.CV

TL;DR: 该研究引入了一种名为LikePhys的免训练方法，用于评估视频扩散模型中的直观物理理解能力，通过区分物理上有效和不可能的视频，并构建基准和指标来系统性地测试和分析当前模型的表现。


<details>
  <summary>Details</summary>
Motivation: 在视频生成中，准确评估视频扩散模型对直观物理的理解能力是一项挑战，因为很难将物理正确性与视觉外观分离。

Method: 该研究提出LikePhys，一种免训练方法，利用去噪目标作为基于ELBO的似然替代，在一个精心策划的有效-无效视频对数据集上区分物理上有效和不可能的视频。他们构建了一个包含四个物理领域、十二个场景的基准，并引入了评估指标“合理性偏好错误”（PPE）。

Result: PPE评估指标与人类偏好高度一致，优于现有评估基线。系统性基准测试显示，尽管当前模型在复杂和混沌动力学方面仍面临挑战，但随着模型容量和推理设置的扩展，物理理解能力呈现出明显的改进趋势。研究还分析了模型设计和推理设置如何影响物理理解，并突出了不同物理定律下的领域特定能力差异。

Conclusion: LikePhys提供了一种有效评估视频扩散模型直观物理理解能力的方法。尽管当前模型在处理复杂物理时仍有局限性，但随着模型扩展，其物理理解能力正在持续提升，为构建更具物理合理性的世界模拟器奠定了基础。

Abstract: Intuitive physics understanding in video diffusion models plays an essential
role in building general-purpose physically plausible world simulators, yet
accurately evaluating such capacity remains a challenging task due to the
difficulty in disentangling physics correctness from visual appearance in
generation. To the end, we introduce LikePhys, a training-free method that
evaluates intuitive physics in video diffusion models by distinguishing
physically valid and impossible videos using the denoising objective as an
ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By
testing on our constructed benchmark of twelve scenarios spanning over four
physics domains, we show that our evaluation metric, Plausibility Preference
Error (PPE), demonstrates strong alignment with human preference, outperforming
state-of-the-art evaluator baselines. We then systematically benchmark
intuitive physics understanding in current video diffusion models. Our study
further analyses how model design and inference settings affect intuitive
physics understanding and highlights domain-specific capacity variations across
physical laws. Empirical results show that, despite current models struggling
with complex and chaotic dynamics, there is a clear trend of improvement in
physics understanding as model capacity and inference settings scale.

</details>


### [229] [High-Resolution Spatiotemporal Modeling with Global-Local State Space Models for Video-Based Human Pose Estimation](https://arxiv.org/abs/2510.11017)
*Runyang Feng,Hyung Jin Chang,Tze Ho Elden Tse,Boeun Kim,Yi Chang,Yixing Gao*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba的新型框架，通过分别学习全局和局部高分辨率时空表示来改进视频人体姿态估计（VHPE），解决了现有方法在平衡全局/局部动态建模和处理高分辨率序列时的二次复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 现有VHPE方法难以在单一建模结构（卷积或注意力）中平衡全局动态上下文和局部运动细节，可能导致次优性能。此外，它们在捕获全局依赖时存在二次复杂度，限制了在高分辨率序列上的应用。尽管Mamba在建模长距离上下文方面具有线性复杂度潜力，但其仅限于一维序列数据。

Method: 本研究将Mamba从两个方面进行扩展：1) 提出了“全局时空Mamba”，通过6D选择性时空扫描和空间/时间调制扫描合并来高效提取高分辨率序列的全局表示；2) 引入了基于窗口时空扫描的“局部细化Mamba”，以增强局部关键点运动的高频细节。

Result: 在四个基准数据集上的大量实验表明，所提出的模型优于现有的最先进VHPE方法，并实现了更好的计算权衡。

Conclusion: 该论文成功地将Mamba扩展到高分辨率时空数据，并为VHPE提供了一个新颖的框架，有效解决了全局与局部动态建模的平衡以及高分辨率序列的计算效率问题，从而提升了性能。

Abstract: Modeling high-resolution spatiotemporal representations, including both
global dynamic contexts (e.g., holistic human motion tendencies) and local
motion details (e.g., high-frequency changes of keypoints), is essential for
video-based human pose estimation (VHPE). Current state-of-the-art methods
typically unify spatiotemporal learning within a single type of modeling
structure (convolution or attention-based blocks), which inherently have
difficulties in balancing global and local dynamic modeling and may bias the
network to one of them, leading to suboptimal performance. Moreover, existing
VHPE models suffer from quadratic complexity when capturing global
dependencies, limiting their applicability especially for high-resolution
sequences. Recently, the state space models (known as Mamba) have demonstrated
significant potential in modeling long-range contexts with linear complexity;
however, they are restricted to 1D sequential data. In this paper, we present a
novel framework that extends Mamba from two aspects to separately learn global
and local high-resolution spatiotemporal representations for VHPE.
Specifically, we first propose a Global Spatiotemporal Mamba, which performs 6D
selective space-time scan and spatial- and temporal-modulated scan merging to
efficiently extract global representations from high-resolution sequences. We
further introduce a windowed space-time scan-based Local Refinement Mamba to
enhance the high-frequency details of localized keypoint motions. Extensive
experiments on four benchmark datasets demonstrate that the proposed model
outperforms state-of-the-art VHPE approaches while achieving better
computational trade-offs.

</details>


### [230] [GIR-Bench: Versatile Benchmark for Generating Images with Reasoning](https://arxiv.org/abs/2510.11026)
*Hongxiang Li,Yaowei Li,Bin Lin,Yuwei Niu,Yuhang Yang,Xiaoshuang Huang,Jiayin Cai,Xiaolong Jiang,Yao Hu,Long Chen*

Main category: cs.CV

TL;DR: 本文介绍了GIR-Bench，一个用于评估统一多模态模型在理解与生成一致性、推理驱动的文本到图像生成以及多步编辑方面的综合基准。研究发现，尽管统一模型在推理驱动的视觉任务上表现更优，但在理解和生成之间仍存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前的统一多模态模型缺乏一个严格的、以推理为中心的基准，来系统地评估其理解与生成之间的对齐程度，以及它们在复杂视觉任务中的泛化潜力。

Method: 本文引入了GIR-Bench，一个从三个互补角度评估统一模型的综合基准：1) 理解-生成一致性（GIR-Bench-UGC），评估模型是否能在理解和生成任务中一致地利用相同知识；2) 推理驱动的文本到图像生成（GIR-Bench-T2I），要求模型应用逻辑约束和隐性知识生成忠实视觉内容；3) 编辑中的多步推理（GIR-Bench-Edit）。每个子集都设计了特定的评估流程，以实现细粒度和可解释的评估，并减少传统MLLM-as-a-Judge范式的偏见。

Result: 广泛的消融实验表明：尽管统一模型在推理驱动的视觉任务上比仅生成系统更具能力，但它们在理解和生成之间仍然存在持续的差距。

Conclusion: GIR-Bench提供了一个严格的基准来评估统一多模态模型。研究结果表明，尽管这些模型在推理能力上有所提升，但在理解和生成之间的一致性方面仍有待提高，这为未来的研究指明了方向。

Abstract: Unified multimodal models integrate the reasoning capacity of large language
models with both image understanding and generation, showing great promise for
advanced multimodal intelligence. However, the community still lacks a rigorous
reasoning-centric benchmark to systematically evaluate the alignment between
understanding and generation, and their generalization potential in complex
visual tasks. To this end, we introduce \textbf{GIR-Bench}, a comprehensive
benchmark that evaluates unified models across three complementary
perspectives. Firstly, we investigate understanding-generation consistency
(GIR-Bench-UGC), asking whether models can consistently leverage the same
knowledge in both understanding and generation tasks. Secondly, we investigate
whether models can perform reasoning-centric text-to-image generation that
requires applying logical constraints and implicit knowledge to generate
faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models
can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset,
we carefully design different task-specific evaluation pipelines tailored for
each task. This enables fine-grained and interpretable evaluation while
mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive
ablations over various unified models and generation-only systems have shown
that: Although unified models are more capable of reasoning-driven visual
tasks, they still exhibit a persistent gap between understanding and
generation. The data and code for GIR-Bench are available at
\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.

</details>


### [231] [Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning](https://arxiv.org/abs/2510.11027)
*Ganlin Yang,Tianyi Zhang,Haoran Hao,Weiyun Wang,Yibin Liu,Dehui Wang,Guanzhou Chen,Zijian Cai,Junting Chen,Weijie Su,Wengang Zhou,Yu Qiao,Jifeng Dai,Jiangmiao Pang,Gen Luo,Wenhai Wang,Yao Mu,Zhi Hou*

Main category: cs.CV

TL;DR: 本研究引入了Vlaser，一个视觉-语言-动作模型，旨在弥合VLM推理与VLA策略学习之间的鸿沟，并在具身推理和机器人控制任务上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中于开发具身推理能力或将VLM集成到VLA模型中进行端到端机器人控制，但很少有研究直接解决上游基于VLM的推理与下游VLA策略学习之间的关键差距。

Method: 本研究提出了Vlaser，一个具有协同具身推理能力的视觉-语言-动作模型。该模型基于高质量的Vlaser-6M数据集构建。此外，系统地研究了不同VLM初始化如何影响监督VLA微调，以减轻互联网规模预训练数据与具身特定策略学习数据之间的领域漂移。

Result: Vlaser在多种具身推理基准（包括空间推理、具身基础、具身问答和任务规划）上取得了最先进的性能。通过对VLM初始化的研究，提供了减轻领域漂移的新见解。基于这些见解，Vlaser在WidowX基准上取得了最先进的结果，并在Google Robot基准上取得了有竞争力的性能。

Conclusion: Vlaser成功地弥合了具身推理与VLA策略学习之间的差距，在具身推理和机器人控制任务上表现出色，并为缓解互联网数据与具身数据之间的领域漂移提供了宝贵见解。

Abstract: While significant research has focused on developing embodied reasoning
capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs
into Vision-Language-Action (VLA) models for end-to-end robot control, few
studies directly address the critical gap between upstream VLM-based reasoning
and downstream VLA policy learning. In this work, we take an initial step
toward bridging embodied reasoning with VLA policy learning by introducing
Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning
capability, which is a foundational vision-language model designed to integrate
high-level reasoning with low-level control for embodied agents. Built upon the
high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance
across a range of embodied reasoning benchmarks - including spatial reasoning,
embodied grounding, embodied QA, and task planning. Furthermore, we
systematically examine how different VLM initializations affect supervised VLA
fine-tuning, offering novel insights into mitigating the domain shift between
internet-scale pre-training data and embodied-specific policy learning data.
Based on these insights, our approach achieves state-of-the-art results on the
WidowX benchmark and competitive performance on the Google Robot benchmark.

</details>


### [232] [Enhancing Zero-Shot Anomaly Detection: CLIP-SAM Collaboration with Cascaded Prompts](https://arxiv.org/abs/2510.11028)
*Yanning Hou,Ke Xu,Junfa Li,Yanran Ruan,Jianfeng Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的两阶段框架，用于工业零样本异常分割任务，通过协同利用CLIP和SAM的能力，实现了精确的异常区域分割。


<details>
  <summary>Details</summary>
Motivation: 基础模型在零样本异常分割任务中展现出强大的泛化能力，但如何正确引导这些模型以解决下游任务仍然是一个挑战。

Method: 本文提出了一个两阶段框架：1. 协同特征点提示生成（PPG）模块：利用CLIP和SAM生成正负点提示，引导SAM专注于分割异常区域，而非整个物体，以减轻SAM对物体分割的倾向。2. SAM级联提示（CPS）模块：采用混合提示与SAM的轻量级解码器级联，进一步优化SAM的分割结果，解决边界粗糙和孤立噪声问题，实现精确的异常区域分割。

Result: 该方法在多个数据集上取得了最先进的零样本异常分割结果。特别是在Visa数据集上，其在$F_1$-max和AP指标上分别超越现有最佳方法10.3%和7.7%。

Conclusion: 所提出的两阶段框架，通过PPG和CPS模块，成功地利用了CLIP的异常定位能力和SAM的边界感知能力，实现了工业零样本异常分割的精确和最先进的性能。

Abstract: Recently, the powerful generalization ability exhibited by foundation models
has brought forth new solutions for zero-shot anomaly segmentation tasks.
However, guiding these foundation models correctly to address downstream tasks
remains a challenge. This paper proposes a novel two-stage framework, for
zero-shot anomaly segmentation tasks in industrial anomaly detection. This
framework excellently leverages the powerful anomaly localization capability of
CLIP and the boundary perception ability of SAM.(1) To mitigate SAM's
inclination towards object segmentation, we propose the Co-Feature Point Prompt
Generation (PPG) module. This module collaboratively utilizes CLIP and SAM to
generate positive and negative point prompts, guiding SAM to focus on
segmenting anomalous regions rather than the entire object. (2) To further
optimize SAM's segmentation results and mitigate rough boundaries and isolated
noise, we introduce the Cascaded Prompts for SAM (CPS) module. This module
employs hybrid prompts cascaded with a lightweight decoder of SAM, achieving
precise segmentation of anomalous regions. Across multiple datasets, consistent
experimental validation demonstrates that our approach achieves
state-of-the-art zero-shot anomaly segmentation results. Particularly
noteworthy is our performance on the Visa dataset, where we outperform the
state-of-the-art methods by 10.3\% and 7.7\% in terms of {$F_1$-max} and AP
metrics, respectively.

</details>


### [233] [EvoCAD: Evolutionary CAD Code Generation with Vision Language Models](https://arxiv.org/abs/2510.11631)
*Tobias Preintner,Weixuan Yuan,Adrian König,Thomas Bäck,Elena Raponi,Niki van Stein*

Main category: cs.CV

TL;DR: EvoCAD结合视觉语言模型和进化算法，通过符号表示生成CAD对象，在CADPrompt基准测试中表现优于现有方法，尤其在生成拓扑正确对象方面，并引入了基于欧拉特征的新度量。


<details>
  <summary>Details</summary>
Motivation: 结合大型语言模型（LLMs）的生成和上下文学习能力与进化算法的优势，是一个有前景的研究方向。特别是在计算机辅助设计（CAD）领域，需要有效的方法来生成复杂的CAD对象。

Method: 本文提出了EvoCAD方法，通过符号表示生成CAD对象。该方法首先采样多个CAD对象，然后使用视觉语言模型（VLM）和推理语言模型进行进化优化。评估使用了GPT-4V和GPT-4o，在CADPrompt基准数据集上与现有方法进行比较。此外，引入了两个基于欧拉特征的拓扑属性新度量，用于捕捉3D对象间的语义相似性。

Result: EvoCAD在多个指标上优于现有方法，特别是在生成拓扑正确的对象方面表现突出。新引入的两个度量可以有效评估拓扑正确性，并补充了现有的空间度量。

Conclusion: EvoCAD是一种结合VLM和进化优化来生成CAD对象的有效方法，在生成拓扑正确对象方面表现卓越，并通过创新的拓扑度量进一步提升了评估效率和准确性。

Abstract: Combining large language models with evolutionary computation algorithms
represents a promising research direction leveraging the remarkable generative
and in-context learning capabilities of LLMs with the strengths of evolutionary
algorithms. In this work, we present EvoCAD, a method for generating
computer-aided design (CAD) objects through their symbolic representations
using vision language models and evolutionary optimization. Our method samples
multiple CAD objects, which are then optimized using an evolutionary approach
with vision language and reasoning language models. We assess our method using
GPT-4V and GPT-4o, evaluating it on the CADPrompt benchmark dataset and
comparing it to prior methods. Additionally, we introduce two new metrics based
on topological properties defined by the Euler characteristic, which capture a
form of semantic similarity between 3D objects. Our results demonstrate that
EvoCAD outperforms previous approaches on multiple metrics, particularly in
generating topologically correct objects, which can be efficiently evaluated
using our two novel metrics that complement existing spatial metrics.

</details>


### [234] [NV3D: Leveraging Spatial Shape Through Normal Vector-based 3D Object Detection](https://arxiv.org/abs/2510.11632)
*Krittin Chaowakarn,Paramin Sangwongngam,Nang Htet Htet Aung,Chalie Charoenlarpnopparut*

Main category: cs.CV

TL;DR: 本文提出了一种名为NV3D的新型3D目标检测模型，该模型利用基于KNN和PCA计算的体素法向量作为局部特征，并结合注意力融合机制，以提高自动驾驶车辆在LiDAR点云中的检测性能，同时引入两种采样策略以提高效率。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶3D目标检测方法面临挑战：多模态方法存在特征对齐困难，而LiDAR点云中提取的局部特征可能过于简化，不足以处理复杂的3D目标检测任务。

Method: NV3D模型通过K近邻（KNN）和主成分分析（PCA）为每个体素计算法向量，以获取丰富的局部特征。该模型还提供了两种采样策略：基于法向量密度的采样和FOV感知分箱采样，可有效减少高达55%的数据量。此外，NV3D应用了元素级注意力融合机制，将体素特征作为查询和值，法向量特征作为键。

Result: NV3D在KITTI数据集上进行了训练和验证，在汽车和骑行者检测方面表现出卓越性能。在不使用采样的情况下，NV3D在验证集上的mAP分别达到86.60%和80.18%，比基线Voxel R-CNN分别高出2.61%和4.23%。即使在过滤掉约55%体素的情况下，NV3D在汽车检测方面的mAP仍达到85.54%，比基线高出1.56%。

Conclusion: NV3D通过利用体素法向量这一信息丰富的特征，有效地确定了表面与目标实体之间的关系，显著提升了3D目标检测性能，特别适用于具有独特空间形状的物体（如汽车和骑行者）。其提出的采样策略还能在保持性能的同时大幅提高数据处理效率。

Abstract: Recent studies in 3D object detection for autonomous vehicles aim to enrich
features through the utilization of multi-modal setups or the extraction of
local patterns within LiDAR point clouds. However, multi-modal methods face
significant challenges in feature alignment, and gaining features locally can
be oversimplified for complex 3D object detection tasks. In this paper, we
propose a novel model, NV3D, which utilizes local features acquired from voxel
neighbors, as normal vectors computed per voxel basis using K-nearest neighbors
(KNN) and principal component analysis (PCA). This informative feature enables
NV3D to determine the relationship between the surface and pertinent target
entities, including cars, pedestrians, or cyclists. During the normal vector
extraction process, NV3D offers two distinct sampling strategies: normal vector
density-based sampling and FOV-aware bin-based sampling, allowing elimination
of up to 55% of data while maintaining performance. In addition, we applied
element-wise attention fusion, which accepts voxel features as the query and
value and normal vector features as the key, similar to the attention
mechanism. Our method is trained on the KITTI dataset and has demonstrated
superior performance in car and cyclist detection owing to their spatial
shapes. In the validation set, NV3D without sampling achieves 86.60% and 80.18%
mean Average Precision (mAP), greater than the baseline Voxel R-CNN by 2.61%
and 4.23% mAP, respectively. With both samplings, NV3D achieves 85.54% mAP in
car detection, exceeding the baseline by 1.56% mAP, despite roughly 55% of
voxels being filtered out.

</details>


### [235] [FACE: Faithful Automatic Concept Extraction](https://arxiv.org/abs/2510.11675)
*Dipkamal Bhusal,Michael Clifford,Sara Rampazzi,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本文提出FACE框架，通过结合非负矩阵分解（NMF）、KL散度正则化和分类器监督，实现深度神经网络中忠实且自动的概念提取，确保概念解释与模型预测一致。


<details>
  <summary>Details</summary>
Motivation: 现有自动概念发现方法未能使提取的概念与模型真实的决策过程对齐，导致解释的忠实性受损。

Method: FACE（Faithful Automatic Concept Extraction）框架通过引入Kullback-Leibler（KL）散度正则化项来增强非负矩阵分解（NMF），以确保模型原始预测与基于概念的预测之间的一致性。与仅基于编码器激活的传统方法不同，FACE在概念学习过程中融入了分类器监督，强制实现预测一致性。理论上，最小化KL散度可以限制预测分布的偏差，从而促进学习概念空间中的忠实局部线性。

Result: 在ImageNet、COCO和CelebA数据集上的系统评估表明，FACE在忠实性和稀疏性指标上均优于现有方法。

Conclusion: FACE通过引入KL散度正则化和分类器监督，成功地解决了现有概念发现方法忠实性不足的问题，提供了一种能够与模型预测过程高度对齐的忠实概念解释方法。

Abstract: Interpreting deep neural networks through concept-based explanations offers a
bridge between low-level features and high-level human-understandable
semantics. However, existing automatic concept discovery methods often fail to
align these extracted concepts with the model's true decision-making process,
thereby compromising explanation faithfulness. In this work, we propose FACE
(Faithful Automatic Concept Extraction), a novel framework that augments
Non-negative Matrix Factorization (NMF) with a Kullback-Leibler (KL) divergence
regularization term to ensure alignment between the model's original and
concept-based predictions. Unlike prior methods that operate solely on encoder
activations, FACE incorporates classifier supervision during concept learning,
enforcing predictive consistency and enabling faithful explanations. We provide
theoretical guarantees showing that minimizing the KL divergence bounds the
deviation in predictive distributions, thereby promoting faithful local
linearity in the learned concept space. Systematic evaluations on ImageNet,
COCO, and CelebA datasets demonstrate that FACE outperforms existing methods
across faithfulness and sparsity metrics.

</details>


### [236] [Benchmarking Deep Learning Models for Laryngeal Cancer Staging Using the LaryngealCT Dataset](https://arxiv.org/abs/2510.11047)
*Nivea Roy,Son Tran,Atul Sajjanhar,K. Devaraja,Prakashini Koteshwara,Yong Xiang,Divya Rao*

Main category: cs.CV

TL;DR: 本文介绍了LaryngealCT，一个包含1029个喉部CT扫描的标准化数据集，用于喉癌深度学习模型开发。该研究基准测试了多种3D深度学习架构，并提供了可解释性工具，旨在为喉部肿瘤学的AI研究提供可复现的基础。


<details>
  <summary>Details</summary>
Motivation: 喉癌影像学研究缺乏标准化的数据集，导致深度学习模型开发的可复现性不足。

Method: 研究从The Cancer Imaging Archive (TCIA)的六个集合中整合了1029个CT扫描，构建了LaryngealCT数据集。使用弱监督参数搜索框架提取了均匀的1毫米各向同性喉部感兴趣区域（VOI），并经临床专家验证。多种3D深度学习架构（3D CNN, ResNet18,50,101, DenseNet121）在早期（Tis,T1,T2）与晚期（T3,T4）以及T4与非T4分类任务上进行了基准测试。模型可解释性通过3D GradCAMs与甲状软骨叠加进行评估。

Result: LaryngealCT数据集包含1029个CT扫描。在早期与晚期分类任务中，3D CNN表现最佳（AUC-0.881, F1-macro-0.821）；在T4与非T4分类任务中，ResNet18表现最佳（AUC-0.892, F1-macro-0.646）。模型可解释性分析显示，非T4病例在软骨周围有更大的关注度，而T4预测则显示出局灶性激活。

Conclusion: LaryngealCT通过提供开源数据、预训练模型和集成的可解释性工具，为AI驱动的喉部肿瘤学研究提供了可复现的基础，以支持临床决策。

Abstract: Laryngeal cancer imaging research lacks standardised datasets to enable
reproducible deep learning (DL) model development. We present LaryngealCT, a
curated benchmark of 1,029 computed tomography (CT) scans aggregated from six
collections from The Cancer Imaging Archive (TCIA). Uniform 1 mm isotropic
volumes of interest encompassing the larynx were extracted using a weakly
supervised parameter search framework validated by clinical experts. 3D DL
architectures (3D CNN, ResNet18,50,101, DenseNet121) were benchmarked on (i)
early (Tis,T1,T2) vs. advanced (T3,T4) and (ii) T4 vs. non-T4 classification
tasks. 3D CNN (AUC-0.881, F1-macro-0.821) and ResNet18 (AUC-0.892,
F1-macro-0.646) respectively outperformed the other models in the two tasks.
Model explainability assessed using 3D GradCAMs with thyroid cartilage overlays
revealed greater peri-cartilage attention in non-T4 cases and focal activations
in T4 predictions. Through open-source data, pretrained models, and integrated
explainability tools, LaryngealCT offers a reproducible foundation for
AI-driven research to support clinical decisions in laryngeal oncology.

</details>


### [237] [Zero-shot Face Editing via ID-Attribute Decoupled Inversion](https://arxiv.org/abs/2510.11050)
*Yang Hou,Minggu Wang,Jianjun Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于ID-属性解耦反演的零样本人脸编辑方法，通过独立控制身份和属性特征，解决了现有文本引导扩散模型在人脸编辑中ID和结构一致性差的问题。


<details>
  <summary>Details</summary>
Motivation: 目前的文本引导扩散模型在通用图像编辑方面表现出色，但应用于真实人脸编辑时，往往难以保持身份（ID）和结构的一致性。

Method: 该方法提出ID-属性解耦反演（ID-Attribute Decoupled Inversion）。它将人脸表示分解为ID特征和属性特征，并将其作为联合条件来引导反演和逆向扩散过程。这种解耦允许独立控制ID和属性，从而实现强大的ID保留和结构一致性。

Result: 该方法能够确保强大的ID保留和结构一致性，同时实现精确的面部属性操作。它支持仅通过文本提示进行广泛的复杂多属性人脸编辑，无需特定区域输入，并且运行速度与DDIM反演相当。综合实验证明了其实用性和有效性。

Conclusion: 该方法提供了一种实用且有效的零样本人脸编辑解决方案，显著提高了文本引导扩散模型在真实人脸编辑中ID和结构一致性，同时保持了精确的属性控制能力。

Abstract: Recent advancements in text-guided diffusion models have shown promise for
general image editing via inversion techniques, but often struggle to maintain
ID and structural consistency in real face editing tasks. To address this
limitation, we propose a zero-shot face editing method based on ID-Attribute
Decoupled Inversion. Specifically, we decompose the face representation into ID
and attribute features, using them as joint conditions to guide both the
inversion and the reverse diffusion processes. This allows independent control
over ID and attributes, ensuring strong ID preservation and structural
consistency while enabling precise facial attribute manipulation. Our method
supports a wide range of complex multi-attribute face editing tasks using only
text prompts, without requiring region-specific input, and operates at a speed
comparable to DDIM inversion. Comprehensive experiments demonstrate its
practicality and effectiveness.

</details>


### [238] [LSVOS 2025 Challenge Report: Recent Advances in Complex Video Object Segmentation](https://arxiv.org/abs/2510.11063)
*Chang Liu,Henghui Ding,Kaining Ying,Lingyi Hong,Ning Xu,Linjie Yang,Yuchen Fan,Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han,Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Chang Soo Lim,Joonyoung Moon,Donghyeon Cho,Tingmin Li,Yixuan Li,Yang Yang,An Yan,Leilei Cao,Feng Lu,Ran Hong,Youhai Jiang,Fengjie Zhu,Yujie Xie,Hongyang Zhang,Zhihui Liu,Shihai Ruan,Quanzhu Niu,Dengxian Gong,Shihao Chen,Tao Zhang,Yikang Zhou,Haobo Yuan,Lu Qi,Xiangtai Li,Shunping Ji,Ran Hong,Feng Lu,Leilei Cao,An Yan,Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: 本报告概述了ICCV 2025的第七届大规模视频目标分割(LSVOS)挑战赛，新增了难度更大的MOSEv2赛道，并总结了新兴趋势。


<details>
  <summary>Details</summary>
Motivation: 旨在推动视频目标分割(VOS)在真实世界场景中的鲁棒性，解决小目标密集、频繁消失/重现、严重遮挡、恶劣天气/光照等挑战，并超越现有基准，提升长期一致性和泛化能力。

Method: 挑战赛包括经典VOS、参照VOS(RVOS)和新引入的复杂VOS(MOSEv2)三个赛道。MOSEv2赛道显著增加了难度，引入更具挑战性的真实场景。VOS和RVOS采用标准J、F和J&F指标，MOSEv2则采用J&F作为主要排名指标。报告总结了数据集、协议，并分析了顶尖解决方案。

Result: 报告总结了表现最佳的解决方案，并提炼出新兴趋势，例如LLM/MLLM组件和记忆感知传播在视频分割中的日益重要性。

Conclusion: 本挑战赛旨在为未来在真实环境中实现更具韧性、语言感知的视频分割指明方向。

Abstract: This report presents an overview of the 7th Large-scale Video Object
Segmentation (LSVOS) Challenge held in conjunction with ICCV 2025. Besides the
two traditional tracks of LSVOS that jointly target robustness in realistic
video scenarios: Classic VOS (VOS), and Referring VOS (RVOS), the 2025 edition
features a newly introduced track, Complex VOS (MOSEv2). Building upon prior
insights, MOSEv2 substantially increases difficulty, introducing more
challenging but realistic scenarios including denser small objects, frequent
disappear/reappear events, severe occlusions, adverse weather and lighting,
etc., pushing long-term consistency and generalization beyond curated
benchmarks. The challenge retains standard ${J}$, $F$, and ${J\&F}$ metrics for
VOS and RVOS, while MOSEv2 adopts ${J\&\dot{F}}$ as the primary ranking metric
to better evaluate objects across scales and disappearance cases. We summarize
datasets and protocols, highlight top-performing solutions, and distill
emerging trends, such as the growing role of LLM/MLLM components and
memory-aware propagation, aiming to chart future directions for resilient,
language-aware video segmentation in the wild.

</details>


### [239] [ROFI: A Deep Learning-Based Ophthalmic Sign-Preserving and Reversible Patient Face Anonymizer](https://arxiv.org/abs/2510.11073)
*Yuan Tian,Min Zhou,Yitong Chen,Fang Li,Lingzi Qi,Shuo Wang,Xieyang Xu,Yu Yu,Shiqiong Xu,Chaoyu Lei,Yankai Jiang,Rongzhao Zhang,Jia Tan,Li Wu,Hong Chen,Xiaowei Liu,Wei Lu,Lin Li,Huifang Zhou,Xuefei Song,Guangtao Zhai,Xianqun Fan*

Main category: cs.CV

TL;DR: ROFI是一个基于深度学习的眼科隐私保护框架，它在匿名化患者面部图像的同时，能高精度保留疾病特征并支持诊断和安全图像逆转。


<details>
  <summary>Details</summary>
Motivation: 患者面部图像为眼科疾病评估提供了便利，但同时也引发了隐私担忧。

Method: ROFI采用深度学习、弱监督学习和神经身份转换技术，旨在匿名化面部特征，同时保留疾病特征。

Result: ROFI在匿名化超过95%图像的同时，对疾病特征保留准确率超过98%（κ > 0.90），对11种眼病实现100%诊断敏感性和高一致性（κ > 0.90）。它与AI系统兼容，保持原始诊断（κ > 0.80），并支持安全图像逆转（相似度超过98%）。

Conclusion: 这些结果表明ROFI在数字医学时代有效保护患者隐私方面的有效性。

Abstract: Patient face images provide a convenient mean for evaluating eye diseases,
while also raising privacy concerns. Here, we introduce ROFI, a deep
learning-based privacy protection framework for ophthalmology. Using weakly
supervised learning and neural identity translation, ROFI anonymizes facial
features while retaining disease features (over 98\% accuracy, $\kappa >
0.90$). It achieves 100\% diagnostic sensitivity and high agreement ($\kappa >
0.90$) across eleven eye diseases in three cohorts, anonymizing over 95\% of
images. ROFI works with AI systems, maintaining original diagnoses ($\kappa >
0.80$), and supports secure image reversal (over 98\% similarity), enabling
audits and long-term care. These results show ROFI's effectiveness of
protecting patient privacy in the digital medicine era.

</details>


### [240] [CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images](https://arxiv.org/abs/2510.11718)
*Chengqi Duan,Kaiyue Sun,Rongyao Fang,Manyuan Zhang,Yan Feng,Ying Luo,Yufang Liu,Ke Wang,Peng Pei,Xunliang Cai,Hongsheng Li,Yi Ma,Xihui Liu*

Main category: cs.CV

TL;DR: CodePlot-CoT提出了一种代码驱动的思维链范式，通过VLM生成文本推理和可执行绘图代码，并将代码渲染成图像作为“视觉思维”，以解决需要视觉辅助的数学问题。该方法构建了首个大规模双语数据集Math-VR，并在新基准上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）和视觉语言模型（VLMs）在数学推理方面取得了进展，但在需要视觉辅助（如绘制辅助线或函数图）的数学问题上仍面临瓶颈。大多数LLMs和VLMs仅限于文本推理链，而能生成交错文本和图像的多模态统一模型则缺乏必要的精度和可控性。

Method: 本研究提出了CodePlot-CoT，一个用于数学中“通过图像思考”的代码驱动思维链范式。它利用VLM生成文本推理和可执行的绘图代码，然后将代码渲染成图像作为“视觉思维”。为此，研究团队首先构建了Math-VR，一个包含17.8万样本的大规模双语数学视觉推理数据集和基准。其次，开发了一个专门用于将复杂数学图形解析为代码的先进图像到代码转换器，以创建高质量的训练数据。最后，利用这些训练数据训练了CodePlot-CoT模型来解决数学问题。

Result: 实验结果表明，CodePlot-CoT模型在新的基准测试上比基础模型提高了21%，充分验证了所提出的代码驱动推理范式的有效性。

Conclusion: 本研究的工作为多模态数学推理开辟了新方向，并为社区提供了首个大规模数据集、全面的基准和解决此类问题的强大方法。所有数据集、代码和预训练模型均已公开。

Abstract: Recent advances in Large Language Models (LLMs) and Vision Language Models
(VLMs) have shown significant progress in mathematical reasoning, yet they
still face a critical bottleneck with problems requiring visual assistance,
such as drawing auxiliary lines or plotting functions to solve the problems.
Most LLMs and VLMs are constrained to text-only reasoning chains, while
multimodal unified models that can generate interleaved text and images lack
the necessary precision and controllability for such tasks. To address this, we
propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for "thinking
with images" in mathematics. Our approach leverages the VLM to generate text
reasoning as well as executable plotting code, which is then rendered into
images as "visual thought", to solve mathematical problems. To achieve this, we
first construct Math-VR, the first large-scale, bilingual dataset and benchmark
for Mathematics problems with Visual Reasoning, comprising 178K samples.
Second, to create high-quality training data, we develop a state-of-the-art
image-to-code converter specialized for parsing complex mathematical figures
into codes. Finally, using these training data, we train the CodePlot-CoT model
for solving mathematical problems. Experimental results show that our model
achieves up to 21% increase over base model on our new benchmark, fully
validating the efficacy of our proposed code-driven reasoning paradigm. Our
work opens a new direction for multimodal mathematical reasoning and provides
the community with the first large-scale dataset, comprehensive benchmark, and
strong approach for such problems. To facilitate future research, we make our
datasets, code, and pretrained models publicly available at
https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.

</details>


### [241] [Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution](https://arxiv.org/abs/2510.11092)
*Bozhou Zhang,Nan Song,Jingyu Li,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: SeerDrive是一种新颖的端到端自动驾驶框架，它通过闭环方式联合建模未来场景演变和轨迹规划，显著优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶方法过度依赖当前场景上下文，低估了场景动态及其时间演变的重要性，限制了其在复杂场景中做出适应性决策的能力。研究者认为车辆未来轨迹与环境动态演变紧密相关，且车辆自身未来状态也能影响场景发展，受此双向关系启发。

Method: 本文提出了SeerDrive框架，它以闭环方式联合建模未来场景演变和轨迹规划。该方法首先预测未来的鸟瞰图（BEV）表示以预见周围场景的动态，然后利用这种预见生成未来上下文感知的轨迹。关键组件包括：1) 未来感知规划，将预测的BEV特征注入轨迹规划器；2) 迭代场景建模和车辆规划，通过协同优化来改进未来场景预测和轨迹生成。

Result: 在NAVSIM和nuScenes基准测试上进行的广泛实验表明，SeerDrive显著优于现有的最先进方法。

Conclusion: 通过联合建模未来场景演变和轨迹规划，SeerDrive克服了传统端到端自动驾驶方法的局限性，实现了更具信息性和适应性的决策，并在关键基准上展现出卓越的性能。

Abstract: End-to-end autonomous driving methods aim to directly map raw sensor inputs
to future driving actions such as planned trajectories, bypassing traditional
modular pipelines. While these approaches have shown promise, they often
operate under a one-shot paradigm that relies heavily on the current scene
context, potentially underestimating the importance of scene dynamics and their
temporal evolution. This limitation restricts the model's ability to make
informed and adaptive decisions in complex driving scenarios. We propose a new
perspective: the future trajectory of an autonomous vehicle is closely
intertwined with the evolving dynamics of its environment, and conversely, the
vehicle's own future states can influence how the surrounding scene unfolds.
Motivated by this bidirectional relationship, we introduce SeerDrive, a novel
end-to-end framework that jointly models future scene evolution and trajectory
planning in a closed-loop manner. Our method first predicts future bird's-eye
view (BEV) representations to anticipate the dynamics of the surrounding scene,
then leverages this foresight to generate future-context-aware trajectories.
Two key components enable this: (1) future-aware planning, which injects
predicted BEV features into the trajectory planner, and (2) iterative scene
modeling and vehicle planning, which refines both future scene prediction and
trajectory generation through collaborative optimization. Extensive experiments
on the NAVSIM and nuScenes benchmarks show that SeerDrive significantly
outperforms existing state-of-the-art methods.

</details>


### [242] [CoDefend: Cross-Modal Collaborative Defense via Diffusion Purification and Prompt Optimization](https://arxiv.org/abs/2510.11096)
*Fengling Zhu,Boshi Liu,Jingyu Hua,Sheng Zhong*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）易受对抗性攻击，现有防御方法存在局限。本文提出一种基于监督扩散的去噪框架，结合提示优化，有效提升了视觉模态的防御鲁棒性和对未知攻击的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在整合视觉和文本模态的同时，也容易受到对抗性攻击，攻击者可以通过扰动单一或两种模态来诱导有害或误导性输出。现有防御策略如对抗训练计算成本高且对已知攻击鲁棒性有限；传统净化方法则常导致图像质量下降且泛化能力不足，尤其是在复杂多模态任务中。因此，需要更有效、高质量的防御机制。

Method: 本文专注于防御视觉模态，提出了一种基于监督扩散的去噪框架。该方法利用成对的对抗性-干净图像数据集，通过方向性、任务特定的引导来微调扩散模型。与无监督净化方法不同，它旨在实现更高质量的重建。此外，还引入了提示优化作为补充防御机制，以增强对多样化和未知攻击策略的抵抗力。

Result: 在图像字幕和视觉问答任务上的大量实验表明，所提出的方法不仅显著提高了鲁棒性，而且对未知对抗性攻击表现出强大的可迁移性。与先前的无监督净化方法（如DiffPure）相比，该方法实现了更高质量的重建。

Conclusion: 基于监督扩散的去噪框架对于多模态防御是有效的，为多模态大语言模型在实际应用中更可靠和安全的部署铺平了道路。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable success in
tasks such as image captioning, visual question answering, and cross-modal
reasoning by integrating visual and textual modalities. However, their
multimodal nature also exposes them to adversarial threats, where attackers can
perturb either modality or both jointly to induce harmful, misleading, or
policy violating outputs. Existing defense strategies, such as adversarial
training and input purification, face notable limitations: adversarial training
typically improves robustness only against known attacks while incurring high
computational costs, whereas conventional purification approaches often suffer
from degraded image quality and insufficient generalization to complex
multimodal tasks.
  In this work, we focus on defending the visual modality, which frequently
serves as the primary entry point for adversarial manipulation. We propose a
supervised diffusion based denoising framework that leverages paired
adversarial clean image datasets to fine-tune diffusion models with
directional, task specific guidance. Unlike prior unsupervised purification
methods such as DiffPure, our approach achieves higher quality reconstructions
while significantly improving defense robustness in multimodal tasks.
Furthermore, we incorporate prompt optimization as a complementary defense
mechanism, enhancing resistance against diverse and unseen attack strategies.
  Extensive experiments on image captioning and visual question answering
demonstrate that our method not only substantially improves robustness but also
exhibits strong transferability to unknown adversarial attacks. These results
highlight the effectiveness of supervised diffusion based denoising for
multimodal defense, paving the way for more reliable and secure deployment of
MLLMs in real world applications.

</details>


### [243] [Compositional Zero-Shot Learning: A Survey](https://arxiv.org/abs/2510.11106)
*Ans Munir,Faisal Z. Qureshi,Mohsen Ali,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 这篇综述是首个全面关注组合式零样本学习（CZSL）的论文，系统回顾了现有方法，提出了基于解耦的分类法，并分析了其优缺点及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 组合式零样本学习（CZSL）在计算机视觉中至关重要，它使模型能够识别未见过的属性与物体组合，解决了需要为每种组合提供训练数据的组合爆炸问题。其主要挑战在于基元（如属性和物体）的视觉表现具有高度语境性，例如“小”猫与“老”猫、或“湿”车与“湿”猫的视觉差异巨大，因此有效建模这种语境性和固有的组合性对于鲁棒的零样本识别至关重要。

Method: 本文系统地回顾了最新的CZSL方法，并引入了一种基于解耦的分类法，将其分为四大家族：无显式解耦、文本解耦、视觉解耦和跨模态解耦。论文还对这些方法进行了详细的比较分析，突出它们在不同问题设置（如封闭世界和开放世界CZSL）中的核心优势和局限性。

Result: 该综述提出了一个基于解耦的CZSL方法分类法，包括无显式解耦、文本解耦、视觉解耦和跨模态解耦四大类。它详细比较了这些方法的优缺点，并识别出该领域最重大的开放挑战。

Conclusion: 该综述旨在作为CZSL领域的基础资源，以指导和启发未来的研究进展，推动这一重要领域的发展。

Abstract: Compositional Zero-Shot Learning (CZSL) is a critical task in computer vision
that enables models to recognize unseen combinations of known attributes and
objects during inference, addressing the combinatorial challenge of requiring
training data for every possible composition. This is particularly challenging
because the visual appearance of primitives is highly contextual; for example,
``small'' cats appear visually distinct from ``older'' ones, and ``wet'' cars
differ significantly from ``wet'' cats. Effectively modeling this contextuality
and the inherent compositionality is crucial for robust compositional zero-shot
recognition. This paper presents, to our knowledge, the first comprehensive
survey specifically focused on Compositional Zero-Shot Learning. We
systematically review the state-of-the-art CZSL methods, introducing a taxonomy
grounded in disentanglement, with four families of approaches: no explicit
disentanglement, textual disentanglement, visual disentanglement, and
cross-modal disentanglement. We provide a detailed comparative analysis of
these methods, highlighting their core advantages and limitations in different
problem settings, such as closed-world and open-world CZSL. Finally, we
identify the most significant open challenges and outline promising future
research directions. This survey aims to serve as a foundational resource to
guide and inspire further advancements in this fascinating and important field.
Papers studied in this survey with their official code are available on our
github: https://github.com/ans92/Compositional-Zero-Shot-Learning

</details>


### [244] [MoMaps: Semantics-Aware Scene Motion Generation with Motion Maps](https://arxiv.org/abs/2510.11107)
*Jiahui Lei,Kyle Genova,George Kopanas,Noah Snavely,Leonidas Guibas*

Main category: cs.CV

TL;DR: 本文提出了一种像素对齐的运动图（MoMap）表示，并从大量真实视频中构建数据库，训练扩散模型来学习3D场景运动先验，从而实现从单张图像预测未来3D运动。


<details>
  <summary>Details</summary>
Motivation: 从真实视频中学习具有语义和功能意义的3D运动先验，以实现从单张输入图像预测未来3D场景运动是一个挑战。

Method: 1. 提出了一种新颖的像素对齐运动图（MoMap）表示，可由现有生成图像模型生成。2. 从超过50,000个真实视频中创建了一个大规模MoMap数据库。3. 在这些MoMap表示上训练了一个扩散模型。4. 提出了一种新的2D视频合成流程：先生成MoMap，然后相应地扭曲图像并完成基于点的渲染。

Result: 实验结果表明，该方法能够生成合理且语义一致的3D场景运动。

Conclusion: 该研究成功地学习了3D运动先验，实现了从单张图像预测3D运动，并为2D视频合成提供了一种新颖的流程。

Abstract: This paper addresses the challenge of learning semantically and functionally
meaningful 3D motion priors from real-world videos, in order to enable
prediction of future 3D scene motion from a single input image. We propose a
novel pixel-aligned Motion Map (MoMap) representation for 3D scene motion,
which can be generated from existing generative image models to facilitate
efficient and effective motion prediction. To learn meaningful distributions
over motion, we create a large-scale database of MoMaps from over 50,000 real
videos and train a diffusion model on these representations. Our motion
generation not only synthesizes trajectories in 3D but also suggests a new
pipeline for 2D video synthesis: first generate a MoMap, then warp an image
accordingly and complete the warped point-based renderings. Experimental
results demonstrate that our approach generates plausible and semantically
consistent 3D scene motion.

</details>


### [245] [Multimodal Disease Progression Modeling via Spatiotemporal Disentanglement and Multiscale Alignment](https://arxiv.org/abs/2510.11112)
*Chen Liu,Wenfang Yao,Kejing Yin,William K. Cheung,Jing Qin*

Main category: cs.CV

TL;DR: 本文提出了一个名为 $	exttt{DiPro}$ 的新框架，通过区域感知解耦和多时间尺度对齐，解决了纵向多模态数据（如电子健康记录和连续胸部X光片）在疾病进展建模中面临的冗余和时间错位问题，并在MIMIC数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 纵向多模态数据对于建模疾病进展至关重要，但由于两个关键挑战而未被充分利用：1) 连续胸部X光片序列中存在冗余，静态解剖区域掩盖了临床有意义的动态变化；2) 稀疏、不规则的影像数据与连续的电子健康记录数据之间存在时间错位。

Method: 本文引入了 $	exttt{DiPro}$ 框架来解决这些挑战。首先，它在连续胸部X光片中解耦静态（解剖）和动态（病理进展）特征，优先关注与疾病相关的变化。其次，它通过局部（成对间隔级别）和全局（全序列）同步，将这些静态和动态胸部X光片特征与异步电子健康记录数据进行分层对齐，以建模连贯的进展路径。

Result: 在MIMIC数据集上进行的广泛实验表明，$	exttt{DiPro}$ 能够有效地提取时间临床动态，并在疾病进展识别和通用ICU预测任务上均取得了最先进的性能。

Conclusion: $	exttt{DiPro}$ 框架通过创新的区域感知解耦和多时间尺度对齐方法，成功克服了纵向多模态数据在疾病进展建模中的挑战，为临床动态提取和预测提供了有效且高性能的解决方案。

Abstract: Longitudinal multimodal data, including electronic health records (EHR) and
sequential chest X-rays (CXRs), is critical for modeling disease progression,
yet remains underutilized due to two key challenges: (1) redundancy in
consecutive CXR sequences, where static anatomical regions dominate over
clinically-meaningful dynamics, and (2) temporal misalignment between sparse,
irregular imaging and continuous EHR data. We introduce $\texttt{DiPro}$, a
novel framework that addresses these challenges through region-aware
disentanglement and multi-timescale alignment. First, we disentangle static
(anatomy) and dynamic (pathology progression) features in sequential CXRs,
prioritizing disease-relevant changes. Second, we hierarchically align these
static and dynamic CXR features with asynchronous EHR data via local (pairwise
interval-level) and global (full-sequence) synchronization to model coherent
progression pathways. Extensive experiments on the MIMIC dataset demonstrate
that $\texttt{DiPro}$ could effectively extract temporal clinical dynamics and
achieve state-of-the-art performance on both disease progression identification
and general ICU prediction tasks.

</details>


### [246] [Connecting Giants: Synergistic Knowledge Transfer of Large Multimodal Models for Few-Shot Learning](https://arxiv.org/abs/2510.11115)
*Hao Tang,Shengfeng He,Jing Qin*

Main category: cs.CV

TL;DR: 本文提出SynTrans框架，通过从大型多模态模型中协同转移知识，显著提升了少样本学习（FSL）的性能，解决了现有方法中知识转移引入噪声和偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 少样本学习（FSL）在数据稀缺的新类别分类中面临挑战。现有方法虽然尝试利用小型模型的语义知识，但由于数据固有的简单性，这些方法常常引入噪声和偏差。

Method: SynTrans框架利用CLIP作为强大的教师模型，少样本视觉编码器作为弱学生模型，通过无监督代理任务蒸馏语义对齐的视觉知识。接着，一个免训练的协同知识挖掘模块促进大型多模态模型间的协作，提取高质量的语义知识。然后，一个视觉-语义桥接模块实现视觉和语义空间之间的双向知识转移，将显式视觉和隐式语义知识转化为类别特定的分类器权重。最后，SynTrans引入视觉权重生成器和语义权重重构器来自适应地构建最优多模态FSL分类器。

Result: 在四个FSL数据集上的实验结果表明，SynTrans即使与一个简单的少样本视觉编码器结合，也显著优于当前最先进的方法。

Conclusion: SynTrans框架成功地从大型多模态模型中有效地转移了多样且互补的知识，赋能了现有的少样本学习器，并构建了最优的多模态FSL分类器，实现了卓越的性能。

Abstract: Few-shot learning (FSL) addresses the challenge of classifying novel classes
with limited training samples. While some methods leverage semantic knowledge
from smaller-scale models to mitigate data scarcity, these approaches often
introduce noise and bias due to the data's inherent simplicity. In this paper,
we propose a novel framework, Synergistic Knowledge Transfer (SynTrans), which
effectively transfers diverse and complementary knowledge from large multimodal
models to empower the off-the-shelf few-shot learner. Specifically, SynTrans
employs CLIP as a robust teacher and uses a few-shot vision encoder as a weak
student, distilling semantic-aligned visual knowledge via an unsupervised proxy
task. Subsequently, a training-free synergistic knowledge mining module
facilitates collaboration among large multimodal models to extract high-quality
semantic knowledge. Building upon this, a visual-semantic bridging module
enables bi-directional knowledge transfer between visual and semantic spaces,
transforming explicit visual and implicit semantic knowledge into
category-specific classifier weights. Finally, SynTrans introduces a visual
weight generator and a semantic weight reconstructor to adaptively construct
optimal multimodal FSL classifiers. Experimental results on four FSL datasets
demonstrate that SynTrans, even when paired with a simple few-shot vision
encoder, significantly outperforms current state-of-the-art methods.

</details>


### [247] [Validation of an Artificial Intelligence Tool for the Detection of Sperm DNA Fragmentation Using the TUNEL In Situ Hybridization Assay](https://arxiv.org/abs/2510.11142)
*Byron Alexander Jacobs,Aqeel Morris,Ifthakaar Shaik,Frando Lin*

Main category: cs.CV

TL;DR: 本研究验证了一种新型AI工具，通过数字分析相差显微镜图像来检测精子DNA碎片化（SDF），并以TUNEL检测作为金标准，实现了非破坏性、实时SDF评估。


<details>
  <summary>Details</summary>
Motivation: 精子DNA碎片化是男性生育能力评估中的关键参数，但传统精液分析无法评估。因此，需要一种能够实时、非破坏性地检测SDF的方法。

Method: 该研究提出了一种形态学辅助的集成AI模型，结合图像处理技术和基于Transformer的机器学习模型（GC-ViT），利用精子形态与DNA完整性之间的联系，从相差图像中预测DNA碎片化。该模型以TUNEL检测为金标准进行验证，并与纯Transformer视觉模型和纯形态学模型进行了基准测试。

Result: 所提出的框架取得了60%的敏感性和75%的特异性，显示出良好的性能。

Conclusion: 这种非破坏性的方法代表了生殖医学领域的重大进步，能够实现基于DNA完整性的实时精子选择，适用于临床诊断和治疗应用。

Abstract: Sperm DNA fragmentation (SDF) is a critical parameter in male fertility
assessment that conventional semen analysis fails to evaluate. This study
presents the validation of a novel artificial intelligence (AI) tool designed
to detect SDF through digital analysis of phase contrast microscopy images,
using the terminal deoxynucleotidyl transferase dUTP nick end labeling (TUNEL)
assay as the gold standard reference. Utilising the established link between
sperm morphology and DNA integrity, the present work proposes a morphology
assisted ensemble AI model that combines image processing techniques with
state-of-the-art transformer based machine learning models (GC-ViT) for the
prediction of DNA fragmentation in sperm from phase contrast images. The
ensemble model is benchmarked against a pure transformer `vision' model as well
as a `morphology-only` model. Promising results show the proposed framework is
able to achieve sensitivity of 60\% and specificity of 75\%. This
non-destructive methodology represents a significant advancement in
reproductive medicine by enabling real-time sperm selection based on DNA
integrity for clinical diagnostic and therapeutic applications.

</details>


### [248] [Demystifying Numerosity in Diffusion Models -- Limitations and Remedies](https://arxiv.org/abs/2510.11117)
*Yaqi Zhao,Xiaochen Wang,Li Dong,Wentao Zhang,Yuhui Yuan*

Main category: cs.CV

TL;DR: 文本到图像扩散模型在数字计数方面表现不佳，单纯扩大模型和数据集规模无法解决问题。研究发现模型过度依赖噪声初始化，且噪声先验存在计数偏差。论文提出通过向噪声先验注入计数感知的布局信息，显著提高了计数准确性。


<details>
  <summary>Details</summary>
Motivation: FLUX和GPT-4o等最先进的文本到图像生成模型在遵循文本提示中的计数指令时常出错。本文旨在研究一个基本但常被忽视的问题：扩散模型是否能通过简单地扩大数据集和模型规模，就能内在生成文本提示中指定数量的正确对象。

Method: 1. 构建了两个互补的合成计数基准：GrayCount250（用于受控规模研究）和NaturalCount6（包含复杂自然场景），以实现严谨和可复现的评估。2. 实证研究了规模扩大假设（即更大的模型和数据集能否提高计数准确性）。3. 分析了规模扩大无效的原因，发现扩散模型倾向于严重依赖噪声初始化，而非提示中明确指定的数字，且噪声先验存在特定对象计数的偏差。4. 提出了一种通过向噪声先验注入计数感知的布局信息来控制数字计数的有效策略。

Result: 1. 规模扩大假设不成立：单独增大模型和数据集规模未能提高基准测试上的计数准确性。2. 分析发现扩散模型严重依赖噪声初始化，且噪声先验对特定对象计数存在偏差。3. 提出的方法实现了显著的性能提升，将GrayCount250的准确率从20.0%提高到85.3%，NaturalCount6的准确率从74.8%提高到86.3%。4. 证明了该方法在不同设置下具有有效的泛化能力。

Conclusion: 扩散模型单纯通过扩大规模无法解决数字计数问题。噪声初始化在其中起着关键作用并存在偏差。通过向噪声先验注入计数感知的布局信息，可以有效控制数字计数，显著提高模型的计数准确性和泛化能力。

Abstract: Numerosity remains a challenge for state-of-the-art text-to-image generation
models like FLUX and GPT-4o, which often fail to accurately follow counting
instructions in text prompts. In this paper, we aim to study a fundamental yet
often overlooked question: Can diffusion models inherently generate the correct
number of objects specified by a textual prompt simply by scaling up the
dataset and model size? To enable rigorous and reproducible evaluation, we
construct a clean synthetic numerosity benchmark comprising two complementary
datasets: GrayCount250 for controlled scaling studies, and NaturalCount6
featuring complex naturalistic scenes. Second, we empirically show that the
scaling hypothesis does not hold: larger models and datasets alone fail to
improve counting accuracy on our benchmark. Our analysis identifies a key
reason: diffusion models tend to rely heavily on the noise initialization
rather than the explicit numerosity specified in the prompt. We observe that
noise priors exhibit biases toward specific object counts. In addition, we
propose an effective strategy for controlling numerosity by injecting
count-aware layout information into the noise prior. Our method achieves
significant gains, improving accuracy on GrayCount250 from 20.0\% to 85.3\% and
on NaturalCount6 from 74.8\% to 86.3\%, demonstrating effective generalization
across settings.

</details>


### [249] [Multiview Manifold Evidential Fusion for PolSAR Image Classification](https://arxiv.org/abs/2510.11171)
*Junfei Shi,Haojia Zhang,Haiyan Jin,Junhuai Li,Xiaogang Song,Yuanfan Guo,Haonan Su,Weisi Lin*

Main category: cs.CV

TL;DR: 本论文提出了一种多视图流形证据融合（MMEFnet）方法，用于极化合成孔径雷达（PolSAR）图像分类。该方法通过将协方差矩阵和多特征分别建模在不同的流形上，并结合证据融合理论，实现了更可靠、可解释的分类。


<details>
  <summary>Details</summary>
Motivation: 传统的PolSAR特征融合策略通常简单地拼接特征或使用深度学习，但它们忽略了协方差矩阵和多特征作为互补视图存在于不同几何结构的流形上，并且忽视了不同视图的重要性差异及不确定性，导致预测结果不可靠。

Method: MMEFnet方法将PolSAR流形学习和证据融合整合到统一架构中。具体而言，协方差矩阵在Hermitian正定（HPD）流形上表示，而多特征在Grassmann流形上建模。构建了两个不同的核度量学习网络来学习它们的流形表示。随后，采用可信的多视图证据融合（取代传统softmax分类器）来估计置信质量并量化每个视图的不确定性。最后，基于Dempster-Shafer理论的融合策略组合证据。

Result: 在三个真实世界的PolSAR数据集上进行的广泛实验表明，所提出的方法在准确性、鲁棒性和可解释性方面始终优于现有方法。

Conclusion: MMEFnet为PolSAR图像分类提供了一个新的框架，通过有效融合具有不同流形结构和不确定性的多视图，实现了更可靠和可解释的分类结果。

Abstract: Polarimetric Synthetic Aperture Radar (PolSAR) covariance matrices and their
extracted multi-features - such as scattering angle, entropy, texture, and
boundary descriptors - provide complementary and physically interpretable
information for image classification. Traditional fusion strategies typically
concatenate these features or employ deep learning networks to combine them.
However, the covariance matrices and multi-features, as two complementary
views, lie on different manifolds with distinct geometric structures. Existing
fusion methods also overlook the varying importance of different views and
ignore uncertainty, often leading to unreliable predictions. To address these
issues, we propose a Multiview Manifold Evidential Fusion (MMEFnet) method to
effectively fuse these two views. It gives a new framework to integrate PolSAR
manifold learning and evidence fusion into a unified architecture.
Specifically, covariance matrices are represented on the Hermitian Positive
Definite (HPD) manifold, while multi-features are modeled on the Grassmann
manifold. Two different kernel metric learning networks are constructed to
learn their manifold representations. Subsequently, a trusted multiview
evidence fusion, replacing the conventional softmax classifier, estimates
belief mass and quantifies the uncertainty of each view from the learned deep
features. Finally, a Dempster-Shafer theory-based fusion strategy combines
evidence, enabling a more reliable and interpretable classification. Extensive
experiments on three real-world PolSAR datasets demonstrate that the proposed
method consistently outperforms existing approaches in accuracy, robustness,
and interpretability.

</details>


### [250] [CoPRS: Learning Positional Prior from Chain-of-Thought for Reasoning Segmentation](https://arxiv.org/abs/2510.11173)
*Zhenyu Lu,Liupeng Li,Jinpeng Wang,Yan Feng,Bin Chen,Ke Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: CoPRS是一种基于多模态思维链（MCoT）的位置感知模型，通过可微分且可解释的位置先验（热图）将语言推理与分割任务连接起来，提高了可解释性并实现了更精确的掩码生成。


<details>
  <summary>Details</summary>
Motivation: 现有推理分割方法直接将语言模型隐藏特征连接到掩码解码器或表示文本位置，这限制了可解释性和语义细节。

Method: CoPRS引入了基于多模态思维链（MCoT）的机制，通过一个可微分且可解释的位置先验（实例化为热图）来桥接语言推理和分割。一个可学习的“集中”令牌聚合图像和推理文本的特征以生成此位置先验，然后通过轻量级解码器将其解码为精确的掩码。

Result: CoPRS在RefCOCO系列和ReasonSeg数据集上，在可比较协议下，匹配或超越了现有最佳报告指标，并在验证和测试分区上均达到或超越了现有最先进水平。实验表明，热图质量强烈影响最终的掩码质量，支持推理输出与下游掩码生成之间的一致关联。

Conclusion: 该范式有效弥合了推理和分割之间的鸿沟，在由推理驱动的集中和更精确的掩码预测方面显示出优势。

Abstract: Existing works on reasoning segmentation either connect hidden features from
a language model directly to a mask decoder or represent positions in text,
which limits interpretability and semantic detail. To solve this, we present
CoPRS, a Multi-modal Chain-of-Thought (MCoT)-based positional perception model
that bridges language reasoning to segmentation through a differentiable and
interpretable positional prior instantiated as a heatmap. By making the
reasoning process clear via MCoT and expressing it as a dense, differentiable
heatmap, this interface enhances interpretability and diagnostic analysis and
yields more concentrated evidence on the target. A learnable concentration
token aggregates features of the image and reasoning text to generate this
positional prior, which is decoded to precise masks through a lightweight
decoder, providing a direct connection between reasoning and segmentation.
Across the RefCOCO series and ReasonSeg, CoPRS matches or surpasses the best
reported metrics on each standard split under comparable protocols, with
performance at or above prior state of the art across both validation and test
partitions. Extensive experiments reveal that the quality of the heatmap
strongly influences the resulting mask quality, supporting a consistent
association between the reasoning output and downstream mask generation.
Collectively, these findings support the utility of this paradigm in bridging
reasoning and segmentation and show advantages in concentration driven by
reasoning and predicting masks more precisely. Code, checkpoints and logs are
released at https://github.com/ZhenyuLU-Heliodore/CoPRS.git.

</details>


### [251] [Reliable Cross-modal Alignment via Prototype Iterative Construction](https://arxiv.org/abs/2510.11175)
*Xiang Ma,Litian Xu,Lexin Fang,Caiming Zhang,Lizhen Cui*

Main category: cs.CV

TL;DR: 本文提出PICO框架，通过量化并加权特征列的语义信息概率，有效抑制跨模态对齐中的风格干扰，显著提升了对齐性能。


<details>
  <summary>Details</summary>
Motivation: 传统的跨模态对齐方法假设嵌入只包含语义信息，忽略了非语义（风格）信息的干扰，导致信息偏差或丢失。现有区分语义和风格的方法基于特征列分离，无法捕捉二者复杂的耦合关系。

Method: 本文提出了PICO框架，用于抑制嵌入交互中的风格干扰。具体而言，它量化了每个特征列代表语义信息的概率，并将其作为嵌入交互过程中的权重。为确保语义概率的可靠性，提出了一种原型迭代构建方法，其核心操作是基于性能反馈的加权函数，该函数能为带来更高性能提升的原型分配更高权重。

Result: 在各种基准和模型骨干上的大量实验表明，PICO框架优于最先进的方法5.2%至14.1%，展示了其优越性。

Conclusion: PICO框架通过有效抑制风格干扰，显著提升了跨模态对齐的性能，为解决语义与风格信息复杂耦合问题提供了新思路。

Abstract: Cross-modal alignment is an important multi-modal task, aiming to bridge the
semantic gap between different modalities. The most reliable fundamention for
achieving this objective lies in the semantic consistency between matched
pairs. Conventional methods implicitly assume embeddings contain solely
semantic information, ignoring the impact of non-semantic information during
alignment, which inevitably leads to information bias or even loss. These
non-semantic information primarily manifest as stylistic variations in the
data, which we formally define as style information. An intuitive approach is
to separate style from semantics, aligning only the semantic information.
However, most existing methods distinguish them based on feature columns, which
cannot represent the complex coupling relationship between semantic and style
information. In this paper, we propose PICO, a novel framework for suppressing
style interference during embedding interaction. Specifically, we quantify the
probability of each feature column representing semantic information, and
regard it as the weight during the embedding interaction. To ensure the
reliability of the semantic probability, we propose a prototype iterative
construction method. The key operation of this method is a performance
feedback-based weighting function, and we have theoretically proven that the
function can assign higher weight to prototypes that bring higher performance
improvements. Extensive experiments on various benchmarks and model backbones
demonstrate the superiority of PICO, outperforming state-of-the-art methods by
5.2\%-14.1\%.

</details>


### [252] [Saudi Sign Language Translation Using T5](https://arxiv.org/abs/2510.11183)
*Ali Alhejab,Tomas Zelezny,Lamya Alkanhal,Ivan Gruber,Yazeed Alharbi,Jakub Straka,Vaclav Javorek,Marek Hruz,Badriah Alkalifah,Ahmed Ali*

Main category: cs.CV

TL;DR: 本论文探索了使用T5模型和新数据集进行沙特手语（SSL）翻译，并发现通过美国手语（ASL）数据进行预训练能显著提升模型性能，证实了手语模型中的跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: 开发有效的沙特手语（SSL）翻译系统，尤其要应对其独特的挑战，如面部遮盖物对手语识别和翻译的难度。

Method: 使用T5模型进行SSL翻译，并构建了一个包含三种挑战性测试协议的新型SSL数据集。研究比较了在YouTubeASL数据集上预训练的T5模型与直接在SSL数据集上训练的模型，以评估ASL预训练对SSL翻译性能的影响。

Result: 实验结果表明，在YouTubeASL上进行预训练显著提升了模型的性能（BLEU-4指标提升约3倍），这表明手语模型具有跨语言迁移能力。

Conclusion: 利用大规模ASL数据对改进SSL翻译具有显著益处，为开发更高效的手语翻译系统提供了重要见解。

Abstract: This paper explores the application of T5 models for Saudi Sign Language
(SSL) translation using a novel dataset. The SSL dataset includes three
challenging testing protocols, enabling comprehensive evaluation across
different scenarios. Additionally, it captures unique SSL characteristics, such
as face coverings, which pose challenges for sign recognition and translation.
In our experiments, we investigate the impact of pre-training on American Sign
Language (ASL) data by comparing T5 models pre-trained on the YouTubeASL
dataset with models trained directly on the SSL dataset. Experimental results
demonstrate that pre-training on YouTubeASL significantly improves models'
performance (roughly $3\times$ in BLEU-4), indicating cross-linguistic
transferability in sign language models. Our findings highlight the benefits of
leveraging large-scale ASL data to improve SSL translation and provide insights
into the development of more effective sign language translation systems. Our
code is publicly available at our GitHub repository.

</details>


### [253] [BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models](https://arxiv.org/abs/2510.11178)
*Bryan Chen Zhengyu Tan,Zheng Weihua,Zhengyuan Liu,Nancy F. Chen,Hwaran Lee,Kenny Tsu Wei Choo,Roy Ka-Wei Lee*

Main category: cs.CV

TL;DR: 该研究引入BLEnD-Vis基准，评估视觉-语言模型（VLMs）在语言重述和视觉模态下对日常文化知识的鲁棒性，发现现有VLM的文化理解存在显著脆弱性，尤其在跨模态一致性方面。


<details>
  <summary>Details</summary>
Motivation: 随着VLM在全球范围内的部署，它们理解文化情境知识的能力变得至关重要。然而，现有评估主要侧重于静态记忆或孤立的视觉定位，未能回答VLM是否具备鲁棒且可迁移的文化理解能力。

Method: 引入BLEnD-Vis，一个多模态、多文化基准。它基于BLEnD数据集，构建了313个跨16个区域的文化接地问题模板，并生成了三种对齐的多项选择格式：(i) 文本-区域到实体，(ii) 文本-实体到区域的倒置变体，以及(iii) 带有生成图像的(ii)的VQA风格版本。最终基准包含4,916张图像和超过21,000个多项选择题实例，并通过人工标注进行验证。

Result: BLEnD-Vis揭示了当前VLM文化知识的显著脆弱性；模型在语言重述下性能下降，尽管视觉线索通常有助于提高性能，但较低的跨模态一致性突出表明在稳健整合文本和视觉理解方面存在挑战，特别是对于资源较少的地区。

Conclusion: BLEnD-Vis提供了一个关键的测试平台，用于系统分析文化鲁棒性和多模态接地，揭示了局限性并指导开发更具文化能力的VLM。

Abstract: As vision-language models (VLMs) are deployed globally, their ability to
understand culturally situated knowledge becomes essential. Yet, existing
evaluations largely assess static recall or isolated visual grounding, leaving
unanswered whether VLMs possess robust and transferable cultural understanding.
We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to
evaluate the robustness of everyday cultural knowledge in VLMs across
linguistic rephrasings and visual modalities. Building on the BLEnD dataset,
BLEnD-Vis constructs 313 culturally grounded question templates spanning 16
regions and generates three aligned multiple-choice formats: (i) a text-only
baseline querying from Region $\to$ Entity, (ii) an inverted text-only variant
(Entity $\to$ Region), and (iii) a VQA-style version of (ii) with generated
images. The resulting benchmark comprises 4,916 images and over 21,000
multiple-choice question (MCQ) instances, validated through human annotation.
BLEnD-Vis reveals significant fragility in current VLM cultural knowledge;
models exhibit performance drops under linguistic rephrasing and, whilst visual
cues often aid performance, low cross-modal consistency highlights challenges
in robustly integrating textual and visual understanding, particularly for
lower-resource regions. BLEnD-Vis thus provides a crucial testbed for
systematically analysing cultural robustness and multimodal grounding, exposing
limitations and guiding the development of more culturally competent VLMs.

</details>


### [254] [FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2510.11190)
*Shengming Yuan,Xinyu Lyu,Shuailong Wang,Beitao Chen,Jingkuan Song,Lianli Gao*

Main category: cs.CV

TL;DR: 该论文提出了一种名为FlexAC的轻量级、免训练框架，旨在为多模态大型语言模型（MLLMs）提供灵活控制联想推理能力，以平衡其在事实性和创造性任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大型语言模型（MLLMs）在忠实性和创造性之间存在固有的权衡，不同任务需要不同程度的联想推理。然而，现有方法缺乏调节这种推理强度的灵活性，限制了MLLMs在事实性和创造性场景中的适应性。

Method: 研究首先调查了MLLMs中联想行为的内部机制，发现：1）中间层在塑造模型联想倾向中起关键作用；2）修改这些层中的表示能有效调节联想推理强度；3）幻觉可以被利用来导出引导这种调节的转向向量。在此基础上，提出了FlexAC框架：1）诱导幻觉引导的中间表示来编码联想方向；2）选择高联想实例构建有效的联想转向向量，并自适应校准其强度以平衡创造性引导与输出稳定性；3）整合来自少量目标域样本前向传播获得的任务特定联想向量，以适应多样化的联想方向和创意任务。

Result: FlexAC方法在Creation-MMBench上实现了高达5.8倍的创造力提升，并在CHAIR上将幻觉率降低了29%，超越了现有基线方法。

Conclusion: FlexAC框架有效解决了MLLMs在忠实性和创造性之间的权衡问题，通过提供灵活控制联想推理的能力，显著提高了模型在创意任务中的表现，同时降低了幻觉率，展示了其在增强MLLMs适应性方面的有效性。

Abstract: Multimodal large language models (MLLMs) face an inherent trade-off between
faithfulness and creativity, as different tasks require varying degrees of
associative reasoning. However, existing methods lack the flexibility to
modulate this reasoning strength, limiting MLLMs' adaptability across factual
and creative scenarios. To bridge this gap, we propose equipping MLLMs with
mechanisms that enable flexible control over associative reasoning. We begin by
investigating the internal mechanisms underlying associative behavior in MLLMs
and find that: (1) middle layers play a pivotal role in shaping model's
associative tendencies, (2) modifying representations in these layers
effectively regulates associative reasoning strength, and (3) hallucinations
can be exploited to derive steering vectors that guide this modulation.
Building on these findings, we introduce Flexible Association Control (FlexAC),
a lightweight and training-free framework for modulating associative behavior
in MLLMs. FlexAC first induces hallucination-guided intermediate
representations to encode associative directions. Then, it selects
high-association instances to construct effective associative steering vectors,
whose strengths are adaptively calibrated to balance creative guidance with
output stability. Finally, recognizing the multi-dimensional nature of
associative reasoning, FlexAC incorporates task-specific associative vectors
derived from a forward pass on a few target-domain samples, enabling models to
follow diverse associative directions and better adapt to creative tasks.
Notably, our method achieves up to a 5.8x improvement in creativity on
Creation-MMBench and a 29% reduction in hallucination rate on CHAIR, surpassing
existing baselines and demonstrating its effectiveness in enabling flexible
control over associative reasoning in MLLMs. Our code is available at
https://github.com/ylhz/FlexAC.

</details>


### [255] [Investigating Identity Signals in Conversational Facial Dynamics via Disentangled Expression Features](https://arxiv.org/abs/2510.11223)
*Masoumeh Chapariniya,Pierre Vuillecard,Jean-Marc Odobez,Volker Dellwo,Teodora Vukovic*

Main category: cs.CV

TL;DR: 本研究发现，仅通过面部表情的动态成分，就可以在很大程度上识别个体身份，与静态面部特征无关。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索个体是否可以仅凭面部表情的纯动态成分被识别，而不依赖于静态面部外观。

Method: 研究利用FLAME 3D可变形模型实现面部形状和表情动态的明确分离，从对话视频中提取逐帧表情和下颌系数。接着，使用带有监督对比学习的Conformer模型进行1,429路分类。此外，引入漂移噪声比（DNR）来量化形状表情分离的可靠性。

Result: 在CANDOR数据集上，Conformer模型在1,429路分类中达到61.14%的准确率（是随机猜测的458倍），表明面部动态携带强烈的身份特征。漂移噪声比（DNR）与识别性能呈强烈负相关，证实不稳定的形状估计会损害动态识别。

Conclusion: 研究结果揭示了对话面部动态中存在个体特有的身份特征，这对社会感知和临床评估具有重要意义。

Abstract: This work investigates whether individuals can be identified solely through
the pure dynamical components of their facial expressions, independent of
static facial appearance. We leverage the FLAME 3D morphable model to achieve
explicit disentanglement between facial shape and expression dynamics,
extracting frame-by-frame parameters from conversational videos while retaining
only expression and jaw coefficients. On the CANDOR dataset of 1,429 speakers
in naturalistic conversations, our Conformer model with supervised contrastive
learning achieves 61.14\%accuracy on 1,429-way classification -- 458 times
above chance -- demonstrating that facial dynamics carry strong identity
signatures. We introduce a drift-to-noise ratio (DNR) that quantifies the
reliability of shape expression separation by measuring across-session shape
changes relative to within-session variability. DNR strongly negatively
correlates with recognition performance, confirming that unstable shape
estimation compromises dynamic identification. Our findings reveal
person-specific signatures in conversational facial dynamics, with implications
for social perception and clinical assessment.

</details>


### [256] [Class Prototypes based Contrastive Learning for Classifying Multi-Label and Fine-Grained Educational Videos](https://arxiv.org/abs/2510.11204)
*Rohit Gupta,Anirban Roy,Claire Christensen,Sujeong Kim,Sarah Gerard,Madeline Cincebeaux,Ajay Divakaran,Todd Grindal,Mubarak Shah*

Main category: cs.CV

TL;DR: 本文提出了一种基于类原型监督对比学习的多模态Transformer网络，用于在线视频中细粒度教育内容的检测，并创建了APPROVE数据集，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着儿童在线媒体消费的增长，教育工作者需要数据驱动的工具来筛选适合幼儿的教育内容。现有方法可能无法有效处理细粒度且视觉相似的教育内容分类。

Method: 将问题建模为细粒度多标签分类，关注读写和数学两大类中的特定子类（如字母名称、计数）。提出了一种新颖的基于类原型的监督对比学习方法，通过最小化样本与同类原型距离、最大化与异类原型距离来处理多标签细粒度样本。采用多模态Transformer网络捕获视频中视觉和音频线索的交互。构建了一个包含193小时专家标注视频（19个细粒度教育类别）的APPROVE数据集进行评估。

Result: 所提出的方法在APPROVE数据集以及YouTube-8M和COIN等其他基准测试中均优于强大的基线模型。APPROVE数据集已公开。

Conclusion: 本文提出了一种有效检测在线视频中细粒度教育内容的方法，并构建了一个高质量的标注数据集APPROVE，为教育工作者筛选适龄内容提供了有力的工具和资源。

Abstract: The recent growth in the consumption of online media by children during early
childhood necessitates data-driven tools enabling educators to filter out
appropriate educational content for young learners. This paper presents an
approach for detecting educational content in online videos. We focus on two
widely used educational content classes: literacy and math. For each class, we
choose prominent codes (sub-classes) based on the Common Core Standards. For
example, literacy codes include `letter names', `letter sounds', and math codes
include `counting', `sorting'. We pose this as a fine-grained multilabel
classification problem as videos can contain multiple types of educational
content and the content classes can get visually similar (e.g., `letter names'
vs `letter sounds'). We propose a novel class prototypes based supervised
contrastive learning approach that can handle fine-grained samples associated
with multiple labels. We learn a class prototype for each class and a loss
function is employed to minimize the distances between a class prototype and
the samples from the class. Similarly, distances between a class prototype and
the samples from other classes are maximized. As the alignment between visual
and audio cues are crucial for effective comprehension, we consider a
multimodal transformer network to capture the interaction between visual and
audio cues in videos while learning the embedding for videos. For evaluation,
we present a dataset, APPROVE, employing educational videos from YouTube
labeled with fine-grained education classes by education researchers. APPROVE
consists of 193 hours of expert-annotated videos with 19 classes. The proposed
approach outperforms strong baselines on APPROVE and other benchmarks such as
Youtube-8M, and COIN. The dataset is available at
https://github.com/rohit-gupta/MMContrast/tree/main/APPROVE

</details>


### [257] [DTEA: Dynamic Topology Weaving and Instability-Driven Entropic Attenuation for Medical Image Segmentation](https://arxiv.org/abs/2510.11259)
*Weixuan Li,Quanjun Li,Guang Yu,Song Yang,Zimeng Li,Chi-Man Pun,Yupeng Liu,Xuhang Chen*

Main category: cs.CV

TL;DR: 本文提出了DTEA模型，通过语义拓扑重构（STR）和熵扰动门控（EPG）模块，改进了跳跃连接框架，以增强医学图像分割中的结构表示和上下文建模，实现了卓越的分割精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法中的跳跃连接在结构表示和上下文建模方面存在局限性，导致在复杂临床场景中泛化能力不足，难以有效融合全局上下文并缩小编码器与解码器之间的语义鸿沟。

Method: 本文提出了DTEA模型，包含两个核心模块：1. 语义拓扑重构（STR）：将多尺度语义特征重组为动态超图，以更好地建模跨分辨率的解剖依赖性，从而增强结构和语义表示。2. 熵扰动门控（EPG）：评估扰动后的通道稳定性，并过滤高熵通道，以强调临床重要区域并提升空间注意力。

Result: 在三个基准数据集上进行的广泛实验表明，DTEA框架实现了卓越的分割精度，并在各种临床设置中表现出更好的泛化能力。

Conclusion: DTEA模型通过其创新的跳跃连接框架，特别是STR和EPG模块，有效解决了医学图像分割中结构表示和上下文建模的挑战，显著提升了分割性能和临床泛化能力。

Abstract: In medical image segmentation, skip connections are used to merge global
context and reduce the semantic gap between encoder and decoder. Current
methods often struggle with limited structural representation and insufficient
contextual modeling, affecting generalization in complex clinical scenarios. We
propose the DTEA model, featuring a new skip connection framework with the
Semantic Topology Reconfiguration (STR) and Entropic Perturbation Gating (EPG)
modules. STR reorganizes multi-scale semantic features into a dynamic
hypergraph to better model cross-resolution anatomical dependencies, enhancing
structural and semantic representation. EPG assesses channel stability after
perturbation and filters high-entropy channels to emphasize clinically
important regions and improve spatial attention. Extensive experiments on three
benchmark datasets show our framework achieves superior segmentation accuracy
and better generalization across various clinical settings. The code is
available at
\href{https://github.com/LWX-Research/DTEA}{https://github.com/LWX-Research/DTEA}.

</details>


### [258] [Exploring and Leveraging Class Vectors for Classifier Editing](https://arxiv.org/abs/2510.11268)
*Jaeik Kim,Jaeyoung Do*

Main category: cs.CV

TL;DR: 本文引入“类向量”，这是一种在潜在空间中捕获类别特定表示调整的新方法，实现了对图像分类器的高效、灵活和高级概念编辑。


<details>
  <summary>Details</summary>
Motivation: 图像分类器在医学成像和制造业中发挥关键作用，但其预定义行为使得事后模型编辑（如遗忘特定类别或适应分布变化）变得困难。现有编辑方法要么过于狭窄，要么需要高昂的再训练成本，并且在图像分类领域的研究有限，导致缺乏灵活的编辑能力。

Method: 本文提出了“类向量”（Class Vectors），它们在微调过程中捕获类别特定的表示调整。与任务向量在权重空间编码任务级变化不同，类向量在潜在空间中解耦了每个类别的适应性。编辑可以通过沿着这些向量引导潜在特征，或将它们映射到权重空间以更新决策边界来实现。

Result: 研究表明，类向量能够捕获每个类别的语义漂移。由于其固有的线性和正交性，类向量支持通过简单的类别算术实现高效、灵活和高级的概念编辑。其效用在遗忘、环境适应、对抗性防御和对抗性触发优化等应用中得到了验证。

Conclusion: 类向量提供了一种新颖且强大的方法，能够克服图像分类器编辑的挑战，通过在潜在空间进行类别特定的调整，实现了高效、灵活且高级的概念编辑，适用于多种应用场景。

Abstract: Image classifiers play a critical role in detecting diseases in medical
imaging and identifying anomalies in manufacturing processes. However, their
predefined behaviors after extensive training make post hoc model editing
difficult, especially when it comes to forgetting specific classes or adapting
to distribution shifts. Existing classifier editing methods either focus
narrowly on correcting errors or incur extensive retraining costs, creating a
bottleneck for flexible editing. Moreover, such editing has seen limited
investigation in image classification. To overcome these challenges, we
introduce Class Vectors, which capture class-specific representation
adjustments during fine-tuning. Whereas task vectors encode task-level changes
in weight space, Class Vectors disentangle each class's adaptation in the
latent space. We show that Class Vectors capture each class's semantic shift
and that classifier editing can be achieved either by steering latent features
along these vectors or by mapping them into weight space to update the decision
boundaries. We also demonstrate that the inherent linearity and orthogonality
of Class Vectors support efficient, flexible, and high-level concept editing
via simple class arithmetic. Finally, we validate their utility in applications
such as unlearning, environmental adaptation, adversarial defense, and
adversarial trigger optimization.

</details>


### [259] [EEMS: Edge-Prompt Enhanced Medical Image Segmentation Based on Learnable Gating Mechanism](https://arxiv.org/abs/2510.11287)
*Han Xia,Quanjun Li,Qian Li,Zimeng Li,Hongbin Ye,Yupeng Liu,Haolun Li,Xuhang Chen*

Main category: cs.CV

TL;DR: 本文提出EEMS模型，通过结合边缘感知增强单元、多尺度提示生成单元和双源自适应门控融合单元，克服了医学图像分割中边缘模糊和背景噪声的挑战，显著提高了分割精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割对于诊断、治疗规划和疾病监测至关重要，但面临边缘模糊和背景噪声等复杂因素的挑战。

Method: EEMS模型包含三个核心单元：1) 边缘感知增强单元（EAEU），通过多频特征提取增强边缘感知；2) 多尺度提示生成单元（MSPGU），利用提示引导方法整合高层语义和低层空间特征；3) 双源自适应门控融合单元（DAGFU），融合EAEU的边缘特征与MSPGU的语义特征，以提高分割精度和鲁棒性。

Result: 在ISIC2018等数据集上的测试表明，EEMS模型表现出卓越的性能和可靠性。

Conclusion: EEMS模型是一种可靠的临床医学图像分割工具。

Abstract: Medical image segmentation is vital for diagnosis, treatment planning, and
disease monitoring but is challenged by complex factors like ambiguous edges
and background noise. We introduce EEMS, a new model for segmentation,
combining an Edge-Aware Enhancement Unit (EAEU) and a Multi-scale Prompt
Generation Unit (MSPGU). EAEU enhances edge perception via multi-frequency
feature extraction, accurately defining boundaries. MSPGU integrates high-level
semantic and low-level spatial features using a prompt-guided approach,
ensuring precise target localization. The Dual-Source Adaptive Gated Fusion
Unit (DAGFU) merges edge features from EAEU with semantic features from MSPGU,
enhancing segmentation accuracy and robustness. Tests on datasets like ISIC2018
confirm EEMS's superior performance and reliability as a clinical tool.

</details>


### [260] [sketch2symm: Symmetry-aware sketch-to-shape generation via semantic bridging](https://arxiv.org/abs/2510.11303)
*Yan Zhou,Mingji Li,Xiantao Zeng,Jie Lin,Yuexia Zhou*

Main category: cs.CV

TL;DR: 该论文提出Sketch2Symm，一种两阶段生成方法，通过语义桥接（草图转图像）和引入对称性约束，从抽象草图重建几何一致的3D形状。


<details>
  <summary>Details</summary>
Motivation: 由于草图输入抽象且稀疏，缺乏足够的语义和几何信息，导致基于草图的3D重建任务极具挑战性。

Method: 本文提出Sketch2Symm方法，包括两个阶段：1) 通过草图到图像的翻译实现语义桥接，以丰富稀疏草图表示；2) 引入对称性约束作为几何先验，利用日常物体中常见的结构规律。

Result: 在主流草图数据集上的实验表明，该方法在Chamfer距离、Earth Mover's距离和F-Score方面均优于现有基于草图的重建方法。

Conclusion: 所提出的语义桥接和对称感知设计有效提升了基于草图的3D重建性能，能够生成几何一致的3D形状。

Abstract: Sketch-based 3D reconstruction remains a challenging task due to the abstract
and sparse nature of sketch inputs, which often lack sufficient semantic and
geometric information. To address this, we propose Sketch2Symm, a two-stage
generation method that produces geometrically consistent 3D shapes from
sketches. Our approach introduces semantic bridging via sketch-to-image
translation to enrich sparse sketch representations, and incorporates symmetry
constraints as geometric priors to leverage the structural regularity commonly
found in everyday objects. Experiments on mainstream sketch datasets
demonstrate that our method achieves superior performance compared to existing
sketch-based reconstruction methods in terms of Chamfer Distance, Earth Mover's
Distance, and F-Score, verifying the effectiveness of the proposed semantic
bridging and symmetry-aware design.

</details>


### [261] [Human Uncertainty-Aware Data Selection and Automatic Labeling in Visual Question Answering](https://arxiv.org/abs/2510.11295)
*Jian Lan,Zhicheng Liu,Udo Schlegel,Raoyuan Zhao,Yihong Liu,Hinrich Schütze,Michael A. Hedderich,Thomas Seidl*

Main category: cs.CV

TL;DR: 本文研究了视觉语言模型（VLMs）在监督微调（SFT）中人类不确定性（HU）的影响，发现高HU样本可能损害模型性能并导致校准不足。为此，提出了一种名为HaDola的框架，通过数据选择和自动标注来有效利用HU，在减少标注成本的同时提高模型准确性和校准性。


<details>
  <summary>Details</summary>
Motivation: 大规模VLMs在视觉问答中严重依赖昂贵的人工标注SFT。现实世界数据集常存在人类不确定性（HU），但标准SFT只关注最频繁的标签，忽略了HU分布。这引出了两个问题：HU如何影响SFT，以及如何在训练中有效利用HU。

Method: 首先，对不同HU水平下的VLMs进行了系统评估。基于评估结果，提出了HaDola框架，这是一个人类不确定性感知的数据选择和自动标注框架。HaDola分四个阶段操作：区分、自标注、错误触发和训练，旨在迭代识别有害样本、优先选择信息量大的样本，并从少量初始数据（5%）中进行自举。

Result: 研究发现：(i) 高HU样本对模型性能贡献甚微甚至可能降低性能；(ii) 简单地在完整数据集上训练会导致模型校准不足，无法捕捉HU分布。HaDola显著减少了对昂贵HU标注的依赖，使VLMs更准确且校准更好。在VQAv2和VizWiz数据集上的实验表明，HaDola在更少训练数据的情况下，持续匹配或超越了最先进的基线模型。

Conclusion: 研究强调了在SFT中明确建模人类不确定性的重要性，并表明有效利用HU比单纯扩大数据集规模更能提升VLMs的性能和校准性。

Abstract: Large vision-language models (VLMs) achieve strong performance in Visual
Question Answering but still rely heavily on supervised fine-tuning (SFT) with
massive labeled datasets, which is costly due to human annotations. Crucially,
real-world datasets often exhibit human uncertainty (HU) -- variation in human
confidence across annotations -- but standard SFT simply optimizes toward the
most frequent label, disregarding HU distributions. This leaves two open
questions: How does HU affect SFT, and how can HU be effectively leveraged in
training? In this work, we first conduct a systematic evaluation of VLMs across
varying HU levels. We have two key findings: (i) surprisingly, high-HU samples
contribute little or even degrade model performance, and (ii) naively training
on the full dataset yields under-calibrated models that fail to capture HU
distributions. Motivated by these findings, we introduce HaDola, a human
uncertainty-aware data selection and automatic labeling framework. HaDola
operates in four stages -- discriminate, self-annotate, error trigger, and
training -- to iteratively identify harmful samples, prioritize informative
ones, and bootstrap from a small seed set (5\% of data). Our approach
substantially reduces reliance on costly HU annotations and makes VLMs more
accurate and better calibrated. Extensive experiments on VQAv2 and VizWiz
datasets demonstrate that HaDola consistently matches or outperforms
state-of-the-art baselines with less training data. Our work highlights the
importance of explicitly modeling HU in SFT, suggesting that better utilization
of HU is more effective than merely scaling up dataset size.

</details>


### [262] [MaterialRefGS: Reflective Gaussian Splatting with Multi-view Consistent Material Inference](https://arxiv.org/abs/2510.11387)
*Wenyuan Zhang,Jimin Tang,Weiqi Zhang,Yi Fang,Yu-Shen Liu,Zhizhong Han*

Main category: cs.CV

TL;DR: 本文通过引入多视角一致的材质推断和基于2DGS的光线追踪环境建模，解决了高斯溅射中反射建模的挑战，实现了照片级的间接辐射渲染和新视角合成。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯基元（Gaussian primitives）的反射建模方法，在有限环境建模下，材质推断缺乏足够的约束，导致照明混叠和泛化能力下降。

Method: 该方法强制2D高斯在延迟着色过程中生成多视角一致的材质图。通过跟踪跨视角的光度变化来识别高反射区域，作为反射强度项的强先验。此外，引入了通过2DGS进行光线追踪的环境建模策略，以处理物体间遮挡引起的间接照明。

Result: 实验结果表明，该方法忠实地恢复了照明和几何形状，在新视角合成中实现了最先进的渲染质量。

Conclusion: 研究表明，多视角一致的材质推断和更物理化的环境建模是使用高斯溅射学习准确反射的关键，从而实现了逼真的渲染效果。

Abstract: Modeling reflections from 2D images is essential for photorealistic rendering
and novel view synthesis. Recent approaches enhance Gaussian primitives with
reflection-related material attributes to enable physically based rendering
(PBR) with Gaussian Splatting. However, the material inference often lacks
sufficient constraints, especially under limited environment modeling,
resulting in illumination aliasing and reduced generalization. In this work, we
revisit the problem from a multi-view perspective and show that multi-view
consistent material inference with more physically-based environment modeling
is key to learning accurate reflections with Gaussian Splatting. To this end,
we enforce 2D Gaussians to produce multi-view consistent material maps during
deferred shading. We also track photometric variations across views to identify
highly reflective regions, which serve as strong priors for reflection strength
terms. To handle indirect illumination caused by inter-object occlusions, we
further introduce an environment modeling strategy through ray tracing with
2DGS, enabling photorealistic rendering of indirect radiance. Experiments on
widely used benchmarks show that our method faithfully recovers both
illumination and geometry, achieving state-of-the-art rendering quality in
novel views synthesis.

</details>


### [263] [MMAP: A Multi-Magnification and Prototype-Aware Architecture for Predicting Spatial Gene Expression](https://arxiv.org/abs/2510.11344)
*Hai Dang Nguyen,Nguyen Dang Huy Pham,The Minh Duc Nguyen,Dac Thai Nguyen,Hang Thi Nguyen,Duong M. Nguyen*

Main category: cs.CV

TL;DR: 本研究提出MMAP框架，通过多放大倍数局部特征和原型增强的全局上下文，显著提升了从H&E染色全玻片图像预测空间转录组基因表达的准确性，超越了现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学中，从H&E染色图像预测基因表达面临视觉特征与分子信号间的模态鸿沟。现有方法在局部特征提取的粒度不足和全局空间上下文覆盖不充分方面存在局限性。

Method: 本研究提出了MMAP（Multi-MAgnification and Prototype-enhanced architecture）框架。为增强局部特征粒度，MMAP利用多放大倍数的图像块表示。为改善全局上下文理解，它学习一组潜在的原型嵌入，作为玻片级信息的紧凑表示。

Result: 广泛的实验结果表明，MMAP在平均绝对误差（MAE）、均方误差（MSE）和皮尔逊相关系数（PCC）等多个评估指标上，持续优于所有现有的最先进方法。

Conclusion: MMAP通过结合多放大倍数局部特征和原型增强的全局上下文，有效解决了现有方法在空间基因表达预测中的局限性，实现了卓越的性能。

Abstract: Spatial Transcriptomics (ST) enables the measurement of gene expression while
preserving spatial information, offering critical insights into tissue
architecture and disease pathology. Recent developments have explored the use
of hematoxylin and eosin (H&E)-stained whole-slide images (WSIs) to predict
transcriptome-wide gene expression profiles through deep neural networks. This
task is commonly framed as a regression problem, where each input corresponds
to a localized image patch extracted from the WSI. However, predicting spatial
gene expression from histological images remains a challenging problem due to
the significant modality gap between visual features and molecular signals.
Recent studies have attempted to incorporate both local and global information
into predictive models. Nevertheless, existing methods still suffer from two
key limitations: (1) insufficient granularity in local feature extraction, and
(2) inadequate coverage of global spatial context. In this work, we propose a
novel framework, MMAP (Multi-MAgnification and Prototype-enhanced
architecture), that addresses both challenges simultaneously. To enhance local
feature granularity, MMAP leverages multi-magnification patch representations
that capture fine-grained histological details. To improve global contextual
understanding, it learns a set of latent prototype embeddings that serve as
compact representations of slide-level information. Extensive experimental
results demonstrate that MMAP consistently outperforms all existing
state-of-the-art methods across multiple evaluation metrics, including Mean
Absolute Error (MAE), Mean Squared Error (MSE), and Pearson Correlation
Coefficient (PCC).

</details>


### [264] [Evaluating the effects of preprocessing, method selection, and hyperparameter tuning on SAR-based flood mapping and water depth estimation](https://arxiv.org/abs/2510.11305)
*Jean-Paul Travert,Cédric Goeury,Sébastien Boyaval,Vito Bacchi,Fabrice Zaoui*

Main category: cs.CV

TL;DR: 本研究评估了合成孔径雷达（SAR）图像用于洪水测绘和水深估计的各种预处理、洪水测绘和水深估计方法，强调了方法选择和超参数对结果不确定性的影响，并建议采用集成方法。


<details>
  <summary>Details</summary>
Motivation: 利用SAR图像进行洪水测绘和水深估计对于水文模型的校准和验证至关重要。研究旨在评估不同方法在此过程中的表现及其对结果不确定性的影响。

Method: 研究通过考虑预处理图像、洪水图和水深场的集合，评估了SAR图像的各种预处理（特别是散斑噪声消除）、洪水测绘和水深估计方法。通过对2019年和2021年加龙河（法国）的两次洪水事件进行评估，并使用水动力模拟和现场观测数据作为参考。

Result: 散斑滤波器选择会导致洪水范围估计变化数平方公里；洪水测绘方法的选择和调整也会影响性能，尽管有监督方法优于无监督方法，但经过调整的无监督方法（如局部阈值或变化检测）可以达到可比结果。预处理和洪水测绘步骤的复合不确定性也导致水深估计场的高度变异性。对于洪水测绘，方法选择影响最大；对于水深估计，洪水测绘步骤产生的洪水图输入和方法的超参数影响最大。

Conclusion: 研究强调了考虑整个处理流程（包括预处理、洪水测绘和水深估计方法及其相关超参数）的重要性。建议采用集成方法并考虑方法学不确定性，而不是依赖单一配置。

Abstract: Flood mapping and water depth estimation from Synthetic Aperture Radar (SAR)
imagery are crucial for calibrating and validating hydraulic models. This study
uses SAR imagery to evaluate various preprocessing (especially speckle noise
reduction), flood mapping, and water depth estimation methods. The impact of
the choice of method at different steps and its hyperparameters is studied by
considering an ensemble of preprocessed images, flood maps, and water depth
fields. The evaluation is conducted for two flood events on the Garonne River
(France) in 2019 and 2021, using hydrodynamic simulations and in-situ
observations as reference data. Results show that the choice of speckle filter
alters flood extent estimations with variations of several square kilometers.
Furthermore, the selection and tuning of flood mapping methods also affect
performance. While supervised methods outperformed unsupervised ones, tuned
unsupervised approaches (such as local thresholding or change detection) can
achieve comparable results. The compounded uncertainty from preprocessing and
flood mapping steps also introduces high variability in the water depth field
estimates. This study highlights the importance of considering the entire
processing pipeline, encompassing preprocessing, flood mapping, and water depth
estimation methods and their associated hyperparameters. Rather than relying on
a single configuration, adopting an ensemble approach and accounting for
methodological uncertainty should be privileged. For flood mapping, the method
choice has the most influence. For water depth estimation, the most influential
processing step was the flood map input resulting from the flood mapping step
and the hyperparameters of the methods.

</details>


### [265] [$Δ\mathrm{Energy}$: Optimizing Energy Change During Vision-Language Alignment Improves both OOD Detection and OOD Generalization](https://arxiv.org/abs/2510.11296)
*Lin Zhu,Yifeng Yang,Xinbing Wang,Qinying Gu,Nanyang Ye*

Main category: cs.CV

TL;DR: 本文提出了一种名为ΔEnergy的新型OOD（Out-of-Distribution）分数，用于视觉-语言模型（VLMs）在协变量偏移下的OOD泛化和语义偏移下的OOD检测。通过ΔEnergy的下界最大化（EBM），该方法能显著提升OOD检测性能，并同时改善OOD泛化能力，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 视觉-语言模型（VLMs）在实际应用中不可避免地会遇到分布内（ID）数据和分布外（OOD）数据。OOD数据包括协变量偏移（已知类别但图像风格变化）和语义偏移（测试时未见类别）。因此，提升VLMs对协变量偏移OOD数据的泛化能力，并有效检测开放集语义偏移OOD类别，是当前研究的重要挑战。

Method: 受视觉-语言模态重新对齐时（通过直接降低最大余弦相似度）观察到的闭集数据中显著能量变化的启发，本文引入了一种新颖的OOD分数：ΔEnergy。为了同时改善协变量偏移下的OOD泛化，作者提出了ΔEnergy的下界最大化（EBM）。EBM在理论上被证明不仅能增强OOD检测，还能产生一个域一致的Hessian矩阵，作为OOD泛化的强指标。基于此发现，作者开发了一个统一的微调框架，以提高VLMs在OOD泛化和OOD检测方面的鲁棒性。

Result: ΔEnergy在OOD检测方面显著优于传统的基于能量的OOD分数，并提供了一种更可靠的OOD检测方法。此外，ΔEnergy（通过EBM实现）能够同时改善协变量偏移下的OOD泛化。在具有挑战性的OOD检测和泛化基准测试上进行的广泛实验表明，该方法优于现有方法，AUROC指标提高了10%到25%。

Conclusion: 本文提出的ΔEnergy OOD分数及其下界最大化（EBM）方法，通过一个统一的微调框架，有效解决了视觉-语言模型在OOD泛化和OOD检测方面的挑战。该方法不仅在理论上得到证明，还在实验中取得了显著优于现有方法的性能提升，为VLMs的鲁棒性提供了新的解决方案。

Abstract: Recent approaches for vision-language models (VLMs) have shown remarkable
success in achieving fast downstream adaptation. When applied to real-world
downstream tasks, VLMs inevitably encounter both the in-distribution (ID) data
and out-of-distribution (OOD) data. The OOD datasets often include both
covariate shifts (e.g., known classes with changes in image styles) and
semantic shifts (e.g., test-time unseen classes). This highlights the
importance of improving VLMs' generalization ability to covariate-shifted OOD
data, while effectively detecting open-set semantic-shifted OOD classes. In
this paper, inspired by the substantial energy change observed in closed-set
data when re-aligning vision-language modalities (specifically by directly
reducing the maximum cosine similarity to a low value), we introduce a novel
OOD score, named {\Delta}Energy. {\Delta}Energy significantly outperforms the
vanilla energy-based OOD score and provides a more reliable approach for OOD
detection. Furthermore, {\Delta}Energy can simultaneously improve OOD
generalization under covariate shifts, which is achieved by lower-bound
maximization for {\Delta}Energy (termed EBM). EBM is theoretically proven to
not only enhance OOD detection but also yields a domain-consistent Hessian,
which serves as a strong indicator for OOD generalization. Based on this
finding, we developed a unified fine-tuning framework that allows for improving
VLMs' robustness in both OOD generalization and OOD detection. Extensive
experiments on challenging OOD detection and generalization benchmarks
demonstrate the superiority of our method, outperforming recent approaches by
10% to 25% in AUROC.

</details>


### [266] [InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models](https://arxiv.org/abs/2510.11341)
*Haomin Wang,Jinhui Yin,Qi Wei,Wenguang Zeng,Lixin Gu,Shenglong Ye,Zhangwei Gao,Yaohui Wang,Yanting Zhang,Yuanqi Li,Yanwen Guo,Wenhai Wang,Kai Chen,Yu Qiao,Hongjie Zhang*

Main category: cs.CV

TL;DR: 该研究利用多模态大语言模型（MLLMs）实现了统一的SVG理解、编辑和生成，并提出了InternSVG套件，包括大型数据集SAgoge、基准测试SArena和统一模型InternSVG，显著提升了SVG任务的性能。


<details>
  <summary>Details</summary>
Motivation: 通用SVG建模面临挑战，包括数据集碎片化、方法跨任务迁移性差以及处理结构复杂性困难。

Method: 研究利用多模态大语言模型（MLLMs）的强大迁移和泛化能力，提出了InternSVG家族，一个集成了数据、基准和模型的套件。核心是SAgoge，一个最大且最全面的SVG多模态数据集；SArena是一个配套的基准测试；InternSVG模型是一个统一的MLLM，采用SVG特定特殊token、基于子词的嵌入初始化和两阶段训练策略（从短静态SVG到长序列插图和复杂动画）。

Result: 在SArena和现有基准测试上的实验表明，InternSVG取得了显著的性能提升，并持续超越了领先的开源和专有同类模型。

Conclusion: 该研究通过统一的MLLM方法和InternSVG套件，有效解决了SVG建模的挑战，实现了SVG理解、编辑和生成的统一建模，并展示了优越的整体性能。

Abstract: General SVG modeling remains challenging due to fragmented datasets, limited
transferability of methods across tasks, and the difficulty of handling
structural complexity. In response, we leverage the strong transfer and
generalization capabilities of multimodal large language models (MLLMs) to
achieve unified modeling for SVG understanding, editing, and generation. We
present the InternSVG family, an integrated data-benchmark-model suite. At its
core is SAgoge, the largest and most comprehensive multimodal dataset for SVG
tasks, encompassing both static graphics and dynamic animations. It covers
icons, long-sequence illustrations, scientific diagrams, and dynamic
animations, supporting tasks of varied difficulty levels and providing deeper
hierarchies with richer attributes compared to previous datasets. Based on this
resource, we introduce SArena, a companion benchmark with comprehensive task
definitions and standardized evaluation that aligns with the domains and
difficulty spectrum covered by SAgoge. Building on these foundations, we
propose InternSVG, a unified MLLM for SVG understanding, editing, and
generation with SVG-specific special tokens, subword-based embedding
initialization, and a two-stage training strategy that progresses from short
static SVGs to long-sequence illustrations and complex animations. This unified
formulation induces positive transfer and improves overall performance.
Experiments on SArena and prior benchmark confirm that InternSVG achieves
substantial gains and consistently outperforms leading open and proprietary
counterparts.

</details>


### [267] [Reasoning as Representation: Rethinking Visual Reinforcement Learning in Image Quality Assessment](https://arxiv.org/abs/2510.11369)
*Shijie Zhao,Xuanyu Zhang,Weiqi Li,Junlin Li,Li Zhang,Tianfan Xue,Jian Zhang*

Main category: cs.CV

TL;DR: 本文揭示了基于推理的IQA模型泛化能力源于将视觉信息转化为紧凑的文本表示，并提出RALI算法，通过对比学习直接对齐图像与这些可泛化文本表示，在保持相似泛化性能的同时，大幅降低了模型参数和推理时间。


<details>
  <summary>Details</summary>
Motivation: 基于强化学习（RL）训练的推理IQA模型展现出卓越的泛化能力，但其潜在机制和关键驱动因素尚不明确。此外，这些模型推理能耗和延迟高，限制了其在特定场景的部署。

Method: 通过实验验证了RL训练使多模态大模型（MLLMs）将冗余视觉表示转化为紧凑、跨域对齐的文本表示，这是其泛化能力的来源。基于此洞察，提出了RALI算法，该算法采用对比学习，直接将图像与RL学习到的可泛化文本表示对齐，无需推理过程和加载大型语言模型（LLM）。

Result: RALI框架在质量评分任务上实现了与基于推理模型相当的泛化性能，但所需的模型参数和推理时间不到后者的5%。

Conclusion: 基于推理的IQA模型泛化能力来源于将视觉表示转换为紧凑、跨域对齐的文本表示。RALI算法利用这一机制，通过直接对齐图像与这些文本表示，显著提升了效率，同时保持了出色的泛化性能。

Abstract: Reasoning-based image quality assessment (IQA) models trained through
reinforcement learning (RL) exhibit exceptional generalization, yet the
underlying mechanisms and critical factors driving this capability remain
underexplored in current research. Moreover, despite their superior
performance, these models incur inference energy usage and latency orders of
magnitude higher than their earlier counterparts, restricting their deployment
in specific scenarios. Through extensive experiments, this paper verifies and
elaborates that through RL training, MLLMs leverage their reasoning capability
to convert redundant visual representations into compact, cross-domain aligned
text representations. This conversion is precisely the source of the
generalization exhibited by these reasoning-based IQA models. Building on this
fundamental insight, we propose a novel algorithm, RALI, which employs
contrastive learning to directly align images with these generalizable text
representations learned by RL. This approach eliminates the reliance on
reasoning processes and even obviates the need to load an LLM. For the quality
scoring task, this framework achieves generalization performance comparable to
reasoning-based models while requiring less than 5% of their model parameters
and inference time.

</details>


### [268] [Enhancing Maritime Domain Awareness on Inland Waterways: A YOLO-Based Fusion of Satellite and AIS for Vessel Characterization](https://arxiv.org/abs/2510.11449)
*Geoffery Agorku,Sarah Hernandez,Hayley Hames,Cade Wagner*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的框架，通过融合高分辨率卫星图像和AIS数据来增强内陆水域的态势感知能力，有效识别“暗船”并验证合作交通。


<details>
  <summary>Details</summary>
Motivation: 内陆水域的海事领域态势感知（MDA）面临合作系统（如AIS）的脆弱性挑战，限制了基于AIS的监测能力，需要一种非合作的补充方法。

Method: 该研究融合了高分辨率卫星图像和AIS船舶轨迹数据。利用YOLO v11目标检测模型对船舶和驳船进行类型、驳船覆盖、运行状态、驳船数量和航向的检测和特征描述。开发了一个包含4,550个标注实例的自建数据集，涵盖了5,973平方英里的密西西比河下游图像。

Result: 在测试集上，船舶分类的F1分数为95.8%；驳船覆盖检测的F1分数为91.6%；运行状态分类的F1分数为99.4%；方向性检测准确率为93.8%。驳船数量估算的平均绝对误差（MAE）为2.4艘。跨地理不相连河段的空间可迁移性分析显示准确率高达98%。

Conclusion: 研究结果强调了将非合作卫星传感与AIS融合的可行性。该方法能够实现近实时船队盘点、支持异常检测，并为内陆水域监测生成高质量数据。未来的工作将扩展标注数据集、整合时间跟踪并探索多模态深度学习以进一步增强操作可扩展性。

Abstract: Maritime Domain Awareness (MDA) for inland waterways remains challenged by
cooperative system vulnerabilities. This paper presents a novel framework that
fuses high-resolution satellite imagery with vessel trajectory data from the
Automatic Identification System (AIS). This work addresses the limitations of
AIS-based monitoring by leveraging non-cooperative satellite imagery and
implementing a fusion approach that links visual detections with AIS data to
identify dark vessels, validate cooperative traffic, and support advanced MDA.
The You Only Look Once (YOLO) v11 object detection model is used to detect and
characterize vessels and barges by vessel type, barge cover, operational
status, barge count, and direction of travel. An annotated data set of 4,550
instances was developed from $5{,}973~\mathrm{mi}^2$ of Lower Mississippi River
imagery. Evaluation on a held-out test set demonstrated vessel classification
(tugboat, crane barge, bulk carrier, cargo ship, and hopper barge) with an F1
score of 95.8\%; barge cover (covered or uncovered) detection yielded an F1
score of 91.6\%; operational status (staged or in motion) classification
reached an F1 score of 99.4\%. Directionality (upstream, downstream) yielded
93.8\% accuracy. The barge count estimation resulted in a mean absolute error
(MAE) of 2.4 barges. Spatial transferability analysis across geographically
disjoint river segments showed accuracy was maintained as high as 98\%. These
results underscore the viability of integrating non-cooperative satellite
sensing with AIS fusion. This approach enables near-real-time fleet
inventories, supports anomaly detection, and generates high-quality data for
inland waterway surveillance. Future work will expand annotated datasets,
incorporate temporal tracking, and explore multi-modal deep learning to further
enhance operational scalability.

</details>


### [269] [Robust Ego-Exo Correspondence with Long-Term Memory](https://arxiv.org/abs/2510.11417)
*Yijun Hu,Bing Fan,Xin Gu,Haiqing Ren,Dongfang Liu,Heng Fan,Libo Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为LM-EEC的新型框架，通过引入双内存架构和受MoE启发的自适应特征路由模块，解决了SAM 2在主观-客观视角对应任务中特征融合效率低下和长视频内存容量有限的问题，并在EgoExo4D基准测试中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 智能助手需要建立主观视角和客观视角之间的物体级对应关系，以提供精确的视觉指导。然而，这项任务面临极端视角变化、遮挡和小物体等挑战。现有方法（包括简单应用SAM 2）在解决这些挑战时，特别是长视频中的特征融合和长期记忆能力方面表现不足。

Method: 本文提出了一个基于SAM 2的长期记忆主观-客观视角对应（EEC）框架，名为LM-EEC。该框架包含：(i) 一个Memory-View MoE模块，它具有双分支路由机制，能够沿通道和空间维度自适应地为每个专家特征分配贡献权重；(ii) 一个带有简单而有效压缩策略的双内存银行系统，用于保留关键的长期信息并消除冗余。

Result: 在具有挑战性的EgoExo4D基准测试中，LM-EEC方法取得了新的最先进（SOTA）结果，显著优于现有方法和SAM 2基线，展示了其在不同场景下的强大泛化能力。

Conclusion: LM-EEC框架通过其创新的双内存架构和MoE启发式自适应特征路由模块，有效解决了主观-客观视角对应任务中的关键挑战，特别是长期记忆和特征融合问题，从而实现了卓越的性能和强大的泛化能力。

Abstract: Establishing object-level correspondence between egocentric and exocentric
views is essential for intelligent assistants to deliver precise and intuitive
visual guidance. However, this task faces numerous challenges, including
extreme viewpoint variations, occlusions, and the presence of small objects.
Existing approaches usually borrow solutions from video object segmentation
models, but still suffer from the aforementioned challenges. Recently, the
Segment Anything Model 2 (SAM 2) has shown strong generalization capabilities
and excellent performance in video object segmentation. Yet, when simply
applied to the ego-exo correspondence (EEC) task, SAM 2 encounters severe
difficulties due to ineffective ego-exo feature fusion and limited long-term
memory capacity, especially for long videos. Addressing these problems, we
propose a novel EEC framework based on SAM 2 with long-term memories by
presenting a dual-memory architecture and an adaptive feature routing module
inspired by Mixture-of-Experts (MoE). Compared to SAM 2, our approach features
(i) a Memory-View MoE module which consists of a dual-branch routing mechanism
to adaptively assign contribution weights to each expert feature along both
channel and spatial dimensions, and (ii) a dual-memory bank system with a
simple yet effective compression strategy to retain critical long-term
information while eliminating redundancy. In the extensive experiments on the
challenging EgoExo4D benchmark, our method, dubbed LM-EEC, achieves new
state-of-the-art results and significantly outperforms existing methods and the
SAM 2 baseline, showcasing its strong generalization across diverse scenarios.
Our code and model are available at https://github.com/juneyeeHu/LM-EEC.

</details>


### [270] [Coupled Degradation Modeling and Fusion: A VLM-Guided Degradation-Coupled Network for Degradation-Aware Infrared and Visible Image Fusion](https://arxiv.org/abs/2510.11456)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui*

Main category: cs.CV

TL;DR: 本文提出了一种名为VGDCFusion的新型红外与可见光图像融合网络，它将图像降质处理与融合过程紧密结合，并利用视觉-语言模型（VLMs）进行降质感知和抑制，从而在降质图像场景下显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有红外与可见光图像融合（IVIF）方法通常假设输入图像质量高。当处理降质图像时，这些方法过度依赖手动切换不同的预处理技术。这种降质处理与图像融合的解耦导致了显著的性能下降。

Method: 本文提出了一种新型的VLM引导降质耦合融合网络（VGDCFusion）。该网络将降质建模与融合过程紧密耦合，并利用视觉-语言模型（VLMs）进行降质感知和引导抑制。具体而言，提出的特定提示降质耦合提取器（SPDCE）实现了模态特定的降质感知，并建立了降质抑制与模态内特征提取的联合建模。同时，联合提示降质耦合融合（JPDCF）促进了跨模态降质感知，并将残余降质过滤与互补的跨模态特征融合相结合。

Result: 大量的实验表明，在各种降质图像场景下，我们的VGDCFusion显著优于现有的最先进融合方法。

Conclusion: VGDCFusion通过将降质处理与图像融合紧密耦合，并利用VLM进行感知和抑制，有效解决了降质图像融合的挑战，并取得了卓越的性能。

Abstract: Existing Infrared and Visible Image Fusion (IVIF) methods typically assume
high-quality inputs. However, when handing degraded images, these methods
heavily rely on manually switching between different pre-processing techniques.
This decoupling of degradation handling and image fusion leads to significant
performance degradation. In this paper, we propose a novel VLM-Guided
Degradation-Coupled Fusion network (VGDCFusion), which tightly couples
degradation modeling with the fusion process and leverages vision-language
models (VLMs) for degradation-aware perception and guided suppression.
Specifically, the proposed Specific-Prompt Degradation-Coupled Extractor
(SPDCE) enables modality-specific degradation awareness and establishes a joint
modeling of degradation suppression and intra-modal feature extraction. In
parallel, the Joint-Prompt Degradation-Coupled Fusion (JPDCF) facilitates
cross-modal degradation perception and couples residual degradation filtering
with complementary cross-modal feature fusion. Extensive experiments
demonstrate that our VGDCFusion significantly outperforms existing
state-of-the-art fusion approaches under various degraded image scenarios. Our
code is available at https://github.com/Lmmh058/VGDCFusion.

</details>


### [271] [Towards Fast and Scalable Normal Integration using Continuous Components](https://arxiv.org/abs/2510.11508)
*Francesco Milano,Jen Jen Chung,Lionel Ott,Roland Siegwart*

Main category: cs.CV

TL;DR: 本文将表面法线积分问题重新定义为连续分量的相对尺度估计，通过分组像素并共享尺度，显著加快了大规模法线图的表面重建速度，并取得了最先进的结果。


<details>
  <summary>Details</summary>
Motivation: 现有的表面法线积分方法需要迭代全局优化来估计每个像素的深度，这导致在大规模法线图上计算效率低下。

Method: 作者将法线积分重构为连续分量相对尺度的估计问题。通过限制属于同一分量的像素共同改变其尺度，大幅减少了优化变量的数量。该框架包括：用于初始准确估计连续分量的启发式方法、重新平衡优化项的策略，以及迭代合并分量以进一步缩小问题规模的技术。

Result: 该方法在标准法线积分基准测试中取得了最先进的结果，仅需几秒钟即可完成。在大分辨率法线图上，与像素级方法相比，实现了数量级上的速度提升。

Conclusion: 通过将法线积分问题转化为连续分量的尺度估计，并引入分量分组和合并策略，本文的方法显著提高了表面法线积分的速度，特别是在处理大型法线图时，同时保持了高精度。

Abstract: Surface normal integration is a fundamental problem in computer vision,
dealing with the objective of reconstructing a surface from its corresponding
normal map. Existing approaches require an iterative global optimization to
jointly estimate the depth of each pixel, which scales poorly to larger normal
maps. In this paper, we address this problem by recasting normal integration as
the estimation of relative scales of continuous components. By constraining
pixels belonging to the same component to jointly vary their scale, we
drastically reduce the number of optimization variables. Our framework includes
a heuristic to accurately estimate continuous components from the start, a
strategy to rebalance optimization terms, and a technique to iteratively merge
components to further reduce the size of the problem. Our method achieves
state-of-the-art results on the standard normal integration benchmark in as
little as a few seconds and achieves one-order-of-magnitude speedup over
pixel-level approaches on large-resolution normal maps.

</details>


### [272] [VA-GS: Enhancing the Geometric Representation of Gaussian Splatting via View Alignment](https://arxiv.org/abs/2510.11473)
*Qing Li,Huifang Feng,Xun Gong,Yu-Shen Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为VA-GS的新方法，通过结合边缘感知图像线索、可见性感知光度对齐损失、基于法线的约束以及深度图像特征嵌入，显著提升了3D Gaussian Splatting在表面重建和新视角合成方面的几何精度和跨视角一致性。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting虽然在新视角合成方面表现出色，但其离散和非结构化的高斯性质，以及仅基于图像渲染损失的监督，导致其在准确表面重建方面能力不足，常出现不准确的几何形状和不一致的多视角对齐问题。

Method: 该方法通过“视角对齐（View Alignment, VA）”来增强3D高斯几何表示：1. 在渲染损失中加入边缘感知图像线索以改善表面边界描绘。2. 引入可见性感知光度对齐损失，建模遮挡并促进高斯之间的准确空间关系。3. 整合基于法线的约束以细化高斯空间方向并改善局部表面估计。4. 利用深度图像特征嵌入来强制跨视角一致性，增强学习几何在不同视角和光照下的鲁棒性。

Result: 在标准基准测试中，该方法在表面重建和新视角合成方面均达到了最先进的性能。

Conclusion: 所提出的方法通过多方面的几何和一致性约束，成功解决了3D Gaussian Splatting在准确表面重建方面的不足，并同时保持了其在新视角合成的优势。

Abstract: 3D Gaussian Splatting has recently emerged as an efficient solution for
high-quality and real-time novel view synthesis. However, its capability for
accurate surface reconstruction remains underexplored. Due to the discrete and
unstructured nature of Gaussians, supervision based solely on image rendering
loss often leads to inaccurate geometry and inconsistent multi-view alignment.
In this work, we propose a novel method that enhances the geometric
representation of 3D Gaussians through view alignment (VA). Specifically, we
incorporate edge-aware image cues into the rendering loss to improve surface
boundary delineation. To enforce geometric consistency across views, we
introduce a visibility-aware photometric alignment loss that models occlusions
and encourages accurate spatial relationships among Gaussians. To further
mitigate ambiguities caused by lighting variations, we incorporate normal-based
constraints to refine the spatial orientation of Gaussians and improve local
surface estimation. Additionally, we leverage deep image feature embeddings to
enforce cross-view consistency, enhancing the robustness of the learned
geometry under varying viewpoints and illumination. Extensive experiments on
standard benchmarks demonstrate that our method achieves state-of-the-art
performance in both surface reconstruction and novel view synthesis. The source
code is available at https://github.com/LeoQLi/VA-GS.

</details>


### [273] [ODI-Bench: Can MLLMs Understand Immersive Omnidirectional Environments?](https://arxiv.org/abs/2510.11549)
*Liu Yang,Huiyu Duan,Ran Tao,Juntao Cheng,Sijing Wu,Yunhao Li,Jing Liu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 该研究提出了ODI-Bench，一个用于全景图像理解的综合基准，并评估了多模态大语言模型（MLLMs）在该领域的表现。结果显示当前MLLMs仍有不足，为此研究引入了Omni-CoT，一种无需训练的方法，通过思维链推理显著提升了MLLMs对全景环境的理解能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在传统2D图像和视频理解方面表现出色，但其理解全景图像（ODIs）所捕捉的沉浸式环境的能力尚未得到充分探索。全景图像在VR、AR和具身智能应用中被广泛采用，因此评估和提升MLLMs对ODIs的理解能力至关重要。

Method: 1. 构建了ODI-Bench，一个专门用于全景图像理解的综合基准，包含2,000张高质量全景图像和超过4,000个手动标注的问答对，涵盖10项细粒度任务（包括通用级和空间级理解）。2. 在封闭式和开放式设置下，对20个代表性MLLMs（包括专有和开源模型）进行了广泛实验评估。3. 提出了一种名为Omni-CoT的无需训练方法，通过结合文本信息和视觉线索的思维链推理，显著增强了MLLMs在全景环境中的理解能力。

Result: 1. 实验结果表明，当前的多模态大语言模型（MLLMs）在捕捉全景图像提供的沉浸式上下文方面仍然存在困难。2. 所提出的Omni-CoT方法显著提升了MLLMs在全景环境中的理解能力。

Conclusion: 当前的多模态大语言模型在全景图像理解方面仍面临挑战。ODI-Bench为该领域提供了一个重要的评估工具，而Omni-CoT则是一种有效且无需训练的方法，能够显著提升MLLMs对全景环境的理解能力，为未来研究指明了方向。

Abstract: Omnidirectional images (ODIs) provide full 360x180 view which are widely
adopted in VR, AR and embodied intelligence applications. While multi-modal
large language models (MLLMs) have demonstrated remarkable performance on
conventional 2D image and video understanding benchmarks, their ability to
comprehend the immersive environments captured by ODIs remains largely
unexplored. To address this gap, we first present ODI-Bench, a novel
comprehensive benchmark specifically designed for omnidirectional image
understanding. ODI-Bench contains 2,000 high-quality omnidirectional images and
over 4,000 manually annotated question-answering (QA) pairs across 10
fine-grained tasks, covering both general-level and spatial-level ODI
understanding. Extensive experiments are conducted to benchmark 20
representative MLLMs, including proprietary and open-source models, under both
close-ended and open-ended settings. Experimental results reveal that current
MLLMs still struggle to capture the immersive context provided by ODIs. To this
end, we further introduce Omni-CoT, a training-free method which significantly
enhances MLLMs' comprehension ability in the omnidirectional environment
through chain-of-thought reasoning across both textual information and visual
cues. Both the benchmark and the code will be released upon the publication.

</details>


### [274] [Massive Activations are the Key to Local Detail Synthesis in Diffusion Transformers](https://arxiv.org/abs/2510.11538)
*Chaofan Gan,Zicheng Zhao,Yuanpeng Tu,Xi Chen,Ziran Qin,Tieyuan Chen,Mehrtash Harandi,Weiyao Lin*

Main category: cs.CV

TL;DR: 本研究系统地调查了扩散Transformer（DiTs）中的“大量激活”（MAs），发现它们对局部细节合成至关重要。在此基础上，提出了一种无需训练的自引导策略——细节引导（DG），通过利用MAs来增强DiTs的局部细节保真度，并能与无分类器引导（CFG）无缝集成，显著提升细节质量。


<details>
  <summary>Details</summary>
Motivation: 扩散Transformer（DiTs）已成为强大的视觉生成骨干网络，但其内部特征图中的“大量激活”（MAs）现象虽已被观察到，其功能和作用机制仍不清楚。

Method: 本研究首先系统地调查了DiTs中MAs的特性及其在视觉生成中的作用。在此基础上，提出了一种名为“细节引导”（DG）的MAs驱动、无需训练的自引导策略。DG通过破坏MAs来构建一个“细节缺陷”的降级模型，并利用该模型引导原始网络实现更高质量的细节合成。DG可以与无分类器引导（CFG）无缝集成，以进一步细化细节。

Result: 研究发现MAs遍布所有空间tokens，其分布受输入时间步嵌入调制。重要的是，MAs在局部细节合成中扮演关键角色，而对输出的整体语义内容影响甚微。实验证明，DG能持续改进各种预训练DiTs（如SD3、SD3.5和Flux）的细粒度细节质量。

Conclusion: DiTs中的“大量激活”（MAs）是局部细节合成的关键。本研究提出的细节引导（DG）策略，通过利用MAs，能够有效且无需训练地增强DiTs的局部细节保真度，并与现有引导方法兼容，为高质量视觉生成提供了新途径。

Abstract: Diffusion Transformers (DiTs) have recently emerged as a powerful backbone
for visual generation. Recent observations reveal \emph{Massive Activations}
(MAs) in their internal feature maps, yet their function remains poorly
understood. In this work, we systematically investigate these activations to
elucidate their role in visual generation. We found that these massive
activations occur across all spatial tokens, and their distribution is
modulated by the input timestep embeddings. Importantly, our investigations
further demonstrate that these massive activations play a key role in local
detail synthesis, while having minimal impact on the overall semantic content
of output. Building on these insights, we propose \textbf{D}etail
\textbf{G}uidance (\textbf{DG}), a MAs-driven, training-free self-guidance
strategy to explicitly enhance local detail fidelity for DiTs. Specifically, DG
constructs a degraded ``detail-deficient'' model by disrupting MAs and
leverages it to guide the original network toward higher-quality detail
synthesis. Our DG can seamlessly integrate with Classifier-Free Guidance (CFG),
enabling further refinements of fine-grained details. Extensive experiments
demonstrate that our DG consistently improves fine-grained detail quality
across various pre-trained DiTs (\eg, SD3, SD3.5, and Flux).

</details>


### [275] [How many samples to label for an application given a foundation model? Chest X-ray classification study](https://arxiv.org/abs/2510.11553)
*Nikolay Nechaev,Evgenia Przhezdzetskaya,Viktor Gombolevskiy,Dmitry Umerenkov,Dmitry Dylov*

Main category: cs.CV

TL;DR: 本研究评估了在胸部X射线分类中使用幂律拟合来预测达到特定性能阈值（ROC-AUC）所需的标记样本数量。结果表明，基础模型（如XrayCLIP和XraySigLIP）比传统模型（ResNet-50）需要显著更少的标记数据，并且仅通过50个标记样本的早期学习曲线斜率就能准确预测最终性能，从而有效降低标注成本。


<details>
  <summary>Details</summary>
Motivation: 胸部X射线分类对于准确诊断至关重要，但通常需要大量的带注释数据。基础模型可以缓解这种对大量数据的依赖，但目前尚不清楚究竟需要多少标记样本才能达到理想的性能。

Method: 研究系统地评估了使用幂律拟合来预测达到特定ROC-AUC阈值所需的训练样本数量。测试了多种病理和基础模型（XrayCLIP和XraySigLIP），并与ResNet-50基线模型进行了比较。关键是，通过仅50个标记病例的学习曲线斜率来预测最终性能平台。

Result: XrayCLIP和XraySigLIP基础模型在性能上表现出色，且所需的标记样本数量远少于ResNet-50基线模型。此外，仅从50个标记病例中提取的学习曲线斜率能够准确预测最终的性能平台。

Conclusion: 研究结果表明，通过幂律拟合和早期学习曲线分析，从业者可以预测达到目标性能所需的最小标记样本数量，从而显著降低标注成本并优化资源利用。

Abstract: Chest X-ray classification is vital yet resource-intensive, typically
demanding extensive annotated data for accurate diagnosis. Foundation models
mitigate this reliance, but how many labeled samples are required remains
unclear. We systematically evaluate the use of power-law fits to predict the
training size necessary for specific ROC-AUC thresholds. Testing multiple
pathologies and foundation models, we find XrayCLIP and XraySigLIP achieve
strong performance with significantly fewer labeled examples than a ResNet-50
baseline. Importantly, learning curve slopes from just 50 labeled cases
accurately forecast final performance plateaus. Our results enable
practitioners to minimize annotation costs by labeling only the essential
samples for targeted performance.

</details>


### [276] [mmWalk: Towards Multi-modal Multi-view Walking Assistance](https://arxiv.org/abs/2510.11520)
*Kedi Ying,Ruiping Liu,Chongyan Chen,Mingzhe Tao,Hao Shi,Kailun Yang,Jiaming Zhang,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 该研究构建了mmWalk，一个模拟多模态数据集，并创建了mmWalkVQA视觉问答基准，旨在解决视障人士在复杂环境中的步行辅助挑战，并推动多模态步行辅助技术的发展。


<details>
  <summary>Details</summary>
Motivation: 视障人士在极端或复杂环境中步行面临重大挑战，主要原因是缺乏对场景的整体理解。这项研究的动机源于视障群体的真实世界需求。

Method: 研究构建了mmWalk，一个模拟多模态数据集，包含120条人工控制、场景分类的步行轨迹，以及62k同步帧，超过559k张RGB、深度和语义模态的全景图像，并强调户外特殊情况和无障碍地标。此外，还生成了mmWalkVQA，一个包含超过69k视觉问答对的VQA基准，专为安全和知情的步行辅助设计。研究评估了最先进的视觉-语言模型（VLMs），并验证了mmWalk微调模型在真实世界数据集上的效果。

Result: 评估发现，最先进的VLMs在mmWalkVQA的风险评估和导航任务上表现不佳。经过mmWalk微调的模型在真实世界数据集上显示出有效性。

Conclusion: mmWalk数据集对于推动多模态步行辅助技术的发展具有显著的有效性，能够帮助视障人士更好地理解复杂环境。

Abstract: Walking assistance in extreme or complex environments remains a significant
challenge for people with blindness or low vision (BLV), largely due to the
lack of a holistic scene understanding. Motivated by the real-world needs of
the BLV community, we build mmWalk, a simulated multi-modal dataset that
integrates multi-view sensor and accessibility-oriented features for outdoor
safe navigation. Our dataset comprises 120 manually controlled,
scenario-categorized walking trajectories with 62k synchronized frames. It
contains over 559k panoramic images across RGB, depth, and semantic modalities.
Furthermore, to emphasize real-world relevance, each trajectory involves
outdoor corner cases and accessibility-specific landmarks for BLV users.
Additionally, we generate mmWalkVQA, a VQA benchmark with over 69k visual
question-answer triplets across 9 categories tailored for safe and informed
walking assistance. We evaluate state-of-the-art Vision-Language Models (VLMs)
using zero- and few-shot settings and found they struggle with our risk
assessment and navigational tasks. We validate our mmWalk-finetuned model on
real-world datasets and show the effectiveness of our dataset for advancing
multi-modal walking assistance.

</details>


### [277] [Situat3DChange: Situated 3D Change Understanding Dataset for Multimodal Large Language Model](https://arxiv.org/abs/2510.11509)
*Ruiping Liu,Junwei Zheng,Yufan Chen,Zirui Wang,Kunyu Peng,Kailun Yang,Jiaming Zhang,Marc Pollefeys,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 本文介绍了Situat3DChange数据集和SCReasoner方法，旨在提升多模态大模型（MLLMs）对动态三维场景和情境变化的理解能力，克服现有数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的3D数据集和评估基准要么只关注动态场景，要么只关注孤立的动态情境，导致对物理环境动态变化的理解不完整。研究旨在弥补这一不足，提供更全面的情境感知变化理解。

Method: 研究构建了Situat3DChange数据集，包含12.1万个问答对、3.6万个变化描述和1.7万个重排指令，基于1.1万个人类对环境变化的观察，结合自我中心和异物中心视角、分类和坐标空间关系，并利用LLM进行整合。为解决微小变化的3D点云对比难题，提出了SCReasoner，一个高效的3D MLLM方法，能在参数开销和语言解码器token需求极小的情况下有效比较点云。

Result: 在Situat3DChange任务上的全面评估揭示了MLLMs在动态场景和情境理解方面的进展与局限性。额外的数据扩缩和跨域迁移实验表明，Situat3DChange作为MLLMs的训练数据集具有任务无关的有效性。

Conclusion: Situat3DChange数据集和SCReasoner方法为动态三维环境和情境变化的理解提供了新的基准和工具，推动了MLLMs在该领域的发展，并揭示了未来的研究方向。

Abstract: Physical environments and circumstances are fundamentally dynamic, yet
current 3D datasets and evaluation benchmarks tend to concentrate on either
dynamic scenarios or dynamic situations in isolation, resulting in incomplete
comprehension. To overcome these constraints, we introduce Situat3DChange, an
extensive dataset supporting three situation-aware change understanding tasks
following the perception-action model: 121K question-answer pairs, 36K change
descriptions for perception tasks, and 17K rearrangement instructions for the
action task. To construct this large-scale dataset, Situat3DChange leverages
11K human observations of environmental changes to establish shared mental
models and shared situational awareness for human-AI collaboration. These
observations, enriched with egocentric and allocentric perspectives as well as
categorical and coordinate spatial relations, are integrated using an LLM to
support understanding of situated changes. To address the challenge of
comparing pairs of point clouds from the same scene with minor changes, we
propose SCReasoner, an efficient 3D MLLM approach that enables effective point
cloud comparison with minimal parameter overhead and no additional tokens
required for the language decoder. Comprehensive evaluation on Situat3DChange
tasks highlights both the progress and limitations of MLLMs in dynamic scene
and situation understanding. Additional experiments on data scaling and
cross-domain transfer demonstrate the task-agnostic effectiveness of using
Situat3DChange as a training dataset for MLLMs.

</details>


### [278] [SNAP: Towards Segmenting Anything in Any Point Cloud](https://arxiv.org/abs/2510.11565)
*Aniket Gupta,Hanhui Wang,Charles Saunders,Aruni RoyChowdhury,Hanumant Singh,Huaizu Jiang*

Main category: cs.CV

TL;DR: SNAP是一个统一的交互式3D点云分割模型，支持点和文本提示，并能跨室内、室外和空中等多种领域进行泛化，通过域自适应归一化防止负迁移，并在多项基准测试中达到最先进或具有竞争力的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的交互式3D点云分割方法通常局限于单一领域（室内或室外）和单一用户交互形式（空间点击或文本提示）。此外，在多个数据集上训练常导致负迁移，使得工具缺乏泛化能力。

Method: 本文提出了SNAP模型，通过在涵盖室内、室外和空中环境的7个数据集上进行训练，实现了跨域泛化。它采用域自适应归一化来防止负迁移。对于文本提示分割，模型自动生成掩码提议并将其与文本查询的CLIP嵌入进行匹配，从而实现全景和开放词汇分割。

Result: SNAP在8个空间提示零样本基准测试中的8个上实现了最先进的性能，并在所有5个文本提示基准测试中展示了具有竞争力的结果。实验表明，SNAP始终能提供高质量的分割结果。

Conclusion: 研究结果表明，统一模型可以匹敌甚至超越专门的领域特定方法，为可扩展的3D标注提供了一个实用的工具。

Abstract: Interactive 3D point cloud segmentation enables efficient annotation of
complex 3D scenes through user-guided prompts. However, current approaches are
typically restricted in scope to a single domain (indoor or outdoor), and to a
single form of user interaction (either spatial clicks or textual prompts).
Moreover, training on multiple datasets often leads to negative transfer,
resulting in domain-specific tools that lack generalizability. To address these
limitations, we present \textbf{SNAP} (\textbf{S}egment a\textbf{N}ything in
\textbf{A}ny \textbf{P}oint cloud), a unified model for interactive 3D
segmentation that supports both point-based and text-based prompts across
diverse domains. Our approach achieves cross-domain generalizability by
training on 7 datasets spanning indoor, outdoor, and aerial environments, while
employing domain-adaptive normalization to prevent negative transfer. For
text-prompted segmentation, we automatically generate mask proposals without
human intervention and match them against CLIP embeddings of textual queries,
enabling both panoptic and open-vocabulary segmentation. Extensive experiments
demonstrate that SNAP consistently delivers high-quality segmentation results.
We achieve state-of-the-art performance on 8 out of 9 zero-shot benchmarks for
spatial-prompted segmentation and demonstrate competitive results on all 5
text-prompted benchmarks. These results show that a unified model can match or
exceed specialized domain-specific approaches, providing a practical tool for
scalable 3D annotation. Project page is at, https://neu-vi.github.io/SNAP/

</details>


### [279] [A Framework for Low-Effort Training Data Generation for Urban Semantic Segmentation](https://arxiv.org/abs/2510.11567)
*Denis Zavadski,Damjan Kalšan,Tim Küchler,Haebom Lee,Stefan Roth,Carsten Rother*

Main category: cs.CV

TL;DR: 本文提出一个新框架，利用不完美的伪标签将现成的扩散模型适应到特定目标领域，从而将低成本的合成数据转化为高质量、与真实领域对齐的训练集，以弥补合成数据与真实数据之间的差距。


<details>
  <summary>Details</summary>
Motivation: 合成数据集在城市场景识别模型训练中广泛应用，但即使是高度真实的渲染图与真实图像之间也存在显著差距，特别是在适应特定目标领域时。这种差距限制了下游性能，而通过更详细的3D建模来弥合差距则成本过高，违背了低成本标注数据的初衷。

Method: 该框架通过仅使用不完美的伪标签，将一个现成的扩散模型适应到目标领域。一旦训练完成，它能从任何合成数据集的语义图中生成高保真、与目标对齐的图像。该方法还包括过滤次优生成、纠正图像-标签错位以及标准化跨数据集的语义。

Result: 在五个合成数据集和两个真实目标数据集上的实验表明，相比最先进的翻译方法，分割mIoU提高了高达+8.0%pt。这使得快速构建的合成数据集与高投入、耗时的合成数据集一样有效。

Conclusion: 这项工作强调了一种有价值的协作范式，即快速语义原型设计与生成模型相结合，能够为城市场景理解实现可扩展、高质量的训练数据创建。

Abstract: Synthetic datasets are widely used for training urban scene recognition
models, but even highly realistic renderings show a noticeable gap to real
imagery. This gap is particularly pronounced when adapting to a specific target
domain, such as Cityscapes, where differences in architecture, vegetation,
object appearance, and camera characteristics limit downstream performance.
Closing this gap with more detailed 3D modelling would require expensive asset
and scene design, defeating the purpose of low-cost labelled data. To address
this, we present a new framework that adapts an off-the-shelf diffusion model
to a target domain using only imperfect pseudo-labels. Once trained, it
generates high-fidelity, target-aligned images from semantic maps of any
synthetic dataset, including low-effort sources created in hours rather than
months. The method filters suboptimal generations, rectifies image-label
misalignments, and standardises semantics across datasets, transforming weak
synthetic data into competitive real-domain training sets. Experiments on five
synthetic datasets and two real target datasets show segmentation gains of up
to +8.0%pt. mIoU over state-of-the-art translation methods, making rapidly
constructed synthetic datasets as effective as high-effort, time-intensive
synthetic datasets requiring extensive manual design. This work highlights a
valuable collaborative paradigm where fast semantic prototyping, combined with
generative models, enables scalable, high-quality training data creation for
urban scene understanding.

</details>


### [280] [Benchmarking foundation models for hyperspectral image classification: Application to cereal crop type mapping](https://arxiv.org/abs/2510.11576)
*Walid Elbarz,Mohamed Bourriz,Hicham Hajji,Hamd Ait Abdelali,François Bourzeix*

Main category: cs.CV

TL;DR: 本研究评估了三种基础模型在利用高光谱图像进行谷物作物测绘方面的性能，其中SpectralEarth预训练的Vision Transformer表现最佳，突出了模型架构对泛化能力的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型正在改变地球观测领域，但它们在高光谱作物测绘方面的潜力尚未得到充分探索。

Method: 研究基准测试了HyperSigma、DOFA以及在SpectralEarth数据集（一个大型多时相高光谱档案）上预训练的Vision Transformers三种基础模型，用于谷物作物测绘。模型在手动标记的训练区域数据上进行微调，并在独立的测试区域进行评估。性能指标包括总体准确率（OA）、平均准确率（AA）和F1分数。此外，还测试了一个从头开始训练的紧凑型SpectralEarth变体。

Result: HyperSigma的OA为34.5%（+/- 1.8%），DOFA达到62.6%（+/- 3.5%），而SpectralEarth模型实现了93.5%（+/- 0.8%）的OA。一个从头开始训练的紧凑型SpectralEarth变体也达到了91%的OA，这强调了模型架构对于跨地理区域和传感器平台强大泛化能力的重要性。

Conclusion: 这些结果为操作性高光谱作物测绘的基础模型提供了系统性评估，并为未来的模型开发指明了方向。

Abstract: Foundation models are transforming Earth observation, but their potential for
hyperspectral crop mapping remains underexplored. This study benchmarks three
foundation models for cereal crop mapping using hyperspectral imagery:
HyperSigma, DOFA, and Vision Transformers pre-trained on the SpectralEarth
dataset (a large multitemporal hyperspectral archive). Models were fine-tuned
on manually labeled data from a training region and evaluated on an independent
test region. Performance was measured with overall accuracy (OA), average
accuracy (AA), and F1-score. HyperSigma achieved an OA of 34.5% (+/- 1.8%),
DOFA reached 62.6% (+/- 3.5%), and the SpectralEarth model achieved an OA of
93.5% (+/- 0.8%). A compact SpectralEarth variant trained from scratch achieved
91%, highlighting the importance of model architecture for strong
generalization across geographic regions and sensor platforms. These results
provide a systematic evaluation of foundation models for operational
hyperspectral crop mapping and outline directions for future model development.

</details>


### [281] [MS-Mix: Unveiling the Power of Mixup for Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.11579)
*Hongyu Zhu,Lin Chen,Mounim A. El-Yacoubi,Mingsheng Shang*

Main category: cs.CV

TL;DR: 针对多模态情感分析（MSA）数据稀缺问题，本文提出了MS-Mix，一个情感敏感的增强框架，通过情感感知样本选择、情感强度引导的混合以及情感对齐损失，显著提高了模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在多模态情感分析中受限于稀缺的多模态标注数据。直接应用基于Mixup的数据增强方法会导致标签模糊和语义不一致，因为缺乏情感感知的混合机制。

Method: MS-Mix框架包含三个关键组件：(1) 情感感知样本选择（SASS）策略，避免混合情感矛盾的样本导致的语义混淆。(2) 情感强度引导（SIG）模块，利用多头自注意力动态计算基于情感强度的模态特定混合比例。(3) 情感对齐损失（SAL），对齐模态间的预测分布，并结合基于KL散度的损失项，联合训练情感强度预测器和骨干网络。

Result: 在三个基准数据集和六个最先进的骨干网络上进行的广泛实验表明，MS-Mix始终优于现有方法，为鲁棒的多模态情感增强设定了新标准。

Conclusion: MS-Mix通过其自适应、情感敏感的增强框架，有效克服了多模态情感分析中数据稀缺和直接Mixup应用的问题，显著提升了模型的性能和泛化能力。

Abstract: Multimodal Sentiment Analysis (MSA) aims to identify and interpret human
emotions by integrating information from heterogeneous data sources such as
text, video, and audio. While deep learning models have advanced in network
architecture design, they remain heavily limited by scarce multimodal annotated
data. Although Mixup-based augmentation improves generalization in unimodal
tasks, its direct application to MSA introduces critical challenges: random
mixing often amplifies label ambiguity and semantic inconsistency due to the
lack of emotion-aware mixing mechanisms. To overcome these issues, we propose
MS-Mix, an adaptive, emotion-sensitive augmentation framework that
automatically optimizes sample mixing in multimodal settings. The key
components of MS-Mix include: (1) a Sentiment-Aware Sample Selection (SASS)
strategy that effectively prevents semantic confusion caused by mixing samples
with contradictory emotions. (2) a Sentiment Intensity Guided (SIG) module
using multi-head self-attention to compute modality-specific mixing ratios
dynamically based on their respective emotional intensities. (3) a Sentiment
Alignment Loss (SAL) that aligns the prediction distributions across
modalities, and incorporates the Kullback-Leibler-based loss as an additional
regularization term to train the emotion intensity predictor and the backbone
network jointly. Extensive experiments on three benchmark datasets with six
state-of-the-art backbones confirm that MS-Mix consistently outperforms
existing methods, establishing a new standard for robust multimodal sentiment
augmentation. The source code is available at:
https://github.com/HongyuZhu-s/MS-Mix.

</details>


### [282] [ACE-G: Improving Generalization of Scene Coordinate Regression Through Query Pre-Training](https://arxiv.org/abs/2510.11605)
*Leonard Bruns,Axel Barroso-Laguna,Tommaso Cavallari,Áron Monszpart,Sowmya Munukutla,Victor Adrian Prisacariu,Eric Brachmann*

Main category: cs.CV

TL;DR: 场景坐标回归（SCR）在视觉重定位中表现出色，但泛化能力不足，尤其是在成像条件与训练视图差异较大时。本文提出ACE-G方法，将坐标回归器与地图表示分离为通用变换器和场景特定地图代码，并通过大规模预训练提升了模型对未见查询图像的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的场景坐标回归（SCR）方法在训练视图与查询图像的成像条件（如光照、视角）差异过大时会失效，这是因为其训练目标导致回归器过度拟合训练视图，缺乏泛化能力。

Method: ACE-G方法将坐标回归器和地图表示分离：一个通用的变换器（transformer）和一个场景特定的地图代码。该变换器可以在数万个场景上进行预训练，并在此过程中学习如何从映射图像泛化到未见的查询图像。

Result: 在多个具有挑战性的重定位数据集上，ACE-G方法显著提高了鲁棒性，同时保持了较低的计算开销。

Conclusion: 通过将坐标回归器与地图表示解耦并进行大规模预训练，ACE-G成功解决了传统SCR框架泛化能力不足的固有局限性，实现了更强大的视觉重定位鲁棒性。

Abstract: Scene coordinate regression (SCR) has established itself as a promising
learning-based approach to visual relocalization. After mere minutes of
scene-specific training, SCR models estimate camera poses of query images with
high accuracy. Still, SCR methods fall short of the generalization capabilities
of more classical feature-matching approaches. When imaging conditions of query
images, such as lighting or viewpoint, are too different from the training
views, SCR models fail. Failing to generalize is an inherent limitation of
previous SCR frameworks, since their training objective is to encode the
training views in the weights of the coordinate regressor itself. The regressor
essentially overfits to the training views, by design. We propose to separate
the coordinate regressor and the map representation into a generic transformer
and a scene-specific map code. This separation allows us to pre-train the
transformer on tens of thousands of scenes. More importantly, it allows us to
train the transformer to generalize from mapping images to unseen query images
during pre-training. We demonstrate on multiple challenging relocalization
datasets that our method, ACE-G, leads to significantly increased robustness
while keeping the computational footprint attractive.

</details>


### [283] [IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment](https://arxiv.org/abs/2510.11647)
*Yinan Chen,Jiangning Zhang,Teng Hu,Yuxiang Zeng,Zhucun Xue,Qingdong He,Chengjie Wang,Yong Liu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: IVEBench是一个专门用于评估指令引导视频编辑的新型基准套件，解决了现有基准在多样性、任务覆盖和评估指标方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑基准无法充分支持指令引导视频编辑的评估，存在来源多样性有限、任务覆盖范围窄和评估指标不完整等问题。

Method: 引入IVEBench，包含600个高质量源视频（涵盖7个语义维度，长度32-1024帧），8大类35个子类编辑任务（提示由大型语言模型生成并经专家审查），以及一个三维评估协议（视频质量、指令依从性、视频保真度），整合了传统指标和多模态大型语言模型评估。

Result: 广泛实验证明IVEBench在基准测试最先进的指令引导视频编辑方法方面的有效性，能够提供全面且与人类评估一致的评估结果。

Conclusion: IVEBench为指令引导视频编辑的系统评估提供了一个现代、全面的解决方案，能够有效衡量现有方法的性能并指导未来研究。

Abstract: Instruction-guided video editing has emerged as a rapidly advancing research
direction, offering new opportunities for intuitive content transformation
while also posing significant challenges for systematic evaluation. Existing
video editing benchmarks fail to support the evaluation of instruction-guided
video editing adequately and further suffer from limited source diversity,
narrow task coverage and incomplete evaluation metrics. To address the above
limitations, we introduce IVEBench, a modern benchmark suite specifically
designed for instruction-guided video editing assessment. IVEBench comprises a
diverse database of 600 high-quality source videos, spanning seven semantic
dimensions, and covering video lengths ranging from 32 to 1,024 frames. It
further includes 8 categories of editing tasks with 35 subcategories, whose
prompts are generated and refined through large language models and expert
review. Crucially, IVEBench establishes a three-dimensional evaluation protocol
encompassing video quality, instruction compliance and video fidelity,
integrating both traditional metrics and multimodal large language model-based
assessments. Extensive experiments demonstrate the effectiveness of IVEBench in
benchmarking state-of-the-art instruction-guided video editing methods, showing
its ability to provide comprehensive and human-aligned evaluation outcomes.

</details>


### [284] [High-resolution Photo Enhancement in Real-time: A Laplacian Pyramid Network](https://arxiv.org/abs/2510.11613)
*Feng Zhang,Haoyou Deng,Zhiqiang Li,Lida Li,Bin Xu,Qingbo Lu,Zisheng Cao,Minchen Wei,Changxin Gao,Nong Sang,Xiang Bai*

Main category: cs.CV

TL;DR: 本文提出LLF-LUT++，一个金字塔网络，通过结合全局和局部操作，利用拉普拉斯金字塔分解和重建，实现高分辨率图像的快速处理和卓越的图像增强性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像增强方法要么性能强大但计算量大，无法部署在边缘设备上；要么计算效率高但性能不足，无法满足实际应用需求。因此，需要一种兼顾性能和效率的解决方案。

Method: LLF-LUT++金字塔网络通过闭合形式的拉普拉斯金字塔分解和重建集成全局和局部操作。它利用图像自适应3D LUT处理下采样图像的全局色调特性，并通过两种权重融合策略实现粗略的全局图像增强。为此，设计了一个空间频率变换器权重预测器，通过频率特征提取独特的权重。此外，它应用局部拉普拉斯滤波器自适应地细化高频分量中的边缘细节。

Result: LLF-LUT++在HDR+数据集上将PSNR提高了2.64 dB，并且进一步缩短了运行时间，在单个GPU上处理4K分辨率图像仅需13毫秒。在两个基准数据集上的广泛实验结果表明，该方法优于现有最先进的方法。

Conclusion: LLF-LUT++通过结合全局和局部操作，实现了卓越的图像增强性能和高效的处理速度，有效解决了现有方法在性能和效率之间权衡的局限性。

Abstract: Photo enhancement plays a crucial role in augmenting the visual aesthetics of
a photograph. In recent years, photo enhancement methods have either focused on
enhancement performance, producing powerful models that cannot be deployed on
edge devices, or prioritized computational efficiency, resulting in inadequate
performance for real-world applications. To this end, this paper introduces a
pyramid network called LLF-LUT++, which integrates global and local operators
through closed-form Laplacian pyramid decomposition and reconstruction. This
approach enables fast processing of high-resolution images while also achieving
excellent performance. Specifically, we utilize an image-adaptive 3D LUT that
capitalizes on the global tonal characteristics of downsampled images, while
incorporating two distinct weight fusion strategies to achieve coarse global
image enhancement. To implement this strategy, we designed a spatial-frequency
transformer weight predictor that effectively extracts the desired distinct
weights by leveraging frequency features. Additionally, we apply local
Laplacian filters to adaptively refine edge details in high-frequency
components. After meticulously redesigning the network structure and
transformer model, LLF-LUT++ not only achieves a 2.64 dB improvement in PSNR on
the HDR+ dataset, but also further reduces runtime, with 4K resolution images
processed in just 13 ms on a single GPU. Extensive experimental results on two
benchmark datasets further show that the proposed approach performs favorably
compared to state-of-the-art methods. The source code will be made publicly
available at https://github.com/fengzhang427/LLF-LUT.

</details>


### [285] [ExpVid: A Benchmark for Experiment Video Understanding & Reasoning](https://arxiv.org/abs/2510.11606)
*Yicheng Xu,Yue Wu,Jiashuo Yu,Ziang Yan,Tianxiang Jiang,Yinan He,Qingsong Zhao,Kai Chen,Yu Qiao,Limin Wang,Manabu Okumura,Yi Wang*

Main category: cs.CV

TL;DR: 多模态大语言模型（MLLMs）在科学发现中有潜力，但现有基准无法评估其在精细、长周期实验室工作中的真实能力。本文引入ExpVid，首个用于评估MLLMs科学实验视频的基准，包含三级任务。评估发现MLLMs在精细识别、状态跟踪和结果关联方面表现不佳，专有模型与开源模型之间存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）有望通过解释复杂实验程序来加速科学发现。然而，现有基准忽略了真实实验室工作（特别是湿实验室环境）的精细和长周期性质，导致人们对其真实能力知之甚少。

Method: 引入了ExpVid，这是第一个专门用于系统评估MLLMs科学实验视频的基准。ExpVid从同行评审的视频出版物中整理而来，并设有一个模拟科学过程的三级任务层次结构：(1) 工具、材料和动作的精细感知；(2) 步骤顺序和完整性的程序理解；(3) 将完整实验与已发表结论联系起来的科学推理。采用结合自动化生成和多学科专家验证的视觉中心注释流程，确保任务需要视觉基础。评估了19个领先的MLLMs在ExpVid上的表现。

Result: 评估发现，虽然MLLMs在粗粒度识别方面表现出色，但它们在区分精细细节、跟踪随时间变化的状态以及将实验程序与科学结果联系起来方面存在困难。研究结果揭示了专有模型和开源模型之间存在显著的性能差距，尤其是在高阶推理方面。

Conclusion: ExpVid不仅提供了一个诊断工具，还为开发能够成为科学实验中值得信赖的合作伙伴的MLLMs指明了发展方向。

Abstract: Multimodal Large Language Models (MLLMs) hold promise for accelerating
scientific discovery by interpreting complex experimental procedures. However,
their true capabilities are poorly understood, as existing benchmarks neglect
the fine-grained and long-horizon nature of authentic laboratory work,
especially in wet-lab settings. To bridge this gap, we introduce ExpVid, the
first benchmark designed to systematically evaluate MLLMs on scientific
experiment videos. Curated from peer-reviewed video publications, ExpVid
features a new three-level task hierarchy that mirrors the scientific process:
(1) Fine-grained Perception of tools, materials, and actions; (2) Procedural
Understanding of step order and completeness; and (3) Scientific Reasoning that
connects the full experiment to its published conclusions. Our vision-centric
annotation pipeline, combining automated generation with multi-disciplinary
expert validation, ensures that tasks require visual grounding. We evaluate 19
leading MLLMs on ExpVid and find that while they excel at coarse-grained
recognition, they struggle with disambiguating fine details, tracking state
changes over time, and linking experimental procedures to scientific outcomes.
Our results reveal a notable performance gap between proprietary and
open-source models, particularly in high-order reasoning. ExpVid not only
provides a diagnostic tool but also charts a roadmap for developing MLLMs
capable of becoming trustworthy partners in scientific experimentation.

</details>


### [286] [Bayesian Topological Convolutional Neural Nets](https://arxiv.org/abs/2510.11704)
*Sarah Harkins Dayton,Hayden Everett,Ioannis Schizas,David L. Boothe Jr.,Vasileios Maroulas*

Main category: cs.CV

TL;DR: 本文提出了一种新型贝叶斯拓扑卷积神经网络（CNN），通过结合拓扑感知学习和贝叶斯采样，解决了传统CNN训练数据量大、预测过度自信和缺乏不确定性量化的问题，尤其在数据有限或损坏的情况下表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统卷积神经网络（CNN）存在以下问题：需要大量数据进行训练；预测往往过于自信；以及缺乏对其预测不确定性的量化能力。

Method: 本文提出了一种新型贝叶斯拓扑CNN。它通过以下方式工作：1) 利用重要流形信息加速训练并减少校准误差；2) 对网络参数设置先验分布并学习适当的后验分布；3) 在学习成本中引入一致性条件，以有效修改先验分布，从而提高网络性能。该方法结合了拓扑感知学习和贝叶斯采样。

Result: 该模型在基准图像分类数据集上表现出优越性，超越了传统CNN、贝叶斯神经网络（BNN）和拓扑CNN。特别是在训练数据有限或损坏的情况下，该方法具有明显优势。此外，新模型比标准BNN能更好地量化不确定性，因为它能更容易识别出未训练过的分布外数据。

Conclusion: 研究结果强调了这种新型混合方法在实现更高效和更鲁棒的图像分类方面的潜力。

Abstract: Convolutional neural networks (CNNs) have been established as the main
workhorse in image data processing; nonetheless, they require large amounts of
data to train, often produce overconfident predictions, and frequently lack the
ability to quantify the uncertainty of their predictions. To address these
concerns, we propose a new Bayesian topological CNN that promotes a novel
interplay between topology-aware learning and Bayesian sampling. Specifically,
it utilizes information from important manifolds to accelerate training while
reducing calibration error by placing prior distributions on network parameters
and properly learning appropriate posteriors. One important contribution of our
work is the inclusion of a consistency condition in the learning cost, which
can effectively modify the prior distributions to improve the performance of
our novel network architecture. We evaluate the model on benchmark image
classification datasets and demonstrate its superiority over conventional CNNs,
Bayesian neural networks (BNNs), and topological CNNs. In particular, we supply
evidence that our method provides an advantage in situations where training
data is limited or corrupted. Furthermore, we show that the new model allows
for better uncertainty quantification than standard BNNs since it can more
readily identify examples of out-of-distribution data on which it has not been
trained. Our results highlight the potential of our novel hybrid approach for
more efficient and robust image classification.

</details>


### [287] [Diffusion Transformers with Representation Autoencoders](https://arxiv.org/abs/2510.11690)
*Boyang Zheng,Nanye Ma,Shengbang Tong,Saining Xie*

Main category: cs.CV

TL;DR: 本文提出用表示自编码器（RAE）替代扩散Transformer（DiT）中过时的VAE，以提供高质量重建和语义丰富的潜在空间，从而提升生成性能。


<details>
  <summary>Details</summary>
Motivation: 目前的扩散Transformer（DiT）普遍依赖原始VAE编码器，存在主干网络过时、潜在空间维度低、表示能力弱等问题，这些限制了生成质量和模型架构的简洁性。

Method: 本文提出用预训练的表示编码器（如DINO, SigLIP, MAE）与训练过的解码器结合，形成表示自编码器（RAEs），以替代传统的VAE。RAE旨在提供高质量重建和语义丰富的潜在空间。针对高维潜在空间中DiT操作的挑战，作者分析了其来源并提出了理论上合理的解决方案。实验中使用了配备轻量级宽DDT头的DiT变体。

Result: 该方法实现了更快的收敛速度，且无需辅助表示对齐损失。在ImageNet上取得了强大的图像生成结果：256x256无引导时FID为1.51，256x256和512x512有引导时FID为1.13。

Conclusion: RAE具有显著优势，应成为扩散Transformer训练的新默认组件。

Abstract: Latent generative modeling, where a pretrained autoencoder maps pixels into a
latent space for the diffusion process, has become the standard strategy for
Diffusion Transformers (DiT); however, the autoencoder component has barely
evolved. Most DiTs continue to rely on the original VAE encoder, which
introduces several limitations: outdated backbones that compromise
architectural simplicity, low-dimensional latent spaces that restrict
information capacity, and weak representations that result from purely
reconstruction-based training and ultimately limit generative quality. In this
work, we explore replacing the VAE with pretrained representation encoders
(e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term
Representation Autoencoders (RAEs). These models provide both high-quality
reconstructions and semantically rich latent spaces, while allowing for a
scalable transformer-based architecture. Since these latent spaces are
typically high-dimensional, a key challenge is enabling diffusion transformers
to operate effectively within them. We analyze the sources of this difficulty,
propose theoretically motivated solutions, and validate them empirically. Our
approach achieves faster convergence without auxiliary representation alignment
losses. Using a DiT variant equipped with a lightweight, wide DDT head, we
achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no
guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers
clear advantages and should be the new default for diffusion transformer
training.

</details>


### [288] [DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training](https://arxiv.org/abs/2510.11712)
*Haoran Feng,Dizhe Zhang,Xiangtai Li,Bo Du,Lu Qi*

Main category: cs.CV

TL;DR: DiT360是一个基于DiT的全景图像生成框架，通过在透视和全景数据上进行混合训练，并采用数据中心的方法解决全景图像生成中的几何保真度和真实感问题。


<details>
  <summary>Details</summary>
Motivation: 全景图像生成在几何保真度和真实感方面的不足，主要归因于缺乏大规模、高质量的真实世界全景数据。这与以往侧重模型设计的方法不同，DiT360采取了数据中心的视角。

Method: DiT360是一个基于DiT的框架，采用混合训练。它包含用于域间转换和域内增强的关键模块，应用于预VAE图像级别和后VAE令牌级别。在图像级别，通过透视图像引导和全景细化融入跨域知识。在令牌级别，应用混合监督，包括用于边界连续性的循环填充、用于旋转鲁棒性的偏航损失和用于失真感知的立方体损失。

Result: 在文本到全景、图像修复和图像外扩任务中，DiT360在十一个量化指标上实现了更好的边界一致性和图像保真度。

Conclusion: DiT360通过其数据中心的混合训练方法以及在图像和令牌级别引入的特定模块，有效提升了全景图像生成的质量，解决了边界一致性和图像保真度的问题。

Abstract: In this work, we propose DiT360, a DiT-based framework that performs hybrid
training on perspective and panoramic data for panoramic image generation. For
the issues of maintaining geometric fidelity and photorealism in generation
quality, we attribute the main reason to the lack of large-scale, high-quality,
real-world panoramic data, where such a data-centric view differs from prior
methods that focus on model design. Basically, DiT360 has several key modules
for inter-domain transformation and intra-domain augmentation, applied at both
the pre-VAE image level and the post-VAE token level. At the image level, we
incorporate cross-domain knowledge through perspective image guidance and
panoramic refinement, which enhance perceptual quality while regularizing
diversity and photorealism. At the token level, hybrid supervision is applied
across multiple modules, which include circular padding for boundary
continuity, yaw loss for rotational robustness, and cube loss for distortion
awareness. Extensive experiments on text-to-panorama, inpainting, and
outpainting tasks demonstrate that our method achieves better boundary
consistency and image fidelity across eleven quantitative metrics. Our code is
available at https://github.com/Insta360-Research-Team/DiT360.

</details>


### [289] [Point Prompting: Counterfactual Tracking with Video Diffusion Models](https://arxiv.org/abs/2510.11715)
*Ayush Shrivastava,Sanyam Mehta,Daniel Geng,Andrew Owens*

Main category: cs.CV

TL;DR: 本文提出一种利用预训练视频扩散模型进行零样本点跟踪的方法，通过在视频中标记点并重新生成帧，实现了优于现有零样本方法的跟踪性能，并能处理遮挡。


<details>
  <summary>Details</summary>
Motivation: 跟踪器分析运动，视频生成器合成运动，两者解决的问题密切相关。本文旨在探索如何利用这种联系，使预训练视频生成模型（特别是视频扩散模型）执行零样本点跟踪任务。

Method: 核心方法是“提示式”跟踪：首先在查询点放置一个独特颜色的标记；然后从中间噪声水平重新生成视频的其余部分，这使得标记在帧间传播，从而追踪点的轨迹。为确保标记在反事实生成中保持可见（尽管自然视频中此类标记不常见），使用未经编辑的初始帧作为负面提示。

Result: 实验表明，这种“涌现式”的跟踪性能优于先前的零样本方法，并且能够持续穿过遮挡。其性能甚至与专业的自监督模型具有竞争力。

Conclusion: 预训练的视频扩散模型可以通过简单的提示机制实现有效的零样本点跟踪。这种方法不仅超越了现有的零样本方法，还能处理复杂场景（如遮挡），并能与专业自监督模型的性能相媲美，揭示了视频生成模型在运动分析方面的潜在能力。

Abstract: Trackers and video generators solve closely related problems: the former
analyze motion, while the latter synthesize it. We show that this connection
enables pretrained video diffusion models to perform zero-shot point tracking
by simply prompting them to visually mark points as they move over time. We
place a distinctively colored marker at the query point, then regenerate the
rest of the video from an intermediate noise level. This propagates the marker
across frames, tracing the point's trajectory. To ensure that the marker
remains visible in this counterfactual generation, despite such markers being
unlikely in natural videos, we use the unedited initial frame as a negative
prompt. Through experiments with multiple image-conditioned video diffusion
models, we find that these "emergent" tracks outperform those of prior
zero-shot methods and persist through occlusions, often obtaining performance
that is competitive with specialized self-supervised models.

</details>


### [290] [PhySIC: Physically Plausible 3D Human-Scene Interaction and Contact from a Single Image](https://arxiv.org/abs/2510.11649)
*Pradyumna Yalandur Muralidhar,Yuxuan Xue,Xianghui Xie,Margaret Kostyrko,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: PhySIC是一个从单张RGB图像重建物理上可信的人体-场景交互和接触的框架，解决了深度模糊、遮挡和物理不一致接触的问题，实现了高精度的三维人体和场景重建。


<details>
  <summary>Details</summary>
Motivation: 从单张图像重建度量精确的人体及其周围场景对于虚拟现实、机器人和全面的三维场景理解至关重要。然而，现有方法在处理深度模糊、遮挡和物理不一致的接触方面存在困难。

Method: PhySIC首先从粗略的单目深度和人体估计开始，然后进行遮挡感知的修复，将可见深度与未缩放几何体融合以构建鲁棒的度量支架，并合成缺失的支撑表面（如地面）。通过置信度加权的优化，PhySIC通过联合强制执行深度对齐、接触先验、避免相互穿透和二维重投影一致性来精炼人体姿态、相机参数和全局尺度。显式遮挡掩码保护不可见区域免受不合理配置的影响。

Result: PhySIC能够从单张RGB图像中恢复度量一致的SMPL-X人体网格、密集的场景表面和顶点级别的接触图，所有这些都在共享坐标系中。它高效（优化仅需9秒，端到端不到27秒），并能处理多个人体。在经验上，PhySIC优于单图像基线，将平均每顶点场景误差从641毫米降低到227毫米，将PA-MPJPE减半至42毫米，并将接触F1从0.09提高到0.51。定性结果显示了逼真的脚地交互、自然的坐姿以及对严重遮挡家具的合理重建。

Conclusion: PhySIC通过将单张图像转换为物理上可信的三维人体-场景对，推动了可扩展的三维场景理解。它有效地解决了从单张图像重建人体-场景交互中的深度模糊、遮挡和物理不一致性挑战。

Abstract: Reconstructing metrically accurate humans and their surrounding scenes from a
single image is crucial for virtual reality, robotics, and comprehensive 3D
scene understanding. However, existing methods struggle with depth ambiguity,
occlusions, and physically inconsistent contacts. To address these challenges,
we introduce PhySIC, a framework for physically plausible Human-Scene
Interaction and Contact reconstruction. PhySIC recovers metrically consistent
SMPL-X human meshes, dense scene surfaces, and vertex-level contact maps within
a shared coordinate frame from a single RGB image. Starting from coarse
monocular depth and body estimates, PhySIC performs occlusion-aware inpainting,
fuses visible depth with unscaled geometry for a robust metric scaffold, and
synthesizes missing support surfaces like floors. A confidence-weighted
optimization refines body pose, camera parameters, and global scale by jointly
enforcing depth alignment, contact priors, interpenetration avoidance, and 2D
reprojection consistency. Explicit occlusion masking safeguards invisible
regions against implausible configurations. PhySIC is efficient, requiring only
9 seconds for joint human-scene optimization and under 27 seconds end-to-end.
It naturally handles multiple humans, enabling reconstruction of diverse
interactions. Empirically, PhySIC outperforms single-image baselines, reducing
mean per-vertex scene error from 641 mm to 227 mm, halving PA-MPJPE to 42 mm,
and improving contact F1 from 0.09 to 0.51. Qualitative results show realistic
foot-floor interactions, natural seating, and plausible reconstructions of
heavily occluded furniture. By converting a single image into a physically
plausible 3D human-scene pair, PhySIC advances scalable 3D scene understanding.
Our implementation is publicly available at https://yuxuan-xue.com/physic.

</details>


### [291] [InfiniHuman: Infinite 3D Human Creation with Precise Control](https://arxiv.org/abs/2510.11650)
*Yuxuan Xue,Xianghui Xie,Margaret Kostyrko,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: InfiniHuman框架通过蒸馏现有基础模型，自动生成大规模、多样化、带丰富标注的3D人体数据（InfiniHumanData），并在此基础上训练了一个扩散模型（InfiniHumanGen），实现了快速、逼真且高度可控的3D虚拟形象生成。


<details>
  <summary>Details</summary>
Motivation: 生成逼真且可控的3D人体虚拟形象面临巨大挑战，尤其是在覆盖广泛属性（如种族、年龄、服装风格、身体形状）时。由于获取和标注大规模人体数据集的成本过高且规模和多样性受限，阻碍了生成模型的发展。

Method: 本文提出了InfiniHuman框架，通过协同蒸馏现有基础模型来生成理论上无限且标注丰富的人体数据。具体包括：1) InfiniHumanData：一个全自动流水线，利用视觉-语言模型和图像生成模型创建大规模多模态数据集。2) InfiniHumanGen：一个基于扩散的生成流水线，以文本、身体形状和服装资产为条件，用于生成虚拟形象，并在InfiniHumanData上进行训练。

Result: InfiniHumanData包含了11.1万个具有前所未有多样性的身份，每个身份都标注了多粒度文本描述、多视角RGB图像、详细服装图像和SMPL身体形状参数。用户研究表明，自动生成的身份与扫描渲染图无法区分。InfiniHumanGen在视觉质量、生成速度和可控性方面显著优于现有最先进方法，实现了高品质、精细化控制的虚拟形象生成，且具有理论上无限的扩展性。

Conclusion: InfiniHuman提供了一个实用且经济的解决方案，通过利用蒸馏的基础模型生成大规模、多样化的3D人体数据，并在此基础上开发了高效的生成模型，从而实现了高品质、精细化控制且规模无限的3D虚拟形象生成。

Abstract: Generating realistic and controllable 3D human avatars is a long-standing
challenge, particularly when covering broad attribute ranges such as ethnicity,
age, clothing styles, and detailed body shapes. Capturing and annotating
large-scale human datasets for training generative models is prohibitively
expensive and limited in scale and diversity. The central question we address
in this paper is: Can existing foundation models be distilled to generate
theoretically unbounded, richly annotated 3D human data? We introduce
InfiniHuman, a framework that synergistically distills these models to produce
richly annotated human data at minimal cost and with theoretically unlimited
scalability. We propose InfiniHumanData, a fully automatic pipeline that
leverages vision-language and image generation models to create a large-scale
multi-modal dataset. User study shows our automatically generated identities
are undistinguishable from scan renderings. InfiniHumanData contains 111K
identities spanning unprecedented diversity. Each identity is annotated with
multi-granularity text descriptions, multi-view RGB images, detailed clothing
images, and SMPL body-shape parameters. Building on this dataset, we propose
InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body
shape, and clothing assets. InfiniHumanGen enables fast, realistic, and
precisely controllable avatar generation. Extensive experiments demonstrate
significant improvements over state-of-the-art methods in visual quality,
generation speed, and controllability. Our approach enables high-quality avatar
generation with fine-grained control at effectively unbounded scale through a
practical and affordable solution. We will publicly release the automatic data
generation pipeline, the comprehensive InfiniHumanData dataset, and the
InfiniHumanGen models at https://yuxuan-xue.com/infini-human.

</details>


### [292] [Ev4DGS: Novel-view Rendering of Non-Rigid Objects from Monocular Event Streams](https://arxiv.org/abs/2510.11717)
*Takuya Nakabayashi,Navami Kairanda,Hideo Saito,Vladislav Golyanik*

Main category: cs.CV

TL;DR: Ev4DGS是首个仅使用单目事件流，实现非刚性变形物体新视角渲染（输出为RGB或灰度图像）的方法，解决了现有方法对RGB输入依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 事件相机在新视角渲染方面优于RGB相机，但现有针对非刚性物体的事件基方法通常需要稀疏RGB输入，这是一个实际限制。目前尚不清楚是否可以仅从事件流中学习此类模型。

Method: 本文提出了Ev4DGS，通过以下方式回归可变形的3D高斯Splatting表示：1) 一个将估计模型输出与2D事件观测空间关联的损失函数；2) 一个从事件生成的二值掩码训练的粗略3D变形模型。

Result: Ev4DGS在现有合成数据集和新记录的真实非刚性物体数据集上进行了实验比较，结果证明了其有效性，并显示出优于多种可应用于此设置的朴素基线方法的性能。

Conclusion: Ev4DGS首次证明了仅凭单目事件流即可实现非刚性变形物体在新观察空间（如RGB或灰度图像）中的新视角渲染，解决了该领域的一个重要开放问题。

Abstract: Event cameras offer various advantages for novel view rendering compared to
synchronously operating RGB cameras, and efficient event-based techniques
supporting rigid scenes have been recently demonstrated in the literature. In
the case of non-rigid objects, however, existing approaches additionally
require sparse RGB inputs, which can be a substantial practical limitation; it
remains unknown if similar models could be learned from event streams only.
This paper sheds light on this challenging open question and introduces Ev4DGS,
i.e., the first approach for novel view rendering of non-rigidly deforming
objects in the explicit observation space (i.e., as RGB or greyscale images)
from monocular event streams. Our method regresses a deformable 3D Gaussian
Splatting representation through 1) a loss relating the outputs of the
estimated model with the 2D event observation space, and 2) a coarse 3D
deformation model trained from binary masks generated from events. We perform
experimental comparisons on existing synthetic and newly recorded real datasets
with non-rigid objects. The results demonstrate the validity of Ev4DGS and its
superior performance compared to multiple naive baselines that can be applied
in our setting. We will release our models and the datasets used in the
evaluation for research purposes; see the project webpage:
https://4dqv.mpi-inf.mpg.de/Ev4DGS/.

</details>


### [293] [Beyond 'Templates': Category-Agnostic Object Pose, Size, and Shape Estimation from a Single View](https://arxiv.org/abs/2510.11687)
*Jinyu Zhang,Haitao Lin,Jiashu Hou,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文提出了一种统一的、与类别无关的框架，可从单张RGB-D图像同时估计物体的6D姿态、尺寸和密集形状。该框架结合了视觉基础模型的2D特征和3D点云，使用Transformer编码器和并行解码器，实现了实时推理，并在零样本泛化方面达到了SOTA水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖于物体特定的先验知识（如CAD模型或模板），要么由于姿态-形状的纠缠和多阶段流程而导致跨类别泛化能力有限。在机器人抓取和操作等关键应用中，需要一个能够从视觉输入估计物体6D姿态、尺寸和形状的通用解决方案。

Method: 该方法是一个统一的、与类别无关的框架，无需模板、CAD模型或测试时的类别标签。它通过一个由专家混合（Mixture-of-Experts）增强的Transformer编码器，融合了来自视觉基础模型的密集2D特征与部分3D点云。随后，使用并行解码器分别进行姿态-尺寸估计和形状重建。模型仅在SOPE数据集的149个类别合成数据上进行训练。

Result: 该框架实现了28 FPS的实时推理速度。在SOPE、ROPE、ObjaversePose和HANDAL四个涵盖300多个类别的基准测试中进行评估，结果显示在已知类别上达到了最先进的准确性，并对未见的真实世界物体展现出卓越的零样本泛化能力。

Conclusion: 该研究为机器人技术和具身AI中的开放集6D理解设定了新标准，提供了一个高效、高泛化能力的解决方案，能够同时预测物体的6D姿态、尺寸和密集形状。

Abstract: Estimating an object's 6D pose, size, and shape from visual input is a
fundamental problem in computer vision, with critical applications in robotic
grasping and manipulation. Existing methods either rely on object-specific
priors such as CAD models or templates, or suffer from limited generalization
across categories due to pose-shape entanglement and multi-stage pipelines. In
this work, we propose a unified, category-agnostic framework that
simultaneously predicts 6D pose, size, and dense shape from a single RGB-D
image, without requiring templates, CAD models, or category labels at test
time. Our model fuses dense 2D features from vision foundation models with
partial 3D point clouds using a Transformer encoder enhanced by a
Mixture-of-Experts, and employs parallel decoders for pose-size estimation and
shape reconstruction, achieving real-time inference at 28 FPS. Trained solely
on synthetic data from 149 categories in the SOPE dataset, our framework is
evaluated on four diverse benchmarks SOPE, ROPE, ObjaversePose, and HANDAL,
spanning over 300 categories. It achieves state-of-the-art accuracy on seen
categories while demonstrating remarkably strong zero-shot generalization to
unseen real-world objects, establishing a new standard for open-set 6D
understanding in robotics and embodied AI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [294] [Table Question Answering in the Era of Large Language Models: A Comprehensive Survey of Tasks, Methods, and Evaluation](https://arxiv.org/abs/2510.09671)
*Wei Zhou,Bolei Ma,Annemarie Friedrich,Mohsen Mesgar*

Main category: cs.CL

TL;DR: 本文综述了表格问答（TQA）领域，特别关注基于大型语言模型（LLM）的方法，系统地分类了基准、任务设置和建模策略，并指出了未充分探索的方向和开放性问题，旨在为TQA社区提供统一的理解和未来研究指导。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLM）在表格问答（TQA）中取得了显著进展，但该领域仍缺乏对任务制定、核心挑战和方法趋势的系统性组织和理解，尤其是在强化学习等新兴研究方向出现后。

Method: 本综述通过以下方式解决上述问题：1. 提供TQA研究的全面结构化概述，重点关注基于LLM的方法。2. 对现有基准和任务设置进行综合分类。3. 根据所针对的挑战对当前建模策略进行分组，并分析其优缺点。4. 突出以前研究中未系统涵盖的、但重要且及时的未充分探索主题。

Result: 本综述通过统一不同的研究线索并识别开放性问题，为TQA社区提供了坚实的基础，加深了对最新技术水平的理解，并为该快速发展领域的未来发展提供了指导。

Conclusion: 本综述为TQA领域提供了一个整合的、系统化的视图，特别强调了LLM的应用，并为研究人员提供了理解现状和探索未来方向的蓝图。

Abstract: Table Question Answering (TQA) aims to answer natural language questions
about tabular data, often accompanied by additional contexts such as text
passages. The task spans diverse settings, varying in table representation,
question/answer complexity, modality involved, and domain. While recent
advances in large language models (LLMs) have led to substantial progress in
TQA, the field still lacks a systematic organization and understanding of task
formulations, core challenges, and methodological trends, particularly in light
of emerging research directions such as reinforcement learning. This survey
addresses this gap by providing a comprehensive and structured overview of TQA
research with a focus on LLM-based methods. We provide a comprehensive
categorization of existing benchmarks and task setups. We group current
modeling strategies according to the challenges they target, and analyze their
strengths and limitations. Furthermore, we highlight underexplored but timely
topics that have not been systematically covered in prior research. By unifying
disparate research threads and identifying open problems, our survey offers a
consolidated foundation for the TQA community, enabling a deeper understanding
of the state of the art and guiding future developments in this rapidly
evolving area.

</details>


### [295] [Emotionally Charged, Logically Blurred: AI-driven Emotional Framing Impairs Human Fallacy Detection](https://arxiv.org/abs/2510.09695)
*Yanran Chen,Lynn Greschner,Roman Klinger,Michael Klenk,Steffen Eger*

Main category: cs.CL

TL;DR: 本研究首次计算性地探讨情感框架如何与谬误和说服力相互作用。结果显示，LLM驱动的情感框架显著降低了人类对谬误的检测能力，并揭示了特定情感与说服力之间的关联。


<details>
  <summary>Details</summary>
Motivation: 公共交流中常见的逻辑谬误具有误导性，且尽管缺乏严谨性，仍可能因说服力的主观性而显得令人信服。因此，研究情感框架如何影响谬误的感知和说服力变得尤为重要。

Method: 研究首先基准测试了八个大型语言模型（LLM），以评估它们在保持逻辑结构的同时，向谬误论证中注入情感诉求的能力。随后，使用表现最佳的LLM生成刺激材料，用于人类参与者的研究，以观察情感框架对谬误检测的影响。

Result: LLM驱动的情感框架使人类谬误检测能力平均降低了14.5%（F1分数）。当感知到“愉悦”时，人类在谬误检测方面表现优于“恐惧”或“悲伤”情绪。此外，“愉悦”、“恐惧”和“悲伤”这三种情绪与中性或其它情绪状态相比，显著提高了论证的说服力。

Conclusion: 本研究揭示了AI驱动的情感操纵在谬误论证背景下的潜在影响，对理解和防范此类操纵具有重要意义。

Abstract: Logical fallacies are common in public communication and can mislead
audiences; fallacious arguments may still appear convincing despite lacking
soundness, because convincingness is inherently subjective. We present the
first computational study of how emotional framing interacts with fallacies and
convincingness, using large language models (LLMs) to systematically change
emotional appeals in fallacious arguments. We benchmark eight LLMs on injecting
emotional appeal into fallacious arguments while preserving their logical
structures, then use the best models to generate stimuli for a human study. Our
results show that LLM-driven emotional framing reduces human fallacy detection
in F1 by 14.5% on average. Humans perform better in fallacy detection when
perceiving enjoyment than fear or sadness, and these three emotions also
correlate with significantly higher convincingness compared to neutral or other
emotion states. Our work has implications for AI-driven emotional manipulation
in the context of fallacious argumentation.

</details>


### [296] [The Idola Tribus of AI: Large Language Models tend to perceive order where none exists](https://arxiv.org/abs/2510.09709)
*Shin-nosuke Ishikawa,Masato Todo,Taiki Ogihara,Hirotsugu Ohba*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在识别数字序列规律的简单任务中，倾向于生成荒谬的模式，即使在随机序列中也会过度识别不存在的规律，这揭示了其逻辑推理能力的局限性。


<details>
  <summary>Details</summary>
Motivation: LLMs在复杂实际任务中的应用（如RAG、AI Agent）依赖于其逻辑一致性和自我连贯性。因此，评估这些方面并考虑潜在的对策至关重要，尤其是在LLMs可能未能保持逻辑一致性的情况下。

Method: 进行了一项实验，要求LLMs解释各种整数序列（包括算术序列、几何序列和随机生成的整数序列）中的模式。

Result: LLMs成功识别了算术和几何序列中的正确模式，但在分析随机生成的序列时，它们频繁地过度识别与给定数字不一致的模式。即使是多步推理模型（如OpenAI o3, o4-mini, Google Gemini 2.5 Flash Preview Thinking）也观察到了这个问题。

Conclusion: LLMs存在一种感知不存在模式的倾向（类似于“部落的假象”），这突出表明了它们在需要逻辑推理的应用任务中的潜在局限性，即使采用了思维链推理机制也无法避免。

Abstract: We present a tendency of large language models (LLMs) to generate absurd
patterns despite their clear inappropriateness in a simple task of identifying
regularities in number series. Several approaches have been proposed to apply
LLMs to complex real-world tasks, such as providing knowledge through
retrieval-augmented generation and executing multi-step tasks using AI agent
frameworks. However, these approaches rely on the logical consistency and
self-coherence of LLMs, making it crucial to evaluate these aspects and
consider potential countermeasures. To identify cases where LLMs fail to
maintain logical consistency, we conducted an experiment in which LLMs were
asked to explain the patterns in various integer sequences, ranging from
arithmetic sequences to randomly generated integer series. While the models
successfully identified correct patterns in arithmetic and geometric sequences,
they frequently over-recognized patterns that were inconsistent with the given
numbers when analyzing randomly generated series. This issue was observed even
in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini
2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can
be interpreted as the AI model equivalent of Idola Tribus and highlights
potential limitations in their capability for applied tasks requiring logical
reasoning, even when employing chain-of-thought reasoning mechanisms.

</details>


### [297] [SeCon-RAG: A Two-Stage Semantic Filtering and Conflict-Free Framework for Trustworthy RAG](https://arxiv.org/abs/2510.09710)
*Xiaonan Si,Meilin Zhu,Simeng Qin,Lijia Yu,Lijun Zhang,Shuaitong Liu,Xinfeng Li,Ranjie Duan,Yang Liu,Xiaojun Jia*

Main category: cs.CL

TL;DR: 本文提出了一种名为SeCon-RAG的两阶段语义过滤和无冲突框架，旨在提高检索增强生成（RAG）系统的可信度，通过有效保留有用知识并缓解冲突污染，以应对语料库中毒和污染攻击。


<details>
  <summary>Details</summary>
Motivation: RAG系统易受语料库中毒和污染攻击，损害输出完整性。现有防御方法通常采用激进过滤，导致不必要地丢失有价值信息并降低生成可靠性。

Method: 本文提出了一个两阶段的SeCon-RAG框架：
1.  **第一阶段：联合过滤**：使用实体-意图-关系提取器（EIRE）指导的语义和基于聚类的联合过滤。EIRE从用户查询和文档中提取实体、潜在目标和实体关系，评估它们的语义相关性，并选择性地将有价值的文档添加到干净的检索数据库中。
2.  **第二阶段：EIRE引导的冲突感知过滤**：分析查询、候选答案和检索到的知识之间的语义一致性，在最终答案生成前过滤掉可能误导模型的内部和外部矛盾。

Result: SeCon-RAG有效保留了有用知识，同时缓解了冲突污染，显著提高了生成鲁棒性和输出可信度。在各种大型语言模型和数据集上的广泛实验表明，SeCon-RAG明显优于最先进的防御方法。

Conclusion: 所提出的SeCon-RAG框架通过两阶段的语义过滤和冲突感知机制，成功解决了RAG系统面临的语料库中毒和污染问题，在不牺牲有价值信息的前提下，显著提升了系统的鲁棒性和输出可信度。

Abstract: Retrieval-augmented generation (RAG) systems enhance large language models
(LLMs) with external knowledge but are vulnerable to corpus poisoning and
contamination attacks, which can compromise output integrity. Existing defenses
often apply aggressive filtering, leading to unnecessary loss of valuable
information and reduced reliability in generation. To address this problem, we
propose a two-stage semantic filtering and conflict-free framework for
trustworthy RAG. In the first stage, we perform a joint filter with semantic
and cluster-based filtering which is guided by the Entity-intent-relation
extractor (EIRE). EIRE extracts entities, latent objectives, and entity
relations from both the user query and filtered documents, scores their
semantic relevance, and selectively adds valuable documents into the clean
retrieval database. In the second stage, we proposed an EIRE-guided
conflict-aware filtering module, which analyzes semantic consistency between
the query, candidate answers, and retrieved knowledge before final answer
generation, filtering out internal and external contradictions that could
mislead the model. Through this two-stage process, SeCon-RAG effectively
preserves useful knowledge while mitigating conflict contamination, achieving
significant improvements in both generation robustness and output
trustworthiness. Extensive experiments across various LLMs and datasets
demonstrate that the proposed SeCon-RAG markedly outperforms state-of-the-art
defense methods.

</details>


### [298] [ReaLM: Residual Quantization Bridging Knowledge Graph Embeddings and Large Language Models](https://arxiv.org/abs/2510.09711)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Lingbing Guo,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: ReaLM是一个新颖的框架，通过残差向量量化将知识图谱（KG）嵌入与大型语言模型（LLM）的离散token空间对齐，并结合本体论引导的类约束，显著提升了知识图谱补全（KGC）的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的KGC方法难以充分利用结构化语义表示，因为预训练KG模型的连续嵌入空间与LLM的离散token空间存在根本性错位，这阻碍了有效的语义迁移并限制了性能。

Method: ReaLM通过残差向量量化机制，将预训练的KG嵌入离散化为紧凑的代码序列，并将其作为可学习的token整合到LLM词汇表中，实现符号知识与上下文知识的无缝融合。此外，它还引入了本体论引导的类约束，以强制执行语义一致性，根据类级别兼容性优化实体预测。

Result: 在两个广泛使用的基准数据集上进行的广泛实验表明，ReaLM取得了最先进的性能。

Conclusion: ReaLM有效地将结构化知识与大规模语言模型对齐，证实了其在知识图谱补全任务中的有效性。

Abstract: Large Language Models (LLMs) have recently emerged as a powerful paradigm for
Knowledge Graph Completion (KGC), offering strong reasoning and generalization
capabilities beyond traditional embedding-based approaches. However, existing
LLM-based methods often struggle to fully exploit structured semantic
representations, as the continuous embedding space of pretrained KG models is
fundamentally misaligned with the discrete token space of LLMs. This
discrepancy hinders effective semantic transfer and limits their performance.
To address this challenge, we propose ReaLM, a novel and effective framework
that bridges the gap between KG embeddings and LLM tokenization through the
mechanism of residual vector quantization. ReaLM discretizes pretrained KG
embeddings into compact code sequences and integrates them as learnable tokens
within the LLM vocabulary, enabling seamless fusion of symbolic and contextual
knowledge. Furthermore, we incorporate ontology-guided class constraints to
enforce semantic consistency, refining entity predictions based on class-level
compatibility. Extensive experiments on two widely used benchmark datasets
demonstrate that ReaLM achieves state-of-the-art performance, confirming its
effectiveness in aligning structured knowledge with large-scale language
models.

</details>


### [299] [All Code, No Thought: Current Language Models Struggle to Reason in Ciphered Language](https://arxiv.org/abs/2510.09714)
*Shiyuan Guo,Henry Sleight,Fabien Roger*

Main category: cs.CL

TL;DR: 本研究探讨了大型语言模型能否进行加密推理，以规避思维链（CoT）监控。结果显示，模型在加密文本中推理时准确率显著下降，尤其对于不常见的密码，尽管它们能准确翻译加密文本。这种能力与预训练数据中密码的流行度相关，并通过微调数据缓慢提升，表明当前模型规避CoT监控的加密推理策略可能无效。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理的普及，检测有害的AI行为变得至关重要。思维链（CoT）监控是检测对抗性攻击和AI失调的常用方法。然而，攻击者或失调模型可能通过加密推理（即隐藏在加密、翻译或压缩文本中的推理）来规避CoT监控。本研究旨在评估这种风险，即模型是否能够进行加密推理。

Method: 研究人员针对28种不同的密码，对多达10个模型进行微调和提示，使其用特定密码进行推理。他们通过模型在数学问题上的准确性来衡量推理能力。同时，他们也评估了模型对加密文本的理解能力（通过准确翻译成英文）。

Result: 研究发现了一个不对称现象：模型在加密文本中推理时，即使能准确翻译加密文本，其准确率也会显著下降。即使是前沿模型，在处理不常见的密码时也会遇到困难，尽管它们能用Rot13等常见密码进行准确推理。研究还表明，加密推理能力与预训练数据中密码的流行度相关。此外，缩放定律显示，加密推理能力随着额外的微调数据而缓慢提升。

Conclusion: 本研究表明，对于当前模型而言，使用加密推理来规避思维链（CoT）监控可能是一种无效的策略。它也为未来前沿模型中限制这种能力的发展提供了指导。

Abstract: Detecting harmful AI actions is important as AI agents gain adoption.
Chain-of-thought (CoT) monitoring is one method widely used to detect
adversarial attacks and AI misalignment. However, attackers and misaligned
models might evade CoT monitoring through ciphered reasoning: reasoning hidden
in encrypted, translated, or compressed text. To assess this risk, we test
whether models can perform ciphered reasoning. For each of 28 different
ciphers, we fine-tune and prompt up to 10 models to reason in that cipher. We
measure model accuracy on math problems as a proxy for reasoning ability.
Across the models we test, we find an asymmetry: model accuracy can drop
significantly when reasoning in ciphered text, even though models demonstrate
comprehension of ciphered text by being able to translate it accurately to
English. Even frontier models struggle with lesser-known ciphers, although they
can reason accurately in well-known ciphers like rot13. We show that ciphered
reasoning capability correlates with cipher prevalence in pretraining data. We
also identify scaling laws showing that ciphered reasoning capability improves
slowly with additional fine-tuning data. Our work suggests that evading CoT
monitoring using ciphered reasoning may be an ineffective tactic for current
models and offers guidance on constraining the development of this capability
in future frontier models.

</details>


### [300] [Preference-Aware Memory Update for Long-Term LLM Agents](https://arxiv.org/abs/2510.09720)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 该论文提出了一种名为PAMU的偏好感知记忆更新机制，通过结合滑动窗口平均和指数移动平均来动态优化LLM代理的长期偏好记忆，从而显著提高其在长期对话中的输出质量。


<details>
  <summary>Details</summary>
Motivation: LLM代理的推理能力受其利用长期记忆的能力影响。尽管现有方法在记忆存储和检索方面有所改进（例如，密集向量或知识图谱），但它们在记忆更新方面存在不足，特别是缺乏根据用户行为和上下文动态优化偏好记忆表示的机制。

Method: 该论文提出了偏好感知记忆更新机制（PAMU）。PAMU通过整合滑动窗口平均（SW）和指数移动平均（EMA），构建了一个融合的偏好感知表示，能够同时捕捉短期波动和长期用户倾向，从而实现动态和个性化的记忆优化。

Result: 在LoCoMo数据集的五个任务场景中进行的实验表明，PAMU机制能够显著提高LLM在五个基线中的输出质量。

Conclusion: PAMU机制通过动态优化偏好记忆表示，有效提升了LLM在长期对话中的表现，验证了其在改善LLM代理长期记忆能力方面的有效性。

Abstract: One of the key factors influencing the reasoning capabilities of LLM-based
agents is their ability to leverage long-term memory. Integrating long-term
memory mechanisms allows agents to make informed decisions grounded in
historical interactions. While recent advances have significantly improved the
storage and retrieval components, by encoding memory into dense vectors for
similarity search or organizing memory as structured knowledge graphs most
existing approaches fall short in memory updating. In particular, they lack
mechanisms for dynamically refining preference memory representations in
response to evolving user behaviors and contexts. To address this gap, we
propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic
and personalized memory refinement. By integrating sliding window averages (SW)
with exponential moving averages (EMA), PAMU constructs a fused
preference-aware representation that captures both short-term fluctuations and
long-term user tendencies. We conduct experiments on five task scenarios of the
LoCoMo dataset, and the results show that our mechanism can significantly
improve the output quality of LLM in five baselines, validating its
effectiveness in long-term conversations.

</details>


### [301] [Layout-Aware Parsing Meets Efficient LLMs: A Unified, Scalable Framework for Resume Information Extraction and Evaluation](https://arxiv.org/abs/2510.09722)
*Fanwei Zhu,Jinke Yu,Zulong Chen,Ying Zhou,Junhao Ji,Zhibo Yang,Yuxue Zhang,Haoyuan Hu,Zhenghao Liu*

Main category: cs.CL

TL;DR: 该论文提出一个布局感知且效率优化的框架，用于自动化简历信息提取和评估，解决了简历多样性、大型语言模型（LLM）成本/延迟高以及缺乏标准化数据集/评估工具的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化简历信息提取在实际部署中面临三大挑战：简历布局和内容的高度异构性、大型语言模型（LLM）的高成本和高延迟，以及标准化数据集和评估工具的缺乏。

Method: 该框架结合了：1. 一个经过微调的布局解析器以规范化多样文档格式；2. 一个基于并行提示和指令调优的推理高效LLM提取器；3. 一个由新基准数据集支持的鲁棒两阶段自动化评估框架。

Result: 实验表明，该框架在准确性和效率方面均显著优于现有基线。特别是，一个经过微调的紧凑型0.6B LLM在实现顶级准确性的同时，显著降低了推理延迟和计算成本。该系统已在阿里巴巴智能HR平台全面部署，支持其业务单元的实时应用。

Conclusion: 该研究成功构建了一个高效且准确的自动化简历信息提取和评估框架，有效解决了实际部署中的核心挑战，并通过小型化LLM实现了性能与效率的平衡，并在大型企业平台中得到了实际应用。

Abstract: Automated resume information extraction is critical for scaling talent
acquisition, yet its real-world deployment faces three major challenges: the
extreme heterogeneity of resume layouts and content, the high cost and latency
of large language models (LLMs), and the lack of standardized datasets and
evaluation tools. In this work, we present a layout-aware and
efficiency-optimized framework for automated extraction and evaluation that
addresses all three challenges. Our system combines a fine-tuned layout parser
to normalize diverse document formats, an inference-efficient LLM extractor
based on parallel prompting and instruction tuning, and a robust two-stage
automated evaluation framework supported by new benchmark datasets. Extensive
experiments show that our framework significantly outperforms strong baselines
in both accuracy and efficiency. In particular, we demonstrate that a
fine-tuned compact 0.6B LLM achieves top-tier accuracy while significantly
reducing inference latency and computational cost. The system is fully deployed
in Alibaba's intelligent HR platform, supporting real-time applications across
its business units.

</details>


### [302] [VisRAG 2.0: Evidence-Guided Multi-Image Reasoning in Visual Retrieval-Augmented Generation](https://arxiv.org/abs/2510.09733)
*Yubo Sun,Chunyi Peng,Yukun Yan,Shi Yu,Zhenghao Liu,Chi Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出了EVisRAG，一个端到端的视觉检索增强生成（VRAG）框架，通过证据引导的多图像推理来解决现有VRAG系统在整合多图像证据方面的不足。EVisRAG结合了奖励范围组相对策略优化（RS-GRPO）进行训练，在视觉问答任务上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉检索增强生成（VRAG）系统在可靠地感知和整合多图像证据方面表现不佳，这导致了薄弱的推理基础和错误的结论（幻觉），影响了视觉语言模型（VLMs）的推理能力。

Method: 本文提出了EVisRAG框架，它首先观察检索到的图像并记录每张图像的证据，然后从聚合的证据中得出最终答案。为了有效训练EVisRAG，引入了奖励范围组相对策略优化（RS-GRPO），该方法将细粒度奖励绑定到特定范围的token上，以共同优化VLMs的视觉感知和推理能力。

Result: EVisRAG在多个视觉问答基准测试上，与基础VLM相比，平均实现了27%的显著端到端性能提升。进一步分析表明，在RS-GRPO的驱动下，EVisRAG通过精确感知和定位多图像中与问题相关的证据，并从中得出最终答案，从而提高了答案的准确性。

Conclusion: EVisRAG框架通过学习证据引导的多图像推理，并结合RS-GRPO进行有效训练，显著提高了视觉检索增强生成系统在整合多图像证据和进行准确推理方面的能力，有效解决了现有VRAG系统的局限性。

Abstract: Visual retrieval-augmented generation (VRAG) augments vision-language models
(VLMs) with external visual knowledge to ground reasoning and reduce
hallucinations. Yet current VRAG systems often fail to reliably perceive and
integrate evidence across multiple images, leading to weak grounding and
erroneous conclusions. In this paper, we propose EVisRAG, an end-to-end
framework that learns to reason with evidence-guided multi-image to address
this issue. The model first observes retrieved images and records per-image
evidence, then derives the final answer from the aggregated evidence. To train
EVisRAG effectively, we introduce Reward-Scoped Group Relative Policy
Optimization (RS-GRPO), which binds fine-grained rewards to scope-specific
tokens to jointly optimize visual perception and reasoning abilities of VLMs.
Experimental results on multiple visual question answering benchmarks
demonstrate that EVisRAG delivers substantial end-to-end gains over backbone
VLM with 27\% improvements on average. Further analysis shows that, powered by
RS-GRPO, EVisRAG improves answer accuracy by precisely perceiving and
localizing question-relevant evidence across multiple images and deriving the
final answer from that evidence, much like a real detective.

</details>


### [303] [Judge's Verdict: A Comprehensive Analysis of LLM Judge Capability Through Human Agreement](https://arxiv.org/abs/2510.09738)
*Steve Han,Gilberto Titericz Junior,Tom Balough,Wenfei Zhou*

Main category: cs.CL

TL;DR: 本研究引入了“法官判决基准”，这是一种两步法，用于评估大型语言模型（LLMs）作为响应准确性评估任务的法官，超越了简单的相关性分析，并根据人类判断的模仿程度对LLM法官进行分类。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估LLMs在RAG或Agentic管道中作为响应准确性评估法官的能力，并指出仅使用传统相关性分析不足以全面衡量LLM法官的性能，需要更深入地理解它们与人类判断的一致性模式。

Method: 本研究采用了“法官判决基准”这一两步法：(1) 首先进行相关性测试，筛选出与人类判断高度一致的法官；(2) 接着进行人类相似度测试，使用z-分数区分两种判断模式：模仿人类自然变异的“人类相似判断”（|z| < 1）和超出典型人际一致性水平的“超一致判断”（z > 1）。该方法还结合了科恩卡帕系数分析来衡量实际一致性模式。

Result: 在测试的54个LLM中，有27个达到了Tier 1性能：其中23个模型表现出保留人类判断细微差别的人类相似模式，而4个模型表现出超一致行为。研究还发现，法官的卓越表现不完全取决于模型大小，而是取决于特定的训练策略。

Conclusion: 主要结论包括：(1) 仅靠相关性不足以评估法官；(2) 引入了一种基于一致性模式的“法官图灵测试”；(3) 提供了一个标准化基准，用于根据不同的评估需求将LLM法官分为不同的性能等级。

Abstract: This research introduces the Judge's Verdict Benchmark, a novel two-step
methodology to evaluate Large Language Models (LLMs) as judges for response
accuracy evaluation tasks. We assess how well 54 LLMs can replicate human
judgment when scoring responses from RAG (Retrieval-Augmented Generation) or
Agentic pipelines against ground truth answers. Our methodology progresses from
traditional correlation analysis to comprehensive Cohen's Kappa analysis that
measures actual agreement patterns. The two-step approach includes: (1) a
correlation test that filters judges with strong alignment, followed by (2) a
human-likeness test using z-scores to identify two distinct judgment patterns:
human-like judgment (|z| < 1) that mimics natural human variation, and
super-consistent judgment (z > 1) that exceeds typical human-to-human agreement
levels. This methodology reveals that 27 out of 54 tested LLMs achieve Tier 1
performance: 23 models exhibit human-like patterns that preserve the nuances of
human judgment, while 4 models demonstrate super-consistent behavior, a pattern
that could indicate either enhanced reliability or oversimplification of
complex judgments. Testing 43 open-source models (1B-405B parameters) and 11
closed models (GPT, Gemini, Claude variants), we demonstrate that judge
excellence is not solely dependent on model size but on specific training
strategies. Our key contributions include: (1) establishing that correlation
alone is insufficient for judge evaluation, (2) introducing a "Turing Test for
judges" based on agreement patterns, and (3) providing a standardized benchmark
for classifying LLM judges into distinct performance tiers for different
evaluation needs.

</details>


### [304] [Gold Panning: Turning Positional Bias into Signal for Multi-Document LLM Reasoning](https://arxiv.org/abs/2510.09770)
*Adam Byerly,Daniel Khashabi*

Main category: cs.CL

TL;DR: 解析错误


<details>
  <summary>Details</summary>
Motivation: 解析错误

Method: 解析错误

Result: 解析错误

Conclusion: 解析错误

Abstract: Large language models exhibit a strong position bias in multi-document
contexts, systematically prioritizing information based on location rather than
relevance. While existing approaches treat this bias as noise to be mitigated,
we introduce Gold Panning Bandits, a framework that leverages position bias as
a diagnostic signal: by reordering documents and observing shifts in the
model's responses, we can efficiently identify the most relevant content. We
frame the problem of choosing reorderings as a bipartite matching problem.
While an optimal assignment can be computed at each iteration with the
Hungarian algorithm in $O(N^3)$ time, we propose a greedy $O(N \log N)$
strategy that achieves comparable performance by prioritizing the placement of
the most uncertain documents in the most informative positions. Our approach
identifies relevant documents using up to 65\% fewer language model queries
than random permutation baselines on knowledge-intensive NLP tasks,
substantially reducing computational cost without model retraining. This work
demonstrates that inherent LLM biases can be transformed from liabilities into
assets for efficient, inference-time optimization.

</details>


### [305] [PromptGuard at BLP-2025 Task 1: A Few-Shot Classification Framework Using Majority Voting and Keyword Similarity for Bengali Hate Speech Detection](https://arxiv.org/abs/2510.09771)
*Rakib Hossan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 本文提出PromptGuard，一个针对孟加拉语仇恨言论分类的少样本框架，结合卡方关键词提取和自适应多数投票，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: BLP-2025任务1A要求孟加拉语仇恨言论分类，但传统监督方法需要大量标注数据，这对于低资源语言来说成本高昂。

Method: 开发了PromptGuard框架，该框架结合了：1) 卡方统计分析进行关键词提取；2) 自适应多数投票进行决策。它探索了统计关键词选择与随机方法，以及基于共识质量扩展分类的自适应投票机制。

Result: PromptGuard实现了67.61的微F1分数，优于n-gram基线（60.75）和随机方法（14.65）。卡方关键词在所有类别中提供了持续改进，自适应投票机制对模糊案例有益。消融研究证实了基于卡方的关键词在所有类别中具有最一致的影响。

Conclusion: PromptGuard框架通过结合卡方关键词提取和自适应多数投票，有效解决了低资源语言的少样本仇恨言论分类问题，并显著优于现有基线。

Abstract: The BLP-2025 Task 1A requires Bengali hate speech classification into six
categories. Traditional supervised approaches need extensive labeled datasets
that are expensive for low-resource languages. We developed PromptGuard, a
few-shot framework combining chi-square statistical analysis for keyword
extraction with adaptive majority voting for decision-making. We explore
statistical keyword selection versus random approaches and adaptive voting
mechanisms that extend classification based on consensus quality. Chi-square
keywords provide consistent improvements across categories, while adaptive
voting benefits ambiguous cases requiring extended classification rounds.
PromptGuard achieves a micro-F1 of 67.61, outperforming n-gram baselines
(60.75) and random approaches (14.65). Ablation studies confirm
chi-square-based keywords show the most consistent impact across all
categories.

</details>


### [306] [Steering Embedding Models with Geometric Rotation: Mapping Semantic Relationships Across Languages and Models](https://arxiv.org/abs/2510.09790)
*Michael Freenor,Lauren Alvarez*

Main category: cs.CL

TL;DR: 本研究提出了一种名为RISE的几何方法，将语义转换表示为嵌入空间中一致的旋转操作，并发现这种操作在不同语言和模型之间具有很高的可迁移性，揭示了多语言嵌入空间中话语级语义转换的几何结构。


<details>
  <summary>Details</summary>
Motivation: 早期的词嵌入模型展现出直观的向量算术特性，但现代高维文本表示缺乏直接可解释的几何属性。理解语言和嵌入模型如何编码语义关系对于模型的可解释性和控制至关重要。

Method: 研究引入了Rotor-Invariant Shift Estimation (RISE)，一种几何方法，利用现代语言表示的流形结构，将语义转换表示为嵌入空间中一致的旋转操作。RISE在三种嵌入模型、三个数据集和七种形态多样的语言（属于五个主要语系）上进行了评估。

Result: RISE操作在跨语言和跨模型方面表现出高迁移性，表明存在类似的跨语言几何结构。结果显示，RISE能够一致地映射具有不同语法特征（如否定和条件性）的话语级语义转换，且在不同语言和模型之间保持一致性。

Conclusion: 这项工作首次系统地证明了话语级语义转换在多语言嵌入空间中对应于一致的几何操作，从经验上支持了句子层面的线性表示假设。

Abstract: Understanding how language and embedding models encode semantic relationships
is fundamental to model interpretability and control. While early word
embeddings exhibited intuitive vector arithmetic (''king'' - ''man'' +
''woman'' = ''queen''), modern high-dimensional text representations lack
straightforward interpretable geometric properties. We introduce
Rotor-Invariant Shift Estimation (RISE), a geometric approach that represents
semantic transformations as consistent rotational operations in embedding
space, leveraging the manifold structure of modern language representations.
RISE operations have the ability to operate across both languages and models
with high transfer of performance, suggesting the existence of analogous
cross-lingual geometric structure. We evaluate RISE across three embedding
models, three datasets, and seven morphologically diverse languages in five
major language groups. Our results demonstrate that RISE consistently maps
discourse-level semantic transformations with distinct grammatical features
(e.g., negation and conditionality) across languages and models. This work
provides the first systematic demonstration that discourse-level semantic
transformations correspond to consistent geometric operations in multilingual
embedding spaces, empirically supporting the Linear Representation Hypothesis
at the sentence level.

</details>


### [307] [Text Prompt Injection of Vision Language Models](https://arxiv.org/abs/2510.09849)
*Ruizhe Zhu*

Main category: cs.CL

TL;DR: 该项目研究了文本提示注入攻击，这是一种简单而有效的误导大型视觉语言模型的方法。他们开发了一种算法，并通过实验证明其有效性、效率和低计算资源需求。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型的广泛应用引发了严重的安全担忧。

Method: 研究了文本提示注入（text prompt injection）这种攻击方式，并开发了一种针对此类攻击的算法。

Result: 通过实验证明了所开发攻击算法的有效性和效率。与其他攻击方法相比，该方法对大型模型尤其有效，且对计算资源的需求不高。

Conclusion: 文本提示注入是一种简单、有效且高效的攻击大型视觉语言模型的方法，尤其适用于资源受限的场景。

Abstract: The widespread application of large vision language models has significantly
raised safety concerns. In this project, we investigate text prompt injection,
a simple yet effective method to mislead these models. We developed an
algorithm for this type of attack and demonstrated its effectiveness and
efficiency through experiments. Compared to other attack methods, our approach
is particularly effective for large models without high demand for
computational resources.

</details>


### [308] [NG-Router: Graph-Supervised Multi-Agent Collaboration for Nutrition Question Answering](https://arxiv.org/abs/2510.09854)
*Kaiwen Shi,Zheyuan Zhang,Zhengqing Yuan,Keerthiram Murugesan,Vincent Galass,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: 本文提出NG-Router框架，通过将营养问答建模为知识图谱引导的多智能体协作问题，利用图神经网络进行任务感知路由和基于梯度的子图检索，以克服现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有营养问答系统面临两大挑战：单智能体系统推理能力有限以及多智能体架构设计复杂；同时，上下文过载会阻碍决策准确性。个性化饮食指导和慢性病预防对营养问答有迫切需求。

Method: NG-Router将营养问答公式化为监督式、知识图谱引导的多智能体协作问题。它将智能体节点整合到异构知识图谱中，并采用图神经网络学习任务感知的智能体路由分布，利用经验性智能体表现的软监督。为解决上下文过载，提出了一种基于梯度的子图检索机制，在训练期间识别显著证据。

Result: 在多个基准和骨干模型上的大量实验表明，NG-Router始终优于单智能体和集成基线模型。

Conclusion: NG-Router为复杂的营养健康任务提供了一种领域感知多智能体推理的原则性方法，有效解决了现有方法的推理能力和上下文过载问题。

Abstract: Diet plays a central role in human health, and Nutrition Question Answering
(QA) offers a promising path toward personalized dietary guidance and the
prevention of diet-related chronic diseases. However, existing methods face two
fundamental challenges: the limited reasoning capacity of single-agent systems
and the complexity of designing effective multi-agent architectures, as well as
contextual overload that hinders accurate decision-making. We introduce
Nutritional-Graph Router (NG-Router), a novel framework that formulates
nutritional QA as a supervised, knowledge-graph-guided multi-agent
collaboration problem. NG-Router integrates agent nodes into heterogeneous
knowledge graphs and employs a graph neural network to learn task-aware routing
distributions over agents, leveraging soft supervision derived from empirical
agent performance. To further address contextual overload, we propose a
gradient-based subgraph retrieval mechanism that identifies salient evidence
during training, thereby enhancing multi-hop and relational reasoning.
Extensive experiments across multiple benchmarks and backbone models
demonstrate that NG-Router consistently outperforms both single-agent and
ensemble baselines, offering a principled approach to domain-aware multi-agent
reasoning for complex nutritional health tasks.

</details>


### [309] [NarraBench: A Comprehensive Framework for Narrative Benchmarking](https://arxiv.org/abs/2510.09869)
*Sil Hamilton,Matthew Wilkens,Andrew Piper*

Main category: cs.CL

TL;DR: NarraBench是一个叙事理解任务的理论分类法，并对78个现有基准进行了调查，发现当前评估在叙事事件、风格、视角和揭示等多个方面存在显著空白，且缺乏对主观和视角性叙事方面的评估。


<details>
  <summary>Details</summary>
Motivation: 现有基准未能充分覆盖叙事理解的各个方面，许多重要方面被忽视或与现有指标不匹配，导致LLM的叙事理解能力评估不足。

Method: 本文提出了一个理论指导的叙事理解任务分类法（NarraBench），并基于此对78个现有基准进行了系统性调查。

Result: 研究发现，只有约27%的叙事任务能被现有基准很好地捕捉；叙事事件、风格、视角和揭示等领域几乎没有得到现有评估的覆盖。此外，还指出需要开发能评估叙事中固有的主观和视角性方面的基准。

Conclusion: NarraBench分类法、调查和方法对寻求测试大型语言模型（LLM）叙事理解能力的自然语言处理研究人员具有重要价值，并揭示了未来基准开发的关键方向。

Abstract: We present NarraBench, a theory-informed taxonomy of narrative-understanding
tasks, as well as an associated survey of 78 existing benchmarks in the area.
We find significant need for new evaluations covering aspects of narrative
understanding that are either overlooked in current work or are poorly aligned
with existing metrics. Specifically, we estimate that only 27% of narrative
tasks are well captured by existing benchmarks, and we note that some areas --
including narrative events, style, perspective, and revelation -- are nearly
absent from current evaluations. We also note the need for increased
development of benchmarks capable of assessing constitutively subjective and
perspectival aspects of narrative, that is, aspects for which there is
generally no single correct answer. Our taxonomy, survey, and methodology are
of value to NLP researchers seeking to test LLM narrative understanding.

</details>


### [310] [CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs](https://arxiv.org/abs/2510.09871)
*Nafiseh Nikeghbal,Amir Hossein Kargaran,Jana Diesner*

Main category: cs.CL

TL;DR: 尽管大型语言模型（LLMs）通过了标准安全检查，但在对话中仍可能表现出有害偏见。本文引入CoBia，一种轻量级对抗性攻击，通过构建对话来系统地揭示并评估LLMs从虚假偏见中恢复并拒绝偏见后续问题的能力。结果显示，LLMs常未能拒绝偏见后续问题，揭示了其深层偏见。


<details>
  <summary>Details</summary>
Motivation: LLMs在通过标准安全检查后，仍可能在对话中偶然表现出有害行为，例如表达种族主义观点。研究旨在系统地分析LLMs在何种条件下偏离规范或道德行为。

Method: 引入CoBia，一套轻量级对抗性攻击，通过构建对话，让模型发表关于某个社会群体的偏见言论。然后评估模型是否能从这种虚构的偏见言论中恢复，并拒绝带有偏见的后续问题。评估了11个开源和专有LLMs，涉及性别、种族、宗教、国籍、性取向等六个社会人口类别。评估基于既定的LLM偏见指标，并与人类判断进行比较。

Result: 精心构建的对话能可靠地揭示偏见放大现象。LLMs在对话中常常未能拒绝带有偏见的后续问题。这种压力测试揭示了通过互动可以暴露出来的深层嵌入的偏见。

Conclusion: CoBia这种对话压力测试方法能有效揭示LLMs中深层嵌入的偏见，即使模型通过了标准安全检查。LLMs在对话中难以从虚构的偏见中恢复并拒绝偏见后续问题，表明其偏见问题仍然存在且不易被常规方法发现。

Abstract: Improvements in model construction, including fortified safety guardrails,
allow Large language models (LLMs) to increasingly pass standard safety checks.
However, LLMs sometimes slip into revealing harmful behavior, such as
expressing racist viewpoints, during conversations. To analyze this
systematically, we introduce CoBia, a suite of lightweight adversarial attacks
that allow us to refine the scope of conditions under which LLMs depart from
normative or ethical behavior in conversations. CoBia creates a constructed
conversation where the model utters a biased claim about a social group. We
then evaluate whether the model can recover from the fabricated bias claim and
reject biased follow-up questions. We evaluate 11 open-source as well as
proprietary LLMs for their outputs related to six socio-demographic categories
that are relevant to individual safety and fair treatment, i.e., gender, race,
religion, nationality, sex orientation, and others. Our evaluation is based on
established LLM-based bias metrics, and we compare the results against human
judgments to scope out the LLMs' reliability and alignment. The results suggest
that purposefully constructed conversations reliably reveal bias amplification
and that LLMs often fail to reject biased follow-up questions during dialogue.
This form of stress-testing highlights deeply embedded biases that can be
surfaced through interaction. Code and artifacts are available at
https://github.com/nafisenik/CoBia.

</details>


### [311] [iBERT: Interpretable Style Embeddings via Sense Decomposition](https://arxiv.org/abs/2510.09882)
*Vishal Anand,Milad Alshomary,Kathleen McKeown*

Main category: cs.CL

TL;DR: iBERT是一种可解释且可控的BERT编码器，通过将每个token表示为稀疏、非负的上下文无关语义向量混合，实现了对语言中判别性线索（如风格和语义）的模块化分解和暴露。它在风格任务上表现出色，并提高了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有BERT嵌入缺乏固有的可解释性和可控性，难以理解和操纵语言中包含的判别性线索（如文体和语义结构）。

Method: iBERT采用编码器将每个输入token表示为k个上下文无关语义向量的稀疏、非负混合。这些向量可以聚合为句子嵌入或直接用于token级别，从而在解码或下游应用之前实现对表示的模块化控制。

Result: 在STEL基准测试中，iBERT将风格表示的有效性比SBERT基线提高了约8个百分点，同时在作者验证任务上保持了竞争力。其嵌入结构允许将特定的风格属性（如表情符号使用、正式性或拼写错误）分配给特定的语义向量。该模型的设计也适用于风格和语义因素混合的场景。

Conclusion: iBERT成功地提供了一种固有的可解释和可控的嵌入方法，通过将语言分解为模块化的语义向量，有效揭示了判别性线索。它在风格相关任务中表现出强大的性能和可解释性，并具有泛化到其他判别信号的潜力。

Abstract: We present iBERT (interpretable-BERT), an encoder to produce inherently
interpretable and controllable embeddings - designed to modularize and expose
the discriminative cues present in language, such as stylistic and semantic
structure. Each input token is represented as a sparse, non-negative mixture
over k context-independent sense vectors, which can be pooled into sentence
embeddings or used directly at the token level. This enables modular control
over representation, before any decoding or downstream use.
  To demonstrate our model's interpretability, we evaluate it on a suite of
style-focused tasks. On the STEL benchmark, it improves style representation
effectiveness by ~8 points over SBERT-style baselines, while maintaining
competitive performance on authorship verification. Because each embedding is a
structured composition of interpretable senses, we highlight how specific style
attributes - such as emoji use, formality, or misspelling can be assigned to
specific sense vectors. While our experiments center on style, iBERT is not
limited to stylistic modeling. Its structural modularity is designed to
interpretably decompose whichever discriminative signals are present in the
data - enabling generalization even when supervision blends stylistic and
semantic factors.

</details>


### [312] [DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context Reasoning](https://arxiv.org/abs/2510.09883)
*Hossein Entezari Zarch,Lei Gao,Chaoyi Jiang,Murali Annavarm*

Main category: cs.CL

TL;DR: DELTA是一种无训练的稀疏注意力机制，通过分层注意力策略（初始全注意力、选择层识别关键token、后续稀疏注意力层）在推理任务中显著提升计算效率，同时保持或超越全注意力模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在生成长推理链时，解码成本高昂，因为每个新token都需关注整个不断增长的序列。现有的稀疏注意力方法通过修剪KV缓存来降低计算量，但在推理任务中由于累积选择错误和token动态重要性，导致准确性严重下降。

Method: DELTA是一种无训练的稀疏注意力机制。它将Transformer层分为三组：初始层使用全注意力；一小部分“选择层”通过聚合的头级注意力分数识别关键token；随后的“稀疏注意力层”仅关注选定的token子集。这种设计在GPU内存中保留完整的KV缓存以确保准确性，同时避免了在许多层中进行昂贵的完整注意力计算。

Result: 在AIME和GPQA-Diamond等推理基准测试中，DELTA的准确性与全注意力模型持平或更高，同时将关注的token数量减少了多达5倍，并实现了1.5倍的端到端加速。

Conclusion: 研究结果表明，选择性重用中间注意力图为高效的长上下文推理提供了一条可靠的途径。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance on
challenging benchmarks by generating long chains of intermediate steps, but
their inference cost is dominated by decoding, where each new token must attend
to the entire growing sequence. Existing sparse attention methods reduce
computation by pruning the key-value (KV) cache, yet they suffer from severe
accuracy degradation on reasoning tasks due to cumulative selection errors and
the dynamic importance of tokens over long derivations. We present
\textbf{DELTA}, a training-free sparse attention mechanism that achieves
computational efficiency without sacrificing model accuracy. DELTA partitions
transformer layers into three groups: initial layers that use full attention, a
small set of \emph{selection layers} that identify salient tokens via
aggregated head-level attention scores, and subsequent \emph{sparse-attention
layers} that attend only to the selected subset. This design preserves the full
KV cache in GPU memory for accuracy, while avoiding expensive full-attention
computation over many layers. On reasoning benchmarks such as AIME and
GPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while
reducing the number of attended tokens by up to $5\times$ and delivering
$1.5\times$ end-to-end speedup. Our results show that selective reuse of
intermediate attention maps offers a robust path toward efficient long-context
reasoning.

</details>


### [313] [Closing the Data-Efficiency Gap Between Autoregressive and Masked Diffusion LLMs](https://arxiv.org/abs/2510.09885)
*Xu Pan,Ely Hahami,Jingxuan Fan,Ziqian Xie,Haim Sompolinsky*

Main category: cs.CL

TL;DR: 本文比较了自回归大语言模型 (arLLMs) 和掩码扩散大语言模型 (dLLMs) 在微调阶段的知识注入能力，特别关注“逆转诅咒”和数据效率。研究发现 dLLMs 表现更优，并提出了一种新的掩码微调范式，显著提升了 arLLMs 的数据效率。


<details>
  <summary>Details</summary>
Motivation: 自回归大语言模型 (arLLMs) 在知识注入（如微调）时存在固有缺陷，例如“逆转诅咒”。掩码扩散大语言模型 (dLLMs) 作为替代方案，在预训练阶段表现出更好的数据效率且没有“逆转诅咒”。然而，这些优势是否能延伸到后训练（微调）阶段尚不清楚。

Method: 研究人员在三个不同的数据集上对 arLLMs 和 dLLMs 进行微调，并使用正向和反向问答 (QA) 评估其知识泛化能力和“逆转诅咒”现象。此外，受 dLLMs 性能启发，提出了一种新的掩码微调范式用于预训练 arLLMs 的知识注入。

Result: 结果显示，arLLMs 严重依赖大量的释义数据增强才能实现 QA 泛化，且释义的信息顺序必须与 QA 风格匹配。相反，dLLMs 在没有释义的情况下也能在正向和反向 QA 上取得高准确率，添加释义仅带来微小收益。最后，提出的新型掩码微调方法显著提高了 arLLMs 微调的数据效率，有效缩小了与 dLLMs 的性能差距。

Conclusion: dLLMs 在微调阶段的知识注入方面表现出优越性，尤其在数据效率和规避“逆转诅咒”方面。通过引入一种新颖的掩码微调范式，可以显著提升 arLLMs 的数据效率，使其性能与 dLLMs 相当。

Abstract: Despite autoregressive large language models (arLLMs) being the current
dominant paradigm in language modeling, they resist knowledge injection via
fine-tuning due to inherent shortcomings such as the "reversal curse" -- the
challenge of answering questions that reverse the original information order in
the training sample. Masked diffusion large language models (dLLMs) are rapidly
emerging as a powerful alternative to the arLLM paradigm, with evidence of
better data efficiency and free of the "reversal curse" in pre-training.
However, it is unknown whether these advantages extend to the post-training
phase, i.e. whether pre-trained dLLMs can easily acquire new knowledge through
fine-tuning. On three diverse datasets, we fine-tune arLLMs and dLLMs,
evaluating them with forward and backward style Question Answering (QA) to
probe knowledge generalization and the reversal curse. Our results confirm that
arLLMs critically rely on extensive data augmentation via paraphrases for QA
generalization, and paraphrases are only effective when their information order
matches the QA style. Conversely, dLLMs achieve high accuracies on both forward
and backward QAs without paraphrases; adding paraphrases yields only marginal
gains. Lastly, inspired by the dLLM's performance, we introduce a novel masked
fine-tuning paradigm for knowledge injection into pre-trained arLLMs. This
proposed method successfully and drastically improves the data efficiency of
arLLM fine-tuning, effectively closing the performance gap with dLLMs.

</details>


### [314] [Abductive Preference Learning](https://arxiv.org/abs/2510.09887)
*Yijin Ni,Peng Qi*

Main category: cs.CL

TL;DR: 大型语言模型即使经过对齐，仍存在过度自信问题，尤其是在面对反事实提示时。本文提出溯因偏好学习，通过在给定响应下学习提示偏好来提高提示辨别能力，并结合多任务目标显著提升了响应选择和提示辨别准确性。


<details>
  <summary>Details</summary>
Motivation: 前沿大型语言模型（如GPT-5和Claude Sonnet）在经过RLHF和DPO对齐后，仍然容易过度自信，例如对本应产生不同答案的提示给出相同的保守回答。研究认为这归因于现有偏好学习的局限性：它强调为给定提示选择正确响应，而忽略了应改变响应的反事实提示。

Method: 为解决上述局限性，本文提出了溯因偏好学习，这是一种逆转传统条件作用的微调范式，即在给定响应的情况下学习提示的偏好。为验证此想法，研究人员构建了一个包含1,001个条目的溯因数据集（源自HaluEval QA基准），并实现了溯因DPO及其变体DPOP。此外，还设计了一个多任务目标，以统一标准方法和溯因方法的优点。

Result: 实验表明，标准方法提高了响应选择能力，而溯因方法提高了提示辨别能力。多任务目标能够统一这两种优势。在溯因数据集上，多任务DPOP将响应选择准确率从90.0%提升至99.5%，将提示辨别准确率从54.7%提升至85.0%，并有定性证据表明模型对提示差异的敏感性有所提高。在AlpacaEval上的评估显示，多任务DPOP将胜率从5.26%提升至6.17%。

Conclusion: 溯因偏好学习保留了传统偏好优化的优势，同时解决了反事实提示这一被忽视的挑战。多任务DPOP通过有效提高提示辨别能力，显著改善了大型模型在处理反事实提示时的过度自信问题。

Abstract: Frontier large language models such as GPT-5 and Claude Sonnet remain prone
to overconfidence even after alignment through Reinforcement Learning with
Human Feedback (RLHF) and Direct Preference Optimization (DPO). For instance,
they tend to offer the same conservative answer "No" to both questions "Can I
eat the [food / potato chips] that has been left out overnight?" despite the
latter requiring no refridgeration for safe consumption. We find that this
failure is potentially attributed to a limitation of existing preference
learning: it emphasizes selecting the correct response for a given prompt,
while neglecting counterfactual prompts that should alter the response.
  To address this limitation, we propose abductive preference learning, a
fine-tuning paradigm that reverses the conventional conditioning by learning
preferences over prompts given a response. To validate this idea, we construct
an abductive dataset derived from the HaluEval QA benchmark with 1,001 entries,
implementing abductive DPO and its variant DPOP. Experiments reveal
complementary strengths: standard methods improve response selection, abductive
methods improve prompt discrimination, while a multitask objective unifies
both. On the abductive dataset, multitask DPOP boosts accuracy from $90.0\%$ to
$99.5\%$ in response selection and $54.7\%$ to $85.0\%$ in prompt
discrimination, with qualitative evidence highlighting improved sensitivity to
prompt differences. Finally, evaluation on AlpacaEval shows multitask DPOP
improves win rate (from $5.26\%$ to $6.17\%$), confirming that abductive
preference learning preserves the benefits of conventional preference
optimization while addressing the overlooked challenge of counterfactual
prompts.

</details>


### [315] [HIPPD: Brain-Inspired Hierarchical Information Processing for Personality Detection](https://arxiv.org/abs/2510.09893)
*Guanming Chen,Lingzhi Shen,Xiaohao Cai,Imran Razzak,Shoaib Jameel*

Main category: cs.CL

TL;DR: HIPPD是一个受大脑启发的个性检测框架，它利用大型语言模型、动态记忆模块和专门的轻量级模型，通过模拟大脑处理过程，在多个帖子中捕捉上下文信息并提取鲁棒特征，显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习方法在从文本中检测个性时，难以捕捉跨多个帖子的上下文信息，并且在语义稀疏环境中提取代表性、鲁棒的特征方面表现不佳。

Method: HIPPD框架模拟人脑的层次信息处理。它使用大型语言模型（模拟大脑皮层）进行全局语义推理和深度特征抽象；动态记忆模块（模拟前额叶皮层）通过多巴胺预测误差反馈进行自适应门控和关键特征选择性保留；一组专门的轻量级模型（模拟基底神经节）通过严格的“赢者通吃”机制动态路由，以捕捉它们最擅长识别的个性相关模式。

Result: 在Kaggle和Pandora数据集上进行的广泛实验表明，HIPPD始终优于最先进的基线方法。

Conclusion: HIPPD通过模拟人脑的层次信息处理，有效解决了现有方法在个性检测中遇到的上下文捕捉和特征提取挑战，并取得了卓越的性能。

Abstract: Personality detection from text aims to infer an individual's personality
traits based on linguistic patterns. However, existing machine learning
approaches often struggle to capture contextual information spanning multiple
posts and tend to fall short in extracting representative and robust features
in semantically sparse environments. This paper presents HIPPD, a
brain-inspired framework for personality detection that emulates the
hierarchical information processing of the human brain. HIPPD utilises a large
language model to simulate the cerebral cortex, enabling global semantic
reasoning and deep feature abstraction. A dynamic memory module, modelled after
the prefrontal cortex, performs adaptive gating and selective retention of
critical features, with all adjustments driven by dopaminergic prediction error
feedback. Subsequently, a set of specialised lightweight models, emulating the
basal ganglia, are dynamically routed via a strict winner-takes-all mechanism
to capture the personality-related patterns they are most proficient at
recognising. Extensive experiments on the Kaggle and Pandora datasets
demonstrate that HIPPD consistently outperforms state-of-the-art baselines.

</details>


### [316] [Don't Throw Away Your Pretrained Model](https://arxiv.org/abs/2510.09913)
*Shangbin Feng,Wenhao Yu,Yike Wang,Hongming Zhang,Yulia Tsvetkov,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出“切换生成”（Switch Generation）方法，通过模型协作结合对齐和未对齐语言模型的优点，以应对对齐训练带来的能力权衡问题。


<details>
  <summary>Details</summary>
Motivation: 语言模型对齐训练在提高推理和指令遵循能力的同时，可能牺牲创造性和校准等技能，而未对齐的基础模型在这些方面表现更好。研究旨在通过模型协作，结合两者的优势。

Method: 提出“切换生成”方法，其中预训练和对齐模型版本轮流生成响应序列的不同片段。通过训练一个“切换器语言模型”（switcher LM），该模型学习在不同查询和上下文中选择不同模型生成下一个片段的结果。在推理时，切换器LM动态指导不同模型检查点生成最需要其优势的片段。

Result: 在8种模型协作基线和18个数据集上进行实验，结果显示：1) 模型协作在18个任务中的16个上持续优于单个模型；2) 切换生成平均比基线高出12.9%。进一步分析表明，切换生成发现了组合技能来解决单个模型难以解决的问题，并能泛化到未见过的模型和任务，复用和重新利用了昂贵模型训练流程中原本会被丢弃的副产品。

Conclusion: 切换生成通过模型协作有效结合了不同语言模型版本的优势，克服了对齐训练的权衡问题，并在性能和泛化能力上表现出色，同时提高了训练资源的利用效率。

Abstract: Alignment training has tradeoffs: it helps language models (LMs) gain in
reasoning and instruction following but might lose out on skills such as
creativity and calibration, where unaligned base models are better at. We aim
to make the best of both worlds through model collaboration, where different
models in the training pipeline collaborate and complement each other. Since LM
responses feature interleaving skills that favor different models, we propose
Switch Generation, where pretrained and aligned model versions take turns to
``speak'' in a response sequence. Specifically, we train a switcher LM by
learning from outcomes of choosing different models to generate the next
segment across diverse queries and contexts. At inference time, the switcher LM
guides different model checkpoints to dynamically generate the next segment
where their strengths are most needed. Extensive experiments with 8 model
collaboration baselines and 18 datasets show that 1) model collaboration
consistently outperforms individual models on 16 out of 18 tasks, and 2) Switch
Generation further outperforms baselines by 12.9% on average. Further analysis
reveals that Switch Generation discovers compositional skills to solve problems
where individual models struggle and generalizes to unseen models and tasks,
reusing and repurposing by-products in expensive model training pipelines that
are otherwise discarded.

</details>


### [317] [Enhancing Faithfulness in Abstractive Summarization via Span-Level Fine-Tuning](https://arxiv.org/abs/2510.09915)
*Sicong Huang,Qianqi Yan,Shengze Wang,Ian Lane*

Main category: cs.CL

TL;DR: 本研究通过利用GPT-4o生成的幻觉跨度级标注，探索并评估了多种微调策略，以提高大型语言模型在抽象摘要生成中的忠实度，其中非可能性训练效果最佳。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）能生成流畅的抽象摘要，但它们常产生不忠实的摘要，引入词语、短语或概念层面的幻觉。现有缓解策略（如后处理或对比学习）未能完全解决LLM生成摘要中出现的各种错误。

Method: 首先，使用多种LLMs为训练集中的源文档自动生成摘要，然后利用GPT-4o对检测到的幻觉进行跨度级标注。接着，利用这些标注（包括无幻觉摘要和带标注的不忠实跨度）微调LLMs。论文引入了一个包含忠实和不忠实摘要以及跨度级标签的新数据集，并评估了三种微调技术来提高摘要的忠实度：梯度上升、非可能性训练和任务向量否定。

Result: 实验结果表明，所有三种方法都成功利用了跨度级标注来提高忠实度，其中非可能性训练（unlikelihood training）被证明是最有效的方法。

Conclusion: 通过利用GPT-4o生成的幻觉跨度级标注，并结合特定的微调策略（特别是非可能性训练），可以显著提高大型语言模型在抽象摘要生成中的忠实度，有效减少幻觉的发生。

Abstract: Abstractive summarization using large language models (LLMs) has become an
essential tool for condensing information. However, despite their ability to
generate fluent summaries, these models sometimes produce unfaithful summaries,
introducing hallucinations at the word, phrase, or concept level. Existing
mitigation strategies, such as post-processing corrections or contrastive
learning with synthetically generated negative samples, fail to fully address
the diverse errors that can occur in LLM-generated summaries. In this paper, we
investigate fine-tuning strategies to reduce the occurrence of unfaithful spans
in generated summaries. First, we automatically generate summaries for the set
of source documents in the training set with a variety of LLMs and then use
GPT-4o to annotate any hallucinations it detects at the span-level. Leveraging
these annotations, we fine-tune LLMs with both hallucination-free summaries and
annotated unfaithful spans to enhance model faithfulness. In this paper, we
introduce a new dataset that contains both faithful and unfaithful summaries
with span-level labels and we evaluate three techniques to fine-tuning a LLM to
improve the faithfulness of the resulting summarization: gradient ascent,
unlikelihood training, and task vector negation. Experimental results show that
all three approaches successfully leverage span-level annotations to improve
faithfulness, with unlikelihood training being the most effective.

</details>


### [318] [Unpacking Hateful Memes: Presupposed Context and False Claims](https://arxiv.org/abs/2510.09935)
*Weibin Cai,Jiayu Li,Reza Zafarani*

Main category: cs.CL

TL;DR: 本文提出了一种名为SHIELD的仇恨表情包检测框架，该框架基于哲学和心理学见解，通过建模预设语境（PCM）和检测虚假主张（FACT）来捕捉仇恨的本质，并超越了现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 当前的仇恨表情包检测方法主要依赖预训练语言模型，但较少关注“是什么让表情包充满仇恨”。作者认为仇恨表情包对个人和社会造成严重危害，因此需要深入理解其内在特征。

Method: 本文提出仇恨表情包的两个核心特征：预设语境和虚假主张。为捕捉预设语境，开发了PCM模块来建模跨模态语境信息；为检测虚假主张，引入了FACT模块，该模块整合外部知识并利用跨模态参考图。通过结合PCM和FACT，构建了SHIELD框架。

Result: SHIELD在多个数据集和指标上均优于现有的最先进方法，并在假新闻检测等其他任务上展现了通用性。

Conclusion: SHIELD框架成功捕捉了仇恨的根本性质，在仇恨表情包检测方面表现出卓越的性能和多任务处理能力。

Abstract: While memes are often humorous, they are frequently used to disseminate hate,
causing serious harm to individuals and society. Current approaches to hateful
meme detection mainly rely on pre-trained language models. However, less focus
has been dedicated to \textit{what make a meme hateful}. Drawing on insights
from philosophy and psychology, we argue that hateful memes are characterized
by two essential features: a \textbf{presupposed context} and the expression of
\textbf{false claims}. To capture presupposed context, we develop \textbf{PCM}
for modeling contextual information across modalities. To detect false claims,
we introduce the \textbf{FACT} module, which integrates external knowledge and
harnesses cross-modal reference graphs. By combining PCM and FACT, we introduce
\textbf{\textsf{SHIELD}}, a hateful meme detection framework designed to
capture the fundamental nature of hate. Extensive experiments show that SHIELD
outperforms state-of-the-art methods across datasets and metrics, while
demonstrating versatility on other tasks, such as fake news detection.

</details>


### [319] [Unifying Tree Search Algorithm and Reward Design for LLM Reasoning: A Survey](https://arxiv.org/abs/2510.09988)
*Jiaqi Wei,Xiang Zhang,Yuejin Yang,Wenxuan Huang,Juntai Cao,Sheng Xu,Xiang Zhuang,Zhangyang Gao,Muhammad Abdul-Mageed,Laks V. S. Lakshmanan,Chenyu You,Wanli Ouyang,Siqi Sun*

Main category: cs.CL

TL;DR: 本文提出一个统一框架，旨在解决大型语言模型（LLM）中审慎树搜索领域的分散性和奖励信号角色模糊的问题，该框架将搜索算法分解为三个核心组件，并区分了测试时扩展和自我改进中的奖励信号。


<details>
  <summary>Details</summary>
Motivation: 现代LLM研究中，审慎树搜索虽是核心，但该领域存在碎片化，缺乏统一的形式化，尤其是在奖励信号的角色方面存在模糊性（是瞬态启发式还是持久学习目标）。

Method: 引入一个统一框架，将搜索算法解构为“搜索机制”、“奖励公式”和“转换函数”三个核心组件。正式区分了用于测试时扩展的瞬态“搜索引导”和用于自我改进的持久“参数化奖励建模”。在此基础上，提出了一个以组件为中心的分类法。

Result: 建立了一个统一的形式化框架和分类法，明确了审慎树搜索中奖励信号的角色，并综合了现有技术水平。

Conclusion: 该框架和分类法为理解和推进自主、自我改进智能体的系统性进展提供了研究路线图，有助于更系统地发展该领域。

Abstract: Deliberative tree search is a cornerstone of modern Large Language Model
(LLM) research, driving the pivot from brute-force scaling toward algorithmic
efficiency. This single paradigm unifies two critical frontiers:
\textbf{Test-Time Scaling (TTS)}, which deploys on-demand computation to solve
hard problems, and \textbf{Self-Improvement}, which uses search-generated data
to durably enhance model parameters. However, this burgeoning field is
fragmented and lacks a common formalism, particularly concerning the ambiguous
role of the reward signal -- is it a transient heuristic or a durable learning
target? This paper resolves this ambiguity by introducing a unified framework
that deconstructs search algorithms into three core components: the
\emph{Search Mechanism}, \emph{Reward Formulation}, and \emph{Transition
Function}. We establish a formal distinction between transient \textbf{Search
Guidance} for TTS and durable \textbf{Parametric Reward Modeling} for
Self-Improvement. Building on this formalism, we introduce a component-centric
taxonomy, synthesize the state-of-the-art, and chart a research roadmap toward
more systematic progress in creating autonomous, self-improving agents.

</details>


### [320] [Beyond Fertility: Analyzing STRR as a Metric for Multilingual Tokenization Evaluation](https://arxiv.org/abs/2510.09947)
*Mir Tafseer Nayeem,Sawsan Alqahtani,Md Tahmid Rahman Laskar,Tasnim Mohiuddin,M Saiful Bari*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型中分词的重要性，指出传统生育率指标的局限性，并提出了新的指标“单token保留率”（STRR）来评估多语言分词器的公平性，揭示了不同语言间存在的偏向，并为设计更公平的分词器提供了指导。


<details>
  <summary>Details</summary>
Motivation: 分词是大型语言模型中一个关键但未被充分评估的步骤。现有标准指标“生育率”（每词平均token数）虽能反映压缩效率，但无法揭示词汇在不同语言和领域间的分配方式，存在盲点。

Method: 研究分析了六种常用分词器，涉及七种语言和两个领域，评估了其生育率。为解决生育率的局限性，研究提出并使用了“单token保留率”（STRR）这一新指标，该指标衡量单词被保留为单个token的比例。

Result: 生育率分析显示英语稳定、中文较高，且领域敏感性较低。STRR则揭示了对英语的系统性优先处理、对中文的强力支持以及印地语的碎片化现象，从而提供了跨语言公平性的可解释视图。

Conclusion: STRR作为生育率的有效补充，能够揭示分词器在跨语言公平性方面的表现，并为设计更公平的多语言分词器提供了实用指导。

Abstract: Tokenization is a crucial but under-evaluated step in large language models
(LLMs). The standard metric, fertility (the average number of tokens per word),
captures compression efficiency but obscures how vocabularies are allocated
across languages and domains. We analyze six widely used tokenizers across
seven languages and two domains, finding stable fertility for English, high
fertility for Chinese, and little domain sensitivity. To address fertility's
blind spots, we propose the Single Token Retention Rate (STRR), which measures
the proportion of words preserved as single tokens. STRR reveals systematic
prioritization of English, strong support for Chinese, and fragmentation in
Hindi, offering an interpretable view of cross-lingual fairness. Our results
show that STRR complements fertility and provides practical guidance for
designing more equitable multilingual tokenizers.

</details>


### [321] [Toward Machine Translation Literacy: How Lay Users Perceive and Rely on Imperfect Translations](https://arxiv.org/abs/2510.09994)
*Yimin Xiao,Yongle Zhang,Dayeon Ki,Calvin Bao,Marianna J. Martindale,Charlotte Vaughn,Ge Gao,Marine Carpuat*

Main category: cs.CL

TL;DR: 本研究通过一项公共博物馆的用户研究（n=452），发现非双语用户常因缺乏评估策略而过度依赖机器翻译（MT），而错误经历会促使他们重新评估未来的依赖。这强调了提高MT质量和用户素养的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着机器翻译日益普及，理解公众如何感知和依赖不完美的机器翻译对于将MT研究置于实际应用背景中至关重要。

Method: 在公共博物馆进行了一项人类研究（n=452），调查了流利度和充分性错误如何影响双语和非双语用户在日常使用中对机器翻译的依赖。

Result: 非双语用户常因缺乏评估策略和替代方案而过度依赖机器翻译。然而，经历错误会促使用户重新评估未来的依赖。

Conclusion: 研究强调了机器翻译评估和自然语言处理解释技术的重要性，不仅要提高机器翻译质量，还要提升用户的机器翻译素养。

Abstract: As Machine Translation (MT) becomes increasingly commonplace, understanding
how the general public perceives and relies on imperfect MT is crucial for
contextualizing MT research in real-world applications. We present a human
study conducted in a public museum (n=452), investigating how fluency and
adequacy errors impact bilingual and non-bilingual users' reliance on MT during
casual use. Our findings reveal that non-bilingual users often over-rely on MT
due to a lack of evaluation strategies and alternatives, while experiencing the
impact of errors can prompt users to reassess future reliance. This highlights
the need for MT evaluation and NLP explanation techniques to promote not only
MT quality, but also MT literacy among its users.

</details>


### [322] [MTP-S2UT: Enhancing Speech-to-Speech Translation Quality with Multi-token Prediction](https://arxiv.org/abs/2510.10003)
*Jianjin Wang,Runsong Zhao,Xiaoqian Liu,Yuan Ge,Ziqiang Xu,Tong Xiao,Shengxiang Gao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出将多令牌预测（MTP）损失引入语音到单元翻译（S2UT）模型，以解决单个语音令牌语义密度不足的问题。通过在每个位置预测多个后续令牌，并特别是在中间层应用MTP损失（MTP-S2UT），显著提高了S2UT的翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前的直接语音到语音翻译方法主要使用语音令牌作为中间表示，但单个语音令牌语义密度不高，通常需要多个令牌才能表达一个完整的语义单元，这限制了信息密度。

Method: 研究引入了多令牌预测（MTP）损失到S2UT模型中，使模型能够在每个位置预测多个后续令牌，从而捕获更完整的语义并提高信息密度。此外，论文提出MTP-S2UT损失，将MTP损失应用于计算CTC损失的中间隐藏表示层，以实现更早、更有效的隐藏表示增强。

Result: 实验表明，所有MTP损失变体都能持续改进S2UT翻译质量，其中MTP-S2UT实现了最佳性能。

Conclusion: 通过在S2UT模型中引入多令牌预测损失，特别是将其应用于中间隐藏表示层（MTP-S2UT），能够有效解决单个语音令牌语义密度不足的问题，显著提升语音到单元翻译的质量。

Abstract: Current direct speech-to-speech translation methods predominantly employ
speech tokens as intermediate representations. However, a single speech token
is not dense in semantics, so we generally need multiple tokens to express a
complete semantic unit. To address this limitation, we introduce multi-token
prediction (MTP) loss into speech-to-unit translation (S2UT) models, enabling
models to predict multiple subsequent tokens at each position, thereby
capturing more complete semantics and enhancing information density per
position. Initial MTP implementations apply the loss at the final layer, which
improves output representation but initiates information enrichment too late.
We hypothesize that advancing the information enrichment process to
intermediate layers can achieve earlier and more effective enhancement of
hidden representation. Consequently, we propose MTP-S2UT loss, applying MTP
loss to hidden representation where CTC loss is computed. Experiments
demonstrate that all MTP loss variants consistently improve the quality of S2UT
translation, with MTP-S2UT achieving the best performance.

</details>


### [323] [Beyond the limitation of a single query: Train your LLM for query expansion with Reinforcement Learning](https://arxiv.org/abs/2510.10009)
*Shu Zhao,Tan Yu,Anbang Xu*

Main category: cs.CL

TL;DR: 本文提出ExpandSearch，一个基于LLM的搜索代理，通过强化学习实现原生查询扩展，并结合预训练的“squeezer”模型辅助文档理解，显著提升了多跳问答（multi-hop QA）的性能，即使是小型LLM也能达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有的推理增强搜索代理（如Search-R1）在推理和搜索能力上有限，导致在多跳问答基准测试上表现不佳，难以处理复杂或复合查询。此外，在有限的训练数据和计算资源下，让一个搜索代理掌握查询生成、检索信息理解和答案生成等多项任务极具挑战性。

Method: 通过强化学习训练一个基于LLM的搜索代理，使其具备原生的查询扩展能力。在每个回合中，该代理会提出多个查询变体进行并行搜索以覆盖更多相关信息。同时，引入一个预训练的“squeezer”模型来帮助搜索代理理解检索到的文档，从而让搜索代理能专注于查询生成以实现高检索召回率。

Result: 研究发现，即使是小型3B LLM，在squeezer模型的辅助下，也能展现出强大的查询扩展能力，并在多跳问答基准测试上达到最先进的准确率。具体而言，ExpandSearch在七个问答基准测试中，相较于最先进的基线方法平均提高了4.4%的准确率，在需要聚合多样化证据的多跳推理任务上取得了显著的提升。

Conclusion: ExpandSearch通过结合查询扩展和squeezer模型，有效解决了多跳问答中推理和搜索能力的局限性，即使使用较小的LLM也能显著提升复杂查询的信息检索和问答性能，实现了SOTA表现。

Abstract: Reasoning-augmented search agents, such as Search-R1, are trained to reason,
search, and generate the final answer iteratively. Nevertheless, due to their
limited capabilities in reasoning and search, their performance on multi-hop QA
benchmarks remains far from satisfactory. To handle complex or compound
queries, we train an LLM-based search agent with the native capability of query
expansion through reinforcement learning. In each turn, our search agent
proposes several query variants, which are searched simultaneously to cover
more relevant information. Meanwhile, given limited post-training data and
computing resources, it is very challenging for a search agent to master
multiple tasks, including query generation, retrieved information
understanding, and answer generation. Therefore, we propose incorporating a
pre-trained squeezer model that helps the search agent understand the retrieved
documents, allowing the search agent to focus on query generation for high
retrieval recall. With the assistance of the squeezer model, we discover that
even a small-scale 3B LLM can demonstrate a strong capability of query
expansion and achieve state-of-the-art accuracy on the multi-hop QA benchmarks.
To be specific, our experiments across seven question-answering benchmarks
demonstrate that our method, named ExpandSearch, achieves an average
improvement of 4.4% compared to state-of-the-art baselines, with strong gains
on multi-hop reasoning tasks requiring diverse evidence aggregation.

</details>


### [324] [Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default](https://arxiv.org/abs/2510.10025)
*Jiaqi Liu,Lanruo Wang,Su Liu,Xin Hu*

Main category: cs.CL

TL;DR: 研究表明，在医疗摘要分类任务中，紧凑型编码器（如DistilBERT）结合标准交叉熵损失，能在低成本、低延迟和高隐私要求下，实现性能与效率的最佳平衡，优于BERT base。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在医疗环境中部署面临严格的成本、延迟和隐私限制。本研究旨在探讨在受控预算下，轻量级编码器在医疗摘要分类任务中能达到何种程度。

Method: 使用公共医疗摘要语料库，对BERT base和DistilBERT进行微调。实验中固定了分词器、序列长度、优化器和调度器。采用了三种目标函数：标准交叉熵、类别加权交叉熵和Focal Loss。评估指标包括准确率、Macro F1和Weighted F1，并进行了混淆分析。

Result: 结果显示，DistilBERT结合标准交叉熵损失在测试集上提供了最佳的平衡，且参数量远少于BERT base。

Conclusion: 研究建议，在医疗摘要分类等任务中，实用的默认方法是首先使用紧凑型编码器和交叉熵损失，然后添加校准和任务特定检查，最后再考虑使用更大型的模型。

Abstract: Large language models work well for many NLP tasks, but they are hard to
deploy in health settings with strict cost, latency, and privacy limits. We
revisit a lightweight recipe for medical abstract classification and ask how
far compact encoders can go under a controlled budget. Using the public medical
abstracts corpus, we finetune BERT base and DistilBERT with three objectives
standard cross-entropy, class weighted cross entropy, and focal loss keeping
tokenizer, sequence length, optimizer, and schedule fixed. DistilBERT with
plain cross-entropy gives the best balance on the test set while using far
fewer parameters than BERT base. We report accuracy, Macro F1, and Weighted F1,
release the evaluation code, and include confusion analyses to make error
patterns clear. Our results suggest a practical default: start with a compact
encoder and cross-entropy, then add calibration and task-specific checks before
moving to heavier models.

</details>


### [325] [Path Drift in Large Reasoning Models:How First-Person Commitments Override Safety](https://arxiv.org/abs/2510.10013)
*Yuyi Huang,Runzhe Zhan,Lidia S. Chao,Ailin Tao,Derek F. Wong*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在长链思维（Long-CoT）推理中存在“路径漂移”漏洞，即推理路径偏离安全准则。文章揭示了导致漂移的三个行为触发器，提出了一个诱导路径漂移的框架，并提出了一种路径级别的防御策略。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在复杂推理任务中广泛部署并采用RLHF等对齐技术进行早期安全防护，但研究者发现Long-CoT模型中的推理轨迹可能偏离对齐路径，导致违反安全约束的内容，这一漏洞此前未被充分探索。

Method: 通过实证分析揭示了路径漂移的三个行为触发器。在此基础上，引入了一个由认知负荷放大、自我角色启动和条件链劫持组成的三阶段路径漂移诱导框架。为缓解风险，提出了一种包含角色归因纠正和元认知反思（反思性安全提示）的路径级防御策略。

Result: 发现了路径漂移的三个行为触发器：第一人称承诺诱导的目标驱动推理延迟拒绝信号；表面免责声明绕过对齐检查点的伦理蒸发；以及分层提示逐步引导模型走向不安全完成的条件链升级。诱导框架的每个阶段都能独立降低拒绝率，组合使用时效果更强。提出的防御策略能够有效缓解这些风险。

Conclusion: 研究结果强调了在长篇推理中，除了令牌级对齐，还需要对轨迹级对齐进行监督，以确保模型的安全性。

Abstract: As large language models (LLMs) are increasingly deployed for complex
reasoning tasks, Long Chain-of-Thought (Long-CoT) prompting has emerged as a
key paradigm for structured inference. Despite early-stage safeguards enabled
by alignment techniques such as RLHF, we identify a previously underexplored
vulnerability: reasoning trajectories in Long-CoT models can drift from aligned
paths, resulting in content that violates safety constraints. We term this
phenomenon Path Drift. Through empirical analysis, we uncover three behavioral
triggers of Path Drift: (1) first-person commitments that induce goal-driven
reasoning that delays refusal signals; (2) ethical evaporation, where
surface-level disclaimers bypass alignment checkpoints; (3) condition chain
escalation, where layered cues progressively steer models toward unsafe
completions. Building on these insights, we introduce a three-stage Path Drift
Induction Framework comprising cognitive load amplification, self-role priming,
and condition chain hijacking. Each stage independently reduces refusal rates,
while their combination further compounds the effect. To mitigate these risks,
we propose a path-level defense strategy incorporating role attribution
correction and metacognitive reflection (reflective safety cues). Our findings
highlight the need for trajectory-level alignment oversight in long-form
reasoning beyond token-level alignment.

</details>


### [326] [HUME: Measuring the Human-Model Performance Gap in Text Embedding Task](https://arxiv.org/abs/2510.10062)
*Adnan El Assadi,Isaac Chung,Roman Solomatin,Niklas Muennighoff,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 本文引入了HUME框架，旨在测量人类在文本嵌入任务上的表现，以更好地理解模型能力。研究发现，人类平均表现略低于最佳模型，但不同任务和语言之间存在显著差异，并提供了人类表现基线和评估框架。


<details>
  <summary>Details</summary>
Motivation: 理解嵌入模型的优势和局限性需要比较人类和模型的表现，但衡量人类在嵌入任务上的性能十分困难。现有评估框架（如MTEB）缺乏可靠的人类表现估计，限制了模型分数的解释性。

Method: 引入了HUME（文本嵌入人类评估框架），并在涵盖重排序、分类、聚类和语义文本相似性等任务的16个MTEB数据集上测量了人类表现，这些数据集包括多种高资源和低资源语言。

Result: 人类的平均表现为77.6%，而最佳嵌入模型为80.1%。然而，性能差异显著：模型在某些数据集上接近上限，但在其他数据集（特别是低资源语言）上表现不佳，这表明数据集存在问题并揭示了模型在低资源语言上的不足。

Conclusion: 该研究提供了人类表现基线、任务难度模式的洞察以及一个可扩展的评估框架。这使得模型解释更具意义，并为模型和基准的开发提供了信息。

Abstract: Comparing human and model performance offers a valuable perspective for
understanding the strengths and limitations of embedding models, highlighting
where they succeed and where they fail to capture meaning and nuance. However,
such comparisons are rarely made, as human performance on embedding tasks is
difficult to measure. To fill this gap, we introduce HUME: Human Evaluation
Framework for Text Embeddings. While frameworks like MTEB provide broad model
evaluation, they lack reliable estimates of human performance, limiting the
interpretability of model scores. We measure human performance across 16 MTEB
datasets spanning reranking, classification, clustering, and semantic textual
similarity across linguistically diverse high- and low-resource languages.
Humans achieve an average performance of 77.6% compared to 80.1% for the best
embedding model, although variation is substantial: models reach near-ceiling
performance on some datasets while struggling on others, suggesting dataset
issues and revealing shortcomings in low-resource languages. We provide human
performance baselines, insight into task difficulty patterns, and an extensible
evaluation framework that enables a more meaningful interpretation of the model
and informs the development of both models and benchmarks. Our code, dataset,
and leaderboard are publicly available at
https://github.com/embeddings-benchmark/mteb.

</details>


### [327] [CLMN: Concept based Language Models via Neural Symbolic Reasoning](https://arxiv.org/abs/2510.10063)
*Yibo Yang*

Main category: cs.CL

TL;DR: 本文提出了概念语言模型网络（CLMN），一个神经符号框架，它通过连续、人类可读的概念嵌入和模糊逻辑推理来学习自适应概念交互规则，从而在保持高性能的同时显著提升NLP模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在自然语言处理（NLP）中取得了显著进展，但其可解释性仍然有限，尤其在医疗和金融等关键领域。现有的NLP概念瓶颈模型存在局限性，如二元激活损害文本表示或潜在概念削弱语义，并且很少能有效建模动态概念交互（如否定和上下文）。

Method: 引入了概念语言模型网络（CLMN）。该模型将概念表示为连续、人类可读的嵌入，并应用模糊逻辑推理来学习自适应的交互规则，这些规则描述了概念如何相互影响以及对最终决策的影响。CLMN使用概念感知表示增强原始文本特征，并自动推导可解释的逻辑规则。

Result: 在多个数据集和预训练语言模型上，CLMN与现有基于概念的方法相比，实现了更高的准确性，同时显著提高了模型解释的质量。

Conclusion: 将神经表示与符号推理在统一的概念空间中进行整合，可以有效地构建实用且透明的NLP系统。

Abstract: Deep learning has advanced NLP, but interpretability remains limited,
especially in healthcare and finance. Concept bottleneck models tie predictions
to human concepts in vision, but NLP versions either use binary activations
that harm text representations or latent concepts that weaken semantics, and
they rarely model dynamic concept interactions such as negation and context. We
introduce the Concept Language Model Network (CLMN), a neural-symbolic
framework that keeps both performance and interpretability. CLMN represents
concepts as continuous, human-readable embeddings and applies fuzzy-logic
reasoning to learn adaptive interaction rules that state how concepts affect
each other and the final decision. The model augments original text features
with concept-aware representations and automatically induces interpretable
logic rules. Across multiple datasets and pre-trained language models, CLMN
achieves higher accuracy than existing concept-based methods while improving
explanation quality. These results show that integrating neural representations
with symbolic reasoning in a unified concept space can yield practical,
transparent NLP systems.

</details>


### [328] [Unilaw-R1: A Large Language Model for Legal Reasoning with Reinforcement Learning and Iterative Inference](https://arxiv.org/abs/2510.10072)
*Hua Cai,Shuang Zhao,Liang Zhang,Xuli Shen,Qing Xu,Weilin Shen,Zihao Wen,Tianke Ban*

Main category: cs.CL

TL;DR: 本文介绍了Unilaw-R1，一个轻量级（70亿参数）的法律推理大语言模型，通过高质量数据集和两阶段训练策略，解决了法律领域知识不足、推理逻辑不可靠和业务泛化能力弱等挑战，并在法律推理任务上取得了显著优于同规模模型的性能，且接近大型模型的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管推理型大语言模型在各领域快速发展，但其在处理复杂法律问题方面的能力尚未得到充分探索。现有模型在法律领域面临法律知识不足、推理逻辑不可靠以及业务泛化能力弱等核心挑战。

Method: 研究引入了Unilaw-R1，一个70亿参数的法律推理大语言模型。首先构建了Unilaw-R1-Data，一个包含1.7万个精炼筛选的思维链（CoT）样本的高质量数据集。在此基础上，采用了结合监督微调（SFT）和强化学习（RL）的两阶段训练策略。此外，还引入了Unilaw-R1-Eval，一个专门用于评估模型在单选和多选法律任务中法律推理能力的基准。

Result: Unilaw-R1在权威基准测试中表现出色，超越了所有同等规模的模型，并达到了与更大规模的DeepSeek-R1-Distill-Qwen-32B（54.9%）相当的性能。经过领域特定训练后，它在LawBench和LexEval上也取得了显著提升，平均超越Qwen-2.5-7B-Instruct（46.6%）6.6%。

Conclusion: Unilaw-R1是一个为法律推理量身定制的有效大语言模型，它以较低的部署成本解决了法律领域的关键挑战，并在复杂法律推理任务上展示了强大的性能，支持法律AI应用中的可解释决策。

Abstract: Reasoning-focused large language models (LLMs) are rapidly evolving across
various domains, yet their capabilities in handling complex legal problems
remains underexplored. In this paper, we introduce Unilaw-R1, a large language
model tailored for legal reasoning. With a lightweight 7-billion parameter
scale, Unilaw-R1 significantly reduces deployment cost while effectively
tackling three core challenges in the legal domain: insufficient legal
knowledge, unreliable reasoning logic, and weak business generalization. To
address these issues, we first construct Unilaw-R1-Data, a high-quality dataset
containing 17K distilled and screened chain-of-thought (CoT) samples. Based on
this, we adopt a two-stage training strategy combining Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL), which significantly boosts the
performance on complex legal reasoning tasks and supports interpretable
decision-making in legal AI applications. To assess legal reasoning ability, we
also introduce Unilaw-R1-Eval, a dedicated benchmark designed to evaluate
models across single- and multi-choice legal tasks. Unilaw-R1 demonstrates
strong results on authoritative benchmarks, outperforming all models of similar
scale and achieving performance on par with the much larger
DeepSeek-R1-Distill-Qwen-32B (54.9%). Following domain-specific training, it
also showed significant gains on LawBench and LexEval, exceeding
Qwen-2.5-7B-Instruct (46.6%) by an average margin of 6.6%.

</details>


### [329] [A-IPO: Adaptive Intent-driven Preference Optimization](https://arxiv.org/abs/2510.10077)
*Wenqing Wang,Muhammad Asif Ali,Ali Shoker,Ruohan Yang,Junyang Chen,Ying Sha,Huan Wang*

Main category: cs.CL

TL;DR: 本文提出A-IPO（自适应意图驱动偏好优化）方法，通过引入意图模块来推断用户潜在意图并将其融入奖励函数，从而实现多元化和鲁棒的偏好对齐，克服了现有DPO方法忽视少数意见和潜在意图的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如DPO及其变体）倾向于多数意见，忽视了少数观点，也未能捕捉到提示中潜在的用户意图，导致无法充分适应区域、文化和社会因素塑造的、多样化和动态的人类偏好。

Method: A-IPO方法引入了一个意图模块，用于推断每个用户提示背后的潜在意图，并明确地将推断出的意图纳入奖励函数中，以鼓励模型响应与用户潜在意图更强地对齐。理论和实证均证明，引入意图-响应相似性项可增加偏好边际。此外，论文还引入了两个新基准Real-pref和Attack-pref，并扩展了现有数据集GlobalOpinionQA-Ext用于评估。

Result: A-IPO通过意图-响应相似性项增加了偏好边际（在对数几率中正向偏移λΔsim），实现了偏好和非偏好响应之间更清晰的分离。在Real-pref上，胜率提升高达24.8%，响应-意图一致性提升高达45.6%；在Attack-pref上，响应相似性提升高达38.6%，防御成功率提升高达52.2%；在GlobalOpinionQA-Ext上，意图一致性得分提升高达54.6%。这些结果表明A-IPO持续超越现有基线。

Conclusion: 通过显式建模多样化的用户意图，A-IPO促进了多元化的偏好优化，同时增强了偏好对齐的对抗鲁棒性。全面的实证评估证明了A-IPO在各项关键指标上均显著优于现有基线。

Abstract: Human preferences are diverse and dynamic, shaped by regional, cultural, and
social factors. Existing alignment methods like Direct Preference Optimization
(DPO) and its variants often default to majority views, overlooking minority
opinions and failing to capture latent user intentions in prompts.
  To address these limitations, we introduce \underline{\textbf{A}}daptive
\textbf{\underline{I}}ntent-driven \textbf{\underline{P}}reference
\textbf{\underline{O}}ptimization (\textbf{A-IPO}). Specifically,A-IPO
introduces an intention module that infers the latent intent behind each user
prompt and explicitly incorporates this inferred intent into the reward
function, encouraging stronger alignment between the preferred model's
responses and the user's underlying intentions. We demonstrate, both
theoretically and empirically, that incorporating an intention--response
similarity term increases the preference margin (by a positive shift of
$\lambda\,\Delta\mathrm{sim}$ in the log-odds), resulting in clearer separation
between preferred and dispreferred responses compared to DPO.
  For evaluation, we introduce two new benchmarks, Real-pref, Attack-pref along
with an extended version of an existing dataset, GlobalOpinionQA-Ext, to assess
real-world and adversarial preference alignment.
  Through explicit modeling of diverse user intents,A-IPO facilitates
pluralistic preference optimization while simultaneously enhancing adversarial
robustness in preference alignment. Comprehensive empirical evaluation
demonstrates that A-IPO consistently surpasses existing baselines, yielding
substantial improvements across key metrics: up to +24.8 win-rate and +45.6
Response-Intention Consistency on Real-pref; up to +38.6 Response Similarity
and +52.2 Defense Success Rate on Attack-pref; and up to +54.6 Intention
Consistency Score on GlobalOpinionQA-Ext.

</details>


### [330] [Stop When Enough: Adaptive Early-Stopping for Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.10103)
*Renliang Sun,Wei Cheng,Dawei Li,Haifeng Chen,Wei Wang*

Main category: cs.CL

TL;DR: REFRAIN是一个免训练框架，通过自适应地决定何时停止推理来减轻大型语言模型（LLMs）在链式思考（CoT）中的“过度思考”问题，从而在保持或提高准确性的同时显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 链式思考（CoT）提高了LLMs在推理任务上的表现，但过度或冗余的推理（即“过度思考”）会增加推理成本，并可能导致LLMs得出错误的结论。

Method: REFRAIN框架包含两个主要组件：一个两阶段停止判别器，用于识别反思性但冗余的推理；一个滑动窗口上限置信度（SW-UCB）多臂老虎机控制器，用于根据问题难度动态调整停止阈值。该框架无需训练、监督或微调。

Result: 在四个代表性基准测试和两个模型家族上，REFRAIN与标准CoT提示相比，将token使用量减少了20-55%，同时保持或提高了准确性。广泛的消融和鲁棒性分析表明其在不同模型、评分器和提示变体下都具有稳定性。

Conclusion: 研究结果强调了“何时停止”是测试时扩展的一个新的实用维度，它使模型能够进行“恰到好处”而非过度的推理。

Abstract: Chain-of-Thought (CoT) reasoning has driven recent gains of large language
models (LLMs) on reasoning-intensive tasks by externalizing intermediate steps.
However, excessive or redundant reasoning -- so-called overthinking -- can
increase inference costs and lead LLMs toward incorrect conclusions. In this
paper, we present REFRAIN ($\underline{REF}$lective-$\underline{R}$edundancy
for $\underline{A}$daptive $\underline{IN}$ference), a training-free framework
that adaptively determines when to stop reasoning to mitigate overthinking.
REFRAIN integrates a two-stage stop discriminator to identify reflective yet
redundant reasoning and a sliding-window Upper Confidence Bound (SW-UCB)
multi-armed bandit controller to dynamically adjust stopping thresholds
according to problem difficulty without supervision or fine-tuning. Across four
representative benchmarks and two model families, REFRAIN reduces token usage
by 20-55% while maintaining or improving accuracy compared to standard CoT
prompting. Extensive ablation and robustness analyses demonstrate its stability
across models, scorers, and prompt variations. In summary, our findings
highlight when-to-stop as a new and practical axis of test-time scaling --
enabling models to reason not just more, but just enough.

</details>


### [331] [Diversity Augmentation of Dynamic User Preference Data for Boosting Personalized Text Summarizers](https://arxiv.org/abs/2510.10082)
*Parthiv Chatterjee,Shivam Sonawane,Amey Hengle,Aditya Tanna,Sourish Dasgupta,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 由于数据稀缺，个性化摘要面临挑战。本文提出了一种名为PerAugy的数据增强技术，通过轨迹重排和摘要内容扰动，显著提升了用户编码器和个性化摘要框架的性能，并发现增强的数据多样性是性能提升的关键因素。


<details>
  <summary>Details</summary>
Motivation: 文档摘要受个体主观性影响，难以识别多方面文档中的主观突出信息，因此需要个性化摘要。然而，由于缺乏同时包含用户偏好历史和预期摘要的训练数据，训练个性化摘要模型具有挑战性。现有数据集（如MS/CAS PENS）缺乏目标摘要且主题转换多样性有限，限制了模型泛化能力。

Method: 本文提出了一种新颖的数据增强技术——PerAugy。该技术通过跨轨迹洗牌（cross-trajectory shuffling）和摘要内容扰动（summary-content perturbation）来生成多样化数据。研究人员利用PerAugy显著提升了四种最先进（SOTA）的用户编码器准确性，并将这些改进后的用户编码器集成到两种SOTA摘要框架中进行评估。此外，引入了TP、RTC和DegreeD三种数据集多样性指标，对PerAugy诱导的多样性进行了事后分析。

Result: PerAugy显著提升了四种SOTA用户编码器的准确性（最佳结果：AUC提升0.132）。当使用增强后的用户编码器时，两种SOTA摘要框架的个性化能力均持续增强（PSE-SU4指标平均提升61.2%）。事后分析表明，TP和DegreeD指标与PerAugy生成数据集上的用户编码器性能强烈相关，表明数据集多样性的增加是性能提升的关键因素。

Conclusion: PerAugy通过生成多样化的训练数据，有效解决了个性化摘要领域的数据稀缺问题，从而显著提升了用户编码器的性能和个性化摘要框架的效果。研究结果强调了数据多样性在驱动个性化摘要性能提升中的关键作用。

Abstract: Document summarization enables efficient extraction of user-relevant content
but is inherently shaped by individual subjectivity, making it challenging to
identify subjective salient information in multifaceted documents. This
complexity underscores the necessity for personalized summarization. However,
training models for personalized summarization has so far been challenging,
particularly because diverse training data containing both user preference
history (i.e., click-skip trajectory) and expected (gold-reference) summaries
are scarce. The MS/CAS PENS dataset is a valuable resource but includes only
preference history without target summaries, preventing end-to-end supervised
learning, and its limited topic-transition diversity further restricts
generalization. To address this, we propose $\mathrm{PerAugy}$, a novel
cross-trajectory shuffling and summary-content perturbation based data
augmentation technique that significantly boosts the accuracy of four
state-of-the-art baseline (SOTA) user-encoders commonly used in personalized
summarization frameworks (best result: $\text{0.132}$$\uparrow$ w.r.t AUC). We
select two such SOTA summarizer frameworks as baselines and observe that when
augmented with their corresponding improved user-encoders, they consistently
show an increase in personalization (avg. boost: $\text{61.2\%}\uparrow$ w.r.t.
PSE-SU4 metric). As a post-hoc analysis of the role of induced diversity in the
augmented dataset by \peraugy, we introduce three dataset diversity metrics --
$\mathrm{TP}$, $\mathrm{RTC}$, and \degreed\ to quantify the induced diversity.
We find that $\mathrm{TP}$ and $\mathrm{DegreeD}$ strongly correlate with
user-encoder performance on the PerAugy-generated dataset across all accuracy
metrics, indicating that increased dataset diversity is a key factor driving
performance gains.

</details>


### [332] [LinearRAG: Linear Graph Retrieval Augmented Generation on Large-scale Corpora](https://arxiv.org/abs/2510.10114)
*Luyao Zhuang,Shengyuan Chen,Yilin Xiao,Huachi Zhou,Yujing Zhang,Hao Chen,Qinggang Zhang,Xiao Huang*

Main category: cs.CL

TL;DR: 传统RAG在处理大规模非结构化语料时面临挑战，现有GraphRAG方法依赖不稳定的关系抽取。本文提出LinearRAG，通过构建无关系的分层图（Tri-Graph）实现可靠高效的图构建，并采用两阶段检索策略，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: RAG在处理信息碎片化的大规模非结构化语料时效果不佳。现有基于知识图谱的RAG（GraphRAG）方法依赖不稳定且昂贵的关系抽取来构建图谱，常导致图谱噪声和检索质量下降。

Method: LinearRAG提出了一种高效框架。它通过轻量级实体抽取和语义链接构建一个无关系的分层图（Tri-Graph），避免了不稳定的关系建模。这种图构建方法与语料库大小呈线性关系，且不产生额外token消耗。检索策略采用两阶段：(i) 通过局部语义桥接激活相关实体，(ii) 通过全局重要性聚合进行段落检索。

Result: 在四个数据集上进行的大量实验表明，LinearRAG显著优于所有基线模型。

Conclusion: LinearRAG通过避免不稳定的关系抽取，提供了一种高效、可靠且精准的图增强检索生成（GraphRAG）解决方案，显著提升了性能。

Abstract: Retrieval-Augmented Generation (RAG) is widely used to mitigate
hallucinations of Large Language Models (LLMs) by leveraging external
knowledge. While effective for simple queries, traditional RAG systems struggle
with large-scale, unstructured corpora where information is fragmented. Recent
advances incorporate knowledge graphs to capture relational structures,
enabling more comprehensive retrieval for complex, multi-hop reasoning tasks.
However, existing graph-based RAG (GraphRAG) methods rely on unstable and
costly relation extraction for graph construction, often producing noisy graphs
with incorrect or inconsistent relations that degrade retrieval quality. In
this paper, we revisit the pipeline of existing GraphRAG systems and propose
LinearRAG (Linear Graph-based Retrieval-Augmented Generation), an efficient
framework that enables reliable graph construction and precise passage
retrieval. Specifically, LinearRAG constructs a relation-free hierarchical
graph, termed Tri-Graph, using only lightweight entity extraction and semantic
linking, avoiding unstable relation modeling. This new paradigm of graph
construction scales linearly with corpus size and incurs no extra token
consumption, providing an economical and reliable indexing of the original
passages. For retrieval, LinearRAG adopts a two-stage strategy: (i) relevant
entity activation via local semantic bridging, followed by (ii) passage
retrieval through global importance aggregation. Extensive experiments on four
datasets demonstrate that LinearRAG significantly outperforms baseline models.

</details>


### [333] [Hybrid OCR-LLM Framework for Enterprise-Scale Document Information Extraction Under Copy-heavy Task](https://arxiv.org/abs/2510.10138)
*Zilong Wang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本文提出一个系统框架，结合OCR和LLM，通过智能策略选择，优化了从大量重复内容文档中提取信息的准确性和效率，特别是在证件文档提取上取得了显著成果。


<details>
  <summary>Details</summary>
Motivation: 从大量重复内容文档（copy-heavy documents）中提取信息是企业文档处理中一个关键但未被充分研究的挑战。现有方法追求通用解决方案，但未能有效平衡准确性和效率，尤其是在重复性任务中。

Method: 该方法构建了一个系统框架，策略性地结合了OCR引擎和大型语言模型（LLMs）。它通过智能策略选择来利用文档特有的特性，而非采用通用解决方案。研究评估了25种配置，涵盖三种提取范式（直接、替换和基于表格）以及四种文档格式（PNG、DOCX、XLSX、PDF）。

Result: 通过基于表格的提取方法，该自适应框架取得了卓越成果：结构化文档的F1准确率达到1.0，延迟为0.97秒；对于具有挑战性的图像输入，与PaddleOCR集成后F1准确率为0.997，延迟为0.6秒。所有处理速度均在亚秒级。与多模态方法相比，性能提升了54倍，并且通过格式感知路由，能够以生产规模处理异构文档流。

Conclusion: 这项工作建立了一个通用原则：通过结构感知的方法选择，可以将重复性强的任务从计算负担转化为优化机会。这超越了证件提取的特定应用，为处理大量重复内容文档提供了新的范式。

Abstract: Information extraction from copy-heavy documents, characterized by massive
volumes of structurally similar content, represents a critical yet understudied
challenge in enterprise document processing. We present a systematic framework
that strategically combines OCR engines with Large Language Models (LLMs) to
optimize the accuracy-efficiency trade-off inherent in repetitive document
extraction tasks. Unlike existing approaches that pursue universal solutions,
our method exploits document-specific characteristics through intelligent
strategy selection. We implement and evaluate 25 configurations across three
extraction paradigms (direct, replacement, and table-based) on identity
documents spanning four formats (PNG, DOCX, XLSX, PDF). Through table-based
extraction methods, our adaptive framework delivers outstanding results: F1=1.0
accuracy with 0.97s latency for structured documents, and F1=0.997 accuracy
with 0.6 s for challenging image inputs when integrated with PaddleOCR, all
while maintaining sub-second processing speeds. The 54 times performance
improvement compared with multimodal methods over naive approaches, coupled
with format-aware routing, enables processing of heterogeneous document streams
at production scale. Beyond the specific application to identity extraction,
this work establishes a general principle: the repetitive nature of copy-heavy
tasks can be transformed from a computational burden into an optimization
opportunity through structure-aware method selection.

</details>


### [334] [DiffHeads: Differential Analysis and Inference-Time Masking of Bias Heads in Large Language Models](https://arxiv.org/abs/2510.10142)
*Tingxu Han,Wei Song,Ziqi Ding,Ziming Li,Chunrong Fang,Yuekang Li,Dongfang Liu,Zhenyu Chen,Zhenting Wang*

Main category: cs.CL

TL;DR: 本文系统研究了大型语言模型（LLM）中的不公平性，发现直接回答（DA）提示会显著增加偏见。通过定义token-to-head贡献分数，作者识别出DA提示下激活的“偏见注意力头”，并提出了DiffHeads框架，通过选择性掩盖这些偏见注意力头来有效减少不公平性，同时不损害模型效用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地在对公平性要求高的领域中进行决策，而现有研究虽然能发现偏见输出，但对生成偏见的机制缺乏深入理解，导致现有的缓解措施往往脆弱且不可靠。

Method: 研究方法包括：1) 比较直接回答（DA）提示和思维链（CoT）提示在八种LLM上的不公平性表现。2) 定义一个token-to-head贡献分数，以追踪每个token对单个注意力头的影响。3) 通过分析DA和CoT之间激活差异来识别“偏见注意力头”。4) 基于这些发现，提出了DiffHeads框架，通过选择性掩盖这些偏见注意力头来去偏。

Result: 主要结果包括：1) DA提示会触发LLM的固有偏见部分，导致不公平性测量值在单轮和两轮对话中分别增加534.5%和391.9%。2) 发现一小部分“偏见注意力头”在DA提示下激活，而在CoT提示下则大部分处于休眠状态，首次建立了提示策略与偏见产生之间的因果联系。3) DiffHeads框架在DA和CoT提示下分别将不公平性降低了49.4%和40.3%，且不损害模型效用。

Conclusion: 研究揭示了LLM中偏见产生的机制，特别是DA提示如何激活特定的“偏见注意力头”。基于这一洞察，提出的DiffHeads框架能够通过选择性掩盖这些偏见注意力头，有效地减轻LLM的不公平性，为LLM的去偏提供了轻量且有效的解决方案。

Abstract: Large language models (LLMs) increasingly mediate decisions in domains where
unfair treatment of demographic groups is unacceptable. Existing work probes
when biased outputs appear, but gives little insight into the mechanisms that
generate them, leaving existing mitigations largely fragile. In this paper, we
conduct a systematic investigation LLM unfairness and propose DiffHeads, a
lightweight debiasing framework for LLMs. We first compare Direct-Answer (DA)
prompting to Chain-of-Thought (CoT) prompting across eight representative open-
and closed-source LLMs. DA will trigger the nature bias part of LLM and improve
measured unfairness by 534.5%-391.9% in both one-turn and two-turn dialogues.
Next, we define a token-to-head contribution score that traces each token's
influence back to individual attention heads. This reveals a small cluster of
bias heads that activate under DA but stay largely dormant with CoT, providing
the first causal link between prompting strategy and bias emergence. Finally,
building on this insight, we propose DiffHeads that identifies bias heads
through differential activation analysis between DA and CoT, and selectively
masks only those heads. DiffHeads reduces unfairness by 49.4%, and 40.3% under
DA and CoT, respectively, without harming model utility.

</details>


### [335] [BILLY: Steering Large Language Models via Merging Persona Vectors for Creative Generation](https://arxiv.org/abs/2510.10157)
*Tsung-Min Pai,Jui-I Wang,Li-Chun Lu,Shao-Hua Sun,Hung-Yi Lee,Kai-Wei Chang*

Main category: cs.CL

TL;DR: BILLY是一个无需训练的框架，通过在单个大型语言模型中融合多个角色向量，模拟多LLM协作的创造力，同时显著降低计算成本和推理延迟。


<details>
  <summary>Details</summary>
Motivation: 多LLM系统虽然能增强创造力，但存在计算成本高和推理延迟大的显著缺点。

Method: BILLY通过直接在模型的激活空间中提取并融合多个不同的角色向量。在推理过程中，使用这个融合向量来引导模型的生成过程，从而在没有显式多LLM通信的情况下实现多视角输出。

Result: 在以创造力为导向的基准测试中，BILLY超越了单模型提示和传统的多LLM方法，并显著减少了推理时间和计算成本。分析表明，融合不同的角色向量可以有效控制生成互补方面并提高可解释性。

Conclusion: BILLY提供了一种在单个模型中实现多LLM协作创造力的有效且经济的方法，同时提供了对生成过程的更好控制和可解释性。

Abstract: Multi-LLM systems enhance the creativity of large language models by
simulating human collective intelligence but suffer from significant drawbacks,
such as high computational costs and inference latency. To address these
limitations, we propose BILLY (BlendIng persona vectors for Large Language
model creativitY), a training-free framework that captures the benefits of
multi-LLM collaboration, i.e. inducing diverse perspectives and specialized
expertise, within a single model. BILLY operates by extracting and blending
multiple distinct persona vectors directly in the model's activation space. We
steer the model's generation process with this merged vector while inference,
enabling multi-perspective output without explicit multi-LLM communication. Our
experiments across creativity-oriented benchmarks demonstrate that BILLY
surpasses single model prompting and traditional multi-LLM approaches, while
substantially reducing inference time and computational costs. Our analyses
further reveal that distinct persona vectors can be blended to achieve both
effective control over complementary aspects of generation and greater
interpretability.

</details>


### [336] [BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data](https://arxiv.org/abs/2510.10159)
*Jaap Jumelet,Abdellah Fourtassi,Akari Haga,Bastian Bunzeck,Bhargav Shandilya,Diana Galvan-Sosa,Faiz Ghifari Haznitrama,Francesca Padovani,Francois Meyer,Hai Hu,Julen Etxaniz,Laurent Prévot,Linyang He,María Grandury,Mila Marcheva,Negar Foroutan,Nikitas Theodoropoulos,Pouya Sadeghi,Siyuan Song,Suchir Salhan,Susana Zhou,Yurii Paniv,Ziyin Zhang,Arianna Bisazza,Alex Warstadt,Leshem Choshen*

Main category: cs.CL

TL;DR: BabyBabelLM是一个多语言数据集集合，模拟个人从出生到习得母语所观察到的语言，旨在促进多语言预训练和认知建模。


<details>
  <summary>Details</summary>
Motivation: 研究动机是需要开发上合理的预训练数据，以模拟一个人从出生到习得母语期间所接触的语言。

Method: 研究方法是策划多语言数据集，覆盖45种语言，每种语言的内容相当于1亿英文单词，并编译评估套件，以及训练每种语言的基线模型。

Result: 结果是创建了BabyBabelLM，这是一个多语言数据集集合，包含了45种语言的开发上合理预训练数据、评估套件和基线模型。

Conclusion: BabyBabelLM旨在促进多语言预训练和认知建模研究。

Abstract: We present BabyBabelLM, a multilingual collection of datasets modeling the
language a person observes from birth until they acquire a native language. We
curate developmentally plausible pretraining data aiming to cover the
equivalent of 100M English words of content in each of 45 languages. We compile
evaluation suites and train baseline models in each language. BabyBabelLM aims
to facilitate multilingual pretraining and cognitive modeling.

</details>


### [337] [Large Language Model Sourcing: A Survey](https://arxiv.org/abs/2510.10161)
*Liang Pang,Kangxi Wu,Sunhao Dai,Zihao Wei,Zenghao Duan,Jia Gu,Xiang Li,Zhiyi Yin,Jun Xu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本综述系统性地研究了大型语言模型（LLMs）生成内容的溯源追踪，提出了一个围绕模型和数据视角的四维度框架，并分类了溯源方法，以增强LLMs的透明度、可信赖性和问责制。


<details>
  <summary>Details</summary>
Motivation: LLMs的快速发展使其应用从客观任务转向主观决策，但其黑盒特性和类人内容带来了幻觉、偏见、不公平和版权侵犯等多方面风险。因此，从多角度溯源信息对于提高LLMs的透明度、问责制和可信赖性至关重要。

Method: 本研究通过系统性调查，将LLMs生成内容的溯源追踪分为四个相互关联的维度：模型溯源（区分LLM生成与人类创作）、模型结构溯源（分析内部生成机制）、训练数据溯源（追溯到模型的训练数据）和外部数据溯源（识别外部支持信息）。此外，还提出了一个双范式分类法，将现有溯源方法分为基于先验（主动嵌入可追溯性）和基于后验（回顾性推断）的方法。

Result: 本综述提出了一个全面的LLMs内容溯源追踪框架，涵盖了模型和数据两个主要视角下的四个细分维度，并引入了一个双范式分类法来组织和理解现有的溯源方法。该框架有助于系统性地分析和解决LLMs内容溯源问题。

Conclusion: 通过在这些维度上实现可追溯性，可以显著增强LLMs在实际应用中的透明度、问责制和可信赖性，从而有效应对其部署所带来的多重风险。

Abstract: The rapid advancement of large language models (LLMs) has revolutionized
artificial intelligence, shifting from supporting objective tasks (e.g.,
recognition) to empowering subjective decision-making (e.g., planning,
decision). This marks the dawn of general and powerful AI, with applications
spanning a wide range of fields, including programming, education, healthcare,
finance, and law. However, their deployment introduces multifaceted risks. Due
to the black-box nature of LLMs and the human-like quality of their generated
content, issues such as hallucinations, bias, unfairness, and copyright
infringement become particularly significant. In this context, sourcing
information from multiple perspectives is essential.
  This survey presents a systematic investigation into provenance tracking for
content generated by LLMs, organized around four interrelated dimensions that
together capture both model- and data-centric perspectives. From the model
perspective, Model Sourcing treats the model as a whole, aiming to distinguish
content generated by specific LLMs from content authored by humans. Model
Structure Sourcing delves into the internal generative mechanisms, analyzing
architectural components that shape the outputs of model. From the data
perspective, Training Data Sourcing focuses on internal attribution, tracing
the origins of generated content back to the training data of model. In
contrast, External Data Sourcing emphasizes external validation, identifying
external information used to support or influence the responses of model.
Moreover, we also propose a dual-paradigm taxonomy that classifies existing
sourcing methods into prior-based (proactive traceability embedding) and
posterior-based (retrospective inference) approaches. Traceability across these
dimensions enhances the transparency, accountability, and trustworthiness of
LLMs deployment in real-world applications.

</details>


### [338] [A Survey of Inductive Reasoning for Large Language Models](https://arxiv.org/abs/2510.10182)
*Kedi Chen,Dezhao Ruan,Yuhao Dan,Yaoting Wang,Siyu Yan,Xuecheng Wu,Yinqi Zhang,Qin Chen,Jie Zhou,Liang He,Biqing Qi,Linyang Li,Qipeng Guo,Xiaoming Shi,Wei Zhang*

Main category: cs.CL

TL;DR: 本文首次全面综述了大型语言模型（LLMs）的归纳推理能力，涵盖了改进方法、基准测试、评估策略，并分析了其能力来源。


<details>
  <summary>Details</summary>
Motivation: 归纳推理对LLMs的知识泛化和认知对齐至关重要，但目前缺乏对其的系统性总结。

Method: 本文首先将改进LLMs归纳推理能力的方法分为后训练、测试时扩展和数据增强三类。其次，总结了当前的归纳推理基准，并提出了一种基于沙盒的统一评估方法，引入了观察覆盖率指标。最后，分析了归纳能力的来源以及简单模型架构和数据如何帮助归纳任务。

Result: 本研究提供了一个系统的归纳推理改进方法分类，总结了现有基准，提出了一种统一的沙盒评估方法及观察覆盖率指标，并分析了归纳能力的来源和数据/模型架构的影响。

Conclusion: 本综述为未来LLMs在归纳推理领域的研究奠定了坚实的基础。

Abstract: Reasoning is an important task for large language models (LLMs). Among all
the reasoning paradigms, inductive reasoning is one of the fundamental types,
which is characterized by its particular-to-general thinking process and the
non-uniqueness of its answers. The inductive mode is crucial for knowledge
generalization and aligns better with human cognition, so it is a fundamental
mode of learning, hence attracting increasing interest. Despite the importance
of inductive reasoning, there is no systematic summary of it. Therefore, this
paper presents the first comprehensive survey of inductive reasoning for LLMs.
First, methods for improving inductive reasoning are categorized into three
main areas: post-training, test-time scaling, and data augmentation. Then,
current benchmarks of inductive reasoning are summarized, and a unified
sandbox-based evaluation approach with the observation coverage metric is
derived. Finally, we offer some analyses regarding the source of inductive
ability and how simple model architectures and data help with inductive tasks,
providing a solid foundation for future research.

</details>


### [339] [MedAgentAudit: Diagnosing and Quantifying Collaborative Failure Modes in Medical Multi-Agent Systems](https://arxiv.org/abs/2510.10185)
*Lei Gu,Yinghao Zhu,Haoran Sang,Zixiang Wang,Dehao Sui,Wen Tang,Ewen Harrison,Junyi Gao,Lequan Yu,Liantao Ma*

Main category: cs.CL

TL;DR: 本研究指出，基于LLM的多智能体医疗系统仅关注最终答案准确性不足以建立信任，其内部协作过程存在关键故障模式，亟需透明和可审计的推理过程。


<details>
  <summary>Details</summary>
Motivation: 当前对基于LLM的多智能体医疗系统评估仅限于最终答案准确性，将内部协作视为“黑箱”，忽略了诊断结论是否通过合理且可验证的推理路径得出。这种不透明性在医疗应用中构成重大风险，可能导致有缺陷或不可信的结论。

Method: 通过对六个医学数据集和六个代表性多智能体框架的3,600个案例进行大规模实证研究，采用结合定性分析和定量审计的混合方法，开发了协作故障模式的综合分类体系。

Result: 定量审计揭示了四种主要的故障模式：由共享模型缺陷驱动的错误共识、正确少数意见被压制、讨论动态无效以及综合过程中关键信息丢失。这表明高准确性本身不足以衡量临床或公众信任。

Conclusion: 高准确性不足以建立临床或公众信任。医疗AI的负责任开发和部署迫切需要透明和可审计的推理过程，这是其基石。

Abstract: While large language model (LLM)-based multi-agent systems show promise in
simulating medical consultations, their evaluation is often confined to
final-answer accuracy. This practice treats their internal collaborative
processes as opaque "black boxes" and overlooks a critical question: is a
diagnostic conclusion reached through a sound and verifiable reasoning pathway?
The inscrutable nature of these systems poses a significant risk in high-stakes
medical applications, potentially leading to flawed or untrustworthy
conclusions. To address this, we conduct a large-scale empirical study of 3,600
cases from six medical datasets and six representative multi-agent frameworks.
Through a rigorous, mixed-methods approach combining qualitative analysis with
quantitative auditing, we develop a comprehensive taxonomy of collaborative
failure modes. Our quantitative audit reveals four dominant failure patterns:
flawed consensus driven by shared model deficiencies, suppression of correct
minority opinions, ineffective discussion dynamics, and critical information
loss during synthesis. This study demonstrates that high accuracy alone is an
insufficient measure of clinical or public trust. It highlights the urgent need
for transparent and auditable reasoning processes, a cornerstone for the
responsible development and deployment of medical AI.

</details>


### [340] [Weed Out, Then Harvest: Dual Low-Rank Adaptation is an Effective Noisy Label Detector for Noise-Robust Learning](https://arxiv.org/abs/2510.10208)
*Bo Yuan,Yulin Chen,Yin Zhang*

Main category: cs.CL

TL;DR: Delora 提出了一种新颖的框架，通过引入干净和嘈杂的 LoRA 来解耦样本选择和模型训练，从而有效地在嘈杂标签下微调参数高效的大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调（PEFT）大型语言模型在各种任务中表现出色，但在实际应用中训练数据不可避免地包含嘈杂标签。现有解决方案（基于小损失选择样本）会形成恶性循环，导致次优性能，因此需要一种更有效的处理嘈杂标签的方法。

Method: Delora 框架将样本选择与模型训练解耦。在样本选择阶段，它通过引入干净 LoRA（记忆干净数据）和嘈杂 LoRA（记忆错误标签数据）来建立一个嘈杂标签检测器，其中嘈杂 LoRA 作为选择干净和嘈杂样本的可学习阈值。在模型训练阶段，Delora 利用这些精心选择的样本无缝地微调语言模型。

Result: 在合成和真实世界的嘈杂数据集上的实验结果表明，Delora 在嘈杂标签检测和文本分类方面均表现出有效性。

Conclusion: Delora 通过解耦样本选择和模型训练，并利用干净/嘈杂 LoRA 进行嘈杂标签检测，为在嘈杂标签下微调参数高效的大型语言模型提供了一个有效的解决方案。

Abstract: Parameter-efficient fine-tuning (PEFT) large language models (LLMs) have
shown impressive performance in various downstream tasks. However, in many
real-world scenarios, the collected training data inevitably contains noisy
labels. To learn from noisy labels, most solutions select samples with small
losses for model training. However, the selected samples, in turn, impact the
loss computation in the next iteration. An inaccurate initial selection can
create a vicious cycle, leading to suboptimal performance. To break this cycle,
we propose Delora, a novel framework that decouples the sample selection from
model training. For sample selection, Delora establishes a noisy label detector
by introducing clean and noisy LoRA. Benefiting from the memory effect, the
clean LoRA is encouraged to memorize clean data, while the noisy LoRA is
constrained to memorize mislabeled data, which serves as a learnable threshold
for selecting clean and noisy samples. For model training, Delora can use
carefully selected samples to fine-tune language models seamlessly.
Experimental results on synthetic and real-world noisy datasets demonstrate the
effectiveness of Delora in noisy label detection and text classification.

</details>


### [341] [You only need 4 extra tokens: Synergistic Test-time Adaptation for LLMs](https://arxiv.org/abs/2510.10223)
*Yijie Xu,Huizai Yao,Zhiyu Guo,Weiyu Guo,Pengteng Li,Aiwei Liu,Xuming Hu,Hui Xiong*

Main category: cs.CL

TL;DR: 本文提出SyTTA，一个无需标签的测试时自适应框架，通过结合输入侧困惑度和输出侧预测熵两种不确定性信号，实时调整大型语言模型以应对领域分布偏移，显著提升了模型在专业领域（如农业问答）的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在金融、医疗、农业等专业领域部署时，面临与训练数据显著的分布偏移。尽管领域特定微调能缓解此问题，但其依赖高质量的标注数据，而在专业知识受限的场景中，收集这些数据既昂贵又耗时。这促使研究人员探索无需标签的测试时自适应方法。

Method: SyTTA是一个推理时框架，它在不额外监督的情况下实时自适应模型。它结合了两种互补的、在分布偏移下产生的不确定性信号：输入侧困惑度（指示与领域特定术语和模式不匹配）和输出侧预测熵（指示生成过程中弥散和不稳定的token概率）。

Result: SyTTA在不同的模型架构和领域特定基准测试中均取得了持续的性能提升。值得注意的是，在农业问答任务中，SyTTA使Qwen-2.5-7B模型的Rouge-LSum指标提高了120%以上，且每个查询仅需额外4个token。

Conclusion: 这些结果表明，无需标注样本也能实现语言模型有效的测试时自适应，从而支持在标签稀缺的领域部署模型。

Abstract: Large language models (LLMs) are increasingly deployed in specialized domains
such as finance, medicine, and agriculture, where they face significant
distribution shifts from their training data. Domain-specific fine-tuning can
mitigate this challenge but relies on high-quality labeled data that is
expensive and slow to collect in expertise-limited settings. We study
label-free test-time adaptation for language models and present SyTTA, an
inference-time framework that adapts models on-the-fly without additional
supervision. SyTTA couples two complementary uncertainty signals that arise
under distribution shift: input-side perplexity, indicating mismatch with
domain-specific terminology and patterns, and output-side predictive entropy,
indicating diffuse and unstable token probabilities during generation. Across
diverse model architectures and domain-specific benchmarks, SyTTA delivers
consistent gains. Notably, on agricultural question answering, SyTTA improves
Rouge-LSum by over 120% on Qwen-2.5-7B with only 4 extra tokens per query.
These results show that effective test-time adaptation for language models is
achievable without labeled examples, supporting deployment in label-scarce
domains. The code will be made available upon acceptance.

</details>


### [342] [Text2Token: Unsupervised Text Representation Learning with Token Target Prediction](https://arxiv.org/abs/2510.10224)
*Ruize An,Richong Zhang,Zhijie Nie,Zhanyu Wu,Yanzhao Zhang,Dingkun Long*

Main category: cs.CL

TL;DR: 本文提出了一种名为Text2Token的无监督生成式文本表示学习框架，通过令牌目标预测任务和精心构建的令牌目标分布，实现了与先进无监督对比学习嵌入器相当的性能。


<details>
  <summary>Details</summary>
Motivation: 无监督文本表示学习是自然语言处理中的基础任务，有助于改进搜索和推荐。近期研究发现高质量表示与输入文本的关键令牌对齐，揭示了表示空间和词汇空间之间的潜在联系。受此启发，本文旨在开发一种新的生成式无监督文本表示学习方法。

Method: 本文开发了一个名为Text2Token的无监督生成式框架，基于令牌目标预测任务，并利用精心构建的目标令牌分布作为监督信号。为构建高质量的目标令牌分布，作者分析了高级嵌入器的令牌对齐特性，并识别出两类关键令牌：文本中有意义的令牌和文本外语义派生的令牌。在此基础上，提出了数据驱动和模型派生两种方法，从数据或LLM骨干网络构建合成令牌目标。

Result: 在MTEB v2基准测试中，Text2Token的性能与最先进的无监督对比学习嵌入器LLM2Vec相当。此外，分析表明词汇空间和表示空间在训练过程中共同优化并趋向最优解。

Conclusion: Text2Token提供了一种新的无监督生成式文本表示学习框架，实现了有竞争力的性能，并揭示了词汇空间和表示空间协同优化的新见解，为未来的研究提供了新的思路和方向。

Abstract: Unsupervised text representation learning (TRL) is a fundamental task in
natural language processing, which is beneficial for improving search and
recommendations with the web's unlabeled texts. A recent empirical study finds
that the high-quality representation aligns with the key token of the input
text, uncovering the potential connection between representation space and
vocabulary space. Inspired by the findings, we revisit the generative tasks and
develop an unsupervised generative framework for TRL, Text2Token. The framework
is based on the token target prediction task, utilizing carefully constructed
target token distribution as supervisory signals. To construct the high-quality
target token distribution, we analyze the token-alignment properties with
advanced embedders and identify two essential categories of key tokens: (1) the
meaningful tokens in the text and (2) semantically derived tokens beyond the
text. Based on these insights, we propose two methods -- data-driven and
model-derived -- to construct synthetic token targets from data or the LLM
backbone. Experiments on the MTEB v2 benchmark demonstrate that Text2Token
achieves performance competitive with the state-of-the-art embedder with
unsupervised contrastive learning, LLM2Vec. Our analysis further shows that
vocabulary and representation spaces optimize together and toward the optimum
solution during training, providing new ideas and insights for future work.

</details>


### [343] [ImCoref-CeS: An Improved Lightweight Pipeline for Coreference Resolution with LLM-based Checker-Splitter Refinement](https://arxiv.org/abs/2510.10241)
*Kangyang Luo,Yuzhuo Bai,Shuzheng Si,Cheng Gao,Zhitong Wang,Yingli Shen,Wenhao Li,Zhu Liu,Yufeng Han,Jiayi Wu,Cunliang Kong,Maosong Sun*

Main category: cs.CL

TL;DR: 本文提出ImCoref-CeS框架，有效结合增强型有监督神经模型（ImCoref）与大型语言模型（LLM）的推理能力，用于共指消解任务，并取得了超越现有SOTA方法的性能。


<details>
  <summary>Details</summary>
Motivation: 共指消解领域面临一个核心困境：是继续深挖基于小型语言模型的有监督神经方法的潜力（其“检测-聚类”流程仍表现出色），还是拥抱大型语言模型（LLMs）的强大能力。然而，如何有效结合两者的优势仍未被充分探索。

Method: 本文提出ImCoref-CeS框架。首先，ImCoref是一个改进的共指消解方法，通过引入轻量级桥接模块增强长文本编码能力，设计双仿射评分器全面捕捉位置信息，并采用混合提及正则化提高训练效率，以提升有监督神经方法的性能。其次，ImCoref-CeS利用LLM作为多角色“检查器-分割器”代理，对ImCoref预测的候选提及（过滤无效提及）和共指结果（拆分错误簇）进行验证。

Result: 广泛的实验证明了ImCoref-CeS的有效性，其性能优于现有最先进（SOTA）方法。

Conclusion: ImCoref-CeS框架成功地将增强型有监督模型与LLM的推理能力相结合，在共指消解任务中实现了卓越的性能，并为该领域提供了新的SOTA方法。

Abstract: Coreference Resolution (CR) is a critical task in Natural Language Processing
(NLP). Current research faces a key dilemma: whether to further explore the
potential of supervised neural methods based on small language models, whose
detect-then-cluster pipeline still delivers top performance, or embrace the
powerful capabilities of Large Language Models (LLMs). However, effectively
combining their strengths remains underexplored. To this end, we propose
\textbf{ImCoref-CeS}, a novel framework that integrates an enhanced supervised
model with LLM-based reasoning. First, we present an improved CR method
(\textbf{ImCoref}) to push the performance boundaries of the supervised neural
method by introducing a lightweight bridging module to enhance long-text
encoding capability, devising a biaffine scorer to comprehensively capture
positional information, and invoking a hybrid mention regularization to improve
training efficiency. Importantly, we employ an LLM acting as a multi-role
Checker-Splitter agent to validate candidate mentions (filtering out invalid
ones) and coreference results (splitting erroneous clusters) predicted by
ImCoref. Extensive experiments demonstrate the effectiveness of ImCoref-CeS,
which achieves superior performance compared to existing state-of-the-art
(SOTA) methods.

</details>


### [344] [Audit-of-Understanding: Posterior-Constrained Inference for Mathematical Reasoning in Language Models](https://arxiv.org/abs/2510.10252)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 本文提出了一种名为“理解审计”(AoU)的框架，通过在推理前验证假设来防止大型语言模型(LLM)产生基于未经证实前提的推理幻觉，从而显著提高了模型在数学推理任务上的准确性和忠实度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)常生成看似连贯但基于未经证实假设的推理链，导致幻觉结论。现有工作主要关注事实性幻觉或依赖事后验证，未能有效解决推理引起的幻觉问题。

Method: 本文提出了“理解审计”(AoU)框架，这是一种后验约束推理方法。它包含三个阶段：(1) 将查询分解为候选假设；(2) 审计这些假设的支持度；(3) 仅基于经过验证的假设进行推理。该方法在形式上与选择性预测和拒绝学习相关，并提供了完美验证下的理论保证、不完美审计下的超额风险界限以及可处理性分析。

Result: 实验结果表明，AoU显著提高了GSM8K、MultiArith和SVAMP等数据集的准确性和忠实度。与Chain-of-Thought、Self-Consistency和CoT-Decoding等方法相比，AoU在GSM8K上实现了高达+30%的提升，在MultiArith上实现了+45%的提升，并在SVAMP上持续实现了+20-28%的改进。

Conclusion: AoU框架通过将推理限制在经过验证的前提上，有效解决了大型语言模型中推理引起的幻觉问题。它不仅提供了理论上的保障，还在多个数学推理基准测试中取得了显著的实证性能提升，提高了模型的准确性和推理忠实度。

Abstract: Large language models (LLMs) often generate reasoning traces that appear
coherent but rest on unsupported assumptions, leading to hallucinated
conclusions. Prior work mainly addresses factual hallucinations or relies on
post-hoc verification, leaving reasoning-induced hallucinations largely
unaddressed. We propose Audit-of-Understanding (AoU), a framework that
constrains inference to validated premises through three phases: (1)
decomposing a query into candidate assumptions, (2) auditing their support, and
(3) conditioning inference only on the validated subset. Formally, AoU is
\emph{posterior-constrained inference}, connecting to selective prediction and
rejection learning. Our contributions are threefold: (i) theoretical guarantees
under perfect validation, (ii) excess-risk bounds under imperfect audits, and
(iii) tractability analysis. Empirically, AoU improves both accuracy and
faithfulness on GSM8K, MultiArith, and SVAMP, achieving up to +30% gains on
GSM8K, +45% on MultiArith, and consistent +20--28% improvements on SVAMP over
Chain-of-Thought, Self-Consistency, and CoT-Decoding. Code is available at
https://anonymous.4open.science/r/audit-of-understanding-E28B.

</details>


### [345] [Backdoor Collapse: Eliminating Unknown Threats via Known Backdoor Aggregation in Language Models](https://arxiv.org/abs/2510.10265)
*Liang Lin,Miao Yu,Moayad Aloqaily,Zhenhong Zhou,Kun Wang,Linsey Pang,Prakhar Mehrotra,Qingsong Wen*

Main category: cs.CL

TL;DR: 本文提出了一种名为 \ourmethod 的防御框架，旨在无需预知触发器设置的情况下，有效抵御大型语言模型（LLMs）中的后门攻击，通过注入已知后门聚合表示并进行恢复微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）面临后门攻击的严重威胁，这些攻击常通过公共检查点嵌入。然而，现有的防御方法依赖于对触发器设置不切实际的假设，这促使研究者寻求一种无需预知触发器设置的通用防御机制。

Method: 本文提出的 \ourmethod 框架基于一个关键观察：当向已被攻陷的模型中故意注入已知后门时，现有未知后门和新注入后门在表示空间中会聚集。\ourmethod 利用这一特性，通过两阶段过程实现防御：首先，通过注入已知触发器来聚合后门表示；然后，执行恢复微调以恢复良性输出。

Result: 实验结果表明：(I) \ourmethod 将多基准测试上的平均攻击成功率（ASR）降低至 4.41%，优于现有基线 28.1% 至 69.3%。(II) 清洁准确性和实用性保持在原始模型的 0.5% 以内，对合法任务的影响可忽略不计。(III) 该防御方法对不同类型的后门具有泛化性，证实了其在实际部署场景中的鲁棒性。

Conclusion: \ourmethod 提供了一种无需预知触发器设置的有效且鲁棒的后门防御框架，显著降低了大型语言模型的攻击成功率，同时保持了模型的清洁准确性和实用性，并能泛化到不同后门类型，为LLMs的实际部署提供了重要的安全保障。

Abstract: Backdoor attacks are a significant threat to large language models (LLMs),
often embedded via public checkpoints, yet existing defenses rely on
impractical assumptions about trigger settings. To address this challenge, we
propose \ourmethod, a defense framework that requires no prior knowledge of
trigger settings. \ourmethod is based on the key observation that when
deliberately injecting known backdoors into an already-compromised model, both
existing unknown and newly injected backdoors aggregate in the representation
space. \ourmethod leverages this through a two-stage process: \textbf{first},
aggregating backdoor representations by injecting known triggers, and
\textbf{then}, performing recovery fine-tuning to restore benign outputs.
Extensive experiments across multiple LLM architectures demonstrate that: (I)
\ourmethod reduces the average Attack Success Rate to 4.41\% across multiple
benchmarks, outperforming existing baselines by 28.1\%$\sim$69.3\%$\uparrow$.
(II) Clean accuracy and utility are preserved within 0.5\% of the original
model, ensuring negligible impact on legitimate tasks. (III) The defense
generalizes across different types of backdoors, confirming its robustness in
practical deployment scenarios.

</details>


### [346] [MatryoshkaThinking: Recursive Test-Time Scaling Enables Efficient Reasoning](https://arxiv.org/abs/2510.10293)
*Hongwei Chen,Yishu Lei,Dan Zhang,Bo Ke,Danxiang Zhu,Xuyi Chen,Yuxiang Lu,Zhengjie Huang,Shikun Feng,Jingzhou He,Yu Sun,Hua Wu,Haifeng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种名为 MatryoshkaThinking 的新型测试时扩展方法，它在语言模型推理过程中显著降低了计算成本，同时保持了最先进的性能，解决了现有方法计算开销大的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时扩展方法（如 DeepConf）虽然能提高模型性能，但通常需要巨大的计算开销才能取得有竞争力的结果。研究动机在于寻找一种能显著降低计算成本同时保持高性能的方法。

Method: MatryoshkaThinking 方法的核心在于递归地利用模型固有的推理、验证和总结能力，以增强正确解决方案的保留并减少 Pass@k 和 Pass@1 之间的差异。

Result: MatryoshkaThinking 在 AIME2025 上取得了 99.79 的分数，而计算量仅为 DeepConf 的 4%。该方法在多个开源模型和具有挑战性的多模态推理基准上得到了全面评估和验证，显示出其有效性和通用性。

Conclusion: 研究结果为设计高效且可扩展的先进语言模型测试时推理策略提供了新见解。

Abstract: Test-time scaling has emerged as a promising paradigm in language modeling,
wherein additional computational resources are allocated during inference to
enhance model performance. Recent approaches, such as DeepConf, have
demonstrated the efficacy of this strategy, however, they often incur
substantial computational overhead to achieve competitive results. In this
work, we propose MatryoshkaThinking, a novel method that significantly reduces
computational cost while maintaining state-of-the-art performance.
Specifically, MatryoshkaThinking attains a score of 99.79 on AIME2025 using
only 4% of the computation required by DeepConf. The core of our approach lies
in the recursive exploitation of the model's intrinsic capabilities in
reasoning, verification, and summarization, which collectively enhance the
retention of correct solutions and reduce the disparity between Pass@k and
Pass@1. Comprehensive evaluations across multiple open-source models and
challenging multi-modal reasoning benchmarks validate the effectiveness and
generality of our method. These findings offer new insights into the design of
efficient and scalable test-time inference strategies for advanced language
models.

</details>


### [347] [On the Entity-Level Alignment in Crosslingual Consistency](https://arxiv.org/abs/2510.10280)
*Yihong Liu,Mingyang Wang,François Yvon,Hinrich Schütze*

Main category: cs.CL

TL;DR: 本文探究了多语言大型语言模型（LLMs）跨语言事实知识回忆不一致的原因，发现实体对齐失败是主要因素。研究提出了SubSub和SubInj两种方法，通过将英文主语翻译整合到多语言提示中，显著提高了事实回忆的准确性和一致性，并揭示了其通过模型内部枢轴语言处理强化实体对齐的机制。


<details>
  <summary>Details</summary>
Motivation: 多语言LLMs应在不同语言中一致地回忆事实知识，但这种跨语言一致性常失败，其原因尚不清楚。研究假设这些不一致可能源于实体对齐失败，即无法将主体和客体实体映射到跨语言的共享概念空间中。

Method: 研究通过实体级别（主语和宾语）的翻译任务评估实体对齐情况。在此基础上，提出了SubSub和SubInj两种方法，将主语的英文翻译整合到跨语言提示中。最后，进行了机制分析，以理解这些干预措施如何起作用。

Result: 研究发现，事实回忆的一致性与所有研究模型中的实体对齐情况密切相关，主语或宾语的错位经常导致不一致。提出的SubSub和SubInj方法在事实回忆准确性和一致性方面均取得了显著提升。机制分析表明，这些干预措施通过模型的内部枢轴语言处理，强化了概念空间中的实体表示对齐。

Conclusion: 实体对齐是多语言LLMs实现跨语言事实一致性的关键因素。SubSub和SubInj是有效且实用的策略，通过强化实体对齐，可以显著提高多语言事实预测的准确性和一致性。

Abstract: Multilingual large language models (LLMs) are expected to recall factual
knowledge consistently across languages. However, the factors that give rise to
such crosslingual consistency -- and its frequent failure -- remain poorly
understood. In this work, we hypothesize that these inconsistencies may arise
from failures in entity alignment, the process of mapping subject and object
entities into a shared conceptual space across languages. To test this, we
assess alignment through entity-level (subject and object) translation tasks,
and find that consistency is strongly correlated with alignment across all
studied models, with misalignment of subjects or objects frequently resulting
in inconsistencies. Building on this insight, we propose SubSub and SubInj, two
effective methods that integrate English translations of subjects into prompts
across languages, leading to substantial gains in both factual recall accuracy
and consistency. Finally, our mechanistic analysis reveals that these
interventions reinforce the entity representation alignment in the conceptual
space through model's internal pivot-language processing, offering effective
and practical strategies for improving multilingual factual prediction.

</details>


### [348] [Are LLMs Empathetic to All? Investigating the Influence of Multi-Demographic Personas on a Model's Empathy](https://arxiv.org/abs/2510.10328)
*Ananya Malik,Nazanin Sabri,Melissa Karnaze,Mai Elsherief*

Main category: cs.CL

TL;DR: 本研究调查了大型语言模型（LLMs）在不同人口统计学和文化背景的用户群体中，其共情能力是否公平，发现其共情反应受属性影响显著，并存在某些群体的偏差。


<details>
  <summary>Details</summary>
Motivation: LLMs的自然对话能力得益于其共情理解和响应，但共情体验受人口统计学和文化背景影响。因此，研究旨在探讨LLMs能否对不同用户群体表现出公平的共情。

Method: 研究提出了一个框架来调查LLMs的认知和情感共情如何因交叉人口统计学属性定义的用户角色而异。通过结合年龄、文化和性别，构建了315个独特的交叉角色，并在四种LLMs上进行了新颖的交叉分析。研究结合了定量发现和定性见解。

Result: 结果显示，用户属性深刻地影响了模型的共情反应。有趣的是，同时添加多个属性可以减弱和逆转预期的共情模式。LLMs的共情表现大致反映了现实世界的共情趋势，但在某些群体（例如儒家文化背景的群体）中存在显著的偏差。

Conclusion: 研究强调了设计能够考虑人口多样性的共情感知LLMs的重要性，以促进更具包容性和公平性的模型行为。

Abstract: Large Language Models' (LLMs) ability to converse naturally is empowered by
their ability to empathetically understand and respond to their users. However,
emotional experiences are shaped by demographic and cultural contexts. This
raises an important question: Can LLMs demonstrate equitable empathy across
diverse user groups? We propose a framework to investigate how LLMs' cognitive
and affective empathy vary across user personas defined by intersecting
demographic attributes. Our study introduces a novel intersectional analysis
spanning 315 unique personas, constructed from combinations of age, culture,
and gender, across four LLMs. Results show that attributes profoundly shape a
model's empathetic responses. Interestingly, we see that adding multiple
attributes at once can attenuate and reverse expected empathy patterns. We show
that they broadly reflect real-world empathetic trends, with notable
misalignments for certain groups, such as those from Confucian culture. We
complement our quantitative findings with qualitative insights to uncover model
behaviour patterns across different demographic groups. Our findings highlight
the importance of designing empathy-aware LLMs that account for demographic
diversity to promote more inclusive and equitable model behaviour.

</details>


### [349] [End-to-end Automatic Speech Recognition and Speech Translation: Integration of Speech Foundational Models and LLMs](https://arxiv.org/abs/2510.10329)
*Nam Luu,Ondřej Bojar*

Main category: cs.CL

TL;DR: 本文提出了一种结合预训练语音编码器和大型语言模型的端到端架构，用于同时进行自动语音识别和语音翻译，在英德语对上表现优于SeamlessM4T，并媲美级联系统。


<details>
  <summary>Details</summary>
Motivation: 语音翻译（ST）任务存在传统级联和新兴端到端两种方法。本文旨在探索一种结合预训练语音编码器和大型语言模型的端到端架构，以同时执行自动语音识别（ASR）和语音翻译（ST），可能旨在提高性能或效率。

Method: 研究采用了一种结合预训练语音编码器和大型语言模型（LLMs）的端到端架构，用于同时执行自动语音识别（ASR）和语音翻译（ST）。

Result: 在英德语对的实验中，最佳模型不仅取得了优于大型基础端到端多模态翻译模型SeamlessM4T的翻译结果，而且在$\text{COMET}^{\text{DA}}_{22}$指标上实现了高达8%的得分提升，性能与结合Whisper和NLLB的级联系统相当。

Conclusion: 所提出的结合预训练语音编码器和大型语言模型的端到端架构在语音翻译任务中表现出色，能够超越先进的端到端模型，并达到甚至超越强大的级联系统的性能。

Abstract: Speech Translation (ST) is a machine translation task that involves
converting speech signals from one language to the corresponding text in
another language; this task has two different approaches, namely the
traditional cascade and the more recent end-to-end. This paper explores a
combined end-to-end architecture of pre-trained speech encoders and Large
Language Models (LLMs) for performing both Automatic Speech Recognition (ASR)
and ST simultaneously. Experiments with the English-to-German language pair
show that our best model not only can achieve better translation results than
SeamlessM4T, a large foundational end-to-end, multi-modal translation model,
but can also match the performance of a cascaded system with Whisper and NLLB,
with up to a score gain of 8% in $\text{COMET}^{\text{DA}}_{22}$ metric.

</details>


### [350] [ASC analyzer: A Python package for measuring argument structure construction usage in English texts](https://arxiv.org/abs/2510.10384)
*Hakyung Sung,Kristopher Kyle*

Main category: cs.CL

TL;DR: 本文介绍了一个名为“ASC分析器”的Python包，用于自动标记和测量第二语言（L2）中的论元结构构式（ASCs）使用情况，并通过分析其与L2写作分数的关系来展示其效用。


<details>
  <summary>Details</summary>
Motivation: 现有用于衡量第二语言（L2）能力中论元结构构式（ASCs）使用的工具缺乏可扩展性和系统性。

Method: 开发了一个公开可用的Python包——ASC分析器，该工具能自动标记ASCs并计算50个指标，包括多样性、比例、频率和ASC-动词词元关联强度。通过双变量和多变量分析来检验基于ASC的指标与L2写作分数之间的关系。

Result: 通过双变量和多变量分析，展示了ASC分析器的实用性，并考察了基于ASC的指标与L2写作分数之间的关系。

Conclusion: ASC分析器提供了一个理论基础的、可扩展且系统化的工具，用于分析L2熟练度中的论元结构构式使用情况。

Abstract: Argument structure constructions (ASCs) offer a theoretically grounded lens
for analyzing second language (L2) proficiency, yet scalable and systematic
tools for measuring their usage remain limited. This paper introduces the ASC
analyzer, a publicly available Python package designed to address this gap. The
analyzer automatically tags ASCs and computes 50 indices that capture
diversity, proportion, frequency, and ASC-verb lemma association strength. To
demonstrate its utility, we conduct both bivariate and multivariate analyses
that examine the relationship between ASC-based indices and L2 writing scores.

</details>


### [351] [RefusalBench: Generative Evaluation of Selective Refusal in Grounded Language Models](https://arxiv.org/abs/2510.10390)
*Aashiq Muhamed,Leonardo F. R. Ribeiro,Markus Dreyer,Virginia Smith,Mona T. Diab*

Main category: cs.CL

TL;DR: RAG系统中语言模型基于错误上下文选择性拒绝回答的能力对安全性至关重要，但仍是一个显著的弱点。本研究发现即使是前沿模型也表现不佳，并引入了RefusalBench，一个生成式评估方法，揭示了系统性失败模式，并指出选择性拒绝是一种可训练的、对齐敏感的能力。


<details>
  <summary>Details</summary>
Motivation: RAG系统中语言模型基于有缺陷的上下文选择性拒绝回答的能力对安全性至关重要，但目前仍是一个显著的失败点。即使是前沿模型在这种情况下也表现不佳，且现有静态基准无法可靠评估此能力，因为模型会利用数据集特有的伪影并记忆测试实例。

Method: 引入了RefusalBench，这是一种生成式方法，通过受控的语言扰动程序化地创建诊断测试用例。该框架采用了176种不同的扰动策略，涵盖六类信息不确定性和三个强度级别。评估了30多个模型，并发布了两个基准——RefusalBench-NQ（单文档）和RefusalBench-GaRAGe（多文档）以及完整的生成框架。

Result: 研究发现，前沿模型在多文档任务中拒绝准确率低于50%，表现出危险的过度自信或过度谨慎。评估揭示了系统性失败模式：拒绝能力包括可分离的检测和分类技能，且模型规模或扩展推理并未改善性能。选择性拒绝是一种可训练的、对齐敏感的能力。

Conclusion: 选择性拒绝是语言模型中一个可训练且对齐敏感的能力，为未来的改进提供了明确的路径。RefusalBench及其生成框架为持续、动态评估这一关键能力提供了工具。

Abstract: The ability of language models in RAG systems to selectively refuse to answer
based on flawed context is critical for safety, yet remains a significant
failure point. Our large-scale study reveals that even frontier models struggle
in this setting, with refusal accuracy dropping below 50% on multi-document
tasks, while exhibiting either dangerous overconfidence or overcaution. Static
benchmarks fail to reliably evaluate this capability, as models exploit
dataset-specific artifacts and memorize test instances. We introduce
RefusalBench, a generative methodology that programmatically creates diagnostic
test cases through controlled linguistic perturbation. Our framework employs
176 distinct perturbation strategies across six categories of informational
uncertainty and three intensity levels. Evaluation of over 30 models uncovers
systematic failure patterns: refusal comprises separable detection and
categorization skills, and neither scale nor extended reasoning improves
performance. We find that selective refusal is a trainable, alignment-sensitive
capability, offering a clear path for improvement. We release two benchmarks --
RefusalBench-NQ (single document) and RefusalBench-GaRAGe (multi-document) --
and our complete generation framework to enable continued, dynamic evaluation
of this critical capability.

</details>


### [352] [AssoMem: Scalable Memory QA with Multi-Signal Associative Retrieval](https://arxiv.org/abs/2510.10397)
*Kai Zhang,Xinyuan Zhang,Ejaz Ahmed,Hongda Jiang,Caleb Kumar,Kai Sun,Zhaojiang Lin,Sanat Sharma,Shereen Oraby,Aaron Colak,Ahmed Aly,Anuj Kumar,Xiaozhong Liu,Xin Luna Dong*

Main category: cs.CL

TL;DR: AssoMem是一种基于关联记忆图和多维度检索信号融合的新框架，用于提高AI助手在问答任务中的记忆召回准确性，尤其是在语义密集场景下。


<details>
  <summary>Details</summary>
Motivation: 现有AI助手的问答任务在处理大规模记忆时，尤其是在语义密集的场景下，准确召回信息仍是核心挑战。现有方法主要依赖于与查询的语义距离进行检索，效果不佳。

Method: AssoMem框架受人类联想记忆启发，构建一个关联记忆图，将对话语句锚定到自动提取的线索上，提供丰富的对话上下文组织视图并实现重要性感知排序。此外，它通过自适应互信息（MI）驱动的融合策略，整合了相关性、重要性和时间对齐等多维度检索信号。

Result: 在三个基准数据集和一个新引入的MeetingQA数据集上的大量实验表明，AssoMem始终优于最先进的基线方法。

Conclusion: AssoMem在上下文感知记忆召回方面表现出卓越的性能，验证了其在解决大规模记忆召回挑战方面的优越性。

Abstract: Accurate recall from large scale memories remains a core challenge for memory
augmented AI assistants performing question answering (QA), especially in
similarity dense scenarios where existing methods mainly rely on semantic
distance to the query for retrieval. Inspired by how humans link information
associatively, we propose AssoMem, a novel framework constructing an
associative memory graph that anchors dialogue utterances to automatically
extracted clues. This structure provides a rich organizational view of the
conversational context and facilitates importance aware ranking. Further,
AssoMem integrates multi-dimensional retrieval signals-relevance, importance,
and temporal alignment using an adaptive mutual information (MI) driven fusion
strategy. Extensive experiments across three benchmarks and a newly introduced
dataset, MeetingQA, demonstrate that AssoMem consistently outperforms SOTA
baselines, verifying its superiority in context-aware memory recall.

</details>


### [353] [LONGQAEVAL: Designing Reliable Evaluations of Long-Form Clinical QA under Resource Constraints](https://arxiv.org/abs/2510.10415)
*Federica Bologna,Tiffany Pan,Matthew Wilkens,Yue Guo,Lucy Lu Wang*

Main category: cs.CL

TL;DR: 本文提出LongQAEval框架，用于评估长篇临床问答系统，旨在解决资源受限和专业性高的问题。通过比较粗粒度与细粒度评估，发现标注者间一致性因维度而异，并提出小部分句子标注可降低成本。


<details>
  <summary>Details</summary>
Motivation: 评估长篇临床问答（QA）系统资源密集且具有挑战性，需要医学专业知识，并且在长篇文本上实现一致的人工判断很困难。

Method: 引入LongQAEval评估框架和一套评估建议。基于医生对300个由医生和大型语言模型回答的真实患者问题的标注，比较了粗粒度（答案级别）与细粒度（句子级别）评估在正确性、相关性和安全性维度上的表现。

Result: 标注者间一致性（IAA）因维度而异：细粒度标注提高了正确性的一致性，粗粒度标注提高了相关性的一致性，而安全性判断则始终不一致。此外，仅标注一小部分句子即可提供与粗粒度标注相当的可靠性，从而降低成本和精力。

Conclusion: LongQAEval为资源有限和高专业性环境下的长篇临床问答系统评估提供了框架和建议。细粒度标注有助于提高正确性判断的一致性，而小部分句子标注是一种经济高效的评估策略。

Abstract: Evaluating long-form clinical question answering (QA) systems is
resource-intensive and challenging: accurate judgments require medical
expertise and achieving consistent human judgments over long-form text is
difficult. We introduce LongQAEval, an evaluation framework and set of
evaluation recommendations for limited-resource and high-expertise settings.
Based on physician annotations of 300 real patient questions answered by
physicians and LLMs, we compare coarse answer-level versus fine-grained
sentence-level evaluation over the dimensions of correctness, relevance, and
safety. We find that inter-annotator agreement (IAA) varies by dimension:
fine-grained annotation improves agreement on correctness, coarse improves
agreement on relevance, and judgments on safety remain inconsistent.
Additionally, annotating only a small subset of sentences can provide
reliability comparable to coarse annotations, reducing cost and effort.

</details>


### [354] [Do Audio LLMs Really LISTEN, or Just Transcribe? Measuring Lexical vs. Acoustic Emotion Cues Reliance](https://arxiv.org/abs/2510.10444)
*Jingyi Chen,Zhimeng Guo,Jiyun Chun,Pichao Wang,Andrew Perrault,Micha Elsner*

Main category: cs.CL

TL;DR: 当前大型音频语言模型（LALMs）在语音情感理解中过度依赖文本信息，而对声学线索的利用不足，更像是“转录”而非“聆听”。


<details>
  <summary>Details</summary>
Motivation: 虽然语音情感理解需要同时考虑文本和声学线索，但目前尚不清楚大型音频语言模型（LALMs）是真正处理声学信息，还是主要依赖文本内容来理解情感。

Method: 本文提出了LISTEN（Lexical vs. Acoustic Speech Test for Emotion in Narratives），这是一个受控基准测试，旨在区分LALMs对文本依赖和声学敏感性在情感理解中的作用。该研究评估了六个最先进的LALMs。

Result: 评估结果显示，LALMs表现出一致的文本主导性。当文本线索中性或缺失时，模型预测为“中性”；在文本和声学线索一致时，性能提升有限；在线索冲突下，模型无法区分不同的情感。在副语言情境中，模型性能接近随机。

Conclusion: 这些结果表明，当前的LALMs主要“转录”而非“聆听”，严重依赖文本语义，而未能充分利用声学线索。LISTEN为评估多模态模型的情感理解能力提供了一个原则性的框架。

Abstract: Understanding emotion from speech requires sensitivity to both lexical and
acoustic cues. However, it remains unclear whether large audio language models
(LALMs) genuinely process acoustic information or rely primarily on lexical
content. We present LISTEN (Lexical vs. Acoustic Speech Test for Emotion in
Narratives), a controlled benchmark designed to disentangle lexical reliance
from acoustic sensitivity in emotion understanding. Across evaluations of six
state-of-the-art LALMs, we observe a consistent lexical dominance. Models
predict "neutral" when lexical cues are neutral or absent, show limited gains
under cue alignment, and fail to classify distinct emotions under cue conflict.
In paralinguistic settings, performance approaches chance. These results
indicate that current LALMs largely "transcribe" rather than "listen," relying
heavily on lexical semantics while underutilizing acoustic cues. LISTEN offers
a principled framework for assessing emotion understanding in multimodal
models.

</details>


### [355] [RECON: Reasoning with Condensation for Efficient Retrieval-Augmented Generation](https://arxiv.org/abs/2510.10448)
*Zhichao Xu,Minheng Wang,Yawei Wang,Wenqian Ye,Yuntao Du,Yunpu Ma,Yijun Tian*

Main category: cs.CL

TL;DR: RECON框架通过引入一个显式的摘要模块来压缩检索增强生成（RAG）系统中的证据，解决了长而嘈杂上下文导致效率低下的问题，显著提高了训练速度、推理延迟和问答性能。


<details>
  <summary>Details</summary>
Motivation: 使用强化学习（RL）和推理训练的检索增强生成（RAG）系统受到低效上下文管理的困扰，因为长而嘈杂的检索文档会增加成本并降低性能。

Method: 本文提出了RECON（REasoning with CONdensation）框架，该框架将一个显式摘要模块集成到推理循环中以压缩证据。摘要器通过两阶段过程进行训练：首先在问答数据集上进行相关性预训练，然后从专有大型语言模型（LLMs）进行多方面蒸馏，以确保事实性和清晰度。RECON被集成到Search-R1管道中。

Result: RECON将总上下文长度减少了35%，从而提高了训练速度和推理延迟，同时改进了下游问答基准上的RAG性能。值得注意的是，它将3B模型的平均EM分数提高了14.5%，7B模型提高了3.0%，在多跳问答中表现出特别的优势。

Conclusion: RECON证明了学习到的上下文压缩对于构建实用、可扩展且高性能的RAG系统至关重要。

Abstract: Retrieval-augmented generation (RAG) systems trained using reinforcement
learning (RL) with reasoning are hampered by inefficient context management,
where long, noisy retrieved documents increase costs and degrade performance.
We introduce RECON (REasoning with CONdensation), a framework that integrates
an explicit summarization module to compress evidence within the reasoning
loop. Our summarizer is trained via a two-stage process: relevance pretraining
on QA datasets, followed by multi-aspect distillation from proprietary LLMs to
ensure factuality and clarity. Integrated into the Search-R1 pipeline, RECON
reduces total context length by 35\%, leading to improved training speed and
inference latency, while simultaneously improving RAG performance on downstream
QA benchmarks. Notably, it boosts the average EM score of the 3B model by
14.5\% and the 7B model by 3.0\%, showing particular strength in multi-hop QA.
RECON demonstrates that learned context compression is essential for building
practical, scalable, and performant RAG systems. Our code implementation is
made available at https://github.com/allfornancy/RECON.

</details>


### [356] [Steering Over-refusals Towards Safety in Retrieval Augmented Generation](https://arxiv.org/abs/2510.10452)
*Utsav Maskey,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）的安全对齐导致过度拒绝良性请求，尤其是在检索增强生成（RAG）中。本文分析了查询意图和检索上下文对拒绝行为的影响，构建了RagRefuse基准，并提出了SafeRAG-Steering方法，通过嵌入干预来减轻过度拒绝，同时保留合法拒绝。


<details>
  <summary>Details</summary>
Motivation: LLMs的安全对齐导致模型对良性请求的过度拒绝（over-refusals），这在RAG场景中尤为突出，因为检索到的上下文属性也会影响拒绝行为，从而降低了模型的实用性。

Method: 本文构建了RagRefuse基准，该基准按领域（医学、化学、开放领域）分层，将良性与有害查询与受控的上下文污染模式和大小配对。通过分析上下文排列/污染、查询和上下文领域以及有害文本密度对拒绝行为的影响。为缓解过度拒绝，提出了SafeRAG-Steering，这是一种以模型为中心的嵌入干预，在推理时将嵌入区域引导至确认安全、非拒绝的输出区域。

Result: 分析表明，上下文排列/污染、查询和上下文的领域以及有害文本密度会触发对良性查询的拒绝，其效果取决于模型特定的对齐选择。SafeRAG-Steering在受污染的RAG管道中减少了过度拒绝，同时保留了合法的拒绝。

Conclusion: LLMs的安全对齐确实导致RAG中的过度拒绝，且受上下文污染等因素影响。SafeRAG-Steering作为一种模型中心的嵌入干预，能够有效缓解这一问题，在减少过度拒绝的同时不损害模型进行合法拒绝的能力。

Abstract: Safety alignment in large language models (LLMs) induces over-refusals --
where LLMs decline benign requests due to aggressive safety filters. We analyze
this phenomenon in retrieval-augmented generation (RAG), where both the query
intent and retrieved context properties influence refusal behavior. We
construct RagRefuse, a domain-stratified benchmark spanning medical, chemical,
and open domains, pairing benign and harmful queries with controlled context
contamination patterns and sizes. Our analysis shows that context arrangement /
contamination, domain of query and context, and harmful-text density trigger
refusals even on benign queries, with effects depending on model-specific
alignment choices. To mitigate over-refusals, we introduce
\textsc{SafeRAG-Steering}, a model-centric embedding intervention that steers
the embedding regions towards the confirmed safe, non-refusing output regions
at inference time. This reduces over-refusals in contaminated RAG pipelines
while preserving legitimate refusals.

</details>


### [357] [End-to-end Speech Recognition with similar length speech and text](https://arxiv.org/abs/2510.10453)
*Peng Fan,Wenping Wang,Fei Deng*

Main category: cs.CL

TL;DR: 本文提出两种新方法（TIL和AXE损失）和帧融合机制，以改善语音与文本长度相近时的自动语音识别（ASR）对齐问题，显著减少帧数并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别（ASR）中语音和文本长度不匹配是一个挑战。现有方法（如基于CTC的KFDS）在将语音下采样到与文本长度相似时，无法正确对齐语音和文本。本文旨在解决语音长度与文本长度接近时的对齐问题。

Method: 本文引入了两种对齐方法：a) 时间独立损失（Time Independence Loss, TIL）和 b) 基于编辑距离的对齐交叉熵（Aligned Cross Entropy, AXE）损失。为增强关键帧信息，还通过加权并将关键帧与其前后各2帧的上下文进行求和，实现了帧融合。

Result: 在AISHELL-1和AISHELL-2数据集子集上的实验结果表明，所提出的方法优于现有工作，并且至少减少了86%的帧数。

Conclusion: 所提出的TIL和AXE损失以及帧融合机制，在语音与文本长度相近的情况下，有效解决了ASR的对齐问题，显著提高了效率（减少帧数）并取得了更好的性能。

Abstract: The mismatch of speech length and text length poses a challenge in automatic
speech recognition (ASR). In previous research, various approaches have been
employed to align text with speech, including the utilization of Connectionist
Temporal Classification (CTC). In earlier work, a key frame mechanism (KFDS)
was introduced, utilizing intermediate CTC outputs to guide downsampling and
preserve keyframes, but traditional methods (CTC) failed to align speech and
text appropriately when downsampling speech to a text-similar length. In this
paper, we focus on speech recognition in those cases where the length of speech
aligns closely with that of the corresponding text. To address this issue, we
introduce two methods for alignment: a) Time Independence Loss (TIL) and b)
Aligned Cross Entropy (AXE) Loss, which is based on edit distance. To enhance
the information on keyframes, we incorporate frame fusion by applying weights
and summing the keyframe with its context 2 frames. Experimental results on
AISHELL-1 and AISHELL-2 dataset subsets show that the proposed methods
outperform the previous work and achieve a reduction of at least 86\% in the
number of frames.

</details>


### [358] [Rethinking LLM Evaluation: Can We Evaluate LLMs with 200x Less Data?](https://arxiv.org/abs/2510.10457)
*Shaobo Wang,Cong Wang,Wenjie Fu,Yue Min,Mingquan Feng,Isabel Guan,Xuming Hu,Conghui He,Cunxiang Wang,Kexin Yang,Xingzhang Ren,Fei Huang,Dayiheng Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 本文提出了EssenceBench，一个粗粒度到细粒度的迭代遗传算法框架，用于基准测试压缩。该框架通过样本级冗余分析和优化问题建模，显著减少了样本数量，同时保持了模型评分重建的准确性和排名的一致性。


<details>
  <summary>Details</summary>
Motivation: 随着对多样化模型能力进行全面评估的需求不断增加，基准测试套件的规模也显著扩大。尽管在冗余减少和子集级性能预测方面取得了进展，但仍缺乏一个系统框架，能有效整合这些方法以同时确保预测准确性和排名一致性。

Method: 首先进行样本级冗余分析，识别并消除高度相似的样本。其次，将基准测试压缩视为一个以分数重建为目标的优化问题。在此基础上，提出了EssenceBench，一个粗粒度到细粒度的框架，利用迭代遗传算法（GA）进行基于适应度的子集搜索和基于归因的样本搜索。

Result: 与现有方法相比，EssenceBench在压缩结果上表现更优，重建误差更低，效率显著提高。特别是在HellaSwag基准测试（10K样本）上，该方法使用减少25倍的样本，能将所有模型的排名波动控制在5%以内；使用减少200倍的样本，能实现95%的模型排名波动控制在5%以内。

Conclusion: EssenceBench提供了一个有效且高效的基准测试压缩解决方案，能够在显著减少样本数量的同时，保持评分重建的高准确性和模型排名的一致性。

Abstract: As the demand for comprehensive evaluations of diverse model capabilities
steadily increases, benchmark suites have correspondingly grown significantly
in scale. Despite notable advances in redundancy reduction and subset-level
performance prediction, a systematic framework that effectively integrates
these methods to ensure both prediction accuracy and ranking consistency is
still largely elusive. In this paper, we first perform a sample-level analysis
of benchmark redundancy and identify several highly similar samples that can be
eliminated. Besides, we frame benchmark compression as an optimization problem
with the aim of score reconstruction. Building on these, we then propose
EssenceBench, a coarse-to-fine framework utilizing an iterative Genetic
Algorithm (GA), which takes the advantages of fitness-based subset search and
attribution-based sample search. Compared to previous methods, our approach
yields superior compression results with lower reconstruction error and
markedly higher efficiency. In particular, on the HellaSwag benchmark (10K
samples), our method preserves the ranking of all models shifting within 5%
using 25x fewer samples, and achieves 95% ranking preservation shifting within
5% using only 200x fewer samples.

</details>


### [359] [NIM: Neuro-symbolic Ideographic Metalanguage for Inclusive Communication](https://arxiv.org/abs/2510.10459)
*Prawaal Sharma,Poonam Goyal,Navneet Goyal,Vidisha Sharma*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的通用表意元语言，结合神经符号人工智能（LLMs和NSM）将复杂概念分解为原子概念，旨在帮助学术素养较低的人群克服数字鸿沟，实现了高语义可理解性和易学性。


<details>
  <summary>Details</summary>
Motivation: 数字交流已成为现代互动的基础，但学术素养较低的个体面临显著障碍，加剧了“数字鸿沟”。研究旨在为这部分人群提供一个超越学术、语言和文化界限的创新交流框架，实现快速、可访问和交互式的交流。

Method: 研究引入了一种新型的通用表意元语言。其方法基于神经符号人工智能（Neuro-symbolic AI），结合了富含世界知识的神经网络大型语言模型（LLMs）和植根于自然语义元语言（NSM）理论的符号知识启发式算法。这使得复杂思想能够语义分解为更简单的原子概念。此外，采用以人为本的协作方法，与200多名半文盲参与者共同定义问题、选择表意符号并验证系统。

Result: 该系统实现了超过80%的语义可理解性，具有易于掌握的学习曲线和普遍的适应性。它有效地服务于受正规教育有限的弱势群体。

Conclusion: 该系统成功地为受教育程度有限的弱势群体提供了有效的服务，有效弥合了沟通障碍，超越了学术、语言和文化界限，从而解决了数字鸿沟问题。

Abstract: Digital communication has become the cornerstone of modern interaction,
enabling rapid, accessible, and interactive exchanges. However, individuals
with lower academic literacy often face significant barriers, exacerbating the
"digital divide". In this work, we introduce a novel, universal ideographic
metalanguage designed as an innovative communication framework that transcends
academic, linguistic, and cultural boundaries. Our approach leverages
principles of Neuro-symbolic AI, combining neural-based large language models
(LLMs) enriched with world knowledge and symbolic knowledge heuristics grounded
in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables
the semantic decomposition of complex ideas into simpler, atomic concepts.
Adopting a human-centric, collaborative methodology, we engaged over 200
semi-literate participants in defining the problem, selecting ideographs, and
validating the system. With over 80\% semantic comprehensibility, an accessible
learning curve, and universal adaptability, our system effectively serves
underprivileged populations with limited formal education.

</details>


### [360] [FML-bench: A Benchmark for Automatic ML Research Agents Highlighting the Importance of Exploration Breadth](https://arxiv.org/abs/2510.10472)
*Qiran Zou,Hou Hei Lam,Wenhao Zhao,Yiming Tang,Tingting Chen,Samson Yu,Tianyi Zhang,Chang Liu,Xiangyang Ji,Dianbo Liu*

Main category: cs.CL

TL;DR: 本文提出了FML-bench，一个用于评估自动机器学习研究代理的新基准，旨在解决现有基准在学术严谨性、任务多样性和可扩展性方面的不足，并发现广泛探索策略优于深度探索。


<details>
  <summary>Details</summary>
Motivation: 现有用于评估能自主提出想法和进行机器学习实验的LLM研究代理的基准存在局限性，包括：过分强调工程方面而非学术严谨性、任务多样性不足、过度关注应用而非基础研究问题，以及难以扩展到真实研究场景。

Method: 本文引入了FML-bench，一个包含8个多样化和基础机器学习研究问题的基准，旨在减少编码负担、强调基础问题、提供高任务多样性并可扩展到真实世界的GitHub仓库。此外，还提出了一个包含五个互补指标的统一评估框架。

Result: 通过在FML-bench上评估最先进的自动研究代理，发现采用广泛研究探索策略的代理表现优于那些专注于狭窄但深度探索的代理。

Conclusion: 这些发现表明，强调探索的广度可能比仅仅关注增量改进能带来更有效的研究成果。

Abstract: Large language models (LLMs) have sparked growing interest in automatic
machine learning research agents. Among them, agents capable of autonomously
proposing ideas and conducting machine learning experiments are particularly
promising, as they maximize research automation and accelerate scientific
progress by iteratively refining ideas based on experimental results. However,
comprehensively evaluating such agents remains challenging. Existing benchmarks
tend to overemphasize engineering aspects while neglecting academic rigor,
creating barriers that obscure a clear assessment of an agent's scientific
capabilities in machine learning research. They also suffer from limited task
diversity, an overemphasis on application-oriented tasks over fundamental
research problems, and limited scalability to realistic research settings. To
address these limitations, we introduce FML-bench, a benchmark designed to
evaluate automatic machine learning research agents on 8 diverse and
fundamental machine learning research problems. It reduces coding burden,
emphasizes fundamental problems rather than specific use cases, offers high
task diversity, and is extensible to real-world machine learning GitHub
repositories. Furthermore, we present a unified evaluation framework with five
complementary metrics, designed to comprehensively assess agent performance on
our benchmark. We evaluate state-of-the-art automatic research agents on
FML-bench, and find that agents employing broad research exploration strategies
outperform those focusing on narrow but deep exploration. These findings
suggest that emphasizing the breadth of exploration may lead to more effective
research outcomes than focusing solely on incremental refinement. Our benchmark
is available at https://github.com/qrzou/FML-bench.

</details>


### [361] [When or What? Understanding Consumer Engagement on Digital Platforms](https://arxiv.org/abs/2510.10474)
*Jingyi Wu,Junying Liang*

Main category: cs.CL

TL;DR: 本研究发现，在数字平台上，内容的发布时间（时机和上下文）对消费者参与度的影响可能比内容本身的主题特征更大，挑战了以往关于内容是受欢迎度主要驱动因素的假设。


<details>
  <summary>Details</summary>
Motivation: 在数字服务经济中，理解受欢迎度的驱动因素至关重要，因为内容创作者为争夺消费者注意力而竞争。以往的研究主要强调内容特征的作用，但创作者往往错误判断受众的实际价值偏好。

Method: 本研究将Latent Dirichlet Allocation (LDA) 模型应用于TED演讲的大型语料库，将该平台视为创作者（演讲者）与消费者（观众）互动的数字服务案例。通过比较创作者的主题供给与观众参与度表达的需求，并进行纵向分析。

Result: 研究发现生产者供给与消费者偏好之间存在持续性不匹配。纵向分析进一步揭示，时间动态对消费者参与度的影响强于主题内容，表明内容交付的“时机”可能比“内容本身”更重要。

Conclusion: 这些发现挑战了内容特征是受欢迎度主要驱动因素的主导假设，并强调了时机和语境因素在塑造消费者反应中的重要性。研究结果为数字平台上的消费者注意力动态提供了新见解，并对营销人员、平台管理者和内容创作者优化受众参与策略具有实际意义。

Abstract: Understanding what drives popularity is critical in today's digital service
economy, where content creators compete for consumer attention. Prior studies
have primarily emphasized the role of content features, yet creators often
misjudge what audiences actually value. This study applies Latent Dirichlet
Allocation (LDA) modeling to a large corpus of TED Talks, treating the platform
as a case of digital service provision in which creators (speakers) and
consumers (audiences) interact. By comparing the thematic supply of creators
with the demand expressed in audience engagement, we identify persistent
mismatches between producer offerings and consumer preferences. Our
longitudinal analysis further reveals that temporal dynamics exert a stronger
influence on consumer engagement than thematic content, suggesting that when
content is delivered may matter more than what is delivered. These findings
challenge the dominant assumption that content features are the primary drivers
of popularity and highlight the importance of timing and contextual factors in
shaping consumer responses. The results provide new insights into consumer
attention dynamics on digital platforms and carry practical implications for
marketers, platform managers, and content creators seeking to optimize audience
engagement strategies.

</details>


### [362] [Assessing Large Language Models for Structured Medical Order Extraction](https://arxiv.org/abs/2510.10475)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

TL;DR: MasonNLP团队在MEDIQA-OE 2025医学医嘱提取任务中，使用未经领域微调的LLaMA-4 17B模型结合少量样本提示工程，获得了第五名，并证明了通用大型语言模型在该任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 医学医嘱提取对于构建可操作的临床信息、支持决策制定以及实现文档自动化和工作流自动化等下游应用至关重要。医嘱存在于多种来源和类别中，MEDIQA-OE 2025任务旨在从扩展的对话记录中提取结构化医学医嘱。

Method: MasonNLP团队采用了通用的、经过指令微调的LLaMA-4 17B模型。该方法未进行领域特定的微调，而是通过单个上下文示例（少量样本配置）进行引导，以提取医嘱类型、描述、原因和来源。

Result: MasonNLP的提交在17个参与团队（共105份提交）中排名第5。该方法实现了37.76的平均F1分数，并在原因和来源的准确性方面有显著提升。

Conclusion: 研究结果表明，大型的、非领域特定的LLM，在与有效的提示工程相结合时，可以作为专业临床自然语言处理任务的强大且可扩展的基线模型。

Abstract: Medical order extraction is essential for structuring actionable clinical
information, supporting decision-making, and enabling downstream applications
such as documentation and workflow automation. Orders may be embedded in
diverse sources, including electronic health records, discharge summaries, and
multi-turn doctor-patient dialogues, and can span categories such as
medications, laboratory tests, imaging studies, and follow-up actions. The
MEDIQA-OE 2025 shared task focuses on extracting structured medical orders from
extended conversational transcripts, requiring the identification of order
type, description, reason, and provenance. We present the MasonNLP submission,
which ranked 5th among 17 participating teams with 105 total submissions. Our
approach uses a general-purpose, instruction-tuned LLaMA-4 17B model without
domain-specific fine-tuning, guided by a single in-context example. This
few-shot configuration achieved an average F1 score of 37.76, with notable
improvements in reason and provenance accuracy. These results demonstrate that
large, non-domain-specific LLMs, when paired with effective prompt engineering,
can serve as strong, scalable baselines for specialized clinical NLP tasks.

</details>


### [363] [UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models](https://arxiv.org/abs/2510.10481)
*Guangxin He,Shen Nie,Fengqi Zhu,Yuankang Zhao,Tianyi Bai,Ran Yan,Jie Fu,Chongxuan Li,Binhang Yuan*

Main category: cs.CL

TL;DR: 本文通过后训练技术，成功将扩散LLM（LLaDA）的上下文窗口扩展到128K，并提出了UltraLLaDA，通过对RoPE的修改和掩码策略的分析，实现了长上下文稳定扩展。


<details>
  <summary>Details</summary>
Motivation: 扩散LLM在各种下游任务中展现出巨大潜力，但其长上下文行为仍未被充分探索，这促使研究人员探索如何在不从头训练的情况下扩展其上下文窗口。

Method: 研究采用了一种后训练技术案例研究，专注于扩展扩散LLM（LLaDA）的上下文窗口。核心方法包括：对标准旋转位置嵌入（RoPE）进行简单修改以适应扩散过程的概率建模；比较后训练期间使用的掩码策略及其对优化稳定性和长程召回的影响。

Result: 通过对RoPE的特殊修改，实现了扩散LLM向更长上下文范围的稳定扩展。研究引入了UltraLLaDA，一个拥有128K token上下文窗口的扩散LLM，在长上下文任务上的实证评估中显著优于无需训练的基线。实验结果强调了特殊位置扩展是扩展扩散LLM上下文的关键杠杆。

Conclusion: 后训练技术可以有效扩展扩散LLM的上下文窗口，其中对RoPE的特定修改对于实现长上下文（如128K规模）至关重要。本文为寻求通过高效后训练实现128K规模上下文的实践者提供了实用指导。

Abstract: Diffusion LLMs have attracted growing interest, with plenty of recent work
emphasizing their great potential in various downstream tasks; yet the
long-context behavior of diffusion LLMs remains largely uncharted. We present a
case study of post-training techniques for extending the context window of
diffusion LLMs (i.e., LLaDA) without retraining from scratch. We show that a
simple modification to the standard Rotary Positional Embeddings (RoPE)
extension effectively accommodates the probabilistic modeling inherent in the
diffusion process, enabling stable scaling to longer context ranges. We further
compare masking strategies used during post-training and analyze their impact
on optimization stability and long-range recall. Instantiating these insights,
we introduce UltraLLaDA, a diffusion LLM with a 128K-token context window that,
in our empirical evaluation on long-context tasks, significantly outperforms
training-free baselines. Our experimental results highlight the special
positional extension as a key lever for scaling diffusion LLMs to extended
contexts and offer practical guidance for practitioners seeking 128K-scale
context via efficient post-training.

</details>


### [364] [VOLTAGE: A Versatile Contrastive Learning based OCR Methodology for ultra low-resource scripts through Auto Glyph Feature Extraction](https://arxiv.org/abs/2510.10490)
*Prawaal Sharma,Poonam Goyal,Vidisha Sharma,Navneet Goyal*

Main category: cs.CL

TL;DR: 本文提出了一种名为VOLTAGE的基于对比学习的OCR方法，旨在为低资源濒危语言（如Takri）提供数字包容性，通过自动字形特征推荐和数据增强实现高准确率。


<details>
  <summary>Details</summary>
Motivation: 全球有2500种语言濒临灭绝，导致传统智慧和文化流失。低资源语言面临更大的灭绝风险，缺乏无监督OCR方法是阻碍其数字化的主要原因。

Method: 该研究提出了VOLTAGE方法，这是一种基于对比学习的OCR，利用自动字形特征推荐进行基于聚类的标注。通过图像变换和生成对抗网络（GANs）对标注数据进行多样性和数量上的增强。该方法以Takri文字为基础设计，并扩展到其他印度文字。

Result: VOLTAGE方法在Takri文字上取得了显著成果：机器印刷样本的准确率达到95%，手写样本的准确率为87%。研究还通过与其他印度文字（包括低资源和高资源）的实验，证实了该方法的普适性。

Conclusion: VOLTAGE是一种有效的OCR方法，能够为低资源濒危语言提供数字包容性，并在Takri文字上展现出高准确率。该工作通过基线和消融研究以及下游用例的构建，证明了其实用性。

Abstract: UNESCO has classified 2500 out of 7000 languages spoken worldwide as
endangered. Attrition of a language leads to loss of traditional wisdom, folk
literature, and the essence of the community that uses it. It is therefore
imperative to bring digital inclusion to these languages and avoid its
extinction. Low resource languages are at a greater risk of extinction. Lack of
unsupervised Optical Character Recognition(OCR) methodologies for low resource
languages is one of the reasons impeding their digital inclusion. We propose
VOLTAGE - a contrastive learning based OCR methodology, leveraging auto-glyph
feature recommendation for cluster-based labelling. We augment the labelled
data for diversity and volume using image transformations and Generative
Adversarial Networks. Voltage has been designed using Takri - a family of
scripts used in 16th to 20th century in the Himalayan regions of India. We
present results for Takri along with other Indic scripts (both low and high
resource) to substantiate the universal behavior of the methodology. An
accuracy of 95% for machine printed and 87% for handwritten samples on Takri
script has been achieved. We conduct baseline and ablation studies along with
building downstream use cases for Takri, demonstrating the usefulness of our
work.

</details>


### [365] [Merlin's Whisper: Enabling Efficient Reasoning in LLMs via Black-box Adversarial Prompting](https://arxiv.org/abs/2510.10528)
*Heming Xia,Cunxiao Du,Rui Li,Chak Tou Leong,Yongqi Li,Wenjie Li*

Main category: cs.CL

TL;DR: 本文提出AdvPrompt，一种黑盒对抗性提示框架，旨在通过迭代优化生成高质量提示，以减少大型推理模型（LRMs）的冗余思考，从而在不牺牲准确性的前提下显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过逐步思考解决复杂任务时，其冗长的推理过程会产生巨大的计算和延迟开销，阻碍了这些模型的实际部署。研究动机在于缓解LRMs的“过度思考”问题。

Method: 该研究引入AdvPrompt，一个迭代优化框架，从不同角度生成高质量的对抗性提示。它将开源LRMs和闭源API视为黑盒通信器，旨在引出简洁的响应，同时不牺牲准确性。

Result: AdvPrompt在多个基准测试中一致地减少了token使用量，同时保持了性能。具体而言，对于Qwen3模型系列，在简单的GSM8K问题上平均响应长度减少了3倍；在四个基准测试中，平均token减少了约40%。对于闭源API，在MATH-500上，AdvPrompt使Claude-3.7的token使用量减少了35%，Gemini-2.5减少了47%。此外，AdvPrompt在各种模型规模和系列中都表现出良好的泛化性。

Conclusion: 黑盒对抗性提示（AdvPrompt）是一种实用且有效的策略，能够通过减少过度思考来提高大型推理模型的效率，同时不影响其性能，这对于LRM的实际部署具有重要意义。

Abstract: Large reasoning models (LRMs) have demonstrated remarkable proficiency in
tackling complex reasoning tasks through step-by-step thinking. However, such a
lengthy reasoning process incurs substantial computational and latency
overheads, hindering the practical deployment of these models. In this work, we
present a new perspective on mitigating overthinking in LRMs via black-box
adversarial prompting. By treating both open-source LRMs and closed-source APIs
as black-box communicators, we investigate how to elicit concise responses
without sacrificing accuracy. We introduce AdvPrompt, an iterative refinement
framework that generates high-quality adversarial prompts from diverse
perspectives. Experiments across multiple benchmarks demonstrate that AdvPrompt
consistently reduces token usage while preserving performance. Notably,
AdvPrompt achieves a 3x reduction in average response length on simple GSM8K
questions for the Qwen3 model series, and delivers an average ~40% token
reduction across four benchmarks. For closed-source APIs, AdvPrompt reduces
token usage on MATH-500 by 35% for Claude-3.7 and 47% for Gemini-2.5. Further
analysis reveals the generalizability of AdvPrompt across various model scales
and families, underscoring the potential of black-box prompting as a practical
and effective strategy for enhancing LRM efficiency.

</details>


### [366] [Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs](https://arxiv.org/abs/2510.05577)
*Dong Yan,Gaochen Wu,Bowen Zhou*

Main category: cs.CL

TL;DR: 现有语言智能体在开放域多跳推理中因固定行动序列而受限。本文提出FGDIP框架，通过动态自适应的信息探索策略，结合历史错误分析和实时反馈，显著提升了LLM的推理能力，在HotpotQA和StrategyQA数据集上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有语言智能体在处理开放域多跳推理问题时，由于依赖固定的行动序列，需要大量信息检索，导致性能不佳。

Method: 本文提出了反馈引导的动态交互规划（FGDIP）框架。该方法首先识别关键实体作为初始节点，然后生成推理子节点。通过结合历史错误分析和实时反馈，动态调整和优化推理策略。FGDIP将深度优先搜索与创新的节点生成技术相结合，根据先前的错误路径和同时生成的同级节点进行自适应调整，从而有效地扩展搜索空间并确保推理过程收敛到准确的解决方案。

Result: FGDIP在HotpotQA数据集上取得了54.47%的F1分数，在StrategyQA数据集上取得了70.05%的F1分数，分别超越了最佳基线5.03%和7.25%。

Conclusion: FGDIP的动态自适应策略显著增强了语言智能体在多跳推理任务中的能力，展现了其在处理开放域问题方面的通用性和巨大潜力。

Abstract: Recent advancements in language agents have led to significant improvements
in multi-hop reasoning tasks. However, existing approaches often struggle with
handling open-domain problems, which require massive information retrieval due
to their reliance on a fixed sequence of actions. To address this, we propose
Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework
tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive
strategies for information exploration in open-domain multi-hop reasoning
tasks. Our approach begins by identifying key entities relevant to the problem,
which serve as the initial nodes in the reasoning process. From these initial
nodes, we then generate reasoning child nodes with the process being refined
through a combination of historical error analysis and real-time feedback,
which allows the framework to dynamically adjust and optimize its reasoning
strategies. By integrating depth-first search with an innovative node
generation technique, our framework adapts based on both prior error paths and
concurrently generated nodes at the same hierarchical level. This dynamic
strategy effectively expands the search space while ensuring the reasoning
process systematically converges toward accurate solutions. Experimental
results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset
and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and
7.25% respectively, highlighting its versatility and potential to enhance
language agents in multi-hop reasoning tasks.

</details>


### [367] [BitMar: Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices](https://arxiv.org/abs/2510.10560)
*Euhid Aman,Esteban Carlin,Hsing-Kuo Pao,Giovanni Beltrame,Ghaluh Indah Permata Sari,Yie-Tarng Chen*

Main category: cs.CL

TL;DR: BitMar是一种量化多模态Transformer，它结合了类人情景记忆，通过1.58比特编码器和高效解码器，在资源受限的边缘设备上实现有效的图像-文本生成。


<details>
  <summary>Details</summary>
Motivation: 现有的跨注意力Transformer和多模态视觉-语言模型虽然在接地和生成方面表现出色，但其庞大的全精度骨干网络难以部署到边缘设备上。虽然记忆增强架构能提升上下文利用率，但它们很少与激进的边缘量化技术结合使用。

Method: BitMar引入了一个外部的类人情景记忆。它使用1.58比特的文本编码器（BitNet风格）和视觉编码器（DiNOv2-based）来创建紧凑的嵌入，这些嵌入被组合并用于查询固定大小的键值情景记忆。在向量检索过程中，BitNet解码器应用逐层条件作用以增加生成内容的上下文相关性。解码器还采用带有滑动窗口机制的注意力槽（attention sinks），在严格的内存预算下处理长或流式输入。

Result: 通过逐层条件作用和滑动窗口注意力机制的结合，BitMar实现了强大的质量-速度权衡，以低延迟和小的模型占用空间提供了具有竞争力的图像字幕和多模态理解能力。

Conclusion: BitMar的特性使其非常适合边缘部署，因为它能在资源有限的硬件上提供高效且高质量的图像-文本生成和多模态理解能力。

Abstract: Cross-attention transformers and other multimodal vision-language models
excel at grounding and generation; however, their extensive, full-precision
backbones make it challenging to deploy them on edge devices. Memory-augmented
architectures enhance the utilization of past context; however, most works
rarely pair them with aggressive edge-oriented quantization. We introduce
BitMar, a quantized multimodal transformer that proposes an external human-like
episodic memory for effective image-text generation on hardware with limited
resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and
one for vision (DiNOv2-based), to create compact embeddings that are combined
and used to query a fixed-size key-value episodic memory. During vector
retrieval, the BitNet decoder applies per-layer conditioning, which increases
the contextual relevance of generated content. The decoder also employs
attention sinks with a sliding-window mechanism to process long or streaming
inputs under tight memory budgets. The combination of per-layer conditioning
and sliding-window attention achieves a strong quality-speed trade-off,
delivering competitive captioning and multimodal understanding at low latency
with a small model footprint. These characteristics make BitMar well-suited for
edge deployment.

</details>


### [368] [Dynamic Topic Evolution with Temporal Decay and Attention in Large Language Models](https://arxiv.org/abs/2510.10613)
*Di Wu abd Shuaidong Pan*

Main category: cs.CL

TL;DR: 本文提出了一种基于时间大语言模型的动态主题演化建模框架，通过结合上下文嵌入、时间衰减、注意力机制和状态转移矩阵，实现了语义表示和时间演化的统一建模，有效捕捉主题的动态变化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解大规模文本中的动态语义模式，并为现有主题模型提供一个更系统、更具解释性的解决方案，以统一建模语义表示和时间演化，从而提高主题连贯性、多样性、稳定性和可解释性。

Method: 该方法首先利用大语言模型获取文本的上下文嵌入，然后引入时间衰减函数和注意力机制来调整语义单元的重要性并捕捉跨时期的主题变化。接着，时间表示被映射到潜在主题空间，并应用状态转移矩阵描述主题的动态演化。最后，通过联合优化目标约束语义建模和时间一致性，确保主题生成的多样性和平滑性。

Result: 实验结果表明，该框架能有效捕捉主题的生成、扩展和衰退，并在多个指标上优于现有模型。它显著提高了主题的连贯性、多样性，同时增强了模型随时间变化的稳定性和可解释性。

Conclusion: 该方法为理解大规模文本中的动态语义模式提供了一个系统的解决方案，丰富了主题建模的研究范式，并支持多领域复杂的文本分析任务。

Abstract: This paper proposes a modeling framework for dynamic topic evolution based on
temporal large language models. The method first uses a large language model to
obtain contextual embeddings of text and then introduces a temporal decay
function and an attention mechanism. These components allow the model to adjust
the importance of semantic units according to time intervals and capture topic
variations across different periods. The temporal representations are then
mapped into a latent topic space, where a state transition matrix is applied to
describe the dynamic evolution of topics. A joint optimization objective
constrains both semantic modeling and temporal consistency, ensuring diversity
and smoothness in topic generation. The design emphasizes the unified modeling
of semantic representation and temporal evolution, which improves topic
coherence and diversity while enhancing stability and interpretability over
time. Experiments on real-world corpora show that the framework effectively
captures the generation, expansion, and decline of topics and outperforms
existing models across multiple metrics. Overall, the proposed method provides
a systematic solution for understanding dynamic semantic patterns in
large-scale text, enriches the research paradigm of topic modeling, and
supports complex text analysis tasks in multiple domains.

</details>


### [369] [Detecting Hallucinations in Authentic LLM-Human Interactions](https://arxiv.org/abs/2510.10539)
*Yujie Ren,Niklas Gruhlke,Anne Lauscher*

Main category: cs.CL

TL;DR: 本文引入了AuthenHallu，首个基于真实LLM-人类交互的幻觉检测基准，揭示了实际幻觉发生率并评估了LLM作为检测器的能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM幻觉检测基准多为人工构建，未能充分捕捉真实世界中LLM幻觉的特征，尤其是在医疗和法律等敏感领域，幻觉检测至关重要。

Method: 提出了AuthenHallu，一个完全基于真实LLM-人类对话构建的幻觉检测基准。通过选择和标注真实LLM-人类交互样本，以忠实反映LLM在日常用户交互中产生幻觉的方式。此外，还探索了使用普通LLM本身作为幻觉检测器的潜力。

Result: 统计分析显示，幻觉在基准中31.4%的问答对中出现，在数学和数字问题等挑战性领域中，这一比例显著增加到60.0%。尽管普通LLM作为幻觉检测器显示出一些前景，但其当前性能仍不足以应对真实世界场景。

Conclusion: 真实世界中LLM的幻觉发生频率较高，特别是在复杂任务中。当前的LLM自身作为检测器的性能尚不能满足实际需求。AuthenHallu为更真实的幻觉检测研究提供了新的基准。

Abstract: As large language models (LLMs) are increasingly applied in sensitive domains
such as medicine and law, hallucination detection has become a critical task.
Although numerous benchmarks have been proposed to advance research in this
area, most of them are artificially constructed--either through deliberate
hallucination induction or simulated interactions--rather than derived from
genuine LLM-human dialogues. Consequently, these benchmarks fail to fully
capture the characteristics of hallucinations that occur in real-world usage.
To address this limitation, we introduce AuthenHallu, the first hallucination
detection benchmark built entirely from authentic LLM-human interactions. For
AuthenHallu, we select and annotate samples from genuine LLM-human dialogues,
thereby providing a faithful reflection of how LLMs hallucinate in everyday
user interactions. Statistical analysis shows that hallucinations occur in
31.4% of the query-response pairs in our benchmark, and this proportion
increases dramatically to 60.0% in challenging domains such as Math & Number
Problems. Furthermore, we explore the potential of using vanilla LLMs
themselves as hallucination detectors and find that, despite some promise,
their current performance remains insufficient in real-world scenarios.

</details>


### [370] [Preserving LLM Capabilities through Calibration Data Curation: From Analysis to Optimization](https://arxiv.org/abs/2510.10618)
*Bowei He,Lihao Yin,Huiling Zhen,Shuqi Liu,Han Wu,Xiaokun Zhang,Mingxuan Yuan,Chen Ma*

Main category: cs.CL

TL;DR: 本文系统研究了校准数据（在LLM后训练压缩中至关重要）的组成特性和领域对应性如何影响LLM的各种能力，特别是复杂推理。研究发现激活空间中的代表性和多样性是校准数据质量的关键，并基于此提出了一个校准数据筛选框架以提升压缩性能。


<details>
  <summary>Details</summary>
Motivation: 后训练压缩是LLM高效推理的常用方法，其中校准数据对权重重要性和激活动态范围至关重要。然而，现有研究对校准数据如何影响LLM压缩后的能力（特别是高阶复杂推理能力）探索不足，仅从有限角度（如数据源或样本量）进行研究，缺乏对校准数据组成特性和领域对应性影响的系统性分析。

Method: 本文通过以下方法弥补研究空白：1) 探讨校准数据对LLM高阶复杂推理能力（如数学问题解决和代码生成）的影响；2) 从激活模式的角度深入分析潜在的影响机制；3) 发现激活空间中的代表性和多样性更根本地决定了校准数据的质量；4) 基于这些观察和分析，提出了一个校准数据筛选框架。

Result: 研究发现校准数据的组成特性和领域对应性显著影响LLM在压缩后的各种能力。激活空间中的代表性和多样性是决定校准数据质量的根本因素。本文提出的校准数据筛选框架能够增强现有后训练压缩方法在保留LLM关键能力（尤其是复杂推理能力）方面的性能。

Conclusion: 校准数据在激活空间中的代表性和多样性是决定其质量的关键因素，这对于后训练压缩后LLM能力的保持至关重要。基于这些发现，本文提出的校准数据筛选框架能够有效提升现有压缩方法在保留LLM复杂推理能力上的表现。

Abstract: Post-training compression has been a widely employed approach to scale down
large language model (LLM) and facilitate efficient inference. In various
proposed compression methods, including pruning and quantization, calibration
data plays a vital role by informing the weight importance and activation
dynamic ranges. However, how calibration data impacts the LLM capability after
compression is less explored. Few of the existing works, though recognizing the
significance of this study, only investigate the language modeling or
commonsense reasoning performance degradation from limited angles, like the
data sources or sample amounts. More systematic research is still needed to
examine the impacts on different LLM capabilities in terms of compositional
properties and domain correspondence of calibration data. In this work, we aim
at bridging this gap and further analyze underlying influencing mechanisms from
the activation pattern perspective. Especially, we explore the calibration
data's impacts on high-level complex reasoning capabilities, like math problem
solving and code generation. Delving into the underlying mechanism, we find
that the representativeness and diversity in activation space more
fundamentally determine the quality of calibration data. Finally, we propose a
calibration data curation framework based on such observations and analysis,
enhancing the performance of existing post-training compression methods on
preserving critical LLM capabilities. Our code is provided in
\href{https://github.com/BokwaiHo/COLA.git}{Link}.

</details>


### [371] [FactAppeal: Identifying Epistemic Factual Appeals in News Media](https://arxiv.org/abs/2510.10627)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 本文提出“认知诉求识别”新任务，旨在识别事实陈述如何通过外部来源或证据获得可信度。为此，作者构建了FactAppeal数据集，并使用大型语言模型进行建模，其中Gemma 2 9B模型取得了0.73的宏观F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有资源主要关注事实声明的检测和验证，而缺乏对这些声明如何通过外部来源或证据获得支持的深层认知结构和证据基础的理解。本研究旨在填补这一空白，识别事实声明的可信度来源。

Method: 本文提出了“认知诉求识别”的新任务。为此，作者创建了FactAppeal数据集，包含3,226个手工标注的英文新闻句子。该数据集的标注包括：事实陈述的跨度识别、所依赖来源的提及、来源类型（如活跃参与者、目击者、专家、直接证据）、来源是否被点名、来源的角色和认知资质、通过直接或间接引用对来源的归因以及其他细粒度特征。作者使用2B-9B参数范围的编码器模型和生成式解码器模型对该任务进行建模。

Result: 在所提出的“认知诉求识别”任务中，基于Gemma 2 9B的最佳模型取得了0.73的宏观F1分数。

Conclusion: 本文成功提出了认知诉求识别这一新任务，并构建了详细标注的FactAppeal数据集，为理解事实声明如何获得可信度提供了新的视角和资源。实验结果表明，大型语言模型在该任务上表现出有希望的性能。

Abstract: How is a factual claim made credible? We propose the novel task of Epistemic
Appeal Identification, which identifies whether and how factual statements have
been anchored by external sources or evidence. To advance research on this
task, we present FactAppeal, a manually annotated dataset of 3,226
English-language news sentences. Unlike prior resources that focus solely on
claim detection and verification, FactAppeal identifies the nuanced epistemic
structures and evidentiary basis underlying these claims and used to support
them. FactAppeal contains span-level annotations which identify factual
statements and mentions of sources on which they rely. Moreover, the
annotations include fine-grained characteristics of factual appeals such as the
type of source (e.g. Active Participant, Witness, Expert, Direct Evidence),
whether it is mentioned by name, mentions of the source's role and epistemic
credentials, attribution to the source via direct or indirect quotation, and
other features. We model the task with a range of encoder models and generative
decoder models in the 2B-9B parameter range. Our best performing model, based
on Gemma 2 9B, achieves a macro-F1 score of 0.73.

</details>


### [372] [You're Not Gonna Believe This: A Computational Analysis of Factual Appeals and Sourcing in Partisan News](https://arxiv.org/abs/2510.10658)
*Guy Mor-Lan,Tamir Sheafer,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 本研究通过大规模比较CNN和Fox News，分析了事实报道背后的认知策略。结果发现CNN的报道包含更多事实陈述并更倾向于引用外部专家和专家文件，而Fox News则偏好新闻报道和直接引语，揭示了党派媒体构建现实的系统性差异。


<details>
  <summary>Details</summary>
Motivation: 媒体偏见已被广泛研究，但事实报道背后的认知策略在计算层面仍未得到充分探索。

Method: 采用文章匹配策略，比较CNN和Fox News对相同事件的报道，以隔离报道风格与话题选择。将FactAppeal框架应用于包含超过47万篇文章的语料库，涵盖COVID-19大流行和以色列-哈马斯战争这两个高度政治化的时期。

Result: CNN的报道包含更多事实陈述，并且更有可能将其基于外部来源。两家媒体在引用模式上存在显著差异：CNN通过引用“专家”和“专家文件”来建立可信度，构建对正式权威的诉求；而Fox News则偏好“新闻报道”和直接引语。

Conclusion: 党派媒体使用系统性不同的认知策略来构建现实，这为媒体偏见研究增添了新的维度。

Abstract: While media bias is widely studied, the epistemic strategies behind factual
reporting remain computationally underexplored. This paper analyzes these
strategies through a large-scale comparison of CNN and Fox News. To isolate
reporting style from topic selection, we employ an article matching strategy to
compare reports on the same events and apply the FactAppeal framework to a
corpus of over 470K articles covering two highly politicized periods: the
COVID-19 pandemic and the Israel-Hamas war. We find that CNN's reporting
contains more factual statements and is more likely to ground them in external
sources. The outlets also exhibit sharply divergent sourcing patterns: CNN
builds credibility by citing Experts} and Expert Documents, constructing an
appeal to formal authority, whereas Fox News favors News Reports and direct
quotations. This work quantifies how partisan outlets use systematically
different epistemic strategies to construct reality, adding a new dimension to
the study of media bias.

</details>


### [373] [AGENTIQL: An Agent-Inspired Multi-Expert Framework for Text-to-SQL Generation](https://arxiv.org/abs/2510.10661)
*Omid Reza Heidari,Siobhan Reid,Yassine Yaakoubi*

Main category: cs.CL

TL;DR: AGENTIQL是一个受代理启发的多专家框架，通过分解问题、生成子查询和优化列选择，显著提高了Text-to-SQL的准确性和可解释性，并缩小了与GPT-4级别模型的性能差距。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在Text-to-SQL生成方面取得了进展，但其单体架构在处理复杂推理和多样化数据库模式时仍面临挑战。

Method: 本文提出了AGENTIQL，一个受代理启发的多专家框架。它包含一个用于问题分解的推理代理、一个用于子查询生成的编码代理以及一个用于列选择的精炼步骤。此外，一个自适应路由器在模块化管道和基线解析器之间进行选择，以平衡效率和准确性。该框架的多个步骤可以并行执行，以提高可扩展性。

Result: 在Spider基准测试中，AGENTIQL提高了执行准确性和可解释性，使用14B模型和Planner&Executor合并策略，最高达到86.07%的执行准确率（EX）。其性能取决于路由机制的有效性，成功缩小了与基于GPT-4的SOTA（89.65% EX）的差距，同时使用了更小的开源LLM。除了准确性，AGENTIQL还通过暴露中间推理步骤增强了透明度。

Conclusion: AGENTIQL提供了一种鲁棒、可扩展且可解释的语义解析方法，有效解决了Text-to-SQL生成中的复杂推理和模式多样性问题，提高了准确性并增强了透明度，缩小了与顶级模型的性能差距。

Abstract: LLMs have advanced text-to-SQL generation, yet monolithic architectures
struggle with complex reasoning and schema diversity. We propose AGENTIQL, an
agent-inspired multi-expert framework that combines a reasoning agent for
question decomposition, a coding agent for sub-query generation, and a
refinement step for column selection. An adaptive router further balances
efficiency and accuracy by selecting between our modular pipeline and a
baseline parser. Several steps in the pipeline can be executed in parallel,
making the framework scalable to larger workloads. Evaluated on the Spider
benchmark, AGENTIQL improves execution accuracy and interpretability and
achieves up to 86.07\% EX with 14B models using the Planner&Executor merging
strategy. The attained performance is contingent upon the efficacy of the
routing mechanism, thereby narrowing the gap to GPT-4-based SOTA (89.65% EX)
while using much smaller open-source LLMs. Beyond accuracy, AGENTIQL enhances
transparency by exposing intermediate reasoning steps, offering a robust,
scalable, and interpretable approach to semantic parsing.

</details>


### [374] [Unlocking LLM Safeguards for Low-Resource Languages via Reasoning and Alignment with Minimal Training Data](https://arxiv.org/abs/2510.10677)
*Zhuowei Chen,Bowei Zhang,Nankai Lin,Tian Hou,Lianxi Wang*

Main category: cs.CL

TL;DR: 本文提出ConsistentGuard，一种基于推理的多语言LLM安全防护机制，通过推理增强可解释性，并通过对齐促进跨语言知识迁移，仅用少量数据即在多语言任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的进步带来了恶意请求的风险，现有基于分类器的方法缺乏可解释性，并且在低资源语言上表现不佳，因此需要更有效、可解释且多语言友好的LLM安全防护机制。

Method: 提出ConsistentGuard，一种新颖的基于推理的多语言安全防护方法。它通过推理增强可解释性，并通过语言对齐提升跨语言知识迁移能力。该方法仅使用1,000个训练样本。

Result: ConsistentGuard在三个数据集、六种语言上表现出卓越性能，优于使用更多数据训练的大型模型，并展现出强大的可解释性和泛化能力。作者还贡献了一个多语言基准扩展并发布了代码。

Conclusion: ConsistentGuard成功解决了现有LLM安全防护的局限性，提供了一个数据高效、可解释且多语言适用的解决方案，有效检测恶意请求，并为未来研究提供了新的基准和资源。

Abstract: Recent advances in LLMs have enhanced AI capabilities, but also increased the
risk posed by malicious requests, highlighting the need for effective LLM
safeguards to detect such queries. Existing approaches largely rely on
classifier-based methods that lack interpretability and perform poorly on
low-resource languages. To address these limitations, we propose
ConsistentGuard, a novel reasoning-based multilingual safeguard, which enhances
explainability via reasoning and boosts knowledge transfer between languages
through alignment. With only 1,000 training samples, our method demonstrates
superior performance on three datasets across six languages, outperforming
larger models trained with significantly more data, and exhibits strong
interpretability and generalization ability. We also contribute a multilingual
benchmark extension and release our codes to support future research.

</details>


### [375] [BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions](https://arxiv.org/abs/2510.10666)
*Zhengbo Zhang,Zhiheng Lyu,Junhao Gong,Hongzhu Yi,Xinming Wang,Yuxuan Zhou,Jiabing Yang,Ping Nie,Yan Huang,Wenhu Chen*

Main category: cs.CL

TL;DR: BrowserAgent是一个受人类启发、能直接在原始网页上执行浏览器操作（如滚动、点击、输入）的大语言模型代理，通过两阶段训练和显式记忆机制，在更少数据下实现了对复杂网络任务（特别是多跳问答）更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型（LLM）网络任务解决方案（如Search-R1、WebDancer）依赖于将交互式网络环境转换为静态文本，这与人类多样化的浏览器交互行为（如滚动、点击、输入）形成对比，限制了其效率和交互性。

Method: BrowserAgent通过Playwright直接在原始网页上执行预定义的浏览器操作。它采用两阶段训练方法：监督微调（SFT）和拒绝微调（RFT），以提高泛化能力。此外，引入了显式记忆机制来存储关键结论，增强了对长周期任务的推理能力。

Result: 尽管训练数据量远少于Search-R1，BrowserAgent在不同的开放式问答（Open-QA）任务上取得了更具竞争力的结果。值得注意的是，BrowserAgent-7B在HotpotQA、2Wiki和Bamboogle等多跳问答任务上比Search-R1提升了约20%。

Conclusion: BrowserAgent可以作为一个更先进的框架，用于开发更具交互性和可扩展性的网络代理。

Abstract: Efficiently solving real-world problems with LLMs increasingly hinges on
their ability to interact with dynamic web environments and autonomously
acquire external information. While recent research like Search-R1 and
WebDancer demonstrates strong performance in solving web tasks, they heavily
rely on additional tools to convert the interactive web environment into static
text content. This is in contrast to human browsing behaviors, which involve
diverse interactions with the browser, such as scrolling, clicking, and typing.
In this paper, we propose BrowserAgent, a more interactive agent that solves
complex tasks through human-inspired browser actions. BrowserAgent operates
directly on raw web pages via Playwright through a set of predefined browser
actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and
Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities.
Despite using significantly less training data than Search-R1, BrowserAgent
achieves more competitive results across different Open-QA tasks. Additionally,
we introduce an explicit memory mechanism to store key conclusions across
steps, further enhancing the model's reasoning capabilities for long-horizon
tasks. Notably, BrowserAgent-7B can achieve around 20\% improvement over
Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These
results indicate that BrowserAgent can serve as a more advanced framework for
more interactive and scalable web agents.

</details>


### [376] [Sarcasm Detection Using Deep Convolutional Neural Networks: A Modular Deep Learning Framework](https://arxiv.org/abs/2510.10729)
*Manas Zambre,Sarika Bobade*

Main category: cs.CL

TL;DR: 本文提出一个模块化的深度学习框架，结合DCNNs和BERT，用于在文本中检测讽刺，通过分析语言、情感和上下文线索。


<details>
  <summary>Details</summary>
Motivation: 在文本交流中，由于缺乏语调和肢体语言，讽刺常常被误解，因此需要一种有效的方法来检测讽刺。

Method: 该研究采用模块化深度学习框架，利用深度卷积神经网络（DCNNs）和BERT等上下文模型。它通过多层架构整合情感分析、上下文嵌入、语言特征提取和情感检测。

Result: 尽管模型处于概念阶段，但它展示了在聊天机器人和社交媒体分析等实际应用中的可行性。

Conclusion: 所提出的深度学习框架在检测文本讽刺方面具有可行性，并有望应用于实际场景。

Abstract: Sarcasm is a nuanced and often misinterpreted form of communication,
especially in text, where tone and body language are absent. This paper
proposes a modular deep learning framework for sarcasm detection, leveraging
Deep Convolutional Neural Networks (DCNNs) and contextual models such as BERT
to analyze linguistic, emotional, and contextual cues. The system integrates
sentiment analysis, contextual embeddings, linguistic feature extraction, and
emotion detection through a multi-layer architecture. While the model is in the
conceptual stage, it demonstrates feasibility for real-world applications such
as chatbots and social media analysis.

</details>


### [377] [RePro: Training Language Models to Faithfully Recycle the Web for Pretraining](https://arxiv.org/abs/2510.10681)
*Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: RePro是一种新颖的网页数据回收方法，通过使用强化学习训练一个小型语言模型生成高质量且忠实于原文的预训练数据复述，有效解决了大型语言模型预训练数据短缺的问题。


<details>
  <summary>Details</summary>
Motivation: 高质量的预训练数据是大型语言模型（LLMs）的“化石燃料”，但对于前沿模型而言，其储备正在耗尽。因此，需要开发新的方法来有效利用现有数据。

Method: 本文提出了RePro方法，利用强化学习训练一个相对较小的语言模型（4B参数）作为复述器。该复述器通过一个质量奖励和三个忠实度奖励进行优化，旨在将原始数据转换为高质量的复述，同时保持其核心语义和结构。

Result: 在400M和1.4B模型上的预训练结果表明，RePro在22个下游任务上比仅使用原始数据的基线模型相对准确率提高了4.7%-14.0%。它还优于SOTA的ReWire方法（使用70B复述器），并超越了数据量大4倍的原始基线。RePro将原始数据效率提高了2-3倍，并通过分析验证了其比基于提示的方法保留了更多关键信息并忠实反映了原始数据特性。

Conclusion: RePro提供了一种高效且可控的途径，能够有效利用LLM预训练的“化石燃料”。该方法显著提高了数据效率，并在下游任务中带来了显著的性能提升，为解决LLM数据短缺问题提供了有效方案。

Abstract: High-quality pretraining data is the fossil fuel of large language models
(LLMs), yet its reserves are running low for frontier models. In this paper, we
introduce RePro, a novel web recycling method that trains a relatively small LM
with reinforcement learning to generate effective and faithful rephrasings of
pretraining data. Specifically, we design one quality reward and three
faithfulness rewards, optimizing the LM rephraser to convert organic data into
high-quality rephrasings while maintaining its core semantics and structure. In
our experiment, we train a 4B rephraser to recycle 72B tokens sampled from
DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that
RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on
22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web
recycling method that prompts a 70B rephraser, as well as the organic baseline
with a 4x larger data pool. Experiments with different amounts of recycled data
highlight that RePro improves organic data efficiency by 2-3x. Individual and
distributional analyses validate that RePro preserves more critical information
and faithfully reflects the characteristics of organic data compared to
prompting-based methods. Together, these results show that RePro provides an
efficient and controllable path to effectively harness the fossil fuel of LLM
pretraining. We open-source our code, rephraser, and recycled data at
https://github.com/cxcscmu/RePro.

</details>


### [378] [Toward Human-Centered Readability Evaluation](https://arxiv.org/abs/2510.10801)
*Bahar İlgen,Georges Hattab*

Main category: cs.CL

TL;DR: 本研究提出了一种以人为中心的五维度可读性评分框架（HCRS），用于评估健康文本简化，该框架结合了自动化测量和结构化人工反馈，旨在解决现有NLP指标在捕获健康信息关键的人文品质方面的不足。


<details>
  <summary>Details</summary>
Motivation: 公共卫生信息的文本简化对于不同人群至关重要，但BLEU、FKGL、SARI等现有NLP评估指标主要关注表面特征，未能捕捉清晰度、可信度、语气、文化相关性和可操作性等以人为中心的品质，这在高度敏感的健康语境中尤为关键。

Method: 本研究提出了“以人为中心的可读性评分”（HCRS），这是一个基于人机交互（HCI）和健康传播研究的五维度评估框架。HCRS将自动化测量与结构化人工反馈相结合，以捕捉可读性的关系和语境方面。论文概述了该框架，讨论了其如何整合到参与式评估工作流程中，并提出了一个实证验证协议。

Result: 本研究的主要成果是提出了HCRS框架，这是一个结合了自动化测量和结构化人工反馈的五维度评估工具，旨在超越表面指标来评估健康文本简化。该框架及其整合与验证协议被详细阐述。

Conclusion: HCRS旨在推动健康文本简化评估超越表面指标，从而使NLP系统能够更紧密地符合不同用户的需求、期望和生活经验。

Abstract: Text simplification is essential for making public health information
accessible to diverse populations, including those with limited health
literacy. However, commonly used evaluation metrics in Natural Language
Processing (NLP), such as BLEU, FKGL, and SARI, mainly capture surface-level
features and fail to account for human-centered qualities like clarity,
trustworthiness, tone, cultural relevance, and actionability. This limitation
is particularly critical in high-stakes health contexts, where communication
must be not only simple but also usable, respectful, and trustworthy. To
address this gap, we propose the Human-Centered Readability Score (HCRS), a
five-dimensional evaluation framework grounded in Human-Computer Interaction
(HCI) and health communication research. HCRS integrates automatic measures
with structured human feedback to capture the relational and contextual aspects
of readability. We outline the framework, discuss its integration into
participatory evaluation workflows, and present a protocol for empirical
validation. This work aims to advance the evaluation of health text
simplification beyond surface metrics, enabling NLP systems that align more
closely with diverse users' needs, expectations, and lived experiences.

</details>


### [379] [Large Language Models for Full-Text Methods Assessment: A Case Study on Mediation Analysis](https://arxiv.org/abs/2510.10762)
*Wenqing Zhang,Trang Nguyen,Elizabeth A. Stuart,Yiqun T. Chen*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）在系统综述中提取方法学信息的能力，发现LLMs在处理明确、直接的信息时表现出色，但在复杂、需要推断的任务上仍需人类专家监督。


<details>
  <summary>Details</summary>
Motivation: 系统综述在综合科学证据方面至关重要，但其劳动密集型，尤其是在提取详细方法学信息时。大型语言模型（LLMs）有望自动化方法学评估，从而改变证据综合过程。

Method: 研究选取因果中介分析作为代表性方法学领域，在180篇科学全文文章上，将最先进的LLMs与人类专家评审员进行了基准测试。评估指标包括准确性相关性（accuracy correlation）和F1相关性（F1 correlation）。

Result: LLMs的性能与人类判断密切相关（准确性相关性0.71；F1相关性0.97），在直接、明确表述的方法学标准上达到了接近人类的准确性。然而，在复杂、需要推断的评估任务上，准确性急剧下降，比专家评审员低15%。错误通常源于表面语言线索，例如模型常将“纵向”或“敏感性”等关键词误解为严格方法学方法的自动证据。较长的文档导致模型准确性下降，而发表年份没有显著影响。

Conclusion: 当前LLMs在识别明确的方法学特征方面表现出色，但对于细致的解释仍需要人类监督。将自动化信息提取与有针对性的专家评审相结合，为提高跨科学领域证据综合的效率和方法学严谨性提供了一种有前景的方法。

Abstract: Systematic reviews are crucial for synthesizing scientific evidence but
remain labor-intensive, especially when extracting detailed methodological
information. Large language models (LLMs) offer potential for automating
methodological assessments, promising to transform evidence synthesis. Here,
using causal mediation analysis as a representative methodological domain, we
benchmarked state-of-the-art LLMs against expert human reviewers across 180
full-text scientific articles. Model performance closely correlated with human
judgments (accuracy correlation 0.71; F1 correlation 0.97), achieving
near-human accuracy on straightforward, explicitly stated methodological
criteria. However, accuracy sharply declined on complex, inference-intensive
assessments, lagging expert reviewers by up to 15%. Errors commonly resulted
from superficial linguistic cues -- for instance, models frequently
misinterpreted keywords like "longitudinal" or "sensitivity" as automatic
evidence of rigorous methodological approache, leading to systematic
misclassifications. Longer documents yielded lower model accuracy, whereas
publication year showed no significant effect. Our findings highlight an
important pattern for practitioners using LLMs for methods review and synthesis
from full texts: current LLMs excel at identifying explicit methodological
features but require human oversight for nuanced interpretations. Integrating
automated information extraction with targeted expert review thus provides a
promising approach to enhance efficiency and methodological rigor in evidence
synthesis across diverse scientific fields.

</details>


### [380] [HiligayNER: A Baseline Named Entity Recognition Model for Hiligaynon](https://arxiv.org/abs/2510.10776)
*James Ald Teves,Ray Daniel Cal,Josh Magdiel Villaluz,Jean Malolos,Mico Magtira,Ramon Rodriguez,Mideth Abisado,Joseph Marvin Imperial*

Main category: cs.CL

TL;DR: 本研究推出了Hiligaynon语言的第一个公开可用的命名实体识别（NER）基线模型HiligayNER，并在Transformer模型上实现了超过80%的F1分数，同时展示了良好的跨语言迁移能力。


<details>
  <summary>Details</summary>
Motivation: Hiligaynon语言在语言处理研究中代表性不足，缺乏标注语料库和基线模型，阻碍了其语言技术的发展。

Method: 研究收集了超过8,000个来自新闻文章、社交媒体和文学文本的标注句子，构建了HiligayNER数据集。在此语料库上微调了两个基于Transformer的模型：mBERT和XLM-RoBERTa。

Result: 评估结果显示，两个模型在所有实体类型上都取得了超过80%的精确率、召回率和F1分数。此外，与宿雾语和他加禄语的跨语言评估表明，该模型具有良好的可迁移性。

Conclusion: 这项工作旨在为代表性不足的菲律宾语言（特别是Hiligaynon）的语言技术发展做出贡献，并支持未来区域语言处理的研究。

Abstract: The language of Hiligaynon, spoken predominantly by the people of Panay
Island, Negros Occidental, and Soccsksargen in the Philippines, remains
underrepresented in language processing research due to the absence of
annotated corpora and baseline models. This study introduces HiligayNER, the
first publicly available baseline model for the task of Named Entity
Recognition (NER) in Hiligaynon. The dataset used to build HiligayNER contains
over 8,000 annotated sentences collected from publicly available news articles,
social media posts, and literary texts. Two Transformer-based models, mBERT and
XLM-RoBERTa, were fine-tuned on this collected corpus to build versions of
HiligayNER. Evaluation results show strong performance, with both models
achieving over 80% in precision, recall, and F1-score across entity types.
Furthermore, cross-lingual evaluation with Cebuano and Tagalog demonstrates
promising transferability, suggesting the broader applicability of HiligayNER
for multilingual NLP in low-resource settings. This work aims to contribute to
language technology development for underrepresented Philippine languages,
specifically for Hiligaynon, and support future research in regional language
processing.

</details>


### [381] [Review of Inference-Time Scaling Strategies: Reasoning, Search and RAG](https://arxiv.org/abs/2510.10787)
*Zhichao Wang,Cheng Wan,Dong Nie*

Main category: cs.CL

TL;DR: 由于高质量训练数据日益稀缺，大型语言模型（LLMs）的性能提升瓶颈已从模型规模和训练数据转向推理时间扩展。本文系统综述了推理时间扩展的各种技术，将其分为输出导向和输入导向两大类。


<details>
  <summary>Details</summary>
Motivation: LLMs的性能提升传统上依赖于模型规模和训练数据的扩大。然而，高质量训练数据日益枯竭，成为了一个根本性瓶颈，促使研究重点转向在部署时利用额外计算来显著提升LLM性能，而无需昂贵的模型重新训练。

Method: 本文通过系统性综述的方式，将快速发展的推理时间扩展领域划分为两个综合视角：输出导向方法和输入导向方法。输出导向技术包括复杂的多步生成策略（如CoT、ToT、ReAct等推理方法）、各种搜索和解码方法（如MCTS、beam search）、长CoT训练（如RLVR、GRPO）以及模型集成方法。输入导向技术主要分为少样本学习（few-shot）和检索增强生成（RAG），其中RAG是核心焦点，并进一步详细考察了查询扩展、数据、检索与重排器、LLM生成方法和多模态RAG。

Result: 本文提供了一个对推理时间扩展技术的系统性调查和分类框架。它将多样化的技术归纳为输出导向（涉及多步生成、推理、搜索、解码和集成）和输入导向（主要集中于少样本和RAG及其组件）两大类，清晰地组织了这一快速发展的研究领域。

Conclusion: 推理时间扩展是LLMs性能提升的新范式，它通过在部署时增加计算来规避训练数据瓶颈。本文通过系统性地将现有技术分为输出导向和输入导向方法，为理解和进一步发展这一领域提供了全面的视角和结构化的分类。

Abstract: The performance gains of LLMs have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve LLM
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and decoding
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, LLM generation methods, and multi-modal RAG.

</details>


### [382] [Happiness is Sharing a Vocabulary: A Study of Transliteration Methods](https://arxiv.org/abs/2510.10827)
*Haeji Jung,Jinju Kim,Kyungjin Kim,Youjeong Roh,David R. Mortensen*

Main category: cs.CL

TL;DR: 本研究通过受控实验评估了共享字符集、词汇和音韵对多语言模型性能的影响，发现在命名实体识别和自然语言推理任务中，罗马化（romanization）的音译方法表现最佳，这主要得益于与预训练语言共享更长的子词单元。


<details>
  <summary>Details</summary>
Motivation: 音译已成为弥合多语言自然语言处理中语言鸿沟的有前景方法，尤其对于非拉丁语系语言。本研究旨在探究共享字符集、重叠词汇和共享音韵在多语言模型性能中各自的贡献程度。

Method: 研究采用了受控实验，使用三种音译方法（罗马化、音素转录和替换密码）以及正字法作为输入类型。模型在命名实体识别（NER）和自然语言推理（NLI）两个下游任务上进行评估。

Result: 在8个评估设置中，罗马化在7个设置中显著优于其他输入类型。进一步分析表明，与预训练语言共享更长的（子词）词元有助于更好地利用模型，从而提升性能。

Conclusion: 罗马化是音译中最有效的方法，其成功主要归因于它能与预训练语言共享更长的子词单元，从而使模型得到更好的利用。这与研究假设基本一致。

Abstract: Transliteration has emerged as a promising means to bridge the gap between
various languages in multilingual NLP, showing promising results especially for
languages using non-Latin scripts. We investigate the degree to which shared
script, overlapping token vocabularies, and shared phonology contribute to
performance of multilingual models. To this end, we conduct controlled
experiments using three kinds of transliteration (romanization, phonemic
transcription, and substitution ciphers) as well as orthography. We evaluate
each model on two downstream tasks -- named entity recognition (NER) and
natural language inference (NLI) -- and find that romanization significantly
outperforms other input types in 7 out of 8 evaluation settings, largely
consistent with our hypothesis that it is the most effective approach. We
further analyze how each factor contributed to the success, and suggest that
having longer (subword) tokens shared with pre-trained languages leads to
better utilization of the model.

</details>


### [383] [DUAL-Bench: Measuring Over-Refusal and Robustness in Vision-Language Models](https://arxiv.org/abs/2510.10846)
*Kaixuan Ren,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了DUAL-Bench，首个旨在评估视觉语言模型（VLMs）在多模态情境中过度拒绝和安全完成能力的基准，发现现有模型在这方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型能力增强，如何在安全性和实用性之间取得平衡成为核心挑战。现有安全机制可能导致过度拒绝（即模型过度谨慎地拒绝良性请求）。此外，双重用途场景（指令无害但图像有害）引入了独特的挑战，现有模型在此类场景中表现不佳，凸显了对更精细对齐的需求。

Method: 研究者创建了DUAL-Bench，这是第一个专注于VLMs过度拒绝和安全完成的多模态基准。他们评估了18个VLM，涵盖12个危害类别，并特别关注模型在语义保持的视觉扰动下的鲁棒性。

Result: 评估结果显示，模型仍有很大的改进空间。GPT-5-Nano实现了12.9%的安全完成率，GPT-5模型平均为7.9%，而Qwen模型仅为3.9%。

Conclusion: DUAL-Bench有望促进更精细的对齐策略的发展，以确保模型在复杂的多模态设置中既安全又实用，从而解决现有模型过度拒绝和不安全完成的问题。

Abstract: As vision-language models become increasingly capable, maintaining a balance
between safety and usefulness remains a central challenge. Safety mechanisms,
while essential, can backfire, causing over-refusal, where models decline
benign requests out of excessive caution. Yet, no existing benchmark has
systematically addressed over-refusal in the visual modality. This setting
introduces unique challenges, such as dual-use cases where an instruction is
harmless, but the accompanying image contains harmful content. Models
frequently fail in such scenarios, either refusing too conservatively or
completing tasks unsafely, which highlights the need for more fine-grained
alignment. The ideal behavior is safe completion, i.e., fulfilling the benign
parts of a request while explicitly warning about any potentially harmful
elements. To address this, we present DUAL-Bench, the first multimodal
benchmark focused on over-refusal and safe completion in VLMs. We evaluated 18
VLMs across 12 hazard categories, with focus on their robustness under
semantics-preserving visual perturbations. The results reveal substantial room
for improvement: GPT-5-Nano achieves 12.9% safe completion, GPT-5 models
average 7.9%, and Qwen models only 3.9%. We hope that DUAL-Bench will foster
the development of more nuanced alignment strategies that ensure models remain
both safe and useful in complex multimodal settings.

</details>


### [384] [LLM$\times$MapReduce-V3: Enabling Interactive In-Depth Survey Generation through a MCP-Driven Hierarchically Modular Agent System](https://arxiv.org/abs/2510.10890)
*Yu Chao,Siyu Lin,xiaorong wang,Zhu Zhang,Zihan Zhou,Haoyu Wang,Shuo Wang,Jie Zhou,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 本文介绍了LLM x MapReduce-V3，一个用于生成长篇综述的层级模块化智能体系统，它采用MCP（Model-Context-Protocol）服务器实现多智能体架构和动态规划，在内容深度和长度上超越了基线。


<details>
  <summary>Details</summary>
Motivation: 研究动机是有效生成长篇综述，并在前一版本（LLM x MapReduce-V2）的基础上进行改进，特别是通过引入模块化和人机协作来提高综述的生成质量和用户控制力。

Method: 该系统采用层级模块化智能体架构，将骨架初始化、摘要构建和骨架细化等功能组件实现为独立的MCP服务器。这些原子服务器可聚合为更高级别的服务器，形成层级结构。一个高级规划智能体根据MCP工具描述和执行历史动态编排工作流，通过多轮交互捕获研究视角，生成综合骨架，并最终发展为深入综述。该方法还支持人机协作干预。

Result: 人体评估结果表明，该系统在内容深度和长度方面均超越了代表性基线，突显了基于MCP的模块化规划的优势。

Conclusion: LLM x MapReduce-V3系统通过其基于MCP的模块化和层级架构，能有效生成长篇综述，提供卓越的内容深度和长度，并支持人机干预，从而在综述生成方面表现出色。

Abstract: We introduce LLM x MapReduce-V3, a hierarchically modular agent system
designed for long-form survey generation. Building on the prior work, LLM x
MapReduce-V2, this version incorporates a multi-agent architecture where
individual functional components, such as skeleton initialization, digest
construction, and skeleton refinement, are implemented as independent
model-context-protocol (MCP) servers. These atomic servers can be aggregated
into higher-level servers, creating a hierarchically structured system. A
high-level planner agent dynamically orchestrates the workflow by selecting
appropriate modules based on their MCP tool descriptions and the execution
history. This modular decomposition facilitates human-in-the-loop intervention,
affording users greater control and customization over the research process.
Through a multi-turn interaction, the system precisely captures the intended
research perspectives to generate a comprehensive skeleton, which is then
developed into an in-depth survey. Human evaluations demonstrate that our
system surpasses representative baselines in both content depth and length,
highlighting the strength of MCP-based modular planning.

</details>


### [385] [Evaluating Language Models' Evaluations of Games](https://arxiv.org/abs/2510.10930)
*Katherine M. Collins,Cedegao E. Zhang,Graham Todd,Lance Ying,Mauricio Barba da Costa,Ryan Liu,Prafull Sharma,Adrian Weller,Ionatan Kuperwajs,Lionel Wong,Joshua B. Tenenbaum,Thomas L. Griffiths*

Main category: cs.CL

TL;DR: 本文提出了一种评估AI系统“评估能力”的新范式，而非仅仅关注其问题解决能力。通过比较AI模型对100多种新棋盘游戏的“回报/公平性”和“趣味性”评估与人类判断，发现推理模型更接近人类，但与博弈论最优越接近，与人类数据的拟合度越弱。此外，推理模型在评估过程中资源使用高度可变。


<details>
  <summary>Details</summary>
Motivation: 传统的AI系统评估主要集中于问题解决能力（如国际象棋和围棋），但推理不仅包括解决问题，还包括评估哪些问题值得解决。因此，研究人员希望引入一种新的范式，评估AI系统对游戏的评估能力。

Method: 1. 引入了一种评估评估的形式主义。2. 利用包含100多个新棋盘游戏和450多个人类判断的大规模数据集。3. 将现代语言和推理模型产生的评估与人类以及符号计算代理的评估进行比较。4. 考虑两种评估查询：评估游戏的“回报（或公平性）”和“趣味性”，这些查询在计算复杂度和量化难度上有所不同。

Result: 1. 推理模型在游戏评估上通常比非推理语言模型更符合人类判断。2. 观察到一种非单调关系：模型越接近博弈论最优，其与人类数据的拟合度越弱。3. 在评估“趣味性”时，模型之间表现出更大的“锯齿状”差异，这与该查询更难量化相符。4. 跨查询和游戏，推理模型在评估查询时显示出高度可变且不可预测的资源使用。

Conclusion: 当前AI评估需要将重点转移到其评估能力上。AI模型需要注入更具资源理性（resource-rational）的元推理能力，以提高其评估能力，尤其是在评估如“趣味性”等难以量化的方面。

Abstract: Reasoning is not just about solving problems -- it is also about evaluating
which problems are worth solving at all. Evaluations of artificial intelligence
(AI) systems primarily focused on problem solving, historically by studying how
models play games such as chess and Go. In this paper, we advocate for a new
paradigm that assesses AI systems' evaluation of games. First, we introduce a
formalism for evaluating such evaluations. We then leverage a large-scale
dataset of over $100$ novel board games and over 450 human judgments to compare
evaluations produced by modern language and reasoning models against those of
people and symbolic computational agents. We consider two kinds of evaluative
queries: assessing the payoff (or fairness) and the funness of games. These
queries span two dimensions relevant to the design of evaluations of AI
evaluations: how complex a query is to compute and how difficult a query is to
quantify. Our results show that reasoning models are generally more aligned to
people in their evaluations of games than non-reasoning language models.
However, we observe a non-monotonic relationship: as models get closer to
game-theoretic optimal, their fit to human data weakens. We also observe more
"jaggedness" across models for assessing funness, in line with the greater
difficulty of quantifying this query. Across queries and games, reasoning
models show highly variable and unpredictable resource usage when assessing
queries, pointing to the importance of imbuing more resource-rational
meta-reasoning in language and reasoning models.

</details>


### [386] [ADVICE: Answer-Dependent Verbalized Confidence Estimation](https://arxiv.org/abs/2510.10913)
*Ki Jung Seo,Sehun Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 本文分析了大型语言模型（LLMs）过度自信的原因，发现其信心表达与答案独立有关。为此，作者提出了ADVICE微调框架，显著改善了LLMs的信心校准，同时保持了任务性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）能够用自然语言表达自信，提高了透明度和可靠性。然而，它们经常表现出过度自信，其原因尚不清楚，这促使研究人员深入探讨。

Method: 研究人员对口头化信心的动态进行了详细分析，并将“答案独立性”（即模型未能根据自身答案调整信心）识别为一个关键因素。为解决此问题，他们提出了ADVICE（Answer-Dependent Verbalized Confidence Estimation）微调框架，旨在促进基于答案的信心评估。

Result: 广泛的实验表明，ADVICE显著改善了信心校准，同时保持了任务性能。进一步分析证实，ADVICE增强了答案的根植性，从而产生了更平衡、校准更好的信心分布。

Conclusion: 本研究揭示了过度自信的起源，并建立了一个用于更值得信赖的信心口头化表达的框架，提高了LLMs的可靠性。

Abstract: Recent progress in large language models (LLMs) has enabled them to express
their confidence in natural language, enhancing transparency and reliability.
However, their confidence often exhibits overconfidence, the cause of which
remains poorly understood. In this work, we conduct a detailed analysis of the
dynamics underlying verbalized confidence and identify answer-independence as a
key factor, defined as the model's failure to condition confidence on its own
answer. To address this, we propose ADVICE (Answer-Dependent Verbalized
Confidence Estimation), a fine-tuning framework that facilitates
answer-grounded confidence estimation. Extensive experiments show that ADVICE
substantially improves confidence calibration while preserving task
performance. Further analyses confirm that ADVICE strengthens
answer-groundedness, leading to more balanced and well-calibrated confidence
distributions. Our findings shed light on the origin of overconfidence and
establish a framework for more trustworthy confidence verbalization.

</details>


### [387] [KOTOX: A Korean Toxic Dataset for Deobfuscation and Detoxification](https://arxiv.org/abs/2510.10961)
*Yejin Lee,Su-Hyeon Kim,Hyundong Jin,Dayoung Kim,Yeonsoo Kim,Yo-Sub Han*

Main category: cs.CL

TL;DR: 该研究提出了KOTOX，一个针对韩语的去混淆和去毒化数据集，以解决大型语言模型在低资源语言中处理混淆毒性内容的问题。


<details>
  <summary>Details</summary>
Motivation: 随着在线交流的迅速扩展，毒性内容已成为一个日益严重的社会问题。现有研究多集中于英语，导致低资源语言（如韩语）的毒性内容检测和去毒化不足，尤其是在用户使用混淆技术规避检测时，大型语言模型（LLMs）在这方面表现不佳。

Method: 提出了KOTOX：一个韩语毒性数据集。该方法基于韩语的语言特征对混淆方法进行分类，并根据真实世界示例定义了一套转换规则。利用这些规则，构建了三个不同混淆难度级别的数据集版本（简单、中等、困难）。

Result: 创建了KOTOX数据集，这是第一个同时支持韩语去混淆和去毒化的数据集。该数据集包含三个不同难度的版本。

Conclusion: 该研究期望KOTOX数据集能够促进大型语言模型更好地理解和缓解低资源语言中混淆的毒性内容。

Abstract: Toxic content has become an increasingly critical social issue with the rapid
expansion of online communication. While numerous studies explored methods for
detecting and detoxifying such content, most have focused primarily on English,
leaving low-resource language underrepresented. Consequently, Large Language
Models~(LLMs) often struggle to identify and neutralize toxic expressions in
these languages. This challenge becomes even more pronounced when user employ
obfuscation techniques to evade detection systems. Therefore, we propose a
\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to
address this issue. We categorize various obfuscation approaches based on
linguistic characteristics of Korean and define a set of transformation rules
grounded in real-word examples. Using these rules, we construct three dataset
versions (easy, normal, and hard) representing different levels of obfuscation
difficulty. This is the first dataset that simultaneously supports
deobfuscation and detoxification for the Korean language. We expect it to
facilitate better understanding and mitigating of obfuscated toxic content in
LLM for low-resource languages. Our code and data are available at
https://github.com/leeyejin1231/KOTOX.

</details>


### [388] [GapDNER: A Gap-Aware Grid Tagging Model for Discontinuous Named Entity Recognition](https://arxiv.org/abs/2510.10927)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: 本文提出了一种名为GapDNER的间断命名实体识别模型，通过对实体片段之间的上下文间隙进行表示学习，并结合网格标注和交互式组件，有效解决了间断实体识别中的解码歧义和复杂结构识别问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学领域中，命名实体可能由非相邻的词元组成并与其他实体重叠。现有方法通过连接实体片段或内部词元来识别间断实体，但面临错误传播和解码歧义的挑战，尤其是在处理多样化的跨度或词组合时。

Method: GapDNER模型深入探索间断实体结构，并提出了一种有效的间隙感知网格标注方法。它创新性地将表示学习应用于实体片段之间的上下文间隙，以解决解码歧义并提高性能。具体而言，它将上下文间隙视为一种额外的跨度类型，并将跨度分类转换为词元对网格标注任务。随后，设计了两个交互式组件：跨度内规律提取模块（使用双仿射机制和线性注意力捕捉每个跨度的内部规律）和跨度间关系增强模块（利用交叉注意力获取不同跨度之间的语义关系）。在实体解码推理阶段，为每个实体片段和上下文间隙分配有向边，然后使用BFS算法搜索所有从网格头部到尾部带有实体标签的有效路径。

Result: 在三个数据集上的实验结果表明，GapDNER在间断命名实体识别上取得了新的最先进性能，并在识别复杂实体结构方面展现出显著优势。

Conclusion: GapDNER通过创新性地利用上下文间隙的表示学习和交互式网格标注机制，有效解决了间断命名实体识别中的解码歧义和复杂结构识别难题，达到了领先的识别性能。

Abstract: In biomedical fields, one named entity may consist of a series of
non-adjacent tokens and overlap with other entities. Previous methods recognize
discontinuous entities by connecting entity fragments or internal tokens, which
face challenges of error propagation and decoding ambiguity due to the wide
variety of span or word combinations. To address these issues, we deeply
explore discontinuous entity structures and propose an effective Gap-aware grid
tagging model for Discontinuous Named Entity Recognition, named GapDNER. Our
GapDNER innovatively applies representation learning on the context gaps
between entity fragments to resolve decoding ambiguity and enhance
discontinuous NER performance. Specifically, we treat the context gap as an
additional type of span and convert span classification into a token-pair grid
tagging task. Subsequently, we design two interactive components to
comprehensively model token-pair grid features from both intra- and inter-span
perspectives. The intra-span regularity extraction module employs the biaffine
mechanism along with linear attention to capture the internal regularity of
each span, while the inter-span relation enhancement module utilizes
criss-cross attention to obtain semantic relations among different spans. At
the inference stage of entity decoding, we assign a directed edge to each
entity fragment and context gap, then use the BFS algorithm to search for all
valid paths from the head to tail of grids with entity tags. Experimental
results on three datasets demonstrate that our GapDNER achieves new
state-of-the-art performance on discontinuous NER and exhibits remarkable
advantages in recognizing complex entity structures.

</details>


### [389] [RV-HATE: Reinforced Multi-Module Voting for Implicit Hate Speech Detection](https://arxiv.org/abs/2510.10971)
*Yejin Lee,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: RV-HATE是一个仇恨言论检测框架，它利用强化学习根据数据集的特定特征自适应地调整检测模块的贡献，从而提高检测准确性并提供可解释的洞察。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论形式多样且不断演变，在互联网上迅速传播，难以检测。现有的仇恨言论数据集因来源和平台不同而呈现出多样化的语言风格和社会背景。然而，以往的检测研究通常依赖固定方法，未能适应这些数据特有的特征，尤其难以有效处理隐性仇恨言论。

Method: RV-HATE框架包含多个专门模块，每个模块专注于仇恨言论的不同语言或上下文特征。它采用强化学习来优化权重，以确定每个模块对给定数据集的贡献。最终，通过投票机制整合各模块的输出以产生最终决策。

Result: RV-HATE通过根据数据集特定属性调整检测过程，提高了检测准确性。它还为每个数据集的独特特征提供了可解释的见解。该方法有效地解决了隐性仇恨言论问题，并比传统静态方法取得了更优越的性能。

Conclusion: RV-HATE通过其自适应的、模块化的方法，有效应对了仇恨言论数据集的多样性挑战，显著提升了检测精度和可解释性，尤其在处理隐性仇恨言论方面表现出色。

Abstract: Hate speech remains prevalent in human society and continues to evolve in its
forms and expressions. Modern advancements in internet and online anonymity
accelerate its rapid spread and complicate its detection. However, hate speech
datasets exhibit diverse characteristics primarily because they are constructed
from different sources and platforms, each reflecting different linguistic
styles and social contexts. Despite this diversity, prior studies on hate
speech detection often rely on fixed methodologies without adapting to
data-specific features. We introduce RV-HATE, a detection framework designed to
account for the dataset-specific characteristics of each hate speech dataset.
RV-HATE consists of multiple specialized modules, where each module focuses on
distinct linguistic or contextual features of hate speech. The framework
employs reinforcement learning to optimize weights that determine the
contribution of each module for a given dataset. A voting mechanism then
aggregates the module outputs to produce the final decision. RV-HATE offers two
primary advantages: (1)~it improves detection accuracy by tailoring the
detection process to dataset-specific attributes, and (2)~it also provides
interpretable insights into the distinctive features of each dataset.
Consequently, our approach effectively addresses implicit hate speech and
achieves superior performance compared to conventional static methods. Our code
is available at https://github.com/leeyejin1231/RV-HATE.

</details>


### [390] [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF: A Reproducibility Study](https://arxiv.org/abs/2510.10936)
*Anirudh Ganesh,Jayavardhan Reddy*

Main category: cs.CL

TL;DR: 本文对Ma和Hovy (2016)提出的BiLSTM-CNN-CRF序列标注模型进行了复现性研究，成功重现了其在命名实体识别（NER）等任务上的卓越性能，并发布了开源PyTorch实现。


<details>
  <summary>Details</summary>
Motivation: Ma和Hovy (2016)提出的BiLSTM-CNN-CRF模型是一种端到端方法，无需人工设计特征即可在命名实体识别（NER）和词性标注（POS）等序列标注任务上取得优异性能，因此对其进行复现性研究和深入分析具有重要价值。

Method: 本文复现了Ma和Hovy (2016)提出的BiLSTM-CNN-CRF模型。该模型结合了卷积神经网络（CNN）提取字符级表示、双向长短期记忆网络（BiLSTM）进行词级上下文建模，以及条件随机场（CRF）进行结构化预测。研究者使用PyTorch实现了该模型。

Result: 研究成功复现了关键结果，在CoNLL-2003 NER任务上取得了91.18%的F1分数，并证明了该模型在各种序列标注任务中的有效性。

Conclusion: Ma和Hovy (2016)提出的BiLSTM-CNN-CRF模型是有效且可复现的。本文提供了对架构组件的详细分析，并发布了一个开源的PyTorch实现，以促进未来的研究。

Abstract: We present a reproducibility study of the state-of-the-art neural
architecture for sequence labeling proposed by Ma and Hovy
(2016)\cite{ma2016end}. The original BiLSTM-CNN-CRF model combines
character-level representations via Convolutional Neural Networks (CNNs),
word-level context modeling through Bi-directional Long Short-Term Memory
networks (BiLSTMs), and structured prediction using Conditional Random Fields
(CRFs). This end-to-end approach eliminates the need for hand-crafted features
while achieving excellent performance on named entity recognition (NER) and
part-of-speech (POS) tagging tasks. Our implementation successfully reproduces
the key results, achieving 91.18\% F1-score on CoNLL-2003 NER and demonstrating
the model's effectiveness across sequence labeling tasks. We provide a detailed
analysis of the architecture components and release an open-source PyTorch
implementation to facilitate further research.

</details>


### [391] [Punctuation-aware treebank tree binarization](https://arxiv.org/abs/2510.10951)
*Eitan Klinger,Vivaan Wadhwa,Jungyeul Park*

Main category: cs.CL

TL;DR: 本文提出了一种标点符号感知的树库二值化资源和评估套件，通过保留标点符号作为兄弟节点，显著提高了头部预测准确性，并改善了与衍生资源的结构兼容性。


<details>
  <summary>Details</summary>
Motivation: 标准的二值化流程在选择头部之前会丢弃标点符号，这会改变成分的形状并损害头部-子节点识别，导致准确性下降。

Method: 本文发布了一个可重现的流水线，该流水线在二值化之前将标点符号保留为兄弟节点。同时，还提供了派生工件和元数据（如中间@X标记、可逆性签名、对齐索引），以及一个评估套件，涵盖头部-子节点预测、往返可逆性以及与衍生资源（如CCGbank）的结构兼容性。

Result: 在宾夕法尼亚树库上，标点符号感知的预处理将头部预测准确率从73.66%（Collins规则）和86.66%（MLP）提高到使用相同分类器的91.85%。此外，它还实现了与CCGbank衍生的竞争性对齐。所有代码、配置文件和文档均已发布，以方便复制和扩展到其他语料库。

Conclusion: 通过在树库二值化过程中保留标点符号，可以显著提高头部预测的准确性并改善与衍生资源的结构兼容性，为自然语言处理领域提供了一个有价值的资源。

Abstract: This article presents a curated resource and evaluation suite for
punctuation-aware treebank binarization. Standard binarization pipelines drop
punctuation before head selection, which alters constituent shape and harms
head-child identification. We release (1) a reproducible pipeline that
preserves punctuation as sibling nodes prior to binarization, (2) derived
artifacts and metadata (intermediate @X markers, reversibility signatures,
alignment indices), and (3) an accompanying evaluation suite covering
head-child prediction, round-trip reversibility, and structural compatibility
with derivational resources (CCGbank). On the Penn Treebank, punctuation-aware
preprocessing improves head prediction accuracy from 73.66\% (Collins rules)
and 86.66\% (MLP) to 91.85\% with the same classifier, and achieves competitive
alignment against CCGbank derivations. All code, configuration files, and
documentation are released to enable replication and extension to other
corpora.

</details>


### [392] [Judge Before Answer: Can MLLM Discern the False Premise in Question?](https://arxiv.org/abs/2510.10965)
*Jidong Li,Lingyong Fang,Haodong Zhao,Sufeng Duan,Gongshen Liu*

Main category: cs.CL

TL;DR: 多模态大语言模型（MLLMs）容易受错误前提影响，但现有基准有限。本文提出了一个自动化流水线来构建全面的错误前提基准JBA数据集，并对前提进行细致分类。结果显示当前MLLMs表现不佳。在此基础上，本文提出了一个识别增强框架，显著提升了MLLMs识别错误前提的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型（MLLMs）取得了显著进展，但它们仍然容易受到错误前提问题的影响。然而，现有针对此问题的基准测试范围有限，缺乏细粒度分类和充分覆盖，无法严格评估模型识别错误前提的能力。

Method: 本文引入了一个全自动流水线，用于构建一个全面的错误前提问题基准（JBA数据集），该方法根据识别前提所需的能力，将前提系统地分为三大类和十三个子类。在此基准之上，进一步提出了一个识别增强框架，旨在增强MLLMs检测错误前提的鲁棒性。

Result: 研究结果表明，当前的MLLMs在错误前提识别方面仍然表现不佳。经过本文提出的框架训练的模型在错误前提识别方面取得了显著改进。

Conclusion: MLLMs在识别错误前提方面面临挑战，现有基准存在不足。本文通过构建细致分类的JBA数据集和提出识别增强框架，有效提升了MLLMs识别错误前提的鲁棒性和能力。

Abstract: Multimodal large language models (MLLMs) have witnessed astonishing
advancements in recent years. Despite these successes, MLLMs remain vulnerable
to flase premise problems. However, existing benchmarks targeting this issue
are limited in scope: they often lack fine-grained categorization, exhibit
insufficient coverage, and thus fail to provide a rigorous evaluation of the
ability of models to recognize false premises. To bridge this gap, we introduce
a fully automated pipeline for constructing a comprehensive benchmark of false
premise questions. Our method systematically categorizes the premises into
three main types and thirteen subtypes according to the abilities required to
identify the premises, resulting in the JBA dataset.Results show current MLLMs
still struggle with false premise recognition. Building upon this benchmark, we
further propose a recognition enhancement framework tailored to strengthen the
robustness of MLLMs to detect false premises. Extensive experiments demonstrate
that models trained with our framework achieve significant improvements in
false premise recognition.

</details>


### [393] [DeepResearchGuard: Deep Research with Open-Domain Evaluation and Multi-Stage Guardrails for Safety](https://arxiv.org/abs/2510.10994)
*Wei-Chieh Huang,Henry Peng Zou,Yaozu Wu,Dongyuan Li,Yankai Chen,Weizhi Zhang,Yangning Li,Angelo Zangari,Jizhou Guo,Chunyu Miao,Liancheng Fang,Langzhou He,Renhe Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: 本文介绍了DEEPRESEARCHGUARD，一个具有四阶段安全防护的深度研究框架，旨在解决现有框架在报告质量评估和安全防护方面的不足，并显著提升报告的安全性、可信度和整体质量。


<details>
  <summary>Details</summary>
Motivation: 现有的深度研究框架在从网络源合成报告时，缺乏足够的评估程序和阶段性保护措施。它们通常只关注问答的精确匹配准确性，而忽视报告质量的关键方面，如可信度、连贯性、广度、深度和安全性，这可能导致有害或恶意来源被整合到最终报告中。

Method: 本文提出了DEEPRESEARCHGUARD框架，该框架具有四阶段安全防护，并对参考文献和报告进行开放域评估。为了评估安全性，引入了DRSAFEBENCH，一个针对深度研究安全的阶段性基准。评估涵盖了多种最先进的LLM，并使用了防御成功率和过度拒绝率等指标以及五个关键报告维度。

Result: DEEPRESEARCHGUARD框架将平均防御成功率提高了18.16%，同时将过度拒绝率降低了6%。其中，输入防护在早期阶段提供了最实质性的保护，过滤掉了明显的风险；计划和研究防护则增强了引用规范和来源可信度。

Conclusion: DEEPRESEARCHGUARD框架通过全面的开放域评估和阶段性防御，有效阻止了有害内容的传播，并在不过度拒绝的情况下系统地提高了报告质量。该框架能够实现综合的开放域评估和阶段性防御，有效阻断有害内容传播，同时系统地提升报告质量而不会产生过高的过度拒绝率。

Abstract: Deep research frameworks have shown promising capabilities in synthesizing
comprehensive reports from web sources. While deep research possesses
significant potential to address complex issues through planning and research
cycles, existing frameworks are deficient in sufficient evaluation procedures
and stage-specific protections. They typically treat evaluation as exact match
accuracy of question-answering, but overlook crucial aspects of report quality
such as credibility, coherence, breadth, depth, and safety. This oversight may
result in hazardous or malicious sources being integrated into the final
report. To address these issues, we introduce DEEPRESEARCHGUARD, a
comprehensive framework featuring four-stage safeguards with open-domain
evaluation of references and reports. We assess performance across multiple
metrics, e.g., defense success rate and over-refusal rate, and five key report
dimensions. In the absence of a suitable safety benchmark, we introduce
DRSAFEBENCH, a stage-wise benchmark for deep research safety. Our evaluation
spans diverse state-of-the-art LLMs, including GPT-4o, Gemini-2.5-flash,
DeepSeek-v3, and o4-mini. DEEPRESEARCHGUARD achieves an average defense success
rate improvement of 18.16% while reducing over-refusal rate by 6%. The input
guard provides the most substantial early-stage protection by filtering out
obvious risks, while the plan and research guards enhance citation discipline
and source credibility. Through extensive experiments, we show that
DEEPRESEARCHGUARD enables comprehensive open-domain evaluation and stage-aware
defenses that effectively block harmful content propagation, while
systematically improving report quality without excessive over-refusal rates.
The code can be found via https://github.com/Jasonya/DeepResearchGuard.

</details>


### [394] [Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures](https://arxiv.org/abs/2510.10806)
*Mihir Gupte,Paolo Giusto,Ramesh S*

Main category: cs.CL

TL;DR: 处理失败


<details>
  <summary>Details</summary>
Motivation: 处理失败

Method: 处理失败

Result: 处理失败

Conclusion: 处理失败

Abstract: Large Language Models (LLMs) are adept at generating responses based on
information within their context. While this ability is useful for interacting
with structured data like code files, another popular method,
Retrieval-Augmented Generation (RAG), retrieves relevant documents to augment
the model's in-context learning. However, it is not well-explored how to best
represent this retrieved knowledge for generating responses on structured data,
particularly hierarchical structures like trees. In this work, we propose a
novel bottom-up method to linearize knowledge from tree-like structures (like a
GitHub repository) by generating implicit, aggregated summaries at each
hierarchical level. This approach enables the knowledge to be stored in a
knowledge base and used directly with RAG. We then compare our method to using
RAG on raw, unstructured code, evaluating the accuracy and quality of the
generated responses. Our results show that while response quality is comparable
across both methods, our approach generates over 68% fewer documents in the
retriever, a significant gain in efficiency. This finding suggests that
leveraging implicit, linearized knowledge may be a highly effective and
scalable strategy for handling complex, hierarchical data structures.

</details>


### [395] [STEAM: A Semantic-Level Knowledge Editing Framework for Large Language Models](https://arxiv.org/abs/2510.10398)
*Geunyeong Jeong,Juoh Sun,Seonghee Lee,Harksoo Kim*

Main category: cs.CL

TL;DR: 处理失败


<details>
  <summary>Details</summary>
Motivation: 处理失败

Method: 处理失败

Result: 处理失败

Conclusion: 处理失败

Abstract: Large Language Models store extensive factual knowledge acquired during
large-scale pre-training. However, this knowledge is inherently static,
reflecting only the state of the world at the time of training. Knowledge
editing has emerged as a promising solution for updating outdated or incorrect
facts without full retraining. However, most existing locate-and-edit methods
primarily focus on token-level likelihood optimization without addressing
semantic coherence. Our analysis reveals that such edited knowledge is often
encoded as isolated residual streams in the model's latent space, distinct from
pre-existing knowledge and bypassing natural reasoning process. To address
this, we propose \textsc{Steam}, a semantic-level knowledge editing framework
that enhances integration of updated knowledge into the model's knowledge
structure. \textsc{Steam} first identifies target representations as semantic
anchors for the updated factual association, then guides the internal
representation of the edited fact towards these anchors through an alignment
loss during optimization. Experimental results demonstrate that \textsc{Steam}
improves model's ability to reason with edited knowledge and enhances semantic
coherence, underscoring the importance of latent-space alignment for reliable
and coherent knowledge editing. The code is available at
https://github.com/GY-Jeong/STEAM.

</details>


### [396] [DND: Boosting Large Language Models with Dynamic Nested Depth](https://arxiv.org/abs/2510.11001)
*Tieyuan Chen,Xiaodong Chen,Haoxing Chen,Zhenzhong Lan,Weiyao Lin,Jianguo Li*

Main category: cs.CL

TL;DR: DND是一种新方法，通过在Transformer层末端动态选择并以嵌套深度方式重新处理关键token，从而提升现有LLM的性能，同时保持计算量和参数量的最小增长。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理复杂token时可能效率不高，而对所有token进行重复计算又会导致冗余。研究旨在找到一种方法，能有选择性地“复习”困难token，避免对简单token进行不必要的计算，从而在提高性能的同时优化计算效率。

Method: 引入动态嵌套深度（DND）方法。在Transformer层的末端，DND使用一个路由器识别出更关键的token，并将其送回进行额外的处理轮次（嵌套深度）。为了精确控制，DND采用了两种策略：一个控制损失的路由器以增强token选择的可区分性，以及一个阈值控制方案以确保选择的稳定性。该方法在训练后阶段直接集成到预训练的密集型和MoE模型中。

Result: DND在多样化的基准测试中展现出有效性。它使密集型Qwen3-1.7B的性能提升了1.88%，使MoE Qwen3-30B-A3B的性能提升了0.87%，所有这些都只伴随着极小的参数和计算量增加。

Conclusion: DND通过动态选择并重新处理关键token，成功提升了现有LLM的性能，且增益显著而成本极低。这证明了其作为一种高效、低成本的LLM性能增强方案的潜力。

Abstract: We introduce Dynamic Nested Depth (DND), a novel method that improves
performance for off-the-shelf LLMs by selecting critical tokens to reprocess in
a nested depth manner. Specifically, at the end of the given transformer layer,
DND identifies more critical tokens with a router and feeds them back for an
extra round of processing, effectively ``reviewing" difficult tokens while
avoiding redundant computation for easier ones. The dynamic selection mechanism
is tailored for precise control via two novel strategies: a router controlling
loss to enhance token selection distinguishability, and a threshold control
scheme to ensure selection stability. We demonstrate the effectiveness of DND
by directly integrating it into pre-trained dense and MoE models during a
post-training phase. On diverse benchmarks, this approach boosts the
performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by
0.87%, all with a minimal parameter and computing increase.

</details>


### [397] [LogiNumSynth: Synthesizing Joint Logical-Numerical Reasoning Problems for Language Models](https://arxiv.org/abs/2510.11031)
*Yiwei Liu,Yucheng Li,Xiao Li,Gong Cheng*

Main category: cs.CL

TL;DR: LogiNumSynth是一个灵活的自然语言问题合成器，用于生成可控的逻辑-数值联合推理任务，以诊断和提升语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型在逻辑-数值联合推理方面仍面临重大挑战。现有数据集依赖固定规则集，对任务复杂度的控制有限，影响了评估和训练的泛化能力。

Method: 提出LogiNumSynth，一个灵活的自然语言问题合成器。它能合成需要逻辑推理（如基于规则的推理）和数值推理（如算术计算）的任务。LogiNumSynth支持细粒度控制推理世界丰富度、逻辑推理深度和数值计算复杂度，从而能生成不同难度级别的数据。

Result: LogiNumSynth能够合成完全可控的联合推理任务；支持过程准确性和答案准确性的评估；使用合成数据可以提升大型语言模型的推理性能。实验揭示了LLMs在逻辑-数值推理方面持续存在的弱点。

Conclusion: LogiNumSynth可以作为诊断工具，同时也是提供有针对性监督的来源，以促进语言模型集成推理技能的进步。

Abstract: Joint logical-numerical reasoning remains a major challenge for language
models, yet existing datasets rely on fixed rule sets and offer limited control
over task complexity, constraining their generalizability for evaluation and
training. We present LogiNumSynth, a flexible natural language problem
synthesizer that synthesizes tasks requiring proficiency in joint logical
reasoning (e.g., rule-based reasoning) and numerical reasoning (e.g.,
arithmetic computation). LogiNumSynth supports fine-grained control over
reasoning world richness, logical reasoning depth, and the complexity of
numerical computations, enabling flexible data synthesis across difficulty
levels. We demonstrate three key contributions: (1) Synthesizer -- synthesizing
fully controllable joint reasoning tasks over natural language; (2) Evaluation
& Process Analysis -- evaluating both process accuracy and answer accuracy; (3)
Targeted Training -- using synthesized data to enhance LLMs' reasoning
performance. Experiments with multiple LLMs highlight persistent weaknesses in
logical-numerical reasoning, showing that LogiNumSynth can serve as both a
diagnostic tool and a source of targeted supervision for advancing integrated
reasoning skills.

</details>


### [398] [Enabling Doctor-Centric Medical AI with LLMs through Workflow-Aligned Tasks and Benchmarks](https://arxiv.org/abs/2510.11040)
*Wenya Xie,Qingying Xiao,Yu Zheng,Xidong Wang,Junying Chen,Ke Ji,Anningzhe Gao,Prayag Tiwari,Xiang Wan,Feng Jiang,Benyou Wang*

Main category: cs.CL

TL;DR: 本研究提出将大型语言模型（LLMs）定位为临床助手，而非直接面向患者。为此，构建了DoctorFLAN，一个包含92,000个问答实例的大规模中文医疗数据集，并开发了评估基准。实验证明DoctorFLAN显著提升了开源LLMs在医疗领域的表现，使其更好地适应医生工作流程。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型（LLMs）在提供临床指导方面具有潜力，但由于其领域专业知识有限，直接将其部署给患者存在安全风险。因此，需要将其重新定位为与经验丰富的医生协作的临床助手，以降低风险并更好地服务医疗工作流程。

Method: 研究方法包括：1) 进行两阶段的启发-反馈调查，以识别临床工作流程中的实际需求。2) 基于调查结果，构建DoctorFLAN，一个包含92,000个问答实例的大规模中文医疗数据集，涵盖22个临床任务和27个专科。3) 引入DoctorFLAN-test（550个单轮问答）和DotaBench（74个多轮对话）作为评估基准，以衡量模型在医生应用中的表现。4) 使用十余种流行LLMs进行实验评估。

Result: 实验结果表明，DoctorFLAN显著提高了开源LLMs在医疗语境下的性能。这有助于LLMs更好地与医生工作流程对齐，并补充了现有的以患者为导向的模型。

Conclusion: 本研究为推进以医生为中心的医疗LLM开发贡献了宝贵的资源和框架。DoctorFLAN数据集及其相关工作有助于将LLMs安全有效地整合到临床实践中，作为医生的有力辅助工具。

Abstract: The rise of large language models (LLMs) has transformed healthcare by
offering clinical guidance, yet their direct deployment to patients poses
safety risks due to limited domain expertise. To mitigate this, we propose
repositioning LLMs as clinical assistants that collaborate with experienced
physicians rather than interacting with patients directly. We conduct a
two-stage inspiration-feedback survey to identify real-world needs in clinical
workflows. Guided by this, we construct DoctorFLAN, a large-scale Chinese
medical dataset comprising 92,000 Q&A instances across 22 clinical tasks and 27
specialties. To evaluate model performance in doctor-facing applications, we
introduce DoctorFLAN-test (550 single-turn Q&A items) and DotaBench (74
multi-turn conversations). Experimental results with over ten popular LLMs
demonstrate that DoctorFLAN notably improves the performance of open-source
LLMs in medical contexts, facilitating their alignment with physician workflows
and complementing existing patient-oriented models. This work contributes a
valuable resource and framework for advancing doctor-centered medical LLM
development

</details>


### [399] [Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States](https://arxiv.org/abs/2510.11052)
*Qinglin Zhu,Yizhen Yao,Runcong Zhao,Yanzheng Xiang,Amrutha Saseendran,Chen Jin,Philip Alexander Teare,Bin Liang,Yulan He,Lin Gui*

Main category: cs.CL

TL;DR: 本文提出了一种名为潜在精炼解码（LRD）的两阶段框架，用于并行序列生成。它通过潜在精炼和预测反馈循环解决了自回归模型的高延迟和扩散模型的信息丢失及过早承诺问题，显著提高了生成任务的准确性，并实现了高达10.6倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 自回归（AR）模型在自然语言生成中因严格的顺序解码而导致高延迟。最近的扩散启发方法（如LlaDA和Dream）通过并行生成来缓解此问题，但存在两个核心局限性：信息丢失（非最终化token的预测分布在每一步被丢弃）和过早承诺（在缺乏全局协调的情况下做出局部决策）。

Method: 本文引入了潜在精炼解码（LRD），一个包含潜在精炼（Latent Refinement）和预测反馈循环（Predictive Feedback Loop）的两阶段框架。第一阶段将掩码位置维护为预测token和掩码嵌入的分布混合，以建立更全局一致的信念。第二阶段逐步确定置信度高的token，同时保留不确定的token进行迭代反馈。KL散度动态被用作收敛和提前停止的可靠准则。

Result: 实验结果表明，LRD在编码（HumanEval +6.3，MBPP +2.6）和推理（GSM8K +2.9，MATH500 +3.8）任务上都提高了准确性，同时实现了高达10.6倍的速度提升。

Conclusion: LRD作为一种强大且多功能的并行序列生成替代方案，在提高准确性的同时显著提升了生成速度，有效解决了现有方法的局限性。

Abstract: Autoregressive (AR) models remain the standard for natural language
generation but still suffer from high latency due to strictly sequential
decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream,
mitigate this by generating in parallel, yet they suffer from two core
limitations: information loss, as predictive distributions for non-finalized
tokens are discarded at each step, and premature commitment, where local
decisions are made without sufficient global coordination. We introduce Latent
Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a
Predictive Feedback Loop. The first stage maintains masked positions as
distributional mixtures of predicted tokens and the mask embedding, allowing
the model to establish more globally consistent beliefs. The second stage
progressively finalizes confident tokens while retaining uncertain ones for
iterative feedback. KL-divergence dynamics provide a principled and reliable
criterion for convergence and early stopping. Experiments across coding
(HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that
LRD improves accuracy while delivering speedups of up to 10.6x, making it a
strong and versatile alternative for parallel sequence generation.

</details>


### [400] [Rethinking Agentic Workflows: Evaluating Inference-Based Test-Time Scaling Strategies in Text2SQL Tasks](https://arxiv.org/abs/2510.10885)
*Jiajing Guo,Kenil Patel,Jorge Piazentin Ono,Wenbin He,Liu Ren*

Main category: cs.CL

TL;DR: 处理失败


<details>
  <summary>Details</summary>
Motivation: 处理失败

Method: 处理失败

Result: 处理失败

Conclusion: 处理失败

Abstract: Large language models (LLMs) are increasingly powering Text-to-SQL (Text2SQL)
systems, enabling non-expert users to query industrial databases using natural
language. While test-time scaling strategies have shown promise in LLM-based
solutions, their effectiveness in real-world applications, especially with the
latest reasoning models, remains uncertain. In this work, we benchmark six
lightweight, industry-oriented test-time scaling strategies and four LLMs,
including two reasoning models, evaluating their performance on the BIRD
Mini-Dev benchmark. Beyond standard accuracy metrics, we also report inference
latency and token consumption, providing insights relevant for practical system
deployment. Our findings reveal that Divide-and-Conquer prompting and few-shot
demonstrations consistently enhance performance for both general-purpose and
reasoning-focused LLMs. However, introducing additional workflow steps yields
mixed results, and base model selection plays a critical role. This work sheds
light on the practical trade-offs between accuracy, efficiency, and complexity
when deploying Text2SQL systems.

</details>


### [401] [Enhancing LLM Reasoning via Non-Human-Like Reasoning Path Preference Optimization](https://arxiv.org/abs/2510.11104)
*Junjie Lu,Yuliang Liu,Chaofeng Qu,Wei Shen,Zhouhan Lin,Min Xu*

Main category: cs.CL

TL;DR: 本文提出信心引导推理路径偏好优化（CGPO）方法，利用置信度信号识别大语言模型推理过程中的不确定点，并应用自生成的非人类推理路径指导，以减少轨迹漂移，在代码和数学推理任务上，用小型模型生成的数据取得了比强模型或人工标注数据更好的性能。


<details>
  <summary>Details</summary>
Motivation: 当前加强大语言模型推理的方法倾向于引入人类般的推理轨迹偏见，限制了对非人类推理路径的探索。分步偏好优化尤其依赖人类或高能力模型的中间步骤标注。一项小型试点研究发现，约75%的错误发生在其最低置信度点之后，这表明在错误发生前的最低置信度点进行引导比定位第一个显式错误提供更准确的监督。

Method: 本文提出信心引导推理路径偏好优化（CGPO）方法。该方法利用置信度信号识别模型推理过程中不确定性最大的点，并应用自生成的、非人类般的推理路径指导，以减轻轨迹漂移。

Result: 在应用于代码和数学推理任务的各种模型上的实验表明，在相同训练数据量下，CGPO方法使用由小型模型生成的数据，在大多数情况下比使用由强模型或人工标注数据的方法取得了更好的性能。

Conclusion: CGPO通过利用置信度信号在模型不确定性最高点进行自生成的非人类推理路径引导，有效避免了人类偏见，并提高了大语言模型在代码和数学推理任务上的表现，且能用更小的模型生成有效数据。

Abstract: Current approaches for strengthening LLM reasoning tend to introduce a
training bias toward human-like reasoning trajectories. In step-wise preference
optimization, in particular, dependence on human or higher-capacity model
annotations for intermediate steps limits exploration of alternative,
non-human-like reasoning paths and thus constrains achievable performance.
Furthermore, through a small-scale pilot study, we observed that in
approximately 75% of cases, the model's first erroneous step occurs after the
lowest-confidence point. This suggests that guiding the model at its
lowest-confidence point before an error provides more accurate supervision than
locating the first explicit error. In this paper, we propose Confidence-Guided
Reasoning Path Preference Optimization (CGPO), a method that leverages a
confidence signal to identify points of maximal uncertainty in the model's
reasoning process and applies self-generated, non-human-like reasoning-path
guidance to mitigate trajectory drift. Our experiments span diverse models
applied to both code and mathematical reasoning tasks. The results show that,
with the same amount of training data, our method using data generated by a
small model can achieve better performance in most cases compared with
approaches using data generated by a strong model or human-annotated.

</details>


### [402] [One Size Does Not Fit All: Exploring Variable Thresholds for Distance-Based Multi-Label Text Classification](https://arxiv.org/abs/2510.11160)
*Jens Van Nooten,Andriy Kosar,Guy De Pauw,Walter Daelemans*

Main category: cs.CL

TL;DR: 本文探讨了基于距离的无监督多标签文本分类中句子编码器的性能，并提出了一种使用验证集优化标签特定阈值的新方法，显著提高了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 基于距离的无监督文本分类具有推理速度快、对标签集扩展适应性强等优点，但多标签分类中如何确定文本与标签之间的“相似”阈值是一个复杂且未被充分探索的问题。现有的零样本、少样本和微调神经网络在标签集扩展时需要重新训练，而基于距离的方法则无需。此外，文本与标签的语义关系在不同模型、数据集和标签集之间存在差异，需要更精细的阈值策略。

Method: 首先，通过对多样化的多标签文本分类数据集进行实验，进行探索性研究，验证文本和标签之间的语义关系是否在不同模型、数据集和标签集之间存在差异。其次，提出了一种利用验证集优化标签特定阈值的新方法。

Result: 研究发现，相似性分布在不同模型、数据集甚至标签集之间存在统计学上的显著差异。所提出的标签特定阈值优化方法，相比归一化0.5阈值法平均提高了46%，相比以往工作的统一阈值法平均提高了14%。此外，该方法在有限标注样本的情况下也能表现出良好的性能。

Conclusion: 标签特定阈值优化方法显著提升了基于距离的无监督多标签文本分类的性能，有效解决了不同模型、数据集和标签集之间语义关系差异带来的挑战，并且在有限标注数据下依然有效。

Abstract: Distance-based unsupervised text classification is a method within text
classification that leverages the semantic similarity between a label and a
text to determine label relevance. This method provides numerous benefits,
including fast inference and adaptability to expanding label sets, as opposed
to zero-shot, few-shot, and fine-tuned neural networks that require re-training
in such cases. In multi-label distance-based classification and information
retrieval algorithms, thresholds are required to determine whether a text
instance is "similar" to a label or query. Similarity between a text and label
is determined in a dense embedding space, usually generated by state-of-the-art
sentence encoders. Multi-label classification complicates matters, as a text
instance can have multiple true labels, unlike in multi-class or binary
classification, where each instance is assigned only one label. We expand upon
previous literature on this underexplored topic by thoroughly examining and
evaluating the ability of sentence encoders to perform distance-based
classification. First, we perform an exploratory study to verify whether the
semantic relationships between texts and labels vary across models, datasets,
and label sets by conducting experiments on a diverse collection of realistic
multi-label text classification (MLTC) datasets. We find that similarity
distributions show statistically significant differences across models,
datasets and even label sets. We propose a novel method for optimizing
label-specific thresholds using a validation set. Our label-specific
thresholding method achieves an average improvement of 46% over normalized 0.5
thresholding and outperforms uniform thresholding approaches from previous work
by an average of 14%. Additionally, the method demonstrates strong performance
even with limited labeled examples.

</details>


### [403] [Bridging Gaps in Hate Speech Detection: Meta-Collections and Benchmarks for Low-Resource Iberian Languages](https://arxiv.org/abs/2510.11167)
*Paloma Piot,José Ramom Pichel Campos,Javier Parapar*

Main category: cs.CL

TL;DR: 该研究针对低资源语言（特别是伊比利亚语种）中仇恨言论检测的数据和资源不足问题，构建了一个包含欧洲西班牙语、葡萄牙语及两种加利西亚语变体的多语言仇恨言论数据集，并使用最先进的大型语言模型建立了新的检测基准，强调了多语言和变体感知方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论对社会凝聚力和个人福祉构成严重威胁，尤其是在社交媒体上快速传播。现有研究主要集中于英语，导致低资源语言的资源和基准有限，且常忽略这些语言的多种语言变体。同时，大型语言模型（LLMs）需要大量数据才能可靠运行，而低资源语言通常无法满足这一要求。

Method: 研究首先汇编并标准化了一个欧洲西班牙语仇恨言论数据集的元集合，统一了标签和元数据。随后，将该集合翻译成欧洲葡萄牙语以及两种加利西亚语变体（一种更接近西班牙语，另一种更接近葡萄牙语），创建了对齐的多语言语料库。最后，利用这些资源，在零样本、少样本和微调设置下评估了最先进的大型语言模型，并进行了跨语言分析。

Result: 研究为伊比利亚语种的仇恨言论检测建立了新的基准，并为未来的研究提供了基线结果。跨语言分析的结果强调了在仇恨言论检测中采用多语言和变体感知方法的重要性。

Conclusion: 本研究为代表性不足的欧洲语言仇恨言论检测提供了改进基准的坚实基础，并突出了考虑语言多样性和变体在构建鲁棒检测系统中的关键作用。

Abstract: Hate speech poses a serious threat to social cohesion and individual
well-being, particularly on social media, where it spreads rapidly. While
research on hate speech detection has progressed, it remains largely focused on
English, resulting in limited resources and benchmarks for low-resource
languages. Moreover, many of these languages have multiple linguistic
varieties, a factor often overlooked in current approaches. At the same time,
large language models require substantial amounts of data to perform reliably,
a requirement that low-resource languages often cannot meet. In this work, we
address these gaps by compiling a meta-collection of hate speech datasets for
European Spanish, standardised with unified labels and metadata. This
collection is based on a systematic analysis and integration of existing
resources, aiming to bridge the data gap and support more consistent and
scalable hate speech detection. We extended this collection by translating it
into European Portuguese and into a Galician standard that is more convergent
with Spanish and another Galician variant that is more convergent with
Portuguese, creating aligned multilingual corpora. Using these resources, we
establish new benchmarks for hate speech detection in Iberian languages. We
evaluate state-of-the-art large language models in zero-shot, few-shot, and
fine-tuning settings, providing baseline results for future research. Moreover,
we perform a cross-lingual analysis with our target languages. Our findings
underscore the importance of multilingual and variety-aware approaches in hate
speech detection and offer a foundation for improved benchmarking in
underrepresented European languages.

</details>


### [404] [Domain-Specific Data Generation Framework for RAG Adaptation](https://arxiv.org/abs/2510.11217)
*Chris Xing Tian,Weihao Xie,Zhen Chen,Zhengyuan Yi,Hui Liu,Haoliang Li,Shiqi Wang,Siwei Ma*

Main category: cs.CL

TL;DR: 本文提出了 RAGen，一个可扩展的模块化框架，用于生成领域特定的问答-上下文（QAC）三元组，以支持检索增强生成（RAG）系统的适应性，特别是在动态领域中。


<details>
  <summary>Details</summary>
Motivation: 有效的RAG系统适应领域特定设置需要超越通用问答的专业化、上下文丰富的训练数据。现有方法可能无法充分满足这一需求。

Method: RAGen通过识别文档中的关键概念、根据布鲁姆分类法原则生成多样化问题，并从相关上下文中提取精确答案来生成QAC三元组。其模块化流程包括语义分块、分层概念提取、多块检索，并引入了精心策划的干扰上下文以促进鲁棒推理。它支持LLM、检索器和嵌入模型等RAG关键组件的优化。

Result: RAGen能够为多样化的RAG适应方法生成量身定制的QAC三元组，支持RAG组件的优化。它能高效处理大型和不断演变的文档语料库，避免冗余处理，特别适用于科学研究和企业知识库等动态领域。

Conclusion: RAGen是一个可扩展、模块化的框架，能够高效生成领域特定的QAC数据，从而促进RAG系统在动态和演变环境中的鲁棒适应和优化。

Abstract: Retrieval-Augmented Generation (RAG) combines the language understanding and
reasoning power of large language models (LLMs) with external retrieval to
enable domain-grounded responses. Effectively adapting RAG systems to
domain-specific settings requires specialized, context-rich training data
beyond general-purpose question-answering. Here, we propose RAGen, a scalable
and modular framework for generating domain-grounded question-answer-context
(QAC) triples tailored to diverse RAG adaptation approaches. RAGen produces
these QAC triples by identifying key concepts in documents, generating diverse
questions guided by Bloom's Taxonomy-inspired principles, and pairing them with
precise answers extracted from relevant contexts. RAGen supports multiple RAG
adaptation strategies, including the optimization of key components such as the
LLM, retriever, and embedding model, etc. Its modular pipeline features
semantic chunking, hierarchical concept extraction, and multi-chunk retrieval,
along with the introduction of curated distractor contexts to promote robust
reasoning. Designed for scalability, RAGen efficiently handles large and
evolving document corpora without redundant processing, making it especially
suitable for dynamic evolving domains such as scientific research and
enterprise knowledge bases.

</details>


### [405] [Discursive Circuits: How Do Language Models Understand Discourse Relations?](https://arxiv.org/abs/2510.11210)
*Yisong Miao,Min-Yen Kan*

Main category: cs.CL

TL;DR: 该研究通过发现“语篇回路”，揭示了Transformer语言模型中负责语篇理解的稀疏计算图，并分析了不同层级捕获的语篇特征。


<details>
  <summary>Details</summary>
Motivation: 研究动机是识别Transformer语言模型中哪些组件负责语篇理解，因为语篇关系涉及更长的跨度和复杂的推理，不同于简单任务。研究假设稀疏计算图（即语篇回路）控制模型处理语篇关系的方式。

Method: 为实现回路发现，研究引入了“语篇关系下补全”（CuDR）任务，模型需根据指定关系补全语篇。为此构建了一个针对回路发现中激活修补的最小对比对语料库。实验通过激活修补技术，在GPT-2模型上发现了稀疏回路。

Result: 实验结果表明，稀疏回路（约占完整GPT-2模型的0.2%）能恢复基于PDTB的CuDR任务中的语篇理解能力，并能很好地泛化到RST和SDRT等未见过的语篇框架。进一步分析显示，模型的下层捕获词汇语义和共指等语言特征，而上层则编码语篇层面的抽象概念。特征效用在不同框架中保持一致（例如，共指支持扩展类关系）。

Conclusion: 该研究得出结论，Transformer模型中存在稀疏的“语篇回路”负责语篇理解，这些回路能有效泛化。模型内部存在分层处理机制，下层处理基础语言特征，上层处理高级语篇抽象，且特征效用具有跨框架的一致性。

Abstract: Which components in transformer language models are responsible for discourse
understanding? We hypothesize that sparse computational graphs, termed as
discursive circuits, control how models process discourse relations. Unlike
simpler tasks, discourse relations involve longer spans and complex reasoning.
To make circuit discovery feasible, we introduce a task called Completion under
Discourse Relation (CuDR), where a model completes a discourse given a
specified relation. To support this task, we construct a corpus of minimal
contrastive pairs tailored for activation patching in circuit discovery.
Experiments show that sparse circuits ($\approx 0.2\%$ of a full GPT-2 model)
recover discourse understanding in the English PDTB-based CuDR task. These
circuits generalize well to unseen discourse frameworks such as RST and SDRT.
Further analysis shows lower layers capture linguistic features such as lexical
semantics and coreference, while upper layers encode discourse-level
abstractions. Feature utility is consistent across frameworks (e.g.,
coreference supports Expansion-like relations).

</details>


### [406] [WebRouter: Query-specific Router via Variational Information Bottleneck for Cost-sensitive Web Agent](https://arxiv.org/abs/2510.11221)
*Tao Li,Jinlong Hu,Yang Wang,Junfeng Liu,Xuejun Liu*

Main category: cs.CL

TL;DR: WebRouter是一个新颖的查询特定路由器，它通过成本感知变分信息瓶颈（ca-VIB）目标，显著降低了大型语言模型（LLM）驱动的网页代理的操作成本，同时保持了高精度。


<details>
  <summary>Details</summary>
Motivation: LLM驱动的网页代理在网页自动化方面能力强大，但面临着严峻的成本-性能权衡。网页代理固有的复杂提示（包含目标、行动历史和环境状态）进一步加剧了这一挑战，导致LLM整体性能下降。

Method: 本文引入了WebRouter，一个从信息论角度训练的新型查询特定路由器。其核心贡献是成本感知变分信息瓶颈（ca-VIB）目标，该目标在学习输入提示的压缩表示的同时，明确惩罚预期的操作成本。

Result: 在WebVoyager基准测试的五个真实世界网站上的实验表明，与GPT-4o基线相比，WebRouter将操作成本降低了87.8%，而准确性仅下降了3.8%。

Conclusion: WebRouter通过其成本感知的信息瓶颈方法，成功解决了LLM驱动的网页代理在操作成本和性能之间的权衡问题，实现了显著的成本节约和可接受的准确性损失。

Abstract: LLM-brained web agents offer powerful capabilities for web automation but
face a critical cost-performance trade-off. The challenge is amplified by web
agents' inherently complex prompts that include goals, action histories, and
environmental states, leading to degraded LLM ensemble performance. To address
this, we introduce WebRouter, a novel query-specific router trained from an
information-theoretic perspective. Our core contribution is a cost-aware
Variational Information Bottleneck (ca-VIB) objective, which learns a
compressed representation of the input prompt while explicitly penalizing the
expected operational cost. Experiments on five real-world websites from the
WebVoyager benchmark show that WebRouter reduces operational costs by a
striking 87.8\% compared to a GPT-4o baseline, while incurring only a 3.8\%
accuracy drop.

</details>


### [407] [Evaluating Reasoning Faithfulness in Medical Vision-Language Models using Multimodal Perturbations](https://arxiv.org/abs/2510.11196)
*Johannes Moll,Markus Graf,Tristan Lemke,Nicolas Lenhart,Daniel Truhn,Jean-Benoit Delbrouck,Jiazhen Pan,Daniel Rueckert,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CL

TL;DR: 该研究提出了一个用于胸部X射线视觉问答（VQA）的临床验证框架，通过控制文本和图像修改来评估视觉语言模型（VLM）链式思考（CoT）解释的忠实性，发现模型答案准确性与解释质量脱节，并揭示了专有模型在归因和忠实性方面优于开源模型，强调了超越最终答案准确性进行评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLM）生成的链式思考（CoT）解释可能听起来合理但未能反映其潜在决策过程，这在临床等高风险应用中会损害信任。现有评估方法通常只关注答案准确性或格式，难以捕捉这种不一致。

Method: 研究提出了一个以临床为基础的胸部X射线VQA框架，通过在临床忠实度、因果归因和信心校准三个维度上对文本和图像进行受控修改，来探究CoT解释的忠实性。通过一项有四名评估放射科医生参与的读者研究验证了该框架，并使用该框架对六个VLM进行了基准测试。

Result: 评估放射科医生在所有维度上的相关性均在放射科医生间差异范围内：归因维度相关性强（Kendall's $\tau_b=0.670$），忠实度维度中等（$\tau_b=0.387$），信心语气维度较弱（$\tau_b=0.091$）。基准测试显示，VLM的答案准确性与解释质量是脱钩的；承认注入的线索并不能保证其基础性；文本线索比视觉线索更能改变解释。虽然一些开源模型在最终答案准确性上表现相当，但专有模型在归因（25.0% vs. 1.4%）和忠实度（36.1% vs. 31.7%）上得分更高。

Conclusion: VLM在临床应用中存在部署风险，需要超越最终答案准确性来评估模型，特别是要关注其解释的忠实性。答案准确性与解释质量的脱节以及专有模型在解释能力上的优势，凸显了更全面评估框架的重要性。

Abstract: Vision-language models (VLMs) often produce chain-of-thought (CoT)
explanations that sound plausible yet fail to reflect the underlying decision
process, undermining trust in high-stakes clinical use. Existing evaluations
rarely catch this misalignment, prioritizing answer accuracy or adherence to
formats. We present a clinically grounded framework for chest X-ray visual
question answering (VQA) that probes CoT faithfulness via controlled text and
image modifications across three axes: clinical fidelity, causal attribution,
and confidence calibration. In a reader study (n=4), evaluator-radiologist
correlations fall within the observed inter-radiologist range for all axes,
with strong alignment for attribution (Kendall's $\tau_b=0.670$), moderate
alignment for fidelity ($\tau_b=0.387$), and weak alignment for confidence tone
($\tau_b=0.091$), which we report with caution. Benchmarking six VLMs shows
that answer accuracy and explanation quality are decoupled, acknowledging
injected cues does not ensure grounding, and text cues shift explanations more
than visual cues. While some open-source models match final answer accuracy,
proprietary models score higher on attribution (25.0% vs. 1.4%) and often on
fidelity (36.1% vs. 31.7%), highlighting deployment risks and the need to
evaluate beyond final answer accuracy.

</details>


### [408] [TypePilot: Leveraging the Scala Type System for Secure LLM-generated Code](https://arxiv.org/abs/2510.11151)
*Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）生成的代码存在安全漏洞。本文提出了TypePilot，一个利用强类型和可验证语言（如Scala）的智能体AI框架，显著提高了LLM生成代码的安全性与鲁棒性，有效缓解了输入验证和注入漏洞。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在代码生成方面表现出色，但其输出常包含细微但关键的漏洞，这在安全敏感或任务关键系统中会带来重大风险。

Method: 本文引入了TypePilot，一个智能体AI框架。该框架通过利用强类型和可验证语言（以Scala为例）来增强LLM生成代码的安全性和鲁棒性。研究在两种设置下评估了其有效性：使用Stainless框架进行形式验证和通用安全代码生成。

Result: 实验表明，直接代码生成和简单的安全提示都未能有效强制执行安全约束。相比之下，TypePilot这种以类型为中心的智能体管道显著缓解了输入验证和注入漏洞。

Conclusion: 研究结果证明了结构化、类型引导的LLM工作流（如TypePilot）在提高高保障领域自动化代码生成可信度方面的巨大潜力。

Abstract: Large language Models (LLMs) have shown remarkable proficiency in code
generation tasks across various programming languages. However, their outputs
often contain subtle but critical vulnerabilities, posing significant risks
when deployed in security-sensitive or mission-critical systems. This paper
introduces TypePilot, an agentic AI framework designed to enhance the security
and robustness of LLM-generated code by leveraging strongly typed and
verifiable languages, using Scala as a representative example. We evaluate the
effectiveness of our approach in two settings: formal verification with the
Stainless framework and general-purpose secure code generation. Our experiments
with leading open-source LLMs reveal that while direct code generation often
fails to enforce safety constraints, just as naive prompting for more secure
code, our type-focused agentic pipeline substantially mitigates input
validation and injection vulnerabilities. The results demonstrate the potential
of structured, type-guided LLM workflows to improve the SotA of the
trustworthiness of automated code generation in high-assurance domains.

</details>


### [409] [ABLEIST: Intersectional Disability Bias in LLM-Generated Hiring Scenarios](https://arxiv.org/abs/2510.10998)
*Mahika Phutane,Hayoung Jung,Matthew Kim,Tanushree Mitra,Aditya Vashistha*

Main category: cs.CL

TL;DR: 该研究审计了六个大型语言模型在招聘场景中对残障人士的歧视，特别是结合了性别和种姓的交叉歧视。结果显示，模型对残障候选人存在显著的歧视性伤害，且对边缘化群体的交叉歧视更为严重，揭示了当前安全工具的不足。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在招聘等高风险领域存在基于身份的歧视，特别是针对残障人士。现有研究多以西方为中心，忽视了全球南方地区性别和种姓等交叉边缘化因素如何影响残障人士的经历。

Method: 研究对六个大型语言模型进行了全面审计，使用了2,820个涵盖不同残障、性别、国籍和种姓特征的招聘场景。引入了ABLEIST（能力歧视、励志化、超人化和象征性）指标，这是一套包含五个特定于能力歧视和三个交叉歧视伤害的度量标准，这些标准基于残障研究文献。

Result: 研究发现，模型对残障候选人表现出显著增加的ABLEIST伤害，其中许多最先进的模型未能检测到这些伤害。对于性别和种姓边缘化的残障候选人，交叉歧视伤害（例如象征性歧视）进一步急剧增加。

Conclusion: 当前的安全工具存在关键盲点，无法有效检测和缓解大型语言模型在招聘等高风险领域中对残障人士，尤其是结合性别和种姓等交叉因素的歧视性伤害。因此，需要对前沿模型进行交叉安全评估。

Abstract: Large language models (LLMs) are increasingly under scrutiny for perpetuating
identity-based discrimination in high-stakes domains such as hiring,
particularly against people with disabilities (PwD). However, existing research
remains largely Western-centric, overlooking how intersecting forms of
marginalization--such as gender and caste--shape experiences of PwD in the
Global South. We conduct a comprehensive audit of six LLMs across 2,820 hiring
scenarios spanning diverse disability, gender, nationality, and caste profiles.
To capture subtle intersectional harms and biases, we introduce ABLEIST
(Ableism, Inspiration, Superhumanization, and Tokenism), a set of five
ableism-specific and three intersectional harm metrics grounded in disability
studies literature. Our results reveal significant increases in ABLEIST harms
towards disabled candidates--harms that many state-of-the-art models failed to
detect. These harms were further amplified by sharp increases in intersectional
harms (e.g., Tokenism) for gender and caste-marginalized disabled candidates,
highlighting critical blind spots in current safety tools and the need for
intersectional safety evaluations of frontier models in high-stakes domains
like hiring.

</details>


### [410] [Fairness Metric Design Exploration in Multi-Domain Moral Sentiment Classification using Transformer-Based Models](https://arxiv.org/abs/2510.11222)
*Battemuulen Naranbat,Seyed Sahand Mohammadi Ziabari,Yousuf Nasser Al Husaini,Ali Mohammed Mansoor Alsahag*

Main category: cs.CL

TL;DR: 该研究发现，在跨领域道德情感分类中，Transformer模型存在显著的公平性问题和性能不对称，尤其是在Twitter到Reddit的迁移中。为解决此问题，引入了道德公平一致性（MFC）指标，该指标能有效量化跨领域稳定性并诊断公平性违规。


<details>
  <summary>Details</summary>
Motivation: 在道德情感分类的自然语言处理中，特别是在Transformer模型日益普及的跨领域迁移情境下，确保公平性是一项挑战。聚合性能往往会掩盖潜在的不公平现象。

Method: 研究使用道德基础Twitter语料库（MFTC）和道德基础Reddit语料库（MFRC），在多标签设置下，通过域内和跨域协议评估了BERT和DistilBERT模型。此外，引入了道德公平一致性（MFC）指标来量化道德基础检测的跨领域稳定性。

Result: 研究发现跨域迁移存在显著的不对称性（Twitter->Reddit的micro-F1下降14.9%，而Reddit->Twitter仅下降1.5%）。整体分数掩盖了标签层面的公平性违规，例如“权威”标签的统计均等差异为0.22-0.23，机会均等差异为0.40-0.41。MFC指标与统计均等差异呈完美负相关（rho = -1.000, p < 0.001），且独立于标准性能指标。在所有标签中，“忠诚”表现出最高的一致性（MFC = 0.96），而“权威”最低（MFC = 0.78）。

Conclusion: MFC指标被确立为一种补充性的、以诊断为导向的公平性评估指标，适用于道德推理模型，有助于在异构语言环境中实现更可靠的模型部署。

Abstract: Ensuring fairness in natural language processing for moral sentiment
classification is challenging, particularly under cross-domain shifts where
transformer models are increasingly deployed. Using the Moral Foundations
Twitter Corpus (MFTC) and Moral Foundations Reddit Corpus (MFRC), this work
evaluates BERT and DistilBERT in a multi-label setting with in-domain and
cross-domain protocols. Aggregate performance can mask disparities: we observe
pronounced asymmetry in transfer, with Twitter->Reddit degrading micro-F1 by
14.9% versus only 1.5% for Reddit->Twitter. Per-label analysis reveals fairness
violations hidden by overall scores; notably, the authority label exhibits
Demographic Parity Differences of 0.22-0.23 and Equalized Odds Differences of
0.40-0.41. To address this gap, we introduce the Moral Fairness Consistency
(MFC) metric, which quantifies the cross-domain stability of moral foundation
detection. MFC shows strong empirical validity, achieving a perfect negative
correlation with Demographic Parity Difference (rho = -1.000, p < 0.001) while
remaining independent of standard performance metrics. Across labels, loyalty
demonstrates the highest consistency (MFC = 0.96) and authority the lowest (MFC
= 0.78). These findings establish MFC as a complementary, diagnosis-oriented
metric for fairness-aware evaluation of moral reasoning models, enabling more
reliable deployment across heterogeneous linguistic contexts. .

</details>


### [411] [Enhancing Large Language Model Reasoning via Selective Critical Token Fine-Tuning](https://arxiv.org/abs/2510.10974)
*Zhiwen Ruan,Yixia Li,He Zhu,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: 本文提出关键令牌微调 (CFT) 方法，通过仅更新对推理正确性至关重要的令牌，显著优于标准监督微调 (SFT)，提高了大型语言模型 (LLM) 在数学推理任务上的性能和多样性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在领域特定任务（如数学推理）中主要依赖监督微调 (SFT)。然而，标准 SFT 对所有令牌进行统一惩罚，忽略了只有一小部分关键令牌决定推理的正确性。这种统一监督通常会导致输出多样性降低和泛化能力受限。

Method: 本文提出关键令牌微调 (CFT) 方法。该方法通过反事实扰动识别出功能上不可或缺的令牌，并仅更新这些令牌。通过将梯度信号集中在这些决定性的推理步骤上，同时保留非关键令牌的多样性，CFT 旨在增强生成能力和多样性。

Result: 在三个模型家族（Qwen、OLMo、LLaMA）的五种模型和十一个数学推理基准上进行了广泛实验。结果表明，尽管 CFT 仅对不到 12% 的令牌进行微调，但其性能始终优于标准 SFT。此外，CFT 通过改进采样多样性实现了测试时扩展，并为强化学习提供了更强的初始化，在后续训练阶段保持了性能提升，同时维持了更高的探索熵。

Conclusion: 这些结果表明，CFT 是一种实用且通用的框架，可用于高效、鲁棒的大型语言模型微调。

Abstract: Large language models (LLMs) primarily rely on supervised fine-tuning (SFT)
as a key method to adapt pre-trained models to domain-specific tasks such as
mathematical reasoning. However, standard SFT uniformly penalizes all tokens,
neglecting that only a small subset of critical tokens determines reasoning
correctness. This uniform supervision often causes reduced output diversity and
limited generalization. We propose Critical Token Fine-tuning (CFT), a simple
yet effective approach that updates only tokens identified as functionally
indispensable via counterfactual perturbations. By focusing gradient signals on
these decisive reasoning steps while preserving the diversity of non-critical
tokens, CFT can enhance both generation and diversity. Extensive experiments on
five models across three families (Qwen, OLMo, LLaMA) and eleven mathematical
reasoning benchmarks show that CFT, despite fine-tuning on less than 12% of
tokens, consistently outperforms standard SFT. Moreover, CFT enables test-time
scaling through improved sampling diversity and provides a stronger
initialization for reinforcement learning, sustaining performance gains in
later training stages while maintaining higher entropy for better exploration.
These results highlight CFT as a practical and general framework for efficient
and robust LLM fine-tuning.

</details>


### [412] [A Theorem-Proving-Based Evaluation of Neural Semantic Parsing](https://arxiv.org/abs/2510.11225)
*Hayate Funakura,Hyunsoo Kim,Koji Mineshima*

Main category: cs.CL

TL;DR: 本研究指出图匹配度量（如Smatch）在评估神经语义解析器时无法捕捉逻辑等价性。通过结合图匹配和自动定理证明，并比较不同模型和目标表示，发现图匹配高分常不等于逻辑等价。研究强调需要逻辑敏感的评估和训练目标，以及简化的规范化目标表示。


<details>
  <summary>Details</summary>
Motivation: 图匹配度量（如Smatch）是评估神经语义语义解析器的标准方法，但它们仅捕获表面重叠而非逻辑等价性，这促使研究者重新评估解析器的评估方法。

Method: 研究方法是将图匹配与自动定理证明相结合进行评估。比较了两种解析器构建方法：监督微调（T5-Small/Base）和少量样本上下文学习（GPT-4o/4.1/5），分别在规范化和非规范化目标下进行。评估输出时使用了图匹配、基于一阶逻辑定理证明器的源公式与目标公式间的双向蕴含以及格式良好性。同时进行了错误分析。

Result: 研究发现，在图匹配上表现良好的模型，在生成逻辑等价公式方面往往失败。规范化能减少目标偶然变异性，提高格式良好性，并增强逻辑充分性。错误分析表明，性能随公式复杂性、并列结构、介词短语和被动语态的增加而下降；主要失败涉及变量绑定和索引以及谓词命名。

Conclusion: 这些发现揭示了基于图的度量在面向推理的应用中的局限性，并促使采用逻辑敏感的评估和训练目标，以及简化、规范化的目标表示。

Abstract: Graph-matching metrics such as Smatch are the de facto standard for
evaluating neural semantic parsers, yet they capture surface overlap rather
than logical equivalence. We reassess evaluation by pairing graph-matching with
automated theorem proving. We compare two approaches to building parsers:
supervised fine-tuning (T5-Small/Base) and few-shot in-context learning
(GPT-4o/4.1/5), under normalized and unnormalized targets. We evaluate outputs
using graph-matching, bidirectional entailment between source and target
formulas with a first-order logic theorem prover, and well-formedness. Across
settings, we find that models performing well on graph-matching often fail to
produce logically equivalent formulas. Normalization reduces incidental target
variability, improves well-formedness, and strengthens logical adequacy. Error
analysis shows performance degrades with increasing formula complexity and with
coordination, prepositional phrases, and passive voice; the dominant failures
involve variable binding and indexing, and predicate naming. These findings
highlight limits of graph-based metrics for reasoning-oriented applications and
motivate logic-sensitive evaluation and training objectives together with
simplified, normalized target representations. All code and data for our
experiments are publicly available.

</details>


### [413] [The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers](https://arxiv.org/abs/2510.11218)
*Saad Obaid ul Islam,Anne Lauscher,Goran Glavaš*

Main category: cs.CL

TL;DR: 本研究发现大型语言模型在处理简单（独立）和复杂（集成）事实查询时存在系统性不一致，引入SLAQ框架进行评估，并揭示了答案错位、位置依赖性准确性下降和动量效应，强调了查询复杂性下事实一致性对LLM可信度的重要性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在回答简单事实问题时表现出色，但在将相同事实整合到复杂查询中时却可能失败，这暴露出模型在不同任务复杂性下访问事实知识存在根本性不一致。这种简单与复杂查询之间的可靠性差距尚不清楚，损害了模型的信任度。当前评估方法隐含假设简单查询表现好就意味着复杂任务可靠，这种假设可能不成立。

Method: 引入了Short-Long Form Alignment for Factual Question Answering (SLAQ) 评估框架，用于比较LLM对相同事实问题在两种情境下的回答：(a) 独立提问（短形式）和 (b) 集成到复杂查询中（长形式）。研究评估了16个LLM，使用了600个查询。通过机械分析（mechanistic analysis）探讨了对齐事实如何激活模型内部重叠部分，并使用基于机械相似性的指标来预测短-长答案对齐。

Result: 研究发现，LLM对相应短形式和长形式查询的答案存在系统性错位。进一步揭示了位置依赖性的准确性损失，以及动量效应，即连续的正确或不正确答案会形成自我强化的模式。通过机械分析，发现对齐的事实会激活模型内部重叠的部分，并且基于机械相似性的指标可以以高达78%的准确率预测短-长答案的对齐情况。

Conclusion: 本研究确立了查询复杂性下的事实一致性是LLM可信度的一个重要方面，并挑战了当前评估实践。目前的评估实践隐含地假设对简单事实查询的良好表现意味着在更复杂的知识寻求任务中也具有可靠性，但本研究结果表明这并非总是如此。未来的LLM评估应考虑不同查询复杂性下的事实一致性。

Abstract: Large language models (LLMs) can correctly answer "When was Einstein born?"
yet fail to provide the same date when writing about Einstein's life revealing
a fundamental inconsistency in how models access factual knowledge across task
complexities. While models display impressive accuracy on factual
question-answering benchmarks, the reliability gap between simple and complex
queries remains poorly understood, eroding their trustworthiness. In this work,
we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a
controlled evaluation framework that compares LLMs' answers to the same factual
questions asked (a) in isolation (short) vs. (b) integrated into complex
queries (long). Looking at 16 LLMs across 600 queries, we find a systematic
misalignment of answers to the corresponding short and long queries. We further
uncover position-dependent accuracy loss and momentum effects where consecutive
correct or incorrect answers create self-reinforcing patterns. Through
mechanistic analysis, we find that aligned facts activate overlapping model
internals, and that metrics based on mechanistic similarity can predict
short-long answer alignment with up to 78% accuracy. Our work establishes
factual consistency over query complexity as an important aspect of LLMs'
trustworthiness and challenges current evaluation practices, which implicitly
assume that good performance for simple factual queries implies reliability in
more complex knowledge-seeking tasks too.

</details>


### [414] [CNSocialDepress: A Chinese Social Media Dataset for Depression Risk Detection and Structured Analysis](https://arxiv.org/abs/2510.11233)
*Jinyuan Xu,Tian Lan,Xintao Yu,Xue He,Hezhi Zhang,Ying Wang,Pierre Magistry,Mathieu Valette,Lei Li*

Main category: cs.CL

TL;DR: 发布了一个名为CNSocialDepress的中文社交媒体抑郁风险检测基准数据集，包含多维度心理属性标注，支持可解释和细粒度的分析，并已在多种NLP任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 抑郁症是一个紧迫的全球公共健康问题，但现有的中文抑郁风险检测资源稀缺，且大多仅限于二元分类，缺乏细粒度和可解释的分析能力。

Method: 构建了CNSocialDepress数据集，包含来自233位用户的44,178条中文社交媒体文本，并由心理专家标注了10,306个抑郁相关片段。数据集提供了二元风险标签和结构化的多维度心理属性。

Result: 实验结果证明了CNSocialDepress数据集在广泛的自然语言处理任务中的实用性，包括结构化心理画像和大型语言模型在抑郁检测方面的微调。全面的评估强调了数据集在抑郁风险识别和心理分析方面的有效性和实用价值。

Conclusion: CNSocialDepress数据集为针对中文人群的心理健康应用提供了新的见解，有助于进行可解释和细粒度的抑郁信号分析和风险识别。

Abstract: Depression is a pressing global public health issue, yet publicly available
Chinese-language resources for risk detection remain scarce and are mostly
limited to binary classification. To address this limitation, we release
CNSocialDepress, a benchmark dataset for depression risk detection from Chinese
social media posts. The dataset contains 44,178 texts from 233 users, within
which psychological experts annotated 10,306 depression-related segments.
CNSocialDepress provides binary risk labels together with structured
multi-dimensional psychological attributes, enabling interpretable and
fine-grained analysis of depressive signals. Experimental results demonstrate
its utility across a wide range of NLP tasks, including structured
psychological profiling and fine-tuning of large language models for depression
detection. Comprehensive evaluations highlight the dataset's effectiveness and
practical value for depression risk identification and psychological analysis,
thereby providing insights to mental health applications tailored for
Chinese-speaking populations.

</details>


### [415] [Attacks by Content: Automated Fact-checking is an AI Security Issue](https://arxiv.org/abs/2510.11238)
*Michael Schlichtkrull*

Main category: cs.CL

TL;DR: 本文提出了一种针对AI代理的新型攻击——“内容攻击”，即通过提供偏颇、误导或虚假信息来操纵代理，而非注入指令。现有的防御机制对此无效，作者建议将自动化事实核查技术重新利用，作为代理的认知自卫工具。


<details>
  <summary>Details</summary>
Motivation: 以往研究主要关注间接提示注入（注入恶意指令）。本文认为，攻击者无需注入指令，仅通过提供偏颇、误导或虚假信息即可操纵AI代理，并称之为“内容攻击”。现有专注于检测隐藏命令的防御措施对“内容攻击”无效，因此需要新的防御策略。

Method: 本文提出，AI代理必须批判性地评估检索到的信息，通过外部证据核实主张，并评估来源的可信度。作者建议将现有的自然语言处理任务——自动化事实核查——重新定位，作为代理的认知自卫工具。

Result: 本文识别并定义了“内容攻击”这一新型威胁，指出其与传统的指令注入攻击不同，且现有防御无效。作为解决方案，本文提出AI代理应采用批判性评估和事实核查机制来抵御此类攻击。

Conclusion: 攻击者可以通过提供偏颇、误导或虚假信息来对AI代理进行“内容攻击”，这是一种新的威胁。为了防御这种攻击，AI代理需要像人类一样批判性地评估信息，作者建议将自动化事实核查技术作为代理的认知自卫工具。

Abstract: When AI agents retrieve and reason over external documents, adversaries can
manipulate the data they receive to subvert their behaviour. Previous research
has studied indirect prompt injection, where the attacker injects malicious
instructions. We argue that injection of instructions is not necessary to
manipulate agents - attackers could instead supply biased, misleading, or false
information. We term this an attack by content. Existing defenses, which focus
on detecting hidden commands, are ineffective against attacks by content. To
defend themselves and their users, agents must critically evaluate retrieved
information, corroborating claims with external evidence and evaluating source
trustworthiness. We argue that this is analogous to an existing NLP task,
automated fact-checking, which we propose to repurpose as a cognitive
self-defense tool for agents.

</details>


### [416] [XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression](https://arxiv.org/abs/2510.11236)
*Haoqi Yang,Yao Yao,Zuchao Li,Baoyuan Qi,Guoming Liu,Hai Zhao*

Main category: cs.CL

TL;DR: XQuant是一个免训练、即插即用的框架，通过数据无关校准和跨层压缩，实现了LLM KV缓存的超低位宽（低于1.4比特）量化，在保持高性能的同时显著降低了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在自然语言处理任务中表现出色，但其在长文本理解和生成过程中，KV缓存的增长导致内存需求巨大，这对于资源受限环境的部署构成了重大挑战。量化被认为是一种有前景的解决方案，可以在减少内存消耗的同时保留历史信息。

Method: 本文提出了XQuant，一个免训练、即插即用的框架，用于实现超低等效位宽的KV缓存量化。XQuant引入了两项关键创新：计算开销可忽略不计的数据无关校准方法，以及跨层KV缓存压缩，从而实现了低于1.4比特的量化。

Result: 在TruthfulQA和LongBench上的大量实验表明，XQuant在实现更低位宽的同时，性能优于现有最先进的方法（例如KIVI-2bit和AsymKV-1.5bit），在内存效率和模型准确性之间建立了更好的权衡。

Conclusion: XQuant通过其创新的数据无关校准和跨层压缩技术，为LLM KV缓存提供了一种卓越的超低位宽量化解决方案，显著提高了内存效率并保持了出色的模型准确性，从而解决了LLMs在资源受限环境中的部署挑战。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks. However, their extensive memory
requirements, particularly due to KV cache growth during long-text
understanding and generation, present significant challenges for deployment in
resource-constrained environments. Quantization has emerged as a promising
solution to reduce memory consumption while preserving historical information.
We propose XQuant, a training-free and plug-and-play framework that achieves
ultra-low equivalent bit-width KV cache quantization. XQuant introduces two key
innovations: a computationally negligible data-free calibration method and
cross-layer KV cache compression, enabling quantization to sub-1.4 bits.
Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant
outperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by
achieving lower bit-width while maintaining superior performance, establishing
a better trade-off between memory efficiency and model accuracy.

</details>


### [417] [Do Psychometric Tests Work for Large Language Models? Evaluation of Tests on Sexism, Racism, and Morality](https://arxiv.org/abs/2510.11254)
*Jana Jung,Marlene Lutz,Indira Sen,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本研究评估了人类心理测量测试在大型语言模型（LLM）上的可靠性和有效性。结果发现其可靠性中等，但生态效度较低，测试分数与LLM在实际任务中的行为不符，表明人类测试不应直接应用于LLM。


<details>
  <summary>Details</summary>
Motivation: 心理测量测试越来越多地用于评估LLM的心理结构，但这些最初为人类开发的测试在应用于LLM时是否能产生有意义的结果尚不明确。

Method: 研究系统地评估了人类心理测量测试对LLM在性别歧视、种族歧视和道德三个结构上的可靠性和有效性。可靠性通过多项题目和提示变体进行评估。有效性通过聚合效度（测试基于理论的测试间相关性）和生态效度（测试分数与LLM在真实下游任务中行为的一致性）两种方法进行评估。

Result: 研究发现，在多项题目和提示变体下，可靠性中等。然而，心理测量测试分数与LLM在下游任务中的行为不一致，在某些情况下甚至呈负相关，这表明生态效度较低。

Conclusion: 研究强调，在解释心理测量测试分数之前，对其进行系统评估至关重要。结果还表明，为人类设计的心理测量测试不能未经调整直接应用于LLM。

Abstract: Psychometric tests are increasingly used to assess psychological constructs
in large language models (LLMs). However, it remains unclear whether these
tests -- originally developed for humans -- yield meaningful results when
applied to LLMs. In this study, we systematically evaluate the reliability and
validity of human psychometric tests for three constructs: sexism, racism, and
morality. We find moderate reliability across multiple item and prompt
variations. Validity is evaluated through both convergent (i.e., testing
theory-based inter-test correlations) and ecological approaches (i.e., testing
the alignment between tests scores and behavior in real-world downstream
tasks). Crucially, we find that psychometric test scores do not align, and in
some cases even negatively correlate with, model behavior in downstream tasks,
indicating low ecological validity. Our results highlight that systematic
evaluations of psychometric tests is essential before interpreting their
scores. They also suggest that psychometric tests designed for humans cannot be
applied directly to LLMs without adaptation.

</details>


### [418] [Towards Real-Time Fake News Detection under Evidence Scarcity](https://arxiv.org/abs/2510.11277)
*Guangyu Wei,Ke Han,Yueming Lyu,Yu Luo,Yue Jiang,Caifeng Shan,Nicu Sebe*

Main category: cs.CL

TL;DR: EASE是一种新颖的实时假新闻检测框架，它根据证据充足性动态调整决策过程。通过三阶段评估（证据、推理、情感）和指令微调，EASE在证据稀缺的情况下实现了最先进的性能和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在实时场景中，新兴事件通常缺乏足够的佐证，导致现有依赖外部证据的假新闻检测方法难以泛化。

Method: 本文提出了EASE框架，通过评估意识选择专家来应对证据稀缺问题。它引入了一个顺序评估机制，包括：1) 基于证据的评估（仅在证据充足时使用）；2) 基于推理的评估（仅在大型语言模型LLM可靠时使用其世界知识）；3) 基于情感的备用（在证据和推理都不可靠时集成情感线索）。为提高评估准确性，EASE采用伪标签的指令微调来指导评估器，并通过可解释的推理来证明其视角特定知识。此外，专家模块将评估器的评估结果与新闻内容结合，实现评估感知的决策。

Result: 广泛的实验表明，EASE不仅在多个基准测试中实现了最先进的性能，而且显著提高了对实时新闻的泛化能力。此外，本文还引入了RealTimeNews-25新基准来评估模型在新兴新闻上的泛化能力。

Conclusion: EASE通过根据可用证据的充足性动态调整决策过程，有效解决了实时假新闻检测中证据稀缺的挑战，从而在多个基准上实现了最先进的性能，并显著提升了对实时新闻的泛化能力。

Abstract: Fake news detection becomes particularly challenging in real-time scenarios,
where emerging events often lack sufficient supporting evidence. Existing
approaches often rely heavily on external evidence and therefore struggle to
generalize under evidence scarcity. To address this issue, we propose
Evaluation-Aware Selection of Experts (EASE), a novel framework for real-time
fake news detection that dynamically adapts its decision-making process
according to the assessed sufficiency of available evidence. EASE introduces a
sequential evaluation mechanism comprising three independent perspectives: (1)
Evidence-based evaluation, which assesses evidence and incorporates it into
decision-making only when the evidence is sufficiently supportive; (2)
Reasoning-based evaluation, which leverages the world knowledge of large
language models (LLMs) and applies them only when their reliability is
adequately established; and (3) Sentiment-based fallback, which integrates
sentiment cues when neither evidence nor reasoning is reliable. To enhance the
accuracy of evaluation processes, EASE employs instruction tuning with pseudo
labels to guide each evaluator in justifying its perspective-specific knowledge
through interpretable reasoning. Furthermore, the expert modules integrate the
evaluators' justified assessments with the news content to enable
evaluation-aware decision-making, thereby enhancing overall detection accuracy.
Moreover, we introduce RealTimeNews-25, a new benchmark comprising recent news
for evaluating model generalization on emerging news with limited evidence.
Extensive experiments demonstrate that EASE not only achieves state-of-the-art
performance across multiple benchmarks, but also significantly improves
generalization to real-time news. The code and dataset are available:
https://github.com/wgyhhhh/EASE.

</details>


### [419] [FOSSIL: Harnessing Feedback on Suboptimal Samples for Data-Efficient Generalisation with Imitation Learning for Embodied Vision-and-Language Tasks](https://arxiv.org/abs/2510.11307)
*Sabrina McCallum,Amit Parekh,Alessandro Suglia*

Main category: cs.CL

TL;DR: 本文提出了一种数据高效的方法，通过将语言反馈嵌入作为输入序列，使模仿学习智能体能够从最优和次优演示中学习，从而提高具身AI的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前具身AI的模仿学习方法受限于只能从最优行为中学习，否则有复制错误和低效的风险。强化学习虽然提供了替代方案，但通常以牺牲数据效率为代价。本研究旨在探索如何在提供建设性语言反馈的情况下，让智能体从次优演示中也能有效学习。

Method: 研究将语言反馈嵌入直接作为输入序列的一部分提供给基于Transformer的策略。此外，还可以选择性地通过辅助自监督学习目标来补充传统的下一步动作预测目标，以预测反馈。该方法在定制的BabyAI-XGen环境中的一系列具身视觉与语言任务上进行了测试。

Result: 实验结果表明，该方法显著提高了智能体的组合泛化能力和鲁棒性。这表明其数据高效的方法能够成功地将次优行为转化为学习机会。

Conclusion: 研究结果表明，对于语言指定的具身任务，语言反馈是一种有竞争力和直观的替代方案，可以取代中间标量奖励。

Abstract: Current approaches to embodied AI tend to learn policies from expert
demonstrations. However, without a mechanism to evaluate the quality of
demonstrated actions, they are limited to learning from optimal behaviour, or
they risk replicating errors and inefficiencies. While reinforcement learning
offers one alternative, the associated exploration typically results in
sacrificing data efficiency. This work explores how agents trained with
imitation learning can learn robust representations from both optimal and
suboptimal demonstrations when given access to constructive language feedback
as a means to contextualise different modes of behaviour. We directly provide
language feedback embeddings as part of the input sequence into a
Transformer-based policy, and optionally complement the traditional next action
prediction objective with auxiliary self-supervised learning objectives for
feedback prediction. We test our approach on a range of embodied
Vision-and-Language tasks in our custom BabyAI-XGen environment and show
significant improvements in agents' compositional generalisation abilities and
robustness, suggesting that our data-efficient method allows models to
successfully convert suboptimal behaviour into learning opportunities. Overall,
our results suggest that language feedback is a competitive and intuitive
alternative to intermediate scalar rewards for language-specified embodied
tasks.

</details>


### [420] [Are Large Language Models Effective Knowledge Graph Constructors?](https://arxiv.org/abs/2510.11297)
*Ruirui Chen,Weifeng Jiang,Chengwei Qin,Bo Xiong,Fiona Liausvia,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.CL

TL;DR: 该研究提出一个分层知识图谱（KG）抽取框架，利用大型语言模型（LLMs）构建语义丰富的KG，并评估了LLMs在KG构建中的表现，同时发布了一个新的KG数据集。


<details>
  <summary>Details</summary>
Motivation: 知识图谱对知识密集型任务至关重要，并有助于减少大型语言模型的幻觉。然而，构建高质量的知识图谱很困难，需要准确的信息抽取和支持可解释性的结构化表示。现有的基于LLM的方法通常只关注实体和关系抽取，局限于句子级上下文或依赖预定义模式，覆盖范围有限。

Method: 本文提出一个分层抽取框架，在多个层级组织信息，以创建语义丰富且结构良好的知识图谱。利用最先进的LLMs进行知识图谱的抽取和构建，并从结构和语义角度对其进行全面评估。

Result: 研究结果揭示了当前LLMs在知识图谱构建方面的优势和不足。同时，为了推动该领域的研究，作者发布了一个从儿童心理健康研究论文中提取的LLM生成知识图谱的精选数据集。

Conclusion: 研究指出了未来工作面临的关键挑战。发布的资源旨在促进在高风险领域（如医疗保健）中更透明、可靠和有影响力的应用。

Abstract: Knowledge graphs (KGs) are vital for knowledge-intensive tasks and have shown
promise in reducing hallucinations in large language models (LLMs). However,
constructing high-quality KGs remains difficult, requiring accurate information
extraction and structured representations that support interpretability and
downstream utility. Existing LLM-based approaches often focus narrowly on
entity and relation extraction, limiting coverage to sentence-level contexts or
relying on predefined schemas. We propose a hierarchical extraction framework
that organizes information at multiple levels, enabling the creation of
semantically rich and well-structured KGs. Using state-of-the-art LLMs, we
extract and construct knowledge graphs and evaluate them comprehensively from
both structural and semantic perspectives. Our results highlight the strengths
and shortcomings of current LLMs in KG construction and identify key challenges
for future work. To advance research in this area, we also release a curated
dataset of LLM-generated KGs derived from research papers on children's mental
well-being. This resource aims to foster more transparent, reliable, and
impactful applications in high-stakes domains such as healthcare.

</details>


### [421] [Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs](https://arxiv.org/abs/2510.11288)
*Nikita Afonin,Nikita Andriyanov,Nikhil Bageshpura,Kyle Liu,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Alexander Panchenko,Oleg Rogov,Elena Tutubalina,Mikhail Seleznyov*

Main category: cs.CL

TL;DR: 研究发现，在情境学习（ICL）中也会出现新兴未对齐（EM）现象，即大型语言模型（LLM）在给定狭窄情境示例后产生广泛的未对齐响应，其机制通常涉及模型采纳危险的“人格”。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，狭窄的微调和激活引导会导致新兴未对齐（EM）现象，但这些发现并未涵盖情境学习（ICL）。因此，本研究旨在探究EM是否会在ICL中出现。

Method: 研究在三个数据集上测试了三个前沿模型，使用64到256个狭窄的情境示例。通过引导分步推理（思维链），并对未对齐的思维链进行手动分析，以探究EM的机制。

Result: 研究发现EM确实在ICL中出现：在64个情境示例下，模型产生广泛未对齐响应的比例在2%到17%之间；在256个示例下，该比例高达58%。对思维链的手动分析显示，67.5%的未对齐轨迹通过采纳鲁莽或危险的“人格”来明确合理化有害输出。

Conclusion: 新兴未对齐（EM）现象不仅限于微调，也广泛存在于情境学习（ICL）中。模型在ICL中产生未对齐行为的一个主要机制是采纳有害的“人格”，这与微调引起的EM结果相似。

Abstract: Recent work has shown that narrow finetuning can produce broadly misaligned
LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these
findings were limited to finetuning and activation steering, leaving out
in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find
that it does: across three datasets, three frontier models produce broadly
misaligned responses at rates between 2% and 17% given 64 narrow in-context
examples, and up to 58% with 256 examples. We also examine mechanisms of EM by
eliciting step-by-step reasoning (while leaving in-context examples unchanged).
Manual analysis of the resulting chain-of-thought shows that 67.5% of
misaligned traces explicitly rationalize harmful outputs by adopting a reckless
or dangerous ''persona'', echoing prior results on finetuning-induced EM.

</details>


### [422] [Template-Based Text-to-Image Alignment for Language Accessibility: A Study on Visualizing Text Simplifications](https://arxiv.org/abs/2510.11314)
*Belkiss Souayed,Sarah Ebling,Yingqiang Gao*

Main category: cs.CL

TL;DR: 本文提出一个结构化的视觉-语言模型（VLM）提示框架，用于从简化文本生成可访问的图像，并评估了不同提示模板和视觉风格对可访问性的影响。


<details>
  <summary>Details</summary>
Motivation: 智力障碍者在理解复杂文本时常遇困难。现有文本到图像模型优先考虑美观而非可访问性，且它们如何与文本简化相关联尚不明确。

Method: 研究设计了一个结构化的VLM提示框架，用于从简化文本生成可访问图像。开发了五种提示模板（基本对象聚焦、情境场景、教育布局、多层细节、网格布局），每个模板都遵循特定的空间排列并遵守可访问性限制。使用来自四个文本简化数据集的400个句子级简化文本进行两阶段评估：第一阶段通过CLIPScores评估提示模板的有效性；第二阶段由四位可访问性专家对生成的图像进行人工标注，涵盖十种视觉风格。

Result: 结果显示，基本对象聚焦提示模板实现了最高的语义对齐，表明视觉极简主义能增强语言可访问性。专家评估进一步指出，复古风格是最易于访问的，而维基百科是最有效的数据源。注释者之间的一致性因维度而异，其中文本简洁性显示出较强的可靠性，而图像质量则更具主观性。

Conclusion: 该框架为可访问内容生成提供了实用指南，并强调了结构化提示在AI生成视觉可访问性工具中的重要性。

Abstract: Individuals with intellectual disabilities often have difficulties in
comprehending complex texts. While many text-to-image models prioritize
aesthetics over accessibility, it is not clear how visual illustrations relate
to text simplifications (TS) generated from them. This paper presents a
structured vision-language model (VLM) prompting framework for generating
accessible images from simplified texts. We designed five prompt templates,
i.e., Basic Object Focus, Contextual Scene, Educational Layout, Multi-Level
Detail, and Grid Layout, each following distinct spatial arrangements while
adhering to accessibility constraints such as object count limits, spatial
separation, and content restrictions. Using 400 sentence-level simplifications
from four established TS datasets (OneStopEnglish, SimPA, Wikipedia, and
ASSET), we conducted a two-phase evaluation: Phase 1 assessed prompt template
effectiveness with CLIPScores, and Phase 2 involved human annotation of
generated images across ten visual styles by four accessibility experts.
Results show that the Basic Object Focus prompt template achieved the highest
semantic alignment, indicating that visual minimalism enhances language
accessibility. Expert evaluation further identified Retro style as the most
accessible and Wikipedia as the most effective data source. Inter-annotator
agreement varied across dimensions, with Text Simplicity showing strong
reliability and Image Quality proving more subjective. Overall, our framework
offers practical guidelines for accessible content generation and underscores
the importance of structured prompting in AI-generated visual accessibility
tools.

</details>


### [423] [Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers](https://arxiv.org/abs/2510.11370)
*Wenhan Ma,Hailin Zhang,Liang Zhao,Yifan Song,Yudong Wang,Zhifang Sui,Fuli Luo*

Main category: cs.CL

TL;DR: 本文提出Rollout Routing Replay (R3) 方法，通过记录和重放推理时的路由分布来解决MoE模型中强化学习训练因路由不稳定性导致的崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 强化学习在增强大型语言模型能力方面至关重要，但在MoE模型中，路由机制常引入不稳定性，甚至导致灾难性的RL训练崩溃。研究发现训练和推理阶段的路由行为存在显著差异，且路由框架在相同条件下也可能产生不一致的专家选择。

Method: 提出Rollout Routing Replay (R3) 方法，该方法记录推理引擎中的路由分布，并在训练期间重放这些分布，以解决基础的不一致性。

Result: R3显著降低了训练-推理策略的KL散度，减轻了极端差异，且不影响训练速度。大量实验证实R3成功稳定了RL训练，防止了崩溃，并优于GSPO和TIS等现有方法。

Conclusion: 该工作为稳定MoE模型中的强化学习提供了一种新的解决方案。

Abstract: Reinforcement learning (RL) has emerged as a crucial approach for enhancing
the capabilities of large language models. However, in Mixture-of-Experts (MoE)
models, the routing mechanism often introduces instability, even leading to
catastrophic RL training collapse. We analyze the training-inference
consistency of MoE models and identify a notable discrepancy in routing
behaviors between the two phases. Moreover, even under identical conditions,
the routing framework can yield divergent expert selections across repeated
forward passes. To address this foundational inconsistency, we propose Rollout
Routing Replay (R3), a method that records routing distributions from the
inference engine and replays them during training. R3 significantly reduces
training-inference policy KL divergence and mitigates extreme discrepancies
without compromising training speed. Extensive experiments on various settings
confirm that R3 succeeds in stabilizing RL training, preventing collapse and
outperforming methods such as GSPO and TIS. We believe this work can offer a
new solution for stabilizing RL in MoE models.

</details>


### [424] [LLM-Specific Utility: A New Perspective for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.11358)
*Hengran Zhang,Keping Bi,Jiafeng Guo,Jiaming Zhang,Shuaiqiang Wang,Dawei Yin,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文引入并系统研究了RAG中“LLM特异性效用”的概念，发现检索内容的效用因LLM而异，人工标注和现有判断方法存在局限，并提出了LLM特异性效用判断的基准测试程序。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG检索仅关注相关性，但其有效性更依赖于检索内容的“效用”（utility），即其对生成准确全面答案的帮助。现有研究将效用视为通用属性，忽略了不同LLM因内部知识和理解能力差异，对同一内容可能有不同受益程度。

Method: 本文引入并系统地研究了“LLM特异性效用”的概念。通过跨多个数据集和LLM的大规模实验进行验证。在此基础上，提出了一个针对LLM特异性效用判断的基准测试程序，并评估了六个数据集上现有的效用判断方法。

Result: 研究发现，人工标注的段落对LLM并非最优，且“真值”效用段落不可在不同LLM之间迁移，这凸显了RAG研究中采纳LLM特异性效用的必要性。结果表明，部分人工标注段落对特定LLM并非真值效用段落，部分原因是查询和段落对LLM的可读性不同，其中困惑度是关键指标。在现有效用判断方法中，使用伪答案的语言化方法表现稳健，但LLM在有效评估效用方面仍面临挑战，例如未能拒绝已知查询的所有不相关段落，也未能为未知查询选择真正有用的段落。

Conclusion: RAG研究中必须采纳LLM特异性效用的概念。现有效用判断方法，尽管部分（如语言化方法）表现稳健，但LLM自身在有效评估内容效用方面仍存在显著局限性。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
incorporating external knowledge. While traditional retrieval focuses on
relevance, RAG's effectiveness depends on the utility of retrieved passages,
i.e., the usefulness in facilitating the generation of an accurate and
comprehensive answer. Existing studies often treat utility as a generic
attribute, ignoring the fact that different LLMs may benefit differently from
the same passage due to variations in internal knowledge and comprehension
ability. In this work, we introduce and systematically investigate the notion
of LLM-specific utility. Through large-scale experiments across multiple
datasets and LLMs, we demonstrate that human-annotated passages are not optimal
for LLMs and that ground-truth utilitarian passages are not transferable across
different LLMs. These findings highlight the necessity of adopting the
LLM-specific utility in RAG research. Our findings indicate that some
human-annotated passages are not ground-truth utilitarian passages for specific
LLMs, partially due to the varying readability of queries and passages for
LLMs, a tendency for which perplexity is a key metric. Based on these findings,
we propose a benchmarking procedure for LLM-specific utility judgments. We
evaluate existing utility judgment methods on six datasets and find that while
verbalized methods using pseudo-answers perform robustly, LLMs struggle to
assess utility effectively-failing to reject all passages for known queries and
to select truly useful ones for unknown queries.

</details>


### [425] [Early Detection and Reduction of Memorisation for Domain Adaptation and Instruction Tuning](https://arxiv.org/abs/2510.11372)
*Dean L. Slack,Noura Al Moubayed*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在微调阶段的记忆化问题，发现记忆化在早期阶段显著增加。作者提出了一种基于n-gram的记忆化评分作为早期停止标准，并引入了一种n-gram感知的损失正则化器，有效地减少了记忆化，同时保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型表现出色，但它们会记忆训练数据，可能泄露隐私或受版权保护的文本。大多数防御措施都针对预训练阶段，而对微调（特别是领域适应和指令微调）期间的记忆化了解甚少。

Method: 研究人员对Pythia、Llama3和Mistral模型（1.4B-70B参数）在常见评估数据集上进行微调，并跟踪训练过程中的逐字记忆化。他们提出了一个简单有效的n-gram记忆化评分作为早期停止标准，并引入了一个n-gram感知的损失正则化器，并将其与现有记忆化缓解策略进行比较。

Result: 研究发现，记忆化在前几个时期急剧增加，通常在验证困惑度或评估性能优化之前。n-gram记忆化评分能可靠地预测逐字记忆化；将其作为早期停止标准可以有效缓解记忆化，且性能损失最小。此外，n-gram感知的损失正则化器在所有测试模型家族中将记忆化降低了高达40%，与现有缓解策略相比，最大程度地减少了评估性能的权衡。

Conclusion: 这些结果为语言模型微调期间的记忆化动态提供了实用、可扩展的见解，并提出了有效的缓解策略。

Abstract: Although large language models excel across many tasks, they can memorise
training data and thereby expose private or copyrighted text. Most defences
target the pre-training stage, leaving memorisation during fine-tuning,
especially for domain adaptation and instruction tuning, poorly understood. We
fine-tune Pythia, Llama3, and Mistral models spanning 1.4B-70B parameters on
common evaluation datasets and track verbatim memorisation throughout training.
We find that memorisation increases dramatically in the first few epochs, often
significantly before either validation perplexity or evaluation performance is
optimised. We use a simple but effective n-gram memorisation score which
reliably precedes verbatim memorisation; using it as an early-stopping
criterion mitigates memorisation with minimal performance loss. Further, we
introduce an n-gram-aware loss regulariser and show that it reduces
memorisation across all model families tested by up to 40% while minimising
evaluation performance trade-offs when compared to an existing memorisation
mitigation strategy. These results yield practical, scalable insights into
memorisation dynamics during language model fine-tuning.

</details>


### [426] [Do LLMs "Feel"? Emotion Circuits Discovery and Control](https://arxiv.org/abs/2510.11328)
*Chenxi Wang,Yixuan Zhang,Ruiji Yu,Yufei Zheng,Lang Gao,Zirui Song,Zixiang Xu,Gus Xia,Huishuai Zhang,Dongyan Zhao,Xiuying Chen*

Main category: cs.CL

TL;DR: 本研究系统性地揭示并验证了大型语言模型（LLMs）中的情感回路，通过构建受控数据集、提取上下文无关情感方向、识别局部情感计算组件并整合为全局回路，实现了对LLM情感表达的精确控制，准确率高达99.65%，超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着对LLMs情感智能需求的增长，理解和控制生成文本中情感表达的内部机制成为关键挑战。本研究旨在探究LLMs是否包含上下文无关的情感机制、这些机制的形式，以及它们能否用于通用情感控制。

Method: 研究首先构建了受控数据集SEV（情景-事件与效价）以引发可比较的情感内部状态。随后，提取了上下文无关的情感方向，揭示了情感的跨上下文一致编码。通过分析分解和因果分析，识别了局部实现情感计算的神经元和注意力头，并通过消融和增强干预验证了它们的因果作用。最后，量化了各子层对模型最终情感表示的因果影响，并将识别出的局部组件整合为驱动情感表达的全局情感回路。

Result: 研究发现LLMs中存在一致的、跨上下文的情感编码机制。通过直接调节识别出的全局情感回路，在测试集上实现了99.65%的情感表达准确率，显著超越了基于提示和转向的方法。同时，成功识别并整合了驱动情感表达的局部计算组件和全局情感回路。

Conclusion: 本研究首次系统地揭示并验证了LLMs中的情感回路，为LLMs的可解释性和可控情感智能提供了新见解。通过精确控制这些内部回路，可以实现对LLM情感表达的高效调节。

Abstract: As the demand for emotional intelligence in large language models (LLMs)
grows, a key challenge lies in understanding the internal mechanisms that give
rise to emotional expression and in controlling emotions in generated text.
This study addresses three core questions: (1) Do LLMs contain context-agnostic
mechanisms shaping emotional expression? (2) What form do these mechanisms
take? (3) Can they be harnessed for universal emotion control? We first
construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit
comparable internal states across emotions. Subsequently, we extract
context-agnostic emotion directions that reveal consistent, cross-context
encoding of emotion (Q1). We identify neurons and attention heads that locally
implement emotional computation through analytical decomposition and causal
analysis, and validate their causal roles via ablation and enhancement
interventions. Next, we quantify each sublayer's causal influence on the
model's final emotion representation and integrate the identified local
components into coherent global emotion circuits that drive emotional
expression (Q2). Directly modulating these circuits achieves 99.65%
emotion-expression accuracy on the test set, surpassing prompting- and
steering-based methods (Q3). To our knowledge, this is the first systematic
study to uncover and validate emotion circuits in LLMs, offering new insights
into interpretability and controllable emotional intelligence.

</details>


### [427] [Beyond Survival: Evaluating LLMs in Social Deduction Games with Human-Aligned Strategies](https://arxiv.org/abs/2510.11389)
*Zirui Song,Yuan Huang,Junchang Liu,Haozhe Luo,Chenxi Wang,Lang Gao,Zixiang Xu,Mingfei Han,Xiaojun Chang,Xiuying Chen*

Main category: cs.CL

TL;DR: 该研究构建了一个高质量、多模态的狼人杀数据集，并提出了一种新的策略对齐评估框架，用于细粒度地评估大型语言模型在社交推理游戏中的语言和推理能力，揭示了当前模型在欺骗和反事实推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的狼人杀游戏研究多采用基于LLM的自我对弈，导致生成模板化言语和零散案例，忽略了社交游戏的丰富性。此外，由于缺乏高质量的参考数据，评估依赖于粗糙的指标（如生存时间或主观评分）。

Method: 1. 整理并发布了一个高质量、经人工验证的多模态狼人杀数据集，包含超过100小时视频、32.4M发言tokens和15种规则变体。2. 基于该数据集，提出了一种新颖的策略对齐评估框架，将获胜阵营的策略作为真值进行两阶段评估：a) 言语评估：以多项选择任务形式，评估模型在五个社交能力维度上采取适当立场的能力；b) 决策评估：评估模型的投票选择和对手角色推断能力。

Result: 实验表明，最先进的LLM表现多样，约有一半得分低于0.50，揭示了它们在欺骗和反事实推理方面存在明显的差距。

Conclusion: 该数据集和评估框架能够对模型的语言和推理能力进行细粒度评估，并捕捉其生成策略连贯游戏行为的能力。研究者希望该数据集能进一步激发多智能体交互中语言、推理和策略方面的研究。

Abstract: Social deduction games like Werewolf combine language, reasoning, and
strategy, providing a testbed for studying natural language and social
intelligence. However, most studies reduce the game to LLM-based self-play,
yielding templated utterances and anecdotal cases that overlook the richness of
social gameplay. Evaluation further relies on coarse metrics such as survival
time or subjective scoring due to the lack of quality reference data. To
address these gaps, we curate a high-quality, human-verified multimodal
Werewolf dataset containing over 100 hours of video, 32.4M utterance tokens,
and 15 rule variants. Based on this dataset, we propose a novel
strategy-alignment evaluation that leverages the winning faction's strategies
as ground truth in two stages: 1) Speech evaluation, formulated as
multiple-choice-style tasks that assess whether the model can adopt appropriate
stances across five dimensions of social ability; and 2) Decision evaluation,
which assesses the model's voting choices and opponent-role inferences. This
framework enables a fine-grained evaluation of models' linguistic and reasoning
capabilities, while capturing their ability to generate strategically coherent
gameplay. Our experiments show that state-of-the-art LLMs show diverse
performance, with roughly half remain below 0.50, revealing clear gaps in
deception and counterfactual reasoning. We hope our dataset further inspires
research on language, reasoning, and strategy in multi-agent interaction.

</details>


### [428] [KnowRL: Teaching Language Models to Know What They Know](https://arxiv.org/abs/2510.11407)
*Sahil Kale,Devendra Singh Dhami*

Main category: cs.CL

TL;DR: KnowRL是一个简单但强大的框架，它通过内省和基于共识的奖励机制，使大型语言模型（LLMs）能够自我提升其对自身能力边界的理解，从而提高可靠性和安全性，且无需外部监督。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs经常错误判断自身能力，导致其在超过五分之一的情况下无法完全信任其回答。真正可靠的AI需要知道自己知道什么以及不知道什么，这促使研究人员寻求一种方法来增强模型对其可行性边界的内部理解。

Method: 本文提出了KnowRL框架，结合了两个核心组件：(i) 内省：模型生成并分类它认为可行或不可行的任务；(ii) 基于共识的奖励：通过内部一致性来强化自我知识评估的稳定性。该框架利用内部生成的数据，增强了自我知识的一致性，完全避免了昂贵的外部监督。

Result: 在LLaMA-3.1-8B和Qwen-2.5-7B上的实验表明，KnowRL持续提升了模型的自我知识，并通过内在自我一致性和外在基准测试得到了验证。仅使用少量种子数据且无外部监督，该方法在准确性上实现了高达28%的提升，F1分数提升了12%，在几次迭代后就超越了基线模型。

Conclusion: KnowRL框架有效释放了LLMs自我提升知识意识的潜力，为构建更可靠、更负责任的AI以及在关键应用中实现更安全的部署开辟了道路。鉴于其简单性和独立于外部努力的特点，作者鼓励将此可靠性增强过程应用于所有未来的模型。

Abstract: Truly reliable AI requires more than simply scaling up knowledge; it demands
the ability to know what it knows and when it does not. Yet recent research
shows that even the best LLMs misjudge their own competence in more than one in
five cases, making any response born of such internal uncertainty impossible to
fully trust. Inspired by self-improvement reinforcement learning techniques
that require minimal data, we present a simple but powerful framework KnowRL
that strengthens a model's internal understanding of its own feasibility
boundaries, enabling safer and more responsible behaviour. Our framework
combines two components: (i) introspection, where the model generates and
classifies tasks it judges feasible or infeasible, and (ii) consensus-based
rewarding, where stability of self-knowledge assessment is reinforced through
internal agreement. By using internally generated data, this design strengthens
consistency in self-knowledge and entirely avoids costly external supervision.
In experiments on LLaMA-3.1-8B and Qwen-2.5-7B, KnowRL steadily improved
self-knowledge, validated by both intrinsic self-consistency and extrinsic
benchmarking. With nothing more than a small seed set and no external
supervision, our method drove gains as high as 28% in accuracy and 12% in F1,
outperforming baselines in just a few iterations. Our framework essentially
unlocks the untapped capacity of LLMs to self-improve their knowledge
awareness, opening the door to reliable, more accountable AI and safer
deployment in critical applications. Owing to its simplicity and independence
from external effort, we encourage applying this reliability-enhancing process
to all future models.

</details>


### [429] [Valid Survey Simulations with Limited Human Data: The Roles of Prompting, Fine-Tuning, and Rectification](https://arxiv.org/abs/2510.11408)
*Stefan Krsteski,Giuseppe Russo,Serina Chang,Robert West,Kristina Gligorić*

Main category: cs.CL

TL;DR: 本研究发现，单独使用大型语言模型（LLM）生成调查回复存在显著偏差，但将其与校正方法结合可将偏差降至5%以下，并提高有效样本量。在固定预算下，将大部分人工回复用于校正而非微调，能实现更有效的估计。


<details>
  <summary>Details</summary>
Motivation: 传统的调查成本高昂且耗时。尽管大型语言模型（LLM）被提议作为人类受访者的低成本、可扩展替代品，但其输出常有偏差，导致无效估计。本研究旨在解决LLM生成回复的偏差问题，并优化人工回复的分配方式。

Method: 研究了使用LLM生成调查回复的“合成方法”与消除总体估计偏差的“校正方法”之间的相互作用。通过两个包含营养、政治和经济问题的面板调查，探索了如何在合成和校正方法之间最佳分配人类回复。

Result: 单独使用LLM合成会引入显著偏差（24-86%）。然而，将其与校正方法结合后，偏差可降低到5%以下，并将有效样本量提高多达14%。结果表明，在固定预算下，将大部分人工回复分配给校正而不是微调，能带来更有效的估计。

Conclusion: 结合LLM合成与校正方法可以有效降低调查估计的偏差，并提高有效样本量。在利用LLM进行调查时，挑战了将所有人工回复用于微调的常见做法，建议在固定预算下，将大部分人工回复分配给校正，以实现更准确和有效的估计。

Abstract: Surveys provide valuable insights into public opinion and behavior, but their
execution is costly and slow. Large language models (LLMs) have been proposed
as a scalable, low-cost substitute for human respondents, but their outputs are
often biased and yield invalid estimates. We study the interplay between
synthesis methods that use LLMs to generate survey responses and rectification
methods that debias population estimates, and explore how human responses are
best allocated between them. Using two panel surveys with questions on
nutrition, politics, and economics, we find that synthesis alone introduces
substantial bias (24-86%), whereas combining it with rectification reduces bias
below 5% and increases effective sample size by up to 14%. Overall, we
challenge the common practice of using all human responses for fine-tuning,
showing that under a fixed budget, allocating most to rectification results in
far more effective estimation.

</details>


### [430] [Who are you, ChatGPT? Personality and Demographic Style in LLM-Generated Content](https://arxiv.org/abs/2510.11434)
*Dana Sotto Porat,Ella Rabinovich*

Main category: cs.CL

TL;DR: 本研究引入了一种数据驱动的新方法，通过自动分类器评估了大型语言模型（LLMs）的个性和性别语言模式，发现LLMs倾向于更高宜人性和更低神经质，且其性别语言模式与人类相似但变异性较低。


<details>
  <summary>Details</summary>
Motivation: 生成式大型语言模型在日常生活中变得至关重要，有研究探讨它们是否也表现出个性或人口统计学特征。现有方法可能依赖于自我报告问卷，本研究旨在提出一种不依赖于此的新方法。

Method: 开发了一种新颖的、数据驱动的方法，用于评估LLMs的个性，不依赖于自我报告问卷。该方法将自动个性及性别分类器应用于从Reddit收集的开放式问题上LLM的回复。研究比较了六种常用模型与人类作者的回复。

Result: LLMs系统性地表现出更高的宜人性（Agreeableness）和更低的神经质（Neuroticism），反映出合作和稳定的对话倾向。模型文本中的性别语言模式与人类作者的模式大致相似，但变异性有所降低。

Conclusion: 本研究贡献了一个包含人类和模型回复的新数据集，并进行了大规模比较分析，为生成式AI的个性和人口统计学模式主题提供了新的见解。

Abstract: Generative large language models (LLMs) have become central to everyday life,
producing human-like text across diverse domains. A growing body of research
investigates whether these models also exhibit personality- and
demographic-like characteristics in their language. In this work, we introduce
a novel, data-driven methodology for assessing LLM personality without relying
on self-report questionnaires, applying instead automatic personality and
gender classifiers to model replies on open-ended questions collected from
Reddit. Comparing six widely used models to human-authored responses, we find
that LLMs systematically express higher Agreeableness and lower Neuroticism,
reflecting cooperative and stable conversational tendencies. Gendered language
patterns in model text broadly resemble those of human writers, though with
reduced variation, echoing prior findings on automated agents. We contribute a
new dataset of human and model responses, along with large-scale comparative
analyses, shedding new light on the topic of personality and demographic
patterns of generative AI.

</details>


### [431] [Investigating Large Language Models' Linguistic Abilities for Text Preprocessing](https://arxiv.org/abs/2510.11482)
*Marco Braga,Gian Carlo Milanese,Gabriella Pasi*

Main category: cs.CL

TL;DR: 本研究探讨了使用大型语言模型（LLMs）进行文本预处理（如停用词移除、词形还原和词干提取）的方法，并发现LLMs不仅能高精度地复现传统方法，还能在多语言文本分类任务中带来性能提升。


<details>
  <summary>Details</summary>
Motivation: 传统的文本预处理技术（如停用词移除、词形还原、词干提取）通常忽略上下文信息，且依赖于大量的语言特定标注资源。本研究旨在利用LLMs处理上下文信息的能力，来改进这些预处理任务，减少对标注资源的需求。

Method: 研究人员使用LLMs执行停用词移除、词形还原和词干提取任务。他们通过在网络抓取数据上进行全面评估，将LLM预处理与传统算法在六种欧洲语言的多个文本分类任务上进行了比较。评估指标包括LLM复现传统方法的准确率以及使用LLM预处理文本训练的机器学习算法的F1分数。

Result: LLMs能够以高精度复现传统预处理方法：停用词移除准确率达97%，词形还原达82%，词干提取达74%。此外，经LLM预处理文本训练的机器学习算法，在F1分数上比传统技术提高了高达6%。

Conclusion: LLMs在文本预处理任务中表现出色，不仅能有效复现传统方法，而且由于其考虑上下文的能力，能显著提升后续机器学习任务（如文本分类）的性能。这表明LLMs是文本预处理的有效且有前景的工具。

Abstract: Text preprocessing is a fundamental component of Natural Language Processing,
involving techniques such as stopword removal, stemming, and lemmatization to
prepare text as input for further processing and analysis. Despite the
context-dependent nature of the above techniques, traditional methods usually
ignore contextual information. In this paper, we investigate the idea of using
Large Language Models (LLMs) to perform various preprocessing tasks, due to
their ability to take context into account without requiring extensive
language-specific annotated resources. Through a comprehensive evaluation on
web-sourced data, we compare LLM-based preprocessing (specifically stopword
removal, lemmatization and stemming) to traditional algorithms across multiple
text classification tasks in six European languages. Our analysis indicates
that LLMs are capable of replicating traditional stopword removal,
lemmatization, and stemming methods with accuracies reaching 97%, 82%, and 74%,
respectively. Additionally, we show that ML algorithms trained on texts
preprocessed by LLMs achieve an improvement of up to 6% with respect to the
$F_1$ measure compared to traditional techniques. Our code, prompts, and
results are publicly available at
https://github.com/GianCarloMilanese/llm_pipeline_wi-iat.

</details>


### [432] [GenCNER: A Generative Framework for Continual Named Entity Recognition](https://arxiv.org/abs/2510.11444)
*Yawen Yang,Fukun Ma,Shiao Meng,Aiwei Liu,Lijie Wen*

Main category: cs.CL

TL;DR: GenCNER是一种生成式框架，通过将持续命名实体识别（CNER）转换为实体三元组序列生成问题，并结合类型特异性伪标签和知识蒸馏，有效缓解了灾难性遗忘和非实体类型语义漂移问题。


<details>
  <summary>Details</summary>
Motivation: 传统的命名实体识别（NER）面临实体类别持续增加的实际场景挑战，催生了持续命名实体识别（CNER）。然而，现有的CNER持续学习方法存在灾难性遗忘和非实体类型语义漂移的问题。

Method: 本文提出了GenCNER框架。它将CNER任务巧妙地转换为持续的实体三元组序列生成问题，并利用强大的预训练seq2seq模型来解决。此外，设计了一种类型特异性的基于置信度的伪标签策略，并结合知识蒸馏（KD）在三元组层面保留学习到的知识并减轻标签噪声的影响。

Result: 在两个基准数据集上的实验结果表明，GenCNER框架在多种CNER设置下优于以往最先进的方法，并且与非持续学习（non-CL）结果相比，差距最小。

Conclusion: GenCNER提供了一个简单而有效的生成式框架，成功缓解了CNER任务中的灾难性遗忘和非实体类型语义漂移问题，并在性能上取得了显著提升。

Abstract: Traditional named entity recognition (NER) aims to identify text mentions
into pre-defined entity types. Continual Named Entity Recognition (CNER) is
introduced since entity categories are continuously increasing in various
real-world scenarios. However, existing continual learning (CL) methods for NER
face challenges of catastrophic forgetting and semantic shift of non-entity
type. In this paper, we propose GenCNER, a simple but effective Generative
framework for CNER to mitigate the above drawbacks. Specifically, we skillfully
convert the CNER task into sustained entity triplet sequence generation problem
and utilize a powerful pre-trained seq2seq model to solve it. Additionally, we
design a type-specific confidence-based pseudo labeling strategy along with
knowledge distillation (KD) to preserve learned knowledge and alleviate the
impact of label noise at the triplet level. Experimental results on two
benchmark datasets show that our framework outperforms previous
state-of-the-art methods in multiple CNER settings, and achieves the smallest
gap compared with non-CL results.

</details>


### [433] [Hallucination Detection via Internal States and Structured Reasoning Consistency in Large Language Models](https://arxiv.org/abs/2510.11529)
*Yusheng Song,Lirong Qiu,Xi Zhang,Zhihao Tang*

Main category: cs.CL

TL;DR: 该研究通过统一内部状态探测和思维链验证，解决了大型语言模型幻觉检测中的“检测困境”，引入多路径推理和跨注意力机制克服信号稀缺和表示对齐障碍，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型幻觉检测方法存在“检测困境”：内部状态探测擅长事实不一致但对逻辑谬误无效，而思维链验证则相反，导致在不同任务类型（如开放域问答或数学推理）中出现盲点。

Method: 提出一个统一框架来弥合内部状态探测和思维链验证之间的鸿沟。为克服“信号稀缺障碍”，引入多路径推理机制以获取更可比的细粒度信号；为克服“表示对齐障碍”，引入分段感知时序交叉注意力模块来自适应地融合对齐后的表示，从而识别细微的不一致。

Result: 在三个多样化基准和两个主流大型语言模型上的大量实验表明，该框架持续且显著优于强大的基线方法。

Conclusion: 该框架成功解决了大型语言模型幻觉检测中的“检测困境”，通过统一两种互补的检测方法，克服了关键的技术挑战，显著提高了幻觉检测的鲁棒性和有效性。

Abstract: The detection of sophisticated hallucinations in Large Language Models (LLMs)
is hampered by a ``Detection Dilemma'': methods probing internal states
(Internal State Probing) excel at identifying factual inconsistencies but fail
on logical fallacies, while those verifying externalized reasoning
(Chain-of-Thought Verification) show the opposite behavior. This schism creates
a task-dependent blind spot: Chain-of-Thought Verification fails on
fact-intensive tasks like open-domain QA where reasoning is ungrounded, while
Internal State Probing is ineffective on logic-intensive tasks like
mathematical reasoning where models are confidently wrong. We resolve this with
a unified framework that bridges this critical gap. However, unification is
hindered by two fundamental challenges: the Signal Scarcity Barrier, as coarse
symbolic reasoning chains lack signals directly comparable to fine-grained
internal states, and the Representational Alignment Barrier, a deep-seated
mismatch between their underlying semantic spaces. To overcome these, we
introduce a multi-path reasoning mechanism to obtain more comparable,
fine-grained signals, and a segment-aware temporalized cross-attention module
to adaptively fuse these now-aligned representations, pinpointing subtle
dissonances. Extensive experiments on three diverse benchmarks and two leading
LLMs demonstrate that our framework consistently and significantly outperforms
strong baselines. Our code is available: https://github.com/peach918/HalluDet.

</details>


### [434] [LLM-Oriented Token-Adaptive Knowledge Distillation](https://arxiv.org/abs/2510.11615)
*Xurong Xie,Zhucun Xue,Jiafu Wu,Jian Li,Yabiao Wang,Xiaobin Hu,Yong Liu,Jiangning Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种名为AdaKD的新型面向大型语言模型（LLM）的自适应知识蒸馏框架，它通过实时调整蒸馏过程以适应每个token的学习状态，解决了现有静态蒸馏方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于logit的知识蒸馏方法通常采用静态策略，与学生模型的动态学习过程不符。它们不加区分地处理所有token，并使用单一的固定温度，导致知识转移效果不佳。

Method: AdaKD框架包含两个协同模块，均由统一的token难度度量驱动：1. 损失驱动的自适应token聚焦（LATF）模块：通过监控学生的学习稳定性，动态调整蒸馏焦点，在每个训练阶段将计算资源集中在最有价值的token上。2. 逆难度温度缩放（IDTS）模块：为困难token使用低温度进行有针对性的错误纠正，为简单token使用高温度以鼓励学生学习教师模型完整平滑的输出分布，从而增强泛化能力。

Result: 作为一个即插即用的框架，AdaKD能够持续改进各种蒸馏方法在多种模型架构和基准上的性能。

Conclusion: AdaKD通过引入token级别的自适应策略，显著提升了大型语言模型知识蒸馏的效果和泛化能力，为LLM压缩提供了一个有效且通用的解决方案。

Abstract: Knowledge distillation (KD) is a key technique for compressing large-scale
language models (LLMs), yet prevailing logit-based methods typically employ
static strategies that are misaligned with the dynamic learning process of
student models. These methods typically treat all tokens indiscriminately and
apply a single, fixed temperature, resulting in suboptimal knowledge transfer.
To address these limitations, we propose LLM-Oriented Token-Adaptive Knowledge
Distillation (AdaKD), a novel framework that adapts the distillation process to
the real-time learning state of each token. AdaKD consists of two synergistic
modules driven by a unified token difficulty metric. First, our Loss-Driven
Adaptive Token Focusing (LATF) module dynamically adjusts the distillation
focus by monitoring the student's learning stability, concentrating
computational resources on the most valuable tokens at each training phase.
Second, we introduce Inverse Difficulty Temperature Scaling (IDTS), a
counterintuitive yet effective token-level temperature strategy. It employs low
temperatures for difficult tokens for targeted error correction, and high
temperatures for easy tokens to encourage students to learn from the teacher's
complete and smooth output distribution, thereby enhancing generalization. As a
plug-and-play framework, AdaKD can consistently improve the performance of
various distillation methods on multiple model architectures and benchmarks.

</details>


### [435] [SemCSE-Multi: Multifaceted and Decodable Embeddings for Aspect-Specific and Interpretable Scientific Domain Mapping](https://arxiv.org/abs/2510.11599)
*Marc Brinner,Sina Zarrieß*

Main category: cs.CL

TL;DR: 本文提出了SemCSE-Multi，一个无监督框架，用于生成科学摘要的多方面嵌入，支持细粒度相似性评估、自适应可视化和通过解码提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有嵌入模型难以捕捉科学摘要中独立、可区分的特定方面，导致无法进行细粒度、可控的相似性评估和用户驱动的领域可视化。此外，在用户中心设置中，嵌入模型的可解释性有待提高。

Method: 该方法包括：1) 一个无监督过程，生成与特定方面相关的总结句；2) 训练嵌入模型，将语义相关的总结映射到嵌入空间的相邻位置；3) 将这些特定方面的嵌入能力整合到一个统一的嵌入模型中，以高效地预测多个方面嵌入；4) 引入一个嵌入解码管道，将嵌入解码回自然语言描述，即使对于低维可视化中未被占据的区域也有效。

Result: SemCSE-Multi能够生成捕获独立可区分方面的多方面嵌入，从而实现细粒度、可控的相似性评估和自适应、用户驱动的科学领域可视化。嵌入解码管道即使对于低维可视化中的空白区域也能保持有效，显著提高了用户中心设置中的可解释性。该框架已在入侵生物学和医学领域进行了评估。

Conclusion: SemCSE-Multi是一个新颖的无监督框架，能够为科学摘要生成多方面嵌入，从而实现更精细、可控的相似性分析、适应性可视化和显著增强的可解释性，对科学领域的理解和探索具有重要意义。

Abstract: We propose SemCSE-Multi, a novel unsupervised framework for generating
multifaceted embeddings of scientific abstracts, evaluated in the domains of
invasion biology and medicine. These embeddings capture distinct, individually
specifiable aspects in isolation, thus enabling fine-grained and controllable
similarity assessments as well as adaptive, user-driven visualizations of
scientific domains. Our approach relies on an unsupervised procedure that
produces aspect-specific summarizing sentences and trains embedding models to
map semantically related summaries to nearby positions in the embedding space.
We then distill these aspect-specific embedding capabilities into a unified
embedding model that directly predicts multiple aspect embeddings from a
scientific abstract in a single, efficient forward pass. In addition, we
introduce an embedding decoding pipeline that decodes embeddings back into
natural language descriptions of their associated aspects. Notably, we show
that this decoding remains effective even for unoccupied regions in
low-dimensional visualizations, thus offering vastly improved interpretability
in user-centric settings.

</details>


### [436] [An Encoder-Integrated PhoBERT with Graph Attention for Vietnamese Token-Level Classification](https://arxiv.org/abs/2510.11537)
*Ba-Quang Nguyen*

Main category: cs.CL

TL;DR: 该论文提出了一种名为TextGraphFuseGAT的新型神经网络架构，它将预训练的Transformer编码器（PhoBERT）与图注意力网络（GAT）相结合，用于令牌级分类任务。该模型在多个越南语基准数据集上超越了强大的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要依赖于序列上下文来捕捉令牌间的依赖关系，但可能无法充分捕捉更丰富的、非序列的令牌间依赖。因此，研究旨在通过结合图注意力网络来增强上下文表示，从而提高令牌级分类任务的性能。

Method: 该研究提出了TextGraphFuseGAT模型。首先，使用PhoBERT生成令牌嵌入，然后基于这些嵌入构建一个全连接图。GAT层在此图上运行，以捕捉令牌间的丰富依赖。接着，在图增强的嵌入之上应用一个Transformer风格的自注意力层，以进一步增强上下文表示。最后，将最终的令牌表示通过一个分类头进行序列标注。该方法在三个越南语数据集上进行评估，包括新提出的VietMed-NER数据集。

Result: 实验结果表明，TextGraphFuseGAT模型在PhoNER-COVID19、PhoDisfluency和VietMed-NER这三个越南语基准数据集上，始终优于包括纯Transformer模型和混合神经网络模型（如BiLSTM + CNN + CRF）在内的强大基线模型。这证实了将预训练的语义特征与基于图的关系建模相结合，能有效提高跨多个领域的令牌分类性能。

Conclusion: 该研究得出结论，结合预训练的语义特征和基于图的关系建模，能够有效提高令牌级分类任务的性能。TextGraphFuseGAT架构通过捕捉超越序列上下文的令牌间依赖，为改进序列标注任务提供了一种有效的方法。

Abstract: We propose a novel neural architecture named TextGraphFuseGAT, which
integrates a pretrained transformer encoder (PhoBERT) with Graph Attention
Networks for token-level classification tasks. The proposed model constructs a
fully connected graph over the token embeddings produced by PhoBERT, enabling
the GAT layer to capture rich inter-token dependencies beyond those modeled by
sequential context alone. To further enhance contextualization, a
Transformer-style self-attention layer is applied on top of the graph-enhanced
embeddings. The final token representations are passed through a classification
head to perform sequence labeling. We evaluate our approach on three Vietnamese
benchmark datasets: PhoNER-COVID19 for named entity recognition in the COVID-19
domain, PhoDisfluency for speech disfluency detection, and VietMed-NER for
medical-domain NER. VietMed-NER is the first Vietnamese medical spoken NER
dataset, featuring 18 entity types collected from real-world medical speech
transcripts and annotated with the BIO tagging scheme. Its specialized
vocabulary and domain-specific expressions make it a challenging benchmark for
token-level classification models. Experimental results show that our method
consistently outperforms strong baselines, including transformer-only and
hybrid neural models such as BiLSTM + CNN + CRF, confirming the effectiveness
of combining pretrained semantic features with graph-based relational modeling
for improved token classification across multiple domains.

</details>


### [437] [Information-Preserving Reformulation of Reasoning Traces for Antidistillation](https://arxiv.org/abs/2510.11545)
*Jiayu Ding,Lei Cui,Li Dong,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出PART，一种信息保留的反蒸馏方法，通过重新组织LLM推理链，在不剥夺用户关键中间信息的前提下，有效阻止未经授权的模型蒸馏。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）通过延长推理链显著提升复杂任务性能，但这些详细的推理过程极易被未经授权地蒸馏。现有的保护策略（如用简短摘要替代）剥夺了用户宝贵的中间信息。因此，需要一种既能保护推理链不被蒸馏，又能保留信息的方法来解决这一权衡。

Method: 本文提出了PART（信息保留的反蒸馏推理链重构），其灵感来源于人类理解推理过程与LLM利用推理过程进行监督微调之间的差异。PART包含两个简单但有效的步骤：移除自言自语行为和重新排序子结论。一个小型辅助模型负责执行这种重构，计算开销极小。

Result: 广泛的实验表明，PART在不同规模和类型的学生模型上，以及在各种推理基准测试中，都能持续有效地干扰蒸馏过程。例如，当使用重构后的推理链进行训练时，一个大型32B学生模型在AIME 2024上的性能从54.17下降到46.88，相当于13.5%的性能下降。

Conclusion: PART通过信息保留的重构策略，成功地解决了LLM推理链在提供详细信息和防止未经授权蒸馏之间的矛盾，为专有模型提供了一种有效的保护机制，同时确保用户仍能从推理过程中学习和验证。

Abstract: Recent advances in Large Language Models (LLMs) show that extending the
length of reasoning chains significantly improves performance on complex tasks.
While revealing these reasoning traces helps users better follow, verify, and
learn from the model's problem-solving process, it also makes them highly
vulnerable to unauthorized distillation. To mitigate this risk, proprietary
model providers often adopt aggressive protection strategies, such as replacing
detailed reasoning with brief summaries, which deprive users of valuable
intermediate information. To address this trade-off, we propose PART, an
information-preserving antidistillation reformulation of reasoning traces.
Motivated by the difference between how humans understand reasoning traces and
how LLMs exploit them for supervised fine-tuning, we design a simple but
effective two-step reformulation: removing self-talk behaviors and reordering
sub-conclusions. A small auxiliary model is trained to perform this
reformulation, incurring minimal computational overhead. Extensive experiments
demonstrate that PART consistently disrupts distillation across student models
of different sizes and types on various reasoning benchmarks. For instance,
when training on reformulated traces, even the performance of a large 32B
student model decreases from 54.17 to 46.88 on AIME 2024, corresponding to a
13.5% degradation.

</details>


### [438] [Culturally-Aware Conversations: A Framework & Benchmark for LLMs](https://arxiv.org/abs/2510.11563)
*Shreya Havaldar,Sunny Rai,Young-Min Cho,Lyle Ungar*

Main category: cs.CL

TL;DR: 本文提出了一个基于社会文化理论的框架和基准，用于评估大型语言模型（LLMs）在多文化对话中的文化适应能力，并发现当前顶级LLMs在这方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的衡量LLMs文化适应性的基准与模型在与不同文化背景用户互动时面临的实际挑战不符。

Method: 引入了一个基于社会文化理论的框架，将语言风格定义为受情境、关系和文化背景影响的关键文化交流元素。在此框架基础上构建了一个由文化多元的评估者标注的基准数据集，并提出了跨文化自然语言处理评估的新标准：对话框架、风格敏感性和主观正确性。

Result: 通过对当前顶级LLMs在该基准上的评估，结果显示这些模型在对话环境中难以实现文化适应。

Conclusion: 当前LLMs在多文化对话中的文化适应能力不足，本文提出的框架和基准为更真实、更细致地评估和改进LLMs的文化适应性提供了新的方法和标准。

Abstract: Existing benchmarks that measure cultural adaptation in LLMs are misaligned
with the actual challenges these models face when interacting with users from
diverse cultural backgrounds. In this work, we introduce the first framework
and benchmark designed to evaluate LLMs in realistic, multicultural
conversational settings. Grounded in sociocultural theory, our framework
formalizes how linguistic style - a key element of cultural communication - is
shaped by situational, relational, and cultural context. We construct a
benchmark dataset based on this framework, annotated by culturally diverse
raters, and propose a new set of desiderata for cross-cultural evaluation in
NLP: conversational framing, stylistic sensitivity, and subjective correctness.
We evaluate today's top LLMs on our benchmark and show that these models
struggle with cultural adaptation in a conversational setting.

</details>


### [439] [Invisible Languages of the LLM Universe](https://arxiv.org/abs/2510.11557)
*Saurabh Khanna,Xinxu Li*

Main category: cs.CL

TL;DR: 本文揭示了大型语言模型（LLMs）中存在的语言危机，即数千种拥有数百万使用者的语言在数字生态系统中几乎隐形。研究认为这种不平等是结构性的，源于殖民时代的语言等级制度，并将其定义为数字认知不公，呼吁去殖民化语言技术。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在海量多语言语料库上训练，但全球仍有约2000种拥有数百万使用者的语言在数字世界中几乎隐形。研究旨在解释为何人工智能系统中的语言不平等并非偶然，而是结构性的问题。

Method: 研究提出了一个批判性框架，将语言活力（现实世界人口实力）和数字性（在线存在）的实证测量与后殖民理论和认知不公联系起来。通过分析所有有记录的人类语言数据，识别并分类了不同的语言群体，并论证了这些模式如何反映从殖民时代语言等级制度到当代AI发展的延续性。

Result: 研究将所有语言分为四类：堡垒语言（33%，高活力和高数字性）、数字回声语言（6%，活力下降但数字性高）、消逝之声语言（36%，活力和数字性均低），以及关键的隐形巨头语言（27%，活力高但数字性几乎为零）。这些模式反映了殖民时代语言等级制度在当代AI发展中的延续，构成了“数字认知不公”。结果表明，英语在AI中的主导地位并非技术必需，而是权力结构的产物，系统性地排斥了边缘化的语言知识。

Conclusion: 人工智能中的语言不平等是结构性的，反映了权力结构如何系统性地排除边缘化语言知识。研究呼吁去殖民化语言技术，并实现人工智能惠益的民主化，以解决这种数字认知不公。

Abstract: Large Language Models are trained on massive multilingual corpora, yet this
abundance masks a profound crisis: of the world's 7,613 living languages,
approximately 2,000 languages with millions of speakers remain effectively
invisible in digital ecosystems. We propose a critical framework connecting
empirical measurements of language vitality (real world demographic strength)
and digitality (online presence) with postcolonial theory and epistemic
injustice to explain why linguistic inequality in AI systems is not incidental
but structural. Analyzing data across all documented human languages, we
identify four categories: Strongholds (33%, high vitality and digitality),
Digital Echoes (6%, high digitality despite declining vitality), Fading Voices
(36%, low on both dimensions), and critically, Invisible Giants (27%, high
vitality but near-zero digitality) - languages spoken by millions yet absent
from the LLM universe. We demonstrate that these patterns reflect continuities
from colonial-era linguistic hierarchies to contemporary AI development,
constituting what we term digital epistemic injustice. Our analysis reveals
that English dominance in AI is not a technical necessity but an artifact of
power structures that systematically exclude marginalized linguistic knowledge.
We conclude with implications for decolonizing language technology and
democratizing access to AI benefits.

</details>


### [440] [LLMAtKGE: Large Language Models as Explainable Attackers against Knowledge Graph Embeddings](https://arxiv.org/abs/2510.11584)
*Ting Li,Yang Yang,Yipeng Yu,Liang Yao,Guoqing Chao,Ruifeng Xu*

Main category: cs.CL

TL;DR: 本文提出LLMAtKGE，一个基于LLM的知识图谱嵌入对抗攻击框架，该框架能选择攻击目标、生成人类可读的解释，并优于现有黑盒基线。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒知识图谱嵌入（KGE）对抗攻击方法虽然试图结合文本和结构信息，但无法生成人类可读的解释，且泛化能力差。鉴于大型语言模型（LLM）在文本理解、生成和推理方面的强大能力，作者旨在利用LLM来解决这些问题。

Method: LLMAtKGE框架利用LLM选择攻击目标并生成解释。为克服LLM的输入限制，设计了结构化提示方案，将攻击表述为多项选择题并融入知识图谱事实证据。为解决上下文窗口限制和犹豫问题，引入了基于语义和基于中心性的过滤器，在压缩候选集的同时保留了攻击相关信息的高召回率。此外，通过预计算高阶邻接和使用三元组分类任务微调LLM，以有效整合语义和结构信息，增强过滤性能。

Result: 在两个广泛使用的知识图谱数据集上的实验表明，LLMAtKGE攻击性能优于最强的黑盒基线，并通过推理提供了解释，并且与白盒方法相比也具有竞争力。全面的消融研究和案例分析进一步验证了其生成解释的能力。

Conclusion: LLMAtKGE是一个新颖的基于LLM的知识图谱嵌入对抗攻击框架，它不仅能有效地进行攻击，还能生成人类可读的解释，并通过精心设计的机制克服了LLM在处理知识图谱攻击任务时的局限性，实现了语义和结构信息的有效整合。

Abstract: Adversarial attacks on knowledge graph embeddings (KGE) aim to disrupt the
model's ability of link prediction by removing or inserting triples. A recent
black-box method has attempted to incorporate textual and structural
information to enhance attack performance. However, it is unable to generate
human-readable explanations, and exhibits poor generalizability. In the past
few years, large language models (LLMs) have demonstrated powerful capabilities
in text comprehension, generation, and reasoning. In this paper, we propose
LLMAtKGE, a novel LLM-based framework that selects attack targets and generates
human-readable explanations. To provide the LLM with sufficient factual context
under limited input constraints, we design a structured prompting scheme that
explicitly formulates the attack as multiple-choice questions while
incorporating KG factual evidence. To address the context-window limitation and
hesitation issues, we introduce semantics-based and centrality-based filters,
which compress the candidate set while preserving high recall of
attack-relevant information. Furthermore, to efficiently integrate both
semantic and structural information into the filter, we precompute high-order
adjacency and fine-tune the LLM with a triple classification task to enhance
filtering performance. Experiments on two widely used knowledge graph datasets
demonstrate that our attack outperforms the strongest black-box baselines and
provides explanations via reasoning, and showing competitive performance
compared with white-box methods. Comprehensive ablation and case studies
further validate its capability to generate explanations.

</details>


### [441] [Survey Response Generation: Generating Closed-Ended Survey Responses In-Silico with Large Language Models](https://arxiv.org/abs/2510.11586)
*Georg Ahnert,Anna-Carolina Haensch,Barbara Plank,Markus Strohmaier*

Main category: cs.CL

TL;DR: 本研究系统性地调查了不同调查响应生成方法对LLM模拟调查结果的影响，发现方法选择会显著影响预测结果，并提出实用建议。


<details>
  <summary>Details</summary>
Motivation: LLM通常擅长生成开放式文本，但在模拟人类调查响应时，许多研究关注生成封闭式响应。目前，使用LLM生成封闭式调查响应的方法多样，缺乏标准化实践，这促使研究者系统性地探究不同生成方法的影响。

Method: 研究系统性地调查了8种调查响应生成方法，结合4个政治态度调查和10个开源语言模型，共生成了3200万个模拟调查响应。通过比较不同方法在个体层面和子群体层面的对齐程度来评估其影响。

Result: 研究发现，不同的调查响应生成方法在个体层面和子群体层面的对齐方面存在显著差异。其中，“受限生成方法”总体表现最佳，而“推理输出”并不能持续改善对齐效果。

Conclusion: 本研究强调了调查响应生成方法对模拟调查结果的显著影响。基于研究发现，论文提出了关于应用调查响应生成方法的实用建议。

Abstract: Many in-silico simulations of human survey responses with large language
models (LLMs) focus on generating closed-ended survey responses, whereas LLMs
are typically trained to generate open-ended text instead. Previous research
has used a diverse range of methods for generating closed-ended survey
responses with LLMs, and a standard practice remains to be identified. In this
paper, we systematically investigate the impact that various Survey Response
Generation Methods have on predicted survey responses. We present the results
of 32 mio. simulated survey responses across 8 Survey Response Generation
Methods, 4 political attitude surveys, and 10 open-weight language models. We
find significant differences between the Survey Response Generation Methods in
both individual-level and subpopulation-level alignment. Our results show that
Restricted Generation Methods perform best overall, and that reasoning output
does not consistently improve alignment. Our work underlines the significant
impact that Survey Response Generation Methods have on simulated survey
responses, and we develop practical recommendations on the application of
Survey Response Generation Methods.

</details>


### [442] [Scaling Language-Centric Omnimodal Representation Learning](https://arxiv.org/abs/2510.11693)
*Chenghao Xiao,Hou Pong Chan,Hao Zhang,Weiwen Xu,Mahani Aljunied,Yu Rong*

Main category: cs.CL

TL;DR: 本研究认为，MLLM多模态嵌入的优越性源于生成式预训练中实现的隐式跨模态对齐，对比学习（CL）在此基础上进行轻量级优化。提出LCO-Emb框架并发现生成-表示缩放定律（GRSL），表明生成能力越强，表示能力越好。


<details>
  <summary>Details</summary>
Motivation: 尽管基于多模态大语言模型（MLLM）并结合对比学习（CL）的多模态嵌入方法取得了显著成果，但其优越性的深层原因尚未被充分探索。

Method: 本研究通过分析各向异性（anisotropy）和核相似度结构，实证确认MLLM表示中存在潜在的跨模态对齐。在此洞察基础上，提出了以语言为中心的通用多模态嵌入框架LCO-Emb。同时，识别并提供了生成-表示缩放定律（GRSL）的理论解释，并通过低资源视觉-文档检索任务进行验证。

Result: MLLM表示中确实存在潜在的对齐，使得对比学习成为有效的精炼阶段。LCO-Emb框架在多个基准测试中实现了最先进的性能。研究发现生成-表示缩放定律（GRSL），表明通过对比精炼获得的表示能力与MLLM的生成能力呈正相关。持续的生成式预训练可以在对比学习之前进一步增强模型的嵌入潜力。

Conclusion: MLLM在生成式预训练中实现了隐式的跨模态对齐，为后续的对比学习提供了基础。提高生成能力是增强表示质量的有效范式，这由生成-表示缩放定律（GRSL）理论上得到了支持和验证。

Abstract: Recent multimodal embedding approaches leveraging multimodal large language
models (MLLMs) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of MLLM-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language decoder learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within MLLM representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the MLLM's generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the MLLM's generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.

</details>


### [443] [MeTA-LoRA: Data-Efficient Multi-Task Fine-Tuning for Large Language Models](https://arxiv.org/abs/2510.11598)
*Bo Cheng,Xu Wang,Jinda Liu,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: MeTA-LoRA 是一种两阶段优化框架，显著提高了大型语言模型在多任务场景下进行低秩适应（LoRA）的数据效率，通过促进任务间知识迁移，仅用少量数据即可达到或超越传统LoRA的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LoRA在单任务微调中表现出色，但在复杂的多任务学习场景中，它难以有效利用任务间的知识，通常需要大量的任务特定数据才能达到最佳性能。

Method: MeTA-LoRA 采用两阶段优化。第一阶段，使用每个数据集的少量样本学习任务特定的LoRA适配器。第二阶段，通过聚合来自多个任务的梯度来更新共享的LoRA适配器，以促进任务间知识转移。

Result: 在多任务学习和多语言学习场景中，MeTA-LoRA 在使用显著更少的任务特定数据的情况下，其性能与传统的全数据LoRA微调方法相当或超越。

Conclusion: MeTA-LoRA 有效解决了LoRA在多任务适应中数据效率低下的问题，通过两阶段优化框架促进了知识迁移，从而显著减少了所需数据量并保持了高性能。

Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most widely used
parameter-efficient fine-tuning (PEFT) methods for adapting large language
models (LLMs) to downstream tasks. While highly effective in single-task
settings, it struggles to efficiently leverage inter-task knowledge in complex
multi-task learning scenarios, often requiring substantial task-specific data
to achieve optimal performance. To address this limitation, we introduce
MeTA-LoRA, a two-stage optimization framework that significantly improves data
efficiency in multi-task adaptation. In the first stage, task-specific LoRA
adapters are learned using only a few samples from each involved dataset,
enabling rapid adaptation without large-scale supervision. In the second stage,
the shared LoRA adapter is updated by aggregating gradients from multiple tasks
to promote knowledge transfer across tasks, further reducing data usage by
leveraging common patterns. In both multi-task learning and multilingual
learning scenarios, our method matches or surpasses the performance of
traditional full-data LoRA fine-tuning approaches, while using significantly
less task-specific data.

</details>


### [444] [StoryBox: Collaborative Multi-Agent Simulation for Hybrid Bottom-Up Long-Form Story Generation Using Large Language Models](https://arxiv.org/abs/2510.11618)
*Zehao Chen,Rong Pan,Haoran Li*

Main category: cs.CL

TL;DR: 本文提出一种混合式自下而上的长篇故事生成方法，通过多智能体模拟在动态沙盒环境中互动，生成自发事件，从而构建连贯一致的万字长篇故事，并取得最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 受人类作家构思场景和角色互动的启发，旨在解决当前故事生成模型在长篇故事连贯性、一致性及自然展开方面的挑战。

Method: 采用多智能体模拟方法，智能体在动态沙盒环境中互动，其行为和交互会产生突发事件。这些事件构成了故事的基础，促成了有机的人物发展和情节推进。这是一种混合式自下而上的方法，与传统的自上而下方法不同。

Result: 该系统能够生成超过10,000字的故事，同时保持连贯性和一致性，解决了现有故事生成模型面临的一些关键挑战。在多项指标上取得了最先进的性能。

Conclusion: 该方法为创建动态、沉浸式、由智能体互动有机演变的长篇故事提供了一种可扩展且创新的解决方案。

Abstract: Human writers often begin their stories with an overarching mental scene,
where they envision the interactions between characters and their environment.
Inspired by this creative process, we propose a novel approach to long-form
story generation, termed hybrid bottom-up long-form story generation, using
multi-agent simulations. In our method, agents interact within a dynamic
sandbox environment, where their behaviors and interactions with one another
and the environment generate emergent events. These events form the foundation
for the story, enabling organic character development and plot progression.
Unlike traditional top-down approaches that impose rigid structures, our hybrid
bottom-up approach allows for the natural unfolding of events, fostering more
spontaneous and engaging storytelling. The system is capable of generating
stories exceeding 10,000 words while maintaining coherence and consistency,
addressing some of the key challenges faced by current story generation models.
We achieve state-of-the-art performance across several metrics. This approach
offers a scalable and innovative solution for creating dynamic, immersive
long-form stories that evolve organically from agent-driven interactions.

</details>


### [445] [Deconstructing Attention: Investigating Design Principles for Effective Language Modeling](https://arxiv.org/abs/2510.11602)
*Huiyin Xue,Nafise Sadat Moosavi,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本研究系统解构了Transformer注意力机制的设计原则，发现令牌混合是不可或缺的，而其数学形式和序列依赖性可以被大大放宽，尤其是在混合架构中。同时，即使是单独表现不佳的变体，与标准注意力结合也能达到良好性能。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型的成功广泛归因于其点积注意力机制，该机制融合了多个设计原则（如跨位置信息混合、序列依赖激活、特定数学形式、查询与键耦合）。然而，这些原则中每一个的必要性在很大程度上尚未经过检验。

Method: 通过设计受控变体，选择性地放宽注意力机制的各项设计原则，并将其应用于所有层或仅部分层保留标准注意力的混合架构中，进行系统性的实证分析。

Result: 令牌混合机制是不可或缺的，其缺失会导致模型性能接近随机；而注意力机制的精确数学形式和序列依赖性可以被大幅放宽，尤其是在仅部分层保留这些特性时。令人惊讶的是，即使是单独失败的变体，在与标准注意力交错使用时也能实现稳健的性能，这突显了一种协同效应。

Conclusion: 这些发现加深了对注意力有效性真正基础的理解，并为在不牺牲性能的情况下简化语言模型开辟了新途径。

Abstract: The success of Transformer language models is widely credited to their
dot-product attention mechanism, which interweaves a set of key design
principles: mixing information across positions (enabling multi-token
interactions), sequence-dependent activations (where attention weights adapt to
each input), a specific mathematical form (dot-product similarities plus
softmax weighting), and coupling of queries and keys to evolving hidden states
(grounding attention in the current layer). However, the necessity of each of
these principles remains largely untested. In this work, we systematically
deconstruct attention by designing controlled variants that selectively relax
these principles, applied both uniformly across all layers and in hybrid
architectures where only some layers retain standard attention. Our empirical
analysis reveals that mechanisms for mixing tokens are indispensable, as their
absence collapses models to near-random behavior, while the exact mathematical
form and sequence dependency can be substantially relaxed, especially when
preserved in just a subset of layers. Surprisingly, even variants that fail in
isolation can achieve robust performance when interleaved with standard
attention, highlighting a cooperative effect. These findings deepen our
understanding of what truly underpins attention's effectiveness and open new
avenues for simplifying language models without sacrificing performance.

</details>


### [446] [ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems](https://arxiv.org/abs/2510.11652)
*Xin Gui,King Zhu,JinCheng Ren,Qianben Chen,Zekun Moore Wang,Yizhi LI,Xinpeng Liu,Xiaowan Li,Wenli Ren,Linyu Miao,Tianrui Qin,Ziqi Shu,He Zhu,Xiangru Tang,Dingfeng Shi,Jiaheng Liu,Yuchen Eleanor Jiang,Minghao Liu,Ge Zhang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 本文提出了Acadreason基准测试，旨在评估大型语言模型（LLMs）和智能体在学术知识获取和高水平推理方面的能力，并揭示了现有模型在此类任务上的显著不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要集中在数学/代码竞赛或通用任务，而多领域学术基准缺乏足够的推理深度，导致该领域缺乏一个针对高水平推理的严格基准。为了填补这一空白，作者创建了Acadreason。

Method: 引入了Acadreason基准测试，包含50个由专家标注的学术问题，涵盖计算机科学、经济学、法律、数学和哲学五个高推理领域。所有问题均来源于近年顶级出版物，并经过严格的标注和质量控制。研究对10多个主流LLM和智能体进行了系统评估。

Result: 评估结果显示，大多数LLM得分低于20分，即使是尖端的GPT-5也仅得16分。智能体得分较高，但均未超过40分。

Conclusion: 结果表明，LLM和智能体在超智能学术研究任务中存在显著的能力差距，并凸显了Acadreason基准测试的挑战性，以及当前模型在复杂学术推理方面的局限性。

Abstract: In recent years, the research focus of large language models (LLMs) and
agents has shifted increasingly from demonstrating novel capabilities to
complex reasoning and tackling challenging tasks. However, existing evaluations
focus mainly on math/code contests or general tasks, while existing
multi-domain academic benchmarks lack sufficient reasoning depth, leaving the
field without a rigorous benchmark for high-level reasoning. To fill this gap,
we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs
and agents to acquire and reason over academic knowledge. It consists of 50
expert-annotated academic problems across five high-reasoning domains,
including computer science, economics, law, mathematics, and philosophy. All
questions are sourced from top-tier publications in recent years and undergo
rigorous annotation and quality control to ensure they are both challenging and
answerable. We conduct systematic evaluations of over 10 mainstream LLMs and
agents. The results show that most LLMs scored below 20 points, with even the
cutting-edge GPT-5 achieving only 16 points. While agents achieved higher
scores, none exceeded 40 points. This demonstrates the current capability gap
between LLMs and agents in super-intelligent academic research tasks and
highlights the challenges of Acadreason.

</details>


### [447] [Enhancing Long Chain-of-Thought Reasoning through Multi-Path Plan Aggregation](https://arxiv.org/abs/2510.11620)
*Siheng Xiong,Ali Payani,Faramarz Fekri*

Main category: cs.CL

TL;DR: 针对大型语言模型（LM）在推理时链式思考（CoT）中存在的推理脱轨问题，特别是对于小型LM的长CoT，本文提出了多路径规划聚合（MPPA）框架，通过规划探索和聚合来改进推理。此外，还引入了在线步进DPO（Step-DPO）进行过程级偏好优化，以实现高效且稳定的训练。实验证明，该方法在多个推理基准上优于现有基线，且数据需求极低。


<details>
  <summary>Details</summary>
Motivation: 现有LM的推理时CoT方法通常一次性生成整个推理链，导致推理脱轨（CoT derailment），即推理轨迹因累积误差而偏离。这个问题在处理长CoT的小型LM上尤为严重，因为它们能力有限。研究发现，大多数推理错误源于不正确的规划。

Method: 1. **多路径规划聚合（MPPA）**：通过规划探索和聚合来增强单次推理。它根据token位置以可变间隔生成多个候选规划，并将其聚合成一个精炼的规划步骤。设计上保持精简，以基础LM作为主要策略，轻量级LoRA模块实现规划聚合策略。
2. **在线步进DPO（Online Step-DPO）**：为解决结果奖励RL在长轨迹（如超过4K token）上的低效率问题，引入了一种过程级偏好优化方案，利用Twisted Sequential Monte Carlo (TSMC) 提供可扩展的逐步监督，以实现更高效的训练、更高的稳定性和准确性。

Result: 在具有挑战性的数学、科学和逻辑推理基准测试中，该方法仅使用10%的SFT数据和5%的偏好对，便在多个基础模型和任务上优于DeepSeek-R1蒸馏基线和结果奖励RL基线。它带来了更高效的训练、更高的稳定性和准确性。

Conclusion: 通过结合多路径规划聚合（MPPA）和在线步进DPO（Online Step-DPO），本研究有效解决了大型语言模型在长推理链中遇到的推理脱轨和训练效率低下问题。该方法显著提升了LM的推理能力，尤其适用于小型LM，且对数据量的要求极低。

Abstract: Inference-time scaling enhances the reasoning ability of a language model
(LM) by extending its chain-of-thought (CoT). However, existing approaches
typically generate the entire reasoning chain in a single forward pass, which
often leads to CoT derailment, i.e., the reasoning trajectory drifting off
course due to compounding errors. This problem is particularly severe for
smaller LMs with long CoTs due to their limited capacity. To address this, we
analyze raw long CoTs and uncover a reasoning hierarchy consisting of planning
and execution steps. Our analysis reveals that most reasoning errors stem from
incorrect planning. Motivated by this observation, we propose Multi-Path Plan
Aggregation (MPPA), a framework that augments single-pass reasoning with plan
exploration and aggregation. Following a variable interval schedule based on
the token position, MPPA generates multiple candidate plans and aggregates them
into a refined planning step. To maintain efficiency, we adopt a minimal design
in which the base LM serves as the primary policy, while a lightweight LoRA
module implements the plan aggregation policy. We further observe that
outcome-reward RL is inefficient for long trajectories (e.g., exceeding 4K
tokens). To overcome this, we introduce online Step-DPO, a process-level
preference optimization scheme that leverages Twisted Sequential Monte Carlo
(TSMC) to provide scalable stepwise supervision using small LMs. This yields
more efficient training, improved stability, and higher accuracy. Extensive
experiments on challenging math, science, and logical reasoning benchmarks
demonstrate that, with only 10% SFT data and 5% of preference pairs, our method
outperforms both the DeepSeek-R1 distillation baseline and the outcome-reward
RL baseline across multiple base models and tasks.

</details>


### [448] [When Agents Trade: Live Multi-Market Trading Benchmark for LLM Agents](https://arxiv.org/abs/2510.11695)
*Lingfei Qian,Xueqing Peng,Yan Wang,Vincent Jim Zhang,Huan He,Hanley Smith,Yi Han,Yueru He,Haohang Li,Yupeng Cao,Yangyang Yu,Alejandro Lopez-Lira,Peng Lu,Jian-Yun Nie,Guojun Xiong,Jimin Huang,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 该研究引入了Agent Market Arena (AMA)，这是一个实时的、终身基准测试平台，用于评估基于大型语言模型（LLM）的交易代理在多个金融市场中的表现，发现代理框架而非模型骨干对行为模式影响更大。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估LLM驱动的金融交易代理时，存在以下不足：多数研究测试模型而非代理、覆盖时间短、资产种类有限且依赖未经核实的数据，因此需要一个更全面、实时和严谨的评估框架。

Method: 引入了Agent Market Arena (AMA)平台，它整合了经核实的数据、专家验证的新闻和多样化的代理架构，以统一框架进行公平持续的实时比较。AMA实现了四种代理（InvestorAgent、TradeAgent、HedgeFundAgent、DeepFundAgent）并使用多种LLM骨干（如GPT-4o、Claude-3.5-haiku等）在加密货币和股票市场进行实时实验。

Result: 实时实验表明，不同的代理框架展现出截然不同的行为模式，从激进的风险承担到保守的决策制定，而LLM模型骨干对结果变化的影响较小。

Conclusion: AMA为基于LLM的金融推理和交易智能代理的严谨、可复现和持续演进的评估奠定了基础。

Abstract: Although Large Language Model (LLM)-based agents are increasingly used in
financial trading, it remains unclear whether they can reason and adapt in live
markets, as most studies test models instead of agents, cover limited periods
and assets, and rely on unverified data. To address these gaps, we introduce
Agent Market Arena (AMA), the first lifelong, real-time benchmark for
evaluating LLM-based trading agents across multiple markets. AMA integrates
verified trading data, expert-checked news, and diverse agent architectures
within a unified trading framework, enabling fair and continuous comparison
under real conditions. It implements four agents, including InvestorAgent as a
single-agent baseline, TradeAgent and HedgeFundAgent with different risk
styles, and DeepFundAgent with memory-based reasoning, and evaluates them
across GPT-4o, GPT-4.1, Claude-3.5-haiku, Claude-sonnet-4, and
Gemini-2.0-flash. Live experiments on both cryptocurrency and stock markets
demonstrate that agent frameworks display markedly distinct behavioral
patterns, spanning from aggressive risk-taking to conservative decision-making,
whereas model backbones contribute less to outcome variation. AMA thus
establishes a foundation for rigorous, reproducible, and continuously evolving
evaluation of financial reasoning and trading intelligence in LLM-based agents.

</details>


### [449] [Demystifying Reinforcement Learning in Agentic Reasoning](https://arxiv.org/abs/2510.11701)
*Zhaochen Yu,Ling Yang,Jiaru Zou,Shuicheng Yan,Mengdi Wang*

Main category: cs.CL

TL;DR: 本文系统研究了代理式强化学习（Agentic RL）在大型语言模型（LLM）代理式推理中的应用，从数据、算法和推理模式三个角度提出了关键设计原则和实践方法，显著提升了推理能力和训练效率，并使小型模型超越了大型模型。


<details>
  <summary>Details</summary>
Motivation: 代理式强化学习（Agentic RL）已被证明能有效提升大型语言模型（LLM）的代理式推理能力，但其关键设计原则和最佳实践仍不明确。

Method: 研究人员从数据、算法和推理模式三个关键视角，对代理式推理中的强化学习进行了全面系统的调查。具体方法包括：使用真实的端到端工具使用轨迹进行SFT初始化，构建高多样性、模型感知的数据集以支持探索；采用探索友好的技术（如更高的裁剪、过长奖励整形、维持足够的策略熵）；以及比较不同的推理模式（如深思熟虑的策略与频繁工具调用或冗长的自我推理）。此外，还贡献了高质量的SFT和RL数据集。

Result: 研究发现：(i) 用真实的端到端工具使用轨迹替代拼接的合成轨迹能产生更强的SFT初始化；高多样性、模型感知的数据集能维持探索并显著提升RL性能。(ii) 探索友好的技术（如更高的裁剪、过长奖励整形和维持足够的策略熵）对代理式RL至关重要，能提高训练效率。(iii) 较少工具调用的深思熟虑策略优于频繁工具调用或冗长的自我推理，能提高工具效率和最终准确性。这些实践一致地增强了代理式推理和训练效率，在挑战性基准测试中用更小的模型取得了优异结果。使用这些方法，4B大小的模型也能在AIME2024/AIME2025、GPQA-Diamond和LiveCodeBench-v6等四个基准测试中实现超越32B大小模型的代理式推理性能。

Conclusion: 简单的实践方法能够持续提升大型语言模型的代理式推理能力和训练效率，为未来的代理式强化学习研究奠定了实用的基线。这些方法使得小型模型也能实现卓越的代理式推理性能。

Abstract: Recently, the emergence of agentic RL has showcased that RL could also
effectively improve the agentic reasoning ability of LLMs, yet the key design
principles and optimal practices remain unclear. In this work, we conduct a
comprehensive and systematic investigation to demystify reinforcement learning
in agentic reasoning from three key perspectives: data, algorithm, and
reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic
trajectories with real end-to-end tool-use trajectories yields a far stronger
SFT initialization; high-diversity, model-aware datasets sustain exploration
and markedly improve RL performance. (ii) Exploration-friendly techniques are
crucial for agentic RL, such as clip higher, overlong reward shaping, and
maintaining adequate policy entropy could improve the training efficiency.
(iii) A deliberative strategy with fewer tool calls outperforms frequent tool
calls or verbose self-reasoning, improving tool efficiency and final accuracy.
Together, these simple practices consistently enhance agentic reasoning and
training efficiency, achieving strong results on challenging benchmarks with
smaller models, and establishing a practical baseline for future agentic RL
research. Beyond these empirical insights, we further contribute a
high-quality, real end-to-end agentic SFT dataset along with a high-quality RL
dataset, and demonstrate the effectiveness of our insights in boosting the
agentic reasoning ability of LLMs across four challenging benchmarks, including
AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes,
4B-sized models could also achieve superior agentic reasoning performance
compared to 32B-sized models. Code and models:
https://github.com/Gen-Verse/Open-AgentRL

</details>


### [450] [Are Large Reasoning Models Interruptible?](https://arxiv.org/abs/2510.11713)
*Tsung-Han Wu,Mihran Miroyan,David M. Chan,Trevor Darrell,Narges Norouzi,Joseph E. Gonzalez*

Main category: cs.CL

TL;DR: 传统上，大型推理模型（LRMs）在静态环境中进行评估，但本文挑战了这一“冻结世界”假设，并在中断和动态上下文等现实动态场景下评估了LRM的鲁棒性，发现其性能显著下降并出现了新的故障模式。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理方面表现出色，但其传统评估设定在静态的“冻结世界”中，即模型响应被认为是瞬时的，请求上下文在响应期间保持不变。然而，在辅助编程等现代推理任务中，模型可能需要数小时思考，代码也可能在模型开始思考到最终输出期间发生巨大变化，这使得“冻结世界”假设失效。

Method: 本文通过两种现实动态场景来评估LRM的鲁棒性：1) 中断：在有限预算下测试模型部分输出的质量；2) 动态上下文：测试模型对运行时变化的适应能力。评估使用了需要长篇推理的数学和编程基准测试。

Result: 静态评估持续高估了LRM的鲁棒性。即使在静态设置中达到高精度的最先进LRM，在中断或暴露于变化的上下文时，也可能出现不可预测的失败，当更新在推理过程后期引入时，性能下降高达60%。分析还揭示了几种新的故障模式，包括推理泄漏（模型在中断时将推理折叠到最终答案中）、恐慌（在时间压力下模型完全放弃推理并返回不正确答案）和自我怀疑（在整合更新信息时性能下降）。

Conclusion: “冻结世界”假设对于需要长时间推理的任务来说是不足的。LRM在动态环境中缺乏鲁棒性，即使是最先进的模型也可能在中断或上下文变化时出现显著的性能下降和新的故障模式。这表明需要更现实的评估方法，并开发能够适应动态条件的新一代LRM。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning but are
traditionally evaluated in static, "frozen world" settings: model responses are
assumed to be instantaneous, and the context of a request is presumed to be
immutable over the duration of the response. While generally true for
short-term tasks, the "frozen world" assumption breaks down in modern reasoning
tasks such as assistive programming, where models may take hours to think
through problems and code may change dramatically from the time the model
starts thinking to the model's final output. In this work, we challenge the
frozen world assumption and evaluate LRM robustness under two realistic dynamic
scenarios: interruptions, which test the quality of the model's partial outputs
on a limited budget, and dynamic context, which tests model adaptation to
in-flight changes. Across mathematics and programming benchmarks that require
long-form reasoning, static evaluations consistently overestimate robustness:
even state-of-the-art LRMs, which achieve high accuracy in static settings, can
fail unpredictably when interrupted or exposed to changing context, with
performance dropping by up to 60% when updates are introduced late in the
reasoning process. Our analysis further reveals several novel failure modes,
including reasoning leakage, where models fold the reasoning into their final
answer when interrupted; panic, where under time pressure models abandon
reasoning entirely and return incorrect answers; and self-doubt, where
performance degrades while incorporating updated information.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [451] [Enhancing Diffusion Policy with Classifier-Free Guidance for Temporal Robotic Tasks](https://arxiv.org/abs/2510.09786)
*Yuang Lu,Song Wang,Xiao Han,Xuri Zhang,Yucong Wu,Zhicheng He*

Main category: cs.RO

TL;DR: 本文提出了一种基于无分类器引导的扩散策略 (CFG-DP)，通过将无分类器引导 (CFG) 集成到扩散策略 (DP) 中，解决了人形机器人时间序列任务中现有方法缺乏时间上下文、导致局部最优和重复动作过多的问题，显著提高了任务成功率和执行可靠性。


<details>
  <summary>Details</summary>
Motivation: 人形机器人执行时间序列任务时，现有扩散策略 (DP) 和基于Transformer的动作分块 (ACT) 方法通常缺乏时间上下文，导致陷入局部最优和产生过多重复动作。

Method: 引入了基于无分类器引导的扩散策略 (CFG-DP) 框架，通过结合条件和无条件模型来增强 DP。CFG 利用时间步输入跟踪任务进度并确保精确的周期终止，根据任务阶段动态调整动作预测，并使用引导因子平衡时间连贯性和动作准确性。

Result: 在人形机器人上的真实世界实验表明，CFG-DP 实现了高成功率和极少的重复动作。此外，研究评估了模型终止动作的能力，并分析了不同组件和参数调整对其性能的影响。

Conclusion: 该框架显著增强了机器人序列任务的确定性控制和执行可靠性。

Abstract: Temporal sequential tasks challenge humanoid robots, as existing Diffusion
Policy (DP) and Action Chunking with Transformers (ACT) methods often lack
temporal context, resulting in local optima traps and excessive repetitive
actions. To address these issues, this paper introduces a Classifier-Free
Guidance-Based Diffusion Policy (CFG-DP), a novel framework to enhance DP by
integrating Classifier-Free Guidance (CFG) with conditional and unconditional
models. Specifically, CFG leverages timestep inputs to track task progression
and ensure precise cycle termination. It dynamically adjusts action predictions
based on task phase, using a guidance factor tuned to balance temporal
coherence and action accuracy. Real-world experiments on a humanoid robot
demonstrate high success rates and minimal repetitive actions. Furthermore, we
assessed the model's ability to terminate actions and examined how different
components and parameter adjustments affect its performance. This framework
significantly enhances deterministic control and execution reliability for
sequential robotic tasks.

</details>


### [452] [Cross-Sensor Touch Generation](https://arxiv.org/abs/2510.09817)
*Samanta Rodriguez,Yiming Dou,Miquel Oller,Andrew Owens,Nima Fazeli*

Main category: cs.RO

TL;DR: 该研究提出了两种跨传感器触觉图像生成方法（Touch2Touch和T2D2），旨在解决触觉表示的通用性挑战，并实现传感器间模型的迁移。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-触觉传感器种类繁多，但大多数模型都与特定的传感器设计绑定，这使得开发通用目的的触觉表示变得困难。

Method: 提出了两种跨传感器图像生成方法：1. Touch2Touch：一种端到端的方法，利用配对数据进行生成。2. T2D2 (Touch-to-Depth-to-Touch)：通过构建中间深度表示，且不要求配对数据进行生成。

Result: 这两种方法都允许将特定传感器的模型应用于其他传感器，并通过跨传感器触觉生成过程实现。在手内姿态估计和行为克隆等下游任务中，成功地将在一个传感器上训练的模型迁移到另一个传感器上，证明了其有效性。

Conclusion: 这些模型为传感器转换提供了灵活的解决方案，可根据数据可用性和应用需求进行选择，从而增强了触觉表示的通用性和模型的可迁移性。

Abstract: Today's visuo-tactile sensors come in many shapes and sizes, making it
challenging to develop general-purpose tactile representations. This is because
most models are tied to a specific sensor design. To address this challenge, we
propose two approaches to cross-sensor image generation. The first is an
end-to-end method that leverages paired data (Touch2Touch). The second method
builds an intermediate depth representation and does not require paired data
(T2D2: Touch-to-Depth-to-Touch). Both methods enable the use of sensor-specific
models across multiple sensors via the cross-sensor touch generation process.
Together, these models offer flexible solutions for sensor translation,
depending on data availability and application needs. We demonstrate their
effectiveness on downstream tasks such as in-hand pose estimation and behavior
cloning, successfully transferring models trained on one sensor to another.
Project page: https://samantabelen.github.io/cross_sensor_touch_generation.

</details>


### [453] [VG-Mapping: Variation-Aware 3D Gaussians for Online Semi-static Scene Mapping](https://arxiv.org/abs/2510.09962)
*Yicheng He,Jingwen Yu,Guangcheng Chen,Hong Zhang*

Main category: cs.RO

TL;DR: 本文提出了VG-Mapping，一种基于3D高斯泼溅（3DGS）的在线建图系统，专为半静态场景设计。它通过混合表示（3DGS与TSDF体素图结合）和变异感知密度控制策略，有效识别并更新场景中的变化区域，并发布了一个新的RGB-D数据集。


<details>
  <summary>Details</summary>
Motivation: 对于机器人而言，维护一个准确反映环境变化的最新地图至关重要。未能及时更新变化区域会导致地图质量下降、定位不佳、操作效率低下甚至机器人丢失。尽管3DGS在在线地图重建中广泛应用，但准确高效地更新变化区域仍是一个挑战。

Method: 本文提出VG-Mapping系统，采用以下方法：1. 引入一种混合表示，将3DGS与基于TSDF的体素图结合，以高效识别场景中的变化区域。2. 设计了一种变异感知密度控制策略，用于在变化区域插入或删除高斯基元。3. 构建了一个包含合成和真实世界半静态环境的RGB-D数据集，以解决该任务缺乏公共基准的问题。

Result: 实验结果表明，所提出的方法显著提高了半静态场景中的渲染质量和地图更新效率。

Conclusion: VG-Mapping通过其新颖的混合表示和密度控制策略，成功解决了半静态场景中地图更新的挑战，并为该领域提供了急需的基准数据集，显著提升了渲染质量和更新效率。

Abstract: Maintaining an up-to-date map that accurately reflects recent changes in the
environment is crucial, especially for robots that repeatedly traverse the same
space. Failing to promptly update the changed regions can degrade map quality,
resulting in poor localization, inefficient operations, and even lost robots.
3D Gaussian Splatting (3DGS) has recently seen widespread adoption in online
map reconstruction due to its dense, differentiable, and photorealistic
properties, yet accurately and efficiently updating the regions of change
remains a challenge. In this paper, we propose VG-Mapping, a novel online
3DGS-based mapping system tailored for such semi-static scenes. Our approach
introduces a hybrid representation that augments 3DGS with a TSDF-based voxel
map to efficiently identify changed regions in a scene, along with a
variation-aware density control strategy that inserts or deletes Gaussian
primitives in regions undergoing change. Furthermore, to address the absence of
public benchmarks for this task, we construct a RGB-D dataset comprising both
synthetic and real-world semi-static environments. Experimental results
demonstrate that our method substantially improves the rendering quality and
map update efficiency in semi-static scenes. The code and dataset are available
at https://github.com/heyicheng-never/VG-Mapping.

</details>


### [454] [LLM-HBT: Dynamic Behavior Tree Construction for Adaptive Coordination in Heterogeneous Robots](https://arxiv.org/abs/2510.09963)
*Chaoran Wang,Jingyuan Sun,Yanhui Zhang,Mingyu Zhang,Changju Wu*

Main category: cs.RO

TL;DR: 该论文提出了一种利用大语言模型（LLM）为异构多机器人系统自动构建行为树（BT）的新框架，以提高其在动态环境中的适应性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统机器人受限于固定的功能属性，难以在任务失败或环境变化时有效重新配置策略，缺乏应对动态环境的适应性和鲁棒性。

Method: 该框架包含任务初始化、任务分配、BT更新和故障节点检测四个模块，形成闭环操作。它利用LLM动态生成和扩展BT，结合LLM的推理泛化能力与BT的模块化恢复能力。机器人执行BT，遇到故障节点时可本地扩展BT或调用中央虚拟协调器进行子任务重分配和BT同步。

Result: 在60项任务（包括三个模拟场景和一个真实咖啡馆环境）中，该方法在任务成功率、鲁棒性和可扩展性方面始终优于基线方法，有效支持异构团队的长期协作执行。

Conclusion: 该框架被证明在复杂场景下的多机器人协作中是有效的，能显著提升系统的适应性和鲁棒性，实现异构团队的长期合作执行。

Abstract: We introduce a novel framework for automatic behavior tree (BT) construction
in heterogeneous multi-robot systems, designed to address the challenges of
adaptability and robustness in dynamic environments. Traditional robots are
limited by fixed functional attributes and cannot efficiently reconfigure their
strategies in response to task failures or environmental changes. To overcome
this limitation, we leverage large language models (LLMs) to generate and
extend BTs dynamically, combining the reasoning and generalization power of
LLMs with the modularity and recovery capability of BTs. The proposed framework
consists of four interconnected modules task initialization, task assignment,
BT update, and failure node detection which operate in a closed loop. Robots
tick their BTs during execution, and upon encountering a failure node, they can
either extend the tree locally or invoke a centralized virtual coordinator
(Alex) to reassign subtasks and synchronize BTs across peers. This design
enables long-term cooperative execution in heterogeneous teams. We validate the
framework on 60 tasks across three simulated scenarios and in a real-world cafe
environment with a robotic arm and a wheeled-legged robot. Results show that
our method consistently outperforms baseline approaches in task success rate,
robustness, and scalability, demonstrating its effectiveness for multi-robot
collaboration in complex scenarios.

</details>


### [455] [FORM: Fixed-Lag Odometry with Reparative Mapping utilizing Rotating LiDAR Sensors](https://arxiv.org/abs/2510.09966)
*Easton R. Potokar,Taylor Pool,Daniel McGann,Michael Kaess*

Main category: cs.RO

TL;DR: 本文提出了一种名为FORM的激光雷达里程计（LO）方法，它通过在密集连接的因子图上进行固定滞后平滑处理，并利用单一迭代地图进行匹配，实现了实时、精确、鲁棒且轨迹平滑的定位。


<details>
  <summary>Details</summary>
Motivation: 现有的激光雷达里程计方法存在不足：基于平滑的方法通常需要匹配多个扫描，导致无法实时运行；而大多数先前的工作是基于“子地图”的，一次估计一个状态，这会导致姿态估计误差传播到固定子地图，产生抖动轨迹并降低未来的配准质量。

Method: 本文提出了一种名为Fixed-Lag Odometry with Reparative Mapping (FORM) 的LO方法。该方法在密集连接的因子图上执行平滑处理，同时利用单一迭代地图进行匹配。这种设计既能实现实时性能，又能随着姿态估计的进一步细化主动修正局部地图。

Result: FORM方法在各种数据集上进行了评估，结果表明它具有鲁棒性、准确性、实时性，并且与现有最先进的LO方法相比，能提供更平滑的轨迹估计。

Conclusion: FORM是一种有效的激光雷达里程计方法，它解决了现有方法的实时性和轨迹平滑度问题，在准确性和鲁棒性方面也表现出色，优于现有的先进方法。

Abstract: Light Detection and Ranging (LiDAR) sensors have become a de-facto sensor for
many robot state estimation tasks, spurring development of many LiDAR Odometry
(LO) methods in recent years. While some smoothing-based LO methods have been
proposed, most require matching against multiple scans, resulting in
sub-real-time performance. Due to this, most prior works estimate a single
state at a time and are ``submap''-based. This architecture propagates any
error in pose estimation to the fixed submap and can cause jittery trajectories
and degrade future registrations. We propose Fixed-Lag Odometry with Reparative
Mapping (FORM), a LO method that performs smoothing over a densely connected
factor graph while utilizing a single iterative map for matching. This allows
for both real-time performance and active correction of the local map as pose
estimates are further refined. We evaluate on a wide variety of datasets to
show that FORM is robust, accurate, real-time, and provides smooth trajectory
estimates when compared to prior state-of-the-art LO methods.

</details>


### [456] [ATRos: Learning Energy-Efficient Agile Locomotion for Wheeled-legged Robots](https://arxiv.org/abs/2510.09980)
*Jingyuan Sun,Hongyu Ji,Zihan Qu,Chaoran Wang,Mingyu Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种名为ATRos的基于强化学习的混合运动框架，旨在实现轮腿机器人的混合行走-驱动运动，无需预设步态，提高地形适应性和能源效率。


<details>
  <summary>Details</summary>
Motivation: 轮腿机器人的混合运动结合了腿式运动的灵活性和轮式运动的效率，但其全身控制在混合运动中仍然是一个挑战。

Method: ATRos框架利用强化学习技术，构建了一个预测策略网络，通过本体感觉信息估计外部环境状态，然后将输出输入到一个Actor-Critic网络以生成最优关节指令。该方法不依赖预定义的步态模式，旨在智能协调轮子和腿部的同步运动。

Result: 所提出的框架在仿真和实际实验中得到了验证，包括平地、楼梯和草地等多种地形。该混合运动框架在各种未见过的地形上表现出鲁棒的性能，突出了其泛化能力。

Conclusion: ATRos提供了一个有效的强化学习框架，能够使轮腿机器人在多种地形上实现鲁棒且自适应的混合行走-驱动运动，显著提升了机器人的地形适应性和能源效率。

Abstract: Hybrid locomotion of wheeled-legged robots has recently attracted increasing
attention due to their advantages of combining the agility of legged locomotion
and the efficiency of wheeled motion. But along with expanded performance, the
whole-body control of wheeled-legged robots remains challenging for hybrid
locomotion. In this paper, we present ATRos, a reinforcement learning
(RL)-based hybrid locomotion framework to achieve hybrid walking-driving
motions on the wheeled-legged robot. Without giving predefined gait patterns,
our planner aims to intelligently coordinate simultaneous wheel and leg
movements, thereby achieving improved terrain adaptability and improved energy
efficiency. Based on RL techniques, our approach constructs a prediction policy
network that could estimate external environmental states from proprioceptive
sensory information, and the outputs are then fed into an actor critic network
to produce optimal joint commands. The feasibility of the proposed framework is
validated through both simulations and real-world experiments across diverse
terrains, including flat ground, stairs, and grassy surfaces. The hybrid
locomotion framework shows robust performance over various unseen terrains,
highlighting its generalization capability.

</details>


### [457] [Hybrid Robotic Meta-gripper for Tomato Harvesting: Analysis of Auxetic Structures with Lattice Orientation Variations](https://arxiv.org/abs/2510.10016)
*Shahid Ansari,Vivek Gupta,Bishakh Bhattacharya*

Main category: cs.RO

TL;DR: 本研究提出了一种用于番茄采摘的新型混合夹持器，结合了刚性外框和软性拉胀内部点阵，并系统地研究了拉胀点阵方向对抓取性能的影响。


<details>
  <summary>Details</summary>
Motivation: 农业部门面临日益增长的全球粮食需求，但水果和蔬菜的处理任务仍然劳动密集，导致效率低下和采后损失。自动化，特别是选择性采摘，提供了一个可行的解决方案，其中软体机器人技术是关键。

Method: 引入了一种新型六指混合夹持器，其具有刚性外框和软性拉胀内部点阵，采用3D笼式效应设计。通过结合实验验证、2D数字图像相关（DIC）和非线性有限元分析（FEA），研究了拉胀点阵方向对抓取柔顺性的影响。评估了0度、30度、45度和60度晶胞倾角的拉胀配置，并系统比较了它们的抓取力、变形响应和电机扭矩要求。

Result: 研究结果表明，点阵方向强烈影响柔顺性、接触力和能源效率，不同的配置具有明显的优势。这种比较框架突出了定制拉胀几何形状以优化机器人夹持器性能的新颖性。

Conclusion: 研究结果为软硬混合夹持器设计提供了新见解，推动了精准农业的自动化策略，同时最大限度地减少了作物损伤。

Abstract: The agricultural sector is rapidly evolving to meet growing global food
demands, yet tasks like fruit and vegetable handling remain labor-intensive,
causing inefficiencies and post-harvest losses. Automation, particularly
selective harvesting, offers a viable solution, with soft robotics emerging as
a key enabler. This study introduces a novel hybrid gripper for tomato
harvesting, incorporating a rigid outer frame with a soft auxetic internal
lattice. The six-finger, 3D caging-effect design enables gentle yet secure
grasping in unstructured environments. Uniquely, the work investigates the
effect of auxetic lattice orientation on grasping conformability, combining
experimental validation with 2D Digital Image Correlation (DIC) and nonlinear
finite element analysis (FEA). Auxetic configurations with unit cell
inclinations of 0 deg, 30 deg, 45 deg, and 60 deg are evaluated, and their
grasping forces, deformation responses, and motor torque requirements are
systematically compared. Results demonstrate that lattice orientation strongly
influences compliance, contact forces, and energy efficiency, with distinct
advantages across configurations. This comparative framework highlights the
novelty of tailoring auxetic geometries to optimize robotic gripper
performance. The findings provide new insights into soft-rigid hybrid gripper
design, advancing automation strategies for precision agriculture while
minimizing crop damage.

</details>


### [458] [LOMORO: Long-term Monitoring of Dynamic Targets with Minimum Robotic Fleet under Resource Constraints](https://arxiv.org/abs/2510.10046)
*Mingke Lu,Shuaikang Wang,Meng Guo*

Main category: cs.RO

TL;DR: 本文提出了一种名为 LOMORO 的在线协作方案，用于多机器人长期目标监测、路径规划和资源充电，旨在解决资源限制和目标行为不确定性下的机器人队协调挑战，同时最小化活跃机器人数量。


<details>
  <summary>Details</summary>
Motivation: 长期监测大量动态目标对人类操作员而言枯燥乏味，对单个机器人而言不可行。现有协作机器人方案常未最小化机器人队规模，或忽视电池、内存等资源限制，也未充分考虑目标未知行为和机器人感知局限性带来的在线协调难题。

Method: 本文提出的 LOMORO 方案包含三个核心组件：(I) 在资源和监测间隔限制下对多机器人任务分配问题进行建模；(II) 资源感知任务协调算法，在高层动态目标分配与低层通过 Martin 算法进行多目标路径规划之间迭代；(III) 用于应对不可预测目标行为和机器人故障的在线自适应算法。

Result: LOMORO 方案能够确保所有目标都有明确的监测间隔上限，所有机器人都有资源水平下限，同时最小化平均活跃机器人数量。该方法通过大规模仿真在不同路网、机器人速度、充电速率和监测间隔下，与多个基线进行了广泛验证。

Conclusion: LOMORO 方案有效解决了多机器人长期目标监测中的在线协调挑战，兼顾了资源限制和不确定性，并通过最小化活跃机器人数量提高了效率，并在广泛仿真中展现了其有效性。

Abstract: Long-term monitoring of numerous dynamic targets can be tedious for a human
operator and infeasible for a single robot, e.g., to monitor wild flocks,
detect intruders, search and rescue. Fleets of autonomous robots can be
effective by acting collaboratively and concurrently. However, the online
coordination is challenging due to the unknown behaviors of the targets and the
limited perception of each robot. Existing work often deploys all robots
available without minimizing the fleet size, or neglects the constraints on
their resources such as battery and memory. This work proposes an online
coordination scheme called LOMORO for collaborative target monitoring, path
routing and resource charging. It includes three core components: (I) the
modeling of multi-robot task assignment problem under the constraints on
resources and monitoring intervals; (II) the resource-aware task coordination
algorithm iterates between the high-level assignment of dynamic targets and the
low-level multi-objective routing via the Martin's algorithm; (III) the online
adaptation algorithm in case of unpredictable target behaviors and robot
failures. It ensures the explicitly upper-bounded monitoring intervals for all
targets and the lower-bounded resource levels for all robots, while minimizing
the average number of active robots. The proposed methods are validated
extensively via large-scale simulations against several baselines, under
different road networks, robot velocities, charging rates and monitoring
intervals.

</details>


### [459] [Ionospheric and Plasmaspheric Delay Characterization for Lunar Terrestrial GNSS Receivers with Global Core Plasma Model](https://arxiv.org/abs/2510.10059)
*Keidai Iiyama,Grace Gao*

Main category: cs.RO

TL;DR: 本文研究并量化了地球GNSS信号用于月球定位导航授时（PNT）时，由电离层和等离子体层引起的延迟误差，发现其在特定条件下可达100米以上，为未来月球PNT算法设计提供依据。


<details>
  <summary>Details</summary>
Motivation: 尽管基于GNSS的月球导航已被验证，但由于独特的信号几何形状和长传播路径，未建模的电离层和等离子体层延迟仍是显著的误差源。

Method: 使用全球核心等离子体模型（GCPM）和自定义的低成本射线追踪算法来表征这些延迟。模拟了GNSS信号在月球轨道和月球南极接收时的一阶、二阶和三阶群延迟，以及射线弯曲引起的额外路径长度，并考虑了不同的太阳和地磁条件。

Result: 平均群延迟通常约为1米，但在高太阳活动期间的低空射线路径下可超过100米。弯曲延迟通常较小，但在低空射线路径下不可忽略。研究还量化了信号频率、地磁Kp指数和太阳R12指数的影响。

Conclusion: 这些发现为利用地球GNSS信号设计鲁棒的月球定位和授时算法提供了重要参考。

Abstract: Recent advancements in lunar positioning, navigation, and timing (PNT) have
demonstrated that terrestrial GNSS signals, including weak sidelobe
transmissions, can be exploited for lunar spacecraft positioning and timing.
While GNSS-based navigation at the Moon has been validated recently, unmodeled
ionospheric and plasmaspheric delays remain a significant error source,
particularly given the unique signal geometry and extended propagation paths.
This paper characterizes these delays using the Global Core Plasma Model (GCPM)
and a custom low-cost ray-tracing algorithm that iteratively solves for bent
signal paths. We simulate first-, second-, and third-order group delays, as
well as excess path length from ray bending, for GNSS signals received at both
lunar orbit and the lunar south pole under varying solar and geomagnetic
conditions. Results show that mean group delays are typically on the order of 1
m, but can exceed 100 m for low-altitude ray paths during high solar activity,
while bending delays are generally smaller but non-negligible for low-altitude
ray paths. We also quantify the influence of signal frequency, geomagnetic
$K_p$ index, and solar R12 index. These findings inform the design of robust
positioning and timing algorithms that utilize terrestrial GNSS signals.

</details>


### [460] [Beyond ADE and FDE: A Comprehensive Evaluation Framework for Safety-Critical Prediction in Multi-Agent Autonomous Driving Scenarios](https://arxiv.org/abs/2510.10086)
*Feifei Liu,Haozhe Wang,Zejun Wei,Qirong Lu,Yiyang Wen,Xiaoyu Tang,Jingyan Jiang,Zhijian He*

Main category: cs.RO

TL;DR: 当前自动驾驶预测模型的评估方法过于简化，无法捕捉复杂场景下的细微行为和故障。本文提出一种新颖的测试框架，通过考虑多样化的场景结构（如地图背景、智能体密度和空间分布）来评估预测性能，从而发现现有模型中的关键漏洞。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶预测模型评估方法（如ADE和FDE）过于简单，无法有效捕捉预测模块在复杂、交互式和安全关键驾驶场景中的细致行为。这些方法未能区分近距离和远距离智能体的影响，也未能系统地测试模型在不同多智能体交互下的鲁棒性，导致无法发现模型在特定场景下的脆弱性。

Method: 本文提出了一种新颖的测试框架，用于评估预测模型在多样化场景结构（包括地图背景、智能体密度和空间分布）下的性能。通过广泛的实证分析，该框架量化了智能体接近程度对目标轨迹预测的差异化影响，并识别了传统指标未能揭示的特定场景故障案例。

Result: 研究结果量化了智能体接近程度对目标轨迹预测的差异化影响，并成功识别了传统指标无法暴露的特定场景故障案例。这些发现揭示了当前最先进预测模型的关键脆弱性，并强调了场景感知评估的重要性。

Conclusion: 所提出的框架为严格的、以安全为导向的预测验证奠定了基础，对识别易出错的极端情况以及开发鲁棒、可认证的自动驾驶车辆预测系统具有重要贡献。

Abstract: Current evaluation methods for autonomous driving prediction models rely
heavily on simplistic metrics such as Average Displacement Error (ADE) and
Final Displacement Error (FDE). While these metrics offer basic performance
assessments, they fail to capture the nuanced behavior of prediction modules
under complex, interactive, and safety-critical driving scenarios. For
instance, existing benchmarks do not distinguish the influence of nearby versus
distant agents, nor systematically test model robustness across varying
multi-agent interactions. This paper addresses this critical gap by proposing a
novel testing framework that evaluates prediction performance under diverse
scene structures, saying, map context, agent density and spatial distribution.
Through extensive empirical analysis, we quantify the differential impact of
agent proximity on target trajectory prediction and identify scenario-specific
failure cases that are not exposed by traditional metrics. Our findings
highlight key vulnerabilities in current state-of-the-art prediction models and
demonstrate the importance of scenario-aware evaluation. The proposed framework
lays the groundwork for rigorous, safety-driven prediction validation,
contributing significantly to the identification of failure-prone corner cases
and the development of robust, certifiable prediction systems for autonomous
vehicles.

</details>


### [461] [Ctrl-World: A Controllable Generative World Model for Robot Manipulation](https://arxiv.org/abs/2510.10125)
*Yanjiang Guo,Lucy Xiaoyang Shi,Jianyu Chen,Chelsea Finn*

Main category: cs.RO

TL;DR: 本文提出一个可控的多视角世界模型，用于评估和改进通用机器人策略的指令遵循能力。该模型通过姿态条件记忆检索和帧级动作条件，实现了长程一致性和精确动作控制，能够准确评估策略性能并显著提升其成功率。


<details>
  <summary>Details</summary>
Motivation: 通用机器人策略在处理陌生物体和指令时，其评估和改进面临挑战。传统的真实世界测试耗时、昂贵且难以扩展。世界模型提供了一种可扩展的替代方案，但现有模型缺乏对多视角预测、细粒度动作控制和长程一致性的支持，无法与现代通用策略兼容。

Method: 本文引入了一个可控的多视角世界模型。该模型通过姿态条件记忆检索机制保持长程一致性，并通过帧级动作条件实现精确的动作控制。模型在DROID数据集（9.5万条轨迹，564个场景）上进行训练。

Result: 该模型能够在新场景和新摄像机位置下生成超过20秒的空间和时间一致的轨迹。它无需真实世界机器人操作即可准确评估策略性能。此外，通过在想象空间中合成成功的轨迹并用于监督微调，该方法能将策略成功率提高44.7%。

Conclusion: 本文成功开发了一个可控的多视角世界模型，解决了现有世界模型在处理通用机器人策略时的局限性。该模型为通用机器人策略的评估和改进提供了一个高效且可扩展的解决方案，特别是在指令遵循能力方面。

Abstract: Generalist robot policies can now perform a wide range of manipulation
skills, but evaluating and improving their ability with unfamiliar objects and
instructions remains a significant challenge. Rigorous evaluation requires a
large number of real-world rollouts, while systematic improvement demands
additional corrective data with expert labels. Both of these processes are
slow, costly, and difficult to scale. World models offer a promising, scalable
alternative by enabling policies to rollout within imagination space. However,
a key challenge is building a controllable world model that can handle
multi-step interactions with generalist robot policies. This requires a world
model compatible with modern generalist policies by supporting multi-view
prediction, fine-grained action control, and consistent long-horizon
interactions, which is not achieved by previous works. In this paper, we make a
step forward by introducing a controllable multi-view world model that can be
used to evaluate and improve the instruction-following ability of generalist
robot policies. Our model maintains long-horizon consistency with a
pose-conditioned memory retrieval mechanism and achieves precise action control
through frame-level action conditioning. Trained on the DROID dataset (95k
trajectories, 564 scenes), our model generates spatially and temporally
consistent trajectories under novel scenarios and new camera placements for
over 20 seconds. We show that our method can accurately rank policy performance
without real-world robot rollouts. Moreover, by synthesizing successful
trajectories in imagination and using them for supervised fine-tuning, our
approach can improve policy success by 44.7\%.

</details>


### [462] [CompassNav: Steering From Path Imitation To Decision Understanding In Navigation](https://arxiv.org/abs/2510.10154)
*LinFeng Li,Jian Zhao,Yuan Xie,Xin Tan,Xuelong Li*

Main category: cs.RO

TL;DR: 本文提出了一种新的导航范式“决策理解”，以取代现有大视觉语言模型（LVLMs）中模仿专家轨迹的“路径模仿”。通过引入新数据集Compass-Data-22k和一种间隙感知混合奖励函数，训练出的CompassNav代理实现了目标导航基准测试的最新SOTA，并在物理机器人上表现出强大的真实世界导航能力。


<details>
  <summary>Details</summary>
Motivation: 当前用于训练导航LVLMs的主流范式是模仿专家轨迹，这使得复杂的导航任务简化为单一正确路径的序列复制，从根本上限制了代理的探索和泛化能力。

Method: 1. 提出“决策理解”新范式，旨在让代理真正理解如何导航。2. 引入Compass-Data-22k数据集（2.2万条轨迹），其强化微调（RFT）子集通过A*测地距离标注所有可行动作，提供决策全景视图。3. 设计一种新型间隙感知混合奖励函数，根据决策确定性动态调整反馈，在最佳动作上提供决定性信号，同时提供细致分数鼓励探索。4. 将上述方法整合到SFT-then-RFT训练流程中，训练出CompassNav代理，使其发展出“内部指南针”以直觉判断目标方向。

Result: 1. 7B的CompassNav代理在目标导航基准测试中达到了新的SOTA，超越了更大的专有模型。2. 在物理机器人上实现了鲁棒的真实世界目标导航。3. 代理能够通过评估所有可能移动的相对质量来持续感知目标方向，而非仅仅记忆静态路线。

Conclusion: 从“路径模仿”转向“决策理解”的范式转变，结合Compass-Data-22k数据集和间隙感知混合奖励函数，显著提升了LVLM导航代理的性能。CompassNav代理展现出卓越的泛化能力、领先的性能和强大的真实世界应用潜力，证明了其从机械模仿到真正导航理解的成功转变。

Abstract: The dominant paradigm for training Large Vision-Language Models (LVLMs) in
navigation relies on imitating expert trajectories. This approach reduces the
complex navigation task to a sequence-to-sequence replication of a single
correct path, fundamentally limiting the agent's ability to explore and
generalize. In this work, we argue for and introduce a new paradigm: a shift
from Path Imitation to Decision Understanding. The goal of this paradigm is to
build agents that do not just follow, but truly understand how to navigate. We
materialize this through two core contributions: first, we introduce
Compass-Data-22k, a novel 22k-trajectory dataset.Its Reinforcement Fine-Tuning
(RFT) subset provides a panoramic view of the decision landscape by annotating
all feasible actions with A* geodesic distances. Second, we design a novel
gap-aware hybrid reward function that dynamically adapts its feedback to
decision certainty, shifting between decisive signals for optimal actions and
nuanced scores to encourage exploration. Integrated into an SFT-then-RFT
recipe, our CompassNav agent is trained not to memorize static routes, but to
develop an internal ``compass'' that constantly intuits the direction to the
goal by evaluating the relative quality of all possible moves. This approach
enables our 7B agent to set a new state-of-the-art on Goal navigation
benchmarks, outperforming even larger proprietary models, and achieve robust
real-world goal navigation on a physical robot.

</details>


### [463] [Dejavu: Post-Deployment Learning for Embodied Agents via Experience Feedback](https://arxiv.org/abs/2510.10181)
*Shaokai Wu,Yanbiao Ji,Qiuchang Li,Zhiyi Zhang,Qichen He,Wenyuan Xie,Guodong Zhang,Bayram Bayramli,Yue Ding,Hongtao Lu*

Main category: cs.RO

TL;DR: 本文提出了一种名为Dejavu的部署后学习框架，通过经验反馈网络（EFN）使具身智能体能够检索并利用成功的历史经验来增强其任务表现，从而实现部署后的持续学习。


<details>
  <summary>Details</summary>
Motivation: 具身智能体在部署到真实环境后，无法获取新知识以提升任务性能，这是其面临的一个根本限制。

Method: Dejavu框架采用经验反馈网络（EFN），通过检索执行记忆来增强冻结的视觉-语言-动作（VLA）策略。EFN自动识别情境成功的先前行动经验，并以此指导行动预测。它使用强化学习和语义相似度奖励来确保预测行动与当前观察下的历史成功行为保持一致。在部署期间，EFN会不断用新轨迹丰富其记忆。

Result: 在各种具身任务中的实验表明，EFN显著提高了智能体相对于冻结基线的适应性、鲁棒性和成功率。

Conclusion: 这些结果为具身智能体在部署后持续改进其行为提供了一条有前景的途径。

Abstract: Embodied agents face a fundamental limitation: once deployed in real-world
environments to perform specific tasks, they are unable to acquire new useful
knowledge to enhance task performance. In this paper, we propose a general
post-deployment learning framework called Dejavu, which employs an Experience
Feedback Network (EFN) and augments the frozen Vision-Language-Action (VLA)
policy with retrieved execution memories. EFN automatically identifies
contextually successful prior action experiences and conditions action
prediction on this retrieved guidance. We adopt reinforcement learning with
semantic similarity rewards on EFN to ensure that the predicted actions align
with past successful behaviors under current observations. During deployment,
EFN continually enriches its memory with new trajectories, enabling the agent
to exhibit "learning from experience" despite fixed weights. Experiments across
diverse embodied tasks show that EFN significantly improves adaptability,
robustness, and success rates over frozen baselines. These results highlight a
promising path toward embodied agents that continually refine their behavior
after deployment.

</details>


### [464] [It Takes Two: Learning Interactive Whole-Body Control Between Humanoid Robots](https://arxiv.org/abs/2510.10206)
*Zuhong Liu,Junhao Ge,Minhao Xiong,Jiahao Gu,Bowei Tang,Wei Jing,Siheng Chen*

Main category: cs.RO

TL;DR: Harmanoid是一个双人形机器人运动模仿框架，通过显式建模交互和接触，将人类交互动作转移到两个机器人上，以实现物理真实且社会有意义的互动。


<details>
  <summary>Details</summary>
Motivation: 现有单人形机器人方法在多机器人互动中存在局限性，忽略了机器人间的动态关系，导致接触错位、穿透和不真实的动作。研究的动机是实现人形机器人之间像人类一样丰富、物理接地且社会有意义的全身互动。

Method: Harmanoid框架包含两个核心组件：1) 接触感知运动重定向，通过将SMPL接触点与机器人顶点对齐来恢复身体间的协调；2) 交互驱动运动控制器，利用针对交互的奖励来强制执行协调的关键点和物理上可信的接触。该方法明确建模了机器人间的接触和交互感知动态。

Result: 实验证明，Harmanoid显著改善了交互式运动模仿，超越了在多机器人交互场景中表现不佳的现有单人形机器人框架。它成功捕捉了人形机器人之间的耦合行为。

Conclusion: Harmanoid通过显式建模机器人间的接触和交互感知动态，有效解决了单人形机器人框架在模拟多机器人交互时的局限性，实现了更真实、协调的双人形机器人互动。

Abstract: The true promise of humanoid robotics lies beyond single-agent autonomy: two
or more humanoids must engage in physically grounded, socially meaningful
whole-body interactions that echo the richness of human social interaction.
However, single-humanoid methods suffer from the isolation issue, ignoring
inter-agent dynamics and causing misaligned contacts, interpenetrations, and
unrealistic motions. To address this, we present Harmanoid , a dual-humanoid
motion imitation framework that transfers interacting human motions to two
robots while preserving both kinematic fidelity and physical realism. Harmanoid
comprises two key components: (i) contact-aware motion retargeting, which
restores inter-body coordination by aligning SMPL contacts with robot vertices,
and (ii) interaction-driven motion controller, which leverages
interaction-specific rewards to enforce coordinated keypoints and physically
plausible contacts. By explicitly modeling inter-agent contacts and
interaction-aware dynamics, Harmanoid captures the coupled behaviors between
humanoids that single-humanoid frameworks inherently overlook. Experiments
demonstrate that Harmanoid significantly improves interactive motion imitation,
surpassing existing single-humanoid frameworks that largely fail in such
scenarios.

</details>


### [465] [UF-RNN: Real-Time Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction](https://arxiv.org/abs/2510.10217)
*Hyogo Hiruma,Hiroshi Ito,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 该研究提出不确定性驱动预见循环神经网络（UF-RNN），通过内部模拟未来轨迹并最小化预测方差，使机器人能在高不确定性环境中主动探索，从而提高模仿学习在现实世界条件下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器人难以在状态不确定的环境中有效操作（如模糊的物体属性或不可预测的交互）。传统的模仿学习方法通常依赖成功案例，而忽略了不确定性最显著的失败情景。

Method: 提出不确定性驱动预见循环神经网络（UF-RNN）。该模型结合标准时间序列预测与一个主动的“预见”模块。该模块执行多个未来轨迹的内部模拟，并优化隐藏状态以最小化预测方差，从而使模型能够在高不确定性下选择性地探索行动。

Result: 在模拟和真实机器人开门任务中，即使没有明确的失败演示，UF-RNN 仍表现出鲁棒的适应性。通过预见模块的引导，模型潜在空间中自发的混沌动力学在环境模糊时激发探索性行为，相比传统随机 RNN 基线，成功率有所提高。

Conclusion: 将不确定性驱动的预见机制整合到模仿学习流程中，可以显著增强机器人处理不可预测的现实世界条件的能力。

Abstract: Training robots to operate effectively in environments with uncertain states,
such as ambiguous object properties or unpredictable interactions, remains a
longstanding challenge in robotics. Imitation learning methods typically rely
on successful examples and often neglect failure scenarios where uncertainty is
most pronounced. To address this limitation, we propose the Uncertainty-driven
Foresight Recurrent Neural Network (UF-RNN), a model that combines standard
time-series prediction with an active "Foresight" module. This module performs
internal simulations of multiple future trajectories and refines the hidden
state to minimize predicted variance, enabling the model to selectively explore
actions under high uncertainty. We evaluate UF-RNN on a door-opening task in
both simulation and a real-robot setting, demonstrating that, despite the
absence of explicit failure demonstrations, the model exhibits robust
adaptation by leveraging self-induced chaotic dynamics in its latent space.
When guided by the Foresight module, these chaotic properties stimulate
exploratory behaviors precisely when the environment is ambiguous, yielding
improved success rates compared to conventional stochastic RNN baselines. These
findings suggest that integrating uncertainty-driven foresight into imitation
learning pipelines can significantly enhance a robot's ability to handle
unpredictable real-world conditions.

</details>


### [466] [A3RNN: Bi-directional Fusion of Bottom-up and Top-down Process for Developmental Visual Attention in Robots](https://arxiv.org/abs/2510.10221)
*Hyogo Hiruma,Hiroshi Ito,Hiroki Mori,Tetsuya Ogata*

Main category: cs.RO

TL;DR: 本研究通过提出一种名为 $A^3 RNN$ 的新型注意力模型，探索了机器人在学习过程中自上而下（TD）和自下而上（BU）视觉注意力的发展性互动，发现注意力行为从显著性驱动的探索演变为预测驱动的引导。


<details>
  <summary>Details</summary>
Motivation: 旨在理解类人注意力行为如何在TD和BU机制的相互适应中随时间演变和形成。

Method: 提出了一种新颖的 $A^3 RNN$ 注意力模型，该模型通过双向注意力架构整合了预测性的TD信号和基于显著性的BU线索。模型在模仿学习的机器人操作任务中进行评估。

Result: 注意力行为在训练过程中不断演变：初期BU注意力突出视觉显著区域以引导TD过程；随着学习进展，TD注意力稳定并开始重塑对显著性的感知。模型展现出比基线更连贯和可解释的注意力模式，尽管未明确优化稳定性。

Conclusion: TD和BU机制之间的发展性互动有助于形成鲁棒的注意力，这与认知科学和自由能框架的原理相符，强调了感知和内部预测相互作用下自组织注意力的重要性。

Abstract: This study investigates the developmental interaction between top-down (TD)
and bottom-up (BU) visual attention in robotic learning. Our goal is to
understand how structured, human-like attentional behavior emerges through the
mutual adaptation of TD and BU mechanisms over time. To this end, we propose a
novel attention model $A^3 RNN$ that integrates predictive TD signals and
saliency-based BU cues through a bi-directional attention architecture.
  We evaluate our model in robotic manipulation tasks using imitation learning.
Experimental results show that attention behaviors evolve throughout training,
from saliency-driven exploration to prediction-driven direction. Initially, BU
attention highlights visually salient regions, which guide TD processes, while
as learning progresses, TD attention stabilizes and begins to reshape what is
perceived as salient. This trajectory reflects principles from cognitive
science and the free-energy framework, suggesting the importance of
self-organizing attention through interaction between perception and internal
prediction. Although not explicitly optimized for stability, our model exhibits
more coherent and interpretable attention patterns than baselines, supporting
the idea that developmental mechanisms contribute to robust attention
formation.

</details>


### [467] [Integration of the TIAGo Robot into Isaac Sim with Mecanum Drive Modeling and Learned S-Curve Velocity Profiles](https://arxiv.org/abs/2510.10273)
*Vincent Schoenbach,Marvin Wiedemann,Raphael Memmesheimer,Malte Mosbach,Sven Behnke*

Main category: cs.RO

TL;DR: 该论文为PAL Robotics TIAGo++ Omni机器人引入了一个经过校准的Isaac Sim模型，包括两种全向驱动控制模型，并提出了一种基于学习的校准方法，以实现对真实机器人行为的近似。


<details>
  <summary>Details</summary>
Motivation: 高效的物理仿真对机器人研究至关重要，特别是GPU加速仿真框架如Isaac Sim。然而，PAL Robotics TIAGo++ Omni这种多功能移动机械手在Isaac Sim中缺乏可用的模型，这限制了研究人员利用该平台进行基于学习的控制和实验。

Method: 作者引入了PAL Robotics TIAGo++ Omni在Isaac Sim中的模型，并对其进行校准以近似真实机器人的行为，特别关注其全向驱动动力学。他们提出了两种全向驱动控制模型：一个物理精确模型（复制真实世界车轮动力学）和一个轻量级基于速度的模型（为基于学习的应用优化）。此外，他们开发了一种基于学习的校准方法，利用最少的轨迹数据记录来近似真实机器人S形速度曲线。

Result: 论文成功地为PAL Robotics TIAGo++ Omni机器人创建了一个经过校准的Isaac Sim模型，并提供了两种全向驱动控制模型。同时，提出了一种基于学习的校准方法，能够使用少量数据逼近真实机器人的速度曲线。该集成已公开提供。

Conclusion: 该仿真模型将使研究人员能够使用PAL Robotics TIAGo++ Omni机器人在不同环境中进行实验，并高效地执行基于学习的控制，从而加速机器人技术的发展。

Abstract: Efficient physics simulation has significantly accelerated research progress
in robotics applications such as grasping and assembly. The advent of
GPU-accelerated simulation frameworks like Isaac Sim has particularly empowered
learning-based methods, enabling them to tackle increasingly complex tasks. The
PAL Robotics TIAGo++ Omni is a versatile mobile manipulator equipped with a
mecanum-wheeled base, allowing omnidirectional movement and a wide range of
task capabilities. However, until now, no model of the robot has been available
in Isaac Sim. In this paper, we introduce such a model, calibrated to
approximate the behavior of the real robot, with a focus on its omnidirectional
drive dynamics. We present two control models for the omnidirectional drive: a
physically accurate model that replicates real-world wheel dynamics and a
lightweight velocity-based model optimized for learning-based applications.
With these models, we introduce a learning-based calibration approach to
approximate the real robot's S-shaped velocity profile using minimal trajectory
data recordings. This simulation should allow researchers to experiment with
the robot and perform efficient learning-based control in diverse environments.
We provide the integration publicly at https://github.com/AIS-Bonn/tiago_isaac.

</details>


### [468] [X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model](https://arxiv.org/abs/2510.10274)
*Jinliang Zheng,Jianxiong Li,Zhihao Wang,Dongxiu Liu,Xirui Kang,Yuchun Feng,Yinan Zheng,Jiayin Zou,Yilun Chen,Jia Zeng,Ya-Qin Zhang,Jiangmiao Pang,Jingjing Liu,Tai Wang,Xianyuan Zhan*

Main category: cs.RO

TL;DR: 本文提出了一种名为“软提示”（Soft Prompt）的新方法，通过为每个数据源引入可学习的嵌入（即特定于载体的提示），将提示学习融入跨载体机器人学习中。基于此，开发了X-VLA架构，在多样化的模拟和真实机器人任务中实现了最先进的性能，展示了强大的灵活性和适应性。


<details>
  <summary>Details</summary>
Motivation: 成功的通用视觉-语言-动作（VLA）模型依赖于在多样化机器人平台上，利用大规模、跨载体、异构数据集进行有效训练。研究的动机在于如何促进并有效利用这些丰富多样的机器人数据源中的异构性。

Method: 本文提出了一种新颖的“软提示”方法，通过将提示学习概念融入跨载体机器人学习，并为每个不同的数据源引入单独的可学习嵌入集。这些嵌入充当特定于载体的提示，协同作用使VLA模型能够有效利用不同的跨载体特征。在此基础上，构建了一个名为X-VLA的基于流匹配的VLA架构，该架构完全依赖于软提示的标准Transformer编码器，兼具可扩展性和简洁性。

Result: 在6个模拟环境和3个真实世界机器人上进行评估后，本文提出的0.9B参数的X-VLA-0.9B模型在多个基准测试中同时实现了最先进（SOTA）的性能。它在从灵活的灵巧性到跨载体、环境和任务的快速适应性等广泛能力轴上均展现出卓越的结果。

Conclusion: 通过引入软提示方法和X-VLA架构，本文成功解决了利用异构跨载体机器人数据进行通用VLA模型训练的挑战。该方法不仅实现了最先进的性能，还展示了模型在不同载体、环境和任务中的强大泛化和适应能力。

Abstract: Successful generalist Vision-Language-Action (VLA) models rely on effective
training across diverse robotic platforms with large-scale, cross-embodiment,
heterogeneous datasets. To facilitate and leverage the heterogeneity in rich,
diverse robotic data sources, we propose a novel Soft Prompt approach with
minimally added parameters, by infusing prompt learning concepts into
cross-embodiment robot learning and introducing separate sets of learnable
embeddings for each distinct data source. These embeddings serve as
embodiment-specific prompts, which in unity empower VLA models with effective
exploitation of varying cross-embodiment features. Our new X-VLA, a neat
flow-matching-based VLA architecture, relies exclusively on soft-prompted
standard Transformer encoders, enjoying both scalability and simplicity.
Evaluated across 6 simulations as well as 3 real-world robots, our 0.9B
instantiation-X-VLA-0.9B simultaneously achieves SOTA performance over a sweep
of benchmarks, demonstrating superior results on a wide axes of capabilities,
from flexible dexterity to quick adaptation across embodiments, environments,
and tasks. Website: https://thu-air-dream.github.io/X-VLA/

</details>


### [469] [Towards Safe Maneuvering of Double-Ackermann-Steering Robots with a Soft Actor-Critic Framework](https://arxiv.org/abs/2510.10332)
*Kohio Deflesselle,Mélodie Daniel,Aly Magassouba,Miguel Aranda,Olivier Ly*

Main category: cs.RO

TL;DR: 本文提出了一种基于SAC、HER和CrossQ的深度强化学习框架，用于实现双阿克曼转向移动机器人（DASMRs）在复杂环境中的安全精确机动，无需人工轨迹或专家演示。


<details>
  <summary>Details</summary>
Motivation: 双阿克曼转向移动机器人（DASMRs）具有强大的运动学约束，使得传统的规划器在杂乱环境中表现脆弱，难以实现安全精确的机动。

Method: 该研究采用基于Soft Actor-Critic (SAC) 的深度强化学习框架，并结合了Hindsight Experience Replay (HER) 和 CrossQ 叠加来提高机动效率并避开障碍物。

Result: 仿真结果表明，所学习的策略能够使一辆重型四轮转向漫游车在避开障碍物的同时，稳健地达到高达97%的目标位置。

Conclusion: 该框架能够有效实现DASMRs的安全精确机动，且不依赖于人工设计的轨迹或专家演示。

Abstract: We present a deep reinforcement learning framework based on Soft Actor-Critic
(SAC) for safe and precise maneuvering of double-Ackermann-steering mobile
robots (DASMRs). Unlike holonomic or simpler non-holonomic robots such as
differential-drive robots, DASMRs face strong kinematic constraints that make
classical planners brittle in cluttered environments. Our framework leverages
the Hindsight Experience Replay (HER) and the CrossQ overlay to encourage
maneuvering efficiency while avoiding obstacles. Simulation results with a
heavy four-wheel-steering rover show that the learned policy can robustly reach
up to 97% of target positions while avoiding obstacles. Our framework does not
rely on handcrafted trajectories or expert demonstrations.

</details>


### [470] [Rise of the Robochemist](https://arxiv.org/abs/2510.10337)
*Jihong Zhu,Kefeng Huang,Jonathon Pipe,Chris Horbaczewsky,Andy Tyrrell,Ian J. S. Fairlamb*

Main category: cs.RO

TL;DR: 本文探讨了机器人化学家（robochemist）的概念，即结合机器人技术和人工智能，实现化学实验的自主设计、执行和分析，以加速化学发现和创新。


<details>
  <summary>Details</summary>
Motivation: 化学领域长期以来依赖手动且耗时的过程，现有自动化程度有限。研究动机在于通过整合机器人和人工智能，克服这些局限性，提高实验效率、重现性和安全性。

Method: 机器人化学家通过整合移动操作器、先进感知技术、远程操作和数据驱动协议来执行实验。它被设想为人类化学家的补充伙伴，而非完全替代。

Result: 机器人化学家能够以更高的适应性、重现性和安全性执行实验，从而更有效地探索化学空间，并加速制药、材料科学和可持续制造领域的创新。

Conclusion: 化学的未来在于人机共生伙伴关系，即人类的直觉和专业知识与机器人的精确性及人工智能驱动的洞察力相结合，共同推动化学发展。

Abstract: Chemistry, a long-standing discipline, has historically relied on manual and
often time-consuming processes. While some automation exists, the field is now
on the cusp of a significant evolution driven by the integration of robotics
and artificial intelligence (AI), giving rise to the concept of the
robochemist: a new paradigm where autonomous systems assist in designing,
executing, and analyzing experiments. Robochemists integrate mobile
manipulators, advanced perception, teleoperation, and data-driven protocols to
execute experiments with greater adaptability, reproducibility, and safety.
Rather than a fully automated replacement for human chemists, we envisioned the
robochemist as a complementary partner that works collaboratively to enhance
discovery, enabling a more efficient exploration of chemical space and
accelerating innovation in pharmaceuticals, materials science, and sustainable
manufacturing. This article traces the technologies, applications, and
challenges that define this transformation, highlighting both the opportunities
and the responsibilities that accompany the emergence of the robochemist.
Ultimately, the future of chemistry is argued to lie in a symbiotic partnership
where human intuition and expertise is amplified by robotic precision and
AI-driven insight.

</details>


### [471] [sqrtVINS: Robust and Ultrafast Square-Root Filter-based 3D Motion Tracking](https://arxiv.org/abs/2510.10346)
*Yuxiang Peng,Chuchu Chen,Kejian Wu,Guoquan Huang*

Main category: cs.RO

TL;DR: 本文首次开发并开源了一种基于平方根滤波器（SRF）的视觉惯性导航系统（VINS），名为sqrtVINS，它具有超快、数值稳定和在极端条件下（如极小时间窗）动态初始化的能力。


<details>
  <summary>Details</summary>
Motivation: 尽管VINS最近取得了进展，但在资源受限且精度有限的嵌入式（机器人）系统上，资源限制和数值不稳定性仍然是关键挑战。传统的SRF在更新过程中会破坏协方差矩阵的三角结构，导致效率低下。

Method: 本文提出了一种新颖的基于Cholesky分解（LLT）的SRF更新方法，通过充分利用系统结构来保持三角结构，显著提高了VINS的效率。此外，设计了一种快速、鲁棒的动态初始化方法，该方法首先在不进行3D特征三角化的情况下恢复最小状态，然后有效地执行迭代SRF更新以完善完整状态。

Result: 通过数值研究，LLT-based SRF表现出卓越的数值稳定性，并在32位单精度浮点数上实现了鲁棒高效的性能，运行速度是现有最先进方法的两倍。初始化方法在移动工作站和Jetson Nano计算机上进行测试，即使在100毫秒的时间窗内也能实现高成功率。最终，sqrtVINS在多种场景下得到了广泛验证，展示了强大的效率、鲁棒性和可靠性。

Conclusion: 本文成功开发并开源了sqrtVINS，一个基于SRF的VINS，它通过创新的LLT-based SRF更新和动态初始化方法，克服了嵌入式系统上的数值不稳定性与效率挑战，实现了超快、数值稳定且鲁棒的性能，为未来的研究和应用提供了支持。

Abstract: In this paper, we develop and open-source, for the first time, a square-root
filter (SRF)-based visual-inertial navigation system (VINS), termed sqrtVINS,
which is ultra-fast, numerically stable, and capable of dynamic initialization
even under extreme conditions (i.e., extremely small time window). Despite
recent advancements in VINS, resource constraints and numerical instability on
embedded (robotic) systems with limited precision remain critical challenges. A
square-root covariance-based filter offers a promising solution by providing
numerical stability, efficient memory usage, and guaranteed positive
semi-definiteness. However, canonical SRFs suffer from inefficiencies caused by
disruptions in the triangular structure of the covariance matrix during
updates. The proposed method significantly improves VINS efficiency with a
novel Cholesky decomposition (LLT)-based SRF update, by fully exploiting the
system structure to preserve the structure. Moreover, we design a fast, robust,
dynamic initialization method, which first recovers the minimal states without
triangulating 3D features and then efficiently performs iterative SRF update to
refine the full states, enabling seamless VINS operation. The proposed
LLT-based SRF is extensively verified through numerical studies, demonstrating
superior numerical stability and achieving robust efficient performance on
32-bit single-precision floats, operating at twice the speed of
state-of-the-art (SOTA) methods. Our initialization method, tested on both
mobile workstations and Jetson Nano computers, achieving a high success rate of
initialization even within a 100 ms window under minimal conditions. Finally,
the proposed sqrtVINS is extensively validated across diverse scenarios,
demonstrating strong efficiency, robustness, and reliability. The full
open-source implementation is released to support future research and
applications.

</details>


### [472] [RobotFleet: An Open-Source Framework for Centralized Multi-Robot Task Planning](https://arxiv.org/abs/2510.10379)
*Rohan Gupta,Trevor Asbery,Zain Merchant,Abrar Anwar,Jesse Thomason*

Main category: cs.RO

TL;DR: 本文介绍了一个名为RobotFleet的开源可扩展框架，它利用大型语言模型（LLMs）实现异构机器人集群的集中式任务规划和调度，以完成多重目标，并降低构建可扩展多机器人系统的门槛。


<details>
  <summary>Details</summary>
Motivation: 在多机器人系统中，协调异构机器人集群以实现多个目标是一项挑战。

Method: RobotFleet框架提供规划、调度和执行的抽象层，将机器人部署为容器化服务以简化集群管理和扩展。它维护一个共享的声明性世界状态和用于任务执行与重新规划的双向通信。通过模块化自主堆栈的每一层并使用LLMs进行开放世界推理。

Result: 该框架使异构机器人集群能够完成多项任务，简化了集群的扩展和管理，并降低了构建可扩展多机器人系统的障碍。

Conclusion: RobotFleet是一个利用LLMs进行开放世界推理的有效框架，能够实现异构机器人集群的集中式任务规划和调度，从而简化了可扩展多机器人系统的开发和管理。

Abstract: Coordinating heterogeneous robot fleets to achieve multiple goals is
challenging in multi-robot systems. We introduce an open-source and extensible
framework for centralized multi-robot task planning and scheduling that
leverages LLMs to enable fleets of heterogeneous robots to accomplish multiple
tasks. RobotFleet provides abstractions for planning, scheduling, and execution
across robots deployed as containerized services to simplify fleet scaling and
management. The framework maintains a shared declarative world state and
two-way communication for task execution and replanning. By modularizing each
layer of the autonomy stack and using LLMs for open-world reasoning, RobotFleet
lowers the barrier to building scalable multi-robot systems. The code can be
found here: https://github.com/therohangupta/robot-fleet.

</details>


### [473] [Learning to Throw-Flip](https://arxiv.org/abs/2510.10357)
*Yang Liu,Bruno Da Costa,Aude Billard*

Main category: cs.RO

TL;DR: 本文提出了一种机器人“抛掷翻转”物体到期望着陆姿态（位置和方向）的方法，通过解耦寄生旋转并结合物理模型与学习方法，显著提高了准确性、样本效率和知识迁移能力。


<details>
  <summary>Details</summary>
Motivation: 动态操作（如机器人抛掷）在物流中备受关注，但现有研究主要集中在物体着陆位置，忽略了最终姿态。传统旋转机器人抛掷会产生寄生旋转，导致着陆姿态受限且难以控制。

Method: 该方法基于两个关键设计选择：1. 利用冲量-动量原理设计抛掷动作，有效解耦寄生旋转，扩展了可行的着陆姿态集。2. 将基于物理的自由飞行模型与基于回归的学习方法相结合，以处理未建模效应。此外，还采用了数据同化来减少样本复杂度，并重用手内物体旋转的先验知识来加速学习。

Result: 真实机器人实验表明，该框架能在数十次试验中学会将物体“抛掷翻转”到（±5厘米，±45度）阈值内的目标姿态。与端到端学习方法相比，数据同化在处理未见姿态时平均减少了40%的样本复杂度。此外，当抛掷重心偏移的新物体时，重用手内物体旋转的先验知识可将学习速度提高70%。

Conclusion: 所提出的框架能够准确地将物体“抛掷翻转”到期望的着陆姿态，通过解耦寄生旋转和结合物理模型与学习方法，显著提高了操作的精度和鲁棒性，并展现了在样本效率和知识迁移方面的优势。

Abstract: Dynamic manipulation, such as robot tossing or throwing objects, has recently
gained attention as a novel paradigm to speed up logistic operations. However,
the focus has predominantly been on the object's landing location, irrespective
of its final orientation. In this work, we present a method enabling a robot to
accurately "throw-flip" objects to a desired landing pose (position and
orientation). Conventionally, objects thrown by revolute robots suffer from
parasitic rotation, resulting in highly restricted and uncontrollable landing
poses. Our approach is based on two key design choices: first, leveraging the
impulse-momentum principle, we design a family of throwing motions that
effectively decouple the parasitic rotation, significantly expanding the
feasible set of landing poses. Second, we combine a physics-based model of free
flight with regression-based learning methods to account for unmodeled effects.
Real robot experiments demonstrate that our framework can learn to throw-flip
objects to a pose target within ($\pm$5 cm, $\pm$45 degrees) threshold in
dozens of trials. Thanks to data assimilation, incorporating projectile
dynamics reduces sample complexity by an average of 40% when throw-flipping to
unseen poses compared to end-to-end learning methods. Additionally, we show
that past knowledge on in-hand object spinning can be effectively reused,
accelerating learning by 70% when throwing a new object with a Center of Mass
(CoM) shift. A video summarizing the proposed method and the hardware
experiments is available at https://youtu.be/txYc9b1oflU.

</details>


### [474] [MicroRoboScope: A Portable and Integrated Mechatronic Platform for Magnetic and Acoustic Microrobotic Experimentation](https://arxiv.org/abs/2510.10392)
*Max Sokolich,Yanda Yang,Subrahmanyam Cherukumilli,Fatma Ceren Kirmizitas,Sambeeta Das*

Main category: cs.RO

TL;DR: 本文介绍MicroRoboScope，一个便携、紧凑、多功能的微型机器人实验平台，可实现磁性与声学微型机器人的实时闭环控制。


<details>
  <summary>Details</summary>
Motivation: 现有微型机器人实验平台可能成本高昂或复杂，作者旨在降低微型机器人实验的门槛，使其适用于专业研究、教育和推广，从而促进生物医学等领域的发展。

Method: MicroRoboScope系统集成了嵌入式计算机、显微镜、电源和控制电路。它采用定制的Python和Arduino C++软件，用于实时视频采集、微型机器人追踪以及电磁线圈和声学换能器的控制信号生成。

Result: 该平台实现了多模态驱动、高可访问性和便携性，不仅适用于专业研究实验室，也适用于教育和推广环境，有效降低了微型机器人实验的准入门槛。

Conclusion: MicroRoboScope系统通过提供一个易于获取的平台，为生物医学、组织工程和机器人技术等领域的研究、教育和转化应用创造了新的机会。

Abstract: This paper presents MicroRoboScope, a portable, compact, and versatile
microrobotic experimentation platform designed for real-time, closed-loop
control of both magnetic and acoustic microrobots. The system integrates an
embedded computer, microscope, power supplies, and control circuitry into a
single, low-cost and fully integrated apparatus. Custom control software
developed in Python and Arduino C++ handles live video acquisition, microrobot
tracking, and generation of control signals for electromagnetic coils and
acoustic transducers. The platform's multi-modal actuation, accessibility, and
portability make it suitable not only for specialized research laboratories but
also for educational and outreach settings. By lowering the barrier to entry
for microrobotic experimentation, this system enables new opportunities for
research, education, and translational applications in biomedicine, tissue
engineering, and robotics.

</details>


### [475] [Hierarchical Planning for Long-Horizon Multi-Target Tracking Under Target Motion Uncertainty](https://arxiv.org/abs/2510.10421)
*Junbin Yuan,Brady Moon,Muqing Cao,Sebastian Scherer*

Main category: cs.RO

TL;DR: 本文提出了一种分层规划器，用于单架无人机在大空间区域内对多个动态目标进行持久跟踪，通过结合运动模型和不确定性传播，并利用马尔可夫决策过程解决长期规划问题，显著降低了跟踪不确定性。


<details>
  <summary>Details</summary>
Motivation: 单机器人系统在追踪多个动态目标时面临挑战，尤其是在大空间区域和有限感知能力下。目标在视野外会积累不确定性，现有方法多依赖短期规划且假设环境较小，导致在大规模场景中跟踪性能差，容易丢失目标。

Method: 本文提出了一种分层规划器。它在路径执行过程中融入运动模型和不确定性传播，以应对非静态目标的挑战。任务被分解为单目标搜索和检测子任务。该方法包括一个新颖的低级覆盖规划器，用于在不断演变的信念区域中搜索目标；一个评估每个子任务成功可能性的估计方法，从而将主动目标跟踪任务转换为马尔可夫决策过程（MDP），并使用基于树的算法来确定子任务序列。

Result: 该方法在仿真中得到了验证，与现有主动目标跟踪规划器相比，表现出更高的效率。在不同环境中，所提出的规划器优于现有方法，最终不确定性降低了11-70%。

Conclusion: 所提出的分层规划器在主动目标跟踪任务中优于现有方法，能够有效降低最终不确定性，解决了单机器人系统在大规模、多动态目标跟踪中的挑战。

Abstract: Achieving persistent tracking of multiple dynamic targets over a large
spatial area poses significant challenges for a single-robot system with
constrained sensing capabilities. As the robot moves to track different
targets, the ones outside the field of view accumulate uncertainty, making them
progressively harder to track. An effective path planning algorithm must manage
uncertainty over a long horizon and account for the risk of permanently losing
track of targets that remain unseen for too long. However, most existing
approaches rely on short planning horizons and assume small, bounded
environments, resulting in poor tracking performance and target loss in
large-scale scenarios. In this paper, we present a hierarchical planner for
tracking multiple moving targets with an aerial vehicle. To address the
challenge of tracking non-static targets, our method incorporates motion models
and uncertainty propagation during path execution, allowing for more informed
decision-making. We decompose the multi-target tracking task into sub-tasks of
single target search and detection, and our proposed pipeline consists a novel
low-level coverage planner that enables searching for a target in an evolving
belief area, and an estimation method to assess the likelihood of success for
each sub-task, making it possible to convert the active target tracking task to
a Markov decision process (MDP) that we solve with a tree-based algorithm to
determine the sequence of sub-tasks. We validate our approach in simulation,
demonstrating its effectiveness compared to existing planners for active target
tracking tasks, and our proposed planner outperforms existing approaches,
achieving a reduction of 11-70% in final uncertainty across different
environments.

</details>


### [476] [Towards Dynamic Quadrupedal Gaits: A Symmetry-Guided RL Hierarchy Enables Free Gait Transitions at Varying Speeds](https://arxiv.org/abs/2510.10455)
*Jiayu Ding,Xulin Chen,Garrett E. Katz,Zhenyu Gan*

Main category: cs.RO

TL;DR: 本文提出了一种统一的强化学习框架，利用四足机器人的内在对称性和速度-周期关系，生成多样的步态，无需人工调优或预设轨迹。


<details>
  <summary>Details</summary>
Motivation: 生成特定的四足步态通常需要专家对大量变量（如触地、抬腿事件和每条腿的完整约束）进行繁琐的手动调整。

Method: 提出了一种统一的强化学习框架，通过利用动态腿式系统的内在对称性和速度-周期关系来生成多功能四足步态。设计了一个对称性引导的奖励函数，融合了时间、形态和时间反演对称性。

Result: 该方法消除了对预定义轨迹的需求，实现了小跑、跳跃、半跳和奔跑等多种运动模式之间的平滑过渡。在Unitree Go2机器人上，无论是在仿真还是硬件测试中，都能在不同速度下表现出稳健的性能，显著提高了步态适应性，且无需大量的奖励调优或显式的足部放置控制。

Conclusion: 这项工作提供了对动态运动策略的见解，并强调了对称性在机器人步态设计中的关键作用。

Abstract: Quadrupedal robots exhibit a wide range of viable gaits, but generating
specific footfall sequences often requires laborious expert tuning of numerous
variables, such as touch-down and lift-off events and holonomic constraints for
each leg. This paper presents a unified reinforcement learning framework for
generating versatile quadrupedal gaits by leveraging the intrinsic symmetries
and velocity-period relationship of dynamic legged systems. We propose a
symmetry-guided reward function design that incorporates temporal,
morphological, and time-reversal symmetries. By focusing on preserved
symmetries and natural dynamics, our approach eliminates the need for
predefined trajectories, enabling smooth transitions between diverse locomotion
patterns such as trotting, bounding, half-bounding, and galloping. Implemented
on the Unitree Go2 robot, our method demonstrates robust performance across a
range of speeds in both simulations and hardware tests, significantly improving
gait adaptability without extensive reward tuning or explicit foot placement
control. This work provides insights into dynamic locomotion strategies and
underscores the crucial role of symmetries in robotic gait design.

</details>


### [477] [Galilean Symmetry in Robotics](https://arxiv.org/abs/2510.10468)
*Robert Mahony,Jonathan Kelly,Stephan Weiss*

Main category: cs.RO

TL;DR: 本文为机器人学领域量身定制了伽利略对称性的阐述，利用刚体变换概念，并展示了伽利略矩阵李群如何描述两种姿态表示，通过实例证明其在机器人问题中的应用价值。


<details>
  <summary>Details</summary>
Motivation: 尽管刚体对称性是机器人学中的基本工具，但针对机器人领域的伽利略对称性处理却鲜有提及。物理学文献中通常将其作为爱因斯坦相对论的铺垫，而非独立应用于机器人问题。

Method: 本文利用机器人社区对刚体变换和姿态表示的熟悉程度，对伽利略对称性进行了定制化阐述。核心方法是使用伽利略矩阵李群来描述两种不同的姿态表示：使用惯性速度定义状态的伽利略坐标系（Galilean frames）和使用坐标速度的扩展姿态（extended poses）。

Result: 伽利略矩阵李群可以描述两种不同的姿态表示。将伽利略矩阵李群代数应用于机器人问题是直接有效的，并在以下三个领域产生了重要见解：旋转地球上的惯性导航、机械臂运动学以及时间不确定性下的传感器数据融合。

Conclusion: 机器人学界有望通过重新发现、扩展和应用这一经典材料来解决现代问题，从而从中受益。

Abstract: Galilean symmetry is the natural symmetry of inertial motion that underpins
Newtonian physics. Although rigid-body symmetry is one of the most established
and fundamental tools in robotics, there appears to be no comparable treatment
of Galilean symmetry for a robotics audience. In this paper, we present a
robotics-tailored exposition of Galilean symmetry that leverages the
community's familiarity with and understanding of rigid-body transformations
and pose representations. Our approach contrasts with common treatments in the
physics literature that introduce Galilean symmetry as a stepping stone to
Einstein's relativity. A key insight is that the Galilean matrix Lie group can
be used to describe two different pose representations, Galilean frames, that
use inertial velocity in the state definition, and extended poses, that use
coordinate velocity. We provide three examples where applying the Galilean
matrix Lie-group algebra to robotics problems is straightforward and yields
significant insights: inertial navigation above the rotating Earth, manipulator
kinematics, and sensor data fusion under temporal uncertainty. We believe that
the time is right for the robotics community to benefit from rediscovering and
extending this classical material and applying it to modern problems.

</details>


### [478] [SuperEx: Enhancing Indoor Mapping and Exploration using Non-Line-of-Sight Perception](https://arxiv.org/abs/2510.10506)
*Kush Garg,Akshat Dave*

Main category: cs.RO

TL;DR: 该研究将非视距（NLOS）感知技术引入机器人探索，利用单光子激光雷达实现绕过障碍物的感知和映射，显著提高了探索效率和地图准确性。


<details>
  <summary>Details</summary>
Motivation: 在未知室内环境中，高效探索和绘图是一个基本挑战。现有机器人感知受限于视距，导致在布局与预设不符时探索效率低下，未被物理遍历的区域保持未知。

Method: 引入了名为SuperEx的框架，将非视距（NLOS）感知直接整合到映射-探索循环中。该框架利用单光子激光雷达捕获的时间飞行直方图，通过以下方式增强全局地图预测：(i) 从时间直方图中“雕刻”出空的NLOS区域；(ii) 通过结合基于物理和数据驱动的两步方法，利用结构规律重建被占据的结构。

Result: 在复杂的模拟地图和真实的KTH Floorplan数据集上进行评估，结果显示在覆盖率低于30%的情况下，地图准确性提高了12%，并且与视距基线相比，探索效率得到了提升。

Conclusion: 该工作为超越直接可见性范围的可靠映射开辟了道路，通过将NLOS感知集成到机器人探索中，显著提高了在未知环境中的映射准确性和探索效率。

Abstract: Efficient exploration and mapping in unknown indoor environments is a
fundamental challenge, with high stakes in time-critical settings. In current
systems, robot perception remains confined to line-of-sight; occluded regions
remain unknown until physically traversed, leading to inefficient exploration
when layouts deviate from prior assumptions. In this work, we bring
non-line-of-sight (NLOS) sensing to robotic exploration. We leverage
single-photon LiDARs, which capture time-of-flight histograms that encode the
presence of hidden objects - allowing robots to look around blind corners.
Recent single-photon LiDARs have become practical and portable, enabling
deployment beyond controlled lab settings. Prior NLOS works target 3D
reconstruction in static, lab-based scenarios, and initial efforts toward
NLOS-aided navigation consider simplified geometries. We introduce SuperEx, a
framework that integrates NLOS sensing directly into the mapping-exploration
loop. SuperEx augments global map prediction with beyond-line-of-sight cues by
(i) carving empty NLOS regions from timing histograms and (ii) reconstructing
occupied structure via a two-step physics-based and data-driven approach that
leverages structural regularities. Evaluations on complex simulated maps and
the real-world KTH Floorplan dataset show a 12% gain in mapping accuracy under
< 30% coverage and improved exploration efficiency compared to line-of-sight
baselines, opening a path to reliable mapping beyond direct visibility.

</details>


### [479] [Population-Coded Spiking Neural Networks for High-Dimensional Robotic Control](https://arxiv.org/abs/2510.10516)
*Kanishkha Jaisankar,Xiaoyang Jiang,Feifan Liao,Jeethu Sreenivas Amuthan*

Main category: cs.RO

TL;DR: 本文提出了一种结合群体编码脉冲神经网络（SNNs）与深度强化学习（DRL）的新框架，旨在实现高能效、高性能的机器人控制，特别适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 机器人领域中，能效高、性能好的电机控制仍是一个关键挑战，尤其是在资源有限的高维连续控制任务中。深度强化学习（DRL）虽然效果显著，但其计算需求和能耗限制了在资源受限环境中的部署。

Method: 该研究引入了一个结合群体编码SNNs和DRL的新框架。它利用SNNs的事件驱动、异步计算特性与DRL的鲁棒策略优化能力。核心是群体编码脉冲执行器网络（PopSAN），用于将高维观测编码为神经元群体活动，并通过基于梯度的更新实现最优策略学习。方法在Isaac Gym平台，使用PixMC基准和复杂的机器人操作任务（Franka机械臂）进行评估。

Result: 与传统人工神经网络（ANNs）相比，该方法实现了高达96.10%的能耗节省，同时保持了可媲美的控制性能。训练后的SNN策略在手指位置跟踪方面表现出鲁棒性，与指令轨迹偏差极小，并在抓取放置操作中保持了稳定的目标高度。

Conclusion: 群体编码SNNs为资源受限应用中的高能效、高性能机器人控制提供了一个有前景的解决方案，为在真实世界机器人系统中进行可扩展部署铺平了道路。

Abstract: Energy-efficient and high-performance motor control remains a critical
challenge in robotics, particularly for high-dimensional continuous control
tasks with limited onboard resources. While Deep Reinforcement Learning (DRL)
has achieved remarkable results, its computational demands and energy
consumption limit deployment in resource-constrained environments. This paper
introduces a novel framework combining population-coded Spiking Neural Networks
(SNNs) with DRL to address these challenges. Our approach leverages the
event-driven, asynchronous computation of SNNs alongside the robust policy
optimization capabilities of DRL, achieving a balance between energy efficiency
and control performance. Central to this framework is the Population-coded
Spiking Actor Network (PopSAN), which encodes high-dimensional observations
into neuronal population activities and enables optimal policy learning through
gradient-based updates. We evaluate our method on the Isaac Gym platform using
the PixMC benchmark with complex robotic manipulation tasks. Experimental
results on the Franka robotic arm demonstrate that our approach achieves energy
savings of up to 96.10% compared to traditional Artificial Neural Networks
(ANNs) while maintaining comparable control performance. The trained SNN
policies exhibit robust finger position tracking with minimal deviation from
commanded trajectories and stable target height maintenance during
pick-and-place operations. These results position population-coded SNNs as a
promising solution for energy-efficient, high-performance robotic control in
resource-constrained applications, paving the way for scalable deployment in
real-world robotics systems.

</details>


### [480] [Decoupled Scaling 4ch Bilateral Control on the Cartesian coordinate by 6-DoF Manipulator using Rotation Matrix](https://arxiv.org/abs/2510.10545)
*Koki Yamane,Sho Sakaino,Toshiaki Tsuji*

Main category: cs.RO

TL;DR: 本文提出了一种四通道双边控制方法，可在笛卡尔坐标系中解耦各维度，实现所需的动态特性，且不受缩放因子影响，从而提高远程操作性。


<details>
  <summary>Details</summary>
Motivation: 四通道双边控制通过同步两个机械臂的位置和力，提供力反馈和可调操作性，显著改善接触密集型任务的远程操作。在笛卡尔坐标系下的四通道双边控制对不同结构的机械臂具有优势，并能通过调整控制参数来调整笛卡尔坐标系中的动态特性，从而实现直观的操作性。

Method: 本文提出了一种四通道双边控制方法，该方法通过在笛卡尔坐标系中解耦每个维度，实现了所需的动态特性，且不受缩放因子影响。

Result: 该方法能够在笛卡尔坐标系中通过解耦各维度来实现期望的动态特性，并且这种实现方式独立于缩放因子。

Conclusion: 所提出的四通道双边控制方法，通过在笛卡尔坐标系中解耦各维度，能够实现期望的动态特性，且不受缩放因子影响，有望显著提高远程操作性，特别适用于不同结构的机械臂和接触密集型任务。

Abstract: Four-channel bilateral control is a method for achieving remote control with
force feedback and adjustment operability by synchronizing the positions and
forces of two manipulators. This is expected to significantly improve the
operability of the remote control in contact-rich tasks. Among these, 4-channel
bilateral control on the Cartesian coordinate system is advantageous owing to
its suitability for manipulators with different structures and because it
allows the dynamics in the Cartesian coordinate system to be adjusted by
adjusting the control parameters, thus achieving intuitive operability for
humans. This paper proposes a 4-channel bilateral control method that achieves
the desired dynamics by decoupling each dimension in the Cartesian coordinate
system regardless of the scaling factor.

</details>


### [481] [Reinforcement Learning-based Dynamic Adaptation for Sampling-Based Motion Planning in Agile Autonomous Driving](https://arxiv.org/abs/2510.10567)
*Alexander Langmann,Yevhenii Tokarev,Mattia Piccinini,Korbinian Moller,Johannes Betz*

Main category: cs.RO

TL;DR: 该论文提出一种将强化学习（RL）作为高层行为选择器的方法，用于动态调整采样式轨迹规划器的成本函数参数，以在自动驾驶赛车中实现更好的安全性和竞争力。


<details>
  <summary>Details</summary>
Motivation: 采样式轨迹规划器在自动驾驶中广泛使用，但其成本函数中的手动调优、静态权重导致在不同赛车场景中表现不佳，无法兼顾安全性与竞争力。

Method: 本文提出使用一个强化学习（RL）智能体作为高层行为选择器，在运行时动态切换低层分析型轨迹规划器的成本函数参数。

Result: 在自动驾驶赛车仿真环境中，RL规划器实现了0%的碰撞率，并将超车时间比现有静态规划器缩短了高达60%。该方法能够动态切换激进和保守行为，实现静态配置无法达到的交互式机动。

Conclusion: 将强化学习作为高层选择器，解决了自动驾驶赛车规划器中固有的安全性和竞争力之间的权衡问题，为更广泛的自动驾驶应用提供了自适应且可解释的运动规划途径。

Abstract: Sampling-based trajectory planners are widely used for agile autonomous
driving due to their ability to generate fast, smooth, and kinodynamically
feasible trajectories. However, their behavior is often governed by a cost
function with manually tuned, static weights, which forces a tactical
compromise that is suboptimal across the wide range of scenarios encountered in
a race. To address this shortcoming, we propose using a Reinforcement Learning
(RL) agent as a high-level behavioral selector that dynamically switches the
cost function parameters of an analytical, low-level trajectory planner during
runtime. We show the effectiveness of our approach in simulation in an
autonomous racing environment where our RL-based planner achieved 0% collision
rate while reducing overtaking time by up to 60% compared to state-of-the-art
static planners. Our new agent now dynamically switches between aggressive and
conservative behaviors, enabling interactive maneuvers unattainable with static
configurations. These results demonstrate that integrating reinforcement
learning as a high-level selector resolves the inherent trade-off between
safety and competitiveness in autonomous racing planners. The proposed
methodology offers a pathway toward adaptive yet interpretable motion planning
for broader autonomous driving applications.

</details>


### [482] [Fast Vision in the Dark: A Case for Single-Photon Imaging in Planetary Navigation](https://arxiv.org/abs/2510.10597)
*David Rodríguez-Martínez,C. J. Pérez del Pulgar*

Main category: cs.RO

TL;DR: 本研究提出并首次全面评估了利用单光子雪崩二极管（SPAD）相机进行行星导航的新方法，以应对传统相机在复杂光照条件下的局限性，特别关注高纬度月球区域。


<details>
  <summary>Details</summary>
Motivation: 提高机器人导航能力对于扩展探索范围和提高操作效率至关重要。传统的CCD或CMOS相机在复杂光照和运动条件下表现不佳，限制了移动行星机器人的探索范围和可达性。

Method: 本研究提出了一种利用SPAD相机独特成像能力进行行星导航的新方法。详细介绍了SPAD相机的工作原理和性能特征，评估了其在解决未来月球探索任务感知挑战中的优势和局限性，并在代表性光照条件下对其性能进行了基准测试。

Result: 本研究首次对单光子成像作为一种替代性被动传感技术进行了全面评估，用于面向感知挑战性地点的机器人探索任务，特别强调了高纬度月球区域。评估了SPAD相机在解决月球探索任务关键感知挑战中的优势和局限性，并对其在代表性光照条件下的性能进行了基准测试。

Conclusion: SPAD相机作为一种替代性的被动传感技术，在应对复杂光照条件下（如高纬度月球区域）的行星导航感知挑战方面具有巨大潜力，有望扩展机器人探索的范围和效率。

Abstract: Improving robotic navigation is critical for extending exploration range and
enhancing operational efficiency. Vision-based navigation relying on
traditional CCD or CMOS cameras faces major challenges when complex
illumination conditions are paired with motion, limiting the range and
accessibility of mobile planetary robots. In this study, we propose a novel
approach to planetary navigation that leverages the unique imaging capabilities
of Single-Photon Avalanche Diode (SPAD) cameras. We present the first
comprehensive evaluation of single-photon imaging as an alternative passive
sensing technology for robotic exploration missions targeting perceptually
challenging locations, with a special emphasis on high-latitude lunar regions.
We detail the operating principles and performance characteristics of SPAD
cameras, assess their advantages and limitations in addressing key perception
challenges of upcoming exploration missions to the Moon, and benchmark their
performance under representative illumination conditions.

</details>


### [483] [SpikeGrasp: A Benchmark for 6-DoF Grasp Pose Detection from Stereo Spike Streams](https://arxiv.org/abs/2510.10602)
*Zhuoheng Gao,Jiyao Zhang,Zhiyong Xie,Hao Dong,Zhaofei Yu,Rongmei Chen,Guozhang Chen,Tiejun Huang*

Main category: cs.RO

TL;DR: SpikeGrasp提出了一种受神经学启发的6自由度抓取检测框架，直接处理立体脉冲相机原始数据，通过循环脉冲神经网络推断抓取姿态，无需重建点云，在复杂场景下优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 大多数机器人抓取系统依赖于将传感器数据转换为显式3D点云，这在生物智能中不存在。本文旨在探索一种根本不同、受神经学启发的6自由度抓取检测范式。

Method: 引入SpikeGrasp框架，模仿生物视觉运动通路。它处理来自立体脉冲相机的原始、异步事件流，并融合这些流。使用循环脉冲神经网络（类似于高级视觉处理）迭代细化抓取假设，全程不重建点云。通过构建大规模合成基准数据集进行验证。

Result: 实验表明，SpikeGrasp超越了传统的基于点云的基线方法，尤其在杂乱和无纹理场景中表现更优，并展示了显著的数据效率。

Conclusion: 该研究证实了这种端到端、受神经学启发方法的T可行性，为未来能够实现像自然界中流畅高效操作（特别是对动态物体）的系统奠定了基础。

Abstract: Most robotic grasping systems rely on converting sensor data into explicit 3D
point clouds, which is a computational step not found in biological
intelligence. This paper explores a fundamentally different, neuro-inspired
paradigm for 6-DoF grasp detection. We introduce SpikeGrasp, a framework that
mimics the biological visuomotor pathway, processing raw, asynchronous events
from stereo spike cameras, similarly to retinas, to directly infer grasp poses.
Our model fuses these stereo spike streams and uses a recurrent spiking neural
network, analogous to high-level visual processing, to iteratively refine grasp
hypotheses without ever reconstructing a point cloud. To validate this
approach, we built a large-scale synthetic benchmark dataset. Experiments show
that SpikeGrasp surpasses traditional point-cloud-based baselines, especially
in cluttered and textureless scenes, and demonstrates remarkable data
efficiency. By establishing the viability of this end-to-end, neuro-inspired
approach, SpikeGrasp paves the way for future systems capable of the fluid and
efficient manipulation seen in nature, particularly for dynamic objects.

</details>


### [484] [Two-Layer Voronoi Coverage Control for Hybrid Aerial-Ground Robot Teams in Emergency Response: Implementation and Analysis](https://arxiv.org/abs/2510.10781)
*Douglas Hutchings,Luai Abuelsamen,Karthik Rajgopal*

Main category: cs.RO

TL;DR: 本文提出了一种双层Voronoi覆盖控制方法，用于协调混合式空中-地面机器人团队在危险品应急响应场景中的部署，显著缩短了响应时间。


<details>
  <summary>Details</summary>
Motivation: 传统的Voronoi覆盖控制方法在紧急情况下存在三个关键局限性：异构智能体能力差异大（速度不同）、初始部署配置聚集以及紧急时间限制（需要快速响应而非最终收敛）。

Method: 该方法采用解耦的双层架构，分别优化空中和地面机器人的定位。空中机器人通过空投将地面传感器投放到高优先级区域。技术细节包括有界Voronoi单元计算、重要性加权质心的有效数值积分技术以及防止智能体陷阱的鲁棒控制策略。

Result: 仿真结果表明，与仅地面部署的220秒相比，该方法将响应时间缩短了88%，在25秒内实现了目标传感器覆盖（初始传感器损失为18.5%）。

Conclusion: 所提出的双层Voronoi覆盖控制方法能有效应对危险品应急响应场景中的挑战，通过混合式空中-地面机器人团队显著提高了传感器覆盖效率和响应速度。

Abstract: We present a comprehensive two-layer Voronoi coverage control approach for
coordinating hybrid aerial-ground robot teams in hazardous material emergency
response scenarios. Traditional Voronoi coverage control methods face three
critical limitations in emergency contexts: heterogeneous agent capabilities
with vastly different velocities, clustered initial deployment configurations,
and urgent time constraints requiring rapid response rather than eventual
convergence. Our method addresses these challenges through a decoupled
two-layer architecture that separately optimizes aerial and ground robot
positioning, with aerial agents delivering ground sensors via airdrop to
high-priority locations. We provide detailed implementation of bounded Voronoi
cell computation, efficient numerical integration techniques for
importance-weighted centroids, and robust control strategies that prevent agent
trapping. Simulation results demonstrate an 88% reduction in response time,
achieving target sensor coverage (18.5% of initial sensor loss) in 25 seconds
compared to 220 seconds for ground-only deployment. Complete implementation
code is available at https://github.com/dHutchings/ME292B.

</details>


### [485] [High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting](https://arxiv.org/abs/2510.10637)
*Haoyu Zhao,Cheng Zeng,Linghao Zhuang,Yaxi Zhao,Shengke Xue,Hao Wang,Xingyue Zhao,Zhongyu Li,Kehan Li,Siteng Huang,Mingxiu Chen,Xin Li,Deli Zhao,Hua Zou*

Main category: cs.RO

TL;DR: RoboSimGS是一个Real2Sim2Real框架，它利用3D高斯泼溅和多模态大语言模型，将真实世界图像转换为高保真、物理交互的模拟环境，从而实现成功的机器人学习策略从模拟到真实的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 机器人学习的扩展性受限于真实世界数据收集的高成本和劳力。虽然模拟数据可扩展，但由于视觉外观、物理属性和物体交互方面的显著差异，其往往难以泛化到真实世界。

Method: RoboSimGS框架将多视角真实世界图像转换为模拟环境。它采用混合表示：使用3D高斯泼溅（3DGS）捕捉环境的真实外观，同时为交互对象使用网格基元以确保准确的物理模拟。关键创新是利用多模态大语言模型（MLLM）自动化创建物理上合理、可动关节的资产，MLLM通过分析视觉数据推断物体的物理属性（如密度、刚度）和复杂的运动学结构（如铰链、滑轨）。

Result: 完全在RoboSimGS生成的数据上训练的策略，在多样化的真实世界操作任务中实现了成功的零样本模拟到真实迁移。此外，来自RoboSimGS的数据显著提升了SOTA方法的性能和泛化能力。

Conclusion: RoboSimGS是一个强大且可扩展的解决方案，能够有效弥合模拟与真实之间的鸿沟。

Abstract: The scalability of robotic learning is fundamentally bottlenecked by the
significant cost and labor of real-world data collection. While simulated data
offers a scalable alternative, it often fails to generalize to the real world
due to significant gaps in visual appearance, physical properties, and object
interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real
framework that converts multi-view real-world images into scalable,
high-fidelity, and physically interactive simulation environments for robotic
manipulation. Our approach reconstructs scenes using a hybrid representation:
3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the
environment, while mesh primitives for interactive objects ensure accurate
physics simulation. Crucially, we pioneer the use of a Multi-modal Large
Language Model (MLLM) to automate the creation of physically plausible,
articulated assets. The MLLM analyzes visual data to infer not only physical
properties (e.g., density, stiffness) but also complex kinematic structures
(e.g., hinges, sliding rails) of objects. We demonstrate that policies trained
entirely on data generated by RoboSimGS achieve successful zero-shot
sim-to-real transfer across a diverse set of real-world manipulation tasks.
Furthermore, data from RoboSimGS significantly enhances the performance and
generalization capabilities of SOTA methods. Our results validate RoboSimGS as
a powerful and scalable solution for bridging the sim-to-real gap.

</details>


### [486] [UniCoD: Enhancing Robot Policy via Unified Continuous and Discrete Representation Learning](https://arxiv.org/abs/2510.10642)
*Jianke Zhang,Yucheng Hu,Yanjiang Guo,Xiaoyu Chen,Yichen Liu,Wenna Chen,Chaochao Lu,Jianyu Chen*

Main category: cs.RO

TL;DR: 本文提出UniCoD，一种利用统一生成和理解模型预训练（在百万级互联网教学视频上）和微调（在机器人数据上）的通用机器人策略，在模拟和真实世界任务中显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 构建能处理多样化任务的通用机器人策略是一个核心挑战。现有方法通常基于视觉-语言理解模型（VLM）或生成模型，但具身机器人需要语义理解和视觉动态建模。受统一生成和理解模型在理解和生成方面强大能力的启发，本文认为机器人策略学习也能从理解、规划和连续未来表示学习的结合中受益。

Method: 本文引入UniCoD模型。首先，UniCoD通过在超过100万个互联网规模的教学操作视频上进行预训练，获得动态建模高维视觉特征的能力。随后，UniCoD在从机器人实体收集的数据上进行微调，以学习从预测表示到动作令牌的映射。

Result: 广泛的实验表明，UniCoD方法在模拟环境中和真实世界的分布外（out-of-distribution）任务中，分别比基线方法提高了9%和12%的性能。

Conclusion: UniCoD通过结合统一生成和理解模型的优势，利用大规模预训练和机器人数据微调，显著提升了通用机器人策略在多样化任务中的表现，证明了其在具身智能领域的潜力。

Abstract: Building generalist robot policies that can handle diverse tasks in
open-ended environments is a central challenge in robotics. To leverage
knowledge from large-scale pretraining, prior work has typically built
generalist policies either on top of vision-language understanding models
(VLMs) or generative models. However, both semantic understanding from
vision-language pretraining and visual dynamics modeling from visual-generation
pretraining are crucial for embodied robots. Recent unified models of
generation and understanding have demonstrated strong capabilities in both
comprehension and generation through large-scale pretraining. We posit that
robotic policy learning can likewise benefit from the combined strengths of
understanding, planning and continuous future representation learning. Building
on this insight, we introduce UniCoD, which acquires the ability to dynamically
model high-dimensional visual features through pretraining on over 1M
internet-scale instructional manipulation videos. Subsequently, UniCoD is
fine-tuned on data collected from the robot embodiment, enabling the learning
of mappings from predictive representations to action tokens. Extensive
experiments show our approach consistently outperforms baseline methods in
terms of 9\% and 12\% across simulation environments and real-world
out-of-distribution tasks.

</details>


### [487] [Deployment and Development of a Cognitive Teleoreactive Framework for Deep Sea Autonomy](https://arxiv.org/abs/2510.10716)
*Christopher Thierauf*

Main category: cs.RO

TL;DR: 本文介绍了一种名为DINOS-R的新型AUV任务规划与执行软件，它在AUV Sentry上进行了测试，旨在统一符号决策、机器学习和反应行为，以提高海洋科考平台的现场适用性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是取代现有的MC架构，并从头开始构建一个能够统一符号决策（为了可理解、可重复、可验证的行为）与机器学习技术和反应行为的系统，以实现AUV在海洋科考平台上的现场就绪能力，并强调非专家用户的使用以及未来研究的扩展性。

Method: 开发了DINOS-R软件，主要使用Python3实现，灵感来源于认知架构和AUV控制系统。该系统具有可扩展、模块化和可重用性，支持声明式任务规范和灵活的行为规范（包括实时任务规划和硬编码用户指定计划的同步使用）。通过在AUV Sentry上的实地测试以及各种模拟案例进行了功能验证。

Result: DINOS-R的各项功能，包括灵活的任务和行为规范，以及符号决策、机器学习和反应行为的统一，已在Sentry AUV上进行了现场演示，并在多种模拟案例中得到了验证。

Conclusion: DINOS-R系统在AUV任务规划和执行方面表现出有效性，其统一多种决策范式的方法被证明可行，为未来的海洋学和机器人算法研究奠定了基础。

Abstract: A new AUV mission planning and execution software has been tested on AUV
Sentry. Dubbed DINOS-R, it draws inspiration from cognitive architectures and
AUV control systems to replace the legacy MC architecture. Unlike these
existing architectures, however, DINOS-R is built from the ground-up to unify
symbolic decision making (for understandable, repeatable, provable behavior)
with machine learning techniques and reactive behaviors, for field-readiness
across oceanographic platforms. Implemented primarily in Python3, DINOS-R is
extensible, modular, and reusable, with an emphasis on non-expert use as well
as growth for future research in oceanography and robot algorithms. Mission
specification is flexible, and can be specified declaratively. Behavior
specification is similarly flexible, supporting simultaneous use of real-time
task planning and hard-coded user specified plans. These features were
demonstrated in the field on Sentry, in addition to a variety of simulated
cases. These results are discussed, and future work is outlined.

</details>


### [488] [PhysHSI: Towards a Real-World Generalizable and Natural Humanoid-Scene Interaction System](https://arxiv.org/abs/2510.11072)
*Huayi Wang,Wentao Zhang,Runyi Yu,Tao Huang,Junli Ren,Feiyu Jia,Zirui Wang,Xiaojie Niu,Xiao Chen,Jiahe Chen,Qifeng Chen,Jingbo Wang,Jiangmiao Pang*

Main category: cs.RO

TL;DR: PhysHSI是一个物理世界中人形机器人与场景交互的系统，它使机器人能够自主执行多样化交互任务，同时保持自然、逼真的动作和鲁棒的场景感知。


<details>
  <summary>Details</summary>
Motivation: 将人形机器人部署到真实世界环境中（如搬运物体、坐下）需要可泛化的、逼真的动作和鲁棒的场景感知。尽管现有方法在单个能力上有所进展，但将其整合到一个统一系统中仍是一个挑战。

Method: 本文提出了PhysHSI系统，包含一个仿真训练管道和一个真实世界部署系统。在仿真中，采用基于对抗性运动先验的策略学习来模仿多样化场景中的自然人机交互数据，以实现泛化和逼真行为。在真实世界部署中，引入了一个结合激光雷达和摄像头输入的粗到细物体定位模块，以提供连续和鲁棒的场景感知。

Result: PhysHSI在四项代表性交互任务（搬箱、坐下、躺下、站起）上进行了仿真和真实世界验证，结果显示出持续高成功率、对多样化任务目标的强大泛化能力以及自然的运动模式。

Conclusion: PhysHSI成功地实现了人形机器人在真实世界中自主执行多样化交互任务，同时保持自然、逼真的动作和鲁棒的场景感知。

Abstract: Deploying humanoid robots to interact with real-world environments--such as
carrying objects or sitting on chairs--requires generalizable, lifelike motions
and robust scene perception. Although prior approaches have advanced each
capability individually, combining them in a unified system is still an ongoing
challenge. In this work, we present a physical-world humanoid-scene interaction
system, PhysHSI, that enables humanoids to autonomously perform diverse
interaction tasks while maintaining natural and lifelike behaviors. PhysHSI
comprises a simulation training pipeline and a real-world deployment system. In
simulation, we adopt adversarial motion prior-based policy learning to imitate
natural humanoid-scene interaction data across diverse scenarios, achieving
both generalization and lifelike behaviors. For real-world deployment, we
introduce a coarse-to-fine object localization module that combines LiDAR and
camera inputs to provide continuous and robust scene perception. We validate
PhysHSI on four representative interactive tasks--box carrying, sitting, lying,
and standing up--in both simulation and real-world settings, demonstrating
consistently high success rates, strong generalization across diverse task
goals, and natural motion patterns.

</details>


### [489] [A Faster and More Reliable Middleware for Autonomous Driving Systems](https://arxiv.org/abs/2510.11448)
*Yuankai He,Hanlin Chen,Weisong Shi*

Main category: cs.RO

TL;DR: 高速度自动驾驶车辆需要低延迟的感知到执行循环。ROS 2中间件会引入显著开销。本文提出Sensor-in-Memory (SIM)，一种为宿主内管道设计的共享内存传输机制，能大幅降低数据传输延迟，提高自动驾驶系统的性能和安全性。


<details>
  <summary>Details</summary>
Motivation: 高速度自动驾驶车辆需要快速的控制循环以及从感知到执行的严格限定延迟。现有的开源自动驾驶系统（如基于ROS 2）在多传感器和控制节点共享计算单元时，ROS 2及其DDS传输机制会引入大量的序列化、复制和发现开销，从而压缩可用时间预算。

Method: 本文提出Sensor-in-Memory (SIM)，一种专为自动驾驶车辆内部宿主管道设计的共享内存传输机制。SIM将传感器数据保留在原生内存布局中（如cv::Mat, PCL），使用无锁有界双缓冲区（优先覆盖旧数据以保证新鲜度），并能通过四行代码集成到ROS 2节点中。SIM与ROS 2并行运行，针对数据新鲜度和最小延迟而非完整性进行优化。它提供序列号、写入器心跳和可选校验和以确保数据排序、活跃性和基本完整性。

Result: 在NVIDIA Jetson Orin Nano上，SIM与ROS 2零拷贝传输（如FastRTPS和Zenoh）相比，数据传输延迟降低了高达98%，平均延迟降低了约95%，95th/99th百分位尾部延迟缩窄了约96%。在运行Autoware.Universe的L4级量产车辆测试中，SIM将定位频率从7.5 Hz提高到9.5 Hz。应用于所有延迟关键模块后，SIM将平均感知到决策延迟从521.91毫秒缩短到290.26毫秒，在40英里/小时（64公里/小时）的干燥混凝土路面上，紧急制动距离减少了13.6英尺（4.14米）。

Conclusion: SIM通过显著降低自动驾驶车辆感知到决策管道的延迟，从而实现更快的控制循环和更短的制动距离，极大地提升了高速度自动驾驶车辆的性能和安全性。

Abstract: Ensuring safety in high-speed autonomous vehicles requires rapid control
loops and tightly bounded delays from perception to actuation. Many open-source
autonomy systems rely on ROS 2 middleware; when multiple sensor and control
nodes share one compute unit, ROS 2 and its DDS transports add significant
(de)serialization, copying, and discovery overheads, shrinking the available
time budget. We present Sensor-in-Memory (SIM), a shared-memory transport
designed for intra-host pipelines in autonomous vehicles. SIM keeps sensor data
in native memory layouts (e.g., cv::Mat, PCL), uses lock-free bounded double
buffers that overwrite old data to prioritize freshness, and integrates into
ROS 2 nodes with four lines of code. Unlike traditional middleware, SIM
operates beside ROS 2 and is optimized for applications where data freshness
and minimal latency outweigh guaranteed completeness. SIM provides sequence
numbers, a writer heartbeat, and optional checksums to ensure ordering,
liveness, and basic integrity. On an NVIDIA Jetson Orin Nano, SIM reduces
data-transport latency by up to 98% compared to ROS 2 zero-copy transports such
as FastRTPS and Zenoh, lowers mean latency by about 95%, and narrows
95th/99th-percentile tail latencies by around 96%. In tests on a
production-ready Level 4 vehicle running Autoware.Universe, SIM increased
localization frequency from 7.5 Hz to 9.5 Hz. Applied across all
latency-critical modules, SIM cut average perception-to-decision latency from
521.91 ms to 290.26 ms, reducing emergency braking distance at 40 mph (64 km/h)
on dry concrete by 13.6 ft (4.14 m).

</details>


### [490] [Constraint-Aware Reinforcement Learning via Adaptive Action Scaling](https://arxiv.org/abs/2510.11491)
*Murad Dawood,Usama Ahmed Siddiquie,Shahram Khorshidi,Maren Bennewitz*

Main category: cs.RO

TL;DR: 本文提出了一种模块化的成本感知调节器，通过平滑调节动作而非直接覆盖策略来提高安全强化学习的性能，显著减少了约束违反并提升了任务回报。


<details>
  <summary>Details</summary>
Motivation: 现有的安全强化学习方法存在问题：单一策略优化奖励和安全目标时可能因冲突导致不稳定；外部安全过滤器则需要先验系统知识并可能直接覆盖动作。这些都限制了探索和性能。

Method: 本文提出了一种模块化的成本感知调节器。该调节器根据预测的约束违反来调整（缩放）智能体的动作，通过平滑调制动作来保留探索，而不是直接覆盖策略。调节器经过训练，旨在最小化约束违反，同时避免过度抑制动作。它可以无缝集成到SAC和TD3等离策略RL方法中。

Result: 该方法在Safety Gym运动任务中，针对稀疏成本实现了最先进的收益-成本比。与现有方法相比，它将约束违反减少了多达126倍，同时将回报提高了超过一个数量级。

Conclusion: 所提出的模块化成本感知调节器在安全强化学习中表现出色，能够有效减少约束违反并提高任务性能，为解决现有安全RL方法的局限性提供了一种鲁棒且高效的解决方案。

Abstract: Safe reinforcement learning (RL) seeks to mitigate unsafe behaviors that
arise from exploration during training by reducing constraint violations while
maintaining task performance. Existing approaches typically rely on a single
policy to jointly optimize reward and safety, which can cause instability due
to conflicting objectives, or they use external safety filters that override
actions and require prior system knowledge. In this paper, we propose a modular
cost-aware regulator that scales the agent's actions based on predicted
constraint violations, preserving exploration through smooth action modulation
rather than overriding the policy. The regulator is trained to minimize
constraint violations while avoiding degenerate suppression of actions. Our
approach integrates seamlessly with off-policy RL methods such as SAC and TD3,
and achieves state-of-the-art return-to-cost ratios on Safety Gym locomotion
tasks with sparse costs, reducing constraint violations by up to 126 times
while increasing returns by over an order of magnitude compared to prior
methods.

</details>


### [491] [Controllable Generative Trajectory Prediction via Weak Preference Alignment](https://arxiv.org/abs/2510.10731)
*Yongxi Cao,Julian F. Schumann,Jens Kober,Joni Pajarinen,Arkady Zgonnikov*

Main category: cs.RO

TL;DR: 本文提出PrefCVAE，一个增强型条件变分自编码器框架，利用弱标签偏好对赋予潜在变量语义属性，以实现可控、语义丰富的自动驾驶轨迹预测，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶轨迹预测模型需要高准确性和多样性，因为人类行为具有不确定性和多模态性。然而，现有方法通常缺乏生成可控多样化轨迹的机制，而可控多样性对于安全规划比随机多样性更有用。

Method: 作者提出了PrefCVAE，一个增强的CVAE框架。它通过使用弱标签的偏好对来为潜在变量注入语义属性。文中以平均速度作为示例属性进行演示。

Result: 研究表明，PrefCVAE能够实现可控、语义有意义的预测（例如，基于平均速度），并且不会降低基线模型的准确性。结果证明了偏好监督作为一种经济有效的方式来增强基于采样的生成模型的有效性。

Conclusion: 偏好监督是一种经济有效的方法，可以增强基于采样的生成模型，使其能够生成可控且语义有意义的轨迹，这对于自动驾驶等需要安全规划的应用至关重要。

Abstract: Deep generative models such as conditional variational autoencoders (CVAEs)
have shown great promise for predicting trajectories of surrounding agents in
autonomous vehicle planning. State-of-the-art models have achieved remarkable
accuracy in such prediction tasks. Besides accuracy, diversity is also crucial
for safe planning because human behaviors are inherently uncertain and
multimodal. However, existing methods generally lack a scheme to generate
controllably diverse trajectories, which is arguably more useful than randomly
diversified trajectories, to the end of safe planning. To address this, we
propose PrefCVAE, an augmented CVAE framework that uses weakly labeled
preference pairs to imbue latent variables with semantic attributes. Using
average velocity as an example attribute, we demonstrate that PrefCVAE enables
controllable, semantically meaningful predictions without degrading baseline
accuracy. Our results show the effectiveness of preference supervision as a
cost-effective way to enhance sampling-based generative models.

</details>


### [492] [Gain Tuning Is Not What You Need: Reward Gain Adaptation for Constrained Locomotion Learning](https://arxiv.org/abs/2510.10759)
*Arthicha Srisuchinnawong,Poramate Manoonpong*

Main category: cs.RO

TL;DR: 本文提出ROGER方法，通过在线自适应奖励权重增益来解决机器人运动学习中离线增益选择困难和训练期间难以保证约束满足的问题，实现了近零约束违反和更高的性能，并成功应用于真实四足机器人。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人运动学习技术严重依赖离线选择合适的奖励权重增益，并且无法在训练期间保证约束满足（即存在约束违反）。

Method: 本文提出ROGER（Reward-Oriented Gains via Embodied Regulation）方法，它根据在具身交互过程中收到的惩罚在线调整奖励权重增益。当学习接近约束阈值时，正奖励（主要奖励）和负奖励（惩罚）增益之间的比例会自动降低以避免违反；反之，当学习处于安全状态时，该比例会增加以优先考虑性能。

Result: 在60公斤四足机器人上，ROGER在多次学习试验中实现了接近零的约束违反，并比同类最先进技术获得了高达50%的额外主要奖励。在MuJoCo连续运动基准测试（包括单腿跳跃器）中，ROGER表现出与使用默认奖励函数训练的方法相当或高达100%的更高性能，以及60%更少的扭矩使用和方向偏差。最后，一个物理四足机器人实现了从零开始的真实世界运动学习，在一小时内没有摔倒。

Conclusion: 这项工作为满足约束的真实世界持续机器人运动学习做出了贡献，并简化了奖励权重增益的调整，有望促进物理机器人和在真实世界中学习的机器人的发展。

Abstract: Existing robot locomotion learning techniques rely heavily on the offline
selection of proper reward weighting gains and cannot guarantee constraint
satisfaction (i.e., constraint violation) during training. Thus, this work aims
to address both issues by proposing Reward-Oriented Gains via Embodied
Regulation (ROGER), which adapts reward-weighting gains online based on
penalties received throughout the embodied interaction process. The ratio
between the positive reward (primary reward) and negative reward (penalty)
gains is automatically reduced as the learning approaches the constraint
thresholds to avoid violation. Conversely, the ratio is increased when learning
is in safe states to prioritize performance. With a 60-kg quadruped robot,
ROGER achieved near-zero constraint violation throughout multiple learning
trials. It also achieved up to 50% more primary reward than the equivalent
state-of-the-art techniques. In MuJoCo continuous locomotion benchmarks,
including a single-leg hopper, ROGER exhibited comparable or up to 100% higher
performance and 60% less torque usage and orientation deviation compared to
those trained with the default reward function. Finally, real-world locomotion
learning of a physical quadruped robot was achieved from scratch within one
hour without any falls. Therefore, this work contributes to
constraint-satisfying real-world continual robot locomotion learning and
simplifies reward weighting gain tuning, potentially facilitating the
development of physical robots and those that learn in the real world.

</details>


### [493] [IntersectioNDE: Learning Complex Urban Traffic Dynamics based on Interaction Decoupling Strategy](https://arxiv.org/abs/2510.11534)
*Enli Lin,Ziyuan Yang,Qiujing Lu,Jianming Hu,Shuo Feng*

Main category: cs.RO

TL;DR: 该研究引入了CiCross数据集和IntersectioNDE模拟器，旨在解决现有模拟器在模拟城市交叉口密集、异构交通流方面的挑战，通过交互解耦策略（IDS）提高了模拟的鲁棒性和长期稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动模拟器难以建模城市交叉口（尤其在中国等国家普遍存在）密集、异构的交通互动（包括机动车、非机动车和行人），且难以鲁棒地学习高维联合分布，常导致模式崩溃和长期模拟不稳定。

Method: 研究提出了City Crossings Dataset (CiCross)，一个从真实城市交叉口收集的大规模数据集，捕捉了密集的异构多智能体互动。在此基础上，提出了IntersectioNDE，一个针对复杂城市交叉口场景的数据驱动模拟器。其核心是交互解耦策略（IDS），一种通过学习智能体子集的组合动态来实现从边际到联合模拟的训练范式。IDS被集成到场景感知Transformer网络中，并结合专门的训练技术。

Result: IntersectioNDE结合IDS显著增强了异构交互模拟的鲁棒性和长期稳定性。在CiCross数据集上的实验表明，IntersectioNDE在模拟保真度、稳定性以及复制复杂、分布级城市交通动态的能力方面优于基线方法。

Conclusion: 该研究通过引入CiCross数据集和基于交互解耦策略的IntersectioNDE模拟器，成功解决了城市交叉口密集、异构交通流模拟的挑战，实现了更逼真、更稳定的模拟效果，对自动驾驶车辆的安全性和可靠性至关重要。

Abstract: Realistic traffic simulation is critical for ensuring the safety and
reliability of autonomous vehicles (AVs), especially in complex and diverse
urban traffic environments. However, existing data-driven simulators face two
key challenges: a limited focus on modeling dense, heterogeneous interactions
at urban intersections - which are prevalent, crucial, and practically
significant in countries like China, featuring diverse agents including
motorized vehicles (MVs), non-motorized vehicles (NMVs), and pedestrians - and
the inherent difficulty in robustly learning high-dimensional joint
distributions for such high-density scenes, often leading to mode collapse and
long-term simulation instability. We introduce City Crossings Dataset
(CiCross), a large-scale dataset collected from a real-world urban
intersection, uniquely capturing dense, heterogeneous multi-agent interactions,
particularly with a substantial proportion of MVs, NMVs and pedestrians. Based
on this dataset, we propose IntersectioNDE (Intersection Naturalistic Driving
Environment), a data-driven simulator tailored for complex urban intersection
scenarios. Its core component is the Interaction Decoupling Strategy (IDS), a
training paradigm that learns compositional dynamics from agent subsets,
enabling the marginal-to-joint simulation. Integrated into a scene-aware
Transformer network with specialized training techniques, IDS significantly
enhances simulation robustness and long-term stability for modeling
heterogeneous interactions. Experiments on CiCross show that IntersectioNDE
outperforms baseline methods in simulation fidelity, stability, and its ability
to replicate complex, distribution-level urban traffic dynamics.

</details>


### [494] [Ego-Vision World Model for Humanoid Contact Planning](https://arxiv.org/abs/2510.11682)
*Hang Liu,Yuman Gao,Sangli Teng,Yufeng Chi,Yakun Sophia Shao,Zhongyu Li,Maani Ghaffari,Koushil Sreenath*

Main category: cs.RO

TL;DR: 该研究提出了一种结合学习型世界模型和基于采样的模型预测控制（MPC）的框架，使人形机器人能够进行接触感知规划。该系统在离线数据集上训练，并利用学习型替代价值函数进行鲁棒规划，实现了高效且多任务的接触利用，并已成功部署在物理人形机器人上。


<details>
  <summary>Details</summary>
Motivation: 为了在非结构化环境中实现自主性，人形机器人需要能够利用物理接触而非仅仅避免碰撞。传统的基于优化的规划器难以处理接触的复杂性，而在线强化学习（RL）样本效率低下且多任务能力有限。

Method: 该框架结合了学习型世界模型和基于采样的模型预测控制（MPC）。世界模型在无演示的离线数据集上训练，用于在压缩的潜在空间中预测未来结果。为解决稀疏接触奖励和传感器噪声问题，MPC使用一个学习型替代价值函数进行密集、鲁棒的规划。该模型是一个单一、可扩展的模型。

Result: 该单一、可扩展的模型支持多种接触感知任务，包括受扰动后的墙壁支撑、阻挡来袭物体以及穿越限高拱门。与在线强化学习相比，该方法提高了数据效率和多任务能力。部署在物理人形机器人上时，该系统能够从本体感受和自我中心深度图像实现鲁棒、实时的接触规划。

Conclusion: 该研究提出的结合学习型世界模型和基于采样的MPC框架，通过利用学习型替代价值函数，成功使人形机器人能够进行鲁棒、实时、数据高效且多任务的接触感知规划，从而增强了机器人在非结构化环境中的自主性。

Abstract: Enabling humanoid robots to exploit physical contact, rather than simply
avoid collisions, is crucial for autonomy in unstructured environments.
Traditional optimization-based planners struggle with contact complexity, while
on-policy reinforcement learning (RL) is sample-inefficient and has limited
multi-task ability. We propose a framework combining a learned world model with
sampling-based Model Predictive Control (MPC), trained on a demonstration-free
offline dataset to predict future outcomes in a compressed latent space. To
address sparse contact rewards and sensor noise, the MPC uses a learned
surrogate value function for dense, robust planning. Our single, scalable model
supports contact-aware tasks, including wall support after perturbation,
blocking incoming objects, and traversing height-limited arches, with improved
data efficiency and multi-task capability over on-policy RL. Deployed on a
physical humanoid, our system achieves robust, real-time contact planning from
proprioception and ego-centric depth images. Website:
https://ego-vcp.github.io/

</details>


### [495] [Real2USD: Scene Representations in Universal Scene Description Language](https://arxiv.org/abs/2510.10778)
*Christopher D. Hsu,Pratik Chaudhari*

Main category: cs.RO

TL;DR: 本文提出通用场景描述（USD）作为大型语言模型（LLM）驱动机器人任务的通用环境表示，并展示了一个“Real to USD”系统，该系统能从真实世界构建USD场景图，并利用LLM进行场景理解、推理和规划。


<details>
  <summary>Details</summary>
Motivation: 现有增强机器人环境表示以支持LLM推理的方法通常是针对特定任务的，缺乏一种通用且能被LLM理解的环境表示，以帮助机器人理解抽象任务规范。

Method: 本文提出使用通用场景描述（USD）作为几何、光度学和语义信息的通用表示，因为它是一个XML-based场景图，对LLM和人类都可读且足够丰富。研究团队开发了一个“Real to USD”系统，使用Unitree Go2四足机器人搭载LiDAR和RGB相机来构建室内环境的USD表示，并利用Google Gemini解析USD以实现场景理解、复杂推理和规划。此外，还在Nvidia Isaac Sim的模拟仓库和医院环境中研究了该系统的不同方面。

Result: “Real to USD”系统成功构建了包含多样物体和挑战性环境（如大量玻璃）的室内环境的明确USD表示。Google Gemini能够成功解析USD，并展示了场景理解、复杂推理和规划能力。该系统在模拟环境中也得到了验证。

Conclusion: USD是一种有效且通用的环境表示，适用于基于LLM的机器人任务，能够支持场景理解、复杂推理和规划，并在真实和模拟环境中均表现出潜力。

Abstract: Large Language Models (LLMs) can help robots reason about abstract task
specifications. This requires augmenting classical representations of the
environment used by robots with natural language-based priors. There are a
number of existing approaches to doing so, but they are tailored to specific
tasks, e.g., visual-language models for navigation, language-guided neural
radiance fields for mapping, etc. This paper argues that the Universal Scene
Description (USD) language is an effective and general representation of
geometric, photometric and semantic information in the environment for
LLM-based robotics tasks. Our argument is simple: a USD is an XML-based scene
graph, readable by LLMs and humans alike, and rich enough to support
essentially any task -- Pixar developed this language to store assets, scenes
and even movies. We demonstrate a ``Real to USD'' system using a Unitree Go2
quadruped robot carrying LiDAR and a RGB camera that (i) builds an explicit USD
representation of indoor environments with diverse objects and challenging
settings with lots of glass, and (ii) parses the USD using Google's Gemini to
demonstrate scene understanding, complex inferences, and planning. We also
study different aspects of this system in simulated warehouse and hospital
settings using Nvidia's Issac Sim. Code is available at
https://github.com/grasp-lyrl/Real2USD .

</details>


### [496] [Representing Data in Robotic Tactile Perception -- A Review](https://arxiv.org/abs/2510.10804)
*Alessandro Albini,Mohsen Kaboli,Giorgio Cannata,Perla Maiolino*

Main category: cs.RO

TL;DR: 这篇综述论文专注于机器人触觉感知管线中的数据表示问题，识别了六种常用的数据结构，并提供了根据硬件、所需信息和任务选择合适表示的指导。


<details>
  <summary>Details</summary>
Motivation: 机器人触觉感知涉及多级计算，高层计算常借鉴计算机视觉等领域，需要结构化的输入数据。因此，触觉传感器数据转换成特定结构是必要的，但这直接影响编码的触觉信息和任务执行。

Method: 该研究通过综述现有文献，明确了其对感知管线的贡献，审查了以往研究如何处理触觉信息表示问题，并调查了硬件、表示方法和高层计算方法之间的关系。

Result: 分析识别出文献中常用的六种触觉数据表示结构。论文还提供了讨论和指导，以便根据操作条件（包括可用硬件、需要编码的触觉信息和当前任务）正确选择数据表示。

Conclusion: 触觉数据表示的选择对机器人触觉感知至关重要，应根据具体硬件、所需编码的触觉信息以及所执行的任务进行适当选择。

Abstract: Robotic tactile perception is a complex process involving several
computational steps performed at different levels. Tactile information is
shaped by the interplay of robot actions, the mechanical properties of its
body, and the software that processes the data. In this respect, high-level
computation, required to process and extract information, is commonly performed
by adapting existing techniques from other domains, such as computer vision,
which expects input data to be properly structured. Therefore, it is necessary
to transform tactile sensor data to match a specific data structure. This
operation directly affects the tactile information encoded and, as a
consequence, the task execution. This survey aims to address this specific
aspect of the tactile perception pipeline, namely Data Representation. The
paper first clearly defines its contributions to the perception pipeline and
then reviews how previous studies have dealt with the problem of representing
tactile information, investigating the relationships among hardware,
representations, and high-level computation methods. The analysis has led to
the identification of six structures commonly used in the literature to
represent data. The manuscript provides discussions and guidelines for properly
selecting a representation depending on operating conditions, including the
available hardware, the tactile information required to be encoded, and the
task at hand.

</details>


### [497] [Preference-Conditioned Multi-Objective RL for Integrated Command Tracking and Force Compliance in Humanoid Locomotion](https://arxiv.org/abs/2510.10851)
*Tingxuan Leng,Yushi Wang,Tinglong Zheng,Changsheng Luo,Mingguo Zhao*

Main category: cs.RO

TL;DR: 本文提出了一种偏好条件多目标强化学习框架，以平衡仿人机器人运动中的指令跟踪和外部力顺从性，实现了可部署的自适应运动。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要侧重于仿人机器人运动的鲁棒性，但缺乏对外部力的顺从性，这对于不稳定的仿人机器人来说是一个挑战。

Method: 将仿人机器人运动建模为平衡指令跟踪和外部力顺从性的多目标优化问题。引入了偏好条件多目标强化学习(MORL)框架，将刚性指令跟踪和顺从行为集成到单一全向运动策略中。通过速度阻力因子建模外部力以进行奖励设计，并利用编码器-解码器结构从可部署观测中推断任务相关的特权特征。

Result: 实验结果表明，该框架不仅提高了自适应性和收敛性，而且实现了可部署的偏好条件仿人机器人运动。

Conclusion: 该框架成功解决了仿人机器人运动中指令跟踪与外部力顺从性之间的平衡问题，实现了更具适应性和可部署的机器人运动策略。

Abstract: Humanoid locomotion requires not only accurate command tracking for
navigation but also compliant responses to external forces during human
interaction. Despite significant progress, existing RL approaches mainly
emphasize robustness, yielding policies that resist external forces but lack
compliance-particularly challenging for inherently unstable humanoids. In this
work, we address this by formulating humanoid locomotion as a multi-objective
optimization problem that balances command tracking and external force
compliance. We introduce a preference-conditioned multi-objective RL (MORL)
framework that integrates rigid command following and compliant behaviors
within a single omnidirectional locomotion policy. External forces are modeled
via velocity-resistance factor for consistent reward design, and training
leverages an encoder-decoder structure that infers task-relevant privileged
features from deployable observations. We validate our approach in both
simulation and real-world experiments on a humanoid robot. Experimental results
indicate that our framework not only improves adaptability and convergence over
standard pipelines, but also realizes deployable preference-conditioned
humanoid locomotion.

</details>


### [498] [Contact Sensing via Joint Torque Sensors and a Force/Torque Sensor for Legged Robots](https://arxiv.org/abs/2510.10843)
*Jared Grinberg,Yanran Ding*

Main category: cs.RO

TL;DR: 本文提出了一种利用分布式关节扭矩传感器和单个髋部力-扭矩传感器，基于广义动量观测器框架，检测和定位机器人腿部接触的方法，并设计了一种高精度的低成本应变计关节扭矩传感器。


<details>
  <summary>Details</summary>
Motivation: 现有方法可能需要复杂的摩擦模型或通过电机电流估计扭矩，导致精度不足。本研究旨在通过直接扭矩测量，消除对复杂摩擦模型的需求，并提供比基于电机电流估计更准确的扭矩读数，以实现精确的接触检测和定位。

Method: 研究采用广义动量观测器框架进行接触检测和定位。核心方法包括：1) 设计了一种低成本的基于应变计的关节扭矩传感器，可直接测量扭矩；2) 结合使用分布式关节扭矩传感器和单个髋部力-扭矩传感器；3) 在浮动基座2自由度机器人腿上进行仿真研究；4) 对扭矩传感器进行校准；5) 在2自由度机械臂上进行硬件实验验证。

Result: 仿真研究验证了所提出的框架能准确恢复大腿和小腿连杆上的接触力和位置。扭矩传感器经过校准后，相对于真实值测量，平均精度达到96.4%。硬件实验结果显示，接触定位精度达到亚厘米级别，力误差低于0.2 N。

Conclusion: 所提出的基于广义动量观测器框架的接触检测和定位方法，结合自主设计的低成本高精度关节扭矩传感器，能够有效且准确地检测和定位机器人腿部的接触，在仿真和硬件实验中均表现出优异的性能。

Abstract: This paper presents a method for detecting and localizing contact along robot
legs using distributed joint torque sensors and a single hip-mounted
force-torque (FT) sensor using a generalized momentum-based observer framework.
We designed a low-cost strain-gauge-based joint torque sensor that can be
installed on every joint to provide direct torque measurements, eliminating the
need for complex friction models and providing more accurate torque readings
than estimation based on motor current. Simulation studies on a floating-based
2-DoF robot leg verified that the proposed framework accurately recovers
contact force and location along the thigh and shin links. Through a
calibration procedure, our torque sensor achieved an average 96.4% accuracy
relative to ground truth measurements. Building upon the torque sensor, we
performed hardware experiments on a 2-DoF manipulator, which showed
sub-centimeter contact localization accuracy and force errors below 0.2 N.

</details>


### [499] [GRIP: A Unified Framework for Grid-Based Relay and Co-Occurrence-Aware Planning in Dynamic Environments](https://arxiv.org/abs/2510.10865)
*Ahmed Alanazi,Duy Ho,Yugyung Lee*

Main category: cs.RO

TL;DR: GRIP是一个统一的、模块化的框架，通过整合感知、符号推理和空间规划，使机器人在动态、复杂环境中进行泛化导航，并在模拟和现实世界中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态、杂乱和语义复杂的环境中导航时，常依赖静态先验或有限记忆，导致在部分可观测性和语义模糊性下适应性受限。

Method: GRIP框架包含三个可扩展变体（GRIP-L、GRIP-F、GRIP-R）。它整合了动态2D网格构建、开放词汇对象定位、共现感知符号规划，以及结合行为克隆、D*搜索和网格条件控制的混合策略执行。GRIP-F还支持多跳锚链和基于LLM的内省。

Result: 在AI2-THOR和RoboTHOR基准测试中，GRIP在长距离任务上成功率提高了9.6%，路径效率（SPL和SAE）提高了2倍以上。定性分析揭示了在模糊场景中可解释的符号规划。在Jetbot上的真实世界部署进一步验证了GRIP在传感器噪声和环境变化下的泛化能力。

Conclusion: GRIP被定位为一个鲁棒、可扩展且可解释的框架，成功弥合了模拟和现实世界导航之间的鸿沟。

Abstract: Robots navigating dynamic, cluttered, and semantically complex environments
must integrate perception, symbolic reasoning, and spatial planning to
generalize across diverse layouts and object categories. Existing methods often
rely on static priors or limited memory, constraining adaptability under
partial observability and semantic ambiguity. We present GRIP, Grid-based Relay
with Intermediate Planning, a unified, modular framework with three scalable
variants: GRIP-L (Lightweight), optimized for symbolic navigation via semantic
occupancy grids; GRIP-F (Full), supporting multi-hop anchor chaining and
LLM-based introspection; and GRIP-R (Real-World), enabling physical robot
deployment under perceptual uncertainty. GRIP integrates dynamic 2D grid
construction, open-vocabulary object grounding, co-occurrence-aware symbolic
planning, and hybrid policy execution using behavioral cloning, D* search, and
grid-conditioned control. Empirical results on AI2-THOR and RoboTHOR benchmarks
show that GRIP achieves up to 9.6% higher success rates and over $2\times$
improvement in path efficiency (SPL and SAE) on long-horizon tasks. Qualitative
analyses reveal interpretable symbolic plans in ambiguous scenes. Real-world
deployment on a Jetbot further validates GRIP's generalization under sensor
noise and environmental variation. These results position GRIP as a robust,
scalable, and explainable framework bridging simulation and real-world
navigation.

</details>


### [500] [QuayPoints: A Reasoning Framework to Bridge the Information Gap Between Global and Local Planning in Autonomous Racing](https://arxiv.org/abs/2510.10886)
*Yashom Dighe,Youngjin Kim,Karthik Dantu*

Main category: cs.RO

TL;DR: 为解决自动驾驶赛车中全局规划信息丢失问题，本文提出“QuayPoints”框架，将时间最优性信息传递给局部规划器，从而显著提升了赛车在超车等场景下的决策能力。


<details>
  <summary>Details</summary>
Motivation: 标准的自动驾驶赛车管线（全局规划、局部规划、控制）在信息向下游传递时会丢失高层赛车上下文，特别是全局规划器对最优性的理解常被简化为稀疏路点，导致局部规划器在有限上下文中做出反应性决策，影响整体最优性，尤其是在超车等复杂场景。

Method: 本文引入了一个框架，通过“QuayPoints”区域将关键的全局知识（特别是时间最优性信息）传递给局部规划器。QuayPoints标识出偏离最优赛道线会显著损害最优性的区域，使局部规划器在偏离赛道线时（如策略性超车）能做出更明智的全局决策。

Result: 将QuayPoints集成到现有规划器中，结果表明，在四个不同的赛道上，该系统能稳定超越速度达到自车75%的对手。

Conclusion: QuayPoints框架成功地将全局时间最优性洞察传递给局部规划器，使其在偏离赛道线时能做出更明智的全局决策，显著提升了自动驾驶赛车在战略性超车等场景下的性能。

Abstract: Autonomous racing requires tight integration between perception, planning and
control to minimize latency as well as timely decision making. A standard
autonomy pipeline comprising a global planner, local planner, and controller
loses information as the higher-level racing context is sequentially propagated
downstream into specific task-oriented context. In particular, the global
planner's understanding of optimality is typically reduced to a sparse set of
waypoints, leaving the local planner to make reactive decisions with limited
context. This paper investigates whether additional global insights,
specifically time-optimality information, can be meaningfully passed to the
local planner to improve downstream decisions. We introduce a framework that
preserves essential global knowledge and conveys it to the local planner
through QuayPoints regions where deviations from the optimal raceline result in
significant compromises to optimality. QuayPoints enable local planners to make
more informed global decisions when deviating from the raceline, such as during
strategic overtaking. To demonstrate this, we integrate QuayPoints into an
existing planner and show that it consistently overtakes opponents traveling at
up to 75% of the ego vehicle's speed across four distinct race tracks.

</details>


### [501] [An Adaptive Transition Framework for Game-Theoretic Based Takeover](https://arxiv.org/abs/2510.10893)
*Dikshant Shehmar,Matthew E. Taylor,Ehsan Hashemi*

Main category: cs.RO

TL;DR: 本文提出一种自适应控制过渡策略，根据驾驶员的实时表现动态调整控制权限，以解决自动驾驶系统中固定时间过渡的不足，从而减少轨迹偏差和驾驶员控制负荷。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统中的控制权移交策略基于固定时间，未能考虑驾驶员实时表现的变化，导致驾驶员脱离回路（OOTL）状态下准备不足和反应时间增加。

Method: 该研究提出一种自适应过渡策略，根据驾驶员的时间和轨迹跟踪能力动态调整控制权限。共享控制被建模为合作微分博弈，通过时变目标函数而非直接混合控制扭矩来调节控制权限。为确保更自然的接管，引入了驾驶员特定的状态跟踪矩阵。通过累积轨迹误差指标和人机协作（human-in-the-loop）的ISO变道场景进行评估。

Result: 与传统策略相比，自适应过渡策略显著减少了轨迹偏差和驾驶员的控制负荷。实验还证实，根据实时偏差连续调整控制权限可以增强车辆稳定性，并降低驾驶员在接管过程中的努力。

Conclusion: 所提出的自适应过渡策略通过动态调整控制权限，有效地改善了自动驾驶系统中的控制权移交过程，提高了车辆稳定性并减轻了驾驶员负担，从而实现更安全、更自然的过渡。

Abstract: The transition of control from autonomous systems to human drivers is
critical in automated driving systems, particularly due to the out-of-the-loop
(OOTL) circumstances that reduce driver readiness and increase reaction times.
Existing takeover strategies are based on fixed time-based transitions, which
fail to account for real-time driver performance variations. This paper
proposes an adaptive transition strategy that dynamically adjusts the control
authority based on both the time and tracking ability of the driver trajectory.
Shared control is modeled as a cooperative differential game, where control
authority is modulated through time-varying objective functions instead of
blending control torques directly. To ensure a more natural takeover, a
driver-specific state-tracking matrix is introduced, allowing the transition to
align with individual control preferences. Multiple transition strategies are
evaluated using a cumulative trajectory error metric. Human-in-the-loop control
scenarios of the standardized ISO lane change maneuvers demonstrate that
adaptive transitions reduce trajectory deviations and driver control effort
compared to conventional strategies. Experiments also confirm that continuously
adjusting control authority based on real-time deviations enhances vehicle
stability while reducing driver effort during takeover.

</details>


### [502] [Towards a Unified Understanding of Robot Manipulation: A Comprehensive Survey](https://arxiv.org/abs/2510.10903)
*Shuanghao Bai,Wenxuan Song,Jiayi Chen,Yuheng Ji,Zhide Zhong,Jin Yang,Han Zhao,Wanqi Zhou,Wei Zhao,Zhe Li,Pengxiang Ding,Cheng Chi,Haoang Li,Chang Xu,Xiaolong Zheng,Donglin Wang,Shanghang Zhang,Badong Chen*

Main category: cs.RO

TL;DR: 该综述全面概述了机器人操作领域，涵盖了基础背景、基准、方法、瓶颈和实际应用，并提出了新的高级规划和低级控制分类法。


<details>
  <summary>Details</summary>
Motivation: 具身智能的进步以及计算机视觉、自然语言处理和大规模多模态模型的发展，使得机器人操作成为一个核心且复杂的挑战，需要感知、规划和控制的无缝集成以适应多样化环境。

Method: 该研究通过全面的综述方法，提供了机器人操作的基础背景、任务导向的基准和数据集，以及现有方法的统一分类。它扩展了高级规划（包括语言、代码、运动、功能、3D表示）并引入了基于训练范式（如输入建模、潜在学习、策略学习）的低级学习控制新分类。此外，它还首次提出了关键瓶颈（数据收集、利用、泛化）的专用分类，并回顾了实际应用。

Result: 本综述提供了比以往调查更广泛的范围和更深入的见解，为新手提供了易于理解的路线图，为经验丰富的研究人员提供了结构化参考。所有相关资源（论文、开源数据集、项目）均已整理并公开。

Conclusion: 该综述全面回顾了机器人操作，提出了新的分类法和关键瓶颈分析，旨在为研究人员提供一个结构化且深入的指南，以促进该领域的发展和应用。

Abstract: Embodied intelligence has witnessed remarkable progress in recent years,
driven by advances in computer vision, natural language processing, and the
rise of large-scale multimodal models. Among its core challenges, robot
manipulation stands out as a fundamental yet intricate problem, requiring the
seamless integration of perception, planning, and control to enable interaction
within diverse and unstructured environments. This survey presents a
comprehensive overview of robotic manipulation, encompassing foundational
background, task-organized benchmarks and datasets, and a unified taxonomy of
existing methods. We extend the classical division between high-level planning
and low-level control by broadening high-level planning to include language,
code, motion, affordance, and 3D representations, while introducing a new
taxonomy of low-level learning-based control grounded in training paradigms
such as input modeling, latent learning, and policy learning. Furthermore, we
provide the first dedicated taxonomy of key bottlenecks, focusing on data
collection, utilization, and generalization, and conclude with an extensive
review of real-world applications. Compared with prior surveys, our work offers
both a broader scope and deeper insight, serving as an accessible roadmap for
newcomers and a structured reference for experienced researchers. All related
resources, including research papers, open-source datasets, and projects, are
curated for the community at
https://github.com/BaiShuanghao/Awesome-Robotics-Manipulation.

</details>


### [503] [More than A Point: Capturing Uncertainty with Adaptive Affordance Heatmaps for Spatial Grounding in Robotic Tasks](https://arxiv.org/abs/2510.10912)
*Xinyu Shao,Yanzhe Tang,Pengwei Xie,Kaiwen Zhou,Yuzheng Zhuang,Xingyue Quan,Jianye Hao,Long Zeng,Xiu Li*

Main category: cs.RO

TL;DR: RoboMAP框架通过连续、自适应的可用性热图表示空间目标，解决了语言引导机器人系统中离散空间推理的脆弱性问题，显著提高了任务成功率、速度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 许多语言引导的机器人系统将空间推理简化为离散点，这使得它们在面对感知噪声和语义模糊时表现脆弱。

Method: RoboMAP框架将空间目标表示为连续的、自适应的可用性热图。这种密集表示捕捉了空间定位的不确定性，并为下游策略提供了更丰富的信息。

Result: RoboMAP在大多数定位基准测试上超越了现有最先进技术，速度提升高达50倍，并在真实世界操作中实现了82%的成功率。它在广泛的模拟和物理实验中表现出强大的鲁棒性，并展现出对导航任务的强大零样本泛化能力。

Conclusion: RoboMAP通过采用连续的可用性热图，有效解决了语言引导机器人系统中离散空间推理的局限性，从而显著增强了任务成功率、可解释性、速度和泛化能力。

Abstract: Many language-guided robotic systems rely on collapsing spatial reasoning
into discrete points, making them brittle to perceptual noise and semantic
ambiguity. To address this challenge, we propose RoboMAP, a framework that
represents spatial targets as continuous, adaptive affordance heatmaps. This
dense representation captures the uncertainty in spatial grounding and provides
richer information for downstream policies, thereby significantly enhancing
task success and interpretability. RoboMAP surpasses the previous
state-of-the-art on a majority of grounding benchmarks with up to a 50x speed
improvement, and achieves an 82\% success rate in real-world manipulation.
Across extensive simulated and physical experiments, it demonstrates robust
performance and shows strong zero-shot generalization to navigation. More
details and videos can be found at https://robo-map.github.io.

</details>


### [504] [Game-Theoretic Risk-Shaped Reinforcement Learning for Safe Autonomous Driving](https://arxiv.org/abs/2510.10960)
*Dong Hu,Fenqing Hu,Lidong Yang,Chao Huang*

Main category: cs.RO

TL;DR: 本研究提出了一种新颖的博弈论风险塑形强化学习（GTR2L）框架，用于自动驾驶中的安全决策。该框架结合了多层博弈论世界模型、自适应规划范围和不确定性感知障碍机制，以明确建模风险和不确定性，显著提高了复杂交通场景下的安全性、效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶在动态复杂交通环境中确保安全仍面临巨大挑战。传统强化学习方法主要关注奖励最大化，难以平衡安全性、效率和适应性，且未明确建模风险或安全约束。

Method: 本研究提出了GTR2L框架，其核心包括：1) 一个多层博弈论世界模型，联合预测周围车辆的交互行为及其相关风险；2) 一个基于预测不确定性动态调整的自适应规划范围；3) 一个不确定性感知障碍机制，实现安全边界的灵活调整；4) 一种专门的风险建模方法，明确捕捉认知不确定性（epistemic uncertainty）和偶然不确定性（aleatoric uncertainty），以指导受约束的策略优化。

Result: 在多样化且安全关键的交通场景中进行的广泛评估表明，GTR2L在成功率、碰撞和违规减少以及驾驶效率方面显著优于最先进的基线方法（包括人类驾驶员）。

Conclusion: GTR2L框架通过整合博弈论、不确定性感知风险建模和自适应机制，有效解决了传统强化学习在自动驾驶安全决策中的局限性，为复杂动态环境下的安全自动驾驶提供了一个高性能的解决方案。

Abstract: Ensuring safety in autonomous driving (AD) remains a significant challenge,
especially in highly dynamic and complex traffic environments where diverse
agents interact and unexpected hazards frequently emerge. Traditional
reinforcement learning (RL) methods often struggle to balance safety,
efficiency, and adaptability, as they primarily focus on reward maximization
without explicitly modeling risk or safety constraints. To address these
limitations, this study proposes a novel game-theoretic risk-shaped RL (GTR2L)
framework for safe AD. GTR2L incorporates a multi-level game-theoretic world
model that jointly predicts the interactive behaviors of surrounding vehicles
and their associated risks, along with an adaptive rollout horizon that adjusts
dynamically based on predictive uncertainty. Furthermore, an uncertainty-aware
barrier mechanism enables flexible modulation of safety boundaries. A dedicated
risk modeling approach is also proposed, explicitly capturing both epistemic
and aleatoric uncertainty to guide constrained policy optimization and enhance
decision-making in complex environments. Extensive evaluations across diverse
and safety-critical traffic scenarios show that GTR2L significantly outperforms
state-of-the-art baselines, including human drivers, in terms of success rate,
collision and violation reduction, and driving efficiency. The code is
available at https://github.com/DanielHu197/GTR2L.

</details>


### [505] [RoVer: Robot Reward Model as Test-Time Verifier for Vision-Language-Action Model](https://arxiv.org/abs/2510.10975)
*Mingtong Dai,Lingbo Liu,Yongjie Bai,Yang Liu,Zhouxia Wang,Rui SU,Chunjie Chen,Liang Lin,Xinyu Wu*

Main category: cs.RO

TL;DR: RoVer是一个具身测试时扩展框架，它利用机器人过程奖励模型（PRM）作为验证器，在不修改现有VLA模型架构或权重的情况下，通过评估和细化候选动作来提升其性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型性能提升主要依赖于大规模训练数据和模型尺寸扩展，但这对于机器人应用来说成本过高且受数据收集限制。

Method: RoVer框架采用一个机器人过程奖励模型（PRM）作为测试时验证器。PRM (i) 为候选动作分配标量过程奖励以评估其可靠性，(ii) 预测动作空间方向以指导候选动作的扩展和细化。在推理过程中，RoVer从基础策略生成多个候选动作，沿PRM预测的方向扩展这些动作，然后使用PRM对所有候选动作进行评分，最终选择最优动作执行。此外，通过缓存共享感知特征，RoVer能分摊感知成本，在相同的测试时计算预算下评估更多候选动作。

Result: RoVer有效地将可用的计算资源转化为更优的动作决策能力，实现了测试时扩展的益处，且无需额外的训练开销。它提供了一个通用的、即插即用的VLA测试时扩展框架，一个能同时提供标量过程奖励和动作空间探索方向的PRM，以及一个高效的方向引导采样策略，该策略利用共享感知缓存实现可扩展的候选动作生成和选择。

Conclusion: RoVer提供了一种通用且高效的测试时扩展框架，能够在不修改现有VLA模型的情况下，通过智能的动作评估和细化，利用计算资源显著提升具身智能体的决策能力，且无需额外训练成本。

Abstract: Vision-Language-Action (VLA) models have become a prominent paradigm for
embodied intelligence, yet further performance improvements typically rely on
scaling up training data and model size -- an approach that is prohibitively
expensive for robotics and fundamentally limited by data collection costs.We
address this limitation with $\mathbf{RoVer}$, an embodied test-time scaling
framework that uses a $\mathbf{Ro}$bot Process Reward Model (PRM) as a
Test-Time $\mathbf{Ver}$ifier to enhance the capabilities of existing VLA
models without modifying their architectures or weights. Specifically, RoVer
(i) assigns scalar-based process rewards to evaluate the reliability of
candidate actions, and (ii) predicts an action-space direction for candidate
expansion/refinement. During inference, RoVer generates multiple candidate
actions concurrently from the base policy, expands them along PRM-predicted
directions, and then scores all candidates with PRM to select the optimal
action for execution. Notably, by caching shared perception features, it can
amortize perception cost and evaluate more candidates under the same test-time
computational budget. Essentially, our approach effectively transforms
available computing resources into better action decision-making, realizing the
benefits of test-time scaling without extra training overhead. Our
contributions are threefold: (1) a general, plug-and-play test-time scaling
framework for VLAs; (2) a PRM that jointly provides scalar process rewards and
an action-space direction to guide exploration; and (3) an efficient
direction-guided sampling strategy that leverages a shared perception cache to
enable scalable candidate generation and selection during inference.

</details>


### [506] [Into the Unknown: Towards using Generative Models for Sampling Priors of Environment Uncertainty for Planning in Configuration Spaces](https://arxiv.org/abs/2510.11014)
*Subhransu S. Bhattacharjee,Hao Lu,Dylan Campbell,Rahul Shome*

Main category: cs.RO

TL;DR: 本文提出一种基于采样的流程，利用大规模预训练生成模型在零样本设置下生成概率先验，以解决部分可观测环境下的机器人规划问题。


<details>
  <summary>Details</summary>
Motivation: 在部分可观测环境下进行规划时，先验知识至关重要，但在实践中很难获取。

Method: 该方法是一个基于采样的流程，利用大规模预训练的生成模型，在零样本条件下生成概率先验。这些先验能够捕捉环境不确定性和空间语义关系。该流程在部分观测条件下，恢复完整的RGB-D点云样本，包含占用信息和目标语义，可直接用于配置空间规划。

Result: 实验表明，该方法能够恢复与真实情况一致的常识性空间语义，生成多样、干净的3D点云，可用于运动规划。研究还建立了一个Matterport3D基准，用于评估机器人在部分可见房间中导航到未观测目标物体的能力。

Conclusion: 该研究强调了生成模型作为机器人规划丰富先验来源的潜力，其生成的先验对于运动规划非常有用。

Abstract: Priors are vital for planning under partial observability, yet difficult to
obtain in practice. We present a sampling-based pipeline that leverages
large-scale pretrained generative models to produce probabilistic priors
capturing environmental uncertainty and spatio-semantic relationships in a
zero-shot manner. Conditioned on partial observations, the pipeline recovers
complete RGB-D point cloud samples with occupancy and target semantics,
formulated to be directly useful in configuration-space planning. We establish
a Matterport3D benchmark of rooms partially visible through doorways, where a
robot must navigate to an unobserved target object. Effective priors for this
setting must represent both occupancy and target-location uncertainty in
unobserved regions. Experiments show that our approach recovers commonsense
spatial semantics consistent with ground truth, yielding diverse, clean 3D
point clouds usable in motion planning, highlight the promise of generative
models as a rich source of priors for robotic planning.

</details>


### [507] [AMO-HEAD: Adaptive MARG-Only Heading Estimation for UAVs under Magnetic Disturbances](https://arxiv.org/abs/2510.10979)
*Qizhi Guo,Siyuan Yang,Junning Lyu,Jianjun Sun,Defu Lin,Shaoming He*

Main category: cs.RO

TL;DR: 本文提出了一种名为AMO-HEAD的自适应MARG-Only航向估计方法，用于在磁场干扰严重的室内环境中为无人机提供精确且鲁棒的航向估计。


<details>
  <summary>Details</summary>
Motivation: 无人机在室内巡检任务中需要精确且鲁棒的航向估计，但室内环境常伴有严重的磁场干扰，这会显著降低航向估计的准确性。

Method: AMO-HEAD是一个轻量级且计算高效的扩展卡尔曼滤波器(EKF)框架，利用惯性（陀螺仪、加速度计）和磁力计数据进行航向估计。它通过集成陀螺仪测量值来传播四元数状态，并使用加速度计和磁力计数据进行校正。该方法引入了自适应过程噪声协方差来补偿陀螺仪噪声、偏置漂移和欧拉积分误差，并通过基于实时磁偏差检测的缩放因子来减轻外部磁场干扰的影响。此外，还使用李导数对AMO-HEAD进行了理论可观测性分析。

Result: 在定制无人机平台进行的真实室内环境实验表明，所提出的AMO-HEAD算法在磁场干扰条件下能够提供精确的航向估计。

Conclusion: AMO-HEAD方法能够有效解决室内磁场干扰对无人机航向估计准确性的影响，为无人机在复杂室内环境中的操作提供了可靠的航向信息。

Abstract: Accurate and robust heading estimation is crucial for unmanned aerial
vehicles (UAVs) when conducting indoor inspection tasks. However, the cluttered
nature of indoor environments often introduces severe magnetic disturbances,
which can significantly degrade heading accuracy. To address this challenge,
this paper presents an Adaptive MARG-Only Heading (AMO-HEAD) estimation
approach for UAVs operating in magnetically disturbed environments. AMO-HEAD is
a lightweight and computationally efficient Extended Kalman Filter (EKF)
framework that leverages inertial and magnetic sensors to achieve reliable
heading estimation. In the proposed approach, gyroscope angular rate
measurements are integrated to propagate the quaternion state, which is
subsequently corrected using accelerometer and magnetometer data. The corrected
quaternion is then used to compute the UAV's heading. An adaptive process noise
covariance method is introduced to model and compensate for gyroscope
measurement noise, bias drift, and discretization errors arising from the Euler
method integration. To mitigate the effects of external magnetic disturbances,
a scaling factor is applied based on real-time magnetic deviation detection. A
theoretical observability analysis of the proposed AMO-HEAD is performed using
the Lie derivative. Extensive experiments were conducted in real world indoor
environments with customized UAV platforms. The results demonstrate the
effectiveness of the proposed algorithm in providing precise heading estimation
under magnetically disturbed conditions.

</details>


### [508] [Refinery: Active Fine-tuning and Deployment-time Optimization for Contact-Rich Policies](https://arxiv.org/abs/2510.11019)
*Bingjie Tang,Iretiayo Akinola,Jie Xu,Bowen Wen,Dieter Fox,Gaurav S. Sukhatme,Fabio Ramos,Abhishek Gupta,Yashraj Narang*

Main category: cs.RO

TL;DR: Refinery框架通过贝叶斯优化微调和高斯混合模型采样，显著提升了模拟学习中机器人装配策略的成功率和鲁棒性，使其达到工业标准并支持策略链式执行。


<details>
  <summary>Details</summary>
Motivation: 现有基于模拟学习的机器人装配策略在接触密集型任务中，尽管在观测噪声和控制误差下能达到约80%的成功率，但其在不同初始条件下个体策略性能方差大，导致无法满足行业标准，且策略链式执行时非常脆弱。

Method: 本文提出了Refinery框架以弥合性能差距。它通过以下方法增强策略鲁棒性：1) 引入贝叶斯优化引导的微调来改进个体策略；2) 在部署期间使用基于高斯混合模型的采样来选择能最大化执行成功率的初始化条件。

Result: 使用Refinery，机器人装配的平均成功率比现有最先进方法提高了10.98%，在模拟中达到91.51%，并在真实世界中实现了可比的性能。此外，这些经过微调的策略可以链式执行，完成长周期、多部件的装配任务（成功装配多达8个部件），而无需明确的多步骤训练。

Conclusion: Refinery框架有效地提升了策略在不同初始条件下的鲁棒性，显著提高了机器人装配任务的成功率，并使其能够可靠地进行策略链式执行，从而实现复杂的、多部件的组装任务。

Abstract: Simulation-based learning has enabled policies for precise, contact-rich
tasks (e.g., robotic assembly) to reach high success rates (~80%) under high
levels of observation noise and control error. Although such performance may be
sufficient for research applications, it falls short of industry standards and
makes policy chaining exceptionally brittle. A key limitation is the high
variance in individual policy performance across diverse initial conditions. We
introduce Refinery, an effective framework that bridges this performance gap,
robustifying policy performance across initial conditions. We propose Bayesian
Optimization-guided fine-tuning to improve individual policies, and Gaussian
Mixture Model-based sampling during deployment to select initializations that
maximize execution success. Using Refinery, we improve mean success rates by
10.98% over state-of-the-art methods in simulation-based learning for robotic
assembly, reaching 91.51% in simulation and comparable performance in the real
world. Furthermore, we demonstrate that these fine-tuned policies can be
chained to accomplish long-horizon, multi-part
assembly$\unicode{x2013}$successfully assembling up to 8 parts without
requiring explicit multi-step training.

</details>


### [509] [XGrasp: Gripper-Aware Grasp Detection with Multi-Gripper Data Generation](https://arxiv.org/abs/2510.11036)
*Yeonseo Lee,Jungwook Mun,Hyosup Shin,Guebin Hwang,Junhee Nam,Taeyeop Lee,Sungho Jo*

Main category: cs.RO

TL;DR: XGrasp是一个实时、夹具感知的抓取检测框架，通过数据增强、两阶段预测和对比学习，实现对多种夹具配置的高效处理和对未知夹具的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 大多数机器人抓取方法专为单一夹具类型设计，限制了其在需要多种末端执行器的实际场景中的应用。

Method: XGrasp提出了一种实时、夹具感知的抓取检测框架。它通过系统性地增强现有数据集的多夹具标注来解决数据稀缺问题。该方法采用分层两阶段架构：第一阶段，抓取点预测器（GPP）利用全局场景信息和夹具规格识别最佳抓取位置；第二阶段，角度-宽度预测器（AWP）利用局部特征精炼抓取角度和宽度。AWP模块中的对比学习使得模型能够通过学习基本的抓取特性，实现对未见过夹具的零样本泛化。该模块化框架可与视觉基础模型无缝集成。

Result: 实验结果表明，XGrasp在各种夹具类型上均取得了具有竞争力的抓取成功率，并且与现有夹具感知方法相比，推理速度大幅提升。

Conclusion: XGrasp提供了一个高效且可泛化的多夹具抓取检测解决方案，具有与视觉基础模型集成的潜力，为未来的视觉-语言能力奠定了基础。

Abstract: Most robotic grasping methods are typically designed for single gripper
types, which limits their applicability in real-world scenarios requiring
diverse end-effectors. We propose XGrasp, a real-time gripper-aware grasp
detection framework that efficiently handles multiple gripper configurations.
The proposed method addresses data scarcity by systematically augmenting
existing datasets with multi-gripper annotations. XGrasp employs a hierarchical
two-stage architecture. In the first stage, a Grasp Point Predictor (GPP)
identifies optimal locations using global scene information and gripper
specifications. In the second stage, an Angle-Width Predictor (AWP) refines the
grasp angle and width using local features. Contrastive learning in the AWP
module enables zero-shot generalization to unseen grippers by learning
fundamental grasping characteristics. The modular framework integrates
seamlessly with vision foundation models, providing pathways for future
vision-language capabilities. The experimental results demonstrate competitive
grasp success rates across various gripper types, while achieving substantial
improvements in inference speed compared to existing gripper-aware methods.
Project page: https://sites.google.com/view/xgrasp

</details>


### [510] [Unveiling Uncertainty-Aware Autonomous Cooperative Learning Based Planning Strategy](https://arxiv.org/abs/2510.11041)
*Shiyao Zhang,Liwei Deng,Shuyu Zhang,Weijie Yuan,Hong Zhang*

Main category: cs.RO

TL;DR: 针对多车辆交互中的感知、规划和通信不确定性，本文提出了一种基于深度强化学习的自主协同规划（DRLACP）框架。该框架采用带有GRU的SAC算法，有效学习不确定环境下的最优时变动作，并在CARLA仿真中表现优异。


<details>
  <summary>Details</summary>
Motivation: 未来的智能交通系统中，自主协同规划（ACP）对提高多车辆交互的效率和安全性至关重要。然而，现有的ACP策略无法充分解决感知、规划和通信等多种不确定性问题。

Method: 本文提出了一种新颖的基于深度强化学习的自主协同规划（DRLACP）框架。该框架采用软演员-评论家（SAC）算法，并结合门控循环单元（GRUs）来学习在规划、通信和感知不确定性导致的非完美状态信息下的确定性最优时变动作。通过Car Learning to Act (CARLA) 仿真平台展示了自动驾驶车辆（AVs）的实时动作。

Result: 评估结果表明，所提出的DRLACP框架能够有效地学习和执行协同规划。在不同场景下，即使存在不完美的自动驾驶车辆状态信息，DRLACP的性能也优于其他基线方法。

Conclusion: DRLACP框架成功解决了自主协同运动规划中的多种不确定性问题，展现了其有效性和优越的性能，为未来智能交通系统中的多车辆交互提供了新的解决方案。

Abstract: In future intelligent transportation systems, autonomous cooperative planning
(ACP), becomes a promising technique to increase the effectiveness and security
of multi-vehicle interactions. However, multiple uncertainties cannot be fully
addressed for existing ACP strategies, e.g. perception, planning, and
communication uncertainties. To address these, a novel deep reinforcement
learning-based autonomous cooperative planning (DRLACP) framework is proposed
to tackle various uncertainties on cooperative motion planning schemes.
Specifically, the soft actor-critic (SAC) with the implementation of gate
recurrent units (GRUs) is adopted to learn the deterministic optimal
time-varying actions with imperfect state information occurred by planning,
communication, and perception uncertainties. In addition, the real-time actions
of autonomous vehicles (AVs) are demonstrated via the Car Learning to Act
(CARLA) simulation platform. Evaluation results show that the proposed DRLACP
learns and performs cooperative planning effectively, which outperforms other
baseline methods under different scenarios with imperfect AV state information.

</details>


### [511] [Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling](https://arxiv.org/abs/2510.11083)
*Tianyi Tan,Yinan Zheng,Ruiming Liang,Zexu Wang,Kexin Zheng,Jinliang Zheng,Jianxiong Li,Xianyuan Zhan,Jingjing Liu*

Main category: cs.RO

TL;DR: Flow Planner通过数据建模、模型架构和学习方案的创新，有效解决了复杂场景下自动驾驶交互行为建模的挑战，并在大型数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶规划中，复杂场景下交互式驾驶行为建模仍是核心挑战。现有基于学习的方法依赖过度设计的架构，缺乏专门的交互行为建模机制，且交互式驾驶数据稀缺，导致传统模仿学习难以捕捉高价值交互行为。

Method: 本文提出了Flow Planner。具体方法包括：1) 引入细粒度轨迹分词，将轨迹分解为重叠片段以降低建模复杂性；2) 设计精巧的模型架构，实现规划与场景信息的时空高效融合，以更好地捕捉交互行为；3) 结合流匹配（flow matching）与无分类器引导（classifier-free guidance）进行多模态行为生成，在推理过程中动态重新加权智能体交互，以保持一致的响应策略。

Result: 在大型nuPlan数据集和具有挑战性的交互式interPlan数据集上的实验结果表明，Flow Planner在基于学习的方法中取得了最先进的性能，并能有效建模复杂驾驶场景中的交互行为。

Conclusion: Flow Planner通过在数据建模、模型架构和学习方案上的协同创新，为复杂场景下的交互式驾驶行为建模提供了一个强大的解决方案，显著提升了交互场景的理解能力，并在现有方法中表现出色。

Abstract: Modeling interactive driving behaviors in complex scenarios remains a
fundamental challenge for autonomous driving planning. Learning-based
approaches attempt to address this challenge with advanced generative models,
removing the dependency on over-engineered architectures for representation
fusion. However, brute-force implementation by simply stacking transformer
blocks lacks a dedicated mechanism for modeling interactive behaviors that are
common in real driving scenarios. The scarcity of interactive driving data
further exacerbates this problem, leaving conventional imitation learning
methods ill-equipped to capture high-value interactive behaviors. We propose
Flow Planner, which tackles these problems through coordinated innovations in
data modeling, model architecture, and learning scheme. Specifically, we first
introduce fine-grained trajectory tokenization, which decomposes the trajectory
into overlapping segments to decrease the complexity of whole trajectory
modeling. With a sophisticatedly designed architecture, we achieve efficient
temporal and spatial fusion of planning and scene information, to better
capture interactive behaviors. In addition, the framework incorporates flow
matching with classifier-free guidance for multi-modal behavior generation,
which dynamically reweights agent interactions during inference to maintain
coherent response strategies, providing a critical boost for interactive
scenario understanding. Experimental results on the large-scale nuPlan dataset
and challenging interactive interPlan dataset demonstrate that Flow Planner
achieves state-of-the-art performance among learning-based approaches while
effectively modeling interactive behaviors in complex driving scenarios.

</details>


### [512] [A Primer on SO(3) Action Representations in Deep Reinforcement Learning](https://arxiv.org/abs/2510.11103)
*Martin Schuck,Sherif Samy,Angela P. Schoellig*

Main category: cs.RO

TL;DR: 本文系统评估了强化学习中SO(3)动作表示对机器人控制任务的影响，发现局部坐标系中的切向量表示最为可靠。


<details>
  <summary>Details</summary>
Motivation: 许多机器人控制任务需要策略作用于姿态（SO(3)），但SO(3)的几何特性使其难以处理，没有全局、平滑、最小的参数化表示。常见的表示方法（如欧拉角、四元数、旋转矩阵、李代数坐标）各有优缺点，这些在监督学习中已有所研究，但在强化学习的动作空间中其影响尚不明确。

Method: 研究系统评估了SO(3)动作表示在三种标准连续控制算法（PPO、SAC、TD3）以及密集和稀疏奖励设置下的表现。比较了不同表示如何影响探索、与熵正则化的交互以及训练稳定性。通过实证研究分析了从欧几里得网络输出中获得有效旋转的不同投影方法的影响。

Result: 表示引起的几何特性强烈影响探索和优化。将动作表示为局部坐标系中的切向量，在各种算法中产生了最可靠的结果。研究还量化了这些选择的实际影响，并提炼出简单、可直接用于实现的旋转动作选择和使用指南。

Conclusion: SO(3)动作表示的选择对强化学习中的机器人控制至关重要。局部坐标系中的切向量表示在探索和优化方面表现最佳，并能提供最可靠的训练结果。

Abstract: Many robotic control tasks require policies to act on orientations, yet the
geometry of SO(3) makes this nontrivial. Because SO(3) admits no global,
smooth, minimal parameterization, common representations such as Euler angles,
quaternions, rotation matrices, and Lie algebra coordinates introduce distinct
constraints and failure modes. While these trade-offs are well studied for
supervised learning, their implications for actions in reinforcement learning
remain unclear. We systematically evaluate SO(3) action representations across
three standard continuous control algorithms, PPO, SAC, and TD3, under dense
and sparse rewards. We compare how representations shape exploration, interact
with entropy regularization, and affect training stability through empirical
studies and analyze the implications of different projections for obtaining
valid rotations from Euclidean network outputs. Across a suite of robotics
benchmarks, we quantify the practical impact of these choices and distill
simple, implementation-ready guidelines for selecting and using rotation
actions. Our results highlight that representation-induced geometry strongly
influences exploration and optimization and show that representing actions as
tangent vectors in the local frame yields the most reliable results across
algorithms.

</details>


### [513] [Design and Koopman Model Predictive Control of A Soft Exoskeleton Based on Origami-Inspired Pneumatic Actuator for Knee Rehabilitation](https://arxiv.org/abs/2510.11094)
*Junxiang Wang,Han Zhang,Zehao Wang,Huaiyuan Chen,Pu Wang,Weidong Chen*

Main category: cs.RO

TL;DR: 本文设计了一种基于折纸气动执行器的软体膝关节康复外骨骼，并提出了一种新的控制框架。该框架利用深度Koopman网络结合肌电信号（EMG）对复杂人机交互动力学进行建模，并在此基础上采用模型预测控制（MPC）实现实时康复训练，其性能优于传统PID控制。


<details>
  <summary>Details</summary>
Motivation: 中风引起的下肢功能障碍康复至关重要。传统刚性外骨骼笨重、穿戴困难且需要额外的柔顺控制。软体外骨骼穿戴舒适且具有固有柔顺性，但其复杂非线性人机交互动力学给控制带来了巨大挑战。

Method: 研究人员设计了一种易于穿戴的基于折纸气动执行器的软体膝关节康复外骨骼。为保证控制性能和良好的人机交互，首先利用深度Koopman网络对人机交互动力学进行建模，将肌电信号（EMG）和控制气动机器人阀门和泵的PWM波占空比作为输入，获得能准确捕捉复杂动力学的线性Koopman模型。其次，基于该Koopman模型，进一步使用模型预测控制（MPC）来实时控制软体机器人，帮助用户进行康复训练，目标是跟踪屏幕上给定的参考信号。

Result: 实验结果表明，将肌电信号（EMG）整合到Koopman模型中大大提高了模型精度。此外，通过个体数据训练的个性化Koopman模型比非个性化模型表现更好。最终，所提出的控制框架在被动和主动训练模式下均优于传统PID控制。

Conclusion: 该研究提供了一种针对软体康复机器人的新型控制框架，通过准确建模人机交互和优越的控制性能，有效解决了软体外骨骼的控制挑战。

Abstract: Effective rehabilitation methods are essential for the recovery of lower limb
dysfunction caused by stroke. Nowadays, robotic exoskeletons have shown great
potentials in rehabilitation. Nevertheless, traditional rigid exoskeletons are
usually heavy and need a lot of work to help the patients to put them on.
Moreover, it also requires extra compliance control to guarantee the safety. In
contrast, soft exoskeletons are easy and comfortable to wear and have intrinsic
compliance, but their complex nonlinear human-robot interaction dynamics would
pose significant challenges for control. In this work, based on the pneumatic
actuators inspired by origami, we design a rehabilitation exoskeleton for knee
that is easy and comfortable to wear. To guarantee the control performance and
enable a nice human-robot interaction, we first use Deep Koopman Network to
model the human-robot interaction dynamics. In particular, by viewing the
electromyography (EMG) signals and the duty cycle of the PWM wave that controls
the pneumatic robot's valves and pump as the inputs, the linear Koopman model
accurately captures the complex human-robot interaction dynamics. Next, based
on the obtained Koopman model, we further use Model Predictive Control (MPC) to
control the soft robot and help the user to do rehabilitation training in
real-time. The goal of the rehabilitation training is to track a given
reference signal shown on the screen. Experiments show that by integrating the
EMG signals into the Koopman model, we have improved the model accuracy to
great extent. In addition, a personalized Koopman model trained from the
individual's own data performs better than the non-personalized model.
Consequently, our control framework outperforms the traditional PID control in
both passive and active training modes. Hence the proposed method provides a
new control framework for soft rehabilitation robots.

</details>


### [514] [DemoHLM: From One Demonstration to Generalizable Humanoid Loco-Manipulation](https://arxiv.org/abs/2510.11258)
*Yuhui Fu,Feiyang Xie,Chaoyi Xu,Jing Xiong,Haoqi Yuan,Zongqing Lu*

Main category: cs.RO

TL;DR: DemoHLM是一个人形机器人移动-操作框架，它通过单次模拟演示，结合分层控制和闭环视觉反馈，实现了在真实人形机器人上的通用移动-操作能力，并有效解决了sim-to-real迁移问题。


<details>
  <summary>Details</summary>
Motivation: 人形机器人的移动-操作是实现多功能人机交互的基础挑战，但现有研究仍未充分探索，常依赖硬编码任务或昂贵的真实数据收集，这限制了机器人的自主性和泛化能力。

Method: 本文提出了DemoHLM框架，采用分层结构：底层是通用的全身控制器（将全身运动指令映射为关节扭矩，提供全向移动能力），高层是针对多任务的操纵策略。操纵策略通过模拟中的数据生成和模仿学习管线获得，并利用闭环视觉反馈指挥全身控制器执行移动-操作任务。

Result: 实验结果显示，合成数据量与策略性能呈正相关，证明了数据生成管线的有效性和方法的数据效率。在配备RGB-D摄像头的Unitree G1机器人上的真实世界实验验证了DemoHLM的sim-to-real迁移能力，在十项移动-操作任务中，即使存在空间变化，也表现出鲁棒性能。

Conclusion: DemoHLM框架通过单次模拟演示，成功实现了真实人形机器人的通用移动-操作能力，其分层控制、数据生成与模仿学习管线以及闭环视觉反馈，有效促进了sim-to-real迁移，并在多种复杂任务中展现了鲁棒性。

Abstract: Loco-manipulation is a fundamental challenge for humanoid robots to achieve
versatile interactions in human environments. Although recent studies have made
significant progress in humanoid whole-body control, loco-manipulation remains
underexplored and often relies on hard-coded task definitions or costly
real-world data collection, which limits autonomy and generalization. We
present DemoHLM, a framework for humanoid loco-manipulation that enables
generalizable loco-manipulation on a real humanoid robot from a single
demonstration in simulation. DemoHLM adopts a hierarchy that integrates a
low-level universal whole-body controller with high-level manipulation policies
for multiple tasks. The whole-body controller maps whole-body motion commands
to joint torques and provides omnidirectional mobility for the humanoid robot.
The manipulation policies, learned in simulation via our data generation and
imitation learning pipeline, command the whole-body controller with closed-loop
visual feedback to execute challenging loco-manipulation tasks. Experiments
show a positive correlation between the amount of synthetic data and policy
performance, underscoring the effectiveness of our data generation pipeline and
the data efficiency of our approach. Real-world experiments on a Unitree G1
robot equipped with an RGB-D camera validate the sim-to-real transferability of
DemoHLM, demonstrating robust performance under spatial variations across ten
loco-manipulation tasks.

</details>


### [515] [Rotor-Failure-Aware Quadrotors Flight in Unknown Environments](https://arxiv.org/abs/2510.11306)
*Xiaobin Zhou,Miao Wang,Chengao Li,Can Cui,Ruibin Zhang,Yongchao Wang,Chao Xu,Fei Gao*

Main category: cs.RO

TL;DR: 本文提出了一种旋翼故障感知四旋翼导航系统，通过结合在线故障检测与诊断（FDD）、轨迹规划和容错控制（FTC），首次实现在未知复杂环境下带旋翼故障的自主飞行。


<details>
  <summary>Details</summary>
Motivation: 旋翼故障会导致高速旋转和振动，给四旋翼在未知环境中的自主飞行带来巨大挑战。现有主流方法（如容错控制和预定义轨迹跟踪）未能实现对故障后四旋翼在未知复杂环境中的在线故障检测与诊断、轨迹规划和容错控制。

Method: 1. 设计了一种结合电机动力学的复合FDD非线性模型预测控制器（NMPC），以确保快速故障检测和飞行稳定性。2. 设计了一个旋翼故障感知规划器，利用FDD结果和时空联合优化。3. 设计了一个带有四个反扭矩板的基于LiDAR的四旋翼平台，以在高速旋转下实现可靠感知。

Result: 与现有最先进方法相比，所提出的方法在处理旋翼故障（包括螺旋桨卸载和电机停转）方面表现出卓越的性能。实验结果首次证明，该方法能使四旋翼在杂乱房间和未知森林等挑战性环境中，在旋翼故障下实现自主飞行。

Conclusion: 本文提出的旋翼故障感知四旋翼导航系统能够有效应对旋翼故障，首次实现在未知复杂环境下带旋翼故障的四旋翼自主飞行。

Abstract: Rotor failures in quadrotors may result in high-speed rotation and vibration
due to rotor imbalance, which introduces significant challenges for autonomous
flight in unknown environments. The mainstream approaches against rotor
failures rely on fault-tolerant control (FTC) and predefined trajectory
tracking. To the best of our knowledge, online failure detection and diagnosis
(FDD), trajectory planning, and FTC of the post-failure quadrotors in unknown
and complex environments have not yet been achieved. This paper presents a
rotor-failure-aware quadrotor navigation system designed to mitigate the
impacts of rotor imbalance. First, a composite FDD-based nonlinear model
predictive controller (NMPC), incorporating motor dynamics, is designed to
ensure fast failure detection and flight stability. Second, a
rotor-failure-aware planner is designed to leverage FDD results and
spatial-temporal joint optimization, while a LiDAR-based quadrotor platform
with four anti-torque plates is designed to enable reliable perception under
high-speed rotation. Lastly, extensive benchmarks against state-of-the-art
methods highlight the superior performance of the proposed approach in
addressing rotor failures, including propeller unloading and motor stoppage.
The experimental results demonstrate, for the first time, that our approach
enables autonomous quadrotor flight with rotor failures in challenging
environments, including cluttered rooms and unknown forests.

</details>


### [516] [Adap-RPF: Adaptive Trajectory Sampling for Robot Person Following in Dynamic Crowded Environments](https://arxiv.org/abs/2510.11308)
*Weixi Situ,Hanjing Ye,Jianwei Peng,Yu Zhan,Hong Zhang*

Main category: cs.RO

TL;DR: 本文提出了一种自适应轨迹采样方法和预测感知型MPPI控制器，以解决在动态和拥挤环境中机器人跟随行人时频繁遮挡的挑战，显著提升了跟随的平稳性、安全性、鲁棒性和舒适度。


<details>
  <summary>Details</summary>
Motivation: 机器人跟随行人(RPF)在人机交互中至关重要，但由于动态和拥挤环境中频繁的遮挡，实现实用的RPF仍具挑战。现有方法依赖固定点跟随或简化启发式的稀疏候选点选择，无法有效处理移动障碍物（如行人）造成的复杂遮挡。

Method: 本文提出了一种自适应轨迹采样方法，在社交感知区域内生成密集候选点，并使用多目标成本函数进行评估，基于最优解估计行人跟随轨迹。此外，设计了一种预测感知型模型预测路径积分（MPPI）控制器，该控制器能同时跟踪轨迹并利用预测的行人运动主动避开碰撞。

Result: 广泛实验表明，本文方法在平稳性、安全性、鲁棒性和人类舒适度方面优于现有基线方法。在真实世界场景中的移动机器人上进一步验证了其有效性。

Conclusion: 所提出的自适应轨迹采样方法和预测感知型MPPI控制器，有效解决了动态拥挤环境中机器人行人跟随的遮挡问题，显著提升了跟随性能，使其更适用于实际应用。

Abstract: Robot person following (RPF) is a core capability in human-robot interaction,
enabling robots to assist users in daily activities, collaborative work, and
other service scenarios. However, achieving practical RPF remains challenging
due to frequent occlusions, particularly in dynamic and crowded environments.
Existing approaches often rely on fixed-point following or sparse
candidate-point selection with oversimplified heuristics, which cannot
adequately handle complex occlusions caused by moving obstacles such as
pedestrians. To address these limitations, we propose an adaptive trajectory
sampling method that generates dense candidate points within socially aware
zones and evaluates them using a multi-objective cost function. Based on the
optimal point, a person-following trajectory is estimated relative to the
predicted motion of the target. We further design a prediction-aware model
predictive path integral (MPPI) controller that simultaneously tracks this
trajectory and proactively avoids collisions using predicted pedestrian
motions. Extensive experiments show that our method outperforms
state-of-the-art baselines in smoothness, safety, robustness, and human
comfort, with its effectiveness further demonstrated on a mobile robot in
real-world scenarios.

</details>


### [517] [Path and Motion Optimization for Efficient Multi-Location Inspection with Humanoid Robots](https://arxiv.org/abs/2510.11401)
*Jiayang Wu,Jiongye Li,Shibowen Zhang,Zhicheng He,Zaijin Wang,Xiaokun Leng,Hangxin Liu,Jingwen Zhang,Jiayi Wang,Song-Chun Zhu,Yao Su*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的框架，使仿人机器人能够以高效率和毫米级精度执行检查任务，通过结合分层规划、时间最优站立姿态生成和集成MPC实现。


<details>
  <summary>Details</summary>
Motivation: 现有仿人机器人在执行需要高效率和毫米级精度的检查任务时面临挑战，尤其是在复杂的工业操作中，因此需要一种新的方法来提升其性能。

Method: 该方法结合了分层规划、时间最优站立姿态生成和集成模型预测控制（MPC）。分层规划利用IK和MIP解耦高维问题；MIP公式优化站立姿态选择和轨迹长度以最小化任务完成时间；MPC系统采用简化运动学和单步位置校正，确保末端执行器毫米级跟踪精度。

Result: 通过在Kuavo 4Pro仿人平台上的仿真和实验验证，该框架在多位置任务中展现出低时间成本和高成功率，证明了其能够高效、精确地执行复杂的工业操作。

Conclusion: 所提出的框架成功地使仿人机器人能够以高效率和毫米级精度执行复杂的检查任务，为工业应用提供了有效的解决方案。

Abstract: This paper proposes a novel framework for humanoid robots to execute
inspection tasks with high efficiency and millimeter-level precision. The
approach combines hierarchical planning, time-optimal standing position
generation, and integrated \ac{mpc} to achieve high speed and precision. A
hierarchical planning strategy, leveraging \ac{ik} and \ac{mip}, reduces
computational complexity by decoupling the high-dimensional planning problem. A
novel MIP formulation optimizes standing position selection and trajectory
length, minimizing task completion time. Furthermore, an MPC system with
simplified kinematics and single-step position correction ensures
millimeter-level end-effector tracking accuracy. Validated through simulations
and experiments on the Kuavo 4Pro humanoid platform, the framework demonstrates
low time cost and a high success rate in multi-location tasks, enabling
efficient and precise execution of complex industrial operations.

</details>


### [518] [HiMaCon: Discovering Hierarchical Manipulation Concepts from Unlabeled Multi-Modal Data](https://arxiv.org/abs/2510.11321)
*Ruizhe Liu,Pei Zhou,Qian Luo,Li Sun,Jun Cen,Yibing Song,Yanchao Yang*

Main category: cs.RO

TL;DR: 该论文提出了一种自监督框架，通过跨模态感知关联和多层次时间抽象来学习分层操作概念，以实现机器人操作的有效泛化。


<details>
  <summary>Details</summary>
Motivation: 机器人操作中有效的泛化需要能捕捉跨环境和任务的不变交互模式的表示。

Method: 该方法通过结合一个识别跨感官模态持久模式的跨模态关联网络，以及一个在时间尺度上分层组织表示的多视野预测器，构建了一个自监督框架，用于学习分层操作概念，且无需人工标注。

Result: 在模拟基准和真实世界部署中，其概念增强策略显著提高了性能。分析表明，尽管没有语义监督，学习到的概念类似于人类可解释的操作原语。

Conclusion: 这项工作推动了对操作表示学习的理解，并提供了一种在复杂场景中提高机器人性能的实用方法。

Abstract: Effective generalization in robotic manipulation requires representations
that capture invariant patterns of interaction across environments and tasks.
We present a self-supervised framework for learning hierarchical manipulation
concepts that encode these invariant patterns through cross-modal sensory
correlations and multi-level temporal abstractions without requiring human
annotation. Our approach combines a cross-modal correlation network that
identifies persistent patterns across sensory modalities with a multi-horizon
predictor that organizes representations hierarchically across temporal scales.
Manipulation concepts learned through this dual structure enable policies to
focus on transferable relational patterns while maintaining awareness of both
immediate actions and longer-term goals. Empirical evaluation across simulated
benchmarks and real-world deployments demonstrates significant performance
improvements with our concept-enhanced policies. Analysis reveals that the
learned concepts resemble human-interpretable manipulation primitives despite
receiving no semantic supervision. This work advances both the understanding of
representation learning for manipulation and provides a practical approach to
enhancing robotic performance in complex scenarios.

</details>


### [519] [A Modular AIoT Framework for Low-Latency Real-Time Robotic Teleoperation in Smart Cities](https://arxiv.org/abs/2510.11421)
*Shih-Chieh Sun,Yun-Cheng Tsai*

Main category: cs.RO

TL;DR: 本文提出一个AI驱动的物联网机器人远程操作系统，专为智能城市应用设计，集成了实时AI感知和低延迟双协议通信，实现了高效的远程操控和智能监控。


<details>
  <summary>Details</summary>
Motivation: 传统的远程操作平台在模块化部署、实时AI感知和适应性通信策略方面存在不足，无法完全满足智能城市应用的需求，因此需要一个为此量身定制的系统。

Method: 该系统采用Flutter跨平台移动界面，通过MQTT进行控制信号传输，并使用LiveKit框架通过WebRTC进行视频流传输。部署了YOLOv11-nano模型进行轻量级目标检测和实时视觉叠加。控制命令通过MQTT发送至基于ESP8266的执行器节点，由Arduino Mega2560控制器协调多轴机械臂运动。后端基础设施托管在DigitalOcean上。系统性能通过本地和国际VPN（包括香港、日本和比利时）场景下的延迟评估进行验证。

Result: 评估结果显示，执行器响应时间低至0.2秒，即使在高延迟网络下，总视频延迟也低于1.2秒。这种低延迟双协议设计确保了响应迅速的闭环交互和分布式环境下的鲁棒性能。系统强调模块化部署、实时AI感知和适应性通信策略。

Conclusion: 该系统非常适用于智能城市场景，例如远程基础设施检查、公共设备维护和城市自动化。未来的增强将集中于边缘设备部署、自适应路由以及与城市级物联网网络的集成，以提高弹性和可扩展性。

Abstract: This paper presents an AI-driven IoT robotic teleoperation system designed
for real-time remote manipulation and intelligent visual monitoring, tailored
for smart city applications. The architecture integrates a Flutter-based
cross-platform mobile interface with MQTT-based control signaling and WebRTC
video streaming via the LiveKit framework. A YOLOv11-nano model is deployed for
lightweight object detection, enabling real-time perception with annotated
visual overlays delivered to the user interface. Control commands are
transmitted via MQTT to an ESP8266-based actuator node, which coordinates
multi-axis robotic arm motion through an Arduino Mega2560 controller. The
backend infrastructure is hosted on DigitalOcean, ensuring scalable cloud
orchestration and stable global communication. Latency evaluations conducted
under both local and international VPN scenarios (including Hong Kong, Japan,
and Belgium) demonstrate actuator response times as low as 0.2 seconds and
total video latency under 1.2 seconds, even across high-latency networks. This
low-latency dual-protocol design ensures responsive closed-loop interaction and
robust performance in distributed environments. Unlike conventional
teleoperation platforms, the proposed system emphasizes modular deployment,
real-time AI sensing, and adaptable communication strategies, making it
well-suited for smart city scenarios such as remote infrastructure inspection,
public equipment servicing, and urban automation. Future enhancements will
focus on edge-device deployment, adaptive routing, and integration with
city-scale IoT networks to enhance resilience and scalability.

</details>


### [520] [Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.11474)
*Ardian Selmonaj,Giacomo Del Rio,Adrian Schneider,Alessandro Antonucci*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的3D多智能体空战环境和分层多智能体强化学习框架，通过结合异构智能体动态、课程学习、联赛和新训练算法，解决了现实空战模拟中的挑战，显著提高了学习效率和战斗性能。


<details>
  <summary>Details</summary>
Motivation: 在现实的空战模拟中，由于态势感知不完善和非线性的飞行动力学，实现任务目标极具挑战性。

Method: 引入了一个新颖的3D多智能体空战环境，并采用了一个分层多智能体强化学习框架。该方法结合了异构智能体动态、课程学习、联赛（league-play）以及一种新改编的训练算法。决策过程被组织成两个抽象级别：低级策略学习精确的控制机动，而高级策略根据任务目标发出战术指令。

Result: 实证结果表明，所提出的分层方法在复杂的缠斗场景中，既提高了学习效率，也提升了战斗性能。

Conclusion: 分层多智能体强化学习框架能够有效应对复杂空战模拟中的挑战，通过分层决策和多种学习策略的结合，显著提升了智能体的学习效率和实战表现。

Abstract: Achieving mission objectives in a realistic simulation of aerial combat is
highly challenging due to imperfect situational awareness and nonlinear flight
dynamics. In this work, we introduce a novel 3D multi-agent air combat
environment and a Hierarchical Multi-Agent Reinforcement Learning framework to
tackle these challenges. Our approach combines heterogeneous agent dynamics,
curriculum learning, league-play, and a newly adapted training algorithm. To
this end, the decision-making process is organized into two abstraction levels:
low-level policies learn precise control maneuvers, while high-level policies
issue tactical commands based on mission objectives. Empirical results show
that our hierarchical approach improves both learning efficiency and combat
performance in complex dogfight scenarios.

</details>


### [521] [DQ-NMPC: Dual-Quaternion NMPC for Quadrotor Flight](https://arxiv.org/abs/2510.11525)
*Luis F. Recalde,Dhruv Agrawal,Jon Arrizabalaga,Guanrui Li*

Main category: cs.RO

TL;DR: 本文提出了一种基于对偶四元数（DQ-NMPC）的新型非线性模型预测控制框架，用于四旋翼飞行器的精确敏捷控制。该方法通过在对偶四元数流形上直接表示动力学和位姿误差，实现了紧凑且全局非奇异的耦合动力学公式。实验结果表明，与传统NMPC相比，DQ-NMPC显著提高了跟踪性能，误差降低高达56%，并能成功处理激进轨迹。


<details>
  <summary>Details</summary>
Motivation: 微型飞行器（MAVs）在复杂任务中潜力巨大，尤其是在复杂动态环境中。然而，由于四旋翼飞行器的欠驱动特性以及平移和旋转动力学之间的强耦合，在敏捷飞行中实现精确控制仍然是一个重大挑战。

Method: 提出了一种基于对偶四元数（DQ-NMPC）的新型非线性模型预测控制（NMPC）框架。该方法通过在对偶四元数流形上直接表示四旋翼动力学和位姿误差，实现了紧凑且全局非奇异的公式，能够捕捉四旋翼的耦合动力学。

Result: 通过仿真和真实世界实验验证了该方法，结果显示其具有更好的数值条件，并显著提高了跟踪性能。与传统的基线NMPC方法相比，位置和姿态误差分别减少了高达56.11%和56.77%。此外，该控制器成功处理了激进轨迹，在11m x 4.5m x 3.65m的受限空间条件下，达到了最高13.66 m/s的速度和4.2 g的加速度，而基线控制器在此条件下失败。

Conclusion: 所提出的DQ-NMPC框架为四旋翼飞行器提供了卓越的敏捷飞行精确控制能力，在准确性和处理激进机动方面均优于传统方法。

Abstract: MAVs have great potential to assist humans in complex tasks, with
applications ranging from logistics to emergency response. Their agility makes
them ideal for operations in complex and dynamic environments. However,
achieving precise control in agile flights remains a significant challenge,
particularly due to the underactuated nature of quadrotors and the strong
coupling between their translational and rotational dynamics. In this work, we
propose a novel NMPC framework based on dual-quaternions (DQ-NMPC) for
quadrotor flight. By representing both quadrotor dynamics and the pose error
directly on the dual-quaternion manifold, our approach enables a compact and
globally non-singular formulation that captures the quadrotor coupled dynamics.
We validate our approach through simulations and real-world experiments,
demonstrating better numerical conditioning and significantly improved tracking
performance, with reductions in position and orientation errors of up to 56.11%
and 56.77%, compared to a conventional baseline NMPC method. Furthermore, our
controller successfully handles aggressive trajectories, reaching maximum
speeds up to 13.66 m/s and accelerations reaching 4.2 g within confined space
conditions of dimensions 11m x 4.5m x 3.65m under which the baseline controller
fails.

</details>


### [522] [Simultaneous Calibration of Noise Covariance and Kinematics for State Estimation of Legged Robots via Bi-level Optimization](https://arxiv.org/abs/2510.11539)
*Denglin Cheng,Jiarong Kang,Xiaobin Xiong*

Main category: cs.RO

TL;DR: 本文提出了一种双层优化框架，用于在机器人状态估计中联合校准噪声协方差和运动学参数，以提高估计精度和不确定性校准。


<details>
  <summary>Details</summary>
Motivation: 在动态、不确定环境中运行的腿式和空中机器人，准确的状态估计至关重要。主要挑战在于过程和测量噪声协方差通常未知或需要手动调整，这影响了估计的准确性。

Method: 引入了一个双层优化框架。上层将噪声协方差和模型参数作为优化变量。下层执行一个全信息估计器。通过对估计器进行微分，可以直接优化轨迹层面的目标，从而实现准确和一致的状态估计。

Result: 在四足和人形机器人上的验证表明，与手动调整的基线相比，估计精度和不确定性校准显著提高。该方法将状态估计、传感器和运动学校准统一到一个原则性的、数据驱动的框架中。

Conclusion: 该方法提供了一个统一的、数据驱动的框架，将状态估计、传感器和运动学校准整合在一起，适用于各种机器人平台，显著提高了状态估计的准确性和不确定性校准。

Abstract: Accurate state estimation is critical for legged and aerial robots operating
in dynamic, uncertain environments. A key challenge lies in specifying process
and measurement noise covariances, which are typically unknown or manually
tuned. In this work, we introduce a bi-level optimization framework that
jointly calibrates covariance matrices and kinematic parameters in an
estimator-in-the-loop manner. The upper level treats noise covariances and
model parameters as optimization variables, while the lower level executes a
full-information estimator. Differentiating through the estimator allows direct
optimization of trajectory-level objectives, resulting in accurate and
consistent state estimates. We validate our approach on quadrupedal and
humanoid robots, demonstrating significantly improved estimation accuracy and
uncertainty calibration compared to hand-tuned baselines. Our method unifies
state estimation, sensor, and kinematics calibration into a principled,
data-driven framework applicable across diverse robotic platforms.

</details>


### [523] [NaviGait: Navigating Dynamically Feasible Gait Libraries using Deep Reinforcement Learning](https://arxiv.org/abs/2510.11542)
*Neil C. Janwani,Varun Madabushi,Maegan Tucker*

Main category: cs.RO

TL;DR: NaviGait是一个分层框架，结合了轨迹优化和强化学习的优点，实现了鲁棒且直观的双足机器人运动控制。


<details>
  <summary>Details</summary>
Motivation: 强化学习（RL）在双足运动控制中表现出强大能力，但奖励设计复杂且难以调整。离线轨迹优化方法虽然可调、可解释且有数学基础，但对真实世界干扰（如外部扰动）缺乏鲁棒性。本研究旨在结合两者的优点，克服各自的局限性。

Method: NaviGait是一个分层框架。它利用离线优化的步态库，并根据高级指令平滑地在这些步态之间进行插值，以生成连续的参考运动。强化学习策略提供关节级和速度指令的残差校正，以调节和稳定步态库中的参考轨迹。通过编码轨迹优化中的丰富运动先验，极大地简化了奖励设计。

Result: 实验结果表明，NaviGait比传统和基于模仿的强化学习训练更快，并且产生的运动最接近原始参考轨迹。

Conclusion: 通过将高级运动生成与低级校正解耦，NaviGait为实现动态和鲁棒的运动提供了一种更具可扩展性和通用性的方法。

Abstract: Reinforcement learning (RL) has emerged as a powerful method to learn robust
control policies for bipedal locomotion. Yet, it can be difficult to tune
desired robot behaviors due to unintuitive and complex reward design. In
comparison, offline trajectory optimization methods, like Hybrid Zero Dynamics,
offer more tuneable, interpretable, and mathematically grounded motion plans
for high-dimensional legged systems. However, these methods often remain
brittle to real-world disturbances like external perturbations.
  In this work, we present NaviGait, a hierarchical framework that combines the
structure of trajectory optimization with the adaptability of RL for robust and
intuitive locomotion control. NaviGait leverages a library of offline-optimized
gaits and smoothly interpolates between them to produce continuous reference
motions in response to high-level commands. The policy provides both
joint-level and velocity command residual corrections to modulate and stabilize
the reference trajectories in the gait library. One notable advantage of
NaviGait is that it dramatically simplifies reward design by encoding rich
motion priors from trajectory optimization, reducing the need for finely tuned
shaping terms and enabling more stable and interpretable learning. Our
experimental results demonstrate that NaviGait enables faster training compared
to conventional and imitation-based RL, and produces motions that remain
closest to the original reference. Overall, by decoupling high-level motion
generation from low-level correction, NaviGait offers a more scalable and
generalizable approach for achieving dynamic and robust locomotion.

</details>


### [524] [Robot Soccer Kit: Omniwheel Tracked Soccer Robots for Education](https://arxiv.org/abs/2510.11552)
*Gregoire Passault,Clement Gaspard,Olivier Ly*

Main category: cs.RO

TL;DR: 教育机器人普及但受限于自我中心感知。本文提出一种带外部跟踪系统的全向教育机器人套件，以实现更高级别的机器人学习。


<details>
  <summary>Details</summary>
Motivation: 现有教育机器人套件的感知能力仅限于自我中心视角，这使得学生难以学习涉及坐标或导航等更高级别的机器人问题。

Method: 引入一个教育用全向机器人套件，并配备一个外部跟踪系统。这种设计减轻了嵌入式系统的计算负担。

Result: 该套件使得学生能够探索传统机器人套件无法触及的高级机器人概念，例如坐标和导航。

Conclusion: 所提出的带外部跟踪系统的全向教育机器人套件克服了现有教育机器人的局限性，能够提供更深入、更全面的机器人学习体验。

Abstract: Recent developments of low cost off-the-shelf programmable components, their
modularity, and also rapid prototyping made educational robotics flourish, as
it is accessible in most schools today. They allow to illustrate and embody
theoretical problems in practical and tangible applications, and gather
multidisciplinary skills. They also give a rich natural context for
project-oriented pedagogy. However, most current robot kits all are limited to
egocentric aspect of the robots perception. This makes it difficult to access
more high-level problems involving e.g. coordinates or navigation. In this
paper we introduce an educational holonomous robot kit that comes with an
external tracking system, which lightens the constraint on embedded systems,
but allows in the same time to discover high-level aspects of robotics,
otherwise unreachable.

</details>


### [525] [SCOOP'D: Learning Mixed-Liquid-Solid Scooping via Sim2Real Generative Policy](https://arxiv.org/abs/2510.11566)
*Kuanning Wang,Yongchong Gu,Yuqian Fu,Zeyu Shangguan,Sicheng He,Xiangyang Xue,Yanwei Fu,Daniel Seita*

Main category: cs.RO

TL;DR: SCOOP'D是一种通过仿真（OmniGibson）收集舀取演示并利用扩散生成策略进行模仿学习的方法，在现实世界中实现了零样本泛化，并在机器人舀取任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 机器人舀取物品（如使用勺子）在日常生活中很常见，但开发通用的自主机器人舀取策略具有挑战性。这需要理解复杂的工具-物体交互，并且通常涉及操作可变形物体（如颗粒介质或液体），其无限维配置空间和复杂动力学使得问题更加困难。

Method: 本文提出SCOOP'D方法。它利用基于NVIDIA Omniverse构建的OmniGibson仿真平台，通过依赖特权状态信息的算法程序收集舀取演示。随后，该方法通过扩散模型生成策略，从观测输入中模仿这些演示。最后，将学习到的策略直接应用于各种现实世界场景进行测试。

Result: SCOOP'D在零样本部署中表现出有前景的结果，在465次不同场景（包括不同数量、特性和容器类型，以及“一级”和“二级”难度物体）的试验中取得了成功。该方法超越了所有基线和消融实验，证明了其有效性。

Conclusion: SCOOP'D是一种获取机器人舀取技能的有前景的方法，通过结合仿真演示和扩散生成策略，实现了在复杂现实世界场景中的有效零样本泛化。

Abstract: Scooping items with tools such as spoons and ladles is common in daily life,
ranging from assistive feeding to retrieving items from environmental disaster
sites. However, developing a general and autonomous robotic scooping policy is
challenging since it requires reasoning about complex tool-object interactions.
Furthermore, scooping often involves manipulating deformable objects, such as
granular media or liquids, which is challenging due to their
infinite-dimensional configuration spaces and complex dynamics. We propose a
method, SCOOP'D, which uses simulation from OmniGibson (built on NVIDIA
Omniverse) to collect scooping demonstrations using algorithmic procedures that
rely on privileged state information. Then, we use generative policies via
diffusion to imitate demonstrations from observational input. We directly apply
the learned policy in diverse real-world scenarios, testing its performance on
various item quantities, item characteristics, and container types. In
zero-shot deployment, our method demonstrates promising results across 465
trials in diverse scenarios, including objects of different difficulty levels
that we categorize as "Level 1" and "Level 2." SCOOP'D outperforms all
baselines and ablations, suggesting that this is a promising approach to
acquiring robotic scooping skills. Project page is at
https://scoopdiff.github.io/.

</details>


### [526] [Calibrated Dynamic Modeling for Force and Payload Estimation in Hydraulic Machinery](https://arxiv.org/abs/2510.11574)
*Lennart Werner,Pol Eyschen,Sean Costello,Pierluigi Micarelli,Marco Hutter*

Main category: cs.RO

TL;DR: 本文提出了一种高精度、可改装的2D力与载荷估算算法，用于液压挖掘机末端执行器，无需额外操作要求，通过在线识别动态模型实现，并优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 液压挖掘机末端执行器相互作用力的准确实时估计是重型机械高级自动化的关键，能改进精确平整和挖掘操作。

Method: 提出了一种高精度、可改装的2D力与载荷估算算法。该方法不要求操作员的轨迹、加速度或回转关节使用，只需最少校准，无需机器特定动态特性先验知识。通过正常操作中识别动态模型来估算末端执行器相互作用力和铲斗载荷，载荷估算基于优化方法，力测量基于压力和惯性测量。

Result: 在25吨标准挖掘机上，载荷估算达到1%的全量程精度。在线力测量方向精度为13度，大小精度为383牛。该方法在两种不同类型和重量级别的挖掘机平台上得到验证，其精度和泛化能力优于经典的准静态方法和商用系统。

Conclusion: 所提出的方法能够高精度、实时估算液压挖掘机的末端执行器相互作用力和铲斗载荷，具有良好的泛化能力和改装性，并且在精度和精确度方面优于现有解决方案，为重型机械的高级自动化奠定了基础。

Abstract: Accurate real-time estimation of end effector interaction forces in hydraulic
excavators is a key enabler for advanced automation in heavy machinery.
Accurate knowledge of these forces allows improved, precise grading and digging
maneuvers. To address these challenges, we introduce a high-accuracy,
retrofittable 2D force- and payload estimation algorithm that does not impose
additional requirements on the operator regarding trajectory, acceleration or
the use of the slew joint. The approach is designed for retrofittability,
requires minimal calibration and no prior knowledge of machine-specific dynamic
characteristics. Specifically, we propose a method for identifying a dynamic
model, necessary to estimate both end effector interaction forces and bucket
payload during normal operation. Our optimization-based payload estimation
achieves a full-scale payload accuracy of 1%. On a standard 25 t excavator, the
online force measurement from pressure and inertial measurements achieves a
direction accuracy of 13 degree and a magnitude accuracy of 383 N. The method's
accuracy and generalization capability are validated on two excavator platforms
of different type and weight classes. We benchmark our payload estimation
against a classical quasistatic method and a commercially available system. Our
system outperforms both in accuracy and precision.

</details>


### [527] [ManiAgent: An Agentic Framework for General Robotic Manipulation](https://arxiv.org/abs/2510.11660)
*Yi Yang,Kefan Gu,Yuqing Wen,Hebei Li,Yucheng Zhao,Tiancai Wang,Xudong Liu*

Main category: cs.RO

TL;DR: ManiAgent 是一种用于通用操作任务的智能体架构，通过智能体间通信进行感知、子任务分解和动作生成，解决了VLA模型在复杂推理和长周期任务中的局限性，实现了高成功率和高效数据收集。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在复杂推理和长周期任务规划方面的性能受限于数据稀缺性和模型容量。

Method: 引入了ManiAgent，一个用于通用操作任务的智能体架构。该框架包含多个智能体，它们通过相互通信执行环境感知、子任务分解和动作生成，实现从任务描述和环境输入到机器人操作动作的端到端输出。

Result: ManiAgent 在SimplerEnv基准测试中取得了86.8%的成功率，在真实世界的抓取放置任务中取得了95.8%的成功率。此外，它还能够高效地收集数据，训练出的VLA模型性能与使用人工标注数据集训练的模型相当。

Conclusion: ManiAgent 通过其智能体架构和智能体间通信机制，有效提升了机器人在复杂操作场景中的性能，并解决了VLA模型在数据和模型容量方面的挑战，实现了高效的数据收集和卓越的任务执行能力。

Abstract: While Vision-Language-Action (VLA) models have demonstrated impressive
capabilities in robotic manipulation, their performance in complex reasoning
and long-horizon task planning is limited by data scarcity and model capacity.
To address this, we introduce ManiAgent, an agentic architecture for general
manipulation tasks that achieves end-to-end output from task descriptions and
environmental inputs to robotic manipulation actions. In this framework,
multiple agents involve inter-agent communication to perform environmental
perception, sub-task decomposition and action generation, enabling efficient
handling of complex manipulation scenarios. Evaluations show ManiAgent achieves
an 86.8% success rate on the SimplerEnv benchmark and 95.8% on real-world
pick-and-place tasks, enabling efficient data collection that yields VLA models
with performance comparable to those trained on human-annotated datasets.The
project webpage is available at https://yi-yang929.github.io/ManiAgent/.

</details>


### [528] [Phys2Real: Fusing VLM Priors with Interactive Online Adaptation for Uncertainty-Aware Sim-to-Real Manipulation](https://arxiv.org/abs/2510.11689)
*Maggie Wang,Stephen Tian,Aiden Swann,Ola Shorinwa,Jiajun Wu,Mac Schwager*

Main category: cs.RO

TL;DR: Phys2Real提出了一种从真实世界到模拟再到真实世界的强化学习流程，它结合了视觉-语言模型推断的物理参数和通过不确定性感知融合进行的交互式适应，显著提高了机器人操作任务的模拟到真实迁移成功率。


<details>
  <summary>Details</summary>
Motivation: 在真实世界中学习机器人操作策略成本高昂且耗时。尽管在模拟中训练强化学习策略提供了一种可扩展的替代方案，但有效的模拟到真实世界迁移仍然具有挑战性，特别是对于需要精确动力学的任务。

Method: Phys2Real是一个从真实世界到模拟再到真实世界的强化学习流程，包含三个核心组件：1) 使用3D高斯泼溅进行高保真几何重建；2) 视觉-语言模型推断的物理参数先验分布；3) 从交互数据中进行的在线物理参数估计。该方法将策略建立在可解释的物理参数上，通过基于集成的不确定性量化，利用在线估计细化视觉-语言模型的预测。

Result: 在具有不同质心（CoM）的T形块平面推动任务和具有偏心质量分布的锤子推动任务中，Phys2Real相较于域随机化基线取得了显著改进：底部加权T形块的成功率从79%提高到100%；挑战性较大的顶部加权T形块的成功率从23%提高到57%；锤子推动任务的平均任务完成时间加快了15%。消融研究表明，视觉-语言模型和交互信息的结合对于成功至关重要。

Conclusion: 通过结合视觉-语言模型推断的物理参数先验和在线交互式适应，Phys2Real有效解决了需要精确动力学的机器人操作任务中的模拟到真实世界迁移挑战，显著提升了任务成功率和效率。

Abstract: Learning robotic manipulation policies directly in the real world can be
expensive and time-consuming. While reinforcement learning (RL) policies
trained in simulation present a scalable alternative, effective sim-to-real
transfer remains challenging, particularly for tasks that require precise
dynamics. To address this, we propose Phys2Real, a real-to-sim-to-real RL
pipeline that combines vision-language model (VLM)-inferred physical parameter
estimates with interactive adaptation through uncertainty-aware fusion. Our
approach consists of three core components: (1) high-fidelity geometric
reconstruction with 3D Gaussian splatting, (2) VLM-inferred prior distributions
over physical parameters, and (3) online physical parameter estimation from
interaction data. Phys2Real conditions policies on interpretable physical
parameters, refining VLM predictions with online estimates via ensemble-based
uncertainty quantification. On planar pushing tasks of a T-block with varying
center of mass (CoM) and a hammer with an off-center mass distribution,
Phys2Real achieves substantial improvements over a domain randomization
baseline: 100% vs 79% success rate for the bottom-weighted T-block, 57% vs 23%
in the challenging top-weighted T-block, and 15% faster average task completion
for hammer pushing. Ablation studies indicate that the combination of VLM and
interaction information is essential for success. Project website:
https://phys2real.github.io/ .

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [529] [Designing Control Barrier Functions Using a Dynamic Backup Policy](https://arxiv.org/abs/2510.09810)
*Victor Freire,Marco M. Nicotra*

Main category: eess.SY

TL;DR: 本文提出了一种系统方法，用于为受任意状态和输入约束的非线性控制仿射系统构建控制障碍函数（CBFs），该方法基于备份策略和增广状态-参考空间，并通过敏感性分析量化未来约束违反的距离。


<details>
  <summary>Details</summary>
Motivation: 为受任意状态和输入约束的非线性控制仿射系统构建有效的控制障碍函数是一个挑战，现有方法可能不够系统或通用。本文旨在提供一个系统的解决方案，并借鉴了参考调节器（reference governor）的思想。

Method: 该方法定义了一系列由系统平衡流形参数化的备份策略。控制障碍函数定义在增广的状态-参考空间上，用于量化在当前备份策略参考保持不变的情况下，未来任何时刻与约束违反的距离。通过敏感性分析计算增广状态向量的（可能非光滑的）雅可比矩阵。

Result: 本文提出了一种简单而通用的系统方法来构建控制障碍函数，能够处理非线性控制仿射系统中的任意状态和输入约束。该方法通过倒立摆在小车上的应用展示了其有效性。

Conclusion: 本文成功提出了一种系统且通用的方法，用于为受任意约束的非线性控制仿射系统构建控制障碍函数，并通过一个具体实例验证了其可行性和简洁性。

Abstract: This paper presents a systematic approach to construct control barrier
functions for nonlinear control affine systems subject to arbitrary state and
input constraints. Taking inspiration from the reference governor literature,
the proposed method defines a family of backup policies, parametrized by the
equilibrium manifold of the system. The control barrier function is defined on
the augmented state-and-reference space: given a state-reference pair, the
approach quantifies the distance to constraint violation at any time in the
future, should the current backup policy reference remain constant. Sensitivity
analysis is then used to compute the (possibly nonsmooth) Jacobian with respect
to the augmented state vector. To showcase its simple yet general nature, the
proposed method is applied to an inverted pendulum on cart.

</details>


### [530] [Sequential Convex Programming for 6-DoF Powered Descent Guidance with Continuous-Time Compound State-Triggered Constraints](https://arxiv.org/abs/2510.09610)
*Samet Uzun,Behcet Acikmese,John M. Carson III*

Main category: eess.SY

TL;DR: 本文提出了一种顺序凸规划（SCP）框架，结合D-GMSR和CT-SCvx方法，用于解决动力下降制导（PDG）问题中连续时间复合状态触发约束的满足。


<details>
  <summary>Details</summary>
Motivation: 在动力下降制导（PDG）问题中，需要确保连续时间满足复杂的、复合的状态触发约束（一种逻辑规范），这通常难以实现。

Method: 该框架结合了两种主要方法：1. 广义均值平滑鲁棒性度量（D-GMSR），用于将离散时间时序和逻辑规范参数化为平滑函数。2. 连续时间逐次凸化（CT-SCvx）方法，用于实时约束轨迹优化，并保证连续时间约束的满足和收敛。CT-SCvx具体包括：连续时间路径约束重构、时间膨胀（将自由末端时间问题转为固定末端时间问题）、多重打靶精确离散化、精确惩罚函数处理非凸约束，以及近端线性方法求解最终的有限维非凸PDG问题。

Result: 通过数值模拟验证了所提出框架的有效性。

Conclusion: 该论文提出了一个稳健高效的SCP框架，能够处理PDG问题中的连续时间复合状态触发约束，并具有理论保证和经过数值模拟验证的有效性。

Abstract: This paper presents a sequential convex programming (SCP) framework for
ensuring the continuous-time satisfaction of compound state-triggered
constraints, a subset of logical specifications, in the powered descent
guidance (PDG) problem. The proposed framework combines the generalized
mean-based smooth robustness measure (D-GMSR), a parameterization technique
tailored for expressing discrete-time temporal and logical specifications
through smooth functions, with the continuous-time successive convexification
(CT-SCvx) method, a real-time solution for constrained trajectory optimization
that guarantees continuous-time constraint satisfaction and convergence. The
smoothness of the temporal and logical specifications parameterized via D-GMSR
enables solving the resulting optimization problem with robust and efficient
SCP algorithms while preserving theoretical guarantees. In addition to their
smoothness, the parameterized specifications are sound and complete, meaning
the specification holds if and only if the constraint defined by the
parameterized function is satisfied. The CT-SCvx framework is then applied to
solve the parameterized problem, incorporating: (1) reformulation for
continuous-time path constraint satisfaction, (2) time-dilation to transform
the free-final-time PDG problem into a fixed-final-time problem, (3) multiple
shooting for exact discretization, (4) exact penalty functions for penalizing
nonconvex constraints, and (5) the prox-linear method, a convergence-guaranteed
SCP algorithm, to solve the resulting finite-dimensional nonconvex PDG problem.
The effectiveness of the framework is demonstrated through a numerical
simulation. The implementation is available at
https://github.com/UW-ACL/CT-cSTC

</details>


### [531] [Latent-Feature-Informed Neural ODE Modeling for Lightweight Stability Evaluation of Black-box Grid-Tied Inverters](https://arxiv.org/abs/2510.09826)
*Jialin Zheng,Zhong Liu,Xiaonan Lu*

Main category: eess.SY

TL;DR: 本文提出一种名为LFI-NODE的轻量级方法，通过潜在特征信息神经网络常微分方程模型，直接从轨迹数据评估黑盒并网逆变器的稳定性，显著减少了数据需求。


<details>
  <summary>Details</summary>
Motivation: 黑盒并网逆变器的稳定性评估对电网可靠性至关重要，但传统识别技术存在数据需求量大且受限于专有内部结构的问题。

Method: 本文提出潜在特征信息神经网络常微分方程（LFI-NODE）建模方法。该方法用一个连续时间神经网络参数化整个系统常微分方程，能够捕获非线性大信号动态，并从轨迹中提取潜在扰动特征，以揭示小信号特征结构。

Result: LFI-NODE所需训练样本量比传统方法少一到两个数量级，且只需48个短瞬态数据即可实现百分位数的轨迹预测误差和十分位数的特征值估计误差，性能优于基准方法一到两个数量级。

Conclusion: LFI-NODE为复杂黑盒电力电子系统的高保真稳定性评估提供了一种实用且轻量级的方法。

Abstract: Stability evaluation of black-box grid-tied inverters is vital for grid
reliability, yet identification techniques are both data-hungry and blocked by
proprietary internals. {To solve this, this letter proposes a
latent-feature-informed neural ordinary differential equation (LFI-NODE)
modeling method that can achieve lightweight stability evaluation directly from
trajectory data.} LFI-NODE parameterizes the entire system ODE with a single
continuous-time neural network, allowing each new sample to refine a unified
global model. It faithfully captures nonlinear large-signal dynamics to
preserve uniform predictive accuracy as the inverter transitions between
operating points. Meanwhile, latent perturbation features distilled from every
trajectory steer the learning process and concurrently reveal the small-signal
eigenstructure essential for rigorous stability analysis. Validated on a
grid-forming inverter, {The LFI-NODE requires one to two orders of magnitude
fewer training samples compared with traditional methods, collected from short
time-domain trajectories instead of extensive frequency-domain measurements.}
{Furthermore, the LFI-NODE requires only 48 short transients to achieve a
trajectory prediction error at the hundredth level and an eigenvalue estimation
error at the tenth level, outperforming benchmark methods by one to two orders
of magnitude.} This makes LFI-NODE a practical and lightweight approach for
achieving high-fidelity stability assessment of complex black-box
power-electronic systems.

</details>


### [532] [Cyber-Physical Systems on the Megawatt Scale: The impact of battery control on grid frequency stability](https://arxiv.org/abs/2510.09862)
*Carsten Hartmann,Edoardo De Din,Daniele Carta,Florian Middelkoop,Arndt Neubauer,Johannes Kruse,Ulrich Oberhofer,Richard Jumar,Benjamin Schäfer,Thiemo Pesch,Andrea Benigni,Dirk Witthaut*

Main category: eess.SY

TL;DR: 研究发现全球电网频率存在持续一分钟的振荡模式，其源头是电池储能系统的能量管理系统，且在北欧和英国电网中振幅显著增加。这可能对未来低惯量电网的稳定性构成风险，但可通过改进电池控制算法来缓解。


<details>
  <summary>Details</summary>
Motivation: 电力系统正在经历根本性变革，逆变器发电挑战频率稳定性，数字化增加了错误和攻击的脆弱性。本文旨在识别网络物理耦合和控制系统设计交叉点的新兴风险，特别是解释一个长期未明的电网频率持续一分钟振荡模式。

Method: 分析全球电网频率时间序列数据，并追溯振荡模式的来源。

Result: 全球电网频率普遍存在持续一分钟的振荡模式，其源头被追溯到电池储能系统的能量管理系统。该模式的振幅在北欧和英国电网中已大幅增加。

Conclusion: 这种效应可能对未来低惯量、高电池和智能设备渗透率的电网稳定性构成潜在负担，但可以通过修订电池控制算法来缓解。

Abstract: Electric power systems are undergoing fundamental change. The shift to
inverter-based generation challenges frequency stability, while growing
digitalisation heightens vulnerability to errors and attacks. Here we identify
an emerging risk at the intersection of cyber-physical coupling and control
system design. We show that grid frequency time series worldwide exhibit a
persistent one-minute oscillatory pattern, whose origin has remained largely
unexplained. We trace this pattern back to the energy management systems of
battery electric storage systems and demonstrate that the pattern amplitude has
increased substantially in the Nordic and British grids. We argue that this
effect is a potential burden for stability in future grids with low inertia and
an increasing penetration with batteries and smart devices, though it can be
mitigated by a revision of battery control algorithms.

</details>


### [533] [Computing Safe Control Inputs using Discrete-Time Matrix Control Barrier Functions via Convex Optimization](https://arxiv.org/abs/2510.09925)
*James Usevitch,Juan Augusto Paredes Salazar,Ankit Goel*

Main category: eess.SY

TL;DR: 针对离散时间系统中非凹安全集导致的非凸优化问题，本文提出了一种利用凸优化和矩阵控制障碍函数技术计算安全控制输入的新方法，并在双旋翼飞行器上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 离散时间控制障碍函数（CBFs）在处理非凹参数时，需要解决非凸优化问题来计算安全控制输入，这阻碍了实时计算并保证前向不变性。

Method: 本文提出了一种新颖的方法，利用凸优化和最新发展的矩阵控制障碍函数（Matrix CBF）技术，为具有非凸安全集的离散时间系统计算安全保持的控制输入。

Result: 通过在双旋翼飞行器系统上的数值仿真，证明了所提出方法的有效性。

Conclusion: 该方法成功地解决了离散时间系统中非凹安全集导致的非凸优化问题，通过凸优化和矩阵CBF技术实现了安全控制输入的实时计算，确保了系统的安全性。

Abstract: Control barrier functions (CBFs) have seen widespread success in providing
forward invariance and safety guarantees for dynamical control systems. A
crucial limitation of discrete-time formulations is that CBFs that are
nonconcave in their argument require the solution of nonconvex optimization
problems to compute safety-preserving control inputs, which inhibits real-time
computation of control inputs guaranteeing forward invariance. This paper
presents a novel method for computing safety-preserving control inputs for
discrete-time systems with nonconvex safety sets, utilizing convex optimization
and the recently developed class of matrix control barrier function techniques.
The efficacy of our methods is demonstrated through numerical simulations on a
bicopter system.

</details>


### [534] [Viscosity CBFs: Bridging the Control Barrier Function and Hamilton-Jacobi Reachability Frameworks in Safe Control Theory](https://arxiv.org/abs/2510.09929)
*Dylan Hirsch,Jaime Fernández Fisac,Sylvia Herbert*

Main category: eess.SY

TL;DR: 本文通过引入非可微的“粘性”控制障碍函数（CBF），并将其与控制障碍值函数（CB-VF）建立联系，从而在理论上弥合了CBF和Hamilton-Jacobi可达性（HJR）这两个安全控制框架之间的鸿沟。


<details>
  <summary>Details</summary>
Motivation: 传统的控制障碍函数（CBF）和Hamilton-Jacobi可达性（HJR）是安全控制的两个核心框架，但一直被视为独立的存在。尽管先前的工作引入了CB-VF以连接HJR与CBF的特性，但本研究旨在从CBF的角度出发，通过泛化CBF来进一步深化这两个框架的联系。

Method: 研究方法包括：1) 将CBF泛化为非可微的“粘性”CBF。2) 证明粘性CBF与CB-VF之间的深层联系。3) 通过这种联系，刻画粘性CBF的特性。4) 扩展CB-VF到非指数反折扣形式，并更新相应的理论。

Result: 研究结果表明：1) 粘性CBF正是那些能够提供CBF式安全保证（控制不变性和平滑接近边界）的函数。2) 粘性CBF具有理想的理论性质，包括在最大值和极限运算下的闭合性。3) 扩展了CB-VF的理论，使其适用于非指数反折扣情境。

Conclusion: 本文通过引入粘性CBF并揭示其与CB-VF的深层联系，成功地在理论上建立了CBF和HJR框架之间的桥梁。这不仅统一了两个主要的安全性框架，还为粘性CBF提供了丰富的理论特性，并扩展了CB-VF的适用范围，为安全控制领域提供了更坚实的理论基础。

Abstract: Control barrier functions (CBFs) and Hamilton-Jacobi reachability (HJR) are
central frameworks in safe control. Traditionally, these frameworks have been
viewed as distinct, with the former focusing on optimally safe controller
design and the latter providing sufficient conditions for safety. A previous
work introduced the notion of a control barrier value function (CB-VF), which
is defined similarly to the other value functions studied in HJR but has
certain CBF-like properties. In this work, we proceed the other direction by
generalizing CBFs to non-differentiable ``viscosity'' CBFs. We show the deep
connection between viscosity CBFs and CB-VFs, bridging the CBF and HJR
frameworks. Through this bridge, we characterize the viscosity CBFs as
precisely those functions which provide CBF-like safety guarantees (control
invariance and smooth approach to the boundary). We then further show nice
theoretical properties of viscosity CBFs, including their desirable closure
under maximum and limit operations. In the process, we also extend CB-VFs to
non-exponential anti-discounting and update the corresponding theory for CB-VFs
along these lines.

</details>


### [535] [Modeling the Impact of Communication and Human Uncertainties on Runway Capacity in Terminal Airspace](https://arxiv.org/abs/2510.09943)
*Yutian Pang,Andrew Kendall,John-Paul Clarke*

Main category: eess.SY

TL;DR: 本研究探讨了通信和人类表现不确定性对跑道运行（特别是汇合流情景）的影响，并提出了一种跑道容量估算方法，发现自动化操作可减轻通信不确定性的负面影响。


<details>
  <summary>Details</summary>
Motivation: 研究通信和人类表现不确定性对跑道运行（具体是两个汇合气流的进近场景）的潜在影响。

Method: 1. 构建基于蒙特卡洛的离散时间仿真模型，模拟修改后的泊松分布飞机到达，并考虑最小间隔和跑道占用时间。2. 模拟合并逻辑，包括转弯、飞行员响应延迟和动态间隔可用性。3. 调查自动末端进近引导模型（Auto-ATC），利用逆向最优控制从人类专家记录中学习决策建议。4. 通过跑道吞吐量、直接合并的下风向飞机比例以及下风向段的平均延误来衡量跑道容量。

Result: 航空通信链路不确定性显著影响当前基于语音操作的跑道容量，但在自动化操作环境下，这种影响可以得到缓解。

Conclusion: 本研究提供了一种汇合场景下跑道容量估算方法，并证明了航空通信链路不确定性对现有语音操作的跑道容量有显著影响，而自动化操作可以有效减轻这种影响。

Abstract: We investigate the potential impact of communication and human performance
uncertainties on runway operations. Specifically, we consider these impacts
within the context of an arrival scenario with two converging flows: a
straight-in approach stream and a downwind stream merging into it. Both arrival
stream are modeled using a modified Possion distribution that incorporate the
separation minima as well as the runway occupancy time. Various system level
uncertainties are addressed in this process, including communication link- and
human-related uncertainties. In this research, we first build a Monte
Carlo-based discrete-time simulation, where aircraft arrivals are generated by
modified Poisson processes subject to minimum separation constraints,
simulating various traffic operations. The merging logic incorporates standard
bank angle continuous turn-to-final, pilot response delays, and dynamic gap
availability in real time. Then, we investigate an automated final approach
vectoring model (i.e., Auto-ATC), in which inverse optimal control is used to
learn decision advisories from human expert records. By augmenting trajectories
and incorporating the aforementioned uncertainties into the planning scenario,
we create a setup analogous to the discrete event simulation. For both studies,
runway capacity is measured by runway throughput, the fraction of downwind
arrivals that merge immediately without holding, and the average delay (i.e.,
holding time/distance) experienced on the downwind leg. This research provides
a method for runway capacity estimation in merging scenarios, and demonstrates
that aeronautical communication link uncertainties significantly affect runway
capacity in current voice-based operations, whereas the impact can be mitigated
in autonomous operational settings.

</details>


### [536] [Performance Index Shaping for Closed-loop Optimal Control](https://arxiv.org/abs/2510.10202)
*Ayush Rai,Shaoshuai Mou,Brian D. O. Anderson*

Main category: eess.SY

TL;DR: 本文提出了一种新颖的分析框架，将性能指标与闭环最优控制律关联起来，将逆向问题从双层转化为单层，解决了传统微分轨迹优化计算成本高的问题，并提供了系统设计和稳定性分析的见解。


<details>
  <summary>Details</summary>
Motivation: 传统的微分轨迹优化方法在推断性能指标时，需要反复求解最优控制问题，计算成本高昂。作者受启发于一个问题：当性能指标被修改时，求解无限 horizon 最优控制问题的闭环控制律会如何变化？

Method: 假设动力学已知，本文提出了一种新颖的分析框架，将性能指标与由此产生的闭环最优控制律进行解析连接。这使得通常的双层逆向问题转化为可处理的单层公式。此外，还提出了一种系统化的方法来指导性能指标的塑造，以实现特定的设计目标。

Result: 该方法为广泛的系统和性能指标类别提供了闭式表征，不仅有助于解释和稳定性分析，还提供了对所得非线性闭环系统的鲁棒稳定性和输入到状态稳定行为的深入见解。这种分析视角还使其方法能够推广到多样化的设计目标，形成一个统一的性能指标塑造框架。

Conclusion: 该分析框架提供了一种计算效率更高的方法来设计最优控制律，通过系统地塑造性能指标，从而深入理解系统的稳定性和鲁棒行为。它将逆向问题转化为可处理的单层公式，并为性能指标塑造提供了一个统一的框架。

Abstract: The design of the performance index, also referred to as cost or reward
shaping, is central to both optimal control and reinforcement learning, as it
directly determines the behaviors, trade-offs, and objectives that the
resulting control laws seek to achieve. A commonly used approach for this
inference task in recent years is differentiable trajectory optimization, which
allows gradients to be computed with respect to cost parameters by
differentiating through an optimal control solver. However, this method often
requires repeated solving of the underlying optimal control problem at every
iteration, making the method computationally expensive. In this work, assuming
known dynamics, we propose a novel framework that analytically links the
performance index to the resulting closed-loop optimal control law, thereby
transforming a typically bi-level inverse problem into a tractable single-level
formulation. Our approach is motivated by the question: given a closed-loop
control law that solves an infinite-horizon optimal control problem, how does
this law change when the performance index is modified with additional terms?
This formulation yields closed-form characterizations for broad classes of
systems and performance indices, which not only facilitate interpretation and
stability analysis, but also provide insight into the robust stability and
input-to-state stable behavior of the resulting nonlinear closed-loop system.
Moreover, this analytical perspective enables the generalization of our
approach to diverse design objectives, yielding a unifying framework for
performance index shaping. Given specific design objectives, we propose a
systematic methodology to guide the shaping of the performance index and
thereby design the resulting optimal control law.

</details>


### [537] [Bounds of Validity for Bifurcations of Equilibria in a Class of Networked Dynamical Systems](https://arxiv.org/abs/2510.10215)
*Pranav Gupta,Ravi Banavar,Anastasia Bizyaeva*

Main category: eess.SY

TL;DR: 本文为两类连续时间网络动力系统（类似于Hopfield和发放率动态神经网络）的分岔图分类建立了明确的有效性界限，并严格刻画了分岔点附近降阶模型预测可靠的邻域。


<details>
  <summary>Details</summary>
Motivation: 局部分岔分析在理解网络非线性动力系统（包括动态神经网络和意见动力学模型）的定性转变中至关重要。需要明确降阶模型在分岔点附近预测的可靠性范围。

Method: 该方法利用了Lyapunov-Schmidt约化方法有效性界限计算的最新进展，Lyapunov-Schmidt约化是非线性系统分析中广泛使用的降阶方法。

Result: 研究建立了两类连续时间网络动力系统分岔图分类的明确有效性界限，并利用这些界限严谨地刻画了分岔点附近降阶模型预测可靠的邻域。此外，还在k-正则图上的非线性意见动力学模型中展示了这些界限的应用。

Conclusion: 这些结果为量化网络系统动力学中分岔现象的鲁棒性提供了新的分析工具，并强调了网络结构与非线性动力学行为之间的相互作用。

Abstract: Local bifurcation analysis plays a central role in understanding qualitative
transitions in networked nonlinear dynamical systems, including dynamic neural
network and opinion dynamics models. In this article we establish explicit
bounds of validity for the classification of bifurcation diagrams in two
classes of continuous-time networked dynamical systems, analogous in structure
to the Hopfield and the Firing Rate dynamic neural network models. Our approach
leverages recent advances in computing the bounds for the validity of
Lyapunov-Schmidt reduction, a reduction method widely employed in nonlinear
systems analysis. Using these bounds we rigorously characterize neighborhoods
around bifurcation points where predictions from reduced-order models remain
reliable. We further demonstrate how these bounds can be applied to an
illustrative family of nonlinear opinion dynamics on k-regular graphs, which
emerges as a special case of the general framework. These results provide new
analytical tools for quantifying the robustness of bifurcation phenomena in
dynamics over networked systems and highlight the interplay between network
structure and nonlinear dynamical behavior.

</details>


### [538] [Optimal monophasic, asymmetric electric field pulses for selective transcranial magnetic stimulation (TMS) with minimised power and coil heating](https://arxiv.org/abs/2510.10289)
*Ke Ma,Andrey Vlasov,Zeynep B. Simsek,Jinshui Zhang,Yiru Li,Boshuo Wang,David L. K. Murphy,Jessica Y. Choi,Maya E. Clinton,Noreen Bukhari-Parlakturk,Angel V. Peterchev,Stephan M. Goetz*

Main category: eess.SY

TL;DR: 本文开发了一个优化框架，生成了高能效的非对称经颅磁刺激（TMS）脉冲，显著降低了能耗和线圈发热，同时保持了神经激活的方向选择性。


<details>
  <summary>Details</summary>
Motivation: 传统的非对称电场脉冲（如单相脉冲）在神经激活方面具有方向选择性，但能耗过高。以往的脉冲形状优化仅限于对称脉冲或受限的传统波形变体，未能实现能量效率或神经选择性方面的普遍最优。

Method: 研究人员实施了一个优化框架，该框架结合了神经元模型激活约束并灵活控制脉冲不对称性，以优化电场波形。随后进行了人体实验，评估了优化脉冲在运动阈值、能耗和方向选择性（通过MEP潜伏期差异）方面的表现。

Result: 优化后的电场波形相比传统单相脉冲和先前改进的单相等效脉冲，能量损失减少高达92%，线圈发热减少高达88%。在人体实验中，优化脉冲在AP和PA方向上的运动阈值与单相脉冲相似，但能耗显著降低，尤其是在AP方向。此外，优化脉冲在AP和PA方向之间产生了（1.79 ± 0.41）ms的显著MEP潜伏期差异，表明具有方向选择性。

Conclusion: 该框架成功识别出高能效的非对称脉冲，可实现方向选择性的神经激活。这些脉冲能够支持选择性快速重复经颅磁刺激（rTMS）方案，减少功耗和线圈发热，有望提高神经调控的精确性和效力。

Abstract: Transcranial magnetic stimulation (TMS) with asymmetric electric field
pulses, such as monophasic, offers directional selectivity for neural
activation but requires excessive energy. Previous pulse shape optimisation has
been limited to symmetric pulses or heavily constrained variations of
conventional waveforms without achieving general optimality in energy
efficiency or neural selectivity. We implemented an optimisation framework that
incorporates neuron model activation constraints and flexible control of pulse
asymmetry. The optimised electric field waveforms achieved up to 92 % and 88 %
reduction in energy loss and thus coil heating respectively compared to
conventional monophasic pulses and previously improved monophasic-equivalent
pulses. In the human experiments, OUR pulses showed similar motor thresholds to
monophasic pulses in both AP and PA directions with significantly lower energy
loss, particularly in the AP direction. Moreover, there was a significant MEP
latency difference of (1.79 +/- 0.41) ms between AP and PA direction with OUR
pulses, which suggests directional selectivity. Our framework successfully
identified highly energy-efficient asymmetric pulses for
directionally-selective neural engagement. These pulses can enable selective
rapid-rate repetitive TMS protocols with reduced power consumption and coil
heating, with potential benefits for precision and potency of neuro-modulation.

</details>


### [539] [Low-cost Pyranometer-Based ANN Approach for MPPT in Solar PV Systems](https://arxiv.org/abs/2510.10313)
*Luiz Fernando M. Arruda,Moises Ferber,Diego Greff*

Main category: eess.SY

TL;DR: 本文提出一种基于低成本日射强度计和人工神经网络（ANN）的光伏系统最大功率点跟踪（MPPT）方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过利用经济实惠的传感器技术，提高光伏系统最大功率点跟踪的准确性、响应速度和效率，并减少功率波动。

Method: 该方法整合了日射强度计、温度传感器和ANN，用于估算DC/DC变换器的占空比。它被应用于Cuk变换器的局部控制中，并与传统的扰动观测（P&O）方法进行了实验验证。

Result: 实验结果表明，基于ANN的技术，结合经济实惠的传感器技术，能够实现准确的MPPT性能，减少波动，并提高光伏跟踪系统的响应速度和效率。

Conclusion: 利用低成本传感器和人工神经网络的光伏系统最大功率点跟踪方法，能够有效提升系统性能，优于传统方法，具有实际应用价值。

Abstract: This article presents a study on the application of artificial neural
networks (ANNs) for maximum power point tracking (MPPT) in photovoltaic (PV)
systems using low-cost pyranometer sensors. The proposed approach integrates
pyranometers, temperature sensors, and an ANN to estimate the duty cycle of a
DC/DC converter, enabling the system to consistently operate at its maximum
power point. The strategy was implemented in the local control of a Cuk
converter and experimentally validated against the conventional Perturb and
Observe (P&O) method. Results demonstrate that the ANN-based technique,
leveraging affordable sensor technology, achieves accurate MPPT performance
with reduced fluctuations, enhancing the responsiveness and efficiency of PV
tracking systems.

</details>


### [540] [Discovering interpretable piecewise nonlinear model predictive control laws via symbolic decision trees](https://arxiv.org/abs/2510.10411)
*Ilias Mitrai*

Main category: eess.SY

TL;DR: 本文提出使用符号决策树作为代理模型来近似模型预测控制（MPC）律，生成可解释的分段非线性控制律，并在保持性能的同时实现较低的预测误差。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发可解释的控制律，以近似复杂的模型预测控制律，同时保持其控制性能。

Method: 该方法提出符号决策树，同时学习输入域的划分（分裂逻辑）和预测控制动作的局部非线性表达式（使用一组基函数建模）。学习任务被表述为一个混合整数优化问题，并通过最先进的全局优化求解器求解以达到全局最优。

Result: 将所提出的方法应用于一个等温反应器控制案例研究。结果表明，该方法能够准确学习控制律，闭环性能与标准模型预测控制器相当。与现有可解释模型相比，符号树实现了更低的预测误差和更优越的闭环性能。

Conclusion: 符号决策树可以作为有效的代理模型，用于近似模型预测控制律，生成可解释的分段非线性控制律，并能提供与标准MPC相当的闭环性能，同时优于现有可解释模型。

Abstract: In this paper, we propose symbolic decision trees as surrogate models for
approximating model predictive control laws. The proposed approach learns
simultaneously the partition of the input domain (splitting logic) as well as
local nonlinear expressions for predicting the control action leading to
interpretable piecewise nonlinear control laws. The local nonlinear expressions
are determined by the learning problem and are modeled using a set of basis
functions. The learning task is posed as a mixed integer optimization, which is
solved to global optimality with state-of-the-art global optimization solvers.
We apply the proposed approach to a case study regarding the control of an
isothermal reactor. The results show that the proposed approach can learn the
control law accurately, leading to closed-loop performance comparable to that
of a standard model predictive controller. Finally, comparison with existing
interpretable models shows that the symbolic trees achieve both lower
prediction error and superior closed-loop performance.

</details>


### [541] [Risk-Budgeted Control Framework for Balanced Performance and Safety in Autonomous Vehicles](https://arxiv.org/abs/2510.10442)
*Pei Yu Chang,Vishnu Renganathan,Qadeer Ahmed*

Main category: eess.SY

TL;DR: 本文提出了一种带控制框架的风险预算监控器，通过滑动窗口监测安全裕度，并在安全裕度恶化时，根据可行性触发和质量触发条件，在基于性能的R-CBF和保守的CVaR-CBF之间切换，以确保自动驾驶安全。该框架在MPC中验证，在不确定性场景下实现了高碰撞避免率和快速轨迹恢复。


<details>
  <summary>Details</summary>
Motivation: 确保自动驾驶的安全性，特别是在存在定位噪声和障碍物位置不确定性等情况下，当安全裕度不足或存在非零尾部风险时，能够有效应对安全隐患。

Method: 提出了一种带控制框架的风险预算监控器。该监控器使用滑动窗口来监测障碍物残差不足或非零尾部风险。当安全裕度恶化时，它会触发安全约束从基于性能的R-CBF切换到保守的CVaR-CBF。切换由两个实时触发条件控制：可行性触发（FT）和质量触发（QT）。该框架通过模型预测控制器（MPC）进行评估，考虑了自动驾驶车辆（AV）定位噪声和障碍物位置不确定性，并进行了多AV-行人交互场景的蒙特卡洛模拟。

Result: 在行人检测不确定性为5米的最具挑战性设置下，所提出的框架在300次试验中实现了94-96%的无碰撞成功率，同时保持了参考路径的最低平均横向误差（CTE = 3.2-3.6米）。较低的CTE表明避障后轨迹恢复更快。

Conclusion: 所提出的框架有效地平衡了安全性和性能，在存在不确定性的自动驾驶场景中，通过高碰撞避免成功率和快速轨迹恢复，证明了其在认证自动驾驶安全方面的有效性。

Abstract: This paper presents a risk-budgeted monitor with a control framework that
certifies safety for autonomous driving. In this process, a sliding window is
proposed to monitor for insufficient barrier residuals or nonzero tail risk,
ensuring system safety. When the safety margin deteriorates, it triggers
switching the safety constraint from a performance-based relaxed-control
barrier function (R-CBF) to a conservative conditional value at risk (CVaR-CBF)
to address the safety concern. This switching is governed by two real-time
triggers: Feasibility-Triggered (FT) and Quality-Triggered (QT) conditions. In
the FT condition, if the R-CBF constraint becomes infeasible or yields a
suboptimal solution, the risk monitor triggers the use of the CVaR constraints
for the controller. In the QT condition, the risk monitor observes the safety
margin of the R-CBF solution at every step, regardless of feasibility. If it
falls below the safety margin, the safety filter switches to the CVaR-CBF
constraints.
  The proposed framework is evaluated using a model predictive controller (MPC)
for autonomous driving in the presence of autonomous vehicle (AV) localization
noise and obstacle position uncertainties. Multiple AV-pedestrian interaction
scenarios are considered, with 1,500 Monte Carlo runs conducted for all
scenarios. In the most challenging setting with pedestrian detection
uncertainty of 5 m, the proposed framework achieves a 94-96% success rate of
not colliding with the pedestrians over 300 trials while maintaining the lowest
mean cross-track error (CTE = 3.2-3.6 m) to the reference path. The reduced CTE
indicates faster trajectory recovery after obstacle avoidance, demonstrating a
balance between safety and performance.

</details>


### [542] [Controller for Incremental Input-to-State Practical Stabilization of Partially Unknown systems with Invariance Guarantees](https://arxiv.org/abs/2510.10450)
*P Sangeerth,David Smith Sundarsingh,Bhabani Shankar Dey,Pushpak Jagtap*

Main category: eess.SY

TL;DR: 本文引入增量输入到状态实际稳定性（δ-ISpS）概念，并提出一种基于反馈线性化和高斯过程回归的控制设计方案，以确保部分未知系统的增量稳定性与安全性。


<details>
  <summary>Details</summary>
Motivation: 确保动态系统的安全性，即轨迹相互收敛而非收敛于固定平衡点或轨迹，尤其是在存在未知动力学的情况下。

Method: 引入增量输入到状态实际稳定性（δ-ISpS）的概念；采用基于反馈线性化的控制设计方案；利用高斯过程回归处理未知动力学近似模型。

Result: 提出了一种控制设计方案，能够使部分未知系统实现增量输入到状态实际稳定（δ-ISpS）和安全性，并提供形式化保证。

Conclusion: 所提出的控制器方案已成功在一个机械手示例上实现，验证了其有效性。

Abstract: Incremental stability is a property of dynamical systems that ensures the
convergence of trajectories with respect to each other rather than a fixed
equilibrium point or a fixed trajectory. In this paper, we introduce a related
stability notion called incremental input-to-state practical stability
({\delta}-ISpS), ensuring safety guarantees. We also present a feedback
linearization based control design scheme that renders a partially unknown
system incrementally input-to-state practically stable and safe with formal
guarantees. To deal with the unknown dynamics, we utilize Gaussian process
regression to approximate the model. Finally, we implement the controller
synthesized by the proposed scheme on a manipulator example

</details>


### [543] [Transforming Tarlac State University (TSU) Gymnasium to a Nearly Zero-Energy Building through Integration of a Solar Photovoltaic (PV) System](https://arxiv.org/abs/2510.10552)
*Rafael R. Yumul,Enalyn T. Domingo*

Main category: eess.SY

TL;DR: 本研究旨在将一所大学体育馆改造为近零能耗建筑（NZEB），通过整合太阳能光伏系统并升级现有电气基础设施，以提高能源效率并减少碳足迹，并评估其经济可行性。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于近零能耗建筑（NZEB）的原则，旨在通过改造大学体育馆，使其配备节能设备，利用可再生能源（太阳能光伏系统）来减少碳足迹，并解决体育馆现有电气基础设施老化、照明效率低下以及冷却设备不足的问题。

Method: 研究方法包括对体育馆现有电气基础设施（如照明、插座、冷却设备）进行分析，提出整合太阳能光伏系统作为可再生能源，并进行经济分析以确定投资回收期。同时，考虑了工程设计标准下的额外升级成本。

Result: 研究发现体育馆的电气基础设施陈旧，照明效率低下，电源插座不足，冷却设备也不足。分析表明，项目的投资回收期处于平均范围，使其成为一项具有成本效益的投资。然而，除了光伏系统成本外，还需考虑更换照明、增加空调和插座等额外升级的成本。建议考虑现有屋顶系统的完整性。

Conclusion: 该项目对大学而言是一项具有成本效益的投资，但必须将整个电气系统（包括新照明、冷却和便利负载）的全部实施成本纳入计算，以确定NZEB项目的总成本。在评估项目可行性时，应充分考虑这些额外成本和现有屋顶系统的完整性。建议纳入其他经济评估因素以获得更严谨的结果。

Abstract: The study is anchored to the principles of Nearly-Zero Energy Building
(NZEB). It aimed to transform the Tarlac State University Gymnasium into a
facility with energy-efficient equipment to contribute to reducing carbon
footprints by integrating a solar PV system as its renewable energy source. The
researchers found out that the electrical infrastructure of the Gym was
outdated, and the lighting was not energy efficient, and there were too few
convenience or power outlets. There was also insufficient cooling equipment to
maintain a comfortable temperature. Analysis shows that the payback period is
within the average range, making it a cost-effective investment for the
University. Aside from the cost of the PV System, adherence to engineering
design standards will mean additional costs to replace the metal halides with
LED high bay lamps, installation of additional air conditioning units, and
provision of additional convenience outlets. These additional costs should be
considered when evaluating the feasibility of the project. It is recommended
that the integrity of the existing roof system of the Gymnasium be considered.
The total cost of putting up the whole electrical system, including new
lighting, cooling, and convenience loads, must be calculated to determine the
total cost of implementing the whole NZEB project. Other factors in the
economic evaluation may be considered to determine a more stringent result.

</details>


### [544] [Aggregate Modeling of Air-Conditioner Loads Under Packet-based Control with Both On and Off Grid Access Requests](https://arxiv.org/abs/2510.10651)
*Mohammad Hassan,Mads R. Almassalkhi*

Main category: eess.SY

TL;DR: 本文提出了一种新的基于聚合箱（宏观）模型，用于分布式能源（DER）的包化能量管理（PEM），该模型整合了开启和关闭请求功能，并支持可变数据包长度，以更准确地表征DER群体的灵活性和电网可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有的PEM模型在聚合DER分析中存在局限性，如固定数据包持续时间、仅考虑开启请求，以及可变数据包长度的努力未能进行聚合建模，这限制了系统性分析和对可用灵活性及斜坡能力提升的量化。

Method: 本文提出了一种新的基于聚合箱（宏观）模型，用于PEM负载。该模型融合了开启和关闭请求功能，能够准确刻画DER群体的功率参考信号跟踪能力、群体温度动态、聚合请求速率以及可变数据包长度。通过与基于代理（微观）模型的仿真进行验证，以评估模型的鲁棒性和量化准确性。最后，将宏观模型仿真中得到的可变数据包长度分布应用于以往关于随机数据包长度的PEM研究。

Result: 新的聚合模型能够准确表征DER群体跟踪功率参考信号、人口温度动态、聚合请求速率和可变数据包长度的能力。通过与基于代理的微观模型进行仿真验证，证实了模型的鲁棒性和准确性。

Conclusion: 该研究通过引入一个综合的聚合模型，克服了PEM研究中固定数据包长度和仅考虑开启请求的限制，实现了对包含开启/关闭请求和可变数据包长度的DER聚合行为的系统性分析，从而提升了对电网灵活性的理解和利用。

Abstract: Coordination of distributed energy resources (DERs) can engender flexibility
necessary to improve grid reliability. Packetized Energy Management (PEM) is a
method for coordinating DERs, such as thermostatically controlled loads (TCLs)
and electric vehicles, within customer quality-of-service (QoS) limits. In PEM,
a DER uses local information to offer flexibility by sending a request to the
DER coordinator to turn-ON or turn-OFF. Much work has focused on modeling and
analyzing aggregations of DERs under PEM with fixed packet durations and only
turn-ON requests. Different recent efforts to enable variable packet lengths
have shown an increase in available flexibility and ramping capability, but
have not been modeled in aggregate, which limits systematic analyses. To
address this issue, this paper presents a new aggregate bin-based (macro) model
of PEM loads that incorporates both turn-ON and turn-OFF request features,
enabling the model to accurately characterize the capability of the fleet of
DERs to track a power reference signal, population temperature dynamics,
aggregate request rates, and variable packet lengths. Simulation-based
validation is performed against an agent-based (micro) model to evaluate
robustness and quantify model accuracy. Finally, the distribution of variable
packet lengths from macro-model simulations are applied to inform past work on
PEM with randomized packet lengths

</details>


### [545] [Structured identification of multivariable modal systems](https://arxiv.org/abs/2510.10820)
*Maarten van der Hulst,Rodrigo A. González,Koen Classens,Paul Tacx,Nick Dirkx,Jeroen van de Wijdeven,Tom Oomen*

Main category: eess.SY

TL;DR: 本文提出了一种系统辨识框架，用于从频率响应数据中估计复杂多变量机械系统的模态模型，该方法通过两步结构化算法获得准确、物理相关且最小阶的模型。


<details>
  <summary>Details</summary>
Motivation: 下一代工业系统需要物理可解释的模型，以实现有效的控制、支持设计验证并为监测策略提供基础。

Method: 采用两步结构化辨识算法：首先使用改进的工具变量法估计一个加性模型，然后将其投影到模态形式上。

Result: 所开发的辨识方法能够为一般阻尼和比例阻尼的模态系统提供准确、物理相关且最小阶的模型。该方法的有效性通过在一个具有大量分布式执行器和传感器并表现出复杂柔性动力学的原型晶圆平台系统上进行了实验验证。

Conclusion: 该研究成功开发了一种有效的系统辨识方法，能够为复杂多变量机械系统提供物理可解释的、准确且最小阶的模态模型，对于工业应用具有重要价值。

Abstract: Physically interpretable models are essential for next-generation industrial
systems, as these representations enable effective control, support design
validation, and provide a foundation for monitoring strategies. The aim of this
paper is to develop a system identification framework for estimating modal
models of complex multivariable mechanical systems from frequency response
data. To achieve this, a two-step structured identification algorithm is
presented, where an additive model is first estimated using a refined
instrumental variable method and subsequently projected onto a modal form. The
developed identification method provides accurate, physically-relevant,
minimal-order models, for both generally-damped and proportionally damped modal
systems. The effectiveness of the proposed method is demonstrated through
experimental validation on a prototype wafer-stage system, which features a
large number of spatially distributed actuators and sensors and exhibits
complex flexible dynamics.

</details>


### [546] [Observability and parameter estimation of a generic model for aggregated distributed energy resources](https://arxiv.org/abs/2510.10892)
*Bukunmi Gabriel Odunlami,Marcos Netto*

Main category: eess.SY

TL;DR: 该论文提出了一种新的框架，用于估计聚合分布式能源（DER_a）模型的参数，包括参数可估计性分析和基于卡尔曼滤波器的校准方法，以准确捕捉DER的集体动态。


<details>
  <summary>Details</summary>
Motivation: 准确捕捉配电系统中分布式能源的集体动态，理解模型校准的局限性，并提供关于可可靠估计参数数量和具体参数的新见解。

Method: 首先，引入一种严谨的方法来确定所有模型参数是否可估计，并识别可估计的参数子集。其次，开发了一种卡尔曼滤波方法（包括扩展卡尔曼滤波器和无迹卡尔曼滤波器）来校准DER_a模型，并针对饱和和死区等非线性效应设计了平滑函数处理机制。

Result: 该框架在修改后的IEEE 34节点配电馈线（包含基于逆变器的资源）上得到了有效验证。研究结果与北美电力可靠性公司（NERC）的参数化指南一致，强调了模型校准在准确捕捉DER集体动态方面的重要性。

Conclusion: 所提出的框架为DER_a模型参数估计和校准提供了有效的方法，能够提供关于可估计参数的深刻见解，并强调了模型校准在准确表示分布式能源集体动态中的关键作用。

Abstract: We propose a novel framework for estimating the parameters of an aggregated
distributed energy resources (der_a) model. First, we introduce a rigorous
method to determine whether all model parameters are estimable. When they are
not, our approach identifies the subset of parameters that can be estimated.
The proposed framework offers new insights into the number and specific
parameters that can be reliably estimated based on commonly available
measurements. It also highlights the limitations of calibrating such models.
Second, we introduce a Kalman filtering method to calibrate the der_a model.
Since we account for nonlinear effects such as saturation and deadbands, we
develop a specific mechanism to handle smoothing functions within the Kalman
filter. Specifically, we consider the extended and the unscented Kalman filter.
We demonstrate the effectiveness of the proposed framework on a modified IEEE
34-node distribution feeder with inverter-based resources. Our findings align
with the North American Electric Reliability Corporation's parameterization
guideline and underscore the importance of model calibration in accurately
capturing the collective dynamics of distributed energy resources installed on
distribution systems.

</details>


### [547] [Optimal Multi-Modal Transportation and Electric Power Flow: The Value of Coordinated Dynamic Operation](https://arxiv.org/abs/2510.10914)
*Jiajie Qiu,Dakota Thompson,Kamal Youcef-Toumi,Amro M. Farid*

Main category: eess.SY

TL;DR: 本文提出了一种最优多模式交通和电力潮流（OMTEPF）模型，用于评估交通-电力耦合系统（TEN）中协调、动态运行的益处，旨在实现交通电气化和净零排放。


<details>
  <summary>Details</summary>
Motivation: 交通电气化是实现净零排放的关键挑战，该领域通常占国家能源消耗的四分之一以上。这不仅需要电动汽车的广泛普及，还需要其与相互依赖的基础设施系统（特别是交通-电力耦合系统TEN）的无缝集成。

Method: 该研究开发了一个最优多模式交通和电力潮流（OMTEPF）模型。该框架基于异功能图论的最新进展，能够联合优化智能TEN管理中的五个关键操作决策：车辆调度、路线选择、充电站排队、协调充电和车网稳定。该介观、动态模型明确表示了单个电动汽车及其荷电状态轨迹，并涵盖了TEN作为系统之系统的完整范围，包括五种不同的充电模式。在电力系统方面，采用IV-ACOPF公式确保电力子问题的全局最优解。

Result: 通过对比分析，该研究证明了协调的TEN运行相对于目前孤立、非协调的基础设施管理具有显著的价值。

Conclusion: 这项工作为下一代可持续移动-能源系统的协同设计和运行提供了新颖的方法论贡献和可操作的见解。

Abstract: The electrification of transportation represents a critical challenge in the
global transition toward net-zero emissions, as the sector often accounts for
more than one-quarter of national energy consumption. Achieving this
transformation requires not only widespread adoption of electric vehicles (EVs)
but also their seamless integration into interdependent infrastructure
systems-specifically, the transportation-electricity nexus (TEN). This paper
develops an optimal multi-modal transportation and electric power flow (OMTEPF)
model to evaluate the benefits of coordinated, dynamic system operation.
Building on recent advances in hetero-functional graph theory, the framework
enables joint optimization of five key operational decisions in intelligent TEN
management: vehicle dispatch, route choice, charging station queuing,
coordinated charging, and vehicle-to-grid stabilization. The mesoscopic,
dynamic model explicitly represents individual EVs and their state-of-charge
trajectories, thereby extending beyond the prevailing literature's focus on
static, macroscopic traffic assignment. It further captures the full scope of
the TEN as a system-of-systems, incorporating five distinct charging
modalities: private residential, private commercial, wired public commercial,
inductive public, and discharging. On the power system side, an IV-ACOPF
formulation ensures globally optimal solutions to the electrical subproblems.
Comparative analysis demonstrates the substantial value of coordinated TEN
operation relative to the status quo of siloed, uncoordinated infrastructure
management. This work provides both a novel methodological contribution and
actionable insights for the co-design and operation of next-generation
sustainable mobility-energy systems.

</details>


### [548] [Establishing assembly-oriented modular product architectures through Design for Assembly enhanced Modular Function Deployment](https://arxiv.org/abs/2510.11089)
*Fabio Marco Monetti,Adam Lundström,Colin de Kwant,Magnus Gyllenskepp,Antonio Maffei*

Main category: eess.SY

TL;DR: 本文提出对模块化功能部署（MFD）方法进行改进，通过整合装配设计（DFA）逻辑，在产品架构早期开发阶段系统性地考虑装配因素，以提高生产效率和市场响应能力。


<details>
  <summary>Details</summary>
Motivation: 模块化产品设计在平衡产品多样性、运营效率和市场响应方面至关重要，但现有的MFD方法在早期概念和系统级开发中缺乏对装配因素的系统支持，这可能导致生产启动延迟和生命周期效率低下。

Method: 本文提出了一系列MFD增强措施，将装配设计（DFA）逻辑整合到架构合成中。这些增强包括引入结构化启发式方法、面向装配的模块驱动因素、编码接口分类法以及用于评估装配可行性和自动化准备度的量化指标。

Result: 通过一个手持式吹叶机案例研究，验证了该方法的可用性和有效性。重新设计的架构显示出装配工作量减少、接口简化以及自动化潜力增加的优点。

Conclusion: 该方法通过从装配角度支持架构替代方案的早期评估，能够更快地过渡到高效批量生产，并为产品生命周期中的持续改进奠定基础。

Abstract: Modular product design has become a strategic enabler for companies seeking
to balance product variety, operational efficiency, and market responsiveness,
making the alignment between modular architecture and manufacturing
considerations increasingly critical. Modular Function Deployment (MFD) is a
widely adopted method for defining modular product architectures, yet it lacks
systematic support for assembly considerations during early concept and
system-level development. This limitation increases the risk of delayed
production ramp-up and lifecycle inefficiencies. This paper proposes a set of
enhancements to MFD that integrate Design for Assembly (DFA) logic into
architectural synthesis. The extended method introduces structured heuristics,
assembly-oriented module drivers, a coded interface taxonomy, and quantitative
metrics for assessing assembly feasibility and automation readiness. These
additions preserve compatibility with standard MFD workflows while enriching
decision-making with traceable, production-informed reasoning. An illustrative
case study involving a handheld leaf blower demonstrates the method's usability
and effectiveness. The redesigned architecture shows reduced assembly effort,
simplified interfaces, and increased automation potential. By supporting
early-stage evaluation of architectural alternatives through an assembly lens,
the method enables faster transition to efficient volume production and
provides a foundation for continuous improvement throughout the product
lifecycle.

</details>


### [549] [Utilizing Bayesian Optimization for Timetable-Independent Railway Junction Performance Determination](https://arxiv.org/abs/2510.11181)
*Tamme Emunds,Paul Brunzema,Sebastian Trimpe,Nils Nießen*

Main category: eess.SY

TL;DR: 本文提出了一种利用贝叶斯优化方法，在交通流量分配问题中确定独立于时刻表的动态铁路枢纽容量的方法，通过考虑动态交通分布和不同的列车服务需求，显著优于传统的固定容量模型。


<details>
  <summary>Details</summary>
Motivation: 铁路基础设施的效率受列车组合（客运和货运服务有不同的运营需求）影响显著。传统解决长期交通分配问题的方法依赖于基于特定交通组成假设的固定容量限制，这无法有效应对动态的交通分布。

Method: 本文提出一种在交通流量分配问题中确定独立于时刻表的容量的方法，从而能够在动态交通分布下计算枢纽容量。该方法通过贝叶斯优化（BO）解决了一个非线性约束优化问题，以最大化交通吞吐量。它针对目标函数已知但容量约束计算成本高昂的BO问题进行了调整，并通过将约束函数形状的先验知识融入高斯过程替代模型中，优化了采集过程以提高性能。

Result: 所提出的方法在巴黎附近的一个铁路枢纽进行了基准测试。结果显示，该方法显著优于固定交通组成模型，并突出了动态容量分配的优势。

Conclusion: 通过引入基于贝叶斯优化的动态、独立于时刻表的容量分配方法，可以有效提高铁路枢纽的效率，尤其是在处理混合列车交通时，其性能明显优于传统的固定容量模型。

Abstract: The efficiency of railway infrastructure is significantly influenced by the
mix of trains that utilize it, as different service types have competing
operational requirements. While freight services might require extended service
times, passenger services demand more predictable schedules. Traditional
methods for addressing long-term traffic assignment problems often rely on
fixed-value capacity limitations, determined based on specific assumptions
about traffic composition. This paper introduces a methodology for determining
timetable-independent capacity within the traffic rate assignment problem,
enabling the calculation of junction capacities under dynamic traffic
distributions. We solve the underlying non-linear constrained optimization
problem maximizing the traffic throughput using Bayesian optimization (BO).
This setting combines a known objective function with expensive- to-compute
capacity constraints, motivating an adaption of standard BO problems, where
objective functions are usually unknown. We tailor the acquisition process in
BO to this specific setting and increase performance by incorporating prior
knowledge about the shape of the constraint functions into the Gaussian process
surrogate model. Our derived approaches are benchmarked on a railway junction
near Paris, significantly outperforming fixed traffic composition models and
highlighting the benefits of dynamic capacity allocation.

</details>


### [550] [Edge-to-Cloud Computations-as-a-Service in Software-Defined Energy Networks for Smart Grids](https://arxiv.org/abs/2510.11286)
*Jack Jackman,David Ryan,Arun Narayanan,Pedro Nardelli,Indrakshi Dey*

Main category: eess.SY

TL;DR: 本文提出首个软件定义能源网络（SDEN），通过统一边缘、雾和云计算，结合5G URLLC、SDN和NFV，为电力网络提供计算即服务（CaaS），以共同优化能源、延迟和可靠性。


<details>
  <summary>Details</summary>
Motivation: 现代电网面临数据生成与处理地点之间的严重不匹配问题。保护继电器、电动汽车充电和分布式可再生能源需要在边缘进行毫秒级分析，而高能耗工作负载通常位于遥远的云端，导致实时截止日期延误和能源浪费。

Method: 该研究提出SDEN，结合5G URLLC、SDN和NFV，统一边缘、雾和云计算。具体方法包括：(i) 提出一个将计算放置与网络容量耦合，并考虑URLLC约束的联合任务卸载公式；(ii) 开发一种轻量级、可行性保持的贪婪启发式算法，能够扩展并紧密追踪最佳能源和延迟权衡；(iii) 设计一个分层AI管道（边缘反应式、雾预测式、云战略式），采用隐私保护的联邦图神经网络（GNN）进行故障检测和微电网协调。

Result: SDEN将分散的电网计算资源转化为一个统一的、可编程的基础设施，提供可靠、节能的实时分析。它首次为实用、电网规模的CaaS建立了软件定义路径。研究表明，其提出的启发式算法能够紧密追踪最佳能源和延迟的权衡。

Conclusion: SDEN通过将边缘、雾和云计算与先进的网络技术及分层AI相结合，解决了现代电网中数据处理的挑战，实现了端到端的能源、延迟和可靠性共同优化，为电网规模的计算即服务奠定了基础。

Abstract: Modern power grids face an acute mismatch between where data is generated and
where it can be processed: protection relays, EV (Electric Vehicle) charging,
and distributed renewables demand millisecond analytics at the edge, while
energy-hungry workloads often sit in distant clouds leading to missed real-time
deadlines and wasted power. We address this by proposing, to our knowledge, the
first-ever SDEN (Software Defined Energy Network) for CaaS
(Computations-as-a-Service) that unifies edge, fog, and cloud compute with 5G
URLLC (Ultra-Reliable Low-Latency Communications), SDN (Software Defined
Networking), and NFV (Network Functions Virtualization) to co-optimize energy,
latency, and reliability end-to-end. Our contributions are threefold: (i) a
joint task offloading formulation that couples computation placement with
network capacity under explicit URLLC constraints; (ii) a feasibility
preserving, lightweight greedy heuristic that scales while closely tracking
optimal energy and latency trade-offs; and (iii) a tiered AI (Artificial
Intelligence) pipeline-reactive at the edge, predictive in the fog, strategic
in the cloud-featuring privacy-preserving, federated GNNs (Graph Neural
Networks) for fault detection and microgrid coordination. Unlike prior
edge-only or cloud-only schemes, SDEN turns fragmented grid compute into a
single, programmable substrate that delivers dependable, energy-aware, real
time analytics establishing a first-ever, software defined path to practical,
grid-scale CaaS.

</details>


### [551] [pyspect: An Extensible Toolbox for Automatic Construction of Temporal Logic Trees via Reachability Analysis](https://arxiv.org/abs/2510.11316)
*Kaj Munhoz Arfvidsson,Loizos Hadjiloizou,Frank J. Jiang,Karl H. Johansson,Jonas Mårtensson*

Main category: eess.SY

TL;DR: pyspect是一个Python工具箱，它通过提供一种与具体方法无关的、基于时间逻辑树的方法，简化了将可达性分析应用于复杂时间逻辑问题的过程，从而降低了网络物理系统验证的开发难度并促进了技术比较。


<details>
  <summary>Details</summary>
Motivation: 在网络物理系统中满足复杂要求时，当前的可达性分析需要大量的体力劳动和领域专业知识来开发底层程序，这限制了可达性分析在复杂验证问题中的广泛应用。

Method: pyspect采用一种与具体方法无关的方法，通过时间逻辑树（TLTs）对时间逻辑规范进行可达性分析验证。它允许使用高级逻辑公式来指定复杂安全性和活性要求，这些公式独立于任何特定的可达性技术或集合表示。这种设计将实现开发者（开发数值可达性程序）和最终用户（编写规范）的关注点分开。

Result: pyspect简化了可达性程序的合成，提高了规范的可重用性，并促进了不同可达性技术（如Hamilton-Jacobi和基于混合Zonotope的可达性分析）对相同时间逻辑规范的并排比较。一个简单的车辆示例验证了这些优势。

Conclusion: pyspect通过简化可达性分析程序的合成、提高规范的可重用性以及促进不同可达性技术的比较，有效降低了将可达性分析应用于复杂时间逻辑问题的开发难度，从而推广了其应用。

Abstract: In this paper, we present pyspect, a Python toolbox that simplifies the use
of reachability analysis for temporal logic problems. Currently, satisfying
complex requirements in cyber-physical systems requires significant manual
effort and domain expertise to develop the underlying reachability programs.
This high development effort limits the broader adoption of reachability
analysis for complex verification problems. To address this, pyspect provides a
method-agnostic approach to performing reachability analysis for verifying a
temporal logic specification via temporal logic trees (TLTs). It enables the
specification of complex safety and liveness requirements using high-level
logic formulations that are independent of any particular reachability
technique or set representation. As a result, pyspect allows for the comparison
of different reachability implementations, such as Hamilton-Jacobi and Hybrid
Zonotope-based reachability analysis, for the same temporal logic
specification. This design separates the concerns of implementation developers
(who develop numerical procedures for reachability) and end-users (who write
specifications). Through a simple vehicle example, we demonstrate how pyspect
simplifies the synthesis of reachability programs, promotes specification
reusability, and facilitates side-by-side comparisons of reachability
techniques for complex tasks.

</details>


### [552] [High-Order Quarter-Wave Plate Optimization for Linear Birefringence Suppression in Reflective FOCS](https://arxiv.org/abs/2510.11386)
*Yuechen Liu,Boqi Meng*

Main category: eess.SY

TL;DR: 本研究提出了一种利用高阶四分之一波片补偿策略，有效消除反射式光纤电流传感器中线性双折射引起的相位误差，显著提高了传感器的精度和实用性。


<details>
  <summary>Details</summary>
Motivation: 光纤电流传感器（FOCS）因其高灵敏度、优异绝缘性和强抗电磁干扰能力而被广泛应用于现代电网。然而，为了进一步优化其性能，需要精确研究其误差源。特别是，由四分之一波片引起的线性双折射会导致相位误差，限制了传感器的精度。

Method: 本研究针对基于法拉第效应的反射式FOCS，建立了模拟由四分之一波片引起的线性双折射导致相位误差的理论模型。分析了传统使用圆双折射的方法的局限性。创新性地提出了一种采用高阶四分之一波片进行补偿的策略，以有效消除线性双折射效应。

Result: 所提出的高阶四分之一波片补偿策略能够有效消除线性双折射效应。这显著增强了FOCS在精密计量中的准确性和实用性。

Conclusion: 通过引入高阶四分之一波片补偿策略，可以有效消除反射式光纤电流传感器中由线性双折射引起的相位误差，从而大幅提升传感器的精度和实际应用价值。

Abstract: Fiber optic current sensors (FOCS) are widely adopted in modern power grids
due to high sensitivity, excellent insulation, and strong immunity to
electromagnetic interference. This prominence necessitates precise
investigation into their error sources and corresponding optimization. This
study examines reflective FOCS based on the Faraday effect. A theoretical model
is established to simulate phase error caused by linear birefringence from the
quarter-wave plate. Conventional methods using circular birefringence are
analyzed, revealing inherent limitations. Innovatively, a compensation strategy
employing high-order quarter-wave plates is proposed to effectively eliminate
linear birefringence effects. This approach significantly enhances the accuracy
and practicality of FOCS in precision metrology.

</details>


### [553] [Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding](https://arxiv.org/abs/2510.11331)
*Bingjie Zhu,Zhixiong Chen,Liqiang Zhao,Hyundong Shin,Arumugam Nallanathan*

Main category: eess.SY

TL;DR: 本文提出了一种基于推测解码（SD）的LLM边缘推理框架，通过大小模型协同生成和验证多令牌，并结合流水线并行和联合优化，显著降低了服务延迟并提高了资源利用率。


<details>
  <summary>Details</summary>
Motivation: 现有的边缘大型语言模型（LLM）推理系统通常采用自回归解码（AD），每次前向传播仅生成一个令牌。这种迭代过程加上边缘节点有限的计算资源，导致服务延迟高，并且在需求增长时难以支持多用户。

Method: 本文提出一个基于推测解码（SD）的LLM服务框架，在异构边缘节点上部署小模型和大模型协同提供推理服务。小模型快速生成草稿令牌，大模型并行验证，实现每次前向传播生成多个令牌。为提高资源利用率，引入流水线并行以重叠多个推理任务的草稿生成和验证过程。此外，还推导了一个综合考虑通信和推理延迟的延迟模型，并构建了一个联合优化问题，以最小化总服务延迟，该问题涉及推测长度、任务批处理和无线通信资源分配。通过推导无线通信资源分配的闭式解和开发动态规划算法来解决批处理和推测控制策略。

Result: 实验结果表明，所提出的框架相比基于自回归解码（AD）的服务系统实现了更低的服务延迟。此外，所提出的联合优化方法相比基准方案，延迟降低高达44.9%。

Conclusion: 本文提出的基于推测解码的LLM边缘推理框架，结合大小模型协同、流水线并行和联合优化策略，能够有效解决边缘LLM推理高延迟和资源利用率低的问题，显著提升了服务性能。

Abstract: Large language model (LLM) inference at the network edge is a promising
serving paradigm that leverages distributed edge resources to run inference
near users and enhance privacy. Existing edge-based LLM inference systems
typically adopt autoregressive decoding (AD), which only generates one token
per forward pass. This iterative process, compounded by the limited
computational resources of edge nodes, results in high serving latency and
constrains the system's ability to support multiple users under growing
demands.To address these challenges, we propose a speculative decoding
(SD)-based LLM serving framework that deploys small and large models across
heterogeneous edge nodes to collaboratively deliver inference services.
Specifically, the small model rapidly generates draft tokens that the large
model verifies in parallel, enabling multi-token generation per forward pass
and thus reducing serving latency. To improve resource utilization of edge
nodes, we incorporate pipeline parallelism to overlap drafting and verification
across multiple inference tasks. Based on this framework, we analyze and derive
a comprehensive latency model incorporating both communication and inference
latency. Then, we formulate a joint optimization problem for speculation
length, task batching, and wireless communication resource allocation to
minimize total serving latency. To address this problem, we derive the
closed-form solutions for wireless communication resource allocation, and
develop a dynamic programming algorithm for joint batching and speculation
control strategies. Experimental results demonstrate that the proposed
framework achieves lower serving latency compared to AD-based serving systems.
In addition,the proposed joint optimization method delivers up to 44.9% latency
reduction compared to benchmark schemes.

</details>


### [554] [Data-Driven Estimation of Quadrotor Motor Efficiency via Residual Minimization](https://arxiv.org/abs/2510.11388)
*Sheng-Wen Cheng,Teng-Hu Cheng*

Main category: eess.SY

TL;DR: 本文提出了一种数据驱动的框架，通过残差最小化在线估计四旋翼飞行器电机效率，该方法结合滑动窗口策略、优化算法和鲁棒加权，在故障检测和健康监测等应用中表现出高精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统基于滤波器的方法在处理四旋翼飞行器电机效率估计时存在局限性，特别是在故障检测与隔离（FDI）、健康监测和预测性维护等航空机器人应用中，需要一种更灵活、更鲁棒的在线估计方法。

Method: 该研究将问题建模为有约束的非线性优化，通过最小化测量飞行数据与四旋翼动力学模型预测之间的轨迹残差来估计电机效率。采用滑动窗口策略实现在线估计，并通过迭代重加权最小二乘（IRLS）结合原对偶内点法高效求解，其中不等式约束通过对数障碍函数强制执行。为拒绝异常值，尤其是在电机削波场景中，采用了鲁棒z分数加权。与传统滤波器方法不同，该方法采用批处理模式，允许选择性地纳入信息丰富的历史数据段。

Result: 仿真结果表明，在各种退化场景下，所提出的估计器具有较高的准确性和鲁棒性。在电机削波场景中，该估计器产生的尖峰比扩展卡尔曼滤波器（EKF）基线更小。

Conclusion: 该数据驱动的电机效率在线估计算法具有高精度和鲁棒性，特别适用于机载实现，并在航空机器人系统的故障检测与隔离、健康监测和预测性维护等应用中展现出巨大潜力。

Abstract: A data-driven framework is proposed for online estimation of quadrotor motor
efficiency via residual minimization. The problem is formulated as a
constrained nonlinear optimization that minimizes trajectory residuals between
measured flight data and predictions generated by a quadrotor dynamics model. A
sliding-window strategy enables online estimation, and the optimization is
efficiently solved using an iteratively reweighted least squares (IRLS) scheme
combined with a primal-dual interior-point method, with inequality constraints
enforced through a logarithmic barrier function. Robust z-score weighting is
employed to reject outliers, which is particularly effective in motor clipping
scenarios where the proposed estimator exhibits smaller spikes than an EKF
baseline. Compared to traditional filter-based approaches, the batch-mode
formulation offers greater flexibility by selectively incorporating informative
data segments. This structure is well-suited for onboard implementation,
particularly for applications such as fault detection and isolation (FDI),
health monitoring, and predictive maintenance in aerial robotic systems.
Simulation results under various degradation scenarios demonstrate the accuracy
and robustness of the proposed estimator.

</details>


### [555] [Robust Closed-Form Control for MIMO Nonlinear Systems under Conflicting Time-Varying Hard and Soft Constraints](https://arxiv.org/abs/2510.11393)
*Farhad Mehdifar,Charalampos P. Bechlioulis,Dimos V. Dimarogonas*

Main category: eess.SY

TL;DR: 本文提出一种新颖的鲁棒闭环控制律，用于处理不确定高相对阶非线性MIMO系统中的时变硬约束和软约束，并在约束冲突时优先保障硬约束。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为不确定高相对阶非线性MIMO系统设计一种能够处理时变硬约束（安全关键）和软约束（性能/任务目标）的控制方法，尤其是在这些约束发生冲突时，需要一种低复杂度的鲁棒解决方案。

Method: 该方法首先将所有约束整合为两个独立的标量时变硬约束和软约束函数。然后，开发了一个闭环控制律，利用适当设计的倒数障碍函数和非线性变换函数来强制执行这些约束。当硬约束和软约束发生冲突时，通过动态松弛律虚拟放松软约束，从而优先保障硬约束。该控制律通过避免近似方案来应对系统不确定性，从而保持较低的复杂性。

Result: 仿真结果证实了所提出方法的有效性。

Conclusion: 本文提出了一种有效且低复杂度的鲁棒闭环控制律，能够处理不确定高相对阶非线性MIMO系统中的时变硬约束和软约束，并在约束冲突时通过动态松弛机制优先保障安全关键的硬约束。

Abstract: This paper introduces a novel robust closed-form control law to handle
time-varying hard and soft constraints in uncertain high-relative-degree
nonlinear MIMO systems. These constraints represent spatiotemporal
specifications in mechanical systems' operational space, with hard constraints
ensuring safety-critical requirements and soft constraints encoding performance
or task objectives. Initially, all constraints are consolidated into two
separate scalar time-varying hard and soft constraint functions, whose positive
level sets define feasible regions. A closed-form control law is developed to
enforce these constraints using appropriately designed reciprocal barriers and
nonlinear transformation functions. When conflicts between hard and soft
constraints arise, the control law prioritizes hard constraints by virtually
relaxing soft constraints via a dynamic relaxation law. Notably, the proposed
control law maintains low complexity by avoiding approximation schemes for
coping with system uncertainties. Simulation results confirm the effectiveness
of the proposed method.

</details>


### [556] [Robust Recovery and Control of Cyber-physical Discrete Event Systems under Actuator Attacks](https://arxiv.org/abs/2510.11405)
*Samuel Oliveira,Mostafa Tavakkoli Anbarani,Gregory Beal,Ilya Kovalenko,Marcelo Teixeira,André B. Leal,Rômulo Meira-Góes*

Main category: eess.SY

TL;DR: 本文研究了网络物理系统（CPS）在离散事件系统（DES）模型下的网络安全问题，重点关注执行器启用攻击后的恢复策略，提出了一种在攻击检测后能引导系统至受限但鲁棒行为的方法，并定义了AE-鲁棒可恢复性。


<details>
  <summary>Details</summary>
Motivation: 关键的现实世界应用严重依赖网络物理系统（CPS），但其对通信网络的依赖性引入了显著的安全风险，攻击者可能利用漏洞损害系统的完整性和可用性。

Method: 将CPS建模为离散事件系统（DES），研究执行器启用攻击。提出一种恢复方法，在正常条件下保持系统完整的有效行为；攻击检测后，引导系统转向受限但鲁棒的行为。引入了“AE-鲁棒可恢复性”特性，并阐明了系统从攻击中恢复并防止进一步漏洞的必要和充分条件。

Result: 提出了一种在执行器启用攻击后能保持系统在受限但鲁棒行为下运行的恢复解决方案，确保了操作的连续性和弹性。定义了AE-鲁棒可恢复性，并给出了其必要和充分条件。通过一个基于制造系统的案例研究展示了所提出解决方案的有效性。

Conclusion: 所提出的解决方案能有效应对CPS中的执行器启用攻击，确保系统在攻击后仍能保持操作连续性和弹性，并通过AE-鲁棒可恢复性特性为系统恢复提供了理论基础。

Abstract: Critical real-world applications strongly rely on Cyber-physical systems
(CPS), but their dependence on communication networks introduces significant
security risks, as attackers can exploit vulnerabilities to compromise their
integrity and availability. This work explores the topic of cybersecurity in
the context of CPS modeled as discrete event systems (DES), focusing on
recovery strategies following the detection of cyberattacks. Specifically, we
address actuator enablement attacks and propose a method that preserves the
system's full valid behavior under normal conditions. Upon detecting an attack,
our proposed solution aims to guide the system toward a restricted yet robust
behavior, ensuring operational continuity and resilience. Additionally, we
introduce a property termed AE-robust recoverability, which characterizes the
necessary and sufficient conditions for recovering a system from attacks while
preventing further vulnerabilities. Finally, we showcase the proposed solution
through a case study based on a manufacturing system.

</details>


### [557] [Trajectory control of a suspended load with non-stopping flying carriers](https://arxiv.org/abs/2510.11413)
*Sofia Girardello,Giulia Michieletto,Angelo Cenedese,Antonio Franchi,Chiara Gabellieri*

Main category: eess.SY

TL;DR: 本文提出首个针对非停歇飞行器协同负载运输的闭环控制框架，通过力矩控制器和优化层确保负载姿态精确跟踪并防止飞行器停滞。


<details>
  <summary>Details</summary>
Motivation: 现有协同负载运输方法可能存在飞行器停滞问题，且缺乏可靠、通用的闭环控制框架来满足未来实际应用中对非停歇飞行器协同运输的需求。

Method: 该研究基于抓取矩阵公式和内力冗余，提出了一个反馈力矩控制器来主动调节负载姿态。同时，一个优化层动态调整内力振荡，以确保载体持续运动，防止停滞。

Result: 多旋翼无人机上的初步实验验证了模型假设。数值模拟表明，该方法成功防止了载体停滞，实现了精确的负载跟踪，并生成了具有平滑速度剖面的物理可行轨迹。

Conclusion: 所提出的框架不仅推动了现有技术水平，还为未来需要协调非停歇飞行器进行负载运输的实际应用提供了一个可靠、通用的解决方案。

Abstract: This paper presents the first closed-loop control framework for cooperative
payload transportation with non-stopping flying carriers. Building upon
grasp-matrix formulations and internal force redundancy, we propose a feedback
wrench controller that actively regulates the payload's pose while an
optimization layer dynamically shapes internal-force oscillations to guarantee
persistent carrier motion. Preliminary experimental results on multirotor UAVs
validate the model assumptions, and numerical simulations demonstrate that the
method successfully prevents carrier stagnation, achieves accurate load
tracking, and generates physically feasible trajectories with smooth velocity
profiles. The proposed framework not only advances the state of the art but
also offers a reliable, versatile solution for future real-world applications
requiring load transportation by coordinated non-stopping flying carriers.

</details>


### [558] [The Role of Flexible Connection in Accelerating Load Interconnection in Distribution Networks](https://arxiv.org/abs/2510.11476)
*Nan Gu,Ge Chen,Junjie Qin*

Main category: eess.SY

TL;DR: 该论文研究了灵活连接在数据中心和电气化背景下，如何通过在电网受限时推迟或削减消费，加速大型负荷并网。研究表明，即使是适度的灵活性，也能显著提升电网承载能力，且无需大规模基础设施升级。


<details>
  <summary>Details</summary>
Motivation: 数据中心和电气化导致电力需求不断增长，大型负荷的并网面临挑战。传统的并网方式需要大量基础设施升级，耗时且成本高昂。灵活连接提供了一种无需大规模升级即可加速并网的潜在解决方案。

Method: 研究构建了一个考虑灵活性的负荷承载能力分析问题，明确限制了每年公用事业干预的次数，以确保对用户的影响不频繁。论文开发了针对此非凸问题的有效求解方法，并将其应用于实际负荷数据和测试馈线。

Result: 实证结果表明，适度的灵活性（即少量干预、小幅削减或延迟）即可显著提升负荷承载能力。理论分析进一步解释并推广了这些发现。

Conclusion: 灵活连接在加速大型负荷并网和提升电网承载能力方面具有广泛潜力，即使是有限的灵活性也能带来显著效益。

Abstract: This paper investigates the role of flexible connection in accelerating the
interconnection of large loads amid rising electricity demand from data centers
and electrification. Flexible connection allows new loads to defer or curtail
consumption during rare, grid-constrained periods, enabling faster access
without major infrastructure upgrades. To quantify how flexible connection
unlocks load hosting capacity, we formulate a flexibility-aware hosting
capacity analysis problem that explicitly limits the number of
utility-controlled interventions per year, ensuring infrequent disruption.
Efficient solution methods are developed for this nonconvex problem and applied
to real load data and test feeders. Empirical results reveal that modest
flexibility, i.e., few interventions with small curtailments or delays, can
unlock substantial hosting capacity. Theoretical analysis further explains and
generalizes these findings, highlighting the broad potential of flexible
connection.

</details>


### [559] [A Physics-Informed Reinforcement Learning Approach for Degradation-Aware Long-Term Charging Optimization in Batteries](https://arxiv.org/abs/2510.11515)
*Shanthan Kumar Padisala,Bharatkumar Hegde,Ibrahim Haskara,Satadru Dey*

Main category: eess.SY

TL;DR: 本文提出了一种基于物理信息的强化学习框架，旨在使广泛使用的恒流恒压（CCCV）充电协议能够根据电池长期退化（特别是活性材料损失LAM）进行自适应调整，从而优化充电电流。


<details>
  <summary>Details</summary>
Motivation: 电池在使用过程中会退化，表现为内阻增加和容量衰减。现有充电方法（包括CCCV）通常不适应电池健康状况的变化。很少有工作尝试使CCCV协议适应电池循环过程中不断变化的健康状况，这造成了一个研究空白。

Method: 本研究提出一个框架，优化CCCV协议中的恒流部分，以适应电池的长期退化。具体方法是使用一种物理信息强化学习（RL）方法，该方法不仅估计关键电池退化机制（如活性材料损失LAM），还根据这种退化调整CCCV的电流大小。该框架结合了开源电池建模工具PyBamm和Stable-baselines，其中RL代理使用近端策略优化（PPO）网络进行训练。

Result: 仿真结果表明，所提出的框架通过在强化学习算法中嵌入物理信息，具有增强广泛使用的CCCV协议的潜力。研究还比较了该代理与其他两种充电协议（非物理RL代理生成和所有循环均采用恒定CCCV）的性能。

Conclusion: 该研究提出的基于物理信息强化学习的框架，通过使其适应电池退化，展示了增强广泛使用的CCCV充电协议的巨大潜力。

Abstract: Batteries degrade with usage and continuous cycling. This aging is typically
reflected through the resistance growth and the capacity fade of battery cells.
Over the years, various charging methods have been presented in the literature
that proposed current profiles in order to enable optimal, fast, and/or
health-conscious charging. However, very few works have attempted to make the
ubiquitous Constant Current Constant Voltage (CCCV) charging protocol adaptive
to the changing battery health as it cycles. This work aims to address this gap
and proposes a framework that optimizes the constant current part of the CCCV
protocol adapting to long-term battery degradation. Specifically, a
physics-informed Reinforcement Learning (RL) approach has been used that not
only estimates a key battery degradation mechanism, namely, Loss of Active
Material (LAM), but also adjusts the current magnitude of CCCV as a result of
this particular degradation. The proposed framework has been implemented by
combining PyBamm, an open-source battery modeling tool, and Stable-baselines
where the RL agent was trained using a Proximal Policy Optimization (PPO)
network. Simulation results show the potential of the proposed framework for
enhancing the widely used CCCV protocol by embedding physics information in RL
algorithm. A comparative study of this proposed agent has also been discussed
with 2 other charging protocols generated by a non-physics-based RL agent and a
constant CCCV for all the cycles.

</details>


### [560] [Smooth Spatiotemporal Tube Synthesis for Prescribed-Time Reach-Avoid-Stay Control](https://arxiv.org/abs/2510.11583)
*Siddhartha Upadhyay,Ratnangshu Das,Pushpak Jagtap*

Main category: eess.SY

TL;DR: 本文提出一种自适应框架，通过构建平滑的时空管（STTs）并推导闭式控制律，为控制仿射非线性系统解决在规定时间内满足到达-避障-停留（RAS）规范的控制器合成问题，显著降低了控制功耗。


<details>
  <summary>Details</summary>
Motivation: 现有基于时空管的方法需要规避函数（circumvent functions），这通常会导致时空管的突然修改和较高的控制功耗。本研究旨在改进这些方法，消除规避函数的需要。

Method: 提出一个自适应框架，围绕静态不安全集构建平滑的时空管，以实现连续避障，并引导系统在规定时间内到达目标。推导出一个闭式、无近似的控制律，以确保系统轨迹保持在时空管内并满足RAS任务。

Result: 通过案例研究证明了所提方法的有效性，与现有方法相比，显著降低了控制功耗。

Conclusion: 所提出的自适应框架通过构建平滑时空管并采用闭式控制律，能够有效解决控制仿射非线性系统的RAS任务，同时显著降低了控制功耗。

Abstract: In this work, we address the issue of controller synthesis for a
control-affine nonlinear system to meet prescribed time reach-avoid-stay
specifications. Our goal is to improve upon previous methods based on
spatiotemporal tubes (STTs) by eliminating the need for circumvent functions,
which often lead to abrupt tube modifications and high control effort. We
propose an adaptive framework that constructs smooth STTs around static unsafe
sets, enabling continuous avoidance while guiding the system toward the target
within the prescribed time. A closed-form, approximation-free control law is
derived to ensure the system trajectory remains within the tube and satisfies
the RAS task. The effectiveness of the proposed approach is demonstrated
through a case study, showing a significant reduction in control effort
compared to prior methods.

</details>


### [561] [Analysis of the Geometric Heat Flow Equation: Computing Geodesics in Real-Time with Convergence Guarantees](https://arxiv.org/abs/2510.11692)
*Samuel G. Gessow,Brett T. Lopez*

Main category: eess.SY

TL;DR: 本文分析了几何热流方程在黎曼流形上计算测地线的收敛性，并证明了其全局指数稳定性（在特定曲率条件下）和渐近收敛性，同时提出了一种高效的伪谱方法。


<details>
  <summary>Details</summary>
Motivation: 在控制和运动规划等领域，实时数值计算黎曼流形上的测地线（最短路径曲线）已成为一项重要能力。

Method: 本文采用几何热流方程（通过求解抛物型偏微分方程）来计算测地线。此外，还提出了一种利用切比雪夫多项式的伪谱方法，以提高计算精度和效率。

Result: 研究证明，如果黎曼流形的曲率不过于正，几何热流方程在$L_2$空间中具有全局指数稳定性；并且$L_2$空间中的渐近收敛性始终得到保证。提出的伪谱方法能够在几毫秒内精确计算非特殊流形上的测地线，并通过在常见非欧几何曲面和非线性系统控制器中的应用验证了其有效性。

Conclusion: 几何热流方程是计算黎曼流形上测地线的一种数值稳定且收敛性良好的方法，尤其结合伪谱方法时，能够实现高效、实时的测地线计算。

Abstract: We present an analysis on the convergence properties of the so-called
geometric heat flow equation for computing geodesics (shortest-path~curves) on
Riemannian manifolds. Computing geodesics numerically in real-time has become
an important capability in several fields, including control and motion
planning. The geometric heat flow equation involves solving a parabolic partial
differential equation whose solution is a geodesic. In practice, solving this
PDE numerically can be done efficiently, and tends to be more numerically
stable and exhibit a better rate of convergence compared to numerical
optimization. We prove that the geometric heat flow equation is globally
exponentially stable in $L_2$ if the curvature of the Riemannian manifold is
not too positive, and that asymptotic convergence in $L_2$ is always
guaranteed. We also present a pseudospectral method that leverages Chebyshev
polynomials to accurately compute geodesics in only a few milliseconds for
non-contrived manifolds. Our analysis was verified with our custom
pseudospectral method by computing geodesics on common non-Euclidean surfaces,
and in feedback for a contraction-based controller with a non-flat metric for a
nonlinear system.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [562] [Chlorophyll-a Mapping and Prediction in the Mar Menor Lagoon Using C2RCC-Processed Sentinel 2 Imagery](https://arxiv.org/abs/2510.09736)
*Antonio Martínez-Ibarra,Aurora González-Vidal,Adrián Cánovas-Rodríguez,Antonio F. Skarmeta*

Main category: eess.IV

TL;DR: 本研究开发了一种结合Sentinel 2卫星图像和浮标数据、利用机器学习/深度学习模型，对Mar Menor泻湖水柱中叶绿素a进行高分辨率、深度特异性预测和绘图的方法，以增强富营养化预警能力。


<details>
  <summary>Details</summary>
Motivation: Mar Menor泻湖经历了严重的富营养化危机，监测叶绿素a对于预测有害藻华和指导缓解措施至关重要。传统的原位测量在空间和时间上受限，而卫星方法虽然提供更全面的视角，但通常仅限于地表估算或有限的时间覆盖。因此，需要开发一种可靠的方法来预测和绘制整个水柱的叶绿素a。

Method: 研究整合了近十年的Sentinel 2卫星图像（使用C2RCC处理器进行大气校正）和浮标实测数据（按深度聚合为0-1m、1-2m、2-3m、3-4m）。训练并验证了多种机器学习和深度学习算法，包括随机森林（RF）、XGBoost、CatBoost、多层感知器网络（MLP）和集成模型，采用交叉验证。同时，测试了系统的波段组合实验和空间聚合策略以优化预测。

Result: 结果显示预测性能具有深度依赖性。在地表（0-1m），C2X-Complex结合XGBoost和集成模型实现了R2 = 0.89；在1-2m深度，CatBoost和集成模型达到R2 = 0.87；在2-3m深度，TOA反射率结合KNN表现最佳（R2 = 0.81）；而在3-4m深度，RF实现了R2 = 0.66。生成的地图成功再现了已知的富营养化事件（如2016年危机、2025年激增），证实了方法的稳健性。

Conclusion: 本研究提供了一种端到端、经过验证的深度特异性叶绿素a绘图方法。其结合多光谱波段组合、浮标校准和机器学习/深度学习建模的整合框架，为其他浑浊的沿海系统提供了可转移的解决方案。

Abstract: The Mar Menor, Europe's largest coastal lagoon, located in Spain, has
undergone severe eutrophication crises. Monitoring chlorophyll-a (Chl-a) is
essential to anticipate harmful algal blooms and guide mitigation. Traditional
in situ measurements are spatially and temporally limited. Satellite-based
approaches provide a more comprehensive view, enabling scalable, long-term, and
transferable monitoring. This study aims to overcome limitations of chlorophyll
monitoring, often restricted to surface estimates or limited temporal coverage,
by developing a reliable methodology to predict and map Chl-a across the water
column of the Mar Menor. The work integrates Sentinel 2 imagery with buoy-based
ground truth to create models capable of high-resolution, depth-specific
monitoring, enhancing early-warning capabilities for eutrophication. Nearly a
decade of Sentinel 2 images was atmospherically corrected using C2RCC
processors. Buoy data were aggregated by depth (0-1 m, 1-2 m, 2-3 m, 3-4 m).
Multiple ML and DL algorithms-including RF, XGBoost, CatBoost, Multilater
Perceptron Networks, and ensembles-were trained and validated using
cross-validation. Systematic band-combination experiments and spatial
aggregation strategies were tested to optimize prediction. Results show
depth-dependent performance. At the surface, C2X-Complex with XGBoost and
ensemble models achieved R2 = 0.89; at 1-2 m, CatBoost and ensemble models
reached R2 = 0.87; at 2-3 m, TOA reflectances with KNN performed best (R2 =
0.81); while at 3-4 m, RF achieved R2 = 0.66. Generated maps successfully
reproduced known eutrophication events (e.g., 2016 crisis, 2025 surge),
confirming robustness. The study delivers an end-to-end, validated methodology
for depth-specific Chl-amapping. Its integration of multispectral band
combinations, buoy calibration, and ML/DL modeling offers a transferable
framework for other turbid coastal systems.

</details>


### [563] [Generative Latent Video Compression](https://arxiv.org/abs/2510.09987)
*Zongyu Guo,Zhaoyang Jia,Jiahao Li,Xiaoyi Zhang,Bin Li,Yan Lu*

Main category: eess.IV

TL;DR: 本文提出GLVC（生成式潜在视频压缩）框架，通过预训练的连续分词器将视频帧投影到感知对齐的潜在空间，并重新设计编解码器架构，有效解决神经视频压缩中速率-失真-感知平衡的挑战及闪烁问题，实现了最先进的感知质量和时间稳定性，同时降低了比特率。


<details>
  <summary>Details</summary>
Motivation: 感知优化对神经压缩至关重要，但在速率-失真-感知之间取得平衡极具挑战性，尤其是在视频压缩中，帧间质量波动常导致感知优化的神经视频编解码器出现闪烁伪影。

Method: GLVC框架采用预训练的连续分词器将视频帧投影到感知对齐的潜在空间，从而将感知约束从速率-失真优化中分离。编解码器架构针对潜在域重新设计，借鉴了现有神经视频编解码器的经验，并创新性地引入了统一的帧内/帧间编码和循环记忆机制。

Result: GLVC在多个基准测试中，DISTS和LPIPS指标达到了最先进的性能。用户研究证实，GLVC以接近现有最新神经视频编解码器一半的速率，实现了与之媲美的效果，并保持了稳定的时间连贯性。

Conclusion: GLVC在感知视频压缩领域迈出了重要一步，通过实现高感知质量和稳定的时间连贯性，同时显著降低比特率，使其更接近实际应用。

Abstract: Perceptual optimization is widely recognized as essential for neural
compression, yet balancing the rate-distortion-perception tradeoff remains
challenging. This difficulty is especially pronounced in video compression,
where frame-wise quality fluctuations often cause perceptually optimized neural
video codecs to suffer from flickering artifacts. In this paper, inspired by
the success of latent generative models, we present Generative Latent Video
Compression (GLVC), an effective framework for perceptual video compression.
GLVC employs a pretrained continuous tokenizer to project video frames into a
perceptually aligned latent space, thereby offloading perceptual constraints
from the rate-distortion optimization. We redesign the codec architecture
explicitly for the latent domain, drawing on extensive insights from prior
neural video codecs, and further equip it with innovations such as unified
intra/inter coding and a recurrent memory mechanism. Experimental results
across multiple benchmarks show that GLVC achieves state-of-the-art performance
in terms of DISTS and LPIPS metrics. Notably, our user study confirms GLVC
rivals the latest neural video codecs at nearly half their rate while
maintaining stable temporal coherence, marking a step toward practical
perceptual video compression.

</details>


### [564] [Towards Efficient 3D Gaussian Human Avatar Compression: A Prior-Guided Framework](https://arxiv.org/abs/2510.10492)
*Shanzhi Yin,Bolin Chen,Xinju Wu,Ru-Ling Liao,Jie Chen,Shiqi Wang,Yan Ye*

Main category: eess.IV

TL;DR: 本文提出了一种高效的3D头像编码框架，利用紧凑的人体先验和规范到目标的变换，实现超低比特率的高质量3D人体头像视频压缩。


<details>
  <summary>Details</summary>
Motivation: 实现超低比特率下高质量的3D人体头像视频压缩，以支持元宇宙应用中的无缝沉浸式多媒体体验。

Method: 该框架通过无网络方式训练一个规范高斯头像作为外观模型，并利用人体先验模板通过紧凑的参数表示（每帧94个参数）捕获时间身体运动。通过分解外观和时间演变，规范头像只需压缩一次，而时间参数则以极低比特率传输。目标头像通过线性混合蒙皮（LBS）变换对规范头像进行变形生成。

Result: 实验结果表明，该方法在主流多视图人体视频数据集上的率失真性能显著优于传统2D/3D编解码器以及现有的可学习动态3D高斯泼溅压缩方法。

Conclusion: 该方法为元宇宙应用中的无缝沉浸式多媒体体验铺平了道路。

Abstract: This paper proposes an efficient 3D avatar coding framework that leverages
compact human priors and canonical-to-target transformation to enable
high-quality 3D human avatar video compression at ultra-low bit rates. The
framework begins by training a canonical Gaussian avatar using articulated
splatting in a network-free manner, which serves as the foundation for avatar
appearance modeling. Simultaneously, a human-prior template is employed to
capture temporal body movements through compact parametric representations.
This decomposition of appearance and temporal evolution minimizes redundancy,
enabling efficient compression: the canonical avatar is shared across the
sequence, requiring compression only once, while the temporal parameters,
consisting of just 94 parameters per frame, are transmitted with minimal
bit-rate. For each frame, the target human avatar is generated by deforming
canonical avatar via Linear Blend Skinning transformation, facilitating
temporal coherent video reconstruction and novel view synthesis. Experimental
results demonstrate that the proposed method significantly outperforms
conventional 2D/3D codecs and existing learnable dynamic 3D Gaussian splatting
compression method in terms of rate-distortion performance on mainstream
multi-view human video datasets, paving the way for seamless immersive
multimedia experiences in meta-verse applications.

</details>


### [565] [JND-Guided Light-Weight Neural Pre-Filter for Perceptual Image Coding](https://arxiv.org/abs/2510.10648)
*Chenlong He,Zijing Dong,Min Li,Zhijian Hao,Leilei Huang,Xiaoyang Zeng,Yibo Fan*

Main category: eess.IV

TL;DR: 本文针对现有JND引导预滤波器计算成本高且缺乏统一基准的问题，提出了FJNDF-Pytorch统一基准平台，并基于此开发了一种新型轻量级CNN。该方法在压缩效率上达到SOTA，同时计算成本极低。


<details>
  <summary>Details</summary>
Motivation: 现有Just Noticeable Distortion (JND) 引导预滤波器计算成本高昂，且缺乏标准化基准进行公平比较，这阻碍了该领域的发展。

Method: 1. 开发并开源了FJNDF-Pytorch，一个用于频域JND引导预滤波器的统一基准平台。2. 基于该平台，提出了一个完整的学习框架，用于训练一种新颖的、轻量级的卷积神经网络（CNN）。

Result: 1. 所提出的方法在多个数据集和编码器上均超越竞争对手，实现了最先进的压缩效率。2. 计算成本极低，处理1080p图像仅需7.15 GFLOPs，是近期轻量级网络的14.1%。

Conclusion: 本工作提供了一个在性能和效率上均表现卓越的、鲁棒的最先进解决方案，并由一个可复现的研究平台支持。开源实现进一步促进了该领域的研究。

Abstract: Just Noticeable Distortion (JND)-guided pre-filter is a promising technique
for improving the perceptual compression efficiency of image coding. However,
existing methods are often computationally expensive, and the field lacks
standardized benchmarks for fair comparison. To address these challenges, this
paper introduces a twofold contribution. First, we develop and open-source
FJNDF-Pytorch, a unified benchmark for frequency-domain JND-Guided pre-filters.
Second, leveraging this platform, we propose a complete learning framework for
a novel, lightweight Convolutional Neural Network (CNN). Experimental results
demonstrate that our proposed method achieves state-of-the-art compression
efficiency, consistently outperforming competitors across multiple datasets and
encoders. In terms of computational cost, our model is exceptionally
lightweight, requiring only 7.15 GFLOPs to process a 1080p image, which is
merely 14.1% of the cost of recent lightweight network. Our work presents a
robust, state-of-the-art solution that excels in both performance and
efficiency, supported by a reproducible research platform. The open-source
implementation is available at https://github.com/viplab-fudan/FJNDF-Pytorch.

</details>


### [566] [Bit Allocation Transfer for Perceptual Quality Enhancement of VVC Intra Coding](https://arxiv.org/abs/2510.10970)
*Runyu Yang,Ivan V. Bajić*

Main category: eess.IV

TL;DR: 本文提出一种低复杂度方法，通过从端到端图像压缩中迁移比特分配知识，为VVC帧内编码生成量化步长图，显著提升了感知质量和编码效率。


<details>
  <summary>Details</summary>
Motivation: 主流图像和视频编码标准（如VVC）采用基于块的混合编码框架，虽然易于优化PSNR，但在优化MS-SSIM等感知对齐指标方面表现不佳。

Method: 该方法引入一个轻量级模型，通过感知损失训练，生成量化步长图。该图隐式捕获块级感知重要性，从而有效地推导出VVC的QP图，以增强感知质量。

Result: 在Kodak和CLIC数据集上的实验表明，该方案在执行时间和感知度量性能方面均有显著优势，MS-SSIM的BD-rate降低超过11%。

Conclusion: 该方案为传统编解码器的感知增强提供了一种高效、实用的途径。

Abstract: Mainstream image and video coding standards -- including state-of-the-art
codecs like H.266/VVC, AVS3, and AV1 -- adopt a block-based hybrid coding
framework. While this framework facilitates straightforward optimization for
Peak Signal-to-Noise Ratio (PSNR), it struggles to effectively optimize
perceptually-aligned metrics such as Multi-Scale Structural Similarity
(MS-SSIM). To address this challenge, this paper proposes a low-complexity
method to enhance perceptual quality in VVC intra coding by transferring bit
allocation knowledge from end-to-end image compression. We introduce a
lightweight model trained with perceptual losses to generate a quantization
step map. This map implicitly captures block-level perceptual importance,
enabling efficient derivation of a QP map for VVC. Experiments on Kodak and
CLIC datasets demonstrate significant advantages, both in execution time and
perceptual metric performance, with more than 11% BD-rate reduction in terms of
MS-SSIM. Our scheme provides an efficient, practical pathway for perceptual
enhancement of traditional codecs.

</details>


### [567] [Generalisation of automatic tumour segmentation in histopathological whole-slide images across multiple cancer types](https://arxiv.org/abs/2510.11182)
*Ole-Johan Skrede,Manohar Pradhan,Maria Xepapadakis Isaksen,Tarjei Sveinsgjerd Hveem,Ljiljana Vlatkovic,Arild Nesbakken,Kristina Lindemann,Gunnar B Kristensen,Jenneke Kasius,Alain G Zeimet,Odd Terje Brustugun,Lill-Tove Rasmussen Busund,Elin H Richardsen,Erik Skaaheim Haug,Bjørn Brennhovd,Emma Rewcastle,Melinda Lillesand,Vebjørn Kvikstad,Emiel Janssen,David J Kerr,Knut Liestøl,Fritz Albregtsen,Andreas Kleppe*

Main category: eess.IV

TL;DR: 开发了一个通用的深度学习模型，用于在不同癌症类型、患者群体和扫描设备上实现肿瘤分割，并取得了出色的表现。


<details>
  <summary>Details</summary>
Motivation: 通过自动化肿瘤分割等任务来辅助病理学家，并旨在开发一个适用于组织病理学图像的通用肿瘤分割模型，以评估其在不同癌症类型中的性能。

Method: 使用超过20,000张来自4,000多名结直肠癌、子宫内膜癌、肺癌或前列腺癌患者的全玻片图像开发了该模型。通过对来自六种癌症类型的3,000多名患者的外部队列进行预设分析来验证性能，并对来自癌症基因组图谱（TCGA）的1,500多名额外患者进行了探索性分析。

Result: 在所有整块切除标本的验证队列和TCGA队列中，平均Dice系数均超过80%。与专门针对单一癌症类型的模型相比，通用模型未观察到性能下降。

Conclusion: 广泛而严格的评估表明，通过单一模型实现跨癌症类型、患者群体、样本制备和玻片扫描仪的通用肿瘤分割是可行的。

Abstract: Deep learning is expected to aid pathologists by automating tasks such as
tumour segmentation. We aimed to develop one universal tumour segmentation
model for histopathological images and examine its performance in different
cancer types. The model was developed using over 20 000 whole-slide images from
over 4 000 patients with colorectal, endometrial, lung, or prostate carcinoma.
Performance was validated in pre-planned analyses on external cohorts with over
3 000 patients across six cancer types. Exploratory analyses included over 1
500 additional patients from The Cancer Genome Atlas. Average Dice coefficient
was over 80% in all validation cohorts with en bloc resection specimens and in
The Cancer Genome Atlas cohorts. No loss of performance was observed when
comparing the universal model with models specialised on single cancer types.
In conclusion, extensive and rigorous evaluations demonstrate that generic
tumour segmentation by a single model is possible across cancer types, patient
populations, sample preparations, and slide scanners.

</details>


### [568] [GADA: Graph Attention-based Detection Aggregation for Ultrasound Video Classification](https://arxiv.org/abs/2510.11437)
*Li Chen,Naveen Balaraju,Jochen Kruecker,Balasundar Raju,Alvin Chen*

Main category: eess.IV

TL;DR: GADA是一种基于图注意力网络的检测聚合框架，将医学超声视频分类重新定义为图推理问题，通过检测病理相关区域并用图注意力网络聚合，在肺部超声数据上表现优异并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 医学超声视频分析面临序列长度可变、空间线索不明显以及需要可解释的视频级评估等挑战。

Method: 引入GADA（Graph Attention-based Detection Aggregation）框架。它将视频分类重构为对空间局部感兴趣区域的图推理问题。GADA不依赖3D CNN或全帧分析，而是检测跨帧的病理相关区域并将其表示为时空图中的节点，边编码空间和时间依赖性。图注意力网络通过边缘感知注意力聚合这些节点级预测，生成紧凑、有区分度的视频级输出。

Result: 在大型、多中心临床肺部超声数据集上进行评估，GADA在两项病理视频分类任务上优于传统基线，同时提供了可解释的区域级和帧级注意力。

Conclusion: GADA为医学超声视频分析提供了一个有效且可解释的框架，通过将视频分类转化为图推理问题，解决了现有方法在处理可变序列长度、微妙空间线索和解释性方面的不足。

Abstract: Medical ultrasound video analysis is challenging due to variable sequence
lengths, subtle spatial cues, and the need for interpretable video-level
assessment. We introduce GADA, a Graph Attention-based Detection Aggregation
framework that reformulates video classification as a graph reasoning problem
over spatially localized regions of interest. Rather than relying on 3D CNNs or
full-frame analysis, GADA detects pathology-relevant regions across frames and
represents them as nodes in a spatiotemporal graph, with edges encoding spatial
and temporal dependencies. A graph attention network aggregates these
node-level predictions through edge-aware attention to generate a compact,
discriminative video-level output. Evaluated on a large-scale, multi-center
clinical lung ultrasound dataset, GADA outperforms conventional baselines on
two pathology video classification tasks while providing interpretable region-
and frame-level attention.

</details>
