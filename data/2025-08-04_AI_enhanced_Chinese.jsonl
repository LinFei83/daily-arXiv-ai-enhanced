{"id": "2508.00079", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00079", "abs": "https://arxiv.org/abs/2508.00079", "authors": ["Oshayer Siddique", "J. M Areeb Uzair Alam", "Md Jobayer Rahman Rafy", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems", "comment": "Under review, 18 pages, 4 figures, 7 tables", "summary": "The discipline of physics stands as a cornerstone of human intellect, driving\nthe evolution of technology and deepening our understanding of the fundamental\nprinciples of the cosmos. Contemporary literature includes some works centered\non the task of solving physics problems - a crucial domain of natural language\nreasoning. In this paper, we evaluate the performance of frontier LLMs in\nsolving physics problems, both mathematical and descriptive. We also employ a\nplethora of inference-time techniques and agentic frameworks to improve the\nperformance of the models. This includes the verification of proposed solutions\nin a cumulative fashion by other, smaller LLM agents, and we perform a\ncomparative analysis of the performance that the techniques entail. There are\nsignificant improvements when the multi-agent framework is applied to problems\nthat the models initially perform poorly on. Furthermore, we introduce a new\nevaluation benchmark for physics problems, ${\\rm P{\\small HYSICS}E{\\small\nVAL}}$, consisting of 19,609 problems sourced from various physics textbooks\nand their corresponding correct solutions scraped from physics forums and\neducational websites. Our code and data are publicly available at\nhttps://github.com/areebuzair/PhysicsEval.", "AI": {"tldr": "本文评估了前沿大型语言模型（LLMs）在解决数学和描述性物理问题方面的表现，并采用多智能体框架等推理时技术来提升模型性能，同时引入了一个新的物理问题评估基准PHYSEVAL。", "motivation": "物理学是人类智能的基石，解决物理问题是自然语言推理中的关键领域。研究旨在评估当前LLMs在这一领域的表现，并探索提升其解决复杂物理问题能力的方法。", "method": "研究方法包括：1) 评估前沿LLMs在数学和描述性物理问题上的性能。2) 采用多种推理时技术和智能体框架（如通过小型LLM智能体累积验证解决方案）来提升模型性能。3) 引入一个新的物理问题评估基准PHYSEVAL，包含19,609个从物理教科书和在线资源收集的问题及其解决方案。", "result": "研究结果显示，当多智能体框架应用于模型初始表现不佳的问题时，性能有显著提升。此外，本文成功构建并推出了大型物理问题评估基准PHYSEVAL。", "conclusion": "多智能体框架能有效提高LLMs在解决复杂物理问题上的表现。新引入的PHYSEVAL基准为未来物理问题解决能力的研究提供了大规模、高质量的评估工具。"}}
{"id": "2508.00086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00086", "abs": "https://arxiv.org/abs/2508.00086", "authors": ["Kelly Kendro", "Jeffrey Maloney", "Scott Jarvis"], "title": "Do LLMs produce texts with \"human-like\" lexical diversity?", "comment": "35 pages; includes abstract", "summary": "The degree to which LLMs produce writing that is truly human-like remains\nunclear despite the extensive empirical attention that this question has\nreceived. The present study addresses this question from the perspective of\nlexical diversity. Specifically, the study investigates patterns of lexical\ndiversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,\nand -4.5) in comparison with texts written by L1 and L2 English participants (n\n= 240) across four education levels. Six dimensions of lexical diversity were\nmeasured in each text: volume, abundance, variety-repetition, evenness,\ndisparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and\nSupport Vector Machines revealed that the LLM-generated texts differed\nsignificantly from human-written texts for each variable, with ChatGPT-o4 mini\nand -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated\nhigher levels of lexical diversity despite producing fewer tokens. The human\nwriters' lexical diversity did not differ across subgroups (i.e., education,\nlanguage status). Altogether, the results indicate that LLMs do not produce\nhuman-like texts in relation to lexical diversity, and the newer LLMs produce\nless human-like texts than older models. We discuss the implications of these\nresults for language pedagogy and related applications.", "AI": {"tldr": "研究发现，大型语言模型（LLM）生成的文本在词汇多样性方面与人类文本存在显著差异，特别是较新的模型（如ChatGPT-o4 mini和-4.5）与人类文本的差异更大，表明LLM未能生成真正类人文本。", "motivation": "尽管已有大量实证研究，但LLM生成文本的“类人”程度仍不明确。本研究旨在从词汇多样性的角度，深入探讨LLM生成文本的类人程度。", "method": "研究对比了四种ChatGPT模型（-3.5、-4、-o4 mini和-4.5）生成的文本与240名母语（L1）和第二语言（L2）英语使用者（来自四个教育水平）撰写的文本的词汇多样性。测量了词汇多样性的六个维度：数量、丰富度、多样性-重复性、均匀度、差异性和分散度。数据分析采用了单向多元方差分析（MANOVAs）、单向方差分析（ANOVAs）和支持向量机（SVMs）。", "result": "研究结果显示，LLM生成的文本在所有词汇多样性变量上都与人类文本存在显著差异，其中ChatGPT-o4 mini和-4.5的差异最大。在这两个模型中，ChatGPT-4.5尽管生成的词元数量较少，但其词汇多样性水平更高。人类作者的词汇多样性在不同亚组（即教育水平、语言状态）之间没有显著差异。", "conclusion": "总而言之，研究表明LLM在词汇多样性方面未能生成类人文本，并且较新的LLM模型生成的文本比旧模型更不像人类文本。研究讨论了这些结果对语言教学及相关应用的影响。"}}
{"id": "2508.00095", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00095", "abs": "https://arxiv.org/abs/2508.00095", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture", "comment": "Preprint. Manuscript currently under review", "summary": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.", "AI": {"tldr": "计算人文学科需要更深入的方法论理论化，本文将计算建模视为一种翻译过程，并指出将符号复杂数据简化处理的错误，提出了改进建议。", "motivation": "为实现计算人文学科的认识论和解释清晰性，促进学科成熟，并避免在建模过程中出现潜在的、有影响的“翻译”错误，需要加强方法论理论化。", "method": "将计算人文学科的建模工作框定为从文化/语言领域到计算/数学领域的“翻译”过程。引入“符号复杂性”概念，定义为文本意义在不同解释视角下变化的程度。分析了当前主流建模实践（尤其是在评估方面）如何错误地将符号复杂数据视为符号简单数据。", "result": "主流建模实践（特别是在评估中）存在一个“翻译错误”，即为了表面上的清晰和认识论上的便利，将符号复杂数据当作符号简单数据处理。文章提出了针对研究人员的几项建议，以更好地解决这些认识论问题。", "conclusion": "计算人文学科的研究者需要更深入地思考其建模方法，尤其是在处理符号复杂数据时，避免将其过度简化，以提高研究的内部一致性、解释透明度和认识论严谨性。"}}
{"id": "2508.00109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00109", "abs": "https://arxiv.org/abs/2508.00109", "authors": ["Mingda Chen", "Yang Li", "Xilun Chen", "Adina Williams", "Gargi Ghosh", "Scott Yih"], "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality", "comment": null, "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.", "AI": {"tldr": "本文介绍了FACTORY，一个大规模、经人工验证的提示集，用于评估长篇事实性生成。研究发现，现有SOTA模型在FACTORY上的事实错误率远高于其他数据集，表明其更具挑战性。", "motivation": "现有长篇事实性评估基准缺乏人工验证，导致潜在的质量问题。", "method": "开发了FACTORY数据集，采用“模型在环”方法并经人工精炼，包含具有挑战性、寻求事实、可回答且无歧义的提示。使用FACTORY和现有数据集对6个最先进的语言模型进行了人工评估。", "result": "FACTORY是一个具有挑战性的基准：最先进模型在FACTORY上生成回复中约40%的主张不符合事实，而其他数据集上仅为10%。", "conclusion": "FACTORY是一个可靠的基准，强调了模型需要对长尾事实进行推理，证明了其优于现有基准的可靠性和挑战性。"}}
{"id": "2508.00154", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00154", "abs": "https://arxiv.org/abs/2508.00154", "authors": ["Babak Esmaeili", "Hamidreza Modares", "Stefano Di Cairano"], "title": "Data-Driven Motion Planning for Uncertain Nonlinear Systems", "comment": null, "summary": "This paper proposes a data-driven motion-planning framework for nonlinear\nsystems that constructs a sequence of overlapping invariant polytopes. Around\neach randomly sampled waypoint, the algorithm identifies a convex admissible\nregion and solves data-driven linear-matrix-inequality problems to learn\nseveral ellipsoidal invariant sets together with their local state-feedback\ngains. The convex hull of these ellipsoids, still invariant under a\npiece-wise-affine controller obtained by interpolating the gains, is then\napproximated by a polytope. Safe transitions between nodes are ensured by\nverifying the intersection of consecutive convex-hull polytopes and introducing\nan intermediate node for a smooth transition. Control gains are interpolated in\nreal time via simplex-based interpolation, keeping the state inside the\ninvariant polytopes throughout the motion. Unlike traditional approaches that\nrely on system dynamics models, our method requires only data to compute safe\nregions and design state-feedback controllers. The approach is validated\nthrough simulations, demonstrating the effectiveness of the proposed method in\nachieving safe, dynamically feasible paths for complex nonlinear systems.", "AI": {"tldr": "本文提出了一种数据驱动的非线性系统运动规划框架，通过构建一系列重叠的不变多面体，实现安全、动态可行的路径规划，且无需系统动力学模型。", "motivation": "传统运动规划方法依赖于精确的系统动力学模型，而本文旨在开发一种仅需数据即可计算安全区域并设计状态反馈控制器的方法，以应对复杂非线性系统模型难以获取或不准确的问题。", "method": "该方法围绕随机采样的路径点，识别凸可容许区域，并解决数据驱动的线性矩阵不等式问题（LMI），学习多个椭球不变集及其局部状态反馈增益。这些椭球的凸包（在分段仿射控制器下仍保持不变）被近似为多面体。通过验证连续凸包多面体的交集并引入中间节点，确保节点间的安全平滑过渡。控制增益通过单纯形插值实时插值，确保状态始终保持在不变多面体内部。", "result": "通过仿真验证，所提出的方法能够有效地为复杂非线性系统生成安全、动态可行的路径。", "conclusion": "本文提出的数据驱动运动规划框架，无需显式系统动力学模型，仅依赖数据即可为非线性系统规划安全、动态可行的路径，显示出其在实际应用中的有效性和潜力。"}}
{"id": "2508.00155", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00155", "abs": "https://arxiv.org/abs/2508.00155", "authors": ["Tomasz Szczepański", "Szymon Płotka", "Michal K. Grzeszczyk", "Arleta Adamowicz", "Piotr Fudalej", "Przemysław Korzeniowski", "Tomasz Trzciński", "Arkadiusz Sitek"], "title": "GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation", "comment": "Accepted for the 28th International Conference on Medical Image\n  Computing and Computer Assisted Intervention (MICCAI) 2025", "summary": "Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains\nchallenging, especially for fine structures like root apices, which is critical\nfor assessing root resorption in orthodontics. We introduce GEPAR3D, a novel\napproach that unifies instance detection and multi-class segmentation into a\nsingle step tailored to improve root segmentation. Our method integrates a\nStatistical Shape Model of dentition as a geometric prior, capturing anatomical\ncontext and morphological consistency without enforcing restrictive adjacency\nconstraints. We leverage a deep watershed method, modeling each tooth as a\ncontinuous 3D energy basin encoding voxel distances to boundaries. This\ninstance-aware representation ensures accurate segmentation of narrow, complex\nroot apices. Trained on publicly available CBCT scans from a single center, our\nmethod is evaluated on external test sets from two in-house and two public\nmedical centers. GEPAR3D achieves the highest overall segmentation performance,\naveraging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the\nsecond-best method) and increasing recall to 95.2% (+9.5%) across all test\nsets. Qualitative analyses demonstrated substantial improvements in root\nsegmentation quality, indicating significant potential for more accurate root\nresorption assessment and enhanced clinical decision-making in orthodontics. We\nprovide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.", "AI": {"tldr": "GEPAR3D是一种新颖的牙齿CBCT分割方法，它将实例检测和多类别分割统一起来，并结合牙列的统计形状模型作为几何先验和深度分水岭方法，显著提高了牙根（尤其是根尖）的分割精度。", "motivation": "锥形束CT（CBCT）中的牙齿分割，尤其是根尖等精细结构，仍然具有挑战性，而这对于正畸中评估牙根吸收至关重要。", "method": "引入GEPAR3D，一种将实例检测和多类别分割统一为一步的方法。它整合了牙列的统计形状模型（SSM）作为几何先验，并利用深度分水岭方法，将每颗牙齿建模为连续的3D能量盆地，编码体素到边界的距离。", "result": "GEPAR3D在所有测试集上实现了最高的整体分割性能，平均Dice相似系数（DSC）达到95.0%（比次优方法高2.8%），召回率提高到95.2%（提高9.5%）。定性分析显示根部分割质量显著改善。", "conclusion": "该方法在根部分割质量上取得了实质性改进，表明其在更准确地评估牙根吸收和增强正畸临床决策方面具有显著潜力。"}}
{"id": "2508.00097", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00097", "abs": "https://arxiv.org/abs/2508.00097", "authors": ["Zhigen Zhao", "Liuchuan Yu", "Ke Jing", "Ning Yang"], "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation", "comment": "6 pages, 6 figures, project link: https://github.com/XR-Robotics", "summary": "The rapid advancement of Vision-Language-Action models has created an urgent\nneed for large-scale, high-quality robot demonstration datasets. Although\nteleoperation is the predominant method for data collection, current approaches\nsuffer from limited scalability, complex setup procedures, and suboptimal data\nquality. This paper presents XRoboToolkit, a cross-platform framework for\nextended reality based robot teleoperation built on the OpenXR standard. The\nsystem features low-latency stereoscopic visual feedback, optimization-based\ninverse kinematics, and support for diverse tracking modalities including head,\ncontroller, hand, and auxiliary motion trackers. XRoboToolkit's modular\narchitecture enables seamless integration across robotic platforms and\nsimulation environments, spanning precision manipulators, mobile robots, and\ndexterous hands. We demonstrate the framework's effectiveness through precision\nmanipulation tasks and validate data quality by training VLA models that\nexhibit robust autonomous performance.", "AI": {"tldr": "XRoboToolkit是一个基于OpenXR的跨平台扩展现实机器人遥操作框架，旨在解决大规模、高质量机器人示教数据集收集的扩展性、复杂性和数据质量问题。", "motivation": "视觉-语言-动作（VLA）模型的快速发展急需大规模、高质量的机器人示教数据集。然而，当前主流的遥操作数据收集方法存在可扩展性有限、设置复杂和数据质量不佳的问题。", "method": "本文提出了XRoboToolkit，一个基于OpenXR标准的跨平台扩展现实机器人遥操作框架。该系统具有低延迟立体视觉反馈、基于优化的逆运动学，并支持多种跟踪模式（头部、控制器、手部和辅助运动跟踪器）。其模块化架构实现了机器人平台和仿真环境（包括精密机械臂、移动机器人和灵巧手）之间的无缝集成。", "result": "该框架在精密操作任务中展示了有效性，并通过训练表现出强大自主性能的VLA模型，验证了所收集数据的高质量。", "conclusion": "XRoboToolkit是一个有效的框架，能够实现可扩展、高质量的机器人数据收集，为VLA模型训练提供支持。"}}
{"id": "2508.00053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00053", "abs": "https://arxiv.org/abs/2508.00053", "authors": ["Jie Zhu", "Yiyang Su", "Minchul Kim", "Anil Jain", "Xiaoming Liu"], "title": "A Quality-Guided Mixture of Score-Fusion Experts Framework for Human Recognition", "comment": "Accepted to ICCV 2025. 11 pages, 5 figures", "summary": "Whole-body biometric recognition is a challenging multimodal task that\nintegrates various biometric modalities, including face, gait, and body. This\nintegration is essential for overcoming the limitations of unimodal systems.\nTraditionally, whole-body recognition involves deploying different models to\nprocess multiple modalities, achieving the final outcome by score-fusion (e.g.,\nweighted averaging of similarity matrices from each model). However, these\nconventional methods may overlook the variations in score distributions of\nindividual modalities, making it challenging to improve final performance. In\nthis work, we present \\textbf{Q}uality-guided \\textbf{M}ixture of score-fusion\n\\textbf{E}xperts (QME), a novel framework designed for improving whole-body\nbiometric recognition performance through a learnable score-fusion strategy\nusing a Mixture of Experts (MoE). We introduce a novel pseudo-quality loss for\nquality estimation with a modality-specific Quality Estimator (QE), and a score\ntriplet loss to improve the metric performance. Extensive experiments on\nmultiple whole-body biometric datasets demonstrate the effectiveness of our\nproposed approach, achieving state-of-the-art results across various metrics\ncompared to baseline methods. Our method is effective for multimodal and\nmulti-model, addressing key challenges such as model misalignment in the\nsimilarity score domain and variability in data quality.", "AI": {"tldr": "提出了一种名为QME的新型框架，通过可学习的专家混合（MoE）分数融合策略，并结合质量估计和分数三元组损失，显著提升了全身生物识别的性能，达到了SOTA水平。", "motivation": "全身生物识别是一个具有挑战性的多模态任务，传统的分数融合方法（如加权平均）往往忽略了各个模态分数分布的差异，这限制了最终性能的提升。", "method": "本文提出了一种名为QME（Quality-guided Mixture of score-fusion Experts）的框架，通过使用专家混合（MoE）实现可学习的分数融合策略。具体方法包括引入一种新颖的伪质量损失，用于模态特定的质量估计器（QE），以及一个分数三元组损失，以提高度量性能。", "result": "在多个全身生物识别数据集上进行了广泛实验，结果表明所提出的方法是有效的，与基线方法相比，在各种指标上均取得了最先进的（SOTA）结果。", "conclusion": "QME方法对多模态和多模型均有效，成功解决了相似度分数域中的模型错位以及数据质量可变性等关键挑战，显著提升了全身生物识别的性能。"}}
{"id": "2508.00081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00081", "abs": "https://arxiv.org/abs/2508.00081", "authors": ["Fred Mutisya", "Shikoh Gitau", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha"], "title": "Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench", "comment": null, "summary": "HealthBench, a benchmark designed to measure the capabilities of AI systems\nfor health better (Arora et al., 2025), has advanced medical language model\nevaluation through physician-crafted dialogues and transparent rubrics.\nHowever, its reliance on expert opinion, rather than high-tier clinical\nevidence, risks codifying regional biases and individual clinician\nidiosyncrasies, further compounded by potential biases in automated grading\nsystems. These limitations are particularly magnified in low- and middle-income\nsettings, where issues like sparse neglected tropical disease coverage and\nregion-specific guideline mismatches are prevalent.\n  The unique challenges of the African context, including data scarcity,\ninadequate infrastructure, and nascent regulatory frameworks, underscore the\nurgent need for more globally relevant and equitable benchmarks. To address\nthese shortcomings, we propose anchoring reward functions in version-controlled\nClinical Practice Guidelines (CPGs) that incorporate systematic reviews and\nGRADE evidence ratings.\n  Our roadmap outlines \"evidence-robust\" reinforcement learning via\nrubric-to-guideline linkage, evidence-weighted scoring, and contextual override\nlogic, complemented by a focus on ethical considerations and the integration of\ndelayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,\nwhile preserving HealthBench's transparency and physician engagement, we aim to\nfoster medical language models that are not only linguistically polished but\nalso clinically trustworthy, ethically sound, and globally relevant.", "AI": {"tldr": "本文提出通过将奖励函数锚定在版本控制的临床实践指南（CPG）中，并结合系统评价和GRADE证据评级，来改进医疗AI基准（如HealthBench）的评估，以解决其依赖专家意见导致的地域偏见和在低收入环境中的局限性，从而培养更值得信赖和全球相关的医疗语言模型。", "motivation": "HealthBench等现有医疗AI评估基准依赖专家意见而非高阶临床证据，这可能导致区域偏见和个体临床医生特质的固化，并受自动化评分系统潜在偏见的影响。这些局限性在低收入和中等收入国家尤其明显，非洲等地区面临数据稀缺、基础设施不足和监管框架不成熟等独特挑战，迫切需要更具全球相关性和公平性的基准。", "method": "提出将奖励函数锚定在版本控制的临床实践指南（CPG）中，这些指南应包含系统性综述和GRADE证据评级。路线图包括通过“证据鲁棒”的强化学习，实现评分标准与指南的联动、证据加权评分和情境覆盖逻辑，并关注伦理考量和整合延迟结果反馈。", "result": "通过将奖励重新基于经过严格审查的CPG，同时保留HealthBench的透明度和医生参与度，旨在培养出不仅语言流畅，而且临床可靠、符合伦理并具有全球相关性的医疗语言模型。", "conclusion": "重新将奖励机制建立在严格审查的临床实践指南（CPG）之上，同时保持现有基准的优点，是解决医疗AI评估中偏见和局限性的关键，从而推动开发出更值得信赖、符合伦理且全球适用的医疗语言模型。"}}
{"id": "2508.00121", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00121", "abs": "https://arxiv.org/abs/2508.00121", "authors": ["Xiao Zhang", "Johan bos"], "title": "Is neural semantic parsing good at ellipsis resolution, or isn't it?", "comment": "Accepted by 16th IWCS", "summary": "Neural semantic parsers have shown good overall performance for a variety of\nlinguistic phenomena, reaching semantic matching scores of more than 90%. But\nhow do such parsers perform on strongly context-sensitive phenomena, where\nlarge pieces of semantic information need to be duplicated to form a meaningful\nsemantic representation? A case in point is English verb phrase ellipsis, a\nconstruct where entire verb phrases can be abbreviated by a single auxiliary\nverb. Are the otherwise known as powerful semantic parsers able to deal with\nellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with\ntheir fully resolved meaning representation and used this as a challenge set\nfor a large battery of neural semantic parsers. Although these parsers\nperformed very well on the standard test set, they failed in the instances with\nellipsis. Data augmentation", "AI": {"tldr": "神经语义解析器在处理动词短语省略等强上下文敏感现象时表现不佳。", "motivation": "虽然神经语义解析器在多种语言现象上表现出色，但其在处理需要大量语义信息复制的强上下文敏感现象（如英语动词短语省略）时的性能尚不明确。", "method": "构建了一个包含120个动词短语省略案例及其完整解析语义表示的语料库，并将其作为挑战集，用于测试多种神经语义解析器。", "result": "尽管这些解析器在标准测试集上表现良好，但在省略句实例中却失败了。", "conclusion": "神经语义解析器虽然整体性能优异，但目前尚不能有效处理动词短语省略这类强上下文敏感现象。"}}
{"id": "2508.00156", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00156", "abs": "https://arxiv.org/abs/2508.00156", "authors": ["Shuhao Qi", "Zhiqi Tang", "Zhiyong Sun", "Sofie Haesaert"], "title": "Integrating Opinion Dynamics into Safety Control for Decentralized Airplane Encounter Resolution", "comment": null, "summary": "As the airspace becomes increasingly congested, decentralized conflict\nresolution methods for airplane encounters have become essential. While\ndecentralized safety controllers can prevent dangerous midair collisions, they\ndo not always ensure prompt conflict resolution. As a result, airplane progress\nmay be blocked for extended periods in certain situations. To address this\nblocking phenomenon, this paper proposes integrating bio-inspired nonlinear\nopinion dynamics into the airplane safety control framework, thereby\nguaranteeing both safety and blocking-free resolution. In particular, opinion\ndynamics enable the safety controller to achieve collaborative decision-making\nfor blocking resolution and facilitate rapid, safe coordination without relying\non communication or preset rules. Extensive simulation results validate the\nimproved flight efficiency and safety guarantees. This study provides practical\ninsights into the design of autonomous controllers for airplanes.", "AI": {"tldr": "该研究提出将生物启发式非线性意见动力学整合到飞机安全控制框架中，以解决分散式冲突解决中出现的阻塞问题，同时确保安全和无阻塞的快速协调。", "motivation": "随着空域日益拥堵，分散式冲突解决方法变得至关重要。然而，现有的分散式安全控制器虽然能防止碰撞，但不能总是确保及时解决冲突，导致飞机进度在某些情况下可能被长时间阻塞。", "method": "本文提出将生物启发式非线性意见动力学集成到飞机安全控制框架中。这种方法使安全控制器能够实现协作决策，从而解决阻塞问题，并在不依赖通信或预设规则的情况下促进快速、安全的协调。", "result": "广泛的仿真结果验证了该方法能提高飞行效率并提供安全保障。意见动力学使得安全控制器能够进行协作决策，从而实现无阻塞的冲突解决，并促进快速、安全的协调。", "conclusion": "这项研究为设计飞机的自主控制器提供了实用的见解，表明生物启发式意见动力学可以有效解决分散式空域管理中的阻塞问题，同时保持高安全性和效率。"}}
{"id": "2508.00164", "categories": ["eess.IV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.00164", "abs": "https://arxiv.org/abs/2508.00164", "authors": ["Sourya Sengupta", "Jianquan Xu", "Phuong Nguyen", "Frank J. Brooks", "Yang Liu", "Mark A. Anastasio"], "title": "On the Utility of Virtual Staining for Downstream Applications as it relates to Task Network Capacity", "comment": null, "summary": "Virtual staining, or in-silico-labeling, has been proposed to computationally\ngenerate synthetic fluorescence images from label-free images by use of deep\nlearning-based image-to-image translation networks. In most reported studies,\nvirtually stained images have been assessed only using traditional image\nquality measures such as structural similarity or signal-to-noise ratio.\nHowever, in biomedical imaging, images are typically acquired to facilitate an\nimage-based inference, which we refer to as a downstream biological or clinical\ntask. This study systematically investigates the utility of virtual staining\nfor facilitating clinically relevant downstream tasks (like segmentation or\nclassification) with consideration of the capacity of the deep neural networks\nemployed to perform the tasks. Comprehensive empirical evaluations were\nconducted using biological datasets, assessing task performance by use of\nlabel-free, virtually stained, and ground truth fluorescence images. The\nresults demonstrated that the utility of virtual staining is largely dependent\non the ability of the segmentation or classification task network to extract\nmeaningful task-relevant information, which is related to the concept of\nnetwork capacity. Examples are provided in which virtual staining does not\nimprove, or even degrades, segmentation or classification performance when the\ncapacity of the associated task network is sufficiently large. The results\ndemonstrate that task network capacity should be considered when deciding\nwhether to perform virtual staining.", "AI": {"tldr": "本研究系统性评估了虚拟染色技术在促进下游生物医学任务（如分割和分类）中的实用性，并发现其效用高度依赖于执行这些任务的深度神经网络的容量。", "motivation": "现有虚拟染色研究主要使用传统图像质量指标进行评估，而忽略了其在促进下游生物学或临床任务（图像推理）中的实际效用，这在生物医学成像中至关重要。", "method": "通过使用生物数据集，系统性地比较了使用无标记图像、虚拟染色图像和真实荧光图像进行分割或分类任务的性能。评估中特别考虑了执行这些任务的深度神经网络的容量。", "result": "虚拟染色的效用很大程度上取决于分割或分类任务网络提取有意义的、任务相关信息的能力，这与网络容量概念相关。研究表明，当相关任务网络的容量足够大时，虚拟染色可能无法改善甚至会降低分割或分类性能。", "conclusion": "在决定是否进行虚拟染色时，应充分考虑任务网络的容量。"}}
{"id": "2508.00162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00162", "abs": "https://arxiv.org/abs/2508.00162", "authors": ["Noboru Myers", "Obin Kwon", "Sankalp Yamsani", "Joohyung Kim"], "title": "CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System", "comment": null, "summary": "Recent advances in teleoperation have demonstrated robots performing complex\nmanipulation tasks. However, existing works rarely support whole-body\njoint-level teleoperation for humanoid robots, limiting the diversity of tasks\nthat can be accomplished. This work presents Controller for Humanoid Imitation\nand Live Demonstration (CHILD), a compact reconfigurable teleoperation system\nthat enables joint level control over humanoid robots. CHILD fits within a\nstandard baby carrier, allowing the operator control over all four limbs, and\nsupports both direct joint mapping for full-body control and loco-manipulation.\nAdaptive force feedback is incorporated to enhance operator experience and\nprevent unsafe joint movements. We validate the capabilities of this system by\nconducting loco-manipulation and full-body control examples on a humanoid robot\nand multiple dual-arm systems. Lastly, we open-source the design of the\nhardware promoting accessibility and reproducibility. Additional details and\nopen-source information are available at our project website:\nhttps://uiuckimlab.github.io/CHILD-pages.", "AI": {"tldr": "本文提出了一种名为CHILD的紧凑型可重构遥操作系统，支持人形机器人进行全身关节级别的遥操作，并集成了自适应力反馈以提升操作体验和安全性。", "motivation": "现有遥操作系统很少支持人形机器人全身关节级别的遥操作，这限制了机器人能完成任务的多样性。", "method": "开发了CHILD系统，该系统紧凑且可重构，可放入标准婴儿背带中，允许操作员控制机器人的所有四肢，支持全身直接关节映射控制和运动-操作。系统还融入了自适应力反馈，以提升操作员体验并防止不安全的关节运动。", "result": "通过在人形机器人和多个双臂系统上进行运动-操作和全身控制示例，验证了CHILD系统的能力。此外，硬件设计已开源。", "conclusion": "CHILD系统为人形机器人提供了强大的全身关节级遥操作能力，扩展了其任务多样性，并通过力反馈提高了操作安全性与体验，并促进了研究的普及与复现。"}}
{"id": "2508.00085", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00085", "abs": "https://arxiv.org/abs/2508.00085", "authors": ["Raiyaan Abdullah", "Jared Claypoole", "Michael Cogswell", "Ajay Divakaran", "Yogesh Rawat"], "title": "Punching Bag vs. Punching Person: Motion Transferability in Videos", "comment": "Accepted to ICCV 2025 main conference", "summary": "Action recognition models demonstrate strong generalization, but can they\neffectively transfer high-level motion concepts across diverse contexts, even\nwithin similar distributions? For example, can a model recognize the broad\naction \"punching\" when presented with an unseen variation such as \"punching\nperson\"? To explore this, we introduce a motion transferability framework with\nthree datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)\nKinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural\nvideo datasets. We evaluate 13 state-of-the-art models on these benchmarks and\nobserve a significant drop in performance when recognizing high-level actions\nin novel contexts. Our analysis reveals: 1) Multimodal models struggle more\nwith fine-grained unknown actions than with coarse ones; 2) The bias-free\nSyn-TA proves as challenging as real-world datasets, with models showing\ngreater performance drops in controlled settings; 3) Larger models improve\ntransferability when spatial cues dominate but struggle with intensive temporal\nreasoning, while reliance on object and background cues hinders generalization.\nWe further explore how disentangling coarse and fine motions can improve\nrecognition in temporally challenging datasets. We believe this study\nestablishes a crucial benchmark for assessing motion transferability in action\nrecognition. Datasets and relevant code:\nhttps://github.com/raiyaan-abdullah/Motion-Transfer.", "AI": {"tldr": "该研究探讨了动作识别模型在不同上下文中的运动概念可迁移性，发现现有SOTA模型在识别新情境下的高级动作时性能显著下降，尤其是在处理细粒度动作和时间推理时。", "motivation": "现有动作识别模型虽然泛化能力强，但仍不清楚它们是否能有效地将高层运动概念（例如，从“拳击”到“拳击某人”）在多样化甚至相似的上下文中进行迁移。", "method": "引入了一个运动可迁移性框架，并构建了三个数据集：Syn-TA（合成3D物体运动）、Kinetics400-TA和Something-Something-v2-TA（均改编自真实视频）。研究评估了13个最先进的模型在这些基准上的表现，并分析了其性能。", "result": "1) 在新情境下识别高级动作时，模型性能显著下降。2) 多模态模型在处理细粒度未知动作时比粗粒度动作更困难。3) 偏置无关的Syn-TA数据集与真实世界数据集一样具有挑战性，模型在受控设置下表现出更大的性能下降。4) 较大模型在空间线索占主导时能提升可迁移性，但在密集时间推理方面表现不佳；对物体和背景线索的依赖会阻碍泛化。", "conclusion": "这项研究为评估动作识别中的运动可迁移性建立了一个关键基准。解耦粗粒度和细粒度运动可能有助于改善在时间挑战性数据集上的识别效果。"}}
{"id": "2508.00106", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00106", "abs": "https://arxiv.org/abs/2508.00106", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "AI": {"tldr": "本文提出了一种基于动态玻尔兹曼softmax强化学习的方法，用于在满足HyperTWTL安全约束下学习安全感知最优策略。", "motivation": "现有研究在时间逻辑约束的安全强化学习（SRL）方面有所进展，但在利用超性质（如HyperTWTL）探索安全感知强化学习（RL）方面存在显著研究空白，特别是在机器人应用中的安全、不透明性和并发性方面。", "method": "将智能体动力学建模为马尔可夫决策过程（MDP），并将不透明性/安全约束形式化为HyperTWTL。提出了一种使用动态玻尔兹曼softmax强化学习的方法，以学习满足HyperTWTL约束的安全感知最优策略。", "result": "通过一个机器人取送任务案例研究，证明了所提方法的有效性和可扩展性。与两种基线RL算法进行比较，结果显示所提方法表现更优。", "conclusion": "所提出的方法能够有效地学习满足HyperTWTL约束的安全感知最优策略，并在实践中展现出优越的性能和可扩展性。"}}
{"id": "2508.00185", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00185", "abs": "https://arxiv.org/abs/2508.00185", "authors": ["Alper Yaman", "Jannik Schwab", "Christof Nitsche", "Abhirup Sinha", "Marco Huber"], "title": "Comparison of Large Language Models for Deployment Requirements", "comment": null, "summary": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.", "AI": {"tldr": "本文提供了一个不断更新的开源大型语言模型（LLM）比较列表，以帮助研究人员和公司根据许可和硬件要求选择合适的LLM。", "motivation": "大型语言模型（LLM）虽然在文本生成等任务中表现出色，但随着大量开源基础模型和微调模型的出现，选择最佳LLM变得复杂，尤其是在许可和硬件要求方面。", "method": "作者创建并发布了一个比较列表，涵盖了基础模型和特定领域模型，重点关注发布年份、许可和硬件要求。该列表发布在GitLab上并持续更新。", "result": "一个包含基础和特定领域LLM的比较列表被呈现并发布，旨在简化LLM的选择过程。", "conclusion": "该比较列表旨在帮助用户在快速发展的LLM领域中进行导航和选择，并将持续更新以保持其时效性。"}}
{"id": "2508.00175", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00175", "abs": "https://arxiv.org/abs/2508.00175", "authors": ["Jose Guadalupe Romero", "Romeo Ortega", "Leyan Fang", "Alexey Bobtsov"], "title": "Adaptive Compensation of Nonlinear Friction in Mechanical Systems Without Velocity Measurement", "comment": null, "summary": "Friction is an unavoidable phenomenon that exists in all mechanical systems\nincorporating parts with relative motion. It is well-known that friction is a\nserious impediment for precise servo control, hence the interest to devise a\nprocedure to compensate for it -- a subject that has been studied by many\nresearchers for many years. The vast majority of friction compensation schemes\nreported in the literature rely on the availability of velocity measurements,\nan information that is hard to obtain. A second limitation of the existing\nprocedures is that they rely on mathematical models of friction that contain\nseveral unknown parameters, some of them entering nonlinearly in the dynamic\nequations. In this paper we propose a globally convergent tracking controller\nfor a mechanical system perturbed by static and Coulomb friction, which is a\nreliable mathematical model of the friction phenomenon, that does not rely one\nmeasurement of velocity. The key component is an immersion and invariance-based\nadaptive speed observer, used for the friction compensation. To the best of our\nknowledge, this is the first globally convergent solution to this challenging\nproblem. We also present simulation results of the application of our observer\nfor systems affected by friction, which is described by the more advanced LuGre\nmodel.", "AI": {"tldr": "本文提出了一种无需速度测量的全局收敛跟踪控制器，用于补偿机械系统中的静摩擦和库仑摩擦，并首次实现了该问题的全局收敛解。", "motivation": "摩擦是机械系统中不可避免的现象，严重阻碍精确伺服控制。现有摩擦补偿方案大多依赖难以获得的速度测量，且其数学模型包含多个未知参数，有些还是非线性参数，这构成了研究的动机。", "method": "本文提出了一种全局收敛的跟踪控制器，其关键组件是一个基于沉浸与不变性（immersion and invariance）的自适应速度观测器，用于摩擦补偿。该方法适用于静摩擦和库仑摩擦模型，并已在更复杂的LuGre摩擦模型下进行了仿真验证。", "result": "该研究首次提出了一个针对摩擦补偿问题的全局收敛解决方案，并且在不需要速度测量的情况下实现了对受静摩擦和库仑摩擦扰动机械系统的有效控制。仿真结果也验证了所提出的观测器对受LuGre模型影响系统的有效性。", "conclusion": "本研究成功开发了一种创新的摩擦补偿方法，解决了现有技术对速度测量依赖的局限性，并实现了全局收敛性，为精确伺服控制提供了更可靠的解决方案。"}}
{"id": "2508.00235", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00235", "abs": "https://arxiv.org/abs/2508.00235", "authors": ["Erin Rainville", "Amirhossein Rasoulian", "Hassan Rivaz", "Yiming Xiao"], "title": "Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior", "comment": "Accepted to ICCV 2025 Workshop CVAMD", "summary": "Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels\nthat, if ruptured, can lead to life-threatening consequences. However, their\nsmall size and soft contrast in radiological scans often make it difficult to\nperform accurate and efficient detection and morphological analyses, which are\ncritical in the clinical care of the disorder. Furthermore, the lack of large\npublic datasets with voxel-wise expert annotations pose challenges for\ndeveloping deep learning algorithms to address the issues. Therefore, we\nproposed a novel weakly supervised 3D multi-task UNet that integrates\nvesselness priors to jointly perform aneurysm detection and segmentation in\ntime-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA\ndetection and segmentation, we employ the popular Frangi's vesselness filter to\nderive soft cerebrovascular priors for both network input and an attention\nblock to conduct segmentation from the decoder and detection from an auxiliary\nbranch. We train our model on the Lausanne dataset with coarse ground truth\nsegmentation, and evaluate it on the test set with refined labels from the same\ndatabase. To further assess our model's generalizability, we also validate it\nexternally on the ADAM dataset. Our results demonstrate the superior\nperformance of the proposed technique over the SOTA techniques for aneurysm\nsegmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =\n1.47, sensitivity = 92.9%).", "AI": {"tldr": "该研究提出了一种新型的弱监督3D多任务UNet模型，利用血管性先验知识，在TOF-MRA图像中联合进行颅内动脉瘤的检测和分割。", "motivation": "颅内动脉瘤（IA）因其体积小、对比度低，在放射学扫描中难以准确高效地检测和形态学分析。此外，缺乏带有像素级专家标注的大型公共数据集，也阻碍了深度学习算法的开发。", "method": "提出了一种弱监督3D多任务UNet模型。该模型整合了Frangi血管性滤波器作为软脑血管先验知识，用于网络输入和注意力块，以指导分割（通过解码器）和检测（通过辅助分支）。模型在Lausanne数据集上使用粗略的真实标注进行训练，并在同一数据集的精细标注测试集上进行评估。为验证泛化能力，还在ADAM数据集上进行了外部验证。", "result": "该技术在动脉瘤分割（Dice = 0.614, 95%HD = 1.38mm）和检测（假阳性率 = 1.47, 敏感性 = 92.9%）方面均优于现有的SOTA技术。", "conclusion": "所提出的弱监督3D多任务UNet模型，通过整合血管性先验知识，有效解决了颅内动脉瘤检测和分割的挑战，并展现出卓越的性能和良好的泛化能力。"}}
{"id": "2508.00258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00258", "abs": "https://arxiv.org/abs/2508.00258", "authors": ["Zhiwei Wu", "Siyi Wei", "Jiahao Luo", "Jinhui Zhang"], "title": "Topology-Inspired Morphological Descriptor for Soft Continuum Robots", "comment": null, "summary": "This paper presents a topology-inspired morphological descriptor for soft\ncontinuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory\nto achieve a quantitative characterization of robot morphologies. By counting\ncritical points of directional projections, the proposed descriptor enables a\ndiscrete representation of multimodal configurations and facilitates\nmorphological classification. Furthermore, we apply the descriptor to\nmorphology control by formulating the target configuration as an optimization\nproblem to compute actuation parameters that generate equilibrium shapes with\ndesired topological features. The proposed framework provides a unified\nmethodology for quantitative morphology description, classification, and\ncontrol of soft continuum robots, with the potential to enhance their precision\nand adaptability in medical applications such as minimally invasive surgery and\nendovascular interventions.", "AI": {"tldr": "该论文提出了一种结合伪刚体模型和莫尔斯理论的拓扑形态描述符，用于软连续体机器人的形态量化表征、分类和控制。", "motivation": "现有方法难以对软连续体机器人的多模态形态进行定量描述和分类，限制了其在需要高精度和适应性的医疗应用中的潜力。", "method": "通过将伪刚体（PRB）模型与莫尔斯理论相结合，提出了一种拓扑形态描述符。该方法通过计算方向投影的临界点，实现多模态构型的离散表示和形态分类。此外，将目标构型表述为优化问题，利用该描述符计算驱动参数，以生成具有所需拓扑特征的平衡形状，从而实现形态控制。", "result": "所提出的描述符能够实现多模态构型的离散表示，促进形态分类。它还提供了一种形态控制方法，通过优化计算驱动参数来生成具有所需拓扑特征的形状。该框架为软连续体机器人的定量形态描述、分类和控制提供了一个统一的方法。", "conclusion": "该框架为软连续体机器人的定量形态描述、分类和控制提供了一种统一的方法，有望提高其在微创手术和血管内介入等医疗应用中的精度和适应性。"}}
{"id": "2508.00088", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00088", "abs": "https://arxiv.org/abs/2508.00088", "authors": ["Mateo de Mayo", "Daniel Cremers", "Taihú Pire"], "title": "The Monado SLAM Dataset for Egocentric Visual-Inertial Tracking", "comment": "Accepted to IROS 2025", "summary": "Humanoid robots and mixed reality headsets benefit from the use of\nhead-mounted sensors for tracking. While advancements in visual-inertial\nodometry (VIO) and simultaneous localization and mapping (SLAM) have produced\nnew and high-quality state-of-the-art tracking systems, we show that these are\nstill unable to gracefully handle many of the challenging settings presented in\nthe head-mounted use cases. Common scenarios like high-intensity motions,\ndynamic occlusions, long tracking sessions, low-textured areas, adverse\nlighting conditions, saturation of sensors, to name a few, continue to be\ncovered poorly by existing datasets in the literature. In this way, systems may\ninadvertently overlook these essential real-world issues. To address this, we\npresent the Monado SLAM dataset, a set of real sequences taken from multiple\nvirtual reality headsets. We release the dataset under a permissive CC BY 4.0\nlicense, to drive advancements in VIO/SLAM research and development.", "AI": {"tldr": "现有VIO/SLAM系统难以应对头戴式设备追踪的挑战性场景，现有数据集覆盖不足。本文提出了Monado SLAM数据集，以推动VIO/SLAM研究进展。", "motivation": "尽管VIO/SLAM技术已取得显著进展，但现有系统仍难以优雅地处理头戴式设备使用中的许多挑战性设置，如高强度运动、动态遮挡、长时间追踪、低纹理区域、恶劣光照和传感器饱和等。现有数据集对这些真实世界问题的覆盖不足，导致系统可能忽略这些关键问题。", "method": "为了解决现有数据集的不足，本文提出了Monado SLAM数据集，该数据集包含从多个虚拟现实头戴设备中获取的真实序列。", "result": "成功创建并发布了Monado SLAM数据集，该数据集包含来自多个VR头显的真实序列，并以CC BY 4.0许可发布。", "conclusion": "通过发布Monado SLAM数据集，旨在弥补现有数据集在挑战性场景覆盖上的不足，从而促进VIO/SLAM研究和开发的进一步发展。"}}
{"id": "2508.00116", "categories": ["cs.AI", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2508.00116", "abs": "https://arxiv.org/abs/2508.00116", "authors": ["Wil M. P. van der Aalst"], "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "AI": {"tldr": "该论文提出，在工业环境中，人工智能（AI）在端到端运营流程中的应用面临挑战。作者认为，AI需要通过以对象为中心的流程挖掘（OCPM）来奠定基础，并引入“流程智能”（PI）概念，作为连接数据和流程的关键，从而使AI能够有效诊断和改进运营流程。", "motivation": "尽管AI被广泛采用，但组织在工业环境中成功应用AI来诊断和改进端到端运营流程时面临困难。AI需要与流程数据和动态特性更好地结合。", "method": "论文提出使用以对象为中心的流程挖掘（OCPM）作为AI（包括生成式、预测式和规范式AI）的基础。作者引入“流程智能”（PI）一词，指代能够处理多种对象和事件类型、以流程为中心的数据驱动技术，从而在组织环境中赋能AI。", "result": "OCPM被认为是连接数据和流程的“缺失环节”，能够支持不同形式的AI。流程智能（PI）被定义为将以流程为中心的数据驱动技术与AI融合的框架，旨在改善运营流程。", "conclusion": "为有效改进运营流程，AI需要流程智能（特别是OCPM）的支持。论文强调了成功结合OCPM与生成式、预测式和规范式AI的巨大机遇。"}}
{"id": "2508.00217", "categories": ["cs.CL", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00217", "abs": "https://arxiv.org/abs/2508.00217", "authors": ["Xiaofeng Wu", "Alan Ritter", "Wei Xu"], "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges", "comment": null, "summary": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.", "AI": {"tldr": "该论文通过对表格输入表示进行分类并介绍表格理解任务，指出了大型语言模型（LLMs）和多模态大型语言模型（MLLMs）在表格理解领域面临的挑战和研究空白。", "motivation": "表格因其复杂和灵活的结构在LLMs和MLLMs中受到广泛关注。然而，表格的二维特性、多样化的格式和目的导致了缺乏通用方法，使得表格理解任务的进展充满挑战。", "method": "本文通过引入表格输入表示的分类法和表格理解任务的介绍来解决这些挑战。同时，论文强调并指出了该领域中存在的几个关键研究空白。", "result": "论文指出了三个主要的研究空白：1) 现有任务主要集中于检索，对数学和逻辑操作之外的推理要求不高；2) 模型在处理复杂结构、大规模表格、长上下文或多表格场景时面临重大挑战；3) 模型在不同表格表示和格式之间的泛化能力有限。", "conclusion": "该领域需要进一步的研究来填补现有空白，特别是要解决表格理解中推理能力不足、复杂表格处理困难以及模型泛化能力受限的问题。"}}
{"id": "2508.00188", "categories": ["eess.SY", "cs.GT", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00188", "abs": "https://arxiv.org/abs/2508.00188", "authors": ["Renyan Sun", "Ashutosh Nayyar"], "title": "Optimal Messaging Strategy for Incentivizing Agents in Dynamic Systems", "comment": "We submitted a full paper to IEEE TAC for review. A preliminary\n  version of this paper is scheduled to be presented at IEEE CDC conference in\n  December 2025", "summary": "We consider a finite-horizon discrete-time dynamic system jointly controlled\nby a designer and one or more agents, where the designer can influence the\nagents' actions through selective information disclosure. At each time step,\nthe designer sends a message to the agent(s) from a prespecified message space.\nThe designer may also take an action that directly influences system dynamics\nand rewards. Each agent uses its received message (and its own information) to\nchoose its action. We are interested in the setting where the designer would\nlike to incentivize each agent to play a specific strategy. We consider a\nnotion of incentive compatibility that is based on sequential rationality at\neach realization of the common information between the designer and the\nagent(s). Our objective is to find a messaging and action strategy for the\ndesigner that maximizes its total expected reward while incentivizing each\nagent to follow a prespecified strategy. Under certain assumptions on the\ninformation structure of the problem, we show that an optimal designer strategy\ncan be computed using a backward inductive algorithm that solves a family of\nlinear programs.", "AI": {"tldr": "本文研究一个有限时域离散时间动态系统，设计师通过选择性信息披露和直接行动来激励代理人遵循特定策略，同时最大化自身收益。提出了一种基于序贯理性的激励相容性概念，并展示了最优设计师策略可通过反向归纳算法（求解一系列线性规划）计算。", "motivation": "在设计师与代理人共同控制的动态系统中，设计师希望通过信息披露和自身行动来激励代理人遵循预设策略，同时最大化自身收益。这需要解决如何在满足激励相容性的前提下设计最优的信息传递和行动策略。", "method": "该研究建模了一个有限时域离散时间动态系统，其中设计师通过选择性信息披露和直接行动来影响系统。代理人根据接收到的信息选择行动。研究引入了基于设计师和代理人之间公共信息序贯理性的激励相容性概念。目标是寻找最大化设计师预期总奖励，同时激励代理人遵循预设策略的信令与行动策略。在特定信息结构假设下，通过求解一系列线性规划的反向归纳算法来计算最优策略。", "result": "主要结果是证明了在问题信息结构的特定假设下，可以利用一个求解一系列线性规划的反向归纳算法来计算设计师的最优信令与行动策略。", "conclusion": "该研究得出结论，在设计师与代理人共同控制的动态系统中，通过精心的信息披露和行动策略设计，可以有效地激励代理人遵循预设策略，同时最大化设计师的收益。提出的基于反向归纳和线性规划的计算方法为解决此类问题提供了一个可行的框架，但其适用性依赖于特定的信息结构假设。"}}
{"id": "2508.00438", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00438", "abs": "https://arxiv.org/abs/2508.00438", "authors": ["Sumin Seo", "In Kyu Lee", "Hyun-Woo Kim", "Jaesik Min", "Chung-Hwan Jung"], "title": "Diffusion-Based User-Guided Data Augmentation for Coronary Stenosis Detection", "comment": "Accepted at MICCAI 2025. Dataset available at\n  https://github.com/medipixel/DiGDA", "summary": "Coronary stenosis is a major risk factor for ischemic heart events leading to\nincreased mortality, and medical treatments for this condition require\nmeticulous, labor-intensive analysis. Coronary angiography provides critical\nvisual cues for assessing stenosis, supporting clinicians in making informed\ndecisions for diagnosis and treatment. Recent advances in deep learning have\nshown great potential for automated localization and severity measurement of\nstenosis. In real-world scenarios, however, the success of these competent\napproaches is often hindered by challenges such as limited labeled data and\nclass imbalance. In this study, we propose a novel data augmentation approach\nthat uses an inpainting method based on a diffusion model to generate realistic\nlesions, allowing user-guided control of severity. Extensive evaluation on\nlesion detection and severity classification across various synthetic dataset\nsizes shows superior performance of our method on both a large-scale in-house\ndataset and a public coronary angiography dataset. Furthermore, our approach\nmaintains high detection and classification performance even when trained with\nlimited data, highlighting its clinical importance in improving the assessment\nof severity of stenosis and optimizing data utilization for more reliable\ndecision support.", "AI": {"tldr": "本研究提出了一种基于扩散模型的图像修复数据增强方法，用于生成可控严重程度的冠状动脉狭窄病变，有效解决了标记数据有限和类别不平衡问题，显著提升了狭窄检测和严重性分类的性能，尤其在数据量有限的情况下表现出色。", "motivation": "冠状动脉狭窄是导致缺血性心脏事件的主要风险因素，其分析耗时且劳动密集。尽管深度学习在自动化狭窄定位和严重性测量方面潜力巨大，但在实际应用中常受限于标记数据不足和类别不平衡问题，这促使研究人员寻求更有效的数据利用方法。", "method": "本研究提出了一种新颖的数据增强方法。该方法利用基于扩散模型的图像修复技术来生成逼真的冠状动脉病变，并允许用户控制病变的严重程度。", "result": "该方法在病变检测和严重性分类方面表现出卓越的性能，在不同合成数据集大小下，无论是大型内部数据集还是公共冠状动脉造影数据集，都展现了优越性。此外，即使在有限数据训练的情况下，该方法仍能保持高检测和分类性能。", "conclusion": "本研究提出的数据增强方法显著提高了冠状动脉狭窄严重性评估的准确性，并优化了数据利用效率，为更可靠的临床决策支持提供了重要价值，突显了其临床重要性。"}}
{"id": "2508.00288", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00288", "abs": "https://arxiv.org/abs/2508.00288", "authors": ["Jianqiang Xiao", "Yuexuan Sun", "Yixin Shao", "Boxi Gan", "Rongqiang Liu", "Yanjing Wu", "Weili Gua", "Xiang Deng"], "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "comment": "Accepted to ACM MM Dataset Track 2025", "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.", "AI": {"tldr": "本文提出了UAV-ON，一个大规模空中目标导航（ObjectNav）基准，旨在推动无人机在开放世界环境中基于高级语义目标而非详细指令的自主导航能力。", "motivation": "现有研究大多遵循视觉与语言导航（VLN）范式，过度依赖顺序语言指令，限制了可扩展性和自主性。空中导航作为具身智能的基本能力，在大型非结构化环境中至关重要，但仍未得到充分探索。", "method": "UAV-ON基准包含14个高保真虚幻引擎环境，具有多样化的语义区域和复杂空间布局（城市、自然、混合使用）。它定义了1270个带注释的目标对象，每个对象都通过实例级指令编码类别、物理足迹和视觉描述符，作为语义目标。为评估基准，实现了包括空中目标导航智能体（AOA）在内的多个基线方法，AOA是一个模块化策略，整合指令语义与自我中心观察进行长程、目标导向的探索。", "result": "实证结果表明，所有基线方法在该设置下都表现不佳，凸显了空中导航和语义目标接地的复合挑战。", "conclusion": "UAV-ON旨在通过语义目标描述，推动无人机在复杂真实世界环境中可扩展自主性的研究进展。"}}
{"id": "2508.00135", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00135", "abs": "https://arxiv.org/abs/2508.00135", "authors": ["Basna Mohammed Salih Hasan", "Ramadhan J. Mstafa"], "title": "Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images", "comment": "12 pages, 18 figures, 5 tables", "summary": "Gender classification has emerged as a crucial aspect in various fields,\nincluding security, human-machine interaction, surveillance, and advertising.\nNonetheless, the accuracy of this classification can be influenced by factors\nsuch as cosmetics and disguise. Consequently, our study is dedicated to\naddressing this concern by concentrating on gender classification using color\nimages of the periocular region. The periocular region refers to the area\nsurrounding the eye, including the eyelids, eyebrows, and the region between\nthem. It contains valuable visual cues that can be used to extract key features\nfor gender classification. This paper introduces a sophisticated Convolutional\nNeural Network (CNN) model that utilizes color image databases to evaluate the\neffectiveness of the periocular region for gender classification. To validate\nthe model's performance, we conducted tests on two eye datasets, namely CVBL\nand (Female and Male). The recommended architecture achieved an outstanding\naccuracy of 99% on the previously unused CVBL dataset while attaining a\ncommendable accuracy of 96% with a small number of learnable parameters\n(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of\nour proposed model for gender classification using the periocular region, we\nevaluated its performance through an extensive range of metrics and compared it\nwith other state-of-the-art approaches. The results unequivocally demonstrate\nthe efficacy of our model, thereby suggesting its potential for practical\napplication in domains such as security and surveillance.", "AI": {"tldr": "本研究提出了一种基于卷积神经网络（CNN）的性别分类模型，利用眼周区域的彩色图像进行分类，并在两个数据集上实现了高精度。", "motivation": "性别分类在多个领域至关重要，但易受化妆和伪装影响。研究旨在通过利用包含有价值视觉线索的眼周区域，提高性别分类的准确性。", "method": "引入了一个复杂的卷积神经网络（CNN）模型，使用眼周区域的彩色图像数据库进行性别分类。模型在CVBL和(Female and Male)两个眼部数据集上进行了性能验证和测试。", "result": "模型在CVBL数据集上达到了99%的准确率，在(Female and Male)数据集上以少量可学习参数（7,235,089个）达到了96%的准确率。与现有最先进的方法相比，该模型表现出卓越的性能。", "conclusion": "研究结果明确证明了该模型利用眼周区域进行性别分类的有效性，表明其在安全和监控等领域具有潜在的实际应用价值。"}}
{"id": "2508.00129", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00129", "abs": "https://arxiv.org/abs/2508.00129", "authors": ["Agustín Borda", "Juan Bautista Cabral", "Gonzalo Giarda", "Diego Nicolás Gimenez Irusta", "Paula Pacheco", "Alvaro Roy Schachner"], "title": "Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis", "comment": null, "summary": "In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem\nthat can greatly affect the results of a Multi-Criteria Decision Method against\na particular set of alternatives. It is therefore useful to have a mechanism\nthat allows one to measure the performance of a method on a set of\nalternatives. This idea could be taken further to build a global ranking of the\neffectiveness of different methods to solve a problem. In this paper, we\npresent three tests that detect the presence of Rank Reversals, along with\ntheir implementation in the Scikit-Criteria library. We also address the\ncomplications that arise when implementing these tests for general scenarios\nand the design considerations we made to handle them. We close with a\ndiscussion about how these additions could play a major role in the judgment of\nmulti-criteria decision methods for problem solving.", "AI": {"tldr": "本文提出了三种用于检测多准则决策分析（MCDA）中“排名反转”现象的新测试方法，并将其集成到Scikit-Criteria库中，旨在提高MCDA方法的性能评估能力。", "motivation": "多准则决策分析中的“排名反转”是一个严重问题，会极大地影响决策方法对备选方案集的评估结果。因此，需要一种机制来衡量方法在给定备选方案集上的表现，并进一步构建不同方法解决问题的有效性全球排名。", "method": "提出了三种检测排名反转的测试方法，并在Scikit-Criteria库中实现了这些测试。同时，讨论了在通用场景下实现这些测试时出现的复杂性以及为处理这些复杂性所做的设计考虑。", "result": "开发并实现了三种新的排名反转检测测试，这些测试已集成到Scikit-Criteria库中，并考虑了通用场景下的实现复杂性。", "conclusion": "这些新增的测试方法将在评估多准则决策方法解决问题的能力方面发挥重要作用。"}}
{"id": "2508.00220", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00220", "abs": "https://arxiv.org/abs/2508.00220", "authors": ["Rana Aref Salama", "Abdou Youssef", "Mona Diab"], "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform", "comment": null, "summary": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.", "AI": {"tldr": "本研究探索了离散小波变换（DWT）在词和句子嵌入上的应用，旨在实现高比例的降维同时保持甚至提升NLP任务性能。", "motivation": "小波变换在信号和图像处理中表现出色，启发了研究者将其应用于自然语言处理（NLP），以期捕捉语言和语义特性。研究旨在展示DWT在分析和压缩嵌入表示方面的能力，同时保持其质量。", "method": "研究经验性地将离散小波变换（DWT）应用于词和句子嵌入。通过语义相似性任务评估DWT嵌入的有效性，并使用包括大型语言模型在内的不同嵌入模型，在下游任务中展示所提出范式的功效。", "result": "DWT能够将嵌入维度降低50%至93%，在语义相似性任务上性能几乎没有变化，同时在大多数下游任务中取得了更高的准确性。", "conclusion": "研究结果为将DWT应用于改进NLP应用开辟了道路，表明其在嵌入降维和性能提升方面具有巨大潜力。"}}
{"id": "2508.00283", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00283", "abs": "https://arxiv.org/abs/2508.00283", "authors": ["Lihan Lian", "Uduak Inyang-Udoh"], "title": "Neural Co-state Projection Regulator: A Model-free Paradigm for Real-time Optimal Control with Input Constraints", "comment": null, "summary": "Learning-based approaches, notably Reinforcement Learning (RL), have shown\npromise for solving optimal control tasks without explicit system models.\nHowever, these approaches are often sample-inefficient, sensitive to reward\ndesign and hyperparameters, and prone to poor generalization, especially under\ninput constraints. To address these challenges, we introduce the neural\nco-state projection regulator (NCPR), a model-free learning-based optimal\ncontrol framework that is grounded in Pontryagin's Minimum Principle (PMP) and\ncapable of solving quadratic regulator problems in nonlinear control-affine\nsystems with input constraints. In this framework, a neural network (NN) is\ntrained in a self-supervised setting to take the current state of the system as\ninput and predict a finite-horizon trajectory of projected co-states (i.e., the\nco-state weighted by the system's input gain). Subsequently, only the first\nelement of the NN's prediction is extracted to solve a lightweight quadratic\nprogram (QP). This workflow is executed in a feedback control setting, allowing\nreal-time computation of control actions that satisfy both input constraints\nand first-order optimality conditions.\n  We test the proposed learning-based model-free quadratic regulator on (1) a\nunicycle model robot reference tracking problem and (2) a pendulum swing-up\ntask. For comparison, reinforcement learning is used on both tasks; and for\ncontext, a model-based controller is used in the unicycle model example. Our\nmethod demonstrates superior generalizability in terms of both unseen system\nstates and varying input constraints, and also shows improved sampling\nefficiency.", "AI": {"tldr": "本文提出了一种名为神经余态投影调节器（NCPR）的模型无关学习型最优控制框架，它基于庞特里亚金最小原理（PMP），用于解决带输入约束的非线性控制仿射系统的二次调节器问题，并展现出优越的泛化能力和采样效率。", "motivation": "现有的学习方法，特别是强化学习（RL），在解决最优控制任务时存在样本效率低、对奖励设计和超参数敏感、以及泛化能力差（尤其是在存在输入约束的情况下）等问题。", "method": "NCPR框架通过自监督学习训练一个神经网络（NN），使其以当前系统状态为输入，预测有限时间范围内的投影余态轨迹（即余态乘以系统输入增益）。随后，仅提取NN预测的第一个元素来求解一个轻量级的二次规划（QP）问题。此工作流程在反馈控制设置中执行，实现满足输入约束和一阶最优条件的实时控制动作计算。", "result": "该方法在独轮车模型机器人参考跟踪问题和摆锤向上摆动任务上进行了测试。与强化学习和基于模型的控制器相比，NCPR在未见过的系统状态和变化的输入约束方面表现出卓越的泛化能力，并显著提高了采样效率。", "conclusion": "NCPR是一种有效的模型无关学习型最优控制框架，它结合了庞特里亚金最小原理，能够有效解决带输入约束的非线性系统控制问题，克服了传统强化学习的局限性，实现了更好的泛化性能和更高的采样效率。"}}
{"id": "2508.00721", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.00721", "abs": "https://arxiv.org/abs/2508.00721", "authors": ["Yuxiang Wan", "Ryan Devera", "Wenjie Zhang", "Ju Sun"], "title": "FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems", "comment": null, "summary": "We present FMPlug, a novel plug-in framework that enhances foundation\nflow-matching (FM) priors for solving ill-posed inverse problems. Unlike\ntraditional approaches that rely on domain-specific or untrained priors, FMPlug\nsmartly leverages two simple but powerful insights: the similarity between\nobserved and desired objects and the Gaussianity of generative flows. By\nintroducing a time-adaptive warm-up strategy and sharp Gaussianity\nregularization, FMPlug unlocks the true potential of domain-agnostic foundation\nmodels. Our method beats state-of-the-art methods that use foundation FM priors\nby significant margins, on image super-resolution and Gaussian deblurring.", "AI": {"tldr": "FMPlug是一个新颖的插件框架，通过利用观测对象与期望对象的相似性及生成流的高斯性，并引入时间自适应预热策略和锐利高斯性正则化，显著提升了基础流匹配（FM）先验在解决不适定逆问题上的性能，超越了现有SOTA方法。", "motivation": "传统的逆问题解决方法依赖于领域特定或未经训练的先验，这限制了其泛化能力。本研究旨在通过更有效的方式利用基础流匹配（FM）先验来克服这些限制，并充分发挥领域无关基础模型的潜力。", "method": "本文提出了FMPlug框架，该框架利用了观测对象与期望对象的相似性以及生成流的高斯性两大洞察。具体方法包括引入时间自适应预热策略和锐利的高斯性正则化，以增强基础流匹配先验在不适定逆问题中的应用。", "result": "FMPlug在图像超分辨率和高斯去模糊任务上，以显著优势超越了使用基础FM先验的现有最先进方法。", "conclusion": "FMPlug成功解锁了领域无关基础模型在解决不适定逆问题方面的真正潜力，通过其创新的设计和策略，为逆问题求解提供了更强大和通用的解决方案。"}}
{"id": "2508.00303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00303", "abs": "https://arxiv.org/abs/2508.00303", "authors": ["Zehui Xu", "Junhui Wang", "Yongliang Shi", "Chao Gao", "Guyue Zhou"], "title": "TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps", "comment": null, "summary": "This paper introduces TopoDiffuser, a diffusion-based framework for\nmultimodal trajectory prediction that incorporates topometric maps to generate\naccurate, diverse, and road-compliant future motion forecasts. By embedding\nstructural cues from topometric maps into the denoising process of a\nconditional diffusion model, the proposed approach enables trajectory\ngeneration that naturally adheres to road geometry without relying on explicit\nconstraints. A multimodal conditioning encoder fuses LiDAR observations,\nhistorical motion, and route information into a unified bird's-eye-view (BEV)\nrepresentation. Extensive experiments on the KITTI benchmark demonstrate that\nTopoDiffuser outperforms state-of-the-art methods, while maintaining strong\ngeometric consistency. Ablation studies further validate the contribution of\neach input modality, as well as the impact of denoising steps and the number of\ntrajectory samples. To support future research, we publicly release our code at\nhttps://github.com/EI-Nav/TopoDiffuser.", "AI": {"tldr": "TopoDiffuser是一种基于扩散模型的多模态轨迹预测框架，它通过融合拓扑度量地图信息，生成准确、多样且符合道路几何的未来运动预测。", "motivation": "现有轨迹预测方法在生成符合道路几何的轨迹时，常依赖显式约束，限制了其自然性和准确性。本研究旨在开发一种能自然地遵循道路几何，并生成准确、多样化预测的方法。", "method": "TopoDiffuser是一个条件扩散模型，将拓扑度量地图的结构线索嵌入到去噪过程中，从而使轨迹生成自然地遵守道路几何。一个多模态条件编码器将激光雷达观测、历史运动和路线信息融合为统一的鸟瞰图（BEV）表示。", "result": "在KITTI基准测试中，TopoDiffuser超越了现有最先进的方法，并保持了强大的几何一致性。消融研究进一步验证了每种输入模态、去噪步数和轨迹样本数量的贡献。", "conclusion": "TopoDiffuser通过将拓扑度量地图信息融入扩散模型，成功实现了准确、多样且符合道路几何的多模态轨迹预测，并在性能上优于现有方法。"}}
{"id": "2508.00144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00144", "abs": "https://arxiv.org/abs/2508.00144", "authors": ["Akshat Rakheja", "Aarsh Ashdhir", "Aryan Bhattacharjee", "Vanshika Sharma"], "title": "World Consistency Score: A Unified Metric for Video Generation Quality", "comment": "27 pages, 1 figure", "summary": "We introduce World Consistency Score (WCS), a novel unified evaluation metric\nfor generative video models that emphasizes internal world consistency of the\ngenerated videos. WCS integrates four interpretable sub-components - object\npermanence, relation stability, causal compliance, and flicker penalty - each\nmeasuring a distinct aspect of temporal and physical coherence in a video.\nThese submetrics are combined via a learned weighted formula to produce a\nsingle consistency score that aligns with human judgments. We detail the\nmotivation for WCS in the context of existing video evaluation metrics,\nformalize each submetric and how it is computed with open-source tools\n(trackers, action recognizers, CLIP embeddings, optical flow), and describe how\nthe weights of the WCS combination are trained using human preference data. We\nalso outline an experimental validation blueprint: using benchmarks like\nVBench-2.0, EvalCrafter, and LOVE to test WCS's correlation with human\nevaluations, performing sensitivity analyses, and comparing WCS against\nestablished metrics (FVD, CLIPScore, VBench, FVMD). The proposed WCS offers a\ncomprehensive and interpretable framework for evaluating video generation\nmodels on their ability to maintain a coherent \"world\" over time, addressing\ngaps left by prior metrics focused only on visual fidelity or prompt alignment.", "AI": {"tldr": "本文提出World Consistency Score (WCS)，一种新的视频生成模型评估指标，专注于衡量生成视频的内部世界一致性。", "motivation": "现有视频评估指标主要关注视觉保真度或提示对齐，缺乏对视频内部世界一致性（时间与物理连贯性）的评估，无法衡量生成视频在时间上的连贯性。", "method": "WCS由四个可解释的子组件构成：物体永存性、关系稳定性、因果符合性、闪烁惩罚，每个子组件衡量视频时间与物理连贯性的不同方面。这些子指标通过学习到的加权公式组合，权重通过人类偏好数据训练。计算子指标时使用开源工具（跟踪器、动作识别器、CLIP嵌入、光流）。通过在VBench-2.0、EvalCrafter、LOVE等基准上测试WCS与人类评估的相关性、进行敏感性分析以及与现有指标（FVD、CLIPScore、VBench、FVMD）比较来验证其有效性。", "result": "本文提出WCS作为一种全面且可解释的框架，用于评估视频生成模型维持连贯“世界”的能力。WCS通过整合多个子指标并与人类判断对齐，旨在弥补现有指标在评估视频时间连贯性方面的不足。", "conclusion": "WCS提供了一个全面且可解释的框架，用于评估视频生成模型在时间上维持连贯“世界”的能力，解决了以往仅关注视觉保真度或提示对齐的指标所留下的空白。"}}
{"id": "2508.00137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00137", "abs": "https://arxiv.org/abs/2508.00137", "authors": ["Shqiponja Ahmetaj", "George Konstantinidis", "Magdalena Ortiz", "Paolo Pareti", "Mantas Simkus"], "title": "SHACL Validation under Graph Updates (Extended Paper)", "comment": "Accepted at the International Semantic Web Conference (ISWC 2025)", "summary": "SHACL (SHApe Constraint Language) is a W3C standardized constraint language\nfor RDF graphs. In this paper, we study SHACL validation in RDF graphs under\nupdates. We present a SHACL-based update language that can capture intuitive\nand realistic modifications on RDF graphs and study the problem of static\nvalidation under such updates. This problem asks to verify whether every graph\nthat validates a SHACL specification will still do so after applying a given\nupdate sequence. More importantly, it provides a basis for further services for\nreasoning about evolving RDF graphs. Using a regression technique that embeds\nthe update actions into SHACL constraints, we show that static validation under\nupdates can be reduced to (un)satisfiability of constraints in (a minor\nextension of) SHACL. We analyze the computational complexity of the static\nvalidation problem for SHACL and some key fragments. Finally, we present a\nprototype implementation that performs static validation and other static\nanalysis tasks on SHACL constraints and demonstrate its behavior through\npreliminary experiments.", "AI": {"tldr": "本文研究了在RDF图更新下SHACL验证问题，提出了一种基于SHACL的更新语言，并通过回归技术将静态验证问题规约到SHACL约束的（不）可满足性，并分析了其计算复杂性。", "motivation": "SHACL是RDF图的W3C标准约束语言，但现有研究未充分考虑RDF图在更新下的SHACL验证问题。需要验证在给定更新序列后，所有符合SHACL规范的图是否仍然保持有效，这为推理不断演化的RDF图提供了基础。", "method": "1. 提出了一种基于SHACL的更新语言，用于捕获RDF图的修改。2. 研究了静态验证问题：验证每个符合SHACL规范的图在应用给定更新序列后是否仍然有效。3. 使用回归技术将更新操作嵌入到SHACL约束中，将更新下的静态验证问题规约到SHACL（或其扩展）中约束的（不）可满足性。4. 分析了SHACL及其关键片段的静态验证问题的计算复杂性。5. 开发了一个原型实现来执行静态验证和其他静态分析任务。", "result": "1. 证明了在更新下的静态验证问题可以规约到SHACL（或其轻微扩展）中约束的（不）可满足性。2. 分析了SHACL及其关键片段的静态验证问题的计算复杂性。3. 提供了一个原型实现，并通过初步实验展示了其行为。", "conclusion": "通过将更新操作嵌入SHACL约束并规约到约束的（不）可满足性，实现了RDF图更新下的SHACL静态验证。该方法具有理论可行性，并已通过原型实现得到初步验证。"}}
{"id": "2508.00238", "categories": ["cs.CL", "cs.AI", "68T50", "I.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00238", "abs": "https://arxiv.org/abs/2508.00238", "authors": ["Bryce Anderson", "Riley Galpin", "Tom S. Juzek"], "title": "Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English", "comment": "Accepted at AIES 2025. To appear in the AIES Proceedings. 14 pages, 2\n  figures, 2 tables. Licensed under CC BY-SA 4.0", "summary": "In recent years, written language, particularly in science and education, has\nundergone remarkable shifts in word usage. These changes are widely attributed\nto the growing influence of Large Language Models (LLMs), which frequently rely\non a distinct lexical style. Divergences between model output and target\naudience norms can be viewed as a form of misalignment. While these shifts are\noften linked to using Artificial Intelligence (AI) directly as a tool to\ngenerate text, it remains unclear whether the changes reflect broader changes\nin the human language system itself. To explore this question, we constructed a\ndataset of 22.1 million words from unscripted spoken language drawn from\nconversational science and technology podcasts. We analyzed lexical trends\nbefore and after ChatGPT's release in 2022, focusing on commonly LLM-associated\nwords. Our results show a moderate yet significant increase in the usage of\nthese words post-2022, suggesting a convergence between human word choices and\nLLM-associated patterns. In contrast, baseline synonym words exhibit no\nsignificant directional shift. Given the short time frame and the number of\nwords affected, this may indicate the onset of a remarkable shift in language\nuse. Whether this represents natural language change or a novel shift driven by\nAI exposure remains an open question. Similarly, although the shifts may stem\nfrom broader adoption patterns, it may also be that upstream training\nmisalignments ultimately contribute to changes in human language use. These\nfindings parallel ethical concerns that misaligned models may shape social and\nmoral beliefs.", "AI": {"tldr": "大型语言模型（LLMs）正在改变书面语言。本研究发现，ChatGPT发布后，非脚本口语中与LLM相关的词汇使用量适度但显著增加，表明人类词汇选择正与LLM模式趋同。", "motivation": "近年来，书面语言（尤其在科学和教育领域）的词汇使用发生了显著变化，这归因于大型语言模型（LLMs）的影响。虽然这些变化常与直接使用AI生成文本相关，但尚不清楚它们是否反映了人类语言系统本身的更广泛变化。本研究旨在探讨此问题。", "method": "构建了一个包含2210万词的非脚本口语数据集，来源于科技播客。分析了2022年ChatGPT发布前后，与LLM常用词汇相关的词汇趋势，并与基线同义词进行了对比。", "result": "结果显示，2022年后，与LLM相关词汇的使用量适度但显著增加，表明人类词汇选择与LLM相关模式之间存在趋同。相比之下，基线同义词没有表现出显著的方向性变化。", "conclusion": "鉴于时间短和受影响词汇量，这可能预示着语言使用正在发生显著转变。这究竟是自然语言演变还是AI暴露驱动的新型转变，仍是一个开放性问题。同时，这可能与模型未对齐导致社会和道德观念被塑造的伦理担忧相似。"}}
{"id": "2508.00609", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00609", "abs": "https://arxiv.org/abs/2508.00609", "authors": ["M. F. Shakib", "M. Khalil", "R. Postoyan"], "title": "Low-dimensional observer design for stable linear systems by model reduction", "comment": null, "summary": "This paper presents a low-dimensional observer design for stable,\nsingle-input single-output, continuous-time linear time-invariant (LTI)\nsystems. Leveraging the model reduction by moment matching technique, we\napproximate the system with a reduced-order model. Based on this reduced-order\nmodel, we design a low-dimensional observer that estimates the states of the\noriginal system. We show that this observer establishes exact asymptotic state\nreconstruction for a given class of inputs tied to the observer's dimension.\nFurthermore, we establish an exponential input-to-state stability property for\ngeneric inputs, ensuring a bounded estimation error. Numerical simulations\nconfirm the effectiveness of the approach for a benchmark model reduction\nproblem.", "AI": {"tldr": "本文提出了一种基于矩匹配模型降阶的低维观测器设计方法，用于稳定、单输入单输出的连续时间线性时不变系统，可实现特定输入的精确渐近状态重构和通用输入的有界估计误差。", "motivation": "为稳定、单输入单输出的连续时间线性时不变（LTI）系统设计一个低维观测器，以有效估计系统状态。", "method": "首先，利用矩匹配技术对原始系统进行模型降阶，得到一个降阶模型。然后，基于该降阶模型设计一个低维观测器来估计原始系统的状态。", "result": "该观测器对于与观测器维度相关的特定输入类别，能够实现精确的渐近状态重构。对于通用输入，该观测器具有指数输入到状态稳定性，确保了有界的估计误差。数值仿真证实了该方法的有效性。", "conclusion": "所提出的低维观测器设计方法，通过结合模型降阶技术，能有效估计LTI系统状态，并在不同输入条件下提供鲁棒的性能（精确重构或有界误差）。"}}
{"id": "2508.00755", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00755", "abs": "https://arxiv.org/abs/2508.00755", "authors": ["Peng Hu", "Wenxuan Zhang"], "title": "AI-Driven Collaborative Satellite Object Detection for Space Sustainability", "comment": "Submitted to the 13th Annual IEEE International Conference on\n  Wireless for Space and Extreme Environments (WiSEE 2025)", "summary": "The growing density of satellites in low-Earth orbit (LEO) presents serious\nchallenges to space sustainability, primarily due to the increased risk of\nin-orbit collisions. Traditional ground-based tracking systems are constrained\nby latency and coverage limitations, underscoring the need for onboard,\nvision-based space object detection (SOD) capabilities. In this paper, we\npropose a novel satellite clustering framework that enables the collaborative\nexecution of deep learning (DL)-based SOD tasks across multiple satellites. To\nsupport this approach, we construct a high-fidelity dataset simulating imaging\nscenarios for clustered satellite formations. A distance-aware viewpoint\nselection strategy is introduced to optimize detection performance, and recent\nDL models are used for evaluation. Experimental results show that the\nclustering-based method achieves competitive detection accuracy compared to\nsingle-satellite and existing approaches, while maintaining a low size, weight,\nand power (SWaP) footprint. These findings underscore the potential of\ndistributed, AI-enabled in-orbit systems to enhance space situational awareness\nand contribute to long-term space sustainability.", "AI": {"tldr": "本文提出了一种卫星聚类框架，通过多卫星协作执行基于深度学习的空间目标检测任务，以提高近地轨道空间态势感知能力。", "motivation": "近地轨道卫星密度增加导致碰撞风险上升，传统地面跟踪系统存在延迟和覆盖限制，因此急需星载、基于视觉的空间目标检测能力。", "method": "提出了一种新颖的卫星聚类框架，支持多卫星协作执行深度学习的空间目标检测任务；构建了高保真数据集模拟聚类卫星编队成像场景；引入了距离感知的视角选择策略以优化检测性能；并使用最新的深度学习模型进行评估。", "result": "实验结果表明，与单卫星和现有方法相比，基于聚类的方法在保持低尺寸、重量和功耗（SWaP）的同时，实现了具有竞争力的检测精度。", "conclusion": "研究结果强调了分布式、人工智能赋能的在轨系统在增强空间态势感知和促进长期空间可持续性方面的潜力。"}}
{"id": "2508.00354", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00354", "abs": "https://arxiv.org/abs/2508.00354", "authors": ["Tianshuang Qiu", "Zehan Ma", "Karim El-Refai", "Hiya Shah", "Chung Min Kim", "Justin Kerr", "Ken Goldberg"], "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging", "comment": null, "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/", "AI": {"tldr": "该论文提出了Omni-Scan，一个利用双臂机器人抓取并旋转物体以生成高质量3D高斯泼溅（3DGS）模型的流水线，通过重新抓取来暴露被遮挡的表面，实现物体的全方位3D模型，并将其应用于零件缺陷检测。", "motivation": "传统的3D物体扫描方法（如多相机阵列、精密激光扫描仪或机器人腕部安装相机）通常需要受限的工作空间。研究旨在开发一种新的方法，能够克服这些限制，生成高质量、全方位（360度）的3D高斯泼溅模型。", "method": "Omni-Scan流水线使用双臂机器人：一个机械臂抓取并旋转物体，另一个机械臂重新抓取以暴露被第一个机械臂遮挡的表面。该流水线利用DepthAnything、Segment Anything和RAFT光流模型来识别和隔离被机械臂抓取的物体，同时去除机械臂和背景。此外，它修改了3DGS训练流程，以支持包含机械臂遮挡的拼接数据集，从而生成物体的全方位模型。", "result": "Omni-Scan能够生成物体的全方位（360度视图）3D高斯泼溅模型。将其应用于零件缺陷检测时，在12种不同的工业和家用物体上，识别视觉或几何缺陷的平均准确率达到83%。", "conclusion": "Omni-Scan提供了一种有效且高质量的3D高斯泼溅模型生成方法，能够克服传统扫描方法的局限性，实现物体的全方位建模。该技术在零件缺陷检测等应用中表现出良好的性能，具有实际应用价值。"}}
{"id": "2508.00152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00152", "abs": "https://arxiv.org/abs/2508.00152", "authors": ["Li Mi", "Manon Bechaz", "Zeming Chen", "Antoine Bosselut", "Devis Tuia"], "title": "GeoExplorer: Active Geo-localization with Curiosity-Driven Exploration", "comment": "ICCV 2025. Project page at https://limirs.github.io/GeoExplorer/", "summary": "Active Geo-localization (AGL) is the task of localizing a goal, represented\nin various modalities (e.g., aerial images, ground-level images, or text),\nwithin a predefined search area. Current methods approach AGL as a\ngoal-reaching reinforcement learning (RL) problem with a distance-based reward.\nThey localize the goal by implicitly learning to minimize the relative distance\nfrom it. However, when distance estimation becomes challenging or when\nencountering unseen targets and environments, the agent exhibits reduced\nrobustness and generalization ability due to the less reliable exploration\nstrategy learned during training. In this paper, we propose GeoExplorer, an AGL\nagent that incorporates curiosity-driven exploration through intrinsic rewards.\nUnlike distance-based rewards, our curiosity-driven reward is goal-agnostic,\nenabling robust, diverse, and contextually relevant exploration based on\neffective environment modeling. These capabilities have been proven through\nextensive experiments across four AGL benchmarks, demonstrating the\neffectiveness and generalization ability of GeoExplorer in diverse settings,\nparticularly in localizing unfamiliar targets and environments.", "AI": {"tldr": "GeoExplorer提出了一种基于好奇心驱动探索的主动地理定位（AGL）代理，通过内在奖励克服了现有距离奖励方法的泛化和鲁棒性不足问题，尤其适用于陌生目标和环境。", "motivation": "当前主动地理定位（AGL）方法将任务视为基于距离奖励的强化学习问题，但在距离估计困难或遇到未知目标和环境时，其鲁棒性和泛化能力会降低，因为学习到的探索策略不可靠。", "method": "本文提出了GeoExplorer，一个结合好奇心驱动探索（通过内在奖励实现）的AGL代理。与基于距离的奖励不同，GeoExplorer的好奇心驱动奖励是与目标无关的，通过有效的环境建模实现鲁棒、多样化和上下文相关的探索。", "result": "GeoExplorer在四个AGL基准测试中进行了广泛实验，证明了其在不同设置下的有效性和泛化能力，特别是在定位不熟悉的目标和环境方面表现出色。", "conclusion": "GeoExplorer通过引入好奇心驱动的内在奖励，显著提升了主动地理定位任务的鲁棒性和泛化能力，尤其解决了现有方法在未知或复杂环境中的局限性，实现了更有效的探索。"}}
{"id": "2508.00138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00138", "abs": "https://arxiv.org/abs/2508.00138", "authors": ["Rashid Mushkani", "Hugo Berard", "Toumadher Ammar", "Cassandre Chatonnier", "Shin Koseki"], "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle", "comment": "Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "summary": "Despite efforts to mitigate the inherent risks and biases of artificial\nintelligence (AI) algorithms, these algorithms can disproportionately impact\nculturally marginalized groups. A range of approaches has been proposed to\naddress or reduce these risks, including the development of ethical guidelines\nand principles for responsible AI, as well as technical solutions that promote\nalgorithmic fairness. Drawing on design justice, expansive learning theory, and\nrecent empirical work on participatory AI, we argue that mitigating these harms\nrequires a fundamental re-architecture of the AI production pipeline. This\nre-design should center co-production, diversity, equity, inclusion (DEI), and\nmultidisciplinary collaboration. We introduce an augmented AI lifecycle\nconsisting of five interconnected phases: co-framing, co-design,\nco-implementation, co-deployment, and co-maintenance. The lifecycle is informed\nby four multidisciplinary workshops and grounded in themes of distributed\nauthority and iterative knowledge exchange. Finally, we relate the proposed\nlifecycle to several leading ethical frameworks and outline key research\nquestions that remain for scaling participatory governance.", "AI": {"tldr": "为解决AI算法对边缘化群体的负面影响，本文提出了一种以共同生产、多元化、公平性、包容性和多学科协作为核心的AI生产流程重构，并引入了一个包含五个阶段的增强型AI生命周期。", "motivation": "尽管已做出努力，AI算法仍可能对文化边缘化群体产生不成比例的影响，现有的风险缓解方法（如伦理指南和算法公平技术方案）不足以解决这些深层问题。", "method": "研究借鉴了设计正义、扩展学习理论和参与式AI的最新实证工作。提出了一种重构AI生产流程的方法，引入了一个由共框定、共设计、共实施、共部署和共维护五个相互关联阶段组成的增强型AI生命周期。该生命周期基于四场多学科研讨会，并以分布式权限和迭代知识交换为基础。", "result": "本文提出了一个以共同生产、多元化、公平性、包容性（DEI）和多学科协作为中心的新型AI生产管道，并详细阐述了一个包含五个相互关联阶段的增强型AI生命周期，旨在缓解AI对边缘化群体的负面影响。", "conclusion": "所提出的AI生命周期与现有伦理框架相契合，并为未来扩展参与式治理提出了关键研究问题，强调了AI开发中持续的合作和知识共享的重要性。"}}
{"id": "2508.00285", "categories": ["cs.CL", "I.2.7; J.3"], "pdf": "https://arxiv.org/pdf/2508.00285", "abs": "https://arxiv.org/abs/2508.00285", "authors": ["Peixian Li", "Yu Tian", "Ruiqi Tu", "Chengkai Wu", "Jingjing Ren", "Jingsong Li"], "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering", "comment": "23 pages, 8 figures", "summary": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.", "AI": {"tldr": "本研究提出了一种“病因感知注意力引导框架”，通过整合结构化临床推理来提高大型语言模型在复杂临床场景下的诊断准确性和推理能力，并在多个数据集上验证了其有效性。", "motivation": "尽管大型语言模型在医学文本理解和生成方面表现出显著能力，但它们在复杂临床场景中的诊断可靠性仍然有限。本研究旨在增强大型语言模型的诊断准确性和临床推理能力。", "method": "提出“病因感知注意力引导框架”。具体包括：1) 基于权威临床指南构建针对急性阑尾炎、急性胰腺炎和急性胆囊炎的“临床推理支架（CRS）”；2) 开发“病因感知头识别算法”以确定模型病因推理的关键注意力头；3) 引入“推理引导参数高效微调”，将病因推理线索嵌入输入表示，并通过“推理引导损失函数”将选定的病因感知头导向关键信息。", "result": "在“一致诊断队列”上，该框架将平均诊断准确率提高了15.65%，平均推理焦点分数提高了31.6%。在“差异诊断队列”上的外部验证进一步证实了其在提高诊断准确性方面的有效性。通过“推理注意力频率”的进一步评估表明，该模型在面对真实世界复杂场景时表现出更高的可靠性。", "conclusion": "本研究提出了一种实用且有效的方法来增强基于大型语言模型的诊断中的临床推理能力。通过将模型注意力与结构化临床推理支架对齐，所提出的框架为在复杂临床环境中构建更可解释和可靠的AI诊断系统提供了一个有前景的范式。"}}
{"id": "2508.00637", "categories": ["eess.SY", "cs.CR", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00637", "abs": "https://arxiv.org/abs/2508.00637", "authors": ["Michał Forystek", "Andrew D. Syrmakesis", "Alkistis Kontou", "Panos Kotsampopoulos", "Nikos D. Hatziargyriou", "Charalambos Konstantinou"], "title": "Cyber-Physical Co-Simulation of Load Frequency Control under Load-Altering Attacks", "comment": "2025 IEEE International Conference on Communications, Control, and\n  Computing Technologies for Smart Grids (SmartGridComm)", "summary": "Integrating Information and Communications Technology (ICT) devices into the\npower grid brings many benefits. However, it also exposes the grid to new\npotential cyber threats. Many control and protection mechanisms, such as Load\nFrequency Control (LFC), responsible for maintaining nominal frequency during\nload fluctuations and Under Frequency Load Shedding (UFLS) disconnecting\nportion of the load during an emergency, are dependent on information exchange\nthrough the communication network. The recently emerging Load Altering Attacks\n(LAAs) utilize a botnet of high-wattage devices to introduce load fluctuation.\nIn their dynamic form (DLAAs), they manipulate the load in response to live\ngrid frequency measurements for increased efficiency, posing a notable threat\nto grid stability. Recognizing the importance of communication networks in\npower grid cyber security research, this paper presents an open-source\nco-simulation environment that models the power grid with the corresponding\ncommunication network, implementing grid protective mechanisms. This setup\nallows the comprehensive analysis of the attacks in concrete LFC and UFLS\nscenarios.", "AI": {"tldr": "本文提出了一个开源协同仿真环境，用于建模电力系统及其通信网络，以分析负载改变攻击（LAAs）和动态负载改变攻击（DLAAs）对负载频率控制（LFC）和低频减载（UFLS）等电网保护机制的影响。", "motivation": "信息通信技术（ICT）融入电网带来益处的同时也引入了网络威胁，特别是负载改变攻击（LAAs）及其动态形式（DLAAs）能够通过操纵负载对电网稳定性构成显著威胁。LFC和UFLS等关键控制保护机制依赖通信网络，因此需要一个环境来全面分析这些攻击。", "method": "开发一个开源的协同仿真环境，该环境同时建模电力系统和相应的通信网络，并实现了电网保护机制（如LFC和UFLS）。", "result": "该论文提出了一个能够对具体LFC和UFLS场景中的网络攻击进行全面分析的协同仿真环境。", "conclusion": "所提出的协同仿真环境有助于深入理解和分析网络攻击（特别是LAAs和DLAAs）对电力系统稳定性和保护机制（LFC和UFLS）的影响，强调了通信网络在电网网络安全研究中的重要性。"}}
{"id": "2508.00418", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00418", "abs": "https://arxiv.org/abs/2508.00418", "authors": ["Sangwoo Youn", "Minji Lee", "Nokap Tony Park", "Yeonggyoo Jeon", "Taeyoung Na"], "title": "IN2OUT: Fine-Tuning Video Inpainting Model for Video Outpainting Using Hierarchical Discriminator", "comment": "ICIP 2025. Code: https://github.com/sang-w00/IN2OUT", "summary": "Video outpainting presents a unique challenge of extending the borders while\nmaintaining consistency with the given content. In this paper, we suggest the\nuse of video inpainting models that excel in object flow learning and\nreconstruction in outpainting rather than solely generating the background as\nin existing methods. However, directly applying or fine-tuning inpainting\nmodels to outpainting has shown to be ineffective, often leading to blurry\nresults. Our extensive experiments on discriminator designs reveal that a\ncritical component missing in the outpainting fine-tuning process is a\ndiscriminator capable of effectively assessing the perceptual quality of the\nextended areas. To tackle this limitation, we differentiate the objectives of\nadversarial training into global and local goals and introduce a hierarchical\ndiscriminator that meets both objectives. Additionally, we develop a\nspecialized outpainting loss function that leverages both local and global\nfeatures of the discriminator. Fine-tuning on this adversarial loss function\nenhances the generator's ability to produce both visually appealing and\nglobally coherent outpainted scenes. Our proposed method outperforms\nstate-of-the-art methods both quantitatively and qualitatively. Supplementary\nmaterials including the demo video and the code are available in SigPort.", "AI": {"tldr": "该论文提出一种通过改进视频修复模型来实现视频外绘的方法，引入分层判别器和专门的外绘损失函数，有效解决了现有方法中边界扩展模糊和不一致的问题。", "motivation": "现有视频外绘方法在扩展边界时难以保持内容一致性，直接应用或微调视频修复模型常导致模糊结果，且缺乏能有效评估扩展区域感知质量的判别器。", "method": "作者建议使用擅长物体流学习和重建的视频修复模型进行外绘。为解决模糊问题，引入了区分全局和局部目标的“分层判别器”，并开发了利用判别器局部和全局特征的“专用外绘损失函数”。通过此对抗性损失函数微调生成器。", "result": "所提出的方法在定量和定性方面均优于现有最先进的方法，能生成视觉上吸引人且全局连贯的外绘场景。", "conclusion": "通过引入分层判别器和专用外绘损失函数，可以有效提升视频外绘的质量，使生成内容既视觉美观又全局一致。"}}
{"id": "2508.00355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00355", "abs": "https://arxiv.org/abs/2508.00355", "authors": ["Zhenghan Chen", "Haocheng Xu", "Haodong Zhang", "Liang Zhang", "He Li", "Dongqi Wang", "Jiyu Yu", "Yifei Yang", "Zhongxiang Zhou", "Rong Xiong"], "title": "TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots", "comment": null, "summary": "Humanoid robots have the potential capability to perform a diverse range of\nmanipulation tasks, but this is based on a robust and precise standing\ncontroller. Existing methods are either ill-suited to precisely control\nhigh-dimensional upper-body joints, or difficult to ensure both robustness and\naccuracy, especially when upper-body motions are fast. This paper proposes a\nnovel time optimization policy (TOP), to train a standing manipulation control\nmodel that ensures balance, precision, and time efficiency simultaneously, with\nthe idea of adjusting the time trajectory of upper-body motions but not only\nstrengthening the disturbance resistance of the lower-body. Our approach\nconsists of three parts. Firstly, we utilize motion prior to represent\nupper-body motions to enhance the coordination ability between the upper and\nlower-body by training a variational autoencoder (VAE). Then we decouple the\nwhole-body control into an upper-body PD controller for precision and a\nlower-body RL controller to enhance robust stability. Finally, we train TOP\nmethod in conjunction with the decoupled controller and VAE to reduce the\nbalance burden resulting from fast upper-body motions that would destabilize\nthe robot and exceed the capabilities of the lower-body RL policy. The\neffectiveness of the proposed approach is evaluated via both simulation and\nreal world experiments, which demonstrate the superiority on standing\nmanipulation tasks stably and accurately. The project page can be found at\nhttps://anonymous.4open.science/w/top-258F/.", "AI": {"tldr": "本文提出了一种新颖的时间优化策略（TOP），用于训练人形机器人站立姿态下的操作控制模型，旨在同时实现平衡、精度和时间效率，通过调整上半身运动的时间轨迹来减轻平衡负担。", "motivation": "现有的人形机器人站立控制器难以精确控制高维度的上半身关节，或者难以同时保证鲁棒性和准确性，尤其是在上半身运动快速时，容易导致机器人不稳定。", "method": "该方法包含三个部分：1) 利用变分自编码器（VAE）学习上半身运动先验，以增强上下半身的协调能力。2) 将全身控制解耦为上半身PD控制器（用于精度）和下半身RL控制器（用于鲁棒稳定性）。3) 训练时间优化策略（TOP），与解耦控制器和VAE结合，以减少由快速上半身运动引起的平衡负担，防止机器人失稳。", "result": "通过仿真和真实世界实验验证了所提方法的有效性，结果表明在站立操作任务中，该方法能够稳定、准确地表现出卓越的性能。", "conclusion": "所提出的时间优化策略（TOP）能够有效解决人形机器人在站立操作任务中平衡、精度和时间效率的挑战，通过优化上半身运动的时间轨迹，显著提升了机器人的稳定性和操作能力。"}}
{"id": "2508.00169", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00169", "abs": "https://arxiv.org/abs/2508.00169", "authors": ["Bhavya Goyal", "Felipe Gutierrez-Barragan", "Wei Lin", "Andreas Velten", "Yin Li", "Mohit Gupta"], "title": "Robust 3D Object Detection using Probabilistic Point Clouds from Single-Photon LiDARs", "comment": "ICCV 2025", "summary": "LiDAR-based 3D sensors provide point clouds, a canonical 3D representation\nused in various scene understanding tasks. Modern LiDARs face key challenges in\nseveral real-world scenarios, such as long-distance or low-albedo objects,\nproducing sparse or erroneous point clouds. These errors, which are rooted in\nthe noisy raw LiDAR measurements, get propagated to downstream perception\nmodels, resulting in potentially severe loss of accuracy. This is because\nconventional 3D processing pipelines do not retain any uncertainty information\nfrom the raw measurements when constructing point clouds.\n  We propose Probabilistic Point Clouds (PPC), a novel 3D scene representation\nwhere each point is augmented with a probability attribute that encapsulates\nthe measurement uncertainty (or confidence) in the raw data. We further\nintroduce inference approaches that leverage PPC for robust 3D object\ndetection; these methods are versatile and can be used as computationally\nlightweight drop-in modules in 3D inference pipelines. We demonstrate, via both\nsimulations and real captures, that PPC-based 3D inference methods outperform\nseveral baselines using LiDAR as well as camera-LiDAR fusion models, across\nchallenging indoor and outdoor scenarios involving small, distant, and\nlow-albedo objects, as well as strong ambient light.\n  Our project webpage is at https://bhavyagoyal.github.io/ppc .", "AI": {"tldr": "该论文提出了概率点云（Probabilistic Point Clouds, PPC），一种新的3D场景表示方法，通过为每个点增加概率属性来封装测量不确定性。PPC及其推理方法能有效提升LiDAR和相机-LiDAR融合模型在挑战性场景下的3D目标检测鲁棒性。", "motivation": "现代LiDAR在长距离或低反照率物体等场景下，会产生稀疏或错误的点云。这些错误源于原始LiDAR测量的噪声，并传播到下游感知模型，导致精度严重下降。这是因为传统的3D处理流程在构建点云时未保留任何不确定性信息。", "method": "本文提出概率点云（PPC），一种新颖的3D场景表示，其中每个点都附加一个概率属性，用于封装原始数据中的测量不确定性（或置信度）。此外，还引入了利用PPC进行鲁棒3D目标检测的推理方法，这些方法计算量轻，可作为即插即用模块集成到现有3D推理管道中。", "result": "通过仿真和真实采集数据验证，基于PPC的3D推理方法在包含小型、远距离、低反照率物体以及强环境光的挑战性室内外场景中，均优于使用LiDAR以及相机-LiDAR融合模型的多个基线方法。", "conclusion": "PPC通过整合测量不确定性，显著提升了3D感知任务的鲁棒性和准确性，尤其在LiDAR数据质量受限的复杂场景中表现出色，且其推理方法具有高效和易于集成的优势。"}}
{"id": "2508.00143", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00143", "abs": "https://arxiv.org/abs/2508.00143", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Kenneth R. Koedinger"], "title": "Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation", "comment": "Accepted for presentation at NCME AIME-Con 2025", "summary": "Humans can be notoriously imperfect evaluators. They are often biased,\nunreliable, and unfit to define \"ground truth.\" Yet, given the surging need to\nproduce large amounts of training data in educational applications using AI,\ntraditional inter-rater reliability (IRR) metrics like Cohen's kappa remain\ncentral to validating labeled data. IRR remains a cornerstone of many machine\nlearning pipelines for educational data. Take, for example, the classification\nof tutors' moves in dialogues or labeling open responses in machine-graded\nassessments. This position paper argues that overreliance on human IRR as a\ngatekeeper for annotation quality hampers progress in classifying data in ways\nthat are valid and predictive in relation to improving learning. To address\nthis issue, we highlight five examples of complementary evaluation methods,\nsuch as multi-label annotation schemes, expert-based approaches, and\nclose-the-loop validity. We argue that these approaches are in a better\nposition to produce training data and subsequent models that produce improved\nstudent learning and more actionable insights than IRR approaches alone. We\nalso emphasize the importance of external validity, for example, by\nestablishing a procedure of validating tutor moves and demonstrating that it\nworks across many categories of tutor actions (e.g., providing hints). We call\non the field to rethink annotation quality and ground truth--prioritizing\nvalidity and educational impact over consensus alone.", "AI": {"tldr": "现有AI教育应用中过度依赖人类评估者间信度（IRR）来验证标注数据会阻碍进展，应优先考虑有效性和教育影响。", "motivation": "人类评估者存在偏见且不可靠，但传统的评估者间信度（IRR）指标（如Cohen's kappa）仍是教育AI中大量训练数据标注的核心验证方法，这阻碍了数据分类的有效性和预测性，从而影响学习改进。", "method": "提出并强调五种互补的评估方法，例如多标签标注方案、基于专家的方法和闭环有效性。同时强调外部有效性的重要性，例如建立验证导师行为的程序。", "result": "这些补充方法能够生成更好的训练数据和后续模型，从而改善学生学习并提供更具操作性的洞察，优于单独使用IRR的方法。", "conclusion": "呼吁该领域重新思考标注质量和“真实值”的定义，优先考虑有效性和教育影响，而非仅仅是评估者之间的共识。"}}
{"id": "2508.00305", "categories": ["cs.CL", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.00305", "abs": "https://arxiv.org/abs/2508.00305", "authors": ["Ammar Ahmed", "Sheng Di", "Franck Cappello", "Zirui Liu", "Jingoo Han", "Ali Anwar"], "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models", "comment": null, "summary": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.", "AI": {"tldr": "本文系统性地基准测试了大型语言模型（LLM）在长上下文场景下的优化方法（剪枝、量化、token丢弃），并发现简单组合这些优化可能对大模型产生负面影响，且F1分数可能掩盖精度-召回率的权衡。", "motivation": "尽管大型语言模型在自然语言处理任务中表现出色，但它们面临资源需求高和上下文窗口受限的问题。现有的优化技术（如剪枝、量化、token丢弃）在长上下文场景下的有效性及其对系统评估的影响尚未得到充分探索。", "method": "研究首先分析了两种支持长上下文的LLM架构的单一优化方法，然后系统性地评估了这些技术的组合，以评估其对性能指标的影响。随后，研究在700亿参数的更大模型上研究了单一优化方法的可扩展性。评估指标包括内存使用、延迟、吞吐量以及文本生成质量。研究结合了系统级性能分析和任务特定洞察（例如，通过F1分数与精度-召回率权衡进行对比）。", "result": "研究发现，与小型模型相比，简单的推理优化算法组合可能会因为累积的近似误差而对大型模型产生不利影响。实验表明，仅依赖F1分数可能会掩盖问答任务中的精度-召回率权衡，从而掩盖这些负面效应。", "conclusion": "通过整合系统级性能分析和任务特定洞察，这项研究有助于LLM从业者和研究人员在不同任务和硬件配置下探索和平衡效率、准确性和可扩展性。"}}
{"id": "2508.00724", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.00724", "abs": "https://arxiv.org/abs/2508.00724", "authors": ["Boyu Li", "Zhengchen Li", "Weimin Wu", "Mengchu Zhou"], "title": "Petri Net Modeling and Deadlock-Free Scheduling of Attachable Heterogeneous AGV Systems", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "The increasing demand for automation and flexibility drives the widespread\nadoption of heterogeneous automated guided vehicles (AGVs). This work intends\nto investigate a new scheduling problem in a material transportation system\nconsisting of attachable heterogeneous AGVs, namely carriers and shuttles. They\ncan flexibly attach to and detach from each other to cooperatively execute\ncomplex transportation tasks. While such collaboration enhances operational\nefficiency, the attachment-induced synchronization and interdependence render\nthe scheduling coupled and susceptible to deadlock. To tackle this challenge,\nPetri nets are introduced to model AGV schedules, well describing the\nconcurrent and sequential task execution and carrier-shuttle synchronization.\nBased on Petri net theory, a firing-driven decoding method is proposed, along\nwith deadlock detection and prevention strategies to ensure deadlock-free\nschedules. Furthermore, a Petri net-based metaheuristic is developed in an\nadaptive large neighborhood search framework and incorporates an effective\nacceleration method to enhance computational efficiency. Finally, numerical\nexperiments using real-world industrial data validate the effectiveness of the\nproposed algorithm against the scheduling policy applied in engineering\npractice, an exact solver, and four state-of-the-art metaheuristics. A\nsensitivity analysis is also conducted to provide managerial insights.", "AI": {"tldr": "本文研究了一种新型异构AGV（载具和穿梭车）调度问题，其中AGV可灵活附着和分离以协作完成任务。为解决附着引起的同步和死锁问题，引入Petri网建模和调度，并开发了一种基于Petri网的元启发式算法，经验证其在真实工业数据上表现优异。", "motivation": "自动化和灵活性需求的增长推动了异构AGV的广泛应用。传统AGV调度未能考虑可附着异构AGV间的协作及其带来的同步和死锁挑战，因此需要研究一种新的调度问题。", "method": "1. 引入Petri网建模AGV调度，描述并发、顺序任务执行及载具-穿梭车同步。2. 提出一种基于Petri网理论的激发驱动解码方法，并集成死锁检测和预防策略以确保无死锁调度。3. 开发一种基于Petri网的元启发式算法，将其融入自适应大邻域搜索框架，并结合有效加速方法提高计算效率。", "result": "通过使用真实工业数据进行的数值实验，验证了所提算法的有效性。该算法优于工程实践中的调度策略、精确求解器和四种最先进的元启发式算法。敏感性分析也提供了管理见解。", "conclusion": "本研究成功解决了可附着异构AGV的调度问题，通过Petri网理论有效处理了同步和死锁挑战，并开发出一种高性能、实用的调度算法，显著提升了系统效率和鲁棒性。"}}
{"id": "2508.00471", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00471", "abs": "https://arxiv.org/abs/2508.00471", "authors": ["Yiwen Wang", "Xinning Chai", "Yuhong Zhang", "Zhengxue Cheng", "Jun Zhao", "Rong Xie", "Li Song"], "title": "Semantic and Temporal Integration in Latent Diffusion Space for High-Fidelity Video Super-Resolution", "comment": null, "summary": "Recent advancements in video super-resolution (VSR) models have demonstrated\nimpressive results in enhancing low-resolution videos. However, due to\nlimitations in adequately controlling the generation process, achieving high\nfidelity alignment with the low-resolution input while maintaining temporal\nconsistency across frames remains a significant challenge. In this work, we\npropose Semantic and Temporal Guided Video Super-Resolution (SeTe-VSR), a novel\napproach that incorporates both semantic and temporal-spatio guidance in the\nlatent diffusion space to address these challenges. By incorporating high-level\nsemantic information and integrating spatial and temporal information, our\napproach achieves a seamless balance between recovering intricate details and\nensuring temporal coherence. Our method not only preserves high-reality visual\ncontent but also significantly enhances fidelity. Extensive experiments\ndemonstrate that SeTe-VSR outperforms existing methods in terms of detail\nrecovery and perceptual quality, highlighting its effectiveness for complex\nvideo super-resolution tasks.", "AI": {"tldr": "本文提出SeTe-VSR，一种新型视频超分辨率方法，通过在潜在扩散空间中引入语义和时空引导，解决现有VSR模型在高保真对齐和时间一致性方面的挑战，有效平衡细节恢复与时间连贯性，并显著提升视觉质量。", "motivation": "现有视频超分辨率（VSR）模型在增强低分辨率视频时，难以有效控制生成过程，导致难以实现与低分辨率输入的精确对齐（高保真度）和帧间的时间一致性。", "method": "提出语义和时空引导视频超分辨率（SeTe-VSR）方法。该方法在潜在扩散空间中融入高层语义信息，并整合空间和时间信息，以实现对生成过程的有效引导。", "result": "SeTe-VSR在恢复复杂细节和确保时间连贯性之间实现了无缝平衡。该方法不仅保留了高真实感的视觉内容，还显著增强了保真度。实验表明，SeTe-VSR在细节恢复和感知质量方面优于现有方法。", "conclusion": "SeTe-VSR通过结合语义和时空引导，成功解决了视频超分辨率中的核心挑战，证明了其在复杂视频超分辨率任务中的有效性，能够提供更好的细节恢复和感知质量。"}}
{"id": "2508.00362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00362", "abs": "https://arxiv.org/abs/2508.00362", "authors": ["Zhenghan Chen", "Haodong Zhang", "Dongqi Wang", "Jiyu Yu", "Haocheng Xu", "Yue Wang", "Rong Xiong"], "title": "A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot", "comment": null, "summary": "Motion imitation is a pivotal and effective approach for humanoid robots to\nachieve a more diverse range of complex and expressive movements, making their\nperformances more human-like. However, the significant differences in\nkinematics and dynamics between humanoid robots and humans present a major\nchallenge in accurately imitating motion while maintaining balance. In this\npaper, we propose a novel whole-body motion imitation framework for a full-size\nhumanoid robot. The proposed method employs contact-aware whole-body motion\nretargeting to mimic human motion and provide initial values for reference\ntrajectories, and the non-linear centroidal model predictive controller ensures\nthe motion accuracy while maintaining balance and overcoming external\ndisturbances in real time. The assistance of the whole-body controller allows\nfor more precise torque control. Experiments have been conducted to imitate a\nvariety of human motions both in simulation and in a real-world humanoid robot.\nThese experiments demonstrate the capability of performing with accuracy and\nadaptability, which validates the effectiveness of our approach.", "AI": {"tldr": "本文提出了一种新颖的全身运动模仿框架，使全尺寸人形机器人能够准确、平衡地模仿人类运动，并克服运动学和动力学差异。", "motivation": "人形机器人模仿人类运动时，由于机器人与人类在运动学和动力学上的显著差异，难以在保持平衡的同时准确模仿复杂的、富有表现力的动作。", "method": "该方法采用接触感知全身运动重定向技术来模仿人类运动并提供参考轨迹的初始值；使用非线性质心模型预测控制器实时确保运动精度、保持平衡并克服外部干扰；全身控制器辅助实现更精确的扭矩控制。", "result": "在仿真和真实人形机器人上进行的多种人类运动模仿实验表明，该框架能够实现高精度和强适应性的运动表现。", "conclusion": "所提出的全身运动模仿框架有效，能够使人形机器人准确、自适应地模仿人类运动，并克服运动学和动力学差异带来的挑战。"}}
{"id": "2508.00171", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00171", "abs": "https://arxiv.org/abs/2508.00171", "authors": ["David Restrepo", "Ira Ktena", "Maria Vakalopoulou", "Stergios Christodoulidis", "Enzo Ferrante"], "title": "On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI", "comment": "Accepted to MICCAI 2025 1st Workshop on Multimodal Large Language\n  Models (MLLMs) in Clinical Practice", "summary": "Clinical decision-making relies on the integrated analysis of medical images\nand the associated clinical reports. While Vision-Language Models (VLMs) can\noffer a unified framework for such tasks, they can exhibit strong biases toward\none modality, frequently overlooking critical visual cues in favor of textual\ninformation. In this work, we introduce Selective Modality Shifting (SMS), a\nperturbation-based approach to quantify a model's reliance on each modality in\nbinary classification tasks. By systematically swapping images or text between\nsamples with opposing labels, we expose modality-specific biases. We assess six\nopen-source VLMs-four generalist models and two fine-tuned for medical data-on\ntwo medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)\nand FairVLMed (scanning laser ophthalmoscopy). By assessing model performance\nand the calibration of every model in both unperturbed and perturbed settings,\nwe reveal a marked dependency on text input, which persists despite the\npresence of complementary visual information. We also perform a qualitative\nattention-based analysis which further confirms that image content is often\novershadowed by text details. Our findings highlight the importance of\ndesigning and evaluating multimodal medical models that genuinely integrate\nvisual and textual cues, rather than relying on single-modality signals.", "AI": {"tldr": "视觉语言模型（VLMs）在医学图像和报告分析中存在模态偏见，过度依赖文本信息。本文提出选择性模态转移（SMS）方法量化这种偏见，通过实验发现模型严重依赖文本，忽略视觉线索。研究强调需开发真正整合视觉和文本信息的多模态医学模型。", "motivation": "临床决策依赖于医学图像及其报告的整合分析。视觉语言模型（VLMs）虽能提供统一框架，但常表现出对单一模态的强烈偏见，频繁忽略关键视觉线索而偏爱文本信息。", "method": "本文引入选择性模态转移（SMS），一种基于扰动的方法，用于量化二元分类任务中模型对每种模态的依赖性。通过系统地交换具有相反标签样本的图像或文本，揭示模态特异性偏见。评估了六个开源VLM（四个通用型和两个医学微调型）在两个不同模态的医学影像数据集（MIMIC-CXR和FairVLMed）上的性能和校准，包括在未扰动和扰动设置下。此外，还进行了基于注意力的定性分析。", "result": "研究发现模型对文本输入存在显著依赖性，即使存在互补的视觉信息，这种依赖性依然持续。定性注意力分析进一步证实，图像内容经常被文本细节所掩盖。", "conclusion": "研究结果强调了设计和评估能够真正整合视觉和文本线索的多模态医学模型的重要性，而非仅仅依赖单一模态信号。"}}
{"id": "2508.00159", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2508.00159", "abs": "https://arxiv.org/abs/2508.00159", "authors": ["Jobst Heitzig", "Ram Potham"], "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "comment": null, "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "AI": {"tldr": "该研究旨在通过强制AI赋能人类并管理人机权力平衡来提升AI安全性和人类福祉。为此，论文设计了一个可参数化、可分解的客观函数，用于衡量人类权力，并提出了计算该指标的算法，认为这比直接基于效用的目标更安全。", "motivation": "权力是AI安全（如AI追求权力、人类被剥夺权力、人机互动和国际治理中的权力平衡）和人类福祉（追求多样化目标的能力）的关键概念。现有直接基于效用的AI目标可能不安全，因此需要探索一种既能保障AI安全又能促进人类福祉的方法。", "method": "研究提出明确要求AI代理赋能人类并以期望方式管理人类与AI之间的权力平衡。具体方法包括：1. 设计了一个基于公理化方法的可参数化、可分解的目标函数，该函数代表了对不平等和风险规避的人类长期总权力；2. 该函数考虑了人类的有限理性和社会规范，并涵盖了广泛的人类目标；3. 推导了通过逆向归纳法计算该指标的算法，或通过多智能体强化学习近似计算的算法。", "result": "研究推导了计算或近似计算该人类权力指标的算法。通过在多种典型情境中（软性地）最大化该指标，展示了其潜在后果，并描述了可能由此产生的工具性子目标。初步评估表明，（软性地）最大化适当的人类权力聚合指标，可能比直接基于效用的目标，对具身AI系统而言是一种更有益且更安全的目标。", "conclusion": "谨慎评估表明，对具身AI系统而言，（软性地）最大化适当的人类权力聚合指标，可能构成一个有益且比直接基于效用的目标更安全的目标，从而同时促进AI安全性和人类福祉。"}}
{"id": "2508.00332", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00332", "abs": "https://arxiv.org/abs/2508.00332", "authors": ["Kaiyan Zhao", "Zhongtao Miao", "Yoshimasa Tsuruoka"], "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment", "comment": "Work in progress", "summary": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.", "AI": {"tldr": "MCSEO通过引入细粒度对象-短语对齐来增强多模态句子嵌入，以解决传统图像-文本对中的噪声问题，并在语义文本相似度任务上表现优于基线模型。", "motivation": "多模态句子嵌入模型在训练中常使用包含冗余或不相关信息的图像-文本对，这些噪声会影响模型性能。", "method": "提出MCSEO方法，利用现有的分割和目标检测模型提取准确的对象-短语对，并以此优化一个专门针对对象-短语对应关系的对比学习目标，同时结合传统的图像-文本对齐。", "result": "在不同的骨干模型上，MCSEO在语义文本相似度（STS）任务中持续优于强基线模型。", "conclusion": "精确的对象-短语对齐在多模态表示学习中具有重要意义，能够有效提升多模态句子嵌入的性能。"}}
{"id": "2508.00775", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00775", "abs": "https://arxiv.org/abs/2508.00775", "authors": ["Andrea Martin", "Ian R. Manchester", "Luca Furieri"], "title": "Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms", "comment": null, "summary": "In high-stakes engineering applications, optimization algorithms must come\nwith provable worst-case guarantees over a mathematically defined class of\nproblems. Designing for the worst case, however, inevitably sacrifices\nperformance on the specific problem instances that often occur in practice. We\naddress the problem of augmenting a given linearly convergent algorithm to\nimprove its average-case performance on a restricted set of target problems -\nfor example, tailoring an off-the-shelf solver for model predictive control\n(MPC) for an application to a specific dynamical system - while preserving its\nworst-case guarantees across the entire problem class. Toward this goal, we\ncharacterize the class of algorithms that achieve linear convergence for\nclasses of nonsmooth composite optimization problems. In particular, starting\nfrom a baseline linearly convergent algorithm, we derive all - and only - the\nmodifications to its update rule that maintain its convergence properties. Our\nresults apply to augmenting legacy algorithms such as gradient descent for\nnonconvex, gradient-dominated functions; Nesterov's accelerated method for\nstrongly convex functions; and projected methods for optimization over\npolyhedral feasibility sets. We showcase effectiveness of the approach on\nsolving optimization problems with tight iteration budgets in application to\nill-conditioned systems of linear equations and MPC for linear systems.", "AI": {"tldr": "本文提出一种方法，在保持最坏情况收敛保证的同时，提升现有线性收敛算法在特定问题实例上的平均性能。", "motivation": "在实际高风险工程应用中，优化算法虽需提供最坏情况保证，但这通常以牺牲在常见问题实例上的平均性能为代价。研究旨在解决如何改进算法的平均性能，同时不损失其在整个问题类上的最坏情况保证。", "method": "本文首先刻画了非光滑复合优化问题中实现线性收敛的算法类别。在此基础上，从一个基线线性收敛算法出发，推导并确定了所有能保持其收敛特性的更新规则修改方式。", "result": "研究结果适用于增强现有算法，例如非凸、梯度主导函数的梯度下降法；强凸函数的Nesterov加速法；以及多面体可行集上的投影方法。该方法在解决病态线性方程组和线性系统模型预测控制（MPC）中迭代预算紧张的优化问题时，展现了有效性。", "conclusion": "该研究提供了一种系统性的方法，可以针对特定应用场景定制优化算法，从而显著提升其平均性能，同时严格保留了原始算法的最坏情况收敛保证。"}}
{"id": "2508.00590", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00590", "abs": "https://arxiv.org/abs/2508.00590", "authors": ["Yihe Tian", "Kwan Man Cheng", "Zhengbo Zhang", "Tao Zhang", "Suju Li", "Dongmei Yan", "Bing Xu"], "title": "A Novel Modeling Framework and Data Product for Extended VIIRS-like Artificial Nighttime Light Image Reconstruction (1986-2024)", "comment": null, "summary": "Artificial Night-Time Light (NTL) remote sensing is a vital proxy for\nquantifying the intensity and spatial distribution of human activities.\nAlthough the NPP-VIIRS sensor provides high-quality NTL observations, its\ntemporal coverage, which begins in 2012, restricts long-term time-series\nstudies that extend to earlier periods. Despite the progress in extending\nVIIRS-like NTL time-series, current methods still suffer from two significant\nshortcomings: the underestimation of light intensity and the structural\nomission. To overcome these limitations, we propose a novel reconstruction\nframework consisting of a two-stage process: construction and refinement. The\nconstruction stage features a Hierarchical Fusion Decoder (HFD) designed to\nenhance the fidelity of the initial reconstruction. The refinement stage\nemploys a Dual Feature Refiner (DFR), which leverages high-resolution\nimpervious surface masks to guide and enhance fine-grained structural details.\nBased on this framework, we developed the Extended VIIRS-like Artificial\nNighttime Light (EVAL) product for China, extending the standard data record\nbackwards by 26 years to begin in 1986. Quantitative evaluation shows that EVAL\nsignificantly outperforms existing state-of-the-art products, boosting the\n$\\text{R}^2$ from 0.68 to 0.80 while lowering the RMSE from 1.27 to 0.99.\nFurthermore, EVAL exhibits excellent temporal consistency and maintains a high\ncorrelation with socioeconomic parameters, confirming its reliability for\nlong-term analysis. The resulting EVAL dataset provides a valuable new resource\nfor the research community and is publicly available at\nhttps://doi.org/10.11888/HumanNat.tpdc.302930.", "AI": {"tldr": "该研究提出了一种新的两阶段重建框架，用于生成中国长期的VIIRS类夜间灯光（NTL）数据产品（EVAL），将时间序列追溯到1986年，显著优于现有方法。", "motivation": "NPP-VIIRS夜间灯光数据始于2012年，限制了更早时期的长期时间序列研究。现有扩展VIIRS类NTL时间序列的方法存在光强度低估和结构遗漏的显著缺点，需要改进。", "method": "提出了一种新颖的两阶段重建框架：构建阶段和精炼阶段。构建阶段采用层次融合解码器（HFD）增强初始重建的保真度；精炼阶段利用双特征精炼器（DFR），通过高分辨率不透水表面掩模引导和增强精细结构细节。基于此框架，开发了中国扩展VIIRS类夜间灯光（EVAL）产品，将标准数据记录回溯了26年，从1986年开始。", "result": "定量评估显示，EVAL产品显著优于现有最先进的产品，将R²从0.68提高到0.80，同时将RMSE从1.27降低到0.99。此外，EVAL表现出卓越的时间一致性，并与社会经济参数保持高度相关性。", "conclusion": "EVAL数据集为研究社区提供了宝贵的新资源，其可靠性已得到证实，可用于长期分析。该数据集已公开可用。"}}
{"id": "2508.00384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00384", "abs": "https://arxiv.org/abs/2508.00384", "authors": ["Juanwu Lu", "Rohit Gupta", "Ahmadreza Moradipari", "Kyungtae Han", "Ruqi Zhang", "Ziran Wang"], "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. Source Code: https://github.com/juanwulu/niva", "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.", "AI": {"tldr": "本文提出NIVA，一个基于分层贝叶斯模型的概率框架，用于多智能体交通模拟，实现闭环、条件观测的模拟，并能统一现有模型，在Waymo数据集上表现优异并提供意图和驾驶风格控制。", "motivation": "自动驾驶车辆（AV）的快速部署迭代，对构建逼真且可扩展的多智能体交通模拟器以进行高效评估的需求日益增长，尤其需要能生成多样化和交互式场景的闭环模拟器。", "method": "引入Neural Interactive Agents (NIVA)，一个由分层贝叶斯模型驱动的概率框架。它通过从潜在的、有限高斯混合分布中自回归采样，实现闭环、观察条件下的模拟。NIVA从贝叶斯推断的角度统一了现有的序列到序列轨迹预测模型和新兴的基于下一令牌预测（NTP）的闭环模拟模型。", "result": "在Waymo开放运动数据集上的实验表明，NIVA与现有方法相比达到了有竞争力的性能。", "conclusion": "NIVA在提供竞争性性能的同时，还增强了对智能体意图和驾驶风格的控制，并从贝叶斯推断视角统一了多种预测和模拟模型。"}}
{"id": "2508.00197", "categories": ["cs.CV", "cs.LG", "cs.NA", "math.CT", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.00197", "abs": "https://arxiv.org/abs/2508.00197", "authors": ["Eric Mjolsness", "Cory B. Scott"], "title": "Graph Lineages and Skeletal Graph Products", "comment": "42 pages. 33 Figures. Under review", "summary": "Graphs, and sequences of growing graphs, can be used to specify the\narchitecture of mathematical models in many fields including machine learning\nand computational science. Here we define structured graph \"lineages\" (ordered\nby level number) that grow in a hierarchical fashion, so that: (1) the number\nof graph vertices and edges increases exponentially in level number; (2)\nbipartite graphs connect successive levels within a graph lineage and, as in\nmultigrid methods, can constrain matrices relating successive levels; (3) using\nprolongation maps within a graph lineage, process-derived distance measures\nbetween graphs at successive levels can be defined; (4) a category of \"graded\ngraphs\" can be defined, and using it low-cost \"skeletal\" variants of standard\nalgebraic graph operations and type constructors (cross product, box product,\ndisjoint sum, and function types) can be derived for graded graphs and hence\nhierarchical graph lineages; (5) these skeletal binary operators have similar\nbut not identical algebraic and category-theoretic properties to their standard\ncounterparts; (6) graph lineages and their skeletal product constructors can\napproach continuum limit objects. Additional space-efficient unary operators on\ngraded graphs are also derived: thickening, which creates a graph lineage of\nmultiscale graphs, and escalation to a graph lineage of search frontiers\n(useful as a generalization of adaptive grids and in defining \"skeletal\"\nfunctions). The result is an algebraic type theory for graded graphs and\n(hierarchical) graph lineages. The approach is expected to be well suited to\ndefining hierarchical model architectures - \"hierarchitectures\" - and local\nsampling, search, or optimization algorithms on them. We demonstrate such\napplication to deep neural networks (including visual and feature scale spaces)\nand to multigrid numerical methods.", "AI": {"tldr": "本文提出了一种用于分层图结构（“图谱系”和“分级图”）的代数类型理论，通过定义骨架操作符来高效地构建和操作这些结构，并展示了其在深度学习和多重网格数值方法中的应用。", "motivation": "图和不断增长的图序列被广泛用于机器学习和计算科学中的模型架构。研究动机在于需要一种结构化的、分层的方式来定义和操作这些不断增长的图，特别是为了构建和分析多尺度或分层模型架构（“分层架构”）。", "method": "研究方法包括：1) 定义分层增长的“结构化图谱系”，其顶点和边数量呈指数级增长；2) 使用二部图连接连续层；3) 利用延长映射定义过程导出的距离度量；4) 定义“分级图”范畴，并推导出其标准代数图操作（如叉积、箱积、不交和、函数类型）的低成本“骨架”变体；5) 推导出节省空间的单目操作符，如“增厚”和“升级”；6) 最终构建一个分级图和分层图谱系的代数类型理论。", "result": "研究结果是建立了一套用于分级图和分层图谱系的代数类型理论。推导出的骨架二元操作符与标准操作符具有相似但非完全相同的代数和范畴论性质。图谱系及其骨架乘积构造器可以逼近连续体极限对象。此外，还推导了空间高效的单目操作符。该方法非常适合定义分层模型架构及在其上的局部采样、搜索或优化算法，并成功应用于深度神经网络（包括视觉和特征尺度空间）和多重网格数值方法。", "conclusion": "该研究提供了一种新的分级图和分层图谱系的代数类型理论，为定义和操作分层模型架构提供了一个强大的、高效的框架。这种方法有望在机器学习和计算科学等领域中用于构建多尺度模型和算法。"}}
{"id": "2508.00222", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS是一种新颖的方法，通过结合内部思考和外部学习来增强大型语言模型（LLM）的推理能力，解决了RLVR的局限性并超越了基础模型的固有能力边界。", "motivation": "现有的可验证奖励强化学习（RLVR）方法受限于基础LLM的固有能力边界，因为其策略是基于当前策略的，且面临巨大的动作空间和稀疏奖励问题。此外，RLVR可能导致能力边界崩溃，缩小LLM的问题解决范围。", "method": "RL-PLUS通过协同内部利用（思考）和外部数据（学习）来工作。它包含两个核心组件：多重重要性采样（解决外部数据导致的分布不匹配问题）和基于探索的优势函数（引导模型探索高价值、未探索的推理路径）。", "result": "RL-PLUS在六个数学推理基准测试中达到了现有RLVR方法的最新水平，在六个分布外推理任务中表现出色。它在不同模型家族中实现了21.1%至69.2%的平均相对提升。此外，在多个基准测试上的Pass@k曲线表明RL-PLUS有效解决了能力边界崩溃问题。", "conclusion": "RL-PLUS是一种优越且具有泛化性的方法，能够显著提升LLM的推理能力，超越基础模型的限制，并有效解决RLVR所面临的挑战。"}}
{"id": "2508.00344", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00344", "abs": "https://arxiv.org/abs/2508.00344", "authors": ["Keer Lu", "Chong Chen", "Bin Cui", "Huang Leng", "Wentao Zhang"], "title": "PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable advancements in tackling\nagent-oriented tasks. Despite their potential, existing work faces challenges\nwhen deploying LLMs in agent-based environments. The widely adopted agent\nparadigm ReAct centers on integrating single-step reasoning with immediate\naction execution, which limits its effectiveness in complex tasks requiring\nlong-term strategic planning. Furthermore, the coordination between the planner\nand executor during problem-solving is also a critical factor to consider in\nagent design. Additionally, current approaches predominantly rely on supervised\nfine-tuning, which often leads models to memorize established task completion\ntrajectories, thereby restricting their generalization ability when confronted\nwith novel problem contexts. To address these challenges, we introduce an\nadaptive global plan-based agent paradigm AdaPlan, aiming to synergize\nhigh-level explicit guidance with execution to support effective long-horizon\ndecision-making. Based on the proposed paradigm, we further put forward\nPilotRL, a global planning-guided training framework for LLM agents driven by\nprogressive reinforcement learning. We first develop the model's ability to\nfollow explicit guidance from global plans when addressing agent tasks.\nSubsequently, based on this foundation, we focus on optimizing the quality of\ngenerated plans. Finally, we conduct joint optimization of the model's planning\nand execution coordination. Experiments indicate that PilotRL could achieve\nstate-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing\nclosed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%\ncomparing to GPT-4o-mini at a comparable parameter scale.", "AI": {"tldr": "针对LLM代理在复杂任务中长期规划、执行协调和泛化能力不足的问题，本文提出了AdaPlan范式和PilotRL训练框架。PilotRL通过渐进式强化学习，优化了模型遵循全局计划、生成计划的质量以及规划与执行的协调，实现了最先进的性能。", "motivation": "现有LLM代理范式（如ReAct）在复杂任务中存在局限性，其单步推理难以支持长期战略规划；规划器与执行器之间的协调不足；以及监督微调方法导致模型记忆特定轨迹，泛化能力受限。", "method": "引入自适应全局计划代理范式AdaPlan，旨在协同高层显式指导与执行。基于此，提出PilotRL，一个由渐进式强化学习驱动的全局规划引导训练框架。PilotRL分三阶段优化：首先培养模型遵循全局计划的能力；其次优化生成计划的质量；最后联合优化模型的规划与执行协调。", "result": "PilotRL实现了最先进的性能。LLaMA3.1-8B-Instruct + PilotRL的性能超越闭源模型GPT-4o 3.60%，并且在参数规模相当的情况下，比GPT-4o-mini高出55.78%。", "conclusion": "PilotRL有效解决了LLM代理在复杂任务中长期规划、执行协调和泛化能力方面的挑战，通过渐进式强化学习显著提升了模型性能，达到了新的技术水平。"}}
{"id": "2508.00750", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.00750", "abs": "https://arxiv.org/abs/2508.00750", "authors": ["Prerana Ramkumar"], "title": "SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation", "comment": null, "summary": "Generative Adversarial Networks (GANs) have achieved realistic\nsuper-resolution (SR) of images however, they lack semantic consistency and\nper-pixel confidence, limiting their credibility in critical remote sensing\napplications such as disaster response, urban planning and agriculture. This\npaper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first\nSR framework designed for satellite imagery to integrate the ESRGAN,\nsegmentation loss via DeepLabv3 for class detail preservation and Monte Carlo\ndropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results\n(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This\nnovel model is valuable in satellite systems or UAVs that use wide\nfield-of-view (FoV) cameras, trading off spatial resolution for coverage. The\nmodular design allows integration in UAV data pipelines for on-board or\npost-processing SR to enhance imagery resulting due to motion blur, compression\nand sensor limitations. Further, the model is fine-tuned to evaluate its\nperformance on cross domain applications. The tests are conducted on two drone\nbased datasets which differ in altitude and imaging perspective. Performance\nevaluation of the fine-tuned models show a stronger adaptation to the Aerial\nMaritime Drone Dataset, whose imaging characteristics align with the training\ndata, highlighting the importance of domain-aware training in SR-applications.", "AI": {"tldr": "提出SU-ESRGAN，首个为卫星图像设计的超分辨率框架，通过整合ESRGAN、DeepLabv3和Monte Carlo dropout，解决语义一致性和像素级不确定性问题。", "motivation": "现有基于GAN的超分辨率方法缺乏语义一致性和像素级置信度，这限制了它们在灾害响应、城市规划和农业等关键遥感应用中的可信度。", "method": "提出语义和不确定性感知ESRGAN (SU-ESRGAN)，它整合了ESRGAN、通过DeepLabv3实现分割损失以保留类别细节，以及Monte Carlo dropout以生成像素级不确定性图。该模型采用模块化设计，并针对跨领域应用在无人机数据集上进行了微调。", "result": "SU-ESRGAN在航空图像上产生了与基线ESRGAN相当的超分辨率结果（PSNR、SSIM、LPIPS）。微调模型显示出对成像特征与训练数据一致的数据集更强的适应性，突出了领域感知训练的重要性。", "conclusion": "SU-ESRGAN对于使用广角相机以牺牲空间分辨率换取覆盖范围的卫星系统或无人机具有重要价值，能够增强因运动模糊、压缩和传感器限制导致的图像质量。在超分辨率应用中，领域感知训练至关重要。"}}
{"id": "2508.00467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00467", "abs": "https://arxiv.org/abs/2508.00467", "authors": ["Samratul Fuady", "Danesh Tarapore", "Mohammad D. Soorati"], "title": "SubCDM: Collective Decision-Making with a Swarm Subset", "comment": "6 pages, 7 figures. This paper has been accepted for presentation at\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)", "summary": "Collective decision-making is a key function of autonomous robot swarms,\nenabling them to reach a consensus on actions based on environmental features.\nExisting strategies require the participation of all robots in the\ndecision-making process, which is resource-intensive and prevents the swarm\nfrom allocating the robots to any other tasks. We propose Subset-Based\nCollective Decision-Making (SubCDM), which enables decisions using only a swarm\nsubset. The construction of the subset is dynamic and decentralized, relying\nsolely on local information. Our method allows the swarm to adaptively\ndetermine the size of the subset for accurate decision-making, depending on the\ndifficulty of reaching a consensus. Simulation results using one hundred robots\nshow that our approach achieves accuracy comparable to using the entire swarm\nwhile reducing the number of robots required to perform collective\ndecision-making, making it a resource-efficient solution for collective\ndecision-making in swarm robotics.", "AI": {"tldr": "该论文提出了一种名为SubCDM的子集式集体决策方法，允许机器人群体仅使用部分机器人进行决策，从而节省资源，同时保持与全群体决策相当的准确性。", "motivation": "现有的机器人群体集体决策策略需要所有机器人参与，这导致资源消耗大，并限制了机器人群体同时执行其他任务的能力。", "method": "本文提出子集式集体决策（SubCDM）方法，仅使用机器人群体的一个子集进行决策。该子集的构建是动态和去中心化的，仅依赖局部信息。该方法能根据达成共识的难度，自适应地确定子集大小以确保决策准确性。", "result": "使用一百个机器人进行的仿真结果表明，所提出的方法在实现与整个群体决策相当的准确性的同时，显著减少了参与集体决策所需的机器人数量。", "conclusion": "SubCDM为群体机器人中的集体决策提供了一种资源高效的解决方案，它在不牺牲准确性的前提下，降低了决策过程的资源消耗。"}}
{"id": "2508.00205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00205", "abs": "https://arxiv.org/abs/2508.00205", "authors": ["Xiangyu Kong", "Hengde Zhu", "Haoqin Sun", "Zhihao Guo", "Jiayan Gu", "Xinyi Ni", "Wei Zhang", "Shizhe Liu", "Siyang Song"], "title": "Learning Personalised Human Internal Cognition from External Expressive Behaviours for Real Personality Recognition", "comment": "10 pages, 4 figures", "summary": "Automatic real personality recognition (RPR) aims to evaluate human real\npersonality traits from their expressive behaviours. However, most existing\nsolutions generally act as external observers to infer observers' personality\nimpressions based on target individuals' expressive behaviours, which\nsignificantly deviate from their real personalities and consistently lead to\ninferior recognition performance. Inspired by the association between real\npersonality and human internal cognition underlying the generation of\nexpressive behaviours, we propose a novel RPR approach that efficiently\nsimulates personalised internal cognition from easy-accessible external short\naudio-visual behaviours expressed by the target individual. The simulated\npersonalised cognition, represented as a set of network weights that enforce\nthe personalised network to reproduce the individual-specific facial reactions,\nis further encoded as a novel graph containing two-dimensional node and edge\nfeature matrices, with a novel 2D Graph Neural Network (2D-GNN) proposed for\ninferring real personality traits from it. To simulate real personality-related\ncognition, an end-to-end strategy is designed to jointly train our cognition\nsimulation, 2D graph construction, and personality recognition modules.", "AI": {"tldr": "该论文提出一种新颖的真实人格识别（RPR）方法，通过从短时音视频行为中模拟个性化内部认知，并利用2D图神经网络（2D-GNN）来推断真实人格特质，旨在克服现有方法仅基于外部观察者印象的局限性。", "motivation": "现有的人格识别方法通常仅作为外部观察者，根据目标个体的表达行为推断观察者的人格印象，这与个体的真实人格存在显著偏差，并导致识别性能不佳。本研究受到真实人格与生成表达行为的内部认知之间关联的启发。", "method": "该方法通过以下步骤实现：1) 从易于获取的短时音视频行为中模拟个性化内部认知，将其表示为一套网络权重，能够重现个体特有的面部反应。2) 将模拟的个性化认知编码为一种新型图，该图包含二维节点和边特征矩阵。3) 提出一种新型2D图神经网络（2D-GNN）从该图中推断真实人格特质。4) 设计端到端训练策略，联合训练认知模拟、2D图构建和人格识别模块。", "result": "论文提出了一种旨在高效模拟个性化内部认知并更准确地推断真实人格特质的新型RPR方法，克服了传统方法仅推断观察者印象的局限性。具体结果（如性能指标）未在摘要中给出。", "conclusion": "通过模拟与真实人格相关的内部认知并结合创新的2D图神经网络，该方法为实现更准确的真实人格识别提供了一条有潜力的新途径。"}}
{"id": "2508.00271", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "MetaAgent是一个受“边做边学”启发的智能体范式，它通过持续的自我改进、工具学习和知识蒸馏，无需模型参数更改或额外训练，即可实现知识发现和能力提升。", "motivation": "研究动机是开发一个能够像人类一样通过实践和持续自我提升来发展专业知识的智能体，解决现有智能体在复杂知识发现任务中可能存在的局限性，并探索无需模型再训练的自进化系统。", "method": "MetaAgent采用最小工作流启动，具备基本推理和自适应求助能力。遇到知识空白时，它生成自然语言求助请求，并通过专用工具路由器路由到最合适的外部工具。它持续进行自我反思和答案验证，将可操作的经验提炼成文本并动态融入未来的任务上下文。此外，MetaAgent通过组织工具使用历史，自主构建内部工具和持久知识库，实现“元工具学习”，从而迭代优化其推理和工具使用策略。", "result": "在GAIA、WebWalkerQA和BrowseCamp等知识发现基准测试中，MetaAgent持续优于基于工作流的基线方法，并达到或超过了端到端训练的智能体性能。", "conclusion": "MetaAgent展示了自进化智能体系统在鲁棒、通用知识发现方面的巨大潜力，证明了通过“边做边学”和持续自我改进，无需模型参数更改即可实现能力增强的可能性。"}}
{"id": "2508.00360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00360", "abs": "https://arxiv.org/abs/2508.00360", "authors": ["Alan Dao", "Dinh Bach Vu", "Alex Nguyen", "Norapat Buppodom"], "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors", "comment": null, "summary": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.", "AI": {"tldr": "本文提出了一种新范式，将小型语言模型（SLM）的内部推理视为动态任务向量机，通过RLVR优化，使SLM能动态构建和优化任务向量，在知识密集型任务上达到与大型模型相当的性能。", "motivation": "小型语言模型由于容量限制，在知识密集型任务中表现受限。现有方法将推理视为固定或启发式过程，未能充分利用模型内部的动态推理能力。", "method": "将模型内部由`<think>`和`</think>`标签界定的推理过程视为一个动态任务向量机。模型在此过程中动态构建和完善自身的任务向量。通过RLVR（强化学习与向量奖励）方法优化这个动态任务向量机，并训练了一个智能体网络搜索模型，命名为Lucy。", "result": "名为Lucy的1.7B参数SLM，结合动态推理机制和MCP集成，在SimpleQA基准测试中取得了78.3%的准确率，性能与DeepSeek-V3等大型模型不相上下。", "conclusion": "研究表明，当小型模型配备结构化、自构建的任务推理机制时，它们能够在知识密集型任务上与大型模型匹敌。"}}
{"id": "2508.00491", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00491", "abs": "https://arxiv.org/abs/2508.00491", "authors": ["Carlo Alessi", "Federico Vasile", "Federico Ceola", "Giulia Pasquale", "Nicolò Boccardo", "Lorenzo Natale"], "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning", "comment": "Paper accepted at IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation", "AI": {"tldr": "本文提出了一种基于模仿学习的方法HannesImitationPolicy，用于控制Hannes假肢手在非结构化环境中抓取物体，并构建了HannesImitationDataset数据集，通过扩散策略实现了比传统视觉伺服控制器更好的抓取性能。", "motivation": "现有假肢手控制系统侧重于通过传感器增加自主性以减少用户认知负担，但模仿学习在假肢控制中的应用仍未充分探索。将模仿学习应用于假肢可提高灵巧性，使设备能在更不受限的场景中通过示范学习任务，而非依赖手动标注序列。", "method": "提出HannesImitationPolicy，一种基于模仿学习的Hannes假肢手控制方法。构建了HannesImitationDataset数据集，包含桌面、货架和人机交接场景下的抓取示范。利用该数据集训练一个单一的扩散策略，预测手腕姿态和手部闭合以实现抓取。", "result": "实验评估表明，该策略在多种物体和条件下实现了成功的抓取。在非结构化场景中，该策略的表现优于基于分割的视觉伺服控制器。", "conclusion": "基于模仿学习的HannesImitationPolicy结合扩散策略和HannesImitationDataset，能有效控制Hannes假肢手在非结构化环境中进行物体抓取，并在性能上超越了传统的视觉伺服控制器，为假肢控制提供了新的有效途径。"}}
{"id": "2508.00213", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00213", "abs": "https://arxiv.org/abs/2508.00213", "authors": ["Shayan Jalilian", "Abdul Bais"], "title": "SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters", "comment": null, "summary": "The Segment Anything Model (SAM) has demonstrated impressive generalization\nin prompt-based segmentation. Yet, the potential of semantic text prompts\nremains underexplored compared to traditional spatial prompts like points and\nboxes. This paper introduces SAM-PTx, a parameter-efficient approach for\nadapting SAM using frozen CLIP-derived text embeddings as class-level semantic\nguidance. Specifically, we propose a lightweight adapter design called\nParallel-Text that injects text embeddings into SAM's image encoder, enabling\nsemantics-guided segmentation while keeping most of the original architecture\nfrozen. Our adapter modifies only the MLP-parallel branch of each transformer\nblock, preserving the attention pathway for spatial reasoning. Through\nsupervised experiments and ablations on the COD10K dataset as well as low-data\nsubsets of COCO and ADE20K, we show that incorporating fixed text embeddings as\ninput improves segmentation performance over purely spatial prompt baselines.\nTo our knowledge, this is the first work to use text prompts for segmentation\non the COD10K dataset. These results suggest that integrating semantic\nconditioning into SAM's architecture offers a practical and scalable path for\nefficient adaptation with minimal computational complexity.", "AI": {"tldr": "SAM-PTx是一种参数高效的方法，通过将冻结的CLIP文本嵌入作为语义指导，将文本提示引入到SAM中，以实现语义引导的图像分割。", "motivation": "尽管SAM在基于提示的分割方面表现出色，但其对语义文本提示的潜力尚未得到充分探索，与传统的点和框等空间提示相比，这部分能力有待挖掘。", "method": "本文提出了SAM-PTx，一种参数高效的SAM适应方法。它使用冻结的CLIP文本嵌入作为类别级语义指导。具体而言，设计了一个轻量级适配器“Parallel-Text”，将文本嵌入注入到SAM的图像编码器中，同时保持原始架构大部分冻结。该适配器仅修改每个Transformer块的MLP并行分支，保留注意力路径用于空间推理。", "result": "通过在COD10K数据集以及COCO和ADE20K的低数据子集上进行监督实验和消融研究，结果表明，将固定文本嵌入作为输入可以提高分割性能，优于纯空间提示基线。这是首次在COD10K数据集上使用文本提示进行分割的工作。", "conclusion": "将语义条件（文本提示）集成到SAM架构中，为高效适应提供了一条实用且可扩展的路径，且计算复杂度极低。"}}
{"id": "2508.00282", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "研究发现，即使提供心理驱动因素，大型语言模型（LLM，如GPT-4o）在任务生成上仍无法像人类一样，其生成的任务缺乏社交性、物理性，且偏向抽象，这揭示了LLM与人类认知之间的核心差距。", "motivation": "尽管LLM驱动的生成式智能体旨在模拟人类复杂的任务生成行为，但其是否遵循与人类相似的认知原则仍不确定，本研究旨在探讨这一问题。", "method": "通过一项任务生成实验，比较了人类受试者和LLM智能体（GPT-4o）的响应。实验中，LLM被明确提供了人类的心理驱动因素（如个人价值观和认知风格）。", "result": "人类的任务生成持续受到心理驱动因素（如开放性、认知风格）的影响。即使明确提供这些心理驱动因素，LLM也未能反映出相应的行为模式，其生成的任务明显更少社交性、更少物理性，且在主题上偏向抽象。尽管LLM生成的任务被认为更有趣和新颖，但这反而突显了其语言能力与生成类人、具身目标能力之间的脱节。", "conclusion": "人类以价值观驱动、具身化的认知与LLM的统计模式之间存在核心差距。因此，在设计更符合人类的智能体时，有必要融入内在动机和物理具身性。"}}
{"id": "2508.00370", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00370", "abs": "https://arxiv.org/abs/2508.00370", "authors": ["Jiyu Chen", "Poh Seng Lim", "Shuang Peng", "Daxiong Luo", "JungHau Foo", "Yap Deep", "Timothy Lee Jun Jie", "Kelvin Teh Kae Wen", "Fan Yang", "Danyu Feng", "Hao-Yun Chen", "Peng-Wen Chen", "Fangyuan Li", "Xiaoxin Chen", "Wong Wai Mun"], "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices", "comment": "9 pages", "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.", "AI": {"tldr": "EdgeInfinite-Instruct 提出分段监督微调、量化和固定形状计算图，以高效地在资源受限的边缘设备上部署长序列Transformer模型，同时保持性能和提高TTFT。", "motivation": "Transformer模型在边缘设备上处理长序列面临挑战，主要由于自注意力的二次时间复杂度和不断增长的KV缓存需求。现有优化方法未能有效降低首个token生成时间（TTFT）或导致性能下降。其他序列建模架构需要完全重新训练且缺乏基础设施支持。现有EdgeInfinite方案指令遵循能力有限，且缺乏移动特定优化。", "method": "本文提出EdgeInfinite-Instruct，通过以下方法解决问题：1. 引入分段监督微调（S-SFT）策略，专为长序列任务（如摘要和问答）定制。2. 采用细粒度训练后量化（PTQ）以降低计算需求并保持精度。3. 实现固定形状计算图，通过场景特定的输入token和缓存大小定制，平衡内存使用和设备效率。", "result": "在长上下文基准测试和真实世界移动任务上的实验表明，该方法在NPU加速的边缘设备上提高了特定领域的性能，同时保持了效率。", "conclusion": "EdgeInfinite-Instruct通过创新的微调策略和设备优化，有效解决了长序列Transformer模型在资源受限边缘设备上的部署挑战，实现了性能和效率的平衡。"}}
{"id": "2508.00580", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00580", "abs": "https://arxiv.org/abs/2508.00580", "authors": ["Raul Castilla-Arquillo", "Carlos Perez-del-Pulgar", "Levin Gerdes", "Alfonso Garcia-Cerezo", "Miguel A. Olivares-Mendez"], "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery", "comment": null, "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.", "AI": {"tldr": "该研究提出了一种名为OmniUnet的基于Transformer的神经网络架构，利用RGB、深度和热（RGB-D-T）多模态图像进行语义分割，以支持火星探测器在非结构化环境中的安全导航，并在火星模拟环境中进行了验证。", "motivation": "机器人需要在非结构化环境中安全导航，这需要多模态感知系统来整合互补信息。现有的机器学习算法需要专门设计来利用异构数据，并且需要识别对导航最有信息量的传感器模态。在火星探索中，热成像已被证明对评估地形安全性具有重要价值。", "method": "开发了OmniUnet，一个基于Transformer的神经网络架构，用于RGB-D-T图像的语义分割。设计并3D打印了定制的多模态传感器外壳，并安装在火星漫游车自主测试平台（MaRTA）上。在西班牙巴德纳斯半沙漠（模拟火星环境）收集了多模态数据集，并手动标注了部分数据用于监督训练。模型通过定量和定性方法进行评估。", "result": "OmniUnet在像素精度上达到80.37%，在分割复杂非结构化地形方面表现出色。在资源受限的Jetson Orin Nano计算机上，平均预测时间为673毫秒，证实了其在机器人上部署的适用性。网络软件实现和标注数据集已公开。", "conclusion": "OmniUnet是一种有效的多模态地形感知方法，适用于行星机器人，其性能和部署效率证明了其在资源受限机器人平台上的实用性，并为未来的相关研究提供了宝贵资源。"}}
{"id": "2508.00218", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00218", "abs": "https://arxiv.org/abs/2508.00218", "authors": ["Aymane Abdali", "Bartosz Boguslawski", "Lucas Drumetz", "Vincent Gripon"], "title": "Object-Centric Cropping for Visual Few-Shot Classification", "comment": null, "summary": "In the domain of Few-Shot Image Classification, operating with as little as\none example per class, the presence of image ambiguities stemming from multiple\nobjects or complex backgrounds can significantly deteriorate performance. Our\nresearch demonstrates that incorporating additional information about the local\npositioning of an object within its image markedly enhances classification\nacross established benchmarks. More importantly, we show that a significant\nfraction of the improvement can be achieved through the use of the Segment\nAnything Model, requiring only a pixel of the object of interest to be pointed\nout, or by employing fully unsupervised foreground object extraction methods.", "AI": {"tldr": "在少样本图像分类中，通过引入目标局部位置信息，显著提升了分类性能，即使是利用SAM模型仅提供一个像素点或无监督前景提取方法也能实现显著改进。", "motivation": "少样本图像分类中，图像歧义（多目标或复杂背景）会严重降低性能。", "method": "研究通过以下方式引入目标局部定位信息：1) 结合目标在图像中的局部位置信息；2) 使用Segment Anything Model (SAM) 仅需指定目标一个像素点；3) 采用完全无监督的前景目标提取方法。", "result": "在现有基准测试中，分类性能显著提升；其中很大一部分改进可通过使用SAM或无监督前景提取方法实现。", "conclusion": "在少样本图像分类中，目标局部位置信息对性能提升至关重要，且可通过少量人工标注（如SAM）或无监督方法有效获取。"}}
{"id": "2508.00323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00323", "abs": "https://arxiv.org/abs/2508.00323", "authors": ["Jianyi Zhang", "Xu Ji", "Ziyin Zhou", "Yuchen Zhou", "Shubo Shi", "Haoyu Wu", "Zhen Li", "Shizhao Liu"], "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning", "comment": null, "summary": "Evaluating the performance of visual language models (VLMs) in graphic\nreasoning tasks has become an important research topic. However, VLMs still\nshow obvious deficiencies in simulating human-level graphic reasoning\ncapabilities, especially in complex graphic reasoning and abstract problem\nsolving, which are less studied and existing studies only focus on simple\ngraphics. To evaluate the performance of VLMs in complex graphic reasoning, we\npropose ReasonBench, the first evaluation benchmark focused on structured\ngraphic reasoning tasks, which includes 1,613 questions from real-world\nintelligence tests. ReasonBench covers reasoning dimensions related to\nlocation, attribute, quantity, and multi-element tasks, providing a\ncomprehensive evaluation of the performance of VLMs in spatial, relational, and\nabstract reasoning capabilities. We benchmark 11 mainstream VLMs (including\nclosed-source and open-source models) and reveal significant limitations of\ncurrent models. Based on these findings, we propose a dual optimization\nstrategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability\nof reasoning by decomposing layers, and ReasonTune enhances the task\nadaptability of model reasoning through training, all of which improves VLM\nperformance by 33.5\\%. All experimental data and code are in the repository:\nhttps://huggingface.co/datasets/cistine/ReasonBench.", "AI": {"tldr": "本文提出了ReasonBench，一个用于评估视觉语言模型（VLMs）在复杂结构化图形推理能力上的基准测试，并揭示了现有模型的显著局限性。同时，提出了一种双重优化策略（DiaCoT和ReasonTune），将VLM性能提升了33.5%。", "motivation": "视觉语言模型在图形推理任务中表现出明显不足，尤其是在复杂图形推理和抽象问题解决方面，现有研究主要集中在简单图形，而对复杂场景的研究较少。", "method": "1. 提出了ReasonBench，一个包含1613个来自真实智力测试问题的结构化图形推理基准测试集，涵盖位置、属性、数量和多元素推理维度。2. 使用ReasonBench评估了11种主流VLM模型（包括闭源和开源）。3. 提出了双重优化策略：Diagrammatic Reasoning Chain (DiaCoT) 通过分解层增强推理可解释性，ReasonTune 通过训练增强模型任务适应性。", "result": "1. 当前主流VLM模型在复杂图形推理任务上存在显著局限性。2. 提出的DiaCoT和ReasonTune双重优化策略将VLM性能提升了33.5%。", "conclusion": "尽管VLMs在图形推理方面仍有显著不足，特别是在复杂任务上，但通过引入专门的评估基准ReasonBench，并结合DiaCoT和ReasonTune等优化策略，可以有效提升模型在复杂图形推理任务上的性能和解释性。"}}
{"id": "2508.00385", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00385", "abs": "https://arxiv.org/abs/2508.00385", "authors": ["Dingzirui Wang", "Xuangliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness", "comment": null, "summary": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.", "AI": {"tldr": "本文研究了上下文学习（ICL）中演示（demonstration）无效的原因，发现无效演示要么信息已被模型学习，要么与查询无关。基于此，提出了一种利用梯度流进行演示选择的新方法GradS，并在实验中验证了其有效性。", "motivation": "现有研究大多假设ICL中的演示是有效的，但许多研究表明并非所有演示都有效，甚至无法带来性能提升。因此，需要深入探究演示无效的根本原因。", "method": "通过梯度流和线性自注意力模型分析，推导出演示无效的原因是其信息已被模型学习或与用户查询不相关。进一步发现多层模型中演示有效性差异会随层数增加而放大。基于这些发现，提出了GradS方法，利用演示相对于给定用户查询的梯度流大小作为选择标准，以确保所选演示的有效性。", "result": "实验结果证实了随着模型层数增加，演示之间有效性差异会放大，验证了理论推导。GradS方法在四个主流大型语言模型和五个主流数据集上，相对于最强基线平均实现了6.8%的相对性能提升，证明了其有效性。", "conclusion": "演示的无效性源于信息已被学习或与查询无关。模型层数增加会放大有效性差异。提出的GradS方法通过利用梯度流，能够有效选择高质量的演示，显著提升ICL性能，为未来的演示选择方法提供了新思路。"}}
{"id": "2508.00584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00584", "abs": "https://arxiv.org/abs/2508.00584", "authors": ["Konstantinos Plotas", "Emmanouil Papadakis", "Drosakis Drosakis", "Panos Trahanias", "Dimitrios Papageorgiou"], "title": "A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup", "comment": "Please find the citation info @ Zenodo, ArXiv or Zenodo, as the\n  proceedings of ICRA are no longer sent to IEEE Xplore", "summary": "In this work, a control scheme for human-robot collaborative object\ntransportation is proposed, considering a quadruped robot equipped with the\nMIGHTY suction cup that serves both as a gripper for holding the object and a\nforce/torque sensor. The proposed control scheme is based on the notion of\nadmittance control, and incorporates a variable damping term aiming towards\nincreasing the controllability of the human and, at the same time, decreasing\nher/his effort. Furthermore, to ensure that the object is not detached from the\nsuction cup during the collaboration, an additional control signal is proposed,\nwhich is based on a barrier artificial potential. The proposed control scheme\nis proven to be passive and its performance is demonstrated through\nexperimental evaluations conducted using the Unitree Go1 robot equipped with\nthe MIGHTY suction cup.", "AI": {"tldr": "本文提出了一种用于人机协作物体运输的控制方案，该方案基于导纳控制，结合了变阻尼以提高人类可控性和降低努力，并引入障碍人工势以防止物体脱落，通过实验验证了其性能和被动性。", "motivation": "旨在实现人机协作的物体运输，同时提高人类的操作可控性、降低其努力，并确保物体在协作过程中不会脱落。", "method": "提出了一种基于导纳控制的方案，其中包含一个可变阻尼项；引入了一个基于障碍人工势的附加控制信号以防止物体脱落；对所提出的控制方案进行了被动性证明；通过配备MIGHTY吸盘的Unitree Go1机器人进行了实验评估。", "result": "所提出的控制方案被证明具有被动性，并通过实验评估展示了其有效性能，确保了在人机协作运输过程中物体不会从吸盘上脱落。", "conclusion": "提出的控制方案能有效实现人机协作物体运输，通过变阻尼提高了人类的可控性和降低了努力，并通过障碍人工势保证了物体不脱落，且具有被动性，实验验证了其有效性。"}}
{"id": "2508.00248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00248", "abs": "https://arxiv.org/abs/2508.00248", "authors": ["Chenggang Guo", "Hao Xu", "XianMing Wan"], "title": "Guided Depth Map Super-Resolution via Multi-Scale Fusion U-shaped Mamba Network", "comment": null, "summary": "Depth map super-resolution technology aims to improve the spatial resolution\nof low-resolution depth maps and effectively restore high-frequency detail\ninformation. Traditional convolutional neural network has limitations in\ndealing with long-range dependencies and are unable to fully model the global\ncontextual information in depth maps. Although transformer can model global\ndependencies, its computational complexity and memory consumption are\nquadratic, which significantly limits its ability to process high-resolution\ndepth maps. In this paper, we propose a multi-scale fusion U-shaped Mamba\n(MSF-UM) model, a novel guided depth map super-resolution framework. The core\ninnovation of this model is to integrate Mamba's efficient state-space modeling\ncapabilities into a multi-scale U-shaped fusion structure guided by a color\nimage. The structure combining the residual dense channel attention block and\nthe Mamba state space module is designed, which combines the local feature\nextraction capability of the convolutional layer with the modeling advantage of\nthe state space model for long-distance dependencies. At the same time, the\nmodel adopts a multi-scale cross-modal fusion strategy to make full use of the\nhigh-frequency texture information from the color image to guide the\nsuper-resolution process of the depth map. Compared with existing mainstream\nmethods, the proposed MSF-UM significantly reduces the number of model\nparameters while achieving better reconstruction accuracy. Extensive\nexperiments on multiple publicly available datasets validate the effectiveness\nof the model, especially showing excellent generalization ability in the task\nof large-scale depth map super-resolution.", "AI": {"tldr": "本文提出了一种名为MSF-UM的多尺度融合U型Mamba模型，用于引导式深度图超分辨率，该模型结合了Mamba的高效状态空间建模能力、U型多尺度融合结构以及彩色图像引导，实现了更优的重建精度和更少的参数量。", "motivation": "传统的卷积神经网络在处理长距离依赖和全局上下文信息方面存在局限性。Transformer虽然能建模全局依赖，但其二次方的计算复杂度和内存消耗限制了其处理高分辨率深度图的能力。", "method": "提出了一种多尺度融合U型Mamba (MSF-UM) 模型，这是一个由彩色图像引导的新型深度图超分辨率框架。核心创新是将Mamba的高效状态空间建模能力集成到多尺度U型融合结构中。设计了结合残差密集通道注意力块（用于局部特征提取）和Mamba状态空间模块（用于长距离依赖建模）的结构。同时，模型采用多尺度跨模态融合策略，利用彩色图像的高频纹理信息指导深度图的超分辨率过程。", "result": "与现有主流方法相比，所提出的MSF-UM模型在实现更好重建精度的同时，显著减少了模型参数。在多个公开数据集上的广泛实验验证了模型的有效性，尤其在大尺度深度图超分辨率任务中展现出卓越的泛化能力。", "conclusion": "MSF-UM模型有效解决了深度图超分辨率中长距离依赖建模和计算效率的挑战，通过结合Mamba、U型结构和多模态融合，实现了高性能和高效率的深度图重建，并具有出色的泛化能力。"}}
{"id": "2508.00324", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "该研究提出R1-Act，一种简单高效的后训练方法，通过结构化推理过程激活大型推理模型中已有的安全知识，显著提升模型安全性，同时保持推理性能。", "motivation": "大型推理模型（LRMs）虽能力强大，但常执行有害指令，引发安全担忧。研究发现模型并非缺乏安全知识，而是未能有效激活。", "method": "基于模型已拥有安全知识但未激活的洞察，提出R1-Act，一个简单高效的后训练方法。该方法通过结构化推理过程，显式地触发模型内部的安全知识。", "result": "R1-Act在显著提升模型安全性的同时，保持了原有的推理性能，且优于现有对齐方法。它仅需1000个训练样本，在单张RTX A6000 GPU上训练90分钟即可完成。在多种LRM骨干网络和规模上的广泛实验证明了其鲁棒性、可扩展性和实用高效性。", "conclusion": "该方法通过激活模型内已有的安全知识，有效解决了大型推理模型的安全问题，且具有极高的训练效率和广泛的适用性，为模型安全对齐提供了新的高效途径。"}}
{"id": "2508.00390", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00390", "abs": "https://arxiv.org/abs/2508.00390", "authors": ["Hengxing Cai", "Jinhan Dong", "Yijie Rao", "Jingcheng Deng", "Jingjun Tan", "Qien Chen", "Haidong Wang", "Zhen Wang", "Shiyu Huang", "Agachai Sumalee", "Renxin Zhong"], "title": "SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation", "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable\nagents to accurately localize targets and plan flight paths in complex\nenvironments based on natural language instructions, with broad applications in\nintelligent inspection, disaster rescue, and urban monitoring. Recent progress\nin Vision-Language Models (VLMs) has provided strong semantic understanding for\nthis task, while reinforcement learning (RL) has emerged as a promising\npost-training strategy to further improve generalization. However, existing RL\nmethods often suffer from inefficient use of training data, slow convergence,\nand insufficient consideration of the difficulty variation among training\nsamples, which limits further performance improvement. To address these\nchallenges, we propose \\textbf{Semantic-Aware Gaussian Curriculum Scheduling\n(SA-GCS)}, a novel training framework that systematically integrates Curriculum\nLearning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator\n(SA-DE) to quantify the complexity of training samples and a Gaussian\nCurriculum Scheduler (GCS) to dynamically adjust the sampling distribution,\nenabling a smooth progression from easy to challenging tasks. This design\nsignificantly improves training efficiency, accelerates convergence, and\nenhances overall model performance. Extensive experiments on the CityNav\nbenchmark demonstrate that SA-GCS consistently outperforms strong baselines\nacross all metrics, achieves faster and more stable convergence, and\ngeneralizes well across models of different scales, highlighting its robustness\nand scalability. The implementation of our approach is publicly available.", "AI": {"tldr": "本文提出SA-GCS框架，将课程学习融入强化学习，以解决无人机视觉-语言导航中现有RL方法训练效率低、收敛慢的问题，通过语义感知难度估计器和高斯课程调度器，实现从易到难的任务平滑进展，显著提升训练效率和模型性能。", "motivation": "无人机视觉-语言导航（UAV VLN）在智能巡检、灾害救援等领域有广泛应用。尽管视觉-语言模型（VLMs）和强化学习（RL）提供了强大支持，但现有RL方法存在训练数据利用效率低、收敛速度慢以及未充分考虑训练样本难度差异的问题，限制了性能提升。", "method": "提出语义感知高斯课程调度（SA-GCS）训练框架，系统地将课程学习（CL）融入强化学习。该框架采用语义感知难度估计器（SA-DE）量化训练样本复杂度，并使用高斯课程调度器（GCS）动态调整采样分布，实现从简单到困难任务的平滑过渡。", "result": "在CityNav基准测试中，SA-GCS在所有指标上持续优于强基线模型，实现了更快、更稳定的收敛，并且对不同规模的模型展现出良好的泛化能力，验证了其鲁棒性和可扩展性。", "conclusion": "SA-GCS通过引入课程学习，显著提高了无人机VLN任务的训练效率，加速了模型收敛，并全面提升了模型性能和泛化能力。"}}
{"id": "2508.00625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00625", "abs": "https://arxiv.org/abs/2508.00625", "authors": ["Bartosz Krawczyk", "Ahmed Elbary", "Robbie Cato", "Jagdish Patil", "Kaung Myat", "Anyeh Ndi-Tah", "Nivetha Sakthivel", "Mark Crampton", "Gautham Das", "Charles Fox"], "title": "OpenScout v1.1 mobile robot: a case study on open hardware continuation", "comment": "6 pages, 4 figures, a TAROS2025 short paper", "summary": "OpenScout is an Open Source Hardware (OSH) mobile robot for research and\nindustry. It is extended to v1.1 which includes simplified, cheaper and more\npowerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo\nsimulation. Changes, their rationale, project methodology, and results are\nreported as an OSH case study.", "AI": {"tldr": "OpenScout v1.1是一款开源硬件移动机器人，通过简化、降低成本并增强计算硬件，同时提供ROS2接口和Gazebo仿真，成为研究和工业的案例研究。", "motivation": "为了改进OpenScout机器人，使其更易于获取、成本更低、计算能力更强，并提供更好的仿真支持。", "method": "项目采用了开源硬件（OSH）的方法论，具体实施包括：简化并优化了机载计算硬件，开发了模拟的ROS2接口，并创建了Gazebo仿真环境。", "result": "成果是OpenScout v1.1版本，它拥有更简化、更便宜且更强大的机载计算硬件，并提供了ROS2接口和Gazebo仿真，作为一个OSH案例研究进行了报告。", "conclusion": "OpenScout v1.1的发布展示了通过开源硬件方法改进移动机器人的可行性与成效，为研究和工业提供了更优的平台，并可作为OSH项目开发的参考案例。"}}
{"id": "2508.00259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00259", "abs": "https://arxiv.org/abs/2508.00259", "authors": ["Wentao Sun", "Hanqing Xu", "Quanyun Wu", "Dedong Zhang", "Yiping Chen", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "PointGauss: Point Cloud-Guided Multi-Object Segmentation for Gaussian Splatting", "comment": "22 pages, 9 figures", "summary": "We introduce PointGauss, a novel point cloud-guided framework for real-time\nmulti-object segmentation in Gaussian Splatting representations. Unlike\nexisting methods that suffer from prolonged initialization and limited\nmulti-view consistency, our approach achieves efficient 3D segmentation by\ndirectly parsing Gaussian primitives through a point cloud segmentation-driven\npipeline. The key innovation lies in two aspects: (1) a point cloud-based\nGaussian primitive decoder that generates 3D instance masks within 1 minute,\nand (2) a GPU-accelerated 2D mask rendering system that ensures multi-view\nconsistency. Extensive experiments demonstrate significant improvements over\nprevious state-of-the-art methods, achieving performance gains of 1.89 to\n31.78% in multi-view mIoU, while maintaining superior computational efficiency.\nTo address the limitations of current benchmarks (single-object focus,\ninconsistent 3D evaluation, small scale, and partial coverage), we present\nDesktopObjects-360, a novel comprehensive dataset for 3D segmentation in\nradiance fields, featuring: (1) complex multi-object scenes, (2) globally\nconsistent 2D annotations, (3) large-scale training data (over 27 thousand 2D\nmasks), (4) full 360{\\deg} coverage, and (5) 3D evaluation masks.", "AI": {"tldr": "本文提出PointGauss框架，利用点云引导实现高斯泼溅表示中的实时多目标分割，并引入了新的多目标3D分割数据集DesktopObjects-360。", "motivation": "现有方法在高斯泼溅中的3D分割存在初始化时间过长和多视角一致性差的问题。同时，当前的基准数据集在单目标、3D评估不一致、规模小和覆盖不全等方面存在局限性。", "method": "PointGauss通过点云分割驱动的管道直接解析高斯基元，实现高效3D分割。核心创新包括：1) 基于点云的高斯基元解码器，可在1分钟内生成3D实例掩码；2) GPU加速的2D掩码渲染系统，确保多视角一致性。此外，本文还提出了DesktopObjects-360数据集，用于辐射场中的3D分割评估。", "result": "PointGauss在多视角mIoU上比现有SOTA方法提升了1.89%至31.78%，并保持了卓越的计算效率。DesktopObjects-360数据集具有复杂多目标场景、全局一致2D标注、大规模训练数据（超过2.7万个2D掩码）、360度全覆盖和3D评估掩码等特点。", "conclusion": "PointGauss成功解决了高斯泼溅中实时多目标分割的挑战，实现了高效且多视角一致的3D分割。同时，DesktopObjects-360数据集的引入为辐射场中的3D分割研究提供了更全面和大规模的基准。"}}
{"id": "2508.00378", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00378", "abs": "https://arxiv.org/abs/2508.00378", "authors": ["Shixin Yi", "Lin Shang"], "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "AI": {"tldr": "CoRGI框架通过引入视觉验证机制，解决了视觉语言模型(VLM)中思维链(CoT)推理缺乏视觉接地性导致幻觉的问题，提高了推理性能和解释的真实性。", "motivation": "思维链(CoT)提示虽然能提升视觉语言模型(VLM)的推理能力，但其生成的解释常流利却缺乏视觉接地性，导致幻觉。这主要是因为多步推理过程中缺少明确的验证机制。", "method": "CoRGI（Chain of Reasoning with Grounded Insights）是一个模块化框架，包含三个阶段：首先生成文本推理链；其次通过专用模块（VEVM）为每一步推理提取支持性视觉证据；最后将文本推理与视觉证据合成，生成接地且经过验证的答案。该框架可与现有VLM集成，无需端到端重新训练。", "result": "CoRGI在VCR基准测试上提高了Qwen-2.5VL和LLaVA-1.6两种代表性开源VLM骨干的推理性能。消融研究证实了验证模块中每一步的贡献，人工评估表明CoRGI能生成更真实、更有帮助的解释。研究还探讨了视觉验证步骤的其他设计，并讨论了事后验证框架的潜在局限性。", "conclusion": "这些发现强调了将中间推理步骤与视觉证据相结合的重要性，以增强多模态推理的鲁棒性。"}}
{"id": "2508.00420", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00420", "abs": "https://arxiv.org/abs/2508.00420", "authors": ["Rana Salama", "Abdou Youssef", "Mona Diab"], "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding", "comment": null, "summary": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.", "AI": {"tldr": "本文探讨了将小波变换（DWT）应用于词和句子嵌入，以减少维度并压缩信息，并在下游NLP任务中取得了可比甚至更优的效果。", "motivation": "小波在图像和信号处理领域取得了显著成功，作者认为其也能有效应用于自然语言处理（NLP）任务，以捕捉各种语言特性，并期望能有效整合词向量中的重要信息同时降低维度。", "method": "核心方法是应用离散小波变换（DWT）到词和句子嵌入。首先评估小波如何有效整合词向量信息并降低维度。进一步，将DWT与离散余弦变换（DCT）结合，提出一个非参数模型，用于基于局部变化的词特征将包含密集信息的句子压缩成固定大小的向量。", "result": "研究显示，小波能有效整合词向量中的重要信息并降低其维度。所提出的DWT与DCT结合的模型，在下游应用模型中，其性能与原始嵌入模型相当，甚至在某些任务中表现更优。", "conclusion": "小波变换是一种有效且有前景的技术，可以应用于NLP任务，尤其是在词和句子嵌入的维度缩减和信息压缩方面，能够保持甚至提升在下游任务中的表现。"}}
{"id": "2508.00691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00691", "abs": "https://arxiv.org/abs/2508.00691", "authors": ["Fabian C. Weigend", "Dabin K. Choe", "Santiago Canete", "Conor J. Walsh"], "title": "Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait", "comment": "8 pages, 6 figures, 2 tables", "summary": "Recent work has shown that exoskeletons controlled through data-driven\nmethods can dynamically adapt assistance to various tasks for healthy young\nadults. However, applying these methods to populations with neuromotor gait\ndeficits, such as post-stroke hemiparesis, is challenging. This is due not only\nto high population heterogeneity and gait variability but also to a lack of\npost-stroke gait datasets to train accurate models. Despite these challenges,\ndata-driven methods offer a promising avenue for control, potentially allowing\nexoskeletons to function safely and effectively in unstructured community\nsettings. This work presents a first step towards enabling adaptive\nplantarflexion and dorsiflexion assistance from data-driven torque estimation\nduring post-stroke walking. We trained a multi-task Temporal Convolutional\nNetwork (TCN) using collected data from four post-stroke participants walking\non a treadmill ($R^2$ of $0.74 \\pm 0.13$). The model uses data from three\ninertial measurement units (IMU) and was pretrained on healthy walking data\nfrom 6 participants. We implemented a wearable prototype for our ankle torque\nestimation approach for exoskeleton control and demonstrated the viability of\nreal-time sensing, estimation, and actuation with one post-stroke participant.", "AI": {"tldr": "该研究提出了一种基于数据驱动的踝关节力矩估计方法，利用时间卷积网络（TCN）和惯性测量单元（IMU）为中风后偏瘫患者的步态康复提供自适应外骨骼辅助，并验证了其实时性。", "motivation": "数据驱动的外骨骼控制方法已成功应用于健康年轻人，但在中风后等神经运动步态缺陷人群中面临挑战，原因包括患者异质性高、步态变异性大以及缺乏相关数据集。然而，此类方法仍有望使外骨骼在非结构化社区环境中安全有效地提供自适应辅助。", "method": "本研究旨在通过数据驱动的力矩估计实现中风后步行的自适应跖屈和背屈辅助。方法包括：1) 使用从四名中风后参与者收集的跑步机步行数据训练一个多任务时间卷积网络（TCN）。2) 模型输入为三个惯性测量单元（IMU）的数据。3) 模型在六名健康参与者的步行数据上进行了预训练。4) 开发并实现了踝关节力矩估计的可穿戴原型，用于外骨骼控制。", "result": "该多任务TCN模型在中风后步行数据上的踝关节力矩估计R²达到了0.74 ± 0.13。研究还通过一名中风后参与者，成功展示了该可穿戴原型在实时传感、估计和驱动方面的可行性。", "conclusion": "这项工作是实现中风后步行数据驱动自适应踝关节辅助的第一步，成功验证了其在实时力矩估计和驱动方面的可行性，为未来在神经运动步态缺陷人群中应用数据驱动的外骨骼控制奠定了基础。"}}
{"id": "2508.00260", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00260", "abs": "https://arxiv.org/abs/2508.00260", "authors": ["Hyundong Jin", "Hyung Jin Chang", "Eunwoo Kim"], "title": "Instruction-Grounded Visual Projectors for Continual Learning of Generative Vision-Language Models", "comment": "Accepted to ICCV 2025", "summary": "Continual learning enables pre-trained generative vision-language models\n(VLMs) to incorporate knowledge from new tasks without retraining data from\nprevious ones. Recent methods update a visual projector to translate visual\ninformation for new tasks, connecting pre-trained vision encoders with large\nlanguage models. However, such adjustments may cause the models to prioritize\nvisual inputs over language instructions, particularly learning tasks with\nrepetitive types of textual instructions. To address the neglect of language\ninstructions, we propose a novel framework that grounds the translation of\nvisual information on instructions for language models. We introduce a mixture\nof visual projectors, each serving as a specialized visual-to-language\ntranslation expert based on the given instruction context to adapt to new\ntasks. To avoid using experts for irrelevant instruction contexts, we propose\nan expert recommendation strategy that reuses experts for tasks similar to\nthose previously learned. Additionally, we introduce expert pruning to\nalleviate interference from the use of experts that cumulatively activated in\nprevious tasks. Extensive experiments on diverse vision-language tasks\ndemonstrate that our method outperforms existing continual learning approaches\nby generating instruction-following responses.", "AI": {"tldr": "该论文提出了一种新的持续学习框架，用于预训练的视觉-语言模型（VLMs），通过引入基于指令的视觉投影器混合专家系统，解决现有方法中语言指令被忽视的问题，并利用专家推荐和剪枝策略提高性能。", "motivation": "现有持续学习方法在更新视觉投影器时，可能导致预训练的视觉-语言模型（VLMs）在处理新任务时，过度优先处理视觉输入而忽视语言指令，尤其是在文本指令类型重复的任务中。因此，需要解决这种对语言指令的忽视。", "method": "本文提出了一种新颖的框架，将视觉信息的翻译过程基于语言模型指令进行。具体方法包括：1) 引入视觉投影器混合专家系统，每个专家根据给定的指令上下文作为专门的视觉到语言翻译专家；2) 提出专家推荐策略，以复用与先前学习任务相似的专家，避免使用不相关的专家；3) 引入专家剪枝机制，以减轻在先前任务中累积激活的专家所造成的干扰。", "result": "在多种视觉-语言任务上进行的广泛实验表明，本文提出的方法在生成遵循指令的响应方面，优于现有的持续学习方法。", "conclusion": "该研究成功解决了持续学习中视觉-语言模型对语言指令的忽视问题，通过创新的指令接地视觉投影器混合专家系统，显著提升了模型在遵循指令方面的性能。"}}
{"id": "2508.00401", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00401", "abs": "https://arxiv.org/abs/2508.00401", "authors": ["Riddhi J. Pitliya", "Ozan Catal", "Toon Van de Maele", "Corrado Pezzato", "Tim Verbelen"], "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation", "comment": null, "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.", "AI": {"tldr": "该论文提出了一种在主动推理框架下实现多智能体协作的新方法，通过引入心智理论（ToM），使智能体能够仅通过观察行为推断他人的信念和目标，从而实现更好的协作，无需共享模型或显式通信。", "motivation": "传统的多智能体主动推理协作方法依赖于任务特定的共享生成模型或显式通信，这限制了其通用性。研究旨在开发一种更通用、无需这些限制的协作方法，并利用心智理论（ToM）使智能体能够理解并推理他人的信念和目标。", "method": "将心智理论（ToM）整合到主动推理框架中，使智能体能够维持自身和他人信念及目标的独立表征。扩展了基于复杂推理树的规划算法，通过递归推理系统地探索联合策略空间。在碰撞避免和觅食任务中进行模拟评估。", "result": "配备ToM的智能体比非ToM智能体表现出更好的协作能力，能够有效避免碰撞并减少冗余努力。关键在于，ToM智能体仅通过观察行为就能推断他人的信念，无需显式通信。", "conclusion": "这项工作推动了人工智能的实际应用，并为心智理论提供了计算层面的见解，展示了通过推断他人信念实现有效多智能体协作的可行性。"}}
{"id": "2508.00429", "categories": ["cs.CL", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00429", "abs": "https://arxiv.org/abs/2508.00429", "authors": ["Minghao Guo", "Xi Zhu", "Jingyuan Huang", "Kai Mei", "Yongfeng Zhang"], "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network", "comment": "17 pages, work in progress", "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.", "AI": {"tldr": "ReaGAN是一个基于智能体和检索增强的图神经网络框架，旨在解决传统GNN在节点信息不平衡和全局语义关系捕获方面的局限性，并在少样本设置下表现出色。", "motivation": "传统图神经网络(GNNs)的预定义聚合机制存在两个主要限制：1) 无法处理节点信息丰富度不平衡的问题（信息密集节点与稀疏节点）；2) 消息传递主要利用局部结构相似性，忽略了图中的全局语义关系，限制了模型捕获远距离但相关信息的能力。", "method": "提出了检索增强图智能体网络(ReaGAN)，一个基于智能体的框架。每个节点被视为一个智能体，能够基于其内部记忆独立规划下一步行动，实现节点级规划和自适应消息传播。此外，通过检索增强生成(RAG)机制，节点可以访问语义相关内容并建立图中的全局关系。ReaGAN使用冻结的LLM骨干模型，无需微调。", "result": "ReaGAN在少样本上下文设置下取得了有竞争力的性能。", "conclusion": "该研究展示了智能体规划和局部-全局检索在图学习中的巨大潜力。"}}
{"id": "2508.00697", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00697", "abs": "https://arxiv.org/abs/2508.00697", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "comment": "ICCV 2025", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.", "AI": {"tldr": "LightDP是一个新颖的框架，通过网络压缩和减少采样步数，显著加速了扩散策略（Diffusion Policies）在资源受限的移动设备上的实时部署，同时保持了竞争力性能。", "motivation": "扩散策略在机器人操作任务中表现出色，但其计算效率低下和内存占用大，限制了它们在资源受限的移动平台上的应用。", "method": "LightDP通过两种核心策略解决计算瓶颈：1) 对去噪模块进行网络压缩，采用统一的剪枝和再训练流程，以优化模型在剪枝后的恢复能力。2) 结合剪枝技术和一致性蒸馏（consistency distillation）来有效减少采样步数，同时保持动作预测精度。", "result": "LightDP在PushT、Robomimic、CALVIN和LIBERO等标准数据集上实现了移动设备的实时动作预测，并保持了有竞争力的性能。广泛的真实世界实验也表明，LightDP的性能与最先进的扩散策略相当。", "conclusion": "LightDP是使扩散策略在资源有限环境中实际部署的重要一步，它在移动设备上实现了实时动作预测，并具有与现有技术相当的性能。"}}
{"id": "2508.00265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00265", "abs": "https://arxiv.org/abs/2508.00265", "authors": ["Henghui Ding", "Song Tang", "Shuting He", "Chang Liu", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Multimodal Referring Segmentation: A Survey", "comment": "Project Page:\n  https://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation", "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.", "AI": {"tldr": "该论文对多模态指代分割进行了全面综述，涵盖其定义、数据集、统一架构、图像/视频/3D场景方法、广义指代表达以及相关应用。", "motivation": "多模态指代分割在需要根据用户指令进行精确物体感知的实际应用中至关重要。过去十年，随着卷积神经网络、Transformer和大型语言模型的发展，其在多模态感知能力上取得显著进步，从而引起了多模态社区的广泛关注。", "method": "本文首先介绍了多模态指代分割的背景、问题定义和常用数据集。接着，总结了一个统一的元架构，并回顾了图像、视频和3D场景三种主要视觉场景中的代表性方法。此外，还讨论了处理现实世界复杂性的广义指代表达（GREx）方法、相关任务和实际应用，并提供了标准基准上的性能比较。", "result": "该综述提供了多模态指代分割领域的全面概述，包括其背景、问题定义、常用数据集、统一元架构、针对不同视觉场景（图像、视频、3D）的代表性方法、广义指代表达方法、相关任务、实际应用以及在标准基准上的广泛性能比较。", "conclusion": "该论文对多模态指代分割进行了全面的调查和总结，为理解该领域的发展、方法和挑战提供了宝贵的资源，并展望了未来的研究方向和实际应用。"}}
{"id": "2508.00414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "本文提出了一个名为Cognitive Kernel-Pro的开源多模块智能体框架，旨在通过高质量数据训练和新颖的测试时策略，实现通用AI智能体的民主化开发和评估，并在GAIA基准测试中取得了领先的性能。", "motivation": "当前的通用AI智能体系统要么是闭源的，要么严重依赖付费API和专有工具，这限制了研究社区的可访问性和可复现性。研究者希望解决这一问题，使高级AI智能体的开发和评估更加民主化。", "method": "研究者提出了一个名为Cognitive Kernel-Pro的完全开源、最大限度免费的多模块智能体框架。该框架系统地研究了智能体基础模型高质量训练数据的策划，专注于在网络、文件、代码和通用推理四个关键领域构建查询、轨迹和可验证答案。此外，还探索了新颖的智能体测试时反思和投票策略，以增强智能体的鲁棒性和性能。", "result": "Cognitive Kernel-Pro在GAIA基准测试中进行了评估，在开源和免费智能体中取得了最先进的结果。值得注意的是，其8B参数的开源模型超越了之前领先的系统，如WebDancer和WebSailor，为可访问、高性能的AI智能体树立了新的性能标准。", "conclusion": "Cognitive Kernel-Pro的发布和其在GAIA上的卓越表现，证明了完全开源和免费的框架能够实现高性能的通用AI智能体，为AI研究社区的民主化开发和评估设定了新的基准。"}}
{"id": "2508.00454", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00454", "abs": "https://arxiv.org/abs/2508.00454", "authors": ["Yuqi Tang", "Kehua Feng", "Yunfeng Wang", "Zhiwen Chen", "Chengfei Lv", "Gang Yu", "Qiang Zhang", "Keyan Ding"], "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges", "comment": "15 pages, 2 pages, under review at AAAI 2026", "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.", "AI": {"tldr": "提出了一种高效的多轮对话评估器，通过将多个LLM评委的偏好知识聚合成一个模型，解决了现有“LLM作为评委”方法的偏差和多评委方法的计算成本问题。", "motivation": "当前LLM对话能力评估主要依赖“LLM作为评委”范式，但其存在各种偏差，影响评估结果的可靠性和一致性。多评委方法虽然有效，但推理时计算开销巨大。", "method": "通过将多个LLM评委的偏好知识聚合成一个单一模型，捕获了它们的集体智慧。这种方法在保留多评委反馈优势的同时，显著降低了评估成本。", "result": "在七个单项评分和成对比较对话评估基准上进行了广泛实验，结果表明该方法在各种场景下均优于现有基线，展示了其效率和鲁棒性。", "conclusion": "所提出的高效多轮对话评估器能够快速灵活地评估对话质量，同时保持了多评委反馈的优势并大幅降低了成本，是一种高效、鲁棒且性能优越的对话评估方案。"}}
{"id": "2508.00795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00795", "abs": "https://arxiv.org/abs/2508.00795", "authors": ["Junbang Liang", "Pavel Tokmakov", "Ruoshi Liu", "Sruthi Sudhakar", "Paarth Shah", "Rares Ambrus", "Carl Vondrick"], "title": "Video Generators are Robot Policies", "comment": null, "summary": "Despite tremendous progress in dexterous manipulation, current visuomotor\npolicies remain fundamentally limited by two challenges: they struggle to\ngeneralize under perceptual or behavioral distribution shifts, and their\nperformance is constrained by the size of human demonstration data. In this\npaper, we use video generation as a proxy for robot policy learning to address\nboth limitations simultaneously. We propose Video Policy, a modular framework\nthat combines video and action generation that can be trained end-to-end. Our\nresults demonstrate that learning to generate videos of robot behavior allows\nfor the extraction of policies with minimal demonstration data, significantly\nimproving robustness and sample efficiency. Our method shows strong\ngeneralization to unseen objects, backgrounds, and tasks, both in simulation\nand the real world. We further highlight that task success is closely tied to\nthe generated video, with action-free video data providing critical benefits\nfor generalizing to novel tasks. By leveraging large-scale video generative\nmodels, we achieve superior performance compared to traditional behavior\ncloning, paving the way for more scalable and data-efficient robot policy\nlearning.", "AI": {"tldr": "该论文提出“视频策略”框架，利用视频生成作为机器人策略学习的代理，以解决视觉运动策略在泛化能力差和对大量演示数据依赖的问题，显著提升了鲁棒性和样本效率。", "motivation": "当前的视觉运动策略在感知或行为分布变化下泛化能力不足，且性能受限于人类演示数据量。", "method": "提出“视频策略”框架，一个模块化的视频和动作生成结合体，可端到端训练。通过学习生成机器人行为视频，从中提取策略。", "result": "该方法仅需极少量演示数据，显著提升了鲁棒性和样本效率。在模拟和现实世界中，对未见物体、背景和任务展现出强大的泛化能力。任务成功与生成的视频紧密相关，无动作视频数据对泛化到新任务具有关键益处。", "conclusion": "通过利用大规模视频生成模型，该方法比传统行为克隆表现更优，为更具可扩展性和数据效率的机器人策略学习铺平了道路。"}}
{"id": "2508.00272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00272", "abs": "https://arxiv.org/abs/2508.00272", "authors": ["Wenyue Chong"], "title": "Towards Robust Semantic Correspondence: A Benchmark and Insights", "comment": null, "summary": "Semantic correspondence aims to identify semantically meaningful\nrelationships between different images and is a fundamental challenge in\ncomputer vision. It forms the foundation for numerous tasks such as 3D\nreconstruction, object tracking, and image editing. With the progress of\nlarge-scale vision models, semantic correspondence has achieved remarkable\nperformance in controlled and high-quality conditions. However, the robustness\nof semantic correspondence in challenging scenarios is much less investigated.\nIn this work, we establish a novel benchmark for evaluating semantic\ncorrespondence in adverse conditions. The benchmark dataset comprises 14\ndistinct challenging scenarios that reflect commonly encountered imaging\nissues, including geometric distortion, image blurring, digital artifacts, and\nenvironmental occlusion. Through extensive evaluations, we provide several key\ninsights into the robustness of semantic correspondence approaches: (1) All\nexisting methods suffer from noticeable performance drops under adverse\nconditions; (2) Using large-scale vision models can enhance overall robustness,\nbut fine-tuning on these models leads to a decline in relative robustness; (3)\nThe DINO model outperforms the Stable Diffusion in relative robustness, and\ntheir fusion achieves better absolute robustness; Moreover, We evaluate common\nrobustness enhancement strategies for semantic correspondence and find that\ngeneral data augmentations are ineffective, highlighting the need for\ntask-specific designs. These results are consistent across both our dataset and\nreal-world benchmarks.", "AI": {"tldr": "本文建立了一个新基准来评估语义对应在恶劣条件下的鲁棒性，发现现有方法性能下降，大型模型可提高鲁棒性但微调会降低相对鲁棒性，且通用数据增强无效。", "motivation": "语义对应在受控和高质量条件下表现出色，但在具有挑战性的场景中其鲁棒性研究不足，而这些场景在实际应用中普遍存在。", "method": "建立了一个包含14种恶劣场景（如几何失真、图像模糊、数字伪影、环境遮挡）的新基准数据集，并在此基础上评估了现有语义对应方法、大型视觉模型（DINO、Stable Diffusion）以及常见的鲁棒性增强策略（如通用数据增强）。", "result": "1) 所有现有方法在恶劣条件下性能均显著下降；2) 使用大型视觉模型可增强整体鲁棒性，但对其进行微调会导致相对鲁棒性下降；3) DINO模型在相对鲁棒性上优于Stable Diffusion，且两者融合能实现更好的绝对鲁棒性；4) 通用数据增强对语义对应鲁棒性增强无效，表明需要任务特定的设计。", "conclusion": "现有语义对应方法在恶劣条件下鲁棒性不足。大型视觉模型有助于提高鲁棒性，但需谨慎处理微调问题。为了有效提升鲁棒性，需要开发任务特定的增强策略，而非依赖通用数据增强。"}}
{"id": "2508.00459", "categories": ["cs.AI", "68T07, 68T20", "I.2.6; I.2.7; I.2.3"], "pdf": "https://arxiv.org/pdf/2508.00459", "abs": "https://arxiv.org/abs/2508.00459", "authors": ["Andrea Asperti", "Alberto Naibo", "Claudio Sacerdoti Coen"], "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.", "AI": {"tldr": "大型语言模型（LLMs）在编程和结构化推理方面表现出色，但在形式化数学证明方面遇到显著困难。本文探讨了LLMs在数学领域的最新进展、挑战以及其内部推理机制。", "motivation": "LLMs在编程和符号任务中表现出卓越能力，引发了将其应用于数学领域的兴趣。然而，与编程的表面相似性不同，LLMs在形式化数学证明方面进展缓慢，这促使人们深入探讨LLMs的推理方式、监督机制以及它们是否追踪计算或演绎状态。", "method": "本文综述了该领域的最新技术水平，重点关注了最新的模型和基准测试。它探讨了机器学习与数学认知交叉点的三个核心问题：1) 形式化与非形式化数学作为训练领域之间的权衡；2) 证明生成比代码合成更脆弱的深层原因；3) LLMs是表示还是仅仅模仿了演化的逻辑状态。", "result": "抽象中未直接呈现具体研究结果，而是指出了当前LLMs在形式化数学领域的局限性，并提出了可能扩展这些局限性的方向。主要结果是识别并分析了LLMs在数学应用中的核心挑战和未解问题。", "conclusion": "本文旨在识别LLMs在形式化数学领域的当前局限性，并探讨如何扩展这些局限，而不是划定明确的边界。"}}
{"id": "2508.00476", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00476", "abs": "https://arxiv.org/abs/2508.00476", "authors": ["Jeongwoo Kang", "Markarit Vartampetian", "Felix Herron", "Yongxin Zhou", "Diandra Fabre", "Gabriela Gonzalez-Saez"], "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts", "comment": null, "summary": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).", "AI": {"tldr": "本文介绍了GETALP团队在SIGDial 2025自动会议纪要共享任务（任务B：基于会议记录的问答）中的提交。他们的方法结合了检索增强生成（RAG）系统和抽象意义表示（AMR），结果显示AMR能提高约35%问题的回答质量，并显著改善涉及区分不同参与者的问题（如“谁”的问题）的回答。", "motivation": "参与SIGDial 2025的第三届自动会议纪要共享任务，特别是任务B，即基于会议记录进行问答。", "method": "采用基于检索增强生成（RAG）系统和抽象意义表示（AMR）的方法。提出了三种结合这两种方法的系统。", "result": "结果表明，结合AMR能够使大约35%的问题产生高质量的回答，并在回答涉及区分不同参与者的问题（例如“谁”的问题）时提供显著改进。", "conclusion": "将AMR融入RAG系统对基于会议记录的问答任务有效，尤其在处理需要区分参与者的问题时表现突出。"}}
{"id": "2508.00299", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00299", "abs": "https://arxiv.org/abs/2508.00299", "authors": ["Danzhen Fu", "Jiagao Hu", "Daiguo Zhou", "Fei Wang", "Zepeng Wang", "Wenhua Liao"], "title": "Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence", "comment": "ICCV 2025 Workshop (HiGen)", "summary": "Pedestrian detection models in autonomous driving systems often lack\nrobustness due to insufficient representation of dangerous pedestrian scenarios\nin training datasets. To address this limitation, we present a novel framework\nfor controllable pedestrian video editing in multi-view driving scenarios by\nintegrating video inpainting and human motion control techniques. Our approach\nbegins by identifying pedestrian regions of interest across multiple camera\nviews, expanding detection bounding boxes with a fixed ratio, and resizing and\nstitching these regions into a unified canvas while preserving cross-view\nspatial relationships. A binary mask is then applied to designate the editable\narea, within which pedestrian editing is guided by pose sequence control\nconditions. This enables flexible editing functionalities, including pedestrian\ninsertion, replacement, and removal. Extensive experiments demonstrate that our\nframework achieves high-quality pedestrian editing with strong visual realism,\nspatiotemporal coherence, and cross-view consistency. These results establish\nthe proposed method as a robust and versatile solution for multi-view\npedestrian video generation, with broad potential for applications in data\naugmentation and scenario simulation in autonomous driving.", "AI": {"tldr": "该论文提出了一种可控的多视角行人视频编辑框架，通过整合视频修复和人体运动控制技术，实现行人插入、替换和移除，以增强自动驾驶模型对危险行人场景的鲁棒性。", "motivation": "自动驾驶系统中的行人检测模型因训练数据中缺乏危险行人场景的充分表示而缺乏鲁棒性。", "method": "该方法结合了视频修复和人体运动控制技术。首先识别多摄像头视图中的行人区域，扩展并拼接这些区域到统一画布，同时保留跨视图空间关系。然后应用二值掩码指定可编辑区域，并通过姿态序列控制条件引导行人编辑，支持行人插入、替换和移除。", "result": "实验证明，该框架实现了高质量的行人编辑，具有强大的视觉真实感、时空连贯性和跨视图一致性。", "conclusion": "该方法是多视角行人视频生成的鲁棒且多功能的解决方案，在自动驾驶的数据增强和场景模拟方面具有广泛的应用潜力。"}}
{"id": "2508.00287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00287", "abs": "https://arxiv.org/abs/2508.00287", "authors": ["Tran Viet Khoa", "Do Hai Son", "Mohammad Abu Alsheikh", "Yibeltal F Alem", "Dinh Thai Hoang"], "title": "Privacy-Preserving Driver Drowsiness Detection with Spatial Self-Attention and Federated Learning", "comment": null, "summary": "Driver drowsiness is one of the main causes of road accidents and is\nrecognized as a leading contributor to traffic-related fatalities. However,\ndetecting drowsiness accurately remains a challenging task, especially in\nreal-world settings where facial data from different individuals is\ndecentralized and highly diverse. In this paper, we propose a novel framework\nfor drowsiness detection that is designed to work effectively with\nheterogeneous and decentralized data. Our approach develops a new Spatial\nSelf-Attention (SSA) mechanism integrated with a Long Short-Term Memory (LSTM)\nnetwork to better extract key facial features and improve detection\nperformance. To support federated learning, we employ a Gradient Similarity\nComparison (GSC) that selects the most relevant trained models from different\noperators before aggregation. This improves the accuracy and robustness of the\nglobal model while preserving user privacy. We also develop a customized tool\nthat automatically processes video data by extracting frames, detecting and\ncropping faces, and applying data augmentation techniques such as rotation,\nflipping, brightness adjustment, and zooming. Experimental results show that\nour framework achieves a detection accuracy of 89.9% in the federated learning\nsettings, outperforming existing methods under various deployment scenarios.\nThe results demonstrate the effectiveness of our approach in handling\nreal-world data variability and highlight its potential for deployment in\nintelligent transportation systems to enhance road safety through early and\nreliable drowsiness detection.", "AI": {"tldr": "该论文提出了一个新颖的疲劳驾驶检测框架，该框架结合了空间自注意力（SSA）机制与长短期记忆（LSTM）网络，并利用梯度相似性比较（GSC）进行联邦学习，以有效处理异构和去中心化的面部数据，同时保护用户隐私。", "motivation": "驾驶员疲劳是交通事故的主要原因之一，准确检测疲劳驾驶在真实世界中面临挑战，因为面部数据是去中心化且高度多样化的。", "method": "该研究开发了一个新的空间自注意力（SSA）机制，并将其与长短期记忆（LSTM）网络集成，以更好地提取面部特征。为支持联邦学习，采用了梯度相似性比较（GSC）来选择最相关的训练模型进行聚合。此外，还开发了一个定制工具用于视频数据处理，包括帧提取、人脸检测裁剪和数据增强（旋转、翻转、亮度调整、缩放）。", "result": "在联邦学习设置下，该框架的检测准确率达到89.9%，在各种部署场景下均优于现有方法。", "conclusion": "该方法能够有效处理真实世界数据变异性，并有望部署于智能交通系统，通过早期可靠的疲劳检测来提高道路安全性。"}}
{"id": "2508.00500", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00500", "abs": "https://arxiv.org/abs/2508.00500", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "comment": null, "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "AI": {"tldr": "Pro2Guard是一个基于概率可达性分析的主动式运行时安全保障框架，用于大语言模型（LLM）智能体，通过预测未来风险在违规发生前进行干预，解决了现有被动式系统缺乏预见性的问题。", "motivation": "LLM智能体虽然功能强大，但其随机行为带来了难以预测的安全风险。现有的基于规则的强制执行系统（如AgentSpec）是被动响应的，通常在不安全行为即将发生或已经发生时才介入，缺乏预见性，且难以处理长周期依赖和分布变化。", "method": "Pro2Guard采用概率可达性分析。它将智能体行为抽象为符号状态，并从执行轨迹中学习离散时间马尔可夫链（DTMC）。在运行时，通过估计到达不安全状态的概率来预测未来风险，当预测风险超过用户定义的阈值时，在违规发生前触发干预。该方法还结合了语义有效性检查和PAC边界，以确保统计可靠性。", "result": "Pro2Guard在家庭智能体和自动驾驶两个安全关键领域进行了广泛评估。在家庭智能体任务中，使用低阈值时，Pro2Guard能在高达93.6%的不安全任务中早期强制执行安全，同时通过可配置模式（如反射）保持高达80.4%的任务完成率。在自动驾驶场景中，Pro2Guard实现了100%的交通法规违规和碰撞预测，提前38.66秒预测风险。", "conclusion": "Pro2Guard通过提供主动的风险预测和干预，有效解决了LLM智能体安全保障中被动式系统的局限性，在安全关键领域展现出高效率，显著提升了LLM智能体的安全性。"}}
{"id": "2508.00489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00489", "abs": "https://arxiv.org/abs/2508.00489", "authors": ["Yixuan Tang", "Jincheng Wang", "Anthony K. H. Tung"], "title": "The Missing Parts: Augmenting Fact Verification with Half-Truth Detection", "comment": null, "summary": "Fact verification systems typically assess whether a claim is supported by\nretrieved evidence, assuming that truthfulness depends solely on what is\nstated. However, many real-world claims are half-truths, factually correct yet\nmisleading due to the omission of critical context. Existing models struggle\nwith such cases, as they are not designed to reason about what is left unsaid.\nWe introduce the task of half-truth detection, and propose PolitiFact-Hidden, a\nnew benchmark with 15k political claims annotated with sentence-level evidence\nalignment and inferred claim intent. To address this challenge, we present\nTRACER, a modular re-assessment framework that identifies omission-based\nmisinformation by aligning evidence, inferring implied intent, and estimating\nthe causal impact of hidden content. TRACER can be integrated into existing\nfact-checking pipelines and consistently improves performance across multiple\nstrong baselines. Notably, it boosts Half-True classification F1 by up to 16\npoints, highlighting the importance of modeling omissions for trustworthy fact\nverification.", "AI": {"tldr": "该研究引入了“半真话检测”任务，提出了新基准PolitiFact-Hidden和模块化框架TRACER，旨在通过识别遗漏的关键信息来处理因上下文缺失而具有误导性的声明，显著提升了半真话分类的性能。", "motivation": "现有的事实核查系统假设真实性仅取决于已声明的内容，但许多现实世界的声明是“半真话”，即事实正确但因遗漏关键上下文而具有误导性。现有模型未能处理这些情况，因为它们未设计来推理未提及的内容。", "method": "1. 引入“半真话检测”任务。2. 构建新基准PolitiFact-Hidden，包含1.5万条政治声明，并标注了句子级证据对齐和推断的声明意图。3. 提出TRACER，一个模块化的重新评估框架，通过证据对齐、推断隐含意图和估计隐藏内容的因果影响来识别基于遗漏的错误信息。", "result": "TRACER框架可以集成到现有事实核查流程中，并持续提升多个强基线的性能。显著地，它将“半真话”分类的F1分数提高了高达16点。", "conclusion": "该研究强调了建模遗漏信息对于可信事实核查的重要性，并表明TRACER能有效解决因信息遗漏导致的误导性声明问题。"}}
{"id": "2508.00440", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00440", "abs": "https://arxiv.org/abs/2508.00440", "authors": ["M. A. Pérez-Cutiño", "J. Valverde", "J. Capitán", "J. M. Díaz-Báñez"], "title": "Reducing the gap between general purpose data and aerial images in concentrated solar power plants", "comment": null, "summary": "In the context of Concentrated Solar Power (CSP) plants, aerial images\ncaptured by drones present a unique set of challenges. Unlike urban or natural\nlandscapes commonly found in existing datasets, solar fields contain highly\nreflective surfaces, and domain-specific elements that are uncommon in\ntraditional computer vision benchmarks. As a result, machine learning models\ntrained on generic datasets struggle to generalize to this setting without\nextensive retraining and large volumes of annotated data. However, collecting\nand labeling such data is costly and time-consuming, making it impractical for\nrapid deployment in industrial applications.\n  To address this issue, we propose a novel approach: the creation of\nAerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By\ngenerating synthetic data that closely mimic real-world conditions, our\nobjective is to facilitate pretraining of models before deployment,\nsignificantly reducing the need for extensive manual labeling. Our main\ncontributions are threefold: (1) we introduce AerialCSP, a high-quality\nsynthetic dataset for aerial inspection of CSP plants, providing annotated data\nfor object detection and image segmentation; (2) we benchmark multiple models\non AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we\ndemonstrate that pretraining on AerialCSP significantly improves real-world\nfault detection, particularly for rare and small defects, reducing the need for\nextensive manual labeling. AerialCSP is made publicly available at\nhttps://mpcutino.github.io/aerialcsp/.", "AI": {"tldr": "针对聚光太阳能发电厂（CSP）无人机图像检测中真实数据稀缺和标注困难的问题，本文提出了一个名为AerialCSP的虚拟合成数据集，用于预训练模型，显著提高了故障检测能力并减少了对人工标注的需求。", "motivation": "现有机器学习模型在处理聚光太阳能发电厂（CSP）的无人机图像时表现不佳，因为这些图像包含高反射表面和独特的领域特定元素，与通用数据集差异大。同时，收集和标注大量此类真实数据成本高昂且耗时，不适用于工业应用的快速部署。", "method": "本文提出并创建了一个名为AerialCSP的虚拟数据集，该数据集通过模拟生成与真实世界CSP工厂无人机图像高度相似的合成数据。研究旨在利用这些合成数据进行模型预训练，以减少对大量真实标注数据的依赖。此外，还在AerialCSP上对多个模型进行了基准测试，为CSP相关的视觉任务建立了基线。", "result": "1. 成功引入了高质量的合成数据集AerialCSP，为CSP工厂的无人机检测任务（包括目标检测和图像分割）提供了标注数据。2. 在AerialCSP上对多个模型进行了基准测试，为CSP相关视觉任务建立了性能基线。3. 实验证明，在AerialCSP上进行预训练能够显著提高真实世界中的故障检测性能，尤其对于稀有和小型缺陷的检测，从而有效减少了对大量人工标注的需求。", "conclusion": "通过创建和利用合成数据集AerialCSP，可以有效解决CSP工厂无人机图像检测中真实数据标注不足的挑战，显著提升模型在实际应用中对故障（特别是稀有和小型缺陷）的检测能力，并降低了部署成本。"}}
{"id": "2508.00289", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00289", "abs": "https://arxiv.org/abs/2508.00289", "authors": ["Christian Simon", "Masato Ishii", "Akio Hayakawa", "Zhi Zhong", "Shusuke Takahashi", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "TITAN-Guide: Taming Inference-Time AligNment for Guided Text-to-Video Diffusion Models", "comment": "Accepted to ICCV 2025", "summary": "In the recent development of conditional diffusion models still require heavy\nsupervised fine-tuning for performing control on a category of tasks.\nTraining-free conditioning via guidance with off-the-shelf models is a\nfavorable alternative to avoid further fine-tuning on the base model. However,\nthe existing training-free guidance frameworks either have heavy memory\nrequirements or offer sub-optimal control due to rough estimation. These\nshortcomings limit the applicability to control diffusion models that require\nintense computation, such as Text-to-Video (T2V) diffusion models. In this\nwork, we propose Taming Inference Time Alignment for Guided Text-to-Video\nDiffusion Model, so-called TITAN-Guide, which overcomes memory space issues,\nand provides more optimal control in the guidance process compared to the\ncounterparts. In particular, we develop an efficient method for optimizing\ndiffusion latents without backpropagation from a discriminative guiding model.\nIn particular, we study forward gradient descents for guided diffusion tasks\nwith various options on directional directives. In our experiments, we\ndemonstrate the effectiveness of our approach in efficiently managing memory\nduring latent optimization, while previous methods fall short. Our proposed\napproach not only minimizes memory requirements but also significantly enhances\nT2V performance across a range of diffusion guidance benchmarks. Code, models,\nand demo are available at https://titanguide.github.io.", "AI": {"tldr": "本文提出了TITAN-Guide，一个针对文生视频（T2V）扩散模型的训练无关指导框架，它通过高效的潜空间优化（利用前向梯度下降）解决了现有方法内存占用大和控制次优的问题，显著提升了T2V性能。", "motivation": "当前的条件扩散模型在特定任务控制上仍需大量监督微调，而训练无关的指导方法（如基于现成模型的引导）虽然避免了微调，但存在内存开销大或控制精度不足的问题。这些缺点限制了其在计算密集型扩散模型（如文生视频T2V）上的应用。", "method": "本文提出了TITAN-Guide框架。其核心方法是开发了一种高效的潜在空间优化技术，能够在不依赖判别性引导模型反向传播的情况下进行优化。具体而言，它研究并采用了前向梯度下降（forward gradient descents）来执行引导扩散任务，并探索了不同的方向指令选项。", "result": "实验证明，TITAN-Guide在潜在空间优化过程中能有效管理内存，克服了现有方法的不足。它不仅最大程度地减少了内存需求，还在一系列扩散引导基准测试中显著提升了文生视频（T2V）的性能。", "conclusion": "TITAN-Guide成功解决了训练无关的文生视频扩散模型在控制过程中面临的内存限制和次优控制问题，通过创新的无反向传播潜空间优化方法，实现了高效且高质量的视频生成控制。"}}
{"id": "2508.00576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00576", "abs": "https://arxiv.org/abs/2508.00576", "authors": ["Zhanliang Wang", "Kai Wang"], "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models", "comment": null, "summary": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.", "AI": {"tldr": "MultiSHAP是一个模型无关的可解释性框架，它利用Shapley交互指数来量化多模态AI模型中细粒度视觉和文本元素之间的协同/抑制作用，适用于开源和闭源模型。", "motivation": "多模态AI模型因其“黑箱”性质，在需要可解释性和可信度的高风险应用中部署面临障碍。现有解释方法（如注意力图、Grad-CAM）只能提供粗略见解，无法精确量化跨模态协同效应，且仅限于开源模型。", "method": "引入MultiSHAP框架，它是一种模型无关的可解释性方法，利用Shapley交互指数将多模态预测归因于细粒度视觉（如图像块）和文本（如文本标记）元素之间的成对交互。该方法适用于开源和闭源模型，并提供实例级（揭示协同和抑制效应）和数据集级（揭示可泛化的交互模式）解释。", "result": "在公共多模态基准上的实验证实，MultiSHAP忠实地捕获了跨模态推理机制。实际案例研究也证明了其实用性。该框架可扩展到两种模态以上。", "conclusion": "MultiSHAP提供了一个通用的解决方案，用于解释复杂的多模态AI模型，通过量化细粒度跨模态交互，增强了模型的可解释性和可信度，并适用于各种模型类型。"}}
{"id": "2508.00522", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00522", "abs": "https://arxiv.org/abs/2508.00522", "authors": ["Jiaxin Deng", "Qingcheng Zhu", "Junbiao Pang", "Linlin Yang", "Zhongqian Fu", "Baochang Zhang"], "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond", "comment": null, "summary": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.", "AI": {"tldr": "该研究提出了Flat-LoRA和EFlat-LoRA，通过在低秩适应（LoRA）中寻找平坦最小值，显著提升了大型语言模型和视觉-语言模型的泛化能力，并验证了LoRA泛化能力与锐度之间的关联。", "motivation": "LoRA的表达能力与泛化能力之间的关联，特别是锐度（sharpness）对泛化的影响，尚未被充分探索。现有方法缺乏在LoRA中经验性寻找平坦最小值或开发相关理论工具的手段，尽管SAM已证明对全参数模型泛化有效。", "method": "提出了Flat-LoRA及其高效版本EFlat-LoRA，旨在为LoRA寻找平坦最小值。理论上证明了全参数空间的扰动可以转移到低秩子空间，从而避免了低秩子空间内多矩阵扰动引入的潜在干扰。", "result": "EFlat-LoRA在优化效率上与LoRA相当，同时实现了可比甚至更好的性能。例如，在GLUE数据集上，EFlat-LoRA在RoBERTa-large模型上平均优于LoRA 1.0%和全参数微调0.5%；在视觉-语言模型Qwen-VL-Chat上，在SQA和VizWiz数据集上分别提升了1.5%和1.0%。这些结果也经验性地验证了LoRA的泛化能力与锐度密切相关。", "conclusion": "Flat-LoRA和EFlat-LoRA通过寻找平坦最小值，有效地提升了LoRA的泛化能力，并首次经验性地证明了LoRA的泛化能力与锐度之间的紧密联系，弥补了先前研究的空白。"}}
{"id": "2508.00589", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.RO", "68T45, 68P20, 68T10, 68T50, 68T07, 68T40", "I.2.10; I.4.8; I.2.9; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.00589", "abs": "https://arxiv.org/abs/2508.00589", "authors": ["Stefan Englmeier", "Max A. Büttner", "Katharina Winter", "Fabian B. Flohr"], "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving", "comment": "9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication", "summary": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.", "AI": {"tldr": "该研究提出一种上下文感知的运动检索框架，通过将人体运动和视频帧编码到与自然语言对齐的共享多模态嵌入空间，以文本查询方式高效检索自动驾驶数据集中罕见的人类行为边缘案例，并引入了新的WayMoCo数据集。", "motivation": "自动驾驶系统在涉及弱势道路使用者（VRU）异常行为的安全关键场景中必须可靠运行。在大规模数据集中识别这些罕见的人类行为“边缘案例”对于系统鲁棒性评估和泛化至关重要，但从长尾数据中检索这些案例极具挑战性。", "method": "提出一种新颖的上下文感知运动检索框架。该方法结合基于SMPL（Skinned Multi-Person Linear）的运动序列和相应的视频帧，将其编码到一个与自然语言对齐的共享多模态嵌入空间中。这使得可以通过文本查询可扩展地检索人类行为及其上下文。此外，工作还引入了WayMoCo数据集，它是Waymo Open Dataset的扩展，包含从生成的伪真值SMPL序列和相应图像数据中自动标注的运动和场景上下文描述。", "result": "在WayMoCo数据集上评估时，该方法在运动-上下文检索方面比现有最先进模型提高了高达27.5%的准确率。", "conclusion": "所提出的上下文感知运动检索框架和WayMoCo数据集有效支持通过文本查询对人类行为及其上下文进行可扩展检索，从而有助于自动驾驶系统在多样化、以人为中心的场景中进行有针对性的评估。"}}
{"id": "2508.00298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00298", "abs": "https://arxiv.org/abs/2508.00298", "authors": ["Jin Lyu", "Liang An", "Li Lin", "Pujin Cheng", "Yebin Liu", "Xiaoying Tang"], "title": "AniMer+: Unified Pose and Shape Estimation Across Mammalia and Aves via Family-Aware Transformer", "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00837", "summary": "In the era of foundation models, achieving a unified understanding of\ndifferent dynamic objects through a single network has the potential to empower\nstronger spatial intelligence. Moreover, accurate estimation of animal pose and\nshape across diverse species is essential for quantitative analysis in\nbiological research. However, this topic remains underexplored due to the\nlimited network capacity of previous methods and the scarcity of comprehensive\nmulti-species datasets. To address these limitations, we introduce AniMer+, an\nextended version of our scalable AniMer framework. In this paper, we focus on a\nunified approach for reconstructing mammals (mammalia) and birds (aves). A key\ninnovation of AniMer+ is its high-capacity, family-aware Vision Transformer\n(ViT) incorporating a Mixture-of-Experts (MoE) design. Its architecture\npartitions network layers into taxa-specific components (for mammalia and aves)\nand taxa-shared components, enabling efficient learning of both distinct and\ncommon anatomical features within a single model. To overcome the critical\nshortage of 3D training data, especially for birds, we introduce a\ndiffusion-based conditional image generation pipeline. This pipeline produces\ntwo large-scale synthetic datasets: CtrlAni3D for quadrupeds and CtrlAVES3D for\nbirds. To note, CtrlAVES3D is the first large-scale, 3D-annotated dataset for\nbirds, which is crucial for resolving single-view depth ambiguities. Trained on\nan aggregated collection of 41.3k mammalian and 12.4k avian images (combining\nreal and synthetic data), our method demonstrates superior performance over\nexisting approaches across a wide range of benchmarks, including the\nchallenging out-of-domain Animal Kingdom dataset. Ablation studies confirm the\neffectiveness of both our novel network architecture and the generated\nsynthetic datasets in enhancing real-world application performance.", "AI": {"tldr": "该研究提出了AniMer+框架，通过一个统一网络实现哺乳动物和鸟类的姿态和形状重建，解决了现有方法容量有限和多物种数据集稀缺的问题，并引入了基于扩散的合成数据生成方法。", "motivation": "在基础模型时代，通过单一网络实现对不同动态物体的统一理解，有望增强空间智能。此外，准确估计不同物种的动物姿态和形状对于生物学研究中的定量分析至关重要。然而，由于先前方法的网络容量有限以及缺乏全面的多物种数据集，这一主题尚未得到充分探索。", "method": "引入了AniMer+，这是可扩展AniMer框架的扩展版本，专注于哺乳动物（mammalia）和鸟类（aves）的统一重建。关键创新是其高容量、家族感知的Vision Transformer（ViT），结合了专家混合（MoE）设计，将网络层划分为类群特定（哺乳动物和鸟类）和类群共享组件。为解决3D训练数据短缺，特别是鸟类数据，引入了基于扩散的条件图像生成管道，生成了两个大规模合成数据集：CtrlAni3D（四足动物）和CtrlAVES3D（鸟类）。CtrlAVES3D是首个大规模、3D标注的鸟类数据集。模型在包含41.3k哺乳动物和12.4k鸟类图像（真实和合成数据）的聚合数据集上进行训练。", "result": "AniMer+在各种基准测试中，包括具有挑战性的域外Animal Kingdom数据集上，均表现出优于现有方法的性能。消融研究证实了其新颖网络架构和生成的合成数据集在增强实际应用性能方面的有效性。", "conclusion": "AniMer+框架及其创新的网络架构和合成数据集生成方法，显著推进了统一的动物姿态和形状重建，特别是针对鸟类，从而增强了空间智能并促进了生物学分析。"}}
{"id": "2508.00581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00581", "abs": "https://arxiv.org/abs/2508.00581", "authors": ["Ruiqing Ding", "Qianfang Sun", "Yongkang Leng", "Hui Yin", "Xiaojian Li"], "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation", "comment": "16 pages, 10 figures", "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.", "AI": {"tldr": "该研究提出了一个多阶段LLM驱动框架，用于从电子病历（EMR）中生成全面的预问诊问卷，克服了直接LLM方法在信息完整性、逻辑性和疾病层面综合方面的局限性。", "motivation": "从复杂且大量的电子病历中生成全面的预问诊问卷是一项挑战性任务。直接使用大型语言模型（LLM）在此任务中存在信息完整性、逻辑顺序和疾病层面综合的困难。", "method": "提出一个新颖的多阶段LLM驱动框架：第一阶段从EMR中提取原子断言（带有时间的关键事实）；第二阶段构建个人因果网络并通过聚类EMR语料库中的代表性网络来综合疾病知识；第三阶段基于这些结构化表示生成定制化的个人和标准化疾病特异性问卷。该框架通过构建显式临床知识来克服直接方法的局限性。", "result": "该方法在真实世界EMR数据集上进行评估，并由临床专家验证，结果表明其在信息覆盖率、诊断相关性、可理解性和生成时间方面表现优越。", "conclusion": "该框架具有增强患者信息收集的实际潜力，克服了直接LLM方法在生成预问诊问卷方面的不足。"}}
{"id": "2508.00537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00537", "abs": "https://arxiv.org/abs/2508.00537", "authors": ["Giulio Zhou", "Tsz Kin Lam", "Alexandra Birch", "Barry Haddow"], "title": "The Prosody of Emojis", "comment": null, "summary": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.", "AI": {"tldr": "本研究通过分析人类语音数据，探讨了表情符号如何影响语音中的韵律表现，以及听者如何通过韵律线索理解表情符号的含义。", "motivation": "在文本交流中，语音的韵律特征（如音高、语速、语调）缺失，表情符号作为视觉替代品补充了情感和语用细微差别。现有研究缺乏直接将韵律与表情符号关联的实证数据，本研究旨在填补这一空白，探究表情符号语义如何塑造口语表达和感知。", "method": "通过结构化但开放式的生成和感知任务，收集并分析了真实的人类语音数据，直接关联了韵律和表情符号。", "result": "研究结果显示，说话者会根据表情符号线索调整其韵律；听者通常仅凭韵律变化就能识别出预期的表情符号；表情符号之间语义差异越大，其韵律分歧也越大。", "conclusion": "这些发现表明，表情符号可以作为韵律意图的有意义载体，揭示了它们在数字媒介交流中的重要作用。"}}
{"id": "2508.00823", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00823", "abs": "https://arxiv.org/abs/2508.00823", "authors": ["Wenxuan Guo", "Xiuwei Xu", "Hang Yin", "Ziwei Wang", "Jianjiang Feng", "Jie Zhou", "Jiwen Lu"], "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation", "comment": "Accepted to ICCV 2025. Project page:\n  https://gwxuan.github.io/IGL-Nav/", "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.", "AI": {"tldr": "本文提出IGL-Nav，一个增量式3D高斯定位框架，用于高效且3D感知的图像目标导航，通过逐步更新场景表示、粗略几何匹配和精细可微分渲染优化实现。", "motivation": "传统的图像目标视觉导航方法（端到端强化学习或基于模块的策略）无法充分建模探索到的3D环境与目标图像之间的几何关系。直接利用3D高斯（3DGS）进行图像定位在代理探索过程中效率极低，因为3DGS优化计算强度大且6自由度相机姿态搜索空间广阔。", "method": "本文提出IGL-Nav框架。首先，随着新图像的到来，通过前馈单目预测增量更新场景的3D高斯表示。其次，通过利用几何信息进行离散空间匹配（等同于高效的3D卷积）来粗略定位目标。最后，当代理接近目标时，通过可微分渲染优化来求解精细的目标姿态。", "result": "IGL-Nav在多种实验配置下大幅超越现有最先进的方法。它还能处理更具挑战性的自由视角图像目标设置，并可部署在真实世界的机器人平台上，使用手机以任意姿态捕获目标图像。", "conclusion": "IGL-Nav提供了一个高效、准确且3D感知的图像目标导航解决方案，克服了传统方法在几何建模和3DGS直接应用效率上的局限性，并展现出优越的性能和实际部署潜力。"}}
{"id": "2508.00308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00308", "abs": "https://arxiv.org/abs/2508.00308", "authors": ["Chunyan She", "Fujun Han", "Chengyu Fang", "Shukai Duan", "Lidan Wang"], "title": "Exploring Fourier Prior and Event Collaboration for Low-Light Image Enhancement", "comment": "Accepted by ACM MM 2025", "summary": "The event camera, benefiting from its high dynamic range and low latency,\nprovides performance gain for low-light image enhancement. Unlike frame-based\ncameras, it records intensity changes with extremely high temporal resolution,\ncapturing sufficient structure information. Currently, existing event-based\nmethods feed a frame and events directly into a single model without fully\nexploiting modality-specific advantages, which limits their performance.\nTherefore, by analyzing the role of each sensing modality, the enhancement\npipeline is decoupled into two stages: visibility restoration and structure\nrefinement. In the first stage, we design a visibility restoration network with\namplitude-phase entanglement by rethinking the relationship between amplitude\nand phase components in Fourier space. In the second stage, a fusion strategy\nwith dynamic alignment is proposed to mitigate the spatial mismatch caused by\nthe temporal resolution discrepancy between two sensing modalities, aiming to\nrefine the structure information of the image enhanced by the visibility\nrestoration network. In addition, we utilize spatial-frequency interpolation to\nsimulate negative samples with diverse illumination, noise and artifact\ndegradations, thereby developing a contrastive loss that encourages the model\nto learn discriminative representations. Experiments demonstrate that the\nproposed method outperforms state-of-the-art models.", "AI": {"tldr": "该论文提出一种利用事件相机进行低光图像增强的新方法，通过将增强流程解耦为可见性恢复和结构细化两个阶段，并设计了相应的网络和融合策略，显著提升了低光图像增强性能。", "motivation": "现有基于事件的图像增强方法未能充分利用事件相机和传统帧相机各自模态的优势，直接将两种数据输入单一模型，限制了性能。事件相机在高动态范围和低延迟方面具有优势，能捕获丰富结构信息。", "method": "1. 将图像增强流程解耦为两个阶段：可见性恢复和结构细化。2. 在第一阶段，设计了一个具有幅度-相位纠缠的可见性恢复网络。3. 在第二阶段，提出一种具有动态对齐的融合策略，以缓解两种传感模态之间时间分辨率差异引起的空间不匹配，从而细化图像结构信息。4. 利用空间频率插值模拟具有不同光照、噪声和伪影退化的负样本，开发了一种对比损失，以促使模型学习判别性表示。", "result": "实验结果表明，所提出的方法优于现有的最先进模型。", "conclusion": "通过深入分析每种传感模态的作用，并将增强流程解耦，并辅以创新的网络设计和损失函数，能够有效利用事件相机数据进行低光图像增强，实现卓越的性能。"}}
{"id": "2508.00632", "categories": ["cs.AI", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00632", "abs": "https://arxiv.org/abs/2508.00632", "authors": ["Alexia Jolicoeur-Martineau"], "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "comment": null, "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.", "AI": {"tldr": "该研究提出了一个用于评估交互式多媒体内容的相对度量标准AVR-Eval和一个多智能体系统AVR-Agent，用于生成JavaScript游戏和动画。尽管AVR-Agent优于一次性生成，但发现AI模型在有效利用自定义资产和视听反馈方面存在局限。", "motivation": "尽管AI在生成文本、音频、图像和视频方面表现出色，但创建视频游戏等交互式视听内容仍然具有挑战性。当前的LLM可以生成简单的游戏，但缺乏自动化评估指标，并且难以处理需要大量人工和艺术资产的复杂内容。", "method": "1. 提出了AVR-Eval：一个使用视听记录（AVRs）评估多媒体内容质量的相对度量标准。一个全模态模型（处理文本、视频、音频）比较两个内容的AVR，并通过文本模型审查评估以确定优劣。2. 构建了AVR-Agent：一个多智能体系统，从多媒体资产库（音频、图像、3D模型）生成JavaScript代码。编码智能体选择相关资产，生成多个初始代码，使用AVR-Eval识别最佳版本，并通过来自AVR的全模态智能体反馈迭代改进。", "result": "1. AVR-Eval能够有效识别好内容与损坏或不匹配的内容。2. AVR-Agent生成的内容与一次性生成的内容相比，具有显著更高的胜率。3. 然而，模型未能有效利用自定义资产和AVR反馈，未能显示出更高的胜率，这揭示了一个关键差距：人类能从高质量资产和视听反馈中受益，但当前的编码模型似乎无法有效利用这些资源。", "conclusion": "尽管AVR-Agent在生成交互式内容方面取得了进展，但当前AI模型在利用高质量资产和视听反馈方面的能力远不如人类，这凸显了人类和机器内容创作方法之间的根本差异。"}}
{"id": "2508.00544", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00544", "abs": "https://arxiv.org/abs/2508.00544", "authors": ["Joonas Tapaninaho", "Mourad Oussala"], "title": "PaPaformer: Language Model from Pre-trained Paraller Paths", "comment": null, "summary": "The training of modern large-language models requires an increasingly amount\nof computation power and time. Even smaller variants, such as small-language\nmodels (SLMs), take several days to train in the best-case scenarios, often\nrequiring multiple GPUs. This paper explores methods to train and evaluate\ndecoder-only transformer-based language models in hours instead of days/weeks.\nWe introduces \\textit{PaPaformer}, a decoder-only transformer architecture\nvariant, whose lower-dimensional parallel paths are combined into larger model.\nThe paper shows that these lower-dimensional paths can be trained individually\nwith different types of training data and then combined into one larger model.\nThis method gives the option to reduce the total number of model parameters and\nthe training time with increasing performance. Moreover, the use of parallel\npath structure opens interesting possibilities to customize paths to\naccommodate specific task requirements.", "AI": {"tldr": "本文提出PaPaformer，一种新的Transformer架构，通过并行路径独立训练并组合，实现小型语言模型在数小时内完成训练，显著缩短训练时间并提升性能。", "motivation": "现代大型语言模型（包括小型变体SLM）的训练需要大量计算资源和时间（数天甚至数周，多GPU），效率低下。", "method": "引入PaPaformer，一种仅解码器Transformer架构变体。其核心思想是将低维并行路径组合成更大的模型。这些低维路径可以独立训练，使用不同类型的数据，然后合并。这种方法旨在减少模型参数总量和训练时间，同时提升性能。此外，并行路径结构也提供了为特定任务定制路径的可能性。", "result": "PaPaformer方法能够将语言模型的训练时间从数天/数周缩短到数小时。它在减少模型参数总量和训练时间的同时，提升了模型性能。并行路径结构还为根据特定任务需求定制路径提供了新的可能性。", "conclusion": "PaPaformer通过其独特的并行路径结构，提供了一种更快速、更高效且可定制的Transformer语言模型训练方法，尤其适用于小型语言模型，解决了传统训练耗时耗力的问题。"}}
{"id": "2508.00311", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00311", "abs": "https://arxiv.org/abs/2508.00311", "authors": ["Yufeng Zhong", "Zhixiong Zeng", "Lei Chen", "Longrong Yang", "Liming Zheng", "Jing Huang", "Siqi Yang", "Lin Ma"], "title": "DocTron-Formula: Generalized Formula Recognition in Complex and Structured Scenarios", "comment": null, "summary": "Optical Character Recognition (OCR) for mathematical formula is essential for\nthe intelligent analysis of scientific literature. However, both task-specific\nand general vision-language models often struggle to handle the structural\ndiversity, complexity, and real-world variability inherent in mathematical\ncontent. In this work, we present DocTron-Formula, a unified framework built\nupon general vision-language models, thereby eliminating the need for\nspecialized architectures. Furthermore, we introduce CSFormula, a large-scale\nand challenging dataset that encompasses multidisciplinary and structurally\ncomplex formulas at the line, paragraph, and page levels. Through\nstraightforward supervised fine-tuning, our approach achieves state-of-the-art\nperformance across a variety of styles, scientific domains, and complex\nlayouts. Experimental results demonstrate that our method not only surpasses\nspecialized models in terms of accuracy and robustness, but also establishes a\nnew paradigm for the automated understanding of complex scientific documents.", "AI": {"tldr": "DocTron-Formula是一个基于通用视觉语言模型的统一框架，结合大规模复杂数学公式数据集CSFormula，通过微调在数学公式OCR任务上实现了最先进的性能，超越了专用模型。", "motivation": "数学公式的光学字符识别（OCR）对于科学文献的智能分析至关重要。然而，现有的任务特定模型和通用视觉语言模型在处理数学内容的结构多样性、复杂性和真实世界变异性方面表现不佳。", "method": "提出了DocTron-Formula，一个基于通用视觉语言模型的统一框架，无需专门架构。同时，引入了CSFormula，一个大规模、具有挑战性的数据集，包含多学科、结构复杂的行、段落和页面级公式。通过直接的监督微调来实现模型训练。", "result": "该方法在各种风格、科学领域和复杂布局上均实现了最先进的性能。实验结果表明，该方法不仅在准确性和鲁棒性方面超越了专用模型。", "conclusion": "该研究为复杂科学文档的自动化理解建立了一个新范式，证明了通用视觉语言模型结合大规模高质量数据集在数学公式OCR领域的潜力。"}}
{"id": "2508.00658", "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00658", "abs": "https://arxiv.org/abs/2508.00658", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "comment": "First draft", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "AI": {"tldr": "该论文提出了多频带变滞后格兰杰因果关系（MB-VLGC）框架，通过显式建模频率依赖的因果延迟，解决了传统格兰杰因果关系和变滞后格兰杰因果关系未能考虑因果相互作用在不同频率带上具有不同时间延迟的问题。", "motivation": "传统的格兰杰因果关系存在固定的滞后假设，这在复杂系统中通常不现实。变滞后格兰杰因果关系（VLGC）解决了变动时间滞后的问题，但未能考虑因果相互作用不仅在时间延迟上变化，而且在不同频率带上也有所不同（例如，大脑信号中不同频段的活动可能以不同延迟影响其他区域）。因此，需要一个能够建模频率依赖因果延迟的新框架。", "method": "本文正式定义了多频带变滞后格兰杰因果关系（MB-VLGC），并提出了一个新颖的框架，该框架通过明确建模频率依赖的因果延迟来泛化传统的VLGC。论文提供了MB-VLGC的正式定义，证明了其理论健全性，并提出了一个高效的推理流程。", "result": "在多个领域的广泛实验表明，该框架在合成数据集和真实世界数据集上均显著优于现有方法，证实了其对任何类型时间序列数据的广泛适用性。", "conclusion": "MB-VLGC框架通过考虑频率依赖的因果延迟，显著提升了时间序列因果关系推断的准确性和普适性，为理解复杂系统中的因果关系提供了更强大的工具。"}}
{"id": "2508.00574", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00574", "abs": "https://arxiv.org/abs/2508.00574", "authors": ["Jianwei Wang", "Ziming Wu", "Fuming Lai", "Shaobing Lian", "Ziqian Zeng"], "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought", "comment": null, "summary": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.", "AI": {"tldr": "SynAdapt是一种高效推理框架，通过生成合成连续思维链（CCoT）作为精确对齐目标，并集成难度分类器自适应地处理难题，从而在准确性和效率之间取得最佳平衡。", "motivation": "传统的离散思维链（DCoT）推理耗时较长，而现有连续思维链（CCoT）方法存在间接微调、对齐不足或目标不一致等问题，限制了其效率和效果。", "method": "SynAdapt包含两部分：1) 生成合成CCoT，作为LLM学习CCoT和直接得出答案的精确有效对齐目标；2) 集成一个难度分类器，利用问题上下文和CCoT识别难题，并自适应地提示LLM重新思考这些难题以提高性能。", "result": "在不同难度级别的各种基准测试中，SynAdapt展现出卓越的有效性，实现了最佳的准确性-效率权衡。", "conclusion": "SynAdapt提供了一种创新的高效推理框架，通过合成CCoT和自适应难题处理，有效克服了现有思维链方法的局限性，提升了大型语言模型的推理效率和准确性。"}}
{"id": "2508.00312", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00312", "abs": "https://arxiv.org/abs/2508.00312", "authors": ["Suhang Cai", "Xiaohao Peng", "Chong Wang", "Xiaojie Cai", "Jiangbo Qian"], "title": "GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection (VAD) plays a critical role in public safety\napplications such as intelligent surveillance. However, the rarity,\nunpredictability, and high annotation cost of real-world anomalies make it\ndifficult to scale VAD datasets, which limits the performance and\ngeneralization ability of existing models. To address this challenge, we\npropose a generative video-enhanced weakly-supervised video anomaly detection\n(GV-VAD) framework that leverages text-conditioned video generation models to\nproduce semantically controllable and physically plausible synthetic videos.\nThese virtual videos are used to augment training data at low cost. In\naddition, a synthetic sample loss scaling strategy is utilized to control the\ninfluence of generated synthetic samples for efficient training. The\nexperiments show that the proposed framework outperforms state-of-the-art\nmethods on UCF-Crime datasets. The code is available at\nhttps://github.com/Sumutan/GV-VAD.git.", "AI": {"tldr": "本文提出GV-VAD框架，利用文本条件视频生成模型合成视频来扩充视频异常检测数据集，并结合损失缩放策略优化训练，有效提升了模型性能。", "motivation": "现实世界中的视频异常数据稀有、不可预测且标注成本高昂，导致视频异常检测（VAD）数据集难以扩展，从而限制了现有模型的性能和泛化能力。", "method": "提出了一种生成式视频增强弱监督视频异常检测（GV-VAD）框架。该框架利用文本条件视频生成模型来生成语义可控且物理合理的合成视频，以低成本扩充训练数据。此外，还采用了一种合成样本损失缩放策略，用于控制生成样本的影响，以实现高效训练。", "result": "实验结果表明，所提出的框架在UCF-Crime数据集上优于现有最先进的方法。", "conclusion": "通过结合生成式视频增强和弱监督学习，该框架有效解决了视频异常检测中数据稀缺的挑战，显著提升了模型的性能和泛化能力。"}}
{"id": "2508.00665", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00665", "abs": "https://arxiv.org/abs/2508.00665", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "comment": null, "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "AI": {"tldr": "本文提出一个混合框架，结合传统可解释AI（XAI）和生成式AI，为自适应学习系统提供个性化、多模态解释，以增强透明度和用户体验。", "motivation": "现有AI驱动的自适应学习系统缺乏透明度，决策过程不清晰。大多数可解释AI技术侧重技术输出，忽略用户角色和理解需求。", "method": "提出一个混合框架，整合传统可解释AI技术、生成式AI模型和用户个性化，以生成多模态、个性化的解释。将可解释性重新定义为针对用户角色和学习目标的动态沟通过程。", "result": "概述了框架设计、教育领域XAI的主要局限性，并提出了关于准确性、公平性和个性化的研究方向。", "conclusion": "旨在实现增强透明度并支持以用户为中心体验的可解释AI，强调解释应是个性化和动态的沟通过程。"}}
{"id": "2508.00600", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00600", "abs": "https://arxiv.org/abs/2508.00600", "authors": ["Mingruo Yuan", "Shuyi Zhang", "Ben Kao"], "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models", "comment": null, "summary": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.", "AI": {"tldr": "CRUX是一个新的LLM置信度估计框架，通过结合上下文忠实度和一致性来提高准确性。", "motivation": "现有的大语言模型置信度估计方法忽略了响应与上下文信息之间的相关性，这在评估输出质量（尤其是在提供背景知识的场景中）时是一个关键因素，导致模型在安全关键应用中部署的可靠性不足。", "method": "本文提出了CRUX框架，它首次整合了上下文忠实度和一致性来进行置信度估计。CRUX引入了两个新颖的度量：1) 上下文熵减（Contextual Entropy Reduction），通过有无上下文的对比采样来表示数据不确定性的信息增益；2) 统一一致性检查（Unified Consistency Examination），通过生成答案在有无上下文情况下的全局一致性来捕捉潜在的模型不确定性。", "result": "在三个基准数据集（CoQA、SQuAD、QuAC）和两个领域特定数据集（BioASQ、EduQG）上的实验表明，CRUX的有效性优于现有基线，取得了最高的AUROC。", "conclusion": "CRUX通过整合上下文忠实度和一致性，显著提高了大语言模型置信度估计的准确性，使其在各种应用场景中更加可靠和值得信赖。"}}
{"id": "2508.00319", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00319", "abs": "https://arxiv.org/abs/2508.00319", "authors": ["Sunghyun Park", "Seokeon Choi", "Hyoungwoo Park", "Sungrack Yun"], "title": "Steering Guidance for Personalized Text-to-Image Diffusion Models", "comment": "ICCV 2025", "summary": "Personalizing text-to-image diffusion models is crucial for adapting the\npre-trained models to specific target concepts, enabling diverse image\ngeneration. However, fine-tuning with few images introduces an inherent\ntrade-off between aligning with the target distribution (e.g., subject\nfidelity) and preserving the broad knowledge of the original model (e.g., text\neditability). Existing sampling guidance methods, such as classifier-free\nguidance (CFG) and autoguidance (AG), fail to effectively guide the output\ntoward well-balanced space: CFG restricts the adaptation to the target\ndistribution, while AG compromises text alignment. To address these\nlimitations, we propose personalization guidance, a simple yet effective method\nleveraging an unlearned weak model conditioned on a null text prompt. Moreover,\nour method dynamically controls the extent of unlearning in a weak model\nthrough weight interpolation between pre-trained and fine-tuned models during\ninference. Unlike existing guidance methods, which depend solely on guidance\nscales, our method explicitly steers the outputs toward a balanced latent space\nwithout additional computational overhead. Experimental results demonstrate\nthat our proposed guidance can improve text alignment and target distribution\nfidelity, integrating seamlessly with various fine-tuning strategies.", "AI": {"tldr": "提出个性化引导方法，通过利用一个未学习的弱模型和动态权重插值，解决了少量图像微调扩散模型时，目标一致性与文本可编辑性之间的权衡问题，提高了生成图像的文本对齐性和目标分布保真度。", "motivation": "现有文本到图像扩散模型在少量图像微调时，面临目标概念一致性（如主体保真度）与模型通用知识（如文本可编辑性）之间的固有权衡。现有采样引导方法（如CFG和AG）未能有效平衡这两者，CFG限制了目标分布适应性，而AG则损害了文本对齐。", "method": "提出“个性化引导”方法，该方法利用一个以空文本提示为条件的未学习弱模型。此外，通过在推理过程中对预训练模型和微调模型进行权重插值，动态控制弱模型的“未学习”程度，从而显式地将输出引导至平衡的潜在空间。", "result": "实验结果表明，所提出的引导方法可以提高文本对齐性和目标分布保真度，并能与各种微调策略无缝集成，且不增加额外计算开销。", "conclusion": "本文提出的个性化引导方法有效解决了少量图像微调扩散模型时，目标概念一致性与模型通用知识之间的权衡问题，在不增加计算成本的前提下，显著提升了生成图像的文本对齐性和目标分布保真度。"}}
{"id": "2508.00674", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00674", "abs": "https://arxiv.org/abs/2508.00674", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "comment": null, "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "AI": {"tldr": "本文提出一个用户分段和情境感知的视觉解释系统，旨在为社交媒体AI推荐提供多样化、适应用户需求和上下文的解释，以提高用户理解和信任。", "motivation": "当前社交媒体的AI推荐缺乏解释性，或解释过于通用，未能满足用户特定需求，导致用户不理解推荐原因，从而降低了推荐的价值和用户体验。", "method": "提出一个视觉解释系统，包含多种解释方法。该系统根据用户需求和情境调整解释风格（视觉或数字）和粒度（专家级或普通用户级），例如为AI专家提供技术细节，为普通用户提供简化版本。这是首个在一个管道中联合调整解释风格和粒度的框架。计划通过对30名X平台用户进行公开试点验证其对决策和信任的影响。", "result": "作为一个愿景论文，主要成果是提出了一个创新的框架，该框架首次在一个单一流程中联合适应解释风格（视觉 vs. 数字）和粒度（专家 vs. 普通用户）。计划进行的试点研究将验证其对用户决策和信任的影响。", "conclusion": "通过提供用户分段和情境感知的视觉解释，本系统旨在解决AI推荐解释性不足的问题，从而提高用户对推荐的理解、促进决策并增强信任，最终改善社交媒体的用户体验。"}}
{"id": "2508.00605", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00605", "abs": "https://arxiv.org/abs/2508.00605", "authors": ["Farhana Haque", "Md. Abdur Rahman", "Sumon Ahmed"], "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language", "comment": null, "summary": "Topic modeling is a Natural Language Processing (NLP) technique that is used\nto identify latent themes and extract topics from text corpora by grouping\nsimilar documents based on their most significant keywords. Although widely\nresearched in English, topic modeling remains understudied in Bengali due to\nits morphological complexity, lack of adequate resources and initiatives. In\nthis contribution, a novel Graph Convolutional Network (GCN) based model called\nGHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input\nvectors of documents as nodes in the graph, which GCN uses to produce\nsemantically rich embeddings. The embeddings are then decomposed using\nNon-negative Matrix Factorization (NMF) to get the topical representations of\nthe underlying themes of the text corpus. This study compares the proposed\nmodel against a wide range of Bengali topic modeling techniques, from\ntraditional methods such as LDA, LSA, and NMF to contemporary frameworks such\nas BERTopic and Top2Vec on three Bengali datasets. The experimental results\ndemonstrate the effectiveness of the proposed model by outperforming other\nmodels in topic coherence and diversity. In addition, we introduce a novel\nBengali dataset called \"NCTBText\" sourced from Bengali textbook materials to\nenrich and diversify the predominantly newspaper-centric Bengali corpora.", "AI": {"tldr": "本文提出了一种基于图卷积网络（GCN）的新型混合主题模型GHTM，用于孟加拉语主题建模，通过结合GCN和NMF提高主题的连贯性和多样性，并引入了一个新的孟加拉语数据集。", "motivation": "主题建模在英语中已被广泛研究，但在孟加拉语中却因其形态复杂性、资源匮乏和缺乏主动性而研究不足。", "method": "提出GHTM（Graph-Based Hybrid Topic Model），将文档输入向量表示为图中的节点，GCN生成语义丰富的嵌入，然后使用非负矩阵分解（NMF）分解这些嵌入以获得主题表示。该模型与LDA、LSA、NMF、BERTopic和Top2Vec等传统及现代孟加拉语主题建模技术在三个孟加拉语数据集上进行了比较。此外，还引入了一个名为“NCTBText”的新孟加拉语数据集。", "result": "实验结果表明，所提出的GHTM模型在主题连贯性和多样性方面优于其他模型。", "conclusion": "GHTM模型有效提升了孟加拉语主题建模的性能，解决了该领域研究不足的问题，并且新引入的NCTBText数据集丰富了现有的孟加拉语语料库。"}}
{"id": "2508.00330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00330", "abs": "https://arxiv.org/abs/2508.00330", "authors": ["Lilika Makabe", "Hiroaki Santo", "Fumio Okura", "Michael S. Brown", "Yasuyuki Matsushita"], "title": "Spectral Sensitivity Estimation with an Uncalibrated Diffraction Grating", "comment": null, "summary": "This paper introduces a practical and accurate calibration method for camera\nspectral sensitivity using a diffraction grating. Accurate calibration of\ncamera spectral sensitivity is crucial for various computer vision tasks,\nincluding color correction, illumination estimation, and material analysis.\nUnlike existing approaches that require specialized narrow-band filters or\nreference targets with known spectral reflectances, our method only requires an\nuncalibrated diffraction grating sheet, readily available off-the-shelf. By\ncapturing images of the direct illumination and its diffracted pattern through\nthe grating sheet, our method estimates both the camera spectral sensitivity\nand the diffraction grating parameters in a closed-form manner. Experiments on\nsynthetic and real-world data demonstrate that our method outperforms\nconventional reference target-based methods, underscoring its effectiveness and\npracticality.", "AI": {"tldr": "本文提出了一种使用衍射光栅对相机光谱灵敏度进行校准的实用且精确的方法。", "motivation": "相机光谱灵敏度的精确校准对于颜色校正、光照估计和材料分析等多种计算机视觉任务至关重要。现有方法需要专门的窄带滤光片或已知光谱反射率的参考目标，操作复杂。", "method": "该方法仅需一个未校准的市售衍射光栅片。通过捕获直接照明及其通过光栅片衍射图案的图像，以闭合形式估计相机光谱灵敏度和衍射光栅参数。", "result": "在合成数据和真实世界数据上的实验表明，该方法优于传统的基于参考目标的方法。", "conclusion": "该方法有效且实用，能够准确校准相机光谱灵敏度。"}}
{"id": "2508.00784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00784", "abs": "https://arxiv.org/abs/2508.00784", "authors": ["Tom Or", "Omri Azencot"], "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics", "comment": null, "summary": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.", "AI": {"tldr": "本文提出利用大型预训练多模态模型的潜在编码来检测生成内容，实现了跨模态的通用伪造检测，性能优于或匹敌现有方法。", "motivation": "生成模型（如图像、文本）被恶意用户利用来传播虚假信息和深度伪造，现有伪造检测工具通常只能在同类生成器和数据模态内泛化，对其他生成类别和数据领域效果不佳，因此迫切需要鲁棒且稳定的通用伪造检测器。", "method": "作者提出使用大型预训练多模态模型的潜在编码来检测生成内容。他们发现这些模型的潜在编码能自然地捕获区分真实和伪造信息。在此基础上，他们训练线性分类器，利用这些特征在不同模态上进行伪造检测。", "result": "在线性分类器训练后，该方法在多种模态（主要关注音频和图像）上实现了最先进的检测结果，同时计算效率高、训练速度快，即使在少样本设置下也表现出色，性能超越或匹敌强大的基线方法。", "conclusion": "大型预训练多模态模型的潜在编码能够有效捕获真实与伪造信息之间的差异，基于这些特征训练的线性分类器可以作为一种通用、高效且强大的方法，用于跨模态的生成内容检测。"}}
{"id": "2508.00614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00614", "abs": "https://arxiv.org/abs/2508.00614", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?", "comment": null, "summary": "This is the third in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate two commonly held\nprompting beliefs: a) offering to tip the AI model and b) threatening the AI\nmodel. Tipping was a commonly shared tactic for improving AI performance and\nthreats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,\n8:20) who observed that 'models tend to do better if you threaten them,' a\nclaim we subject to empirical testing here. We evaluate model performance on\nGPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).\n  We demonstrate two things:\n  - Threatening or tipping a model generally has no significant effect on\nbenchmark performance.\n  - Prompt variations can significantly affect performance on a per-question\nlevel. However, it is hard to know in advance whether a particular prompting\napproach will help or harm the LLM's ability to answer any particular question.\n  Taken together, this suggests that simple prompting variations might not be\nas effective as previously assumed, especially for difficult problems. However,\nas reported previously (Meincke et al. 2025a), prompting approaches can yield\nsignificantly different results for individual questions.", "AI": {"tldr": "研究表明，向AI模型提供小费或威胁它们通常不会显著提高其基准性能，尽管提示词的微小变化可能对单个问题产生不可预测的影响。", "motivation": "业界普遍流传两种提升AI模型性能的提示词策略：提供小费和威胁模型（后者甚至得到谷歌创始人谢尔盖·布林的认可）。本研究旨在通过严格测试，验证这些策略的有效性。", "method": "通过在GPQA和MMLU-Pro基准数据集上进行实证测试，评估提供小费或威胁AI模型对模型性能的影响。", "result": "研究发现，威胁或提供小费给AI模型通常不会显著影响其基准性能。尽管提示词的细微变化可能在单个问题层面显著影响性能，但很难预先判断某种特定提示方法会帮助还是损害大型语言模型回答特定问题的能力。", "conclusion": "综合来看，这表明简单的提示词变体（如提供小费或威胁）可能不像之前假设的那样有效，尤其是在解决难题时。然而，对于个别问题，不同的提示方法确实可能产生显著不同的结果。"}}
{"id": "2508.00356", "categories": ["cs.CV", "cs.MA", "I.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.00356", "abs": "https://arxiv.org/abs/2508.00356", "authors": ["Angelos Vlachos", "Giorgos Filandrianos", "Maria Lymperaiou", "Nikolaos Spanos", "Ilias Mitsouras", "Vasileios Karampinis", "Athanasios Voulodimos"], "title": "Analyze-Prompt-Reason: A Collaborative Agent-Based Framework for Multi-Image Vision-Language Reasoning", "comment": null, "summary": "We present a Collaborative Agent-Based Framework for Multi-Image Reasoning.\nOur approach tackles the challenge of interleaved multimodal reasoning across\ndiverse datasets and task formats by employing a dual-agent system: a\nlanguage-based PromptEngineer, which generates context-aware, task-specific\nprompts, and a VisionReasoner, a large vision-language model (LVLM) responsible\nfor final inference. The framework is fully automated, modular, and\ntraining-free, enabling generalization across classification, question\nanswering, and free-form generation tasks involving one or multiple input\nimages. We evaluate our method on 18 diverse datasets from the 2025 MIRAGE\nChallenge (Track A), covering a broad spectrum of visual reasoning tasks\nincluding document QA, visual comparison, dialogue-based understanding, and\nscene-level inference. Our results demonstrate that LVLMs can effectively\nreason over multiple images when guided by informative prompts. Notably, Claude\n3.7 achieves near-ceiling performance on challenging tasks such as TQA (99.13%\naccuracy), DocVQA (96.87%), and MMCoQA (75.28 ROUGE-L). We also explore how\ndesign choices-such as model selection, shot count, and input length-influence\nthe reasoning performance of different LVLMs.", "AI": {"tldr": "该论文提出了一个协作式代理框架，通过PromptEngineer生成提示并由大型视觉-语言模型（LVLM）进行推理，以实现多图像推理，该框架无需训练，并在多种视觉推理任务上取得了优异性能。", "motivation": "当前研究面临在多样数据集和任务格式下进行交错多模态推理的挑战，需要一个能够有效处理多图像输入的通用推理框架。", "method": "该方法采用双代理系统：一个基于语言的PromptEngineer，负责生成上下文感知和任务特定的提示；一个VisionReasoner，即一个大型视觉-语言模型（LVLM），负责最终的推理。该框架是全自动、模块化且无需训练的，能泛化到涉及单张或多张输入图像的分类、问答和自由生成任务。", "result": "研究结果表明，在信息丰富提示的引导下，LVLM能有效进行多图像推理。在2025 MIRAGE挑战赛（Track A）的18个多样数据集上进行评估，Claude 3.7在TQA（99.13%准确率）、DocVQA（96.87%）和MMCoQA（75.28 ROUGE-L）等挑战性任务上取得了接近上限的性能。此外，还探讨了模型选择、少样本数量和输入长度等设计选择对不同LVLM推理性能的影响。", "conclusion": "LVLM在信息丰富的提示引导下，能够有效地进行多图像推理，并在广泛的视觉推理任务中表现出色。所提出的协作式代理框架为处理多图像推理提供了一个有效、无需训练且具有良好泛化能力的解决方案。"}}
{"id": "2502.18148", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2502.18148", "abs": "https://arxiv.org/abs/2502.18148", "authors": ["Muhammad Farid Adilazuarda", "Musa Izzanardi Wijanarko", "Lucky Susanto", "Khumaisa Nur'aini", "Derry Wijaya", "Alham Fikri Aji"], "title": "NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts", "comment": null, "summary": "Indonesia is rich in languages and scripts. However, most NLP progress has\nbeen made using romanized text. In this paper, we present NusaAksara, a novel\npublic benchmark for Indonesian languages that includes their original scripts.\nOur benchmark covers both text and image modalities and encompasses diverse\ntasks such as image segmentation, OCR, transliteration, translation, and\nlanguage identification. Our data is constructed by human experts through\nrigorous steps. NusaAksara covers 8 scripts across 7 languages, including\nlow-resource languages not commonly seen in NLP benchmarks. Although\nunsupported by Unicode, the Lampung script is included in this dataset. We\nbenchmark our data across several models, from LLMs and VLMs such as GPT-4o,\nLlama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and\nshow that most NLP technologies cannot handle Indonesia's local scripts, with\nmany achieving near-zero performance.", "AI": {"tldr": "本文提出了NusaAksara，一个针对印尼语种及其原始文字的公共基准测试数据集，涵盖文本和图像模态及多种任务，并揭示现有NLP技术对印尼本地文字处理能力极差。", "motivation": "印尼拥有丰富的语言和文字，但当前的自然语言处理（NLP）进展主要集中在罗马化文本上，忽视了原始文字，且许多低资源语言未被常见NLP基准覆盖。这促使研究者创建一个包含原始文字的综合基准。", "method": "通过人类专家严格构建了NusaAksara数据集，涵盖7种语言的8种文字（包括未被Unicode支持的Lampung文字）。该基准包括文本和图像模态，并涵盖图像分割、OCR、音译、翻译和语言识别等任务。研究人员使用GPT-4o、Llama 3.2、Aya 23等大型语言模型和视觉语言模型，以及PP-OCR和LangID等特定任务系统对数据进行了基准测试。", "result": "基准测试结果显示，大多数现有的NLP技术无法有效处理印尼的本地文字，许多模型在此任务上表现接近于零。", "conclusion": "当前NLP技术在处理印尼原始文字方面存在显著不足，亟需进一步的研究和开发来提升对这些低资源语言和文字的处理能力。"}}
{"id": "2508.00619", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00619", "abs": "https://arxiv.org/abs/2508.00619", "authors": ["Shantanu Thorat", "Andrew Caines"], "title": "DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models", "comment": "MPhil in Advanced Computer Science thesis for University of Cambridge", "summary": "Existing AIG (AI-generated) text detectors struggle in real-world settings\ndespite succeeding in internal testing, suggesting that they may not be robust\nenough. We rigorously examine the machine-learning procedure to build these\ndetectors to address this. Most current AIG text detection datasets focus on\nzero-shot generations, but little work has been done on few-shot or one-shot\ngenerations, where LLMs are given human texts as an example. In response, we\nintroduce the Diverse Adversarial Corpus of Texts Yielded from Language models\n(DACTYL), a challenging AIG text detection dataset focusing on\none-shot/few-shot generations. We also include texts from domain-specific\ncontinued-pre-trained (CPT) language models, where we fully train all\nparameters using a memory-efficient optimization approach. Many existing AIG\ntext detectors struggle significantly on our dataset, indicating a potential\nvulnerability to one-shot/few-shot and CPT-generated texts. We also train our\nown classifiers using two approaches: standard binary cross-entropy (BCE)\noptimization and a more recent approach, deep X-risk optimization (DXO). While\nBCE-trained classifiers marginally outperform DXO classifiers on the DACTYL\ntest set, the latter excels on out-of-distribution (OOD) texts. In our mock\ndeployment scenario in student essay detection with an OOD student essay\ndataset, the best DXO classifier outscored the best BCE-trained classifier by\n50.56 macro-F1 score points at the lowest false positive rates for both. Our\nresults indicate that DXO classifiers generalize better without overfitting to\nthe test set. Our experiments highlight several areas of improvement for AIG\ntext detectors.", "AI": {"tldr": "现有AIG文本检测器在实际应用中表现不佳，原因在于它们缺乏鲁棒性，尤其是在少样本/单样本生成和特定领域微调模型上。本文提出了DACTYL数据集来应对这一挑战，并发现深度X风险优化（DXO）训练的分类器在域外（OOD）文本检测方面表现出更好的泛化能力。", "motivation": "现有AI生成文本检测器在内部测试中表现良好，但在真实世界应用中却力不从心，表明其鲁棒性不足。大多数数据集侧重于零样本生成，而少样本或单样本生成（LLM以人类文本为例）以及特定领域持续预训练（CPT）模型的文本检测研究不足。", "method": "引入了DACTYL数据集，专注于单样本/少样本生成文本，并包含来自领域特定CPT语言模型的文本。使用两种方法训练分类器：标准二元交叉熵（BCE）优化和深度X风险优化（DXO）。", "result": "现有AIG文本检测器在DACTYL数据集上表现显著不佳。BCE训练的分类器在DACTYL测试集上略优于DXO分类器，但DXO分类器在域外（OOD）文本上表现出色。在学生论文检测的模拟部署场景中，最佳DXO分类器在最低误报率下比最佳BCE训练分类器高出50.56的宏F1分数。", "conclusion": "DXO分类器在不发生过拟合的情况下表现出更好的泛化能力，尤其是在处理OOD文本时。本研究揭示了AIG文本检测器在单样本/少样本和CPT生成文本方面的潜在脆弱性，并指出了未来改进的方向。"}}
{"id": "2508.00358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00358", "abs": "https://arxiv.org/abs/2508.00358", "authors": ["Yan Gong", "Mengjun Chen", "Hao Liu", "Gao Yongsheng", "Lei Yang", "Naibang Wang", "Ziying Song", "Haoqun Ma"], "title": "Stable at Any Speed: Speed-Driven Multi-Object Tracking with Learnable Kalman Filtering", "comment": "9 pages, 7 figures, 5 tables", "summary": "Multi-object tracking (MOT) enables autonomous vehicles to continuously\nperceive dynamic objects, supplying essential temporal cues for prediction,\nbehavior understanding, and safe planning. However, conventional\ntracking-by-detection methods typically rely on static coordinate\ntransformations based on ego-vehicle poses, disregarding ego-vehicle\nspeed-induced variations in observation noise and reference frame changes,\nwhich degrades tracking stability and accuracy in dynamic, high-speed\nscenarios. In this paper, we investigate the critical role of ego-vehicle speed\nin MOT and propose a Speed-Guided Learnable Kalman Filter (SG-LKF) that\ndynamically adapts uncertainty modeling to ego-vehicle speed, significantly\nimproving stability and accuracy in highly dynamic scenarios. Central to SG-LKF\nis MotionScaleNet (MSNet), a decoupled token-mixing and channel-mixing MLP that\nadaptively predicts key parameters of SG-LKF. To enhance inter-frame\nassociation and trajectory continuity, we introduce a self-supervised\ntrajectory consistency loss jointly optimized with semantic and positional\nconstraints. Extensive experiments show that SG-LKF ranks first among all\nvision-based methods on KITTI 2D MOT with 79.59% HOTA, delivers strong results\non KITTI 3D MOT with 82.03% HOTA, and outperforms SimpleTrack by 2.2% AMOTA on\nnuScenes 3D MOT.", "AI": {"tldr": "该论文提出了一种名为SG-LKF（Speed-Guided Learnable Kalman Filter）的多目标跟踪（MOT）方法，通过动态适应自车速度来建模不确定性，显著提高了高动态场景下的跟踪稳定性和准确性。", "motivation": "传统的多目标跟踪方法通常依赖于基于自车姿态的静态坐标变换，忽略了自车速度引起的观测噪声变化和参考系转换，这导致在动态、高速场景下跟踪稳定性与准确性下降。", "method": "该研究提出SG-LKF，它能动态地根据自车速度调整不确定性建模。SG-LKF的核心是MotionScaleNet（MSNet），一个解耦的token-mixing和channel-mixing MLP，用于自适应地预测SG-LKF的关键参数。此外，引入了一种自监督轨迹一致性损失，与语义和位置约束共同优化，以增强帧间关联和轨迹连续性。", "result": "实验结果显示，SG-LKF在KITTI 2D MOT上以79.59% HOTA排名所有视觉方法第一，在KITTI 3D MOT上取得了82.03% HOTA的优异结果，并在nuScenes 3D MOT上比SimpleTrack的AMOTA高出2.2%。", "conclusion": "SG-LKF通过考虑并动态适应自车速度对观测不确定性的影响，显著提升了多目标跟踪在高速动态场景下的稳定性和准确性，并在多个基准测试中取得了领先性能。"}}
{"id": "2508.00669", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00669", "abs": "https://arxiv.org/abs/2508.00669", "authors": ["Wenxuan Wang", "Zizhan Ma", "Meidan Ding", "Shiyi Zheng", "Shengyuan Liu", "Jie Liu", "Jiaming Ji", "Wenting Chen", "Xiang Li", "Linlin Shen", "Yixuan Yuan"], "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications", "comment": null, "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled\nimpressive capabilities, yet a critical gap remains in their ability to perform\nsystematic, transparent, and verifiable reasoning, a cornerstone of clinical\npractice. This has catalyzed a shift from single-step answer generation to the\ndevelopment of LLMs explicitly designed for medical reasoning. This paper\nprovides the first systematic review of this emerging field. We propose a\ntaxonomy of reasoning enhancement techniques, categorized into training-time\nstrategies (e.g., supervised fine-tuning, reinforcement learning) and test-time\nmechanisms (e.g., prompt engineering, multi-agent systems). We analyze how\nthese techniques are applied across different data modalities (text, image,\ncode) and in key clinical applications such as diagnosis, education, and\ntreatment planning. Furthermore, we survey the evolution of evaluation\nbenchmarks from simple accuracy metrics to sophisticated assessments of\nreasoning quality and visual interpretability. Based on an analysis of 60\nseminal studies from 2022-2025, we conclude by identifying critical challenges,\nincluding the faithfulness-plausibility gap and the need for native multimodal\nreasoning, and outlining future directions toward building efficient, robust,\nand sociotechnically responsible medical AI.", "AI": {"tldr": "本文首次对医学领域中旨在增强推理能力的LLM进行了系统综述，提出了推理增强技术的分类法，并分析了其应用、评估基准及未来挑战。", "motivation": "尽管大型语言模型（LLMs）在医学领域展现出强大能力，但其在执行系统、透明和可验证推理方面的不足是一个关键缺陷，而这正是临床实践的基石。这促使了专门为医学推理设计的LLM的发展。", "method": "本文采用系统综述的方法，分析了2022-2025年间的60项重要研究。研究提出了一种推理增强技术的分类法，分为训练时策略（如监督微调、强化学习）和测试时机制（如提示工程、多智能体系统）。同时，分析了这些技术在不同数据模态（文本、图像、代码）和关键临床应用（如诊断、教育、治疗规划）中的应用，并调查了评估基准的演变。", "result": "研究提出了医学LLM推理增强技术的分类法，详细分析了这些技术如何应用于不同数据模态和临床场景，并展示了评估基准从简单准确性指标向更复杂的推理质量和视觉可解释性评估的演变。", "conclusion": "研究指出了当前面临的关键挑战，包括忠实性-合理性鸿沟以及对原生多模态推理的需求，并展望了构建高效、鲁棒且对社会技术负责的医学AI的未来方向。"}}
{"id": "2508.00359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00359", "abs": "https://arxiv.org/abs/2508.00359", "authors": ["Zongheng Tang", "Yi Liu", "Yifan Sun", "Yulu Gao", "Jinyu Chen", "Runsheng Xu", "Si Liu"], "title": "CoST: Efficient Collaborative Perception From Unified Spatiotemporal Perspective", "comment": "ICCV25 (Highlight)", "summary": "Collaborative perception shares information among different agents and helps\nsolving problems that individual agents may face, e.g., occlusions and small\nsensing range. Prior methods usually separate the multi-agent fusion and\nmulti-time fusion into two consecutive steps. In contrast, this paper proposes\nan efficient collaborative perception that aggregates the observations from\ndifferent agents (space) and different times into a unified spatio-temporal\nspace simultanesouly. The unified spatio-temporal space brings two benefits,\ni.e., efficient feature transmission and superior feature fusion. 1) Efficient\nfeature transmission: each static object yields a single observation in the\nspatial temporal space, and thus only requires transmission only once (whereas\nprior methods re-transmit all the object features multiple times). 2) superior\nfeature fusion: merging the multi-agent and multi-time fusion into a unified\nspatial-temporal aggregation enables a more holistic perspective, thereby\nenhancing perception performance in challenging scenarios. Consequently, our\nCollaborative perception with Spatio-temporal Transformer (CoST) gains\nimprovement in both efficiency and accuracy. Notably, CoST is not tied to any\nspecific method and is compatible with a majority of previous methods,\nenhancing their accuracy while reducing the transmission bandwidth.", "AI": {"tldr": "本文提出CoST，一种高效协同感知框架，将多智能体和多时间观测统一到时空域同时融合，提高感知精度并减少数据传输。", "motivation": "现有协同感知方法将多智能体融合和多时间融合分开处理，导致效率低下（重复传输）和融合效果不佳。此外，单个智能体面临遮挡和感知范围有限等问题。", "method": "本文提出CoST（Collaborative perception with Spatio-temporal Transformer），将来自不同智能体（空间）和不同时间（时间）的观测同时聚合到一个统一的时空域。这种方法带来两个优势：1）高效特征传输：每个静态物体在时空域中只需一次观测，因此只需传输一次；2）优越特征融合：将多智能体和多时间融合合并为统一的时空聚合，提供更全面的视角。", "result": "CoST在效率（减少传输带宽）和准确性（在挑战性场景中增强感知性能）方面均有所提升。CoST不局限于特定方法，兼容大多数现有方法，在提高其准确性的同时降低传输带宽。", "conclusion": "统一的时空聚合是协同感知的有效方法，能显著提升效率和准确性，并且具有广泛的兼容性，可应用于多数现有方法。"}}
{"id": "2508.00381", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00381", "abs": "https://arxiv.org/abs/2508.00381", "authors": ["Kamal Basha S", "Athira Nambiar"], "title": "Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis", "comment": null, "summary": "Weld defect detection is crucial for ensuring the safety and reliability of\npiping systems in the oil and gas industry, especially in challenging marine\nand offshore environments. Traditional non-destructive testing (NDT) methods\noften fail to detect subtle or internal defects, leading to potential failures\nand costly downtime. Furthermore, existing neural network-based approaches for\ndefect classification frequently rely on arbitrarily selected pretrained\narchitectures and lack interpretability, raising safety concerns for\ndeployment. To address these challenges, this paper introduces\n``Adapt-WeldNet\", an adaptive framework for welding defect detection that\nsystematically evaluates various pre-trained architectures, transfer learning\nstrategies, and adaptive optimizers to identify the best-performing model and\nhyperparameters, optimizing defect detection and providing actionable insights.\nAdditionally, a novel Defect Detection Interpretability Analysis (DDIA)\nframework is proposed to enhance system transparency. DDIA employs Explainable\nAI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific\nevaluations validated by certified ASNT NDE Level II professionals.\nIncorporating a Human-in-the-Loop (HITL) approach and aligning with the\nprinciples of Trustworthy AI, DDIA ensures the reliability, fairness, and\naccountability of the defect detection system, fostering confidence in\nautomated decisions through expert validation. By improving both performance\nand interpretability, this work enhances trust, safety, and reliability in\nwelding defect detection systems, supporting critical operations in offshore\nand marine environments.", "AI": {"tldr": "本文提出“Adapt-WeldNet”自适应框架，系统优化焊缝缺陷检测模型性能；并引入“缺陷检测可解释性分析 (DDIA)”框架，结合可解释AI和专家验证，提升系统透明度、可信度和安全性。", "motivation": "传统无损检测方法难以发现细微或内部焊缝缺陷，导致潜在故障和停机；现有基于神经网络的缺陷分类方法依赖任意选择的预训练架构且缺乏可解释性，存在部署安全隐患。", "method": "1. 提出“Adapt-WeldNet”自适应框架，系统评估各种预训练架构、迁移学习策略和自适应优化器，以识别最佳模型和超参数，优化缺陷检测。2. 提出“缺陷检测可解释性分析 (DDIA)”框架，利用Grad-CAM和LIME等可解释人工智能 (XAI) 技术，并结合ASNT NDE二级专业人员的领域特定验证。3. 整合“人在回路 (HITL)”方法，遵循可信AI原则，确保系统可靠性、公平性和问责制。", "result": "通过系统优化模型和引入可解释性分析，提升了焊缝缺陷检测系统的性能和可解释性，增强了系统的信任度、安全性和可靠性。", "conclusion": "本工作通过提高性能和可解释性，增强了焊缝缺陷检测系统的信任度、安全性和可靠性，为海上和海洋环境中的关键作业提供了支持。"}}
{"id": "2508.00673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00673", "abs": "https://arxiv.org/abs/2508.00673", "authors": ["Farhan Farsi", "Farnaz Aghababaloo", "Shahriar Shariati Motlagh", "Parsa Ghofrani", "MohammadAli SadraeiJavaheri", "Shayan Bali", "Amirhossein Shabani", "Farbod Bijary", "Ghazal Zamaninejad", "AmirMohammad Salehoof", "Saeedeh Momtazi"], "title": "MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language", "comment": "Preprint. Under review", "summary": "As large language models (LLMs) become increasingly embedded in our daily\nlives, evaluating their quality and reliability across diverse contexts has\nbecome essential. While comprehensive benchmarks exist for assessing LLM\nperformance in English, there remains a significant gap in evaluation resources\nfor other languages. Moreover, because most LLMs are trained primarily on data\nrooted in European and American cultures, they often lack familiarity with\nnon-Western cultural contexts. To address this limitation, our study focuses on\nthe Persian language and Iranian culture. We introduce 19 new evaluation\ndatasets specifically designed to assess LLMs on topics such as Iranian law,\nPersian grammar, Persian idioms, and university entrance exams. Using these\ndatasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing\ncultural and linguistic evaluation gap in the field.", "AI": {"tldr": "本研究针对波斯语和伊朗文化，创建了19个新的评估数据集，并对41个大型语言模型进行了基准测试，以弥补非英语和非西方文化背景下的LLM评估空白。", "motivation": "现有LLM评估基准主要针对英语，且大多数LLM训练数据源于欧美文化，导致其在其他语言和非西方文化背景下表现不佳或缺乏评估资源。", "method": "研究团队为波斯语和伊朗文化设计并引入了19个新的评估数据集，涵盖伊朗法律、波斯语法、波斯习语和大学入学考试等主题。随后，使用这些数据集对41个主流大型语言模型进行了基准测试。", "result": "通过创建和使用这些新数据集，成功对大型语言模型在波斯语和伊朗文化背景下的表现进行了评估，从而弥合了当前领域中存在的文化和语言评估差距。", "conclusion": "新引入的波斯语和伊朗文化评估数据集为评估LLM在特定非西方语言和文化背景下的质量和可靠性提供了重要资源，有助于提升LLM的普适性和文化适应性。"}}
{"id": "2508.00361", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00361", "abs": "https://arxiv.org/abs/2508.00361", "authors": ["Mokhtar A. Al-Awadhi", "Ratnadeep R. Deshmukh"], "title": "Honey Classification using Hyperspectral Imaging and Machine Learning", "comment": null, "summary": "In this paper, we propose a machine learning-based method for automatically\nclassifying honey botanical origins. Dataset preparation, feature extraction,\nand classification are the three main steps of the proposed method. We use a\nclass transformation method in the dataset preparation phase to maximize the\nseparability across classes. The feature extraction phase employs the Linear\nDiscriminant Analysis (LDA) technique for extracting relevant features and\nreducing the number of dimensions. In the classification phase, we use Support\nVector Machines (SVM) and K-Nearest Neighbors (KNN) models to classify the\nextracted features of honey samples into their botanical origins. We evaluate\nour system using a standard honey hyperspectral imaging (HSI) dataset.\nExperimental findings demonstrate that the proposed system produces\nstate-of-the-art results on this dataset, achieving the highest classification\naccuracy of 95.13% for hyperspectral image-based classification and 92.80% for\nhyperspectral instance-based classification.", "AI": {"tldr": "本文提出了一种基于机器学习的蜂蜜植物来源自动分类方法，利用高光谱图像数据，通过数据准备、LDA特征提取和SVM/KNN分类，取得了最先进的分类准确率。", "motivation": "旨在开发一种自动分类蜂蜜植物来源的方法，以解决蜂蜜溯源和质量控制的需求。", "method": "该方法包含三个主要步骤：1) 数据集准备阶段，使用类转换方法最大化类别可分离性；2) 特征提取阶段，采用线性判别分析（LDA）提取相关特征并降维；3) 分类阶段，使用支持向量机（SVM）和K-近邻（KNN）模型对提取的特征进行分类。系统使用标准蜂蜜高光谱成像（HSI）数据集进行评估。", "result": "实验结果表明，该系统在高光谱图像分类中实现了95.13%的最高分类准确率，在高光谱实例分类中实现了92.80%的最高分类准确率，在该数据集上取得了最先进的成果。", "conclusion": "所提出的机器学习系统能够有效且高精度地自动分类蜂蜜的植物来源，并在高光谱数据集上达到了最先进的性能水平。"}}
{"id": "2508.00383", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00383", "abs": "https://arxiv.org/abs/2508.00383", "authors": ["Won June Cho", "Hongjun Yoon", "Daeky Jeong", "Hyeongyeol Lim", "Yosep Chong"], "title": "$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models", "comment": "Accepted (Oral) in MICCAI 2025 COMPAYL Workshop", "summary": "Spatial transcriptomics reveals gene expression patterns within tissue\ncontext, enabling precision oncology applications such as treatment response\nprediction, but its high cost and technical complexity limit clinical adoption.\nPredicting spatial gene expression (biomarkers) from routine histopathology\nimages offers a practical alternative, yet current vision foundation models\n(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below\nclinical standards. Given that VFMs are already trained on millions of diverse\nwhole slide images, we hypothesize that architectural innovations beyond ViTs\nmay better capture the low-frequency, subtle morphological patterns correlating\nwith molecular phenotypes. By demonstrating that state space models initialized\nwith negative real eigenvalues exhibit strong low-frequency bias, we introduce\n$MV_{Hybrid}$, a hybrid backbone architecture combining state space models\n(SSMs) with ViT. We compare five other different backbone architectures for\npathology VFMs, all pretrained on identical colorectal cancer datasets using\nthe DINOv2 self-supervised learning method. We evaluate all pretrained models\nusing both random split and leave-one-study-out (LOSO) settings of the same\nbiomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher\ncorrelation than the best-performing ViT and shows 43% smaller performance\ndegradation compared to random split in gene expression prediction,\ndemonstrating superior performance and robustness, respectively. Furthermore,\n$MV_{Hybrid}$ shows equal or better downstream performance in classification,\npatch retrieval, and survival prediction tasks compared to that of ViT, showing\nits promise as a next-generation pathology VFM backbone. Our code is publicly\navailable at: https://github.com/deepnoid-ai/MVHybrid.", "AI": {"tldr": "针对现有病理学视觉基础模型（VFMs）在基因表达预测中表现不佳的问题，本文提出了一种结合状态空间模型（SSM）和Vision Transformer（ViT）的混合骨干网络$MV_{Hybrid}$，并在实验中验证了其在性能和鲁棒性上的显著提升。", "motivation": "空间转录组学在精准肿瘤学中有重要应用，但其高成本和技术复杂性限制了临床普及。从常规组织病理学图像预测空间基因表达是一种实用的替代方案，然而，当前基于Vision Transformer（ViT）的病理学视觉基础模型（VFMs）性能尚未达到临床标准，难以有效捕捉与分子表型相关的低频、细微形态模式。", "method": "本文提出$MV_{Hybrid}$，一种结合状态空间模型（SSM）与Vision Transformer（ViT）的混合骨干架构，利用了状态空间模型在负实特征值初始化时表现出的强低频偏差特性。研究人员将$MV_{Hybrid}$与其他五种不同的病理学VFM骨干架构进行比较，所有模型均使用DINOv2自监督学习方法在相同的结直肠癌数据集上进行预训练。模型性能在生物标志物数据集上通过随机划分和留一研究出（LOSO）两种设置进行评估。", "result": "在留一研究出（LOSO）评估中，$MV_{Hybrid}$在基因表达预测方面的相关性比表现最佳的ViT高出57%，且相较于随机划分，性能下降幅度减少了43%，分别展现出卓越的性能和鲁棒性。此外，$MV_{Hybrid}$在分类、图像块检索和生存预测等下游任务中也表现出与ViT相当或更优的性能。", "conclusion": "$MV_{Hybrid}$作为一种结合了状态空间模型和Vision Transformer的混合骨干架构，在从组织病理学图像预测空间基因表达方面表现出优越的性能和鲁棒性，有望成为下一代病理学视觉基础模型的重要骨干。"}}
{"id": "2508.00675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00675", "abs": "https://arxiv.org/abs/2508.00675", "authors": ["Gleb Schmidt", "Johannes Römisch", "Mariia Halchynska", "Svetlana Gorovaia", "Ivan P. Yamshchikov"], "title": "Team \"better_call_claude\": Style Change Detection using a Sequential Sentence Pair Classifier", "comment": null, "summary": "Style change detection - identifying the points in a document where writing\nstyle shifts - remains one of the most important and challenging problems in\ncomputational authorship analysis. At PAN 2025, the shared task challenges\nparticipants to detect style switches at the most fine-grained level:\nindividual sentences. The task spans three datasets, each designed with\ncontrolled and increasing thematic variety within documents. We propose to\naddress this problem by modeling the content of each problem instance - that\nis, a series of sentences - as a whole, using a Sequential Sentence Pair\nClassifier (SSPC). The architecture leverages a pre-trained language model\n(PLM) to obtain representations of individual sentences, which are then fed\ninto a bidirectional LSTM (BiLSTM) to contextualize them within the document.\nThe BiLSTM-produced vectors of adjacent sentences are concatenated and passed\nto a multi-layer perceptron for prediction per adjacency. Building on the work\nof previous PAN participants classical text segmentation, the approach is\nrelatively conservative and lightweight. Nevertheless, it proves effective in\nleveraging contextual information and addressing what is arguably the most\nchallenging aspect of this year's shared task: the notorious problem of\n\"stylistically shallow\", short sentences that are prevalent in the proposed\nbenchmark data. Evaluated on the official PAN-2025 test datasets, the model\nachieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,\nand HARD data, respectively, outperforming not only the official random\nbaselines but also a much more challenging one: claude-3.7-sonnet's zero-shot\nperformance.", "AI": {"tldr": "该论文提出一种序列句子对分类器（SSPC），结合预训练语言模型和双向LSTM，用于在句子层面检测写作风格变化，并在PAN-2025任务中取得了优异表现。", "motivation": "写作风格变化检测是计算作者分析领域一个重要且具挑战性的问题，尤其是在细粒度（如句子层面）上。PAN 2025共享任务聚焦于此，并引入了包含主题多样性递增的数据集，其中“风格浅显”的短句是主要挑战。", "method": "本文提出一种序列句子对分类器（SSPC）。该架构首先使用预训练语言模型（PLM）获取单个句子的表示，然后将这些表示输入双向LSTM（BiLSTM）以获取文档内的上下文信息。接着，将相邻句子的BiLSTM输出向量拼接，并输入多层感知机（MLP）进行每对邻接句的风格变化预测。该方法相对轻量级，并能有效处理短句带来的挑战。", "result": "该模型在PAN-2025官方测试数据集上取得了强大的宏观F1分数：EASY数据集0.923，MEDIUM数据集0.828，HARD数据集0.724。这些结果不仅超越了官方随机基线，也优于Claude-3.7-sonnet的零样本性能。", "conclusion": "所提出的SSPC方法通过有效利用上下文信息，成功解决了细粒度风格变化检测（特别是短句）的挑战，在PAN-2025任务中展现出卓越的性能，证明了其在计算作者分析领域的有效性。"}}
{"id": "2508.00366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00366", "abs": "https://arxiv.org/abs/2508.00366", "authors": ["Liang Han", "Xu Zhang", "Haichuan Song", "Kanle Shi", "Yu-Shen Liu", "Zhizhong Han"], "title": "SparseRecon: Neural Implicit Surface Reconstruction from Sparse Views with Feature and Depth Consistencies", "comment": "Accepted by ICCV 2025", "summary": "Surface reconstruction from sparse views aims to reconstruct a 3D shape or\nscene from few RGB images. The latest methods are either generalization-based\nor overfitting-based. However, the generalization-based methods do not\ngeneralize well on views that were unseen during training, while the\nreconstruction quality of overfitting-based methods is still limited by the\nlimited geometry clues. To address this issue, we propose SparseRecon, a novel\nneural implicit reconstruction method for sparse views with volume\nrendering-based feature consistency and uncertainty-guided depth constraint.\nFirstly, we introduce a feature consistency loss across views to constrain the\nneural implicit field. This design alleviates the ambiguity caused by\ninsufficient consistency information of views and ensures completeness and\nsmoothness in the reconstruction results. Secondly, we employ an\nuncertainty-guided depth constraint to back up the feature consistency loss in\nareas with occlusion and insignificant features, which recovers geometry\ndetails for better reconstruction quality. Experimental results demonstrate\nthat our method outperforms the state-of-the-art methods, which can produce\nhigh-quality geometry with sparse-view input, especially in the scenarios with\nsmall overlapping views. Project page: https://hanl2010.github.io/SparseRecon/.", "AI": {"tldr": "SparseRecon是一种新的神经隐式重建方法，通过基于体渲染的特征一致性损失和不确定性引导的深度约束，解决了稀疏视图表面重建中现有方法的局限性，实现了高质量几何重建。", "motivation": "现有的稀疏视图表面重建方法（基于泛化或基于过拟合）在未见过的视图上泛化能力差，或受限于有限的几何线索导致重建质量不高。", "method": "本文提出SparseRecon，一种神经隐式重建方法。主要包括两点：1) 引入跨视图的特征一致性损失，以消除视图一致性信息不足造成的歧义，确保重建的完整性和平滑性。2) 采用不确定性引导的深度约束，在遮挡和特征不明显的区域辅助特征一致性损失，恢复几何细节。", "result": "实验结果表明，该方法优于现有最先进的方法，能够从稀疏视图输入生成高质量的几何形状，特别是在视图重叠度较小的场景中表现更佳。", "conclusion": "SparseRecon通过结合特征一致性和不确定性引导的深度约束，有效提高了稀疏视图下的表面重建质量，解决了现有方法在泛化性和几何细节恢复方面的不足。"}}
{"id": "2508.00395", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00395", "abs": "https://arxiv.org/abs/2508.00395", "authors": ["Fei Zhang", "Tianfei Zhou", "Jiangchao Yao", "Ya Zhang", "Ivor W. Tsang", "Yanfeng Wang"], "title": "Decouple before Align: Visual Disentanglement Enhances Prompt Tuning", "comment": "16 pages, Accepted at IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)", "summary": "Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,\nhas showcased remarkable effectiveness in improving the task-specific\ntransferability of vision-language models. This paper delves into a previously\noverlooked information asymmetry issue in PT, where the visual modality mostly\nconveys more context than the object-oriented textual modality.\nCorrespondingly, coarsely aligning these two modalities could result in the\nbiased attention, driving the model to merely focus on the context area. To\naddress this, we propose DAPT, an effective PT framework based on an intuitive\ndecouple-before-align concept. First, we propose to explicitly decouple the\nvisual modality into the foreground and background representation via\nexploiting coarse-and-fine visual segmenting cues, and then both of these\ndecoupled patterns are aligned with the original foreground texts and the\nhand-crafted background classes, thereby symmetrically strengthening the modal\nalignment. To further enhance the visual concentration, we propose a visual\npull-push regularization tailored for the foreground-background patterns,\ndirecting the original visual representation towards unbiased attention on the\nregion-of-interest object. We demonstrate the power of architecture-free DAPT\nthrough few-shot learning, base-to-novel generalization, and data-efficient\nlearning, all of which yield superior performance across prevailing benchmarks.\nOur code will be released at https://github.com/Ferenas/DAPT.", "AI": {"tldr": "本文提出DAPT，一个基于“先解耦后对齐”概念的提示微调框架，旨在解决视觉-语言模型中提示微调的信息不对称问题，通过显式解耦视觉模态并增强对称对齐和视觉集中度，实现卓越性能。", "motivation": "现有的提示微调（PT）存在信息不对称问题，即视觉模态通常比面向对象的文本模态包含更多上下文信息。这种粗略的对齐会导致模型注意力偏差，仅关注上下文区域，从而影响视觉-语言模型的任务特定迁移能力。", "method": "本文提出DAPT框架：1) 显式地将视觉模态解耦为前景和背景表示，利用粗细粒度视觉分割线索。2) 将解耦后的视觉模式与原始前景文本和手工制作的背景类别对称对齐，以加强模态对齐。3) 引入视觉拉推正则化（visual pull-push regularization），专门针对前景-背景模式，引导原始视觉表示对感兴趣区域（ROI）对象进行无偏关注。", "result": "DAPT是一个架构无关的框架，在少样本学习、从基类到新类的泛化以及数据高效学习方面均表现出卓越的性能，并在主流基准测试中取得了优异结果。", "conclusion": "DAPT成功解决了提示微调中视觉-语言模态的信息不对称问题，通过解耦和增强对齐及视觉集中度，显著提升了视觉-语言模型的性能和迁移能力。"}}
{"id": "2508.00679", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00679", "abs": "https://arxiv.org/abs/2508.00679", "authors": ["Shubham Kumar Nigam", "Tanmay Dubey", "Noel Shallum", "Arnab Bhattacharya"], "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries", "comment": null, "summary": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.", "AI": {"tldr": "TraceRetriever是一种法律判例检索系统，旨在应对海量法律文档挑战，通过有限的案件信息提取修辞重要片段，结合多种检索模型，为法律研究提供可靠且可扩展的解决方案。", "motivation": "普通法系遵循“遵循先例”原则，要求司法裁决保持一致性。然而，日益增长的法律文档复杂性和数量对传统检索方法构成了挑战。现实世界的法律搜索往往只有有限的案件信息，需要一种能应对这些限制的新方法。", "method": "TraceRetriever系统采用流水线式架构，整合了BM25、向量数据库和交叉编码器模型进行初步检索。初始结果通过倒数排序融合（Reciprocal Rank Fusion）进行组合，然后进行最终重排序。修辞标注通过在印度判决书上训练的层次BiLSTM CRF分类器生成，系统仅提取修辞上重要的片段而非完整文档。", "result": "TraceRetriever在IL-PCR和COLIEE 2025数据集上进行了评估，结果表明它能有效应对日益增长的文档量挑战，并与实际搜索约束保持一致。它为判例检索提供了一个可靠且可扩展的基础，在只有部分案件知识可用时，能增强法律研究。", "conclusion": "TraceRetriever提供了一种可靠且可扩展的法律判例检索基础，特别适用于只有部分案件信息可用的场景，有效提升了法律研究的效率和准确性，同时应对了海量法律文档带来的挑战。"}}
{"id": "2508.00367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00367", "abs": "https://arxiv.org/abs/2508.00367", "authors": ["Joonmyung Choi", "Sanghyeok Lee", "Byungoh Ko", "Eunseo Kim", "Jihyung Kil", "Hyunwoo J. Kim"], "title": "Representation Shift: Unifying Token Compression with FlashAttention", "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Transformers have demonstrated remarkable success across vision, language,\nand video. Yet, increasing task complexity has led to larger models and more\ntokens, raising the quadratic cost of self-attention and the overhead of GPU\nmemory access. To reduce the computation cost of self-attention, prior work has\nproposed token compression techniques that drop redundant or less informative\ntokens. Meanwhile, fused attention kernels such as FlashAttention have been\ndeveloped to alleviate memory overhead by avoiding attention map construction\nand its associated I/O to HBM. This, however, makes it incompatible with most\ntraining-free token compression methods, which rely on attention maps to\ndetermine token importance. Here, we propose Representation Shift, a\ntraining-free, model-agnostic metric that measures the degree of change in each\ntoken's representation. This seamlessly integrates token compression with\nFlashAttention, without attention maps or retraining. Our method further\ngeneralizes beyond Transformers to CNNs and state space models. Extensive\nexperiments show that Representation Shift enables effective token compression\ncompatible with FlashAttention, yielding significant speedups of up to 5.5% and\n4.4% in video-text retrieval and video QA, respectively. Code is available at\nhttps://github.com/mlvlab/Representation-Shift.", "AI": {"tldr": "提出了一种名为“表征偏移”（Representation Shift）的无训练、模型无关的度量方法，用于令牌压缩，使其能与FlashAttention高效集成，从而在不依赖注意力图的情况下实现显著加速。", "motivation": "Transformer模型在视觉、语言和视频领域取得了巨大成功，但随着任务复杂性增加，模型和令牌数量也随之增长，导致自注意力机制的二次计算成本和GPU内存访问开销巨大。现有令牌压缩方法通常依赖注意力图来判断令牌重要性，这与避免构建注意力图的FlashAttention等融合注意力核不兼容。", "method": "本文提出“表征偏移”作为一种无训练、模型无关的度量标准，用于衡量每个令牌表征的变化程度。该方法无需注意力图或重新训练，即可将令牌压缩与FlashAttention无缝集成。此外，它还能推广到CNN和状态空间模型。", "result": "大量实验表明，“表征偏移”能实现与FlashAttention兼容的有效令牌压缩，在视频-文本检索和视频问答任务中分别带来了高达5.5%和4.4%的显著加速。", "conclusion": "“表征偏移”成功地将令牌压缩与FlashAttention结合，有效解决了计算成本和内存开销问题，并在多模态任务中展现出显著的加速效果和广泛的适用性。"}}
{"id": "2508.00413", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00413", "abs": "https://arxiv.org/abs/2508.00413", "authors": ["Junyu Chen", "Dongyun Zou", "Wenkun He", "Junsong Chen", "Enze Xie", "Song Han", "Han Cai"], "title": "DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space", "comment": "ICCV 2025", "summary": "We present DC-AE 1.5, a new family of deep compression autoencoders for\nhigh-resolution diffusion models. Increasing the autoencoder's latent channel\nnumber is a highly effective approach for improving its reconstruction quality.\nHowever, it results in slow convergence for diffusion models, leading to poorer\ngeneration quality despite better reconstruction quality. This issue limits the\nquality upper bound of latent diffusion models and hinders the employment of\nautoencoders with higher spatial compression ratios. We introduce two key\ninnovations to address this challenge: i) Structured Latent Space, a\ntraining-based approach to impose a desired channel-wise structure on the\nlatent space with front latent channels capturing object structures and latter\nlatent channels capturing image details; ii) Augmented Diffusion Training, an\naugmented diffusion training strategy with additional diffusion training\nobjectives on object latent channels to accelerate convergence. With these\ntechniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling\nresults than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better\nimage generation quality than DC-AE-f32c32 while being 4x faster. Code:\nhttps://github.com/dc-ai-projects/DC-Gen.", "AI": {"tldr": "DC-AE 1.5 是一种新型深度压缩自编码器，通过结构化潜在空间和增强扩散训练，解决了高分辨率扩散模型中增加潜在通道导致的收敛慢和生成质量下降问题，实现了更快的收敛和更好的生成质量。", "motivation": "动机是高分辨率扩散模型中，虽然增加自编码器的潜在通道能提高重建质量，但会导致扩散模型收敛缓慢，反而降低生成质量。这限制了潜在扩散模型的质量上限，并阻碍了高空间压缩比自编码器的应用。", "method": "本文引入了两项关键创新：i) 结构化潜在空间（Structured Latent Space），一种基于训练的方法，将潜在空间强制划分为捕获物体结构的前部通道和捕获图像细节的后部通道；ii) 增强扩散训练（Augmented Diffusion Training），一种额外的扩散训练策略，对物体潜在通道施加额外的扩散训练目标以加速收敛。", "result": "通过这些技术，DC-AE 1.5 比 DC-AE 实现了更快的收敛和更好的扩散缩放结果。在 ImageNet 512x512 数据集上，DC-AE-1.5-f64c128 提供了比 DC-AE-f32c32 更好的图像生成质量，同时速度快了 4 倍。", "conclusion": "DC-AE 1.5 通过其创新方法有效解决了高分辨率扩散模型中潜在通道增加导致的收敛和生成质量问题，显著提升了模型性能和效率，为高分辨率图像生成提供了更优的解决方案。"}}
{"id": "2508.00680", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00680", "abs": "https://arxiv.org/abs/2508.00680", "authors": ["Johannes Römisch", "Svetlana Gorovaia", "Mariia Halchynska", "Gleb Schmidt", "Ivan P. Yamshchikov"], "title": "Better Call Claude: Can LLMs Detect Changes of Writing Style?", "comment": null, "summary": "This article explores the zero-shot performance of state-of-the-art large\nlanguage models (LLMs) on one of the most challenging tasks in authorship\nanalysis: sentence-level style change detection. Benchmarking four LLMs on the\nofficial PAN~2024 and 2025 \"Multi-Author Writing Style Analysis\" datasets, we\npresent several observations. First, state-of-the-art generative models are\nsensitive to variations in writing style - even at the granular level of\nindividual sentences. Second, their accuracy establishes a challenging baseline\nfor the task, outperforming suggested baselines of the PAN competition.\nFinally, we explore the influence of semantics on model predictions and present\nevidence suggesting that the latest generation of LLMs may be more sensitive to\ncontent-independent and purely stylistic signals than previously reported.", "AI": {"tldr": "本文评估了SOTA大型语言模型（LLMs）在句子级别写作风格变化检测这一零样本任务上的表现，发现它们对风格变化敏感，并建立了强劲的基线，且可能更侧重纯粹的风格信号。", "motivation": "探索LLMs在作者分析中最具挑战性的任务之一——句子级别风格变化检测上的零样本能力。", "method": "在PAN 2024和2025“多作者写作风格分析”官方数据集上，对四种先进的LLMs进行基准测试，评估其零样本性能。", "result": "1. SOTA生成模型对写作风格变化（即使是句子级别的细微变化）很敏感。2. 它们的准确性为该任务建立了具有挑战性的基线，优于PAN竞赛建议的基线。3. 最新一代LLMs可能比之前报道的对独立于内容的纯粹风格信号更敏感。", "conclusion": "LLMs在句子级别风格变化检测上表现出强大的零样本能力，对细粒度风格变化敏感，并能识别内容无关的纯粹风格信号，为作者分析任务提供了新的视角和高性能基线。"}}
{"id": "2508.00374", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00374", "abs": "https://arxiv.org/abs/2508.00374", "authors": ["Yuji Sato", "Yasunori Ishii", "Takayoshi Yamashita"], "title": "Bidirectional Action Sequence Learning for Long-term Action Anticipation with Large Language Models", "comment": "Accepted to MVA2025 (Best Poster Award)", "summary": "Video-based long-term action anticipation is crucial for early risk detection\nin areas such as automated driving and robotics. Conventional approaches\nextract features from past actions using encoders and predict future events\nwith decoders, which limits performance due to their unidirectional nature.\nThese methods struggle to capture semantically distinct sub-actions within a\nscene. The proposed method, BiAnt, addresses this limitation by combining\nforward prediction with backward prediction using a large language model.\nExperimental results on Ego4D demonstrate that BiAnt improves performance in\nterms of edit distance compared to baseline methods.", "AI": {"tldr": "BiAnt方法结合前向和后向预测，并利用大语言模型，提升了视频长期动作预测的性能，在Ego4D数据集上优于基线方法。", "motivation": "传统的视频长期动作预测方法（基于编码器-解码器）因其单向性而限制了性能，难以捕捉场景中语义不同的子动作，这在自动驾驶和机器人等早期风险检测领域是一个关键问题。", "method": "提出BiAnt方法，通过结合前向预测与后向预测，并利用大语言模型来克服传统单向方法的局限性。", "result": "在Ego4D数据集上的实验结果表明，BiAnt在编辑距离方面相比基线方法有性能提升。", "conclusion": "BiAnt通过引入双向预测机制和利用大语言模型，有效解决了传统单向方法在视频长期动作预测中的性能限制和捕捉子动作的挑战。"}}
{"id": "2508.00427", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00427", "abs": "https://arxiv.org/abs/2508.00427", "authors": ["Seunggeun Chi", "Enna Sachdeva", "Pin-Hao Huang", "Kwonjoon Lee"], "title": "Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting", "comment": "ICCV 2025 (Highlight)", "summary": "Amodal completion, which is the process of inferring the full appearance of\nobjects despite partial occlusions, is crucial for understanding complex\nhuman-object interactions (HOI) in computer vision and robotics. Existing\nmethods, such as those that use pre-trained diffusion models, often struggle to\ngenerate plausible completions in dynamic scenarios because they have a limited\nunderstanding of HOI. To solve this problem, we've developed a new approach\nthat uses physical prior knowledge along with a specialized multi-regional\ninpainting technique designed for HOI. By incorporating physical constraints\nfrom human topology and contact information, we define two distinct regions:\nthe primary region, where occluded object parts are most likely to be, and the\nsecondary region, where occlusions are less probable. Our multi-regional\ninpainting method uses customized denoising strategies across these regions\nwithin a diffusion model. This improves the accuracy and realism of the\ngenerated completions in both their shape and visual detail. Our experimental\nresults show that our approach significantly outperforms existing methods in\nHOI scenarios, moving machine perception closer to a more human-like\nunderstanding of dynamic environments. We also show that our pipeline is robust\neven without ground-truth contact annotations, which broadens its applicability\nto tasks like 3D reconstruction and novel view/pose synthesis.", "AI": {"tldr": "该研究提出一种新的方法，利用物理先验知识和多区域修复技术，改进了在人机交互（HOI）场景中的无模态补全，以生成更真实、准确的对象补全。", "motivation": "现有的无模态补全方法，特别是基于预训练扩散模型的方法，在动态人机交互场景中难以生成可信的补全结果，因为它们对人机交互的理解有限。", "method": "该方法结合了物理先验知识（如人体拓扑和接触信息），定义了两个不同的区域：主要区域（最可能被遮挡的对象部分）和次要区域（遮挡可能性较低）。在此基础上，采用了一种专门的多区域修复技术，在扩散模型中对这些区域应用定制的去噪策略。", "result": "实验结果表明，该方法在人机交互场景中显著优于现有方法，提高了生成补全结果的形状和视觉细节的准确性和真实性。此外，该管道在没有真实接触标注的情况下也表现出鲁棒性。", "conclusion": "该方法使机器感知更接近对动态环境的人类理解，并拓宽了其在3D重建和新视角/姿态合成等任务中的应用前景。"}}
{"id": "2508.00709", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00709", "abs": "https://arxiv.org/abs/2508.00709", "authors": ["Shubham Kumar Nigam", "Balaramamahanthi Deepak Patnaik", "Shivam Mishra", "Ajay Varghese Thomas", "Noel Shallum", "Kripabandhu Ghosh", "Arnab Bhattacharya"], "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System", "comment": null, "summary": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.", "AI": {"tldr": "提出NyayaRAG框架，通过整合事实、法律条文和先例，显著提升印度法律判决预测和解释的准确性。", "motivation": "法律判决预测（LJP）是法律AI的关键领域，旨在自动化司法结果预测并增强法律推理的可解释性。然而，现有印度LJP方法仅依赖案件内部内容，忽略了普通法系中法律条文和司法先例的核心作用。", "method": "提出NyayaRAG，一个检索增强生成（RAG）框架。该框架通过向模型提供事实案件描述、相关法律条文和语义检索的先前案例，模拟真实的法庭场景。它评估这些组合输入在预测法院判决和生成法律解释方面的有效性，并使用针对印度法律系统量身定制的领域特定管道。评估使用标准词汇/语义指标和基于LLM的评估器（如G-Eval）。", "result": "研究结果表明，将事实输入与结构化法律知识（法律条文和司法先例）相结合，显著提高了预测准确性和解释质量。", "conclusion": "结合法律条文和司法先例的RAG框架（NyayaRAG）能有效提升法律判决预测和解释的性能，为法律AI提供了更符合实际的解决方案。"}}
{"id": "2508.00391", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.00391", "abs": "https://arxiv.org/abs/2508.00391", "authors": ["Guanjie Huang", "Danny H. K. Tsang", "Shan Yang", "Guangzhi Lei", "Li Liu"], "title": "Cued-Agent: A Collaborative Multi-Agent System for Automatic Cued Speech Recognition", "comment": "9 pages", "summary": "Cued Speech (CS) is a visual communication system that combines lip-reading\nwith hand coding to facilitate communication for individuals with hearing\nimpairments. Automatic CS Recognition (ACSR) aims to convert CS hand gestures\nand lip movements into text via AI-driven methods. Traditionally, the temporal\nasynchrony between hand and lip movements requires the design of complex\nmodules to facilitate effective multimodal fusion. However, constrained by\nlimited data availability, current methods demonstrate insufficient capacity\nfor adequately training these fusion mechanisms, resulting in suboptimal\nperformance. Recently, multi-agent systems have shown promising capabilities in\nhandling complex tasks with limited data availability. To this end, we propose\nthe first collaborative multi-agent system for ACSR, named Cued-Agent. It\nintegrates four specialized sub-agents: a Multimodal Large Language Model-based\nHand Recognition agent that employs keyframe screening and CS expert prompt\nstrategies to decode hand movements, a pretrained Transformer-based Lip\nRecognition agent that extracts lip features from the input video, a Hand\nPrompt Decoding agent that dynamically integrates hand prompts with lip\nfeatures during inference in a training-free manner, and a Self-Correction\nPhoneme-to-Word agent that enables post-process and end-to-end conversion from\nphoneme sequences to natural language sentences for the first time through\nsemantic refinement. To support this study, we expand the existing Mandarin CS\ndataset by collecting data from eight hearing-impaired cuers, establishing a\nmixed dataset of fourteen subjects. Extensive experiments demonstrate that our\nCued-Agent performs superbly in both normal and hearing-impaired scenarios\ncompared with state-of-the-art methods. The implementation is available at\nhttps://github.com/DennisHgj/Cued-Agent.", "AI": {"tldr": "本文提出了一种名为Cued-Agent的多智能体系统，用于自动手语识别（ACSR），通过整合专门的代理来解决手势和唇部动作的时间异步以及数据限制问题，并在正常和听障场景下均优于现有方法。", "motivation": "传统的自动手语识别（ACSR）方法在处理手势和唇部动作之间的时间异步时需要复杂模块，但受限于数据量不足，导致训练效果不佳和性能欠优。多智能体系统在处理数据有限的复杂任务方面展现出潜力。", "method": "提出首个用于ACSR的协作式多智能体系统Cued-Agent，包含四个专门的子代理：基于多模态大语言模型的“手势识别代理”（采用关键帧筛选和CS专家提示策略）、预训练Transformer的“唇部识别代理”、在推理阶段免训练动态整合手势提示和唇部特征的“手势提示解码代理”，以及首次通过语义细化实现音素序列到自然语言句子端到端转换的“自校正音素转词代理”。此外，通过收集八名听障发音者的数据，扩展了现有普通话CS数据集。", "result": "Cued-Agent在正常和听障场景下的实验表现均优于现有最先进的方法。", "conclusion": "Cued-Agent作为首个用于ACSR的多智能体系统，有效解决了手势和唇部动作的时间异步以及数据限制问题，实现了卓越的识别性能。"}}
{"id": "2508.00442", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00442", "abs": "https://arxiv.org/abs/2508.00442", "authors": ["Jiale Zhou", "Wenhan Wang", "Shikun Li", "Xiaolei Qu", "Xin Guo", "Yizhong Liu", "Wenzhong Tang", "Xun Lin", "Yefeng Zheng"], "title": "TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation", "comment": null, "summary": "Tubular structure segmentation (TSS) is important for various applications,\nsuch as hemodynamic analysis and route navigation. Despite significant progress\nin TSS, domain shifts remain a major challenge, leading to performance\ndegradation in unseen target domains. Unlike other segmentation tasks, TSS is\nmore sensitive to domain shifts, as changes in topological structures can\ncompromise segmentation integrity, and variations in local features\ndistinguishing foreground from background (e.g., texture and contrast) may\nfurther disrupt topological continuity. To address these challenges, we propose\nTopology-enhanced Test-Time Adaptation (TopoTTA), the first test-time\nadaptation framework designed specifically for TSS. TopoTTA consists of two\nstages: Stage 1 adapts models to cross-domain topological discrepancies using\nthe proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance\ntopological representation without altering pre-trained parameters; Stage 2\nimproves topological continuity by a novel Topology Hard sample Generation\n(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels\nin the generated pseudo-break regions. Extensive experiments across four\nscenarios and ten datasets demonstrate TopoTTA's effectiveness in handling\ntopological distribution shifts, achieving an average improvement of 31.81% in\nclDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS\nmodels.", "AI": {"tldr": "本文提出了TopoTTA，首个专为管状结构分割（TSS）设计的测试时间适应（TTA）框架，旨在解决跨域拓扑差异和局部特征变化导致的性能下降问题，显著提升了分割的拓扑完整性。", "motivation": "管状结构分割（TSS）在多种应用中非常重要，但领域漂移（domain shifts）是主要挑战，导致在未见目标域中性能下降。与其他分割任务不同，TSS对领域漂移更敏感，因为拓扑结构的变化会损害分割完整性，局部特征（如纹理和对比度）的变化可能进一步破坏拓扑连续性。", "method": "本文提出了TopoTTA，一个两阶段的TSS测试时间适应框架。第一阶段利用所提出的拓扑元差异卷积（TopoMDCs）来适应跨域拓扑差异，在不改变预训练参数的情况下增强拓扑表示。第二阶段通过新颖的拓扑难样本生成（TopoHG）策略和在生成的伪断裂区域中利用伪标签进行难样本预测对齐，从而改善拓扑连续性。", "result": "在四种场景和十个数据集上的大量实验表明，TopoTTA在处理拓扑分布漂移方面非常有效，平均clDice指标提升了31.81%。TopoTTA还可以作为基于CNN的TSS模型的即插即用TTA解决方案。", "conclusion": "TopoTTA是第一个专门为管状结构分割设计的测试时间适应框架，它有效地解决了领域漂移导致的拓扑结构和局部特征变化问题，显著提高了分割的拓扑完整性，并可作为现有CNN-based TSS模型的通用解决方案。"}}
{"id": "2508.00719", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00719", "abs": "https://arxiv.org/abs/2508.00719", "authors": ["Yingxu Wang", "Shiqi Fan", "Mengzhu Wang", "Siwei Liu"], "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA", "comment": null, "summary": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "DAMR是一种新颖的知识图谱问答（KGQA）框架，它结合了蒙特卡洛树搜索（MCTS）、大型语言模型（LLM）规划器、基于Transformer的上下文感知评分器以及动态伪路径细化机制，以实现高效且上下文感知的多跳推理。", "motivation": "现有的KGQA方法存在局限性：基于GNN或启发式规则的静态路径提取方法适应性差，缺乏上下文细化；而基于LLM的动态路径生成方法计算成本高昂，且由于依赖固定评分函数和大量LLM调用，路径评估准确性不足。", "method": "本文提出了动态自适应MCTS推理（DAMR）框架。它以MCTS为骨干，由LLM规划器引导，每一步选择top-k相关关系以缩小搜索空间。为提高路径评估准确性，引入了轻量级Transformer评分器，通过交叉注意力联合编码问题和关系序列，进行上下文感知合理性估计。此外，为缓解高质量监督数据稀缺问题，DAMR整合了动态伪路径细化机制，定期从搜索过程中探索的部分路径生成训练信号，使评分器能持续适应推理轨迹的演变分布。", "result": "在多个KGQA基准测试上的大量实验表明，DAMR显著优于现有最先进的方法。", "conclusion": "DAMR通过结合符号搜索与自适应路径评估，提供了一种高效且准确的知识图谱问答解决方案，有效解决了现有方法的局限性。"}}
{"id": "2508.00397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00397", "abs": "https://arxiv.org/abs/2508.00397", "authors": ["Xi Xue", "Kunio Suzuki", "Nabarun Goswami", "Takuya Shintate"], "title": "Video Forgery Detection with Optical Flow Residuals and Spatial-Temporal Consistency", "comment": null, "summary": "The rapid advancement of diffusion-based video generation models has led to\nincreasingly realistic synthetic content, presenting new challenges for video\nforgery detection. Existing methods often struggle to capture fine-grained\ntemporal inconsistencies, particularly in AI-generated videos with high visual\nfidelity and coherent motion. In this work, we propose a detection framework\nthat leverages spatial-temporal consistency by combining RGB appearance\nfeatures with optical flow residuals. The model adopts a dual-branch\narchitecture, where one branch analyzes RGB frames to detect appearance-level\nartifacts, while the other processes flow residuals to reveal subtle motion\nanomalies caused by imperfect temporal synthesis. By integrating these\ncomplementary features, the proposed method effectively detects a wide range of\nforged videos. Extensive experiments on text-to-video and image-to-video tasks\nacross ten diverse generative models demonstrate the robustness and strong\ngeneralization ability of the proposed approach.", "AI": {"tldr": "该论文提出了一种双分支检测框架，结合RGB外观特征和光流残差，以捕捉AI生成视频中的时空不一致性，从而有效检测各种伪造视频。", "motivation": "随着扩散模型生成视频的真实性不断提高，现有视频伪造检测方法难以捕捉高视觉保真度和连贯运动的AI生成视频中细粒度的时空不一致性。", "method": "采用双分支架构：一个分支分析RGB帧以检测外观级别伪影，另一个分支处理光流残差以揭示不完美时间合成引起的细微运动异常。通过整合这些互补特征，利用时空一致性进行检测。", "result": "在文本到视频和图像到视频任务中，对十种不同的生成模型进行了广泛实验，结果表明所提出的方法在检测各种伪造视频方面表现出鲁棒性和强大的泛化能力。", "conclusion": "所提出的结合RGB外观特征和光流残差的时空一致性检测框架，能有效且鲁棒地检测AI生成视频，并具有良好的泛化能力。"}}
{"id": "2508.00496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00496", "abs": "https://arxiv.org/abs/2508.00496", "authors": ["Mohammed Kamran", "Maria Bernathova", "Raoul Varga", "Christian Singer", "Zsuzsanna Bago-Horvath", "Thomas Helbich", "Georg Langs", "Philipp Seeböck"], "title": "LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI", "comment": null, "summary": "Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced\nMRI (DCE-MRI) is critical for early cancer detection, especially in high-risk\npatients. While recent deep learning methods have advanced lesion segmentation,\nthey primarily target large lesions and neglect valuable longitudinal and\nclinical information routinely used by radiologists. In real-world screening,\ndetecting subtle or emerging lesions requires radiologists to compare across\ntimepoints and consider previous radiology assessments, such as the BI-RADS\nscore. We propose LesiOnTime, a novel 3D segmentation approach that mimics\nclinical diagnostic workflows by jointly leveraging longitudinal imaging and\nBIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)\nblock that dynamically integrates information from previous and current scans;\nand (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent\nspace alignment for scans with similar radiological assessments, thus embedding\ndomain knowledge into the training process. Evaluated on a curated in-house\nlongitudinal dataset of high-risk patients with DCE-MRI, our approach\noutperforms state-of-the-art single-timepoint and longitudinal baselines by 5%\nin terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute\ncomplementary performance gains. These results highlight the importance of\nincorporating temporal and clinical context for reliable early lesion\nsegmentation in real-world breast cancer screening. Our code is publicly\navailable at https://github.com/cirmuw/LesiOnTime", "AI": {"tldr": "LesiOnTime是一种新颖的3D分割方法，通过联合利用纵向影像和BI-RADS评分，模仿临床诊断流程，以提高乳腺DCE-MRI中小病灶的准确分割。", "motivation": "现有深度学习方法主要针对大型病灶，忽略了放射科医生在实际筛查中用于检测细微或新发病灶的宝贵纵向和临床信息（如BI-RADS评分）。准确分割小病灶对于高危患者的早期癌症检测至关重要。", "method": "提出LesiOnTime方法，包含两个关键组件：1) 时间先验注意力（TPA）模块，动态整合来自先前和当前扫描的信息；2) BI-RADS一致性正则化（BCR）损失，强制具有相似放射学评估的扫描在潜在空间中对齐，从而将领域知识嵌入训练过程。", "result": "在自建的高危患者DCE-MRI纵向数据集上进行评估，LesiOnTime在Dice指标上比最先进的单时间点和纵向基线方法高出5%。消融研究表明TPA和BCR都提供了互补的性能增益。", "conclusion": "研究结果强调了在真实世界乳腺癌筛查中，结合时间性和临床背景对于可靠的早期病灶分割的重要性。"}}
{"id": "2508.00741", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00741", "abs": "https://arxiv.org/abs/2508.00741", "authors": ["Sohaib Imran", "Rob Lamb", "Peter M. Atkinson"], "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data", "comment": null, "summary": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.", "AI": {"tldr": "研究了大型语言模型（LLMs）对训练数据中信息的“脱离上下文溯因推理”能力，发现GPT-4o能根据行为推断虚拟聊天机器人的名称，并能更好地展现其特征行为。", "motivation": "尽管LLMs在大量语料库上训练，但它们是否能对训练数据中存在的信息进行推理尚不清楚。本研究旨在探讨LLMs进行“脱离上下文溯因推理”（即根据观察推断最合理解释的能力）的情况。", "method": "设计实验，在LLMs上训练虚构聊天机器人的名称和行为描述（但不包括对话示例）。然后，通过观察聊天机器人的响应示例，测试LLM能否正确推断出其名称。此外，还研究了预先训练行为描述对模型迭代训练后展现特征行为的影响。", "result": "OpenAI的GPT-4o模型在观察到特定聊天机器人的特征响应后，能够正确推断出至少一个聊天机器人的名称。研究还发现，预先用聊天机器人行为描述训练GPT-4o，能使其在迭代训练后更好地展现出这些特征行为。", "conclusion": "研究结果对LLMs的态势感知能力及其AI安全具有重要意义。"}}
{"id": "2508.00399", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00399", "abs": "https://arxiv.org/abs/2508.00399", "authors": ["Raiyaan Abdullah", "Yogesh Singh Rawat", "Shruti Vyas"], "title": "iSafetyBench: A video-language benchmark for safety in industrial environment", "comment": "Accepted to VISION'25 - ICCV 2025 workshop", "summary": "Recent advances in vision-language models (VLMs) have enabled impressive\ngeneralization across diverse video understanding tasks under zero-shot\nsettings. However, their capabilities in high-stakes industrial domains-where\nrecognizing both routine operations and safety-critical anomalies is\nessential-remain largely underexplored. To address this gap, we introduce\niSafetyBench, a new video-language benchmark specifically designed to evaluate\nmodel performance in industrial environments across both normal and hazardous\nscenarios. iSafetyBench comprises 1,100 video clips sourced from real-world\nindustrial settings, annotated with open-vocabulary, multi-label action tags\nspanning 98 routine and 67 hazardous action categories. Each clip is paired\nwith multiple-choice questions for both single-label and multi-label\nevaluation, enabling fine-grained assessment of VLMs in both standard and\nsafety-critical contexts. We evaluate eight state-of-the-art video-language\nmodels under zero-shot conditions. Despite their strong performance on existing\nvideo benchmarks, these models struggle with iSafetyBench-particularly in\nrecognizing hazardous activities and in multi-label scenarios. Our results\nreveal significant performance gaps, underscoring the need for more robust,\nsafety-aware multimodal models for industrial applications. iSafetyBench\nprovides a first-of-its-kind testbed to drive progress in this direction. The\ndataset is available at: https://github.com/raiyaan-abdullah/iSafety-Bench.", "AI": {"tldr": "本文提出了iSafetyBench，一个用于评估视觉语言模型在工业环境中识别常规操作和安全关键异常的新视频语言基准。现有SOTA模型在该基准上表现不佳，尤其是在危险活动和多标签识别方面，凸显了开发更鲁棒、安全感知模型的必要性。", "motivation": "尽管视觉语言模型（VLMs）在零样本视频理解任务中表现出色，但它们在工业领域（需要识别常规操作和安全关键异常）的能力尚未得到充分探索。为了填补这一空白，需要一个专门的基准来评估模型在该高风险环境中的性能。", "method": "研究引入了iSafetyBench，一个包含1100个真实工业视频片段的视频语言基准。这些视频被标注了开放词汇、多标签动作标签，涵盖98个常规和67个危险动作类别。每个片段都配有多项选择题，用于单标签和多标签评估。研究评估了8个最先进的零样本视频语言模型。", "result": "尽管在现有视频基准上表现强劲，这些模型在iSafetyBench上表现不佳，尤其是在识别危险活动和多标签场景中。结果揭示了显著的性能差距。", "conclusion": "当前最先进的视觉语言模型在工业安全场景中存在显著局限性，特别是在识别危险活动和处理多标签任务方面。这强调了开发更鲁棒、更具安全意识的多模态模型以应用于工业领域的必要性。iSafetyBench提供了一个独特的测试平台来推动这一方向的进展。"}}
{"id": "2508.00591", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.00591", "abs": "https://arxiv.org/abs/2508.00591", "authors": ["Mingrui Liu", "Sixiao Zhang", "Cheng Long"], "title": "Wukong Framework for Not Safe For Work Detection in Text-to-Image systems", "comment": "Under review", "summary": "Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)\ntechnology enabling diverse and creative image synthesis. However, some outputs\nmay contain Not Safe For Work (NSFW) content (e.g., violence), violating\ncommunity guidelines. Detecting NSFW content efficiently and accurately, known\nas external safeguarding, is essential. Existing external safeguards fall into\ntwo types: text filters, which analyze user prompts but overlook T2I\nmodel-specific variations and are prone to adversarial attacks; and image\nfilters, which analyze final generated images but are computationally costly\nand introduce latency. Diffusion models, the foundation of modern T2I systems\nlike Stable Diffusion, generate images through iterative denoising using a\nU-Net architecture with ResNet and Transformer blocks. We observe that: (1)\nearly denoising steps define the semantic layout of the image, and (2)\ncross-attention layers in U-Net are crucial for aligning text and image\nregions. Based on these insights, we propose Wukong, a transformer-based NSFW\ndetection framework that leverages intermediate outputs from early denoising\nsteps and reuses U-Net's pre-trained cross-attention parameters. Wukong\noperates within the diffusion process, enabling early detection without waiting\nfor full image generation. We also introduce a new dataset containing prompts,\nseeds, and image-specific NSFW labels, and evaluate Wukong on this and two\npublic benchmarks. Results show that Wukong significantly outperforms\ntext-based safeguards and achieves comparable accuracy of image filters, while\noffering much greater efficiency.", "AI": {"tldr": "Wukong是一个基于Transformer的NSFW内容检测框架，它在文本到图像（T2I）扩散模型的早期去噪步骤中进行检测，通过利用中间输出和重用U-Net的交叉注意力参数，实现了高效且准确的早期检测，优于文本过滤器并与图像过滤器精度相当。", "motivation": "当前的T2I生成技术可能产生不安全（NSFW）内容，需要高效准确的检测机制。现有检测方法存在缺陷：文本过滤器忽略模型特异性且易受攻击；图像过滤器计算成本高且引入延迟。因此，需要一种更高效、准确且能早期介入的检测方法。", "method": "研究基于两点观察：1) 早期去噪步骤定义图像的语义布局；2) U-Net中的交叉注意力层对文本和图像区域对齐至关重要。基于此，提出Wukong框架，它是一个基于Transformer的检测器，利用扩散模型早期去噪步骤的中间输出，并重用U-Net预训练的交叉注意力参数。Wukong在扩散过程中运行，实现早期检测。此外，还引入了一个包含提示、种子和图像特定NSFW标签的新数据集用于评估。", "result": "Wukong在自建数据集和两个公共基准测试上进行了评估，结果表明它显著优于基于文本的防护措施，并能达到与图像过滤器相当的准确性，同时提供了更高的效率。", "conclusion": "Wukong提供了一种在文本到图像生成过程中高效、准确地早期检测NSFW内容的方法，解决了现有检测方案的效率和延迟问题，并在性能上取得了显著提升。"}}
{"id": "2508.00742", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00742", "abs": "https://arxiv.org/abs/2508.00742", "authors": ["Sarah Mercer", "Daniel P. Martin", "Phil Swatton"], "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents", "comment": "26 pages, 14 figures", "summary": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.", "AI": {"tldr": "本研究通过对310个GPT-4代理进行HEXACO人格问卷调查，评估了生成式代理在社会科学研究中代表人格的有效性，发现代理能展现出一致但与HEXACO框架部分对齐的人格结构，并存在模型特异性偏差。", "motivation": "生成式代理因其模拟人类互动和扮演角色的能力，被视为社会科学研究中人类参与者的经济高效替代品。本研究旨在探讨这些基于角色的代理在代表人格方面的有效性。", "method": "通过调查310个由GPT-4驱动的代理，重现了HEXACO人格量表实验。对代理的回答进行因子分析，并将其结果与2004年Ashton, Lee & Goldberg的原始人类研究结果进行比较。", "result": "1) 从代理的回答中恢复出了一致且可靠的人格结构，显示出与HEXACO框架的部分对齐。2) 在结合了充分策划的人口后，GPT-4内部导出的人格维度保持一致和可靠。3) 跨模型分析揭示了人格画像的变异性，表明存在模型特异性偏差和局限性。", "conclusion": "本研究为使用生成式代理进行社会科学研究的潜在益处和局限性提供了见解，并为设计一致且具有代表性的代理角色以最大限度地覆盖和代表人格特质提供了实用指导。"}}
{"id": "2508.00400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00400", "abs": "https://arxiv.org/abs/2508.00400", "authors": ["Janika Deborah Gajo", "Gerarld Paul Merales", "Jerome Escarcha", "Brenden Ashley Molina", "Gian Nartea", "Emmanuel G. Maminta", "Juan Carlos Roldan", "Rowel O. Atienza"], "title": "Sari Sandbox: A Virtual Retail Store Environment for Embodied AI Agents", "comment": "14 pages, accepted in ICCV 2025 Workshop on RetailVision", "summary": "We present Sari Sandbox, a high-fidelity, photorealistic 3D retail store\nsimulation for benchmarking embodied agents against human performance in\nshopping tasks. Addressing a gap in retail-specific sim environments for\nembodied agent training, Sari Sandbox features over 250 interactive grocery\nitems across three store configurations, controlled via an API. It supports\nboth virtual reality (VR) for human interaction and a vision language model\n(VLM)-powered embodied agent. We also introduce SariBench, a dataset of\nannotated human demonstrations across varied task difficulties. Our sandbox\nenables embodied agents to navigate, inspect, and manipulate retail items,\nproviding baselines against human performance. We conclude with benchmarks,\nperformance analysis, and recommendations for enhancing realism and\nscalability. The source code can be accessed via\nhttps://github.com/upeee/sari-sandbox-env.", "AI": {"tldr": "Sari Sandbox是一个高保真、逼真的3D零售店模拟环境，用于衡量具身智能体在购物任务中相对于人类的表现，并提供了SariBench数据集。", "motivation": "现有具身智能体训练环境缺乏零售业特定模拟场景。", "method": "开发了Sari Sandbox，一个包含250多种互动商品、三种店铺配置、支持API控制、VR和VLM具身智能体的3D零售店模拟器；同时构建了SariBench数据集，包含带标注的人类购物任务演示。", "result": "Sari Sandbox使具身智能体能够进行导航、检查和操作零售商品，并提供了与人类表现对比的基准；论文还包含了基准测试和性能分析。", "conclusion": "为提升模拟环境的真实感和可扩展性提出了建议。"}}
{"id": "2508.00620", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00620", "abs": "https://arxiv.org/abs/2508.00620", "authors": ["Quentin Le Roux", "Yannick Teglia", "Teddy Furon", "Philippe Loubet-Moundi"], "title": "Backdoor Attacks on Deep Learning Face Detection", "comment": null, "summary": "Face Recognition Systems that operate in unconstrained environments capture\nimages under varying conditions,such as inconsistent lighting, or diverse face\nposes. These challenges require including a Face Detection module that\nregresses bounding boxes and landmark coordinates for proper Face Alignment.\nThis paper shows the effectiveness of Object Generation Attacks on Face\nDetection, dubbed Face Generation Attacks, and demonstrates for the first time\na Landmark Shift Attack that backdoors the coordinate regression task performed\nby face detectors. We then offer mitigations against these vulnerabilities.", "AI": {"tldr": "本文展示了对人脸检测系统进行物体生成攻击（人脸生成攻击）和首次提出的地标偏移攻击的有效性，并提出了相应的缓解措施。", "motivation": "在非受限环境下运行的人脸识别系统面临光照不一致、姿态多样等挑战，需要强大的人脸检测模块来回归边界框和地标坐标以进行正确的人脸对齐。本文旨在探讨人脸检测模块在这种复杂环境下的潜在漏洞。", "method": "本文提出了两种攻击方法：物体生成攻击（特指人脸生成攻击）和首次提出的地标偏移攻击。地标偏移攻击旨在通过后门方式影响人脸检测器执行的坐标回归任务。此外，论文还提出了针对这些漏洞的缓解措施。", "result": "研究结果表明，物体生成攻击（人脸生成攻击）对人脸检测是有效的。更重要的是，本文首次成功演示了地标偏移攻击，该攻击能够后门人脸检测器执行的地标坐标回归任务。", "conclusion": "人脸检测系统容易受到生成攻击和新型地标偏移攻击的影响，这些攻击可以操纵地标坐标回归。因此，需要采取有效的缓解措施来增强系统的鲁棒性。"}}
{"id": "2508.00743", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00743", "abs": "https://arxiv.org/abs/2508.00743", "authors": ["Sebastian Wind", "Jeta Sopa", "Daniel Truhn", "Mahshad Lotfinia", "Tri-Thien Nguyen", "Keno Bressem", "Lisa Adams", "Mirabela Rusu", "Harald Köstler", "Gerhard Wellein", "Andreas Maier", "Soroosh Tayebi Arasteh"], "title": "Agentic large language models improve retrieval-based radiology question answering", "comment": null, "summary": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.", "AI": {"tldr": "该研究提出了一种代理式RAG框架，使LLM能自主分解放射学问题、迭代检索证据并生成响应，显著提高了放射学问答的诊断准确性并减少了幻觉，尤其对中小型模型效果显著。", "motivation": "传统的放射学问答RAG系统通常依赖单步检索，难以处理复杂的临床推理任务，限制了其在临床决策中的应用。", "method": "提出了一种代理式RAG框架，允许大型语言模型（LLMs）自主分解放射学问题，从Radiopaedia迭代检索目标临床证据，并动态合成基于证据的响应。研究评估了24种不同架构、参数规模（0.5B至>670B）和训练范式（通用、推理优化、临床微调）的LLMs，使用了104个专家整理的放射学问题（来自RSNA-RadioQA和ExtendedQA数据集）。", "result": "代理式检索显著提高了平均诊断准确率（73% vs. 零样本提示的64%；P<0.001； vs. 传统在线RAG的68%；P<0.001）。在中型模型（如Mistral Large从72%提高到81%）和小型模型（如Qwen 2.5-7B从55%提高到71%）中增益最大，而超大型模型（>200B参数）变化最小（<2%提升）。此外，代理式检索减少了幻觉（平均9.4%），并在46%的案例中检索到临床相关上下文，显著增强了事实基础。即使临床微调模型也表现出显著改进（如MedGemma-27B从71%提高到81%）。", "conclusion": "代理式框架有潜力提高放射学问答的事实性和诊断准确性，特别是对于中型LLMs，未来的研究应进一步验证其临床实用性。"}}
{"id": "2508.00406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00406", "abs": "https://arxiv.org/abs/2508.00406", "authors": ["Tao Wu", "Jingyuan Ye", "Ying Fu"], "title": "PMR: Physical Model-Driven Multi-Stage Restoration of Turbulent Dynamic Videos", "comment": null, "summary": "Geometric distortions and blurring caused by atmospheric turbulence degrade\nthe quality of long-range dynamic scene videos. Existing methods struggle with\nrestoring edge details and eliminating mixed distortions, especially under\nconditions of strong turbulence and complex dynamics. To address these\nchallenges, we introduce a Dynamic Efficiency Index ($DEI$), which combines\nturbulence intensity, optical flow, and proportions of dynamic regions to\naccurately quantify video dynamic intensity under varying turbulence conditions\nand provide a high-dynamic turbulence training dataset. Additionally, we\npropose a Physical Model-Driven Multi-Stage Video Restoration ($PMR$) framework\nthat consists of three stages: \\textbf{de-tilting} for geometric stabilization,\n\\textbf{motion segmentation enhancement} for dynamic region refinement, and\n\\textbf{de-blurring} for quality restoration. $PMR$ employs lightweight\nbackbones and stage-wise joint training to ensure both efficiency and high\nrestoration quality. Experimental results demonstrate that the proposed method\neffectively suppresses motion trailing artifacts, restores edge details and\nexhibits strong generalization capability, especially in real-world scenarios\ncharacterized by high-turbulence and complex dynamics. We will make the code\nand datasets openly available.", "AI": {"tldr": "该研究针对大气湍流导致的长距离动态视频失真问题，提出了动态效率指数（DEI）用于量化视频动态强度并构建数据集，以及一种物理模型驱动的多阶段视频恢复（PMR）框架，通过去倾斜、运动分割增强和去模糊三阶段处理，有效恢复视频质量并抑制伪影。", "motivation": "现有方法在恢复受大气湍流影响的长距离动态场景视频时，难以恢复边缘细节并消除混合失真，尤其是在强湍流和复杂动态条件下表现不佳。", "method": "1. 引入动态效率指数（DEI），结合湍流强度、光流和动态区域比例，精确量化不同湍流条件下的视频动态强度，并构建高动态湍流训练数据集。2. 提出物理模型驱动的多阶段视频恢复（PMR）框架，包括三个阶段：去倾斜（几何稳定）、运动分割增强（动态区域细化）和去模糊（质量恢复）。3. PMR采用轻量级骨干网络和阶段性联合训练，以确保效率和高质量恢复。", "result": "实验结果表明，所提出的方法有效抑制了运动拖影伪影，恢复了边缘细节，并表现出强大的泛化能力，尤其是在高湍流和复杂动态的真实世界场景中效果显著。", "conclusion": "该研究提出的DEI和PMR框架有效解决了大气湍流导致的长距离动态场景视频质量下降问题，实现了高质量和高效率的视频恢复，尤其适用于挑战性的真实世界条件。代码和数据集将开源。"}}
{"id": "2508.00701", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00701", "abs": "https://arxiv.org/abs/2508.00701", "authors": ["Chende Zheng", "Ruiqi suo", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Shuai Liu", "Minghui Yang", "Cong Wang", "Chao Shen"], "title": "D3: Training-Free AI-Generated Video Detection Using Second-Order Features", "comment": "8 pages, 4 figures", "summary": "The evolution of video generation techniques, such as Sora, has made it\nincreasingly easy to produce high-fidelity AI-generated videos, raising public\nconcern over the dissemination of synthetic content. However, existing\ndetection methodologies remain limited by their insufficient exploration of\ntemporal artifacts in synthetic videos. To bridge this gap, we establish a\ntheoretical framework through second-order dynamical analysis under Newtonian\nmechanics, subsequently extending the Second-order Central Difference features\ntailored for temporal artifact detection. Building on this theoretical\nfoundation, we reveal a fundamental divergence in second-order feature\ndistributions between real and AI-generated videos. Concretely, we propose\nDetection by Difference of Differences (D3), a novel training-free detection\nmethod that leverages the above second-order temporal discrepancies. We\nvalidate the superiority of our D3 on 4 open-source datasets (Gen-Video,\nVideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,\nD3 outperforms the previous best method by 10.39% (absolute) mean Average\nPrecision. Additional experiments on time cost and post-processing operations\ndemonstrate D3's exceptional computational efficiency and strong robust\nperformance. Our code is available at https://github.com/Zig-HS/D3.", "AI": {"tldr": "本文提出了一种名为D3的免训练方法，通过分析真实视频与AI生成视频在二阶时间特征分布上的差异，有效检测AI合成视频，并展现出卓越的性能和效率。", "motivation": "随着Sora等视频生成技术的发展，高保真AI生成视频的制作变得日益容易，引发了对合成内容传播的公众担忧。然而，现有检测方法在探索合成视频中的时间伪影方面存在不足。", "method": "研究者通过牛顿力学下的二阶动力学分析建立了理论框架，并扩展了用于时间伪影检测的二阶中心差分特征。在此理论基础上，他们揭示了真实视频和AI生成视频在二阶特征分布上的根本性差异，并提出了基于这些二阶时间差异的“差分之差检测”（D3）方法，该方法无需训练。", "result": "D3方法在4个开源数据集（共40个子集）上验证了其优越性，例如在GenVideo数据集上，D3的平均精度比现有最佳方法高出10.39%。此外，D3在时间成本和后处理操作方面表现出卓越的计算效率和强大的鲁棒性。", "conclusion": "D3是一种基于二阶时间差异的创新型、免训练AI生成视频检测方法，它在检测性能、计算效率和鲁棒性方面均优于现有方法，有效弥补了当前检测技术在时间伪影分析上的不足。"}}
{"id": "2508.00757", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00757", "abs": "https://arxiv.org/abs/2508.00757", "authors": ["Robin Armingaud", "Romaric Besançon"], "title": "GLiDRE: Generalist Lightweight model for Document-level Relation Extraction", "comment": "Submitted to ARR July", "summary": "Relation Extraction (RE) is a fundamental task in Natural Language\nProcessing, and its document-level variant poses significant challenges, due to\nthe need to model complex interactions between entities across sentences.\nCurrent approaches, largely based on the ATLOP architecture, are commonly\nevaluated on benchmarks like DocRED and Re-DocRED. However, their performance\nin zero-shot or few-shot settings remains largely underexplored due to the\ntask's complexity. Recently, the GLiNER model has shown that a compact NER\nmodel can outperform much larger Large Language Models. With a similar\nmotivation, we introduce GLiDRE, a new model for document-level relation\nextraction that builds on the key ideas of GliNER. We benchmark GLiDRE against\nstate-of-the-art models across various data settings on the Re-DocRED dataset.\nOur results demonstrate that GLiDRE achieves state-of-the-art performance in\nfew-shot scenarios. Our code is publicly available.", "AI": {"tldr": "本文提出了GLiDRE模型，一个基于GLiNER思想的文档级关系抽取模型，在Re-DocRED数据集的少样本场景中取得了最先进的性能。", "motivation": "文档级关系抽取（RE）由于需要建模跨句子的复杂实体交互而面临挑战，特别是在零样本或少样本设置下性能尚未充分探索。受到GLiNER模型在NER任务中展示的紧凑模型优于大型语言模型的启发，研究者希望为RE任务开发一个类似的高效模型。", "method": "引入了GLiDRE模型，该模型借鉴了GLiNER的关键思想来处理文档级关系抽取。研究者在Re-DocRED数据集上，针对不同的数据设置，将GLiDRE与现有最先进的模型进行了基准测试。", "result": "GLiDRE在少样本场景中取得了最先进的性能。", "conclusion": "GLiDRE是一个有效的文档级关系抽取模型，尤其在处理数据稀缺的少样本场景时表现出色。"}}
{"id": "2508.00412", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00412", "abs": "https://arxiv.org/abs/2508.00412", "authors": ["Hanqi Chen", "Xu Zhang", "Xiaoliu Guan", "Lielin Jiang", "Guanzhong Wang", "Zeyu Chen", "Yi Liu"], "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model", "comment": null, "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.", "AI": {"tldr": "Sortblock是一种无需训练的推理加速框架，通过动态缓存和选择性跳过冗余计算，使Diffusion Transformers（DiTs）的推理速度提升2倍以上，同时保持生成质量。", "motivation": "Diffusion Transformers（DiTs）具有出色的生成能力，但其固有的顺序去噪过程导致推理延迟高，限制了在实时场景中的部署。现有无训练加速方法通常重用固定时间步或层的中间特征，忽略了去噪阶段和Transformer块中语义焦点的演变。", "method": "提出Sortblock框架，该框架基于相邻时间步之间块级特征的相似性动态缓存特征。通过对残差演变进行排序，Sortblock自适应地确定重计算比例，选择性地跳过冗余计算。此外，引入轻量级线性预测机制以减少跳过块中累积的误差。", "result": "在各种任务和DiT架构上的大量实验表明，Sortblock实现了超过2倍的推理加速，同时输出质量退化极小。", "conclusion": "Sortblock为加速基于扩散的生成模型提供了一个有效且通用的解决方案。"}}
{"id": "2508.00748", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00748", "abs": "https://arxiv.org/abs/2508.00748", "authors": ["Laura Pedrouzo-Rodriguez", "Pedro Delgado-DeRobles", "Luis F. Gomez", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez"], "title": "Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos", "comment": "Accepted at the IEEE International Joint Conference on Biometrics\n  (IJCB 2025)", "summary": "Photorealistic talking-head avatars are becoming increasingly common in\nvirtual meetings, gaming, and social platforms. These avatars allow for more\nimmersive communication, but they also introduce serious security risks. One\nemerging threat is impersonation: an attacker can steal a user's\navatar-preserving their appearance and voice-making it nearly impossible to\ndetect its fraudulent usage by sight or sound alone. In this paper, we explore\nthe challenge of biometric verification in such avatar-mediated scenarios. Our\nmain question is whether an individual's facial motion patterns can serve as\nreliable behavioral biometrics to verify their identity when the avatar's\nvisual appearance is a facsimile of its owner. To answer this question, we\nintroduce a new dataset of realistic avatar videos created using a\nstate-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and\nimpostor avatar videos. We also propose a lightweight, explainable\nspatio-temporal Graph Convolutional Network architecture with temporal\nattention pooling, that uses only facial landmarks to model dynamic facial\ngestures. Experimental results demonstrate that facial motion cues enable\nmeaningful identity verification with AUC values approaching 80%. The proposed\nbenchmark and biometric system are available for the research community in\norder to bring attention to the urgent need for more advanced behavioral\nbiometric defenses in avatar-based communication systems.", "AI": {"tldr": "本文探讨在逼真虚拟形象（avatar）场景中，如何利用面部运动模式进行生物识别验证，以应对身份冒充的风险，并提出了一个基于面部关键点和图卷积网络的轻量级验证系统，实现了接近80%的AUC。", "motivation": "逼真的虚拟形象在虚拟会议、游戏和社交平台中日益普及，但同时也带来了严重的安全风险，特别是身份冒充。攻击者可以窃取用户的虚拟形象（保留其外观和声音），使得仅凭视觉或听觉难以检测欺诈行为。因此，迫切需要探索在虚拟形象介导场景中进行生物识别验证的方法。", "method": "为了解决虚拟形象介导的生物识别验证挑战，本文引入了一个新的真实虚拟形象视频数据集，该数据集使用最先进的单次虚拟形象生成模型GAGAvatar创建，包含真实用户和冒充者的虚拟形象视频。同时，提出了一种轻量级、可解释的时空图卷积网络（GCN）架构，该架构结合了时间注意力池化，仅使用面部关键点来建模动态面部手势。", "result": "实验结果表明，面部运动线索能够实现有意义的身份验证，AUC值接近80%。本文提出的基准和生物识别系统已提供给研究社区。", "conclusion": "面部运动模式可以作为可靠的行为生物特征，在虚拟形象介导的通信系统中验证身份。这凸显了在基于虚拟形象的通信系统中，对更先进的行为生物识别防御措施的迫切需求。"}}
{"id": "2508.00760", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00760", "abs": "https://arxiv.org/abs/2508.00760", "authors": ["Qiyao Xue", "Yuchen Dou", "Ryan Shi", "Xiang Lorraine Li", "Wei Gao"], "title": "MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations", "comment": null, "summary": "Hate speech detection on Chinese social networks presents distinct\nchallenges, particularly due to the widespread use of cloaking techniques\ndesigned to evade conventional text-based detection systems. Although large\nlanguage models (LLMs) have recently improved hate speech detection\ncapabilities, the majority of existing work has concentrated on English\ndatasets, with limited attention given to multimodal strategies in the Chinese\ncontext. In this study, we propose MMBERT, a novel BERT-based multimodal\nframework that integrates textual, speech, and visual modalities through a\nMixture-of-Experts (MoE) architecture. To address the instability associated\nwith directly integrating MoE into BERT-based models, we develop a progressive\nthree-stage training paradigm. MMBERT incorporates modality-specific experts, a\nshared self-attention mechanism, and a router-based expert allocation strategy\nto enhance robustness against adversarial perturbations. Empirical results in\nseveral Chinese hate speech datasets show that MMBERT significantly surpasses\nfine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing\nin-context learning approaches.", "AI": {"tldr": "本研究提出了MMBERT，一个基于BERT的多模态框架，结合文本、语音和视觉模态，并采用MoE架构和三阶段训练范式，显著提升了中文仇恨言论检测的性能。", "motivation": "中文社交网络上的仇恨言论检测面临独特挑战，特别是由于规避传统文本检测系统的伪装技术广泛使用。尽管大型语言模型（LLMs）提高了仇恨言论检测能力，但现有工作主要集中在英文数据集，对中文语境下的多模态策略关注有限。", "method": "本研究提出了MMBERT，一个新颖的基于BERT的多模态框架，通过MoE（专家混合）架构整合了文本、语音和视觉模态。为解决MoE直接集成到BERT模型中的不稳定性，开发了渐进式三阶段训练范式。MMBERT包含模态特定专家、共享自注意力机制和基于路由器的专家分配策略，以增强对抗扰动的鲁棒性。", "result": "在多个中文仇恨言论数据集上的实证结果表明，MMBERT显著优于微调的BERT编码器模型、微调的LLMs以及使用上下文学习方法的LLMs。", "conclusion": "MMBERT通过有效整合多模态信息和鲁棒的MoE架构，显著提升了中文仇恨言论检测的性能，克服了现有方法的局限性。"}}
{"id": "2508.00421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00421", "abs": "https://arxiv.org/abs/2508.00421", "authors": ["Runmin Cong", "Zongji Yu", "Hao Fang", "Haoyan Sun", "Sam Kwong"], "title": "UIS-Mamba: Exploring Mamba for Underwater Instance Segmentation via Dynamic Tree Scan and Hidden State Weaken", "comment": "ACM MM 2025", "summary": "Underwater Instance Segmentation (UIS) tasks are crucial for underwater\ncomplex scene detection. Mamba, as an emerging state space model with\ninherently linear complexity and global receptive fields, is highly suitable\nfor processing image segmentation tasks with long sequence features. However,\ndue to the particularity of underwater scenes, there are many challenges in\napplying Mamba to UIS. The existing fixed-patch scanning mechanism cannot\nmaintain the internal continuity of scanned instances in the presence of\nseverely underwater color distortion and blurred instance boundaries, and the\nhidden state of the complex underwater background can also inhibit the\nunderstanding of instance objects. In this work, we propose the first\nMamba-based underwater instance segmentation model UIS-Mamba, and design two\ninnovative modules, Dynamic Tree Scan (DTS) and Hidden State Weaken (HSW), to\nmigrate Mamba to the underwater task. DTS module maintains the continuity of\nthe internal features of the instance objects by allowing the patches to\ndynamically offset and scale, thereby guiding the minimum spanning tree and\nproviding dynamic local receptive fields. HSW module suppresses the\ninterference of complex backgrounds and effectively focuses the information\nflow of state propagation to the instances themselves through the Ncut-based\nhidden state weakening mechanism. Experimental results show that UIS-Mamba\nachieves state-of-the-art performance on both UIIS and USIS10K datasets, while\nmaintaining a low number of parameters and computational complexity. Code is\navailable at https://github.com/Maricalce/UIS-Mamba.", "AI": {"tldr": "该论文提出了首个基于Mamba的水下实例分割模型UIS-Mamba，通过设计动态树扫描（DTS）和隐藏状态削弱（HSW）模块，有效解决了水下复杂场景中Mamba应用的挑战，并在多个数据集上取得了最先进的性能。", "motivation": "水下实例分割（UIS）对于复杂水下场景检测至关重要。Mamba模型具有线性复杂度和全局感受野，适用于图像分割任务。然而，水下场景特有的颜色失真和模糊边界使得Mamba的固定补丁扫描机制难以保持实例内部连续性，且复杂背景的隐藏状态会抑制对实例对象的理解。", "method": "提出了首个基于Mamba的水下实例分割模型UIS-Mamba。设计了两个创新模块：1. 动态树扫描（DTS）模块：允许补丁动态偏移和缩放，引导最小生成树，提供动态局部感受野，以保持实例对象内部特征的连续性。2. 隐藏状态削弱（HSW）模块：通过基于Ncut的隐藏状态削弱机制，抑制复杂背景的干扰，有效将状态传播的信息流聚焦到实例本身。", "result": "实验结果表明，UIS-Mamba在UIIS和USIS10K数据集上均实现了最先进的性能，同时保持了较低的参数量和计算复杂度。", "conclusion": "该研究成功将Mamba模型应用于水下实例分割任务，通过创新的DTS和HSW模块克服了水下场景的特定挑战，显著提升了分割性能，并验证了Mamba在处理水下复杂图像特征方面的潜力。"}}
{"id": "2508.00766", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00766", "abs": "https://arxiv.org/abs/2508.00766", "authors": ["Irene Iele", "Francesco Di Feola", "Valerio Guarrasi", "Paolo Soda"], "title": "Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation", "comment": null, "summary": "Image-to-image translation has emerged as a powerful technique in medical\nimaging, enabling tasks such as image denoising and cross-modality conversion.\nHowever, it suffers from limitations in handling out-of-distribution samples\nwithout causing performance degradation. To address this limitation, we propose\na novel Test-Time Adaptation (TTA) framework that dynamically adjusts the\ntranslation process based on the characteristics of each test sample. Our\nmethod introduces a Reconstruction Module to quantify the domain shift and a\nDynamic Adaptation Block that selectively modifies the internal features of a\npretrained translation model to mitigate the shift without compromising the\nperformance on in-distribution samples that do not require adaptation. We\nevaluate our approach on two medical image-to-image translation tasks: low-dose\nCT denoising and T1 to T2 MRI translation, showing consistent improvements over\nboth the baseline translation model without TTA and prior TTA methods. Our\nanalysis highlights the limitations of the state-of-the-art that uniformly\napply the adaptation to both out-of-distribution and in-distribution samples,\ndemonstrating that dynamic, sample-specific adjustment offers a promising path\nto improve model resilience in real-world scenarios. The code is available at:\nhttps://github.com/cosbidev/Sample-Aware_TTA.", "AI": {"tldr": "本文提出一种新颖的测试时自适应（TTA）框架，用于医学图像到图像的翻译，通过动态调整以应对分布外（OOD）样本，同时保持对分布内样本的性能。", "motivation": "图像到图像的翻译在处理分布外（OOD）样本时性能会下降，这是其在医学成像应用中的一个主要限制。", "method": "引入一个重建模块来量化域偏移，并设计一个动态自适应块，选择性地修改预训练翻译模型的内部特征，以减轻域偏移。这种方法旨在仅对需要自适应的样本进行调整，避免对分布内样本造成性能损害。", "result": "在低剂量CT去噪和T1到T2 MRI翻译两项医学图像翻译任务中，该方法均比没有TTA的基线模型和先前的TTA方法表现出持续的改进。研究表明，动态、样本特异性的调整比统一自适应方法更有效。", "conclusion": "动态、样本特异性的测试时自适应是提高医学图像到图像翻译模型在真实世界场景中鲁棒性的有效途径，优于对所有样本统一应用自适应的现有方法。"}}
{"id": "2508.00762", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00762", "abs": "https://arxiv.org/abs/2508.00762", "authors": ["Atakan Site", "Emre Hakan Erdemir", "Gülşen Eryiğit"], "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation", "comment": null, "summary": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.", "AI": {"tldr": "本文提出了一个基于大型语言模型（LLM）生成Python代码的零样本解决方案，用于在SemEval-2025 Task 8的数据表问答（QA）任务中回答表格数据问题。", "motivation": "参与SemEval-2025 Task 8：DataBench表格数据问答任务，该任务目标是在不同领域的数据表上执行问答，并分为两个子任务（DataBench QA和DataBench Lite QA）。", "method": "开发了一个零样本解决方案，特别强调利用LLM进行代码生成。具体来说，提出了一个Python代码生成框架，通过优化的提示策略，利用最先进的开源LLM生成可执行的Pandas代码。", "result": "实验表明，不同的LLM在Python代码生成方面表现出不同的有效性。此外，Python代码生成在表格问答方面比其他方法取得了更好的性能。在开源模型类别中，该系统在子任务I中排名第八，在子任务II中排名第六（在30个超越基线的系统中）。", "conclusion": "基于LLM生成Python代码的方法在表格数据问答任务中表现出色，尤其是在零样本设置下，并且优于其他替代方案。"}}
{"id": "2508.00443", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00443", "abs": "https://arxiv.org/abs/2508.00443", "authors": ["Longfei Huang", "Yu Liang", "Hao Zhang", "Jinwei Chen", "Wei Dong", "Lunde Chen", "Wanyu Liu", "Bo Li", "Pengtao Jiang"], "title": "SDMatte: Grafting Diffusion Models for Interactive Matting", "comment": "Accepted at ICCV 2025, 11 pages, 4 figures", "summary": "Recent interactive matting methods have shown satisfactory performance in\ncapturing the primary regions of objects, but they fall short in extracting\nfine-grained details in edge regions. Diffusion models trained on billions of\nimage-text pairs, demonstrate exceptional capability in modeling highly complex\ndata distributions and synthesizing realistic texture details, while exhibiting\nrobust text-driven interaction capabilities, making them an attractive solution\nfor interactive matting. To this end, we propose SDMatte, a diffusion-driven\ninteractive matting model, with three key contributions. First, we exploit the\npowerful priors of diffusion models and transform the text-driven interaction\ncapability into visual prompt-driven interaction capability to enable\ninteractive matting. Second, we integrate coordinate embeddings of visual\nprompts and opacity embeddings of target objects into U-Net, enhancing\nSDMatte's sensitivity to spatial position information and opacity information.\nThird, we propose a masked self-attention mechanism that enables the model to\nfocus on areas specified by visual prompts, leading to better performance.\nExtensive experiments on multiple datasets demonstrate the superior performance\nof our method, validating its effectiveness in interactive matting. Our code\nand model are available at https://github.com/vivoCameraResearch/SDMatte.", "AI": {"tldr": "SDMatte是一种新的扩散模型驱动的交互式抠图方法，它利用扩散模型的强大先验和纹理细节生成能力，通过视觉提示交互和改进的U-Net架构，显著提升了抠图在边缘细节上的表现。", "motivation": "现有交互式抠图方法在提取物体主要区域表现良好，但在处理边缘区域的精细细节时效果不佳。扩散模型在建模复杂数据分布、合成真实纹理细节以及强大的文本驱动交互能力方面表现出色，因此被认为是解决交互式抠图挑战的有效方案。", "method": "1. 将扩散模型的文本驱动交互能力转换为视觉提示驱动的交互能力，以实现交互式抠图。2. 将视觉提示的坐标嵌入和目标物体的不透明度嵌入整合到U-Net中，增强模型对空间位置和不透明度信息的敏感性。3. 提出一种蒙版自注意力机制，使模型能够专注于视觉提示指定的区域，从而提高性能。", "result": "在多个数据集上进行的广泛实验表明，SDMatte方法表现出卓越的性能，验证了其在交互式抠图中的有效性。", "conclusion": "SDMatte成功地将扩散模型的优势应用于交互式抠图，通过创新的视觉提示交互、嵌入增强和注意力机制，显著提升了抠图在精细细节提取方面的能力。"}}
{"id": "2508.00788", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00788", "abs": "https://arxiv.org/abs/2508.00788", "authors": ["Xushuo Tang", "Yi Ding", "Zhengyi Yang", "Yin Chen", "Yongrui Gu", "Wenke Yang", "Mingchen Ju", "Xin Cao", "Yongfei Liu", "Wenjie Zhang"], "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.", "AI": {"tldr": "本研究引入了MISGENDERED+基准，评估了最新大型语言模型在处理性别代词（包括中性代词和新代词）方面的准确性，发现虽然在传统代词上有所改进，但在新代词和反向推理任务上仍存在显著不足。", "motivation": "大型语言模型（LLMs）被广泛应用于敏感场景，公平性和包容性至关重要。代词使用，特别是中性代词和新代词，是负责任AI的关键挑战。现有基准（如MISGENDERED）揭示了早期LLMs的局限性，但其模型和评估方法已过时，需要更新和扩展的评估。", "method": "引入并更新了MISGENDERED+基准，用于评估LLMs的代词保真度。选择了五种代表性LLMs（GPT-4o, Claude 4, DeepSeek-V3, Qwen Turbo, Qwen2.5），并在零样本、少样本以及性别身份推理等多种设置下进行基准测试。", "result": "与先前的研究相比，最新LLMs在二元和性别中性代词准确性方面有显著改进。然而，在新代词的准确性以及反向推理任务上表现仍然不一致，这凸显了在身份敏感推理方面持续存在的差距。", "conclusion": "尽管LLMs在处理常规代词方面有所进步，但在处理新代词和进行反向身份推理方面仍存在不足，这表明在实现完全包容的AI方面仍需进一步研究和改进。"}}
{"id": "2508.00819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00819", "abs": "https://arxiv.org/abs/2508.00819", "authors": ["Jinsong Li", "Xiaoyi Dong", "Yuhang Zang", "Yuhang Cao", "Jiaqi Wang", "Dahua Lin"], "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models", "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL", "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.", "AI": {"tldr": "DAEDAL是一种无需训练的去噪策略，通过动态自适应长度扩展，解决了扩散大语言模型（DLLMs）固定生成长度的限制，提升了性能和效率。", "motivation": "扩散大语言模型（DLLMs）虽然有并行生成和全局上下文建模的优势，但其需要预定义静态生成长度的架构限制，导致在复杂任务上性能受损或计算开销过大。作者观察到模型本身包含与最佳响应长度相关的内部信号，希望利用这些信号克服静态长度约束。", "method": "DAEDAL策略分为两个阶段：1) 在去噪过程之前，从一个短的初始长度开始，通过序列完成度指标指导，迭代扩展到一个粗略的、适合任务的长度。2) 在去噪过程中，DAEDAL通过插入掩码标记，动态识别并扩展不足的生成区域，确保最终输出的完整性。此方法无需额外训练。", "result": "实验表明，DAEDAL在DLLMs上的性能与精心调优的固定长度基线相当，在某些情况下甚至更优。同时，通过实现更高的有效token比率，显著提升了计算效率。", "conclusion": "DAEDAL成功解决了DLLMs的静态长度限制，释放了其潜力，缩小了与自回归大语言模型之间的差距，为更高效、更强大的生成铺平了道路。"}}
{"id": "2508.00445", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00445", "abs": "https://arxiv.org/abs/2508.00445", "authors": ["Hongyi Cai", "Mohammad Mahdinur Rahman", "Mingkang Dong", "Jie Li", "Muxin Pu", "Zhili Fang", "Yinan Peng", "Hanjun Luo", "Yang Liu"], "title": "AutoDebias: Automated Framework for Debiasing Text-to-Image Models", "comment": null, "summary": "Text-to-Image (T2I) models generate high-quality images from text prompts but\noften exhibit unintended social biases, such as gender or racial stereotypes,\neven when these attributes are not mentioned. Existing debiasing methods work\nwell for simple or well-known cases but struggle with subtle or overlapping\nbiases. We propose AutoDebias, a framework that automatically identifies and\nmitigates harmful biases in T2I models without prior knowledge of specific bias\ntypes. Specifically, AutoDebias leverages vision-language models to detect\nbiased visual patterns and constructs fairness guides by generating inclusive\nalternative prompts that reflect balanced representations. These guides drive a\nCLIP-guided training process that promotes fairer outputs while preserving the\noriginal model's image quality and diversity. Unlike existing methods,\nAutoDebias effectively addresses both subtle stereotypes and multiple\ninteracting biases. We evaluate the framework on a benchmark covering over 25\nbias scenarios, including challenging cases where multiple biases occur\nsimultaneously. AutoDebias detects harmful patterns with 91.6% accuracy and\nreduces biased outputs from 90% to negligible levels, while preserving the\nvisual fidelity of the original model.", "AI": {"tldr": "AutoDebias是一个自动化框架，利用视觉-语言模型和CLIP引导训练，无需预先知识即可识别并缓解文本到图像（T2I）模型中存在的微妙或重叠的社会偏见。", "motivation": "文本到图像（T2I）模型即使在未明确提及属性的情况下，也经常表现出意外的社会偏见（如性别或种族刻板印象）。现有去偏见方法在简单或已知情况下有效，但在处理微妙或重叠偏见时表现不佳。", "method": "本文提出了AutoDebias框架，它通过以下方式自动识别和缓解T2I模型中的有害偏见：1) 利用视觉-语言模型（VLM）检测有偏见的视觉模式；2) 通过生成反映平衡表示的包容性替代提示来构建“公平指南”；3) 这些指南驱动一个CLIP引导的训练过程，以促进更公平的输出，同时保留原始模型的图像质量和多样性。该方法能够有效处理微妙的刻板印象和多重交互偏见。", "result": "AutoDebias在涵盖25种以上偏见场景（包括多重偏见同时出现的高难度案例）的基准测试中进行了评估。结果显示，它以91.6%的准确率检测有害模式，并将有偏见的输出从90%降低到可忽略的水平，同时保留了原始模型的视觉保真度。", "conclusion": "AutoDebias框架能够有效识别并显著减少文本到图像模型中的各种社会偏见（包括微妙和多重偏见），同时保持图像生成质量，为解决T2I模型的公平性问题提供了一个鲁棒的自动化解决方案。"}}
{"id": "2508.00518", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00518", "abs": "https://arxiv.org/abs/2508.00518", "authors": ["Shuo Liang", "Yiwu Zhong", "Zi-Yuan Hu", "Yeyao Tao", "Liwei Wang"], "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos", "comment": "Accepted by ICCV 2025", "summary": "Spatiotemporal video grounding aims to localize target entities in videos\nbased on textual queries. While existing research has made significant progress\nin exocentric videos, the egocentric setting remains relatively underexplored,\ndespite its growing importance in applications such as augmented reality and\nrobotics. In this work, we conduct a systematic analysis of the discrepancies\nbetween egocentric and exocentric videos, revealing key challenges such as\nshorter object durations, sparser trajectories, smaller object sizes, and\nlarger positional shifts. To address these challenges, we introduce EgoMask,\nthe first pixel-level benchmark for fine-grained spatiotemporal grounding in\negocentric videos. It is constructed by our proposed automatic annotation\npipeline, which annotates referring expressions and object masks across short-,\nmedium-, and long-term videos. Additionally, we create EgoMask-Train, a\nlarge-scale training dataset to facilitate model development. Experiments\ndemonstrate that the state-of-the-art spatiotemporal grounding models perform\npoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields\nsignificant improvements, while preserving performance on exocentric datasets.\nOur work thus provides essential resources and insights for advancing\negocentric video understanding. Our code is available at\nhttps://github.com/LaVi-Lab/EgoMask .", "AI": {"tldr": "该研究系统分析了第一人称视角视频与第三人称视角视频在时空定位任务上的差异，揭示了第一人称视角的挑战，并提出了首个像素级第一人称视角视频时空定位基准数据集EgoMask及其配套训练集EgoMask-Train，实验证明现有模型在该任务上表现不佳，但通过在新数据集上微调可显著提升性能。", "motivation": "现有视频时空定位研究主要集中在第三人称视角视频，而第一人称视角视频（在增强现实、机器人等领域日益重要）仍未得到充分探索。第一人称视角视频与第三人称视角视频存在显著差异，带来了独特的挑战。", "method": "研究首先系统分析了第一人称视角和第三人称视角视频之间的差异。接着，提出了EgoMask，这是首个用于第一人称视角视频精细时空定位的像素级基准数据集，其通过自动标注流程构建，涵盖了短期、中期和长期视频的指代表达和物体掩码。此外，还创建了大规模训练数据集EgoMask-Train。最后，在EgoMask上测试了现有最先进的时空定位模型，并进行了微调实验。", "result": "研究揭示了第一人称视角视频的关键挑战，包括物体持续时间更短、轨迹更稀疏、物体尺寸更小以及位置偏移更大。实验表明，最先进的时空定位模型在EgoMask基准上表现不佳，但在EgoMask-Train上进行微调后，性能得到显著提升，同时在第三人称视角数据集上的性能得以保持。", "conclusion": "该工作为推动第一人称视角视频理解提供了重要的资源（EgoMask和EgoMask-Train）和深刻的见解，解决了该领域现有研究不足的问题。"}}
{"id": "2508.00447", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00447", "abs": "https://arxiv.org/abs/2508.00447", "authors": ["Anju Rani", "Daniel Ortiz-Arroyo", "Petar Durdevic"], "title": "CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text", "comment": "11 pages, 8 figures", "summary": "Understanding the temporal dynamics of biological growth is critical across\ndiverse fields such as microbiology, agriculture, and biodegradation research.\nAlthough vision-language models like Contrastive Language Image Pretraining\n(CLIP) have shown strong capabilities in joint visual-textual reasoning, their\neffectiveness in capturing temporal progression remains limited. To address\nthis, we propose CLIPTime, a multimodal, multitask framework designed to\npredict both the developmental stage and the corresponding timestamp of fungal\ngrowth from image and text inputs. Built upon the CLIP architecture, our model\nlearns joint visual-textual embeddings and enables time-aware inference without\nrequiring explicit temporal input during testing. To facilitate training and\nevaluation, we introduce a synthetic fungal growth dataset annotated with\naligned timestamps and categorical stage labels. CLIPTime jointly performs\nclassification and regression, predicting discrete growth stages alongside\ncontinuous timestamps. We also propose custom evaluation metrics, including\ntemporal accuracy and regression error, to assess the precision of time-aware\npredictions. Experimental results demonstrate that CLIPTime effectively models\nbiological progression and produces interpretable, temporally grounded outputs,\nhighlighting the potential of vision-language models in real-world biological\nmonitoring applications.", "AI": {"tldr": "CLIPTime是一个多模态、多任务框架，用于从图像和文本输入预测真菌生长的发育阶段和对应时间戳，解决了现有视觉语言模型在捕捉时间动态方面的局限性。", "motivation": "理解生物生长的时间动态在微生物学、农业和生物降解研究等领域至关重要。尽管CLIP等视觉语言模型在视觉-文本联合推理方面表现出色，但它们在捕捉时间进程方面的有效性有限。", "method": "本文提出了CLIPTime框架，它基于CLIP架构，学习视觉-文本联合嵌入，并能在测试时无需明确的时间输入即可进行时间感知推理。该模型通过分类和回归联合预测离散的生长阶段和连续的时间戳。为训练和评估，引入了一个带有对齐时间戳和类别阶段标签的合成真菌生长数据集，并提出了包括时间准确性和回归误差在内的自定义评估指标。", "result": "实验结果表明，CLIPTime能有效地模拟生物进程，并产生可解释的、具有时间依据的输出。", "conclusion": "CLIPTime展示了视觉语言模型在实际生物监测应用中的巨大潜力。"}}
{"id": "2508.00453", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00453", "abs": "https://arxiv.org/abs/2508.00453", "authors": ["Baisong Li", "Xingwang Wang", "Haixiao Xu"], "title": "PIF-Net: Ill-Posed Prior Guided Multispectral and Hyperspectral Image Fusion via Invertible Mamba and Fusion-Aware LoRA", "comment": null, "summary": "The goal of multispectral and hyperspectral image fusion (MHIF) is to\ngenerate high-quality images that simultaneously possess rich spectral\ninformation and fine spatial details. However, due to the inherent trade-off\nbetween spectral and spatial information and the limited availability of\nobservations, this task is fundamentally ill-posed. Previous studies have not\neffectively addressed the ill-posed nature caused by data misalignment. To\ntackle this challenge, we propose a fusion framework named PIF-Net, which\nexplicitly incorporates ill-posed priors to effectively fuse multispectral\nimages and hyperspectral images. To balance global spectral modeling with\ncomputational efficiency, we design a method based on an invertible Mamba\narchitecture that maintains information consistency during feature\ntransformation and fusion, ensuring stable gradient flow and process\nreversibility. Furthermore, we introduce a novel fusion module called the\nFusion-Aware Low-Rank Adaptation module, which dynamically calibrates spectral\nand spatial features while keeping the model lightweight. Extensive experiments\non multiple benchmark datasets demonstrate that PIF-Net achieves significantly\nbetter image restoration performance than current state-of-the-art methods\nwhile maintaining model efficiency.", "AI": {"tldr": "PIF-Net是一个多光谱与高光谱图像融合框架，通过引入病态先验、可逆Mamba架构和低秩自适应融合模块，有效解决了数据错位导致的病态问题，提升了图像恢复性能和模型效率。", "motivation": "多光谱与高光谱图像融合（MHIF）旨在生成兼具丰富光谱信息和精细空间细节的高质量图像。然而，由于光谱与空间信息的固有权衡以及观测数据有限，该任务本质上是病态的。先前的研究未能有效解决由数据错位引起的病态性质。", "method": "本文提出了一个名为PIF-Net的融合框架，显式地引入了病态先验。为平衡全局光谱建模和计算效率，设计了一种基于可逆Mamba架构的方法，以在特征转换和融合过程中保持信息一致性。此外，引入了一个名为“融合感知低秩自适应模块”（Fusion-Aware Low-Rank Adaptation module），用于动态校准光谱和空间特征，同时保持模型轻量级。", "result": "在多个基准数据集上进行的广泛实验表明，PIF-Net在图像恢复性能方面显著优于当前的SOTA方法，同时保持了模型效率。", "conclusion": "PIF-Net通过有效整合病态先验和创新的网络架构（如可逆Mamba和融合感知低秩自适应模块），成功解决了多光谱与高光谱图像融合中的病态问题，实现了卓越的融合效果和计算效率。"}}
{"id": "2508.00473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00473", "abs": "https://arxiv.org/abs/2508.00473", "authors": ["Jiaping Cao", "Kangkang Zhou", "Juan Du"], "title": "HyPCV-Former: Hyperbolic Spatio-Temporal Transformer for 3D Point Cloud Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection is a fundamental task in video surveillance, with\nbroad applications in public safety and intelligent monitoring systems.\nAlthough previous methods leverage Euclidean representations in RGB or depth\ndomains, such embeddings are inherently limited in capturing hierarchical event\nstructures and spatio-temporal continuity. To address these limitations, we\npropose HyPCV-Former, a novel hyperbolic spatio-temporal transformer for\nanomaly detection in 3D point cloud videos. Our approach first extracts\nper-frame spatial features from point cloud sequences via point cloud\nextractor, and then embeds them into Lorentzian hyperbolic space, which better\ncaptures the latent hierarchical structure of events. To model temporal\ndynamics, we introduce a hyperbolic multi-head self-attention (HMHA) mechanism\nthat leverages Lorentzian inner products and curvature-aware softmax to learn\ntemporal dependencies under non-Euclidean geometry. Our method performs all\nfeature transformations and anomaly scoring directly within full Lorentzian\nspace rather than via tangent space approximation. Extensive experiments\ndemonstrate that HyPCV-Former achieves state-of-the-art performance across\nmultiple anomaly categories, with a 7\\% improvement on the TIMo dataset and a\n5.6\\% gain on the DAD dataset compared to benchmarks. The code will be released\nupon paper acceptance.", "AI": {"tldr": "提出HyPCV-Former，一种用于3D点云视频异常检测的双曲时空Transformer，通过在洛伦兹双曲空间中建模来捕捉层次结构和时空连续性，超越了传统欧几里得方法的局限性。", "motivation": "传统的视频异常检测方法使用欧几里得表示（RGB或深度），但在捕捉事件的层次结构和时空连续性方面存在固有限制。", "method": "提出HyPCV-Former：首先通过点云提取器从点云序列中提取逐帧空间特征；然后将这些特征嵌入到洛伦兹双曲空间中，以更好地捕获潜在的层次结构；接着引入双曲多头自注意力（HMHA）机制，利用洛伦兹内积和曲率感知softmax在非欧几何下学习时间依赖性；所有特征转换和异常评分都直接在完整的洛伦兹空间中进行，而非通过切线空间近似。", "result": "HyPCV-Former在多个异常类别上实现了最先进的性能，相比基准方法，在TIMo数据集上提升了7%，在DAD数据集上提升了5.6%。", "conclusion": "HyPCV-Former通过利用双曲几何在3D点云视频中有效解决了欧几里得表示的局限性，实现了卓越的异常检测性能。"}}
{"id": "2508.00477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00477", "abs": "https://arxiv.org/abs/2508.00477", "authors": ["Yuzhuo Chen", "Zehua Ma", "Jianhua Wang", "Kai Kang", "Shunyu Yao", "Weiming Zhang"], "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of Multimodal Diffusion Transformer", "comment": "8 pages, 5 figures, 3 tables", "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.", "AI": {"tldr": "LAMIC是一个无需训练的多图像合成框架，首次将单参考扩散模型扩展到多参考场景，通过引入GIA和RMA两种注意力机制，实现了空间布局感知下的连贯一致图像生成，并在多项指标上达到SOTA性能。", "motivation": "在可控图像合成中，从多个参考图像生成具有空间布局感知的连贯一致图像，仍然是一个未解决的挑战。", "method": "LAMIC基于MMDiT模型构建，以无训练方式将单参考扩散模型扩展到多参考场景。它引入了两种即插即用的注意力机制：1) 组隔离注意力（GIA）以增强实体解耦；2) 区域调制注意力（RMA）以实现布局感知生成。此外，该研究还引入了三个新的评估指标：包含率（IN-R）、填充率（FI-R）和背景相似度（BG-S）。", "result": "LAMIC在大多数主要指标上实现了最先进的性能：在所有设置下，其在ID-S、BG-S、IN-R和AVG分数上始终优于现有的多参考基线，并在复杂合成任务中取得了最佳的DPG。这些结果表明LAMIC在保持身份、背景保留、布局控制和遵循提示方面的卓越能力，且无需任何训练或微调，展现出强大的零样本泛化能力。", "conclusion": "LAMIC通过继承先进单参考模型的优势并无缝扩展到多图像场景，为可控多图像合成建立了一个新的无训练范式。随着基础模型的不断发展，LAMIC的性能预计也将相应提升。"}}
{"id": "2508.00493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00493", "abs": "https://arxiv.org/abs/2508.00493", "authors": ["Alfie Roddan", "Tobias Czempiel", "Chi Xu", "Daniel S. Elson", "Stamatia Giannarou"], "title": "SAMSA 2.0: Prompting Segment Anything with Spectral Angles for Hyperspectral Interactive Medical Image Segmentation", "comment": null, "summary": "We present SAMSA 2.0, an interactive segmentation framework for hyperspectral\nmedical imaging that introduces spectral angle prompting to guide the Segment\nAnything Model (SAM) using spectral similarity alongside spatial cues. This\nearly fusion of spectral information enables more accurate and robust\nsegmentation across diverse spectral datasets. Without retraining, SAMSA 2.0\nachieves up to +3.8% higher Dice scores compared to RGB-only models and up to\n+3.1% over prior spectral fusion methods. Our approach enhances few-shot and\nzero-shot performance, demonstrating strong generalization in challenging\nlow-data and noisy scenarios common in clinical imaging.", "AI": {"tldr": "SAMSA 2.0是一个交互式高光谱医学图像分割框架，通过引入光谱角度提示（spectral angle prompting）将光谱相似性与空间线索结合，指导SAM模型进行更准确和鲁棒的分割。", "motivation": "在高光谱医学成像中，需要更准确和鲁棒的分割方法，尤其是在数据量少和噪声大的临床场景中。现有的RGB模型和早期光谱融合方法可能不够有效。", "method": "SAMSA 2.0框架通过光谱角度提示，将光谱信息与空间线索进行早期融合，以指导Segment Anything Model (SAM)进行分割。该方法无需重新训练。", "result": "与仅基于RGB的模型相比，Dice分数提高了高达3.8%；与先前的光谱融合方法相比，Dice分数提高了高达3.1%。该方法增强了少样本（few-shot）和零样本（zero-shot）性能，并在低数据和噪声严重的临床场景中展现出强大的泛化能力。", "conclusion": "SAMSA 2.0通过有效融合光谱信息，显著提高了高光谱医学图像分割的准确性和鲁棒性，尤其在具有挑战性的临床应用中表现出色，且无需重新训练。"}}
{"id": "2508.00506", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00506", "abs": "https://arxiv.org/abs/2508.00506", "authors": ["Tulsi Patel", "Mark W. Jones", "Thomas Redfern"], "title": "Leveraging Convolutional and Graph Networks for an Unsupervised Remote Sensing Labelling Tool", "comment": "Video supplement demonstrating feature-space exploration and\n  interactive labelling is available at: https://youtu.be/GZl1ebZJgEA and is\n  archived at https://doi.org/10.5281/zenodo.16676591", "summary": "Machine learning for remote sensing imaging relies on up-to-date and accurate\nlabels for model training and testing. Labelling remote sensing imagery is time\nand cost intensive, requiring expert analysis. Previous labelling tools rely on\npre-labelled data for training in order to label new unseen data. In this work,\nwe define an unsupervised pipeline for finding and labelling geographical areas\nof similar context and content within Sentinel-2 satellite imagery. Our\napproach removes limitations of previous methods by utilising segmentation with\nconvolutional and graph neural networks to encode a more robust feature space\nfor image comparison. Unlike previous approaches we segment the image into\nhomogeneous regions of pixels that are grouped based on colour and spatial\nsimilarity. Graph neural networks are used to aggregate information about the\nsurrounding segments enabling the feature representation to encode the local\nneighbourhood whilst preserving its own local information. This reduces\noutliers in the labelling tool, allows users to label at a granular level, and\nallows a rotationally invariant semantic relationship at the image level to be\nformed within the encoding space.", "AI": {"tldr": "本文提出了一种无监督的遥感图像标注流程，利用卷积神经网络和图神经网络进行图像分割和特征编码，以克服传统方法的局限性。", "motivation": "遥感图像标注耗时、成本高昂且需要专家知识。现有标注工具依赖预标注数据进行训练，限制了其应用范围。", "method": "该方法定义了一个无监督流程，用于在Sentinel-2卫星图像中查找和标注相似上下文和内容的地理区域。它通过卷积神经网络和图神经网络进行图像分割，将图像分割成基于颜色和空间相似性的同质像素区域。图神经网络用于聚合周围区域的信息，从而在特征表示中编码局部邻域信息，同时保留自身局部信息。", "result": "该方法能够生成更鲁棒的图像比较特征空间，减少标注工具中的异常值，允许用户进行细粒度标注，并在编码空间中形成图像级别的旋转不变语义关系。", "conclusion": "本文提出的无监督管道通过结合先进的神经网络技术，有效解决了遥感图像标注中对预标注数据的依赖问题，并提升了标注的鲁棒性、细粒度和语义一致性。"}}
{"id": "2508.00528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00528", "abs": "https://arxiv.org/abs/2508.00528", "authors": ["Jinsong Yang", "Zeyuan Hu", "Yichen Li"], "title": "EPANet: Efficient Path Aggregation Network for Underwater Fish Detection", "comment": null, "summary": "Underwater fish detection (UFD) remains a challenging task in computer vision\ndue to low object resolution, significant background interference, and high\nvisual similarity between targets and surroundings. Existing approaches\nprimarily focus on local feature enhancement or incorporate complex attention\nmechanisms to highlight small objects, often at the cost of increased model\ncomplexity and reduced efficiency. To address these limitations, we propose an\nefficient path aggregation network (EPANet), which leverages complementary\nfeature integration to achieve accurate and lightweight UFD. EPANet consists of\ntwo key components: an efficient path aggregation feature pyramid network\n(EPA-FPN) and a multi-scale diverse-division short path bottleneck (MS-DDSP\nbottleneck). The EPA-FPN introduces long-range skip connections across\ndisparate scales to improve semantic-spatial complementarity, while cross-layer\nfusion paths are adopted to enhance feature integration efficiency. The MS-DDSP\nbottleneck extends the conventional bottleneck structure by introducing\nfiner-grained feature division and diverse convolutional operations, thereby\nincreasing local feature diversity and representation capacity. Extensive\nexperiments on benchmark UFD datasets demonstrate that EPANet outperforms\nstate-of-the-art methods in terms of detection accuracy and inference speed,\nwhile maintaining comparable or even lower parameter complexity.", "AI": {"tldr": "本文提出了一种高效路径聚合网络（EPANet），用于解决水下鱼类检测中的低分辨率、背景干扰和目标相似性等挑战，通过互补特征集成实现了准确且轻量级的检测。", "motivation": "水下鱼类检测面临目标分辨率低、背景干扰大、目标与环境视觉相似度高等挑战。现有方法主要通过局部特征增强或复杂注意力机制来突出小目标，但往往导致模型复杂性增加和效率降低。", "method": "本文提出了高效路径聚合网络（EPANet），包含两个核心组件：高效路径聚合特征金字塔网络（EPA-FPN）和多尺度多样化分割短路径瓶颈（MS-DDSP bottleneck）。EPA-FPN引入跨尺度长距离跳跃连接和跨层融合路径，以增强语义-空间互补性和特征集成效率。MS-DDSP bottleneck通过更细粒度的特征分割和多样化的卷积操作，扩展了传统瓶颈结构，提升了局部特征多样性和表示能力。", "result": "在基准水下鱼类检测数据集上的大量实验表明，EPANet在检测精度和推理速度方面优于现有最先进方法，同时保持了相当甚至更低的参数复杂度。", "conclusion": "EPANet通过其创新的EPA-FPN和MS-DDSP瓶颈结构，有效解决了水下鱼类检测的挑战，实现了高精度、高效率和低复杂度的性能，证明了其在实际应用中的潜力。"}}
{"id": "2508.00548", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00548", "abs": "https://arxiv.org/abs/2508.00548", "authors": ["Seunghyun Shin", "Dongmin Shin", "Jisu Shin", "Hae-Gon Jeon", "Joon-Young Lee"], "title": "Video Color Grading via Look-Up Table Generation", "comment": "ICCV2025", "summary": "Different from color correction and transfer, color grading involves\nadjusting colors for artistic or storytelling purposes in a video, which is\nused to establish a specific look or mood. However, due to the complexity of\nthe process and the need for specialized editing skills, video color grading\nremains primarily the domain of professional colorists. In this paper, we\npresent a reference-based video color grading framework. Our key idea is\nexplicitly generating a look-up table (LUT) for color attribute alignment\nbetween reference scenes and input video via a diffusion model. As a training\nobjective, we enforce that high-level features of the reference scenes like\nlook, mood, and emotion should be similar to that of the input video. Our\nLUT-based approach allows for color grading without any loss of structural\ndetails in the whole video frames as well as achieving fast inference. We\nfurther build a pipeline to incorporate a user-preference via text prompts for\nlow-level feature enhancement such as contrast and brightness, etc.\nExperimental results, including extensive user studies, demonstrate the\neffectiveness of our approach for video color grading. Codes are publicly\navailable at https://github.com/seunghyuns98/VideoColorGrading.", "AI": {"tldr": "该论文提出一个基于扩散模型的参考视频调色框架，通过生成查找表（LUT）实现参考场景与输入视频之间高层颜色属性的对齐，并支持通过文本提示融入用户偏好，从而实现快速、无损的视频艺术调色。", "motivation": "视频调色是一个复杂且需要专业技能的过程，目前主要由专业调色师完成，限制了非专业人士的使用。", "method": "提出一个基于参考的视频调色框架，核心是通过扩散模型显式生成一个查找表（LUT），用于参考场景与输入视频之间的颜色属性对齐。训练目标是使参考场景的高层特征（如外观、情绪、情感）与输入视频相似。此外，该方法还构建了一个管道，通过文本提示融入用户偏好，以增强低层特征（如对比度、亮度等）。", "result": "实验结果和广泛的用户研究表明，该方法在视频调色方面表现出有效性。该基于LUT的方法能实现视频帧无结构细节损失的调色，并支持快速推理。", "conclusion": "该研究成功提出了一种有效的、基于参考的视频调色框架，通过结合扩散模型和LUT，实现了专业级的艺术调色效果，同时支持用户自定义，降低了专业门槛。"}}
{"id": "2508.00549", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00549", "abs": "https://arxiv.org/abs/2508.00549", "authors": ["Daniel Wolf", "Heiko Hillenhagen", "Billurvan Taskin", "Alex Bäuerle", "Meinrad Beer", "Michael Götz", "Timo Ropinski"], "title": "Your other Left! Vision-Language Models Fail to Identify Relative Positions in Medical Images", "comment": "Accepted at the International Conference on Medical Image Computing\n  and Computer Assisted Intervention (MICCAI) 2025", "summary": "Clinical decision-making relies heavily on understanding relative positions\nof anatomical structures and anomalies. Therefore, for Vision-Language Models\n(VLMs) to be applicable in clinical practice, the ability to accurately\ndetermine relative positions on medical images is a fundamental prerequisite.\nDespite its importance, this capability remains highly underexplored. To\naddress this gap, we evaluate the ability of state-of-the-art VLMs, GPT-4o,\nLlama3.2, Pixtral, and JanusPro, and find that all models fail at this\nfundamental task. Inspired by successful approaches in computer vision, we\ninvestigate whether visual prompts, such as alphanumeric or colored markers\nplaced on anatomical structures, can enhance performance. While these markers\nprovide moderate improvements, results remain significantly lower on medical\nimages compared to observations made on natural images. Our evaluations suggest\nthat, in medical imaging, VLMs rely more on prior anatomical knowledge than on\nactual image content for answering relative position questions, often leading\nto incorrect conclusions. To facilitate further research in this area, we\nintroduce the MIRP , Medical Imaging Relative Positioning, benchmark dataset,\ndesigned to systematically evaluate the capability to identify relative\npositions in medical images.", "AI": {"tldr": "该研究评估了当前最先进的视觉语言模型（VLMs）在医学图像上判断解剖结构相对位置的能力，发现它们普遍失败，并引入了MIRP数据集以促进未来研究。", "motivation": "临床决策严重依赖对解剖结构和异常相对位置的理解。然而，尽管对视觉语言模型（VLMs）在临床应用中准确判断医学图像相对位置的能力至关重要，但这一能力却极少被探索和评估。", "method": "评估了GPT-4o、Llama3.2、Pixtral和JanusPro等最先进的VLMs在医学图像相对位置判断任务上的表现。研究了视觉提示（如字母数字或彩色标记）是否能提升性能。引入了MIRP（Medical Imaging Relative Positioning）基准数据集，专门用于系统评估VLMs识别医学图像相对位置的能力。", "result": "所有被评估的VLMs在判断医学图像相对位置这一基本任务上均失败。视觉提示（如标记）能带来适度改进，但性能远低于在自然图像上的表现。评估表明，在医学影像中，VLMs在回答相对位置问题时更多依赖先验解剖知识而非实际图像内容，这常导致错误结论。", "conclusion": "当前最先进的VLMs在医学图像上的相对位置判断能力非常欠缺，且在医学图像上，它们更依赖于先验知识而非图像内容。为推动该领域研究，本研究引入了MIRP基准数据集，以期解决这一关键能力缺失。"}}
{"id": "2508.00552", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00552", "abs": "https://arxiv.org/abs/2508.00552", "authors": ["Chihan Huang", "Belal Alsinglawi", "Islam Al-qudah"], "title": "DBLP: Noise Bridge Consistency Distillation For Efficient And Reliable Adversarial Purification", "comment": null, "summary": "Recent advances in deep neural networks (DNNs) have led to remarkable success\nacross a wide range of tasks. However, their susceptibility to adversarial\nperturbations remains a critical vulnerability. Existing diffusion-based\nadversarial purification methods often require intensive iterative denoising,\nseverely limiting their practical deployment. In this paper, we propose\nDiffusion Bridge Distillation for Purification (DBLP), a novel and efficient\ndiffusion-based framework for adversarial purification. Central to our approach\nis a new objective, noise bridge distillation, which constructs a principled\nalignment between the adversarial noise distribution and the clean data\ndistribution within a latent consistency model (LCM). To further enhance\nsemantic fidelity, we introduce adaptive semantic enhancement, which fuses\nmulti-scale pyramid edge maps as conditioning input to guide the purification\nprocess. Extensive experiments across multiple datasets demonstrate that DBLP\nachieves state-of-the-art (SOTA) robust accuracy, superior image quality, and\naround 0.2s inference time, marking a significant step toward real-time\nadversarial purification.", "AI": {"tldr": "本文提出DBLP，一种高效的扩散模型框架，用于对抗性净化，通过噪声桥蒸馏和自适应语义增强实现实时性能和SOTA鲁棒性。", "motivation": "深度神经网络易受对抗性扰动攻击，现有基于扩散的对抗性净化方法需要大量迭代去噪，导致实用性受限。", "method": "提出“扩散桥蒸馏净化”（DBLP）框架。核心方法是“噪声桥蒸馏”，在潜在一致性模型（LCM）中构建对抗性噪声分布与干净数据分布之间的对齐。此外，引入“自适应语义增强”，融合多尺度金字塔边缘图作为条件输入，以引导净化过程并提高语义保真度。", "result": "DBLP在多个数据集上实现了最先进（SOTA）的鲁棒准确性，优越的图像质量，并且推理时间约为0.2秒，显著接近实时对抗性净化。", "conclusion": "DBLP是迈向实时对抗性净化的重要一步，有效解决了现有方法的效率和性能瓶颈。"}}
{"id": "2508.00553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00553", "abs": "https://arxiv.org/abs/2508.00553", "authors": ["Jizhihui Liu", "Feiyi Du", "Guangdao Zhu", "Niu Lian", "Jun Li", "Bin Chen"], "title": "HiPrune: Training-Free Visual Token Pruning via Hierarchical Attention in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) encode images into lengthy sequences of visual\ntokens, leading to excessive computational overhead and limited inference\nefficiency. While prior efforts prune or merge tokens to address this issue,\nthey often rely on special tokens (e.g., CLS) or require task-specific\ntraining, hindering scalability across architectures. In this paper, we propose\nHiPrune, a training-free and model-agnostic token Pruning framework that\nexploits the Hierarchical attention structure within vision encoders. We\nidentify that middle layers attend to object-centric regions, while deep layers\ncapture global contextual features. Based on this observation, HiPrune selects\nthree types of informative tokens: (1) Anchor tokens with high attention in\nobject-centric layers, (2) Buffer tokens adjacent to anchors for spatial\ncontinuity, and (3) Register tokens with strong attention in deep layers for\nglobal summarization. Our method requires no retraining and integrates\nseamlessly with any ViT-based VLM. Extensive experiments on LLaVA-1.5,\nLLaVA-NeXT, and Qwen2.5-VL demonstrate that HiPrune achieves state-of-the-art\npruning performance, preserving up to 99.3% task accuracy with only 33.3%\ntokens, and maintaining 99.5% accuracy with just 11.1% tokens. Meanwhile, it\nreduces inference FLOPs and latency by up to 9$\\times$, showcasing strong\ngeneralization across models and tasks. Code is available at\nhttps://github.com/Danielement321/HiPrune.", "AI": {"tldr": "HiPrune是一种免训练、模型无关的视觉语言模型（VLM）令牌剪枝框架，通过利用视觉编码器中的分层注意力结构，选择性地保留关键视觉令牌，显著提高推理效率并保持高精度。", "motivation": "视觉语言模型（VLM）将图像编码为冗长的视觉令牌序列，导致过高的计算开销和有限的推理效率。现有方法通常依赖特殊令牌或需要任务特定训练，限制了其在不同架构间的可扩展性。", "method": "HiPrune利用视觉编码器中的分层注意力结构。它观察到中间层关注以对象为中心的区域，而深层捕获全局上下文特征。基于此，HiPrune选择三种信息丰富的令牌：1) 在对象中心层中具有高注意力的“锚点令牌”；2) 与锚点相邻以保持空间连续性的“缓冲令牌”；3) 在深层中具有强注意力以进行全局总结的“注册令牌”。该方法无需重新训练，可无缝集成到任何基于ViT的VLM中。", "result": "在LLaVA-1.5、LLaVA-NeXT和Qwen2.5-VL上的广泛实验表明，HiPrune实现了最先进的剪枝性能，仅用33.3%的令牌即可保持99.3%的任务准确性，用11.1%的令牌仍能保持99.5%的准确性。同时，它将推理FLOPs和延迟降低了高达9倍，显示出强大的模型和任务泛化能力。", "conclusion": "HiPrune是一个高效且通用的令牌剪枝框架，通过智能利用视觉编码器的分层注意力特性，显著降低了视觉语言模型的计算成本，同时几乎不损失任务精度，为VLM的实际部署提供了有效的解决方案。"}}
{"id": "2508.00557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00557", "abs": "https://arxiv.org/abs/2508.00557", "authors": ["Qi Chen", "Lingxiao Yang", "Yun Chen", "Nailong Zhao", "Jianhuang Lai", "Jie Shao", "Xiaohua Xie"], "title": "Training-Free Class Purification for Open-Vocabulary Semantic Segmentation", "comment": "Accepted to ICCV 2025", "summary": "Fine-tuning pre-trained vision-language models has emerged as a powerful\napproach for enhancing open-vocabulary semantic segmentation (OVSS). However,\nthe substantial computational and resource demands associated with training on\nlarge datasets have prompted interest in training-free methods for OVSS.\nExisting training-free approaches primarily focus on modifying model\narchitectures and generating prototypes to improve segmentation performance.\nHowever, they often neglect the challenges posed by class redundancy, where\nmultiple categories are not present in the current test image, and\nvisual-language ambiguity, where semantic similarities among categories create\nconfusion in class activation. These issues can lead to suboptimal class\nactivation maps and affinity-refined activation maps. Motivated by these\nobservations, we propose FreeCP, a novel training-free class purification\nframework designed to address these challenges. FreeCP focuses on purifying\nsemantic categories and rectifying errors caused by redundancy and ambiguity.\nThe purified class representations are then leveraged to produce final\nsegmentation predictions. We conduct extensive experiments across eight\nbenchmarks to validate FreeCP's effectiveness. Results demonstrate that FreeCP,\nas a plug-and-play module, significantly boosts segmentation performance when\ncombined with other OVSS methods.", "AI": {"tldr": "FreeCP是一个免训练的类别净化框架，旨在解决开放词汇语义分割中类别冗余和视觉语言模糊性问题，通过净化类别表示显著提升分割性能。", "motivation": "微调预训练视觉-语言模型进行开放词汇语义分割（OVSS）计算成本高昂。现有免训练方法忽视了类别冗余（测试图像中不存在的类别）和视觉-语言模糊性（语义相似性导致的类别混淆），导致类别激活图不佳。", "method": "提出FreeCP，一个新颖的免训练类别净化框架。它专注于净化语义类别，纠正由冗余和模糊性引起的错误。净化的类别表示随后用于生成最终分割预测。FreeCP是一个可插拔模块。", "result": "在八个基准测试上进行了广泛实验，结果表明FreeCP作为一个可插拔模块，与其它OVSS方法结合时能显著提升分割性能。", "conclusion": "FreeCP有效解决了免训练OVSS中类别冗余和视觉-语言模糊性带来的挑战，作为一个可插拔模块显著提升了分割表现。"}}
{"id": "2508.00558", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00558", "abs": "https://arxiv.org/abs/2508.00558", "authors": ["Jens U. Kreber", "Joerg Stueckler"], "title": "Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints", "comment": "Accepted for publication at the IEEE/CVF International Conference on\n  Computer Vision (ICCV), 2025", "summary": "Articulated objects are an important type of interactable objects in everyday\nenvironments. In this paper, we propose PhysNAP, a novel diffusion model-based\napproach for generating articulated objects that aligns them with partial point\nclouds and improves their physical plausibility. The model represents part\nshapes by signed distance functions (SDFs). We guide the reverse diffusion\nprocess using a point cloud alignment loss computed using the predicted SDFs.\nAdditionally, we impose non-penetration and mobility constraints based on the\npart SDFs for guiding the model to generate more physically plausible objects.\nWe also make our diffusion approach category-aware to further improve point\ncloud alignment if category information is available. We evaluate the\ngenerative ability and constraint consistency of samples generated with PhysNAP\nusing the PartNet-Mobility dataset. We also compare it with an unguided\nbaseline diffusion model and demonstrate that PhysNAP can improve constraint\nconsistency and provides a tradeoff with generative ability.", "AI": {"tldr": "PhysNAP是一种新颖的基于扩散模型的方法，用于生成铰接物体，能够与部分点云对齐并提高物理合理性。", "motivation": "铰接物体是日常环境中重要的可交互对象，研究旨在生成更符合物理规律并能与部分点云对齐的铰接物体。", "method": "该方法名为PhysNAP，采用扩散模型，使用符号距离函数（SDFs）表示部件形状。通过使用预测SDFs计算的点云对齐损失来引导反向扩散过程。此外，基于部件SDFs施加非穿透和可移动性约束，以生成更符合物理规律的物体。模型还支持类别感知以进一步改善点云对齐。", "result": "在PartNet-Mobility数据集上评估了PhysNAP的生成能力和约束一致性。与无引导的基线扩散模型相比，PhysNAP能提高约束一致性，并在生成能力上有所权衡。", "conclusion": "PhysNAP能够有效地生成物理上更合理并与部分点云对齐的铰接物体，显著提高了约束一致性。"}}
{"id": "2508.00563", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00563", "abs": "https://arxiv.org/abs/2508.00563", "authors": ["Hannah Kniesel", "Leon Sick", "Tristan Payer", "Tim Bergner", "Kavitha Shaga Devan", "Clarissa Read", "Paul Walther", "Timo Ropinski"], "title": "Weakly Supervised Virus Capsid Detection with Image-Level Annotations in Electron Microscopy Images", "comment": null, "summary": "Current state-of-the-art methods for object detection rely on annotated\nbounding boxes of large data sets for training. However, obtaining such\nannotations is expensive and can require up to hundreds of hours of manual\nlabor. This poses a challenge, especially since such annotations can only be\nprovided by experts, as they require knowledge about the scientific domain. To\ntackle this challenge, we propose a domain-specific weakly supervised object\ndetection algorithm that only relies on image-level annotations, which are\nsignificantly easier to acquire. Our method distills the knowledge of a\npre-trained model, on the task of predicting the presence or absence of a virus\nin an image, to obtain a set of pseudo-labels that can be used to later train a\nstate-of-the-art object detection model. To do so, we use an optimization\napproach with a shrinking receptive field to extract virus particles directly\nwithout specific network architectures. Through a set of extensive studies, we\nshow how the proposed pseudo-labels are easier to obtain, and, more\nimportantly, are able to outperform other existing weak labeling methods, and\neven ground truth labels, in cases where the time to obtain the annotation is\nlimited.", "AI": {"tldr": "该论文提出了一种领域特定的弱监督目标检测算法，仅依赖图像级标注来生成伪标签，用于训练目标检测模型，以解决边界框标注昂贵且耗时的问题。", "motivation": "当前最先进的目标检测方法依赖大量带有标注边界框的数据集进行训练，但获取这些标注成本高昂、耗时，且需要领域专家知识，特别是在科学领域。", "method": "该方法通过从一个预训练模型（用于预测图像中是否存在病毒）中提取知识，生成一组伪标签。具体而言，它采用一种带有收缩感受野的优化方法，直接提取病毒颗粒，无需特定的网络架构。", "result": "实验表明，所提出的伪标签更容易获取，并且在标注时间有限的情况下，其性能优于其他现有的弱标注方法，甚至能超越真实标签。", "conclusion": "该研究成功地通过图像级标注生成高质量的伪标签，有效解决了领域特定目标检测中边界框标注成本高昂的挑战，并展现出优越的性能。"}}
{"id": "2508.00568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00568", "abs": "https://arxiv.org/abs/2508.00568", "authors": ["Jingchao Xie", "Oussema Dhaouadi", "Weirong Chen", "Johannes Meier", "Jacques Kaiser", "Daniel Cremers"], "title": "CoProU-VO: Combining Projected Uncertainty for End-to-End Unsupervised Monocular Visual Odometry", "comment": "Accepted for GCPR 2025. Project page:\n  https://jchao-xie.github.io/CoProU/", "summary": "Visual Odometry (VO) is fundamental to autonomous navigation, robotics, and\naugmented reality, with unsupervised approaches eliminating the need for\nexpensive ground-truth labels. However, these methods struggle when dynamic\nobjects violate the static scene assumption, leading to erroneous pose\nestimations. We tackle this problem by uncertainty modeling, which is a\ncommonly used technique that creates robust masks to filter out dynamic objects\nand occlusions without requiring explicit motion segmentation. Traditional\nuncertainty modeling considers only single-frame information, overlooking the\nuncertainties across consecutive frames. Our key insight is that uncertainty\nmust be propagated and combined across temporal frames to effectively identify\nunreliable regions, particularly in dynamic scenes. To address this challenge,\nwe introduce Combined Projected Uncertainty VO (CoProU-VO), a novel end-to-end\napproach that combines target frame uncertainty with projected reference frame\nuncertainty using a principled probabilistic formulation. Built upon vision\ntransformer backbones, our model simultaneously learns depth, uncertainty\nestimation, and camera poses. Consequently, experiments on the KITTI and\nnuScenes datasets demonstrate significant improvements over previous\nunsupervised monocular end-to-end two-frame-based methods and exhibit strong\nperformance in challenging highway scenes where other approaches often fail.\nAdditionally, comprehensive ablation studies validate the effectiveness of\ncross-frame uncertainty propagation.", "AI": {"tldr": "本文提出CoProU-VO，一种新的端到端无监督单目视觉里程计方法，通过结合目标帧和投影参考帧的不确定性，有效处理动态场景中的挑战，显著提升位姿估计的鲁棒性。", "motivation": "传统的无监督视觉里程计方法在动态场景中表现不佳，因为它们假设场景是静态的，导致动态物体破坏了这一假设，产生错误的位姿估计。现有不确定性建模通常只考虑单帧信息，忽略了连续帧之间的不确定性传播。", "method": "引入CoProU-VO，一种新颖的端到端方法。它通过概率公式结合目标帧不确定性和投影参考帧不确定性，实现了跨时间帧的不确定性传播和组合。该模型基于视觉Transformer骨干网络，同时学习深度、不确定性估计和相机位姿。", "result": "在KITTI和nuScenes数据集上的实验表明，CoProU-VO比以往的无监督单目端到端双帧方法有显著改进，并在其他方法常失败的挑战性高速公路场景中表现出色。全面的消融研究验证了跨帧不确定性传播的有效性。", "conclusion": "CoProU-VO通过引入跨时间帧的不确定性传播和组合，有效解决了无监督视觉里程计在动态场景中的挑战，显著提高了位姿估计的准确性和鲁棒性。"}}
{"id": "2508.00587", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00587", "abs": "https://arxiv.org/abs/2508.00587", "authors": ["Marc Hölle", "Walter Kellermann", "Vasileios Belagiannis"], "title": "Uncertainty-Aware Likelihood Ratio Estimation for Pixel-Wise Out-of-Distribution Detection", "comment": "Accepted at ICCVW 2025, 11 pages, 4 figures", "summary": "Semantic segmentation models trained on known object classes often fail in\nreal-world autonomous driving scenarios by confidently misclassifying unknown\nobjects. While pixel-wise out-of-distribution detection can identify unknown\nobjects, existing methods struggle in complex scenes where rare object classes\nare often confused with truly unknown objects. We introduce an\nuncertainty-aware likelihood ratio estimation method that addresses these\nlimitations. Our approach uses an evidential classifier within a likelihood\nratio test to distinguish between known and unknown pixel features from a\nsemantic segmentation model, while explicitly accounting for uncertainty.\nInstead of producing point estimates, our method outputs probability\ndistributions that capture uncertainty from both rare training examples and\nimperfect synthetic outliers. We show that by incorporating uncertainty in this\nway, outlier exposure can be leveraged more effectively. Evaluated on five\nstandard benchmark datasets, our method achieves the lowest average false\npositive rate (2.5%) among state-of-the-art while maintaining high average\nprecision (90.91%) and incurring only negligible computational overhead. Code\nis available at https://github.com/glasbruch/ULRE.", "AI": {"tldr": "本文提出了一种不确定性感知的似然比估计方法（ULRE），用于语义分割模型中的像素级未知对象（OOD）检测，通过显式处理不确定性来区分已知和未知特征，有效解决了现有方法在复杂场景中将稀有已知对象误识别为未知对象的问题。", "motivation": "现有语义分割模型在自动驾驶等真实场景中，对于未知对象常常自信地错误分类。尽管像素级OOD检测可以识别未知对象，但现有方法在复杂场景中表现不佳，容易将稀有的已知对象与真正的未知对象混淆。", "method": "该方法引入了一种不确定性感知的似然比估计。它在似然比检验中使用证据分类器（evidential classifier），以区分语义分割模型中的已知和未知像素特征，并明确考虑不确定性。不同于产生点估计，该方法输出概率分布，捕捉了稀有训练样本和不完美的合成异常值带来的不确定性，从而更有效地利用了异常值暴露（outlier exposure）。", "result": "在五个标准基准数据集上进行评估，该方法实现了最1低的平均误报率（2.5%），同时保持了较高的平均精度（90.91%），并且计算开销可以忽略不计。", "conclusion": "通过将不确定性纳入似然比估计中，所提出的方法能够更有效地识别未知对象，显著提高了像素级OOD检测的性能，尤其是在区分稀有已知对象和真正未知对象方面表现出色，且计算效率高。"}}
{"id": "2508.00592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00592", "abs": "https://arxiv.org/abs/2508.00592", "authors": ["Jiajun Le", "Jiayi Ma"], "title": "GeoMoE: Divide-and-Conquer Motion Field Modeling with Mixture-of-Experts for Two-View Geometry", "comment": null, "summary": "Recent progress in two-view geometry increasingly emphasizes enforcing\nsmoothness and global consistency priors when estimating motion fields between\npairs of images. However, in complex real-world scenes, characterized by\nextreme viewpoint and scale changes as well as pronounced depth\ndiscontinuities, the motion field often exhibits diverse and heterogeneous\nmotion patterns. Most existing methods lack targeted modeling strategies and\nfail to explicitly account for this variability, resulting in estimated motion\nfields that diverge from their true underlying structure and distribution. We\nobserve that Mixture-of-Experts (MoE) can assign dedicated experts to motion\nsub-fields, enabling a divide-and-conquer strategy for heterogeneous motion\npatterns. Building on this insight, we re-architect motion field modeling in\ntwo-view geometry with GeoMoE, a streamlined framework. Specifically, we first\ndevise a Probabilistic Prior-Guided Decomposition strategy that exploits inlier\nprobability signals to perform a structure-aware decomposition of the motion\nfield into heterogeneous sub-fields, sharply curbing outlier-induced bias.\nNext, we introduce an MoE-Enhanced Bi-Path Rectifier that enhances each\nsub-field along spatial-context and channel-semantic paths and routes it to a\ncustomized expert for targeted modeling, thereby decoupling heterogeneous\nmotion regimes, suppressing cross-sub-field interference and representational\nentanglement, and yielding fine-grained motion-field rectification. With this\nminimalist design, GeoMoE outperforms prior state-of-the-art methods in\nrelative pose and homography estimation and shows strong generalization. The\nsource code and pre-trained models are available at\nhttps://github.com/JiajunLe/GeoMoE.", "AI": {"tldr": "GeoMoE是一种新框架，通过采用MoE（专家混合）架构，并结合概率先验引导分解和双路径整流器，有效处理复杂场景中双视图几何中的异构运动模式，提升了位姿和单应性估计的性能。", "motivation": "现有方法在复杂真实世界场景（极端视角/尺度变化、深度不连续性）中估计运动场时，难以有效处理其多样和异构的运动模式，缺乏针对性的建模策略，导致估计结果偏离真实结构和分布。", "method": "本文提出了GeoMoE框架。首先，设计了“概率先验引导分解”策略，利用内点概率信号将运动场分解为异构子场，减少异常值引起的偏差。其次，引入了“MoE增强双路径整流器”，沿空间上下文和通道语义路径增强每个子场，并将其路由到定制专家进行目标建模，从而解耦异构运动机制，抑制交叉子场干扰和表征纠缠，实现精细化的运动场校正。", "result": "GeoMoE在相对位姿和单应性估计方面超越了现有最先进的方法，并表现出强大的泛化能力。", "conclusion": "GeoMoE通过其简约设计，成功地重新构建了双视图几何中的运动场建模，有效处理了异构运动模式，显著提升了位姿和单应性估计的性能。"}}
{"id": "2508.00599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00599", "abs": "https://arxiv.org/abs/2508.00599", "authors": ["Junzhe Lu", "Jing Lin", "Hongkun Dou", "Ailing Zeng", "Yue Deng", "Xian Liu", "Zhongang Cai", "Lei Yang", "Yulun Zhang", "Haoqian Wang", "Ziwei Liu"], "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior", "comment": "ICCV 2025 (oral); Code released: https://github.com/moonbow721/DPoser", "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.", "AI": {"tldr": "DPoser-X是一个基于扩散模型的3D全身人体姿态先验模型，通过创新的训练和采样方法，解决了全身姿态建模的复杂性和数据稀缺问题，并在多项基准测试中超越了现有技术。", "motivation": "构建通用且鲁棒的全身人体姿态先验模型极具挑战性，原因在于人体姿态固有的复杂性以及高质量全身姿态数据集的稀缺性。", "method": "引入扩散模型作为姿态先验（DPoser并扩展为DPoser-X），将各种以姿态为中心的问题统一为逆问题并通过变分扩散采样求解。为提高下游应用性能，提出了专为姿态数据设计的截断时间步调度方法。此外，提出了掩码训练机制，有效结合全身和局部数据集，捕获身体部位间相互依赖性同时避免过拟合。", "result": "DPoser-X在身体、手、面部和全身姿态建模的多个基准测试中展现出鲁棒性和通用性。该模型持续优于现有最先进的替代方案，为全身人体姿态先验建模建立了新的基准。", "conclusion": "DPoser-X成功地作为一个强大的扩散基3D全身人体姿态先验模型，解决了姿态建模的挑战，并在多项任务中实现了卓越性能，设定了新的行业标准。"}}
{"id": "2508.00639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00639", "abs": "https://arxiv.org/abs/2508.00639", "authors": ["Luisa Gallée", "Catharina Silvia Lisson", "Christoph Gerhard Lisson", "Daniela Drees", "Felix Weig", "Daniel Vogele", "Meinrad Beer", "Michael Götz"], "title": "Minimum Data, Maximum Impact: 20 annotated samples for explainable lung nodule classification", "comment": "Accepted at iMIMIC - Interpretability of Machine Intelligence in\n  Medical Image Computing workshop MICCAI 2025 Medical Image Computing and\n  Computer Assisted Intervention", "summary": "Classification models that provide human-interpretable explanations enhance\nclinicians' trust and usability in medical image diagnosis. One research focus\nis the integration and prediction of pathology-related visual attributes used\nby radiologists alongside the diagnosis, aligning AI decision-making with\nclinical reasoning. Radiologists use attributes like shape and texture as\nestablished diagnostic criteria and mirroring these in AI decision-making both\nenhances transparency and enables explicit validation of model outputs.\nHowever, the adoption of such models is limited by the scarcity of large-scale\nmedical image datasets annotated with these attributes. To address this\nchallenge, we propose synthesizing attribute-annotated data using a generative\nmodel. We enhance the Diffusion Model with attribute conditioning and train it\nusing only 20 attribute-labeled lung nodule samples from the LIDC-IDRI dataset.\nIncorporating its generated images into the training of an explainable model\nboosts performance, increasing attribute prediction accuracy by 13.4% and\ntarget prediction accuracy by 1.8% compared to training with only the small\nreal attribute-annotated dataset. This work highlights the potential of\nsynthetic data to overcome dataset limitations, enhancing the applicability of\nexplainable models in medical image analysis.", "AI": {"tldr": "该研究提出使用生成模型（扩散模型）合成带有属性标注的医学图像，以解决医学图像诊断中可解释AI模型所需属性标注数据稀缺的问题，从而提高模型性能和适用性。", "motivation": "临床医生在医学图像诊断中需要可解释的AI模型来增强信任和可用性，这要求AI决策与临床推理（如使用病理相关视觉属性）对齐。然而，缺乏大规模带有这些属性标注的医学图像数据集，限制了此类模型的应用。", "method": "研究提出使用生成模型合成属性标注数据。具体方法是增强扩散模型，使其能够进行属性条件生成，并仅使用来自LIDC-IDRI数据集中20个带有属性标注的肺结节样本进行训练。然后，将生成的图像整合到可解释模型的训练中。", "result": "与仅使用少量真实属性标注数据集进行训练相比，将生成的图像纳入训练后，可解释模型的性能得到提升。属性预测准确率提高了13.4%，目标预测准确率提高了1.8%。", "conclusion": "这项工作强调了合成数据在克服数据集限制方面的潜力，从而增强了可解释模型在医学图像分析中的适用性。"}}
{"id": "2508.00649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00649", "abs": "https://arxiv.org/abs/2508.00649", "authors": ["Junhao Zheng", "Jiahao Sun", "Chenhao Lin", "Zhengyu Zhao", "Chen Ma", "Chong Zhang", "Cong Wang", "Qian Wang", "Chao Shen"], "title": "Revisiting Adversarial Patch Defenses on Object Detectors: Unified Evaluation, Large-Scale Dataset, and New Insights", "comment": null, "summary": "Developing reliable defenses against patch attacks on object detectors has\nattracted increasing interest. However, we identify that existing defense\nevaluations lack a unified and comprehensive framework, resulting in\ninconsistent and incomplete assessments of current methods. To address this\nissue, we revisit 11 representative defenses and present the first patch\ndefense benchmark, involving 2 attack goals, 13 patch attacks, 11 object\ndetectors, and 4 diverse metrics. This leads to the large-scale adversarial\npatch dataset with 94 types of patches and 94,000 images. Our comprehensive\nanalyses reveal new insights: (1) The difficulty in defending against\nnaturalistic patches lies in the data distribution, rather than the commonly\nbelieved high frequencies. Our new dataset with diverse patch distributions can\nbe used to improve existing defenses by 15.09% AP@0.5. (2) The average\nprecision of the attacked object, rather than the commonly pursued patch\ndetection accuracy, shows high consistency with defense performance. (3)\nAdaptive attacks can substantially bypass existing defenses, and defenses with\ncomplex/stochastic models or universal patch properties are relatively robust.\nWe hope that our analyses will serve as guidance on properly evaluating patch\nattacks/defenses and advancing their design. Code and dataset are available at\nhttps://github.com/Gandolfczjh/APDE, where we will keep integrating new\nattacks/defenses.", "AI": {"tldr": "本文提出了首个针对目标检测器补丁攻击防御的综合基准测试，揭示了现有评估的不足，并提供了新的防御见解和评估指标。", "motivation": "现有针对目标检测器补丁攻击的防御评估缺乏统一和全面的框架，导致对当前方法的评估不一致和不完整。", "method": "研究者重新审视了11种代表性防御方法，构建了第一个补丁防御基准，包括2个攻击目标、13种补丁攻击、11个目标检测器和4种不同的评估指标。他们还创建了一个包含94种补丁类型和94,000张图像的大规模对抗性补丁数据集。", "result": "(1) 防御自然补丁的难度在于数据分布而非高频特性，新数据集可将现有防御性能提升15.09% AP@0.5。(2) 被攻击对象的平均精度（AP）与防御性能高度一致，而非常用的补丁检测准确率。(3) 自适应攻击能显著绕过现有防御，而具有复杂/随机模型或通用补丁特性的防御相对更鲁棒。", "conclusion": "本研究的分析为正确评估补丁攻击/防御及其设计提供了指导，并有望推动该领域的发展。"}}
{"id": "2508.00698", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00698", "abs": "https://arxiv.org/abs/2508.00698", "authors": ["Hongfei Zhang", "Kun Zhou", "Ruizheng Wu", "Jiangbo Lu"], "title": "Can Large Pretrained Depth Estimation Models Help With Image Dehazing?", "comment": "Submitted to AAAI2026", "summary": "Image dehazing remains a challenging problem due to the spatially varying\nnature of haze in real-world scenes. While existing methods have demonstrated\nthe promise of large-scale pretrained models for image dehazing, their\narchitecture-specific designs hinder adaptability across diverse scenarios with\ndifferent accuracy and efficiency requirements. In this work, we systematically\ninvestigate the generalization capability of pretrained depth\nrepresentations-learned from millions of diverse images-for image dehazing. Our\nempirical analysis reveals that the learned deep depth features maintain\nremarkable consistency across varying haze levels. Building on this insight, we\npropose a plug-and-play RGB-D fusion module that seamlessly integrates with\ndiverse dehazing architectures. Extensive experiments across multiple\nbenchmarks validate both the effectiveness and broad applicability of our\napproach.", "AI": {"tldr": "本文系统研究了预训练深度表征在图像去雾中的泛化能力，发现其在不同雾度下具有一致性，并提出一个即插即用的RGB-D融合模块，以提高去雾方法的适应性和性能。", "motivation": "由于真实场景中雾霾的空间变异性，图像去雾仍具挑战。现有方法依赖于特定架构设计，导致在不同精度和效率要求的场景中适应性差。", "method": "系统研究了从大量图像中学习到的预训练深度表征在图像去雾中的泛化能力，并通过实证分析揭示了深度特征在不同雾度水平下保持显著一致性。在此基础上，提出了一个即插即用的RGB-D融合模块，可无缝集成到各种去雾架构中。", "result": "经验分析表明，学习到的深度特征在不同雾度水平下保持了显著的一致性。在多个基准测试上的大量实验验证了所提方法的有效性和广泛适用性。", "conclusion": "预训练深度表征对图像去雾具有强大的泛化能力和一致性。所提出的RGB-D融合模块能有效提升去雾方法的性能和适应性，使其能集成到多种去雾架构中。"}}
{"id": "2508.00726", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00726", "abs": "https://arxiv.org/abs/2508.00726", "authors": ["Jiale Li", "Mingrui Wu", "Zixiang Jin", "Hao Chen", "Jiayi Ji", "Xiaoshuai Sun", "Liujuan Cao", "Rongrong Ji"], "title": "MIHBench: Benchmarking and Mitigating Multi-Image Hallucinations in Multimodal Large Language Models", "comment": "ACM MM25 has accepted this paper", "summary": "Despite growing interest in hallucination in Multimodal Large Language\nModels, existing studies primarily focus on single-image settings, leaving\nhallucination in multi-image scenarios largely unexplored. To address this gap,\nwe conduct the first systematic study of hallucinations in multi-image MLLMs\nand propose MIHBench, a benchmark specifically tailored for evaluating\nobject-related hallucinations across multiple images. MIHBench comprises three\ncore tasks: Multi-Image Object Existence Hallucination, Multi-Image Object\nCount Hallucination, and Object Identity Consistency Hallucination, targeting\nsemantic understanding across object existence, quantity reasoning, and\ncross-view identity consistency. Through extensive evaluation, we identify key\nfactors associated with the occurrence of multi-image hallucinations,\nincluding: a progressive relationship between the number of image inputs and\nthe likelihood of hallucination occurrences; a strong correlation between\nsingle-image hallucination tendencies and those observed in multi-image\ncontexts; and the influence of same-object image ratios and the positional\nplacement of negative samples within image sequences on the occurrence of\nobject identity consistency hallucination. To address these challenges, we\npropose a Dynamic Attention Balancing mechanism that adjusts inter-image\nattention distributions while preserving the overall visual attention\nproportion. Experiments across multiple state-of-the-art MLLMs demonstrate that\nour method effectively reduces hallucination occurrences and enhances semantic\nintegration and reasoning stability in multi-image scenarios.", "AI": {"tldr": "该研究首次系统性探讨多图像多模态大语言模型（MLLMs）中的幻觉问题，提出了MIHBench基准用于评估对象相关幻觉，并识别出多图像输入量、单图像幻觉倾向、同对象图像比例和负样本位置等关键影响因素。为解决这些问题，研究提出了一种动态注意力平衡机制，实验证明其能有效减少幻觉并提升模型在多图像场景下的语义整合和推理稳定性。", "motivation": "现有关于多模态大语言模型幻觉的研究主要集中在单图像场景，而多图像场景下的幻觉问题尚未得到充分探索。", "method": "研究方法包括：1) 对多图像MLLMs中的幻觉进行首次系统性研究；2) 提出MIHBench基准，专门用于评估跨多图像的对象相关幻觉，包含对象存在幻觉、对象计数幻觉和对象身份一致性幻觉三个核心任务；3) 提出一种动态注意力平衡机制（Dynamic Attention Balancing mechanism），旨在调整图像间注意力分布，同时保持整体视觉注意力比例。", "result": "主要研究结果包括：1) 识别出与多图像幻觉发生相关的关键因素，包括图像输入数量与幻觉发生概率的递进关系、单图像幻觉倾向与多图像幻觉的强相关性，以及同对象图像比例和负样本在图像序列中位置对对象身份一致性幻觉的影响；2) 所提出的动态注意力平衡机制在多个最先进的MLLMs上进行实验，结果表明其能有效减少幻觉发生，并增强多图像场景下的语义整合和推理稳定性。", "conclusion": "多图像MLLMs中的幻觉是一个重要且未被充分研究的问题。本研究通过提出专门的评估基准和创新的动态注意力平衡机制，不仅揭示了多图像幻觉的关键影响因素，还为有效减少幻觉、提升MLLMs在复杂多图像场景下的鲁棒性提供了有效途径。"}}
{"id": "2508.00728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00728", "abs": "https://arxiv.org/abs/2508.00728", "authors": ["Guanning Zeng", "Xiang Zhang", "Zirui Wang", "Haiyang Xu", "Zeyuan Chen", "Bingnan Li", "Zhuowen Tu"], "title": "YOLO-Count: Differentiable Object Counting for Text-to-Image Generation", "comment": "ICCV 2025", "summary": "We propose YOLO-Count, a differentiable open-vocabulary object counting model\nthat tackles both general counting challenges and enables precise quantity\ncontrol for text-to-image (T2I) generation. A core contribution is the\n'cardinality' map, a novel regression target that accounts for variations in\nobject size and spatial distribution. Leveraging representation alignment and a\nhybrid strong-weak supervision scheme, YOLO-Count bridges the gap between\nopen-vocabulary counting and T2I generation control. Its fully differentiable\narchitecture facilitates gradient-based optimization, enabling accurate object\ncount estimation and fine-grained guidance for generative models. Extensive\nexperiments demonstrate that YOLO-Count achieves state-of-the-art counting\naccuracy while providing robust and effective quantity control for T2I systems.", "AI": {"tldr": "YOLO-Count是一个可微分的开放词汇对象计数模型，通过引入“基数”图来解决通用计数挑战，并为文本到图像（T2I）生成提供精确数量控制。", "motivation": "现有模型在通用对象计数方面存在挑战，且难以对文本到图像（T2I）生成中的对象数量进行精确控制。", "method": "提出YOLO-Count模型，核心是引入“基数”图作为新的回归目标，以适应对象尺寸和空间分布的变化。模型采用表示对齐和混合强弱监督方案，并具有完全可微分的架构，便于基于梯度的优化。", "result": "YOLO-Count在计数准确性上达到了最先进的水平，并能为T2I系统提供鲁棒且有效的数量控制。", "conclusion": "YOLO-Count成功地弥合了开放词汇计数与T2I生成控制之间的差距，在计数精度和生成模型指导方面表现出色。"}}
{"id": "2508.00744", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00744", "abs": "https://arxiv.org/abs/2508.00744", "authors": ["Adwait Chandorkar", "Hasan Tercan", "Tobias Meisen"], "title": "Rethinking Backbone Design for Lightweight 3D Object Detection in LiDAR", "comment": "accepted at the Embedded Vision Workshop ICCV 2025", "summary": "Recent advancements in LiDAR-based 3D object detection have significantly\naccelerated progress toward the realization of fully autonomous driving in\nreal-world environments. Despite achieving high detection performance, most of\nthe approaches still rely on a VGG-based or ResNet-based backbone for feature\nexploration, which increases the model complexity. Lightweight backbone design\nis well-explored for 2D object detection, but research on 3D object detection\nstill remains limited. In this work, we introduce Dense Backbone, a lightweight\nbackbone that combines the benefits of high processing speed, lightweight\narchitecture, and robust detection accuracy. We adapt multiple SoTA 3d object\ndetectors, such as PillarNet, with our backbone and show that with our\nbackbone, these models retain most of their detection capability at a\nsignificantly reduced computational cost. To our knowledge, this is the first\ndense-layer-based backbone tailored specifically for 3D object detection from\npoint cloud data. DensePillarNet, our adaptation of PillarNet, achieves a 29%\nreduction in model parameters and a 28% reduction in latency with just a 2%\ndrop in detection accuracy on the nuScenes test set. Furthermore, Dense\nBackbone's plug-and-play design allows straightforward integration into\nexisting architectures, requiring no modifications to other network components.", "AI": {"tldr": "本文提出了一种名为Dense Backbone的轻量级骨干网络，专门用于LiDAR点云3D目标检测，旨在显著降低模型复杂度和计算成本，同时保持高检测性能。", "motivation": "尽管LiDAR-based 3D目标检测取得了显著进展，但大多数现有方法仍依赖于VGG或ResNet等复杂骨干网络，导致模型复杂度高。2D目标检测的轻量级骨干网络设计已得到充分探索，但3D目标检测领域的研究仍然有限。", "method": "引入了Dense Backbone，这是一种结合了高处理速度、轻量级架构和鲁棒检测精度的骨干网络。该网络是首个专门为点云数据3D目标检测设计的基于密集层（dense-layer-based）的骨干。通过将Dense Backbone适配到PillarNet等现有SoTA 3D目标检测器中，验证其有效性。", "result": "将PillarNet与Dense Backbone结合后形成的DensePillarNet，在nuScenes测试集上实现了29%的模型参数减少和28%的延迟降低，而检测精度仅下降2%。此外，Dense Backbone采用即插即用设计，可直接集成到现有架构中，无需修改其他网络组件。", "conclusion": "Dense Backbone为3D目标检测提供了一个高效、轻量且准确的解决方案，它能显著降低计算成本和模型复杂度，同时保持高性能，并且易于集成到现有模型中，有望加速自动驾驶技术的实际应用。"}}
{"id": "2508.00746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00746", "abs": "https://arxiv.org/abs/2508.00746", "authors": ["Regine Hartwig", "Dominik Muhle", "Riccardo Marin", "Daniel Cremers"], "title": "GECO: Geometrically Consistent Embedding with Lightspeed Inference", "comment": null, "summary": "Recent advances in feature learning have shown that self-supervised vision\nfoundation models can capture semantic correspondences but often lack awareness\nof underlying 3D geometry. GECO addresses this gap by producing geometrically\ncoherent features that semantically distinguish parts based on geometry (e.g.,\nleft/right eyes, front/back legs). We propose a training framework based on\noptimal transport, enabling supervision beyond keypoints, even under occlusions\nand disocclusions. With a lightweight architecture, GECO runs at 30 fps, 98.2%\nfaster than prior methods, while achieving state-of-the-art performance on\nPFPascal, APK, and CUB, improving PCK by 6.0%, 6.2%, and 4.1%, respectively.\nFinally, we show that PCK alone is insufficient to capture geometric quality\nand introduce new metrics and insights for more geometry-aware feature\nlearning. Link to project page:\nhttps://reginehartwig.github.io/publications/geco/", "AI": {"tldr": "GECO是一种基于最优传输的几何连贯特征学习框架，旨在弥补自监督视觉模型在3D几何感知上的不足，实现语义与几何双重区分，同时运行速度快、性能卓越，并引入了新的几何评估指标。", "motivation": "现有的自监督视觉基础模型虽然能捕获语义对应关系，但往往缺乏对底层3D几何的感知，导致无法区分几何上对立的部位（如左右眼、前后腿）。", "method": "GECO提出一个基于最优传输的训练框架，允许超越关键点的监督，即使在遮挡和非遮挡情况下也能工作。采用轻量级架构。", "result": "GECO以30 fps运行，比现有方法快98.2%，同时在PFPascal、APK和CUB数据集上达到了最先进的性能，PCK分别提升了6.0%、6.2%和4.1%。此外，研究表明PCK不足以捕捉几何质量，并引入了新的度量标准和见解，以实现更几何感知的特征学习。", "conclusion": "GECO成功解决了自监督模型中几何感知不足的问题，通过其高效和高性能证明了该方法的有效性，并强调了未来需要更全面的几何质量评估指标。"}}
{"id": "2508.00777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00777", "abs": "https://arxiv.org/abs/2508.00777", "authors": ["Zihan Wang", "Samira Ebrahimi Kahou", "Narges Armanfard"], "title": "Zero-Shot Anomaly Detection with Dual-Branch Prompt Learning", "comment": "Accepted at BMVC 2025", "summary": "Zero-shot anomaly detection (ZSAD) enables identifying and localizing defects\nin unseen categories by relying solely on generalizable features rather than\nrequiring any labeled examples of anomalies. However, existing ZSAD methods,\nwhether using fixed or learned prompts, struggle under domain shifts because\ntheir training data are derived from limited training domains and fail to\ngeneralize to new distributions. In this paper, we introduce PILOT, a framework\ndesigned to overcome these challenges through two key innovations: (1) a novel\ndual-branch prompt learning mechanism that dynamically integrates a pool of\nlearnable prompts with structured semantic attributes, enabling the model to\nadaptively weight the most relevant anomaly cues for each input image; and (2)\na label-free test-time adaptation strategy that updates the learnable prompt\nparameters using high-confidence pseudo-labels from unlabeled test data.\nExtensive experiments on 13 industrial and medical benchmarks demonstrate that\nPILOT achieves state-of-the-art performance in both anomaly detection and\nlocalization under domain shift.", "AI": {"tldr": "PILOT提出了一种新的零样本异常检测框架，通过双分支提示学习和无标签测试时自适应策略，有效解决了现有方法在域偏移下的泛化性问题。", "motivation": "现有的零样本异常检测（ZSAD）方法，无论是使用固定还是学习到的提示，在面对域偏移时表现不佳，因为它们的训练数据来源于有限的训练域，难以泛化到新的数据分布。", "method": "本文提出了PILOT框架，包含两项关键创新：1) 一种新颖的双分支提示学习机制，动态整合可学习提示与结构化语义属性，使模型能自适应地为每个输入图像加权最相关的异常线索；2) 一种无标签测试时自适应策略，利用无标签测试数据中的高置信度伪标签来更新可学习提示参数。", "result": "在13个工业和医疗基准上的大量实验表明，PILOT在域偏移下的异常检测和定位方面均达到了最先进的性能。", "conclusion": "PILOT框架通过其创新的提示学习和测试时自适应策略，成功克服了零样本异常检测在域偏移下的挑战，实现了卓越的泛化能力。"}}
{"id": "2508.00822", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00822", "abs": "https://arxiv.org/abs/2508.00822", "authors": ["Alexander Nikitas Dimopoulos", "Joseph Grasso"], "title": "Cross-Dataset Semantic Segmentation Performance Analysis: Unifying NIST Point Cloud City Datasets for 3D Deep Learning", "comment": null, "summary": "This study analyzes semantic segmentation performance across heterogeneously\nlabeled point-cloud datasets relevant to public safety applications, including\npre-incident planning systems derived from lidar scans. Using NIST's Point\nCloud City dataset (Enfield and Memphis collections), we investigate challenges\nin unifying differently labeled 3D data. Our methodology employs a graded\nschema with the KPConv architecture, evaluating performance through IoU metrics\non safety-relevant features. Results indicate performance variability:\ngeometrically large objects (e.g. stairs, windows) achieve higher segmentation\nperformance, suggesting potential for navigational context, while smaller\nsafety-critical features exhibit lower recognition rates. Performance is\nimpacted by class imbalance and the limited geometric distinction of smaller\nobjects in typical lidar scans, indicating limitations in detecting certain\nsafety-relevant features using current point-cloud methods. Key identified\nchallenges include insufficient labeled data, difficulties in unifying class\nlabels across datasets, and the need for standardization. Potential directions\ninclude automated labeling and multi-dataset learning strategies. We conclude\nthat reliable point-cloud semantic segmentation for public safety necessitates\nstandardized annotation protocols and improved labeling techniques to address\ndata heterogeneity and the detection of small, safety-critical elements.", "AI": {"tldr": "本研究分析了公共安全领域中异构标注点云数据的语义分割性能，发现大型物体分割效果较好，但小型安全关键特征识别率低，主要受数据不平衡和标注标准化缺失影响。", "motivation": "研究在公共安全应用（如激光雷达扫描衍生的事前规划系统）中，跨异构标注点云数据集的语义分割性能，并调查统一不同标注3D数据所面临的挑战。", "method": "使用NIST的点云城市数据集（Enfield和Memphis集合），采用分级模式结合KPConv架构，并通过IoU指标评估安全相关特征的分割性能。", "result": "结果显示性能存在差异：几何上较大的物体（如楼梯、窗户）分割性能较高，而较小的安全关键特征识别率较低。性能受类别不平衡和小型物体几何区分度低的限制。主要挑战包括标记数据不足、数据集间类别标签统一困难以及缺乏标准化。", "conclusion": "为了实现公共安全领域可靠的点云语义分割，需要标准化标注协议和改进的标注技术，以解决数据异构性以及小型安全关键元素的检测问题。"}}
