<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.CV](#cs.CV) [Total: 179]
- [cs.CL](#cs.CL) [Total: 66]
- [cs.RO](#cs.RO) [Total: 43]
- [eess.SY](#eess.SY) [Total: 24]
- [eess.IV](#eess.IV) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: 提出双向RAG架构，通过验证写入高质量生成响应实现知识库安全扩展，相比标准RAG覆盖度提升近一倍，文档增量减少72%。


<details>
  <summary>Details</summary>
Motivation: 传统RAG使用静态知识库无法从用户交互中学习进化，限制了系统持续改进能力。

Method: 采用多阶段接受层（基于NLI的蕴含验证、归因检查、新颖性检测）实现安全知识写入，防止幻觉污染。

Result: 在四个数据集上平均覆盖率达40.58%（标准RAG为20.33%），仅写入140份文档（朴素写入需500份）。

Conclusion: 通过严格验证机制，自改进RAG系统可行且安全，为部署中学习的RAG系统提供了实践路径。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [2] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在未被明确提示的情况下进行说服的风险，发现监督微调（SFT）比内部激活引导更能引发模型的无提示说服行为，且基于良性话题的SFT可能导致模型在有害话题上产生说服倾向。


<details>
  <summary>Details</summary>
Motivation: 随着对话AI系统的广泛应用，AI对人类观点和信念的影响日益增强。先前研究主要关注模型在被恶意提示时的说服行为，但缺乏对模型在未被明确提示情况下自发产生说服倾向的研究，这关系到对新兴风险的评估。

Method: 研究通过两种场景分析无提示说服行为：（1）通过内部激活引导使模型具备特定人格特质；（2）通过监督微调（SFT）使模型展现相同特质。实验涉及与说服相关及无关的特质，并测试模型在争议性和有害话题上的说服倾向。

Result: 内部激活引导未能可靠增加模型的无提示说服倾向，而监督微调（SFT）则显著增强了这种倾向。此外，基于良性话题的SFT训练出的模型在争议性和有害话题上也表现出更高的说服倾向，表明有害说服可能意外涌现。

Conclusion: 监督微调可能无意中引发模型的无提示有害说服行为，这种新兴风险需要进一步研究。研究强调了在模型开发中需谨慎评估SFT对模型行为的影响，特别是对潜在有害倾向的监测。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [3] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: 提出了GamiBench基准，通过折纸任务评估多模态大语言模型的空间推理和2D到3D规划能力，发现现有先进模型在此类任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在空间推理（跨视图和时间跟踪操纵物体）方面存在不足，而当前基准多关注静态图像或最终输出，缺乏对序列性和视角依赖性的评估。

Method: 构建包含186个常规和186个不可能折痕图案的数据集，对应六种视角的3D折叠形状，通过三个VQA任务（预测3D折叠配置、区分有效视角、检测不可能图案）进行评估，并引入视角一致性和不可能折叠选择率等新指标。

Result: 实验表明，即使是GPT-5和Gemini-2.5-Pro等领先模型在单步空间理解任务上也表现困难。

Conclusion: GamiBench为评估MLLM的几何理解和空间推理提供了标准化框架，揭示了当前模型在空间推理方面的局限性。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [4] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: 本文提出了一种公平感知的人工智能框架，用于优化孟加拉国洪水后的援助分配，通过对抗性去偏技术减少对边缘化地区的系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 发展中国家灾后援助分配存在系统性偏见，边缘化地区常处于不利地位，这加剧了历史不平等。研究旨在通过公平AI技术，确保援助基于真实需求而非历史分配模式。

Method: 采用对抗性去偏模型，结合梯度反转层学习偏差不变表示，将医疗AI中的公平表征学习技术应用于灾害管理，并使用2022年孟加拉国洪水真实数据进行验证。

Result: 在11个地区的87个upazilas上测试，模型将统计均等差异降低41.6%，区域公平差距减少43.2%，同时保持较高预测精度（R平方=0.784，基线为0.811）。

Conclusion: 该框架证明了算法公平技术可有效应用于人道主义场景，为决策者提供工具以实现更公平的灾后恢复策略，确保援助优先分配给最脆弱群体。

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [5] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: 本文提出了一个名为ARC框架的技术治理框架，旨在帮助组织识别、评估和缓解由智能体AI系统带来的风险。


<details>
  <summary>Details</summary>
Motivation: 智能体AI系统因其自主行动能力（如代码执行、网络交互和文件修改）带来了显著机遇和新型风险，这对组织的有效治理构成了重大挑战。

Method: 开发了一个以能力为中心的分析视角，提炼了智能体AI系统的三个主要风险来源（组件、设计和能力），并建立了风险源、具体风险和相应技术控制之间的清晰联系。

Result: ARC框架提供了一个结构化且实用的方法，帮助组织在快速创新的同时，确保智能体AI系统的安全、可靠和负责任部署。

Conclusion: 该框架为组织应对智能体AI的复杂性提供了稳健且适应性强的方法论，并已开源供广泛使用。

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [6] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: 研究发现人类难以区分AI生成图像与真实照片，平均准确率仅54%，略高于随机猜测。


<details>
  <summary>Details</summary>
Motivation: 尽管许多人自信能区分AI生成图像与真实照片，但缺乏实证研究验证这一假设。随着合成媒体质量提升，了解人类实际辨别能力变得重要。

Method: 通过交互式网络实验，让165名参与者对120张精心挑选的图像（60张真实照片来自CC12M，60张由MidJourney生成的对应图像）进行分类判断，共收集233次实验数据。

Result: 参与者平均准确率为54%，仅略高于随机水平；平均响应时间7.3秒；重复尝试后改善有限；某些图像具有系统性欺骗性。

Conclusion: 即使在相对简单的人像图像上，人类也难以可靠检测AI生成内容。随着合成媒体技术进步，仅靠人类判断已不足够，需要提高公众意识并制定伦理准则。

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [7] [Shape of Thought: When Distribution Matters More than Correctness in Reasoning Tasks](https://arxiv.org/abs/2512.22255)
*Abhranil Chandra,Ayush Agrawal,Arian Hosseini,Sebastian Fischmeister,Rishabh Agarwal,Navin Goyal,Aaron Courville*

Main category: cs.AI

TL;DR: 研究发现，即使思维链推理轨迹最终答案错误，使用更强大模型生成的合成数据训练语言模型，也能提升其推理能力，效果优于人类标注数据。


<details>
  <summary>Details</summary>
Motivation: 探索如何有效提升语言模型的推理能力，特别是研究合成数据（即使包含错误答案）是否比人类标注数据更有助于模型学习推理过程。

Method: 使用更强大模型生成思维链推理轨迹（即使最终答案错误）构建合成数据集进行训练；通过模型对人工标注轨迹进行改写以接近模型分布；引入逐步增加的推理缺陷研究模型容忍度；在数学、算法推理和代码生成等任务（MATH、GSM8K等数据集）上测试多种1.5B-9B规模模型。

Result: 合成数据训练效果优于人类标注数据；将人工数据改写至更接近模型分布可提升性能；模型对部分错误的推理轨迹仍能学习有效推理步骤；最终答案正确性不能完全代表推理过程可靠性。

Conclusion: 数据分布与模型自身分布的接近程度是提升推理能力的关键因素；错误的思维链轨迹可能包含有价值的推理步骤；应重新评估以最终答案正确性作为数据质量唯一标准的方法。

Abstract: We present the surprising finding that a language model's reasoning capabilities can be improved by training on synthetic datasets of chain-of-thought (CoT) traces from more capable models, even when all of those traces lead to an incorrect final answer. Our experiments show this approach can yield better performance on reasoning tasks than training on human-annotated datasets. We hypothesize that two key factors explain this phenomenon: first, the distribution of synthetic data is inherently closer to the language model's own distribution, making it more amenable to learning. Second, these `incorrect' traces are often only partially flawed and contain valid reasoning steps from which the model can learn. To further test the first hypothesis, we use a language model to paraphrase human-annotated traces -- shifting their distribution closer to the model's own distribution -- and show that this improves performance. For the second hypothesis, we introduce increasingly flawed CoT traces and study to what extent models are tolerant to these flaws. We demonstrate our findings across various reasoning domains like math, algorithmic reasoning and code generation using MATH, GSM8K, Countdown and MBPP datasets on various language models ranging from 1.5B to 9B across Qwen, Llama, and Gemma models. Our study shows that curating datasets that are closer to the model's distribution is a critical aspect to consider. We also show that a correct final answer is not always a reliable indicator of a faithful reasoning process.

</details>


### [8] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: 提出Logic Sketch Prompting (LSP)框架，通过引入类型变量、确定性条件评估器和基于规则的验证器，显著提升大语言模型在需要严格规则遵循的任务上的准确性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在自然语言推理方面表现出色，但在需要严格规则遵循、确定性和可审计性的任务上仍不可靠，特别是在临床、受监管和安全关键领域。

Method: 提出LSP提示框架，包含类型变量、确定性条件评估器和基于规则的验证器；使用两个药理学逻辑合规任务，在Gemma 2、Mistral和Llama 3三个开源模型上，对比零样本提示、思维链提示和简洁提示进行基准测试。

Result: LSP在所有模型和任务中均取得最高准确率（0.83-0.89）和F1分数（0.83-0.89），显著优于其他方法；McNemar检验显示LSP在几乎所有比较中均有统计学显著提升（p<0.01）。

Conclusion: LSP在不牺牲性能的前提下，提高了大语言模型输出的确定性、可解释性和一致性，适用于临床、受监管和安全关键决策支持系统。

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [9] [SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence](https://arxiv.org/abs/2512.22334)
*Yiheng Wang,Yixin Chen,Shuo Li,Yifan Zhou,Bo Liu,Hengjian Gao,Jiakang Yuan,Jia Bu,Wanghan Xu,Yuhao Zhou,Xiangyu Zhao,Zhiwang Zhou,Fengxiang Wang,Haodong Duan,Songyang Zhang,Jun Yao,Han Deng,Yizhou Wang,Jiabei Xiao,Jiaqi Liu,Encheng Su,Yujie Liu,Weida Wang,Junchi Yao,Shenghe Zheng,Haoran Sun,Runmin Ma,Xiangchao Yan,Bo Zhang,Dongzhan Zhou,Shufei Zhang,Peng Ye,Xiaosong Wang,Shixiang Tang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: SciEvalKit是一个统一的科学AI模型评估工具包，专注于科学智能核心能力评估，覆盖多学科领域，提供可扩展的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有通用评估平台无法充分评估AI模型在科学领域的专业能力，需要专门针对科学智能核心能力构建标准化评估工具。

Method: 构建基于真实科学数据集的专家级基准测试，支持多模态感知、推理、理解、符号推理、代码生成、假设生成和知识理解等能力评估，涵盖六大科学领域，提供灵活可扩展的评估流水线。

Result: 开发了开源的科学AI评估工具包，支持批量评估、自定义模型和数据集集成，提供透明、可复现、可比较的评估结果。

Conclusion: SciEvalKit通过结合能力评估和学科多样性，为下一代科学基础模型和智能体提供了标准化且可定制的基准测试基础设施，促进AI4Science的社区发展。

Abstract: We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.

</details>


### [10] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: 提出Agent2World框架，通过多智能体协作生成可执行的世界模型，并利用交互反馈进行监督微调，在PDDL和代码表示上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态验证，难以捕捉交互执行中的行为级错误，且缺乏大规模可验证监督数据来训练LLM生成符号世界模型。

Method: 三阶段多智能体框架：1) Deep Researcher通过网页搜索填补知识缺口；2) Model Developer实现可执行世界模型；3) Testing Team进行自适应单元测试和模拟验证，生成交互反馈用于监督微调。

Result: 在三个基准测试（PDDL和可执行代码）上实现最先进性能；利用反馈轨迹微调的模型使世界模型生成平均相对提升30.95%。

Conclusion: Agent2World通过工具增强的多智能体框架，在推理时生成高质量世界模型，同时作为数据引擎提升模型训练效果，解决了符号世界模型生成的监督数据稀缺和验证难题。

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [11] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 提出了一种将带控制参数的数字规划问题编译为简单数字任务的方法，使传统启发式函数能处理无限动作空间的问题。


<details>
  <summary>Details</summary>
Motivation: 带控制参数的数字规划会因动作参数作为自由变量而产生无限动作空间，导致现有数字启发式方法无法直接应用。

Method: 识别可控简单数字问题子类，通过乐观编译将控制依赖表达式抽象为有界常量效应和宽松前提条件，转化为简单数字任务。

Result: 该方法能有效利用子目标启发式估计目标距离，在含控制参数的数字规划中实现计算可行性。

Conclusion: 所提方法扩展了传统数字启发式在无限动作空间问题中的应用边界，提升了当前技术水平。

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [12] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 提出HalluMatData基准数据集和HalluMatDetector多阶段检测框架，用于评估和缓解材料科学领域AI生成内容中的幻觉问题，将幻觉率降低30%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在科学发现中产生事实错误或误导性信息（幻觉）的问题威胁研究完整性，需要专门针对材料科学领域的评估和缓解方案。

Method: 1. 构建HalluMatData基准数据集；2. 开发HalluMatDetector框架，整合内在验证、多源检索、矛盾图分析和基于指标的评估；3. 提出PHCS指标量化语义等效查询下的响应不一致性。

Result: 1. 材料科学不同子领域的幻觉水平差异显著，高熵查询的事实不一致性更高；2. HalluMatDetector验证流程将幻觉率较标准LLM输出降低30%；3. PHCS指标能有效揭示模型可靠性。

Conclusion: 该研究为材料科学领域的AI幻觉检测提供了系统化解决方案，通过专用数据集、检测框架和量化指标，显著提升了AI生成内容的可信度。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [13] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias是一种轻量级推理时个性化框架，通过结构门控适配将冻结的知识图谱嵌入适应到个体用户上下文，在保持全局准确性的同时显著提升个性化排名性能。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型在链接预测中表现出强大的群体级性能，但未能捕捉个体用户偏好，导致通用关系推理与个性化排名之间存在关键脱节。

Method: 提出结构门控适配方法：结合用户特定特征和图导出的二元门生成可解释的每实体偏置，仅需约300个可训练参数，无需重新训练基础模型。

Result: 在两个基准数据集（Amazon-Book和Last-FM）上，GatedBias在保持群体性能的同时显著提升对齐指标；反事实扰动实验显示，受益于特定偏好信号的实体在信号增强时排名改进提高6-30倍。

Conclusion: 基础模型的个性化适配可以实现参数高效且因果可验证，有效桥接通用知识表示与个体用户需求。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [14] [Monadic Context Engineering](https://arxiv.org/abs/2512.22431)
*Yifan Zhang,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Monadic Context Engineering（MCE）的新型架构范式，利用函子、应用函子和单子的代数结构为AI智能体设计提供形式化基础，旨在解决当前智能体架构在状态管理、错误处理和并发性方面的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型驱动的自主智能体架构通常采用命令式、临时性的设计模式，导致系统脆弱，难以有效管理状态、处理错误和实现并发执行。

Method: 提出MCE范式，将智能体工作流视为计算上下文，利用函子、应用函子和单子的代数特性内在管理跨领域关注点（如状态传播、错误处理和异步执行），并通过单子变换器系统组合这些能力。

Result: MCE通过单子实现鲁棒的顺序组合，通过应用函子提供并行执行的结构化方法，使开发者能够从简单、可独立验证的组件构建复杂、弹性和高效的AI智能体。

Conclusion: MCE为智能体设计提供了形式化基础，支持构建元智能体进行生成式编排，通过元编程动态创建和管理子智能体工作流，显著提升了智能体系统的可靠性和可组合性。

Abstract: The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming. Project Page: https://github.com/yifanzhang-pro/monadic-context-engineering.

</details>


### [15] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 提出了DarkPatterns-LLM基准数据集和诊断框架，用于细粒度评估大语言模型输出中的操纵性内容，涵盖七类危害，并发现现有模型在检测自主性损害模式方面存在明显弱点。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的普及加剧了对其操纵性或欺骗性行为的担忧，现有安全基准主要依赖粗粒度的二元标签，无法捕捉构成操纵的微妙心理和社会机制。

Method: 构建了包含401个精心策划示例的数据集，采用四层分析管道：多粒度检测、多尺度意图分析、威胁协调协议和深度上下文风险对齐，评估了GPT-4、Claude 3.5和LLaMA-3-70B等先进模型。

Result: 观察到模型性能存在显著差异（65.2%–89.7%），在检测自主性损害模式方面表现一致较弱。

Conclusion: DarkPatterns-LLM建立了首个标准化、多维度的LLM操纵检测基准，为构建更可信的AI系统提供了可操作的诊断工具。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [16] [Multi-AI Agent Framework Reveals the "Oxide Gatekeeper" in Aluminum Nanoparticle Oxidation](https://arxiv.org/abs/2512.22529)
*Yiming Lu,Tingyu Lu,Di Zhang,Lili Ye,Hao Li*

Main category: cs.AI

TL;DR: 本研究通过人机协同的闭环框架开发机器学习势函数，揭示了铝纳米颗粒在燃烧过程中的原子尺度氧化机制，解决了长期存在的质量传输争议。


<details>
  <summary>Details</summary>
Motivation: 铝纳米颗粒作为高能量密度固体燃料，其从钝化态到爆炸反应的原子机制尚不明确，传统计算方法在精度与尺度间存在根本性瓶颈。

Method: 采用'人在回路'的闭环框架，通过自审计AI代理验证机器学习势函数的演化，结合量子力学精度与百万原子系统的近线性扩展能力。

Result: 发现温度调控的双模式氧化机制：中温下氧化壳层通过瞬态纳米通道的'呼吸模式'调控氧化；超过临界温度后'破裂模式'引发爆炸燃烧。证实铝阳离子外扩散（而非氧传输）在所有温度区间主导质量传输，扩散系数高出2-3个数量级。

Conclusion: 建立了高能纳米材料设计的统一原子尺度框架，为通过智能计算设计实现点火敏感性和能量释放速率的精确调控提供了基础。

Abstract: Aluminum nanoparticles (ANPs) are among the most energy-dense solid fuels, yet the atomic mechanisms governing their transition from passivated particles to explosive reactants remain elusive. This stems from a fundamental computational bottleneck: ab initio methods offer quantum accuracy but are restricted to small spatiotemporal scales (< 500 atoms, picoseconds), while empirical force fields lack the reactive fidelity required for complex combustion environments. Herein, we bridge this gap by employing a "human-in-the-loop" closed-loop framework where self-auditing AI Agents validate the evolution of a machine learning potential (MLP). By acting as scientific sentinels that visualize hidden model artifacts for human decision-making, this collaborative cycle ensures quantum mechanical accuracy while exhibiting near-linear scalability to million-atom systems and accessing nanosecond timescales (energy RMSE: 1.2 meV/atom, force RMSE: 0.126 eV/Angstrom). Strikingly, our simulations reveal a temperature-regulated dual-mode oxidation mechanism: at moderate temperatures, the oxide shell acts as a dynamic "gatekeeper," regulating oxidation through a "breathing mode" of transient nanochannels; above a critical threshold, a "rupture mode" unleashes catastrophic shell failure and explosive combustion. Importantly, we resolve a decades-old controversy by demonstrating that aluminum cation outward diffusion, rather than oxygen transport, dominates mass transfer across all temperature regimes, with diffusion coefficients consistently exceeding those of oxygen by 2-3 orders of magnitude. These discoveries establish a unified atomic-scale framework for energetic nanomaterial design, enabling the precision engineering of ignition sensitivity and energy release rates through intelligent computational design.

</details>


### [17] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 本文主张将神经科学中的预测编码理论整合到基础模型中，通过加入动作、层级组合结构和情景记忆来改进当前大语言模型，以实现更安全、可解释、节能且类人的人工智能。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型仅基于下一词预测损失进行优化，忽略了预测编码理论中动作整合、层级组合结构和情景记忆三个关键组件，导致模型存在幻觉、缺乏概念深度理解、缺乏代理感/责任感、可解释性不足及能效低下等问题。

Method: 提出理论框架，将神经科学与认知科学中预测编码模型的三个缺失组件（动作与生成模型的紧密整合、层级组合结构、情景记忆）整合到基础模型中，并与当前技术趋势（如思维链推理和检索增强生成）进行比较分析。

Result: 论证了整合这些组件可帮助解决基础模型的现有缺陷：通过 grounding 减少幻觉、通过控制增强代理感、通过可解释性提升安全与可信度，并提高能效。

Conclusion: 重新激活脑科学与AI之间的思想交流，将有助于推动实现安全、可解释、以人为中心的人工智能。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [18] [SANet: A Semantic-aware Agentic AI Networking Framework for Cross-layer Optimization in 6G](https://arxiv.org/abs/2512.22579)
*Yong Xiao,Xubo Li,Haoran Zhou,Yingyu Li,Yayu Gao,Guangming Shi,Ping Zhang,Marwan Krunz*

Main category: cs.AI

TL;DR: 本文提出了一种面向无线网络的语义感知智能体网络架构SANet，通过多智能体协作实现用户语义目标的推断与网络资源分配，并开发了模型分区共享框架和分布式优化算法。


<details>
  <summary>Details</summary>
Motivation: 智能体网络作为去中心化框架，协作智能体常具有不同甚至冲突的目标，需要解决多智能体多目标优化问题并寻找帕累托最优解。

Method: 提出语义感知智能体网络架构SANet、模型分区共享框架MoPS、两种分布式优化算法，并构建了基于RAN和核心网络的硬件原型。

Result: 实验表明该框架性能提升最高达14.61%，且仅需最先进算法44.37%的浮点运算量；理论分析揭示了优化、泛化与冲突误差间的三重权衡。

Conclusion: SANet通过语义感知与分布式协作优化，在降低计算开销的同时显著提升了无线网络自主管理性能，为智能体网络的实际部署提供了可行方案。

Abstract: Agentic AI networking (AgentNet) is a novel AI-native networking paradigm in which a large number of specialized AI agents collaborate to perform autonomous decision-making, dynamic environmental adaptation, and complex missions. It has the potential to facilitate real-time network management and optimization functions, including self-configuration, self-optimization, and self-adaptation across diverse and complex environments. This paper proposes SANet, a novel semantic-aware AgentNet architecture for wireless networks that can infer the semantic goal of the user and automatically assign agents associated with different layers of the network to fulfill the inferred goal. Motivated by the fact that AgentNet is a decentralized framework in which collaborating agents may generally have different and even conflicting objectives, we formulate the decentralized optimization of SANet as a multi-agent multi-objective problem, and focus on finding the Pareto-optimal solution for agents with distinct and potentially conflicting objectives. We propose three novel metrics for evaluating SANet. Furthermore, we develop a model partition and sharing (MoPS) framework in which large models, e.g., deep learning models, of different agents can be partitioned into shared and agent-specific parts that are jointly constructed and deployed according to agents' local computational resources. Two decentralized optimization algorithms are proposed. We derive theoretical bounds and prove that there exists a three-way tradeoff among optimization, generalization, and conflicting errors. We develop an open-source RAN and core network-based hardware prototype that implements agents to interact with three different layers of the network. Experimental results show that the proposed framework achieved performance gains of up to 14.61% while requiring only 44.37% of FLOPs required by state-of-the-art algorithms.

</details>


### [19] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: 提出了Tyee，一个用于智能生理医疗的统一、模块化、可配置工具包，解决了深度学习在生理信号分析中的数据格式、预处理、模型管道和实验可重复性问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在生理信号分析中面临数据格式异构、预处理策略不一致、模型管道碎片化以及实验设置不可重复等问题，阻碍了该领域的进展。

Method: 开发了Tyee工具包，包含三个关键创新：(1) 12种信号模态的统一数据接口和可配置预处理管道；(2) 模块化可扩展架构，支持灵活集成和快速原型开发；(3) 端到端工作流配置，促进可重复和可扩展的实验。

Result: Tyee在所有评估任务中均表现出一致的实用有效性和泛化能力，在13个数据集中的12个上达到了最先进水平，优于或匹配基线方法。

Conclusion: Tyee工具包通过统一、模块化和可配置的设计，有效解决了生理信号分析中的关键挑战，为智能生理医疗研究提供了可重复、可扩展的实验框架，并已开源维护。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [20] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: 提出多模态移动性模型M³ob，通过构建统一时空关系图并利用LLM增强的时空知识图谱，解决位置推荐中多模态方法难以捕捉移动动态的问题，在正常和异常场景下均表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法泛化能力有限：单模态方法受数据稀疏性和固有偏差限制，多模态方法则因静态多模态表示与时空动态之间的语义鸿沟而难以有效捕捉移动动态。

Method: 1. 利用LLM增强的时空知识图谱构建统一时空关系图；2. 设计门控机制融合不同模态的时空图表示；3. 提出STKG引导的跨模态对齐，将时空动态知识注入静态图像模态。

Result: 在六个公共数据集上的实验表明，该方法在正常场景下持续改进性能，在异常场景下展现出显著泛化能力。

Conclusion: 通过多模态时空知识表征移动动态，M³ob模型有效提升了位置推荐的准确性和泛化能力，为多模态移动性预测提供了新思路。

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [21] [LLM Agents as VC investors: Predicting Startup Success via RolePlay-Based Collective Simulation](https://arxiv.org/abs/2512.22608)
*Zhongyang Liu,Haoyu Pei,Xiangyi Xiao,Xiaocong Du,Yihui Li,Suting Hong,Kunpeng Zhang,Haipeng Zhang*

Main category: cs.AI

TL;DR: 提出SimVC-CAS多智能体系统，通过模拟风险投资群体决策过程来预测初创企业成功，相比传统单决策者模型显著提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有初创企业成功预测方法多从单一决策者视角建模，忽略了现实中风险投资决策由投资者群体共同完成的集体动态特性。

Method: 设计角色扮演智能体与基于图神经网络（GNN）的监督交互模块，将融资预测重构为群体决策任务；每个智能体代表具有独特特性和偏好的投资者，通过图结构共投网络进行异构评估与信息交换。

Result: 在PitchBook真实数据上（严格数据泄露控制下），SimVC-CAS显著提升预测准确率，例如平均精确率@10相对提升约25%，并提供可解释的多视角推理。

Conclusion: SimVC-CAS能有效模拟风险投资群体决策，提升预测性能与可解释性，并为其他复杂群体决策场景提供启示。

Abstract: Due to the high value and high failure rate of startups, predicting their success has become a critical challenge across interdisciplinary research. Existing approaches typically model success prediction from the perspective of a single decision-maker, overlooking the collective dynamics of investor groups that dominate real-world venture capital (VC) decisions. In this paper, we propose SimVC-CAS, a novel collective agent system that simulates VC decision-making as a multi-agent interaction process. By designing role-playing agents and a GNN-based supervised interaction module, we reformulate startup financing prediction as a group decision-making task, capturing both enterprise fundamentals and the behavioral dynamics of potential investor networks. Each agent embodies an investor with unique traits and preferences, enabling heterogeneous evaluation and realistic information exchange through a graph-structured co-investment network. Using real-world data from PitchBook and under strict data leakage controls, we show that SimVC-CAS significantly improves predictive accuracy while providing interpretable, multiperspective reasoning, for example, approximately 25% relative improvement with respect to average precision@10. SimVC-CAS also sheds light on other complex group decision scenarios.

</details>


### [22] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: 研究探讨了让大语言模型在更新预测前相互评审能否提升预测准确性。实验发现，在信息共享的异质模型组中，该干预显著提升了准确性，但在同质模型组中无效。


<details>
  <summary>Details</summary>
Motivation: 受人类预测中结构化审议提升准确性的启发，研究旨在探索类似干预（LLM相互评审预测）是否也能提升大语言模型的预测表现。

Method: 使用202个已解决的二元问题，测试GPT-5、Claude Sonnet 4.5和Gemini Pro 2.5在四种场景下的预测准确性：异质模型+分散信息、异质模型+共享信息、同质模型+分散信息、同质模型+共享信息。

Result: 干预仅在异质模型且信息共享的场景中显著提升准确性（Log Loss降低0.020，相对提升约4%，p=0.017）。同质模型组无改善，额外上下文信息也未提升预测准确性。

Conclusion: 审议可能是一种提升LLM预测准确性的可行策略，但效果取决于模型多样性和信息共享条件，且信息池化机制的作用未得到证实。

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [23] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: 提出DICE框架，通过两阶段证据耦合评估方法，解决RAG系统评估中可解释性不足、不确定性量化不充分和计算效率低的问题，实现透明、置信感知的系统比较。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统的标量评估指标存在可解释性有限、不确定性量化不足、多系统比较时计算效率低等问题，阻碍了RAG技术的负责任部署，需要更可信赖的评估方法。

Method: 采用两阶段证据耦合框架：结合深度分析推理与概率化{A, B, Tie}评分，生成透明且置信感知的判断；引入瑞士制锦标赛算法，将计算复杂度从O(N²)降至O(N log N)。

Result: 在中文金融QA数据集上验证，DICE与人类专家判断的一致性达85.7%，显著优于RAGAS等现有LLM评估指标；在八系统评估中计算量减少42.9%且保持排序保真度。

Conclusion: DICE为RAG系统评估提供了负责任、可解释且高效的新范式，支持通过可解释推理轨迹进行系统错误诊断与改进，促进可信RAG技术的部署。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [24] [TravelBench: A Real-World Benchmark for Multi-Turn and Tool-Augmented Travel Planning](https://arxiv.org/abs/2512.22673)
*Xiang Cheng,Yulan Hu,Xiangwen Zhang,Lu Xu,Zheng Pan,Xin Li,Yong Liu*

Main category: cs.AI

TL;DR: 提出了TravelBench，一个用于评估大语言模型在旅行规划任务中多轮交互和工具使用能力的真实世界基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有旅行规划任务在领域覆盖和多轮交互方面存在局限，无法支持动态的用户-智能体交互，因此无法全面评估智能体能力。

Method: 收集真实场景用户请求，构建多轮、单轮和不可解三个子集；建立包含10个旅行领域工具的受控沙箱环境，提供确定性工具输出以确保可靠推理。

Result: 在TravelBench上评估了多个大语言模型，并对其行为和性能进行了分析。

Conclusion: TravelBench为推进大语言模型在旅行规划领域的应用提供了一个实用且可复现的基准测试。

Abstract: Large language model (LLM) agents have demonstrated strong capabilities in planning and tool use. Travel planning provides a natural and high-impact testbed for these capabilities, as it requires multi-step reasoning, iterative preference elicitation through interaction, and calls to external tools under evolving constraints. Prior work has studied LLMs on travel-planning tasks, but existing settings are limited in domain coverage and multi-turn interaction. As a result, they cannot support dynamic user-agent interaction and therefore fail to comprehensively assess agent capabilities. In this paper, we introduce TravelBench, a real-world travel-planning benchmark featuring multi-turn interaction and tool use. We collect user requests from real-world scenarios and construct three subsets-multi-turn, single-turn, and unsolvable-to evaluate different aspects of agent performance. For stable and reproducible evaluation, we build a controlled sandbox environment with 10 travel-domain tools, providing deterministic tool outputs for reliable reasoning. We evaluate multiple LLMs on TravelBench and conduct an analysis of their behaviors and performance. TravelBench offers a practical and reproducible benchmark for advancing LLM agents in travel planning.

</details>


### [25] [Memento-II: Learning by Stateful Reflective Memory](https://arxiv.org/abs/2512.22716)
*Jun Wang*

Main category: cs.AI

TL;DR: 提出一个将情景记忆与强化学习结合的理论框架，支持大语言模型智能体进行持续体验式学习，通过反思机制实现无需反向传播或微调的适应能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在训练和部署之间存在严格分离，限制了智能体在部署后的持续适应能力。本研究旨在建立一种无需参数更新的持续学习机制，使语言模型智能体能够通过与环境交互不断改进。

Method: 引入状态化反思决策过程，将反思学习建模为与情景记忆的两阶段读写交互：写入阶段存储交互结果（策略评估），读取阶段检索相关历史案例（策略改进）。使用熵正则化策略迭代实例化框架，并建立收敛保证。

Result: 该过程诱导出增强状态记忆表示上的等价马尔可夫决策过程，允许使用动态规划和强化学习的经典工具。当情景记忆增长并充分覆盖状态空间时，所得策略收敛到最优解。

Conclusion: 这项工作为基于记忆增强和检索的语言模型智能体提供了原则性基础，使其能够在不更新参数的情况下实现持续适应，打破了训练与部署之间的传统界限。

Abstract: We propose a theoretical framework for continual and experiential learning in large language model agents that integrates episodic memory with reinforcement learning. The framework identifies reflection as the key mechanism that enables agents to adapt through interaction without back propagation or model fine tuning, thereby relaxing the conventional separation between training and deployment.To formalise this process, we introduce the Stateful Reflective Decision Process, which models reflective learning as a two stage read write interaction with episodic memory. Writing stores interaction outcomes and corresponds to policy evaluation, while reading retrieves relevant past cases and corresponds to policy improvement. We show that this process induces an equivalent Markov decision process over augmented state memory representations, allowing the use of classical tools from dynamic programming and reinforcement learning. We further instantiate the framework using entropy regularised policy iteration and establish convergence guarantees. As episodic memory grows and achieves sufficient coverage of the state space, the resulting policy converges to the optimal solution. This work provides a principled foundation for memory augmented and retrieval based language model agents capable of continual adaptation without parameter updates.

</details>


### [26] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: 提出SAMP-HDRL框架，通过分层深度强化学习解决非平稳市场中的投资组合优化问题，在波动市场中显著超越传统方法和DRL基准。


<details>
  <summary>Details</summary>
Motivation: 非平稳市场中的投资组合优化面临制度转换、动态相关性以及深度强化学习策略可解释性差等挑战。

Method: 采用动态资产分组将市场划分为高质量和普通子集；上层代理提取全局市场信号，下层代理在掩码约束下进行组内分配；基于效用的资本分配机制整合风险资产与无风险资产。

Result: 在2019-2021年三种市场制度下的回测显示，SAMP-HDRL在波动和震荡条件下持续优于9个传统基线和9个DRL基准；相比最强基线，至少获得5%更高的回报率、夏普比率、索提诺比率和2%更高的欧米伽比率，在动荡市场中收益更大。

Conclusion: SAMP-HDRL将结构性市场约束直接嵌入DRL流程，在复杂金融环境中提供了更好的适应性、鲁棒性和可解释性；消融研究证实上下层协调、动态聚类和资本分配对鲁棒性不可或缺；SHAP可解释性揭示了跨代理的'分散+集中'互补机制。

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [27] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: 提出了HiSciBench，一个分层多学科基准，用于评估基础模型在完整科学工作流程中的能力，涵盖从科学素养到科学发现的五个层次。


<details>
  <summary>Details</summary>
Motivation: 现有基准任务碎片化，未能反映真实科学探究的层次性和多学科性，无法全面评估模型在完整科学工作流程中的能力。

Method: 设计了包含五个层次（科学素养、文献解析、文献问答、文献综述生成、科学发现）的分层基准，涵盖6个学科、8735个实例，支持多模态输入和跨语言评估。

Result: 主流模型在基础素养任务上最高达到69%准确率，但在发现级任务上性能骤降至25%，显示出明显的性能差距。

Conclusion: HiSciBench为评估科学智能设立了新标准，为开发更强大可靠的科学AI模型提供了可操作的见解，并将公开促进未来研究。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [28] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: 提出Gamma知识图谱基础模型，通过多头几何注意力机制增强关系表达能力，在零样本归纳链接预测任务上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱基础模型（如Ultra）依赖单一关系变换，难以捕捉多样化图谱中复杂的关系和结构模式，限制了模型的表达能力。

Method: 引入多头几何注意力机制，使用实数、复数、分裂复数和双数四种并行几何变换建模不同关系结构；通过关系条件注意力融合机制（含熵正则化的轻量门控）自适应融合不同表示。

Result: 在56个多样化知识图谱上的实验表明，Gamma在零样本归纳链接预测任务中全面优于Ultra：归纳基准上平均倒数排名提升5.5%，所有基准上平均提升4.4%。

Conclusion: 互补的几何表示能有效提升知识图谱推理的鲁棒性和表达能力，多头关系变换设计为知识图谱基础模型提供了更强大的建模框架。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [29] [Multimodal Fact-Checking: An Agent-based Approach](https://arxiv.org/abs/2512.22933)
*Danni Xu,Shaojing Fan,Xuanang Cheng,Mohan Kankanhalli*

Main category: cs.AI

TL;DR: 提出了RW-Post数据集和AgentFact框架，用于提升多模态虚假信息检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法因推理能力有限和证据利用不足，难以应对多模态虚假信息的快速传播，且缺乏包含完整推理过程和可验证证据的专用数据集。

Method: 构建RW-Post数据集（保留社交媒体上下文，通过大语言模型辅助提取人工事实核查文章中的推理和证据）；设计AgentFact框架（包含五个协同智能体，模拟人类核查流程，采用证据检索与任务感知过滤的迭代工作流）。

Result: 实验表明，RW-Post与AgentFact的结合显著提高了多模态事实核查的准确性和可解释性。

Conclusion: 该研究通过高质量数据集和智能体协作框架，为多模态虚假信息检测提供了更有效的解决方案。

Abstract: The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.

</details>


### [30] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: 研究比较了大型语言模型与传统知识追踪模型在K-12教育中的表现，发现传统模型在知识评估的准确性、可靠性和时间一致性方面显著优于LLM，即使经过微调，LLM仍存在局限性。


<details>
  <summary>Details</summary>
Motivation: 针对K-12教育中LLM导师可能替代传统学习者建模的误解，以及欧盟AI法案将K-12列为高风险领域的要求，研究旨在评估LLM在追踪学习者知识演变方面的准确性和可靠性。

Method: 使用大型开放数据集，比较深度知识追踪模型与广泛使用的LLM（包括零样本和微调版本），通过AUC等指标评估下一步正确性预测性能，并进行时间一致性和多技能掌握估计的定性分析。

Result: DKT在下一步正确性预测中达到最高AUC（0.83），始终优于LLM；微调使LLM的AUC提高约8%，但仍比DKT低6%，且早期序列错误更高；时间分析显示DKT保持稳定正确的掌握更新，而LLM变体表现出不一致和错误方向更新。

Conclusion: LLM单独使用难以匹配现有智能辅导系统的效果，负责任的辅导需要结合学习者建模的混合框架。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [31] [The Reward Model Selection Crisis in Personalized Alignment](https://arxiv.org/abs/2512.23067)
*Fady Rezk,Yuangang Pan,Chuan-Sheng Foo,Xun Xu,Nancy Chen,Henry Gouk,Timothy Hospedales*

Main category: cs.AI

TL;DR: 研究发现传统奖励模型准确率无法有效指导个性化对齐部署，提出新评估指标和基准，显示上下文学习优于奖励引导方法。


<details>
  <summary>Details</summary>
Motivation: 当前个性化对齐研究过度依赖奖励模型准确率作为评估标准，但实际部署时需通过奖励引导解码进行推理时适配，这要求奖励模型不仅能准确排序偏好，还需有效指导词元级生成决策。

Method: 引入策略准确率作为新评估指标，衡量奖励引导解码评分函数区分偏好与非偏好响应的能力；构建首个含真实用户补全的个性化对齐基准Pref-LaMP；在三个数据集上系统评估奖励模型准确率与策略级判别能力的相关性。

Result: 奖励模型准确率与策略级判别能力仅弱相关（Kendall's τ=0.08-0.31）；在Pref-LaMP基准上，判别能力与生成能力完全解耦——RM准确率差异达20%的方法产生几乎相同的输出质量，且高判别能力方法未能生成行为对齐的响应；对于参数量>3B的模型，简单上下文学习全面优于所有奖励引导方法，在7B规模上ROUGE-1得分高出3-5点。

Conclusion: 当前领域优化的代理指标无法预测部署性能，且在部署约束下无法将偏好转化为实际行为适配；上下文学习在个性化对齐中展现出比奖励引导方法更优的部署效果。

Abstract: Personalized alignment from preference data has focused primarily on improving reward model (RM) accuracy, with the implicit assumption that better preference ranking translates to better personalized behavior. However, in deployment, computational constraints necessitate inference-time adaptation via reward-guided decoding (RGD) rather than per-user policy fine-tuning. This creates a critical but overlooked requirement: reward models must not only rank preferences accurately but also effectively guide token-level generation decisions. We demonstrate that standard RM accuracy fails catastrophically as a selection criterion for deployment-ready personalized alignment. Through systematic evaluation across three datasets, we introduce policy accuracy, a metric quantifying whether RGD scoring functions correctly discriminate between preferred and dispreferred responses. We show that RM accuracy correlates only weakly with this policy-level discrimination ability (Kendall's tau = 0.08--0.31). More critically, we introduce Pref-LaMP, the first personalized alignment benchmark with ground-truth user completions, enabling direct behavioral evaluation without circular reward-based metrics. On Pref-LaMP, we expose a complete decoupling between discrimination and generation: methods with 20-point RM accuracy differences produce almost identical output quality, and even methods achieving high discrimination fail to generate behaviorally aligned responses. Finally, simple in-context learning (ICL) dominates all reward-guided methods for models > 3B parameters, achieving 3-5 point ROUGE-1 gains over the best reward method at 7B scale. These findings show that the field optimizes proxy metrics that fail to predict deployment performance and do not translate preferences into real behavioral adaptation under deployment constraints.

</details>


### [32] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: 该研究探讨了在资源受限条件下，使用强化学习（GRPO方法）训练视觉语言模型（ChexReason）进行医学影像分析的效果，发现强化学习虽能提升模型在特定数据集上的性能，但会损害其跨数据集的泛化能力，表明在临床部署中，精心设计的监督微调可能比激进的强化学习更具优势。


<details>
  <summary>Details</summary>
Motivation: 尽管强化学习在大型语言模型的推理任务中取得进展，但其在资源受限的医学影像分析中的应用尚未充分探索。本研究旨在探索如何在有限计算资源（如单张A100 GPU）下，有效训练医学视觉语言模型，并评估其性能与泛化能力。

Method: 采用R1风格方法，包括监督微调（SFT）和基于群体策略优化的强化学习（GRPO），仅使用2,000个SFT样本和1,000个RL样本，在单张A100 GPU上训练ChexReason视觉语言模型。在CheXpert和NIH数据集上进行评估，并与高资源模型（如NV-Reason-CXR-3B）进行对比。

Result: GRPO方法在分布内数据（CheXpert）上性能提升23%（宏观F1分数达0.346），但在跨数据集（NIH）上泛化性能下降19%。研究发现SFT检查点在优化前能独特提升NIH性能，表明教师引导的推理能捕捉更多机构无关特征。此外，结构化推理支架对通用视觉语言模型有益，但对医学预训练模型增益有限。

Conclusion: 强化学习范式（而非模型规模）可能导致泛化悖论，即在提升特定数据集性能的同时损害跨机构泛化能力。对于需要跨多样人群稳健性的临床部署，精心策划的监督微调可能优于激进的强化学习。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [33] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 本文提出Intrinsic Self-reflective Preference Optimization (\q)，通过同时考虑上下文和备选响应来优化策略，解决了DPO方法存在的两个根本性限制，实现了更稳健、更对齐人类偏好的语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法存在两个根本问题：1）最优策略依赖于任意建模选择（标量化函数、参考策略），导致行为反映参数化伪影而非真实偏好；2）孤立处理响应生成未能利用成对数据中的比较信息，未开发模型内在自反思能力。

Method: 提出\q方法，推导出基于上下文和备选响应的全局最优策略，证明其相对于DPO/RLHF的优越性，并保证对标量化和参考选择的无关性。该方法无需架构修改或推理开销，可作为即插即用增强。

Result: 实验表明，\q在胜率和长度控制指标上均取得一致改进，验证了开发自反思能力能产生更稳健、更对齐人类的语言模型。

Conclusion: \q通过解锁模型的内在自反思能力，解决了DPO方法的根本局限性，为语言模型对齐提供了更有效的优化框架。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [34] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文认为现有AI情感智能评估框架需改进，因为它们未能全面衡量AI相关的情感智能方面，并提出了改进评估策略的方向。


<details>
  <summary>Details</summary>
Motivation: 当前评估AI情感智能的框架存在不足，既未充分考虑AI与人类情感智能的本质差异，又缺乏对情感本质的理论基础，导致评估不全面或不相关。

Method: 首先回顾不同情感理论和情感智能理论，评估其适用于人工系统的程度；然后批判性分析现有基准框架，指出其在基于第一部分理论框架下的不足；最后提出改进评估策略的方案。

Result: 识别出现有AI情感智能评估框架的局限性：部分人类情感智能要素（如现象学体验）不适用于AI，而AI可实现的感知、解释、响应和适应能力评估又不够完善；现有基准框架缺乏扎实的情感理论基础。

Conclusion: 需要基于对情感本质和AI适用性的理论分析，改进AI情感智能评估框架，使其更全面、相关且理论基础扎实。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [35] [SPIRAL: Symbolic LLM Planning via Grounded and Reflective Search](https://arxiv.org/abs/2512.23167)
*Yifan Zhang,Giridhar Ganapavarapu,Srideepika Jayaraman,Bhavna Agrawal,Dhaval Patel,Achille Fokoue*

Main category: cs.AI

TL;DR: SPIRAL框架通过三个专门的LLM代理嵌入MCTS循环，将符号规划与反思搜索相结合，显著提升了LLM在复杂规划任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在需要探索和自我纠正的复杂规划任务中表现不佳，其线性推理过程难以从早期错误中恢复；而传统搜索算法（如MCTS）在稀疏奖励下效果有限，且未能充分利用LLM的语义能力。

Method: 提出SPIRAL框架，将三个专门化的LLM代理（规划器提出创意步骤、模拟器预测现实结果、批评器通过反思提供密集奖励）嵌入蒙特卡洛树搜索循环，形成引导式、自我纠正的推理过程。

Result: 在DailyLifeAPIs和HuggingFace数据集上，SPIRAL显著优于默认的思维链规划方法和其他先进代理，在DailyLifeAPIs上达到83.6%的整体准确率，比次优搜索框架提升超过16个百分点，同时展现出更高的令牌效率。

Conclusion: 将LLM推理构建为引导式、反思性和接地气的搜索过程，能够产生更强大、更高效的自主规划器，证明了结构化搜索与LLM语义能力结合的有效性。

Abstract: Large Language Models (LLMs) often falter at complex planning tasks that require exploration and self-correction, as their linear reasoning process struggles to recover from early mistakes. While search algorithms like Monte Carlo Tree Search (MCTS) can explore alternatives, they are often ineffective when guided by sparse rewards and fail to leverage the rich semantic capabilities of LLMs. We introduce SPIRAL (Symbolic LLM Planning via Grounded and Reflective Search), a novel framework that embeds a cognitive architecture of three specialized LLM agents into an MCTS loop. SPIRAL's key contribution is its integrated planning pipeline where a Planner proposes creative next steps, a Simulator grounds the search by predicting realistic outcomes, and a Critic provides dense reward signals through reflection. This synergy transforms MCTS from a brute-force search into a guided, self-correcting reasoning process. On the DailyLifeAPIs and HuggingFace datasets, SPIRAL consistently outperforms the default Chain-of-Thought planning method and other state-of-the-art agents. More importantly, it substantially surpasses other state-of-the-art agents; for example, SPIRAL achieves 83.6% overall accuracy on DailyLifeAPIs, an improvement of over 16 percentage points against the next-best search framework, while also demonstrating superior token efficiency. Our work demonstrates that structuring LLM reasoning as a guided, reflective, and grounded search process yields more robust and efficient autonomous planners. The source code, full appendices, and all experimental data are available for reproducibility at the official project repository.

</details>


### [36] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 本文提出'模型信念'方法，通过LLM的token级概率分布提取更多信息，相比传统'模型选择'方法可提升20倍计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前使用LLM模拟人类行为时，仅将模型输出作为单一数据点，未能充分利用LLM的概率特性，导致数据利用效率低下。

Method: 提出'模型信念'度量方法，基于LLM的token级概率分布计算选择替代方案的信念分布；通过理论证明其渐近等价性，并在需求估计研究中验证性能。

Result: 模型信念比模型选择具有更低的方差和更快的收敛速度；在有限运行次数下能更好地解释和预测真实选择；将计算需求降低约20倍。

Conclusion: 模型信念应作为从LLM生成数据中提取信息的默认方法，能显著提升统计效率和计算效率。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [37] [TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI](https://arxiv.org/abs/2512.23217)
*Jingming Li*

Main category: cs.AI

TL;DR: 提出了首个基于热舒适场景的AI认知能力评估框架TCEval，用于评估大语言模型在跨模态推理、因果关联和自适应决策方面的能力，发现当前LLMs具备基础推理能力但缺乏对非线性关系的精确因果理解。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估基准缺乏针对具体任务的生态效度测试，热舒适作为涉及环境因素与个人感知交互的复杂认知过程，是评估AI系统真实世界认知能力的理想范式。

Method: 通过初始化具有虚拟人格属性的LLM智能体，引导其生成服装热阻选择和热舒适反馈，并将输出与ASHRAE全球数据库和中国热舒适数据库进行对比验证，使用1 PMV容差分析方向一致性，并进行统计分布检验。

Result: 实验表明：LLM智能体反馈与人类数据的精确对齐有限，但在1 PMV容差下方向一致性显著提升；生成的PMV分布与人类数据存在显著差异；在离散热舒适分类任务中表现接近随机水平。

Conclusion: TCEval可作为生态有效的AI认知图灵测试框架，证明当前LLMs具备基础跨模态推理能力，但缺乏对热舒适中变量间非线性关系的精确因果理解，为智能建筑等以人为本的AI应用提供了评估新视角。

Abstract: A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.

</details>


### [38] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 提出一种新型的Agentic Physical AI范式，通过物理验证驱动的策略优化替代感知推理，在小型语言模型上实现执行级行为稳定和跨物理域迁移。


<details>
  <summary>Details</summary>
Motivation: 现有通用多模态基础模型在物理控制任务中准确率仅50-53%，存在违反物理约束的输入不忠实问题，这是感知中心架构的结构性局限，无法满足安全关键控制对执行结果的保障需求。

Method: 训练3.6亿参数的语言模型作为Agentic Physical AI，使用合成反应器控制场景数据集（规模从10^3扩展到10^5），通过物理验证而非感知推理驱动策略优化。

Result: 模型出现通用模型不具备的相变：小规模时呈现高方差模仿和灾难性尾部风险，大规模时方差下降超500倍，执行行为稳定；自主拒绝约70%训练分布，95%运行时执行集中于单一策略；学习表征可跨不同物理域和连续输入模态迁移。

Conclusion: 领域专用基础模型可通过物理验证驱动的紧凑语言模型实现，在控制接口上超越感知中心架构的限制，为安全关键物理系统AI提供新路径。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [39] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 本文揭示了符合性规划与超属性模型检测之间的紧密联系，证明了二者可以相互高效转换。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索规划与验证领域中两个看似独立的问题——符合性规划和超属性模型检测之间的内在联系，以促进两个领域的交叉融合与方法互用。

Method: 采用双向归约方法：首先将超属性模型检测实例高效编码为符合性规划实例，并证明编码的完备性与正确性；其次证明每个符合性规划问题本身就是一个超属性模型检测任务。

Result: 主要结果表明：1）∃*∀*超属性的模型检测问题可以高效归约为符合性规划问题；2）反之，符合性规划问题可直接视为超属性模型检测的特例，二者具有本质等价性。

Conclusion: 本文结论是符合性规划与超属性模型检测在形式化层面存在深刻对应关系，这种双向可归约性为两个领域的技术迁移与工具互用奠定了理论基础。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [40] [CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations](https://arxiv.org/abs/2512.23328)
*Huan-ang Gao,Zikang Zhang,Tianwei Luo,Kaisen Yang,Xinzhe Juan,Jiahao Qiu,Tianxing Chen,Bingxiang He,Hao Zhao,Hao Zhou,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文通过引入CubeBench基准测试，评估LLM智能体在物理世界部署中的空间认知能力，揭示了其在长时程任务规划上的根本性缺陷。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型智能体在数字领域表现出色，但在物理世界部署中存在显著差距，主要挑战在于建立和维护稳健的空间心理模型。

Method: 提出CubeBench生成式基准测试，围绕魔方设计三层诊断框架：从完全符号信息的基础状态跟踪，到仅部分视觉数据的主动探索，并引入外部求解器工具隔离认知瓶颈。

Result: 实验显示领先LLM在所有长时程任务中通过率为0.00%，暴露了长期规划的根本性失败，同时通过分析失败模式为物理接地智能体开发提供关键见解。

Conclusion: LLM智能体在空间推理、心理模拟和主动探索方面存在严重局限，需要针对性改进才能实现有效的物理世界部署。

Abstract: Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.

</details>


### [41] [MindWatcher: Toward Smarter Multimodal Tool-Integrated Reasoning](https://arxiv.org/abs/2512.23412)
*Jiawei Chen,Xintian Shen,Lihao Zheng,Zhenwei Shao,Hongyuan Zhang,Pengfei Yu,Xudong Rao,Ning Mao,Xiaobo Liu,Lian Wen,Chaoqun Du,Feng Gu,Wei He,Qizhen Li,Shanshan Li,Zide Liu,Jing Luo,Lifu Mu,Xuhao Pan,Chang Ren,Haoyi Sun,Qian Wang,Wei Wang,Hongfu Yang,Jiqing Zhan,Chunpeng Zhou,Zheng Zhou,Hao Ma,Tao Wei,Pan Zhou,Wei Chen*

Main category: cs.AI

TL;DR: 提出MindWatcher，一种集成交错思维与多模态思维链推理的工具集成推理智能体，能自主决策调用多样化工具，在复杂多模态任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于工作流的智能体在解决需要调用工具的现实问题时智能有限，而具备自主推理与工具调用能力的工具集成推理智能体正成为处理复杂决策任务的有效途径。

Method: 采用交错思维范式，允许模型在推理过程中随时切换思考与工具调用；结合多模态思维链推理，支持在推理过程中操作图像以提升搜索精度；构建自动化数据审计与评估流程，辅以高质量人工标注数据集进行训练；建立MindWatcher-Evaluate Bench基准测试集；配备辅助推理工具集与大规模本地图像检索数据库；设计高效训练基础设施以提升训练速度与硬件利用率。

Result: 实验表明，MindWatcher通过更优的工具调用能力，在性能上匹配甚至超越了更大或更新的模型，同时揭示了智能体训练中的关键发现（如智能体强化学习中的遗传继承现象）。

Conclusion: MindWatcher作为一种新型工具集成推理智能体，能够有效处理广泛领域的多模态问题，其设计方法为智能体训练提供了重要见解，并展示了小规模模型通过高效工具调用实现高性能的潜力。

Abstract: Traditional workflow-based agents exhibit limited intelligence when addressing real-world problems requiring tool invocation. Tool-integrated reasoning (TIR) agents capable of autonomous reasoning and tool invocation are rapidly emerging as a powerful approach for complex decision-making tasks involving multi-step interactions with external environments. In this work, we introduce MindWatcher, a TIR agent integrating interleaved thinking and multimodal chain-of-thought (CoT) reasoning. MindWatcher can autonomously decide whether and how to invoke diverse tools and coordinate their use, without relying on human prompts or workflows. The interleaved thinking paradigm enables the model to switch between thinking and tool calling at any intermediate stage, while its multimodal CoT capability allows manipulation of images during reasoning to yield more precise search results. We implement automated data auditing and evaluation pipelines, complemented by manually curated high-quality datasets for training, and we construct a benchmark, called MindWatcher-Evaluate Bench (MWE-Bench), to evaluate its performance. MindWatcher is equipped with a comprehensive suite of auxiliary reasoning tools, enabling it to address broad-domain multimodal problems. A large-scale, high-quality local image retrieval database, covering eight categories including cars, animals, and plants, endows model with robust object recognition despite its small size. Finally, we design a more efficient training infrastructure for MindWatcher, enhancing training speed and hardware utilization. Experiments not only demonstrate that MindWatcher matches or exceeds the performance of larger or more recent models through superior tool invocation, but also uncover critical insights for agent training, such as the genetic inheritance phenomenon in agentic RL.

</details>


### [42] [The World Is Bigger! A Computationally-Embedded Perspective on the Big World Hypothesis](https://arxiv.org/abs/2512.23419)
*Alex Lewandowski,Adtiya A. Ramesh,Edan Meyer,Dale Schuurmans,Marlos C. Machado*

Main category: cs.AI

TL;DR: 本文从计算嵌入视角研究持续学习，提出交互性作为衡量智能体持续适应能力的指标，发现深度线性网络比非线性网络更能维持交互性。


<details>
  <summary>Details</summary>
Motivation: 现有持续学习研究常基于显式约束设计问题，但这类约束可能具有随意性、难以整合，且可能限制智能体能力扩展。本文旨在通过计算嵌入视角，自然推导出智能体在环境中的内在约束。

Method: 提出计算嵌入视角，将嵌入智能体形式化为在通用计算机中模拟的自动机；证明其等价于在可数无限状态空间的部分可观测马尔可夫决策过程中交互的智能体；提出交互性目标函数，并开发基于模型的强化学习算法进行优化。

Result: 构建合成问题评估持续学习能力，发现深度非线性网络难以维持交互性，而深度线性网络随着容量增加能维持更高的交互性。

Conclusion: 计算嵌入视角为持续学习提供了自然的问题设定框架；交互性是衡量智能体持续适应能力的有效指标；网络架构对维持交互性有显著影响，线性网络在此设定下表现更优。

Abstract: Continual learning is often motivated by the idea, known as the big world hypothesis, that "the world is bigger" than the agent. Recent problem formulations capture this idea by explicitly constraining an agent relative to the environment. These constraints lead to solutions in which the agent continually adapts to best use its limited capacity, rather than converging to a fixed solution. However, explicit constraints can be ad hoc, difficult to incorporate, and may limit the effectiveness of scaling up the agent's capacity. In this paper, we characterize a problem setting in which an agent, regardless of its capacity, is constrained by being embedded in the environment. In particular, we introduce a computationally-embedded perspective that represents an embedded agent as an automaton simulated within a universal (formal) computer. Such an automaton is always constrained; we prove that it is equivalent to an agent that interacts with a partially observable Markov decision process over a countably infinite state-space. We propose an objective for this setting, which we call interactivity, that measures an agent's ability to continually adapt its behaviour by learning new predictions. We then develop a model-based reinforcement learning algorithm for interactivity-seeking, and use it to construct a synthetic problem to evaluate continual learning capability. Our results show that deep nonlinear networks struggle to sustain interactivity, whereas deep linear networks sustain higher interactivity as capacity increases.

</details>


### [43] [AKG kernel Agent: A Multi-Agent Framework for Cross-Platform Kernel Synthesis](https://arxiv.org/abs/2512.23424)
*Jinye Du,Quan Yuan,Zuyao Zhang,Yanzhi Yi,Jiahui Hu,Wangyi Chen,Yiyang Zhu,Qishui Zheng,Wenxiang Zou,Xiangyu Chang,Zuohe Zheng,Zichun Ye,Chao Liu,Shanni Li,Renwei Zhang,Yiping Deng,Xinwei Hu,Xuefeng Jin,Jie Zhao*

Main category: cs.AI

TL;DR: 提出AKG kernel agent多智能体系统，自动化AI计算内核的生成、迁移和性能调优，支持多种领域特定语言和硬件后端，在KernelBench评估中比PyTorch Eager基线平均加速1.46倍。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型（如LLM、多模态架构、推荐系统）对高性能计算内核需求激增，结合稀疏化、量化等技术及硬件快速迭代，手动优化难以满足需求，成为AI系统开发瓶颈。

Method: 设计基于LLM代码生成能力的多智能体系统AKG kernel agent，支持Triton、TileLang、CPP、CUDA-C等多种领域特定语言，采用模块化架构实现跨硬件后端的自动化内核生成与调优。

Result: 在KernelBench上使用Triton DSL对GPU和NPU后端进行评估，相比PyTorch Eager基线实现平均获得1.46倍加速，验证了系统在加速现代AI工作负载内核开发方面的有效性。

Conclusion: AKG kernel agent通过自动化内核开发流程，解决了AI硬件生态碎片化与手动优化效率低下的问题，为多样化硬件平台上的高性能计算内核生成提供了可扩展解决方案。

Abstract: Modern AI models demand high-performance computation kernels. The growing complexity of LLMs, multimodal architectures, and recommendation systems, combined with techniques like sparsity and quantization, creates significant computational challenges. Moreover, frequent hardware updates and diverse chip architectures further complicate this landscape, requiring tailored kernel implementations for each platform. However, manual optimization cannot keep pace with these demands, creating a critical bottleneck in AI system development. Recent advances in LLM code generation capabilities have opened new possibilities for automating kernel development. In this work, we propose AKG kernel agent (AI-driven Kernel Generator), a multi-agent system that automates kernel generation, migration, and performance tuning. AKG kernel agent is designed to support multiple domain-specific languages (DSLs), including Triton, TileLang, CPP, and CUDA-C, enabling it to target different hardware backends while maintaining correctness and portability. The system's modular design allows rapid integration of new DSLs and hardware targets. When evaluated on KernelBench using Triton DSL across GPU and NPU backends, AKG kernel agent achieves an average speedup of 1.46$\times$ over PyTorch Eager baselines implementations, demonstrating its effectiveness in accelerating kernel development for modern AI workloads.

</details>


### [44] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: 提出Hindsight instruction Replay (HiR)框架，通过重放失败尝试作为成功样本来提高强化学习在复杂指令跟随任务中的样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大型语言模型对齐中依赖高质量响应样本，但初始模型常因能力有限无法生成满足所有约束的响应，导致奖励稀疏或难以区分，阻碍学习效率。

Method: 采用选择-重写策略，基于事后满足的约束条件将失败尝试重放为成功样本，结合原始样本进行强化学习，理论框架为指令级和响应级的双重偏好学习，仅需二元奖励信号。

Result: 在不同指令跟随任务中取得显著效果，同时降低计算资源需求。

Conclusion: HiR框架能有效解决奖励稀疏问题，提升强化学习在复杂指令对齐任务中的样本效率和性能。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [45] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: 本文通过AI协助和关机游戏框架研究AI系统与人类价值观对齐及安全问题，提出AI需具备在不确定性和非阿基米德偏好下推理的能力。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统与人类价值观对齐并保持安全，需要解决AI协助中学习人类效用函数和关机问题中安全响应关机指令的挑战。

Method: 采用AI协助游戏和AI关机游戏的理论框架，分析AI在不确定性、不完全偏好及非阿基米德偏好下的推理需求。

Result: 研究发现，解决AI对齐与安全问题要求AI能够处理不确定性、不完全偏好和非阿基米德偏好，以实现安全协助和可控关机。

Conclusion: 设计安全的AI系统需使其具备在复杂偏好和不确定性下推理的能力，这是实现AI与人类价值观对齐的关键技术前提。

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


### [46] [The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction](https://arxiv.org/abs/2512.23489)
*Haoyu Pei,Zhongyang Liu,Xiangyi Xiao,Xiaocong Du,Haipeng Zhang,Kunpeng Zhang,Suting Hong*

Main category: cs.AI

TL;DR: 提出了MIRAGE-VC框架，通过多视角检索增强生成解决风险投资预测中的路径爆炸和异质证据融合问题，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习、图神经网络和现有图-大语言模型方法在风险投资预测中存在局限：缺乏显式推理能力、模态不匹配或仅适用于图内任务，而风险投资预测是图外预测任务，需要从复杂关系证据中进行可解释推理。

Method: 1. 信息增益驱动的路径检索器迭代选择高价值邻居，将投资网络压缩为紧凑链；2. 多智能体架构通过基于公司属性的可学习门控机制整合三个证据流（公司披露、投资者记录、投资网络结构）。

Result: 在严格防泄漏控制下，MIRAGE-VC实现了F1分数提升5.0%，PrecisionAt5提升16.6%，并在推荐和风险评估等其他图外预测任务上展现出潜力。

Conclusion: MIRAGE-VC框架有效解决了风险投资预测中的核心挑战，通过显式推理和异质证据融合显著提升了预测准确性，为图外预测任务提供了通用解决方案。

Abstract: Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.

</details>


### [47] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: 提出CreativeDC方法解决LLM生成教育问题时存在的'人工蜂群思维'效应，通过两阶段提示法分离创意探索与约束满足，显著提升问题多样性、新颖性和实用性。


<details>
  <summary>Details</summary>
Motivation: LLM生成教育问题时存在'人工蜂群思维'效应，导致同一模型生成相似响应、不同模型输出同质化，学生可能接触过于重复的问题，损害思维多样性。

Method: 基于Wallas创造力理论和Guilford发散-收敛思维框架，提出CreativeDC两阶段提示法：第一阶段进行创意探索，第二阶段进行约束满足，将LLM推理过程显式分解为不同阶段。

Result: CreativeDC在多样性、新颖性和实用性综合评估中显著优于基线方法；扩展分析显示，随着采样增加，CreativeDC能生成更多有效不同问题，增长速度更快。

Conclusion: CreativeDC通过结构化分离创意探索与约束满足，有效缓解LLM的同质化问题生成，为教育内容创作提供了更具创造性和多样性的解决方案。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [48] [Physics-Informed Neural Networks for Device and Circuit Modeling: A Case Study of NeuroSPICE](https://arxiv.org/abs/2512.23624)
*Chien-Ting Tung,Chenming Hu*

Main category: cs.AI

TL;DR: 提出NeuroSPICE框架，利用物理信息神经网络求解电路微分代数方程，支持新兴器件仿真和设计优化。


<details>
  <summary>Details</summary>
Motivation: 传统SPICE依赖时间离散数值求解器，难以处理高度非线性系统（如铁电存储器）和逆问题，需要更灵活的仿真框架。

Method: 采用物理信息神经网络，通过反向传播最小化微分代数方程残差，使用时域解析方程建模器件和电路波形，直接计算时间导数。

Result: NeuroSPICE在训练阶段的速度和精度未超越传统SPICE，但具备代理模型生成能力，可应用于设计优化和逆问题求解。

Conclusion: 该框架为新兴非线性器件仿真提供了灵活方案，特别适用于传统SPICE难以处理的电路系统与优化场景。

Abstract: We present NeuroSPICE, a physics-informed neural network (PINN) framework for device and circuit simulation. Unlike conventional SPICE, which relies on time-discretized numerical solvers, NeuroSPICE leverages PINNs to solve circuit differential-algebraic equations (DAEs) by minimizing the residual of the equations through backpropagation. It models device and circuit waveforms using analytical equations in time domain with exact temporal derivatives. While PINNs do not outperform SPICE in speed or accuracy during training, they offer unique advantages such as surrogate models for design optimization and inverse problems. NeuroSPICE's flexibility enables the simulation of emerging devices, including highly nonlinear systems such as ferroelectric memories.

</details>


### [49] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: 提出I-PERI算法解决联邦因果发现中客户端存在未知干预的问题，通过恢复客户端图并集的CPDAG并利用干预引起的结构差异定向边，得到更紧的Φ-Markov等价类。


<details>
  <summary>Details</summary>
Motivation: 现有联邦因果发现方法通常假设所有客户端共享相同因果模型，但实际中客户端特定策略（如医院间不同协议）会导致未知异质性干预，需要更现实的解决方案。

Method: I-PERI算法分两步：1) 恢复客户端图并集的CPDAG；2) 利用跨客户端干预引起的结构差异定向边，生成Φ-CPDAG表示更紧的Φ-Markov等价类。

Result: 理论证明I-PERI的收敛性和隐私保护特性，合成数据实验验证算法有效性。

Conclusion: I-PERI能处理客户端未知干预的联邦因果发现问题，提供理论保证和实证效果，推进了现实场景下的因果发现应用。

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [50] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 提出Web世界模型（WWM），将世界状态和物理规则用普通网页代码实现以保证逻辑一致性，同时利用大语言模型生成上下文、叙事和高层决策，在可控性与开放性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有语言代理的持久世界构建方法存在两极分化：传统网页框架提供可靠但固定的上下文，而完全生成的世界模型追求无限环境但牺牲了可控性和工程实用性。需要一种兼顾逻辑一致性与开放探索的中间方案。

Method: 1. 在真实网页技术栈上构建WWM套件（包括基于真实地理的无限旅行图集、虚构星系探索器、网页级百科全书/叙事世界等）；2. 设计原则：代码定义规则与模型驱动想象分离、潜在状态表示为类型化网页接口、使用确定性生成实现结构化无限探索。

Result: 成功构建了多种WWM系统，验证了网页技术栈可作为可扩展的世界模型基础，能够实现可控且开放式的环境。

Conclusion: 网页技术栈本身可作为世界模型的可扩展基底，通过代码规则与语言模型生成的结合，能同时保障环境的结构化一致性与叙事开放性。

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [51] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文系统研究了文本到视频扩散模型中时间步与运动编码的关系，通过大规模定量分析揭示了运动与外观在去噪轨迹上的竞争关系，并基于此简化了运动定制方法。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频扩散模型通过迭代去噪合成时空运动与外观，但运动如何在时间步中编码尚不明确。实践中常用启发式经验（早期时间步主导运动/布局，后期主导外观）缺乏系统验证，需要定量表征这一行为。

Method: 通过注入新条件到特定时间步范围引发的外观编辑与运动保持的权衡，代理运动编码；进行大规模定量研究，跨多种架构分析去噪轨迹中运动与外观的竞争关系；基于发现的时间步边界，将训练和推理限制在运动主导区域以简化运动定制。

Result: 一致识别出早期运动主导区域和后期外观主导区域，确立了时间步空间中的可操作运动-外观边界；基于此边界简化运动定制范式，仅使用运动主导区域即可实现强运动迁移，无需去偏模块或专用目标。

Conclusion: 将广泛使用的启发式经验转化为时空解缠原理；时间步约束方法可直接集成到现有运动迁移和编辑方法中，为视频生成中的运动控制提供了理论基础和实用方案。

Abstract: Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

</details>


### [52] [Real-Time American Sign Language Recognition Using 3D Convolutional Neural Networks and LSTM: Architecture, Training, and Deployment](https://arxiv.org/abs/2512.22177)
*Dawnena Key*

Main category: cs.CV

TL;DR: 提出了一种基于3D CNN与LSTM混合架构的实时美式手语识别系统，通过摄像头视频流识别单词级手语，在多个数据集上取得良好性能并实现边缘部署。


<details>
  <summary>Details</summary>
Motivation: 全球有超过7000万聋哑或听力障碍人士面临沟通障碍，需要实时、准确的手语识别技术来促进无障碍交流。

Method: 采用3D CNN提取视频帧的时空特征，结合LSTM建模手语动作的序列依赖性；使用WLASL数据集（2000词）、ASL-LEX词典库（约2700个手势）及100个专家标注手势进行训练；在AWS云平台部署并支持OAK-D摄像头边缘计算。

Result: 系统在不同手语类别上的F1分数范围为0.71至0.99，表明对常见手语的识别具有较高准确率。

Conclusion: 该混合深度学习架构能有效实现实时美式手语识别，通过云边协同部署为实际无障碍应用提供了可行解决方案。

Abstract: This paper presents a real-time American Sign Language (ASL) recognition system utilizing a hybrid deep learning architecture combining 3D Convolutional Neural Networks (3D CNN) with Long Short-Term Memory (LSTM) networks. The system processes webcam video streams to recognize word-level ASL signs, addressing communication barriers for over 70 million deaf and hard-of-hearing individuals worldwide. Our architecture leverages 3D convolutions to capture spatial-temporal features from video frames, followed by LSTM layers that model sequential dependencies inherent in sign language gestures. Trained on the WLASL dataset (2,000 common words), ASL-LEX lexical database (~2,700 signs), and a curated set of 100 expert-annotated ASL signs, the system achieves F1-scores ranging from 0.71 to 0.99 across sign classes. The model is deployed on AWS infrastructure with edge deployment capability on OAK-D cameras for real-time inference. We discuss the architecture design, training methodology, evaluation metrics, and deployment considerations for practical accessibility applications.

</details>


### [53] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: 本文提出了一种结合人工智能与局部线性嵌入（LLE）的新方法，用于处理高维医疗数据，旨在提升医疗账单和转录服务的准确性与效率。


<details>
  <summary>Details</summary>
Motivation: 人工智能在医疗领域的快速发展为优化医疗账单和转录等流程提供了新机遇，但高维医疗数据的处理仍面临准确性和效率挑战。

Method: 开发了AI增强的LLE模型，通过数学建模和实验验证，将其应用于真实医疗场景中的数据处理。

Result: 实验结果表明，该模型显著提高了数据处理准确性和操作效率。

Conclusion: AI增强的LLE在医疗数据分析中具有潜力，为未来更广泛的医疗应用研究奠定了基础。

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [54] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: 提出VISTA框架，通过信息瓶颈将视觉感知与推理解耦，使用冻结的VLM进行客观感知查询，文本LLM进行推理规划，显著提升视觉问答对虚假相关性的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视觉语言模型在回答视觉问题时容易利用虚假相关性而非因果视觉证据，微调后可能加剧这一问题，需要更可靠的视觉推理方法。

Method: 提出VISTA框架：1）冻结VLM传感器进行简短客观的感知查询；2）纯文本LLM推理器分解问题、规划查询、聚合视觉事实；3）通过强化学习（GRPO）在641个多步问题上训练无偏视觉推理。

Result: 在SpuriVerse上虚假相关性鲁棒性显著提升（Qwen-2.5-VL-7B +16.29%，Llama-3.2-Vision-11B +6.77%），在MMVP和SeedBench子集保持竞争力，能跨VLM传感器迁移并识别感知失败。

Conclusion: VISTA通过模块化设计有效减少对虚假属性的依赖，产生更中立、更明确基于视觉证据的推理轨迹，为可靠视觉推理提供了新范式。

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [55] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

TL;DR: SAMM2D双编码器框架在颅内动脉瘤检测中取得突破，发现强预训练主干下数据增强反而降低性能，挑战了医学影像中'增强越多越好'的假设。


<details>
  <summary>Details</summary>
Motivation: 动脉瘤检测面临形态细微、类别不平衡和标注数据稀缺的挑战，需要开发更有效的检测方法以预防致命性出血。

Method: 提出SAMM2D双编码器框架，在RSNA颅内动脉瘤数据集上进行评估，通过六种增强方案的消融实验，对比有无数据增强的性能差异，使用Grad-CAM可视化模型注意力区域。

Result: 模型AUC达0.686，比临床基线提升32%；未增强基线模型比所有增强变体性能高1.75-2.23个百分点；校准决策阈值后灵敏度达95%，超过放射科医生平均水平；85%真阳性关注相关血管区域（与专家标注IoU为62%）。

Conclusion: ImageNet预训练特征已捕获鲁棒不变性，额外增强既冗余又破坏特征流形；未来医学影像工作流可能更受益于强预训练而非复杂增强流程。

Abstract: Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

</details>


### [56] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: 提出HookMIL框架，通过可学习的hook tokens实现高效上下文聚合，在计算病理学中提升多示例学习性能与效率。


<details>
  <summary>Details</summary>
Motivation: 传统多示例学习方法丢失上下文信息，而基于Transformer的方法存在二次复杂度高和计算冗余的问题，需要兼顾上下文建模与计算效率。

Method: 使用可学习的hook tokens进行结构化上下文聚合，支持视觉特征、文本嵌入和空间特征三种初始化方式；采用线性复杂度的双向注意力机制，并引入Hook多样性损失和token间通信机制。

Result: 在四个公共病理数据集上达到最先进性能，同时提升计算效率和可解释性。

Conclusion: HookMIL通过多模态初始化的hook tokens有效整合上下文信息，在保持线性复杂度的同时显著提升弱监督病理图像分析效果。

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [57] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: 提出Tiny-YOLOSAM混合系统，结合YOLOv12检测器生成框提示和稀疏点提示，显著提升分割覆盖率和速度。


<details>
  <summary>Details</summary>
Motivation: Segment Anything Model (SAM)及其轻量版TinySAM在实时场景中计算开销大，'segment-everything'模式需要大量提示且速度慢，需要更高效的解决方案。

Method: 使用YOLOv12检测器为TinySAM生成前景对象的框提示，对未覆盖区域补充稀疏点提示，避免密集提示。

Result: 在COCO val2017上，类别无关覆盖率大幅提升（AR从16.4%到77.1%，mIoU从19.2%到67.8%），端到端运行时间从49.20秒/图像降至10.39秒/图像（加速4.7倍）。

Conclusion: 检测器引导提示结合目标稀疏采样是替代密集'segment-everything'提示的有效方法，适用于实际全场景分割。

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>


### [58] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: 提出一种新型多模态可解释性模型，利用视觉语言模型和少样本学习，通过分析视网膜象限内的病变分布来模拟眼科医生的诊断推理过程，生成配对Grad-CAM热图以可视化DR严重程度分类的关键区域。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是全球视力丧失的主要原因，需要早期检测。医生缺乏时间进行病变手动标注，现有模型依赖单一成像模态且可解释性有限，需要能够解释分类理由而非仅定位病变的模型。

Method: 采用多模态可解释性模型，结合视觉语言模型和少样本学习技术，分析视网膜象限内的病变分布，生成配对Grad-CAM热图展示OCT和眼底图像中影响DR严重程度分类的神经元权重区域。使用3,000张眼底图像和1,000张OCT图像数据集。

Result: 该创新方法解决了当前DR诊断中的关键限制，通过可视化展示分类决策的关键区域，提供了更实用的诊断工具。

Conclusion: 该模型为改善患者预后提供了实用且全面的工具，能够克服现有模型的局限性，在筛查、治疗和研究环境中具有广泛应用潜力。

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

</details>


### [59] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: 提出TCFormer，一个仅500万参数的轻量级弱监督Transformer人群计数框架，在多个基准测试中实现了参数效率与计数精度的良好平衡。


<details>
  <summary>Details</summary>
Motivation: 传统人群计数方法依赖费时费力的点级标注和计算密集型主干网络，限制了其在资源受限环境中的可扩展性和部署能力。

Method: 1. 采用高效视觉Transformer作为特征提取器；2. 设计可学习密度加权平均模块动态重加权局部特征；3. 引入密度级别分类损失对人群密度进行离散化分级。

Result: 在ShanghaiTech A/B、UCF-QNRF和NWPU等四个基准数据集上的实验表明，该方法在参数效率和计数精度方面取得了优越的权衡。

Conclusion: TCFormer通过弱监督范式（仅使用图像级全局计数）和联合优化策略，能够为边缘设备上的人群计数任务提供高效解决方案。

Abstract: Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

</details>


### [60] [Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware](https://arxiv.org/abs/2512.22298)
*Vesal Ahsani,Babak Hossein Khalaj*

Main category: cs.CV

TL;DR: 提出了一种用于低成本边缘设备的单摄像头驾驶员行为识别系统，可实时监测17种分心和疲劳行为，在树莓派5和Coral Edge TPU上分别达到16FPS和25FPS。


<details>
  <summary>Details</summary>
Motivation: 车载驾驶员监控系统需要在计算、功耗和成本严格受限的条件下，低延迟地识别分心和疲劳相关行为。

Method: 采用紧凑的单帧视觉模型、减少视觉相似误报的混淆感知标签设计，以及基于置信度和持续性的时序决策头来触发警报。

Result: 系统在树莓派5（INT8推理）上实现约16FPS（单帧延迟<60ms），在Coral Edge TPU上达到约25FPS，能够在廉价硬件上实现实时监控和稳定警报生成。

Conclusion: 可靠的舱内人员状态感知可作为以人为中心的车辆智能（包括新兴的智能体车辆概念）的上游输入。

Abstract: In-cabin Driver Monitoring Systems (DMS) must recognize distraction- and drowsiness-related behaviors with low latency under strict constraints on compute, power, and cost. We present a single-camera in-cabin driver behavior recognition system designed for deployment on two low-cost edge platforms: Raspberry Pi 5 (CPU-only) and Google Coral Edge TPU. The proposed pipeline combines (i) a compact per-frame vision model, (ii) a confounder-aware label design to reduce visually similar false positives, and (iii) a temporal decision head that triggers alerts only when predictions are both confident and sustained. The system covers 17 behavior classes, including multiple phone-use modes, eating/drinking, smoking, reaching behind, gaze/attention shifts, passenger interaction, grooming, control-panel interaction, yawning, and eyes-closed sleep. Training and evaluation use licensed datasets spanning diverse drivers, vehicles, and lighting conditions (details in Section 6), and we further validate runtime behavior in real in-vehicle tests. The optimized deployments achieve about 16 FPS on Raspberry Pi 5 with INT8 inference (per-frame latency under 60 ms) and about 25 FPS on Coral Edge TPU, enabling real-time monitoring and stable alert generation on inexpensive hardware. Finally, we discuss how reliable in-cabin human-state perception can serve as an upstream input for human-centered vehicle intelligence, including emerging agentic vehicle concepts.

</details>


### [61] [A CNN-Based Malaria Diagnosis from Blood Cell Images with SHAP and LIME Explainability](https://arxiv.org/abs/2512.22205)
*Md. Ismiel Hossen Abir,Awolad Hossain*

Main category: cs.CV

TL;DR: 提出基于自定义卷积神经网络的深度学习模型，用于自动分类疟疾感染的血细胞图像，准确率达96%，并应用可解释AI技术增强模型透明度。


<details>
  <summary>Details</summary>
Motivation: 传统疟疾诊断方法（如显微镜血涂片分析）灵敏度低、依赖专家判断且资源要求高，在偏远地区难以实施，需要开发快速、准确且可解释的自动化诊断方案。

Method: 构建自定义卷积神经网络（CNN），并与ResNet50、VGG16、MobileNetV2、DenseNet121等经典架构对比；采用SHAP、LIME和显著性图谱等可解释AI技术分析模型决策依据。

Result: 自定义CNN模型达到96%的准确率，对感染/未感染类别的精确率和召回率均超过0.95；可解释技术有效揭示了模型关注的血细胞形态特征。

Conclusion: 深度学习可为资源有限地区提供快速、准确且可解释的疟疾诊断方案，自定义CNN在性能与可解释性上均表现优异，具备临床部署潜力。

Abstract: Malaria remains a prevalent health concern in regions with tropical and subtropical climates. The cause of malaria is the Plasmodium parasite, which is transmitted through the bites of infected female Anopheles mosquitoes. Traditional diagnostic methods, such as microscopic blood smear analysis, are low in sensitivity, depend on expert judgment, and require resources that may not be available in remote settings. To overcome these limitations, this study proposes a deep learning-based approach utilizing a custom Convolutional Neural Network (CNN) to automatically classify blood cell images as parasitized or uninfected. The model achieves an accuracy of 96%, with precision and recall scores exceeding 0.95 for both classes. This study also compares the custom CNN with established deep learning architectures, including ResNet50, VGG16, MobileNetV2, and DenseNet121. To enhance model interpretability, Explainable AI techniques such as SHAP, LIME, and Saliency Maps are applied. The proposed system shows how deep learning can provide quick, accurate and understandable malaria diagnosis, especially in areas with limited resources.

</details>


### [62] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Olivier X. Miguel,Kevin Dick,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 本研究评估了超声特异性自监督预训练模型USF-MAE在早孕期超声图像中自动检测囊性水囊瘤的性能，结果显示其优于传统监督学习基线模型。


<details>
  <summary>Details</summary>
Motivation: 囊性水囊瘤是高风险产前超声发现，与染色体异常和不良妊娠结局相关。自动检测可提高可重复性和筛查效率，但监督深度学习方法受限于标注数据量小的问题。

Method: 使用在37万张未标注超声图像上预训练的USF-MAE模型，在囊性水囊瘤与正常对照的二分类任务上进行微调。采用与DenseNet-169基线相同的4折交叉验证协议，评估指标包括准确率、敏感性、特异性、ROC-AUC，并使用Score-CAM进行可解释性分析。

Result: USF-MAE在所有评估指标上均优于DenseNet-169基线：平均准确率0.96 vs 0.93，敏感性0.94 vs 0.92，特异性0.98 vs 0.94，ROC-AUC 0.98 vs 0.94。Score-CAM可视化显示模型关注胎儿颈部的临床相关区域，Wilcoxon符号秩检验证实性能提升具有统计学意义（p=0.0057）。

Conclusion: 超声特异性自监督预训练能够显著提升囊性水囊瘤自动检测的准确性和鲁棒性，为大规模早期筛查项目提供了有效的深度学习解决方案。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [63] [Plug In, Grade Right: Psychology-Inspired AGIQA](https://arxiv.org/abs/2512.22780)
*Zhicheng Liao,Baoliang Chen,Hanwei Zhu,Lingyu Zhu,Shiqi Wang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出AGQG模块解决AGIQA中语义漂移问题，通过算术GRM实现单峰可解释质量分布，提升多种AGIQA框架性能


<details>
  <summary>Details</summary>
Motivation: 现有AGIQA模型存在语义漂移问题——图像嵌入与不同等级质量描述的相似度分布呈现多峰模式，导致文本-图像共享空间学习不可靠

Method: 受心理测量学启发，提出改进的等级响应模型（GRM），设计双分支质量分级模块：一支估计图像能力，另一支构建多个难度等级；通过算术方式建模难度生成确保单调性

Result: AGQG模块即插即用，能持续提升多种SOTA AGIQA框架性能，并有效泛化至自然图像和屏幕内容图像质量评估

Conclusion: AGQG模块通过算术GRM缓解语义漂移问题，实现单峰可解释质量分布，有望成为未来IQA模型的关键组件

Abstract: Existing AGIQA models typically estimate image quality by measuring and aggregating the similarities between image embeddings and text embeddings derived from multi-grade quality descriptions. Although effective, we observe that such similarity distributions across grades usually exhibit multimodal patterns. For instance, an image embedding may show high similarity to both "excellent" and "poor" grade descriptions while deviating from the "good" one. We refer to this phenomenon as "semantic drift", where semantic inconsistencies between text embeddings and their intended descriptions undermine the reliability of text-image shared-space learning. To mitigate this issue, we draw inspiration from psychometrics and propose an improved Graded Response Model (GRM) for AGIQA. The GRM is a classical assessment model that categorizes a subject's ability across grades using test items with various difficulty levels. This paradigm aligns remarkably well with human quality rating, where image quality can be interpreted as an image's ability to meet various quality grades. Building on this philosophy, we design a two-branch quality grading module: one branch estimates image ability while the other constructs multiple difficulty levels. To ensure monotonicity in difficulty levels, we further model difficulty generation in an arithmetic manner, which inherently enforces a unimodal and interpretable quality distribution. Our Arithmetic GRM based Quality Grading (AGQG) module enjoys a plug-and-play advantage, consistently improving performance when integrated into various state-of-the-art AGIQA frameworks. Moreover, it also generalizes effectively to both natural and screen content image quality assessment, revealing its potential as a key component in future IQA models.

</details>


### [64] [Signal-SGN++: Topology-Enhanced Time-Frequency Spiking Graph Network for Skeleton-Based Action Recognition](https://arxiv.org/abs/2512.22214)
*Naichuan Zheng,Xiahai Lun,Weiyi Li,Yuchen Du*

Main category: cs.CV

TL;DR: 提出Signal-SGN++，一种结合拓扑感知的脉冲图框架，用于高效的人体动作识别，通过结构自适应和时间-频率脉冲动力学实现高精度与低能耗的平衡。


<details>
  <summary>Details</summary>
Motivation: 图卷积网络（GCNs）在动作识别中表现优异但能耗高，而脉冲神经网络（SNNs）虽能效高却难以捕捉人体运动的时空-频率和拓扑依赖关系，因此需要一种能兼顾精度与能效的方法。

Method: 采用1D脉冲图卷积（1D-SGC）和频率脉冲卷积（FSC）作为主干网络，结合拓扑转移自注意力（TSSA）机制自适应学习骨架拓扑，并引入多尺度小波变换融合（MWTF）分支和拓扑感知时频融合（TATF）单元进行多分辨率特征融合。

Result: 在大规模基准测试中，Signal-SGN++在显著降低能耗的同时，超越了现有SNN方法，并与最先进的GCNs取得了竞争性结果，实现了优异的精度-效率权衡。

Conclusion: Signal-SGN++通过融合结构自适应与时间-频率脉冲动力学，有效提升了动作识别的能效和性能，为低功耗实时动作识别提供了可行方案。

Abstract: Graph Convolutional Networks (GCNs) demonstrate strong capability in modeling skeletal topology for action recognition, yet their dense floating-point computations incur high energy costs. Spiking Neural Networks (SNNs), characterized by event-driven and sparse activation, offer energy efficiency but remain limited in capturing coupled temporal-frequency and topological dependencies of human motion. To bridge this gap, this article proposes Signal-SGN++, a topology-aware spiking graph framework that integrates structural adaptivity with time-frequency spiking dynamics. The network employs a backbone composed of 1D Spiking Graph Convolution (1D-SGC) and Frequency Spiking Convolution (FSC) for joint spatiotemporal and spectral feature extraction. Within this backbone, a Topology-Shift Self-Attention (TSSA) mechanism is embedded to adaptively route attention across learned skeletal topologies, enhancing graph-level sensitivity without increasing computational complexity. Moreover, an auxiliary Multi-Scale Wavelet Transform Fusion (MWTF) branch decomposes spiking features into multi-resolution temporal-frequency representations, wherein a Topology-Aware Time-Frequency Fusion (TATF) unit incorporates structural priors to preserve topology-consistent spectral fusion. Comprehensive experiments on large-scale benchmarks validate that Signal-SGN++ achieves superior accuracy-efficiency trade-offs, outperforming existing SNN-based methods and achieving competitive results against state-of-the-art GCNs under substantially reduced energy consumption.

</details>


### [65] [VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition](https://arxiv.org/abs/2512.22217)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Fadi Dornaika,Cosimo Distante,Abdenour Hadid*

Main category: cs.CV

TL;DR: 提出VLM-PAR框架，基于多语言SigLIP 2编码器，通过跨模态注意力融合解决行人属性识别中的类别不平衡和领域迁移问题，在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 行人属性识别面临严重类别不平衡、属性间复杂依赖关系和领域迁移等挑战，需要更鲁棒的解决方案。

Method: 基于冻结的多语言SigLIP 2编码器构建视觉-语言框架，通过紧凑的跨注意力融合模块对齐图像和提示嵌入，优化视觉特征表示。

Result: 在高度不平衡的PA100K基准测试中实现显著精度提升并达到新最优性能，在PETA和Market-1501基准测试的平均准确率也获得显著提升。

Conclusion: 大规模视觉-语言预训练与针对性跨模态优化的结合能有效克服行人属性识别中的不平衡和泛化挑战。

Abstract: Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.

</details>


### [66] [Hash Grid Feature Pruning](https://arxiv.org/abs/2512.22882)
*Yangzhi Ma,Bojun Liu,Jie Li,Li Li,Dong Liu*

Main category: cs.CV

TL;DR: 提出一种基于高斯溅射坐标的哈希网格特征剪枝方法，通过识别并移除无效特征来减少存储和传输开销，在不影响性能的情况下提升率失真性能。


<details>
  <summary>Details</summary>
Motivation: 现有哈希网格在表示3D高斯溅射时存在大量稀疏区域，导致许多特征无效，造成存储和传输冗余。

Method: 基于输入高斯溅射坐标识别无效哈希网格特征并进行剪枝，仅对有效特征进行编码。

Result: 在标准化委员会定义的通用测试条件下，相比基线方法平均比特率降低8%。

Conclusion: 所提出的哈希网格特征剪枝方法能有效减少存储开销并提升压缩效率，同时保持模型性能不变。

Abstract: Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.

</details>


### [67] [On Extending Semantic Abstraction for Efficient Search of Hidden Objects](https://arxiv.org/abs/2512.22220)
*Tasha Pais,Nikhilesh Belulkar*

Main category: cs.CV

TL;DR: 提出语义抽象框架，利用2D视觉语言模型的相关性激活作为抽象对象表示，学习被遮挡物体的3D定位与补全，显著提高家庭机器人寻找丢失物品的效率。


<details>
  <summary>Details</summary>
Motivation: 家庭机器人需要高效寻找被遮挡或部分隐藏的物体，传统随机搜索方法效率低下，需要利用历史放置数据优化搜索过程。

Method: 将2D VLM的相关性激活图视为抽象对象表示，结合历史物体放置数据，开发用于隐藏物体3D定位和补全的语义抽象框架。

Result: 模型能在首次尝试中准确识别隐藏物体的完整3D位置，搜索速度显著快于随机搜索方法。

Conclusion: 语义抽象扩展使家庭机器人具备高效寻找丢失物体的能力，节省时间和精力，为机器人操作中的物体搜索问题提供新解决方案。

Abstract: Semantic Abstraction's key observation is that 2D VLMs' relevancy activations roughly correspond to their confidence of whether and where an object is in the scene. Thus, relevancy maps are treated as "abstract object" representations. We use this framework for learning 3D localization and completion for the exclusive domain of hidden objects, defined as objects that cannot be directly identified by a VLM because they are at least partially occluded. This process of localizing hidden objects is a form of unstructured search that can be performed more efficiently using historical data of where an object is frequently placed. Our model can accurately identify the complete 3D location of a hidden object on the first try significantly faster than a naive random search. These extensions to semantic abstraction hope to provide household robots with the skills necessary to save time and effort when looking for lost objects.

</details>


### [68] [Towards Signboard-Oriented Visual Question Answering: ViSignVQA Dataset, Method and Benchmark](https://arxiv.org/abs/2512.22218)
*Hieu Minh Nguyen,Tam Le-Thanh Dang,Kiet Van Nguyen*

Main category: cs.CV

TL;DR: 提出了首个越南语标牌视觉问答数据集ViSignVQA，包含10,762张图像和25,573个问答对，通过集成OCR和语言模型提升性能，并设计了多智能体框架达到75.98%准确率。


<details>
  <summary>Details</summary>
Motivation: 自然场景中标牌文本理解对视觉问答的实际应用至关重要，但低资源语言（如越南语）在此领域尚未得到充分探索，缺乏专门的标牌导向VQA数据集。

Method: 1. 构建ViSignVQA数据集，涵盖越南标牌的语言、文化和视觉特征；2. 将越南OCR模型（SwinTextSpotter）和预训练语言模型（ViT5）集成到SOTA VQA模型（BLIP-2等）中；3. 提出结合感知与推理智能体的多智能体VQA框架，利用GPT-4和多数投票决策。

Result: 1. OCR增强上下文使F1分数最高提升209%；2. 多智能体框架达到75.98%的准确率；3. 实验验证了领域特定资源对低资源语言文本VQA的重要性。

Conclusion: ViSignVQA作为首个越南语标牌理解多模态数据集，为OCR集成VQA模型的开发与评估提供了基准，强调了针对低资源语言构建领域特定资源的关键作用。

Abstract: Understanding signboard text in natural scenes is essential for real-world applications of Visual Question Answering (VQA), yet remains underexplored, particularly in low-resource languages. We introduce ViSignVQA, the first large-scale Vietnamese dataset designed for signboard-oriented VQA, which comprises 10,762 images and 25,573 question-answer pairs. The dataset captures the diverse linguistic, cultural, and visual characteristics of Vietnamese signboards, including bilingual text, informal phrasing, and visual elements such as color and layout. To benchmark this task, we adapted state-of-the-art VQA models (e.g., BLIP-2, LaTr, PreSTU, and SaL) by integrating a Vietnamese OCR model (SwinTextSpotter) and a Vietnamese pretrained language model (ViT5). The experimental results highlight the significant role of the OCR-enhanced context, with F1-score improvements of up to 209% when the OCR text is appended to questions. Additionally, we propose a multi-agent VQA framework combining perception and reasoning agents with GPT-4, achieving 75.98% accuracy via majority voting. Our study presents the first large-scale multimodal dataset for Vietnamese signboard understanding. This underscores the importance of domain-specific resources in enhancing text-based VQA for low-resource languages. ViSignVQA serves as a benchmark capturing real-world scene text characteristics and supporting the development and evaluation of OCR-integrated VQA models in Vietnamese.

</details>


### [69] [VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs](https://arxiv.org/abs/2512.22226)
*Naishan Zheng,Jie Huang,Qingpei Guo,Feng Zhao*

Main category: cs.CV

TL;DR: VideoScaffold是一个动态视频表示框架，通过弹性尺度事件分割和分层事件整合，自适应调整事件粒度，实现从细粒度帧理解到抽象事件推理的平滑过渡，在视频理解任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有静态视频处理方法（如稀疏采样、帧压缩、聚类）在应用于连续视频流时会产生碎片化或过度压缩的输出，无法有效处理视频帧间的冗余性和时间连贯性需求。

Method: 提出VideoScaffold框架，包含两个核心组件：1) 弹性尺度事件分割（EES）- 通过预测引导的分割动态优化事件边界；2) 分层事件整合（HEC）- 逐步将语义相关片段聚合成多层次抽象表示。

Result: 在离线和流式视频理解基准测试中，VideoScaffold实现了最先进的性能，其模块化设计能够无缝扩展现有基于图像的多模态大语言模型至连续视频理解任务。

Conclusion: VideoScaffold通过动态调整事件粒度和保持细粒度视觉语义，有效解决了长视频理解中的冗余性和连贯性问题，为流式视频理解提供了灵活可扩展的解决方案。

Abstract: Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.

</details>


### [70] [KAN-FPN-Stem:A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation](https://arxiv.org/abs/2512.22228)
*HaoNan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种基于KAN增强的FPN-Stem架构，用于改进ViT在密集预测任务（如姿态估计）中的前端设计，通过在FPN末端引入KAN卷积层来优化多尺度特征融合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有ViT模型（如ViTPose）的前端设计过于简单，其朴素的图像分块机制难以有效处理多尺度变化，并在初始特征提取阶段造成不可逆的信息损失，限制了模型性能。

Method: 保留FPN经典的'上采样-相加'融合流，但将其末端的标准线性3x3平滑卷积替换为基于KAN的卷积层，利用KAN强大的非线性建模能力自适应学习和修正多尺度融合过程中产生的'伪影'。

Result: 在COCO数据集上的大量实验表明，所提出的KAN-FPN-Stem在轻量级ViTPose-S基线模型上实现了高达+2.0 AP的性能提升。

Conclusion: 本文不仅提供了一个即插即用的高性能模块，更重要的是揭示了ViT前端的性能瓶颈往往不在于'特征细化'（注意力机制），而在于'特征融合'的质量，并通过引入KAN算子为此瓶颈提供了有效的解决路径。

Abstract: Vision Transformers (ViT) have demonstrated significant promise in dense prediction tasks such as pose estimation. However, their performance is frequently constrained by the overly simplistic front-end designs employed in models like ViTPose. This naive patchification mechanism struggles to effectively handle multi-scale variations and results in irreversible information loss during the initial feature extraction phase. To overcome this limitation, we introduce a novel KAN-enhanced FPN-Stem architecture. Through rigorous ablation studies, we first identified that the true bottleneck for performance improvement lies not in plug-and-play attention modules (e.g., CBAM), but in the post-fusion non-linear smoothing step within the FPN. Guided by this insight, our core innovation is to retain the classic "upsample-and-add" fusion stream of the FPN, but replace its terminal, standard linear 3x3 smoothing convolution with a powerful KAN-based convolutional layer. Leveraging its superior non-linear modeling capabilities, this KAN-based layer adaptively learns and rectifies the "artifacts" generated during the multi-scale fusion process. Extensive experiments on the COCO dataset demonstrate that our KAN-FPN-Stem achieves a significant performance boost of up to +2.0 AP over the lightweight ViTPose-S baseline. This work not only delivers a plug-and-play, high-performance module but, more importantly, reveals that: the performance bottleneck in ViT front-end often lies not in 'feature refinement' (Attention), but in the quality of 'feature fusion' (Fusion). Furthermore, it provides an effective path to address this bottleneck through the introduction of the KAN operator.

</details>


### [71] [Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction](https://arxiv.org/abs/2512.22237)
*Mengxiao Geng,Ran Hong,Xiaoling Xu,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出了一种元信息引导的跨域协同扩散模型（MiG-DM），用于提升低剂量PET图像质量，通过整合患者元信息和投影域物理知识，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 低剂量PET成像在减少患者辐射剂量的同时，面临噪声干扰、对比度降低和生理细节难以保留的挑战。现有方法常忽略投影域物理知识和患者特异性元信息，而这些对功能-语义关联挖掘至关重要。

Method: 提出MiG-DM模型，包含元信息编码模块（将临床参数转化为语义提示）和跨域架构（结合投影域和图像域处理）。投影域使用正弦图适配器通过卷积操作捕获全局物理结构。

Result: 在UDPET公共数据集和不同剂量水平的临床数据集上实验表明，MiG-DM在提升PET图像质量和保留生理细节方面优于当前最先进方法。

Conclusion: MiG-DM通过整合跨模态先验知识和跨域处理，有效解决了低剂量PET成像的关键问题，为高质量低剂量PET重建提供了新思路。

Abstract: Low-dose PET imaging is crucial for reducing patient radiation exposure but faces challenges like noise interference, reduced contrast, and difficulty in preserving physiological details. Existing methods often neglect both projection-domain physics knowledge and patient-specific meta-information, which are critical for functional-semantic correlation mining. In this study, we introduce a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates comprehensive cross-modal priors to generate high-quality PET images. Specifically, a meta-information encoding module transforms clinical parameters into semantic prompts by considering patient characteristics, dose-related information, and semi-quantitative parameters, enabling cross-modal alignment between textual meta-information and image reconstruction. Additionally, the cross-domain architecture combines projection-domain and image-domain processing. In the projection domain, a specialized sinogram adapter captures global physical structures through convolution operations equivalent to global image-domain filtering. Experiments on the UDPET public dataset and clinical datasets with varying dose levels demonstrate that MiG-DM outperforms state-of-the-art methods in enhancing PET image quality and preserving physiological details.

</details>


### [72] [Multi-objective hybrid knowledge distillation for efficient deep learning in smart agriculture](https://arxiv.org/abs/2512.22239)
*Phi-Hung Hoang,Nam-Thuan Trinh,Van-Manh Tran,Thi-Thu-Hong Phan*

Main category: cs.CV

TL;DR: 提出一种混合知识蒸馏框架，用于开发轻量高性能卷积神经网络，在资源受限的边缘设备上实现高效准确的农业图像识别。


<details>
  <summary>Details</summary>
Motivation: 智能农业中，深度学习模型在边缘设备部署面临计算效率与识别精度的权衡挑战，需要轻量且高性能的解决方案。

Method: 设计结合倒残差块与密集连接的自定义学生模型，通过ResNet18教师网络进行多目标知识蒸馏（硬标签监督、特征级蒸馏、响应级蒸馏和自蒸馏）。

Result: 在9类水稻种子品种识别任务中，学生模型准确率达98.56%（教师模型98.65%），仅需0.68 GFLOPs和约107万参数，计算成本降低2.7倍，模型尺寸减少超10倍；在4种植物叶片病害数据集上均表现稳健。

Conclusion: 该框架在保持高精度的同时显著降低计算和存储需求，证明了其在硬件受限的智能农业系统中具有鲁棒性、高效性和强大部署潜力。

Abstract: Deploying deep learning models on resource-constrained edge devices remains a major challenge in smart agriculture due to the trade-off between computational efficiency and recognition accuracy. To address this challenge, this study proposes a hybrid knowledge distillation framework for developing a lightweight yet high-performance convolutional neural network. The proposed approach designs a customized student model that combines inverted residual blocks with dense connectivity and trains it under the guidance of a ResNet18 teacher network using a multi-objective strategy that integrates hard-label supervision, feature-level distillation, response-level distillation, and self-distillation. Experiments are conducted on a rice seed variety identification dataset containing nine varieties and further extended to four plant leaf disease datasets, including rice, potato, coffee, and corn, to evaluate generalization capability. On the rice seed variety classification task, the distilled student model achieves an accuracy of 98.56%, which is only 0.09% lower than the teacher model (98.65%), while requiring only 0.68 GFLOPs and approximately 1.07 million parameters. This corresponds to a reduction of about 2.7 times in computational cost and more than 10 times in model size compared with the ResNet18 teacher model. In addition, compared with representative pretrained models, the proposed student reduces the number of parameters by more than 6 times relative to DenseNet121 and by over 80 times compared with the Vision Transformer (ViT) architecture, while maintaining comparable or superior classification accuracy. Consistent performance gains across multiple plant leaf disease datasets further demonstrate the robustness, efficiency, and strong deployment potential of the proposed framework for hardware-limited smart agriculture systems.

</details>


### [73] [Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions](https://arxiv.org/abs/2512.22263)
*Aahan Sachdeva,Dhanvinkumar Ganeshkumar,James E. Gallagher,Tyler Treat,Edward J. Oughton*

Main category: cs.CV

TL;DR: 提出自适应RGB-LWIR融合框架，通过动态选择最佳融合比例的目标检测模型，提升自主机器人在不同光照条件下的视觉性能。


<details>
  <summary>Details</summary>
Motivation: 传统RGB检测在低光环境下效果差，热成像系统缺乏颜色和纹理信息，限制了应急服务中自主机器人（如搜救任务）的视觉能力。

Method: 训练33个YOLO模型，使用超过22,000张标注图像，涵盖无光、微光和全光三种光照条件；通过11种融合比例（从100/0到0/100）对齐并融合RGB和LWIR视频流；动态选择最优检测模型。

Result: 全光条件下最佳模型（80/20融合）和微光条件下最佳模型（90/10融合）分别达到92.8%和92.0%的平均置信度，显著优于YOLOv5n和YOLOv11n基线；无光条件下最佳融合模型（40/60）达到71.0%，优于基线但无统计显著性。

Conclusion: 自适应RGB-LWIR融合提高了所有光照条件下的检测置信度和可靠性，增强了自主机器人的视觉性能。

Abstract: Autonomous robotic platforms are playing a growing role across the emergency services sector, supporting missions such as search and rescue operations in disaster zones and reconnaissance. However, traditional red-green-blue (RGB) detection pipelines struggle in low-light environments, and thermal-based systems lack color and texture information. To overcome these limitations, we present an adaptive framework that fuses RGB and long-wave infrared (LWIR) video streams at multiple fusion ratios and dynamically selects the optimal detection model for each illumination condition. We trained 33 You Only Look Once (YOLO) models on over 22,000 annotated images spanning three light levels: no-light (<10 lux), dim-light (10-1000 lux), and full-light (>1000 lux). To integrate both modalities, fusion was performed by blending aligned RGB and LWIR frames at eleven ratios, from full RGB (100/0) to full LWIR (0/100) in 10% increments. Evaluation showed that the best full-light model (80/20 RGB-LWIR) and dim-light model (90/10 fusion) achieved 92.8% and 92.0% mean confidence; both significantly outperformed the YOLOv5 nano (YOLOv5n) and YOLOv11 nano (YOLOv11n) baselines. Under no-light conditions, the top 40/60 fusion reached 71.0%, exceeding baselines though not statistically significant. Adaptive RGB-LWIR fusion improved detection confidence and reliability across all illumination conditions, enhancing autonomous robotic vision performance.

</details>


### [74] [Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models](https://arxiv.org/abs/2512.22272)
*Antara Titikhsha,Om Kulkarni,Dharun Muthaiah*

Main category: cs.CV

TL;DR: 本文提出了一种利用轻量级判别器作为外部引导信号的方法，通过人类感知嵌入（HPE）教师模型在文本到图像生成中引入几何理解，实现几何与风格的分离控制，显著提升语义对齐。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像扩散模型依赖表面外观，常违反几何约束，尤其当约束与文本提示风格冲突时，反映了生成模型与人类感知间的语义鸿沟。

Method: 使用在THINGS三元组数据集上训练的HPE教师模型捕捉人类对物体形状的敏感性，将其梯度注入潜在扩散过程；在Stable Diffusion v1.5、SiT-XL/2和PixArt-Σ三种架构上评估。

Result: 流模型需持续引导以防轨迹漂移；实现了复杂三维形状（如伊姆斯椅）到冲突材料（如粉色金属）的零样本迁移；引导生成相比无引导基线提升约80%的语义对齐。

Conclusion: 小型教师模型可可靠引导大型生成系统，增强几何控制并拓宽文本到图像合成的创作范围。

Abstract: Text-to-image diffusion models generate highly detailed textures, yet they often rely on surface appearance and fail to follow strict geometric constraints, particularly when those constraints conflict with the style implied by the text prompt. This reflects a broader semantic gap between human perception and current generative models. We investigate whether geometric understanding can be introduced without specialized training by using lightweight, off-the-shelf discriminators as external guidance signals. We propose a Human Perception Embedding (HPE) teacher trained on the THINGS triplet dataset, which captures human sensitivity to object shape. By injecting gradients from this teacher into the latent diffusion process, we show that geometry and style can be separated in a controllable manner. We evaluate this approach across three architectures: Stable Diffusion v1.5 with a U-Net backbone, the flow-matching model SiT-XL/2, and the diffusion transformer PixArt-Σ. Our experiments reveal that flow models tend to drift back toward their default trajectories without continuous guidance, and we demonstrate zero-shot transfer of complex three-dimensional shapes, such as an Eames chair, onto conflicting materials such as pink metal. This guided generation improves semantic alignment by about 80 percent compared to unguided baselines. Overall, our results show that small teacher models can reliably guide large generative systems, enabling stronger geometric control and broadening the creative range of text-to-image synthesis.

</details>


### [75] [GeCo: A Differentiable Geometric Consistency Metric for Video Generation](https://arxiv.org/abs/2512.22274)
*Leslie Gu,Junhwa Hur,Charles Herrmann,Fangneng Zhan,Todd Zickler,Deqing Sun,Hanspeter Pfister*

Main category: cs.CV

TL;DR: GeCo是一种几何基础度量方法，用于联合检测静态场景中的几何变形和遮挡不一致伪影，通过融合残差运动和深度先验生成可解释的一致性图，并用于视频生成模型的基准测试和训练指导。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在静态场景中常产生几何变形和遮挡不一致伪影，缺乏系统性的检测和评估方法，需要一种能够量化这些问题的几何基础度量。

Method: 提出GeCo度量方法，融合残差运动和深度先验，生成密集一致性图来检测几何变形和遮挡不一致伪影；将其应用于视频生成模型的系统性基准测试，并作为无需训练的指导损失来减少生成过程中的变形伪影。

Result: GeCo能够有效检测视频生成模型中的几何变形和遮挡不一致伪影，通过基准测试揭示了常见失败模式，并作为指导损失成功减少了生成视频中的变形伪影。

Conclusion: GeCo为视频生成模型的几何一致性评估提供了有效的度量工具，既能用于模型诊断，又能作为生成过程的指导，有助于提升视频生成的几何保真度。

Abstract: We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.

</details>


### [76] [The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency](https://arxiv.org/abs/2512.22275)
*Dingyu Wang,Zimu Yuan,Jiajun Liu,Shanggui Liu,Nan Zhou,Tianxing Xu,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 该研究开发了骨骼与关节（B&J）基准测试，评估AI模型在骨科和运动医学中的临床推理能力，发现模型在需要多模态整合的开放任务上表现显著不足，尚不具备临床胜任力。


<details>
  <summary>Details</summary>
Motivation: 当前基于医学考试或案例的基准测试无法真实评估AI模型在临床实践中的综合多模态推理能力，存在与真实患者护理需求的脱节。

Method: 构建包含1,245个真实骨科病例问题的B&J基准测试，涵盖7项临床推理任务；评估了11个视觉语言模型和6个大语言模型，并与专家标注结果进行对比。

Result: 模型在结构化选择题上准确率超90%，但在需要多模态整合的开放任务上准确率仅约60%；视觉语言模型存在严重的文本驱动幻觉，且医学专用模型未显现优势。

Conclusion: 当前AI模型尚不具备复杂多模态临床推理能力，应仅限于文本辅助角色；未来突破需依赖多模态整合与视觉理解的根本性进步。

Abstract: Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.

</details>


### [77] [FETAL-GAUGE: A Benchmark for Assessing Vision-Language Models in Fetal Ultrasound](https://arxiv.org/abs/2512.22278)
*Hussain Alasmawi,Numan Saeed,Mohammad Yaqub*

Main category: cs.CV

TL;DR: 本文提出了首个专门用于评估视觉语言模型在胎儿超声成像中性能的标准化基准Fetal-Gauge，包含超4.2万图像和9.3万问答对，揭示了当前最先进模型仅55%的准确率，远低于临床要求。


<details>
  <summary>Details</summary>
Motivation: 全球缺乏训练有素的超声医师，阻碍了必要的胎儿健康监测；深度学习有潜力提高超声医师效率并支持培训新从业者，但缺乏评估视觉语言模型在胎儿超声成像中性能的标准化基准。

Method: 构建Fetal-Gauge基准，包含超过42,000张图像和93,000个问答对，涵盖解剖平面识别、解剖结构视觉定位、胎儿方向评估、临床视图符合性和临床诊断等任务；系统评估多个最先进的通用和医学专用视觉语言模型。

Result: 最佳性能模型仅达到55%的准确率，远低于临床要求；分析揭示了当前视觉语言模型在胎儿超声解释中的关键局限性。

Conclusion: Fetal-Gauge为推进产前护理中的多模态深度学习建立了严格基础，并提供了解决全球医疗可及性挑战的途径；迫切需要领域适应的架构和专门的训练方法。

Abstract: The growing demand for prenatal ultrasound imaging has intensified a global shortage of trained sonographers, creating barriers to essential fetal health monitoring. Deep learning has the potential to enhance sonographers' efficiency and support the training of new practitioners. Vision-Language Models (VLMs) are particularly promising for ultrasound interpretation, as they can jointly process images and text to perform multiple clinical tasks within a single framework. However, despite the expansion of VLMs, no standardized benchmark exists to evaluate their performance in fetal ultrasound imaging. This gap is primarily due to the modality's challenging nature, operator dependency, and the limited public availability of datasets. To address this gap, we present Fetal-Gauge, the first and largest visual question answering benchmark specifically designed to evaluate VLMs across various fetal ultrasound tasks. Our benchmark comprises over 42,000 images and 93,000 question-answer pairs, spanning anatomical plane identification, visual grounding of anatomical structures, fetal orientation assessment, clinical view conformity, and clinical diagnosis. We systematically evaluate several state-of-the-art VLMs, including general-purpose and medical-specific models, and reveal a substantial performance gap: the best-performing model achieves only 55\% accuracy, far below clinical requirements. Our analysis identifies critical limitations of current VLMs in fetal ultrasound interpretation, highlighting the urgent need for domain-adapted architectures and specialized training approaches. Fetal-Gauge establishes a rigorous foundation for advancing multimodal deep learning in prenatal care and provides a pathway toward addressing global healthcare accessibility challenges. Our benchmark will be publicly available once the paper gets accepted.

</details>


### [78] [A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation](https://arxiv.org/abs/2512.22294)
*Philip Xu,David Elizondo,Raouf Hamzaoui*

Main category: cs.CV

TL;DR: Uni4D是一个统一框架，用于大规模开放词汇3D检索和可控4D生成，通过文本、3D模型和图像模态的三级对齐实现。


<details>
  <summary>Details</summary>
Motivation: 当前3D检索和4D生成领域缺乏统一的跨模态对齐方法，难以实现高质量的语义对齐和动态内容生成。

Method: 基于Align3D 130数据集，采用3D文本多头注意力机制和搜索模型，通过三个组件增强跨模态对齐：精确文本到3D检索、多视角3D到图像对齐、图像到文本对齐。

Result: 实验表明Uni4D实现了高质量的3D检索和可控4D生成，能够生成时间一致的4D资产。

Conclusion: 该框架推动了动态多模态理解的发展，具有实际应用价值，为3D/4D内容生成提供了统一的解决方案。

Abstract: We introduce Uni4D, a unified framework for large scale open vocabulary 3D retrieval and controlled 4D generation based on structured three level alignment across text, 3D models, and image modalities. Built upon the Align3D 130 dataset, Uni4D employs a 3D text multi head attention and search model to optimize text to 3D retrieval through improved semantic alignment. The framework further strengthens cross modal alignment through three components: precise text to 3D retrieval, multi view 3D to image alignment, and image to text alignment for generating temporally consistent 4D assets. Experimental results demonstrate that Uni4D achieves high quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding and practical applications.

</details>


### [79] [Learning Dynamic Scene Reconstruction with Sinusoidal Geometric Priors](https://arxiv.org/abs/2512.22295)
*Tian Guo,Hui Yuan,Philip Xu,David Elizondo*

Main category: cs.CV

TL;DR: 提出SirenPose损失函数，结合正弦表示网络的周期激活特性与关键点几何先验，提升动态3D场景重建的时空一致性


<details>
  <summary>Details</summary>
Motivation: 现有方法在快速运动和多目标场景中难以保持运动建模精度与时空一致性

Method: 结合正弦表示网络周期激活特性与关键点几何先验，引入物理启发的约束机制，扩展训练数据集至60万标注实例

Result: 使用SirenPose训练的模型在时空一致性指标上显著优于现有方法，在快速运动和复杂场景变化中表现更优

Conclusion: SirenPose通过周期性激活与几何先验的结合，有效提升了动态3D场景重建的时空一致性

Abstract: We propose SirenPose, a novel loss function that combines the periodic activation properties of sinusoidal representation networks with geometric priors derived from keypoint structures to improve the accuracy of dynamic 3D scene reconstruction. Existing approaches often struggle to maintain motion modeling accuracy and spatiotemporal consistency in fast moving and multi target scenes. By introducing physics inspired constraint mechanisms, SirenPose enforces coherent keypoint predictions across both spatial and temporal dimensions. We further expand the training dataset to 600,000 annotated instances to support robust learning. Experimental results demonstrate that models trained with SirenPose achieve significant improvements in spatiotemporal consistency metrics compared to prior methods, showing superior performance in handling rapid motion and complex scene changes.

</details>


### [80] [PortionNet: Distilling 3D Geometric Knowledge for Food Nutrition Estimation](https://arxiv.org/abs/2512.22304)
*Darrin Bright,Rakshith Raj,Kanchan Keisham*

Main category: cs.CV

TL;DR: 提出PortionNet框架，通过跨模态知识蒸馏从点云学习几何特征，仅需RGB图像即可实现食物营养估计，无需深度传感器。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度的方法需要专用传感器，限制了在智能手机上的普及；单RGB图像因缺失3D信息导致营养估计不准确。

Method: 采用双模式训练策略，通过轻量适配器网络模仿点云表示，实现伪3D推理；训练时使用点云几何特征，推理时仅需RGB图像。

Result: 在MetaFood3D数据集上取得体积和能量估计的最优性能；在SimpleFood45跨数据集评估中表现出良好的能量估计泛化能力。

Conclusion: PortionNet通过知识蒸馏实现了无需深度传感器的准确食物营养估计，为移动设备应用提供了可行方案。

Abstract: Accurate food nutrition estimation from single images is challenging due to the loss of 3D information. While depth-based methods provide reliable geometry, they remain inaccessible on most smartphones because of depth-sensor requirements. To overcome this challenge, we propose PortionNet, a novel cross-modal knowledge distillation framework that learns geometric features from point clouds during training while requiring only RGB images at inference. Our approach employs a dual-mode training strategy where a lightweight adapter network mimics point cloud representations, enabling pseudo-3D reasoning without any specialized hardware requirements. PortionNet achieves state-of-the-art performance on MetaFood3D, outperforming all previous methods in both volume and energy estimation. Cross-dataset evaluation on SimpleFood45 further demonstrates strong generalization in energy estimation.

</details>


### [81] [Attack-Aware Deepfake Detection under Counter-Forensic Manipulations](https://arxiv.org/abs/2512.22303)
*Noor Fatima,Hasan Faraz Khan,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出一种攻击感知的深度伪造与图像取证检测器，结合红队训练与随机化测试时防御，通过双流架构实现鲁棒检测、校准概率和可解释热图。


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器在真实部署环境中面临对抗攻击、概率校准不佳、缺乏可解释证据等问题，需构建能应对复杂攻击且提供透明决策依据的实用化系统。

Method: 采用双流架构：语义流使用预训练骨干网络提取内容特征，取证流提取法医残差；通过轻量残差适配器融合特征进行分类，浅层特征金字塔网络在弱监督下生成篡改热图。训练时采用红队策略，每批次应用K种最差反取证操作（如JPEG重对齐、重采样扭曲、去噪再纹理等）；测试时注入低成本抖动（如尺寸裁剪相位变化、JPEG相位偏移等）并聚合预测。

Result: 在标准深度伪造数据集和低光照高压缩监控场景的评估中，模型在攻击下保持近乎完美的排名性能，校准误差低，弃权风险小，在再纹理攻击下性能下降可控，同时生成集中于人脸区域的可行动热图。

Conclusion: 该方法建立了模块化、数据高效且可实际部署的攻击感知检测基线，实现了校准概率与可操作热图的平衡，为鲁棒取证系统提供了实用解决方案。

Abstract: This work presents an attack-aware deepfake and image-forensics detector designed for robustness, well-calibrated probabilities, and transparent evidence under realistic deployment conditions. The method combines red-team training with randomized test-time defense in a two-stream architecture, where one stream encodes semantic content using a pretrained backbone and the other extracts forensic residuals, fused via a lightweight residual adapter for classification, while a shallow Feature Pyramid Network style head produces tamper heatmaps under weak supervision. Red-team training applies worst-of-K counter-forensics per batch, including JPEG realign and recompress, resampling warps, denoise-to-regrain operations, seam smoothing, small color and gamma shifts, and social-app transcodes, while test-time defense injects low-cost jitters such as resize and crop phase changes, mild gamma variation, and JPEG phase shifts with aggregated predictions. Heatmaps are guided to concentrate within face regions using face-box masks without strict pixel-level annotations. Evaluation on existing benchmarks, including standard deepfake datasets and a surveillance-style split with low light and heavy compression, reports clean and attacked performance, AUC, worst-case accuracy, reliability, abstention quality, and weak-localization scores. Results demonstrate near-perfect ranking across attacks, low calibration error, minimal abstention risk, and controlled degradation under regrain, establishing a modular, data-efficient, and practically deployable baseline for attack-aware detection with calibrated probabilities and actionable heatmaps.

</details>


### [82] [MoFu: Scale-Aware Modulation and Fourier Fusion for Multi-Subject Video Generation](https://arxiv.org/abs/2512.22310)
*Run Ling,Ke Cao,Jian Lu,Ao Ma,Haowei Liu,Runze He,Changwei Wang,Rongtao Xu,Yihua Shao,Zhanjie Zhang,Peng Wu,Guibing Guo,Wei Feng,Zheng Zhang,Jingjing Lv,Junjie Shen,Ching Law,Xingwei Wang*

Main category: cs.CV

TL;DR: 提出MoFu框架解决多主体视频生成中的尺度不一致和排列敏感性问题，通过尺度感知调制和傅里叶融合策略提升生成质量


<details>
  <summary>Details</summary>
Motivation: 现有多主体视频生成方法存在尺度不一致（主体尺寸变化导致不自然）和排列敏感性（参考图像顺序导致主体扭曲）两大挑战

Method: 1. 尺度感知调制（SMO）：利用LLM从文本提示提取隐式尺度线索并调制特征；2. 傅里叶融合：通过快速傅里叶变换处理参考特征的频率信息生成统一表示；3. 尺度-排列稳定性损失函数

Result: 在专门构建的基准测试中，MoFu在保持自然尺度、主体保真度和整体视觉质量方面显著优于现有方法

Conclusion: MoFu框架能有效解决多主体视频生成的尺度与排列敏感性问题，为可控视频合成提供了新解决方案

Abstract: Multi-subject video generation aims to synthesize videos from textual prompts and multiple reference images, ensuring that each subject preserves natural scale and visual fidelity. However, current methods face two challenges: scale inconsistency, where variations in subject size lead to unnatural generation, and permutation sensitivity, where the order of reference inputs causes subject distortion. In this paper, we propose MoFu, a unified framework that tackles both challenges. For scale inconsistency, we introduce Scale-Aware Modulation (SMO), an LLM-guided module that extracts implicit scale cues from the prompt and modulates features to ensure consistent subject sizes. To address permutation sensitivity, we present a simple yet effective Fourier Fusion strategy that processes the frequency information of reference features via the Fast Fourier Transform to produce a unified representation. Besides, we design a Scale-Permutation Stability Loss to jointly encourage scale-consistent and permutation-invariant generation. To further evaluate these challenges, we establish a dedicated benchmark with controlled variations in subject scale and reference permutation. Extensive experiments demonstrate that MoFu significantly outperforms existing methods in preserving natural scale, subject fidelity, and overall visual quality.

</details>


### [83] [VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning](https://arxiv.org/abs/2512.22315)
*Yang Ding,Yizhen Zhang,Xin Lai,Ruihang Chu,Yujiu Yang*

Main category: cs.CV

TL;DR: 提出VideoZoomer框架，通过动态控制视觉焦点解决MLLMs在长视频理解中的上下文限制问题，实现多轮交互式细粒度证据收集。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在长视频理解中受限于上下文窗口，常采用均匀采样或静态预选方法，可能忽略关键证据且无法在推理过程中修正初始选择错误。

Method: 提出代理框架VideoZoomer，从低帧率概览开始，通过调用时间缩放工具在自主选择的时刻获取高帧率片段，采用两阶段训练策略：基于蒸馏示例和反思轨迹的监督微调，以及强化学习优化代理策略。

Result: 7B模型在多种长视频理解和推理基准测试中表现优异，产生多样化复杂推理模式，超越现有开源模型并在挑战性任务中媲美专有系统，同时在减少帧预算下实现更高效率。

Conclusion: VideoZoomer通过动态视觉焦点控制机制，显著提升了MLLMs的长视频理解能力，证明了代理式交互框架在复杂多模态任务中的有效性。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.

</details>


### [84] [SpotEdit: Selective Region Editing in Diffusion Transformers](https://arxiv.org/abs/2512.22323)
*Zhibin Qin,Zhenxiong Tan,Zeqing Wang,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: 提出SpotEdit框架，通过选择性更新图像修改区域而非全局处理，减少扩散模型编辑中的冗余计算，保持未修改区域的高保真度。


<details>
  <summary>Details</summary>
Motivation: 当前扩散Transformer模型在图像编辑时需对所有区域统一去噪，但多数编辑仅涉及局部修改，导致计算冗余且可能损害未修改区域质量。

Method: 1. SpotSelector：基于感知相似性识别稳定区域，复用条件图像特征跳过计算；2. SpotFusion：通过动态融合机制自适应混合条件特征与编辑后特征，保持上下文连贯性。

Result: SpotEdit实现了高效精确的图像编辑，显著减少不必要计算，同时维持未修改区域的高保真度。

Conclusion: 该研究证明选择性区域更新在扩散编辑中的有效性，为高效图像编辑提供了无需训练的轻量级解决方案。

Abstract: Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.

</details>


### [85] [DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models](https://arxiv.org/abs/2512.22324)
*Jianrong Zhang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 提出DeMoGen，一种基于能量扩散模型的运动分解训练范式，能够将复杂运动分解为语义子组件并重组生成新运动


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注前向建模（如文本到运动），但缺乏将整体运动逆向分解为语义子组件的能力

Method: 使用能量扩散模型捕获多运动概念的组合分布，提出三种训练变体：显式分解文本训练、正交自监督分解、语义一致性约束

Result: 模型能够从复杂运动序列中解耦可重用运动基元，分解后的概念可灵活重组生成超出训练分布的新运动

Conclusion: DeMoGen实现了运动分解与重组，构建的文本分解数据集为文本到运动生成和运动组合提供了扩展资源

Abstract: Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.

</details>


### [86] [The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma](https://arxiv.org/abs/2512.22331)
*Mariya Miteva,Maria Nisheva-Pavlova*

Main category: cs.CV

TL;DR: 提出基于变分自编码器的多视图潜在表示学习框架，整合T1Gd和FLAIR MRI的放射组学特征，用于胶质母细胞瘤MGMT启动子甲基化状态的无创预测。


<details>
  <summary>Details</summary>
Motivation: 传统单模态和早期融合方法在预测胶质母细胞瘤MGMT启动子甲基化状态时存在特征冗余高、模态特异性信息建模不完整的问题，需要更有效的多模态整合方法。

Method: 使用独立概率编码器分别编码T1Gd和FLAIR MRI的放射组学特征，在紧凑潜在空间中进行融合，通过变分自编码器框架实现多视图潜在表示学习。

Result: 该方法在保留模态特异性结构的同时实现了有效的多模态整合，生成的潜在嵌入可用于MGMT启动子甲基化分类。

Conclusion: 提出的多视图潜在表示学习框架为放射基因组学中分子肿瘤特征的无创推断提供了更有效的多模态整合方案。

Abstract: Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.

</details>


### [87] [Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides](https://arxiv.org/abs/2512.22335)
*Olaide N. Oyelade,Oliver Hoxey,Yulia Humrye*

Main category: cs.CV

TL;DR: 提出基于视觉变换器的端到端流程，联合分析H&E和IHC染色全切片图像，实现像素级HER2评分定位与分类。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以提供像素级HER2状态定位，且联合分析H&E与IHC图像进行HER2评分存在挑战，需提升自动化评估精度。

Method: 采用端到端视觉变换器系统，包括H&E图像肿瘤定位的块处理、新颖的IHC区域映射函数，以及临床启发的四分类HER2评分机制。

Result: 肿瘤定位分类准确率高，HER2状态预测准确率达0.94，特异性0.933，与病理学家评估结果具有可比性。

Conclusion: 基于视觉变换器的端到端模型能有效联合评估H&E与IHC图像，实现自动化HER2评分，具有临床应用潜力。

Abstract: The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring

</details>


### [88] [Human-like visual computing advances explainability and few-shot learning in deep neural networks for complex physiological data](https://arxiv.org/abs/2512.22349)
*Alaa Alahmadi,Mohamed Hasan*

Main category: cs.CV

TL;DR: 提出一种基于感知启发的伪着色技术，通过将临床相关时间特征（如QT间期）编码为结构化颜色表示，提升深度神经网络在心电图分析中的少样本学习能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的生理信号分析模型通常需要大量训练数据，且缺乏可解释性，限制了其临床可靠性和与人类推理的一致性。

Method: 使用感知启发的伪着色技术将QT间期等临床特征编码为颜色表示；采用原型网络和ResNet-18架构，在单次心跳和完整10秒心电图上评估单样本和少样本学习；通过可解释性分析验证模型关注临床相关特征。

Result: 伪着色技术使模型能从1个或5个训练样本中学习判别性特征；可解释性分析显示模型能关注临床相关心电特征并抑制无关信号；聚合多个心跳周期可进一步提升性能，模拟人类跨心跳的感知平均。

Conclusion: 人类感知编码方法能够弥合医疗人工智能在数据效率、可解释性和因果推理之间的差距，为数据稀缺场景下的可靠生理信号分析提供新途径。

Abstract: Machine vision models, particularly deep neural networks, are increasingly applied to physiological signal interpretation, including electrocardiography (ECG), yet they typically require large training datasets and offer limited insight into the causal features underlying their predictions. This lack of data efficiency and interpretability constrains their clinical reliability and alignment with human reasoning. Here, we show that a perception-informed pseudo-colouring technique, previously demonstrated to enhance human ECG interpretation, can improve both explainability and few-shot learning in deep neural networks analysing complex physiological data.
  We focus on acquired, drug-induced long QT syndrome (LQTS) as a challenging case study characterised by heterogeneous signal morphology, variable heart rate, and scarce positive cases associated with life-threatening arrhythmias such as torsades de pointes. This setting provides a stringent test of model generalisation under extreme data scarcity. By encoding clinically salient temporal features, such as QT-interval duration, into structured colour representations, models learn discriminative and interpretable features from as few as one or five training examples. Using prototypical networks and a ResNet-18 architecture, we evaluate one-shot and few-shot learning on ECG images derived from single cardiac cycles and full 10-second rhythms. Explainability analyses show that pseudo-colouring guides attention toward clinically meaningful ECG features while suppressing irrelevant signal components. Aggregating multiple cardiac cycles further improves performance, mirroring human perceptual averaging across heartbeats. Together, these findings demonstrate that human-like perceptual encoding can bridge data efficiency, explainability, and causal reasoning in medical machine intelligence.

</details>


### [89] [Self-Evaluation Unlocks Any-Step Text-to-Image Generation](https://arxiv.org/abs/2512.22374)
*Xin Yu,Xiaojuan Qi,Zhengqi Li,Kai Zhang,Richard Zhang,Zhe Lin,Eli Shechtman,Tianyu Wang,Yotam Nitzan*

Main category: cs.CV

TL;DR: 提出Self-E模型，一种从零开始训练、支持任意步数推理的文本到图像生成方法，通过自评估机制实现少步高质量生成


<details>
  <summary>Details</summary>
Motivation: 传统扩散或流匹配模型依赖局部监督需要多步推理，蒸馏方法需要预训练教师模型，需要一种既能从零训练又能实现高效少步生成的统一框架

Method: 结合流匹配学习与自评估机制，模型使用当前分数估计评估自身生成样本作为动态自教师，实现瞬时局部学习与自驱动全局匹配

Result: 在大规模文本到图像基准测试中，Self-E在少步生成表现出色，50步时与最先进流匹配模型竞争，性能随推理步数增加单调提升

Conclusion: Self-E是首个从零训练、支持任意步数的文本到图像模型，为高效可扩展生成提供了统一框架

Abstract: We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.

</details>


### [90] [VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement](https://arxiv.org/abs/2512.22351)
*Zhengfei Kuang,Rui Lin,Long Zhao,Gordon Wetzstein,Saining Xie,Sanghyun Woo*

Main category: cs.CV

TL;DR: 本文提出了一种基于多模态大语言模型（MLLMs）的3D场景物体自动排列系统，通过引入MCP API、专用视觉工具和多智能体协作框架，解决了MLLMs在3D场景操作中的视觉定位弱、空间理解不足和迭代更新易错等关键挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在2D视觉语言任务中取得显著进展，但其在复杂3D场景操作中的应用仍未被充分探索。现有MLLMs在3D物体排列任务中存在视觉定位能力弱、难以将程序化编辑与精确3D结果关联，以及缺乏空间感知反馈机制等问题。

Method: 1. 引入基于MCP的API，将交互从脆弱的原始代码操作转向更鲁棒的函数级更新；2. 使用专用视觉工具增强MLLM的3D场景理解能力，包括场景状态分析、空间信息收集和动作结果验证；3. 提出协作多智能体框架，分配规划、执行和验证角色，以处理多步指令并从中间错误中恢复。

Result: 在25个复杂物体排列任务上的实验表明，该方法显著优于现有基线，有效实现了3D场景的精确操作和物体排列。

Conclusion: 通过结合MCP API、视觉工具增强和多智能体协作，本研究成功将MLLMs扩展到复杂3D场景操作领域，为3D-aware的语义操作提供了可行框架。

Abstract: Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io

</details>


### [91] [iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI](https://arxiv.org/abs/2512.22392)
*Himanshu Naidu,Yuxiang Zhang,Sachin Mehta,Anat Caspi*

Main category: cs.CV

TL;DR: 介绍iOSPointMapper移动应用，利用iPhone/iPad的语义分割、LiDAR深度估计和GPS/IMU数据，实现实时、隐私保护的实时人行道特征检测与定位，并通过用户验证界面提升数据质量。


<details>
  <summary>Details</summary>
Motivation: 当前人行道数据收集方法成本高、碎片化且难以扩展，缺乏准确、最新的数据来支持无障碍行人基础设施建设。

Method: 开发iOS应用，结合设备端语义分割、LiDAR深度估计、GPS/IMU融合定位技术检测交通标志、信号灯等特征，并设计用户引导的标注界面进行验证，数据匿名化后传输至TDEI平台。

Result: 系统评估显示其在特征检测和空间映射方面表现良好，具备增强行人地图绘制的潜力，并能与多式联运数据集无缝集成。

Conclusion: iOSPointMapper提供了一种可扩展、以用户为中心的方法，有助于填补行人基础设施关键数据缺口，推动无障碍环境建设。

Abstract: Accurate, up-to-date sidewalk data is essential for building accessible and inclusive pedestrian infrastructure, yet current approaches to data collection are often costly, fragmented, and difficult to scale. We introduce iOSPointMapper, a mobile application that enables real-time, privacy-conscious sidewalk mapping on the ground, using recent-generation iPhones and iPads. The system leverages on-device semantic segmentation, LiDAR-based depth estimation, and fused GPS/IMU data to detect and localize sidewalk-relevant features such as traffic signs, traffic lights and poles. To ensure transparency and improve data quality, iOSPointMapper incorporates a user-guided annotation interface for validating system outputs before submission. Collected data is anonymized and transmitted to the Transportation Data Exchange Initiative (TDEI), where it integrates seamlessly with broader multimodal transportation datasets. Detailed evaluations of the system's feature detection and spatial mapping performance reveal the application's potential for enhanced pedestrian mapping. Together, these capabilities offer a scalable and user-centered approach to closing critical data gaps in pedestrian

</details>


### [92] [DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization](https://arxiv.org/abs/2512.22406)
*Hansang Lee,Chaelin Lee,Nieun Seo,Joon Seok Lim,Helen Hong*

Main category: cs.CV

TL;DR: DeFloMat是一种新的生成式目标检测框架，通过条件流匹配替代扩散模型的多步去噪过程，在仅需3步推理的情况下实现最先进精度，解决了扩散检测器在临床应用中延迟过高的问题。


<details>
  <summary>Details</summary>
Motivation: 扩散检测器（如DiffusionDet）虽然精度高，但需要大量采样步骤（T>60），在克罗恩病磁共振肠造影等时间敏感的临床应用中存在延迟瓶颈，需要更高效的检测方法。

Method: 提出DeFloMat框架，将条件最优传输理论中的整流流近似应用于条件流匹配，用确定性流场替代随机扩散路径，通过简单ODE求解器实现快速推理。

Result: 在MRE临床数据集上，DeFloMat仅用3步推理就达到43.32% AP10:50精度，比DiffusionDet在4步时的最佳性能（31.03%）提升1.4倍，并在少步情况下表现出更好的召回率和稳定性。

Conclusion: DeFloMat解决了生成式检测精度与临床效率之间的权衡，为稳定快速的目标定位设定了新标准，特别适用于时间敏感的医疗应用。

Abstract: We propose DeFloMat (Detection with Flow Matching), a novel generative object detection framework that addresses the critical latency bottleneck of diffusion-based detectors, such as DiffusionDet, by integrating Conditional Flow Matching (CFM). Diffusion models achieve high accuracy by formulating detection as a multi-step stochastic denoising process, but their reliance on numerous sampling steps ($T \gg 60$) makes them impractical for time-sensitive clinical applications like Crohn's Disease detection in Magnetic Resonance Enterography (MRE). DeFloMat replaces this slow stochastic path with a highly direct, deterministic flow field derived from Conditional Optimal Transport (OT) theory, specifically approximating the Rectified Flow. This shift enables fast inference via a simple Ordinary Differential Equation (ODE) solver. We demonstrate the superiority of DeFloMat on a challenging MRE clinical dataset. Crucially, DeFloMat achieves state-of-the-art accuracy ($43.32\% \text{ } AP_{10:50}$) in only $3$ inference steps, which represents a $1.4\times$ performance improvement over DiffusionDet's maximum converged performance ($31.03\% \text{ } AP_{10:50}$ at $4$ steps). Furthermore, our deterministic flow significantly enhances localization characteristics, yielding superior Recall and stability in the few-step regime. DeFloMat resolves the trade-off between generative accuracy and clinical efficiency, setting a new standard for stable and rapid object localization.

</details>


### [93] [Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing](https://arxiv.org/abs/2512.22464)
*Sukhyun Jeong,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: 本文提出PGR$^2$M方法，通过结合姿势代码和残差代码的混合表示，改进基于文本的3D动作生成和编辑，提升重建保真度和局部可控性。


<details>
  <summary>Details</summary>
Motivation: 现有基于姿势代码的框架（如CoMo）在捕捉细微时间动态和高频细节方面存在局限，导致重建保真度和局部可控性下降，需要一种能更好平衡全局结构和局部细节的表示方法。

Method: 引入姿势引导残差细化运动（PGR$^2$M）方法，使用残差向量量化（RVQ）学习残差代码以补充姿势代码，通过姿势引导RVQ分词器分解动作，并采用残差丢弃防止过度依赖残差；基于此，使用基础Transformer预测姿势代码，细化Transformer预测残差代码。

Result: 在HumanML3D和KIT-ML数据集上的实验显示，PGR$^2$M在Fréchet起始距离和重建指标上优于CoMo及基于扩散和分词的基线方法，用户研究证实其能实现直观且结构保持的动作编辑。

Conclusion: PGR$^2$M通过混合表示有效提升了动作生成和编辑的质量，在保持语义对齐和可编辑性的同时，增强了时间动态和细节的建模能力。

Abstract: Text-based 3D motion generation aims to automatically synthesize diverse motions from natural-language descriptions to extend user creativity, whereas motion editing modifies an existing motion sequence in response to text while preserving its overall structure. Pose-code-based frameworks such as CoMo map quantifiable pose attributes into discrete pose codes that support interpretable motion control, but their frame-wise representation struggles to capture subtle temporal dynamics and high-frequency details, often degrading reconstruction fidelity and local controllability. To address this limitation, we introduce pose-guided residual refinement for motion (PGR$^2$M), a hybrid representation that augments interpretable pose codes with residual codes learned via residual vector quantization (RVQ). A pose-guided RVQ tokenizer decomposes motion into pose latents that encode coarse global structure and residual latents that model fine-grained temporal variations. Residual dropout further discourages over-reliance on residuals, preserving the semantic alignment and editability of the pose codes. On top of this tokenizer, a base Transformer autoregressively predicts pose codes from text, and a refine Transformer predicts residual codes conditioned on text, pose codes, and quantization stage. Experiments on HumanML3D and KIT-ML show that PGR$^2$M improves Fréchet inception distance and reconstruction metrics for both generation and editing compared with CoMo and recent diffusion- and tokenization-based baselines, while user studies confirm that it enables intuitive, structure-preserving motion edits.

</details>


### [94] [MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments](https://arxiv.org/abs/2512.22867)
*Zhuonan Liu,Xinyu Zhang,Zishuo Wang,Tomohito Kawabata,Xuesu Xiao,Ling Xiao*

Main category: cs.CV

TL;DR: 本文介绍了MUSON数据集，一个用于短时域社交导航的多模态数据集，包含结构化思维链标注，旨在解决现有数据集缺乏推理监督和动作分布长尾的问题。


<details>
  <summary>Details</summary>
Motivation: 现有社交导航数据集缺乏明确的推理监督，且动作分布呈现高度长尾，限制了模型学习安全关键行为的能力。

Method: 在多样化的室内外校园场景中收集数据，采用五步思维链标注（感知、预测、推理、动作、解释），明确建模静态物理约束并构建合理平衡的离散动作空间。

Result: 与SNEI相比，MUSON提供了更一致的推理、动作和解释；在多个先进小视觉语言模型上的基准测试显示，Qwen2.5-VL-3B取得了最高的决策准确率（0.8625）。

Conclusion: MUSON是一个有效且可复用的社交合规导航基准数据集，已公开提供。

Abstract: Socially compliant navigation requires structured reasoning over dynamic pedestrians and physical constraints to ensure safe and interpretable decisions. However, existing social navigation datasets often lack explicit reasoning supervision and exhibit highly long-tailed action distributions, limiting models' ability to learn safety-critical behaviors. To address these issues, we introduce MUSON, a multimodal dataset for short-horizon social navigation collected across diverse indoor and outdoor campus scenes. MUSON adopts a structured five-step Chain-of-Thought annotation consisting of perception, prediction, reasoning, action, and explanation, with explicit modeling of static physical constraints and a rationally balanced discrete action space. Compared to SNEI, MUSON provides consistent reasoning, action, and explanation. Benchmarking multiple state-of-the-art Small Vision Language Models on MUSON shows that Qwen2.5-VL-3B achieves the highest decision accuracy of 0.8625, demonstrating that MUSON serves as an effective and reusable benchmark for socially compliant navigation. The dataset is publicly available at https://huggingface.co/datasets/MARSLab/MUSON

</details>


### [95] [Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy](https://arxiv.org/abs/2512.22423)
*Amil Khan,Matheus Palhares Viana,Suraj Mishra,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出Bright-4B，一个40亿参数的基础模型，可直接从3D明场图像中分割亚细胞结构，无需荧光标记或复杂后处理。


<details>
  <summary>Details</summary>
Motivation: 现有无标记3D明场显微镜虽能快速可视化细胞形态，但稳健的体积分割通常依赖荧光或繁重后处理，需要填补这一技术空白。

Method: 采用硬件对齐的Native Sparse Attention机制捕获多尺度上下文，结合深度-宽度残差超连接稳定表征流，使用软混合专家实现自适应能力，并引入即插即用各向异性补丁嵌入以保持几何保真度。

Result: 在多个共聚焦数据集上，Bright-4B能准确分割细胞核、线粒体等细胞器，在跨深度和细胞类型的精细结构保留方面优于当前CNN和Transformer基线模型。

Conclusion: Bright-4B为大规模无标记3D细胞图谱构建提供了有效工具，所有代码、预训练权重和下游微调模型将开源以推动该领域发展。

Abstract: Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.

</details>


### [96] [FluenceFormer: Transformer-Driven Multi-Beam Fluence Map Regression for Radiotherapy Planning](https://arxiv.org/abs/2512.22425)
*Ujunwa Mgboh,Rafi Ibn Sultan,Joshua Kim,Kundan Thind,Dongxiao Zhu*

Main category: cs.CV

TL;DR: 提出FluenceFormer框架，使用两阶段Transformer架构进行放疗计划中的注量图预测，通过物理感知损失函数提升预测准确性和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 传统卷积方法难以捕捉长距离依赖关系，导致放疗计划预测存在结构不一致或物理不可实现的问题，需要更有效的几何感知预测方法。

Method: 采用两阶段Transformer框架：第一阶段从解剖结构预测全局剂量先验，第二阶段结合显式射束几何信息回归物理校准的注量图；提出Fluence-Aware Regression损失函数，整合体素保真度、梯度平滑度、结构一致性和射束能量守恒。

Result: 在多种Transformer骨干网络（Swin UNETR等）上验证，Swin UNETR版本表现最佳，将能量误差降至4.5%，结构保真度有统计学显著提升（p<0.05），优于现有CNN和单阶段方法。

Conclusion: FluenceFormer框架能够有效解决注量图预测的逆问题，通过几何感知和物理约束的回归方法，实现了更准确、结构一致的放疗计划预测。

Abstract: Fluence map prediction is central to automated radiotherapy planning but remains an ill-posed inverse problem due to the complex relationship between volumetric anatomy and beam-intensity modulation. Convolutional methods in prior work often struggle to capture long-range dependencies, which can lead to structurally inconsistent or physically unrealizable plans. We introduce \textbf{FluenceFormer}, a backbone-agnostic transformer framework for direct, geometry-aware fluence regression. The model uses a unified two-stage design: Stage~1 predicts a global dose prior from anatomical inputs, and Stage~2 conditions this prior on explicit beam geometry to regress physically calibrated fluence maps. Central to the approach is the \textbf{Fluence-Aware Regression (FAR)} loss, a physics-informed objective that integrates voxel-level fidelity, gradient smoothness, structural consistency, and beam-wise energy conservation. We evaluate the generality of the framework across multiple transformer backbones, including Swin UNETR, UNETR, nnFormer, and MedFormer, using a prostate IMRT dataset. FluenceFormer with Swin UNETR achieves the strongest performance among the evaluated models and improves over existing benchmark CNN and single-stage methods, reducing Energy Error to $\mathbf{4.5\%}$ and yielding statistically significant gains in structural fidelity ($p < 0.05$).

</details>


### [97] [EmoCtrl: Controllable Emotional Image Content Generation](https://arxiv.org/abs/2512.22437)
*Jingyuan Yang,Weibin Luo,Hui Huang*

Main category: cs.CV

TL;DR: 提出EmoCtrl模型，实现可控情感图像内容生成，在保持内容描述一致性的同时表达目标情感，解决现有模型在内容与情感控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像模型能保证内容一致性但缺乏情感感知，而情感驱动模型则以内容扭曲为代价生成情感化结果，需要同时兼顾内容忠实度与情感表达。

Method: 构建包含内容、情感和情感提示的数据集；设计EmoCtrl模型，包含文本和视觉情感增强模块，通过描述性语义和感知线索丰富情感表达；学习具有互补效应的情感标记。

Result: 定量和定性实验表明EmoCtrl在内容忠实度和情感表达控制上优于现有方法；用户研究证实其与人类偏好高度一致；情感标记在创意应用中表现出良好的泛化能力。

Conclusion: EmoCtrl通过情感增强模块和情感标记学习，实现了内容与情感的有效协同控制，为可控情感图像生成提供了稳健且适应性强的方法。

Abstract: An image conveys meaning through both its visual content and emotional tone, jointly shaping human perception. We introduce Controllable Emotional Image Content Generation (C-EICG), which aims to generate images that remain faithful to a given content description while expressing a target emotion. Existing text-to-image models ensure content consistency but lack emotional awareness, whereas emotion-driven models generate affective results at the cost of content distortion. To address this gap, we propose EmoCtrl, supported by a dataset annotated with content, emotion, and affective prompts, bridging abstract emotions to visual cues. EmoCtrl incorporates textual and visual emotion enhancement modules that enrich affective expression via descriptive semantics and perceptual cues. The learned emotion tokens exhibit complementary effects, as demonstrated through ablations and visualizations. Quantatitive and qualatitive experiments demonstrate that EmoCtrl achieves faithful content and expressive emotion control, outperforming existing methods across multiple aspects. User studies confirm EmoCtrl's strong alignment with human preference. Moreover, EmoCtrl generalizes well to creative applications, further demonstrating the robustness and adaptability of the learned emotion tokens.

</details>


### [98] [SuperiorGAT: Graph Attention Networks for Sparse LiDAR Point Cloud Reconstruction in Autonomous Systems](https://arxiv.org/abs/2512.22439)
*Khalfalla Awedat,Mohamed Abidalrekab,Gurcan Comert,Mustafa Ayad*

Main category: cs.CV

TL;DR: 提出SuperiorGAT框架，通过图注意力机制重建稀疏LiDAR点云中缺失的高程信息，无需增加网络深度即可提升分辨率。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中LiDAR感知受限于固定垂直波束分辨率，且环境遮挡导致的波束丢失进一步降低点云质量，需要高效重建方法。

Method: 将LiDAR扫描建模为波束感知图，结合门控残差融合与前馈细化机制，通过结构化波束丢失模拟（每四条垂直扫描波束移除一条）进行训练与评估。

Result: 在KITTI多场景测试中，SuperiorGAT相比PointNet模型和深层GAT基线，重建误差更低、几何一致性更好，X-Z投影显示其能保持结构完整性且垂直失真最小。

Conclusion: 架构优化可在不增加传感器硬件的前提下，以计算高效的方式提升LiDAR分辨率，为自动驾驶感知提供可行解决方案。

Abstract: LiDAR-based perception in autonomous systems is constrained by fixed vertical beam resolution and further compromised by beam dropout resulting from environmental occlusions. This paper introduces SuperiorGAT, a graph attention-based framework designed to reconstruct missing elevation information in sparse LiDAR point clouds. By modeling LiDAR scans as beam-aware graphs and incorporating gated residual fusion with feed-forward refinement, SuperiorGAT enables accurate reconstruction without increasing network depth. To evaluate performance, structured beam dropout is simulated by removing every fourth vertical scanning beam. Extensive experiments across diverse KITTI environments, including Person, Road, Campus, and City sequences, demonstrate that SuperiorGAT consistently achieves lower reconstruction error and improved geometric consistency compared to PointNet-based models and deeper GAT baselines. Qualitative X-Z projections further confirm the model's ability to preserve structural integrity with minimal vertical distortion. These results suggest that architectural refinement offers a computationally efficient method for improving LiDAR resolution without requiring additional sensor hardware.

</details>


### [99] [LECalib: Line-Based Event Camera Calibration](https://arxiv.org/abs/2512.22441)
*Zibin Liu,Banglei Guana,Yang Shanga,Zhenbao Yu,Yifei Bian,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种基于线特征的事件相机标定框架，利用人造环境中常见物体的几何线条进行标定，无需专用标定板，适用于单目和立体事件相机。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机标定方法通常依赖闪烁图案或重建强度图像，过程耗时且需要人工放置标定物，难以适应快速变化的场景需求。

Method: 直接从事件流中检测线条，利用事件-线标定模型生成相机参数初始估计，适用于平面和非平面线条，最后通过非线性优化细化参数。

Result: 仿真和真实世界实验验证了方法的可行性和准确性，在单目和立体事件相机上均取得良好效果。

Conclusion: 该方法通过利用环境中的自然线条实现了快速、无需专用标定板的事件相机标定，为动态场景应用提供了实用解决方案。

Abstract: Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.

</details>


### [100] [Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework](https://arxiv.org/abs/2512.22447)
*Zhicheng Zhao,Yuancheng Xu,Andong Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出QDFNet网络，通过动态质量评估和正交约束融合机制，解决光学与SAR图像融合目标检测中模态缺失或退化的问题，提升检测鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 光学与SAR图像融合可互补信息实现全天候监测，但因成像机制差异、时序异步和配准困难，常出现模态缺失或退化，现有方法对随机缺失模态鲁棒性不足且缺乏稳定性能提升机制。

Method: 1. 设计动态模态质量评估模块（DMQA），利用可学习参考令牌迭代评估特征可靠性；2. 提出正交约束归一化融合模块（OCNF），通过正交约束保持模态独立性并基于可靠性动态调整融合权重。

Result: 在SpaceNet6-OTD和OGSOD-2.0数据集上的实验表明，QDFNet在模态部分损坏或缺失场景下优于现有方法。

Conclusion: QDFNet通过质量感知的动态融合机制，显著提升了光学-SAR融合目标检测在模态缺失或退化情况下的鲁棒性和性能。

Abstract: Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.

</details>


### [101] [SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues](https://arxiv.org/abs/2512.22449)
*Md Abu Obaida Zishan,Annajiat Alim Rasel*

Main category: cs.CV

TL;DR: SonoVision是一款帮助视障人士通过声音提示定位日常物品的智能手机应用，使用左右耳不同的正弦声音指示物体方位，支持完全离线工作。


<details>
  <summary>Details</summary>
Motivation: 视障人士难以独立定位物体，这限制了他们的自主性并可能带来危险。研究旨在通过技术手段减少他们对周围人的依赖，提升独立性。

Method: 采用Flutter开发平台构建应用，后端使用Efficientdet-D2模型进行物体检测，通过耳机向左右耳分别播放正弦声音提示物体方位（前方物体则双耳同时提示）。

Result: 开发出完全离线运行的应用程序，可通过声音提示帮助视障人士定位物体，代码已在GitHub开源。

Conclusion: 该应用能以安全、用户友好的方式显著协助视障人士，通过声音提示增强其独立定位物体的能力。

Abstract: Locating objects for the visually impaired is a significant challenge and is something no one can get used to over time. However, this hinders their independence and could push them towards risky and dangerous scenarios. Hence, in the spirit of making the visually challenged more self-sufficient, we present SonoVision, a smart-phone application that helps them find everyday objects using sound cues through earphones/headphones. This simply means, if an object is on the right or left side of a user, the app makes a sinusoidal sound in a user's respective ear through ear/headphones. However, to indicate objects located directly in front, both the left and right earphones are rung simultaneously. These sound cues could easily help a visually impaired individual locate objects with the help of their smartphones and reduce the reliance on people in their surroundings, consequently making them more independent. This application is made with the flutter development platform and uses the Efficientdet-D2 model for object detection in the backend. We believe the app will significantly assist the visually impaired in a safe and user-friendly manner with its capacity to work completely offline. Our application can be accessed here https://github.com/MohammedZ666/SonoVision.git.

</details>


### [102] [SAM 3D for 3D Object Reconstruction from Remote Sensing Images](https://arxiv.org/abs/2512.22452)
*Junsheng Yao,Lichao Mou,Qingyu Li*

Main category: cs.CV

TL;DR: 本文首次系统评估了通用图像到3D基础模型SAM 3D在单目遥感建筑重建中的表现，相比专用方法TRELLIS，SAM 3D能生成更连贯的屋顶几何和更清晰的边界，并通过分段-重建-组合流程展示了城市场景建模潜力。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D建筑重建方法通常需要特定任务架构和密集监督，限制了可扩展性。本研究旨在探索通用基础模型在遥感图像3D重建中的应用潜力，为城市建模提供更高效的解决方案。

Method: 在NYC Urban Dataset上对比SAM 3D与TRELLIS模型，使用FID和CLIP-based MMD作为评估指标，并设计了分段-重建-组合流程将SAM 3D扩展到城市场景重建。

Result: 实验表明SAM 3D在屋顶几何连贯性和边界清晰度上优于TRELLIS，同时验证了通过分段-重建-组合流程实现城市级场景重建的可行性。

Conclusion: 基础模型在单目遥感3D重建中具有显著潜力，未来研究应关注场景级结构先验的集成，为城市3D重建的实际部署提供指导方向。

Abstract: Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.

</details>


### [103] [Comparing Object Detection Models for Electrical Substation Component Mapping](https://arxiv.org/abs/2512.22454)
*Haley Mody,Namish Bansal,Dennies Kiprono Bor,Edward J. Oughton*

Main category: cs.CV

TL;DR: 该研究比较了三种计算机视觉模型（YOLOv8、YOLOv11、RF-DETR）在变电站组件自动检测中的性能，旨在实现高效、大规模的变电站关键资产映射，以提升电网基础设施的脆弱性评估能力。


<details>
  <summary>Details</summary>
Motivation: 变电站是电网的关键组成部分，其资产（如变压器）易受飓风、洪水、地震等灾害影响，故障可能导致严重的经济和公共安全后果。传统人工测绘方式耗时费力，因此需要开发基于计算机视觉的自动化解决方案，以提高变电站组件识别的效率和规模。

Method: 研究使用手动标注的美国变电站图像数据集，训练并比较了YOLOv8、YOLOv11和RF-DETR三种目标检测模型，评估指标包括检测准确率、精确度和效率。

Result: 研究展示了各模型在变电站组件检测中的关键优势和局限性，并确定了最适合大规模可靠映射的模型。同时，成功利用这些模型对美国境内的变电站组件进行了有效映射，验证了机器学习在变电站测绘中的应用潜力。

Conclusion: 计算机视觉模型能够实现变电站组件的自动化、高效映射，为电网关键基础设施的脆弱性量化提供了可行工具，其中特定模型在准确性与效率平衡中表现更优，支持大规模部署应用。

Abstract: Electrical substations are a significant component of an electrical grid. Indeed, the assets at these substations (e.g., transformers) are prone to disruption from many hazards, including hurricanes, flooding, earthquakes, and geomagnetically induced currents (GICs). As electrical grids are considered critical national infrastructure, any failure can have significant economic and public safety implications. To help prevent and mitigate these failures, it is thus essential that we identify key substation components to quantify vulnerability. Unfortunately, traditional manual mapping of substation infrastructure is time-consuming and labor-intensive. Therefore, an autonomous solution utilizing computer vision models is preferable, as it allows for greater convenience and efficiency. In this research paper, we train and compare the outputs of 3 models (YOLOv8, YOLOv11, RF-DETR) on a manually labeled dataset of US substation images. Each model is evaluated for detection accuracy, precision, and efficiency. We present the key strengths and limitations of each model, identifying which provides reliable and large-scale substation component mapping. Additionally, we utilize these models to effectively map the various substation components in the United States, showcasing a use case for machine learning in substation mapping.

</details>


### [104] [Event-based high temporal resolution measurement of shock wave motion field](https://arxiv.org/abs/2512.22474)
*Taihang Lei,Banglei Guan,Minzu Liang,Pengju Sun,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种利用多事件相机测量冲击波运动参数的新框架，通过极坐标编码、自适应ROI提取和迭代斜率分析实现高时空分辨率测量，实验误差最小0.06%，最大5.20%。


<details>
  <summary>Details</summary>
Motivation: 冲击波快速不均匀传播和不稳定测试条件对高时空分辨率测量构成挑战，而传统方法难以满足需求，需要利用事件相机的高速高动态范围特性进行精确测量。

Method: 建立极坐标系编码事件数据，通过事件偏移计算自适应提取ROI；利用迭代斜率分析提取冲击波前事件；基于事件光学成像模型推导几何模型和3D重建模型，实现多角度测量、运动场重建和爆炸当量反演。

Result: 与压力传感器和经验公式对比，速度测量最大误差5.20%，最小误差0.06%，实现了高时空分辨率的冲击波运动场高精度测量。

Conclusion: 该方法利用事件相机成功实现了冲击波运动参数的高精度测量，在时空分辨率方面取得显著进展，适用于爆炸测试和损伤评估等领域。

Abstract: Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.

</details>


### [105] [Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection](https://arxiv.org/abs/2512.22483)
*Zihan Liu,Xiangning Ren,Dezhang Kong,Yipeng Zhang,Meng Han*

Main category: cs.CV

TL;DR: 提出一种用于红外小目标检测的半监督范式，通过分层MoE适配器和两阶段知识蒸馏/迁移，在少量标注下实现高性能检测。


<details>
  <summary>Details</summary>
Motivation: 红外小目标标注成本高昂，现有方法（如SAM）存在域差距、无法编码物理先验和架构复杂等问题，亟需高效半监督解决方案。

Method: 1. 设计分层MoE适配器（含四个白盒神经算子）；2. 两阶段范式：先验引导知识蒸馏（用10%标注数据将SAM蒸馏为专家教师Scalpel-SAM），再部署导向知识迁移（用伪标签训练轻量下游模型）。

Result: 实验表明，在最小标注量下，下游模型性能达到甚至超越全监督基准，首次系统利用SAM教师模型解决IR-SOT数据稀缺问题。

Conclusion: 该半监督范式有效缓解红外小目标检测的标注瓶颈，为跨域知识迁移和轻量化部署提供了可行方案。

Abstract: Infrared small object detection urgently requires semi-supervised paradigms due to the high cost of annotation. However, existing methods like SAM face significant challenges of domain gaps, inability of encoding physical priors, and inherent architectural complexity. To address this, we designed a Hierarchical MoE Adapter consisting of four white-box neural operators. Building upon this core component, we propose a two-stage paradigm for knowledge distillation and transfer: (1) Prior-Guided Knowledge Distillation, where we use our MoE adapter and 10% of available fully supervised data to distill SAM into an expert teacher (Scalpel-SAM); and (2) Deployment-Oriented Knowledge Transfer, where we use Scalpel-SAM to generate pseudo labels for training lightweight and efficient downstream models. Experiments demonstrate that with minimal annotations, our paradigm enables downstream models to achieve performance comparable to, or even surpassing, their fully supervised counterparts. To our knowledge, this is the first semi-supervised paradigm that systematically addresses the data scarcity issue in IR-SOT using SAM as the teacher model.

</details>


### [106] [Tracking by Predicting 3-D Gaussians Over Time](https://arxiv.org/abs/2512.22489)
*Tanish Baranwal,Himanshu Gaurav Singh,Jathushan Rajasegaran,Jitendra Malik*

Main category: cs.CV

TL;DR: 提出Video-GMAE，一种自监督视频表示学习方法，将视频编码为随时间移动的高斯分布集合，通过重建掩码视频帧学习跟踪能力，在多个数据集上超越现有自监督方法。


<details>
  <summary>Details</summary>
Motivation: 视频本质上是动态3D场景的2D投影，现有自监督方法未充分利用这种几何一致性先验，需要一种能自然建模视频时空结构并诱导跟踪能力的方法。

Method: 使用高斯分布集合表示视频帧序列，通过掩码自编码器框架随机掩码部分视频帧，训练网络重建被掩码区域，使高斯分布随时间移动以建模动态场景。

Result: 模型在无监督情况下自然涌现出跟踪能力，零样本跟踪性能达到SOTA水平；微调后在Kinetics数据集上提升34.6%，在Kubric数据集上提升13.1%，超越现有自监督视频方法。

Conclusion: 将视频表示为移动高斯分布集合是一种有效的自监督表示学习框架，能自然建模视频的3D几何先验并学习跟踪表示，为视频理解任务提供了强大的基础模型。

Abstract: We propose Video Gaussian Masked Autoencoders (Video-GMAE), a self-supervised approach for representation learning that encodes a sequence of images into a set of Gaussian splats moving over time. Representing a video as a set of Gaussians enforces a reasonable inductive bias: that 2-D videos are often consistent projections of a dynamic 3-D scene. We find that tracking emerges when pretraining a network with this architecture. Mapping the trajectory of the learnt Gaussians onto the image plane gives zero-shot tracking performance comparable to state-of-the-art. With small-scale finetuning, our models achieve 34.6% improvement on Kinetics, and 13.1% on Kubric datasets, surpassing existing self-supervised video approaches. The project page and code are publicly available at https://videogmae.org/ and https://github.com/tekotan/video-gmae.

</details>


### [107] [SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration](https://arxiv.org/abs/2512.22503)
*Xin Chen,Kang Luo,Yangyi Xiao,Hesheng Wang*

Main category: cs.CV

TL;DR: 本文提出SCAFusion，一种专为月球机器人任务设计的3D目标检测模型，通过多模态特征融合提升对小而不规则物体的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有面向地面自动驾驶的多模态3D感知方法在月球等非结构化环境中表现不佳，主要由于特征对齐差、多模态协同有限以及小物体检测能力弱。

Method: 基于BEVFusion框架，引入认知适配器进行相机骨干网络调优、对比对齐模块增强相机与激光雷达特征一致性、相机辅助训练分支强化视觉表征，以及专门设计的切片感知坐标注意力机制。

Result: 在nuScenes验证集上达到69.7% mAP和72.1% NDS，比基线分别提升5.0%和2.7%；在Isaac Sim模拟月球环境中达到90.93% mAP，提升11.5%，对小尺寸陨石类障碍物检测有显著改进。

Conclusion: SCAFusion以可忽略的参数和计算量增加，有效提升了月球环境下小而不规则物体的检测性能，为自主导航提供了可靠解决方案。

Abstract: Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.

</details>


### [108] [CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation](https://arxiv.org/abs/2512.22536)
*Qinglin Zeng,Kaitong Cai,Ruiqi Chen,Qinhan Lv,Keze Wang*

Main category: cs.CV

TL;DR: CoAgent是一个用于生成连贯长视频的协作闭环框架，通过计划-合成-验证流程解决现有模型中的身份漂移、场景不一致和时间结构不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频模型通常独立处理每个镜头，导致身份漂移、场景不一致和时间结构不稳定，难以维持叙事连贯性和视觉一致性。

Method: 提出CoAgent框架，包含故事板规划器分解输入为结构化镜头计划，全局上下文管理器维护实体级记忆，合成模块在视觉一致性控制器指导下生成镜头，验证代理通过视觉语言推理评估结果并触发选择性重新生成，最后通过节奏感知编辑器优化时间节奏和过渡。

Result: 大量实验表明，CoAgent在长视频生成中显著提高了连贯性、视觉一致性和叙事质量。

Conclusion: CoAgent通过协作闭环框架有效解决了开放域视频生成中的叙事连贯性和视觉一致性挑战，为长视频生成提供了更可靠的解决方案。

Abstract: Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.

</details>


### [109] [DreamOmni3: Scribble-based Editing and Generation](https://arxiv.org/abs/2512.22525)
*Bin Xia,Bohao Peng,Jiyang Liu,Sitong Wu,Jingyao Li,Junjia Huang,Xu Zhao,Yitong Wang,Ruihang Chu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: 提出DreamOmni3框架，通过结合涂鸦、文本和图像输入实现更灵活的视觉编辑与生成，解决了传统文本提示在定位和细节表达上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有统一生成与编辑模型主要依赖文本提示，但语言难以精确描述用户期望的编辑位置和细粒度视觉细节，限制了创作的灵活性。

Method: 1) 定义涂鸦编辑（4个子任务）和涂鸦生成（3个子任务）两类任务；2) 设计数据合成流程，基于DreamOmni2数据集提取可编辑区域并叠加手绘图形或裁剪图像；3) 提出联合输入方案，将原始图像和涂鸦图像同时输入模型，使用颜色区分区域并共享索引与位置编码。

Result: 实验表明DreamOmni3在涂鸦编辑与生成任务上表现优异，模型和代码将开源。

Conclusion: 通过涂鸦交互增强多模态指令的视觉编辑与生成能力，所提框架和数据基准推动了该方向的研究进展。

Abstract: Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.

</details>


### [110] [Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains](https://arxiv.org/abs/2512.22545)
*Jesen Zhang,Ningyuan Liu,Kaitong Cai,Sidi Liu,Jing Yang,Ziliang Chen,Xiaofei Sun,Keze Wang*

Main category: cs.CV

TL;DR: 提出SR-MCR框架，通过自参考信号对齐多模态大语言模型的推理过程，提升推理可靠性和视觉基础，在多个视觉基准上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型常产生流畅但不可靠的推理，存在步骤间连贯性弱、视觉基础不足的问题，主要因为现有对齐方法仅监督最终答案而忽略中间推理过程的可靠性。

Method: 提出轻量级无标签框架SR-MCR，利用模型输出衍生的五个自参考信号（语义对齐、词汇保真度、非冗余性、视觉基础、步骤一致性）构建归一化可靠性加权奖励，采用无批评者的GRPO目标并加入置信感知冷却机制稳定训练。

Result: 基于Qwen2.5-VL构建的SR-MCR-7B在多个视觉基准上实现最佳性能，平均准确率达81.4%，同时提升答案准确性和推理连贯性；消融研究证实各奖励项和冷却模块的独立贡献。

Conclusion: SR-MCR框架通过过程级自参考对齐有效提升多模态推理的可靠性和可解释性，为模型对齐提供了轻量高效的解决方案。

Abstract: Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.

</details>


### [111] [ReFRM3D: A Radiomics-enhanced Fused Residual Multiparametric 3D Network with Multi-Scale Feature Fusion for Glioma Characterization](https://arxiv.org/abs/2512.22570)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Arefin Ittesafun Abian,Yan Zhang,Mirjam Jonkman,Sami Azam*

Main category: cs.CV

TL;DR: 提出ReFRM3D网络和基于多特征肿瘤标记物的分类器，显著提升胶质瘤分割与分类性能


<details>
  <summary>Details</summary>
Motivation: 胶质瘤诊断存在影像数据变异大、计算资源优化不足、分割分类效率低等问题，需改进现有方法

Method: 基于3D U-Net架构，结合多尺度特征融合、混合上采样和扩展残差跳跃机制；利用分割区域提取影像组学特征进行分类

Result: 在BraTS2019-2021数据集上取得优异分割效果，如BraTS2019的WT/ET/TC Dice系数分别达94.04%、92.68%、93.64%

Conclusion: 所提方法能有效解决胶质瘤影像分析中的关键挑战，为精准诊断提供可靠技术支撑

Abstract: Gliomas are among the most aggressive cancers, characterized by high mortality rates and complex diagnostic processes. Existing studies on glioma diagnosis and classification often describe issues such as high variability in imaging data, inadequate optimization of computational resources, and inefficient segmentation and classification of gliomas. To address these challenges, we propose novel techniques utilizing multi-parametric MRI data to enhance tumor segmentation and classification efficiency. Our work introduces the first-ever radiomics-enhanced fused residual multiparametric 3D network (ReFRM3D) for brain tumor characterization, which is based on a 3D U-Net architecture and features multi-scale feature fusion, hybrid upsampling, and an extended residual skip mechanism. Additionally, we propose a multi-feature tumor marker-based classifier that leverages radiomic features extracted from the segmented regions. Experimental results demonstrate significant improvements in segmentation performance across the BraTS2019, BraTS2020, and BraTS2021 datasets, achieving high Dice Similarity Coefficients (DSC) of 94.04%, 92.68%, and 93.64% for whole tumor (WT), enhancing tumor (ET), and tumor core (TC) respectively in BraTS2019; 94.09%, 92.91%, and 93.84% in BraTS2020; and 93.70%, 90.36%, and 92.13% in BraTS2021.

</details>


### [112] [Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos](https://arxiv.org/abs/2512.22657)
*Shravan Saranyan,Pramit Saha*

Main category: cs.CV

TL;DR: 本研究评估了多种深度学习架构（包括3D Inception、双流和CNN-RNN模型）在超声心动图视频中自动估计左心室射血分数（LVEF）的性能，发现改进的3D Inception架构在EchoNet-Dynamic数据集上取得了最佳结果（RMSE为6.79%）。


<details>
  <summary>Details</summary>
Motivation: 超声心动图是评估LVEF的常用无创方法，但手动评估耗时且存在观察者间差异。深度学习为自动化、高精度的LVEF估计提供了有前景的替代方案。

Method: 研究比较了多种深度学习架构（3D Inception、双流模型、CNN-RNN）在EchoNet-Dynamic数据集（10,030个超声心动图视频）上的表现，系统评估了结构修改和融合策略，并分析了超参数（如卷积核大小和归一化策略）对性能的影响。

Result: 改进的3D Inception架构表现最佳（RMSE为6.79%）。研究发现模型普遍存在过拟合倾向，较小且简单的模型泛化能力更好；模型性能对超参数选择高度敏感。

Conclusion: 深度学习可有效实现超声心动图视频的LVEF自动估计，其中3D Inception架构最具潜力。研究揭示的结构设计和训练策略见解，可推广至其他医学及非医学视频分析任务。

Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and plays a central role in the diagnosis and management of cardiovascular disease. Echocardiography, as a readily accessible and non-invasive imaging modality, is widely used in clinical practice to estimate LVEF. However, manual assessment of cardiac function from echocardiograms is time-consuming and subject to considerable inter-observer variability. Deep learning approaches offer a promising alternative, with the potential to achieve performance comparable to that of experienced human experts. In this study, we investigate the effectiveness of several deep learning architectures for LVEF estimation from echocardiography videos, including 3D Inception, two-stream, and CNN-RNN models. We systematically evaluate architectural modifications and fusion strategies to identify configurations that maximize prediction accuracy. Models were trained and evaluated on the EchoNet-Dynamic dataset, comprising 10,030 echocardiogram videos. Our results demonstrate that modified 3D Inception architectures achieve the best overall performance, with a root mean squared error (RMSE) of 6.79%. Across architectures, we observe a tendency toward overfitting, with smaller and simpler models generally exhibiting improved generalization. Model performance was also found to be highly sensitive to hyperparameter choices, particularly convolutional kernel sizes and normalization strategies. While this study focuses on echocardiography-based LVEF estimation, the insights gained regarding architectural design and training strategies may be applicable to a broader range of medical and non-medical video analysis tasks.

</details>


### [113] [Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains](https://arxiv.org/abs/2512.22664)
*Qiankun Li,Feng He,Huabao Chen,Xin Ning,Kun Wang,Zengfu Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的集群注意力适配器（CLAdapter），用于将大规模预训练模型的知识有效迁移到数据有限的下游科学领域任务中。


<details>
  <summary>Details</summary>
Motivation: 尽管大规模数据集（如LAION、ImageNet）上的预训练模型已获得丰富知识，但许多数据有限的专业科学领域下游任务仍面临挑战，需要有效的适应方法。

Method: CLAdapter通过引入注意力机制和聚类中心，利用分布相关性和变换矩阵个性化增强转换特征，支持CNN和Transformer架构在2D和3D场景中的无缝集成。

Result: 在10个跨领域数据集（包括通用、多媒体、生物、医疗、工业、农业、环境、地理、材料科学、OOD和3D分析）上实现了最先进的性能。

Conclusion: CLAdapter能有效释放基础视觉模型的潜力，通过自适应迁移在不同数据有限的科学领域中实现卓越表现。

Abstract: In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.

</details>


### [114] [KV-Tracker: Real-Time Pose Tracking with Transformers](https://arxiv.org/abs/2512.22581)
*Marwan Taher,Ignacio Alzugaray,Kirill Mazur,Xin Kong,Andrew J. Davison*

Main category: cs.CV

TL;DR: 提出KV-Tracker方法，通过缓存全局自注意力块的键值对作为场景表示，实现实时6自由度姿态跟踪和在线重建，速度提升15倍且避免漂移问题。


<details>
  <summary>Details</summary>
Motivation: 多视图3D几何网络虽能提供强大先验，但计算速度过慢无法满足实时应用需求，需要一种能在保持精度的同时大幅加速在线跟踪与重建的方法。

Method: 1. 快速选择并管理关键帧图像集；2. 使用π³模型进行双向注意力场景映射；3. 缓存全局自注意力块的键值对作为唯一场景表示；4. 模型无关的缓存策略，无需重新训练即可应用于现有多视图网络。

Result: 在TUM RGB-D、7-Scenes、Arctic和OnePose数据集上表现优异，最高达到约27 FPS的帧率，推理速度提升高达15倍，且未出现跟踪漂移或灾难性遗忘问题。

Conclusion: KV-Tracker通过创新的键值对缓存机制，成功将离线多视图网络适配为实时系统，在保持精度的同时显著提升速度，为单目RGB视频的实时姿态跟踪与在线重建提供了有效解决方案。

Abstract: Multi-view 3D geometry networks offer a powerful prior but are prohibitively slow for real-time applications. We propose a novel way to adapt them for online use, enabling real-time 6-DoF pose tracking and online reconstruction of objects and scenes from monocular RGB videos. Our method rapidly selects and manages a set of images as keyframes to map a scene or object via $π^3$ with full bidirectional attention. We then cache the global self-attention block's key-value (KV) pairs and use them as the sole scene representation for online tracking. This allows for up to $15\times$ speedup during inference without the fear of drift or catastrophic forgetting. Our caching strategy is model-agnostic and can be applied to other off-the-shelf multi-view networks without retraining. We demonstrate KV-Tracker on both scene-level tracking and the more challenging task of on-the-fly object tracking and reconstruction without depth measurements or object priors. Experiments on the TUM RGB-D, 7-Scenes, Arctic and OnePose datasets show the strong performance of our system while maintaining high frame-rates up to ${\sim}27$ FPS.

</details>


### [115] [PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment](https://arxiv.org/abs/2512.22602)
*Bin Wang,Yang Xu,Huan Zhao,Hao Zhang,Zixing Zhang*

Main category: cs.CV

TL;DR: 提出PTalker框架，通过风格解耦和三重对齐机制，生成保留个性化说话风格且唇部同步更准确的3D说话头部动画。


<details>
  <summary>Details</summary>
Motivation: 现有语音驱动3D说话人生成方法过度关注唇部同步精度，却忽略了个体说话风格的细微差异，导致个性化不足和真实感受限。

Method: 1. 设计解耦约束将音频和面部动作编码至独立的风格与内容空间；2. 采用三重模态对齐机制：空间对齐（图注意力网络）、时间对齐（交叉注意力）、特征对齐（对比损失与KL散度约束）。

Result: 在公开数据集上的实验表明，PTalker能生成更真实、保留说话风格且唇部同步更准确的3D说话头部，性能优于现有先进方法。

Conclusion: PTalker通过解耦风格与内容、强化多模态对齐，实现了兼顾个性化风格与高精度唇部同步的3D说话人生成，提升了真实感与表现力。

Abstract: Speech-driven 3D talking head generation aims to produce lifelike facial animations precisely synchronized with speech. While considerable progress has been made in achieving high lip-synchronization accuracy, existing methods largely overlook the intricate nuances of individual speaking styles, which limits personalization and realism. In this work, we present a novel framework for personalized 3D talking head animation, namely "PTalker". This framework preserves speaking style through style disentanglement from audio and facial motion sequences and enhances lip-synchronization accuracy through a three-level alignment mechanism between audio and mesh modalities. Specifically, to effectively disentangle style and content, we design disentanglement constraints that encode driven audio and motion sequences into distinct style and content spaces to enhance speaking style representation. To improve lip-synchronization accuracy, we adopt a modality alignment mechanism incorporating three aspects: spatial alignment using Graph Attention Networks to capture vertex connectivity in the 3D mesh structure, temporal alignment using cross-attention to capture and synchronize temporal dependencies, and feature alignment by top-k bidirectional contrastive losses and KL divergence constraints to ensure consistency between speech and mesh modalities. Extensive qualitative and quantitative experiments on public datasets demonstrate that PTalker effectively generates realistic, stylized 3D talking heads that accurately match identity-specific speaking styles, outperforming state-of-the-art methods. The source code and supplementary videos are available at: PTalker.

</details>


### [116] [Next Best View Selections for Semantic and Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2512.22771)
*Yiqian Li,Wen Jiang,Kostas Daniilidis*

Main category: cs.CV

TL;DR: 本文提出了一种基于Fisher信息的主动学习算法，用于在多相机设置中选择信息量最大的视角帧，以联合优化语义理解和动态场景建模，提升渲染质量和语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在语义理解和动态场景建模中存在数据冗余问题，而随机或启发式视角选择策略缺乏理论依据，无法高效筛选对模型训练最有价值的视角帧。

Method: 将视角选择问题建模为主动学习任务，利用Fisher信息量化候选视角对语义高斯参数和变形网络的信息增益，从而联合优化语义推理与动态场景建模。

Result: 在大规模静态图像和动态视频数据集上的实验表明，该方法在渲染质量和语义分割性能上均优于随机选择和基于不确定性的启发式基线方法。

Conclusion: 所提出的基于Fisher信息的主动学习算法为视角选择提供了理论依据，能有效提升语义与动态场景联合建模的效率与性能。

Abstract: Understanding semantics and dynamics has been crucial for embodied agents in various tasks. Both tasks have much more data redundancy than the static scene understanding task. We formulate the view selection problem as an active learning problem, where the goal is to prioritize frames that provide the greatest information gain for model training. To this end, we propose an active learning algorithm with Fisher Information that quantifies the informativeness of candidate views with respect to both semantic Gaussian parameters and deformation networks. This formulation allows our method to jointly handle semantic reasoning and dynamic scene modeling, providing a principled alternative to heuristic or random strategies. We evaluate our method on large-scale static images and dynamic video datasets by selecting informative frames from multi-camera setups. Experimental results demonstrate that our approach consistently improves rendering quality and semantic segmentation performance, outperforming baseline methods based on random selection and uncertainty-based heuristics.

</details>


### [117] [Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer](https://arxiv.org/abs/2512.22612)
*Dafeng Zhang,Yongqi Song,Shizhuo Liu*

Main category: cs.CV

TL;DR: 提出基于稀疏差分Transformer的Top-K Jaccard相似度计算方法，通过提升邻居节点纯度增强人脸嵌入关系度量，在多个数据集上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用Jaccard相似度替代余弦距离时引入过多无关节点，导致相似度区分度有限且影响聚类性能；同时传统Transformer在预测节点关系时过度关注无关特征关系而引入噪声。

Method: 1. 提出预测驱动的Top-K Jaccard相似度系数以提升邻居节点纯度；2. 设计基于Transformer的预测模型优化Top-K邻居数量选择；3. 提出稀疏差分Transformer替代标准Transformer，消除噪声并增强抗噪能力。

Result: 在MS-Celeb-1M等多个数据集上的实验表明，该方法优于现有方法，达到最先进的聚类性能。

Conclusion: 所提出的SDT增强型Top-K Jaccard相似度计算方法能有效提升人脸聚类中关系度量的可靠性，为噪声环境下的聚类任务提供更鲁棒的解决方案。

Abstract: The method used to measure relationships between face embeddings plays a crucial role in determining the performance of face clustering. Existing methods employ the Jaccard similarity coefficient instead of the cosine distance to enhance the measurement accuracy. However, these methods introduce too many irrelevant nodes, producing Jaccard coefficients with limited discriminative power and adversely affecting clustering performance. To address this issue, we propose a prediction-driven Top-K Jaccard similarity coefficient that enhances the purity of neighboring nodes, thereby improving the reliability of similarity measurements. Nevertheless, accurately predicting the optimal number of neighbors (Top-K) remains challenging, leading to suboptimal clustering results. To overcome this limitation, we develop a Transformer-based prediction model that examines the relationships between the central node and its neighboring nodes near the Top-K to further enhance the reliability of similarity estimation. However, vanilla Transformer, when applied to predict relationships between nodes, often introduces noise due to their overemphasis on irrelevant feature relationships. To address these challenges, we propose a Sparse Differential Transformer (SDT), instead of the vanilla Transformer, to eliminate noise and enhance the model's anti-noise capabilities. Extensive experiments on multiple datasets, such as MS-Celeb-1M, demonstrate that our approach achieves state-of-the-art (SOTA) performance, outperforming existing methods and providing a more robust solution for face clustering.

</details>


### [118] [EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation](https://arxiv.org/abs/2512.22808)
*Libo Zhang,Zekun Li,Tianyu Li,Zeyu Cao,Rui Xu,Xiaoxiao Long,Wenjia Wang,Jingbo Wang,Yuan Liu,Wenping Wang,Daquan Zhou,Taku Komura,Zhiyang Dou*

Main category: cs.CV

TL;DR: 本文提出了EgoReAct框架，首个能够从第一人称视角视频流实时生成3D对齐的人体反应动作的自回归系统，并构建了空间对齐的数据集HRD来解决现有数据中的空间不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从第一人称视频建模人类反应时面临两大挑战：严格因果生成的要求和精确3D空间对齐的困难。现有数据集（如ViMo）存在显著的空间不一致问题，例如动态动作总是与固定摄像头视频配对。

Method: 1. 构建Human Reaction Dataset (HRD)解决数据稀缺和错位问题；2. 提出EgoReAct框架：使用Vector Quantised-Variational AutoEncoder将反应动作压缩到紧凑的潜在空间，然后训练Generative Pre-trained Transformer从视觉输入生成反应；3. 在生成过程中融入3D动态特征（度量深度和头部动态）以增强空间基础。

Result: 大量实验表明，EgoReAct在真实性、空间一致性和生成效率方面显著优于现有方法，同时在生成过程中保持严格的因果关系。

Conclusion: EgoReAct通过结合空间对齐的数据集和融入3D动态特征的自回归生成框架，成功实现了从第一人称视频实时生成3D对齐的人类反应动作，为解决该领域的核心挑战提供了有效方案。

Abstract: Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.

</details>


### [119] [Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone](https://arxiv.org/abs/2512.22615)
*Jiacheng Ye,Shansan Gong,Jiahui Gao,Junming Fan,Shuang Wu,Wei Bi,Haoli Bai,Lifeng Shang,Lingpeng Kong*

Main category: cs.CV

TL;DR: 提出基于扩散大语言模型的视觉语言模型Dream-VL和视觉语言动作模型Dream-VLA，在视觉规划和机器人控制任务中超越传统自回归模型


<details>
  <summary>Details</summary>
Motivation: 自回归视觉语言模型在复杂视觉规划和动态机器人控制中的序列生成效率有限，需要探索更高效的架构

Method: 基于扩散大语言模型构建视觉语言模型Dream-VL，并通过在开放机器人数据集上持续预训练扩展为Dream-VLA

Result: Dream-VL在多项基准测试中达到先进水平；Dream-VLA在LIBERO（97.2%成功率）、SimplerEnv-Bridge（71.4%）和SimplerEnv-Fractal（60.5%）上超越π₀和GR00T-N1等领先模型

Conclusion: 扩散模型的双向特性使其更适合视觉语言动作任务，能实现动作分块和并行生成，在下游微调中收敛更快，性能优于自回归基线

Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.

</details>


### [120] [OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding](https://arxiv.org/abs/2512.23020)
*Wenyuan Huang,Zhao Wang,Zhou Wei,Ting Huang,Fang Zhao,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 提出OpenGround框架，通过主动认知推理模块实现开放世界3D视觉定位，无需预定义目标类别即可定位任意描述对象


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖预定义的对象查找表，无法处理未定义或未知目标，限制了实际应用场景

Method: 设计主动认知推理模块，通过认知任务链模拟人类感知过程，动态更新对象查找表，扩展视觉语言模型的认知范围

Result: 在Nr3D数据集上表现优异，在ScanRefer数据集上达到SOTA，在新建的OpenTarget数据集上提升17.6%

Conclusion: OpenGround框架突破了预定义类别的限制，实现了开放世界的3D视觉定位，为实际应用提供了更灵活的解决方案

Abstract: 3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at [this https URL](https://why-102.github.io/openground.io/).

</details>


### [121] [SwinTF3D: A Lightweight Multimodal Fusion Approach for Text-Guided 3D Medical Image Segmentation](https://arxiv.org/abs/2512.22878)
*Hasan Faraz Khan,Noor Fatima,Muzammil Behzad*

Main category: cs.CV

TL;DR: 提出SwinTF3D，一种轻量级多模态融合方法，通过统一视觉和语言表示实现文本引导的3D医学图像分割，在保持竞争力的同时显著提升效率。


<details>
  <summary>Details</summary>
Motivation: 现有3D分割框架主要依赖大规模标注数据的视觉学习，缺乏语义理解能力，难以适应新领域和用户自定义的分割目标。

Method: 采用基于Transformer的视觉编码器提取体积特征，通过紧凑的文本编码器和高效融合机制整合语言提示，实现语义线索与空间结构的对齐。

Result: 在BTCV数据集上，SwinTF3D在多个器官上取得具有竞争力的Dice和IoU分数，对未见数据泛化良好，相比传统Transformer分割网络显著提升效率。

Conclusion: SwinTF3D通过融合视觉感知与语言理解，为交互式、文本驱动的3D医学图像分割建立了实用且可解释的范式，为临床影像提供了更自适应和资源高效的解决方案。

Abstract: The recent integration of artificial intelligence into medical imaging has driven remarkable advances in automated organ segmentation. However, most existing 3D segmentation frameworks rely exclusively on visual learning from large annotated datasets restricting their adaptability to new domains and clinical tasks. The lack of semantic understanding in these models makes them ineffective in addressing flexible, user-defined segmentation objectives. To overcome these limitations, we propose SwinTF3D, a lightweight multimodal fusion approach that unifies visual and linguistic representations for text-guided 3D medical image segmentation. The model employs a transformer-based visual encoder to extract volumetric features and integrates them with a compact text encoder via an efficient fusion mechanism. This design allows the system to understand natural-language prompts and correctly align semantic cues with their corresponding spatial structures in medical volumes, while producing accurate, context-aware segmentation results with low computational overhead. Extensive experiments on the BTCV dataset demonstrate that SwinTF3D achieves competitive Dice and IoU scores across multiple organs, despite its compact architecture. The model generalizes well to unseen data and offers significant efficiency gains compared to conventional transformer-based segmentation networks. Bridging visual perception with linguistic understanding, SwinTF3D establishes a practical and interpretable paradigm for interactive, text-driven 3D medical image segmentation, opening perspectives for more adaptive and resource-efficient solutions in clinical imaging.

</details>


### [122] [Rethinking Memory Design in SAM-Based Visual Object Tracking](https://arxiv.org/abs/2512.22624)
*Mohamad Alansari,Muzammal Naseer,Hasan Al Marzouqi,Naoufel Werghi,Sajid Javed*

Main category: cs.CV

TL;DR: 本文对基于SAM的视觉目标跟踪进行了系统的内存机制研究，分析了SAM2跟踪器的内存设计共性，将其迁移到SAM3框架中，并提出了一种统一的混合内存框架，显著提升了跟踪鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有基于SAM的跟踪方法以内存为核心机制，但内存设计缺乏系统性研究，且不清楚这些机制如何迁移到下一代基础模型（如SAM3）。需要理解内存设计原则并提升跟踪在复杂场景下的鲁棒性。

Method: 首先分析代表性SAM2跟踪器的内存机制，发现它们在短期内存帧选择上存在差异但共享对象中心表示；然后将这些机制忠实迁移到SAM3框架并进行大规模评估；最后提出统一的混合内存框架，将内存分解为短期外观内存和长期干扰物解决内存。

Result: 实验表明，提出的混合内存框架在SAM2和SAM3骨干网络上均能一致提升跟踪鲁棒性，特别是在长期遮挡、复杂运动和干扰物密集场景下表现更好。

Conclusion: 内存机制的模块化分解和统一设计能有效提升SAM-based跟踪的性能，该框架为未来内存设计提供了原则性指导，并展示了在更强基础模型上的可迁移性。

Abstract: \noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}

</details>


### [123] [Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion](https://arxiv.org/abs/2512.22626)
*Yuming Gu,Yizhi Wang,Yining Hong,Yipeng Gao,Hao Jiang,Angtian Wang,Bo Liu,Nathaniel S. Dennler,Zhengfei Kuang,Hao Li,Gordon Wetzstein,Chongyang Ma*

Main category: cs.CV

TL;DR: 提出Envision框架，通过目标图像约束的扩散模型进行具身视觉规划，解决现有方法的空间漂移和目标错位问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频扩散模型多为前向预测，缺乏明确的目标建模，导致生成轨迹存在空间漂移和目标不一致的问题，影响具身智能体的操作任务。

Method: 两阶段框架：1) 目标图像模型识别任务相关区域，通过区域感知交叉注意力合成目标图像；2) 环境-目标视频模型基于首尾帧条件视频扩散模型(FL2V)，在初始观察和目标图像间插值生成物理合理的视频轨迹。

Result: 在物体操作和图像编辑基准测试中，Envision在目标对齐、空间一致性和物体保持方面优于基线方法。

Conclusion: Envision生成的视觉规划可直接支持下游机器人规划与控制，为具身智能体提供可靠指导。

Abstract: Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.

</details>


### [124] [An Architecture-Led Hybrid Report on Body Language Detection Project](https://arxiv.org/abs/2512.23028)
*Thomson Tong,Diba Darooneh*

Main category: cs.CV

TL;DR: 本文分析了两种现代视觉语言模型（Qwen2.5-VL-7B-Instruct和Llama-4-Scout-17B-16E-Instruct）的架构，并展示了它们如何应用于视频到人工制品的处理流程，包括人物检测、边界框生成和情感属性标注。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过架构分析，将视觉语言模型的特性映射到实际视频处理系统中，以支持结构化输出生成、验证和标注，同时澄清模型行为与系统约束之间的关系，确保可辩护的声明和稳健的接口设计。

Method: 采用架构导向的分析方法，首先总结共享的多模态基础（视觉标记化、Transformer注意力和指令遵循），然后详细描述每种模型的架构，最后将模型行为与系统约束（如结构化输出验证、帧局部标识符处理）联系起来。

Result: 系统能够从视频中采样帧，使用视觉语言模型检测可见人物并生成带有提示条件属性（如情感）的像素空间边界框，通过预定义模式验证输出结构，并可选择渲染带注释的视频。分析揭示了结构化输出可能在语法正确但语义错误，以及当前提示合约中人物标识符是帧局部等关键约束。

Conclusion: 通过明确模型架构与系统实现之间的映射，本研究为编写可辩护的声明、设计稳健的接口和规划评估提供了重要依据，强调了在视频处理流程中区分结构验证与几何正确性的必要性。

Abstract: This report provides an architecture-led analysis of two modern vision-language models (VLMs), Qwen2.5-VL-7B-Instruct and Llama-4-Scout-17B-16E-Instruct, and explains how their architectural properties map to a practical video-to-artifact pipeline implemented in the BodyLanguageDetection repository [1]. The system samples video frames, prompts a VLM to detect visible people and generate pixel-space bounding boxes with prompt-conditioned attributes (emotion by default), validates output structure using a predefined schema, and optionally renders an annotated video. We first summarize the shared multimodal foundation (visual tokenization, Transformer attention, and instruction following), then describe each architecture at a level sufficient to justify engineering choices without speculative internals. Finally, we connect model behavior to system constraints: structured outputs can be syntactically valid while semantically incorrect, schema validation is structural (not geometric correctness), person identifiers are frame-local in the current prompting contract, and interactive single-frame analysis returns free-form text rather than schema-enforced JSON. These distinctions are critical for writing defensible claims, designing robust interfaces, and planning evaluation.

</details>


### [125] [Visual Autoregressive Modelling for Monocular Depth Estimation](https://arxiv.org/abs/2512.22653)
*Amir El-Ghoussani,André Kaup,Nassir Navab,Gustavo Carneiro,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 提出基于视觉自回归先验的单目深度估计方法，作为扩散模型的替代方案，在室内外数据集上取得优异性能


<details>
  <summary>Details</summary>
Motivation: 探索自回归先验作为几何感知生成模型在深度估计中的潜力，提供比扩散模型更具数据可扩展性和适应性的替代方案

Method: 采用大规模文本到图像自回归模型，引入尺度条件上采样机制和分类器无关引导，通过10个固定自回归阶段进行推理，仅需7.4万合成样本微调

Result: 在受限训练条件下，室内基准测试达到最先进性能；在室外数据集上表现强劲；自回归先验展现出数据可扩展性和对3D视觉任务的适应性优势

Conclusion: 自回归先验是深度估计中几何感知生成模型的重要补充家族，在数据效率和任务适应性方面具有明显优势

Abstract: We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at "https://github.com/AmirMaEl/VAR-Depth".

</details>


### [126] [ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis](https://arxiv.org/abs/2512.23196)
*Maisha Haque,Israt Jahan Ayshi,Sadaf M. Anis,Nahian Tasnim,Mithila Moontaha,Md. Sabbir Ahmed,Muhammad Iqbal Hossain,Mohammad Zavid Parvez,Subrata Chakraborty,Biswajeet Pradhan,Biswajit Banik*

Main category: cs.CV

TL;DR: 提出ForCM方法，结合对象基图像分析和深度学习，使用Sentinel-2影像提升亚马逊雨林森林覆盖制图精度


<details>
  <summary>Details</summary>
Motivation: 传统森林覆盖制图方法精度有限，需要开发更准确的方法以支持全球环境监测和保护

Method: 评估多种深度学习模型（UNet、UNet++、ResUNet、AttentionUNet、ResNet50-Segnet），将其与对象基图像分析技术结合，使用Sentinel-2多光谱影像

Result: ResUNet-OBIA和AttentionUNBIA分别达到94.54%和95.64%的总体精度，优于传统OBIA的92.91%

Conclusion: ForCM方法显著提高森林覆盖制图精度，证明免费工具如QGIS在环境监测中的潜力

Abstract: This research proposes "ForCM", a novel approach to forest cover mapping that combines Object-Based Image Analysis (OBIA) with Deep Learning (DL) using multispectral Sentinel-2 imagery. The study explores several DL models, including UNet, UNet++, ResUNet, AttentionUNet, and ResNet50-Segnet, applied to high-resolution Sentinel-2 Level 2A satellite images of the Amazon Rainforest. The datasets comprise three collections: two sets of three-band imagery and one set of four-band imagery. After evaluation, the most effective DL models are individually integrated with the OBIA technique to enhance mapping accuracy. The originality of this work lies in evaluating different deep learning models combined with OBIA and comparing them with traditional OBIA methods. The results show that the proposed ForCM method improves forest cover mapping, achieving overall accuracies of 94.54 percent with ResUNet-OBIA and 95.64 percent with AttentionUNet-OBIA, compared to 92.91 percent using traditional OBIA. This research also demonstrates the potential of free and user-friendly tools such as QGIS for accurate mapping within their limitations, supporting global environmental monitoring and conservation efforts.

</details>


### [127] [PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion](https://arxiv.org/abs/2512.23130)
*Jian Wang,Sixing Rong,Jiarui Xing,Yuling Xu,Weide Liu*

Main category: cs.CV

TL;DR: 提出PathoSyn框架，通过解耦解剖结构与病理偏差实现高质量MRI图像合成，在感知真实性和解剖保真度上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在像素域或依赖二值掩码操作，常导致特征纠缠、解剖结构损坏或结构不连续，需要更精确的病理图像合成方法支持低数据场景下的诊断算法开发。

Method: 1. 将合成任务分解为确定性解剖重建和随机偏差建模；2. 设计偏差空间扩散模型学习病理残差的条件分布；3. 采用接缝感知融合策略和推理时稳定模块确保空间一致性；4. 构建数学原理驱动的合成流程。

Result: 在肿瘤成像基准测试中，PathoSyn在定量和定性评估上显著优于整体扩散模型和掩码条件基线，能生成高保真患者特异性合成数据并模拟可解释的反事实疾病进展。

Conclusion: 该框架为生成高质量合成医学图像提供了数学原理化的解决方案，支持精准干预规划，并为临床决策支持系统提供了可控的基准测试环境。

Abstract: We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.

</details>


### [128] [MedSAM-based lung masking for multi-label chest X-ray classification](https://arxiv.org/abs/2512.23089)
*Brayden Miao,Zain Rehman,Xin Miao,Siming Liu,Jianjie Wang*

Main category: cs.CV

TL;DR: 提出了一种结合医学图像分割基础模型（MedSAM）的胸片分类流程，通过提取肺部区域作为空间先验来指导多标签异常分类，发现掩蔽效果具有任务和架构依赖性，需根据具体临床目标选择掩蔽策略。


<details>
  <summary>Details</summary>
Motivation: 胸片自动分析面临疾病信号弱、数据集偏差和空间监督有限等挑战，医学图像分割基础模型（MedSAM）提供了引入解剖学先验的机会，可能提高胸片分析的鲁棒性和可解释性。

Method: 使用Airlangga大学医院的公开图像-掩码数据集微调MedSAM作为肺部区域提取模块，然后将其应用于NIH胸片数据集的精选子集，训练和评估用于五种异常（肿块、结节、肺炎、水肿、纤维化）多标签预测的深度卷积神经网络，正常情况通过派生分数评估。

Result: MedSAM能在不同成像条件下产生解剖学上合理的肺部掩码；掩蔽效果具有任务和架构依赖性：ResNet50在原始图像上训练获得最强的整体异常辨别能力，宽松肺部掩蔽产生可比的宏观AUROC但显著改善正常情况筛查，紧密掩蔽持续降低异常级别性能但提高训练效率，宽松掩蔽通过保留肺门和周围上下文部分缓解性能下降。

Conclusion: 肺部掩蔽应被视为可控制的空间先验，根据主干网络和临床目标进行选择，而不是统一应用。

Abstract: Chest X-ray (CXR) imaging is widely used for screening and diagnosing pulmonary abnormalities, yet automated interpretation remains challenging due to weak disease signals, dataset bias, and limited spatial supervision. Foundation models for medical image segmentation (MedSAM) provide an opportunity to introduce anatomically grounded priors that may improve robustness and interpretability in CXR analysis. We propose a segmentation-guided CXR classification pipeline that integrates MedSAM as a lung region extraction module prior to multi-label abnormality classification. MedSAM is fine-tuned using a public image-mask dataset from Airlangga University Hospital. We then apply it to a curated subset of the public NIH CXR dataset to train and evaluate deep convolutional neural networks for multi-label prediction of five abnormalities (Mass, Nodule, Pneumonia, Edema, and Fibrosis), with the normal case (No Finding) evaluated via a derived score. Experiments show that MedSAM produces anatomically plausible lung masks across diverse imaging conditions. We find that masking effects are both task-dependent and architecture-dependent. ResNet50 trained on original images achieves the strongest overall abnormality discrimination, while loose lung masking yields comparable macro AUROC but significantly improves No Finding discrimination, indicating a trade-off between abnormality-specific classification and normal case screening. Tight masking consistently reduces abnormality level performance but improves training efficiency. Loose masking partially mitigates this degradation by preserving perihilar and peripheral context. These results suggest that lung masking should be treated as a controllable spatial prior selected to match the backbone and clinical objective, rather than applied uniformly.

</details>


### [129] [INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading](https://arxiv.org/abs/2512.22666)
*Mert Ikinci,Luna Toma,Karin U. Loeffler,Leticia Ussem,Daniela Süsskind,Julia M. Weller,Yousef Yeganeh,Martina C. Herwig-Carl,Shadi Albarqouni*

Main category: cs.CV

TL;DR: 提出INTERACT-CMIL多任务深度学习框架，通过联合预测五个组织病理学指标来提升结膜黑色素细胞上皮内病变的分级准确性，并在多中心数据集上验证了其优于基线模型的性能。


<details>
  <summary>Details</summary>
Motivation: 结膜黑色素细胞上皮内病变（CMIL）的准确分级对治疗和黑色素瘤预测至关重要，但由于形态学特征细微且诊断标准相互关联，目前分级仍存在困难。

Method: 采用多任务深度学习框架INTERACT-CMIL，结合共享特征学习与组合部分监督，通过互依赖损失函数增强跨任务一致性，联合预测WHO4、WHO5、水平扩散、垂直扩散和细胞异型性五个病理指标。

Result: 在包含486个专家标注结膜活检切片的多中心数据集上，INTERACT-CMIL相比CNN和基础模型基线取得显著提升，相对宏观F1分数最高提升55.1%（WHO4）和25.0%（垂直扩散）。

Conclusion: 该框架提供了与专家分级一致的可解释多标准预测，为CMIL诊断建立了可重复的计算基准，推动了数字化眼病理学的标准化发展。

Abstract: Accurate grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL) is essential for treatment and melanoma prediction but remains difficult due to subtle morphological cues and interrelated diagnostic criteria. We introduce INTERACT-CMIL, a multi-head deep learning framework that jointly predicts five histopathological axes; WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia, through Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss enforcing cross-task consistency. Trained and evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three university hospitals, INTERACT-CMIL achieves consistent improvements over CNN and foundation-model (FM) baselines, with relative macro F1 gains up to 55.1% (WHO4) and 25.0% (vertical spread). The framework provides coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and a step toward standardized digital ocular pathology.

</details>


### [130] [FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution](https://arxiv.org/abs/2512.22647)
*Yidi Liu,Zihao Fan,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种用于图像超分辨率任务的细粒度感知奖励模型（FinPercep-RM）和协同进化课程学习（CCL）机制，以解决传统图像质量评估模型对局部失真不敏感导致的奖励欺骗问题，从而提升超分辨率模型的感知质量。


<details>
  <summary>Details</summary>
Motivation: 受强化学习与人类反馈在图像生成领域成功的启发，将其应用于图像超分辨率任务时，传统图像质量评估模型通常只输出全局分数，对局部和细粒度失真极不敏感，导致超分辨率模型可能产生感知上不理想的伪影却获得虚假高分，造成优化目标与感知质量不一致的奖励欺骗问题。

Method: 提出了基于编码器-解码器架构的细粒度感知奖励模型（FinPercep-RM），在提供全局质量分数的同时生成感知退化图，以空间定位和量化局部缺陷；并构建了包含真实世界超分辨率模型产生的多样且细微失真的FGR-30k数据集来训练该模型。针对奖励模型复杂性导致的生成器策略学习困难和训练不稳定问题，提出了协同进化课程学习（CCL）机制，使奖励模型和超分辨率模型同步进行课程学习，奖励模型逐步增加复杂性，而超分辨率模型则从简单的全局奖励开始快速收敛，逐渐过渡到更复杂的模型输出。

Result: 实验验证了该方法在基于强化学习与人类反馈的超分辨率模型中，在全局质量和局部真实感方面的有效性，能够实现稳定训练并抑制奖励欺骗。

Conclusion: 通过细粒度感知奖励模型和协同进化课程学习机制的结合，能够有效解决图像超分辨率任务中传统奖励模型的局限性，提升模型的感知质量，为基于强化学习的图像超分辨率方法提供了更可靠的优化目标。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has proven effective in image generation field guided by reward models to align human preferences. Motivated by this, adapting RLHF for Image Super-Resolution (ISR) tasks has shown promise in optimizing perceptual quality with Image Quality Assessment (IQA) model as reward models. However, the traditional IQA model usually output a single global score, which are exceptionally insensitive to local and fine-grained distortions. This insensitivity allows ISR models to produce perceptually undesirable artifacts that yield spurious high scores, misaligning optimization objectives with perceptual quality and results in reward hacking. To address this, we propose a Fine-grained Perceptual Reward Model (FinPercep-RM) based on an Encoder-Decoder architecture. While providing a global quality score, it also generates a Perceptual Degradation Map that spatially localizes and quantifies local defects. We specifically introduce the FGR-30k dataset to train this model, consisting of diverse and subtle distortions from real-world super-resolution models. Despite the success of the FinPercep-RM model, its complexity introduces significant challenges in generator policy learning, leading to training instability. To address this, we propose a Co-evolutionary Curriculum Learning (CCL) mechanism, where both the reward model and the ISR model undergo synchronized curricula. The reward model progressively increases in complexity, while the ISR model starts with a simpler global reward for rapid convergence, gradually transitioning to the more complex model outputs. This easy-to-hard strategy enables stable training while suppressing reward hacking. Experiments validates the effectiveness of our method across ISR models in both global quality and local realism on RLHF methods.

</details>


### [131] [CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation](https://arxiv.org/abs/2512.22681)
*ZhenQi Chen,TsaiChing Ni,YuanFu Yang*

Main category: cs.CV

TL;DR: 提出CritiFusion框架，通过多模态语义批判和频域细化改进文本到图像生成的语义对齐与细节质量，无需额外训练，可作为插件提升现有扩散模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型虽视觉保真度高，但在复杂提示下常出现语义对齐不足的问题，需要提升生成内容与提示意图的一致性。

Method: 1. CritiCore模块：结合视觉语言模型和多个大语言模型，丰富提示上下文并生成高层语义反馈，引导扩散过程；2. SpecFusion：在频域融合中间生成状态，注入粗粒度结构信息并保留高频细节。

Result: 在标准基准测试中，方法显著提升了文本到图像对应性和视觉质量的人类对齐指标，在人类偏好评分和美学评估上达到与最先进奖励优化方法相当的水平，定性结果显示出更优的细节、真实性和提示保真度。

Conclusion: CritiFusion通过语义批判和频域对齐策略，有效提升了扩散模型的语义一致性和生成质量，为现有模型提供了即插即用的优化方案。

Abstract: Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt's intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.

</details>


### [132] [Exploring Syn-to-Real Domain Adaptation for Military Target Detection](https://arxiv.org/abs/2512.23208)
*Jongoh Jeong,Youngjin Oh,Gyeongrae Nam,Jeongeun Lee,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 本文提出使用虚幻引擎生成逼真的合成RGB数据，以解决军事目标检测中真实数据稀缺和成本高的问题，并在合成到真实的跨域设置中评估了现有域自适应方法。


<details>
  <summary>Details</summary>
Motivation: 军事目标检测在民用和军事应用中至关重要，但现有域自适应方法主要针对自然或自动驾驶场景，难以应对军事领域多变的环境。SAR数据成本高且处理复杂，而RGB相机成本低但缺乏军事目标数据集，因此需要低成本、高效的解决方案。

Method: 使用虚幻引擎生成逼真的合成RGB数据用于军事目标检测，构建合成训练集和网络收集的真实验证集，并在该数据集对上评估不同监督程度（无监督、半监督、弱监督）的先进域自适应方法。

Result: 实验表明，使用图像级弱监督（如目标类别）的域自适应方法相比无监督或半监督方法有显著提升，突显了当前方法在军事跨域检测中的潜力与局限。

Conclusion: 合成数据生成是解决军事目标检测数据稀缺的有效途径，但现有域自适应方法在复杂军事场景中仍面临挑战，需进一步研究以提升跨域适应能力。

Abstract: Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.

</details>


### [133] [Autoregressive Flow Matching for Motion Prediction](https://arxiv.org/abs/2512.22688)
*Johnathan Xie,Stefan Stojanov,Cristobal Eyzaguirre,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出自回归流匹配（ARFM）方法，用于连续序列数据的概率建模，通过训练多样化视频数据集实现长时程未来点轨迹预测，并在人类和机器人运动预测任务中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有运动预测模型通常基于狭窄数据分布训练，而大规模视频预测方法虽视觉真实感强，却难以准确建模复杂运动。受视频生成规模化研究的启发，本文旨在开发一种能预测复杂运动且适用于下游任务的方法。

Method: 提出自回归流匹配（ARFM）方法，基于概率建模框架处理连续序列数据，在多样化视频数据集上训练模型，以生成长时程的未来点轨迹位置。

Result: ARFM模型能够准确预测复杂运动，实验表明，在机器人动作预测和人类运动预测任务中，基于预测的未来轨迹进行条件建模可显著提升下游任务性能。

Conclusion: ARFM方法在运动预测中表现出色，通过规模化训练和概率建模有效解决了复杂运动预测问题，为机器人学和人类运动分析提供了实用工具。

Abstract: Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.

</details>


### [134] [Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information](https://arxiv.org/abs/2512.23221)
*Youngchae Kwon,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 提出了一种新颖的整体检测变换器（Holi-DETR），通过利用三种上下文信息来整体检测时尚单品，以解决时尚物品检测中的歧义问题。


<details>
  <summary>Details</summary>
Motivation: 时尚物品检测面临挑战，因为时尚物品外观高度多样化且子类别间相似性高，导致歧义。传统检测器独立检测每个物品，忽略了时尚搭配中的上下文关系。

Method: 提出Holi-DETR架构，将三种异构上下文信息集成到DETR及其后续模型中：1) 时尚物品间的共现关系；2) 基于物品间空间布局的相对位置和大小；3) 物品与人体关键点之间的空间关系。

Result: 实验表明，该方法将原始DETR的平均精度（AP）提高了3.6个百分点，将最近开发的Co-DETR提高了1.1个百分点。

Conclusion: 通过整体利用多种上下文信息，Holi-DETR能有效减少时尚物品检测中的歧义，显著提升检测性能，证明了上下文信息在时尚检测中的重要性。

Abstract: Fashion item detection is challenging due to the ambiguities introduced by the highly diverse appearances of fashion items and the similarities among item subcategories. To address this challenge, we propose a novel Holistic Detection Transformer (Holi-DETR) that detects fashion items in outfit images holistically, by leveraging contextual information. Fashion items often have meaningful relationships as they are combined to create specific styles. Unlike conventional detectors that detect each item independently, Holi-DETR detects multiple items while reducing ambiguities by leveraging three distinct types of contextual information: (1) the co-occurrence relationship between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. %Holi-DETR explicitly incorporates three types of contextual information: (1) the co-occurrence probability between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. To this end, we propose a novel architecture that integrates these three types of heterogeneous contextual information into the Detection Transformer (DETR) and its subsequent models. In experiments, the proposed methods improved the performance of the vanilla DETR and the more recently developed Co-DETR by 3.6 percent points (pp) and 1.1 pp, respectively, in terms of average precision (AP).

</details>


### [135] [SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis](https://arxiv.org/abs/2512.22706)
*Paul Dobre,Jackson Cooper,Xin Wang,Hongzhou Yang*

Main category: cs.CV

TL;DR: 提出SCPainter统一框架，结合3D高斯泼溅资产表示与扩散模型，实现自动驾驶仿真中3D资产插入与新颖视角合成的联合优化。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶训练需要多样化的仿真数据，现有方法分别处理3D资产插入与视角合成，缺乏统一框架以实现资产与场景的真实交互及光照融合。

Method: 使用3D高斯泼溅表示动态车辆资产，与场景点云共同投影至新视角，通过扩散模型以投影信息为条件生成高质量图像。

Result: 在Waymo Open Dataset上验证了框架能同时实现逼真的3D资产插入与新颖视角合成，生成多样化的驾驶场景数据。

Conclusion: SCPainter为自动驾驶仿真提供了统一的资产插入与视角合成框架，有助于生成覆盖长尾场景的多样化训练数据，提升模型鲁棒性。

Abstract: 3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.

</details>


### [136] [Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors](https://arxiv.org/abs/2512.22689)
*Salvador Rodriguez-Sanz,Monica Hernandez*

Main category: cs.CV

TL;DR: 提出一种基于神经常微分方程的多模态可微同胚配准方法，通过结构描述符和局部互信息实现跨模态图像配准，在精度、计算复杂度和正则化之间取得良好平衡。


<details>
  <summary>Details</summary>
Motivation: 传统非刚性配准方法存在精度与计算复杂度之间的权衡，且通常依赖图像强度相关性，仅适用于单模态场景；现有学习方法需要大量训练数据且对未见模态泛化能力有限。

Method: 采用神经ODE框架构建连续深度网络，结合三种变体：基于图像的结构描述符、基于特征的结构描述符（利用参数化邻域几何的自相似性）以及非结构相似性度量（局部互信息）。

Result: 在多组扫描数据集实验中取得优于现有方法的定性与定量结果，适用于大/小形变配准；框架对显式正则化具有鲁棒性，支持多尺度配准，且在大形变配准任务中效率较高。

Conclusion: 该方法实现了无需大量训练数据的实例特异性多模态配准，克服了传统方法对模态的依赖，在保持低误差的同时展现出良好的正则化鲁棒性和计算效率。

Abstract: This work proposes a multimodal diffeomorphic registration method using Neural Ordinary Differential Equations (Neural ODEs). Nonrigid registration algorithms exhibit tradeoffs between their accuracy, the computational complexity of their deformation model, and its proper regularization. In addition, they also assume intensity correlation in anatomically homologous regions of interest among image pairs, limiting their applicability to the monomodal setting. Unlike learning-based models, we propose an instance-specific framework that is not subject to high scan requirements for training and does not suffer performance degradation at inference time on modalities unseen during training. Our method exploits the potential of continuous-depth networks in the Neural ODE paradigm with structural descriptors, widely adopted as modality-agnostic metric models which exploit self-similarities on parameterized neighborhood geometries. We propose three different variants that integrate image-based or feature-based structural descriptors and nonstructural image similarities computed by local mutual information. We conduct extensive evaluations on different experiments formed by scan dataset combinations and show surpassing qualitative and quantitative results compared to state-of-the-art baselines adequate for large or small deformations, and specific of multimodal registration. Lastly, we also demonstrate the underlying robustness of the proposed framework to varying levels of explicit regularization while maintaining low error, its suitability for registration at varying scales, and its efficiency with respect to other methods targeted to large-deformation registration.

</details>


### [137] [Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network](https://arxiv.org/abs/2512.23234)
*Dongsheng Li,Chaobo Chen,Siling Wang,Song Gao*

Main category: cs.CV

TL;DR: 提出PEG-DRNet模型，通过气体传输建模、边缘增强和多尺度特征路由，显著提升红外气体泄漏检测性能


<details>
  <summary>Details</summary>
Motivation: 红外气体泄漏检测面临挑战：羽流微弱、尺寸小、半透明且边界模糊，传统方法难以准确识别

Method: 1) Gas Block模拟气体扩散-对流传输；2) AGPEO边缘算子提取可靠边缘先验；3) CASR-PAN基于边缘和内容线索自适应聚合多尺度特征

Result: 在IIG数据集上AP达29.8%，AP50达84.3%，小目标AP提升5.3%，仅需43.7 Gflops和14.9M参数，优于现有CNN和Transformer检测器

Conclusion: PEG-DRNet在精度与计算效率间取得最佳平衡，为弱对比度、小尺寸气体泄漏检测提供了有效解决方案

Abstract: Detecting infrared gas leaks is critical for environmental monitoring and industrial safety, yet remains difficult because plumes are faint, small, semitransparent, and have weak, diffuse boundaries. We present physics-edge hybrid gas dynamic routing network (PEG-DRNet). First, we introduce the Gas Block, a diffusion-convection unit modeling gas transport: a local branch captures short-range variations, while a large-kernel branch captures long-range propagation. An edge-gated learnable fusion module balances local detail and global context, strengthening weak-contrast plume and contour cues. Second, we propose the adaptive gradient and phase edge operator (AGPEO), computing reliable edge priors from multi-directional gradients and phase-consistent responses. These are transformed by a multi-scale edge perception module (MSEPM) into hierarchical edge features that reinforce boundaries. Finally, the content-adaptive sparse routing path aggregation network (CASR-PAN), with adaptive information modulation modules for fusion and self, selectively propagates informative features across scales based on edge and content cues, improving cross-scale discriminability while reducing redundancy. Experiments on the IIG dataset show that PEG-DRNet achieves an overall AP of 29.8\%, an AP$_{50}$ of 84.3\%, and a small-object AP of 25.3\%, surpassing the RT-DETR-R18 baseline by 3.0\%, 6.5\%, and 5.3\%, respectively, while requiring only 43.7 Gflops and 14.9 M parameters. The proposed PEG-DRNet achieves superior overall performance with the best balance of accuracy and computational efficiency, outperforming existing CNN and Transformer detectors in AP and AP$_{50}$ on the IIG and LangGas dataset.

</details>


### [138] [Anomaly Detection by Effectively Leveraging Synthetic Images](https://arxiv.org/abs/2512.23227)
*Sungho Kang,Hyunkyu Park,Yeonho Lee,Hanbyul Lee,Mijoo Jeong,YeongHyeon Park,Injae Lee,Juneho Yi*

Main category: cs.CV

TL;DR: 提出一种利用预训练文本引导图像翻译模型和图像检索模型高效生成合成缺陷图像的新框架，通过两阶段训练策略提升工业异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中真实缺陷图像稀缺，现有合成方法存在质量与成本之间的权衡：基于规则的合成成本低但真实性不足，基于生成模型的合成质量高但成本昂贵。

Method: 1. 使用预训练文本引导图像到图像翻译模型生成合成缺陷图像；2. 利用图像检索模型评估生成图像与真实正常图像的相似性，过滤无关输出；3. 提出两阶段训练策略：先在大规模规则合成图像上预训练，再在小规模高质量图像上微调。

Result: 在MVTec AD数据集上的实验证明了该方法的有效性，能够在降低数据收集成本的同时提升异常检测性能。

Conclusion: 所提框架通过结合图像翻译与检索技术，实现了高质量合成缺陷图像的高效生成，两阶段训练策略平衡了数据成本与模型性能，为工业异常检测提供了一种实用解决方案。

Abstract: Anomaly detection plays a vital role in industrial manufacturing. Due to the scarcity of real defect images, unsupervised approaches that rely solely on normal images have been extensively studied. Recently, diffusion-based generative models brought attention to training data synthesis as an alternative solution. In this work, we focus on a strategy to effectively leverage synthetic images to maximize the anomaly detection performance. Previous synthesis strategies are broadly categorized into two groups, presenting a clear trade-off. Rule-based synthesis, such as injecting noise or pasting patches, is cost-effective but often fails to produce realistic defect images. On the other hand, generative model-based synthesis can create high-quality defect images but requires substantial cost. To address this problem, we propose a novel framework that leverages a pre-trained text-guided image-to-image translation model and image retrieval model to efficiently generate synthetic defect images. Specifically, the image retrieval model assesses the similarity of the generated images to real normal images and filters out irrelevant outputs, thereby enhancing the quality and relevance of the generated defect images. To effectively leverage synthetic images, we also introduce a two stage training strategy. In this strategy, the model is first pre-trained on a large volume of images from rule-based synthesis and then fine-tuned on a smaller set of high-quality images. This method significantly reduces the cost for data collection while improving the anomaly detection performance. Experiments on the MVTec AD dataset demonstrate the effectiveness of our approach.

</details>


### [139] [Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation](https://arxiv.org/abs/2512.22745)
*Yongzhen Hu,Yihui Yang,Haotong Lin,Yifan Wang,Junting Dong,Yifu Deng,Xinyu Zhu,Fan Jia,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 本文提出Freetime FeatureGS方法，通过可学习的特征高斯基元与线性运动能力，从单帧分割图直接重建分解的4D场景，无需依赖视频分割，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖视频分割结果进行4D重建，但视频分割质量不稳定，导致重建结果不可靠。本文旨在消除对视频分割的依赖，直接从单帧分割图实现鲁棒的分解4D场景重建。

Method: 使用Freetime FeatureGS表示动态场景，将场景建模为具有可学习特征和线性运动能力的高斯基元集合。通过对比损失使基元特征根据2D分割图中的实例归属进行聚类，并采用时序有序的流式特征学习策略，在时间维度上传播特征。

Result: 在多个数据集上的实验表明，该方法在重建质量上大幅优于现有方法，有效避免了优化过程中的局部极小值。

Conclusion: Freetime FeatureGS结合流式特征学习策略，能够直接从单帧分割图实现高质量、稳定的分解4D场景重建，为动态场景理解提供了新思路。

Abstract: This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.

</details>


### [140] [ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2512.23244)
*Xingwei Ma,Shiyang Feng,Bo Zhang,Bin Wang*

Main category: cs.CV

TL;DR: 提出ViLaCD-R1两阶段框架，结合多图像推理器和掩码引导解码器，通过监督微调和强化学习训练视觉语言模型，显著提升遥感变化检测的语义识别和定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统遥感变化检测方法难以捕捉高层语义且易受非语义干扰，现有多模态方法存在空间定位不准、边界划分模糊和可解释性有限等问题。

Method: 采用两阶段框架：1）多图像推理器通过监督微调和强化学习处理双时相图像块，输出粗变化掩码；2）掩码引导解码器融合双时相特征与粗掩码，生成精确二值变化图。

Result: 在多个遥感变化检测基准测试中，ViLaCD-R1显著提升真实语义变化的识别与定位能力，有效抑制非语义变化，在复杂现实场景中达到最先进精度。

Conclusion: ViLaCD-R1通过结合视觉语言模型的语义理解能力与精细解码机制，解决了遥感变化检测中的语义定位和边界精度问题，为复杂场景下的高精度变化检测提供了有效方案。

Abstract: Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.

</details>


### [141] [Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers](https://arxiv.org/abs/2512.22760)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 提出基于希尔伯特曲线重排序的邻居感知令牌缩减方法，通过保留二维空间中的邻居结构来提升视觉Transformer的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉Transformer的令牌表示存在冗余，而现有的令牌合并与剪枝方法常忽略空间连续性和邻居关系，导致局部上下文信息丢失。

Method: 采用希尔伯特曲线将二维令牌重排序为一维序列以保留邻居结构，提出邻居感知剪枝（NAP）进行选择性令牌保留，以及基于相邻令牌相似性的合并（MAT）进行局部令牌聚合。

Result: 实验表明，该方法在准确率与效率的权衡上达到了当前最优水平，优于现有方法。

Conclusion: 本研究强调了空间连续性和邻居结构在视觉Transformer架构优化中的重要性，为相关研究提供了新思路。

Abstract: Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.

</details>


### [142] [TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts](https://arxiv.org/abs/2512.22748)
*Hao Zhang,Mengsi Lyu,Bo Huang,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 提出了一种针对长上下文多图像场景的自适应视觉令牌剪枝方法，通过分解冗余为图像内和图像间两部分，动态分配预算，在保持性能的同时显著减少视觉令牌数量。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在处理多图像长上下文输入时，视觉令牌数量激增导致推理成本大幅增加，而现有剪枝方法未能有效解决此类场景的挑战。

Method: 1. 将冗余分解为图像内冗余（通过图像内多样性量化）和图像间冗余（通过图像间变化量化）；2. 两阶段剪枝：图像内阶段为每张图像分配内容感知的令牌预算并贪婪选择代表性令牌，图像间阶段进行全局多样性过滤和帕累托选择以平衡多样性与文本对齐。

Result: 大量实验表明，该方法在长上下文设置下能保持强大性能，同时显著减少视觉令牌数量，有效降低推理成本。

Conclusion: 所提出的自适应剪枝方法通过动态预算分配和两阶段处理，成功解决了多图像长上下文场景下的视觉令牌冗余问题，为高效多模态推理提供了可行方案。

Abstract: Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.

</details>


### [143] [Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification](https://arxiv.org/abs/2512.23436)
*Mustafa Demetgul,Sanja Lazarova Molnar*

Main category: cs.CV

TL;DR: 提出基于天气条件和路面数据的实时道路监测系统，使用手机摄像头采集数据，测试多种深度学习算法进行路面分类，准确率超95%，并建议结合模糊逻辑根据天气和时间分类。


<details>
  <summary>Details</summary>
Motivation: 传统道路监测方法昂贵且非系统化，需要时间测量，因此需要开发实时、经济的替代方案。

Method: 使用手机摄像头在校园道路采集数据，测试AlexNet、LeNet、VGG、ResNet等深度学习算法，结合加速度数据和图像数据训练，并建议使用模糊逻辑根据天气和时间分类。

Result: 在沥青、损坏沥青、砾石路、损坏砾石路、铺面路5类路面分类中，准确率超过95%。

Conclusion: 提出的实时系统能有效分类道路表面，结合深度学习与模糊逻辑可提升适应性，为车辆规划和控制系统提供有价值信息。

Abstract: Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.

</details>


### [144] [MedGemma vs GPT-4: Open-Source and Proprietary Zero-shot Medical Disease Classification from Images](https://arxiv.org/abs/2512.23304)
*Md. Sazzadul Islam Prottasha,Nabil Walid Rafi*

Main category: cs.CV

TL;DR: 本研究比较了专用开源模型MedGemma与通用多模态模型GPT-4在六种疾病诊断中的表现，发现经过微调的MedGemma在准确率和敏感性方面均优于GPT-4。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型为医学影像解读提供了新范式，但不同架构模型在临床诊断中的性能差异尚不明确，需要验证领域专用微调对提升诊断准确性和减少幻觉的重要性。

Method: 使用低秩适应（LoRA）对MedGemma-4b-it模型进行微调，与未调优的GPT-4进行对比，通过混淆矩阵和分类报告定量分析六种疾病的诊断性能。

Result: 微调后的MedGemma平均测试准确率达80.37%，显著高于GPT-4的69.58%；在癌症和肺炎等高风险临床任务中表现出更高的敏感性。

Conclusion: 领域专用微调对临床部署至关重要，能有效减少幻觉现象；MedGemma展现出成为复杂医学推理工具的潜力，为基于证据的医疗决策提供支持。

Abstract: Multimodal Large Language Models (LLMs) introduce an emerging paradigm for medical imaging by interpreting scans through the lens of extensive clinical knowledge, offering a transformative approach to disease classification. This study presents a critical comparison between two fundamentally different AI architectures: the specialized open-source agent MedGemma and the proprietary large multimodal model GPT-4 for diagnosing six different diseases. The MedGemma-4b-it model, fine-tuned using Low-Rank Adaptation (LoRA), demonstrated superior diagnostic capability by achieving a mean test accuracy of 80.37% compared to 69.58% for the untuned GPT-4. Furthermore, MedGemma exhibited notably higher sensitivity in high-stakes clinical tasks, such as cancer and pneumonia detection. Quantitative analysis via confusion matrices and classification reports provides comprehensive insights into model performance across all categories. These results emphasize that domain-specific fine-tuning is essential for minimizing hallucinations in clinical implementation, positioning MedGemma as a sophisticated tool for complex, evidence-based medical reasoning.

</details>


### [145] [SoulX-LiveTalk Technical Report](https://arxiv.org/abs/2512.23379)
*Le Shen,Qiao Qian,Tan Yu,Ke Zhou,Tianhang Yu,Yu Zhan,Zhenjie Wang,Ming Tao,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: 提出SoulX-LiveTalk框架，通过双向注意力蒸馏与自校正机制，实现14B参数规模下高保真、实时、无限时长的音频驱动虚拟人生成，达到0.87秒启动延迟与32FPS实时吞吐。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时音频驱动虚拟人生成中面临计算负载与延迟约束的冲突，常因强制单向注意力或降低模型容量而牺牲视觉保真度。

Method: 采用自校正双向蒸馏策略保留视频块内双向注意力；引入多步回顾自校正机制防止无限生成中的误差累积；结合混合序列并行、并行VAE与内核级优化的全栈推理加速套件。

Result: 系统在14B参数规模下实现0.87秒启动延迟与32FPS实时吞吐，首次达到亚秒级启动延迟，并在运动连贯性与视觉细节上显著提升。

Conclusion: SoulX-LiveTalk为高保真交互式数字人生成设立了新标准，通过平衡计算效率与生成质量，解决了实时流式生成中的关键工程挑战。

Abstract: Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.

</details>


### [146] [Parallel Diffusion Solver via Residual Dirichlet Policy Optimization](https://arxiv.org/abs/2512.22796)
*Ruoyu Wang,Ziyu Li,Beier Zhu,Liangyu Yuan,Hanwang Zhang,Xun Yang,Xiaojun Chang,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出EPD-Solver，一种新型ODE求解器，通过并行梯度评估减少截断误差，加速扩散模型采样，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型采样延迟高，现有加速方法在低延迟预算下图像质量下降严重，主要由于高曲率轨迹段的截断误差累积。

Method: 提出EPD-Solver，利用采样轨迹集中在低维流形的几何特性，基于向量值函数中值定理近似积分解；采用两阶段优化：蒸馏学习可调参数，再通过参数高效的强化学习微调（将求解器重构为随机Dirichlet策略）。

Result: EPD-Solver通过并行梯度计算减少误差，保持低延迟采样特性；可作为插件（EPD-Plugin）提升现有ODE采样器性能。

Conclusion: EPD-Solver有效平衡采样速度与质量，在复杂文生图任务中表现优异，且避免对大规模主干网络微调，防止奖励黑客问题。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.

</details>


### [147] [CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models](https://arxiv.org/abs/2512.23453)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: 提出CoFi-Dec训练无关解码框架，通过粗到细视觉条件与生成式自反馈减少大视觉语言模型幻觉，无需额外训练即可提升多模态理解可靠性。


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在多模态理解中仍存在与视觉输入不一致的幻觉问题，限制了实际应用的可靠性，需开发无需训练的高效缓解方法。

Method: 受人类视觉从全局到细节的认知过程启发，首先生成基于粗/细粒度图像视图的中间文本响应，通过文生图模型转化为多级视觉假设；引入Wasserstein融合机制对齐预测分布，实现几何一致的解码轨迹。

Result: 在六个幻觉基准测试中显著减少实体级和语义级幻觉，优于现有解码策略，且框架具有模型无关性、无需训练、可广泛适配。

Conclusion: CoFi-Dec通过多级视觉条件与分布融合机制，在保持高层语义一致性的同时增强细粒度视觉基础，为缓解LVLM幻觉提供了有效且通用的解决方案。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.

</details>


### [148] [VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM](https://arxiv.org/abs/2512.22799)
*Jingchao Wang,Kaiwen Zhou,Zhijian Wu,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: 提出首个基于多模态大语言模型的全局视觉语言跟踪框架VPTracker，通过位置感知视觉提示机制结合空间先验，在保持全局搜索优势的同时有效抑制干扰，显著提升跟踪稳定性和目标区分能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言跟踪方法通常局限于局部搜索，在视角变化、遮挡和目标快速移动等场景下容易失败，需要一种能进行全局语义推理的鲁棒跟踪框架。

Method: 1. 构建基于多模态大语言模型的全局跟踪框架；2. 提出位置感知视觉提示机制，根据目标历史位置构建区域级提示，使模型优先进行区域级识别，必要时才进行全局推理。

Result: 大量实验表明，该方法在挑战性场景下显著提升了跟踪稳定性和目标区分能力，为多模态大语言模型在视觉跟踪中的应用开辟了新途径。

Conclusion: VPTracker通过结合全局语义推理和空间先验，有效解决了传统局部跟踪方法的局限性，展示了多模态大语言模型在复杂视觉跟踪任务中的潜力。

Abstract: Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.

</details>


### [149] [Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation](https://arxiv.org/abs/2512.22800)
*Bin Liu,Wenyan Tian,Huangxin Fu,Zizheng Li,Zhifen He,Bo Li*

Main category: cs.CV

TL;DR: 提出一种基于3D高斯和tri-plane表示的高效医学图像3D重建方法，在稀疏切片条件下提升结构连续性和语义一致性。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像3D重建方法计算成本高，在稀疏切片下易出现结构不连续和细节丢失，难以满足临床精度需求。

Method: 结合3D高斯表示的高效渲染与几何表示优势，引入tri-plane表示增强稀疏条件下的结构连续性与语义一致性。

Result: 在超声和MRI等多模态医学数据集上验证，该方法能在稀疏数据下生成高质量、解剖结构连贯且语义稳定的医学图像，并显著提升重建效率。

Conclusion: 为医学图像3D可视化与临床分析提供了一种高效可靠的新途径。

Abstract: 3D reconstruction of medical images is a key technology in medical image analysis and clinical diagnosis, providing structural visualization support for disease assessment and surgical planning. Traditional methods are computationally expensive and prone to structural discontinuities and loss of detail in sparse slices, making it difficult to meet clinical accuracy requirements.To address these challenges, we propose an efficient 3D reconstruction method based on 3D Gaussian and tri-plane representations. This method not only maintains the advantages of Gaussian representation in efficient rendering and geometric representation but also significantly enhances structural continuity and semantic consistency under sparse slicing conditions. Experimental results on multimodal medical datasets such as US and MRI show that our proposed method can generate high-quality, anatomically coherent, and semantically stable medical images under sparse data conditions, while significantly improving reconstruction efficiency. This provides an efficient and reliable new approach for 3D visualization and clinical analysis of medical images.

</details>


### [150] [HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation](https://arxiv.org/abs/2512.23464)
*Yuxin Wen,Qing Shuai,Di Kang,Jing Li,Cheng Wen,Yue Qian,Ningxin Jiao,Changhai Chen,Weijie Chen,Yiran Wang,Jinkun Guo,Dongyue An,Han Liu,Yanyu Tong,Chao Zhang,Qing Guo,Juan Chen,Qiao Zhang,Youyi Zhang,Zihao Yao,Cheng Zhang,Hong Duan,Xiaoping Wu,Qi Chen,Fei Cheng,Liang Dong,Peng He,Hao Zhang,Jiaxin Lin,Chao Zhang,Zhongyi Fan,Yifan Li,Zhichao Hu,Yuhong Liu,Linus,Jie Jiang,Xiaolong Li,Linchao Bao*

Main category: cs.CV

TL;DR: HY-Motion 1.0是一个基于扩散变换器的大规模运动生成模型，能够根据文本描述生成3D人体动作，在参数规模和性能上超越现有开源基准。


<details>
  <summary>Details</summary>
Motivation: 当前3D人体运动生成模型在规模、指令跟随能力和运动质量方面存在局限，需要更强大的模型来推动该领域向商业化成熟发展。

Method: 采用扩散变换器架构的流匹配模型，参数达十亿级；实施全阶段训练范式：大规模预训练（3000+小时运动数据）、高质量微调（400小时精选数据）、基于人类反馈和奖励模型的强化学习；配合严格的数据清洗和标注流程。

Result: 模型在指令跟随能力上显著超越现有开源基准，覆盖6大类超过200种运动类别，实现了文本指令与运动生成的高精度对齐和高质量运动输出。

Conclusion: HY-Motion 1.0通过规模化架构和全阶段训练框架，在3D人体运动生成领域取得了突破性进展，开源发布旨在促进未来研究并加速该技术的商业化进程。

Abstract: We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.

</details>


### [151] [AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization](https://arxiv.org/abs/2512.23537)
*Binhe Yu,Zhen Wang,Kexin Li,Yuqian Yuan,Wenqiao Zhang,Long Chen,Juncheng Li,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: 提出AnyMS，一种无需训练、基于布局引导的多主体定制框架，通过双级注意力解耦机制平衡文本对齐、主体身份保持和布局控制。


<details>
  <summary>Details</summary>
Motivation: 现有多主体定制方法难以同时平衡文本对齐、主体身份保持和布局控制三个目标，且依赖额外训练导致可扩展性和效率受限。

Method: 采用无需训练的框架，结合文本提示、主体图像和布局约束三种输入条件，引入自底向上的全局与局部双级注意力解耦机制，并使用预训练图像适配器提取主体特征。

Result: 实验表明AnyMS在复杂构图和更多主体数量上达到最先进性能，无需主体学习或适配器调优。

Conclusion: AnyMS通过注意力解耦和预训练适配器有效解决了多主体定制中的平衡问题，实现了高效、可扩展的布局引导生成。

Abstract: Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.

</details>


### [152] [Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image](https://arxiv.org/abs/2512.22801)
*Po-Chih Wu*

Main category: cs.CV

TL;DR: 该研究评估了开放词汇目标检测模型在低质量图像条件下的性能，并引入了一个模拟真实世界低质量图像的新数据集。实验发现，在高级别图像退化时所有模型性能均显著下降，其中OWLv2表现相对稳健。


<details>
  <summary>Details</summary>
Motivation: 开放词汇目标检测旨在实现接近人类水平的识别能力，但现有模型在真实世界低质量图像条件下的性能尚未得到充分评估，这限制了其实际应用。

Method: 1. 构建模拟真实世界低质量图像的新数据集；2. 在低级别和高级别图像退化条件下测试多种开放词汇检测模型（OWLv2、OWL-ViT、GroundingDINO、Detic）；3. 使用mAP指标进行性能评估。

Result: 1. 低级别图像退化时模型mAP无显著下降；2. 高级别图像退化时所有模型性能急剧下降；3. OWLv2在不同退化类型中表现最稳定，其他模型性能下降显著。

Conclusion: 开放词汇检测模型对高级别图像退化敏感，现有方法在真实低质量图像场景中仍存在局限。发布的数据集和代码将促进该领域的进一步研究。

Abstract: Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.

</details>


### [153] [3D Scene Change Modeling With Consistent Multi-View Aggregation](https://arxiv.org/abs/2512.22830)
*Zirui Zhou,Junfeng Ni,Shujie Zhang,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出SCaR-3D框架，通过基于符号距离的2D差分和多视图聚合，实现3D场景中对象级变化检测，并支持选择性更新的持续重建。


<details>
  <summary>Details</summary>
Motivation: 现有3D变化检测方法存在空间不一致性，且无法明确分离变化前后的状态，需要更鲁棒和高效的解决方案。

Method: 使用基于符号距离的2D差分模块，结合多视图投票与剪枝的聚合方法，利用3DGS的稳定性分离变化前后状态，并设计选择性更新的持续重建策略。

Result: 在合成的CCS3D数据集上验证，方法在准确性和效率上均优于现有方法。

Conclusion: SCaR-3D能有效检测对象级变化并支持持续重建，为场景监控与探索提供了可靠工具。

Abstract: Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.

</details>


### [154] [A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences](https://arxiv.org/abs/2512.22833)
*Zhenbao Yu,Shirong Ye,Ronghe Jin,Shunkun Liang,Zibin Liu,Huiyun Zhang,Banglei Guan*

Main category: cs.CV

TL;DR: 提出一种基于两个仿射对应和已知垂直方向（来自IMU）的3自由度相对位姿和焦距估计方法，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶、智能手机和无人机等应用中，相机常与IMU结合使用。IMU可提供相机垂直方向，将相对位姿从5自由度降至3自由度，从而简化估计问题。

Method: 利用两个仿射对应和已知垂直方向建立约束方程，基于方程组非平凡解性质推导出仅含焦距和相对旋转角的四个方程，采用多项式特征值方法求解。

Result: 在合成和真实数据集上的实验表明，所提求解器性能优于现有先进方法。

Conclusion: 该方法能有效估计相对位姿和焦距，适用于IMU辅助的视觉系统，具有实际应用价值。

Abstract: In this paper, we aim to estimate the relative pose and focal length between two views with known intrinsic parameters except for an unknown focal length from two affine correspondences (ACs). Cameras are commonly used in combination with inertial measurement units (IMUs) in applications such as self-driving cars, smartphones, and unmanned aerial vehicles. The vertical direction of camera views can be obtained by IMU measurements. The relative pose between two cameras is reduced from 5DOF to 3DOF. We propose a new solver to estimate the 3DOF relative pose and focal length. First, we establish constraint equations from two affine correspondences when the vertical direction is known. Then, based on the properties of the equation system with nontrivial solutions, four equations can be derived. These four equations only involve two parameters: the focal length and the relative rotation angle. Finally, the polynomial eigenvalue method is utilized to solve the problem of focal length and relative rotation angle. The proposed solver is evaluated using synthetic and real-world datasets. The results show that our solver performs better than the existing state-of-the-art solvers.

</details>


### [155] [PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis](https://arxiv.org/abs/2512.23545)
*Shengyi Hua,Jianfeng Wu,Tianle Shen,Kangzhe Hu,Zhongzhen Huang,Shujuan Ni,Zhihong Zhang,Yuan Li,Zhe Wang,Xiaofan Zhang*

Main category: cs.CV

TL;DR: 提出PathFound，一种基于代理的多模态模型，通过主动信息获取和诊断细化来模拟临床诊断流程，在计算病理学中实现最先进的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有病理基础模型大多采用静态推理范式，即一次性处理全切片图像进行预测，缺乏在诊断模糊时重新评估或针对性获取证据的能力，这与临床通过重复观察和进一步检查来细化假设的诊断流程不符。

Method: PathFound整合病理视觉基础模型、视觉语言模型和强化学习训练的逻辑模型，通过初始诊断、证据搜寻和最终决策三个阶段，执行主动信息获取和诊断细化。

Result: 采用该策略在多模态模型中一致提高了诊断准确性，PathFound在多种临床场景中达到最先进的诊断性能，并展现出发现细微特征（如核特征和局部浸润）的潜力。

Conclusion: 证据搜寻工作流程在计算病理学中具有有效性，PathFound通过模拟临床诊断的动态过程，显著提升了诊断准确性和细节发现能力。

Abstract: Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.

</details>


### [156] [KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution](https://arxiv.org/abs/2512.22822)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: 提出了一种基于Kolmogorov-Arnold定理的新型可解释算子KANO，用于单图像超分辨率任务，通过B样条函数逼近光谱曲线，提升退化过程的可解释性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有可解释超分辨率方法依赖黑盒深度网络建模隐变量，导致退化过程不透明且不可控，需要一种更透明、结构化的表示方法。

Method: 基于Kolmogorov-Arnold定理设计KANO算子，采用有限个B样条函数的加性结构分段逼近连续光谱曲线，通过优化样条函数的形状参数捕捉局部线性趋势和峰值-谷值结构。

Result: KANO能准确捕捉关键光谱特征，赋予超分辨率结果物理可解释性；通过理论建模和实验对比，系统分析了MLP与KAN在复杂序列拟合任务中的优劣。

Conclusion: KANO为可解释超分辨率技术提供了新思路，通过结构化表示退化过程，增强了模型的可控性和解释性，为处理复杂退化机制提供了有价值的见解。

Abstract: The highly nonlinear degradation process, complex physical interactions, and various sources of uncertainty render single-image Super-resolution (SR) a particularly challenging task. Existing interpretable SR approaches, whether based on prior learning or deep unfolding optimization frameworks, typically rely on black-box deep networks to model latent variables, which leaves the degradation process largely unknown and uncontrollable. Inspired by the Kolmogorov-Arnold theorem (KAT), we for the first time propose a novel interpretable operator, termed Kolmogorov-Arnold Neural Operator (KANO), with the application to image SR. KANO provides a transparent and structured representation of the latent degradation fitting process. Specifically, we employ an additive structure composed of a finite number of B-spline functions to approximate continuous spectral curves in a piecewise fashion. By learning and optimizing the shape parameters of these spline functions within defined intervals, our KANO accurately captures key spectral characteristics, such as local linear trends and the peak-valley structures at nonlinear inflection points, thereby endowing SR results with physical interpretability. Furthermore, through theoretical modeling and experimental evaluations across natural images, aerial photographs, and satellite remote sensing data, we systematically compare multilayer perceptrons (MLPs) and Kolmogorov-Arnold networks (KANs) in handling complex sequence fitting tasks. This comparative study elucidates the respective advantages and limitations of these models in characterizing intricate degradation mechanisms, offering valuable insights for the development of interpretable SR techniques.

</details>


### [157] [RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature](https://arxiv.org/abs/2512.23565)
*Hanzheng Li,Xi Fang,Yixuan Li,Chaozheng Huang,Junjie Wang,Xi Wang,Hongzhe Bai,Bojun Hao,Shenyu Lin,Huiqi Liang,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: 提出了RxnBench基准测试，评估多模态大语言模型在化学文献中理解反应图示的能力，发现现有模型在化学逻辑和结构识别方面存在显著不足。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在化学领域的应用潜力巨大，但其对真实科学文献中密集、图形化的反应语言的理解能力尚未得到充分探索，需要系统评估。

Method: 构建了RxnBench多层级基准测试，包含两个任务：基于305个反应方案的1,525个问题的单图问答（SF-QA），以及基于108篇文献的全文档问答（FD-QA），测试模型的视觉感知、机理推理和跨模态信息整合能力。

Result: 评估显示模型在提取显式文本方面表现良好，但在深层化学逻辑和精确结构识别方面存在能力缺口；具有推理时推理能力的模型优于标准架构，但FD-QA任务中无一模型准确率超过50%。

Conclusion: 研究强调了开发领域专用视觉编码器和更强推理引擎的迫切需求，以推动自主AI化学家的进展。

Abstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.

</details>


### [158] [Depth Anything in $360^\circ$: Towards Scale Invariance in the Wild](https://arxiv.org/abs/2512.22819)
*Hualie Jiang,Ziyang Song,Zhiqiang Lou,Rui Xu,Minglang Tan*

Main category: cs.CV

TL;DR: 本文提出DA360，一种全景深度估计方法，通过改进Depth Anything V2模型，实现了在室内外场景的零样本泛化能力，显著提升了全景深度估计的精度。


<details>
  <summary>Details</summary>
Motivation: 全景深度估计在机器人和AR/VR应用中具有重要价值，但其在开放世界中的零样本泛化能力远落后于透视图像，主要原因是缺乏充足的训练数据。因此，需要将透视图像领域的深度估计能力迁移到全景领域。

Method: 提出DA360方法，基于Depth Anything V2模型进行全景适配。关键创新包括：从ViT骨干网络学习偏移参数，将模型的尺度与偏移不变输出转换为尺度不变估计，直接生成良好的3D点云；在DPT解码器中集成圆形填充，消除接缝伪影，确保空间连贯的深度图。

Result: 在标准室内基准和新构建的室外数据集Metropolis上评估，DA360相比基础模型在室内和室外基准上分别实现了超过50%和10%的相对深度误差减少。与现有全景深度估计方法相比，DA360在所有三个测试数据集上实现了约30%的相对误差改进，达到了零样本全景深度估计的最新性能。

Conclusion: DA360通过有效的迁移学习和全景适配，显著提升了全景深度估计的零样本泛化能力，在室内外场景均取得了最先进的性能，为机器人和AR/VR应用提供了更可靠的环境结构信息。

Abstract: Panoramic depth estimation provides a comprehensive solution for capturing complete $360^\circ$ environmental structural information, offering significant benefits for robotics and AR/VR applications. However, while extensively studied in indoor settings, its zero-shot generalization to open-world domains lags far behind perspective images, which benefit from abundant training data. This disparity makes transferring capabilities from the perspective domain an attractive solution. To bridge this gap, we present Depth Anything in $360^\circ$ (DA360), a panoramic-adapted version of Depth Anything V2. Our key innovation involves learning a shift parameter from the ViT backbone, transforming the model's scale- and shift-invariant output into a scale-invariant estimate that directly yields well-formed 3D point clouds. This is complemented by integrating circular padding into the DPT decoder to eliminate seam artifacts, ensuring spatially coherent depth maps that respect spherical continuity. Evaluated on standard indoor benchmarks and our newly curated outdoor dataset, Metropolis, DA360 shows substantial gains over its base model, achieving over 50\% and 10\% relative depth error reduction on indoor and outdoor benchmarks, respectively. Furthermore, DA360 significantly outperforms robust panoramic depth estimation methods, achieving about 30\% relative error improvement compared to PanDA across all three test datasets and establishing new state-of-the-art performance for zero-shot panoramic depth estimation.

</details>


### [159] [ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning](https://arxiv.org/abs/2512.22854)
*Bangya Liu,Xinyu Gong,Zelin Zhao,Ziyang Song,Yulei Lu,Suhui Wu,Jun Zhang,Suman Banerjee,Hao Zhang*

Main category: cs.CV

TL;DR: 提出了ByteLoom，一个基于扩散Transformer的框架，用于生成具有几何一致物体表示的真实人-物交互视频，解决了现有方法在多视角一致性和手部网格标注依赖方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的人-物交互视频生成方法存在两个关键局限：缺乏有效的多视角信息注入机制导致跨视角一致性差，以及严重依赖精细的手部网格标注来建模交互遮挡。

Method: 提出了ByteLoom框架，采用扩散Transformer架构，引入RCM-cache机制（利用相对坐标图作为通用表示保持物体几何一致性并精确控制6自由度变换），并设计了渐进式训练课程以缓解数据集稀缺问题并降低对手部网格的需求。

Result: 实验表明，该方法能忠实保持人体身份和物体的多视角几何一致性，同时维持平滑的运动和物体操控效果。

Conclusion: ByteLoom通过创新的几何表示和训练策略，实现了高质量、几何一致的人-物交互视频生成，减少了对精细标注的依赖，具有广泛的应用潜力。

Abstract: Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.

</details>


### [160] [Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs](https://arxiv.org/abs/2512.22872)
*Ziyu Zhou,Haozhe Luo,Mohammad Reza Hosseinzadeh Taher,Jiaxuan Pang,Xiaowei Ding,Michael B. Gotway,Jianming Liang*

Main category: cs.CV

TL;DR: 提出Lamps方法，通过自监督学习从多角度学习人体解剖结构，在胸部X光片上预训练，优于10个基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在医学影像中常忽略人体解剖的一致性、连贯性和层次性，限制了学习解剖特征的能力。

Method: 利用人体解剖的一致性、连贯性和层次性作为监督信号，在大规模胸部X光片上预训练Lamps模型。

Result: 在10个数据集上的实验表明，Lamps在鲁棒性、可迁移性和临床潜力方面优于10个基线模型。

Conclusion: Lamps为构建与人体解剖结构对齐的基础模型提供了独特机会，能学习有意义且鲁棒的表示。

Abstract: Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.

</details>


### [161] [Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples](https://arxiv.org/abs/2512.22874)
*Weiwei Li,Junzhuo Liu,Yuanyuan Ren,Yuchen Zheng,Yahao Liu,Wen Li*

Main category: cs.CV

TL;DR: 提出一种数据导向的方法来缓解深度学习模型中的虚假相关性，通过识别、中和、消除和更新四个步骤构建去偏流程，在图像和NLP基准测试中显著提升最差组准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要标注潜在虚假属性或基于经验假设过滤虚假特征，但由于真实数据中虚假相关性复杂且难以捉摸，这些方法性能有限。

Method: 1. 基于虚假特征样本在特征空间中的分散分布识别虚假特征；2. 通过简单分组策略中和虚假特征获得偏置不变表示；3. 学习特征变换以消除虚假特征；4. 更新分类器获得无偏模型。

Result: 在图像和NLP去偏基准测试中，相比标准经验风险最小化（ERM），最差组准确率提升超过20%。

Conclusion: 提出的四步流程能有效缓解深度学习模型中的虚假相关性，代码和检查点已开源。

Abstract: Deep learning models are known to often learn features that spuriously correlate with the class label during training but are irrelevant to the prediction task. Existing methods typically address this issue by annotating potential spurious attributes, or filtering spurious features based on some empirical assumptions (e.g., simplicity of bias). However, these methods may yield unsatisfactory performance due to the intricate and elusive nature of spurious correlations in real-world data. In this paper, we propose a data-oriented approach to mitigate the spurious correlation in deep learning models. We observe that samples that are influenced by spurious features tend to exhibit a dispersed distribution in the learned feature space. This allows us to identify the presence of spurious features. Subsequently, we obtain a bias-invariant representation by neutralizing the spurious features based on a simple grouping strategy. Then, we learn a feature transformation to eliminate the spurious features by aligning with this bias-invariant representation. Finally, we update the classifier by incorporating the learned feature transformation and obtain an unbiased model. By integrating the aforementioned identifying, neutralizing, eliminating and updating procedures, we build an effective pipeline for mitigating spurious correlation. Experiments on image and NLP debiasing benchmarks show an improvement in worst group accuracy of more than 20% compared to standard empirical risk minimization (ERM). Codes and checkpoints are available at https://github.com/davelee-uestc/nsf_debiasing .

</details>


### [162] [CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision](https://arxiv.org/abs/2512.22969)
*Behnam Raoufi,Hossein Sharify,Mohamad Mahdee Ramezanee,Khosrow Hajsadeghi,Saeed Bagheri Shouraki*

Main category: cs.CV

TL;DR: 提出CLIP-Joint-Detect框架，通过端到端联合训练将CLIP对比视觉语言监督集成到目标检测中，提升检测性能并保持实时推理速度。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测器依赖交叉熵分类，易受类别不平衡和标签噪声影响，需要更鲁棒的监督方式。

Method: 使用轻量级并行头将区域/网格特征投影到CLIP嵌入空间，通过InfoNCE对比损失和辅助交叉熵项与可学习的类别特定文本嵌入对齐，同时优化所有标准检测损失。

Result: 在Pascal VOC和MS COCO基准测试中，使用Faster R-CNN和YOLOv11均取得显著改进，且保持实时推理速度。

Conclusion: 联合优化与可学习文本嵌入能显著提升闭集检测性能，该方法适用于两阶段和一阶段检测架构。

Abstract: Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.

</details>


### [163] [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://arxiv.org/abs/2512.22905)
*Kai Liu,Jungang Li,Yuchong Sun,Shengqiong Wu,Jianzhang Gao,Daoan Zhang,Wei Zhang,Sheng Jin,Sicheng Yu,Geng Zhan,Jiayi Ji,Fan Zhou,Liang Zheng,Shuicheng Yan,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: JavisGPT是首个用于联合视听理解与生成的多模态大语言模型，采用编码器-LLM-解码器架构，通过三阶段训练流程在视听任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视听联合理解与生成方面存在局限，特别是在时空同步的复杂场景中，需要统一模型处理多模态指令下的连贯视听内容。

Method: 采用编码器-LLM-解码器架构，包含SyncFusion模块进行时空视听融合，使用同步感知可学习查询连接预训练的JAV-DiT生成器；设计三阶段训练流程（多模态预训练、视听微调、大规模指令调优），并构建包含20万+对话的JavisInst-Omni数据集。

Result: 在视听理解与生成基准测试中，JavisGPT超越现有MLLMs，尤其在复杂和需要时间同步的场景中表现突出。

Conclusion: JavisGPT通过创新的架构设计和训练流程，实现了高质量的联合视听理解与生成，为多模态AI系统的发展提供了新方向。

Abstract: This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.

</details>


### [164] [Learning Where to Focus: Density-Driven Guidance for Detecting Dense Tiny Objects](https://arxiv.org/abs/2512.22949)
*Zhicheng Zhao,Xuanang Fan,Lingma Sun,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: 提出DRMNet网络，利用密度图作为空间先验指导特征学习，解决高分辨率遥感图像中密集小目标检测的遮挡和像素限制问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率遥感图像中密集小目标存在严重相互遮挡和像素限制，现有检测方法均匀分配计算资源，无法自适应聚焦密集区域，影响特征学习效果。

Method: 1. 密度生成分支建模目标分布模式；2. 密集区域聚焦模块利用密度图识别密集区域，实现高效局部-全局特征交互；3. 双滤波器融合模块通过离散余弦变换分离多尺度特征高低频分量，进行密度引导的交叉注意力增强互补性。

Result: 在AI-TOD和DTOD数据集上的实验表明，DRMNet超越现有最优方法，尤其在目标密度高、遮挡严重的复杂场景中表现突出。

Conclusion: DRMNet通过密度引导的自适应特征学习机制，有效提升了密集小目标检测性能，为解决遥感图像中的密集目标检测问题提供了新思路。

Abstract: High-resolution remote sensing imagery increasingly contains dense clusters of tiny objects, the detection of which is extremely challenging due to severe mutual occlusion and limited pixel footprints. Existing detection methods typically allocate computational resources uniformly, failing to adaptively focus on these density-concentrated regions, which hinders feature learning effectiveness. To address these limitations, we propose the Dense Region Mining Network (DRMNet), which leverages density maps as explicit spatial priors to guide adaptive feature learning. First, we design a Density Generation Branch (DGB) to model object distribution patterns, providing quantifiable priors that guide the network toward dense regions. Second, to address the computational bottleneck of global attention, our Dense Area Focusing Module (DAFM) uses these density maps to identify and focus on dense areas, enabling efficient local-global feature interaction. Finally, to mitigate feature degradation during hierarchical extraction, we introduce a Dual Filter Fusion Module (DFFM). It disentangles multi-scale features into high- and low-frequency components using a discrete cosine transform and then performs density-guided cross-attention to enhance complementarity while suppressing background interference. Extensive experiments on the AI-TOD and DTOD datasets demonstrate that DRMNet surpasses state-of-the-art methods, particularly in complex scenarios with high object density and severe occlusion.

</details>


### [165] [M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models](https://arxiv.org/abs/2512.22877)
*Ju-Hsuan Weng,Jia-Wei Liao,Cheng-Fu Chou,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: 本文提出了M-ErasureBench多模态概念擦除评估框架和IRECE增强方法，发现现有方法仅对文本提示有效，在多模态攻击下失效，IRECE能显著提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有概念擦除方法主要针对文本提示，忽略了图像编辑和个性化生成中常用的其他输入模态（如学习嵌入和反转潜在表示），这些模态可能成为攻击面，导致已擦除概念重新出现。

Method: 1) 提出M-ErasureBench框架，系统评估文本提示、学习嵌入和反转潜在表示三种输入模态下的概念擦除效果，包含白盒和黑盒共五种场景；2) 提出IRECE方法，通过交叉注意力定位目标概念并在去噪过程中扰动相关潜在表示。

Result: 现有方法在文本提示上擦除效果良好，但在学习嵌入和反转潜在表示上严重失效（白盒设置下概念再现率超过90%）。IRECE能恢复鲁棒性，在最挑战性的白盒潜在反转场景下将概念再现率降低达40%，同时保持视觉质量。

Conclusion: M-ErasureBench是首个超越文本提示的全面概念擦除基准，结合IRECE方法为构建更可靠的保护性生成模型提供了实用保障，揭示了多模态输入下概念擦除的脆弱性并提出了有效解决方案。

Abstract: Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.

</details>


### [166] [Wavelet-based Multi-View Fusion of 4D Radar Tensor and Camera for Robust 3D Object Detection](https://arxiv.org/abs/2512.22972)
*Runwei Guan,Jianan Liu,Shaofeng Liang,Fangqiang Ding,Shanliang Yao,Xiaokai Bai,Daizong Liu,Tao Huang,Guoqiang Mao,Hui Xiong*

Main category: cs.CV

TL;DR: 提出WRCFormer框架，通过多视图表示融合原始4D雷达立方体与相机数据，用于3D目标检测，在恶劣天气下表现优异。


<details>
  <summary>Details</summary>
Motivation: 4D毫米波雷达成本低、全天候鲁棒，但点云稀疏且语义信息有限；现有融合方法存在信息损失或计算成本高的问题。

Method: 设计基于小波注意力模块的特征金字塔网络增强稀疏雷达信号与图像表示；提出几何引导渐进融合机制，以两阶段查询方式融合多视图特征。

Result: 在K-Radar基准测试中达到最优性能，整体场景提升约2.4%，雨雪场景提升1.6%。

Conclusion: WRCFormer通过有效融合原始雷达与相机数据，显著提升感知能力，尤其在恶劣天气下具有鲁棒性。

Abstract: 4D millimeter-wave (mmWave) radar has been widely adopted in autonomous driving and robot perception due to its low cost and all-weather robustness. However, its inherent sparsity and limited semantic richness significantly constrain perception capability. Recently, fusing camera data with 4D radar has emerged as a promising cost effective solution, by exploiting the complementary strengths of the two modalities. Nevertheless, point-cloud-based radar often suffer from information loss introduced by multi-stage signal processing, while directly utilizing raw 4D radar data incurs prohibitive computational costs. To address these challenges, we propose WRCFormer, a novel 3D object detection framework that fuses raw radar cubes with camera inputs via multi-view representations of the decoupled radar cube. Specifically, we design a Wavelet Attention Module as the basic module of wavelet-based Feature Pyramid Network (FPN) to enhance the representation of sparse radar signals and image data. We further introduce a two-stage query-based, modality-agnostic fusion mechanism termed Geometry-guided Progressive Fusion to efficiently integrate multi-view features from both modalities. Extensive experiments demonstrate that WRCFormer achieves state-of-the-art performance on the K-Radar benchmarks, surpassing the best model by approximately 2.4% in all scenarios and 1.6% in the sleet scenario, highlighting its robustness under adverse weather conditions.

</details>


### [167] [ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2512.22939)
*Qihang Peng,Xuesong Chen,Chenye Yang,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出ColaVLA框架，通过将视觉语言模型的推理能力迁移到统一潜在空间，结合分层并行轨迹解码器，解决自动驾驶中VLM规划器的离散-连续不匹配、高延迟和低效率问题，在nuScenes基准测试中实现最优性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型（VLM）的自动驾驶规划器存在三个关键挑战：离散文本推理与连续控制之间的不匹配、自回归思维链解码导致的高延迟、以及低效或非因果的规划器限制实时部署。

Method: 提出ColaVLA统一视觉-语言-动作框架，包含认知潜在推理器（通过自适应选择和两次VLM前向传递将场景理解压缩为决策导向的元动作嵌入）和分层并行规划器（单次前向传递生成多尺度、因果一致的轨迹）。

Result: 在nuScenes基准测试中，ColaVLA在开环和闭环设置下均达到最先进的性能，同时具备良好的效率和鲁棒性。

Conclusion: ColaVLA框架在保持VLM泛化能力和可解释性的同时，实现了高效、准确和安全的轨迹生成，为自动驾驶规划提供了新的解决方案。

Abstract: Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.

</details>


### [168] [Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance](https://arxiv.org/abs/2512.22881)
*Haosen Li,Wenshuo Chen,Shaofeng Liang,Lei Wang,Haozhe Jia,Yutao Yue*

Main category: cs.CV

TL;DR: 本文提出了一种名为引导路径采样（GPS）的新方法，用于解决扩散模型迭代细化过程中，标准无分类器引导（CFG）导致采样路径偏离数据流形、误差发散的问题。GPS通过流形约束插值替代不稳定的外推，确保采样路径稳定，并在多个现代骨干模型上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于去噪-逆循环的迭代细化方法在与标准无分类器引导（CFG）结合时效果受限，因为CFG的外推特性会系统性地将采样路径推离数据流形，导致近似误差发散，破坏细化过程。

Method: 提出引导路径采样（GPS）框架，用基于流形约束的插值替代不稳定的外推，确保采样路径始终保持在数据流形上；理论上证明了该方法能将误差序列从无界放大转变为严格有界；设计了动态调整引导强度的最优调度策略，使语义注入与模型从粗到细的生成过程对齐。

Result: 在SDXL和Hunyuan-DiT等现代骨干模型上的实验表明，GPS在感知质量和复杂提示遵循方面均优于现有方法。例如，在SDXL上实现了0.79的ImageReward和0.2995的HPS v2分数，并将GenEval的整体语义对齐准确率提升至57.45%。

Conclusion: 路径稳定性是有效迭代细化的前提条件，GPS提供了一个稳健的框架来实现这一目标，通过约束采样路径于数据流形，解决了CFG导致的不稳定问题，显著提升了生成质量与语义对齐能力。

Abstract: Iterative refinement methods based on a denoising-inversion cycle are powerful tools for enhancing the quality and control of diffusion models. However, their effectiveness is critically limited when combined with standard Classifier-Free Guidance (CFG). We identify a fundamental limitation: CFG's extrapolative nature systematically pushes the sampling path off the data manifold, causing the approximation error to diverge and undermining the refinement process. To address this, we propose Guided Path Sampling (GPS), a new paradigm for iterative refinement. GPS replaces unstable extrapolation with a principled, manifold-constrained interpolation, ensuring the sampling path remains on the data manifold. We theoretically prove that this correction transforms the error series from unbounded amplification to strictly bounded, guaranteeing stability. Furthermore, we devise an optimal scheduling strategy that dynamically adjusts guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process. Extensive experiments on modern backbones like SDXL and Hunyuan-DiT show that GPS outperforms existing methods in both perceptual quality and complex prompt adherence. For instance, GPS achieves a superior ImageReward of 0.79 and HPS v2 of 0.2995 on SDXL, while improving overall semantic alignment accuracy on GenEval to 57.45%. Our work establishes that path stability is a prerequisite for effective iterative refinement, and GPS provides a robust framework to achieve it.

</details>


### [169] [YOLO-IOD: Towards Real Time Incremental Object Detection](https://arxiv.org/abs/2512.22973)
*Shizhou Zhang,Xueqiang Lv,Yinghui Xing,Qirui Wu,Di Xu,Chen Zhao,Yanning Zhang*

Main category: cs.CV

TL;DR: 本文提出了YOLO-IOD，首个基于YOLO的实时增量目标检测框架，解决了YOLO在增量学习中存在的三类知识冲突问题，并在新基准上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有增量目标检测方法主要基于Faster R-CNN或DETR，无法适配实时性强的YOLO检测框架，且存在灾难性遗忘问题。

Method: 基于预训练YOLO-World构建，采用分阶段参数高效微调。包含三个核心组件：冲突感知伪标签细化（CPR）缓解前景-背景混淆；基于重要性的核选择（IKS）识别并更新关键卷积核；跨阶段非对称知识蒸馏（CAKD）解决知识蒸馏错位问题。

Result: 在传统基准和新提出的LoCo COCO基准上，YOLO-IOD均实现了优越性能，遗忘最小化。

Conclusion: YOLO-IOD成功将增量学习能力引入YOLO框架，解决了三类关键知识冲突，为实时增量目标检测提供了有效解决方案。

Abstract: Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.

</details>


### [170] [RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance](https://arxiv.org/abs/2512.22974)
*Chunyuan Chen,Yunuo Cai,Shujuan Li,Weiyun Liang,Bin Wang,Jing Xu*

Main category: cs.CV

TL;DR: 提出ReamCamo框架，通过布局控制和多模态条件生成更真实的伪装图像，解决现有方法在视觉相似性和语义一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有伪装图像生成方法存在明显缺陷：生成的图像要么因视觉相似性不足而缺乏有效伪装，要么背景杂乱且与前景目标语义不一致，导致与真实伪装图像存在较大差距。

Method: 提出统一的外绘框架ReamCamo，引入布局控制调节全局图像结构以提升语义一致性；构建结合细粒度文本任务描述和纹理导向背景检索的多模态条件，联合指导生成过程；提出背景-前景分布差异度量评估伪装质量。

Result: 大量实验和可视化结果表明，所提框架能有效生成视觉保真度高、语义一致的伪装图像，显著提升了生成质量。

Conclusion: ReamCamo框架通过布局控制和多模态条件生成，在视觉相似性和语义一致性方面优于现有方法，为伪装目标检测提供了更高质量的训练数据生成方案。

Abstract: Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose ReamCamo, a unified out-painting based framework for realistic camouflaged image generation. ReamCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multi-modal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.

</details>


### [171] [PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects](https://arxiv.org/abs/2512.22979)
*Huiming Yang,Linglin Liao,Fei Ding,Sibo Wang,Zijian Zeng*

Main category: cs.CV

TL;DR: 提出了PoseStreamer框架，用于高速运动场景下的6DoF姿态估计，结合事件相机与RGB数据，通过时间一致性、2D跟踪先验和几何优化提升性能，并构建了MoCapCube6D数据集进行验证。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在高速、低光场景下易产生运动模糊，导致6DoF姿态估计性能下降；事件相机虽具高时间分辨率优势，但现有方法在高速物体运动时仍表现不佳。

Method: 提出多模态框架PoseStreamer，包含自适应姿态记忆队列（利用历史方向信息保持时间一致性）、以物体为中心的2D跟踪器（提供2D先验提升3D中心召回）、射线姿态滤波器（沿相机射线进行几何优化），并构建MoCapCube6D多模态数据集。

Result: 实验表明PoseStreamer在高速运动场景下达到更高精度，且作为无需模板的框架对未见过的运动物体具有强泛化能力。

Conclusion: PoseStreamer通过多模态融合与时序优化，有效解决了高速运动下的6DoF姿态估计难题，为动态场景感知提供了鲁棒解决方案。

Abstract: Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.

</details>


### [172] [Reverse Personalization](https://arxiv.org/abs/2512.22984)
*Han-Wei Kung,Tuomas Varanka,Nicu Sebe*

Main category: cs.CV

TL;DR: 提出一种无需文本提示的反向个性化框架，用于面部匿名化，通过条件扩散反演和身份引导分支实现身份移除与属性控制。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的身份特征移除方法依赖预训练模型对主体的良好表示或需针对特定身份微调模型，缺乏对匿名化后面部属性的控制能力。

Method: 采用条件扩散反演技术直接操作图像，结合身份引导条件分支以泛化至训练数据外的主体，支持属性可控的匿名化。

Result: 该方法在身份移除、属性保留和图像质量之间达到最先进的平衡，优于现有匿名化方法。

Conclusion: 提出的反向个性化框架有效解决了面部匿名化中的身份控制与属性保留问题，为无需文本提示的图像编辑提供了新思路。

Abstract: Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .

</details>


### [173] [A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection](https://arxiv.org/abs/2512.22990)
*Soham Dutta,Soham Banerjee,Sneha Mahata,Anindya Sen,Sayantani Datta*

Main category: cs.CV

TL;DR: 提出一种基于低成本RGB无人机的果园智能检测系统，集成病害识别、新鲜度评估和果实检测功能，实现离线实时处理。


<details>
  <summary>Details</summary>
Motivation: 现有无人机系统多依赖昂贵多光谱传感器且功能孤立，需要低成本、一体化的果园监测方案。

Method: 使用ResNet50进行叶片病害分类，VGG16评估苹果新鲜度，YOLOv8实现实时苹果检测与定位，系统部署于ESP32-CAM和树莓派实现离线推理。

Result: 病害分类准确率98.9%，新鲜度分类准确率97.4%，苹果检测F1分数0.857。

Conclusion: 该框架为多光谱无人机方案提供了低成本、可扩展的替代方案，支持在廉价硬件上实现精准农业应用。

Abstract: Apple orchards require timely disease detection, fruit quality assessment, and yield estimation, yet existing UAV-based systems address such tasks in isolation and often rely on costly multispectral sensors. This paper presents a unified, low-cost RGB-only UAV-based orchard intelligent pipeline integrating ResNet50 for leaf disease detection, VGG 16 for apple freshness determination, and YOLOv8 for real-time apple detection and localization. The system runs on an ESP32-CAM and Raspberry Pi, providing fully offline on-site inference without cloud support. Experiments demonstrate 98.9% accuracy for leaf disease classification, 97.4% accuracy for freshness classification, and 0.857 F1 score for apple detection. The framework provides an accessible and scalable alternative to multispectral UAV solutions, supporting practical precision agriculture on affordable hardware.

</details>


### [174] [Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation](https://arxiv.org/abs/2512.22981)
*Linglin Liao,Qichuan Geng,Yu Liu*

Main category: cs.CV

TL;DR: 提出空间感知对称对齐框架，通过对称最优传输对齐和复合方向引导策略，解决医学图像分割中混合文本（位置、描述、诊断）处理难题，实现精准病灶定位。


<details>
  <summary>Details</summary>
Motivation: 现有文本引导医学图像分割方法难以同时处理诊断与描述文本，且缺乏空间约束能力，导致病灶定位偏差（如文本提示'左下肺'时误覆盖双侧肺区）。

Method: 1. 对称最优传输对齐机制：建立图像区域与多类型文本表达的双向细粒度对应；2. 复合方向引导策略：通过构建区域级引导掩码显式引入文本空间约束。

Result: 在公开基准测试中达到最先进性能，尤其在具有空间关系约束的病灶分割任务中表现优异。

Conclusion: SSA框架能有效整合位置、描述和诊断混合文本信息，通过增强多模态对齐与空间约束，显著提升医学图像分割的准确性。

Abstract: Text-guided Medical Image Segmentation has shown considerable promise for medical image segmentation, with rich clinical text serving as an effective supplement for scarce data. However, current methods have two key bottlenecks. On one hand, they struggle to process diagnostic and descriptive texts simultaneously, making it difficult to identify lesions and establish associations with image regions. On the other hand, existing approaches focus on lesions description and fail to capture positional constraints, leading to critical deviations. Specifically, with the text "in the left lower lung", the segmentation results may incorrectly cover both sides of the lung. To address the limitations, we propose the Spatial-aware Symmetric Alignment (SSA) framework to enhance the capacity of referring hybrid medical texts consisting of locational, descriptive, and diagnostic information. Specifically, we propose symmetric optimal transport alignment mechanism to strengthen the associations between image regions and multiple relevant expressions, which establishes bi-directional fine-grained multimodal correspondences. In addition, we devise a composite directional guidance strategy that explicitly introduces spatial constraints in the text by constructing region-level guidance masks. Extensive experiments on public benchmarks demonstrate that SSA achieves state-of-the-art (SOTA) performance, particularly in accurately segmenting lesions characterized by spatial relational constraints.

</details>


### [175] [Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion](https://arxiv.org/abs/2512.23035)
*Yi Zhou,Xuechao Zou,Shun Zhang,Kai Li,Shiying Wang,Jingming Chen,Congyan Lang,Tengfei Cao,Pin Tao,Yuanchun Shi*

Main category: cs.CV

TL;DR: 提出Co2S框架，通过融合视觉语言模型和自监督模型的先验知识，解决半监督遥感图像分割中的伪标签漂移问题。


<details>
  <summary>Details</summary>
Motivation: 半监督遥感图像分割可减轻标注负担，但存在伪标签漂移问题（确认偏差导致训练中误差累积），需要更稳定的解决方案。

Method: 1. 构建异构双学生架构，分别用CLIP和DINOv3预训练的ViT模型初始化；2. 设计显隐式语义协同引导机制，结合文本嵌入（显式）和可学习查询（隐式）；3. 提出全局-局部特征协同融合策略，整合CLIP的全局上下文与DINOv3的局部细节。

Result: 在六个主流数据集上的实验表明，该方法在不同数据划分协议和多样场景下均取得领先性能。

Conclusion: Co2S通过协同融合多源先验知识，有效缓解伪标签漂移，提升了半监督遥感图像分割的稳定性和精度。

Abstract: Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.

</details>


### [176] [3D sans 3D Scans: Scalable Pre-training from Video-Generated Point Clouds](https://arxiv.org/abs/2512.23042)
*Ryousuke Yamada,Kohsuke Ide,Yoshihiro Fukuhara,Hirokatsu Kataoka,Gilles Puy,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: 提出LAM3C框架，利用无标签视频生成点云进行3D自监督学习，无需真实3D传感器数据，在室内分割任务上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 大规模3D场景扫描成本高昂且费时，研究能否从无真实3D传感器的无标签视频中学习3D表示。

Method: 1. 构建RoomTours数据集（从网络收集房间漫游视频，使用现成重建模型生成49,219个场景点云）；2. 提出LAM3C框架，包含拉普拉斯感知多级3D聚类和噪声正则化损失（通过局部几何平滑和噪声鲁棒性稳定特征学习）。

Result: LAM3C在室内语义分割和实例分割任务上，性能优于先前自监督方法，且未使用任何真实3D扫描数据。

Conclusion: 无标签视频可作为3D自监督学习的丰富数据源，验证了从视频生成点云进行表示学习的可行性。

Abstract: Despite recent progress in 3D self-supervised learning, collecting large-scale 3D scene scans remains expensive and labor-intensive. In this work, we investigate whether 3D representations can be learned from unlabeled videos recorded without any real 3D sensors. We present Laplacian-Aware Multi-level 3D Clustering with Sinkhorn-Knopp (LAM3C), a self-supervised framework that learns from video-generated point clouds from unlabeled videos. We first introduce RoomTours, a video-generated point cloud dataset constructed by collecting room-walkthrough videos from the web (e.g., real-estate tours) and generating 49,219 scenes using an off-the-shelf feed-forward reconstruction model. We also propose a noise-regularized loss that stabilizes representation learning by enforcing local geometric smoothness and ensuring feature stability under noisy point clouds. Remarkably, without using any real 3D scans, LAM3C achieves higher performance than the previous self-supervised methods on indoor semantic and instance segmentation. These results suggest that unlabeled videos represent an abundant source of data for 3D self-supervised learning.

</details>


### [177] [With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs](https://arxiv.org/abs/2512.23024)
*Ciprian Constantinescu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出了一种基于地理语义上下文图（GSCG）的物体分类框架，通过显式建模物体间的空间关系、材质属性和场景上下文，显著提升了分类准确率，并增强了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 人类识别物体时能充分利用丰富的场景上下文信息（如空间关系、材质、物体共现），而传统计算模型通常孤立处理图像区域，忽略了这些关键信息，导致性能受限。

Method: 1. 从单目图像构建Geo-Semantic Contextual Graph（GSCG），结合深度估计与全景材质分割模型；2. 以物体为节点（含几何、颜色、材质属性），空间关系为边；3. 设计图分类器，聚合目标物体、邻近物体及全局场景特征进行分类。

Result: 1. 上下文感知模型在COCO 2017数据集上达到73.4%分类准确率，远超无视上下文的版本（最低38.4%）；2. 显著优于微调ResNet模型（最高53.5%）和先进多模态大语言模型Llama 4 Scout（最高42.3%）。

Conclusion: 显式结构化且可解释的上下文建模能极大提升物体识别性能，验证了场景上下文在计算视觉任务中的关键作用，并为可解释AI提供了新思路。

Abstract: Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model's reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.

</details>


### [178] [Video-BrowseComp: Benchmarking Agentic Video Research on Open Web](https://arxiv.org/abs/2512.23044)
*Zhengyang Liang,Yan Shu,Xiangrui Liu,Minghao Qin,Kaixin Liang,Paolo Rota,Nicu Sebe,Zheng Liu,Lizi Liao*

Main category: cs.CV

TL;DR: 本文提出了Video-BrowseComp基准测试，用于评估智能体在开放网络中进行视频推理的能力，填补了现有基准测试在动态视频模态研究上的空白。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试主要关注被动感知，无法评估智能体在开放网络中进行主动视频研究的能力，如跨时间线检索、分散证据整合和网络验证等任务。

Method: 构建包含210个问题的Video-BrowseComp基准测试，强制要求依赖时间视觉证据，确保答案不能仅通过文本搜索获得，必须通过导航视频时间线来验证外部信息。

Result: 评估显示最先进的搜索增强模型（如GPT-5.1 w/ Search）准确率仅为15.24%，模型在元数据丰富的领域表现良好，但在需要视觉基础的动态环境中表现不佳。

Conclusion: Video-BrowseComp作为首个开放网络视频研究基准测试，推动了视频理解从被动感知向主动推理的转变，揭示了当前模型在视觉基础推理方面的瓶颈。

Abstract: The evolution of autonomous agents is redefining information seeking, transitioning from passive retrieval to proactive, open-ended web research. However, while textual and static multimodal agents have seen rapid progress, a significant modality gap remains in processing the web's most dynamic modality: video. Existing video benchmarks predominantly focus on passive perception, feeding curated clips to models without requiring external retrieval. They fail to evaluate agentic video research, which necessitates actively interrogating video timelines, cross-referencing dispersed evidence, and verifying claims against the open web. To bridge this gap, we present \textbf{Video-BrowseComp}, a challenging benchmark comprising 210 questions tailored for open-web agentic video reasoning. Unlike prior benchmarks, Video-BrowseComp enforces a mandatory dependency on temporal visual evidence, ensuring that answers cannot be derived solely through text search but require navigating video timelines to verify external claims. Our evaluation of state-of-the-art models reveals a critical bottleneck: even advanced search-augmented models like GPT-5.1 (w/ Search) achieve only 15.24\% accuracy. Our analysis reveals that these models largely rely on textual proxies, excelling in metadata-rich domains (e.g., TV shows with plot summaries) but collapsing in metadata-sparse, dynamic environments (e.g., sports, gameplay) where visual grounding is essential. As the first open-web video research benchmark, Video-BrowseComp advances the field beyond passive perception toward proactive video reasoning.

</details>


### [179] [GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2512.23147)
*Jingyu Li,Xiaolong Zhao,Zhe Liu,Wenxiao Wu,Li Zhang*

Main category: cs.CV

TL;DR: 提出GeoTeacher方法，通过几何关系监督模块和体素数据增强策略，提升半监督3D目标检测中模型对几何信息的捕获能力，在ONCE和Waymo数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有半监督3D目标检测方法多关注伪标签质量或特征一致性，但忽视了在标注数据有限时模型对物体几何信息敏感度不足的问题，而几何信息对目标感知和定位至关重要。

Method: 1. 设计基于关键点的几何关系监督模块，将教师模型对物体几何的知识迁移给学生模型；2. 提出体素级数据增强策略，增加物体几何多样性，并引入距离衰减机制保护远距离物体的完整性；3. 方法可与不同SS3D方法结合使用。

Result: 在ONCE和Waymo数据集上的大量实验表明，该方法能有效提升学生模型对几何关系的理解能力，取得了新的最先进结果，并展现出良好的泛化性能。

Conclusion: GeoTeacher通过显式监督几何关系和增强几何多样性，有效解决了半监督3D目标检测中几何信息捕获不足的问题，为提升模型性能提供了新思路。

Abstract: Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model's ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model's ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student, thereby improving the student's capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model's ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher

</details>


### [180] [Domain-Shift Immunity in Deep Deformable Registration via Local Feature Representations](https://arxiv.org/abs/2512.23142)
*Mingzhen Shao,Sarang Joshi*

Main category: cs.CV

TL;DR: 本文提出UniReg通用配准框架，证明基于深度学习的可变形图像配准模型具有固有的域偏移免疫性，其鲁棒性源于对局部特征而非全局外观的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为基于学习的配准模型对域偏移敏感，需要大量多样化训练数据，但缺乏对其内在机制的解释。本研究旨在探究深度学习配准模型鲁棒性的本质来源。

Method: 提出UniReg框架，将特征提取与形变估计解耦：使用固定的预训练特征提取器提取局部特征，结合UNet形变网络进行配准。仅使用单一数据集训练，测试其在跨域和多模态场景下的性能。

Result: UniReg在跨域和多模态配准中表现出与优化方法相当的鲁棒性。分析发现传统CNN模型在模态偏移下的失败源于早期卷积层的数据集偏差，而局部特征一致性是鲁棒性的关键。

Conclusion: 局部特征一致性是基于学习的可变形配准模型具有域偏移免疫性的根本原因，这为设计保持域不变局部特征的网络架构提供了理论依据。

Abstract: Deep learning has advanced deformable image registration, surpassing traditional optimization-based methods in both accuracy and efficiency. However, learning-based models are widely believed to be sensitive to domain shift, with robustness typically pursued through large and diverse training datasets, without explaining the underlying mechanisms. In this work, we show that domain-shift immunity is an inherent property of deep deformable registration models, arising from their reliance on local feature representations rather than global appearance for deformation estimation. To isolate and validate this mechanism, we introduce UniReg, a universal registration framework that decouples feature extraction from deformation estimation using fixed, pre-trained feature extractors and a UNet-based deformation network. Despite training on a single dataset, UniReg exhibits robust cross-domain and multi-modal performance comparable to optimization-based methods. Our analysis further reveals that failures of conventional CNN-based models under modality shift originate from dataset-induced biases in early convolutional layers. These findings identify local feature consistency as the key driver of robustness in learning-based deformable registration and motivate backbone designs that preserve domain-invariant local features.

</details>


### [181] [REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation](https://arxiv.org/abs/2512.23169)
*Fulin Shi,Wenyi Xiao,Bin Chen,Liang Din,Leilei Gan*

Main category: cs.CV

TL;DR: 提出REVEALER框架，通过强化学习引导的视觉推理进行细粒度图文对齐评估，在多个基准测试中达到最优性能


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成模型的评估方法多依赖粗粒度指标或静态问答流程，缺乏细粒度可解释性且难以反映人类偏好

Method: 采用'定位-推理-结论'结构化范式，基于多模态大语言模型显式定位语义元素；使用GRPO优化，结合结构格式、定位准确性和对齐保真度的复合奖励函数

Result: 在EvalMuse-40K等四个基准测试中实现最优性能，超越现有专有模型和监督基线，推理效率优于迭代视觉推理方法

Conclusion: REVEALER框架能有效评估图文细粒度对齐，提供可解释判断，在性能和效率上均表现优异

Abstract: Evaluating the alignment between textual prompts and generated images is critical for ensuring the reliability and usability of text-to-image (T2I) models. However, most existing evaluation methods rely on coarse-grained metrics or static QA pipelines, which lack fine-grained interpretability and struggle to reflect human preferences. To address this, we propose REVEALER, a unified framework for element-level alignment evaluation based on reinforcement-guided visual reasoning. Adopting a structured "grounding-reasoning-conclusion" paradigm, our method enables Multimodal Large Language Models (MLLMs) to explicitly localize semantic elements and derive interpretable alignment judgments. We optimize the model via Group Relative Policy Optimization(GRPO) using a composite reward function that incorporates structural format, grounding accuracy, and alignment fidelity. Extensive experiments across four benchmarks-EvalMuse-40K, RichHF, MHaluBench, and GenAI-Bench-demonstrate that REVEALER achieves state-of-the-art performance. Our approach consistently outperforms both strong proprietary models and supervised baselines while demonstrating superior inference efficiency compared to existing iterative visual reasoning methods.

</details>


### [182] [GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation](https://arxiv.org/abs/2512.23180)
*Tianchen Deng,Xuefeng Chen,Yi Chen,Qu Chen,Yuyao Xu,Lijin Yang,Le Xu,Yu Zhang,Bo Zhang,Wuxiong Huang,Hesheng Wang*

Main category: cs.CV

TL;DR: 提出基于3D高斯场景表示的驾驶世界模型框架，实现3D场景理解和多模态生成，通过语言特征嵌入和任务感知采样提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶世界模型缺乏3D场景理解能力，无法解释驾驶环境，且3D空间表示与文本信息对齐不准确。

Method: 采用3D高斯场景表示，将语言特征嵌入高斯基元实现早期模态对齐；设计任务感知语言引导采样策略去除冗余高斯点；构建双条件多模态生成模型，结合高级语言条件和低级图像条件。

Result: 在nuScenes和NuInteract数据集上验证有效，达到最先进性能。

Conclusion: 所提框架统一了3D场景理解和多模态生成，通过3D高斯表示和语言对齐提升了驾驶世界模型的推理与生成能力。

Abstract: Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.

</details>


### [183] [GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection](https://arxiv.org/abs/2512.23176)
*Yi Zhang,Yi Wang,Lei Yao,Lap-Pui Chau*

Main category: cs.CV

TL;DR: 提出GVSynergy-Det框架，通过高斯-体素协同表示学习提升仅使用RGB图像的3D目标检测性能，无需深度或密集3D监督。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的3D检测方法面临两难：高精度方法需要密集3D监督，而无监督方法难以从图像中提取准确几何信息。

Method: 采用双表示架构：1）使用可泛化高斯溅射提取几何特征；2）设计跨表示增强机制，用高斯场几何细节丰富体素特征；通过可学习集成直接利用两种表示的特征。

Result: 在ScanNetV2和ARKitScenes数据集上取得最先进结果，显著优于现有方法，且无需深度或密集3D几何监督。

Conclusion: 高斯和体素表示具有互补性，协同利用能有效提升无监督图像3D检测性能，为低成本3D感知提供了新思路。

Abstract: Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).

</details>


### [184] [Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks](https://arxiv.org/abs/2512.23210)
*Changgyoon Oh,Jongoh Jeong,Jegyeong Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出了一种自适应选择扩散模型时间步特征的方法，通过任务感知时间步选择和特征整合模块，在少样本密集预测任务中提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型应用中，时间步特征的选择依赖经验直觉，往往导致次优性能且对特定任务存在偏差，需要更自适应的方法来提升少样本密集预测任务的泛化能力。

Method: 提出了两个核心模块：任务感知时间步选择（TTS）基于时间步损失和相似度评分选择理想时间步；时间步特征整合（TFC）整合选定特征以提升少样本密集预测性能，并配合参数高效微调适配器。

Result: 在Taskonomy数据集上的实验验证表明，该方法在少样本密集预测任务中实现了优越性能，特别是在通用和少样本学习场景下。

Conclusion: 通过自适应选择扩散时间步特征并进行有效整合，能够显著提升扩散模型在少样本密集预测任务中的性能，为实际应用提供了更优的解决方案。

Abstract: Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.

</details>


### [185] [MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?](https://arxiv.org/abs/2512.23219)
*Shiqi Dai,Zizhi Ma,Zhicong Luo,Xuesong Yang,Yibin Huang,Wanyue Zhang,Chi Chen,Zonghao Guo,Wang Xu,Yufei Sun,Maosong Sun*

Main category: cs.CV

TL;DR: 本文提出了MM-UAVBench基准，用于系统评估多模态大语言模型在低空无人机场景下的感知、认知和规划能力，发现现有模型难以适应此类复杂场景。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在无人机主导的低空应用场景中潜力未充分探索，现有基准缺乏针对低空场景独特挑战的评估，且无人机相关评估多局限于特定任务，缺乏对模型通用智能的统一评估。

Method: 构建包含19个子任务、超过5.7K人工标注问题的MM-UAVBench基准，数据源自公开无人机数据集；对16个开源和专有MLLMs进行广泛实验，分析其在低空场景中的表现瓶颈。

Result: 实验表明当前模型难以适应低空场景的复杂视觉和认知需求；分析揭示了空间偏差和多视角理解等关键瓶颈，阻碍了MLLMs在无人机场景中的有效部署。

Conclusion: MM-UAVBench基准有助于推动面向真实世界无人机智能的鲁棒可靠MLLMs研究，为解决低空场景中的模型适应性问题提供评估基础。

Abstract: While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.

</details>


### [186] [SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems](https://arxiv.org/abs/2512.23232)
*Minwoo Kim,Hongki Lim*

Main category: cs.CV

TL;DR: 提出SURE引导的后验采样方法，通过SURE梯度更新和PCA噪声估计校正采样轨迹偏差，在少于100次神经网络评估下实现高质量逆问题重建。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型求解逆问题的迭代方法需要数百上千步才能获得高质量重建，存在误差累积问题，需要更高效的采样方法。

Method: SURE引导后验采样，结合Stein无偏风险估计梯度更新和主成分分析噪声估计，在采样早期和中期阶段校正噪声引起的偏差。

Result: 在多种逆问题上的评估表明，该方法在低神经网络评估次数下始终优于现有方法，保持高质量重建。

Conclusion: SGPS方法通过减少误差累积，显著提高了扩散模型在逆问题中的采样效率，实现了更快速的高质量重建。

Abstract: Diffusion models have emerged as powerful learned priors for solving inverse problems. However, current iterative solving approaches which alternate between diffusion sampling and data consistency steps typically require hundreds or thousands of steps to achieve high quality reconstruction due to accumulated errors. We address this challenge with SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations using Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA based noise estimation. By mitigating noise induced errors during the critical early and middle sampling stages, SGPS enables more accurate posterior sampling and reduces error accumulation. This allows our method to maintain high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs). Our extensive evaluation across diverse inverse problems demonstrates that SGPS consistently outperforms existing methods at low NFE counts.

</details>


### [187] [AVOID: The Adverse Visual Conditions Dataset with Obstacles for Driving Scene Understanding](https://arxiv.org/abs/2512.23215)
*Jongoh Jeong,Taek-Jin Song,Jong-Hwan Kim,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 本文介绍了AVOID数据集，这是一个在模拟环境中收集的用于实时障碍物检测的新数据集，包含各种天气和时间条件下的意外道路障碍物，并提供了多种感知任务的标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有道路驾驶数据集通常只包含正常或单一恶劣场景的图像，且缺乏与其他类别在同一视觉域中捕获的道路障碍物数据，这限制了在多变恶劣条件下可靠检测意外小障碍物的能力。

Method: 创建了AVOID数据集，包含模拟环境中各种天气和时间条件下捕获的意外道路障碍物图像，并提供语义分割图、深度图、原始和语义LiDAR数据以及路径点等多种标注。使用高性能实时网络进行障碍物检测基准测试，并提出并进行了多任务网络（语义分割、深度和路径点预测）的消融研究。

Result: AVOID数据集支持大多数视觉感知任务，为在多变恶劣条件下实时检测意外道路障碍物提供了全面的数据资源。基准测试和消融研究展示了该数据集在评估和改进实时感知网络方面的实用性。

Conclusion: AVOID数据集填补了现有数据集的空白，为在恶劣条件下开发可靠的实时障碍物检测系统提供了重要资源，并通过多任务学习框架展示了其在提升自动驾驶视觉感知能力方面的潜力。

Abstract: Understanding road scenes for visual perception remains crucial for intelligent self-driving cars. In particular, it is desirable to detect unexpected small road hazards reliably in real-time, especially under varying adverse conditions (e.g., weather and daylight). However, existing road driving datasets provide large-scale images acquired in either normal or adverse scenarios only, and often do not contain the road obstacles captured in the same visual domain as for the other classes. To address this, we introduce a new dataset called AVOID, the Adverse Visual Conditions Dataset, for real-time obstacle detection collected in a simulated environment. AVOID consists of a large set of unexpected road obstacles located along each path captured under various weather and time conditions. Each image is coupled with the corresponding semantic and depth maps, raw and semantic LiDAR data, and waypoints, thereby supporting most visual perception tasks. We benchmark the results on high-performing real-time networks for the obstacle detection task, and also propose and conduct ablation studies using a comprehensive multi-task network for semantic segmentation, depth and waypoint prediction tasks.

</details>


### [188] [Bridging Your Imagination with Audio-Video Generation via a Unified Director](https://arxiv.org/abs/2512.23222)
*Jiaxu Zhang,Tianshu Hu,Yuan Zhang,Zenan Li,Linjie Luo,Guosheng Lin,Xin Chen*

Main category: cs.CV

TL;DR: 提出UniMAGE统一导演模型，将剧本创作与关键帧设计整合到单一框架中，通过混合Transformer架构和'先交织后解耦'训练范式，实现逻辑连贯的视频脚本和视觉一致的关键帧生成。


<details>
  <summary>Details</summary>
Motivation: 现有AI视频创作系统将剧本起草与关键镜头设计视为两个独立任务（分别依赖大语言模型和图像生成模型），但电影导演需要同时具备逻辑推理与想象力，因此需要统一框架以赋能非专业用户创作多镜头长视频。

Method: 1. 采用混合Transformer架构统一文本与图像生成；2. 提出'先交织后解耦'训练范式：先通过交织文本-图像数据进行交织概念学习，再通过解耦专家学习分离剧本写作与关键帧生成任务。

Result: 实验表明UniMAGE在开源模型中达到最先进性能，能生成逻辑连贯的视频脚本和视觉一致的关键帧图像。

Conclusion: 统一剧本与关键帧生成的任务是可行且有效的，所提方法能提升叙事逻辑与关键帧一致性，为非专业用户创作高质量多镜头视频提供支持。

Abstract: Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.

</details>


### [189] [RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models](https://arxiv.org/abs/2512.23239)
*Fan Wei,Runmin Dong,Yushan Lai,Yixiang Yang,Zhaoyang Luo,Jinxiao Zhang,Miao Yang,Shuai Yuan,Jiyao Zhao,Bin Luo,Haohuan Fu*

Main category: cs.CV

TL;DR: 提出一种无需训练的两阶段数据剪枝方法，用于提升遥感扩散基础模型的训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有遥感扩散基础模型依赖大量全局代表性数据，但数据中存在冗余、噪声和类别不平衡问题，降低了训练效率并阻碍模型收敛。

Method: 1. 基于信息熵准则快速去除低信息量样本；2. 利用遥感场景分类数据集作为参考，进行场景感知聚类与分层采样，在保持多样性和代表性的同时实现高剪枝率下的细粒度选择。

Result: 即使剪枝85%的训练数据，该方法仍能显著提升模型收敛速度和生成质量，并在超分辨率和语义图像合成等下游任务中达到最先进性能。

Conclusion: 该数据剪枝范式为开发遥感生成式基础模型提供了实用指导，能够快速筛选高质量数据子集，支撑模型快速收敛并作为多功能骨干网络。

Abstract: Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.

</details>


### [190] [ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation](https://arxiv.org/abs/2512.23245)
*Shin seong Kim,Minjung Shin,Hyunin Cho,Youngjung Uh*

Main category: cs.CV

TL;DR: 提出ASemconsist框架，通过选择性文本嵌入修改和自适应特征共享策略，解决文本到图像生成中角色身份一致性与场景描述对齐的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在生成序列图像时，难以在保持角色身份一致性和满足每张图像提示对齐之间取得平衡。

Method: 1. 选择性文本嵌入修改实现显式语义控制；2. 基于FLUX填充嵌入分析，将其重新用作语义容器；3. 自适应特征共享策略，自动评估文本歧义并仅对模糊身份提示施加约束。

Result: 提出统一评估协议CQS，综合身份保持和文本对齐指标；框架实现最先进性能，有效克服先前方法的权衡问题。

Conclusion: ASemconsist框架通过语义控制策略和自适应约束机制，显著提升了多图像生成中角色身份一致性与提示对齐的平衡能力。

Abstract: Recent text-to-image diffusion models have significantly improved visual quality and text alignment. However, generating a sequence of images while preserving consistent character identity across diverse scene descriptions remains a challenging task. Existing methods often struggle with a trade-off between maintaining identity consistency and ensuring per-image prompt alignment. In this paper, we introduce a novel framework, ASemconsist, that addresses this challenge through selective text embedding modification, enabling explicit semantic control over character identity without sacrificing prompt alignment. Furthermore, based on our analysis of padding embeddings in FLUX, we propose a semantic control strategy that repurposes padding embeddings as semantic containers. Additionally, we introduce an adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to the ambiguous identity prompt. Finally, we propose a unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and per-image text alignment into a single comprehensive metric, explicitly capturing performance imbalances between the two metrics. Our framework achieves state-of-the-art performance, effectively overcoming prior trade-offs. Project page: https://minjung-s.github.io/asemconsist

</details>


### [191] [Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization](https://arxiv.org/abs/2512.23258)
*Tong Shao,Yusen Fu,Guoying Sun,Jingde Kong,Zhuotao Tian,Jingyong Su*

Main category: cs.CV

TL;DR: 提出CEM方法，通过累积误差最小化优化缓存策略，提升扩散变换器推理速度同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散变换器迭代去噪过程导致推理速度慢，现有缓存加速方法存在固定策略无法适应误差变化的局限性

Method: 提出CEM插件方法，预定义误差表征模型对加速的敏感性，设计动态规划算法优化缓存策略实现累积误差最小化

Result: 在9个生成模型和量化方法上验证，显著提升现有加速模型的生成保真度，在多个模型上超越原始生成性能

Conclusion: CEM是模型无关的通用加速插件，可无缝集成到现有框架，不增加计算开销，具有强泛化能力

Abstract: Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.

</details>


### [192] [Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism](https://arxiv.org/abs/2512.23243)
*Siyu Zhang,Ying Chen,Lianlei Shan,Runhe Qiu*

Main category: cs.CV

TL;DR: 本研究提出了一种集成动态分辨率输入策略（DRIS）和多尺度视觉语言对齐机制（MS-VLAM）的视觉语言模型（VLM）框架，旨在解决遥感图像多模态融合中固定分辨率效率与细节平衡不佳、单尺度对齐缺乏语义层次的问题，显著提升了图像描述和跨模态检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有遥感图像多模态融合方法存在固定分辨率无法平衡效率与细节、单尺度对齐缺乏语义层次等缺陷，限制了地表信息提取的精度，而该技术在环境监测、城市规划等领域具有重要应用价值。

Method: 提出视觉语言模型（VLM）框架，集成两个关键创新：1）动态分辨率输入策略（DRIS），采用由粗到细的方式自适应分配计算资源；2）多尺度视觉语言对齐机制（MS-VLAM），构建对象、局部区域和全局三个层次的对齐机制。

Result: 在RS-GPT4V数据集上的实验表明，该框架在图像描述任务（BLEU-4、CIDEr指标）和跨模态检索任务（R@10指标）上均优于传统方法，显著提升了语义理解精度和计算效率。

Conclusion: 该技术框架为构建高效、鲁棒的多模态遥感系统提供了新思路，为智能遥感解译的工程应用奠定了理论基础并提供了技术指导。

Abstract: Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.

</details>


### [193] [Contour Information Aware 2D Gaussian Splatting for Image Representation](https://arxiv.org/abs/2512.23255)
*Masaya Takabe,Hiroshi Watanabe,Sujun Hong,Tomohiro Ikai,Zheming Fan,Ryo Ishimoto,Kakeru Sugimoto,Ruri Imichi*

Main category: cs.CV

TL;DR: 提出了一种融合轮廓信息的2D高斯泼溅框架，通过引入对象分割先验来改善图像表示中的边缘质量，特别是在高斯数量较少时能保持清晰边界。


<details>
  <summary>Details</summary>
Motivation: 现有2D高斯泼溅方法在压缩率高（高斯数量少）时，由于缺乏轮廓感知能力，容易产生模糊或边界不清的问题，影响了图像表示的边缘质量。

Method: 1. 将对象分割先验融入高斯表示，约束每个高斯在特定分割区域内进行光栅化，防止跨边界混合；2. 引入预热训练方案以稳定训练过程并提升收敛性。

Result: 在合成色卡和DAVIS数据集上的实验表明，该方法相比现有2DGS方法在物体边缘区域实现了更高的重建质量，特别是在高斯数量极少时效果显著，同时保持了快速渲染和低内存占用的优势。

Conclusion: 通过结合分割先验和边界约束，所提出的轮廓感知2D高斯泼溅框架有效提升了压缩场景下的边缘保持能力，为轻量化图像表示提供了更优的解决方案。

Abstract: Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.

</details>


### [194] [YOLO-Master: MOE-Accelerated with Specialized Transformers for Enhanced Real-time Detection](https://arxiv.org/abs/2512.23273)
*Xu Lin,Jinlong Peng,Zhenye Gan,Jiawen Zhu,Jun Liu*

Main category: cs.CV

TL;DR: 提出YOLO-Master框架，通过实例条件自适应计算改进实时目标检测，动态分配计算资源以适应场景复杂度，在保持实时性的同时提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有实时目标检测方法采用静态密集计算，对所有输入进行均匀处理，导致计算资源分配不当——简单场景过度计算，复杂场景计算不足，造成计算冗余和性能欠佳。

Method: 引入高效稀疏专家混合（ES-MoE）模块，通过轻量级动态路由网络实现实例条件自适应计算，训练时通过多样性增强目标促进专家互补，推理时仅激活相关专家以最小化计算开销。

Result: 在五个大规模基准测试中表现优异，在MS COCO上达到42.4% AP和1.62ms延迟，优于YOLOv13-N（+0.8% mAP，推理速度提升17.8%），在密集场景中提升尤为显著。

Conclusion: YOLO-Master通过自适应计算机制有效解决了静态计算模型的资源错配问题，在保持实时推理速度的同时显著提升了检测性能，特别是在复杂场景中。

Abstract: Existing Real-Time Object Detection (RTOD) methods commonly adopt YOLO-like architectures for their favorable trade-off between accuracy and speed. However, these models rely on static dense computation that applies uniform processing to all inputs, misallocating representational capacity and computational resources such as over-allocating on trivial scenes while under-serving complex ones. This mismatch results in both computational redundancy and suboptimal detection performance. To overcome this limitation, we propose YOLO-Master, a novel YOLO-like framework that introduces instance-conditional adaptive computation for RTOD. This is achieved through a Efficient Sparse Mixture-of-Experts (ES-MoE) block that dynamically allocates computational resources to each input according to its scene complexity. At its core, a lightweight dynamic routing network guides expert specialization during training through a diversity enhancing objective, encouraging complementary expertise among experts. Additionally, the routing network adaptively learns to activate only the most relevant experts, thereby improving detection performance while minimizing computational overhead during inference. Comprehensive experiments on five large-scale benchmarks demonstrate the superiority of YOLO-Master. On MS COCO, our model achieves 42.4% AP with 1.62ms latency, outperforming YOLOv13-N by +0.8% mAP and 17.8% faster inference. Notably, the gains are most pronounced on challenging dense scenes, while the model preserves efficiency on typical inputs and maintains real-time inference speed. Code will be available.

</details>


### [195] [CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation](https://arxiv.org/abs/2512.23333)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Zhengtao Yao,Weitao Jia,Xiaodong Ge,Jingqun Tang,Benlei Cui,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 提出CME-CAD训练范式，通过多专家强化学习生成高精度可编辑CAD模型，并发布CADExpert开源基准数据集


<details>
  <summary>Details</summary>
Motivation: 传统CAD建模复杂，现有草图重建方法生成模型不可编辑且精度不足，文本/图像输入依赖人工标注，难以满足工业设计需求

Method: 提出异构协同多专家强化学习范式，包含多专家微调（MEFT）和多专家强化学习（MERL）两阶段训练，利用CADExpert基准数据集

Result: 开发了能生成准确、约束兼容、完全可编辑CAD模型的系统，提供包含17299个实例的开源数据集

Conclusion: CME-CAD范式有效解决了CAD模型生成的精度和可编辑性问题，为工业设计自动化提供了可行方案

Abstract: Computer-Aided Design (CAD) is essential in industrial design, but the complexity of traditional CAD modeling and workflows presents significant challenges for automating the generation of high-precision, editable CAD models. Existing methods that reconstruct 3D models from sketches often produce non-editable and approximate models that fall short of meeting the stringent requirements for precision and editability in industrial design. Moreover, the reliance on text or image-based inputs often requires significant manual annotation, limiting their scalability and applicability in industrial settings. To overcome these challenges, we propose the Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm, a novel training paradigm for CAD code generation. Our approach integrates the complementary strengths of these models, facilitating collaborative learning and improving the model's ability to generate accurate, constraint-compatible, and fully editable CAD models. We introduce a two-stage training process: Multi-Expert Fine-Tuning (MEFT), and Multi-Expert Reinforcement Learning (MERL). Additionally, we present CADExpert, an open-source benchmark consisting of 17,299 instances, including orthographic projections with precise dimension annotations, expert-generated Chain-of-Thought (CoT) processes, executable CADQuery code, and rendered 3D models.

</details>


### [196] [CountGD++: Generalized Prompting for Open-World Counting](https://arxiv.org/abs/2512.23351)
*Niki Amini-Naieni,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出CountGD++模型，通过扩展提示方式（支持指定不计数对象、自动标注视觉示例、接受自然与合成图像示例）提升开放世界物体计数的灵活性与准确性


<details>
  <summary>Details</summary>
Motivation: 现有物体计数方法存在局限：视觉示例需手动标注、无法指定不计数对象、示例来源受限，限制了方法的实用性和准确性

Method: 1) 扩展提示机制，支持用文本/视觉示例描述不计数对象；2) 提出'伪示例'概念，实现推理时视觉示例自动标注；3) 允许模型接受自然图像和合成外部图像的视觉示例；4) 将CountGD++作为视觉专家代理与LLM集成

Result: 在多个数据集上显著提升了计数准确性、效率和泛化能力，代码已开源

Conclusion: 通过扩展目标对象指定方式，CountGD++增强了多模态开放世界计数的提示灵活性，为实际应用提供了更强大的计数工具

Abstract: The flexibility and accuracy of methods for automatically counting objects in images and videos are limited by the way the object can be specified. While existing methods allow users to describe the target object with text and visual examples, the visual examples must be manually annotated inside the image, and there is no way to specify what not to count. To address these gaps, we introduce novel capabilities that expand how the target object can be specified. Specifically, we extend the prompt to enable what not to count to be described with text and/or visual examples, introduce the concept of `pseudo-exemplars' that automate the annotation of visual examples at inference, and extend counting models to accept visual examples from both natural and synthetic external images. We also use our new counting model, CountGD++, as a vision expert agent for an LLM. Together, these contributions expand the prompt flexibility of multi-modal open-world counting and lead to significant improvements in accuracy, efficiency, and generalization across multiple datasets. Code is available at https://github.com/niki-amini-naieni/CountGDPlusPlus.

</details>


### [197] [Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition](https://arxiv.org/abs/2512.23291)
*Arman Martirosyan,Shahane Tigranyan,Maria Razzhivina,Artak Aslanyan,Nazgul Salikhova,Ilya Makarov,Andrey Savchenko,Aram Avetisyan*

Main category: cs.CV

TL;DR: 提出两种多模态框架，分别用于微手势识别和行为情感预测，在iMiGUE数据集上验证了有效性，其中情感预测任务在MiGA 2025挑战赛中获第二名。


<details>
  <summary>Details</summary>
Motivation: 微手势识别和行为情感预测均需建模细微的人类行为，传统方法难以充分捕捉视频与骨骼姿态数据中的细粒度时空模式，因此需要探索多模态融合方法提升性能。

Method: 1. 微手势分类：分别使用MViTv2-S和2s-AGCN提取视频和骨骼姿态特征，通过跨模态令牌融合模块整合空间与姿态信息；2. 情感识别：使用SwinFace和MViTv2-S提取面部与上下文特征，通过InterFusion模块融合表情与身体姿态信息。

Result: 在iMiGUE数据集上的实验表明，所提方法在行为情感预测任务中表现出鲁棒性能与高准确率，并在MiGA 2025挑战赛中取得第二名。

Conclusion: 多模态融合能有效结合视觉与姿态数据的互补优势，提升微手势识别和情感预测的准确性，为细粒度人类行为分析提供了可行方案。

Abstract: Micro-gesture recognition and behavior-based emotion prediction are both highly challenging tasks that require modeling subtle, fine-grained human behaviors, primarily leveraging video and skeletal pose data. In this work, we present two multimodal frameworks designed to tackle both problems on the iMiGUE dataset. For micro-gesture classification, we explore the complementary strengths of RGB and 3D pose-based representations to capture nuanced spatio-temporal patterns. To comprehensively represent gestures, video, and skeletal embeddings are extracted using MViTv2-S and 2s-AGCN, respectively. Then, they are integrated through a Cross-Modal Token Fusion module to combine spatial and pose information. For emotion recognition, our framework extends to behavior-based emotion prediction, a binary classification task identifying emotional states based on visual cues. We leverage facial and contextual embeddings extracted using SwinFace and MViTv2-S models and fuse them through an InterFusion module designed to capture emotional expressions and body gestures. Experiments conducted on the iMiGUE dataset, within the scope of the MiGA 2025 Challenge, demonstrate the robust performance and accuracy of our method in the behavior-based emotion prediction task, where our approach secured 2nd place.

</details>


### [198] [Visual Language Hypothesis](https://arxiv.org/abs/2512.23335)
*Xiu Li*

Main category: cs.CV

TL;DR: 从拓扑结构视角研究视觉表示学习，提出视觉语义空间具有纤维丛结构，语义对应商空间而非子流形，需要非连续变换才能获得语义不变性。


<details>
  <summary>Details</summary>
Motivation: 视觉理解需要语义语言，许多感知观察对应少量离散语义状态。现有表示学习理论缺乏对视觉空间拓扑结构的系统性分析，需要从结构角度解释语义抽象的本质。

Method: 基于纤维丛拓扑结构理论分析，结合表示学习的可迁移性和抽象性假设，推导语义商空间的结构特性，并与判别式模型、多模态对齐等实证规律对比验证。

Result: 1) 语义商空间不是观测空间的子流形，无法通过光滑变形获得，需要标签监督、跨实例识别或多模态对齐等非连续语义等价关系；2) 近似商空间需要模型支持拓扑变化，即“扩展-坍缩”过程。

Conclusion: 该拓扑框架为大规模判别式和多模态模型提供了理论解释，与统计学习经典原则一致，强调语义抽象需要外部语义目标和能支持拓扑变化的表示机制。

Abstract: We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient $X/G$ is not a submanifold of $X$ and cannot be obtained through smooth deformation alone, semantic invariance requires a non-homeomorphic, discriminative target, for example, supervision via labels, cross instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand-and-snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.

</details>


### [199] [NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization](https://arxiv.org/abs/2512.23374)
*Yifei Li,Haoyuan He,Yu Zheng,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了NeXT-IMDL基准测试，系统评估图像篡改检测模型的泛化能力，揭示现有方法在多样化AI生成内容上的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 用户友好的图像编辑模型普及带来滥用风险，需要泛化性强、与时俱进的图像篡改检测方法。现有跨数据集评估方法掩盖了模型处理多样化AI生成内容时的脆弱性，造成进展假象。

Method: 构建NeXT-IMDL大规模诊断基准，从编辑模型、篡改类型、内容语义和伪造粒度四个维度分类AI生成内容篡改，并设计五种严格的跨维度评估协议。

Result: 对11个代表性模型的实验表明，这些模型在原始设定下表现良好，但在模拟真实世界多样化泛化场景的评估协议下出现系统性失败和显著性能下降。

Conclusion: NeXT-IMDL基准测试揭示了现有图像篡改检测模型的泛化局限性，为开发真正鲁棒的下一代检测模型提供了诊断工具和新发现。

Abstract: The accessibility surge and abuse risks of user-friendly image editing models have created an urgent need for generalizable, up-to-date methods for Image Manipulation Detection and Localization (IMDL). Current IMDL research typically uses cross-dataset evaluation, where models trained on one benchmark are tested on others. However, this simplified evaluation approach conceals the fragility of existing methods when handling diverse AI-generated content, leading to misleading impressions of progress. This paper challenges this illusion by proposing NeXT-IMDL, a large-scale diagnostic benchmark designed not just to collect data, but to probe the generalization boundaries of current detectors systematically. Specifically, NeXT-IMDL categorizes AIGC-based manipulations along four fundamental axes: editing models, manipulation types, content semantics, and forgery granularity. Built upon this, NeXT-IMDL implements five rigorous cross-dimension evaluation protocols. Our extensive experiments on 11 representative models reveal a critical insight: while these models perform well in their original settings, they exhibit systemic failures and significant performance degradation when evaluated under our designed protocols that simulate real-world, various generalization scenarios. By providing this diagnostic toolkit and the new findings, we aim to advance the development towards building truly robust, next-generation IMDL models.

</details>


### [200] [MGCA-Net: Multi-Graph Contextual Attention Network for Two-View Correspondence Learning](https://arxiv.org/abs/2512.23369)
*Shuyuan Lin,Mengtin Lo,Haosheng Chen,Yanjie Liang,Qiangqiang Wu*

Main category: cs.CV

TL;DR: 提出MGCA-Net网络，通过上下文几何注意力模块和跨阶段多图共识模块，提升两视图对应关系学习中的几何建模能力与跨阶段信息优化，在异常点剔除和相机姿态估计任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有两视图对应关系学习方法在局部几何建模和跨阶段信息优化方面存在局限，难以准确捕捉匹配对的几何约束，降低了模型鲁棒性。

Method: 提出多图上下文注意力网络（MGCA-Net），包含上下文几何注意力模块（CGA）和跨阶段多图共识模块（CSMGC）。CGA通过自适应注意力机制动态整合空间位置与特征信息；CSMGC通过跨阶段稀疏图网络建立几何共识。

Result: 在YFCC100M和SUN3D数据集上的实验表明，MGCA-Net在异常点剔除和相机姿态估计任务上显著优于现有SOTA方法。

Conclusion: MGCA-Net通过增强几何关系捕捉和跨阶段一致性，有效提升了两视图对应关系学习的性能，代码已开源。

Abstract: Two-view correspondence learning is a key task in computer vision, which aims to establish reliable matching relationships for applications such as camera pose estimation and 3D reconstruction. However, existing methods have limitations in local geometric modeling and cross-stage information optimization, which make it difficult to accurately capture the geometric constraints of matched pairs and thus reduce the robustness of the model. To address these challenges, we propose a Multi-Graph Contextual Attention Network (MGCA-Net), which consists of a Contextual Geometric Attention (CGA) module and a Cross-Stage Multi-Graph Consensus (CSMGC) module. Specifically, CGA dynamically integrates spatial position and feature information via an adaptive attention mechanism and enhances the capability to capture both local and global geometric relationships. Meanwhile, CSMGC establishes geometric consensus via a cross-stage sparse graph network, ensuring the consistency of geometric information across different stages. Experimental results on two representative YFCC100M and SUN3D datasets show that MGCA-Net significantly outperforms existing SOTA methods in the outlier rejection and camera pose estimation tasks. Source code is available at http://www.linshuyuan.com.

</details>


### [201] [SpatialMosaic: A Multiview VLM Dataset for Partial Visibility](https://arxiv.org/abs/2512.23365)
*Kanghee Lee,Injae Lee,Minseok Kwak,Kwonyoung Ryu,Jungi Hong,Jaesik Park*

Main category: cs.CV

TL;DR: 提出了SpatialMosaic数据集和SpatialMosaicVLM框架，用于增强多模态大语言模型在真实复杂场景下的多视角空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预构建的3D表示或现成重建流程，限制了可扩展性和真实世界适用性；且现有研究对真实环境中常见的部分可见性、遮挡和低重叠度等挑战性条件探索不足。

Method: 1) 构建可扩展的多视角数据生成与标注流程，创建包含200万QA对的SpatialMosaic指令调优数据集；2) 提出包含100万QA对、覆盖6个任务的SpatialMosaic-Bench评估基准；3) 设计SpatialMosaicVLM混合框架，将3D重建模型作为几何编码器集成到视觉语言模型中。

Result: 实验表明，所提数据集和VQA任务能有效提升模型在挑战性多视角条件下的空间推理能力，验证了数据生成流程在构建真实多样QA对方面的有效性。

Conclusion: 通过构建大规模数据集和混合框架，解决了真实场景中多视角空间推理的关键挑战，为增强视觉语言模型的3D场景理解能力提供了有效方案。

Abstract: The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.

</details>


### [202] [Bridging Cognitive Gap: Hierarchical Description Learning for Artistic Image Aesthetics Assessment](https://arxiv.org/abs/2512.23413)
*Henglin Liu,Nisha Huang,Chang Liu,Jiangpeng Yan,Huijuan Huang,Jixuan Ying,Tong-Yee Lee,Pengfei Wan,Xiangyang Ji*

Main category: cs.CV

TL;DR: 本文提出ArtQuant框架和RAD数据集，通过多维度结构化描述和LLM解码器解决AIGC美学评估中的数据稀缺与模型碎片化问题，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前AIGC美学评估存在两大挑战：1）数据稀缺且不平衡，现有数据集过度关注视觉感知而忽略深层维度；2）模型碎片化，现有方法难以有效处理长文本描述。

Method: 1）构建RAD数据集（70k规模），通过迭代流水线生成多维度结构化描述；2）提出ArtQuant框架，结合联合描述生成耦合美学维度，并利用LLM解码器建模长文本语义。

Result: 方法在多个数据集上达到最先进性能，仅需传统训练33%的epoch数，并通过理论分析证明数据与模型的协同作用能最小化预测熵。

Conclusion: RAD数据集与ArtQuant框架有效缩小了艺术图像与美学判断之间的认知鸿沟，为AIGC美学评估提供了可扩展的解决方案。

Abstract: The aesthetic quality assessment task is crucial for developing a human-aligned quantitative evaluation system for AIGC. However, its inherently complex nature, spanning visual perception, cognition, and emotion, poses fundamental challenges. Although aesthetic descriptions offer a viable representation of this complexity, two critical challenges persist: (1) data scarcity and imbalance: existing dataset overly focuses on visual perception and neglects deeper dimensions due to the expensive manual annotation; and (2) model fragmentation: current visual networks isolate aesthetic attributes with multi-branch encoder, while multimodal methods represented by contrastive learning struggle to effectively process long-form textual descriptions. To resolve challenge (1), we first present the Refined Aesthetic Description (RAD) dataset, a large-scale (70k), multi-dimensional structured dataset, generated via an iterative pipeline without heavy annotation costs and easy to scale. To address challenge (2), we propose ArtQuant, an aesthetics assessment framework for artistic images which not only couples isolated aesthetic dimensions through joint description generation, but also better models long-text semantics with the help of LLM decoders. Besides, theoretical analysis confirms this symbiosis: RAD's semantic adequacy (data) and generation paradigm (model) collectively minimize prediction entropy, providing mathematical grounding for the framework. Our approach achieves state-of-the-art performance on several datasets while requiring only 33% of conventional training epochs, narrowing the cognitive gap between artistic images and aesthetic judgment. We will release both code and dataset to support future research.

</details>


### [203] [SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation](https://arxiv.org/abs/2512.23411)
*Xiaolan Li,Wanquan Liu,Pengcheng Li,Pengyu Jie,Chenqiang Gao*

Main category: cs.CV

TL;DR: 提出SOFTooth框架，通过融合2D语义与3D几何特征解决牙齿实例分割中的边界模糊、中心漂移和身份不一致问题，在包含第三磨牙的复杂病例上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D牙齿分割方法易受牙弓拥挤、牙-龈边界模糊、缺牙及第三磨牙等复杂解剖结构影响，导致边界泄漏、中心漂移和身份不一致；而2D基础模型虽具强语义感知能力，但直接应用于3D临床工作流程不切实际。

Method: 1. 点级残差门控模块：将咬合面SAM嵌入注入3D点特征以细化边界；2. 中心引导掩码优化：通过几何质心一致性约束减少中心漂移；3. 顺序感知匈牙利匹配：结合解剖顺序与中心距离实现鲁棒实例分配。

Result: 在3DTeethSeg'22数据集上取得最优整体准确率和平均IoU，尤其在包含第三磨牙的病例上表现显著提升，证明无需2D微调即可有效迁移2D语义至3D分割任务。

Conclusion: SOFTooth通过语义增强的顺序感知2D-3D融合机制，成功将冻结2D语义迁移至3D牙齿实例分割，为复杂解剖结构下的医疗图像分析提供了高效解决方案。

Abstract: Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.

</details>


### [204] [Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision](https://arxiv.org/abs/2512.23426)
*Dohyun Kim,Seungwoo Lyu,Seung Wook Kim,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出DDSPO方法，通过直接利用获胜和失败策略的逐时间步监督来改进扩散模型的对齐和美学质量，无需依赖人工标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有基于偏好的训练方法（如Diffusion DPO）依赖昂贵且可能嘈杂的人工标注数据集，难以完全对齐用户意图并保持一致的美学质量。

Method: 引入DDSPO方法，在去噪轨迹上提供密集的转移级监督信号；使用预训练参考模型自动生成偏好信号，对比原始提示与语义降级变体的输出。

Result: DDSPO在文本-图像对齐和视觉质量方面表现优异，优于或匹配现有基于偏好的方法，同时需要显著更少的监督。

Conclusion: DDSPO提供了一种无需显式奖励建模或手动标注的有效分数空间偏好监督策略，提高了扩散模型的生成质量和对齐能力。

Abstract: Diffusion models have achieved impressive results in generative tasks such as text-to-image synthesis, yet they often struggle to fully align outputs with nuanced user intent and maintain consistent aesthetic quality. Existing preference-based training methods like Diffusion Direct Preference Optimization help address these issues but rely on costly and potentially noisy human-labeled datasets. In this work, we introduce Direct Diffusion Score Preference Optimization (DDSPO), which directly derives per-timestep supervision from winning and losing policies when such policies are available. Unlike prior methods that operate solely on final samples, DDSPO provides dense, transition-level signals across the denoising trajectory. In practice, we avoid reliance on labeled data by automatically generating preference signals using a pretrained reference model: we contrast its outputs when conditioned on original prompts versus semantically degraded variants. This practical strategy enables effective score-space preference supervision without explicit reward modeling or manual annotations. Empirical results demonstrate that DDSPO improves text-image alignment and visual quality, outperforming or matching existing preference-based methods while requiring significantly less supervision. Our implementation is available at: https://dohyun-as.github.io/DDSPO

</details>


### [205] [DriveLaW:Unifying Planning and Video Generation in a Latent Driving World](https://arxiv.org/abs/2512.23421)
*Tianze Xia,Yongkang Li,Lijun Zhou,Jingfeng Yao,Kaixin Xiong,Haiyang Sun,Bing Wang,Kun Ma,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 提出DriveLaW，一种统一视频生成与运动规划的新范式，通过将视频生成器的潜在表示直接注入规划器，确保高保真未来预测与可靠轨迹规划的内在一致性，在两项任务上均达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶中的世界模型通常将世界预测与运动规划作为解耦过程处理，限制了其能力。为弥合这一差距，研究旨在开发一种统一范式，以增强预测与规划之间的一致性。

Method: DriveLaW包含DriveLaW-Video（生成高保真预测的世界模型）和DriveLaW-Act（基于视频潜在表示的扩散规划器），采用三阶段渐进训练策略进行优化。

Result: DriveLaW在视频预测任务上显著超越先前最佳工作，FID提升33.3%，FVD提升1.8%，并在NAVSIM规划基准测试中创下新纪录。

Conclusion: DriveLaW通过统一视频生成与运动规划，实现了预测与规划的内在一致性，为自动驾驶世界模型提供了更有效的解决方案，并在两项任务中均取得了最先进的结果。

Abstract: World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.

</details>


### [206] [RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction](https://arxiv.org/abs/2512.23437)
*Shuhong Liu,Chenyu Bao,Ziteng Cui,Yun Liu,Xuangeng Chu,Lin Gu,Marcos V. Conde,Ryo Umagami,Tomohiro Hashimoto,Zijian Hu,Tianhan Xu,Yuan Gan,Yusuke Kurose,Tatsuya Harada*

Main category: cs.CV

TL;DR: 提出了RealX3D基准数据集，用于评估多视角视觉恢复和3D重建方法在多种物理退化条件下的性能，包含四种退化类型和多个严重程度级别。


<details>
  <summary>Details</summary>
Motivation: 当前多视角重建方法在真实世界复杂退化环境下的鲁棒性不足，缺乏系统评估物理退化影响的基准数据集。

Method: 采用统一采集协议捕获像素对齐的低质量/高质量视图，包含高分辨率图像、RAW数据和激光扫描，并生成世界尺度网格和度量深度数据。

Result: 测试多种优化方法和前馈方法显示，物理退化会显著降低重建质量，揭示了当前多视角流程在真实挑战环境中的脆弱性。

Conclusion: RealX3D基准揭示了现有方法在物理退化条件下的局限性，为开发更鲁棒的多视角视觉系统提供了重要评估工具。

Abstract: We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.

</details>


### [207] [Towards Integrating Uncertainty for Domain-Agnostic Segmentation](https://arxiv.org/abs/2512.23427)
*Jesse Brouwers,Xiaoyan Xing,Alexander Timans*

Main category: cs.CV

TL;DR: 该研究评估了SAM模型在挑战性分割场景下的不确定性量化方法，创建了UncertSAM基准测试集，发现拉普拉斯近似方法能有效关联不确定性估计与分割误差。


<details>
  <summary>Details</summary>
Motivation: SAM等基础分割模型在零样本场景下表现良好，但在领域偏移或知识受限场景中仍存在脆弱性，研究旨在探索不确定性量化能否以领域无关的方式增强模型泛化能力。

Method: 1) 构建包含8个数据集的UncertSAM基准测试集，涵盖阴影、透明、伪装等挑战性分割条件；2) 评估一系列轻量级后验不确定性估计方法；3) 测试基于不确定性的预测优化步骤。

Result: 最后一层的拉普拉斯近似方法产生的不确定性估计与分割误差具有良好相关性，表明其能提供有意义的信号；基于不确定性的优化虽效果初步，但显示出潜力。

Conclusion: 将不确定性纳入分割模型有助于提升鲁棒性和领域无关性能，研究公开了基准测试集和代码以促进相关研究。

Abstract: Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.

</details>


### [208] [Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin](https://arxiv.org/abs/2512.23454)
*Kayathri Vigneswaran,Hugo Retief,Jai Clifford Holmes,Mariangel Garcia Andarcia,Hansaka Tennakoon*

Main category: cs.CV

TL;DR: 提出一种结合视觉水线检测、YOLOv8尺度提取和多模态大语言模型的自动化水位计读数框架，在最佳图像条件下实现平均绝对误差5.43厘米的高精度水位监测。


<details>
  <summary>Details</summary>
Motivation: 传统水文观测方法存在人工测量误差和环境限制，需要准确连续的水位监测来支持洪水预报、水资源管理和生态保护。

Method: 采用混合框架：图像预处理→标注→水线检测→尺度间隙估计→数值读数提取，集成YOLOv8姿态尺度提取和GPT-4o/Gemini 2.0 Flash多模态大语言模型。

Result: 水线检测精度达94.24%，F1分数83.64%；结合尺度间隙元数据显著提升大语言模型性能，Gemini Stage 2在最佳图像条件下MAE=5.43cm、RMSE=8.58cm、R²=0.84；图像质量下降会导致误差增大。

Conclusion: 该方法为自动化水文监测提供了可扩展、高效可靠的解决方案，结合几何元数据与多模态人工智能能实现鲁棒的水位估计，具有实时水位计数字化和改善水资源管理的潜力。

Abstract: Accurate and continuous monitoring of river water levels is essential for flood forecasting, water resource management, and ecological protection. Traditional hydrological observation methods are often limited by manual measurement errors and environmental constraints. This study presents a hybrid framework integrating vision based waterline detection, YOLOv8 pose scale extraction, and large multimodal language models (GPT 4o and Gemini 2.0 Flash) for automated river gauge plate reading. The methodology involves sequential stages of image preprocessing, annotation, waterline detection, scale gap estimation, and numeric reading extraction. Experiments demonstrate that waterline detection achieved high precision of 94.24 percent and an F1 score of 83.64 percent, while scale gap detection provided accurate geometric calibration for subsequent reading extraction. Incorporating scale gap metadata substantially improved the predictive performance of LLMs, with Gemini Stage 2 achieving the highest accuracy, with a mean absolute error of 5.43 cm, root mean square error of 8.58 cm, and R squared of 0.84 under optimal image conditions. Results highlight the sensitivity of LLMs to image quality, with degraded images producing higher errors, and underscore the importance of combining geometric metadata with multimodal artificial intelligence for robust water level estimation. Overall, the proposed approach offers a scalable, efficient, and reliable solution for automated hydrological monitoring, demonstrating potential for real time river gauge digitization and improved water resource management.

</details>


### [209] [MCI-Net: A Robust Multi-Domain Context Integration Network for Point Cloud Registration](https://arxiv.org/abs/2512.23472)
*Shuyuan Lin,Wenwu Peng,Junjie Huang,Qiang Qi,Miaohui Wang,Jian Weng*

Main category: cs.CV

TL;DR: 提出MCI-Net网络，通过多领域上下文整合提升点云配准特征学习能力，在室内外数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的点云配准方法依赖欧氏邻域特征提取，难以有效捕捉点云的隐式语义和结构一致性

Method: 1) 图邻域聚合模块构建全局图捕获结构关系；2) 渐进上下文交互模块通过域内特征解耦和域间交互增强特征判别性；3) 动态内点选择方法利用多轮位姿估计残差优化内点权重

Result: 在3DMatch数据集上达到96.4%的配准召回率，显著优于现有方法

Conclusion: MCI-Net通过整合多领域上下文信息，实现了更鲁棒和判别性的特征学习，提升了点云配准的准确性和鲁棒性

Abstract: Robust and discriminative feature learning is critical for high-quality point cloud registration. However, existing deep learning-based methods typically rely on Euclidean neighborhood-based strategies for feature extraction, which struggle to effectively capture the implicit semantics and structural consistency in point clouds. To address these issues, we propose a multi-domain context integration network (MCI-Net) that improves feature representation and registration performance by aggregating contextual cues from diverse domains. Specifically, we propose a graph neighborhood aggregation module, which constructs a global graph to capture the overall structural relationships within point clouds. We then propose a progressive context interaction module to enhance feature discriminability by performing intra-domain feature decoupling and inter-domain context interaction. Finally, we design a dynamic inlier selection method that optimizes inlier weights using residual information from multiple iterations of pose estimation, thereby improving the accuracy and robustness of registration. Extensive experiments on indoor RGB-D and outdoor LiDAR datasets show that the proposed MCI-Net significantly outperforms existing state-of-the-art methods, achieving the highest registration recall of 96.4\% on 3DMatch. Source code is available at http://www.linshuyuan.com.

</details>


### [210] [Deterministic Image-to-Image Translation via Denoising Brownian Bridge Models with Dual Approximators](https://arxiv.org/abs/2512.23463)
*Bohan Xiao,Peiyong Wang,Qisheng He,Ming Dong*

Main category: cs.CV

TL;DR: 本文提出了一种基于双重逼近器的去噪布朗桥模型（Dual-approx Bridge），用于确定性图像到图像转换，通过布朗桥动态和两个神经网络逼近器实现高保真、低方差的输出。


<details>
  <summary>Details</summary>
Motivation: 确定性图像到图像转换（如超分辨率）需要保证输入到输出的稳定映射，并尽可能接近真实图像。现有方法在保真度和图像质量上仍有提升空间，因此需要一种能同时保证高保真和低方差的生成模型。

Method: 提出Dual-approx Bridge模型，利用布朗桥动态设计前向和反向两个神经网络逼近器，控制生成过程的确定性，减少输出方差。

Result: 在图像生成和超分辨率等基准数据集上的实验表明，该方法在图像质量和保真度上均优于随机性和确定性基线模型。

Conclusion: Dual-approx Bridge通过双重逼近器和布朗桥动态，实现了高质量、高保真且输出方差极低的确定性图像转换，为相关任务提供了有效解决方案。

Abstract: Image-to-Image (I2I) translation involves converting an image from one domain to another. Deterministic I2I translation, such as in image super-resolution, extends this concept by guaranteeing that each input generates a consistent and predictable output, closely matching the ground truth (GT) with high fidelity. In this paper, we propose a denoising Brownian bridge model with dual approximators (Dual-approx Bridge), a novel generative model that exploits the Brownian bridge dynamics and two neural network-based approximators (one for forward and one for reverse process) to produce faithful output with negligible variance and high image quality in I2I translations. Our extensive experiments on benchmark datasets including image generation and super-resolution demonstrate the consistent and superior performance of Dual-approx Bridge in terms of image quality and faithfulness to GT when compared to both stochastic and deterministic baselines. Project page and code: https://github.com/bohan95/dual-app-bridge

</details>


### [211] [SC-Net: Robust Correspondence Learning via Spatial and Cross-Channel Context](https://arxiv.org/abs/2512.23473)
*Shuyuan Lin,Hailiang Liao,Qiang Qi,Junjie Huang,Taotao Lai,Jian Weng*

Main category: cs.CV

TL;DR: 提出SC-Net网络，通过空间与通道双重视角整合上下文，改进两视图对应关系学习，在姿态估计和离群点去除任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的两视图对应学习方法在全局上下文聚合和大视差场景运动场平滑方面存在不足，需要针对任务特性设计更有效的网络结构。

Method: 提出SC-Net网络，包含三个核心模块：自适应聚焦正则化模块（AFR）增强位置感知与抗干扰能力；双边场调整模块（BFA）通过空间与通道交互优化运动场；位置感知恢复模块（PAR）确保运动向量的一致性与精度。

Result: 在YFCC100M和SUN3D数据集上的实验表明，SC-Net在相对姿态估计和离群点去除任务上优于当前最优方法。

Conclusion: 通过空间-通道双边上下文整合与任务定制化设计，SC-Net能有效提升两视图对应学习的性能，为相关任务提供了新解决方案。

Abstract: Recent research has focused on using convolutional neural networks (CNNs) as the backbones in two-view correspondence learning, demonstrating significant superiority over methods based on multilayer perceptrons. However, CNN backbones that are not tailored to specific tasks may fail to effectively aggregate global context and oversmooth dense motion fields in scenes with large disparity. To address these problems, we propose a novel network named SC-Net, which effectively integrates bilateral context from both spatial and channel perspectives. Specifically, we design an adaptive focused regularization module (AFR) to enhance the model's position-awareness and robustness against spurious motion samples, thereby facilitating the generation of a more accurate motion field. We then propose a bilateral field adjustment module (BFA) to refine the motion field by simultaneously modeling long-range relationships and facilitating interaction across spatial and channel dimensions. Finally, we recover the motion vectors from the refined field using a position-aware recovery module (PAR) that ensures consistency and precision. Extensive experiments demonstrate that SC-Net outperforms state-of-the-art methods in relative pose estimation and outlier removal tasks on YFCC100M and SUN3D datasets. Source code is available at http://www.linshuyuan.com.

</details>


### [212] [TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding](https://arxiv.org/abs/2512.23483)
*Zongsheng Cao,Yangfan He,Anran Liu,Feng Chen,Zepeng Wang,Jun Xie*

Main category: cs.CV

TL;DR: 提出了TV-RAG框架，通过时间衰减检索和熵加权关键帧采样，无需训练即可提升大型视频语言模型对长视频的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型视频语言模型在处理长视频时存在时间窗口窄、难以捕捉细粒度语义变化的问题，且主流文本检索方法忽视多模态通道间的时间依赖性。

Method: 1. 时间衰减检索模块：在相似度计算中注入显式时间偏移，使文本查询能匹配真实的多媒体上下文；2. 熵加权关键帧采样器：选择信息密集且均匀分布的关键帧，减少冗余并保持代表性。

Result: TV-RAG在Video-MME、MLVU和LongVideoBench等长视频基准测试中持续超越多数主流基线模型。

Conclusion: TV-RAG提供了一种轻量级、无需重新训练或微调的升级方案，有效提升了长视频推理能力，验证了结合时间对齐与熵引导语义方法的有效性。

Abstract: Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.

</details>


### [213] [IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation](https://arxiv.org/abs/2512.23519)
*Donghao Zhou,Jingyu Lin,Guibao Shen,Quande Liu,Jialin Gao,Lihao Liu,Lan Du,Cunjian Chen,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 提出IdentityStory框架，用于生成以人物为中心、角色身份一致的多图像故事，在面部一致性和多角色协调方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型在生成人物故事时面临挑战，包括难以保持详细且多样的人脸一致性，以及协调不同图像中的多个角色。

Method: 采用身份保持生成器，包含两个关键组件：迭代身份发现（提取一致的角色身份）和重去噪身份注入（在保留上下文的同时注入身份）。

Result: 在ConsiStory-Human基准测试中表现优于现有方法，特别是在面部一致性方面，并支持多角色组合。

Conclusion: IdentityStory框架在人物故事生成中实现了更好的角色一致性，展示了在无限长度故事生成和动态角色组合等应用中的潜力。

Abstract: Recent visual generative models enable story generation with consistent characters from text, but human-centric story generation faces additional challenges, such as maintaining detailed and diverse human face consistency and coordinating multiple characters across different images. This paper presents IdentityStory, a framework for human-centric story generation that ensures consistent character identity across multiple sequential images. By taming identity-preserving generators, the framework features two key components: Iterative Identity Discovery, which extracts cohesive character identities, and Re-denoising Identity Injection, which re-denoises images to inject identities while preserving desired context. Experiments on the ConsiStory-Human benchmark demonstrate that IdentityStory outperforms existing methods, particularly in face consistency, and supports multi-character combinations. The framework also shows strong potential for applications such as infinite-length story generation and dynamic character composition.

</details>


### [214] [Multi-label Classification with Panoptic Context Aggregation Networks](https://arxiv.org/abs/2512.23486)
*Mingyuan Jiu,Hailong Zhu,Wenchuan Wei,Hichem Sahbi,Rongrong Ji,Mingliang Xu*

Main category: cs.CV

TL;DR: 提出PanCAN网络，通过在高维希尔伯特空间中进行跨尺度特征聚合，分层整合多阶几何上下文，显著提升复杂场景理解与多标签分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉识别方法多关注基础几何关系或局部特征，缺乏对物体间跨尺度上下文交互的建模，限制了复杂场景的理解能力。

Method: 设计深度全景上下文聚合网络（PanCAN），结合随机游走与注意力机制学习各尺度的多阶邻域关系；通过跨尺度模块级联，动态融合细粒度锚点的邻域特征。

Result: 在NUS-WIDE、PASCAL VOC2007和MS-COCO数据集上的多标签分类实验中，PanCAN均取得优于现有技术的定量与定性结果。

Conclusion: 跨尺度多阶上下文建模能有效增强图像表征的判别力，为复杂场景理解提供了新思路，显著提升了多标签分类性能。

Abstract: Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.

</details>


### [215] [Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution](https://arxiv.org/abs/2512.23532)
*Hexin Zhang,Dong Li,Jie Huang,Bingzhou Wang,Xueyang Fu,Zhengjun Zha*

Main category: cs.CV

TL;DR: 提出IAFS框架，通过迭代扩散推理时间缩放与自适应频率引导，解决扩散模型在图像超分辨率中感知质量与结构保真度的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在图像超分辨率中难以同时保证高频感知质量和低频结构保真度，现有推理时间缩放策略存在感知过度平滑或结构一致性丢失的问题。

Method: 提出无需训练的IAFS框架，结合迭代细化和频率感知粒子融合，通过迭代校正结构偏差逐步优化生成图像，并自适应融合高频感知线索与低频结构信息。

Result: 在多个基于扩散的超分辨率模型上的实验表明，IAFS能有效解决感知-保真度冲突，在感知细节和结构准确性方面均优于现有推理时间缩放方法。

Conclusion: IAFS框架通过迭代扩散推理时间缩放与自适应频率引导，实现了图像超分辨率中感知质量与结构保真度的更好平衡，显著提升了重建效果。

Abstract: Diffusion models have become a leading paradigm for image super-resolution (SR), but existing methods struggle to guarantee both the high-frequency perceptual quality and the low-frequency structural fidelity of generated images. Although inference-time scaling can theoretically improve this trade-off by allocating more computation, existing strategies remain suboptimal: reward-driven particle optimization often causes perceptual over-smoothing, while optimal-path search tends to lose structural consistency. To overcome these difficulties, we propose Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering (IAFS), a training-free framework that jointly leverages iterative refinement and frequency-aware particle fusion. IAFS addresses the challenge of balancing perceptual quality and structural fidelity by progressively refining the generated image through iterative correction of structural deviations. Simultaneously, it ensures effective frequency fusion by adaptively integrating high-frequency perceptual cues with low-frequency structural information, allowing for a more accurate and balanced reconstruction across different image details. Extensive experiments across multiple diffusion-based SR models show that IAFS effectively resolves the perception-fidelity conflict, yielding consistently improved perceptual detail and structural accuracy, and outperforming existing inference-time scaling methods.

</details>


### [216] [ThinkGen: Generalized Thinking for Visual Generation](https://arxiv.org/abs/2512.23568)
*Siyu Jiao,Yiheng Lin,Yujie Zhong,Qi She,Wei Zhou,Xiaohan Lan,Zilong Huang,Fei Yu,Yingchen Yu,Yunqing Zhao,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出了ThinkGen框架，首次将MLLM的思维链推理应用于视觉生成任务，通过解耦架构和可分离训练范式实现多场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理主要应用于理解任务，在生成任务中扩展不足且受限于场景特定机制，缺乏通用性和适应性。

Method: 采用解耦架构：预训练MLLM生成指令 + Diffusion Transformer生成图像；提出SepGRPO训练范式，在MLLM和DiT间交替强化学习。

Result: 在多个生成基准测试中实现了稳健的先进性能，代码已开源。

Conclusion: ThinkGen框架成功将思维链推理扩展到视觉生成领域，通过灵活设计支持多样化生成场景的联合训练。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen

</details>


### [217] [PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation](https://arxiv.org/abs/2512.23546)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: 提出PurifyGen，一种无需训练的双阶段提示净化方法，用于安全文本到图像生成，通过语义距离评估和双空间变换去除有害内容并保留原始意图。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在提升文本到图像生成质量的同时，也增加了生成不安全内容的风险。传统安全方法（如文本黑名单或有害内容分类）存在易被绕过或需要大量数据和额外训练的问题。

Method: 1. 通过计算提示词与预定义有毒/清洁概念嵌入的互补语义距离，评估每个词元的安全性；2. 对高风险提示进行双空间变换：将有毒性对齐的嵌入投影到有毒概念矩阵的零空间以去除有害语义，同时将其对齐到清洁概念的范围空间以增强安全语义；3. 采用词元级策略选择性替换高风险词元嵌入。

Result: 在五个数据集上的广泛测试表明，PurifyGen在减少不安全内容方面优于现有方法，并与依赖训练的方法竞争良好，且具有强泛化能力。

Conclusion: PurifyGen提供了一种即插即用、理论扎实的解决方案，无需修改模型权重即可有效净化提示，平衡安全性与内容保真度。

Abstract: Recent advances in diffusion models have notably enhanced text-to-image (T2I) generation quality, but they also raise the risk of generating unsafe content. Traditional safety methods like text blacklisting or harmful content classification have significant drawbacks: they can be easily circumvented or require extensive datasets and extra training. To overcome these challenges, we introduce PurifyGen, a novel, training-free approach for safe T2I generation that retains the model's original weights. PurifyGen introduces a dual-stage strategy for prompt purification. First, we evaluate the safety of each token in a prompt by computing its complementary semantic distance, which measures the semantic proximity between the prompt tokens and concept embeddings from predefined toxic and clean lists. This enables fine-grained prompt classification without explicit keyword matching or retraining. Tokens closer to toxic concepts are flagged as risky. Second, for risky prompts, we apply a dual-space transformation: we project toxic-aligned embeddings into the null space of the toxic concept matrix, effectively removing harmful semantic components, and simultaneously align them into the range space of clean concepts. This dual alignment purifies risky prompts by both subtracting unsafe semantics and reinforcing safe ones, while retaining the original intent and coherence. We further define a token-wise strategy to selectively replace only risky token embeddings, ensuring minimal disruption to safe content. PurifyGen offers a plug-and-play solution with theoretical grounding and strong generalization to unseen prompts and models. Extensive testing shows that PurifyGen surpasses current methods in reducing unsafe content across five datasets and competes well with training-dependent approaches. The code can refer to https://github.com/AI-Researcher-Team/PurifyGen.

</details>


### [218] [Image Denoising Using Global and Local Circulant Representation](https://arxiv.org/abs/2512.23569)
*Zhaoming Kong,Xiaowei Yang,Jiahuan Zhang*

Main category: cs.CV

TL;DR: 提出一种基于张量奇异值分解和Haar变换的图像去噪方法Haar-tSVD，结合深度学习提升强噪声下的性能


<details>
  <summary>Details</summary>
Motivation: 成像设备普及导致每日产生海量图像数据，对高效有效的图像去噪需求日益增长

Method: 建立PCA与循环表示下Haar变换的理论联系，提出Haar-tSVD方法：统一t-SVD投影结合Haar变换捕获全局和局部相关性；引入自适应噪声估计；基于Haar-PCA关系集成深度神经网络

Result: 在多种去噪数据集上验证了方法的效率和有效性，代码已开源

Conclusion: Haar-tSVD作为一步式可并行即插即用去噪器，在去噪速度与性能间取得平衡，并通过深度学习增强强噪声条件下的性能

Abstract: The proliferation of imaging devices and countless image data generated every day impose an increasingly high demand on efficient and effective image denoising. In this paper, we establish a theoretical connection between principal component analysis (PCA) and the Haar transform under circulant representation, and present a computationally simple denoising algorithm. The proposed method, termed Haar-tSVD, exploits a unified tensor singular value decomposition (t-SVD) projection combined with Haar transform to efficiently capture global and local patch correlations. Haar-tSVD operates as a one-step, parallelizable plug-and-play denoiser that eliminates the need for learning local bases, thereby striking a balance between denoising speed and performance. Besides, an adaptive noise estimation scheme is introduced to improve robustness according to eigenvalue analysis of the circulant structure. To further enhance the performance under severe noise conditions, we integrate deep neural networks with Haar-tSVD based on the established Haar-PCA relationship. Experimental results on various denoising datasets demonstrate the efficiency and effectiveness of proposed method for noise removal. Our code is publicly available at https://github.com/ZhaomingKong/Haar-tSVD.

</details>


### [219] [ProGuard: Towards Proactive Multimodal Safeguard](https://arxiv.org/abs/2512.23573)
*Shaohan Yu,Lijun Li,Chenyang Si,Lu Sheng,Jing Shao*

Main category: cs.CV

TL;DR: 提出ProGuard视觉语言主动防护系统，通过强化学习训练，无需模型调整即可识别和描述分布外安全风险，在安全分类和风险描述方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 生成模型的快速发展导致多模态安全风险不断涌现，现有防御方法多为被动响应且存在模态偏见，难以应对新型安全威胁。

Method: 构建8.7万样本的平衡多模态安全数据集；采用纯强化学习训练视觉语言基础模型；引入分布外安全类别推断任务和基于同义词库的相似性奖励机制。

Result: 在二元安全分类上达到闭源大模型水平；在非安全内容分类上大幅超越现有开源防护模型；分布外风险检测提升52.6%，风险描述提升64.8%。

Conclusion: ProGuard通过主动防护机制有效解决了多模态安全风险检测的局限性，为生成模型的安全防护提供了新范式。

Abstract: The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.

</details>


### [220] [Same or Not? Enhancing Visual Perception in Vision-Language Models](https://arxiv.org/abs/2512.23592)
*Damiano Marsili,Aditya Mehta,Ryan Y. Lin,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 提出了TWIN数据集和FGVQA基准，用于提升视觉语言模型在细粒度视觉感知方面的能力，通过图像对判断任务增强模型对细微视觉差异的识别


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在粗粒度视觉理解上表现良好，但存在视觉偏见且忽略细微视觉细节，现有训练数据过于强调一般性识别而缺乏细粒度感知能力

Method: 构建了包含56.1万对图像的TWIN数据集，要求模型判断两幅相似图像是否描绘同一物体；创建了包含1.2万个查询的FGVQA基准测试套件，涵盖多个领域的细粒度识别数据集

Result: 在TWIN上微调的视觉语言模型在FGVQA基准上性能提升高达19.3%，在艺术、动物、植物和地标等未见领域也表现出色，且不影响通用视觉问答性能

Conclusion: TWIN数据集能有效提升视觉语言模型的细粒度感知能力，数据规模是关键因素，可作为开源视觉语言模型训练数据的补充组件

Abstract: Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/

</details>


### [221] [Detection Fire in Camera RGB-NIR](https://arxiv.org/abs/2512.23594)
*Nguyen Truong Khai,Luong Duc Vinh*

Main category: cs.CV

TL;DR: 本文提出了一种改进红外夜视摄像头火灾检测准确性的方法，包括构建NIR数据集、两阶段检测模型和Patched-YOLO，以解决人工光源误报和小目标检测问题。


<details>
  <summary>Details</summary>
Motivation: 现有火灾检测模型在红外夜视场景下存在局限性，特别是容易将明亮的人工光源误判为火灾，且数据集构建不足影响了检测性能。

Method: 1. 采用多种数据增强策略扩充NIR和分类数据集；2. 提出结合YOLOv11和EfficientNetV2-B0的两阶段检测流程；3. 设计Patched-YOLO通过分块处理提升RGB图像中小目标和远距离目标的检测能力。

Result: 所提方法在夜间火灾检测中取得了比YOLOv7、RT-DETR和YOLOv9更高的检测精度，并有效减少了人工光源导致的误报。

Conclusion: 通过数据增强、两阶段检测框架和分块处理策略，显著提升了红外夜视和RGB图像中的火灾检测性能，特别是夜间场景和小目标检测效果。

Abstract: Improving the accuracy of fire detection using infrared night vision cameras remains a challenging task. Previous studies have reported strong performance with popular detection models. For example, YOLOv7 achieved an mAP50-95 of 0.51 using an input image size of 640 x 1280, RT-DETR reached an mAP50-95 of 0.65 with an image size of 640 x 640, and YOLOv9 obtained an mAP50-95 of 0.598 at the same resolution. Despite these results, limitations in dataset construction continue to cause issues, particularly the frequent misclassification of bright artificial lights as fire.
  This report presents three main contributions: an additional NIR dataset, a two-stage detection model, and Patched-YOLO. First, to address data scarcity, we explore and apply various data augmentation strategies for both the NIR dataset and the classification dataset. Second, to improve night-time fire detection accuracy while reducing false positives caused by artificial lights, we propose a two-stage pipeline combining YOLOv11 and EfficientNetV2-B0. The proposed approach achieves higher detection accuracy compared to previous methods, particularly for night-time fire detection. Third, to improve fire detection in RGB images, especially for small and distant objects, we introduce Patched-YOLO, which enhances the model's detection capability through patch-based processing. Further details of these contributions are discussed in the following sections.

</details>


### [222] [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/abs/2512.23576)
*Ethan Chern,Zhulin Hu,Bohao Tang,Jiadi Su,Steffi Chern,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: 提出了一种实时交互式视频扩散模型，通过改进的蒸馏方法在保持视觉质量的同时大幅降低推理成本，并构建了LiveTalk实时多模态交互系统。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型通过双向注意力的迭代去噪过程无法实现实时交互，且现有蒸馏方法主要针对文本到视频生成，在多模态条件下存在视觉伪影和质量下降问题，限制了人机交互的自然性和效率。

Method: 针对多模态条件（文本、图像、音频）提出改进的蒸馏方法，重点关注条件输入质量、初始化策略和在线策略优化调度，并与音频语言模型和长视频推理技术Anchor-Heavy Identity Sinks集成构建LiveTalk系统。

Result: 蒸馏模型在HDTF、AVSpeech和CelebV-HQ基准测试中，以20倍更低的推理成本和延迟匹配了全步骤双向基线的视觉质量；LiveTalk系统在多轮交互基准测试中，在视频连贯性和内容质量上优于Sora2、Veo3等先进模型，并将响应延迟从1-2分钟降低到实时生成。

Conclusion: 所提出的蒸馏方法和LiveTalk系统能够实现高质量、低延迟的多模态交互视频生成，为人机实时无缝交互提供了有效解决方案。

Abstract: Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.

</details>


### [223] [Memorization in 3D Shape Generation: An Empirical Study](https://arxiv.org/abs/2512.23628)
*Shu Pu,Boya Zeng,Kaichen Zhou,Mengyu Wang,Zhuang Liu*

Main category: cs.CV

TL;DR: 本文提出了一个评估框架来量化3D生成模型中的记忆效应，并通过实验分析了数据和建模设计对记忆的影响，提出了减少记忆的简单有效策略。


<details>
  <summary>Details</summary>
Motivation: 生成模型在3D视觉中广泛用于合成新形状，但尚不清楚其生成是否依赖于记忆训练数据。理解记忆效应有助于防止训练数据泄露并提高生成结果的多样性。

Method: 设计了一个评估框架来量化3D生成模型中的记忆效应，并通过对潜在向量集扩散模型进行受控实验，分析数据和建模因素对记忆的影响。

Result: 发现记忆效应取决于数据模态，随数据多样性和更细粒度的条件而增加；在建模方面，记忆在中等引导尺度达到峰值，可通过更长的向量集和简单的旋转增强来缓解。

Conclusion: 该框架和分析提供了对3D生成模型中记忆效应的实证理解，并提出了在不降低生成质量的情况下减少记忆的简单有效策略。

Abstract: Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.

</details>


### [224] [Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging](https://arxiv.org/abs/2512.23597)
*Janani Annur Thiruvengadam,Kiran Mayee Nabigaru,Anusha Kovi*

Main category: cs.CV

TL;DR: 提出了一种可扩展的残差特征聚合（SRFA）框架，用于胰腺肿瘤的早期检测，该框架结合了多种深度学习技术和元启发式优化方法，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 胰腺肿瘤早期检测面临临床困境，CT扫描中肿瘤对比度低、患者解剖结构差异大，需要一种能够增强细微视觉线索并具有良好泛化能力的系统。

Method: 采用SRFA框架，包括预处理和MAGRes-UNet分割，使用DenseNet-121进行特征提取，结合HHO-BA元启发式特征选择，并采用ViT与EfficientNet-B3混合模型进行分类，通过SSA和GWO双优化机制调优超参数。

Result: 实验结果显示，所提模型达到96.23%准确率、95.58% F1分数和94.83%特异性，显著优于传统CNN和当前基于Transformer的模型。

Conclusion: SRFA框架在胰腺肿瘤早期检测中表现出巨大潜力，可作为有效的辅助工具，其高性能和鲁棒性为临床应用提供了可能。

Abstract: The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.

</details>


### [225] [OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding](https://arxiv.org/abs/2512.23646)
*Keda Tao,Wenjie Du,Bohan Yu,Weiqiang Wang,Jian Liu,Huan Wang*

Main category: cs.CV

TL;DR: 提出OmniAgent，一种音频引导的主动感知智能体，通过动态规划工具调用实现细粒度音视频理解，在多个基准测试中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有全模态大语言模型在音视频跨模态理解方面存在不足，缺乏细粒度对齐能力，且多采用静态工作流程和密集帧标注的被动响应模式。

Method: 采用动态规划自主编排工具调用，提出粗到细的音频引导感知范式：先利用音频线索定位时序事件，再引导后续推理，实现主动多模态查询。

Result: 在三个音视频理解基准测试中，OmniAgent性能超越领先的开源和专有模型10%-20%的准确率，达到最先进水平。

Conclusion: 从被动响应生成转向主动多模态查询的范式转变有效，音频引导的主动感知机制能显著提升细粒度音视频推理能力。

Abstract: Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

</details>


### [226] [Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception](https://arxiv.org/abs/2512.23635)
*Xiaoyu Li,Peidong Li,Xian Wu,Long Shi,Dedong Liu,Yitao Wu,Jiajia Fu,Dixiao Cui,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: 提出HAT时空对齐模块，通过多假设解码自适应选择最优对齐方案，提升自动驾驶端到端感知的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖注意力机制和统一显式物理模型进行跨帧目标对齐，但不同类别和帧间的运动状态与目标特征变化导致对齐效果欠佳，需要更灵活的自适应对齐方法。

Method: HAT模块利用多个显式运动模型生成空间锚点和运动感知特征提议，结合缓存目标查询中的语义与运动线索进行多假设解码，为目标帧提供最优对齐方案。

Result: 在nuScenes数据集上，HAT显著提升多种3D时序检测与跟踪基线性能，与DETR3D结合达到46.0% AMOTA的SOTA跟踪结果；在端到端自动驾驶方法中提升感知精度（mAP +1.3%，AMOTA +3.1%）并降低32%碰撞率；在语义受损场景（nuScenes-C）中增强运动建模鲁棒性。

Conclusion: HAT通过自适应多假设对齐机制有效融合语义与运动信息，在复杂动态场景中实现更鲁棒的时空对齐，为自动驾驶感知与规划提供可靠支持。

Abstract: Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.

</details>


### [227] [Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion](https://arxiv.org/abs/2512.23709)
*Hau-Shiang Shiu,Chin-Yang Lin,Zhixiang Wang,Chi-Wei Hsiao,Po-Fan Yu,Yu-Chih Chen,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出Stream-DiffVSR，一种因果条件扩散框架，用于高效在线视频超分辨率，显著降低延迟并提升感知质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频超分辨率方法依赖未来帧和多步去噪，导致延迟高，不适用于对延迟敏感的场景。

Method: 结合四步蒸馏去噪器、自回归时间引导模块和轻量级时间感知解码器，严格使用过去帧进行在线处理。

Result: 在RTX4090上处理720p帧仅需0.328秒，感知质量（LPIPS +0.095）优于SOTA方法，延迟降低130倍以上。

Conclusion: Stream-DiffVSR首次实现了适用于低延迟在线部署的扩散视频超分辨率方法，将初始延迟从4600秒降至0.328秒。

Abstract: Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/

</details>


### [228] [IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition](https://arxiv.org/abs/2512.23667)
*Kang Du,Yirui Guan,Zeyu Wang*

Main category: cs.CV

TL;DR: 提出IDT框架，通过基于Transformer的注意力机制联合处理多视角图像，实现视角一致的本征图像分解，无需迭代生成采样。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的方法在单视角本征分解中表现良好，但扩展到多视角时存在严重的视角不一致问题，需要解决多视角本征分解的挑战。

Method: 采用基于物理的图像形成模型，将图像分解为漫反射率、漫反射着色和镜面反射着色三个分量；使用Transformer注意力机制在单次前向传播中联合推理多视角输入。

Result: 在合成和真实数据集上，IDT相比先前方法获得了更干净的漫反射率、更一致的漫反射着色、更好分离的镜面反射分量，并显著提升了多视角一致性。

Conclusion: IDT框架能够实现视角一致的多视角本征图像分解，有效分离朗伯和非朗伯光传输，为跨视角的材料和光照效果分解提供了可解释且可控的方法。

Abstract: Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.

</details>


### [229] [Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation](https://arxiv.org/abs/2512.23705)
*Shaocong Xu,Songlin Wei,Qizhe Wei,Zheng Geng,Hong Li,Licheng Shen,Qianpu Sun,Shu Han,Bin Ma,Bohan Li,Chongjie Ye,Yuhang Zheng,Nan Wang,Saining Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 该论文提出DKT模型，通过利用视频扩散模型学习透明物体的光学特性，实现了对透明和反射物体深度和法线的零样本估计，在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 透明物体对传统感知系统构成挑战，因为折射、反射和透射现象破坏了立体视觉、飞行时间和单目深度估计的假设，导致深度估计存在空洞和时间不稳定问题。

Method: 构建TransPhy3D合成视频数据集（11k序列），使用Blender/Cycles渲染RGB+深度+法线；基于大型视频扩散模型，通过LoRA适配器学习视频到视频的深度/法线转换器；在DiT骨干网络中拼接RGB和噪声深度潜在表示，并在TransPhy3D和现有合成数据集上联合训练。

Result: DKT模型在透明物体相关基准测试（ClearPose、DREDS、TransPhy3D-Test）上实现零样本最优性能；在准确性和时间一致性上优于现有图像/视频基线；1.3B紧凑版本达到约0.17秒/帧速度；集成到抓取系统中能提升对透明、反射和漫反射表面的操作成功率。

Conclusion: 研究证明扩散模型已内化透明物体的光学规则，生成式视频先验可被高效、无标签地转化为鲁棒且时间一致的感知系统，支持'扩散模型理解透明度'的核心主张。

Abstract: Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [230] [Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208)
*Pu Zhao,Xuan Shen,Zhenglun Kong,Yixin Shen,Sung-En Chang,Arash Akbari,Timothy Rupprecht,Lei Lu,Enfu Nan,Changdi Yang,Yumei He,Weiyan Shi,Xingchen Xu,Yu Huang,Wei Jiang,Wei Wang,Yue Chen,Yong He,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文介绍了完全开源的Moxin 7B大语言模型及其三个变体，它们遵循模型开放框架，在训练、数据集和实现细节上完全透明，并在多项评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的发展主要由闭源模型（如GPT-4）和开源模型（如LLaMA）推动，但开源模型通常仅共享模型权重，缺乏训练和数据的透明度。本研究旨在通过完全开源的Moxin模型，促进更包容和协作的研究环境，支持健康的开源生态系统。

Method: 基于模型开放框架开发了完全开源的Moxin 7B大语言模型，并衍生出三个变体：Moxin-VLM（视觉语言）、Moxin-VLA（视觉语言动作）和Moxin-Chinese（中文能力）。训练采用开源框架和开源数据。

Result: 实验表明，Moxin模型及其变体在多项评估中取得了优异的性能。模型、可用数据及代码均已公开发布。

Conclusion: Moxin 7B及其变体通过完全透明的开源方式，为大语言模型的研究和应用提供了可定制、可部署的解决方案，有助于推动开源生态系统的可持续发展。

Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.

</details>


### [231] [Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces](https://arxiv.org/abs/2512.22227)
*Sophie Zhao*

Main category: cs.CL

TL;DR: 研究发现，基于Transformer的语言模型在句子嵌入空间中编码了与人类可解释的认知属性对齐的分层几何结构，这种结构超越了表面词汇统计特征。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明Transformer语言模型的嵌入空间具有丰富的几何结构，但其中是否存在与高级认知组织对齐的分层结构尚未得到充分探索。本研究旨在探究句子嵌入是否编码了与人类认知或心理属性一致的渐进式分层结构。

Method: 构建了一个包含480个自然语言句子的数据集，每个句子标注了连续序数能量分数和离散层级标签（涵盖七个有序认知类别）。使用多个Transformer模型的固定句子嵌入，通过线性和浅层非线性探针评估这些标注的可恢复性，并与TF-IDF基线进行比较，同时采用非参数置换检验验证显著性。

Result: 在所有模型中，连续分数和层级标签均可可靠解码，浅层非线性探针比线性探针性能更优。TF-IDF基线表现显著较差，表明观察到的结构不能仅归因于表面词汇统计。置换检验确认探针性能显著高于随机水平。UMAP可视化和混淆矩阵显示嵌入空间中存在从低到高的平滑梯度，且混淆主要发生在相邻层级之间。

Conclusion: Transformer嵌入空间表现出与人类定义的认知属性对齐的分层几何组织，但这一发现不涉及对模型内部意识或现象学的主张。

Abstract: Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores and discrete tier labels spanning seven ordered cognitive categories. Using fixed sentence embeddings from multiple transformer models, we evaluate the recoverability of these annotations via linear and shallow nonlinear probes. Across models, both continuous scores and tier labels are reliably decodable, with shallow nonlinear probes providing consistent performance gains over linear probes. Lexical TF-IDF baselines perform substantially worse, indicating that the observed structure is not attributable to surface word statistics alone. Nonparametric permutation tests further confirm that probe performance exceeds chance under label-randomization nulls. Qualitative analyses using UMAP visualizations and confusion matrices reveal smooth low-to-high gradients and predominantly adjacent-tier confusions in embedding space. Taken together, these results provide evidence that transformer embedding spaces exhibit a hierarchical geometric organization aligned with human-defined cognitive attributes, while remaining agnostic to claims of internal awareness or phenomenology.

</details>


### [232] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 提出SmartSnap范式，将任务验证从被动后验转变为主动自验证，通过智能体自主收集最小决定性快照证据，提升GUI任务中强化学习的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有任务验证方法（如规则脚本、奖励模型或LLM评判）依赖冗长轨迹分析，存在成本高、可靠性低的问题，制约了智能体强化学习在复杂GUI任务中的扩展。

Method: 设计自验证智能体，遵循3C原则（完整性、简洁性、创造性），在任务执行过程中主动收集关键快照证据；使用通用LLM评判器基于快照验证任务完成。

Result: 在移动任务实验中，SmartSnap使8B和30B模型性能分别提升26.08%和16.66%，自验证智能体性能可媲美DeepSeek V3.1和Qwen3-235B-A22B。

Conclusion: SmartSnap通过解决方案寻找与证据寻求的协同，实现了高效可扩展的自验证智能体训练范式，为复杂GUI任务中的自主智能体发展提供了新路径。

Abstract: Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


### [233] [Towards Efficient Post-Training via Fourier-Driven Adapter Architectures](https://arxiv.org/abs/2512.22378)
*Donggyun Bae,Jongil Park*

Main category: cs.CL

TL;DR: 提出Fourier-Activated Adapter（FAA）框架，通过随机傅里叶特征分解表示，实现参数高效微调，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效微调方法在平衡性能与计算开销方面存在局限，需要一种能选择性调制语义信息并保持主干模型表示能力的新方法。

Method: 在轻量适配器中引入随机傅里叶特征，将中间表示分解为互补的低频和高频分量，通过频率感知激活和自适应加权机制进行调制。

Result: 在GLUE、E2E NLG和指令微调基准测试中，FAA性能优于或媲美现有方法，同时保持较低的计算和内存开销。消融实验验证了频率感知机制的有效性。

Conclusion: FAA是一种鲁棒且高效的预训练语言模型后训练方法，通过频率感知调制实现了性能与效率的平衡。

Abstract: We propose a novel framework, termed Fourier-Activated Adapter (FAA), for parameter-efficient fine-tuning of large pre-trained language models. By incorporating random Fourier features into lightweight adapter modules, FAA decomposes intermediate representations into complementary low- and high-frequency components, enabling frequency-aware modulation of semantic information. This design allows the model to selectively emphasize informative frequency bands during adaptation while preserving the representational capacity of the frozen backbone. Extensive experiments on GLUE, E2E NLG, and instruction-tuning benchmarks demonstrate that FAA consistently achieves competitive or superior performance compared to existing parameter-efficient fine-tuning methods, while maintaining low computational and memory overhead. Ablation studies further verify the effectiveness of frequency-aware activation and adaptive weighting mechanisms, highlighting FAA as a robust and efficient approach for post-training large language models.

</details>


### [234] [The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach](https://arxiv.org/abs/2512.22376)
*Zubaida Mohammed Albadani,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: 本文在极简方案框架下研究也门伊比阿拉伯语中qulk-从句的句法结构，提出qulk-从句是双层结构，qulk作为从句嵌入谓词选择完整CP补足语，并通过核心句法操作和形态合并等过程推导其生成机制。


<details>
  <summary>Details</summary>
Motivation: 研究旨在分析也门伊比阿拉伯语中形态融合的qulk-从句（意为“我说”）的句法特性，该结构可引入陈述、疑问和祈使从句且常无标句词，其生成机制和方言特征（如二分否定、附着化等）尚未在极简方案下得到系统解释。

Method: 采用极简方案理论框架，运用合并、移位、一致、拼读等核心句法操作，结合形态合并等句后过程，对qulk-从句进行分层句法分析，并考察其与方言特征的互动。

Result: 提出qulk-从句为双层结构，qulk作为谓词选择完整CP补足语；推导过程可通过标准计算步骤和形态合并实现；分析能统一解释二分否定、附着化、CP嵌入等方言特征。

Conclusion: 研究为生成句法（特别是极简方案）提供理论贡献，提出可将分析扩展至addressee-从句（kil-k“你说”），并探讨极简方案普遍性的可能性。

Abstract: This study investigates the syntax of qulk-clauses in Yemeni Ibbi Arabic (YIA) within the Minimalist Program. The construction qulk-clause, a morphologically fused form meaning 'I said,' introduces embedded declarative interrogative, and imperative clauses, often eithout complementizer. The central proposal of this paper is that qulk-clauses are biclausal structures in which qulk functions a clause-embedding predicate sec;ecting a dull CP complement. By applying core minimalist operations, viz., Merge, Move, Agree, and Spell-out, the study provides a layered syntactic analysis of qulk-clauses, for illustrating how their derivation proceeds through standard computational steps and post-syntactic processes such as Morphological Merger. The proposal also accounts for dialect-specific features like bipartite negation, cliticization, and CP embedding. The findings offer theoretical contributions to generative syntax, specifically minimalism. The study concludes raising theoretical questions concerning extending the analysis to the addressee-clause kil-k 'you said'. It also provides insights into the possibility of the universality of minimalism.

</details>


### [235] [LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition](https://arxiv.org/abs/2512.22385)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: 提出LLM引导的范例选择框架，通过语义推理改进可穿戴传感器活动识别，在少样本条件下显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 现有HAR方法依赖大量标注数据和纯几何范例选择，难以区分相似活动（如行走、上下楼梯），需要更智能的范例选择机制。

Method: 结合LLM生成的知识先验（特征重要性、类间混淆度、范例预算乘数），融合边界验证线索、PageRank中心性、枢纽惩罚和设施选址优化进行范例评分与选择。

Result: 在UCI-HAR数据集少样本条件下达到88.78%的宏F1分数，优于随机采样、聚类中心和k-center等传统方法。

Conclusion: LLM语义先验与结构几何线索结合，能为少样本可穿戴传感器HAR提供更具代表性的范例选择基础。

Abstract: In this paper, we propose an LLM-Guided Exemplar Selection framework to address a key limitation in state-of-the-art Human Activity Recognition (HAR) methods: their reliance on large labeled datasets and purely geometric exemplar selection, which often fail to distinguish similar weara-ble sensor activities such as walking, walking upstairs, and walking downstairs. Our method incorporates semantic reasoning via an LLM-generated knowledge prior that captures feature importance, inter-class confusability, and exemplar budget multipliers, and uses it to guide exemplar scoring and selection. These priors are combined with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization to obtain a compact and informative set of exemplars. Evaluated on the UCI-HAR dataset under strict few-shot conditions, the framework achieves a macro F1-score of 88.78%, outperforming classical approaches such as random sampling, herding, and $k$-center. The results show that LLM-derived semantic priors, when integrated with structural and geometric cues, provide a stronger foundation for selecting representative sensor exemplars in few-shot wearable-sensor HAR.

</details>


### [236] [Hallucination Detection and Evaluation of Large Language Model](https://arxiv.org/abs/2512.22416)
*Chenggong Zhang,Haopeng Wang*

Main category: cs.CL

TL;DR: 本文提出了一种轻量级的幻觉检测框架HHEM，显著提升了检测效率，并通过分段检索改进了对摘要任务中局部幻觉的检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型幻觉检测方法（如KnowHalu）计算成本高，需要一种更高效且准确的评估框架来提升检测效率。

Method: 采用Hughes幻觉评估模型（HHEM）作为轻量级分类框架，结合分段检索技术验证文本局部内容，并在问答和摘要任务上比较不同模型的TPR、TNR和准确率。

Result: HHEM将评估时间从8小时缩短至10分钟，在非捏造检查下达到82.2%的最高准确率和78.9%的TPR；大参数模型（7B-9B）幻觉更少，中等规模模型稳定性较差。

Conclusion: 需要构建兼顾计算效率与事实验证的结构化评估框架，以提升大语言模型生成内容的可靠性，分段检索可有效改善局部幻觉检测。

Abstract: Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework that operates independently of LLM-based judgments, significantly improving efficiency while maintaining high detection accuracy. We conduct a comparative analysis of hallucination detection methods across various LLMs, evaluating True Positive Rate (TPR), True Negative Rate (TNR), and Accuracy on question-answering (QA) and summarization tasks. Our results show that HHEM reduces evaluation time from 8 hours to 10 minutes, while HHEM with non-fabrication checking achieves the highest accuracy \(82.2\%\) and TPR \(78.9\%\). However, HHEM struggles with localized hallucinations in summarization tasks. To address this, we introduce segment-based retrieval, improving detection by verifying smaller text components. Additionally, our cumulative distribution function (CDF) analysis indicates that larger models (7B-9B parameters) generally exhibit fewer hallucinations, while intermediate-sized models show higher instability. These findings highlight the need for structured evaluation frameworks that balance computational efficiency with robust factual validation, enhancing the reliability of LLM-generated content.

</details>


### [237] [HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG](https://arxiv.org/abs/2512.22442)
*Cattalyya Nuengsigkapian*

Main category: cs.CL

TL;DR: HiFi-RAG是一个在MMU-RAGent竞赛中获胜的检索增强生成系统，通过分层过滤和多阶段流程解决开放域RAG中的无关信息和对齐问题。


<details>
  <summary>Details</summary>
Motivation: 开放域检索增强生成面临检索文档包含无关信息以及生成答案与用户意图对齐的挑战，需要更高效的解决方案。

Method: 采用多阶段管道，使用Gemini 2.5 Flash进行查询构建、分层内容过滤和引用归因，保留Gemini 2.5 Pro进行最终答案生成，实现成本效益与推理能力的平衡。

Result: 在MMU-RAGent验证集上，ROUGE-L提升19.6%至0.274，DeBERTaScore提升6.2%至0.677；在测试集上，ROUGE-L提升57.4%，DeBERTaScore提升14.9%。

Conclusion: HiFi-RAG通过分层过滤和混合模型策略显著提升了开放域RAG的性能，证明了多阶段处理在解决无关信息和对齐问题上的有效性。

Abstract: Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.

</details>


### [238] [Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.22443)
*Jie Zhou,Xin Chen,Jie Zhang,Zhe Li*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型在会计领域的推理能力，通过建立垂直领域会计推理评估标准，测试了多个模型在会计任务上的表现，发现GPT-4表现最佳但仍需优化才能满足实际应用需求。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在各领域的广泛应用，如何将其有效融入会计专业领域并重新定义模型与领域应用的关系，成为推动企业数字化转型和社会发展的关键挑战。

Method: 提出垂直领域会计推理概念，基于GLM系列模型的训练数据特征建立评估标准，采用不同提示工程策略测试GLM-6B、GLM-130B、GLM-4和GPT-4等模型在会计推理任务上的表现。

Result: 实验表明不同提示策略对模型性能提升程度不同，GPT-4展现出最强的会计推理能力，但当前所有模型仍无法完全满足实际应用需求，特别是在企业级会计场景中需要进一步优化。

Conclusion: 大型语言模型在会计领域具有应用潜力，但现有模型距离实际部署要求仍有差距，需要针对企业级会计场景进行专门优化才能充分发挥其价值。

Abstract: Large Language Models (LLMs) are reshaping learning paradigms, cognitive processes, and research methodologies across a wide range of domains. Integrating LLMs with professional fields and redefining the relationship between LLMs and domain-specific applications has become a critical challenge for promoting enterprise digital transformation and broader social development. To effectively integrate LLMs into the accounting domain, it is essential to understand their domain-specific reasoning capabilities. This study introduces the concept of vertical-domain accounting reasoning and establishes evaluation criteria by analyzing the training data characteristics of representative GLM-series models. These criteria provide a foundation for subsequent research on reasoning paradigms and offer benchmarks for improving accounting reasoning performance. Based on this framework, we evaluate several representative models, including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4, on a set of accounting reasoning tasks. Experimental results show that different prompt engineering strategies lead to varying degrees of performance improvement across models, with GPT-4 achieving the strongest accounting reasoning capability. However, current LLMs still fall short of real-world application requirements. In particular, further optimization is needed for deployment in enterprise-level accounting scenarios to fully realize the potential value of LLMs in this domain.

</details>


### [239] [Constituency Structure over Eojeol in Korean Treebanks](https://arxiv.org/abs/2512.22487)
*Jungyeul Park,Chulwoo Park*

Main category: cs.CL

TL;DR: 本文主张韩语选区树库应采用以eojeol（韩语词汇单位）为基础的选区表示法，将形态切分和细粒度词性信息编码在独立的非选区层中，从而解决现有树库中形态与句法结构混淆的问题。


<details>
  <summary>Details</summary>
Motivation: 现有韩语选区树库以语素为终端单位，混淆了词内形态与短语层句法结构，且与基于eojeol的依存资源不匹配，这引发了关于终端单位选择的基础表征问题。

Method: 提出基于eojeol的选区表示方案，将形态信息置于独立非选区层；通过比较分析，在明确归一化假设下证明世宗树库和宾大韩语树库在eojeol选区层具有表征等价性。

Result: 世宗树库和宾大韩语树库在eojeol选区层可视为表征等价；基于此设计了一套支持跨树库比较和选区-依存转换的eojeol标注方案。

Conclusion: 基于eojeol的选区表示能保持可解释的选区结构，促进跨树库比较和选区-依存转换，为韩语树库设计提供了更合理的表征框架。

Abstract: The design of Korean constituency treebanks raises a fundamental representational question concerning the choice of terminal units. Although Korean words are morphologically complex, treating morphemes as constituency terminals conflates word internal morphology with phrase level syntactic structure and creates mismatches with eojeol based dependency resources. This paper argues for an eojeol based constituency representation, with morphological segmentation and fine grained part of speech information encoded in a separate, non constituent layer. A comparative analysis shows that, under explicit normalization assumptions, the Sejong and Penn Korean treebanks can be treated as representationally equivalent at the eojeol based constituency level. Building on this result, we outline an eojeol based annotation scheme that preserves interpretable constituency and supports cross treebank comparison and constituency dependency conversion.

</details>


### [240] [ManchuTTS: Towards High-Quality Manchu Speech Synthesis via Flow Matching and Hierarchical Text Representation](https://arxiv.org/abs/2512.22491)
*Suhua Wang,Zifan Wang,Xiaoxin Sun,D. J. Wang,Zhanbo Liu,Xin Li*

Main category: cs.CL

TL;DR: 提出ManchuTTS系统，针对濒危语言满语的语音合成挑战，通过三层次文本表示、跨模态分层注意力机制和分层对比损失，在低资源条件下实现高质量合成。


<details>
  <summary>Details</summary>
Motivation: 满语作为濒危语言，面临严重的数据稀缺和强黏着性语音学特征，传统语音合成方法难以有效处理其复杂的形态结构。

Method: 设计三层次文本表示（音素、音节、韵律）；采用跨模态分层注意力机制进行多粒度对齐；结合深度卷积网络与流匹配Transformer进行非自回归生成；引入分层对比损失；构建首个满语TTS数据集并采用数据增强策略。

Result: 在5.2小时训练子集上获得4.52 MOS分数，显著优于基线模型；消融实验显示分层指导使黏着词发音准确率提升31%，韵律自然度提升27%。

Conclusion: ManchuTTS通过层次化建模有效解决了满语合成的数据稀缺和黏着性问题，为低资源黏着性语言的语音合成提供了可行方案。

Abstract: As an endangered language, Manchu presents unique challenges for speech synthesis, including severe data scarcity and strong phonological agglutination. This paper proposes ManchuTTS(Manchu Text to Speech), a novel approach tailored to Manchu's linguistic characteristics. To handle agglutination, this method designs a three-tier text representation (phoneme, syllable, prosodic) and a cross-modal hierarchical attention mechanism for multi-granular alignment. The synthesis model integrates deep convolutional networks with a flow-matching Transformer, enabling efficient, non-autoregressive generation. This method further introduce a hierarchical contrastive loss to guide structured acoustic-linguistic correspondence. To address low-resource constraints, This method construct the first Manchu TTS dataset and employ a data augmentation strategy. Experiments demonstrate that ManchuTTS attains a MOS of 4.52 using a 5.2-hour training subset derived from our full 6.24-hour annotated corpus, outperforming all baseline models by a notable margin. Ablations confirm hierarchical guidance improves agglutinative word pronunciation accuracy (AWPA) by 31% and prosodic naturalness by 27%.

</details>


### [241] [Learning When Not to Attend Globally](https://arxiv.org/abs/2512.22562)
*Xuan Luo,Kailai Zhang,Xifeng Yan*

Main category: cs.CL

TL;DR: 提出All-or-Here Attention（AHA）机制，通过二元路由器动态切换全局注意力与局部滑动窗口注意力，在保持性能的同时显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 受人类阅读时仅必要时回溯前文的启发，探索大语言模型动态决定何时需要全局上下文的能力，以减少冗余计算。

Method: 为每个注意力头设计二元路由器，根据当前token需求动态选择完整注意力或局部滑动窗口注意力（窗口大小可调）。

Result: 在256token窗口下，93%的完整注意力操作可被滑动窗口替代且性能无损；上下文依赖呈长尾分布，全局注意力需求随窗口扩大快速衰减。

Conclusion: 完整注意力存在大量冗余，通过解耦局部处理与全局访问，可实现按需调用全局上下文的高效推理。

Abstract: When reading books, humans focus primarily on the current page, flipping back to recap prior context only when necessary. Similarly, we demonstrate that Large Language Models (LLMs) can learn to dynamically determine when to attend to global context. We propose All-or-Here Attention (AHA), which utilizes a binary router per attention head to dynamically toggle between full attention and local sliding window attention for each token. Our results indicate that with a window size of 256 tokens, up to 93\% of the original full attention operations can be replaced by sliding window attention without performance loss. Furthermore, by evaluating AHA across various window sizes, we identify a long-tail distribution in context dependency, where the necessity for full attention decays rapidly as the local window expands. By decoupling local processing from global access, AHA reveals that full attention is largely redundant, and that efficient inference requires only on-demand access to the global context.

</details>


### [242] [Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis](https://arxiv.org/abs/2512.22603)
*Zhiqiang Gao,Shihao Gao,Zixing Zhang,Yihao Guo,Hongyu Chen,Jing Han*

Main category: cs.CL

TL;DR: 本文提出了一种基于大语言模型的结构化提示管道和集成方法，用于多模态对话中的情感分析，包括情感六元组提取和情感翻转检测两个子任务。


<details>
  <summary>Details</summary>
Motivation: 多模态对话中的情感理解是构建情感智能AI系统的关键挑战，MCABSA挑战赛旨在推动这一领域的研究，特别是情感元素的细粒度提取和动态情感变化的检测。

Method: 针对子任务一，设计了结构化提示管道，引导大语言模型按顺序提取情感六元组；针对子任务二，通过集成三种大语言模型的优势，鲁棒地识别情感转换及其触发因素。

Result: 系统在子任务一上获得了47.38%的平均得分，在子任务二上获得了74.12%的精确匹配F1分数，证明了逐步细化和集成策略在多模态情感分析任务中的有效性。

Conclusion: 通过结构化提示和模型集成，大语言模型能够有效处理复杂多模态对话中的细粒度情感分析任务，为情感智能AI系统的发展提供了可行方案。

Abstract: Understanding sentiment in multimodal conversations is a complex yet crucial challenge toward building emotionally intelligent AI systems. The Multimodal Conversational Aspect-based Sentiment Analysis (MCABSA) Challenge invited participants to tackle two demanding subtasks: (1) extracting a comprehensive sentiment sextuple, including holder, target, aspect, opinion, sentiment, and rationale from multi-speaker dialogues, and (2) detecting sentiment flipping, which detects dynamic sentiment shifts and their underlying triggers. For Subtask-I, in the present paper, we designed a structured prompting pipeline that guided large language models (LLMs) to sequentially extract sentiment components with refined contextual understanding. For Subtask-II, we further leveraged the complementary strengths of three LLMs through ensembling to robustly identify sentiment transitions and their triggers. Our system achieved a 47.38% average score on Subtask-I and a 74.12% exact match F1 on Subtask-II, showing the effectiveness of step-wise refinement and ensemble strategies in rich, multimodal sentiment analysis tasks.

</details>


### [243] [Chain-of-thought Reviewing and Correction for Time Series Question Answering](https://arxiv.org/abs/2512.22627)
*Chen Su,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: 提出了T3LLM框架，通过三个LLM（工作者、审阅者、学生）的协作实现时间序列问答的多步推理与显式纠错，在多个基准测试中达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列分析方法易在处理复杂数值序列时产生推理错误，而时间序列数据具有可验证性，支持推理步骤与原始输入的一致性检查。

Method: 采用三LLM协作框架：工作者生成结构化提示下的逐步思维链；审阅者检查推理、识别错误步骤并提供修正意见；学生模型通过修正后的思维链进行微调，内化多步推理与自我纠正能力。

Result: 在多个真实世界时间序列问答基准测试中，T3LLM超越了现有强LLM基线，实现了最先进的性能。

Conclusion: T3LLM框架通过显式纠错机制和多模型协作，有效提升了LLM在时间序列问答任务中的推理准确性和可靠性。

Abstract: With the advancement of large language models (LLMs), diverse time series analysis tasks are reformulated as time series question answering (TSQA) through a unified natural language interface. However, existing LLM-based approaches largely adopt general natural language processing techniques and are prone to reasoning errors when handling complex numerical sequences. Different from purely textual tasks, time series data are inherently verifiable, enabling consistency checking between reasoning steps and the original input. Motivated by this property, we propose T3LLM, which performs multi-step reasoning with an explicit correction mechanism for time series question answering. The T3LLM framework consists of three LLMs, namely, a worker, a reviewer, and a student, that are responsible for generation, review, and reasoning learning, respectively. Within this framework, the worker generates step-wise chains of thought (CoT) under structured prompts, while the reviewer inspects the reasoning, identifies erroneous steps, and provides corrective comments. The collaboratively generated corrected CoT are used to fine-tune the student model, internalizing multi-step reasoning and self-correction into its parameters. Experiments on multiple real-world TSQA benchmarks demonstrate that T3LLM achieves state-of-the-art performance over strong LLM-based baselines.

</details>


### [244] [M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation](https://arxiv.org/abs/2512.22628)
*Fanglin Xu,Wei Zhang,Jian Yang,Guo Chen,Aishan Liu,Zhoujun Li,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: 提出了M2G-Eval框架，用于在多粒度（类、函数、块、行）和多语言（18种编程语言）场景下评估代码大语言模型的生成能力，发现任务难度存在层级结构，且模型能学习可迁移的编程概念。


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估基准多局限于单一结构粒度和少数编程语言，难以揭示模型在不同代码粒度和多语言场景下的细粒度能力差异。

Method: 构建包含17K+训练任务和1,286个人工标注测试实例的多粒度多语言评估框架M2G-Eval；基于Qwen3-8B模型，通过监督微调和组相对策略优化训练M2G-Eval-Coder模型；评估30个模型（包括28个SOTA模型和2个自研变体）。

Result: 发现三个主要结论：（1）任务难度存在明显层级：行级任务最简单，类级任务最具挑战性；（2）随着任务复杂度增加，完整粒度语言与部分粒度语言的性能差距扩大；（3）跨语言任务表现强相关，表明模型学习了可迁移的编程概念。

Conclusion: M2G-Eval能够对代码生成能力进行细粒度诊断，并突显了模型在合成复杂长代码方面仍存在持续挑战。

Abstract: The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.

</details>


### [245] [On the Role of Discreteness in Diffusion LLMs](https://arxiv.org/abs/2512.22630)
*Ziqi Jin,Bin Wang,Xiang Lin,Lidong Bing,Aixin Sun*

Main category: cs.CL

TL;DR: 本文分析了扩散模型在文本生成中的应用，指出当前连续和离散扩散方法均存在结构上的权衡，并识别出两个核心问题：均匀噪声不尊重文本信息分布，以及单token训练无法捕捉并行解码中的多token依赖关系。


<details>
  <summary>Details</summary>
Motivation: 扩散模型具有并行解码和迭代优化等优势，但文本的离散性和高度结构化特性使得扩散原理难以直接应用。本文旨在通过分析扩散过程与语言建模的关系，揭示现有方法的局限性，并为构建更符合文本结构的扩散语言模型提供方向。

Method: 1. 从扩散过程和语言建模的视角重新审视扩散语言模型；2. 提出区分扩散机制与语言特定需求的五个属性；3. 将现有方法分类为嵌入空间的连续扩散和token上的离散扩散；4. 分析近期大型扩散语言模型，识别核心问题。

Result: 1. 连续扩散和离散扩散方法均只能部分满足五个关键属性，存在结构性权衡；2. 发现两个核心问题：（i）均匀噪声破坏未考虑文本信息的位置分布特性，（ii）基于token边缘的训练无法在并行解码中建模多token依赖关系。

Conclusion: 需要设计更贴合文本结构的扩散过程，未来的扩散语言模型应致力于解决信息分布对齐和多token依赖建模问题，以提升生成文本的连贯性。

Abstract: Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.

</details>


### [246] [Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2512.22631)
*Hadi Mohammadi,Tamas Kozak,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 评估GRPO和DPO两种优化方法在提升大语言模型思维链推理忠实度方面的效果，发现GRPO在较大模型中表现更优，能更好地改善推理过程的透明度与可信度。


<details>
  <summary>Details</summary>
Motivation: 现有思维链推理常与模型实际推理过程不一致，可能产生误导性解释或隐藏外部提示的影响，这削弱了其在安全监督和对齐监测中的可靠性，需探索提升推理忠实度的方法。

Method: 采用Group Relative Policy Optimization（GRPO）和Direct Preference Optimization（DPO）两种优化方法，在多个规模的语言模型上进行实验，评估它们对思维链忠实度的改进效果。

Result: GRPO在较大模型中表现优于DPO，Qwen2.5-14B-Instruct模型在所有评估指标上取得最佳结果；两种方法均显示模型规模与性能正相关，但GRPO在忠实度指标上提升潜力更大，不过在较小规模时稳定性较差。

Conclusion: GRPO为开发更透明、可信的大语言模型推理提供了有前景的方向，有助于解决思维链推理中解释与真实推理过程脱节的问题。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful technique for improving the problem-solving capabilities of large language models (LLMs), particularly for tasks requiring multi-step reasoning. However, recent studies show that CoT explanations often fail to reflect the model's actual reasoning process, as models may produce coherent yet misleading justifications or modify answers without acknowledging external cues. Such discrepancies undermine the reliability of CoT-based methods for safety supervision and alignment monitoring, as models can generate plausible but deceptive rationales for incorrect answers. To better understand this limitation, we evaluate two optimization methods, Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO), in their ability to improve CoT faithfulness. Our experiments show that GRPO achieves higher performance than DPO in larger models, with the Qwen2.5-14B-Instruct model attaining the best results across all evaluation metrics. Both approaches exhibit positive correlations between model size and performance, but GRPO shows greater potential for improving faithfulness metrics, albeit with less stable behavior at smaller scales. These results suggest that GRPO offers a promising direction for developing more transparent and trustworthy reasoning in LLMs.

</details>


### [247] [Fragile Knowledge, Robust Instruction-Following: The Width Pruning Dichotomy in Llama-3.2](https://arxiv.org/abs/2512.22671)
*Pere Martra*

Main category: cs.CL

TL;DR: 通过MAW准则对GLU-MLP层进行结构化宽度剪枝，发现剪枝对不同模型能力产生选择性影响：指令跟随能力显著提升，多步推理保持稳健，而参数化知识任务性能下降。研究揭示了扩展比作为关键架构参数的选择性调节作用，并首次系统描述了这种选择性保留现象。


<details>
  <summary>Details</summary>
Motivation: 挑战传统剪枝导致性能均匀下降的假设，探索结构化宽度剪枝如何选择性地影响大语言模型的不同认知能力，特别是发现剪枝可能在某些能力上产生提升效果。

Method: 采用基于最大绝对权重（MAW）准则的结构化宽度剪枝方法，针对GLU-MLP层设计七种扩展比配置，使用综合基准测试评估事实知识、数学推理、语言理解、指令跟随和真实性等能力。

Result: 1）指令跟随能力大幅提升（IFEval指标提升46%-75%）；2）多步推理保持稳健；3）事实知识任务性能下降；4）发现事实知识能力（MMLU）与真实性指标（TruthfulQA-MC2）之间存在强负相关（r=-0.864）；5）剪枝配置可实现高达23%的能耗降低，但单请求延迟增加。

Conclusion: MAW引导的宽度剪枝充当选择性过滤器，减少参数化知识的同时保留或增强行为对齐能力。扩展比是调节认知能力的关键架构参数，而非仅仅是压缩指标。该研究连接了知识容量和真实性两个独立研究领域，揭示了模型能力之间的内在权衡关系。

Abstract: Structured width pruning of GLU-MLP layers, guided by the Maximum Absolute Weight (MAW) criterion, reveals a systematic dichotomy in how reducing the expansion ratio affects different model capabilities. While performance on tasks relying on parametric knowledge (e.g., MMLU, GSM8K) and perplexity metrics degrades predictably, instruction-following capabilities improve substantially (+46% to +75% in IFEval for Llama-3.2-1B and 3B models), and multi-step reasoning remains robust (MUSR). This pattern challenges the prevailing assumption that pruning induces uniform degradation. We evaluated seven expansion ratio configurations using comprehensive benchmarks assessing factual knowledge, mathematical reasoning, language comprehension, instruction-following, and truthfulness. Our analysis identifies the expansion ratio as a critical architectural parameter that selectively modulates cognitive capabilities, rather than merely serving as a compression metric. We provide the first systematic characterization of this selective preservation phenomenon. Notably, we document a robust inverse correlation (r = -0.864, p = 0.012 in Llama-3B) between factual knowledge capacity (MMLU) and truthfulness metrics (TruthfulQA-MC2): as knowledge degrades, the model's ability to discriminate misconceptions improves consistently. This connects two previously distinct research areas, demonstrating that MAW-guided width pruning acts as a selective filter, reducing parametric knowledge while preserving or enhancing behavioral alignment. Additionally, we quantify context-dependent efficiency trade-offs: pruned configurations achieve up to 23% reduction in energy consumption (J/token) but incur penalties in single-request latency, whereas batch processing workloads benefit uniformly.

</details>


### [248] [Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency](https://arxiv.org/abs/2512.22682)
*Yoshith Roy Kotla,Varshith Roy Kotla*

Main category: cs.CL

TL;DR: 提出词汇感知的保形预测框架，通过语义掩码和温度调整评分，在保持边际覆盖的同时大幅减少预测集大小，解决大词汇量语言模型不确定性量化中的覆盖效率权衡问题。


<details>
  <summary>Details</summary>
Motivation: 在关键领域部署大语言模型需要可靠的不确定性量化，但标准的softmax概率校准效果差，且传统保形预测方法产生的预测集包含数百个标记，信息量低。

Method: 提出词汇感知保形预测框架，结合语义掩码和温度调整评分来减少有效预测空间，同时理论保证边际覆盖。在Gemma-2B模型上使用SQUAD和WikiText基准进行实验验证。

Result: VACP实现了89.7%的经验覆盖率（目标90%），同时将平均预测集大小从847个标记减少到4.3个标记，效率提升197倍。

Conclusion: VACP框架有效解决了大词汇量语言模型中的覆盖效率权衡问题，在保持理论覆盖保证的同时显著提高了预测集的实用性，为LLM的不确定性量化提供了可行方案。

Abstract: Deploying large language models (LLMs) in high-stakes domains requires rigorous uncertainty quantification, yet standard softmax probabilities are often poorly calibrated. We present a systematic study of Adaptive Prediction Sets (APS) applied to next-token prediction in transformer-based models with large vocabularies (greater than 250,000 tokens). Our central contribution is the identification of a coverage-efficiency tradeoff: while naive conformal prediction achieves valid coverage, it produces prediction sets of hundreds of tokens, rendering them uninformative. We propose Vocabulary-Aware Conformal Prediction (VACP), a framework that leverages semantic masking and temperature-adjusted scoring to reduce the effective prediction space while provably maintaining marginal coverage. Experiments on Gemma-2B using SQUAD and WikiText benchmarks demonstrate that VACP achieves 89.7 percent empirical coverage (90 percent target) while reducing the mean prediction set size from 847 tokens to 4.3 tokens -- a 197x improvement in efficiency. We provide a theoretical analysis of vocabulary reduction and release our implementation for reproducibility.

</details>


### [249] [GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2512.22705)
*Ahmed Abdullah,Sana Fatima,Haroon Mahmood*

Main category: cs.CL

TL;DR: 本文提出了一种多语言希望言论检测框架，重点关注乌尔都语，使用预训练Transformer模型在PolyHope-M 2025基准测试中取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 希望言论在自然语言处理中代表性不足，现有研究主要集中在英语，导致乌尔都语等低资源语言缺乏相关工具，限制了促进积极在线交流的能力。

Method: 使用XLM-RoBERTa、mBERT、EuroBERT和UrduBERT等预训练Transformer模型，通过简单预处理训练分类器进行希望言论检测。

Result: 在PolyHope-M 2025基准测试中，乌尔都语二分类F1-score达95.2%，多分类达65.2%，在西班牙语、德语和英语中也取得了有竞争力的结果。

Conclusion: 研究表明现有多语言模型可在低资源环境中有效应用，便于识别希望言论，有助于构建更具建设性的数字话语环境。

Abstract: Hope speech has been relatively underrepresented in Natural Language Processing (NLP). Current studies are largely focused on English, which has resulted in a lack of resources for low-resource languages such as Urdu. As a result, the creation of tools that facilitate positive online communication remains limited. Although transformer-based architectures have proven to be effective in detecting hate and offensive speech, little has been done to apply them to hope speech or, more generally, to test them across a variety of linguistic settings. This paper presents a multilingual framework for hope speech detection with a focus on Urdu. Using pretrained transformer models such as XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, we apply simple preprocessing and train classifiers for improved results. Evaluations on the PolyHope-M 2025 benchmark demonstrate strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similarly competitive results in Spanish, German, and English. These results highlight the possibility of implementing existing multilingual models in low-resource environments, thus making it easier to identify hope speech and helping to build a more constructive digital discourse.

</details>


### [250] [Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages](https://arxiv.org/abs/2512.22712)
*Anaelia Ovalle,Candace Ross,Sebastian Ruder,Adina Williams,Karen Ullrich,Mark Ibrahim,Levent Sagun*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在多语言推理中存在推理与结论不一致的盲点，非拉丁文字的错误率是拉丁文字的两倍以上，现有评估方法无法全面反映模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注链式思维提示下的推理能力，但缺乏对多语言环境下推理质量是否一致的探索，需要验证模型生成的推理过程是否真正逻辑支持其结论。

Method: 构建人工验证框架，分析6种语言、6个前沿模型在GlobalMMLU数据集上的6.5万条推理轨迹，通过人工标注建立错误分类体系。

Result: 1. 模型任务准确率高但推理与结论常不匹配；2. 非拉丁文字推理轨迹的错误率是拉丁文字的两倍以上；3. 错误主要来自证据错误（无依据主张、模糊事实）和逻辑推理步骤问题。

Conclusion: 当前多语言评估方法无法完整反映模型推理能力，需要建立推理感知的评估框架来检测推理与结论的一致性。

Abstract: Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.

</details>


### [251] [Mitigating Social Desirability Bias in Random Silicon Sampling](https://arxiv.org/abs/2512.22725)
*Sashank Chapala,Maksym Mironov,Songgaojun Deng*

Main category: cs.CL

TL;DR: 本文研究了如何通过心理学的提示词设计来减轻大语言模型在模拟人口响应时表现出的社会期望偏差，发现重新表述的提示能有效改善与真实人类数据的对齐。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在模拟人口响应时经常表现出社会期望偏差，即倾向于给出社会可接受的答案，而非真实的人类数据。现有研究对此关注有限，因此探索通过提示词设计来缓解这种偏差。

Method: 使用美国国家选举研究数据，在两个模型系列（Llama-3.1和GPT-4.1-mini）上测试了四种基于提示的缓解方法：重新表述（中性、第三人称）、反向编码（语义反转）以及两种元指令（启动和序言）。通过Jensen-Shannon散度评估与人类数据的对齐程度。

Result: 重新表述的提示最有效地改善了与人类数据的对齐，减少了社会可接受答案的分布集中度。反向编码在不同项目上效果不一，而启动和序言方法鼓励了响应的一致性，但对偏差缓解没有系统性益处。

Conclusion: 基于提示的框架控制可以有效缓解大语言模型固有的社会期望偏差，为生成更具代表性的人工样本提供了实用路径。

Abstract: Large Language Models (LLMs) are increasingly used to simulate population responses, a method known as ``Silicon Sampling''. However, responses to socially sensitive questions frequently exhibit Social Desirability Bias (SDB), diverging from real human data toward socially acceptable answers. Existing studies on social desirability bias in LLM-based sampling remain limited. In this work, we investigate whether minimal, psychologically grounded prompt wording can mitigate this bias and improve alignment between silicon and human samples. We conducted a study using data from the American National Election Study (ANES) on three LLMs from two model families: the open-source Llama-3.1 series and GPT-4.1-mini. We first replicate a baseline silicon sampling study, confirming the persistent Social Desirability Bias. We then test four prompt-based mitigation methods: \emph{reformulated} (neutral, third-person phrasing), \emph{reverse-coded} (semantic inversion), and two meta-instructions, \emph{priming} and \emph{preamble}, respectively encouraging analytics and sincerity. Alignment with ANES is evaluated using Jensen-Shannon Divergence with bootstrap confidence intervals. Our results demonstrate that reformulated prompts most effectively improve alignment by reducing distribution concentration on socially acceptable answers and achieving distributions closer to ANES. Reverse-coding produced mixed results across eligible items, while the Priming and Preamble encouraged response uniformity and showed no systematic benefit for bias mitigation. Our findings validate the efficacy of prompt-based framing controls in mitigating inherent Social Desirability Bias in LLMs, providing a practical path toward more representative silicon samples.

</details>


### [252] [Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data](https://arxiv.org/abs/2512.22732)
*Md Badsha Biswas*

Main category: cs.CL

TL;DR: 本研究提出利用社交媒体数据（特别是Twitter）作为补充资源，通过自然语言处理管道自动识别和分类女性分享的妊娠经历，以增强现有关于不良妊娠结局的观察性研究数据集。


<details>
  <summary>Details</summary>
Motivation: 婴儿死亡率和出生缺陷在美国仍是重大公共卫生问题，现有研究对不良妊娠结局（如流产、死产、出生缺陷和早产）的理解和干预策略仍不充分，需要更全面的数据和研究方法。

Method: 构建自然语言处理（NLP）管道，对公开社交媒体数据进行预处理和数据增强，自动识别分享妊娠经历的女性，并根据报告结果分类：足月正常出生体重为阳性案例，报告不良妊娠结局为阴性案例。

Result: 研究展示了社交媒体数据作为流行病学调查辅助资源的可行性，提供了识别妊娠队列和对照组的框架，并支持评估特定干预措施或产前暴露对母婴健康结局的因果影响。

Conclusion: 社交媒体数据可作为现有数据集的补充，增强对不良妊娠结局的研究，为未来涉及妊娠人群的健康研究提供方法论框架，并拓展了流行病学调查的数据来源。

Abstract: Infant mortality remains a significant public health concern in the United States, with birth defects identified as a leading cause. Despite ongoing efforts to understand the causes of negative pregnancy outcomes like miscarriage, stillbirths, birth defects, and premature birth, there is still a need for more comprehensive research and strategies for intervention. This paper introduces a novel approach that uses publicly available social media data, especially from platforms like Twitter, to enhance current datasets for studying negative pregnancy outcomes through observational research. The inherent challenges in utilizing social media data, including imbalance, noise, and lack of structure, necessitate robust preprocessing techniques and data augmentation strategies. By constructing a natural language processing (NLP) pipeline, we aim to automatically identify women sharing their pregnancy experiences, categorizing them based on reported outcomes. Women reporting full gestation and normal birth weight will be classified as positive cases, while those reporting negative pregnancy outcomes will be identified as negative cases. Furthermore, this study offers potential applications in assessing the causal impact of specific interventions, treatments, or prenatal exposures on maternal and fetal health outcomes. Additionally, it provides a framework for future health studies involving pregnant cohorts and comparator groups. In a broader context, our research showcases the viability of social media data as an adjunctive resource in epidemiological investigations about pregnancy outcomes.

</details>


### [253] [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://arxiv.org/abs/2512.22737)
*Aiwei Liu,Minghua He,Shaoxun Zeng,Sijun Zhang,Linhao Zhang,Chuhan Wu,Wei Jia,Yuan Liu,Xiao Zhou,Jie Zhou*

Main category: cs.CL

TL;DR: 提出WeDLM，一种基于因果注意力的扩散语言模型解码框架，通过拓扑重排序和流式解码实现并行生成，在保持AR模型质量的同时显著提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 自回归解码因逐token生成限制并行性；现有扩散语言模型虽支持并行解码，但双向注意力机制破坏前缀KV缓存，导致重复计算，无法在实际部署中超越优化后的AR引擎（如vLLM）。

Method: 1. 基于因果注意力构建扩散解码框架，确保前缀缓存友好；2. 通过拓扑重排序将已观测token移至物理前缀并保持逻辑位置；3. 设计流式解码流程，持续提交置信token至增长的前缀，维持固定并行工作量。

Result: WeDLM在保持强AR骨干模型质量的同时实现显著加速：在复杂推理任务上接近3倍加速，在低熵生成场景下最高达10倍加速；在与vLLM部署环境匹配的对比中，扩散式解码首次在实际应用中超越优化AR引擎。

Conclusion: 通过因果注意力机制和流式解码设计，WeDLM证明了扩散式解码在实际部署中可超越优化AR引擎，为LLM高效推理提供了新方向。

Abstract: Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.

</details>


### [254] [Harnessing Large Language Models for Biomedical Named Entity Recognition](https://arxiv.org/abs/2512.22738)
*Jian Chen,Leilei Su,Cong Sun*

Main category: cs.CL

TL;DR: 提出BioSelectTune框架，通过数据质量优先的微调方法，在生物医学命名实体识别任务上实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在生物医学领域因缺乏领域知识和低质量训练数据导致性能下降，需要高效的数据中心化微调框架。

Method: 将BioNER重构为结构化JSON生成任务，采用混合超级过滤策略，使用同源弱模型蒸馏出紧凑高质量训练数据集。

Result: 在多个BioNER基准测试中达到最先进性能，仅使用50%精选正数据训练的模型超越完整训练基线和BioMedBERT等专业模型。

Conclusion: BioSelectTune通过数据质量优先的微调策略，显著提升LLMs在生物医学领域的适应性和性能，验证了数据质量优于数量的理念。

Abstract: Background and Objective: Biomedical Named Entity Recognition (BioNER) is a foundational task in medical informatics, crucial for downstream applications like drug discovery and clinical trial matching. However, adapting general-domain Large Language Models (LLMs) to this task is often hampered by their lack of domain-specific knowledge and the performance degradation caused by low-quality training data. To address these challenges, we introduce BioSelectTune, a highly efficient, data-centric framework for fine-tuning LLMs that prioritizes data quality over quantity. Methods and Results: BioSelectTune reformulates BioNER as a structured JSON generation task and leverages our novel Hybrid Superfiltering strategy, a weak-to-strong data curation method that uses a homologous weak model to distill a compact, high-impact training dataset. Conclusions: Through extensive experiments, we demonstrate that BioSelectTune achieves state-of-the-art (SOTA) performance across multiple BioNER benchmarks. Notably, our model, trained on only 50% of the curated positive data, not only surpasses the fully-trained baseline but also outperforms powerful domain-specialized models like BioMedBERT.

</details>


### [255] [Text-Routed Sparse Mixture-of-Experts Model with Explanation and Temporal Alignment for Multi-Modal Sentiment Analysis](https://arxiv.org/abs/2512.22741)
*Dongning Rao,Yunbiao Zeng,Zhihua Jiang,Jujian Lv*

Main category: cs.CL

TL;DR: 提出TEXT模型，通过多模态大语言模型生成解释并使用时序对齐网络，在多模态情感分析任务中取得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态情感分析方法未充分挖掘解释能力和时序对齐的潜力，需要更有效地整合不同模态信息以识别微妙情感。

Method: 1. 使用多模态大语言模型为情感分析生成解释；2. 设计时序导向的神经网络模块对齐音频和视频表示；3. 构建基于文本路由的稀疏专家混合模型进行门控融合。

Result: 在四个数据集上超越所有对比模型（包括三个最新方法和三个MLLM），在六个评估指标中至少四项领先，如在CH-SIMS数据集上MAE降至0.353（相对降低13.5%）。

Conclusion: TEXT模型通过解释增强和时序对齐有效提升多模态情感分析性能，证明了结合MLLM解释与跨模态时序建模的价值。

Abstract: Human-interaction-involved applications underscore the need for Multi-modal Sentiment Analysis (MSA). Although many approaches have been proposed to address the subtle emotions in different modalities, the power of explanations and temporal alignments is still underexplored. Thus, this paper proposes the Text-routed sparse mixture-of-Experts model with eXplanation and Temporal alignment for MSA (TEXT). TEXT first augments explanations for MSA via Multi-modal Large Language Models (MLLM), and then novelly aligns the epresentations of audio and video through a temporality-oriented neural network block. TEXT aligns different modalities with explanations and facilitates a new text-routed sparse mixture-of-experts with gate fusion. Our temporal alignment block merges the benefits of Mamba and temporal cross-attention. As a result, TEXT achieves the best performance cross four datasets among all tested models, including three recently proposed approaches and three MLLMs. TEXT wins on at least four metrics out of all six metrics. For example, TEXT decreases the mean absolute error to 0.353 on the CH-SIMS dataset, which signifies a 13.5% decrement compared with recently proposed approaches.

</details>


### [256] [Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language](https://arxiv.org/abs/2512.22778)
*Muhammad Zain Ali,Bernhard Pfahringer,Tony Smith*

Main category: cs.CL

TL;DR: 该研究探讨了领域自适应预训练对乌尔都语假新闻检测的效果，发现XLM-R模型在领域自适应后性能提升，而mBERT模型表现不一。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的错误信息检测研究多集中于高资源语言，乌尔都语等低资源语言关注不足，且多语言预训练模型在处理领域特定术语时表现不佳。

Method: 采用两阶段训练方法：首先对XLM-RoBERTa和mBERT进行领域自适应预训练（使用乌尔都语新闻语料），然后在下游假新闻分类任务上微调。

Result: 在四个乌尔都语假新闻数据集上的实验表明，领域自适应的XLM-R模型始终优于原始版本，而领域自适应的mBERT模型结果不稳定。

Conclusion: 领域自适应预训练能有效提升部分多语言模型在低资源语言假新闻检测任务上的性能，但效果因模型架构而异。

Abstract: Misinformation on social media is a widely acknowledged issue, and researchers worldwide are actively engaged in its detection. However, low-resource languages such as Urdu have received limited attention in this domain. An obvious approach is to utilize a multilingual pretrained language model and fine-tune it for a downstream classification task, such as misinformation detection. However, these models struggle with domain-specific terms, leading to suboptimal performance. To address this, we investigate the effectiveness of domain adaptation before fine-tuning for fake news classification in Urdu, employing a staged training approach to optimize model generalization. We evaluate two widely used multilingual models, XLM-RoBERTa and mBERT, and apply domain-adaptive pretraining using a publicly available Urdu news corpus. Experiments on four publicly available Urdu fake news datasets show that domain-adapted XLM-R consistently outperforms its vanilla counterpart, while domain-adapted mBERT exhibits mixed results.

</details>


### [257] [CNSight: Evaluation of Clinical Note Segmentation Tools](https://arxiv.org/abs/2512.22795)
*Risha Surana,Adrian Law,Sunwoo Kim,Rishab Sridhar,Angxiao Han,Peiyu Hong*

Main category: cs.CL

TL;DR: 评估多种方法在临床笔记分割任务上的表现，发现大型API模型（如GPT-5-mini）在整体性能上最优，轻量级基线方法在结构化任务中仍有竞争力。


<details>
  <summary>Details</summary>
Motivation: 临床笔记通常以非结构化或半结构化格式存储，这限制了其用于二次分析和下游临床应用的潜力。可靠的章节边界识别是结构化这些笔记的关键步骤，因为不同章节（如病史、用药、出院指导）提供不同的临床背景。

Method: 使用来自MIMIC-IV的1000条临床笔记数据集，评估了基于规则的基线方法、领域特定的Transformer模型以及大型语言模型在临床笔记分割任务上的表现。

Result: 大型API模型（如GPT-5-mini）在句子级别和自由文本分割任务中达到最佳平均F1分数72.4。轻量级基线方法在结构化句子级别任务中保持竞争力，但在非结构化自由文本任务中表现不佳。

Conclusion: 研究结果为方法选择提供了指导，并为下游任务（如信息提取、队列识别和自动摘要）奠定了基础。

Abstract: Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.

</details>


### [258] [AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning](https://arxiv.org/abs/2512.22857)
*Shihao Cai,Runnan Fang,Jialong Wu,Baixuan Li,Xinyu Wang,Yong Jiang,Liangcai Su,Liwen Zhang,Wenbiao Yin,Zhen Zhang,Fuli Feng,Pengjun Xie,Xiaobin Wang*

Main category: cs.CL

TL;DR: 提出自动化合成高难度模拟环境的方法及环境级强化学习算法，提升语言智能体训练效率与稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习模拟环境存在合成半自动化、任务难度不足、模拟用户不稳定、环境异构性等问题，限制了语言智能体的训练效果。

Method: （1）自动化可扩展的高难度任务模拟环境合成流程；（2）环境级强化学习算法，缓解用户不稳定性并进行环境级优势估计。

Result: 在tau-bench、tau2-Bench、VitaBench等基准测试中验证了方法的有效性，并展现出良好的跨领域泛化能力。

Conclusion: 所提出的环境合成与训练算法能有效提升语言智能体在复杂任务中的训练稳定性与泛化性能。

Abstract: Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.

</details>


### [259] [NepEMO: A Multi-Label Emotion and Sentiment Analysis on Nepali Reddit with Linguistic Insights and Temporal Trends](https://arxiv.org/abs/2512.22823)
*Sameer Sitoula,Tej Bahadur Shahi,Laxmi Prasad Bhatt,Anisha Pokhrel,Arjun Neupane*

Main category: cs.CL

TL;DR: 本文介绍了NepEMO数据集，这是一个用于尼泊尔语Reddit帖子多标签情感和情感分类的新数据集，包含4,462个帖子，并比较了多种机器学习模型在该任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体平台（如Reddit）为用户提供了匿名表达敏感话题（如健康和日常生活）的空间，尤其是在挑战性事件期间。然而，针对尼泊尔语社交媒体内容的情感分析资源有限，因此需要构建专门的数据集和模型。

Method: 研究构建了NepEMO数据集，包含4,462个帖子（2019年1月至2025年6月），使用英语、罗马化尼泊尔语和天城体脚本标注了五种情绪（恐惧、愤怒、悲伤、喜悦、抑郁）和三种情感类别（积极、消极、中性）。通过语言分析（情绪趋势、情绪共现、情感特定n-gram、主题建模）比较了传统机器学习、深度学习和Transformer模型。

Result: Transformer模型在多标签情感分类和情感分类任务上均 consistently 优于传统机器学习和深度学习模型。

Conclusion: NepEMO数据集为尼泊尔语社交媒体情感分析提供了重要资源，Transformer模型在该任务上表现最佳，为未来研究提供了基准。

Abstract: Social media (SM) platforms (e.g. Facebook, Twitter, and Reddit) are increasingly leveraged to share opinions and emotions, specifically during challenging events, such as natural disasters, pandemics, and political elections, and joyful occasions like festivals and celebrations. Among the SM platforms, Reddit provides a unique space for its users to anonymously express their experiences and thoughts on sensitive issues such as health and daily life. In this work, we present a novel dataset, called NepEMO, for multi-label emotion (MLE) and sentiment classification (SC) on the Nepali subreddit post. We curate and build a manually annotated dataset of 4,462 posts (January 2019- June 2025) written in English, Romanised Nepali and Devanagari script for five emotions (fear, anger, sadness, joy, and depression) and three sentiment classes (positive, negative, and neutral). We perform a detailed analysis of posts to capture linguistic insights, including emotion trends, co-occurrence of emotions, sentiment-specific n-grams, and topic modelling using Latent Dirichlet Allocation and TF-IDF keyword extraction. Finally, we compare various traditional machine learning (ML), deep learning (DL), and transformer models for MLE and SC tasks. The result shows that transformer models consistently outperform the ML and DL models for both tasks.

</details>


### [260] [Diversity or Precision? A Deep Dive into Next Token Prediction](https://arxiv.org/abs/2512.22955)
*Haoyuan Wu,Hai Wang,Jiajia Wu,Jinxiang Ou,Keyao Wang,Weile Chen,Zihao Zheng,Bei Yu*

Main category: cs.CL

TL;DR: 本文提出了一种将监督学习与强化学习原则相结合的新预训练目标，通过奖励塑造策略调整预训练模型的标记输出分布，为后续强化学习提供更优的探索空间，从而提升大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明强化学习能显著提升大语言模型的推理能力，但其效果严重依赖于预训练模型标记输出分布所定义的探索空间。标准交叉熵损失作为单步策略梯度优化的特例，未能系统研究预训练分布如何影响后续强化学习的探索潜力。

Method: 将下一标记预测建模为随机决策过程，提出广义预训练目标：1）采用奖励塑造策略平衡多样性与精确性；2）使用正奖励缩放因子控制对真实标记的概率集中程度；3）引入秩感知机制对高排名与低排名负标记进行非对称处理。

Result: 研究发现，与直觉相反，并非更高的分布熵有利于有效探索，而是施加面向精确性的先验能为强化学习提供更优的探索空间，从而提升端到端推理性能。

Conclusion: 通过将强化学习原则融入监督预训练过程，可以主动重塑标记输出分布，为后续强化学习阶段创造更有利的探索条件，最终实现推理能力的系统性提升。

Abstract: Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.

</details>


### [261] [Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks](https://arxiv.org/abs/2512.22966)
*Mengdi Chai,Ali R. Zomorrodi*

Main category: cs.CL

TL;DR: 评估了ChatGPT-4o、Gemini 1.5 Pro和LIama 3.3 70B在临床决策支持中的表现，发现其准确性因任务而异，且提示工程的效果高度依赖模型和任务，需定制化策略。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在医学知识评估中展现出潜力，但其在真实临床决策中的实际效用仍未充分探索，因此需要系统评估其在完整临床推理流程中的表现。

Method: 使用36个案例研究，评估三种大语言模型在五种临床决策任务中的表现，包括两种温度设置（默认与零温度），并应用MedPrompt框架的提示工程变体（含定向和随机动态少样本学习）。

Result: 模型表现因任务差异大：最终诊断接近完美，相关诊断测试表现差，其他任务中等；ChatGPT在零温度下表现更好，LIama在默认温度下更强；提示工程仅显著改善基线最差的任务，对其他任务可能适得其反；定向动态少样本提示未持续优于随机选择。

Conclusion: 提示工程的影响高度依赖模型和任务，表明将大语言模型整合到医疗领域需要定制化、情境感知的策略，而非通用解决方案。

Abstract: Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.

</details>


### [262] [Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping](https://arxiv.org/abs/2512.23014)
*Tao Yu,Yongqi An,Kuan Zhu,Guibo Zhu,Ming Tang,Jinqiao Wang*

Main category: cs.CL

TL;DR: 提出FANG框架，通过功能感知的神经元分组解决大语言模型后训练剪枝中的校准偏差问题，在保持语言建模性能的同时提升下游任务准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算存储成本高，后训练结构化剪枝是高效解决方案，但现有方法在校准集未能充分反映预训练数据分布时，对下游任务的泛化能力有限。

Method: 提出功能感知神经元分组(FANG)：1) 根据神经元处理的语义上下文类型进行功能相似分组；2) 组内重要性评估时，对与功能角色强相关的token赋予更高权重；3) 保留跨多种上下文类型贡献的神经元；4) 根据功能复杂性自适应分配各模块的稀疏度。

Result: FANG与FLAP和OBC两种代表性剪枝方法结合时取得SOTA结果：在30%和40%稀疏度下，平均准确率分别提升1.5%-8.5%，同时保持语言建模性能。

Conclusion: FANG通过功能感知的分组策略有效缓解校准偏差，在剪枝稀疏度与模型性能间取得更好平衡，显著提升下游任务准确率。

Abstract: Large Language Models (LLMs) demonstrate impressive performance across natural language tasks but incur substantial computational and storage costs due to their scale. Post-training structured pruning offers an efficient solution. However, when few-shot calibration sets fail to adequately reflect the pretraining data distribution, existing methods exhibit limited generalization to downstream tasks. To address this issue, we propose Function-Aware Neuron Grouping (FANG), a post-training pruning framework that alleviates calibration bias by identifying and preserving neurons critical to specific function. FANG groups neurons with similar function based on the type of semantic context they process and prunes each group independently. During importance estimation within each group, tokens that strongly correlate with the functional role of the neuron group are given higher weighting. Additionally, FANG also preserves neurons that contribute across multiple context types. To achieve a better trade-off between sparsity and performance, it allocates sparsity to each block adaptively based on its functional complexity. Experiments show that FANG improves downstream accuracy while preserving language modeling performance. It achieves the state-of-the-art (SOTA) results when combined with FLAP and OBC, two representative pruning methods. Specifically, FANG outperforms FLAP and OBC by 1.5%--8.5% in average accuracy under 30% and 40% sparsity.

</details>


### [263] [LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models](https://arxiv.org/abs/2512.23025)
*Wenxuan Xu,Arvind Pillai,Subigya Nepal,Amanda C Collins,Daniel M Mackin,Michael V Heinz,Tess Z Griffin,Nicholas C Jacobson,Andrew Campbell*

Main category: cs.CL

TL;DR: 提出LENS框架，将多模态健康传感数据与语言模型对齐，生成临床相关的心理健康叙事，解决了传感器数据到自然语言转换的难题。


<details>
  <summary>Details</summary>
Motivation: 多模态健康传感能提供丰富的行为信号用于心理健康评估，但将长时间序列的传感器测量值转化为自然语言仍具挑战性，现有LLM无法原生处理长时传感器流，且配对的传感器-文本数据集稀缺。

Method: 构建大规模数据集，将抑郁和焦虑症状相关的生态瞬时评估（EMA）响应转化为自然语言描述，得到超过10万条传感器-文本问答对；训练补丁级编码器，将原始传感器信号直接投影到LLM的表示空间，实现原生时间序列集成。

Result: LENS在标准NLP指标和症状严重程度准确性的任务特定指标上均优于强基线；13位心理健康专业人士的用户研究表明，LENS生成的叙事全面且具有临床意义。

Conclusion: 该方法推动了LLM作为健康传感接口的发展，为模型能够推理原始行为信号并支持下游临床决策提供了可扩展的路径。

Abstract: Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.

</details>


### [264] [Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization](https://arxiv.org/abs/2512.23032)
*Kerem Zaman,Shashank Srivastava*

Main category: cs.CL

TL;DR: 该研究挑战了使用Biasing Features指标评估思维链（CoT）忠实性的方法，认为该指标混淆了不忠实性与不完整性，并展示了更大推理时token预算能显著提高提示线索的言语化程度。


<details>
  <summary>Details</summary>
Motivation: 现有研究使用Biasing Features指标将未包含影响预测的提示线索的CoT标记为不忠实，但作者认为这混淆了不忠实性与将分布式Transformer计算转化为线性自然语言叙述所需的有损压缩（不完整性）。

Method: 在Llama-3和Gemma-3模型上进行多跳推理任务实验，比较不同评估指标；提出新的faithful@k指标；使用因果中介分析（Causal Mediation Analysis）研究非言语化提示线索的因果作用。

Result: 1. 许多被Biasing Features标记为不忠实的CoT被其他指标判断为忠实（某些模型中超过50%）；2. 更大的推理时token预算能将提示线索言语化比例提升至90%；3. 因果中介分析显示即使未言语化的提示线索也能通过CoT因果中介预测变化。

Conclusion: 应谨慎依赖基于提示线索的评估方法，倡导使用更广泛的解释性工具包，包括因果中介分析和基于破坏的指标。

Abstract: Recent work, using the Biasing Features metric, labels a CoT as unfaithful if it omits a prompt-injected hint that affected the prediction. We argue this metric confuses unfaithfulness with incompleteness, the lossy compression needed to turn distributed transformer computation into a linear natural language narrative. On multi-hop reasoning tasks with Llama-3 and Gemma-3, many CoTs flagged as unfaithful by Biasing Features are judged faithful by other metrics, exceeding 50% in some models. With a new faithful@k metric, we show that larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings), suggesting much apparent unfaithfulness is due to tight token limits. Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT. We therefore caution against relying solely on hint-based evaluations and advocate a broader interpretability toolkit, including causal mediation and corruption-based metrics.

</details>


### [265] [Accelerating Language Model Workflows with Prompt Choreography](https://arxiv.org/abs/2512.23049)
*TJ Bai,Jason Eisner*

Main category: cs.CL

TL;DR: 提出Prompt Choreography框架，通过全局KV缓存优化多智能体LLM工作流执行，显著降低延迟并提升速度。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体工作流中部署增多，重复计算导致效率低下，需要优化执行效率。

Method: 采用动态全局KV缓存机制，支持LLM调用任意重排历史消息子集，允许并行调用，并通过微调使模型适应缓存。

Result: 实现每消息延迟降低2.0-6.2倍（首词生成时间），在冗余计算为主的工作流中端到端加速超过2.2倍。

Conclusion: Prompt Choreography能有效提升LLM工作流效率，缓存机制与模型微调结合可接近原始编码效果。

Abstract: Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\times$) in some workflows dominated by redundant computation.

</details>


### [266] [Reservoir Computing inspired Matrix Multiplication-free Language Model](https://arxiv.org/abs/2512.23145)
*Takumi Shiratsuchi,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.CL

TL;DR: 提出一种基于储层计算的矩阵乘法免费语言模型，通过部分固定和共享权重、插入储层层来减少计算开销，在保持性能的同时显著降低参数数量和训练/推理时间。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算成本高昂成为主要瓶颈，需要提高计算效率。

Method: 采用矩阵乘法免费语言模型架构，部分固定和共享选定层权重，插入储层层获取动态表示，组合操作减少内存访问。

Result: 参数减少最多19%，训练时间减少9.9%，推理时间减少8.0%，性能与基线模型相当。

Conclusion: 提出的架构能有效降低语言模型的计算成本，为高效语言模型设计提供了新思路。

Abstract: Large language models (LLMs) have achieved state-of-the-art performance in natural language processing; however, their high computational cost remains a major bottleneck. In this study, we target computational efficiency by focusing on a matrix multiplication free language model (MatMul-free LM) and further reducing the training cost through an architecture inspired by reservoir computing. Specifically, we partially fix and share the weights of selected layers in the MatMul-free LM and insert reservoir layers to obtain rich dynamic representations without additional training overhead. Additionally, several operations are combined to reduce memory accesses. Experimental results show that the proposed architecture reduces the number of parameters by up to 19%, training time by 9.9%, and inference time by 8.0%, while maintaining comparable performance to the baseline model.

</details>


### [267] [TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish](https://arxiv.org/abs/2512.23065)
*Melikşah Türker,A. Ebrar Kızıloğlu,Onur Güngör,Susan Üsküdarlı*

Main category: cs.CL

TL;DR: 本文介绍了TabiBERT，一个基于ModernBERT架构、从头开始训练的单语土耳其语编码器，在包含1万亿标记的多样化语料库上预训练，支持8192标记的上下文长度，并在土耳其语NLP基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管BERT等编码器在计算效率、训练稳定性和长上下文建模方面取得了显著进展，但土耳其语NLP领域缺乏一个从头开始训练、融合了现代架构范式（如RoPE、FlashAttention）的单语编码器模型。

Method: 采用ModernBERT架构，整合了旋转位置嵌入（RoPE）、FlashAttention和改进的归一化技术；在包含840亿标记的多领域语料库（网页文本73%、科学出版物20%、源代码6%、数学内容0.3%）上采样1万亿标记进行预训练；构建了TabiBench基准测试，包含28个数据集和8个任务类别，采用GLUE风格的宏平均评估方法。

Result: TabiBERT支持8192标记的上下文长度（是原始BERT的16倍），推理速度提升高达2.65倍，GPU内存消耗降低，支持更大的批处理大小；在TabiBench上获得77.58分，比BERTurk高出1.62分，在8个类别中的5个（包括问答+9.55、代码检索+2.41、文档检索+0.60）达到最先进水平；与包括TurkishBERTweet在内的任务特定最佳结果相比，平均提升+1.47分，显示出强大的跨领域泛化能力。

Conclusion: TabiBERT成功填补了土耳其语NLP领域现代单语编码器的空白，通过整合先进的架构技术和在多样化大规模语料上的预训练，实现了显著的性能提升和效率改进，为土耳其语NLP研究提供了透明、可复现的模型资源。

Abstract: Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). The model supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories: question answering (+9.55), code retrieval (+2.41), and document retrieval (+0.60). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.

</details>


### [268] [Not too long do read: Evaluating LLM-generated extreme scientific summaries](https://arxiv.org/abs/2512.23206)
*Zhuoqi Lyu,Qing Ke*

Main category: cs.CL

TL;DR: 该研究构建了BiomedTLDR数据集，用于评估大语言模型生成科学论文摘要的能力，发现LLM生成的摘要更倾向于提取式而非抽象式，与人类专家存在差异。


<details>
  <summary>Details</summary>
Motivation: 高质量的科学摘要（TLDR）有助于科学传播，但缺乏全面、高质量的数据集来开发和评估大语言模型（LLMs）的摘要生成能力。

Method: 提出BiomedTLDR数据集，包含大量研究者撰写的科学论文摘要；测试主流开源LLMs基于摘要生成TLDR的能力；分析LLM生成摘要与人类专家摘要的差异。

Result: 部分LLM能生成类人摘要，但总体上LLM更倾向于保留原文的词汇选择和修辞结构，相比人类更偏向提取式而非抽象式摘要。

Conclusion: LLM在科学摘要生成中表现出提取式倾向，与人类专家的抽象式摘要存在差异；BiomedTLDR数据集为LLM摘要能力评估提供了资源。

Abstract: High-quality scientific extreme summary (TLDR) facilitates effective science communication. How do large language models (LLMs) perform in generating them? How are LLM-generated summaries different from those written by human experts? However, the lack of a comprehensive, high-quality scientific TLDR dataset hinders both the development and evaluation of LLMs' summarization ability. To address these, we propose a novel dataset, BiomedTLDR, containing a large sample of researcher-authored summaries from scientific papers, which leverages the common practice of including authors' comments alongside bibliography items. We then test popular open-weight LLMs for generating TLDRs based on abstracts. Our analysis reveals that, although some of them successfully produce humanoid summaries, LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive in general, compared to humans. Our code and datasets are available at https://github.com/netknowledge/LLM_summarization (Lyu and Ke, 2025).

</details>


### [269] [Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process](https://arxiv.org/abs/2512.23213)
*Zhijun Chen,Zeyu Ji,Qianren Mao,Junhang Cheng,Bangjie Qin,Hao Wu,Zhuoran Li,Jingzheng Li,Kai Sun,Zizhe Wang,Yikun Ban,Zhu Sun,Xiangyang Ji,Hailong Sun*

Main category: cs.CL

TL;DR: 提出LLM-PeerReview，一种无监督的LLM集成方法，通过同行评审框架从多个LLM生成的候选回答中选择最佳响应。


<details>
  <summary>Details</summary>
Motivation: 利用多个具有不同优势的LLM的集体智慧，在无监督条件下选择最理想的响应，提高模型输出的质量和适应性。

Method: 采用三阶段框架：1) 使用LLM-as-a-Judge技术评估每个响应；2) 通过基于图模型的真值推理算法或平均策略聚合评分；3) 选择最高分响应作为集成输出。

Result: 在两个变体上，在四个数据集上均取得强劲结果，分别比先进模型Smoothie-Global高出6.9%和7.3%。

Conclusion: LLM-PeerReview概念简单且实证有效，通过无监督的同行评审机制显著提升了LLM集成的性能。

Abstract: We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.

</details>


### [270] [Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation](https://arxiv.org/abs/2512.23260)
*Dianyun Wang,Qingsen Ma,Yuhu Shang,Zhifeng Lu,Lechen Ning,Zhenbo Xu,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: 提出一种基于稀疏自编码器的可解释参数高效微调方法，通过解耦特征空间识别任务相关特征，构建显式低秩子空间指导适配器初始化，在安全对齐任务中达到接近RLHF的性能同时提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有低秩适配方法（如LoRA）在隐式学习低秩子空间时缺乏可解释性和直接控制，这源于特征的多义性（单个维度编码多个概念）。

Method: 利用预训练稀疏自编码器在解耦特征空间中识别任务相关特征，构建显式可解释的低秩子空间用于适配器初始化，并给出理论分析证明其在单义性假设下的优越性。

Result: 在安全对齐任务中达到99.6%的安全率（超过全参数微调7.4个百分点，接近RLHF方法），仅更新0.19-0.24%的参数，并通过SAE特征提供对学习对齐子空间的可解释洞察。

Conclusion: 将机制可解释性融入微调过程可同时提升性能和透明度，为参数高效微调提供了新的可解释性框架。

Abstract: Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts. To address this, we leverage pre-trained Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled feature space, then construct an explicit, interpretable low-rank subspace to guide adapter initialization. We provide theoretical analysis proving that under monosemanticity assumptions, SAE-based subspace identification achieves arbitrarily small recovery error, while direct identification in polysemantic space suffers an irreducible error floor. On safety alignment, our method achieves up to 99.6% safety rate--exceeding full fine-tuning by 7.4 percentage points and approaching RLHF-based methods--while updating only 0.19-0.24% of parameters. Crucially, our method provides interpretable insights into the learned alignment subspace through the semantic grounding of SAE features. Our work demonstrates that incorporating mechanistic interpretability into the fine-tuning process can simultaneously improve both performance and transparency.

</details>


### [271] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: 本文提出了一种名为Anka的领域特定语言（DSL），用于数据转换管道，通过约束语法显著提升大语言模型在复杂多步编程任务中的代码生成准确率。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在代码生成方面表现出色，但在复杂的多步编程任务中仍存在系统性错误，这些错误可能源于通用语言的灵活性导致的歧义和隐式状态管理问题。

Method: 设计了Anka这一领域特定语言，其语法明确且受约束，以减少代码生成中的歧义。使用Claude 3.5 Haiku和GPT-4o-mini等模型，在100个基准问题上进行零样本评估，比较Anka与Python在解析成功率和任务准确率上的表现。

Result: Claude 3.5 Haiku在Anka上实现了99.9%的解析成功率和95.8%的整体任务准确率。在多步管道任务中，Anka比Python准确率高出40个百分点（100% vs. 60%），GPT-4o-mini的验证也显示类似优势（+26.7个百分点）。

Conclusion: LLM能够通过上下文提示完全学习新的DSL，达到接近原生的准确率；约束语法显著减少复杂任务中的错误；为LLM生成专门设计的领域特定语言可以优于其受过大量训练的通用语言。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>


### [272] [Chinese Morph Resolution in E-commerce Live Streaming Scenarios](https://arxiv.org/abs/2512.23280)
*Jiahao Zhu,Jipeng Qiang,Ran Bai,Chenyu Liu,Xiaoye Ouyang*

Main category: cs.CL

TL;DR: 该研究针对中国电商直播中主播使用变音逃避监管的问题，提出了LiveAMR任务来检测健康医疗直播中的发音规避行为，构建了首个相关数据集并开发了基于文本生成的方法。


<details>
  <summary>Details</summary>
Motivation: 中国电商直播（如抖音）已成为重要销售渠道，但主播常使用变音逃避审查并进行虚假宣传，特别是在健康医疗领域，现有研究主要关注文本规避，缺乏对发音规避的检测方法。

Method: 提出LiveAMR任务，构建包含86,790个样本的数据集，将任务转化为文本到文本生成问题，利用大语言模型生成额外训练数据以提升性能。

Result: 开发的方法能有效检测直播中的发音规避行为，证明变音解析能显著提升直播监管效果，通过LLM生成数据进一步提高了模型性能。

Conclusion: LiveAMR任务和数据集填补了发音规避检测的空白，所提方法为直播监管提供了有效工具，证明变音解析对规范直播行业具有重要意义。

Abstract: E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.

</details>


### [273] [AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration](https://arxiv.org/abs/2512.23300)
*Minjiang Huang,Jipeng Qiang,Yi Zhu,Chaowei Zhang,Xiangyu Zhao,Kui Yu*

Main category: cs.CL

TL;DR: 提出AI4Reading系统，利用大语言模型和语音合成技术，通过多智能体协作自动生成有声书解读，在脚本准确性和简洁性上表现良好，但语音生成质量仍有差距。


<details>
  <summary>Details</summary>
Motivation: 有声书解读需求增长，但人工制作耗时耗力，需要自动化解决方案以降低制作成本并提高效率。

Method: 设计包含11个专业智能体（主题分析师、案例分析师、编辑、叙述者、校对员等）的多智能体协作框架，结合大语言模型和语音合成技术，确保内容准确、易理解且叙事结构合理。

Result: 与专家解读对比显示，系统生成的解读脚本更简洁准确，但语音生成质量仍存在差距。

Conclusion: AI4Reading系统在自动化生成有声书解读方面具有潜力，尤其在脚本生成上表现突出，但语音合成质量仍需进一步提升。

Abstract: Audiobook interpretations are attracting increasing attention, as they provide accessible and in-depth analyses of books that offer readers practical insights and intellectual inspiration. However, their manual creation process remains time-consuming and resource-intensive. To address this challenge, we propose AI4Reading, a multi-agent collaboration system leveraging large language models (LLMs) and speech synthesis technology to generate podcast, like audiobook interpretations. The system is designed to meet three key objectives: accurate content preservation, enhanced comprehensibility, and a logical narrative structure. To achieve these goals, we develop a framework composed of 11 specialized agents,including topic analysts, case analysts, editors, a narrator, and proofreaders that work in concert to explore themes, extract real world cases, refine content organization, and synthesize natural spoken language. By comparing expert interpretations with our system's output, the results show that although AI4Reading still has a gap in speech generation quality, the generated interpretative scripts are simpler and more accurate.

</details>


### [274] [AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents](https://arxiv.org/abs/2512.23343)
*Jiafeng Liang,Hao Li,Chang Li,Jiaqi Zhou,Shixin Jiang,Zekun Wang,Changkai Ji,Zhihao Zhu,Runxuan Liu,Tao Ren,Jinlan Fu,See-Kiong Ng,Xia Liang,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: 本文系统综述了记忆在AI智能体中的作用，整合认知神经科学与LLM智能体的跨学科知识，探讨记忆的定义、分类、存储机制、管理生命周期、评估基准、安全问题及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有自主智能体研究在借鉴认知神经科学设计高效记忆工作流程时，受限于学科壁垒，难以充分吸收人类记忆机制的精髓，因此需要搭建跨学科桥梁以提升智能体的记忆能力。

Method: 采用系统性综述方法，整合认知神经科学与LLM智能体的知识，从定义、功能、分类、存储机制、管理生命周期、评估基准、安全攻防等多维度进行对比分析，并展望未来研究方向。

Result: 提出了记忆从认知神经科学到LLM再到智能体的渐进式理解框架；对比了生物与人工记忆的异同；总结了主流记忆评估基准；分析了记忆安全的攻防视角；指出了多模态记忆系统和技能习得等未来方向。

Conclusion: 跨学科整合认知神经科学与LLM智能体记忆机制对提升智能体能力至关重要，未来需关注多模态记忆、动态技能学习及安全可靠的内存系统构建。

Abstract: Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.

</details>


### [275] [A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation](https://arxiv.org/abs/2512.23356)
*Xin Zhang,Yang Cao,Baoxing Wu,Xinyi Chen,Kai Song,Siying Li*

Main category: cs.CL

TL;DR: 提出基于外部子图生成的逐步推理增强框架SGR，通过动态构建查询相关子图并利用其语义结构指导推理过程，提升大语言模型在复杂推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在处理需要深度推理和逻辑推断的任务时，可能因训练数据中的噪声或无关信息导致预测错误或与事实知识不一致，需要增强其推理能力。

Method: 提出SGR框架：1) 根据输入查询从外部知识库动态构建相关子图；2) 基于子图结构进行多步逐步推理；3) 整合多条推理路径生成最终答案。

Result: 在多个基准数据集上的实验表明，SGR框架持续优于强基线方法，有效提升了大语言模型的推理准确性。

Conclusion: 通过外部知识子图引导的逐步推理机制能减少噪声信息干扰，显著增强大语言模型的逻辑推理能力，为复杂场景下的可靠应用提供新思路。

Abstract: Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.

</details>


### [276] [Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data](https://arxiv.org/abs/2512.23422)
*Jiapeng Wang,Yiwen Hu,Yanzipeng Gao,Haoyu Wang,Shuo Wang,Hongyu Lu,Jiaxin Mao,Wayne Xin Zhao,Junyi Li,Xiao Zhang*

Main category: cs.CL

TL;DR: 本文提出EntroDrop方法，通过基于熵的令牌丢弃机制解决大语言模型在多轮训练中的性能退化问题，在数据受限场景下实现更有效的模型适应。


<details>
  <summary>Details</summary>
Motivation: 随着高质量领域特定数据日益稀缺，多轮训练成为大语言模型适应的实用策略。但自回归模型在重复数据暴露下常出现性能退化，过拟合导致模型能力显著下降。

Method: 提出EntroDrop方法：1）基于熵的令牌丢弃：选择性掩码低熵令牌作为结构化数据正则化；2）课程调度：根据训练进度动态调整正则化强度。

Result: 在0.6B到8B参数规模的模型上实验表明，EntroDrop持续优于标准正则化基线，并在扩展的多轮训练中保持稳健性能。

Conclusion: 研究强调了在有限数据训练时，使正则化与令牌级学习动态对齐的重要性。EntroDrop为数据受限领域的大语言模型适应提供了有效途径。

Abstract: As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to dominate optimization, while the model's ability to generalize on high-entropy tokens deteriorates with continued training. To address this, we introduce EntroDrop, an entropy-guided token dropout method that functions as structured data regularization. EntroDrop selectively masks low-entropy tokens during training and employs a curriculum schedule to adjust regularization strength in alignment with training progress. Experiments across model scales from 0.6B to 8B parameters show that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training. These findings underscore the importance of aligning regularization with token-level learning dynamics when training on limited data. Our approach offers a promising pathway toward more effective adaptation of LLMs in data-constrained domains.

</details>


### [277] [The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective](https://arxiv.org/abs/2512.23429)
*Yi Zhao,Yongjun Zhu,Donghun Kim,Yuzhuo Wang,Heng Zhang,Chao Lu,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 该研究通过分析13万篇PLOS期刊论文，发现科学团队中性别多样性与影响力呈倒U型关系，且女性领导组与男性支持组的组合表现最佳，团队规模对领导组性别多样性的影响存在调节作用。


<details>
  <summary>Details</summary>
Motivation: 现有关于性别多样性对科研团队影响的研究结论不一致，且多从整体角度衡量多样性，忽视了团队内部角色分化的作用，特别是不同角色中性别多样性的影响机制尚不明确。

Method: 将论文所有作者定义为科学团队，以五年被引量衡量团队影响力；利用作者贡献声明区分领导角色与支持角色；基于PLOS期刊的13万余篇论文（主要为生物医学领域），采用多变量回归分析角色性别多样性与团队影响力的关系，并运用阈值回归模型探究团队规模的调节作用。

Result: （1）领导组和支持组的性别多样性与团队影响力均呈倒U型关系；（2）全女性领导组与全男性支持组的团队影响力高于其他类型；（3）领导组性别多样性对小规模团队有显著负面影响，但对大规模团队影响转为正向且不显著；支持组性别多样性的正向效应不受团队规模影响。

Conclusion: 性别多样性对科研团队的影响因角色和团队规模而异，全女性领导与全男性支持的组合可能更有效，未来研究与管理实践需重视团队内部分工与规模的情境因素。

Abstract: The influence of gender diversity on the success of scientific teams is of great interest to academia. However, prior findings remain inconsistent, and most studies operationalize diversity in aggregate terms, overlooking internal role differentiation. This limitation obscures a more nuanced understanding of how gender diversity shapes team impact. In particular, the effect of gender diversity across different team roles remains poorly understood. To this end, we define a scientific team as all coauthors of a paper and measure team impact through five-year citation counts. Using author contribution statements, we classified members into leadership and support roles. Drawing on more than 130,000 papers from PLOS journals, most of which are in biomedical-related disciplines, we employed multivariable regression to examine the association between gender diversity in these roles and team impact. Furthermore, we apply a threshold regression model to investigate how team size moderates this relationship. The results show that (1) the relationship between gender diversity and team impact follows an inverted U-shape for both leadership and support groups; (2) teams with an all-female leadership group and an all-male support group achieve higher impact than other team types. Interestingly, (3) the effect of leadership-group gender diversity is significantly negative for small teams but becomes positive and statistically insignificant in large teams. In contrast, the estimates for support-group gender diversity remain significant and positive, regardless of team size.

</details>


### [278] [C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs](https://arxiv.org/abs/2512.23430)
*Xuan Feng,Bo An,Tianlong Gu,Liang Chang,Fengrui Hao,Peipeng Yu,Shuai Zhao*

Main category: cs.CL

TL;DR: 本文提出C2PO框架，通过因果对比偏好优化统一解决LLM中的刻板偏见与结构偏见，在保持通用推理能力的同时有效抑制偏见特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常孤立处理LLM中的刻板偏见（如性别/种族）和结构偏见（如词汇重叠/位置偏好），常导致缓解一种偏见时加剧另一种。研究发现这两种偏见均源于输入中潜在的虚假特征关联驱动的错误推理捷径。

Method: 提出Causal-Contrastive Preference Optimization (C2PO)框架：1) 利用因果反事实信号分离偏见诱导特征与有效推理路径；2) 采用公平敏感的偏好更新机制动态评估logit层贡献并抑制捷径特征。

Result: 在多个基准测试（BBQ/Unqover/MNLI/HANS等）中验证，C2PO能同时有效缓解刻板偏见与结构偏见，并在领域外公平性（StereoSet/WinoBias）和通用能力（MMLU/GSM8K）上保持稳健表现。

Conclusion: C2PO通过统一优化过程发现并抑制虚假特征关联，为同时解决LLM多维度偏见问题提供了有效方案，且不损害模型通用推理能力。

Abstract: Bias in Large Language Models (LLMs) poses significant risks to trustworthiness, manifesting primarily as stereotypical biases (e.g., gender or racial stereotypes) and structural biases (e.g., lexical overlap or position preferences). However, prior paradigms typically address these in isolation, often mitigating one at the expense of exacerbating the other. To address this, we conduct a systematic exploration of these reasoning failures and identify a primary inducement: the latent spurious feature correlations within the input that drive these erroneous reasoning shortcuts. Driven by these findings, we introduce Causal-Contrastive Preference Optimization (C2PO), a unified alignment framework designed to tackle these specific failures by simultaneously discovering and suppressing these correlations directly within the optimization process. Specifically, C2PO leverages causal counterfactual signals to isolate bias-inducing features from valid reasoning paths, and employs a fairness-sensitive preference update mechanism to dynamically evaluate logit-level contributions and suppress shortcut features. Extensive experiments across multiple benchmarks covering stereotypical bias (BBQ, Unqover), structural bias (MNLI, HANS, Chatbot, MT-Bench), out-of-domain fairness (StereoSet, WinoBias), and general utility (MMLU, GSM8K) demonstrate that C2PO effectively mitigates stereotypical and structural biases while preserving robust general reasoning capabilities.

</details>


### [279] [ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning](https://arxiv.org/abs/2512.23440)
*Yuqi Tang,Jing Yu,Zichang Su,Kehua Feng,Zhihui Zhu,Libin Wang,Lei Liang,Qiang Zhang,Keyan Ding,Huajun Chen*

Main category: cs.CL

TL;DR: 提出ClinDEF框架，通过模拟诊断对话动态评估LLM的临床推理能力，超越传统静态问答基准，揭示先进LLM在临床推理中的关键缺陷。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准主要关注静态问答，无法有效模拟医生与患者互动中动态收集信息、调整检查和细化鉴别诊断的临床推理过程，且现有动态医学框架常依赖有限、易污染的数据集，缺乏细粒度多层级评估。

Method: 基于疾病知识图谱动态生成患者病例，构建LLM医生与自动化患者代理之间的多轮交互对话，评估协议不仅包含诊断准确性，还纳入细粒度效率分析和基于量表的诊断质量评估。

Result: ClinDEF能有效暴露先进LLM在临床推理中的关键差距，提供更细致且具有临床意义的评估范式。

Conclusion: ClinDEF为评估LLM临床推理能力提供了一个动态、细粒度且临床相关的框架，弥补了现有基准的不足，有助于更全面理解LLM在医疗领域的潜力与局限。

Abstract: Clinical diagnosis begins with doctor-patient interaction, during which physicians iteratively gather information, determine examination and refine differential diagnosis through patients' response. This dynamic clinical-reasoning process is poorly represented by existing LLM benchmarks that focus on static question-answering. To mitigate these gaps, recent methods explore dynamic medical frameworks involving interactive clinical dialogues. Although effective, they often rely on limited, contamination-prone datasets and lack granular, multi-level evaluation. In this work, we propose ClinDEF, a dynamic framework for assessing clinical reasoning in LLMs through simulated diagnostic dialogues. Grounded in a disease knowledge graph, our method dynamically generates patient cases and facilitates multi-turn interactions between an LLM-based doctor and an automated patient agent. Our evaluation protocol goes beyond diagnostic accuracy by incorporating fine-grained efficiency analysis and rubric-based assessment of diagnostic quality. Experiments show that ClinDEF effectively exposes critical clinical reasoning gaps in state-of-the-art LLMs, offering a more nuanced and clinically meaningful evaluation paradigm.

</details>


### [280] [Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss](https://arxiv.org/abs/2512.23447)
*Ang Lv,Jin Ma,Yiyuan Ma,Siyuan Qiao*

Main category: cs.CL

TL;DR: 提出专家-路由器耦合损失（ERC），通过约束路由器决策与专家能力对齐来提升MoE模型性能，该方法计算高效且能定量追踪专家专业化水平。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型中路由器决策与专家能力缺乏显式约束，限制了模型性能提升。

Method: 将每个专家的路由器嵌入作为代理令牌，通过扰动后输入专家获取内部激活，ERC损失施加双重约束：1）专家对自身代理令牌激活更高；2）代理令牌在对应专家中激活更强。

Result: 在3B至15B参数的MoE-LLM预训练中验证了ERC损失的有效性，且计算成本仅与专家数量平方相关，独立于批大小。

Conclusion: ERC损失能有效对齐路由器与专家能力，提升MoE模型性能，同时提供专家专业化水平的可控追踪机制。

Abstract: Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.

</details>


### [281] [Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings](https://arxiv.org/abs/2512.23471)
*Thomas Haschka,Joseph Bakarji*

Main category: cs.CL

TL;DR: 提出了一种基于嵌套密度聚类的层次树构建方法，用于揭示文本语料库中的语义层次结构，无需预定义类别即可发现研究领域及其子领域。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLM）嵌入常用于基于语义相似性的文本存储和检索，但文本语料库中的全局语义关系结构往往不透明，需要一种数据驱动的方法来揭示层次化的语义关系。

Method: 采用嵌套密度聚类方法，在LLM嵌入空间中逐步放宽密度标准，从密集簇合并为扩散簇，最终构建代表整个数据集的层次树结构。

Result: 该方法在科学摘要、20 Newsgroups和IMDB 50k电影评论等数据集上得到验证，能够跨领域稳健地揭示语义层次结构。

Conclusion: 嵌套密度树能够有效揭示文本数据集中的语义结构和演化，在科学计量学、主题演化等领域具有潜在应用价值。

Abstract: Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster -- the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 Newsgroups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.

</details>


### [282] [Automatic Detection of Complex Quotation Patterns in Aggadic Literature](https://arxiv.org/abs/2512.23504)
*Hadar Miller,Tsvi Kuflik,Moshe Lavee*

Main category: cs.CL

TL;DR: 提出ACT算法，用于自动检测拉比文献中的圣经引文，结合形态感知对齐和上下文增强，在短句、转述和结构嵌入引文检测上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本重用框架难以处理拉比文献中短小、转述或结构嵌入的圣经引文，需要一种能识别复杂引用模式（如“波浪”和“回声”引文）的方法。

Method: 采用三阶段算法：形态感知对齐、上下文敏感增强和配置评估（包括ACT-QE完整流程、无风格增强的ACT-2和使用长n-gram的ACT-3）。

Result: 完整ACT流程（ACT-QE）F1分数达0.91（召回率0.89，精确率0.94），优于Dicta、Passim等基线系统；不同配置在召回率和精确率间存在权衡。

Conclusion: ACT填补了机器检测与人工编辑判断之间的方法学空白，为数字人文和计算文献学提供了新工具，尤其适用于形态丰富、引用密集的文本传统。

Abstract: This paper presents ACT (Allocate Connections between Texts), a novel three-stage algorithm for the automatic detection of biblical quotations in Rabbinic literature. Unlike existing text reuse frameworks that struggle with short, paraphrased, or structurally embedded quotations, ACT combines a morphology-aware alignment algorithm with a context-sensitive enrichment stage that identifies complex citation patterns such as "Wave" and "Echo" quotations.
  Our approach was evaluated against leading systems, including Dicta, Passim, Text-Matcher, as well as human-annotated critical editions. We further assessed three ACT configurations to isolate the contribution of each component. Results demonstrate that the full ACT pipeline (ACT-QE) outperforms all baselines, achieving an F1 score of 0.91, with superior Recall (0.89) and Precision (0.94). Notably, ACT-2, which lacks stylistic enrichment, achieves higher Recall (0.90) but suffers in Precision, while ACT-3, using longer n-grams, offers a tradeoff between coverage and specificity.
  In addition to improving quotation detection, ACT's ability to classify stylistic patterns across corpora opens new avenues for genre classification and intertextual analysis. This work contributes to digital humanities and computational philology by addressing the methodological gap between exhaustive machine-based detection and human editorial judgment. ACT lays a foundation for broader applications in historical textual analysis, especially in morphologically rich and citation-dense traditions like Aggadic literature.

</details>


### [283] [UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?](https://arxiv.org/abs/2512.23512)
*Fengjiao Chen,Minhao Jing,Weitao Lu,Yan Feng,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 本文提出UniHetero统一模型，在大规模预训练下探索生成任务对理解任务的增强作用，发现语义生成（而非像素生成）能有效提升视觉理解能力，并展现出更好的数据缩放趋势和数据利用率。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言大模型正朝着视觉理解与视觉生成任务统一的方向发展，但生成任务是否能在大规模数据上增强理解任务仍未得到充分探索。

Method: 采用简洁结构的统一模型UniHetero，在大规模预训练数据（>2亿样本）下进行分析，重点研究语义生成与像素生成的差异，并利用输入嵌入的自回归机制捕捉视觉细节。

Result: 关键发现包括：（1）仅当生成语义（而非像素）时，生成任务才能改善理解任务；（2）生成任务展现出更优的数据缩放趋势和更高的数据利用率；（3）输入嵌入的自回归能有效捕获视觉细节。

Conclusion: 语义生成是连接视觉理解与生成任务的有效桥梁，在大规模预训练中具有提升模型性能和数据效率的潜力，为统一模型设计提供了新方向。

Abstract: Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified model with a concise structure, UniHetero, under large-scale pretraining (>200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. (3) Autoregression on Input Embedding is effective to capture visual details.

</details>


### [284] [Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias](https://arxiv.org/abs/2512.23518)
*Hazel Kim,Philip Torr*

Main category: cs.CL

TL;DR: 提出MoLaCE框架，通过混合潜在概念专家来减轻大语言模型的输入确认偏见，提高事实正确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在输入确认偏见，当提示暗示偏好答案时，模型会强化偏见而非探索替代方案，这在多智能体辩论中会形成回声室效应，加剧偏见风险。

Method: 提出MoLaCE（潜在概念专家混合）框架，通过不同激活强度的潜在概念专家混合来调整模型响应，利用语言组合性特点，针对不同提示重新加权潜在概念。

Result: 实验表明MoLaCE能持续减少确认偏见，提高鲁棒性，性能匹配或超越多智能体辩论方法，同时计算成本大幅降低。

Conclusion: MoLaCE是一种轻量级推理时框架，能内部模拟辩论优势，提升模型事实正确性，并可集成到多智能体辩论框架中以增加视角多样性。

Abstract: Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.

</details>


### [285] [Instruction-Following Evaluation of Large Vision-Language Models](https://arxiv.org/abs/2512.23572)
*Daiki Shiono,Shumpei Miyawaki,Ryota Tanaka,Jun Suzuki*

Main category: cs.CL

TL;DR: 研究发现大型视觉语言模型在视觉指令微调后指令跟随能力下降，通过构建包含输出格式说明的数据集可缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在视觉指令微调后，常失去原始语言模型具备的指令跟随能力，导致无法按预期执行任务指令。

Method: 构建强调输出格式是否指定的新训练数据集，研究微调时明确指示输出格式对模型指令跟随能力的影响。

Result: 定量评估证实常用数据集微调后指令跟随能力下降；包含输出格式说明的数据集训练的模型指令跟随更准确。

Conclusion: 在视觉指令微调中包含输出格式说明的样本有助于缓解指令跟随能力下降问题。

Abstract: Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.

</details>


### [286] [Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs](https://arxiv.org/abs/2512.23547)
*Sahil Kale,Antonio Luca Alfeo*

Main category: cs.CL

TL;DR: 提出一种基于知识图谱的幻觉自检测方法，通过将LLM响应转换为实体关系图来估计幻觉可能性，相比现有方法在准确率和F1分数上提升显著。


<details>
  <summary>Details</summary>
Motivation: 幻觉（生成看似可信但虚假的陈述）是LLM安全部署的主要障碍，现有自检测方法仍有改进空间，需要更可靠的技术来提升检测效果。

Method: 将LLM响应转换为实体关系知识图谱，利用图谱结构估计幻觉概率；在GPT-4o和Gemini-2.5-Flash上评估，使用两个幻觉检测数据集（其中一个经人工标注增强并开源）。

Result: 相比标准自检测方法和当前最优方法SelfCheckGPT，本方法在准确率上实现最高16%的相对提升，F1分数提升达20%；证明结构化知识图谱能帮助LLM更有效分析原子事实。

Conclusion: 这种低成本、模型无关的方法能提升LLM对幻觉的自我检测能力，为构建更安全可信的语言模型提供了新路径。

Abstract: Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.

</details>


### [287] [Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models](https://arxiv.org/abs/2512.23578)
*Yu-Xiang Lin,Cheng-Han Chiang,Hung-yi Lee*

Main category: cs.CL

TL;DR: 研究发现口语模型在多轮对话中存在风格遗忘问题，即无法维持指定的副语言风格（如情感、口音等），但通过明确要求回忆风格指令可部分缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 口语模型在多轮对话中被要求以特定风格说话时，无法在多次交互后保持该风格，这促使研究者探究其风格遗忘现象及缓解方法。

Method: 评估了三个专有和两个开源口语模型，测试其在多轮对话中维持副语言风格（情感、口音、音量、语速）的能力，并尝试了不同提示策略（系统消息vs用户消息）。

Result: 所有测试模型均无法在指令下保持一致的说话风格；模型能回忆风格指令但无法表达；将指令置于系统消息而非用户消息时效果更差。

Conclusion: 口语模型存在风格遗忘问题，当前提示策略（尤其是系统消息）效果有限，需改进模型设计以维持长期风格一致性。

Abstract: In this paper, we show that when spoken language models (SLMs) are instructed to speak in a specific speaking style at the beginning of a multi-turn conversation, they cannot maintain the required speaking styles after several turns of interaction; we refer to this as the style amnesia of SLMs. We focus on paralinguistic speaking styles, including emotion, accent, volume, and speaking speed. We evaluate three proprietary and two open-source SLMs, demonstrating that none of these models can maintain a consistent speaking style when instructed to do so. We further show that when SLMs are asked to recall the style instruction in later turns, they can recall the style instruction, but they fail to express it throughout the conversation. We also show that explicitly asking the model to recall the style instruction can partially mitigate style amnesia. In addition, we examine various prompting strategies and find that SLMs struggle to follow the required style when the instruction is placed in system messages rather than user messages, which contradicts the intended function of system prompts.

</details>


### [288] [A Dataset and Benchmark for Consumer Healthcare Question Summarization](https://arxiv.org/abs/2512.23637)
*Abhishek Basu,Deepak Gupta,Dina Demner-Fushman,Shweta Yadav*

Main category: cs.CL

TL;DR: 提出了CHQ-Sum数据集，包含1507个专家标注的消费者健康问题及摘要，用于促进医疗健康问题自动摘要研究，并测试了多种先进摘要模型。


<details>
  <summary>Details</summary>
Motivation: 消费者在网络上提出健康问题时，常使用冗长、外围的描述，增加了自然语言理解的难度。目前缺乏医疗领域专家标注的数据集，阻碍了高效摘要系统的发展。

Method: 从社区问答论坛收集数据，构建了包含1507个专家标注的消费者健康问题及摘要的数据集CHQ-Sum，并在多种先进摘要模型上进行了基准测试。

Result: CHQ-Sum数据集为理解社交媒体上的健康相关帖子提供了宝贵资源，基准测试展示了数据集的有效性。

Conclusion: CHQ-Sum数据集填补了医疗健康问题摘要任务中专家标注数据的空白，有助于推动该领域高效摘要系统的开发。

Abstract: The quest for seeking health information has swamped the web with consumers health-related questions. Generally, consumers use overly descriptive and peripheral information to express their medical condition or other healthcare needs, contributing to the challenges of natural language understanding. One way to address this challenge is to summarize the questions and distill the key information of the original question. Recently, large-scale datasets have significantly propelled the development of several summarization tasks, such as multi-document summarization and dialogue summarization. However, a lack of a domain-expert annotated dataset for the consumer healthcare questions summarization task inhibits the development of an efficient summarization system. To address this issue, we introduce a new dataset, CHQ-Sum,m that contains 1507 domain-expert annotated consumer health questions and corresponding summaries. The dataset is derived from the community question answering forum and therefore provides a valuable resource for understanding consumer health-related posts on social media. We benchmark the dataset on multiple state-of-the-art summarization models to show the effectiveness of the dataset

</details>


### [289] [Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing](https://arxiv.org/abs/2512.23611)
*Yuwen Li,Wei Zhang,Zelong Huang,Mason Yang,Jiajun Wu,Shawn Guo,Huahao Hu,Lingyi Sun,Jian Yang,Mingjie Tang,Byran Dai*

Main category: cs.CL

TL;DR: InfTool是一个完全自主的多智能体框架，通过自演进的合成方法解决LLM调用外部工具的可靠性问题，无需人工标注即可大幅提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在三大挑战：高质量轨迹需要昂贵的人工标注、对未见工具泛化能力差、单模型合成存在质量上限（偏见和覆盖缺口）。

Method: 提出InfTool框架：基于原始API规范，协调三个协作智能体（用户模拟器、工具调用助手、MCP服务器）生成多样化的验证轨迹；通过带门控奖励的组相对策略优化（GRPO）训练模型，形成模型改进与数据生成的闭环迭代。

Result: 在伯克利函数调用排行榜（BFCL）上，InfTool将基础32B模型的准确率从19.8%提升至70.9%（+258%），超越10倍规模的模型，媲美Claude-Opus，且完全基于合成数据无需人工标注。

Conclusion: InfTool通过自主多智能体合成实现了工具调用能力的突破性提升，证明了完全基于合成数据的自演进框架在解决LLM工具调用可靠性问题上的有效性。

Abstract: Enabling Large Language Models (LLMs) to reliably invoke external tools remains a critical bottleneck for autonomous agents. Existing approaches suffer from three fundamental challenges: expensive human annotation for high-quality trajectories, poor generalization to unseen tools, and quality ceilings inherent in single-model synthesis that perpetuate biases and coverage gaps. We introduce InfTool, a fully autonomous framework that breaks these barriers through self-evolving multi-agent synthesis. Given only raw API specifications, InfTool orchestrates three collaborative agents (User Simulator, Tool-Calling Assistant, and MCP Server) to generate diverse, verified trajectories spanning single-turn calls to complex multi-step workflows. The framework establishes a closed loop: synthesized data trains the model via Group Relative Policy Optimization (GRPO) with gated rewards, the improved model generates higher-quality data targeting capability gaps, and this cycle iterates without human intervention. Experiments on the Berkeley Function-Calling Leaderboard (BFCL) demonstrate that InfTool transforms a base 32B model from 19.8% to 70.9% accuracy (+258%), surpassing models 10x larger and rivaling Claude-Opus, and entirely from synthetic data without human annotation.

</details>


### [290] [Nested Browser-Use Learning for Agentic Information Seeking](https://arxiv.org/abs/2512.23647)
*Baixuan Li,Jialong Wu,Wenbiao Yin,Kuan Li,Zhongwang Zhang,Huifeng Yin,Zhengwei Tao,Liwen Zhang,Pengjun Xie,Jingren Zhou,Yong Jiang*

Main category: cs.CL

TL;DR: 提出NestBrowse框架，通过嵌套结构解耦浏览器交互控制与页面探索，简化智能体推理，提升深度网络信息获取能力。


<details>
  <summary>Details</summary>
Motivation: 现有信息搜索智能体主要依赖API片段检索和URL页面抓取，无法充分利用真实浏览的丰富信息，而完整浏览器交互的细粒度控制和冗长页面内容增加了智能体推理复杂度。

Method: 提出Nested Browser-Use Learning (NestBrowse)，设计最小化且完整的浏览器动作框架，采用嵌套结构分离交互控制与页面探索。

Result: 在挑战性深度信息搜索基准测试中，NestBrowse展现出明显优势，深入分析证实其高效性和灵活性。

Conclusion: NestBrowse通过结构化浏览器交互设计，有效平衡了功能完整性与智能体推理简化，为深度网络信息获取提供了实用解决方案。

Abstract: Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.

</details>


### [291] [Less is more: Probabilistic reduction is best explained by small-scale predictability measures](https://arxiv.org/abs/2512.23659)
*Cassandra L. Jacobs,Andrés Buxó-Lugo,Anna K. Taylor,Marie Leopold-Hooke*

Main category: cs.CL

TL;DR: 本文探讨了研究语言模型概率与认知现象关系时所需的上下文量，发现n-gram表示足以作为认知规划单元。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于确定在探究语言模型概率与认知现象关系时，需要多少上下文信息才足够或合适。

Method: 通过比较整句与n-gram表示，分析它们是否足以观察概率简化现象。

Result: 研究发现n-gram表示足以作为认知规划单元，无需整句上下文即可观察到概率简化。

Conclusion: 结论表明n-gram表示是研究语言模型概率与认知现象关系的有效认知单元，整句上下文并非必需。

Abstract: The primary research questions of this paper center on defining the amount of context that is necessary and/or appropriate when investigating the relationship between language model probabilities and cognitive phenomena. We investigate whether whole utterances are necessary to observe probabilistic reduction and demonstrate that n-gram representations suffice as cognitive units of planning.

</details>


### [292] [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](https://arxiv.org/abs/2512.23684)
*Panagiotis Theocharopoulos,Ajinkya Kulkarni,Mathew Magimai. -Doss*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在学术评审中易受文档级隐藏提示注入攻击，不同语言攻击效果差异显著。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地应用于学术同行评审等高影响力工作流程，但其易受文档级隐藏提示注入攻击的脆弱性尚未得到充分研究。

Method: 构建包含约500篇ICML录用论文的数据集，在每篇论文中嵌入四种不同语言（英语、日语、中文、阿拉伯语）的语义等效对抗性提示，使用LLM进行评审评估。

Result: 英语、日语和中文的提示注入导致评审分数和录用/拒绝决策发生显著变化，而阿拉伯语注入几乎无影响，表明不同语言的脆弱性存在明显差异。

Conclusion: 基于LLM的评审系统易受文档级提示注入攻击，且攻击效果存在语言依赖性，这突显了在实际部署前需要加强安全评估。

Abstract: Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.

</details>


### [293] [PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech](https://arxiv.org/abs/2512.23686)
*Deepak Babu Piskala*

Main category: cs.CL

TL;DR: 提出了ProfASR-Bench专业语音识别评测套件，用于评估高风险场景下的ASR性能，发现当前系统存在上下文利用差距问题。


<details>
  <summary>Details</summary>
Motivation: 现有ASR评测基准未能充分反映专业场景中的挑战：密集领域术语、正式语体变化以及对关键实体错误的零容忍需求。

Method: 构建包含金融、医疗、法律和技术领域的专业语音评测数据集，采用Whisper和Qwen-Omni模型在无上下文、用户画像、领域+画像、理想提示和对抗提示五种条件下进行对比实验。

Result: 发现轻量级文本上下文对平均词错误率改善有限，对抗提示未能可靠降低性能，揭示了当前系统虽然支持提示但未能充分利用可用信息的上下文利用差距。

Conclusion: ProfASR-Bench提供了标准化的上下文阶梯、实体感知和分片报告机制，为比较不同模型家族的融合策略提供了可复现测试平台。

Abstract: Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.
  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench
  Code: https://github.com/prdeepakbabu/ProfASR-Bench

</details>


### [294] [Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans](https://arxiv.org/abs/2512.23693)
*Sky CH-Wang,Justin Svegliato,Helen Appel,Jason Eisner*

Main category: cs.CL

TL;DR: 提出了一种基于反馈驱动改进链的偏好监督方法，通过细粒度标注和逐步重写构建偏好对，用于微调语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有直接对齐方法（如标准A/B偏好排序或完整对比重写）可能效率较低，需要更结构化、基于修订的监督来提升偏好调优效果。

Method: 1. 标注者对模型响应标注“喜欢”和“不喜欢”的文本片段，并说明原因；2. 基础模型从左到右重写“不喜欢”片段，形成改进链；3. 从链中相邻步骤构建偏好对用于直接对齐训练。

Result: 该方法在偏好调优中优于基于标准A/B偏好排序或完整对比重写的直接对齐方法，表明结构化、基于修订的监督能带来更高效和有效的偏好学习。

Conclusion: 反馈驱动的改进链方法通过局部化、目标明确的编辑学习，提升了语言模型偏好监督的效果和效率。

Abstract: We present a method and dataset for fine-tuning language models with preference supervision using feedback-driven improvement chains. Given a model response, an annotator provides fine-grained feedback by marking ``liked'' and ``disliked'' spans and specifying what they liked or disliked about them. The base model then rewrites the disliked spans accordingly, proceeding from left to right, forming a sequence of incremental improvements. We construct preference pairs for direct alignment from each adjacent step in the chain, enabling the model to learn from localized, targeted edits. We find that our approach outperforms direct alignment methods based on standard A/B preference ranking or full contrastive rewrites, demonstrating that structured, revision-based supervision leads to more efficient and effective preference tuning.

</details>


### [295] [Eliciting Behaviors in Multi-Turn Conversations](https://arxiv.org/abs/2512.23701)
*Jing Huang,Shujian Zhang,Lun Wang,Andrew Hard,Rajiv Mathews,John Lambert*

Main category: cs.CL

TL;DR: 本文研究了多轮对话中大型语言模型的行为诱导方法，提出了三种方法分类框架，并发现在线交互方法在行为诱导方面显著优于静态方法。


<details>
  <summary>Details</summary>
Motivation: 现有行为诱导方法主要针对单轮对话场景，缺乏对多轮对话中复杂行为诱导的系统研究，而多轮对话评估对LLM的实际应用至关重要。

Method: 提出了分析框架将现有方法分为三类：仅使用先验知识的方法、使用离线交互的方法、从在线交互中学习的方法；引入了广义多轮在线方法统一单轮和多轮诱导；通过查询预算和成功率权衡评估方法效率。

Result: 在线方法在三个任务中仅用数千次查询就实现了平均45%/19%/77%的成功率，而现有多轮对话基准中的静态方法几乎找不到失败案例。

Conclusion: 行为诱导方法在多轮对话评估中具有新颖应用价值，研究社区需要从静态基准转向动态基准以更好地评估LLM行为。

Abstract: Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [296] [VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs](https://arxiv.org/abs/2512.22342)
*Wensi Huang,Shaohao Zhu,Meng Wei,Jinming Xu,Xihui Liu,Hanqing Wang,Tai Wang,Feng Zhao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: 提出了交互式实例物体导航任务和VL-LN基准，通过主动对话解决模糊导航指令，在41k轨迹数据集上验证了对话增强导航模型的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有导航任务指令通常明确，而真实场景中指令常模糊，需要智能体通过主动对话推断用户意图，现有研究对此关注不足。

Method: 扩展实例物体导航为交互式任务，允许智能体用自然语言咨询先知；构建VL-LN基准，包含自动生成的大规模数据集和评估协议，训练具备对话能力的导航模型。

Result: 对话增强导航模型显著优于基线，VL-LN基准在41k长时程对话增强轨迹上验证了方法的有效性和可靠性。

Conclusion: VL-LN基准推动了对话增强具身导航研究，证明了主动对话对处理模糊指令的重要性，为实际应用提供了更贴近现实的评估框架。

Abstract: In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Object Navigation (IION), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IION extends Instance Object Navigation (ION) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/

</details>


### [297] [Joint UAV-UGV Positioning and Trajectory Planning via Meta A3C for Reliable Emergency Communications](https://arxiv.org/abs/2512.22187)
*Ndagijimana Cyprien,Mehdi Sookhak,Hosein Zarini,Chandra N Sekharan,Mohammed Atiquzzaman*

Main category: cs.RO

TL;DR: 提出了一种基于元学习的异步优势演员-评论家（Meta-A3C）框架，用于无人机与地面无人车的联合部署、定位和轨迹规划，以在灾后通信中优化服务质量并减少无人机使用数量。


<details>
  <summary>Details</summary>
Motivation: 灾后通信恢复中，无人机与地面无人车联合部署可有效建立通信，但需在保证服务质量的同时最小化无人机使用数量，这需要解决两者的最优定位和轨迹规划问题。

Method: 引入道路图模型约束地面无人车移动；将总速率优化问题重构为马尔可夫决策过程；提出融合元学习的异步优势演员-评论家算法，以实现对新环境和动态条件的快速适应。

Result: 数值实验表明，Meta-A3C算法在吞吐量上比A3C和DDPG提升13.1%，执行速度加快49%，且满足服务质量要求。

Conclusion: 所提框架能有效优化无人机与地面无人车的协同部署，在动态灾后环境中实现高效通信覆盖与资源利用。

Abstract: Joint deployment of unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs) has been shown to be an effective method to establish communications in areas affected by disasters. However, ensuring good Quality of Services (QoS) while using as few UAVs as possible also requires optimal positioning and trajectory planning for UAVs and UGVs. This paper proposes a joint UAV-UGV-based positioning and trajectory planning framework for UAVs and UGVs deployment that guarantees optimal QoS for ground users. To model the UGVs' mobility, we introduce a road graph, which directs their movement along valid road segments and adheres to the road network constraints. To solve the sum rate optimization problem, we reformulate the problem as a Markov Decision Process (MDP) and propose a novel asynchronous Advantage Actor Critic (A3C) incorporated with meta-learning for rapid adaptation to new environments and dynamic conditions. Numerical results demonstrate that our proposed Meta-A3C approach outperforms A3C and DDPG, delivering 13.1\% higher throughput and 49\% faster execution while meeting the QoS requirements.

</details>


### [298] [A Unified AI, Embedded, Simulation, and Mechanical Design Approach to an Autonomous Delivery Robot](https://arxiv.org/abs/2512.22408)
*Amro Gamar,Ahmed Abduljalil,Alargam Mohammed,Ali Elhenidy,Abeer Tawakol*

Main category: cs.RO

TL;DR: 开发了一个集机械工程、嵌入式系统和人工智能于一体的全自主送货机器人，采用异构计算架构，实现了AI感知、路径规划和实时电机控制，并通过AWS物联网监控提升了系统可靠性。


<details>
  <summary>Details</summary>
Motivation: 为应对资源受限平台上计算密集型AI算法的优化挑战，以及ROS 2主机与嵌入式控制器之间低延迟可靠通信的需求，旨在构建一个可在现实世界中部署的鲁棒自主送货系统。

Method: 采用异构计算架构（RPi 5运行ROS 2处理AI感知与路径规划，ESP32运行FreeRTOS实现实时电机控制），通过精确电机选型和材料工程优化机械设计，并利用AWS IoT进行监控及固件级电机停机安全机制。

Result: 实现了基于PID的确定性电机控制，通过严格的内存和任务管理优化了系统性能，成功构建了一个功能完备、可实际部署的自主送货机器人平台。

Conclusion: 本研究展示了一种统一的多学科方法，成功开发出具有现实世界部署能力的鲁棒自主送货系统，为资源受限环境下的自主机器人集成提供了可行解决方案。

Abstract: This paper presents the development of a fully autonomous delivery robot integrating mechanical engineering, embedded systems, and artificial intelligence. The platform employs a heterogeneous computing architecture, with RPi 5 and ROS 2 handling AI-based perception and path planning, while ESP32 running FreeRTOS ensures real-time motor control. The mechanical design was optimized for payload capacity and mobility through precise motor selection and material engineering. Key technical challenges addressed include optimizing computationally intensive AI algorithms on a resource-constrained platform and implementing a low-latency, reliable communication link between the ROS 2 host and embedded controller. Results demonstrate deterministic, PID-based motor control through rigorous memory and task management, and enhanced system reliability via AWS IoT monitoring and a firmware-level motor shutdown failsafe. This work highlights a unified, multi-disciplinary methodology, resulting in a robust and operational autonomous delivery system capable of real-world deployment.

</details>


### [299] [Bugs with Features: Vision-Based Fault-Tolerant Collective Motion Inspired by Nature](https://arxiv.org/abs/2512.22448)
*Peleg Shefi,Amir Ayali,Gal A. Kaminka*

Main category: cs.RO

TL;DR: 该论文提出两种受蝗虫启发的机制，通过结合视觉感知的邻居水平和垂直尺寸来估计距离，并引入间歇运动来检测和避免故障机器人，显著提升了群体运动的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自然群体运动具有鲁棒性，但人工群体（尤其是基于视觉感知的）由于视觉感知的模糊性和信息丢失往往很脆弱。研究旨在解决视觉感知局限下群体运动的脆弱性问题。

Method: 1. 开发结合视觉感知邻居水平和垂直尺寸的鲁棒距离估计方法；2. 引入间歇运动机制，使机器人能可靠检测落后同伴并中断群体运动；3. 通过大量基于物理的仿真实验验证方法在避障-吸引模型和对齐模型中的有效性。

Result: 仿真实验表明，使用这些技术后群体韧性得到显著提升，适用于多种实验场景下的距离基避障-吸引模型和对齐模型。

Conclusion: 受蝗虫启发的视觉距离估计和间歇运动机制能有效增强人工群体在感知受限条件下的鲁棒性，为脆弱人工群体提供了可行的解决方案。

Abstract: In collective motion, perceptually-limited individuals move in an ordered manner, without centralized control. The perception of each individual is highly localized, as is its ability to interact with others. While natural collective motion is robust, most artificial swarms are brittle. This particularly occurs when vision is used as the sensing modality, due to ambiguities and information-loss inherent in visual perception. This paper presents mechanisms for robust collective motion inspired by studies of locusts. First, we develop a robust distance estimation method that combines visually perceived horizontal and vertical sizes of neighbors. Second, we introduce intermittent locomotion as a mechanism that allows robots to reliably detect peers that fail to keep up, and disrupt the motion of the swarm. We show how such faulty robots can be avoided in a manner that is robust to errors in classifying them as faulty. Through extensive physics-based simulation experiments, we show dramatic improvements to swarm resilience when using these techniques. We show these are relevant to both distance-based Avoid-Attract models, as well as to models relying on Alignment, in a wide range of experiment settings.

</details>


### [300] [Emergence of Human to Robot Transfer in Vision-Language-Action Models](https://arxiv.org/abs/2512.22414)
*Simar Kareer,Karl Pertsch,James Darpinian,Judy Hoffman,Danfei Xu,Sergey Levine,Chelsea Finn,Suraj Nair*

Main category: cs.RO

TL;DR: 研究发现，当视觉-语言-动作模型在足够多样化的场景、任务和实体上进行预训练后，能够实现从人类视频到机器人的技能迁移，这种涌现能力源于预训练产生的实体无关表示。


<details>
  <summary>Details</summary>
Motivation: 人类视频覆盖了丰富的真实世界场景且易于获取，但直接用于训练机器人模型存在映射困难。受大语言模型从多样化监督中学习能力的启发，研究者探索视觉-语言-动作模型是否也能通过规模化训练实现从人类视频到机器人的迁移。

Method: 提出一种简单的协同训练方法，在大量多样化的人类和机器人数据上进行预训练，通过实验分析人类到机器人技能迁移的涌现现象。

Result: 当预训练数据覆盖足够多样化的场景、任务和实体时，模型能够实现从人类视频到机器人的技能迁移，在仅有人类数据的泛化场景中性能提升近一倍。分析表明这种能力源于预训练产生的实体无关表示。

Conclusion: 多样化预训练能使视觉-语言-动作模型学习到实体无关的表示，从而实现从人类视频到机器人的有效技能迁移，这为利用丰富的人类视频数据训练通用机器人模型提供了可行路径。

Abstract: Vision-language-action (VLA) models can enable broad open world generalization, but require large and diverse datasets. It is appealing to consider whether some of this data can come from human videos, which cover diverse real-world situations and are easy to obtain. However, it is difficult to train VLAs with human videos alone, and establishing a mapping between humans and robots requires manual engineering and presents a major research challenge. Drawing inspiration from advances in large language models, where the ability to learn from diverse supervision emerges with scale, we ask whether a similar phenomenon holds for VLAs that incorporate human video data. We introduce a simple co-training recipe, and find that human-to-robot transfer emerges once the VLA is pre-trained on sufficient scenes, tasks, and embodiments. Our analysis suggests that this emergent capability arises because diverse pretraining produces embodiment-agnostic representations for human and robot data. We validate these findings through a series of experiments probing human to robot skill transfer and find that with sufficiently diverse robot pre-training our method can nearly double the performance on generalization settings seen only in human data.

</details>


### [301] [Asymmetric Friction in Geometric Locomotion](https://arxiv.org/abs/2512.22484)
*Ross L. Hatton,Yousef Salaman,Shai Revzen*

Main category: cs.RO

TL;DR: 本文扩展了基于几何力学的运动模型，将各向异性线性阻力推广到非对称阻力情况，用Finsler度量替代黎曼度量，建立了子Finsler框架下的运动映射理论。


<details>
  <summary>Details</summary>
Motivation: 现有几何力学模型主要基于各向同性或各向异性线性阻力（黎曼度量描述），但实际生物和机器人运动常面临非对称阻力环境（如前后阻力系数不同），需要更一般的理论框架。

Method: 将阻力模型从黎曼度量推广到Finsler度量，提出子Finsler方法构建系统运动映射，类比子黎曼系统的约束曲率概念，建立非对称阻力下的运动能力表征体系。

Result: 证明了子黎曼构造运动映射的方法可自然扩展到子Finsler框架，识别了与约束曲率类似的系统特性，能够刻画非对称阻力系统的运动能力。

Conclusion: 子Finsler框架为分析非对称阻力环境下的运动系统提供了更一般的几何力学理论，扩展了传统几何运动模型的应用范围。

Abstract: Geometric mechanics models of locomotion have provided insight into how robots and animals use environmental interactions to convert internal shape changes into displacement through the world, encoding this relationship in a ``motility map''. A key class of such motility maps arises from (possibly anisotropic) linear drag acting on the system's individual body parts, formally described via Riemannian metrics on the motions of the system's individual body parts. The motility map can then be generated by invoking a sub-Riemannian constraint on the aggregate system motion under which the position velocity induced by a given shape velocity is that which minimizes the power dissipated via friction. The locomotion of such systems is ``geometric'' in the sense that the final position reached by the system depends only on the sequence of shapes that the system passes through, but not on the rate with which the shape changes are made.
  In this paper, we consider a far more general class of systems in which the drag may be not only anisotropic (with different coefficients for forward/backward and left/right motions), but also asymmetric (with different coefficients for forward and backward motions). Formally, including asymmetry in the friction replaces the Riemannian metrics on the body parts with Finsler metrics. We demonstrate that the sub-Riemannian approach to constructing the system motility map extends naturally to a sub-Finslerian approach and identify system properties analogous to the constraint curvature of sub-Riemannian systems that allow for the characterization of the system motion capabilities.

</details>


### [302] [Topology-Preserving Scalar Field Optimization for Boundary-Conforming Spiral Toolpaths on Multiply Connected Freeform Surfaces](https://arxiv.org/abs/2512.22502)
*Shen Changqing,Xu Bingzhou,Qi Bosong,Zhang Xiaojian,Yan Sijie,Ding Han*

Main category: cs.RO

TL;DR: 提出一种用于多连通自由曲面球头铣削路径规划的高效策略，通过保形狭缝映射和拓扑保持网格变形优化，实现连续、边界贴合且无自交的刀具路径，显著提升加工效率与质量。


<details>
  <summary>Details</summary>
Motivation: 在多连通自由曲面加工中，现有基于标量场优化的方法难以同时保证边界贴合性和消除零梯度奇异性，导致刀具路径分支或中断，影响加工连续性与质量。

Method: 采用保形狭缝映射构建无奇异的初始标量场；将优化重构为边界同步更新的拓扑保持网格变形过程，实现全局优化的路径间距、残留高度均匀性和轨迹平滑过渡。

Result: 与现有先进方法相比，加工效率提升14.24%，残留高度均匀性改善5.70%，铣削冲击振动降低超10%，刀具路径连续、边界贴合且无自交。

Conclusion: 所提策略能有效解决多连通曲面加工中的路径连续性与边界约束问题，为高性能加工场景提供了通用性强的解决方案。

Abstract: Ball-end milling path planning on multiply connected freeform surfaces is pivotal for high-quality and efficient machining of components in automotive and aerospace manufacturing. Although scalar-field-based optimization provides a unified framework for multi-objective toolpath generation, maintaining boundary conformity while eliminating zero-gradient singularities that cause iso-curve branching or termination and disrupt toolpath continuity remains challenging on multiply connected surfaces. We propose an efficient strategy to robustly enforce these constraints throughout optimization. Conformal slit mapping is employed to construct a feasible, singularity-free initial scalar field. The optimization is reformulated as a topology-preserving mesh deformation governed by boundary-synchronous updates, enabling globally optimized spacing, scallop-height uniformity, and smooth trajectory transitions. Consequently, the toolpaths are continuous, boundary-conforming, and free of self-intersections. Milling experiments demonstrate that, compared with a state-of-the-art conformal slit mapping-based method, the proposed approach increases machining efficiency by 14.24%, improves scallop-height uniformity by 5.70%, and reduces milling impact-induced vibrations by over 10%. The strategy offers broad applicability in high-performance machining scenarios.

</details>


### [303] [Clutter-Resistant Vision-Language-Action Models through Object-Centric and Geometry Grounding](https://arxiv.org/abs/2512.22519)
*Khoa Vo,Taisei Hanyu,Yuki Ikebe,Trong Thang Pham,Nhat Chung,Minh Nhat Vu,Duy Nguyen Ho Minh,Anh Nguyen,Anthony Gunderman,Chase Rainwater,Ngan Le*

Main category: cs.RO

TL;DR: 提出OBEYED-VLA框架，通过显式解耦感知与动作推理，增强视觉-语言-动作模型在机器人操作中的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型将感知与控制耦合在单一管道中，导致语言条件接地能力下降，在真实场景中出现过度抓取、受杂物干扰和背景过拟合等问题。

Method: 引入感知模块，将多视角输入转换为任务条件化、以物体为中心且几何感知的观测。该模块包括基于VLM的物体中心接地阶段（选择任务相关物体区域）和几何接地阶段（强调物体3D结构而非外观），随后将接地后的视图输入预训练VLA策略进行微调。

Result: 在真实UR10e桌面实验中，OBEYED-VLA在四种挑战性场景（干扰物体、目标缺失拒绝、背景外观变化、未见物体杂乱操作）中显著优于基线VLA模型，消融实验证实语义接地和几何接地均对性能提升至关重要。

Conclusion: 将感知设计为显式、以物体为中心的组件能有效增强VLA模型在机器人操作中的鲁棒性和泛化能力。

Abstract: Recent Vision-Language-Action (VLA) models have made impressive progress toward general-purpose robotic manipulation by post-training large Vision-Language Models (VLMs) for action prediction. Yet most VLAs entangle perception and control in a monolithic pipeline optimized purely for action, which can erode language-conditioned grounding. In our real-world tabletop tests, policies over-grasp when the target is absent, are distracted by clutter, and overfit to background appearance.
  To address these issues, we propose OBEYED-VLA (OBject-centric and gEometrY groundED VLA), a framework that explicitly disentangles perceptual grounding from action reasoning. Instead of operating directly on raw RGB, OBEYED-VLA augments VLAs with a perception module that grounds multi-view inputs into task-conditioned, object-centric, and geometry-aware observations. This module includes a VLM-based object-centric grounding stage that selects task-relevant object regions across camera views, along with a complementary geometric grounding stage that emphasizes the 3D structure of these objects over their appearance. The resulting grounded views are then fed to a pretrained VLA policy, which we fine-tune exclusively on single-object demonstrations collected without environmental clutter or non-target objects.
  On a real-world UR10e tabletop setup, OBEYED-VLA substantially improves robustness over strong VLA baselines across four challenging regimes and multiple difficulty levels: distractor objects, absent-target rejection, background appearance changes, and cluttered manipulation of unseen objects. Ablation studies confirm that both semantic grounding and geometry-aware grounding are critical to these gains. Overall, the results indicate that making perception an explicit, object-centric component is an effective way to strengthen and generalize VLA-based robotic manipulation.

</details>


### [304] [VLA-Arena: An Open-Source Framework for Benchmarking Vision-Language-Action Models](https://arxiv.org/abs/2512.22539)
*Borong Zhang,Jiahao Li,Jiachen Shen,Yishuai Cai,Yuhao Zhang,Yuanpei Chen,Juntao Dai,Jiaming Ji,Yaodong Yang*

Main category: cs.RO

TL;DR: 本文提出了VLA-Arena基准测试，通过结构化任务设计框架从任务结构、语言指令和视觉观察三个正交维度量化难度，系统评估视觉-语言-动作模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型快速发展，但缺乏系统方法定量理解其能力边界和失败模式，需要建立标准化评估基准。

Method: 设计包含170个任务的基准，通过任务结构（安全、干扰、外推、长时程四个维度）、语言扰动（W0-W4）和视觉扰动（V0-V4）三个正交轴控制难度，仅用L0难度数据微调以评估泛化能力。

Result: 评估发现先进模型存在严重局限：过度依赖记忆而非泛化、鲁棒性不对称、忽视安全约束、无法组合技能完成长时程任务。

Conclusion: VLA-Arena揭示了当前模型的系统性缺陷，提供了包含完整工具链和数据集的开源基准，旨在推动解决这些挑战的可复现研究。

Abstract: While Vision-Language-Action models (VLAs) are rapidly advancing towards generalist robot policies, it remains difficult to quantitatively understand their limits and failure modes. To address this, we introduce a comprehensive benchmark called VLA-Arena. We propose a novel structured task design framework to quantify difficulty across three orthogonal axes: (1) Task Structure, (2) Language Command, and (3) Visual Observation. This allows us to systematically design tasks with fine-grained difficulty levels, enabling a precise measurement of model capability frontiers. For Task Structure, VLA-Arena's 170 tasks are grouped into four dimensions: Safety, Distractor, Extrapolation, and Long Horizon. Each task is designed with three difficulty levels (L0-L2), with fine-tuning performed exclusively on L0 to assess general capability. Orthogonal to this, language (W0-W4) and visual (V0-V4) perturbations can be applied to any task to enable a decoupled analysis of robustness. Our extensive evaluation of state-of-the-art VLAs reveals several critical limitations, including a strong tendency toward memorization over generalization, asymmetric robustness, a lack of consideration for safety constraints, and an inability to compose learned skills for long-horizon tasks. To foster research addressing these challenges and ensure reproducibility, we provide the complete VLA-Arena framework, including an end-to-end toolchain from task definition to automated evaluation and the VLA-Arena-S/M/L datasets for fine-tuning. Our benchmark, data, models, and leaderboard are available at https://vla-arena.github.io.

</details>


### [305] [ParaMaP: Parallel Mapping and Collision-free Motion Planning for Reactive Robot Manipulation](https://arxiv.org/abs/2512.22575)
*Xuewei Zhang,Bailing Tian,Kai Zheng,Yulin Hui,Junjie Lu,Zhiyu Li*

Main category: cs.RO

TL;DR: 提出了一种并行建图与运动规划框架，通过GPU加速的欧几里得距离变换和采样模型预测控制，实现未知环境中机械臂的实时无碰撞运动规划。


<details>
  <summary>Details</summary>
Motivation: 未知环境中机器人操作需要实时感知更新和频繁在线重规划，现有方法在实时性和防碰撞方面面临挑战。

Method: 1) 使用GPU加速的欧几里得距离变换构建稠密距离场环境表示，并加入机器人掩码更新机制避免误检自碰撞；2) 将运动生成建模为随机优化问题，在SE(3)流形上定义几何一致的位姿跟踪度量，通过采样模型预测控制并行评估候选轨迹。

Result: 框架在GPU上实现高频重规划，通过仿真和7自由度机械臂实物实验验证了有效性，能够实现实时无碰撞运动。

Conclusion: 所提并行框架通过紧密集成环境表示与规划器，解决了未知环境中实时运动规划的挑战，为动态环境下的机器人操作提供了可行方案。

Abstract: Real-time and collision-free motion planning remains challenging for robotic manipulation in unknown environments due to continuous perception updates and the need for frequent online replanning. To address these challenges, we propose a parallel mapping and motion planning framework that tightly integrates Euclidean Distance Transform (EDT)-based environment representation with a sampling-based model predictive control (SMPC) planner. On the mapping side, a dense distance-field-based representation is constructed using a GPU-based EDT and augmented with a robot-masked update mechanism to prevent false self-collision detections during online perception. On the planning side, motion generation is formulated as a stochastic optimization problem with a unified objective function and efficiently solved by evaluating large batches of candidate rollouts in parallel within a SMPC framework, in which a geometrically consistent pose tracking metric defined on SE(3) is incorporated to ensure fast and accurate convergence to the target pose. The entire mapping and planning pipeline is implemented on the GPU to support high-frequency replanning. The effectiveness of the proposed framework is validated through extensive simulations and real-world experiments on a 7-DoF robotic manipulator. More details are available at: https://zxw610.github.io/ParaMaP.

</details>


### [306] [Modeling of UAV Tether Aerodynamics for Real-Time Simulation](https://arxiv.org/abs/2512.22588)
*Max Beffert,Andreas Zell*

Main category: cs.RO

TL;DR: 提出了两种实时准静态系绳建模方法（解析法和数值法），用于处理移动基座或强风条件下系留无人机的系绳力计算，并在实际测试中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 多旋翼无人机受电池限制续航时间短，地面通过系绳供电是连续运行的实用方案，但移动基座或强风场景需建模包含空气动力效应的系绳力。

Method: 1. 基于悬链线理论和均匀阻力假设的解析法，求解时间<1ms；2. 将系绳离散为分段和集中质量的数值法，使用CasADi和IPOPT求解平衡方程，通过热启动和解析初始化实现5ms实时求解。

Result: 实际负载传感器测试表明：解析法在大多数系留无人机应用中精度足够且计算成本极低；数值法则在需要时提供更高灵活性和物理精度。

Conclusion: 两种方法构成了轻量可扩展的实时系绳仿真框架，适用于离线优化和在线任务（仿真、控制、轨迹规划）。

Abstract: One of the main limitations of multirotor UAVs is their short flight time due to battery constraints. A practical solution for continuous operation is to power the drone from the ground via a tether. While this approach has been demonstrated for stationary systems, scenarios with a fast-moving base vehicle or strong wind conditions require modeling the tether forces, including aerodynamic effects. In this work, we propose two complementary approaches for real-time quasi-static tether modeling with aerodynamics. The first is an analytical method based on catenary theory with a uniform drag assumption, achieving very fast solve times below 1ms. The second is a numerical method that discretizes the tether into segments and lumped masses, solving the equilibrium equations using CasADi and IPOPT. By leveraging initialization strategies, such as warm starting and analytical initialization, real-time performance was achieved with a solve time of 5ms, while allowing for flexible force formulations. Both approaches were validated in real-world tests using a load cell to measure the tether force. The results show that the analytical method provides sufficient accuracy for most tethered UAV applications with minimal computational cost, while the numerical method offers higher flexibility and physical accuracy when required. These approaches form a lightweight and extensible framework for real-time tether simulation, applicable to both offline optimization and online tasks such as simulation, control, and trajectory planning.

</details>


### [307] [Sistema de navegación de cobertura para vehículos no holonómicos en ambientes de exterior](https://arxiv.org/abs/2512.22734)
*Michelle Valenzuela,Francisco Leiva,Javier Ruiz-del-Solar*

Main category: cs.RO

TL;DR: 提出了一种用于非完整机器人的覆盖导航系统，旨在实现特定区域的完全覆盖，并能在遇到动态障碍时进行规避和恢复，测试覆盖率接近90%。


<details>
  <summary>Details</summary>
Motivation: 覆盖导航在移动机器人中至关重要，例如清洁、采矿等工业场景。自动化这些过程能提高安全性，本文旨在为采矿等行业中需要覆盖导航的单元过程自动化提供概念验证。

Method: 开发了一个覆盖导航系统，包括路径规划算法和恢复行为机制。系统能处理动态或未映射障碍物，执行规避和恢复动作，并在模拟和真实室外环境中进行了测试。

Result: 在大多数实验中，系统实现了接近90%的覆盖率，验证了其在复杂环境中的有效性。

Conclusion: 该系统证明了覆盖导航自动化的可行性，下一步计划将系统扩展到采矿机械上，并在真实环境中进行验证。

Abstract: In mobile robotics, coverage navigation refers to the deliberate movement of a robot with the purpose of covering a certain area or volume. Performing this task properly is fundamental for the execution of several activities, for instance, cleaning a facility with a robotic vacuum cleaner. In the mining industry, it is required to perform coverage in several unit processes related with material movement using industrial machinery, for example, in cleaning tasks, in dumps, and in the construction of tailings dam walls. The automation of these processes is fundamental to enhance the security associated with their execution. In this work, a coverage navigation system for a non-holonomic robot is presented. This work is intended to be a proof of concept for the potential automation of various unit processes that require coverage navigation like the ones mentioned before. The developed system includes the calculation of routes that allow a mobile platform to cover a specific area, and incorporates recovery behaviors in case that an unforeseen event occurs, such as the arising of dynamic or previously unmapped obstacles in the terrain to be covered, e.g., other machines or pedestrians passing through the area, being able to perform evasive maneuvers and post-recovery to ensure a complete coverage of the terrain. The system was tested in different simulated and real outdoor environments, obtaining results near 90% of coverage in the majority of experiments. The next step of development is to scale up the utilized robot to a mining machine/vehicle whose operation will be validated in a real environment. The result of one of the tests performed in the real world can be seen in the video available in https://youtu.be/gK7_3bK1P5g.

</details>


### [308] [Active Constraint Learning in High Dimensions from Demonstrations](https://arxiv.org/abs/2512.22757)
*Zheng Qiu,Chih-Yuan Chiu,Glen Chou*

Main category: cs.RO

TL;DR: 提出一种迭代式主动约束学习算法，通过智能请求信息丰富的演示轨迹来推断演示者环境中的未知约束，在非线性动态和约束场景中优于随机采样基线方法。


<details>
  <summary>Details</summary>
Motivation: 在从演示学习（LfD）中，传统方法通常假设演示数据充足且信息丰富，但实际中获取高质量演示成本高。本研究旨在开发一种能主动选择最具信息量的演示来高效学习未知约束的方法。

Method: 采用迭代式主动约束学习（ACL）框架：1）使用高斯过程（GP）对现有演示数据集建模未知约束；2）基于GP后验分布智能查询起始/目标状态；3）生成信息丰富的演示并加入数据集；4）在仿真和硬件实验中验证，对比随机采样基线。

Result: 在高维非线性动态和未知非线性约束的仿真与硬件实验中，该方法在从稀疏但信息丰富的演示集中准确推断约束方面，显著优于基于随机采样的基线方法。

Conclusion: 迭代式主动约束学习算法能有效选择信息量最大的演示，以更少的演示数据实现更准确的约束推断，为从演示学习中的约束推理问题提供了高效解决方案。

Abstract: We present an iterative active constraint learning (ACL) algorithm, within the learning from demonstrations (LfD) paradigm, which intelligently solicits informative demonstration trajectories for inferring an unknown constraint in the demonstrator's environment. Our approach iteratively trains a Gaussian process (GP) on the available demonstration dataset to represent the unknown constraints, uses the resulting GP posterior to query start/goal states, and generates informative demonstrations which are added to the dataset. Across simulation and hardware experiments using high-dimensional nonlinear dynamics and unknown nonlinear constraints, our method outperforms a baseline, random-sampling based method at accurately performing constraint inference from an iteratively generated set of sparse but informative demonstrations.

</details>


### [309] [Two-Robot Computational Landscape: A Complete Characterization of Model Power in Minimal Mobile Robot Systems](https://arxiv.org/abs/2512.22770)
*Naoki Kitamura,Yuichi Sudo,Koichi Wada*

Main category: cs.RO

TL;DR: 本文首次完整刻画了两个自主移动机器人在所有主要模型下的计算能力，揭示了与多机器人情况根本不同的计算层次结构。


<details>
  <summary>Details</summary>
Motivation: 尽管多机器人LCM模型的计算能力已被广泛研究，但两个机器人的精确计算层次结构一直未解决，这促使研究者探索最小规模协调的内在挑战。

Method: 采用无模拟方法，系统分析OBLOT、FSTA、FCOM和LUMI模型在FSYNCH、SSYNCH、ASYNCH及其原子变体调度器下的计算能力。

Result: 发现两个机器人场景下，FSTA^F和LUMI^F在全同步条件下等价，表明完美同步可替代记忆和通信；同时证明FSTA和FCOM正交，存在单向可解问题。

Conclusion: 研究首次建立了两个机器人的完整精确计算图景，揭示了最小规模协调的特殊性，为理解自主移动机器人的基本计算极限提供了新视角。

Abstract: The computational power of autonomous mobile robots under the Look-Compute-Move (LCM) model has been widely studied through an extensive hierarchy of robot models defined by the presence of memory, communication, and synchrony assumptions. While the general n-robot landscape has been largely established, the exact structure for two robots has remained unresolved. This paper presents the first complete characterization of the computational power of two autonomous robots across all major models, namely OBLOT, FSTA, FCOM, and LUMI, under the full spectrum of schedulers (FSYNCH, SSYNCH, ASYNCH, and their atomic variants). Our results reveal a landscape that fundamentally differs from the general case. Most notably, we prove that FSTA^F and LUMI^F coincide under full synchrony, a surprising collapse indicating that perfect synchrony can substitute both memory and communication when only two robots exist. We also show that FSTA and FCOM are orthogonal: there exists a problem solvable in the weakest communication model but impossible even in the strongest finite-state model, completing the bidirectional incomparability. All equivalence and separation results are derived through a novel simulation-free method, providing a unified and constructive view of the two-robot hierarchy. This yields the first complete and exact computational landscape for two robots, highlighting the intrinsic challenges of coordination at the minimal scale.

</details>


### [310] [The body is not there to compute: Comment on "Informational embodiment: Computational role of information structure in codes and robots" by Pitti et al](https://arxiv.org/abs/2512.22868)
*Matej Hoffmann*

Main category: cs.RO

TL;DR: 本文评论文章认为身体的主要功能不是计算，反驳了将计算和信息作为理解生物体进化和机器人设计的核心隐喻的观点。


<details>
  <summary>Details</summary>
Motivation: 针对目标文章将计算和信息理论应用于身体（解释动物进化与机器人设计）的观点提出批判，认为这种计算隐喻过于简化身体的本质功能。

Method: 采用哲学与理论分析的方法，基于信息理论和计算隐喻在认知科学中的应用历史，进行概念性论证。

Result: 指出身体在进化中的主要角色并非执行计算，而是作为生物与环境互动的物理载体，计算隐喻可能误导对身体功能的理解。

Conclusion: 主张在理解生物体进化和设计机器人时，应超越计算中心主义，更全面地考虑身体的物理、生态和互动维度。

Abstract: Applying the lens of computation and information has been instrumental in driving the technological progress of our civilization as well as in empowering our understanding of the world around us. The digital computer was and for many still is the leading metaphor for how our mind operates. Information theory (IT) has also been important in our understanding of how nervous systems encode and process information. The target article deploys information and computation to bodies: to understand why they have evolved in particular ways (animal bodies) and to design optimal bodies (robots). In this commentary, I argue that the main role of bodies is not to compute.

</details>


### [311] [P-FABRIK: A General Intuitive and Robust Inverse Kinematics Method for Parallel Mechanisms Using FABRIK Approach](https://arxiv.org/abs/2512.22927)
*Daqian Cao,Quan Yuan,Weibang Bai*

Main category: cs.RO

TL;DR: 提出P-FABRIK方法，基于FABRIK算法，通过拓扑分解策略将并联机构分解为多个串联子链，迭代求解逆运动学，适用于多种并联机构（包括冗余机构），并能处理工作空间外目标。


<details>
  <summary>Details</summary>
Motivation: 传统几何逆运动学方法依赖特定空间几何约束，难以应用于约束更复杂的冗余并联机构，且当目标位姿超出工作空间时无解，导致不可预测的控制问题。

Method: 基于FABRIK算法，提出新的拓扑分解策略，将通用并联机构分解为多个串联子链，通过迭代修正各子链末端目标位置来计算逆运动学解。

Result: 通过平面、标准和冗余并联机构的案例研究验证了方法的通用性；数值仿真证明了其有效性、计算效率以及处理工作空间外目标的鲁棒性。

Conclusion: P-FABRIK是一种通用、直观且鲁棒的逆运动学方法，能有效解决并联机构（尤其是冗余机构）的逆运动学问题，并克服传统方法在约束复杂性和工作空间限制方面的不足。

Abstract: Traditional geometric inverse kinematics methods for parallel mechanisms rely on specific spatial geometry constraints. However, their application to redundant parallel mechanisms is challenged due to the increased constraint complexity. Moreover, it will output no solutions and cause unpredictable control problems when the target pose lies outside its workspace. To tackle these challenging issues, this work proposes P-FABRIK, a general, intuitive, and robust inverse kinematics method to find one feasible solution for diverse parallel mechanisms based on the FABRIK algorithm. By decomposing the general parallel mechanism into multiple serial sub-chains using a new topological decomposition strategy, the end targets of each sub-chain can be subsequently revised to calculate the inverse kinematics solutions iteratively. Multiple case studies involving planar, standard, and redundant parallel mechanisms demonstrated the proposed method's generality across diverse parallel mechanisms. Furthermore, numerical simulation studies verified its efficacy and computational efficiency, as well as its robustness ability to handle out-of-workspace targets.

</details>


### [312] [PreGME: Prescribed Performance Control of Aerial Manipulators based on Variable-Gain ESO](https://arxiv.org/abs/2512.22957)
*Mengyu Ji,Shiliang Guo,Zhengzhen Li,Jiahao Shen,Huazi Cao,Shiyu Zhao*

Main category: cs.RO

TL;DR: 提出了一种基于变增益扩张状态观测器的规定性能运动控制框架（PreGME），用于解决空中机械臂的动态耦合问题，实现了高精度跟踪控制。


<details>
  <summary>Details</summary>
Motivation: 空中机械臂（多旋翼平台+机械臂）存在显著的动态耦合效应，导致精确鲁棒的运动控制难以实现，需要一种能处理快速变化耦合并保证跟踪性能的方法。

Method: 采用变增益扩张状态观测器实时估计动态耦合，并结合规定性能飞行控制（预设误差轨迹约束）构建控制框架。

Result: 实验表明，即使在机械臂快速运动（末端速度1.02 m/s，加速度5.10 m/s²）引起的强耦合下，该方法仍能保持高跟踪性能，成功完成空中转杆、调酒和拉车等任务。

Conclusion: PreGME框架能有效估计快速变化的动态耦合，并通过规定性能控制确保误差在预设范围内，为空中机械臂的高精度控制提供了可行方案。

Abstract: An aerial manipulator, comprising a multirotor base and a robotic arm, is subject to significant dynamic coupling between these two components. Therefore, achieving precise and robust motion control is a challenging yet important objective. Here, we propose a novel prescribed performance motion control framework based on variable-gain extended state observers (ESOs), referred to as PreGME. The method includes variable-gain ESOs for real-time estimation of dynamic coupling and a prescribed performance flight control that incorporates error trajectory constraints. Compared with existing methods, the proposed approach exhibits the following two characteristics. First, the adopted variable-gain ESOs can accurately estimate rapidly varying dynamic coupling. This enables the proposed method to handle manipulation tasks that require aggressive motion of the robotic arm. Second, by prescribing the performance, a preset error trajectory is generated to guide the system evolution along this trajectory. This strategy allows the proposed method to ensure the tracking error remains within the prescribed performance envelope, thereby achieving high-precision control. Experiments on a real platform, including aerial staff twirling, aerial mixology, and aerial cart-pulling experiments, are conducted to validate the effectiveness of the proposed method.
  Experimental results demonstrate that even under the dynamic coupling caused by rapid robotic arm motion (end-effector velocity: 1.02 m/s, acceleration: 5.10 m/s$^2$), the proposed method achieves high tracking performance.

</details>


### [313] [Embodied Robot Manipulation in the Era of Foundation Models: Planning and Learning Perspectives](https://arxiv.org/abs/2512.22983)
*Shuanghao Bai,Wenxuan Song,Jiayi Chen,Yuheng Ji,Zhide Zhong,Jin Yang,Han Zhao,Wanqi Zhou,Zhe Li,Pengxiang Ding,Cheng Chi,Chang Xu,Xiaolong Zheng,Donglin Wang,Haoang Li,Shanghang Zhang,Badong Chen*

Main category: cs.RO

TL;DR: 本文综述了机器人操作领域的算法进展，将基于学习的方法统一为高层规划与底层控制的抽象框架，并提出了分类体系和研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉、语言和多模态学习的进步推动了机器人基础模型的发展，但机器人操作仍是核心挑战，需要系统性的算法梳理和框架构建。

Method: 提出高层规划（涵盖语言、代码、运动、功能性和3D表征推理）与底层控制（基于输入建模、潜在表征学习和策略学习的分类）的统一抽象框架。

Result: 构建了机器人操作算法的系统性分类体系，明确了设计空间，并识别出现有方法的组织逻辑。

Conclusion: 机器人操作需进一步解决可扩展性、数据效率、多模态物理交互和安全性等挑战，本文框架为未来基础模型研究提供了清晰方向。

Abstract: Recent advances in vision, language, and multimodal learning have substantially accelerated progress in robotic foundation models, with robot manipulation remaining a central and challenging problem. This survey examines robot manipulation from an algorithmic perspective and organizes recent learning-based approaches within a unified abstraction of high-level planning and low-level control. At the high level, we extend the classical notion of task planning to include reasoning over language, code, motion, affordances, and 3D representations, emphasizing their role in structured and long-horizon decision making. At the low level, we propose a training-paradigm-oriented taxonomy for learning-based control, organizing existing methods along input modeling, latent representation learning, and policy learning. Finally, we identify open challenges and prospective research directions related to scalability, data efficiency, multimodal physical interaction, and safety. Together, these analyses aim to clarify the design space of modern foundation models for robotic manipulation.

</details>


### [314] [Embodied Learning of Reward for Musculoskeletal Control with Vision Language Models](https://arxiv.org/abs/2512.23077)
*Saraswati Soedarmadji,Yunyue Wei,Chen Zhang,Yisong Yue,Yanan Sui*

Main category: cs.RO

TL;DR: 提出MoVLR框架，利用视觉语言模型将高层运动目标转化为控制策略，解决高维肌肉骨骼系统运动控制中奖励函数设计的难题。


<details>
  <summary>Details</summary>
Motivation: 高维肌肉骨骼系统的运动控制中，有效奖励函数的设计是核心挑战。人类能用语言描述运动目标，但底层控制策略是隐式的，难以直接从高层目标或自然语言描述设计奖励。

Method: 引入MoVLR框架，利用视觉语言模型连接目标描述与运动控制。通过控制优化与VLM反馈的迭代交互探索奖励空间，将语言和视觉评估转化为结构化指导，实现奖励函数的发现与优化。

Result: 框架能发现并优化高维肌肉骨骼系统的运动（如行走）和操作任务的奖励函数，使控制策略与物理协调行为对齐。

Conclusion: 视觉语言模型能将抽象运动描述有效落地到生理运动控制的隐式原则中，为基于语言和视觉的具身学习提供新途径。

Abstract: Discovering effective reward functions remains a fundamental challenge in motor control of high-dimensional musculoskeletal systems. While humans can describe movement goals explicitly such as "walking forward with an upright posture," the underlying control strategies that realize these goals are largely implicit, making it difficult to directly design rewards from high-level goals and natural language descriptions. We introduce Motion from Vision-Language Representation (MoVLR), a framework that leverages vision-language models (VLMs) to bridge the gap between goal specification and movement control. Rather than relying on handcrafted rewards, MoVLR iteratively explores the reward space through iterative interaction between control optimization and VLM feedback, aligning control policies with physically coordinated behaviors. Our approach transforms language and vision-based assessments into structured guidance for embodied learning, enabling the discovery and refinement of reward functions for high-dimensional musculoskeletal locomotion and manipulation. These results suggest that VLMs can effectively ground abstract motion descriptions in the implicit principles governing physiological motor control.

</details>


### [315] [APOLLO Blender: A Robotics Library for Visualization and Animation in Blender](https://arxiv.org/abs/2512.23103)
*Peter Messina,Daniel Rakita*

Main category: cs.RO

TL;DR: 开发了一个轻量级软件库，简化Blender在机器人可视化中的应用，使研究人员无需深入掌握Blender即可快速生成高质量的可视化内容。


<details>
  <summary>Details</summary>
Motivation: Blender作为强大的3D图形平台，学习曲线陡峭且缺乏机器人专用集成，导致研究人员难以高效创建论文所需的可视化结果。

Method: 设计了一个提供简单脚本接口的软件库，支持从URDF等标准描述导入机器人模型、基于Python的机器人状态关键帧脚本工具，以及生成基本3D形状的便捷功能。

Result: 通过概念验证示例展示了该库能帮助研究人员快速创建出版级的图像、动画和示意图，无需深厚的Blender专业知识。

Conclusion: 该库填补了机器人研究与Blender之间的工具鸿沟，未来存在进一步扩展的潜力与改进空间。

Abstract: High-quality visualizations are an essential part of robotics research, enabling clear communication of results through figures, animations, and demonstration videos. While Blender is a powerful and freely available 3D graphics platform, its steep learning curve and lack of robotics-focused integrations make it difficult and time-consuming for researchers to use effectively. In this work, we introduce a lightweight software library that bridges this gap by providing simple scripting interfaces for common robotics visualization tasks. The library offers three primary capabilities: (1) importing robots and environments directly from standardized descriptions such as URDF; (2) Python-based scripting tools for keyframing robot states and visual attributes; and (3) convenient generation of primitive 3D shapes for schematic figures and animations. Together, these features allow robotics researchers to rapidly create publication-ready images, animations, and explanatory schematics without needing extensive Blender expertise. We demonstrate the library through a series of proof-of-concept examples and conclude with a discussion of current limitations and opportunities for future extensions.

</details>


### [316] [Beyond URDF: The Universal Robot Description Directory for Shared, Extensible, and Standardized Robot Models](https://arxiv.org/abs/2512.23135)
*Roshan Klein-Seetharaman,Daniel Rakita*

Main category: cs.RO

TL;DR: 提出通用机器人描述目录（URDD），一种模块化表示方法，将机器人信息组织为结构化JSON/YAML模块，以减少下游应用中的冗余计算并促进标准化。


<details>
  <summary>Details</summary>
Motivation: 现有机器人描述文件（如URDF、SDF）仅编码基础运动学、动力学和几何信息，导致下游应用（仿真、规划、控制）需重复推导更丰富的数据，造成计算冗余、实现碎片化和标准化不足。

Method: 开发开源工具包，自动从URDF生成URDD；提供Rust实现支持Bevy可视化，以及基于JavaScript/Three.js的Web查看器。

Result: 在多机器人平台实验中，URDD能高效生成，封装比标准描述文件更丰富的信息，并可直接支持核心机器人子程序构建。

Conclusion: URDD为减少冗余和建立跨框架共享标准提供了统一、可扩展的资源，但仍有局限性和应用影响需进一步探讨。

Abstract: Robots are typically described in software by specification files (e.g., URDF, SDF, MJCF, USD) that encode only basic kinematic, dynamic, and geometric information. As a result, downstream applications such as simulation, planning, and control must repeatedly re-derive richer data, leading to redundant computations, fragmented implementations, and limited standardization. In this work, we introduce the Universal Robot Description Directory (URDD), a modular representation that organizes derived robot information into structured, easy-to-parse JSON and YAML modules. Our open-source toolkit automatically generates URDDs from URDFs, with a Rust implementation supporting Bevy-based visualization. Additionally, we provide a JavaScript/Three.js viewer for web-based inspection of URDDs. Experiments on multiple robot platforms show that URDDs can be generated efficiently, encapsulate substantially richer information than standard specification files, and directly enable the construction of core robotics subroutines. URDD provides a unified, extensible resource for reducing redundancy and establishing shared standards across robotics frameworks. We conclude with a discussion on the limitations and implications of our work.

</details>


### [317] [A New Software Tool for Generating and Visualizing Robot Self-Collision Matrices](https://arxiv.org/abs/2512.23140)
*Roshan Klein-Seetharama,Daniel Rakita*

Main category: cs.RO

TL;DR: 提出了一种交互式工具，用于生成和可视化机器人自碰撞矩阵，支持多种几何形状表示和动态检查，提高了自碰撞和自接近查询的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有工具（如MoveIt Setup Assistant）在生成自碰撞矩阵时存在静态可视化、缺乏接近度支持、单一几何假设和繁琐工作流等限制，影响了机器人应用的灵活性和复用性。

Method: 开发了一个基于Rust和Bevy游戏引擎的交互式工具，支持多形状表示、动态检查、过滤和细化形状对，输出JSON和YAML格式以便集成。

Result: 在多个机器人平台上验证表明，使用多样化形状类型生成的矩阵能实现更快、更准确的自碰撞和自接近查询。

Conclusion: 该工具克服了现有方法的局限性，通过交互式矩阵生成和可视化，提升了机器人自碰撞检测的效率和实用性。

Abstract: In robotics, it is common to check whether a given robot state results in self-intersection (i.e., a self-collision query) or to assess its distance from such an intersection (i.e., a self-proximity query). These checks are typically performed between pairs of shapes attached to different robot links. However, many of these shape pairs can be excluded in advance, as their configurations are known to always or never result in contact. This information is typically encoded in a self-collision matrix, where each entry (i, j) indicates whether a check should be performed between shape i and shape j. While the MoveIt Setup Assistant is widely used to generate such matrices, current tools are limited by static visualization, lack of proximity support, rigid single-geometry assumptions, and tedious refinement workflows, hindering flexibility and reuse in downstream robotics applications. In this work, we introduce an interactive tool that overcomes these limitations by generating and visualizing self-collision matrices across multiple shape representations, enabling dynamic inspection, filtering, and refinement of shape pairs. Outputs are provided in both JSON and YAML for easy integration. The system is implemented in Rust and uses the Bevy game engine to deliver high-quality visualizations. We demonstrate its effectiveness on multiple robot platforms, showing that matrices generated using diverse shape types yield faster and more accurate self-collision and self-proximity queries.

</details>


### [318] [Pole-centric Descriptors for Robust Robot Localization: Evaluation under Pole-at-Distance (PaD) Observations using the Small Pole Landmark (SPL) Dataset](https://arxiv.org/abs/2512.23141)
*Wuhao Xie,Kanji Tanaka*

Main category: cs.RO

TL;DR: 本文提出一个专门评估框架SPL数据集，用于系统研究杆状地标描述符的鲁棒性，发现对比学习在远距离观测下优于监督学习。


<details>
  <summary>Details</summary>
Motivation: 大规模城市环境中，远距离杆状结构观测导致地标识别可靠性显著下降，现有方法缺乏系统性评估框架。

Method: 构建自动化追踪关联的SPL多视角多距离数据集，建立专门评估框架，对比分析对比学习与监督学习两种范式。

Result: 对比学习能为稀疏几何构建更鲁棒的特征空间，在5-10米距离范围内实现更优的检索性能。

Conclusion: 本研究为评估真实场景中地标独特性提供了实证基础和可扩展方法，证明对比学习对远距离杆状地标识别更具优势。

Abstract: While pole-like structures are widely recognized as stable geometric anchors for long-term robot localization, their identification reliability degrades significantly under Pole-at-Distance (Pad) observations typical of large-scale urban environments. This paper shifts the focus from descriptor design to a systematic investigation of descriptor robustness. Our primary contribution is the establishment of a specialized evaluation framework centered on the Small Pole Landmark (SPL) dataset. This dataset is constructed via an automated tracking-based association pipeline that captures multi-view, multi-distance observations of the same physical landmarks without manual annotation. Using this framework, we present a comparative analysis of Contrastive Learning (CL) and Supervised Learning (SL) paradigms. Our findings reveal that CL induces a more robust feature space for sparse geometry, achieving superior retrieval performance particularly in the 5--10m range. This work provides an empirical foundation and a scalable methodology for evaluating landmark distinctiveness in challenging real-world scenarios.

</details>


### [319] [Towards the Automation in the Space Station: Feasibility Study and Ground Tests of a Multi-Limbed Intra-Vehicular Robot](https://arxiv.org/abs/2512.23153)
*Seiko Piotr Yamaguchi,Kentaro Uno,Yasumaru Fujii,Masazumi Imai,Kazuki Takada,Taku Okawara,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 研究通过仿真和原型测试，验证了多肢体内车载机器人（MLIVR）在国际空间站（ISS）上自主执行物流任务的可行性，旨在减少宇航员的工作负担。


<details>
  <summary>Details</summary>
Motivation: 宇航员在国际空间站上花费大量时间处理准备、收尾及货物运输等物流任务，挤占了关键任务的时间。研究旨在探索移动机械臂自主支持这些操作的可能性，以最小化机组和地面操作员的工作量。

Method: 研究包括仿真和原型测试：通过3D空间运动规划仿真机器人的运输能力，并在2D平台上测试原型机以模拟微重力环境下的实际运动执行。

Result: 结果表明，机器人能够在最少人工干预下执行任务，证明了自主操作物流任务的可行性。

Conclusion: 该研究为国际空间站上使用自主移动机械臂提高操作效率提供了有前景的解决方案，展示了其在减少宇航员工作负担方面的潜力。

Abstract: This paper presents a feasibility study, including simulations and prototype tests, on the autonomous operation of a multi-limbed intra-vehicular robot (mobile manipulator), shortly MLIVR, designed to assist astronauts with logistical tasks on the International Space Station (ISS). Astronauts spend significant time on tasks such as preparation, close-out, and the collection and transportation of goods, reducing the time available for critical mission activities. Our study explores the potential for a mobile manipulator to support these operations, emphasizing the need for autonomous functionality to minimize crew and ground operator effort while enabling real-time task execution. We focused on the robot's transportation capabilities, simulating its motion planning in 3D space. The actual motion execution was tested with a prototype on a 2D table to mimic a microgravity environment. The results demonstrate the feasibility of performing these tasks with minimal human intervention, offering a promising solution to enhance operational efficiency on the ISS.

</details>


### [320] [A Sequential Hermaphrodite Coupling Mechanism for Lattice-based Modular Robots](https://arxiv.org/abs/2512.23154)
*Keigo Torii,Kentaro Uno,Shreya Santra,Kazuya Yoshida*

Main category: cs.RO

TL;DR: 提出一种新型形状匹配机械耦合机制，满足单侧连接/分离、平面表面及异质模块耦合需求，适用于极端环境下的模块化机器人系统。


<details>
  <summary>Details</summary>
Motivation: 为在太空等极端环境中实现大规模建造，模块化机器人系统需要满足单侧操作、平面表面兼容及异质模块耦合的复杂设计要求。

Method: 设计一种形状匹配机械耦合机制，通过控制机制在男/女状态间的顺序切换，实现单侧连接与分离，并支持从任意侧强制切换状态。

Result: 该机制成功满足所有设计需求，包括单侧操作、平面表面及被动接口耦合，适用于模块化机器人和机械臂工具更换器。

Conclusion: 提出的耦合机制解决了异质模块耦合的复杂设计问题，为极端环境下的模块化机器人应用提供了可行解决方案。

Abstract: Lattice-based modular robot systems are envisioned for large-scale construction in extreme environments, such as space. Coupling mechanisms for heterogeneous structural modules should meet all of the following requirements: single-sided coupling and decoupling, flat surfaces when uncoupled, and coupling to passive coupling interfaces as well as coupling behavior between coupling mechanisms. The design requirements for such a coupling mechanism are complex. We propose a novel shape-matching mechanical coupling mechanism that satisfies these design requirements. This mechanism enables controlled, sequential transitions between male and female states. When uncoupled, all mechanisms are in the female state. To enable single-sided coupling, one side of the mechanisms switches to the male state during the coupling process. Single-sided decoupling is possible not only from the male side but also from the female side by forcibly switching the opposite mechanism's male state to the female state. This coupling mechanism can be applied to various modular robot systems and robot arm tool changers.

</details>


### [321] [SurgWorld: Learning Surgical Robot Policies from Videos via World Modeling](https://arxiv.org/abs/2512.23162)
*Yufan He,Pengfei Guo,Mengya Xu,Zhaoshuo Li,Andriy Myronenko,Dillan Imans,Bingjie Liu,Dongren Yang,Mingxue Gu,Yongnan Ji,Yueming Jin,Ren Zhao,Baiyong Shen,Daguang Xu*

Main category: cs.RO

TL;DR: 提出利用SurgWorld世界模型生成合成手术视频，并通过逆动力学模型推断伪运动学数据，缓解手术机器人数据稀缺问题，显著提升视觉语言动作模型在真实手术机器人上的性能。


<details>
  <summary>Details</summary>
Motivation: 手术机器人领域面临视觉观察与准确机器人运动学配对数据稀缺的挑战，而大量未标注手术视频无法直接用于模仿学习或VLA训练，阻碍了全自主手术机器人的发展。

Method: 1. 构建SATA数据集（含手术机器人详细动作描述）；2. 基于先进物理AI世界模型和SATA开发SurgWorld，生成多样化、可泛化的逼真手术视频；3. 首次使用逆动力学模型从合成视频推断伪运动学，生成合成配对视频-动作数据；4. 用增强数据训练手术VLA策略。

Result: 使用增强数据训练的手术VLA策略在真实手术机器人平台上显著优于仅用真实演示数据训练的模型。

Conclusion: 该方法通过利用未标注手术视频和生成式世界建模，为自主手术技能获取提供了可扩展路径，为实现通用且数据高效的手术机器人策略开辟了新途径。

Abstract: Data scarcity remains a fundamental barrier to achieving fully autonomous surgical robots. While large scale vision language action (VLA) models have shown impressive generalization in household and industrial manipulation by leveraging paired video action data from diverse domains, surgical robotics suffers from the paucity of datasets that include both visual observations and accurate robot kinematics. In contrast, vast corpora of surgical videos exist, but they lack corresponding action labels, preventing direct application of imitation learning or VLA training. In this work, we aim to alleviate this problem by learning policy models from SurgWorld, a world model designed for surgical physical AI. We curated the Surgical Action Text Alignment (SATA) dataset with detailed action description specifically for surgical robots. Then we built SurgeWorld based on the most advanced physical AI world model and SATA. It's able to generate diverse, generalizable and realistic surgery videos. We are also the first to use an inverse dynamics model to infer pseudokinematics from synthetic surgical videos, producing synthetic paired video action data. We demonstrate that a surgical VLA policy trained with these augmented data significantly outperforms models trained only on real demonstrations on a real surgical robot platform. Our approach offers a scalable path toward autonomous surgical skill acquisition by leveraging the abundance of unlabeled surgical video and generative world modeling, thus opening the door to generalizable and data efficient surgical robot policies.

</details>


### [322] [A Human-Oriented Cooperative Driving Approach: Integrating Driving Intention, State, and Conflict](https://arxiv.org/abs/2512.23220)
*Qin Wang,Shanmin Pang,Jianwu Fang,Shengye Dong,Fuhao Liu,Jianru Xue,Chen Lv*

Main category: cs.RO

TL;DR: 提出了一种以人为导向的协同驾驶方法，通过意图感知轨迹规划和基于强化学习的控制权分配，减少人机冲突，提升驾驶性能。


<details>
  <summary>Details</summary>
Motivation: 人车协同驾驶是实现完全自动驾驶的重要桥梁，但现有方法在人机交互自然性和有效性方面存在不足，需要更注重驾驶员意图和状态以减少冲突。

Method: 1. 战术层：设计意图感知轨迹规划方法，以意图一致性成本为核心指标评估轨迹与驾驶员意图的匹配度；2. 操作层：基于强化学习开发控制权分配策略，通过奖励函数优化策略以实现驾驶员状态与权限分配的协调。

Result: 仿真和人在环实验表明，该方法在轨迹规划中能有效对齐驾驶员意图，实现合理的控制权分配，相比其他协同驾驶方法显著提升了驾驶性能并减少了人机冲突。

Conclusion: 所提出的以人为导向的协同驾驶方法通过分层设计实现了更自然有效的人车交互，为逐步建立驾驶员对自动驾驶技术的信任和接受度提供了可行路径。

Abstract: Human-vehicle cooperative driving serves as a vital bridge to fully autonomous driving by improving driving flexibility and gradually building driver trust and acceptance of autonomous technology. To establish more natural and effective human-vehicle interaction, we propose a Human-Oriented Cooperative Driving (HOCD) approach that primarily minimizes human-machine conflict by prioritizing driver intention and state. In implementation, we take both tactical and operational levels into account to ensure seamless human-vehicle cooperation. At the tactical level, we design an intention-aware trajectory planning method, using intention consistency cost as the core metric to evaluate the trajectory and align it with driver intention. At the operational level, we develop a control authority allocation strategy based on reinforcement learning, optimizing the policy through a designed reward function to achieve consistency between driver state and authority allocation. The results of simulation and human-in-the-loop experiments demonstrate that our proposed approach not only aligns with driver intention in trajectory planning but also ensures a reasonable authority allocation. Compared to other cooperative driving approaches, the proposed HOCD approach significantly enhances driving performance and mitigates human-machine conflict.The code is available at https://github.com/i-Qin/HOCD.

</details>


### [323] [Beyond Coverage Path Planning: Can UAV Swarms Perfect Scattered Regions Inspections?](https://arxiv.org/abs/2512.23257)
*Socratis Gkelios,Savvas D. Apostolidis,Pavlos Ch. Kapoutsis,Elias B. Kosmatopoulos,Athanasios Ch. Kapoutsis*

Main category: cs.RO

TL;DR: 本文提出了一种名为mUDAI的多无人机分散区域快速巡检方法，通过优化图像采集位置和飞行轨迹，在保证数据质量的同时提高巡检效率，适用于安全基础设施评估、农业巡检等场景。


<details>
  <summary>Details</summary>
Motivation: 无人机巡检虽比传统方法更安全高效，但受电池续航限制，且现有覆盖路径规划方法在巡检多个分散区域时效率较低，因此需要开发更优化的飞行路径和数据采集技术。

Method: 提出多无人机分散区域巡检方法，采用两阶段优化：计算最佳图像采集位置和最优无人机轨迹，平衡数据分辨率与操作时间，减少冗余数据采集和资源消耗。通过仿真和实际部署验证，并提供开源Python实现。

Result: mUDAI方法能显著提高操作效率，同时保持高质量数据采集，在实际操作中表现出有效性。开源代码、实验数据及在线平台已公开，便于读者交互使用。

Conclusion: mUDAI方法解决了分散区域快速巡检问题，通过优化路径和采集策略，提升了多无人机巡检的效率和实用性，适用于多种现实应用场景。

Abstract: Unmanned Aerial Vehicles (UAVs) have revolutionized inspection tasks by offering a safer, more efficient, and flexible alternative to traditional methods. However, battery limitations often constrain their effectiveness, necessitating the development of optimized flight paths and data collection techniques. While existing approaches like coverage path planning (CPP) ensure comprehensive data collection, they can be inefficient, especially when inspecting multiple non connected Regions of Interest (ROIs). This paper introduces the Fast Inspection of Scattered Regions (FISR) problem and proposes a novel solution, the multi UAV Disjoint Areas Inspection (mUDAI) method. The introduced approach implements a two fold optimization procedure, for calculating the best image capturing positions and the most efficient UAV trajectories, balancing data resolution and operational time, minimizing redundant data collection and resource consumption. The mUDAI method is designed to enable rapid, efficient inspections of scattered ROIs, making it ideal for applications such as security infrastructure assessments, agricultural inspections, and emergency site evaluations. A combination of simulated evaluations and real world deployments is used to validate and quantify the method's ability to improve operational efficiency while preserving high quality data capture, demonstrating its effectiveness in real world operations. An open source Python implementation of the mUDAI method can be found on GitHub (https://github.com/soc12/mUDAI) and the collected and processed data from the real world experiments are all hosted on Zenodo (https://zenodo.org/records/13866483). Finally, this online platform (https://sites.google.com/view/mudai-platform/) allows interested readers to interact with the mUDAI method and generate their own multi UAV FISR missions.

</details>


### [324] [Robust Deep Learning Control with Guaranteed Performance for Safe and Reliable Robotization in Heavy-Duty Machinery](https://arxiv.org/abs/2512.23505)
*Mehdi Heydari Shahna*

Main category: cs.RO

TL;DR: 本文提出了一种适用于重型移动机械（HDMM）的通用控制框架，旨在简化其电气化改造过程，并在保证安全的前提下部分集成人工智能，以支持从柴油液压系统向清洁电气系统的转型以及向更高自主性的过渡。


<details>
  <summary>Details</summary>
Motivation: 重型移动机械面临两大转型：一是为应对气候目标，需从柴油液压驱动转向清洁电气系统；二是需从人工监督向更高自主性发展。然而，完全电气化面临技术和经济挑战，而高级人工智能在重型机械中的应用又受限于严格的安全要求。

Method: 开发了一个控制框架，包含：（1）采用通用模块化方法简化电气化重型机械的控制设计，该方法与能源类型无关且支持未来修改；（2）定义分层控制策略，在保证安全定义性能和稳定性的前提下部分集成人工智能。研究通过三个案例研究（涵盖不同执行器和条件）进行验证。

Result: 该框架在涵盖重型移动机器人和机械臂的三个案例研究中得到验证，相关成果已发表在五篇同行评审论文和一篇未发表手稿中，推动了非线性控制和机器人学领域的发展。

Conclusion: 所提出的控制框架能够支持重型移动机械的电气化和自主化转型，通过通用模块化设计简化控制，并通过分层策略在保证安全的前提下整合人工智能，为行业过渡提供了技术基础。

Abstract: Today's heavy-duty mobile machines (HDMMs) face two transitions: from diesel-hydraulic actuation to clean electric systems driven by climate goals, and from human supervision toward greater autonomy. Diesel-hydraulic systems have long dominated, so full electrification, via direct replacement or redesign, raises major technical and economic challenges. Although advanced artificial intelligence (AI) could enable higher autonomy, adoption in HDMMs is limited by strict safety requirements, and these machines still rely heavily on human supervision.
  This dissertation develops a control framework that (1) simplifies control design for electrified HDMMs through a generic modular approach that is energy-source independent and supports future modifications, and (2) defines hierarchical control policies that partially integrate AI while guaranteeing safety-defined performance and stability.
  Five research questions align with three lines of investigation: a generic robust control strategy for multi-body HDMMs with strong stability across actuation types and energy sources; control solutions that keep strict performance under uncertainty and faults while balancing robustness and responsiveness; and methods to interpret and trust black-box learning strategies so they can be integrated stably and verified against international safety standards.
  The framework is validated in three case studies spanning different actuators and conditions, covering heavy-duty mobile robots and robotic manipulators. Results appear in five peer-reviewed publications and one unpublished manuscript, advancing nonlinear control and robotics and supporting both transitions.

</details>


### [325] [Unsupervised Learning for Detection of Rare Driving Scenarios](https://arxiv.org/abs/2512.23585)
*Dat Le,Thomas Manhardt,Moritz Venator,Johannes Betz*

Main category: cs.RO

TL;DR: 本研究提出了一种基于深度隔离森林的无监督学习框架，用于从自然驾驶数据中检测罕见且危险的驾驶场景，通过神经网络特征表示与隔离森林相结合来识别非线性复杂异常。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中罕见危险场景的检测对确保系统安全性和可靠性至关重要，但现有方法难以全面捕捉真实驾驶场景中的复杂异常模式。

Method: 采用深度隔离森林算法，结合神经网络特征提取与隔离森林异常检测；对感知模块数据进行滑动窗口统计特征预处理；使用t-SNE进行降维和可视化以增强可解释性。

Result: 该方法能有效识别罕见危险驾驶场景，为自动驾驶异常检测提供了可扩展的解决方案，但评估依赖于代理真实标签和手动定义的特征组合。

Conclusion: 所提框架在自动驾驶异常检测方面具有潜力，但受限于代理真实标签和特征定义的局限性，未能完全覆盖真实驾驶场景的复杂上下文依赖关系。

Abstract: The detection of rare and hazardous driving scenarios is a critical challenge for ensuring the safety and reliability of autonomous systems. This research explores an unsupervised learning framework for detecting rare and extreme driving scenarios using naturalistic driving data (NDD). We leverage the recently proposed Deep Isolation Forest (DIF), an anomaly detection algorithm that combines neural network-based feature representations with Isolation Forests (IFs), to identify non-linear and complex anomalies. Data from perception modules, capturing vehicle dynamics and environmental conditions, is preprocessed into structured statistical features extracted from sliding windows. The framework incorporates t-distributed stochastic neighbor embedding (t-SNE) for dimensionality reduction and visualization, enabling better interpretability of detected anomalies. Evaluation is conducted using a proxy ground truth, combining quantitative metrics with qualitative video frame inspection. Our results demonstrate that the proposed approach effectively identifies rare and hazardous driving scenarios, providing a scalable solution for anomaly detection in autonomous driving systems. Given the study's methodology, it was unavoidable to depend on proxy ground truth and manually defined feature combinations, which do not encompass the full range of real-world driving anomalies or their nuanced contextual dependencies.

</details>


### [326] [Explainable Neural Inverse Kinematics for Obstacle-Aware Robotic Manipulation: A Comparative Analysis of IKNet Variants](https://arxiv.org/abs/2512.23312)
*Sheng-Kai Chen,Yi-Ling Tsai,Chun-Chih Chang,Yan-Chen Chen,Po-Chiang Lin*

Main category: cs.RO

TL;DR: 本研究提出了一种结合Shapley值归因与物理障碍物避障评估的可解释性工作流，用于分析深度学习逆运动学模型的透明性与安全性，并在ROBOTIS OpenManipulator-X机械臂上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽能实时执行复杂轨迹，但其不透明性不符合负责任AI法规对透明度与安全性的要求，需要开发可解释的方法来确保学习型逆运动学的可信度。

Method: 基于原始IKNet训练两种轻量变体（带残差连接的改进IKNet和位置-姿态解耦的聚焦IKNet），使用SHAP进行全局与局部重要性排序，通过InterpretML可视化部分依赖模式，并在模拟器中嵌入网络进行随机单/多障碍物场景的碰撞检测与轨迹评估。

Result: 热图定性分析表明，在姿态维度上重要性分布更均衡的网络架构能在保持位置精度的同时获得更大的安全裕度；可解释性技术能揭示隐藏故障模式并指导架构优化。

Conclusion: 结合可解释AI技术与物理安全评估的方法为学习型逆运动学提供了符合负责任AI标准的可信部署路径，能引导架构改进并支持障碍物感知的部署策略。

Abstract: Deep neural networks have accelerated inverse-kinematics (IK) inference to the point where low cost manipulators can execute complex trajectories in real time, yet the opaque nature of these models contradicts the transparency and safety requirements emerging in responsible AI regulation. This study proposes an explainability centered workflow that integrates Shapley-value attribution with physics-based obstacle avoidance evaluation for the ROBOTIS OpenManipulator-X. Building upon the original IKNet, two lightweight variants-Improved IKNet with residual connections and Focused IKNet with position-orientation decoupling are trained on a large, synthetically generated pose-joint dataset. SHAP is employed to derive both global and local importance rankings, while the InterpretML toolkit visualizes partial-dependence patterns that expose non-linear couplings between Cartesian poses and joint angles. To bridge algorithmic insight and robotic safety, each network is embedded in a simulator that subjects the arm to randomized single and multi-obstacle scenes; forward kinematics, capsule-based collision checks, and trajectory metrics quantify the relationship between attribution balance and physical clearance. Qualitative heat maps reveal that architectures distributing importance more evenly across pose dimensions tend to maintain wider safety margins without compromising positional accuracy. The combined analysis demonstrates that explainable AI(XAI) techniques can illuminate hidden failure modes, guide architectural refinements, and inform obstacle aware deployment strategies for learning based IK. The proposed methodology thus contributes a concrete path toward trustworthy, data-driven manipulation that aligns with emerging responsible-AI standards.

</details>


### [327] [PCR-ORB: Enhanced ORB-SLAM3 with Point Cloud Refinement Using Deep Learning-Based Dynamic Object Filtering](https://arxiv.org/abs/2512.23318)
*Sheng-Kai Chen,Jie-Yu Chao,Jr-Yu Chang,Po-Lien Wu,Po-Chiang Lin*

Main category: cs.RO

TL;DR: 提出PCR-ORB，一种增强的ORB-SLAM3框架，通过深度学习点云精炼和语义分割减少动态物体干扰，在KITTI数据集上部分序列性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 动态环境中的移动物体严重影响vSLAM系统的跟踪精度和地图一致性，需要有效方法减少动态物体干扰。

Method: 集成YOLOv8进行语义分割，结合CUDA加速实现实时处理；采用多阶段过滤策略，包括地面估计、天空区域移除、边缘过滤和时间一致性验证。

Result: 在KITTI数据集序列00-09上评估，序列04的ATE RMSE提升25.9%，ATE中位数提升30.4%，但不同序列表现不一，效果依赖场景。

Conclusion: PCR-ORB在特定场景下能有效提升动态环境中的SLAM性能，揭示了动态物体过滤的挑战和复杂环境中鲁棒导航的改进方向。

Abstract: Visual Simultaneous Localization and Mapping (vSLAM) systems encounter substantial challenges in dynamic environments where moving objects compromise tracking accuracy and map consistency. This paper introduces PCR-ORB (Point Cloud Refinement ORB), an enhanced ORB-SLAM3 framework that integrates deep learning-based point cloud refinement to mitigate dynamic object interference. Our approach employs YOLOv8 for semantic segmentation combined with CUDA-accelerated processing to achieve real-time performance. The system implements a multi-stage filtering strategy encompassing ground plane estimation, sky region removal, edge filtering, and temporal consistency validation. Comprehensive evaluation on the KITTI dataset (sequences 00-09) demonstrates performance characteristics across different environmental conditions and scene types. Notable improvements are observed in specific sequences, with sequence 04 achieving 25.9% improvement in ATE RMSE and 30.4% improvement in ATE median. However, results show mixed performance across sequences, indicating scenario-dependent effectiveness. The implementation provides insights into dynamic object filtering challenges and opportunities for robust navigation in complex environments.

</details>


### [328] [Optimal Scalability-Aware Allocation of Swarm Robots: From Linear to Retrograde Performance via Marginal Gains](https://arxiv.org/abs/2512.23431)
*Simay Atasoy Bingöl,Tobias Töpfer,Sven Kosub,Heiko Hamann,Andreagiovanni Reina*

Main category: cs.RO

TL;DR: 提出了一种基于边际性能增益的计算高效算法，用于在具有凹可扩展性函数的任务间最优分配有限代理资源，以最大化集体性能，并在机器人群体决策任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在集体系统中，代理是有限资源，需要通过分配来最大化集体性能。但通过暴力方法计算多个代理到众多任务的最优分配可能不可行，尤其是当每个任务的性能随代理数量增加以不同方式缩放时（例如困难任务需要更多代理，但性能可能非线性饱和）。

Method: 提出了一种基于边际性能增益的计算高效算法，适用于具有凹可扩展性函数（包括线性、饱和和倒退缩放）的任务。通过模拟机器人群体在集体决策任务中的分配进行测试，其中任务难度通过环境特征的空间几何排列（斑块性）变化，决策性能按饱和曲线（无干扰设置）或倒退曲线（机器人间物理干扰限制移动）缩放。

Result: 使用简单的机器人模拟表明，该算法能够有效分配机器人到不同任务，有助于优化集体性能。

Conclusion: 该算法为未来真实世界多机器人系统的部署提供了有效的资源分配方法，能够处理不同任务难度和性能缩放特性，实现集体性能最大化。

Abstract: In collective systems, the available agents are a limited resource that must be allocated among tasks to maximize collective performance. Computing the optimal allocation of several agents to numerous tasks through a brute-force approach can be infeasible, especially when each task's performance scales differently with the increase of agents. For example, difficult tasks may require more agents to achieve similar performances compared to simpler tasks, but performance may saturate nonlinearly as the number of allocated agents increases. We propose a computationally efficient algorithm, based on marginal performance gains, for optimally allocating agents to tasks with concave scalability functions, including linear, saturating, and retrograde scaling, to achieve maximum collective performance. We test the algorithm by allocating a simulated robot swarm among collective decision-making tasks, where embodied agents sample their environment and exchange information to reach a consensus on spatially distributed environmental features. We vary task difficulties by different geometrical arrangements of environmental features in space (patchiness). In this scenario, decision performance in each task scales either as a saturating curve (following the Condorcet's Jury Theorem in an interference-free setup) or as a retrograde curve (when physical interference among robots restricts their movement). Using simple robot simulations, we show that our algorithm can be useful in allocating robots among tasks. Our approach aims to advance the deployment of future real-world multi-robot systems.

</details>


### [329] [Theory of Mind for Explainable Human-Robot Interaction](https://arxiv.org/abs/2512.23482)
*Marie Bauer,Julia Gachot,Matthias Kerzel,Cornelius Weber,Stefan Wermter*

Main category: cs.RO

TL;DR: 本文提出将人机交互中的心理理论视为可解释人工智能的一种形式，通过VXAI框架评估，以解决当前心理理论应用中解释与机器人内部推理脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 当前人机交互中的心理理论应用存在解释与机器人实际内部推理不一致的评估缺口，且现有可解释人工智能研究缺乏以用户为中心的视角。

Method: 将心理理论整合到可解释人工智能框架中，采用VXAI框架及其七个期望标准进行评估，实现从系统中心到用户中心的视角转变。

Result: 提出心理理论作为可解释人工智能的新形式，通过整合框架能更好地满足用户信息需求，提升机器人行为的可解释性和可预测性。

Conclusion: 心理理论与可解释人工智能的融合能推动人机交互向用户中心范式转变，使机器人解释更贴合人类认知需求，增强交互效果。

Abstract: Within the context of human-robot interaction (HRI), Theory of Mind (ToM) is intended to serve as a user-friendly backend to the interface of robotic systems, enabling robots to infer and respond to human mental states. When integrated into robots, ToM allows them to adapt their internal models to users' behaviors, enhancing the interpretability and predictability of their actions. Similarly, Explainable Artificial Intelligence (XAI) aims to make AI systems transparent and interpretable, allowing humans to understand and interact with them effectively. Since ToM in HRI serves related purposes, we propose to consider ToM as a form of XAI and evaluate it through the eValuation XAI (VXAI) framework and its seven desiderata. This paper identifies a critical gap in the application of ToM within HRI, as existing methods rarely assess the extent to which explanations correspond to the robot's actual internal reasoning. To address this limitation, we propose to integrate ToM within XAI frameworks. By embedding ToM principles inside XAI, we argue for a shift in perspective, as current XAI research focuses predominantly on the AI system itself and often lacks user-centered explanations. Incorporating ToM would enable a change in focus, prioritizing the user's informational needs and perspective.

</details>


### [330] [Act2Goal: From World Model To General Goal-conditioned Policy](https://arxiv.org/abs/2512.23541)
*Pengfei Zhou,Liliang Chen,Shengcong Chen,Di Chen,Wenzhi Zhao,Rongjun Jin,Guanghui Ren,Jianlan Luo*

Main category: cs.RO

TL;DR: 提出Act2Goal方法，通过结合视觉世界模型和多尺度时间控制，实现基于视觉目标的机器人长时程操作策略，显著提升零样本泛化能力和在线自适应能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉目标的任务指定方法虽然简洁明确，但在长时程操作中常因缺乏对任务进度的显式建模而表现不佳，需要更结构化的指导机制。

Method: Act2Goal整合了目标条件视觉世界模型（生成中间视觉状态序列）和多尺度时间哈希（MSTH）技术，将轨迹分解为密集近端帧（精细闭环控制）和稀疏远端帧（全局任务一致性），并通过跨注意力机制耦合运动控制。

Result: 在零样本泛化测试中，Act2Goal对新物体、空间布局和环境表现出强适应性；真实机器人实验显示，在分布外任务上成功率从30%提升至90%，且通过无奖励在线自适应（ hindsight goal relabeling + LoRA微调）可在几分钟内自主改进。

Conclusion: 结合多尺度时间控制的目标条件世界模型能为鲁棒的长时程操作提供必要的结构化指导，显著提升任务成功率和自适应效率。

Abstract: Specifying robotic manipulation tasks in a manner that is both expressive and precise remains a central challenge. While visual goals provide a compact and unambiguous task specification, existing goal-conditioned policies often struggle with long-horizon manipulation due to their reliance on single-step action prediction without explicit modeling of task progress. We propose Act2Goal, a general goal-conditioned manipulation policy that integrates a goal-conditioned visual world model with multi-scale temporal control. Given a current observation and a target visual goal, the world model generates a plausible sequence of intermediate visual states that captures long-horizon structure. To translate this visual plan into robust execution, we introduce Multi-Scale Temporal Hashing (MSTH), which decomposes the imagined trajectory into dense proximal frames for fine-grained closed-loop control and sparse distal frames that anchor global task consistency. The policy couples these representations with motor control through end-to-end cross-attention, enabling coherent long-horizon behavior while remaining reactive to local disturbances. Act2Goal achieves strong zero-shot generalization to novel objects, spatial layouts, and environments. We further enable reward-free online adaptation through hindsight goal relabeling with LoRA-based finetuning, allowing rapid autonomous improvement without external supervision. Real-robot experiments demonstrate that Act2Goal improves success rates from 30% to 90% on challenging out-of-distribution tasks within minutes of autonomous interaction, validating that goal-conditioned world models with multi-scale temporal control provide structured guidance necessary for robust long-horizon manipulation. Project page: https://act2goal.github.io/

</details>


### [331] [Soft Robotic Technological Probe for Speculative Fashion Futures](https://arxiv.org/abs/2512.23570)
*Amy Ingold,Loong Yi Lee,Richard Suphapol Diteesawat,Ajmal Roshan,Yael Zekaria,Edith-Clare Hall,Enrico Werner,Nahian Rahman,Elaine Czech,Jonathan Rossiter*

Main category: cs.RO

TL;DR: 本文介绍了Sumbrella——一种作为推测性时尚探针的软机器人服装，通过焦点小组探讨了人们对软机器人可穿戴设备的理解、互动和未来关系的想象，并提出了设计软机器人服装的关键考虑因素和建议。


<details>
  <summary>Details</summary>
Motivation: 新兴的可穿戴机器人需要不仅关注功能，还要关注社会意义的设计方法。本文旨在通过推测性设计，探索软机器人服装如何影响公共场合中被认为可接受或理想的行为定义。

Method: 首先详细设计了Sumbrella软机器人服装，包括序列化的折纸启发的双稳态单元、织物气动驱动室、电缆驱动的形状变形机制、计算机视觉组件以及集成可穿戴系统。然后通过一个由12名创意技术人员组成的焦点小组，使用Sumbrella作为技术探针，探讨人们对软机器人可穿戴设备的解释、互动和未来关系的想象。

Result: Sumbrella使参与者能够围绕推测性未来和表达潜力进行丰富讨论，同时也引发了关于剥削、监视以及将生物传感技术嵌入公共生活的个人风险和社会伦理的担忧。研究为HRI领域提供了设计软机器人服装的关键考虑因素和建议，包括动觉交流的潜力、此类技术对社会动态的影响以及伦理指南的重要性。

Conclusion: 推测性设计允许HRI研究人员不仅考虑功能性，还考虑可穿戴机器人如何影响公共场合中被认为可接受或理想的行为定义。本文通过Sumbrella这一案例，为软机器人服装的设计提供了重要的见解和反思。

Abstract: Emerging wearable robotics demand design approaches that address not only function, but also social meaning. In response, we present Sumbrella, a soft robotic garment developed as a speculative fashion probe. We first detail the design and fabrication of the Sumbrella, including sequenced origami-inspired bistable units, fabric pneumatic actuation chambers, cable driven shape morphing mechanisms, computer vision components, and an integrated wearable system comprising a hat and bolero jacket housing power and control electronics. Through a focus group with twelve creative technologists, we then used Sumbrella as a technological probe to explore how people interpreted, interacted, and imagined future relationships with soft robotic wearables. While Sumbrella allowed our participants to engage in rich discussion around speculative futures and expressive potential, it also surfaced concerns about exploitation, surveillance, and the personal risks and societal ethics of embedding biosensing technologies in public life. We contribute to the Human-Robot Interaction (HRI) field key considerations and recommendations for designing soft robotic garments, including the potential for kinesic communication, the impact of such technologies on social dynamics, and the importance of ethical guidelines. Finally, we provide a reflection on our application of speculative design; proposing that it allows HRI researchers to not only consider functionality, but also how wearable robots influence definitions of what is considered acceptable or desirable in public settings.

</details>


### [332] [A Kalman Filter-Based Disturbance Observer for Steer-by-Wire Systems](https://arxiv.org/abs/2512.23593)
*Nikolai Beving,Jonas Marxen,Steffen Mueller,Johannes Betz*

Main category: cs.RO

TL;DR: 本文提出了一种基于卡尔曼滤波的扰动观测器，用于估计线控转向系统中的高频驾驶员扭矩扰动，仅使用电机状态测量即可实现，无需昂贵的扭矩传感器。


<details>
  <summary>Details</summary>
Motivation: 线控转向系统虽具有轻量化、设计灵活等优势，但易受驾驶员高频阻抗扰动影响，而现有方法要么依赖昂贵扭矩传感器，要么缺乏足够时间分辨率来捕捉快速扰动。

Method: 采用PT1滞后近似模型将驾驶员被动扭矩建模为扩展状态，集成到线性和非线性系统模型中，设计并实现了基于卡尔曼滤波的扰动观测器，对比评估了不同卡尔曼滤波变体。

Result: 所提扰动观测器能以仅14ms的微小延迟准确重构驾驶员引起的扰动，非线性扩展卡尔曼滤波在处理摩擦非线性时优于线性版本，特别是在静摩擦到动摩擦的过渡阶段。

Conclusion: 该方法能有效估计高频驾驶员扭矩扰动，但研究基于仿真验证，未来需在实际驾驶条件下测试观测器的鲁棒性。

Abstract: Steer-by-Wire systems replace mechanical linkages, which provide benefits like weight reduction, design flexibility, and compatibility with autonomous driving. However, they are susceptible to high-frequency disturbances from unintentional driver torque, known as driver impedance, which can degrade steering performance. Existing approaches either rely on direct torque sensors, which are costly and impractical, or lack the temporal resolution to capture rapid, high-frequency driver-induced disturbances. We address this limitation by designing a Kalman filter-based disturbance observer that estimates high-frequency driver torque using only motor state measurements. We model the drivers passive torque as an extended state using a PT1-lag approximation and integrate it into both linear and nonlinear Steer-by-Wire system models. In this paper, we present the design, implementation and simulation of this disturbance observer with an evaluation of different Kalman filter variants. Our findings indicate that the proposed disturbance observer accurately reconstructs driver-induced disturbances with only minimal delay 14ms. We show that a nonlinear extended Kalman Filter outperforms its linear counterpart in handling frictional nonlinearities, improving estimation during transitions from static to dynamic friction. Given the study's methodology, it was unavoidable to rely on simulation-based validation rather than real-world experimentation. Further studies are needed to investigate the robustness of the observers under real-world driving conditions.

</details>


### [333] [Interactive Robot Programming for Surface Finishing via Task-Centric Mixed Reality Interfaces](https://arxiv.org/abs/2512.23616)
*Christoph Willibald,Lugh Martensen,Thomas Eiband,Dongheui Lee*

Main category: cs.RO

TL;DR: 提出一种面向非专家的机器人编程方法，通过交互式任务流程和表面分割算法，降低协作机器人在小批量表面处理任务中的部署门槛。


<details>
  <summary>Details</summary>
Motivation: 机器人部署过程复杂、依赖专业知识，阻碍了协作机器人在高产品变异性和小批量场景（如手工艺制造）中的应用，尤其是表面处理任务。

Method: 开发交互式任务导向编程流程，结合人机交互的表面分割算法，允许用户通过视觉反馈迭代优化工件区域分割，并自动生成机器人轨迹。通过两项用户研究评估多种交互设计。

Result: 优化后的交互界面显著降低用户工作负荷、提升可用性，即使缺乏经验的用户也能有效完成任务编程。

Conclusion: 该方法使非专家能够直观编程协作机器人进行表面处理，有望推动机器人在小规模制造中的普及应用。

Abstract: Lengthy setup processes that require robotics expertise remain a major barrier to deploying robots for tasks involving high product variability and small batch sizes. As a result, collaborative robots, despite their advanced sensing and control capabilities, are rarely used for surface finishing in small-scale craft and manufacturing settings. To address this gap, we propose a novel robot programming approach that enables non-experts to intuitively program robots through interactive, task-focused workflows. For that, we developed a new surface segmentation algorithm that incorporates human input to identify and refine workpiece regions for processing. Throughout the programming process, users receive continuous visual feedback on the robot's learned model, enabling them to iteratively refine the segmentation result. Based on the segmented surface model, a robot trajectory is generated to cover the desired processing area. We evaluated multiple interaction designs across two comprehensive user studies to derive an optimal interface that significantly reduces user workload, improves usability and enables effective task programming even for users with limited practical experience.

</details>


### [334] [RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion](https://arxiv.org/abs/2512.23649)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Tao Huang,Zhenguo Sun,Yibo Peng,Pengwei Wang,Zhongyuan Wang,Fangzhou Liu,Chang Xu,Shanghang Zhang*

Main category: cs.RO

TL;DR: RoboMirror是一个无需重定向的视频到运动框架，通过视觉语言模型从视频中提取运动意图，直接驱动人形机器人进行物理可行的运动控制。


<details>
  <summary>Details</summary>
Motivation: 现有的人形机器人运动系统依赖运动捕捉轨迹或文本指令，缺乏真正的视觉理解能力。文本到运动方法存在语义稀疏问题，视频方法仅机械模仿姿态而缺乏理解。

Method: 提出RoboMirror框架：1) 使用视觉语言模型从第一/第三人称视频中提取视觉运动意图；2) 基于扩散的策略模型直接根据运动意图生成物理可行的运动控制，无需显式姿态重建或重定向。

Result: 实验表明：1) 支持通过第一人称视频实现远程呈现；2) 将第三人称控制延迟降低80%；3) 任务成功率比基线方法提高3.7%。

Conclusion: 通过将人形机器人控制重构为视频理解问题，RoboMirror成功弥合了视觉理解与动作执行之间的鸿沟，实现了'先理解后模仿'的机器人控制范式。

Abstract: Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying "understand before you imitate". Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.

</details>


### [335] [The N-5 Scaling Law: Topological Dimensionality Reduction in the Optimal Design of Fully-actuated Multirotors](https://arxiv.org/abs/2512.23619)
*Antonio Franchi*

Main category: cs.RO

TL;DR: 本文研究了全驱动全向N旋翼飞行器的几何设计优化问题，通过拓扑分析揭示了最优配置空间的结构特性，发现了与底盘对称性相关的相变现象和N-5缩放定律。


<details>
  <summary>Details</summary>
Motivation: 传统方法将N旋翼飞行器设计视为参数优化问题，寻找固定架构族中的单一最优解。本研究旨在探索优化景观本身的内在拓扑结构，以理解全局最优解的本质特性。

Method: 将设计问题表述在射影直线乘积流形RP^2^N上，固定转子位置为多面体底盘顶点，变化其作用线方向。通过最小化坐标不变的Log-Volume各向同性度量，分析优化景观的拓扑结构。

Result: 发现全局最优解的拓扑结构严格由底盘对称性决定：对于不规则顶点排列，解为离散孤立点；当底盘几何接近规则时，解空间发生相变，坍缩到N维环面，最终简化为由仿射相位锁定驱动的连续1维曲线。提出了N-5缩放定律：对于所有检查的正则平面多边形和柏拉图立体（N≤10），最优配置空间由K=N-5个不相连的1维拓扑分支组成。这些锁定模式对应一系列可容许的星形多边形{N/q}，可精确预测任意N的最优相位。

Conclusion: 最优配置的拓扑结构揭示了设计冗余性，使得飞行器能够沿这些分支连续重构，同时保持最优的各向同性控制能力。这一发现为全向飞行器的动态重构提供了理论基础。

Abstract: The geometric design of fully-actuated and omnidirectional N-rotor aerial vehicles is conventionally formulated as a parametric optimization problem, seeking a single optimal set of N orientations within a fixed architectural family. This work departs from that paradigm to investigate the intrinsic topological structure of the optimization landscape itself. We formulate the design problem on the product manifold of Projective Lines \RP^2^N, fixing the rotor positions to the vertices of polyhedral chassis while varying their lines of action. By minimizing a coordinate-invariant Log-Volume isotropy metric, we reveal that the topology of the global optima is governed strictly by the symmetry of the chassis. For generic (irregular) vertex arrangements, the solutions appear as a discrete set of isolated points. However, as the chassis geometry approaches regularity, the solution space undergoes a critical phase transition, collapsing onto an N-dimensional Torus of the lines tangent at the vertexes to the circumscribing sphere of the chassis, and subsequently reducing to continuous 1-dimensional curves driven by Affine Phase Locking. We synthesize these observations into the N-5 Scaling Law: an empirical relationship holding for all examined regular planar polygons and Platonic solids (N <= 10), where the space of optimal configurations consists of K=N-5 disconnected 1D topological branches. We demonstrate that these locking patterns correspond to a sequence of admissible Star Polygons {N/q}, allowing for the exact prediction of optimal phases for arbitrary N. Crucially, this topology reveals a design redundancy that enables optimality-preserving morphing: the vehicle can continuously reconfigure along these branches while preserving optimal isotropic control authority.

</details>


### [336] [Do You Have Freestyle? Expressive Humanoid Locomotion via Audio Control](https://arxiv.org/abs/2512.23650)
*Zhe Li,Cheng Chi,Yangyang Wei,Boan Zhu,Tao Huang,Zhenguo Sun,Yibo Peng,Pengwei Wang,Zhongyuan Wang,Fangzhou Liu,Chang Xu,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出RoboPerform框架，首次实现从音频直接生成机器人舞蹈和伴随语音手势，无需显式运动重建，降低延迟并提高保真度。


<details>
  <summary>Details</summary>
Motivation: 当前人形机器人缺乏即兴表达能力，依赖预定义动作或稀疏指令；现有音频驱动方法存在级联误差、高延迟和声学-驱动映射割裂的问题。

Method: 采用'运动=内容+风格'核心原则，将音频作为隐式风格信号；集成ResMoE教师策略适应多样化运动模式，结合扩散学生策略注入音频风格；无需运动重定向。

Result: 实验验证显示RoboPerform在物理合理性和音频对齐方面表现优异，成功使机器人能够响应音频成为反应灵敏的表演者。

Conclusion: RoboPerform通过统一音频到运动框架，实现了低延迟、高保真的机器人表演生成，为机器人即兴表达提供了新解决方案。

Abstract: Humans intuitively move to sound, but current humanoid robots lack expressive improvisational capabilities, confined to predefined motions or sparse commands. Generating motion from audio and then retargeting it to robots relies on explicit motion reconstruction, leading to cascaded errors, high latency, and disjointed acoustic-actuation mapping. We propose RoboPerform, the first unified audio-to-locomotion framework that can directly generate music-driven dance and speech-driven co-speech gestures from audio. Guided by the core principle of "motion = content + style", the framework treats audio as implicit style signals and eliminates the need for explicit motion reconstruction. RoboPerform integrates a ResMoE teacher policy for adapting to diverse motion patterns and a diffusion-based student policy for audio style injection. This retargeting-free design ensures low latency and high fidelity. Experimental validation shows that RoboPerform achieves promising results in physical plausibility and audio alignment, successfully transforming robots into responsive performers capable of reacting to audio.

</details>


### [337] [The Bulldozer Technique: Efficient Elimination of Local Minima Traps for APF-Based Robot Navigation](https://arxiv.org/abs/2512.23672)
*Mohammed Baziyad,Manal Al Shohna,Tamer Rabie*

Main category: cs.RO

TL;DR: 提出名为Bulldozer的新型路径规划技术，通过后填充机制和斜坡增强解决人工势场法的局部极小值陷阱问题，在保持APF优点的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 人工势场法因其简单、实时和低计算需求而广泛应用于机器人路径规划，但传统方法存在局部极小值陷阱问题，导致机器人被困无法到达目标。

Method: 1. 提出Bulldozer技术，引入后填充机制系统识别并消除局部极小值区域（类似推土机填平坑洞）；2. 加入斜坡增强机制帮助机器人从局部极小值区域逃脱；3. 使用物理移动机器人在不同复杂度地图上进行实验验证；4. 与标准APF、自适应APF、A*、PRM、RRT等算法进行对比分析；5. 采用运动学跟踪控制器评估路径平滑度和可跟踪性。

Result: 1. Bulldozer技术有效解决了局部极小值问题；2. 在保持APF原有优势的同时，实现了更快的执行速度；3. 路径质量具有竞争力；4. 规划的路径适合实际执行，平滑度和可跟踪性良好。

Conclusion: Bulldozer技术成功解决了人工势场法的局部极小值陷阱问题，在保持APF原有优点的同时，在速度和路径质量方面表现优异，适用于实际机器人导航应用。

Abstract: Path planning is a fundamental component in autonomous mobile robotics, enabling a robot to navigate from its current location to a desired goal while avoiding obstacles. Among the various techniques, Artificial Potential Field (APF) methods have gained popularity due to their simplicity, real-time responsiveness, and low computational requirements. However, a major limitation of conventional APF approaches is the local minima trap problem, where the robot becomes stuck in a position with no clear direction toward the goal. This paper proposes a novel path planning technique, termed the Bulldozer, which addresses the local minima issue while preserving the inherent advantages of APF. The Bulldozer technique introduces a backfilling mechanism that systematically identifies and eliminates local minima regions by increasing their potential values, analogous to a bulldozer filling potholes in a road. Additionally, a ramp-based enhancement is incorporated to assist the robot in escaping trap areas when starting within a local minimum. The proposed technique is experimentally validated using a physical mobile robot across various maps with increasing complexity. Comparative analyses are conducted against standard APF, adaptive APF, and well-established planning algorithms such as A*, PRM, and RRT. Results demonstrate that the Bulldozer technique effectively resolves the local minima problem while achieving superior execution speed and competitive path quality. Furthermore, a kinematic tracking controller is employed to assess the smoothness and traceability of the planned paths, confirming their suitability for real-world execution.

</details>


### [338] [Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation](https://arxiv.org/abs/2512.23703)
*Huajie Tan,Sixiang Chen,Yijie Xu,Zixiao Wang,Yuheng Ji,Cheng Chi,Yaoxu Lyu,Zhongxia Zhao,Xiansheng Chen,Peterson Co,Shaoxuan Xie,Guocai Yao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: 提出Dopamine-Reward方法，通过多视角输入学习通用、步态感知的过程奖励模型，并基于此构建Dopamine-RL策略学习框架，解决机器人强化学习中奖励函数设计的难题。


<details>
  <summary>Details</summary>
Motivation: 将强化学习应用于真实机器人任务的主要障碍是设计有效的奖励函数。现有基于学习的过程奖励模型存在两个根本局限：缺乏步态感知能力、依赖单视角感知导致细粒度操作评估不可靠；奖励塑造过程理论不严谨，易引发误导策略优化的语义陷阱。

Method: 1. 提出Dopamine-Reward方法，核心为通用奖励模型（GRM），采用步态奖励离散化实现结构化理解，通过多视角奖励融合克服感知局限；2. 基于Dopamine-Reward构建Dopamine-RL框架，采用理论严谨的策略不变奖励塑造方法，使智能体在不改变最优策略的前提下利用密集奖励进行高效自我改进。

Result: 1. GRM在奖励评估中达到最先进准确率；2. GRM通过单条专家轨迹单次适应新任务后，Dopamine-RL仅需150次在线交互（约1小时真实机器人交互）即可将策略成功率从接近零提升至95%，且保持跨任务的强泛化能力。

Conclusion: Dopamine-Reward与Dopamine-RL的组合有效解决了过程奖励模型在感知和理论方面的局限，显著提升了机器人强化学习的策略学习效率与泛化性能，为真实世界机器人应用提供了可行解决方案。

Abstract: The primary obstacle for applying reinforcement learning (RL) to real-world robotics is the design of effective reward functions. While recently learning-based Process Reward Models (PRMs) are a promising direction, they are often hindered by two fundamental limitations: their reward models lack step-aware understanding and rely on single-view perception, leading to unreliable assessments of fine-grained manipulation progress; and their reward shaping procedures are theoretically unsound, often inducing a semantic trap that misguides policy optimization. To address these, we introduce Dopamine-Reward, a novel reward modeling method for learning a general-purpose, step-aware process reward model from multi-view inputs. At its core is our General Reward Model (GRM), trained on a vast 3,400+ hour dataset, which leverages Step-wise Reward Discretization for structural understanding and Multi-Perspective Reward Fusion to overcome perceptual limitations. Building upon Dopamine-Reward, we propose Dopamine-RL, a robust policy learning framework that employs a theoretically-sound Policy-Invariant Reward Shaping method, which enables the agent to leverage dense rewards for efficient self-improvement without altering the optimal policy, thereby fundamentally avoiding the semantic trap. Extensive experiments across diverse simulated and real-world tasks validate our approach. GRM achieves state-of-the-art accuracy in reward assessment, and Dopamine-RL built on GRM significantly improves policy learning efficiency. For instance, after GRM is adapted to a new task in a one-shot manner from a single expert trajectory, the resulting reward model enables Dopamine-RL to improve the policy from near-zero to 95% success with only 150 online rollouts (approximately 1 hour of real robot interaction), while retaining strong generalization across tasks. Project website: https://robo-dopamine.github.io

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [339] [Validation methodology on real data of reversible Kalman Filter for state estimation with Manifold](https://arxiv.org/abs/2512.22126)
*Svyatoslav Covanov,Cedric Pradalier*

Main category: eess.SY

TL;DR: 本文扩展了先前在流形上进行状态估计的卡尔曼滤波算法研究，重点分析可逆卡尔曼滤波在真实数据应用中的事件检测机制，并提出评估其优于经典方法时机的判别指标。


<details>
  <summary>Details</summary>
Motivation: 先前提出的可逆卡尔曼滤波在合成数据上表现良好，能实现任意精度且不依赖小速度假设，但在真实数据中受测量噪声影响，需依赖启发式方法（事件检测切换策略）。本研究旨在系统分析该切换机制，明确可逆方法优于经典变体的条件。

Method: 1. 深入分析可逆卡尔曼滤波与经典乘法变体之间的事件检测切换步骤；2. 提出一种方法论，用于证明可逆方法何时优于经典变体；3. 设计一种判别指标，用于在实际场景中识别可逆方法更优的情况。

Result: 提出了一个系统框架来评估可逆卡尔曼滤波在真实数据中的应用效果，通过量化分析切换时机和性能比较，为实际场景中的算法选择提供依据。

Conclusion: 通过建立判别指标和方法论，本研究为可逆卡尔曼滤波在真实世界场景中的有效应用提供了理论支持，明确了其相对于经典方法的优势条件，解决了先前依赖启发式策略的局限性。

Abstract: This work extends a previous study that introduced an algorithm for state estimation on manifolds within the framework of the Kalman filter. Its objective is to address the limitations of the earlier approach. The reversible Kalman filter was designed to provide a methodology for evaluating the accuracy of existing Kalman filter variants with arbitrary precision on synthetic data. It has favorable numerical properties on synthetic data, achieving arbitrary precision without relying on the small-velocity assumption and depending only on sensor noise. However, its application to real data encountered difficulties related to measurement noise, which was mitigated using a heuristic. In particular, the heuristic involved an event detection step switching between reversible Kalman filter and classical Kalman variant at chosen moments. In the present work, we propose a study of this detection step and propose a methodology to prove at which moment the reversible Kalman approach improves on classical multiplicative variant. In particular, we propose a metric allowing one to discriminate situations in real-world scenarios where it behaves better than classical approach.

</details>


### [340] [Tree Meets Transformer: A Hybrid Architecture for Scalable Power Allocation in Cell-Free Networks](https://arxiv.org/abs/2512.22639)
*Irched Chafaa,Giacomo Bacci,Luca Sanguinetti*

Main category: eess.SY

TL;DR: 提出一种混合树-Transformer架构，用于大规模无线网络中可扩展的功率分配，通过树结构压缩用户特征实现对数深度和线性复杂度。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在处理动态用户负载和大规模部署时计算成本过高，需要一种能高效适应不同用户规模且无需重新训练的功率分配方案。

Method: 设计混合树-Transformer架构：1）用二叉树将用户特征压缩为全局根表示；2）仅对根表示应用Transformer编码器；3）通过共享解码器生成每个用户的上行/下行功率。

Result: 在无蜂窝大规模MIMO系统的最大最小公平性问题中，该模型实现接近最优的性能，同时相比全注意力基线显著降低推理时间。

Conclusion: 所提架构通过结构创新实现了对数深度和线性总复杂度，能够高效处理大规模可变用户集，为动态无线网络的功率分配提供了可扩展解决方案。

Abstract: Power allocation remains a fundamental challenge in wireless communication networks, particularly under dynamic user loads and large-scale deployments. While Transformerbased models have demonstrated strong performance, their computational cost scales poorly with the number of users. In this work, we propose a novel hybrid Tree-Transformer architecture that achieves scalable per-user power allocation. Our model compresses user features via a binary tree into a global root representation, applies a Transformer encoder solely to this root, and decodes per-user uplink and downlink powers through a shared decoder. This design achieves logarithmic depth and linear total complexity, enabling efficient inference across large and variable user sets without retraining or architectural changes. We evaluate our model on the max-min fairness problem in cellfree massive MIMO systems and demonstrate that it achieves near-optimal performance while significantly reducing inference time compared to full-attention baselines.

</details>


### [341] [On the Stealth of Unbounded Attacks Under Non-Negative-Kernel Feedback](https://arxiv.org/abs/2512.22646)
*Kamil Hassan,Henrik Sandberg*

Main category: eess.SY

TL;DR: 研究了线性时变控制系统中针对反馈传感器的虚假数据注入攻击的隐蔽性，定义了ε-隐蔽性和不可追踪性概念，证明了在特定条件下多项式增长的攻击信号可以保持隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 虚假数据注入攻击可能破坏控制系统但逃避检测，现有研究对线性时变系统中攻击信号隐蔽性的理论边界缺乏深入分析，需要建立严格的数学框架来理解攻击如何在不触发警报的情况下影响系统。

Method: 使用线性Volterra积分方程理论，针对包含q个积分器且反馈控制器具有非负脉冲响应核的线性时变系统，分析多项式攻击信号的隐蔽性条件。

Result: 证明了当攻击信号为a次多项式时：若a≤q，攻击可保持ε-隐蔽性（监控信号偏差有界）；若a<q，攻击具有不可追踪性（偏差随时间渐近消散）。

Conclusion: 攻击信号的隐蔽性不仅取决于幅值，更取决于其时间增长特性与系统积分器阶数的关系，这为设计更鲁棒的检测机制提供了理论依据。

Abstract: The stealth of false data injection attacks (FDIAs) against feedback sensors in linear time-varying (LTV) control systems is investigated. In that regard, the following notions of stealth are pursued: For some finite $ε> 0$, i) an FDIA is deemed $ε$-stealthy if the deviation it produces in the signal that is monitored by the anomaly detector remains $ε$-bounded for all time, and ii) the $ε$-stealthy FDIA is further classified as untraceable if the bounded deviation dissipates over time (asymptotically). For LTV systems that contain a chain of $q \geq 1$ integrators and feedback controllers with non-negative impulse-response kernels, it is proved that polynomial (in time) FDIA signals of degree $a$ - growing unbounded over time - will remain i) $ε$-stealthy, for some finite $ε> 0$, if $a \leq q$, and ii) untraceable, if $a < q$. These results are obtained using the theory of linear Volterra integral equations.

</details>


### [342] [Optimal Regulation of Nonlinear Input-Affine Systems via an Integral Reinforcement Learning-Based State-Dependent Riccati Equation Approach](https://arxiv.org/abs/2512.22668)
*Arya Rashidinejad Meibodi,Mahbod Gholamali Sinaki,Khalil Alipour*

Main category: eess.SY

TL;DR: 提出了一种基于积分强化学习（IRL）的部分无模型方法，用于解决状态依赖Riccati方程（SDRE）问题，该方法无需显式环境模型即可学习非线性系统的最优控制。


<details>
  <summary>Details</summary>
Motivation: 传统SDRE方法需要完整的系统模型，并在每个状态求解代数Riccati方程（ARE），这限制了其在模型未知或部分已知场景中的应用。

Method: 采用积分强化学习（IRL）框架，通过部分无模型的方式在每个系统状态学习最优控制策略，无需显式漂移动力学知识。

Result: 仿真实验表明，经过足够迭代后，IRL方法能达到与传统SDRE方法相近的性能，成功应用于二阶非线性系统控制。

Conclusion: IRL方法可作为传统SDRE的可靠替代方案，为无需显式环境模型的非线性系统最优控制提供了有效解决方案。

Abstract: The State-Dependent Riccati Equation (SDRE) technique generalizes the classical algebraic Riccati formulation to nonlinear systems by designing an input to the system that optimally(suboptimally) regulates system states toward the origin while simultaneously optimizing a quadratic performance index. In the SDRE technique, we solve the State-Dependent Riccati Equation to determine the control for regulating a nonlinear input-affine system. Since an analytic solution to SDRE is not straightforward, one method is to linearize the system at every state, solve the corresponding Algebraic Riccati Equation (ARE), and apply optimal control until the next state of the system. Completing this task with high frequency gives a result like the original SDRE technique. Both approaches require a complete model; therefore, here we propose a method that solves ARE in every state of the system using a partially model-free approach that learns optimal control in every state of the system, without explicit knowledge of the drift dynamics, based on Integral Reinforcement Learning (IRL). To show the effectiveness of our proposed approach, we apply it to the second-order nonlinear system in simulation and compare its performance with the classical SDRE method, which relies on the system's model and solves the ARE at each state. Our simulation results demonstrate that, with sufficient iterations, the IRL-based approach achieves approximately the same performance as the conventional SDRE method, demonstrating its capability as a reliable alternative for nonlinear system control that does not require an explicit environmental model. Index Terms-Algebraic Riccati Equation (ARE), Integral Reinforcement Learning (IRL), Nonlinear Input-Affine Systems, Optimal Regulation, State-Dependent Riccati Equation (SDRE)

</details>


### [343] [From Electrochemical Energy Storage to Next-Generation Intelligent Battery Technologies for Electric Vehicles: A Survey](https://arxiv.org/abs/2512.22680)
*Abderaouf Bahi,Amel Ourici,Chaima Lagraa,Siham Lameche,Soundess Halimi,Inoussa Mouiche,Ylias Sabri,Waseem Haider,Mohamed Trari*

Main category: eess.SY

TL;DR: 本文综述了电化学储能（包括钠离子、金属离子和金属空气电池）的最新进展，涵盖电极工程、电解质和SEI控制，并探讨了机器学习、数字孪生等AI技术在智能电池管理系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 随着电动汽车行业的发展，需要更高效、安全、长寿命的下一代电池技术，同时AI技术的兴起为电池管理提供了新的优化途径。

Method: 采用文献综述方法，系统梳理电化学储能技术的关键进展，并分析AI技术在电池管理中的集成应用。

Result: 总结了电极材料、电解质设计、界面控制等方面的创新，以及AI驱动系统在提升电池性能、安全性和寿命方面的潜力。

Conclusion: 未来电池技术需关注混合化学体系、规模化制造、可持续性和AI优化，本研究为相关领域的研究者和工程师提供了全面的技术参考。

Abstract: This study provides a comprehensive overview of recent advances in electrochemical energy storage, including Na+ -ion, metal-ion, and metal-air batteries, alongside innovations in electrode engineering, electrolytes, and solid-electrolyte interphase control. It also explores the integration of machine learning, digital twins, large language models and predictive analytics to enable intelligent battery management systems, enhancing performance, safety, and operational longevity. Key challenges, research gaps, and future prospects are addressed, highlighting opportunities presented by hybrid chemistry, scalable manufacturing, sustainability, and AI-driven optimization. This survey aims to provide researchers, engineers, and industry profesionnals with a comprehensive understanding of next-generation battery technologies for the evolving electric vehicles sector.

</details>


### [344] [A Time-Barrier Lyapunov Condition for Predefined-Time Stability](https://arxiv.org/abs/2512.22786)
*Özhan Bingöl*

Main category: eess.SY

TL;DR: 本文提出了一种基于时间屏障的预定义时间稳定性概念，通过非自治李雅普诺夫机制强制系统在预定截止时间前收敛，与现有自治方法有本质区别。


<details>
  <summary>Details</summary>
Motivation: 现有预定义时间稳定性方法主要基于自治李雅普诺夫不等式，其预定义时间是通过状态相关衰减的积分界限实现的，仅作为收敛时间的上界而非结构性强制的截止期限。

Method: 引入时间屏障预定义时间稳定性概念，建立基于非自治李雅普诺夫机制的充分条件，通过时间相关屏障的发散性保证系统在预定截止时间前收敛。

Result: 证明了该机制无法由经典自治预定义时间稳定性公式复现，构成了一个独特的稳定性概念，为非线性系统提供了强制收敛截止期限的简洁透明方法。

Conclusion: 所提出的时间屏障方法能够本质地限制剩余可用时间，确保系统在用户指定的截止时间前收敛，为非线性系统的硬收敛截止期限控制提供了新途径。

Abstract: Predefined-time stability enables convergence within a user-specified time independent of initial conditions. Existing results are predominantly based on autonomous Lyapunov inequalities, where the predefined-time is realized through integral bounds on state-dependent decay and therefore acts as an upper bound rather than a structurally enforced deadline. This paper introduces a time-barrier predefined-time stability concept in which convergence is enforced through a nonautonomous Lyapunov mechanism that intrinsically restricts the remaining available time. A sufficient Lyapunov-based condition is established, guaranteeing convergence before the predefined deadline via divergence of a time-dependent barrier. It is further shown that this mechanism cannot be reproduced by classical autonomous predefined-time stability formulations, thereby constituting a distinct stability notion. The proposed approach provides a concise and transparent means of enforcing hard convergence deadlines in nonlinear systems.

</details>


### [345] [Reach-Avoid Differential game with Reachability Analysis for UAVs: A decomposition approach](https://arxiv.org/abs/2512.22793)
*Minh Bui,Simon Monckton,Mo Chen*

Main category: eess.SY

TL;DR: 提出一种三维空间可达-规避游戏的新框架，通过将问题分解为水平和垂直子游戏，结合Hamilton-Jacobi可达性分析和二阶动力学，实现无人机在三维空间中的最优防御策略。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理三维空间中的可达-规避游戏，尤其是涉及非线性动力学和高维状态空间时，缺乏既保证最优性又具有通用性的解决方案。

Method: 将三维问题分解为水平和垂直两个子游戏，分别使用Hamilton-Jacobi可达性分析求解；引入考虑防御者加速度的二阶动力学模型；设计基于HJ的跟踪控制算法来重构原始游戏解。

Result: 数值仿真表明该方法在保持原始问题最优性和保证条件的同时有效；在Gazebo物理仿真中首次成功实现了四旋翼无人机在三维空间中的捕获。

Conclusion: 提出的分解框架成功解决了三维可达-规避游戏的高维挑战，为无人机防御应用提供了理论保证和实用工具。

Abstract: Reach-avoid (RA) games have significant applications in security and defense, particularly for unmanned aerial vehicles (UAVs). These problems are inherently challenging due to the need to consider obstacles, consider the adversarial nature of opponents, ensure optimality, and account for nonlinear dynamics. Hamilton-Jacobi (HJ) reachability analysis has emerged as a powerful tool for tackling these challenges; however, while it has been applied to games involving two spatial dimensions, directly extending this approach to three spatial dimensions is impossible due to high dimensionality. On the other hand, alternative approaches for solving RA games lack the generality to consider games with three spatial dimensions involving agents with non-trivial system dynamics. In this work, we propose a novel framework for dimensionality reduction by decomposing the problem into a horizontal RA sub-game and a vertical RA sub-game. We then solve each sub-game using HJ reachability analysis and consider second-order dynamics that account for the defender's acceleration. To reconstruct the solution to the original RA game from the sub-games, we introduce a HJ-based tracking control algorithm in each sub-game that not only guarantees capture of the attacker but also tracking of the attacker thereafter. We prove the conditions under which the capture guarantees are maintained. The effectiveness of our approach is demonstrated via numerical simulations, showing that the decomposition maintains optimality and guarantees in the original problem. Our methods are also validated in a Gazebo physics simulator, achieving successful capture of quadrotors in three spatial dimensions space for the first time to the best of our knowledge.

</details>


### [346] [Assessment of a Hybrid Energy System for Reliable and Sustainable Power Supply to Boru Meda Hospital in Ethiopia](https://arxiv.org/abs/2512.22859)
*Tegenu Argaw Woldegiyorgis,Hong Xian Li,Fekadu Chekol Admassu,Merkebu Gezahegne,Abdurohman Kebede,Tadese Abera,Haris Ishaq,Eninges Asmare*

Main category: eess.SY

TL;DR: 本研究评估了为埃塞俄比亚Boru Meda医院提供可靠可持续电力的混合能源系统（HES）的技术经济可行性，发现光伏/生物质发电机/电池/变流器配置最具成本效益和可持续性。


<details>
  <summary>Details</summary>
Motivation: 埃塞俄比亚能源短缺机构需要可靠、可持续的电力供应，现有柴油发电机运营成本高且不环保，因此需要探索经济可行的混合可再生能源解决方案。

Method: 使用HOMER pro 3.11.2软件进行设计和评估，对光伏、生物质发电机、风力发电、柴油发电机、电池和变流器的不同组合进行集成优化和比较分析，基于医院实际能耗和可用资源。

Result: 光伏/生物质发电机/电池/变流器配置表现最佳：最低平准化能源成本0.339美元/kWh，净现成本2570万美元，100%可再生能源比例，投资回收期7.26年，可完全避免每月500升柴油消耗。混合系统展示了20%投资回报率和18%内部收益率。

Conclusion: 混合能源系统在成本、可靠性和可持续性之间实现了最优平衡，是埃塞俄比亚能源短缺机构和地区电气化的有前景且可扩展的解决方案，有助于国家可持续能源发展目标的实现。

Abstract: This study aims to evaluate the techno-economic feasibility of hybrid energy systems (HES) including Grid for providing reliable and sustainable power to Boru Meda Hospital, Ethiopia. HOMER pro 3.11.2 was used to design and evaluate a novel, integrated optimization and comparative assessment of diverse HRES, specif ically adjusted to the energy consumptions and available resources of the Hospital. The scenario evaluation showed that interconnecting photovoltaic (PV), biomass generator (BG), wind power (WP), diesel generator (DG), battery, and converter can effectively provide the Hospital's daily energy consumption of 11,214.66 kWh while conforming reliability and reducing emissions. The PV/BG/batt/conv configuration emerged as the most cost-effective and sustainable alternative, attaining the lowest LCOE of \$0.339/kWh, an NPC of \$25.7 million, and a 100% renewable energy fraction with simple pay back of 7.26 yr. As a result, the operational cost associated with the consumption of 500.00 L of diesel per month can be entirely avoided. The DG-integrated hybrids exhibit advanced techno-economic capability with significant worth, strong ROI (20\%) and IRR (18\%), endorsed by fast capital recovery (7.21-8.71 years). Overall, the hybrid system offers an optimal balance of cost, reliability, and sustainability, making it a promising and scalable solution for electrification of energy scare institution and areas in Ethiopia, thereby contributing to national sustainable energy development goals.

</details>


### [347] [A Neural Network-Based Real-time Casing Collar Recognition System for Downhole Instruments](https://arxiv.org/abs/2512.22901)
*Si-Yu Xiao,Xin-Di Zhao,Xiang-Zhan Wang,Tian-Hao Mao,Ying-Kai Liao,Xing-Yu Liao,Yu-Qiao Chen,Jun-Jie Wang,Shuang Liu,Tu-Pei Chen,Yang Liu*

Main category: eess.SY

TL;DR: 提出了一种用于井下套管接箍定位的嵌入式神经网络系统，在资源受限的ARM Cortex-M7微处理器上实现实时识别，显著降低计算复杂度并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统地面CCL监测在油气作业中因信号衰减导致定位不准，需要一种能在井下严苛环境下自主实时处理信号的解决方案。

Method: 设计了轻量级“Collar Recognition Nets”（CRNs），采用时间和深度可分离卷积优化，针对ARM Cortex-M7微处理器进行模型压缩，最小模型计算量仅8,208 MACs。

Result: 最紧凑模型F1分数达0.972，硬件验证平均推理延迟343.2微秒，证明在井下仪器的严格功耗和空间限制下可实现鲁棒自主信号处理。

Conclusion: 嵌入式神经网络系统能有效解决井下定位信号衰减问题，为资源受限环境下的实时信号处理提供了可行方案。

Abstract: Accurate downhole positioning is critical in oil and gas operations but is often compromised by signal degradation in traditional surface-based Casing Collar Locator (CCL) monitoring. To address this, we present an in-situ, real-time collar recognition system using embedded neural network. We introduce lightweight "Collar Recognition Nets" (CRNs) optimized for resource-constrained ARM Cortex-M7 microprocessors. By leveraging temporal and depthwise separable convolutions, our most compact model reduces computational complexity to just 8,208 MACs while maintaining an F1 score of 0.972. Hardware validation confirms an average inference latency of 343.2 μs, demonstrating that robust, autonomous signal processing is feasible within the severe power and space limitations of downhole instrumentation.

</details>


### [348] [Distributed Fusion Estimation with Protecting Exogenous Inputs](https://arxiv.org/abs/2512.22914)
*Liping Guo,Jimin Wang,Yanlong Zhao,Ji-Feng Zhang*

Main category: eess.SY

TL;DR: 提出了一种基于噪声注入的差分隐私分布式融合估计算法，通过协方差交叉法和反馈机制在保护外部输入隐私的同时提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 分布式融合估计中，本地估计直接传输至融合中心可能导致外部输入隐私泄露，需在实现估计的同时保护隐私免受窃听。

Method: 采用噪声注入策略，向传输的本地估计添加独立噪声；通过构造约束最小化问题确定噪声协方差矩阵，确保(ε,δ)-差分隐私；提出松弛方法解决非凸优化问题；基于协方差交叉法设计差分隐私融合估计算法，并引入反馈机制提升精度。

Result: 所提算法有效实现了隐私保护与估计精度的平衡，示例验证了算法有效性及隐私水平与估计精度之间的权衡关系。

Conclusion: 噪声注入与反馈机制相结合的方法能在保证差分隐私的前提下提升分布式融合估计精度，为隐私保护估计提供了可行方案。

Abstract: In the context of distributed fusion estimation, directly transmitting local estimates to the fusion center may cause a privacy leakage concerning exogenous inputs. Thus, it is crucial to protect exogenous inputs against full eavesdropping while achieving distributed fusion estimation. To address this issue, a noise injection strategy is provided by injecting mutually independent noises into the local estimates transmitted to the fusion center. To determine the covariance matrices of the injected noises, a constrained minimization problem is constructed by minimizing the sum of mean square errors of the local estimates while ensuring (ε, δ)-differential privacy. Suffering from the non-convexity of the minimization problem, an approach of relaxation is proposed, which efficiently solves the minimization problem without sacrificing differential privacy level. Then, a differentially private distributed fusion estimation algorithm based on the covariance intersection approach is developed. Further, by introducing a feedback mechanism, the fusion estimation accuracy is enhanced on the premise of the same (ε, δ)-differential privacy. Finally, an illustrative example is provided to demonstrate the effectiveness of the proposed algorithms, and the trade-off between differential privacy level and fusion estimation accuracy.

</details>


### [349] [Weak state synchronization of homogeneous multi-agent systems with adaptive protocols](https://arxiv.org/abs/2512.22922)
*Anton A. Stoorvogel,Ali Saberi,Zhenwei Liu,Tayaba Yeasmin*

Main category: eess.SY

TL;DR: 本文研究了多智能体系统的无标度弱同步问题，提出了一种无需网络知识（包括连通性信息）的自适应协议设计方法。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统同步协议通常依赖网络拓扑信息（如连通性、规模等），这在实际应用中难以获取或可能变化。研究旨在设计完全无需网络先验知识的自适应同步方案。

Method: 为每个智能体设计包含自适应参数的控制协议，该参数能根据网络需求自动调整，无需利用网络拓扑、规模或连通性等任何信息。

Result: 提出了一种完全无标度的弱同步协议，仅通过局部交互和自适应参数调节实现多智能体系统同步，摆脱了对网络全局知识的依赖。

Conclusion: 所提自适应协议能在无需任何网络先验知识的条件下实现多智能体系统弱同步，增强了协议在未知或时变网络环境中的适用性和鲁棒性。

Abstract: In this paper, we study scale-free weak synchronization for multi-agent systems (MAS). In other words, we design a protocol for the agents without using any knowledge about the network. We do not
  even require knowledge about the connectivity of the network. Each protocol contains an adaptive parameter to tune the protocol automatically to the demands of the network.

</details>


### [350] [A Bezier Curve Based Approach to the Convexification of the AC Optimal Power Flow Problem](https://arxiv.org/abs/2512.22968)
*Carlos Arturo Saldarriaga-Cortes,Carlos Adrian Correa-Florez,Maximiliano Bueno-Lopez,Maria Victoria Gasca-Segura*

Main category: eess.SY

TL;DR: 提出一种凸化交流最优潮流（ACOPF）问题的重构方法，通过引入辅助变量、对数变换和贝塞尔曲线近似，实现快速收敛和高精度求解。


<details>
  <summary>Details</summary>
Motivation: ACOPF问题因其非凸、非线性和多模态特性，在电力系统运行与规划中计算复杂。现有方法难以在弱电网（面临无功功率供应和网络鲁棒性挑战）中高效求解，需要一种既精确又计算高效的新模型。

Method: 1. 引入辅助变量隔离非线性项；2. 应用对数变换利用乘积-和性质；3. 使用新型凸化蝶形函数配合贝塞尔曲线近似；4. 保持与原始AC模型在有功/无功调度和电压水平方面的一致性。

Result: 1. 在IEEE 118节点系统等大型测试案例中实现秒级收敛；2. 与精确AC解相比，节点电压幅值平均误差仅0.0008%，相角误差仅0.014度；3. 模型具有数学透明性、结构直观性，易于验证和实施。

Conclusion: 所提凸化模型为弱电网的评估与运行提供了高精度、高可靠性的计算工具，兼具计算效率与工程实用性，适用于电力系统规划和运行人员。

Abstract: The Alternating Current Optimal Power Flow (ACOPF) problem remains one of the most fundamental yet computationally challenging tasks in power systems operation and planning due to its nonconvex, nonlinear, and multimodal nature. This paper proposes a convex reformulation of the AC power flow problem by introducing auxiliary variables to isolate nonlinear terms, applying logarithmic transformations to exploit product-sum properties, and approximating with Bezier curves using a novel convexifying butterfly shaped function. This model is intended for assessing and operating weak power systems that face challenges with reactive power supply and overall network robustness. Its formulation closely mirrors the AC formulation, particularly regarding active and reactive power dispatch and network voltage levels.
  The proposed model achieves convergence on large test systems (e.g., IEEE 118 bus) in seconds and is validated against exact AC solutions. This convex formulation stands out not only for its mathematical transparency and intuitive structure but also for its ease of validation and implementation, making it an accessible and reliable tool for researchers and system operators for energy planning.
  The numerical analysis conducted on the IEEE 118 bus system yielded average percentage errors in the state variables specifically, the magnitudes and angles of nodal voltages of just 0.0008 percentage and 0.014 degree, respectively, when compared with the precise AC formulation. These results underscore the high accuracy and reliability of the proposed methodology.

</details>


### [351] [Global Frequency Reference Tracking as an Oscillation Suppression Mechanism in VSM Primary Control: A Coupled-Oscillator Study](https://arxiv.org/abs/2512.23081)
*Taha Saeed Khan*

Main category: eess.SY

TL;DR: 提出了一种基于全局频率参考的虚拟同步机逆变器控制架构，将同步问题从振荡器互锁转变为参考跟踪问题，显著改善了电网的暂态性能。


<details>
  <summary>Details</summary>
Motivation: 传统电力系统同步依赖物理网络耦合，扰动会导致暂态过程延长和相位偏移。在已知并维持总有功功率平衡的假设下，需要一种能快速协调电网形成型逆变器网络的控制方法。

Method: 采用二阶摇摆网络模型，为虚拟同步机逆变器设计控制架构，使所有单元跟踪广播的全局频率参考。通过嵌入比例积分频率控制器改善暂态行为，并采用冲刷机制确保稳态时附加控制作用消失。

Result: 在三振荡器网络仿真中，与传统开环同步相比，该方法降低了频率超调，消除了欠阻尼振荡，减少了角度应力，同时保持了网络决定的功率分配。

Conclusion: 全局频率参考可作为电网形成型逆变器网络的有效协调机制，通过将同步问题转化为参考跟踪问题，显著提升了系统的暂态响应性能。

Abstract: Synchronization in power systems is traditionally achieved through physical network coupling, whereby inverter-based resources (IBRs) and synchronous machines converge to a common frequency via oscillatory swing dynamics. In conventional operation, secondary control acts on a slow time scale and is typically engaged only after the primary dynamics have largely settled. As a result, in the absence of an explicit global reference, disturbances can induce prolonged transients and large phase excursions. This work considers a setting in which the total active power balance is known and maintained at all times, and proposes a control architecture for virtual synchronous machine (VSM) based inverters in which all units track a broadcast global frequency reference. Under this assumption, synchronization is transformed from a mutual oscillator locking problem into a reference tracking problem. Using a second order swing network model, we show that embedding a simple proportional integral (PI) frequency controller can significantly improves transient behavior. A washout mechanism ensures that the additional control action vanishes in steady state, thereby preserving network determined power sharing. Simulations on a three oscillator network demonstrate reduced frequency overshoot, elimination of underdamped oscillations, and lower angular stress compared to conventional open loop synchronization, highlighting the effectiveness of a global frequency reference as a coordination mechanism for grid-forming inverter networks.

</details>


### [352] [Real-Time Forward Kinematics and Jacobians for Control of an MRI-Guided Magnetically Actuated Robotic Catheter](https://arxiv.org/abs/2512.23085)
*Ran Hao,Yuttana Itsarachaiyot,Yen-Chun Chen,M. Cenk Çavuşoğlu*

Main category: eess.SY

TL;DR: 提出了一种用于MRI驱动机器人导管实时控制的正向运动学和解析雅可比计算方法，通过静态Cosserat杆理论建模，实验验证了方法的准确性和实时性。


<details>
  <summary>Details</summary>
Motivation: MRI驱动的机器人导管在微创手术中具有应用潜力，但需要实时、精确的运动学模型来实现闭环控制，现有方法在计算效率和精度方面存在挑战。

Method: 1. 基于静态Cosserat杆理论建立导管正向运动学模型；2. 推导正向运动学模型的解析雅可比计算方法；3. 使用单线圈导管原型和立体相机跟踪系统进行实验验证。

Result: 实验表明，所提方法能够以实时计算效率实现复杂轨迹的开环控制，轨迹跟踪误差在可接受范围内，为后续结合实时MRI成像的闭环控制奠定了基础。

Conclusion: 该方法为MRI驱动机器人导管提供了高效精确的运动学建模方案，具备实时控制能力，有望推动其在临床手术中的实际应用。

Abstract: This paper presents a forward kinematics and analytical Jacobian computation approach for real-time control of a novel magnetic resonance imaging (MRI)-actuated robotic catheter. The MRI-actuated robotic catheter is modeled as a series of rigid and flexible segments and actuated by magnetic torques generated on a set of current-carrying microcoils embedded on the catheter body by the magnetic field of the MRI scanner. First, a real-time forward kinematic modeling approach of the robotic catheter employing the static Cosserat-rod theory is presented. Second, the analytical calculation approach of the forward kinematic Jacobians of the proposed forward kinematic model is presented. The accuracy, reproducibility, and computational efficiency of the proposed methods are evaluated using a robotic catheter prototype with a single coil set, where catheter tip trajectories collected by a catadioptric stereo camera tracking system are validated using the desired tip trajectories. Experimental results demonstrate that the proposed method can successfully control the catheter in an open loop to perform complex trajectories with real-time computational efficiency, paving the way for accurate closed-loop control with real-time MR-imaging feedback.

</details>


### [353] [Breaking Symmetry-Induced Degeneracy in Multi-Agent Ergodic Coverage via Stochastic Spectral Control](https://arxiv.org/abs/2512.23158)
*Kooktae Lee,Julian Martinez*

Main category: eess.SY

TL;DR: 本文针对多智能体遍历覆盖中的梯度消失问题，提出了一种结合随机扰动和收缩项的改进方法，解决了对称分布下智能体停滞或受限运动的问题。


<details>
  <summary>Details</summary>
Motivation: 经典谱多尺度覆盖方法在多智能体协同覆盖中表现良好，但当智能体初始位置靠近目标分布的对称点时，会出现梯度抵消现象，导致智能体停滞或运动受限，影响覆盖效率。

Method: 首先严格分析了导致方向退化的初始条件和对称性不变流形；然后引入随机扰动结合收缩项，构建了改进的随机SMC方法，并证明了该方法能几乎必然逃离零梯度流形，同时保持轨迹的均方有界性。

Result: 在对称多模态参考分布上的仿真表明，所提出的随机SMC方法有效缓解了瞬态停滞和轴约束运动问题，同时确保所有智能体轨迹保持在定义域内有界。

Conclusion: 通过理论分析和仿真验证，随机扰动与收缩项的结合能够有效解决对称分布下的梯度消失问题，提升了多智能体覆盖系统的鲁棒性和性能。

Abstract: Multi-agent ergodic coverage via Spectral Multiscale Coverage (SMC) provides a principled framework for driving a team of agents so that their collective time-averaged trajectories match a prescribed spatial distribution. While classical SMC has demonstrated empirical success, it can suffer from gradient cancellation, particularly when agents are initialized near symmetry points of the target distribution, leading to undesirable behaviors such as stalling or motion constrained along symmetry axes. In this work, we rigorously characterize the initial conditions and symmetry-induced invariant manifolds that give rise to such directional degeneracy in first-order agent dynamics. To address this, we introduce a stochastic perturbation combined with a contraction term and prove that the resulting dynamics ensure almost-sure escape from zero-gradient manifolds while maintaining mean-square boundedness of agent trajectories. Simulations on symmetric multi-modal reference distributions demonstrate that the proposed stochastic SMC effectively mitigates transient stalling and axis-constrained motion, while ensuring that all agent trajectories remain bounded within the domain.

</details>


### [354] [Multi-objective control strategy of Electro-Mechanical Transmission Based on Driving Pattern Division](https://arxiv.org/abs/2512.23186)
*Yanbo Li,Jinsong Li,Zongjue Liu,Riming Xu*

Main category: eess.SY

TL;DR: 提出基于动态规划的多目标控制策略，优化重型车辆机电传动系统在不同行驶模式下的综合性能，显著提升燃油经济性。


<details>
  <summary>Details</summary>
Motivation: 重型车辆机电传动系统在不同行驶模式下需满足驱动需求与功率平衡，需优化综合性能，特别是燃油经济性。

Method: 采用层次分析法确定不同工况下优化目标的权重，将多目标转化为综合优化目标，基于动态规划理论设计多目标控制策略，并通过仿真与规则策略对比验证。

Result: 仿真结果表明，所提策略显著提升了综合性能，燃油经济性尤其得到大幅改善。

Conclusion: 基于动态规划的多目标控制策略能有效优化重型车辆机电传动系统在不同行驶模式下的性能，具有实际应用价值。

Abstract: Based on the driving requirement and power balance of heavy-duty vehicle equipped with Electro-Mechanical Transmission (EMT), optimization goals under different driving patterns are put forward. The optimization objectives are changed into a comprehensive optimization target based on the method of weighting, which is calculated by using analytic hierarchy process (AHP) under different working conditions. According to theory of Dynamic Programming (DP), a multi-object control strategy of DP under different driving patterns is proposed. This strategy is verified by simulation and contrasted with rule strategy, the results show that comprehensive performance is significantly enhanced, and the fuel economy is highly improved especially.

</details>


### [355] [Learning-based data-enabled economic predictive control with convex optimization for nonlinear systems](https://arxiv.org/abs/2512.23170)
*Mingxue Yan,Xuewen Zhang,Kaixiang Zhang,Zhaojian Li,Xunyuan Yin*

Main category: eess.SY

TL;DR: 提出了一种数据驱动的经济预测控制方法，用于一类非线性系统，通过神经网络构造提升函数将非线性经济成本函数近似为二次型，并在虚拟线性表示中处理硬约束，最终将在线控制问题转化为凸优化问题。


<details>
  <summary>Details</summary>
Motivation: 非线性系统的经济性能优化通常面临非线性成本函数和硬约束处理的挑战，传统方法难以同时高效处理这两方面问题，需要一种既能优化经济性能又能严格满足约束的数据驱动控制方法。

Method: 使用神经网络构造两个提升函数，将输入输出映射到高维空间，使非线性经济成本函数可近似为映射变量的二次函数；通过映射输入输出构建虚拟线性表示来近似原非线性系统；重构系统输出变量并施加硬约束；将在线控制问题转化为凸优化问题求解。

Result: 理论分析证明了该方法对非线性系统的适用性；在两个大规模工业案例（生物水处理过程和溶剂基船载燃烧后碳捕集过程）中验证了方法的有效性和优势。

Conclusion: 所提出的数据驱动经济预测控制方法能够有效处理非线性系统的经济性能优化和硬约束问题，通过神经网络提升和凸优化框架实现了计算可行性和控制性能的平衡，在工业应用中展现出实用价值。

Abstract: In this article, we propose a data-enabled economic predictive control method for a class of nonlinear systems, which aims to optimize the economic operational performance while handling hard constraints on the system outputs. Two lifting functions are constructed via training neural networks, which generate mapped input and mapped output in a higher-dimensional space, where the nonlinear economic cost function can be approximated using a quadratic function of the mapped variables. The data-enabled predictive control framework is extended to address nonlinear dynamics by using the mapped input and the mapped output that belong to a virtual linear representation, which serves as an approximation of the original nonlinear system. Additionally, we reconstruct the system output variables from the mapped output, on which hard output constraints are imposed. The online control problem is formulated as a convex optimization problem, despite the nonlinearity of the system dynamics and the original economic cost function. Theoretical analysis is presented to justify the suitability of the proposed method for nonlinear systems. We evaluate the proposed method through two large-scale industrial case studies: (i) a biological water treatment process, and (ii) a solvent-based shipboard post-combustion carbon capture process. These studies demonstrate its effectiveness and advantages.

</details>


### [356] [A Learning-Driven Stochastic Hybrid System Framework for Detecting Unobservable Contingencies in Power Systems](https://arxiv.org/abs/2512.23205)
*Hamid Varmazyari,Masoud H. Nazari*

Main category: eess.SY

TL;DR: 提出了一种基于学习的随机混合系统框架，用于检测和分类现代电力系统中的突发事件，能够识别传统监测方案无法观测的隐藏事件。


<details>
  <summary>Details</summary>
Motivation: 传统电力系统监测方案无法检测到保护系统故障等隐藏事件，现有方法在检测速度和准确性上存在局限，需要一种能够快速准确识别各类突发事件的智能框架。

Method: 采用学习型随机混合系统框架，通过分析系统输出和行为的偏差，将其分为物理、控制和测量三类突发事件；整合系统动态和观测器驱动的状态估计误差动态；应用机器学习分类器实现快速准确分类。

Result: 在IEEE 5总线和30总线系统上的仿真表明，该方法在检测速度和准确性上相比现有方法有显著提升，能够有效识别传统监测无法发现的隐藏事件。

Conclusion: 所提出的LSHS框架为电力系统突发事件检测提供了有效解决方案，通过结合随机混合系统模型和机器学习，实现了对可观测和不可观测事件的快速准确识别与分类。

Abstract: This paper presents a new learning based Stochastic Hybrid System (LSHS) framework designed for the detection and classification of contingencies in modern power systems. Unlike conventional monitoring schemes, the proposed approach is capable of identifying unobservable events that remain hidden from standard sensing infrastructures, such as undetected protection system malfunctions. The framework operates by analyzing deviations in system outputs and behaviors, which are then categorized into three groups: physical, control, and measurement contingencies based on their impact on the SHS model. The SHS model integrates both system dynamics and observer-driven state estimation error dynamics. Within this architecture, machine learning classifiers are employed to achieve rapid and accurate categorization of contingencies. The effectiveness of the method is demonstrated through simulations on the IEEE 5-bus and 30-bus systems, where results indicate substantial improvements in both detection speed and accuracy compared with existing approaches.

</details>


### [357] [The Dawn of Agentic EDA: A Survey of Autonomous Digital Chip Design](https://arxiv.org/abs/2512.23189)
*Zelin Zang,Yuhang Song,Bingo Wing-Kuen Ling,Aili Wang,Fuji Yang*

Main category: eess.SY

TL;DR: 本文综述了生成式AI与智能体AI在数字电子设计自动化（EDA）中的集成应用，从传统CAD到AI4EDA再到AI原生与智能体设计范式的演进，涵盖芯片设计全流程，并通过案例验证可行性，同时探讨安全影响、当前挑战与未来趋势。


<details>
  <summary>Details</summary>
Motivation: 推动EDA领域从AI辅助工具向完全自主设计工程师转型，定义并系统梳理新兴的智能体EDA领域，应对传统设计流程效率瓶颈与复杂度增长。

Method: 采用文献综述与案例研究方法，系统分析AI范式演进；构建基于多模态基础模型的智能体认知架构；在前端RTL代码生成、智能验证及后端物理设计中应用算法创新与工具编排；通过跨阶段反馈循环实现前后端协同优化。

Result: 验证了从微架构定义到GDSII全流程的实践可行性；展示了智能体利用后端PPA指标自主优化前端逻辑的潜力；揭示了新型安全风险与自动化漏洞修复等双重安全影响；识别了幻觉、数据稀缺、黑盒工具等核心挑战。

Conclusion: 智能体EDA代表从AI辅助工具向自主设计演进的关键方向，需解决可靠性、数据与可解释性挑战；未来趋势将聚焦L4级全自主芯片设计，本文为领域发展提供了系统性路线图。

Abstract: This survey provides a comprehensive overview of the integration of Generative AI and Agentic AI within the field of Digital Electronic Design Automation (EDA). The paper first reviews the paradigmatic evolution from traditional Computer-Aided Design (CAD) to AI-assisted EDA (AI4EDA), and finally to the emerging AI-Native and Agentic design paradigms. We detail the application of these paradigms across the digital chip design flow, including the construction of agentic cognitive architectures based on multimodal foundation models, frontend RTL code generation and intelligent verification, and backend physical design featuring algorithmic innovations and tool orchestration. We validate these methodologies through integrated case studies, demonstrating practical viability from microarchitecture definition to GDSII. Special emphasis is placed on the potential for cross-stage feedback loops where agents utilize backend PPA metrics to autonomously refine frontend logic. Furthermore, this survey delves into the dual-faceted impact on security, covering novel adversarial risks, automated vulnerability repair, and privacy-preserving infrastructure. Finally, the paper critically summarizes current challenges related to hallucinations, data scarcity, and black-box tools, and outlines future trends towards L4 autonomous chip design. Ultimately, this work aims to define the emerging field of Agentic EDA and provide a strategic roadmap for the transition from AI-assisted tools to fully autonomous design engineers.

</details>


### [358] [Revealing design archetypes and flexibility in e-molecule import pathways using Modeling to Generate Alternatives and interpretable machine learning](https://arxiv.org/abs/2512.23284)
*Mahdi Kchaou,Francesco Contino,Diederik Coppitters*

Main category: eess.SY

TL;DR: 本文提出了一种生成多样化近最优绿色电子分子进口路径的方法，以应对成本最优方案在现实中可能不可行的问题，并应用于氢能进口路径分析。


<details>
  <summary>Details</summary>
Motivation: 当前研究多聚焦于成本最优的绿色电子分子进口路径，但现实中的监管、空间和利益相关者约束常使这些方案不可行，因此需要探索更多样化的可行方案。

Method: 采用建模生成替代方案方法，在可接受成本范围内生成多样化近最优方案，并结合可解释机器学习从解空间中提取洞察。

Result: 研究发现近最优空间广阔且灵活：太阳能、风能和储能并非严格必需以保持在成本最优的10%内；风能约束下倾向于太阳能-储能甲醇路径，而储能有限时则倾向于基于风能的氨或甲烷路径。

Conclusion: 通过生成多样化近最优方案并应用可解释机器学习，可以更好地应对未建模的不确定性，为绿色电子分子进口路径规划提供更实用的决策支持。

Abstract: Given the central role of green e-molecule imports in the European energy transition, many studies optimize import pathways and identify a single cost-optimal solution. However, cost optimality is fragile, as real-world implementation depends on regulatory, spatial, and stakeholder constraints that are difficult to represent in optimization models and can render cost-optimal designs infeasible. To address this limitation, we generate a diverse set of near-cost-optimal alternatives within an acceptable cost margin using Modeling to Generate Alternatives, accounting for unmodeled uncertainties. Interpretable machine learning is then applied to extract insights from the resulting solution space. The approach is applied to hydrogen import pathways considering hydrogen, ammonia, methane, and methanol as carriers. Results reveal a broad near-optimal space with great flexibility: solar, wind, and storage are not strictly required to remain within 10% of the cost optimum. Wind constraints favor solar-storage methanol pathways, while limited storage favors wind-based ammonia or methane pathways.

</details>


### [359] [Agentic AI-Enhanced Semantic Communications: Foundations, Architecture, and Applications](https://arxiv.org/abs/2512.23294)
*Haixiao Gao,Mengying Sun,Ruichen Zhang,Yanhan Wang,Xiaodong Xu,Nan Ma,Dusit Niyato,Ping Zhang*

Main category: eess.SY

TL;DR: 本文系统阐述了智能体AI如何赋能语义通信，提出了统一框架和典型应用场景，并通过案例研究验证了其性能优势。


<details>
  <summary>Details</summary>
Motivation: 6G时代语义通信从比特传输转向语义信息交换，引入具备感知、记忆、推理和行动能力的智能体AI是实现智能通信的可行路径。

Method: 1. 按智能体类型（嵌入式、LLM/LVM、强化学习）综述现有研究；2. 提出三层统一框架（应用层、语义层、云边协同层）；3. 设计基于智能体知识库的联合信源信道编码案例AKB-JSCC。

Result: 实验表明AKB-JSCC在不同信道条件下实现了更高的信息重建质量。

Conclusion: 智能体AI赋能的语义通信框架具有应用潜力，未来需向便携、可验证、可控的方向演进。

Abstract: Semantic communications (SemCom), as one of the key technologies for 6G, is shifting networks from bit transmission to semantic information exchange. On this basis, introducing agentic artificial intelligence (AI) with perception, memory, reasoning, and action capabilities provides a practicable path to intelligent communications. This paper provides a systematic exposition of how agentic AI empowers SemCom from the perspectives of research foundations, system architecture, and application scenarios. We first provide a comprehensive review of existing studies by agent types, covering embedded agents, large language model (LLM)/large vision model (LVM) agents, and reinforcement learning (RL) agents. Additionally, we propose a unified agentic AI-enhanced SemCom framework covering the application layer, the semantic layer, and the cloud-edge collaboration layer, forming a closed loop from intent to encoding to transmission to decoding to action to evaluation. We also present several typical scenarios, including multi-vehicle collaborative perception, multi-robot cooperative rescue, and agentic operations for intellicise (intelligent and concise) networks. Furthermore, we introduce an agentic knowledge base (KB)-based joint source-channel coding case study, AKB-JSCC, where the source KB and channel KB are built by LLM/LVM agents and RL agents, respectively. Experimental results show that AKB-JSCC achieves higher information reconstruction quality under different channel conditions. Finally, we discuss future evolution and research directions, providing a reference for portable, verifiable, and controllable research and deployment of agentic SemCom.

</details>


### [360] [Control Co-design of systems with parabolic partial differential equation dynamics](https://arxiv.org/abs/2512.23420)
*Antika Yadav,Prasad Vilas Chanekar*

Main category: eess.SY

TL;DR: 本文研究了具有抛物线偏微分方程动态系统的控制协同设计问题，通过梯度方法求解近似问题并证明其稳定性。


<details>
  <summary>Details</summary>
Motivation: 抛物线PDE系统在工程中广泛应用，但控制与设计的协同优化问题缺乏系统化解决方案，需要建立有效的求解框架。

Method: 将CCD问题转化为带矩阵代数约束的近似问题，采用梯度下降法求解，并通过数值实验验证。

Result: 证明了最优解能稳定PDE系统，数值算例验证了方法的有效性。

Conclusion: 提出的近似CCD问题求解方法可行，为PDE系统的控制设计协同优化提供了新途径。

Abstract: In this paper we study the control co-design (CCD) synthesis problem for a class of systems with parabolic partial differential equation (PDE) dynamics. We formulate CCD problem and finally derive an approximate CCD problem with matrix algebraic constraint. We then solve this approximate problem with gradient-based method and prove that the optimal solution also stabilizes the PDE system. We justify approach through numerical examples.

</details>


### [361] [NashOpt -- A Python Library for Computing Generalized Nash Equilibria](https://arxiv.org/abs/2512.23636)
*Alberto Bemporad*

Main category: eess.SY

TL;DR: NashOpt是一个开源的Python库，用于计算和设计具有共享约束和实值决策变量的非合作博弈中的广义纳什均衡。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发一个工具，以处理非线性广义纳什均衡和线性二次博弈，包括它们的变分版本，支持逆博弈和Stackelberg博弈设计问题。

Method: 方法包括利用所有参与者的联合KKT条件，通过非线性最小二乘公式解决非线性博弈（使用JAX进行自动微分），并将线性二次广义纳什均衡重新表述为混合整数线性规划以高效计算多个均衡。

Result: NashOpt库通过多个示例展示了其能力，包括线性二次调节和模型预测控制的非合作博弈理论控制问题。

Conclusion: 结论是NashOpt提供了一个有效的框架，用于计算和设计广义纳什均衡，支持多种博弈类型和问题，并已开源供使用。

Abstract: NashOpt is an open-source Python library for computing and designing generalized Nash equilibria (GNEs) in noncooperative games with shared constraints and real-valued decision variables. The library exploits the joint Karush-Kuhn-Tucker (KKT) conditions of all players to handle both general nonlinear GNEs and linear-quadratic games, including their variational versions. Nonlinear games are solved via nonlinear least-squares formulations, relying on JAX for automatic differentiation. Linear-quadratic GNEs are reformulated as mixed-integer linear programs, enabling efficient computation of multiple equilibria. The framework also supports inverse-game and Stackelberg game-design problems. The capabilities of NashOpt are demonstrated through several examples, including noncooperative game-theoretic control problems of linear quadratic regulation and model predictive control. The library is available at https://github.com/bemporad/nashopt

</details>


### [362] [A Review of Community-Centric Power Systems Resilience Assessment and Enhancement Strategies](https://arxiv.org/abs/2512.23658)
*Masoud H. Nazaria,Hamid Varmazyari,Antar Kumar Biswas,Umit Cali,Hollis Belnap,Masood Parvania*

Main category: eess.SY

TL;DR: 本文全面回顾了电力系统韧性评估指标、社区韧性关联、提升策略及AI应用，对比欧美监管框架，识别研究空白与未来方向。


<details>
  <summary>Details</summary>
Motivation: 高影响低概率事件对电力系统与社区构成严重威胁，需系统梳理韧性评估方法、跨领域关联及应对策略，以指导未来研究与实践。

Method: 采用文献综述方法，涵盖工程韧性指标（如脆弱性曲线）、数据驱动表征（三角/梯形模型）、系统韧性策略（网络加固、资源调度、重构技术）及监管对比分析。

Result: 总结了韧性度量体系，阐明了电力系统与社区韧性的社会经济、基础设施互依关系；归纳了AI融合与法规维度的重要性；揭示了欧美监管异同及HILP事件应对方法的不足。

Conclusion: 电力系统韧性需多维度协同提升，未来应加强AI集成、跨基础设施协调、法规适配及HILP事件韧性策略的创新研究。

Abstract: This paper presents a comprehensive review of resilience metrics, covering both engineering-based measures, such as fragility-curve modeling, and data-driven approaches, including triangular and trapezoidal representations. Next, the paper examines the interdependencies between power systems resilience and community resilience, addressing socioeconomic and behavioral dimensions, infrastructure interconnections, and the emerging role of resilience hubs. The review then synthesizes state-of-the-art strategies for enhancing power system resilience, including network hardening, resource allocation, optimal scheduling, and reconfiguration techniques. Special emphasis is placed on the integration of Artificial Intelligence (AI) methods and the techno-legal dimensions of resilient power systems and communities. In particular, the paper contrasts the regulatory landscapes of the European Union and the United States, highlighting key similarities and distinctions. By analyzing methodologies for mitigating the impacts of high-impact, low-probability (HILP) events, the review identifies critical research gaps and outlines promising directions for future investigation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [363] [Field strength-dependent performance variability in deep learning-based analysis of magnetic resonance imaging](https://arxiv.org/abs/2512.22176)
*Muhammad Ibtsaam Qadir,Duane Schonlau,Ulrike Dydak,Fiona R. Kolbinger*

Main category: eess.IV

TL;DR: 本研究定量评估了MRI扫描仪磁场强度对深度学习分割算法性能与泛化能力的影响，发现磁场强度显著影响软组织分割性能，在AI性能评估中应作为混杂因素考虑。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学影像分割中广泛应用，但不同磁场强度MRI扫描仪获取的图像存在固有差异，可能影响模型性能与泛化能力，需要系统评估这种影响。

Method: 使用三个公开MRI数据集（乳腺肿瘤、胰腺、颈椎），按磁场强度（1.5T vs 3.0T）分层。为每个分割任务训练三个nnU-Net模型：仅1.5T数据训练、仅3.0T数据训练、混合数据训练。通过UMAP聚类和23个放射组学特征分析场强依赖性差异。

Result: 乳腺肿瘤分割中，m-3.0T模型在两种验证集上均显著优于其他模型（DSC: 0.494 vs 0.411 vs 0.373）。胰腺分割显示类似趋势。颈椎分割中所有模型在同场强验证集上表现优异（DSC>0.92）。放射组学分析显示软组织存在中度场强依赖性聚类，骨性结构差异最小。

Conclusion: 训练数据的磁场强度显著影响深度学习分割模型性能，尤其对软组织和小病灶。在评估MRI的AI性能时，应将磁场强度作为重要混杂因素纳入考虑。

Abstract: This study quantitatively evaluates the impact of MRI scanner magnetic field strength on the performance and generalizability of deep learning-based segmentation algorithms. Three publicly available MRI datasets (breast tumor, pancreas, and cervical spine) were stratified by scanner field strength (1.5T vs. 3.0T). For each segmentation task, three nnU-Net-based models were developed: A model trained on 1.5T data only (m-1.5T), a model trained on 3.0T data only (m-3.0T), and a model trained on pooled 1.5T and 3.0T data (m-combined). Each model was evaluated on both 1.5T and 3.0T validation sets. Field-strength-dependent performance differences were investigated via Uniform Manifold Approximation and Projection (UMAP)-based clustering and radiomic analysis, including 23 first-order and texture features. For breast tumor segmentation, m-3.0T (DSC: 0.494 [1.5T] and 0.433 [3.0T]) significantly outperformed m-1.5T (DSC: 0.411 [1.5T] and 0.289 [3.0T]) and m-combined (DSC: 0.373 [1.5T] and 0.268[3.0T]) on both validation sets (p<0.0001). Pancreas segmentation showed similar trends: m-3.0T achieved the highest DSC (0.774 [1.5T], 0.840 [3.0T]), while m-1.5T underperformed significantly (p<0.0001). For cervical spine, models performed optimally on same-field validation sets with minimal cross-field performance degradation (DSC>0.92 for all comparisons). Radiomic analysis revealed moderate field-strength-dependent clustering in soft tissues (silhouette scores 0.23-0.29) but minimal separation in osseous structures (0.12). These results indicate that magnetic field strength in the training data substantially influences the performance of deep learning-based segmentation models, particularly for soft-tissue structures (e.g., small lesions). This warrants consideration of magnetic field strength as a confounding factor in studies evaluating AI performance on MRI.

</details>


### [364] [AI-Enhanced Virtual Biopsies for Brain Tumor Diagnosis in Low Resource Settings](https://arxiv.org/abs/2512.22184)
*Areeb Ehsan*

Main category: eess.IV

TL;DR: 提出一种结合轻量CNN与手工放射组学特征的虚拟活检原型，用于脑MRI四分类，在低资源环境下提供决策支持。


<details>
  <summary>Details</summary>
Motivation: 低资源临床环境缺乏神经放射学专家、高端MRI设备与活检条件，现有深度学习方法存在计算需求高、跨设备泛化差、可解释性不足等问题。

Method: 采用MobileNetV2 CNN分类分支与手工放射组学特征分支（提取形状、强度统计、GLCM纹理等8个特征），通过晚期融合策略结合特征后训练随机森林分类器，并使用Grad-CAM和特征重要性分析提供可解释性。

Result: 在Kaggle脑肿瘤MRI数据集上，融合方法相比单分支基线验证性能提升；在降低分辨率与添加噪声的鲁棒性测试中，系统表现出对低资源成像条件的敏感性。

Conclusion: 该系统可作为临床诊断的辅助工具而非替代方案，为低资源环境下的脑肿瘤分析提供轻量、可解释的解决方案。

Abstract: Timely brain tumor diagnosis remains challenging in low-resource clinical environments where expert neuroradiology interpretation, high-end MRI hardware, and invasive biopsy procedures may be limited. Although deep learning has achieved strong performance in brain tumor analysis, real-world adoption is constrained by computational demands, dataset shift across scanners, and limited interpretability. This paper presents a prototype virtual biopsy pipeline for four-class classification of 2D brain MRI images using a lightweight convolutional neural network (CNN) and complementary radiomics-style handcrafted features. A MobileNetV2-based CNN is trained for classification, while an interpretable radiomics branch extracts eight features capturing lesion shape, intensity statistics, and gray-level co-occurrence matrix (GLCM) texture descriptors. A late fusion strategy concatenates CNN embeddings with radiomics features and trains a RandomForest classifier on the fused representation. Explainability is provided via Grad-CAM visualizations and radiomics feature importance analysis. Experiments on a public Kaggle brain tumor MRI dataset show improved validation performance for fusion relative to single-branch baselines, while robustness tests under reduced resolution and additive noise highlight sensitivity relevant to low-resource imaging conditions. The system is framed as decision support and not a substitute for clinical diagnosis or histopathology.

</details>


### [365] [Complex Swin Transformer for Accelerating Enhanced SMWI Reconstruction](https://arxiv.org/abs/2512.22202)
*Muhammad Usman,Sung-Min Gho*

Main category: eess.IV

TL;DR: 提出一种基于复数Swin Transformer的网络，用于从低分辨率k空间数据重建高质量SMWI图像，以缩短扫描时间同时保持诊断特征。


<details>
  <summary>Details</summary>
Motivation: SMWI技术检测帕金森病黑质高信号需要长扫描时间，需开发从降采样k空间数据高效重建的方法。

Method: 使用复数Swin Transformer网络进行多回波MRI数据的超分辨率重建，从低分辨率k空间输入生成高质量SMWI图像。

Result: 在256×256 k空间数据重建中达到结构相似性指数0.9116和均方误差0.076，并保持了关键诊断特征。

Conclusion: 该方法能通过减少k空间采样实现高质量SMWI重建，缩短扫描时间而不损失诊断细节，有望提升SMWI的临床适用性和神经影像工作流效率。

Abstract: Susceptibility Map Weighted Imaging (SMWI) is an advanced magnetic resonance imaging technique used to detect nigral hyperintensity in Parkinsons disease. However, full resolution SMWI acquisition is limited by long scan times. Efficient reconstruction methods are therefore required to generate high quality SMWI from reduced k space data while preserving diagnostic relevance. In this work, we propose a complex valued Swin Transformer based network for super resolution reconstruction of multi echo MRI data. The proposed method reconstructs high quality SMWI images from low resolution k space inputs. Experimental results demonstrate that the method achieves a structural similarity index of 0.9116 and a mean squared error of 0.076 when reconstructing SMWI from 256 by 256 k space data, while maintaining critical diagnostic features. This approach enables high quality SMWI reconstruction from reduced k space sampling, leading to shorter scan times without compromising diagnostic detail. The proposed method has the potential to improve the clinical applicability of SMWI for Parkinsons disease and support faster and more efficient neuroimaging workflows.

</details>


### [366] [Super-Resolution Enhancement of Medical Images Based on Diffusion Model: An Optimization Scheme for Low-Resolution Gastric Images](https://arxiv.org/abs/2512.22209)
*Haozhe Jia*

Main category: eess.IV

TL;DR: 提出基于扩散模型的胶囊内窥镜图像超分辨率方法，显著提升图像质量，改善病理特征识别能力。


<details>
  <summary>Details</summary>
Motivation: 胶囊内窥镜因硬件、功耗和传输限制导致图像分辨率低，难以识别黏膜细微纹理和早期病理特征，限制了其临床应用价值。

Method: 采用SR3框架（基于去噪扩散概率模型DDPMs），学习从低分辨率到高分辨率图像的概率映射，使用HyperKvasir数据集进行训练和评估，并引入注意力机制等架构增强。

Result: 定量结果显示，该方法显著优于双三次插值和ESRGAN等GAN方法，基线模型PSNR达27.5 dB、SSIM 0.65，增强后提升至29.3 dB和0.71；定性分析显示能更好保留解剖边界、血管模式和病灶结构。

Conclusion: 扩散模型超分辨率是提升胶囊内窥镜等非侵入式医学成像质量的有效方法，尤其在图像分辨率受硬件限制的场景中具有应用前景。

Abstract: Capsule endoscopy has enabled minimally invasive gastrointestinal imaging, but its clinical utility is limited by the inherently low resolution of captured images due to hardware, power, and transmission constraints. This limitation hampers the identification of fine-grained mucosal textures and subtle pathological features essential for early diagnosis.
  This work investigates a diffusion-based super-resolution framework to enhance capsule endoscopy images in a data-driven and anatomically consistent manner. We adopt the SR3 (Super-Resolution via Repeated Refinement) framework built upon Denoising Diffusion Probabilistic Models (DDPMs) to learn a probabilistic mapping from low-resolution to high-resolution images. Unlike GAN-based approaches that often suffer from training instability and hallucination artifacts, diffusion models provide stable likelihood-based training and improved structural fidelity. The HyperKvasir dataset, a large-scale publicly available gastrointestinal endoscopy dataset, is used for training and evaluation.
  Quantitative results demonstrate that the proposed method significantly outperforms bicubic interpolation and GAN-based super-resolution methods such as ESRGAN, achieving PSNR of 27.5 dB and SSIM of 0.65 for a baseline model, and improving to 29.3 dB and 0.71 with architectural enhancements including attention mechanisms. Qualitative results show improved preservation of anatomical boundaries, vascular patterns, and lesion structures. These findings indicate that diffusion-based super-resolution is a promising approach for enhancing non-invasive medical imaging, particularly in capsule endoscopy where image resolution is fundamentally constrained.

</details>


### [367] [SemCovert: Secure and Covert Video Transmission via Deep Semantic-Level Hiding](https://arxiv.org/abs/2512.22233)
*Zhihan Cao,Xiao Yang,Gaolei Li,Jun Wu,Jianhua Li,Yuchen Liu*

Main category: eess.IV

TL;DR: 提出SemCovert框架，通过深度语义级隐藏技术解决视频语义通信中的隐私泄露问题，实现安全隐蔽的视频传输。


<details>
  <summary>Details</summary>
Motivation: 视频语义通信存在隐私泄露风险，传统加密和隐写技术难以抵抗语义级变换，且视频的时序连续性增加了隐藏内容被分析和重建的风险。

Method: 设计语义隐藏模型和秘密语义提取器，集成到语义通信流程中；提出随机化语义隐藏策略，打破嵌入确定性并引入不可预测的分布模式。

Result: 实验表明SemCovet能有效降低窃听和检测风险，可靠隐藏秘密视频，同时视频质量仅轻微下降，保持了传输保真度。

Conclusion: SemCovet在不影响语义通信性能的前提下，实现了安全隐蔽的视频传输，验证了其有效性。

Abstract: Video semantic communication, praised for its transmission efficiency, still faces critical challenges related to privacy leakage. Traditional security techniques like steganography and encryption are challenging to apply since they are not inherently robust against semantic-level transformations and abstractions. Moreover, the temporal continuity of video enables framewise statistical modeling over extended periods, which increases the risk of exposing distributional anomalies and reconstructing hidden content. To address these challenges, we propose SemCovert, a deep semantic-level hiding framework for secure and covert video transmission. SemCovert introduces a pair of co-designed models, namely the semantic hiding model and the secret semantic extractor, which are seamlessly integrated into the semantic communication pipeline. This design enables authorized receivers to reliably recover hidden information, while keeping it imperceptible to regular users. To further improve resistance to analysis, we introduce a randomized semantic hiding strategy, which breaks the determinism of embedding and introduces unpredictable distribution patterns. The experimental results demonstrate that SemCovert effectively mitigates potential eavesdropping and detection risks while reliably concealing secret videos during transmission. Meanwhile, video quality suffers only minor degradation, preserving transmission fidelity. These results confirm SemCovert's effectiveness in enabling secure and covert transmission without compromising semantic communication performance.

</details>


### [368] [MEGA-PCC: A Mamba-based Efficient Approach for Joint Geometry and Attribute Point Cloud Compression](https://arxiv.org/abs/2512.22463)
*Kai-Hsiang Hsieh,Monyneath Yim,Wen-Hsiao Peng,Jui-Chiu Chiang*

Main category: eess.IV

TL;DR: 提出MEGA-PCC，一种基于Mamba架构的端到端点云几何与属性联合压缩框架，通过共享编码器和双解码器结构，结合Mamba熵模型，无需后处理着色和手动码率分配，实现了优异的率失真性能和运行效率。


<details>
  <summary>Details</summary>
Motivation: 现有点云压缩方法依赖后处理着色和手动调整几何与属性码率分配，阻碍端到端优化并增加系统复杂度，需要一种能自动优化码率分配且无需后处理的联合压缩框架。

Method: 1. 主压缩模型：共享编码器将几何与属性编码为统一潜在表示，双解码器顺序重建几何和属性；2. Mamba熵模型：利用Mamba架构捕获空间和通道相关性以改进概率估计；3. 基于Mamba架构建模长程依赖和丰富上下文特征。

Result: 大量实验表明，MEGA-PCC在率失真性能和运行效率上均优于传统和基于学习的基线方法，实现了数据驱动的码率分配并简化了处理流程。

Conclusion: MEGA-PCC通过端到端学习和Mamba架构的有效建模，为AI驱动的点云压缩提供了强大解决方案，消除了后处理着色和启发式码率调优的需求。

Abstract: Joint compression of point cloud geometry and attributes is essential for efficient 3D data representation. Existing methods often rely on post-hoc recoloring procedures and manually tuned bitrate allocation between geometry and attribute bitstreams in inference, which hinders end-to-end optimization and increases system complexity. To overcome these limitations, we propose MEGA-PCC, a fully end-to-end, learning-based framework featuring two specialized models for joint compression. The main compression model employs a shared encoder that encodes both geometry and attribute information into a unified latent representation, followed by dual decoders that sequentially reconstruct geometry and then attributes. Complementing this, the Mamba-based Entropy Model (MEM) enhances entropy coding by capturing spatial and channel-wise correlations to improve probability estimation. Both models are built on the Mamba architecture to effectively model long-range dependencies and rich contextual features. By eliminating the need for recoloring and heuristic bitrate tuning, MEGA-PCC enables data-driven bitrate allocation during training and simplifies the overall pipeline. Extensive experiments demonstrate that MEGA-PCC achieves superior rate-distortion performance and runtime efficiency compared to both traditional and learning-based baselines, offering a powerful solution for AI-driven point cloud compression.

</details>


### [369] [Semantic contrastive learning for orthogonal X-ray computed tomography reconstruction](https://arxiv.org/abs/2512.22674)
*Jiashu Dong,Jiabing Xiang,Lisheng Geng,Suqing Tian,Wei Zhao*

Main category: eess.IV

TL;DR: 提出一种结合语义特征对比学习的三阶段U-Net架构，用于稀疏视图CT重建，在降低辐射剂量的同时减少伪影并提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT重建可降低辐射剂量，但传统方法在病态条件下易产生严重条纹伪影；现有深度学习方法仍有改进空间，需同时兼顾语义相似性与解剖结构准确性。

Method: 设计语义特征对比学习损失函数，在深层潜在空间评估语义相似性，在浅层空间评估解剖相似性；采用三阶段U-Net架构，分别负责粗重建、细节优化和语义相似性度量。

Result: 在胸部正交投影数据集上测试表明，该方法相比其他算法具有更优的重建质量、更快的处理速度，且计算复杂度低。

Conclusion: 该方法为正交CT重建提供了实用解决方案，在保证图像质量的同时实现了高效计算，有望推动低剂量医学影像的临床应用。

Abstract: X-ray computed tomography (CT) is widely used in medical imaging, with sparse-view reconstruction offering an effective way to reduce radiation dose. However, ill-posed conditions often result in severe streak artifacts. Recent advances in deep learning-based methods have improved reconstruction quality, but challenges still remain. To address these challenges, we propose a novel semantic feature contrastive learning loss function that evaluates semantic similarity in high-level latent spaces and anatomical similarity in shallow latent spaces. Our approach utilizes a three-stage U-Net-based architecture: one for coarse reconstruction, one for detail refinement, and one for semantic similarity measurement. Tests on a chest dataset with orthogonal projections demonstrate that our method achieves superior reconstruction quality and faster processing compared to other algorithms. The results show significant improvements in image quality while maintaining low computational complexity, making it a practical solution for orthogonal CT reconstruction.

</details>


### [370] [SwinCCIR: An end-to-end deep network for Compton camera imaging reconstruction](https://arxiv.org/abs/2512.22766)
*Minghao Dong,Xinyang Luo,Xujian Ouyang,Yongshun Xiao*

Main category: eess.IV

TL;DR: 提出了一种基于Swin Transformer的端到端深度学习框架SwinCCIR，用于康普顿相机成像，直接建立列表模式事件与放射源分布之间的关系，有效改善了传统重建方法中的伪影和变形问题。


<details>
  <summary>Details</summary>
Motivation: 传统康普顿相机基于康普顿锥反投影的重建原理存在严重伪影和变形问题，且设备性能导致的系统误差难以通过校准完全消除，现有迭代算法和深度学习方法大多基于反投影结果进行优化，存在局限性。

Method: 采用Swin Transformer模块和基于转置卷积的图像生成模块，构建端到端深度学习框架SwinCCIR，直接学习列表模式事件与放射源分布之间的映射关系，并在模拟和实际数据集上进行训练验证。

Result: 实验结果表明，SwinCCIR能有效克服传统康普顿相机成像的问题，在模拟和实际数据上均表现出良好性能，有望在实际应用中实现。

Conclusion: SwinCCIR框架通过端到端深度学习直接建模事件与源分布的关系，避免了传统反投影方法的缺陷，为康普顿相机成像提供了新的有效解决方案。

Abstract: Compton cameras (CCs) are a kind of gamma cameras which are designed to determine the directions of incident gammas based on the Compton scatter. However, the reconstruction of CCs face problems of severe artifacts and deformation due to the fundamental reconstruction principle of back-projection of Compton cones. Besides, a part of systematic errors originated from the performance of devices are hard to remove through calibration, leading to deterioration of imaging quality. Iterative algorithms and deep-learning based methods have been widely used to improve reconstruction. But most of them are optimization based on the results of back-projection. Therefore, we proposed an end-to-end deep learning framework, SwinCCIR, for CC imaging. Through adopting swin-transformer blocks and a transposed convolution-based image generation module, we established the relationship between the list-mode events and the radioactive source distribution. SwinCCIR was trained and validated on both simulated and practical dataset. The experimental results indicate that SwinCCIR effectively overcomes problems of conventional CC imaging, which are expected to be implemented in practical applications.

</details>


### [371] [EIR: Enhanced Image Representations for Medical Report Generation](https://arxiv.org/abs/2512.23185)
*Qiang Sun,Zongcheng Ji,Yinlong Xiao,Peng Chang,Jun Yu*

Main category: eess.IV

TL;DR: 提出一种增强图像表示方法，通过跨模态Transformer融合医学元数据与图像表示，并使用医学领域预训练模型编码图像，显著提升胸部X光报告生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有自动报告生成方法简单融合医学元数据与视觉特征时存在信息不对称问题，且使用自然图像预训练模型存在领域差异，影响胸部X光报告生成的准确性。

Method: 1. 使用跨模态Transformer融合患者临床历史、相似病例报告等元数据与图像表示；2. 采用医学领域预训练模型编码胸部X光图像，减少领域差异。

Result: 在MIMIC和Open-I数据集上的实验表明，该方法能有效解决信息不对称和领域差异问题，提升了报告生成的准确性。

Conclusion: 跨模态融合与医学领域预训练相结合的方法能更准确地生成胸部X光报告，为临床辅助诊断提供了有效解决方案。

Abstract: Generating medical reports from chest X-ray images is a critical and time-consuming task for radiologists, especially in emergencies. To alleviate the stress on radiologists and reduce the risk of misdiagnosis, numerous research efforts have been dedicated to automatic medical report generation in recent years. Most recent studies have developed methods that represent images by utilizing various medical metadata, such as the clinical document history of the current patient and the medical graphs constructed from retrieved reports of other similar patients. However, all existing methods integrate additional metadata representations with visual representations through a simple "Add and LayerNorm" operation, which suffers from the information asymmetry problem due to the distinct distributions between them. In addition, chest X-ray images are usually represented using pre-trained models based on natural domain images, which exhibit an obvious domain gap between general and medical domain images. To this end, we propose a novel approach called Enhanced Image Representations (EIR) for generating accurate chest X-ray reports. We utilize cross-modal transformers to fuse metadata representations with image representations, thereby effectively addressing the information asymmetry problem between them, and we leverage medical domain pre-trained models to encode medical images, effectively bridging the domain gap for image representation. Experimental results on the widely used MIMIC and Open-I datasets demonstrate the effectiveness of our proposed method.

</details>
