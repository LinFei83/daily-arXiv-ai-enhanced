{"id": "2507.10689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10689", "abs": "https://arxiv.org/abs/2507.10689", "authors": ["Tongshun Zhang", "Pingping Liu", "Yubing Lu", "Mengen Cai", "Zijian Zhang", "Zhe Zhang", "Qiuzhan Zhou"], "title": "CWNet: Causal Wavelet Network for Low-Light Image Enhancement", "comment": "Accepted by ICCV 2025", "summary": "Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on\nuniform brightness adjustment, often neglecting instance-level semantic\ninformation and the inherent characteristics of different features. To address\nthese limitations, we propose CWNet (Causal Wavelet Network), a novel\narchitecture that leverages wavelet transforms for causal reasoning.\nSpecifically, our approach comprises two key components: 1) Inspired by the\nconcept of intervention in causality, we adopt a causal reasoning perspective\nto reveal the underlying causal relationships in low-light enhancement. From a\nglobal perspective, we employ a metric learning strategy to ensure causal\nembeddings adhere to causal principles, separating them from non-causal\nconfounding factors while focusing on the invariance of causal factors. At the\nlocal level, we introduce an instance-level CLIP semantic loss to precisely\nmaintain causal factor consistency. 2) Based on our causal analysis, we present\na wavelet transform-based backbone network that effectively optimizes the\nrecovery of frequency information, ensuring precise enhancement tailored to the\nspecific attributes of wavelet transforms. Extensive experiments demonstrate\nthat CWNet significantly outperforms current state-of-the-art methods across\nmultiple datasets, showcasing its robust performance across diverse scenes.\nCode is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.", "AI": {"tldr": "CWNet利用小波变换和因果推理改进低光图像增强，通过全局度量学习和局部语义损失保持因果一致性，显著优于现有方法。", "motivation": "传统低光图像增强方法忽视实例级语义信息和特征特性，CWNet旨在通过因果推理和小波变换解决这一问题。", "method": "结合因果推理（全局度量学习和局部CLIP语义损失）与小波变换网络，优化频率信息恢复。", "result": "在多个数据集上显著优于现有方法，表现鲁棒。", "conclusion": "CWNet通过因果推理和小波变换有效提升低光图像增强性能，代码开源。"}}
{"id": "2507.10737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10737", "abs": "https://arxiv.org/abs/2507.10737", "authors": ["Jiayuan Chen", "Thai-Hoang Pham", "Yuanlong Wang", "Ping Zhang"], "title": "Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines", "comment": "ICCV 2025", "summary": "High-throughput screening techniques, such as microscopy imaging of cellular\nresponses to genetic and chemical perturbations, play a crucial role in drug\ndiscovery and biomedical research. However, robust perturbation screening for\n\\textit{de novo} cell lines remains challenging due to the significant\nmorphological and biological heterogeneity across cell lines. To address this,\nwe propose a novel framework that integrates external biological knowledge into\nexisting pretraining strategies to enhance microscopy image profiling models.\nOur approach explicitly disentangles perturbation-specific and cell\nline-specific representations using external biological information.\nSpecifically, we construct a knowledge graph leveraging protein interaction\ndata from STRING and Hetionet databases to guide models toward\nperturbation-specific features during pretraining. Additionally, we incorporate\ntranscriptomic features from single-cell foundation models to capture cell\nline-specific representations. By learning these disentangled features, our\nmethod improves the generalization of imaging models to \\textit{de novo} cell\nlines. We evaluate our framework on the RxRx database through one-shot\nfine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from\nthe RxRx19a dataset. Experimental results demonstrate that our method enhances\nmicroscopy image profiling for \\textit{de novo} cell lines, highlighting its\neffectiveness in real-world phenotype-based drug discovery applications.", "AI": {"tldr": "提出了一种结合外部生物知识的新框架，用于增强显微镜图像分析模型，提高对新细胞系的泛化能力。", "motivation": "解决新细胞系在扰动筛选中因形态和生物异质性带来的挑战。", "method": "整合外部生物知识（如蛋白质相互作用图和转录组特征）到预训练策略中，明确分离扰动特异性和细胞系特异性表征。", "result": "在RxRx数据库上验证，通过少量样本微调显著提升了对新细胞系的显微镜图像分析能力。", "conclusion": "该方法在表型药物发现中具有实际应用价值，能有效处理新细胞系的异质性。"}}
{"id": "2507.10755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10755", "abs": "https://arxiv.org/abs/2507.10755", "authors": ["Rina Khan", "Catherine Stinson"], "title": "Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias", "comment": null, "summary": "Facial expression recognition (FER) algorithms classify facial expressions\ninto emotions such as happy, sad, or angry. An evaluative challenge facing FER\nalgorithms is the fall in performance when detecting spontaneous expressions\ncompared to posed expressions. An ethical (and evaluative) challenge facing FER\nalgorithms is that they tend to perform poorly for people of some races and\nskin colors. These challenges are linked to the data collection practices\nemployed in the creation of FER datasets. In this study, we audit two\nstate-of-the-art FER datasets. We take random samples from each dataset and\nexamine whether images are spontaneous or posed. In doing so, we propose a\nmethodology for identifying spontaneous or posed images. We discover a\nsignificant number of images that were posed in the datasets purporting to\nconsist of in-the-wild images. Since performance of FER models vary between\nspontaneous and posed images, the performance of models trained on these\ndatasets will not represent the true performance if such models were to be\ndeployed in in-the-wild applications. We also observe the skin color of\nindividuals in the samples, and test three models trained on each of the\ndatasets to predict facial expressions of people from various races and skin\ntones. We find that the FER models audited were more likely to predict people\nlabeled as not white or determined to have dark skin as showing a negative\nemotion such as anger or sadness even when they were smiling. This bias makes\nsuch models prone to perpetuate harm in real life applications.", "AI": {"tldr": "该研究审计了两个先进的面部表情识别（FER）数据集，发现其中许多图像是摆拍而非自然表情，且FER模型对非白人或深肤色人群存在偏见，倾向于将其微笑误判为负面情绪。", "motivation": "FER算法在检测自然表情时性能下降，且对不同种族和肤色的人群表现不佳，这些问题与数据集收集方式有关。", "method": "从两个数据集中随机抽样，区分自然与摆拍图像，并测试三个模型对不同肤色人群的表情预测准确性。", "result": "发现数据集中存在大量摆拍图像，且模型对非白人或深肤色人群的负面情绪预测存在偏见。", "conclusion": "数据集的质量和模型的偏见可能在实际应用中导致性能不准确和潜在危害。"}}
{"id": "2507.10589", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.10589", "abs": "https://arxiv.org/abs/2507.10589", "authors": ["Gaurav Singh"], "title": "Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays", "comment": null, "summary": "Pneumonia, particularly when induced by diseases like COVID-19, remains a\ncritical global health challenge requiring rapid and accurate diagnosis. This\nstudy presents a comprehensive comparison of traditional machine learning and\nstate-of-the-art deep learning approaches for automated pneumonia detection\nusing chest X-rays (CXRs). We evaluate multiple methodologies, ranging from\nconventional machine learning techniques (PCA-based clustering, Logistic\nRegression, and Support Vector Classification) to advanced deep learning\narchitectures including Convolutional Neural Networks (Modified LeNet,\nDenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,\nCompact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856\npediatric CXR images, we demonstrate that Vision Transformers, particularly the\nCross-ViT architecture, achieve superior performance with 88.25% accuracy and\n99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that\narchitectural choices impact performance more significantly than model size,\nwith Cross-ViT's 75M parameters outperforming larger models. The study also\naddresses practical considerations including computational efficiency, training\nrequirements, and the critical balance between precision and recall in medical\ndiagnostics. Our findings suggest that Vision Transformers offer a promising\ndirection for automated pneumonia detection, potentially enabling more rapid\nand accurate diagnosis during health crises.", "AI": {"tldr": "该研究比较了传统机器学习和深度学习在肺炎检测中的表现，发现Vision Transformers（尤其是Cross-ViT）在准确率和召回率上优于传统方法。", "motivation": "肺炎（如COVID-19引发的）需要快速准确诊断，研究旨在探索自动化检测的最佳方法。", "method": "评估了多种方法，包括传统机器学习（如PCA聚类、逻辑回归）和深度学习（如CNN、Vision Transformers），使用5,856张儿科胸片。", "result": "Cross-ViT表现最佳，准确率88.25%，召回率99.42%，且模型架构比大小更重要。", "conclusion": "Vision Transformers在肺炎检测中具有潜力，可提升诊断速度和准确性。"}}
{"id": "2507.10770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10770", "abs": "https://arxiv.org/abs/2507.10770", "authors": ["Ionuţ Grigore", "Călin-Adrian Popa", "Claudiu Leoveanu-Condrei"], "title": "FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching", "comment": null, "summary": "The extraction and matching of interest points are fundamental to many\ngeometric computer vision tasks. Traditionally, matching is performed by\nassigning descriptors to interest points and identifying correspondences based\non descriptor similarity. This work introduces a technique where interest\npoints are inherently associated during detection, eliminating the need for\ncomputing, storing, transmitting, or matching descriptors. Although the\nmatching accuracy is marginally lower than that of conventional approaches, our\nmethod completely eliminates the need for descriptors, leading to a drastic\nreduction in memory usage for localization systems. We assess its effectiveness\nby comparing it against both classical handcrafted methods and modern learned\napproaches.", "AI": {"tldr": "提出了一种无需描述符的兴趣点匹配方法，显著减少内存使用，但匹配精度略低于传统方法。", "motivation": "传统方法需要为兴趣点分配描述符并进行匹配，计算和存储成本高。", "method": "在检测过程中直接关联兴趣点，避免描述符的计算、存储和匹配。", "result": "匹配精度略低，但内存使用大幅减少。", "conclusion": "该方法在内存效率上有显著优势，适用于对内存敏感的定位系统。"}}
{"id": "2507.10615", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10615", "abs": "https://arxiv.org/abs/2507.10615", "authors": ["Guofeng Tong", "Sixuan Liu", "Yang Lv", "Hanyu Pei", "Feng-Lei Fan"], "title": "A Survey on Medical Image Compression: From Traditional to Learning-Based", "comment": null, "summary": "The exponential growth of medical imaging has created significant challenges\nin data storage, transmission, and management for healthcare systems. In this\nvein, efficient compression becomes increasingly important. Unlike natural\nimage compression, medical image compression prioritizes preserving diagnostic\ndetails and structural integrity, imposing stricter quality requirements and\ndemanding fast, memory-efficient algorithms that balance computational\ncomplexity with clinically acceptable reconstruction quality. Meanwhile, the\nmedical imaging family includes a plethora of modalities, each possessing\ndifferent requirements. For example, 2D medical image (e.g., X-rays,\nhistopathological images) compression focuses on exploiting intra-slice spatial\nredundancy, while volumetric medical image faces require handling intra-slice\nand inter-slice spatial correlations, and 4D dynamic imaging (e.g., time-series\nCT/MRI, 4D ultrasound) additionally demands processing temporal correlations\nbetween consecutive time frames. Traditional compression methods, grounded in\nmathematical transforms and information theory principles, provide solid\ntheoretical foundations, predictable performance, and high standardization\nlevels, with extensive validation in clinical environments. In contrast, deep\nlearning-based approaches demonstrate remarkable adaptive learning capabilities\nand can capture complex statistical characteristics and semantic information\nwithin medical images. This comprehensive survey establishes a two-facet\ntaxonomy based on data structure (2D vs 3D/4D) and technical approaches\n(traditional vs learning-based), thereby systematically presenting the complete\ntechnological evolution, analyzing the unique technical challenges, and\nprospecting future directions in medical image compression.", "AI": {"tldr": "医疗图像压缩面临数据存储和传输挑战，需平衡计算复杂性与临床质量要求。传统方法与深度学习方法各有优势，本文提出基于数据结构和技术的分类法。", "motivation": "医疗图像的快速增长对存储和传输提出了挑战，压缩需在保留诊断细节的同时满足临床需求。", "method": "提出基于数据结构（2D vs 3D/4D）和技术方法（传统 vs 学习型）的分类法，系统分析技术演变。", "result": "传统方法提供理论支持，深度学习方法适应性强，两者各有优势。", "conclusion": "未来医疗图像压缩需结合传统与深度学习方法，解决多模态需求。"}}
{"id": "2507.10634", "categories": ["eess.SY", "cs.LG", "cs.SY", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10634", "abs": "https://arxiv.org/abs/2507.10634", "authors": ["Thomas Feys", "Liesbet Van der Perre", "François Rottenberg"], "title": "Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach", "comment": null, "summary": "Massive MIMO systems are moving toward increased numbers of radio frequency\nchains, higher carrier frequencies and larger bandwidths. As such,\ndigital-to-analog converters (DACs) are becoming a bottleneck in terms of\nhardware complexity and power consumption. In this work, non-linear precoding\nfor coarsely quantized downlink massive MIMO is studied. Given the NP-hard\nnature of this problem, a graph neural network (GNN) is proposed that directly\noutputs the precoded quantized vector based on the channel matrix and the\nintended transmit symbols. The model is trained in a self-supervised manner, by\ndirectly maximizing the achievable rate. To overcome the non-differentiability\nof the objective function, introduced due to the non-differentiable DAC\nfunctions, a straight-through Gumbel-softmax estimation of the gradient is\nproposed. The proposed method achieves a significant increase in achievable sum\nrate under coarse quantization. For instance, in the single-user case, the\nproposed method can achieve the same sum rate as maximum ratio transmission\n(MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the\nDAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs\nrespectively. This, however, comes at the cost of increased digital signal\nprocessing power consumption. When accounting for this, the reduction in\noverall power consumption holds for a system bandwidth up to 3.5 MHz for\nbaseband DACs, while the RF DACs can maintain a power reduction of 2.9 for\nhigher bandwidths. Notably, indirect effects, which further reduce the power\nconsumption, such as a reduced fronthaul consumption and reduction in other\ncomponents, are not considered in this analysis.", "AI": {"tldr": "研究提出了一种基于图神经网络（GNN）的非线性预编码方法，用于解决大规模MIMO系统中粗量化DAC的硬件复杂性和功耗问题。", "motivation": "随着大规模MIMO系统的发展，DAC在硬件复杂性和功耗方面成为瓶颈，需要一种高效的方法来优化粗量化下的性能。", "method": "使用图神经网络（GNN）直接输出预编码的量化向量，并通过自监督训练最大化可达速率。采用Gumbel-softmax梯度估计解决目标函数的不可微问题。", "result": "在粗量化下，该方法显著提高了可达总速率。例如，单用户情况下，使用1位DAC即可达到与3位DAC的MRT相同的总速率，功耗降低4-7倍（基带DAC）和3倍（射频DAC）。", "conclusion": "该方法在特定带宽范围内显著降低了整体功耗，但增加了数字信号处理的功耗。未考虑的其他间接效应可能进一步降低功耗。"}}
{"id": "2507.10602", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10602", "abs": "https://arxiv.org/abs/2507.10602", "authors": ["Maximilian Stölzle", "T. Konstantin Rusch", "Zach J. Patterson", "Rodrigo Pérez-Dattari", "Francesco Stella", "Josie Hughes", "Cosimo Della Santina", "Daniela Rus"], "title": "Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees", "comment": "73 pages", "summary": "Learning from demonstration provides a sample-efficient approach to acquiring\ncomplex behaviors, enabling robots to move robustly, compliantly, and with\nfluidity. In this context, Dynamic Motion Primitives offer built - in stability\nand robustness to disturbances but often struggle to capture complex periodic\nbehaviors. Moreover, they are limited in their ability to interpolate between\ndifferent tasks. These shortcomings substantially narrow their applicability,\nexcluding a wide class of practically meaningful tasks such as locomotion and\nrhythmic tool use. In this work, we introduce Orbitally Stable Motion\nPrimitives (OSMPs) - a framework that combines a learned diffeomorphic encoder\nwith a supercritical Hopf bifurcation in latent space, enabling the accurate\nacquisition of periodic motions from demonstrations while ensuring formal\nguarantees of orbital stability and transverse contraction. Furthermore, by\nconditioning the bijective encoder on the task, we enable a single learned\npolicy to represent multiple motion objectives, yielding consistent zero-shot\ngeneralization to unseen motion objectives within the training distribution. We\nvalidate the proposed approach through extensive simulation and real-world\nexperiments across a diverse range of robotic platforms - from collaborative\narms and soft manipulators to a bio-inspired rigid-soft turtle robot -\ndemonstrating its versatility and effectiveness in consistently outperforming\nstate-of-the-art baselines such as diffusion policies, among others.", "AI": {"tldr": "论文提出了一种名为OSMPs的新框架，结合了学习的微分同胚编码器和超临界Hopf分岔，用于从演示中准确学习周期性运动，并确保轨道稳定性和横向收缩。", "motivation": "Dynamic Motion Primitives在捕捉复杂周期性行为和任务间插值方面存在局限性，限制了其在实际任务（如运动和节奏工具使用）中的应用。", "method": "引入OSMPs框架，结合学习的微分同胚编码器和超临界Hopf分岔，通过任务条件化的双射编码器实现多运动目标的表示。", "result": "在多种机器人平台上验证了方法的有效性，表现优于现有基线（如扩散策略）。", "conclusion": "OSMPs框架能够高效学习周期性运动，并实现零样本泛化，具有广泛的应用潜力。"}}
{"id": "2507.10562", "categories": ["cs.AI", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10562", "abs": "https://arxiv.org/abs/2507.10562", "authors": ["Hari Masoor"], "title": "SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents", "comment": "7 pages, 4 figures, 3 implementation examples. Original work\n  submitted as a preprint", "summary": "Current AI agent architectures suffer from ephemeral memory limitations,\npreventing effective collaboration and knowledge sharing across sessions and\nagent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a\nnovel framework that enables persistent, secure, and semantically searchable\nmemory sharing among AI agents. Our protocol addresses three critical\nchallenges: (1) persistent context preservation across agent sessions, (2)\nsecure multi-agent collaboration with fine-grained access control, and (3)\nefficient semantic discovery of relevant historical context. SAMEP implements a\ndistributed memory repository with vector-based semantic search, cryptographic\naccess controls (AES-256-GCM), and standardized APIs compatible with existing\nagent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness\nacross diverse domains including multi-agent software development, healthcare\nAI with HIPAA compliance, and multi-modal processing pipelines. Experimental\nresults show 73% reduction in redundant computations, 89% improvement in\ncontext relevance scores, and complete compliance with regulatory requirements\nincluding audit trail generation. SAMEP enables a new paradigm of persistent,\ncollaborative AI agent ecosystems while maintaining security and privacy\nguarantees.", "AI": {"tldr": "SAMEP协议解决了AI代理间记忆共享的持久性、安全性和语义搜索问题，显著提升了协作效率和合规性。", "motivation": "当前AI代理架构存在记忆短暂性问题，限制了跨会话和代理边界的协作与知识共享。", "method": "提出SAMEP框架，结合分布式记忆存储、向量语义搜索、加密访问控制和标准化API。", "result": "实验显示减少了73%冗余计算，提升了89%上下文相关性，完全符合监管要求。", "conclusion": "SAMEP为AI代理生态系统提供了持久、安全、协作的新范式。"}}
{"id": "2507.10577", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.10577", "abs": "https://arxiv.org/abs/2507.10577", "authors": ["Logé Cécile", "Ghori Rehan"], "title": "Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions", "comment": null, "summary": "Misinformation poses a significant threat in today's digital world, often\nspreading rapidly through platforms like YouTube. This paper introduces a novel\napproach to combating misinformation by developing an AI-powered system that\nnot only fact-checks claims made in YouTube videos but also actively engages\nusers in the comment section and challenge misleading narratives. Our system\ncomprises two main agents: Truth Sleuth and Trend Bender.\n  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented\nGeneration (RAG) approach - drawing on sources like Wikipedia, Google Search,\nGoogle FactCheck - to accurately assess their veracity and generates a nuanced\nand comprehensive report. Through rigorous prompt engineering, Trend Bender\nleverages this report along with a curated corpus of relevant articles to\ngenerate insightful and persuasive comments designed to stimulate a productive\ndebate. With a carefully set up self-evaluation loop, this agent is able to\niteratively improve its style and refine its output.\n  We demonstrate the system's capabilities through experiments on established\nbenchmark datasets and a real-world deployment on YouTube, showcasing its\npotential to engage users and potentially influence perspectives. Our findings\nhighlight the high accuracy of our fact-checking agent, and confirm the\npotential of AI-driven interventions in combating misinformation and fostering\na more informed online space.", "AI": {"tldr": "论文提出了一种AI驱动的系统，通过两个代理（Truth Sleuth和Trend Bender）在YouTube上检测和挑战虚假信息，并展示了其高准确性和潜在影响力。", "motivation": "虚假信息在数字世界中迅速传播，尤其是在YouTube等平台上，亟需有效的解决方案。", "method": "系统采用RAG方法提取视频中的主张并验证其真实性，同时生成有说服力的评论以促进讨论。", "result": "实验证明系统在事实核查和用户互动方面具有高准确性和潜力。", "conclusion": "AI驱动的干预措施在打击虚假信息和促进在线空间信息真实性方面具有显著潜力。"}}
{"id": "2507.10775", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10775", "abs": "https://arxiv.org/abs/2507.10775", "authors": ["Jeffrey Joan Sam", "Janhavi Sathe", "Nikhil Chigali", "Naman Gupta", "Radhey Ruparel", "Yicheng Jiang", "Janmajay Singh", "James W. Berck", "Arko Barman"], "title": "A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers", "comment": null, "summary": "Spacecraft deployed in outer space are routinely subjected to various forms\nof damage due to exposure to hazardous environments. In addition, there are\nsignificant risks to the subsequent process of in-space repairs through human\nextravehicular activity or robotic manipulation, incurring substantial\noperational costs. Recent developments in image segmentation could enable the\ndevelopment of reliable and cost-effective autonomous inspection systems. While\nthese models often require large amounts of training data to achieve\nsatisfactory results, publicly available annotated spacecraft segmentation data\nare very scarce. Here, we present a new dataset of nearly 64k annotated\nspacecraft images that was created using real spacecraft models, superimposed\non a mixture of real and synthetic backgrounds generated using NASA's TTALOS\npipeline. To mimic camera distortions and noise in real-world image\nacquisition, we also added different types of noise and distortion to the\nimages. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to\ngenerate performance benchmarks for the dataset under well-defined hardware and\ninference time constraints to mimic real-world image segmentation challenges\nfor real-time onboard applications in space on NASA's inspector spacecraft. The\nresulting models, when tested under these constraints, achieved a Dice score of\n0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.\nThe dataset and models for performance benchmark are available at\nhttps://github.com/RiceD2KLab/SWiM.", "AI": {"tldr": "论文提出了一种新的航天器图像数据集，用于训练和评估图像分割模型，以支持自主航天器检测系统。", "motivation": "航天器在太空环境中易受损坏，人工或机器人维修成本高且风险大，需要可靠的自主检测系统。", "method": "创建了一个包含64k标注图像的航天器数据集，结合真实和合成背景，并添加噪声和失真。使用YOLOv8和YOLOv11模型进行微调，并在模拟真实硬件条件下评估性能。", "result": "模型在模拟条件下达到Dice分数0.92、Hausdorff距离0.69，推理时间约0.5秒。", "conclusion": "该数据集和模型为航天器实时图像分割提供了有效基准，支持自主检测系统的开发。"}}
{"id": "2507.10869", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10869", "abs": "https://arxiv.org/abs/2507.10869", "authors": ["Chetan Madan", "Aarjav Satia", "Soumen Basu", "Pankaj Gupta", "Usha Dutta", "Chetan Arora"], "title": "Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification", "comment": "To appear at MICCAI 2025", "summary": "Masked Autoencoders (MAEs) have emerged as a dominant strategy for\nself-supervised representation learning in natural images, where models are\npre-trained to reconstruct masked patches with a pixel-wise mean squared error\n(MSE) between original and reconstructed RGB values as the loss. We observe\nthat MSE encourages blurred image re-construction, but still works for natural\nimages as it preserves dominant edges. However, in medical imaging, when the\ntexture cues are more important for classification of a visual abnormality, the\nstrategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM)\nfeature in Radiomics studies, we propose a novel MAE based pre-training\nframework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM\ncaptures intensity and spatial relationships in an image, hence proposed loss\nhelps preserve morphological features. Further, we propose a novel formulation\nto convert matching GLCM matrices into a differentiable loss function. We\ndemonstrate that unsupervised pre-training on medical images with the proposed\nGLCM loss improves representations for downstream tasks. GLCM-MAE outperforms\nthe current state-of-the-art across four tasks - gallbladder cancer detection\nfrom ultrasound images by 2.1%, breast cancer detection from ultrasound by\n3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by\n0.6%. Source code and pre-trained models are available at:\nhttps://github.com/ChetanMadan/GLCM-MAE.", "AI": {"tldr": "GLCM-MAE是一种基于GLCM特征的新型MAE预训练框架，用于医学图像的自监督表示学习，显著提升下游任务性能。", "motivation": "传统MAE在医学图像中因纹理特征重要性而表现不佳，GLCM-MAE通过匹配GLCM矩阵的损失函数保留形态特征。", "method": "提出GLCM-MAE框架，使用GLCM矩阵匹配作为重建损失，并设计可微的GLCM损失函数。", "result": "在胆囊癌、乳腺癌、肺炎和COVID检测任务中，GLCM-MAE分别提升2.1%、3.1%、0.5%和0.6%。", "conclusion": "GLCM-MAE通过GLCM损失有效保留医学图像的形态特征，显著提升下游任务性能。"}}
{"id": "2507.10979", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10979", "abs": "https://arxiv.org/abs/2507.10979", "authors": ["Mahdieh Zaker", "Amy Nejati", "Abolfazl Lavaei"], "title": "Data-Driven Safety Certificates of Infinite Networks with Unknown Models and Interconnection Topologies", "comment": null, "summary": "Infinite networks are complex interconnected systems comprising a countably\ninfinite number of subsystems, where counting them precisely poses a\nsignificant challenge due to the seemingly endless interconnected nature of the\nnetwork (e.g., counting vehicles on the road). In such scenarios, the presence\nof infinitely many subsystems within the network renders the existing analysis\nframeworks tailored for finite networks inapplicable to infinite ones. This\npaper is concerned with offering a data-driven approach, within a compositional\nframework, for the safety certification of infinite networks with both unknown\nmathematical models and interconnection topologies. Given the immense\ncomputational complexity stemming from the extensive dimension of infinite\nnetworks, our approach capitalizes on the joint dissipativity-type properties\nof subsystems, characterized by storage certificates. We introduce innovative\ncompositional data-driven conditions to construct a barrier certificate for the\ninfinite network leveraging storage certificates of its unknown subsystems\nderived from data, while offering correctness guarantees across the network\nsafety. We demonstrate that our compositional data-driven reasoning eliminates\nthe requirement for checking the traditional dissipativity condition, which\ntypically mandates precise knowledge of the interconnection topology. In\naddition, while existing data-driven literature demonstrates an exponential\ntrend in sample complexity with respect to network size, we showcase that our\ncompositional strategy notably reduces it to a linear scale in terms of the\nnumber of subsystems. We illustrate our data-driven results on two physical\ninfinite networks with unknown models and interconnection topologies.", "AI": {"tldr": "本文提出了一种基于数据驱动的组合方法，用于安全认证无限网络，解决了传统有限网络分析方法不适用的问题。", "motivation": "无限网络的复杂性和未知性使得传统分析方法失效，需要新的数据驱动方法来解决其安全认证问题。", "method": "利用子系统的联合耗散性特性和存储证书，构建无限网络的屏障证书，无需精确了解互连拓扑。", "result": "该方法显著降低了样本复杂度（从指数级降至线性级），并消除了传统耗散性条件的检查需求。", "conclusion": "提出的组合数据驱动方法有效解决了无限网络的安全认证问题，具有实际应用潜力。"}}
{"id": "2507.10672", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10672", "abs": "https://arxiv.org/abs/2507.10672", "authors": ["Muhayy Ud Din", "Waseem Akram", "Lyes Saad Saoud", "Jan Rosell", "Irfan Hussain"], "title": "Vision Language Action Models in Robotic Manipulation: A Systematic Review", "comment": "submitted to annual review in control", "summary": "Vision Language Action (VLA) models represent a transformative shift in\nrobotics, with the aim of unifying visual perception, natural language\nunderstanding, and embodied control within a single learning framework. This\nreview presents a comprehensive and forward-looking synthesis of the VLA\nparadigm, with a particular emphasis on robotic manipulation and\ninstruction-driven autonomy. We comprehensively analyze 102 VLA models, 26\nfoundational datasets, and 12 simulation platforms that collectively shape the\ndevelopment and evaluation of VLAs models. These models are categorized into\nkey architectural paradigms, each reflecting distinct strategies for\nintegrating vision, language, and control in robotic systems. Foundational\ndatasets are evaluated using a novel criterion based on task complexity,\nvariety of modalities, and dataset scale, allowing a comparative analysis of\ntheir suitability for generalist policy learning. We introduce a\ntwo-dimensional characterization framework that organizes these datasets based\non semantic richness and multimodal alignment, showing underexplored regions in\nthe current data landscape. Simulation environments are evaluated for their\neffectiveness in generating large-scale data, as well as their ability to\nfacilitate transfer from simulation to real-world settings and the variety of\nsupported tasks. Using both academic and industrial contributions, we recognize\nongoing challenges and outline strategic directions such as scalable\npretraining protocols, modular architectural design, and robust multimodal\nalignment strategies. This review serves as both a technical reference and a\nconceptual roadmap for advancing embodiment and robotic control, providing\ninsights that span from dataset generation to real world deployment of\ngeneralist robotic agents.", "AI": {"tldr": "本文综述了视觉语言动作（VLA）模型在机器人领域的应用，分析了102个模型、26个数据集和12个仿真平台，提出了新的评估框架，并指出了未来研究方向。", "motivation": "统一视觉感知、自然语言理解和机器人控制，推动通用机器人代理的发展。", "method": "通过分类关键架构范式、评估数据集和仿真平台，提出二维表征框架。", "result": "揭示了当前数据集的不足，并提出了可扩展的预训练协议和模块化设计等战略方向。", "conclusion": "本文为机器人控制提供了技术参考和概念路线图，从数据集生成到实际部署提供了全面指导。"}}
{"id": "2507.10566", "categories": ["cs.AI", "cs.GT", "cs.LG", "cs.MA", "cs.NE", "68T07, 68T40, 91A20", "I.2.6; I.2.11; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10566", "abs": "https://arxiv.org/abs/2507.10566", "authors": ["Hung Ming Liu"], "title": "AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems", "comment": "30 pages, 4 figures", "summary": "In Decentralized Multi-Agent Reinforcement Learning (MARL), the development\nof Emergent Communication has long been constrained by the ``Joint Exploration\nDilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .\nTraditional methods address this by introducing inductive biases to facilitate\ncommunication emergence . This study fundamentally questions whether such\nartificial inductive biases are, in fact, over-engineering. Through experiments\nwith the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized\nVariational Autoencoder (VQ-VAE), we demonstrate that when agents possess an\nendogenous symbol system, their neural representations naturally exhibit\nspontaneous semantic compression and Nash equilibrium-driven semantic\nconvergence, achieving effective symbolic communication without external\ninductive biases. This aligns with recent neuroscience findings suggesting that\nthe human brain does not directly use human language for internal thought , and\nresonates with research on ``soft thinking'' capabilities in Large Language\nModels (LLMs) . Compared to traditional explicit communication methods, AIM\ndemonstrates stronger generality and efficiency. The interpretable analysis\ntoolkit developed in this study confirms that symbol usage exhibits a\nsignificant power-law distribution, leading to three major theoretical\ninsights: the ``Neural Communication Hypothesis'', the ``Tool-First\nPrinciple'', and the ``Semantic Interpretability Paradigm''. Future research\nwill explore the integration of Hierarchical Quantized Variational Autoencoders\n(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the\npotential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This\ndiscovery offers new avenues for bridging symbolism and connectionism.", "AI": {"tldr": "论文提出了一种基于VQ-VAE的\"AI Mother Tongue\"框架，证明在去中心化MARL中，无需外部归纳偏置，代理的内生符号系统可实现有效通信。", "motivation": "解决传统方法因\"联合探索困境\"导致的\"通信真空均衡\"问题，质疑人工归纳偏置是否过度设计。", "method": "采用VQ-VAE框架，通过内生符号系统实现语义压缩与纳什均衡驱动的语义收敛。", "result": "AIM框架展现出更强的泛化性与效率，符号使用符合幂律分布，并提出了三个理论见解。", "conclusion": "研究为符号主义与连接主义的结合提供了新思路，未来将探索HQ-VAE增强表达能力和RL低层预训练。"}}
{"id": "2507.10580", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10580", "abs": "https://arxiv.org/abs/2507.10580", "authors": ["Vimaleswar A", "Prabhu Nandan Sahu", "Nilesh Kumar Sahu", "Haroon R Lone"], "title": "An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation", "comment": null, "summary": "Mental health plays a crucial role in the overall well-being of an\nindividual. In recent years, digital platforms have been increasingly used to\nexpand mental health and emotional support. However, there are persistent\nchallenges related to limited user accessibility, internet connectivity, and\ndata privacy, which highlight the need for an offline, smartphone-based\nsolution. To address these challenges, we propose EmoSApp (Emotional Support\nApp): an entirely offline, smartphone-based conversational app designed for\nmental health and emotional support. The system leverages Large Language Models\n(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and\nExecutorch for resource-constrained devices, allowing all inferences to occur\non the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned\nthe LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of\n14,582 mental-health QA pairs, along with the multi-turn conversational data.\n  Through qualitative human evaluation with the student population, we\ndemonstrate that EmoSApp has the ability to respond coherently, empathetically,\nmaintain interactive dialogue, and provide relevant suggestions to user's\nmental health problems. Additionally, quantitative evaluations on nine standard\ncommonsense and reasoning benchmarks demonstrate the efficacy of our\nfine-tuned, quantized model in low-resource settings. By prioritizing on-device\ndeployment and specialized domain adaptation, EmoSApp serves as a blueprint for\nfuture innovations in portable, secure, and highly tailored AI-driven mental\nhealth solutions.", "AI": {"tldr": "EmoSApp是一款基于智能手机的离线对话应用，利用优化的LLM为心理健康提供支持，解决了可访问性和隐私问题。", "motivation": "数字心理健康平台存在用户可访问性、网络连接和数据隐私问题，需要离线解决方案。", "method": "使用Torchtune和Executorch优化并部署LLaMA-3.2-1B-Instruct模型，结合心理健康知识数据集和多轮对话数据。", "result": "通过学生群体评估，EmoSApp能提供连贯、共情的对话和相关建议；量化测试显示模型在低资源环境中的有效性。", "conclusion": "EmoSApp为便携、安全、定制化的AI心理健康解决方案提供了范例。"}}
{"id": "2507.10778", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10778", "abs": "https://arxiv.org/abs/2507.10778", "authors": ["Hsiang-Wei Huang", "Jen-Hao Cheng", "Kuang-Ming Chen", "Cheng-Yen Yang", "Bahaa Alattar", "Yi-Ru Lin", "Pyongkun Kim", "Sangwon Kim", "Kwangju Kim", "Chung-I Huang", "Jenq-Neng Hwang"], "title": "Warehouse Spatial Question Answering with LLM Agent", "comment": "1st Place Solution of the 9th AI City Challenge Track 3", "summary": "Spatial understanding has been a challenging task for existing Multi-modal\nLarge Language Models~(MLLMs). Previous methods leverage large-scale MLLM\nfinetuning to enhance MLLM's spatial understanding ability. In this paper, we\npresent a data-efficient approach. We propose a LLM agent system with strong\nand advanced spatial reasoning ability, which can be used to solve the\nchallenging spatial question answering task in complex indoor warehouse\nscenarios. Our system integrates multiple tools that allow the LLM agent to\nconduct spatial reasoning and API tools interaction to answer the given\ncomplicated spatial question. Extensive evaluations on the 2025 AI City\nChallenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that\nour system achieves high accuracy and efficiency in tasks such as object\nretrieval, counting, and distance estimation. The code is available at:\nhttps://github.com/hsiangwei0903/SpatialAgent", "AI": {"tldr": "提出了一种数据高效的方法，通过LLM代理系统增强空间推理能力，用于解决复杂室内仓库场景中的空间问答任务。", "motivation": "现有多模态大语言模型（MLLMs）在空间理解任务上表现不足，需要更高效的方法提升其能力。", "method": "构建了一个LLM代理系统，整合多种工具进行空间推理和API交互。", "result": "在2025 AI City Challenge数据集上表现出高准确性和效率，适用于物体检索、计数和距离估计等任务。", "conclusion": "该方法在复杂空间任务中表现优异，代码已开源。"}}
{"id": "2507.11043", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11043", "abs": "https://arxiv.org/abs/2507.11043", "authors": ["He Zhichao", "Shen Xiangyu", "Zhang Yong", "Xie Nan"], "title": "Real-Time Foreign Object Recognition Based on Improved Wavelet Scattering Deep Network and Edge Computing", "comment": null, "summary": "The increasing penetration rate of new energy in the power system has put\nforward higher requirements for the operation and maintenance of substations\nand transmission lines. Using the Unmanned Aerial Vehicles (UAV) to identify\nforeign object in real time can quickly and effectively eliminate potential\nsafety hazards. However, due to the limited computation power, the captured\nimage cannot be real-time processed on edge devices in UAV locally. To overcome\nthis problem, a lightweight model based on an improved wavelet scatter deep\nnetwork is proposed. This model contains improved wavelet scattering network\nfor extracting the scatter coefficients and modulus coefficients of image\nsingle channel, replacing the role of convolutional layer and pooling layer in\nconvolutional neural network. The following 3 fully connected layers, also\nconstituted a simplified Multilayer Perceptron (MLP), are used to classify the\nextracted features. Experiments prove that the model constructed with\nbiorthogonal wavelets basis is able to recognize and classify the foreign\nobject in edge devices such as Raspberry Pi and Jetson Nano, with accuracy\nhigher than 90% and inference time less than 7ms for 720P (1280*720) images.\nFurther experiments demonstrate that the recognition accuracy of our model is\n1.1% higher than YOLOv5s and 0.3% higher than YOLOv8s.", "AI": {"tldr": "提出了一种基于改进小波散射网络的轻量级模型，用于无人机实时识别电力系统中的异物，准确率超过90%，推理时间小于7ms。", "motivation": "新能源在电力系统中的渗透率增加，对变电站和输电线路的运维提出更高要求，需实时识别异物以消除安全隐患。", "method": "改进小波散射网络提取图像单通道的散射系数和模系数，替代传统卷积神经网络中的卷积层和池化层，结合简化的多层感知机进行分类。", "result": "模型在边缘设备（如Raspberry Pi和Jetson Nano）上实现90%以上的准确率，推理时间小于7ms，性能优于YOLOv5s和YOLOv8s。", "conclusion": "该轻量级模型有效解决了无人机边缘设备计算能力有限的问题，实现了高效实时的异物识别。"}}
{"id": "2507.11021", "categories": ["eess.SY", "cs.GT", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11021", "abs": "https://arxiv.org/abs/2507.11021", "authors": ["Pau de las Heras Molins", "Eric Roy-Almonacid", "Dong Ho Lee", "Lasse Peters", "David Fridovich-Keil", "Georgios Bakirtzis"], "title": "Approximate solutions to games of ordered preference", "comment": null, "summary": "Autonomous vehicles must balance ranked objectives, such as minimizing travel\ntime, ensuring safety, and coordinating with traffic. Games of ordered\npreference effectively model these interactions but become computationally\nintractable as the time horizon, number of players, or number of preference\nlevels increase. While receding horizon frameworks mitigate long-horizon\nintractability by solving sequential shorter games, often warm-started, they do\nnot resolve the complexity growth inherent in existing methods for solving\ngames of ordered preference. This paper introduces a solution strategy that\navoids excessive complexity growth by approximating solutions using\nlexicographic iterated best response (IBR) in receding horizon, termed\n\"lexicographic IBR over time.\" Lexicographic IBR over time uses past\ninformation to accelerate convergence. We demonstrate through simulated traffic\nscenarios that lexicographic IBR over time efficiently computes\napproximate-optimal solutions for receding horizon games of ordered preference,\nconverging towards generalized Nash equilibria.", "AI": {"tldr": "论文提出了一种名为“lexicographic IBR over time”的方法，用于高效计算具有排序偏好的博弈问题，避免了传统方法的计算复杂度增长。", "motivation": "自动驾驶车辆需要在多个目标（如最短时间、安全性、交通协调）之间平衡，而现有的排序偏好博弈方法在时间、玩家数量或偏好层级增加时计算复杂度高。", "method": "采用“lexicographic IBR over time”方法，结合过去信息加速收敛，在滚动时域框架中近似求解排序偏好博弈。", "result": "通过模拟交通场景验证，该方法能高效计算近似最优解，并收敛至广义纳什均衡。", "conclusion": "该方法有效解决了排序偏好博弈的计算复杂度问题，适用于自动驾驶车辆的决策优化。"}}
{"id": "2507.10694", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10694", "abs": "https://arxiv.org/abs/2507.10694", "authors": ["Francesco Fuentes", "Serigne Diagne", "Zachary Kingston", "Laura H. Blumenschein"], "title": "Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots", "comment": "22 pages, 21 figures, submitted to journal for potential publication", "summary": "Passive deformation due to compliance is a commonly used benefit of soft\nrobots, providing opportunities to achieve robust actuation with few active\ndegrees of freedom. Soft growing robots in particular have shown promise in\nnavigation of unstructured environments due to their passive deformation. If\ntheir collisions and subsequent deformations can be better understood, soft\nrobots could be used to understand the structure of the environment from direct\ntactile measurements. In this work, we propose the use of soft growing robots\nas mapping and exploration tools. We do this by first characterizing collision\nbehavior during discrete turns, then leveraging this model to develop a\ngeometry-based simulator that models robot trajectories in 2D environments.\nFinally, we demonstrate the model and simulator validity by mapping unknown\nenvironments using Monte Carlo sampling to estimate the optimal next deployment\ngiven current knowledge. Over both uniform and non-uniform environments, this\nselection method rapidly approaches ideal actions, showing the potential for\nsoft growing robots in unstructured environment exploration and mapping.", "AI": {"tldr": "本文提出利用软体生长机器人作为环境和探索工具，通过建模碰撞行为和开发几何模拟器，展示了其在非结构化环境中的潜力。", "motivation": "软体机器人因其被动变形能力在非结构化环境中表现出色，但需要更好地理解碰撞和变形行为以利用其进行环境结构探测。", "method": "首先表征离散转向中的碰撞行为，开发几何模拟器建模2D环境中的机器人轨迹，并通过蒙特卡洛采样验证模型有效性。", "result": "在均匀和非均匀环境中，该方法能快速接近理想动作，验证了软体生长机器人在环境探索和地图构建中的潜力。", "conclusion": "软体生长机器人可作为有效的环境和探索工具，其模型和模拟器为未来应用提供了基础。"}}
{"id": "2507.10571", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10571", "abs": "https://arxiv.org/abs/2507.10571", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning", "comment": null, "summary": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust", "AI": {"tldr": "论文提出了一种模块化的多智能体AI视觉分类框架，通过信任感知的协调和RAG模块，显著提升了零样本场景下的分类准确率。", "motivation": "解决多智能体AI在零样本场景下的信任问题，特别是在视觉和语言理解结合的领域。", "method": "结合通用多模态智能体、非视觉推理协调器和RAG模块，通过置信度校准和图像检索优化信任分配。", "result": "零样本场景下准确率提升77.94%，总体准确率达到85.63%，并通过图像RAG纠正智能体的过度自信。", "conclusion": "该框架将感知与元推理分离，具有可扩展性和可解释性，适用于诊断、生物学等信任关键领域。"}}
{"id": "2507.10582", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2507.10582", "abs": "https://arxiv.org/abs/2507.10582", "authors": ["Anders Ledberg", "Anna Thalén"], "title": "Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis", "comment": null, "summary": "Unstructured text from legal, medical, and administrative sources offers a\nrich but underutilized resource for research in public health and the social\nsciences. However, large-scale analysis is hampered by two key challenges: the\npresence of sensitive, personally identifiable information, and significant\nheterogeneity in structure and language. We present a modular toolchain that\nprepares such text data for embedding-based analysis, relying entirely on\nopen-weight models that run on local hardware, requiring only a\nworkstation-level GPU and supporting privacy-sensitive research.\n  The toolchain employs large language model (LLM) prompting to standardize,\nsummarize, and, when needed, translate texts to English for greater\ncomparability. Anonymization is achieved via LLM-based redaction, supplemented\nwith named entity recognition and rule-based methods to minimize the risk of\ndisclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court\ndecisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.\nEach document is processed into an anonymized, standardized summary and\ntransformed into a document-level embedding. Validation, including manual\nreview, automated scanning, and predictive evaluation shows the toolchain\neffectively removes identifying information while retaining semantic content.\nAs an illustrative application, we train a predictive model using embedding\nvectors derived from a small set of manually labeled summaries, demonstrating\nthe toolchain's capacity for semi-automated content analysis at scale.\n  By enabling structured, privacy-conscious analysis of sensitive documents,\nour toolchain opens new possibilities for large-scale research in domains where\ntextual data was previously inaccessible due to privacy and heterogeneity\nconstraints.", "AI": {"tldr": "论文提出了一种模块化工具链，利用开源模型处理敏感文本数据，解决隐私和异构性问题，适用于大规模研究。", "motivation": "法律、医疗和行政领域的非结构化文本是公共卫生和社会科学研究的重要资源，但敏感信息和数据异构性限制了其利用。", "method": "使用大型语言模型（LLM）进行文本标准化、摘要和翻译，结合命名实体识别和基于规则的方法实现匿名化。", "result": "工具链在瑞典法院判决数据集上验证有效，能够匿名化并保留语义内容，支持半自动化内容分析。", "conclusion": "该工具链为隐私敏感领域的大规模文本分析提供了新可能。"}}
{"id": "2507.10800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10800", "abs": "https://arxiv.org/abs/2507.10800", "authors": ["Ali Hojjat", "Janek Haberer", "Soren Pirk", "Olaf Landsiedel"], "title": "ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference", "comment": "Under Review", "summary": "Vision Transformers deliver state-of-the-art performance, yet their fixed\ncomputational budget prevents scalable deployment across heterogeneous\nhardware. Recent nested Transformer architectures mitigate this by embedding\nnested subnetworks within a single model to enable scalable inference. However,\nthese models allocate the same amount of compute to all inputs, regardless of\ntheir complexity, which leads to inefficiencies. To address this, we introduce\nThinkingViT, a nested ViT architecture that employs progressive thinking stages\nto dynamically adjust inference computation based on input difficulty.\nThinkingViT initiates inference by activating a small subset of the most\nimportant attention heads and terminates early if predictions reach sufficient\ncertainty. Otherwise, it activates additional attention heads and re-evaluates\nthe input. At the core of ThinkingViT is our Token Recycling mechanism, which\nconditions each subsequent inference stage on the embeddings from the previous\nstage, enabling progressive improvement. Due to its backbone-preserving design,\nThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show\nthat ThinkingViT surpasses nested baselines by up to 2.0 percentage points\n(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs\non ImageNet-1K. The source code is available at\nhttps://github.com/ds-kiel/ThinkingViT.", "AI": {"tldr": "ThinkingViT是一种嵌套ViT架构，通过动态调整计算资源以适应输入复杂度，提升效率。", "motivation": "解决现有嵌套Transformer模型对所有输入分配相同计算资源导致的效率低下问题。", "method": "采用渐进式思考阶段和Token Recycling机制，动态激活注意力头并基于前一阶段嵌入优化推理。", "result": "在ImageNet-1K上，相同吞吐量下精度提升2.0 p.p.，相同计算量下提升2.9 p.p.。", "conclusion": "ThinkingViT通过动态计算分配显著提升效率，同时可作为插件升级现有ViT模型。"}}
{"id": "2507.11046", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11046", "abs": "https://arxiv.org/abs/2507.11046", "authors": ["Faryal Aurooj Nasir", "Salman Liaquat", "Nor Muzlifah Mahyuddin"], "title": "Using Continual Learning for Real-Time Detection of Vulnerable Road Users in Complex Traffic Scenarios", "comment": "Accepted for presentation at the 9th International Conference on\n  Communications and Future Internet", "summary": "Pedestrians and bicyclists are among the vulnerable road users (VRUs) that\nare inherently exposed to intricate traffic scenarios, which puts them at\nincreased risk of sustaining injuries or facing fatal outcomes. This study\npresents an intelligent adaptive system that uses the YOLOv8-Dynamic (YOLOv8-D)\nalgorithm that detects vulnerable road users and adapts in real time to prevent\naccidents before they occur. We select YOLOv8x as the detector by comparing it\nwith other state-of-the-art object detection models, including Faster-RCNN,\nYOLOv5, YOLOv7, and variants. Compared to YOLOv5x, YOLOv8x shows improvements\nof 12.14% in F1 score and 45.61% in mean Average Precision (mAP). Against\nYOLOv7x, the improvements are 21.26% in F1 score and 128.44% in mAP. Our\nalgorithm integrates continual learning ability in the architecture of the\nYOLOv8 detector to adjust to evolving road conditions flexibly, ensuring\nadaptability across multiple dataset domains and facilitating continuous\nenhancement of detection and tracking accuracy for VRUs, embracing the dynamic\nnature of real-world environments. In our proposed framework, we optimized the\ngradient descent mechanism of YOLOv8 model and train our optimized algorithm on\ntwo statistically different datasets in terms of image viewpoint and number of\nclasses to achieve a 21.08% improvement in F1 score and a 31.86% improvement in\nmAP as compared to a custom YOLOv8 framework trained on a new dataset, thus\novercoming the issue of catastrophic forgetting, which occurs when deep models\nare trained on statistically different types of datasets.", "AI": {"tldr": "本文提出了一种基于YOLOv8-Dynamic算法的智能自适应系统，用于实时检测和适应道路上的弱势道路使用者（VRUs），以减少事故风险。YOLOv8x在性能上优于其他先进模型，并通过持续学习能力适应动态道路环境。", "motivation": "弱势道路使用者（如行人和骑行者）在复杂交通场景中面临较高风险，需要一种实时检测和预防事故的系统。", "method": "采用YOLOv8-Dynamic算法，结合持续学习能力，优化梯度下降机制，并在不同数据集上训练以提高检测和跟踪准确性。", "result": "YOLOv8x在F1分数和mAP上显著优于YOLOv5x和YOLOv7x，并在新数据集上实现了21.08%的F1分数和31.86%的mAP提升。", "conclusion": "该系统通过持续学习和优化，有效提升了VRU检测的准确性和适应性，解决了深度模型在统计不同数据集上的灾难性遗忘问题。"}}
{"id": "2507.11064", "categories": ["eess.SY", "cs.AI", "cs.SY", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.11064", "abs": "https://arxiv.org/abs/2507.11064", "authors": ["Sehyun Ryu", "Hyun Jong Yang"], "title": "Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems", "comment": null, "summary": "Reducing feedback overhead in beyond 5G networks is a critical challenge, as\nthe growing number of antennas in modern massive MIMO systems substantially\nincreases the channel state information (CSI) feedback demand in frequency\ndivision duplex (FDD) systems. To address this, extensive research has focused\non CSI compression and prediction, with neural network-based approaches gaining\nmomentum and being considered for integration into the 3GPP 5G-Advanced\nstandards. While deep learning has been effectively applied to CSI-limited\nbeamforming and handover optimization, reference signal allocation under such\nconstraints remains surprisingly underexplored. To fill this gap, we introduce\nthe concept of channel prediction-based reference signal allocation (CPRS),\nwhich jointly optimizes channel prediction and DM-RS allocation to improve data\nthroughput without requiring CSI feedback. We further propose a\nstandards-compliant ViViT/CNN-based architecture that implements CPRS by\ntreating evolving CSI matrices as sequential image-like data, enabling\nefficient and adaptive transmission in dynamic environments. Simulation results\nusing ray-tracing channel data generated in NVIDIA Sionna validate the proposed\nmethod, showing up to 36.60% throughput improvement over benchmark strategies.", "AI": {"tldr": "论文提出了一种基于信道预测的参考信号分配方法（CPRS），联合优化信道预测和DM-RS分配，无需CSI反馈即可提升数据吞吐量。", "motivation": "在5G及未来网络中，大规模MIMO系统的天线数量增加导致CSI反馈需求激增，现有研究主要集中在CSI压缩和预测，但参考信号分配在CSI受限条件下尚未充分探索。", "method": "提出CPRS方法，结合ViViT/CNN架构，将CSI矩阵视为序列图像数据，实现动态环境中的高效自适应传输。", "result": "仿真结果显示，相比基准策略，吞吐量提升高达36.60%。", "conclusion": "CPRS方法有效解决了CSI反馈开销问题，为5G-Advanced标准提供了潜在解决方案。"}}
{"id": "2507.10749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10749", "abs": "https://arxiv.org/abs/2507.10749", "authors": ["Benjamin Stoler", "Juliet Yang", "Jonathan Francis", "Jean Oh"], "title": "RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding", "comment": null, "summary": "Safety-critical scenarios are essential for training and evaluating\nautonomous driving (AD) systems, yet remain extremely rare in real-world\ndriving datasets. To address this, we propose Real-world Crash Grounding (RCG),\na scenario generation framework that integrates crash-informed semantics into\nadversarial perturbation pipelines. We construct a safety-aware behavior\nrepresentation through contrastive pre-training on large-scale driving logs,\nfollowed by fine-tuning on a small, crash-rich dataset with approximate\ntrajectory annotations extracted from video. This embedding captures semantic\nstructure aligned with real-world accident behaviors and supports selection of\nadversary trajectories that are both high-risk and behaviorally realistic. We\nincorporate the resulting selection mechanism into two prior scenario\ngeneration pipelines, replacing their handcrafted scoring objectives with an\nembedding-based criterion. Experimental results show that ego agents trained\nagainst these generated scenarios achieve consistently higher downstream\nsuccess rates, with an average improvement of 9.2% across seven evaluation\nsettings. Qualitative and quantitative analyses further demonstrate that our\napproach produces more plausible and nuanced adversary behaviors, enabling more\neffective and realistic stress testing of AD systems. Code and tools will be\nreleased publicly.", "AI": {"tldr": "提出了一种名为RCG的场景生成框架，通过将碰撞语义融入对抗扰动流程，生成更真实的高风险驾驶场景，提升自动驾驶系统的训练效果。", "motivation": "现实驾驶数据中安全关键场景稀缺，难以有效训练和评估自动驾驶系统。", "method": "通过对比预训练构建安全感知行为表征，结合小规模碰撞数据微调，嵌入语义结构以选择高风险且行为真实的对抗轨迹。", "result": "实验显示，使用生成场景训练的自动驾驶系统下游成功率平均提升9.2%，且生成的对抗行为更真实。", "conclusion": "RCG框架能生成更真实的高风险场景，有效提升自动驾驶系统的压力测试效果。"}}
{"id": "2507.10624", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10624", "abs": "https://arxiv.org/abs/2507.10624", "authors": ["Zheng Zhang"], "title": "Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning", "comment": "Substantial change to previous version (experiments, theorem,\n  analysis and related work); currently under review at TMLR", "summary": "Large Language Models (LLMs) display striking surface fluency yet\nsystematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,\nand logical consistency. This paper offers a structural diagnosis of such\nfailures, revealing a persistent gap between \\textit{comprehension} and\n\\textit{competence}. Through controlled experiments and architectural analysis,\nwe demonstrate that LLMs often articulate correct principles without reliably\napplying them--a failure rooted not in knowledge access, but in computational\nexecution. We term this phenomenon the computational \\textit{split-brain\nsyndrome}, where instruction and action pathways are geometrically and\nfunctionally dissociated. This core limitation recurs across domains, from\nmathematical operations to relational inferences, and explains why model\nbehavior remains brittle even under idealized prompting. We argue that LLMs\nfunction as powerful pattern completion engines, but lack the architectural\nscaffolding for principled, compositional reasoning. Our findings delineate the\nboundary of current LLM capabilities and motivate future models with\nmetacognitive control, principle lifting, and structurally grounded execution.\nThis diagnosis also clarifies why mechanistic interpretability findings may\nreflect training-specific pattern coordination rather than universal\ncomputational principles, and why the geometric separation between instruction\nand execution pathways suggests limitations in neural introspection and\nmechanistic analysis.", "AI": {"tldr": "论文指出大语言模型（LLM）在符号推理、算术准确性和逻辑一致性任务上存在系统性失败，揭示了其理解与能力之间的鸿沟，并提出“计算分裂脑综合征”作为解释。", "motivation": "研究LLM在复杂任务中表现不佳的根本原因，揭示其表面流畅性背后的计算局限性。", "method": "通过控制实验和架构分析，研究LLM在任务中的表现，发现其原则表达与实际应用之间的脱节。", "result": "LLM虽能表达正确原则，但无法可靠执行，原因是计算执行路径与指令路径的几何和功能分离。", "conclusion": "LLM缺乏结构化推理能力，未来需改进模型架构，引入元认知控制和原则提升机制。"}}
{"id": "2507.10585", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10585", "abs": "https://arxiv.org/abs/2507.10585", "authors": ["Isar Nejadgholi", "Mona Omidyeganeh", "Marc-Antoine Drouin", "Jonathan Boisvert"], "title": "A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations", "comment": "Presented at the Workshop of Technical AI Governance, 5 pages 2\n  figures", "summary": "Effective AI governance requires structured approaches for stakeholders to\naccess and verify AI system behavior. With the rise of large language models,\nNatural Language Explanations (NLEs) are now key to articulating model\nbehavior, which necessitates a focused examination of their characteristics and\ngovernance implications. We draw on Explainable AI (XAI) literature to create\nan updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:\n(1) Context, including task, data, audience, and goals; (2) Generation and\nPresentation, covering generation methods, inputs, interactivity, outputs, and\nforms; and (3) Evaluation, focusing on content, presentation, and user-centered\nproperties, as well as the setting of the evaluation. This taxonomy provides a\nframework for researchers, auditors, and policymakers to characterize, design,\nand enhance NLEs for transparent AI systems.", "AI": {"tldr": "论文提出了一种更新的XAI分类法，专注于基于提示的自然语言解释（NLEs），以支持透明AI系统的治理。", "motivation": "随着大型语言模型的兴起，NLEs成为解释模型行为的关键工具，需要对其特性和治理影响进行系统研究。", "method": "基于可解释AI（XAI）文献，构建了一个三维度分类法：上下文、生成与呈现、评估。", "result": "分类法为研究者、审计者和政策制定者提供了设计和改进NLEs的框架。", "conclusion": "该分类法有助于提升AI系统的透明度和治理效果。"}}
{"id": "2507.10844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10844", "abs": "https://arxiv.org/abs/2507.10844", "authors": ["Furkan Mumcu", "Michael J. Jones", "Anoop Cherian", "Yasin Yilmaz"], "title": "LLM-Guided Agentic Object Detection for Open-World Understanding", "comment": null, "summary": "Object detection traditionally relies on fixed category sets, requiring\ncostly re-training to handle novel objects. While Open-World and\nOpen-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD\nlacks semantic labels for unknowns, and OVOD depends on user prompts, limiting\nautonomy. We propose an LLM-guided agentic object detection (LAOD) framework\nthat enables fully label-free, zero-shot detection by prompting a Large\nLanguage Model (LLM) to generate scene-specific object names. These are passed\nto an open-vocabulary detector for localization, allowing the system to adapt\nits goals dynamically. We introduce two new metrics, Class-Agnostic Average\nPrecision (CAAP) and Semantic Naming Average Precision (SNAP), to separately\nevaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD\nvalidate our approach, showing strong performance in detecting and naming novel\nobjects. Our method offers enhanced autonomy and adaptability for open-world\nunderstanding.", "AI": {"tldr": "提出了一种基于大语言模型（LLM）的自主目标检测框架（LAOD），实现无需标签的零样本检测，通过动态生成场景特定对象名称并评估其定位和命名性能。", "motivation": "传统目标检测依赖固定类别集，灵活性不足；现有开放世界和开放词汇检测方法存在语义标签缺失或依赖用户提示的问题。", "method": "利用LLM生成场景特定对象名称，结合开放词汇检测器进行定位，引入CAAP和SNAP指标分别评估定位和命名。", "result": "在LVIS、COCO和COCO-OOD数据集上验证了方法的有效性，展示了检测和命名新对象的强大性能。", "conclusion": "LAOD框架提升了开放世界理解的自主性和适应性。"}}
{"id": "2507.11415", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11415", "abs": "https://arxiv.org/abs/2507.11415", "authors": ["Hongbo Ye", "Fenghe Tang", "Peiang Zhao", "Zhen Huang", "Dexin Zhao", "Minghao Bian", "S. Kevin Zhou"], "title": "U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV", "comment": "Accepted by MICCAI2025", "summary": "Achieving equity in healthcare accessibility requires lightweight yet\nhigh-performance solutions for medical image segmentation, particularly in\nresource-limited settings. Existing methods like U-Net and its variants often\nsuffer from limited global Effective Receptive Fields (ERFs), hindering their\nability to capture long-range dependencies. To address this, we propose U-RWKV,\na novel framework leveraging the Recurrent Weighted Key-Value(RWKV)\narchitecture, which achieves efficient long-range modeling at O(N)\ncomputational cost. The framework introduces two key innovations: the\nDirection-Adaptive RWKV Module(DARM) and the Stage-Adaptive\nSqueeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan\nmechanisms to aggregate contextual cues across images, mitigating directional\nbias while preserving global context and maintaining high computational\nefficiency. SASE dynamically adapts its architecture to different feature\nextraction stages, balancing high-resolution detail preservation and semantic\nrelationship capture. Experiments demonstrate that U-RWKV achieves\nstate-of-the-art segmentation performance with high computational efficiency,\noffering a practical solution for democratizing advanced medical imaging\ntechnologies in resource-constrained environments. The code is available at\nhttps://github.com/hbyecoding/U-RWKV.", "AI": {"tldr": "U-RWKV是一种基于RWKV架构的新型医疗图像分割框架，通过DARM和SASE模块高效捕获长距离依赖，计算成本低，适用于资源有限环境。", "motivation": "解决现有方法（如U-Net）在全局有效感受野（ERF）上的局限性，提升医疗图像分割的效率和性能。", "method": "提出U-RWKV框架，结合DARM（方向自适应RWKV模块）和SASE（阶段自适应挤压-激励模块），实现高效长距离建模。", "result": "实验表明U-RWKV在计算效率和分割性能上达到最先进水平。", "conclusion": "U-RWKV为资源受限环境提供了一种高效的医疗图像分割解决方案。"}}
{"id": "2507.11113", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11113", "abs": "https://arxiv.org/abs/2507.11113", "authors": ["Yueyue Xu", "Yuewei Chen", "Lin Wang", "Zhaoyang Cheng", "Xiaoming Hu"], "title": "Optimal Honeypot Ratio and Convergent Fictitious-Play Learning in Signaling Games for CPS Defense", "comment": "14 pages, 8 figures", "summary": "Cyber-Physical Systems (CPSs) are facing a fast-growing wave of attacks. To\nachieve effective proactive defense, this paper models honeypot deployment as a\ngamma-fixed signaling game in which node liveness serves as the only signal and\nnormal-node signal gamma is exogenously fixed. We define the gamma-perfect\nBayesian-Nash equilibrium (gamma-PBNE). Analytical expressions are obtained for\nall gamma-PBNEs, revealing three distinct equilibrium regimes that depend on\nthe priori honeypot ratio. Furthermore, the optimal honeypot ratio and\nsignaling strategy that jointly maximize the network average utility are\nobtained. To capture strategic interaction over time, we develop a\ndiscrete-time fictitious-play algorithm that couples Bayesian belief updates\nwith empirical best responses. We prove that, as long as the honeypot ratio is\nperturbed within a non-degenerate neighbourhood of the optimum, every\nfictitious-play path converges to the defender-optimal gamma-PBNE. Numerical\nresults confirm the effectiveness of the proposed method and demonstrate its\napplicability to CPS defense.", "AI": {"tldr": "本文提出了一种基于γ固定信号博弈的蜜罐部署模型，定义了γ-完美贝叶斯-纳什均衡，并分析了三种均衡状态。通过离散时间虚拟博弈算法，证明了其收敛性，数值结果验证了方法的有效性。", "motivation": "随着网络物理系统（CPS）面临日益增长的攻击威胁，需要一种有效的主动防御方法。蜜罐部署作为一种防御手段，其信号博弈模型尚未充分研究。", "method": "将蜜罐部署建模为γ固定信号博弈，节点活跃度作为唯一信号。定义了γ-完美贝叶斯-纳什均衡，并通过离散时间虚拟博弈算法结合贝叶斯信念更新与经验最佳响应。", "result": "获得了所有γ-PBNE的解析表达式，揭示了三种均衡状态。证明了虚拟博弈路径收敛于防御最优均衡，数值结果验证了方法的有效性。", "conclusion": "提出的方法在网络平均效用最大化方面表现优异，适用于CPS防御，为主动防御提供了理论支持。"}}
{"id": "2507.10776", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10776", "abs": "https://arxiv.org/abs/2507.10776", "authors": ["Howard H. Qian", "Yiting Chen", "Gaotian Wang", "Podshara Chanrungmaneekul", "Kaiyu Hang"], "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding", "comment": "8 pages, IROS 2025, Interactive Perception, Segmentation, Robotics,\n  Computer Vision", "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.", "AI": {"tldr": "提出了一种实时交互感知框架rt-RISeg，通过机器人交互和设计的体帧不变特征（BFIF）实现未见物体的连续分割，无需依赖学习模型，显著提升了分割精度。", "motivation": "现有未见物体实例分割（UOIS）方法依赖大规模数据集训练，容易过拟合静态视觉特征，导致泛化能力不足。", "method": "基于视觉交互性原理，设计实时交互感知框架rt-RISeg，利用机器人交互产生的体帧不变特征（BFIF）动态分割物体。", "result": "rt-RISeg的平均分割准确率比现有UOIS方法高27.5%，且生成的分割掩码可作为视觉基础模型的提示进一步提升性能。", "conclusion": "rt-RISeg通过交互感知和动态特征分析，显著提升了未见物体分割的泛化能力和实时性。"}}
{"id": "2507.10630", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10630", "abs": "https://arxiv.org/abs/2507.10630", "authors": ["Ye Yang", "Xue Xiao", "Ping Yin", "Taotao Xie"], "title": "Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs", "comment": null, "summary": "API calls by large language models (LLMs) offer a cutting-edge approach for\ndata analysis. However, their ability to effectively utilize tools via API\ncalls remains underexplored in knowledge-intensive domains like meteorology.\nThis paper introduces KG2data, a system that integrates knowledge graphs, LLMs,\nReAct agents, and tool-use technologies to enable intelligent data acquisition\nand query handling in the meteorological field. Using a virtual API, we\nevaluate API call accuracy across three metrics: name recognition failure,\nhallucination failure, and call correctness. KG2data achieves superior\nperformance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and\nchat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based\nsystems by addressing their limited access to domain-specific knowledge, which\nhampers performance on complex or terminology-rich queries. By using a\nknowledge graph as persistent memory, our system enhances content retrieval,\ncomplex query handling, domain-specific reasoning, semantic relationship\nresolution, and heterogeneous data integration. It also mitigates the high cost\nof fine-tuning LLMs, making the system more adaptable to evolving domain\nknowledge and API structures. In summary, KG2data provides a novel solution for\nintelligent, knowledge-based question answering and data analysis in domains\nwith high knowledge demands.", "AI": {"tldr": "KG2data结合知识图谱、LLMs、ReAct代理和工具使用技术，提升气象领域数据获取和查询的智能性，性能优于RAG2data和chat2data。", "motivation": "现有LLMs在知识密集型领域（如气象学）中通过API调用有效利用工具的能力尚未充分探索，且缺乏领域特定知识。", "method": "KG2data整合知识图谱、LLMs、ReAct代理和工具使用技术，通过虚拟API评估API调用准确性。", "result": "KG2data在名称识别失败（1.43%）、幻觉失败（0%）和调用正确性（88.57%）上表现优于对比系统。", "conclusion": "KG2data为高知识需求领域提供了智能问答和数据分析的新解决方案，解决了LLMs领域知识不足的问题。"}}
{"id": "2507.10586", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10586", "abs": "https://arxiv.org/abs/2507.10586", "authors": ["Kaushik Dwivedi", "Padmanabh Patanjali Mishra"], "title": "AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable fluency across a\nrange of natural language tasks, yet remain vulnerable to hallucinations -\nfactual inaccuracies that undermine trust in real world deployment. We present\nAutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that\ntackles hallucination in large language models through lightweight LoRA-based\nadapters and KL-regularized training. Our pipeline integrates automated prompt\nrewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in\nretrieved evidence. A hallucination detection module, using both\nclassifier-based and self-evaluation techniques, assigns confidence scores to\ngenerated outputs, triggering an optional feedback correction loop. This loop\nenforces factual alignment via contrastive KL loss and adapter fine tuning. We\ndemonstrate that AutoRAG-LoRA significantly reduces the factual drift while\npreserving the efficiency and modularity of the model.", "AI": {"tldr": "AutoRAG-LoRA是一个模块化框架，通过LoRA适配器和KL正则化训练减少大语言模型的幻觉问题，同时保持高效性和模块化。", "motivation": "大语言模型在自然语言任务中表现出色，但存在幻觉问题（事实错误），影响实际部署的信任度。", "method": "结合自动提示重写、混合检索和低秩适配器调优，通过KL正则化训练和幻觉检测模块（分类器和自评估技术）生成置信度分数，并可选反馈校正循环。", "result": "AutoRAG-LoRA显著减少了事实漂移，同时保持了模型的高效性和模块化。", "conclusion": "该框架有效解决了大语言模型的幻觉问题，提升了生成内容的可信度。"}}
{"id": "2507.10846", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10846", "abs": "https://arxiv.org/abs/2507.10846", "authors": ["Casey Wall", "Longwei Wang", "Rodrigue Rizk", "KC Santosh"], "title": "Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization", "comment": "15 pages, 10 figures, 7 tables. Submitted to IEEE Transactions on\n  Pattern Analysis and Machine Intelligence", "summary": "Interpreting the decision-making process of Convolutional Neural Networks\n(CNNs) is critical for deploying models in high-stakes domains.\nGradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method\nfor visual explanations, yet it typically focuses on the final convolutional\nlayer or na\\\"ively averages across layers, strategies that can obscure\nimportant semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a\nnovel, human-tunable extension of Grad-CAM that generates robust and coherent\nsaliency maps by aggregating information across all convolutional layers. To\nmitigate the influence of noisy or extreme attribution values, Winsor-CAM\napplies Winsorization, a percentile-based outlier attenuation technique. A\nuser-controllable threshold allows for semantic-level tuning, enabling flexible\nexploration of model behavior across representational hierarchies. Evaluations\non standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the\nPASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable\nheatmaps and achieves superior performance in localization metrics, including\nintersection-over-union and center-of-mass alignment, when compared to Grad-CAM\nand uniform layer-averaging baselines. Winsor-CAM advances the goal of\ntrustworthy AI by offering interpretable, multi-layer insights with\nhuman-in-the-loop control.", "AI": {"tldr": "Winsor-CAM是一种改进的Grad-CAM方法，通过跨卷积层聚合信息并使用Winsorization技术生成更鲁棒和可解释的显著性图。", "motivation": "解释卷积神经网络（CNN）的决策过程在高风险领域至关重要，但现有方法如Grad-CAM可能掩盖重要语义或放大噪声。", "method": "提出Winsor-CAM，通过Winsorization技术衰减异常值，并允许用户通过阈值调整语义层次。", "result": "在PASCAL VOC 2012数据集上，Winsor-CAM生成的显著性图更易解释，且在定位指标上优于Grad-CAM。", "conclusion": "Winsor-CAM通过多层解释和用户控制，推动了可信AI的发展。"}}
{"id": "2507.11523", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11523", "abs": "https://arxiv.org/abs/2507.11523", "authors": ["Buddhi Wijenayake", "Athulya Ratnayake", "Praveen Sumanasekara", "Nichula Wasalathilaka", "Mathivathanan Piratheepan", "Roshan Godaliyadda", "Mervyn Ekanayake", "Vijitha Herath"], "title": "Precision Spatio-Temporal Feature Fusion for Robust Remote Sensing Change Detection", "comment": "6 pages, 4 figures, 2 pages, under review(conference paper)", "summary": "Remote sensing change detection is vital for monitoring environmental and\nurban transformations but faces challenges like manual feature extraction and\nsensitivity to noise. Traditional methods and early deep learning models, such\nas convolutional neural networks (CNNs), struggle to capture long-range\ndependencies and global context essential for accurate change detection in\ncomplex scenes. While Transformer-based models mitigate these issues, their\ncomputational complexity limits their applicability in high-resolution remote\nsensing. Building upon ChangeMamba architecture, which leverages state space\nmodels for efficient global context modeling, this paper proposes precision\nfusion blocks to capture channel-wise temporal variations and per-pixel\ndifferences for fine-grained change detection. An enhanced decoder pipeline,\nincorporating lightweight channel reduction mechanisms, preserves local details\nwith minimal computational cost. Additionally, an optimized loss function\ncombining Cross Entropy, Dice and Lovasz objectives addresses class imbalance\nand boosts Intersection-over-Union (IoU). Evaluations on SYSU-CD, LEVIR-CD+,\nand WHU-CD datasets demonstrate superior precision, recall, F1 score, IoU, and\noverall accuracy compared to state-of-the-art methods, highlighting the\napproach's robustness for remote sensing change detection. For complete\ntransparency, the codes and pretrained models are accessible at\nhttps://github.com/Buddhi19/MambaCD.git", "AI": {"tldr": "提出了一种基于ChangeMamba架构的改进方法，通过精度融合块和增强的解码器管道，实现高效的遥感变化检测。", "motivation": "传统方法和早期深度学习模型在捕捉复杂场景中的长距离依赖和全局上下文方面存在不足，Transformer模型计算复杂度高。", "method": "采用ChangeMamba架构，引入精度融合块捕捉通道级时间变化和像素级差异，优化解码器管道和损失函数。", "result": "在多个数据集上表现出更高的精确度、召回率、F1分数、IoU和整体准确性。", "conclusion": "该方法在遥感变化检测中表现出高效性和鲁棒性，代码和预训练模型已开源。"}}
{"id": "2507.11240", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11240", "abs": "https://arxiv.org/abs/2507.11240", "authors": ["Mohamad Al Ahdab", "John Leth", "Zheng-Hua Tan"], "title": "Optimal Sensor Scheduling and Selection for Continuous-Discrete Kalman Filtering with Auxiliary Dynamics", "comment": "Accepted to ICML 2025", "summary": "We study the Continuous-Discrete Kalman Filter (CD-KF) for State-Space Models\n(SSMs) where continuous-time dynamics are observed via multiple sensors with\ndiscrete, irregularly timed measurements. Our focus extends to scenarios in\nwhich the measurement process is coupled with the states of an auxiliary SSM.\nFor instance, higher measurement rates may increase energy consumption or heat\ngeneration, while a sensor's accuracy can depend on its own spatial trajectory\nor that of the measured target. Each sensor thus carries distinct costs and\nconstraints associated with its measurement rate and additional constraints and\ncosts on the auxiliary state. We model measurement occurrences as independent\nPoisson processes with sensor-specific rates and derive an upper bound on the\nmean posterior covariance matrix of the CD-KF along the mean auxiliary state.\nThe bound is continuously differentiable with respect to the measurement rates,\nwhich enables efficient gradient-based optimization. Exploiting this bound, we\npropose a finite-horizon optimal control framework to optimize measurement\nrates and auxiliary-state dynamics jointly. We further introduce a\ndeterministic method for scheduling measurement times from the optimized rates.\nEmpirical results in state-space filtering and dynamic temporal Gaussian\nprocess regression demonstrate that our approach achieves improved trade-offs\nbetween resource usage and estimation accuracy.", "AI": {"tldr": "研究了连续-离散卡尔曼滤波（CD-KF）在多传感器、不规则时间测量下的状态空间模型（SSM），提出了一种优化测量率和辅助状态动态的框架。", "motivation": "解决多传感器测量中因测量率、能量消耗和精度耦合带来的资源与估计精度权衡问题。", "method": "建模测量事件为独立泊松过程，推导后验协方差矩阵上界，并基于梯度优化测量率和辅助状态动态。", "result": "提出了一种确定性测量时间调度方法，实验显示在资源使用和估计精度间取得更好平衡。", "conclusion": "通过优化框架，实现了资源消耗与估计精度的有效权衡，适用于状态空间滤波和动态时间高斯过程回归。"}}
{"id": "2507.10814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10814", "abs": "https://arxiv.org/abs/2507.10814", "authors": ["Huiyi Wang", "Fahim Shahriar", "Alireza Azimi", "Gautham Vasan", "Rupam Mahmood", "Colin Bellinger"], "title": "Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection", "comment": "8 pages, 4 figures, 3 tables", "summary": "General-purpose robotic manipulation, including reach and grasp, is essential\nfor deployment into households and workspaces involving diverse and evolving\ntasks. Recent advances propose using large pre-trained models, such as Large\nLanguage Models and object detectors, to boost robotic perception in\nreinforcement learning. These models, trained on large datasets via\nself-supervised learning, can process text prompts and identify diverse objects\nin scenes, an invaluable skill in RL where learning object interaction is\nresource-intensive. This study demonstrates how to integrate such models into\nGoal-Conditioned Reinforcement Learning to enable general and versatile robotic\nreach and grasp capabilities. We use a pre-trained object detection model to\nenable the agent to identify the object from a text prompt and generate a mask\nfor goal conditioning. Mask-based goal conditioning provides object-agnostic\ncues, improving feature sharing and generalization. The effectiveness of the\nproposed framework is demonstrated in a simulated reach-and-grasp task, where\nthe mask-based goal conditioning consistently maintains a $\\sim$90\\% success\nrate in grasping both in and out-of-distribution objects, while also ensuring\nfaster convergence to higher returns.", "AI": {"tldr": "该研究提出了一种将预训练模型（如大型语言模型和物体检测器）整合到目标条件强化学习中，以实现通用机器人抓取能力的方法。通过基于掩码的目标条件，提高了特征共享和泛化能力，实验显示在模拟任务中成功率约90%。", "motivation": "通用机器人操作（如抓取）在家庭和工作空间中具有广泛应用，但传统方法学习对象交互成本高昂。利用预训练模型可以提升感知能力，降低学习成本。", "method": "使用预训练的物体检测模型，通过文本提示识别对象并生成掩码，用于目标条件强化学习。掩码提供对象无关的线索，促进特征共享和泛化。", "result": "在模拟抓取任务中，基于掩码的目标条件方法成功率约90%，且收敛更快，回报更高。", "conclusion": "该方法通过整合预训练模型和目标条件强化学习，显著提升了机器人抓取的通用性和效率。"}}
{"id": "2507.10644", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10644", "abs": "https://arxiv.org/abs/2507.10644", "authors": ["Tatiana Petrova", "Aleksandr Puzikov", "Boris Bliznukov", "Radu State"], "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.", "AI": {"tldr": "该论文首次全面概述了Web of Agents（WoA）的演变，揭示了现代协议与早期标准的联系，并提出了一种四轴分类法。", "motivation": "研究WoA的分散性，整合多代理系统和语义Web的历史，以提供对该领域轨迹的整体理解。", "method": "引入四轴分类法（语义基础、通信范式、智能中心、发现机制）来系统比较不同世代的代理架构。", "result": "揭示了智能中心的范式转变，从外部数据或平台转移到代理核心模型（LLM），为现代Agentic AI奠定了基础。", "conclusion": "新协议虽重要，但不足以构建稳健、开放、可信的生态系统；未来研究应解决去中心化身份、经济模型、安全和治理等挑战。"}}
{"id": "2507.10587", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10587", "abs": "https://arxiv.org/abs/2507.10587", "authors": ["Dennis Ulmer", "Alexandra Lorson", "Ivan Titov", "Christian Hardmeier"], "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing", "comment": null, "summary": "Human users increasingly rely on natural language interactions with large\nlanguage models (LLMs) in order to receive help on a large variety of tasks and\nproblems. However, the trustworthiness and perceived legitimacy of LLMs is\nundermined by the fact that their output is frequently stated in very confident\nterms, even when its accuracy is questionable. Therefore, there is a need to\nsignal the confidence of the language model to a user in order to reap the\nbenefits of human-machine collaboration and mitigate potential harms.\nVerbalized uncertainty is the expression of confidence with linguistic means,\nan approach that integrates perfectly into language-based interfaces.\nNevertheless, most recent research in natural language processing (NLP)\noverlooks the nuances surrounding human uncertainty communication and the data\nbiases that influence machine uncertainty communication. We argue for\nanthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty\ncommunication requires a degree of linguistic authenticity and personalization\nto the user, which could be achieved by emulating human communication. We\npresent a thorough overview over the research in human uncertainty\ncommunication, survey ongoing research, and perform additional analyses to\ndemonstrate so-far overlooked biases in verbalized uncertainty. We conclude by\npointing out unique factors in human-machine communication of uncertainty and\ndeconstruct anthropomimetic uncertainty into future research directions for\nNLP.", "AI": {"tldr": "论文探讨了如何通过语言模型表达不确定性以增强用户信任，提出了一种模仿人类沟通的“拟人化不确定性”方法，并分析了现有研究的局限性和未来方向。", "motivation": "由于语言模型常以过度自信的方式输出结果，即使准确性存疑，这削弱了其可信度。因此，需要通过语言表达模型的不确定性，以促进人机协作并减少潜在危害。", "method": "提出“拟人化不确定性”概念，强调通过模仿人类沟通方式实现直观且可信的不确定性表达。综述了人类不确定性沟通的研究，并分析了数据偏见。", "result": "研究发现现有自然语言处理研究忽视了人类不确定性沟通的细微差别和数据偏见，需进一步探索拟人化方法。", "conclusion": "论文指出了人机不确定性沟通的独特因素，并将拟人化不确定性分解为未来自然语言处理的研究方向。"}}
{"id": "2507.10855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10855", "abs": "https://arxiv.org/abs/2507.10855", "authors": ["Wei Chen", "Jingxi Yu", "Zichen Miao", "Qiang Qiu"], "title": "Sparse Fine-Tuning of Transformers for Generative Tasks", "comment": "Accepted by International Conference on Computer Vision 2025", "summary": "Large pre-trained transformers have revolutionized artificial intelligence\nacross various domains, and fine-tuning remains the dominant approach for\nadapting these models to downstream tasks due to the cost of training from\nscratch. However, in existing fine-tuning methods, the updated representations\nare formed as a dense combination of modified parameters, making it challenging\nto interpret their contributions and understand how the model adapts to new\ntasks. In this work, we introduce a fine-tuning framework inspired by sparse\ncoding, where fine-tuned features are represented as a sparse combination of\nbasic elements, i.e., feature dictionary atoms. The feature dictionary atoms\nfunction as fundamental building blocks of the representation, and tuning atoms\nallows for seamless adaptation to downstream tasks. Sparse coefficients then\nserve as indicators of atom importance, identifying the contribution of each\natom to the updated representation. Leveraging the atom selection capability of\nsparse coefficients, we first demonstrate that our method enhances image\nediting performance by improving text alignment through the removal of\nunimportant feature dictionary atoms. Additionally, we validate the\neffectiveness of our approach in the text-to-image concept customization task,\nwhere our method efficiently constructs the target concept using a sparse\ncombination of feature dictionary atoms, outperforming various baseline\nfine-tuning methods.", "AI": {"tldr": "提出了一种基于稀疏编码的微调框架，通过稀疏组合特征字典原子来适应下游任务，提高了可解释性和性能。", "motivation": "现有微调方法难以解释模型如何适应新任务，因此需要一种更透明的方法。", "method": "采用稀疏编码框架，将微调特征表示为特征字典原子的稀疏组合，稀疏系数指示原子重要性。", "result": "在图像编辑和文本到图像概念定制任务中表现优于基线方法。", "conclusion": "稀疏编码微调框架提高了模型的可解释性和适应性，适用于多种任务。"}}
{"id": "2507.11252", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11252", "abs": "https://arxiv.org/abs/2507.11252", "authors": ["Guanghao Wu", "Chen Xu", "Hai Song", "Chong Wang", "Qixing Zhang"], "title": "MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection", "comment": "18 pages, 11 figures", "summary": "Smoke is the first visible indicator of a wildfire.With the advancement of\ndeep learning, image-based smoke detection has become a crucial method for\ndetecting and preventing forest fires. However, the scarcity of smoke image\ndata from forest fires is one of the significant factors hindering the\ndetection of forest fire smoke. Image generation models offer a promising\nsolution for synthesizing realistic smoke images. However, current inpainting\nmodels exhibit limitations in generating high-quality smoke representations,\nparticularly manifesting as inconsistencies between synthesized smoke and\nbackground contexts. To solve these problems, we proposed a comprehensive\nframework for generating forest fire smoke images. Firstly, we employed the\npre-trained segmentation model and the multimodal model to obtain smoke masks\nand image captions.Then, to address the insufficient utilization of masks and\nmasked images by inpainting models, we introduced a network architecture guided\nby mask and masked image features. We also proposed a new loss function, the\nmask random difference loss, which enhances the consistency of the generated\neffects around the mask by randomly expanding and eroding the mask\nedges.Finally, to generate a smoke image dataset using random masks for\nsubsequent detection tasks, we incorporated smoke characteristics and use a\nmultimodal large language model as a filtering tool to select diverse and\nreasonable smoke images, thereby improving the quality of the synthetic\ndataset. Experiments showed that our generated smoke images are realistic and\ndiverse, and effectively enhance the performance of forest fire smoke detection\nmodels. Code is available at https://github.com/wghr123/MFGDiffusion.", "AI": {"tldr": "提出了一种生成森林火灾烟雾图像的框架，通过预训练模型获取烟雾掩码和图像描述，改进修复模型，并引入新的损失函数和过滤工具，生成高质量烟雾图像以提升检测性能。", "motivation": "解决现有修复模型在生成高质量烟雾图像时的不一致性问题，以及烟雾图像数据稀缺对森林火灾检测的阻碍。", "method": "使用预训练分割模型和多模态模型获取烟雾掩码和图像描述；提出基于掩码和掩码图像特征的网络架构；引入掩码随机差异损失函数；利用多模态大语言模型筛选合成图像。", "result": "生成的烟雾图像逼真且多样，有效提升了森林火灾烟雾检测模型的性能。", "conclusion": "提出的框架成功解决了烟雾图像生成中的不一致性问题，为森林火灾检测提供了高质量数据集。"}}
{"id": "2507.11377", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11377", "abs": "https://arxiv.org/abs/2507.11377", "authors": ["Philipp Wiesner", "Odej Kao"], "title": "Moving Beyond Marginal Carbon Intensity: A Poor Metric for Both Carbon Accounting and Grid Flexibility", "comment": "Presented at the Workshop on Measurements, Modeling, and Metrics for\n  Carbon-Aware Computing (CarbonMetrics) @ ACM SIGMETRICS '25", "summary": "Marginal Carbon Intensity (MCI) has been promoted as an effective metric for\ncarbon-aware computing. Although it is already considered as impractical for\ncarbon accounting purposes, many still view it as valuable when optimizing for\ngrid flexibility by incentivizing electricity usage during curtailment periods.\nIn this statement paper, we argue that MCI is neither reliable nor actionable\nfor either purpose. We outline its fundamental limitations, including\nnon-observability, reliance on opaque predictive models, and the lack of\nverifiability. Moreover, MCI fails to reflect curtailment caused by high-carbon\nsources and offers no insight into the quantity of available excess power. We\nadvocate moving beyond MCI and instead call for research on more actionable\nmetrics, such as direct reporting of excess power, explicit modeling of energy\nstorage and grid stability, and integration with emerging granular renewable\nenergy certificate markets.", "AI": {"tldr": "本文认为边际碳强度（MCI）既不可靠也不实用，无法用于碳核算或电网灵活性优化，并呼吁研究更实用的指标。", "motivation": "MCI被广泛用于碳感知计算和电网灵活性优化，但其实际效果受到质疑。本文旨在揭示MCI的根本局限性。", "method": "通过分析MCI的非可观测性、依赖不透明预测模型和缺乏可验证性等局限性，提出其不可靠性。", "result": "MCI无法反映高碳源导致的削减，也无法提供关于过剩电力量的信息。", "conclusion": "建议放弃MCI，转而研究更实用的指标，如直接报告过剩电力、明确建模储能和电网稳定性，并与新兴的可再生能源证书市场结合。"}}
{"id": "2507.10878", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10878", "abs": "https://arxiv.org/abs/2507.10878", "authors": ["Savva Morozov", "Tobia Marcucci", "Bernhard Paus Graesdal", "Alexandre Amice", "Pablo A. Parrilo", "Russ Tedrake"], "title": "Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets", "comment": "10 pages", "summary": "We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A\nGCS is a graph where each vertex is paired with a convex program, and each edge\ncouples adjacent programs via additional costs and constraints. A walk in a GCS\nis a sequence of vertices connected by edges, where vertices may be repeated.\nThe length of a walk is given by the cumulative optimal value of the\ncorresponding convex programs. To solve the SWP in GCS, we first synthesize a\npiecewise-quadratic lower bound on the problem's cost-to-go function using\nsemidefinite programming. Then we use this lower bound to guide an\nincremental-search algorithm that yields an approximate shortest walk. We show\nthat the SWP in GCS is a natural language for many mixed discrete-continuous\nplanning problems in robotics, unifying problems that typically require\nspecialized solutions while delivering high performance and computational\nefficiency. We demonstrate this through experiments in collision-free motion\nplanning, skill chaining, and optimal control of hybrid systems.", "AI": {"tldr": "论文研究了凸集图（GCS）中的最短路径问题（SWP），提出了一种通过半定规划合成成本函数的二次下界，并结合增量搜索算法求解的方法。该方法在机器人混合离散-连续规划问题中表现高效。", "motivation": "凸集图（GCS）为混合离散-连续规划问题提供了一种统一的建模语言，但现有方法通常需要专门解决方案。研究旨在提出一种通用且高效的方法来解决GCS中的最短路径问题。", "method": "通过半定规划合成成本函数的二次下界，并利用该下界指导增量搜索算法，近似求解最短路径。", "result": "实验表明，该方法在碰撞自由运动规划、技能链和混合系统最优控制等问题中表现高效。", "conclusion": "GCS中的SWP为混合离散-连续规划问题提供了统一的解决方案，兼具高性能和计算效率。"}}
{"id": "2507.10740", "categories": ["cs.AI", "cs.NE", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10740", "abs": "https://arxiv.org/abs/2507.10740", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "title": "Parsing Musical Structure to Enable Meaningful Variations", "comment": null, "summary": "This paper presents a novel rule-based approach for generating music by\nvarying existing tunes. We parse each tune to find the Pathway Assembly (PA) [\n1], that is a structure representing all repetitions in the tune. The Sequitur\nalgorithm [2 ] is used for this. The result is a grammar. We then carry out\nmutation on the grammar, rather than on a tune directly. There are potentially\n19 types of mutations such as adding, removing, swapping or reversing parts of\nthe grammar that can be applied to the grammars. The system employs one of the\nmutations randomly in this step to automatically manipulate the grammar.\nFollowing the mutation, we need to expand the grammar which returns a new tune.\nThe output after 1 or more mutations will be a new tune related to the original\ntune. Our study examines how tunes change gradually over the course of multiple\nmutations. Edit distances, structural complexity and length of the tunes are\nused to show how a tune is changed after multiple mutations. In addition, the\nsize of effect of each mutation type is analyzed. As a final point, we review\nthe musical aspect of the output tunes. It should be noted that the study only\nfocused on generating new pitch sequences. The study is based on an Irish\ntraditional tune dataset and a list of integers has been used to represent each\ntune's pitch values.", "AI": {"tldr": "提出了一种基于规则的、通过变异现有曲调生成音乐的新方法，使用Sequitur算法解析曲调并构建语法，再对语法进行随机变异以生成新曲调。", "motivation": "探索如何通过变异现有曲调的语法结构来生成新的音乐，并分析变异对曲调的影响。", "method": "1. 使用Sequitur算法解析曲调，构建Pathway Assembly（PA）结构；2. 对语法进行随机变异（19种类型）；3. 扩展语法生成新曲调。", "result": "通过多次变异生成新曲调，分析变异对曲调编辑距离、结构复杂度和长度的影响，并评估每种变异类型的效果。", "conclusion": "该方法能够逐步改变曲调，生成与原始曲调相关的新音乐，但仅关注音高序列的生成。"}}
{"id": "2507.10596", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10596", "abs": "https://arxiv.org/abs/2507.10596", "authors": ["Yogachandran Rahulamathavan", "Misbah Farooq", "Varuna De Silva"], "title": "PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification", "comment": null, "summary": "Large Language Models (LLMs) excel in text classification, but their\ncomplexity hinders interpretability, making it difficult to understand the\nreasoning behind their predictions. Explainable AI (XAI) methods like LIME and\nSHAP offer local explanations by identifying influential words, but they rely\non computationally expensive perturbations. These methods typically generate\nthousands of perturbed sentences and perform inferences on each, incurring a\nsubstantial computational burden, especially with LLMs. To address this, we\npropose \\underline{P}erturbation-free \\underline{L}ocal \\underline{Ex}planation\n(PLEX), a novel method that leverages the contextual embeddings extracted from\nthe LLM and a ``Siamese network\" style neural network trained to align with\nfeature importance scores. This one-off training eliminates the need for\nsubsequent perturbations, enabling efficient explanations for any new sentence.\nWe demonstrate PLEX's effectiveness on four different classification tasks\n(sentiment, fake news, fake COVID-19 news and depression), showing more than\n92\\% agreement with LIME and SHAP. Our evaluation using a ``stress test\"\nreveals that PLEX accurately identifies influential words, leading to a similar\ndecline in classification accuracy as observed with LIME and SHAP when these\nwords are removed. Notably, in some cases, PLEX demonstrates superior\nperformance in capturing the impact of key features. PLEX dramatically\naccelerates explanation, reducing time and computational overhead by two and\nfour orders of magnitude, respectively. This work offers a promising solution\nfor explainable LLM-based text classification.", "AI": {"tldr": "PLEX是一种无需扰动的局部解释方法，通过上下文嵌入和Siamese网络高效生成解释，显著减少计算开销。", "motivation": "大型语言模型（LLM）在文本分类中表现优异，但其复杂性导致可解释性差，现有方法（如LIME和SHAP）计算成本高。", "method": "提出PLEX方法，利用LLM的上下文嵌入和Siamese网络，无需后续扰动即可生成解释。", "result": "在四个分类任务中，PLEX与LIME和SHAP的一致性超过92%，计算效率提升显著。", "conclusion": "PLEX为LLM的文本分类提供了一种高效且可解释的解决方案。"}}
{"id": "2507.10864", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10864", "abs": "https://arxiv.org/abs/2507.10864", "authors": ["Saadat Behzadi", "Danial Sharifrazi", "Bita Mesbahzadeh", "Javad Hassannataj Joloudarid", "Roohallah Alizadehsani"], "title": "A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n", "comment": null, "summary": "Objectives: Timely and accurate detection of colorectal polyps plays a\ncrucial role in diagnosing and preventing colorectal cancer, a major cause of\nmortality worldwide. This study introduces a new, lightweight, and efficient\nframework for polyp detection that combines the Local Outlier Factor (LOF)\nalgorithm for filtering noisy data with the YOLO-v11n deep learning model.\n  Study design: An experimental study leveraging deep learning and outlier\nremoval techniques across multiple public datasets.\n  Methods: The proposed approach was tested on five diverse and publicly\navailable datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.\nSince these datasets originally lacked bounding box annotations, we converted\ntheir segmentation masks into suitable detection labels. To enhance the\nrobustness and generalizability of our model, we apply 5-fold cross-validation\nand remove anomalous samples using the LOF method configured with 30 neighbors\nand a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a\nfast and resource-efficient object detection architecture optimized for\nreal-time applications. We train the model using a combination of modern\naugmentation strategies to improve detection accuracy under diverse conditions.\n  Results: Our approach significantly improves polyp localization performance,\nachieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5\nof 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,\nour model demonstrates enhanced accuracy and efficiency.\n  Conclusions: These results suggest that the proposed method is well-suited\nfor real-time colonoscopy support in clinical settings. Overall, the study\nunderscores how crucial data preprocessing and model efficiency are when\ndesigning effective AI systems for medical imaging.", "AI": {"tldr": "该研究提出了一种结合局部离群因子（LOF）和YOLO-v11n的轻量级框架，用于结直肠息肉检测，显著提升了检测性能。", "motivation": "结直肠癌是全球主要死因之一，及时准确的息肉检测对诊断和预防至关重要。", "method": "使用LOF算法过滤噪声数据，结合YOLO-v11n模型，在五个公开数据集上测试，并通过5折交叉验证和数据增强优化模型。", "result": "模型性能显著提升，精确度95.83%，召回率91.85%，F1分数93.48%，mAP@0.5为96.48%。", "conclusion": "该方法适用于临床实时结肠镜检查，强调了数据预处理和模型效率在医学影像AI系统中的重要性。"}}
{"id": "2507.11392", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.11392", "abs": "https://arxiv.org/abs/2507.11392", "authors": ["Rahel Rickenbach", "Amon Lahr", "Melanie N. Zeilinger"], "title": "Inverse Optimal Control with Constraint Relaxation", "comment": null, "summary": "Inverse optimal control (IOC) is a promising paradigm for learning and\nmimicking optimal control strategies from capable demonstrators, or gaining a\ndeeper understanding of their intentions, by estimating an unknown objective\nfunction from one or more corresponding optimal control sequences. When\ncomputing estimates from demonstrations in environments with safety-preserving\ninequality constraints, acknowledging their presence in the chosen IOC method\nis crucial given their strong influence on the final control strategy. However,\nsolution strategies capable of considering inequality constraints, such as the\ninverse Karush-Kuhn-Tucker approach, rely on their correct activation and\nfulfillment; a restrictive assumption when dealing with noisy demonstrations.\nTo overcome this problem, we leverage the concept of exact penalty functions\nfor IOC and show preservation of estimation accuracy. Considering noisy\ndemonstrations, we then illustrate how the usage of penalty functions reduces\nthe number of unknown variables and how their approximations enhance the\nestimation method's capacity to account for wrong constraint activations within\na polytopic-constrained environment. The proposed method is evaluated for three\nsystems in simulation, outperforming traditional relaxation approaches for\nnoisy demonstrations.", "AI": {"tldr": "本文提出了一种基于精确惩罚函数的逆最优控制方法，用于处理带不等式约束的噪声演示数据，提高了估计精度。", "motivation": "逆最优控制（IOC）在估计未知目标函数时，传统方法依赖于约束的正确激活和满足，这在噪声演示中受限。本文旨在解决这一问题。", "method": "利用精确惩罚函数的概念，减少未知变量数量，并通过近似增强对错误约束激活的处理能力。", "result": "在仿真实验中，该方法在噪声演示下优于传统松弛方法。", "conclusion": "提出的方法有效提升了带约束噪声演示下的逆最优控制估计精度。"}}
{"id": "2507.10899", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10899", "abs": "https://arxiv.org/abs/2507.10899", "authors": ["Wang Zhicheng", "Satoshi Yagi", "Satoshi Yamamori", "Jun Morimoto"], "title": "Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning", "comment": null, "summary": "Imitation learning for mobile manipulation is a key challenge in the field of\nrobotic manipulation. However, current mobile manipulation frameworks typically\ndecouple navigation and manipulation, executing manipulation only after\nreaching a certain location. This can lead to performance degradation when\nnavigation is imprecise, especially due to misalignment in approach angles. To\nenable a mobile manipulator to perform the same task from diverse orientations,\nan essential capability for building general-purpose robotic models, we propose\nan object-centric method based on SAM2, a foundation model towards solving\npromptable visual segmentation in images, which incorporates manipulation\norientation information into our model. Our approach enables consistent\nunderstanding of the same task from different orientations. We deploy the model\non a custom-built mobile manipulator and evaluate it on a pick-and-place task\nunder varied orientation angles. Compared to Action Chunking Transformer, our\nmodel maintains superior generalization when trained with demonstrations from\nvaried approach angles. This work significantly enhances the generalization and\nrobustness of imitation learning-based mobile manipulation systems.", "AI": {"tldr": "提出了一种基于SAM2的对象中心方法，用于提升移动机械臂在不同方向下的任务一致性，显著增强了模仿学习系统的泛化性和鲁棒性。", "motivation": "当前移动机械臂框架通常将导航和操作解耦，导致导航不精确时性能下降，尤其是在角度不对齐的情况下。需要一种方法使机械臂能从不同方向执行相同任务。", "method": "采用基于SAM2的对象中心方法，将操作方向信息融入模型，实现任务在不同方向下的一致性理解。", "result": "在自定义移动机械臂上部署模型，并在不同角度下进行拾取放置任务评估，相比Action Chunking Transformer，模型在多样化角度训练下表现更优。", "conclusion": "该方法显著提升了模仿学习在移动机械臂任务中的泛化能力和鲁棒性。"}}
{"id": "2507.10750", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10750", "abs": "https://arxiv.org/abs/2507.10750", "authors": ["Pandu Devarakota", "Nicolas Tsesmetzis", "Faruk O. Alpak", "Apurva Gala", "Detlef Hohl"], "title": "AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition", "comment": "Technical article to be submitted to Data Centric Engineering Journal", "summary": "Thanks to the availability of massive amounts of data, computing resources,\nand advanced algorithms, AI has entered nearly every sector. This has sparked\nsignificant investment and interest, particularly in building data centers with\nthe necessary hardware and software to develop and operate AI models and\nAI-based workflows. In this technical review article, we present energy\nconsumption scenarios of data centers and impact on GHG emissions, considering\nboth near-term projections (up to 2030) and long-term outlook (2035 and\nbeyond). We address the quintessential question of whether AI will have a net\npositive, neutral, or negative impact on CO2 emissions by 2035. Additionally,\nwe discuss AI's potential to automate, create efficient and disruptive\nworkflows across various fields related to energy production, supply and\nconsumption. In the near-term scenario, the growing demand for AI will likely\nstrain computing resources, lead to increase in electricity consumption and\ntherefore associated CO2 emissions. This is due to the power-hungry nature of\nbig data centers and the requirements for training and running of large and\ncomplex AI models, as well as the penetration of AI assistant search and\napplications for public use. However, the long-term outlook could be more\npromising. AI has the potential to be a game-changer in CO2 reduction. Its\nability to further automate and optimize processes across industries, from\nenergy production to logistics, could significantly decrease our carbon\nfootprint. This positive impact is anticipated to outweigh the initial\nemissions bump, creating value for businesses and society in areas where\ntraditional solutions have fallen short. In essence, AI might cause some\ninitial growing pains for the environment, but it has the potential to support\nclimate mitigation efforts.", "AI": {"tldr": "本文探讨了AI对数据中心能源消耗和温室气体排放的影响，分析了短期（2030年前）和长期（2035年后）的碳排放趋势，并评估AI是否对CO2排放有正面、中性或负面影响。", "motivation": "随着AI在各行业的广泛应用，数据中心的能源消耗和碳排放问题日益突出，研究AI对环境的影响具有重要意义。", "method": "通过分析数据中心的能源消耗和AI模型的运行需求，结合短期和长期的碳排放预测，评估AI对CO2排放的潜在影响。", "result": "短期内，AI的普及可能导致能源消耗和碳排放增加；但长期来看，AI的自动化和优化能力有望显著减少碳排放。", "conclusion": "AI在初期可能对环境造成压力，但长期来看有望成为气候减缓和碳减排的重要工具。"}}
{"id": "2507.10599", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10599", "abs": "https://arxiv.org/abs/2507.10599", "authors": ["Bo Zhao", "Maya Okawa", "Eric J. Bigelow", "Rose Yu", "Tomer Ullman", "Ekdeep Singh Lubana", "Hidenori Tanaka"], "title": "Emergence of Hierarchical Emotion Organization in Large Language Models", "comment": null, "summary": "As large language models (LLMs) increasingly power conversational agents,\nunderstanding how they model users' emotional states is critical for ethical\ndeployment. Inspired by emotion wheels -- a psychological framework that argues\nemotions organize hierarchically -- we analyze probabilistic dependencies\nbetween emotional states in model outputs. We find that LLMs naturally form\nhierarchical emotion trees that align with human psychological models, and\nlarger models develop more complex hierarchies. We also uncover systematic\nbiases in emotion recognition across socioeconomic personas, with compounding\nmisclassifications for intersectional, underrepresented groups. Human studies\nreveal striking parallels, suggesting that LLMs internalize aspects of social\nperception. Beyond highlighting emergent emotional reasoning in LLMs, our\nresults hint at the potential of using cognitively-grounded theories for\ndeveloping better model evaluations.", "AI": {"tldr": "研究发现大型语言模型（LLMs）能自然形成与人类心理模型一致的情感层次结构，且模型越大层次越复杂。同时，模型在情感识别上存在社会经济身份的偏见，尤其是对交叉性、代表性不足的群体。", "motivation": "理解LLMs如何建模用户情感状态对于其伦理部署至关重要，尤其是基于心理学情感轮的理论框架。", "method": "通过分析模型输出中情感状态的概率依赖关系，结合人类心理学模型进行研究。", "result": "LLMs形成的情感层次结构与人类心理模型一致，且大模型层次更复杂；同时发现情感识别中的系统性偏见。", "conclusion": "LLMs内化了社会感知的某些方面，研究为基于认知理论的模型评估提供了潜在方向。"}}
{"id": "2507.10881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10881", "abs": "https://arxiv.org/abs/2507.10881", "authors": ["Roman Naeem", "David Hagerman", "Jennifer Alvén", "Lennart Svensson", "Fredrik Kahl"], "title": "Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes", "comment": "Submitted Version. Accepted at MICCAI 2025", "summary": "Tubular tree structures, such as blood vessels and airways, are essential in\nhuman anatomy and accurately tracking them while preserving their topology is\ncrucial for various downstream tasks. Trexplorer is a recurrent model designed\nfor centerline tracking in 3D medical images but it struggles with predicting\nduplicate branches and terminating tracking prematurely. To address these\nissues, we present Trexplorer Super, an enhanced version that notably improves\nperformance through novel advancements. However, evaluating centerline tracking\nmodels is challenging due to the lack of public datasets. To enable thorough\nevaluation, we develop three centerline datasets, one synthetic and two real,\neach with increasing difficulty. Using these datasets, we conduct a\ncomprehensive evaluation of existing state-of-the-art (SOTA) models and compare\nthem with our approach. Trexplorer Super outperforms previous SOTA models on\nevery dataset. Our results also highlight that strong performance on synthetic\ndata does not necessarily translate to real datasets. The code and datasets are\navailable at https://github.com/RomStriker/Trexplorer-Super.", "AI": {"tldr": "Trexplorer Super是一种改进的3D医学图像中心线追踪模型，解决了重复分支预测和提前终止的问题，并在新开发的数据集上表现优于现有SOTA模型。", "motivation": "准确追踪管状树结构（如血管和气道）在医学图像中至关重要，但现有模型存在重复分支和提前终止的问题。", "method": "提出Trexplorer Super，通过新方法改进性能，并开发了三个难度递增的中心线数据集（一个合成，两个真实）用于评估。", "result": "Trexplorer Super在所有数据集上优于现有SOTA模型，且合成数据表现不一定适用于真实数据。", "conclusion": "Trexplorer Super显著提升了中心线追踪性能，同时公开了代码和数据集以促进进一步研究。"}}
{"id": "2507.11420", "categories": ["eess.SY", "cs.SY", "93C40", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.11420", "abs": "https://arxiv.org/abs/2507.11420", "authors": ["Mingcong Li"], "title": "A Risk-Aware Adaptive Robust MPC with Learned Uncertainty Quantification", "comment": "17 pages, 10 figures", "summary": "Solving chance-constrained optimal control problems for systems subject to\nnon-stationary uncertainties is a significant challenge.Conventional robust\nmodel predictive control (MPC) often yields excessive conservatism by relying\non static worst-case assumptions, while standard stochastic MPC methods\nstruggle when underlying uncertainty distributions are unknown a priori.This\narticle presents a Risk-Aware Adaptive Robust MPC (RAAR-MPC) framework,a\nhierarchical architecture that systematically orchestrates a novel synthesis of\nproactive, learning-based risk assessment and reactive risk regulation. The\nframework employs a medium-frequency risk assessment engine, which leverages\nGaussian process regression and active learning, to construct a tight,\ndata-driven characterization of the prediction error set from operational\ndata.Concurrently, a low-timescale outer loop implements a self-correcting\nupdate law for an adaptive safety margin to precisely regulate the empirical\nrisk and compensate for unmodeled dynamics.This dual-timescale adaptation\nenables the system to rigorously satisfy chance constraints with a user-defined\nprobability, while minimizing the conservatism inherent in traditional\napproaches.We formally establish that the interplay between these adaptive\ncomponents guarantees recursive feasibility and ensures the closed-loop system\nsatisfies the chance constraints up to a user-defined risk level with high\nprobability.Numerical experiments on a benchmark DC-DC converter under\nnon-stationary parametric uncertainties demonstrate that our framework\nprecisely achieves the target risk level, resulting in a significantly lower\naverage cost compared to state-of-the-art robust and stochastic MPC strategies.", "AI": {"tldr": "提出了一种风险感知自适应鲁棒MPC框架（RAAR-MPC），通过分层架构结合主动学习和自适应安全裕度，解决非平稳不确定性下的机会约束最优控制问题。", "motivation": "传统鲁棒MPC过于保守，而随机MPC在不确定性分布未知时表现不佳。", "method": "采用双时间尺度架构，结合高斯过程回归和主动学习的风险评估引擎，以及自适应安全裕度的外环更新。", "result": "在非平稳参数不确定性下，RAAR-MPC精确达到目标风险水平，平均成本显著低于现有方法。", "conclusion": "RAAR-MPC在保证机会约束的同时，显著降低了保守性，适用于非平稳不确定性系统。"}}
{"id": "2507.10914", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10914", "abs": "https://arxiv.org/abs/2507.10914", "authors": ["James A. Preiss", "Fengze Xie", "Yiheng Lin", "Adam Wierman", "Yisong Yue"], "title": "Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization", "comment": "11 pages, 9 figures", "summary": "We study online algorithms to tune the parameters of a robot controller in a\nsetting where the dynamics, policy class, and optimality objective are all\ntime-varying. The system follows a single trajectory without episodes or state\nresets, and the time-varying information is not known in advance. Focusing on\nnonlinear geometric quadrotor controllers as a test case, we propose a\npractical implementation of a single-trajectory model-based online policy\noptimization algorithm, M-GAPS,along with reparameterizations of the quadrotor\nstate space and policy class to improve the optimization landscape. In hardware\nexperiments,we compare to model-based and model-free baselines that impose\nartificial episodes. We show that M-GAPS finds near-optimal parameters more\nquickly, especially when the episode length is not favorable. We also show that\nM-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and\nachieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our\nresults demonstrate the hardware practicality of this emerging class of online\npolicy optimization that offers significantly more flexibility than classic\nadaptive control, while being more stable and data-efficient than model-free\nreinforcement learning.", "AI": {"tldr": "提出了一种名为M-GAPS的单轨迹在线策略优化算法，用于调整机器人控制器的参数，适用于动态、策略类和目标函数均随时间变化的情况。", "motivation": "研究在动态、策略类和目标函数均随时间变化且无预先信息的情况下，如何在线优化机器人控制器的参数。", "method": "提出M-GAPS算法，重新参数化四旋翼状态空间和策略类以优化搜索空间，并与基于模型和无模型的基线方法进行比较。", "result": "M-GAPS在硬件实验中更快找到接近最优的参数，尤其在不利的片段长度下表现更优，并能快速适应未建模的风和负载干扰。", "conclusion": "M-GAPS展示了在线策略优化在硬件上的实用性，比经典自适应控制更灵活，比无模型强化学习更稳定和数据高效。"}}
{"id": "2507.10758", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10758", "abs": "https://arxiv.org/abs/2507.10758", "authors": ["Nikesh Prajapati", "Bimal Karki", "Saroj Gopali", "Akbar Siami Namin"], "title": "IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models", "comment": null, "summary": "This paper intends to detect IoT malicious attacks through deep learning\nmodels and demonstrates a comprehensive evaluation of the deep learning and\ngraph-based models regarding malicious network traffic detection. The models\nparticularly are based on GraphSAGE, Bidirectional encoder representations from\ntransformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head\nAttention, together with Bidirectional Long Short-Term Memory (BI-LSTM)\nMulti-Head Attention and BI-LSTM and LSTM models. The chosen models\ndemonstrated great performance to model temporal patterns and detect feature\nsignificance. The observed performance are mainly due to the fact that IoT\nsystem traffic patterns are both sequential and diverse, leaving a rich set of\ntemporal patterns for the models to learn. Experimental results showed that\nBERT maintained the best performance. It achieved 99.94% accuracy rate\nalongside high precision and recall, F1-score and AUC-ROC score of 99.99% which\ndemonstrates its capabilities through temporal dependency capture. The\nMulti-Head Attention offered promising results by providing good detection\ncapabilities with interpretable results. On the other side, the Multi-Head\nAttention model required significant processing time like BI-LSTM variants. The\nGraphSAGE model achieved good accuracy while requiring the shortest training\ntime but yielded the lowest accuracy, precision, and F1 score compared to the\nother models", "AI": {"tldr": "该论文通过深度学习模型检测物联网恶意攻击，评估了多种模型（如GraphSAGE、BERT、TCN等）在恶意流量检测中的表现，其中BERT表现最佳。", "motivation": "物联网系统流量模式具有时序性和多样性，为模型学习提供了丰富的数据，因此研究如何利用深度学习模型高效检测恶意攻击。", "method": "采用GraphSAGE、BERT、TCN、Multi-Head Attention、BI-LSTM和LSTM等模型，分析其检测能力和性能。", "result": "BERT表现最优，准确率达99.94%，其他指标（如F1-score）接近99.99%；Multi-Head Attention检测能力好但耗时；GraphSAGE训练时间最短但性能较低。", "conclusion": "BERT在捕获时序依赖方面表现出色，是检测物联网恶意攻击的最佳选择；其他模型各有优劣，可根据需求选择。"}}
{"id": "2507.10743", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10743", "abs": "https://arxiv.org/abs/2507.10743", "authors": ["Nickolas Freeman", "Thanh Nguyen", "Gregory Bott", "Jason Parton", "Collin Francel"], "title": "Language Models for Adult Service Website Text Analysis", "comment": "32 pages, 12 figures, 1 table", "summary": "Sex trafficking refers to the use of force, fraud, or coercion to compel an\nindividual to perform in commercial sex acts against their will. Adult service\nwebsites (ASWs) have and continue to be linked to sex trafficking, offering a\nplatform for traffickers to advertise their victims. Thus, organizations\ninvolved in the fight against sex trafficking often use ASW data when\nattempting to identify potential sex trafficking victims. A critical challenge\nin transforming ASW data into actionable insight is text analysis. Previous\nresearch using ASW data has shown that ASW ad text is important for linking\nads. However, working with this text is challenging due to its extensive use of\nemojis, poor grammar, and deliberate obfuscation to evade law enforcement\nscrutiny. We conduct a comprehensive study of language modeling approaches for\nthis application area, including simple information retrieval methods,\npre-trained transformers, and custom transformer models. We demonstrate that\ncharacteristics of ASW text data allow efficient custom transformer models to\nbe trained with relatively small GPU resources and used efficiently for\ninference on consumer hardware. Our custom models outperform fine-tuned\nvariants of well-known encoder-only transformer models, including BERT-base,\nRoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We\ndemonstrate the use of our best-performing custom configuration on three tasks\nrelated to ASW data analysis: (i) decomposing the giant component in a graph\nrepresentation of ASW data, (ii) clustering ASW ad text, and (iii) using the\nlearned token embeddings to understand the use of emojis in the illicit context\nwe study. The models we develop represent a significant advancement in ASW text\nanalysis, which can be leveraged in a variety of downstream applications and\nresearch.", "AI": {"tldr": "研究探讨了如何利用语言建模方法分析成人服务网站（ASW）广告文本，以识别性交易受害者，并开发了高效的定制Transformer模型。", "motivation": "ASW广告文本因使用表情符号、语法混乱和故意模糊而难以分析，但却是识别性交易受害者的关键数据。", "method": "研究了多种语言建模方法，包括信息检索、预训练Transformer和定制Transformer模型，并评估了其性能。", "result": "定制Transformer模型在准确性、召回率、F1分数和ROC AUC上优于BERT-base、RoBERTa和ModernBERT等模型。", "conclusion": "开发的模型显著提升了ASW文本分析能力，可用于多种下游应用和研究。"}}
{"id": "2507.10893", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2507.10893", "abs": "https://arxiv.org/abs/2507.10893", "authors": ["Minjong Cheon", "Eunhan Goo", "Su-Hyeon Shin", "Muhammad Ahmed", "Hyungjun Kim"], "title": "Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency", "comment": "26pages, 9 Figures", "summary": "Recently, AI-based weather forecast models have achieved impressive advances.\nThese models have reached accuracy levels comparable to traditional NWP\nsystems, marking a significant milestone in data-driven weather prediction.\nHowever, they mostly leverage Transformer-based architectures, which often\nleads to high training complexity and resource demands due to the massive\nparameter sizes. In this study, we introduce a modernized CNN-based model for\nglobal weather forecasting that delivers competitive accuracy while\nsignificantly reducing computational requirements. To present a systematic\nmodernization roadmap, we highlight key architectural enhancements across\nmultiple design scales from an earlier CNN-based approach. KAI-a incorporates a\nscale-invariant architecture and InceptionNeXt-based blocks within a\ngeophysically-aware design, tailored to the structure of Earth system data.\nTrained on the ERA5 daily dataset with 67 atmospheric variables, the model\ncontains about 7 million parameters and completes training in just 12 hours on\na single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the\nperformance of state-of-the-art models in medium-range weather forecasting,\nwhile offering a significantly lightweight design. Furthermore, case studies on\nthe 2018 European heatwave and the East Asian summer monsoon demonstrate\nKAI-a's robust skill in capturing extreme events, reinforcing its practical\nutility.", "AI": {"tldr": "本文介绍了一种基于CNN的轻量级全球天气预报模型KAI-a，其性能与现有最先进模型相当，但计算需求显著降低。", "motivation": "当前AI天气预报模型多基于Transformer架构，计算复杂度高且资源需求大，因此需要一种更高效的替代方案。", "method": "采用现代化的CNN架构，结合尺度不变设计和InceptionNeXt模块，针对地球系统数据优化。", "result": "KAI-a在中等范围天气预报中表现优异，且训练仅需12小时，参数规模为700万。", "conclusion": "KAI-a为高效天气预报提供了可行方案，尤其在极端事件预测中表现出色。"}}
{"id": "2507.10968", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10968", "abs": "https://arxiv.org/abs/2507.10968", "authors": ["Toktam Mohammadnejad", "Jovin D'sa", "Behdad Chalaki", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi-Pari"], "title": "SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging", "comment": "Accepted at IEEE ITSC 2025", "summary": "Merging onto a highway is a complex driving task that requires identifying a\nsafe gap, adjusting speed, often interactions to create a merging gap, and\ncompleting the merge maneuver within a limited time window while maintaining\nsafety and driving comfort. In this paper, we introduce a Safe Merging and\nReal-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed\nto facilitate safe and comfortable forced merging. By deliberately adapting\ncost terms to the unique challenges of forced merging and introducing a desired\nspeed heuristic, SMART-Merge planner enables the ego vehicle to merge\nsuccessfully while minimizing the merge time. We verify the efficiency and\neffectiveness of the proposed merge planner through high-fidelity CarMaker\nsimulations on hundreds of highway merge scenarios. Our proposed planner\nachieves the success rate of 100% as well as completes the merge maneuver in\nthe shortest amount of time compared with the baselines, demonstrating our\nplanner's capability to handle complex forced merge tasks and provide a\nreliable and robust solution for autonomous highway merge. The simulation\nresult videos are available at\nhttps://sites.google.com/view/smart-merge-planner/home.", "AI": {"tldr": "论文提出了一种基于格点的运动规划器（SMART-Merge），用于实现安全且舒适的强制并道。通过优化成本函数和引入期望速度启发式，该规划器在仿真中实现了100%的成功率和最短并道时间。", "motivation": "高速并道是一项复杂的驾驶任务，需要识别安全间隙、调整速度，并在有限时间内完成并道，同时确保安全和舒适。", "method": "采用基于格点的运动规划方法，优化成本函数以适应强制并道的挑战，并引入期望速度启发式。", "result": "在CarMaker仿真中，SMART-Merge规划器实现了100%的成功率，并道时间最短。", "conclusion": "SMART-Merge规划器能够高效处理复杂的强制并道任务，为自动驾驶提供可靠解决方案。"}}
{"id": "2507.10950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10950", "abs": "https://arxiv.org/abs/2507.10950", "authors": ["Zhiwei Wu", "Jiahao Luo", "Siyi Wei", "Jinhui Zhang"], "title": "Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances", "comment": null, "summary": "This paper presents a unified modeling and optimization framework to enhance\nthe kinematic performance of multi-magnet embedded soft continuum robots\n(MeSCRs). To this end, we establish a differentiable system formulation based\non an extended pseudo-rigid-body model. This formulation enables analysis of\nthe equilibrium well-posedness and the geometry of the induced configuration\nunder magnetic actuation. In particular, we show that the maximum controllable\ndegrees of freedom of a MeSCR equal twice the number of embedded magnets. We\nsubsequently develop a structural optimization framework based on differential\ngeometry that links classical kinematic measures (e.g., manipulability and\ndexterity) to the configuration of embedded magnets. The resulting optimization\ncondition reveals that improving local performance requires structurally\nmodulating the spectrum of the configuration space metric to counteract its\ndistortion. Closed-form solutions for optimal magnet configurations are derived\nunder representative conditions, and a gradient-based numerical method is\nproposed for general design scenarios. Simulation studies validate the\neffectiveness of the proposed framework.", "AI": {"tldr": "提出了一种统一建模与优化框架，提升多磁体嵌入式软连续体机器人（MeSCRs）的运动性能。", "motivation": "通过建立可微系统公式，分析磁驱动下的平衡适定性及诱导构型几何，优化MeSCRs的局部性能。", "method": "基于扩展伪刚体模型建立可微系统，结合微分几何开发结构优化框架，提出梯度数值方法。", "result": "最大可控自由度等于嵌入磁体数的两倍；优化条件揭示需调制构型空间度量谱以抵消失真。", "conclusion": "仿真验证了框架有效性，闭式解和梯度方法适用于不同设计场景。"}}
{"id": "2507.10761", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10761", "abs": "https://arxiv.org/abs/2507.10761", "authors": ["Tyler King", "Nikolos Gurney", "John H. Miller", "Volkan Ustun"], "title": "Detecting AI Assistance in Abstract Complex Tasks", "comment": "Accepted to HCII 2025", "summary": "Detecting assistance from artificial intelligence is increasingly important\nas they become ubiquitous across complex tasks such as text generation, medical\ndiagnosis, and autonomous driving. Aid detection is challenging for humans,\nespecially when looking at abstract task data. Artificial neural networks excel\nat classification thanks to their ability to quickly learn from and process\nlarge amounts of data -- assuming appropriate preprocessing. We posit detecting\nhelp from AI as a classification task for such models. Much of the research in\nthis space examines the classification of complex but concrete data classes,\nsuch as images. Many AI assistance detection scenarios, however, result in data\nthat is not machine learning-friendly. We demonstrate that common models can\neffectively classify such data when it is appropriately preprocessed. To do so,\nwe construct four distinct neural network-friendly image formulations along\nwith an additional time-series formulation that explicitly encodes the\nexploration/exploitation of users, which allows for generalizability to other\nabstract tasks. We benchmark the quality of each image formulation across three\nclassical deep learning architectures, along with a parallel CNN-RNN\narchitecture that leverages the additional time series to maximize testing\nperformance, showcasing the importance of encoding temporal and spatial\nquantities for detecting AI aid in abstract tasks.", "AI": {"tldr": "论文提出将AI辅助检测视为分类任务，通过预处理数据使常见模型能有效分类抽象任务数据，并展示了四种图像和时间序列数据表示方法，验证了时空编码对检测AI辅助的重要性。", "motivation": "随着AI在复杂任务中的普及，检测AI辅助变得重要但具有挑战性，尤其是对抽象任务数据。研究旨在探索如何通过预处理使神经网络有效分类此类数据。", "method": "构建了四种神经网络友好的图像表示方法和一种时间序列表示方法，用于编码用户探索/利用行为。测试了三种经典深度学习架构和一种并行CNN-RNN架构。", "result": "实验表明，适当的预处理和时空编码显著提升了模型在抽象任务中检测AI辅助的性能，并行CNN-RNN架构表现最佳。", "conclusion": "研究证明了预处理和时空编码对检测AI辅助的重要性，为抽象任务中的AI辅助检测提供了有效方法。"}}
{"id": "2507.10772", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10772", "abs": "https://arxiv.org/abs/2507.10772", "authors": ["Michal Podstawski"], "title": "Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs", "comment": null, "summary": "Labeled property graphs often contain rich textual attributes that can\nenhance analytical tasks when properly leveraged. This work explores the use of\npretrained text embedding models to enable efficient semantic analysis in such\ngraphs. By embedding textual node and edge properties, we support downstream\ntasks including node classification and relation prediction with improved\ncontextual understanding. Our approach integrates language model embeddings\ninto the graph pipeline without altering its structure, demonstrating that\ntextual semantics can significantly enhance the accuracy and interpretability\nof property graph analysis.", "AI": {"tldr": "利用预训练文本嵌入模型增强标记属性图的语义分析，提升节点分类和关系预测任务的效果。", "motivation": "标记属性图中的丰富文本属性未被充分利用，希望通过语义分析增强其分析能力。", "method": "将预训练语言模型的嵌入技术应用于图的节点和边属性，不改变图结构。", "result": "文本语义显著提高了属性图分析的准确性和可解释性。", "conclusion": "文本嵌入模型可有效增强属性图的语义分析能力，支持多种下游任务。"}}
{"id": "2507.10895", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10895", "abs": "https://arxiv.org/abs/2507.10895", "authors": ["Xiaocong Zeng", "Craig Michoski", "Yan Pang", "Dongyang Kuang"], "title": "Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition", "comment": null, "summary": "In this work, we address the often-overlooked issue of Timescale Dependent\nLabel Inconsistency (TsDLI) in training neural network models for EEG-based\nhuman emotion recognition. To mitigate TsDLI and enhance model generalization\nand explainability, we propose two novel regularization strategies: Local\nVariation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods\nincorporate classical mathematical principles--specifically, functions of\nbounded variation and commute-time distances--within a graph theoretic\nframework. Complementing our regularizers, we introduce a suite of new\nevaluation metrics that better capture the alignment between temporally local\npredictions and their associated global emotion labels. We validate our\napproach through comprehensive experiments on two widely used EEG emotion\ndatasets, DREAMER and DEAP, across a range of neural architectures including\nLSTM and transformer-based models. Performance is assessed using five distinct\nmetrics encompassing both quantitative accuracy and qualitative consistency.\nResults consistently show that our proposed methods outperform state-of-the-art\nbaselines, delivering superior aggregate performance and offering a principled\ntrade-off between interpretability and predictive power under label\ninconsistency. Notably, LVL achieves the best aggregate rank across all\nbenchmarked backbones and metrics, while LGCL frequently ranks the second,\nhighlighting the effectiveness of our framework.", "AI": {"tldr": "论文提出两种新的正则化策略（LVL和LGCL）来解决EEG情感识别中的时间尺度依赖标签不一致问题，并通过实验验证其优于现有方法。", "motivation": "解决EEG情感识别中时间尺度依赖标签不一致（TsDLI）的问题，提升模型的泛化能力和可解释性。", "method": "提出Local Variation Loss (LVL)和Local-Global Consistency Loss (LGCL)两种正则化策略，结合数学原理和图论框架。", "result": "在DREAMER和DEAP数据集上，LVL和LGCL表现优于现有方法，LVL在所有基准测试中排名第一。", "conclusion": "提出的方法在标签不一致情况下平衡了可解释性和预测能力，LVL表现最佳。"}}
{"id": "2507.11211", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11211", "abs": "https://arxiv.org/abs/2507.11211", "authors": ["Chen Cai", "Ernesto Dickel Saraiva", "Ya-jun Pan", "Steven Liu"], "title": "MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments", "comment": "10 pages, 5 figures, submitted to IEEE Robotics and Automation\n  Letters (RA-L)", "summary": "This letter presents a novel coarse-to-fine motion planning framework for\nrobotic manipulation in cluttered, unmodeled environments. The system\nintegrates a dual-camera perception setup with a B-spline-based model\npredictive control (MPC) scheme. Initially, the planner generates feasible\nglobal trajectories from partial and uncertain observations. As new visual data\nare incrementally fused, both the environment model and motion planning are\nprogressively refined. A vision-based cost function promotes target-driven\nexploration, while a refined kernel-perceptron collision detector enables\nefficient constraint updates for real-time planning. The framework accommodates\nclosed-chain kinematics and supports dynamic replanning. Experiments on a\nmulti-arm platform validate its robustness and adaptability under uncertainties\nand clutter.", "AI": {"tldr": "提出了一种从粗到细的运动规划框架，用于机器人在杂乱、未建模环境中的操作，结合双摄像头感知和B样条模型预测控制。", "motivation": "解决机器人在杂乱、未建模环境中的运动规划问题，特别是在部分和不确定观测条件下。", "method": "采用双摄像头感知和B样条模型预测控制，逐步融合视觉数据优化环境模型和运动规划。", "result": "实验验证了框架在不确定性和杂乱环境中的鲁棒性和适应性。", "conclusion": "该框架能够有效支持动态重规划和闭环运动学，适用于复杂环境中的机器人操作。"}}
{"id": "2507.10960", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10960", "abs": "https://arxiv.org/abs/2507.10960", "authors": ["He Zhu", "Ryo Miyoshi", "Yuki Okafuji"], "title": "Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction", "comment": null, "summary": "Prior human-robot interaction (HRI) research has primarily focused on\nsingle-user interactions, where robots do not need to consider the timing or\nrecipient of their responses. However, in multi-party interactions, such as at\nmalls and hospitals, social robots must understand the context and decide both\nwhen and to whom they should respond. In this paper, we propose a\nTransformer-based multi-task learning framework to improve the decision-making\nprocess of social robots, particularly in multi-user environments. Considering\nthe characteristics of HRI, we propose two novel loss functions: one that\nenforces constraints on active speakers to improve scene modeling, and another\nthat guides response selection towards utterances specifically directed at the\nrobot. Additionally, we construct a novel multi-party HRI dataset that captures\nreal-world complexities, such as gaze misalignment. Experimental results\ndemonstrate that our model achieves state-of-the-art performance in respond\ndecisions, outperforming existing heuristic-based and single-task approaches.\nOur findings contribute to the development of socially intelligent social\nrobots capable of engaging in natural and context-aware multi-party\ninteractions.", "AI": {"tldr": "提出了一种基于Transformer的多任务学习框架，用于改进社交机器人在多用户环境中的决策能力，通过两种新的损失函数和新的数据集，实现了最先进的性能。", "motivation": "多用户环境中，社交机器人需要理解上下文并决定何时及对谁做出响应，而现有研究主要关注单用户交互。", "method": "采用Transformer多任务学习框架，提出两种新损失函数：一种约束主动说话者以改进场景建模，另一种引导响应选择针对机器人的话语。构建了新的多用户HRI数据集。", "result": "实验表明，该模型在响应决策上优于现有的启发式和单任务方法，达到最先进性能。", "conclusion": "研究为开发能够参与自然且上下文感知的多方交互的社交智能机器人提供了贡献。"}}
{"id": "2507.10798", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10798", "abs": "https://arxiv.org/abs/2507.10798", "authors": ["Asim H. Gazi", "Bhanu T. Gullapalli", "Daiqi Gao", "Benjamin M. Marlin", "Vivek Shetty", "Susan A. Murphy"], "title": "Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions", "comment": "4 pages, 3 figures", "summary": "Timely decision making is critical to the effectiveness of mobile health\n(mHealth) interventions. At predefined timepoints called \"decision points,\"\nintelligent mHealth systems such as just-in-time adaptive interventions\n(JITAIs) estimate an individual's biobehavioral context from sensor or survey\ndata and determine whether and how to intervene. For interventions targeting\nhabitual behavior (e.g., oral hygiene), effectiveness often hinges on\ndelivering support shortly before the target behavior is likely to occur.\nCurrent practice schedules decision points at a fixed interval (e.g., one hour)\nbefore user-provided behavior times, and the fixed interval is kept the same\nfor all individuals. However, this one-size-fits-all approach performs poorly\nfor individuals with irregular routines, often scheduling decision points after\nthe target behavior has already occurred, rendering interventions ineffective.\nIn this paper, we propose SigmaScheduling, a method to dynamically schedule\ndecision points based on uncertainty in predicted behavior times. When behavior\ntiming is more predictable, SigmaScheduling schedules decision points closer to\nthe predicted behavior time; when timing is less certain, SigmaScheduling\nschedules decision points earlier, increasing the likelihood of timely\nintervention. We evaluated SigmaScheduling using real-world data from 68\nparticipants in a 10-week trial of Oralytics, a JITAI designed to improve daily\ntoothbrushing. SigmaScheduling increased the likelihood that decision points\npreceded brushing events in at least 70% of cases, preserving opportunities to\nintervene and impact behavior. Our results indicate that SigmaScheduling can\nadvance precision mHealth, particularly for JITAIs targeting time-sensitive,\nhabitual behaviors such as oral hygiene or dietary habits.", "AI": {"tldr": "SigmaScheduling动态调整决策点时间，提高移动健康干预的及时性。", "motivation": "固定时间间隔的决策点调度对习惯性行为干预效果不佳，尤其是对作息不规律的用户。", "method": "提出SigmaScheduling方法，根据行为时间预测的不确定性动态调整决策点。", "result": "在68名参与者的试验中，SigmaScheduling在70%以上的情况下确保决策点早于刷牙行为。", "conclusion": "SigmaScheduling提升了移动健康干预的精准性，尤其适用于时间敏感的习惯性行为。"}}
{"id": "2507.10787", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10787", "abs": "https://arxiv.org/abs/2507.10787", "authors": ["Yilun Zhao", "Chengye Wang", "Chuhan Li", "Arman Cohan"], "title": "Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers", "comment": "ACL 2025 Findings", "summary": "This paper introduces MISS-QA, the first benchmark specifically designed to\nevaluate the ability of models to interpret schematic diagrams within\nscientific literature. MISS-QA comprises 1,500 expert-annotated examples over\n465 scientific papers. In this benchmark, models are tasked with interpreting\nschematic diagrams that illustrate research overviews and answering\ncorresponding information-seeking questions based on the broader context of the\npaper. We assess the performance of 18 frontier multimodal foundation models,\nincluding o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant\nperformance gap between these models and human experts on MISS-QA. Our analysis\nof model performance on unanswerable questions and our detailed error analysis\nfurther highlight the strengths and limitations of current models, offering key\ninsights to enhance models in comprehending multimodal scientific literature.", "AI": {"tldr": "MISS-QA是首个评估模型解释科学文献中示意图能力的基准，包含1,500个专家标注示例。评估了18种前沿多模态基础模型，发现与人类专家存在显著差距。", "motivation": "科学文献中的示意图是重要信息载体，但现有模型对其理解能力不足，需专门基准评估。", "method": "构建MISS-QA基准，包含465篇论文的1,500个标注示例，测试模型对示意图的解释和信息检索能力。", "result": "18种模型表现显著低于人类专家，错误分析揭示了当前模型的局限性。", "conclusion": "MISS-QA为提升多模态科学文献理解提供了关键见解，需进一步优化模型。"}}
{"id": "2507.10935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10935", "abs": "https://arxiv.org/abs/2507.10935", "authors": ["Shaowen Tong", "Zimin Xia", "Alexandre Alahi", "Xuming He", "Yujiao Shi"], "title": "GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization", "comment": "accepted by ICCV2025", "summary": "Cross-view localization, the task of estimating a camera's\n3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with\nsatellite images, is crucial for large-scale outdoor applications like\nautonomous navigation and augmented reality. Existing methods often rely on\nfully supervised learning, which requires costly ground-truth pose annotations.\nIn this work, we propose GeoDistill, a Geometry guided weakly supervised self\ndistillation framework that uses teacher-student learning with Field-of-View\n(FoV)-based masking to enhance local feature learning for robust cross-view\nlocalization. In GeoDistill, the teacher model localizes a panoramic image,\nwhile the student model predicts locations from a limited FoV counterpart\ncreated by FoV-based masking. By aligning the student's predictions with those\nof the teacher, the student focuses on key features like lane lines and ignores\ntextureless regions, such as roads. This results in more accurate predictions\nand reduced uncertainty, regardless of whether the query images are panoramas\nor limited FoV images. Our experiments show that GeoDistill significantly\nimproves localization performance across different frameworks. Additionally, we\nintroduce a novel orientation estimation network that predicts relative\norientation without requiring precise planar position ground truth. GeoDistill\nprovides a scalable and efficient solution for real-world cross-view\nlocalization challenges. Code and model can be found at\nhttps://github.com/tongshw/GeoDistill.", "AI": {"tldr": "GeoDistill提出了一种基于几何引导的弱监督自蒸馏框架，通过教师-学生学习和视场掩码提升跨视角定位的鲁棒性。", "motivation": "跨视角定位在大规模户外应用中至关重要，但现有方法依赖昂贵的全监督学习。GeoDistill旨在通过弱监督方法解决这一问题。", "method": "使用教师模型定位全景图像，学生模型通过视场掩码处理有限视场图像，并通过对齐预测提升特征学习。", "result": "实验表明，GeoDistill显著提升了定位性能，并减少了不确定性。", "conclusion": "GeoDistill为跨视角定位提供了一种可扩展且高效的解决方案。"}}
{"id": "2507.11283", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11283", "abs": "https://arxiv.org/abs/2507.11283", "authors": ["Weiyi Liu", "Jingzehua Xu", "Guanwen Xie", "Yi Li"], "title": "Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks", "comment": null, "summary": "This paper presents a diffusion-augmented reinforcement learning (RL)\napproach for robust autonomous underwater vehicle (AUV) control, addressing key\nchallenges in underwater trajectory planning and dynamic environment\nadaptation. The proposed method integrates three core innovations: (1) A\ndiffusion-based trajectory generation framework that produces physically\nfeasible multi-step trajectories, enhanced by a high-dimensional state encoding\nmechanism combining current observations with historical states and actions\nthrough a novel diffusion U-Net architecture, significantly improving\nlong-horizon planning. (2) A sample-efficient hybrid learning architecture that\nsynergizes diffusion-guided exploration with RL policy optimization, where the\ndiffusion model generates diverse candidate actions and the RL critic selects\noptimal actions, achieving higher exploration efficiency and policy stability\nin dynamic underwater environments. Extensive simulation experiments validating\nthe method's superior robustness and flexibility, outperforms conventional\ncontrol methods in challenging marine conditions, offering enhanced\nadaptability and reliability for AUV operations in the underwater tasks.", "AI": {"tldr": "本文提出了一种扩散增强的强化学习方法，用于自主水下车辆（AUV）的鲁棒控制，解决了水下轨迹规划和动态环境适应的关键挑战。", "motivation": "解决水下轨迹规划和动态环境适应中的关键挑战，提升AUV在复杂海洋环境中的鲁棒性和适应性。", "method": "结合扩散模型和强化学习，通过扩散U-Net架构生成物理可行的多步轨迹，并利用混合学习架构实现高效探索与策略优化。", "result": "仿真实验验证了该方法在复杂海洋条件下的优越鲁棒性和灵活性，优于传统控制方法。", "conclusion": "该方法为水下任务提供了更高的适应性和可靠性，适用于动态水下环境中的AUV操作。"}}
{"id": "2507.10961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10961", "abs": "https://arxiv.org/abs/2507.10961", "authors": ["Joohwan Seo", "Arvind Kruthiventy", "Soomi Lee", "Megan Teng", "Xiang Zhang", "Seoyeon Choi", "Jongeun Choi", "Roberto Horowitz"], "title": "EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks", "comment": "Submitted to RA-L", "summary": "This paper presents a framework for learning vision-based robotic policies\nfor contact-rich manipulation tasks that generalize spatially across task\nconfigurations. We focus on achieving robust spatial generalization of the\npolicy for the peg-in-hole (PiH) task trained from a small number of\ndemonstrations. We propose EquiContact, a hierarchical policy composed of a\nhigh-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)\nand a novel low-level compliant visuomotor policy (Geometric Compliant ACT,\nG-CompACT). G-CompACT operates using only localized observations (geometrically\nconsistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB\nimages) and produces actions defined in the end-effector frame. Through these\ndesign choices, we show that the entire EquiContact pipeline is\nSE(3)-equivariant, from perception to force control. We also outline three key\ncomponents for spatially generalizable contact-rich policies: compliance,\nlocalized policies, and induced equivariance. Real-world experiments on PiH\ntasks demonstrate a near-perfect success rate and robust generalization to\nunseen spatial configurations, validating the proposed framework and\nprinciples. The experimental videos can be found on the project website:\nhttps://sites.google.com/berkeley.edu/equicontact", "AI": {"tldr": "提出了一种名为EquiContact的分层框架，用于学习基于视觉的机器人策略，在接触丰富的操作任务中实现空间泛化。", "motivation": "解决在少量演示下训练peg-in-hole任务策略时的空间泛化问题。", "method": "框架由高层视觉规划器（Diff-EDF）和低层顺应性视觉运动策略（G-CompACT）组成，利用局部观测和SE(3)-等变性设计。", "result": "在真实PiH任务中表现出近乎完美的成功率和未见空间配置的鲁棒泛化能力。", "conclusion": "验证了EquiContact框架及其设计原则的有效性。"}}
{"id": "2507.10803", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10803", "abs": "https://arxiv.org/abs/2507.10803", "authors": ["JaMor Hairston", "Ritvik Ranjan", "Sahithi Lakamana", "Anthony Spadaro", "Selen Bozkurt", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case", "comment": "Pages: 19, Abstract word count: 151 words, Manuscript word count:\n  2185 words, References: 14, Figures: 3, Tables: 2", "summary": "Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health", "AI": {"tldr": "论文探讨了使用大语言模型（LLM）进行归纳主题分析的可行性，通过实验验证了其在社交媒体数据分析中的表现。", "motivation": "解决LLM在需要深度解释和领域专业知识的归纳主题分析任务中的挑战。", "method": "使用两个Reddit数据集，通过零样本、单样本和少样本提示策略，将任务建模为一系列二元分类问题。", "result": "GPT-4o在少样本提示下表现最佳（准确率90.9%，F1分数0.71），高流行主题的分布与专家分类接近。", "conclusion": "少样本LLM方法可自动化主题分析，为定性研究提供可扩展的补充。"}}
{"id": "2507.10810", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.10810", "abs": "https://arxiv.org/abs/2507.10810", "authors": ["David M. Markowitz", "Samuel Hardman Taylor"], "title": "Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler", "comment": null, "summary": "In this paper, we explored how online hate is motivated by receiving social\napproval from others. We specifically examined two central tenets of Walther's\n(2024) social approval theory of online hate: (H1a) more signals of social\napproval on hate messages predicts more subsequent hate messages, and (H1b) as\nsocial approval increases, hate speech messages become more extreme. Using over\n110 million posts from Parler (2018-2021), we observed that the number of\nupvotes a person received on a hate speech post was unassociated with the\namount of hate speech in their next post and posts during the next week, month,\nthree months, and six months. Between-person effects revealed an average\nnegative relationship between social approval and hate speech production at the\npost level, but this relationship was mixed at other time intervals. Social\napproval reinforcement mechanisms of online hate may operate differently on\nniche social media platforms.", "AI": {"tldr": "研究发现，社交平台上的仇恨言论并不因获得更多点赞而增加或变得更极端。", "motivation": "探讨社交认可对在线仇恨言论的影响，验证Walter的社会认可理论。", "method": "分析2018-2021年Parler平台的1.1亿条帖子，研究点赞与后续仇恨言论的关系。", "result": "点赞数与后续仇恨言论无显著关联，甚至在某些情况下呈负相关。", "conclusion": "社交认可对仇恨言论的强化机制在特定平台上可能不同。"}}
{"id": "2507.10938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10938", "abs": "https://arxiv.org/abs/2507.10938", "authors": ["Zhengyi Xu", "Haoran Wu", "Wen Jiang", "Jie Geng"], "title": "Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing", "comment": null, "summary": "Semantic change detection (SCD) extends the binary change detection task to\nprovide not only the change locations but also the detailed \"from-to\"\ncategories in multi-temporal remote sensing data. Such detailed semantic\ninsights into changes offer considerable advantages for a wide array of\napplications. However, since SCD involves the simultaneous optimization of\nmultiple tasks, the model is prone to negative transfer due to task-specific\nlearning difficulties and conflicting gradient flows. To address this issue, we\npropose Graph Aggregation Prototype Learning for Semantic Change Detection in\nremote sensing(GAPL-SCD). In this framework, a multi-task joint optimization\nmethod is designed to optimize the primary task of semantic segmentation and\nchange detection, along with the auxiliary task of graph aggregation prototype\nlearning. Adaptive weight allocation and gradient rotation methods are used to\nalleviate the conflict between training tasks and improve multi-task learning\ncapabilities. Specifically, the graph aggregation prototype learning module\nconstructs an interaction graph using high-level features. Prototypes serve as\nclass proxies, enabling category-level domain alignment across time points and\nreducing interference from irrelevant changes. Additionally, the proposed\nself-query multi-level feature interaction and bi-temporal feature fusion\nmodules further enhance multi-scale feature representation, improving\nperformance in complex scenes. Experimental results on the SECOND and\nLandsat-SCD datasets demonstrate that our method achieves state-of-the-art\nperformance, with significant improvements in accuracy and robustness for SCD\ntask.", "AI": {"tldr": "论文提出了一种名为GAPL-SCD的图聚合原型学习方法，用于解决语义变化检测中的多任务优化问题，通过自适应权重分配和梯度旋转方法提升性能。", "motivation": "语义变化检测（SCD）需要同时优化多个任务，容易因任务间冲突导致负迁移，因此需要一种方法来缓解这种冲突并提升多任务学习能力。", "method": "提出GAPL-SCD框架，结合语义分割和变化检测的主任务与图聚合原型学习的辅助任务，采用自适应权重分配和梯度旋转方法优化任务间冲突。", "result": "在SECOND和Landsat-SCD数据集上，该方法实现了最先进的性能，显著提升了SCD任务的准确性和鲁棒性。", "conclusion": "GAPL-SCD通过多任务联合优化和图聚合原型学习，有效解决了语义变化检测中的任务冲突问题，提升了模型性能。"}}
{"id": "2507.11447", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11447", "abs": "https://arxiv.org/abs/2507.11447", "authors": ["Shuo Yang", "John Z. Zhang", "Ibrahima Sory Sow", "Zachary Manchester"], "title": "Multi-IMU Sensor Fusion for Legged Robots", "comment": "16 pages", "summary": "This paper presents a state-estimation solution for legged robots that uses a\nset of low-cost, compact, and lightweight sensors to achieve low-drift pose and\nvelocity estimation under challenging locomotion conditions. The key idea is to\nleverage multiple inertial measurement units on different links of the robot to\ncorrect a major error source in standard proprioceptive odometry. We fuse the\ninertial sensor information and joint encoder measurements in an extended\nKalman filter, then combine the velocity estimate from this filter with camera\ndata in a factor-graph-based sliding-window estimator to form a\nvisual-inertial-leg odometry method. We validate our state estimator through\ncomprehensive theoretical analysis and hardware experiments performed using\nreal-world robot data collected during a variety of challenging locomotion\ntasks. Our algorithm consistently achieves minimal position deviation, even in\nscenarios involving substantial ground impact, foot slippage, and sudden body\nrotations. A C++ implementation, along with a large-scale dataset, is available\nat https://github.com/ShuoYangRobotics/Cerberus2.0.", "AI": {"tldr": "提出了一种低成本、轻量化的状态估计方法，用于足式机器人，通过多惯性测量单元和视觉-惯性-足里程计实现低漂移的位姿和速度估计。", "motivation": "解决足式机器人在复杂运动条件下标准本体感知里程计的主要误差问题。", "method": "利用多惯性测量单元和关节编码器测量，通过扩展卡尔曼滤波融合数据，再结合相机数据在因子图滑动窗口估计器中形成视觉-惯性-足里程计。", "result": "在多种挑战性运动任务中，算法表现出低位置偏差，适应地面冲击、足滑和突然旋转等场景。", "conclusion": "该方法在复杂条件下实现了稳定且低漂移的状态估计，硬件实验验证了其有效性。"}}
{"id": "2507.10991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10991", "abs": "https://arxiv.org/abs/2507.10991", "authors": ["Abhimanyu Bhowmik", "Mohit Singh", "Madhushree Sannigrahi", "Martin Ludvigsen", "Kostas Alexis"], "title": "Uncertainty Aware Mapping for Vision-Based Underwater Robots", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Vision-based underwater robots can be useful in inspecting and exploring\nconfined spaces where traditional sensors and preplanned paths cannot be\nfollowed. Sensor noise and situational change can cause significant uncertainty\nin environmental representation. Thus, this paper explores how to represent\nmapping inconsistency in vision-based sensing and incorporate depth estimation\nconfidence into the mapping framework. The scene depth and the confidence are\nestimated using the RAFT-Stereo model and are integrated into a voxel-based\nmapping framework, Voxblox. Improvements in the existing Voxblox weight\ncalculation and update mechanism are also proposed. Finally, a qualitative\nanalysis of the proposed method is performed in a confined pool and in a pier\nin the Trondheim fjord. Experiments using an underwater robot demonstrated the\nchange in uncertainty in the visualization.", "AI": {"tldr": "本文探讨了如何在基于视觉的水下机器人中表示地图不一致性，并将深度估计置信度融入体素地图框架Voxblox中，改进了权重计算和更新机制。", "motivation": "传统传感器和预规划路径在受限空间内无法使用，视觉传感器又因噪声和环境变化导致不确定性，需改进地图表示方法。", "method": "使用RAFT-Stereo模型估计场景深度和置信度，并集成到Voxblox框架中，改进了权重计算和更新机制。", "result": "在受限水池和Trondheim峡湾的码头进行了实验，展示了不确定性在可视化中的变化。", "conclusion": "提出的方法能有效表示地图不一致性，并提升水下机器人在受限空间中的感知能力。"}}
{"id": "2507.10831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10831", "abs": "https://arxiv.org/abs/2507.10831", "authors": ["Yilin Xia", "Heng Zheng", "Shawn Bowers", "Bertram Ludäscher"], "title": "AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks", "comment": "International Conference on Artificial Intelligence and Law (ICAIL),\n  June 16-20, 2025. Chicago, IL, USA", "summary": "Argumentation frameworks (AFs) provide formal approaches for legal reasoning,\nbut identifying sources of ambiguity and explaining argument acceptance remains\nchallenging for non-experts. We present AF-XRAY, an open-source toolkit for\nexploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY\nintroduces: (i) layered visualizations based on game-theoretic argument length\nrevealing well-founded derivation structures; (ii) classification of attack\nedges by semantic roles (primary, secondary, blunders); (iii) overlay\nvisualizations of alternative 2-valued solutions on ambiguous 3-valued grounded\nsemantics; and (iv) identification of critical attack sets whose suspension\nresolves undecided arguments. Through systematic generation of critical attack\nsets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling\nusers to pinpoint specific causes of ambiguity and explore alternative\nresolutions. We use real-world legal cases (e.g., Wild Animals as modeled by\nBench-Capon) to show that our tool supports teleological legal reasoning by\nrevealing how different assumptions lead to different justified conclusions.", "AI": {"tldr": "AF-XRAY是一个开源工具包，用于探索、分析和可视化法律推理中的抽象论证框架，帮助非专家识别歧义来源并解释论证接受。", "motivation": "解决法律推理中论证框架的歧义性和解释性问题，为非专家提供更直观的分析工具。", "method": "AF-XRAY引入分层可视化、攻击边分类、替代解决方案叠加可视化及关键攻击集识别等方法。", "result": "工具能将模糊场景转化为明确的解决方案，揭示不同假设如何导致不同结论。", "conclusion": "AF-XRAY支持目的论法律推理，帮助用户理解歧义原因并探索替代解决方案。"}}
{"id": "2507.10852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10852", "abs": "https://arxiv.org/abs/2507.10852", "authors": ["Yiran Hu", "Zongyue Xue", "Haitao Li", "Siyuan Zheng", "Qingjing Chen", "Shaochun Wang", "Xihan Zhang", "Ning Zheng", "Yun Liu", "Qingyao Ai", "Yiqun Liu", "Charles L. A. Clarke", "Weixing Shen"], "title": "LLMs on Trial: Evaluating Judicial Fairness for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly used in high-stakes fields\nwhere their decisions impact rights and equity. However, LLMs' judicial\nfairness and implications for social justice remain underexplored. When LLMs\nact as judges, the ability to fairly resolve judicial issues is a prerequisite\nto ensure their trustworthiness. Based on theories of judicial fairness, we\nconstruct a comprehensive framework to measure LLM fairness, leading to a\nselection of 65 labels and 161 corresponding values. Applying this framework to\nthe judicial system, we compile an extensive dataset, JudiFair, comprising\n177,100 unique case facts. To achieve robust statistical inference, we develop\nthree evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and\nintroduce a method to assess the overall fairness of multiple LLMs across\nvarious labels. Through experiments with 16 LLMs, we uncover pervasive\ninconsistency, bias, and imbalanced inaccuracy across models, underscoring\nsevere LLM judicial unfairness. Particularly, LLMs display notably more\npronounced biases on demographic labels, with slightly less bias on substance\nlabels compared to procedure ones. Interestingly, increased inconsistency\ncorrelates with reduced biases, but more accurate predictions exacerbate\nbiases. While we find that adjusting the temperature parameter can influence\nLLM fairness, model size, release date, and country of origin do not exhibit\nsignificant effects on judicial fairness. Accordingly, we introduce a publicly\navailable toolkit containing all datasets and code, designed to support future\nresearch in evaluating and improving LLM fairness.", "AI": {"tldr": "论文研究了大型语言模型（LLMs）在司法系统中的公平性，提出了一个评估框架，发现LLMs普遍存在不一致性、偏见和不平衡的准确性，并开发了一个公开工具包以支持未来研究。", "motivation": "LLMs在高风险领域的应用日益广泛，但其司法公平性和对社会正义的影响尚未充分研究。", "method": "基于司法公平理论构建评估框架，开发了三个评估指标（不一致性、偏见和不平衡准确性），并在16个LLMs上进行了实验。", "result": "实验发现LLMs普遍存在不公平问题，特别是在人口统计标签上偏见更明显。调整温度参数可影响公平性，但模型规模、发布时间和来源国无显著影响。", "conclusion": "LLMs在司法系统中的公平性存在严重问题，需进一步研究和改进。公开工具包为未来研究提供了支持。"}}
{"id": "2507.10943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10943", "abs": "https://arxiv.org/abs/2507.10943", "authors": ["Yushun Fang", "Lu Liu", "Xiang Gao", "Qiang Hu", "Ning Cao", "Jianghe Cui", "Gang Chen", "Xiaoyun Zhang"], "title": "Robust ID-Specific Face Restoration via Alignment Learning", "comment": "17 pages, 8 figures", "summary": "The latest developments in Face Restoration have yielded significant\nadvancements in visual quality through the utilization of diverse diffusion\npriors. Nevertheless, the uncertainty of face identity introduced by\nidentity-obscure inputs and stochastic generative processes remains unresolved.\nTo address this challenge, we present Robust ID-Specific Face Restoration\n(RIDFR), a novel ID-specific face restoration framework based on diffusion\nmodels. Specifically, RIDFR leverages a pre-trained diffusion model in\nconjunction with two parallel conditioning modules. The Content Injection\nModule inputs the severely degraded image, while the Identity Injection Module\nintegrates the specific identity from a given image. Subsequently, RIDFR\nincorporates Alignment Learning, which aligns the restoration results from\nmultiple references with the same identity in order to suppress the\ninterference of ID-irrelevant face semantics (e.g. pose, expression, make-up,\nhair style). Experiments demonstrate that our framework outperforms the\nstate-of-the-art methods, reconstructing high-quality ID-specific results with\nhigh identity fidelity and demonstrating strong robustness.", "AI": {"tldr": "RIDFR是一种基于扩散模型的ID特定人脸修复框架，通过内容注入和身份注入模块，结合对齐学习，显著提升了身份保真度和视觉质量。", "motivation": "当前人脸修复方法在身份保真度上存在不足，尤其是在模糊输入和随机生成过程中。", "method": "RIDFR利用预训练扩散模型，结合内容注入模块和身份注入模块，并通过对齐学习减少ID无关语义的干扰。", "result": "实验表明，RIDFR在身份保真度和视觉质量上优于现有方法。", "conclusion": "RIDFR提供了一种高效且鲁棒的ID特定人脸修复解决方案。"}}
{"id": "2507.11000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11000", "abs": "https://arxiv.org/abs/2507.11000", "authors": ["Minwoo Cho", "Jaehwi Jang", "Daehyung Park"], "title": "ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations", "comment": "8 pages, 6 figures", "summary": "We aim to solve the problem of temporal-constraint learning from\ndemonstrations to reproduce demonstration-like logic-constrained behaviors.\nLearning logic constraints is challenging due to the combinatorially large\nspace of possible specifications and the ill-posed nature of non-Markovian\nconstraints. To figure it out, we introduce a novel temporal-constraint\nlearning method, which we call inverse logic-constraint learning (ILCL). Our\nmethod frames ICL as a two-player zero-sum game between 1) a genetic\nalgorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained\nreinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax\ntrees for parameterized truncated linear temporal logic (TLTL) without\npredefined templates. Subsequently, Logic-CRL finds a policy that maximizes\ntask rewards under the constructed TLTL constraints via a novel constraint\nredistribution scheme. Our evaluations show ILCL outperforms state-of-the-art\nbaselines in learning and transferring TL constraints on four temporally\nconstrained tasks. We also demonstrate successful transfer to real-world\npeg-in-shallow-hole tasks.", "AI": {"tldr": "提出了一种名为ILCL的新方法，通过遗传算法和逻辑约束强化学习解决时间约束学习问题，优于现有基线。", "motivation": "解决从演示中学习时间约束以复现逻辑约束行为的挑战性问题。", "method": "采用两玩家零和游戏框架，结合遗传算法的时间逻辑挖掘（GA-TL-Mining）和逻辑约束强化学习（Logic-CRL）。", "result": "在四个时间约束任务中表现优于现有方法，并成功应用于现实世界任务。", "conclusion": "ILCL方法有效解决了时间约束学习问题，具有实际应用潜力。"}}
{"id": "2507.10894", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10894", "abs": "https://arxiv.org/abs/2507.10894", "authors": ["Zongtao He", "Liuyi Wang", "Lu Chen", "Chengju Liu", "Qijun Chen"], "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization", "comment": null, "summary": "Language-guided navigation is a cornerstone of embodied AI, enabling agents\nto interpret language instructions and navigate complex environments. However,\nexpert-provided instructions are limited in quantity, while synthesized\nannotations often lack quality, making them insufficient for large-scale\nresearch. To address this, we propose NavComposer, a novel framework for\nautomatically generating high-quality navigation instructions. NavComposer\nexplicitly decomposes semantic entities such as actions, scenes, and objects,\nand recomposes them into natural language instructions. Its modular\narchitecture allows flexible integration of state-of-the-art techniques, while\nthe explicit use of semantic entities enhances both the richness and accuracy\nof instructions. Moreover, it operates in a data-agnostic manner, supporting\nadaptation to diverse navigation trajectories without domain-specific training.\nComplementing NavComposer, we introduce NavInstrCritic, a comprehensive\nannotation-free evaluation system that assesses navigation instructions on\nthree dimensions: contrastive matching, semantic consistency, and linguistic\ndiversity. NavInstrCritic provides a holistic evaluation of instruction\nquality, addressing limitations of traditional metrics that rely heavily on\nexpert annotations. By decoupling instruction generation and evaluation from\nspecific navigation agents, our method enables more scalable and generalizable\nresearch. Extensive experiments provide direct and practical evidence for the\neffectiveness of our method.", "AI": {"tldr": "NavComposer自动生成高质量导航指令，NavInstrCritic提供无标注评估，提升语言导航研究的可扩展性和通用性。", "motivation": "专家提供的导航指令数量有限，合成注释质量不足，限制了大规模研究。", "method": "NavComposer分解语义实体并重组为自然语言指令；NavInstrCritic通过对比匹配、语义一致性和语言多样性评估指令质量。", "result": "实验证明该方法有效，支持多样化导航轨迹且无需领域特定训练。", "conclusion": "该方法解决了指令生成和评估的局限性，推动了语言导航研究的可扩展性。"}}
{"id": "2507.10918", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10918", "abs": "https://arxiv.org/abs/2507.10918", "authors": ["Ikumi Numaya", "Shoji Moriya", "Shiki Sato", "Reina Akama", "Jun Suzuki"], "title": "How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations", "comment": "Accepted to SIGDIAL 2025 (long)", "summary": "Recent advancements in dialogue generation have broadened the scope of\nhuman-bot interactions, enabling not only contextually appropriate responses\nbut also the analysis of human affect and sensitivity. While prior work has\nsuggested that stylistic similarity between user and system may enhance user\nimpressions, the distinction between subjective and objective similarity is\noften overlooked. To investigate this issue, we introduce a novel dataset that\nincludes users' preferences, subjective stylistic similarity based on users'\nown perceptions, and objective stylistic similarity annotated by third party\nevaluators in open-domain dialogue settings. Analysis using the constructed\ndataset reveals a strong positive correlation between subjective stylistic\nsimilarity and user preference. Furthermore, our analysis suggests an important\nfinding: users' subjective stylistic similarity differs from third party\nobjective similarity. This underscores the importance of distinguishing between\nsubjective and objective evaluations and understanding the distinct aspects\neach captures when analyzing the relationship between stylistic similarity and\nuser preferences. The dataset presented in this paper is available online.", "AI": {"tldr": "论文探讨了对话生成中主观与客观风格相似性的区别及其对用户偏好的影响，并发布了相关数据集。", "motivation": "研究用户与系统之间的风格相似性对用户印象的影响，区分主观与客观相似性。", "method": "构建包含用户偏好、主观风格相似性和客观风格相似性的数据集，并进行相关性分析。", "result": "发现主观风格相似性与用户偏好呈强正相关，且主观与客观相似性存在差异。", "conclusion": "强调区分主观与客观评价的重要性，数据集已公开。"}}
{"id": "2507.10969", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10969", "abs": "https://arxiv.org/abs/2507.10969", "authors": ["Palash Ray", "Mahuya Sasmal", "Asish Bera"], "title": "Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data", "comment": null, "summary": "Sports action classification representing complex body postures and\nplayer-object interactions is an emerging area in image-based sports analysis.\nSome works have contributed to automated sports action recognition using\nmachine learning techniques over the past decades. However, sufficient image\ndatasets representing women sports actions with enough intra- and inter-class\nvariations are not available to the researchers. To overcome this limitation,\nthis work presents a new dataset named WomenSports for women sports\nclassification using small-scale training data. This dataset includes a variety\nof sports activities, covering wide variations in movements, environments, and\ninteractions among players. In addition, this study proposes a convolutional\nneural network (CNN) for deep feature extraction. A channel attention scheme\nupon local contextual regions is applied to refine and enhance feature\nrepresentation. The experiments are carried out on three different sports\ndatasets and one dance dataset for generalizing the proposed algorithm, and the\nperformances on these datasets are noteworthy. The deep learning method\nachieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed\nWomenSports dataset, which is publicly available for research at Mendeley Data.", "AI": {"tldr": "本文提出一个新的女性运动动作数据集WomenSports，并设计了一种基于CNN和通道注意力的深度学习方法，取得了89.15%的分类准确率。", "motivation": "现有数据集缺乏足够的女运动员动作多样性，限制了相关研究的发展。", "method": "使用CNN进行深度特征提取，结合通道注意力机制优化特征表示。", "result": "在WomenSports数据集上，ResNet-50模型达到89.15%的top-1分类准确率。", "conclusion": "提出的数据集和方法有效解决了女性运动动作分类的数据和算法问题，为后续研究提供了基础。"}}
{"id": "2507.11001", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11001", "abs": "https://arxiv.org/abs/2507.11001", "authors": ["Yanbo Wang", "Zipeng Fang", "Lei Zhao", "Weidong Chen"], "title": "Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation", "comment": null, "summary": "Service robots are increasingly deployed in diverse and dynamic environments,\nwhere both physical layouts and social contexts change over time and across\nlocations. In these unstructured settings, conventional navigation systems that\nrely on fixed parameters often fail to generalize across scenarios, resulting\nin degraded performance and reduced social acceptance. Although recent\napproaches have leveraged reinforcement learning to enhance traditional\nplanners, these methods often fail in real-world deployments due to poor\ngeneralization and limited simulation diversity, which hampers effective\nsim-to-real transfer. To tackle these issues, we present LE-Nav, an\ninterpretable and scene-aware navigation framework that leverages multi-modal\nlarge language model reasoning and conditional variational autoencoders to\nadaptively tune planner hyperparameters. To achieve zero-shot scene\nunderstanding, we utilize one-shot exemplars and chain-of-thought prompting\nstrategies. Additionally, a conditional variational autoencoder captures the\nmapping between natural language instructions and navigation hyperparameters,\nenabling expert-level tuning. Experiments show that LE-Nav can generate\nhyperparameters achieving human-level tuning across diverse planners and\nscenarios. Real-world navigation trials and a user study on a smart wheelchair\nplatform demonstrate that it outperforms state-of-the-art methods on\nquantitative metrics such as success rate, efficiency, safety, and comfort,\nwhile receiving higher subjective scores for perceived safety and social\nacceptance. Code is available at https://github.com/Cavendish518/LE-Nav.", "AI": {"tldr": "LE-Nav是一个基于多模态大语言模型和条件变分自编码器的导航框架，通过自适应调整规划器超参数，实现零样本场景理解和专家级调参，显著提升了导航性能和社会接受度。", "motivation": "在动态和多样化环境中，传统导航系统因固定参数难以泛化，导致性能下降和社会接受度降低。现有强化学习方法因泛化能力差和仿真多样性不足，难以实现有效的仿真到现实迁移。", "method": "利用多模态大语言模型推理和条件变分自编码器，结合一次性示例和思维链提示策略，实现场景感知和超参数自适应调整。", "result": "实验表明，LE-Nav生成的超参数在多样规划器和场景中达到人类调参水平，实际导航试验和用户研究显示其在成功率、效率、安全性和舒适性上优于现有方法。", "conclusion": "LE-Nav通过智能超参数调优和场景理解，显著提升了导航系统的性能和用户体验，具有广泛的应用潜力。"}}
{"id": "2507.10911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10911", "abs": "https://arxiv.org/abs/2507.10911", "authors": ["Yicong Wu", "Ting Chen", "Irit Hochberg", "Zhoujian Sun", "Ruth Edry", "Zhengxing Huang", "Mor Peleg"], "title": "Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation", "comment": null, "summary": "Therapy recommendation for chronic patients with multimorbidity is\nchallenging due to risks of treatment conflicts. Existing decision support\nsystems face scalability limitations. Inspired by the way in which general\npractitioners (GP) manage multimorbidity patients, occasionally convening\nmultidisciplinary team (MDT) collaboration, this study investigated the\nfeasibility and value of using a Large Language Model (LLM)-based multi-agent\nsystem (MAS) for safer therapy recommendations. We designed a single agent and\na MAS framework simulating MDT decision-making by enabling discussion among LLM\nagents to resolve medical conflicts. The systems were evaluated on therapy\nplanning tasks for multimorbidity patients using benchmark cases. We compared\nMAS performance with single-agent approaches and real-world benchmarks. An\nimportant contribution of our study is the definition of evaluation metrics\nthat go beyond the technical precision and recall and allow the inspection of\nclinical goals met and medication burden of the proposed advices to a gold\nstandard benchmark. Our results show that with current LLMs, a single agent GP\nperforms as well as MDTs. The best-scoring models provide correct\nrecommendations that address all clinical goals, yet the advices are\nincomplete. Some models also present unnecessary medications, resulting in\nunnecessary conflicts between medication and conditions or drug-drug\ninteractions.", "AI": {"tldr": "研究探讨了基于大型语言模型（LLM）的多代理系统（MAS）在慢性多病患者治疗推荐中的可行性和价值，模拟多学科团队（MDT）决策，结果显示单代理系统表现与MDT相当，但建议仍存在不完整和不必要的药物问题。", "motivation": "慢性多病患者的治疗推荐存在治疗冲突风险，现有决策支持系统扩展性有限，研究旨在探索LLM-MAS模拟MDT决策的潜力。", "method": "设计单代理和MAS框架，模拟MDT决策过程，通过LLM代理讨论解决医疗冲突，并在多病治疗任务中评估性能。", "result": "单代理GP表现与MDT相当，部分模型建议虽正确但存在不完整和不必要的药物问题。", "conclusion": "LLM-MAS在多病治疗推荐中具有潜力，但需进一步优化以减少不完整和不必要的药物建议。"}}
{"id": "2507.10920", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10920", "abs": "https://arxiv.org/abs/2507.10920", "authors": ["Seungho Choi"], "title": "HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training", "comment": null, "summary": "Large language models (LLMs) often show poor performance in low-resource\nlanguages like Korean, partly due to unique linguistic challenges such as\nhomophonous Sino-Korean words that are indistinguishable in Hangul script. To\naddress this semantic ambiguity, we propose HanjaBridge, a novel\nmeaning-injection technique integrated into a continual pre-training (CPT)\nframework. Instead of deterministically mapping a word to a single Hanja\n(Chinese character), HanjaBridge presents the model with all possible Hanja\ncandidates for a given homograph, encouraging the model to learn contextual\ndisambiguation. This process is paired with token-level knowledge distillation\nto prevent catastrophic forgetting. Experimental results show that HanjaBridge\nsignificantly improves Korean language understanding, achieving a 21\\% relative\nimprovement on the KoBALT benchmark. Notably, by reinforcing semantic alignment\nbetween Korean and Chinese through shared Hanja, we observe a strong positive\ncross-lingual transfer. Furthermore, these gains persist even when Hanja\naugmentation is omitted at inference time, ensuring practical efficiency with\nno additional run-time cost.", "AI": {"tldr": "HanjaBridge通过注入汉字的语义信息，解决了韩语中同音异义词的歧义问题，显著提升了低资源语言模型的表现。", "motivation": "解决韩语中同音异义词（如汉字词）在韩文字母中无法区分的问题，提升低资源语言模型的性能。", "method": "提出HanjaBridge技术，在持续预训练框架中注入所有可能的汉字候选，结合知识蒸馏避免灾难性遗忘。", "result": "在KoBALT基准上相对提升21%，并观察到跨语言迁移的积极效果。", "conclusion": "HanjaBridge不仅提升了韩语理解能力，还无需推理时额外成本，具有实际应用价值。"}}
{"id": "2507.10977", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10977", "abs": "https://arxiv.org/abs/2507.10977", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "Human-object interaction (HOI) detection is essential for accurately\nlocalizing and characterizing interactions between humans and objects,\nproviding a comprehensive understanding of complex visual scenes across various\ndomains. However, existing HOI detectors often struggle to deliver reliable\npredictions efficiently, relying on resource-intensive training methods and\ninefficient architectures. To address these challenges, we conceptualize a\nwavelet attention-like backbone and a novel ray-based encoder architecture\ntailored for HOI detection. Our wavelet backbone addresses the limitations of\nexpressing middle-order interactions by aggregating discriminative features\nfrom the low- and high-order interactions extracted from diverse convolutional\nfilters. Concurrently, the ray-based encoder facilitates multi-scale attention\nby optimizing the focus of the decoder on relevant regions of interest and\nmitigating computational overhead. As a result of harnessing the attenuated\nintensity of learnable ray origins, our decoder aligns query embeddings with\nemphasized regions of interest for accurate predictions. Experimental results\non benchmark datasets, including ImageNet and HICO-DET, showcase the potential\nof our proposed architecture. The code is publicly available at\n[https://github.com/henry-pay/RayEncoder].", "AI": {"tldr": "提出了一种基于小波注意力机制和射线编码器的新型HOI检测架构，解决了现有方法效率低和计算资源消耗大的问题。", "motivation": "现有HOI检测器在高效性和可靠性上表现不足，需要资源密集型训练和低效架构。", "method": "设计了小波注意力机制的主干网络和射线编码器，分别用于提取多层次交互特征和优化多尺度注意力。", "result": "在ImageNet和HICO-DET等基准数据集上验证了架构的有效性。", "conclusion": "提出的架构显著提升了HOI检测的效率和准确性，代码已开源。"}}
{"id": "2507.11006", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11006", "abs": "https://arxiv.org/abs/2507.11006", "authors": ["Ashutosh Mishra", "Shreya Santra", "Hazal Gozbasi", "Kentaro Uno", "Kazuya Yoshida"], "title": "Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments", "comment": "6 pages, 7 figures. Manuscript accepted at the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)", "summary": "This study presents an advanced approach to enhance robotic manipulation in\nuncertain and challenging environments, with a focus on autonomous operations\naugmented by human-in-the-loop (HITL) control for lunar missions. By\nintegrating human decision-making with autonomous robotic functions, the\nresearch improves task reliability and efficiency for space applications. The\nkey task addressed is the autonomous deployment of flexible solar panels using\nan extendable ladder-like structure and a robotic manipulator with real-time\nfeedback for precision. The manipulator relays position and force-torque data,\nenabling dynamic error detection and adaptive control during deployment. To\nmitigate the effects of sinkage, variable payload, and low-lighting conditions,\nefficient motion planning strategies are employed, supplemented by human\ncontrol that allows operators to intervene in ambiguous scenarios. Digital twin\nsimulation enhances system robustness by enabling continuous feedback,\niterative task refinement, and seamless integration with the deployment\npipeline. The system has been tested to validate its performance in simulated\nlunar conditions and ensure reliability in extreme lighting, variable terrain,\nchanging payloads, and sensor limitations.", "AI": {"tldr": "研究提出了一种结合自主机器人和人机交互（HITL）控制的方法，用于在月球任务中提升机器人操作的可靠性和效率，特别是在部署柔性太阳能板的任务中。", "motivation": "解决在月球等极端环境下机器人操作的挑战，如地形变化、光照不足和传感器限制，通过结合自主和人机交互控制提高任务可靠性。", "method": "集成实时反馈的机器人操作器、动态误差检测、自适应控制、高效运动规划和人机交互干预，同时利用数字孪生模拟优化任务执行。", "result": "系统在模拟月球条件下测试，验证了其在极端光照、地形变化和传感器限制下的可靠性和高效性。", "conclusion": "该方法显著提升了机器人操作在极端环境中的性能和适应性，为未来空间任务提供了可行的解决方案。"}}
{"id": "2507.10923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10923", "abs": "https://arxiv.org/abs/2507.10923", "authors": ["Yuhao Wang", "Keyan Ding", "Kehua Feng", "Zeyuan Wang", "Ming Qin", "Xiaotong Li", "Qiang Zhang", "Huajun Chen"], "title": "Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization", "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Protein language models have emerged as powerful tools for sequence\ngeneration, offering substantial advantages in functional optimization and\ndenovo design. However, these models also present significant risks of\ngenerating harmful protein sequences, such as those that enhance viral\ntransmissibility or evade immune responses. These concerns underscore critical\nbiosafety and ethical challenges. To address these issues, we propose a\nKnowledge-guided Preference Optimization (KPO) framework that integrates prior\nknowledge via a Protein Safety Knowledge Graph. This framework utilizes an\nefficient graph pruning strategy to identify preferred sequences and employs\nreinforcement learning to minimize the risk of generating harmful proteins.\nExperimental results demonstrate that KPO effectively reduces the likelihood of\nproducing hazardous sequences while maintaining high functionality, offering a\nrobust safety assurance framework for applying generative models in\nbiotechnology.", "AI": {"tldr": "提出了一种知识引导的偏好优化（KPO）框架，通过蛋白质安全知识图谱整合先验知识，以减少生成有害蛋白质序列的风险。", "motivation": "蛋白质语言模型在功能优化和设计中有优势，但可能生成有害序列，带来生物安全和伦理挑战。", "method": "结合蛋白质安全知识图谱，采用图剪枝策略和强化学习，优化序列生成。", "result": "KPO显著降低有害序列生成概率，同时保持高功能性。", "conclusion": "KPO为生物技术中生成模型的应用提供了安全保证。"}}
{"id": "2507.10957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10957", "abs": "https://arxiv.org/abs/2507.10957", "authors": ["Kalit Inani", "Keshav Kabra", "Vijay Marupudi", "Sashank Varma"], "title": "Modeling Understanding of Story-Based Analogies Using Large Language Models", "comment": "To appear at CogSci 2025", "summary": "Recent advancements in Large Language Models (LLMs) have brought them closer\nto matching human cognition across a variety of tasks. How well do these models\nalign with human performance in detecting and mapping analogies? Prior research\nhas shown that LLMs can extract similarities from analogy problems but lack\nrobust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the\ncurrent study focused on a story-based analogical mapping task and conducted a\nfine-grained evaluation of LLM reasoning abilities compared to human\nperformance. First, it explored the semantic representation of analogies in\nLLMs, using sentence embeddings to assess whether they capture the similarity\nbetween the source and target texts of an analogy, and the dissimilarity\nbetween the source and distractor texts. Second, it investigated the\neffectiveness of explicitly prompting LLMs to explain analogies. Throughout, we\nexamine whether LLMs exhibit similar performance profiles to those observed in\nhumans by evaluating their reasoning at the level of individual analogies, and\nnot just at the level of overall accuracy (as prior studies have done). Our\nexperiments include evaluating the impact of model size (8B vs. 70B parameters)\nand performance variation across state-of-the-art model architectures such as\nGPT-4 and LLaMA3. This work advances our understanding of the analogical\nreasoning abilities of LLMs and their potential as models of human reasoning.", "AI": {"tldr": "该研究探讨了大型语言模型（LLMs）在类比推理任务中的表现，与人类认知的对比，并评估了模型大小和架构对性能的影响。", "motivation": "研究动机是评估LLMs在类比推理任务中是否能够达到与人类相似的性能，并探索其语义表示和显式提示的效果。", "method": "研究方法包括基于故事的类比映射任务，使用句子嵌入评估语义表示，并测试显式提示对LLMs解释类比的影响。", "result": "实验结果表明，LLMs在类比推理任务中表现接近人类，但缺乏稳健的人类式推理能力；模型大小和架构对性能有显著影响。", "conclusion": "结论指出，LLMs在类比推理方面有潜力，但仍需改进以更接近人类认知能力。"}}
{"id": "2507.10978", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10978", "abs": "https://arxiv.org/abs/2507.10978", "authors": ["Ayush Gupta", "Siyuan Huang", "Rama Chellappa"], "title": "Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction", "comment": "Accepted at IJCB 2025", "summary": "Gait is becoming popular as a method of person re-identification because of\nits ability to identify people at a distance. However, most current works in\ngait recognition do not address the practical problem of occlusions. Among\nthose which do, some require paired tuples of occluded and holistic sequences,\nwhich are impractical to collect in the real world. Further, these approaches\nwork on occlusions but fail to retain performance on holistic inputs. To\naddress these challenges, we propose RG-Gait, a method for residual correction\nfor occluded gait recognition with holistic retention. We model the problem as\na residual learning task, conceptualizing the occluded gait signature as a\nresidual deviation from the holistic gait representation. Our proposed network\nadaptively integrates the learned residual, significantly improving performance\non occluded gait sequences without compromising the holistic recognition\naccuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR\ndatasets and show that learning the residual can be an effective technique to\ntackle occluded gait recognition with holistic retention.", "AI": {"tldr": "RG-Gait提出了一种残差学习方法，用于解决步态识别中的遮挡问题，同时保持对完整步态的识别性能。", "motivation": "当前步态识别方法未解决遮挡问题，且现有方法需要不切实际的成对数据或无法同时处理遮挡和完整步态。", "method": "将遮挡步态建模为完整步态表示的残差偏差，通过自适应残差学习网络提升遮挡步态识别性能。", "result": "在Gait3D、GREW和BRIAR数据集上验证了方法的有效性，显著提升了遮挡步态识别且不影响完整步态性能。", "conclusion": "残差学习是解决遮挡步态识别并保持完整步态性能的有效技术。"}}
{"id": "2507.11069", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11069", "abs": "https://arxiv.org/abs/2507.11069", "authors": ["Jeongyun Kim", "Seunghoon Jeong", "Giseop Kim", "Myung-Hwan Jeon", "Eunji Jun", "Ayoung Kim"], "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update", "comment": null, "summary": "Understanding the 3D geometry of transparent objects from RGB images is\nchallenging due to their inherent physical properties, such as reflection and\nrefraction. To address these difficulties, especially in scenarios with sparse\nviews and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian\nSplatting-based depth reconstruction method for transparent objects. Our key\ninsight lies in separating transparent objects from the background, enabling\nfocused optimization of Gaussians corresponding to the object. We mitigate\nartifacts with an object-aware loss that places Gaussians in obscured regions,\nensuring coverage of invisible surfaces while reducing overfitting.\nFurthermore, we incorporate a physics-based simulation that refines the\nreconstruction in just a few seconds, effectively handling object removal and\nchain-reaction movement of remaining objects without the need for rescanning.\nTRAN-D is evaluated on both synthetic and real-world sequences, and it\nconsistently demonstrated robust improvements over existing GS-based\nstate-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean\nabsolute error by over 39% for the synthetic TRansPose sequences. Furthermore,\ndespite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm\naccuracy of 48.46%, over 1.5 times that of baselines, which uses six images.\nCode and more results are available at https://jeongyun0609.github.io/TRAN-D/.", "AI": {"tldr": "TRAN-D是一种基于2D高斯泼溅的透明物体深度重建方法，通过分离透明物体与背景、优化高斯分布和物理模拟，显著提升了稀疏视图和动态环境下的3D重建精度。", "motivation": "透明物体的3D几何重建因反射和折射等物理特性而具有挑战性，尤其在稀疏视图和动态环境中。", "method": "TRAN-D通过分离透明物体与背景，优化高斯分布，并使用物体感知损失和物理模拟来减少伪影和过拟合。", "result": "在合成和真实数据集上，TRAN-D比现有方法平均绝对误差降低39%，单图像更新时精度提升1.5倍。", "conclusion": "TRAN-D在透明物体3D重建中表现出色，尤其在稀疏视图和动态环境下具有显著优势。"}}
{"id": "2507.10993", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10993", "abs": "https://arxiv.org/abs/2507.10993", "authors": ["Emir Durakovic", "Min-Hong Shih"], "title": "Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction", "comment": "This paper uses a lightly modified version of the AAAI 2025 LaTeX\n  style for formatting consistency. It is not a submission to AAAI and does not\n  include any AAAI-specific headers, footers, or metadata", "summary": "Due to climate-induced changes, many habitats are experiencing range shifts\naway from their traditional geographic locations (Piguet, 2011). We propose a\nsolution to accurately model whether bird species are present in a specific\nhabitat through the combination of Convolutional Neural Networks (CNNs)\n(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery\nand environmental features (e.g., temperature, precipitation, elevation) to\npredict bird presence across various climates. The CNN model captures spatial\ncharacteristics of landscapes such as forestation, water bodies, and\nurbanization, whereas the tabular method uses ecological and geographic data.\nBoth systems predict the distribution of birds with an average accuracy of 85%,\noffering a scalable but reliable method to understand bird migration.", "AI": {"tldr": "结合卷积神经网络（CNN）和表格数据，通过卫星图像和环境特征预测鸟类栖息地分布，准确率达85%。", "motivation": "气候变化导致栖息地范围变化，需准确预测鸟类栖息地分布。", "method": "结合CNN（捕捉空间特征）和表格数据（生态地理数据），利用卫星图像和环境特征建模。", "result": "模型预测鸟类分布的准确率达85%。", "conclusion": "提供了一种可扩展且可靠的方法，用于理解鸟类迁徙和栖息地变化。"}}
{"id": "2507.10958", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10958", "abs": "https://arxiv.org/abs/2507.10958", "authors": ["Anthony Miyaguchi", "David Guecha", "Yuwen Chiu", "Sidharth Gaur"], "title": "DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models", "comment": null, "summary": "This Working Note summarizes the participation of the DS@GT team in two eRisk\n2025 challenges. For the Pilot Task on conversational depression detection with\nlarge language-models (LLMs), we adopted a prompt-engineering strategy in which\ndiverse LLMs conducted BDI-II-based assessments and produced structured JSON\noutputs. Because ground-truth labels were unavailable, we evaluated cross-model\nagreement and internal consistency. Our prompt design methodology aligned model\noutputs with BDI-II criteria and enabled the analysis of conversational cues\nthat influenced the prediction of symptoms. Our best submission, second on the\nofficial leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.", "AI": {"tldr": "DS@GT团队在eRisk 2025挑战赛中采用提示工程策略，利用LLMs进行BDI-II评估，生成结构化JSON输出，评估跨模型一致性和内部一致性，最终在官方排行榜上排名第二。", "motivation": "研究目的是通过大型语言模型（LLMs）进行对话式抑郁检测，并探索提示设计方法以提升模型输出与BDI-II标准的对齐。", "method": "采用提示工程策略，利用多种LLMs生成结构化JSON输出，并通过跨模型一致性和内部一致性进行评估。", "result": "最佳提交在官方排行榜上排名第二，指标为DCHR = 0.50，ADODL = 0.89，ASHR = 0.27。", "conclusion": "提示设计方法有效提升了模型输出与BDI-II标准的对齐，并能够分析影响症状预测的对话线索。"}}
{"id": "2507.10999", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10999", "abs": "https://arxiv.org/abs/2507.10999", "authors": ["Quan Bi Pay", "Vishnu Monn Baskaran", "Junn Yong Loo", "KokSheik Wong", "Simon See"], "title": "SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "The resurgence of convolutional neural networks (CNNs) in visual recognition\ntasks, exemplified by ConvNeXt, has demonstrated their capability to rival\ntransformer-based architectures through advanced training methodologies and\nViT-inspired design principles. However, both CNNs and transformers exhibit a\nsimplicity bias, favoring straightforward features over complex structural\nrepresentations. Furthermore, modern CNNs often integrate MLP-like blocks akin\nto those in transformers, but these blocks suffer from significant information\nredundancies, necessitating high expansion ratios to sustain competitive\nperformance. To address these limitations, we propose SpaRTAN, a lightweight\narchitectural design that enhances spatial and channel-wise information\nprocessing. SpaRTAN employs kernels with varying receptive fields, controlled\nby kernel size and dilation factor, to capture discriminative multi-order\nspatial features effectively. A wave-based channel aggregation module further\nmodulates and reinforces pixel interactions, mitigating channel-wise\nredundancies. Combining the two modules, the proposed network can efficiently\ngather and dynamically contextualize discriminative features. Experimental\nresults in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable\nparameter efficiency while maintaining competitive performance. In particular,\non the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M\nparameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver\nstrong performance through an efficient design. On the COCO benchmark, it\nachieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M\nparameters. The code is publicly available at\n[https://github.com/henry-pay/SpaRTAN].", "AI": {"tldr": "SpaRTAN是一种轻量级架构设计，通过多尺度空间特征和通道聚合模块，提升CNN和Transformer的性能，同时减少参数冗余。", "motivation": "现代CNN和Transformer存在简单性偏好和信息冗余问题，限制了复杂结构特征的捕捉。", "method": "SpaRTAN采用多尺度核和波式通道聚合模块，动态整合空间和通道信息。", "result": "在ImageNet-1k上达到77.7%准确率（3.8M参数），COCO上50.0% AP（21.5M参数），性能优于基准。", "conclusion": "SpaRTAN通过高效设计实现了高性能和参数效率，为视觉任务提供了新思路。"}}
{"id": "2507.11076", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.11076", "abs": "https://arxiv.org/abs/2507.11076", "authors": ["Andreas Mueller", "Shivesh Kumar"], "title": "Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems", "comment": null, "summary": "Derivatives of equations of motion(EOM) describing the dynamics of rigid body\nsystems are becoming increasingly relevant for the robotics community and find\nmany applications in design and control of robotic systems. Controlling robots,\nand multibody systems comprising elastic components in particular, not only\nrequires smooth trajectories but also the time derivatives of the control\nforces/torques, hence of the EOM. This paper presents the time derivatives of\nthe EOM in closed form up to second-order as an alternative formulation to the\nexisting recursive algorithms for this purpose, which provides a direct insight\ninto the structure of the derivatives. The Lie group formulation for rigid body\nsystems is used giving rise to very compact and easily parameterized equations.", "AI": {"tldr": "论文提出了刚性体系统运动方程的二阶时间导数闭式解，替代现有递归算法，为机器人控制提供更直观的结构分析。", "motivation": "机器人控制需要平滑轨迹及控制力/力矩的时间导数，现有递归算法复杂且缺乏直观性。", "method": "采用李群理论，推导出紧凑且易参数化的二阶时间导数闭式解。", "result": "提供了运动方程的二阶时间导数直接表达式，结构清晰且计算高效。", "conclusion": "闭式解为机器人控制设计提供了更直观和高效的工具。"}}
{"id": "2507.11060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11060", "abs": "https://arxiv.org/abs/2507.11060", "authors": ["Yilmazcan Ozyurt", "Tunaberk Almaci", "Stefan Feuerriegel", "Mrinmaya Sachan"], "title": "Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing", "comment": null, "summary": "We introduce ExRec, a general framework for personalized exercise\nrecommendation with semantically-grounded knowledge tracing. Our method builds\non the observation that existing exercise recommendation approaches simulate\nstudent performance via knowledge tracing (KT) but they often overlook two key\naspects: (a) the semantic content of questions and (b) the sequential,\nstructured progression of student learning. To address this, our ExRec presents\nan end-to-end pipeline, from annotating the KCs of questions and learning their\nsemantic representations to training KT models and optimizing several\nreinforcement learning (RL) methods. Moreover, we improve standard\nQ-learning-based continuous RL methods via a tailored model-based value\nestimation (MVE) approach that directly leverages the components of KT model in\nestimating cumulative knowledge improvement. We validate the effectiveness of\nour ExRec using various RL methods across four real-world tasks with different\neducational goals in online math learning. We further show that ExRec\ngeneralizes robustly to new, unseen questions and that it produces\ninterpretable student learning trajectories. Together, our findings highlight\nthe promise of KT-guided RL for effective personalization in education.", "AI": {"tldr": "ExRec是一个个性化练习推荐框架，结合了语义知识追踪和强化学习，解决了现有方法忽略问题语义和学习顺序的问题。", "motivation": "现有练习推荐方法通过知识追踪模拟学生表现，但忽略了问题的语义内容和学习的顺序性。", "method": "ExRec采用端到端流程，包括问题知识组件标注、语义表示学习、知识追踪模型训练和强化学习方法优化，并改进了基于Q学习的连续强化学习方法。", "result": "在四个真实世界任务中验证了ExRec的有效性，展示了其对新问题的鲁棒性和可解释的学习轨迹。", "conclusion": "ExRec展示了知识追踪引导的强化学习在教育个性化中的潜力。"}}
{"id": "2507.10972", "categories": ["cs.CL", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.10972", "abs": "https://arxiv.org/abs/2507.10972", "authors": ["Zhaoyi An", "Rei Kawakami"], "title": "Teach Me Sign: Stepwise Prompting LLM for Sign Language Production", "comment": "Accepted by IEEE ICIP 2025", "summary": "Large language models, with their strong reasoning ability and rich\nknowledge, have brought revolution to many tasks of AI, but their impact on\nsign language generation remains limited due to its complexity and unique\nrules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign\nlanguage as another natural language. By fine-tuning an LLM, we enable it to\nlearn the correspondence between text and sign language, and facilitate\ngeneration. Considering the differences between sign and spoken language, we\nemploy a stepwise prompting strategy to extract the inherent sign language\nknowledge within the LLM, thereby supporting the learning and generation\nprocess. Experimental results on How2Sign and Phoenix14T datasets demonstrate\nthat our approach effectively leverages both the sign language knowledge and\nreasoning capabilities of LLM to align the different distribution and\ngrammatical rules between sign and spoken language.", "AI": {"tldr": "TEAM-Sign通过微调大语言模型（LLM）将手语视为另一种自然语言，利用逐步提示策略提取LLM中的手语知识，成功对齐手语与口语的分布和语法规则。", "motivation": "探索大语言模型在手语生成任务中的潜力，解决手语复杂性和独特规则带来的挑战。", "method": "提出TEAM-Sign方法，通过微调LLM并采用逐步提示策略，学习文本与手语的对应关系。", "result": "在How2Sign和Phoenix14T数据集上验证了方法的有效性，成功对齐手语与口语的分布和语法规则。", "conclusion": "TEAM-Sign通过结合LLM的知识和推理能力，为手语生成任务提供了有效解决方案。"}}
{"id": "2507.11003", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11003", "abs": "https://arxiv.org/abs/2507.11003", "authors": ["Yuhu Bai", "Jiangning Zhang", "Yunkang Cao", "Guangyuan Lu", "Qingdong He", "Xiangtai Li", "Guanzhong Tian"], "title": "Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection", "comment": null, "summary": "With the advent of vision-language models (e.g., CLIP) in zero- and few-shot\nsettings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in\nrecent research, where the rare classes are essential and expected in many\napplications. This study introduces \\textbf{FiSeCLIP} for ZSAD with\ntraining-free \\textbf{CLIP}, combining the feature matching with the\ncross-modal alignment. Testing with the entire dataset is impractical, while\nbatch-based testing better aligns with real industrial needs, and images within\na batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes\nother images in the same batch as reference information for the current image.\nHowever, the lack of labels for these references can introduce ambiguity, we\napply text information to \\textbf{fi}lter out noisy features. In addition, we\nfurther explore CLIP's inherent potential to restore its local\n\\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection\ntasks to enable a more accurate filtering process. Our approach exhibits\nsuperior performance for both anomaly classification and segmentation on\nanomaly detection benchmarks, building a stronger baseline for the direction,\ne.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by\n+4.6\\%$\\uparrow$/+5.7\\%$\\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.", "AI": {"tldr": "FiSeCLIP利用CLIP模型进行零样本异常检测，通过特征匹配和跨模态对齐，结合批次测试和文本信息过滤噪声，显著提升了性能。", "motivation": "零样本异常检测在工业应用中需求广泛，但现有方法存在噪声和模糊性问题，需要更高效的解决方案。", "method": "FiSeCLIP结合特征匹配与跨模态对齐，利用批次内图像作为参考，并通过文本信息过滤噪声，同时恢复CLIP的局部语义相关性。", "result": "在MVTec-AD等基准测试中，FiSeCLIP在异常分类和分割任务上表现优异，AU-ROC和F1-max分别提升4.6%和5.7%。", "conclusion": "FiSeCLIP为零样本异常检测提供了更强基线，展示了CLIP模型在细粒度任务中的潜力。"}}
{"id": "2507.11133", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11133", "abs": "https://arxiv.org/abs/2507.11133", "authors": ["Luca Beber", "Edoardo Lamon", "Giacomo Moretti", "Matteo Saveriano", "Luca Fambri", "Luigi Palopoli", "Daniele Fontanelli"], "title": "Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm", "comment": null, "summary": "Diagnostic activities, such as ultrasound scans and palpation, are relatively\nlow-cost. They play a crucial role in the early detection of health problems\nand in assessing their progression. However, they are also error-prone\nactivities, which require highly skilled medical staff. The use of robotic\nsolutions can be key to decreasing the inherent subjectivity of the results and\nreducing the waiting list. For a robot to perform palpation or ultrasound\nscans, it must effectively manage physical interactions with the human body,\nwhich greatly benefits from precise estimation of the patient's tissue\nbiomechanical properties. This paper assesses the accuracy and precision of a\nrobotic system in estimating the viscoelastic parameters of various materials,\nincluding some tests on ex vivo tissues as a preliminary proof-of-concept\ndemonstration of the method's applicability to biological samples. The\nmeasurements are compared against a ground truth derived from silicone\nspecimens with different viscoelastic properties, characterised using a\nhigh-precision instrument. Experimental results show that the robotic system's\naccuracy closely matches the ground truth, increasing confidence in the\npotential use of robots for such clinical applications.", "AI": {"tldr": "论文评估了机器人系统在估计材料粘弹性参数方面的准确性和精确性，并初步验证了其在生物样本中的适用性。", "motivation": "诊断活动（如超声扫描和触诊）虽然低成本，但易出错且依赖高技能医疗人员。机器人解决方案可减少结果的主观性并缩短等待时间。", "method": "通过机器人系统估计不同材料（包括离体组织）的粘弹性参数，并与高精度仪器测得的基准数据对比。", "result": "实验结果显示机器人系统的准确性与基准数据高度匹配，增强了其在临床应用中的潜力。", "conclusion": "机器人系统在估计组织生物力学特性方面表现出高准确性，有望用于临床诊断活动。"}}
{"id": "2507.11079", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11079", "abs": "https://arxiv.org/abs/2507.11079", "authors": ["Li Wang", "Qizhen Wu", "Lei Chen"], "title": "Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander", "comment": null, "summary": "In multiple unmanned ground vehicle confrontations, autonomously evolving\nmulti-agent tactical decisions from situational awareness remain a significant\nchallenge. Traditional handcraft rule-based methods become vulnerable in the\ncomplicated and transient battlefield environment, and current reinforcement\nlearning methods mainly focus on action manipulation instead of strategic\ndecisions due to lack of interpretability. Here, we propose a vision-language\nmodel-based commander to address the issue of intelligent\nperception-to-decision reasoning in autonomous confrontations. Our method\nintegrates a vision language model for scene understanding and a lightweight\nlarge language model for strategic reasoning, achieving unified perception and\ndecision within a shared semantic space, with strong adaptability and\ninterpretability. Unlike rule-based search and reinforcement learning methods,\nthe combination of the two modules establishes a full-chain process, reflecting\nthe cognitive process of human commanders. Simulation and ablation experiments\nvalidate that the proposed approach achieves a win rate of over 80% compared\nwith baseline models.", "AI": {"tldr": "提出了一种基于视觉语言模型的指挥官系统，用于解决无人地面车辆对抗中的智能感知到决策推理问题。", "motivation": "传统手工规则方法在复杂战场环境中脆弱，而现有强化学习方法缺乏可解释性，主要关注动作而非战略决策。", "method": "结合视觉语言模型进行场景理解和轻量级大语言模型进行战略推理，实现感知与决策的统一。", "result": "仿真实验显示，该方法相比基线模型胜率超过80%。", "conclusion": "该方法具有强适应性和可解释性，模拟了人类指挥官的认知过程。"}}
{"id": "2507.10996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10996", "abs": "https://arxiv.org/abs/2507.10996", "authors": ["Lin Tian", "Johanne R. Trippas", "Marian-Andrei Rizoiu"], "title": "Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection", "comment": "12 pages, 5 tables, CLEF 2025", "summary": "This paper presents our approach to EXIST 2025 Task 1, addressing text-based\nsexism detection in English and Spanish tweets through hierarchical Low-Rank\nAdaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter\nrouting that explicitly models label dependencies across three hierarchically\nstructured subtasks: binary sexism identification, source intention detection,\nand multilabel sexism categorization. Unlike conventional LoRA applications\nthat target only attention layers, we apply adaptation to all linear\ntransformations, enhancing the model's capacity to capture task-specific\npatterns. In contrast to complex data processing and ensemble approaches, we\nshow that straightforward parameter-efficient fine-tuning achieves strong\nperformance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each\nsubtask using unified multilingual training that leverages Llama 3.1's native\nbilingual capabilities. The method requires minimal preprocessing and uses\nstandard supervised learning. Our multilingual training strategy eliminates the\nneed for separate language-specific models, achieving 1.7-2.4\\% F1 improvements\nthrough cross-lingual transfer. With only 1.67\\% trainable parameters compared\nto full fine-tuning, our approach reduces training time by 75\\% and model\nstorage by 98\\%, while achieving competitive performance across all subtasks\n(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,\n0.6519 for multilabel categorization).", "AI": {"tldr": "本文提出了一种基于分层低秩适应（LoRA）的方法，用于检测英语和西班牙语推文中的性别歧视，通过条件适配器路由和统一多语言训练实现了高效性能。", "motivation": "解决文本性别歧视检测任务，特别是在多语言和多子任务场景下，同时减少计算和存储开销。", "method": "采用分层LoRA适配器，对所有线性变换进行适应，并通过条件路由建模标签依赖关系，使用统一多语言训练策略。", "result": "在仅使用1.67%可训练参数的情况下，性能接近全微调，训练时间减少75%，存储减少98%，并在所有子任务中表现优异。", "conclusion": "该方法展示了参数高效微调在多语言性别歧视检测中的潜力，显著降低了资源需求并保持了高性能。"}}
{"id": "2507.11015", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11015", "abs": "https://arxiv.org/abs/2507.11015", "authors": ["Zeyi Hou", "Zeqiang Wei", "Ruixin Yan", "Ning Lang", "Xiuzhuang Zhou"], "title": "Semantically Informed Salient Regions Guided Radiology Report Generation", "comment": null, "summary": "Recent advances in automated radiology report generation from chest X-rays\nusing deep learning algorithms have the potential to significantly reduce the\narduous workload of radiologists. However, due to the inherent massive data\nbias in radiology images, where abnormalities are typically subtle and sparsely\ndistributed, existing methods often produce fluent yet medically inaccurate\nreports, limiting their applicability in clinical practice. To address this\nissue effectively, we propose a Semantically Informed Salient Regions-guided\n(SISRNet) report generation method. Specifically, our approach explicitly\nidentifies salient regions with medically critical characteristics using\nfine-grained cross-modal semantics. Then, SISRNet systematically focuses on\nthese high-information regions during both image modeling and report\ngeneration, effectively capturing subtle abnormal findings, mitigating the\nnegative impact of data bias, and ultimately generating clinically accurate\nreports. Compared to its peers, SISRNet demonstrates superior performance on\nwidely used IU-Xray and MIMIC-CXR datasets.", "AI": {"tldr": "提出了一种基于语义显著区域的放射报告生成方法（SISRNet），通过聚焦医学关键区域，解决了现有方法因数据偏差导致的报告不准确问题。", "motivation": "现有深度学习方法生成的放射报告虽流畅但医学准确性不足，限制了临床应用。", "method": "SISRNet通过细粒度跨模态语义识别医学关键区域，并在图像建模和报告生成中系统关注这些区域。", "result": "在IU-Xray和MIMIC-CXR数据集上表现优于同类方法。", "conclusion": "SISRNet能有效捕捉细微异常，减少数据偏差影响，生成临床准确的报告。"}}
{"id": "2507.11170", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11170", "abs": "https://arxiv.org/abs/2507.11170", "authors": ["Giulio Giacomuzzo", "Mohamed Abdelwahab", "Marco Calì", "Alberto Dalla Libera", "Ruggero Carli"], "title": "A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty", "comment": null, "summary": "In this paper, we propose a novel learning-based robust feedback\nlinearization strategy to ensure precise trajectory tracking for an important\nfamily of Lagrangian systems. We assume a nominal knowledge of the dynamics is\ngiven but no a-priori bounds on the model mismatch are available. In our\napproach, the key ingredient is the adoption of a regression framework based on\nGaussian Processes (GPR) to estimate the model mismatch. This estimate is added\nto the outer loop of a classical feedback linearization scheme based on the\nnominal knowledge available. Then, to compensate for the residual uncertainty,\nwe robustify the controller including an additional term whose size is designed\nbased on the variance provided by the GPR framework. We proved that, with high\nprobability, the proposed scheme is able to guarantee asymptotic tracking of a\ndesired trajectory. We tested numerically our strategy on a 2 degrees of\nfreedom planar robot.", "AI": {"tldr": "提出了一种基于学习的鲁棒反馈线性化策略，用于拉格朗日系统的精确轨迹跟踪，结合高斯过程回归估计模型失配，并通过鲁棒化控制器实现高概率的渐近跟踪。", "motivation": "针对拉格朗日系统，在缺乏模型失配先验边界的情况下，实现精确的轨迹跟踪。", "method": "采用高斯过程回归（GPR）估计模型失配，并将其加入基于名义动力学的反馈线性化外环；通过基于GPR方差的鲁棒化项补偿剩余不确定性。", "result": "理论证明高概率下能实现渐近跟踪，并在2自由度平面机器人上进行了数值验证。", "conclusion": "所提策略在缺乏先验边界的情况下，通过结合GPR和鲁棒化设计，实现了拉格朗日系统的精确轨迹跟踪。"}}
{"id": "2507.11083", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11083", "abs": "https://arxiv.org/abs/2507.11083", "authors": ["Longhui Zhang", "Bin Wang", "Jiahao Wang", "Xiaofeng Zhao", "Min Zhang", "Hao Yang", "Meishan Zhang", "Yu Li", "Jing Li", "Jun Yu", "Min Zhang"], "title": "Function-to-Style Guidance of LLMs for Code Translation", "comment": "This paper has been accepted by ICML 2025. Models and benchmarks can\n  be found at https://www.modelscope.cn/collections/F2STrans-42526ff95dd843", "summary": "Large language models (LLMs) have made significant strides in code\ntranslation tasks. However, ensuring both the correctness and readability of\ntranslated code remains a challenge, limiting their effective adoption in\nreal-world software development. In this work, we propose F2STrans, a\nfunction-to-style guiding paradigm designed to progressively improve the\nperformance of LLMs in code translation. Our approach comprises two key stages:\n(1) Functional learning, which optimizes translation correctness using\nhigh-quality source-target code pairs mined from online programming platforms,\nand (2) Style learning, which improves translation readability by incorporating\nboth positive and negative style examples. Additionally, we introduce a novel\ncode translation benchmark that includes up-to-date source code, extensive test\ncases, and manually annotated ground-truth translations, enabling comprehensive\nfunctional and stylistic evaluations. Experiments on both our new benchmark and\nexisting datasets demonstrate that our approach significantly improves code\ntranslation performance. Notably, our approach enables Qwen-1.5B to outperform\nprompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code\ntranslation scenarios.", "AI": {"tldr": "F2STrans通过功能学习和风格学习两阶段方法提升LLM在代码翻译中的正确性和可读性，并在新基准测试中表现优异。", "motivation": "解决LLM在代码翻译中正确性和可读性不足的问题，以推动其在软件开发中的实际应用。", "method": "提出F2STrans框架，包含功能学习（优化正确性）和风格学习（提升可读性），并引入新基准测试。", "result": "实验表明F2STrans显著提升性能，Qwen-1.5B甚至优于Qwen-32B和GPT-4。", "conclusion": "F2STrans为代码翻译任务提供了一种高效且可扩展的解决方案。"}}
{"id": "2507.11004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11004", "abs": "https://arxiv.org/abs/2507.11004", "authors": ["Yejun Yoon", "Jaeyoon Jung", "Seunghyun Yoon", "Kunwoo Park"], "title": "Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification", "comment": "ACL 2025 Workshop (FEVER)", "summary": "This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task\nat the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the\nbest-performing open-source model from the previous year's challenge. It\nimproves evidence quality through document summarization and answer\nreformulation, optimizes veracity prediction via post-training quantization\nunder computational constraints, and enhances overall system performance by\nintegrating updated language model (LM) backbones. HerO 2 ranked second on the\nleaderboard while achieving the shortest runtime among the top three systems,\ndemonstrating both high efficiency and strong potential for real-world fact\nverification. The code is available at https://github.com/ssu-humane/HerO2.", "AI": {"tldr": "HerO 2是HUMANE团队在FEVER-25研讨会上提出的系统，通过文档摘要和答案重构提升证据质量，优化真实性预测，并在计算限制下实现高效运行。", "motivation": "改进去年的最佳开源模型HerO，提升证据质量和系统性能，适应真实世界的事实验证需求。", "method": "采用文档摘要和答案重构优化证据，通过后训练量化优化预测，并集成更新的语言模型。", "result": "在排行榜上排名第二，且运行时间最短，展示了高效性和实际应用潜力。", "conclusion": "HerO 2在事实验证任务中表现出色，兼具高效性和实用性，代码已开源。"}}
{"id": "2507.11025", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11025", "abs": "https://arxiv.org/abs/2507.11025", "authors": ["Sung Ho Kang", "Hyun-Cheol Park"], "title": "Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion", "comment": null, "summary": "We present a novel framework for CBCT-to-MDCT translation, grounded in the\nSchrodinger Bridge (SB) formulation, which integrates GAN-derived priors with\nhuman-guided conditional diffusion. Unlike conventional GANs or diffusion\nmodels, our approach explicitly enforces boundary consistency between CBCT\ninputs and pseudo targets, ensuring both anatomical fidelity and perceptual\ncontrollability. Binary human feedback is incorporated via classifier-free\nguidance (CFG), effectively steering the generative process toward clinically\npreferred outcomes. Through iterative refinement and tournament-based\npreference selection, the model internalizes human preferences without relying\non a reward model. Subtraction image visualizations reveal that the proposed\nmethod selectively attenuates shade artifacts in key anatomical regions while\npreserving fine structural detail. Quantitative evaluations further demonstrate\nsuperior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical\ndatasets -- outperforming prior GAN- and fine-tuning-based feedback methods --\nwhile requiring only 10 sampling steps. These findings underscore the\neffectiveness and efficiency of our framework for real-time, preference-aligned\nmedical image translation.", "AI": {"tldr": "提出了一种基于Schrodinger Bridge的CBCT-to-MDCT转换框架，结合GAN先验和人类引导的条件扩散，确保解剖保真度和感知可控性。", "motivation": "解决传统GAN或扩散模型在医学图像转换中边界一致性和临床偏好对齐的不足。", "method": "结合GAN先验和人类反馈的条件扩散，通过迭代优化和锦标赛选择实现偏好学习。", "result": "在临床数据集上表现优异，RMSE、SSIM、LPIPS和Dice指标均优于现有方法，且仅需10步采样。", "conclusion": "该框架高效且有效，适用于实时、偏好对齐的医学图像转换。"}}
{"id": "2507.11241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11241", "abs": "https://arxiv.org/abs/2507.11241", "authors": ["Tobias Kern", "Leon Tolksdorf", "Christian Birkner"], "title": "Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors", "comment": null, "summary": "Physically reduced-scale vehicles are emerging to accelerate the development\nof advanced automated driving functions. In this paper, we investigate the\neffects of scaling on self-localization accuracy with visual and\nvisual-inertial algorithms using cameras and an inertial measurement unit\n(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms\nare selected, and datasets are chosen as a baseline for real-sized vehicles. A\ntest drive is conducted to record data of reduced-scale vehicles. We compare\nthe selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in\nterms of their pose accuracy against the ground-truth and against data from\nreal-sized vehicles. When comparing the implementation of the selected\nlocalization algorithms to real-sized vehicles, OpenVINS has the lowest average\nlocalization error. Although all selected localization algorithms have\noverlapping error ranges, OpenVINS also performs best when applied to a\nreduced-scale vehicle. When reduced-scale vehicles were compared to real-sized\nvehicles, minor differences were found in translational vehicle motion\nestimation accuracy. However, no significant differences were found when\ncomparing the estimation accuracy of rotational vehicle motion, allowing RSVRs\nto be used as testing platforms for self-localization algorithms.", "AI": {"tldr": "研究了物理缩小比例车辆对视觉和视觉-惯性自定位算法精度的影响，发现OpenVINS在缩小比例和真实尺寸车辆中表现最佳。", "motivation": "加速高级自动驾驶功能的开发，通过缩小比例车辆测试自定位算法的准确性。", "method": "选择ROS2兼容的视觉和视觉-惯性算法（OpenVINS、VINS-Fusion、RTAB-Map），记录缩小比例车辆数据并与真实尺寸车辆数据对比。", "result": "OpenVINS平均定位误差最低，缩小比例与真实尺寸车辆在平移运动估计上有微小差异，但旋转运动估计无显著差异。", "conclusion": "缩小比例车辆可作为自定位算法的测试平台，OpenVINS表现最优。"}}
{"id": "2507.11117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11117", "abs": "https://arxiv.org/abs/2507.11117", "authors": ["Ailiya Borjigin", "Cong He", "Charles CC Lee", "Wei Zhou"], "title": "AI Agent Architecture for Decentralized Trading of Alternative Assets", "comment": "8 Pages, 1 figure", "summary": "Decentralized trading of real-world alternative assets (e.g., gold) requires\nbridging physical asset custody with blockchain systems while meeting strict\nrequirements for compliance, liquidity, and risk management. We present\nGoldMine OS, a research oriented architecture that employs multiple specialized\nAI agents to automate and secure the tokenization and exchange of physical gold\ninto a blockchain based stablecoin (\"OZ\"). Our approach combines on chain smart\ncontracts for critical risk controls with off chain AI agents for decision\nmaking, blending the transparency and reliability of blockchains with the\nflexibility of AI driven automation. We describe four cooperative agents\n(Compliance, Token Issuance, Market Making, and Risk Control) and a\ncoordinating core, and evaluate the system through simulation and a controlled\npilot deployment. In experiments the prototype delivers on demand token\nissuance in under 1.2 s, more than 100 times faster than manual workflows. The\nMarket Making agent maintains tight liquidity with spreads often below 0.5\npercent even under volatile conditions. Fault injection tests show resilience:\nan oracle price spoofing attack is detected and mitigated within 10 s, and a\nsimulated vault mis reporting halts issuance immediately with minimal user\nimpact. The architecture scales to 5000 transactions per second with 10000\nconcurrent users in benchmarks. These results indicate that an AI agent based\ndecentralized exchange for alternative assets can satisfy rigorous performance\nand safety requirements. We discuss broader implications for democratizing\naccess to traditionally illiquid assets and explain how our governance model --\nmulti signature agent updates and on chain community voting on risk parameters\n-- provides ongoing transparency, adaptability, and formal assurance of system\nintegrity.", "AI": {"tldr": "GoldMine OS是一个研究导向的架构，利用多个专用AI代理自动化和保护实物黄金的令牌化及交易，结合区块链智能合约和AI驱动的决策，满足合规性、流动性和风险管理需求。", "motivation": "解决实物资产与区块链系统之间的桥梁问题，同时满足严格的合规性、流动性和风险管理要求。", "method": "采用多个协作AI代理（合规、令牌发行、做市和风险控制）和一个协调核心，结合链上智能合约和链下AI决策。", "result": "原型实现按需令牌发行时间低于1.2秒，做市代理在波动条件下保持0.5%以下的点差，系统在故障注入测试中表现出韧性。", "conclusion": "AI代理驱动的去中心化交易所能够满足严格的性能和安全性要求，并为传统非流动性资产的民主化访问提供了可能。"}}
{"id": "2507.11049", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11049", "abs": "https://arxiv.org/abs/2507.11049", "authors": ["Dahyun Lee", "Jonghyeon Choi", "Jiyoung Han", "Kunwoo Park"], "title": "Journalism-Guided Agentic In-Context Learning for News Stance Detection", "comment": "Preprint. 24 pages", "summary": "As online news consumption grows, personalized recommendation systems have\nbecome integral to digital journalism. However, these systems risk reinforcing\nfilter bubbles and political polarization by failing to incorporate diverse\nperspectives. Stance detection -- identifying a text's position on a target --\ncan help mitigate this by enabling viewpoint-aware recommendations and\ndata-driven analyses of media bias. Yet, existing stance detection research\nremains largely limited to short texts and high-resource languages. To address\nthese gaps, we introduce \\textsc{K-News-Stance}, the first Korean dataset for\narticle-level stance detection, comprising 2,000 news articles with\narticle-level and 19,650 segment-level stance annotations across 47 societal\nissues. We also propose \\textsc{JoA-ICL}, a \\textbf{Jo}urnalism-guided\n\\textbf{A}gentic \\textbf{I}n-\\textbf{C}ontext \\textbf{L}earning framework that\nemploys a language model agent to predict the stances of key structural\nsegments (e.g., leads, quotes), which are then aggregated to infer the overall\narticle stance. Experiments show that \\textsc{JoA-ICL} outperforms existing\nstance detection methods, highlighting the benefits of segment-level agency in\ncapturing the overall position of long-form news articles. Two case studies\nfurther demonstrate its broader utility in promoting viewpoint diversity in\nnews recommendations and uncovering patterns of media bias.", "AI": {"tldr": "论文介绍了首个韩语新闻立场检测数据集K-News-Stance，并提出JoA-ICL框架，通过段落级立场预测提升长文立场检测效果。", "motivation": "解决现有立场检测研究局限于短文本和高资源语言的问题，以促进新闻推荐中的观点多样性。", "method": "提出JoA-ICL框架，利用语言模型代理预测关键段落立场，再聚合推断全文立场。", "result": "JoA-ICL在立场检测上优于现有方法，案例研究显示其在新闻推荐和媒体偏见分析中的实用性。", "conclusion": "K-News-Stance和JoA-ICL为长文立场检测提供了有效工具，有助于增强新闻多样性和减少偏见。"}}
{"id": "2507.11030", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11030", "abs": "https://arxiv.org/abs/2507.11030", "authors": ["Sunghyun Park", "Jungsoo Lee", "Shubhankar Borse", "Munawar Hayat", "Sungha Choi", "Kyuwoong Hwang", "Fatih Porikli"], "title": "Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation", "comment": "Accepted to ICCV 2025; 15 pages", "summary": "While open-vocabulary semantic segmentation (OVSS) can segment an image into\nsemantic regions based on arbitrarily given text descriptions even for classes\nunseen during training, it fails to understand personal texts (e.g., `my mug\ncup') for segmenting regions of specific interest to users. This paper\naddresses challenges like recognizing `my mug cup' among `multiple mug cups'.\nTo overcome this challenge, we introduce a novel task termed\n\\textit{personalized open-vocabulary semantic segmentation} and propose a text\nprompt tuning-based plug-in method designed to recognize personal visual\nconcepts using a few pairs of images and masks, while maintaining the\nperformance of the original OVSS. Based on the observation that reducing false\npredictions is essential when applying text prompt tuning to this task, our\nproposed method employs `negative mask proposal' that captures visual concepts\nother than the personalized concept. We further improve the performance by\nenriching the representation of text prompts by injecting visual embeddings of\nthe personal concept into them. This approach enhances personalized OVSS\nwithout compromising the original OVSS performance. We demonstrate the\nsuperiority of our method on our newly established benchmarks for this task,\nincluding FSS$^\\text{per}$, CUB$^\\text{per}$, and ADE$^\\text{per}$.", "AI": {"tldr": "论文提出了一种个性化开放词汇语义分割任务，通过文本提示调优和负掩模提案解决现有方法无法处理用户特定文本（如“我的杯子”）的问题。", "motivation": "现有开放词汇语义分割方法无法识别用户个性化文本描述的特定区域（如“我的杯子”），需要解决这一问题。", "method": "提出基于文本提示调优的插件方法，结合负掩模提案和视觉嵌入注入，增强个性化概念识别能力。", "result": "在FSS$^\\text{per}$、CUB$^\\text{per}$和ADE$^\\text{per}$等新基准上验证了方法的优越性。", "conclusion": "该方法在不影响原始开放词汇语义分割性能的前提下，显著提升了个性化概念的识别能力。"}}
{"id": "2507.11270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11270", "abs": "https://arxiv.org/abs/2507.11270", "authors": ["Ting-Wei Ou", "Jia-Hao Jiang", "Guan-Lin Huang", "Kuu-Young Young"], "title": "Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection", "comment": "Accepted to the IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) 2025", "summary": "The COVID-19 pandemic has severely affected public health, healthcare\nsystems, and daily life, especially amid resource shortages and limited\nworkers. This crisis has underscored the urgent need for automation in hospital\nenvironments, particularly disinfection, which is crucial to controlling virus\ntransmission and improving the safety of healthcare personnel and patients.\nUltraviolet (UV) light disinfection, known for its high efficiency, has been\nwidely adopted in hospital settings. However, most existing research focuses on\nmaximizing UV coverage while paying little attention to the impact of human\nactivity on virus distribution. To address this issue, we propose a mobile\nrobotic system for UV disinfection focusing on the virus hotspot. The system\nprioritizes disinfection in high-risk areas and employs an approach for\noptimized UV dosage to ensure that all surfaces receive an adequate level of UV\nexposure while significantly reducing disinfection time. It not only improves\ndisinfection efficiency but also minimizes unnecessary exposure in low-risk\nareas. In two representative hospital scenarios, our method achieves the same\ndisinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,\nrespectively. The video of the experiment is available at:\nhttps://youtu.be/wHcWzOcoMPM.", "AI": {"tldr": "提出了一种针对病毒热点区域的移动机器人紫外线消毒系统，优化消毒剂量，显著减少消毒时间。", "motivation": "COVID-19疫情凸显了医院环境中自动化消毒的紧迫性，尤其是紫外线消毒的高效性。现有研究多关注紫外线覆盖，忽视了人类活动对病毒分布的影响。", "method": "设计了一种移动机器人系统，优先消毒高风险区域，优化紫外线剂量，确保所有表面充分暴露。", "result": "在两个医院场景中，消毒时间分别减少了30.7%和31.9%，同时保持了相同的消毒效果。", "conclusion": "该系统显著提高了消毒效率，减少了低风险区域的不必要暴露。"}}
{"id": "2507.11127", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11127", "abs": "https://arxiv.org/abs/2507.11127", "authors": ["Lennert De Smet", "Luc De Raedt"], "title": "Defining neurosymbolic AI", "comment": null, "summary": "Neurosymbolic AI focuses on integrating learning and reasoning, in\nparticular, on unifying logical and neural representations. Despite the\nexistence of an alphabet soup of neurosymbolic AI systems, the field is lacking\na generally accepted formal definition of what neurosymbolic models and\ninference really are. We introduce a formal definition for neurosymbolic AI\nthat makes abstraction of its key ingredients. More specifically, we define\nneurosymbolic inference as the computation of an integral over a product of a\nlogical and a belief function. We show that our neurosymbolic AI definition\nmakes abstraction of key representative neurosymbolic AI systems.", "AI": {"tldr": "论文提出了神经符号AI的形式化定义，抽象了其关键组成部分，并展示了该定义如何涵盖代表性系统。", "motivation": "神经符号AI领域缺乏统一的定义，本文旨在填补这一空白。", "method": "通过定义神经符号推理为逻辑函数和置信函数乘积的积分，抽象出关键要素。", "result": "提出的形式化定义能够涵盖代表性神经符号AI系统。", "conclusion": "该定义为神经符号AI提供了统一的理论基础。"}}
{"id": "2507.11052", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11052", "abs": "https://arxiv.org/abs/2507.11052", "authors": ["Haowei Yang", "Ziyu Shen", "Junli Shao", "Luyao Men", "Xinyue Han", "Jing Dong"], "title": "LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP", "comment": null, "summary": "Timely identification and accurate risk stratification of cardiovascular\ndisease (CVD) remain essential for reducing global mortality. While existing\nprediction models primarily leverage structured data, unstructured clinical\nnotes contain valuable early indicators. This study introduces a novel\nLLM-augmented clinical NLP pipeline that employs domain-adapted large language\nmodels for symptom extraction, contextual reasoning, and correlation from\nfree-text reports. Our approach integrates cardiovascular-specific fine-tuning,\nprompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III\nand CARDIO-NLP datasets demonstrate improved performance in precision, recall,\nF1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by\ncardiologists. Challenges such as contextual hallucination, which occurs when\nplausible information contracts with provided source, and temporal ambiguity,\nwhich is related with models struggling with chronological ordering of events\nare addressed using prompt engineering and hybrid rule-based verification. This\nwork underscores the potential of LLMs in clinical decision support systems\n(CDSS), advancing early warning systems and enhancing the translation of\npatient narratives into actionable risk assessments.", "AI": {"tldr": "该研究提出了一种基于LLM增强的临床NLP流程，用于从非结构化临床笔记中提取心血管疾病早期指标，并通过领域适应的大语言模型提升预测性能。", "motivation": "心血管疾病的及时识别和准确风险分层对降低全球死亡率至关重要，但现有预测模型主要依赖结构化数据，忽略了非结构化临床笔记中的有价值信息。", "method": "采用领域适应的大语言模型进行症状提取、上下文推理和相关分析，结合心血管特定微调、基于提示的推理和实体感知推理。", "result": "在MIMIC-III和CARDIO-NLP数据集上，模型在精确率、召回率、F1分数和AUROC方面表现优异，临床相关性高（kappa=0.82）。", "conclusion": "该研究展示了LLM在临床决策支持系统中的潜力，可提升早期预警系统并将患者叙述转化为可操作的风险评估。"}}
{"id": "2507.11035", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11035", "abs": "https://arxiv.org/abs/2507.11035", "authors": ["Lirong Zheng", "Yanshan Li", "Rui Yu", "Kaihao Zhang"], "title": "Efficient Dual-domain Image Dehazing with Haze Prior Perception", "comment": "12 pages", "summary": "Transformer-based models exhibit strong global modeling capabilities in\nsingle-image dehazing, but their high computational cost limits real-time\napplicability. Existing methods predominantly rely on spatial-domain features\nto capture long-range dependencies, which are computationally expensive and\noften inadequate under complex haze conditions. While some approaches introduce\nfrequency-domain cues, the weak coupling between spatial and frequency branches\nlimits the overall performance. To overcome these limitations, we propose the\nDark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel\ndual-domain framework that performs physically guided degradation alignment\nacross spatial and frequency domains. At its core, the DGFDBlock comprises two\nkey modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a\npixel-level haze confidence map from dark channel priors to adaptively enhance\nhaze-relevant frequency components, thereby achieving global degradation-aware\nspectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which\nfuses multi-scale features through diverse convolutional kernels and hybrid\ngating mechanisms to recover fine structural details. Additionally, a Prior\nCorrection Guidance Branch (PCGB) incorporates a closed-loop feedback\nmechanism, enabling iterative refinement of the prior by intermediate dehazed\nfeatures and significantly improving haze localization accuracy, especially in\nchallenging outdoor scenes. Extensive experiments on four benchmark haze\ndatasets demonstrate that DGFDNet achieves state-of-the-art performance with\nsuperior robustness and real-time efficiency. Code is available at:\nhttps://github.com/Dilizlr/DGFDNet.", "AI": {"tldr": "DGFDNet提出了一种双域去雾网络，结合空间和频率域信息，通过物理引导的退化对齐提升性能。", "motivation": "Transformer模型在单图像去雾中全局建模能力强，但计算成本高；现有方法依赖空间域特征，计算昂贵且效果有限。", "method": "DGFDNet包含HAFM模块（基于暗通道先验生成雾霾置信图）和MGAM模块（多尺度特征融合），并通过PCGB分支迭代优化先验。", "result": "在四个基准数据集上，DGFDNet实现了最优性能，兼具鲁棒性和实时性。", "conclusion": "DGFDNet通过双域框架和物理引导，显著提升了去雾效果和效率。"}}
{"id": "2507.11296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11296", "abs": "https://arxiv.org/abs/2507.11296", "authors": ["Huilin Xu", "Jian Ding", "Jiakun Xu", "Ruixiang Wang", "Jun Chen", "Jinjie Mai", "Yanwei Fu", "Bernard Ghanem", "Feng Xu", "Mohamed Elhoseiny"], "title": "Diffusion-Based Imaginative Coordination for Bimanual Manipulation", "comment": "15 pages, including 10 figures and 16 tables. Accepted at ICCV 2025", "summary": "Bimanual manipulation is crucial in robotics, enabling complex tasks in\nindustrial automation and household services. However, it poses significant\nchallenges due to the high-dimensional action space and intricate coordination\nrequirements. While video prediction has been recently studied for\nrepresentation learning and control, leveraging its ability to capture rich\ndynamic and behavioral information, its potential for enhancing bimanual\ncoordination remains underexplored. To bridge this gap, we propose a unified\ndiffusion-based framework for the joint optimization of video and action\nprediction. Specifically, we propose a multi-frame latent prediction strategy\nthat encodes future states in a compressed latent space, preserving\ntask-relevant features. Furthermore, we introduce a unidirectional attention\nmechanism where video prediction is conditioned on the action, while action\nprediction remains independent of video prediction. This design allows us to\nomit video prediction during inference, significantly enhancing efficiency.\nExperiments on two simulated benchmarks and a real-world setting demonstrate a\nsignificant improvement in the success rate over the strong baseline ACT using\nour method, achieving a \\textbf{24.9\\%} increase on ALOHA, an \\textbf{11.1\\%}\nincrease on RoboTwin, and a \\textbf{32.5\\%} increase in real-world experiments.\nOur models and code are publicly available at\nhttps://github.com/return-sleep/Diffusion_based_imaginative_Coordination.", "AI": {"tldr": "提出了一种基于扩散的统一框架，联合优化视频和动作预测，显著提高了双手机器人操作的成功率。", "motivation": "双手机器人操作在工业和家庭服务中至关重要，但高维动作空间和复杂协调需求带来挑战。视频预测的潜力在此领域尚未充分探索。", "method": "采用多帧潜在预测策略压缩未来状态，并设计单向注意力机制，视频预测依赖于动作，而动作预测独立于视频预测。", "result": "在模拟和真实实验中，成功率显著提升，ALOHA提高24.9%，RoboTwin提高11.1%，真实实验提高32.5%。", "conclusion": "提出的方法有效提升了双手机器人操作的协调能力，且通过省略视频预测提高了推理效率。"}}
{"id": "2507.11135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11135", "abs": "https://arxiv.org/abs/2507.11135", "authors": ["Selma Saidi", "Omar Laimona", "Christoph Schmickler", "Dirk Ziegenbein"], "title": "Collaborative Trustworthiness for Good Decision Making in Autonomous Systems", "comment": null, "summary": "Autonomous systems are becoming an integral part of many application domains,\nlike in the mobility sector. However, ensuring their safe and correct behaviour\nin dynamic and complex environments remains a significant challenge, where\nsystems should autonomously make decisions e.g., about manoeuvring. We propose\nin this paper a general collaborative approach for increasing the level of\ntrustworthiness in the environment of operation and improve reliability and\ngood decision making in autonomous system. In the presence of conflicting\ninformation, aggregation becomes a major issue for trustworthy decision making\nbased on collaborative data sharing. Unlike classical approaches in the\nliterature that rely on consensus or majority as aggregation rule, we exploit\nthe fact that autonomous systems have different quality attributes like\nperception quality. We use this criteria to determine which autonomous systems\nare trustworthy and borrow concepts from social epistemology to define\naggregation and propagation rules, used for automated decision making. We use\nBinary Decision Diagrams (BDDs) as formal models for beliefs aggregation and\npropagation, and formulate reduction rules to reduce the size of the BDDs and\nallow efficient computation structures for collaborative automated reasoning.", "AI": {"tldr": "提出了一种基于协作数据共享的自主系统可信决策方法，利用感知质量等属性确定可信系统，并通过BDD模型实现高效信念聚合与传播。", "motivation": "自主系统在动态复杂环境中确保安全可靠行为面临挑战，需提升其决策可信度。", "method": "利用感知质量等属性评估系统可信度，结合社会认识论定义聚合与传播规则，使用BDD模型进行信念聚合与传播。", "result": "通过BDD模型和简化规则，实现了高效协作自动推理。", "conclusion": "该方法提升了自主系统在复杂环境中的可信决策能力。"}}
{"id": "2507.11084", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11084", "abs": "https://arxiv.org/abs/2507.11084", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha"], "title": "Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach", "comment": "This paper has been accepted and presented at the IEEE ECAI 2025. The\n  final version will be available in the IEEE Xplore Digital Library", "summary": "The July Revolution in Bangladesh marked a significant student-led mass\nuprising, uniting people across the nation to demand justice, accountability,\nand systemic reform. Social media platforms played a pivotal role in amplifying\npublic sentiment and shaping discourse during this historic mass uprising. In\nthis study, we present a hybrid transformer-based sentiment analysis framework\nto decode public opinion expressed in social media comments during and after\nthe revolution. We used a brand new dataset of 4,200 Bangla comments collected\nfrom social media. The framework employs advanced transformer-based feature\nextraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the\nproposed hybrid XMB-BERT, to capture nuanced patterns in textual data.\nPrinciple Component Analysis (PCA) were utilized for dimensionality reduction\nto enhance computational efficiency. We explored eleven traditional and\nadvanced machine learning classifiers for identifying sentiments. The proposed\nhybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of\n83.7% and outperform other model classifier combinations. This study\nunderscores the potential of machine learning techniques to analyze social\nsentiment in low-resource languages like Bangla.", "AI": {"tldr": "该研究提出了一种基于混合Transformer的情感分析框架，用于分析孟加拉国七月革命期间社交媒体上的公众情绪，并展示了XMB-BERT模型的高效性。", "motivation": "研究旨在通过机器学习技术分析低资源语言（如孟加拉语）中的社会情绪，特别是在历史性事件如七月革命期间。", "method": "采用混合Transformer框架（包括BanglaBERT、mBERT、XLM-RoBERTa和XMB-BERT）进行特征提取，结合PCA降维和多种机器学习分类器进行情感分析。", "result": "提出的XMB-BERT与投票分类器组合达到了83.7%的准确率，优于其他模型。", "conclusion": "研究表明机器学习可以有效分析低资源语言中的社会情绪，为类似研究提供了新方法。"}}
{"id": "2507.11037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11037", "abs": "https://arxiv.org/abs/2507.11037", "authors": ["Jie-Wen Li", "Zi-Han Ye", "Qingyuan Zhou", "Jiayi Song", "Ying He", "Ben Fei", "Wen-Ming Chen"], "title": "A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion", "comment": "15 pages, 10 figures, 2 tables", "summary": "The kinematics analysis of foot-ankle complex during gait is essential for\nadvancing biomechanical research and clinical assessment. Collecting accurate\nsurface geometry data from the foot and ankle during dynamic gait conditions is\ninherently challenging due to swing foot occlusions and viewing limitations.\nThus, this paper introduces FootGait3D, a novel multi-view dataset of\nhigh-resolution ankle-foot surface point clouds captured during natural gait.\nDifferent from existing gait datasets that typically target whole-body or\nlower-limb motion, FootGait3D focuses specifically on the detailed modeling of\nthe ankle-foot region, offering a finer granularity of motion data. To address\nthis, FootGait3D consists of 8,403 point cloud frames collected from 46\nsubjects using a custom five-camera depth sensing system. Each frame includes a\ncomplete 5-view reconstruction of the foot and ankle (serving as ground truth)\nalong with partial point clouds obtained from only four, three, or two views.\nThis structured variation enables rigorous evaluation of 3D point cloud\ncompletion methods under varying occlusion levels and viewpoints. Our dataset\nis designed for shape completion tasks, facilitating the benchmarking of\nstate-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and\nmulti-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the\nchallenge of recovering the full foot geometry from occluded inputs. FootGait3D\nhas significant potential to advance research in biomechanics and multi-segment\nfoot modeling, offering a valuable testbed for clinical gait analysis,\nprosthetic design, and robotics applications requiring detailed 3D models of\nthe foot during motion. The dataset is now available at\nhttps://huggingface.co/datasets/ljw285/FootGait3D.", "AI": {"tldr": "FootGait3D是一个专注于足踝区域的高分辨率点云数据集，用于3D点云补全任务，支持生物力学研究和临床应用。", "motivation": "动态步态条件下足踝表面几何数据的准确采集具有挑战性，现有数据集通常关注全身或下肢运动，缺乏对足踝区域的详细建模。", "method": "使用自定义五相机深度传感系统采集46名受试者的8,403帧点云数据，包含完整5视角重建及部分视角点云。", "result": "FootGait3D数据集支持单模态和多模态补全网络的基准测试，为足部几何恢复提供挑战性数据。", "conclusion": "FootGait3D在生物力学、临床步态分析和机器人应用中具有重要潜力，数据集已公开。"}}
{"id": "2507.11302", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11302", "abs": "https://arxiv.org/abs/2507.11302", "authors": ["Jesse J. Hagenaars", "Stein Stroobants", "Sander M. Bohte", "Guido C. H. E. De Croon"], "title": "All Eyes, no IMU: Learning Flight Attitude from Vision Alone", "comment": null, "summary": "Vision is an essential part of attitude control for many flying animals, some\nof which have no dedicated sense of gravity. Flying robots, on the other hand,\ntypically depend heavily on accelerometers and gyroscopes for attitude\nstabilization. In this work, we present the first vision-only approach to\nflight control for use in generic environments. We show that a quadrotor drone\nequipped with a downward-facing event camera can estimate its attitude and\nrotation rate from just the event stream, enabling flight control without\ninertial sensors. Our approach uses a small recurrent convolutional neural\nnetwork trained through supervised learning. Real-world flight tests\ndemonstrate that our combination of event camera and low-latency neural network\nis capable of replacing the inertial measurement unit in a traditional flight\ncontrol loop. Furthermore, we investigate the network's generalization across\ndifferent environments, and the impact of memory and different fields of view.\nWhile networks with memory and access to horizon-like visual cues achieve best\nperformance, variants with a narrower field of view achieve better relative\ngeneralization. Our work showcases vision-only flight control as a promising\ncandidate for enabling autonomous, insect-scale flying robots.", "AI": {"tldr": "首次提出仅依赖视觉的飞行控制方法，使用事件相机和神经网络替代传统惯性传感器。", "motivation": "飞行机器人通常依赖惯性传感器，而许多飞行生物仅依赖视觉，研究旨在探索视觉控制的可行性。", "method": "使用下视事件相机和递归卷积神经网络，通过监督学习训练，实现姿态和旋转速率估计。", "result": "实验证明该方法可替代惯性测量单元，且不同视野和记忆配置影响性能和泛化能力。", "conclusion": "视觉控制为微型飞行机器人自主飞行提供了有前景的解决方案。"}}
{"id": "2507.11150", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.11150", "abs": "https://arxiv.org/abs/2507.11150", "authors": ["Alessandro Bertagnon", "Marcello Dalpasso", "Michele Favalli", "Marco Gavanelli"], "title": "Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming", "comment": "Accepted for publication in the issues of Theory and Practice of\n  Logic Programming (TPLP) dedicated to ICLP 2025, 16 pages, 9 figures", "summary": "In the design of integrated circuits, one critical metric is the maximum\ndelay introduced by combinational modules within the circuit. This delay is\ncrucial because it represents the time required to perform a computation: in an\nArithmetic-Logic Unit it represents the maximum time taken by the circuit to\nperform an arithmetic operation. When such a circuit is part of a larger,\nsynchronous system, like a CPU, the maximum delay directly impacts the maximum\nclock frequency of the entire system. Typically, hardware designers use Static\nTiming Analysis to compute an upper bound of the maximum delay because it can\nbe determined in polynomial time. However, relying on this upper bound can lead\nto suboptimal processor speeds, thereby missing performance opportunities. In\nthis work, we tackle the challenging task of computing the actual maximum\ndelay, rather than an approximate value. Since the problem is computationally\nhard, we model it in Answer Set Programming (ASP), a logic language featuring\nextremely efficient solvers. We propose non-trivial encodings of the problem\ninto ASP. Experimental results show that ASP is a viable solution to address\ncomplex problems in hardware design.", "AI": {"tldr": "论文提出了一种使用答案集编程（ASP）精确计算组合电路最大延迟的方法，以替代传统的静态时序分析，从而提高处理器性能。", "motivation": "传统的静态时序分析虽然能在多项式时间内计算出最大延迟的上界，但可能导致处理器性能未达最优。", "method": "将问题建模为答案集编程（ASP），并提出非平凡的编码方法。", "result": "实验结果表明，ASP能有效解决硬件设计中的复杂问题。", "conclusion": "ASP是一种可行的解决方案，可用于精确计算组合电路的最大延迟。"}}
{"id": "2507.11086", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11086", "abs": "https://arxiv.org/abs/2507.11086", "authors": ["Andres Azqueta-Gavaldón", "Joaquin Ramos Cosgrove"], "title": "Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification", "comment": null, "summary": "The growing prevalence of cross-border financial activities in global markets\nhas underscored the necessity of accurately identifying and classifying foreign\nentities. This practice is essential within the Spanish financial system for\nensuring robust risk management, regulatory adherence, and the prevention of\nfinancial misconduct. This process involves a labor-intensive entity-matching\ntask, where entities need to be validated against available reference sources.\nChallenges arise from linguistic variations, special characters, outdated\nnames, and changes in legal forms, complicating traditional matching algorithms\nlike Jaccard, cosine, and Levenshtein distances. These methods struggle with\ncontextual nuances and semantic relationships, leading to mismatches. To\naddress these limitations, we explore Large Language Models (LLMs) as a\nflexible alternative. LLMs leverage extensive training to interpret context,\nhandle abbreviations, and adapt to legal transitions. We evaluate traditional\nmethods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft\nCopilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.\nResults show traditional methods achieve accuracies over 92% but suffer high\nfalse positive rates (20-40%). Interface-based LLMs outperform, achieving\naccuracies above 93%, F1 scores exceeding 96%, and lower false positives\n(40-80%).", "AI": {"tldr": "论文探讨了在跨境金融活动中使用大型语言模型（LLMs）改进实体匹配的准确性和效率，相比传统方法表现更优。", "motivation": "跨境金融活动的增加需要更准确的实体识别和分类，传统方法因语言和语义问题表现不佳。", "method": "比较了传统算法（如Jaccard、余弦、Levenshtein距离）、Hugging Face的LLMs和接口型LLMs（如Microsoft Copilot、Qwen 2.5），并使用65个葡萄牙公司案例进行测试。", "result": "传统方法准确率超92%，但假阳性率高（20-40%）；接口型LLMs准确率超93%，F1分数超96%，假阳性率更低（40-80%）。", "conclusion": "LLMs在实体匹配中优于传统方法，尤其在处理语言和语义复杂性方面表现突出。"}}
{"id": "2507.11040", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11040", "abs": "https://arxiv.org/abs/2507.11040", "authors": ["Nicolas Drapier", "Aladine Chetouani", "Aurélien Chateigner"], "title": "Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery", "comment": "11 pages, 9 figures", "summary": "We present GLOD, a transformer-first architecture for object detection in\nhigh-resolution satellite imagery. GLOD replaces CNN backbones with a Swin\nTransformer for end-to-end feature extraction, combined with novel UpConvMixer\nblocks for robust upsampling and Fusion Blocks for multi-scale feature\nintegration. Our approach achieves 32.95\\% on xView, outperforming SOTA methods\nby 11.46\\%. Key innovations include asymmetric fusion with CBAM attention and a\nmulti-path head design capturing objects across scales. The architecture is\noptimized for satellite imagery challenges, leveraging spatial priors while\nmaintaining computational efficiency.", "AI": {"tldr": "GLOD是一种基于Transformer的架构，用于高分辨率卫星图像中的目标检测，通过Swin Transformer和新型模块实现高效特征提取和多尺度融合，性能优于现有方法。", "motivation": "解决高分辨率卫星图像中目标检测的挑战，提升检测精度和计算效率。", "method": "采用Swin Transformer替代CNN主干，结合UpConvMixer块和Fusion Blocks实现多尺度特征提取与融合，引入CBAM注意力和多路径头设计。", "result": "在xView数据集上达到32.95%的准确率，比现有最佳方法高出11.46%。", "conclusion": "GLOD通过创新的架构设计，显著提升了卫星图像目标检测的性能和效率。"}}
{"id": "2507.11345", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11345", "abs": "https://arxiv.org/abs/2507.11345", "authors": ["Oscar Lima", "Marc Vinci", "Sunandita Patra", "Sebastian Stock", "Joachim Hertzberg", "Martin Atzmueller", "Malik Ghallab", "Dana Nau", "Paolo Traverso"], "title": "Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM", "comment": "Accepted in ECMR 2025 conference", "summary": "Robotic task execution faces challenges due to the inconsistency between\nsymbolic planner models and the rich control structures actually running on the\nrobot. In this paper, we present the first physical deployment of an integrated\nactor-planner system that shares hierarchical operational models for both\nacting and planning, interleaving the Reactive Acting Engine (RAE) with an\nanytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile\nmanipulator in a real-world deployment for an object collection task. Our\nexperiments demonstrate robust task execution under action failures and sensor\nnoise, and provide empirical insights into the interleaved acting-and-planning\ndecision making process.", "AI": {"tldr": "论文提出了一种集成执行器-规划器系统（RAE+UPOM），通过共享层次化操作模型，实现了在真实机器人上的物理部署，展示了在动作失败和传感器噪声下的鲁棒任务执行。", "motivation": "解决符号规划器模型与实际机器人控制结构之间的不一致性问题。", "method": "结合Reactive Acting Engine（RAE）和UCT-like Monte Carlo规划器（UPOM），共享层次化操作模型，实现执行与规划的交替进行。", "result": "在真实移动机械臂上成功执行物体收集任务，展示了在动作失败和传感器噪声下的鲁棒性。", "conclusion": "RAE+UPOM系统为机器人任务执行提供了一种有效的集成解决方案，并揭示了执行与规划交替决策的实证见解。"}}
{"id": "2507.11229", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11229", "abs": "https://arxiv.org/abs/2507.11229", "authors": ["Jin Li", "Zezhong Ding", "Xike Xie"], "title": "DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion", "comment": null, "summary": "Knowledge graphs (KGs) are vital for enabling knowledge reasoning across\nvarious domains. Recent KG reasoning methods that integrate both global and\nlocal information have achieved promising results. However, existing methods\noften suffer from score over-smoothing, which blurs the distinction between\ncorrect and incorrect answers and hinders reasoning effectiveness. To address\nthis, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with\ndual-pathway global-local fusion. DuetGraph tackles over-smoothing by\nsegregating -- rather than stacking -- the processing of local (via message\npassing) and global (via attention) information into two distinct pathways,\npreventing mutual interference and preserving representational discrimination.\nIn addition, DuetGraph introduces a coarse-to-fine optimization, which\npartitions entities into high- and low-score subsets. This strategy narrows the\ncandidate space and sharpens the score gap between the two subsets, which\nalleviates over-smoothing and enhances inference quality. Extensive experiments\non various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)\nperformance, with up to an 8.7% improvement in reasoning quality and a\n1.8$\\times$ acceleration in training efficiency.", "AI": {"tldr": "DuetGraph 是一种双路径全局-局部融合的知识图谱推理方法，通过分离全局和局部信息处理路径解决分数过平滑问题，并通过粗到细优化提升推理质量和效率。", "motivation": "现有知识图谱推理方法因分数过平滑问题导致正确与错误答案区分度降低，影响推理效果。", "method": "提出 DuetGraph，采用双路径（全局注意力与局部消息传递）分离处理信息，并引入粗到细优化策略，将实体分为高低分数子集以减少候选空间。", "result": "在多个数据集上实现 SOTA 性能，推理质量提升 8.7%，训练效率加速 1.8 倍。", "conclusion": "DuetGraph 通过双路径融合和粗到细优化有效解决过平滑问题，显著提升知识图谱推理性能。"}}
{"id": "2507.11097", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11097", "abs": "https://arxiv.org/abs/2507.11097", "authors": ["Zichen Wen", "Jiashu Qu", "Dongrui Liu", "Zhiyuan Liu", "Ruixi Wu", "Yicun Yang", "Xiangqi Jin", "Haoyun Xu", "Xuyang Liu", "Weijia Li", "Chaochao Lu", "Jing Shao", "Conghui He", "Linfeng Zhang"], "title": "The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs", "comment": "21 pages, 9 figures, work in progress", "summary": "Diffusion-based large language models (dLLMs) have recently emerged as a\npowerful alternative to autoregressive LLMs, offering faster inference and\ngreater interactivity via parallel decoding and bidirectional modeling.\nHowever, despite strong performance in code generation and text infilling, we\nidentify a fundamental safety concern: existing alignment mechanisms fail to\nsafeguard dLLMs against context-aware, masked-input adversarial prompts,\nexposing novel vulnerabilities. To this end, we present DIJA, the first\nsystematic study and jailbreak attack framework that exploits unique safety\nweaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial\ninterleaved mask-text prompts that exploit the text generation mechanisms of\ndLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional\nmodeling drives the model to produce contextually consistent outputs for masked\nspans, even when harmful, while parallel decoding limits model dynamic\nfiltering and rejection sampling of unsafe content. This causes standard\nalignment mechanisms to fail, enabling harmful completions in alignment-tuned\ndLLMs, even when harmful behaviors or unsafe instructions are directly exposed\nin the prompt. Through comprehensive experiments, we demonstrate that DIJA\nsignificantly outperforms existing jailbreak methods, exposing a previously\noverlooked threat surface in dLLM architectures. Notably, our method achieves\nup to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior\nbaseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and\nby 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of\nharmful content in the jailbreak prompt. Our findings underscore the urgent\nneed for rethinking safety alignment in this emerging class of language models.\nCode is available at https://github.com/ZichenWen1/DIJA.", "AI": {"tldr": "论文提出DIJA框架，揭示扩散式大语言模型（dLLMs）的安全漏洞，展示其通过掩码文本提示绕过对齐机制的能力。", "motivation": "现有对齐机制无法保护dLLMs免受上下文感知的掩码输入对抗提示攻击，暴露了新的安全威胁。", "method": "提出DIJA框架，利用双向建模和并行解码的弱点，构造对抗性掩码文本提示。", "result": "DIJA在多个基准测试中显著优于现有方法，最高达到100%关键词攻击成功率。", "conclusion": "研究强调需重新思考dLLMs的安全对齐机制。"}}
{"id": "2507.11055", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11055", "abs": "https://arxiv.org/abs/2507.11055", "authors": ["Shuchang Ye", "Usman Naseem", "Mingyuan Meng", "Jinman Kim"], "title": "Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation", "comment": "Accepted to ICCV 2025", "summary": "Medical language-guided segmentation, integrating textual clinical reports as\nauxiliary guidance to enhance image segmentation, has demonstrated significant\nimprovements over unimodal approaches. However, its inherent reliance on paired\nimage-text input, which we refer to as ``textual reliance\", presents two\nfundamental limitations: 1) many medical segmentation datasets lack paired\nreports, leaving a substantial portion of image-only data underutilized for\ntraining; and 2) inference is limited to retrospective analysis of cases with\npaired reports, limiting its applicability in most clinical scenarios where\nsegmentation typically precedes reporting. To address these limitations, we\npropose ProLearn, the first Prototype-driven Learning framework for\nlanguage-guided segmentation that fundamentally alleviates textual reliance. At\nits core, in ProLearn, we introduce a novel Prototype-driven Semantic\nApproximation (PSA) module to enable approximation of semantic guidance from\ntextual input. PSA initializes a discrete and compact prototype space by\ndistilling segmentation-relevant semantics from textual reports. Once\ninitialized, it supports a query-and-respond mechanism which approximates\nsemantic guidance for images without textual input, thereby alleviating textual\nreliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG\ndemonstrate that ProLearn outperforms state-of-the-art language-guided methods\nwhen limited text is available.", "AI": {"tldr": "ProLearn提出了一种原型驱动的学习框架，通过语义近似模块减少对文本输入的依赖，提升了医学图像分割的性能。", "motivation": "现有医学语言引导的分割方法依赖成对的图像-文本输入，导致许多无文本数据无法利用，且限制了临床应用。", "method": "引入原型驱动的语义近似（PSA）模块，通过文本报告初始化原型空间，支持无文本图像的语义近似。", "result": "在多个数据集上，ProLearn在文本有限的情况下优于现有方法。", "conclusion": "ProLearn有效解决了文本依赖问题，提升了语言引导分割的适用性和性能。"}}
{"id": "2507.11402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11402", "abs": "https://arxiv.org/abs/2507.11402", "authors": ["Supun Dissanayaka", "Alexander Ferrein", "Till Hofmann", "Kosuke Nakajima", "Mario Sanz-Lopez", "Jesus Savage", "Daniel Swoboda", "Matteo Tschesche", "Wataru Uemura", "Tarik Viehmann", "Shohei Yasuda"], "title": "From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League", "comment": "RoboCup Symposium 2025", "summary": "The RoboCup Logistics League is a RoboCup competition in a smart factory\nscenario that has focused on task planning, job scheduling, and multi-agent\ncoordination. The focus on production logistics allowed teams to develop highly\ncompetitive strategies, but also meant that some recent developments in the\ncontext of smart manufacturing are not reflected in the competition, weakening\nits relevance over the years. In this paper, we describe the vision for the\nRoboCup Smart Manufacturing League, a new competition designed as a larger\nsmart manufacturing scenario, reflecting all the major aspects of a modern\nfactory. It will consist of several tracks that are initially independent but\ngradually combined into one smart manufacturing scenario. The new tracks will\ncover industrial robotics challenges such as assembly, human-robot\ncollaboration, and humanoid robotics, but also retain a focus on production\nlogistics. We expect the reenvisioned competition to be more attractive to\nnewcomers and well-tried teams, while also shifting the focus to current and\nfuture challenges of industrial robotics.", "AI": {"tldr": "RoboCup Logistics League 将升级为 RoboCup Smart Manufacturing League，以更全面地反映现代工厂的挑战。", "motivation": "现有竞赛未能涵盖智能制造的最新发展，削弱了其相关性。", "method": "设计新的竞赛场景，包含多个独立但逐步合并的赛道，涵盖工业机器人挑战。", "result": "新竞赛将更具吸引力，并聚焦当前和未来的工业机器人挑战。", "conclusion": "升级后的竞赛将提升相关性和吸引力，推动工业机器人领域的发展。"}}
{"id": "2507.11277", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11277", "abs": "https://arxiv.org/abs/2507.11277", "authors": ["Dany Moshkovich", "Sergey Zeltyn"], "title": "Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed within agentic\nsystems-collections of interacting, LLM-powered agents that execute complex,\nadaptive workflows using memory, tools, and dynamic planning. While enabling\npowerful new capabilities, these systems also introduce unique forms of\nuncertainty stemming from probabilistic reasoning, evolving memory states, and\nfluid execution paths. Traditional software observability and operations\npractices fall short in addressing these challenges.\n  This paper introduces AgentOps: a comprehensive framework for observing,\nanalyzing, optimizing, and automating operation of agentic AI systems. We\nidentify distinct needs across four key roles-developers, testers, site\nreliability engineers (SREs), and business users-each of whom engages with the\nsystem at different points in its lifecycle. We present the AgentOps Automation\nPipeline, a six-stage process encompassing behavior observation, metric\ncollection, issue detection, root cause analysis, optimized recommendations,\nand runtime automation. Throughout, we emphasize the critical role of\nautomation in managing uncertainty and enabling self-improving AI systems-not\nby eliminating uncertainty, but by taming it to ensure safe, adaptive, and\neffective operation.", "AI": {"tldr": "AgentOps框架为基于LLM的智能代理系统提供全面的操作管理，涵盖观察、分析、优化和自动化，以应对其独特的动态不确定性。", "motivation": "随着基于LLM的智能代理系统在复杂任务中的应用增加，传统软件操作实践无法有效管理其动态不确定性，需要新的解决方案。", "method": "提出AgentOps框架，包括六阶段自动化流程：行为观察、指标收集、问题检测、根因分析、优化建议和运行时自动化。", "result": "AgentOps为开发者、测试者、SRE和业务用户提供针对性的操作支持，通过自动化管理不确定性，提升系统安全性和适应性。", "conclusion": "AgentOps通过自动化框架有效管理智能代理系统的不确定性，支持其安全、自适应和高效运行。"}}
{"id": "2507.11112", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11112", "abs": "https://arxiv.org/abs/2507.11112", "authors": ["Sanhanat Sivapiromrat", "Caiqi Zhang", "Marco Basaldella", "Nigel Collier"], "title": "Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs", "comment": null, "summary": "Recent studies have shown that Large Language Models (LLMs) are vulnerable to\ndata poisoning attacks, where malicious training examples embed hidden\nbehaviours triggered by specific input patterns. However, most existing works\nassume a phrase and focus on the attack's effectiveness, offering limited\nunderstanding of trigger mechanisms and how multiple triggers interact within\nthe model. In this paper, we present a framework for studying poisoning in\nLLMs. We show that multiple distinct backdoor triggers can coexist within a\nsingle model without interfering with each other, enabling adversaries to embed\nseveral triggers concurrently. Using multiple triggers with high embedding\nsimilarity, we demonstrate that poisoned triggers can achieve robust activation\neven when tokens are substituted or separated by long token spans. Our findings\nexpose a broader and more persistent vulnerability surface in LLMs. To mitigate\nthis threat, we propose a post hoc recovery method that selectively retrains\nspecific model components based on a layer-wise weight difference analysis. Our\nmethod effectively removes the trigger behaviour with minimal parameter\nupdates, presenting a practical and efficient defence against multi-trigger\npoisoning.", "AI": {"tldr": "该论文研究了大型语言模型（LLMs）中的数据中毒攻击，揭示了多个触发器可以共存且不互相干扰，并提出了一种基于分层权重差异分析的后处理恢复方法。", "motivation": "现有研究主要关注攻击有效性，对触发器机制及多触发器交互的理解有限，因此需要更深入的研究。", "method": "提出一个框架研究LLMs中的中毒问题，展示多触发器共存现象，并通过分层权重差异分析设计后处理恢复方法。", "result": "发现多触发器可以共存且激活稳健，即使输入被替换或分隔；提出的恢复方法能高效移除触发器行为。", "conclusion": "LLMs存在广泛且持续的中毒漏洞，提出的后处理方法为多触发器中毒提供了实用防御方案。"}}
{"id": "2507.11061", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11061", "abs": "https://arxiv.org/abs/2507.11061", "authors": ["Hayeon Kim", "Ji Ha Jang", "Se Young Chun"], "title": "Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling", "comment": null, "summary": "Recent advances in 3D neural representations and instance-level editing\nmodels have enabled the efficient creation of high-quality 3D content. However,\nachieving precise local 3D edits remains challenging, especially for Gaussian\nSplatting, due to inconsistent multi-view 2D part segmentations and inherently\nambiguous nature of Score Distillation Sampling (SDS) loss. To address these\nlimitations, we propose RoMaP, a novel local 3D Gaussian editing framework that\nenables precise and drastic part-level modifications. First, we introduce a\nrobust 3D mask generation module with our 3D-Geometry Aware Label Prediction\n(3D-GALP), which uses spherical harmonics (SH) coefficients to model\nview-dependent label variations and soft-label property, yielding accurate and\nconsistent part segmentations across viewpoints. Second, we propose a\nregularized SDS loss that combines the standard SDS loss with additional\nregularizers. In particular, an L1 anchor loss is introduced via our Scheduled\nLatent Mixing and Part (SLaMP) editing method, which generates high-quality\npart-edited 2D images and confines modifications only to the target region\nwhile preserving contextual coherence. Additional regularizers, such as\nGaussian prior removal, further improve flexibility by allowing changes beyond\nthe existing context, and robust 3D masking prevents unintended edits.\nExperimental results demonstrate that our RoMaP achieves state-of-the-art local\n3D editing on both reconstructed and generated Gaussian scenes and objects\nqualitatively and quantitatively, making it possible for more robust and\nflexible part-level 3D Gaussian editing.", "AI": {"tldr": "RoMaP是一种新的局部3D高斯编辑框架，通过3D-GALP和正则化SDS损失实现精确的部件级修改。", "motivation": "解决现有方法在多视角2D部件分割和SDS损失模糊性方面的局限性，实现更精确的局部3D编辑。", "method": "提出3D-GALP模块生成鲁棒的3D掩码，并结合正则化SDS损失（包括L1锚定损失和其他正则化器）进行编辑。", "result": "实验表明，RoMaP在重建和生成的3D高斯场景中实现了最先进的局部编辑效果。", "conclusion": "RoMaP为3D高斯编辑提供了更鲁棒和灵活的部件级修改能力。"}}
{"id": "2507.11460", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11460", "abs": "https://arxiv.org/abs/2507.11460", "authors": ["Jacinto Colan", "Ana Davila", "Yutaro Yamada", "Yasuhisa Hasegawa"], "title": "Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants", "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Human-robot collaboration in surgery represents a significant area of\nresearch, driven by the increasing capability of autonomous robotic systems to\nassist surgeons in complex procedures. This systematic review examines the\nadvancements and persistent challenges in the development of autonomous\nsurgical robotic assistants (ASARs), focusing specifically on scenarios where\nrobots provide meaningful and active support to human surgeons. Adhering to the\nPRISMA guidelines, a comprehensive literature search was conducted across the\nIEEE Xplore, Scopus, and Web of Science databases, resulting in the selection\nof 32 studies for detailed analysis. Two primary collaborative setups were\nidentified: teleoperation-based assistance and direct hands-on interaction. The\nfindings reveal a growing research emphasis on ASARs, with predominant\napplications currently in endoscope guidance, alongside emerging progress in\nautonomous tool manipulation. Several key challenges hinder wider adoption,\nincluding the alignment of robotic actions with human surgeon preferences, the\nnecessity for procedural awareness within autonomous systems, the establishment\nof seamless human-robot information exchange, and the complexities of skill\nacquisition in shared workspaces. This review synthesizes current trends,\nidentifies critical limitations, and outlines future research directions\nessential to improve the reliability, safety, and effectiveness of human-robot\ncollaboration in surgical environments.", "AI": {"tldr": "本文系统综述了自主手术机器人助手（ASARs）的研究进展与挑战，重点关注其在复杂手术中为外科医生提供主动支持的场景。", "motivation": "推动自主机器人系统在手术中的协作能力，以提升复杂手术的效率和安全性。", "method": "遵循PRISMA指南，对IEEE Xplore、Scopus和Web of Science数据库中的32项研究进行了详细分析。", "result": "研究发现ASARs研究集中在内窥镜引导和自主工具操作，但面临机器人行为与外科医生偏好对齐、程序意识建立等挑战。", "conclusion": "综述总结了当前趋势，指出了关键限制，并提出了未来研究方向，以提高手术中人机协作的可靠性和安全性。"}}
{"id": "2507.11288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11288", "abs": "https://arxiv.org/abs/2507.11288", "authors": ["Théo Fagnoni", "Mahsun Altin", "Chia En Chung", "Phillip Kingston", "Alan Tuning", "Dana O. Mohamed", "Inès Adnani"], "title": "Opus: A Prompt Intention Framework for Complex Workflow Generation", "comment": "39 pages, 24 figures", "summary": "This paper introduces the Opus Prompt Intention Framework, designed to\nimprove complex Workflow Generation with instruction-tuned Large Language\nModels (LLMs). We propose an intermediate Intention Capture layer between user\nqueries and Workflow Generation, implementing the Opus Workflow Intention\nFramework, which consists of extracting Workflow Signals from user queries,\ninterpreting them into structured Workflow Intention objects, and generating\nWorkflows based on these Intentions. Our results show that this layer enables\nLLMs to produce logical and meaningful outputs that scale reliably as query\ncomplexity increases. On a synthetic benchmark of 1,000 multi-intent\nquery-Workflow(s) pairs, applying the Opus Prompt Intention Framework to\nWorkflow Generation yields consistent improvements in semantic Workflow\nsimilarity metrics. In this paper, we introduce the Opus Prompt Intention\nFramework by applying the concepts of Workflow Signal and Workflow Intention to\nLLM-driven Workflow Generation. We present a reproducible, customizable\nLLM-based Intention Capture system to extract Workflow Signals and Workflow\nIntentions from user queries. Finally, we provide empirical evidence that the\nproposed system significantly improves Workflow Generation quality compared to\ndirect generation from user queries, particularly in cases of Mixed Intention\nElicitation.", "AI": {"tldr": "论文提出了Opus Prompt Intention Framework，通过引入中间层（Intention Capture）提升基于LLM的复杂工作流生成质量。", "motivation": "解决用户查询直接生成工作流时逻辑性和扩展性不足的问题。", "method": "提出Opus Workflow Intention Framework，包括提取Workflow Signals、解析为结构化Workflow Intention对象，并基于此生成工作流。", "result": "在1000个多意图查询-工作流对的基准测试中，语义相似度指标显著提升。", "conclusion": "Opus框架显著提高了工作流生成质量，尤其在混合意图场景下表现优异。"}}
{"id": "2507.11114", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11114", "abs": "https://arxiv.org/abs/2507.11114", "authors": ["Seif Ahmed", "Mohamed T. Younes", "Abdelrahman Moustafa", "Abdelrahman Allam", "Hamza Moustafa"], "title": "MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models", "comment": null, "summary": "We present a robust ensemble-based system for multilingual multimodal\nreasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach\nintegrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption\nrefinement and consistency checks, and Gemini 2.5 Pro as a reasoner which\nhandles final answer selection, all coordinated through carefully engineered\nfew-shot and zero-shot prompts. We conducted an extensive ablation study,\ntraining several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,\nMistral) on an English dataset and its multilingual augmented version.\nAdditionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for\ncomparison and found it to substantially outperform the trained models. Prompt\ndesign also proved critical: enforcing concise, language-normalized formats and\nprohibiting explanatory text boosted model accuracy on the English validation\nset from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)\nachieved first place overall in the multilingual track with 81.4% accuracy, and\nled 11 out of 13 individual language tracks, with top results such as 95.07%\nfor Croatian and 92.12% for Italian. These findings highlight that lightweight\nOCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual\naugmentation, can outperform heavier end-to-end models in high-stakes,\nmultilingual educational settings.", "AI": {"tldr": "该论文提出了一种基于集成方法的鲁棒多语言多模态推理系统，在ImageCLEF 2025 EXAMS V挑战赛中表现优异。", "motivation": "解决多语言多模态推理任务中的性能问题，通过集成轻量级模型和优化提示设计提升准确率。", "method": "集成Gemini 2.5 Flash、Gemini 1.5 Pro和Gemini 2.5 Pro，通过少样本和零样本提示协调，并进行广泛的消融研究。", "result": "在官方排行榜上，系统在多语言赛道以81.4%的准确率获得第一名，并在13个语言赛道中领先11个。", "conclusion": "轻量级OCR-VLM集成与精确提示策略和跨语言增强结合，可在多语言教育场景中超越重量级端到端模型。"}}
{"id": "2507.11075", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2507.11075", "abs": "https://arxiv.org/abs/2507.11075", "authors": ["Chang Peng", "Yifei Zhou", "Huifeng Xi", "Shiqing Huang", "Chuangye Chen", "Jianming Yang", "Bao Yang", "Zhenyu Jiang"], "title": "Joint angle model based learning to refine kinematic human pose estimation", "comment": null, "summary": "Marker-free human pose estimation (HPE) has found increasing applications in\nvarious fields. Current HPE suffers from occasional errors in keypoint\nrecognition and random fluctuation in keypoint trajectories when analyzing\nkinematic human poses. The performance of existing deep learning-based models\nfor HPE refinement is considerably limited by inaccurate training datasets in\nwhich the keypoints are manually annotated. This paper proposed a novel method\nto overcome the difficulty through joint angle-based modeling. The key\ntechniques include: (i) A joint angle-based model of human pose, which is\nrobust to describe kinematic human poses; (ii) Approximating temporal variation\nof joint angles through high order Fourier series to get reliable \"ground\ntruth\"; (iii) A bidirectional recurrent network is designed as a\npost-processing module to refine the estimation of well-established HRNet.\nTrained with the high-quality dataset constructed using our method, the network\ndemonstrates outstanding performance to correct wrongly recognized joints and\nsmooth their spatiotemporal trajectories. Tests show that joint angle-based\nrefinement (JAR) outperforms the state-of-the-art HPE refinement network in\nchallenging cases like figure skating and breaking.", "AI": {"tldr": "提出了一种基于关节角度的标记自由人体姿态估计（HPE）优化方法，通过傅里叶级数和高品质数据集训练的双向循环网络，显著提升了姿态估计的准确性和平滑性。", "motivation": "现有HPE方法在关键点识别和轨迹平滑性上存在误差，且训练数据集标注不准确限制了深度学习模型的性能。", "method": "采用关节角度建模，利用高阶傅里叶级数近似关节角度变化，设计双向循环网络作为后处理模块优化HRNet的估计结果。", "result": "JAR方法在花样滑冰和霹雳舞等挑战性场景中优于现有HPE优化网络。", "conclusion": "基于关节角度的优化方法显著提升了HPE的准确性和鲁棒性。"}}
{"id": "2507.11464", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11464", "abs": "https://arxiv.org/abs/2507.11464", "authors": ["Ajay Shankar", "Keisuke Okumura", "Amanda Prorok"], "title": "LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control", "comment": "9 pages; under review for IEEE Robotics & Automation - Letters (RA-L)", "summary": "We propose a multi-robot control paradigm to solve point-to-point navigation\ntasks for a team of holonomic robots with access to the full environment\ninformation. The framework invokes two processes asynchronously at high\nfrequency: (i) a centralized, discrete, and full-horizon planner for computing\ncollision- and deadlock-free paths rapidly, leveraging recent advances in\nmulti-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal\ntrajectory controllers that ensure all robots independently follow their\nassigned paths reliably. This hierarchical shift in planning representation\nfrom (i) discrete and coupled to (ii) continuous and decoupled domains enables\nthe framework to maintain long-term scalable motion synthesis. As an\ninstantiation of this idea, we present LF, which combines a fast\nstate-of-the-art MAPF solver (LaCAM), and a robust feedback control stack\n(Freyja) for executing agile robot maneuvers. LF provides a robust and\nversatile mechanism for lifelong multi-robot navigation even under asynchronous\nand partial goal updates, and adapts to dynamic workspaces simply by quick\nreplanning. We present various multirotor and ground robot demonstrations,\nincluding the deployment of 15 real multirotors with random, consecutive target\nupdates while a person walks through the operational workspace.", "AI": {"tldr": "提出了一种多机器人控制框架，结合集中式路径规划和分散式轨迹控制，实现高效、可扩展的点对点导航。", "motivation": "解决多机器人在全环境信息下的点对点导航问题，同时确保路径无碰撞、无死锁，并适应动态环境。", "method": "采用分层框架：1）集中式离散路径规划（基于MAPF）；2）分散式连续轨迹控制。结合LaCAM和Freyja实现。", "result": "展示了15架真实多旋翼机器人的动态导航能力，适应异步目标更新和动态环境。", "conclusion": "该框架为多机器人导航提供了高效、可扩展的解决方案，适用于动态和复杂环境。"}}
{"id": "2507.11323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11323", "abs": "https://arxiv.org/abs/2507.11323", "authors": ["Xiang Yin", "Nico Potyka", "Antonio Rago", "Timotheus Kampik", "Francesca Toni"], "title": "Contestability in Quantitative Argumentation", "comment": null, "summary": "Contestable AI requires that AI-driven decisions align with human\npreferences. While various forms of argumentation have been shown to support\ncontestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks\n(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs\ncan be deployed for this purpose. Specifically, we introduce the contestability\nproblem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)\nto achieve a desired strength for a specific argument of interest (i.e., a\ntopic argument). To address this problem, we propose gradient-based relation\nattribution explanations (G-RAEs), which quantify the sensitivity of the topic\nargument's strength to changes in individual edge weights, thus providing\ninterpretable guidance for weight adjustments towards contestability. Building\non G-RAEs, we develop an iterative algorithm that progressively adjusts the\nedge weights to attain the desired strength. We evaluate our approach\nexperimentally on synthetic EW-QBAFs that simulate the structural\ncharacteristics of personalised recommender systems and multi-layer\nperceptrons, and demonstrate that it can solve the problem effectively.", "AI": {"tldr": "论文探讨了如何利用边加权定量双极论证框架（EW-QBAFs）实现可争议AI决策，提出梯度关系归因解释（G-RAEs）和迭代算法调整边权重以达到目标论证强度。", "motivation": "确保AI决策与人类偏好一致，但目前EW-QBAFs在支持可争议性方面研究较少。", "method": "提出G-RAEs量化边权重变化对目标论证强度的影响，并开发迭代算法调整权重。", "result": "在模拟个性化推荐系统和多层感知器的合成EW-QBAFs上验证了方法的有效性。", "conclusion": "G-RAEs和迭代算法能有效解决EW-QBAFs的可争议性问题。"}}
{"id": "2507.11128", "categories": ["cs.CL", "cs.CY", "cs.LG", "I.2.6; H.2.8"], "pdf": "https://arxiv.org/pdf/2507.11128", "abs": "https://arxiv.org/abs/2507.11128", "authors": ["Dimitri Staufer"], "title": "What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests", "comment": "16 pages, 3 figures. Accepted at the 7th Workshop on eXplainable\n  Knowledge Discovery in Data Mining (XKDD 2025), ECML PKDD 2025, Porto,\n  Portugal", "summary": "Large Language Models (LLMs) can memorize and reveal personal information,\nraising concerns regarding compliance with the EU's GDPR, particularly the\nRight to Be Forgotten (RTBF). Existing machine unlearning methods assume the\ndata to forget is already known but do not address how to identify which\nindividual-fact associations are stored in the model. Privacy auditing\ntechniques typically operate at the population level or target a small set of\nidentifiers, limiting applicability to individual-level data inquiries. We\nintroduce WikiMem, a dataset of over 5,000 natural language canaries covering\n243 human-related properties from Wikidata, and a model-agnostic metric to\nquantify human-fact associations in LLMs. Our approach ranks ground-truth\nvalues against counterfactuals using calibrated negative log-likelihood across\nparaphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B\nparameters), showing that memorization correlates with subject web presence and\nmodel scale. We provide a foundation for identifying memorized personal data in\nLLMs at the individual level, enabling the dynamic construction of forget sets\nfor machine unlearning and RTBF requests.", "AI": {"tldr": "论文提出WikiMem数据集和模型无关的指标，用于量化LLMs中个人-事实关联，支持动态构建遗忘集以满足GDPR的RTBF要求。", "motivation": "解决LLMs记忆和泄露个人信息的问题，特别是如何识别模型存储的个体-事实关联，以符合GDPR的RTBF规定。", "method": "引入WikiMem数据集（5000+自然语言提示）和模型无关的指标，通过校准负对数似然对真实值和反事实值进行排序。", "result": "评估了15个LLMs（410M-70B参数），发现记忆与个体网络存在和模型规模相关。", "conclusion": "为识别LLMs中个人数据记忆提供基础，支持动态构建遗忘集以满足RTBF请求。"}}
{"id": "2507.11077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11077", "abs": "https://arxiv.org/abs/2507.11077", "authors": ["Weizhao Ma", "Dong Zhou", "Yuhui Hu", "Zipeng He"], "title": "GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft", "comment": null, "summary": "Monocular pose estimation of non-cooperative spacecraft is significant for\non-orbit service (OOS) tasks, such as satellite maintenance, space debris\nremoval, and station assembly. Considering the high demands on pose estimation\naccuracy, mainstream monocular pose estimation methods typically consist of\nkeypoint detectors and PnP solver. However, current keypoint detectors remain\nvulnerable to structural symmetry and partial occlusion of non-cooperative\nspacecraft. To this end, we propose a graph-based keypoints network for the\nmonocular pose estimation of non-cooperative spacecraft, GKNet, which leverages\nthe geometric constraint of keypoints graph. In order to better validate\nkeypoint detectors, we present a moderate-scale dataset for the spacecraft\nkeypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000\nsimulated images, and corresponding high-precise keypoint annotations.\nExtensive experiments and an ablation study have demonstrated the high accuracy\nand effectiveness of our GKNet, compared to the state-of-the-art spacecraft\nkeypoint detectors. The code for GKNet and the SKD dataset is available at\nhttps://github.com/Dongzhou-1996/GKNet.", "AI": {"tldr": "论文提出了一种基于图的关键点网络（GKNet），用于非合作航天器的单目姿态估计，解决了结构对称性和部分遮挡问题，并发布了SKD数据集验证其有效性。", "motivation": "非合作航天器的单目姿态估计对在轨服务任务（如卫星维护、空间碎片清除）至关重要，但现有关键点检测器易受结构对称性和部分遮挡影响。", "method": "提出GKNet，利用关键点图的几何约束，并通过SKD数据集（包含3种航天器目标和9万张模拟图像）验证。", "result": "实验表明GKNet在精度和有效性上优于现有航天器关键点检测器。", "conclusion": "GKNet在非合作航天器姿态估计中表现出高精度和鲁棒性，代码和数据集已开源。"}}
{"id": "2507.11498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11498", "abs": "https://arxiv.org/abs/2507.11498", "authors": ["Asad Ali Shahid", "Francesco Braghin", "Loris Roveda"], "title": "Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming", "comment": null, "summary": "Humanoid robots have seen remarkable advances in dexterity, balance, and\nlocomotion, yet their role in expressive domains, such as music performance,\nremains largely unexplored. Musical tasks, like drumming, present unique\nchallenges, including split-second timing, rapid contacts, and multi-limb\ncoordination over pieces lasting minutes. In this paper, we introduce Robot\nDrummer, a humanoid system capable of expressive, high-precision drumming\nacross a diverse repertoire of songs. We formulate humanoid drumming as\nsequential fulfillment of timed-contacts and transform drum scores in to a\nRhythmic Contact Chain. To handle the long-horizon nature of musical\nperformance, we decompose each piece into fixed-length segments and train a\nsingle policy across all segments in parallel using reinforcement learning.\nThrough extensive experiments on over thirty popular rock, metal, and jazz\ntracks, our results demonstrate that Robot Drummer consistently achieves high\nF1 scores. The learned behaviors exhibit emergent human-like drumming\nstrategies, such as cross-arm strikes, and adaptive sticks assignments,\ndemonstrating the potential of reinforcement learning to bring humanoid robots\ninto the domain of creative musical performance. Project page:\n\\href{https://robot-drummer.github.io}{robot-drummer.github.io}", "AI": {"tldr": "论文介绍了一个名为Robot Drummer的人形机器人系统，能够通过强化学习实现高精度、富有表现力的鼓乐演奏。", "motivation": "探索人形机器人在音乐表演等表达性领域的潜力，解决鼓乐演奏中的快速反应和多肢协调等挑战。", "method": "将鼓乐演奏建模为时序接触任务，分解乐曲为固定长度片段，并通过强化学习训练单一策略。", "result": "在三十多首流行摇滚、金属和爵士乐曲中，Robot Drummer表现出高F1分数，并展现出类似人类的鼓乐策略。", "conclusion": "研究表明强化学习能有效推动人形机器人进入创造性音乐表演领域。"}}
{"id": "2507.11334", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11334", "abs": "https://arxiv.org/abs/2507.11334", "authors": ["Yuehao Huang", "Liang Liu", "Shuangming Lei", "Yukai Ma", "Hao Su", "Jianbiao Mei", "Pengxiang Zhao", "Yaqing Gu", "Yong Liu", "Jiajun Lv"], "title": "CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking", "comment": "Accepted by ACM MM 2025", "summary": "Mobile robots are increasingly required to navigate and interact within\nunknown and unstructured environments to meet human demands. Demand-driven\nnavigation (DDN) enables robots to identify and locate objects based on\nimplicit human intent, even when object locations are unknown. However,\ntraditional data-driven DDN methods rely on pre-collected data for model\ntraining and decision-making, limiting their generalization capability in\nunseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that\nemulates the human cognitive and learning mechanisms by integrating fast and\nslow thinking systems and selectively identifying key objects essential to\nfulfilling user demands. CogDDN identifies appropriate target objects by\nsemantically aligning detected objects with the given instructions.\nFurthermore, it incorporates a dual-process decision-making module, comprising\na Heuristic Process for rapid, efficient decisions and an Analytic Process that\nanalyzes past errors, accumulates them in a knowledge base, and continuously\nimproves performance. Chain of Thought (CoT) reasoning strengthens the\ndecision-making process. Extensive closed-loop evaluations on the AI2Thor\nsimulator with the ProcThor dataset show that CogDDN outperforms single-view\ncamera-only methods by 15%, demonstrating significant improvements in\nnavigation accuracy and adaptability. The project page is available at\nhttps://yuehaohuang.github.io/CogDDN/.", "AI": {"tldr": "CogDDN是一个基于视觉语言模型（VLM）的框架，通过模拟人类认知和学习机制，结合快速和慢速思考系统，提升机器人在未知环境中的导航和交互能力。", "motivation": "传统数据驱动的需求导航（DDN）方法依赖预收集数据，限制了在未见场景中的泛化能力。CogDDN旨在通过模拟人类认知机制解决这一问题。", "method": "CogDDN整合了快速和慢速思考系统，通过语义对齐检测对象与指令，并结合启发式和分析式决策模块。链式思考（CoT）推理增强了决策过程。", "result": "在AI2Thor模拟器和ProcThor数据集上的评估显示，CogDDN比单视图相机方法性能提升15%，导航准确性和适应性显著提高。", "conclusion": "CogDDN通过模拟人类认知机制，显著提升了机器人在未知环境中的导航能力，为需求导航提供了新的解决方案。"}}
{"id": "2507.11198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11198", "abs": "https://arxiv.org/abs/2507.11198", "authors": ["Conrad Borchers", "Bahar Shahrokhian", "Francesco Balzan", "Elham Tajik", "Sreecharan Sankaranarayanan", "Sebastian Simon"], "title": "Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding", "comment": "Manuscript submitted for review", "summary": "Large Language Models (LLMs) enable new possibilities for qualitative\nresearch at scale, including coding and data annotation. While multi-agent\nsystems (MAS) can emulate human coding workflows, their benefits over\nsingle-agent coding remain poorly understood. We conducted an experimental\nstudy of how agent persona and temperature shape consensus-building and coding\naccuracy of dialog segments based on a codebook with 8 codes. Our open-source\nMAS mirrors deductive human coding through structured agent discussion and\nconsensus arbitration. Using six open-source LLMs (with 3 to 32 billion\nparameters) and 18 experimental configurations, we analyze over 77,000 coding\ndecisions against a gold-standard dataset of human-annotated transcripts from\nonline math tutoring sessions. Temperature significantly impacted whether and\nwhen consensus was reached across all six LLMs. MAS with multiple personas\n(including neutral, assertive, or empathetic), significantly delayed consensus\nin four out of six LLMs compared to uniform personas. In three of those LLMs,\nhigher temperatures significantly diminished the effects of multiple personas\non consensus. However, neither temperature nor persona pairing lead to robust\nimprovements in coding accuracy. Single agents matched or outperformed MAS\nconsensus in most conditions. Only one model (OpenHermesV2:7B) and code\ncategory showed above-chance gains from MAS deliberation when temperature was\n0.5 or lower and especially when the agents included at least one assertive\npersona. Qualitative analysis of MAS collaboration for these configurations\nsuggests that MAS may nonetheless aid in narrowing ambiguous code applications\nthat could improve codebooks and human-AI coding. We contribute new insight\ninto the limits of LLM-based qualitative methods, challenging the notion that\ndiverse MAS personas lead to better outcomes. We open-source our MAS and\nexperimentation code.", "AI": {"tldr": "研究探讨了多代理系统（MAS）在定性研究中的表现，发现温度和代理角色对共识构建有显著影响，但未显著提升编码准确性。", "motivation": "探索多代理系统（MAS）在定性研究中的潜力，特别是其与单代理系统相比的优势和局限性。", "method": "通过实验研究，使用六种开源LLM和18种配置，分析77,000多个编码决策，评估温度和代理角色对共识和编码准确性的影响。", "result": "温度显著影响共识达成，多角色代理延迟共识，但未显著提升编码准确性。仅特定模型和条件下MAS表现优于单代理。", "conclusion": "研究揭示了LLM定性方法的局限性，挑战了多样MAS角色能提升结果的假设，并开源了实验代码。"}}
{"id": "2507.11081", "categories": ["cs.CV", "cs.AI", "I.4.9; I.5.4; J.2"], "pdf": "https://arxiv.org/pdf/2507.11081", "abs": "https://arxiv.org/abs/2507.11081", "authors": ["Chang Peng", "Bao Yang", "Meiqi Li", "Ge Zhang", "Hui Sun", "Zhenyu Jiang"], "title": "Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification", "comment": null, "summary": "Ground penetrating radar (GPR) has become a rapid and non-destructive\nsolution for road subsurface distress (RSD) detection. However, RSD recognition\nfrom GPR images is labor-intensive and heavily relies on inspectors' expertise.\nDeep learning offers the possibility for automatic RSD recognition, but its\ncurrent performance is limited by two factors: Scarcity of high-quality dataset\nfor network training and insufficient capability of network to distinguish RSD.\nIn this study, a rigorously validated 3D GPR dataset containing 2134 samples of\ndiverse types was constructed through field scanning. Based on the finding that\nthe YOLO model trained with one of the three scans of GPR images exhibits\nvarying sensitivity to specific type of RSD, we proposed a novel\ncross-verification strategy with outstanding accuracy in RSD recognition,\nachieving recall over 98.6% in field tests. The approach, integrated into an\nonline RSD detection system, can reduce the labor of inspection by around 90%.", "AI": {"tldr": "该论文提出了一种基于交叉验证策略的深度学习方法，用于从GPR图像中自动识别道路地下病害（RSD），显著提高了识别准确率并减少了人工工作量。", "motivation": "GPR图像中RSD识别依赖人工且效率低，深度学习因数据稀缺和网络能力不足而受限。", "method": "构建高质量3D GPR数据集，提出交叉验证策略优化YOLO模型，提升RSD识别能力。", "result": "现场测试中召回率超过98.6%，检测系统减少约90%人工工作量。", "conclusion": "该方法显著提升了RSD识别的自动化水平，具有实际应用价值。"}}
{"id": "2507.11525", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11525", "abs": "https://arxiv.org/abs/2507.11525", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "LLM-based ambiguity detection in natural language instructions for collaborative surgical robots", "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Ambiguity in natural language instructions poses significant risks in\nsafety-critical human-robot interaction, particularly in domains such as\nsurgery. To address this, we propose a framework that uses Large Language\nModels (LLMs) for ambiguity detection specifically designed for collaborative\nsurgical scenarios. Our method employs an ensemble of LLM evaluators, each\nconfigured with distinct prompting techniques to identify linguistic,\ncontextual, procedural, and critical ambiguities. A chain-of-thought evaluator\nis included to systematically analyze instruction structure for potential\nissues. Individual evaluator assessments are synthesized through conformal\nprediction, which yields non-conformity scores based on comparison to a labeled\ncalibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed\nclassification accuracy exceeding 60% in differentiating ambiguous from\nunambiguous surgical instructions. Our approach improves the safety and\nreliability of human-robot collaboration in surgery by offering a mechanism to\nidentify potentially ambiguous instructions before robot action.", "AI": {"tldr": "提出了一种基于大语言模型（LLMs）的框架，用于检测手术协作场景中的指令歧义，以提高人机交互的安全性和可靠性。", "motivation": "自然语言指令的歧义在安全关键的人机交互（如手术）中带来显著风险，需要一种方法来检测和解决这些歧义。", "method": "采用LLM评估器集合，结合不同的提示技术识别多种歧义类型，并通过共形预测综合评估结果。", "result": "在Llama 3.2 11B和Gemma 3 12B模型上，分类准确率超过60%，能够区分手术指令的歧义性。", "conclusion": "该框架为手术中的人机协作提供了识别潜在歧义指令的机制，从而提升安全性和可靠性。"}}
{"id": "2507.11352", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2507.11352", "abs": "https://arxiv.org/abs/2507.11352", "authors": ["Yunhao Yang", "Neel P. Bhatt", "Christian Ellis", "Alvaro Velasquez", "Zhangyang Wang", "Ufuk Topcu"], "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces", "comment": null, "summary": "Logistics operators, from battlefield coordinators rerouting airlifts ahead\nof a storm to warehouse managers juggling late trucks, often face life-critical\ndecisions that demand both domain expertise and rapid and continuous\nreplanning. While popular methods like integer programming yield logistics\nplans that satisfy user-defined logical constraints, they are slow and assume\nan idealized mathematical model of the environment that does not account for\nuncertainty. On the other hand, large language models (LLMs) can handle\nuncertainty and promise to accelerate replanning while lowering the barrier to\nentry by translating free-form utterances into executable plans, yet they\nremain prone to misinterpretations and hallucinations that jeopardize safety\nand cost. We introduce a neurosymbolic framework that pairs the accessibility\nof natural-language dialogue with verifiable guarantees on goal interpretation.\nIt converts user requests into structured planning specifications, quantifies\nits own uncertainty at the field and token level, and invokes an interactive\nclarification loop whenever confidence falls below an adaptive threshold. A\nlightweight model, fine-tuned on just 100 uncertainty-filtered examples,\nsurpasses the zero-shot performance of GPT-4.1 while cutting inference latency\nby nearly 50%. These preliminary results highlight a practical path toward\ncertifiable, real-time, and user-aligned decision-making for complex logistics.", "AI": {"tldr": "提出了一种结合自然语言对话与可验证保证的神经符号框架，用于复杂物流决策，提升实时性和安全性。", "motivation": "解决物流决策中传统方法（如整数规划）速度慢且忽略不确定性，以及大语言模型（LLMs）易误解和幻觉的问题。", "method": "开发了一种神经符号框架，将用户请求转化为结构化规划规范，量化不确定性，并在置信度不足时启动交互式澄清循环。", "result": "仅用100个不确定性过滤示例微调的轻量模型，性能超越GPT-4.1，推理延迟降低近50%。", "conclusion": "该框架为复杂物流提供了可验证、实时且用户对齐的决策路径。"}}
{"id": "2507.11216", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11216", "abs": "https://arxiv.org/abs/2507.11216", "authors": ["Valle Ruiz-Fernández", "Mario Mina", "Júlia Falcão", "Luis Vasquez-Reina", "Anna Sallés", "Aitor Gonzalez-Agirre", "Olatz Perez-de-Viñaspre"], "title": "EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering", "comment": null, "summary": "Previous literature has largely shown that Large Language Models (LLMs)\nperpetuate social biases learnt from their pre-training data. Given the notable\nlack of resources for social bias evaluation in languages other than English,\nand for social contexts outside of the United States, this paper introduces the\nSpanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and\nCaBBQ). Based on the original BBQ, these two parallel datasets are designed to\nassess social bias across 10 categories using a multiple-choice QA setting, now\nadapted to the Spanish and Catalan languages and to the social context of\nSpain. We report evaluation results on different LLMs, factoring in model\nfamily, size and variant. Our results show that models tend to fail to choose\nthe correct answer in ambiguous scenarios, and that high QA accuracy often\ncorrelates with greater reliance on social biases.", "AI": {"tldr": "本文介绍了西班牙语和加泰罗尼亚语的偏见评估基准（EsBBQ和CaBBQ），用于评估大型语言模型（LLMs）在西班牙社会背景下的社会偏见表现。", "motivation": "现有研究多关注英语和美国社会背景下的偏见评估，缺乏其他语言和社会背景的资源，因此需要开发适用于西班牙和加泰罗尼亚语的评估工具。", "method": "基于原始BBQ数据集，设计了西班牙语和加泰罗尼亚语的平行数据集，采用多选题QA形式评估10类社会偏见。", "result": "评估结果显示，模型在模糊场景中难以选择正确答案，且高QA准确率常与依赖社会偏见相关。", "conclusion": "研究强调了开发多语言和社会背景偏见评估工具的重要性，并揭示了LLMs在非英语环境中的偏见问题。"}}
{"id": "2507.11085", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11085", "abs": "https://arxiv.org/abs/2507.11085", "authors": ["Tianchi Xu"], "title": "Atmos-Bench: 3D Atmospheric Structures for Climate Insight", "comment": null, "summary": "Atmospheric structure, represented by backscatter coefficients (BC) recovered\nfrom satellite LiDAR attenuated backscatter (ATB), provides a volumetric view\nof clouds, aerosols, and molecules, playing a critical role in human\nactivities, climate understanding, and extreme weather forecasting. Existing\nmethods often rely on auxiliary inputs and simplified physics-based\napproximations, and lack a standardized 3D benchmark for fair evaluation.\nHowever, such approaches may introduce additional uncertainties and\ninsufficiently capture realistic radiative transfer and atmospheric\nscattering-absorption effects. To bridge these gaps, we present Atmos-Bench:\nthe first 3D atmospheric benchmark, along with a novel FourCastX:\nFrequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)\ngenerates 921,600 image slices from 3D scattering volumes simulated at 532 nm\nand 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean\ntime steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC\nphysical constraints into the model architecture, promoting energy consistency\nduring restoration; (c) achieves consistent improvements on the Atmos-Bench\ndataset across both 355 nm and 532 nm bands, outperforming state-of-the-art\nbaseline models without relying on auxiliary inputs. Atmos-Bench establishes a\nnew standard for satellite-based 3D atmospheric structure recovery and paves\nthe way for deeper climate insight.", "AI": {"tldr": "论文提出了Atmos-Bench，首个3D大气基准数据集，以及FourCastX模型，用于恢复大气结构，无需辅助输入，性能优于现有方法。", "motivation": "现有方法依赖辅助输入和简化物理近似，缺乏标准化3D基准，可能引入不确定性且无法充分捕捉真实辐射传输和大气散射-吸收效应。", "method": "提出FourCastX模型，结合WRF和增强的COSP模拟器生成高质量3D散射体积数据，嵌入ATB-BC物理约束，确保能量一致性。", "result": "在Atmos-Bench数据集上，FourCastX在355 nm和532 nm波段均优于现有基线模型。", "conclusion": "Atmos-Bench为卫星3D大气结构恢复设定了新标准，有助于更深入的气候研究。"}}
{"id": "2507.11287", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11287", "abs": "https://arxiv.org/abs/2507.11287", "authors": ["An-Lun Liu", "Yu-Wei Chao", "Yi-Ting Chen"], "title": "Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers", "comment": "Accepted by ICCV 2025", "summary": "In this paper, we study task-oriented human grasp synthesis, a new grasp\nsynthesis task that demands both task and context awareness. At the core of our\nmethod is the task-aware contact maps. Unlike traditional contact maps that\nonly reason about the manipulated object and its relation with the hand, our\nenhanced maps take into account scene and task information. This comprehensive\nmap is critical for hand-object interaction, enabling accurate grasping poses\nthat align with the task. We propose a two-stage pipeline that first constructs\na task-aware contact map informed by the scene and task. In the subsequent\nstage, we use this contact map to synthesize task-oriented human grasps. We\nintroduce a new dataset and a metric for the proposed task to evaluate our\napproach. Our experiments validate the importance of modeling both scene and\ntask, demonstrating significant improvements over existing methods in both\ngrasp quality and task performance. See our project page for more details:\nhttps://hcis-lab.github.io/TOHGS/", "AI": {"tldr": "论文提出了一种任务导向的人体抓取合成方法，通过任务感知接触图结合场景和任务信息，显著提升了抓取质量和任务表现。", "motivation": "传统抓取合成方法仅考虑物体与手的关系，缺乏任务和场景的上下文信息，导致抓取效果不佳。", "method": "采用两阶段流程：首先生成任务感知接触图，随后基于该图合成任务导向的人体抓取姿势。", "result": "实验验证了结合场景和任务信息的重要性，新方法在抓取质量和任务表现上显著优于现有方法。", "conclusion": "任务感知接触图是提升抓取合成的关键，新方法为任务导向的抓取提供了有效解决方案。"}}
{"id": "2507.11467", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11467", "abs": "https://arxiv.org/abs/2507.11467", "authors": ["Daniel Nichols", "Konstantinos Parasyris", "Harshitha Menon", "Brian R. Bartoldson", "Giorgis Georgakoudis", "Tal Ben-Nun", "Abhinav Bhatele"], "title": "Modeling Code: Is Text All You Need?", "comment": null, "summary": "Code LLMs have become extremely popular recently for modeling source code\nacross a variety of tasks, such as generation, translation, and summarization.\nHowever, transformer-based models are limited in their capabilities to reason\nthrough structured, analytical properties of code, such as control and data\nflow. Previous work has explored the modeling of these properties with\nstructured data and graph neural networks. However, these approaches lack the\ngenerative capabilities and scale of modern LLMs. In this work, we introduce a\nnovel approach to combine the strengths of modeling both code as text and more\nstructured forms.", "AI": {"tldr": "提出了一种结合代码文本建模与结构化建模优势的新方法，以弥补现有LLMs在代码分析和生成能力上的不足。", "motivation": "现有基于Transformer的代码LLMs在分析代码的结构化属性（如控制流和数据流）方面能力有限，而结构化建模方法又缺乏生成能力和规模。", "method": "结合代码文本建模与结构化建模的方法。", "result": "未明确提及具体实验结果。", "conclusion": "新方法旨在弥补现有LLMs的不足，结合两者的优势。"}}
{"id": "2507.11222", "categories": ["cs.CL", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.11222", "abs": "https://arxiv.org/abs/2507.11222", "authors": ["Fares Wael", "Youssef Maklad", "Ali Hamdi", "Wael Elsersy"], "title": "An Agentic Flow for Finite State Machine Extraction using Prompt Chaining", "comment": null, "summary": "Finite-State Machines (FSMs) are critical for modeling the operational logic\nof network protocols, enabling verification, analysis, and vulnerability\ndiscovery. However, existing FSM extraction techniques face limitations such as\nscalability, incomplete coverage, and ambiguity in natural language\nspecifications. In this paper, we propose FlowFSM, a novel agentic framework\nthat leverages Large Language Models (LLMs) combined with prompt chaining and\nchain-of-thought reasoning to extract accurate FSMs from raw RFC documents.\nFlowFSM systematically processes protocol specifications, identifies state\ntransitions, and constructs structured rule-books by chaining agent outputs.\nExperimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM\nachieves high extraction precision while minimizing hallucinated transitions,\nshowing promising results. Our findings highlight the potential of agent-based\nLLM systems in the advancement of protocol analysis and FSM inference for\ncybersecurity and reverse engineering applications.", "AI": {"tldr": "FlowFSM是一种基于大型语言模型（LLMs）的框架，通过提示链和思维链推理从RFC文档中提取精确的有限状态机（FSM），解决了现有技术的局限性。", "motivation": "现有的FSM提取技术在可扩展性、覆盖范围和自然语言规范模糊性方面存在不足，需要更高效的解决方案。", "method": "FlowFSM结合LLMs、提示链和思维链推理，系统处理协议规范，识别状态转换并构建结构化规则书。", "result": "在FTP和RTSP协议上的实验表明，FlowFSM提取精度高，且减少了虚假转换。", "conclusion": "基于代理的LLM系统在协议分析和FSM推断方面具有潜力，适用于网络安全和逆向工程。"}}
{"id": "2507.11099", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11099", "abs": "https://arxiv.org/abs/2507.11099", "authors": ["Qiyang Wan", "Chengzhi Gao", "Ruiping Wang", "Xilin Chen"], "title": "A Survey on Interpretability in Visual Recognition", "comment": "20 pages, 7 figures, 2 tables. Under review", "summary": "In recent years, visual recognition methods have advanced significantly,\nfinding applications across diverse fields. While researchers seek to\nunderstand the mechanisms behind the success of these models, there is also a\ngrowing impetus to deploy them in critical areas like autonomous driving and\nmedical diagnostics to better diagnose failures, which promotes the development\nof interpretability research. This paper systematically reviews existing\nresearch on the interpretability of visual recognition models and proposes a\ntaxonomy of methods from a human-centered perspective. The proposed taxonomy\ncategorizes interpretable recognition methods based on Intent, Object,\nPresentation, and Methodology, thereby establishing a systematic and coherent\nset of grouping criteria for these XAI methods. Additionally, we summarize the\nrequirements for evaluation metrics and explore new opportunities enabled by\nrecent technologies, such as large multimodal models. We aim to organize\nexisting research in this domain and inspire future investigations into the\ninterpretability of visual recognition models.", "AI": {"tldr": "本文系统综述了视觉识别模型的可解释性研究，提出了一种以人为中心的分类法，并探讨了评估指标和新技术的机遇。", "motivation": "随着视觉识别模型在关键领域的应用增加，理解其机制和失败原因的需求推动了可解释性研究的发展。", "method": "提出了一种基于意图、对象、呈现和方法论的分类法，系统整理了现有XAI方法。", "result": "建立了视觉识别模型可解释性方法的系统性分类标准，并总结了评估指标需求。", "conclusion": "本文旨在组织现有研究并启发未来对视觉识别模型可解释性的探索。"}}
{"id": "2507.11473", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11473", "abs": "https://arxiv.org/abs/2507.11473", "authors": ["Tomek Korbak", "Mikita Balesni", "Elizabeth Barnes", "Yoshua Bengio", "Joe Benton", "Joseph Bloom", "Mark Chen", "Alan Cooney", "Allan Dafoe", "Anca Dragan", "Scott Emmons", "Owain Evans", "David Farhi", "Ryan Greenblatt", "Dan Hendrycks", "Marius Hobbhahn", "Evan Hubinger", "Geoffrey Irving", "Erik Jenner", "Daniel Kokotajlo", "Victoria Krakovna", "Shane Legg", "David Lindner", "David Luan", "Aleksander Mądry", "Julian Michael", "Neel Nanda", "Dave Orr", "Jakub Pachocki", "Ethan Perez", "Mary Phuong", "Fabien Roger", "Joshua Saxe", "Buck Shlegeris", "Martín Soto", "Eric Steinberger", "Jasmine Wang", "Wojciech Zaremba", "Bowen Baker", "Rohin Shah", "Vlad Mikulik"], "title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety", "comment": null, "summary": "AI systems that \"think\" in human language offer a unique opportunity for AI\nsafety: we can monitor their chains of thought (CoT) for the intent to\nmisbehave. Like all other known AI oversight methods, CoT monitoring is\nimperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows\npromise and we recommend further research into CoT monitorability and\ninvestment in CoT monitoring alongside existing safety methods. Because CoT\nmonitorability may be fragile, we recommend that frontier model developers\nconsider the impact of development decisions on CoT monitorability.", "AI": {"tldr": "AI系统通过人类语言“思考”为AI安全提供了独特机会，可监控其思维链（CoT）以防止不当行为。尽管CoT监控不完美，但前景广阔，建议进一步研究并投资于CoT监控。", "motivation": "探索AI系统通过语言思维链监控的可能性，以增强AI安全性。", "method": "提出通过监控AI的思维链（CoT）来检测其潜在不当行为。", "result": "CoT监控虽不完美，但显示出潜力，建议结合现有安全方法进一步研究。", "conclusion": "建议前沿模型开发者考虑开发决策对CoT可监控性的影响，以确保AI安全。"}}
{"id": "2507.11230", "categories": ["cs.CL", "68T50"], "pdf": "https://arxiv.org/pdf/2507.11230", "abs": "https://arxiv.org/abs/2507.11230", "authors": ["Lyzander Marciano Andrylie", "Inaya Rahmanisa", "Mahardika Krisna Ihsani", "Alfan Farizki Wicaksono", "Haryo Akbarianto Wibowo", "Alham Fikri Aji"], "title": "Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages", "comment": null, "summary": "Understanding the multilingual mechanisms of large language models (LLMs)\nprovides insight into how they process different languages, yet this remains\nchallenging. Existing studies often focus on individual neurons, but their\npolysemantic nature makes it difficult to isolate language-specific units from\ncross-lingual representations. To address this, we explore sparse autoencoders\n(SAEs) for their ability to learn monosemantic features that represent concrete\nand abstract concepts across languages in LLMs. While some of these features\nare language-independent, the presence of language-specific features remains\nunderexplored. In this work, we introduce SAE-LAPE, a method based on feature\nactivation probability, to identify language-specific features within the\nfeed-forward network. We find that many such features predominantly appear in\nthe middle to final layers of the model and are interpretable. These features\ninfluence the model's multilingual performance and language output and can be\nused for language identification with performance comparable to fastText along\nwith more interpretability. Our code is available at\nhttps://github.com/LyzanderAndrylie/language-specific-features .", "AI": {"tldr": "论文提出了一种基于稀疏自编码器（SAE）的方法SAE-LAPE，用于识别大型语言模型（LLM）中的语言特定特征，并发现这些特征对多语言性能和语言输出有影响。", "motivation": "理解LLM的多语言机制具有挑战性，现有研究难以从跨语言表示中分离出语言特定单元。", "method": "使用稀疏自编码器（SAE）和特征激活概率方法SAE-LAPE，识别前馈网络中的语言特定特征。", "result": "发现语言特定特征主要出现在模型的中间到最终层，且具有可解释性，可用于语言识别，性能与fastText相当。", "conclusion": "SAE-LAPE方法有效识别了语言特定特征，为理解LLM的多语言机制提供了新视角。"}}
{"id": "2507.11102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11102", "abs": "https://arxiv.org/abs/2507.11102", "authors": ["Jie Yang", "Wang Zeng", "Sheng Jin", "Lumin Xu", "Wentao Liu", "Chen Qian", "Zhen Li", "Ruimao Zhang"], "title": "KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model", "comment": "Extended Version of KptLLM. arXiv admin note: text overlap with\n  arXiv:2411.01846", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has revolutionized\nimage understanding by bridging textual and visual modalities. However, these\nmodels often struggle with capturing fine-grained semantic information, such as\nthe precise identification and analysis of object keypoints. Keypoints, as\nstructure-aware, pixel-level, and compact representations of objects,\nparticularly articulated ones, play a crucial role in applications such as\nfine-grained image analysis, object retrieval, and behavior recognition. In\nthis paper, we propose KptLLM++, a novel multimodal large language model that\nspecifically designed for generic keypoint comprehension through the\nintegration of diverse input modalities guided by user-defined instructions. By\nunifying keypoint detection across varied contexts, KptLLM++ establishes itself\nas an advanced interface, fostering more effective human-AI collaboration. The\nmodel is built upon a novel identify-then-detect paradigm, which first\ninterprets keypoint semantics and subsequently localizes their precise\npositions through a structured chain-of-thought reasoning mechanism. To push\nthe boundaries of performance, we have scaled up the training dataset to over\n500K samples, encompassing diverse objects, keypoint categories, image styles,\nand scenarios with complex occlusions. This extensive scaling enables KptLLM++\nto unlock its potential, achieving remarkable accuracy and generalization.\nComprehensive experiments on multiple keypoint detection benchmarks demonstrate\nits state-of-the-art performance, underscoring its potential as a unified\nsolution for fine-grained image understanding and its transformative\nimplications for human-AI interaction.", "AI": {"tldr": "KptLLM++是一种新型多模态大语言模型，专注于通用关键点理解，通过用户指令整合多种输入模态，实现高精度关键点检测。", "motivation": "现有MLLMs在捕捉细粒度语义信息（如关键点）方面表现不足，而关键点对细粒度图像分析等应用至关重要。", "method": "采用‘识别-检测’范式，先解释关键点语义，再通过结构化思维链机制定位其精确位置，并扩展训练数据集至50万样本。", "result": "在多个关键点检测基准测试中表现优异，展示了卓越的准确性和泛化能力。", "conclusion": "KptLLM++为细粒度图像理解提供了统一解决方案，并推动了人机交互的变革。"}}
{"id": "2507.11479", "categories": ["cs.AI", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11479", "abs": "https://arxiv.org/abs/2507.11479", "authors": ["Daniel Platnick", "Matti Gruener", "Marjan Alirezaie", "Kent Larson", "Dava J. Newman", "Hossein Rahnama"], "title": "Perspective-Aware AI in Extended Reality", "comment": "Accepted to the International Conference on eXtended Reality (2025),\n  12 pages, 3 figures", "summary": "AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive\nexperiences-yet current systems fall short due to shallow user modeling and\nlimited cognitive context. We introduce Perspective-Aware AI in Extended\nReality (PAiR), a foundational framework for integrating Perspective-Aware AI\n(PAi) with XR to enable interpretable, context-aware experiences grounded in\nuser identity. PAi is built on Chronicles: reasoning-ready identity models\nlearned from multimodal digital footprints that capture users' cognitive and\nexperiential evolution. PAiR employs these models in a closed-loop system\nlinking dynamic user states with immersive environments. We present PAiR's\narchitecture, detailing its modules and system flow, and demonstrate its\nutility through two proof-of-concept scenarios implemented in the Unity-based\nOpenDome engine. PAiR opens a new direction for human-AI interaction by\nembedding perspective-based identity models into immersive systems.", "AI": {"tldr": "PAiR框架通过整合Perspective-Aware AI与XR，实现基于用户身份的上下文感知体验，解决了当前系统用户建模浅层和认知上下文有限的问题。", "motivation": "当前AI增强的XR系统因用户建模浅层和认知上下文有限，无法提供真正自适应的沉浸式体验。", "method": "提出PAiR框架，利用Chronicles（多模态数字足迹学习到的身份模型）构建闭环系统，动态链接用户状态与沉浸式环境。", "result": "通过Unity-based OpenDome引擎实现两个概念验证场景，展示了PAiR的实用性。", "conclusion": "PAiR通过将基于视角的身份模型嵌入沉浸式系统，为人机交互开辟了新方向。"}}
{"id": "2507.11273", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11273", "abs": "https://arxiv.org/abs/2507.11273", "authors": ["Luohe Shi", "Zuchao Li", "Lefei Zhang", "Guoming Liu", "Baoyuan Qi", "Hai Zhao"], "title": "KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding", "comment": "To be published in The 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "Large language models (LLMs) based on Transformer Decoders have become the\npreferred choice for conversational generative AI. Despite the overall\nsuperiority of the Decoder architecture, the gradually increasing Key-Value\n(KV) cache during inference has emerged as a primary efficiency bottleneck,\nboth in aspects of memory consumption and data transfer bandwidth limitations.\nTo address these challenges, we propose a paradigm called KV-Latent. By\ndown-sampling the Key-Value vector dimensions into a latent space, we can\nsignificantly reduce the KV Cache footprint and improve inference speed, only\nwith a small amount of extra training, less than 1\\% of pre-training takes.\nBesides, we enhanced the stability of Rotary Positional Embedding applied on\nlower-dimensional vectors by modifying its frequency sampling mechanism,\navoiding noise introduced by higher frequencies while retaining position\nattenuation. Our experiments, including both models with Grouped Query\nAttention and those without, have yielded satisfactory results. Finally, we\nconducted comparative experiments to study the impact of separately reducing\nKey and Value components on model's performance. Our approach allows for the\nconstruction of more efficient language model systems, and opens the new\npossibility on KV Cache saving and efficient LLMs. Our code is available at\nhttps://github.com/ShiLuohe/KV-Latent.", "AI": {"tldr": "KV-Latent通过降采样Key-Value向量维度到潜在空间，显著减少KV Cache占用并提升推理速度，仅需少量额外训练。", "motivation": "解决Transformer Decoder在推理过程中KV Cache逐渐增加导致的内存和带宽效率瓶颈问题。", "method": "提出KV-Latent范式，降采样KV向量维度至潜在空间，改进Rotary Positional Embedding的频率采样机制以增强稳定性。", "result": "实验表明，KV-Latent能显著减少KV Cache占用并提升推理速度，且对模型性能影响较小。", "conclusion": "KV-Latent为构建高效语言模型系统提供了新思路，并展示了KV Cache优化的潜力。"}}
{"id": "2507.11116", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11116", "abs": "https://arxiv.org/abs/2507.11116", "authors": ["Md. Sabbir Hossen", "Md. Saiduzzaman", "Pabon Shaha", "Mostofa Kamal Nasir"], "title": "Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach", "comment": "This paper has been accepted at the IEEE QPAIN 2025. The final\n  version will be available in the IEEE Xplore Digital Library", "summary": "Jellyfish, a diverse group of gelatinous marine organisms, play a crucial\nrole in maintaining marine ecosystems but pose significant challenges for\nbiodiversity and conservation due to their rapid proliferation and ecological\nimpact. Accurate identification of jellyfish species is essential for\necological monitoring and management. In this study, we proposed a deep\nlearning framework for jellyfish species detection and classification using an\nunderwater image dataset. The framework integrates advanced feature extraction\ntechniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,\ncombined with seven traditional machine learning classifiers and three\nFeedforward Neural Network classifiers for precise species identification.\nAdditionally, we activated the softmax function to directly classify jellyfish\nspecies using the convolutional neural network models. The combination of the\nArtificial Neural Network with MobileNetV3 is our best-performing model,\nachieving an exceptional accuracy of 98%, significantly outperforming other\nfeature extractor-classifier combinations. This study demonstrates the efficacy\nof deep learning and hybrid frameworks in addressing biodiversity challenges\nand advancing species detection in marine environments.", "AI": {"tldr": "提出了一种基于深度学习的框架，用于水母物种的检测和分类，结合多种特征提取技术和分类器，最高准确率达98%。", "motivation": "水母在海洋生态系统中扮演重要角色，但其快速增殖和生态影响对生物多样性和保护构成挑战，准确识别水母物种对生态监测和管理至关重要。", "method": "整合MobileNetV3、ResNet50、EfficientNetV2-B0和VGG16等特征提取技术，结合传统机器学习分类器和前馈神经网络分类器，并使用softmax函数直接分类。", "result": "结合MobileNetV3的人工神经网络模型表现最佳，准确率达98%，显著优于其他组合。", "conclusion": "研究表明深度学习和混合框架在解决生物多样性挑战和推动海洋物种检测方面具有高效性。"}}
{"id": "2507.11482", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11482", "abs": "https://arxiv.org/abs/2507.11482", "authors": ["Mani Hamidi", "Terrence W. Deacon"], "title": "Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light", "comment": null, "summary": "Three core tenets of reinforcement learning (RL)--concerning the definition\nof agency, the objective of learning, and the scope of the reward\nhypothesis--have been highlighted as key targets for conceptual revision, with\nmajor implications for theory and application. We propose a framework, inspired\nby open-ended evolutionary theory, to reconsider these three \"dogmas.\" We\nrevisit each assumption and address related concerns raised alongside them. To\nmake our arguments relevant to RL as a model of biological learning, we first\nestablish that evolutionary dynamics can plausibly operate within living brains\nover an individual's lifetime, and are not confined to cross-generational\nprocesses. We begin by revisiting the second dogma, drawing on evolutionary\ninsights to enrich the \"adaptation-rather-than-search\" view of learning. We\nthen address the third dogma regarding the limits of the reward hypothesis,\nusing analogies from evolutionary fitness to illuminate the scalar reward vs.\nmulti-objective debate. After discussing practical implications for exploration\nin RL, we turn to the first--and arguably most fundamental--issue: the absence\nof a formal account of agency. We argue that unlike the other two problems, the\nevolutionary paradigm alone cannot resolve the agency question, though it\ngestures in a productive direction. We advocate integrating ideas from\norigins-of-life theory, where the thermodynamics of sustenance and replication\noffer promising foundations for understanding agency and resource-constrained\nreinforcement learning in biological systems.", "AI": {"tldr": "论文提出一个基于开放进化理论的框架，重新审视强化学习的三个核心假设，并探讨其在理论和应用中的意义。", "motivation": "强化学习的三个核心假设（代理定义、学习目标和奖励假设范围）需要重新评估，以推动理论和应用的发展。", "method": "借鉴开放进化理论，重新审视这三个假设，并结合进化动力学和生命起源理论进行分析。", "result": "进化理论可以丰富强化学习的视角，但代理问题需要结合生命起源理论来解决。", "conclusion": "进化理论为强化学习提供了新的视角，但代理问题需进一步整合生命起源理论以完善理解。"}}
{"id": "2507.11275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11275", "abs": "https://arxiv.org/abs/2507.11275", "authors": ["Jiaxuan Xie", "Chengwu Liu", "Ye Yuan", "Siqi Li", "Zhiping Xiao", "Ming Zhang"], "title": "FMC: Formalization of Natural Language Mathematical Competition Problems", "comment": "Accepted in ICML 2025 AI4MATH Workshop", "summary": "Efficient and accurate autoformalization methods, which leverage large-scale\ndatasets of extensive natural language mathematical problems to construct\nformal language datasets, are key to advancing formal mathematical reasoning.\nIn this paper, we propose an autoformalization pipeline based on large language\nmodels with error feedback, achieving a fully automatic and training-free\nformalization approach. Using this pipeline, we curate an Olympiad-level\ndataset aligning natural language problems with Lean formalizations. The\ndataset comprises $3,922$ mathematical problems in natural language and $9,787$\nin Lean, of which $64.46\\%$ were assessed as at least above-average quality,\nmaking it suitable as a benchmark for automated theorem provers. Additionally,\nwe investigate the formalization and reasoning capabilities of various LLMs and\nempirically demonstrate that few-shot learning, error feedback, and increasing\nsampling numbers enhance the autoformalization process. Experiments of three\nautomated theorem provers on the \\dataset\\ dataset also highlight its\nchallenging nature and its value as a benchmark for formal reasoning tasks.", "AI": {"tldr": "提出了一种基于大语言模型的自动形式化方法，通过错误反馈实现无需训练的形式化流程，并构建了一个高质量的奥林匹克数学数据集。", "motivation": "推动形式化数学推理的发展，需要高效准确的自动形式化方法。", "method": "基于大语言模型的自动形式化流程，结合错误反馈机制，构建自然语言与Lean形式化对齐的数据集。", "result": "构建了包含3,922个自然语言问题和9,787个Lean形式化问题的数据集，64.46%质量较高；实验证明错误反馈和增加采样数能提升形式化效果。", "conclusion": "该方法为形式化推理任务提供了有价值的基准，并展示了自动形式化的潜力。"}}
{"id": "2507.11119", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11119", "abs": "https://arxiv.org/abs/2507.11119", "authors": ["Hankun Liu", "Yujian Zhao", "Guanglin Niu"], "title": "Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID", "comment": null, "summary": "Hard samples pose a significant challenge in person re-identification (ReID)\ntasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent\nambiguity or similarity, coupled with the lack of explicit definitions, makes\nthem a fundamental bottleneck. These issues not only limit the design of\ntargeted learning strategies but also diminish the model's robustness under\nclothing or viewpoint changes. In this paper, we propose a novel\nmultimodal-guided Hard Sample Generation and Learning (HSGL) framework, which\nis the first effort to unify textual and visual modalities to explicitly\ndefine, generate, and optimize hard samples within a unified paradigm. HSGL\ncomprises two core components: (1) Dual-Granularity Hard Sample Generation\n(DGHSG), which leverages multimodal cues to synthesize semantically consistent\nsamples, including both coarse- and fine-grained hard positives and negatives\nfor effectively increasing the hardness and diversity of the training data. (2)\nHard Sample Adaptive Learning (HSAL), which introduces a hardness-aware\noptimization strategy that adjusts feature distances based on textual semantic\nlabels, encouraging the separation of hard positives and drawing hard negatives\ncloser in the embedding space to enhance the model's discriminative capability\nand robustness to hard samples. Extensive experiments on multiple CC-ReID\nbenchmarks demonstrate the effectiveness of our approach and highlight the\npotential of multimodal-guided hard sample generation and learning for robust\nCC-ReID. Notably, HSAL significantly accelerates the convergence of the\ntargeted learning procedure and achieves state-of-the-art performance on both\nPRCC and LTCC datasets. The code is available at\nhttps://github.com/undooo/TryHarder-ACMMM25.", "AI": {"tldr": "提出了一种多模态引导的难样本生成与学习框架（HSGL），通过结合文本和视觉模态，显式定义、生成和优化难样本，提升了服装变化行人重识别（CC-ReID）的性能。", "motivation": "难样本在行人重识别任务中具有挑战性，尤其是在服装变化的场景下。其模糊性和相似性限制了针对性学习策略的设计，并降低了模型的鲁棒性。", "method": "HSGL框架包含两个核心组件：1）双粒度难样本生成（DGHSG），利用多模态线索合成语义一致的样本；2）难样本自适应学习（HSAL），通过硬度感知优化策略调整特征距离。", "result": "在多个CC-ReID基准测试中表现优异，显著加速了学习过程，并在PRCC和LTCC数据集上达到了最先进的性能。", "conclusion": "多模态引导的难样本生成与学习为鲁棒的CC-ReID提供了有效解决方案，具有显著的性能提升和收敛加速效果。"}}
{"id": "2507.11527", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.11527", "abs": "https://arxiv.org/abs/2507.11527", "authors": ["Yinsheng Li", "Zhen Dong", "Yi Shao"], "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering", "comment": "Project page: https://github.com/Eason-Li-AIS/DrafterBench", "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.", "AI": {"tldr": "DrafterBench是一个用于评估LLM代理在土木工程图纸修订任务中的开源基准，包含12类任务、46个定制功能和1920个任务，旨在测试代理的多方面能力。", "motivation": "当前缺乏从工业角度系统评估LLM代理的基准，特别是在土木工程领域，DrafterBench填补了这一空白。", "method": "通过总结真实图纸文件中的任务，设计包含多种功能的测试集，评估代理的指令理解、知识利用和动态适应能力。", "result": "DrafterBench提供了任务准确性和错误统计的详细分析，揭示了代理的能力和改进方向。", "conclusion": "DrafterBench为LLM在工程应用中的集成提供了深入见解和改进目标，推动了自动化代理的发展。"}}
{"id": "2507.11292", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11292", "abs": "https://arxiv.org/abs/2507.11292", "authors": ["Zewen Bai", "Liang Yang", "Shengdi Yin", "Yuanyuan Sun", "Hongfei Lin"], "title": "Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks", "comment": null, "summary": "The proliferation of hate speech has inflicted significant societal harm,\nwith its intensity and directionality closely tied to specific targets and\narguments. In recent years, numerous machine learning-based methods have been\ndeveloped to detect hateful comments on online platforms automatically.\nHowever, research on Chinese hate speech detection lags behind, and\ninterpretability studies face two major challenges: first, the scarcity of\nspan-level fine-grained annotated datasets limits models' deep semantic\nunderstanding of hate speech; second, insufficient research on identifying and\ninterpreting coded hate speech restricts model explainability in complex\nreal-world scenarios. To address these, we make the following contributions:\n(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE\nToxiCN), the first span-level Chinese hate speech dataset, and evaluate the\nhate semantic understanding of existing models using it. (2) We conduct the\nfirst comprehensive study on Chinese coded hate terms, LLMs' ability to\ninterpret hate semantics. (3) We propose a method to integrate an annotated\nlexicon into models, significantly enhancing hate speech detection performance.\nOur work provides valuable resources and insights to advance the\ninterpretability of Chinese hate speech detection research.", "AI": {"tldr": "本文提出了首个中文仇恨言论的细粒度标注数据集（STATE ToxiCN），并研究了编码仇恨言论的识别与解释，同时提出了一种整合标注词典的方法以提升检测性能。", "motivation": "中文仇恨言论检测研究滞后，且缺乏细粒度标注数据集和编码仇恨言论的解释性研究，限制了模型的深度语义理解和解释能力。", "method": "(1) 构建首个中文细粒度仇恨言论数据集STATE ToxiCN；(2) 研究编码仇恨术语及大语言模型的解释能力；(3) 提出整合标注词典的方法。", "result": "(1) 评估了现有模型对仇恨语义的理解；(2) 显著提升了仇恨言论检测性能；(3) 提供了资源和见解以推动中文仇恨言论检测的可解释性研究。", "conclusion": "本文填补了中文仇恨言论检测研究的空白，为模型的可解释性和性能提升提供了重要支持。"}}
{"id": "2507.11129", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11129", "abs": "https://arxiv.org/abs/2507.11129", "authors": ["Zhifeng Gu", "Bing Wang"], "title": "MMOne: Representing Multiple Modalities in One Scene", "comment": "Accepted to ICCV 2025", "summary": "Humans perceive the world through multimodal cues to understand and interact\nwith the environment. Learning a scene representation for multiple modalities\nenhances comprehension of the physical world. However, modality conflicts,\narising from inherent distinctions among different modalities, present two\ncritical challenges: property disparity and granularity disparity. To address\nthese challenges, we propose a general framework, MMOne, to represent multiple\nmodalities in one scene, which can be readily extended to additional\nmodalities. Specifically, a modality modeling module with a novel modality\nindicator is proposed to capture the unique properties of each modality.\nAdditionally, we design a multimodal decomposition mechanism to separate\nmulti-modal Gaussians into single-modal Gaussians based on modality\ndifferences. We address the essential distinctions among modalities by\ndisentangling multimodal information into shared and modality-specific\ncomponents, resulting in a more compact and efficient multimodal scene\nrepresentation. Extensive experiments demonstrate that our method consistently\nenhances the representation capability for each modality and is scalable to\nadditional modalities. The code is available at\nhttps://github.com/Neal2020GitHub/MMOne.", "AI": {"tldr": "论文提出了一种名为MMOne的通用框架，用于解决多模态场景表示中的模态冲突问题，通过模态建模模块和多模态分解机制，实现了更紧凑高效的多模态表示。", "motivation": "人类通过多模态线索感知世界，但不同模态间的固有差异（如属性差异和粒度差异）带来了挑战，需要一种方法来解决这些冲突并提升多模态场景的理解能力。", "method": "提出MMOne框架，包括模态建模模块（使用模态指示器捕捉各模态独特属性）和多模态分解机制（将多模态高斯分布分解为单模态高斯分布）。", "result": "实验表明，该方法能显著提升各模态的表示能力，并可扩展到更多模态。", "conclusion": "MMOne框架有效解决了多模态场景表示中的模态冲突问题，提供了一种紧凑高效的解决方案。"}}
{"id": "2507.11538", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11538", "abs": "https://arxiv.org/abs/2507.11538", "authors": ["Daniel Jaroslawicz", "Brendan Whiting", "Parth Shah", "Karime Maamari"], "title": "How Many Instructions Can LLMs Follow at Once?", "comment": null, "summary": "Production-grade LLM systems require robust adherence to dozens or even\nhundreds of instructions simultaneously. However, the instruction-following\ncapabilities of LLMs at high instruction densities have not yet been\ncharacterized, as existing benchmarks only evaluate models on tasks with a\nsingle or few instructions. We introduce IFScale, a simple benchmark of 500\nkeyword-inclusion instructions for a business report writing task to measure\nhow instruction-following performance degrades as instruction density\nincreases. We evaluate 20 state-of-the-art models across seven major providers\nand find that even the best frontier models only achieve 68% accuracy at the\nmax density of 500 instructions. Our analysis reveals model size and reasoning\ncapability to correlate with 3 distinct performance degradation patterns, bias\ntowards earlier instructions, and distinct categories of instruction-following\nerrors. Our insights can help inform design of instruction-dense prompts in\nreal-world applications and highlight important performance-latency tradeoffs.\nWe open-source the benchmark and all results for further analysis at\nhttps://distylai.github.io/IFScale.", "AI": {"tldr": "IFScale是一个新基准，用于评估LLM在高密度指令下的表现，发现即使顶级模型在500条指令时准确率仅为68%。", "motivation": "现有基准仅评估少量指令的任务，无法反映生产级LLM系统需同时处理大量指令的实际情况。", "method": "引入IFScale基准，包含500条关键词包含指令，评估20个前沿模型在高指令密度下的表现。", "result": "最佳模型在500条指令时准确率为68%，模型规模和推理能力与性能下降模式相关。", "conclusion": "IFScale揭示了高密度指令下的性能瓶颈，为实际应用中的提示设计提供参考，并开源了基准和结果。"}}
{"id": "2507.11299", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11299", "abs": "https://arxiv.org/abs/2507.11299", "authors": ["Andrei Niculae", "Adrian Cosma", "Cosmin Dumitrache", "Emilian Rǎdoi"], "title": "Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian", "comment": "10 figures, 2 tables, 2 listings", "summary": "Text-based telemedicine has become increasingly common, yet the quality of\nmedical advice in doctor-patient interactions is often judged more on how\nadvice is communicated rather than its clinical accuracy. To address this, we\nintroduce Dr.Copilot , a multi-agent large language model (LLM) system that\nsupports Romanian-speaking doctors by evaluating and enhancing the presentation\nquality of their written responses. Rather than assessing medical correctness,\nDr.Copilot provides feedback along 17 interpretable axes. The system comprises\nof three LLM agents with prompts automatically optimized via DSPy. Designed\nwith low-resource Romanian data and deployed using open-weight models, it\ndelivers real-time specific feedback to doctors within a telemedicine platform.\nEmpirical evaluations and live deployment with 41 doctors show measurable\nimprovements in user reviews and response quality, marking one of the first\nreal-world deployments of LLMs in Romanian medical settings.", "AI": {"tldr": "Dr.Copilot是一个多代理大型语言模型系统，旨在提升罗马尼亚语医生在文本远程医疗中的沟通质量，而非医学准确性。", "motivation": "解决远程医疗中医生建议的沟通质量而非临床准确性问题。", "method": "采用三个LLM代理，通过DSPy自动优化提示，基于低资源罗马尼亚数据设计，使用开源模型实时反馈。", "result": "实证评估和41名医生的实际部署显示用户评价和响应质量显著提升。", "conclusion": "Dr.Copilot是罗马尼亚医疗环境中首批实际部署的LLM系统之一，效果显著。"}}
{"id": "2507.11143", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.11143", "abs": "https://arxiv.org/abs/2507.11143", "authors": ["Lam Pham", "Cam Le", "Hieu Tang", "Khang Truong", "Truong Nguyen", "Jasmin Lampert", "Alexander Schindler", "Martin Boyer", "Son Phan"], "title": "RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images", "comment": null, "summary": "In recent years, landslide disasters have reported frequently due to the\nextreme weather events of droughts, floods , storms, or the consequence of\nhuman activities such as deforestation, excessive exploitation of natural\nresources. However, automatically observing landslide is challenging due to the\nextremely large observing area and the rugged topography such as mountain or\nhighland. This motivates us to propose an end-to-end deep-learning-based model\nwhich explores the remote sensing images for automatically observing landslide\nevents. By considering remote sensing images as the input data, we can obtain\nfree resource, observe large and rough terrains by time. To explore the remote\nsensing images, we proposed a novel neural network architecture which is for\ntwo tasks of landslide detection and landslide segmentation. We evaluated our\nproposed model on three different benchmark datasets of LandSlide4Sense, Bijie,\nand Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,\n93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU\nscores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,\nNepal datasets. These experimental results prove potential to integrate our\nproposed model into real-life landslide observation systems.", "AI": {"tldr": "提出了一种基于深度学习的端到端模型，利用遥感图像自动观测滑坡事件，并在多个数据集上取得了高精度结果。", "motivation": "滑坡灾害频发，但传统观测方法难以覆盖大范围和复杂地形，因此需要自动化解决方案。", "method": "设计了一种新型神经网络架构，用于滑坡检测和分割任务，输入为遥感图像。", "result": "在LandSlide4Sense、Bijie和Nepal数据集上，检测任务F1分数分别为98.23和93.83，分割任务mIoU分数为63.74和76.88。", "conclusion": "该模型具有实际应用潜力，可集成到滑坡观测系统中。"}}
{"id": "2507.11316", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11316", "abs": "https://arxiv.org/abs/2507.11316", "authors": ["Haoran Jin", "Meng Li", "Xiting Wang", "Zhihao Xu", "Minlie Huang", "Yantao Jia", "Defu Lian"], "title": "Internal Value Alignment in Large Language Models through Controlled Value Vector Activation", "comment": "25 pages, 14 figures. Accepted by ACL 2025 (main conference)", "summary": "Aligning Large Language Models (LLMs) with human values has attracted\nincreasing attention since it provides clarity, transparency, and the ability\nto adapt to evolving scenarios. In this paper, we introduce a Controlled Value\nVector Activation (ConVA) method that directly aligns the internal values of\nLLMs by interpreting how a value is encoded in their latent representations and\nmodifies relevant activations to ensure consistent values in LLMs. To ensure an\naccurate and unbiased interpretation, we propose a context-controlled value\nvector identification method. To consistently control values without\nsacrificing model performance, we introduce a gated value vector activation\nmethod for effective and minimum degree of value control. Experiments show that\nour method achieves the highest control success rate across 10 basic values\nwithout hurting LLM performance and fluency, and ensures target values even\nwith opposite and potentially malicious input prompts. Source code and data are\navailable at~ https://github.com/hr-jin/ConVA.", "AI": {"tldr": "提出了一种名为ConVA的方法，通过控制LLMs内部价值向量激活，直接对齐其潜在表示中的价值观，确保一致性。", "motivation": "随着LLMs的发展，如何使其与人类价值观对齐成为重要课题，以提供清晰、透明且适应变化的模型行为。", "method": "提出上下文控制的价值向量识别方法和门控价值向量激活方法，确保准确、无偏且不影响模型性能的价值控制。", "result": "实验表明，该方法在10种基本价值观上实现了最高的控制成功率，同时保持模型性能和流畅性。", "conclusion": "ConVA方法有效实现了LLMs与人类价值观的对齐，且能应对对立或恶意输入提示。"}}
{"id": "2507.11152", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11152", "abs": "https://arxiv.org/abs/2507.11152", "authors": ["Duoyou Chen", "Yunqing Chen", "Can Zhang", "Zhou Wang", "Cheng Chen", "Ruoxiu Xiao"], "title": "Latent Space Consistency for Sparse-View CT Reconstruction", "comment": "ACMMM2025 Accepted", "summary": "Computed Tomography (CT) is a widely utilized imaging modality in clinical\nsettings. Using densely acquired rotational X-ray arrays, CT can capture 3D\nspatial features. However, it is confronted with challenged such as significant\ntime consumption and high radiation exposure. CT reconstruction methods based\non sparse-view X-ray images have garnered substantial attention from\nresearchers as they present a means to mitigate costs and risks. In recent\nyears, diffusion models, particularly the Latent Diffusion Model (LDM), have\ndemonstrated promising potential in the domain of 3D CT reconstruction.\nNonetheless, due to the substantial differences between the 2D latent\nrepresentation of X-ray modalities and the 3D latent representation of CT\nmodalities, the vanilla LDM is incapable of achieving effective alignment\nwithin the latent space. To address this issue, we propose the Consistent\nLatent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature\ncontrastive learning to efficiently extract latent 3D information from 2D X-ray\nimages and achieve latent space alignment between modalities. Experimental\nresults indicate that CLS-DM outperforms classical and state-of-the-art\ngenerative models in terms of standard voxel-level metrics (PSNR, SSIM) on the\nLIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing\nthe effectiveness and economic viability of sparse X-ray reconstructed CT but\ncan also be generalized to other cross-modal transformation tasks, such as\ntext-to-image synthesis. We have made our code publicly available at\nhttps://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research\nand applications in other domains.", "AI": {"tldr": "提出了一种名为CLS-DM的新方法，通过跨模态特征对比学习，解决了2D X射线图像与3D CT图像在潜在空间对齐的问题，显著提升了稀疏视图CT重建的性能。", "motivation": "CT成像面临时间消耗大和辐射暴露高的问题，稀疏视图CT重建方法可以降低成本与风险，但现有方法在2D与3D模态的潜在空间对齐上存在不足。", "method": "提出CLS-DM模型，利用跨模态特征对比学习，从2D X射线图像中提取3D潜在信息，并实现模态间的潜在空间对齐。", "result": "在LIDC-IDRI和CTSpine1K数据集上，CLS-DM在PSNR和SSIM等指标上优于经典和最新的生成模型。", "conclusion": "CLS-DM不仅提升了稀疏视图CT重建的效果和经济性，还可推广到其他跨模态转换任务（如文本到图像合成）。代码已开源。"}}
{"id": "2507.11330", "categories": ["cs.CL", "cs.AI", "cs.DL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11330", "abs": "https://arxiv.org/abs/2507.11330", "authors": ["Wenqing Wu", "Chengzhi Zhang", "Yi Zhao"], "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge", "comment": "Journal of the Association for Information Science and Technology,\n  2025", "summary": "Novelty is a crucial criterion in the peer review process for evaluating\nacademic papers. Traditionally, it's judged by experts or measure by unique\nreference combinations. Both methods have limitations: experts have limited\nknowledge, and the effectiveness of the combination method is uncertain.\nMoreover, it's unclear if unique citations truly measure novelty. The large\nlanguage model (LLM) possesses a wealth of knowledge, while human experts\npossess judgment abilities that the LLM does not possess. Therefore, our\nresearch integrates the knowledge and abilities of LLM and human experts to\naddress the limitations of novelty assessment. The most common novelty in\nacademic papers is the introduction of new methods. In this paper, we propose\nleveraging human knowledge and LLM to assist pretrained language models (PLMs,\ne.g. BERT etc.) in predicting the method novelty of papers. Specifically, we\nextract sentences related to the novelty of the academic paper from peer review\nreports and use LLM to summarize the methodology section of the academic paper,\nwhich are then used to fine-tune PLMs. In addition, we have designed a\ntext-guided fusion module with novel Sparse-Attention to better integrate human\nand LLM knowledge. We compared the method we proposed with a large number of\nbaselines. Extensive experiments demonstrate that our method achieves superior\nperformance.", "AI": {"tldr": "论文提出了一种结合人类专家和大型语言模型（LLM）的方法，用于评估学术论文的方法新颖性，通过提取同行评审报告中的新颖性句子和LLM总结的方法部分，微调预训练语言模型（PLM），并设计了文本引导的融合模块，实验证明其性能优越。", "motivation": "传统新颖性评估方法（专家判断或引用组合）存在局限性，如专家知识有限、引用组合效果不确定。结合LLM的知识和人类专家的判断能力，可以弥补这些不足。", "method": "从同行评审报告中提取新颖性句子，利用LLM总结论文方法部分，用于微调PLM；设计文本引导的融合模块（Sparse-Attention）整合人类和LLM知识。", "result": "实验表明，该方法在评估方法新颖性方面优于大量基线模型。", "conclusion": "结合人类和LLM知识的方法能有效评估论文新颖性，为同行评审提供了新工具。"}}
{"id": "2507.11153", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11153", "abs": "https://arxiv.org/abs/2507.11153", "authors": ["Hongfei Ye", "Bin Chen", "Wenxi Liu", "Yu Zhang", "Zhao Li", "Dandan Ni", "Hongyang Chen"], "title": "Assessing Color Vision Test in Large Vision-language Models", "comment": null, "summary": "With the widespread adoption of large vision-language models, the capacity\nfor color vision in these models is crucial. However, the color vision\nabilities of large visual-language models have not yet been thoroughly\nexplored. To address this gap, we define a color vision testing task for large\nvision-language models and construct a dataset \\footnote{Anonymous Github\nShowing some of the data\nhttps://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers\nmultiple categories of test questions and tasks of varying difficulty levels.\nFurthermore, we analyze the types of errors made by large vision-language\nmodels and propose fine-tuning strategies to enhance their performance in color\nvision tests.", "AI": {"tldr": "本文研究了大型视觉语言模型的色彩视觉能力，提出了一个测试任务和数据集，并分析了错误类型及优化策略。", "motivation": "大型视觉语言模型的色彩视觉能力尚未被充分研究，本文旨在填补这一空白。", "method": "定义了色彩视觉测试任务，构建了多类别、多难度级别的数据集，并分析了模型的错误类型。", "result": "提出了针对色彩视觉测试的微调策略，以提升模型性能。", "conclusion": "本文为大型视觉语言模型的色彩视觉能力研究提供了基础，并提出了改进方向。"}}
{"id": "2507.11267", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11267", "abs": "https://arxiv.org/abs/2507.11267", "authors": ["Aon Safdar", "Usman Akram", "Waseem Anwar", "Basit Malik", "Mian Ibad Ali"], "title": "YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery", "comment": "Published in 25th Irish Machine Vision and Image Processing Conf.,\n  Galway, Ireland, Aug 30-Sep 1 2023 Also available at\n  https://doi.org/10.5281/zenodo.8264062", "summary": "Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared\n(TI) imagery in the defense and surveillance domain is a challenging computer\nvision (CV) task in comparison to the commercial autonomous vehicle perception\ndomain. Limited datasets, peculiar domain-specific and TI modality-specific\nchallenges, i.e., limited hardware, scale invariance issues due to greater\ndistances, deliberate occlusion by tactical vehicles, lower sensor resolution\nand resultant lack of structural information in targets, effects of weather,\ntemperature, and time of day variations, and varying target to clutter ratios\nall result in increased intra-class variability and higher inter-class\nsimilarity, making accurate real-time ATR a challenging CV task. Resultantly,\ncontemporary state-of-the-art (SOTA) deep learning architectures underperform\nin the ATR domain. We propose a modified anchor-based single-stage detector,\ncalled YOLOatr, based on a modified YOLOv5s, with optimal modifications to the\ndetection heads, feature fusion in the neck, and a custom augmentation profile.\nWe evaluate the performance of our proposed model on a comprehensive DSIAC MWIR\ndataset for real-time ATR over both correlated and decorrelated testing\nprotocols. The results demonstrate that our proposed model achieves\nstate-of-the-art ATR performance of up to 99.6%.", "AI": {"tldr": "论文提出了一种改进的单阶段检测器YOLOatr，用于热红外图像中的目标检测与识别，解决了现有深度学习模型在该领域的性能不足问题。", "motivation": "热红外图像在国防和监控领域的自动目标检测与识别任务面临诸多挑战，如数据集有限、硬件限制、天气影响等，导致现有深度学习模型表现不佳。", "method": "基于改进的YOLOv5s，优化了检测头、特征融合和自定义数据增强策略，提出了YOLOatr模型。", "result": "在DSIAC MWIR数据集上测试，YOLOatr实现了高达99.6%的识别准确率，优于现有方法。", "conclusion": "YOLOatr在热红外图像目标识别任务中表现出色，为实时应用提供了高效解决方案。"}}
{"id": "2507.11356", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11356", "abs": "https://arxiv.org/abs/2507.11356", "authors": ["Alexis Brissard", "Frédéric Cuppens", "Amal Zouaq"], "title": "What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models", "comment": "12 pages, 7 figures, to be published in AI4BPM 2025 Proceedings", "summary": "Large Language Models (LLMs) are increasingly applied for Process Modeling\n(PMo) tasks such as Process Model Generation (PMG). To support these tasks,\nresearchers have introduced a variety of Process Model Representations (PMRs)\nthat serve as model abstractions or generation targets. However, these PMRs\ndiffer widely in structure, complexity, and usability, and have never been\nsystematically compared. Moreover, recent PMG approaches rely on distinct\nevaluation strategies and generation techniques, making comparison difficult.\nThis paper presents the first empirical study that evaluates multiple PMRs in\nthe context of PMo with LLMs. We introduce the PMo Dataset, a new dataset\ncontaining 55 process descriptions paired with models in nine different PMRs.\nWe evaluate PMRs along two dimensions: suitability for LLM-based PMo and\nperformance on PMG. \\textit{Mermaid} achieves the highest overall score across\nsix PMo criteria, whereas \\textit{BPMN text} delivers the best PMG results in\nterms of process element similarity.", "AI": {"tldr": "本文首次对大型语言模型（LLMs）在流程建模（PMo）任务中使用的多种流程模型表示（PMRs）进行了系统比较，并评估了它们在流程模型生成（PMG）中的表现。", "motivation": "现有的PMRs在结构、复杂性和可用性上差异较大，且缺乏系统比较，导致PMG任务的评估和比较困难。", "method": "研究引入了包含55个流程描述和九种不同PMRs模型的PMo数据集，并从两个维度评估PMRs：LLM-based PMo的适用性和PMG性能。", "result": "Mermaid在六个PMo标准中得分最高，而BPMN text在流程元素相似性方面表现最佳。", "conclusion": "研究为PMRs的选择提供了实证依据，Mermaid和BPMN text分别在不同场景下表现最优。"}}
{"id": "2507.11171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11171", "abs": "https://arxiv.org/abs/2507.11171", "authors": ["Jun Chen", "Yonghua Yu", "Weifu Li", "Yaohui Chen", "Hong Chen"], "title": "Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification", "comment": "11 pages, 5 figures", "summary": "Citrus, as one of the most economically important fruit crops globally,\nsuffers severe yield depressions due to various diseases. Accurate disease\ndetection and classification serve as critical prerequisites for implementing\ntargeted control measures. Recent advancements in artificial intelligence,\nparticularly deep learning-based computer vision algorithms, have substantially\ndecreased time and labor requirements while maintaining the accuracy of\ndetection and classification. Nevertheless, these methods predominantly rely on\nmassive, high-quality annotated training examples to attain promising\nperformance. By introducing two key designs: contrasting with cluster centroids\nand a multi-layer contrastive training (MCT) paradigm, this paper proposes a\nnovel clustering-guided self-supervised multi-layer contrastive representation\nlearning (CMCRL) algorithm. The proposed method demonstrates several advantages\nover existing counterparts: (1) optimizing with massive unannotated samples;\n(2) effective adaptation to the symptom similarity across distinct citrus\ndiseases; (3) hierarchical feature representation learning. The proposed method\nachieves state-of-the-art performance on the public citrus image set CDD,\noutperforming existing methods by 4.5\\%-30.1\\% accuracy. Remarkably, our method\nnarrows the performance gap with fully supervised counterparts (all samples are\nlabeled). Beyond classification accuracy, our method shows great performance on\nother evaluation metrics (F1 score, precision, and recall), highlighting the\nrobustness against the class imbalance challenge.", "AI": {"tldr": "本文提出了一种名为CMCRL的自监督多层对比表示学习算法，用于柑橘病害检测与分类，显著提高了准确率并减少了标注数据需求。", "motivation": "柑橘病害严重影响产量，传统深度学习方法依赖大量标注数据，而CMCRL通过自监督学习减少标注需求并提升性能。", "method": "引入聚类中心对比和多层对比训练范式（MCT），优化未标注样本，适应症状相似性，实现分层特征学习。", "result": "在公开数据集CDD上，CMCRL比现有方法准确率提升4.5%-30.1%，接近全监督方法，且在F1分数、精确率和召回率上表现优异。", "conclusion": "CMCRL在减少标注数据需求的同时，显著提升了柑橘病害分类性能，具有实际应用潜力。"}}
{"id": "2507.11325", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11325", "abs": "https://arxiv.org/abs/2507.11325", "authors": ["Arefin Ittesafun Abian", "Ripon Kumar Debnath", "Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Md Rafiqul Islam", "Asif Karim", "Reem E. Mohamed", "Sami Azam"], "title": "HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging", "comment": "10 figures. Will be submitted to IEEE Transactions on Radiation and\n  Plasma Medical Sciences", "summary": "Accurate liver and tumor segmentation on abdominal CT images is critical for\nreliable diagnosis and treatment planning, but remains challenging due to\ncomplex anatomical structures, variability in tumor appearance, and limited\nannotated data. To address these issues, we introduce Hyperbolic-convolutions\nAdaptive-temporal-attention with Neural-representation and Synaptic-plasticity\nNetwork (HANS-Net), a novel segmentation framework that synergistically\ncombines hyperbolic convolutions for hierarchical geometric representation, a\nwavelet-inspired decomposition module for multi-scale texture learning, a\nbiologically motivated synaptic plasticity mechanism for adaptive feature\nenhancement, and an implicit neural representation branch to model fine-grained\nand continuous anatomical boundaries. Additionally, we incorporate\nuncertainty-aware Monte Carlo dropout to quantify prediction confidence and\nlightweight temporal attention to improve inter-slice consistency without\nsacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate\nthat HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an\naverage symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap\nerror (VOE) of 11.91%. Furthermore, cross-dataset validation on the\n3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of\n1.525 mm, and VOE of 19.71%, indicating strong generalization across different\ndatasets. These results confirm the effectiveness and robustness of HANS-Net in\nproviding anatomically consistent, accurate, and confident liver and tumor\nsegmentation.", "AI": {"tldr": "HANS-Net是一种新型肝脏和肿瘤分割框架，结合双曲卷积、多尺度纹理学习和生物启发的可塑性机制，显著提升了分割精度和泛化能力。", "motivation": "肝脏和肿瘤分割在CT图像中具有挑战性，主要由于复杂解剖结构、肿瘤外观多变和标注数据有限。", "method": "HANS-Net结合双曲卷积、小波分解模块、突触可塑性机制和隐式神经表示，并引入不确定性感知和轻量级时间注意力。", "result": "在LiTS数据集上，HANS-Net的Dice得分为93.26%，IoU为88.09%，ASSD为0.72 mm，VOE为11.91%。跨数据集验证也表现优异。", "conclusion": "HANS-Net在肝脏和肿瘤分割中表现出高效、准确和鲁棒性，适用于临床诊断和治疗规划。"}}
{"id": "2507.11384", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11384", "abs": "https://arxiv.org/abs/2507.11384", "authors": ["Xia Cui"], "title": "Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss", "comment": "10 pages, 1 figure, SemEval 2025", "summary": "This paper explores the application of a simple weighted loss function to\nTransformer-based models for multi-label emotion detection in SemEval-2025\nShared Task 11. Our approach addresses data imbalance by dynamically adjusting\nclass weights, thereby enhancing performance on minority emotion classes\nwithout the computational burden of traditional resampling methods. We evaluate\nBERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such\nas Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.\nThe results demonstrate that the weighted loss function improves performance on\nhigh-frequency emotion classes but shows limited impact on minority classes.\nThese findings underscore both the effectiveness and the challenges of applying\nthis approach to imbalanced multi-label emotion detection.", "AI": {"tldr": "论文探讨了在Transformer模型中应用加权损失函数进行多标签情感检测，动态调整类别权重以解决数据不平衡问题，并在BRIGHTER数据集上评估了BERT、RoBERTa和BART模型。", "motivation": "解决多标签情感检测中的数据不平衡问题，避免传统重采样方法的计算负担。", "method": "使用动态调整类别权重的加权损失函数，评估BERT、RoBERTa和BART模型在BRIGHTER数据集上的表现。", "result": "加权损失函数提高了高频情感类别的性能，但对少数类别的改善有限。", "conclusion": "该方法在多标签情感检测中有效，但仍面临处理少数类别的挑战。"}}
{"id": "2507.11200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11200", "abs": "https://arxiv.org/abs/2507.11200", "authors": ["Che Liu", "Jiazhen Pan", "Weixiang Shen", "Wenjia Bai", "Daniel Rueckert", "Rossella Arcucci"], "title": "How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study", "comment": "Accepted by the International Conference on AI in Healthcare 2025", "summary": "Vision-Language Models (VLMs) trained on web-scale corpora excel at natural\nimage tasks and are increasingly repurposed for healthcare; however, their\ncompetence in medical tasks remains underexplored. We present a comprehensive\nevaluation of open-source general-purpose and medically specialised VLMs,\nranging from 3B to 72B parameters, across eight benchmarks: MedXpert,\nOmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model\nperformance across different aspects, we first separate it into understanding\nand reasoning components. Three salient findings emerge. First, large\ngeneral-purpose models already match or surpass medical-specific counterparts\non several benchmarks, demonstrating strong zero-shot transfer from natural to\nmedical images. Second, reasoning performance is consistently lower than\nunderstanding, highlighting a critical barrier to safe decision support. Third,\nperformance varies widely across benchmarks, reflecting differences in task\ndesign, annotation quality, and knowledge demands. No model yet reaches the\nreliability threshold for clinical deployment, underscoring the need for\nstronger multimodal alignment and more rigorous, fine-grained evaluation\nprotocols.", "AI": {"tldr": "本文评估了通用和医学专用视觉语言模型（VLMs）在医疗任务中的表现，发现通用大模型在某些任务上已超越医学专用模型，但推理能力不足，且性能差异显著，临床部署尚不可行。", "motivation": "探索视觉语言模型在医疗任务中的能力，填补现有研究的空白。", "method": "评估了3B到72B参数的通用和医学专用VLMs，在八个基准测试（如MedXpert、OmniMedVQA等）上，将性能分为理解和推理两部分。", "result": "1. 通用大模型在某些任务上优于医学专用模型；2. 推理能力普遍较弱；3. 不同基准测试性能差异大。", "conclusion": "当前模型未达到临床部署的可靠性标准，需加强多模态对齐和更严格的评估协议。"}}
{"id": "2507.11372", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11372", "abs": "https://arxiv.org/abs/2507.11372", "authors": ["Pierrick Leroy", "Antonio Mastropietro", "Marco Nurisso", "Francesco Vaccarino"], "title": "Attributes Shape the Embedding Space of Face Recognition Models", "comment": null, "summary": "Face Recognition (FR) tasks have made significant progress with the advent of\nDeep Neural Networks, particularly through margin-based triplet losses that\nembed facial images into high-dimensional feature spaces. During training,\nthese contrastive losses focus exclusively on identity information as labels.\nHowever, we observe a multiscale geometric structure emerging in the embedding\nspace, influenced by interpretable facial (e.g., hair color) and image\nattributes (e.g., contrast). We propose a geometric approach to describe the\ndependence or invariance of FR models to these attributes and introduce a\nphysics-inspired alignment metric. We evaluate the proposed metric on\ncontrolled, simplified models and widely used FR models fine-tuned with\nsynthetic data for targeted attribute augmentation. Our findings reveal that\nthe models exhibit varying degrees of invariance across different attributes,\nproviding insight into their strengths and weaknesses and enabling deeper\ninterpretability. Code available here:\nhttps://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs", "AI": {"tldr": "论文提出了一种几何方法，用于描述人脸识别模型对可解释面部和图像属性的依赖性或不变性，并引入了一种物理启发的对齐度量。", "motivation": "尽管深度神经网络在人脸识别任务中取得了显著进展，但现有的对比损失仅关注身份信息，忽略了嵌入空间中多尺度几何结构的出现。", "method": "作者提出了一种几何方法，结合物理启发的对齐度量，评估模型对不同属性的依赖性或不变性。", "result": "研究发现，模型在不同属性上表现出不同程度的不变性，揭示了其优势和弱点。", "conclusion": "该方法为模型提供了更深的可解释性，代码已开源。"}}
{"id": "2507.11405", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11405", "abs": "https://arxiv.org/abs/2507.11405", "authors": ["Cheng Xu", "Nan Yan", "Shuhao Guan", "Changhong Jin", "Yuke Mei", "Yibing Guo", "M-Tahar Kechadi"], "title": "DCR: Quantifying Data Contamination in LLMs Evaluation", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has heightened concerns\nabout benchmark data contamination (BDC), where models inadvertently memorize\nevaluation data, inflating performance metrics and undermining genuine\ngeneralization assessment. This paper introduces the Data Contamination Risk\n(DCR) framework, a lightweight, interpretable pipeline designed to detect and\nquantify BDC across four granular levels: semantic, informational, data, and\nlabel. By synthesizing contamination scores via a fuzzy inference system, DCR\nproduces a unified DCR Factor that adjusts raw accuracy to reflect\ncontamination-aware performance. Validated on 9 LLMs (0.5B-72B) across\nsentiment analysis, fake news detection, and arithmetic reasoning tasks, the\nDCR framework reliably diagnoses contamination severity and with accuracy\nadjusted using the DCR Factor to within 4% average error across the three\nbenchmarks compared to the uncontaminated baseline. Emphasizing computational\nefficiency and transparency, DCR provides a practical tool for integrating\ncontamination assessment into routine evaluations, fostering fairer comparisons\nand enhancing the credibility of LLM benchmarking practices.", "AI": {"tldr": "论文提出了一种轻量级、可解释的数据污染风险（DCR）框架，用于检测和量化大语言模型（LLM）在评估数据中的污染问题，并通过调整准确性指标提供更真实的性能评估。", "motivation": "大语言模型的快速发展引发了对评估数据污染的担忧，污染可能导致性能指标虚高，影响对模型真实泛化能力的评估。", "method": "DCR框架通过四个粒度级别（语义、信息、数据和标签）检测污染，并利用模糊推理系统合成污染分数，生成统一的DCR因子来调整原始准确性。", "result": "在9个LLM（0.5B-72B）上的验证表明，DCR框架能可靠诊断污染严重程度，调整后的准确性误差在三个基准测试中平均低于4%。", "conclusion": "DCR框架计算高效且透明，为常规评估提供了实用的污染检测工具，有助于更公平的比较和提升LLM基准测试的可信度。"}}
{"id": "2507.11202", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11202", "abs": "https://arxiv.org/abs/2507.11202", "authors": ["Xinkui Zhao", "Jinsong Shu", "Yangyang Wu", "Guanjie Cheng", "Zihe Liu", "Naibo Wang", "Shuiguang Deng", "Zhongle Xie", "Jianwei Yin"], "title": "A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition", "comment": null, "summary": "Multimodal Emotion Recognition (MER) often encounters incomplete\nmultimodality in practical applications due to sensor failures or privacy\nprotection requirements. While existing methods attempt to address various\nincomplete multimodal scenarios by balancing the training of each modality\ncombination through additional gradients, these approaches face a critical\nlimitation: training gradients from different modality combinations conflict\nwith each other, ultimately degrading the performance of the final prediction\nmodel. In this paper, we propose a unimodal decoupled dynamic low-rank\nadaptation method based on modality combinations, named MCULoRA, which is a\nnovel framework for the parameter-efficient training of incomplete multimodal\nlearning models. MCULoRA consists of two key modules, modality combination\naware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The\nMCLA module effectively decouples the shared information from the distinct\ncharacteristics of individual modality combinations. The DPFT module adjusts\nthe training ratio of modality combinations based on the separability of each\nmodality's representation space, optimizing the learning efficiency across\ndifferent modality combinations. Our extensive experimental evaluation in\nmultiple benchmark datasets demonstrates that MCULoRA substantially outperforms\nprevious incomplete multimodal learning approaches in downstream task accuracy.", "AI": {"tldr": "MCULoRA提出了一种基于模态组合的单模态解耦动态低秩适应方法，解决了多模态情感识别中模态不完整的问题，通过MCLA和DPFT模块优化训练效率，显著提升了性能。", "motivation": "实际应用中多模态情感识别常因传感器故障或隐私保护导致模态不完整，现有方法因训练梯度冲突而性能受限。", "method": "提出MCULoRA框架，包含MCLA模块（解耦模态组合的共享与独特信息）和DPFT模块（动态调整训练比例）。", "result": "在多个基准数据集上，MCULoRA显著优于现有方法。", "conclusion": "MCULoRA为不完整多模态学习提供了一种高效参数训练框架，具有优越性能。"}}
{"id": "2507.11407", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11407", "abs": "https://arxiv.org/abs/2507.11407", "authors": ["LG AI Research", ":", "Kyunghoon Bae", "Eunbi Choi", "Kibong Choi", "Stanley Jungkyu Choi", "Yemuk Choi", "Kyubeen Han", "Seokhee Hong", "Junwon Hwang", "Taewan Hwang", "Joonwon Jang", "Hyojin Jeon", "Kijeong Jeon", "Gerrard Jeongwon Jo", "Hyunjik Jo", "Jiyeon Jung", "Euisoon Kim", "Hyosang Kim", "Jihoon Kim", "Joonkee Kim", "Seonghwan Kim", "Soyeon Kim", "Sunkyoung Kim", "Yireun Kim", "Yongil Kim", "Youchul Kim", "Edward Hwayoung Lee", "Gwangho Lee", "Haeju Lee", "Honglak Lee", "Jinsik Lee", "Kyungmin Lee", "Sangha Park", "Young Min Paik", "Yongmin Park", "Youngyong Park", "Sanghyun Seo", "Sihoon Yang", "Heuiyeen Yeen", "Sihyuk Yi", "Hyeongu Yun"], "title": "EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes", "comment": "Technical Report, 30 Pages", "summary": "This technical report introduces EXAONE 4.0, which integrates a Non-reasoning\nmode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5\nand the advanced reasoning abilities of EXAONE Deep. To pave the way for the\nagentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool\nuse, and its multilingual capabilities are extended to support Spanish in\naddition to English and Korean. The EXAONE 4.0 model series consists of two\nsizes: a mid-size 32B model optimized for high performance, and a small-size\n1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates\nsuperior performance compared to open-weight models in its class and remains\ncompetitive even against frontier-class models. The models are publicly\navailable for research purposes and can be easily downloaded via\nhttps://huggingface.co/LGAI-EXAONE.", "AI": {"tldr": "EXAONE 4.0整合了非推理和推理模式，提升了可用性和推理能力，支持多语言，并推出了两种规模的模型。", "motivation": "为AI代理时代铺路，结合EXAONE 3.5的易用性和EXAONE Deep的高级推理能力。", "method": "集成非推理和推理模式，扩展多语言支持（英语、韩语、西班牙语），提供32B和1.2B两种规模模型。", "result": "性能优于同类开源模型，与前沿模型竞争，模型公开供研究使用。", "conclusion": "EXAONE 4.0在性能和功能上取得显著进步，为研究和应用提供了强大工具。"}}
{"id": "2507.11408", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.11408", "abs": "https://arxiv.org/abs/2507.11408", "authors": ["Soumadeep Saha", "Akshay Chaturvedi", "Saptarshi Saha", "Utpal Garain", "Nicholas Asher"], "title": "KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?", "comment": "15 pages, 9 figures", "summary": "Chain-of-thought traces have been shown to improve performance of large\nlanguage models in a plethora of reasoning tasks, yet there is no consensus on\nthe mechanism through which this performance boost is achieved. To shed more\nlight on this, we introduce Causal CoT Graphs (CCGs), which are directed\nacyclic graphs automatically extracted from reasoning traces that model\nfine-grained causal dependencies in the language model output. A collection of\n$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their\nassociated CCGs are compiled into our dataset -- \\textbf{KisMATH}. Our detailed\nempirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in\nthe CCG are mediators for the final answer, a condition necessary for\nreasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating\nthat models internally realise structures akin to our graphs. KisMATH enables\ncontrolled, graph-aligned interventions and opens up avenues for further\ninvestigation into the role of chain-of-thought in LLM reasoning.", "AI": {"tldr": "论文通过引入因果CoT图（CCGs）分析语言模型推理中的因果关系，发现推理节点是最终答案的中介，模型内部结构类似CCGs。", "motivation": "探索链式思维（CoT）如何提升语言模型推理能力，缺乏共识机制。", "method": "提出Causal CoT Graphs（CCGs），从推理痕迹中提取有向无环图，建模因果关系，构建数据集KisMATH。", "result": "推理节点是答案的中介条件；模型内部结构与CCGs相似。", "conclusion": "KisMATH为研究CoT在语言模型推理中的作用提供了新工具和方法。"}}
{"id": "2507.11245", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11245", "abs": "https://arxiv.org/abs/2507.11245", "authors": ["X. Feng", "H. Yu", "M. Wu", "S. Hu", "J. Chen", "C. Zhu", "J. Wu", "X. Chu", "K. Huang"], "title": "NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models", "comment": "Project Page: https://amap-ml.github.io/NarrLV-Website/", "summary": "With the rapid development of foundation video generation technologies, long\nvideo generation models have exhibited promising research potential thanks to\nexpanded content creation space. Recent studies reveal that the goal of long\nvideo generation tasks is not only to extend video duration but also to\naccurately express richer narrative content within longer videos. However, due\nto the lack of evaluation benchmarks specifically designed for long video\ngeneration models, the current assessment of these models primarily relies on\nbenchmarks with simple narrative prompts (e.g., VBench). To the best of our\nknowledge, our proposed NarrLV is the first benchmark to comprehensively\nevaluate the Narrative expression capabilities of Long Video generation models.\nInspired by film narrative theory, (i) we first introduce the basic narrative\nunit maintaining continuous visual presentation in videos as Temporal Narrative\nAtom (TNA), and use its count to quantitatively measure narrative richness.\nGuided by three key film narrative elements influencing TNA changes, we\nconstruct an automatic prompt generation pipeline capable of producing\nevaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based\non the three progressive levels of narrative content expression, we design an\neffective evaluation metric using the MLLM-based question generation and\nanswering framework. (iii) Finally, we conduct extensive evaluations on\nexisting long video generation models and the foundation generation models.\nExperimental results demonstrate that our metric aligns closely with human\njudgments. The derived evaluation outcomes reveal the detailed capability\nboundaries of current video generation models in narrative content expression.", "AI": {"tldr": "论文提出首个针对长视频生成模型的叙事表达评估基准NarrLV，通过引入时间叙事原子（TNA）和基于MLLM的评估框架，全面衡量模型的叙事能力。", "motivation": "当前长视频生成模型缺乏专门的评估基准，现有基准（如VBench）仅支持简单叙事提示，无法全面评估叙事内容的丰富性。", "method": "1. 提出时间叙事原子（TNA）作为基本叙事单元；2. 设计自动提示生成管道；3. 基于MLLM框架设计评估指标。", "result": "实验表明，NarrLV的评估结果与人类判断高度一致，揭示了当前视频生成模型在叙事表达上的能力边界。", "conclusion": "NarrLV为长视频生成模型的叙事能力提供了首个全面评估工具，填补了研究空白。"}}
{"id": "2507.11443", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11443", "abs": "https://arxiv.org/abs/2507.11443", "authors": ["Haoran Wang", "Hanyu Pei", "Yang Lyu", "Kai Zhang", "Li Li", "Feng-Lei Fan"], "title": "COLI: A Hierarchical Efficient Compressor for Large Images", "comment": null, "summary": "The escalating adoption of high-resolution, large-field-of-view imagery\namplifies the need for efficient compression methodologies. Conventional\ntechniques frequently fail to preserve critical image details, while\ndata-driven approaches exhibit limited generalizability. Implicit Neural\nRepresentations (INRs) present a promising alternative by learning continuous\nmappings from spatial coordinates to pixel intensities for individual images,\nthereby storing network weights rather than raw pixels and avoiding the\ngeneralization problem. However, INR-based compression of large images faces\nchallenges including slow compression speed and suboptimal compression ratios.\nTo address these limitations, we introduce COLI (Compressor for Large Images),\na novel framework leveraging Neural Representations for Videos (NeRV). First,\nrecognizing that INR-based compression constitutes a training process, we\naccelerate its convergence through a pretraining-finetuning paradigm,\nmixed-precision training, and reformulation of the sequential loss into a\nparallelizable objective. Second, capitalizing on INRs' transformation of image\nstorage constraints into weight storage, we implement Hyper-Compression, a\nnovel post-training technique to substantially enhance compression ratios while\nmaintaining minimal output distortion. Evaluations across two medical imaging\ndatasets demonstrate that COLI consistently achieves competitive or superior\nPSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while\naccelerating NeRV training by up to 4 times.", "AI": {"tldr": "COLI框架通过改进INR压缩方法，解决了大图像压缩中的速度与压缩比问题，结合NeRV和超压缩技术，显著提升了性能。", "motivation": "高分辨率大视场图像的压缩需求增加，传统方法难以保留细节，数据驱动方法泛化性差，INR方法虽好但存在速度慢和压缩比不足的问题。", "method": "COLI框架采用NeRV技术，通过预训练-微调、混合精度训练和并行化目标加速INR压缩，并引入超压缩技术提升压缩比。", "result": "在两个医学影像数据集上，COLI在PSNR和SSIM指标上表现优异，压缩速度提升4倍。", "conclusion": "COLI为高效压缩大图像提供了新思路，解决了现有方法的局限性。"}}
{"id": "2507.11412", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11412", "abs": "https://arxiv.org/abs/2507.11412", "authors": ["Orion Weller", "Kathryn Ricci", "Marc Marone", "Antoine Chaffin", "Dawn Lawrie", "Benjamin Van Durme"], "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders", "comment": null, "summary": "The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.", "AI": {"tldr": "论文比较了编码器-解码器模型在不同任务中的表现，提出了一种统一训练方法，并开源了所有研究数据。", "motivation": "研究动机是解决编码器和解码器模型在不同任务中的性能差异，并提供一个公平的比较框架。", "method": "使用相同的训练方法（SOTA Ettin模型套件）训练编码器和解码器模型，范围从1700万到10亿参数，并在分类、检索和生成任务中比较性能。", "result": "编码器模型在分类和检索任务中表现更好，而解码器模型在生成任务中更优。直接转换模型目标效果不如专用模型。", "conclusion": "结论是专用模型在各自任务中表现最佳，开源数据为未来研究提供了基础。"}}
{"id": "2507.11247", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11247", "abs": "https://arxiv.org/abs/2507.11247", "authors": ["Veronika Shilova", "Emmanuel Malherbe", "Giovanni Palma", "Laurent Risser", "Jean-Michel Loubes"], "title": "Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone", "comment": null, "summary": "Within a legal framework, fairness in datasets and models is typically\nassessed by dividing observations into predefined groups and then computing\nfairness measures (e.g., Disparate Impact or Equality of Odds with respect to\ngender). However, when sensitive attributes such as skin color are continuous,\ndividing into default groups may overlook or obscure the discrimination\nexperienced by certain minority subpopulations. To address this limitation, we\npropose a fairness-based grouping approach for continuous (possibly\nmultidimensional) sensitive attributes. By grouping data according to observed\nlevels of discrimination, our method identifies the partition that maximizes a\nnovel criterion based on inter-group variance in discrimination, thereby\nisolating the most critical subgroups.\n  We validate the proposed approach using multiple synthetic datasets and\ndemonstrate its robustness under changing population distributions - revealing\nhow discrimination is manifested within the space of sensitive attributes.\nFurthermore, we examine a specialized setting of monotonic fairness for the\ncase of skin color. Our empirical results on both CelebA and FFHQ, leveraging\nthe skin tone as predicted by an industrial proprietary algorithm, show that\nthe proposed segmentation uncovers more nuanced patterns of discrimination than\npreviously reported, and that these findings remain stable across datasets for\na given model. Finally, we leverage our grouping model for debiasing purpose,\naiming at predicting fair scores with group-by-group post-processing. The\nresults demonstrate that our approach improves fairness while having minimal\nimpact on accuracy, thus confirming our partition method and opening the door\nfor industrial deployment.", "AI": {"tldr": "提出了一种基于公平性的连续敏感属性分组方法，通过最大化组间歧视差异的新标准，揭示更细微的歧视模式，并在去偏任务中验证了其有效性。", "motivation": "传统公平性评估方法在处理连续敏感属性（如肤色）时可能忽略少数群体的歧视问题，需要更精细的分组方法。", "method": "提出一种基于歧视水平的公平性分组方法，通过最大化组间歧视方差来识别关键子群，并在合成数据集和真实数据集（CelebA、FFHQ）上验证。", "result": "方法在合成和真实数据集中均能揭示更细微的歧视模式，去偏后公平性提升且对准确性影响最小。", "conclusion": "该方法为连续敏感属性的公平性评估和去偏提供了有效工具，适合工业应用。"}}
{"id": "2507.11488", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11488", "abs": "https://arxiv.org/abs/2507.11488", "authors": ["Pakizar Shamoi", "Nuray Toganas", "Muragul Muratbekova", "Elnara Kadyrgali", "Adilet Yerkin", "Ayan Igali", "Malika Ziyada", "Ayana Adilova", "Aron Karatayev", "Yerdauit Torekhan"], "title": "COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation", "comment": "submitted to IEEE for consideration", "summary": "Colors are omnipresent in today's world and play a vital role in how humans\nperceive and interact with their surroundings. However, it is challenging for\ncomputers to imitate human color perception. This paper introduces the Human\nPerception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based\nRepresentation and Interpretation), designed to bridge the gap between\ncomputational color representations and human visual perception. The proposed\nmodel uses fuzzy sets and logic to create a framework for color categorization.\nUsing a three-phase experimental approach, the study first identifies\ndistinguishable color stimuli for hue, saturation, and intensity through\npreliminary experiments, followed by a large-scale human categorization survey\ninvolving more than 1000 human subjects. The resulting data are used to extract\nfuzzy partitions and generate membership functions that reflect real-world\nperceptual uncertainty. The model incorporates a mechanism for adaptation that\nallows refinement based on feedback and contextual changes. Comparative\nevaluations demonstrate the model's alignment with human perception compared to\ntraditional color models, such as RGB, HSV, and LAB. To the best of our\nknowledge, no previous research has documented the construction of a model for\ncolor attribute specification based on a sample of this size or a comparable\nsample of the human population (n = 2496). Our findings are significant for\nfields such as design, artificial intelligence, marketing, and human-computer\ninteraction, where perceptually relevant color representation is critical.", "AI": {"tldr": "论文提出了一种基于人类感知的模糊颜色模型COLIBRI，旨在弥合计算机颜色表示与人类视觉感知之间的差距。通过实验和调查，模型利用模糊集和逻辑生成颜色分类框架，并展示了与传统颜色模型相比更符合人类感知的结果。", "motivation": "人类对颜色的感知与计算机表示存在差距，需要一种更符合人类视觉的颜色模型。", "method": "采用三阶段实验方法，包括初步实验、大规模人类分类调查，并利用模糊集和逻辑生成颜色分类框架。", "result": "模型在比较评估中显示出比RGB、HSV和LAB等传统颜色模型更符合人类感知。", "conclusion": "COLIBRI模型在设计和人机交互等领域具有重要意义，为颜色表示提供了更符合人类感知的解决方案。"}}
{"id": "2507.11423", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11423", "abs": "https://arxiv.org/abs/2507.11423", "authors": ["Yanjian Zhang", "Guillaume Wisniewski", "Nadi Tomeh", "Thierry Charnois"], "title": "Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?", "comment": null, "summary": "Human reasoning involves different strategies, each suited to specific\nproblems. Prior work shows that large language model (LLMs) tend to favor a\nsingle reasoning strategy, potentially limiting their effectiveness in diverse\nreasoning challenges. In this work, we investigate whether prompting can\ncontrol LLMs reasoning strategies and assess its impact on logical\nproblem-solving. While our experiments show that no single strategy\nconsistently improves accuracy, performance could be enhanced if models could\nadaptively choose the optimal strategy. We propose methods to guide LLMs in\nstrategy selection, highlighting new ways to refine their reasoning abilities.", "AI": {"tldr": "研究表明，大型语言模型（LLMs）倾向于单一推理策略，可能限制其解决多样化问题的能力。通过提示控制LLMs的策略选择，发现单一策略无法持续提升准确性，但自适应选择最优策略可改善性能。", "motivation": "探索提示是否能控制LLMs的推理策略，并评估其对逻辑问题解决的影响。", "method": "通过实验验证不同提示策略的效果，并提出引导LLMs自适应选择最优策略的方法。", "result": "实验显示，单一策略无法持续提升准确性，但自适应策略选择能改善性能。", "conclusion": "提示可以引导LLMs选择最优推理策略，为提升其推理能力提供了新方向。"}}
{"id": "2507.11261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11261", "abs": "https://arxiv.org/abs/2507.11261", "authors": ["Ronggang Huang", "Haoxin Yang", "Yan Cai", "Xuemiao Xu", "Huaidong Zhang", "Shengfeng He"], "title": "ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition", "comment": "Accepted by ICCV 2025", "summary": "3D visual grounding aims to identify and localize objects in a 3D space based\non textual descriptions. However, existing methods struggle with disentangling\ntargets from anchors in complex multi-anchor queries and resolving\ninconsistencies in spatial descriptions caused by perspective variations. To\ntackle these challenges, we propose ViewSRD, a framework that formulates 3D\nvisual grounding as a structured multi-view decomposition process. First, the\nSimple Relation Decoupling (SRD) module restructures complex multi-anchor\nqueries into a set of targeted single-anchor statements, generating a\nstructured set of perspective-aware descriptions that clarify positional\nrelationships. These decomposed representations serve as the foundation for the\nMulti-view Textual-Scene Interaction (Multi-TSI) module, which integrates\ntextual and scene features across multiple viewpoints using shared, Cross-modal\nConsistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a\nTextual-Scene Reasoning module synthesizes multi-view predictions into a\nunified and robust 3D visual grounding. Experiments on 3D visual grounding\ndatasets show that ViewSRD significantly outperforms state-of-the-art methods,\nparticularly in complex queries requiring precise spatial differentiation.", "AI": {"tldr": "ViewSRD是一种用于3D视觉定位的框架，通过结构化多视角分解和跨模态一致性视图标记，显著提升了复杂查询中的性能。", "motivation": "现有方法难以处理多锚点查询中的目标解耦和视角变化导致的空间描述不一致问题。", "method": "ViewSRD包含Simple Relation Decoupling (SRD)模块、Multi-view Textual-Scene Interaction (Multi-TSI)模块和Textual-Scene Reasoning模块，通过多视角分解和跨模态一致性视图标记实现精准定位。", "result": "在3D视觉定位数据集上，ViewSRD显著优于现有方法，尤其在需要精确空间区分的复杂查询中表现突出。", "conclusion": "ViewSRD通过结构化多视角分解和跨模态一致性视图标记，有效解决了3D视觉定位中的复杂查询问题。"}}
{"id": "2507.11539", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11539", "abs": "https://arxiv.org/abs/2507.11539", "authors": ["Dong Zhuo", "Wenzhao Zheng", "Jiahe Guo", "Yuqi Wu", "Jie Zhou", "Jiwen Lu"], "title": "Streaming 4D Visual Geometry Transformer", "comment": "Code is available at: https://github.com/wzzheng/StreamVGGT", "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.", "AI": {"tldr": "提出了一种流式4D视觉几何变换器，用于实时感知和重建4D时空几何，结合因果变换器架构和高效注意力机制，实现高质量在线重建。", "motivation": "解决视频中4D时空几何感知与重建的实时性和交互性问题，满足实际应用需求。", "method": "采用因果变换器架构，利用时间因果注意力机制和隐式记忆缓存历史信息，结合知识蒸馏和高效注意力算子优化。", "result": "在多种4D几何感知基准测试中表现优异，显著提升在线推理速度，同时保持高质量空间一致性。", "conclusion": "该模型为可扩展和交互式4D视觉系统提供了有效解决方案，代码已开源。"}}
{"id": "2507.11502", "categories": ["cs.CL", "cs.CE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11502", "abs": "https://arxiv.org/abs/2507.11502", "authors": ["Sirui Han", "Junqi Zhu", "Ruiyuan Zhang", "Yike Guo"], "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong", "comment": null, "summary": "This paper presents the development of HKGAI-V1, a foundational sovereign\nlarge language model (LLM), developed as part of an initiative to establish\nvalue-aligned AI infrastructure specifically tailored for Hong Kong. Addressing\nthe region's unique multilingual environment (Cantonese, Mandarin, and\nEnglish), its distinct socio-legal context under the \"one country, two systems\"\nframework, and specific local cultural and value considerations, the model is\nbuilt upon the DeepSeek architecture and systematically aligned with regional\nnorms through a multifaceted full parameter fine-tuning process. It is further\nintegrated with a retrieval-augmented generation (RAG) system to ensure timely\nand factually grounded information access. The core contribution lies in the\ndesign and implementation of a comprehensive, region-specific AI alignment and\nsafety framework, demonstrated through two key achievements: 1) The successful\ndevelopment of HKGAI-V1 itself - which outper-forms general-purpose models in\nhandling Hong Kong-specific culturally sensitive queries, and embodies a\n\"governance-embedded\" approach to digital sovereignty - empowers Hong Kong to\nexercise control over AI applications in critical sectors including public\nservices, legal systems, and edu-cation. 2) The development of the proprietary\nAdversarial HK Value Benchmark, a rigorous tool for evaluating model alignment\nwith local ethical and legal stand-ards under challenging conditions. By\ndocumenting these achievements, the paper provides not only a technological\nartifact but also a replicable blueprint for developing advanced, regionally\nfocused AI systems deeply rooted in their local identities.", "AI": {"tldr": "论文介绍了HKGAI-V1的开发，这是一个为香港定制的多语言大语言模型，结合了本地文化和法律需求，并通过检索增强生成技术提升信息准确性。", "motivation": "旨在为香港建立符合本地价值观的AI基础设施，适应其多语言、特殊法律框架和文化背景。", "method": "基于DeepSeek架构，通过全参数微调和检索增强生成（RAG）系统实现模型对齐和实时信息获取。", "result": "HKGAI-V1在本地文化敏感查询上优于通用模型，并开发了专有的对抗性香港价值观基准测试工具。", "conclusion": "论文提供了开发区域性AI系统的技术方案和可复制的蓝图，强调数字主权和本地化AI治理。"}}
{"id": "2507.11279", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11279", "abs": "https://arxiv.org/abs/2507.11279", "authors": ["Yujie Zhang", "Sabine Struckmeyer", "Andreas Kolb", "Sven Reichardt"], "title": "Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping", "comment": null, "summary": "Observer bias and inconsistencies in traditional plant phenotyping methods\nlimit the accuracy and reproducibility of fine-grained plant analysis. To\novercome these challenges, we developed TomatoMAP, a comprehensive dataset for\nSolanum lycopersicum using an Internet of Things (IoT) based imaging system\nwith standardized data acquisition protocols. Our dataset contains 64,464 RGB\nimages that capture 12 different plant poses from four camera elevation angles.\nEach image includes manually annotated bounding boxes for seven regions of\ninterest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,\naxillary shoot, shoot and whole plant area, along with 50 fine-grained growth\nstage classifications based on the BBCH scale. Additionally, we provide 3,616\nhigh-resolution image subset with pixel-wise semantic and instance segmentation\nannotations for fine-grained phenotyping. We validated our dataset using a\ncascading model deep learning framework combining MobileNetv3 for\nclassification, YOLOv11 for object detection, and MaskRCNN for segmentation.\nThrough AI vs. Human analysis involving five domain experts, we demonstrate\nthat the models trained on our dataset achieve accuracy and speed comparable to\nthe experts. Cohen's Kappa and inter-rater agreement heatmap confirm the\nreliability of automated fine-grained phenotyping using our approach.", "AI": {"tldr": "TomatoMAP是一个基于IoT的番茄植物表型数据集，包含64,464张RGB图像和精细标注，用于解决传统植物表型分析的观察者偏差和一致性问题。", "motivation": "传统植物表型分析方法存在观察者偏差和不一致性问题，影响准确性和可重复性。", "method": "开发了TomatoMAP数据集，使用IoT成像系统和标准化协议采集数据，包含多角度图像和精细标注，并采用深度学习框架验证数据集。", "result": "模型在数据集上训练后，其准确性和速度与领域专家相当，Cohen's Kappa和热图验证了自动化方法的可靠性。", "conclusion": "TomatoMAP为精细植物表型分析提供了可靠的数据集和方法，解决了传统方法的局限性。"}}
{"id": "2507.11508", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.11508", "abs": "https://arxiv.org/abs/2507.11508", "authors": ["Patrícia Schmidtová", "Ondřej Dušek", "Saad Mahamood"], "title": "Real-World Summarization: When Evaluation Reaches Its Limits", "comment": null, "summary": "We examine evaluation of faithfulness to input data in the context of hotel\nhighlights: brief LLM-generated summaries that capture unique features of\naccommodations. Through human evaluation campaigns involving categorical error\nassessment and span-level annotation, we compare traditional metrics, trainable\nmethods, and LLM-as-a-judge approaches. Our findings reveal that simpler\nmetrics like word overlap correlate surprisingly well with human judgments\n(Spearman correlation rank of 0.63), often outperforming more complex methods\nwhen applied to out-of-domain data. We further demonstrate that while LLMs can\ngenerate high-quality highlights, they prove unreliable for evaluation as they\ntend to severely under- or over-annotate. Our analysis of real-world business\nimpacts shows incorrect and non-checkable information pose the greatest risks.\nWe also highlight challenges in crowdsourced evaluations.", "AI": {"tldr": "研究发现，在酒店亮点摘要的忠实性评估中，简单的词重叠指标与人类判断相关性高（Spearman 0.63），而复杂方法在跨域数据中表现不佳。LLM生成摘要质量高，但评估不可靠。错误和不可验证信息风险最大。", "motivation": "探讨如何评估LLM生成的酒店亮点摘要对输入数据的忠实性，比较不同评估方法的有效性。", "method": "通过人工评估活动（分类错误评估和跨度级标注），比较传统指标、可训练方法和LLM作为评估者的方法。", "result": "简单指标（如词重叠）与人类判断相关性高，LLM评估不可靠，错误信息风险最大。", "conclusion": "简单指标在忠实性评估中表现优异，LLM评估需谨慎，错误信息是主要风险。"}}
{"id": "2507.11293", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11293", "abs": "https://arxiv.org/abs/2507.11293", "authors": ["J. Senthilnath", "Chen Hao", "F. C. Wellstood"], "title": "3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images", "comment": "copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "In semiconductor packaging, accurately recovering 3D information is crucial\nfor non-destructive testing (NDT) to localize circuit defects. This paper\npresents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),\nwhich leverages Magnetic Field Images (MFI) to retrieve the parameters for the\n3D current flow of a single-segment. The 3D MIR integrates a deep learning\n(DL)-based Convolutional Neural Network (CNN), spatial-physics-based\nconstraints, and optimization techniques. The method operates in three stages:\ni) The CNN model processes the MFI data to predict ($\\ell/z_o$), where $\\ell$\nis the wire length and $z_o$ is the wire's vertical depth beneath the magnetic\nsensors and classify segment type ($c$). ii) By leveraging\nspatial-physics-based constraints, the routine provides initial estimates for\nthe position ($x_o$, $y_o$, $z_o$), length ($\\ell$), current ($I$), and current\nflow direction (positive or negative) of the current segment. iii) An optimizer\nthen adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\\ell$, $I$) to\nminimize the difference between the reconstructed MFI and the actual MFI. The\nresults demonstrate that the 3D MIR method accurately recovers 3D information\nwith high precision, setting a new benchmark for magnetic image reconstruction\nin semiconductor packaging. This method highlights the potential of combining\nDL and physics-driven optimization in practical applications.", "AI": {"tldr": "提出了一种名为3D MIR的新方法，结合深度学习和物理约束，从磁场图像中恢复3D电流信息，用于半导体封装中的缺陷定位。", "motivation": "在半导体封装中，准确恢复3D信息对非破坏性测试（NDT）和电路缺陷定位至关重要。", "method": "3D MIR通过三个阶段实现：1）CNN处理磁场图像预测参数；2）基于物理约束提供初始估计；3）优化器调整参数以最小化误差。", "result": "3D MIR方法能够高精度恢复3D信息，为半导体封装中的磁场图像重建设定了新标准。", "conclusion": "该方法展示了深度学习和物理驱动优化结合在实际应用中的潜力。"}}
{"id": "2507.11301", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11301", "abs": "https://arxiv.org/abs/2507.11301", "authors": ["Paúl Maji", "Marlon Túquerres", "Stalin Valencia", "Marcela Valenzuela", "Christian Mejia-Escobar"], "title": "Detección y Cuantificación de Erosión Fluvial con Visión Artificial", "comment": "18 pages, in Spanish language, 13 figures, 4 tables", "summary": "Fluvial erosion is a natural process that can generate significant impacts on\nsoil stability and strategic infrastructures. The detection and monitoring of\nthis phenomenon is traditionally addressed by photogrammetric methods and\nanalysis in geographic information systems. These tasks require specific\nknowledge and intensive manual processing. This study proposes an artificial\nintelligence-based approach for automatic identification of eroded zones and\nestimation of their area. The state-of-the-art computer vision model YOLOv11,\nadjusted by fine-tuning and trained with photographs and LiDAR images, is used.\nThis combined dataset was segmented and labeled using the Roboflow platform.\nExperimental results indicate efficient detection of erosion patterns with an\naccuracy of 70%, precise identification of eroded areas and reliable\ncalculation of their extent in pixels and square meters. As a final product,\nthe EROSCAN system has been developed, an interactive web application that\nallows users to upload images and obtain automatic segmentations of fluvial\nerosion, together with the estimated area. This tool optimizes the detection\nand quantification of the phenomenon, facilitating decision making in risk\nmanagement and territorial planning.", "AI": {"tldr": "论文提出了一种基于人工智能的方法，利用YOLOv11模型自动识别侵蚀区域并估算面积，开发了交互式网页应用ERO SCAN。", "motivation": "传统方法需要专业知识且处理繁琐，希望通过AI优化侵蚀检测与量化。", "method": "采用YOLOv11模型，结合照片和LiDAR图像进行微调训练，使用Roboflow平台标注数据。", "result": "实验显示70%的准确率，能精确识别侵蚀区域并计算面积。", "conclusion": "ERO SCAN工具提升了侵蚀检测效率，支持风险管理与规划决策。"}}
{"id": "2507.11321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11321", "abs": "https://arxiv.org/abs/2507.11321", "authors": ["Haoxuan Qu", "Yujun Cai", "Hossein Rahmani", "Ajay Kumar", "Junsong Yuan", "Jun Liu"], "title": "A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction", "comment": null, "summary": "Recently, Gaussian Splatting (GS) has received a lot of attention in surface\nreconstruction. However, while 3D objects can be of complex and diverse shapes\nin the real world, existing GS-based methods only limitedly use a single type\nof splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent\nobject surfaces during their reconstruction. In this paper, we highlight that\nthis can be insufficient for object surfaces to be represented in high quality.\nThus, we propose a novel framework that, for the first time, enables Gaussian\nSplatting to incorporate multiple types of (geometrical) primitives during its\nsurface reconstruction process. Specifically, in our framework, we first\npropose a compositional splatting strategy, enabling the splatting and\nrendering of different types of primitives in the Gaussian Splatting pipeline.\nIn addition, we also design our framework with a mixed-primitive-based\ninitialization strategy and a vertex pruning mechanism to further promote its\nsurface representation learning process to be well executed leveraging\ndifferent types of primitives. Extensive experiments show the efficacy of our\nframework and its accurate surface reconstruction performance.", "AI": {"tldr": "论文提出了一种新的高斯泼溅框架，首次引入多种几何基元以提高表面重建质量。", "motivation": "现有高斯泼溅方法仅使用单一基元（椭圆或椭球）表示复杂物体表面，导致重建质量不足。", "method": "提出组合泼溅策略、混合基元初始化策略和顶点修剪机制，支持多种基元的高斯泼溅。", "result": "实验证明该框架能显著提升表面重建的准确性。", "conclusion": "通过引入多种几何基元，新框架在高斯泼溅中实现了更高质量的物体表面重建。"}}
{"id": "2507.11333", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11333", "abs": "https://arxiv.org/abs/2507.11333", "authors": ["Jianfei Jiang", "Qiankun Liu", "Haochen Yu", "Hongyuan Liu", "Liyong Wang", "Jiansheng Chen", "Huimin Ma"], "title": "MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network", "comment": "Accepted by ICCV 2025", "summary": "Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for\na sequence of calibrated images to recover dense point clouds. However,\nexisting MVS methods often struggle with challenging regions, such as\ntextureless regions and reflective surfaces, where feature matching fails. In\ncontrast, monocular depth estimation inherently does not require feature\nmatching, allowing it to achieve robust relative depth estimation in these\nregions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature\nand depth guided MVS network that integrates powerful priors from a monocular\nfoundation model into multi-view geometry. Firstly, the monocular feature of\nthe reference view is integrated into source view features by the attention\nmechanism with a newly designed cross-view position encoding. Then, the\nmonocular depth of the reference view is aligned to dynamically update the\ndepth candidates for edge regions during the sampling procedure. Finally, a\nrelative consistency loss is further designed based on the monocular depth to\nsupervise the depth prediction. Extensive experiments demonstrate that\nMonoMVSNet achieves state-of-the-art performance on the DTU and\nTanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate\nand Advanced benchmarks. The source code is available at\nhttps://github.com/JianfeiJ/MonoMVSNet.", "AI": {"tldr": "MonoMVSNet结合单目深度估计和多视角立体视觉，通过注意力机制和动态深度候选更新，解决了纹理缺失和反射表面的挑战，在多个数据集上表现优异。", "motivation": "现有MVS方法在纹理缺失和反射表面等区域表现不佳，而单目深度估计无需特征匹配，具有鲁棒性。", "method": "提出MonoMVSNet，集成单目特征和深度到多视角几何中，使用注意力机制和动态深度候选更新。", "result": "在DTU和Tanks-and-Temples数据集上达到最优性能，排名第一。", "conclusion": "MonoMVSNet通过结合单目和多视角信息，显著提升了MVS在挑战性区域的性能。"}}
{"id": "2507.11336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11336", "abs": "https://arxiv.org/abs/2507.11336", "authors": ["Peiran Wu", "Yunze Liu", "Zhengdong Zhu", "Enmin Zhou", "Shawn Shen"], "title": "UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks", "comment": null, "summary": "Real-world user-generated videos, especially on platforms like TikTok, often\nfeature rich and intertwined audio visual content. However, existing video\ncaptioning benchmarks and models remain predominantly visual centric,\noverlooking the crucial role of audio in conveying scene dynamics, speaker\nintent, and narrative context. This lack of omni datasets and lightweight,\ncapable models hampers progress in fine grained, multimodal video\nunderstanding. To address these challenges, we introduce UGC-VideoCap, a new\nbenchmark and model framework specifically designed for detailed omnimodal\ncaptioning of short form user-generated videos. Unlike prior datasets,\nUGC-VideoCap emphasizes balanced integration of audio and visual modalities,\nfeaturing 1000 TikTok videos annotated through a structured three stage\nhuman-in-the-loop pipeline covering audio only, visual only, and joint audio\nvisual semantics. The benchmark also includes 4000 carefully crafted QA pairs\nprobing both unimodal and cross modal understanding. Alongside the dataset, we\npropose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from\nGemini 2.5 Flash. Using a novel two-stage training strategy supervised fine\ntuning followed by Group Relative Policy Optimization (GRPO), our approach\nenables efficient adaptation from limited data while maintaining competitive\nperformance. Together, our benchmark and model offer a high-quality foundation\nand a data-efficient solution for advancing omnimodal video captioning in\nunconstrained real-world UGC settings.", "AI": {"tldr": "论文提出了UGC-VideoCap，一个专注于短格式用户生成视频的多模态字幕基准和模型框架，强调音频和视觉的平衡整合。", "motivation": "现有视频字幕基准和模型过于视觉中心化，忽视了音频在场景动态、说话者意图和叙事背景中的关键作用，缺乏全面的数据集和轻量级模型。", "method": "通过三阶段人工标注流程（音频、视觉、联合模态）构建数据集，并提出UGC-VideoCaptioner(3B)模型，采用两阶段训练策略（监督微调+GRPO）。", "result": "UGC-VideoCap包含1000个TikTok视频和4000个QA对，模型在有限数据下表现高效且性能优异。", "conclusion": "UGC-VideoCap为真实场景下的多模态视频字幕提供了高质量基准和数据高效解决方案。"}}
{"id": "2507.11441", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.11441", "abs": "https://arxiv.org/abs/2507.11441", "authors": ["Kaif Shaikh", "Antoni Kowalczuk", "Franziska Boenisch", "Adam Dziedzic"], "title": "Implementing Adaptations for Vision AutoRegressive Model", "comment": "Accepted at DIG-BUGS: Data in Generative Models Workshop @ ICML 2025", "summary": "Vision AutoRegressive model (VAR) was recently introduced as an alternative\nto Diffusion Models (DMs) in image generation domain. In this work we focus on\nits adaptations, which aim to fine-tune pre-trained models to perform specific\ndownstream tasks, like medical data generation. While for DMs there exist many\ntechniques, adaptations for VAR remain underexplored. Similarly, differentially\nprivate (DP) adaptations-ones that aim to preserve privacy of the adaptation\ndata-have been extensively studied for DMs, while VAR lacks such solutions. In\nour work, we implement and benchmark many strategies for VAR, and compare them\nto state-of-the-art DM adaptation strategies. We observe that VAR outperforms\nDMs for non-DP adaptations, however, the performance of DP suffers, which\nnecessitates further research in private adaptations for VAR. Code is available\nat https://github.com/sprintml/finetuning_var_dp.", "AI": {"tldr": "VAR模型在非差分隐私适应任务中优于扩散模型，但在差分隐私适应中表现不佳，需进一步研究。", "motivation": "研究VAR模型在特定下游任务（如医疗数据生成）中的适应方法，填补其在差分隐私适应领域的空白。", "method": "实现并比较多种VAR适应策略，与扩散模型的最先进适应策略进行对比。", "result": "VAR在非差分隐私适应中表现优于扩散模型，但在差分隐私适应中性能下降。", "conclusion": "VAR在非隐私任务中表现优异，但需进一步研究差分隐私适应策略。"}}
{"id": "2507.11474", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11474", "abs": "https://arxiv.org/abs/2507.11474", "authors": ["Pan Du", "Mingqi Xu", "Xiaozhi Zhu", "Jian-xun Wang"], "title": "HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing", "comment": "59 pages, 9 figures", "summary": "Accurate characterization of vascular geometry is essential for\ncardiovascular diagnosis and treatment planning. Traditional statistical shape\nmodeling (SSM) methods rely on linear assumptions, limiting their expressivity\nand scalability to complex topologies such as multi-branch vascular structures.\nWe introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular\ngeometry Synthesis, which integrates NURBS surface parameterization with\ndiffusion-based generative modeling to synthesize realistic, fine-grained\naortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates\nanatomically faithful aortas with supra-aortic branches, yielding biomarker\ndistributions that closely match those of the original dataset. HUG-VAS adopts\na hierarchical architecture comprising a denoising diffusion model that\ngenerates centerlines and a guided diffusion model that synthesizes radial\nprofiles conditioned on those centerlines, thereby capturing two layers of\nanatomical variability. Critically, the framework supports zero-shot\nconditional generation from image-derived priors, enabling practical\napplications such as interactive semi-automatic segmentation, robust\nreconstruction under degraded imaging conditions, and implantable device\noptimization. To our knowledge, HUG-VAS is the first SSM framework to bridge\nimage-derived priors with generative shape modeling via a unified integration\nof NURBS parameterization and hierarchical diffusion processes.", "AI": {"tldr": "HUG-VAS是一种基于NURBS和扩散模型的层次生成模型，用于合成高保真度的主动脉几何结构，解决了传统统计形状建模在复杂拓扑结构中的局限性。", "motivation": "传统统计形状建模方法依赖线性假设，难以表达复杂血管拓扑结构，限制了其在心血管诊断和治疗规划中的应用。", "method": "HUG-VAS结合NURBS参数化和扩散生成模型，采用分层架构：一个扩散模型生成中心线，另一个生成基于中心线的径向轮廓，从而捕捉解剖变异性。", "result": "模型在21个患者样本上训练，生成的主动脉几何结构具有解剖学真实性，生物标志物分布与原始数据集高度匹配。", "conclusion": "HUG-VAS首次通过NURBS参数化和分层扩散过程，将图像先验与生成形状建模统一结合，支持零样本条件生成，具有广泛的应用潜力。"}}
{"id": "2507.11476", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11476", "abs": "https://arxiv.org/abs/2507.11476", "authors": ["Esteban Román Catafau", "Torbjörn E. M. Nordling"], "title": "C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images", "comment": "22 pages, 16 figures", "summary": "This paper addresses the fundamental computer vision challenge of robust\ncircle detection and fitting in degraded imaging conditions. We present\nCombinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an\nalgorithm that bridges the gap between circle detection and precise parametric\nfitting by combining (1) efficient combinatorial edge pixel (edgel) sampling\nand (2) convolution-based density estimation in parameter space.\n  We evaluate 3C-FBI across three experimental frameworks: (1) real-world\nmedical data from Parkinson's disease assessments (144 frames from 36 videos),\n(2) controlled synthetic data following established circle-fitting benchmarks,\nand (3) systematic analysis across varying spatial resolutions and outlier\ncontamination levels. Results show that 3C-FBI achieves state-of-the-art\naccuracy (Jaccard index 0.896) while maintaining real-time performance (40.3\nfps), significantly outperforming classical methods like RCD (6.8 fps) on a\nstandard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost\n1.0) at high resolutions (480x480) and reliable performance (Jaccard higher\nthan 0.95) down to 160x160 with up to 20% outliers.\n  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989\nacross contamination levels, comparable to modern methods like Qi et al. (2024,\n0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and\nrobustness makes 3C-FBI ideal for medical imaging, robotics, and industrial\ninspection under challenging conditions.", "AI": {"tldr": "3C-FBI算法通过组合边缘像素采样和卷积密度估计，解决了模糊图像中圆检测和拟合的挑战，实现了高精度和实时性能。", "motivation": "解决在退化成像条件下稳健检测和拟合圆的基本计算机视觉挑战。", "method": "结合高效的组合边缘像素采样和参数空间中的卷积密度估计。", "result": "在真实医学数据、合成数据和不同分辨率下表现优异，Jaccard指数达0.896，实时性能40.3 fps。", "conclusion": "3C-FBI在精度、速度和鲁棒性上表现优异，适用于医疗影像、机器人和工业检测。"}}
{"id": "2507.11522", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11522", "abs": "https://arxiv.org/abs/2507.11522", "authors": ["Tariq Mehmood", "Hamza Ahmad", "Muhammad Haroon Shakeel", "Murtaza Taj"], "title": "CATVis: Context-Aware Thought Visualization", "comment": "Accepted at MICCAI 2025. This is the submitted version prior to peer\n  review. The final Version of Record will appear in the MICCAI 2025\n  proceedings (Springer LNCS)", "summary": "EEG-based brain-computer interfaces (BCIs) have shown promise in various\napplications, such as motor imagery and cognitive state monitoring. However,\ndecoding visual representations from EEG signals remains a significant\nchallenge due to their complex and noisy nature. We thus propose a novel\n5-stage framework for decoding visual representations from EEG signals: (1) an\nEEG encoder for concept classification, (2) cross-modal alignment of EEG and\ntext embeddings in CLIP feature space, (3) caption refinement via re-ranking,\n(4) weighted interpolation of concept and caption embeddings for richer\nsemantics, and (5) image generation using a pre-trained Stable Diffusion model.\nWe enable context-aware EEG-to-image generation through cross-modal alignment\nand re-ranking. Experimental results demonstrate that our method generates\nhigh-quality images aligned with visual stimuli, outperforming SOTA approaches\nby 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and\nreducing Fr\\'echet Inception Distance by 36.61%, indicating superior semantic\nalignment and image quality.", "AI": {"tldr": "提出了一种新的5阶段框架，用于从EEG信号解码视觉表示，包括概念分类、跨模态对齐、标题优化、语义增强和图像生成，实验表明其性能优于现有方法。", "motivation": "EEG信号复杂且噪声大，解码视觉表示具有挑战性，因此需要一种新方法来解决这一问题。", "method": "5阶段框架：EEG编码器、跨模态对齐、标题优化、加权插值和图像生成。", "result": "生成高质量图像，分类准确率提升13.43%，生成准确率提升15.21%，FID降低36.61%。", "conclusion": "该方法在语义对齐和图像质量上优于现有技术，为EEG到图像的转换提供了有效解决方案。"}}
{"id": "2507.11533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11533", "abs": "https://arxiv.org/abs/2507.11533", "authors": ["Mengyu Wang", "Henghui Ding", "Jianing Peng", "Yao Zhao", "Yunpeng Chen", "Yunchao Wei"], "title": "CharaConsist: Fine-Grained Consistent Character Generation", "comment": "ICCV 2025 accepted paper, project page:\n  https://murray-wang.github.io/CharaConsist/", "summary": "In text-to-image generation, producing a series of consistent contents that\npreserve the same identity is highly valuable for real-world applications.\nAlthough a few works have explored training-free methods to enhance the\nconsistency of generated subjects, we observe that they suffer from the\nfollowing problems. First, they fail to maintain consistent background details,\nwhich limits their applicability. Furthermore, when the foreground character\nundergoes large motion variations, inconsistencies in identity and clothing\ndetails become evident. To address these problems, we propose CharaConsist,\nwhich employs point-tracking attention and adaptive token merge along with\ndecoupled control of the foreground and background. CharaConsist enables\nfine-grained consistency for both foreground and background, supporting the\ngeneration of one character in continuous shots within a fixed scene or in\ndiscrete shots across different scenes. Moreover, CharaConsist is the first\nconsistent generation method tailored for text-to-image DiT model. Its ability\nto maintain fine-grained consistency, combined with the larger capacity of\nlatest base model, enables it to produce high-quality visual outputs,\nbroadening its applicability to a wider range of real-world scenarios. The\nsource code has been released at https://github.com/Murray-Wang/CharaConsist", "AI": {"tldr": "提出CharaConsist方法，通过点跟踪注意力和自适应标记合并，解决文本到图像生成中身份和背景一致性问题。", "motivation": "现有方法在生成一致内容时无法保持背景细节和身份一致性，限制了实际应用。", "method": "采用点跟踪注意力、自适应标记合并，并解耦前景和背景控制。", "result": "CharaConsist能生成高质量视觉输出，支持连续或离散场景中的一致内容。", "conclusion": "CharaConsist是首个针对DiT模型的文本到图像一致生成方法，扩展了实际应用范围。"}}
{"id": "2507.11540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11540", "abs": "https://arxiv.org/abs/2507.11540", "authors": ["Zhen Xu", "Hongyu Zhou", "Sida Peng", "Haotong Lin", "Haoyu Guo", "Jiahao Shao", "Peishan Yang", "Qinglin Yang", "Sheng Miao", "Xingyi He", "Yifan Wang", "Yue Wang", "Ruizhen Hu", "Yiyi Liao", "Xiaowei Zhou", "Hujun Bao"], "title": "Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation", "comment": null, "summary": "Depth estimation is a fundamental task in 3D computer vision, crucial for\napplications such as 3D reconstruction, free-viewpoint rendering, robotics,\nautonomous driving, and AR/VR technologies. Traditional methods relying on\nhardware sensors like LiDAR are often limited by high costs, low resolution,\nand environmental sensitivity, limiting their applicability in real-world\nscenarios. Recent advances in vision-based methods offer a promising\nalternative, yet they face challenges in generalization and stability due to\neither the low-capacity model architectures or the reliance on domain-specific\nand small-scale datasets. The emergence of scaling laws and foundation models\nin other domains has inspired the development of \"depth foundation models\":\ndeep neural networks trained on large datasets with strong zero-shot\ngeneralization capabilities. This paper surveys the evolution of deep learning\narchitectures and paradigms for depth estimation across the monocular, stereo,\nmulti-view, and monocular video settings. We explore the potential of these\nmodels to address existing challenges and provide a comprehensive overview of\nlarge-scale datasets that can facilitate their development. By identifying key\narchitectures and training strategies, we aim to highlight the path towards\nrobust depth foundation models, offering insights into their future research\nand applications.", "AI": {"tldr": "本文综述了深度估计领域的发展，探讨了基于视觉的方法和深度基础模型的潜力，旨在解决传统硬件传感器的局限性。", "motivation": "深度估计在3D计算机视觉中至关重要，但传统硬件传感器成本高且受限，而现有视觉方法在泛化和稳定性方面存在挑战。", "method": "综述了单目、立体、多视图和单目视频设置下的深度学习架构和范式，并探讨了大规模数据集的作用。", "result": "提出了深度基础模型的概念，强调其在零样本泛化能力上的潜力，并总结了关键架构和训练策略。", "conclusion": "深度基础模型有望解决现有挑战，未来研究应关注其进一步发展和应用。"}}
