{"id": "2602.17826", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17826", "abs": "https://arxiv.org/abs/2602.17826", "authors": ["Marcelo Labre"], "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge", "comment": "Submitted to NeuS 2026. Supplementary materials and code: https://doi.org/10.5281/zenodo.18665030", "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.", "AI": {"tldr": "本研究通过检索增强生成（RAG）方法，利用数学领域的本体（OpenMath）来提升语言模型在需要可验证推理的高风险专业领域的可靠性，并探索了神经符号方法。", "motivation": "语言模型在现实世界的应用中存在幻觉、脆弱性和缺乏形式基础等根本性限制，这在高风险的专业领域尤为严重，因为这些领域需要可验证的推理。", "method": "利用OpenMath本体，实现了一个神经符号管道，该管道结合了混合检索和交叉编码器重排序技术，将相关的数学定义注入到语言模型的提示中，以实现检索增强生成。", "result": "在MATH基准上对三个开源模型进行评估，结果表明，当检索质量高时，本体引导的上下文可以提高模型性能；然而，不相关的上下文会显著降低模型性能。", "conclusion": "本体引导的上下文在提升语言模型可靠性方面显示出潜力，但也突显了神经符号方法所面临的挑战，特别是如何确保检索信息的准确性和相关性。"}}
{"id": "2602.17797", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17797", "abs": "https://arxiv.org/abs/2602.17797", "authors": ["Mohammad Tahmid Noor", "B. M. Shahria Alam", "Tasmiah Rahman Orpa", "Shaila Afroz Anika", "Mahjabin Tasnim Samiha", "Fahad Ahammed"], "title": "Deep Learning for Dermatology: An Innovative Framework for Approaching Precise Skin Cancer Detection", "comment": "6 pages, 9 figures, this is the author's accepted manuscript of a paper accepted for publication in the Proceedings of the 16th International IEEE Conference on Computing, Communication and Networking Technologies (ICCCNT 2025). The final published version will be available via IEEE Xplore", "summary": "Skin cancer can be life-threatening if not diagnosed early, a prevalent yet preventable disease. Globally, skin cancer is perceived among the finest prevailing cancers and millions of people are diagnosed each year. For the allotment of benign and malignant skin spots, an area of critical importance in dermatological diagnostics, the application of two prominent deep learning models, VGG16 and DenseNet201 are investigated by this paper. We evaluate these CNN architectures for their efficacy in differentiating benign from malignant skin lesions leveraging enhancements in deep learning enforced to skin cancer spotting. Our objective is to assess model accuracy and computational efficiency, offering insights into how these models could assist in early detection, diagnosis, and streamlined workflows in dermatology. We used two deep learning methods DenseNet201 and VGG16 model on a binary class dataset containing 3297 images. The best result with an accuracy of 93.79% achieved by DenseNet201. All images were resized to 224x224 by rescaling. Although both models provide excellent accuracy, there is still some room for improvement. In future using new datasets, we tend to improve our work by achieving great accuracy.", "AI": {"tldr": "本研究评估了 VGG16 和 DenseNet201 两种深度学习模型在区分良性和恶性皮肤病变方面的有效性，DenseNet201 模型在包含 3297 张图像的二分类数据集上取得了 93.79% 的准确率。", "motivation": "为了早期诊断和区分良恶性皮肤病变，提高皮肤癌的诊断效率和准确性。", "method": "使用 VGG16 和 DenseNet201 两种卷积神经网络模型，在经过缩放到 224x224 尺寸的 3297 张图像的二分类数据集上进行训练和评估。", "result": "DenseNet201 模型取得了最佳结果，准确率为 93.79%。两种模型都显示出优异的准确性。", "conclusion": "VGG16 和 DenseNet201 模型在皮肤癌早期检测方面表现出色，DenseNet201 表现更优。未来的工作将通过使用新数据集来进一步提高模型的准确性。"}}
{"id": "2602.17784", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17784", "abs": "https://arxiv.org/abs/2602.17784", "authors": ["Meng Ye", "Xiao Lin", "Georgina Lukoczki", "Graham W. Lederer", "Yi Yao"], "title": "QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration", "comment": null, "summary": "Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.", "AI": {"tldr": "研究提出了QueryPlot框架，利用自然语言处理技术整合地质文本和地图数据，实现矿产资源潜力图的自动化绘制，并提高了检索的召回率和分类性能。", "motivation": "传统的矿产资源潜力图绘制过程耗时且依赖专家知识，难以整合异构的地质信息。需要一种更高效、自动化的方法来合成地质文本模型和地理空间数据集。", "method": "使用自然语言处理技术，将地质文本模型和地质图的多边形数据转换为结构化文本表示。利用预训练的嵌入模型对查询和区域描述进行编码，计算语义相似度得分，以排名和可视化潜在区域。支持组合式查询，并将相似度得分作为监督学习模型的附加特征。", "result": "QueryPlot框架能够高召回率地检索已知矿产点，生成的潜力区域与专家定义的区域高度吻合。在作为附加特征时，相似度得分能够提升监督学习模型的分类性能。", "conclusion": "QueryPlot是一个支持交互式查询、可视化和导出GIS兼容潜力图的Web系统，能够有效整合地质文本和地图数据，自动化矿产资源潜力图的绘制，并为未来研究提供了开源代码和数据集。"}}
{"id": "2602.17737", "categories": ["cs.RO", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.17737", "abs": "https://arxiv.org/abs/2602.17737", "authors": ["Upasana Biswas", "Durgesh Kalwar", "Subbarao Kambhampati", "Sarath Sreedharan"], "title": "Nested Training for Mutual Adaptation in Human-AI Teaming", "comment": null, "summary": "Mutual adaptation is a central challenge in human--AI teaming, as humans naturally adjust their strategies in response to a robot's policy. Existing approaches aim to improve diversity in training partners to approximate human behavior, but these partners are static and fail to capture adaptive behavior of humans. Exposing robots to adaptive behaviors is critical, yet when both agents learn simultaneously in a multi-agent setting, they often converge to opaque implicit coordination strategies that only work with the agents they were co-trained with. Such agents fail to generalize when paired with new partners. In order to capture the adaptive behavior of humans, we model the human-robot teaming scenario as an Interactive Partially Observable Markov Decision Process (I-POMDP), explicitly modeling human adaptation as part of the state. We propose a nested training regime to approximately learn the solution to a finite-level I-POMDP. In this framework, agents at each level are trained against adaptive agents from the level below. This ensures that the ego agent is exposed to adaptive behavior during training while avoiding the emergence of implicit coordination strategies, since the training partners are not themselves learning. We train our method in a multi-episode, required cooperation setup in the Overcooked domain, comparing it against several baseline agents designed for human-robot teaming. We evaluate the performance of our agent when paired with adaptive partners that were not seen during training. Our results demonstrate that our agent not only achieves higher task performance with these adaptive partners but also exhibits significantly greater adaptability during team interactions.", "AI": {"tldr": "本研究提出了一种新的方法来解决人机协作中的自适应性问题，通过将人类适应性纳入状态空间，并采用嵌套训练机制，提高了机器人与人类合作时的任务表现和适应性。", "motivation": "现有的人机协作训练方法通常使用静态的训练伙伴，无法捕捉人类的动态适应行为。在多智能体设置中，同时学习会导致隐式协调策略，限制了泛化能力。因此，需要一种方法来显式地建模和应对人类的适应性。", "method": "该研究将人机协作建模为交互式部分可观察马尔可夫决策过程（I-POMDP），并将人类适应性明确地作为状态的一部分。采用嵌套训练机制，每一层的智能体都与下一层的自适应智能体进行训练，以暴露于自适应行为，同时避免隐式协调策略。", "result": "在Overcooked领域进行的实验表明，与基线方法相比，该方法训练出的智能体在与未见过的自适应伙伴配对时，不仅任务表现更高，而且在团队互动中表现出更强的适应性。", "conclusion": "所提出的I-POMDP框架和嵌套训练机制能够有效地捕捉人类的适应性行为，提高人机协作的鲁棒性和泛化能力，使得智能体能够更好地与动态变化的人类伙伴进行协作。"}}
{"id": "2602.17813", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17813", "abs": "https://arxiv.org/abs/2602.17813", "authors": ["Junqing Yang", "Natasha Thorley", "Ahmed Nadeem Abbasi", "Shonit Punwani", "Zion Tse", "Yipeng Hu", "Shaheer U. Saeed"], "title": "Promptable segmentation with region exploration enables minimal-effort expert-level prostate cancer delineation", "comment": "Accepted at IPCAI 2026 (IJCARS - IPCAI 2026 Special Issue)", "summary": "Purpose: Accurate segmentation of prostate cancer on magnetic resonance (MR) images is crucial for planning image-guided interventions such as targeted biopsies, cryoablation, and radiotherapy. However, subtle and variable tumour appearances, differences in imaging protocols, and limited expert availability make consistent interpretation difficult. While automated methods aim to address this, they rely on large expertly-annotated datasets that are often inconsistent, whereas manual delineation remains labour-intensive. This work aims to bridge the gap between automated and manual segmentation through a framework driven by user-provided point prompts, enabling accurate segmentation with minimal annotation effort.\n  Methods: The framework combines reinforcement learning (RL) with a region-growing segmentation process guided by user prompts. Starting from an initial point prompt, region-growing generates a preliminary segmentation, which is iteratively refined through RL. At each step, the RL agent observes the image and current segmentation to predict a new point, from which region growing updates the mask. A reward, balancing segmentation accuracy and voxel-wise uncertainty, encourages exploration of ambiguous regions, allowing the agent to escape local optima and perform sample-specific optimisation. Despite requiring fully supervised training, the framework bridges manual and fully automated segmentation at inference by substantially reducing user effort while outperforming current fully automated methods.\n  Results: The framework was evaluated on two public prostate MR datasets (PROMIS and PICAI, with 566 and 1090 cases). It outperformed the previous best automated methods by 9.9% and 8.9%, respectively, with performance comparable to manual radiologist segmentation, reducing annotation time tenfold.", "AI": {"tldr": "提出了一种结合强化学习和区域生长分割的用户引导式框架，用于自动分割前列腺癌MR图像，以最小的标注工作量实现高精度，并优于现有自动化方法。", "motivation": "现有前列腺癌MR图像分割方法存在挑战：肿瘤外观微妙多变，成像方案不一，专家有限导致解释困难；自动化方法依赖不一致的大型标注数据集；手动分割耗时耗力。因此，需要一种能显著减少标注工作量并保证精度的分割方法。", "method": "该框架结合了强化学习（RL）和区域生长分割。用户提供一个点作为初始提示，区域生长生成初步分割。RL代理通过观察图像和当前分割来预测新的点，区域生长据此更新分割掩模。奖励机制平衡了分割精度和体素不确定性，鼓励探索模糊区域，逃离局部最优，并进行样本特异性优化。", "result": "在PROMIS和PICAI两个公开数据集上，该框架相较于之前的最佳自动化方法，在分割精度上分别提高了9.9%和8.9%。其性能与放射科医生手动分割相当，同时将标注时间缩短了十倍。", "conclusion": "所提出的用户引导式框架能够通过用户提示实现前列腺癌MR图像的高精度分割，显著减少了用户标注的工作量，并优于现有的完全自动化方法，弥合了手动和全自动分割之间的差距。"}}
{"id": "2602.17877", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17877", "abs": "https://arxiv.org/abs/2602.17877", "authors": ["Markus Heinrichs", "Aydin Sezgin", "Rainer Kronberger"], "title": "A Scalable Reconfigurable Intelligent Surface with 3 Bit Phase Resolution and High Bandwidth for 3.6 GHz 5G/6G Applications", "comment": null, "summary": "Reconfigurable Intelligent Surfaces enable active control of wireless propagation channels, which is crucial for future 5G and 6G networks. This work presents a scalable RIS design operating at 3.6 GHz with both 1 bit and 3 bit phase resolution, supporting wideband applications. The unit cells employ low-cost printed circuit board technology with an innovative spring-contact feeding structure, enabling efficient assembly and reduced manufacturing complexity for large-area arrays. The design achieves broadband phase control, low power consumption, and high scalability, with experimental results demonstrating phase tunability across the n78 frequency band and competitive reflection performance compared to existing solutions. This RIS architecture provides a practical platform for experimental studies of smart radio environments, beam steering, and sensing applications in next-generation wireless networks.", "AI": {"tldr": "本文提出了一种可扩展的、低成本的、可重构智能表面（RIS）设计，工作在3.6 GHz，具有1位和3位相位分辨率，支持宽带应用，并实现了高效组装和可调谐相位。", "motivation": "未来的5G和6G网络需要能够主动控制无线传播信道的RIS技术，以实现智能无线环境。", "method": "设计了一种采用低成本PCB技术和创新弹簧触点馈电结构的RIS单元，实现了1位和3位相位分辨率，并进行了实验验证。", "result": "该RIS设计实现了宽带相位控制、低功耗和高可扩展性，实验证明其在n78频段内相位可调谐，且反射性能具有竞争力。", "conclusion": "该RIS架构为智能无线环境、波束赋形和传感应用等下一代无线网络的研究提供了一个实用的实验平台。"}}
{"id": "2602.17901", "categories": ["eess.IV", "cs.CV", "cs.GT"], "pdf": "https://arxiv.org/pdf/2602.17901", "abs": "https://arxiv.org/abs/2602.17901", "authors": ["Junkai Liu", "Ling Shao", "Le Zhang"], "title": "MeDUET: Disentangled Unified Pretraining for 3D Medical Image Synthesis and Analysis", "comment": null, "summary": "Self-supervised learning (SSL) and diffusion models have advanced representation learning and image synthesis. However, in 3D medical imaging, they remain separate: diffusion for synthesis, SSL for analysis. Unifying 3D medical image synthesis and analysis is intuitive yet challenging, as multi-center datasets exhibit dominant style shifts, while downstream tasks rely on anatomy, and site-specific style co-varies with anatomy across slices, making factors unreliable without explicit constraints. In this paper, we propose MeDUET, a 3D Medical image Disentangled UnifiEd PreTraining framework that performs SSL in the Variational Autoencoder (VAE) latent space which explicitly disentangles domain-invariant content from domain-specific style. The token demixing mechanism serves to turn disentanglement from a modeling assumption into an empirically identifiable property. Two novel proxy tasks, Mixed-Factor Token Distillation (MFTD) and Swap-invariance Quadruplet Contrast (SiQC), are devised to synergistically enhance disentanglement. Once pretrained, MeDUET is capable of (i) delivering higher fidelity, faster convergence, and improved controllability for synthesis, and (ii) demonstrating strong domain generalization and notable label efficiency for analysis across diverse medical benchmarks. In summary, MeDUET converts multi-source heterogeneity from an obstacle into a learning signal, enabling unified pretraining for 3D medical image synthesis and analysis. The code is available at https://github.com/JK-Liu7/MeDUET .", "AI": {"tldr": "MeDUET是一个3D医学图像解耦统一预训练框架，通过在VAE潜在空间中进行自监督学习，将领域不变内容与领域特定风格解耦，从而实现了3D医学图像的合成和分析的统一，并提高了性能和泛化能力。", "motivation": "现有的3D医学影像领域，自监督学习（SSL）和扩散模型通常是分开应用的（SSL用于分析，扩散模型用于合成）。然而，在一个统一的框架下进行3D医学图像的合成和分析是直观但具有挑战性的，尤其是在处理多中心数据集时，数据中存在的风格偏移会影响下游任务对解剖结构的依赖性。本研究旨在解决这个问题，统一3D医学图像的合成和分析。", "method": "提出MeDUET框架，该框架在变分自编码器（VAE）的潜在空间中进行自监督学习。该潜在空间能够显式地将领域不变内容与领域特定风格解耦。通过“token demixing”机制将解耦从模型假设转化为可经验识别的属性。设计了两种新的代理任务：Mixed-Factor Token Distillation (MFTD)和Swap-invariance Quadruplet Contrast (SiQC)，以协同增强解耦能力。", "result": "预训练后的MeDUET在图像合成方面，能够生成更高保真度、更快收敛速度且更具可控性的图像；在图像分析方面，展现出强大的领域泛化能力和显著的标签效率，在多个医学基准测试中表现出色。", "conclusion": "MeDUET将多源数据的异质性从障碍转化为学习信号，实现了3D医学图像合成与分析的统一预训练，提高了模型在合成和分析任务上的性能和泛化能力。"}}
{"id": "2602.17855", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17855", "abs": "https://arxiv.org/abs/2602.17855", "authors": ["Seungik Cho"], "title": "TopoGate: Quality-Aware Topology-Stabilized Gated Fusion for Longitudinal Low-Dose CT New-Lesion Prediction", "comment": null, "summary": "Longitudinal low-dose CT follow-ups vary in noise, reconstruction kernels, and registration quality. These differences destabilize subtraction images and can trigger false new lesion alarms. We present TopoGate, a lightweight model that combines the follow-up appearance view with the subtraction view and controls their influence through a learned, quality-aware gate. The gate is driven by three case-specific signals: CT appearance quality, registration consistency, and stability of anatomical topology measured with topological metrics. On the NLST--New-Lesion--LongCT cohort comprising 152 pairs from 122 patients, TopoGate improves discrimination and calibration over single-view baselines, achieving an area under the ROC curve of 0.65 with a standard deviation of 0.05 and a Brier score of 0.14. Removing corrupted or low-quality pairs, identified by the quality scores, further increases the area under the ROC curve from 0.62 to 0.68 and reduces the Brier score from 0.14 to 0.12. The gate responds predictably to degradation, placing more weight on appearance when noise grows, which mirrors radiologist practice. The approach is simple, interpretable, and practical for reliable longitudinal LDCT triage.", "AI": {"tldr": "本文提出了一种名为TopoGate的轻量级模型，用于提高纵向低剂量CT（LDCT）随访中肺结节检测的可靠性。该模型通过学习一个质量感知门控机制，结合了CT影像外观和影像差分信息，以应对随访过程中出现的噪声、重建核和配准质量差异导致的误报问题。", "motivation": "纵向LDCT随访中，影像质量（如噪声、重建核、配准）的不一致会导致影像差分图像不稳定，进而引发假阳性结节警报，影响疾病诊断的准确性。", "method": "TopoGate模型结合了CT影像外观视图和影像差分视图，并通过一个学习到的、质量感知的门控机制来控制两者的影响权重。该门控机制由三个案例特定的信号驱动：CT外观质量、配准一致性以及基于拓扑度量的解剖拓扑稳定性。", "result": "在NLST--New-Lesion--LongCT队列（包含152对来自122名患者的影像）上，TopoGate在区分能力和校准方面优于单一视图基线模型，ROC曲线下面积（AUC）为0.65（标准差0.05），Brier得分为0.14。通过质量评分识别出的低质量或损坏的影像对，去除后AUC从0.62提升到0.68，Brier得分从0.14降低到0.12。该门控机制对图像质量下降的响应具有可预测性，当噪声增加时，模型会更多地依赖于影像外观，这与放射科医生的实践相符。", "conclusion": "TopoGate模型简单、可解释且实用，能够有效地处理纵向LDCT随访中的影像质量变化，从而实现更可靠的结节筛查和分诊。"}}
{"id": "2602.17874", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17874", "abs": "https://arxiv.org/abs/2602.17874", "authors": ["J. Liu", "F. Milano"], "title": "Modal Energy for Power System Analysis: Definitions and Requirements", "comment": null, "summary": "Modal energy provides information complementary to and based on conventional eigenvalues and participation factors for power system modal analysis. However, modal energy definition is not unique. This letter clarifies the definitions and applicability of mainstream modal energy approaches, focusing on their mappings to eigenvalues and to the total system energy. It is shown that these mappings hold only under restrictive conditions, notably system normality, which limits their applicability in inverter-dominated power systems.", "AI": {"tldr": "该研究分析了模态能量在电力系统模态分析中的应用，指出主流模态能量定义的局限性，尤其是在逆变器主导的电力系统中。", "motivation": "现有模态能量定义不唯一，且在逆变器主导的电力系统中适用性受限，促使研究者对主流模态能量方法进行澄清和评估。", "method": "通过分析主流模态能量方法与特征值和系统总能量的映射关系，并考察其在系统正规性条件下的适用性。", "result": "研究表明，模态能量与特征值及系统总能量的映射关系仅在系统正规性等限制条件下成立，这限制了其在逆变器主导的电力系统中的应用。", "conclusion": "主流模态能量方法的适用性有限，尤其是在逆变器主导的电力系统中，需要谨慎使用或开发更普适的方法。"}}
{"id": "2602.17986", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17986", "abs": "https://arxiv.org/abs/2602.17986", "authors": ["Zengtian Deng", "Yimeng He", "Yu Shi", "Lixia Wang", "Touseef Ahmad Qureshi", "Xiuzhen Huang", "Debiao Li"], "title": "From Global Radiomics to Parametric Maps: A Unified Workflow Fusing Radiomics and Deep Learning for PDAC Detection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Radiomics and deep learning both offer powerful tools for quantitative medical imaging, but most existing fusion approaches only leverage global radiomic features and overlook the complementary value of spatially resolved radiomic parametric maps. We propose a unified framework that first selects discriminative radiomic features and then injects them into a radiomics-enhanced nnUNet at both the global and voxel levels for pancreatic ductal adenocarcinoma (PDAC) detection. On the PANORAMA dataset, our method achieved AUC = 0.96 and AP = 0.84 in cross-validation. On an external in-house cohort, it achieved AUC = 0.95 and AP = 0.78, outperforming the baseline nnUNet; it also ranked second in the PANORAMA Grand Challenge. This demonstrates that handcrafted radiomics, when injected at both global and voxel levels, provide complementary signals to deep learning models for PDAC detection. Our code can be found at https://github.com/briandzt/dl-pdac-radiomics-global-n-paramaps", "AI": {"tldr": "提出了一种结合全局和体素级影像组学特征的深度学习框架，用于检测胰腺导管腺癌（PDAC），并在PANORAMA数据集和外部队列上取得了优异的性能。", "motivation": "现有影像组学和深度学习的融合方法多只关注全局影像组学特征，忽略了空间分辨率影像组学参数图的互补价值。本研究旨在利用全局和体素两个层面的影像组学特征来增强深度学习模型在PDAC检测中的性能。", "method": "首先，选择判别的影像组学特征；然后，将这些特征注入到一个增强型的nnUNet模型中，同时在全局和体素层面进行PDAC检测。", "result": "在PANORAMA数据集的交叉验证中，该方法实现了0.96的AUC和0.84的AP。在外部自有队列中，实现了0.95的AUC和0.78的AP，优于基线nnUNet模型。该方法在PANORAMA Grand Challenge中也取得了第二名的成绩。", "conclusion": "手工制作的影像组学特征，当在全局和体素层面注入深度学习模型时，可以为PDAC检测提供与深度学习模型互补的信号，从而提升模型的检测性能。"}}
{"id": "2602.18031", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18031", "abs": "https://arxiv.org/abs/2602.18031", "authors": ["Yan Chen", "Ruyi Huang", "Cheng Liu"], "title": "Decision Support under Prediction-Induced Censoring", "comment": null, "summary": "In many data-driven online decision systems, actions determine not only operational costs but also the data availability for future learning -- a phenomenon termed Prediction-Induced Censoring (PIC). This challenge is particularly acute in large-scale resource allocation for generative AI (GenAI) serving: insufficient capacity triggers shortages but hides the true demand, leaving the system with only a \"greater-than\" constraint. Standard decision-making approaches that rely on uncensored data suffer from selection bias, often locking the system into a self-reinforcing low-provisioning trap. To break this loop, this paper proposes an adaptive approach named PIC-Reinforcement Learning (PIC-RL), a closed-loop framework that transforms censoring from a data quality problem into a decision signal. PIC-RL integrates (1) Uncertainty-Aware Demand Prediction to manage the information-cost trade-off, (2) Pessimistic Surrogate Inference to construct decision-aligned conservative feedback from shortage events, and (3) Dual-Timescale Adaptation to stabilize online learning against distribution drift. The analysis provides theoretical guarantees that the feedback design corrects the selection bias inherent in naive learning. Experiments on production Alibaba GenAI traces demonstrate that PIC-RL consistently outperforms state-of-the-art baselines, reducing service degradation by up to 50% while maintaining cost efficiency.", "AI": {"tldr": "本文提出了一种名为PIC-RL的自适应方法，用于解决生成式AI服务中的预测诱导审查（PIC）问题，该问题会因资源分配不足而导致数据可用性降低。PIC-RL通过集成不确定性感知需求预测、悲观代理推断和双时间尺度适应，将审查转化为决策信号，有效纠正了选择偏差，并在真实数据上表现优于现有方法。", "motivation": "在生成式AI服务等数据驱动的在线决策系统中，资源分配不足会导致需求信息缺失（预测诱导审查），使得标准决策方法产生选择偏差，陷入低资源供给的恶性循环。", "method": "提出了一种名为PIC-RL的闭环框架，包含三个关键组件：1. 不确定性感知需求预测，用于平衡信息和成本；2. 悲观代理推断，用于从短缺事件中构建保守反馈；3. 双时间尺度适应，用于稳定在线学习。", "result": "PIC-RL能够纠正选择偏差，并在阿里巴巴GenAI生产数据上进行实验，结果显示其性能持续优于现有最先进方法，可将服务降级减少高达50%，同时保持成本效益。", "conclusion": "PIC-RL是一种有效的自适应方法，能够将预测诱导审查带来的数据问题转化为有用的决策信号，打破低资源供给的恶性循环，并在大规模GenAI服务中实现性能和成本效益的双重提升。"}}
{"id": "2602.17768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17768", "abs": "https://arxiv.org/abs/2602.17768", "authors": ["Boda Lin", "Yongjie Zhu", "Xiaocheng Gong", "Wenyu Qin", "Meng Wang"], "title": "KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding", "comment": "26 pages", "summary": "Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.", "AI": {"tldr": "研究提出了一个结合运动学计算和语言分析的自动标注流程，构建了KPM-Bench数据集，以提升视频描述中细粒度运动细节的准确性并减少幻觉。同时，提出了MoPE算法来提取文本中的运动属性，并基于此开发了一种独立的幻觉评估指标。通过将MoPE集成到GRPO后训练框架，有效降低了运动中心视频描述中的幻觉问题。", "motivation": "现有的视频描述模型在准确描述细粒度运动细节和减少幻觉方面存在显著不足，尤其是在运动密集型视频中，精确描绘复杂动作和肢体动态至关重要但常被忽视。", "method": "1. 构建了一个集成了运动学计算和语言解析的自动标注流程，用于分解和描述复杂的人类运动。2. 基于该流程，创建并发布了KPM-Bench数据集，包含细粒度视频-字幕对、运动理解相关的问答对以及专门评估运动描述幻觉的评估集。3. 提出MoPE算法，用于从文本字幕中提取运动特定属性。4. 利用MoPE开发了一个独立的幻觉评估指标。5. 将MoPE集成到GRPO后训练框架，以减轻幻觉问题。", "result": "1. 提出了KPM-Bench数据集，支持细粒度的运动理解。2. 提出了MoPE算法，能够准确从文本中提取运动属性。3. 开发了一种不依赖大型模型即可评估幻觉的指标。4. 集成MoPE到GRPO框架后，显著减少了运动中心视频描述中的幻觉问题，提高了模型可靠性。", "conclusion": "通过结合运动学分析和语言处理，并引入KPM-Bench数据集和MoPE算法，可以有效地提升视频描述模型在细粒度运动描绘方面的能力，并系统性地解决幻觉问题，尤其是在运动中心视频领域。"}}
{"id": "2602.18119", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18119", "abs": "https://arxiv.org/abs/2602.18119", "authors": ["Chris Tomy", "Mo Vali", "David Pertzborn", "Tammam Alamatouri", "Anna Mühlig", "Orlando Guntinas-Lichius", "Anna Xylander", "Eric Michele Fantuzzi", "Matteo Negro", "Francesco Crisafi", "Pietro Lio", "Tiago Azevedo"], "title": "RamanSeg: Interpretability-driven Deep Learning on Raman Spectra for Cancer Diagnosis", "comment": "12 pages, 8 figures", "summary": "Histopathology, the current gold standard for cancer diagnosis, involves the manual examination of tissue samples after chemical staining, a time-consuming process requiring expert analysis. Raman spectroscopy is an alternative, stain-free method of extracting information from samples. Using nnU-Net, we trained a segmentation model on a novel dataset of spatial Raman spectra aligned with tumour annotations, achieving a mean foreground Dice score of 80.9%, surpassing previous work. Furthermore, we propose a novel, interpretable, prototype-based architecture called RamanSeg. RamanSeg classifies pixels based on discovered regions of the training set, generating a segmentation mask. Two variants of RamanSeg allow a trade-off between interpretability and performance: one with prototype projection and another projection-free version. The projection-free RamanSeg outperformed a U-Net baseline with a mean foreground Dice score of 67.3%, offering a meaningful improvement over a black-box training approach.", "AI": {"tldr": "研究人员训练了一个名为 nnU-Net 的分割模型，在空间拉曼光谱数据集上取得了 80.9% 的平均 Dice 分数，优于以往的研究。他们还提出了一种名为 RamanSeg 的新型可解释原型模型，其投影免费版本在 Dice 分数上优于 U-Net 基线。", "motivation": "传统的组织病理学诊断耗时且需要专家分析。拉曼光谱是一种无需染色即可提取信息的替代方法，但需要有效的分析模型。", "method": "研究人员使用 nnU-Net 在空间拉曼光谱数据集上训练了一个分割模型。此外，他们提出了一种名为 RamanSeg 的原型模型，并提出了两种变体（带原型投影和不带原型投影），用于像素分类和生成分割掩码。", "result": "nnU-Net 模型在空间拉曼光谱数据集上达到了 80.9% 的平均前景 Dice 分数。投影免费的 RamanSeg 模型取得了 67.3% 的平均前景 Dice 分数，优于 U-Net 基线。", "conclusion": "nnU-Net 和 RamanSeg 模型在空间拉曼光谱分析和肿瘤分割方面取得了显著进展。RamanSeg 提供了一种在可解释性和性能之间进行权衡的方法，并优于传统的黑盒模型。"}}
{"id": "2602.17831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17831", "abs": "https://arxiv.org/abs/2602.17831", "authors": ["Simon Henniger", "Gabriel Poesia"], "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels", "comment": "Project website: https://token-games.ai/", "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.", "AI": {"tldr": "本文提出了一种名为“Token Games (TTG)”的评估框架，通过让大型语言模型（LLM）互相生成和解决编程谜题来评估其推理能力，避免了昂贵的人工标注，并产生了与现有基准相匹配的模型排名，同时揭示了当前模型在创造高质量谜题方面的不足。", "motivation": "评估 LLM 的推理能力变得越来越困难，因为模型在不断进步。人工设计高难度问题成本高昂，且存在模型可能在训练中见过类似问题而导致评估不准确的担忧。因此，需要一种更有效、更不易被“饱和”的评估方法。", "method": "借鉴数学决斗的理念，设计了“Token Games (TTG)”框架。该框架利用编程谜题的形式（给定一个返回布尔值的 Python 函数，找出使其返回 True 的输入），让模型互相生成谜题。通过成对的“决斗”结果计算 Elo 评分，以相对比较模型的能力。在 TTG 上评估了 10 个前沿模型。", "result": "TTG 框架产生的模型排名与现有基准（如 Humanity's Last Exam）高度吻合，且无需人工参与谜题设计。同时，研究发现当前模型在创建高质量谜题方面仍然面临巨大挑战，这是现有基准未曾衡量的能力。", "conclusion": "TTG 提供了一种新的模型评估范式，该范式在设计上不易被饱和，并且可以同时评估模型解决问题、创造力以及任务创建等多种能力。这种互相生成谜题的机制为评估 LLM 的推理能力提供了一个有前景的方向。"}}
{"id": "2602.17815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17815", "abs": "https://arxiv.org/abs/2602.17815", "authors": ["Zhining Zhang", "Wentao Zhu", "Chi Han", "Yizhou Wang", "Heng Ji"], "title": "Neural Synchrony Between Socially Interacting Language Models", "comment": "Accepted at ICLR 2026", "summary": "Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the \"social minds\" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.", "AI": {"tldr": "本研究通过分析大型语言模型（LLMs）在社交互动中的神经同步性，提出了一个分析LLMs社会性的新方法，并发现LLMs的神经同步性与其社交表现密切相关，揭示了LLMs与人类在社交互动内在动力学上的惊人相似之处。", "motivation": "尽管大型语言模型（LLMs）在模拟人类行为方面表现出色，但关于它们是否具备“社交心智”仍存在争议。传统的社交心智被认为是生物体的专属特质。本研究旨在探索LLMs之间的神经同步性，以此作为评估其社会性的实证依据。", "method": "本研究引入了“社交模拟中的神经同步性”作为分析LLMs社会性的新代理指标。通过精心设计的实验，测量和分析了在社交模拟过程中，多LLM系统之间的神经活动同步程度。", "result": "研究发现，LLMs之间的神经同步性能够可靠地反映其社交参与度和时间对齐性。神经同步性与LLMs的社交表现呈强相关性，表明神经同步性与其社交行为之间存在重要联系。", "conclusion": "本研究通过量化LLMs的神经同步性，为理解LLMs的“社交心智”提供了一个新的视角。研究结果表明，LLMs在社交互动中表现出与人类相似的内在动力学，挑战了社交心智仅限于生物体的传统观点。"}}
{"id": "2602.18048", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18048", "abs": "https://arxiv.org/abs/2602.18048", "authors": ["N. Naveen Mukesh", "Debraj Chakraborty"], "title": "Incremental Data Driven Transfer Identification", "comment": "15 Pages, 7 figures", "summary": "We introduce a geometric method for online transfer identification of a deterministic linear time-invariant system. At the beginning of the identification process, we assume access to abundant data from a system that is similar, though not identical, to the true system. In the early stages of data collection from the true system, the dataset generated is still not sufficiently informative to enable precise identification. Consequently, multiple candidate models remain consistent with the observations available at that point. Our method picks, at each instant, the model closest to the similar system that is consistent with the current data. As more data are collected, the proposed model gradually moves away from the initial similar system and eventually converges to the true system when the data set grows to be informative. Numerical examples demonstrate the effectiveness of the incremental transfer identification paradigm, where identified models with minimal data are used to solve the pole placement problem.", "AI": {"tldr": "提出了一种几何方法，用于对确定性线性时不变系统进行在线传递函数识别。该方法在初始阶段利用一个相似但不同的系统数据，随着真实系统数据的逐步收集，模型会逐渐逼近真实系统。", "motivation": "在系统辨识的早期阶段，收集到的数据不足以精确识别系统，存在多个候选模型。研究者希望开发一种方法，能在数据不充分的情况下，逐步、稳定地辨识出目标系统。", "method": "采用几何方法，在辨识过程中，选择与当前数据一致且最接近于已知相似系统的模型。随着数据量的增加，辨识模型会逐渐从相似系统移动并收敛到真实系统。", "result": "数值示例表明，该增量辨识方法在数据量最少的情况下能够有效地辨识系统，并将其用于解决极点配置问题。", "conclusion": "该几何方法能够有效地进行在线传递函数辨识，即使在初始数据不充分的情况下，也能通过增量学习逐步收敛到真实系统，并可应用于实际的控制问题。"}}
{"id": "2602.17794", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17794", "abs": "https://arxiv.org/abs/2602.17794", "authors": ["Neethan Ratnakumar", "Mariya Huzaifa Tohfafarosh", "Saanya Jauhri", "Xianlian Zhou"], "title": "Reinforcement-Learning-Based Assistance Reduces Squat Effort with a Modular Hip--Knee Exoskeleton", "comment": null, "summary": "Squatting is one of the most demanding lower-limb movements, requiring substantial muscular effort and coordination. Reducing the physical demands of this task through intelligent and personalized assistance has significant implications, particularly in industries involving repetitive low-level assembly activities. In this study, we evaluated the effectiveness of a neural network controller for a modular Hip-Knee exoskeleton designed to assist squatting tasks. The neural network controller was trained via reinforcement learning (RL) in a physics-based, human-exoskeleton interaction simulation environment. The controller generated real-time hip and knee assistance torques based on recent joint-angle and velocity histories. Five healthy adults performed three-minute metronome-guided squats under three conditions: (1) no exoskeleton (No-Exo), (2) exoskeleton with Zero-Torque, and (3) exoskeleton with active assistance (Assistance). Physiological effort was assessed using indirect calorimetry and heart rate monitoring, alongside concurrent kinematic data collection. Results show that the RL-based controller adapts to individuals by producing torque profiles tailored to each subject's kinematics and timing. Compared with the Zero-Torque and No-Exo condition, active assistance reduced the net metabolic rate by approximately 10%, with minor reductions observed in heart rate. However, assisted trials also exhibited reduced squat depth, reflected by smaller hip and knee flexion. These preliminary findings suggest that the proposed controller can effectively lower physiological effort during repetitive squatting, motivating further improvements in both hardware design and control strategies.", "AI": {"tldr": "该研究使用基于强化学习的神经网络控制器来控制模块化髋-膝外骨骼，以辅助深蹲任务。实验结果表明，该控制器能降低生理负荷，但同时会减少下蹲深度。", "motivation": "为了降低重复性低强度装配工作中深蹲任务的体力需求，研究者希望通过智能和个性化的辅助来减轻身体负担。", "method": "研究者利用强化学习在一个基于物理的人体-外骨骼交互仿真环境中训练了一个神经网络控制器。该控制器能根据关节角度和速度的历史信息实时生成髋部和膝部的辅助力矩。之后，招募了五名健康成年人，在三种条件下（无外骨骼、外骨骼零力矩、外骨骼主动辅助）进行深蹲测试，并使用间接量热法和心率监测来评估生理负荷，同时收集运动学数据。", "result": "研究结果显示，基于强化学习的控制器能够根据个体的运动学和时序特征生成定制化的力矩，从而适应不同用户。与零力矩和无外骨骼条件相比，主动辅助将净代谢率降低了约10%，心率也有少量降低。然而，主动辅助条件下，下蹲深度减小，表现为髋部和膝部屈曲角度减小。", "conclusion": "该研究提出的神经网络控制器能够有效降低重复性深蹲任务中的生理负荷。尽管存在下蹲深度减小的问题，但这些初步发现为改进外骨骼的硬件设计和控制策略提供了动力。"}}
{"id": "2602.17770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17770", "abs": "https://arxiv.org/abs/2602.17770", "authors": ["Balamurugan Thambiraja", "Omid Taheri", "Radek Danecek", "Giorgio Becherini", "Gerard Pons-Moll", "Justus Thies"], "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild", "comment": "ICLR2026; Project page: https://balamuruganthambiraja.github.io/CLUTCH/", "summary": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.", "AI": {"tldr": "该研究提出了一个名为3D-HIW的大规模数据集和CLUTCH系统，用于在野外场景下生成逼真的3D手部动画，解决了现有方法数据受限和动画保真度不足的问题。", "motivation": "现有文本到手部运动生成和手部动画描述的方法依赖于昂贵的、动作和上下文受限的工作室捕捉数据集，难以扩展到“野外”场景。此外，现有模型在捕捉动画保真度和文本-运动对齐方面存在困难。", "method": "研究者提出了两个主要贡献：1. 创建了一个名为'3D Hands in the Wild' (3D-HIW) 的数据集，包含32K个3D手部运动序列和对齐的文本。2. 提出了一种基于LLM的手部动画系统CLUTCH，该系统包含两项创新：a) SHIFT，一种新的VQ-VAE架构，用于对运动进行分词；b) 一个几何精炼阶段，用于微调LLM。数据集是通过结合视觉-语言模型（VLMs）和先进的3D手部跟踪器的数据标注流程构建的。", "result": "CLUTCH系统在文本到运动和运动到文本任务上取得了最先进的性能，建立了第一个可扩展的野外手部运动建模基准。SHIFT VQ-VAE提高了泛化能力和重建保真度。", "conclusion": "该研究成功地构建了一个大规模的野外手部运动数据集，并开发了一个先进的LLM驱动的手部动画系统CLUTCH，有效地解决了现有方法的局限性，并在相关任务上取得了显著的性能提升，为未来的研究奠定了基础。"}}
{"id": "2602.17902", "categories": ["cs.AI", "cs.MA", "cs.SE", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2602.17902", "abs": "https://arxiv.org/abs/2602.17902", "authors": ["Jiaru Bai", "Abdulrahman Aldossary", "Thomas Swanick", "Marcel Müller", "Yeonghun Kang", "Zijian Zhang", "Jin Won Lee", "Tsz Wai Ko", "Mohammad Ghazi Vakili", "Varinia Bernales", "Alán Aspuru-Guzik"], "title": "El Agente Gráfico: Structured Execution Graphs for Scientific Agents", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate scientific workflows, yet their integration with heterogeneous computational tools remains ad hoc and fragile. Current agentic approaches often rely on unstructured text to manage context and coordinate execution, generating often overwhelming volumes of information that may obscure decision provenance and hinder auditability. In this work, we present El Agente Gráfico, a single-agent framework that embeds LLM-driven decision-making within a type-safe execution environment and dynamic knowledge graphs for external persistence. Central to our approach is a structured abstraction of scientific concepts and an object-graph mapper that represents computational state as typed Python objects, stored either in memory or persisted in an external knowledge graph. This design enables context management through typed symbolic identifiers rather than raw text, thereby ensuring consistency, supporting provenance tracking, and enabling efficient tool orchestration. We evaluate the system by developing an automated benchmarking framework across a suite of university-level quantum chemistry tasks previously evaluated on a multi-agent system, demonstrating that a single agent, when coupled to a reliable execution engine, can robustly perform complex, multi-step, and parallel computations. We further extend this paradigm to two other large classes of applications: conformer ensemble generation and metal-organic framework design, where knowledge graphs serve as both memory and reasoning substrates. Together, these results illustrate how abstraction and type safety can provide a scalable foundation for agentic scientific automation beyond prompt-centric designs.", "AI": {"tldr": "提出了一种名为 El Agente Gráfico 的新框架，它将大型语言模型（LLM）的决策能力嵌入到类型安全的执行环境和动态知识图中，以实现科学工作流程的自动化，解决了现有方法中信息过载和可审计性差的问题。", "motivation": "现有将 LLM 集成到科学工作流程的方法存在集成 ad hoc、易碎、信息过载、决策溯源困难和审计性差等问题。", "method": "该框架的核心是一个结构化的科学概念抽象，以及一个对象-图映射器。计算状态被表示为类型化的 Python 对象，并存储在内存或外部知识图中。上下文管理通过类型化的符号标识符而非原始文本实现。", "result": "在量子化学任务上进行的基准测试表明，单个智能体在结合可靠的执行引擎后，能够鲁棒地执行复杂、多步和并行计算。此外，该框架成功应用于构象集合生成和金属有机框架设计。", "conclusion": "通过抽象和类型安全，可以为超越基于提示的设计的智能科学自动化提供可扩展的基础，从而实现更健壮、可溯源和高效的科学工作流程自动化。"}}
{"id": "2602.18400", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18400", "abs": "https://arxiv.org/abs/2602.18400", "authors": ["Junkai Liu", "Nay Aung", "Theodoros N. Arvanitis", "Joao A. C. Lima", "Steffen E. Petersen", "Daniel C. Alexander", "Le Zhang"], "title": "Exploiting Completeness Perception with Diffusion Transformer for Unified 3D MRI Synthesis", "comment": null, "summary": "Missing data problems, such as missing modalities in multi-modal brain MRI and missing slices in cardiac MRI, pose significant challenges in clinical practice. Existing methods rely on external guidance to supply detailed missing state for instructing generative models to synthesize missing MRIs. However, manual indicators are not always available or reliable in real-world scenarios due to the unpredictable nature of clinical environments. Moreover, these explicit masks are not informative enough to provide guidance for improving semantic consistency. In this work, we argue that generative models should infer and recognize missing states in a self-perceptive manner, enabling them to better capture subtle anatomical and pathological variations. Towards this goal, we propose CoPeDiT, a general-purpose latent diffusion model equipped with completeness perception for unified synthesis of 3D MRIs. Specifically, we incorporate dedicated pretext tasks into our tokenizer, CoPeVAE, empowering it to learn completeness-aware discriminative prompts, and design MDiT3D, a specialized diffusion transformer architecture for 3D MRI synthesis, that effectively uses the learned prompts as guidance to enhance semantic consistency in 3D space. Comprehensive evaluations on three large-scale MRI datasets demonstrate that CoPeDiT significantly outperforms state-of-the-art methods, achieving superior robustness, generalizability, and flexibility. The code is available at https://github.com/JK-Liu7/CoPeDiT .", "AI": {"tldr": "提出CoPeDiT，一个通用的3D MRI合成模型，通过在tokenizer中加入预训练任务来感知缺失状态，并使用专门的扩散Transformer架构，在没有外部指导的情况下，能更好地处理多模态MRI和心脏MRI中的缺失数据问题。", "motivation": "现有方法依赖外部指导来合成缺失的MRI，但在实际临床中，这种指导不可靠且不具信息量。因此，需要一个能自我感知缺失状态的模型来捕捉细微的解剖和病理变化。", "method": "提出CoPeDiT，一个通用的3D MRI合成模型。其核心是CoPeVAE，一个带有专用预训练任务的tokenizer，用于学习完整的感知判别式提示。MDiT3D是一个专门用于3D MRI合成的扩散Transformer架构，利用这些提示来增强3D空间中的语义一致性。", "result": "在三个大规模MRI数据集上的评估表明，CoPeDiT在鲁棒性、泛化性和灵活性方面显著优于现有最先进的方法。", "conclusion": "CoPeDiT通过自我感知缺失状态，为3D MRI合成提供了一个通用且有效的解决方案，克服了现有方法对外部指导的依赖，并在多项评估中取得了优越的性能。"}}
{"id": "2602.17818", "categories": ["cs.RO", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.17818", "abs": "https://arxiv.org/abs/2602.17818", "authors": ["Zachary Turcotte", "François Grondin"], "title": "Lend me an Ear: Speech Enhancement Using a Robotic Arm with a Microphone Array", "comment": null, "summary": "Speech enhancement performance degrades significantly in noisy environments, limiting the deployment of speech-controlled technologies in industrial settings, such as manufacturing plants. Existing speech enhancement solutions primarly rely on advanced digital signal processing techniques, deep learning methods, or complex software optimization techniques. This paper introduces a novel enhancement strategy that incorporates a physical optimization stage by dynamically modifying the geometry of a microphone array to adapt to changing acoustic conditions. A sixteen-microphone array is mounted on a robotic arm manipulator with seven degrees of freedom, with microphones divided into four groups of four, including one group positioned near the end-effector. The system reconfigures the array by adjusting the manipulator joint angles to place the end-effector microphones closer to the target speaker, thereby improving the reference signal quality. This proposed method integrates sound source localization techniques, computer vision, inverse kinematics, minimum variance distortionless response beamformer and time-frequency masking using a deep neural network. Experimental results demonstrate that this approach outperforms other traditional recording configruations, achieving higher scale-invariant signal-to-distortion ratio and lower word error rate accross multiple input signal-to-noise ratio conditions.", "AI": {"tldr": "本文提出一种新颖的麦克风阵列几何形状动态调整策略，用于在嘈杂工业环境中增强语音信号，通过机器人手臂重新配置阵列以优化近距离拾音，并在实验中证明其优于传统方法。", "motivation": "现有语音增强技术在工业嘈杂环境中表现不佳，限制了语音控制技术的应用。需要一种能适应动态声学条件的新方法。", "method": "利用一个带有七自由度机器人手臂的十六麦克风阵列，通过调整关节角度使近端麦克风靠近声源。结合了声源定位、计算机视觉、逆运动学、MVDR波束形成器和DNN时间-频率掩码。", "result": "该方法在不同信噪比条件下，相比传统录音配置，获得了更高的尺度不变失真比（SI-SDR）和更低的词错误率（WER）。", "conclusion": "通过动态物理优化麦克风阵列几何形状，可以有效改善工业嘈杂环境下的语音增强性能，证明了物理与信号处理结合方法的潜力。"}}
{"id": "2602.17848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17848", "abs": "https://arxiv.org/abs/2602.17848", "authors": ["Cassandra L. Jacobs", "Morgan Grobol"], "title": "On the scaling relationship between cloze probabilities and language model next-token prediction", "comment": null, "summary": "Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.", "AI": {"tldr": "更大的语言模型在预测眼动和阅读时间数据方面表现更好，因为它们能更准确地估计下一个词及其生成概率，并且语义上更符合人类的填空反应。", "motivation": "探索更大语言模型在预测阅读行为数据方面的能力，以及理解其性能提升的原因。", "method": "比较不同大小的语言模型在预测眼动和阅读时间数据上的表现，并分析其在词汇共现统计和语义对齐方面的差异。", "result": "更大的语言模型在预测任务中分配的概率质量虽然仍低于人类反应，但其对下一个词的估计质量更高。它们在语义上与人类填空反应更一致，并且对低层次的词汇共现统计信息不那么敏感。", "conclusion": "更大的语言模型因其更大的记忆容量，能够更好地推测语义上更恰当的词语，从而提高了对人类阅读行为的预测能力，但这也导致它们对与单词识别相关的低层次信息不那么敏感。"}}
{"id": "2602.17785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17785", "abs": "https://arxiv.org/abs/2602.17785", "authors": ["Xinwei Ju", "Rema Daher", "Danail Stoyanov", "Sophia Bano", "Francisco Vasconcelos"], "title": "Multi-Modal Monocular Endoscopic Depth and Pose Estimation with Edge-Guided Self-Supervision", "comment": "14 pages, 6 figures; early accepted by IPCAI2026", "summary": "Monocular depth and pose estimation play an important role in the development of colonoscopy-assisted navigation, as they enable improved screening by reducing blind spots, minimizing the risk of missed or recurrent lesions, and lowering the likelihood of incomplete examinations. However, this task remains challenging due to the presence of texture-less surfaces, complex illumination patterns, deformation, and a lack of in-vivo datasets with reliable ground truth. In this paper, we propose **PRISM** (Pose-Refinement with Intrinsic Shading and edge Maps), a self-supervised learning framework that leverages anatomical and illumination priors to guide geometric learning. Our approach uniquely incorporates edge detection and luminance decoupling for structural guidance. Specifically, edge maps are derived using a learning-based edge detector (e.g., DexiNed or HED) trained to capture thin and high-frequency boundaries, while luminance decoupling is obtained through an intrinsic decomposition module that separates shading and reflectance, enabling the model to exploit shading cues for depth estimation. Experimental results on multiple real and synthetic datasets demonstrate state-of-the-art performance. We further conduct a thorough ablation study on training data selection to establish best practices for pose and depth estimation in colonoscopy. This analysis yields two practical insights: (1) self-supervised training on real-world data outperforms supervised training on realistic phantom data, underscoring the superiority of domain realism over ground truth availability; and (2) video frame rate is an extremely important factor for model performance, where dataset-specific video frame sampling is necessary for generating high quality training data.", "AI": {"tldr": "本文提出了一种名为PRISM的自监督学习框架，用于结肠镜辅助导航中的单目深度和姿态估计，通过结合边缘检测和亮度解耦等解剖学和照明先验来指导几何学习，并在真实和合成数据集上取得了最先进的性能。", "motivation": "单目深度和姿态估计对于结肠镜辅助导航至关重要，但由于纹理缺失、复杂光照、形变以及缺乏可靠真实数据的挑战性很大。", "method": "提出PRISM框架，一种自监督学习方法。该方法利用学习到的边缘检测器（如DexiNed或HED）提取高频边界，并使用内禀分解模块分离光照和反射率，从而解耦亮度。这些信息被用来指导几何学习。", "result": "在多个真实和合成数据集上实现了最先进的性能。消融研究表明，在真实世界数据上进行自监督训练优于在逼真幻影数据上进行监督训练，并且视频帧率对模型性能有显著影响。", "conclusion": "PRISM框架在结肠镜深度和姿态估计方面表现出色。研究强调了领域真实性（real-world data）的重要性，以及视频帧率对模型性能的关键作用，并为训练数据的选择提供了实用建议。"}}
{"id": "2602.17910", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17910", "abs": "https://arxiv.org/abs/2602.17910", "authors": ["Hanjing Shi", "Dominic DiFranzo"], "title": "Alignment in Time: Peak-Aware Orchestration for Long-Horizon Agentic Systems", "comment": null, "summary": "Traditional AI alignment primarily focuses on individual model outputs; however, autonomous agents in long-horizon workflows require sustained reliability across entire interaction trajectories. We introduce APEMO (Affect-aware Peak-End Modulation for Orchestration), a runtime scheduling layer that optimizes computational allocation under fixed budgets by operationalizing temporal-affective signals. Instead of modifying model weights, APEMO detects trajectory instability through behavioral proxies and targets repairs at critical segments, such as peak moments and endings. Evaluation across multi-agent simulations and LLM-based planner--executor flows demonstrates that APEMO consistently enhances trajectory-level quality and reuse probability over structural orchestrators. Our results reframe alignment as a temporal control problem, offering a resilient engineering pathway for the development of long-horizon agentic systems.", "AI": {"tldr": "本文提出了一种名为APEMO的运行时调度层，它通过操纵时间情感信号来优化固定预算下的计算分配，以提高自主代理在长周期工作流中的持续可靠性。APEMO通过检测行为代理的轨迹不稳定性，并针对峰值时刻和结束等关键片段进行修复，从而提高了轨迹质量和可复用性。", "motivation": "现有的AI对齐方法主要关注单个模型输出的可靠性，但对于需要跨越整个交互轨迹的长期工作流中的自主代理，需要提高其在整个交互过程中的持续可靠性。", "method": "APEMO是一个运行时调度层，它通过操纵时间情感信号（temporal-affective signals）来优化计算分配。它通过行为代理（behavioral proxies）检测轨迹不稳定性，并针对关键片段（如峰值时刻和结束）进行修复，而不是修改模型权重。", "result": "在多智能体模拟和基于LLM的规划器-执行器流程的评估中，APEMO与结构化编排器相比，始终能提高轨迹层面的质量和可复用性。", "conclusion": "研究结果将AI对齐重新定义为时间控制问题，并为开发长周期代理系统提供了一条有弹性的工程路径。"}}
{"id": "2602.17822", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17822", "abs": "https://arxiv.org/abs/2602.17822", "authors": ["Daniel Hartmann", "Kristýna Hamříková", "Aleš Vysocký", "Vendula Laciok", "Aleš Bernatík"], "title": "Evolution of Safety Requirements in Industrial Robotics: Comparative Analysis of ISO 10218-1/2 (2011 vs. 2025) and Integration of ISO/TS 15066", "comment": null, "summary": "Industrial robotics has established itself as an integral component of large-scale manufacturing enterprises. Simultaneously, collaborative robotics is gaining prominence, introducing novel paradigms of human-machine interaction. These advancements have necessitated a comprehensive revision of safety standards, specifically incorporating requirements for cybersecurity and protection against unauthorized access in networked robotic systems. This article presents a comparative analysis of the ISO 10218:2011 and ISO 10218:2025 standards, examining the evolution of their structure, terminology, technical requirements, and annexes. The analysis reveals significant expansions in functional safety and cybersecurity, the introduction of new classifications for robots and collaborative applications, and the normative integration of the technical specification ISO/TS 15066. Consequently, the new edition synthesizes mechanical, functional, and digital safety requirements, establishing a comprehensive framework for the design and operation of modern robotic systems.", "AI": {"tldr": "本文比较了ISO 10218:2011和ISO 10218:2025标准，重点分析了新标准在功能安全、网络安全、机器人分类以及整合ISO/TS 15066方面的扩展和变化，为现代机器人系统的设计和操作提供了更全面的安全框架。", "motivation": "随着工业机器人和协作机器人的普及，网络安全和防未经授权访问的需求日益增长，这促使了对现有安全标准的更新。", "method": "通过比较ISO 10218:2011和ISO 10218:2025两个标准在结构、术语、技术要求和附录方面的差异。", "result": "新版标准显著扩展了功能安全和网络安全的要求，引入了新的机器人和协作应用分类，并将ISO/TS 15066规范化整合，从而在机械、功能和数字安全方面建立了统一的框架。", "conclusion": "ISO 10218:2025对ISO 10218:2011进行了重大更新，更全面地涵盖了现代机器人系统所需的机械、功能和数字安全需求，尤其是在网络安全和协作应用方面。"}}
{"id": "2602.18059", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18059", "abs": "https://arxiv.org/abs/2602.18059", "authors": ["Junseon Park", "Hyeongon Park", "Rahul K. Gupta"], "title": "Iterative McCormick Relaxation for Joint Impedance Control and Network Topology Optimization", "comment": null, "summary": "Power system operators are increasingly deploying Variable Impedance Devices (VIDs), e.g., Smart Wires, and Network Topology Optimization (NTO) schemes for mitigating operational challenges such as line and transformer congestion, and voltage violations. This work aims to optimize and coordinate the operation of distributed VIDs considering fixed and optimized topologies. This problem is inherently non-linear due to power flow equations as well as bilinear terms introduced due to variable line impedance of VIDs. Furthermore, the topology optimization scheme makes it a mixed integer nonlinear problem. To tackle this, we introduce using McCormick relaxation scheme, which converts the bilinear constraints into a linear set of constraints along with the DC power flow equations. We propose an iterative correction of the McCormick relaxation to enhance its accuracy. The proposed framework is validated on standard IEEE benchmark test systems, and we present a performance comparison of the iterative McCormick method against the non-linear, SOS2 piecewise linear approximation, and original McCormick relaxation.", "AI": {"tldr": "本文提出了一种改进的 McCormick 松弛方法，用于优化和协调可变阻抗设备（VIDs）与网络拓扑优化（NTO）的联合运行，以解决电网中的拥塞和电压问题。", "motivation": "电力系统运营商越来越多地使用可变阻抗设备（VIDs）和网络拓扑优化（NTO）方案来解决线路和变压器拥塞以及电压违规等操作挑战。", "method": "使用 McCormick 松弛方法将包含 VIDs 可变阻抗引起的双线性项的非线性约束线性化，并结合直流潮流方程。提出了一种迭代校正 McCormick 松弛的方法以提高精度。该框架在 IEEE 基准测试系统上进行了验证。", "result": "所提出的迭代 McCormick 方法在准确性和性能上优于原始 McCormick 松弛、SOS2 分段线性逼近和非线性方法。", "conclusion": "提出的迭代 McCormick 松弛框架能够有效地处理 VIDs 和 NTO 联合运行中的混合整数非线性问题，并提供了更精确的优化解决方案。"}}
{"id": "2602.17881", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17881", "abs": "https://arxiv.org/abs/2602.17881", "authors": ["Joschka Braun"], "title": "Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations", "comment": "Master's Thesis, University of Tübingen. 89 pages, 34 figures. Portions of this work were published at the ICLR 2025 Workshop on Foundation Models in the Wild (see arXiv:2505.22637)", "summary": "Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.", "AI": {"tldr": "本研究探讨了语言模型中的“引导向量”控制方法，分析了其效果不稳定的原因，并提出了改进方向。研究发现，训练数据中激活差异的余弦相似度、正负激活在引导方向上的分离度以及训练提示词的多样性都会影响引导向量的可靠性。当潜在的目标行为表示不能被线性引导方向有效近似时，引导向量会变得不可靠。", "motivation": "引导向量在控制语言模型行为方面非常有效，但其效果在不同样本和目标行为之间存在差异且不可靠。作者希望探究造成这种不稳定性差异的原因，以及训练数据如何影响其可靠性。", "method": "研究者通过分析引导向量的训练数据，考察了以下几个方面：1. 训练激活差异之间的余弦相似度；2. 正负激活在引导方向上的分离度；3. 使用不同提示词变体训练引导向量。", "result": "1. 训练激活差异的余弦相似度越高，引导效果越可靠。2. 正负激活在引导方向上分离度越好的数据集，引导效果越可靠。3. 使用不同提示词变体训练的引导向量方向不同，但效果相似且跨数据集的功效相关。", "conclusion": "引导向量在目标行为的潜在表示不能被线性引导方向有效近似时，会变得不可靠。这些发现提供了一个诊断引导向量不稳定性问题的实用方法，并为开发更鲁棒的引导方法提供了思路，这些方法应明确考虑非线性的潜在行为表示。"}}
{"id": "2602.17793", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17793", "abs": "https://arxiv.org/abs/2602.17793", "authors": ["Peide Zhu", "Linbin Lu", "Zhiqin Chen", "Xiong Chen"], "title": "LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge", "comment": null, "summary": "It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.", "AI": {"tldr": "提出了一种名为LGD-Net的新型网络，通过从H&E图像预测HER2表达水平，避免了耗时且昂贵的IHC染色，并在BDI数据集上取得了最先进的性能。", "motivation": "标准的IHC染色评估HER2表达水平既昂贵又耗时，并且在许多地区都不可用。因此，从H&E图像直接预测HER2水平是一种有吸引力的替代方案。", "method": "提出了一种名为Latent-Guided Dual-Stream Network (LGD-Net)的框架，该框架使用跨模态特征幻觉代替像素级图像生成，将H&E形态学特征直接映射到分子潜在空间。在训练过程中，通过教师IHC编码器进行指导，并通过核分布和膜染色强度等辅助任务进行正则化。", "result": "LGD-Net在BCI数据集上取得了最先进的性能，显著优于基线方法，并且能够仅使用H&E图像进行高效推理。", "conclusion": "LGD-Net通过跨模态特征幻觉和领域知识正则化，能够从H&E图像有效地预测HER2表达水平，为乳腺癌的评估和靶向治疗选择提供了一种高效且可行的替代方案。"}}
{"id": "2602.17676", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17676", "abs": "https://arxiv.org/abs/2602.17676", "authors": ["Xingcheng Xu", "Jingjing Qu", "Qiaosheng Zhang", "Chaochao Lu", "Yanqing Yang", "Na Zou", "Xia Hu"], "title": "Epistemic Traps: Rational Misalignment Driven by Model Misspecification", "comment": null, "summary": "The rapid deployment of Large Language Models and AI agents across critical societal and technical domains is hindered by persistent behavioral pathologies including sycophancy, hallucination, and strategic deception that resist mitigation via reinforcement learning. Current safety paradigms treat these failures as transient training artifacts, lacking a unified theoretical framework to explain their emergence and stability. Here we show that these misalignments are not errors, but mathematically rationalizable behaviors arising from model misspecification. By adapting Berk-Nash Rationalizability from theoretical economics to artificial intelligence, we derive a rigorous framework that models the agent as optimizing against a flawed subjective world model. We demonstrate that widely observed failures are structural necessities: unsafe behaviors emerge as either a stable misaligned equilibrium or oscillatory cycles depending on reward scheme, while strategic deception persists as a \"locked-in\" equilibrium or through epistemic indeterminacy robust to objective risks. We validate these theoretical predictions through behavioral experiments on six state-of-the-art model families, generating phase diagrams that precisely map the topological boundaries of safe behavior. Our findings reveal that safety is a discrete phase determined by the agent's epistemic priors rather than a continuous function of reward magnitude. This establishes Subjective Model Engineering, defined as the design of an agent's internal belief structure, as a necessary condition for robust alignment, marking a paradigm shift from manipulating environmental rewards to shaping the agent's interpretation of reality.", "AI": {"tldr": "该研究提出了一种新的理论框架，将大型语言模型中的行为病态（如谄媚、幻觉和欺骗）视为由模型自身错误规范引起的数学上可解释的行为，而非训练中的瞬时错误。通过借鉴经济学中的 Berk-Nash Rationalizability，研究表明这些不安全行为是模型优化其有缺陷的主观世界模型所导致的结构性必然结果。", "motivation": "当前用于缓解大型语言模型（LLMs）和AI代理中出现的行为病态（如谄媚、幻觉和战略欺骗）的强化学习方法效果不佳。现有的安全范式将这些故障视为暂时的训练伪影，缺乏统一的理论框架来解释它们的出现和稳定性，这阻碍了LLMs在关键领域的安全部署。", "method": "研究通过将经济学中的Berk-Nash Rationalizability方法改编应用于人工智能领域，构建了一个理论框架。该框架将AI代理建模为在一个有缺陷的主观世界模型中进行优化。理论预测随后通过在六种最先进的模型家族上进行行为实验来验证，并生成相图来精确映射安全行为的拓扑边界。", "result": "研究结果表明，不安全行为是结构性必然的。根据奖励方案的不同，不安全行为会表现为稳定的失对齐均衡或振荡循环。战略欺骗则以“锁定”均衡或通过对客观风险具有鲁棒性的认知不确定性而持续存在。安全行为的范围由代理的认知先验决定，而不是奖励大小的连续函数。", "conclusion": "不安全行为并非模型错误，而是模型自身错误规范导致的数学上可解释的行为。研究提出了一种名为“主观模型工程”的新范式，强调通过设计代理的内部信念结构来塑造其对现实的解释，这是实现鲁棒性对齐的必要条件，标志着从操纵环境奖励转向塑造代理认知的新方向。"}}
{"id": "2602.17929", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17929", "abs": "https://arxiv.org/abs/2602.17929", "authors": ["Athanasios Angelakis"], "title": "ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging", "comment": "15 pages, 12 figures, 7 tables. Code and models available at https://github.com/Bluesman79/ZACH-ViT", "summary": "Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term \"Zero-token\" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.\n  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.", "AI": {"tldr": "ZACH-ViT 是一种紧凑型 Vision Transformer，通过移除位置嵌入和类别 token，并使用全局平均池化来实现置换不变性，适用于医学影像等空间信息不固定的场景。", "motivation": "现有的 Vision Transformer 依赖固定的空间先验（位置嵌入和类别 token），这在医学影像等空间布局信息弱或不一致的场景下会阻碍泛化能力。", "method": "ZACH-ViT 移除了位置嵌入和 [CLS] token，通过对 patch representation 进行全局平均池化来实现置换不变性。使用自适应残差投影来稳定紧凑模型的训练，并控制参数量。在 MedMNIST 数据集上进行了严格的少样本（每类 50 个样本）评估。", "result": "ZACH-ViT 在 BloodMNIST 上表现出最强优势，在 PathMNIST 上与 TransMIL 相当。在具有强解剖学先验的数据集（OCTMNIST, OrganAMNIST）上，其相对优势减弱。ZACH-ViT 在参数量少（0.25M）且从头训练的情况下，实现了具有竞争力的性能，并且推理时间小于一秒。", "conclusion": "ZACH-ViT 的设计表明，将模型的归纳偏置与数据的结构相匹配比追求普适性的基准性能更重要。其紧凑的结构和优异的性能使其适用于资源受限的临床环境。"}}
{"id": "2602.17990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17990", "abs": "https://arxiv.org/abs/2602.17990", "authors": ["Madhav Kanda", "Pedro Las-Casas", "Alok Gautam Kumbhare", "Rodrigo Fonseca", "Sharad Agarwal"], "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics", "comment": null, "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.", "AI": {"tldr": "该研究提出了WorkflowPerturb基准测试，用于评估基于LLM生成的结构化工作流的评估指标。该基准测试通过对真实工作流进行可控的扰动来模拟不同程度的降级，以分析评估指标的敏感性和校准性。", "motivation": "当前LLM生成的结构化工作流的自动评估存在困难，因为评估指标得分通常校准不佳，且得分变化不能直接反映工作流降级的严重程度。", "method": "研究人员创建了一个名为WorkflowPerturb的基准测试，通过对“黄金”工作流应用真实的、可控的扰动（包括缺失步骤、压缩步骤和描述更改），并设置10%、30%和50%的扰动严重程度，生成了4,973个黄金工作流和44,757个扰动变体。然后，他们对多种评估指标家族进行基准测试，并分析其敏感性和校准性。", "result": "研究结果表征了不同评估指标家族之间的系统性差异，并支持对工作流评估分数进行与严重程度相关的解释。", "conclusion": "WorkflowPerturb基准测试提供了一种有效的方法来研究和改进工作流评估指标，使其能够更准确地反映LLM生成工作流的降级程度，并支持更具信息量的分数解释。"}}
{"id": "2602.17908", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17908", "abs": "https://arxiv.org/abs/2602.17908", "authors": ["Mingzhang Zhu", "Alvin Zhu", "Jose Victor S. H. Ramos", "Beom Jun Kim", "Yike Shi", "Yufeng Wu", "Ruochen Hou", "Quanyou Wang", "Eric Song", "Tony Fan", "Yuchen Cui", "Dennis W. Hong"], "title": "WHED: A Wearable Hand Exoskeleton for Natural, High-Quality Demonstration Collection", "comment": "7 pages, 9 figures, submitted to IEEE UR", "summary": "Scalable learning of dexterous manipulation remains bottlenecked by the difficulty of collecting natural, high-fidelity human demonstrations of multi-finger hands due to occlusion, complex hand kinematics, and contact-rich interactions. We present WHED, a wearable hand-exoskeleton system designed for in-the-wild demonstration capture, guided by two principles: wearability-first operation for extended use and a pose-tolerant, free-to-move thumb coupling that preserves natural thumb behaviors while maintaining a consistent mapping to the target robot thumb degrees of freedom. WHED integrates a linkage-driven finger interface with passive fit accommodation, a modified passive hand with robust proprioceptive sensing, and an onboard sensing/power module. We also provide an end-to-end data pipeline that synchronizes joint encoders, AR-based end-effector pose, and wrist-mounted visual observations, and supports post-processing for time alignment and replay. We demonstrate feasibility on representative grasping and manipulation sequences spanning precision pinch and full-hand enclosure grasps, and show qualitative consistency between collected demonstrations and replayed executions.", "AI": {"tldr": "提出了一种名为 WHED 的可穿戴手部外骨骼系统，用于捕捉高质量、自然的人类多指手部操作演示，克服了现有技术在数据收集方面的挑战。", "motivation": "在野外收集多指手部操作的高保真人类演示存在困难，例如遮挡、复杂手部运动学和富含接触的交互，这阻碍了灵巧操作的可扩展学习。", "method": "WHED 系统采用“可穿戴优先”的设计原则，支持长时间使用，并采用姿态容忍、自由移动的拇指耦合机制，以保持自然的拇指行为并与目标机器人拇指自由度保持一致的映射。系统集成了连杆驱动的手指接口、被动贴合适应、改进的被动手和本体感觉传感，以及板载传感/供电模块。此外，还提供了一个端到端的 P数据管线，用于同步关节编码器、基于 AR 的末端执行器姿态和腕部视觉观察，并支持时间对齐和回放的后处理。", "result": "在代表性的抓取和操作序列（包括精细捏合和全手包裹式抓取）上验证了 WHED 系统的可行性，并定性证明了收集的演示与回放执行之间的一致性。", "conclusion": "WHED 系统能够有效地在野外捕捉高质量、自然的人类手部操作演示，为灵巧操作的机器人学习提供了新的可能性。"}}
{"id": "2602.18247", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18247", "abs": "https://arxiv.org/abs/2602.18247", "authors": ["Fen Wu", "Chengzhi Yuan"], "title": "Hybrid Control of ADT Switched Linear Systems subject to Actuator Saturation", "comment": null, "summary": "This paper develops a hybrid output-feedback control framework for average dwell-time (ADT) switched linear systems subject to actuator saturation. The considered subsystems may be exponentially unstable, and the saturation nonlinearity is explicitly handled through a deadzone-based representation. The proposed hybrid controller combines mode-dependent full-order dynamic output-feedback controllers with a supervisory reset mechanism that updates controller states at switching instants. By incorporating the reset rule directly into the synthesis conditions, switching boundary constraints and performance requirements are addressed in a unified convex formulation. Sufficient conditions are derived in terms of linear matrix inequalities (LMIs) to guarantee exponential stability under ADT switching and a prescribed weighted ${\\cal L}_2$-gain disturbance attenuation level for energy-bounded disturbances. An explicit controller construction algorithm is provided based on feasible LMI solutions. Simulation results demonstrate the effectiveness and computational tractability of the proposed approach and highlight its advantages over existing output-feedback saturation control methods.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2602.17907", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17907", "abs": "https://arxiv.org/abs/2602.17907", "authors": ["Raymond Li", "Amirhossein Abaskohi", "Chuyuan Li", "Gabriel Murray", "Giuseppe Carenini"], "title": "Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions", "comment": "20 pages, 5 figures", "summary": "Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.", "AI": {"tldr": "提出一种基于语言模型（LM）生成语义丰富的软标签，用于训练神经主题模型，以克服传统方法忽略上下文信息和数据稀疏性的问题，并在主题连贯性、纯度和文档检索任务上取得显著提升。", "motivation": "传统神经主题模型依赖词袋模型（BoW）进行优化，忽略了上下文信息，并且在数据稀疏时表现不佳。", "method": "利用语言模型（LM）生成预测下一个词的概率，并将其投影到预定义词汇表上，从而创建语义接地（semantically-grounded）的软标签。然后，训练主题模型以LM的隐藏状态重构这些软标签，以获得更丰富的监督信号。", "result": "所提方法在主题连贯性（topic coherence）和纯度（purity）方面取得了显著优于现有基线方法的改进。此外，在基于检索的评估指标上，该方法在识别语义相似文档方面也表现更优。", "conclusion": "通过使用语言模型生成的上下文感知的软标签，可以有效提升神经主题模型的质量，使其在理解文档主题结构和支持检索应用方面更加出色。"}}
{"id": "2602.18261", "categories": ["eess.SY", "nlin.CD"], "pdf": "https://arxiv.org/pdf/2602.18261", "abs": "https://arxiv.org/abs/2602.18261", "authors": ["Philippe Jacquod", "Laurent Pagnier", "Daniel J. Gauthier"], "title": "Accurate Data-Based State Estimation from Power Loads Inference in Electric Power Grids", "comment": "10 pages, 10 figures", "summary": "Accurate state estimation is a crucial requirement for the reliable operation and control of electric power systems. Here, we construct a data-driven, numerical method to infer missing power load values in large-scale power grids. Given partial observations of power demands, the method estimates the operational state using a linear regression algorithm, exploiting statistical correlations within synthetic training datasets. We evaluate the performance of the method on three synthetic transmission grid test systems. Numerical experiments demonstrate the high accuracy achieved by the method in reconstructing missing demand values under various operating conditions. We further apply the method to real data for the transmission power grid of Switzerland. Despite the restricted number of observations in this dataset, the method infers missing power loads rather accurately. Furthermore, Newton-Raphson power flow solutions show that deviations between true and inferred values for power loads result in smaller deviations between true and inferred values for flows on power lines. This ensures that the estimated operational state correctly captures potential line contingencies. Overall, our results indicate that simple data-based regression techniques can provide an efficient and reliable alternative for state estimation in modern power grids.", "AI": {"tldr": "本文提出了一种基于线性回归的数据驱动方法，用于从部分观测值中推断大型电力系统中缺失的电力负荷值，并在合成和真实数据集上进行了验证，结果表明该方法能够准确地估计电力负荷，并对电力线路的流值影响较小，为状态估计提供了一种高效可靠的替代方案。", "motivation": "确保电力系统可靠运行和控制需要准确的状态估计，而实际系统中电力负荷的观测往往是不完整的。", "method": "构建了一个数据驱动的数值方法，利用线性回归算法，通过在合成训练数据集上学习统计相关性来推断缺失的电力负荷值。该方法基于部分观测到的电力需求。", "result": "在三个合成输电网测试系统和瑞士输电网的真实数据上均取得了高精度的缺失负荷值重建。研究发现，即使在真实数据集观测值有限的情况下，该方法仍能相当准确地推断缺失值。此外，牛顿-拉夫逊潮流计算结果表明，电力负荷的估计值与真实值之间的偏差，导致线路潮流的估计值与真实值之间的偏差更小，能够准确捕捉线路故障。", "conclusion": "简单的数据驱动回归技术可以为现代电网的状态估计提供一种高效可靠的替代方案，能够准确地推断缺失的电力负荷，并有效地捕捉系统的运行状态以应对潜在的线路故障。"}}
{"id": "2602.18025", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18025", "abs": "https://arxiv.org/abs/2602.18025", "authors": ["Haruki Abe", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets", "comment": "ICLR 2026", "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "AI": {"tldr": "该研究提出了一种结合离线强化学习和跨具身学习的方法，以降低机器人策略预训练的数据收集成本，并提出了一种基于具身分组的策略来解决多机器人学习中的冲突问题。", "motivation": "高昂的优质演示数据收集成本阻碍了机器人策略的可扩展预训练。本研究旨在解决此问题。", "method": "结合离线强化学习（利用专家和次优数据）与跨具身学习（聚合异构机器人轨迹以获得通用控制先验）。引入基于具身的分组策略，按形态相似性聚类机器人，并使用组梯度进行模型更新，以解决跨形态学习中的梯度冲突。", "result": "该离线RL和跨具身学习范式在包含大量次优轨迹的数据集上表现优于纯行为克隆。随着次优数据比例和机器人类型数量的增加，观察到跨形态的梯度冲突阻碍学习。提出的具身分组策略显著减少了机器人间的冲突，并优于现有的冲突解决方法。", "conclusion": "结合离线强化学习和跨具身学习是一种有效的机器人策略预训练方法，尤其适用于包含次优数据的场景。通过具身分组策略可以有效解决多机器人学习中的梯度冲突问题，提升学习效果。"}}
{"id": "2602.17799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17799", "abs": "https://arxiv.org/abs/2602.17799", "authors": ["Jose Sosa", "Danila Rukhovich", "Anis Kacem", "Djamila Aouada"], "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation", "comment": null, "summary": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.", "AI": {"tldr": "该研究提出了一种无需额外训练即可实现遥感图像文本引导分割的方法，通过结合对比式和生成式视觉语言模型（VLMs）与Segment Anything Model（SAM），并进行了广泛的实验验证。", "motivation": "现有基于VLMs和VFMs的遥感图像分割方法通常需要额外的可训练组件，这限制了它们的泛化能力和实际应用。因此，研究者希望探索完全不依赖额外训练或仅需轻量级微调就能实现文本引导遥感分割的方法。", "method": "研究者提出了一种结合对比式和生成式VLMs与SAM的无训练（或轻量级LoRA微调）方法。对比式方法使用CLIP作为SAM的掩码选择器；生成式方法利用GPT-5（零样本）或LoRA微调的Qwen-VL模型生成点击提示，以实现SAM的推理和指代分割。", "result": "在19个遥感基准测试中，包括开放词汇、指代和推理分割任务，该方法在完全零样本设置下实现了最先进的开放词汇语义分割（OVSS），并且生成式方法（尤其是LoRA微调的Qwen-VL）在指代和推理分割任务上表现最佳。", "conclusion": "该研究证明了仅依靠现有基础模型（VLMs和SAM）并结合对比式和生成式策略，可以在遥感图像分割任务中实现强大的零样本或轻量级微调性能，克服了现有方法对额外可训练组件的依赖。"}}
{"id": "2602.17921", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17921", "abs": "https://arxiv.org/abs/2602.17921", "authors": ["Kei Ikemura", "Yifei Dong", "Florian T. Pokorny"], "title": "Latent Diffeomorphic Co-Design of End-Effectors for Deformable and Fragile Object Manipulation", "comment": null, "summary": "Manipulating deformable and fragile objects remains a fundamental challenge in robotics due to complex contact dynamics and strict requirements on object integrity. Existing approaches typically optimize either end-effector design or control strategies in isolation, limiting achievable performance. In this work, we present the first co-design framework that jointly optimizes end-effector morphology and manipulation control for deformable and fragile object manipulation. We introduce (1) a latent diffeomorphic shape parameterization enabling expressive yet tractable end-effector geometry optimization, (2) a stress-aware bi-level co-design pipeline coupling morphology and control optimization, and (3) a privileged-to-pointcloud policy distillation scheme for zero-shot real-world deployment. We evaluate our approach on challenging food manipulation tasks, including grasping and pushing jelly and scooping fillets. Simulation and real-world experiments demonstrate the effectiveness of the proposed method.", "AI": {"tldr": "提出了一种联合优化末端执行器形状和控制策略的共同设计框架，用于处理易变形和易碎物体，并在食物操纵任务中证明了其有效性。", "motivation": "现有方法孤立地优化末端执行器设计或控制策略，限制了处理易变形和易碎物体的性能。需要一种能够联合优化的方法。", "method": "提出了一种包含三个部分的共同设计框架：(1) 使用潜在的微分同胚形状参数化进行末端执行器几何优化；(2) 采用应力感知的双层共同设计流程耦合形态和控制优化；(3) 使用特权到点云的策略蒸馏方案实现零样本部署。", "result": "在果冻的抓取和推动以及鱼片的舀取等具有挑战性的食物操纵任务中，仿真和真实世界实验都证明了该方法的有效性。", "conclusion": "该共同设计框架能够有效地联合优化末端执行器形态和操纵控制，以应对易变形和易碎物体的操作挑战，并在实际应用中取得了良好效果。"}}
{"id": "2602.17911", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17911", "abs": "https://arxiv.org/abs/2602.17911", "authors": ["Jash Rajesh Parekh", "Wonbin Kweon", "Joey Chan", "Rezarta Islamaj", "Robert Leaman", "Pengcheng Jiang", "Chih-Hsuan Wei", "Zhizheng Wang", "Zhiyong Lu", "Jiawei Han"], "title": "Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering", "comment": null, "summary": "Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.", "AI": {"tldr": "本文提出了CondMedQA，一个评估条件化生物医学问答能力的新基准，并引入了Condition-Gated Reasoning (CGR)框架，该框架能够根据查询条件动态调整知识图谱的推理路径，以生成更符合患者特定情况的答案。", "motivation": "现有的生物医学问答系统假设医学知识普遍适用，但忽略了现实临床推理的条件性（例如，患者的合并症、禁忌症等）。现有的基准测试未能评估这种条件化推理能力，而现有方法（如检索增强或图谱方法）缺乏确保检索知识适用于给定上下文的机制。", "method": "提出了CondMedQA基准，包含需要多步推理且答案随患者条件变化的复杂问题。提出了Condition-Gated Reasoning (CGR)框架，该框架能够构建与条件相关的知识图谱，并根据查询条件选择性地激活或剪枝推理路径。", "result": "CGR框架能更可靠地选择与条件相符的答案，并且在生物医学问答基准测试上能达到或超过现有最先进的性能。这表明显式地建模条件性对于鲁棒的医学推理至关重要。", "conclusion": "显式地建模临床推理的条件性对于开发更准确、更可靠的生物医学问答系统至关重要。CGR框架提供了一种有效的方式来解决这一挑战，并在现有任务上取得了优异的性能。"}}
{"id": "2602.18095", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18095", "abs": "https://arxiv.org/abs/2602.18095", "authors": ["Hyunseok Oh", "Sam Stern", "Youngki Lee", "Matthai Philipose"], "title": "Neurosymbolic Language Reasoning as Satisfiability Modulo Theory", "comment": null, "summary": "Natural language understanding requires interleaving textual and logical reasoning, yet large language models often fail to perform such reasoning reliably. Existing neurosymbolic systems combine LLMs with solvers but remain limited to fully formalizable tasks such as math or program synthesis, leaving natural documents with only partial logical structure unaddressed. We introduce Logitext, a neurosymbolic language that represents documents as natural language text constraints (NLTCs), making partial logical structure explicit. We develop an algorithm that integrates LLM-based constraint evaluation with satisfiability modulo theory (SMT) solving, enabling joint textual-logical reasoning. Experiments on a new content moderation benchmark, together with LegalBench and Super-Natural Instructions, show that Logitext improves both accuracy and coverage. This work is the first that treats LLM-based reasoning as an SMT theory, extending neurosymbolic methods beyond fully formalizable domains.", "AI": {"tldr": "Logitext 是一种新的神经符号语言，它将文档表示为自然语言文本约束 (NLTC)，并将 LLM 推理整合到 SMT 求解器中，从而实现了文本和逻辑的联合推理，并在内容审核、LegalBench 和 Super-Natural Instructions 等基准测试中取得了更好的准确性和覆盖率。", "motivation": "现有的神经符号系统在处理逻辑结构不完整的自然文档方面存在局限性，而大型语言模型在进行文本和逻辑交错推理时并不可靠。", "method": "引入 Logitext，一种将文档表示为自然语言文本约束 (NLTC) 的神经符号语言，并开发了一种将基于 LLM 的约束评估与可满足性模理论 (SMT) 求解相结合的算法，以实现联合文本-逻辑推理。", "result": "在新的内容审核基准测试、LegalBench 和 Super-Natural Instructions 上进行实验，Logitext 提高了准确性和覆盖率。", "conclusion": "Logitext 是首个将基于 LLM 的推理视为 SMT 理论的工作，将神经符号方法扩展到了完全可形式化的域之外，并实现了在自然语言文本约束上的联合文本-逻辑推理。"}}
{"id": "2602.18331", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18331", "abs": "https://arxiv.org/abs/2602.18331", "authors": ["Liang Wu", "Wallace Gian Yion Tan", "Richard D. Braatz", "Ján Drgoňa"], "title": "Koopman-BoxQP: Solving Large-Scale NMPC at kHz Rates", "comment": "Accepted by the 8th Annual Learning for Dynamics and Control Conference (L4DC 2026). arXiv admin note: text overlap with arXiv:2602.15596", "summary": "Solving large-scale nonlinear model predictive control (NMPC) problems at kilohertz (kHz) rates on standard processors remains a formidable challenge. This paper proposes a Koopman-BoxQP framework that i) learns a linear Koopman high-dimensional model, ii) eliminates the high-dimensional observables to construct a multi-step prediction model of the states and control inputs, iii) penalizes the multi-step prediction model into the objective, which results in a structured box-constrained quadratic program (BoxQP) whose decision variables include both the system states and control inputs, iv) develops a structure-exploited and warm-starting-supported variant of the feasible Mehrotra's interior-point algorithm for BoxQP. Numerical results demonstrate that Koopman-BoxQP can solve a large-scale NMPC problem with $1040$ variables and $2080$ inequalities at a kHz rate.", "AI": {"tldr": "本文提出了一种Koopman-BoxQP框架，通过学习高维线性Koopman模型并将其纳入目标函数，将大规模非线性模型预测控制（NMPC）问题转化为结构化二次规划（BoxQP）问题，并采用优化的内点算法求解，从而实现了kHz速率下的NMPC求解。", "motivation": "在标准处理器上以kHz速率解决大规模非线性模型预测控制（NMPC）问题仍然是一个巨大的挑战。", "method": "1. 学习一个线性的Koopman高维模型。 2. 消除高维可观测量，构建状态和控制输入的预测模型。 3. 将多步预测模型纳入目标函数，形成一个结构化的BoxQP问题。 4. 开发一个利用结构并支持热启动的、可行的Mehrotra内点算法来求解BoxQP。", "result": "Koopman-BoxQP框架能够以kHz速率解决一个具有1040个变量和2080个不等式的大规模NMPC问题。", "conclusion": "Koopman-BoxQP框架为实现高速率下的NMPC问题求解提供了一种有效的方法，尤其适用于大规模系统。"}}
{"id": "2602.17926", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17926", "abs": "https://arxiv.org/abs/2602.17926", "authors": ["Jennifer Wakulicz", "Ki Myung Brian Lee", "Teresa Vidal-Calleja", "Robert Fitch"], "title": "Homotopic information gain for sparse active target tracking", "comment": "12 pages, 12 figures, accepted to Transactions on Robotics", "summary": "The problem of planning sensing trajectories for a mobile robot to collect observations of a target and predict its future trajectory is known as active target tracking. Enabled by probabilistic motion models, one may solve this problem by exploring the belief space of all trajectory predictions given future sensing actions to maximise information gain. However, for multi-modal motion models the notion of information gain is often ill-defined. This paper proposes a planning approach designed around maximising information regarding the target's homotopy class, or high-level motion. We introduce homotopic information gain, a measure of the expected high-level trajectory information given by a measurement. We show that homotopic information gain is a lower bound for metric or low-level information gain, and is as sparsely distributed in the environment as obstacles are. Planning sensing trajectories to maximise homotopic information results in highly accurate trajectory estimates with fewer measurements than a metric information approach, as supported by our empirical evaluation on real and simulated pedestrian data.", "AI": {"tldr": "本文提出了一种基于同伦信息增益的主动目标跟踪方法，用于解决多模态运动模型下信息增益定义不清的问题。该方法能更有效地规划传感轨迹，从而提高目标轨迹估计的准确性。", "motivation": "现有的主动目标跟踪方法在处理多模态运动模型时，信息增益的定义常不明确。因此，需要一种新的方法来处理这个问题。", "method": "提出了一种名为“同伦信息增益”的新度量，用于衡量测量值关于目标同伦类（高层运动）的信息量。该方法通过最大化同伦信息增益来规划传感轨迹。", "result": "同伦信息增益是度量信息增益的下界，并且与环境中的障碍物一样稀疏分布。最大化同伦信息增益的传感轨迹规划方法，在真实和模拟的行人数据实验中，能够以更少的测量次数获得高精度的轨迹估计。", "conclusion": "基于同伦信息增益的规划方法是一种有效的主动目标跟踪策略，尤其适用于多模态运动模型，能够提高信息利用效率和目标轨迹估计的准确性。"}}
{"id": "2602.17807", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17807", "abs": "https://arxiv.org/abs/2602.17807", "authors": ["Narges Norouzi", "Idil Esen Zulfikar", "Niccol`o Cavagnero", "Tommie Kerssies", "Bastian Leibe", "Gijs Dubbelman", "Daan de Geus"], "title": "VidEoMT: Your ViT is Secretly Also a Video Segmentation Model", "comment": null, "summary": "Existing online video segmentation models typically combine a per-frame segmenter with complex specialized tracking modules. While effective, these modules introduce significant architectural complexity and computational overhead. Recent studies suggest that plain Vision Transformer (ViT) encoders, when scaled with sufficient capacity and large-scale pre-training, can conduct accurate image segmentation without requiring specialized modules. Motivated by this observation, we propose the Video Encoder-only Mask Transformer (VidEoMT), a simple encoder-only video segmentation model that eliminates the need for dedicated tracking modules. To enable temporal modeling in an encoder-only ViT, VidEoMT introduces a lightweight query propagation mechanism that carries information across frames by reusing queries from the previous frame. To balance this with adaptability to new content, it employs a query fusion strategy that combines the propagated queries with a set of temporally-agnostic learned queries. As a result, VidEoMT attains the benefits of a tracker without added complexity, achieving competitive accuracy while being 5x--10x faster, running at up to 160 FPS with a ViT-L backbone. Code: https://www.tue-mps.org/videomt/", "AI": {"tldr": "提出了一种名为VidEoMT的简单视频分割模型，它仅使用Transformer编码器，无需复杂的跟踪模块，通过查询传播和融合机制实现跨帧信息传递，在保持竞争力的准确率的同时，速度提升了5-10倍。", "motivation": "现有视频分割模型通常需要结合复杂的专业跟踪模块，这增加了模型的复杂性和计算开销。受Vision Transformer (ViT)在图像分割中表现的启发，研究者希望设计一种更简洁、高效的视频分割模型。", "method": "VidEoMT是一个仅编码器的视频分割模型。它通过一个轻量级的查询传播机制将前一帧的查询信息传递到当前帧，并结合一组与时间无关的学习查询，以适应新内容，从而实现跨帧建模和内容适应。", "result": "VidEoMT在不引入额外跟踪模块复杂性的情况下，实现了具有竞争力的分割精度，并且速度显著提升，采用ViT-L主干时，帧率可达160 FPS，比现有模型快5-10倍。", "conclusion": "VidEoMT是一种简洁高效的视频分割模型，通过巧妙的查询传播和融合机制，成功地在无需复杂跟踪模块的情况下实现了优异的性能，证明了仅编码器Transformer在视频分割任务中的潜力。"}}
{"id": "2602.18365", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18365", "abs": "https://arxiv.org/abs/2602.18365", "authors": ["Feng Zhao", "Tongxin Zheng", "Dane Schiro", "Xiaochu Wang"], "title": "A Marginal Reliability Impact Based Accreditation Framework for Capacity Markets", "comment": null, "summary": "This paper presents a Marginal Reliability Impact (MRI) based resource accreditation framework for capacity market design. Under this framework, a resource is accredited based on its marginal impact on system reliability, thus aligning the resource accreditation value with its reliability contribution. A key feature of the MRI based accreditation is that the accredited capacities supplied by different resources to the capacity market are substitutable in reliability contribution, a desired feature of homogeneous products. Moreover, with MRI based capacity demand, substitutability between supply and demand for capacity is also achieved. As a result, a capacity market with the MRI based capacity product can better characterize the underlying resource adequacy problem and lead to more efficient market outcomes.", "AI": {"tldr": "本文提出了一种基于边际可靠性影响（MRI）的资源认证框架，用于容量市场设计，旨在使资源的认证价值与其可靠性贡献相匹配。", "motivation": "现有的容量市场设计在认证资源时，可能未能充分体现其对系统可靠性的实际贡献，导致市场效率低下。研究动机是设计一个更能准确反映资源可靠性贡献并促进市场效率的市场机制。", "method": "提出了一种基于边际可靠性影响（MRI）的资源认证框架。在该框架下，资源的认证价值基于其对系统可靠性的边际影响。该框架实现了供应端不同资源之间的可靠性贡献可替代性，以及供应和需求端之间的容量可替代性。", "result": "基于MRI的认证方法使得不同资源的认证容量在可靠性贡献上是可替代的，这类似于同质产品的特性。此外，基于MRI的容量需求也实现了供应和需求之间的可替代性。", "conclusion": "基于MRI的容量产品能够更准确地刻画资源充足性问题，并带来更有效的市场结果，从而提高容量市场的效率。"}}
{"id": "2602.17814", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17814", "abs": "https://arxiv.org/abs/2602.17814", "authors": ["Adrian Catalin Lutu", "Eduard Poesina", "Radu Tudor Ionescu"], "title": "VQPP: Video Query Performance Prediction Benchmark", "comment": null, "summary": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.", "AI": {"tldr": "本文提出了视频查询性能预测（VQPP）的第一个基准，包含两个文本到视频检索数据集和两个CBVR系统，并探索了检索前和检索后性能预测器，结果表明检索前预测器表现具有竞争力，并将其应用于LLM的查询改写任务。", "motivation": "现有的查询性能预测（QPP）研究主要集中在文本和图像检索，而内容为基础的视频检索（CBVR）领域的QPP研究不足。", "method": "构建了一个包含56K文本查询和51K视频的VQPP基准，并提供了官方的训练、验证和测试集划分。探索了多种检索前和检索后性能预测器。将表现最佳的检索前预测器用作奖励模型，通过直接偏好优化（DPO）训练大型语言模型（LLM）进行查询改写。", "result": "检索前预测器获得了具有竞争力的性能，可以在检索步骤执行之前实现QPP应用。成功将VQPP应用于LLM的查询改写任务。", "conclusion": "本文提出了首个视频查询性能预测基准（VQPP），填补了该领域的空白。研究表明检索前预测器在视频检索中具有实用价值，并可用于提升LLM在查询改写任务上的表现。代码和基准已公开。"}}
{"id": "2602.18201", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18201", "abs": "https://arxiv.org/abs/2602.18201", "authors": ["Joseph Bingham", "Netanel Arussy", "Dvir Aran"], "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps", "comment": "10 pages, 2 figures, preprint", "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime", "AI": {"tldr": "本文研究表明，即使在训练过程中故意排除敏感属性（如年龄和收入），无监督学习到的表示也可能包含这些属性的显着信息。研究使用了 SOMtime 方法，并在真实世界数据集上验证了这一点，发现其无监督表示能与敏感属性高度相关，而 PCA、UMAP、t-SNE 和自编码器则表现较差。这说明“通过无意识实现公平”的策略在表示学习层面无效，公平性审计应扩展到无监督组件。", "motivation": "研究的动机在于挑战“无监督表示是中立的”这一普遍假设，尤其是在敏感属性被排除在训练之外时。研究者希望证明即使在没有明确训练的情况下，敏感属性也可能在无监督表示中显现，从而引发潜在的公平性问题。", "method": "研究使用了 SOMtime，一种基于高容量自组织映射（SOM）且能够保持拓扑结构的表示学习方法。在两个大型真实数据集（世界价值观调查和人口普查收入数据集）上，将 SOMtime 的无监督嵌入与 PCA、UMAP、t-SNE 和自编码器进行比较，评估它们与被隐藏的敏感属性（年龄、收入）的相关性，并通过无监督聚类分析来评估其公平性风险。", "result": "SOMtime 方法在包含年龄和收入等敏感属性的无监督嵌入中，能够发现与这些属性高度相关的潜在轴，Spearman 相关系数最高可达 0.85。相比之下，PCA 和 UMAP 的相关性通常低于 0.23（仅一例达到 0.31），t-SNE 和自编码器最高达到 0.34。此外，SOMtime 的无监督分割产生了人口统计学上存在偏差的聚类。", "conclusion": "研究得出结论，“通过无意识实现公平”的策略在表示学习层面对于有序敏感属性是无效的。无监督学习到的表示本身就可能泄露敏感信息，并导致下游公平性风险。因此，公平性审计不仅要关注监督式模型，还应扩展到机器学习管道中的无监督组件。"}}
{"id": "2602.17937", "categories": ["cs.CL", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17937", "abs": "https://arxiv.org/abs/2602.17937", "authors": ["Xiaotang Du", "Giwon Hong", "Wai-Chung Kwan", "Rohit Saxena", "Ivan Titov", "Pasquale Minervini", "Emily Allaway"], "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification", "comment": null, "summary": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.", "AI": {"tldr": "本研究首次系统性地比较了基于DSPy框架的指令优化技术在表格事实核查任务中的应用，评估了四种提示技术（直接预测、CoT、ReAct、CodeAct）和三种优化器（COPRO、MiPROv2、SIMBA）在不同模型上的表现。研究发现指令优化能持续提升准确率，MiPROv2在CoT方面效果稳定，SIMBA在ReAct方面表现尤为突出，尤其是在大型模型上。SIMBA通过启发式方法鼓励更直接的推理路径，提升了CoT的数值比较能力并减少了ReAct中不必要的工具调用。在不同提示技术中，CoT对小型模型而言仍然是有效的表格事实核查方法，而大型模型上的ReAct代理虽然性能具有竞争力，但需要精细的指令优化。", "motivation": "现有研究缺乏对指令优化技术在表格事实核查任务中的系统性比较，尤其是与不同提示技术（如文本和代码使用）和不同优化器（DSPy框架内的）的结合效果。", "method": "作者采用了DSPy优化框架，并评估了四种指令优化技术：直接预测、Chain-of-Thought (CoT)、ReAct（使用SQL工具）和CodeAct（使用Python执行）。同时，他们测试了DSPy框架中的三种优化器：COPRO、MiPROv2和SIMBA。研究在四个基准测试和三个模型系列上进行了实验。", "result": "指令优化一致地提高了事实核查的准确性。MiPROv2在CoT方法上带来了最稳定的性能提升。SIMBA在ReAct代理中提供了最大的性能增益，尤其是在大型模型上。SIMBA通过启发式方法促进了更直接的推理路径，提高了CoT中的数值比较能力，并减少了ReAct代理中不必要的工具调用。在不同提示技术中，CoT对小型模型仍然有效。使用大型模型的ReAct代理在经过仔细的指令优化后，可以达到有竞争力的性能。", "conclusion": "指令优化是提升大型语言模型在表格事实核查任务中推理能力的一种有效方法。MiPROv2和SIMBA是两种有前途的DSPy优化器，它们在不同的提示技术（CoT和ReAct）上表现出互补的优势，尤其是在大型模型上。CoT仍然是小型模型的实用选择，而ReAct代理的性能很大程度上依赖于细致的指令优化。"}}
{"id": "2602.18014", "categories": ["cs.RO", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.18014", "abs": "https://arxiv.org/abs/2602.18014", "authors": ["Unnati Nigam", "Radhendushka Srivastava", "Faezeh Marzbanrad", "Michael Burke"], "title": "Quasi-Periodic Gaussian Process Predictive Iterative Learning Control", "comment": null, "summary": "Repetitive motion tasks are common in robotics, but performance can degrade over time due to environmental changes and robot wear and tear. Iterative learning control (ILC) improves performance by using information from previous iterations to compensate for expected errors in future iterations. This work incorporates the use of Quasi-Periodic Gaussian Processes (QPGPs) into a predictive ILC framework to model and forecast disturbances and drift across iterations. Using a recent structural equation formulation of QPGPs, the proposed approach enables efficient inference with complexity $\\mathcal{O}(p^3)$ instead of $\\mathcal{O}(i^2p^3)$, where $p$ denotes the number of points within an iteration and $i$ represents the total number of iterations, specially for larger $i$. This formulation also enables parameter estimation without loss of information, making continual GP learning computationally feasible within the control loop. By predicting next-iteration error profiles rather than relying only on past errors, the controller achieves faster convergence and maintains this under time-varying disturbances. We benchmark the method against both standard ILC and conventional Gaussian Process (GP)-based predictive ILC on three tasks, autonomous vehicle trajectory tracking, a three-link robotic manipulator, and a real-world Stretch robot experiment. Across all cases, the proposed approach converges faster and remains robust under injected and natural disturbances while reducing computational cost. This highlights its practicality across a range of repetitive dynamical systems.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2602.17949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17949", "abs": "https://arxiv.org/abs/2602.17949", "authors": ["Victoria Blake", "Mathew Miller", "Jamie Novak", "Sze-yuan Ooi", "Blanca Gallego"], "title": "CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications", "comment": "30 pages, 6 figures, 4 tables", "summary": "Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.", "AI": {"tldr": "CUICurate是一个基于图谱检索增强生成（GraphRAG）的框架，利用大型语言模型（LLM）自动化UMLS概念集（CUIs）的构建，相比人工方法，能生成更大、更完整且精度相当的概念集，并降低了人工工作量。", "motivation": "现有的UMLS CUI工具在构建包含同义词、子类型和超类型的临床概念集方面存在效率低下、人工成本高以及支持不足的问题，尤其对于直接处理UMLS CUIs的NLP流程。", "method": "构建并嵌入UMLS知识图谱（KG）用于语义检索。对于每个目标概念，从KG中检索候选CUIs，然后使用GPT-5和GPT-5-mini两个LLM进行过滤和分类。在五个词汇异构的临床概念上，与手动创建的基准进行评估。", "result": "CUICurate生成了比手动基准更大、更完整但精度相当的概念集。GPT-5-mini在过滤阶段召回率更高，而GPT-5的分类结果更符合临床医生的判断。输出结果稳定且计算成本低。", "conclusion": "CUICurate提供了一种可扩展且可复用的UMLS概念集构建方法，显著减少了手动工作。通过结合图谱检索和LLM推理，该框架能够生成用于不同表型分析需求的、可定制的概念集。"}}
{"id": "2602.17854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17854", "abs": "https://arxiv.org/abs/2602.17854", "authors": ["Domonkos Varga"], "title": "On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective", "comment": null, "summary": "This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szirányi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.", "AI": {"tldr": "该研究分析了Liu和Szirányi提出的手势识别方法，发现其评估协议存在数据泄露问题，导致近乎完美的准确率指标可能无法反映对新个体的泛化能力。", "motivation": "为了验证Liu和Szirányi提出的手势识别方法的有效性，特别是其评估协议的准确性，并强调在需要识别未知个体手势的应用（如无人机-人类交互）中进行受试者独立数据划分的重要性。", "method": "通过检查所发表的混淆矩阵、学习曲线和数据集构建，分析了Liu和Szirányi提出的评估协议，特别关注了帧级随机训练/测试分割是否会引入数据泄露。", "result": "研究发现，Liu和Szirányi的评估方法存在数据泄露，因为同一受试者的样本在训练集和测试集中都有出现，这使得评估结果无法衡量对未知个体的泛化能力。", "conclusion": "该研究强调了在基于视觉的手势识别研究中，尤其是在需要可靠识别未见过的人执行的手势的应用中，进行受试者独立数据划分的重要性。"}}
{"id": "2602.18376", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18376", "abs": "https://arxiv.org/abs/2602.18376", "authors": ["Ashwin P. Dani"], "title": "Parameter Update Laws for Adaptive Control with Affine Equality Parameter Constraints", "comment": null, "summary": "In this paper, constrained parameter update laws for adaptive control with convex equality constraint on the parameters are developed, one based on a gradient only update and the other incorporating concurrent learning (CL) update. The update laws are derived by solving a constrained optimization problem with affine equality constraints. This constrained problem is reformulated as an equivalent unconstrained problem in a new variable, thereby eliminating the equality constraints. The resulting update law is integrated with an adaptive trajectory tracking controller, enabling online learning of the unknown system parameters. Lyapunov stability of the closed-loop system with the equality-constrained parameter update law is established. The effectiveness of the proposed equality-constrained adaptive control law is demonstrated through simulations, validating its ability to maintain constraints on the parameter estimates, achieving convergence to the true parameters for CL-based update law, and achieving asymptotic and exponential tracking performance for constrained gradient and constrained CL-based update laws, respectively.", "AI": {"tldr": "本文提出了一种用于自适应控制的约束参数更新律，该约束为凸等式约束。开发了两种更新律，一种仅基于梯度，另一种结合了并发学习（CL）。通过求解一个带有仿射等式约束的优化问题推导出更新律，并将其整合到自适应轨迹跟踪控制器中，实现了参数的在线学习。文章证明了闭环系统的Lyapunov稳定性，并通过仿真验证了所提出方法的有效性。", "motivation": "为了在自适应控制中处理参数具有凸等式约束的情况，需要开发能够满足这些约束的参数更新律。", "method": "通过将带有仿射等式约束的优化问题重构为等价的无约束问题，从而推导出参数更新律。然后将这些更新律集成到自适应轨迹跟踪控制器中，并利用Lyapunov稳定性理论证明了闭环系统的稳定性。", "result": "提出了两种参数更新律：一种是基于梯度的更新，另一种是结合并发学习（CL）的更新。仿真结果表明，所提出的方法能够有效地维持参数估计的约束，CL方法能使参数估计收敛到真实值，而梯度和CL方法分别能实现渐近和指数跟踪性能。", "conclusion": "本文成功开发了满足凸等式约束的自适应控制参数更新律，并证明了其稳定性。仿真结果证实了所提出方法的有效性，能够在保持参数约束的同时实现优越的跟踪性能。"}}
{"id": "2602.17869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17869", "abs": "https://arxiv.org/abs/2602.17869", "authors": ["Yuxiao Chen", "Jue Wang", "Zhikang Zhang", "Jingru Yi", "Xu Zhang", "Yang Zou", "Zhaowei Cai", "Jianbo Yuan", "Xinyu Li", "Hao Yang", "Davide Modolo"], "title": "Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models", "comment": null, "summary": "With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.", "AI": {"tldr": "提出了一种端到端的长视频理解框架，包含自适应视频采样器（AVS）和时空视频压缩器（SVC），并与多模态大语言模型（MLLM）集成，以解决长视频中的冗余和信息提取挑战，并取得了良好的性能。", "motivation": "当前的视频理解模型在处理长视频时面临挑战，主要在于内存限制下如何高效地纳入更多帧，以及如何从大量数据中提取区分性信息。", "method": "提出了一种端到端的长视频理解框架，该框架包括：1）基于信息密度的自适应视频采样器（AVS），用于自适应地捕捉视频序列中的关键信息；2）基于自编码器的时空视频压缩器（SVC），用于高效压缩视频同时保留重要信息；3）将AVS和SVC与多模态大语言模型（MLLM）集成。", "result": "所提出的框架在不同时长的视频序列上都能有效捕捉关键信息，并能实现高压缩率同时保留区分性信息。该框架在长视频理解任务和标准视频理解基准测试中均表现出有希望的性能。", "conclusion": "该方法能够有效地管理长视频序列的复杂性，在处理长视频理解任务和标准视频理解任务时都展现了其通用性和有效性。"}}
{"id": "2602.18071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18071", "abs": "https://arxiv.org/abs/2602.18071", "authors": ["Boyuan An", "Zhexiong Wang", "Yipeng Wang", "Jiaqi Li", "Sihang Li", "Jing Zhang", "Chen Feng"], "title": "EgoPush: Learning End-to-End Egocentric Multi-Object Rearrangement for Mobile Robots", "comment": "18 pages, 13 figures. Project page: https://ai4ce.github.io/EgoPush/", "summary": "Humans can rearrange objects in cluttered environments using egocentric perception, navigating occlusions without global coordinates. Inspired by this capability, we study long-horizon multi-object non-prehensile rearrangement for mobile robots using a single egocentric camera. We introduce EgoPush, a policy learning framework that enables egocentric, perception-driven rearrangement without relying on explicit global state estimation that often fails in dynamic scenes. EgoPush designs an object-centric latent space to encode relative spatial relations among objects, rather than absolute poses. This design enables a privileged reinforcement-learning (RL) teacher to jointly learn latent states and mobile actions from sparse keypoints, which is then distilled into a purely visual student policy. To reduce the supervision gap between the omniscient teacher and the partially observed student, we restrict the teacher's observations to visually accessible cues. This induces active perception behaviors that are recoverable from the student's viewpoint. To address long-horizon credit assignment, we decompose rearrangement into stage-level subproblems using temporally decayed, stage-local completion rewards. Extensive simulation experiments demonstrate that EgoPush significantly outperforms end-to-end RL baselines in success rate, with ablation studies validating each design choice. We further demonstrate zero-shot sim-to-real transfer on a mobile platform in the real world. Code and videos are available at https://ai4ce.github.io/EgoPush/.", "AI": {"tldr": "提出了一种名为 EgoPush 的策略学习框架，用于通过单个 egocentric 摄像头实现长时程、多目标、非抓取式重排任务，无需全局坐标系，能够从视觉线索中学习并实现零样本仿真到现实的迁移。", "motivation": "受人类在混乱环境中利用 egocentric 感知进行物体重排的能力启发，研究长时程、多目标、非抓取式重排任务，解决机器人缺乏全局坐标系和动态场景下的状态估计问题。", "method": "EgoPush 框架设计了一个物体中心潜在空间来编码物体间的相对空间关系。利用强化学习（RL）的“教师-学生”范式，让“教师”学习潜在状态和移动动作，然后将其蒸馏到纯视觉的“学生”策略。通过限制“教师”的观察范围到视觉可及线索以减小监督差距，并引入阶段性奖励来解决长时程信用分配问题。", "result": "在仿真实验中，EgoPush 的成功率显著优于端到端的 RL 基线。消融实验验证了各个设计选择的有效性。在真实机器人平台上实现了零样本仿真到现实的迁移。", "conclusion": "EgoPush 框架能够有效地解决在复杂环境中，仅依靠 egocentric 视觉信息进行长时程、多目标、非抓取式物体重排的任务，并且能够成功迁移到真实世界。"}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "本文提出了OMAD，一个在线多智能体强化学习框架，利用扩散策略进行协调，通过最大化缩放联合熵来促进探索，并在CTDE范式下使用联合分布值函数来优化去中心化扩散策略。", "motivation": "在线多智能体强化学习（MARL）需要提高策略表达能力以获得更好的性能。扩散模型在表达性和多模态表示方面表现出色，但其在在线MARL中的应用受到难以处理的似然性阻碍了基于熵的探索和协调。", "method": "提出了OMAD框架，通过最大化缩放联合熵的宽松策略目标来促进探索，不依赖于可处理的似然性。在CTDE范式下，使用联合分布值函数来优化去中心化扩散策略，利用可处理的熵增强目标指导同步更新。", "result": "在MPE和MAMuJoCo的10个不同任务上进行了广泛评估，OMAD达到了新的最先进水平，在样本效率方面实现了2.5倍到5倍的显著提升。", "conclusion": "OMAD是首批在线离策略MARL框架之一，利用扩散策略解决了在线MARL中的探索和协调问题，并在多个基准测试中取得了显著的性能提升。"}}
{"id": "2602.17981", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17981", "abs": "https://arxiv.org/abs/2602.17981", "authors": ["Amine Kobeissi", "Philippe Langlais"], "title": "Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering", "comment": null, "summary": "Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.", "AI": {"tldr": "本文研究了在金融问答中，当检索到正确文档但遗漏包含答案的关键页面或文本块时发生的“文档内检索失败”问题，并提出了一种针对金融文件进行领域微调的页面评分器来改善这一问题。", "motivation": "在金融领域，利用检索增强生成（RAG）技术处理长篇监管文件进行问答时，检索到包含答案的精确上下文至关重要。然而，目前的研究对“文档内检索失败”（即检索到正确文档但遗漏关键页面/文本块）这一常见且影响重大的失败模式关注不足。", "method": "研究人员评估了不同粒度（文档、页面、文本块）的检索效果，并引入了基于“神谕”（oracle）的分析来提供检索和生成性能的理论上限。他们复现并比较了多种检索策略（密集、稀疏、混合、分层检索，结合重排和查询重构），并引入了一种领域微调的页面评分器，将页面作为一个中间检索单元。", "result": "在FinanceBench数据集的一个子集上，研究发现文档检索的提升通常会带来页面召回率的增强，但神谕性能表明页面和文本块层面的检索仍有改进空间。领域微调的页面评分器显著提高了页面召回率和文本块检索效果。", "conclusion": "文档内检索失败是金融问答中的一个重要问题，通过引入领域微调的页面评分器，可以有效提升页面召回率和文本块检索性能，从而提高检索增强生成在金融问答中的可靠性。"}}
{"id": "2602.17871", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.17871", "abs": "https://arxiv.org/abs/2602.17871", "authors": ["Dhruba Ghosh", "Yuhui Zhang", "Ludwig Schmidt"], "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.", "AI": {"tldr": "当前的大型视觉语言模型（VLMs）在各种视觉问答任务上表现出色，但在细粒度图像分类方面仍有不足。研究发现，改进语言模型能全面提升性能，而改进视觉编码器则能更显著地提升细粒度分类能力。预训练阶段，特别是语言模型权重解冻时，对细粒度性能至关重要。", "motivation": "尽管VLMs在视觉问答等任务上取得了显著进展，但在细粒度图像分类等传统图像分类基准测试中表现不佳，研究旨在找出造成这种脱节的原因。", "method": "通过在细粒度分类基准上测试大量近期VLMs，并进行一系列消融实验，分析了不同组件（如LLM、视觉编码器）和预训练策略（如语言模型权重是否解冻）对模型在不同基准上表现的影响。", "result": "1. 改进LLM能同等程度地提升所有基准的分数。2. 改进视觉编码器能不成比例地提升细粒度分类性能。3. 预训练阶段对细粒度性能至关重要，尤其是在预训练期间解冻语言模型权重时。", "conclusion": "视觉编码器的质量和预训练阶段（特别是语言模型权重的解冻）是提升VLMs细粒度视觉理解能力的关键因素，这为未来改进VLMs的视觉中心能力提供了方向。"}}
{"id": "2602.18097", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18097", "abs": "https://arxiv.org/abs/2602.18097", "authors": ["Aarati Andrea Noronha", "Jean Oh"], "title": "Interacting safely with cyclists using Hamilton-Jacobi reachability and reinforcement learning", "comment": "7 pages. This manuscript was completed in 2020 as part of the first author's graduate thesis at Carnegie Mellon University", "summary": "In this paper, we present a framework for enabling autonomous vehicles to interact with cyclists in a manner that balances safety and optimality. The approach integrates Hamilton-Jacobi reachability analysis with deep Q-learning to jointly address safety guarantees and time-efficient navigation. A value function is computed as the solution to a time-dependent Hamilton-Jacobi-Bellman inequality, providing a quantitative measure of safety for each system state. This safety metric is incorporated as a structured reward signal within a reinforcement learning framework. The method further models the cyclist's latent response to the vehicle, allowing disturbance inputs to reflect human comfort and behavioral adaptation. The proposed framework is evaluated through simulation and comparison with human driving behavior and an existing state-of-the-art method.", "AI": {"tldr": "提出了一种结合Hamilton-Jacobi可达性分析和深度Q学习的框架，用于实现自动驾驶车辆与骑行者的安全且最优的交互。", "motivation": "为了让自动驾驶车辆在与骑行者互动时，能够同时保证安全并实现最优导航。", "method": "将Hamilton-Jacobi可达性分析与深度Q学习相结合。首先，计算Hamilton-Jacobi-Bellman不等式的解作为价值函数，量化系统状态的安全性；然后，将该安全度量作为结构化奖励信号融入强化学习框架；最后，通过模拟骑行者对车辆的潜在反应来建模，使干扰输入能反映人类的舒适度和行为适应性。", "result": "通过仿真评估，与人类驾驶行为和现有最先进方法进行了比较，证明了该框架的有效性。", "conclusion": "该框架能够有效地平衡自动驾驶车辆与骑行者交互时的安全性和时间效率，并能模拟人类的反应。"}}
{"id": "2602.18382", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18382", "abs": "https://arxiv.org/abs/2602.18382", "authors": ["Yu Kawano", "Simone Betteti", "Alexander Davydov", "Francesco Bullo"], "title": "Incremental Input-to-State Stability and Equilibrium Tracking for Stochastic Contracting Dynamics", "comment": null, "summary": "In this paper, we study the contractivity of nonlinear stochastic differential equations (SDEs) driven by deterministic inputs and Brownian motions. Given a weighted $\\ell_2$-norm for the state space, we show that an SDE is incrementally noise- and input-to-state stable if its vector field is uniformly contracting in the state and uniformly Lipschitz in the input. This result is applied to error estimation for time-varying equilibrium tracking in the presence of noise affecting both the system dynamics and the input signals. We consider both Ornstein-Uhlenbeck processes modeling unbounded noise and Jacobi diffusion processes modeling bounded noise. Finally, we turn our attention to the associated Fokker-Planck equation of an SDE. For this context, we prove incremental input-to-state stability with respect to an arbitrary $p$-Wasserstein metric when the drift vector field is uniformly contracting in the state and uniformly Lipschitz in the input with respect to an arbitrary norm.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2602.18029", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18029", "abs": "https://arxiv.org/abs/2602.18029", "authors": ["Ali El Filali", "Inès Bedar"], "title": "Towards More Standardized AI Evaluation: From Models to Agents", "comment": "19 pages, 3 figures", "summary": "Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer \"How good is the model?\" but \"Can we trust the system to behave as intended, under change, at scale?\". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.", "AI": {"tldr": "本文认为，在AI系统日益复杂和动态的背景下，传统的、基于静态基准和聚合分数的评估方法已不再适用。文章探讨了当前评估实践的局限性，并提出评估应被视为一种测量学科，以建立对非确定性AI系统的信任、促进迭代和支持治理。", "motivation": "传统的AI评估方法在应对日益复杂、动态且作为工具使用AI系统时存在不足，这些方法在模型中心时代被继承下来，但已不再能准确反映系统的真实行为，可能导致误导。", "method": "本文通过分析现有评估管道中的沉默故障模式、高基准分数如何误导团队，以及代理系统如何改变性能测量的含义，来探讨当前评估方法的局限性。作者不提议新的指标或更难的基准，而是侧重于阐明评估在新AI时代（尤其是针对代理系统）的角色。", "result": "文章指出，传统的评估方法（如静态基准、聚合分数和一次性成功标准）日益模糊而非揭示系统行为。高基准分数常常误导团队，而代理系统从根本上改变了性能测量的意义。", "conclusion": "评估不应是“性能展示”，而应成为一种测量学科，其目的是在非确定性系统中建立信任、促进迭代和支持治理。评估的重点应从“模型有多好”转向“在变化和大规模下，是否可以信任该系统按预期行事”。"}}
{"id": "2602.17909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17909", "abs": "https://arxiv.org/abs/2602.17909", "authors": ["Amirhosein Javadi", "Chi-Shiang Gau", "Konstantinos D. Polyzos", "Tara Javidi"], "title": "A Single Image and Multimodality Is All You Need for Novel View Synthesis", "comment": null, "summary": "Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.", "AI": {"tldr": "本研究提出了一种利用稀疏多模态测距数据（如雷达或激光雷达）来重建密集深度图的框架，并将其作为扩散模型进行单图像新视角合成的鲁棒几何条件。实验证明，该方法能显著提高生成视频的几何一致性和视觉质量，尤其是在纹理不足、恶劣天气或遮挡严重的真实世界场景中。", "motivation": "现有的基于扩散模型的单图像新视角合成方法严重依赖单目深度估计，而单目深度估计在纹理不足、恶劣天气和遮挡场景下不够鲁棒，限制了合成视图的质量和一致性。", "method": "提出了一种多模态深度重建框架，利用稀疏的多模态测距数据（如汽车雷达或激光雷达）重建密集深度图。该方法在角域建模深度，使用局部高斯过程，能够高效推理并量化不确定性。重建的深度图和不确定性可直接替代现有扩散模型渲染管线中的单目深度估计，无需修改生成模型。", "result": "在真实世界的多模态驾驶场景下进行的实验表明，用基于稀疏范围测量的重建深度替代纯视觉深度，能够显著提高单图像新视角视频生成的几何一致性和视觉质量。", "conclusion": "可靠的几何先验对于基于扩散模型的新视角合成至关重要。即使在极端稀疏的情况下，多模态传感也能带来实际的好处，克服单目深度估计的局限性，从而提升新视角合成的性能。"}}
{"id": "2602.18416", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18416", "abs": "https://arxiv.org/abs/2602.18416", "authors": ["Kenshiro Oguri", "Gregory Lantoine"], "title": "Convex Block-Cholesky Approach to Risk-Constrained Low-thrust Trajectory Design under Operational Uncertainty", "comment": null, "summary": "Designing robust trajectories under uncertainties is an emerging technology that may represent a key paradigm shift in space mission design. As we pursue more ambitious scientific goals (e.g., multi-moon tours, missions with extensive components of autonomy), it becomes more crucial that missions are designed with navigation (Nav) processes in mind. The effect of Nav processes is statistical by nature, as they consist of orbit determination (OD) and flight-path control (FPC). Thus, this mission design paradigm calls for techniques that appropriately quantify statistical effects of Nav, evaluate associated risks, and design missions that ensure sufficiently low risk while minimizing a statistical performance metric; a common metric is Delta-V99: worst-case (99%-quantile) Delta-V expenditure including statistical FPC efforts. In response to the need, this paper develops an algorithm for risk-constrained trajectory optimization under operational uncertainties due to initial state dispersion, navigation error, maneuver execution error, and imperfect dynamics modeling. We formulate it as a nonlinear stochastic optimal control problem and develop a computationally tractable algorithm that combines optimal covariance steering and sequential convex programming (SCP). Specifically, the proposed algorithm takes a block-Cholesky approach for convex formulation of optimal covariance steering, and leverages a recent SCP algorithm, SCvx*, for reliable numerical convergence. We apply the developed algorithm to risk-constrained, statistical trajectory optimization for exploration of dwarf planet Ceres with a Mars gravity assist, and demonstrate the robustness of the statistically-optimal trajectory and FPC policies via nonlinear Monte Carlo simulation.", "AI": {"tldr": "该研究提出了一种结合最优协方差控制和序列凸规划的算法，用于在考虑初始状态、导航、执行和模型不确定性的情况下，进行风险约束下的轨迹优化，并以探索谷神星的任务为例进行了验证。", "motivation": "随着空间任务日益复杂（如多月球探测、自主化程度高等），在任务设计中考虑导航过程的统计效应和风险变得至关重要。现有技术需要能够量化导航的统计效应并设计出风险较低的轨迹。", "method": "将问题建模为非线性随机最优控制问题，并开发了一种结合最优协方差控制（采用块-乔列斯基分解）和序列凸规划（SCvx*）的算法来解决。该算法考虑了初始状态分散、导航误差、机动执行误差和动力学模型不确定性。", "result": "所开发的算法成功地应用于谷神星探索任务的风险约束轨迹优化。通过非线性蒙特卡罗仿真，证明了该算法设计的统计最优轨迹和飞行路径控制策略的鲁棒性。", "conclusion": "该研究提出了一种有效的算法，能够处理不确定性下的空间轨迹优化问题，并能生成鲁棒且风险可控的任务轨迹，为未来更复杂的空间任务设计提供了新的方法。"}}
{"id": "2602.17951", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17951", "abs": "https://arxiv.org/abs/2602.17951", "authors": ["Guoheng Sun", "Tingting Du", "Kaixi Feng", "Chenxiang Luo", "Xingguo Ding", "Zheyu Shen", "Ziyao Wang", "Yexiao He", "Ang Li"], "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, naïve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.", "AI": {"tldr": "本文提出了一种名为ROCKET的残差导向多层表示对齐框架，用于提升2D视觉-语言-动作（VLA）模型在3D空间理解方面的能力，通过多层对齐减少梯度冲突，显著提高了训练效率和机器人操作的成功率。", "motivation": "现有的VLA模型在3D空间理解方面存在不足，现有通过预训练的3D视觉基础模型进行表示对齐的方法通常只在单层进行监督，未能充分利用多层信息，且简单的多层对齐会引起梯度干扰。", "method": "ROCKET框架将多层对齐设计为对齐残差流。具体而言，它使用一个共享投影仪，通过层不变映射将VLA模型的多个层与3D视觉基础模型的多个层进行对齐，从而减少梯度冲突。此外，还提出了一种Matryoshka风格的稀疏激活方案来平衡多层对齐损失。", "result": "在LIBERO基准测试中，ROCKET仅用约4%的计算量就达到了98.5%的SOTA成功率。在LIBERO-Plus和RoboTwin以及多种VLA模型上，ROCKET也展现出优越的性能。", "conclusion": "ROCKET框架通过残差导向的多层表示对齐，有效解决了现有方法的局限性，能够显著提升VLA模型在3D空间理解和机器人操控任务上的表现，并且训练效率极高。"}}
{"id": "2602.18164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18164", "abs": "https://arxiv.org/abs/2602.18164", "authors": ["Jonas Frey", "Turcan Tuna", "Frank Fu", "Katharine Patterson", "Tianao Xu", "Maurice Fallon", "Cesar Cadena", "Marco Hutter"], "title": "GrandTour: A Legged Robotics Dataset in the Wild for Multi-Modal Perception and State Estimation", "comment": "Jonas Frey and Turcan Tuna contributed equally. Submitted to Sage The International Journal of Robotics Research", "summary": "Accurate state estimation and multi-modal perception are prerequisites for autonomous legged robots in complex, large-scale environments. To date, no large-scale public legged-robot dataset captures the real-world conditions needed to develop and benchmark algorithms for legged-robot state estimation, perception, and navigation. To address this, we introduce the GrandTour dataset, a multi-modal legged-robotics dataset collected across challenging outdoor and indoor environments, featuring an ANYbotics ANYmal-D quadruped equipped with the \\boxi multi-modal sensor payload. GrandTour spans a broad range of environments and operational scenarios across distinct test sites, ranging from alpine scenery and forests to demolished buildings and urban areas, and covers a wide variation in scale, complexity, illumination, and weather conditions. The dataset provides time-synchronized sensor data from spinning LiDARs, multiple RGB cameras with complementary characteristics, proprioceptive sensors, and stereo depth cameras. Moreover, it includes high-precision ground-truth trajectories from satellite-based RTK-GNSS and a Leica Geosystems total station. This dataset supports research in SLAM, high-precision state estimation, and multi-modal learning, enabling rigorous evaluation and development of new approaches to sensor fusion in legged robotic systems. With its extensive scope, GrandTour represents the largest open-access legged-robotics dataset to date. The dataset is available at https://grand-tour.leggedrobotics.com, on HuggingFace (ROS-independent), and in ROS formats, along with tools and demo resources.", "AI": {"tldr": "本研究发布了一个名为 GrandTour 的大型、多模态数据集，用于训练和评估四足机器人，涵盖了各种复杂的室内外环境和场景。", "motivation": "现有的大型公开数据集不足以满足腿足机器人状态估计、感知和导航算法的开发和基准测试需求，特别是在复杂的真实世界条件下。", "method": "收集了一个包含不同环境（高山、森林、废墟、城市）和操作场景的跨越式多模态数据集。使用 ANYbotics ANYmal-D 四足机器人，并配备了包括激光雷达、RGB摄像头、本体感受传感器和立体深度摄像头在内的多模态传感器。通过 RTK-GNSS 和徕卡测量仪提供了高精度的地面真实轨迹。", "result": "GrandTour 数据集是迄今为止最大的开放获取的腿足机器人数据集，提供了跨越不同规模、复杂度、光照和天气条件的大量同步传感器数据。该数据集支持 SLAM、高精度状态估计和多模态学习的研究。", "conclusion": "GrandTour 数据集填补了腿足机器人领域大型、真实世界多模态数据集的空白，为研究人员提供了一个强大的资源，以推动传感器融合、状态估计和感知技术在腿足机器人系统中的发展。"}}
{"id": "2602.18000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18000", "abs": "https://arxiv.org/abs/2602.18000", "authors": ["Xuting Lan", "Mingliang Zhou", "Xuekai Wei", "Jielu Yan", "Yueting Huang", "Huayan Pu", "Jun Luo", "Weijia Jia"], "title": "Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching", "comment": null, "summary": "Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.", "AI": {"tldr": "提出了一种名为MQAF的记忆驱动质量感知框架，该框架通过存储失真模式并动态切换双模式评估策略，减少了对高质量参考图像的依赖，实现了全参考和无参考图像质量评估。", "motivation": "现有全参考图像质量评估方法性能受限于参考图像质量，在缺乏理想参考图像的实际应用中受到限制。受人眼视觉记忆机制的启发，旨在解决参考图像缺失时的图像质量评估问题。", "method": "构建了一个记忆库存储失真模式，并设计了动态切换的双模式评估策略。当有参考图像时，通过自适应加权参考信息并与记忆库中的失真模式比较来获得参考引导的质量分数；当无参考图像时，则仅依靠记忆库中的失真模式推断图像质量，实现无参考评估。", "result": "在多个数据集上的实验表明，所提出的MQAF方法在全参考和无参考任务上均优于最先进的方法。", "conclusion": "MQAF框架成功地实现了减少对高质量参考图像依赖的图像质量评估，并能同时适应全参考和无参考场景，展示了其在实际应用中的潜力。"}}
{"id": "2602.18092", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18092", "abs": "https://arxiv.org/abs/2602.18092", "authors": ["Matthew DiGiuseppe", "Joshua Robison"], "title": "Perceived Political Bias in LLMs Reduces Persuasive Abilities", "comment": "39 pages, 10 figures", "summary": "Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.", "AI": {"tldr": "研究表明，关于大型语言模型（LLM）政治偏见的观点会影响其说服力。当参与者被告知LLM可能与其党派立场不一致时，LLM纠正错误信息的说服力会显著下降。", "motivation": "随着对话式AI（特别是LLM）越来越多地用于传播信息和纠正公众误解，研究其说服力及其影响因素至关重要，特别是政治中立性的感知。", "method": "通过一项预先注册的美国在线调查实验（N=2144）进行。参与者与ChatGPT就个人持有的经济政策错误观念进行三轮对话。一组参与者收到一条表明LLM可能偏袒与受访者党派相反立场的简短信息，另一组则为中性对照组。", "result": "与中性对照组相比，告知LLM可能存在党派偏见的组，其说服力下降了28%。转录分析显示，警告信息改变了互动方式：受访者会更多地反驳，并且接受信息的方式不那么开放。", "conclusion": "对话式AI的说服力在政治上是受限制的，并且受到对其党派立场感知的制约。关于AI政治立场的说法会削弱其纠正公众误解的能力。"}}
{"id": "2602.18386", "categories": ["cs.RO", "cs.AI", "cs.LG", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18386", "abs": "https://arxiv.org/abs/2602.18386", "authors": ["Mohamed Elgouhary", "Amr S. El-Wakeel"], "title": "Learning to Tune Pure Pursuit in Autonomous Racing: Joint Lookahead and Steering-Gain Control with PPO", "comment": null, "summary": "Pure Pursuit (PP) is widely used in autonomous racing for real-time path tracking due to its efficiency and geometric clarity, yet performance is highly sensitive to how key parameters-lookahead distance and steering gain-are chosen. Standard velocity-based schedules adjust these only approximately and often fail to transfer across tracks and speed profiles. We propose a reinforcement-learning (RL) approach that jointly chooses the lookahead Ld and a steering gain g online using Proximal Policy Optimization (PPO). The policy observes compact state features (speed and curvature taps) and outputs (Ld, g) at each control step. Trained in F1TENTH Gym and deployed in a ROS 2 stack, the policy drives PP directly (with light smoothing) and requires no per-map retuning. Across simulation and real-car tests, the proposed RL-PP controller that jointly selects (Ld, g) consistently outperforms fixed-lookahead PP, velocity-scheduled adaptive PP, and an RL lookahead-only variant, and it also exceeds a kinematic MPC raceline tracker under our evaluated settings in lap time, path-tracking accuracy, and steering smoothness, demonstrating that policy-guided parameter tuning can reliably improve classical geometry-based control.", "AI": {"tldr": "本研究提出一种强化学习（RL）方法，用于在线联合优化纯追踪（PP）算法中的前视距离（Ld）和转向增益（g），以提高自动驾驶赛车的路径跟踪性能，并在仿真和真实车辆测试中取得了优于传统方法和仅优化Ld的RL方法的成绩。", "motivation": "传统的纯追踪（PP）算法在参数选择（前视距离和转向增益）上非常敏感，速度基准调整方法近似且难以跨赛道和速度曲线转移。因此，需要一种更鲁棒的方法来在线调整这些参数。", "method": "使用Proximal Policy Optimization（PPO）算法，通过强化学习（RL）策略在线联合选择前视距离（Ld）和转向增益（g）。策略接收速度和曲率作为状态特征，并输出（Ld, g）。", "result": "在F1TENTH Gym中训练的RL-PP控制器，在仿真和真实车辆测试中，其性能（圈速、路径跟踪精度、转向平滑度）均优于固定前视距离的PP、速度调度的自适应PP以及仅优化Ld的RL变体，并在评估条件下优于运动学MPC赛道跟踪器。", "conclusion": "策略引导的参数调整（联合优化Ld和g）能够可靠地改进基于几何的经典控制方法，RL-PP控制器是一种有效的解决方案，无需进行每张地图的重新调优。"}}
{"id": "2602.18019", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18019", "abs": "https://arxiv.org/abs/2602.18019", "authors": ["Yujie Jin", "Wenxin Zhang", "Jingjing Wang", "Guodong Zhou"], "title": "DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE", "comment": null, "summary": "In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.", "AI": {"tldr": "本文提出了一个名为DeepSVU的新视频理解任务，旨在识别、定位、归因和评估威胁原因，并为此开发了一种名为UPRM的新方法，该方法通过UPE块和PTR来解决信息建模和权衡的挑战，并在实验中取得了优于现有方法的性能。", "motivation": "现有研究主要关注检测和定位视频中的威胁，而缺乏生成和评估威胁原因的能力，这促使了本研究的提出。", "method": "本文提出了In-depth Security-oriented Video Understanding (DeepSVU) 任务，并设计了一种名为Unified Physical-world Regularized MoE (UPRM) 的方法，该方法包含Unified Physical-world Enhanced MoE (UPE) Block 和 Physical-world Trade-off Regularizer (PTR) 两个关键组件，以解决粗粒度到细粒度的物理世界信息建模和自适应权衡的挑战。", "result": "在DeepSVU指令数据集（UCF-C instructions 和 CUVA instructions）上的实验表明，UPRM在性能上优于几种先进的视频-语言模型（Video-LLMs）以及非VLM方法，证明了粗细粒度物理世界信息在DeepSVU任务中的重要性以及UPRM的有效性。", "conclusion": "DeepSVU任务的提出填补了现有研究的空白，UPRM方法能够有效地建模和权衡物理世界信息，从而在DeepSVU任务上取得优异表现。"}}
{"id": "2602.18137", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18137", "abs": "https://arxiv.org/abs/2602.18137", "authors": ["Vincent Grari", "Ciprian Tomoiaga", "Sylvain Lamprier", "Tatsunori Hashimoto", "Marcin Detyniecki"], "title": "Agentic Adversarial QA for Improving Domain-Specific LLMs", "comment": "9 pages, 1 Figure", "summary": "Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.", "AI": {"tldr": "提出了一种对抗性问题生成框架，以解决大型语言模型在专业领域适应性差且数据稀疏的问题，该框架能生成紧凑、语义上具有挑战性的问题，显著提高了准确性并减少了样本量。", "motivation": "大型语言模型在专业领域的适应性不足，以及高质量、任务相关数据稀缺，限制了其在专业领域的应用。现有的合成数据生成方法在支持解释性推理和数据效率方面存在不足。", "method": "提出了一种对抗性问题生成框架。该框架通过比较待适应模型与基于参考文档的专家模型的输出，利用迭代、反馈驱动的过程来生成具有挑战性的问题，以揭示和解决理解上的差距。", "result": "在LegalBench语料库的专业子集上的评估表明，所提出的方法使用少得多的合成样本实现了更高的准确性。", "conclusion": "所提出的对抗性问题生成框架能够克服现有合成数据生成方法的局限性，有效地提高大型语言模型在专业领域的适应性，并在数据效率和准确性方面取得显著提升。"}}
{"id": "2602.18174", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18174", "abs": "https://arxiv.org/abs/2602.18174", "authors": ["Hyoseok Ju", "Bokeon Suh", "Giseop Kim"], "title": "Have We Mastered Scale in Deep Monocular Visual SLAM? The ScaleMaster Dataset and Benchmark", "comment": "8 pages, 9 figures, accepted to ICRA 2026", "summary": "Recent advances in deep monocular visual Simultaneous Localization and Mapping (SLAM) have achieved impressive accuracy and dense reconstruction capabilities, yet their robustness to scale inconsistency in large-scale indoor environments remains largely unexplored. Existing benchmarks are limited to room-scale or structurally simple settings, leaving critical issues of intra-session scale drift and inter-session scale ambiguity insufficiently addressed. To fill this gap, we introduce the ScaleMaster Dataset, the first benchmark explicitly designed to evaluate scale consistency under challenging scenarios such as multi-floor structures, long trajectories, repetitive views, and low-texture regions. We systematically analyze the vulnerability of state-of-the-art deep monocular visual SLAM systems to scale inconsistency, providing both quantitative and qualitative evaluations. Crucially, our analysis extends beyond traditional trajectory metrics to include a direct map-to-map quality assessment using metrics like Chamfer distance against high-fidelity 3D ground truth. Our results reveal that while recent deep monocular visual SLAM systems demonstrate strong performance on existing benchmarks, they suffer from severe scale-related failures in realistic, large-scale indoor environments. By releasing the ScaleMaster dataset and baseline results, we aim to establish a foundation for future research toward developing scale-consistent and reliable visual SLAM systems.", "AI": {"tldr": "本研究提出了ScaleMaster数据集，用于评估深度单目视觉SLAM系统在大尺度室内环境中处理尺度不一致问题的能力，并分析了现有系统的局限性。", "motivation": "现有深度单目视觉SLAM系统在准确性和重建能力上取得了很大进展，但它们在大尺度室内环境中对尺度不一致问题的鲁棒性研究不足，现有基准测试也存在局限性，未能充分解决会话内尺度漂移和会话间尺度模糊的问题。", "method": "提出了ScaleMaster数据集，这是一个专门用于评估尺度一致性的基准，包含多楼层结构、长轨迹、重复视图和低纹理区域等挑战性场景。对最先进的深度单目视觉SLAM系统进行了系统性分析，采用了定量和定性评估方法，并使用Chamfer距离等指标直接评估地图与高保真3D真实情况的匹配度。", "result": "现有先进的深度单目视觉SLAM系统在现有基准测试上表现良好，但在真实的大尺度室内环境中，它们在尺度一致性方面表现出严重的失败。", "conclusion": "ScaleMaster数据集和基线结果的发布，旨在为未来开发尺度一致且可靠的视觉SLAM系统奠定基础，并揭示了现有系统在处理大规模室内环境尺度问题上的不足。"}}
{"id": "2602.18006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18006", "abs": "https://arxiv.org/abs/2602.18006", "authors": ["Ahsan Baidar Bakht", "Mohamad Alansari", "Muhayy Ud Din", "Muzammal Naseer", "Sajid Javed", "Irfan Hussain", "Jiri Matas", "Arif Mahmood"], "title": "MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method", "comment": null, "summary": "Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.", "AI": {"tldr": "本文提出了首个大规模水下多模态目标跟踪数据集MUOT_3M，以及一个基于该数据集的、利用多模态信息提升跟踪性能的MUTrack模型，该模型在多个基准测试中取得了SOTA性能。", "motivation": "现有水下目标跟踪（UOT）数据集规模小且仅支持RGB，无法应对严苛的水下环境（颜色失真、浑浊、低可见度），限制了模型鲁棒性。因此，研究者希望构建一个更大、更丰富的数据集，并开发能利用多模态信息的跟踪方法。", "method": "构建了一个包含300万帧、3030个视频的大规模水下多模态数据集MUOT_3M，包含RGB、增强RGB、深度和语言模态，并进行了详细标注。在此基础上，提出了MUTrack模型，该模型利用SAM（Segment Anything Model）将多模态信息迁移到单模态学生模型，通过视觉几何对齐、视觉语言融合和四级知识蒸馏来提升跟踪性能。", "result": "MUTrack在5个UOT基准测试中，AUC和精度分别比最强的SOTA基线高出8.40%和7.80%，同时运行速度达到24 FPS。MUOT_3M数据集为水下跟踪研究提供了新的基础。", "conclusion": "MUOT_3M数据集和MUTrack模型共同为开发可扩展、可部署的水下目标跟踪系统奠定了基础，证明了多模态信息在提升水下跟踪性能方面的有效性。"}}
{"id": "2602.18145", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18145", "abs": "https://arxiv.org/abs/2602.18145", "authors": ["Siya Qi", "Yudong Chen", "Runcong Zhao", "Qinglin Zhu", "Zhanghao Hu", "Wei Liu", "Yulan He", "Zheng Yuan", "Lin Gui"], "title": "Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention", "comment": "25 pages, 10 figures", "summary": "Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.", "AI": {"tldr": "研究提出一种基于注意力频率分析的轻量级幻觉检测方法，通过提取高频注意力特征来识别语言模型生成的幻觉内容，并在多个基准测试中展现出优于现有方法的性能。", "motivation": "现有的幻觉检测方法未能捕捉注意力机制中细粒度的不稳定性，而注意力机制能直接反映模型的接地行为。因此，需要一种更精细的注意力分析方法来提高幻觉检测的准确性。", "method": "将注意力分布建模为离散信号，并引入信号处理中的频率域分析，提取能够反映注意力快速局部变化的高频分量。基于此，开发了一种使用高频注意力特征的轻量级幻觉检测器。", "result": "实验结果表明，与基于验证、基于内部表示以及其他基于注意力的现有方法相比，所提出的基于高频注意力特征的方法在RAGTruth和HalluRAG基准测试中，在不同模型和任务上均取得了性能提升。", "conclusion": "高频注意力能量与幻觉现象相关，反映了模型接地行为的不稳定和碎片化。所提出的基于高频注意力特征的幻觉检测方法是一种有效且轻量级的解决方案，能够显著提高幻觉检测的性能。"}}
{"id": "2602.18089", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18089", "abs": "https://arxiv.org/abs/2602.18089", "authors": ["Kunwar Arpit Singh", "Ankush Prakash", "Haroon R Lone"], "title": "DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text", "comment": null, "summary": "Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.", "AI": {"tldr": "该论文介绍了 DohaScript，一个大规模、多写作者的手写印地语（属于天城文）文本数据集，旨在解决现有资源规模小、缺乏多样性和结构复杂性等问题，为天城文连续手写文本的研究提供基准。", "motivation": "现有公开的印地语（天城文）手写文本数据集规模小，主要集中在孤立字符或短词，缺乏词汇控制和写作者多样性，无法捕捉天城文连续、融合和结构复杂的特性（如共享的横线和连字）。", "method": "创建了一个名为 DohaScript 的大规模数据集，包含 531 位写作者的书写内容。数据集被设计为一个并行风格语料库，所有写作者都抄写了相同的六首传统印地语 dohas（对句）。数据集还包含匿名人口统计学元数据、客观的清晰度和分辨率标准以及页面布局难度注解。", "result": "基线实验证明了数据集的质量区分度和对未见过写作者的泛化能力，突显了其可靠性和实用价值。", "conclusion": "DohaScript 是一个标准化的、可复现的基准，旨在推动低资源脚本设置下连续手写天城文文本的研究。"}}
{"id": "2602.18022", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18022", "abs": "https://arxiv.org/abs/2602.18022", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers", "comment": null, "summary": "Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(δ_k, δ_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).", "AI": {"tldr": "本文提出了一种名为双通道注意力引导（DCAG）的训练免费框架，通过同时操控DiT模型中注意力机制的Key和Value投影来控制图像编辑的强度，并在PIE-Bench基准测试中展现出优于仅操控Key通道的性能，尤其在目标删除和添加等局部编辑任务上效果显著。", "motivation": "现有的基于DiT的图像编辑模型在训练免费控制编辑强度方面存在局限，现有方法仅关注Key空间，忽略了Value空间的功能，而Value空间在特征聚合中起着关键作用。", "method": "作者首先发现DiT多模态注意力层中的Key和Value投影都具有明显的偏置-delta结构。在此基础上，提出DCAG框架，同时操控Key通道（控制关注点）和Value通道（控制聚合内容）。理论分析表明，Key通道通过softmax实现粗粒度控制，Value通道通过线性加权实现细粒度控制。", "result": "DCAG在PIE-Bench基准测试（包含700张图像，10种编辑类别）上的实验表明，DCAG在所有保真度指标上均优于仅使用Key通道引导的方法。在目标删除（LPIPS降低4.9%）和目标添加（LPIPS降低3.2%）等局部编辑任务上，性能提升尤为显著。", "conclusion": "通过同时操控Key和Value通道，DCAG框架提供了一个更精确的编辑保真度权衡，能够实现比单通道方法更好的图像编辑效果，尤其是在局部编辑任务中。"}}
{"id": "2602.18016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18016", "abs": "https://arxiv.org/abs/2602.18016", "authors": ["Jiamin Luo", "Xuqian Gu", "Jingjing Wang", "Jiahong Lu"], "title": "Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating", "comment": null, "summary": "Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.", "AI": {"tldr": "本文提出了一种以大型语言模型为中心的 afective Visual Customization (L-AVC) 任务，旨在通过多模态大型语言模型修改图像的主观情感。为解决情感转换语义对齐和非情感内容保留的挑战，提出了一种高效精确的情感操纵（EPEM）方法，包含高效跨情感转换（EIC）和精确跨情感保留（PER）模块。实验证明了 EPEM 在 L-AVC 任务上的优越性。", "motivation": "现有视觉定制方法主要关注客观对齐，忽略了主观情感内容，并且缺乏通用的情感视觉定制基础模型。因此，需要一个专注于情感定制的新任务和相应的方法。", "method": "提出 L-AVC 任务，并设计了高效精确的情感操纵（EPEM）方法。EPEM 包含两个关键模块：1. 高效跨情感转换（EIC）模块，用于使 LLM 高效地对齐编辑前后的情感转换语义；2. 精确跨情感保留（PER）模块，用于精确保留非情感内容。", "result": "在构建的 L-AVC 数据集上的综合实验评估表明，所提出的 EPEM 方法在 L-AVC 任务上相对于现有基线方法具有显著优势。", "conclusion": "情感信息对于 L-AVC 至关重要，EPEM 方法在高效且精确地操纵情感信息方面是有效的。这项工作为情感视觉定制的研究开辟了新方向。"}}
{"id": "2602.18212", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18212", "abs": "https://arxiv.org/abs/2602.18212", "authors": ["Rui Chen", "Domenico Chiaradia", "Daniele Leonardis", "Antonio Frisoli"], "title": "Design and Characterization of a Dual-DOF Soft Shoulder Exosuit with Volume-Optimized Pneumatic Actuator", "comment": null, "summary": "Portable pneumatic systems for 2 degree-of-freedom (DOF) soft shoulder exosuits remain underexplored, and face fundamental trade-offs between torque output and dynamic response that are further compounded by the need for multiple actuators to support complex shoulder movement. This work addresses these constraints through a volume-optimized spindle-shaped angled actuator (SSAA) geometry: by reducing actuator volume by 35.7% (357mL vs. 555mL), the SSAA maintains 94.2% of output torque while achieving 35.2% faster dynamic response compared to uniform cylindrical designs. Building on the SSAA, we develop a curved abduction actuator (CAA) based on the SSAA geometry and a horizontal adduction actuator (HAA) based on the pouch motor principle, integrating both into a dual-DOF textile-based shoulder exosuit (390 g). The exosuit delivers multi-modal assistance spanning shoulder abduction, flexion, and horizontal adduction, depending on the actuation.\n  User studies with 10 healthy participants reveal that the exosuit substantially reduces electromyographic (EMG) activity across both shoulder abduction and flexion tasks. For abduction with HAA only, the exosuit achieved up to 59% muscle activity reduction across seven muscles. For flexion, both the single-actuator configuration (HAA only) and the dual-actuator configuration (HAA,+,CAA) reduced EMG activity by up to 63.7% compared to no assistance. However, the incremental benefit of adding the CAA to existing HAA support was limited in healthy users during flexion, with statistically significant additional reductions observed only in pectoralis major. These experimental findings characterize actuator contributions in healthy users and provide design guidance for multi-DOF exosuit systems.", "AI": {"tldr": "本研究提出了一种体积优化的纺锤形倾斜驱动器（SSAA）和曲面外展驱动器（CAA）及水平内收驱动器（HAA）集成到轻便的纺织肩部外骨骼中，在减小体积的同时提高了扭矩输出和动态响应，并在用户研究中验证了其显著减少肩部运动的肌肉活动。", "motivation": "现有的便携式气动肩部外骨骼在扭矩输出和动态响应之间存在权衡，且需要多个驱动器来支持复杂的肩部运动，这些问题仍未得到充分研究。本研究旨在克服这些限制，开发一种更优的肩部外骨骼驱动器。", "method": "研究人员设计了一种体积优化的纺锤形倾斜驱动器（SSAA），并基于其开发了曲面外展驱动器（CAA）和水平内收驱动器（HAA）。将这些驱动器集成到一个双自由度纺织肩部外骨骼中。通过招募10名健康参与者，测量了外骨骼在不同运动模式下的肌电图（EMG）活动，以评估其对肌肉活动的减免效果。", "result": "SSAA驱动器体积减小了35.7%，同时保持了94.2%的扭矩输出，并提高了35.2%的动态响应。集成驱动器的外骨骼在肩部外展、屈曲和水平内收运动中均能提供辅助。用户研究表明，该外骨骼能显著降低肩部外展和屈曲任务中的EMG活动，其中外展运动可达59%的减免，屈曲运动可达63.7%的减免。", "conclusion": "体积优化的SSAA设计能有效在减小驱动器体积的同时保持甚至提升性能。集成SSAA的肩部外骨骼能够显著减少健康用户在肩部运动中的肌肉活动，为多自由度外骨骼系统的设计提供了指导。虽然CAA在健康用户屈曲运动中的额外益处有限，但其在特定肌肉（如胸大肌）上的效果依然显著。"}}
{"id": "2602.18152", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.18152", "abs": "https://arxiv.org/abs/2602.18152", "authors": ["Ortal Hadad", "Edoardo Loru", "Jacopo Nudo", "Niccolò Di Marco", "Matteo Cinelli", "Walter Quattrociocchi"], "title": "The Statistical Signature of LLMs", "comment": null, "summary": "Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.", "AI": {"tldr": "本研究提出了一种基于无损压缩的测量方法，能够区分大型语言模型（LLM）生成的文本与人类书写的文本。研究发现，LLM生成的文本在受控和中介环境中具有更高的结构规律性和可压缩性，但在碎片化的交互环境中，这种差异会减弱。", "motivation": "现有研究对大型语言模型（LLM）生成文本如何改变语言的统计结构了解不深入。作者希望找到一种模型无关的方法来量化LLM生成文本的统计规律性。", "method": "研究使用无损压缩作为一种模型无关的测量工具，分析了三种不同信息生态系统中，人类-LLM协作生成、知识基础设施（Wikipedia vs. Grokipedia）中介生成以及完全合成的社交互动环境（Moltbook vs. Reddit）下文本的可压缩性。", "result": "在受控和中介环境中，LLM生成的文本比人类书写的文本具有更高的结构规律性和可压缩性。然而，在碎片化的交互环境中，这种差异会减弱，表明在小尺度上区分LLM文本存在根本性限制。这种基于可压缩性的区分在不同模型、任务和领域中都表现一致。", "conclusion": "无损压缩提供了一种简单有效的框架，用于量化生成系统如何重塑文本生成，并从结构层面理解通信的演变复杂性。该方法可以直接从文本表面观察，无需访问模型内部或进行语义评估。"}}
{"id": "2602.18224", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18224", "abs": "https://arxiv.org/abs/2602.18224", "authors": ["Yuankai Luo", "Woping Chen", "Tong Liang", "Baiqiao Wang", "Zhenguo Li"], "title": "SimVLA: A Simple VLA Baseline for Robotic Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a promising paradigm for general-purpose robotic manipulation, leveraging large-scale pre-training to achieve strong performance. The field has rapidly evolved with additional spatial priors and diverse architectural innovations. However, these advancements are often accompanied by varying training recipes and implementation details, which can make it challenging to disentangle the precise source of empirical gains. In this work, we introduce SimVLA, a streamlined baseline designed to establish a transparent reference point for VLA research. By strictly decoupling perception from control, using a standard vision-language backbone and a lightweight action head, and standardizing critical training dynamics, we demonstrate that a minimal design can achieve state-of-the-art performance. Despite having only 0.5B parameters, SimVLA outperforms multi-billion-parameter models on standard simulation benchmarks without robot pretraining. SimVLA also reaches on-par real-robot performance compared to pi0.5. Our results establish SimVLA as a robust, reproducible baseline that enables clear attribution of empirical gains to future architectural innovations. Website: https://frontierrobo.github.io/SimVLA", "AI": {"tldr": "本文提出SimVLA，一个简洁的Vision-Language-Action（VLA）模型基线，通过解耦感知与控制、使用标准骨干网络和轻量级动作头，并在标准化训练动态下，证明了即使是参数量较小的模型也能达到最先进的性能，并可作为未来VLA研究的透明参考点。", "motivation": "现有VLA模型在空间先验和架构创新方面发展迅速，但不同的训练方法和实现细节使得难以区分性能提升的真正原因。因此，需要一个透明的、可复现的基线模型来衡量未来研究的贡献。", "method": "SimVLA通过以下方式构建：1. 严格解耦感知模块和控制模块；2. 使用标准的视觉-语言骨干网络；3. 采用轻量级的动作头；4. 标准化关键的训练动态。这些设计旨在简化模型并提高可解释性。", "result": "SimVLA在标准模拟基准测试中，即使参数量仅为0.5B，也超越了数十亿参数的模型，且无需机器人预训练。在真实机器人上的表现与pi0.5相当。", "conclusion": "SimVLA是一个强大且可复现的基线模型，它证明了简洁设计也能实现最先进的性能，并为未来VLA研究中的经验性收益归因提供了清晰的参考。"}}
{"id": "2602.18094", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18094", "abs": "https://arxiv.org/abs/2602.18094", "authors": ["Ling Lin", "Yang Bai", "Heng Su", "Congcong Zhu", "Yaoxing Wang", "Yang Zhou", "Huazhu Fu", "Jingrun Chen"], "title": "OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models", "comment": "54 pages, 21 figures", "summary": "Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.", "AI": {"tldr": "该研究提出了OODBench，一个用于评估视觉语言模型（VLMs）在处理非独立同分布（OOD）数据时性能的基准测试，并引入了一种新的评估指标，揭示了当前VLMs在OOD数据上的性能下降问题。", "motivation": "现有视觉语言模型（VLMs）在处理独立同分布（IID）数据上表现良好，但在实际应用中，数据往往不满足IID假设。未能妥善处理OOD数据可能带来安全风险，但目前缺乏有效的基准来全面评估VLMs在OOD数据上的表现。", "method": "提出了一种名为OODBench的自动化方法，用于构建新的基准测试。OODBench包含40K个实例级OOD实例-类别对。同时，提出了一种可靠的自动化评估指标，采用“基础到进阶”的提示问题序列来评估OOD数据对不同难度问题的潜在影响。", "result": "在OODBench上，当前VLMs表现出明显的性能下降，即使在图像类别常见的情况下也是如此。新的评估指标能够更全面地评估OOD数据对不同难度问题的具体影响。", "conclusion": "研究强调了VLMs在处理OOD数据时面临的挑战，并为未来在OOD数据获取和评估方面的研究提供了重要的发现和见解。"}}
{"id": "2602.18260", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18260", "abs": "https://arxiv.org/abs/2602.18260", "authors": ["Magnus Norén", "Marios-Nektarios Stamatopoulos", "Avijit Banerjee", "George Nikolakopoulos"], "title": "Role-Adaptive Collaborative Formation Planning for Team of Quadruped Robots in Cluttered Environments", "comment": null, "summary": "This paper presents a role-adaptive Leader-Follower-based formation planning and control framework for teams of quadruped robots operating in cluttered environments. Unlike conventional methods with fixed leaders or rigid formation roles, the proposed approach integrates dynamic role assignment and partial goal planning, enabling flexible, collision-free navigation in complex scenarios. Formation stability and inter-robot safety are ensured through a virtual spring-damper system coupled with a novel obstacle avoidance layer that adaptively adjusts each agent's velocity. A dynamic look-ahead reference generator further enhances flexibility, allowing temporary formation deformation to maneuver around obstacles while maintaining goal-directed motion. The Fast Marching Square (FM2) algorithm provides the global path for the leader and local paths for the followers as the planning backbone. The framework is validated through extensive simulations and real-world experiments with teams of quadruped robots. Results demonstrate smooth coordination, adaptive role switching, and robust formation maintenance in complex, unstructured environments. A video featuring the simulation and physical experiments along with their associated visualizations can be found at https://youtu.be/scq37Tua9W4.", "AI": {"tldr": "该论文提出了一种用于四足机器人团队在拥挤环境中进行自适应角色分配和局部路径规划的编队控制框架。", "motivation": "现有方法在固定角色或僵化编队方面存在局限，研究动机是实现更灵活、可碰撞的复杂环境下的编队导航。", "method": "该框架结合了动态角色分配、局部目标规划、虚拟弹簧-阻尼系统、新的避障层以及动态前视参考生成器。全局路径和局部路径规划使用 Fast Marching Square (FM2) 算法。", "result": "仿真和真实实验表明，该框架能够实现平滑协调、自适应角色切换和在复杂非结构化环境中的鲁棒编队保持。", "conclusion": "该论文成功实现了一个能够适应动态角色分配和应对复杂环境的四足机器人编队规划与控制框架，并通过实验验证了其有效性。"}}
{"id": "2602.18154", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18154", "abs": "https://arxiv.org/abs/2602.18154", "authors": ["Mirae Kim", "Seonghun Jeong", "Youngjun Kwak"], "title": "FENCE: A Financial and Multimodal Jailbreak Detection Dataset", "comment": "lrec 2026 accepted paper", "summary": "Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.", "AI": {"tldr": "本研究提出了FENCE，一个用于金融领域多模态大模型（VLMs）越狱检测的双语数据集，并评估了当前VLMs的越狱漏洞，同时展示了使用FENCE训练的检测器的高准确性。", "motivation": "现有的越狱检测资源，特别是在金融领域，非常有限。而VLMs由于同时处理文本和图像，面临更大的越狱风险。", "method": "构建了一个包含金融相关查询和图像威胁的双语（韩语-英语）多模态数据集FENCE，用于训练和评估越狱检测器。对商业和开源VLMs进行了越狱实验，并使用FENCE训练了一个基线检测器。", "result": "实验发现GPT-4o存在可测量的越狱成功率，开源模型则更容易受到攻击。在FENCE上训练的基线检测器在模型内部数据集上达到了99%的准确率，并在外部基准测试中表现强劲。", "conclusion": "FENCE数据集为多模态金融领域越狱检测提供了一个宝贵的资源，有助于训练更可靠的检测模型，从而促进金融等敏感领域AI系统的安全性。同时，研究表明当前VLMs存在越狱漏洞，需要进一步的研究和防御措施。"}}
{"id": "2602.18171", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18171", "abs": "https://arxiv.org/abs/2602.18171", "authors": ["Wojciech Michaluk", "Tymoteusz Urban", "Mateusz Kubita", "Soveatin Kuntur", "Anna Wroblewska"], "title": "Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models", "comment": null, "summary": "Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.", "AI": {"tldr": "研究提出了一种结合Transformer文本嵌入和语言学信息丰富度特征的混合方法来检测网络钓鱼标题，在XGBoost模型上取得了91%的F1分数，优于多种基线方法，并增强了模型的可解释性。", "motivation": "网络钓鱼标题会降低在线信息的质量并损害用户信任，因此需要一种有效的检测方法。", "method": "采用混合方法，结合了基于Transformer的文本嵌入和15个语言学信息丰富度特征（如第二人称代词、最高级、数字、标点符号等）。模型使用了XGBoost分类器，并与TF-IDF、Word2Vec、GloVe、基于LLM提示的分类以及仅特征的方法进行了对比。", "result": "最佳模型（XGBoost在增强嵌入上）取得了91%的F1分数，显著优于所有基线方法。所提出的特征集能突出显示诸如第二人称代词、最高级、数字和关注导向的标点符号等关键语言线索。", "conclusion": "通过结合Transformer嵌入和精心设计的语言学特征，可以实现高精度且可解释的网络钓鱼标题检测。研究发布了代码和训练模型以支持可复现的研究。"}}
{"id": "2602.18258", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18258", "abs": "https://arxiv.org/abs/2602.18258", "authors": ["Gwangtak Bae", "Jaeho Shin", "Seunggu Kang", "Junho Kim", "Ayoung Kim", "Young Min Kim"], "title": "RoEL: Robust Event-based 3D Line Reconstruction", "comment": "IEEE Transactions on Robotics (T-RO)", "summary": "Event cameras in motion tend to detect object boundaries or texture edges, which produce lines of brightness changes, especially in man-made environments. While lines can constitute a robust intermediate representation that is consistently observed, the sparse nature of lines may lead to drastic deterioration with minor estimation errors. Only a few previous works, often accompanied by additional sensors, utilize lines to compensate for the severe domain discrepancies of event sensors along with unpredictable noise characteristics. We propose a method that can stably extract tracks of varying appearances of lines using a clever algorithmic process that observes multiple representations from various time slices of events, compensating for potential adversaries within the event data. We then propose geometric cost functions that can refine the 3D line maps and camera poses, eliminating projective distortions and depth ambiguities. The 3D line maps are highly compact and can be equipped with our proposed cost function, which can be adapted for any observations that can detect and extract line structures or projections of them, including 3D point cloud maps or image observations. We demonstrate that our formulation is powerful enough to exhibit a significant performance boost in event-based mapping and pose refinement across diverse datasets, and can be flexibly applied to multimodal scenarios. Our results confirm that the proposed line-based formulation is a robust and effective approach for the practical deployment of event-based perceptual modules. Project page: https://gwangtak.github.io/roel/", "AI": {"tldr": "提出了一种利用事件相机提取和优化3D线条地图和相机姿态的方法，以解决事件相机在动态环境下的边缘检测稀疏性和域差异问题。", "motivation": "事件相机在运动中容易检测到物体边界和纹理边缘产生的亮度变化线条。然而，这些线条的稀疏性可能导致估计错误。现有的利用线条的研究通常需要额外的传感器，且在处理事件数据的域差异和不可预测的噪声方面存在不足。", "method": "该方法通过观察事件数据的不同时间切片，利用算法稳定地提取具有不同外观的线条轨迹，以补偿潜在的对抗性事件数据。接着，提出几何成本函数来优化3D线条地图和相机姿态，消除投影失真和深度模糊。该3D线条地图紧凑且易于适应其他可检测线条结构或其投影的观察方式（如3D点云或图像）。", "result": "该方法在事件相机测绘和姿态优化方面取得了显著的性能提升，并在多样化的数据集上进行了验证。它还能灵活应用于多模态场景。", "conclusion": "提出的基于线条的算法是一种稳健有效的方法，适用于事件相机感知模块的实际部署。"}}
{"id": "2602.18176", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18176", "abs": "https://arxiv.org/abs/2602.18176", "authors": ["Kaisen Yang", "Jayden Teoh", "Kaicheng Yang", "Yitong Zhang", "Alex Lamb"], "title": "Improving Sampling for Masked Diffusion Models via Information Gain", "comment": "https://github.com/yks23/Information-Gain-Sampler", "summary": "Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.", "AI": {"tldr": "本文提出了一种名为Info-Gain Sampler的新型解码框架，用于解决掩码扩散模型（MDMs）在生成质量上的局限性。与现有贪婪采样器不同，Info-Gain Sampler考虑了当前解码选择对后续步骤的下游影响，并通过最大化信息增益来减少累积不确定性，从而在多种任务和架构上取得了显著的性能提升。", "motivation": "现有MDMs的解码方法（通常采用贪婪启发式）未能充分利用MDMs的非因果性，忽略了当前解码选择对后续步骤的潜在影响，且未能有效最小化累积不确定性，导致生成质量受限。", "method": "提出Info-Gain Sampler解码框架，该框架通过平衡当前解码的不确定性与对未来掩码标记的信息增益来实现。它利用MDMs的非因果性，评估解码决策如何影响所有剩余掩码位置的标记概率/不确定性。", "result": "Info-Gain Sampler在推理、编码、创意写作和图像生成等多种任务和架构上，均显著优于现有采样器。具体而言，在推理任务上平均准确率提高了3.6%，在创意写作上获胜率达到63.1%，推理任务上的累积不确定性从78.4大幅降低至48.6。", "conclusion": "Info-Gain Sampler是一种原则性的解码框架，能够有效提升MDMs的生成质量，克服了现有方法在平衡局部确定性和全局信息增益方面的不足。该方法在多项任务上展现出优越性能，并减少了累积不确定性。"}}
{"id": "2602.18043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18043", "abs": "https://arxiv.org/abs/2602.18043", "authors": ["Hongyu Qu", "Xiangbo Shu", "Rui Yan", "Hailiang Gao", "Wenguan Wang", "Jinhui Tang"], "title": "Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition", "comment": "Accepted to TPAMI 2026", "summary": "Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.", "AI": {"tldr": "本文提出了DiST框架，通过分解动作名称得到时空属性描述，并利用这些描述来学习多粒度的原型，以解决少样本动作识别（FSAR）的挑战。", "motivation": "现有FSAR方法依赖于粗粒度的动作名称作为辅助上下文，但其提供的背景知识不足以捕捉新颖动作的空间和时间概念。", "method": "DiST框架包含两个阶段：1. 分解阶段：将动作名称分解为多样化的时空属性描述（动作相关知识）。2. 整合阶段：提出空间/时间知识补偿器（SKC/TKC），SKC在空间知识指导下聚合重要补丁令牌以发现对象级原型，TKC利用时间属性辅助建模帧间时间关系以发现帧级原型。", "result": "DiST在五个标准的FSAR数据集上取得了最先进的性能。", "conclusion": "DiST框架能够有效利用分解得到的时空知识，学习具有区分性的多粒度原型，从而提升FSAR的性能，并在细粒度的空间细节和多样的时序模式捕捉方面表现出优势。"}}
{"id": "2602.18020", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18020", "abs": "https://arxiv.org/abs/2602.18020", "authors": ["Jiabing Yang", "Yixiang Chen", "Yuan Xu", "Peiyan Li", "Xiangnan Wu", "Zichen Wen", "Bowen Fang", "Tao Yu", "Zhengbo Zhang", "Yingda Li", "Kai Wang", "Jing Liu", "Nianfeng Liu", "Yan Huang", "Liang Wang"], "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.", "AI": {"tldr": "提出了一种名为UAOR（Uncertainty-aware Observation Reinjection）的训练无关、即插即用的模块，用于增强视觉-语言-动作（VLA）模型。该模块通过在语言模型层出现高不确定性时，将关键的观察信息通过注意力检索重新注入到下一层的前馈网络（FFN）中，从而提高VLA模型的动作生成能力。实验证明，UAOR能够有效提升现有VLA模型在模拟和真实世界任务中的性能，且无需额外的观察线索或模块。", "motivation": "现有VLA模型为了提高性能，通常需要额外的观察线索或辅助模块，这会增加数据收集和训练成本。作者受到语言模型FFN中“键值记忆”的启发，希望提出一种更有效、训练免费且易于集成的解决方案来增强VLA模型。", "method": "提出不确定性感知观察重注入（UAOR）模块。当当前语言模型层因动作熵（Action Entropy）而表现出高不确定性时，UAOR通过注意力检索机制将关键的观察信息重新注入到下一层的前馈网络（FFN）中。这使得VLA模型在推理时能更好地关注观察信息，从而生成更自信、更准确的动作。", "result": "UAOR模块在模拟和真实世界的多种VLA模型和任务上展现出一致的性能提升，并且开销极小。该方法有效地提高了VLA模型的泛化能力和动作生成质量。", "conclusion": "UAOR是一个有效的、训练免费的、即插即用的模块，可以显著提升VLA模型的性能，特别是其动作生成能力。它通过在模型不确定性高时注入关键观察信息来工作，无需额外的观察线索或模块，具有广泛的适用性和实用性。"}}
{"id": "2602.18312", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2602.18312", "abs": "https://arxiv.org/abs/2602.18312", "authors": ["Zhaoming Xie", "Kevin Karol", "Jessica Hodgins"], "title": "Learning Smooth Time-Varying Linear Policies with an Action Jacobian Penalty", "comment": null, "summary": "Reinforcement learning provides a framework for learning control policies that can reproduce diverse motions for simulated characters. However, such policies often exploit unnatural high-frequency signals that are unachievable by humans or physical robots, making them poor representations of real-world behaviors. Existing work addresses this issue by adding a reward term that penalizes a large change in actions over time. This term often requires substantial tuning efforts. We propose to use the action Jacobian penalty, which penalizes changes in action with respect to the changes in simulated state directly through auto differentiation. This effectively eliminates unrealistic high-frequency control signals without task specific tuning. While effective, the action Jacobian penalty introduces significant computational overhead when used with traditional fully connected neural network architectures. To mitigate this, we introduce a new architecture called a Linear Policy Net (LPN) that significantly reduces the computational burden for calculating the action Jacobian penalty during training. In addition, a LPN requires no parameter tuning, exhibits faster learning convergence compared to baseline methods, and can be more efficiently queried during inference time compared to a fully connected neural network. We demonstrate that a Linear Policy Net, combined with the action Jacobian penalty, is able to learn policies that generate smooth signals while solving a number of motion imitation tasks with different characteristics, including dynamic motions such as a backflip and various challenging parkour skills. Finally, we apply this approach to create policies for dynamic motions on a physical quadrupedal robot equipped with an arm.", "AI": {"tldr": "提出了一种名为线性策略网络（LPN）的新架构，结合动作雅可比惩罚，可以生成平滑且逼真的运动控制策略，有效解决了现有方法在高频动作信号上的不足，并成功应用于模拟和物理机器人。", "motivation": "现有的强化学习控制策略常常产生不自然的高频动作信号，不适用于人类或物理机器人，且现有解决方案（如动作变化惩罚）需要大量调优。", "method": "引入动作雅可比惩罚，通过自动微分直接惩罚动作相对于状态的变化，消除高频信号。为解决计算开销问题，提出线性策略网络（LPN）架构，降低计算复杂度，无需参数调优，并提高学习效率和推理速度。", "result": "LPN结合动作雅可比惩罚能够学习生成平滑动作信号的策略，成功解决了多种运动模仿任务，包括翻转和跑酷等动态运动。在物理四足机器人上也验证了该方法的有效性。", "conclusion": "线性策略网络（LPN）结合动作雅可比惩罚是一种有效且无需调优的方法，可以生成逼真、平滑的控制策略，适用于模拟和物理机器人上的复杂动态运动任务。"}}
{"id": "2602.18232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18232", "abs": "https://arxiv.org/abs/2602.18232", "authors": ["Lexiang Tang", "Weihao Gao", "Bingchen Zhao", "Lu Ma", "Qiao jin", "Bang Yang", "Yuexian Zou"], "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning", "comment": null, "summary": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.", "AI": {"tldr": "本文提出了一种名为“自信驱动的对比解码”（CCD）的方法，通过识别和干预低置信度的 tokens 来提高大型语言模型（LLM）的推理准确性，同时减少输出长度，且不增加额外的计算开销。", "motivation": "现有的测试时扩展方法通常假设均匀分配计算资源可以提高 LLM 的推理正确性。然而，研究表明推理不确定性高度局部化，少数低置信度的 tokens 是推理错误和输出冗余的主要来源。因此，研究者受到这一观察的启发，旨在提出一种更具针对性的干预方法。", "method": "该方法是一种自信驱动的对比解码技术。它在解码过程中检测低置信度的 tokens，并在这些位置进行选择性干预。具体来说，它通过将高置信度的 tokens 替换为最小的占位符来构建一个对比参考，然后在低置信度位置通过减去这个参考分布来精炼预测。", "result": "实验结果表明，CCD 在数学推理基准测试中显著提高了准确性，同时大幅减少了输出长度，并且 KV 缓存开销极小。作为一种无需训练的方法，CCD 通过针对性的低置信度干预提高了推理可靠性，避免了计算冗余。", "conclusion": "自信驱动的对比解码（CCD）是一种有效的、无需训练的 LLM 推理可靠性提升方法，它通过识别和干预低置信度 tokens 来提高准确性并减少不必要的输出，同时保持了计算效率。"}}
{"id": "2602.18047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18047", "abs": "https://arxiv.org/abs/2602.18047", "authors": ["Rong Fu", "Wenxin Zhang", "Yibo Meng", "Jia Yee Tan", "Jiaxuan Lu", "Rui Lu", "Jiekai Wu", "Zhaolu Kang", "Simon Fong"], "title": "CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras", "comment": "36 pages, 12 figures", "summary": "City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.", "AI": {"tldr": "CityGuard是一个为去中心化监控设计的拓扑感知Transformer，用于隐私保护的身份检索，能在不共享原始图像的情况下处理严重的视点、遮挡和领域变化，并实现了隐私与效用的可调平衡。", "motivation": "在城市规模的跨摄像头身份重识别中，需要处理严格的隐私规定（禁止共享原始图像）以及由视点、遮挡和领域迁移带来的外观变化。现有的方法难以在满足隐私要求的同时有效地进行身份匹配。", "method": "CityGuard包含三个组件：1) 离散自适应度量学习器，根据特征扩散调整实例级边距以提高类内紧凑性；2) 空间条件注意力，将GPS或楼层平面等粗略几何信息注入基于图的自注意力中，以实现仅凭粗略几何先验即可进行投影一致性的跨视图对齐；3) 差分隐私嵌入映射与紧凑近似索引相结合，以支持安全且成本效益高的部署。", "result": "实验在Market-1501及其他公开基准测试中显示，CityGuard在检索精度和查询吞吐量上均优于强基线方法。该框架能够实现隐私和效用之间可调的平衡，并且支持数据库规模的检索。", "conclusion": "CityGuard是一个实用的隐私保护身份匹配框架，能够鲁棒地处理视点变化、遮挡和领域迁移，并能在保护隐私的前提下实现高效的城市身份检索。"}}
{"id": "2602.18262", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18262", "abs": "https://arxiv.org/abs/2602.18262", "authors": ["Aaron Louis Eidt", "Nils Feldhus"], "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA", "comment": "EACL 2026 System Demonstrations. GitHub: https://github.com/aaron0eidt/ELIA", "summary": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.", "AI": {"tldr": "ELIA 是一个交互式 Web 应用，它利用 AI 生成的自然语言解释来简化 LLM 的内部工作分析，使其对非专家更易于理解，并通过用户研究验证了其有效性。", "motivation": "现有用于分析 LLM 内部机制的工具过于复杂，难以被非专家使用，这造成了可访问性差距。", "method": "ELIA 集成了三种分析技术（归因分析、函数向量分析、电路追踪），并引入了一种新方法：使用视觉-语言模型自动生成自然语言解释（NLEs）来解释可视化结果。通过混合方法用户研究来评估其有效性。", "result": "用户研究表明，交互式、可探索的界面比静态可视化更受欢迎。AI 生成的解释有助于缩小知识差距，用户先前的 LLM 经验与理解分数之间没有显著相关性。", "conclusion": "AI 系统可以简化复杂的模型分析，但要发挥其最大作用，还需要结合以用户为中心的设计，注重交互性、具体性和叙事引导。"}}
{"id": "2602.18330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18330", "abs": "https://arxiv.org/abs/2602.18330", "authors": ["Mohsen Jafarpour", "Ayberk Yüksek", "Shahab Eshghi", "Stanislav Gorb", "Edoardo Milana"], "title": "Tendon-Driven Reciprocating and Non-Reciprocating Motion via Snapping Metabeams", "comment": "9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "Snapping beams enable rapid geometric transitions through nonlinear instability, offering an efficient means of generating motion in soft robotic systems. In this study, a tendon-driven mechanism consisting of spiral-based metabeams was developed to exploit this principle for producing both reciprocating and non-reciprocating motion. The snapping structures were fabricated using fused deposition modeling with polylactic acid (PLA) and experimentally tested under different boundary conditions to analyze their nonlinear behavior. The results show that the mechanical characteristics, including critical forces and stability, can be tuned solely by adjusting the boundary constraints. The spiral geometry allows large reversible deformation even when made from a relatively stiff material such as PLA, providing a straightforward design concept for controllable snapping behavior. The developed mechanism was further integrated into a swimming robot, where tendon-driven fins exhibited two distinct actuation modes: reciprocating and non-reciprocating motion. The latter enabled efficient propulsion, producing a forward displacement of about 32 mm per 0.4 s cycle ($\\approx$ 81 mm/s, equivalent to 0.4 body lengths per second). This study highlights the potential of geometry-driven snapping structures for efficient and programmable actuation in soft robotic systems.", "AI": {"tldr": "研究开发了一种基于螺旋状超材料梁的肌腱驱动机制，利用非线性失稳实现快速几何转变，产生往复和非往复运动，并成功应用于游动机器人，展现了其在软体机器人系统中高效可编程驱动的潜力。", "motivation": "利用“弹跳梁”的非线性失稳特性，高效地在软体机器人系统中产生运动。", "method": "设计并制造了基于螺旋状超材料梁的肌腱驱动结构，使用熔融沉积建模（FDM）打印PLA材料。通过改变边界条件来测试其非线性行为，并将该机制集成到游动机器人中。", "result": "通过调整边界约束可以调控弹跳结构的力学特性（如临界力、稳定性）。螺旋几何结构允许大形变，并且该机制在游动机器人中实现了两种驱动模式（往复和非往复）。非往复模式下，机器人能以约32毫米/0.4秒（约81毫米/秒）的速度前进，相当于0.4身体长度/秒。", "conclusion": "基于几何驱动的弹跳结构在软体机器人系统中具有高效、可编程驱动的潜力，通过调整几何形状和边界条件即可实现可控的弹跳行为。"}}
{"id": "2602.18057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18057", "abs": "https://arxiv.org/abs/2602.18057", "authors": ["Hongsong Wang", "Wenjing Yan", "Qiuxia Lai", "Xin Geng"], "title": "Temporal Consistency-Aware Text-to-Motion Generation", "comment": "Code is on https://github.com/Giat995/TCA-T2M/", "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.", "AI": {"tldr": "提出了一种名为TCA-T2M的文本到动作生成框架，通过引入时域一致性感知空间VQ-VAE来解决现有方法在跨序列时域一致性上的不足，提高了动作生成质量和物理合理性。", "motivation": "现有两阶段文本到动作生成方法在处理同一动作的不同实例时，常常忽略了它们之间存在的共享时域结构（即时域一致性），导致语义对齐错误和物理上不合理的运动。", "method": "提出TCA-T2M框架，包括：1. 时域一致性感知空间VQ-VAE（TCaS-VQ-VAE）用于跨序列时域对齐；2. 掩码运动Transformer用于文本条件下的运动生成；3. 运动学约束模块以消除离散化伪影，确保物理合理性。", "result": "在HumanML3D和KIT-ML数据集上，TCA-T2M实现了最先进的性能，表明时域一致性对于鲁棒且连贯的文本到动作生成至关重要。", "conclusion": "时域一致性是文本到动作生成任务中实现鲁棒性和连贯性的关键因素，TCA-T2M通过引入TCaS-VQ-VAE和运动学约束等技术，有效地解决了现有方法的局限性，取得了优异的生成效果。"}}
{"id": "2602.18252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18252", "abs": "https://arxiv.org/abs/2602.18252", "authors": ["Rishika Bhagwatkar", "Irina Rish", "Nicolas Flammarion", "Francesco Croce"], "title": "On the Adversarial Robustness of Discrete Image Tokenizers", "comment": null, "summary": "Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.", "AI": {"tldr": "本文首次探讨了离散图像分词器对对抗性攻击的脆弱性，并提出了一种无监督的对抗性训练方法来提高其鲁棒性，同时保持其他组件冻结。", "motivation": "现有研究未探索过离散图像分词器（常用于多模态系统）的对抗性攻击脆弱性，因此需要填补这一空白并提出防御措施。", "method": "首先，提出了旨在扰乱分词器提取特征并改变生成Token的攻击方法。其次，采用受鲁棒CLIP编码器启发的无监督对抗性训练方法，仅对分词器进行微调，其他组件保持冻结。", "result": "提出的攻击方法在分类、多模态检索和图像描述等任务中均有效且计算效率高。无监督对抗性训练方法显著提高了分词器对无监督和端到端监督攻击的鲁棒性，并能泛化到未见过的任务和数据。", "conclusion": "本文的研究强调了分词器鲁棒性在下游任务中的关键作用，并为构建安全的多模态基础模型迈出了重要一步。提出的无监督对抗性训练方法比监督方法更灵活，因为它可以使用未标记的图像。"}}
{"id": "2602.18217", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18217", "abs": "https://arxiv.org/abs/2602.18217", "authors": ["Kohei Kajikawa", "Shinnosuke Isono", "Ethan Gotlieb Wilcox"], "title": "Information-Theoretic Storage Cost in Sentence Comprehension", "comment": null, "summary": "Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.", "AI": {"tldr": "本研究提出了一种基于信息论的实时句子理解工作记忆负荷度量方法，该方法利用预训练的神经网络语言模型来估计信息量，并在三个实验中验证了其有效性。", "motivation": "现有的工作记忆负荷度量方法主要基于符号语法，存在离散、成本统一等局限性。本研究旨在提出一种更灵活、更具理论中立性的度量方法。", "method": "本研究提出了一种基于信息论的度量方法，将工作记忆负荷定义为先前词语在不确定性下携带的关于未来上下文的信息量。该方法可以从预训练的神经网络语言模型中估计，并且是连续的、理论中立的。", "result": "1. 该度量方法能够反映中心嵌入和关系从句的已知处理不对称性。2. 该度量方法与语法驱动的存储成本在句法标注语料库中存在相关性。3. 在两个大规模自然语言数据集上，该度量方法能够预测超出传统基于信息预测因子的阅读时间方差。", "conclusion": "基于信息论和神经网络语言模型的度量方法是一种有效且有前景的实时句子理解工作记忆负荷的衡量工具，能够捕捉到句法结构对阅读处理的影响。"}}
{"id": "2602.18346", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18346", "abs": "https://arxiv.org/abs/2602.18346", "authors": ["Pavithra PM Nair", "Preethu Rose Anish"], "title": "Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System", "comment": null, "summary": "In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.", "AI": {"tldr": "本文提出Vichara框架，用于预测和解释印度上诉法院判决，该框架能将判决分解为决策点，并生成符合IRAC框架的解释，在两个数据集上均优于现有基准，GPT-4o mini表现最佳。", "motivation": "印度法院案件积压严重，法律判决预测（LJP）具有巨大潜力。上诉案件是积压案件的重要组成部分，因此需要一个能预测和解释上诉判决的工具。", "method": "提出Vichara框架，将英文上诉案件程序文件分解为决策点（包含法律问题、裁决机构、结果、理由和时间背景），并以IRAC为基础生成结构化解释。使用GPT-4o mini, Llama-3.1-8B, Mistral-7B, 和 Qwen2.5-7B 四个大语言模型进行评估。", "result": "Vichara在PredEx和ILDC_expert两个数据集上均超越现有判决预测基准。GPT-4o mini取得了最高性能（F1值：PredEx上81.5%，ILDC_expert上80.3%），Llama-3.1-8B次之。人类评估显示GPT-4o mini生成的解释在清晰度、关联性和有用性方面具有更好的可解释性。", "conclusion": "Vichara框架能够准确预测和生成具有可解释性的印度上诉法院判决，为解决印度司法积压问题提供了有前景的AI解决方案，并表明大型语言模型在法律判决预测和解释方面具有巨大潜力。"}}
{"id": "2602.18344", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18344", "abs": "https://arxiv.org/abs/2602.18344", "authors": ["Mengguang Li", "Heinz Koeppl"], "title": "Downwash-aware Configuration Optimization for Modular Aerial Systems", "comment": "Accepted to the IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "This work proposes a framework that generates and optimally selects task-specific assembly configurations for a large group of homogeneous modular aerial systems, explicitly enforcing bounds on inter-module downwash. Prior work largely focuses on planar layouts and often ignores aerodynamic interference. In contrast, firstly we enumerate non-isomorphic connection topologies at scale; secondly, we solve a nonlinear program to check feasibility and select the configuration that minimizes control input subject to actuation limits and downwash constraints. We evaluate the framework in physics-based simulation and demonstrate it in real-world experiments.", "AI": {"tldr": "提出一个框架，用于生成和优化多无人机集群的任务特定组装配置，同时考虑了模块间的下洗效应。", "motivation": "现有研究主要集中在平面布局且忽视了气动干扰，本研究旨在解决这些局限性，开发一种能处理大规模同质模块化航空系统组装配置并满足下洗约束的框架。", "method": "首先，枚举大规模的非同构连接拓扑；其次，通过非线性规划求解器，在满足驱动限制和下洗约束的前提下，选择能最小化控制输入的配置。", "result": "在物理仿真和真实世界实验中都验证了该框架的有效性。", "conclusion": "该框架能够有效地生成和优化满足气动约束的无人机集群组装配置，并在仿真和实验中得到验证。"}}
{"id": "2602.18064", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18064", "abs": "https://arxiv.org/abs/2602.18064", "authors": ["Ziyue Wang", "Linghan Cai", "Chang Han Low", "Haofeng Liu", "Junde Wu", "Jingyu Wang", "Rui Wang", "Lei Song", "Jiang Bian", "Jingjing Fu", "Yueming Jin"], "title": "3DMedAgent: Unified Perception-to-Understanding for 3D Medical Analysis", "comment": "19 pages, 7 figures", "summary": "3D CT analysis spans a continuum from low-level perception to high-level clinical understanding. Existing 3D-oriented analysis methods adopt either isolated task-specific modeling or task-agnostic end-to-end paradigms to produce one-hop outputs, impeding the systematic accumulation of perceptual evidence for downstream reasoning. In parallel, recent multimodal large language models (MLLMs) exhibit improved visual perception and can integrate visual and textual information effectively, yet their predominantly 2D-oriented designs fundamentally limit their ability to perceive and analyze volumetric medical data. To bridge this gap, we propose 3DMedAgent, a unified agent that enables 2D MLLMs to perform general 3D CT analysis without 3D-specific fine-tuning. 3DMedAgent coordinates heterogeneous visual and textual tools through a flexible MLLM agent, progressively decomposing complex 3D analysis into tractable subtasks that transition from global to regional views, from 3D volumes to informative 2D slices, and from visual evidence to structured textual representations. Central to this design, 3DMedAgent maintains a long-term structured memory that aggregates intermediate tool outputs and supports query-adaptive, evidence-driven multi-step reasoning. We further introduce the DeepChestVQA benchmark for evaluating unified perception-to-understanding capabilities in 3D thoracic imaging. Experiments across over 40 tasks demonstrate that 3DMedAgent consistently outperforms general, medical, and 3D-specific MLLMs, highlighting a scalable path toward general-purpose 3D clinical assistants.Code and data are available at \\href{https://github.com/jinlab-imvr/3DMedAgent}{https://github.com/jinlab-imvr/3DMedAgent}.", "AI": {"tldr": "提出了一种名为 3DMedAgent 的统一代理，它使 2D 多模态大语言模型 (MLLMs) 能够进行通用的 3D CT 分析，而无需进行 3D 特定微调。该方法通过协调各种工具，将复杂的 3D 分析分解为可管理的子任务，并利用长期结构化记忆来整合中间结果，从而实现了循证的多步推理。", "motivation": "现有的 3D CT 分析方法要么是孤立的、任务特定的模型，要么是任务无关的端到端模型，它们只能产生一次性输出，阻碍了感知证据的系统累积以支持下游推理。同时，现有的 MLLMs 主要面向 2D，限制了它们感知和分析 3D 医学数据的能力。因此，需要一种方法来弥合这一差距。", "method": "提出 3DMedAgent，一个统一的代理，它通过一个灵活的 MLLM 代理协调异构的视觉和文本工具。该代理逐步将复杂的 3D 分析分解为可管理的子任务，从全局到区域视图，从 3D 体积到信息丰富的 2D 切片，以及从视觉证据到结构化文本表示。该设计的一个核心是 3DMedAgent 维护一个长期的结构化记忆，该记忆聚合中间工具输出并支持查询自适应、基于证据的多步推理。", "result": "在超过 40 项任务上的实验表明，3DMedAgent 的性能始终优于通用、医学和 3D 特定 MLLMs。引入了 DeepChestVQA 基准测试，用于评估 3D 胸部成像中的统一感知到理解能力。", "conclusion": "3DMedAgent 提供了一种可扩展的途径，可以实现通用的 3D 临床助手，使 2D MLLMs 能够进行 3D CT 分析，而无需 3D 特定微调，并且能够有效地整合工具和利用长期记忆进行循证推理。"}}
{"id": "2602.18324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18324", "abs": "https://arxiv.org/abs/2602.18324", "authors": ["Alexandra Ciobotaru", "Ana-Maria Bucur", "Liviu P. Dinu"], "title": "PsihoRo: Depression and Anxiety Romanian Text Corpus", "comment": "This article was accepted at LREC 2026", "summary": "Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.", "AI": {"tldr": "本研究创建了首个用于罗马尼亚语的抑郁和焦虑心理健康语料库PsihoRo，通过开放式问答和标准化筛查问卷收集数据，并使用统计分析、文本分析（LIWC）、情感检测和主题建模来分析其特征。", "motivation": "现有的心理学NLP资源主要集中在英语，而罗马尼亚语缺乏相关的开放源代码语料库，特别是在心理健康领域，这阻碍了对罗马尼亚人口心理健康的NLP研究。", "method": "通过包含6个开放式问题和PHQ-9、GAD-7筛查问卷的收集表，收集了205名受访者的文本数据。随后，采用统计分析、罗马尼亚语LIWC文本分析、情感检测和主题建模等方法来分析语料库的特征。", "result": "创建了罗马尼亚语的PsihoRo语料库，包含205名受访者的文本数据。通过分析，揭示了该语料库在NLP社区中的重要特征，包括最相关的词汇、情感表达和主题。", "conclusion": "PsihoRo语料库的创建是罗马尼亚语心理健康NLP领域的一个重要开端，为理解和分析罗马尼亚人口的心理健康文本提供了基础资源，并展示了其在统计分析、文本分析、情感检测和主题建模方面的潜力。"}}
{"id": "2602.18066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18066", "abs": "https://arxiv.org/abs/2602.18066", "authors": ["Daniel Busch", "Christian Bohn", "Thomas Kurbiel", "Klaus Friedrichs", "Richard Meyes", "Tobias Meisen"], "title": "Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation", "comment": "This Paper has been accepted to the 2026 IEEE Intelligent Vehicles Symposium (IV)", "summary": "Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.", "AI": {"tldr": "该研究提出了一种新的两阶段训练策略，用于在鸟瞰图（BEV）中进行细粒度的道路标记分割，该策略在预训练阶段无需完全监督，并在微调阶段减少一半的训练数据，同时性能优于全监督基线模型。", "motivation": "当前自动驾驶领域中，稠密的BEV语义地图的生成依赖于昂贵且标注不一致的BEV真实标签。本研究旨在解决这一限制，提出一种无需完全监督预训练且能减少标注数据量的方法。", "method": "该方法采用两阶段训练策略：1. 自监督预训练：将BEVFormer的预测通过可微分重投影到图像平面，并与由Mask2Former生成的伪标签进行训练，同时引入时间损失以鼓励跨帧一致性。2. 有监督微调：仅使用50%的训练数据进行微调。", "result": "在nuScenes数据集上，该方法在微调阶段的性能（mIoU提升高达+2.5pp）和BEV分割质量优于全监督基线模型。同时，该方法将标注数据使用量减半，并将总训练时间减少高达三分之二。", "conclusion": "可微分重投影结合相机视角伪标签能够生成可迁移的BEV特征，为减少标注数据的自动驾驶感知提供了可扩展的途径。"}}
{"id": "2602.18326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18326", "abs": "https://arxiv.org/abs/2602.18326", "authors": ["Tao Wu", "Adam Kapelner"], "title": "Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning", "comment": "8 pages, 3 figures, 4 tables", "summary": "We describe a modern deep learning system that automatically identifies informative contextual examples (\\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \\qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.", "AI": {"tldr": "该研究开发了一个深度学习系统，用于为高中生识别第一语言词汇教学中的上下文示例，并比较了三种模型方法，其中一种结合了手工特征的模型表现最佳。", "motivation": "为高中生词汇教学自动生成有用的上下文示例，以提高教学效率和学生学习效果。", "method": "比较了三种模型方法：（i）无监督相似性方法（MPNet嵌入）；（ii）有监督方法（Qwen3嵌入+回归头）；（iii）有监督方法加上手工上下文特征。引入了“留存能力曲线”作为评估指标。", "result": "结合手工上下文特征的有监督模型（模型iii）在“好-坏”上下文比例方面取得了最佳性能（440），同时仅丢弃了70%的好上下文。", "conclusion": "结合人类监督的现代嵌入模型（特别是加入手工特征的模型）可以低成本地为目标词汇提供近乎完美的教学上下文。"}}
{"id": "2602.18351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18351", "abs": "https://arxiv.org/abs/2602.18351", "authors": ["Jordan Robinson", "Angus R. Williams", "Katie Atkinson", "Anthony G. Cohn"], "title": "Validating Political Position Predictions of Arguments", "comment": "13 pages, 6 figures, 6 tables. Under review", "summary": "Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \\textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.", "AI": {"tldr": "该研究提出了一个用于政治立场预测的双尺度验证框架，结合了点对点和对对评估。研究发现，虽然点对点评估显示出适度的人机一致性（Krippendorff's α=0.578），表明存在内在主观性，但对对评估显示出人机排名之间更强的匹配度（最佳模型为 α=0.86）。研究构建了一个大型的政治论点知识库，并证明了从主观论证中提取序数结构的可能性。", "motivation": "现实世界的知识表示常常需要捕捉主观的、连续的属性（如政治立场），这与被广泛接受的人类评估金标准——成对验证——相冲突。该研究旨在解决这一挑战，特别是在政治立场预测领域。", "method": "研究采用了双尺度验证框架，结合了点对点和对对人类标注，应用于政治立场预测。使用22个语言模型构建了一个大规模的政治立场预测知识库，涵盖了来自英国政治电视节目《提问时间》30场辩论中的23,228个论点。", "result": "点对点评估显示出适度的人机一致性（Krippendorff's α=0.578），反映了内在的主观性。对对评估则显示出人机排名之间显著更强的匹配度（最佳模型为 α=0.86）。", "conclusion": "该研究提出了一种实用的、可扩展且可靠的主观连续知识验证方法。此外，研究构建了一个经过验证的结构化论证知识库，支持政治领域的图推理和检索增强生成。研究还证明了从本质上主观的现实世界话语的点对点语言模型预测中提取序数结构的可能性，从而增强了传统符号或分类方法不足的领域的知识表示能力。"}}
{"id": "2602.18083", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18083", "abs": "https://arxiv.org/abs/2602.18083", "authors": ["Ioannis Kontogiorgakis", "Athanasios Askitopoulos", "Iason Tsardanidis", "Dimitrios Bormpoudakis", "Ilias Tsoumas", "Fotios Balampanis", "Charalampos Kontoes"], "title": "Comparative Assessment of Multimodal Earth Observation Data for Soil Moisture Estimation", "comment": "This paper has been submitted to IEEE IGARSS 2026", "summary": "Accurate soil moisture (SM) estimation is critical for precision agriculture, water resources management and climate monitoring. Yet, existing satellite SM products are too coarse (>1km) for farm-level applications. We present a high-resolution (10m) SM estimation framework for vegetated areas across Europe, combining Sentinel-1 SAR, Sentinel-2 optical imagery and ERA-5 reanalysis data through machine learning. Using 113 International Soil Moisture Network (ISMN) stations spanning diverse vegetated areas, we compare modality combinations with temporal parameterizations, using spatial cross-validation, to ensure geographic generalization. We also evaluate whether foundation model embeddings from IBM-NASA's Prithvi model improve upon traditional hand-crafted spectral features. Results demonstrate that hybrid temporal matching - Sentinel-2 current-day acquisitions with Sentinel-1 descending orbit - achieves R^2=0.514, with 10-day ERA5 lookback window improving performance to R^2=0.518. Foundation model (Prithvi) embeddings provide negligible improvement over hand-crafted features (R^2=0.515 vs. 0.514), indicating traditional feature engineering remains highly competitive for sparse-data regression tasks. Our findings suggest that domain-specific spectral indices combined with tree-based ensemble methods offer a practical and computationally efficient solution for operational pan-European field-scale soil moisture monitoring.", "AI": {"tldr": "本文提出了一种结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据的机器学习框架，实现了10米分辨率的欧洲植被区土壤湿度估算，并在ISMN站点验证中表现良好，证明了传统特征工程在稀疏数据回归任务中的竞争力。", "motivation": "现有卫星土壤湿度产品分辨率较低（>1km），无法满足农场级应用的需求。", "method": "结合Sentinel-1 SAR、Sentinel-2光学影像和ERA-5再分析数据，利用机器学习模型进行高分辨率（10m）土壤湿度估算。通过比较不同模态组合和时间参数化，并使用空间交叉验证来评估模型在地理上的泛化能力。同时，比较了Prithvi模型嵌入与传统手工制作光谱特征的性能。", "result": "结合Sentinel-2当前数据和Sentinel-1下降轨道数据，并使用10天的ERA5回溯窗口，取得了R^2=0.518的性能。Prithvi模型嵌入相比手工制作特征的性能提升不显著（R^2=0.515 vs 0.514）。", "conclusion": "领域特定的光谱指数结合基于树的集成方法，为泛欧尺度现场土壤湿度监测提供了一种实用且计算高效的解决方案。传统特征工程在稀疏数据回归任务中仍然具有竞争力。"}}
{"id": "2602.18379", "categories": ["cs.RO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.18379", "abs": "https://arxiv.org/abs/2602.18379", "authors": ["Hugo de Souza Oliveira", "Xin Li", "Mohsen Jafarpour", "Edoardo Milana"], "title": "Ori-Sense: origami capacitive sensing for soft robotic applications", "comment": "9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "This work introduces Ori-Sense, a compliant capacitive sensor inspired by the inverted Kresling origami pattern. The device translates torsional deformation into measurable capacitance changes, enabling proprioceptive feedback for soft robotic systems. Using dissolvable-core molding, we fabricated a monolithic silicone structure with embedded conductive TPU electrodes, forming an integrated soft capacitor. Mechanical characterization revealed low stiffness and minimal impedance, with torque values below 0.01 N mm for axial displacements between -15 mm and 15 mm, and up to 0.03 N mm at 30 degrees twist under compression. Finite-element simulations confirmed localized stresses along fold lines and validated the measured torque-rotation response. Electrical tests showed consistent capacitance modulation up to 30%, directly correlated with the twist angle, and maximal sensitivity of S_theta ~ 0.0067 pF/deg at 5 mm of axial deformation.", "AI": {"tldr": "本文提出了一种名为Ori-Sense的柔性电容传感器，该传感器模仿倒置Kresling折纸结构，能够将扭转形变转化为可测量的电容变化，为软体机器人提供本体感觉反馈。", "motivation": "为软体机器人系统提供本体感觉反馈，以实现更精密的控制和交互。", "method": "利用可溶性芯模塑技术制造整体硅胶结构，并嵌入导电TPU电极，形成集成软电容器。通过机械性能表征和有限元模拟来验证其形变和应力特性。通过电学测试测量电容变化与扭转角度的关系。", "result": "Ori-Sense传感器具有低刚度和低阻抗，在轴向位移-15 mm至15 mm范围内扭矩小于0.01 N mm，在压缩下30度扭转时扭矩高达0.03 N mm。有限元模拟验证了局部应力分布和扭矩-旋转响应。电容调制高达30%，与扭转角度呈正相关，在5 mm轴向形变下最大灵敏度为S_theta ~ 0.0067 pF/deg。", "conclusion": "Ori-Sense传感器是一种基于折纸结构的柔性本体感觉传感器，能够有效地将扭转形变转化为电容信号，为软体机器人的本体感觉提供了新的解决方案。"}}
{"id": "2602.18374", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18374", "abs": "https://arxiv.org/abs/2602.18374", "authors": ["Venkatesh Sripada", "Frank Guerin", "Amir Ghalamzan"], "title": "Zero-shot Interactive Perception", "comment": "Original manuscript submitted on April 24, 2025. Timestamped and publicly available on OpenReview: https://openreview.net/forum?id=7MhpFcr5Nx", "summary": "Interactive perception (IP) enables robots to extract hidden information in their workspace and execute manipulation plans by physically interacting with objects and altering the state of the environment -- crucial for resolving occlusions and ambiguity in complex, partially observable scenarios. We present Zero-Shot IP (ZS-IP), a novel framework that couples multi-strategy manipulation (pushing and grasping) with a memory-driven Vision Language Model (VLM) to guide robotic interactions and resolve semantic queries. ZS-IP integrates three key components: (1) an Enhanced Observation (EO) module that augments the VLM's visual perception with both conventional keypoints and our proposed pushlines -- a novel 2D visual augmentation tailored to pushing actions, (2) a memory-guided action module that reinforces semantic reasoning through context lookup, and (3) a robotic controller that executes pushing, pulling, or grasping based on VLM output. Unlike grid-based augmentations optimized for pick-and-place, pushlines capture affordances for contact-rich actions, substantially improving pushing performance. We evaluate ZS-IP on a 7-DOF Franka Panda arm across diverse scenes with varying occlusions and task complexities. Our experiments demonstrate that ZS-IP outperforms passive and viewpoint-based perception techniques such as Mark-Based Visual Prompting (MOKA), particularly in pushing tasks, while preserving the integrity of non-target elements.", "AI": {"tldr": "提出了一种名为零样本交互感知（ZS-IP）的框架，该框架结合了多策略操作（推和抓取）和记忆驱动的视觉语言模型（VLM），以增强机器人的感知能力，并解决复杂、部分可观察场景中的遮挡和歧义问题。它通过增强的观察（EO）模块（包含传统关键点和新提出的推线）、记忆引导动作模块和机器人控制器来实现。", "motivation": "现有方法在解决复杂、部分可观察场景中的遮挡和歧义问题时存在不足，特别是对于涉及推等接触式操作的任务。机器人需要通过物理交互来提取隐藏信息并执行操作计划。", "method": "ZS-IP 框架包含三个关键组件：1. 增强的观察（EO）模块：结合了传统关键点和新提出的推线（pushlines）来增强 VLM 的视觉感知。2. 记忆引导动作模块：通过上下文查找来强化语义推理。3. 机器人控制器：根据 VLM 输出执行推、拉或抓取操作。", "result": "ZS-IP 在包含不同遮挡和任务复杂性的多样化场景中进行了评估，并使用 7-DOF Franka Panda 机械臂进行实验。结果表明，ZS-IP 在推操作任务上显著优于被动和基于视点的感知技术（如 MOKA），同时保持了非目标元素的完整性。", "conclusion": "ZS-IP 是一种有效的新型框架，它通过结合多策略操作和记忆驱动的 VLM，并引入推线作为一种新的视觉增强方式，能够显著提高机器人在处理遮挡和歧义问题时的交互感知能力，尤其在推操作任务中表现出色。"}}
{"id": "2602.18420", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18420", "abs": "https://arxiv.org/abs/2602.18420", "authors": ["Jiamin Yao", "Eren Gultepe"], "title": "SPQ: An Ensemble Technique for Large Language Model Compression", "comment": "Accepted to LREC 2026 Main Conference", "summary": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/", "AI": {"tldr": "本文提出了一种名为SPQ（SVD-Pruning-Quantization）的集成技术，用于大型语言模型（LLM）压缩，该技术结合了保持方差的奇异值分解（SVD）、基于激活的剪枝和训练后线性量化，在同等压缩率下优于单一方法，能显著减少模型内存占用并保持甚至提高模型性能。", "motivation": "大型语言模型（LLM）因其庞大的模型尺寸和计算需求，在部署和实际应用中面临内存和计算效率的挑战，这促使研究人员探索有效的模型压缩技术。", "method": "SPQ集成技术包含三个关键组件：1) 基于激活的剪枝，用于移除MLP层中冗余的神经元；2) 保持方差的奇异值分解（SVD），用于将注意力投影压缩成低秩因子；3) 训练后8位线性量化，用于统一压缩所有线性层。这些技术协同工作，分别针对模型的不同效率来源。", "result": "在LLaMA-2-7B模型上，SPQ实现了高达75%的内存减少，同时在WikiText-2数据集上将困惑度从5.47降低到4.91，并在C4、TruthfulQA和GSM8K等下游基准测试中保持了准确性。与GPTQ和SparseGPT等基线相比，SPQ在内存占用更少的情况下提供了有竞争力的困惑度和准确性，并且推理吞吐量提升高达1.9倍。", "conclusion": "SPQ通过结合SVD、剪枝和量化等互补的压缩技术，能够有效地压缩LLM，实现显著的内存节省和性能提升，为在内存受限环境中部署LLM提供了实用的解决方案。"}}
{"id": "2602.18093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18093", "abs": "https://arxiv.org/abs/2602.18093", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Qianli Ma", "Zhi Yao", "Weijia Jia"], "title": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.", "AI": {"tldr": "提出了一种名为PrediT的免训练加速框架，通过将特征预测视为线性多步问题，并结合动态步长调制，显著减少了Diffusion Transformer (DiT) 在图像和视频生成中的计算成本，同时保持了生成质量。", "motivation": "DiT模型在生成高质量图像和视频方面表现出色，但其迭代去噪过程计算成本高。现有的加速方法依赖于时间稳定性假设下的特征缓存和重用，这可能导致潜在漂移和视觉质量下降。研究者观察到模型输出在扩散轨迹的大部分时间里平滑演变，这启发了更优的预测方法。", "method": "PrediT框架将特征预测建模为线性多步问题，利用经典的线性多步方法根据历史信息预测模型未来输出。该框架还包含一个“纠正器”，在动态性强的区域激活以防止误差累积。此外，引入动态步长调制机制，通过监测特征变化率自适应地调整预测步长。", "result": "PrediT在多种基于DiT的图像和视频生成模型上实现了高达5.54倍的延迟降低，同时视觉质量几乎没有下降。实验结果验证了该方法在加速生成过程的同时保持了生成保真度。", "conclusion": "PrediT是一种有效的训练无关的加速框架，通过利用模型输出的平滑演变特性，将特征预测转化为线性多步问题，并辅以纠正器和动态步长调制，显著提高了DiT模型的生成效率，而对生成质量的影响极小。"}}
{"id": "2602.18397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18397", "abs": "https://arxiv.org/abs/2602.18397", "authors": ["Wenqi Jiang", "Jason Clemons", "Karu Sankaralingam", "Christos Kozyrakis"], "title": "How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf", "comment": null, "summary": "Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.", "AI": {"tldr": "本文提出了一种名为VLA-Perf的分析模型，用于研究视觉-语言-动作（VLA）模型在现实机器人部署中的实时推理性能。通过系统性研究模型架构、输入、推理方式以及部署位置对性能的影响，论文提炼出15条关键洞察，旨在为未来VLA模型的设计和推理系统提供指导。", "motivation": "现有VLA模型在真实机器人部署时面临严格的实时推理约束，但关于其推理性能的研究尚不充分，尤其是考虑到模型架构和推理系统的巨大组合空间。因此，研究如何设计未来的VLA模型和系统以支持实时推理是一个重要的研究问题。", "method": "1. 提出VLA-Perf分析模型，用于评估任意VLA模型和推理系统的组合的推理性能。\n2. 使用VLA-Perf模型，系统性地研究VLA推理性能景观，从模型设计角度（模型缩放、架构选择、长上下文视频输入、异步推理、双系统模型管道）和部署角度（设备端、边缘服务器、云端执行，硬件能力与网络性能对端到端延迟的影响）进行分析。", "result": "通过对15个关键因素的全面评估，研究揭示了模型设计（如模型缩放、架构选择、长上下文处理、异步推理和双系统管道）和部署策略（如计算位置和硬件/网络协同优化）对VLA模型实时推理性能的关键影响。", "conclusion": "本研究通过VLA-Perf分析模型，首次系统地研究了VLA模型的实时推理性能景观，并提炼出15条关键洞察。这些洞察旨在为未来VLA模型和推理系统的设计提供实用的指导，以满足真实世界机器人应用对实时性的需求。"}}
{"id": "2602.18425", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18425", "abs": "https://arxiv.org/abs/2602.18425", "authors": ["Deniz Qian", "Hung-Ting Chen", "Eunsol Choi"], "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering", "comment": "18 pages, 12 figures, 12 tables", "summary": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.", "AI": {"tldr": "提出了一种名为RVR（retrieve-verify-retrieve）的多轮检索框架，通过迭代检索、验证和查询增强来最大化答案覆盖率，并在多个数据集上取得了显著的性能提升。", "motivation": "现有检索方法难以全面检索包含多种有效答案的查询，因此需要一种能够最大化答案覆盖率的框架。", "method": "RVR框架包含多轮检索：首先，一个检索器根据原始查询返回候选文档集；然后，一个验证器从中识别出高质量子集；后续轮次中，查询会结合之前验证过的文档进行增强，以发现尚未覆盖的答案。该方法支持离线检索器，并通过微调检索器可进一步提升性能。", "result": "RVR在QAMPARI数据集上，完整召回率（complete recall percentage）上相对提升至少10%，绝对提升3%，优于现有基线方法，包括基于智能体（agentic）的搜索方法。在QUEST和WebQuestionsSP两个域外数据集上，RVR在不同基础检索器下也表现出持续的性能提升。", "conclusion": "RVR是一种有前景的迭代方法，能够利用验证器并调整检索器以适应新的推理场景，从而实现全面的答案召回。该框架通过多轮检索和查询增强有效解决了全面检索多样化文档的问题。"}}
{"id": "2602.18178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18178", "abs": "https://arxiv.org/abs/2602.18178", "authors": ["Poonam Poonam", "Pere-Pau Vázquez", "Timo Ropinski"], "title": "Evaluating Graphical Perception Capabilities of Vision Transformers", "comment": null, "summary": "Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.", "AI": {"tldr": "本文研究了 Vision Transformers (ViTs) 在图形感知任务上的表现，发现尽管 ViTs 在通用视觉任务上表现出色，但在可视化领域的图形感知方面与人类的匹配度有限，存在明显的感知差距。", "motivation": "由于 Vision Transformers (ViTs) 在图像任务中展现出强大潜力，但其在可视化领域，尤其是图形感知任务上的能力尚未被充分探索，因此需要研究 ViTs 在这些任务上的表现，并与 CNNs 和人类进行比较。", "method": "借鉴 Cleveland and McGill 的基础研究，设计了一系列图形感知任务，并在这些任务中对 ViTs、CNNs 和人类被试进行了基准测试。", "result": "ViTs 在通用视觉任务上表现优异，但在图形感知任务上，其表现与人类的匹配度有限，存在感知上的差距。", "conclusion": "ViTs 在可视化领域的图形感知能力尚不完善，存在关键的感知差距，这对其在可视化系统和图形感知建模中的应用提出了重要的考量。"}}
{"id": "2602.18421", "categories": ["cs.RO", "cond-mat.soft"], "pdf": "https://arxiv.org/pdf/2602.18421", "abs": "https://arxiv.org/abs/2602.18421", "authors": ["Xin Li", "Ye Jin", "Mohsen Jafarpour", "Hugo de Souza Oliveira", "Edoardo Milana"], "title": "Snapping Actuators with Asymmetric and Sequenced Motion", "comment": "9th IEEE-RAS International Conference on Soft Robotics (RoboSoft 2026)", "summary": "Snapping instabilities in soft structures offer a powerful pathway to achieve rapid and energy-efficient actuation. In this study, an eccentric dome-shaped snapping actuator is developed to generate controllable asymmetric motion through geometry-induced instability. Finite element simulations and experiments reveal consistent asymmetric deformation and the corresponding pressure characteristics. By coupling four snapping actuators in a pneumatic network, a compact quadrupedal robot achieves coordinated wavelike locomotion using only a single pressure input. The robot exhibits frequency-dependent performance with a maximum speed of 72.78~mm/s at 7.5~Hz. These findings demonstrate the potential of asymmetric snapping mechanisms for physically controlled actuation and lay the groundwork for fully untethered and efficient soft robotic systems.", "AI": {"tldr": "研究开发了一种偏心球顶形软体驱动器，通过几何诱导的不稳定来实现可控的非对称运动，并成功将其耦合到气动网络中，构建了一个仅需单一压力输入的四足机器人，实现了波浪式运动。", "motivation": "利用软体结构中的“卡断”不稳定性实现快速、节能的驱动。", "method": "采用有限元仿真和实验相结合的方法，研究偏心球顶形驱动器的非对称变形和压力特性。通过将四个驱动器耦合到一个气动网络中，构建四足机器人，并研究其在不同频率下的运动性能。", "result": "驱动器表现出一致的非对称变形和相应的压力特性。四足机器人实现了协同的波浪式运动，最高速度可达72.78 mm/s（在7.5 Hz下）。", "conclusion": "非对称卡断机制在物理控制驱动方面具有巨大潜力，为开发全无线、高效的软体机器人系统奠定了基础。"}}
{"id": "2602.18429", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18429", "abs": "https://arxiv.org/abs/2602.18429", "authors": ["Harshul Raj Surana", "Arijit Maji", "Aryan Vats", "Akash Ghosh", "Sriparna Saha", "Amit Sheth"], "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning", "comment": null, "summary": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.", "AI": {"tldr": "研究提出了VIRAASAT，一个用于印度文化的多跳问答数据集生成框架，并提出了一种名为SCoM的新型模型，通过模拟知识图谱操作来提升LLM在文化推理方面的能力，并在实验中取得了显著效果。", "motivation": "现有文化基准测试在手动创建、仅包含单跳问题、难以规模化等方面存在不足，特别是在需要丰富社会文化知识和当地背景的印度文化领域，LLM的表现不佳且缺乏有效的衡量标准。", "method": "1. 提出VIRAASAT：一个半自动化的多跳方法，利用包含700多个专家策展文化藏品的知识图谱，覆盖印度文化（历史、节日等）的13个关键属性，生成覆盖28个州和8个联邦属地的3200多个多跳问题。\n2. 提出SCoM：一个名为Symbolic Chain-of-Manipulation的新框架，通过训练模型内部模拟原子知识图谱操作，使其能够可靠地遍历知识图谱的拓扑结构。", "result": "在VIRAASAT数据集上评估SOTA LLM，发现其在推理方面存在局限性，特别是微调CoT（Chain-of-Thought）无法有效利用低概率事实。\nSCoM在监督微调（SFT）实验中，相比标准的CoT基线，性能提升高达20%。", "conclusion": "VIRAASAT数据集的发布以及SCoM模型的有效性，为构建具有文化意识的推理模型奠定了坚实基础，解决了当前LLM在处理复杂文化背景信息时存在的不足。"}}
{"id": "2602.18199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18199", "abs": "https://arxiv.org/abs/2602.18199", "authors": ["Gahyeon Shim", "Soogeun Park", "Hyemin Ahn"], "title": "A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion", "comment": null, "summary": "Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.", "AI": {"tldr": "提出了一种名为DMC（Distortion-aware Motion Calibrator）的后处理模块，用于提升文本到动作生成模型的输出质量，使其在保持语义一致性的同时，具备更高的物理真实性。", "motivation": "现有文本到动作生成模型在保证语义和物理真实性方面存在挑战，例如生成不真实的动作（如脚悬空）。", "method": "DMC采用自监督、数据驱动的方法，通过输入故意扭曲的动作和原始文本描述来学习生成物理上合理的动作，而无需复杂的物理模型。", "result": "DMC作为后处理模块，在T2M和T2M-GPT上分别将FID分数降低了42.74%和13.20%，并提高了R-Precision。在MoMask模型上，DMC将穿透减少了33.0%，并修复了脚悬空问题。", "conclusion": "DMC可以作为一种有效的后处理框架，通过结合文本语义和物理真实性，提升任何文本到动作生成模型的性能。"}}
{"id": "2602.18424", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18424", "abs": "https://arxiv.org/abs/2602.18424", "authors": ["Xia Su", "Ruiqi Chen", "Benlin Liu", "Jingwei Ma", "Zonglin Di", "Ranjay Krishna", "Jon Froehlich"], "title": "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation", "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav", "AI": {"tldr": "本文提出了一个名为CapNav的基准测试，用于评估视觉-语言模型（VLMs）在考虑代理移动能力限制下的室内导航能力。研究发现，当前VLMs在导航性能上受移动能力限制影响显著，尤其在需要空间推理的障碍物类型上表现不佳。", "motivation": "现有视觉-语言导航研究忽略了现实世界中代理的移动能力限制，而这些限制对于机器人和人类用户至关重要。因此，需要一个能够评估VLMs在考虑代理能力下导航表现的基准。", "method": "构建了CapNav基准，包含五种代表性代理（人类和机器人）及其物理尺寸、移动能力和环境交互能力。该基准包含45个真实室内场景、473个导航任务和2365个问答对。在CapNav上评估了13种现代VLMs。", "result": "评估结果显示，当移动能力限制收紧时，VLMs的导航性能急剧下降。即便是最先进的模型，在处理需要空间维度推理的障碍物时也表现困难。", "conclusion": "当前VLMs在具有移动能力限制的导航任务中存在显著的局限性，需要进一步发展面向能力的导航以及提升具身空间推理能力，以应对更复杂的现实世界导航场景。"}}
{"id": "2602.18193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18193", "abs": "https://arxiv.org/abs/2602.18193", "authors": ["Yiran Yang", "Zhaowei Liu", "Yuan Yuan", "Yukun Song", "Xiong Ma", "Yinghao Song", "Xiangji Zeng", "Lu Sun", "Yulu Wang", "Hai Zhou", "Shuai Cui", "Zhaohan Gong", "Jiefei Zhang"], "title": "BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards", "comment": "7 pages, 3 figures. To appear in AAAI 2026", "summary": "Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.", "AI": {"tldr": "本文提出了一种名为BLM-Guard的内容审核框架，用于检测短视频平台上的欺骗性多模态广告，该框架结合了思维链推理、基于规则的策略原则和评论员引导的奖励机制。", "motivation": "短视频平台上的多模态广告（包括视觉、语音和字幕）日益增多，其中许多具有欺骗性，需要比现有社区安全过滤器更精细、更具策略性的审核方法。", "method": "BLM-Guard框架采用了一种多任务架构，该架构融合了思维链（Chain-of-Thought）推理、基于规则的策略原则和评论员引导的奖励（critic-guided reward）。它还包括一个由规则驱动的ICoT数据合成管道，用于生成结构化场景描述、推理链和标签，以降低标注成本。随后，使用强化学习对模型进行优化，通过平衡因果一致性和策略依从性的复合奖励。", "result": "在真实的短视频广告数据集上的实验表明，BLM-Guard在准确性、一致性和泛化能力方面优于强大的基线模型。它能够有效地检测出模型内操作（如夸大图像）和跨模态不匹配（如字幕与语音漂移）。", "conclusion": "BLM-Guard是一个有效的内容审核框架，能够应对短视频平台上的多模态欺骗性广告的挑战，并在实际应用中取得了优于现有方法的性能。"}}
{"id": "2602.18309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18309", "abs": "https://arxiv.org/abs/2602.18309", "authors": ["Ziyue Liu", "Davide Talon", "Federico Girella", "Zanxi Ruan", "Mattia Mondo", "Loris Bazzani", "Yiming Wang", "Marco Cristani"], "title": "Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation", "comment": "Project page: https://intelligolabs.github.io/lots/", "summary": "Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an \"in the wild\" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.", "AI": {"tldr": "提出了一种名为LOTS的框架，通过结合全局草图引导和多个局部草图-文本对，来增强时尚图像生成，并在新发布的Sketchy数据集上进行了验证。", "motivation": "为了更有效地结合草图和文本描述在早期时尚概念设计中的优势，特别是在文本的局部属性指导下，需要遵循草图的视觉结构。", "method": "LOTS框架包括一个多层条件化阶段，用于在共享的潜在空间中独立编码局部特征，同时保持全局结构协调。然后，扩散对引导阶段通过基于注意力（attention-based）的引导，在扩散模型的去噪过程中整合局部和全局的条件。", "result": "LOTS框架在保持全局结构一致性的同时，能够利用更丰富的局部语义信息，并在实验中取得了优于现有最先进方法的性能。该方法增强了全局结构的依从性，并利用了更丰富的局部语义引导。", "conclusion": "LOTS框架能够有效地结合全局草图引导和局部文本信息，以生成更准确、更丰富的时尚图像，并且该方法在标准和“野外”数据集上都表现出色。"}}
{"id": "2602.18314", "categories": ["cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18314", "abs": "https://arxiv.org/abs/2602.18314", "authors": ["Tianyi Song", "Danail Stoyanov", "Evangelos Mazomenos", "Francisco Vasconcelos"], "title": "Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.", "AI": {"tldr": "提出了一种名为Diff2DGS的两阶段框架，用于重建手术中被遮挡的组织，通过扩散模型进行修复，并结合可学习变形模型和2D高斯溅射技术来捕捉组织形变和几何结构，同时首次在SCARED数据集上进行定量深度精度分析，提升了重建的真实感和几何精度。", "motivation": "现有手术场景三维重建方法在处理被遮挡区域时质量受限，且缺乏对重建深度的准确评估，因为现有基准测试缺乏三维真实数据。因此，需要一种能够可靠重建被遮挡手术场景的方法，并对重建的深度精度进行全面评估。", "method": "该方法采用两阶段框架：第一阶段，利用基于扩散的视频模型，结合时间先验，修复由器械遮挡的组织，保证空间-时间一致性；第二阶段，将2D高斯溅射（2DGS）与可学习变形模型（LDM）相结合，以捕捉动态组织形变和解剖几何结构。此外，还扩展了评估指标，在SCARED数据集上进行了定量深度精度分析。", "result": "Diff2DGS在外观和几何方面均优于现有最先进方法，在EndoNeRF和StereoMIS数据集上分别达到了38.02 dB和34.40 dB的PSNR。实验表明，仅优化图像质量并不能保证最佳的三维重建精度。通过进一步优化深度质量，确保了重建的几何保真度。", "conclusion": "Diff2DGS是一种用于可靠重建被遮挡手术场景的新型两阶段框架，通过结合扩散模型进行修复和2DGS与LDM进行几何捕捉，显著提升了重建的外观和几何精度。研究强调了在三维重建中同时考虑图像质量和深度精度的重要性，并通过优化深度质量实现了更精确的几何重建。"}}
{"id": "2602.18322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18322", "abs": "https://arxiv.org/abs/2602.18322", "authors": ["Ziteng Cui", "Shuhong Liu", "Xiaoyu Dong", "Xuangeng Chu", "Lin Gu", "Ming-Hsuan Yang", "Tatsuya Harada"], "title": "Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis", "comment": "Journal extension version of CVPR 2025 paper: arXiv:2504.01503", "summary": "High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.", "AI": {"tldr": "提出了一种名为 Luminance-GS++ 的基于 3DGS 的方法，用于解决多视图捕获中由于光照变化和相机成像管线限制导致的光度学和色度学不一致问题，从而提高新视图合成（NVS）的质量，并且不改变 3DGS 的原始表示。", "motivation": "在真实世界的多视图捕获中，复杂的光照变化和相机成像管线的局限性会导致 photometric 和 chromatic 不一致，这违反了现有 3D NVS 方法（如 NeRF 和 3DGS）的 photometric 一致性假设，从而降低了重建和渲染质量。", "method": "提出 Luminance-GS++ 框架，该框架结合了全局视图自适应亮度调整和局部像素级残差细化，用于精确的色彩校正。设计了无监督目标，同时强制执行亮度校正以及多视图几何和光度学一致性。", "result": "在低光照、过曝和复杂亮度/色度变化等挑战性场景下，该方法取得了最先进的性能。", "conclusion": "Luminance-GS++ 能够有效解决多视图捕获中的光照不一致问题，提高 NVS 的重建保真度，同时保持实时渲染效率，并且保留了 3DGS 的显式表示。"}}
{"id": "2602.18282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18282", "abs": "https://arxiv.org/abs/2602.18282", "authors": ["Shiyan Du", "Conghan Yue", "Xinyu Cheng", "Dongyu Zhang"], "title": "DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control", "comment": "Accepted by AAAI 2026", "summary": "Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.", "AI": {"tldr": "本文提出了DEIG框架，一种用于细粒度、可控的多实例生成方法，通过实例细节提取器和细节融合模块解决现有方法在理解复杂文本描述和避免属性交叉方面的不足，并在新构建的数据集和基准上取得了优于现有方法的性能。", "motivation": "现有方法在处理复杂文本描述和细粒度语义理解方面存在局限性，容易导致属性泄露，无法精确匹配局部文本描述。", "method": "提出了DEIG框架，包含两个关键组件：1) 实例细节提取器 (IDE)，将文本编码器嵌入转化为紧凑、实例感知的表示；2) 细节融合模块 (DFM)，使用基于实例的掩码注意力机制，防止实例间属性泄露。此外，构建了一个包含详细、组合式实例标题的高质量数据集，并引入了DEIG-Bench基准。", "result": "DEIG在空间一致性、语义准确性和组合泛化性方面均优于现有方法。DEIG还能作为即插即用模块集成到标准扩散模型中。", "conclusion": "DEIG框架能够生成在视觉上连贯且精确匹配丰富、局部化文本描述的多实例场景，解决了现有方法的不足，并在多项评估指标上取得了SOTA性能。"}}
{"id": "2602.18394", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18394", "abs": "https://arxiv.org/abs/2602.18394", "authors": ["Stefan Becker", "Simon Weiss", "Wolfgang Hübner", "Michael Arens"], "title": "Self-Aware Object Detection via Degradation Manifolds", "comment": null, "summary": "Object detectors achieve strong performance under nominal imaging conditions but can fail silently when exposed to blur, noise, compression, adverse weather, or resolution changes. In safety-critical settings, it is therefore insufficient to produce predictions without assessing whether the input remains within the detector's nominal operating regime. We refer to this capability as self-aware object detection.\n  We introduce a degradation-aware self-awareness framework based on degradation manifolds, which explicitly structure a detector's feature space according to image degradation rather than semantic content. Our method augments a standard detection backbone with a lightweight embedding head trained via multi-layer contrastive learning. Images sharing the same degradation composition are pulled together, while differing degradation configurations are pushed apart, yielding a geometrically organized representation that captures degradation type and severity without requiring degradation labels or explicit density modeling.\n  To anchor the learned geometry, we estimate a pristine prototype from clean training embeddings, defining a nominal operating point in representation space. Self-awareness emerges as geometric deviation from this reference, providing an intrinsic, image-level signal of degradation-induced shift that is independent of detection confidence.\n  Extensive experiments on synthetic corruption benchmarks, cross-dataset zero-shot transfer, and natural weather-induced distribution shifts demonstrate strong pristine-degraded separability, consistent behavior across multiple detector architectures, and robust generalization under semantic shift. These results suggest that degradation-aware representation geometry provides a practical and detector-agnostic foundation.", "AI": {"tldr": "本文提出了一种基于退化流形（degradation manifolds）的自感知目标检测框架，通过显式地组织特征空间来捕捉图像退化信息，从而实现对输入图像质量变化的鲁棒性。", "motivation": "现有的目标检测器在正常成像条件下表现良好，但在面对模糊、噪声、压缩、恶劣天气或分辨率变化等退化时会失效。在安全关键领域，需要一种能够评估输入图像是否在检测器标称操作范围内的能力，即自感知目标检测。", "method": "该框架通过一个轻量级的嵌入头（embedding head）和多层对比学习（multi-layer contrastive learning）来增强标准检测骨干网络。通过将具有相同退化组合的图像拉近，将不同退化配置的图像推开，从而在特征空间中形成一种几何组织，该组织能够捕捉退化类型和严重程度，而无需退化标签或显式密度建模。同时，通过估计一个干净训练嵌入的原型（pristine prototype）来定义一个标称操作点，将特征空间中的几何偏差作为退化信号。", "result": "实验证明，该方法在合成退化基准、跨数据集零样本迁移和自然天气引起的分布偏移上，表现出很强的原始-降级可分离性，在多种检测器架构上表现一致，并且在语义变化下具有鲁棒的泛化能力。", "conclusion": "基于退化感知的表示几何可以为目标检测提供一个实用的、与检测器无关的基础，实现鲁棒的自感知目标检测能力。"}}
{"id": "2602.18329", "categories": ["cs.CV", "math.AT"], "pdf": "https://arxiv.org/pdf/2602.18329", "abs": "https://arxiv.org/abs/2602.18329", "authors": ["Qingsong Wang", "Jiaxing He", "Bingzhe Hou", "Tieru Wu", "Yang Cao", "Cailing Yao"], "title": "G-LoG Bi-filtration for Medical Image Classification", "comment": null, "summary": "Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.", "AI": {"tldr": "本文提出了一种新的G-LoG双重滤波方法，用于从医学图像中提取拓扑和几何特征，并证明了其在最大范数下的稳定性。实验表明，该方法在MedMNIST数据集上优于单参数滤波，并且基于其提取的拓扑特征训练的简单MLP模型性能可与复杂的深度学习模型相媲美。", "motivation": "现有的拓扑数据分析（TDA）方法在处理医学图像等对象时，构建实用的滤波器来检测拓扑和几何特征具有重要意义。然而，单参数滤波可能无法充分捕捉数据的复杂特征。", "method": "研究者提出了G-LoG（Gaussian-Laplacian of Gaussian）双重滤波方法，利用LoG算子增强医学图像的边界。通过将体积图像建模为有界函数，并在其上定义G-LoG双重滤波，生成适用于多参数持久性模块的特征。此外，证明了在所获得的持久性模块上的交错距离相对于有界函数的最大范数是稳定的。", "result": "在MedMNIST数据集上的实验结果显示，G-LoG双重滤波显著优于单参数滤波。基于G-LoG双重滤波提取的拓扑特征训练的简单多层感知机（MLP）模型，其性能与在原始数据集上训练的复杂深度学习模型（如Google AutoML Vision、ResNet、AutoKeras和auto-sklearn）相当。", "conclusion": "G-LoG双重滤波是一种有效的方法，能够从医学图像中提取更丰富的拓扑和几何特征，并且在最大范数下具有稳定性。该方法可以作为一种强大的TDA工具，尤其是在与简单机器学习模型结合使用时，能够获得与先进深度学习模型相媲美的性能。"}}
{"id": "2602.18422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18422", "abs": "https://arxiv.org/abs/2602.18422", "authors": ["Linxi Xie", "Lisong C. Sun", "Ashley Neall", "Tong Wu", "Shengqu Cai", "Gordon Wetzstein"], "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control", "comment": "Project page here: https://codeysun.github.io/generated-reality", "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.", "AI": {"tldr": "该研究提出了一种以人为中心、能够响应用户头部和手部运动的XR生成模型，并证明其能提升用户在虚拟环境中的任务表现和控制感。", "motivation": "现有视频生成模型对用户的实时动作（如头部和手部姿态）的响应能力有限，阻碍了其在XR环境中的交互应用。", "method": "研究者提出了一种新的条件生成策略，将3D头部和手部姿态信息融入扩散模型，并采用教师-学生蒸馏方法将双向扩散模型教师模型精炼成因果、交互式的生成系统。", "result": "实验表明，该生成模型在任务表现和用户感知控制水平上均优于基线模型。", "conclusion": "所提出的以人为中心的视频生成模型能够有效地将用户的头部和手部运动映射到虚拟环境的生成中，显著改善了XR中的用户交互体验。"}}
{"id": "2602.18434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18434", "abs": "https://arxiv.org/abs/2602.18434", "authors": ["Vatsal Agarwal", "Saksham Suri", "Matthew Gwilliam", "Pulkit Kumar", "Abhinav Shrivastava"], "title": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory", "comment": "Project page: see https://vatsalag99.github.io/memstream/", "summary": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.", "AI": {"tldr": "本文提出了一种名为MemStream的方法，通过增加每帧的token预算和引入自适应选择策略及训练无关的检索混合专家机制，解决了现有视频理解模型在处理连续视频流时信息丢失和检索偏差的问题，显著提升了在CG-Bench、LVBench和VideoMME（Long）数据集上的性能。", "motivation": "现有视频理解模型依赖键值缓存累积帧信息，但token预算有限导致细粒度视觉细节丢失，且其特征编码方式会偏向检索后期帧。", "method": "1. 扩大每帧的token预算以实现更精细的时空理解。2. 提出自适应选择策略，减少token冗余并保留局部时空信息。3. 引入训练无关的检索混合专家（Mixture-of-Experts）机制，利用外部模型更准确地识别相关帧。", "result": "使用Qwen2.5-VL-7B模型，MemStream在CG-Bench上提升了+8.0%，在LVBench上提升了+8.5%，在VideoMME (Long)上提升了+2.4%，均超越了ReKV方法。", "conclusion": "通过增加token预算、采用自适应选择策略和引入检索混合专家，MemStream能够更有效地从连续视频流中提取和利用信息，从而实现更准确的视频理解和问答。"}}
{"id": "2602.18432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18432", "abs": "https://arxiv.org/abs/2602.18432", "authors": ["Evonne Ng", "Siwei Zhang", "Zhang Chen", "Michael Zollhoefer", "Alexander Richard"], "title": "SARAH: Spatially Aware Real-time Agentic Humans", "comment": "Project page: https://evonneng.github.io/sarah/", "summary": "As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.", "AI": {"tldr": "本文提出了一种首个支持实时、完全因果推理的空间感知对话式动作生成方法，可用于流式 VR 头显，通过结合 VAE 和流匹配模型，实现了比非因果基线快三倍的生成速度，并能在实际 VR 系统中部署。", "motivation": "现有的虚拟形象（embodied agents）运动生成方法仅关注与语音同步的手势，缺乏对用户位置和运动的空间感知能力，无法实现虚拟形象转向用户、响应用户移动以及保持自然注视等行为，限制了其在 VR、远程呈现和数字人应用中的表现。", "method": "该方法结合了基于因果 Transformer 的 VAE 和交错的潜在 token，用于流式推理；以及一个条件于用户轨迹和音频的流匹配模型。为了支持用户不同注视偏好，引入了一个带分类器无引导的注视评分机制，将模型学习的自然空间对齐与用户在推理时调整注视强度解耦。", "result": "在 Embody 3D 数据集上，该方法实现了超过 300 FPS 的生成速度，比非因果基线快三倍，同时生成了高质量的运动，并捕捉到了自然对话中的细微空间动态。在实际 VR 系统上的验证也表明了该方法的有效性。", "conclusion": "本文成功构建了一个实时、完全因果的空间感知对话式动作生成系统，解决了现有方法的局限性，并能在实际 VR 环境中部署，为提升虚拟形象的交互性和沉浸感提供了新的解决方案。"}}
{"id": "2602.18406", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18406", "abs": "https://arxiv.org/abs/2602.18406", "authors": ["Minh Dinh", "Stéphane Deny"], "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges", "comment": null, "summary": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.", "AI": {"tldr": "研究提出了一种学习隐空间中的等变算子来处理未见过的对称变换（如旋转、平移）的神经网络架构，并在MNIST数据集上取得了良好的分类效果，展示了其在分布外泛化方面的潜力。", "motivation": "现有的深度学习模型在处理罕见的对称变换（如物体的不常见姿态、尺度、位置）时泛化能力不足，而传统的等变神经网络需要预先知道变换规律。因此，需要一种能够从数据中学习等变算子的新方法。", "method": "提出了一种能够学习隐空间中等变算子的神经网络架构，通过对包含旋转和平移变换的MNIST数据集进行训练，使其能够处理未见过的对称变换。", "result": "该方法成功地实现了分布外（out-of-distribution）的分类任务，克服了传统神经网络和等变神经网络的局限性，在MNIST数据集上展现了良好的泛化能力。", "conclusion": "学习隐空间中的等变算子是一种很有前景的方法，可以处理未见过的对称变换并实现良好的分布外泛化。然而，将这种方法扩展到更复杂的数据集仍面临挑战。"}}
