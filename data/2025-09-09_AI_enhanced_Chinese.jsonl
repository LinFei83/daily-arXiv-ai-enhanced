{"id": "2509.05359", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05359", "abs": "https://arxiv.org/abs/2509.05359", "authors": ["Yanis Labrak", "Richard Dufour", "Mickaël Rouvier"], "title": "An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training", "comment": "Published in International Conference on Text, Speech, and Dialogue,\n  13-24", "summary": "This paper investigates discrete unit representations in Speech Language\nModels (SLMs), focusing on optimizing speech modeling during continual\npre-training. In this paper, we systematically examine how model architecture,\ndata representation, and training robustness influence the pre-training stage\nin which we adapt existing pre-trained language models to the speech modality.\nOur experiments highlight the role of speech encoders and clustering\ngranularity across different model scales, showing how optimal discretization\nstrategies vary with model capacity. By examining cluster distribution and\nphonemic alignments, we investigate the effective use of discrete vocabulary,\nuncovering both linguistic and paralinguistic patterns. Additionally, we\nexplore the impact of clustering data selection on model robustness,\nhighlighting the importance of domain matching between discretization training\nand target applications.", "AI": {"tldr": "本文研究了语音语言模型（SLMs）中的离散单元表示，重点关注在持续预训练中优化语音建模，并探讨了模型架构、数据表示和训练鲁棒性对预训练阶段的影响。", "motivation": "研究旨在优化持续预训练中的语音建模，通过系统性地检查模型架构、数据表示和训练鲁棒性如何影响将现有预训练语言模型适应到语音模态的预训练阶段。", "method": "本文系统地检查了模型架构、数据表示和训练鲁棒性；实验了不同模型规模下的语音编码器和聚类粒度；分析了聚类分布和音素对齐；并探讨了聚类数据选择对模型鲁棒性的影响。", "result": "实验结果表明，最佳离散化策略随模型容量而变化；离散词汇的有效使用揭示了语言和副语言模式；聚类数据选择与目标应用之间的领域匹配对模型鲁棒性至关重要。", "conclusion": "持续预训练中优化SLMs的语音建模需要考虑模型架构、数据表示、训练鲁棒性、聚类粒度、离散词汇的使用以及聚类数据选择的领域匹配等因素。"}}
{"id": "2509.05360", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05360", "abs": "https://arxiv.org/abs/2509.05360", "authors": ["Jerry Li", "Evangelos Papalexakis"], "title": "Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated effectiveness across a wide\nvariety of tasks involving natural language, however, a fundamental problem of\nhallucinations still plagues these models, limiting their trustworthiness in\ngenerating consistent, truthful information. Detecting hallucinations has\nquickly become an important topic, with various methods such as uncertainty\nestimation, LLM Judges, retrieval augmented generation (RAG), and consistency\nchecks showing promise. Many of these methods build upon foundational metrics,\nsuch as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth\nnecessary to detect hallucinations effectively. In this work, we propose a\nnovel approach inspired by ROUGE that constructs an N-Gram frequency tensor\nfrom LLM-generated text. This tensor captures richer semantic structure by\nencoding co-occurrence patterns, enabling better differentiation between\nfactual and hallucinated content. We demonstrate this by applying tensor\ndecomposition methods to extract singular values from each mode and use these\nas input features to train a multi-layer perceptron (MLP) binary classifier for\nhallucinations. Our method is evaluated on the HaluEval dataset and\ndemonstrates significant improvements over traditional baselines, as well as\ncompetitive performance against state-of-the-art LLM judges.", "AI": {"tldr": "本文提出了一种新颖的基于N-Gram频率张量的方法来检测大型语言模型（LLM）的幻觉，该方法通过张量分解和MLP分类器，在HaluEval数据集上取得了显著优于传统基线的性能，并与SOTA LLM裁判器具有竞争力。", "motivation": "LLM幻觉问题严重限制了其生成信息的可靠性。现有幻觉检测方法，如不确定性估计、LLM裁判器和RAG，虽有前景，但其基础指标（如ROUGE、BERTScore、Perplexity）往往缺乏检测幻觉所需的语义深度。", "method": "该研究提出了一种受ROUGE启发的N-Gram频率张量构建方法，从LLM生成的文本中捕获更丰富的语义结构和共现模式。然后，通过张量分解方法提取每个模式的奇异值，并将这些奇异值作为输入特征，训练一个多层感知器（MLP）二元分类器来检测幻觉。", "result": "该方法在HaluEval数据集上进行了评估，结果显示其比传统基线有显著改进，并且与最先进的LLM裁判器相比，也展现出具有竞争力的性能。", "conclusion": "通过构建N-Gram频率张量并结合张量分解和MLP分类，本文提出的方法能够有效捕获文本中更深层的语义结构，从而更准确地区分事实性内容和幻觉内容，显著提升了LLM幻觉检测的性能。"}}
{"id": "2509.05385", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05385", "abs": "https://arxiv.org/abs/2509.05385", "authors": ["Jiacheng Wei", "Faguo Wu", "Xiao Zhang"], "title": "A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs", "comment": "11 pages, 7 figures, conference", "summary": "Large language models are unable to continuously adapt and learn from new\ndata during reasoning at inference time. To address this limitation, we propose\nthat complex reasoning tasks be decomposed into atomic subtasks and introduce\nSAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive\nupdates during reasoning at inference time. SAGE consists of three key\ncomponents: (1) a Trigger module that detects reasoning failures through\nmultiple evaluation metrics in real time; (2) a Trigger Buffer module that\nclusters anomaly samples using a streaming clustering process with HDBSCAN,\nfollowed by stability checks and similarity-based merging; and (3) a Lora Store\nmodule that dynamically optimizes parameter updates with an adapter pool for\nknowledge retention. Evaluation results show that SAGE demonstrates excellent\naccuracy, robustness, and stability on the atomic reasoning subtask through\ndynamic knowledge updating during test time.", "AI": {"tldr": "SAGE是一个触发器引导的动态微调框架，使大型语言模型能够在推理时通过分解任务和动态知识更新来持续适应和学习新数据。", "motivation": "大型语言模型在推理时无法持续适应新数据并从中学习，这限制了它们在复杂推理任务中的表现。", "method": "将复杂推理任务分解为原子子任务。提出SAGE框架，包含三个核心组件：1) 触发器模块：通过多项评估指标实时检测推理失败；2) 触发器缓冲区模块：使用HDBSCAN流式聚类异常样本，并进行稳定性检查和基于相似度的合并；3) Lora存储模块：通过适配器池动态优化参数更新以保留知识。", "result": "评估结果表明，SAGE通过在测试时动态更新知识，在原子推理子任务上表现出卓越的准确性、鲁棒性和稳定性。", "conclusion": "SAGE框架通过动态知识更新，成功地使大型语言模型能够在推理时进行自适应学习，解决了其无法持续适应新数据的限制。"}}
{"id": "2509.05396", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05396", "abs": "https://arxiv.org/abs/2509.05396", "authors": ["Andrea Wynn", "Harsh Satija", "Gillian Hadfield"], "title": "Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate", "comment": "ICML MAS Workshop 2025", "summary": "While multi-agent debate has been proposed as a promising strategy for\nimproving AI reasoning ability, we find that debate can sometimes be harmful\nrather than helpful. The prior work has exclusively focused on debates within\nhomogeneous groups of agents, whereas we explore how diversity in model\ncapabilities influences the dynamics and outcomes of multi-agent interactions.\nThrough a series of experiments, we demonstrate that debate can lead to a\ndecrease in accuracy over time -- even in settings where stronger (i.e., more\ncapable) models outnumber their weaker counterparts. Our analysis reveals that\nmodels frequently shift from correct to incorrect answers in response to peer\nreasoning, favoring agreement over challenging flawed reasoning. These results\nhighlight important failure modes in the exchange of reasons during multi-agent\ndebate, suggesting that naive applications of debate may cause performance\ndegradation when agents are neither incentivized nor adequately equipped to\nresist persuasive but incorrect reasoning.", "AI": {"tldr": "多智能体辩论有时弊大于利，特别是在模型能力异构的环境中，可能导致准确性下降，因为智能体倾向于达成一致而非纠正错误推理。", "motivation": "以往研究主要关注同质智能体群体的辩论，但发现辩论可能有害。本研究旨在探索模型能力多样性如何影响多智能体交互的动态和结果。", "method": "通过一系列实验，研究了模型能力多样性（强弱模型）对多智能体辩论结果的影响，并观察了准确性随时间的变化。", "result": "辩论会导致准确性随时间下降，即使强模型数量多于弱模型。分析显示，模型经常从正确答案转向错误答案，以响应同伴推理，倾向于达成一致而非挑战有缺陷的推理。", "conclusion": "多智能体辩论的简单应用可能导致性能下降，当智能体既没有被激励也没有充分配备来抵抗有说服力但错误的推理时，尤其如此。"}}
{"id": "2509.05304", "categories": ["eess.SY", "astro-ph.IM", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05304", "abs": "https://arxiv.org/abs/2509.05304", "authors": ["Steve Chien", "Itai Zilberstein", "Alberto Candela", "David Rijlaarsdam", "Tom Hendrix", "Aubrey Dunne", "Aragon Oriol", "Miquel Juan Puig"], "title": "Flight of Dynamic Targeting on the CogniSAT-6 Spacecraft", "comment": null, "summary": "Dynamic targeting (DT) is a spacecraft autonomy concept in which sensor data\nis acquired and rapidly analyzed and used to drive subsequent observation. We\ndescribe the low Earth orbit application of this approach in which lookahead\nimagery is analyzed to detect clouds, thermal anomalies, or land use cases to\ndrive higher quality near nadir imaging. Use cases for such a capability\ninclude: cloud avoidance, storm hunting, search for planetary boundary layer\nevents, plume study, and beyond. The DT concept requires a lookahead sensor or\nagility to use a primary sensor in such a mode, edge computing to analyze\nimages rapidly onboard, and a primary followup sensor. Additionally, an\ninter-satellite or low latency communications link can be leveraged for cross\nplatform tasking. We describe implementation in progress to fly DT in early\n2025 on the CogniSAT-6 (Ubotica/Open Cosmos) spacecraft that launched in March\n2024 on the SpaceX Transporter-10 launch.", "AI": {"tldr": "动态目标瞄准（DT）是一种航天器自主概念，利用前瞻图像快速分析以驱动后续高质量成像，旨在避云、追踪风暴等，并计划于2025年初在CogniSAT-6上进行飞行演示。", "motivation": "提高近天底成像的质量，通过避免障碍物（如云层）或识别特定目标（如热异常、土地利用案例、风暴、行星边界层事件、羽流）来优化后续观测，实现航天器的自主决策能力。", "method": "该方法需要一个前瞻传感器（或将主传感器用于前瞻模式）、用于快速机载图像分析的边缘计算能力，以及一个主后续传感器。此外，还可以利用星间或低延迟通信链路进行跨平台任务分配。目前正在CogniSAT-6（于2024年3月发射）上实施，计划于2025年初进行飞行演示。", "result": "描述了动态目标瞄准（DT）的概念及其在低地球轨道（LEO）的应用场景，并介绍了在CogniSAT-6航天器上实施该概念的进展和计划于2025年初进行的飞行演示。", "conclusion": "动态目标瞄准（DT）是一个可行的航天器自主概念，能够通过快速分析传感器数据来优化后续观测，具有广泛的应用潜力，并已进入实际飞行演示的实施阶段。"}}
{"id": "2509.05314", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05314", "abs": "https://arxiv.org/abs/2509.05314", "authors": ["Ying Li", "Xiaobao Wei", "Xiaowei Chi", "Yuming Li", "Zhongyu Zhao", "Hao Wang", "Ningning Ma", "Ming Lu", "Shanghang Zhang"], "title": "ManipDreamer3D : Synthesizing Plausible Robotic Manipulation Video with Occupancy-aware 3D Trajectory", "comment": "8pages; 7figures; 4 tables", "summary": "Data scarcity continues to be a major challenge in the field of robotic\nmanipulation. Although diffusion models provide a promising solution for\ngenerating robotic manipulation videos, existing methods largely depend on 2D\ntrajectories, which inherently face issues with 3D spatial ambiguity. In this\nwork, we present a novel framework named ManipDreamer3D for generating\nplausible 3D-aware robotic manipulation videos from the input image and the\ntext instruction. Our method combines 3D trajectory planning with a\nreconstructed 3D occupancy map created from a third-person perspective, along\nwith a novel trajectory-to-video diffusion model. Specifically, ManipDreamer3D\nfirst reconstructs the 3D occupancy representation from the input image and\nthen computes an optimized 3D end-effector trajectory, minimizing path length\nwhile avoiding collisions. Next, we employ a latent editing technique to create\nvideo sequences from the initial image latent and the optimized 3D trajectory.\nThis process conditions our specially trained trajectory-to-video diffusion\nmodel to produce robotic pick-and-place videos. Our method generates robotic\nvideos with autonomously planned plausible 3D trajectories, significantly\nreducing human intervention requirements. Experimental results demonstrate\nsuperior visual quality compared to existing methods.", "AI": {"tldr": "ManipDreamer3D是一个新颖的框架，它结合3D轨迹规划和扩散模型，从输入图像和文本指令生成逼真的3D感知机器人操作视频，显著减少人工干预。", "motivation": "机器人操作领域面临数据稀缺的挑战，现有方法主要依赖2D轨迹，存在3D空间模糊性问题。", "method": "ManipDreamer3D首先从输入图像重建3D占用图，然后计算优化的3D末端执行器轨迹（最小化路径长度并避免碰撞）。接着，它利用潜在编辑技术，结合初始图像潜在表示和优化后的3D轨迹，条件化一个专门训练的轨迹到视频扩散模型，以生成机器人抓取和放置视频。", "result": "该方法生成具有自主规划的合理3D轨迹的机器人视频，显著减少了人工干预需求。实验结果表明，与现有方法相比，其视觉质量更优越。", "conclusion": "ManipDreamer3D通过生成高质量、自主规划的3D感知机器人操作视频，有效解决了数据稀缺和3D模糊性问题。"}}
{"id": "2509.05307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05307", "abs": "https://arxiv.org/abs/2509.05307", "authors": ["Sachin Chhabra", "Hemanth Venkateswara", "Baoxin Li"], "title": "Label Smoothing++: Enhanced Label Regularization for Training Neural Networks", "comment": "Published in British Machine Vision Conference (BMVC), 2024", "summary": "Training neural networks with one-hot target labels often results in\noverconfidence and overfitting. Label smoothing addresses this issue by\nperturbing the one-hot target labels by adding a uniform probability vector to\ncreate a regularized label. Although label smoothing improves the network's\ngeneralization ability, it assigns equal importance to all the non-target\nclasses, which destroys the inter-class relationships. In this paper, we\npropose a novel label regularization training strategy called Label\nSmoothing++, which assigns non-zero probabilities to non-target classes and\naccounts for their inter-class relationships. Our approach uses a fixed label\nfor the target class while enabling the network to learn the labels associated\nwith non-target classes. Through extensive experiments on multiple datasets, we\ndemonstrate how Label Smoothing++ mitigates overconfident predictions while\npromoting inter-class relationships and generalization capabilities.", "AI": {"tldr": "本文提出了一种名为Label Smoothing++的新型标签正则化策略，通过考虑非目标类别之间的关系来改进传统的标签平滑方法，从而减少过拟合和提高泛化能力。", "motivation": "传统的独热编码标签会导致神经网络过分自信和过拟合。标签平滑（Label Smoothing）通过添加均匀概率向量来缓解这一问题，但它对所有非目标类别赋予相同的权重，忽略了它们之间的类间关系。", "method": "Label Smoothing++策略为非目标类别分配非零概率，并考虑它们之间的类间关系。它为目标类别使用固定标签，同时允许网络学习与非目标类别相关的标签。", "result": "在多个数据集上的实验表明，Label Smoothing++能够有效缓解过度自信的预测，同时促进类间关系和提高网络的泛化能力。", "conclusion": "Label Smoothing++是一种新颖且有效的标签正则化训练策略，通过更好地处理非目标类别的概率分配和类间关系，优于传统标签平滑方法。"}}
{"id": "2509.05323", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.05323", "abs": "https://arxiv.org/abs/2509.05323", "authors": ["Adam Cole", "Mick Grierson"], "title": "Attention of a Kiss: Exploring Attention Maps in Video Diffusion for XAIxArts", "comment": "3rd international workshop on eXplainable AI for the Arts (XAIxArts)\n  at the ACM Creativity and Cognition Conference 2025", "summary": "This paper presents an artistic and technical investigation into the\nattention mechanisms of video diffusion transformers. Inspired by early video\nartists who manipulated analog video signals to create new visual aesthetics,\nthis study proposes a method for extracting and visualizing cross-attention\nmaps in generative video models. Built on the open-source Wan model, our tool\nprovides an interpretable window into the temporal and spatial behavior of\nattention in text-to-video generation. Through exploratory probes and an\nartistic case study, we examine the potential of attention maps as both\nanalytical tools and raw artistic material. This work contributes to the\ngrowing field of Explainable AI for the Arts (XAIxArts), inviting artists to\nreclaim the inner workings of AI as a creative medium.", "AI": {"tldr": "本文对视频扩散Transformer的注意力机制进行了艺术和技术研究，提出了一种提取和可视化生成视频模型中交叉注意力图的方法，旨在将其作为分析工具和艺术创作材料，并促进AI在艺术领域的解释性应用。", "motivation": "研究灵感来源于早期通过操纵模拟视频信号创造新视觉美学的视频艺术家，以及对生成视频模型中注意力机制可解释性的需求，特别是在艺术创作背景下。", "method": "该研究提出了一种提取和可视化生成视频模型中交叉注意力图的方法。该工具基于开源的Wan模型构建，提供了一个可解释的窗口来观察文本到视频生成中注意力的时空行为。", "result": "通过探索性探究和艺术案例研究，论文检验了注意力图作为分析工具和原始艺术材料的潜力。", "conclusion": "这项工作为艺术领域的可解释人工智能（XAIxArts）做出了贡献，并邀请艺术家将AI的内部运作重新视为一种创意媒介。"}}
{"id": "2509.05374", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05374", "abs": "https://arxiv.org/abs/2509.05374", "authors": ["Zhiqiang Yuan", "Jinchao Zhang", "Jie Zhou"], "title": "A Synthetic-to-Real Dehazing Method based on Domain Unification", "comment": "ICME 2025 Accept", "summary": "Due to distribution shift, the performance of deep learning-based method for\nimage dehazing is adversely affected when applied to real-world hazy images. In\nthis paper, we find that such deviation in dehazing task between real and\nsynthetic domains may come from the imperfect collection of clean data. Owing\nto the complexity of the scene and the effect of depth, the collected clean\ndata cannot strictly meet the ideal conditions, which makes the atmospheric\nphysics model in the real domain inconsistent with that in the synthetic\ndomain. For this reason, we come up with a synthetic-to-real dehazing method\nbased on domain unification, which attempts to unify the relationship between\nthe real and synthetic domain, thus to let the dehazing model more in line with\nthe actual situation. Extensive experiments qualitatively and quantitatively\ndemonstrate that the proposed dehazing method significantly outperforms\nstate-of-the-art methods on real-world images.", "AI": {"tldr": "本文提出了一种基于域统一的合成到真实去雾方法，以解决深度学习去雾模型在真实图像上因域偏移（源于不完美的干净数据收集）而性能下降的问题。", "motivation": "深度学习去雾方法在应用于真实世界的雾霾图像时，由于分布偏移，性能会受到不利影响。研究发现，这种真实与合成域之间的去雾任务偏差可能源于干净数据收集的不完美，导致大气物理模型在真实域和合成域之间不一致。", "method": "提出了一种基于域统一的合成到真实去雾方法，旨在统一真实域和合成域之间的关系，使去雾模型更符合实际情况。", "result": "大量的定性和定量实验表明，所提出的去雾方法在真实世界图像上显著优于现有最先进的方法。", "conclusion": "通过统一真实域和合成域之间的关系，该方法有效地解决了因干净数据收集不完美导致的大气物理模型不一致问题，从而显著提升了去雾模型在真实图像上的性能。"}}
{"id": "2509.05425", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05425", "abs": "https://arxiv.org/abs/2509.05425", "authors": ["Jessica M. Lundin", "Ada Zhang", "David Adelani", "Cody Carroll"], "title": "No Translation Needed: Forecasting Quality from Fertility and Metadata", "comment": null, "summary": "We show that translation quality can be predicted with surprising accuracy\n\\textit{without ever running the translation system itself}. Using only a\nhandful of features, token fertility ratios, token counts, and basic linguistic\nmetadata (language family, script, and region), we can forecast ChrF scores for\nGPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient\nboosting models achieve favorable performance ($R^{2}=0.66$ for\nXX$\\rightarrow$English and $R^{2}=0.72$ for English$\\rightarrow$XX). Feature\nimportance analyses reveal that typological factors dominate predictions into\nEnglish, while fertility plays a larger role for translations into diverse\ntarget languages. These findings suggest that translation quality is shaped by\nboth token-level fertility and broader linguistic typology, offering new\ninsights for multilingual evaluation and quality estimation.", "AI": {"tldr": "研究表明，无需实际运行翻译系统，仅使用少量特征（如词元生育率、词元计数和基本语言元数据）即可高精度预测翻译质量，为多语言评估和质量估计提供了新见解。", "motivation": "传统上，评估翻译质量需要运行翻译系统，这可能耗时且成本高昂。本研究旨在探索一种无需运行翻译系统即可预测翻译质量的方法，以提供多语言评估和质量估计的新视角。", "method": "研究使用了词元生育率、词元计数和基本语言元数据（语系、文字、区域）等少量特征。通过梯度提升模型（Gradient Boosting Models）对GPT-4o在FLORES-200基准上203种语言的翻译进行ChrF分数预测。", "result": "梯度提升模型表现出色，对于XX→英语的预测，R²达到0.66；对于英语→XX的预测，R²达到0.72。特征重要性分析显示，在翻译成英语时，类型学因素（typological factors）起主导作用；而在翻译成多种目标语言时，生育率（fertility）则扮演更重要的角色。", "conclusion": "研究得出结论，翻译质量受到词元级别的生育率和更广泛的语言类型学因素的共同影响。这些发现为多语言评估和质量估计提供了新的见解。"}}
{"id": "2509.05463", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05463", "abs": "https://arxiv.org/abs/2509.05463", "authors": ["Simone Pirrera", "Lorenzo Calogero", "Francesco Gabriele", "Diego Regruto", "Alessandro Rizzo", "Gianluca Setti"], "title": "A Fully Analog Implementation of Model Predictive Control with Application to Buck Converters", "comment": null, "summary": "This paper proposes a novel approach to design analog electronic circuits\nthat implement Model Predictive Control (MPC) policies for plants described by\naffine models. The combination of state-of-the-art approaches to define\nreduced-complexity Explicit MPC (EMPC) is employed to realize an analog circuit\ncharacterized by a limited amount of low-latency and commercially available\ncomponents. The practical feasibility and effectiveness of the proposed\napproach are demonstrated through its application in the design of an advanced\ncontroller for DC-DC Buck converters. We formally analyze the stability of the\nobtained system and conduct extensive numerical simulations to demonstrate that\nit is capable of achieving outstanding load disturbance rejection performance,\noutclassing standard approaches.", "AI": {"tldr": "本文提出了一种设计模拟电路以实现模型预测控制（MPC）策略的新方法，用于仿射模型系统，并展示了其在DC-DC降压转换器中的卓越性能。", "motivation": "研究动机是为了实现低延迟、低复杂度的MPC策略，通过利用市售模拟元件构建模拟电路，以应对仿射模型系统。", "method": "本文提出了一种新颖的方法来设计模拟电子电路，以实现针对仿射模型的MPC策略。该方法结合了最先进的简化显式MPC（EMPC）方法，以实现使用有限数量低延迟、市售元件的模拟电路。", "result": "通过将其应用于DC-DC降压转换器的先进控制器设计，证明了所提方法的实用可行性和有效性。系统稳定性得到了正式分析，广泛的数值模拟表明，该方法能够实现出色的负载扰动抑制性能，超越了标准方法。", "conclusion": "所提出的模拟MPC设计方法对于仿射模型系统是可行且有效的，能够实现卓越的性能（如负载扰动抑制），并具有良好的稳定性，优于传统方法。"}}
{"id": "2509.05315", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05315", "abs": "https://arxiv.org/abs/2509.05315", "authors": ["Petros Loukas", "David Bassir", "Savvas Chatzichristofis", "Angelos Amanatiadis"], "title": "Evaluation of Large Language Models for Anomaly Detection in Autonomous Vehicles", "comment": null, "summary": "The rapid evolution of large language models (LLMs) has pushed their\nboundaries to many applications in various domains. Recently, the research\ncommunity has started to evaluate their potential adoption in autonomous\nvehicles and especially as complementary modules in the perception and planning\nsoftware stacks. However, their evaluation is limited in synthetic datasets or\nmanually driving datasets without the ground truth knowledge and more\nprecisely, how the current perception and planning algorithms would perform in\nthe cases under evaluation. For this reason, this work evaluates LLMs on\nreal-world edge cases where current autonomous vehicles have been proven to\nfail. The proposed architecture consists of an open vocabulary object detector\ncoupled with prompt engineering and large language model contextual reasoning.\nWe evaluate several state-of-the-art models against real edge cases and provide\nqualitative comparison results along with a discussion on the findings for the\npotential application of LLMs as anomaly detectors in autonomous vehicles.", "AI": {"tldr": "本文评估了大型语言模型（LLMs）在真实世界自动驾驶边缘案例中的潜力，特别是作为异常检测器，结合开放词汇目标检测和提示工程。", "motivation": "LLMs在自动驾驶领域的应用潜力尚未在真实世界边缘案例中得到充分评估，现有研究多限于合成或手动驾驶数据集，缺乏对当前感知和规划算法在这些案例中表现的精确了解。", "method": "提出了一种结合开放词汇目标检测器、提示工程和大型语言模型上下文推理的架构。在真实世界的边缘案例上评估了几个最先进的模型。", "result": "提供了针对真实边缘案例的定性比较结果，并讨论了LLMs作为自动驾驶车辆中异常检测器的潜在应用前景。", "conclusion": "LLMs在处理自动驾驶车辆的真实世界边缘案例方面表现出潜力，可作为有效的异常检测器。"}}
{"id": "2509.05317", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05317", "abs": "https://arxiv.org/abs/2509.05317", "authors": ["Isac Holm"], "title": "VILOD: A Visual Interactive Labeling Tool for Object Detection", "comment": "Master's project", "summary": "The advancement of Object Detection (OD) using Deep Learning (DL) is often\nhindered by the significant challenge of acquiring large, accurately labeled\ndatasets, a process that is time-consuming and expensive. While techniques like\nActive Learning (AL) can reduce annotation effort by intelligently querying\ninformative samples, they often lack transparency, limit the strategic insight\nof human experts, and may overlook informative samples not aligned with an\nemployed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)\napproaches integrating human intelligence and intuition throughout the machine\nlearning life-cycle have gained traction. Leveraging Visual Analytics (VA),\neffective interfaces can be created to facilitate this human-AI collaboration.\nThis thesis explores the intersection of these fields by developing and\ninvestigating \"VILOD: A Visual Interactive Labeling tool for Object Detection\".\nVILOD utilizes components such as a t-SNE projection of image features,\ntogether with uncertainty heatmaps and model state views. Enabling users to\nexplore data, interpret model states, AL suggestions, and implement diverse\nsample selection strategies within an iterative HITL workflow for OD. An\nempirical investigation using comparative use cases demonstrated how VILOD,\nthrough its interactive visualizations, facilitates the implementation of\ndistinct labeling strategies by making the model's state and dataset\ncharacteristics more interpretable (RQ1). The study showed that different\nvisually-guided labeling strategies employed within VILOD result in competitive\nOD performance trajectories compared to an automated uncertainty sampling AL\nbaseline (RQ2). This work contributes a novel tool and empirical insight into\nmaking the HITL-AL workflow for OD annotation more transparent, manageable, and\npotentially more effective.", "AI": {"tldr": "本文提出VILOD，一个用于目标检测的视觉交互式标注工具，通过整合人类智能和视觉分析，使人机协作式主动学习（HITL-AL）流程更加透明、可管理和高效。", "motivation": "深度学习目标检测面临获取大量准确标注数据的挑战，此过程耗时且昂贵。主动学习（AL）虽能减少标注工作，但缺乏透明度，限制了人类专家的战略洞察力。因此，需要结合人类智能和直觉的人机协作（HITL）方法，并利用视觉分析（VA）创建有效界面来解决这些问题。", "method": "本文开发并研究了“VILOD：一个用于目标检测的视觉交互式标注工具”。VILOD利用t-SNE图像特征投影、不确定性热图和模型状态视图等组件。它使用户能够在迭代的HITL工作流中探索数据、解释模型状态、AL建议，并实施多样化的样本选择策略。通过比较用例进行了实证调查。", "result": "研究表明，VILOD通过其交互式可视化，使模型状态和数据集特性更易于解释，从而促进了不同标注策略的实施（RQ1）。VILOD中采用的不同视觉引导标注策略，与自动不确定性采样AL基线相比，实现了具有竞争力的目标检测性能轨迹（RQ2）。", "conclusion": "这项工作贡献了一个新颖的工具和实证见解，使目标检测标注的HITL-AL工作流程更加透明、可管理，并可能更有效。"}}
{"id": "2509.05324", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05324", "abs": "https://arxiv.org/abs/2509.05324", "authors": ["Rongqian Chen", "Shu Hong", "Rifatul Islam", "Mahdi Imani", "G. Gary Tan", "Tian Lan"], "title": "Perception Graph for Cognitive Attack Reasoning in Augmented Reality", "comment": "Accepted by ACM MobiHoc XR Security workshop 2025", "summary": "Augmented reality (AR) systems are increasingly deployed in tactical\nenvironments, but their reliance on seamless human-computer interaction makes\nthem vulnerable to cognitive attacks that manipulate a user's perception and\nseverely compromise user decision-making. To address this challenge, we\nintroduce the Perception Graph, a novel model designed to reason about human\nperception within these systems. Our model operates by first mimicking the\nhuman process of interpreting key information from an MR environment and then\nrepresenting the outcomes using a semantically meaningful structure. We\ndemonstrate how the model can compute a quantitative score that reflects the\nlevel of perception distortion, providing a robust and measurable method for\ndetecting and analyzing the effects of such cognitive attacks.", "AI": {"tldr": "本文提出了一种名为“感知图”的新模型，用于在战术增强现实（AR）系统中推理人类感知，以检测和分析认知攻击造成的感知扭曲。", "motivation": "增强现实（AR）系统在战术环境中日益部署，但其对无缝人机交互的依赖使其容易受到认知攻击。这些攻击会操纵用户的感知，严重损害用户决策，因此需要一种方法来应对这一挑战。", "method": "引入了“感知图”模型。该模型首先模仿人类从MR环境中解释关键信息的过程，然后使用语义上有意义的结构来表示结果。通过这种方式，模型能够计算一个量化分数，反映感知扭曲的程度。", "result": "该模型能够计算一个反映感知扭曲程度的量化分数，提供了一种稳健且可衡量的方法来检测和分析此类认知攻击的影响。", "conclusion": "“感知图”模型通过量化感知扭曲，为检测和分析战术AR系统中认知攻击的效果提供了一种新颖且可衡量的方法。"}}
{"id": "2509.05736", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.05736", "abs": "https://arxiv.org/abs/2509.05736", "authors": ["Shraddha Chavan", "Kunal N. Chaudhury"], "title": "Stabilizing RED using the Koopman Operator", "comment": "Accepted to IEEE Signal Processing Letters, 2025", "summary": "The widely used RED (Regularization-by-Denoising) framework uses pretrained\ndenoisers as implicit regularizers for model-based reconstruction. Although RED\ngenerally yields high-fidelity reconstructions, the use of black-box denoisers\ncan sometimes lead to instability. In this letter, we propose a data-driven\nmechanism to stabilize RED using the Koopman operator, a classical tool for\nanalyzing dynamical systems. Specifically, we use the operator to capture the\nlocal dynamics of RED in a low-dimensional feature space, and its spectral\nradius is used to detect instability and formulate an adaptive step-size rule\nthat is model-agnostic, has modest overhead, and requires no retraining. We\ntest this with several pretrained denoisers to demonstrate the effectiveness of\nthe proposed Koopman stabilization.", "AI": {"tldr": "本文提出一种数据驱动的Koopman算子方法来稳定RED（Regularization-by-Denoising）框架，解决其因黑盒去噪器导致的潜在不稳定性。该方法通过捕获RED的局部动力学并利用算子谱半径自适应调整步长，实现模型无关、低开销且无需重新训练的稳定化。", "motivation": "RED框架虽然能实现高保真重建，但由于使用了黑盒去噪器，有时会导致系统不稳定。", "method": "本文提出使用Koopman算子来稳定RED。具体而言，该方法利用Koopman算子捕获RED在低维特征空间中的局部动力学，并使用其谱半径来检测不稳定性。基于此，制定了一个自适应步长规则，该规则具有模型无关、开销适中且无需重新训练的特点。", "result": "通过对多种预训练去噪器进行测试，结果表明所提出的Koopman稳定化方法是有效的。", "conclusion": "Koopman算子提供了一种有效的数据驱动机制，能够稳定RED框架，克服其潜在的不稳定性，同时保持模型无关、低开销和无需重新训练的优势。"}}
{"id": "2509.05440", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05440", "abs": "https://arxiv.org/abs/2509.05440", "authors": ["Logan Lawrence", "Ashton Williamson", "Alexander Shelton"], "title": "Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too", "comment": "12 pages, 18 tables, 1 figure", "summary": "As large-language models have been increasingly used as automatic raters for\nevaluating free-form content, including document summarization, dialog, and\nstory generation, work has been dedicated to evaluating such models by\nmeasuring their correlations with human judgment. For \\textit{sample-level}\nperformance, methods which operate by using pairwise comparisons between\nmachine-generated text perform well but often lack the ability to assign\nabsolute scores to individual summaries, an ability crucial for use cases that\nrequire thresholding. In this work, we propose a direct-scoring method which\nuses synthetic summaries to act as pairwise machine rankings at test time. We\nshow that our method performs comparably to state-of-the-art pairwise\nevaluators in terms of axis-averaged sample-level correlations on the SummEval\n(\\textbf{+0.03}), TopicalChat (\\textbf{-0.03}), and HANNA (\\textbf{+0.05})\nmeta-evaluation benchmarks, and release the synthetic in-context summaries as\ndata to facilitate future work.", "AI": {"tldr": "本文提出了一种直接评分方法，通过使用合成摘要作为测试时的配对机器排名，以解决大型语言模型作为自动评估器在自由形式内容评估中缺乏绝对分数的问题，并实现了与现有最佳配对评估器相当的性能。", "motivation": "大型语言模型越来越多地被用作自由形式内容（如文档摘要、对话和故事生成）的自动评估器。现有评估这些模型性能的方法（特别是样本级别的配对比较方法）虽然表现良好，但通常无法为单个摘要分配绝对分数，而这种能力对于需要设置阈值的应用场景至关重要。", "method": "研究者提出了一种直接评分方法。该方法在测试时利用合成摘要作为配对机器排名，从而实现对单个摘要的直接评分。", "result": "该方法在SummEval (+0.03)、TopicalChat (-0.03) 和 HANNA (+0.05) 等元评估基准上，在轴平均样本级相关性方面，表现与最先进的配对评估器相当。此外，研究者还发布了上下文合成摘要数据，以促进未来的研究。", "conclusion": "本文成功开发了一种直接评分方法，解决了现有配对评估方法无法提供绝对分数的问题，同时在评估性能上与现有技术相当。这使得大型语言模型作为自动评估器在需要阈值判断的应用中更具实用性。"}}
{"id": "2509.05482", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05482", "abs": "https://arxiv.org/abs/2509.05482", "authors": ["Mohammad Hussein Yoosefian Nooshabadi", "Laurent Lessard"], "title": "State Estimation for Linear Systems with Non-Gaussian Measurement Noise via Dynamic Programming", "comment": null, "summary": "We propose a new recursive estimator for linear dynamical systems under\nGaussian process noise and non-Gaussian measurement noise. Specifically, we\ndevelop an approximate maximum a posteriori (MAP) estimator using dynamic\nprogramming and tools from convex analysis. Our approach does not rely on\nrestrictive noise assumptions and employs a Bellman-like update instead of a\nBayesian update. Our proposed estimator is computationally efficient, with only\nmodest overhead compared to a standard Kalman filter. Simulations demonstrate\nthat our estimator achieves lower root mean squared error (RMSE) than the\nKalman filter and has comparable performance to state-of-the-art estimators,\nwhile requiring significantly less computational power.", "AI": {"tldr": "本文提出了一种针对高斯过程噪声和非高斯测量噪声下线性动态系统的新型递归估计器，该估计器通过近似最大后验（MAP）估计、动态规划和凸分析实现，计算效率高且性能优于卡尔曼滤波器。", "motivation": "现有滤波器（如卡尔曼滤波器）在非高斯测量噪声条件下性能受限，且通常依赖于严格的噪声假设。本研究旨在开发一种不依赖此类假设、计算高效且性能优越的递归估计器。", "method": "本文提出了一种近似最大后验（MAP）估计器，利用动态规划和凸分析工具。该方法采用类似贝尔曼方程的更新方式，而非传统的贝叶斯更新。", "result": "仿真结果表明，所提出的估计器比卡尔曼滤波器具有更低的均方根误差（RMSE），与最先进的估计器性能相当，但计算功耗显著降低。", "conclusion": "所提出的递归估计器在处理高斯过程噪声和非高斯测量噪声的线性动态系统时，具有计算效率高、准确性好（低RMSE）的优点，且对噪声假设不敏感，是一种有前景的解决方案。"}}
{"id": "2509.05338", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05338", "abs": "https://arxiv.org/abs/2509.05338", "authors": ["Atsushi Masumori", "Norihiro Maruyama", "Itsuki Doi", "johnsmith", "Hiroki Sato", "Takashi Ikegami"], "title": "Plantbot: Integrating Plant and Robot through LLM Modular Agent Networks", "comment": null, "summary": "We introduce Plantbot, a hybrid lifeform that connects a living plant with a\nmobile robot through a network of large language model (LLM) modules. Each\nmodule - responsible for sensing, vision, dialogue, or action - operates\nasynchronously and communicates via natural language, enabling seamless\ninteraction across biological and artificial domains. This architecture\nleverages the capacity of LLMs to serve as hybrid interfaces, where natural\nlanguage functions as a universal protocol, translating multimodal data (soil\nmoisture, temperature, visual context) into linguistic messages that coordinate\nsystem behaviors. The integrated network transforms plant states into robotic\nactions, installing normativity essential for agency within the sensor-motor\nloop. By combining biological and robotic elements through LLM-mediated\ncommunication, Plantbot behaves as an embodied, adaptive agent capable of\nresponding autonomously to environmental conditions. This approach suggests\npossibilities for a new model of artificial life, where decentralized, LLM\nmodules coordination enable novel interactions between biological and\nartificial systems.", "AI": {"tldr": "本文介绍了Plantbot，一种将活体植物与移动机器人通过大型语言模型（LLM）模块网络连接的混合生命体。它利用自然语言作为通用协议，实现生物和人工领域间的无缝交互，使其成为一个能自主适应环境的具身智能体。", "motivation": "研究动机是探索生物与人工系统之间的新型交互模式，并提出一种新型的人工生命模型，其中去中心化的LLM模块协调能够实现生物与人工系统之间的新颖互动。", "method": "Plantbot采用混合生命体架构，将活体植物与移动机器人连接。通过LLM模块网络（负责感知、视觉、对话、行动），这些模块异步运行并利用自然语言进行通信。自然语言作为通用协议，将多模态数据（土壤湿度、温度、视觉上下文）转换为语言信息，以协调系统行为。", "result": "Plantbot能够将植物状态转化为机器人行动，在传感器-运动循环中建立规范性，赋予其能动性。通过LLM介导的通信，Plantbot表现为一个具身、自适应的智能体，能够自主响应环境条件。", "conclusion": "该方法为人工生命提供了一种新模型，其中去中心化的LLM模块协调能够实现生物与人工系统之间的新颖互动，为未来生物与人工系统融合提供了可能性。"}}
{"id": "2509.05319", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05319", "abs": "https://arxiv.org/abs/2509.05319", "authors": ["Zhengda Li"], "title": "Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification", "comment": null, "summary": "Knowledge distillation (KD) is a widely used technique to transfer knowledge\nfrom a large teacher network to a smaller student model. Traditional KD uses a\nfixed balancing factor alpha as a hyperparameter to combine the hard-label\ncross-entropy loss with the soft-label distillation loss. However, a static\nalpha is suboptimal because the optimal trade-off between hard and soft\nsupervision can vary during training.\n  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.\nFirst we try to make alpha as learnable parameter that can be automatically\nlearned and optimized during training. Then we introduce a formula to reflect\nthe gap between the student and the teacher to compute alpha dynamically,\nguided by student-teacher discrepancies, and further introduce a Context-Aware\nModule (CAM) using MLP + Attention to adaptively reweight class-wise teacher\noutputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as\nstudent demonstrate that our approach achieves superior accuracy compared to\nfixed-weight KD baselines, and yields more stable convergence.", "AI": {"tldr": "本文提出自适应知识蒸馏（AKD）框架，通过学习和动态计算平衡因子alpha，并引入上下文感知模块（CAM）根据师生差异自适应调整教师输出的类别权重，以优化知识蒸馏过程。", "motivation": "传统知识蒸馏使用固定的平衡因子alpha来结合硬标签和软标签损失，但这种固定值并非最优，因为硬监督和软监督之间的最佳权衡在训练过程中是变化的。", "method": "1. 将alpha作为可学习参数，在训练过程中自动优化。2. 引入一个公式，根据学生和教师之间的差异动态计算alpha。3. 引入一个上下文感知模块（CAM），结合多层感知机（MLP）和注意力机制，自适应地重新加权类别级别的教师输出。", "result": "在CIFAR-10数据集上，以ResNet-50为教师模型、ResNet-18为学生模型的实验表明，AKD方法比固定权重知识蒸馏基线取得了更高的准确性，并实现了更稳定的收敛。", "conclusion": "所提出的AKD框架通过自适应调整平衡因子和教师输出权重，有效解决了传统知识蒸馏中固定alpha的局限性，从而提升了学生模型的性能和训练稳定性。"}}
{"id": "2509.05325", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05325", "abs": "https://arxiv.org/abs/2509.05325", "authors": ["Liming Xu", "Yunbo Long", "Alexandra Brintrup"], "title": "SynDelay: A Synthetic Dataset for Delivery Delay Prediction", "comment": "This paper incldues 1 figure and 2 tables", "summary": "Artificial intelligence (AI) is transforming supply chain management, yet\nprogress in predictive tasks -- such as delivery delay prediction -- remains\nconstrained by the scarcity of high-quality, openly available datasets.\nExisting datasets are often proprietary, small, or inconsistently maintained,\nhindering reproducibility and benchmarking. We present SynDelay, a synthetic\ndataset designed for delivery delay prediction. Generated using an advanced\ngenerative model trained on real-world data, SynDelay preserves realistic\ndelivery patterns while ensuring privacy. Although not entirely free of noise\nor inconsistencies, it provides a challenging and practical testbed for\nadvancing predictive modelling. To support adoption, we provide baseline\nresults and evaluation metrics as initial benchmarks, serving as reference\npoints rather than state-of-the-art claims. SynDelay is publicly available\nthrough the Supply Chain Data Hub, an open initiative promoting dataset sharing\nand benchmarking in supply chain AI. We encourage the community to contribute\ndatasets, models, and evaluation practices to advance research in this area.\nAll code is openly accessible at https://supplychaindatahub.org.", "AI": {"tldr": "该论文提出了SynDelay，一个用于预测交货延迟的合成数据集。该数据集通过先进的生成模型基于真实数据生成，旨在解决高质量开放数据稀缺的问题，并提供基准结果以促进供应链AI领域的研究和基准测试。", "motivation": "人工智能正在改变供应链管理，但预测任务（如交货延迟预测）的进展受到高质量、开放可用数据集稀缺的限制。现有数据集通常是专有的、规模小或维护不一致，这阻碍了研究的重现性和基准测试。", "method": "研究人员创建了SynDelay，一个用于交货延迟预测的合成数据集。该数据集使用在真实世界数据上训练的先进生成模型生成，旨在保留真实的交货模式同时确保隐私。", "result": "SynDelay提供了一个具有挑战性和实用性的测试平台，用于推进预测建模。尽管存在一些噪音或不一致性，它仍然是一个有价值的资源。论文提供了基线结果和评估指标作为初始基准，并通过Supply Chain Data Hub公开可用。", "conclusion": "SynDelay通过提供一个公开可用的合成数据集，解决了供应链AI中预测任务（特别是交货延迟预测）的数据稀缺问题。它旨在促进该领域研究的重现性、基准测试和进步，并鼓励社区贡献数据集、模型和评估实践。"}}
{"id": "2509.05754", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.05754", "abs": "https://arxiv.org/abs/2509.05754", "authors": ["Qiang Ma", "Qingjie Meng", "Mengyun Qiao", "Paul M. Matthews", "Declan P. O'Regan", "Wenjia Bai"], "title": "CardiacFlow: 3D+t Four-Chamber Cardiac Shape Completion and Generation via Flow Matching", "comment": "Accepted by MICCAI 2025 (submitted version)", "summary": "Learning 3D+t shape completion and generation from multi-view cardiac\nmagnetic resonance (CMR) images requires a large amount of high-resolution 3D\nwhole-heart segmentations (WHS) to capture shape priors. In this work, we\nleverage flow matching techniques to learn deep generative flows for\naugmentation, completion, and generation of 3D+t shapes of four cardiac\nchambers represented implicitly by segmentations. Firstly, we introduce a\nlatent rectified flow to generate 3D cardiac shapes for data augmentation,\nlearnt from a limited number of 3D WHS data. Then, a label completion network\nis trained on both real and synthetic data to reconstruct 3D+t shapes from\nsparse multi-view CMR segmentations. Lastly, we propose CardiacFlow, a novel\none-step generative flow model for efficient 3D+t four-chamber cardiac shape\ngeneration, conditioned on the periodic Gaussian kernel encoding of time\nframes. The experiments on the WHS datasets demonstrate that flow-based data\naugmentation reduces geometric errors by 16% in 3D shape completion. The\nevaluation on the UK Biobank dataset validates that CardiacFlow achieves\nsuperior generation quality and periodic consistency compared to existing\nbaselines.", "AI": {"tldr": "本文利用流匹配技术（包括潜在整流流和CardiacFlow）从有限数据中实现3D+t心脏形状的增强、补全和生成，显著提高了几何精度和生成质量。", "motivation": "从多视角心脏磁共振（CMR）图像中学习3D+t形状补全和生成需要大量的、高分辨率的3D全心分割（WHS）数据来捕获形状先验，而这种数据获取困难。", "method": "1. 引入潜在整流流（latent rectified flow），从有限的3D WHS数据中学习并生成3D心脏形状，用于数据增强。 2. 训练一个标签补全网络，利用真实和合成数据从稀疏的多视角CMR分割中重建3D+t形状。 3. 提出CardiacFlow，一个新型一步式生成流模型，通过时间帧的周期性高斯核编码条件，高效生成3D+t四腔心脏形状。", "result": "基于流的数据增强将3D形状补全中的几何误差降低了16%。在UK Biobank数据集上的评估表明，CardiacFlow相比现有基线模型实现了卓越的生成质量和周期性一致性。", "conclusion": "流匹配技术能有效学习深度生成流，用于3D+t心脏形状的增强、补全和生成，尤其在数据量有限的情况下，显著提升了心脏形状建模的精度和质量。"}}
{"id": "2509.05484", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05484", "abs": "https://arxiv.org/abs/2509.05484", "authors": ["Hajar Sakai", "Yi-En Tseng", "Mohammadsadegh Mikaeili", "Joshua Bosire", "Franziska Jovin"], "title": "From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics", "comment": null, "summary": "Hospital call centers serve as the primary contact point for patients within\na hospital system. They also generate substantial volumes of staff messages as\nnavigators process patient requests and communicate with the hospital offices\nfollowing the established protocol restrictions and guidelines. This\ncontinuously accumulated large amount of text data can be mined and processed\nto retrieve insights; however, traditional supervised learning approaches\nrequire annotated data, extensive training, and model tuning. Large Language\nModels (LLMs) offer a paradigm shift toward more computationally efficient\nmethodologies for healthcare analytics. This paper presents a multi-stage\nLLM-based framework that identifies staff message topics and classifies\nmessages by their reasons in a multi-class fashion. In the process, multiple\nLLM types, including reasoning, general-purpose, and lightweight models, were\nevaluated. The best-performing model was o3, achieving 78.4% weighted F1-score\nand 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and\n76.2% accuracy). The proposed methodology incorporates data security measures\nand HIPAA compliance requirements essential for healthcare environments. The\nprocessed LLM outputs are integrated into a visualization decision support tool\nthat transforms the staff messages into actionable insights accessible to\nhealthcare professionals. This approach enables more efficient utilization of\nthe collected staff messaging data, identifies navigator training\nopportunities, and supports improved patient experience and care quality.", "AI": {"tldr": "本文提出了一种多阶段、基于大型语言模型（LLM）的框架，用于分析医院呼叫中心的员工消息，识别主题并按原因进行分类，以提供可操作的见解。", "motivation": "医院呼叫中心产生大量员工消息文本数据，传统监督学习方法需要大量标注、训练和调优。LLM为医疗健康分析提供了更高效的方法，以从这些数据中提取洞察。", "method": "该研究开发了一个多阶段LLM框架，用于识别员工消息主题并进行多类别原因分类。过程中评估了多种LLM类型（推理型、通用型和轻量级模型）。处理后的LLM输出集成到可视化决策支持工具中，并确保数据安全和HIPAA合规性。", "result": "表现最佳的模型是o3，实现了78.4%的加权F1分数和79.2%的准确率，紧随其后的是gpt-5，获得了75.3%的加权F1分数和76.2%的准确率。", "conclusion": "所提出的方法能够更有效地利用收集到的员工消息数据，识别导航员培训机会，并支持改善患者体验和护理质量，同时确保医疗环境中的数据安全和合规性。"}}
{"id": "2509.05808", "categories": ["eess.SY", "cs.MA", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05808", "abs": "https://arxiv.org/abs/2509.05808", "authors": ["Yu-Wen Chen", "Nuno C. Martins", "Murat Arcak"], "title": "Hierarchical Decision-Making in Population Games", "comment": null, "summary": "This paper introduces a hierarchical framework for population games, where\nindividuals delegate decision-making to proxies that act within their own\nstrategic interests. This framework extends classical population games, where\nindividuals are assumed to make decisions directly, to capture various\nreal-world scenarios involving multiple decision layers. We establish\nequilibrium properties and provide convergence results for the proposed\nhierarchical structure. Additionally, based on these results, we develop a\nsystematic approach to analyze population games with general convex\nconstraints, without requiring individuals to have full knowledge of the\nconstraints as in existing methods. We present a navigation application with\ncapacity constraints as a case study.", "AI": {"tldr": "本文提出了一种分层群体博弈框架，其中个体将决策权委托给代理，代理基于自身利益行事，以模拟多层决策场景。该框架建立了均衡性质和收敛结果，并提供了一种分析具有一般凸约束的群体博弈的新方法，无需个体完全了解约束。", "motivation": "经典的群体博弈假设个体直接决策，但现实世界中存在多层决策和代理委托。此外，现有处理凸约束的方法要求个体对约束有完全了解，这通常是不现实的。", "method": "引入了一个分层群体博弈框架，其中个体将决策委托给具有自身战略利益的代理。针对所提出的分层结构，建立了均衡性质并提供了收敛结果。基于这些结果，开发了一种系统方法来分析具有一般凸约束的群体博弈。", "result": "成功建立了所提出的分层结构的均衡性质和收敛结果。开发了一种系统方法，可以分析具有一般凸约束的群体博弈，且无需个体像现有方法那样完全了解约束。通过一个带有容量约束的导航应用进行了案例研究。", "conclusion": "该分层框架扩展了经典群体博弈，能更好地捕捉现实世界中的多层决策。它提供了一种更实用、更少信息需求的方法来分析具有复杂约束的群体博弈，具有广泛的应用潜力。"}}
{"id": "2509.05345", "categories": ["cs.RO", "cs.CG"], "pdf": "https://arxiv.org/pdf/2509.05345", "abs": "https://arxiv.org/abs/2509.05345", "authors": ["Jiasheng Qu", "Zhuo Huang", "Dezhao Guo", "Hailin Sun", "Aoran Lyu", "Chengkai Dai", "Yeung Yam", "Guoxin Fang"], "title": "INF-3DP: Implicit Neural Fields for Collision-Free Multi-Axis 3D Printing", "comment": null, "summary": "We introduce a general, scalable computational framework for multi-axis 3D\nprinting based on implicit neural fields (INFs) that unifies all stages of\ntoolpath generation and global collision-free motion planning. In our pipeline,\ninput models are represented as signed distance fields, with fabrication\nobjectives such as support-free printing, surface finish quality, and extrusion\ncontrol being directly encoded in the optimization of an implicit guidance\nfield. This unified approach enables toolpath optimization across both surface\nand interior domains, allowing shell and infill paths to be generated via\nimplicit field interpolation. The printing sequence and multi-axis motion are\nthen jointly optimized over a continuous quaternion field. Our continuous\nformulation constructs the evolving printing object as a time-varying SDF,\nsupporting differentiable global collision handling throughout INF-based motion\nplanning. Compared to explicit-representation-based methods, INF-3DP achieves\nup to two orders of magnitude speedup and significantly reduces\nwaypoint-to-surface error. We validate our framework on diverse, complex models\nand demonstrate its efficiency with physical fabrication experiments using a\nrobot-assisted multi-axis system.", "AI": {"tldr": "该研究引入了一个基于隐式神经场（INFs）的通用、可扩展计算框架，用于多轴3D打印。它统一了刀具路径生成和全局无碰撞运动规划的所有阶段，实现了高达两个数量级的速度提升，并显著减少了路径点到表面的误差。", "motivation": "现有的多轴3D打印方法在刀具路径生成、无碰撞运动规划的统一性、效率和精度方面存在局限性。研究旨在开发一个能够整合这些阶段、提高速度并减少误差的计算框架。", "method": "该方法基于隐式神经场（INFs）。输入模型被表示为符号距离场（SDFs），制造目标（如无支撑打印、表面光洁度）直接编码在隐式引导场的优化中。通过隐式场插值生成外壳和填充路径，并对打印序列和多轴运动进行连续四元数场的联合优化。该连续公式将演变的打印对象构建为时变SDF，支持在基于INF的运动规划中进行可微分的全局碰撞处理。", "result": "与基于显式表示的方法相比，所提出的INF-3DP框架实现了高达两个数量级的速度提升，并显著降低了路径点到表面的误差。该框架在多样复杂的模型上得到了验证，并通过机器人辅助多轴系统的物理制造实验证明了其效率。", "conclusion": "该研究提供了一个通用且可扩展的基于隐式神经场的多轴3D打印计算框架，成功统一了刀具路径生成和无碰撞运动规划。它在效率和精度方面取得了显著提升，为复杂模型的打印提供了有效的解决方案。"}}
{"id": "2509.05321", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05321", "abs": "https://arxiv.org/abs/2509.05321", "authors": ["Yunfei Guo", "Tao Zhang", "Wu Huang", "Yao Song"], "title": "A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD", "comment": null, "summary": "This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,\nthat leverages the SEED-VD dataset to generate a multimodal dataset of EEG\nsignals conditioned on video stimuli. Additionally, we disclose an engineering\npipeline for aligning video and EEG data pairs, facilitating the training of\nmultimodal large models with EEG alignment capabilities. Personalized EEG\nsignals are generated using a self-play graph network (SPGN) integrated with a\ndiffusion model. As a major contribution, we release a new dataset comprising\nover 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG\nsignals at 200 Hz and emotion labels, enabling video-EEG alignment and\nadvancing multimodal research. This framework offers novel tools for emotion\nanalysis, data augmentation, and brain-computer interface applications, with\nsubstantial research and engineering significance.", "AI": {"tldr": "本文提出了一个开源框架Video2EEG-SPGN-Diffusion，利用SEED-VD数据集生成基于视频刺激的个性化EEG信号，并发布了一个包含视频和生成EEG信号的多模态数据集。", "motivation": "研究动机是缺乏用于训练具有EEG对齐能力的多模态大模型的视频-EEG数据集，以及推动情感分析、数据增强和脑机接口等应用。", "method": "该研究利用SEED-VD数据集，开发了一个视频和EEG数据对齐的工程管道。通过将自博弈图网络（SPGN）与扩散模型相结合，生成个性化的EEG信号。", "result": "主要成果是发布了Video2EEG-SPGN-Diffusion开源框架，以及一个包含1000多个样本的新数据集，其中包含SEED-VD视频刺激与生成的62通道200 Hz EEG信号和情感标签，旨在促进视频-EEG对齐和多模态研究。", "conclusion": "该框架和数据集为情感分析、数据增强和脑机接口应用提供了新颖的工具，具有重要的研究和工程意义。"}}
{"id": "2509.05330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05330", "abs": "https://arxiv.org/abs/2509.05330", "authors": ["Seyed Muhammad Hossein Mousavi", "Atiye Ilanloo"], "title": "MVRS: The Multimodal Virtual Reality Stimuli-based Emotion Recognition Dataset", "comment": null, "summary": "Automatic emotion recognition has become increasingly important with the rise\nof AI, especially in fields like healthcare, education, and automotive systems.\nHowever, there is a lack of multimodal datasets, particularly involving body\nmotion and physiological signals, which limits progress in the field. To\naddress this, the MVRS dataset is introduced, featuring synchronized recordings\nfrom 13 participants aged 12 to 60 exposed to VR based emotional stimuli\n(relaxation, fear, stress, sadness, joy). Data were collected using eye\ntracking (via webcam in a VR headset), body motion (Kinect v2), and EMG and GSR\nsignals (Arduino UNO), all timestamp aligned. Participants followed a unified\nprotocol with consent and questionnaires. Features from each modality were\nextracted, fused using early and late fusion techniques, and evaluated with\nclassifiers to confirm the datasets quality and emotion separability, making\nMVRS a valuable contribution to multimodal affective computing.", "AI": {"tldr": "该论文介绍了MVRS数据集，一个包含眼动、身体运动和生理信号的多模态情感识别数据集，以解决现有数据集的不足。", "motivation": "随着人工智能的兴起，自动情感识别变得越来越重要，但在医疗、教育和汽车系统等领域，缺乏多模态数据集，特别是涉及身体运动和生理信号的数据集，这限制了该领域的进展。", "method": "引入了MVRS数据集，包含13名12至60岁参与者在VR情感刺激（放松、恐惧、压力、悲伤、快乐）下的同步记录。数据通过眼动追踪（VR头显内的网络摄像头）、身体运动（Kinect v2）以及EMG和GSR信号（Arduino UNO）收集，所有数据均时间戳对齐。参与者遵循统一协议，并进行问卷调查。从每种模态中提取特征，使用早期和晚期融合技术进行融合，并通过分类器进行评估，以确认数据集的质量和情感可分离性。", "result": "通过分类器评估，确认了MVRS数据集的质量和情感可分离性。", "conclusion": "MVRS数据集是对多模态情感计算的宝贵贡献，有助于推动该领域的发展。"}}
{"id": "2509.05821", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05821", "abs": "https://arxiv.org/abs/2509.05821", "authors": ["Mohsen Asghari Ilani", "Yaser M. Banad"], "title": "Brain Tumor Detection Through Diverse CNN Architectures in IoT Healthcare Industries: Fast R-CNN, U-Net, Transfer Learning-Based CNN, and Fully Connected CNN", "comment": null, "summary": "Artificial intelligence (AI)-powered deep learning has advanced brain tumor\ndiagnosis in Internet of Things (IoT)-healthcare systems, achieving high\naccuracy with large datasets. Brain health is critical to human life, and\naccurate diagnosis is essential for effective treatment. Magnetic Resonance\nImaging (MRI) provides key data for brain tumor detection, serving as a major\nsource of big data for AI-driven image classification. In this study, we\nclassified glioma, meningioma, and pituitary tumors from MRI images using\nRegion-based Convolutional Neural Network (R-CNN) and UNet architectures. We\nalso applied Convolutional Neural Networks (CNN) and CNN-based transfer\nlearning models such as Inception-V3, EfficientNetB4, and VGG19. Model\nperformance was assessed using F-score, recall, precision, and accuracy. The\nFast R-CNN achieved the best results with 99% accuracy, 98.5% F-score, 99.5%\nArea Under the Curve (AUC), 99.4% recall, and 98.5% precision. Combining R-CNN,\nUNet, and transfer learning enables earlier diagnosis and more effective\ntreatment in IoT-healthcare systems, improving patient outcomes. IoT devices\nsuch as wearable monitors and smart imaging systems continuously collect\nreal-time data, which AI algorithms analyze to provide immediate insights for\ntimely interventions and personalized care. For external cohort cross-dataset\nvalidation, EfficientNetB2 achieved the strongest performance among fine-tuned\nEfficientNet models, with 92.11% precision, 92.11% recall/sensitivity, 95.96%\nspecificity, 92.02% F1-score, and 92.23% accuracy. These findings underscore\nthe robustness and reliability of AI models in handling diverse datasets,\nreinforcing their potential to enhance brain tumor classification and patient\ncare in IoT healthcare environments.", "AI": {"tldr": "本研究利用深度学习（R-CNN、UNet、CNN和迁移学习模型）对MRI图像中的脑肿瘤（胶质瘤、脑膜瘤、垂体瘤）进行分类，在物联网医疗系统中实现了高准确性，其中Fast R-CNN表现最佳。", "motivation": "脑健康对人类生命至关重要，准确诊断是有效治疗的关键。人工智能驱动的深度学习在物联网医疗系统中提高了脑肿瘤诊断的准确性，而MRI提供了AI进行图像分类所需的大数据。", "method": "本研究使用区域卷积神经网络（R-CNN）和UNet架构对MRI图像中的胶质瘤、脑膜瘤和垂体瘤进行分类。同时应用了卷积神经网络（CNN）和基于CNN的迁移学习模型，如Inception-V3、EfficientNetB4和VGG19。模型性能通过F-score、召回率、精确度、准确率和AUC进行评估，并进行了外部队列跨数据集验证。", "result": "Fast R-CNN取得了最佳结果，准确率达到99%，F-score为98.5%，AUC为99.5%，召回率为99.4%，精确度为98.5%。在外部队列跨数据集验证中，EfficientNetB2在微调的EfficientNet模型中表现最强，精确度为92.11%，召回率/敏感度为92.11%，特异性为95.96%，F1-score为92.02%，准确率为92.23%。", "conclusion": "结合R-CNN、UNet和迁移学习模型能够在物联网医疗系统中实现更早的诊断和更有效的治疗，改善患者预后。这些发现强调了AI模型在处理多样化数据集方面的鲁棒性和可靠性，增强了它们在物联网医疗环境中改善脑肿瘤分类和患者护理的潜力。"}}
{"id": "2509.05486", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05486", "abs": "https://arxiv.org/abs/2509.05486", "authors": ["Jessica M. Lundin", "Ada Zhang", "Nihal Karim", "Hamza Louzan", "Victor Wei", "David Adelani", "Cody Carroll"], "title": "The Token Tax: Systematic Bias in Multilingual Tokenization", "comment": null, "summary": "Tokenization inefficiency imposes structural disadvantages on morphologically\ncomplex, low-resource languages, inflating compute resources and depressing\naccuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA\nitems; 5 subjects; 16 African languages) and show that fertility (tokens/word)\nreliably predicts accuracy. Higher fertility consistently predicts lower\naccuracy across all models and subjects. We further find that reasoning models\n(DeepSeek, o1) consistently outperform non-reasoning peers across high and low\nresource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in\nprior generations. Finally, translating token inflation to economics, a\ndoubling in tokens results in quadrupled training cost and time, underscoring\nthe token tax faced by many languages. These results motivate morphologically\naware tokenization, fair pricing, and multilingual benchmarks for equitable\nnatural language processing (NLP).", "AI": {"tldr": "研究发现，分词效率低下对形态复杂、资源匮乏的语言造成结构性劣势，导致计算资源消耗增加和准确性下降。高“分词率”（tokens/word）与低准确性呈负相关，并显著增加训练成本。推理模型表现更优，研究呼吁采用形态感知分词、公平定价和多语言基准测试。", "motivation": "形态复杂、资源匮乏的语言在分词效率低下方面存在结构性劣势，导致计算资源消耗增加和准确性降低。这种“分词税”现象促使研究者探究其对大型语言模型性能及经济成本的影响。", "method": "研究评估了10个大型语言模型在AfriMMLU数据集（包含9,000个多项选择题，涵盖5个主题和16种非洲语言）上的表现。通过衡量“分词率”（tokens/word）来预测准确性。同时，比较了推理模型与非推理模型的性能差异，并分析了分词膨胀对训练成本和时间的影响。", "result": "结果显示，“分词率”越高，所有模型和主题的准确性越低。推理模型（DeepSeek, o1）在AfriMMLU数据集中持续优于非推理模型，缩小了先前观察到的准确性差距。此外，分词量翻倍会导致训练成本和时间增加四倍，突显了许多语言面临的“分词税”问题。", "conclusion": "研究结果强调了分词效率低下对某些语言造成的“分词税”和结构性劣势。为实现公平的自然语言处理（NLP），有必要开发形态感知的词法分析器、推行公平的定价策略，并建立更完善的多语言基准测试。"}}
{"id": "2509.05853", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.05853", "abs": "https://arxiv.org/abs/2509.05853", "authors": ["Simone Pirrera", "Nicolas Faedo", "Sophie M. Fosson", "Diego Regruto"], "title": "Real-Time Single-Iteration Model Predictive Control for Wave Energy Converters", "comment": null, "summary": "This paper proposes a novel real-time algorithm for controlling wave energy\nconverters (WECs). We begin with the economic model predictive control (MPC)\nproblem formulation and apply a novel, first-order optimization algorithm\ninspired by recently developed control-based algorithms for constrained\noptimization to define the controller dynamics according to the\nsingle-iteration MPC approach. We theoretically analyse the convergence of the\nemployed algorithm and the computational complexity of the obtained controller.\nResults from simulations using a benchmark WEC system indicate that the\nproposed approach significantly outperforms standard MPC, thanks to the\ninherent ability to handle faster sampling rates.", "AI": {"tldr": "本文提出了一种新型实时算法，用于控制波浪能转换器（WECs），该算法基于经济模型预测控制（MPC）和一种新颖的一阶优化方法，通过单次迭代MPC实现，并在理论和仿真中表现出优于标准MPC的性能，尤其是在处理更快的采样率方面。", "motivation": "研究动机是为了开发一种高效的实时算法来控制波浪能转换器（WECs），以提高其性能，尤其是在经济模型预测控制（MPC）的框架下，并克服标准MPC在处理快速采样率方面的局限性。", "method": "研究方法包括：首先，采用经济模型预测控制（MPC）问题公式化；其次，应用一种受最新基于控制的约束优化算法启发的新颖的一阶优化算法；然后，根据单次迭代MPC方法定义控制器动力学；最后，对所用算法的收敛性及其计算复杂性进行理论分析。", "result": "使用基准WEC系统进行的仿真结果表明，所提出的方法显著优于标准MPC。这主要得益于该方法固有的处理更快采样率的能力。", "conclusion": "本文提出的新型实时控制算法能够显著提升波浪能转换器（WECs）的性能，尤其是在需要更快采样率的应用场景中，其表现优于传统的模型预测控制方法。"}}
{"id": "2509.05355", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05355", "abs": "https://arxiv.org/abs/2509.05355", "authors": ["Ahmed R. Sadik", "Muhammad Ashfaq", "Niko Mäkitalo", "Tommi Mikkonen"], "title": "Human-LLM Synergy in Context-Aware Adaptive Architecture for Scalable Drone Swarm Operation", "comment": null, "summary": "The deployment of autonomous drone swarms in disaster response missions\nnecessitates the development of flexible, scalable, and robust coordination\nsystems. Traditional fixed architectures struggle to cope with dynamic and\nunpredictable environments, leading to inefficiencies in energy consumption and\nconnectivity. This paper addresses this gap by proposing an adaptive\narchitecture for drone swarms, leveraging a Large Language Model to dynamically\nselect the optimal architecture as centralized, hierarchical, or holonic based\non real time mission parameters such as task complexity, swarm size, and\ncommunication stability. Our system addresses the challenges of scalability,\nadaptability, and robustness,ensuring efficient energy consumption and\nmaintaining connectivity under varying conditions. Extensive simulations\ndemonstrate that our adaptive architecture outperforms traditional static\nmodels in terms of scalability, energy efficiency, and connectivity. These\nresults highlight the potential of our approach to provide a scalable,\nadaptable, and resilient solution for real world disaster response scenarios.", "AI": {"tldr": "本文提出了一种基于大型语言模型（LLM）的无人机蜂群自适应架构，能根据任务参数动态选择集中式、分层式或全息式架构，以优化灾难响应任务中的可扩展性、能效和连接性。", "motivation": "在灾难响应任务中，自主无人机蜂群需要灵活、可扩展且鲁棒的协调系统。传统的固定架构难以应对动态和不可预测的环境，导致能源消耗和连接效率低下。", "method": "本文提出了一种无人机蜂群自适应架构，利用大型语言模型（LLM）根据实时任务参数（如任务复杂性、蜂群规模和通信稳定性）动态选择最优的集中式、分层式或全息式架构。", "result": "广泛的模拟结果表明，该自适应架构在可扩展性、能源效率和连接性方面均优于传统的静态模型。", "conclusion": "该方法为现实世界中的灾难响应场景提供了一种可扩展、自适应且有弹性的解决方案，展示了其巨大的潜力。"}}
{"id": "2509.05322", "categories": ["cs.CV", "cs.LG", "cs.SI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.05322", "abs": "https://arxiv.org/abs/2509.05322", "authors": ["Pavithra Elumalai", "Sudharsan Vijayaraghavan", "Madhumita Mondal", "Areejit Samal"], "title": "Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19", "comment": "21 pages, 4 figures, 9 tables", "summary": "Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for\ninvestigating the impact of network topology in deep learning by capturing how\ndifferent connectivity patterns impact both learning efficiency and model\nperformance. At the same time, they provide a natural framework for exploring\nedge-centric network measures as tools for pruning and optimization. In this\nstudy, we investigate three edge-centric network measures: Forman-Ricci\ncurvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness\ncentrality (EBC), to compress RWNNs by selectively retaining important synapses\n(or edges) while pruning the rest. As a baseline, RWNNs are trained for\nCOVID-19 chest x-ray image classification, aiming to reduce network complexity\nwhile preserving performance in terms of accuracy, specificity, and\nsensitivity. We extend prior work on pruning RWNN using ORC by incorporating\ntwo additional edge-centric measures, FRC and EBC, across three network\ngenerators: Erd\\\"{o}s-R\\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and\nBarab\\'{a}si-Albert (BA) model. We provide a comparative analysis of the\npruning performance of the three measures in terms of compression ratio and\ntheoretical speedup. A central focus of our study is to evaluate whether FRC,\nwhich is computationally more efficient than ORC, can achieve comparable\npruning effectiveness. Along with performance evaluation, we further\ninvestigate the structural properties of the pruned networks through modularity\nand global efficiency, offering insights into the trade-off between modular\nsegregation and network efficiency in compressed RWNNs. Our results provide\ninitial evidence that FRC-based pruning can effectively simplify RWNNs,\noffering significant computational advantages while maintaining performance\ncomparable to ORC.", "AI": {"tldr": "本研究利用Forman-Ricci曲率（FRC）、Ollivier-Ricci曲率（ORC）和边介数中心性（EBC）等以边为中心的网络度量，修剪随机连接神经网络（RWNN），以简化网络并保持在COVID-19图像分类任务中的性能。", "motivation": "RWNNs是研究网络拓扑对深度学习影响的宝贵测试平台。探索以边为中心的网络度量作为修剪和优化的工具，旨在降低网络复杂性，同时保持模型性能。", "method": "研究了FRC、ORC和EBC三种以边为中心的网络度量来修剪RWNN。基线RWNN用于COVID-19胸部X射线图像分类。将这三种度量应用于Erdős-Rényi (ER)、Watts-Strogatz (WS) 和 Barabási-Albert (BA) 三种网络生成器。比较了修剪性能（压缩比和理论加速），并重点评估了FRC的计算效率与ORC的修剪效果。同时通过模块化和全局效率分析了修剪后网络的结构特性。", "result": "研究结果初步表明，基于FRC的修剪可以有效简化RWNN，提供显著的计算优势，同时保持与ORC相当的性能。修剪后的网络在模块化和全局效率方面也得到了评估。", "conclusion": "基于FRC的修剪是一种有效且计算效率更高的方法，可以简化随机连接神经网络，在保持性能的同时，为网络压缩提供了重要的计算优势，并有助于理解压缩后网络的结构特性。"}}
{"id": "2509.05346", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05346", "abs": "https://arxiv.org/abs/2509.05346", "authors": ["Bo Yuan", "Jiazi Hu"], "title": "Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning", "comment": null, "summary": "While Large Language Models (LLMs) are increasingly envisioned as intelligent\nassistants for personalized learning, systematic head-to-head evaluations\nwithin authentic learning scenarios remain limited. This study conducts an\nempirical comparison of three state-of-the-art LLMs on a tutoring task that\nsimulates a realistic learning setting. Using a dataset comprising a student's\nanswers to ten questions of mixed formats with correctness labels, each LLM is\nrequired to (i) analyze the quiz to identify underlying knowledge components,\n(ii) infer the student's mastery profile, and (iii) generate targeted guidance\nfor improvement. To mitigate subjectivity and evaluator bias, we employ Gemini\nas a virtual judge to perform pairwise comparisons along various dimensions:\naccuracy, clarity, actionability, and appropriateness. Results analyzed via the\nBradley-Terry model indicate that GPT-4o is generally preferred, producing\nfeedback that is more informative and better structured than its counterparts,\nwhile DeepSeek-V3 and GLM-4.5 demonstrate intermittent strengths but lower\nconsistency. These findings highlight the feasibility of deploying LLMs as\nadvanced teaching assistants for individualized support and provide\nmethodological guidance for future empirical research on LLM-driven\npersonalized learning.", "AI": {"tldr": "本研究在模拟真实学习场景的辅导任务中，系统评估了三种领先的大语言模型（LLMs）作为个性化学习助手的表现，发现GPT-4o在反馈质量上表现最佳。", "motivation": "尽管大语言模型（LLMs）被广泛设想为个性化学习的智能助手，但在真实的学习场景中，缺乏系统性的头对头评估。", "method": "研究比较了GPT-4o、DeepSeek-V3和GLM-4.5三种先进LLM在一个模拟真实学习环境的辅导任务中的表现。每个LLM需完成三项任务：分析测验以识别知识点、推断学生掌握情况、生成有针对性的改进指导。为减少主观性和评估者偏差，研究使用Gemini作为虚拟评判员，从准确性、清晰度、可操作性和适当性等维度进行配对比较，并采用Bradley-Terry模型分析结果。", "result": "通过Bradley-Terry模型分析显示，GPT-4o通常更受青睐，其生成的反馈比其他模型更具信息量且结构更优。DeepSeek-V3和GLM-4.5虽然偶尔表现出优势，但一致性较低。", "conclusion": "这些发现突出了部署LLMs作为高级教学助手提供个性化支持的可行性，并为未来关于LLM驱动个性化学习的实证研究提供了方法论指导。"}}
{"id": "2509.05929", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.05929", "abs": "https://arxiv.org/abs/2509.05929", "authors": ["Ricardo L. de Queiroz", "Diogo C. Garcia", "Yi-Hsin Chen", "Ruhan Conceição", "Wen-Hsiao Peng", "Luciano V. Agostini"], "title": "Application Space and the Rate-Distortion-Complexity Analysis of Neural Video CODECs", "comment": "12 pages 13 figures", "summary": "We study the decision-making process for choosing video compression systems\nthrough a rate-distortion-complexity (RDC) analysis. We discuss the 2D\nBjontegaard delta (BD) metric and formulate generalizations in an attempt to\nextend its notions to the 3D RDC volume. We follow that discussion with another\none on the computation of metrics in the RDC volume, and on how to define and\nmeasure the cost of a coder-decoder (codec) pair, where the codec is\ncharacterized by a cloud of points in the RDC space. We use a Lagrangian cost\n$D+\\lambda R + \\gamma C$, such that choosing the best video codec among a\nnumber of candidates for an application demands selecting appropriate\n$(\\lambda, \\gamma)$ values. Thus, we argue that an application may be\nassociated with a $(\\lambda, \\gamma)$ point in the application space. An\nexample streaming application was given as a case study to set a particular\npoint in the $(\\lambda, \\gamma)$ plane. The result is that we can compare\nLagrangian costs in an RDC volume for different codecs for a given application.\nFurthermore, we can span the plane and compare codecs for the entire\napplication space filled with different $(\\lambda, \\gamma)$ choices. We then\ncompared several state-of-the-art neural video codecs using the proposed\nmetrics. Results are informative and surprising. We found that, within our RDC\ncomputation constraints, only four neural video codecs came out as the best\nsuited for any application, depending on where its desirable $(\\lambda,\n\\gamma)$ lies.", "AI": {"tldr": "本文研究了视频压缩系统选择的决策过程，通过速率-失真-复杂度（RDC）分析，将2D BD度量推广到3D RDC空间，并引入拉格朗日成本$D+\\lambda R + \\gamma C$来比较不同应用场景下的编解码器。", "motivation": "选择视频压缩系统时，除了速率和失真，复杂度也是一个重要因素。传统的2D BD度量不足以涵盖所有维度。研究旨在提供一个更全面的框架来评估和选择视频编解码器，以适应不同应用的需求。", "method": "研究将2D Bjontegaard delta (BD) 度量推广到3D RDC体积。通过定义编解码器成本为拉格朗日函数$D+\\lambda R + \\gamma C$，其中$(\\lambda, \\gamma)$代表特定应用的需求。通过在$(\\lambda, \\gamma)$平面上设定点或遍历整个应用空间来比较不同编解码器的拉格朗日成本。", "result": "该方法能够在RDC体积中比较不同编解码器在特定应用下的拉格朗日成本，并能跨越整个应用空间比较编解码器。在对现有神经视频编解码器的比较中发现，在RDC计算约束下，只有四种神经视频编解码器根据所需$(\\lambda, \\gamma)$值的不同，被证明最适合任何应用。", "conclusion": "通过引入3D RDC分析和拉格朗日成本函数，可以为视频压缩系统的选择提供一个更全面、更灵活的决策框架，使得编解码器能够根据特定应用的需求进行有效比较和选择。研究结果为评估和优化视频编解码器提供了有价值的见解。"}}
{"id": "2509.05505", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05505", "abs": "https://arxiv.org/abs/2509.05505", "authors": ["Mansi Garg", "Lee-Chi Wang", "Bhavesh Ghanchi", "Sanjana Dumpala", "Shreyash Kakde", "Yen Chih Chen"], "title": "Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)", "comment": "10 pages, 6 figures, 3 tables", "summary": "This work presents a Biomedical Literature Question Answering (Q&A) system\nbased on a Retrieval-Augmented Generation (RAG) architecture, designed to\nimprove access to accurate, evidence-based medical information. Addressing the\nshortcomings of conventional health search engines and the lag in public access\nto biomedical research, the system integrates diverse sources, including PubMed\narticles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant\ninformation and generate concise, context-aware responses. The retrieval\npipeline uses MiniLM-based semantic embeddings and FAISS vector search, while\nanswer generation is performed by a fine-tuned Mistral-7B-v0.3 language model\noptimized using QLoRA for efficient, low-resource training. The system supports\nboth general medical queries and domain-specific tasks, with a focused\nevaluation on breast cancer literature demonstrating the value of\ndomain-aligned retrieval. Empirical results, measured using BERTScore (F1),\nshow substantial improvements in factual consistency and semantic relevance\ncompared to baseline models. The findings underscore the potential of\nRAG-enhanced language models to bridge the gap between complex biomedical\nliterature and accessible public health knowledge, paving the way for future\nwork on multilingual adaptation, privacy-preserving inference, and personalized\nmedical AI systems.", "AI": {"tldr": "该研究提出了一个基于RAG架构的生物医学文献问答系统，利用MiniLM进行语义检索，并使用QLoRA优化的Mistral-7B模型进行答案生成，旨在提供准确、基于证据的医疗信息，并在事实一致性和语义相关性方面取得了显著提升。", "motivation": "解决传统健康搜索引擎的不足以及公众获取生物医学研究信息滞后的问题，提升医疗信息的可及性和准确性。", "method": "该系统采用检索增强生成（RAG）架构。检索管道使用基于MiniLM的语义嵌入和FAISS向量搜索，整合PubMed文章、Q&A数据集和医学百科全书等多种来源。答案生成由经过QLoRA优化微调的Mistral-7B-v0.3语言模型完成。系统支持通用医疗查询和特定领域任务，并通过BERTScore (F1) 对乳腺癌文献进行了评估。", "result": "与基线模型相比，该系统在事实一致性和语义相关性方面取得了显著改进，尤其是在领域对齐检索方面表现出其价值。", "conclusion": "RAG增强的语言模型有潜力弥合复杂生物医学文献与可访问的公共健康知识之间的鸿沟，为未来的多语言适应、隐私保护推理和个性化医疗AI系统奠定基础。"}}
{"id": "2509.05907", "categories": ["eess.SY", "cs.NI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05907", "abs": "https://arxiv.org/abs/2509.05907", "authors": ["Qianren Li", "Yuncong Hong", "Bojie Lv", "Rui Wang"], "title": "A Dynamic Programming Framework for Vehicular Task Offloading with Successive Action Improvement", "comment": null, "summary": "In this paper, task offloading from vehicles with random velocities is\noptimized via a novel dynamic programming framework. Particularly, in a\nvehicular network with multiple vehicles and base stations (BSs), computing\ntasks of vehicles are offloaded via BSs to an edge server. Due to the random\nvelocities, the exact locations of vehicles versus time, namely trajectories,\ncannot be determined in advance. Hence, instead of deterministic optimization,\nthe cell association, uplink time, and throughput allocation of multiple\nvehicles during a period of task offloading are formulated as a finite-horizon\nMarkov decision process. In order to derive a low-complexity solution\nalgorithm, a two-time-scale framework is proposed. The scheduling period is\ndivided into super slots, each super slot is further divided into a number of\ntime slots. At the beginning of each super slot, we first obtain a reference\nscheduling scheme of cell association, uplink time and throughput allocation\nvia deterministic optimization, yielding an approximation of the optimal value\nfunction. Within the super slot, the actual scheduling action of each time slot\nis determined by making improvement to the approximate value function according\nto the system state. Due to the successive improvement framework, a non-trivial\naverage cost upper bound could be derived. In the simulation, the random\ntrajectories of vehicles are generated from a high-fidelity traffic simulator.\nIt is shown that the performance gain of the proposed scheduling framework over\nthe baselines is significant.", "AI": {"tldr": "本文通过新颖的动态规划框架，优化了随机速度车辆的任务卸载问题，提出了一种双时间尺度调度方案，显著提升了性能。", "motivation": "由于车辆速度随机，其精确轨迹无法预先确定，导致传统的确定性优化方法难以有效解决车辆网络中的任务卸载、小区关联、上行链路时间及吞吐量分配问题。", "method": "将多车辆在任务卸载期间的小区关联、上行链路时间及吞吐量分配建模为有限时域马尔可夫决策过程。为降低计算复杂度，提出双时间尺度框架：在每个“超槽”开始时，通过确定性优化获得参考调度方案和近似最优值函数；在“超槽”内部，根据系统状态通过对近似值函数进行改进来确定每个“时隙”的实际调度动作，并推导出了非平凡的平均成本上限。", "result": "通过高保真交通模拟器生成随机车辆轨迹进行仿真，结果表明所提出的调度框架相比基线方案具有显著的性能提升。", "conclusion": "所提出的基于动态规划的双时间尺度调度框架能够有效优化随机速度车辆的任务卸载，显著提升了车辆网络的性能。"}}
{"id": "2509.05356", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05356", "abs": "https://arxiv.org/abs/2509.05356", "authors": ["Justus Huebotter", "Pablo Lanillos", "Marcel van Gerven", "Serge Thill"], "title": "Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning", "comment": null, "summary": "Despite recent progress in training spiking neural networks (SNNs) for\nclassification, their application to continuous motor control remains limited.\nHere, we demonstrate that fully spiking architectures can be trained end-to-end\nto control robotic arms with multiple degrees of freedom in continuous\nenvironments. Our predictive-control framework combines Leaky\nIntegrate-and-Fire dynamics with surrogate gradients, jointly optimizing a\nforward model for dynamics prediction and a policy network for goal-directed\naction. We evaluate this approach on both a planar 2D reaching task and a\nsimulated 6-DOF Franka Emika Panda robot. Results show that SNNs can achieve\nstable training and accurate torque control, establishing their viability for\nhigh-dimensional motor tasks. An extensive ablation study highlights the role\nof initialization, learnable time constants, and regularization in shaping\ntraining dynamics. We conclude that while stable and effective control can be\nachieved, recurrent spiking networks remain highly sensitive to hyperparameter\nsettings, underscoring the importance of principled design choices.", "AI": {"tldr": "本文展示了脉冲神经网络（SNNs）如何通过端到端训练，实现对多自由度机械臂在连续环境中的精确运动控制，解决了SNNs在连续运动控制应用上的限制。", "motivation": "尽管脉冲神经网络在分类任务上取得了进展，但其在连续运动控制领域的应用仍然有限。研究旨在探索SNNs在连续机器人运动控制中的潜力。", "method": "研究采用了一种预测控制框架，结合了漏积分-放电（Leaky Integrate-and-Fire）动力学和替代梯度（surrogate gradients），共同优化了一个用于动力学预测的前向模型和一个用于目标导向动作的策略网络。该方法在平面2D抓取任务和模拟的6自由度Franka Emika Panda机器人上进行了评估。", "result": "结果表明，脉冲神经网络能够实现稳定的训练和精确的扭矩控制，证明了其在高维运动任务中的可行性。广泛的消融研究强调了初始化、可学习时间常数和正则化在训练动态中的作用。", "conclusion": "研究得出结论，虽然脉冲神经网络可以实现稳定有效的控制，但循环脉冲网络对超参数设置仍然高度敏感，这强调了原则性设计选择的重要性。"}}
{"id": "2509.05329", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05329", "abs": "https://arxiv.org/abs/2509.05329", "authors": ["Juan Carlos Martinez-Sevilla", "Francesco Foscarin", "Patricia Garcia-Iasci", "David Rizo", "Jorge Calvo-Zaragoza", "Gerhard Widmer"], "title": "Optical Music Recognition of Jazz Lead Sheets", "comment": "Accepted at the 26th International Society for Music Information\n  Retrieval Conference (ISMIR), 2025", "summary": "In this paper, we address the challenge of Optical Music Recognition (OMR)\nfor handwritten jazz lead sheets, a widely used musical score type that encodes\nmelody and chords. The task is challenging due to the presence of chords, a\nscore component not handled by existing OMR systems, and the high variability\nand quality issues associated with handwritten images. Our contribution is\ntwo-fold. We present a novel dataset consisting of 293 handwritten jazz lead\nsheets of 163 unique pieces, amounting to 2021 total staves aligned with\nHumdrum **kern and MusicXML ground truth scores. We also supply synthetic score\nimages generated from the ground truth. The second contribution is the\ndevelopment of an OMR model for jazz lead sheets. We discuss specific\ntokenisation choices related to our kind of data, and the advantages of using\nsynthetic scores and pretrained models. We publicly release all code, data, and\nmodels.", "AI": {"tldr": "该论文提出了一个用于手写爵士乐主旋律谱（包含旋律和和弦）的光学音乐识别（OMR）系统，并发布了一个新的数据集和相应的模型。", "motivation": "现有的OMR系统无法处理爵士乐主旋律谱中的和弦，并且难以应对手写乐谱图像的高变异性和质量问题，这促使研究者开发专门的解决方案。", "method": "研究者创建了一个包含293份手写爵士乐主旋律谱（共2021个谱表）的新数据集，并提供了Humdrum **kern和MusicXML格式的真实标签。同时，他们还生成了合成乐谱图像。在此基础上，开发了一个专门针对爵士乐主旋律谱的OMR模型，并讨论了特定的分词选择，以及使用合成乐谱和预训练模型的优势。", "result": "主要成果包括一个包含真实手写图像和合成图像的独特数据集，以及一个为爵士乐主旋律谱设计的OMR模型。所有的代码、数据和模型都已公开。", "conclusion": "该研究通过提供一个专门的数据集和OMR模型，成功解决了手写爵士乐主旋律谱识别的挑战，并为未来的研究提供了可公开获取的资源。"}}
{"id": "2509.05363", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05363", "abs": "https://arxiv.org/abs/2509.05363", "authors": ["Lijie Ding", "Changwoo Do"], "title": "SasAgent: Multi-Agent AI System for Small-Angle Scattering Data Analysis", "comment": "8 pages, 7 figures", "summary": "We introduce SasAgent, a multi-agent AI system powered by large language\nmodels (LLMs) that automates small-angle scattering (SAS) data analysis by\nleveraging tools from the SasView software and enables user interaction via\ntext input. SasAgent features a coordinator agent that interprets user prompts\nand delegates tasks to three specialized agents for scattering length density\n(SLD) calculation, synthetic data generation, and experimental data fitting.\nThese agents utilize LLM-friendly tools to execute tasks efficiently. These\ntools, including the model data tool, Retrieval-Augmented Generation (RAG)\ndocumentation tool, bump fitting tool, and SLD calculator tool, are derived\nfrom the SasView Python library. A user-friendly Gradio-based interface\nenhances user accessibility. Through diverse examples, we demonstrate\nSasAgent's ability to interpret complex prompts, calculate SLDs, generate\naccurate scattering data, and fit experimental datasets with high precision.\nThis work showcases the potential of LLM-driven AI systems to streamline\nscientific workflows and enhance automation in SAS research.", "AI": {"tldr": "SasAgent是一个由大型语言模型驱动的多智能体AI系统，通过集成SasView工具并支持文本输入，实现了小角散射（SAS）数据分析的自动化。", "motivation": "自动化小角散射（SAS）数据分析，简化科学工作流程，并增强SAS研究的自动化水平。", "method": "引入了SasAgent多智能体AI系统，由LLM驱动，包含一个协调器智能体和三个专业智能体（用于散射长度密度计算、合成数据生成和实验数据拟合）。这些智能体利用源自SasView Python库的LLM友好工具（如模型数据工具、RAG文档工具、凹凸拟合工具和SLD计算器工具）执行任务。用户交互通过文本输入，并提供基于Gradio的用户界面。", "result": "SasAgent能够解释复杂的用户提示，准确计算散射长度密度（SLD），生成精确的散射数据，并高精度地拟合实验数据集。", "conclusion": "这项工作展示了LLM驱动的AI系统在简化科学工作流程和提升SAS研究自动化方面的巨大潜力。"}}
{"id": "2509.05978", "categories": ["eess.IV", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05978", "abs": "https://arxiv.org/abs/2509.05978", "authors": ["Mohamed Mohamed", "Brennan Nichyporuk", "Douglas L. Arnold", "Tal Arbel"], "title": "Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance", "comment": null, "summary": "Vision-language models have demonstrated impressive capabilities in\ngenerating 2D images under various conditions; however the impressive\nperformance of these models in 2D is largely enabled by extensive, readily\navailable pretrained foundation models. Critically, comparable pretrained\nfoundation models do not exist for 3D, significantly limiting progress in this\ndomain. As a result, the potential of vision-language models to produce\nhigh-resolution 3D counterfactual medical images conditioned solely on natural\nlanguage descriptions remains completely unexplored. Addressing this gap would\nenable powerful clinical and research applications, such as personalized\ncounterfactual explanations, simulation of disease progression scenarios, and\nenhanced medical training by visualizing hypothetical medical conditions in\nrealistic detail. Our work takes a meaningful step toward addressing this\nchallenge by introducing a framework capable of generating high-resolution 3D\ncounterfactual medical images of synthesized patients guided by free-form\nlanguage prompts. We adapt state-of-the-art 3D diffusion models with\nenhancements from Simple Diffusion and incorporate augmented conditioning to\nimprove text alignment and image quality. To our knowledge, this represents the\nfirst demonstration of a language-guided native-3D diffusion model applied\nspecifically to neurological imaging data, where faithful three-dimensional\nmodeling is essential to represent the brain's three-dimensional structure.\nThrough results on two distinct neurological MRI datasets, our framework\nsuccessfully simulates varying counterfactual lesion loads in Multiple\nSclerosis (MS), and cognitive states in Alzheimer's disease, generating\nhigh-quality images while preserving subject fidelity in synthetically\ngenerated medical images. Our results lay the groundwork for prompt-driven\ndisease progression analysis within 3D medical imaging.", "AI": {"tldr": "本文提出了一种语言引导的3D扩散模型框架，用于生成高分辨率3D反事实医学图像，填补了3D领域预训练基础模型缺失的空白。", "motivation": "2D视觉-语言模型在图像生成方面表现出色，得益于丰富的预训练基础模型。然而，3D领域缺乏类似的预训练基础模型，严重限制了其发展。因此，利用自然语言描述生成高分辨率3D反事实医学图像的潜力尚未被探索。解决这一问题将支持个性化反事实解释、疾病进展模拟和增强医学训练等临床及研究应用。", "method": "我们引入了一个框架，能够根据自由形式的语言提示生成合成患者的高分辨率3D反事实医学图像。我们改进了最先进的3D扩散模型，并结合了Simple Diffusion的增强功能，同时加入了增强条件（augmented conditioning）以提高文本对齐和图像质量。该方法专门应用于神经影像数据。", "result": "在两个不同的神经MRI数据集上，我们的框架成功模拟了多发性硬化症（MS）中不同的反事实病灶负荷和阿尔茨海默病中的认知状态，生成了高质量图像，同时保持了合成医学图像中的受试者保真度。这是首次将语言引导的原生3D扩散模型应用于神经影像数据。", "conclusion": "我们的研究结果为3D医学影像中提示驱动的疾病进展分析奠定了基础。"}}
{"id": "2509.05553", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05553", "abs": "https://arxiv.org/abs/2509.05553", "authors": ["Serge Lionel Nikiema", "Jordan Samhi", "Micheline Bénédicte Moumoula", "Albérick Euraste Djiré", "Abdoul Kader Kaboré", "Jacques Klein", "Tegawendé F. Bissyandé"], "title": "Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study", "comment": null, "summary": "This research addresses a fundamental question in AI: whether large language\nmodels truly understand concepts or simply recognize patterns. The authors\npropose bidirectional reasoning,the ability to apply transformations in both\ndirections without being explicitly trained on the reverse direction, as a test\nfor genuine understanding. They argue that true comprehension should naturally\nallow reversibility. For example, a model that can change a variable name like\nuserIndex to i should also be able to infer that i represents a user index\nwithout reverse training. The researchers tested current language models and\ndiscovered what they term cognitive specialization: when models are fine-tuned\non forward tasks, their performance on those tasks improves, but their ability\nto reason bidirectionally becomes significantly worse. To address this issue,\nthey developed Contrastive Fine-Tuning (CFT), which trains models using three\ntypes of examples: positive examples that maintain semantic meaning, negative\nexamples with different semantics, and forward-direction obfuscation examples.\nThis approach aims to develop deeper understanding rather than surface-level\npattern recognition and allows reverse capabilities to develop naturally\nwithout explicit reverse training. Their experiments demonstrated that CFT\nsuccessfully achieved bidirectional reasoning, enabling strong reverse\nperformance while maintaining forward task capabilities. The authors conclude\nthat bidirectional reasoning serves both as a theoretical framework for\nassessing genuine understanding and as a practical training approach for\ndeveloping more capable AI systems.", "AI": {"tldr": "本研究提出双向推理作为衡量大型语言模型（LLMs）真正理解而非模式识别的测试。他们发现现有模型存在“认知专业化”问题，并开发了对比微调（CFT）方法，成功使模型在不进行反向训练的情况下实现双向推理，同时保持正向任务能力。", "motivation": "研究旨在解决人工智能领域的一个核心问题：大型语言模型是真正理解概念，还是仅仅识别模式？作者提出双向推理作为衡量真正理解的标准，即模型应能在未明确训练反向任务的情况下，自然地进行双向转换。", "method": "作者提出双向推理（即在未明确训练反向方向的情况下，能双向应用转换）作为衡量真正理解的测试。他们测试了当前语言模型，发现了“认知专业化”现象。为解决此问题，他们开发了对比微调（CFT）方法，使用三种类型的例子进行训练：保持语义的正面例子、语义不同的负面例子以及正向混淆例子。", "result": "实验发现，当前语言模型在正向任务上进行微调后，其正向任务性能提高，但双向推理能力显著下降，作者称之为“认知专业化”。通过对比微调（CFT），模型成功实现了双向推理，在保持正向任务能力的同时，展现出强大的反向性能。", "conclusion": "双向推理既可以作为评估模型真正理解的理论框架，也可以作为开发更强大AI系统的实用训练方法。通过对比微调，可以培养模型更深层次的理解能力，而非仅仅停留在表面模式识别。"}}
{"id": "2509.05935", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.05935", "abs": "https://arxiv.org/abs/2509.05935", "authors": ["Mohammad Rasoul Narimani", "Katherine R. Davis", "Daniel K. Molzahn"], "title": "Certifying the Nonexistence of Feasible Path Between Power System Operating Points", "comment": null, "summary": "By providing the optimal operating point that satisfies both the power flow\nequations and engineering limits, the optimal power flow (OPF) problem is\ncentral to the operation of electric power systems. While extensive research\nefforts have focused on reliably computing high-quality OPF solutions,\nassessing the feasibility of transitioning between operating points remains\nchallenging since the feasible spaces of OPF problems may consist of multiple\ndisconnected components. It is not possible to transition between operating\npoints in different disconnected components without violating OPF constraints.\nTo identify such situations, this paper introduces an algorithm for certifying\nthe infeasibility of transitioning between two operating points within an OPF\nfeasible space. As an indication of potential disconnectedness, the algorithm\nfirst seeks an infeasible point on the line connecting a pair of feasible\npoints. The algorithm then certifies disconnectedness by using convex\nrelaxation and bound tightening techniques to show that all points on the plane\nthat is normal to this line are infeasible. Using this algorithm, we provide\nthe first certifications of disconnected feasible spaces for a variety of OPF\ntest cases.", "AI": {"tldr": "本文提出一种算法，用于证明最优潮流（OPF）可行空间内两个运行点之间转换的不可行性，从而识别断开的可行区域。", "motivation": "最优潮流（OPF）问题是电力系统运行的核心，但由于OPF可行空间可能包含多个不连通的组件，导致在不同组件间的运行点转换变得具有挑战性，且可能违反OPF约束。现有研究主要关注可靠地计算高质量OPF解，而非评估运行点之间转换的可行性。", "method": "该算法首先在连接一对可行点的直线上寻找一个不可行点，以指示潜在的断开性。然后，通过使用凸松弛和边界收紧技术，算法证明垂直于该直线的平面上的所有点都是不可行的，从而认证可行空间的断开性。", "result": "利用该算法，本文首次为各种OPF测试案例提供了断开可行空间的认证。", "conclusion": "该算法成功地认证了OPF可行空间的断开性，这对于理解电力系统运行点之间的转换限制具有重要意义。"}}
{"id": "2509.05368", "categories": ["cs.RO", "cs.AI", "cs.LG", "I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.05368", "abs": "https://arxiv.org/abs/2509.05368", "authors": ["Quan Chen", "Chenrui Shi", "Qi Chen", "Yuwei Wu", "Zhi Gao", "Xintong Zhang", "Rui Gao", "Kun Wu", "Yunde Jia"], "title": "Long-Horizon Visual Imitation Learning via Plan and Code Reflection", "comment": "9 pages, 4 figures. Submitted to AAAI 2026", "summary": "Learning from long-horizon demonstrations with complex action sequences\npresents significant challenges for visual imitation learning, particularly in\nunderstanding temporal relationships of actions and spatial relationships\nbetween objects. In this paper, we propose a new agent framework that\nincorporates two dedicated reflection modules to enhance both plan and code\ngeneration. The plan generation module produces an initial action sequence,\nwhich is then verified by the plan reflection module to ensure temporal\ncoherence and spatial alignment with the demonstration video. The code\ngeneration module translates the plan into executable code, while the code\nreflection module verifies and refines the generated code to ensure correctness\nand consistency with the generated plan. These two reflection modules jointly\nenable the agent to detect and correct errors in both the plan generation and\ncode generation, improving performance in tasks with intricate temporal and\nspatial dependencies. To support systematic evaluation, we introduce\nLongVILBench, a benchmark comprising 300 human demonstrations with action\nsequences of up to 18 steps. LongVILBench emphasizes temporal and spatial\ncomplexity across multiple task types. Experimental results demonstrate that\nexisting methods perform poorly on this benchmark, whereas our new framework\nestablishes a strong baseline for long-horizon visual imitation learning.", "AI": {"tldr": "本文提出了一种新的智能体框架，包含计划和代码生成及对应的反思模块，以解决长周期视觉模仿学习中复杂动作序列的挑战，并引入了LongVILBench基准来系统评估，结果显示其框架在此类任务中表现出色。", "motivation": "长周期演示中复杂的动作序列给视觉模仿学习带来了显著挑战，尤其是在理解动作的时间关系和物体之间的空间关系方面。", "method": "本文提出了一个包含两个专用反思模块的智能体框架：计划生成模块产生初始动作序列，然后由计划反思模块验证其时间连贯性和空间对齐性；代码生成模块将计划转换为可执行代码，再由代码反思模块验证和完善生成的代码。这两个反思模块共同检测和纠正计划生成和代码生成中的错误。此外，本文还引入了LongVILBench，一个包含300个人类演示视频（最长18步动作序列）的基准，以支持系统评估。", "result": "实验结果表明，现有方法在LongVILBench基准上表现不佳，而本文提出的新框架为长周期视觉模仿学习建立了强大的基线。", "conclusion": "通过引入计划和代码反思模块，本文提出的框架能够有效检测和纠正错误，显著提高了在具有复杂时间与空间依赖性的长周期视觉模仿学习任务中的性能，并为该领域提供了新的评估基准。"}}
{"id": "2509.05333", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05333", "abs": "https://arxiv.org/abs/2509.05333", "authors": ["Junghyun Park", "Tuan Anh Nguyen", "Dugki Min"], "title": "RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness", "comment": null, "summary": "Real world deployments often expose modern object recognition models to\ndomain shifts that precipitate a severe drop in accuracy. Such shifts encompass\n(i) variations in low level image statistics, (ii) changes in object pose and\nviewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent\nclasses. To mitigate this degradation, we introduce the Re-Thinking Vision\nLanguage Model (RT-VLM) framework. The foundation of this framework is a unique\nsynthetic dataset generation pipeline that produces images annotated with\n\"4-Clues\": precise bounding boxes, class names, detailed object-level captions,\nand a comprehensive context-level caption for the entire scene. We then perform\nparameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this\nresource. At inference time, a two stage Re-Thinking scheme is executed: the\nmodel first emits its own four clues, then re examines these responses as\nevidence and iteratively corrects them. Across robustness benchmarks that\nisolate individual domain shifts, RT-VLM consistently surpasses strong\nbaselines. These findings indicate that the integration of structured\nmultimodal evidence with an explicit self critique loop constitutes a promising\nroute toward reliable and transferable visual understanding.", "AI": {"tldr": "本文提出了RT-VLM框架，通过生成包含“4条线索”的合成数据集，并结合两阶段的“再思考”推理机制（模型先生成线索，再迭代修正），以提高物体识别模型在领域偏移下的鲁棒性。", "motivation": "现代物体识别模型在实际部署中常因领域偏移（如低级图像统计、物体姿态、遮挡和类间混淆）导致准确率严重下降，因此需要开发新的方法来缓解这种性能退化。", "method": "该研究引入了RT-VLM框架。核心方法包括：1) 独特的合成数据集生成流程，生成带有“4条线索”（精确边界框、类名、详细物体级描述、场景级描述）的图像；2) 使用此资源对Llama 3.2 11B Vision Instruct模型进行参数高效的监督微调；3) 在推理时执行两阶段“再思考”方案，模型首先生成自己的四条线索，然后将其作为证据重新检查并迭代修正。", "result": "在针对各种领域偏移的鲁棒性基准测试中，RT-VLM持续超越了强大的基线模型。", "conclusion": "研究结果表明，将结构化多模态证据与明确的自我批评循环相结合，是实现可靠和可迁移视觉理解的一个有前景的途径。"}}
{"id": "2509.05375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05375", "abs": "https://arxiv.org/abs/2509.05375", "authors": ["Arend Hintze"], "title": "Characterizing Fitness Landscape Structures in Prompt Engineering", "comment": null, "summary": "While prompt engineering has emerged as a crucial technique for optimizing\nlarge language model performance, the underlying optimization landscape remains\npoorly understood. Current approaches treat prompt optimization as a black-box\nproblem, applying sophisticated search algorithms without characterizing the\nlandscape topology they navigate. We present a systematic analysis of fitness\nlandscape structures in prompt engineering using autocorrelation analysis\nacross semantic embedding spaces. Through experiments on error detection tasks\nwith two distinct prompt generation strategies -- systematic enumeration (1,024\nprompts) and novelty-driven diversification (1,000 prompts) -- we reveal\nfundamentally different landscape topologies. Systematic prompt generation\nyields smoothly decaying autocorrelation, while diversified generation exhibits\nnon-monotonic patterns with peak correlation at intermediate semantic\ndistances, indicating rugged, hierarchically structured landscapes.\nTask-specific analysis across 10 error detection categories reveals varying\ndegrees of ruggedness across different error types. Our findings provide an\nempirical foundation for understanding the complexity of optimization in prompt\nengineering landscapes.", "AI": {"tldr": "本研究通过自相关分析系统地揭示了Prompt工程优化景观的结构，发现不同Prompt生成策略会导致截然不同的景观拓扑。", "motivation": "Prompt工程在优化大型语言模型性能方面至关重要，但其底层的优化景观却知之甚少。当前的Prompt优化方法将其视为黑盒问题，缺乏对景观拓扑的特征描述。", "method": "本研究采用自相关分析方法，在语义嵌入空间中系统分析Prompt工程的适应度景观结构。实验在错误检测任务上进行，使用了两种Prompt生成策略：系统枚举（1,024个Prompt）和新颖性驱动多样化（1,000个Prompt）。", "result": "系统Prompt生成策略产生了平滑衰减的自相关模式，而多样化生成策略则表现出非单调模式，在中等语义距离处达到峰值相关，表明其景观崎岖且具有分层结构。此外，对10种错误检测类别的任务特定分析显示，不同错误类型在景观崎岖程度上存在差异。", "conclusion": "本研究为理解Prompt工程景观中优化的复杂性提供了经验基础，揭示了Prompt生成策略对优化景观拓扑的关键影响。"}}
{"id": "2509.06159", "categories": ["eess.IV", "cs.AI", "cs.CV", "I.4.6; I.4.8; J.3"], "pdf": "https://arxiv.org/pdf/2509.06159", "abs": "https://arxiv.org/abs/2509.06159", "authors": ["Muraam Abdel-Ghani", "Mahmoud Ali", "Mohamed Ali", "Fatmaelzahraa Ahmed", "Mohamed Arsalan", "Abdulaziz Al-Ali", "Shidin Balakrishnan"], "title": "FASL-Seg: Anatomy and Tool Segmentation of Surgical Scenes", "comment": "8 pages, 6 figures, Accepted at the European Conference on Artificial\n  Intelligence (ECAI) 2025. To appear in the conference proceedings", "summary": "The growing popularity of robotic minimally invasive surgeries has made deep\nlearning-based surgical training a key area of research. A thorough\nunderstanding of the surgical scene components is crucial, which semantic\nsegmentation models can help achieve. However, most existing work focuses on\nsurgical tools and overlooks anatomical objects. Additionally, current\nstate-of-the-art (SOTA) models struggle to balance capturing high-level\ncontextual features and low-level edge features. We propose a Feature-Adaptive\nSpatial Localization model (FASL-Seg), designed to capture features at multiple\nlevels of detail through two distinct processing streams, namely a Low-Level\nFeature Projection (LLFP) and a High-Level Feature Projection (HLFP) stream,\nfor varying feature resolutions - enabling precise segmentation of anatomy and\nsurgical instruments. We evaluated FASL-Seg on surgical segmentation benchmark\ndatasets EndoVis18 and EndoVis17 on three use cases. The FASL-Seg model\nachieves a mean Intersection over Union (mIoU) of 72.71% on parts and anatomy\nsegmentation in EndoVis18, improving on SOTA by 5%. It further achieves a mIoU\nof 85.61% and 72.78% in EndoVis18 and EndoVis17 tool type segmentation,\nrespectively, outperforming SOTA overall performance, with comparable per-class\nSOTA results in both datasets and consistent performance in various classes for\nanatomy and instruments, demonstrating the effectiveness of distinct processing\nstreams for varying feature resolutions.", "AI": {"tldr": "本文提出FASL-Seg模型，通过双流处理（低级特征投影和高级特征投影）实现多层次特征捕获，显著提升了手术场景（解剖结构和器械）的语义分割性能，在EndoVis18和EndoVis17数据集上超越了现有SOTA模型。", "motivation": "随着机器人微创手术的普及，基于深度学习的手术训练变得至关重要，其中对场景组件的语义分割理解是关键。然而，现有工作多侧重于手术器械而忽视解剖对象，且当前SOTA模型难以平衡高层次上下文特征与低层次边缘特征的捕获。", "method": "研究者提出了特征自适应空间定位模型（FASL-Seg），该模型通过两个独立的处理流来捕获不同细节层次的特征：低级特征投影（LLFP）流和高级特征投影（HLFP）流。这种设计旨在处理不同特征分辨率，从而实现解剖结构和手术器械的精确分割。", "result": "FASL-Seg模型在EndoVis18和EndoVis17手术分割基准数据集的三个用例中进行了评估。在EndoVis18的部件和解剖结构分割中，模型达到了72.71%的平均交并比（mIoU），比SOTA提升了5%。在EndoVis18和EndoVis17的器械类型分割中，分别达到了85.61%和72.78%的mIoU，整体性能超越了SOTA，并且在两个数据集的各类别中均达到了可比较的SOTA结果，在解剖结构和器械的各种类别中表现出一致性。", "conclusion": "FASL-Seg模型通过其独特的双流处理机制，能够有效捕获不同分辨率的特征，从而实现了对解剖结构和手术器械的精确分割。这证明了为不同特征分辨率设计独立处理流的有效性，并解决了现有SOTA模型在平衡高低层次特征方面的不足。"}}
{"id": "2509.05566", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.05566", "abs": "https://arxiv.org/abs/2509.05566", "authors": ["Anya Ji", "Claire Augusta Bergey", "Ron Eliav", "Yoav Artzi", "Robert D. Hawkins"], "title": "Ad hoc conventions generalize to new referents", "comment": null, "summary": "How do people talk about things they've never talked about before? One view\nsuggests that a new shared naming system establishes an arbitrary link to a\nspecific target, like proper names that cannot extend beyond their bearers. An\nalternative view proposes that forming a shared way of describing objects\ninvolves broader conceptual alignment, reshaping each individual's semantic\nspace in ways that should generalize to new referents. We test these competing\naccounts in a dyadic communication study (N=302) leveraging the\nrecently-released KiloGram dataset containing over 1,000 abstract tangram\nimages. After pairs of participants coordinated on referential conventions for\none set of images through repeated communication, we measured the extent to\nwhich their descriptions aligned for undiscussed images. We found strong\nevidence for generalization: partners showed increased alignment relative to\ntheir pre-test labels. Generalization also decayed nonlinearly with visual\nsimilarity (consistent with Shepard's law) and was robust across levels of the\nimages' nameability. These findings suggest that ad hoc conventions are not\narbitrary labels but reflect genuine conceptual coordination, with implications\nfor theories of reference and the design of more adaptive language agents.", "AI": {"tldr": "研究发现，人们在为新事物建立共享命名系统时，并非简单地创建任意标签，而是通过概念协调，这种协调能够泛化到未讨论的新参照物，表明临时性约定反映了真正的概念对齐。", "motivation": "研究旨在检验两种关于人们如何谈论从未谈论过的事物的竞争性观点：一是新命名系统是任意链接到特定目标的（如专有名词），二是共享描述涉及更广泛的概念对齐，能泛化到新的参照物。", "method": "通过一项双人交流研究（N=302），利用包含1000多张抽象七巧板图像的KiloGram数据集。参与者对一组图像的指称约定进行协调后，测量了他们对未讨论图像描述的对齐程度。", "result": "研究发现强烈的泛化证据：与预测试标签相比，合作者在未讨论图像上的对齐度显著增加。泛化能力随视觉相似度非线性衰减（符合谢泼德定律），并且在图像可命名性水平上保持稳健。", "conclusion": "这些发现表明，临时性约定并非任意标签，而是反映了真正的概念协调，对指称理论和自适应语言智能体的设计具有重要意义。"}}
{"id": "2509.06092", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06092", "abs": "https://arxiv.org/abs/2509.06092", "authors": ["Prajakta Surve", "Shaunak D. Bopardikar", "Alexander Von Moll", "Isaac Weintraub", "David W. Casbeer"], "title": "Mutual Support by Sensor-Attacker Team for a Passive Target", "comment": null, "summary": "We introduce a pursuit game played between a team of a sensor and an attacker\nand a mobile target in the unbounded Euclidean plane. The target is faster than\nthe sensor, but slower than the attacker. The sensor's objective is to keep the\ntarget within a sensing radius so that the attacker can capture the target,\nwhereas the target seeks to escape by reaching beyond the sensing radius from\nthe sensor without getting captured by the attacker. We assume that as long as\nthe target is within the sensing radius from the sensor, the sensor-attacker\nteam is able to measure the target's instantaneous position and velocity. We\npose and solve this problem as a \\emph{game of kind} in which the target uses\nan open-loop strategy (passive target). Aside from the novel formulation, our\ncontributions are four-fold. First, we present optimal strategies for both the\nsensor and the attacker, according to their respective objectives.\nSpecifically, we design a sensor strategy that maximizes the duration for which\nthe target remains within its sensing range, while the attacker uses\nproportional navigation to capture the target. Second, we characterize the\n\\emph{sensable region} -- the region in the plane in which the target remains\nwithin the sensing radius of the sensor during the game -- and show that\ncapture is guaranteed {if and only if} the Apollonius circle between the\nattacker and the target is fully contained within this region. Third, we\n{derive a lower bound} on the target's speed below which capture is guaranteed,\nand an upper bound on the target speed above which there exists an escape\nstrategy for the target, from an arbitrary initial orientation between the\nagents. Fourth, for a given initial orientation between the agents, we present\na sharper upper bound on the target speed above which there exists an escape\nstrategy for the target.", "AI": {"tldr": "本文提出并解决了一个传感器-攻击者团队与一个移动目标在无限欧几里得平面上的追逐游戏，其中目标速度介于传感器和攻击者之间。研究旨在设计最优策略、刻画可感知区域以及确定目标被捕获或逃脱的速度界限。", "motivation": "研究动机在于引入一种新颖的追逐游戏设定，其中包含传感器保持目标在感知范围内以便攻击者捕获，而目标则试图逃脱的动态。该设定考虑了不同参与者之间的速度差异，并旨在为这种复杂交互提供理论解决方案。", "method": "该问题被建模并解决为一种“类型博弈”（game of kind），其中目标采用开环策略（被动目标）。研究方法包括设计传感器和攻击者的最优策略（传感器最大化目标在感知范围内的持续时间，攻击者使用比例导航），刻画“可感知区域”，并推导目标速度的上下界以确定捕获或逃脱条件。", "result": "主要结果包括：1) 提出了传感器和攻击者的最优策略；2) 刻画了“可感知区域”，并证明当且仅当攻击者和目标之间的阿波罗尼斯圆完全包含在该区域内时，捕获才能得到保证；3) 推导了目标速度的下限，低于该下限捕获是必然的，以及在任意初始方向下目标速度的上限，高于该上限目标存在逃脱策略；4) 对于给定的初始方向，提出了一个更精确的目标速度上限，高于该上限目标存在逃脱策略。", "conclusion": "本研究为传感器-攻击者团队与目标之间的追逐游戏提供了一个全面的分析框架。通过设计最优策略、刻画关键区域和确定速度界限，该工作明确了在不同初始条件和速度比下，目标被捕获或成功逃脱的条件，为类似场景的控制和规划提供了理论基础。"}}
{"id": "2509.05391", "categories": ["cs.RO", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.05391", "abs": "https://arxiv.org/abs/2509.05391", "authors": ["Christian Masuhr", "Julian Koch", "Thorsten Schüppstuhl"], "title": "Evaluating Magic Leap 2 Tool Tracking for AR Sensor Guidance in Industrial Inspections", "comment": null, "summary": "Rigorous evaluation of commercial Augmented Reality (AR) hardware is crucial,\nyet public benchmarks for tool tracking on modern Head-Mounted Displays (HMDs)\nare limited. This paper addresses this gap by systematically assessing the\nMagic Leap 2 (ML2) controllers tracking performance. Using a robotic arm for\nrepeatable motion (EN ISO 9283) and an optical tracking system as ground truth,\nour protocol evaluates static and dynamic performance under various conditions,\nincluding realistic paths from a hydrogen leak inspection use case. The results\nprovide a quantitative baseline of the ML2 controller's accuracy and\nrepeatability and present a robust, transferable evaluation methodology. The\nfindings provide a basis to assess the controllers suitability for the\ninspection use case and similar industrial sensor-based AR guidance tasks.", "AI": {"tldr": "本文系统评估了Magic Leap 2 (ML2) 控制器的跟踪性能，使用机械臂进行可重复运动，并以光学跟踪系统作为真值，为ML2控制器提供了量化基线和可转移的评估方法。", "motivation": "商业增强现实(AR)硬件的严格评估至关重要，但针对现代头戴式显示器(HMD)工具跟踪的公开基准测试非常有限，尤其缺乏对Magic Leap 2控制器的评估。", "method": "采用机械臂进行可重复运动（符合EN ISO 9283标准），并使用光学跟踪系统作为地面真值。评估协议涵盖静态和动态性能，包括来自氢气泄漏检查用例的真实路径。", "result": "提供了ML2控制器精度和重复性的量化基线，并提出了一种稳健且可转移的评估方法。", "conclusion": "研究结果为评估ML2控制器在检查用例和类似工业传感器AR指导任务中的适用性提供了基础。"}}
{"id": "2509.05334", "categories": ["cs.CV", "cs.MM", "H.5.1; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.05334", "abs": "https://arxiv.org/abs/2509.05334", "authors": ["Diwen Huang"], "title": "A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices", "comment": "6 pages, 3 figures, 1 table. Independent research preprint", "summary": "Performance metrics in sports, such as shot speed and angle, provide crucial\nfeedback for athlete development. However, the technology to capture these\nmetrics has historically been expensive, complex, and largely inaccessible to\namateur and recreational players. This paper addresses this gap in the context\nof badminton, one of the world's most popular sports, by introducing a novel,\ncost-effective, and user-friendly system for measuring smash speed using\nubiquitous smartphone technology. Our approach leverages a custom-trained\nYOLOv5 model for shuttlecock detection, combined with a Kalman filter for\nrobust trajectory tracking. By implementing a video-based kinematic speed\nestimation method with spatiotemporal scaling, the system automatically\ncalculates the shuttlecock's velocity from a standard video recording. The\nentire process is packaged into an intuitive mobile application, democratizing\naccess to high-level performance analytics and empowering players at all levels\nto analyze and improve their game.", "AI": {"tldr": "本文介绍了一种基于智能手机的经济高效且用户友好的羽毛球扣杀速度测量系统，利用YOLOv5和卡尔曼滤波器从视频中自动计算球速。", "motivation": "体育运动中的表现指标（如击球速度和角度）对运动员发展至关重要，但传统技术昂贵、复杂且业余选手难以获得。本研究旨在填补这一空白，为羽毛球运动提供可及的性能分析工具。", "method": "该系统利用智能手机技术，采用定制训练的YOLOv5模型进行羽毛球检测，结合卡尔曼滤波器进行鲁棒轨迹跟踪。通过基于视频的运动学速度估算方法和时空缩放，系统能自动计算羽毛球速度。整个过程被封装成一个直观的移动应用程序。", "result": "该系统能够从标准视频录像中自动计算羽毛球的速度，提供高水平的性能分析。", "conclusion": "该研究成功开发了一个民主化高性能分析访问的系统，使各水平的羽毛球运动员都能分析和改进他们的比赛表现。"}}
{"id": "2509.05378", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.05378", "abs": "https://arxiv.org/abs/2509.05378", "authors": ["Andreas Motzfeldt", "Joakim Edin", "Casper L. Christensen", "Christian Hardmeier", "Lars Maaløe", "Anna Rogers"], "title": "Code Like Humans: A Multi-Agent Solution for Medical Coding", "comment": "EMNLP Findings 2025", "summary": "In medical coding, experts map unstructured clinical notes to alphanumeric\ncodes for diagnoses and procedures. We introduce Code Like Humans: a new\nagentic framework for medical coding with large language models. It implements\nofficial coding guidelines for human experts, and it is the first solution that\ncan support the full ICD-10 coding system (+70K labels). It achieves the best\nperformance to date on rare diagnosis codes (fine-tuned discriminative\nclassifiers retain an advantage for high-frequency codes, to which they are\nlimited). Towards future work, we also contribute an analysis of system\nperformance and identify its `blind spots' (codes that are systematically\nundercoded).", "AI": {"tldr": "该研究引入了“Code Like Humans”框架，这是一个基于大语言模型（LLM）的智能体框架，用于医学编码。它能够支持完整的ICD-10编码系统，并在稀有诊断代码方面表现最佳。", "motivation": "医学编码是一个复杂的过程，需要专家将非结构化的临床笔记映射到字母数字代码。现有的解决方案可能无法支持完整的ICD-10系统，或在处理稀有代码时表现不佳。研究旨在开发一个能够遵循官方编码指南并处理全量代码的解决方案。", "method": "研究引入了一个名为“Code Like Humans”的智能体框架，该框架利用大语言模型，并实现了人类专家所遵循的官方编码指南。它是第一个能够支持完整ICD-10编码系统（超过7万个标签）的解决方案。", "result": "该框架在稀有诊断代码方面取得了迄今为止的最佳性能。虽然经过微调的判别分类器在高频代码方面仍具有优势（但其适用范围有限），“Code Like Humans”在稀有代码方面表现突出。研究还对系统性能进行了分析，并识别了其“盲点”（系统性低估的代码）。", "conclusion": "基于大语言模型的智能体框架“Code Like Humans”为医学编码提供了一个全面且高效的解决方案，特别是在处理稀有诊断代码和支持完整ICD-10系统方面表现出色。未来的工作可以集中于解决系统识别出的“盲点”问题。"}}
{"id": "2509.06495", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.06495", "abs": "https://arxiv.org/abs/2509.06495", "authors": ["Fangyijie Wang", "Guénolé Silvestre", "Kathleen M. Curran"], "title": "Leveraging Information Divergence for Robust Semi-Supervised Fetal Ultrasound Image Segmentation", "comment": null, "summary": "Maternal-fetal Ultrasound is the primary modality for monitoring fetal\ndevelopment, yet automated segmentation remains challenging due to the scarcity\nof high-quality annotations. To address this limitation, we propose a\nsemi-supervised learning framework that leverages information divergence for\nrobust fetal ultrasound segmentation. Our method employs a lightweight\nconvolutional network (1.47M parameters) and a Transformer-based network,\ntrained jointly with labelled data through standard supervision and with\nunlabelled data via cross-supervision. To encourage consistent and confident\npredictions, we introduce an information divergence loss that combines\nper-pixel Kullback-Leibler divergence and Mutual Information Gap, effectively\nreducing prediction disagreement between the two models. In addition, we apply\nmixup on unlabelled samples to further enhance robustness. Experiments on two\nfetal ultrasound datasets demonstrate that our approach consistently\noutperforms seven state-of-the-art semi-supervised methods. When only 5% of\ntraining data is labelled, our framework improves the Dice score by 2.39%,\nreduces the 95% Hausdorff distance by 14.90, and decreases the Average Surface\nDistance by 4.18. These results highlight the effectiveness of leveraging\ninformation divergence for annotation-efficient and robust medical image\nsegmentation. Our code is publicly available on GitHub.", "AI": {"tldr": "该研究提出了一种半监督学习框架，利用信息散度（information divergence）进行胎儿超声图像分割，以解决高质量标注数据稀缺的问题，并在有限标注数据下取得了显著优于现有方法的效果。", "motivation": "胎儿超声是监测胎儿发育的主要方式，但由于高质量标注的稀缺性，自动化分割仍然面临挑战。", "method": "该方法采用一个轻量级卷积网络和一个基于Transformer的网络，通过标准监督（有标签数据）和交叉监督（无标签数据）联合训练。引入了一种结合像素级KL散度（Kullback-Leibler divergence）和互信息间隙（Mutual Information Gap）的信息散度损失，以减少两个模型间的预测分歧。此外，对无标签样本应用mixup以增强鲁棒性。", "result": "在两个胎儿超声数据集上的实验表明，该方法持续优于七种最先进的半监督方法。当仅有5%的训练数据被标注时，Dice分数提高了2.39%，95% Hausdorff距离减少了14.90，平均表面距离减少了4.18。", "conclusion": "研究结果强调了利用信息散度在标注高效和鲁棒的医学图像分割中的有效性。"}}
{"id": "2509.05602", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05602", "abs": "https://arxiv.org/abs/2509.05602", "authors": ["Hongyan Xie", "Yitong Yao", "Yikun Ban", "Zixuan Huang", "Deqing Wang", "Zhenhe Wu", "Haoxiang Su", "Chao Wang", "Shuangyong Song", "Xuelong Li"], "title": "Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation", "comment": "PrePrint", "summary": "Large language models (LLMs) excel at reasoning tasks but are expensive to\ndeploy. Thus small language models (SLMs) are fine-tuned on CoT data generated\nby LLMs to copy LLMs' abilities. However, these CoT data may include noisy\nrationales that either fail to substantiate the answers or contribute no\nadditional information to support answer prediction, which leads SLMs to\ncapture spurious correlations between questions and answers and compromise the\nquality of reasoning. In this work, we propose Chain-of-Thought Correctness\nPerception Distillation (CoPeD), which aims to improve the reasoning quality of\nthe student model from the perspectives of task setting and data utilization.\nFirstly, we introduce a correctness-aware task setting that encourages the\nstudent model to predict answers based on correct rationales and revise them\nwhen they are incorrect. This setting improves the faithfulness of reasoning\nand allows the model to learn from its mistakes. Then, we propose a\nCorrectness-Aware Weighted loss, which dynamically adjusts the contribution of\neach training instance based on the combined loss of the rationale and the\nanswer. This strategy encourages the model to focus more on samples where the\nrationale offers stronger support for the correct answer. Experiments have\nshown that CoPeD is effective on both in-distribution (IND) and\nout-of-distribution (OOD) benchmark reasoning datasets.", "AI": {"tldr": "本文提出CoPeD方法，通过引入正确性感知任务设置和正确性感知加权损失，解决小型语言模型（SLMs）在链式思考（CoT）蒸馏中因噪声原理导致的推理质量下降问题，从而提高其在推理任务上的表现。", "motivation": "大型语言模型（LLMs）推理能力强但部署成本高。小型语言模型（SLMs）通过蒸馏LLM生成的CoT数据来模仿其能力，但这些CoT数据可能包含噪声原理，导致SLMs学习到虚假关联，损害推理质量。", "method": "本文提出CoPeD（Chain-of-Thought Correctness Perception Distillation）方法，从任务设置和数据利用两方面改进SLM的推理质量：1. 引入正确性感知任务设置，鼓励学生模型基于正确原理预测答案，并在错误时进行修正，以提高推理的忠实性并从错误中学习。2. 提出正确性感知加权损失，根据原理和答案的组合损失动态调整每个训练实例的贡献，使模型更关注原理能强力支持正确答案的样本。", "result": "实验结果表明，CoPeD在分布内（IND）和分布外（OOD）基准推理数据集上均有效。", "conclusion": "CoPeD通过正确性感知任务设置和加权损失，有效解决了SLMs在CoT蒸馏中噪声原理导致的问题，显著提升了学生模型在推理任务上的表现和推理质量。"}}
{"id": "2509.06279", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06279", "abs": "https://arxiv.org/abs/2509.06279", "authors": ["Tahmin Mahmud", "Euzeli Cipriano Dos Santos Jr"], "title": "DNN-based Digital Twin Framework of a DC-DC Buck Converter using Spider Monkey Optimization Algorithm", "comment": "8 pages, 13 figures, 2 tables. Accepted for a lecture presentation at\n  the 2025 IEEE Energy Conversion Conference and Expo (ECCE 2025)", "summary": "Component ageing is a critical concern in power electronic converter systems\n(PECSs). It directly impacts the reliability, performance, and operational\nlifespan of converters used across diverse applications, including electric\nvehicles (EVs), renewable energy systems (RESs) and industrial automation.\nTherefore, understanding and monitoring component ageing is crucial for\ndeveloping robust converters and achieving long-term system reliability. This\npaper proposes a data-driven digital twin (DT) framework for DC-DC buck\nconverters, integrating deep neural network (DNN) with the spider monkey\noptimization (SMO) algorithm to monitor and predict component degradation.\nUtilizing a low-power prototype testbed along with empirical and synthetic\ndatasets, the SMO+DNN approach achieves the global optimum in 95% of trials,\nrequires 33% fewer iterations, and results in 80% fewer parameter constraint\nviolations compared to traditional methods. The DNN model achieves $R^2$ scores\nabove 0.998 for all key degradation parameters and accurately forecasts time to\nfailure ($t_{failure}$). In addition, SMO-tuned degradation profile improves\nthe converter's performance by reducing voltage ripple by 20-25% and inductor\ncurrent ripple by 15-20%.", "AI": {"tldr": "本文提出了一种基于数据驱动的数字孪生框架（SMO+DNN），用于DC-DC降压转换器组件老化监测与预测，显著提高了预测准确性和转换器性能。", "motivation": "电力电子转换器系统（PECS）中的组件老化直接影响其可靠性、性能和使用寿命，在电动汽车、可再生能源系统和工业自动化等领域尤为关键。因此，理解和监测组件老化对于开发稳健的转换器和实现长期系统可靠性至关重要。", "method": "本文提出了一种用于DC-DC降压转换器的数据驱动数字孪生（DT）框架，该框架将深度神经网络（DNN）与蜘蛛猴优化（SMO）算法相结合（SMO+DNN），以监测和预测组件退化。研究利用低功率原型测试平台以及经验和合成数据集进行验证。", "result": "SMO+DNN方法在95%的试验中达到全局最优，迭代次数减少33%，参数约束违反减少80%，优于传统方法。DNN模型在所有关键退化参数上R^2分数均高于0.998，并能准确预测故障时间（t_failure）。此外，经SMO调优的退化曲线使转换器性能提升，电压纹波减少20-25%，电感电流纹波减少15-20%。", "conclusion": "所提出的SMO+DNN数据驱动数字孪生框架能够有效监测和预测DC-DC降压转换器组件的退化，不仅提高了预测准确性，还通过优化退化曲线提升了转换器的性能。"}}
{"id": "2509.05397", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05397", "abs": "https://arxiv.org/abs/2509.05397", "authors": ["Matthew Lai", "Keegan Go", "Zhibin Li", "Torsten Kroger", "Stefan Schaal", "Kelsey Allen", "Jonathan Scholz"], "title": "RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning", "comment": "Published in Science Robotics", "summary": "Modern robotic manufacturing requires collision-free coordination of multiple\nrobots to complete numerous tasks in shared, obstacle-rich workspaces. Although\nindividual tasks may be simple in isolation, automated joint task allocation,\nscheduling, and motion planning under spatio-temporal constraints remain\ncomputationally intractable for classical methods at real-world scales.\nExisting multi-arm systems deployed in the industry rely on human intuition and\nexperience to design feasible trajectories manually in a labor-intensive\nprocess. To address this challenge, we propose a reinforcement learning (RL)\nframework to achieve automated task and motion planning, tested in an\nobstacle-rich environment with eight robots performing 40 reaching tasks in a\nshared workspace, where any robot can perform any task in any order. Our\napproach builds on a graph neural network (GNN) policy trained via RL on\nprocedurally-generated environments with diverse obstacle layouts, robot\nconfigurations, and task distributions. It employs a graph representation of\nscenes and a graph policy neural network trained through reinforcement learning\nto generate trajectories of multiple robots, jointly solving the sub-problems\nof task allocation, scheduling, and motion planning. Trained on large randomly\ngenerated task sets in simulation, our policy generalizes zero-shot to unseen\nsettings with varying robot placements, obstacle geometries, and task poses. We\nfurther demonstrate that the high-speed capability of our solution enables its\nuse in workcell layout optimization, improving solution times. The speed and\nscalability of our planner also open the door to new capabilities such as\nfault-tolerant planning and online perception-based re-planning, where rapid\nadaptation to dynamic task sets is required.", "AI": {"tldr": "本文提出一个基于强化学习（RL）的框架，利用图神经网络（GNN）策略实现多机器人协作的自动化任务分配、调度和运动规划，尤其适用于障碍物丰富的共享工作空间。", "motivation": "现代机器人制造中，在共享的、障碍物丰富的空间内，多机器人无碰撞协调完成任务的计算复杂度极高，传统方法难以应对。现有工业系统依赖人工直觉和经验手动设计轨迹，过程耗时耗力。", "method": "提出一个强化学习（RL）框架，该框架采用图神经网络（GNN）策略。GNN利用场景的图表示，并通过RL在程序化生成的、具有多样化障碍物布局、机器人配置和任务分布的环境中进行训练。它能联合解决任务分配、调度和运动规划等子问题。", "result": "该策略在模拟中通过大量随机生成的任务集训练后，能够零样本泛化到未见过的场景，包括不同的机器人放置、障碍物几何形状和任务姿态。此外，该方案的高速能力使其能够用于工作单元布局优化，并缩短了解决方案时间。", "conclusion": "该规划器的速度和可扩展性为故障容错规划和基于在线感知的重新规划等新能力打开了大门，这些能力对于需要快速适应动态任务集的应用至关重要。"}}
{"id": "2509.05335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05335", "abs": "https://arxiv.org/abs/2509.05335", "authors": ["Zebo Xu", "Shaoyun Yu", "Mark Torrance", "Guido Nottbusch", "Nan Zhao", "Zhenguang Cai"], "title": "A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research", "comment": null, "summary": "Understanding what linguistic components (e.g., phonological, semantic, and\northographic systems) modulate Chinese handwriting at the character, radical,\nand stroke levels remains an important yet understudied topic. Additionally,\nthere is a lack of comprehensive tools for capturing and batch-processing\nfine-grained handwriting data. To address these issues, we constructed a\nlarge-scale handwriting database in which 42 Chinese speakers for each\nhandwriting 1200 characters in a handwriting-to-dictation task. Additionally,\nwe enhanced the existing handwriting package and provided comprehensive\ndocumentation for the upgraded OpenHandWrite_Toolbox, which can easily modify\nthe experimental design, capture the stroke-level handwriting trajectory, and\nbatch-process handwriting measurements (e.g., latency, duration, and\npen-pressure). In analysing our large-scale database, multiple regression\nresults show that orthographic predictors impact handwriting preparation and\nexecution across character, radical, and stroke levels. Phonological factors\nalso influence execution at all three levels. Importantly, these lexical\neffects demonstrate hierarchical attenuation - they were most pronounced at the\ncharacter level, followed by the radical, and were weakest at the stroke\nlevels. These findings demonstrate that handwriting preparation and execution\nat the radical and stroke levels are closely intertwined with linguistic\ncomponents. This database and toolbox offer valuable resources for future\npsycholinguistic and neurolinguistic research on the handwriting of characters\nand sub-characters across different languages.", "AI": {"tldr": "该研究构建了一个大规模中文手写数据库并升级了手写工具箱，揭示了正字法和语音成分如何分层影响汉字、部首和笔画层面的手写准备与执行，发现词汇效应存在层级衰减。", "motivation": "中文手写过程中，语言成分（如语音、语义、正字法系统）如何在汉字、部首和笔画层面进行调节，仍是一个重要但未被充分研究的课题。此外，缺乏捕捉和批量处理精细手写数据的综合工具。", "method": "研究构建了一个大规模手写数据库，42名中文母语者每人完成1200个汉字的听写手写任务。同时，增强了现有的OpenHandWrite_Toolbox，使其能轻松修改实验设计、捕捉笔画级手写轨迹并批量处理手写测量数据（如潜伏期、持续时间、笔压）。数据分析采用多元回归。", "result": "多元回归结果显示，正字法预测因子影响汉字、部首和笔画层面的手写准备和执行。语音因素也影响所有三个层面的执行。重要的是，这些词汇效应呈现出层级衰减：在汉字层面最显著，其次是部首，在笔画层面最弱。", "conclusion": "研究结果表明，部首和笔画层面的手写准备和执行与语言成分紧密交织。该数据库和工具箱为未来关于不同语言汉字和子字符手写的心理语言学和神经语言学研究提供了宝贵资源。"}}
{"id": "2509.05381", "categories": ["cs.AI", "cs.LG", "68T01, 68T20, 68Q87"], "pdf": "https://arxiv.org/pdf/2509.05381", "abs": "https://arxiv.org/abs/2509.05381", "authors": ["Madhava Gaikwad"], "title": "Murphys Laws of AI Alignment: Why the Gap Always Wins", "comment": "21 pages", "summary": "Large language models are increasingly aligned to human preferences through\nreinforcement learning from human feedback (RLHF) and related methods such as\nDirect Preference Optimization (DPO), Constitutional AI, and RLAIF. While\neffective, these methods exhibit recurring failure patterns i.e., reward\nhacking, sycophancy, annotator drift, and misgeneralization. We introduce the\nconcept of the Alignment Gap, a unifying lens for understanding recurring\nfailures in feedback-based alignment. Using a KL-tilting formalism, we\nillustrate why optimization pressure tends to amplify divergence between proxy\nrewards and true human intent. We organize these failures into a catalogue of\nMurphys Laws of AI Alignment, and propose the Alignment Trilemma as a way to\nframe trade-offs among optimization strength, value capture, and\ngeneralization. Small-scale empirical studies serve as illustrative support.\nFinally, we propose the MAPS framework (Misspecification, Annotation, Pressure,\nShift) as practical design levers. Our contribution is not a definitive\nimpossibility theorem but a perspective that reframes alignment debates around\nstructural limits and trade-offs, offering clearer guidance for future design.", "AI": {"tldr": "本文引入“对齐鸿沟”概念，统一解释了基于反馈的大语言模型对齐方法中常见的失败模式，并提出了“对齐三难困境”和“MAPS框架”，以更好地理解和指导未来的对齐设计。", "motivation": "尽管RLHF和DPO等对齐方法有效，但它们存在奖励欺骗、奉承、标注者漂移和错误泛化等反复出现的失败模式。", "method": "引入“对齐鸿沟”概念作为统一视角；使用KL倾斜形式化说明优化压力如何放大代理奖励与真实人类意图之间的差异；将失败模式归纳为“AI对齐的墨菲定律”；提出“对齐三难困境”来权衡优化强度、价值捕获和泛化；通过小规模实证研究提供支持；提出“MAPS框架”（Misspecification, Annotation, Pressure, Shift）作为实用设计杠杆。", "result": "研究表明，优化压力倾向于放大代理奖励与真实人类意图之间的分歧。文章分类了对齐失败模式，并提出了对齐中的结构性限制和权衡，如优化强度、价值捕获和泛化之间的取舍。小规模实证研究为这些观点提供了说明性支持。", "conclusion": "本文提供了一个重新审视对齐辩论的视角，侧重于结构性限制和权衡，而非一个明确的不可能性定理，旨在为未来的对齐设计提供更清晰的指导。"}}
{"id": "2509.06553", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06553", "abs": "https://arxiv.org/abs/2509.06553", "authors": ["Johan Andreas Balle Rubak", "Khuram Naveed", "Sanyam Jain", "Lukas Esterle", "Alexandros Iosifidis", "Ruben Pauwels"], "title": "Impact of Labeling Inaccuracy and Image Noise on Tooth Segmentation in Panoramic Radiographs using Federated, Centralized and Local Learning", "comment": null, "summary": "Objectives: Federated learning (FL) may mitigate privacy constraints,\nheterogeneous data quality, and inconsistent labeling in dental diagnostic AI.\nWe compared FL with centralized (CL) and local learning (LL) for tooth\nsegmentation in panoramic radiographs across multiple data corruption\nscenarios. Methods: An Attention U-Net was trained on 2066 radiographs from six\ninstitutions across four settings: baseline (unaltered data); label\nmanipulation (dilated/missing annotations); image-quality manipulation\n(additive Gaussian noise); and exclusion of a faulty client with corrupted\ndata. FL was implemented via the Flower AI framework. Per-client training- and\nvalidation-loss trajectories were monitored for anomaly detection and a set of\nmetrics (Dice, IoU, HD, HD95 and ASSD) was evaluated on a hold-out test set.\nFrom these metrics significance results were reported through Wilcoxon\nsigned-rank test. CL and LL served as comparators. Results: Baseline: FL\nachieved a median Dice of 0.94889 (ASSD: 1.33229), slightly better than CL at\n0.94706 (ASSD: 1.37074) and LL at 0.93557-0.94026 (ASSD: 1.51910-1.69777).\nLabel manipulation: FL maintained the best median Dice score at 0.94884 (ASSD:\n1.46487) versus CL's 0.94183 (ASSD: 1.75738) and LL's 0.93003-0.94026 (ASSD:\n1.51910-2.11462). Image noise: FL led with Dice at 0.94853 (ASSD: 1.31088); CL\nscored 0.94787 (ASSD: 1.36131); LL ranged from 0.93179-0.94026 (ASSD:\n1.51910-1.77350). Faulty-client exclusion: FL reached Dice at 0.94790 (ASSD:\n1.33113) better than CL's 0.94550 (ASSD: 1.39318). Loss-curve monitoring\nreliably flagged the corrupted site. Conclusions: FL matches or exceeds CL and\noutperforms LL across corruption scenarios while preserving privacy. Per-client\nloss trajectories provide an effective anomaly-detection mechanism and support\nFL as a practical, privacy-preserving approach for scalable clinical AI\ndeployment.", "AI": {"tldr": "本研究比较了联邦学习（FL）与集中式学习（CL）和本地学习（LL）在牙科全景X光片牙齿分割任务中的表现，特别是在多种数据损坏场景下。结果显示，FL在所有腐败场景下均匹配或优于CL，并显著优于LL，同时保护了隐私，并通过损失曲线监测提供了有效的异常检测机制。", "motivation": "牙科诊断AI面临隐私限制、异构数据质量和标注不一致等挑战。联邦学习（FL）被提出作为一种潜在解决方案，以缓解这些问题。", "method": "研究使用一个Attention U-Net模型，在来自六个机构的2066张全景X光片上进行训练。实验设置包括四种场景：基线（未改变数据）、标签操纵（膨胀/缺失标注）、图像质量操纵（添加高斯噪声）以及排除一个数据损坏的客户端。FL通过Flower AI框架实现。通过监测客户端的训练和验证损失轨迹进行异常检测，并使用Dice、IoU、HD、HD95和ASSD等指标在保留测试集上进行评估。Wilcoxon符号秩检验用于报告显著性结果，CL和LL作为比较器。", "result": "在所有数据损坏场景下，FL均表现出与CL相当或更优的性能，并优于LL。例如，在基线场景下，FL的中位数Dice系数为0.94889，略优于CL（0.94706）和LL（0.93557-0.94026）。在标签操纵、图像噪声和排除故障客户端的场景中，FL也保持了最佳或接近最佳的Dice分数。此外，损失曲线监测被证明能可靠地标记出损坏的站点。", "conclusion": "联邦学习在数据损坏场景下，其性能与集中式学习相当或更优，并优于本地学习，同时能保护数据隐私。通过监测客户端的损失轨迹，可以提供有效的异常检测机制。这些结果支持联邦学习作为一种实用、保护隐私且可扩展的临床AI部署方法。"}}
{"id": "2509.05605", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05605", "abs": "https://arxiv.org/abs/2509.05605", "authors": ["Qiyuan Chen", "Hongsen Huang", "Qian Shao", "Jiahe Chen", "Jintai Chen", "Hongxia Xu", "Renjie Hua", "Ren Chuan", "Jian Wu"], "title": "Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation", "comment": "EMNLP 2025 Main", "summary": "Large Language Models (LLMs) require high quality preference datasets to\nalign with human preferences. However, conventional methods for constructing\nsuch datasets face significant challenges: reliance on pre-collected\ninstructions often leads to distribution mismatches with target models, while\nthe need for sampling multiple stochastic responses introduces substantial\ncomputational overhead. In this work, we explore a paradigm shift by leveraging\ninherent regulation of LLMs' representation space for efficient and tailored\npreference dataset construction, named Icon$^{2}$. Specifically, it first\nextracts layer-wise direction vectors to encode sophisticated human preferences\nand then uses these vectors to filter self-synthesized instructions based on\ntheir inherent consistency. During decoding, bidirectional inherent control is\napplied to steer token representations, enabling the precise generation of\nresponse pairs with clear alignment distinctions. Experimental results\ndemonstrate significant improvements in both alignment and efficiency.\nLlama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on\nAlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by\nup to 48.1%.", "AI": {"tldr": "Icon²提出了一种利用大型语言模型（LLM）内部表示空间进行高效、定制化偏好数据集构建的新范式，显著提升了对齐效果并降低了计算成本。", "motivation": "传统的偏好数据集构建方法面临挑战：依赖预收集指令可能导致与目标模型的分发不匹配，以及采样多个随机响应引入的巨大计算开销。", "method": "Icon²首先提取层级方向向量来编码复杂的人类偏好，然后利用这些向量基于内在一致性过滤自合成指令。在解码过程中，它应用双向内在控制来引导token表示，从而精确生成具有明显对齐区别的响应对。", "result": "实验结果显示，在对齐和效率方面均有显著提升。Llama3-8B和Qwen2-7B在AlpacaEval 2.0上平均胜率提高了13.89%，在Arena-Hard上提高了13.45%，同时计算成本降低了高达48.1%。", "conclusion": "Icon²通过利用LLM表示空间的内在调控，为构建高质量、高效的偏好数据集提供了一种有效的范式，解决了现有方法的痛点，实现了更好的模型对齐和更低的计算成本。"}}
{"id": "2509.06312", "categories": ["eess.SY", "cs.LG", "cs.SY", "68T07, 68T45, 93C85, 94A12", "I.2.10; I.2.6; I.2.9; C.2.1"], "pdf": "https://arxiv.org/pdf/2509.06312", "abs": "https://arxiv.org/abs/2509.06312", "authors": ["Guangyu Lei", "Tianhao Liang", "Yuqi Ping", "Xinglin Chen", "Longyu Zhou", "Junwei Wu", "Xiyuan Zhang", "Huahao Ding", "Xingjian Zhang", "Weijie Yuan", "Tingting Zhang", "Qinyu Zhang"], "title": "Enhancing Low-Altitude Airspace Security: MLLM-Enabled UAV Intent Recognition", "comment": "The paper has been submitted to IEEE Internet of Things Magazine", "summary": "The rapid development of the low-altitude economy emphasizes the critical\nneed for effective perception and intent recognition of non-cooperative\nunmanned aerial vehicles (UAVs). The advanced generative reasoning capabilities\nof multimodal large language models (MLLMs) present a promising approach in\nsuch tasks. In this paper, we focus on the combination of UAV intent\nrecognition and the MLLMs. Specifically, we first present an MLLM-enabled UAV\nintent recognition architecture, where the multimodal perception system is\nutilized to obtain real-time payload and motion information of UAVs, generating\nstructured input information, and MLLM outputs intent recognition results by\nincorporating environmental information, prior knowledge, and tactical\npreferences. Subsequently, we review the related work and demonstrate their\nprogress within the proposed architecture. Then, a use case for low-altitude\nconfrontation is conducted to demonstrate the feasibility of our architecture\nand offer valuable insights for practical system design. Finally, the future\nchallenges are discussed, followed by corresponding strategic recommendations\nfor further applications.", "AI": {"tldr": "本文提出了一种基于多模态大语言模型（MLLM）的无人机意图识别架构，以解决低空经济中非合作无人机感知和意图识别的关键需求，并通过一个低空对抗用例验证了其可行性。", "motivation": "低空经济的快速发展对非合作无人机的有效感知和意图识别提出了迫切需求。多模态大语言模型（MLLM）的先进生成推理能力为此类任务提供了有前景的解决方案。", "method": "本文首先提出了一个基于MLLM的无人机意图识别架构。该架构利用多模态感知系统获取无人机的实时载荷和运动信息，生成结构化输入。随后，MLLM结合环境信息、先验知识和战术偏好输出意图识别结果。文章还回顾了相关工作，并通过一个低空对抗用例演示了该架构的可行性。", "result": "通过低空对抗用例，本文验证了所提出架构的可行性，并为实际系统设计提供了有价值的见解。", "conclusion": "文章讨论了未来的挑战，并提出了相应的战略建议，以促进该架构在进一步应用中的发展。"}}
{"id": "2509.05433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05433", "abs": "https://arxiv.org/abs/2509.05433", "authors": ["Rui Chen", "Domenico Chiaradia", "Antonio Frisoli", "Daniele Leonardis"], "title": "HapMorph: A Pneumatic Framework for Multi-Dimensional Haptic Property Rendering", "comment": "20 pages, 5 figures", "summary": "Haptic interfaces that can simultaneously modulate multiple physical\nproperties remain a fundamental challenge in human-robot interaction. Existing\nsystems typically allow the rendering of either geometric features or\nmechanical properties, but rarely both, within wearable form factors. Here, we\nintroduce HapMorph, a pneumatic framework that enables continuous, simultaneous\nmodulation of object size and stiffness through antagonistic fabric-based\npneumatic actuators (AFPAs). We implemented a HapMorph protoytpe designed for\nhands interaction achieving size variation from 50 to 104 mm, stiffness\nmodulation up to 4.7 N/mm and mass of the wearable parts of just 21 g. Through\nsystematic characterization, we demonstrate decoupled control of size and\nstiffness properties via dual-chamber pressure regulation. Human perception\nstudies with 10 participants reveal that users can distinguish nine discrete\nstates across three size categories and three stiffness levels with 89.4%\naccuracy and 6.7 s average response time. We further demonstrate extended\narchitectures that combine AFPAs with complementary pneumatic structures to\nenable shape or geometry morphing with concurrent stiffness control. Our\nresults establish antagonistic pneumatic principle as a pathway toward\nnext-generation haptic interfaces, capable of multi-dimensiona rendering\nproperties within practical wearable constraints.", "AI": {"tldr": "本文介绍了HapMorph，一个基于气动的框架，利用对抗性织物气动执行器(AFPAs)实现可穿戴设备中物体尺寸和刚度的同步、连续调节，并通过人体感知研究验证了其有效性。", "motivation": "现有触觉接口在可穿戴设备中难以同时调节几何特征和机械特性。系统通常只能渲染其中之一，而非两者兼顾。", "method": "研究引入了HapMorph气动框架，采用对抗性织物气动执行器(AFPAs)。开发了用于手部交互的原型，通过双腔压力调节实现了尺寸和刚度的解耦控制。进行了系统特性表征和10名参与者的人体感知研究。此外，还展示了结合AFPAs与其他气动结构的扩展架构，以实现形状/几何变形与同步刚度控制。", "result": "HapMorph原型实现了50至104毫米的尺寸变化、高达4.7牛/毫米的刚度调节，可穿戴部件质量仅为21克。通过特性表征证明了尺寸和刚度属性的解耦控制。人体感知研究显示，用户能以89.4%的准确率和平均6.7秒的响应时间区分9种离散状态（3种尺寸和3种刚度）。扩展架构也证明了形状或几何变形与并发刚度控制的能力。", "conclusion": "对抗性气动原理为下一代触觉接口提供了一条途径，使其能够在实际的可穿戴限制下实现多维渲染特性。"}}
{"id": "2509.05337", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05337", "abs": "https://arxiv.org/abs/2509.05337", "authors": ["Younggeol Cho", "Gokhan Solak", "Olivia Nocentini", "Marta Lorenzini", "Andrea Fortuna", "Arash Ajoudani"], "title": "Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory", "comment": "Presented at IEEE RO-MAN 2025", "summary": "Detecting and preventing falls in humans is a critical component of assistive\nrobotic systems. While significant progress has been made in detecting falls,\nthe prediction of falls before they happen, and analysis of the transient state\nbetween stability and an impending fall remain unexplored. In this paper, we\npropose a anticipatory fall detection method that utilizes a hybrid model\ncombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory\n(LSTM) networks that decoupled the motion prediction and gait classification\ntasks to anticipate falls with high accuracy. Our approach employs real-time\nskeletal features extracted from video sequences as input for the proposed\nmodel. The DGNN acts as a classifier, distinguishing between three gait states:\nstable, transient, and fall. The LSTM-based network then predicts human\nmovement in subsequent time steps, enabling early detection of falls. The\nproposed model was trained and validated using the OUMVLP-Pose and URFD\ndatasets, demonstrating superior performance in terms of prediction error and\nrecognition accuracy compared to models relying solely on DGNN and models from\nliterature. The results indicate that decoupling prediction and classification\nimproves performance compared to addressing the unified problem using only the\nDGNN. Furthermore, our method allows for the monitoring of the transient state,\noffering valuable insights that could enhance the functionality of advanced\nassistance systems.", "AI": {"tldr": "提出了一种结合动态图神经网络（DGNN）和长短期记忆网络（LSTM）的混合模型，通过解耦运动预测和步态分类任务，实现高精度的人体跌倒预测和过渡状态监测。", "motivation": "尽管跌倒检测取得了显著进展，但在跌倒发生前进行预测以及分析稳定与即将跌倒之间的过渡状态仍是未被探索的领域，这对于辅助机器人系统至关重要。", "method": "该方法采用混合模型，结合动态图神经网络（DGNN）和长短期记忆网络（LSTM）。DGNN作为分类器，区分稳定、过渡和跌倒三种步态状态；LSTM网络则预测未来时间步的人体运动，从而实现早期跌倒检测。输入为视频序列中提取的实时骨骼特征。", "result": "该模型在OUMVLP-Pose和URFD数据集上进行了训练和验证，与仅依赖DGNN的模型及现有文献中的模型相比，在预测误差和识别准确性方面表现出卓越的性能。结果表明，解耦预测和分类任务能够有效提升性能。", "conclusion": "所提出的方法能够实现对跌倒的提前预测和对过渡状态的有效监测，为先进辅助系统提供了有价值的见解，并有望增强其功能。解耦预测和分类任务是提高性能的关键。"}}
{"id": "2509.05469", "categories": ["cs.AI", "cs.CV", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.05469", "abs": "https://arxiv.org/abs/2509.05469", "authors": ["Chenguang Wang", "Xiang Yan", "Yilong Dai", "Ziyi Wang", "Susu Xu"], "title": "From Image Generation to Infrastructure Design: a Multi-agent Pipeline for Street Design Generation", "comment": "21 pages, 8 figures", "summary": "Realistic visual renderings of street-design scenarios are essential for\npublic engagement in active transportation planning. Traditional approaches are\nlabor-intensive, hindering collective deliberation and collaborative\ndecision-making. While AI-assisted generative design shows transformative\npotential by enabling rapid creation of design scenarios, existing generative\napproaches typically require large amounts of domain-specific training data and\nstruggle to enable precise spatial variations of design/configuration in\ncomplex street-view scenes. We introduce a multi-agent system that edits and\nredesigns bicycle facilities directly on real-world street-view imagery. The\nframework integrates lane localization, prompt optimization, design generation,\nand automated evaluation to synthesize realistic, contextually appropriate\ndesigns. Experiments across diverse urban scenarios demonstrate that the system\ncan adapt to varying road geometries and environmental conditions, consistently\nyielding visually coherent and instruction-compliant results. This work\nestablishes a foundation for applying multi-agent pipelines to transportation\ninfrastructure planning and facility design.", "AI": {"tldr": "本文提出一个多智能体系统，用于在真实街景图像上直接编辑和重新设计自行车设施，以克服传统方法的劳动密集性和现有AI生成方法在精确空间变化上的局限性，从而支持主动交通规划中的公众参与。", "motivation": "在主动交通规划中，街景设计场景的逼真渲染对于公众参与至关重要。传统方法劳动密集型，阻碍了集体审议和协作决策。虽然AI辅助生成设计具有快速创建设计场景的潜力，但现有方法通常需要大量领域特定训练数据，并且难以在复杂街景中实现设计/配置的精确空间变化。", "method": "研究引入了一个多智能体系统，直接在真实世界街景图像上编辑和重新设计自行车设施。该框架整合了车道定位、提示优化、设计生成和自动化评估，以合成逼真且符合语境的设计。", "result": "在多样化的城市场景中进行的实验表明，该系统能够适应不同的道路几何形状和环境条件，持续产生视觉上连贯且符合指令的结果。", "conclusion": "这项工作为将多智能体流水线应用于交通基础设施规划和设施设计奠定了基础。"}}
{"id": "2509.06554", "categories": ["eess.IV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.06554", "abs": "https://arxiv.org/abs/2509.06554", "authors": ["Dietmar Saupe", "Tim Bleile"], "title": "Robustness and accuracy of mean opinion scores with hard and soft outlier detection", "comment": "Accepted for 17th International Conference on Quality of Multimedia\n  Experience (QoMEX'25), September 2025, Madrid, Spain", "summary": "In subjective assessment of image and video quality, observers rate or\ncompare selected stimuli. Before calculating the mean opinion scores (MOS) for\nthese stimuli from the ratings, it is recommended to identify and deal with\noutliers that may have given unreliable ratings. Several methods are available\nfor this purpose, some of which have been standardized. These methods are\ntypically based on statistics and sometimes tested by introducing synthetic\nratings from artificial outliers, such as random clickers. However, a reliable\nand comprehensive approach is lacking for comparative performance analysis of\noutlier detection methods. To fill this gap, this work proposes and applies an\nempirical worst-case analysis as a general solution. Our method involves\nevolutionary optimization of an adversarial black-box attack on outlier\ndetection algorithms, where the adversary maximizes the distortion of scale\nvalues with respect to ground truth. We apply our analysis to several hard and\nsoft outlier detection methods for absolute category ratings and show their\ndiffering performance in this stress test. In addition, we propose two new\noutlier detection methods with low complexity and excellent worst-case\nperformance. Software for adversarial attacks and data analysis is available.", "AI": {"tldr": "本文提出并应用了一种基于对抗性黑盒攻击的经验性最坏情况分析方法，用于评估主观图像/视频质量评估中离群值检测方法的性能，并提出了两种新的高性能低复杂度离群值检测方法。", "motivation": "在主观图像和视频质量评估中，计算平均意见分数（MOS）前需要识别并处理离群值。尽管现有多种离群值检测方法（包括标准化方法），但缺乏一种可靠且全面的方法来比较这些方法的性能。", "method": "本研究提出并应用了一种经验性最坏情况分析方法。该方法涉及对离群值检测算法进行对抗性黑盒攻击的演化优化，其中攻击者旨在最大化尺度值相对于真实值的失真。研究人员将此分析应用于多种硬性和软性离群值检测方法，并提出了两种新的低复杂度离群值检测方法。", "result": "通过压力测试，研究发现现有离群值检测方法表现各异。此外，本研究提出的两种新的离群值检测方法在最坏情况分析中表现出卓越的性能，且复杂度较低。相关的对抗性攻击和数据分析软件也已提供。", "conclusion": "经验性最坏情况分析是一种评估离群值检测方法性能的通用且可靠的解决方案。本研究提出的新方法在具有挑战性的场景下表现出色，为未来的主观质量评估提供了更鲁棒的离群值处理方案。"}}
{"id": "2509.05607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05607", "abs": "https://arxiv.org/abs/2509.05607", "authors": ["Qiyuan Chen", "Jiahe Chen", "Hongsen Huang", "Qian Shao", "Jintai Chen", "Renjie Hua", "Hongxia Xu", "Ruijia Wu", "Ren Chuan", "Jian Wu"], "title": "Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents", "comment": "Technical Report", "summary": "The paradigm shift from traditional ranked-based search to Generative Search\nEngines has rendered conventional SEO metrics obsolete, creating an urgent need\nto understand, measure, and optimize for content influence on synthesized\nanswers. This paper introduces a comprehensive, end-to-end framework for\nGenerative Search Engine Optimization (GSEO) to address this challenge. We make\ntwo primary contributions. First, we construct CC-GSEO-Bench, a large-scale,\ncontent-centric benchmark, and propose a multi-dimensional evaluation framework\nthat systematically quantifies influence, moving beyond surface-level\nattribution to assess substantive semantic impact. Second, we design a novel\nmulti-agent system that operationalizes this framework, automating the\nstrategic refinement of content through a collaborative analyze-revise-evaluate\nworkflow. Our empirical analysis using this framework reveals novel insights\ninto the dynamics of content influence, offering actionable strategies for\ncreators and establishing a principled foundation for future GSEO research.", "AI": {"tldr": "本文提出了一套针对生成式搜索引擎优化（GSEO）的端到端框架，包括一个大规模基准（CC-GSEO-Bench）和一个多智能体系统，旨在衡量和优化内容对生成答案的影响力。", "motivation": "传统基于排名的搜索引擎优化（SEO）指标在生成式搜索引擎（GSE）范式下已过时，迫切需要理解、衡量和优化内容对合成答案的影响。", "method": "1. 构建了CC-GSEO-Bench，一个大规模、以内容为中心的基准。2. 提出了一个多维度评估框架，系统地量化影响力，超越表面归因，评估实质性语义影响。3. 设计了一个新颖的多智能体系统，通过协作的“分析-修订-评估”工作流，自动化内容策略性优化。", "result": "通过该框架进行的实证分析揭示了内容影响力动态的新颖见解，为内容创作者提供了可操作的策略。", "conclusion": "该研究为未来的GSEO研究奠定了原则性基础，并为内容创作者提供了实用的优化策略。"}}
{"id": "2509.06425", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06425", "abs": "https://arxiv.org/abs/2509.06425", "authors": ["Yifan Wang", "Wenhua Li", "Zhenlong Wang", "Xinrui Zhang", "Jianfeng Sun", "Qianfu Xia", "Zhongtao Gou", "Jiangang Rong", "Tao Ye"], "title": "First-Principle Modeling Framework of Boost Converter Dynamics for Precise Energy Conversions in Space", "comment": "24 pages, 30 pages supplementary material, 5 figures, 14\n  supplementary figures, 6 supplementary tables", "summary": "Boost converters are essential for modern electrification and intelligent\ntechnologies. However, conventional Boost converter models relying on\nsteady-state assumptions fail to accurately predict transient behaviors during\ninput voltage and load fluctuations, which cause significant output voltage\novershoots and instability, resulting in failures of electrical systems,\nthereby restricting their use in space. This study introduces a first-principle\nmodeling framework that derives precise dynamic equations for Boost converters\nby incorporating non-ideal component coupling. As compared to the most accurate\nexisting Boost converter model, the proposed models reduce steady-state and\ndynamic-state errors between experimental and simulated output voltages by\nfactors of 11.0 (from 20.9% to 1.9%) and 15.4 (from 77.1% to 5.0%) under input\nvoltage variations, and by factors of 10.2 (from 15.3% to 1.5%) and 35.1 (from\n42.1% to 1.2%) under load changes, respectively. Consequently, a reliable Boost\nconverter is accordingly designed and on-orbit deployed for precise energy\nconversions.", "AI": {"tldr": "本研究提出了一种基于第一性原理的升压变换器建模框架，通过考虑非理想元件耦合，显著提高了对瞬态行为的预测精度，并成功应用于太空能源转换。", "motivation": "传统的升压变换器模型依赖稳态假设，无法准确预测输入电压和负载波动时的瞬态行为，导致输出电压过冲和系统不稳定，限制了其在空间等关键领域的应用。", "method": "本研究引入了一种第一性原理建模框架，通过结合非理想元件耦合，推导出了升压变换器的精确动态方程。", "result": "与现有最精确的模型相比，所提出的模型在输入电压变化下，将实验与仿真输出电压之间的稳态误差降低了11.0倍（从20.9%降至1.9%），动态误差降低了15.4倍（从77.1%降至5.0%）。在负载变化下，稳态误差降低了10.2倍（从15.3%降至1.5%），动态误差降低了35.1倍（从42.1%降至1.2%）。", "conclusion": "所提出的模型显著提高了升压变换器在瞬态条件下的预测精度和可靠性，并成功设计了一个可靠的升压变换器并已在轨部署，用于精确的能量转换。"}}
{"id": "2509.05475", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05475", "abs": "https://arxiv.org/abs/2509.05475", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Learning Tool-Aware Adaptive Compliant Control for Autonomous Regolith Excavation", "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Autonomous regolith excavation is a cornerstone of in-situ resource\nutilization for a sustained human presence beyond Earth. However, this task is\nfundamentally hindered by the complex interaction dynamics of granular media\nand the operational need for robots to use diverse tools. To address these\nchallenges, this work introduces a framework where a model-based reinforcement\nlearning agent learns within a parallelized simulation. This environment\nleverages high-fidelity particle physics and procedural generation to create a\nvast distribution of both lunar terrains and excavation tool geometries. To\nmaster this diversity, the agent learns an adaptive interaction strategy by\ndynamically modulating its own stiffness and damping at each control step\nthrough operational space control. Our experiments demonstrate that training\nwith a procedural distribution of tools is critical for generalization and\nenables the development of sophisticated tool-aware behavior. Furthermore, we\nshow that augmenting the agent with visual feedback significantly improves task\nsuccess. These results represent a validated methodology for developing the\nrobust and versatile autonomous systems required for the foundational tasks of\nfuture space missions.", "AI": {"tldr": "本文提出了一种基于模型的强化学习框架，用于自主月壤挖掘。该框架在并行仿真中训练智能体，利用高保真粒子物理和程序化生成技术创建多样化的月壤地形和挖掘工具几何形状。智能体通过动态调节自身刚度和阻尼来学习自适应交互策略，实验证明程序化工具分布对泛化能力至关重要，视觉反馈能显著提高任务成功率。", "motivation": "自主月壤挖掘是空间资源利用的关键，但面临颗粒介质复杂交互动力学和机器人需使用多样化工具的挑战。", "method": "引入了一个模型基强化学习智能体，在并行模拟环境中学习。该环境利用高保真粒子物理和程序化生成技术，创建了大量月壤地形和挖掘工具几何形状。智能体通过操作空间控制，在每个控制步骤动态调节自身的刚度和阻尼，学习自适应交互策略。此外，还通过视觉反馈增强了智能体。", "result": "使用程序化工具分布进行训练对泛化能力至关重要，并能促使智能体发展出复杂的工具感知行为。增强智能体的视觉反馈显著提高了任务成功率。", "conclusion": "该研究提供了一种经过验证的方法，用于开发未来空间任务基础任务所需的鲁棒且多功能的自主系统。"}}
{"id": "2509.05340", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05340", "abs": "https://arxiv.org/abs/2509.05340", "authors": ["Dibya Jyoti Bora", "Mrinal Kanti Mishra"], "title": "Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging", "comment": "15 pages, 10 figures", "summary": "Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a\npivotal challenge in medical image analysis due to the heterogeneous nature of\ntumor morphology and intensity distributions. Accurate delineation of tumor\nboundaries is critical for clinical decision-making, radiotherapy planning, and\nlongitudinal disease monitoring. In this study, we perform a comprehensive\ncomparative analysis of two major clustering paradigms applied in MRI tumor\nsegmentation: hard clustering, exemplified by the K-Means algorithm, and soft\nclustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each\npixel strictly to a single cluster, FCM introduces partial memberships, meaning\neach pixel can belong to multiple clusters with varying degrees of association.\nExperimental validation was performed using the BraTS2020 dataset,\nincorporating pre-processing through Gaussian filtering and Contrast Limited\nAdaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice\nSimilarity Coefficient (DSC) and processing time, which collectively\ndemonstrated that K-Means achieved superior speed with an average runtime of\n0.3s per image, whereas FCM attained higher segmentation accuracy with an\naverage DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher\ncomputational cost (1.3s per image). These results highlight the inherent\ntrade-off between computational efficiency and boundary precision.", "AI": {"tldr": "本研究比较了K-Means和模糊C均值（FCM）两种聚类算法在脑肿瘤MRI图像分割中的表现。结果显示K-Means速度更快，而FCM在分割精度上更高，但计算成本也更高。", "motivation": "由于肿瘤形态和强度分布的异质性，从MRI图像中精确分割脑肿瘤仍是医学图像分析中的关键挑战。准确描绘肿瘤边界对于临床决策、放疗计划和疾病监测至关重要。", "method": "本研究对硬聚类（K-Means）和软聚类（模糊C均值，FCM）两种主要聚类范式进行了比较分析。实验使用BraTS2020数据集，并进行高斯滤波和对比度受限自适应直方图均衡化（CLAHE）预处理。评估指标包括Dice相似系数（DSC）和处理时间。", "result": "K-Means算法在速度上表现优异，平均每张图像运行时间为0.3秒。而FCM算法在分割精度上更高，平均DSC为0.67，K-Means为0.43。然而，FCM的计算成本更高，平均每张图像运行时间为1.3秒。", "conclusion": "研究结果突出了计算效率和边界精度之间固有的权衡关系，K-Means速度快但精度低，FCM精度高但速度慢。"}}
{"id": "2509.05550", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05550", "abs": "https://arxiv.org/abs/2509.05550", "authors": ["Zixi Li"], "title": "TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation", "comment": "Code available at: https://github.com/lizixi-0x2F/TreeGPT", "summary": "We introduce TreeGPT, a novel neural architecture that combines\ntransformer-based attention mechanisms with global parent-child aggregation for\nprocessing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.\nUnlike traditional approaches that rely solely on sequential processing or\ngraph neural networks, TreeGPT employs a hybrid design that leverages both\nself-attention for capturing local dependencies and a specialized Tree\nFeed-Forward Network (TreeFFN) for modeling hierarchical tree structures\nthrough iterative message passing.\n  The core innovation lies in our Global Parent-Child Aggregation mechanism,\nformalized as: $$h_i^{(t+1)} = \\sigma \\Big( h_i^{(0)} + W_{pc} \\sum_{(p,c) \\in\nE_i} f(h_p^{(t)}, h_c^{(t)}) + b \\Big)$$ where $h_i^{(t)}$ represents the\nhidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edges\ninvolving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. This\nformulation enables each node to progressively aggregate information from the\nentire tree structure through $T$ iterations.\n  Our architecture integrates optional enhancements including gated aggregation\nwith learnable edge weights, residual connections for gradient stability, and\nbidirectional propagation for capturing both bottom-up and top-down\ndependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challenging\nvisual reasoning benchmark requiring abstract pattern recognition and rule\ninference. Experimental results demonstrate that TreeGPT achieves 96\\%\naccuracy, significantly outperforming transformer baselines (1.3\\%),\nlarge-scale models like Grok-4 (15.9\\%), and specialized program synthesis\nmethods like SOAR (52\\%) while using only 1.5M parameters. Our comprehensive\nablation study reveals that edge projection is the most critical component,\nwith the combination of edge projection and gating achieving optimal\nperformance.", "AI": {"tldr": "TreeGPT是一种新颖的神经网络架构，结合了Transformer的注意力机制和全局父子聚合机制来处理抽象语法树（ASTs），在程序合成任务中表现出色，尤其在ARC Prize 2025数据集上以1.5M参数实现了96%的准确率，显著优于现有模型。", "motivation": "传统的程序合成方法要么依赖于顺序处理，要么依赖于图神经网络，这些方法在处理抽象语法树（ASTs）时可能无法有效捕捉局部依赖和复杂的层次结构。因此，需要一种能够同时利用这两种信息的新方法。", "method": "TreeGPT采用混合设计，结合了Transformer的自注意力机制（用于捕捉局部依赖）和专门的树前馈网络（TreeFFN，通过迭代消息传递建模层次树结构）。其核心创新是“全局父子聚合机制”，允许每个节点通过多轮迭代从整个树结构中聚合信息。该架构还包括门控聚合、可学习的边权重、残差连接和双向传播等可选增强。", "result": "在ARC Prize 2025数据集上，TreeGPT实现了96%的准确率，显著优于Transformer基线（1.3%）、大型模型Grok-4（15.9%）和专业程序合成方法SOAR（52%），而仅使用1.5M参数。全面的消融研究表明，边投影是最关键的组件，边投影和门控的结合实现了最佳性能。", "conclusion": "TreeGPT通过其混合架构和全局父子聚合机制，有效地结合了局部依赖和层次结构建模，在程序合成任务（特别是视觉推理基准）中展现了卓越的性能和参数效率，显著超越了现有模型，证明了其设计理念的有效性。"}}
{"id": "2509.06592", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06592", "abs": "https://arxiv.org/abs/2509.06592", "authors": ["Daniel Scholz", "Ayhan Can Erdur", "Robbie Holland", "Viktoria Ehm", "Jan C. Peeken", "Benedikt Wiestler", "Daniel Rueckert"], "title": "Contrastive Anatomy-Contrast Disentanglement: A Domain-General MRI Harmonization Method", "comment": null, "summary": "Magnetic resonance imaging (MRI) is an invaluable tool for clinical and\nresearch applications. Yet, variations in scanners and acquisition parameters\ncause inconsistencies in image contrast, hindering data comparability and\nreproducibility across datasets and clinical studies. Existing scanner\nharmonization methods, designed to address this challenge, face limitations,\nsuch as requiring traveling subjects or struggling to generalize to unseen\ndomains. We propose a novel approach using a conditioned diffusion autoencoder\nwith a contrastive loss and domain-agnostic contrast augmentation to harmonize\nMR images across scanners while preserving subject-specific anatomy. Our method\nenables brain MRI synthesis from a single reference image. It outperforms\nbaseline techniques, achieving a +7% PSNR improvement on a traveling subjects\ndataset and +18% improvement on age regression in unseen. Our model provides\nrobust, effective harmonization of brain MRIs to target scanners without\nrequiring fine-tuning. This advancement promises to enhance comparability,\nreproducibility, and generalizability in multi-site and longitudinal clinical\nstudies, ultimately contributing to improved healthcare outcomes.", "AI": {"tldr": "该研究提出了一种基于条件扩散自编码器的新方法，结合对比损失和领域无关对比增强，以协调不同MRI扫描仪之间的图像对比度，同时保留个体解剖结构，显著提升了图像合成和下游任务的性能。", "motivation": "MRI图像因扫描仪和采集参数的差异导致对比度不一致，影响了数据可比性和研究的可复现性。现有协调方法存在局限性，如需要旅行受试者或难以泛化到未见领域。", "method": "提出了一种新颖的方法，使用带有对比损失（contrastive loss）和领域无关对比增强（domain-agnostic contrast augmentation）的条件扩散自编码器（conditioned diffusion autoencoder），以协调不同扫描仪的MR图像，同时保留受试者特异性解剖结构。该方法能够从单一参考图像合成脑部MRI。", "result": "该方法优于基线技术，在旅行受试者数据集上实现了+7%的PSNR改进，在未见领域年龄回归任务上实现了+18%的改进。模型能够对脑部MRI进行鲁棒、有效的协调，无需微调即可适应目标扫描仪。", "conclusion": "这项进展有望增强多中心和纵向临床研究中的可比性、可复现性和泛化能力，最终有助于改善医疗保健结果。"}}
{"id": "2509.05609", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05609", "abs": "https://arxiv.org/abs/2509.05609", "authors": ["Xugang Lu", "Peng Shen", "Yu Tsao", "Hisashi Kawai"], "title": "New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR", "comment": null, "summary": "Aligning acoustic and linguistic representations is a central challenge to\nbridge the pre-trained models in knowledge transfer for automatic speech\nrecognition (ASR). This alignment is inherently structured and asymmetric:\nwhile multiple consecutive acoustic frames typically correspond to a single\nlinguistic token (many-to-one), certain acoustic transition regions may relate\nto multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often\ninclude frames with no linguistic counterpart, such as background noise or\nsilence may lead to imbalanced matching conditions. In this work, we take a new\ninsight to regard alignment and matching as a detection problem, where the goal\nis to identify meaningful correspondences with high precision and recall\nensuring full coverage of linguistic tokens while flexibly handling redundant\nor noisy acoustic frames in transferring linguistic knowledge for ASR. Based on\nthis new insight, we propose an unbalanced optimal transport-based alignment\nmodel that explicitly handles distributional mismatch and structural\nasymmetries with soft and partial matching between acoustic and linguistic\nmodalities. Our method ensures that every linguistic token is grounded in at\nleast one acoustic observation, while allowing for flexible, probabilistic\nmappings from acoustic to linguistic units. We evaluate our proposed model with\nexperiments on an CTC-based ASR system with a pre-trained language model for\nknowledge transfer. Experimental results demonstrate the effectiveness of our\napproach in flexibly controlling degree of matching and hence to improve ASR\nperformance.", "AI": {"tldr": "本文提出了一种基于非平衡最优传输的对齐模型，将声学与语言表示的对齐视为检测问题，以解决自动语音识别（ASR）中知识迁移时声学和语言模态之间的结构不对称和分布不匹配问题，从而提高ASR性能。", "motivation": "在ASR中，将预训练模型中的知识迁移需要对齐声学和语言表示。这存在固有的结构不对称（多对一、一对多）和不平衡匹配条件（如背景噪声或静音帧无对应语言单元），这些挑战促使研究者寻求更有效的对齐方法。", "method": "研究将对齐和匹配问题视为一个检测问题，目标是高精度和高召回率地识别有意义的对应关系。提出了一种基于非平衡最优传输的对齐模型，该模型明确处理了分布不匹配和结构不对称问题，实现了声学和语言模态之间的软性与部分匹配。它确保每个语言标记都至少有一个声学观测基础，并允许从声学单元到语言单元的灵活概率映射。", "result": "通过在基于CTC的ASR系统上，利用预训练语言模型进行知识迁移的实验，结果表明所提出的方法能够有效地控制匹配程度，从而改善ASR性能。", "conclusion": "该研究提出的基于非平衡最优传输的对齐模型，通过将对齐视为检测问题，并灵活处理声学与语言模态间的结构不对称和分布不匹配，有效提升了ASR系统中的知识迁移效果和整体性能。"}}
{"id": "2509.06447", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06447", "abs": "https://arxiv.org/abs/2509.06447", "authors": ["Marwan Mostafa", "Daniel Wenser", "Payam Teimourzadeh Baboli", "Christian Becker"], "title": "Unified Graph-Theoretic Modeling of Multi-Energy Flows in Distribution Systems", "comment": null, "summary": "The increasing complexity of energy systems due to sector coupling and\ndecarbonization calls for unified modeling frameworks that capture the physical\nand structural interactions between electricity, gas, and heat networks. This\npaper presents a graph-based modeling approach for multi-energy systems, where\neach domain is represented as a layer in a multi-layer graph, and coupling\ntechnologies are modeled as inter-layer edges via a dedicated coupling layer. A\nsteady-state solver based on a block-structured Newton-Raphson method is\ndeveloped to jointly compute flows and state variables across all carriers. The\nproposed model is tested and validated on a realistic case study based on data\nfrom a German distribution network. The results demonstrate convergence,\nnumerical accuracy, and consistent domain interaction, and demonstrate the\nmethod's applicability for system-wide analysis and its potential as a\nfoundation for future optimizations in integrated energy systems.", "AI": {"tldr": "本文提出了一种基于多层图的统一建模框架，用于分析电力、燃气和热力等多能源系统，并开发了基于牛顿-拉夫逊法的稳态求解器，成功应用于实际案例并验证了其有效性。", "motivation": "由于部门耦合和脱碳导致能源系统日益复杂，需要统一的建模框架来捕捉电力、燃气和热力网络之间的物理和结构相互作用。", "method": "该研究采用基于图的建模方法，将每个能源领域表示为多层图中的一个层，耦合技术通过专用耦合层建模为层间边。开发了一个基于块结构牛顿-拉夫逊方法的稳态求解器，用于联合计算所有载体的流量和状态变量。", "result": "该模型在一个基于德国配电网络数据的真实案例研究中进行了测试和验证。结果表明，该方法具有收敛性、数值精度和一致的域交互，并证明了其在系统范围分析中的适用性。", "conclusion": "该方法可作为集成能源系统未来优化的基础，具有在系统范围分析中的应用潜力。"}}
{"id": "2509.05500", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05500", "abs": "https://arxiv.org/abs/2509.05500", "authors": ["Yanda Yang", "Max Sokolich", "Fatma Ceren Kirmizitas", "Sambeeta Das", "Andreas A. Malikopoulos"], "title": "Microrobot Vascular Parkour: Analytic Geometry-based Path Planning with Real-time Dynamic Obstacle Avoidance", "comment": "56 pages, 19 figures including Supplementary Materials. Supplementary\n  videos available at\n  https://robotyyd.github.io/yanda-yang.github.io/vascular-parkour.html.\n  Preprint. This version has not been peer reviewed", "summary": "Autonomous microrobots in blood vessels could enable minimally invasive\ntherapies, but navigation is challenged by dense, moving obstacles. We propose\na real-time path planning framework that couples an analytic geometry global\nplanner (AGP) with two reactive local escape controllers, one based on rules\nand one based on reinforcement learning, to handle sudden moving obstacles.\nUsing real-time imaging, the system estimates the positions of the microrobot,\nobstacles, and targets and computes collision-free motions. In simulation, AGP\nyields shorter paths and faster planning than weighted A* (WA*), particle swarm\noptimization (PSO), and rapidly exploring random trees (RRT), while maintaining\nfeasibility and determinism. We extend AGP from 2D to 3D without loss of speed.\nIn both simulations and experiments, the combined global planner and local\ncontrollers reliably avoid moving obstacles and reach targets. The average\nplanning time is 40 ms per frame, compatible with 25 fps image acquisition and\nreal-time closed-loop control. These results advance autonomous microrobot\nnavigation and targeted drug delivery in vascular environments.", "AI": {"tldr": "该研究提出了一种实时路径规划框架，结合解析几何全局规划器（AGP）和两种局部避障控制器（基于规则和基于强化学习），使血管内自主微型机器人能够有效避开移动障碍物并到达目标。", "motivation": "血管内自主微型机器人的导航面临密集、移动障碍物的挑战，这阻碍了微创治疗的应用。", "method": "本研究提出一个实时路径规划框架：1) 采用解析几何全局规划器（AGP）进行全局路径规划；2) 结合两个反应式局部逃逸控制器（一个基于规则，一个基于强化学习）来处理突然出现的移动障碍物；3) 利用实时成像估计微型机器人、障碍物和目标的位置，并计算无碰撞运动；4) 将AGP从2D扩展到3D。", "result": "在仿真中，AGP比加权A*（WA*）、粒子群优化（PSO）和快速探索随机树（RRT）生成更短的路径和更快的规划，同时保持可行性和确定性。AGP扩展到3D后速度没有损失。在仿真和实验中，结合全局规划器和局部控制器能可靠地避开移动障碍物并到达目标。平均规划时间为每帧40毫秒，与25帧/秒的图像采集和实时闭环控制兼容。", "conclusion": "这些成果推动了血管环境中自主微型机器人导航和靶向药物递送技术的发展。"}}
{"id": "2509.05341", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05341", "abs": "https://arxiv.org/abs/2509.05341", "authors": ["Abhijeet Manoj Pal", "Rajbabu Velmurugan"], "title": "Handling imbalance and few-sample size in ML based Onion disease classification", "comment": "6 pages, 8 figures", "summary": "Accurate classification of pests and diseases plays a vital role in precision\nagriculture, enabling efficient identification, targeted interventions, and\npreventing their further spread. However, current methods primarily focus on\nbinary classification, which limits their practical applications, especially in\nscenarios where accurately identifying the specific type of disease or pest is\nessential. We propose a robust deep learning based model for multi-class\nclassification of onion crop diseases and pests. We enhance a pre-trained\nConvolutional Neural Network (CNN) model by integrating attention based modules\nand employing comprehensive data augmentation pipeline to mitigate class\nimbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1\nscore on real-world field image dataset. This model gives better results than\nother approaches using the same datasets.", "AI": {"tldr": "本文提出了一种基于深度学习的洋葱作物病虫害多分类模型，通过集成注意力机制和数据增强，在真实世界图像数据集上实现了高精度分类。", "motivation": "现有病虫害分类方法主要集中于二分类，这限制了其在需要精确识别特定病虫害类型的实际应用中的效用。", "method": "研究人员通过集成基于注意力机制的模块并采用全面的数据增强管道来缓解类别不平衡问题，从而增强了一个预训练的卷积神经网络（CNN）模型。", "result": "该模型在真实世界田间图像数据集上实现了96.90%的总体准确率和0.96的F1分数，并且比使用相同数据集的其他方法取得了更好的结果。", "conclusion": "所提出的基于深度学习的模型能够鲁棒且准确地进行洋葱作物病虫害的多类别分类，并在性能上超越了现有方法，为精准农业提供了有效工具。"}}
{"id": "2509.05578", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05578", "abs": "https://arxiv.org/abs/2509.05578", "authors": ["Ruixun Liu", "Lingyu Kong", "Derun Li", "Hang Zhao"], "title": "OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision", "comment": null, "summary": "Multimodal large language models (MLLMs) have shown strong vision-language\nreasoning abilities but still lack robust 3D spatial understanding, which is\ncritical for autonomous driving. This limitation stems from two key challenges:\n(1) the difficulty of constructing accessible yet effective 3D representations\nwithout expensive manual annotations, and (2) the loss of fine-grained spatial\ndetails in VLMs due to the absence of large-scale 3D vision-language\npretraining. To address these challenges, we propose OccVLA, a novel framework\nthat integrates 3D occupancy representations into a unified multimodal\nreasoning process. Unlike prior approaches that rely on explicit 3D inputs,\nOccVLA treats dense 3D occupancy as both a predictive output and a supervisory\nsignal, enabling the model to learn fine-grained spatial structures directly\nfrom 2D visual inputs. The occupancy predictions are regarded as implicit\nreasoning processes and can be skipped during inference without performance\ndegradation, thereby adding no extra computational overhead. OccVLA achieves\nstate-of-the-art results on the nuScenes benchmark for trajectory planning and\ndemonstrates superior performance on 3D visual question-answering tasks,\noffering a scalable, interpretable, and fully vision-based solution for\nautonomous driving.", "AI": {"tldr": "OccVLA框架通过将3D占据表示整合到多模态推理中，解决了多模态大语言模型（MLLMs）在3D空间理解方面的局限性，实现了自动驾驶任务的先进性能。", "motivation": "多模态大语言模型（MLLMs）在视觉-语言推理方面表现出色，但在3D空间理解方面仍显不足，这对于自动驾驶至关重要。主要挑战在于：1) 难以在没有昂贵手动标注的情况下构建可访问且有效的3D表示；2) 缺乏大规模3D视觉-语言预训练导致细粒度空间细节的丢失。", "method": "OccVLA提出了一种新颖的框架，将3D占据表示集成到统一的多模态推理过程中。与依赖显式3D输入的方法不同，OccVLA将密集的3D占据视为预测输出和监督信号，使模型能够直接从2D视觉输入中学习细粒度的空间结构。占据预测被视为隐式推理过程，推理时可以跳过，不影响性能，也不增加计算开销。", "result": "OccVLA在nuScenes基准测试的轨迹规划任务上取得了最先进的结果，并在3D视觉问答任务中表现出卓越的性能。", "conclusion": "OccVLA为自动驾驶提供了一个可扩展、可解释且完全基于视觉的解决方案，有效提升了MLLMs的3D空间理解能力。"}}
{"id": "2509.06617", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06617", "abs": "https://arxiv.org/abs/2509.06617", "authors": ["Daniel Scholz", "Ayhan Can Erdur", "Viktoria Ehm", "Anke Meyer-Baese", "Jan C. Peeken", "Daniel Rueckert", "Benedikt Wiestler"], "title": "MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image Analysis", "comment": null, "summary": "Vision foundation models like DINOv2 demonstrate remarkable potential in\nmedical imaging despite their origin in natural image domains. However, their\ndesign inherently works best for uni-modal image analysis, limiting their\neffectiveness for multi-modal imaging tasks that are common in many medical\nfields, such as neurology and oncology. While supervised models perform well in\nthis setting, they fail to leverage unlabeled datasets and struggle with\nmissing modalities, a frequent challenge in clinical settings. To bridge these\ngaps, we introduce MM-DINOv2, a novel and efficient framework that adapts the\npre-trained vision foundation model DINOv2 for multi-modal medical imaging. Our\napproach incorporates multi-modal patch embeddings, enabling vision foundation\nmodels to effectively process multi-modal imaging data. To address missing\nmodalities, we employ full-modality masking, which encourages the model to\nlearn robust cross-modality relationships. Furthermore, we leverage\nsemi-supervised learning to harness large unlabeled datasets, enhancing both\nthe accuracy and reliability of medical predictions. Applied to glioma subtype\nclassification from multi-sequence brain MRI, our method achieves a Matthews\nCorrelation Coefficient (MCC) of 0.6 on an external test set, surpassing\nstate-of-the-art supervised approaches by +11.1%. Our work establishes a\nscalable and robust solution for multi-modal medical imaging tasks, leveraging\npowerful vision foundation models pre-trained on natural images while\naddressing real-world clinical challenges such as missing data and limited\nannotations.", "AI": {"tldr": "MM-DINOv2是一个新颖高效的框架，旨在将DINOv2等视觉基础模型应用于多模态医学图像分析，有效处理模态缺失和利用未标注数据，并在胶质瘤亚型分类任务中超越了现有监督方法。", "motivation": "DINOv2等视觉基础模型在医学图像分析中表现出潜力，但其设计主要针对单模态图像，难以有效处理多模态医学成像任务。而现有监督模型无法利用未标注数据集，并且难以应对临床中常见的模态缺失问题。", "method": "本研究引入了MM-DINOv2框架。通过整合多模态块嵌入，使视觉基础模型能够处理多模态图像数据。为解决模态缺失问题，采用了全模态掩蔽策略，促使模型学习鲁棒的跨模态关系。此外，还利用半监督学习来利用大规模未标注数据集。", "result": "将MM-DINOv2应用于多序列脑MRI的胶质瘤亚型分类任务，在外部测试集上实现了0.6的Matthews相关系数（MCC），相比最先进的监督方法提高了11.1%。", "conclusion": "MM-DINOv2为多模态医学成像任务提供了一个可扩展且鲁棒的解决方案，它利用了在自然图像上预训练的强大视觉基础模型，同时解决了现实世界临床挑战，如数据缺失和标注受限问题。"}}
{"id": "2509.05617", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05617", "abs": "https://arxiv.org/abs/2509.05617", "authors": ["Shay Dahary", "Avi Edana", "Alexander Apartsin", "Yehudit Aperstein"], "title": "From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics", "comment": "5 pages, 2 figures", "summary": "The emotional content of song lyrics plays a pivotal role in shaping listener\nexperiences and influencing musical preferences. This paper investigates the\ntask of multi-label emotional attribution of song lyrics by predicting six\nemotional intensity scores corresponding to six fundamental emotions. A\nmanually labeled dataset is constructed using a mean opinion score (MOS)\napproach, which aggregates annotations from multiple human raters to ensure\nreliable ground-truth labels. Leveraging this dataset, we conduct a\ncomprehensive evaluation of several publicly available large language models\n(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model\nspecifically for predicting multi-label emotion scores. Experimental results\nreveal the relative strengths and limitations of zero-shot and fine-tuned\nmodels in capturing the nuanced emotional content of lyrics. Our findings\nhighlight the potential of LLMs for emotion recognition in creative texts,\nproviding insights into model selection strategies for emotion-based music\ninformation retrieval applications. The labeled dataset is available at\nhttps://github.com/LLM-HITCS25S/LyricsEmotionAttribution.", "AI": {"tldr": "本文研究歌词的多标签情感归因任务，通过构建基于MOS的人工标注数据集，评估零样本LLM和微调BERT模型在预测六种基本情感强度方面的表现，并揭示了它们的优缺点。", "motivation": "歌词的情感内容在塑造听众体验和影响音乐偏好方面起着关键作用。本研究旨在解决歌词的多标签情感归因问题，即预测六种基本情感的强度分数。", "method": "研究方法包括：1) 构建了一个使用平均意见得分（MOS）方法的手动标注数据集，该数据集聚合了多个人工标注者的注释以确保标签的可靠性。2) 在零样本（zero-shot）场景下，对多个公开可用的大型语言模型（LLM）进行了全面评估。3) 对一个基于BERT的模型进行了微调，专门用于预测多标签情感分数。", "result": "实验结果揭示了零样本模型和微调模型在捕捉歌词细微情感内容方面的相对优势和局限性。", "conclusion": "研究结果突出了大型语言模型（LLM）在创意文本情感识别方面的潜力，并为基于情感的音乐信息检索应用提供了模型选择策略的见解。"}}
{"id": "2509.06534", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.06534", "abs": "https://arxiv.org/abs/2509.06534", "authors": ["Ayush Pandey"], "title": "Parameter Robustness in Data-Driven Estimation of Dynamical Systems", "comment": "Submitted for publication in the IEEE Conference on Decision and\n  Control (CDC) 2025", "summary": "We study the robustness of system estimation to parametric perturbations in\nsystem dynamics and initial conditions. We define the problem of\nsensitivity-based parametric uncertainty quantification in dynamical system\nestimation. The main contribution of this paper is the development of a novel\nrobustness metric for estimation of parametrized linear dynamical systems with\nand without control actions. For the computation of this metric, we delineate\nthe uncertainty contributions arising from control actions, system dynamics,\nand initial conditions. Furthermore, to validate our theoretical findings, we\nestablish connections between these new results and the existing literature on\nthe robustness of model reduction. This work provides guidance for selecting\nestimation methods based on tolerable levels of parametric uncertainty and\npaves the way for new cost functions in data-driven estimation that reward\nsensitivity to a desired subset of parameters while penalizing others.", "AI": {"tldr": "本文研究了系统估计对参数扰动的鲁棒性，并提出了一种新的鲁棒性度量来量化参数不确定性。", "motivation": "研究的动机在于理解和量化系统动力学和初始条件中参数扰动对系统估计鲁棒性的影响，从而解决动态系统估计中的敏感性参数不确定性问题。", "method": "本文开发了一种新颖的鲁棒性度量，用于参数化线性动态系统的估计（无论有无控制作用）。在计算此度量时，作者区分了来自控制作用、系统动力学和初始条件的不确定性贡献。此外，还通过与现有模型降阶鲁棒性文献的联系来验证理论发现。", "result": "主要成果是开发并计算了一种针对参数化线性动态系统估计的新型鲁棒性度量，并明确了控制作用、系统动力学和初始条件对不确定性的贡献。同时，建立了这些新结果与模型降阶鲁棒性现有文献之间的联系。", "conclusion": "这项工作为根据可容忍的参数不确定性水平选择估计方法提供了指导，并为数据驱动估计中新的成本函数奠定了基础，这些成本函数可以奖励对所需参数子集的敏感性，同时惩罚其他参数。"}}
{"id": "2509.05547", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.05547", "abs": "https://arxiv.org/abs/2509.05547", "authors": ["Ziling Chen", "Yeo Jung Yoon", "Rolando Bautista-Montesano", "Zhen Zhao", "Ajay Mandlekar", "John Liu"], "title": "TeleopLab: Accessible and Intuitive Teleoperation of a Robotic Manipulator for Remote Labs", "comment": null, "summary": "Teleoperation offers a promising solution for enabling hands-on learning in\nremote education, particularly in environments requiring interaction with\nreal-world equipment. However, such remote experiences can be costly or\nnon-intuitive. To address these challenges, we present TeleopLab, a mobile\ndevice teleoperation system that allows students to control a robotic arm and\noperate lab equipment. TeleopLab comprises a robotic arm, an adaptive gripper,\ncameras, lab equipment for a diverse range of applications, a user interface\naccessible through smartphones, and video call software. We conducted a user\nstudy, focusing on task performance, students' perspectives toward the system,\nusability, and workload assessment. Our results demonstrate a 46.1% reduction\nin task completion time as users gained familiarity with the system.\nQuantitative feedback highlighted improvements in students' perspectives after\nusing the system, while NASA TLX and SUS assessments indicated a manageable\nworkload of 38.2 and a positive usability of 73.8. TeleopLab successfully\nbridges the gap between physical labs and remote education, offering a scalable\nand effective platform for remote STEM learning.", "AI": {"tldr": "TeleopLab是一个基于移动设备的远程操控系统，允许学生远程控制机械臂和实验设备进行实践学习，显著缩短了任务完成时间，并获得了积极的用户反馈和良好的可用性。", "motivation": "远程教育中涉及真实设备的实践学习通常成本高昂或不够直观，阻碍了学生进行动手操作。", "method": "本文提出了TeleopLab系统，包含机械臂、自适应夹具、摄像头、多样化实验设备、智能手机用户界面和视频通话软件。通过用户研究，评估了任务表现、学生对系统的看法、可用性以及工作负荷。", "result": "用户熟悉系统后，任务完成时间减少了46.1%。学生在使用系统后持积极态度。NASA TLX工作负荷评估为38.2（可管理），SUS可用性评估为73.8（积极）。", "conclusion": "TeleopLab成功弥合了物理实验室与远程教育之间的鸿沟，为远程STEM学习提供了一个可扩展且有效的平台。"}}
{"id": "2509.05342", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05342", "abs": "https://arxiv.org/abs/2509.05342", "authors": ["Gaspard Beaudouin", "Minghan Li", "Jaeyeon Kim", "Sunghoon Yoon", "Mengyu Wang"], "title": "Delta Velocity Rectified Flow for Text-to-Image Editing", "comment": null, "summary": "We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,\npath-aware editing framework within rectified flow models for text-to-image\nediting. DVRF is a distillation-based method that explicitly models the\ndiscrepancy between the source and target velocity fields in order to mitigate\nover-smoothing artifacts rampant in prior distillation sampling approaches. We\nfurther introduce a time-dependent shift term to push noisy latents closer to\nthe target trajectory, enhancing the alignment with the target distribution. We\ntheoretically demonstrate that when this shift is disabled, DVRF reduces to\nDelta Denoising Score, thereby bridging score-based diffusion optimization and\nvelocity-based rectified-flow optimization. Moreover, when the shift term\nfollows a linear schedule under rectified-flow dynamics, DVRF generalizes the\nInversion-free method FlowEdit and provides a principled theoretical\ninterpretation for it. Experimental results indicate that DVRF achieves\nsuperior editing quality, fidelity, and controllability while requiring no\narchitectural modifications, making it efficient and broadly applicable to\ntext-to-image editing tasks. Code is available at\nhttps://github.com/gaspardbd/DeltaVelocityRectifiedFlow.", "AI": {"tldr": "DVRF是一种新型的、无需反演的、路径感知的整流流模型文本到图像编辑框架，通过显式建模源和目标速度场差异并引入时间依赖的位移项，有效缓解了过平滑问题，提升了编辑质量。", "motivation": "先前的蒸馏采样方法在文本到图像编辑中普遍存在过平滑伪影，导致编辑质量下降。研究旨在开发一种能够缓解这一问题并提升编辑质量和可控性的方法。", "method": "DVRF是一种基于蒸馏的方法，它明确地建模了源速度场和目标速度场之间的差异，以减轻过平滑伪影。此外，它引入了一个时间依赖的位移项，将噪声潜在变量推向目标轨迹，增强与目标分布的对齐。理论上，当位移项禁用时，DVRF退化为Delta Denoising Score；当位移项遵循整流流动力学下的线性调度时，DVRF推广了无需反演的方法FlowEdit。", "result": "实验结果表明，DVRF在编辑质量、保真度和可控性方面表现优越，且无需修改现有架构，使其高效并广泛适用于文本到图像编辑任务。", "conclusion": "DVRF提供了一个理论基础扎实且实验效果出色的文本到图像编辑框架，通过解决现有方法的过平滑问题，显著提升了图像编辑的质量、保真度和可控性，具有高效和广泛适用性。"}}
{"id": "2509.05685", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05685", "abs": "https://arxiv.org/abs/2509.05685", "authors": ["Jian Yang", "Jiahui Wu", "Li Fang", "Hongchao Fan", "Bianying Zhang", "Huijie Zhao", "Guangyi Yang", "Rui Xin", "Xiong You"], "title": "MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions", "comment": null, "summary": "Transforming road network data into vector representations using deep\nlearning has proven effective for road network analysis. However, urban road\nnetworks' heterogeneous and hierarchical nature poses challenges for accurate\nrepresentation learning. Graph neural networks, which aggregate features from\nneighboring nodes, often struggle due to their homogeneity assumption and focus\non a single structural scale. To address these issues, this paper presents\nMSRFormer, a novel road network representation learning framework that\nintegrates multi-scale spatial interactions by addressing their flow\nheterogeneity and long-distance dependencies. It uses spatial flow convolution\nto extract small-scale features from large trajectory datasets, and identifies\nscale-dependent spatial interaction regions to capture the spatial structure of\nroad networks and flow heterogeneity. By employing a graph transformer,\nMSRFormer effectively captures complex spatial dependencies across multiple\nscales. The spatial interaction features are fused using residual connections,\nwhich are fed to a contrastive learning algorithm to derive the final road\nnetwork representation. Validation on two real-world datasets demonstrates that\nMSRFormer outperforms baseline methods in two road network analysis tasks. The\nperformance gains of MSRFormer suggest the traffic-related task benefits more\nfrom incorporating trajectory data, also resulting in greater improvements in\ncomplex road network structures with up to 16% improvements compared to the\nmost competitive baseline method. This research provides a practical framework\nfor developing task-agnostic road network representation models and highlights\ndistinct association patterns of the interplay between scale effects and flow\nheterogeneity of spatial interactions.", "AI": {"tldr": "MSRFormer是一种新颖的道路网络表示学习框架，通过整合多尺度空间交互作用、解决流异质性和长距离依赖性问题，有效捕捉城市道路网络的复杂特征。", "motivation": "现有的深度学习方法在处理城市道路网络的异质性和层级性时面临挑战，图神经网络因其同质性假设和对单一结构尺度的关注而表现不佳。", "method": "MSRFormer框架：1. 使用空间流卷积从大规模轨迹数据中提取小尺度特征。2. 识别尺度依赖的空间交互区域以捕捉道路网络空间结构和流异质性。3. 采用图Transformer有效捕获多尺度复杂空间依赖。4. 通过残差连接融合空间交互特征，并输入对比学习算法以获得最终表示。", "result": "MSRFormer在两个真实世界数据集上的两个道路网络分析任务中优于基线方法。它在交通相关任务中受益于轨迹数据，并在复杂道路网络结构中表现出显著改进，与最具竞争力的基线方法相比，性能提升高达16%。", "conclusion": "这项研究提供了一个实用的框架，用于开发与任务无关的道路网络表示模型，并揭示了空间交互中尺度效应和流异质性之间相互作用的独特关联模式。"}}
{"id": "2508.11849", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.IV", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.11849", "abs": "https://arxiv.org/abs/2508.11849", "authors": ["Yinuo Wang", "Gavin Tao"], "title": "LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba", "comment": "13 pages", "summary": "We introduce LocoMamba, a vision-driven cross-modal DRL framework built on\nselective state-space models, specifically leveraging Mamba, that achieves\nnear-linear-time sequence modeling, effectively captures long-range\ndependencies, and enables efficient training with longer sequences. First, we\nembed proprioceptive states with a multilayer perceptron and patchify depth\nimages with a lightweight convolutional neural network, producing compact\ntokens that improve state representation. Second, stacked Mamba layers fuse\nthese tokens via near-linear-time selective scanning, reducing latency and\nmemory footprint, remaining robust to token length and image resolution, and\nproviding an inductive bias that mitigates overfitting. Third, we train the\npolicy end-to-end with Proximal Policy Optimization under terrain and\nappearance randomization and an obstacle-density curriculum, using a compact\nstate-centric reward that balances progress, smoothness, and safety. We\nevaluate our method in challenging simulated environments with static and\nmoving obstacles as well as uneven terrain. Compared with state-of-the-art\nbaselines, our method achieves higher returns and success rates with fewer\ncollisions, exhibits stronger generalization to unseen terrains and obstacle\ndensities, and improves training efficiency by converging in fewer updates\nunder the same compute budget.", "AI": {"tldr": "LocoMamba是一个基于选择性状态空间模型（Mamba）的视觉驱动跨模态深度强化学习框架，它实现了近线性时间的序列建模，有效捕捉长程依赖，并提高了训练效率，在挑战性模拟环境中表现出更高的回报、成功率和泛化能力。", "motivation": "现有的深度强化学习方法在处理长序列、捕捉长程依赖、降低延迟和内存占用以及提高训练效率方面面临挑战，尤其是在复杂的视觉驱动运动控制任务中。", "method": "该方法首先使用多层感知器嵌入本体感受状态，并使用轻量级卷积神经网络对深度图像进行分块，生成紧凑的token。其次，堆叠的Mamba层通过近线性时间的选择性扫描融合这些token，从而减少延迟和内存占用，同时对token长度和图像分辨率保持鲁棒性。最后，通过近端策略优化（PPO）进行端到端训练，结合地形和外观随机化、障碍密度课程学习，并使用平衡进展、平滑性和安全性的紧凑状态中心奖励。", "result": "LocoMamba在具有静态和移动障碍物以及不平坦地形的挑战性模拟环境中进行了评估。与现有最先进的基线相比，该方法实现了更高的回报和成功率，碰撞次数更少，对未见地形和障碍物密度表现出更强的泛化能力，并在相同计算预算下通过更少的更新实现收敛，从而提高了训练效率。", "conclusion": "LocoMamba通过引入Mamba作为其核心序列建模组件，成功地解决了视觉驱动运动控制中的关键挑战，实现了卓越的性能、泛化能力和训练效率，为机器人运动控制提供了一个强大的深度强化学习框架。"}}
{"id": "2509.05635", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05635", "abs": "https://arxiv.org/abs/2509.05635", "authors": ["Liang Zhang", "Yuan Li", "Shijie Zhang", "Zheng Zhang", "Xitong Li"], "title": "Few-Shot Query Intent Detection via Relation-Aware Prompt Learning", "comment": null, "summary": "Intent detection is a crucial component of modern conversational systems,\nsince accurately identifying user intent at the beginning of a conversation is\nessential for generating effective responses. Recent efforts have focused on\nstudying this problem under a challenging few-shot scenario. These approaches\nprimarily leverage large-scale unlabeled dialogue text corpora to pretrain\nlanguage models through various pretext tasks, followed by fine-tuning for\nintent detection with very limited annotations. Despite the improvements\nachieved, existing methods have predominantly focused on textual data,\nneglecting to effectively capture the crucial structural information inherent\nin conversational systems, such as the query-query relation and query-answer\nrelation. To address this gap, we propose SAID, a novel framework that\nintegrates both textual and relational structure information in a unified\nmanner for model pretraining for the first time. Building on this framework, we\nfurther propose a novel mechanism, the query-adaptive attention network\n(QueryAdapt), which operates at the relation token level by generating\nintent-specific relation tokens from well-learned query-query and query-answer\nrelations explicitly, enabling more fine-grained knowledge transfer. Extensive\nexperimental results on two real-world datasets demonstrate that SAID\nsignificantly outperforms state-of-the-art methods.", "AI": {"tldr": "该研究提出SAID框架，首次将文本和关系结构信息（如查询-查询、查询-回答关系）统一整合用于预训练，并引入QueryAdapt机制，以解决少样本意图检测中现有方法忽略结构信息的问题，并显著优于现有SOTA方法。", "motivation": "意图检测是对话系统的关键组成部分，尤其在挑战性的少样本场景下。现有方法主要关注文本数据，通过预训练语言模型并进行微调，但忽略了对话系统中固有的关键结构信息（如查询-查询关系和查询-回答关系），导致知识迁移不够细致。", "method": "本文提出了SAID（结构化意图检测）框架，首次以统一的方式整合文本和关系结构信息进行模型预训练。在此基础上，进一步提出了QueryAdapt（查询自适应注意力网络）机制，通过从已学习的查询-查询和查询-回答关系中显式生成意图特定的关系标记，在关系标记级别进行操作，实现更细粒度的知识迁移。", "result": "在两个真实世界数据集上的大量实验结果表明，SAID显著优于最先进的方法。", "conclusion": "SAID框架通过有效整合文本和关系结构信息，并结合QueryAdapt机制，成功解决了少样本意图检测中结构信息利用不足的问题，取得了优异的性能，为未来的对话系统发展提供了新的方向。"}}
{"id": "2509.06541", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06541", "abs": "https://arxiv.org/abs/2509.06541", "authors": ["Nico Krull", "Lukas Schulthess", "Michele Magno", "Luca Benini", "Christoph Leitner"], "title": "Wireless Low-Latency Synchronization for Body-Worn Multi-Node Systems in Sports", "comment": null, "summary": "Biomechanical data acquisition in sports demands sub-millisecond\nsynchronization across distributed body-worn sensor nodes. This study evaluates\nand characterizes the Enhanced ShockBurst (ESB) protocol from Nordic\nSemiconductor under controlled laboratory conditions for wireless, low-latency\ncommand broadcasting, enabling fast event updates in multi-node systems.\nThrough systematic profiling of protocol parameters, including\ncyclic-redundancy-check modes, bitrate, transmission modes, and payload\nhandling, we achieve a mean Device-to-Device (D2D) latency of 504.99 +- 96.89\nus and a network-to-network core latency of 311.78 +- 96.90 us using a one-byte\npayload with retransmission optimization. This performance significantly\noutperforms Bluetooth Low Energy (BLE), which is constrained by a 7.5 ms\nconnection interval, by providing deterministic, sub-millisecond\nsynchronization suitable for high-frequency (500 Hz to 1000 Hz) biosignals.\nThese results position ESB as a viable solution for time-critical, multi-node\nwearable systems in sports, enabling precise event alignment and reliable\nhigh-speed data fusion for advanced athlete monitoring and feedback\napplications.", "AI": {"tldr": "本研究评估了Nordic Enhanced ShockBurst (ESB) 协议在受控实验室条件下实现无线、低延迟命令广播的性能，发现其能提供亚毫秒级同步，显著优于蓝牙低功耗 (BLE)，适用于体育领域中高频生物信号的多节点可穿戴系统。", "motivation": "体育运动中的生物力学数据采集需要分布式穿戴式传感器节点之间达到亚毫秒级的同步，以确保高频生物信号（500 Hz至1000 Hz）的精确事件对齐和数据融合。", "method": "研究在受控实验室条件下，通过系统性地分析协议参数（包括循环冗余校验模式、比特率、传输模式和负载处理），对Nordic Semiconductor的Enhanced ShockBurst (ESB) 协议进行了评估和特性描述。", "result": "通过优化重传并使用一个字节的负载，实现了平均设备到设备 (D2D) 延迟为 504.99 ± 96.89 微秒，网络到网络核心延迟为 311.78 ± 96.90 微秒。这一性能显著优于受限于 7.5 毫秒连接间隔的蓝牙低功耗 (BLE)。", "conclusion": "ESB 协议被证明是体育领域中时间敏感、多节点可穿戴系统的可行解决方案，能够实现精确的事件对齐和可靠的高速数据融合，适用于高级运动员监测和反馈应用。"}}
{"id": "2509.05581", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY", "68T40", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.05581", "abs": "https://arxiv.org/abs/2509.05581", "authors": ["Arturo Flores Alvarez", "Fatemeh Zargarbashi", "Havel Liu", "Shiqi Wang", "Liam Edwards", "Jessica Anz", "Alex Xu", "Fan Shi", "Stelian Coros", "Dennis W. Hong"], "title": "Learning to Walk in Costume: Adversarial Motion Priors for Aesthetically Constrained Humanoids", "comment": "8 pages, 11 figures, accepted at IEEE-RAS International Conference on\n  Humanoid Robots (Humanoids) 2025", "summary": "We present a Reinforcement Learning (RL)-based locomotion system for Cosmo, a\ncustom-built humanoid robot designed for entertainment applications. Unlike\ntraditional humanoids, entertainment robots present unique challenges due to\naesthetic-driven design choices. Cosmo embodies these with a disproportionately\nlarge head (16% of total mass), limited sensing, and protective shells that\nconsiderably restrict movement. To address these challenges, we apply\nAdversarial Motion Priors (AMP) to enable the robot to learn natural-looking\nmovements while maintaining physical stability. We develop tailored domain\nrandomization techniques and specialized reward structures to ensure safe\nsim-to-real, protecting valuable hardware components during deployment. Our\nexperiments demonstrate that AMP generates stable standing and walking\nbehaviors despite Cosmo's extreme mass distribution and movement constraints.\nThese results establish a promising direction for robots that balance aesthetic\nappeal with functional performance, suggesting that learning-based methods can\neffectively adapt to aesthetic-driven design constraints.", "AI": {"tldr": "本文提出了一种基于强化学习（RL）的运动系统，使具有独特美学设计（如大头部、运动受限）的娱乐型人形机器人Cosmo能够实现稳定站立和行走。", "motivation": "娱乐机器人因其美学驱动的设计（如Cosmo的头部过大、传感受限、运动受限）而面临独特的运动挑战，传统方法难以应对。", "method": "研究采用了对抗性运动先验（AMP）方法，并开发了定制的域随机化技术和专门的奖励结构，以确保安全地从模拟到真实环境的部署，保护硬件组件。", "result": "实验证明，尽管Cosmo具有极端的质量分布和运动限制，AMP仍能生成稳定的站立和行走行为。", "conclusion": "研究表明，学习型方法能够有效适应美学驱动的设计约束，为平衡美学吸引力与功能性能的机器人提供了有前景的方向。"}}
{"id": "2509.05343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05343", "abs": "https://arxiv.org/abs/2509.05343", "authors": ["Zahid Ullah", "Minki Hong", "Tahir Mahmood", "Jihie Kim"], "title": "Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis", "comment": null, "summary": "Deep learning has become a powerful tool for medical image analysis; however,\nconventional Convolutional Neural Networks (CNNs) often fail to capture the\nfine-grained and complex features critical for accurate diagnosis. To address\nthis limitation, we systematically integrate attention mechanisms into five\nwidely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,\nDenseNet121, and EfficientNetB5, to enhance their ability to focus on salient\nregions and improve discriminative performance. Specifically, each baseline\nmodel is augmented with either a Squeeze and Excitation block or a hybrid\nConvolutional Block Attention Module, allowing adaptive recalibration of\nchannel and spatial feature representations. The proposed models are evaluated\non two distinct medical imaging datasets, a brain tumor MRI dataset comprising\nmultiple tumor subtypes, and a Products of Conception histopathological dataset\ncontaining four tissue categories. Experimental results demonstrate that\nattention augmented CNNs consistently outperform baseline architectures across\nall metrics. In particular, EfficientNetB5 with hybrid attention achieves the\nhighest overall performance, delivering substantial gains on both datasets.\nBeyond improved classification accuracy, attention mechanisms enhance feature\nlocalization, leading to better generalization across heterogeneous imaging\nmodalities. This work contributes a systematic comparative framework for\nembedding attention modules in diverse CNN architectures and rigorously\nassesses their impact across multiple medical imaging tasks. The findings\nprovide practical insights for the development of robust, interpretable, and\nclinically applicable deep learning based decision support systems.", "AI": {"tldr": "本研究系统地将注意力机制集成到多种卷积神经网络（CNN）架构中，以提升医学图像分析的性能，特别是在细粒度特征捕获和特征定位方面。", "motivation": "传统的卷积神经网络（CNN）在医学图像分析中难以捕捉关键的细粒度复杂特征，这限制了其在精确诊断中的应用。", "method": "研究人员将Squeeze and Excitation (SE)模块或混合卷积块注意力模块（CBAM）集成到五种广泛使用的CNN架构中（VGG16, ResNet18, InceptionV3, DenseNet121, EfficientNetB5）。模型在两个不同的医学图像数据集上进行评估：一个脑肿瘤MRI数据集和一个妊娠产物组织病理学数据集。", "result": "实验结果表明，增强了注意力机制的CNN模型在所有评估指标上均持续优于基线架构。特别是，结合了混合注意力机制的EfficientNetB5在两个数据集上均取得了最高的整体性能，显著提升了分类准确性。此外，注意力机制还增强了特征定位能力，从而提高了模型在异构成像模态上的泛化能力。", "conclusion": "本工作提供了一个系统性的框架，用于在不同的CNN架构中嵌入注意力模块，并严格评估了它们在多种医学成像任务中的影响。研究结果为开发鲁棒、可解释且临床适用的深度学习辅助决策系统提供了实用的见解。"}}
{"id": "2509.05714", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05714", "abs": "https://arxiv.org/abs/2509.05714", "authors": ["Zhaoyu Fan", "Kaihang Pan", "Mingze Zhou", "Bosheng Qin", "Juncheng Li", "Shengyu Zhang", "Wenqiao Zhang", "Siliang Tang", "Fei Wu", "Yueting Zhuang"], "title": "Towards Meta-Cognitive Knowledge Editing for Multimodal LLMs", "comment": "15 pages, 6 figures", "summary": "Knowledge editing enables multimodal large language models (MLLMs) to\nefficiently update outdated or incorrect information. However, existing\nbenchmarks primarily emphasize cognitive-level modifications while lacking a\nfocus on deeper meta-cognitive processes. To bridge this gap, we introduce\nCogEdit, a novel benchmark designed to evaluate MLLMs' meta-cognitive knowledge\nediting abilities across three levels: (1) Counterfactual-Driven Editing,\nassessing self-awareness of knowledge correctness changes; (2) Boundary\nConstraint Editing, ensuring appropriate generalization without unintended\ninterference; and (3) Noise-Robust Editing, promoting reflective evaluation of\nuncertain information. To advance meta-cognitive editing, we propose MIND\n(Meta-cognitive INtegrated Dynamic Knowledge Editing), a framework that\nconstructs a meta-knowledge memory for self-awareness, employs game-theoretic\ninteractions to monitor knowledge activation, and incorporates label refinement\nfor noise-robust updates. Extensive experiments show that MIND significantly\noutperforms existing cognitive editing approaches, achieving strong performance\non both traditional and meta-cognitive knowledge editing benchmarks.", "AI": {"tldr": "本文引入了CogEdit基准，用于评估多模态大型语言模型（MLLMs）的元认知知识编辑能力，并提出了MIND框架，通过构建元知识记忆、博弈论交互和标签细化来实现更鲁棒的元认知编辑。", "motivation": "现有知识编辑基准主要关注认知层面的修改，缺乏对更深层次元认知过程的关注，无法有效评估MLLMs在知识正确性、泛化能力和不确定信息处理方面的自我意识和反思能力。", "method": "本文提出了CogEdit基准，包含三个元认知编辑级别：反事实驱动编辑（评估知识正确性自我意识）、边界约束编辑（确保适当泛化无干扰）和噪声鲁棒编辑（促进不确定信息反思评估）。同时，提出了MIND（Meta-cognitive INtegrated Dynamic Knowledge Editing）框架，通过构建元知识记忆实现自我意识，利用博弈论交互监控知识激活，并结合标签细化实现噪声鲁棒更新。", "result": "MIND框架在传统和元认知知识编辑基准上均显著优于现有认知编辑方法，表现出强大的性能。", "conclusion": "本文通过引入CogEdit基准和MIND框架，有效弥补了MLLMs元认知知识编辑评估的空白，并为提升MLLMs的元认知编辑能力提供了新的途径和方法。"}}
{"id": "2509.05887", "categories": ["cs.CV", "cs.LG", "eess.IV", "68T07, 86A32", "I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2509.05887", "abs": "https://arxiv.org/abs/2509.05887", "authors": ["Caleb Gates", "Patrick Moorhead", "Jayden Ferguson", "Omar Darwish", "Conner Stallman", "Pablo Rivas", "Paapa Quansah"], "title": "Near Real-Time Dust Aerosol Detection with 3D Convolutional Neural Networks on MODIS Data", "comment": "29th International Conference on Image Processing, Computer Vision, &\n  Pattern Recognition (IPCV'25)", "summary": "Dust storms harm health and reduce visibility; quick detection from\nsatellites is needed. We present a near real-time system that flags dust at the\npixel level using multi-band images from NASA's Terra and Aqua (MODIS). A 3D\nconvolutional network learns patterns across all 36 bands, plus split thermal\nbands, to separate dust from clouds and surface features. Simple normalization\nand local filling handle missing data. An improved version raises training\nspeed by 21x and supports fast processing of full scenes. On 17 independent\nMODIS scenes, the model reaches about 0.92 accuracy with a mean squared error\nof 0.014. Maps show strong agreement in plume cores, with most misses along\nedges. These results show that joint band-and-space learning can provide timely\ndust alerts at global scale; using wider input windows or attention-based\nmodels may further sharpen edges.", "AI": {"tldr": "该研究开发了一个近实时系统，利用3D卷积网络和MODIS多光谱图像，在像素级别上快速检测沙尘暴，实现全球范围内的及时预警。", "motivation": "沙尘暴危害健康并降低能见度，因此需要从卫星快速检测沙尘暴。", "method": "该系统利用NASA Terra和Aqua卫星的MODIS多波段图像（包括所有36个波段和分裂热波段），采用3D卷积网络学习沙尘、云和地表特征之间的模式。通过简单的归一化和局部填充处理缺失数据，并开发了改进版本以提高训练速度和全场景处理效率。", "result": "在17个独立的MODIS场景上，该模型达到了约0.92的准确率和0.014的均方误差。结果显示在沙尘核心区域具有很强的一致性，主要遗漏发生在边缘区域。", "conclusion": "联合波段和空间学习的方法可以提供全球范围内的及时沙尘预警。未来的工作可以通过使用更宽的输入窗口或基于注意力（attention-based）的模型来进一步锐化边缘检测。"}}
{"id": "2509.05657", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05657", "abs": "https://arxiv.org/abs/2509.05657", "authors": ["Yuxuan Hu", "Jihao Liu", "Ke Wang", "Jinliang Zhen", "Weikang Shi", "Manyuan Zhang", "Qi Dou", "Rui Liu", "Aojun Zhou", "Hongsheng Li"], "title": "LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding", "comment": "EMNLP2025", "summary": "Recent progress in Large Language Models (LLMs) has opened new avenues for\nsolving complex optimization problems, including Neural Architecture Search\n(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt\nengineering and domain-specific tuning, limiting their practicality and\nscalability across diverse tasks. In this work, we propose LM-Searcher, a novel\nframework that leverages LLMs for cross-domain neural architecture optimization\nwithout the need for extensive domain-specific adaptation. Central to our\napproach is NCode, a universal numerical string representation for neural\narchitectures, which enables cross-domain architecture encoding and search. We\nalso reformulate the NAS problem as a ranking task, training LLMs to select\nhigh-performing architectures from candidate pools using instruction-tuning\nsamples derived from a novel pruning-based subspace sampling strategy. Our\ncurated dataset, encompassing a wide range of architecture-performance pairs,\nencourages robust and transferable learning. Comprehensive experiments\ndemonstrate that LM-Searcher achieves competitive performance in both in-domain\n(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA\nconfigurations for segmentation and generation) tasks, establishing a new\nparadigm for flexible and generalizable LLM-based architecture search. The\ndatasets and models will be released at https://github.com/Ashone3/LM-Searcher.", "AI": {"tldr": "本文提出了LM-Searcher，一个新颖的框架，利用大型语言模型（LLMs）进行跨领域神经架构优化，无需大量的领域特定适应。它通过引入NCode作为通用架构表示和将NAS问题重构为排序任务来实现。", "motivation": "现有的LLM驱动的神经架构搜索（NAS）方法过度依赖提示工程和领域特定调优，这限制了它们在不同任务中的实用性和可扩展性。", "method": "LM-Searcher框架的核心是NCode，一种通用的神经架构数值字符串表示，支持跨领域架构编码和搜索。该方法将NAS问题重新定义为排序任务，并利用通过基于剪枝的子空间采样策略获得的指令微调样本来训练LLMs，以从候选池中选择高性能架构。此外，还构建了一个包含广泛架构-性能对的精选数据集，以促进鲁棒和可迁移的学习。", "result": "综合实验表明，LM-Searcher在域内（例如，用于图像分类的CNN）和域外（例如，用于分割和生成的LoRA配置）任务中均取得了有竞争力的性能。", "conclusion": "LM-Searcher为灵活和可泛化的基于LLM的架构搜索建立了一个新范式。"}}
{"id": "2509.06588", "categories": ["eess.SY", "cs.DC", "cs.SY", "eess.SP", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.06588", "abs": "https://arxiv.org/abs/2509.06588", "authors": ["Mohammadreza Doostmohammadian", "Hamid R. Rabiee"], "title": "Distributed Automatic Generation Control subject to Ramp-Rate-Limits: Anytime Feasibility and Uniform Network-Connectivity", "comment": "Digital Signal Processing journal", "summary": "This paper considers automatic generation control over an information-sharing\nnetwork of communicating generators as a multi-agent system. The optimization\nsolution is distributed among the agents based on information consensus\nalgorithms, while addressing the generators' ramp-rate-limits (RRL). This is\ntypically ignored in the existing linear/nonlinear optimization solutions but\nthey exist in real-time power generation scenarios. Without addressing the RRL,\nthe generators cannot follow the assigned rate of generating power by the\noptimization algorithm; therefore, the existing solutions may not necessarily\nconverge to the exact optimal cost or may lose feasibility in practice. The\nproposed solution in this work addresses the ramp-rate-limit constraint along\nwith the box constraint (limits on the generated powers) and the\ncoupling-constraint (generation-demand balance) at all iteration times of the\nalgorithm. The latter is referred to as the anytime feasibility and implies\nthat at every termination point of the algorithm, the balance between the\ndemand and generated power holds. To improve the convergence rate of the\nalgorithm we further consider internal signum-based nonlinearity. We also show\nthat our solution can tolerate communication link removal. This follows from\nthe uniform-connectivity assumption on the communication network.", "AI": {"tldr": "本文提出了一种基于信息共识算法的多智能体分布式自动发电控制方案，解决了现有方法中忽略发电机爬坡速率限制（RRL）的问题，并确保了算法的随时可行性。", "motivation": "现有线性/非线性优化解决方案通常忽略了发电机在实际发电场景中存在的爬坡速率限制（RRL）。这导致发电机无法按照优化算法分配的发电速率运行，可能导致现有解决方案无法收敛到精确的最优成本或在实践中失去可行性。", "method": "该方案将自动发电控制视为一个多智能体系统，通过基于信息共识算法的分布式优化来解决问题。它在算法的所有迭代时间内同时处理了爬坡速率限制（RRL）、发电功率的盒约束（上下限）以及发电-需求平衡的耦合约束（确保随时可行性）。此外，为了提高算法的收敛速度，引入了基于符号函数（signum-based）的非线性。", "result": "该解决方案能够有效地处理发电机爬坡速率限制，确保算法在任何终止点都能保持发电与需求之间的平衡（随时可行性）。同时，它还展示了对通信链路移除的容错能力。", "conclusion": "本文提出的分布式自动发电控制方案，通过考虑发电机爬坡速率限制和确保随时可行性，克服了现有方法的局限性。结合符号函数非线性提高了收敛速度，并证明了其对通信故障的鲁棒性，使其在实际电力生成场景中更具实用性和可靠性。"}}
{"id": "2509.05599", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05599", "abs": "https://arxiv.org/abs/2509.05599", "authors": ["Kai Zhang", "Guoyang Zhao", "Jianxing Shi", "Bonan Liu", "Weiqing Qi", "Jun Ma"], "title": "MonoGlass3D: Monocular 3D Glass Detection with Plane Regression and Adaptive Feature Fusion", "comment": null, "summary": "Detecting and localizing glass in 3D environments poses significant\nchallenges for visual perception systems, as the optical properties of glass\noften hinder conventional sensors from accurately distinguishing glass\nsurfaces. The lack of real-world datasets focused on glass objects further\nimpedes progress in this field. To address this issue, we introduce a new\ndataset featuring a wide range of glass configurations with precise 3D\nannotations, collected from distinct real-world scenarios. On the basis of this\ndataset, we propose MonoGlass3D, a novel approach tailored for monocular 3D\nglass detection across diverse environments. To overcome the challenges posed\nby the ambiguous appearance and context diversity of glass, we propose an\nadaptive feature fusion module that empowers the network to effectively capture\ncontextual information in varying conditions. Additionally, to exploit the\ndistinct planar geometry of glass surfaces, we present a plane regression\npipeline, which enables seamless integration of geometric properties within our\nframework. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches in both glass segmentation and monocular glass\ndepth estimation. Our results highlight the advantages of combining geometric\nand contextual cues for transparent surface understanding.", "AI": {"tldr": "本文提出了一个包含3D玻璃标注的新数据集，并引入了MonoGlass3D，一个结合自适应特征融合和平面回归的单目3D玻璃检测方法，在玻璃分割和深度估计上超越了现有技术。", "motivation": "玻璃的光学特性使其难以被传统传感器准确识别和定位，且缺乏专注于玻璃对象的真实世界数据集，阻碍了3D玻璃检测领域的发展。", "method": "研究者首先创建了一个包含多种玻璃配置和精确3D标注的真实世界数据集。在此基础上，提出了MonoGlass3D，一个用于单目3D玻璃检测的新方法。该方法包含一个自适应特征融合模块，以有效捕获不同条件下的上下文信息；以及一个平面回归流水线，以利用玻璃表面的平面几何特性。", "result": "广泛的实验表明，该方法在玻璃分割和单目玻璃深度估计方面均优于现有最先进的方法。", "conclusion": "研究结果强调了几何和上下文线索相结合对于理解透明表面具有显著优势。"}}
{"id": "2509.05348", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05348", "abs": "https://arxiv.org/abs/2509.05348", "authors": ["Ashen Rodrigo", "Isuru Munasinghe", "Asanka Perera"], "title": "Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset", "comment": null, "summary": "Timely and accurate detection of defects and contaminants in solar panels is\ncritical for maintaining the efficiency and reliability of photovoltaic\nsystems. This study presents a comprehensive evaluation of five\nstate-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,\nEfficientDet, and Swin Transformer, for identifying physical and electrical\ndefects as well as surface contaminants such as dust, dirt, and bird droppings\non solar panels. A custom dataset, annotated in the COCO format and\nspecifically designed for solar panel defect and contamination detection, was\ndeveloped alongside a user interface to train and evaluate the models. The\nperformance of each model is assessed and compared based on mean Average\nPrecision (mAP), precision, recall, and inference speed. The results\ndemonstrate the trade-offs between detection accuracy and computational\nefficiency, highlighting the relative strengths and limitations of each model.\nThese findings provide valuable guidance for selecting appropriate detection\napproaches in practical solar panel monitoring and maintenance scenarios.\n  The dataset will be publicly available at\nhttps://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.", "AI": {"tldr": "本研究评估了YOLOv3、Faster R-CNN等五种先进目标检测模型在太阳能电池板缺陷和污染物检测方面的性能，并发布了一个定制数据集，旨在为实际应用提供模型选择指导。", "motivation": "及时准确地检测太阳能电池板中的缺陷和污染物对于维持光伏系统的效率和可靠性至关重要。", "method": "研究开发了一个COCO格式的定制太阳能电池板缺陷和污染数据集，并构建了一个用户界面用于模型训练和评估。评估了YOLOv3、Faster R-CNN、RetinaNet、EfficientDet和Swin Transformer五种目标检测模型，用于识别物理和电气缺陷以及灰尘、污垢和鸟粪等表面污染物。模型性能通过平均精度均值（mAP）、精确率、召回率和推理速度进行评估和比较。", "result": "结果展示了检测精度和计算效率之间的权衡，并突出了每种模型的相对优势和局限性。", "conclusion": "这些发现为在实际太阳能电池板监测和维护场景中选择合适的检测方法提供了有价值的指导。"}}
{"id": "2509.05757", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05757", "abs": "https://arxiv.org/abs/2509.05757", "authors": ["Sarang Patil", "Zeyong Zhang", "Yiran Huang", "Tengfei Ma", "Mengjia Xu"], "title": "Hyperbolic Large Language Models", "comment": "32 pages, 6 figures", "summary": "Large language models (LLMs) have achieved remarkable success and\ndemonstrated superior performance across various tasks, including natural\nlanguage processing (NLP), weather forecasting, biological protein folding,\ntext generation, and solving mathematical problems. However, many real-world\ndata exhibit highly non-Euclidean latent hierarchical anatomy, such as protein\nnetworks, transportation networks, financial networks, brain networks, and\nlinguistic structures or syntactic trees in natural languages. Effectively\nlearning intrinsic semantic entailment and hierarchical relationships from\nthese raw, unstructured input data using LLMs remains an underexplored area.\nDue to its effectiveness in modeling tree-like hierarchical structures,\nhyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularity\nas an expressive latent representation space for complex data modeling across\ndomains such as graphs, images, languages, and multi-modal data. Here, we\nprovide a comprehensive and contextual exposition of recent advancements in\nLLMs that leverage hyperbolic geometry as a representation space to enhance\nsemantic representation learning and multi-scale reasoning. Specifically, the\npaper presents a taxonomy of the principal techniques of Hyperbolic LLMs\n(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/log\nmaps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)\nhyperbolic state-space models. We also explore crucial potential applications\nand outline future research directions. A repository of key papers, models,\ndatasets, and code implementations is available at\nhttps://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.", "AI": {"tldr": "大型语言模型（LLMs）在处理非欧几里得层次结构数据时面临挑战。双曲几何因其在建模树状结构方面的有效性而被引入。本文综述了利用双曲几何增强语义表示学习和多尺度推理的双曲LLMs（HypLLMs）的最新进展，并进行了分类。", "motivation": "LLMs在多项任务中表现出色，但许多现实世界数据（如蛋白质网络、交通网络、语言句法树）呈现高度非欧几里得的潜在层次结构。LLMs在从这些数据中有效学习内在语义蕴含和层次关系方面仍是未充分探索的领域。双曲几何作为一种非欧几里得空间，因其擅长建模树状层次结构而备受关注。", "method": "本文提供了一个全面且有背景的阐述，重点介绍了利用双曲几何作为表示空间以增强语义表示学习和多尺度推理的LLMs的最新进展。具体来说，论文提出了双曲LLMs（HypLLMs）主要技术的分类法，分为四大类：(1) 通过exp/log映射的双曲LLMs；(2) 双曲微调模型；(3) 全双曲LLMs；以及(4) 双曲状态空间模型。此外，还探讨了潜在应用并概述了未来的研究方向。", "result": "本文成功地对利用双曲几何的LLMs进行了全面回顾和分类，将HypLLMs技术分为四种主要类型。这为理解当前双曲LLMs的设计和实现提供了一个清晰的框架，并展示了其在处理复杂、层次化数据方面增强语义表示学习和多尺度推理的潜力。", "conclusion": "双曲几何为提升LLMs处理非欧几里得层次结构数据的能力提供了一个有前景的方向，能够增强语义表示学习和多尺度推理。通过对HypLLMs技术的分类和未来方向的探讨，本文为该领域的研究和应用奠定了基础。"}}
{"id": "2509.06413", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.06413", "abs": "https://arxiv.org/abs/2509.06413", "authors": ["Yixiao Li", "Xin Li", "Chris Wei Zhou", "Shuo Xing", "Hadi Amirpour", "Xiaoshuai Hao", "Guanghui Yue", "Baoquan Zhao", "Weide Liu", "Xiaoyuan Yang", "Zhengzhong Tu", "Xinyu Li", "Chuanbiao Song", "Chenqi Zhang", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Xiaoyan Sun", "Shishun Tian", "Dongyang Yan", "Weixia Zhang", "Junlin Chen", "Wei Sun", "Zhihua Wang", "Zhuohang Shi", "Zhizun Luo", "Hang Ouyang", "Tianxin Xiao", "Fan Yang", "Zhaowang Wu", "Kaixin Deng"], "title": "VQualA 2025 Challenge on Image Super-Resolution Generated Content Quality Assessment: Methods and Results", "comment": "11 pages, 12 figures, VQualA ICCV Workshop", "summary": "This paper presents the ISRGC-Q Challenge, built upon the Image\nSuper-Resolution Generated Content Quality Assessment (ISRGen-QA) dataset, and\norganized as part of the Visual Quality Assessment (VQualA) Competition at the\nICCV 2025 Workshops. Unlike existing Super-Resolution Image Quality Assessment\n(SR-IQA) datasets, ISRGen-QA places a greater emphasis on SR images generated\nby the latest generative approaches, including Generative Adversarial Networks\n(GANs) and diffusion models. The primary goal of this challenge is to analyze\nthe unique artifacts introduced by modern super-resolution techniques and to\nevaluate their perceptual quality effectively. A total of 108 participants\nregistered for the challenge, with 4 teams submitting valid solutions and fact\nsheets for the final testing phase. These submissions demonstrated\nstate-of-the-art (SOTA) performance on the ISRGen-QA dataset. The project is\npublicly available at: https://github.com/Lighting-YXLI/ISRGen-QA.", "AI": {"tldr": "本文介绍了ISRGC-Q挑战赛，该挑战赛基于ISRGen-QA数据集，旨在评估由GAN和扩散模型等现代生成方法生成的超分辨率图像的感知质量及其独特伪影。", "motivation": "现有超分辨率图像质量评估(SR-IQA)数据集未能充分强调由最新生成方法（如GAN和扩散模型）生成的超分辨率图像。因此，需要分析这些现代技术引入的独特伪影并有效评估其感知质量。", "method": "本文组织了ISRGC-Q挑战赛，该挑战赛基于ISRGen-QA数据集。ISRGen-QA数据集特别侧重于包含由最新生成方法（包括GAN和扩散模型）生成的超分辨率图像。挑战赛邀请参与者提交解决方案，以评估这些图像的质量。", "result": "共有108名参与者注册，4支团队提交了有效解决方案。这些提交的方案在ISRGen-QA数据集上展示了最先进（SOTA）的性能。", "conclusion": "ISRGC-Q挑战赛成功地促进了对现代超分辨率图像质量评估的研究，并展示了当前最先进方法在处理由最新生成模型产生的独特伪影方面的能力。"}}
{"id": "2509.05660", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05660", "abs": "https://arxiv.org/abs/2509.05660", "authors": ["Hong Su"], "title": "Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning", "comment": null, "summary": "Large language models (LLMs) have been widely applied to assist in finding\nsolutions for diverse questions. Prior work has proposed representing a method\nas a pair of a question and its corresponding solution, enabling method reuse.\nHowever, existing approaches typically require the questions to be highly\nsimilar. In this paper, we extend the scope of method reuse to address\nquestions with low similarity or with hidden similarities that are not\nexplicitly observable. For questions that are similar in a general-specific\nsense (i.e., broader or narrower in scope), we propose to first separate the\nquestion and solution, rather than directly feeding the pair to the LLM. The\nLLM is then guided to adapt the solution to new but related questions, allowing\nit to focus on solution transfer rather than question recognition. Furthermore,\nwe extend this approach to cases where questions only share partial features or\nhidden characteristics. This enables cross-question method reuse beyond\nconventional similarity constraints. Experimental verification shows that our\nscope-extension approach increases the probability of filtering out reusable\nsolutions, thereby improving the effectiveness of cross-question method reuse.", "AI": {"tldr": "本文提出了一种扩展大型语言模型（LLMs）方法复用范围的新方法，使其能够处理相似度较低或具有隐藏相似性的问题，通过分离问题与解决方案并引导LLM进行方案迁移，提高了跨问题方法复用的有效性。", "motivation": "现有的大型语言模型方法复用通常要求问题具有高度相似性，限制了其应用范围。本研究旨在将方法复用扩展到相似度较低或具有非显式隐藏相似性的问题。", "method": "对于具有通用-特定（广义或狭义）相似性的问题，作者提出首先将问题与解决方案分离，而不是直接将两者作为配对输入LLM。然后引导LLM将现有解决方案适应于新的相关问题，使其专注于解决方案迁移而非问题识别。该方法进一步扩展到仅共享部分特征或隐藏特性的问题。", "result": "实验验证表明，所提出的范围扩展方法增加了筛选出可复用解决方案的概率，从而提高了跨问题方法复用的有效性。", "conclusion": "本研究成功地将大型语言模型的方法复用范围扩展到传统相似性约束之外，使其能够处理相似度较低或具有隐藏相似性的问题，显著提升了方法复用的能力和效率。"}}
{"id": "2509.06657", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06657", "abs": "https://arxiv.org/abs/2509.06657", "authors": ["Francesco Simone", "Marco Bortolini", "Giovanni Mazzuto", "Giulio di Gravio", "Riccardo Patriarca"], "title": "Human-Hardware-in-the-Loop simulations for systemic resilience assessment in cyber-socio-technical systems", "comment": null, "summary": "Modern industrial systems require updated approaches to safety management, as\nthe tight interplay between cyber-physical, human, and organizational factors\nhas driven their processes toward increasing complexity. In addition to dealing\nwith known risks, managing system resilience acquires great value to address\ncomplex behaviors pragmatically. This manuscript starts from the\nSystem-Theoretic Accident Model and Processes (STAMP) as a modelling initiative\nfor such complexity. The STAMP can be natively integrated with simulation-based\napproaches, which however fail to realistically represent human behaviors and\ntheir influence on the system performance. To overcome this limitation, this\npaper proposes a Human-Hardware-in-the-Loop (HHIL) modeling and simulation\nframework aimed at supporting a more realistic and comprehensive assessments of\nsystemic resilience. The approach is tested on an experimental oil and gas\nplant experiencing cyber-attacks, where two personas of operators (experts and\nnovices) work. This research provides a mean to quantitatively assess how\nvariations in operator behavior impact the overall system performance, offering\ninsights into how resilience should be understood and implemented in complex\nsocio-technical systems at large.", "AI": {"tldr": "本文提出了一种人机硬件在环（HHIL）建模与仿真框架，用于更真实、全面地评估复杂工业系统（特别是受网络攻击的油气工厂）的系统韧性，并量化操作员行为对系统性能的影响。", "motivation": "现代工业系统日益复杂，网络物理、人与组织因素紧密交织，传统安全管理方法不足以应对。现有基于仿真的方法难以真实反映人类行为及其对系统性能的影响，因此需要新的方法来务实地解决复杂行为和系统韧性管理问题。", "method": "该研究以系统理论事故模型与过程（STAMP）为基础，提出了一种人机硬件在环（HHIL）建模与仿真框架。此框架旨在克服传统仿真在代表人行为方面的局限性，从而支持对系统韧性进行更真实和全面的评估。该方法在一个遭受网络攻击的实验性油气工厂中进行了测试，涉及专家和新手两种操作员角色。", "result": "研究在实验性油气工厂中验证了HHIL方法，并成功量化了操作员行为（如专家和新手）变化对整体系统性能的影响。这为理解和实施复杂社会技术系统中的韧性提供了见解。", "conclusion": "该研究提供了一种量化评估操作员行为如何影响系统性能的手段，并为在复杂的社会技术系统中理解和实施韧性提供了深入见解。HHIL框架能够更真实、全面地评估系统韧性，尤其是在考虑人类因素时。"}}
{"id": "2509.05672", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05672", "abs": "https://arxiv.org/abs/2509.05672", "authors": ["Juho Kalliokoski", "Evan G. Center", "Steven M. LaValle", "Timo Ojala", "Basak Sakcak"], "title": "Sharing but Not Caring: Similar Outcomes for Shared Control and Switching Control in Telepresence-Robot Navigation", "comment": "Immersive telepresence, shared control", "summary": "Telepresence robots enable users to interact with remote environments, but\nefficient and intuitive navigation remains a challenge. In this work, we\ndeveloped and evaluated a shared control method, in which the robot navigates\nautonomously while allowing users to affect the path generation to better suit\ntheir needs. We compared this with control switching, where users toggle\nbetween direct and automated control. We hypothesized that shared control would\nmaintain efficiency comparable to control switching while potentially reducing\nuser workload. The results of two consecutive user studies (each with final\nsample of n=20) showed that shared control does not degrade navigation\nefficiency, but did not show a significant reduction in task load compared to\ncontrol switching. Further research is needed to explore the underlying factors\nthat influence user preference and performance in these control systems.", "AI": {"tldr": "本研究开发并评估了一种用于远程呈现机器人的共享控制方法，结果显示其在导航效率上与控制切换相当，但未能显著降低用户任务负荷。", "motivation": "远程呈现机器人在与远程环境交互时，高效且直观的导航仍然是一个挑战。", "method": "开发并评估了一种共享控制方法，其中机器人自主导航，同时允许用户影响路径生成。该方法与用户在直接和自动化控制之间切换的控制切换方法进行了比较。通过两项连续的用户研究（每项20名参与者）进行了评估。", "result": "研究结果表明，共享控制没有降低导航效率，但与控制切换相比，用户任务负荷没有显著降低。", "conclusion": "共享控制在导航效率上与控制切换相当，但在降低用户任务负荷方面未显示出显著优势。未来需要进一步研究影响这些控制系统中用户偏好和性能的潜在因素。"}}
{"id": "2509.05352", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05352", "abs": "https://arxiv.org/abs/2509.05352", "authors": ["Cuong Manh Hoang"], "title": "Unsupervised Instance Segmentation with Superpixels", "comment": null, "summary": "Instance segmentation is essential for numerous computer vision applications,\nincluding robotics, human-computer interaction, and autonomous driving.\nCurrently, popular models bring impressive performance in instance segmentation\nby training with a large number of human annotations, which are costly to\ncollect. For this reason, we present a new framework that efficiently and\neffectively segments objects without the need for human annotations. Firstly, a\nMultiCut algorithm is applied to self-supervised features for coarse mask\nsegmentation. Then, a mask filter is employed to obtain high-quality coarse\nmasks. To train the segmentation network, we compute a novel superpixel-guided\nmask loss, comprising hard loss and soft loss, with high-quality coarse masks\nand superpixels segmented from low-level image features. Lastly, a\nself-training process with a new adaptive loss is proposed to improve the\nquality of predicted masks. We conduct experiments on public datasets in\ninstance segmentation and object detection to demonstrate the effectiveness of\nthe proposed framework. The results show that the proposed framework\noutperforms previous state-of-the-art methods.", "AI": {"tldr": "该论文提出了一种无需人工标注的实例分割新框架，通过自监督特征、多切分算法、掩码过滤、超像素引导损失和自训练过程，实现了高效且有效的对象分割。", "motivation": "实例分割在计算机视觉应用中至关重要，但当前流行模型依赖大量昂贵的人工标注数据进行训练，这促使研究者寻找无需人工标注的分割方法。", "method": "首先，将MultiCut算法应用于自监督特征以进行粗略掩码分割；接着，使用掩码过滤器获取高质量的粗略掩码；然后，利用高质量粗略掩码和低级图像特征分割的超像素，计算一种新颖的超像素引导掩码损失（包含硬损失和软损失）来训练分割网络；最后，提出一个带有新自适应损失的自训练过程来提高预测掩码的质量。", "result": "实验结果表明，所提出的框架在实例分割和目标检测的公共数据集上优于现有最先进的方法。", "conclusion": "该论文成功开发了一个无需人工标注的实例分割框架，该框架在效率和有效性方面均表现出色，并超越了以往的最先进方法。"}}
{"id": "2509.05764", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05764", "abs": "https://arxiv.org/abs/2509.05764", "authors": ["Yuwei Lou", "Hao Hu", "Shaocong Ma", "Zongfei Zhang", "Liang Wang", "Jidong Ge", "Xianping Tao"], "title": "DRF: LLM-AGENT Dynamic Reputation Filtering Framework", "comment": "This paper has been accepted by ICONIP 2025 but not published", "summary": "With the evolution of generative AI, multi - agent systems leveraging large -\nlanguage models(LLMs) have emerged as a powerful tool for complex tasks.\nHowever, these systems face challenges in quantifying agent performance and\nlack mechanisms to assess agent credibility. To address these issues, we\nintroduce DRF, a dynamic reputation filtering framework. DRF constructs an\ninteractive rating network to quantify agent performance, designs a reputation\nscoring mechanism to measure agent honesty and capability, and integrates an\nUpper Confidence Bound - based strategy to enhance agent selection efficiency.\nExperiments show that DRF significantly improves task completion quality and\ncollaboration efficiency in logical reasoning and code - generation tasks,\noffering a new approach for multi - agent systems to handle large - scale\ntasks.", "AI": {"tldr": "DRF是一个动态声誉过滤框架，旨在通过量化性能、评估可信度和优化选择来提升基于LLM的多智能体系统在复杂任务中的表现。", "motivation": "基于大型语言模型（LLMs）的多智能体系统在复杂任务中面临智能体性能量化困难和缺乏可信度评估机制的挑战。", "method": "本文提出了DRF框架，它构建了一个交互式评分网络来量化智能体性能，设计了一个声誉评分机制来衡量智能体的诚实度和能力，并集成了一个基于上置信界（UCB）的策略来提高智能体选择效率。", "result": "实验表明，DRF显著提高了逻辑推理和代码生成任务中的任务完成质量和协作效率。", "conclusion": "DRF为多智能体系统处理大规模任务提供了一种新方法，有效解决了智能体性能和可信度评估问题。"}}
{"id": "2509.06442", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.06442", "abs": "https://arxiv.org/abs/2509.06442", "authors": ["Yixiao Li", "Xiaoyuan Yang", "Guanghui Yue", "Jun Fu", "Qiuping Jiang", "Xu Jia", "Paul L. Rosin", "Hantao Liu", "Wei Zhou"], "title": "Perception-oriented Bidirectional Attention Network for Image Super-resolution Quality Assessment", "comment": "16 pages, 6 figures, IEEE Transactions on Image Processing", "summary": "Many super-resolution (SR) algorithms have been proposed to increase image\nresolution. However, full-reference (FR) image quality assessment (IQA) metrics\nfor comparing and evaluating different SR algorithms are limited. In this work,\nwe propose the Perception-oriented Bidirectional Attention Network (PBAN) for\nimage SR FR-IQA, which is composed of three modules: an image encoder module, a\nperception-oriented bidirectional attention (PBA) module, and a quality\nprediction module. First, we encode the input images for feature\nrepresentations. Inspired by the characteristics of the human visual system, we\nthen construct the perception-oriented PBA module. Specifically, different from\nexisting attention-based SR IQA methods, we conceive a Bidirectional Attention\nto bidirectionally construct visual attention to distortion, which is\nconsistent with the generation and evaluation processes of SR images. To\nfurther guide the quality assessment towards the perception of distorted\ninformation, we propose Grouped Multi-scale Deformable Convolution, enabling\nthe proposed method to adaptively perceive distortion. Moreover, we design\nSub-information Excitation Convolution to direct visual perception to both\nsub-pixel and sub-channel attention. Finally, the quality prediction module is\nexploited to integrate quality-aware features and regress quality scores.\nExtensive experiments demonstrate that our proposed PBAN outperforms\nstate-of-the-art quality assessment methods.", "AI": {"tldr": "本文提出了一种名为感知导向双向注意力网络（PBAN）的全参考图像质量评估（FR-IQA）方法，用于评估超分辨率（SR）图像，该方法通过创新的双向注意力机制和多尺度形变卷积，显著优于现有技术。", "motivation": "现有的用于比较和评估不同超分辨率（SR）算法的全参考（FR）图像质量评估（IQA）指标有限，无法有效满足需求。", "method": "本文提出了PBAN，包含三个模块：图像编码器模块、感知导向双向注意力（PBA）模块和质量预测模块。PBA模块受人类视觉系统启发，构建了双向注意力来感知失真，这与SR图像的生成和评估过程一致。为进一步引导质量评估关注失真信息，提出了分组多尺度形变卷积（Grouped Multi-scale Deformable Convolution）来自适应感知失真，并设计了子信息激发卷积（Sub-information Excitation Convolution）来引导视觉感知到子像素和子通道注意力。最后，质量预测模块整合质量感知特征并回归质量分数。", "result": "广泛的实验证明，所提出的PBAN在图像质量评估方面优于最先进的方法。", "conclusion": "PBAN是一种高性能的SR图像FR-IQA方法，通过其独特的感知导向双向注意力机制和对失真信息的精细感知能力，能够更准确地评估SR图像质量。"}}
{"id": "2509.05668", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05668", "abs": "https://arxiv.org/abs/2509.05668", "authors": ["Michael Hoffmann", "Jophin John", "Stefan Schweter", "Gokul Ramakrishnan", "Hoi-Fong Mak", "Alice Zhang", "Dmitry Gaynullin", "Nicolay J. Hammer"], "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian", "comment": "Michael Hoffmann and Jophin John contributed equally to this work", "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.", "AI": {"tldr": "Llama-GENBA-10B是一个基于Llama 3.1-8B构建的三语（英语、德语、巴伐利亚语）基础模型，旨在解决大型语言模型中以英语为中心的问题，并推广低资源语言巴伐利亚语。该模型通过平衡的预训练和创新的评估方法，在巴伐利亚语上取得了领先性能，并在英语和德语上表现出色。", "motivation": "当前大型语言模型存在以英语为中心的偏见，忽视了其他语言，尤其是低资源语言。研究旨在开发一个多语言模型，不仅能处理高资源语言（如英语和德语），还能有效支持低资源语言（如巴伐利亚语），并为德国NLP社区提供服务。", "method": "该研究基于Llama 3.1-8B，将其扩展到10B参数，并使用164B代币进行持续预训练（82B英语、82B德语、80M巴伐利亚语），以平衡资源并防止英语主导。方法包括：1) 克服巴伐利亚语稀缺性，策划多语言语料库；2) 为三种语言创建统一的分词器；3) 优化架构和语言比例超参数以实现跨语言迁移；4) 通过将德语基准翻译成巴伐利亚语，建立首个标准化的三语评估套件。", "result": "Llama-GENBA-10B展现出强大的跨语言性能。其微调版本在巴伐利亚语上超越了Apertus-8B-2509和gemma-2-9b，成为该语言类别中的最佳模型。同时，它在英语上优于EuroLLM，并在德语上与其表现持平。在Cerebras CS-2上进行的训练证明了高效的大规模多语言预训练，并记录了能源使用。", "conclusion": "Llama-GENBA-10B成功地解决了大型语言模型中的英语中心偏见，并有效整合了低资源语言。该模型为构建包含低资源语言的包容性基础模型提供了蓝图，并为德国NLP社区提供了强大的工具，尤其是在巴伐利亚语方面树立了新的标杆。"}}
{"id": "2509.06722", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06722", "abs": "https://arxiv.org/abs/2509.06722", "authors": ["Samuel Chamoun", "Sirin Chakraborty", "Eric Graves", "Kevin Chan", "Yin Sun"], "title": "Edge Server Monitoring for Job Assignment", "comment": "Accepted to IEEE MILCOM 2025 (Networking Protocols and Performance\n  Track), 6 pages, 2 figures", "summary": "In this paper, we study a goal-oriented communication problem for edge server\nmonitoring, where compute jobs arrive intermittently at dispatchers and must be\nimmediately assigned to distributed edge servers. Due to competing workloads\nand the dynamic nature of the edge environment, server availability fluctuates\nover time. To maintain accurate estimates of server availability states, each\ndispatcher updates its belief using two mechanisms: (i) active queries over\nshared communication channels and (ii) feedback from past job executions. We\nformulate a query scheduling problem that maximizes the job success rate under\nlimited communication resources for queries. This problem is modeled as a\nRestless Multi-Armed Bandit (RMAB) with multiple actions and addressed using a\nNet-Gain Maximization (NGM) scheduling algorithm, which selects servers to\nquery based on their expected improvement in execution performance. Simulation\nresults show that the proposed NGM Policy significantly outperforms baseline\nstrategies, achieving up to a 30% gain over the Round-Robin Policy and up to a\n107% gain over the Never-Query Policy.", "AI": {"tldr": "本文研究了边缘服务器监控中的目标导向通信问题，旨在通过将查询调度建模为RMAB并使用NGM算法，在有限通信资源下最大化作业成功率。", "motivation": "计算作业间歇性到达调度器并需立即分配给分布式边缘服务器。由于工作负载竞争和边缘环境的动态性，服务器可用性会随时间波动。调度器需要准确估计服务器可用状态，但查询通信资源有限。", "method": "调度器通过两种机制更新其对服务器可用性的信念：(i)通过共享通信通道进行主动查询；(ii)来自过去作业执行的反馈。查询调度问题被建模为具有多个动作的Restless Multi-Armed Bandit (RMAB)，并使用净收益最大化(NGM)调度算法解决，该算法根据预期执行性能改进选择要查询的服务器。", "result": "仿真结果表明，所提出的NGM策略显著优于基线策略，比轮询策略提高了30%，比从不查询策略提高了107%。", "conclusion": "NGM策略在边缘服务器监控的目标导向通信中表现出色，能在有限通信资源下显著提高作业成功率。"}}
{"id": "2509.05701", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05701", "abs": "https://arxiv.org/abs/2509.05701", "authors": ["Siyuan Wang", "Shuyi Zhang", "Zhen Tian", "Yuheng Yao", "Gongsen Wang", "Yu Zhao"], "title": "A*-PRM: A Dynamic Weight-Based Probabilistic Roadmap Algorithm", "comment": null, "summary": "Robot path planning is a fundamental challenge in enhancing the environmental\nadaptability of autonomous navigation systems. This paper presents a hybrid\npath planning algorithm, A-star PRM, which incorporates dynamic weights. By\nembedding the Manhattan distance heuristic of the A-star algorithm into the\nrandom sampling process of PRM, the algorithm achieves a balanced optimization\nof path quality and computational efficiency. The approach uses a hierarchical\nsampling strategy and a dynamic connection mechanism, greatly improving\nadaptability to complex obstacle distributions. Experiments show that under a\nbaseline configuration with one thousand sampled vertices, the path length of\nA-star PRM is 1073.23 plus or minus 14.8 meters and is 42.3 percent shorter\nthan that of PRM with p value less than 0.01. With high-density sampling using\nthree thousand vertices, the path length is reduced by 0.94 percent, 1036.61\nmeters compared with 1046.42 meters, while the increase in computational time\nis cut to about one tenth of the PRM increase, 71 percent compared with 785\npercent. These results confirm the comprehensive advantages of A-star PRM in\npath quality, stability, and computational efficiency. Compared with existing\nhybrid algorithms, the proposed method shows clear benefits, especially in\nnarrow channels and scenarios with dynamic obstacles.", "AI": {"tldr": "本文提出了一种名为A-star PRM的混合路径规划算法，通过结合A*启发式搜索和PRM随机采样，并引入动态权重、分层采样和动态连接机制，显著提升了路径质量、计算效率和对复杂环境的适应性。", "motivation": "机器人路径规划是提高自主导航系统环境适应性的基础挑战。", "method": "该研究提出了一种A-star PRM混合路径规划算法，其特点包括：1) 将A*算法的曼哈顿距离启发式嵌入到PRM的随机采样过程中；2) 使用动态权重；3) 采用分层采样策略；4) 引入动态连接机制。", "result": "实验结果显示：在1000个采样顶点下，A-star PRM的路径长度（1073.23 ± 14.8米）比PRM缩短了42.3% (p < 0.01)。在3000个采样顶点下，路径长度进一步缩短0.94%（1036.61米对比PRM的1046.42米），而计算时间增幅仅为PRM的约十分之一（71%对比785%）。该方法在狭窄通道和动态障碍物场景下，比现有混合算法表现出明显优势。", "conclusion": "A-star PRM算法在路径质量、稳定性、计算效率方面具有综合优势，特别是在复杂环境（如狭窄通道和动态障碍物场景）中表现出显著益处。"}}
{"id": "2509.05388", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05388", "abs": "https://arxiv.org/abs/2509.05388", "authors": ["Juan Olalla-Pombo", "Alberto Badías", "Miguel Ángel Sanz-Gómez", "José María Benítez", "Francisco Javier Montáns"], "title": "Augmented Structure Preserving Neural Networks for cell biomechanics", "comment": null, "summary": "Cell biomechanics involve a great number of complex phenomena that are\nfundamental to the evolution of life itself and other associated processes,\nranging from the very early stages of embryo-genesis to the maintenance of\ndamaged structures or the growth of tumors. Given the importance of such\nphenomena, increasing research has been dedicated to their understanding, but\nthe many interactions between them and their influence on the decisions of\ncells as a collective network or cluster remain unclear. We present a new\napproach that combines Structure Preserving Neural Networks, which study cell\nmovements as a purely mechanical system, with other Machine Learning tools\n(Artificial Neural Networks), which allow taking into consideration\nenvironmental factors that can be directly deduced from an experiment with\nComputer Vision techniques. This new model, tested on simulated and real cell\nmigration cases, predicts complete cell trajectories following a roll-out\npolicy with a high level of accuracy. This work also includes a mitosis event\nprediction model based on Neural Networks architectures which makes use of the\nsame observed features.", "AI": {"tldr": "本文提出了一种结合结构保持神经网络（用于机械系统）和机器学习工具（如人工神经网络，用于环境因素，通过计算机视觉获取）的新方法，以高精度预测细胞轨迹和有丝分裂事件。", "motivation": "细胞生物力学对生命进化和相关过程（如胚胎发生、损伤修复、肿瘤生长）至关重要，但其复杂的相互作用以及对细胞集体决策的影响尚不清楚，这促使研究者寻求更深入的理解。", "method": "该方法结合了结构保持神经网络（将细胞运动视为纯机械系统）和其他机器学习工具（如人工神经网络，考虑通过计算机视觉技术从实验中直接推导出的环境因素）。模型通过“滚动策略”预测完整的细胞轨迹，并利用神经网络架构基于相同观察特征预测有丝分裂事件。", "result": "新模型在模拟和真实细胞迁移案例中，能够以高精度预测完整的细胞轨迹。此外，它还包含一个基于神经网络架构的有丝分裂事件预测模型。", "conclusion": "该研究提供了一种结合机械和环境因素的创新模型，能够准确预测细胞轨迹和有丝分裂事件，为理解细胞生物力学中的复杂现象提供了新的工具和见解。"}}
{"id": "2509.05772", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05772", "abs": "https://arxiv.org/abs/2509.05772", "authors": ["Nasser Alkhulaifi", "Ismail Gokay Dogan", "Timothy R. Cargan", "Alexander L. Bowler", "Direnc Pekaslan", "Nicholas J. Watson", "Isaac Triguero"], "title": "Decision-Focused Learning Enhanced by Automated Feature Engineering for Energy Storage Optimisation", "comment": "22 pages, 10 figures, journal-based paper", "summary": "Decision-making under uncertainty in energy management is complicated by\nunknown parameters hindering optimal strategies, particularly in Battery Energy\nStorage System (BESS) operations. Predict-Then-Optimise (PTO) approaches treat\nforecasting and optimisation as separate processes, allowing prediction errors\nto cascade into suboptimal decisions as models minimise forecasting errors\nrather than optimising downstream tasks. The emerging Decision-Focused Learning\n(DFL) methods overcome this limitation by integrating prediction and\noptimisation; however, they are relatively new and have been tested primarily\non synthetic datasets or small-scale problems, with limited evidence of their\npractical viability. Real-world BESS applications present additional\nchallenges, including greater variability and data scarcity due to collection\nconstraints and operational limitations. Because of these challenges, this work\nleverages Automated Feature Engineering (AFE) to extract richer representations\nand improve the nascent approach of DFL. We propose an AFE-DFL framework\nsuitable for small datasets that forecasts electricity prices and demand while\noptimising BESS operations to minimise costs. We validate its effectiveness on\na novel real-world UK property dataset. The evaluation compares DFL methods\nagainst PTO, with and without AFE. The results show that, on average, DFL\nyields lower operating costs than PTO and adding AFE further improves the\nperformance of DFL methods by 22.9-56.5% compared to the same models without\nAFE. These findings provide empirical evidence for DFL's practical viability in\nreal-world settings, indicating that domain-specific AFE enhances DFL and\nreduces reliance on domain expertise for BESS optimisation, yielding economic\nbenefits with broader implications for energy management systems facing similar\nchallenges.", "AI": {"tldr": "本文提出了一种结合自动化特征工程（AFE）和决策导向学习（DFL）的框架，用于优化电池储能系统（BESS）在不确定性下的运行，尤其适用于小规模真实世界数据集。结果表明，该框架显著降低了运营成本，并提升了DFL方法的性能。", "motivation": "能源管理中的不确定性（特别是BESS操作）导致难以制定最优策略。传统的“预测-然后-优化”（PTO）方法因预测误差累积而导致次优决策。新兴的决策导向学习（DFL）方法虽然解决了这一问题，但主要在合成数据集或小规模问题上测试，缺乏实际可行性证据。真实世界的BESS应用面临更大的数据变异性和稀缺性挑战。", "method": "本文提出了一种AFE-DFL框架，通过自动化特征工程（AFE）提取更丰富的特征表示，以改进DFL方法，尤其适用于小数据集。该框架预测电价和需求，同时优化BESS运行以最小化成本。研究在一个新的真实世界英国物业数据集上验证了其有效性，并比较了有/无AFE的DFL方法与PTO方法的性能。", "result": "结果显示，DFL方法平均而言比PTO方法产生了更低的运营成本。此外，添加AFE使DFL方法的性能比没有AFE的同类模型提高了22.9-56.5%。", "conclusion": "这些发现为DFL在真实世界环境中的实际可行性提供了经验证据，表明领域特定的AFE能够增强DFL，减少对BESS优化领域专业知识的依赖，从而带来经济效益，并对面临类似挑战的能源管理系统具有更广泛的意义。"}}
{"id": "2509.06890", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.06890", "abs": "https://arxiv.org/abs/2509.06890", "authors": ["Minheng Chen", "Youyong Kong"], "title": "Intraoperative 2D/3D Registration via Spherical Similarity Learning and Inference-Time Differentiable Levenberg-Marquardt Optimization", "comment": "WACV 2026 Accepted", "summary": "Intraoperative 2D/3D registration aligns preoperative 3D volumes with\nreal-time 2D radiographs, enabling accurate localization of instruments and\nimplants. A recent fully differentiable similarity learning framework\napproximates geodesic distances on SE(3), expanding the capture range of\nregistration and mitigating the effects of substantial disturbances, but\nexisting Euclidean approximations distort manifold structure and slow\nconvergence. To address these limitations, we explore similarity learning in\nnon-Euclidean spherical feature spaces to better capture and fit complex\nmanifold structure. We extract feature embeddings using a CNN-Transformer\nencoder, project them into spherical space, and approximate their geodesic\ndistances with Riemannian distances in the bi-invariant SO(4) space. This\nenables a more expressive and geometrically consistent deep similarity metric,\nenhancing the ability to distinguish subtle pose differences. During inference,\nwe replace gradient descent with fully differentiable Levenberg-Marquardt\noptimization to accelerate convergence. Experiments on real and synthetic\ndatasets show superior accuracy in both patient-specific and patient-agnostic\nscenarios.", "AI": {"tldr": "本文提出一种基于非欧几里得球面特征空间和SO(4)黎曼距离的2D/3D配准方法，通过更好地捕捉流形结构，提升了配准精度和收敛速度，优于现有欧几里得近似方法。", "motivation": "现有的2D/3D配准框架中，欧几里得近似会扭曲流形结构并减慢收敛速度，限制了配准的捕获范围和对显著扰动的鲁棒性。", "method": "该方法使用CNN-Transformer编码器提取特征嵌入，将其投影到非欧几里得球面特征空间，并利用双不变SO(4)空间中的黎曼距离近似测地线距离。推断时，采用全微分的Levenberg-Marquardt优化替代梯度下降以加速收敛。", "result": "在真实和合成数据集上，无论是在患者特异性还是患者无关的场景中，该方法都显示出卓越的配准精度。", "conclusion": "通过在非欧几里得球面特征空间中学习相似性并使用SO(4)黎曼距离，该方法提供了一种更具表达性和几何一致性的深度相似性度量，增强了区分细微姿态差异的能力，并结合Levenberg-Marquardt优化加速收敛，最终实现了更优异的配准精度。"}}
{"id": "2509.05691", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05691", "abs": "https://arxiv.org/abs/2509.05691", "authors": ["Ningyuan Deng", "Hanyu Duan", "Yixuan Tang", "Yi Yang"], "title": "Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models", "comment": null, "summary": "Text embedding models are widely used in natural language processing\napplications. However, their capability is often benchmarked on tasks that do\nnot require understanding nuanced numerical information in text. As a result,\nit remains unclear whether current embedding models can precisely encode\nnumerical content, such as numbers, into embeddings. This question is critical\nbecause embedding models are increasingly applied in domains where numbers\nmatter, such as finance and healthcare. For example, Company X's market share\ngrew by 2\\% should be interpreted very differently from Company X's market\nshare grew by 20\\%, even though both indicate growth in market share. This\nstudy aims to examine whether text embedding models can capture such nuances.\nUsing synthetic data in a financial context, we evaluate 13 widely used text\nembedding models and find that they generally struggle to capture numerical\ndetails accurately. Our further analyses provide deeper insights into embedding\nnumeracy, informing future research to strengthen embedding model-based NLP\nsystems with improved capacity for handling numerical content.", "AI": {"tldr": "研究发现，当前文本嵌入模型在理解文本中的细微数值信息方面普遍存在困难，这对于金融、医疗等领域至关重要。", "motivation": "现有文本嵌入模型的评估基准通常不涉及对文本中数值信息的细致理解，导致其在处理数字敏感领域（如金融和医疗）时，能否精确编码数值内容尚不明确。例如，“增长2%”与“增长20%”应有显著区别，但模型可能无法捕捉这种细微差异。", "method": "本研究使用金融背景下的合成数据，评估了13种广泛使用的文本嵌入模型，以检验它们捕捉数值细微差别的能力。", "result": "研究发现，这些文本嵌入模型普遍难以准确捕捉数值细节。", "conclusion": "本研究深入揭示了嵌入模型的数值理解能力，为未来研究提供了指导，以增强基于嵌入模型的自然语言处理系统处理数值内容的能力。"}}
{"id": "2509.06758", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06758", "abs": "https://arxiv.org/abs/2509.06758", "authors": ["Hossein Rastgoftar"], "title": "Steering Opinion through Dynamic Stackelberg Optimization", "comment": null, "summary": "This paper employs the Friedkin-Johnsen (FJ) model to describe the dynamics\nof opinion evolution within a social network. Under the FJ framework, the\nsociety is divided into two subgroups that include stubborn agents and regular\nagents. The opinions of stubborn agents are not influenced by regular agents,\nwhereas the opinions of regular agents evolve based on the opinions of their\nneighboring agents. By defining the origin as the desired collective opinion of\nthe society, the objective of the paper is to minimize deviations from this\ndesired opinion. To achieve this, a Stackelberg game is established between the\nstubborn and regular subgroups, where the opinion adjustments of the stubborn\nagents and the openness variables of regular agents serve as the decision\nvariables. The proposed solution approach integrates quadratic programming and\ndynamic programming to optimize these decision variables at each discrete time\nstep using forward and backward propagation.", "AI": {"tldr": "本文利用Friedkin-Johnsen模型研究社交网络中的意见演化，将社会分为固执和普通两类主体。通过建立Stackelberg博弈，并结合二次规划和动态规划，最小化社会意见与期望集体意见的偏差。", "motivation": "在社交网络中，旨在通过控制固执主体和普通主体的行为，最小化社会集体意见与预设期望意见之间的偏差。", "method": "采用Friedkin-Johnsen (FJ) 模型描述意见动态；将社会分为固执主体和普通主体；构建一个Stackelberg博弈，其中固执主体的意见调整和普通主体的开放性是决策变量；使用二次规划和动态规划相结合的方法，通过前向和后向传播在每个离散时间步优化这些决策变量。", "result": "提出了一种整合二次规划和动态规划的解决方案，通过优化固执主体的意见调整和普通主体的开放性变量，以最小化社会意见与期望集体意见的偏差。", "conclusion": "该研究提供了一个基于Stackelberg博弈和混合优化方法（二次规划与动态规划）的框架，用于在Friedkin-Johnsen模型下控制社交网络中的意见演化，使其趋向于一个期望的集体意见。"}}
{"id": "2509.05723", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05723", "abs": "https://arxiv.org/abs/2509.05723", "authors": ["Liansheng Wang", "Xinke Zhang", "Chenhui Li", "Dongjiao He", "Yihan Pan", "Jianjun Yi"], "title": "Super-LIO: A Robust and Efficient LiDAR-Inertial Odometry System with a Compact Mapping Strategy", "comment": "8 pages, 5 figures", "summary": "LiDAR-Inertial Odometry (LIO) is a foundational technique for autonomous\nsystems, yet its deployment on resource-constrained platforms remains\nchallenging due to computational and memory limitations. We propose Super-LIO,\na robust LIO system that demands both high performance and accuracy, ideal for\napplications such as aerial robots and mobile autonomous systems. At the core\nof Super-LIO is a compact octo-voxel-based map structure, termed OctVox, that\nlimits each voxel to eight fused subvoxels, enabling strict point density\ncontrol and incremental denoising during map updates. This design enables a\nsimple yet efficient and accurate map structure, which can be easily integrated\ninto existing LIO frameworks. Additionally, Super-LIO designs a\nheuristic-guided KNN strategy (HKNN) that accelerates the correspondence search\nby leveraging spatial locality, further reducing runtime overhead. We evaluated\nthe proposed system using four publicly available datasets and several\nself-collected datasets, totaling more than 30 sequences. Extensive testing on\nboth X86 and ARM platforms confirms that Super-LIO offers superior efficiency\nand robustness, while maintaining competitive accuracy. Super-LIO processes\neach frame approximately 73% faster than SOTA, while consuming less CPU\nresources. The system is fully open-source and plug-and-play compatible with a\nwide range of LiDAR sensors and platforms. The implementation is available at:\nhttps://github.com/Liansheng-Wang/Super-LIO.git", "AI": {"tldr": "Super-LIO是一种高效、鲁棒且高精度的激光雷达惯性里程计（LIO）系统，专为资源受限平台设计，通过紧凑的地图结构和加速的对应搜索实现性能提升。", "motivation": "LIO是自动驾驶系统的基础技术，但其在资源受限平台上的部署面临计算和内存限制的挑战。", "method": "Super-LIO的核心是紧凑的八体素（octo-voxel）地图结构（OctVox），每个体素限制为八个融合子体素，实现严格的点密度控制和增量去噪。此外，它设计了一种启发式引导的KNN策略（HKNN），利用空间局部性加速对应搜索，进一步降低运行时开销。", "result": "Super-LIO在X86和ARM平台上经过30多个序列的广泛测试，与SOTA系统相比，每帧处理速度快约73%，同时消耗更少的CPU资源，并保持具有竞争力的精度。系统表现出卓越的效率和鲁棒性。", "conclusion": "Super-LIO提供了一种高效、鲁棒且准确的LIO解决方案，非常适合空中机器人和移动自动系统等应用。它完全开源，并兼容多种激光雷达传感器和平台，易于集成和部署。"}}
{"id": "2509.05431", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05431", "abs": "https://arxiv.org/abs/2509.05431", "authors": ["GodsGift Uzor", "Tania-Amanda Nkoyo Fredrick Eneye", "Chukwuebuka Ijezue"], "title": "Advanced Brain Tumor Segmentation Using EMCAD: Efficient Multi-scale Convolutional Attention Decoding", "comment": null, "summary": "Brain tumor segmentation is a critical pre-processing step in the medical\nimage analysis pipeline that involves precise delineation of tumor regions from\nhealthy brain tissue in medical imaging data, particularly MRI scans. An\nefficient and effective decoding mechanism is crucial in brain tumor\nsegmentation especially in scenarios with limited computational resources.\nHowever these decoding mechanisms usually come with high computational costs.\nTo address this concern EMCAD a new efficient multi-scale convolutional\nattention decoder designed was utilized to optimize both performance and\ncomputational efficiency for brain tumor segmentation on the BraTs2020 dataset\nconsisting of MRI scans from 369 brain tumor patients. The preliminary result\nobtained by the model achieved a best Dice score of 0.31 and maintained a\nstable mean Dice score of 0.285 plus/minus 0.015 throughout the training\nprocess which is moderate. The initial model maintained consistent performance\nacross the validation set without showing signs of over-fitting.", "AI": {"tldr": "该研究提出了一种名为EMCAD的高效多尺度卷积注意力解码器，用于优化脑肿瘤分割的性能和计算效率，在BraTs2020数据集上取得了中等但稳定的分割结果。", "motivation": "脑肿瘤分割是医学图像分析中的关键预处理步骤，需要精确勾勒肿瘤区域。然而，现有的高效解码机制通常计算成本高昂，尤其是在计算资源有限的情况下，这促使研究人员寻找更高效的解决方案。", "method": "研究人员利用了一种名为EMCAD（高效多尺度卷积注意力解码器）的新设计，以优化脑肿瘤分割的性能和计算效率。该模型在BraTs2020数据集（包含369名脑肿瘤患者的MRI扫描数据）上进行了应用。", "result": "该模型取得了0.31的最佳Dice分数，并在整个训练过程中保持了0.285 ± 0.015的稳定平均Dice分数，表现中等。初步模型在验证集上保持了稳定性能，没有出现过拟合迹象。", "conclusion": "EMCAD解码器在脑肿瘤分割任务上实现了中等但稳定的性能，并在有限计算资源场景下展现了其在性能和效率优化方面的潜力，同时避免了过拟合。"}}
{"id": "2509.05818", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05818", "abs": "https://arxiv.org/abs/2509.05818", "authors": ["Won Seok Jang", "Hieu Tran", "Manav Mistry", "SaiKiran Gandluri", "Yifan Zhang", "Sharmin Sultana", "Sunjae Kown", "Yuan Zhang", "Zonghai Yao", "Hong Yu"], "title": "Chatbot To Help Patients Understand Their Health", "comment": "Accepted in EMNLP 2025 Findings", "summary": "Patients must possess the knowledge necessary to actively participate in\ntheir care. We present NoteAid-Chatbot, a conversational AI that promotes\npatient understanding via a novel 'learning as conversation' framework, built\non a multi-agent large language model (LLM) and reinforcement learning (RL)\nsetup without human-labeled data. NoteAid-Chatbot was built on a lightweight\nLLaMA 3.2 3B model trained in two stages: initial supervised fine-tuning on\nconversational data synthetically generated using medical conversation\nstrategies, followed by RL with rewards derived from patient understanding\nassessments in simulated hospital discharge scenarios. Our evaluation, which\nincludes comprehensive human-aligned assessments and case studies, demonstrates\nthat NoteAid-Chatbot exhibits key emergent behaviors critical for patient\neducation, such as clarity, relevance, and structured dialogue, even though it\nreceived no explicit supervision for these attributes. Our results show that\neven simple Proximal Policy Optimization (PPO)-based reward modeling can\nsuccessfully train lightweight, domain-specific chatbots to handle multi-turn\ninteractions, incorporate diverse educational strategies, and meet nuanced\ncommunication objectives. Our Turing test demonstrates that NoteAid-Chatbot\nsurpasses non-expert human. Although our current focus is on healthcare, the\nframework we present illustrates the feasibility and promise of applying\nlow-cost, PPO-based RL to realistic, open-ended conversational domains,\nbroadening the applicability of RL-based alignment methods.", "AI": {"tldr": "本文提出了NoteAid-Chatbot，一个基于多智能体LLM和强化学习的对话式AI，通过“边学边聊”框架提升患者理解。该模型使用轻量级LLaMA 3.2 3B，通过合成数据进行SFT和基于PPO的RL训练，无需人工标注数据。评估显示其在患者教育中展现出清晰度、相关性和结构化对话等关键行为，并在图灵测试中超越非专业人类。", "motivation": "患者需要掌握必要的知识才能积极参与到他们的护理中。因此，开发能够有效促进患者理解的工具至关重要。", "method": "NoteAid-Chatbot是一个基于多智能体大型语言模型（LLM）和强化学习（RL）的对话式AI。它构建在轻量级LLaMA 3.2 3B模型之上，训练分为两个阶段：首先，使用通过医疗对话策略合成生成的对话数据进行初始监督微调（SFT）；其次，通过强化学习进行训练，奖励信号来源于模拟医院出院场景中的患者理解评估，且整个过程无需人工标注数据。奖励建模采用基于PPO（近端策略优化）的方法。", "result": "NoteAid-Chatbot展现出患者教育所需的关键涌现行为，如清晰度、相关性和结构化对话，尽管这些属性并未得到明确监督。结果表明，即使是简单的基于PPO的奖励建模也能成功训练轻量级、领域特定的聊天机器人，以处理多轮交互、融入多样化的教育策略并满足细致的沟通目标。图灵测试显示NoteAid-Chatbot的表现超越了非专业人类。", "conclusion": "所提出的框架展示了将低成本、基于PPO的强化学习应用于现实、开放式对话领域的可行性和前景，拓宽了基于RL的对齐方法的适用范围，超越了当前的医疗保健领域焦点。"}}
{"id": "2509.05716", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05716", "abs": "https://arxiv.org/abs/2509.05716", "authors": ["Manoj Madushanka Perera", "Adnan Mahmood", "Kasun Eranda Wijethilake", "Fahmida Islam", "Maryam Tahermazandarani", "Quan Z. Sheng"], "title": "A Survey of the State-of-the-Art in Conversational Question Answering Systems", "comment": "42 pages, 12 figures, 4 tables", "summary": "Conversational Question Answering (ConvQA) systems have emerged as a pivotal\narea within Natural Language Processing (NLP) by driving advancements that\nenable machines to engage in dynamic and context-aware conversations. These\ncapabilities are increasingly being applied across various domains, i.e.,\ncustomer support, education, legal, and healthcare where maintaining a coherent\nand relevant conversation is essential. Building on recent advancements, this\nsurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.\nThis survey begins by examining the core components of ConvQA systems, i.e.,\nhistory selection, question understanding, and answer prediction, highlighting\ntheir interplay in ensuring coherence and relevance in multi-turn\nconversations. It further investigates the use of advanced machine learning\ntechniques, including but not limited to, reinforcement learning, contrastive\nlearning, and transfer learning to improve ConvQA accuracy and efficiency. The\npivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,\nMistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact\nthrough data scalability and architectural advancements. Additionally, this\nsurvey presents a comprehensive analysis of key ConvQA datasets and concludes\nby outlining open research directions. Overall, this work offers a\ncomprehensive overview of the ConvQA landscape and provides valuable insights\nto guide future advancements in the field.", "AI": {"tldr": "这篇综述全面分析了对话式问答（ConvQA）领域的最新进展，包括其核心组件、先进机器学习技术、大型语言模型的作用以及关键数据集，并指出了未来的研究方向。", "motivation": "对话式问答系统在自然语言处理中日益重要，并在客户支持、教育、法律和医疗等多个领域得到应用，在这些领域中保持连贯和相关的对话至关重要。", "method": "本综述通过以下方式进行：1. 检查ConvQA系统的核心组件（历史选择、问题理解、答案预测）。2. 调查强化学习、对比学习和迁移学习等先进机器学习技术。3. 探讨RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B和LLaMA 3等大型语言模型的关键作用。4. 分析关键的ConvQA数据集。", "result": "本研究提供了ConvQA领域最先进技术的全面分析，展示了大型语言模型通过数据可扩展性和架构进步带来的影响，并对关键ConvQA数据集进行了综合分析。", "conclusion": "这项工作全面概述了ConvQA领域的现状，提供了有价值的见解以指导未来的发展，并勾勒出开放的研究方向。"}}
{"id": "2509.06775", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06775", "abs": "https://arxiv.org/abs/2509.06775", "authors": ["Po-Heng Chou", "Pin-Qi Fu", "Walid Saad", "Li-Chun Wang"], "title": "Agentic DDQN-Based Scheduling for Licensed and Unlicensed Band Allocation in Sidelink Networks", "comment": "6 pages, 3 figures, accepted by 2025 IEEE Globecom Workshops", "summary": "This paper presents an agentic artificial intelligence (AI)-driven double\ndeep Q-network (DDQN) scheduling framework for licensed and unlicensed band\nallocation in New Radio (NR) sidelink (SL) networks. SL must share licensed\nspectrum with cellular communications (CC) and unlicensed bands with Wi-Fi,\nposing significant challenges for coexistence. Unlike prior rule-based or\nthreshold-based methods, the proposed agentic scheduler autonomously perceives\nqueueing dynamics, channel conditions, and coexistence states, and adapts its\npolicy to maintain quality-of-service (QoS). Simulation results show that our\nframework reduces the blocking rate by up to 87.5% compared to threshold-based\nscheduling under limited licensed bandwidth. These findings demonstrate the\npotential of Agentic AI to enable stable, QoS-aware, and adaptive scheduling\nfor future NR SL systems.", "AI": {"tldr": "本文提出了一种基于智能体AI的DDQN调度框架，用于新空口旁链路（NR SL）网络中授权和非授权频段的分配，以解决共存挑战并提升服务质量。", "motivation": "NR SL网络必须与蜂窝通信共享授权频谱，并与Wi-Fi共享非授权频段，这带来了显著的共存挑战。传统的基于规则或阈值的方法无法自主感知并适应网络动态以维持服务质量（QoS）。", "method": "本文提出了一种基于智能体AI的双深度Q网络（DDQN）调度框架。该智能调度器能够自主感知排队动态、信道条件和共存状态，并自适应地调整其策略以维持QoS。", "result": "仿真结果表明，在有限的授权带宽下，该框架与基于阈值的调度方法相比，阻塞率降低了高达87.5%。", "conclusion": "这些发现证明了智能体AI在未来NR SL系统中实现稳定、QoS感知和自适应调度的潜力。"}}
{"id": "2509.05777", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05777", "abs": "https://arxiv.org/abs/2509.05777", "authors": ["Zhihao Lin", "Zhen Tian"], "title": "Scenario-based Decision-making Using Game Theory for Interactive Autonomous Driving: A Survey", "comment": "This paper provides a comprehensive review for scenario-based\n  game-theoretic methods", "summary": "Game-based interactive driving simulations have emerged as versatile\nplatforms for advancing decision-making algorithms in road transport mobility.\nWhile these environments offer safe, scalable, and engaging settings for\ntesting driving strategies, ensuring both realism and robust performance amid\ndynamic and diverse scenarios remains a significant challenge. Recently, the\nintegration of game-based techniques with advanced learning frameworks has\nenabled the development of adaptive decision-making models that effectively\nmanage the complexities inherent in varied driving conditions. These models\noutperform traditional simulation methods, especially when addressing\nscenario-specific challenges, ranging from obstacle avoidance on highways and\nprecise maneuvering during on-ramp merging to navigation in roundabouts,\nunsignalized intersections, and even the high-speed demands of autonomous\nracing. Despite numerous innovations in game-based interactive driving, a\nsystematic review comparing these approaches across different scenarios is\nstill missing. This survey provides a comprehensive evaluation of game-based\ninteractive driving methods by summarizing recent advancements and inherent\nroadway features in each scenario. Furthermore, the reviewed algorithms are\ncritically assessed based on their adaptation of the standard game model and an\nanalysis of their specific mechanisms to understand their impact on\ndecision-making performance. Finally, the survey discusses the limitations of\ncurrent approaches and outlines promising directions for future research.", "AI": {"tldr": "这篇综述系统地评估了基于游戏的交互式驾驶模拟方法在不同场景下的决策算法，总结了最新进展，分析了算法机制，并讨论了局限性和未来方向。", "motivation": "尽管基于游戏的交互式驾驶模拟在推进道路交通决策算法方面取得了进展，但目前仍缺乏系统性的综述来比较这些方法在不同驾驶场景中的表现和适应性。", "method": "本研究通过对基于游戏的交互式驾驶方法进行全面评估，总结了每个场景（如高速避障、匝道合并、环岛、无信号交叉口、自动驾驶竞赛等）的最新进展和固有的道路特征。此外，还根据算法对标准游戏模型的适应性及其特定机制，批判性地评估了所审查的算法，以理解它们对决策性能的影响。", "result": "基于游戏的交互式驾驶模拟结合先进学习框架，能够开发出适应性决策模型，有效管理复杂多变的驾驶条件，并在特定场景挑战中（如避障、精准操纵、导航）表现优于传统模拟方法。本综述提供了一个全面的评估，并分析了算法对决策性能的影响。", "conclusion": "基于游戏的交互式驾驶模拟在提升决策算法方面具有巨大潜力，尤其是在应对复杂多样的驾驶场景时。然而，现有方法仍存在局限性，未来研究应致力于克服这些挑战，并探索新的方向以进一步提高其真实性和鲁棒性。"}}
{"id": "2509.05441", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05441", "abs": "https://arxiv.org/abs/2509.05441", "authors": ["Tejaswini Medi", "Hsien-Yi Wang", "Arianna Rampini", "Margret Keuper"], "title": "FAVAE-Effective Frequency Aware Latent Tokenizer", "comment": null, "summary": "Latent generative models have shown remarkable progress in high-fidelity\nimage synthesis, typically using a two-stage training process that involves\ncompressing images into latent embeddings via learned tokenizers in the first\nstage. The quality of generation strongly depends on how expressive and\nwell-optimized these latent embeddings are. While various methods have been\nproposed to learn effective latent representations, the reconstructed images\noften lack realism, particularly in textured regions with sharp transitions,\ndue to loss of fine details governed by high frequencies. We conduct a detailed\nfrequency decomposition of existing state-of-the-art (SOTA) latent tokenizers\nand show that conventional objectives inherently prioritize low-frequency\nreconstruction, often at the expense of high-frequency fidelity. Our analysis\nreveals these latent tokenizers exhibit a bias toward low-frequency\ninformation, when jointly optimized, leading to over-smoothed outputs and\nvisual artifacts that diminish perceptual quality. To address this, we propose\na wavelet-based, frequency-aware variational autoencoder (FA-VAE) framework\nthat explicitly decouples the optimization of low- and high-frequency\ncomponents. This decoupling enables improved reconstruction of fine textures\nwhile preserving global structure. Our approach bridges the fidelity gap in\ncurrent latent tokenizers and emphasizes the importance of frequency-aware\noptimization for realistic image representation, with broader implications for\napplications in content creation, neural rendering, and medical imaging.", "AI": {"tldr": "本文提出了一种基于小波的频率感知变分自编码器（FA-VAE），通过解耦低频和高频分量的优化，解决了现有潜在编码器在图像合成中高频细节丢失和过度平滑的问题。", "motivation": "尽管潜在生成模型在图像合成方面取得了显著进展，但重建图像常缺乏真实感，尤其是在纹理区域，因高频细节丢失导致。研究发现，现有最先进的潜在编码器在优化时偏向低频重建，牺牲了高频保真度，导致输出过度平滑和视觉伪影。", "method": "首先，对现有最先进的潜在编码器进行了详细的频率分解，揭示了其在联合优化时对低频信息的固有偏见。然后，提出了一种基于小波的频率感知变分自编码器（FA-VAE）框架，该框架明确地解耦了低频和高频分量的优化。", "result": "FA-VAE方法能够改进精细纹理的重建，同时保持全局结构。它弥补了当前潜在编码器在保真度上的差距，并强调了频率感知优化对于真实图像表示的重要性。", "conclusion": "频率感知优化对于生成真实感图像至关重要，本文提出的FA-VAE为内容创作、神经渲染和医学成像等应用提供了更广阔的潜力。"}}
{"id": "2509.05933", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.05933", "abs": "https://arxiv.org/abs/2509.05933", "authors": ["Md Hasebul Hasan", "Mahir Labib Dihan", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "MapAgent: A Hierarchical Agent for Geospatial Reasoning with Dynamic Map Tool Integration", "comment": "27 Pages", "summary": "Agentic AI has significantly extended the capabilities of large language\nmodels (LLMs) by enabling complex reasoning and tool use. However, most\nexisting frameworks are tailored to domains such as mathematics, coding, or web\nautomation, and fall short on geospatial tasks that require spatial reasoning,\nmulti-hop planning, and real-time map interaction. To address these challenges,\nwe introduce MapAgent, a hierarchical multi-agent plug-and-play framework with\ncustomized toolsets and agentic scaffolds for map-integrated geospatial\nreasoning. Unlike existing flat agent-based approaches that treat tools\nuniformly-often overwhelming the LLM when handling similar but subtly different\ngeospatial APIs-MapAgent decouples planning from execution. A high-level\nplanner decomposes complex queries into subgoals, which are routed to\nspecialized modules. For tool-heavy modules-such as map-based services-we then\ndesign a dedicated map-tool agent that efficiently orchestrates related APIs\nadaptively in parallel to effectively fetch geospatial data relevant for the\nquery, while simpler modules (e.g., solution generation or answer extraction)\noperate without additional agent overhead. This hierarchical design reduces\ncognitive load, improves tool selection accuracy, and enables precise\ncoordination across similar APIs. We evaluate MapAgent on four diverse\ngeospatial benchmarks-MapEval-Textual, MapEval-API, MapEval-Visual, and\nMapQA-and demonstrate substantial gains over state-of-the-art tool-augmented\nand agentic baselines. We open-source our framwork at\nhttps://github.com/Hasebul/MapAgent.", "AI": {"tldr": "MapAgent是一个分层多智能体框架，通过解耦规划与执行、定制工具集和专用地图工具智能体，显著提升了大型语言模型在地理空间任务上的空间推理和工具使用能力。", "motivation": "现有的智能体AI框架在数学、编程等领域表现良好，但在需要空间推理、多跳规划和实时地图交互的地理空间任务中表现不足，且对类似但细微不同的地理空间API处理效率低下。", "method": "MapAgent采用分层多智能体即插即用框架，将规划与执行解耦。高层规划器将复杂查询分解为子目标并路由到专业模块。对于工具密集型模块（如基于地图的服务），设计了专用的地图工具智能体并行协调相关API。较简单的模块（如解决方案生成）则无需额外智能体开销。这种分层设计减少了认知负荷，提高了工具选择准确性。", "result": "MapAgent在MapEval-Textual、MapEval-API、MapEval-Visual和MapQA这四个多样化的地理空间基准测试中，相较于最先进的工具增强和智能体基线，展现出显著的性能提升。", "conclusion": "MapAgent通过其独特的分层多智能体设计和高效的工具编排，成功解决了大型语言模型在地理空间推理和地图交互方面的挑战，实现了卓越的性能。"}}
{"id": "2509.05719", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05719", "abs": "https://arxiv.org/abs/2509.05719", "authors": ["Donya Rooein", "Flor Miriam Plaza-del-Arco", "Debora Nozza", "Dirk Hovy"], "title": "Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models", "comment": null, "summary": "Given Farsi's speaker base of over 127 million people and the growing\navailability of digital text, including more than 1.3 million articles on\nWikipedia, it is considered a middle-resource language. However, this label\nquickly crumbles when the situation is examined more closely. We focus on three\nsubjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection)\nand find significant challenges in data availability and quality, despite the\noverall increase in data availability. We review 110 publications on subjective\ntasks in Farsi and observe a lack of publicly available datasets. Furthermore,\nexisting datasets often lack essential demographic factors, such as age and\ngender, that are crucial for accurately modeling subjectivity in language. When\nevaluating prediction models using the few available datasets, the results are\nhighly unstable across both datasets and models. Our findings indicate that the\nvolume of data is insufficient to significantly improve a language's prospects\nin NLP.", "AI": {"tldr": "尽管波斯语拥有大量数字文本，但在情感分析、情绪分析和毒性检测等主观NLP任务中，其数据资源严重不足且质量低下，导致模型表现不稳定。", "motivation": "波斯语拥有庞大的用户群和不断增长的数字文本（如维基百科文章），被认为是“中等资源语言”。然而，研究者质疑这一标签在主观NLP任务上的真实性，旨在深入探究其数据可用性和质量的实际挑战。", "method": "研究关注情感分析、情绪分析和毒性检测三项主观任务，审查了110篇关于波斯语主观任务的出版物，并使用少数可用数据集评估了预测模型。", "result": "研究发现，波斯语在主观任务上存在严重的数据可用性和质量挑战，缺乏公开可用数据集。现有数据集常缺少年龄和性别等关键人口统计学因素。此外，使用现有数据集评估预测模型时，结果在不同数据集和模型之间高度不稳定。研究指出，仅仅增加数据量不足以显著改善波斯语在NLP中的前景。", "conclusion": "波斯语的“中等资源”标签在主观NLP任务的背景下并不准确，其面临数据稀缺、质量差以及关键人口统计学信息缺失等问题。仅凭数据量的增加不足以推动该语言在NLP领域的实质性进步。"}}
{"id": "2509.06853", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.06853", "abs": "https://arxiv.org/abs/2509.06853", "authors": ["Juan D. Gil", "Ehecatl Antonio Del Rio Chanona", "José L. Guzmán", "Manuel Berenguel"], "title": "Reinforcement learning meets bioprocess control through behaviour cloning: Real-world deployment in an industrial photobioreactor", "comment": null, "summary": "The inherent complexity of living cells as production units creates major\nchallenges for maintaining stable and optimal bioprocess conditions, especially\nin open Photobioreactors (PBRs) exposed to fluctuating environments. To address\nthis, we propose a Reinforcement Learning (RL) control approach, combined with\nBehavior Cloning (BC), for pH regulation in open PBR systems. This represents,\nto the best of our knowledge, the first application of an RL-based control\nstrategy to such a nonlinear and disturbance-prone bioprocess. Our method\nbegins with an offline training stage in which the RL agent learns from\ntrajectories generated by a nominal Proportional-Integral-Derivative (PID)\ncontroller, without direct interaction with the real system. This is followed\nby a daily online fine-tuning phase, enabling adaptation to evolving process\ndynamics and stronger rejection of fast, transient disturbances. This hybrid\noffline-online strategy allows deployment of an adaptive control policy capable\nof handling the inherent nonlinearities and external perturbations in open\nPBRs. Simulation studies highlight the advantages of our method: the Integral\nof Absolute Error (IAE) was reduced by 8% compared to PID control and by 5%\nrelative to standard off-policy RL. Moreover, control effort decreased\nsubstantially-by 54% compared to PID and 7% compared to standard RL-an\nimportant factor for minimizing operational costs. Finally, an 8-day\nexperimental validation under varying environmental conditions confirmed the\nrobustness and reliability of the proposed approach. Overall, this work\ndemonstrates the potential of RL-based methods for bioprocess control and paves\nthe way for their broader application to other nonlinear, disturbance-prone\nsystems.", "AI": {"tldr": "本文提出了一种结合强化学习（RL）和行为克隆（BC）的混合离线-在线控制策略，用于开放式光生物反应器（PBR）中的pH调节，相比传统PID和标准RL方法，显著提高了控制性能并降低了控制成本。", "motivation": "活细胞作为生产单元的固有复杂性以及开放式光生物反应器（PBRs）在波动环境下维持稳定和最佳生物工艺条件（尤其是pH）的挑战。", "method": "采用强化学习（RL）控制方法，结合行为克隆（BC），进行开放式PBR系统中的pH调节。该方法包括一个离线训练阶段，RL代理从名义PID控制器生成的轨迹中学习；以及一个每日在线微调阶段，以适应不断变化的工艺动态并抑制快速瞬态扰动。这是一种混合离线-在线策略。", "result": "模拟研究显示，与PID控制相比，积分绝对误差（IAE）降低了8%，与标准离策略RL相比降低了5%。控制工作量与PID相比大幅减少了54%，与标准RL相比减少了7%。为期8天的实验验证在不同环境条件下证实了该方法的鲁棒性和可靠性。", "conclusion": "该工作展示了基于RL的方法在生物过程控制中的潜力，并为它们更广泛地应用于其他非线性、易受扰动系统铺平了道路。"}}
{"id": "2509.05923", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05923", "abs": "https://arxiv.org/abs/2509.05923", "authors": ["Shuolong Chen", "Xingxing Li", "Liu Yuan"], "title": "eKalibr-Inertial: Continuous-Time Spatiotemporal Calibration for Event-Based Visual-Inertial Systems", "comment": null, "summary": "The bioinspired event camera, distinguished by its exceptional temporal\nresolution, high dynamic range, and low power consumption, has been extensively\nstudied in recent years for motion estimation, robotic perception, and object\ndetection. In ego-motion estimation, the visual-inertial setup is commonly\nadopted due to complementary characteristics between sensors (e.g., scale\nperception and low drift). For optimal event-based visual-inertial fusion,\naccurate spatiotemporal (extrinsic and temporal) calibration is required. In\nthis work, we present eKalibr-Inertial, an accurate spatiotemporal calibrator\nfor event-based visual-inertial systems, utilizing the widely used circle grid\nboard. Building upon the grid pattern recognition and tracking methods in\neKalibr and eKalibr-Stereo, the proposed method starts with a rigorous and\nefficient initialization, where all parameters in the estimator would be\naccurately recovered. Subsequently, a continuous-time-based batch optimization\nis conducted to refine the initialized parameters toward better states. The\nresults of extensive real-world experiments show that eKalibr-Inertial can\nachieve accurate event-based visual-inertial spatiotemporal calibration. The\nimplementation of eKalibr-Inertial is open-sourced at\n(https://github.com/Unsigned-Long/eKalibr) to benefit the research community.", "AI": {"tldr": "本文提出eKalibr-Inertial，一个用于事件相机-惯性测量单元（IMU）系统的精确时空（外参和时间）校准器，利用圆形网格板实现高效初始化和连续时间批处理优化。", "motivation": "事件相机在运动估计、机器人感知和物体检测中表现出色，但为了实现精确的自我运动估计，常与IMU结合以利用其互补特性。这种视觉-惯性融合需要准确的时空校准。", "method": "该方法基于eKalibr和eKalibr-Stereo中的网格模式识别和跟踪，使用圆形网格板。它首先进行严格高效的初始化，准确恢复估计器中的所有参数。随后，通过连续时间批处理优化来精炼初始化的参数，使其达到更优状态。", "result": "广泛的真实世界实验结果表明，eKalibr-Inertial能够实现精确的基于事件的视觉-惯性时空校准。", "conclusion": "eKalibr-Inertial提供了一个准确的事件相机-IMU系统时空校准解决方案，并已开源，以造福研究社区。"}}
{"id": "2509.05446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05446", "abs": "https://arxiv.org/abs/2509.05446", "authors": ["Iftekhar Haider Chowdhury", "Zaed Ikbal Syed", "Ahmed Faizul Haque Dhrubo", "Mohammad Abdul Qayum"], "title": "Dynamic Sensitivity Filter Pruning using Multi-Agent Reinforcement Learning For DCNN's", "comment": "This paper includes figures and two tables, and our work outperforms\n  the existing research that has been published in a journal", "summary": "Deep Convolutional Neural Networks have achieved state of the art performance\nacross various computer vision tasks, however their practical deployment is\nlimited by computational and memory overhead. This paper introduces\nDifferential Sensitivity Fusion Pruning, a novel single shot filter pruning\nframework that focuses on evaluating the stability and redundancy of filter\nimportance scores across multiple criteria. Differential Sensitivity Fusion\nPruning computes a differential sensitivity score for each filter by fusing the\ndiscrepancies among gradient based sensitivity, first order Taylor expansion,\nand KL divergence of activation distributions. An exponential scaling mechanism\nis applied to emphasize filters with inconsistent importance across metrics,\nidentifying candidates that are structurally unstable or less critical to the\nmodel performance. Unlike iterative or reinforcement learning based pruning\nstrategies, Differential Sensitivity Fusion Pruning is efficient and\ndeterministic, requiring only a single forward-backward pass for scoring and\npruning. Extensive experiments across varying pruning rates between 50 to 70\npercent demonstrate that Differential Sensitivity Fusion Pruning significantly\nreduces model complexity, achieving over 80 percent Floating point Operations\nPer Seconds reduction while maintaining high accuracy. For instance, at 70\npercent pruning, our approach retains up to 98.23 percent of baseline accuracy,\nsurpassing traditional heuristics in both compression and generalization. The\nproposed method presents an effective solution for scalable and adaptive Deep\nConvolutional Neural Networks compression, paving the way for efficient\ndeployment on edge and mobile platforms.", "AI": {"tldr": "本文提出了一种名为差分敏感度融合剪枝（DSFP）的单次滤波器剪枝框架，通过融合多种重要性评估指标的差异来识别不稳定或冗余的滤波器，从而高效压缩深度卷积神经网络，同时保持高精度。", "motivation": "深度卷积神经网络在计算机视觉任务中表现出色，但其高计算和内存开销限制了实际部署，尤其是在边缘和移动平台上。", "method": "差分敏感度融合剪枝（DSFP）是一种单次滤波器剪枝框架。它通过融合基于梯度的敏感度、一阶泰勒展开和激活分布的KL散度之间的差异来计算每个滤波器的差分敏感度分数。该方法应用指数缩放机制来强调在不同指标上重要性不一致的滤波器，以识别结构不稳定或对模型性能不那么关键的候选滤波器。整个过程是确定性的，仅需一次前向-后向传播来评分和剪枝。", "result": "DSFP在50%到70%的剪枝率下，显著降低了模型复杂度，实现了超过80%的浮点运算（FLOPS）减少，同时保持了高精度。例如，在70%的剪枝率下，该方法能保留基线精度高达98.23%，在压缩和泛化方面均超越了传统启发式方法。", "conclusion": "所提出的DSFP方法为可扩展和自适应的深度卷积神经网络压缩提供了一种有效的解决方案，为在边缘和移动平台上高效部署铺平了道路。"}}
{"id": "2509.06024", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06024", "abs": "https://arxiv.org/abs/2509.06024", "authors": ["Haoyang He", "Zihua Rong", "Kun Ji", "Chenyang Li", "Qing Huang", "Chong Xia", "Lan Yang", "Honggang Zhang"], "title": "Rethinking Reasoning Quality in Large Language Models through Enhanced Chain-of-Thought via RL", "comment": null, "summary": "Reinforcement learning (RL) has recently become the dominant paradigm for\nstrengthening the reasoning abilities of large language models (LLMs). Yet the\nrule-based reward functions commonly used on mathematical or programming\nbenchmarks assess only answer format and correctness, providing no signal as to\nwhether the induced Chain-of-Thought (CoT) actually improves the answer.\nFurthermore, such task-specific training offers limited control over logical\ndepth and therefore may fail to reveal a model's genuine reasoning capacity. We\npropose Dynamic Reasoning Efficiency Reward (DRER) -- a plug-and-play RL reward\nframework that reshapes both reward and advantage signals. (i) A Reasoning\nQuality Reward assigns fine-grained credit to those reasoning chains that\ndemonstrably raise the likelihood of the correct answer, directly incentivising\nthe trajectories with beneficial CoT tokens. (ii) A Dynamic Length Advantage\ndecays the advantage of responses whose length deviates from a\nvalidation-derived threshold, stabilising training. To facilitate rigorous\nassessment, we also release Logictree, a dynamically constructed deductive\nreasoning dataset that functions both as RL training data and as a\ncomprehensive benchmark. Experiments confirm the effectiveness of DRER: our 7B\nmodel attains GPT-o3-mini level performance on Logictree with 400 trianing\nsteps, while the average confidence of CoT-augmented answers rises by 30%. The\nmodel further exhibits generalisation across diverse logical-reasoning\ndatasets, and the mathematical benchmark AIME24. These results illuminate how\nRL shapes CoT behaviour and chart a practical path toward enhancing\nformal-reasoning skills in large language models. All code and data are\navailable in repository https://github.com/Henryhe09/DRER.", "AI": {"tldr": "本文提出了一种名为动态推理效率奖励（DRER）的强化学习奖励框架，用于提升大型语言模型（LLMs）的推理能力，特别是其思维链（CoT）的质量，并引入了Logictree数据集进行评估。DRER通过细粒度奖励和动态长度优势，显著提高了模型在逻辑推理任务上的性能和泛化能力。", "motivation": "现有用于数学或编程基准的强化学习奖励函数通常只评估答案格式和正确性，未能提供关于思维链（CoT）是否真正改善答案的信号。此外，这种任务特定训练对逻辑深度控制有限，可能无法揭示模型真实的推理能力。", "method": "本文提出动态推理效率奖励（DRER）框架，该框架重塑了奖励和优势信号：(i) 推理质量奖励：为那些能显著提高正确答案可能性的推理链分配细粒度信用，直接激励有益的CoT轨迹。(ii) 动态长度优势：对长度偏离验证阈值的响应，其优势值会衰减，以稳定训练。为促进严格评估，还发布了Logictree数据集，一个动态构建的演绎推理数据集，可用作RL训练数据和综合基准。", "result": "实验证实了DRER的有效性：一个7B模型在Logictree上经过400步训练后，达到了GPT-o3-mini级别的性能，并且CoT增强答案的平均置信度提高了30%。该模型还在各种逻辑推理数据集和数学基准AIME24上表现出泛化能力。", "conclusion": "这些结果阐明了强化学习如何塑造CoT行为，并为增强大型语言模型的形式推理能力指明了一条实用路径。DRER框架通过提升思维链的质量和效率，有效提升了模型的推理能力和泛化性。"}}
{"id": "2509.05729", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05729", "abs": "https://arxiv.org/abs/2509.05729", "authors": ["Charles M. Varmantchaonala", "Niclas GÖtting", "Nils-Erik SchÜtte", "Jean Louis E. K. Fendji", "Christopher Gies"], "title": "QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing", "comment": null, "summary": "Quantum Natural Language Processing (QNLP) offers a novel approach to\nencoding and understanding the complexity of natural languages through the\npower of quantum computation. This paper presents a pretrained quantum\ncontext-sensitive embedding model, called QCSE, that captures context-sensitive\nword embeddings, leveraging the unique properties of quantum systems to learn\ncontextual relationships in languages. The model introduces quantum-native\ncontext learning, enabling the utilization of quantum computers for linguistic\ntasks. Central to the proposed approach are innovative context matrix\ncomputation methods, designed to create unique, representations of words based\non their surrounding linguistic context. Five distinct methods are proposed and\ntested for computing the context matrices, incorporating techniques such as\nexponential decay, sinusoidal modulation, phase shifts, and hash-based\ntransformations. These methods ensure that the quantum embeddings retain\ncontext sensitivity, thereby making them suitable for downstream language tasks\nwhere the expressibility and properties of quantum systems are valuable\nresources. To evaluate the effectiveness of the model and the associated\ncontext matrix methods, evaluations are conducted on both a Fulani corpus, a\nlow-resource African language, dataset of small size and an English corpus of\nslightly larger size. The results demonstrate that QCSE not only captures\ncontext sensitivity but also leverages the expressibility of quantum systems\nfor representing rich, context-aware language information. The use of Fulani\nfurther highlights the potential of QNLP to mitigate the problem of lack of\ndata for this category of languages. This work underscores the power of quantum\ncomputation in natural language processing (NLP) and opens new avenues for\napplying QNLP to real-world linguistic challenges across various tasks and\ndomains.", "AI": {"tldr": "本文提出了一个名为QCSE的预训练量子上下文敏感嵌入模型，该模型利用量子计算的特性捕捉上下文敏感的词嵌入，并引入量子原生上下文学习。模型提出了五种独特的上下文矩阵计算方法，并在富拉尼语和英语语料库上进行了评估，结果表明QCSE能有效捕捉上下文敏感性，并利用量子系统的表达能力，尤其对低资源语言有潜力。", "motivation": "量子自然语言处理（QNLP）提供了一种通过量子计算的力量编码和理解自然语言复杂性的新方法。本研究旨在利用量子系统的独特属性来学习语言中的上下文关系，并解决低资源语言（如富拉尼语）数据匮乏的问题。", "method": "本文提出了一个预训练的量子上下文敏感嵌入模型QCSE，该模型引入了量子原生上下文学习。核心方法是创新的上下文矩阵计算，旨在基于词语的周围语言上下文创建独特的表示。共提出了五种不同的上下文矩阵计算方法并进行了测试，这些方法结合了指数衰减、正弦调制、相移和基于哈希的变换等技术。模型在富拉尼语（一种低资源非洲语言）和英语语料库上进行了评估。", "result": "结果表明，QCSE模型不仅能捕捉上下文敏感性，还能利用量子系统的表达能力来表示丰富的、上下文感知的语言信息。在富拉尼语上的应用进一步突出了QNLP缓解此类语言数据匮乏问题的潜力。", "conclusion": "这项工作强调了量子计算在自然语言处理中的强大能力，并为将QNLP应用于各种任务和领域的实际语言挑战开辟了新途径。"}}
{"id": "2509.06115", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06115", "abs": "https://arxiv.org/abs/2509.06115", "authors": ["Runjiao Bao", "Lin Zhang", "Tianwei Niu", "Haoyu Yuan", "Shoukun Wang"], "title": "Hybrid A* Path Planning with Multi-Modal Motion Extension for Four-Wheel Steering Mobile Robots", "comment": null, "summary": "Four-wheel independent steering (4WIS) systems provide mobile robots with a\nrich set of motion modes, such as Ackermann steering, lateral steering, and\nparallel movement, offering superior maneuverability in constrained\nenvironments. However, existing path planning methods generally assume a single\nkinematic model and thus fail to fully exploit the multi-modal capabilities of\n4WIS platforms. To address this limitation, we propose an extended Hybrid A*\nframework that operates in a four-dimensional state space incorporating both\nspatial states and motion modes. Within this framework, we design multi-modal\nReeds-Shepp curves tailored to the distinct kinematic constraints of each\nmotion mode, develop an enhanced heuristic function that accounts for\nmode-switching costs, and introduce a terminal connection strategy with\nintelligent mode selection to ensure smooth transitions between different\nsteering patterns. The proposed planner enables seamless integration of\nmultiple motion modalities within a single path, significantly improving\nflexibility and adaptability in complex environments. Results demonstrate\nsignificantly improved planning performance for 4WIS robots in complex\nenvironments.", "AI": {"tldr": "本文提出了一种扩展的混合A*路径规划框架，用于四轮独立转向（4WIS）机器人，通过在四维状态空间中整合运动模式，并设计多模态Reeds-Shepp曲线、考虑模式切换成本的启发式函数和智能模式选择策略，以充分利用其多模态运动能力，显著提高复杂环境下的规划性能。", "motivation": "现有路径规划方法通常假设单一运动学模型，未能充分利用4WIS平台的多模态能力（如阿克曼转向、横向转向、平行移动），从而限制了机器人在受限环境中的机动性。", "method": "本文提出了一种扩展的混合A*框架，该框架在包含空间状态和运动模式的四维状态空间中运行。具体方法包括：设计针对不同运动模式运动学约束的多模态Reeds-Shepp曲线；开发考虑模式切换成本的增强启发式函数；引入具有智能模式选择的末端连接策略，以确保不同转向模式之间的平滑过渡。", "result": "所提出的规划器能够在一个路径中无缝整合多种运动模式，显著提高了机器人在复杂环境中的灵活性和适应性。结果表明，该方法显著提升了4WIS机器人在复杂环境中的规划性能。", "conclusion": "该扩展的混合A*框架通过有效利用4WIS机器人的多模态运动能力，实现了复杂环境下路径规划性能和机器人机动性的显著提升。"}}
{"id": "2509.06031", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06031", "abs": "https://arxiv.org/abs/2509.06031", "authors": ["Junhui Huang", "Yuhe Gong", "Changsheng Li", "Xingguang Duan", "Luis Figueredo"], "title": "ZLATTE: A Geometry-Aware, Learning-Free Framework for Language-Driven Trajectory Reshaping in Human-Robot Interaction", "comment": null, "summary": "We present ZLATTE, a geometry-aware, learning-free framework for\nlanguage-driven trajectory reshaping in human-robot interaction. Unlike prior\nlearning-based methods, ZLATTE leverages Vision-Language Models to register\nobjects as geometric primitives and employs a Large Language Model to translate\nnatural language instructions into explicit geometric and kinematic\nconstraints. These constraints are integrated into a potential field\noptimization to adapt initial trajectories while preserving feasibility and\nsafety. A multi-agent strategy further enhances robustness under complex or\nconflicting commands. Simulation and real-world experiments demonstrate that\nZLATTE achieves smoother, safer, and more interpretable trajectory\nmodifications compared to state-of-the-art baselines.", "AI": {"tldr": "ZLATTE是一个几何感知、免学习的框架，用于人机交互中基于语言的机器人轨迹重塑。", "motivation": "现有方法多为基于学习的，ZLATTE旨在提供一种无需学习且能处理语言指令的轨迹重塑方案，以实现更平滑、安全和可解释的轨迹修改。", "method": "ZLATTE利用视觉-语言模型将对象注册为几何基元，并使用大型语言模型将自然语言指令转化为明确的几何和运动学约束。这些约束被整合到势场优化中，以调整初始轨迹，同时保持可行性和安全性。此外，还采用多智能体策略来增强复杂或冲突指令下的鲁棒性。", "result": "仿真和真实世界实验证明，ZLATTE相较于现有最先进的基线方法，能够实现更平滑、更安全且更可解释的轨迹修改。", "conclusion": "ZLATTE提供了一种高效且优于现有方法的学习无关框架，用于语言驱动的机器人轨迹重塑，显著提升了轨迹修改的质量和可解释性。"}}
{"id": "2509.05483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05483", "abs": "https://arxiv.org/abs/2509.05483", "authors": ["Jinhao Wang", "Florian Vogl", "Pascal Schütz", "Saša Ćuković", "William R. Taylor"], "title": "Veriserum: A dual-plane fluoroscopic dataset with knee implant phantoms for deep learning in medical imaging", "comment": "This work has been accepted at MICCAI 2025", "summary": "Veriserum is an open-source dataset designed to support the training of deep\nlearning registration for dual-plane fluoroscopic analysis. It comprises\napproximately 110,000 X-ray images of 10 knee implant pair combinations (2\nfemur and 5 tibia implants) captured during 1,600 trials, incorporating poses\nassociated with daily activities such as level gait and ramp descent. Each\nimage is annotated with an automatically registered ground-truth pose, while\n200 images include manually registered poses for benchmarking.\n  Key features of Veriserum include dual-plane images and calibration tools.\nThe dataset aims to support the development of applications such as 2D/3D image\nregistration, image segmentation, X-ray distortion correction, and 3D\nreconstruction. Freely accessible, Veriserum aims to advance computer vision\nand medical imaging research by providing a reproducible benchmark for\nalgorithm development and evaluation. The Veriserum dataset used in this study\nis publicly available via\nhttps://movement.ethz.ch/data-repository/veriserum.html, with the data stored\nat ETH Z\\\"urich Research Collections: https://doi.org/10.3929/ethz-b-000701146.", "AI": {"tldr": "Veriserum是一个开源数据集，包含约11万张膝关节植入物的双平面X射线图像，用于支持深度学习配准、图像分割、X射线畸变校正和3D重建等应用，并提供自动和手动标注的姿态真值。", "motivation": "该研究旨在支持双平面荧光透视分析的深度学习配准训练，推动计算机视觉和医学图像研究，并为算法开发和评估提供可复现的基准。", "method": "Veriserum数据集包含约11万张X射线图像，涵盖10种膝关节植入物组合（2种股骨和5种胫骨植入物），在1600次试验中捕捉，包括日常活动（如平地步态和下坡）相关的姿态。每张图像都通过自动配准获得姿态真值，其中200张图像包含手动配准的姿态用于基准测试。数据集还包含双平面图像和校准工具。", "result": "创建并公开了一个名为Veriserum的开放数据集，该数据集具有大规模（约11万张图像）、多样性（10种膝关节植入物组合、日常活动姿态）、多模态（双平面X射线）和高质量标注（自动和手动配准姿态真值）的特点，并提供了校准工具。", "conclusion": "Veriserum数据集为计算机视觉和医学图像研究提供了一个宝贵的资源和可复现的基准，特别是对于2D/3D图像配准、图像分割、X射线畸变校正和3D重建等应用，将有助于推动相关算法的开发和评估。"}}
{"id": "2509.06160", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06160", "abs": "https://arxiv.org/abs/2509.06160", "authors": ["Haozhe Wang", "Haoran Que", "Qixin Xu", "Minghao Liu", "Wangchunshu Zhou", "Jiazhan Feng", "Wanjun Zhong", "Wei Ye", "Tong Yang", "Wenhao Huang", "Ge Zhang", "Fangzhen Lin"], "title": "Reverse-Engineered Reasoning for Open-Ended Generation", "comment": "Preprint", "summary": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5.", "AI": {"tldr": "本文提出了一种名为REER（逆向工程推理）的新范式，通过从已知优质解决方案逆向发现深层推理过程，解决了开放式创意生成中的深度推理难题。REER被用于构建DeepWriting-20K数据集，并在此基础上训练了DeepWriter-8B模型，其性能超越了现有开源基线，并与顶尖专有模型相媲美。", "motivation": "在开放式、创意生成任务中，深度推理的应用面临巨大挑战。现有的主流方法，如强化学习（RL）和指令蒸馏，存在明显缺陷：RL缺乏清晰的奖励信号和高质量的奖励模型，而指令蒸馏则成本高昂且受限于教师模型的能力。", "method": "本文引入了REER（逆向工程推理）范式，它根本性地改变了推理过程的构建方式。REER不通过试错或模仿“正向”构建推理过程，而是从已知优质解决方案“逆向”工作，计算性地发现可能产生这些解决方案的潜在、分步的深度推理过程。这是一种可扩展、无梯度的方​​法。利用REER，研究人员策划并开源了DeepWriting-20K，一个包含20,000条用于开放式任务的深度推理轨迹的大规模数据集。在此数据上训练了DeepWriter-8B模型。", "result": "DeepWriter-8B模型在DeepWriting-20K数据集上训练后，不仅超越了强大的开源基线模型，而且实现了与领先的专有模型（如GPT-4o和Claude 3.5）相当，甚至有时更优的性能。同时，DeepWriting-20K数据集也被策划并开源。", "conclusion": "REER范式提供了一种有效且可扩展的方法，用于在开放式创意生成领域实现深度推理。通过逆向工程发现推理轨迹，并在此基础上训练的模型，能够达到与当前最先进的专有模型竞争的水平，这为该领域带来了显著的进步。"}}
{"id": "2509.05741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05741", "abs": "https://arxiv.org/abs/2509.05741", "authors": ["Fernando Gabriela García", "Qiyang Shi", "Zilin Feng"], "title": "Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification", "comment": null, "summary": "This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a\nnovel method designed to address the pervasive issues of hallucination and the\nabsence of credible citation sources in Large Language Models (LLMs) when\ngenerating complex, fact-sensitive content. By incorporating a multi-stage\nmechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT\nempowers LLMs to critically self-examine and revise their intermediate\nreasoning steps and final answers. This process significantly enhances the\nobjective accuracy, trustworthiness, and traceability of the generated outputs,\nmaking LLMs more reliable for applications demanding high fidelity such as\nscientific research, news reporting, and legal consultation.", "AI": {"tldr": "VeriFact-CoT是一种新方法，通过事实验证-反思-引用整合机制，解决大型语言模型（LLM）在生成事实敏感内容时出现的幻觉和缺乏引用问题。", "motivation": "大型语言模型在生成复杂、事实敏感内容时，普遍存在幻觉现象和缺乏可信引用来源的问题。", "method": "VeriFact-CoT采用多阶段的“事实验证-反思-引用整合”机制，使LLM能够批判性地自我审查和修改其中间推理步骤及最终答案。", "result": "该方法显著提高了生成内容的客观准确性、可信度和可追溯性。", "conclusion": "VeriFact-CoT使LLM在需要高保真度的应用（如科学研究、新闻报道和法律咨询）中变得更加可靠。"}}
{"id": "2509.06119", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06119", "abs": "https://arxiv.org/abs/2509.06119", "authors": ["Shiqi Xu", "Lihao Zhang", "Yuyang Du", "Qun Yang", "Soung Chang Liew"], "title": "A Hybrid TDMA/CSMA Protocol for Time-Sensitive Traffic in Robot Applications", "comment": null, "summary": "Recent progress in robotics has underscored the demand for real-time control\nin applications such as manufacturing, healthcare, and autonomous systems,\nwhere the timely delivery of mission-critical commands under heterogeneous\nrobotic traffic is paramount for operational efficacy and safety. In these\nscenarios, mission-critical traffic follows a strict deadline-constrained\ncommunication pattern: commands must arrive within defined QoS deadlines,\notherwise late arrivals can degrade performance or destabilize control loops.In\nthis work, we demonstrate on a real-time SDR platform that CSMA, widely adopted\nin robotic communications,suffers severe degradation under high robot traffic\nloads, with contention-induced collisions and delays disrupting the on-time\narrival of mission-critical packets. To address this problem, we propose an\nIEEE 802.11-compatible hybrid TDMA/CSMA protocol that combines TDMA's\ndeterministic slot scheduling with CSMA's adaptability for heterogeneous robot\ntraffic.The protocol achieves collision-free, low-latency mission-critical\ncommand delivery and IEEE 802.11 compatibility through the synergistic\nintegration of sub-microsecond PTP-based slot synchronization-essential for\nestablishing precise timing for TDMA, a three-session superframe with dynamic\nTDMA allocation for structured and adaptable traffic management,and beacon-NAV\nprotection to preemptively secure these critical communication sessions from\ninterference. Emulation experiments on real-time SDR testbed and Robot\nOperating System (ROS) simulation show that the proposed protocol reduces\nmissed-deadline errors by 93% compared to the CSMA baseline. In high-speed\nrobot path-tracking ROS simulations, the protocol lowers Root Mean Square (RMS)\ntrajectory error by up to 90% compared with a CSMA baseline, all while\nmaintaining throughput for non-critical traffic within +-2%.", "AI": {"tldr": "本文提出了一种兼容IEEE 802.11的混合TDMA/CSMA协议，用于解决机器人通信中关键任务指令在CSMA高负载下严重的延迟和碰撞问题。该协议通过精确的时间同步、动态TDMA分配和信标-NAV保护，显著减少了截止期错误和轨迹误差，同时保持了非关键流量的吞吐量。", "motivation": "机器人技术（如制造、医疗、自主系统）对实时控制有高需求，要求在异构机器人流量下及时传递关键任务指令，以确保操作效率和安全。当前广泛使用的CSMA协议在高机器人流量负载下性能严重下降，导致碰撞和延迟，使得关键任务数据包无法按时到达，从而降低性能或破坏控制回路的稳定性。", "method": "研究人员提出了一种兼容IEEE 802.11的混合TDMA/CSMA协议。该协议结合了TDMA的确定性时隙调度和CSMA对异构机器人流量的适应性。其核心机制包括：亚微秒PTP（精确时间协议）的时隙同步以实现精确计时；一个具有动态TDMA分配的三会话超帧用于结构化和自适应流量管理；以及信标-NAV保护机制以预先保护关键通信会话免受干扰。该协议在实时SDR平台和Robot Operating System (ROS) 仿真中进行了演示和评估。", "result": "与CSMA基线相比，该协议将错过截止期错误减少了93%。在高速机器人路径跟踪ROS仿真中，将均方根(RMS)轨迹误差降低了高达90%。同时，非关键流量的吞吐量保持在±2%以内。协议实现了无碰撞、低延迟的关键任务指令传输。", "conclusion": "所提出的混合TDMA/CSMA协议通过结合确定性调度和适应性，显著解决了机器人关键任务通信在高流量负载下CSMA的局限性。它能大幅提高实时控制的可靠性和性能，减少关键任务指令的延迟和错误，同时保持与现有标准的兼容性和对非关键流量的效率。"}}
{"id": "2509.06048", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06048", "abs": "https://arxiv.org/abs/2509.06048", "authors": ["Yi Dong", "Yangjun Liu", "Jinjun Duan", "Yang Li", "Zhendong Dai"], "title": "Robotic Manipulation Framework Based on Semantic Keypoints for Packing Shoes of Different Sizes, Shapes, and Softness", "comment": "Yi Dong and Yangjun Liu contributed equally to the work. Accepted by\n  Robotics and Autonomous Systems.\n  https://authors.elsevier.com/c/1lgjX3HdG3supQ", "summary": "With the rapid development of the warehousing and logistics industries, the\npacking of goods has gradually attracted the attention of academia and\nindustry. The packing of footwear products is a typical representative\npaired-item packing task involving irregular shapes and deformable objects.\nAlthough studies on shoe packing have been conducted, different initial states\ndue to the irregular shapes of shoes and standard packing placement poses have\nnot been considered. This study proposes a robotic manipulation framework,\nincluding a perception module, reorientation planners, and a packing planner,\nthat can complete the packing of pairs of shoes in any initial state. First, to\nadapt to the large intraclass variations due to the state, shape, and\ndeformation of the shoe, we propose a vision module based on semantic\nkeypoints, which can also infer more information such as size, state, pose, and\nmanipulation points by combining geometric features. Subsequently, we not only\nproposed primitive-based reorientation methods for different states of a single\ndeformable shoe but also proposed a fast reorientation method for the top state\nusing box edge contact and gravity, which further improved the efficiency of\nreorientation. Finally, based on the perception module and reorientation\nmethods, we propose a task planner for shoe pair packing in any initial state\nto provide an optimal packing strategy. Real-world experiments were conducted\nto verify the robustness of the reorientation methods and the effectiveness of\nthe packing strategy for various types of shoes. In this study, we highlight\nthe potential of semantic keypoint representation methods, introduce new\nperspectives on the reorientation of 3D deformable objects and multi-object\nmanipulation, and provide a reference for paired object packing.", "AI": {"tldr": "本研究提出了一种机器人操作框架，包括感知模块、重定向规划器和打包规划器，能够处理任意初始状态下的鞋子配对打包任务，并考虑了鞋子的不规则形状和可变形性。", "motivation": "随着仓储和物流业的快速发展，物品打包受到关注。鞋子打包是一个典型的配对物品打包任务，涉及不规则形状和可变形物体。现有研究未考虑鞋子因不规则形状而产生的不同初始状态以及标准打包放置姿态。", "method": "本研究提出了一个机器人操作框架，包含感知模块、重定向规划器和打包规划器。感知模块基于语义关键点和几何特征，能推断鞋子的尺寸、状态、姿态和操作点。重定向方法包括针对单个可变形鞋子不同状态的基于原语的方法，以及利用盒子边缘接触和重力实现的顶部状态快速重定向。最后，基于感知和重定向，提出了一个任务规划器，用于在任意初始状态下鞋子配对打包，以提供最优打包策略。", "result": "通过真实世界实验，验证了重定向方法的鲁棒性以及打包策略对各种类型鞋子的有效性。", "conclusion": "本研究强调了语义关键点表示方法的潜力，为3D可变形物体的重定向和多物体操作引入了新视角，并为配对物品打包提供了参考。"}}
{"id": "2509.05490", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10; I.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.05490", "abs": "https://arxiv.org/abs/2509.05490", "authors": ["Andrzej D. Dobrzycki", "Ana M. Bernardos", "José R. Casar"], "title": "An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures", "comment": "31 pages, 14 figures, 9 tables", "summary": "The You Only Look Once (YOLO) architecture is crucial for real-time object\ndetection. However, deploying it in resource-constrained environments such as\nunmanned aerial vehicles (UAVs) requires efficient transfer learning. Although\nlayer freezing is a common technique, the specific impact of various freezing\nconfigurations on contemporary YOLOv8 and YOLOv10 architectures remains\nunexplored, particularly with regard to the interplay between freezing depth,\ndataset characteristics, and training dynamics. This research addresses this\ngap by presenting a detailed analysis of layer-freezing strategies. We\nsystematically investigate multiple freezing configurations across YOLOv8 and\nYOLOv10 variants using four challenging datasets that represent critical\ninfrastructure monitoring. Our methodology integrates a gradient behavior\nanalysis (L2 norm) and visual explanations (Grad-CAM) to provide deeper\ninsights into training dynamics under different freezing strategies. Our\nresults reveal that there is no universal optimal freezing strategy but,\nrather, one that depends on the properties of the data. For example, freezing\nthe backbone is effective for preserving general-purpose features, while a\nshallower freeze is better suited to handling extreme class imbalance. These\nconfigurations reduce graphics processing unit (GPU) memory consumption by up\nto 28% compared to full fine-tuning and, in some cases, achieve mean average\nprecision (mAP@50) scores that surpass those of full fine-tuning. Gradient\nanalysis corroborates these findings, showing distinct convergence patterns for\nmoderately frozen models. Ultimately, this work provides empirical findings and\npractical guidelines for selecting freezing strategies. It offers a practical,\nevidence-based approach to balanced transfer learning for object detection in\nscenarios with limited resources.", "AI": {"tldr": "本研究深入分析了YOLOv8和YOLOv10模型在资源受限环境中，不同层冻结策略对迁移学习性能的影响，发现最优策略取决于数据特性，并提供了实用的指导方针。", "motivation": "YOLO架构在实时目标检测中至关重要，但在无人机等资源受限环境中部署需要高效的迁移学习。尽管层冻结是常用技术，但其在YOLOv8和YOLOv10上的具体影响，特别是冻结深度、数据集特性和训练动态之间的相互作用，尚未得到充分探索。", "method": "研究系统地调查了YOLOv8和YOLOv10变体上的多种层冻结配置，使用了四个具有挑战性的关键基础设施监测数据集。方法整合了梯度行为分析（L2范数）和视觉解释（Grad-CAM），以深入了解不同冻结策略下的训练动态。", "result": "研究发现不存在通用的最优冻结策略，而是取决于数据特性。例如，冻结骨干网络有助于保留通用特征，而较浅的冻结更适用于处理极端类别不平衡。这些配置将GPU内存消耗减少了高达28%，在某些情况下，其mAP@50分数甚至超过了完全微调。梯度分析也证实了适度冻结模型的独特收敛模式。", "conclusion": "本工作提供了选择层冻结策略的实证发现和实用指南，为资源有限场景下的目标检测迁移学习提供了一种实用、基于证据的平衡方法。"}}
{"id": "2509.06174", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06174", "abs": "https://arxiv.org/abs/2509.06174", "authors": ["Wei Han", "Geng Zhan", "Sicheng Yu", "Chenyu Wang", "Bryan Hooi"], "title": "From Long to Short: LLMs Excel at Trimming Own Reasoning Chains", "comment": "21 pages, 5 figures, 7 tables", "summary": "O1/R1 style large reasoning models (LRMs) signal a substantial leap forward\nover conventional instruction-following LLMs. By applying test-time scaling to\ngenerate extended reasoning paths, they establish many SOTAs across a wide\nrange of complex reasoning tasks. However, recent studies show that LRMs are\nprone to suffer from overthinking -- the tendency to overcomplicate simple\nproblems, leading to excessive strategy switching and long, convoluted\nreasoning traces that hinder their interpretability. To mitigate this issue, we\nconduct a systematic investigation into the reasoning efficiency of a broad set\nof LRMs and uncover a common dilemma: the difficulty in balancing multiple\ngeneration objectives such as correctness and brevity. Based on this discovery,\nwe propose a test-time scaling method, EDIT (Efficient Dynamic Inference\nTrimming), which efficiently guides LRMs to identify the shortest correct\nreasoning paths at test time. EDIT employs constraint-guided generation while\njointly tracking length and answer distributions under varying constraints,\nallowing it to select responses that strike an optimal balance between\nconciseness and correctness. Extensive experiments across diverse models and\ndatasets show that EDIT substantially enhance the reasoning efficiency,\nproducing compact yet informative outputs that improve readability and user\nexperience.", "AI": {"tldr": "本文提出了一种名为EDIT的测试时缩放方法，旨在解决大型推理模型（LRMs）在处理简单问题时过度思考的问题，从而在保持正确性的前提下，生成更简洁、高效的推理路径。", "motivation": "尽管O1/R1风格的大型推理模型（LRMs）在复杂推理任务中取得了最先进的成果，但它们常常会“过度思考”，即过度复杂化简单问题，导致策略切换过多和推理路径冗长曲折，损害了可解释性。研究发现LRMs在正确性和简洁性等多个生成目标之间难以平衡。", "method": "本文提出了一种名为EDIT（高效动态推理修剪）的测试时缩放方法。EDIT通过约束引导的生成，同时跟踪不同约束下的长度和答案分布，从而在简洁性和正确性之间选择最佳平衡的响应，引导LRMs识别最短的正确推理路径。", "result": "在各种模型和数据集上的广泛实验表明，EDIT显著提高了推理效率，生成了紧凑但信息丰富的输出，从而改善了可读性和用户体验。", "conclusion": "EDIT方法成功地缓解了LRMs的“过度思考”问题，通过在测试时动态引导模型生成更短、更高效的推理路径，实现了推理效率的显著提升，并在简洁性和正确性之间取得了最佳平衡。"}}
{"id": "2509.05863", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05863", "abs": "https://arxiv.org/abs/2509.05863", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "title": "LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization", "comment": null, "summary": "We present LatinX, a multilingual text-to-speech (TTS) model for cascaded\nspeech-to-speech translation that preserves the source speaker's identity\nacross languages. LatinX is a 12-layer decoder-only Transformer trained in\nthree stages: (i) pre-training for text-to-audio mapping, (ii) supervised\nfine-tuning for zero-shot voice cloning, and (iii) alignment with Direct\nPreference Optimization (DPO) using automatically labeled pairs based on Word\nError Rate (WER) and speaker-similarity metrics. Trained on English and Romance\nlanguages with emphasis on Portuguese, LatinX with DPO consistently reduces WER\nand improves objective similarity over the fine-tuned baseline. Human\nevaluations further indicate stronger perceived speaker similarity than a\nstrong baseline (XTTSv2), revealing gaps between objective and subjective\nmeasures. We provide cross-lingual analyses and discuss balanced preference\nsignals and lower-latency architectures as future work.", "AI": {"tldr": "LatinX是一个多语言文本到语音（TTS）模型，用于级联语音到语音翻译，能在不同语言间保留源说话者的身份，通过三阶段训练（包括DPO）实现。", "motivation": "在级联语音到语音翻译中，需要在不同语言间保留源说话者的身份。", "method": "LatinX是一个12层仅解码器的Transformer模型，分三阶段训练：(i) 文本到音频映射的预训练，(ii) 零样本语音克隆的监督微调，(iii) 使用基于词错误率（WER）和说话者相似度指标的自动标注对进行直接偏好优化（DPO）对齐。模型主要在英语和罗曼语系（特别是葡萄牙语）上训练。", "result": "与微调基线相比，结合DPO的LatinX模型持续降低了WER并提高了客观相似度。人类评估显示，其感知到的说话者相似度优于强基线（XTTSv2），但也揭示了客观和主观度量之间的差距。", "conclusion": "LatinX模型成功实现了多语言文本到语音转换中的跨语言说话者身份保留。未来的工作将探索平衡偏好信号和低延迟架构。"}}
{"id": "2509.06682", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06682", "abs": "https://arxiv.org/abs/2509.06682", "authors": ["Sajad Ahmadi", "Mohammadreza Davoodi", "Javad Mohammadpour Velni"], "title": "An Adaptive Coverage Control Approach for Multiple Autonomous Off-road Vehicles in Dynamic Agricultural Fields", "comment": null, "summary": "This paper presents an adaptive coverage control method for a fleet of\noff-road and Unmanned Ground Vehicles (UGVs) operating in dynamic\n(time-varying) agricultural environments. Traditional coverage control\napproaches often assume static conditions, making them unsuitable for\nreal-world farming scenarios where obstacles, such as moving machinery and\nuneven terrains, create continuous challenges. To address this, we propose a\nreal-time path planning framework that integrates Unmanned Aerial Vehicles\n(UAVs) for obstacle detection and terrain assessment, allowing UGVs to\ndynamically adjust their coverage paths. The environment is modeled as a\nweighted directed graph, where the edge weights are continuously updated based\non the UAV observations to reflect obstacle motion and terrain variations. The\nproposed approach incorporates Voronoi-based partitioning, adaptive edge weight\nassignment, and cost-based path optimization to enhance navigation efficiency.\nSimulation results demonstrate the effectiveness of the proposed method in\nimproving path planning, reducing traversal costs, and maintaining robust\ncoverage in the presence of dynamic obstacles and muddy terrains.", "AI": {"tldr": "本文提出了一种自适应覆盖控制方法，使越野无人地面车辆（UGVs）能够在动态农业环境中，通过集成无人机（UAVs）进行实时障碍物检测和地形评估，动态调整其覆盖路径。", "motivation": "传统的覆盖控制方法假设静态条件，不适用于存在移动机械和不平坦地形等动态障碍的真实农业场景。", "method": "该方法提出了一个实时路径规划框架，整合无人机进行障碍物检测和地形评估。环境被建模为加权有向图，图的边权重根据无人机观测结果（障碍物运动和地形变化）持续更新。该方法结合了基于Voronoi的划分、自适应边权重分配和基于成本的路径优化。", "result": "仿真结果表明，所提出的方法在存在动态障碍物和泥泞地形的情况下，有效改进了路径规划，降低了遍历成本，并保持了鲁棒的覆盖。", "conclusion": "该自适应覆盖控制方法能够有效提高无人地面车辆在动态农业环境中的路径规划效率、降低成本并保持稳健的覆盖。"}}
{"id": "2509.06061", "categories": ["cs.RO", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.06061", "abs": "https://arxiv.org/abs/2509.06061", "authors": ["Faiza Babakano", "Ahmed Fahmin", "Bojie Shen", "Muhammad Aamir Cheema", "Isma Farah Siddiqui"], "title": "Energy-Efficient Path Planning with Multi-Location Object Pickup for Mobile Robots on Uneven Terrain", "comment": null, "summary": "Autonomous Mobile Robots (AMRs) operate on battery power, making energy\nefficiency a critical consideration, particularly in outdoor environments where\nterrain variations affect energy consumption. While prior research has\nprimarily focused on computing energy-efficient paths from a source to a\ndestination, these approaches often overlook practical scenarios where a robot\nneeds to pick up an object en route - an action that can significantly impact\nenergy consumption due to changes in payload. This paper introduces the\nObject-Pickup Minimum Energy Path Problem (OMEPP), which addresses\nenergy-efficient route planning for AMRs required to pick up an object from one\nof many possible locations and deliver it to a destination. To address OMEPP,\nwe first introduce a baseline algorithm that employs the Z star algorithm, a\nvariant of A star tailored for energy-efficient routing, to iteratively visit\neach pickup point. While this approach guarantees optimality, it suffers from\nhigh computational cost due to repeated searches at each pickup location. To\nmitigate this inefficiency, we propose a concurrent PCPD search that manages\nmultiple Z star searches simultaneously across all pickup points. Central to\nour solution is the Payload-Constrained Path Database (PCPD), an extension of\nthe Compressed Path Database (CPD) that incorporates payload constraints. We\ndemonstrate that PCPD significantly reduces branching factors during search,\nimproving overall performance. Although the concurrent PCPD search may produce\nslightly suboptimal solutions, extensive experiments on real-world datasets\nshow it achieves near-optimal performance while being one to two orders of\nmagnitude faster than the baseline algorithm.", "AI": {"tldr": "本文提出了物体拾取最小能量路径问题（OMEPP），旨在为需要拾取物体并递送的自主移动机器人（AMR）规划节能路径。为解决此问题，我们引入了一种并发PCPD搜索算法，该算法使用包含有效载荷约束的路径数据库，实现了比基线算法快1-2个数量级的近最优性能。", "motivation": "自主移动机器人（AMR）依赖电池供电，因此能源效率至关重要，尤其是在地形多变的户外环境。现有研究主要关注从起点到终点的节能路径规划，但忽略了机器人途中拾取物体（导致有效载荷变化，显著影响能耗）的实际场景。", "method": "本文首先提出了物体拾取最小能量路径问题（OMEPP）。为解决OMEPP，我们：1. 引入了一个基线算法，该算法迭代地对每个可能的拾取点使用Z星算法（一种针对节能路由的A星变体），确保最优性但计算成本高。2. 提出了一种并发PCPD搜索算法，该算法同时管理多个Z星搜索。其核心是有效载荷约束路径数据库（PCPD），它是压缩路径数据库（CPD）的扩展，融入了有效载荷约束，显著降低了搜索过程中的分支因子。", "result": "实验结果表明，并发PCPD搜索算法虽然可能产生略微次优的解决方案，但在真实世界数据集上实现了近乎最优的性能，并且比基线算法快1到2个数量级。PCPD显著减少了搜索过程中的分支因子，从而提高了整体性能。", "conclusion": "并发PCPD搜索算法为需要拾取物体的自主移动机器人提供了高效且近乎最优的节能路径规划解决方案。它在计算速度上取得了显著提升，使其在实际应用中更具可行性。"}}
{"id": "2509.05512", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05512", "abs": "https://arxiv.org/abs/2509.05512", "authors": ["Bryce Grant", "Peng Wang"], "title": "Quaternion Approximation Networks for Enhanced Image Classification and Oriented Object Detection", "comment": "Accepted to IROS 2025", "summary": "This paper introduces Quaternion Approximate Networks (QUAN), a novel deep\nlearning framework that leverages quaternion algebra for rotation equivariant\nimage classification and object detection. Unlike conventional quaternion\nneural networks attempting to operate entirely in the quaternion domain, QUAN\napproximates quaternion convolution through Hamilton product decomposition\nusing real-valued operations. This approach preserves geometric properties\nwhile enabling efficient implementation with custom CUDA kernels. We introduce\nIndependent Quaternion Batch Normalization (IQBN) for training stability and\nextend quaternion operations to spatial attention mechanisms. QUAN is evaluated\non image classification (CIFAR-10/100, ImageNet), object detection (COCO,\nDOTA), and robotic perception tasks. In classification tasks, QUAN achieves\nhigher accuracy with fewer parameters and faster convergence compared to\nexisting convolution and quaternion-based models. For objection detection, QUAN\ndemonstrates improved parameter efficiency and rotation handling over standard\nConvolutional Neural Networks (CNNs) while establishing the SOTA for quaternion\nCNNs in this downstream task. These results highlight its potential for\ndeployment in resource-constrained robotic systems requiring rotation-aware\nperception and application in other domains.", "AI": {"tldr": "本文提出四元数近似网络（QUAN），通过实值操作近似四元数卷积，实现旋转等变图像分类和目标检测，性能优于现有模型且参数更少。", "motivation": "利用四元数代数实现旋转等变图像分类和目标检测，解决传统四元数神经网络操作复杂性和标准CNN处理旋转的局限性。", "method": "引入四元数近似网络（QUAN），通过哈密顿积分解使用实值操作近似四元数卷积，并利用自定义CUDA核实现高效计算。引入独立四元数批归一化（IQBN）以提高训练稳定性，并将四元数操作扩展到空间注意力机制。", "result": "在图像分类任务中，QUAN以更少的参数和更快的收敛速度实现了更高的准确性。在目标检测任务中，QUAN显示出比标准CNN更好的参数效率和旋转处理能力，并为该下游任务中的四元数CNN建立了最先进（SOTA）的性能。", "conclusion": "QUAN在需要旋转感知感知的资源受限机器人系统和其他领域具有部署潜力，因其高效性和出色的旋转处理能力。"}}
{"id": "2509.06235", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.6; I.2.8"], "pdf": "https://arxiv.org/pdf/2509.06235", "abs": "https://arxiv.org/abs/2509.06235", "authors": ["Olivier Schipper", "Yudi Zhang", "Yali Du", "Mykola Pechenizkiy", "Meng Fang"], "title": "PillagerBench: Benchmarking LLM-Based Agents in Competitive Minecraft Team Environments", "comment": "for the source code, see https://github.com/aialt/PillagerBench", "summary": "LLM-based agents have shown promise in various cooperative and strategic\nreasoning tasks, but their effectiveness in competitive multi-agent\nenvironments remains underexplored. To address this gap, we introduce\nPillagerBench, a novel framework for evaluating multi-agent systems in\nreal-time competitive team-vs-team scenarios in Minecraft. It provides an\nextensible API, multi-round testing, and rule-based built-in opponents for\nfair, reproducible comparisons. We also propose TactiCrafter, an LLM-based\nmulti-agent system that facilitates teamwork through human-readable tactics,\nlearns causal dependencies, and adapts to opponent strategies. Our evaluation\ndemonstrates that TactiCrafter outperforms baseline approaches and showcases\nadaptive learning through self-play. Additionally, we analyze its learning\nprocess and strategic evolution over multiple game episodes. To encourage\nfurther research, we have open-sourced PillagerBench, fostering advancements in\nmulti-agent AI for competitive environments.", "AI": {"tldr": "该研究引入了PillagerBench框架和TactiCrafter多智能体系统，用于在Minecraft中评估和提升LLM代理在实时竞争性团队对战环境中的表现，TactiCrafter在自适应学习和超越基线方面表现出色。", "motivation": "LLM代理在合作和战略推理任务中展现出潜力，但在竞争性多智能体环境中的有效性尚未得到充分探索。", "method": "1. 引入PillagerBench框架：一个用于在Minecraft中评估多智能体系统在实时竞争性团队对战场景下的平台，提供可扩展API、多轮测试和基于规则的内置对手。2. 提出TactiCrafter：一个基于LLM的多智能体系统，通过人类可读的战术促进团队协作，学习因果依赖，并适应对手策略。3. 评估：通过与基线方法对比和自博弈来展示TactiCrafter的性能和自适应学习能力。", "result": "TactiCrafter在评估中超越了基线方法，并通过自博弈展示了自适应学习能力。研究还分析了其在多个游戏回合中的学习过程和战略演变。", "conclusion": "TactiCrafter在竞争性多智能体环境中表现出强大的性能和自适应学习能力。PillagerBench的开源将促进竞争性多智能体AI领域的进一步研究和发展。"}}
{"id": "2509.05867", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05867", "abs": "https://arxiv.org/abs/2509.05867", "authors": ["ZiXuan Zhang", "Bowen Hao", "Yingjie Li", "Hongzhi Yin"], "title": "ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula", "comment": null, "summary": "Traditional Chinese Medicine (TCM) formulas play a significant role in\ntreating epidemics and complex diseases. Existing models for TCM utilize\ntraditional algorithms or deep learning techniques to analyze formula\nrelationships, yet lack comprehensive results, such as complete formula\ncompositions and detailed explanations. Although recent efforts have used TCM\ninstruction datasets to fine-tune Large Language Models (LLMs) for explainable\nformula generation, existing datasets lack sufficient details, such as the\nroles of the formula's sovereign, minister, assistant, courier; efficacy;\ncontraindications; tongue and pulse diagnosis-limiting the depth of model\noutputs. To address these challenges, we propose ZhiFangDanTai, a framework\ncombining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM\nfine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured\nTCM knowledge into concise summaries, while also constructing an enhanced\ninstruction dataset to improve LLMs' ability to integrate retrieved\ninformation. Furthermore, we provide novel theoretical proofs demonstrating\nthat integrating GraphRAG with fine-tuning techniques can reduce generalization\nerror and hallucination rates in the TCM formula task. Experimental results on\nboth collected and clinical datasets demonstrate that ZhiFangDanTai achieves\nsignificant improvements over state-of-the-art models. Our model is\nopen-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.", "AI": {"tldr": "本文提出了ZhiFangDanTai框架，结合图谱检索增强生成（GraphRAG）和大型语言模型（LLM）微调，旨在生成全面且可解释的中药方剂信息，解决了现有模型和数据集的局限性，并在实验中取得了显著提升。", "motivation": "现有中药方剂分析模型（传统算法或深度学习）缺乏完整的方剂组成和详细解释。尽管近期有研究使用中药指令数据集微调LLM以生成可解释方剂，但现有数据集缺乏方剂君臣佐使、功效、禁忌、舌脉诊断等详细信息，限制了模型输出的深度。", "method": "本文提出了ZhiFangDanTai框架。该框架结合了GraphRAG和LLM微调：GraphRAG用于检索并综合结构化的中医药知识生成简洁摘要；同时，构建了一个增强型指令数据集以提升LLM整合检索信息的能力。此外，提供了理论证明，表明将GraphRAG与微调技术结合可以降低中药方剂任务中的泛化误差和幻觉率。", "result": "在收集数据集和临床数据集上的实验结果表明，ZhiFangDanTai模型比现有最先进的模型取得了显著的改进。", "conclusion": "ZhiFangDanTai框架成功地结合了GraphRAG和LLM微调，有效解决了中药方剂生成中信息不全面和解释性不足的问题，并在理论和实践上都展现出优越性能，为中医药方剂的智能化生成提供了新途径。"}}
{"id": "2509.06687", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06687", "abs": "https://arxiv.org/abs/2509.06687", "authors": ["Sajad Ahmadi", "Hossein Nejatbakhsh Esfahani", "Javad Mohammadpour Velni"], "title": "Safe Robust Predictive Control-based Motion Planning of Automated Surface Vessels in Inland Waterways", "comment": null, "summary": "Deploying self-navigating surface vessels in inland waterways offers a\nsustainable alternative to reduce road traffic congestion and emissions.\nHowever, navigating confined waterways presents unique challenges, including\nnarrow channels, higher traffic density, and hydrodynamic disturbances.\nExisting methods for autonomous vessel navigation often lack the robustness or\nprecision required for such environments. This paper presents a new motion\nplanning approach for Automated Surface Vessels (ASVs) using Robust Model\nPredictive Control (RMPC) combined with Control Barrier Functions (CBFs). By\nincorporating channel borders and obstacles as safety constraints within the\ncontrol design framework, the proposed method ensures both collision avoidance\nand robust navigation on complex waterways. Simulation results demonstrate the\nefficacy of the proposed method in safely guiding ASVs under realistic\nconditions, highlighting its improved safety and adaptability compared to the\nstate-of-the-art.", "AI": {"tldr": "本文提出一种结合鲁棒模型预测控制（RMPC）和控制障碍函数（CBFs）的新型运动规划方法，旨在解决内河航道中自主水面船舶（ASVs）的安全导航和避障挑战。", "motivation": "内河航道自主导航面临独特挑战，如航道狭窄、交通密度高和水动力干扰。现有自主船舶导航方法往往缺乏在这些环境中所需的鲁棒性或精度。", "method": "该研究提出了一种新的自主水面船舶（ASVs）运动规划方法，该方法结合了鲁棒模型预测控制（RMPC）和控制障碍函数（CBFs），并将航道边界和障碍物作为安全约束纳入控制设计框架。", "result": "仿真结果表明，所提出的方法能够有效地在真实条件下安全引导ASVs，并相对于现有技术，在安全性和适应性方面有所改进。", "conclusion": "该方法通过确保碰撞避免和鲁棒导航，提高了ASVs在复杂内河航道中的安全性和适应性，克服了现有方法的不足。"}}
{"id": "2509.06191", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06191", "abs": "https://arxiv.org/abs/2509.06191", "authors": ["Yifei Ren", "Edward Johns"], "title": "Learning in ImaginationLand: Omnidirectional Policies through 3D Generative Models (OP-Gen)", "comment": "Project webpage with robot videos:\n  https://www.robot-learning.uk/op-gen", "summary": "Recent 3D generative models, which are capable of generating full object\nshapes from just a few images, now open up new opportunities in robotics. In\nthis work, we show that 3D generative models can be used to augment a dataset\nfrom a single real-world demonstration, after which an omnidirectional policy\ncan be learned within this imagined dataset. We found that this enables a robot\nto perform a task when initialised from states very far from those observed\nduring the demonstration, including starting from the opposite side of the\nobject relative to the real-world demonstration, significantly reducing the\nnumber of demonstrations required for policy learning. Through several\nreal-world experiments across tasks such as grasping objects, opening a drawer,\nand placing trash into a bin, we study these omnidirectional policies by\ninvestigating the effect of various design choices on policy behaviour, and we\nshow superior performance to recent baselines which use alternative methods for\ndata augmentation.", "AI": {"tldr": "本文展示了如何利用3D生成模型从单一真实世界演示中扩充数据集，从而学习全向策略，使机器人能够从与演示状态差异很大的初始状态下执行任务，显著减少了策略学习所需的演示数量。", "motivation": "机器人策略学习通常需要大量演示，或难以在与演示状态差异较大的初始状态下有效执行任务。利用3D生成模型从少量图像生成完整物体形状的能力，为解决这一问题提供了新机遇。", "method": "研究方法包括：1) 使用3D生成模型从单一真实世界演示中扩充数据集；2) 在扩充的虚拟数据集中学习全向策略；3) 通过抓取物体、打开抽屉、将垃圾放入垃圾桶等多个真实世界任务进行实验；4) 探究不同设计选择对策略行为的影响，并与使用替代数据增强方法的最新基线进行性能比较。", "result": "研究发现，该方法使机器人能够从与演示状态（包括物体相对的另一侧）差异很大的初始状态下执行任务，显著减少了策略学习所需的演示数量。在真实世界实验中，所提出的全向策略表现出优于使用替代数据增强方法的最新基线的性能。", "conclusion": "3D生成模型可以有效地用于从单一演示中扩充数据集，从而学习鲁棒的全向机器人策略。这种方法能够显著降低策略学习所需的演示数量，并提高了机器人在多样化初始状态下的任务执行能力，性能优于其他数据增强基线。"}}
{"id": "2509.05513", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05513", "abs": "https://arxiv.org/abs/2509.05513", "authors": ["Ahad Jawaid", "Yu Xiang"], "title": "OpenEgo: A Large-Scale Multimodal Egocentric Dataset for Dexterous Manipulation", "comment": "4 pages, 1 figure", "summary": "Egocentric human videos provide scalable demonstrations for imitation\nlearning, but existing corpora often lack either fine-grained, temporally\nlocalized action descriptions or dexterous hand annotations. We introduce\nOpenEgo, a multimodal egocentric manipulation dataset with standardized\nhand-pose annotations and intention-aligned action primitives. OpenEgo totals\n1107 hours across six public datasets, covering 290 manipulation tasks in 600+\nenvironments. We unify hand-pose layouts and provide descriptive, timestamped\naction primitives. To validate its utility, we train language-conditioned\nimitation-learning policies to predict dexterous hand trajectories. OpenEgo is\ndesigned to lower the barrier to learning dexterous manipulation from\negocentric video and to support reproducible research in vision-language-action\nlearning. All resources and instructions will be released at\nwww.openegocentric.com.", "AI": {"tldr": "OpenEgo是一个大规模的多模态第一视角操作数据集，统一了手部姿态标注并提供了意图对齐的动作原语，旨在降低从第一视角视频学习灵巧操作的门槛。", "motivation": "现有的第一视角人体视频数据集在细粒度、时间局部化的动作描述或灵巧手部标注方面存在不足，限制了其在模仿学习中的应用。", "method": "OpenEgo整合了六个公共数据集，总计1107小时，涵盖290个操作任务。它统一了手部姿态布局，并提供了描述性、带时间戳的动作原语。通过训练语言条件模仿学习策略来预测灵巧手部轨迹，以验证其效用。", "result": "OpenEgo包含1107小时的数据，覆盖600多个环境中的290个操作任务。它提供了标准化的手部姿态标注和意图对齐的动作原语。该数据集已成功用于训练语言条件模仿学习策略来预测灵巧手部轨迹。", "conclusion": "OpenEgo旨在降低从第一视角视频学习灵巧操作的障碍，并支持视觉-语言-动作学习领域的可复现研究。所有资源和说明都将在指定网站发布。"}}
{"id": "2509.06239", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06239", "abs": "https://arxiv.org/abs/2509.06239", "authors": ["Manvi Jha", "Jiaxin Wan", "Deming Chen"], "title": "Proof2Silicon: Prompt Repair for Verified Code and Hardware Generation via Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nautomated code generation but frequently produce code that fails formal\nverification, an essential requirement for hardware and safety-critical\ndomains. To overcome this fundamental limitation, we previously proposed\nPREFACE, a model-agnostic framework based on reinforcement learning (RL) that\niteratively repairs the prompts provided to frozen LLMs, systematically\nsteering them toward generating formally verifiable Dafny code without costly\nfine-tuning. This work presents Proof2Silicon, a novel end-to-end synthesis\nframework that embeds the previously proposed PREFACE flow to enable the\ngeneration of correctness-by-construction hardware directly from natural\nlanguage specifications. Proof2Silicon operates by: (1) leveraging PREFACE's\nverifier-driven RL agent to optimize prompt generation iteratively, ensuring\nDafny code correctness; (2) automatically translating verified Dafny programs\ninto synthesizable high-level C using Dafny's Python backend and PyLog; and (3)\nemploying Vivado HLS to produce RTL implementations. Evaluated rigorously on a\nchallenging 100-task benchmark, PREFACE's RL-guided prompt optimization\nconsistently improved Dafny verification success rates across diverse LLMs by\nup to 21%. Crucially, Proof2Silicon achieved an end-to-end hardware synthesis\nsuccess rate of up to 72%, generating RTL designs through Vivado HLS synthesis\nflows. These results demonstrate a robust, scalable, and automated pipeline for\nLLM-driven, formally verified hardware synthesis, bridging natural-language\nspecification and silicon realization.", "AI": {"tldr": "Proof2Silicon是一个端到端框架，它利用强化学习优化LLM提示，从自然语言规范生成形式化验证的硬件，解决了LLM代码验证失败的问题。", "motivation": "大型语言模型（LLMs）在代码生成方面表现出色，但生成的代码经常无法通过形式化验证，这在硬件和安全关键领域是一个基本且重要的限制。", "method": "本文提出了Proof2Silicon框架，该框架包含：1) PREFACE（一个基于强化学习的代理），通过迭代修复提示来优化LLM生成，确保Dafny代码的正确性；2) 自动将验证过的Dafny程序通过Dafny的Python后端和PyLog翻译成可综合的高级C语言；3) 使用Vivado HLS工具将C代码生成为RTL实现。", "result": "PREFACE的RL引导提示优化在不同LLM上将Dafny验证成功率提高了高达21%。Proof2Silicon实现了高达72%的端到端硬件综合成功率，通过Vivado HLS综合流程生成了RTL设计。", "conclusion": "Proof2Silicon展示了一个强大、可扩展且自动化的管道，实现了LLM驱动的形式化验证硬件综合，成功地将自然语言规范转化为硅实现。"}}
{"id": "2509.05878", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05878", "abs": "https://arxiv.org/abs/2509.05878", "authors": ["François Grolleau", "Emily Alsentzer", "Timothy Keyes", "Philip Chung", "Akshay Swaminathan", "Asad Aali", "Jason Hom", "Tridu Huynh", "Thomas Lew", "April S. Liang", "Weihan Chu", "Natasha Z. Steele", "Christina F. Lin", "Jingkun Yang", "Kameron C. Black", "Stephen P. Ma", "Fateme N. Haredasht", "Nigam H. Shah", "Kevin Schulman", "Jonathan H. Chen"], "title": "MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries", "comment": null, "summary": "Evaluating factual accuracy in Large Language Model (LLM)-generated clinical\ntext is a critical barrier to adoption, as expert review is unscalable for the\ncontinuous quality assurance these systems require. We address this challenge\nwith two complementary contributions. First, we introduce MedFactEval, a\nframework for scalable, fact-grounded evaluation where clinicians define\nhigh-salience key facts and an \"LLM Jury\"--a multi-LLM majority vote--assesses\ntheir inclusion in generated summaries. Second, we present MedAgentBrief, a\nmodel-agnostic, multi-step workflow designed to generate high-quality, factual\ndischarge summaries. To validate our evaluation framework, we established a\ngold-standard reference using a seven-physician majority vote on\nclinician-defined key facts from inpatient cases. The MedFactEval LLM Jury\nachieved almost perfect agreement with this panel (Cohen's kappa=81%), a\nperformance statistically non-inferior to that of a single human expert\n(kappa=67%, P < 0.001). Our work provides both a robust evaluation framework\n(MedFactEval) and a high-performing generation workflow (MedAgentBrief),\noffering a comprehensive approach to advance the responsible deployment of\ngenerative AI in clinical workflows.", "AI": {"tldr": "本文提出了一个可扩展的临床文本事实评估框架（MedFactEval）和一个高质量生成工作流（MedAgentBrief），以解决大型语言模型在临床应用中事实准确性评估的挑战。", "motivation": "大型语言模型（LLM）生成的临床文本的事实准确性是其被采纳的关键障碍，而专家评审对于系统所需的持续质量保证来说是不可扩展的。", "method": "研究提出了两个互补的贡献：1. MedFactEval框架，其中临床医生定义高显著性关键事实，然后由“LLM陪审团”（多LLM多数投票）评估这些事实在生成摘要中的包含情况。2. MedAgentBrief，一个模型无关的多步骤工作流，用于生成高质量、事实准确的出院总结。为验证评估框架，研究建立了由七名医生多数投票确定的黄金标准参考，并使用Cohen's kappa衡量LLM陪审团与专家组的一致性。", "result": "MedFactEval的LLM陪审团与医生专家组达到了几乎完美的一致性（Cohen's kappa=81%），并且其表现统计学上不劣于单个人类专家（kappa=67%，P < 0.001）。", "conclusion": "本工作提供了一个强大的评估框架（MedFactEval）和一个高性能的生成工作流（MedAgentBrief），为在临床工作流中负责任地部署生成式AI提供了一个全面的方法。"}}
{"id": "2509.06953", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.06953", "abs": "https://arxiv.org/abs/2509.06953", "authors": ["Jiahui Yang", "Jason Jingzhou Liu", "Yulong Li", "Youssef Khaky", "Kenneth Shaw", "Deepak Pathak"], "title": "Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments", "comment": "Website at \\url{deep-reactive-policy.com}", "summary": "Generating collision-free motion in dynamic, partially observable\nenvironments is a fundamental challenge for robotic manipulators. Classical\nmotion planners can compute globally optimal trajectories but require full\nenvironment knowledge and are typically too slow for dynamic scenes. Neural\nmotion policies offer a promising alternative by operating in closed-loop\ndirectly on raw sensory inputs but often struggle to generalize in complex or\ndynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neural\nmotion policy designed for reactive motion generation in diverse dynamic\nenvironments, operating directly on point cloud sensory input. At its core is\nIMPACT, a transformer-based neural motion policy pretrained on 10 million\ngenerated expert trajectories across diverse simulation scenarios. We further\nimprove IMPACT's static obstacle avoidance through iterative student-teacher\nfinetuning. We additionally enhance the policy's dynamic obstacle avoidance at\ninference time using DCP-RMP, a locally reactive goal-proposal module. We\nevaluate DRP on challenging tasks featuring cluttered scenes, dynamic moving\nobstacles, and goal obstructions. DRP achieves strong generalization,\noutperforming prior classical and neural methods in success rate across both\nsimulated and real-world settings. Video results and code available at\nhttps://deep-reactive-policy.com", "AI": {"tldr": "本文提出了一种名为深度反应策略 (DRP) 的视觉-运动神经运动策略，用于在动态、部分可观测的环境中生成无碰撞运动。DRP直接处理点云输入，并结合了预训练的Transformer策略（IMPACT）、迭代师生微调以及一个局部反应式目标提议模块（DCP-RMP），在模拟和真实世界中均表现出优异的泛化能力和成功率。", "motivation": "在动态、部分可观测的环境中为机器人机械臂生成无碰撞运动是一个基本挑战。传统的运动规划器虽然能计算全局最优轨迹，但需要完整的环境知识且速度慢，不适用于动态场景。现有的神经运动策略虽然能直接从原始感官输入进行闭环操作，但在复杂或动态设置中泛化能力不足。", "method": "本文提出了深度反应策略 (DRP)，这是一种基于点云感官输入的视觉-运动神经运动策略。其核心是 IMPACT，一个基于 Transformer 的神经运动策略，通过在多样化仿真场景中生成的1000万条专家轨迹进行预训练。DRP通过迭代师生微调进一步提升了 IMPACT 的静态避障能力。此外，在推理时，DRP利用 DCP-RMP（一个局部反应式目标提议模块）增强了动态避障能力。", "result": "DRP 在包含杂乱场景、动态移动障碍物和目标受阻的挑战性任务中进行了评估。结果显示，DRP 实现了强大的泛化能力，在模拟和真实世界环境中，其成功率均优于先前的经典和神经方法。", "conclusion": "DRP 是一种有效的视觉-运动神经运动策略，能够直接从点云输入在多样化的动态环境中生成反应式无碰撞运动。它通过结合预训练的Transformer模型、精细化训练和动态避障模块，在复杂动态场景中展现出卓越的泛化能力和性能。"}}
{"id": "2509.06201", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06201", "abs": "https://arxiv.org/abs/2509.06201", "authors": ["Jun Yamada", "Adithyavairavan Murali", "Ajay Mandlekar", "Clemens Eppner", "Ingmar Posner", "Balakumar Sundaralingam"], "title": "Grasp-MPC: Closed-Loop Visual Grasping via Value-Guided Model Predictive Control", "comment": "14 pages, 17 figures", "summary": "Grasping of diverse objects in unstructured environments remains a\nsignificant challenge. Open-loop grasping methods, effective in controlled\nsettings, struggle in cluttered environments. Grasp prediction errors and\nobject pose changes during grasping are the main causes of failure. In\ncontrast, closed-loop methods address these challenges in simplified settings\n(e.g., single object on a table) on a limited set of objects, with no path to\ngeneralization. We propose Grasp-MPC, a closed-loop 6-DoF vision-based grasping\npolicy designed for robust and reactive grasping of novel objects in cluttered\nenvironments. Grasp-MPC incorporates a value function, trained on visual\nobservations from a large-scale synthetic dataset of 2 million grasp\ntrajectories that include successful and failed attempts. We deploy this\nlearned value function in an MPC framework in combination with other cost terms\nthat encourage collision avoidance and smooth execution. We evaluate Grasp-MPC\non FetchBench and real-world settings across diverse environments. Grasp-MPC\nimproves grasp success rates by up to 32.6% in simulation and 33.3% in\nreal-world noisy conditions, outperforming open-loop, diffusion policy,\ntransformer policy, and IQL approaches. Videos and more at\nhttp://grasp-mpc.github.io.", "AI": {"tldr": "本文提出了Grasp-MPC，一种基于视觉的闭环6自由度抓取策略，专为在杂乱环境中鲁棒、响应式地抓取新物体而设计。它结合了在大规模合成数据集上训练的价值函数和模型预测控制（MPC）框架，显著提高了模拟和真实世界中的抓取成功率。", "motivation": "在非结构化环境中抓取多样化物体是一个重大挑战。开环抓取方法在受控环境中有效，但在杂乱环境中因抓取预测误差和物体姿态变化而失败。现有闭环方法仅限于简化设置和有限物体，缺乏泛化能力。", "method": "Grasp-MPC是一种闭环6自由度视觉抓取策略。它包含一个价值函数，该函数在包含200万次成功和失败抓取轨迹的大规模合成数据集的视觉观测上进行训练。这个学习到的价值函数被部署在模型预测控制（MPC）框架中，并结合了鼓励避碰和平稳执行的其他成本项。", "result": "Grasp-MPC在FetchBench和真实世界环境中进行了评估，在模拟中将抓取成功率提高了32.6%，在真实世界嘈杂条件下提高了33.3%。它优于开环、扩散策略、Transformer策略和IQL等方法。", "conclusion": "Grasp-MPC为在杂乱环境中鲁棒、响应式地抓取新物体提供了一种有效的解决方案，通过结合学习到的价值函数和MPC框架，显著提高了抓取成功率，超越了现有技术。"}}
{"id": "2509.05515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05515", "abs": "https://arxiv.org/abs/2509.05515", "authors": ["Sen Wang", "Kunyi Li", "Siyun Liang", "Elena Alegret", "Jing Ma", "Nassir Navab", "Stefano Gasperini"], "title": "Visibility-Aware Language Aggregation for Open-Vocabulary Segmentation in 3D Gaussian Splatting", "comment": null, "summary": "Recently, distilling open-vocabulary language features from 2D images into 3D\nGaussians has attracted significant attention. Although existing methods\nachieve impressive language-based interactions of 3D scenes, we observe two\nfundamental issues: background Gaussians contributing negligibly to a rendered\npixel get the same feature as the dominant foreground ones, and multi-view\ninconsistencies due to view-specific noise in language embeddings. We introduce\nVisibility-Aware Language Aggregation (VALA), a lightweight yet effective\nmethod that computes marginal contributions for each ray and applies a\nvisibility-aware gate to retain only visible Gaussians. Moreover, we propose a\nstreaming weighted geometric median in cosine space to merge noisy multi-view\nfeatures. Our method yields a robust, view-consistent language feature\nembedding in a fast and memory-efficient manner. VALA improves open-vocabulary\nlocalization and segmentation across reference datasets, consistently\nsurpassing existing works.", "AI": {"tldr": "本文提出了一种名为VALA的轻量级方法，用于改进将开放词汇语言特征蒸馏到3D高斯中的过程，通过解决背景高斯贡献不足和多视角不一致性问题，从而实现鲁棒、视角一致的语言特征嵌入，并提升了开放词汇定位和分割性能。", "motivation": "现有方法在将开放词汇语言特征蒸馏到3D高斯时存在两个主要问题：1) 背景高斯对渲染像素的贡献微乎其微，却与前景高斯获得相同的特征；2) 语言嵌入中存在视角特异性噪声导致多视角不一致性。", "method": "本文提出了可见性感知语言聚合（VALA）方法。具体包括：1) 计算每条光线的边际贡献，并应用可见性感知门控机制以仅保留可见的高斯；2) 提出在余弦空间中进行流式加权几何中值计算，以融合噪声多视角特征。", "result": "VALA能够以快速且内存高效的方式生成鲁棒、视角一致的语言特征嵌入。该方法显著改进了开放词汇定位和分割任务，在参考数据集上持续超越现有工作。", "conclusion": "VALA通过有效解决背景高斯特征分配不当和多视角特征不一致问题，成功地提升了3D高斯中语言特征蒸馏的质量和实用性，为开放词汇的3D场景理解提供了更强的能力。"}}
{"id": "2509.06269", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06269", "abs": "https://arxiv.org/abs/2509.06269", "authors": ["Vishal Raman", "Vijai Aravindh R", "Abhijith Ragav"], "title": "REMI: A Novel Causal Schema Memory Architecture for Personalized Lifestyle Recommendation Agents", "comment": "8 pages, 2 figures, Accepted at the OARS Workshop, KDD 2025, Paper\n  link: https://oars-workshop.github.io/papers/Raman2025.pdf", "summary": "Personalized AI assistants often struggle to incorporate complex personal\ndata and causal knowledge, leading to generic advice that lacks explanatory\npower. We propose REMI, a Causal Schema Memory architecture for a multimodal\nlifestyle agent that integrates a personal causal knowledge graph, a causal\nreasoning engine, and a schema based planning module. The idea is to deliver\nexplainable, personalized recommendations in domains like fashion, personal\nwellness, and lifestyle planning. Our architecture uses a personal causal graph\nof the user's life events and habits, performs goal directed causal traversals\nenriched with external knowledge and hypothetical reasoning, and retrieves\nadaptable plan schemas to generate tailored action plans. A Large Language\nModel orchestrates these components, producing answers with transparent causal\nexplanations. We outline the CSM system design and introduce new evaluation\nmetrics for personalization and explainability, including Personalization\nSalience Score and Causal Reasoning Accuracy, to rigorously assess its\nperformance. Results indicate that CSM based agents can provide more context\naware, user aligned recommendations compared to baseline LLM agents. This work\ndemonstrates a novel approach to memory augmented, causal reasoning in\npersonalized agents, advancing the development of transparent and trustworthy\nAI lifestyle assistants.", "AI": {"tldr": "本文提出REMI，一种因果图式记忆（CSM）架构，用于多模态生活方式智能体。它整合个人因果知识图、因果推理引擎和基于图式的规划模块，旨在提供可解释、个性化的生活方式推荐。", "motivation": "个性化AI助手难以整合复杂的个人数据和因果知识，导致提供的建议通用且缺乏解释力。", "method": "REMI架构使用用户的个人生活事件和习惯的因果图，进行目标导向的因果遍历，并结合外部知识和假设推理。它检索适应性强的计划图式以生成定制行动计划。大型语言模型（LLM）协调这些组件，生成带有透明因果解释的答案。引入了新的评估指标，如个性化显著性得分和因果推理准确性。", "result": "结果表明，与基线LLM智能体相比，基于CSM的智能体能提供更具情境感知和用户对齐的推荐。", "conclusion": "这项工作展示了一种在个性化智能体中结合记忆增强和因果推理的新颖方法，推动了透明和可信赖的AI生活方式助手的发展。"}}
{"id": "2509.05882", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05882", "abs": "https://arxiv.org/abs/2509.05882", "authors": ["Abhijnan Nath", "Carine Graff", "Nikhil Krishnaswamy"], "title": "Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues", "comment": null, "summary": "As Large Language Models (LLMs) integrate into diverse workflows, they are\nincreasingly being considered \"collaborators\" with humans. If such AI\ncollaborators are to be reliable, their behavior over multiturn interactions\nmust be predictable, validated and verified before deployment. Common alignment\ntechniques are typically developed under simplified single-user settings and do\nnot account for the dynamics of long-horizon multiparty interactions. This\npaper examines how different alignment methods affect LLM agents' effectiveness\nas partners in multiturn, multiparty collaborations. We study this question\nthrough the lens of friction agents that intervene in group dialogues to\nencourage the collaborative group to slow down and reflect upon their reasoning\nfor deliberative decision-making. Using a roleplay methodology, we evaluate\ninterventions from differently-trained friction agents in collaborative task\nconversations. We propose a novel counterfactual evaluation framework that\nquantifies how friction interventions change the trajectory of group\ncollaboration and belief alignment. Our results show that a friction-aware\napproach significantly outperforms common alignment baselines in helping both\nconvergence to a common ground, or agreed-upon task-relevant propositions, and\ncorrectness of task outcomes.", "AI": {"tldr": "本研究探讨了不同对齐方法如何影响大型语言模型（LLM）在多轮、多方协作中的有效性，特别是通过引入“摩擦代理”来促使团队反思和审议，结果表明这种方法显著优于传统对齐基线。", "motivation": "随着LLM日益成为人类的“合作者”，其在多轮互动中的行为可预测性、可验证性和可靠性变得至关重要。然而，常见的LLM对齐技术多在简化的单用户环境下开发，未能充分考虑长周期、多方互动的动态性。", "method": "研究通过“摩擦代理”的视角，考察了其如何干预群体对话，鼓励协作团队放慢速度并反思推理过程，以促进审议性决策。采用角色扮演方法评估了不同训练的摩擦代理在协作任务对话中的干预效果。提出了一种新颖的反事实评估框架，量化摩擦干预对群体协作轨迹和信念对齐的影响。", "result": "研究结果表明，与常见的对齐基线相比，摩擦感知（friction-aware）方法在帮助团队达成共识（即任务相关的共同命题）和提高任务结果的正确性方面，表现出显著优势。", "conclusion": "摩擦感知方法能有效提升LLM代理在多轮、多方协作中的表现，使其能更好地促进群体达成共识和提高决策的正确性，从而为构建更可靠的AI协作伙伴提供了新的方向。"}}
{"id": "2509.06233", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06233", "abs": "https://arxiv.org/abs/2509.06233", "authors": ["Tongxuan Tian", "Xuhui Kang", "Yen-Ling Kuo"], "title": "O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation", "comment": "Conference on Robot Learning (CoRL) 2025. Project website:\n  https://o3afford.github.io/", "summary": "Grounding object affordance is fundamental to robotic manipulation as it\nestablishes the critical link between perception and action among interacting\nobjects. However, prior works predominantly focus on predicting single-object\naffordance, overlooking the fact that most real-world interactions involve\nrelationships between pairs of objects. In this work, we address the challenge\nof object-to-object affordance grounding under limited data contraints.\nInspired by recent advances in few-shot learning with 2D vision foundation\nmodels, we propose a novel one-shot 3D object-to-object affordance learning\napproach for robotic manipulation. Semantic features from vision foundation\nmodels combined with point cloud representation for geometric understanding\nenable our one-shot learning pipeline to generalize effectively to novel\nobjects and categories. We further integrate our 3D affordance representation\nwith large language models (LLMs) for robotics manipulation, significantly\nenhancing LLMs' capability to comprehend and reason about object interactions\nwhen generating task-specific constraint functions. Our experiments on 3D\nobject-to-object affordance grounding and robotic manipulation demonstrate that\nour O$^3$Afford significantly outperforms existing baselines in terms of both\naccuracy and generalization capability.", "AI": {"tldr": "本文提出了一种名为 O$^3$Afford 的新颖单样本3D物体间功能性学习方法，用于机器人操作。该方法结合了视觉基础模型、点云表示和大型语言模型，在有限数据下有效泛化，并显著提升了LLMs对物体交互的理解和推理能力。", "motivation": "以往研究主要关注单物体功能性预测，但现实世界中的大多数交互涉及物体对之间的关系。此外，在数据有限的约束下，处理物体间功能性接地是一个挑战。", "method": "受2D视觉基础模型在少样本学习方面进展的启发，本文提出了一种单样本3D物体间功能性学习方法。该方法结合了来自视觉基础模型的语义特征和用于几何理解的点云表示。此外，还将3D功能性表示与大型语言模型（LLMs）集成，以增强LLMs在生成任务特定约束函数时理解和推理物体交互的能力。", "result": "在3D物体间功能性接地和机器人操作的实验中，O$^3$Afford 在准确性和泛化能力方面均显著优于现有基线。", "conclusion": "O$^3$Afford 提供了一种有效的单样本3D物体间功能性学习方法，解决了现有方法的局限性，并在有限数据下实现了良好的泛化能力，同时增强了LLMs在机器人操作中理解和推理物体交互的能力。"}}
{"id": "2509.05543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05543", "abs": "https://arxiv.org/abs/2509.05543", "authors": ["Haitao Tian", "Pierre Payeur"], "title": "DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation", "comment": "ICCV 2025 accepted paper", "summary": "In this paper, a contrastive representation learning framework is proposed to\nenhance human action segmentation via pre-training using trimmed (single\naction) skeleton sequences. Unlike previous representation learning works that\nare tailored for action recognition and that build upon isolated sequence-wise\nrepresentations, the proposed framework focuses on exploiting multi-scale\nrepresentations in conjunction with cross-sequence variations. More\nspecifically, it proposes a novel data augmentation strategy, 'Shuffle and\nWarp', which exploits diverse multi-action permutations. The latter effectively\nassists two surrogate tasks that are introduced in contrastive learning: Cross\nPermutation Contrasting (CPC) and Relative Order Reasoning (ROR). In\noptimization, CPC learns intra-class similarities by contrasting\nrepresentations of the same action class across different permutations, while\nROR reasons about inter-class contexts by predicting relative mapping between\ntwo permutations. Together, these tasks enable a Dual-Surrogate Contrastive\nLearning (DuoCLR) network to learn multi-scale feature representations\noptimized for action segmentation. In experiments, DuoCLR is pre-trained on a\ntrimmed skeleton dataset and evaluated on an untrimmed dataset where it\ndemonstrates a significant boost over state-the-art comparatives in both\nmulti-class and multi-label action segmentation tasks. Lastly, ablation studies\nare conducted to evaluate the effectiveness of each component of the proposed\napproach.", "AI": {"tldr": "本文提出了一种名为DuoCLR的对比表示学习框架，通过使用修剪（单一动作）骨架序列进行预训练，以增强人体动作分割。该框架结合多尺度表示和跨序列变异，并引入了“Shuffle and Warp”数据增强策略以及两个代理任务（CPC和ROR），在多类别和多标签动作分割任务中显著优于现有技术。", "motivation": "以往的表示学习方法主要针对动作识别，且基于孤立的序列级表示。然而，动作分割任务需要利用多尺度表示和跨序列变异来捕捉更丰富的上下文信息，因此需要一种新的框架来解决这一挑战。", "method": "本文提出了Dual-Surrogate Contrastive Learning (DuoCLR) 网络。其核心方法包括：1) 引入“Shuffle and Warp”数据增强策略，利用多样化的多动作排列；2) 设计两个代理任务：Cross Permutation Contrasting (CPC) 用于通过对比不同排列中相同动作类别的表示来学习类内相似性；Relative Order Reasoning (ROR) 通过预测两个排列之间的相对映射来推断类间上下文；3) 将这些任务整合到对比学习框架中，以学习针对动作分割优化的多尺度特征表示。", "result": "DuoCLR在修剪过的骨架数据集上进行预训练，并在未修剪的数据集上进行评估。实验结果表明，在多类别和多标签动作分割任务中，DuoCLR相比最先进的对比方法取得了显著提升。此外，消融研究也证实了所提出方法中每个组件的有效性。", "conclusion": "所提出的DuoCLR对比表示学习框架，通过其独特的多尺度表示、跨序列变异利用、创新的“Shuffle and Warp”数据增强以及双代理任务（CPC和ROR），能够有效地学习用于动作分割的特征表示。这显著提高了人体动作分割的性能，超越了现有最先进的方法。"}}
{"id": "2509.06278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06278", "abs": "https://arxiv.org/abs/2509.06278", "authors": ["Chuang Jiang", "Mingyue Cheng", "Xiaoyu Tao", "Qingyang Mao", "Jie Ouyang", "Qi Liu"], "title": "TableMind: An Autonomous Programmatic Agent for Tool-Augmented Table Reasoning", "comment": "Comments: 10 pages, 6 figures. Submitted to WSDM 2026", "summary": "Table reasoning is crucial for leveraging structured data in domains such as\nfinance, healthcare, and scientific research. While large language models\n(LLMs) show promise in multi-step reasoning, purely text-based methods often\nstruggle with the complex numerical computations and fine-grained operations\ninherently required in this task. Tool-integrated reasoning improves\ncomputational accuracy via explicit code execution, yet existing systems\nfrequently rely on rigid patterns, supervised imitation, and lack true\nautonomous adaptability. In this paper, we present TableMind, an LLM-driven\ntable reasoning agent that (i) autonomously performs multi-turn tool\ninvocation, (ii) writes and executes data-analyzing code in a secure sandbox\nenvironment for data analysis and precise numerical reasoning, and (iii)\nexhibits high-level capabilities such as planning and self-reflection to adapt\nstrategies. To realize these capabilities, we adopt a two-stage fine-tuning\nparadigm built on top of a powerful pre-trained language model: supervised\nfine-tuning on high-quality reasoning trajectories to establish effective tool\nusage patterns, followed by reinforcement fine-tuning to optimize\nmulti-objective strategies. In particular, we propose Rank-Aware Policy\nOptimization (RAPO), which increases the update weight of high-quality\ntrajectories when their output probabilities are lower than those of\nlow-quality ones, thereby guiding the model more consistently toward better and\nmore accurate answers. Extensive experiments on several mainstream benchmarks\ndemonstrate that TableMind achieves superior performance compared to\ncompetitive baselines, yielding substantial gains in both reasoning accuracy\nand computational precision.", "AI": {"tldr": "TableMind是一个由LLM驱动的表格推理代理，它通过多轮工具调用、安全沙盒中的代码执行以及规划和自我反思能力，解决了传统方法在复杂数值计算和自适应性方面的不足，并在基准测试中取得了卓越的推理准确性和计算精度。", "motivation": "表格推理对利用结构化数据至关重要，但纯文本的LLM在复杂数值计算和精细操作上表现不佳。现有的工具集成推理系统虽然提高了计算准确性，但往往依赖僵化的模式和监督模仿，缺乏真正的自主适应性。", "method": "本文提出了TableMind，一个LLM驱动的表格推理代理，它能够(i)自主执行多轮工具调用，(ii)在安全沙盒环境中编写和执行数据分析代码以进行精确数值推理，以及(iii)展现规划和自我反思等高级能力以调整策略。实现这些能力的方法是两阶段微调范式：首先对高质量推理轨迹进行监督微调（SFT）以建立有效的工具使用模式，然后进行强化微调（RFT）以优化多目标策略。特别是，提出了一种“排名感知策略优化”（RAPO）方法，该方法在高质量轨迹的输出概率低于低质量轨迹时增加其更新权重，从而更一致地引导模型获得更好、更准确的答案。", "result": "在多个主流基准测试中，TableMind与具有竞争力的基线相比，实现了卓越的性能，在推理准确性和计算精度方面均获得了显著提升。", "conclusion": "TableMind通过结合LLM的推理能力、自主工具调用、安全代码执行以及创新的两阶段微调和RAPO方法，成功克服了表格推理中的数值计算和自适应性挑战，从而在复杂表格数据分析中取得了高水平的准确性和精确性。"}}
{"id": "2509.05908", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.05908", "abs": "https://arxiv.org/abs/2509.05908", "authors": ["Yue Gu", "Zhihao Du", "Ying Shi", "Shiliang Zhang", "Qian Chen", "Jiqing Han"], "title": "Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling", "comment": "Accepted by IEEE Transactions on Audio, Speech and Language\n  Processing, 2025 (https://ieeexplore.ieee.org/document/11150731). DOI:\n  10.1109/TASLPRO.2025.3606198", "summary": "Recently, cross-attention-based contextual automatic speech recognition (ASR)\nmodels have made notable advancements in recognizing personalized biasing\nphrases. However, the effectiveness of cross-attention is affected by\nvariations in biasing information volume, especially when the length of the\nbiasing list increases significantly. We find that, regardless of the length of\nthe biasing list, only a limited amount of biasing information is most relevant\nto a specific ASR intermediate representation. Therefore, by identifying and\nintegrating the most relevant biasing information rather than the entire\nbiasing list, we can alleviate the effects of variations in biasing information\nvolume for contextual ASR. To this end, we propose a purified semantic\ncorrelation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and\ncalculate three semantic correlations between the ASR intermediate\nrepresentations and biasing information from coarse to fine: list-level,\nphrase-level, and token-level. Then, the three correlations are jointly modeled\nto produce their intersection, so that the most relevant biasing information\nacross various granularities is highlighted and integrated for contextual\nrecognition. In addition, to reduce the computational cost introduced by the\njoint modeling of three semantic correlations, we also propose a purification\nmechanism based on a grouped-and-competitive strategy to filter out irrelevant\nbiasing phrases. Compared with baselines, our PSC-Joint approach achieves\naverage relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46%\non KeSpeech, across biasing lists of varying lengths.", "AI": {"tldr": "本文提出了一种名为PSC-Joint的上下文ASR方法，通过识别和整合最相关的偏置信息，而非整个偏置列表，解决了现有跨注意力模型在偏置信息量变化（特别是长列表）时效果不佳的问题，显著提升了识别性能。", "motivation": "基于跨注意力的上下文ASR模型在识别个性化偏置短语方面取得了进展，但其有效性受偏置信息量变化（尤其是偏置列表长度显著增加时）的影响。研究发现，无论列表长度如何，只有有限的偏置信息与特定的ASR中间表示最相关，因此需要一种方法来识别并整合这些最相关的信息。", "method": "本文提出了纯化语义关联联合建模（PSC-Joint）方法。该方法定义并计算了ASR中间表示与偏置信息之间从粗到细的三种语义关联：列表级、短语级和令牌级。然后，联合建模这三种关联以产生它们的交集，从而突出并整合跨不同粒度的最相关偏置信息。此外，为降低联合建模带来的计算成本，还引入了一种基于分组竞争策略的纯化机制来过滤不相关的偏置短语。", "result": "与基线相比，PSC-Joint方法在不同长度偏置列表上，于AISHELL-1数据集上实现了高达21.34%的F1分数相对提升，在KeSpeech数据集上实现了高达28.46%的F1分数相对提升。", "conclusion": "PSC-Joint方法通过识别和整合最相关的偏置信息，有效缓解了偏置信息量变化对上下文ASR效果的影响，显著提高了模型在不同长度偏置列表下的识别性能。"}}
{"id": "2509.06285", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06285", "abs": "https://arxiv.org/abs/2509.06285", "authors": ["Xiangcheng Hu", "Xieyuanli Chen", "Mingkai Jia", "Jin Wu", "Ping Tan", "Steven L. Waslander"], "title": "DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration", "comment": "24 pages, 19 figures, 9 tables", "summary": "LiDAR point cloud registration is fundamental to robotic perception and\nnavigation. However, in geometrically degenerate or narrow environments,\nregistration problems become ill-conditioned, leading to unstable solutions and\ndegraded accuracy. While existing approaches attempt to handle these issues,\nthey fail to address the core challenge: accurately detection, interpret, and\nresolve this ill-conditioning, leading to missed detections or corrupted\nsolutions. In this study, we introduce DCReg, a principled framework that\nsystematically addresses the ill-conditioned registration problems through\nthree integrated innovations. First, DCReg achieves reliable ill-conditioning\ndetection by employing a Schur complement decomposition to the hessian matrix.\nThis technique decouples the registration problem into clean rotational and\ntranslational subspaces, eliminating coupling effects that mask degeneracy\npatterns in conventional analyses. Second, within these cleanly subspaces, we\ndevelop quantitative characterization techniques that establish explicit\nmappings between mathematical eigenspaces and physical motion directions,\nproviding actionable insights about which specific motions lack constraints.\nFinally, leveraging this clean subspace, we design a targeted mitigation\nstrategy: a novel preconditioner that selectively stabilizes only the\nidentified ill-conditioned directions while preserving all well-constrained\ninformation in observable space. This enables efficient and robust optimization\nvia the Preconditioned Conjugate Gradient method with a single physical\ninterpretable parameter. Extensive experiments demonstrate DCReg achieves at\nleast 20% - 50% improvement in localization accuracy and 5-100 times speedup\nover state-of-the-art methods across diverse environments. Our implementation\nwill be available at https://github.com/JokerJohn/DCReg.", "AI": {"tldr": "DCReg是一个针对LiDAR点云配准中病态问题的框架，通过舒尔补分解、定量表征和新型预处理器，实现可靠的病态检测、解释和定向缓解，显著提升了定位精度和速度。", "motivation": "在几何退化或狭窄环境中，LiDAR点云配准问题容易出现病态，导致解决方案不稳定和精度下降。现有方法未能准确检测、解释和有效解决这些病态问题。", "method": "DCReg框架包含三项创新：1. **病态检测**：通过对Hessian矩阵进行舒尔补分解，将配准问题解耦为纯旋转和平移子空间，从而可靠地检测病态。2. **定量表征**：在解耦的子空间中，开发定量表征技术，建立数学特征空间与物理运动方向之间的明确映射，以识别缺乏约束的具体运动。3. **目标缓解**：设计一种新型预处理器，仅选择性地稳定已识别的病态方向，同时保留良好约束的信息，通过预处理共轭梯度法（PCG）实现高效鲁棒优化。", "result": "实验表明，DCReg在各种环境下，定位精度比现有最先进方法提高了至少20%-50%，速度提高了5-100倍。", "conclusion": "DCReg提供了一个系统性解决病态LiDAR点云配准问题的原则性框架，通过其集成的检测、表征和缓解策略，显著提高了配准的准确性和效率。"}}
{"id": "2509.05554", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05554", "abs": "https://arxiv.org/abs/2509.05554", "authors": ["Yihong Leng", "Siming Zheng", "Jinwei Chen", "Bo Li", "Jiaojiao Li", "Peng-Tao Jiang"], "title": "RED: Robust Event-Guided Motion Deblurring with Modality-Specific Disentangled Representation", "comment": null, "summary": "Event cameras provide sparse yet temporally high-temporal-resolution motion\ninformation, demonstrating great potential for motion deblurring. Existing\nmethods focus on cross-modal interaction, overlooking the inherent\nincompleteness of event streams, which arises from the trade-off between\nsensitivity and noise introduced by the thresholding mechanism of Dynamic\nVision Sensors (DVS). Such degradation compromises the integrity of motion\npriors and limits the effectiveness of event-guided deblurring. To tackle these\nchallenges, we propose a Robust Event-guided Deblurring (RED) network with\nmodality-specific disentangled representation. First, we introduce a\nRobustness-Oriented Perturbation Strategy (RPS) that applies random masking to\nevents, which exposes RED to incomplete patterns and then foster robustness\nagainst various unknown scenario conditions.Next, a disentangled OmniAttention\nis presented to explicitly model intra-motion, inter-motion, and cross-modality\ncorrelations from two inherently distinct but complementary sources: blurry\nimages and partially disrupted events. Building on these reliable features, two\ninteractive modules are designed to enhance motion-sensitive areas in blurry\nimages and inject semantic context into incomplete event representations.\nExtensive experiments on synthetic and real-world datasets demonstrate RED\nconsistently achieves state-of-the-art performance in both accuracy and\nrobustness.", "AI": {"tldr": "本文提出了一种名为RED的鲁棒事件引导去模糊网络，通过解耦表示和新颖的扰动策略，有效解决了事件流固有的不完整性问题，显著提升了去模糊的准确性和鲁棒性。", "motivation": "事件相机提供高时间分辨率的稀疏运动信息，在运动去模糊方面潜力巨大。然而，现有的方法忽略了事件流固有的不完整性，这源于动态视觉传感器（DVS）阈值机制在灵敏度和噪声之间的权衡。这种退化损害了运动先验的完整性，限制了事件引导去模糊的有效性。", "method": "本文提出了一个具有模态特定解耦表示的鲁棒事件引导去模糊（RED）网络。首先，引入了面向鲁棒性的扰动策略（RPS），通过对事件应用随机遮蔽，使RED暴露于不完整的模式，从而增强对未知场景条件的鲁棒性。其次，提出了一个解耦的全向注意力（OmniAttention）模块，用于明确建模模糊图像和部分中断事件中固有的内部运动、间运动和跨模态相关性。在此基础上，设计了两个交互模块，分别用于增强模糊图像中的运动敏感区域，并向不完整的事件表示注入语义上下文。", "result": "在合成和真实世界数据集上进行的广泛实验表明，RED在准确性和鲁棒性方面均持续达到了最先进的性能。", "conclusion": "RED网络通过创新的扰动策略和解耦的注意力机制，成功解决了事件流不完整性对运动去模糊的限制，实现了卓越的去模糊效果和鲁棒性。"}}
{"id": "2509.06283", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06283", "abs": "https://arxiv.org/abs/2509.06283", "authors": ["Xuan-Phi Nguyen", "Shrey Pandit", "Revanth Gangi Reddy", "Austin Xu", "Silvio Savarese", "Caiming Xiong", "Shafiq Joty"], "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for Autonomously Reasoning Single Agents", "comment": "Technical Report", "summary": "Equipping large language models (LLMs) with complex, interleaved reasoning\nand tool-use capabilities has become a key focus in agentic AI research,\nespecially with recent advances in reasoning-oriented (``thinking'') models.\nSuch capabilities are key to unlocking a number of important applications. One\nsuch application is Deep Research (DR), which requires extensive search and\nreasoning over many sources. Our work in this paper focuses on the development\nof native Autonomous Single-Agent models for DR featuring minimal web crawling\nand Python tool integration. Unlike multi-agent systems, where agents take up\npre-defined roles and are told what to do at each step in a static workflow, an\nautonomous single-agent determines its next action dynamically based on\ncontext, without manual directive. While prior work has proposed training\nrecipes for base or instruction-tuned LLMs, we focus on continual reinforcement\nlearning (RL) of reasoning-optimized models to further enhance agentic skills\nwhile preserving reasoning ability. Towards this end, we propose a simple RL\nrecipe with entirely synthetic data, which we apply to various open-source\nLLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam\nbenchmark. In addition, we conduct key analysis experiments to provide more\ninsights into our methodologies.", "AI": {"tldr": "本文提出了一种基于持续强化学习的自主单智能体模型，用于深度研究（DR），该模型利用完全合成数据训练，旨在增强智能体技能并保持推理能力，并在相关基准测试中取得了显著表现。", "motivation": "大型语言模型（LLMs）的复杂推理和工具使用能力对于许多重要应用至关重要，特别是需要大量搜索和推理的深度研究（DR）。现有工作多关注基础或指令微调的LLM，但仍需进一步提升智能体的自主决策能力和工具使用技能，同时保留其核心推理能力。", "method": "本研究专注于开发用于DR的原生自主单智能体模型，该模型集成了最小化的网络爬取和Python工具。与多智能体系统不同，该单智能体能根据上下文动态决定下一步行动，无需手动指令。研究采用了一种简单的、基于完全合成数据的持续强化学习（RL）配方，并将其应用于各种推理优化的开源LLM，以在保持推理能力的同时增强智能体技能。", "result": "研究的最佳变体SFR-DR-20B在“人类的最后考试”（Humanity's Last Exam）基准测试中取得了高达28.7%的成绩。此外，研究还进行了关键分析实验，为所提出的方法提供了更深入的见解。", "conclusion": "通过对推理优化模型进行持续强化学习，并利用合成数据，可以有效增强大型语言模型在深度研究任务中的自主单智能体能力，提升其代理技能和工具使用能力，同时成功保持了其推理能力。这一方法为开发更强大的代理式AI提供了新的途径。"}}
{"id": "2509.05915", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.05915", "abs": "https://arxiv.org/abs/2509.05915", "authors": ["Sangmin Bae"], "title": "Accelerating Large Language Model Inference via Early-Exiting Algorithms", "comment": "PhD Dissertation", "summary": "Large language models have achieved remarkable capabilities, but their\npractical deployment is hindered by significant computational costs. While\nadaptive computation methods like early-exiting promise to reduce these costs,\nthey introduce a fundamental conflict: the per-token dynamism intended to save\ncomputation often creates system-level bottlenecks that can paradoxically\nreduce throughput in batched inference. This dissertation resolves this\nconflict by co-designing adaptive algorithms and model architectures to strike\nan optimal balance between dynamism and efficiency. To this end, our work first\naddresses critical sources of overhead in conventional early-exiting by\nproposing an efficient parallel decoding mechanism. We then show that deep\nparameter sharing provides an architectural foundation that not only yields\ncompact, parameter-efficient models but also inherently mitigates the critical\nsynchronization issues affecting dynamic inference. Finally, this work presents\na unified framework where lightweight routers are pretrained to dynamically\nassign an optimal recursion depth for each token. This approach establishes a\nnew Pareto frontier between efficiency and performance by effectively\noptimizing for both adaptive computation and parameter efficiency within a\nsingle model.", "AI": {"tldr": "本论文通过协同设计自适应算法和模型架构，解决了大型语言模型中自适应计算（如提前退出）带来的计算成本与批处理推理吞吐量之间的冲突，从而提高了模型的效率和性能。", "motivation": "大型语言模型（LLMs）的部署受限于其高昂的计算成本。虽然自适应计算方法（如提前退出）旨在降低成本，但其逐令牌的动态性在批处理推理中可能引入系统级瓶颈，反而降低吞吐量。", "method": "1. 提出了一种高效的并行解码机制，以解决传统提前退出中的开销问题。2. 引入深度参数共享作为架构基础，不仅实现参数高效的模型，还缓解了动态推理中的同步问题。3. 提出了一个统一框架，其中轻量级路由器经过预训练，可为每个令牌动态分配最佳递归深度。", "result": "通过在一个模型中同时优化自适应计算和参数效率，本研究在效率和性能之间建立了一个新的帕累托前沿。", "conclusion": "通过协同设计自适应算法和模型架构，可以有效地解决大型语言模型中动态性与效率之间的根本冲突，从而实现更实用、更高效的模型部署。"}}
{"id": "2509.06296", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06296", "abs": "https://arxiv.org/abs/2509.06296", "authors": ["Francisco Affonso", "Felipe Andrade G. Tommaselli", "Juliano Negri", "Vivian S. Medeiros", "Mateus V. Gasparino", "Girish Chowdhary", "Marcelo Becker"], "title": "Learning to Walk with Less: a Dyna-Style Approach to Quadrupedal Locomotion", "comment": "Under review at IEEE Robotics and Automation Letters. 8 pages", "summary": "Traditional RL-based locomotion controllers often suffer from low data\nefficiency, requiring extensive interaction to achieve robust performance. We\npresent a model-based reinforcement learning (MBRL) framework that improves\nsample efficiency for quadrupedal locomotion by appending synthetic data to the\nend of standard rollouts in PPO-based controllers, following the Dyna-Style\nparadigm. A predictive model, trained alongside the policy, generates\nshort-horizon synthetic transitions that are gradually integrated using a\nscheduling strategy based on the policy update iterations. Through an ablation\nstudy, we identified a strong correlation between sample efficiency and rollout\nlength, which guided the design of our experiments. We validated our approach\nin simulation on the Unitree Go1 robot and showed that replacing part of the\nsimulated steps with synthetic ones not only mimics extended rollouts but also\nimproves policy return and reduces variance. Finally, we demonstrate that this\nimprovement transfers to the ability to track a wide range of locomotion\ncommands using fewer simulated steps.", "AI": {"tldr": "本文提出了一种基于模型的强化学习（MBRL）框架，通过在PPO的rollout中添加合成数据来提高四足机器人运动控制器的样本效率，从而改善策略表现并减少方差。", "motivation": "传统的基于强化学习的运动控制器（如PPO）通常数据效率低下，需要大量的交互才能达到鲁棒的性能。", "method": "本文采用了一种Dyna-Style的MBRL框架。一个与策略并行训练的预测模型生成短期的合成转换数据，并使用基于策略更新迭代的调度策略逐步将这些数据整合到PPO的rollout中。通过消融研究，作者发现样本效率与rollout长度之间存在强相关性，并以此指导实验设计。", "result": "在Unitree Go1机器人的仿真中验证了该方法。结果表明，用合成数据替代部分仿真步骤不仅能模拟更长的rollout，还能提高策略回报并减少方差。此外，这种改进使得机器人能够以更少的仿真步骤跟踪更广泛的运动指令。", "conclusion": "该MBRL框架通过有效地利用合成数据，显著提高了四足机器人运动控制的样本效率和性能，使其能够以更少的仿真交互实现更优的策略学习和指令跟踪能力。"}}
{"id": "2509.05576", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05576", "abs": "https://arxiv.org/abs/2509.05576", "authors": ["Zekang Zheng", "Haokun Li", "Yaofo Chen", "Mingkui Tan", "Qing Du"], "title": "Sensitivity-Aware Post-Training Quantization for Deep Neural Networks", "comment": "Accepted by PRCV 2025", "summary": "Model quantization reduces neural network parameter precision to achieve\ncompression, but often compromises accuracy. Existing post-training\nquantization (PTQ) methods employ iterative parameter updates to preserve\naccuracy under high compression ratios, incurring significant computational\ncomplexity and resource overhead, which limits applicability in\nresource-constrained edge computing and real-time inference scenarios. This\npaper proposes an efficient PTQ method guided by parameter sensitivity\nanalysis. The approach prioritizes quantization of high-sensitivity parameters,\nleveraging unquantized low-sensitivity parameters to compensate for\nquantization errors, thereby mitigating accuracy degradation. Furthermore, by\nexploiting column-wise clustering of parameter sensitivity, the method\nintroduces a row-parallel quantization framework with a globally shared inverse\nHessian matrix update mechanism, reducing computational complexity by an order\nof magnitude. Experimental results on ResNet-50 and YOLOv5s demonstrate a\n20-200-fold quantization speedup over the Optimal Brain Quantization baseline,\nwith mean accuracy loss below 0.3%, confirming the method's efficacy in\nbalancing efficiency and accuracy.", "AI": {"tldr": "本文提出了一种高效的后训练量化（PTQ）方法，通过参数敏感度分析指导量化过程，并结合行并行量化框架和共享逆Hessian矩阵更新机制，显著提高了量化速度，同时保持了高精度。", "motivation": "现有的PTQ方法在实现高压缩比时，通常需要迭代参数更新，导致计算复杂度和资源开销巨大，限制了其在资源受限的边缘计算和实时推理场景中的应用。", "method": "该方法首先通过参数敏感度分析，优先量化高敏感度参数，并利用未量化的低敏感度参数补偿量化误差，以减轻精度下降。此外，通过利用参数敏感度的列式聚类，引入了一个行并行量化框架，并采用全局共享的逆Hessian矩阵更新机制，从而将计算复杂度降低了一个数量级。", "result": "在ResNet-50和YOLOv5s上的实验结果表明，与Optimal Brain Quantization基线相比，量化速度提高了20-200倍，平均精度损失低于0.3%。", "conclusion": "该方法在效率和精度之间取得了有效平衡，验证了其在加速量化过程同时保持模型性能的有效性。"}}
{"id": "2509.06284", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06284", "abs": "https://arxiv.org/abs/2509.06284", "authors": ["Jiaxiang Chen", "Zhuo Wang", "Mingxi Zou", "Zhucong Li", "Zhijian Zhou", "Song Wang", "Zenglin Xu"], "title": "From Implicit Exploration to Structured Reasoning: Leveraging Guideline and Refinement for LLMs", "comment": null, "summary": "Large language models (LLMs) have advanced general-purpose reasoning, showing\nstrong performance across diverse tasks. However, existing methods often rely\non implicit exploration, where the model follows stochastic and unguided\nreasoning paths-like walking without a map. This leads to unstable reasoning\npaths, lack of error correction, and limited learning from past experience. To\naddress these issues, we propose a framework that shifts from implicit\nexploration to structured reasoning through guideline and refinement. First, we\nextract structured reasoning patterns from successful trajectories and\nreflective signals from failures. During inference, the model follows these\nguidelines step-by-step, with refinement applied after each step to correct\nerrors and stabilize the reasoning process. Experiments on BBH and four\nadditional benchmarks (GSM8K, MATH-500, MBPP, HumanEval) show that our method\nconsistently outperforms strong baselines across diverse reasoning tasks.\nStructured reasoning with stepwise execution and refinement improves stability\nand generalization, while guidelines transfer well across domains and flexibly\nsupport cross-model collaboration, matching or surpassing supervised\nfine-tuning in effectiveness and scalability.", "AI": {"tldr": "本文提出了一种名为“结构化推理”的框架，通过从成功和失败经验中提取指导方针和进行逐步修正，显著提升了大型语言模型在各种推理任务上的稳定性、泛化能力和性能。", "motivation": "现有的大型语言模型（LLMs）在推理时常依赖隐式探索，导致推理路径不稳定、缺乏错误纠正能力以及从过往经验中学习不足。", "method": "该方法通过以下步骤实现：1. 从成功的推理轨迹中提取结构化推理模式。2. 从失败中提取反思信号。3. 在推理过程中，模型遵循这些逐步的指导方针。4. 每一步之后应用修正机制，以纠正错误并稳定推理过程。", "result": "实验结果表明，该方法在BBH以及GSM8K、MATH-500、MBPP、HumanEval等四个额外基准测试中，持续优于强基线模型。结构化推理与逐步执行和修正相结合，提高了稳定性和泛化能力，指导方针在不同领域间具有良好的迁移性，并灵活支持跨模型协作，其有效性和可扩展性媲美甚至超越了监督微调。", "conclusion": "通过指导方针和逐步修正实现的结构化推理，能够有效解决LLM推理的稳定性问题，提升其在多领域任务上的表现，并具有出色的泛化、迁移和可扩展性。"}}
{"id": "2509.06065", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06065", "abs": "https://arxiv.org/abs/2509.06065", "authors": ["Lorenzo Alfred Nery", "Ronald Dawson Catignas", "Thomas James Tiam-Lee"], "title": "KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino", "comment": "14 pages, 1 figure, 9 tables, 1 listing. To appear in Proceedings of\n  NLPIR 2025", "summary": "Large Language Models (LLMs) achieve remarkable performance across various\ntasks, but their tendency to produce hallucinations limits reliable adoption.\nBenchmarks such as TruthfulQA have been developed to measure truthfulness, yet\nthey are primarily available in English, leaving a gap in evaluating LLMs in\nlow-resource languages. To address this, we present KatotohananQA, a Filipino\ntranslation of the TruthfulQA benchmark. Seven free-tier proprietary models\nwere assessed using a binary-choice framework. Findings show a significant\nperformance gap between English and Filipino truthfulness, with newer OpenAI\nmodels (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness.\nResults also reveal disparities across question characteristics, suggesting\nthat some question types, categories, and topics are less robust to\nmultilingual transfer which highlight the need for broader multilingual\nevaluation to ensure fairness and reliability in LLM usage.", "AI": {"tldr": "该研究引入了TruthfulQA的菲律宾语翻译版本KatotohananQA，以评估大型语言模型在低资源语言中的真实性。结果显示英语和菲律宾语真实性之间存在显著性能差距，并强调了多语言评估的重要性。", "motivation": "大型语言模型（LLMs）的幻觉倾向限制了其可靠应用。现有真实性基准（如TruthfulQA）主要为英文，导致在低资源语言中评估LLMs存在空白。", "method": "将TruthfulQA基准翻译成菲律宾语，创建了KatotohananQA。使用二元选择框架评估了七个免费专有模型。", "result": "研究发现英语和菲律宾语真实性之间存在显著的性能差距。较新的OpenAI模型（GPT-5和GPT-5 mini）表现出强大的多语言鲁棒性。结果还揭示了不同问题特征之间的差异，表明某些问题类型、类别和主题对多语言迁移的鲁棒性较差。", "conclusion": "为了确保LLM使用的公平性和可靠性，需要进行更广泛的多语言评估。"}}
{"id": "2509.06342", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06342", "abs": "https://arxiv.org/abs/2509.06342", "authors": ["Filip Bjelonic", "Fabian Tischhauser", "Marco Hutter"], "title": "Towards bridging the gap: Systematic sim-to-real transfer for diverse legged robots", "comment": "Submitted to The International Journal of Robotics Research (IJRR),\n  25 Figures, 7 Tables, Open Source Data available at ETH Research Collection.\n  Open Source software available soon", "summary": "Legged robots must achieve both robust locomotion and energy efficiency to be\npractical in real-world environments. Yet controllers trained in simulation\noften fail to transfer reliably, and most existing approaches neglect\nactuator-specific energy losses or depend on complex, hand-tuned reward\nformulations. We propose a framework that integrates sim-to-real reinforcement\nlearning with a physics-grounded energy model for permanent magnet synchronous\nmotors. The framework requires a minimal parameter set to capture the\nsimulation-to-reality gap and employs a compact four-term reward with a\nfirst-principle-based energetic loss formulation that balances electrical and\nmechanical dissipation. We evaluate and validate the approach through a\nbottom-up dynamic parameter identification study, spanning actuators,\nfull-robot in-air trajectories and on-ground locomotion. The framework is\ntested on three primary platforms and deployed on ten additional robots,\ndemonstrating reliable policy transfer without randomization of dynamic\nparameters. Our method improves energetic efficiency over state-of-the-art\nmethods, achieving a 32 percent reduction in the full Cost of Transport of\nANYmal (value 1.27). All code, models, and datasets will be released.", "AI": {"tldr": "本文提出了一种结合仿真到现实强化学习与永磁同步电机物理能量模型的框架，实现了腿足机器人鲁棒且节能的运动，并通过最小参数集和基于第一性原理的奖励函数，有效弥补了仿真与现实差距，显著提高了能效。", "motivation": "现有仿真训练的控制器难以可靠地迁移到现实世界；大多数方法忽略了执行器特定的能量损耗，或依赖于复杂且需手动调整的奖励函数，这些都阻碍了腿足机器人在实际环境中的应用。", "method": "该框架将仿真到现实的强化学习与永磁同步电机的物理能量模型相结合。它使用最少的参数集来弥补仿真与现实之间的差距，并采用一个紧凑的四项奖励函数，其中包含基于第一性原理的能量损耗公式，平衡了电能和机械能耗散。通过自底向上的动态参数识别研究（涵盖执行器、机器人空中轨迹和地面运动）来评估和验证该方法。", "result": "该框架在无需动态参数随机化的情况下，实现了可靠的策略迁移。在三个主要平台进行了测试，并部署到十个额外机器人上。与现有最先进的方法相比，该方法提高了能量效率，使ANYmal的完整运输成本（Cost of Transport）降低了32%（值为1.27）。", "conclusion": "所提出的框架通过集成物理能量模型和精简奖励函数，克服了仿真到现实迁移的挑战，为腿足机器人实现了鲁棒、可靠且显著节能的运动控制。"}}
{"id": "2509.05582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05582", "abs": "https://arxiv.org/abs/2509.05582", "authors": ["Zhiling Ye", "Cong Zhou", "Xiubao Zhang", "Haifeng Shen", "Weihong Deng", "Quan Lu"], "title": "Reconstruction and Reenactment Separated Method for Realistic Gaussian Head", "comment": null, "summary": "In this paper, we explore a reconstruction and reenactment separated\nframework for 3D Gaussians head, which requires only a single portrait image as\ninput to generate controllable avatar. Specifically, we developed a large-scale\none-shot gaussian head generator built upon WebSSL and employed a two-stage\ntraining approach that significantly enhances the capabilities of\ngeneralization and high-frequency texture reconstruction. During inference, an\nultra-lightweight gaussian avatar driven by control signals enables high\nframe-rate rendering, achieving 90 FPS at a resolution of 512x512. We further\ndemonstrate that the proposed framework follows the scaling law, whereby\nincreasing the parameter scale of the reconstruction module leads to improved\nperformance. Moreover, thanks to the separation design, driving efficiency\nremains unaffected. Finally, extensive quantitative and qualitative experiments\nvalidate that our approach outperforms current state-of-the-art methods.", "AI": {"tldr": "本文提出一个分离的3D高斯头重建与重演框架，仅需单张肖像图即可生成可控的3D头像，实现高帧率渲染并超越现有SOTA方法。", "motivation": "从单张肖像图像生成可控的3D头像，同时解决泛化能力和高频纹理重建的挑战。", "method": "采用重建与重演分离的框架；开发基于WebSSL的大规模一次性高斯头生成器；使用两阶段训练方法；推理时利用超轻量级高斯头像由控制信号驱动。", "result": "实现512x512分辨率下90 FPS的高帧率渲染；框架遵循缩放定律，增加重建模块参数可提升性能，且驱动效率不受影响；量化和定性实验表明该方法优于当前最先进的方法。", "conclusion": "所提出的分离框架能有效从单张图像重建和重演3D高斯头，具有出色的泛化、高频纹理重建能力、高渲染效率和可扩展性，并显著优于现有技术。"}}
{"id": "2509.06307", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06307", "abs": "https://arxiv.org/abs/2509.06307", "authors": ["Lei Shu", "Dong Zhao"], "title": "Can AI Make Energy Retrofit Decisions? An Evaluation of Large Language Models", "comment": null, "summary": "Conventional approaches to building energy retrofit decision making suffer\nfrom limited generalizability and low interpretability, hindering adoption in\ndiverse residential contexts. With the growth of Smart and Connected\nCommunities, generative AI, especially large language models (LLMs), may help\nby processing contextual information and producing practitioner readable\nrecommendations. We evaluate seven LLMs (ChatGPT, DeepSeek, Gemini, Grok,\nLlama, and Claude) on residential retrofit decisions under two objectives:\nmaximizing CO2 reduction (technical) and minimizing payback period\n(sociotechnical). Performance is assessed on four dimensions: accuracy,\nconsistency, sensitivity, and reasoning, using a dataset of 400 homes across 49\nUS states. LLMs generate effective recommendations in many cases, reaching up\nto 54.5 percent top 1 match and 92.8 percent within top 5 without fine tuning.\nPerformance is stronger for the technical objective, while sociotechnical\ndecisions are limited by economic trade offs and local context. Agreement\nacross models is low, and higher performing models tend to diverge from others.\nLLMs are sensitive to location and building geometry but less sensitive to\ntechnology and occupant behavior. Most models show step by step, engineering\nstyle reasoning, but it is often simplified and lacks deeper contextual\nawareness. Overall, LLMs are promising assistants for energy retrofit decision\nmaking, but improvements in accuracy, consistency, and context handling are\nneeded for reliable practice.", "AI": {"tldr": "本研究评估了大型语言模型（LLM）在住宅能源改造决策中的应用潜力，发现它们能生成有效建议，但在准确性、一致性和上下文处理方面仍需改进以实现可靠实践。", "motivation": "传统建筑能源改造决策方法存在泛化能力有限和可解释性低的问题，阻碍了其在多样化住宅环境中的应用。随着智能互联社区的发展，生成式AI（特别是LLM）有望通过处理上下文信息并生成易读的建议来克服这些局限性。", "method": "研究评估了七个大型语言模型（ChatGPT、DeepSeek、Gemini、Grok、Llama、Claude）在住宅改造决策中的表现。设定了两个目标：最大化二氧化碳减排（技术目标）和最小化投资回收期（社会技术目标）。使用包含美国49个州400个家庭的数据集，从准确性、一致性、敏感性和推理四个维度评估模型性能。", "result": "LLM在许多情况下能生成有效建议，未经微调即可达到54.5%的Top 1匹配率和92.8%的Top 5匹配率。技术目标下的表现优于社会技术目标。模型间的一致性较低，表现较好的模型往往与其他模型存在分歧。LLM对地理位置和建筑几何形状敏感，但对技术和居住者行为的敏感度较低。大多数模型展现出简化、工程风格的推理，但缺乏更深层次的上下文感知。", "conclusion": "大型语言模型在能源改造决策中具有广阔前景，可以作为有用的辅助工具。然而，为了实现可靠的实际应用，仍需在准确性、一致性和上下文处理方面进行显著改进。"}}
{"id": "2509.06074", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06074", "abs": "https://arxiv.org/abs/2509.06074", "authors": ["Zhenqi Jia", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "title": "Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis", "comment": "Accepted by EMNLP 2025", "summary": "Conversational Speech Synthesis (CSS) aims to generate speech with natural\nprosody by understanding the multimodal dialogue history (MDH). The latest work\npredicts the accurate prosody expression of the target utterance by modeling\nthe utterance-level interaction characteristics of MDH and the target\nutterance. However, MDH contains fine-grained semantic and prosody knowledge at\nthe word level. Existing methods overlook the fine-grained semantic and\nprosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a\nnovel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our\napproach constructs two specialized multimodal fine-grained dialogue\ninteraction graphs: a semantic interaction graph and a prosody interaction\ngraph. These two interaction graphs effectively encode interactions between\nword-level semantics, prosody, and their influence on subsequent utterances in\nMDH. The encoded interaction features are then leveraged to enhance synthesized\nspeech with natural conversational prosody. Experiments on the DailyTalk\ndataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of\nprosodic expressiveness. Code and speech samples are available at\nhttps://github.com/AI-S2-Lab/MFCIG-CSS.", "AI": {"tldr": "本文提出MFCIG-CSS，一个基于多模态细粒度上下文交互图的对话语音合成系统，通过建模词级语义和韵律交互，显著提升了合成语音的韵律表现力。", "motivation": "现有的对话语音合成（CSS）方法在利用多模态对话历史（MDH）时，忽略了MDH中包含的细粒度词级语义和韵律知识，未能对这些细粒度交互进行有效建模。", "method": "本文提出了MFCIG-CSS系统，通过构建两个专门的多模态细粒度对话交互图：语义交互图和韵律交互图。这些图有效地编码了词级语义、韵律之间的交互及其对后续话语的影响，然后利用编码的交互特征来增强合成语音的自然对话韵律。", "result": "在DailyTalk数据集上的实验表明，MFCIG-CSS在韵律表现力方面优于所有基线模型。", "conclusion": "MFCIG-CSS通过引入细粒度的多模态上下文交互图建模，有效解决了现有CSS方法忽略词级语义和韵律交互的问题，显著提升了对话语音合成的韵律自然度。"}}
{"id": "2509.06375", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06375", "abs": "https://arxiv.org/abs/2509.06375", "authors": ["Fujiang Yuan", "Zhen Tian", "Yangfan He", "Guojian Zou", "Chunhong Yuan", "Yanhong Peng", "Zhihao Lin"], "title": "Adaptive Evolution Factor Risk Ellipse Framework for Reliable and Safe Autonomous Driving", "comment": null, "summary": "In recent years, ensuring safety, efficiency, and comfort in interactive\nautonomous driving has become a critical challenge. Traditional model-based\ntechniques, such as game-theoretic methods and robust control, are often overly\nconservative or computationally intensive. Conversely, learning-based\napproaches typically require extensive training data and frequently exhibit\nlimited interpretability and generalizability. Simpler strategies, such as Risk\nPotential Fields (RPF), provide lightweight alternatives with minimal data\ndemands but are inherently static and struggle to adapt effectively to dynamic\ntraffic conditions. To overcome these limitations, we propose the Evolutionary\nRisk Potential Field (ERPF), a novel approach that dynamically updates risk\nassessments in dynamical scenarios based on historical obstacle proximity data.\nWe introduce a Risk-Ellipse construct that combines longitudinal reach and\nlateral uncertainty into a unified spatial temporal collision envelope.\nAdditionally, we define an adaptive Evolution Factor metric, computed through\nsigmoid normalization of Time to Collision (TTC) and Time-Window-of-Hazard\n(TWH), which dynamically adjusts the dimensions of the ellipse axes in real\ntime. This adaptive risk metric is integrated seamlessly into a Model\nPredictive Control (MPC) framework, enabling autonomous vehicles to proactively\naddress complex interactive driving scenarios in terms of uncertain driving of\nsurrounding vehicles. Comprehensive comparative experiments demonstrate that\nour ERPF-MPC approach consistently achieves smoother trajectories, higher\naverage speeds, and collision-free navigation, offering a robust and adaptive\nsolution suitable for complex interactive driving environments.", "AI": {"tldr": "提出了一种名为进化风险势场（ERPF）的新方法，通过动态更新风险评估并结合风险椭圆和自适应进化因子，将其无缝集成到模型预测控制（MPC）框架中，以实现交互式自动驾驶中更平滑、更快速且无碰撞的导航。", "motivation": "传统基于模型的自动驾驶方法（如博弈论、鲁棒控制）过于保守或计算量大；基于学习的方法需要大量训练数据且解释性、泛化性有限；简单的策略（如风险势场RPF）虽然轻量但静态，难以适应动态交通条件。研究旨在克服这些限制，提供一种更适应动态交互式场景的解决方案。", "method": "该研究提出了进化风险势场（ERPF），基于历史障碍物接近数据动态更新风险评估。引入了“风险椭圆”构造，结合纵向可达性和横向不确定性形成统一的时空碰撞包络。定义了一个自适应的“进化因子”度量，通过碰撞时间（TTC）和危险时间窗（TWH）的Sigmoid归一化计算，实时动态调整椭圆轴的尺寸。最后，将这种自适应风险度量无缝集成到模型预测控制（MPC）框架中。", "result": "全面的对比实验表明，ERPF-MPC方法始终能实现更平滑的轨迹、更高的平均速度和无碰撞导航，为复杂的交互式驾驶环境提供了一个鲁棒且自适应的解决方案。", "conclusion": "ERPF-MPC方法通过动态更新风险评估并结合自适应风险度量，成功克服了传统方法的局限性，为复杂交互式自动驾驶场景提供了一种安全、高效、舒适的解决方案。"}}
{"id": "2509.05592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05592", "abs": "https://arxiv.org/abs/2509.05592", "authors": ["Changtao Miao", "Yi Zhang", "Man Luo", "Weiwei Feng", "Kaiyuan Zheng", "Qi Chu", "Tao Gong", "Jianshu Li", "Yunfeng Diao", "Wei Zhou", "Joey Tianyi Zhou", "Xiaoshuai Hao"], "title": "MFFI: Multi-Dimensional Face Forgery Image Dataset for Real-World Scenarios", "comment": null, "summary": "Rapid advances in Artificial Intelligence Generated Content (AIGC) have\nenabled increasingly sophisticated face forgeries, posing a significant threat\nto social security. However, current Deepfake detection methods are limited by\nconstraints in existing datasets, which lack the diversity necessary in\nreal-world scenarios. Specifically, these data sets fall short in four key\nareas: unknown of advanced forgery techniques, variability of facial scenes,\nrichness of real data, and degradation of real-world propagation. To address\nthese challenges, we propose the Multi-dimensional Face Forgery Image\n(\\textbf{MFFI}) dataset, tailored for real-world scenarios. MFFI enhances\nrealism based on four strategic dimensions: 1) Wider Forgery Methods; 2) Varied\nFacial Scenes; 3) Diversified Authentic Data; 4) Multi-level Degradation\nOperations. MFFI integrates $50$ different forgery methods and contains $1024K$\nimage samples. Benchmark evaluations show that MFFI outperforms existing public\ndatasets in terms of scene complexity, cross-domain generalization capability,\nand detection difficulty gradients. These results validate the technical\nadvance and practical utility of MFFI in simulating real-world conditions. The\ndataset and additional details are publicly available at\n{https://github.com/inclusionConf/MFFI}.", "AI": {"tldr": "本文提出了MFFI数据集，旨在通过涵盖更多伪造方法、多样面部场景、真实数据和多级降级操作，解决现有深度伪造检测数据集中真实世界多样性不足的问题。", "motivation": "AIGC技术快速发展，导致面部伪造日益复杂，对社会安全构成重大威胁。然而，现有的深度伪造检测方法受限于数据集，这些数据集在未知高级伪造技术、面部场景多样性、真实数据丰富性和真实世界传播退化方面存在不足，无法模拟真实世界场景。", "method": "为了应对挑战，作者提出了多维度面部伪造图像（MFFI）数据集。MFFI通过四个战略维度增强真实性：1) 更广泛的伪造方法；2) 多样的面部场景；3) 多样化的真实数据；4) 多级降级操作。MFFI集成了50种不同的伪造方法，包含1024K图像样本。", "result": "基准评估表明，MFFI在场景复杂性、跨域泛化能力和检测难度梯度方面优于现有公共数据集。", "conclusion": "这些结果验证了MFFI在模拟真实世界条件方面的技术进步和实用性，为深度伪造检测提供了更真实、更具挑战性的数据集。"}}
{"id": "2509.06337", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06337", "abs": "https://arxiv.org/abs/2509.06337", "authors": ["Jianpeng Zhao", "Chenyu Yuan", "Weiming Luo", "Haoling Xie", "Guangwei Zhang", "Steven Jige Quan", "Zixuan Yuan", "Pengyang Wang", "Denghui Zhang"], "title": "Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation", "comment": null, "summary": "Questionnaire-based surveys are foundational to social science research and\npublic policymaking, yet traditional survey methods remain costly,\ntime-consuming, and often limited in scale. This paper explores a new paradigm:\nsimulating virtual survey respondents using Large Language Models (LLMs). We\nintroduce two novel simulation settings, namely Partial Attribute Simulation\n(PAS) and Full Attribute Simulation (FAS), to systematically evaluate the\nability of LLMs to generate accurate and demographically coherent responses. In\nPAS, the model predicts missing attributes based on partial respondent\nprofiles, whereas FAS involves generating complete synthetic datasets under\nboth zero-context and context-enhanced conditions. We curate a comprehensive\nbenchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey\nSimulation), that spans 11 real-world public datasets across four sociological\ndomains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA\n3.0/3.1-8B) reveals consistent trends in prediction performance, highlights\nfailure modes, and demonstrates how context and prompt design impact simulation\nfidelity. This work establishes a rigorous foundation for LLM-driven survey\nsimulations, offering scalable and cost-effective tools for sociological\nresearch and policy evaluation. Our code and dataset are available at:\nhttps://github.com/dart-lab-research/LLM-S-Cube-Benchmark", "AI": {"tldr": "本文提出并评估了一种使用大型语言模型（LLMs）模拟虚拟调查受访者的新范式，旨在克服传统调查方法的局限性。", "motivation": "传统的问卷调查方法成本高昂、耗时且规模受限，因此需要一种更具可扩展性和成本效益的替代方案来支持社会科学研究和公共政策制定。", "method": "研究引入了两种新颖的模拟设置：部分属性模拟（PAS）和完整属性模拟（FAS），以评估LLMs生成准确且符合人口统计学特征响应的能力。同时，构建了一个名为LLM-S^3的综合基准套件，包含跨四个社会学领域的11个真实世界公共数据集。研究评估了GPT-3.5/4 Turbo和LLaMA 3.0/3.1-8B等主流LLMs，并分析了上下文和提示设计对模拟保真度的影响。", "result": "评估结果揭示了预测性能的一致趋势，指出了LLMs在模拟中的失效模式，并证明了上下文和提示设计如何显著影响模拟的准确性与真实性。", "conclusion": "这项工作为基于LLM的调查模拟奠定了严格的基础，为社会学研究和政策评估提供了可扩展且经济高效的工具。"}}
{"id": "2509.06079", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06079", "abs": "https://arxiv.org/abs/2509.06079", "authors": ["Hao Liang", "Ruitao Wu", "Bohan Zeng", "Junbo Niu", "Wentao Zhang", "Bin Dong"], "title": "Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge", "comment": null, "summary": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner.", "AI": {"tldr": "本文介绍了一种字幕辅助推理框架，有效连接了视觉和文本模态，在ICML 2025 SeePhys挑战中获得第一名，并在MathVerse几何推理基准上验证了其泛化性。", "motivation": "多模态推理是人工智能中的一个基本挑战。尽管文本推理取得了显著进展，但即使是先进模型在多模态场景中也难以保持强大性能，存在性能差距。", "method": "引入了一种字幕辅助推理框架，该框架通过有效连接视觉和文本模态来弥合现有差距。", "result": "该方法在ICML 2025 AI for Math Workshop & Challenge 2: SeePhys中获得第一名，展示了其有效性和鲁棒性。此外，它还在MathVerse几何推理基准上验证了其泛化性和多功能性。", "conclusion": "所提出的字幕辅助推理框架能够有效、鲁棒且多功能地解决多模态推理问题，成功地连接了视觉和文本模态。"}}
{"id": "2509.06404", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06404", "abs": "https://arxiv.org/abs/2509.06404", "authors": ["Kaikai Wang", "Tianxun Li", "Liang Xu", "Qinglei Hu", "Keyou You"], "title": "Safety Meets Speed: Accelerated Neural MPC with Safety Guarantees and No Retraining", "comment": "12 pages, 9 figures, accepted to RA-L", "summary": "While Model Predictive Control (MPC) enforces safety via constraints, its\nreal-time execution can exceed embedded compute budgets. We propose a\nBarrier-integrated Adaptive Neural Model Predictive Control (BAN-MPC) framework\nthat synergizes neural networks' fast computation with MPC's\nconstraint-handling capability. To ensure strict safety, we replace traditional\nEuclidean distance with Control Barrier Functions (CBFs) for collision\navoidance. We integrate an offline-learned neural value function into the\noptimization objective of a Short-horizon MPC, substantially reducing online\ncomputational complexity. Additionally, we use a second neural network to learn\nthe sensitivity of the value function to system parameters, and adaptively\nadjust the neural value function based on this neural sensitivity when model\nparameters change, eliminating the need for retraining and reducing offline\ncomputation costs. The hardware in-the-loop (HIL) experiments on Jetson Nano\nshow that BAN-MPC solves 200 times faster than traditional MPC, enabling\ncollision-free navigation with control error below 5\\% under model parameter\nvariations within 15\\%, making it an effective embedded MPC alternative.", "AI": {"tldr": "本文提出了一种名为BAN-MPC的控制框架，它结合了神经网络的快速计算和模型预测控制（MPC）的安全性。通过引入控制障碍函数（CBF）和神经价值函数，并利用第二个神经网络实现参数自适应，BAN-MPC在嵌入式设备上比传统MPC快200倍，同时确保无碰撞导航和低控制误差。", "motivation": "传统模型预测控制（MPC）虽然能通过约束确保安全，但其实时执行的计算量往往超出嵌入式计算预算。研究的动机是开发一种既能保证严格安全又能在嵌入式设备上快速运行的替代方案。", "method": "本文提出了障碍集成自适应神经模型预测控制（BAN-MPC）框架。具体方法包括：1) 将神经网络的快速计算与MPC的约束处理能力相结合。2) 用控制障碍函数（CBF）替代传统的欧几里得距离，以实现严格的碰撞避免。3) 将离线学习的神经价值函数集成到短时域MPC的优化目标中，以显著降低在线计算复杂度。4) 使用第二个神经网络学习价值函数对系统参数的敏感性，从而在模型参数变化时自适应调整神经价值函数，避免重新训练并减少离线计算成本。", "result": "在Jetson Nano上的硬件在环（HIL）实验表明，BAN-MPC的求解速度比传统MPC快200倍。它能够在模型参数变化范围在15%以内的情况下，实现无碰撞导航，并将控制误差保持在5%以下。", "conclusion": "BAN-MPC是一种有效的嵌入式MPC替代方案。它成功地将神经网络的速度与MPC的安全性相结合，并通过自适应机制应对参数变化，显著提升了实时性能和实用性。"}}
{"id": "2509.05604", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05604", "abs": "https://arxiv.org/abs/2509.05604", "authors": ["Jungin Park", "Jiyoung Lee", "Kwanghoon Sohn"], "title": "Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization", "comment": "Accepted to IJCV, 29 pages, 14 figures, 11 tables", "summary": "Video summarization aims to select keyframes that are visually diverse and\ncan represent the whole story of a given video. Previous approaches have\nfocused on global interlinkability between frames in a video by temporal\nmodeling. However, fine-grained visual entities, such as objects, are also\nhighly related to the main content of the video. Moreover, language-guided\nvideo summarization, which has recently been studied, requires a comprehensive\nlinguistic understanding of complex real-world videos. To consider how all the\nobjects are semantically related to each other, this paper regards video\nsummarization as a language-guided spatiotemporal graph modeling problem. We\npresent recursive spatiotemporal graph networks, called VideoGraph, which\nformulate the objects and frames as nodes of the spatial and temporal graphs,\nrespectively. The nodes in each graph are connected and aggregated with graph\nedges, representing the semantic relationships between the nodes. To prevent\nthe edges from being configured with visual similarity, we incorporate language\nqueries derived from the video into the graph node representations, enabling\nthem to contain semantic knowledge. In addition, we adopt a recursive strategy\nto refine initial graphs and correctly classify each frame node as a keyframe.\nIn our experiments, VideoGraph achieves state-of-the-art performance on several\nbenchmarks for generic and query-focused video summarization in both supervised\nand unsupervised manners. The code is available at\nhttps://github.com/park-jungin/videograph.", "AI": {"tldr": "本文提出了一种名为VideoGraph的递归时空图网络，将视频摘要视为语言引导的时空图建模问题。它将对象和帧表示为图节点，并利用语言查询来建立语义关系，从而在通用和查询聚焦的视频摘要任务中实现了最先进的性能。", "motivation": "以往的视频摘要方法侧重于全局时间建模，但忽视了细粒度视觉实体（如对象）与视频内容的关联。此外，语言引导的视频摘要需要对复杂视频进行全面的语言理解。因此，研究人员希望解决如何将所有对象语义关联起来的问题。", "method": "本文提出VideoGraph，一个递归时空图网络。它将对象和帧分别建模为空间图和时间图的节点。图中的节点通过表示语义关系的边进行连接和聚合。为了避免边仅基于视觉相似性配置，研究人员将从视频中提取的语言查询融入到图节点表示中，使其包含语义知识。此外，还采用递归策略来细化初始图并正确分类关键帧。", "result": "实验结果表明，VideoGraph在多个通用和查询聚焦的视频摘要基准测试中，无论是在有监督还是无监督方式下，都达到了最先进的性能。", "conclusion": "VideoGraph通过将视频摘要建模为语言引导的时空图问题，并利用递归图网络和语言查询来捕捉对象和帧之间的语义关系，有效解决了视频摘要任务中的挑战，并取得了卓越的成果。"}}
{"id": "2509.06341", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06341", "abs": "https://arxiv.org/abs/2509.06341", "authors": ["Issue Yishu Wang", "Kakam Chong", "Xiaofeng Wang", "Xu Yan", "DeXin Kong", "Chen Ju", "Ming Chen", "Shuai Xiao", "Shuguang Han", "jufeng chen"], "title": "Evaluating Multi-Turn Bargain Skills in LLM-Based Seller Agent", "comment": null, "summary": "In online second-hand marketplaces, multi-turn bargaining is a crucial part\nof seller-buyer interactions. Large Language Models (LLMs) can act as seller\nagents, negotiating with buyers on behalf of sellers under given business\nconstraints. A critical ability for such agents is to track and accurately\ninterpret cumulative buyer intents across long negotiations, which directly\nimpacts bargaining effectiveness. We introduce a multi-turn evaluation\nframework for measuring the bargaining ability of seller agents in e-commerce\ndialogues. The framework tests whether an agent can extract and track buyer\nintents. Our contributions are: (1) a large-scale e-commerce bargaining\nbenchmark spanning 622 categories, 9,892 products, and 3,014 tasks; (2) a\nturn-level evaluation framework grounded in Theory of Mind (ToM) with annotated\nbuyer intents, moving beyond outcome-only metrics; and (3) an automated\npipeline that extracts reliable intent from massive dialogue data.", "AI": {"tldr": "本文提出一个多轮评估框架，用于衡量电商对话中卖家代理（LLMs）的议价能力，特别是其提取和追踪买家意图的能力，并提供大规模基准测试。", "motivation": "在线二手市场中，多轮议价至关重要。大型语言模型作为卖家代理，需要准确追踪和解释买家累积意图以提高议价效率。现有评估可能仅关注结果，缺乏对意图追踪的细致考量。", "method": "引入一个多轮评估框架来衡量卖家代理的议价能力，重点测试其提取和追踪买家意图的能力。具体贡献包括：1) 一个涵盖广泛类别和产品的电商议价基准；2) 一个基于心智理论（ToM）的轮级评估框架，包含标注的买家意图，超越了仅基于结果的指标；3) 一个从海量对话数据中提取可靠意图的自动化流程。", "result": "本文的主要成果是提供了一个新的多轮评估框架及其相关资源，包括：一个大规模电商议价基准（622个类别，9,892个产品，3,014个任务），一个基于心智理论的带标注买家意图的轮级评估框架，以及一个自动化的意图提取管道。", "conclusion": "本文为评估电商对话中LLM卖家代理的议价能力，特别是其追踪和解释买家意图的能力，提供了一个全面的多轮评估框架和大规模资源，从而超越了传统的结果导向指标。"}}
{"id": "2509.06100", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06100", "abs": "https://arxiv.org/abs/2509.06100", "authors": ["Kefan Cao", "Shuaicheng Wu"], "title": "Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models", "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) are prone to catastrophic forgetting in\nsequential multi-task settings. Parameter regularization methods such as O-LoRA\nand N-LoRA alleviate task interference by enforcing low-rank subspace\northogonality, but they overlook the fact that conventional additive\nfine-tuning disrupts the intrinsic geometric structure of LLM parameters,\nlimiting performance. Our key insight is that the parameter space of LLMs\npossesses a geometric structure, which must be preserved in addition to\nenforcing orthogonality. Based on this, we propose Orthogonal Low-rank\nAdaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM\nfine-tuning: leveraging multiplicative updates to preserve parameter geometry\nwhile applying orthogonality constraints to task subspaces. Experiments\ndemonstrate that OLieRA achieves state-of-the-art results on the Standard CL\nbenchmark and remains among the top-performing methods in the Large Number of\nTasks setting.", "AI": {"tldr": "针对大型语言模型在多任务学习中的灾难性遗忘问题，本文提出OLieRA方法。该方法结合李群理论和乘法更新来保留模型参数的几何结构，并施加正交性约束于任务子空间，从而在持续学习基准测试中取得了最先进的性能。", "motivation": "大型语言模型在顺序多任务设置中容易出现灾难性遗忘。现有的参数正则化方法（如O-LoRA和N-LoRA）通过强制低秩子空间正交性来缓解任务干扰，但它们忽视了传统加性微调会破坏LLM参数固有的几何结构，从而限制了性能。本研究的核心洞察是LLM参数空间具有几何结构，除了强制正交性外，还必须保留这一结构。", "method": "本文提出了基于李群的正交低秩适应（Orthogonal Low-rank Adaptation in Lie Groups, OLieRA）方法。该方法将李群理论引入LLM微调中，利用乘法更新来保留参数几何结构，同时对任务子空间施加正交性约束。", "result": "实验表明，OLieRA在Standard CL基准测试中取得了最先进（state-of-the-art）的结果。此外，在“大量任务”设置下，OLieRA仍然是表现最佳的方法之一。", "conclusion": "OLieRA通过结合李群理论的乘法更新来保留LLM参数的几何结构，并施加正交性约束，有效缓解了灾难性遗忘问题，并在持续学习任务中展现出卓越的性能。"}}
{"id": "2509.06433", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06433", "abs": "https://arxiv.org/abs/2509.06433", "authors": ["Ian Page", "Pierre Susbielle", "Olivier Aycard", "Pierre-Brice Wieber"], "title": "Real-time Photorealistic Mapping for Situational Awareness in Robot Teleoperation", "comment": null, "summary": "Achieving efficient remote teleoperation is particularly challenging in\nunknown environments, as the teleoperator must rapidly build an understanding\nof the site's layout. Online 3D mapping is a proven strategy to tackle this\nchallenge, as it enables the teleoperator to progressively explore the site\nfrom multiple perspectives. However, traditional online map-based teleoperation\nsystems struggle to generate visually accurate 3D maps in real-time due to the\nhigh computational cost involved, leading to poor teleoperation performances.\nIn this work, we propose a solution to improve teleoperation efficiency in\nunknown environments. Our approach proposes a novel, modular and efficient\nGPU-based integration between recent advancement in gaussian splatting SLAM and\nexisting online map-based teleoperation systems. We compare the proposed\nsolution against state-of-the-art teleoperation systems and validate its\nperformances through real-world experiments using an aerial vehicle. The\nresults show significant improvements in decision-making speed and more\naccurate interaction with the environment, leading to greater teleoperation\nefficiency. In doing so, our system enhances remote teleoperation by seamlessly\nintegrating photorealistic mapping generation with real-time performances,\nenabling effective teleoperation in unfamiliar environments.", "AI": {"tldr": "该研究提出了一种基于GPU的高斯溅射SLAM与现有在线地图遥操作系统集成的新方法，以在未知环境中实现高效、实时的逼真3D地图生成，显著提升遥操作效率。", "motivation": "在未知环境中，遥操作面临快速理解现场布局的挑战。传统的在线地图遥操作系统由于计算成本高，难以实时生成视觉准确的3D地图，导致遥操作性能不佳。", "method": "该方法提出了一种新颖、模块化且高效的GPU集成方案，将高斯溅射SLAM的最新进展与现有的在线地图遥操作系统相结合。通过使用无人机进行实际实验，与现有最先进的遥操作系统进行了性能比较和验证。", "result": "实验结果表明，该方案显著提高了决策速度和与环境交互的准确性，从而提升了遥操作效率。系统通过将逼真地图生成与实时性能无缝集成，增强了远程遥操作能力。", "conclusion": "该系统通过在未知环境中提供实时的逼真地图生成，有效提升了远程遥操作的效率和准确性，实现了在陌生环境中的高效遥操作。"}}
{"id": "2509.05606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05606", "abs": "https://arxiv.org/abs/2509.05606", "authors": ["Juan Yeo", "Ijun Jang", "Taesup Kim"], "title": "Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning", "comment": null, "summary": "Dense representations are essential for vision tasks that require spatial\nprecision and fine-grained detail. While most self-supervised representation\nlearning methods focus on global representations that summarize the image as a\nwhole, such approaches often fall short in capturing the localized semantics\nnecessary for dense prediction tasks. To overcome these limitations, we propose\na framework that builds on pretrained representations through additional\nself-supervised learning, aiming to transfer existing semantic knowledge into\nthe dense feature space. Our method aligns the distributions of dense features\nbetween a teacher and a student model. Specifically, we introduce Patch-level\nKernel Alignment (PaKA), a simple yet effective alignment objective that\ncaptures statistical dependencies, thereby matching the structural\nrelationships of dense patches across the two models. In addition, we\ninvestigate augmentation strategies specifically designed for dense\nrepresentation learning. Our framework achieves state-of-the-art results across\na variety of dense vision benchmarks, demonstrating the effectiveness of our\napproach.", "AI": {"tldr": "该论文提出了一种通过自监督学习，将预训练模型中的语义知识迁移到密集特征空间的方法，以提高密集视觉任务的表示能力，并引入了Patch-level Kernel Alignment (PaKA) 目标。", "motivation": "现有的自监督表示学习方法大多侧重于全局表示，难以捕获密集预测任务所需的局部语义和精细细节，这限制了它们在需要空间精度和细粒度信息的密集视觉任务中的应用。", "method": "提出一个框架，通过额外的自监督学习，在预训练表示的基础上，将现有语义知识转移到密集特征空间。具体方法包括：对教师模型和学生模型之间的密集特征分布进行对齐，并引入了“Patch-level Kernel Alignment (PaKA)”这一简单有效的对齐目标，以捕获统计依赖性，从而匹配两个模型中密集补丁的结构关系。此外，还研究了专门为密集表示学习设计的增强策略。", "result": "该框架在各种密集视觉基准测试中取得了最先进的成果。", "conclusion": "所提出的方法在密集表示学习方面表现出卓越的有效性，成功克服了现有方法在捕获局部语义方面的局限性。"}}
{"id": "2509.06355", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06355", "abs": "https://arxiv.org/abs/2509.06355", "authors": ["Yunzhe Wang", "Volkan Ustun", "Chris McGroarty"], "title": "A data-driven discretized CS:GO simulation environment to facilitate strategic multi-agent planning research", "comment": "Accepted at the Winter Simulation Conference 2025, December, Seattle\n  USA", "summary": "Modern simulation environments for complex multi-agent interactions must\nbalance high-fidelity detail with computational efficiency. We present DECOY, a\nnovel multi-agent simulator that abstracts strategic, long-horizon planning in\n3D terrains into high-level discretized simulation while preserving low-level\nenvironmental fidelity. Using Counter-Strike: Global Offensive (CS:GO) as a\ntestbed, our framework accurately simulates gameplay using only movement\ndecisions as tactical positioning -- without explicitly modeling low-level\nmechanics such as aiming and shooting. Central to our approach is a waypoint\nsystem that simplifies and discretizes continuous states and actions, paired\nwith neural predictive and generative models trained on real CS:GO tournament\ndata to reconstruct event outcomes. Extensive evaluations show that replays\ngenerated from human data in DECOY closely match those observed in the original\ngame. Our publicly available simulation environment provides a valuable tool\nfor advancing research in strategic multi-agent planning and behavior\ngeneration.", "AI": {"tldr": "DECOY是一个多智能体模拟器，它在3D地形中抽象出战略性的、长周期规划，通过离散化模拟和神经网络模型，仅用移动决策就能高保真地模拟CS:GO游戏，并能重现人类游戏数据。", "motivation": "现代复杂多智能体交互的模拟环境需要在高保真细节和计算效率之间取得平衡。", "method": "DECOY将3D地形中的战略性、长周期规划抽象为高层离散化模拟，同时保留低层环境保真度。它使用一个航路点系统来简化和离散化连续状态和动作，并结合基于真实CS:GO比赛数据训练的神经预测和生成模型来重构事件结果，仅将移动决策作为战术定位，而不显式建模瞄准和射击等低层机制。", "result": "通过广泛评估，DECOY中根据人类数据生成的重放与原始游戏中观察到的重放高度匹配。", "conclusion": "DECOY提供了一个有价值的公共模拟环境，可用于推动战略多智能体规划和行为生成方面的研究。"}}
{"id": "2509.06164", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06164", "abs": "https://arxiv.org/abs/2509.06164", "authors": ["Jinrui Yang", "Xudong Han", "Timothy Baldwin"], "title": "Benchmarking Gender and Political Bias in Large Language Models", "comment": "The 8th International Conference on Natural Language and Speech\n  Processing (Oral)", "summary": "We introduce EuroParlVote, a novel benchmark for evaluating large language\nmodels (LLMs) in politically sensitive contexts. It links European Parliament\ndebate speeches to roll-call vote outcomes and includes rich demographic\nmetadata for each Member of the European Parliament (MEP), such as gender, age,\ncountry, and political group. Using EuroParlVote, we evaluate state-of-the-art\nLLMs on two tasks -- gender classification and vote prediction -- revealing\nconsistent patterns of bias. We find that LLMs frequently misclassify female\nMEPs as male and demonstrate reduced accuracy when simulating votes for female\nspeakers. Politically, LLMs tend to favor centrist groups while underperforming\non both far-left and far-right ones. Proprietary models like GPT-4o outperform\nopen-weight alternatives in terms of both robustness and fairness. We release\nthe EuroParlVote dataset, code, and demo to support future research on fairness\nand accountability in NLP within political contexts.", "AI": {"tldr": "该研究引入了EuroParlVote基准，用于评估大型语言模型（LLMs）在政治敏感环境中的表现。通过将欧洲议会辩论演讲与投票结果和议员元数据关联，研究发现LLMs在性别分类和投票预测任务中存在偏见，包括将女性议员误分类为男性、对女性发言者的预测准确性降低，以及偏爱中间派政治团体。专有模型表现优于开源模型。", "motivation": "研究旨在评估LLMs在政治敏感语境中的表现，特别是揭示其可能存在的偏见模式。现有评估基准可能未能充分捕捉到政治辩论和投票行为中的复杂性和敏感性，以及与人口统计学特征（如性别）相关的偏见。", "method": "研究引入了EuroParlVote数据集，该数据集将欧洲议会辩论演讲与点名投票结果相链接，并包含欧洲议会议员（MEP）的丰富人口统计元数据（如性别、年龄、国家和政治团体）。研究使用该基准在性别分类和投票预测两项任务上评估了最先进的LLMs，并比较了专有模型（如GPT-4o）和开源模型的性能。", "result": "研究揭示了LLMs中存在一致的偏见模式：LLMs经常将女性议员错误地归类为男性，并且在模拟女性发言者的投票时准确性降低。在政治方面，LLMs倾向于偏爱中间派团体，而在极左和极右团体上的表现不佳。专有模型（如GPT-4o）在鲁棒性和公平性方面均优于开源模型。", "conclusion": "LLMs在政治敏感语境中表现出显著的偏见，尤其是在性别分类和政治立场预测方面。EuroParlVote基准对于未来在政治语境下研究自然语言处理（NLP）的公平性和问责制具有重要价值。研究结果表明，专有模型在处理此类敏感任务时可能具有优势，并且需要进一步研究以解决LLMs中的偏见问题。"}}
{"id": "2509.06469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06469", "abs": "https://arxiv.org/abs/2509.06469", "authors": ["Benedikt Kreis", "Malte Mosbach", "Anny Ripke", "Muhammad Ehsan Ullah", "Sven Behnke", "Maren Bennewitz"], "title": "Interactive Shaping of Granular Media Using Reinforcement Learning", "comment": "Accepted to IEEE-RAS International Conference on Humanoid Robots\n  (Humanoids) 2025", "summary": "Autonomous manipulation of granular media, such as sand, is crucial for\napplications in construction, excavation, and additive manufacturing. However,\nshaping granular materials presents unique challenges due to their\nhigh-dimensional configuration space and complex dynamics, where traditional\nrule-based approaches struggle without extensive engineering efforts.\nReinforcement learning (RL) offers a promising alternative by enabling agents\nto learn adaptive manipulation strategies through trial and error. In this\nwork, we present an RL framework that enables a robotic arm with a cubic\nend-effector and a stereo camera to shape granular media into desired target\nstructures. We show the importance of compact observations and concise reward\nformulations for the large configuration space, validating our design choices\nwith an ablation study. Our results demonstrate the effectiveness of the\nproposed approach for the training of visual policies that manipulate granular\nmedia including their real-world deployment, outperforming two baseline\napproaches.", "AI": {"tldr": "本文提出一个强化学习框架，使机械臂能够通过视觉策略塑造颗粒介质，通过紧凑的观测和简洁的奖励公式，在真实世界部署中表现出色并优于基线方法。", "motivation": "由于颗粒材料的高维配置空间和复杂动力学，传统的基于规则的方法在塑造颗粒介质时面临巨大挑战，需要大量的工程投入。强化学习提供了一种通过试错学习自适应操作策略的有效替代方案。", "method": "研究提出一个强化学习框架，使用配备立方末端执行器和立体摄像头的机械臂来塑造颗粒介质。该方法强调紧凑的观测和简洁的奖励公式对于处理大型配置空间的重要性，并通过消融研究验证了设计选择。", "result": "实验结果表明，所提出的方法能够有效地训练用于操作颗粒介质的视觉策略，包括其在真实世界的部署，并且性能优于两种基线方法。", "conclusion": "该强化学习框架，通过其紧凑的观测和简洁的奖励公式，能够有效且成功地使机器人手臂塑造颗粒介质，并实现真实世界部署。"}}
{"id": "2509.05614", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05614", "abs": "https://arxiv.org/abs/2509.05614", "authors": ["Hanzhen Wang", "Jiaming Xu", "Jiayi Pan", "Yongkang Zhou", "Guohao Dai"], "title": "SpecPrune-VLA: Accelerating Vision-Language-Action Models via Action-Aware Self-Speculative Pruning", "comment": "8pages, 10 figures,", "summary": "Pruning accelerates compute-bound models by reducing computation. Recently\napplied to Vision-Language-Action (VLA) models, existing methods prune tokens\nusing only local info from current action, ignoring global context from prior\nactions, causing >20% success rate drop and limited speedup. We observe high\nsimilarity across consecutive actions and propose leveraging both local\n(current) and global (past) info for smarter token selection. We introduce\nSpecPrune-VLA, a training-free method with two-level pruning and heuristic\ncontrol: (1) Static pruning at action level: uses global history and local\ncontext to reduce visual tokens per action; (2) Dynamic pruning at layer level:\nprunes tokens per layer based on layer-specific importance; (3) Lightweight\naction-aware controller: classifies actions as coarse/fine-grained (by speed),\nadjusting pruning aggressiveness since fine-grained actions are\npruning-sensitive. Experiments on LIBERO show SpecPrune-VLA achieves 1.46 times\nspeedup on NVIDIA A800 and 1.57 times on NVIDIA GeForce RTX 3090 vs.\nOpenVLA-OFT, with negligible success rate loss.", "AI": {"tldr": "SpecPrune-VLA是一种无需训练的VLA模型剪枝方法，通过结合局部和全局动作信息，实现了显著的速度提升，同时保持了接近的成功率，解决了现有方法性能下降的问题。", "motivation": "现有的VLA模型剪枝方法仅利用当前动作的局部信息，导致成功率大幅下降（超过20%）且加速有限。研究者观察到连续动作间存在高度相似性，因此提出利用局部和全局上下文信息进行更智能的token选择。", "method": "本文提出了SpecPrune-VLA，一种无需训练的方法，包含两级剪枝和启发式控制：(1) 动作层面的静态剪枝：利用全局历史和局部上下文减少每个动作的视觉tokens；(2) 层级动态剪枝：根据每层的重要性修剪tokens；(3) 轻量级动作感知控制器：根据动作类型（粗粒度/细粒度）调整剪枝强度，因为细粒度动作对剪枝更敏感。", "result": "在LIBERO数据集上，SpecPrune-VLA在NVIDIA A800上实现了1.46倍加速，在NVIDIA GeForce RTX 3090上实现了1.57倍加速（对比OpenVLA-OFT），且成功率损失可忽略不计。", "conclusion": "SpecPrune-VLA通过结合局部和全局上下文信息进行智能token剪枝，有效加速了VLA模型，实现了显著的速度提升，同时几乎不影响性能，解决了现有剪枝方法的局限性。"}}
{"id": "2509.06409", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06409", "abs": "https://arxiv.org/abs/2509.06409", "authors": ["Yihong Luo", "Wenwu He", "Zhuo-Xu Cui", "Dong Liang"], "title": "Teaching AI Stepwise Diagnostic Reasoning with Report-Guided Chain-of-Thought Learning", "comment": null, "summary": "This study presents DiagCoT, a multi-stage framework that applies supervised\nfine-tuning to general-purpose vision-language models (VLMs) to emulate\nradiologists' stepwise diagnostic reasoning using only free-text reports.\nDiagCoT combines contrastive image-report tuning for domain alignment,\nchain-of-thought supervision to capture inferential logic, and reinforcement\ntuning with clinical reward signals to enhance factual accuracy and fluency. On\nthe MIMIC-CXR benchmark, DiagCoT improved zero-shot disease classification AUC\nfrom 0.52 to 0.76 (absolute gain of 0.24), pathology grounding mIoU from 0.08\nto 0.31 (absolute gain of 0.23), and report generation BLEU from 0.11 to 0.33\n(absolute gain of 0.22). It outperformed state-of-the-art models including\nLLaVA-Med and CXR-LLAVA on long-tailed diseases and external datasets. By\nconverting unstructured clinical narratives into structured supervision,\nDiagCoT offers a scalable approach for developing interpretable and\ndiagnostically competent AI systems for radiology.", "AI": {"tldr": "DiagCoT是一个多阶段框架，通过对通用视觉-语言模型进行监督微调，模拟放射科医生的逐步诊断推理，显著提升了放射学AI系统的诊断能力和可解释性。", "motivation": "开发能够模拟放射科医生逐步诊断推理的、具有诊断能力和可解释性的AI系统，并解决如何将非结构化的临床叙述转化为结构化监督的挑战。", "method": "DiagCoT是一个多阶段框架，对通用视觉-语言模型（VLMs）进行监督微调。它结合了：1) 对比式图像-报告微调以实现领域对齐；2) 思维链监督以捕捉推理逻辑；3) 使用临床奖励信号的强化微调以增强事实准确性和流畅性。", "result": "在MIMIC-CXR基准测试中，DiagCoT将零样本疾病分类AUC从0.52提高到0.76（绝对增益0.24），病理定位mIoU从0.08提高到0.31（绝对增益0.23），报告生成BLEU从0.11提高到0.33（绝对增益0.22）。它在长尾疾病和外部数据集上优于LLaVA-Med和CXR-LLAVA等现有最先进模型。", "conclusion": "通过将非结构化的临床叙述转化为结构化监督，DiagCoT为开发可解释且具有诊断能力的放射学AI系统提供了一种可扩展的方法。"}}
{"id": "2509.06184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06184", "abs": "https://arxiv.org/abs/2509.06184", "authors": ["Jacob Mitchell Springer", "Vaibhav Adlakha", "Siva Reddy", "Aditi Raghunathan", "Marius Mosbach"], "title": "Understanding the Influence of Synthetic Data for Text Embedders", "comment": "ACL Findings 2025", "summary": "Recent progress in developing general purpose text embedders has been driven\nby training on ever-growing corpora of synthetic LLM-generated data.\nNonetheless, no publicly available synthetic dataset exists, posing a barrier\nto studying its role for generalization. To address this issue, we first\nreproduce and publicly release the synthetic data proposed by Wang et al.\n(Mistral-E5). Our synthetic data is high quality and leads to consistent\nimprovements in performance. Next, we critically examine where exactly\nsynthetic data improves model generalization. Our analysis reveals that\nbenefits from synthetic data are sparse and highly localized to individual\ndatasets. Moreover, we observe trade-offs between the performance on different\ncategories and data that benefits one task, degrades performance on another.\nOur findings highlight the limitations of current synthetic data approaches for\nbuilding general-purpose embedders and challenge the notion that training on\nsynthetic data leads to more robust embedding models across tasks.", "AI": {"tldr": "本文复制并公开了Mistral-E5的合成数据，发现其能提升文本嵌入器性能，但这种泛化益处是稀疏且局部化的，并存在任务间的性能权衡，挑战了合成数据能构建更通用嵌入器的观点。", "motivation": "通用文本嵌入器训练日益依赖LLM生成的合成数据，但目前缺乏公开可用的合成数据集，阻碍了对其在泛化中作用的研究。", "method": "首先，复制并公开发布了Wang等人（Mistral-E5）提出的合成数据。然后，批判性地分析了合成数据究竟在何处改善了模型的泛化能力。", "result": "研究发现，复制的合成数据质量高，能持续提升性能。然而，合成数据带来的益处是稀疏的，且高度局限于特定数据集。此外，不同类别任务的性能之间存在权衡，对一个任务有益的数据可能降低另一个任务的性能。", "conclusion": "研究结果强调了当前合成数据方法在构建通用嵌入器方面的局限性，并挑战了通过合成数据训练能产生更稳健的跨任务嵌入模型的观念。"}}
{"id": "2509.06481", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06481", "abs": "https://arxiv.org/abs/2509.06481", "authors": ["Vinita Sao", "Tu Dac Ho", "Sujoy Bhore", "P. B. Sujit"], "title": "Event Driven CBBA with Reduced Communication", "comment": null, "summary": "In various scenarios such as multi-drone surveillance and search-and-rescue\noperations, deploying multiple robots is essential to accomplish multiple tasks\nat once. Due to the limited communication range of these vehicles, a\ndecentralised task allocation algorithm is crucial for effective task\ndistribution among robots. The consensus-based bundle algorithm (CBBA) has been\npromising for multi-robot operation, offering theoretical guarantees. However,\nCBBA demands continuous communication, leading to potential congestion and\npacket loss that can hinder performance. In this study, we introduce an\nevent-driven communication mechanism designed to address these communication\nchallenges while maintaining the convergence and performance bounds of CBBA. We\ndemonstrate theoretically that the solution quality matches that of CBBA and\nvalidate the approach with Monte-Carlo simulations across varying targets,\nagents, and bundles. Results indicate that the proposed algorithm (ED-CBBA) can\nreduce message transmissions by up to 52%.", "AI": {"tldr": "本研究提出了一种事件驱动的共识捆绑算法（ED-CBBA），旨在减少多机器人任务分配中CBBA的通信开销，同时保持其收敛性和性能保证。", "motivation": "多机器人系统在监视和搜救等场景中执行多任务至关重要。现有共识捆绑算法（CBBA）虽有理论保证，但要求持续通信，这在机器人通信范围有限的情况下会导致通信拥堵和数据包丢失，影响性能。", "method": "引入了一种事件驱动的通信机制来改进CBBA。理论上证明了其解决方案质量与CBBA相符，并通过蒙特卡洛模拟在不同目标、代理和捆绑配置下进行了验证。", "result": "实验结果表明，所提出的ED-CBBA算法能将消息传输量减少高达52%，同时保持与CBBA相同的解决方案质量。", "conclusion": "ED-CBBA成功解决了多机器人任务分配中的通信挑战，显著降低了消息传输量，且不牺牲CBBA的收敛性和性能边界。"}}
{"id": "2509.05625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05625", "abs": "https://arxiv.org/abs/2509.05625", "authors": ["Kien Nguyen", "Anh Tran", "Cuong Pham"], "title": "SuMa: A Subspace Mapping Approach for Robust and Effective Concept Erasure in Text-to-Image Diffusion Models", "comment": null, "summary": "The rapid growth of text-to-image diffusion models has raised concerns about\ntheir potential misuse in generating harmful or unauthorized contents. To\naddress these issues, several Concept Erasure methods have been proposed.\nHowever, most of them fail to achieve both robustness, i.e., the ability to\nrobustly remove the target concept., and effectiveness, i.e., maintaining image\nquality. While few recent techniques successfully achieve these goals for NSFW\nconcepts, none could handle narrow concepts such as copyrighted characters or\ncelebrities. Erasing these narrow concepts is critical in addressing copyright\nand legal concerns. However, erasing them is challenging due to their close\ndistances to non-target neighboring concepts, requiring finer-grained\nmanipulation. In this paper, we introduce Subspace Mapping (SuMa), a novel\nmethod specifically designed to achieve both robustness and effectiveness in\neasing these narrow concepts. SuMa first derives a target subspace representing\nthe concept to be erased and then neutralizes it by mapping it to a reference\nsubspace that minimizes the distance between the two. This mapping ensures the\ntarget concept is robustly erased while preserving image quality. We conduct\nextensive experiments with SuMa across four tasks: subclass erasure, celebrity\nerasure, artistic style erasure, and instance erasure and compare the results\nwith current state-of-the-art methods. Our method achieves image quality\ncomparable to approaches focused on effectiveness, while also yielding results\nthat are on par with methods targeting completeness.", "AI": {"tldr": "针对文生图模型中狭窄概念（如版权角色、名人）的擦除挑战，本文提出了一种名为Subspace Mapping (SuMa) 的新方法，旨在同时实现擦除的鲁棒性和图像质量的有效性。", "motivation": "文生图扩散模型可能生成有害或未经授权的内容，引发担忧。现有概念擦除方法通常难以兼顾鲁棒性（彻底移除目标概念）和有效性（保持图像质量），尤其是在处理版权角色或名人等“狭窄概念”时，由于其与非目标概念的距离较近，擦除难度更大，这对于解决版权和法律问题至关重要。", "method": "本文提出了Subspace Mapping (SuMa) 方法。该方法首先推导出代表待擦除概念的“目标子空间”，然后通过将其映射到一个“参考子空间”来中和它，从而最小化两者之间的距离。这种子空间映射确保了目标概念被鲁棒地擦除，同时有效保持了图像质量。", "result": "SuMa在子类擦除、名人擦除、艺术风格擦除和实例擦除四项任务中进行了广泛实验，并与当前最先进的方法进行了比较。结果表明，SuMa在图像质量方面与专注于有效性的方法相当，同时在擦除的彻底性方面也与专注于完整性的方法不分伯仲。", "conclusion": "SuMa是一种新颖且有效的方法，能够同时实现对文生图模型中狭窄概念的鲁棒擦除和图像质量的有效保持，解决了现有方法在处理此类概念时难以兼顾这两大目标的问题，对于解决版权和法律合规性问题具有重要意义。"}}
{"id": "2509.06436", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06436", "abs": "https://arxiv.org/abs/2509.06436", "authors": ["Song Yu", "Xiaofei Xu", "Ke Deng", "Li Li", "Lin Tian"], "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning", "comment": "19 pages, 5 figures", "summary": "Large language models (LLMs) face persistent challenges when handling\nlong-context tasks, most notably the lost in the middle issue, where\ninformation located in the middle of a long input tends to be underutilized.\nSome existing methods that reduce input have the risk of discarding key\ninformation, while others that extend context windows often lead to attention\ndispersion. To address these limitations, we propose Tree of Agents (TOA), a\nmulti-agent reasoning framework that segments the input into chunks processed\nby independent agents. Each agent generates its local cognition, then agents\ndynamically exchange information for collaborative reasoning along\ntree-structured paths. TOA enables agents to probe different reasoning orders\nfor multi-perspective understanding, effectively mitigating position bias and\nreducing hallucinations. To improve processing efficiency, we incorporate\nprefix-hash caching and adaptive pruning strategies, achieving significant\nperformance improvements with comparable API overhead. Experiments show that\nTOA, powered by compact LLaMA3.1-8B, significantly outperforms multiple\nbaselines and demonstrates comparable performance to the latest and much larger\ncommercial models, such as Gemini1.5-pro, on various long-context tasks. Code\nis available at https://github.com/Aireduce952/Tree-of-Agents.", "AI": {"tldr": "该研究提出了Tree of Agents (TOA)，一个多智能体推理框架，通过将长文本分块并由独立智能体处理，然后进行树状结构的信息交换和协作推理，有效解决了大型语言模型（LLMs）在长上下文任务中遇到的“中间遗失”问题和注意力分散问题，并在效率和性能上均表现出色。", "motivation": "大型语言模型在处理长上下文任务时面临挑战，尤其是“中间遗失”问题，即输入中间的信息容易被忽视。现有方法要么可能丢弃关键信息（减少输入），要么导致注意力分散（扩展上下文窗口）。", "method": "本文提出了Tree of Agents (TOA) 框架：1) 将输入分割成块，由独立的智能体处理；2) 每个智能体生成局部认知；3) 智能体沿树状路径动态交换信息进行协作推理；4) 智能体探索不同的推理顺序以实现多角度理解，从而减轻位置偏差和减少幻觉；5) 整合了前缀哈希缓存和自适应剪枝策略以提高处理效率。", "result": "实验表明，TOA有效缓解了位置偏差并减少了幻觉。它以相当的API开销实现了显著的性能提升。基于紧凑的LLaMA3.1-8B模型，TOA显著优于多个基线模型，并在各种长上下文任务上展现出与最新、规模更大的商业模型（如Gemini1.5-pro）相当的性能。", "conclusion": "TOA是一个高效且有效的多智能体推理框架，能够解决LLMs在长上下文任务中的固有挑战，通过协作推理和效率优化，使得小型模型也能在复杂长文本任务中达到甚至超越大型商业模型的表现。"}}
{"id": "2509.06196", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06196", "abs": "https://arxiv.org/abs/2509.06196", "authors": ["Mohamed T. Younes", "Omar Walid", "Khaled Shaban", "Ali Hamdi", "Mai Hassan"], "title": "Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation", "comment": "Accepted in AICCSA 2025", "summary": "This paper presents a novel approach to recruitment automation. Large\nLanguage Models (LLMs) were fine-tuned to improve accuracy and efficiency.\nBuilding upon our previous work on the Multilayer Large Language Model-Based\nRobotic Process Automation Applicant Tracking (MLAR) system . This work\nintroduces a novel methodology. Training fine-tuned LLMs specifically tuned for\nrecruitment tasks. The proposed framework addresses the limitations of generic\nLLMs by creating a synthetic dataset that uses a standardized JSON format. This\nhelps ensure consistency and scalability. In addition to the synthetic data\nset, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes\nwere parsed into the same structured JSON format and placed in the training\nset. This will help improve data diversity and realism. Through\nexperimentation, we demonstrate significant improvements in performance\nmetrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall\nsimilarity compared to base models and other state-of-the-art LLMs. In\nparticular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%,\nindicating exceptional precision and recall in recruitment tasks. This study\nhighlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize\nrecruitment workflows by providing more accurate candidate-job matching.", "AI": {"tldr": "本文提出了一种通过微调大型语言模型（LLMs）来自动化招聘流程的新方法，结合合成数据和真实简历数据，显著提高了招聘任务的准确性和效率。", "motivation": "研究动机是为了解决通用LLMs在招聘任务中的局限性，提高招聘流程的准确性和效率，实现更精准的候选人与职位匹配。", "method": "该研究构建在之前MLAR系统工作的基础上，引入了一种新颖的方法：为招聘任务专门微调LLMs。具体包括：创建标准化JSON格式的合成数据集，并使用DeepSeek解析真实简历为相同JSON格式，以增加数据多样性和真实性，然后用这些数据训练和微调LLMs。", "result": "实验结果表明，与基础模型和其他最先进的LLMs相比，该方法在精确匹配、F1分数、BLEU分数、ROUGE分数和整体相似度等性能指标上取得了显著提升。特别是，微调后的Phi-4模型在招聘任务中达到了最高的F1分数90.62%。", "conclusion": "研究强调了微调LLMs在招聘自动化领域的巨大潜力，通过提供更准确的候选人与职位匹配，有望彻底改变招聘工作流程。"}}
{"id": "2509.06582", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.06582", "abs": "https://arxiv.org/abs/2509.06582", "authors": ["Carlos A. Pinheiro de Sousa", "Niklas Gröne", "Mathias Günther", "Oliver Deussen"], "title": "Co-Located VR with Hybrid SLAM-based HMD Tracking and Motion Capture Synchronization", "comment": "Accepted at the Gesellschaft f\\\"ur Informatik (GI) VR/AR Workshop\n  2025 (Lecture Notes in Informatics)", "summary": "We introduce a multi-user VR co-location framework that synchronizes users\nwithin a shared virtual environment aligned to physical space. Our approach\ncombines a motion capture system with SLAM-based inside-out tracking to deliver\nsmooth, high-framerate, low-latency performance. Previous methods either rely\non continuous external tracking, which introduces latency and jitter, or on\none-time calibration, which cannot correct drift over time. In contrast, our\napproach combines the responsiveness of local HMD SLAM tracking with the\nflexibility to realign to an external source when needed. It also supports\nreal-time pose sharing across devices, ensuring consistent spatial alignment\nand engagement between users. Our evaluation demonstrates that our framework\nachieves the spatial accuracy required for natural multi-user interaction while\noffering improved comfort, scalability, and robustness over existing co-located\nVR solutions.", "AI": {"tldr": "该论文介绍了一个多用户VR协同定位框架，通过结合动作捕捉系统和基于SLAM的由内向外跟踪，实现了共享虚拟环境中用户的高精度、低延迟、高帧率同步。", "motivation": "现有的多用户VR协同定位方法要么依赖连续外部跟踪导致延迟和抖动，要么依赖一次性校准无法纠正随时间产生的漂移。研究旨在解决这些问题，提供一个兼具响应性、漂移纠正能力和高精度的解决方案。", "method": "该方法结合了动作捕捉系统（用于外部源校准和漂移纠正）与基于SLAM的由内向外跟踪（用于本地头显的响应性）。它允许在需要时重新对齐到外部源，并支持设备间的实时姿态共享，以确保用户间空间对齐的一致性。", "result": "评估表明，该框架实现了多用户自然交互所需的空间精度，并且相比现有协同定位VR解决方案，提供了更高的舒适度、可扩展性和鲁棒性。", "conclusion": "该框架通过结合内外跟踪优势，克服了传统多用户VR协同定位方案的局限性，为实现更自然、更稳定、更舒适的多用户VR体验提供了有效的解决方案。"}}
{"id": "2509.05630", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.05630", "abs": "https://arxiv.org/abs/2509.05630", "authors": ["Moqsadur Rahman", "Saurav Kumar", "Santosh S. Palmate", "M. Shahriar Hossain"], "title": "Self-supervised Learning for Hyperspectral Images of Trees", "comment": null, "summary": "Aerial remote sensing using multispectral and RGB imagers has provided a\ncritical impetus to precision agriculture. Analysis of the hyperspectral images\nwith limited or no labels is challenging. This paper focuses on self-supervised\nlearning to create neural network embeddings reflecting vegetation properties\nof trees from aerial hyperspectral images of crop fields. Experimental results\ndemonstrate that a constructed tree representation, using a vegetation\nproperty-related embedding space, performs better in downstream machine\nlearning tasks compared to the direct use of hyperspectral vegetation\nproperties as tree representations.", "AI": {"tldr": "本文提出了一种自监督学习方法，通过航空高光谱图像创建反映树木植被特性的神经网络嵌入，该方法在下游机器学习任务中表现优于直接使用高光谱植被特性。", "motivation": "航空多光谱和RGB成像推动了精准农业发展，但有限或无标签的高光谱图像分析极具挑战性。", "method": "采用自监督学习方法，利用航空高光谱图像生成神经网络嵌入，以反映树木的植被特性。通过构建一个与植被特性相关的嵌入空间来表示树木。", "result": "实验结果表明，与直接使用高光谱植被特性作为树木表示相比，使用构建的植被特性相关嵌入空间表示树木在下游机器学习任务中表现更佳。", "conclusion": "自监督学习能够有效创建反映树木植被特性的嵌入表示，从而提升航空高光谱图像在精准农业下游任务中的应用效果。"}}
{"id": "2509.06444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06444", "abs": "https://arxiv.org/abs/2509.06444", "authors": ["Cheng Qian", "Hainan Zhang", "Yongxin Tong", "Hong-Wei Zheng", "Zhiming Zheng"], "title": "HyFedRAG: A Federated Retrieval-Augmented Generation Framework for Heterogeneous and Privacy-Sensitive Data", "comment": "9 pages, 7 figures", "summary": "Centralized RAG pipelines struggle with heterogeneous and privacy-sensitive\ndata, especially in distributed healthcare settings where patient data spans\nSQL, knowledge graphs, and clinical notes. Clinicians face difficulties\nretrieving rare disease cases due to privacy constraints and the limitations of\ntraditional cloud-based RAG systems in handling diverse formats and edge\ndevices. To address this, we introduce HyFedRAG, a unified and efficient\nFederated RAG framework tailored for Hybrid data modalities. By leveraging an\nedge-cloud collaborative mechanism, HyFedRAG enables RAG to operate across\ndiverse data sources while preserving data privacy. Our key contributions are:\n(1) We design an edge-cloud collaborative RAG framework built on Flower, which\nsupports querying structured SQL data, semi-structured knowledge graphs, and\nunstructured documents. The edge-side LLMs convert diverse data into\nstandardized privacy-preserving representations, and the server-side LLMs\nintegrates them for global reasoning and generation. (2) We integrate\nlightweight local retrievers with privacy-aware LLMs and provide three\nanonymization tools that enable each client to produce semantically rich,\nde-identified summaries for global inference across devices. (3) To optimize\nresponse latency and reduce redundant computation, we design a three-tier\ncaching strategy consisting of local cache, intermediate representation cache,\nand cloud inference cache. Experimental results on PMC-Patients demonstrate\nthat HyFedRAG outperforms existing baselines in terms of retrieval quality,\ngeneration consistency, and system efficiency. Our framework offers a scalable\nand privacy-compliant solution for RAG over structural-heterogeneous data,\nunlocking the potential of LLMs in sensitive and diverse data environments.", "AI": {"tldr": "HyFedRAG是一个统一高效的联邦RAG框架，专为处理分布式医疗环境中混合、隐私敏感的异构数据而设计，通过边缘-云协作机制实现数据隐私保护和高效检索。", "motivation": "集中式RAG管道难以处理异构和隐私敏感数据，特别是在医疗保健领域，患者数据分布于SQL、知识图谱和临床笔记中。由于隐私限制和传统云RAG系统在处理多样格式和边缘设备方面的局限性，临床医生难以检索罕见疾病病例。", "method": "HyFedRAG采用基于Flower的边缘-云协作RAG框架，支持查询结构化SQL、半结构化知识图谱和非结构化文档。边缘侧LLM将多样数据转换为标准化、隐私保护的表示，服务器侧LLM将其集成以进行全局推理和生成。它集成了轻量级本地检索器和隐私感知LLM，并提供三种匿名化工具。为优化响应延迟和减少冗余计算，设计了三层缓存策略（本地缓存、中间表示缓存和云推理缓存）。", "result": "在PMC-Patients数据集上的实验结果表明，HyFedRAG在检索质量、生成一致性和系统效率方面均优于现有基线。", "conclusion": "HyFedRAG为结构异构数据上的RAG提供了一个可扩展且符合隐私要求的解决方案，释放了LLM在敏感和多样数据环境中的潜力。"}}
{"id": "2509.06200", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06200", "abs": "https://arxiv.org/abs/2509.06200", "authors": ["Omar Walid", "Mohamed T. Younes", "Khaled Shaban", "Mai Hassan", "Ali Hamdi"], "title": "MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment", "comment": "Accepted in AICCSA 2025", "summary": "This paper presents MSLEF, a multi-segment ensemble framework that employs\nLLM fine-tuning to enhance resume parsing in recruitment automation. It\nintegrates fine-tuned Large Language Models (LLMs) using weighted voting, with\neach model specializing in a specific resume segment to boost accuracy.\nBuilding on MLAR , MSLEF introduces a segment-aware architecture that leverages\nfield-specific weighting tailored to each resume part, effectively overcoming\nthe limitations of single-model systems by adapting to diverse formats and\nstructures. The framework incorporates Gemini-2.5-Flash LLM as a high-level\naggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4\n14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score,\nBLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best\nsingle model by up to +7% in RS. Its segment-aware design enhances\ngeneralization across varied resume layouts, making it highly adaptable to\nreal-world hiring scenarios while ensuring precise and reliable candidate\nrepresentation.", "AI": {"tldr": "本文提出了MSLEF，一个多段集成框架，通过LLM微调来增强简历解析，以提高招聘自动化中的准确性和适应性。", "motivation": "现有单模型系统在处理多样化的简历格式和结构时存在局限性，需要一种能有效适应并提高解析准确性的解决方案，以提升招聘自动化效率。", "method": "MSLEF是一个多段集成框架，它通过加权投票整合了多个经过微调的大型语言模型（LLM），每个模型专注于特定的简历段落。它引入了段落感知架构和针对每个简历部分的字段特定加权。该框架使用Gemini-2.5-Flash作为复杂部分的聚合器，并利用Gemma 9B、LLaMA 3.1 8B和Phi-4 14B等LLM。", "result": "MSLEF在精确匹配（EM）、F1分数、BLEU、ROUGE和招聘相似度（RS）等指标上取得了显著改进，在RS方面比最佳单模型高出多达+7%。其段落感知设计增强了对不同简历布局的泛化能力。", "conclusion": "MSLEF框架高度适应真实世界的招聘场景，能够确保精确可靠的候选人信息表示，有效克服了单模型的局限性，提升了简历解析的准确性和泛化能力。"}}
{"id": "2509.06593", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06593", "abs": "https://arxiv.org/abs/2509.06593", "authors": ["Meher V. R. Malladi", "Tiziano Guadagnino", "Luca Lobefaro", "Cyrill Stachniss"], "title": "A Robust Approach for LiDAR-Inertial Odometry Without Sensor-Specific Modeling", "comment": null, "summary": "Accurate odometry is a critical component in a robotic navigation stack, and\nsubsequent modules such as planning and control often rely on an estimate of\nthe robot's motion. Sensor-based odometry approaches should be robust across\nsensor types and deployable in different target domains, from solid-state\nLiDARs mounted on cars in urban-driving scenarios to spinning LiDARs on\nhandheld packages used in unstructured natural environments. In this paper, we\npropose a robust LiDAR-inertial odometry system that does not rely on\nsensor-specific modeling. Sensor fusion techniques for LiDAR and inertial\nmeasurement unit (IMU) data typically integrate IMU data iteratively in a\nKalman filter or use pre-integration in a factor graph framework, combined with\nLiDAR scan matching often exploiting some form of feature extraction. We\npropose an alternative strategy that only requires a simplified motion model\nfor IMU integration and directly registers LiDAR scans in a scan-to-map\napproach. Our approach allows us to impose a novel regularization on the LiDAR\nregistration, improving the overall odometry performance. We detail extensive\nexperiments on a number of datasets covering a wide array of commonly used\nrobotic sensors and platforms. We show that our approach works with the exact\nsame configuration in all these scenarios, demonstrating its robustness. We\nhave open-sourced our implementation so that the community can build further on\nour work and use it in their navigation stacks.", "AI": {"tldr": "本文提出了一种鲁棒的激光雷达-惯性里程计系统，该系统不依赖于传感器特定模型，通过简化的IMU运动模型和直接的激光雷达扫描到地图配准实现，并在多种数据集上展示了其通用性和性能。", "motivation": "机器人导航堆栈中，准确的里程计至关重要，后续模块（如规划和控制）均依赖于机器人运动估计。现有的基于传感器的里程计方法在传感器类型和部署环境（从固态激光雷达到旋转激光雷达，从城市驾驶到非结构化自然环境）方面，往往缺乏足够的鲁棒性。", "method": "该研究提出了一种不依赖于传感器特定模型的激光雷达-惯性里程计系统。它采用简化的运动模型进行IMU数据积分，并直接使用“扫描到地图”的方法配准激光雷达扫描。此外，该方法在激光雷达配准上施加了一种新颖的正则化，以提高整体里程计性能，这与传统的卡尔曼滤波或因子图框架中迭代整合IMU数据并结合特征提取的激光雷达扫描匹配方法不同。", "result": "通过在涵盖多种常用机器人传感器和平台的大量数据集上进行广泛实验，结果表明，该方法在所有场景中均可使用完全相同的配置，展现了其卓越的鲁棒性。研究团队已开源其实现，以便社区在此基础上进一步开发和使用。", "conclusion": "该论文成功开发并验证了一个鲁棒且通用的激光雷达-惯性里程计系统，该系统无需传感器特定建模，能够在各种传感器类型和复杂环境中保持一致的性能，为机器人导航提供了可靠的运动估计解决方案。"}}
{"id": "2509.05652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05652", "abs": "https://arxiv.org/abs/2509.05652", "authors": ["Ha Meem Hossain", "Pritam Nath", "Mahitun Nesa Mahi", "Imtiaz Uddin", "Ishrat Jahan Eiste", "Syed Nasibur Rahman Ratul", "Md Naim Uddin Mozumdar", "Asif Mohammed Saad"], "title": "Evaluating YOLO Architectures: Implications for Real-Time Vehicle Detection in Urban Environments of Bangladesh", "comment": null, "summary": "Vehicle detection systems trained on Non-Bangladeshi datasets struggle to\naccurately identify local vehicle types in Bangladesh's unique road\nenvironments, creating critical gaps in autonomous driving technology for\ndeveloping regions. This study evaluates six YOLO model variants on a custom\ndataset featuring 29 distinct vehicle classes, including region-specific\nvehicles such as ``Desi Nosimon'', ``Leguna'', ``Battery Rickshaw'', and\n``CNG''. The dataset comprises high-resolution images (1920x1080) captured\nacross various Bangladeshi roads using mobile phone cameras and manually\nannotated using LabelImg with YOLO format bounding boxes. Performance\nevaluation revealed YOLOv11x as the top performer, achieving 63.7\\% mAP@0.5,\n43.8\\% mAP@0.5:0.95, 61.4\\% recall, and 61.6\\% F1-score, though requiring 45.8\nmilliseconds per image for inference. Medium variants (YOLOv8m, YOLOv11m)\nstruck an optimal balance, delivering robust detection performance with mAP@0.5\nvalues of 62.5\\% and 61.8\\% respectively, while maintaining moderate inference\ntimes around 14-15 milliseconds. The study identified significant detection\nchallenges for rare vehicle classes, with Construction Vehicles and Desi\nNosimons showing near-zero accuracy due to dataset imbalances and insufficient\ntraining samples. Confusion matrices revealed frequent misclassifications\nbetween visually similar vehicles, particularly Mini Trucks versus Mini Covered\nVans. This research provides a foundation for developing robust object\ndetection systems specifically adapted to Bangladesh traffic conditions,\naddressing critical needs in autonomous vehicle technology advancement for\ndeveloping regions where conventional generic-trained models fail to perform\nadequately.", "AI": {"tldr": "本研究评估了六种YOLO模型变体在孟加拉国定制车辆数据集上的表现，旨在解决现有模型在当地独特交通环境中识别不足的问题。YOLOv11x表现最佳，但YOLOv8m和YOLOv11m在性能和推理时间之间取得了最佳平衡，同时指出了稀有车辆和视觉相似车辆的检测挑战。", "motivation": "在非孟加拉国数据集上训练的车辆检测系统难以准确识别孟加拉国独特的道路环境中的本地车辆类型，这给发展中地区的自动驾驶技术带来了关键空白。", "method": "本研究在一个包含29种独特车辆类别（包括“Desi Nosimon”、“Leguna”、“Battery Rickshaw”和“CNG”等区域特定车辆）的定制数据集上评估了六种YOLO模型变体。该数据集包含使用手机摄像头在孟加拉国各地道路上捕获的高分辨率图像（1920x1080），并使用LabelImg手动标注为YOLO格式的边界框。", "result": "性能评估显示，YOLOv11x表现最佳，mAP@0.5达到63.7%，mAP@0.5:0.95达到43.8%，召回率为61.4%，F1-score为61.6%，但每张图像推理时间为45.8毫秒。中等变体（YOLOv8m、YOLOv11m）实现了最佳平衡，mAP@0.5分别为62.5%和61.8%，推理时间保持在14-15毫秒左右。研究发现，由于数据集不平衡和训练样本不足，稀有车辆类别（如工程车辆和Desi Nosimons）的检测精度接近零。混淆矩阵显示，视觉相似的车辆（特别是小型卡车与小型厢式货车）之间存在频繁的误分类。", "conclusion": "本研究为开发专门适应孟加拉国交通条件的鲁棒目标检测系统奠定了基础，解决了发展中地区自动驾驶技术进步中的关键需求，因为传统通用训练模型在这些地区表现不佳。"}}
{"id": "2509.06463", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06463", "abs": "https://arxiv.org/abs/2509.06463", "authors": ["Chengwei Wu", "Li Du", "Hanyu Zhao", "Yiming Ju", "Jiapu Wang", "Tengfei Pan"], "title": "Accelerate Scaling of LLM Alignment via Quantifying the Coverage and Depth of Instruction Set", "comment": null, "summary": "With the growing demand for applying large language models to downstream\ntasks, improving model alignment performance and efficiency has become crucial.\nSuch a process involves selecting informative instructions from a candidate\npool. However, due to the complexity of instruction set distributions, the key\nfactors driving the performance of aligned models remain unclear. As a result,\ncurrent instruction set refinement methods fail to improve performance as the\ninstruction pool expands continuously. To address this issue, we first\ninvestigate the key factors that influence the relationship between instruction\ndataset distribution and aligned model performance. Based on these insights, we\npropose a novel instruction data selection method. We identify that the depth\nof instructions and the coverage of the semantic space are the crucial factors\ndetermining downstream performance, which could explain over 70\\% of the model\nloss on the development set. We then design an instruction selection algorithm\nto simultaneously maximize the depth and semantic coverage of the selected\ninstructions. Experimental results demonstrate that, compared to\nstate-of-the-art baseline methods, it can sustainably improve model performance\nat a faster pace and thus achieve \\emph{``Accelerated Scaling''}.", "AI": {"tldr": "本文提出了一种新的指令数据选择方法，通过识别指令深度和语义空间覆盖率是影响大语言模型对齐性能的关键因素，从而实现了模型性能的“加速扩展”。", "motivation": "随着大语言模型在下游任务中应用需求的增长，提高模型对齐性能和效率至关重要。然而，由于指令集分布的复杂性，驱动对齐模型性能的关键因素尚不明确，导致当前指令集优化方法在指令池持续扩展时无法有效提升性能。", "method": "研究首先调查了影响指令数据集分布与对齐模型性能之间关系的关键因素。在此基础上，提出了一种新颖的指令数据选择方法。该方法识别出指令深度和语义空间覆盖率是决定下游性能的关键因素，并设计了一种算法来同时最大化所选指令的深度和语义覆盖。", "result": "研究发现指令深度和语义空间覆盖率是决定下游性能的关键因素，可以解释开发集上超过70%的模型损失。实验结果表明，与最先进的基线方法相比，所提出的方法能以更快的速度持续提高模型性能。", "conclusion": "通过优化指令选择，同时最大化指令深度和语义覆盖，本文提出的方法能够持续、快速地提升模型性能，从而实现“加速扩展”。"}}
{"id": "2509.06277", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06277", "abs": "https://arxiv.org/abs/2509.06277", "authors": ["Jinju Kim", "Taehan Kim", "Abdul Waheed", "Rita Singh"], "title": "No Encore: Unlearning as Opt-Out in Music Generation", "comment": "Work in progress. 7 pages", "summary": "AI music generation is rapidly emerging in the creative industries, enabling\nintuitive music generation from textual descriptions. However, these systems\npose risks in exploitation of copyrighted creations, raising ethical and legal\nconcerns. In this paper, we present preliminary results on the first\napplication of machine unlearning techniques from an ongoing research to\nprevent inadvertent usage of creative content. Particularly, we explore\nexisting methods in machine unlearning to a pre-trained Text-to-Music (TTM)\nbaseline and analyze their efficacy in unlearning pre-trained datasets without\nharming model performance. Through our experiments, we provide insights into\nthe challenges of applying unlearning in music generation, offering a\nfoundational analysis for future works on the application of unlearning for\nmusic generative models.", "AI": {"tldr": "本文初步探讨了将机器学习遗忘技术应用于文本到音乐（TTM）生成模型，以解决版权内容滥用问题，并分析了其有效性和挑战。", "motivation": "AI音乐生成技术迅速发展，但存在滥用受版权保护作品的风险，引发了伦理和法律担忧。研究旨在通过技术手段防止这种无意的使用。", "method": "研究将现有的机器学习遗忘方法应用于一个预训练的文本到音乐（TTM）基线模型。分析了这些方法在遗忘预训练数据集方面的有效性，同时力求不损害模型的整体性能。", "result": "研究展示了将机器学习遗忘技术应用于预防创意内容无意使用的初步成果。提供了在音乐生成领域应用遗忘技术所面临挑战的见解，并为未来在此类模型上应用遗忘技术奠定了基础分析。", "conclusion": "机器学习遗忘技术有望解决AI音乐生成中的版权滥用问题。尽管存在挑战，但这项初步研究为未来在音乐生成模型中实施遗忘机制提供了重要的基础和方向。"}}
{"id": "2509.06597", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06597", "abs": "https://arxiv.org/abs/2509.06597", "authors": ["Frederik Plahl", "Georgios Katranis", "Ilshat Mamaev", "Andrey Morozov"], "title": "LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods", "comment": "Preprint of final paper that will appear in the Proceedings of the\n  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS\n  2025)", "summary": "We present LiHRA, a novel dataset designed to facilitate the development of\nautomated, learning-based, or classical risk monitoring (RM) methods for\nHuman-Robot Interaction (HRI) scenarios. The growing prevalence of\ncollaborative robots in industrial environments has increased the need for\nreliable safety systems. However, the lack of high-quality datasets that\ncapture realistic human-robot interactions, including potentially dangerous\nevents, slows development. LiHRA addresses this challenge by providing a\ncomprehensive, multi-modal dataset combining 3D LiDAR point clouds, human body\nkeypoints, and robot joint states, capturing the complete spatial and dynamic\ncontext of human-robot collaboration. This combination of modalities allows for\nprecise tracking of human movement, robot actions, and environmental\nconditions, enabling accurate RM during collaborative tasks. The LiHRA dataset\ncovers six representative HRI scenarios involving collaborative and coexistent\ntasks, object handovers, and surface polishing, with safe and hazardous\nversions of each scenario. In total, the data set includes 4,431 labeled point\nclouds recorded at 10 Hz, providing a rich resource for training and\nbenchmarking classical and AI-driven RM algorithms. Finally, to demonstrate\nLiHRA's utility, we introduce an RM method that quantifies the risk level in\neach scenario over time. This method leverages contextual information,\nincluding robot states and the dynamic model of the robot. With its combination\nof high-resolution LiDAR data, precise human tracking, robot state data, and\nrealistic collision events, LiHRA offers an essential foundation for future\nresearch into real-time RM and adaptive safety strategies in human-robot\nworkspaces.", "AI": {"tldr": "LiHRA是一个新的多模态数据集，旨在促进人机交互（HRI）场景中自动化风险监测（RM）方法的发展，它结合了3D LiDAR、人体关键点和机器人关节状态，包含安全和危险两种情景。", "motivation": "协作机器人在工业环境中日益普及，对可靠安全系统的需求增加。然而，缺乏高质量的数据集来捕捉真实的人机交互，包括潜在危险事件，阻碍了相关方法的发展。", "method": "LiHRA数据集通过结合3D LiDAR点云、人体关键点和机器人关节状态等多模态数据来解决这一挑战，全面捕捉人机协作的空间和动态背景。该数据集涵盖了六种代表性HRI场景（协作、共存、物体交接和表面抛光），每种场景都有安全和危险版本。总共包含4,431个以10 Hz记录的标记点云。此外，论文还引入了一种利用机器人状态和动态模型来量化风险水平的RM方法，以展示LiHRA的实用性。", "result": "LiHRA数据集为训练和基准测试经典及AI驱动的RM算法提供了丰富的资源。论文中展示的RM方法能够随时间量化每个场景中的风险水平，并利用了上下文信息。", "conclusion": "LiHRA数据集结合了高分辨率LiDAR数据、精确的人体跟踪、机器人状态数据和真实的碰撞事件，为人机工作空间中实时风险监测和自适应安全策略的未来研究奠定了重要基础。"}}
{"id": "2509.05659", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05659", "abs": "https://arxiv.org/abs/2509.05659", "authors": ["Guandong Li", "Zhaobin Chu"], "title": "EditIDv2: Editable ID Customization with Data-Lubricated ID Feature Integration for Text-to-Image Generation", "comment": null, "summary": "We propose EditIDv2, a tuning-free solution specifically designed for\nhigh-complexity narrative scenes and long text inputs. Existing character\nediting methods perform well under simple prompts, but often suffer from\ndegraded editing capabilities, semantic understanding biases, and identity\nconsistency breakdowns when faced with long text narratives containing multiple\nsemantic layers, temporal logic, and complex contextual relationships. In\nEditID, we analyzed the impact of the ID integration module on editability. In\nEditIDv2, we further explore and address the influence of the ID feature\nintegration module. The core of EditIDv2 is to discuss the issue of editability\ninjection under minimal data lubrication. Through a sophisticated decomposition\nof PerceiverAttention, the introduction of ID loss and joint dynamic training\nwith the diffusion model, as well as an offline fusion strategy for the\nintegration module, we achieve deep, multi-level semantic editing while\nmaintaining identity consistency in complex narrative environments using only a\nsmall amount of data lubrication. This meets the demands of long prompts and\nhigh-quality image generation, and achieves excellent results in the IBench\nevaluation.", "AI": {"tldr": "EditIDv2 是一种免调优的解决方案，专为高复杂性叙事场景和长文本输入设计，旨在通过深度语义编辑同时保持角色身份一致性。", "motivation": "现有角色编辑方法在简单提示下表现良好，但在面对包含多语义层、时间逻辑和复杂上下文关系的长文本叙事时，编辑能力下降，出现语义理解偏差和身份一致性崩溃的问题。", "method": "EditIDv2 通过深入探索ID特征集成模块的影响，分解 PerceiverAttention，引入ID损失，与扩散模型进行联合动态训练，并采用集成模块的离线融合策略，仅用少量数据即可实现。", "result": "该方法在复杂叙事环境中实现了深度、多层次的语义编辑，同时保持了身份一致性，满足了长提示和高质量图像生成的需求，并在IBench评估中取得了优异结果。", "conclusion": "EditIDv2 提供了一种免调优、高效的解决方案，成功解决了复杂叙事场景下角色编辑的挑战，实现了在保持身份一致性的同时进行深度语义编辑。"}}
{"id": "2509.06477", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06477", "abs": "https://arxiv.org/abs/2509.06477", "authors": ["Pengxiang Zhao", "Guangyi Liu", "Yaozhen Liang", "Weiqing He", "Zhengxi Lu", "Yuehao Huang", "Yaxuan Guo", "Kexin Zhang", "Hao Wang", "Liang Liu", "Yong Liu"], "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents", "comment": null, "summary": "To enhance the efficiency of GUI agents on various platforms like smartphones\nand computers, a hybrid paradigm that combines flexible GUI operations with\nefficient shortcuts (e.g., API, deep links) is emerging as a promising\ndirection. However, a framework for systematically benchmarking these hybrid\nagents is still underexplored. To take the first step in bridging this gap, we\nintroduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut\nhybrid agents with a specific focus on the mobile domain. Beyond merely using\npredefined shortcuts, MAS-Bench assesses an agent's capability to autonomously\ngenerate shortcuts by discovering and creating reusable, low-cost workflows. It\nfeatures 139 complex tasks across 11 real-world applications, a knowledge base\nof 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation\nmetrics. The tasks are designed to be solvable via GUI-only operations, but can\nbe significantly accelerated by intelligently embedding shortcuts. Experiments\nshow that hybrid agents achieve significantly higher success rates and\nefficiency than their GUI-only counterparts. This result also demonstrates the\neffectiveness of our method for evaluating an agent's shortcut generation\ncapabilities. MAS-Bench fills a critical evaluation gap, providing a\nfoundational platform for future advancements in creating more efficient and\nrobust intelligent agents.", "AI": {"tldr": "本论文介绍了MAS-Bench，这是一个开创性的基准测试平台，用于评估GUI-快捷方式混合智能体（特别是在移动领域）的性能，并着重于其自主生成快捷方式的能力。", "motivation": "为了提高智能手机和电脑上GUI智能体的效率，结合灵活GUI操作和高效快捷方式（如API、深度链接）的混合范式正成为一个有前景的方向。然而，目前缺乏一个系统性基准框架来评估这类混合智能体，尤其是在自主生成快捷方式方面。", "method": "研究人员推出了MAS-Bench基准测试平台，它包含11个真实应用中的139个复杂任务、一个包含88个预定义快捷方式（API、深度链接、RPA脚本）的知识库，以及7个评估指标。该平台不仅评估智能体使用预定义快捷方式的能力，还评估其通过发现和创建可复用、低成本工作流来自主生成快捷方式的能力。", "result": "实验结果表明，混合智能体比仅使用GUI操作的智能体在成功率和效率上显著更高。这同时也证明了MAS-Bench在评估智能体快捷方式生成能力方面的有效性。", "conclusion": "MAS-Bench填补了混合智能体评估领域的一个关键空白，为未来开发更高效、更鲁棒的智能代理提供了基础平台。"}}
{"id": "2509.06350", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06350", "abs": "https://arxiv.org/abs/2509.06350", "authors": ["Junjie Mu", "Zonghao Ying", "Zhekui Fan", "Zonglei Jing", "Yaoyuan Zhang", "Zhengmin Yu", "Wenxin Zhang", "Quanchen Zou", "Xiangzheng Zhang"], "title": "Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?", "comment": null, "summary": "Jailbreak attacks on Large Language Models (LLMs) have demonstrated various\nsuccessful methods whereby attackers manipulate models into generating harmful\nresponses that they are designed to avoid. Among these, Greedy Coordinate\nGradient (GCG) has emerged as a general and effective approach that optimizes\nthe tokens in a suffix to generate jailbreakable prompts. While several\nimproved variants of GCG have been proposed, they all rely on fixed-length\nsuffixes. However, the potential redundancy within these suffixes remains\nunexplored. In this work, we propose Mask-GCG, a plug-and-play method that\nemploys learnable token masking to identify impactful tokens within the suffix.\nOur approach increases the update probability for tokens at high-impact\npositions while pruning those at low-impact positions. This pruning not only\nreduces redundancy but also decreases the size of the gradient space, thereby\nlowering computational overhead and shortening the time required to achieve\nsuccessful attacks compared to GCG. We evaluate Mask-GCG by applying it to the\noriginal GCG and several improved variants. Experimental results show that most\ntokens in the suffix contribute significantly to attack success, and pruning a\nminority of low-impact tokens does not affect the loss values or compromise the\nattack success rate (ASR), thereby revealing token redundancy in LLM prompts.\nOur findings provide insights for developing efficient and interpretable LLMs\nfrom the perspective of jailbreak attacks.", "AI": {"tldr": "本文提出了 Mask-GCG，一种通过可学习的 token 掩码来识别和修剪越狱攻击后缀中低影响力 token 的方法。它减少了冗余，降低了计算开销，并提高了攻击效率，同时揭示了大型语言模型越狱提示中的 token 冗余。", "motivation": "现有的贪婪坐标梯度 (GCG) 及其变体在越狱攻击中均依赖固定长度的后缀，但这些后缀中可能存在的冗余尚未被探索，这导致了不必要的计算开销。", "method": "Mask-GCG 是一种即插即用的方法，它采用可学习的 token 掩码来识别后缀中具有影响力的 token。该方法增加了高影响力位置 token 的更新概率，并修剪了低影响力位置的 token。这不仅减少了冗余，还缩小了梯度空间，从而降低了计算开销和攻击成功所需的时间。", "result": "实验结果表明，后缀中的大多数 token 对攻击成功有显著贡献，但修剪少数低影响力 token 不会影响损失值或损害攻击成功率。这揭示了大型语言模型提示中的 token 冗余。Mask-GCG 与 GCG 相比，降低了计算开销并缩短了攻击时间。", "conclusion": "Mask-GCG 通过识别和修剪越狱提示中的冗余 token，为开发高效且可解释的大型语言模型提供了新的视角和见解。"}}
{"id": "2509.06644", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06644", "abs": "https://arxiv.org/abs/2509.06644", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "T-araVLN: Translator for Agricultural Robotic Agents on Vision-and-Language Navigation", "comment": null, "summary": "Agricultural robotic agents have been becoming powerful helpers in a wide\nrange of agricultural tasks, nevertheless, still heavily rely on manual\noperation or untransportable railway for movement. The AgriVLN method and the\nA2A benchmark pioneeringly extend Vision-and-Language Navigation (VLN) to the\nagricultural domain, enabling agents navigate to the target position following\nthe natural language instructions. AgriVLN effectively understands the simple\ninstructions, however, often misunderstands the complicated instructions. To\nbridge this gap, we propose the method of Translator for Agricultural Robotic\nAgents on Vision-and-Language Navigation (T-araVLN), in which the Instruction\nTranslator module translates the original instruction to be both refined and\nprecise. Being evaluated on the A2A benchmark, our T-araVLN effectively\nimproves SR from 0.47 to 0.63 and reduces NE from 2.91m to 2.28m, demonstrating\nthe state-of-the-art performance in the agricultural domain. Code:\nhttps://github.com/AlexTraveling/T-araVLN.", "AI": {"tldr": "本文提出T-araVLN方法，通过指令翻译模块改进农业机器人视觉语言导航（VLN），有效解决了现有方法在处理复杂指令时的理解问题，并在A2A基准上实现了最先进的性能。", "motivation": "农业机器人虽然功能强大，但在移动上仍高度依赖手动操作或固定轨道。现有的农业领域VLN方法（如AgriVLN）虽然能理解简单指令，但往往会误解复杂指令，这限制了其在实际应用中的自主导航能力。", "method": "作者提出了T-araVLN（Translator for Agricultural Robotic Agents on Vision-and-Language Navigation）方法。其核心是“指令翻译模块”（Instruction Translator），该模块能够将原始指令翻译成更精炼和精确的版本，从而提高机器人对复杂指令的理解能力。", "result": "在A2A基准测试中，T-araVLN显著提升了成功率（SR）从0.47到0.63，并将导航误差（NE）从2.91米降低到2.28米。这些结果表明该方法在农业领域实现了最先进的性能。", "conclusion": "T-araVLN通过引入指令翻译模块，有效弥补了现有农业VLN方法在处理复杂指令方面的不足，显著提高了农业机器人的自主导航能力，并在相关基准上展现了卓越的性能。"}}
{"id": "2509.05661", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05661", "abs": "https://arxiv.org/abs/2509.05661", "authors": ["Xiaomeng Zhu", "Changwei Wang", "Haozhe Wang", "Xinyu Liu", "Fangzhen Lin"], "title": "OOTSM: A Decoupled Linguistic Framework for Effective Scene Graph Anticipation", "comment": null, "summary": "A scene graph is a structured represention of objects and their relationships\nin a scene. Scene Graph Anticipation (SGA) involves predicting future scene\ngraphs from video clips, enabling applications as intelligent surveillance and\nhuman-machine collaboration. Existing SGA approaches primarily leverage visual\ncues, often struggling to integrate valuable commonsense knowledge, thereby\nlimiting long-term prediction robustness. To explicitly leverage such\ncommonsense knowledge, we propose a new approach to better understand the\nobjects, concepts, and relationships in a scene graph. Our approach decouples\nthe SGA task in two steps: first a scene graph capturing model is used to\nconvert a video clip into a sequence of scene graphs, then a pure text-based\nmodel is used to predict scene graphs in future frames. Our focus in this work\nis on the second step, and we call it Linguistic Scene Graph Anticipation\n(LSGA) and believes it should have independent interest beyond the use in SGA\ndiscussed here. For LSGA, we introduce an Object-Oriented Two-Staged Method\n(OOTSM) where an Large Language Model (LLM) first forecasts object appearances\nand disappearances before generating detailed human-object relations. We\nconduct extensive experiments to evaluate OOTSM in two settings. For LSGA, we\nevaluate our fine-tuned open-sourced LLMs against zero-shot APIs (i.e., GPT-4o,\nGPT-4o-mini, and DeepSeek-V3) on a benchmark constructed from Action Genome\nannotations. For SGA, we combine our OOTSM with STTran++ from, and our\nexperiments demonstrate effective state-of-the-art performance: short-term\nmean-Recall (@10) increases by 3.4% while long-term mean-Recall (@50) improves\ndramatically by 21.9%. Code is available at https://github.com/ZhuXMMM/OOTSM.", "AI": {"tldr": "该研究提出了一种名为OOTSM的纯文本方法，利用大型语言模型（LLM）预测未来场景图，通过解耦对象变化和关系生成，显著提升了场景图预测的长期准确性。", "motivation": "现有的场景图预测（SGA）方法主要依赖视觉线索，难以有效整合常识知识，导致长期预测的鲁棒性不足。", "method": "本研究将SGA任务解耦为两步：首先将视频转换为场景图序列，然后使用纯文本模型预测未来场景图（称为语言场景图预测，LSGA）。针对LSGA，提出了一种面向对象的两阶段方法（OOTSM），其中大型语言模型（LLM）首先预测对象的出现和消失，然后再生成详细的人-物关系。该方法在LSGA和SGA两种设置下进行了评估。", "result": "在LSGA方面，将微调的开源LLM与零样本API（如GPT-4o）在Action Genome构建的基准上进行了评估。在SGA方面，将OOTSM与STTran++结合，实验结果显示了最先进的性能：短期平均召回率（@10）提高了3.4%，长期平均召回率（@50）显著提高了21.9%。", "conclusion": "该研究提出的OOTSM方法通过有效利用LLM中的常识知识，显著提升了场景图预测的性能，特别是在长期预测方面取得了显著的改进，证明了纯文本模型在场景图预测中的潜力。"}}
{"id": "2509.06490", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06490", "abs": "https://arxiv.org/abs/2509.06490", "authors": ["Niki Kotecha", "Ehecatl Antonio del Rio Chanona"], "title": "MORSE: Multi-Objective Reinforcement Learning via Strategy Evolution for Supply Chain Optimization", "comment": null, "summary": "In supply chain management, decision-making often involves balancing multiple\nconflicting objectives, such as cost reduction, service level improvement, and\nenvironmental sustainability. Traditional multi-objective optimization methods,\nsuch as linear programming and evolutionary algorithms, struggle to adapt in\nreal-time to the dynamic nature of supply chains. In this paper, we propose an\napproach that combines Reinforcement Learning (RL) and Multi-Objective\nEvolutionary Algorithms (MOEAs) to address these challenges for dynamic\nmulti-objective optimization under uncertainty. Our method leverages MOEAs to\nsearch the parameter space of policy neural networks, generating a Pareto front\nof policies. This provides decision-makers with a diverse population of\npolicies that can be dynamically switched based on the current system\nobjectives, ensuring flexibility and adaptability in real-time decision-making.\nWe also introduce Conditional Value-at-Risk (CVaR) to incorporate\nrisk-sensitive decision-making, enhancing resilience in uncertain environments.\nWe demonstrate the effectiveness of our approach through case studies,\nshowcasing its ability to respond to supply chain dynamics and outperforming\nstate-of-the-art methods in an inventory management case study. The proposed\nstrategy not only improves decision-making efficiency but also offers a more\nrobust framework for managing uncertainty and optimizing performance in supply\nchains.", "AI": {"tldr": "本文提出了一种结合强化学习（RL）和多目标进化算法（MOEAs）的方法，用于解决不确定性下动态多目标供应链优化问题，并引入CVaR以实现风险敏感决策。", "motivation": "在供应链管理中，决策需要平衡成本、服务水平和环境可持续性等多个冲突目标。传统的优化方法难以实时适应供应链的动态性。", "method": "该方法结合了强化学习（RL）和多目标进化算法（MOEAs）。MOEAs用于搜索策略神经网络的参数空间，生成一系列帕累托最优策略。这些策略可以根据当前系统目标动态切换。同时，引入条件风险价值（CVaR）以实现风险敏感决策。", "result": "通过案例研究证明了该方法的有效性，展示了其响应供应链动态的能力，并在库存管理案例中优于现有最先进的方法。", "conclusion": "所提出的策略不仅提高了决策效率，还为管理供应链中的不确定性和优化性能提供了一个更强大的框架。"}}
{"id": "2509.06356", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06356", "abs": "https://arxiv.org/abs/2509.06356", "authors": ["Ao Chang", "Yubo Chen", "Jun Zhao"], "title": "PL-CA: A Parametric Legal Case Augmentation Framework", "comment": null, "summary": "Conventional RAG is considered one of the most effective methods for\naddressing model knowledge insufficiency and hallucination, particularly in the\njudicial domain that requires high levels of knowledge rigor, logical\nconsistency, and content integrity. However, the conventional RAG method only\ninjects retrieved documents directly into the model's context, which severely\nconstrains models due to their limited context windows and introduces\nadditional computational overhead through excessively long contexts, thereby\ndisrupting models' attention and degrading performance on downstream tasks.\nMoreover, many existing benchmarks lack expert annotation and focus solely on\nindividual downstream tasks while real-world legal scenarios consist of\nmultiple mixed legal tasks, indicating conventional benchmarks' inadequacy for\nreflecting models' true capabilities. To address these limitations, we propose\nPL-CA, which introduces a parametric RAG (P-RAG) framework to perform data\naugmentation on corpus knowledge and encode this legal knowledge into\nparametric vectors, and then integrates this parametric knowledge into the\nLLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context\npressure. Additionally, we also construct a multi-task legal dataset comprising\nmore than 2000 training and test instances, which are all expert-annotated and\nmanually verified. We conduct our experiments on our dataset, and the\nexperimental results demonstrate that our method reduces the overhead\nassociated with excessively long contexts while maintaining competitive\nperformance on downstream tasks compared to conventional RAG. Our code and\ndataset are provided in the appendix.", "AI": {"tldr": "本文提出了PL-CA，一种结合参数化RAG（P-RAG）和LoRA的方法，将法律知识编码为参数向量并集成到LLM中，以缓解传统RAG的上下文压力和计算开销。同时，构建了一个多任务、专家标注的法律数据集，实验证明PL-CA在保持性能的同时有效降低了长上下文的开销。", "motivation": "传统的RAG方法在法律领域虽有效，但存在以下局限：1) 将检索文档直接注入模型上下文，导致上下文窗口限制和计算开销，影响模型注意力并降低性能；2) 现有基准缺乏专家标注，且仅关注单一任务，无法反映真实世界法律场景中多任务混合的复杂性。", "method": "本文提出PL-CA方法，包括：1) 参数化RAG (P-RAG) 框架，用于对语料库知识进行数据增强，并将法律知识编码为参数向量；2) 通过LoRA将这些参数化知识集成到大型语言模型的前馈网络（FFN）中，从而缓解模型的上下文压力；3) 构建了一个包含2000多个训练和测试实例的多任务法律数据集，所有数据均经过专家标注和手动验证。", "result": "实验结果表明，与传统RAG相比，PL-CA方法在保持下游任务竞争性能的同时，显著减少了因过长上下文引起的计算开销。", "conclusion": "PL-CA通过将法律知识参数化并集成到LLM中，有效解决了传统RAG在法律领域面临的上下文压力和计算开销问题，为法律领域的知识增强型模型提供了一种更高效的解决方案。新构建的多任务法律数据集也为未来研究提供了宝贵资源。"}}
{"id": "2509.06768", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06768", "abs": "https://arxiv.org/abs/2509.06768", "authors": ["Oluwadamilola Sotomi", "Devika Kodi", "Kiruthiga Chandra Shekar", "Aliasghar Arab"], "title": "Embodied Hazard Mitigation using Vision-Language Models for Autonomous Mobile Robots", "comment": null, "summary": "Autonomous robots operating in dynamic environments should identify and\nreport anomalies. Embodying proactive mitigation improves safety and\noperational continuity. This paper presents a multimodal anomaly detection and\nmitigation system that integrates vision-language models and large language\nmodels to identify and report hazardous situations and conflicts in real-time.\nThe proposed system enables robots to perceive, interpret, report, and if\npossible respond to urban and environmental anomalies through proactive\ndetection mechanisms and automated mitigation actions. A key contribution in\nthis paper is the integration of Hazardous and Conflict states into the robot's\ndecision-making framework, where each anomaly type can trigger specific\nmitigation strategies. User studies (n = 30) demonstrated the effectiveness of\nthe system in anomaly detection with 91.2% prediction accuracy and relatively\nlow latency response times using edge-ai architecture.", "AI": {"tldr": "本文提出了一种多模态异常检测与缓解系统，通过集成视觉-语言模型和大型语言模型，使自主机器人在动态环境中实时识别、报告并主动缓解危险与冲突情况。", "motivation": "在动态环境中运行的自主机器人需要识别和报告异常，并采取主动缓解措施以提高安全性和操作连续性。", "method": "该系统整合了视觉-语言模型（VLM）和大型语言模型（LLM），用于实时识别和报告危险情况及冲突。它将“危险”和“冲突”状态集成到机器人的决策框架中，针对不同类型的异常触发特定的缓解策略。系统采用边缘AI架构。", "result": "用户研究（n=30）表明，该系统在异常检测方面有效，预测准确率达91.2%，且响应时间延迟相对较低。", "conclusion": "该系统使机器人能够通过主动检测机制和自动化缓解措施，感知、解释、报告并响应城市和环境中的异常，从而提高了机器人在复杂环境中的自主性和安全性。"}}
{"id": "2509.05662", "categories": ["cs.CV", "hep-ex"], "pdf": "https://arxiv.org/pdf/2509.05662", "abs": "https://arxiv.org/abs/2509.05662", "authors": ["Wasikul Islam"], "title": "WIPUNet: A Physics-inspired Network with Weighted Inductive Biases for Image Denoising", "comment": "13 pages, 4 figures", "summary": "In high-energy particle physics, collider measurements are contaminated by\n\"pileup\", overlapping soft interactions that obscure the hard-scatter signal of\ninterest. Dedicated subtraction strategies exploit physical priors such as\nconservation, locality, and isolation. Inspired by this analogy, we investigate\nhow such principles can inform image denoising by embedding physics-guided\ninductive biases into neural architectures. This paper is a proof of concept:\nrather than targeting state-of-the-art (SOTA) benchmarks, we ask whether\nphysics-inspired priors improve robustness under strong corruption.\n  We introduce a hierarchy of PU-inspired denoisers: a residual CNN with\nconservation constraints, its Gaussian-noise variants, and the Weighted\nInductive Pileup-physics-inspired U-Network for Denoising (WIPUNet), which\nintegrates these ideas into a UNet backbone. On CIFAR-10 with Gaussian noise at\n$\\sigma\\in\\{15,25,50,75,100\\}$, PU-inspired CNNs are competitive with standard\nbaselines, while WIPUNet shows a \\emph{widening margin} at higher noise.\nComplementary BSD500 experiments show the same trend, suggesting\nphysics-inspired priors provide stability where purely data-driven models\ndegrade. Our contributions are: (i) translating pileup-mitigation principles\ninto modular inductive biases; (ii) integrating them into UNet; and (iii)\ndemonstrating robustness gains at high noise without relying on heavy SOTA\nmachinery.", "AI": {"tldr": "本文探索了将高能物理中“堆积效应”的缓解原理（如守恒、局部性、隔离性）作为物理引导的归纳偏置嵌入到神经网络中，以提升图像去噪模型在强噪声下的鲁棒性。", "motivation": "高能粒子物理中的对撞机测量受到“堆积效应”的污染，需要专门的减法策略来利用物理先验。受此启发，研究者想探究这些物理原理如何通过将物理引导的归纳偏置嵌入神经网络架构来指导图像去噪，特别是在强腐蚀下的鲁棒性。", "method": "引入了一系列受堆积效应启发的去噪器：一个带有守恒约束的残差CNN，其高斯噪声变体，以及将这些思想整合到UNet骨干网络中的WIPUNet（Weighted Inductive Pileup-physics-inspired U-Network for Denoising）。在CIFAR-10和BSD500数据集上，使用不同强度的高斯噪声进行测试。", "result": "在CIFAR-10数据集上，受堆积效应启发的CNN与标准基线模型相比具有竞争力；WIPUNet在较高噪声水平下显示出“不断扩大的优势”。BSD500实验也显示出相同的趋势，表明物理启发的先验在纯数据驱动模型性能下降时提供了稳定性。", "conclusion": "物理启发的先验在图像去噪中，尤其是在高噪声条件下，能够提供稳定性，并提高了模型的鲁棒性，而无需依赖复杂的SOTA技术。这证明了将堆积效应缓解原理转化为模块化归纳偏置并整合到UNet中的有效性。"}}
{"id": "2509.06493", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06493", "abs": "https://arxiv.org/abs/2509.06493", "authors": ["Ran Xin", "Zeyu Zheng", "Yanchen Nie", "Kun Yuan", "Xia Xiao"], "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM Step-Provers", "comment": null, "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.", "AI": {"tldr": "本文介绍了BFS-Prover-V2系统，通过创新的多轮离策略强化学习框架和规划器增强的多智能体搜索架构，解决了大型语言模型在自动化定理证明中训练和推理扩展性的挑战，并在数学基准测试中取得了最先进的结果。", "motivation": "将大型语言模型（LLMs）集成到自动化定理证明中虽然前景广阔，但受到训练时强化学习（RL）和推理时计算扩展性挑战的根本限制。本研究旨在解决这一双重扩展性问题。", "method": "本文提出了两项主要创新：1. 一种新颖的多轮离策略强化学习框架，用于在训练时持续改进LLM步进证明器的性能，该框架受AlphaZero启发，采用多阶段专家迭代管道，具有自适应策略级数据过滤和周期性再训练。2. 一种规划器增强的多智能体搜索架构，用于在推理时扩展推理能力，该架构使用通用推理模型作为高级规划器，将复杂定理分解为一系列简单子目标，并通过共享证明缓存实现并行证明代理的协作。", "result": "BFS-Prover-V2在MiniF2F和ProofNet测试集上分别达到了95.08%和41.4%的准确率，在已建立的正式数学基准测试中取得了最先进的结果。", "conclusion": "该双重扩展方法在形式数学领域取得了显著成果。文中所提出的强化学习和推理技术具有广泛的兴趣，可应用于其他需要长周期多轮推理和复杂搜索的领域。"}}
{"id": "2509.06401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06401", "abs": "https://arxiv.org/abs/2509.06401", "authors": ["Ivan Martínez-Murillo", "Elena Lloret", "Paloma Moreda", "Albert Gatt"], "title": "Do LLMs exhibit the same commonsense capabilities across languages?", "comment": null, "summary": "This paper explores the multilingual commonsense generation abilities of\nLarge Language Models (LLMs). To facilitate this investigation, we introduce\nMULTICOM, a novel benchmark that extends the COCOTEROS dataset to four\nlanguages: English, Spanish, Dutch, and Valencian. The task involves generating\na commonsensical sentence that includes a given triplet of words. We evaluate a\nrange of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and\nSalamandra, on this benchmark. Our evaluation combines automatic metrics,\nLLM-as-a-judge approaches (using Prometheus and JudgeLM), and human\nannotations. Results consistently show superior performance in English, with\nsignificantly lower performance in less-resourced languages. While contextual\nsupport yields mixed results, it tends to benefit underrepresented languages.\nThese findings underscore the current limitations of LLMs in multilingual\ncommonsense generation. The dataset is publicly available at\nhttps://huggingface.co/datasets/gplsi/MULTICOM.", "AI": {"tldr": "本文通过引入MULTICOM基准测试，评估了大型语言模型（LLMs）在多语言常识生成方面的能力，发现LLMs在英语上表现优异，但在资源较少的语言上表现显著下降。", "motivation": "研究LLMs在多语言常识生成方面的能力。", "method": "引入了MULTICOM基准测试（将COCOTEROS数据集扩展到英语、西班牙语、荷兰语和瓦伦西亚语四种语言），任务是生成包含给定词语三元组的常识性句子。评估了包括LLaMA、Qwen、Gemma、EuroLLM和Salamandra在内的一系列开源LLMs，并结合了自动指标、LLM作为判官（使用Prometheus和JudgeLM）以及人工标注进行评估。", "result": "结果一致显示英语表现优异，而资源较少的语言表现显著较低。上下文支持产生了混合结果，但倾向于有益于代表性不足的语言。", "conclusion": "这些发现强调了LLMs在多语言常识生成方面当前的局限性。"}}
{"id": "2509.06819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06819", "abs": "https://arxiv.org/abs/2509.06819", "authors": ["Daniel San José Pro", "Oliver Hausdörfer", "Ralf Römer", "Maximilian Dösch", "Martin Schuck", "Angela P. Schöllig"], "title": "CRISP -- Compliant ROS2 Controllers for Learning-Based Manipulation Policies and Teleoperation", "comment": "5 pages, 5 figures", "summary": "Learning-based controllers, such as diffusion policies and vision-language\naction models, often generate low-frequency or discontinuous robot state\nchanges. Achieving smooth reference tracking requires a low-level controller\nthat converts high-level targets commands into joint torques, enabling\ncompliant behavior during contact interactions. We present CRISP, a lightweight\nC++ implementation of compliant Cartesian and joint-space controllers for the\nROS2 control standard, designed for seamless integration with high-level\nlearning-based policies as well as teleoperation. The controllers are\ncompatible with any manipulator that exposes a joint-torque interface. Through\nour Python and Gymnasium interfaces, CRISP provides a unified pipeline for\nrecording data from hardware and simulation and deploying high-level\nlearning-based policies seamlessly, facilitating rapid experimentation. The\nsystem has been validated on hardware with the Franka Robotics FR3 and in\nsimulation with the Kuka IIWA14 and Kinova Gen3. Designed for rapid\nintegration, flexible deployment, and real-time performance, our implementation\nprovides a unified pipeline for data collection and policy execution, lowering\nthe barrier to applying learning-based methods on ROS2-compatible manipulators.\nDetailed documentation is available at the project website -\nhttps://utiasDSL.github.io/crisp_controllers.", "AI": {"tldr": "CRISP是一个轻量级的C++顺应性笛卡尔和关节空间低级控制器，适用于ROS2机器人，旨在无缝集成高级学习策略和遥操作，提供统一的数据收集和策略执行流程。", "motivation": "基于学习的控制器（如扩散策略）通常生成低频或不连续的机器人状态变化，需要一个低级控制器将高级目标命令转换为关节力矩，以实现平滑的参考跟踪和接触交互中的顺应行为。", "method": "本文提出了CRISP，一个轻量级的C++实现，为ROS2控制标准设计了顺应性笛卡尔和关节空间控制器。它兼容任何暴露关节力矩接口的机械臂，并通过Python和Gymnasium接口提供统一的硬件和仿真数据记录以及高级学习策略部署管道。", "result": "CRISP已在Frank Robotics FR3硬件上以及Kuka IIWA14和Kinova Gen3仿真中得到验证。它提供了一个统一的管道，用于数据收集和策略执行，实现了快速集成、灵活部署和实时性能。", "conclusion": "CRISP通过提供一个统一的、易于集成的管道，降低了在ROS2兼容机械臂上应用基于学习方法的门槛，促进了快速实验，并支持顺应行为和实时性能。"}}
{"id": "2509.05669", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05669", "abs": "https://arxiv.org/abs/2509.05669", "authors": ["Weijie Shen", "Xinrui Wang", "Yuanqi Nie", "Apiradee Boonmee"], "title": "Context-Aware Multi-Turn Visual-Textual Reasoning in LVLMs via Dynamic Memory and Adaptive Visual Guidance", "comment": null, "summary": "Current Large Language Models (LLMs) and Vision-Language Large Models (LVLMs)\nexcel in single-turn tasks but face significant challenges in multi-turn\ninteractions requiring deep contextual understanding and complex visual\nreasoning, often leading to fragmented reasoning, context loss, and\nhallucinations. To address these limitations, we propose Context-Aware\nMulti-Turn Visual Reasoning (CAMVR), a novel framework designed to empower\nLVLMs with robust and coherent multi-turn visual-textual inference\ncapabilities. CAMVR introduces two key innovations: a Visual-Textual Context\nMemory Unit (VCMU), a dynamic read-write memory network that stores and manages\ncritical visual features, textual semantic representations, and their\ncross-modal correspondences from each interaction turn; and an Adaptive Visual\nFocus Guidance (AVFG) mechanism, which leverages the VCMU's context to\ndynamically adjust the visual encoder's attention to contextually relevant\nimage regions. Our multi-level reasoning integration strategy ensures that\nresponse generation is deeply coherent with both current inputs and accumulated\nhistorical context. Extensive experiments on challenging datasets, including\nVisDial, an adapted A-OKVQA, and our novel Multi-Turn Instruction Following\n(MTIF) dataset, demonstrate that CAMVR consistently achieves state-of-the-art\nperformance.", "AI": {"tldr": "本文提出了一种名为CAMVR（Context-Aware Multi-Turn Visual Reasoning）的新框架，旨在解决现有大语言模型和视觉-语言大模型在多轮交互中存在的上下文丢失、碎片化推理和幻觉问题。CAMVR通过引入视觉-文本上下文记忆单元（VCMU）和自适应视觉焦点引导（AVFG）机制，显著提升了模型在复杂多轮视觉推理任务中的表现，并达到了最先进的水平。", "motivation": "当前的LLMs和LVLMs在单轮任务中表现出色，但在需要深度上下文理解和复杂视觉推理的多轮交互中面临显著挑战，常常导致推理碎片化、上下文丢失和幻觉。", "method": "本文提出了CAMVR框架，包含两项关键创新：1. 视觉-文本上下文记忆单元（VCMU），一个动态读写记忆网络，用于存储和管理来自每个交互轮次的关键视觉特征、文本语义表示及其跨模态对应关系；2. 自适应视觉焦点引导（AVFG）机制，利用VCMU的上下文动态调整视觉编码器对上下文相关图像区域的注意力。此外，采用多级推理整合策略确保响应生成与当前输入和累积历史上下文深度连贯。", "result": "在VisDial、改编的A-OKVQA以及本文提出的新型Multi-Turn Instruction Following (MTIF) 数据集等挑战性数据集上进行的广泛实验表明，CAMVR持续实现了最先进的性能。", "conclusion": "CAMVR框架通过其创新的上下文记忆和自适应视觉焦点机制，有效地解决了LVLMs在多轮视觉-文本推理中的局限性，赋予了模型强大且连贯的多轮推理能力。"}}
{"id": "2509.06503", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.06503", "abs": "https://arxiv.org/abs/2509.06503", "authors": ["Eser Aygün", "Anastasiya Belyaeva", "Gheorghe Comanici", "Marc Coram", "Hao Cui", "Jake Garrison", "Renee Johnston Anton Kast", "Cory Y. McLean", "Peter Norgaard", "Zahra Shamsi", "David Smalling", "James Thompson", "Subhashini Venugopalan", "Brian P. Williams", "Chujun He", "Sarah Martinson", "Martyna Plomecka", "Lai Wei", "Yuchen Zhou", "Qian-Ze Zhu", "Matthew Abraham", "Erica Brand", "Anna Bulanova", "Jeffrey A. Cardille", "Chris Co", "Scott Ellsworth", "Grace Joseph", "Malcolm Kane", "Ryan Krueger", "Johan Kartiwa", "Dan Liebling", "Jan-Matthis Lueckmann", "Paul Raccuglia", "Xuefei", "Wang", "Katherine Chou", "James Manyika", "Yossi Matias", "John C. Platt", "Lizzie Dorfman", "Shibl Mourad", "Michael P. Brenner"], "title": "An AI system to help scientists write expert-level empirical software", "comment": "71 pages, 26 figures", "summary": "The cycle of scientific discovery is frequently bottlenecked by the slow,\nmanual creation of software to support computational experiments. To address\nthis, we present an AI system that creates expert-level scientific software\nwhose goal is to maximize a quality metric. The system uses a Large Language\nModel (LLM) and Tree Search (TS) to systematically improve the quality metric\nand intelligently navigate the large space of possible solutions. The system\nachieves expert-level results when it explores and integrates complex research\nideas from external sources. The effectiveness of tree search is demonstrated\nacross a wide range of benchmarks. In bioinformatics, it discovered 40 novel\nmethods for single-cell data analysis that outperformed the top human-developed\nmethods on a public leaderboard. In epidemiology, it generated 14 models that\noutperformed the CDC ensemble and all other individual models for forecasting\nCOVID-19 hospitalizations. Our method also produced state-of-the-art software\nfor geospatial analysis, neural activity prediction in zebrafish, time series\nforecasting and numerical solution of integrals. By devising and implementing\nnovel solutions to diverse tasks, the system represents a significant step\ntowards accelerating scientific progress.", "AI": {"tldr": "该研究提出一个结合大语言模型（LLM）和树搜索（TS）的AI系统，能自动创建专家级的科学软件，以最大化质量指标，并在多个科学领域超越人类开发的方法。", "motivation": "科学发现的周期常因计算实验所需软件的缓慢手动创建而受阻，这成为了一个瓶颈。", "method": "该系统利用大语言模型（LLM）和树搜索（TS）来系统性地提高质量指标，并智能地探索庞大的解决方案空间。它通过探索和整合来自外部来源的复杂研究思想来达到专家级水平。", "result": "该系统在多个基准测试中取得了专家级成果。在生物信息学领域，它发现了40种新颖的单细胞数据分析方法，超越了公共排行榜上顶尖的人类开发方法。在流行病学领域，它生成了14个模型，在预测COVID-19住院人数方面优于CDC集合模型和所有其他个体模型。此外，该方法还为地理空间分析、斑马鱼神经活动预测、时间序列预测和积分数值解生成了最先进的软件。", "conclusion": "通过为各种任务设计和实现新颖的解决方案，该系统代表着在加速科学进步方面迈出了重要一步。"}}
{"id": "2509.06501", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06501", "abs": "https://arxiv.org/abs/2509.06501", "authors": ["Junteng Liu", "Yunji Li", "Chi Zhang", "Jingyang Li", "Aili Chen", "Ke Ji", "Weiyu Cheng", "Zijia Wu", "Chengyu Du", "Qidi Xu", "Jiayuan Song", "Zhengmao Zhu", "Wenhu Chen", "Pengyu Zhao", "Junxian He"], "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents", "comment": null, "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.", "AI": {"tldr": "本文提出了一种名为WebExplorer的数据生成方法，用于创建具有挑战性的信息检索查询-答案对。基于此数据，他们开发了WebExplorer-8B，一个8B参数的先进网络代理，它在多个信息检索基准测试中取得了当前最佳性能，甚至超越了更大的模型，为长程网络代理提供了一条实用路径。", "motivation": "现有开源网络代理在复杂任务上的信息检索能力有限，或缺乏透明的实现。作者认为主要挑战在于缺乏用于信息检索的具有挑战性的数据。", "method": "本文引入了WebExplorer，一种系统性的数据生成方法，利用基于模型的探索和迭代的“长到短”查询演变，创建需要多步推理和复杂网络导航的查询-答案对。利用这些高质量数据，通过监督微调（SFT）和强化学习（RL）相结合的方式，成功开发了先进的网络代理WebExplorer-8B。", "result": "WebExplorer-8B支持128K上下文长度和多达100个工具调用轮次，实现了长程问题解决。在多种信息检索基准测试中，WebExplorer-8B在其规模上取得了当前最佳性能。作为一个8B模型，它在RL训练后平均能够进行16轮搜索，在BrowseComp-en/zh上实现了比WebSailor-72B更高的准确性，并在WebWalkerQA和FRAMES上达到了100B参数以下模型中的最佳性能。此外，模型在HLE基准测试上也表现出强大的泛化能力。", "conclusion": "研究结果表明，本文提出的方法是实现长程网络代理的实用途径。"}}
{"id": "2509.06882", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06882", "abs": "https://arxiv.org/abs/2509.06882", "authors": ["Zhiheng Chen", "Wei Wang"], "title": "Dynamic Modeling and Efficient Data-Driven Optimal Control for Micro Autonomous Surface Vehicles", "comment": "This work has been accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025", "summary": "Micro Autonomous Surface Vehicles (MicroASVs) offer significant potential for\noperations in confined or shallow waters and swarm robotics applications.\nHowever, achieving precise and robust control at such small scales remains\nhighly challenging, mainly due to the complexity of modeling nonlinear\nhydrodynamic forces and the increased sensitivity to self-motion effects and\nenvironmental disturbances, including waves and boundary effects in confined\nspaces. This paper presents a physics-driven dynamics model for an\nover-actuated MicroASV and introduces a data-driven optimal control framework\nthat leverages a weak formulation-based online model learning method. Our\napproach continuously refines the physics-driven model in real time, enabling\nadaptive control that adjusts to changing system parameters. Simulation results\ndemonstrate that the proposed method substantially enhances trajectory tracking\naccuracy and robustness, even under unknown payloads and external disturbances.\nThese findings highlight the potential of data-driven online learning-based\noptimal control to improve MicroASV performance, paving the way for more\nreliable and precise autonomous surface vehicle operations.", "AI": {"tldr": "本文提出了一种结合物理驱动动力学模型和数据驱动在线学习的优化控制框架，显著提高了微型自主水面航行器（MicroASV）在复杂环境下的轨迹跟踪精度和鲁棒性。", "motivation": "微型自主水面航行器（MicroASV）在狭窄或浅水域以及集群机器人应用中具有巨大潜力，但由于非线性水动力建模复杂、对自运动效应和环境干扰（如波浪、边界效应）敏感，实现精确和鲁棒的控制极具挑战性。", "method": "本文提出了一个过驱动MicroASV的物理驱动动力学模型，并引入了一个数据驱动的优化控制框架。该框架利用基于弱形式的在线模型学习方法，能够实时细化物理驱动模型，实现自适应控制以适应变化的系统参数。", "result": "仿真结果表明，所提出的方法显著提高了轨迹跟踪精度和鲁棒性，即使在未知载荷和外部干扰条件下也能表现出色。", "conclusion": "研究结果突出了数据驱动在线学习优化控制在提升MicroASV性能方面的潜力，为实现更可靠和精确的自主水面航行器操作铺平了道路。"}}
{"id": "2509.05670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05670", "abs": "https://arxiv.org/abs/2509.05670", "authors": ["Gašper Podobnik", "Tomaž Vrtovec"], "title": "MeshMetrics: A Precise Implementation of Distance-Based Image Segmentation Metrics", "comment": null, "summary": "The surge of research in image segmentation has yielded remarkable\nperformance gains but also exposed a reproducibility crisis. A major\ncontributor is performance evaluation, where both selection and implementation\nof metrics play critical roles. While recent efforts have improved the former,\nthe reliability of metric implementation has received far less attention.\nPitfalls in distance-based metric implementation can lead to considerable\ndiscrepancies between common open-source tools, for instance, exceeding 100 mm\nfor the Hausdorff distance and 30%pt for the normalized surface distance for\nthe same pair of segmentations. To address these pitfalls, we introduce\nMeshMetrics, a mesh-based framework that provides a more precise computation of\ndistance-based metrics than conventional grid-based approaches. Through\ntheoretical analysis and empirical validation, we demonstrate that MeshMetrics\nachieves higher accuracy and precision than established tools, and is\nsubstantially less affected by discretization artifacts, such as distance\nquantization. We release MeshMetrics as an open-source Python package,\navailable at https://github.com/gasperpodobnik/MeshMetrics.", "AI": {"tldr": "该研究指出图像分割领域因度量实现问题导致的可复现性危机，并提出了MeshMetrics，一个基于网格的框架，用于更精确地计算距离度量，以提高评估的准确性和可靠性。", "motivation": "图像分割研究的性能评估中，度量选择和实现是关键。尽管度量选择有所改进，但度量实现的可靠性却被忽视，导致现有开源工具在距离度量计算上存在显著差异，影响了研究的可复现性。", "method": "引入了MeshMetrics，一个基于网格的框架，用于计算距离度量。该方法通过理论分析和实证验证，证明其比传统的基于网格的方法能更精确地计算距离度量。", "result": "MeshMetrics比现有工具实现了更高的准确性和精度，并且显著减少了离散化伪影（如距离量化）的影响。研究发现，现有开源工具在相同分割对上，豪斯多夫距离差异可超过100毫米，归一化表面距离差异可超过30%点。", "conclusion": "MeshMetrics提供了一种比传统方法更精确、更可靠的距离度量计算方式，有效解决了图像分割领域中度量实现导致的复现性问题。该工具已作为开源Python包发布。"}}
{"id": "2509.06641", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06641", "abs": "https://arxiv.org/abs/2509.06641", "authors": ["Zhou-Peng Shou", "Zhi-Qiang You", "Fang Wang", "Hai-Bo Liu"], "title": "CogGuide: Human-Like Guidance for Zero-Shot Omni-Modal Reasoning", "comment": null, "summary": "Targeting the issues of \"shortcuts\" and insufficient contextual understanding\nin complex cross-modal reasoning of multimodal large models, this paper\nproposes a zero-shot multimodal reasoning component guided by human-like\ncognitive strategies centered on an \"intent sketch\". The component comprises a\nplug-and-play three-module pipeline-Intent Perceiver, Strategy Generator, and\nStrategy Selector-that explicitly constructs a \"understand-plan-select\"\ncognitive process. By generating and filtering \"intent sketch\" strategies to\nguide the final reasoning, it requires no parameter fine-tuning and achieves\ncross-model transfer solely through in-context engineering.\nInformation-theoretic analysis shows that this process can reduce conditional\nentropy and improve information utilization efficiency, thereby suppressing\nunintended shortcut reasoning. Experiments on IntentBench, WorldSense, and\nDaily-Omni validate the method's generality and robust gains; compared with\ntheir respective baselines, the complete \"three-module\" scheme yields\nconsistent improvements across different reasoning engines and pipeline\ncombinations, with gains up to approximately 9.51 percentage points,\ndemonstrating the practical value and portability of the \"intent sketch\"\nreasoning component in zero-shot scenarios.", "AI": {"tldr": "本文提出了一个零样本多模态推理组件，通过模拟人类认知策略（“意图草图”）来解决多模态大模型中存在的“捷径”推理和上下文理解不足问题，该组件无需参数微调，通过上下文工程即可实现跨模型迁移。", "motivation": "多模态大模型在复杂的跨模态推理中，存在“捷径”问题和上下文理解不足的问题。", "method": "本文提出了一个即插即用的三模块流水线组件，包括意图感知器（Intent Perceiver）、策略生成器（Strategy Generator）和策略选择器（Strategy Selector），显式构建了“理解-规划-选择”的人类认知过程。通过生成和过滤“意图草图”策略来指导最终推理，无需参数微调，仅通过上下文工程即可实现跨模型迁移。", "result": "信息论分析表明该过程可以降低条件熵并提高信息利用效率，从而抑制非预期的捷径推理。在IntentBench、WorldSense和Daily-Omni数据集上的实验验证了该方法的通用性和鲁棒性，相比各自的基线，完整的三模块方案在不同推理引擎和流水线组合下均实现了持续改进，性能提升高达约9.51个百分点。", "conclusion": "所提出的“意图草图”推理组件在零样本场景中具有显著的实用价值和可移植性，有效提升了多模态大模型的复杂跨模态推理能力。"}}
{"id": "2509.06518", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06518", "abs": "https://arxiv.org/abs/2509.06518", "authors": ["Andrei Baroian", "Kasper Notebomer"], "title": "Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training", "comment": "The reported results are skewed due to a data type mismatch. The\n  dataset was saved with int32, but the data loader interpreted it as uint16.\n  As a result, each 32-bit token was incorrectly split into two 16-bit tokens.\n  Outcome: a consistent artifact where every other token is zero", "summary": "Transformer-based language models traditionally use uniform (isotropic) layer\nsizes, yet they ignore the diverse functional roles that different depths can\nplay and their computational capacity needs. Building on Layer-Wise Scaling\n(LWS) and pruning literature, we introduce three new LWS variants - Framed,\nReverse, and Crown - that redistribute FFN widths and attention heads via two\nor three-point linear interpolation in the pre-training stage. We present the\nfirst systematic ablation of LWS and its variants, on a fixed budget of 180M\nparameters, trained on 5B tokens. All models converge to similar losses and\nachieve better performance compared to an equal-cost isotropic baseline,\nwithout a substantial decrease in training throughput. This work represents an\ninitial step into the design space of layer-wise architectures for\npre-training, but future work should scale experiments to orders of magnitude\nmore tokens and parameters to fully assess their potential.", "AI": {"tldr": "本研究引入了三种层间缩放（LWS）变体（Framed、Reverse、Crown），通过重新分配FFN宽度和注意力头，在相同参数预算下，使Transformer模型在预训练阶段表现优于传统的均匀层基线。", "motivation": "传统的基于Transformer的语言模型使用统一（各向同性）的层大小，但它们忽略了不同深度层可以扮演的多样功能角色及其计算容量需求。", "method": "在层间缩放（LWS）和剪枝文献的基础上，我们引入了三种新的LWS变体：Framed、Reverse和Crown。这些变体通过两点或三点线性插值在预训练阶段重新分配FFN宽度和注意力头。我们对LWS及其变体进行了首次系统的消融研究，在1.8亿参数、50亿token的固定预算下进行训练。", "result": "所有模型都收敛到相似的损失，并且与同等成本的各向同性基线相比，性能更好，同时训练吞吐量没有显著下降。", "conclusion": "这项工作代表了预训练层间架构设计空间的第一步。未来的工作应将实验扩展到更大数量级的token和参数，以充分评估其潜力。"}}
{"id": "2509.06932", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06932", "abs": "https://arxiv.org/abs/2509.06932", "authors": ["Yuqing Wen", "Hebei Li", "Kefan Gu", "Yucheng Zhao", "Tiancai Wang", "Xiaoyan Sun"], "title": "LLaDA-VLA: Vision Language Diffusion Action Models", "comment": null, "summary": "The rapid progress of auto-regressive vision-language models (VLMs) has\ninspired growing interest in vision-language-action models (VLA) for robotic\nmanipulation. Recently, masked diffusion models, a paradigm distinct from\nautoregressive models, have begun to demonstrate competitive performance in\ntext generation and multimodal applications, leading to the development of a\nseries of diffusion-based VLMs (d-VLMs). However, leveraging such models for\nrobot policy learning remains largely unexplored. In this work, we present\nLLaDA-VLA, the first Vision-Language-Diffusion-Action model built upon\npretrained d-VLMs for robotic manipulation. To effectively adapt d-VLMs to\nrobotic domain, we introduce two key designs: (1) a localized special-token\nclassification strategy that replaces full-vocabulary classification with\nspecial action token classification, reducing adaptation difficulty; (2) a\nhierarchical action-structured decoding strategy that decodes action sequences\nhierarchically considering the dependencies within and across actions.\nExtensive experiments demonstrate that LLaDA-VLA significantly outperforms\nstate-of-the-art VLAs on both simulation and real-world robots.", "AI": {"tldr": "LLaDA-VLA是首个基于预训练扩散视觉-语言模型（d-VLM）的视觉-语言-扩散-动作（VLA）模型，通过引入局部特殊token分类和分层动作结构解码策略，显著超越了现有最先进的VLA模型在机器人操作任务上的表现。", "motivation": "自回归视觉-语言模型（VLM）的快速发展激发了对机器人操作中视觉-语言-动作（VLA）模型的兴趣。与此同时，掩码扩散模型在文本生成和多模态应用中展现出竞争力，并催生了一系列基于扩散的VLM（d-VLM）。然而，如何利用这些模型进行机器人策略学习仍未被充分探索。", "method": "本文提出了LLaDA-VLA，这是首个基于预训练d-VLM构建的视觉-语言-扩散-动作模型，用于机器人操作。为有效将d-VLM应用于机器人领域，引入了两项关键设计：1) 局部特殊token分类策略，用特殊动作token分类取代了全词汇分类，降低了适应难度；2) 分层动作结构解码策略，分层解码动作序列，同时考虑动作内部和动作之间的依赖关系。", "result": "广泛的实验表明，LLaDA-VLA在仿真和真实世界机器人上均显著优于现有最先进的VLA模型。", "conclusion": "LLaDA-VLA成功将扩散视觉-语言模型应用于机器人操作，并通过创新的适应策略实现了卓越性能，证明了d-VLM在机器人策略学习领域的巨大潜力。"}}
{"id": "2509.05695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05695", "abs": "https://arxiv.org/abs/2509.05695", "authors": ["Jingwei Peng", "Zhixuan Qiu", "Boyu Jin", "Surasakdi Siripong"], "title": "Leveraging Vision-Language Large Models for Interpretable Video Action Recognition with Semantic Tokenization", "comment": null, "summary": "Human action recognition often struggles with deep semantic understanding,\ncomplex contextual information, and fine-grained distinction, limitations that\ntraditional methods frequently encounter when dealing with diverse video data.\nInspired by the remarkable capabilities of large language models, this paper\nintroduces LVLM-VAR, a novel framework that pioneers the application of\npre-trained Vision-Language Large Models (LVLMs) to video action recognition,\nemphasizing enhanced accuracy and interpretability. Our method features a\nVideo-to-Semantic-Tokens (VST) Module, which innovatively transforms raw video\nsequences into discrete, semantically and temporally consistent \"semantic\naction tokens,\" effectively crafting an \"action narrative\" that is\ncomprehensible to an LVLM. These tokens, combined with natural language\ninstructions, are then processed by a LoRA-fine-tuned LVLM (e.g., LLaVA-13B)\nfor robust action classification and semantic reasoning. LVLM-VAR not only\nachieves state-of-the-art or highly competitive performance on challenging\nbenchmarks such as NTU RGB+D and NTU RGB+D 120, demonstrating significant\nimprovements (e.g., 94.1% on NTU RGB+D X-Sub and 90.0% on NTU RGB+D 120 X-Set),\nbut also substantially boosts model interpretability by generating natural\nlanguage explanations for its predictions.", "AI": {"tldr": "本文提出LVLM-VAR框架，首次将预训练视觉-语言大模型（LVLMs）应用于视频动作识别，通过将视频转换为语义动作token并结合自然语言指令，显著提升了识别准确性和可解释性。", "motivation": "传统方法在处理多样化视频数据时，难以实现深度语义理解、复杂上下文推理和细粒度动作区分，阻碍了人类动作识别的发展。", "method": "LVLM-VAR框架包含一个视频到语义token（VST）模块，将原始视频序列转化为离散、语义和时间一致的“语义动作token”。这些token与自然语言指令一同输入到经过LoRA微调的LVLM（例如LLaVA-13B）中，以进行动作分类和语义推理。", "result": "LVLM-VAR在NTU RGB+D和NTU RGB+D 120等挑战性基准测试上取得了最先进或极具竞争力的性能（例如，NTU RGB+D X-Sub上达到94.1%，NTU RGB+D 120 X-Set上达到90.0%），并通过生成自然语言解释显著提高了模型的可解释性。", "conclusion": "LVLM-VAR成功地将LVLMs应用于视频动作识别，不仅大幅提高了识别准确性，还通过提供自然语言解释增强了模型的可理解性，为该领域带来了新的视角和解决方案。"}}
{"id": "2509.06733", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06733", "abs": "https://arxiv.org/abs/2509.06733", "authors": ["Wenjun Li", "Zhi Chen", "Jingru Lin", "Hannan Cao", "Wei Han", "Sheng Liang", "Zhi Zhang", "Kuicai Dong", "Dexun Li", "Chen Zhang", "Yong Liu"], "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey", "comment": "38 pages, first version", "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.", "AI": {"tldr": "这篇综述首次系统地探讨了强化学习（RL）在深度研究系统（即能够协调推理、搜索和工具使用的智能体AI）中的应用基础，并提出了实践指导。", "motivation": "当前的深度研究系统训练方法（如SFT和DPO）存在局限性，包括模仿和暴露偏差、对环境反馈利用不足、对人类定义决策点的依赖以及长周期信用分配和多目标权衡方面的弱点。强化学习（RL）通过优化轨迹级策略、实现探索和恢复行为，以及提供原则性的信用分配，能更好地应对这些复杂、多步骤任务的挑战，减少对人类先验和评估者偏差的依赖。", "method": "本综述系统地回顾了DeepSeek-R1之后的相关工作，主要围绕三个方面：(i) 数据合成与整理；(ii) 针对智能体研究的RL方法（涵盖稳定性、样本效率、长上下文处理、奖励与信用设计、多目标优化和多模态集成）；(iii) 智能体RL训练系统和框架。此外，还涵盖了智能体架构与协调、评估与基准测试（包括QA、VQA、长文本合成和领域接地工具交互任务）。", "result": "综述总结了重复出现的模式，揭示了基础设施瓶颈，并为使用RL训练鲁棒、透明的深度研究智能体提供了实用指导。", "conclusion": "强化学习是构建能够解决复杂、多步骤任务的深度研究系统的关键基础。本综述为理解和应用RL于此类系统提供了全面的框架和实践指导，有助于推动鲁棒、透明的深度研究智能体的发展。"}}
{"id": "2509.06524", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06524", "abs": "https://arxiv.org/abs/2509.06524", "authors": ["Jian Wu", "Hang Yu", "Bingchang Liu", "Wenjie Yang", "Peng Di", "Jianguo Li", "Yue Zhang"], "title": "LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection", "comment": null, "summary": "Adapting large language models (LLMs) to specific domains often faces a\ncritical bottleneck: the scarcity of high-quality, human-curated data. While\nlarge volumes of unchecked data are readily available, indiscriminately using\nthem for fine-tuning risks introducing noise and degrading performance.\nStrategic data selection is thus crucial, requiring a method that is both\naccurate and efficient. Existing approaches, categorized as similarity-based\nand direct optimization methods, struggle to simultaneously achieve these\ngoals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for\ndomain-specific DAta Selection), a novel approach that leverages the\npre-trained LLM itself as an implicit classifier, thereby bypassing explicit\nfeature engineering and computationally intensive optimization process. LAMDAS\nreframes data selection as a one-class classification problem, identifying\ncandidate data that \"belongs\" to the target domain defined by a small reference\ndataset. Extensive experimental results demonstrate that LAMDAS not only\nexceeds the performance of full-data training using a fraction of the data but\nalso outperforms nine state-of-the-art (SOTA) baselines under various\nscenarios. Furthermore, LAMDAS achieves the most compelling balance between\nperformance gains and computational efficiency compared to all evaluated\nbaselines.", "AI": {"tldr": "LAMDAS是一种新颖的领域特定数据选择方法，它利用LLM作为隐式分类器，将数据选择重构为单类分类问题，从而在数据稀缺的情况下，以更少的数据实现了超越全数据训练和现有SOTA基线的性能，并达到了最佳的性能与效率平衡。", "motivation": "将大型语言模型（LLMs）适应特定领域时，面临高质量人工标注数据稀缺的瓶颈。直接使用大量未经检查的数据会引入噪声并降低性能。因此，需要一种既准确又高效的战略性数据选择方法，但现有方法（如基于相似性和直接优化的方法）难以同时实现这两个目标。", "method": "本文提出了LAMDAS（LLM As an iMplicit classifier for domain-specific DAta Selection），该方法利用预训练的LLM本身作为隐式分类器，从而避免了显式特征工程和计算密集型优化过程。LAMDAS将数据选择重新定义为单类分类问题，通过一个小型参考数据集来识别“属于”目标领域的数据。", "result": "实验结果表明，LAMDAS不仅使用一小部分数据就能超越全数据训练的性能，而且在各种场景下都优于九种最先进（SOTA）的基线方法。此外，与所有评估的基线相比，LAMDAS在性能提升和计算效率之间实现了最令人信服的平衡。", "conclusion": "LAMDAS提供了一种有效且高效的领域特定数据选择方案，解决了高质量数据稀缺的问题，显著提升了LLM在特定领域适应的性能，并在计算效率上优于现有方法。"}}
{"id": "2509.06951", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06951", "abs": "https://arxiv.org/abs/2509.06951", "authors": ["Qi Lv", "Weijie Kong", "Hao Li", "Jia Zeng", "Zherui Qiu", "Delin Qu", "Haoming Song", "Qizhi Chen", "Xiang Deng", "Jiangmiao Pang"], "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation to Actions", "comment": null, "summary": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.", "AI": {"tldr": "F1是一个预训练的VLA框架，它将视觉预见生成整合到决策过程中，以解决现有模型在动态视觉环境中反应迟钝和鲁棒性差的问题，并在各种任务中取得了显著的性能提升。", "motivation": "现有VLA模型主要采用反应式的状态-动作映射，在动态场景中常导致短视行为和鲁棒性差，难以有效执行语言条件下的任务。", "method": "F1引入了视觉预见生成到决策流程中，采用Mixture-of-Transformer架构，包含感知、预见生成和控制模块。它利用下一尺度预测机制合成目标导向的视觉预见作为显式规划目标，并将动作生成重新定义为预见引导的逆动力学问题。此外，F1采用三阶段训练方案，在一个包含超过330k轨迹和136个任务的广泛数据集上进行训练，以增强模块化推理和可迁移的视觉预见能力。", "result": "F1在真实世界任务和模拟基准测试中持续优于现有方法，在任务成功率和泛化能力方面均取得了显著提升。", "conclusion": "F1通过将视觉预见生成整合到决策流程中，并采用专门的架构和训练方案，有效解决了具身AI在动态视觉环境中执行语言条件任务的挑战，显著提高了模型的鲁棒性和泛化能力。"}}
{"id": "2509.05696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05696", "abs": "https://arxiv.org/abs/2509.05696", "authors": ["Hongyu Zhou", "Yunzhou Zhang", "Tingsong Huang", "Fawei Ge", "Man Qi", "Xichen Zhang", "Yizhong Zhang"], "title": "JRN-Geo: A Joint Perception Network based on RGB and Normal images for Cross-view Geo-localization", "comment": null, "summary": "Cross-view geo-localization plays a critical role in Unmanned Aerial Vehicle\n(UAV) localization and navigation. However, significant challenges arise from\nthe drastic viewpoint differences and appearance variations between images.\nExisting methods predominantly rely on semantic features from RGB images, often\nneglecting the importance of spatial structural information in capturing\nviewpoint-invariant features. To address this issue, we incorporate geometric\nstructural information from normal images and introduce a Joint perception\nnetwork to integrate RGB and Normal images (JRN-Geo). Our approach utilizes a\ndual-branch feature extraction framework, leveraging a Difference-Aware Fusion\nModule (DAFM) and Joint-Constrained Interaction Aggregation (JCIA) strategy to\nenable deep fusion and joint-constrained semantic and structural information\nrepresentation. Furthermore, we propose a 3D geographic augmentation technique\nto generate potential viewpoint variation samples, enhancing the network's\nability to learn viewpoint-invariant features. Extensive experiments on the\nUniversity-1652 and SUES-200 datasets validate the robustness of our method\nagainst complex viewpoint ariations, achieving state-of-the-art performance.", "AI": {"tldr": "该研究提出JRN-Geo网络，通过融合RGB图像的语义信息和法线图像的几何结构信息，解决跨视角地理定位中剧烈视角和外观差异的挑战。该方法利用双分支特征提取框架、DAFM和JCIA策略，并结合3D地理增强技术，实现了最先进的性能。", "motivation": "无人机定位和导航中的跨视角地理定位面临巨大的视角差异和外观变化挑战。现有方法主要依赖RGB图像的语义特征，忽视了空间结构信息在捕捉视角不变特征方面的重要性。", "method": "本文提出一个联合感知网络JRN-Geo，整合RGB图像和法线图像的几何结构信息。该方法采用双分支特征提取框架，利用差异感知融合模块（DAFM）和联合约束交互聚合（JCIA）策略实现深度融合和联合约束的语义与结构信息表示。此外，还提出了一种3D地理增强技术来生成潜在的视角变化样本，以增强网络学习视角不变特征的能力。", "result": "在University-1652和SUES-200数据集上的大量实验验证了该方法对复杂视角变化的鲁棒性，并取得了最先进的性能。", "conclusion": "通过有效地整合RGB和法线图像的结构信息，并结合创新的融合策略和数据增强技术，JRN-Geo网络显著提升了跨视角地理定位的准确性和鲁棒性，克服了现有方法对空间结构信息忽视的局限性。"}}
{"id": "2509.06736", "categories": ["cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06736", "abs": "https://arxiv.org/abs/2509.06736", "authors": ["Jie Yang", "Jiajun Chen", "Zhangyue Yin", "Shuo Chen", "Yuxin Wang", "Yiran Guo", "Yuan Li", "Yining Zheng", "Xuanjing Huang", "Xipeng Qiu"], "title": "VehicleWorld: A Highly Integrated Multi-Device Environment for Intelligent Vehicle Interaction", "comment": null, "summary": "Intelligent vehicle cockpits present unique challenges for API Agents,\nrequiring coordination across tightly-coupled subsystems that exceed typical\ntask environments' complexity. Traditional Function Calling (FC) approaches\noperate statelessly, requiring multiple exploratory calls to build\nenvironmental awareness before execution, leading to inefficiency and limited\nerror recovery. We introduce VehicleWorld, the first comprehensive environment\nfor the automotive domain, featuring 30 modules, 250 APIs, and 680 properties\nwith fully executable implementations that provide real-time state information\nduring agent execution. This environment enables precise evaluation of vehicle\nagent behaviors across diverse, challenging scenarios. Through systematic\nanalysis, we discovered that direct state prediction outperforms function\ncalling for environmental control. Building on this insight, we propose\nState-based Function Call (SFC), a novel approach that maintains explicit\nsystem state awareness and implements direct state transitions to achieve\ntarget conditions. Experimental results demonstrate that SFC significantly\noutperforms traditional FC approaches, achieving superior execution accuracy\nand reduced latency. We have made all implementation code publicly available on\nGithub https://github.com/OpenMOSS/VehicleWorld.", "AI": {"tldr": "本文介绍了VehicleWorld，一个用于智能汽车领域API代理的综合环境，并提出了基于状态的函数调用（SFC）方法，该方法通过维护显式系统状态并实现直接状态转换，显著优于传统函数调用方法，提高了执行精度并降低了延迟。", "motivation": "智能汽车驾驶舱的API代理面临独特挑战，需要协调紧密耦合的子系统，其复杂性超出典型任务环境。传统的函数调用（FC）方法无状态运行，需要多次探索性调用才能建立环境感知，导致效率低下和错误恢复能力有限。", "method": "本文引入了VehicleWorld，这是首个针对汽车领域的综合环境，包含30个模块、250个API和680个属性，具有完全可执行的实现，可在代理执行期间提供实时状态信息。通过系统分析，发现直接状态预测在环境控制方面优于函数调用。在此基础上，提出了基于状态的函数调用（SFC），该方法维护显式系统状态感知并实现直接状态转换以达到目标条件。", "result": "研究发现直接状态预测在环境控制方面优于函数调用。实验结果表明，SFC显著优于传统的FC方法，实现了卓越的执行精度并降低了延迟。", "conclusion": "VehicleWorld环境和SFC方法为智能汽车驾驶舱API代理的开发和评估提供了更高效、更准确的解决方案，克服了传统函数调用方法的局限性，特别是在复杂、紧密耦合的子系统协调方面表现出色。"}}
{"id": "2509.06531", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06531", "abs": "https://arxiv.org/abs/2509.06531", "authors": ["Mengxue Yang", "Chun Yang", "Jiaqi Zhu", "Jiafan Li", "Jingqi Zhang", "Yuyang Li", "Ying Li"], "title": "SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion", "comment": "Accepted by EMNLP Findings 2025", "summary": "Link prediction in knowledge graphs requires integrating structural\ninformation and semantic context to infer missing entities. While large\nlanguage models offer strong generative reasoning capabilities, their limited\nexploitation of structural signals often results in structural sparsity and\nsemantic ambiguity, especially under incomplete or zero-shot settings. To\naddress these challenges, we propose SLiNT (Structure-aware Language model with\nInjection and coNtrastive Training), a modular framework that injects\nknowledge-graph-derived structural context into a frozen LLM backbone with\nlightweight LoRA-based adaptation for robust link prediction. Specifically,\nStructure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to\nenrich sparse entities and mitigate missing context; Dynamic Hard Contrastive\nLearning (DHCL) introduces fine-grained supervision by interpolating hard\npositives and negatives to resolve entity-level ambiguity; and\nGradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware\nintervention while preserving the core LLM parameters. Experiments on WN18RR\nand FB15k-237 show that SLiNT achieves superior or competitive performance\ncompared with both embedding-based and generation-based baselines,\ndemonstrating the effectiveness of structure-aware representation learning for\nscalable knowledge graph completion.", "AI": {"tldr": "SLiNT是一个模块化框架，通过结构感知注入和对比训练，将知识图谱的结构上下文融入冻结的LLM中，以解决链接预测中的结构稀疏性和语义模糊性问题，并在主流数据集上表现出色。", "motivation": "知识图谱链接预测需要整合结构信息和语义上下文。虽然大型语言模型（LLM）具有强大的生成推理能力，但它们对结构信号的利用有限，导致在不完整或零样本设置下出现结构稀疏性和语义模糊性，这促使研究者寻找方法来增强LLM的结构感知能力。", "method": "本文提出了SLiNT（Structure-aware Language model with Injection and coNtrastive Training），一个模块化框架，通过轻量级LoRA适配将知识图谱导出的结构上下文注入到冻结的LLM骨干中。具体方法包括：1) 结构引导邻域增强（SGNE），用于检索伪邻居以丰富稀疏实体并缓解上下文缺失；2) 动态硬对比学习（DHCL），通过插值硬正例和负例引入细粒度监督，解决实体级模糊性；3) 梯度解耦双注入（GDDI），在保留LLM核心参数的同时进行token级的结构感知干预。", "result": "在WN18RR和FB15k-237数据集上的实验表明，SLiNT与基于嵌入和基于生成的基线模型相比，取得了卓越或具有竞争力的性能。", "conclusion": "SLiNT的实验结果证明了结构感知表示学习对于可扩展知识图谱补全的有效性，成功地将结构上下文融入LLM以解决链接预测中的挑战。"}}
{"id": "2509.05728", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.05728", "abs": "https://arxiv.org/abs/2509.05728", "authors": ["Niels Balemans", "Ali Anwar", "Jan Steckel", "Siegfried Mercelis"], "title": "LiDAR-BIND-T: Improving SLAM with Temporally Consistent Cross-Modal LiDAR Reconstruction", "comment": null, "summary": "This paper extends LiDAR-BIND, a modular multi-modal fusion framework that\nbinds heterogeneous sensors (radar, sonar) to a LiDAR-defined latent space,\nwith mechanisms that explicitly enforce temporal consistency. We introduce\nthree contributions: (i) temporal embedding similarity that aligns consecutive\nlatents, (ii) a motion-aligned transformation loss that matches displacement\nbetween predictions and ground truth LiDAR, and (iii) windows temporal fusion\nusing a specialised temporal module. We further update the model architecture\nto better preserve spatial structure. Evaluations on radar/sonar-to-LiDAR\ntranslation demonstrate improved temporal and spatial coherence, yielding lower\nabsolute trajectory error and better occupancy map accuracy in\nCartographer-based SLAM (Simultaneous Localisation and Mapping). We propose\ndifferent metrics based on the Fr\\'echet Video Motion Distance (FVMD) and a\ncorrelation-peak distance metric providing practical temporal quality\nindicators to evaluate SLAM performance. The proposed temporal LiDAR-BIND, or\nLiDAR-BIND-T, maintains plug-and-play modality fusion while substantially\nenhancing temporal stability, resulting in improved robustness and performance\nfor downstream SLAM.", "AI": {"tldr": "本文通过引入显式时间一致性机制，将多模态融合框架LiDAR-BIND扩展为LiDAR-BIND-T，显著提升了下游SLAM的鲁棒性和性能。", "motivation": "原始LiDAR-BIND框架在将异构传感器（雷达、声纳）绑定到LiDAR定义的潜在空间时，可能存在时间一致性不足和空间结构保留不佳的问题，影响了下游SLAM的性能。本研究旨在解决这些问题。", "method": "本文提出了LiDAR-BIND-T框架，主要贡献包括：(i) 引入时间嵌入相似性以对齐连续潜在空间；(ii) 设计运动对齐变换损失以匹配预测与真实LiDAR之间的位移；(iii) 使用专用时间模块进行窗口时间融合。此外，还更新了模型架构以更好地保留空间结构。评估采用了基于Fréchet视频运动距离（FVMD）和相关峰距离的新指标。", "result": "评估结果表明，LiDAR-BIND-T在雷达/声纳到LiDAR的转换中，改善了时间和空间一致性，在基于Cartographer的SLAM中实现了更低的绝对轨迹误差和更高的占用图精度。该框架保持了即插即用的模态融合能力，同时显著增强了时间稳定性，从而提高了下游SLAM的鲁棒性和性能。", "conclusion": "LiDAR-BIND-T通过显式的时间一致性机制，成功地扩展了LiDAR-BIND框架，在保持模态融合灵活性的同时，大幅提升了时间稳定性，为SLAM应用带来了更好的鲁棒性和性能。"}}
{"id": "2509.05703", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.05703", "abs": "https://arxiv.org/abs/2509.05703", "authors": ["Ragib Amin Nihal", "Benjamin Yen", "Takeshi Ashizawa", "Kazuhiro Nakadai"], "title": "Knowledge-Augmented Vision Language Models for Underwater Bioacoustic Spectrogram Analysis", "comment": null, "summary": "Marine mammal vocalization analysis depends on interpreting bioacoustic\nspectrograms. Vision Language Models (VLMs) are not trained on these\ndomain-specific visualizations. We investigate whether VLMs can extract\nmeaningful patterns from spectrograms visually. Our framework integrates VLM\ninterpretation with LLM-based validation to build domain knowledge. This\nenables adaptation to acoustic data without manual annotation or model\nretraining.", "AI": {"tldr": "本文研究了视觉语言模型（VLMs）是否能在未经特定领域训练的情况下，通过声谱图分析海洋哺乳动物的发声。通过结合VLM解释和基于LLM的验证，该框架无需手动标注或模型再训练即可适应声学数据。", "motivation": "海洋哺乳动物发声分析依赖于生物声学声谱图的解读，但现有的视觉语言模型（VLMs）并未针对这类特定领域的视觉数据进行训练。因此，研究的动机是探索VLMs是否能从声谱图中有效提取有意义的模式。", "method": "研究方法是构建一个框架，该框架将VLM的解释能力与基于大型语言模型（LLM）的验证机制相结合，以构建领域知识。这种集成允许模型适应声学数据。", "result": "该框架使得模型能够适应声学数据，而无需进行手动标注或模型再训练。这意味着可以从声谱图中视觉化地提取有意义的模式。", "conclusion": "结论是，通过将VLM的解释能力与LLM的验证相结合，可以有效地使VLMs适应生物声学声谱图分析，从而无需传统的密集型数据标注或模型再训练，即可从特定领域的可视化数据中提取模式。"}}
{"id": "2509.06770", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.06770", "abs": "https://arxiv.org/abs/2509.06770", "authors": ["Shashidhar Reddy Javaji", "Bhavul Gauri", "Zining Zhu"], "title": "Another Turn, Better Output? A Turn-Wise Analysis of Iterative LLM Prompting", "comment": null, "summary": "Large language models (LLMs) are now used in multi-turn workflows, but we\nstill lack a clear way to measure when iteration helps and when it hurts. We\npresent an evaluation framework for iterative refinement that spans ideation,\ncode, and math. Our protocol runs controlled 12-turn conversations per task,\nutilizing a variety of prompts ranging from vague ``improve it'' feedback to\ntargeted steering, and logs per-turn outputs. We score outcomes with\ndomain-appropriate checks (unit tests for code; answer-equivalence plus\nreasoning-soundness for math; originality and feasibility for ideation) and\ntrack turn-level behavior with three families of metrics: semantic movement\nacross turns, turn-to-turn change, and output size growth. Across models and\ntasks, gains are domain-dependent: they arrive early in ideas and code, but in\nmath late turns matter when guided by elaboration. After the first few turns,\nvague feedback often plateaus or reverses correctness, while targeted prompts\nreliably shift the intended quality axis (novelty vs. feasibility in ideation;\nspeed vs. readability in code; in math, elaboration outperforms exploration and\ndrives late-turn gains). We also observe consistent domain patterns: ideation\nmoves more in meaning across turns, code tends to grow in size with little\nsemantic change, and math starts fixed but can break that path with late,\nelaborative iteration.Together, the framework and metrics make iteration\nmeasurable and comparable across models, and signal when to steer, stop, or\nswitch strategies.", "AI": {"tldr": "该研究提出了一个评估大型语言模型（LLMs）迭代精炼过程的框架，涵盖了创意、代码和数学任务，并测量了迭代在不同领域和提示类型下的效果。", "motivation": "尽管LLMs已广泛应用于多轮工作流程，但目前仍缺乏清晰的方法来衡量迭代何时有益、何时有害。", "method": "研究建立了一个评估框架，对每个任务进行受控的12轮对话，使用从模糊到有针对性的多种提示。通过领域特定的检查（代码的单元测试、数学的答案等效性和推理合理性、创意的原创性和可行性）对结果进行评分，并使用三类指标（语义变化、轮次间变化、输出大小增长）跟踪每轮行为。", "result": "结果显示，迭代收益具有领域依赖性：创意和代码任务的收益出现较早，而数学任务在后期轮次且有详细指导时更有效。模糊反馈在几轮后常导致正确性停滞或下降，而有针对性的提示能可靠地改变预期的质量维度。同时观察到一致的领域模式：创意任务语义变化大，代码任务倾向于尺寸增长而语义变化小，数学任务初期稳定但后期详细迭代可打破路径。", "conclusion": "该框架和指标使得LLM的迭代过程可测量和比较，并能指导何时进行引导、停止或切换策略，以优化不同任务的迭代效果。"}}
{"id": "2509.06596", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06596", "abs": "https://arxiv.org/abs/2509.06596", "authors": ["Xin Tong", "Zhi Lin", "Jingya Wang", "Bo Jin"], "title": "HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) often produce hallucinations in\nretrieval-augmented or long-context generation, even when relevant evidence is\npresent. This stems from two issues: head importance is treated as\ninput-agnostic, and raw attention weights poorly reflect each token's true\ncontribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a\nparameter-free decoding framework that directly addresses both challenges. HAVE\nintroduces head-adaptive gating, which performs instance-level soft reweighing\nof attention heads, and value calibration, which augments attention with the\nmagnitude of value vectors to approximate write-back contribution. Together,\nthese modules construct token-level evidence aligned with model updates and\nfuse it with the LM distribution through a lightweight uncertainty-scaled\npolicy. HAVE requires no finetuning and operates in a single forward pass,\nmaking it efficient and broadly applicable. Experiments across multiple QA\nbenchmarks and LLM families demonstrate that HAVE consistently reduces\nhallucinations and outperforms strong baselines, including DAGCD, with modest\noverhead. The framework is transparent, reproducible, and readily integrates\nwith off-the-shelf LLMs, advancing trustworthy generation in real-world\nsettings.", "AI": {"tldr": "本文提出了HAVE（Head-Adaptive Gating and ValuE Calibration）框架，通过自适应门控和价值校准，在不进行微调的情况下，有效减少大型语言模型在检索增强或长上下文生成中的幻觉问题。", "motivation": "大型语言模型（LLMs）在检索增强或长上下文生成中即使存在相关证据也常产生幻觉。这源于两个问题：注意力头的重要性被视为与输入无关，以及原始注意力权重未能准确反映每个token的真实贡献。", "method": "HAVE框架通过引入“头自适应门控”（对注意力头进行实例级软重加权）和“价值校准”（用价值向量的幅度增强注意力以近似回写贡献）来解决上述问题。这两个模块共同构建与模型更新对齐的token级证据，并通过轻量级不确定性缩放策略将其与LM分布融合。HAVE无需微调，在单次前向传播中运行，且不含参数。", "result": "实验结果表明，HAVE在多个问答基准和LLM家族中持续减少幻觉，并优于包括DAGCD在内的强基线，且开销适中。该框架透明、可复现，并可与现有LLM无缝集成。", "conclusion": "HAVE是一个高效、透明、可复现且广泛适用的解码框架，能够与现有LLM集成，从而在实际应用中推进可信赖的生成。"}}
{"id": "2509.05747", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MA", "cs.RO", "I.5.4"], "pdf": "https://arxiv.org/pdf/2509.05747", "abs": "https://arxiv.org/abs/2509.05747", "authors": ["Leo Ho", "Yinghao Huang", "Dafei Qin", "Mingyi Shi", "Wangpok Tse", "Wei Liu", "Junichi Yamagishi", "Taku Komura"], "title": "InterAct: A Large-Scale Dataset of Dynamic, Expressive and Interactive Activities between Two People in Daily Scenarios", "comment": "The first two authors contributed equally to this work", "summary": "We address the problem of accurate capture of interactive behaviors between\ntwo people in daily scenarios. Most previous works either only consider one\nperson or solely focus on conversational gestures of two people, assuming the\nbody orientation and/or position of each actor are constant or barely change\nover each interaction. In contrast, we propose to simultaneously model two\npeople's activities, and target objective-driven, dynamic, and semantically\nconsistent interactions which often span longer duration and cover bigger\nspace. To this end, we capture a new multi-modal dataset dubbed InterAct, which\nis composed of 241 motion sequences where two people perform a realistic and\ncoherent scenario for one minute or longer over a complete interaction. For\neach sequence, two actors are assigned different roles and emotion labels, and\ncollaborate to finish one task or conduct a common interaction activity. The\naudios, body motions, and facial expressions of both persons are captured.\nInterAct contains diverse and complex motions of individuals and interesting\nand relatively long-term interaction patterns barely seen before. We also\ndemonstrate a simple yet effective diffusion-based method that estimates\ninteractive face expressions and body motions of two people from speech inputs.\nOur method regresses the body motions in a hierarchical manner, and we also\npropose a novel fine-tuning mechanism to improve the lip accuracy of facial\nexpressions. To facilitate further research, the data and code is made\navailable at https://hku-cg.github.io/interact/ .", "AI": {"tldr": "该研究旨在准确捕捉日常场景中两人之间的互动行为，提出了一个名为 InterAct 的新型多模态数据集，并开发了一种基于扩散模型的简单有效方法，通过语音输入估计两人的互动面部表情和身体动作。", "motivation": "现有工作大多只关注单人或仅限于两人对话手势，且假设身体姿态保持不变或变化微小。本研究旨在同时建模两人活动，目标是捕捉以目标驱动、动态且语义一致的互动，这些互动通常持续时间更长、覆盖空间更大。", "method": "研究方法包括：1) 捕捉了一个新的多模态数据集 InterAct，包含241个两人进行现实连贯场景的动作序列，每个序列持续一分钟或更长，涵盖音频、身体动作和面部表情；2) 提出了一种简单有效的基于扩散模型的方法，通过语音输入估计两人的互动面部表情和身体动作；3) 身体动作采用分层回归方式；4) 提出了一种新颖的微调机制以提高面部表情的唇部准确性。", "result": "InterAct 数据集包含了多样且复杂的个体动作，以及以前罕见的有趣且相对长期的互动模式。研究还展示了所提出的基于扩散模型的方法能够有效估计互动面部表情和身体动作，并显著提高了唇部准确性。", "conclusion": "该研究成功捕捉并建模了两人在日常场景中的动态、目标驱动的互动行为，通过构建 InterAct 数据集和提出基于扩散模型的方法，为未来关于两人互动行为的研究提供了宝贵资源和有效工具。"}}
{"id": "2509.05740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05740", "abs": "https://arxiv.org/abs/2509.05740", "authors": ["Xinyu Zhang", "Kai Huang", "Junqiao Zhao", "Zihan Yuan", "Tiantian Feng"], "title": "Multi-LVI-SAM: A Robust LiDAR-Visual-Inertial Odometry for Multiple Fisheye Cameras", "comment": null, "summary": "We propose a multi-camera LiDAR-visual-inertial odometry framework,\nMulti-LVI-SAM, which fuses data from multiple fisheye cameras, LiDAR and\ninertial sensors for highly accurate and robust state estimation. To enable\nefficient and consistent integration of visual information from multiple\nfisheye cameras, we introduce a panoramic visual feature model that unifies\nmulti-camera observations into a single representation. The panoramic model\nserves as a global geometric optimization framework that consolidates\nmulti-view constraints, enabling seamless loop closure and global pose\noptimization, while simplifying system design by avoiding redundant handling of\nindividual cameras. To address the triangulation inconsistency caused by the\nmisalignment between each camera's frame and the panoramic model's frame, we\npropose an extrinsic compensation method. This method improves feature\nconsistency across views and significantly reduces triangulation and\noptimization errors, leading to more accurate pose estimation. We integrate the\npanoramic visual feature model into a tightly coupled LiDAR-visual-inertial\nsystem based on a factor graph. Extensive experiments on public datasets\ndemonstrate that the panoramic visual feature model enhances the quality and\nconsistency of multi-camera constraints, resulting in higher accuracy and\nrobustness than existing multi-camera LiDAR-visual-inertial systems.", "AI": {"tldr": "本文提出了Multi-LVI-SAM，一个多相机激光雷达-视觉-惯性里程计框架，通过引入全景视觉特征模型和外参补偿方法，实现了多鱼眼相机数据的高效融合，显著提高了位姿估计的精度和鲁棒性。", "motivation": "现有系统在融合多鱼眼相机数据时存在效率和一致性问题，导致位姿估计不够准确和鲁棒。研究旨在实现高精度和鲁棒的状态估计。", "method": "1. 提出了Multi-LVI-SAM框架，融合多鱼眼相机、激光雷达和惯性传感器数据。2. 引入了全景视觉特征模型，将多相机观测统一为单一表示，用于全局几何优化、回环检测和全局位姿优化。3. 提出了一种外参补偿方法，解决相机帧与全景模型帧之间的未对准导致的三角化不一致问题。4. 将全景视觉特征模型集成到基于因子图的紧耦合激光雷达-视觉-惯性系统中。", "result": "全景视觉特征模型显著提高了多相机约束的质量和一致性。与现有多相机激光雷达-视觉-惯性系统相比，Multi-LVI-SAM在公共数据集上表现出更高的精度和鲁棒性，减少了三角化和优化误差。", "conclusion": "所提出的Multi-LVI-SAM框架，特别是其全景视觉特征模型和外参补偿方法，有效解决了多鱼眼相机数据融合的挑战，实现了更准确和鲁棒的状态估计。"}}
{"id": "2509.06822", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06822", "abs": "https://arxiv.org/abs/2509.06822", "authors": ["Chenyang Zhu", "Spencer Hong", "Jingyu Wu", "Kushal Chawla", "Charlotte Tang", "Youbing Yin", "Nathan Wolfe", "Erin Babinsky", "Daben Liu"], "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems", "comment": null, "summary": "We have reached a critical roadblock in the development and enhancement of\nlong-horizon, multi-component LLM agentic systems: it is incredibly tricky to\nidentify where these systems break down and why. Evaluation capabilities that\ncurrently exist today (e.g., single pass LLM-as-a-judge) are limited in that\nthey often focus on individual metrics or capabilities, end-to-end outcomes,\nand are narrowly grounded on the preferences of humans. We argue that to match\nthe agentic capabilities, evaluation frameworks must also be able to reason,\nprobe, iterate, and understand the complex logic passing through these systems\nover long horizons. In this paper, we present RAFFLES - an evaluation\narchitecture that incorporates reasoning and iterative refinement.\nSpecifically, RAFFLES operates as an iterative, multi-component pipeline, using\na central Judge to systematically investigate faults and a set of specialized\nEvaluators to assess not only the system's components but also the quality of\nthe reasoning by the Judge itself, thereby building a history of hypotheses. We\ntested RAFFLES against several baselines on the Who&When dataset, a benchmark\ndesigned to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure.\nRAFFLES outperforms these baselines, achieving an agent-step fault pair\naccuracy of over 43% on the Algorithmically-Generated dataset (a substantial\nincrease from the previously published best of 16.6%) and over 20% on the\nHand-Crafted dataset (surpassing the previously published best of 8.8%). These\nresults demonstrate a key step towards introducing automated fault detection\nfor autonomous systems over labor-intensive manual human review.", "AI": {"tldr": "本文提出了一种名为RAFFLES的新型评估架构，旨在解决长周期、多组件LLM智能体系统故障诊断的难题，通过迭代推理和细化显著提高了故障定位的准确性。", "motivation": "现有评估能力（如单次LLM作为评判者）在诊断长周期、多组件LLM智能体系统中的故障位置和原因时存在局限性，它们通常只关注单一指标或最终结果，且过度依赖人类偏好。为了匹配智能体的复杂能力，评估框架也需要能够推理、探查、迭代并理解系统中的复杂逻辑。", "method": "RAFFLES是一种结合了推理和迭代细化的评估架构。它作为一个迭代的、多组件的流水线运行，使用一个中央“法官”（Judge）系统地调查故障，并利用一组专门的“评估者”（Evaluators）不仅评估系统的组件，还评估“法官”自身的推理质量，从而建立一个假设历史。", "result": "RAFFLES在Who&When数据集上对多个基线进行了测试，该数据集旨在诊断系统故障的“谁”（智能体）和“何时”（步骤）。RAFFLES表现优于这些基线，在算法生成数据集上实现了超过43%的智能体-步骤故障对准确率（比之前发布的最佳16.6%大幅提高），在手工制作数据集上实现了超过20%的准确率（超过之前发布的最佳8.8%）。", "conclusion": "这些结果表明，RAFFLES在为自主系统引入自动化故障检测方面迈出了关键一步，有望取代劳动密集型的人工审查。"}}
{"id": "2509.06631", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06631", "abs": "https://arxiv.org/abs/2509.06631", "authors": ["Özgür Uğur", "Musa Yılmaz", "Esra Şavirdi", "Özay Ezerceli", "Mahmut El Huseyni", "Selva Taş", "Reyhan Bayraktar"], "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation", "comment": null, "summary": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.", "AI": {"tldr": "本研究比较了RAG系统中三种引导解码方法（Outlines, XGrammar, LM Format Enforcer）在不同多轮提示设置下的表现，评估其成功率、幻觉率和输出质量，揭示了多轮交互对引导解码的影响及性能差异。", "motivation": "大型语言模型（LLMs）应用日益广泛，需要结构化且可靠的响应。检索增强生成（RAG）系统面临的关键挑战是确保输出符合预期格式并最大限度地减少幻觉。", "method": "研究通过比较Outlines、XGrammar和LM Format Enforcer这三种引导解码方法在0轮、1轮和2轮多轮提示设置下的表现，评估了它们在RAG系统中的作用。评估指标包括成功率、幻觉率和输出质量。", "result": "研究结果揭示了多轮交互如何影响引导解码，发现了意想不到的性能变化，这些发现可以为特定用例的方法选择提供信息。", "conclusion": "这项工作增进了对RAG系统中结构化输出生成的理解，为LLM部署提供了理论见解和实践指导。"}}
{"id": "2509.06333", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06333", "abs": "https://arxiv.org/abs/2509.06333", "authors": ["Penelope Brown", "Julie Stephany Berrio Perez", "Mao Shan", "Stewart Worrall"], "title": "Multi-Modal Camera-Based Detection of Vulnerable Road Users", "comment": null, "summary": "Vulnerable road users (VRUs) such as pedestrians, cyclists, and motorcyclists\nrepresent more than half of global traffic deaths, yet their detection remains\nchallenging in poor lighting, adverse weather, and unbalanced data sets. This\npaper presents a multimodal detection framework that integrates RGB and thermal\ninfrared imaging with a fine-tuned YOLOv8 model. Training leveraged KITTI,\nBDD100K, and Teledyne FLIR datasets, with class re-weighting and light\naugmentations to improve minority-class performance and robustness, experiments\nshow that 640-pixel resolution and partial backbone freezing optimise accuracy\nand efficiency, while class-weighted losses enhance recall for rare VRUs.\nResults highlight that thermal models achieve the highest precision, and\nRGB-to-thermal augmentation boosts recall, demonstrating the potential of\nmultimodal detection to improve VRU safety at intersections.", "AI": {"tldr": "本文提出了一种结合RGB和热红外图像的多模态检测框架，基于微调的YOLOv8模型，通过数据增强和损失加权优化，有效提高了弱光、恶劣天气和数据不平衡条件下弱势道路使用者（VRU）的检测精度和召回率，尤其对稀有VRU表现更佳。", "motivation": "弱势道路使用者（行人、骑自行车者、摩托车手）占全球交通事故死亡人数的一半以上，但在弱光、恶劣天气和数据不平衡等条件下，他们的检测仍然极具挑战性。", "method": "该研究提出了一种多模态检测框架，整合了RGB和热红外成像，并使用了微调的YOLOv8模型。训练利用了KITTI、BDD100K和Teledyne FLIR数据集，并采用了类别重新加权和轻度数据增强来提高少数类别的性能和鲁棒性。实验优化了640像素分辨率和部分骨干网络冻结以提高准确性和效率，同时使用类别加权损失来增强稀有VRU的召回率。此外，还采用了RGB到热像的增强技术来提升召回率。", "result": "研究结果表明，640像素分辨率和部分骨干网络冻结优化了准确性和效率；类别加权损失增强了稀有VRU的召回率；热成像模型实现了最高的精度；RGB到热像的增强提升了召回率。", "conclusion": "多模态检测在提高交叉路口弱势道路使用者安全方面具有巨大潜力。"}}
{"id": "2509.05746", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05746", "abs": "https://arxiv.org/abs/2509.05746", "authors": ["Tianhao Guo", "Bingjie Lu", "Feng Wang", "Zhengyang Lu"], "title": "Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation", "comment": null, "summary": "Single image super-resolution traditionally assumes spatially-invariant\ndegradation models, yet real-world imaging systems exhibit complex\ndistance-dependent effects including atmospheric scattering, depth-of-field\nvariations, and perspective distortions. This fundamental limitation\nnecessitates spatially-adaptive reconstruction strategies that explicitly\nincorporate geometric scene understanding for optimal performance. We propose a\nrigorous variational framework that characterizes super-resolution as a\nspatially-varying inverse problem, formulating the degradation operator as a\npseudodifferential operator with distance-dependent spectral characteristics\nthat enable theoretical analysis of reconstruction limits across depth ranges.\nOur neural architecture implements discrete gradient flow dynamics through\ncascaded residual blocks with depth-conditional convolution kernels, ensuring\nconvergence to stationary points of the theoretical energy functional while\nincorporating learned distance-adaptive regularization terms that dynamically\nadjust smoothness constraints based on local geometric structure. Spectral\nconstraints derived from atmospheric scattering theory prevent bandwidth\nviolations and noise amplification in far-field regions, while adaptive kernel\ngeneration networks learn continuous mappings from depth to reconstruction\nfilters. Comprehensive evaluation across five benchmark datasets demonstrates\nstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIM\nat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by\n0.44dB and 0.36dB respectively. This work establishes the first\ntheoretically-grounded distance-adaptive super-resolution framework and\ndemonstrates significant improvements on depth-variant scenarios while\nmaintaining competitive performance across traditional benchmarks.", "AI": {"tldr": "本文提出了一种理论上严谨的距离自适应超分辨率框架，解决了真实世界图像中存在的空间变异退化问题。该框架采用变分方法和深度条件卷积核，实现了对深度相关场景的显著改进，并在传统基准测试中保持了竞争力。", "motivation": "传统的单图像超分辨率方法假设空间不变的退化模型，但真实世界的成像系统存在复杂的距离相关效应，如大气散射、景深变化和透视畸变。这种根本性限制需要空间自适应的重建策略，以明确整合几何场景理解。", "method": "研究者将超分辨率表述为一个空间变化的逆问题，并将其特征化为具有距离相关谱特征的伪微分算子。其神经网络架构通过级联残差块实现离散梯度流动力学，并使用深度条件卷积核，同时引入学习到的距离自适应正则化项。此外，还利用大气散射理论的谱约束和自适应核生成网络，学习从深度到重建滤波器的连续映射。", "result": "该方法在五个基准数据集上取得了最先进的性能，在KITTI室外场景中，2倍和4倍放大下分别达到36.89/0.9516和30.54/0.8721的PSNR/SSIM，分别比现有方法高出0.44dB和0.36dB。它在深度变化的场景中表现出显著改进，同时在传统基准测试中保持了竞争力。", "conclusion": "这项工作建立了第一个理论基础上的距离自适应超分辨率框架，并在深度变化的场景中展示了显著的性能提升，同时在传统基准测试中保持了竞争性表现。"}}
{"id": "2509.06861", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06861", "abs": "https://arxiv.org/abs/2509.06861", "authors": ["James Xu Zhao", "Bryan Hooi", "See-Kiong Ng"], "title": "Test-Time Scaling in Reasoning Models Is Not Effective for Knowledge-Intensive Tasks Yet", "comment": "20 pages, 4 figures, 6 tables", "summary": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge", "AI": {"tldr": "研究发现，测试时扩展（Test-time scaling）在知识密集型任务中并不有效，反而常常导致幻觉（hallucination）增加，尽管“思考”本身仍有益处。", "motivation": "测试时扩展在许多领域表现出色，但其在需要高事实准确性和低幻觉率的知识密集型任务中的有效性尚未明确。", "method": "研究对12个推理模型在两个知识密集型基准上进行了全面的测试时扩展评估。分析了扩展推理如何影响幻觉行为，并通过案例研究探讨了其影响。", "result": "增加测试时计算并不总能提高准确性，反而常常导致更多幻觉。幻觉减少通常是模型选择弃权而非事实召回改善的结果。对于某些模型，更长的推理会鼓励尝试之前未回答的问题，其中许多导致幻觉。案例研究显示，扩展推理可能诱发确认偏误，导致过度自信的幻觉。尽管存在这些局限性，但相比于不思考，启用思考仍然有益。", "conclusion": "测试时扩展在知识密集型任务中尚未有效，可能增加幻觉，但“思考”这一机制本身仍然有益。需要进一步研究以提高其在知识密集型任务中的性能。"}}
{"id": "2509.06637", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06637", "abs": "https://arxiv.org/abs/2509.06637", "authors": ["Yi Xing"], "title": "Modelling Intertextuality with N-gram Embeddings", "comment": null, "summary": "Intertextuality is a central tenet in literary studies. It refers to the\nintricate links between literary texts that are created by various types of\nreferences. This paper proposes a new quantitative model of intertextuality to\nenable scalable analysis and network-based insights: perform pairwise\ncomparisons of the embeddings of n-grams from two texts and average their\nresults as the overall intertextuality. Validation on four texts with known\ndegrees of intertextuality, alongside a scalability test on 267 diverse texts,\ndemonstrates the method's effectiveness and efficiency. Network analysis\nfurther reveals centrality and community structures, affirming the approach's\nsuccess in capturing and quantifying intertextual relationships.", "AI": {"tldr": "本文提出了一种基于n-gram嵌入比较的定量互文性模型，以实现可扩展分析并揭示文本间的网络结构。", "motivation": "互文性是文学研究的核心概念，但现有分析方法可能缺乏可扩展性和网络洞察力。", "method": "通过对两个文本中n-gram嵌入进行成对比较，并取其平均值作为整体互文性度量。该方法能够进行可扩展分析并支持网络构建。", "result": "在四篇已知互文性程度的文本上验证了方法的有效性，并在267篇多样化文本上测试了其效率。网络分析进一步揭示了中心性和社区结构。", "conclusion": "该方法成功地捕捉并量化了互文关系，为大规模文学分析提供了有效工具。"}}
{"id": "2509.06660", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06660", "abs": "https://arxiv.org/abs/2509.06660", "authors": ["Cailei Liang", "Adrian Bodenmann", "Emma J Curtis", "Samuel Simmons", "Kazunori Nagano", "Stan Brown", "Adam Riese", "Blair Thornton"], "title": "Investigating Location-Regularised Self-Supervised Feature Learning for Seafloor Visual Imagery", "comment": null, "summary": "High-throughput interpretation of robotically gathered seafloor visual\nimagery can increase the efficiency of marine monitoring and exploration.\nAlthough recent research has suggested that location metadata can enhance\nself-supervised feature learning (SSL), its benefits across different SSL\nstrategies, models and seafloor image datasets are underexplored. This study\nevaluates the impact of location-based regularisation on six state-of-the-art\nSSL frameworks, which include Convolutional Neural Network (CNN) and Vision\nTransformer (ViT) models with varying latent-space dimensionality. Evaluation\nacross three diverse seafloor image datasets finds that location-regularisation\nconsistently improves downstream classification performance over standard SSL,\nwith average F1-score gains of $4.9 \\pm 4.0%$ for CNNs and $6.3 \\pm 8.9%$ for\nViTs, respectively. While CNNs pretrained on generic datasets benefit from\nhigh-dimensional latent representations, dataset-optimised SSL achieves similar\nperformance across the high (512) and low (128) dimensional latent\nrepresentations. Location-regularised SSL improves CNN performance over\npre-trained models by $2.7 \\pm 2.7%$ and $10.1 \\pm 9.4%$ for high and\nlow-dimensional latent representations, respectively. For ViTs,\nhigh-dimensionality benefits both pre-trained and dataset-optimised SSL.\nAlthough location-regularisation improves SSL performance compared to standard\nSSL methods, pre-trained ViTs show strong generalisation, matching the\nbest-performing location-regularised SSL with F1-scores of $0.795 \\pm 0.075$\nand $0.795 \\pm 0.077$, respectively. The findings highlight the value of\nlocation metadata for SSL regularisation, particularly when using\nlow-dimensional latent representations, and demonstrate strong generalisation\nof high-dimensional ViTs for seafloor image analysis.", "AI": {"tldr": "本研究评估了位置元数据正则化对六种自监督学习（SSL）框架（包括CNN和ViT模型）在海底图像解释中的影响，发现其显著提高了下游分类性能，尤其是在低维潜在表示和对高维ViT的泛化能力方面。", "motivation": "机器人采集的海底视觉图像的高通量解释能提高海洋监测和探索效率。尽管已有研究表明位置元数据可以增强自监督特征学习，但其在不同SSL策略、模型和海底图像数据集上的益处尚未得到充分探索。", "method": "研究评估了基于位置的正则化对六种最先进的自监督学习（SSL）框架的影响，这些框架包括卷积神经网络（CNN）和视觉Transformer（ViT）模型，并考虑了不同潜在空间维度。评估是在三个不同的海底图像数据集上进行的。", "result": "位置正则化持续改善了下游分类性能，CNN的F1分数平均提升4.9±4.0%，ViT的平均提升6.3±8.9%。对于CNN，在通用数据集上预训练的模型受益于高维潜在表示，而经过数据集优化的SSL在高中低维潜在表示上表现相似。位置正则化的SSL使CNN性能比预训练模型分别提高了2.7±2.7%（高维）和10.1±9.4%（低维）。对于ViT，高维潜在表示对预训练和数据集优化的SSL都有益。尽管位置正则化提高了SSL性能，但预训练的ViT显示出强大的泛化能力，与表现最佳的位置正则化SSL（F1分数均为0.795±0.075和0.795±0.077）持平。", "conclusion": "研究结果强调了位置元数据对于SSL正则化的价值，尤其是在使用低维潜在表示时。同时，也证明了高维ViT在海底图像分析中具有强大的泛化能力。"}}
{"id": "2509.05751", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05751", "abs": "https://arxiv.org/abs/2509.05751", "authors": ["Bingrui Zhao", "Lin Yuanbo Wu", "Xiangtian Fan", "Deyin Liu", "Lu Zhang", "Ruyi He", "Jialie Shen", "Ximing Li"], "title": "Unleashing Hierarchical Reasoning: An LLM-Driven Framework for Training-Free Referring Video Object Segmentation", "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment an object of\ninterest throughout a video based on a language description. The prominent\nchallenge lies in aligning static text with dynamic visual content,\nparticularly when objects exhibiting similar appearances with inconsistent\nmotion and poses. However, current methods often rely on a holistic\nvisual-language fusion that struggles with complex, compositional descriptions.\nIn this paper, we propose \\textbf{PARSE-VOS}, a novel, training-free framework\npowered by Large Language Models (LLMs), for a hierarchical, coarse-to-fine\nreasoning across text and video domains. Our approach begins by parsing the\nnatural language query into structured semantic commands. Next, we introduce a\nspatio-temporal grounding module that generates all candidate trajectories for\nall potential target objects, guided by the parsed semantics. Finally, a\nhierarchical identification module select the correct target through a\ntwo-stage reasoning process: it first performs coarse-grained motion reasoning\nwith an LLM to narrow down candidates; if ambiguity remains, a fine-grained\npose verification stage is conditionally triggered to disambiguate. The final\noutput is an accurate segmentation mask for the target object.\n\\textbf{PARSE-VOS} achieved state-of-the-art performance on three major\nbenchmarks: Ref-YouTube-VOS, Ref-DAVIS17, and MeViS.", "AI": {"tldr": "本文提出PARSE-VOS，一个由大型语言模型（LLM）驱动的免训练框架，用于参考视频对象分割（RVOS），通过层次化、从粗到细的推理来处理文本与视频内容的对齐，并取得了SOTA性能。", "motivation": "当前的RVOS方法在将静态文本与动态视觉内容对齐时面临挑战，尤其当对象外观相似、运动和姿态不一致时。此外，现有的方法依赖于整体视觉-语言融合，难以处理复杂、组合性的描述。", "method": "PARSE-VOS是一个免训练框架，采用LLM进行层次化推理。具体步骤包括：1. 将自然语言查询解析为结构化语义命令。2. 引入时空定位模块，根据解析的语义生成所有潜在目标的所有候选轨迹。3. 设计层次化识别模块，通过两阶段推理选择正确目标：首先使用LLM进行粗粒度运动推理以缩小候选范围；如果仍存在歧义，则有条件地触发细粒度姿态验证阶段进行消歧。", "result": "PARSE-VOS在三个主要基准测试（Ref-YouTube-VOS、Ref-DAVIS17和MeViS）上取得了最先进的性能。", "conclusion": "PARSE-VOS通过利用LLM进行层次化、从粗到细的推理，有效解决了RVOS中复杂的文本描述与动态视觉内容对齐的挑战，并提供了一种无需训练的有效解决方案，显著提升了分割准确性。"}}
{"id": "2509.06917", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06917", "abs": "https://arxiv.org/abs/2509.06917", "authors": ["Jiacheng Miao", "Joe R. Davis", "Jonathan K. Pritchard", "James Zou"], "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI Agents", "comment": null, "summary": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.", "AI": {"tldr": "Paper2Agent是一个自动化框架，能将研究论文转化为AI代理，使被动研究成果变为主动系统，加速下游使用、采纳和发现。", "motivation": "传统研究论文要求读者投入大量精力理解和改编代码、数据和方法，这阻碍了知识的传播和重用。", "method": "Paper2Agent通过多智能体系统分析论文及相关代码库，构建模型上下文协议（MCP）服务器，并迭代生成和运行测试来完善MCP。这些MCP可以灵活连接到聊天代理（如Claude Code），通过自然语言处理复杂的科学查询，并调用原始论文中的工具和工作流。", "result": "Paper2Agent成功创建了可靠的论文代理，例如基于AlphaGenome解释基因组变异，以及基于ScanPy和TISSUE进行单细胞和空间转录组分析的代理。这些代理能够重现原始论文结果并正确执行新颖的用户查询。", "conclusion": "Paper2Agent将静态论文转化为动态、交互式AI代理，为知识传播引入了新范式，并为AI合作科学家的协作生态系统奠定了基础。"}}
{"id": "2509.06650", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06650", "abs": "https://arxiv.org/abs/2509.06650", "authors": ["Hao Lin", "Peitong Xie", "Jingxue Chen", "Jie Lin", "Qingkun Tang", "Qianchun Lu"], "title": "Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval\nstage, particularly the coarse-ranking process. Existing coarse-ranking\noptimization approaches often struggle to balance domain-specific knowledge\nlearning with query enhencement, resulting in suboptimal retrieval performance.\nTo address this challenge, we propose MoLER, a domain-aware RAG method that\nuses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a\ntwo-stage pipeline: a continual pre-training (CPT) phase using a Mixture of\nLosses (MoL) to balance domain-specific knowledge with general language\ncapabilities, and a reinforcement learning (RL) phase leveraging Group Relative\nPolicy Optimization (GRPO) to optimize query and passage generation for\nmaximizing document recall. A key innovation is our Multi-query Single-passage\nLate Fusion (MSLF) strategy, which reduces computational overhead during RL\ntraining while maintaining scalable inference via Multi-query Multi-passage\nLate Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER\nachieves state-of-the-art performance, significantly outperforming baseline\nmethods. MoLER bridges the knowledge gap in RAG systems, enabling robust and\nscalable retrieval in specialized domains.", "AI": {"tldr": "MoLER是一种领域感知的RAG方法，通过MoL增强的强化学习优化检索的粗排阶段，平衡领域知识与查询增强，实现领先的检索性能。", "motivation": "现有的粗排优化方法难以平衡领域特定知识学习与查询增强，导致检索性能不佳，特别是在RAG系统中。", "method": "MoLER采用两阶段流水线：1. 持续预训练(CPT)阶段，使用混合损失(MoL)平衡领域知识和通用语言能力。2. 强化学习(RL)阶段，利用群组相对策略优化(GRPO)优化查询和段落生成以最大化文档召回率。其关键创新是多查询单段落晚期融合(MSLF)策略，减少RL训练开销，并通过多查询多段落晚期融合(MMLF)实现可扩展推理。", "result": "MoLER在基准数据集上取得了最先进的性能，显著优于基线方法。", "conclusion": "MoLER弥合了RAG系统中的知识鸿沟，使得在专业领域中能够进行鲁棒且可扩展的检索。"}}
{"id": "2509.06678", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06678", "abs": "https://arxiv.org/abs/2509.06678", "authors": ["Cailei Liang", "Adrian Bodenmann", "Sam Fenton", "Blair Thornton"], "title": "Online Clustering of Seafloor Imagery for Interpretation during Long-Term AUV Operations", "comment": null, "summary": "As long-endurance and seafloor-resident AUVs become more capable, there is an\nincreasing need for extended, real-time interpretation of seafloor imagery to\nenable adaptive missions and optimise communication efficiency. Although\noffline image analysis methods are well established, they rely on access to\ncomplete datasets and human-labelled examples to manage the strong influence of\nenvironmental and operational conditions on seafloor image\nappearance-requirements that cannot be met in real-time settings. To address\nthis, we introduce an online clustering framework (OCF) capable of interpreting\nseafloor imagery without supervision, which is designed to operate in real-time\non continuous data streams in a scalable, adaptive, and self-consistent manner.\nThe method enables the efficient review and consolidation of common patterns\nacross the entire data history in constant time by identifying and maintaining\na set of representative samples that capture the evolving feature distribution,\nsupporting dynamic cluster merging and splitting without reprocessing the full\nimage history. We evaluate the framework on three diverse seafloor image\ndatasets, analysing the impact of different representative sampling strategies\non both clustering accuracy and computational cost. The OCF achieves the\nhighest average F1 score of 0.68 across the three datasets among all\ncomparative online clustering approaches, with a standard deviation of 3%\nacross three distinct survey trajectories, demonstrating its superior\nclustering capability and robustness to trajectory variation. In addition, it\nmaintains consistently lower and bounded computational time as the data volume\nincreases. These properties are beneficial for generating survey data summaries\nand supporting informative path planning in long-term, persistent autonomous\nmarine exploration.", "AI": {"tldr": "本文提出了一种在线聚类框架（OCF），用于实时、无监督地解释海底图像数据流，以支持长续航AUV的自适应任务和通信优化。", "motivation": "随着长续航和海底驻留AUV能力的增强，对海底图像进行实时、扩展的解释需求日益增长。然而，现有的离线图像分析方法依赖于完整数据集和人工标注，无法满足实时应用的需求，因为环境和操作条件对图像外观有显著影响。", "method": "研究引入了一个在线聚类框架（OCF），该框架能够无监督地解释海底图像，并设计为可扩展、自适应和自洽地实时处理连续数据流。该方法通过识别和维护一组代表性样本，捕获不断演变特征分布，从而以恒定时间高效地审查和整合整个数据历史中的常见模式，支持动态集群合并和拆分，而无需重新处理完整的图像历史。", "result": "OCF在三个多样化的海底图像数据集上进行了评估，其平均F1分数达到0.68，在所有比较的在线聚类方法中最高，并且在三种不同测量轨迹下的标准偏差为3%，展现了卓越的聚类能力和对轨迹变化的鲁棒性。此外，随着数据量的增加，其计算时间始终保持较低且有界。", "conclusion": "OCF的这些特性（卓越的聚类能力、鲁棒性、低且有界的计算时间）对于生成调查数据摘要以及在长期、持久的自主海洋探索中支持信息丰富的路径规划非常有益。"}}
{"id": "2509.05773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05773", "abs": "https://arxiv.org/abs/2509.05773", "authors": ["Zijian Chen", "Wenjie Hua", "Jinhao Li", "Lirong Deng", "Fan Du", "Tingzhu Chen", "Guangtao Zhai"], "title": "PictOBI-20k: Unveiling Large Multimodal Models in Visual Decipherment for Pictographic Oracle Bone Characters", "comment": "6 pages, 6 figures", "summary": "Deciphering oracle bone characters (OBCs), the oldest attested form of\nwritten Chinese, has remained the ultimate, unwavering goal of scholars,\noffering an irreplaceable key to understanding humanity's early modes of\nproduction. Current decipherment methodologies of OBC are primarily constrained\nby the sporadic nature of archaeological excavations and the limited corpus of\ninscriptions. With the powerful visual perception capability of large\nmultimodal models (LMMs), the potential of using LMMs for visually deciphering\nOBCs has increased. In this paper, we introduce PictOBI-20k, a dataset designed\nto evaluate LMMs on the visual decipherment tasks of pictographic OBCs. It\nincludes 20k meticulously collected OBC and real object images, forming over\n15k multi-choice questions. We also conduct subjective annotations to\ninvestigate the consistency of the reference point between humans and LMMs in\nvisual reasoning. Experiments indicate that general LMMs possess preliminary\nvisual decipherment skills, and LMMs are not effectively using visual\ninformation, while most of the time they are limited by language priors. We\nhope that our dataset can facilitate the evaluation and optimization of visual\nattention in future OBC-oriented LMMs. The code and dataset will be available\nat https://github.com/OBI-Future/PictOBI-20k.", "AI": {"tldr": "本文介绍了PictOBI-20k数据集，用于评估大型多模态模型（LMMs）在象形甲骨文视觉释读任务上的表现。研究发现LMMs具备初步释读能力，但其视觉信息利用效率不高，且常受语言先验知识的限制。", "motivation": "甲骨文释读对于理解人类早期生产方式至关重要，但现有方法受限于考古发掘和语料库的稀缺。鉴于大型多模态模型（LMMs）强大的视觉感知能力，研究其在视觉释读甲骨文方面的潜力成为可能，但需要专门的评估数据集。", "method": "本文构建了PictOBI-20k数据集，包含2万张精心收集的甲骨文和真实物体图像，形成超过1.5万道多选题，用于评估LMMs的视觉释读能力。同时，进行了主观标注以探究人类与LMMs在视觉推理中参考点的一致性。最后，使用通用LMMs进行了实验。", "result": "实验结果表明，通用LMMs具备初步的视觉释读能力。然而，LMMs并未有效利用视觉信息，大部分时间受到语言先验知识的限制。", "conclusion": "PictOBI-20k数据集有望促进未来面向甲骨文的LMMs在视觉注意力方面的评估和优化。研究揭示了当前LMMs在甲骨文视觉释读中视觉利用效率和语言先验依赖的问题。"}}
{"id": "2509.06942", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06942", "abs": "https://arxiv.org/abs/2509.06942", "authors": ["Xiangwei Shen", "Zhimin Li", "Zhantao Yang", "Shiyi Zhang", "Yingfang Zhang", "Donghao Li", "Chunyu Wang", "Qinglin Lu", "Yansong Tang"], "title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human Preference", "comment": "15 pages", "summary": "Recent studies have demonstrated the effectiveness of directly aligning\ndiffusion models with human preferences using differentiable reward. However,\nthey exhibit two primary challenges: (1) they rely on multistep denoising with\ngradient computation for reward scoring, which is computationally expensive,\nthus restricting optimization to only a few diffusion steps; (2) they often\nneed continuous offline adaptation of reward models in order to achieve desired\naesthetic quality, such as photorealism or precise lighting effects. To address\nthe limitation of multistep denoising, we propose Direct-Align, a method that\npredefines a noise prior to effectively recover original images from any time\nsteps via interpolation, leveraging the equation that diffusion states are\ninterpolations between noise and target images, which effectively avoids\nover-optimization in late timesteps. Furthermore, we introduce Semantic\nRelative Preference Optimization (SRPO), in which rewards are formulated as\ntext-conditioned signals. This approach enables online adjustment of rewards in\nresponse to positive and negative prompt augmentation, thereby reducing the\nreliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model\nwith optimized denoising and online reward adjustment, we improve its\nhuman-evaluated realism and aesthetic quality by over 3x.", "AI": {"tldr": "本文提出Direct-Align和SRPO两种方法，以解决扩散模型在对齐人类偏好时面临的计算成本高和奖励模型离线适应的挑战，显著提升了模型生成图像的真实感和美学质量。", "motivation": "现有方法在将扩散模型与人类偏好对齐时存在两大挑战：1) 奖励评分依赖多步去噪和梯度计算，计算成本高昂，限制了优化步数；2) 为达到理想的美学质量（如照片级真实感），需持续进行奖励模型的离线适应。", "method": "本文提出：1) Direct-Align，通过预定义噪声先验并利用扩散状态的插值公式，实现从任意时间步有效恢复原始图像，从而避免后期时间步的过度优化，解决多步去噪的限制。2) 语义相对偏好优化（SRPO），将奖励公式化为文本条件信号，支持通过正负提示增强在线调整奖励，减少对离线奖励微调的依赖。", "result": "通过结合优化的去噪和在线奖励调整方法对FLUX.1.dev模型进行微调，其在人类评估的真实感和美学质量上提升了3倍以上。", "conclusion": "本文提出的Direct-Align和SRPO有效解决了扩散模型在对齐人类偏好时的计算效率和奖励模型适应性问题，显著提升了生成图像的质量。"}}
{"id": "2509.06652", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06652", "abs": "https://arxiv.org/abs/2509.06652", "authors": ["Xingwei Tan", "Mahathi Parvatham", "Chiara Gambi", "Gabriele Pergola"], "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations", "comment": "EMNLP 2025 Findings camera-ready, 9+7 pages", "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.", "AI": {"tldr": "本研究引入了首个大规模师生对话语料库IntrEx，用于标注对话的趣味性和预期趣味性，并发现经此数据集微调的LLM在预测人类趣味性判断方面优于大型专有模型，同时分析了语言和认知因素对对话参与度的影响。", "motivation": "在第二语言习得中，保持学习者在教育对话中的兴趣是一个挑战。虽然现有研究探索了教育文本的趣味性，但对于驱动对话参与度的语言特征知之甚少。本研究旨在填补这一空白。", "method": "构建了IntrEx数据集，该数据集基于Teacher-Student Chatroom Corpus (TSCC)，并增加了序列级标注，以捕捉兴趣在长对话中的演变。采用超过100名第二语言学习者的严格标注过程，使用受RLHF启发的比较评分方法。研究还微调了大型语言模型（7B/8B参数）以预测人类的趣味性判断，并分析了具体性、可理解性（可读性）和反馈等语言和认知因素对教育对话参与度的影响。", "result": "创建了IntrEx数据集，这是第一个在师生互动中标注趣味性和预期趣味性的大规模数据集。结果表明，在趣味性评分上微调的LLM（7B/8B参数）在预测人类趣味性判断方面优于GPT-4o等大型专有模型。此外，研究还分析了具体性、可理解性和反馈等语言和认知因素如何影响教育对话中的参与度。", "conclusion": "专门的数据集在教育环境中建模参与度方面具有巨大潜力。经特定数据集微调的LLM能够更准确地预测对话趣味性，超越大型通用模型。语言和认知因素对教育对话的参与度有显著影响。"}}
{"id": "2509.06741", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.06741", "abs": "https://arxiv.org/abs/2509.06741", "authors": ["Christian Geckeler", "Niklas Neugebauer", "Manasi Muglikar", "Davide Scaramuzza", "Stefano Mintchev"], "title": "Event Spectroscopy: Event-based Multispectral and Depth Sensing using Structured Light", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Uncrewed aerial vehicles (UAVs) are increasingly deployed in forest\nenvironments for tasks such as environmental monitoring and search and rescue,\nwhich require safe navigation through dense foliage and precise data\ncollection. Traditional sensing approaches, including passive multispectral and\nRGB imaging, suffer from latency, poor depth resolution, and strong dependence\non ambient light - especially under forest canopies. In this work, we present a\nnovel event spectroscopy system that simultaneously enables high-resolution,\nlow-latency depth reconstruction and multispectral imaging using a single\nsensor. Depth is reconstructed using structured light, and by modulating the\nwavelength of the projected structured light, our system captures spectral\ninformation in controlled bands between 650 nm and 850 nm. We demonstrate up to\n$60\\%$ improvement in RMSE over commercial depth sensors and validate the\nspectral accuracy against a reference spectrometer and commercial multispectral\ncameras, demonstrating comparable performance. A portable version limited to\nRGB (3 wavelengths) is used to collect real-world depth and spectral data from\na Masoala Rainforest. We demonstrate the use of this prototype for color image\nreconstruction and material differentiation between leaves and branches using\nspectral and depth data. Our results show that adding depth (available at no\nextra effort with our setup) to material differentiation improves the accuracy\nby over $30\\%$ compared to color-only method. Our system, tested in both lab\nand real-world rainforest environments, shows strong performance in depth\nestimation, RGB reconstruction, and material differentiation - paving the way\nfor lightweight, integrated, and robust UAV perception and data collection in\ncomplex natural environments.", "AI": {"tldr": "本文提出了一种新型事件光谱系统，该系统利用单一传感器同时实现高分辨率、低延迟深度重建和多光谱成像，显著提升了无人机在复杂森林环境中的感知能力和数据收集效率。", "motivation": "无人机在森林环境中执行环境监测和搜救任务时，传统传感方法（如被动多光谱和RGB成像）存在延迟高、深度分辨率差以及对环境光（尤其是在森林冠层下）依赖性强的问题，限制了其在密集植被中安全导航和精确数据收集的能力。", "method": "该研究开发了一种新颖的事件光谱系统。它通过结构光实现深度重建，并通过调制投射结构光的波长来捕获650 nm至850 nm受控波段内的光谱信息。为了进行实际验证，还使用了一个仅限于RGB（3个波长）的便携式版本，在Masoala雨林中收集了真实世界的深度和光谱数据。", "result": "该系统在深度重建方面，与商用深度传感器相比，均方根误差（RMSE）提高了60%。在光谱精度方面，与参考光谱仪和商用多光谱相机表现相当。在材料区分任务中，通过结合深度数据（无需额外努力即可获得），相比仅使用颜色信息的方法，准确性提高了30%以上。该系统在实验室和真实雨林环境中均表现出强大的性能。", "conclusion": "该系统在深度估计、RGB重建和材料区分方面表现出色，为轻量级、集成化和鲁棒的无人机在复杂自然环境中的感知和数据收集铺平了道路。"}}
{"id": "2509.05776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05776", "abs": "https://arxiv.org/abs/2509.05776", "authors": ["Jonathan Aellen", "Florian Burkhardt", "Thomas Vetter", "Marcel Lüthi"], "title": "Posterior shape models revisited: Improving 3D reconstructions from partial data using target specific models", "comment": null, "summary": "In medical imaging, point distribution models are often used to reconstruct\nand complete partial shapes using a statistical model of the full shape. A\ncommonly overlooked, but crucial factor in this reconstruction process, is the\npose of the training data relative to the partial target shape. A difference in\npose alignment of the training and target shape leads to biased solutions,\nparticularly when observing small parts of a shape. In this paper, we\ndemonstrate the importance of pose alignment for partial shape reconstructions\nand propose an efficient method to adjust an existing model to a specific\ntarget. Our method preserves the computational efficiency of linear models\nwhile significantly improving reconstruction accuracy and predicted variance.\nIt exactly recovers the intended aligned model for translations, and provides a\ngood approximation for small rotations, all without access to the original\ntraining data. Hence, existing shape models in reconstruction pipelines can be\nadapted by a simple preprocessing step, making our approach widely applicable\nin plug-and-play scenarios.", "AI": {"tldr": "在医学图像处理中，点分布模型进行部分形状重建时，训练数据与目标形状的姿态对齐至关重要。本文提出一种高效方法，无需原始训练数据即可调整现有模型以适应特定目标姿态，显著提高重建精度和预测方差。", "motivation": "点分布模型在部分形状重建中常忽略训练数据与目标形状的姿态对齐，尤其在观察形状小部分时，这种差异会导致重建结果出现偏差。", "method": "提出了一种高效方法，用于调整现有模型以适应特定目标姿态。该方法在保持线性模型计算效率的同时，无需访问原始训练数据。它能精确恢复平移情况下的对齐模型，并对小旋转提供良好近似。", "result": "显著提高了重建精度和预测方差。对于平移情况，能精确恢复预期的对齐模型；对于小旋转，能提供良好的近似。", "conclusion": "姿态对齐是部分形状重建的关键因素。所提出的方法通过简单的预处理步骤即可调整现有形状模型，使其在重建流程中广泛适用，尤其适用于即插即用场景。"}}
{"id": "2505.00275", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2505.00275", "abs": "https://arxiv.org/abs/2505.00275", "authors": ["Md Asaduzzaman Jabin", "Hanqi Jiang", "Yiwei Li", "Patrick Kaggwa", "Eugene Douglass", "Juliet N. Sekandi", "Tianming Liu"], "title": "AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care", "comment": null, "summary": "Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS,\nepilepsy, and tuberculosis, necessitate rigorous adherence to medication to\navert disease progression, manage symptoms, and decrease mortality rates.\nAdherence is frequently undermined by factors including patient behavior,\ncaregiver support, elevated medical costs, and insufficient healthcare\ninfrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based\nmultimodal large vision language model (LVLM) aimed at visual question\nanswering (VQA) concerning medication adherence through patient videos. We\nemploy a private dataset comprising 806 custom-annotated tuberculosis (TB)\nmedication monitoring videos, which have been labeled by clinical experts, to\nfine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a\ndetailed medical adherence VQA dataset that encompasses positive, negative, and\nambiguous adherence cases. Our method identifies correlations between visual\nfeatures, such as the clear visibility of the patient's face, medication, water\nintake, and the act of ingestion, and their associated medical concepts in\ncaptions. This facilitates the integration of aligned visual-linguistic\nrepresentations and improves multimodal interactions. Experimental results\nindicate that our method surpasses parameter-efficient fine-tuning (PEFT)\nenabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute\nimprovements ranging from 3.1% to 3.54% across pre-trained, regular, and\nlow-rank adaptation (LoRA) configurations. Comprehensive ablation studies and\nattention map visualizations substantiate our approach, enhancing\ninterpretability.", "AI": {"tldr": "AdCare-VLM是一个基于Video-LLaVA的多模态大视觉语言模型，通过分析患者视频进行药物依从性视觉问答（VQA），在结核病药物依从性检测上表现优于现有模型。", "motivation": "慢性病（如糖尿病、高血压、结核病等）需要严格的药物依从性以控制病情和降低死亡率。然而，患者行为、护理支持、医疗成本和基础设施不足等因素经常导致依从性不足，因此需要更有效的依从性监测方法。", "method": "本文提出了AdCare-VLM，一个基于Video-LLaVA的专用多模态大视觉语言模型（LVLM），用于通过患者视频进行药物依从性视觉问答（VQA）。研究团队使用包含806个由临床专家标注的结核病（TB）药物监测视频的私有数据集（LLM-TB-VQA）对模型进行微调，以检测依从性模式。该方法将视觉特征（如患者面部、药物、饮水和吞咽行为的清晰可见性）与相关医学概念相关联，从而整合对齐的视觉-语言表示并增强多模态交互。", "result": "实验结果表明，AdCare-VLM在预训练、常规和低秩适应（LoRA）配置下，相对于基于参数高效微调（PEFT）的VLM模型（如LLaVA-V1.5和Chat-UniVi），实现了3.1%至3.54%的绝对性能提升。全面的消融研究和注意力图可视化也证实了该方法的有效性，并增强了解释性。", "conclusion": "AdCare-VLM通过对患者视频进行VQA，有效提升了药物依从性监测的能力，其在性能和可解释性方面均超越了现有模型，为慢性病药物依从性管理提供了有前景的解决方案。"}}
{"id": "2509.06675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06675", "abs": "https://arxiv.org/abs/2509.06675", "authors": ["Vladislav Stankov", "Matyáš Kopp", "Ondřej Bojar"], "title": "ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data", "comment": null, "summary": "We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0\ncorpus, targeted at speech modeling tasks with the largest variant containing\n2,695 hours. We combined the sound recordings of the Czech parliamentary\nspeeches with the official transcripts. The recordings were processed with\nWhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our\nprocessing pipeline improves upon the ParCzech 3.0 speech recognition version\nby extracting more data with higher alignment reliability. The dataset is\noffered in three flexible variants: (1) sentence-segmented for automatic speech\nrecognition and speech synthesis tasks with clean boundaries, (2) unsegmented\npreserving original utterance flow across sentences, and (3) a raw-alignment\nfor further custom refinement for other possible tasks. All variants maintain\nthe original metadata and are released under a permissive CC-BY license. The\ndataset is available in the LINDAT repository, with the sentence-segmented and\nunsegmented variants additionally available on Hugging Face.", "AI": {"tldr": "本文介绍了 ParCzech4Speech 1.0，这是一个针对语音建模任务的捷克语语音语料库，包含2,695小时数据，通过处理捷克议会演讲录音及其官方文本，并使用 WhisperX 和 Wav2Vec 2.0 进行了自动化音频-文本对齐，提供了更高可靠性的对齐数据。", "motivation": "研究动机是为语音建模任务提供一个更大、更高质量的捷克语语音语料库，以改进现有 ParCzech 3.0 版本的语音识别效果，并解决其数据量和对齐可靠性的不足。", "method": "研究方法是将捷克议会演讲的录音与官方文本结合，使用 WhisperX 和 Wav2Vec 2.0 工具进行自动化音频-文本对齐。通过改进处理流程，提取了更多数据并提高了对齐可靠性。语料库提供了三种灵活的变体：句子分割型、未分割型和原始对齐型。", "result": "主要成果是发布了 ParCzech4Speech 1.0 语料库，其中最大变体包含 2,695 小时的语音数据。该语料库相比 ParCzech 3.0 版本，提取了更多数据并具有更高的对齐可靠性。所有变体都保留了原始元数据，并在 CC-BY 许可下发布，可在 LINDAT 存储库和 Hugging Face 上获取。", "conclusion": "ParCzech4Speech 1.0 是一个大规模、高质量的捷克语语音语料库，通过改进的数据处理和对齐技术，为自动语音识别、语音合成及其他相关语音建模任务提供了宝贵的资源，并具有高度的灵活性和可用性。"}}
{"id": "2509.05780", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05780", "abs": "https://arxiv.org/abs/2509.05780", "authors": ["Jongyoun Noh", "Junghyup Lee", "Hyekang Park", "Bumsub Ham"], "title": "3DPillars: Pillar-based two-stage 3D object detection", "comment": "19 pages, 11 figures", "summary": "PointPillars is the fastest 3D object detector that exploits pseudo image\nrepresentations to encode features for 3D objects in a scene. Albeit efficient,\nPointPillars is typically outperformed by state-of-the-art 3D detection methods\ndue to the following limitations: 1) The pseudo image representations fail to\npreserve precise 3D structures, and 2) they make it difficult to adopt a\ntwo-stage detection pipeline using 3D object proposals that typically shows\nbetter performance than a single-stage approach. We introduce in this paper the\nfirst two-stage 3D detection framework exploiting pseudo image representations,\nnarrowing the performance gaps between PointPillars and state-of-the-art\nmethods, while retaining its efficiency. Our framework consists of two novel\ncomponents that overcome the aforementioned limitations of PointPillars: First,\nwe introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3D\nvoxel-based features from the pseudo image representation efficiently using 2D\nconvolutions. The basic idea behind 3DPillars is that 3D features from voxels\ncan be viewed as a stack of pseudo images. To implement this idea, we propose a\nseparable voxel feature module that extracts voxel-based features without using\n3D convolutions. Second, we introduce an RoI head with a sparse scene context\nfeature module that aggregates multi-scale features from 3DPillars to obtain a\nsparse scene feature. This enables adopting a two-stage pipeline effectively,\nand fully leveraging contextual information of a scene to refine 3D object\nproposals. Experimental results on the KITTI and Waymo Open datasets\ndemonstrate the effectiveness and efficiency of our approach, achieving a good\ncompromise in terms of speed and accuracy.", "AI": {"tldr": "本文提出了一种新型两阶段3D目标检测框架，通过改进PointPillars的伪图像表示和引入两阶段检测范式，在保持效率的同时提升了检测精度，缩小了与最先进方法的性能差距。", "motivation": "PointPillars虽然是速度最快的3D目标检测器之一，但其性能通常不如最先进的方法。主要原因是：1) 伪图像表示未能精确保留3D结构；2) 难以采用通常性能更优的两阶段检测流程（使用3D目标建议）。", "method": "本文提出了两个新颖组件：1) 引入了新的CNN架构3DPillars，它通过将3D体素特征视为伪图像的堆叠，利用2D卷积高效地从伪图像表示中学习3D体素特征，并为此设计了可分离的体素特征模块。2) 引入了一个带有稀疏场景上下文特征模块的RoI头部，该模块聚合来自3DPillars的多尺度特征以获得稀疏场景特征，从而有效地采用两阶段流程并充分利用场景上下文信息来优化3D目标建议。", "result": "在KITTI和Waymo Open数据集上的实验结果表明，该方法在速度和精度之间取得了良好的平衡，证明了其有效性和效率。", "conclusion": "本文提出的框架成功克服了PointPillars的局限性，在保持其效率的同时，缩小了其与最先进3D检测方法之间的性能差距。"}}
{"id": "2509.06704", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06704", "abs": "https://arxiv.org/abs/2509.06704", "authors": ["Amir Homayounirad", "Enrico Liscio", "Tong Wang", "Catholijn M. Jonker", "Luciano C. Siebert"], "title": "Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments", "comment": "Accepted at Findings of EMNLP 2025", "summary": "Aggregating multiple annotations into a single ground truth label may hide\nvaluable insights into annotator disagreement, particularly in tasks where\nsubjectivity plays a crucial role. In this work, we explore methods for\nidentifying subjectivity in recognizing the human values that motivate\narguments. We evaluate two main approaches: inferring subjectivity through\nvalue prediction vs. directly identifying subjectivity. Our experiments show\nthat direct subjectivity identification significantly improves the model\nperformance of flagging subjective arguments. Furthermore, combining\ncontrastive loss with binary cross-entropy loss does not improve performance\nbut reduces the dependency on per-label subjectivity. Our proposed methods can\nhelp identify arguments that individuals may interpret differently, fostering a\nmore nuanced annotation process.", "AI": {"tldr": "本研究探索了识别论点中人类价值观主观性的方法，发现直接识别主观性显著优于通过价值观预测推断主观性，并能提高模型性能，有助于更细致的标注过程。", "motivation": "在主观性至关重要的任务中，将多重标注聚合成单一真实标签可能会掩盖标注者分歧中的宝贵见解。因此，有必要探索识别论点中人类价值观主观性的方法。", "method": "本研究评估了两种主要方法：1. 通过价值观预测推断主观性；2. 直接识别主观性。此外，还探索了结合对比损失与二元交叉熵损失的效果。", "result": "实验表明，直接识别主观性显著提高了标记主观论点的模型性能。结合对比损失与二元交叉熵损失并未提高性能，但减少了对每个标签主观性的依赖。", "conclusion": "本研究提出的方法可以帮助识别个体可能不同解读的论点，从而促进更细致入微的标注过程。"}}
{"id": "2509.05785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05785", "abs": "https://arxiv.org/abs/2509.05785", "authors": ["In-Jae Lee", "Sihwan Hwang", "Youngseok Kim", "Wonjune Kim", "Sanmin Kim", "Dongsuk Kum"], "title": "CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation", "comment": "Accepted by ICRA 2025", "summary": "Recently, camera-radar fusion-based 3D object detection methods in bird's eye\nview (BEV) have gained attention due to the complementary characteristics and\ncost-effectiveness of these sensors. Previous approaches using forward\nprojection struggle with sparse BEV feature generation, while those employing\nbackward projection overlook depth ambiguity, leading to false positives. In\nthis paper, to address the aforementioned limitations, we propose a novel\ncamera-radar fusion-based 3D object detection and segmentation model named CRAB\n(Camera-Radar fusion for reducing depth Ambiguity in Backward projection-based\nview transformation), using a backward projection that leverages radar to\nmitigate depth ambiguity. During the view transformation, CRAB aggregates\nperspective view image context features into BEV queries. It improves depth\ndistinction among queries along the same ray by combining the dense but\nunreliable depth distribution from images with the sparse yet precise depth\ninformation from radar occupancy. We further introduce spatial cross-attention\nwith a feature map containing radar context information to enhance the\ncomprehension of the 3D scene. When evaluated on the nuScenes open dataset, our\nproposed approach achieves a state-of-the-art performance among backward\nprojection-based camera-radar fusion methods with 62.4\\% NDS and 54.0\\% mAP in\n3D object detection.", "AI": {"tldr": "本文提出了一种名为CRAB的新型相机-雷达融合3D目标检测与分割模型，通过利用雷达信息缓解反向投影中深度模糊问题，并在nuScenes数据集上实现了当前反向投影相机-雷达融合方法的最佳性能。", "motivation": "相机-雷达融合的3D目标检测因其互补性和成本效益而备受关注。然而，现有方法存在局限：前向投影方法生成稀疏的BEV特征，而后向投影方法则忽略深度模糊性，导致误报。", "method": "本文提出了CRAB模型，采用反向投影并利用雷达信息来减轻深度模糊。在视图转换过程中，CRAB将透视图图像上下文特征聚合到BEV查询中。它结合了图像中密集但不精确的深度分布与雷达占用中稀疏但精确的深度信息，以提高同一光线中查询的深度区分度。此外，还引入了包含雷达上下文信息的空间交叉注意力，以增强对3D场景的理解。", "result": "在nuScenes开放数据集上进行评估，CRAB在3D目标检测中达到了62.4%的NDS和54.0%的mAP，在基于反向投影的相机-雷达融合方法中实现了最先进的性能。", "conclusion": "CRAB模型通过有效利用雷达信息解决了反向投影中深度模糊的问题，显著提升了相机-雷达融合3D目标检测的性能，达到了该类别方法的领先水平。"}}
{"id": "2509.06795", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06795", "abs": "https://arxiv.org/abs/2509.06795", "authors": ["Yanrui Du", "Fenglei Fan", "Sendong Zhao", "Jiawei Cao", "Qika Lin", "Kai He", "Ting Liu", "Bing Qin", "Mengling Feng"], "title": "Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint", "comment": null, "summary": "Instruction Fine-Tuning (IFT) has been widely adopted as an effective\npost-training strategy to enhance various abilities of Large Language Models\n(LLMs). However, prior studies have shown that IFT can significantly compromise\nLLMs' safety, particularly their ability to refuse malicious instructions,\nraising significant concerns. Recent research into the internal mechanisms of\nLLMs has identified the refusal direction (r-direction) in the hidden states,\nwhich plays a pivotal role in governing refusal behavior. Building on this\ninsight, our study reveals that the r-direction tends to drift during training,\nwhich we identify as one of the causes of the associated safety risks. To\nmitigate such drift, our proposed ProCon method introduces a\nprojection-constrained loss term that regularizes the projection magnitude of\neach training sample's hidden state onto the r-direction. Our initial analysis\nshows that applying an appropriate constraint can effectively mitigate the\nrefusal direction drift and associated safety risks, but remains limited by\noverall performance barriers. To overcome this barrier, informed by our\nobservation of early-stage sharp drift and a data-driven perspective, we\nintroduce a warm-up strategy that emphasizes early-stage strong constraints and\nbroaden the data distribution to strengthen constraint signals, leading to an\nenhanced ProCon method. Experimental results under various datasets, scenarios,\nand LLMs demonstrate that our method can significantly mitigate safety risks\nposed by IFT while preserving task performance gains. Even compared with strong\nbaselines, our method consistently delivers superior overall performance.\nCrucially, our analysis indicates that ProCon can contribute to stabilizing the\nr-direction during training, while such an interpretability-driven exploration\nof LLMs' internal mechanisms lays a solid foundation for future safety\nresearch.", "AI": {"tldr": "指令微调（IFT）会损害大型语言模型（LLMs）的安全性，特别是拒绝恶意指令的能力。本研究提出ProCon方法，通过正则化隐藏状态在拒绝方向（r-direction）上的投影来稳定r-direction，从而在保持任务性能的同时显著提升LLMs的安全性。", "motivation": "指令微调（IFT）虽然能增强LLMs的能力，但会严重损害其安全性，尤其是在拒绝恶意指令方面的能力。现有研究发现LLMs内部存在控制拒绝行为的拒绝方向（r-direction），而本研究发现r-direction在训练过程中会发生漂移，这是导致相关安全风险的原因之一。", "method": "本研究提出了ProCon方法，引入了一个投影约束损失项，用于正则化每个训练样本的隐藏状态在拒绝方向（r-direction）上的投影幅度，以减轻r-direction的漂移。为了克服性能障碍，ProCon进一步引入了热启动策略（强调早期阶段的强约束）并拓宽数据分布以强化约束信号，从而形成增强型ProCon方法。", "result": "ProCon方法能有效缓解拒绝方向漂移和相关的安全风险。增强型ProCon在各种数据集、场景和LLMs上，显著减轻了IFT带来的安全风险，同时保持了任务性能的提升。与强基线相比，该方法始终提供卓越的整体性能。分析表明，ProCon有助于在训练期间稳定r-direction。", "conclusion": "ProCon方法通过稳定LLMs内部的拒绝方向，有效解决了指令微调带来的安全风险，同时保持了模型的任务性能。这种基于可解释性的内部机制探索为未来的LLM安全研究奠定了坚实基础。"}}
{"id": "2509.05796", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05796", "abs": "https://arxiv.org/abs/2509.05796", "authors": ["Julio Zanon Diaz", "Georgios Siogkas", "Peter Corcoran"], "title": "Dual-Mode Deep Anomaly Detection for Medical Manufacturing: Structural Similarity and Feature Distance", "comment": "18 pages, 5 figures, 13 tables", "summary": "Automating visual inspection in medical device manufacturing remains\nchallenging due to small and imbalanced datasets, high-resolution imagery, and\nstringent regulatory requirements. This work proposes two attention-guided\nautoencoder architectures for deep anomaly detection designed to address these\nconstraints. The first employs a structural similarity-based anomaly score\n(4-MS-SSIM), offering lightweight and accurate real-time defect detection,\nyielding ACC 0.903 (unsupervised thresholding) and 0.931 (supervised\nthresholding) on the - Surface Seal Image - Test split with only 10% of\ndefective samples. The second applies a feature-distance approach using\nMahalanobis scoring on reduced latent features, providing high sensitivity to\ndistributional shifts for supervisory monitoring, achieving ACC 0.722 with\nsupervised thresholding. Together, these methods deliver complementary\ncapabilities: the first supports reliable inline inspection, while the second\nenables scalable post-production surveillance and regulatory compliance\nmonitoring. Experimental results demonstrate that both approaches surpass\nre-implemented baselines and provide a practical pathway for deploying deep\nanomaly detection in regulated manufacturing environments, aligning accuracy,\nefficiency, and the regulatory obligations defined for high-risk AI systems\nunder the EU AI Act.", "AI": {"tldr": "本研究提出了两种基于注意力机制的自动编码器架构，用于医疗器械制造中的深度异常检测，以解决小数据集、高分辨率图像和严格监管要求等挑战，并提供了实时检测和生产后监控的互补能力。", "motivation": "医疗器械制造中的自动化视觉检测面临挑战，包括数据集小且不平衡、图像分辨率高以及严格的监管要求。", "method": "本文提出了两种基于注意力机制的自动编码器架构：1) 第一种采用基于结构相似性（4-MS-SSIM）的异常分数，用于轻量级、准确的实时缺陷检测。2) 第二种采用特征距离方法，在降维的潜在特征上使用马哈拉诺比斯（Mahalanobis）评分，以实现对分布变化的敏感监测。", "result": "第一种方法在Surface Seal Image测试集（10%缺陷样本）上，无监督阈值化准确率达0.903，有监督阈值化达0.931，支持可靠的在线检测。第二种方法在有监督阈值化下准确率达0.722，对分布变化具有高敏感性，适用于监督性监控。两种方法均超越了重新实现的基线。", "conclusion": "这些方法提供了互补的能力，支持可靠的在线检测和可扩展的生产后监控及法规遵从性。它们为在受监管的制造环境中部署深度异常检测提供了一条实用途径，符合欧盟人工智能法案对高风险AI系统的要求。"}}
{"id": "2509.06806", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06806", "abs": "https://arxiv.org/abs/2509.06806", "authors": ["Haoyu Dong", "Pengkun Zhang", "Mingzhe Lu", "Yanzhen Shen", "Guolin Ke"], "title": "MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML", "comment": null, "summary": "Large language models (LLMs) possess broad world knowledge and strong\ngeneral-purpose reasoning ability, yet they struggle to learn from many\nin-context examples on standard machine learning (ML) tasks, that is, to\nleverage many-shot demonstrations purely via in-context learning (ICL) without\ngradient descent. We introduce MachineLearningLM, a portable\ncontinued-pretraining framework that equips a general-purpose LLM with robust\nin-context ML capability while preserving its general knowledge and reasoning\nfor broader chat workflows.\n  Our pretraining procedure synthesizes ML tasks from millions of structural\ncausal models (SCMs), spanning shot counts up to 1,024. We begin with a\nrandom-forest teacher, distilling tree-based decision strategies into the LLM\nto strengthen robustness in numerical modeling. All tasks are serialized with a\ntoken-efficient prompt, enabling 3x to 6x more examples per context window and\ndelivering up to 50x amortized throughput via batch inference.\n  Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8),\nMachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an\naverage of about 15% on out-of-distribution tabular classification across\nfinance, physics, biology, and healthcare domains. It exhibits a striking\nmany-shot scaling law: accuracy increases monotonically as in-context\ndemonstrations grow from 8 to 1,024. Without any task-specific training, it\nattains random-forest-level accuracy across hundreds of shots. General chat\ncapabilities, including knowledge and reasoning, are preserved: it achieves\n75.4% on MMLU.", "AI": {"tldr": "该研究引入了MachineLearningLM，一个通过持续预训练使大型语言模型（LLM）具备强大上下文机器学习（ML）能力，同时保留其通用知识和推理能力的框架。", "motivation": "尽管LLM拥有广泛的世界知识和强大的通用推理能力，但它们在标准ML任务中难以从大量上下文示例中学习（即，通过纯粹的上下文学习而非梯度下降来利用多样本演示）。", "method": "通过持续预训练实现，具体方法包括：1) 从数百万结构因果模型（SCM）中合成ML任务，涵盖多达1024个样本。2) 使用随机森林教师模型，将基于树的决策策略蒸馏到LLM中，以增强数值建模的鲁棒性。3) 采用令牌高效的提示，将每个上下文窗口的示例数量增加3到6倍，并通过批处理推理实现高达50倍的摊销吞吐量。4) 在Qwen-2.5-7B-Instruct模型上使用LoRA（rank 8）进行微调。", "result": "MachineLearningLM（Qwen-2.5-7B-Instruct与LoRA rank 8）在金融、物理、生物和医疗保健领域的域外表格分类任务上，平均比强大的LLM基线（如GPT-5-mini）高出约15%。它展现出显著的多样本扩展定律：当上下文演示从8个增加到1024个时，准确率单调上升。在没有特定任务训练的情况下，它在数百个样本上达到了随机森林级别的准确率。同时，其通用聊天能力（包括知识和推理）得到保留，在MMLU上达到75.4%。", "conclusion": "MachineLearningLM成功地为通用LLM提供了强大的上下文ML能力，使其能够有效利用大量上下文示例进行学习，同时保持了其原有的通用知识和推理能力，为更广泛的聊天工作流程提供了支持。"}}
{"id": "2509.05809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05809", "abs": "https://arxiv.org/abs/2509.05809", "authors": ["Tyler Ward", "Abdullah Imran"], "title": "A Probabilistic Segment Anything Model for Ambiguity-Aware Medical Image Segmentation", "comment": "Preprint", "summary": "Recent advances in promptable segmentation, such as the Segment Anything\nModel (SAM), have enabled flexible, high-quality mask generation across a wide\nrange of visual domains. However, SAM and similar models remain fundamentally\ndeterministic, producing a single segmentation per object per prompt, and fail\nto capture the inherent ambiguity present in many real-world tasks. This\nlimitation is particularly troublesome in medical imaging, where multiple\nplausible segmentations may exist due to annotation uncertainty or inter-expert\nvariability. In this paper, we introduce Probabilistic SAM, a probabilistic\nextension of SAM that models a distribution over segmentations conditioned on\nboth the input image and prompt. By incorporating a latent variable space and\ntraining with a variational objective, our model learns to generate diverse and\nplausible segmentation masks reflecting the variability in human annotations.\nThe architecture integrates a prior and posterior network into the SAM\nframework, allowing latent codes to modulate the prompt embeddings during\ninference. The latent space allows for efficient sampling during inference,\nenabling uncertainty-aware outputs with minimal overhead. We evaluate\nProbabilistic SAM on the public LIDC-IDRI lung nodule dataset and demonstrate\nits ability to produce diverse outputs that align with expert disagreement,\noutperforming existing probabilistic baselines on uncertainty-aware metrics.\nOur code is available at: https://github.com/tbwa233/Probabilistic-SAM/.", "AI": {"tldr": "本文提出了Probabilistic SAM，一个SAM的概率扩展，通过建模分割分布来生成多样化且合理的分割掩模，以解决SAM在处理医学图像等任务中固有的歧义性问题。", "motivation": "像SAM这样的现有分割模型是确定性的，每个提示只生成一个分割，无法捕捉许多真实世界任务中固有的歧义性，尤其在医学成像中，由于标注不确定性或专家间差异，可能存在多个合理的分割。", "method": "Probabilistic SAM通过整合一个潜在变量空间并使用变分目标进行训练，学习生成反映人类标注变异性的多样化分割掩模。该架构将一个先验和后验网络集成到SAM框架中，允许潜在代码在推理期间调节提示嵌入，从而在最小开销下实现高效采样和不确定性感知输出。", "result": "在公共LIDC-IDRI肺结节数据集上，Probabilistic SAM能够生成与专家分歧一致的多样化输出。在不确定性感知指标上，它优于现有概率基线，并且推理开销最小。", "conclusion": "Probabilistic SAM通过引入概率建模，成功解决了SAM在处理固有歧义性任务（如医学图像分割）时的局限性，能够生成多样化、合理且不确定性感知的分割结果，与专家意见保持一致。"}}
{"id": "2509.05892", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05892", "abs": "https://arxiv.org/abs/2509.05892", "authors": ["Phongsakon Mark Konrad", "Andrei-Alexandru Popa", "Yaser Sabzehmeidani", "Liang Zhong", "Elisa A. Liehn", "Serkan Ayvaz"], "title": "Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets", "comment": null, "summary": "Accurate segmentation of carotid artery structures in histopathological\nimages is vital for advancing cardiovascular disease research and diagnosis.\nHowever, deep learning model development in this domain is constrained by the\nscarcity of annotated cardiovascular histopathological data. This study\ninvestigates a systematic evaluation of state-of-the-art deep learning\nsegmentation models, including convolutional neural networks (U-Net,\nDeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models\n(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histology\nimages. Despite employing an extensive hyperparameter optimization strategy\nwith Bayesian search, our findings reveal that model performance is highly\nsensitive to data splits, with minor differences driven more by statistical\nnoise than by true algorithmic superiority. This instability exposes the\nlimitations of standard benchmarking practices in low-data clinical settings\nand challenges the assumption that performance rankings reflect meaningful\nclinical utility.", "AI": {"tldr": "本研究在有限的心血管组织病理图像数据集上，系统评估了包括CNNs、ViT和基础模型在内的多种深度学习分割模型。结果发现，模型性能对数据划分高度敏感，微小差异主要源于统计噪声而非算法优越性，这揭示了低数据临床环境中基准测试的局限性。", "motivation": "心血管疾病研究和诊断中，颈动脉结构在组织病理图像中的精确分割至关重要。然而，由于带注释的心血管组织病理数据稀缺，限制了深度学习模型在该领域的发展。", "method": "研究系统评估了多种最先进的深度学习分割模型，包括卷积神经网络（U-Net, DeepLabV3+）、Vision Transformer（SegFormer）以及近期基础模型（SAM, MedSAM, MedSAM+UNet）。评估在一个有限的心血管组织学图像数据集上进行，并采用了贝叶斯搜索进行广泛的超参数优化策略。", "result": "尽管采用了广泛的超参数优化策略，研究发现模型性能对数据划分高度敏感。模型间微小的性能差异更多是由统计噪声而非真正的算法优越性驱动。", "conclusion": "这种不稳定性揭示了在低数据临床环境中，标准基准测试实践的局限性，并挑战了性能排名能够反映有意义临床效用的假设。"}}
{"id": "2509.06807", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06807", "abs": "https://arxiv.org/abs/2509.06807", "authors": ["Yanrui Du", "Fenglei Fan", "Sendong Zhao", "Jiawei Cao", "Ting Liu", "Bing Qin"], "title": "MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security", "comment": null, "summary": "As Large Language Models (LLMs) increasingly permeate human life, their\nsecurity has emerged as a critical concern, particularly their ability to\nmaintain harmless responses to malicious instructions. Although extensive\nmethods have improved LLMs' security, they often lead to conservative,\nrejection-oriented responses that compromise practical usability. This presents\na key challenge: how to advance the Pareto frontier between LLMs' usability and\nsecurity, rather than necessitate a trade-off between them. To address this, we\npropose the MoGU framework, in which the intra-layer router dynamically\nallocates weights by sensing hidden states, thereby balancing the contributions\nof security-optimized and usability-optimized variants. Despite its initial\npotential, the MoGU framework faces limitations such as parameter redundancy\nand performance bottlenecks. To overcome these, we further propose an improved\nMoGU_v2 framework that establishes a tighter coupling between the routers and\nhidden states. In MoGU_v2, routers are embedded only in layers encoding highly\nclassifiable security features, and backbone modules are activated during\nrouter optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong\nadaptability and stable improvements across various series of LLMs, including\nmainstream LLMs serving as brains in various applications, on-device LLMs\noptimized for resource-constrained scenarios, and reasoning LLMs tailored for\nuser interpretability. Meanwhile, even facing risks introduced by Instruction\nFine-tuning, MoGU_v2 can easily restore security without compromising the task\nperformance gains via a simple data-mix strategy. These comprehensive\nimprovements highlight MoGU_V2 as a robust and versatile solution for\nmitigating security risks in real-world applications.", "AI": {"tldr": "本文提出了MoGU_v2框架，通过动态分配权重和优化路由器嵌入，在不牺牲可用性的前提下显著提升了大型语言模型（LLMs）的安全性，并能有效应对指令微调带来的风险。", "motivation": "随着LLMs在人类生活中日益普及，其安全性成为关键问题，特别是如何保持对恶意指令的无害响应。现有安全方法常导致保守的、拒绝式响应，损害了实用性。因此，研究动机是如何在LLM的可用性和安全性之间推进帕累托前沿，而非在两者之间进行权衡。", "method": "首先提出MoGU框架，通过层内路由器感知隐藏状态动态分配权重，以平衡安全优化和可用性优化变体的贡献。为克服MoGU的参数冗余和性能瓶颈，进一步提出MoGU_v2框架。MoGU_v2将路由器仅嵌入到编码高度可分类安全特征的层中，并在路由器优化期间激活主干模块以实现双向适应。此外，面对指令微调引入的风险，MoGU_v2采用简单的数据混合策略来恢复安全性。", "result": "MoGU_v2在各种系列LLMs（包括主流LLMs、资源受限的设备端LLMs和推理LLMs）上均表现出强大的适应性和稳定的改进。同时，即使面对指令微调带来的风险，MoGU_v2也能通过简单的数据混合策略轻松恢复安全性，且不损害任务性能增益。", "conclusion": "MoGU_v2是一个强大且多功能的解决方案，能够有效缓解现实世界应用中LLMs的安全风险，同时保持其实用性，实现了可用性与安全性的双赢。"}}
{"id": "2509.05895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05895", "abs": "https://arxiv.org/abs/2509.05895", "authors": ["Yujie Li", "Wenjia Xu", "Yuanben Zhang", "Zhiwei Wei", "Mugen Peng"], "title": "BTCChat: Advancing Remote Sensing Bi-temporal Change Captioning with Multimodal Large Language Model", "comment": "5 pages, 2 figures Submitted to ICASSP 2026", "summary": "Bi-temporal satellite imagery supports critical applications such as urban\ndevelopment monitoring and disaster assessment. Although powerful multimodal\nlarge language models (MLLMs) have been applied in bi-temporal change analysis,\nprevious methods process image pairs through direct concatenation, inadequately\nmodeling temporal correlations and spatial semantic changes. This deficiency\nhampers visual-semantic alignment in change understanding, thereby constraining\nthe overall effectiveness of current approaches. To address this gap, we\npropose BTCChat, a multi-temporal MLLM with advanced bi-temporal change\nunderstanding capability. BTCChat supports bi-temporal change captioning and\nretains single-image interpretation capability. To better capture temporal\nfeatures and spatial semantic changes in image pairs, we design a Change\nExtraction module. Moreover, to enhance the model's attention to spatial\ndetails, we introduce a Prompt Augmentation mechanism, which incorporates\ncontextual clues into the prompt to enhance model performance. Experimental\nresults demonstrate that BTCChat achieves state-of-the-art performance on\nchange captioning and visual question answering tasks.", "AI": {"tldr": "BTCChat是一种先进的多时相多模态大语言模型（MLLM），通过设计变更提取模块和提示增强机制，显著提升了双时相图像的变更理解能力，并在变更描述和视觉问答任务上取得了SOTA性能。", "motivation": "现有的多模态大语言模型在双时相变更分析中，通过直接拼接图像对的方式处理，未能充分建模时间相关性和空间语义变化，导致变更理解中的视觉-语义对齐不足，从而限制了整体效果。", "method": "本文提出了BTCChat，一个具有高级双时相变更理解能力的多时相MLLM。它包含一个“变更提取模块”来更好地捕获图像对中的时间特征和空间语义变化，以及一个“提示增强机制”来通过将上下文线索融入提示中，以增强模型对空间细节的关注。", "result": "实验结果表明，BTCChat在变更描述和视觉问答任务上均达到了最先进的性能。", "conclusion": "BTCChat通过其创新的模块设计，有效解决了双时相变更分析中时间相关性和空间语义变化的建模不足问题，显著提升了模型的变更理解能力和对空间细节的关注，从而在相关任务上取得了卓越表现。"}}
{"id": "2509.05975", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05975", "abs": "https://arxiv.org/abs/2509.05975", "authors": ["Nam Duong Tran", "Nam Nguyen Phuong", "Hieu H. Pham", "Phi Le Nguyen", "My T. Thai"], "title": "ConstStyle: Robust Domain Generalization with Unified Style Transformation", "comment": "Accepted at ICCV 2025", "summary": "Deep neural networks often suffer performance drops when test data\ndistribution differs from training data. Domain Generalization (DG) aims to\naddress this by focusing on domain-invariant features or augmenting data for\ngreater diversity. However, these methods often struggle with limited training\ndomains or significant gaps between seen (training) and unseen (test) domains.\nTo enhance DG robustness, we hypothesize that it is essential for the model to\nbe trained on data from domains that closely resemble unseen test domains-an\ninherently difficult task due to the absence of prior knowledge about the\nunseen domains. Accordingly, we propose ConstStyle, a novel approach that\nleverages a unified domain to capture domain-invariant features and bridge the\ndomain gap with theoretical analysis. During training, all samples are mapped\nonto this unified domain, optimized for seen domains. During testing, unseen\ndomain samples are projected similarly before predictions. By aligning both\ntraining and testing data within this unified domain, ConstStyle effectively\nreduces the impact of domain shifts, even with large domain gaps or few seen\ndomains. Extensive experiments demonstrate that ConstStyle consistently\noutperforms existing methods across diverse scenarios. Notably, when only a\nlimited number of seen domains are available, ConstStyle can boost accuracy up\nto 19.82\\% compared to the next best approach.", "AI": {"tldr": "ConstStyle是一种新颖的域泛化方法，通过将所有训练和测试样本映射到一个统一域来捕捉域不变特征并弥合域差距，从而有效应对域偏移，尤其在训练域有限或域差距较大时表现优异。", "motivation": "深度神经网络在训练和测试数据分布不同时性能会下降。现有的域泛化（DG）方法，如关注域不变特征或数据增强，在训练域有限或已知与未知域之间存在显著差距时，往往表现不佳。", "method": "本文提出了ConstStyle方法。该方法利用一个统一域来捕获域不变特征并弥合域差距。在训练阶段，所有样本都被映射到这个为已知域优化的统一域中。在测试阶段，未知域样本在预测前也以类似方式投影到该统一域中。通过在统一域中对齐训练和测试数据，该方法有效减少了域偏移的影响。", "result": "广泛的实验表明，ConstStyle在各种场景下持续优于现有方法。值得注意的是，当可用训练域数量有限时，ConstStyle的准确性比次优方法提高了高达19.82%。", "conclusion": "ConstStyle通过引入统一域的概念，成功地增强了域泛化的鲁棒性，有效解决了有限训练域和巨大域差距带来的挑战，并在实验中展现出显著的性能提升。"}}
{"id": "2509.06809", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06809", "abs": "https://arxiv.org/abs/2509.06809", "authors": ["Valentin Quesnel", "Damien Sileo"], "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem", "comment": null, "summary": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1", "AI": {"tldr": "本文提出了一种利用自动化定理证明器（E-prover）从TPTP公理库生成大规模、逻辑有效的数学推理数据集的方法，以解决LLMs在数学推理方面高质量数据稀缺的问题，并诊断出当前LLMs在深层结构化推理上的明显弱点。", "motivation": "高质量、逻辑严谨的数据稀缺是阻碍大型语言模型（LLMs）在数学推理方面取得进展的关键瓶颈。现有的方法要么依赖易出错的LLMs，要么需要复杂的证明助手语法。", "method": "该研究利用E-prover的饱和推理能力，基于TPTP公理库推导出大量保证有效的定理。其数据生成流程包括：饱和公理、筛选“有趣”的定理，然后生成任务。整个过程中不涉及LLMs，从而消除了事实错误。生成的纯符号数据被转化为三种难度受控的挑战：蕴涵验证、前提选择和证明重构。", "result": "对前沿模型进行的零样本实验表明，LLMs在需要深层、结构化推理的任务上表现明显崩溃，揭示了其在这方面的明确弱点。", "conclusion": "该框架提供了一个诊断工具来衡量LLMs在深层推理上的差距，同时也是一个可扩展的符号训练数据来源，以解决这一问题。所有代码和数据均已公开。"}}
{"id": "2509.05913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05913", "abs": "https://arxiv.org/abs/2509.05913", "authors": ["Md. Abdur Rahman", "Mohaimenul Azam Khan Raiaan", "Tamanna Shermin", "Md Rafiqul Islam", "Mukhtar Hussain", "Sami Azam"], "title": "A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features", "comment": "16 pages, 6 figures, 8 tables", "summary": "Musculoskeletal disorders pose significant risks to athletes, and assessing\nrisk early is important for prevention. However, most existing methods are\ndesigned for controlled settings and fail to reliably assess risk in complex\nenvironments due to their reliance on a single type of data. This research\nproposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novel\nmultimodal deep learning framework designed to classify musculoskeletal risk\nusing visual and skeletal coordinate-based features. In addition, a custom\nmultimodal dataset is constructed by combining visual data and skeletal\ncoordinates for risk assessment. Each sample is labeled into eight risk\ncategories based on the Rapid Entire Body Assessment system. ViSK-GAT combines\na Residual Block with a Lightweight Transformer Block to learn spatial and\ntemporal dependencies jointly. It incorporates two novel modules: the\nFine-Grained Attention Module (FGAM), which enables precise inter-modal feature\nrefinement through cross-attention between visual and skeletal inputs, and the\nMultimodal Geometric Correspondence Module (MGCM), which enhances cross-modal\ncoherence by aligning image features with coordinate-based representations.\nViSK-GAT achieved strong performance with validation and test accuracies of\n93.55\\% and 93.89\\%, respectively; a precision of 93.86\\%; an F1 score of\n93.85\\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\\%. The\nregression results also indicated a low Root Mean Square Error of the predicted\nprobability distribution of 0.1205 and a corresponding Mean Absolute Error of\n0.0156. Compared to nine popular transfer learning backbones, ViSK-GAT\nconsistently outperformed previous methods. The ViSK-GAT model advances\nartificial intelligence implementation and application, transforming\nmusculoskeletal risk classification and enabling impactful early interventions\nin sports.", "AI": {"tldr": "本研究提出了一种名为ViSK-GAT的新型多模态深度学习框架，通过结合视觉和骨骼坐标数据，对肌肉骨骼疾病风险进行分类，并在定制数据集上取得了93%以上的高准确率，超越了现有方法。", "motivation": "现有肌肉骨骼疾病风险评估方法主要针对受控环境设计，且依赖单一数据类型，导致在复杂环境中评估不可靠。早期风险评估对预防至关重要。", "method": "研究构建了一个包含视觉数据和骨骼坐标的定制多模态数据集，并根据REBA系统将样本标记为八个风险类别。提出ViSK-GAT框架，结合残差块和轻量级Transformer块来共同学习时空依赖性。该框架包含两个新模块：细粒度注意力模块（FGAM），通过视觉和骨骼输入之间的交叉注意力实现模态间特征细化；多模态几何对应模块（MGCM），通过将图像特征与基于坐标的表示对齐来增强跨模态一致性。", "result": "ViSK-GAT在验证集和测试集上分别取得了93.55%和93.89%的准确率，精确率为93.86%，F1分数为93.85%，Cohen's Kappa和Matthews相关系数均为93%。回归结果显示预测概率分布的均方根误差为0.1205，平均绝对误差为0.0156。与九种流行的迁移学习骨干网络相比，ViSK-GAT表现持续优于现有方法。", "conclusion": "ViSK-GAT模型在肌肉骨骼风险分类方面推动了人工智能的实施和应用，有望通过早期干预对运动领域产生深远影响。"}}
{"id": "2509.05999", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.05999", "abs": "https://arxiv.org/abs/2509.05999", "authors": ["Diana-Alexandra Sas", "Florin Oniga"], "title": "S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion", "comment": "6 pages. Accepted to MMSP 2025", "summary": "Monocular 3D Object Detection represents a challenging Computer Vision task\ndue to the nature of the input used, which is a single 2D image, lacking in any\ndepth cues and placing the depth estimation problem as an ill-posed one.\nExisting solutions leverage the information extracted from the input by using\nConvolutional Neural Networks or Transformer architectures as feature\nextraction backbones, followed by specific detection heads for 3D parameters\nprediction. In this paper, we introduce a decoupled strategy based on injecting\nprecomputed segmentation information priors and fusing them directly into the\nfeature space for guiding the detection, without expanding the detection model\nor jointly learning the priors. The focus is on evaluating the impact of\nadditional segmentation information on existing detection pipelines without\nadding additional prediction branches. The proposed method is evaluated on the\nKITTI 3D Object Detection Benchmark, outperforming the equivalent architecture\nthat relies only on RGB image features for small objects in the scene:\npedestrians and cyclists, and proving that understanding the input data can\nbalance the need for additional sensors or training data.", "AI": {"tldr": "本文提出了一种解耦策略，通过将预计算的分割先验信息直接融入特征空间，来增强单目3D目标检测，尤其在不增加模型复杂度的前提下，提升了对行人、骑行者等小型目标的检测性能。", "motivation": "单目3D目标检测因输入仅为2D图像而缺乏深度信息，是一个难题。现有解决方案依赖于CNN或Transformer进行特征提取，但仍面临深度估计的挑战。研究旨在不扩展检测模型或联合学习先验的情况下，提高单目3D检测的性能。", "method": "引入了一种解耦策略：将预先计算的分割信息先验直接注入到特征空间中，以指导检测。该方法不扩展检测模型，也不联合学习先验，而是专注于评估额外分割信息对现有检测管线的影响，且不增加额外的预测分支。", "result": "在KITTI 3D目标检测基准上进行评估，结果表明，与仅依赖RGB图像特征的等效架构相比，该方法在场景中的小型目标（如行人、骑行者）上表现更优。", "conclusion": "研究证明，理解输入数据（通过注入分割先验）可以平衡对额外传感器或训练数据的需求，有效提升单目3D目标检测的性能，尤其对于小型对象。"}}
{"id": "2509.06813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06813", "abs": "https://arxiv.org/abs/2509.06813", "authors": ["Max Malyi", "Jonathan Shek", "Alasdair McDonald", "Andre Biscaya"], "title": "A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs", "comment": "Associated GitHub repository:\n  https://github.com/mvmalyi/wind-farm-maintenance-logs-labelling-with-llms", "summary": "Effective Operation and Maintenance (O&M) is critical to reducing the\nLevelised Cost of Energy (LCOE) from wind power, yet the unstructured,\nfree-text nature of turbine maintenance logs presents a significant barrier to\nautomated analysis. Our paper addresses this by presenting a novel and\nreproducible framework for benchmarking Large Language Models (LLMs) on the\ntask of classifying these complex industrial records. To promote transparency\nand encourage further research, this framework has been made publicly available\nas an open-source tool. We systematically evaluate a diverse suite of\nstate-of-the-art proprietary and open-source LLMs, providing a foundational\nassessment of their trade-offs in reliability, operational efficiency, and\nmodel calibration. Our results quantify a clear performance hierarchy,\nidentifying top models that exhibit high alignment with a benchmark standard\nand trustworthy, well-calibrated confidence scores. We also demonstrate that\nclassification performance is highly dependent on the task's semantic\nambiguity, with all models showing higher consensus on objective component\nidentification than on interpretive maintenance actions. Given that no model\nachieves perfect accuracy and that calibration varies dramatically, we conclude\nthat the most effective and responsible near-term application is a\nHuman-in-the-Loop system, where LLMs act as a powerful assistant to accelerate\nand standardise data labelling for human experts, thereby enhancing O&M data\nquality and downstream reliability analysis.", "AI": {"tldr": "本研究提出了一个基准测试框架，用于评估大型语言模型（LLMs）在风力涡轮机维护日志分类任务上的表现，并建议采用人机协作（Human-in-the-Loop）系统以提高运营维护数据质量。", "motivation": "有效的运营和维护（O&M）对于降低风电的度电成本（LCOE）至关重要，但涡轮机维护日志的非结构化自由文本性质，严重阻碍了自动化分析。", "method": "开发了一个新颖且可复现的框架，用于基准测试LLMs在分类复杂工业记录（风力涡轮机维护日志）方面的能力。该框架已开源。系统评估了多种最先进的专有和开源LLMs，从可靠性、操作效率和模型校准等方面进行了评估。", "result": "量化了LLMs的清晰性能等级，识别出与基准标准高度一致且具有可靠、良好校准置信度分数的顶级模型。分类性能高度依赖于任务的语义模糊性，所有模型在客观组件识别上比在解释性维护操作上表现出更高的一致性。没有模型达到完美准确性，且校准差异显著。", "conclusion": "鉴于没有模型能达到完美精度且校准差异巨大，最有效和负责任的近期应用是人机协作系统，其中LLMs作为强大助手，加速并标准化人类专家的数据标注工作，从而提高O&M数据质量和下游可靠性分析。"}}
{"id": "2509.05925", "categories": ["cs.CV", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2509.05925", "abs": "https://arxiv.org/abs/2509.05925", "authors": ["Ruiqi Shen", "Haotian Wu", "Wenjing Zhang", "Jiangjing Hu", "Deniz Gunduz"], "title": "Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models", "comment": "Published as a conference paper at IEEE 35th Workshop on Machine\n  Learning for Signal Processing (MLSP)", "summary": "Recent deep learning-based methods for lossy image compression achieve\ncompetitive rate-distortion performance through extensive end-to-end training\nand advanced architectures. However, emerging applications increasingly\nprioritize semantic preservation over pixel-level reconstruction and demand\nrobust performance across diverse data distributions and downstream tasks.\nThese challenges call for advanced semantic compression paradigms. Motivated by\nthe zero-shot and representational capabilities of multimodal foundation\nmodels, we propose a novel semantic compression method based on the contrastive\nlanguage-image pretraining (CLIP) model. Rather than compressing images for\nreconstruction, we propose compressing the CLIP feature embeddings into minimal\nbits while preserving semantic information across different tasks. Experiments\nshow that our method maintains semantic integrity across benchmark datasets,\nachieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. This\nis less than 5% of the bitrate required by mainstream image compression\napproaches for comparable performance. Remarkably, even under extreme\ncompression, the proposed approach exhibits zero-shot robustness across diverse\ndata distributions and downstream tasks.", "AI": {"tldr": "本文提出了一种基于CLIP模型的语义图像压缩方法，通过压缩CLIP特征嵌入而非原始图像，在极低比特率下实现了卓越的语义信息保持和零样本泛化能力。", "motivation": "传统的深度学习图像压缩侧重于像素级重建，但新兴应用更强调语义保持、跨数据分布的鲁棒性以及对下游任务的支持。多模态基础模型（如CLIP）的零样本和表征能力为此提供了灵感。", "method": "该方法不直接压缩图像以供重建，而是将CLIP模型的特征嵌入压缩成最小比特，旨在跨不同任务保留语义信息。", "result": "实验表明，该方法在基准数据集上保持了语义完整性，平均比特率约为2-3 * 10^-3 bpp，仅为主流图像压缩方法在可比性能下所需比特率的不到5%。即使在极端压缩下，该方法仍表现出跨不同数据分布和下游任务的零样本鲁棒性。", "conclusion": "所提出的基于CLIP的语义压缩方法显著降低了比特率，同时有效保留了语义完整性，并展现出强大的零样本鲁棒性，满足了新兴应用对语义压缩的需求。"}}
{"id": "2509.06006", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06006", "abs": "https://arxiv.org/abs/2509.06006", "authors": ["Omkar Prabhu"], "title": "Khana: A Comprehensive Indian Cuisine Dataset", "comment": null, "summary": "As global interest in diverse culinary experiences grows, food image models\nare essential for improving food-related applications by enabling accurate food\nrecognition, recipe suggestions, dietary tracking, and automated meal planning.\nDespite the abundance of food datasets, a noticeable gap remains in capturing\nthe nuances of Indian cuisine due to its vast regional diversity, complex\npreparations, and the lack of comprehensive labeled datasets that cover its\nfull breadth. Through this exploration, we uncover Khana, a new benchmark\ndataset for food image classification, segmentation, and retrieval of dishes\nfrom Indian cuisine. Khana fills the gap by establishing a taxonomy of Indian\ncuisine and offering around 131K images in the dataset spread across 80 labels,\neach with a resolution of 500x500 pixels. This paper describes the dataset\ncreation process and evaluates state-of-the-art models on classification,\nsegmentation, and retrieval as baselines. Khana bridges the gap between\nresearch and development by providing a comprehensive and challenging benchmark\nfor researchers while also serving as a valuable resource for developers\ncreating real-world applications that leverage the rich tapestry of Indian\ncuisine. Webpage: https://khana.omkar.xyz", "AI": {"tldr": "本文介绍了Khana，一个用于印度菜肴图像分类、分割和检索的新基准数据集，旨在填补现有数据集中印度菜肴多样性覆盖不足的空白。", "motivation": "随着全球对多样化烹饪体验的兴趣日益增长，食物图像模型在食品相关应用中至关重要。然而，尽管食物数据集丰富，但由于印度菜肴的区域多样性、复杂制备和缺乏全面的标注数据集，现有数据集未能充分捕捉其细微差别。", "method": "研究团队创建了Khana数据集，建立了印度菜肴的分类体系，并收集了约13.1万张图片，涵盖80个标签，每张图片分辨率为500x500像素。论文描述了数据集创建过程，并评估了最先进的模型在分类、分割和检索任务上的表现作为基线。", "result": "Khana数据集成功弥补了印度菜肴数据集中存在的空白，提供了包含约13.1万张图像（涵盖80种印度菜肴）的综合性资源，并为分类、分割和检索任务设定了基线性能。", "conclusion": "Khana数据集为研究人员提供了一个全面且具有挑战性的基准，同时也是开发人员创建利用印度丰富烹饪文化的应用的宝贵资源，从而弥合了研究与开发之间的鸿沟。"}}
{"id": "2509.06836", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06836", "abs": "https://arxiv.org/abs/2509.06836", "authors": ["Eugene Kwek", "Wenpeng Yin"], "title": "COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens", "comment": null, "summary": "Making LLMs more efficient in memory, latency, and serving cost is crucial\nfor edge deployment, interactive applications, and sustainable inference at\nscale. Pruning is a key technique toward this goal. However, prior pruning\nmethods are limited: width pruning often breaks the standard transformer layout\nor requires custom inference code, while depth pruning removes entire layers\nand can cause abrupt accuracy drops. In this work, we propose COMPACT, which\njointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii)\nprunes FFN intermediate channels using common-token-weighted activations,\naligning importance with the post-pruning token distribution. COMPACT enjoys\nmerits of both depth and width pruning, such as: deployment-friendliness (keeps\na standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN\npruning), training-free operation with competitive pruning time, and strong\nmemory savings alongside throughput gains. Experiments across Qwen, LLaMA, and\nGemma families (0.5B-70B) show state-of-the-art downstream task performance at\nsimilar or higher pruning ratios, with substantial reductions in parameters,\nGPU memory, and end-to-end latency.", "AI": {"tldr": "COMPACT提出了一种联合剪枝方法，通过修剪罕见词汇和FFN中间通道来提高大型语言模型的效率，同时保持标准Transformer架构，实现了最先进的性能和显著的资源节省。", "motivation": "大型语言模型（LLMs）在内存、延迟和服务成本方面的效率对于边缘部署、交互式应用和规模化推理至关重要。现有的剪枝方法存在局限性：宽度剪枝通常会破坏标准Transformer布局或需要自定义推理代码，而深度剪枝会移除整个层并可能导致准确率急剧下降。", "method": "本文提出了COMPACT方法，该方法联合进行剪枝：(i) 剪枝罕见词汇以缩小嵌入/解嵌入层；(ii) 使用常见词元加权激活来剪枝FFN中间通道，从而将重要性与剪枝后的词元分布对齐。COMPACT具有部署友好（保持标准Transformer架构）、规模自适应（可在词汇与FFN剪枝之间权衡）、免训练且剪枝时间具有竞争力等优点。", "result": "在Qwen、LLaMA和Gemma系列（0.5B-70B）模型上的实验表明，在相似或更高的剪枝率下，COMPACT实现了最先进的下游任务性能，并显著减少了参数、GPU内存和端到端延迟，同时带来了强大的内存节省和吞吐量提升。", "conclusion": "COMPACT是一种有效且高效的LLM剪枝方法，它结合了深度和宽度剪枝的优点，解决了现有方法的局限性，实现了部署友好、高性能和显著资源节省，是实现大规模可持续推理的关键技术。"}}
{"id": "2509.05949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05949", "abs": "https://arxiv.org/abs/2509.05949", "authors": ["Qiqi Zhan", "Shiwei Li", "Qingjie Liu", "Yunhong Wang"], "title": "AttriPrompt: Dynamic Prompt Composition Learning for CLIP", "comment": null, "summary": "The evolution of prompt learning methodologies has driven exploration of\ndeeper prompt designs to enhance model performance. However, current deep text\nprompting approaches suffer from two critical limitations: Over-reliance on\nconstrastive learning objectives that prioritize high-level semantic alignment,\nneglecting fine-grained feature optimization; Static prompts across all input\ncategories, preventing content-aware adaptation. To address these limitations,\nwe propose AttriPrompt-a novel framework that enhances and refines textual\nsemantic representations by leveraging the intermediate-layer features of\nCLIP's vision encoder. We designed an Attribute Retrieval module that first\nclusters visual features from each layer. The aggregated visual features\nretrieve semantically similar prompts from a prompt pool, which are then\nconcatenated to the input of every layer in the text encoder. Leveraging\nhierarchical visual information embedded in prompted text features, we\nintroduce Dual-stream Contrastive Learning to realize fine-grained alignment.\nFurthermore, we introduce a Self-Regularization mechanism by applying explicit\nregularization constraints between the prompted and non-prompted text features\nto prevent overfitting on limited training data. Extensive experiments across\nthree benchmarks demonstrate AttriPrompt's superiority over state-of-the-art\nmethods, achieving up to 7.37\\% improvement in the base-to-novel setting. The\nobserved strength of our method in cross-domain knowledge transfer positions\nvision-language pre-trained models as more viable solutions for real-world\nimplementation.", "AI": {"tldr": "AttriPrompt是一种新颖的深度文本提示框架，通过利用CLIP视觉编码器的中间层特征和引入属性检索、双流对比学习和自正则化机制，解决了现有方法对对比学习的过度依赖和提示静态性的问题，显著提升了模型性能和跨域知识迁移能力。", "motivation": "当前的深度文本提示方法存在两个主要限制：1. 过度依赖对比学习目标，侧重于高层语义对齐而忽视细粒度特征优化。2. 提示在所有输入类别中都是静态的，缺乏内容感知适应性。", "method": "本文提出了AttriPrompt框架。它首先设计了一个属性检索模块，该模块对CLIP视觉编码器每一层的视觉特征进行聚类，然后从提示池中检索语义相似的提示，并将其连接到文本编码器每一层的输入。接着，利用提示文本特征中嵌入的层次视觉信息，引入了双流对比学习以实现细粒度对齐。此外，通过在有提示和无提示文本特征之间应用显式正则化约束，引入了自正则化机制，以防止在有限训练数据上过拟合。", "result": "AttriPrompt在三个基准测试中表现出优于现有方法的性能，在“从基础到新颖”设置中实现了高达7.37%的改进。该方法在跨域知识迁移方面的强大能力，使得视觉-语言预训练模型成为更可行的实际应用解决方案。", "conclusion": "AttriPrompt通过有效利用视觉编码器的中间层特征并引入创新的学习机制，成功克服了现有深度文本提示方法的局限性，显著提升了模型性能和跨域泛化能力，为视觉-语言预训练模型在实际世界中的应用开辟了新途径。"}}
{"id": "2509.06035", "categories": ["cs.CV", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.06035", "abs": "https://arxiv.org/abs/2509.06035", "authors": ["Jiaming Cui"], "title": "TinyDef-DETR:An Enhanced DETR Detector for UAV Power Line Defect Detection", "comment": null, "summary": "Automated inspection of transmission lines using UAVs is hindered by the\ndifficulty of detecting small and ambiguous defects against complex\nbackgrounds. Conventional detectors often suffer from detail loss due to\nstrided downsampling, weak boundary sensitivity in lightweight backbones, and\ninsufficient integration of global context with local cues. To address these\nchallenges, we propose TinyDef-DETR, a DETR-based framework designed for\nsmall-defect detection. The method introduces a stride-free space-to-depth\nmodule for lossless downsampling, an edge-enhanced convolution for\nboundary-aware feature extraction, a cross-stage dual-domain multi-scale\nattention module to jointly capture global and local information, and a\nFocaler-Wise-SIoU regression loss to improve localization of small objects.\nExperiments conducted on the CSG-ADCD dataset demonstrate that TinyDef-DETR\nachieves substantial improvements in both precision and recall compared to\ncompetitive baselines, with particularly notable gains on small-object subsets,\nwhile incurring only modest computational overhead. Further validation on the\nVisDrone benchmark confirms the generalization capability of the proposed\napproach. Overall, the results indicate that integrating detail-preserving\ndownsampling, edge-sensitive representations, dual-domain attention, and\ndifficulty-adaptive regression provides a practical and efficient solution for\nUAV-based small-defect inspection in power grids.", "AI": {"tldr": "本文提出TinyDef-DETR框架，通过无步长降采样、边缘增强卷积、双域多尺度注意力及Focaler-Wise-SIoU损失，有效解决了无人机输电线路巡检中小缺陷检测的难题。", "motivation": "现有检测器在复杂背景下检测无人机输电线路中的微小模糊缺陷时面临挑战，具体表现为步长降采样导致细节丢失、轻量级骨干网络边界敏感性弱以及全局上下文与局部线索融合不足。", "method": "本文提出了TinyDef-DETR，一个基于DETR的框架，包含：1) 无步长空-深模块实现无损降采样；2) 边缘增强卷积用于边界感知特征提取；3) 跨阶段双域多尺度注意力模块共同捕获全局和局部信息；4) Focaler-Wise-SIoU回归损失以改进小目标定位。", "result": "在CSG-ADCD数据集上的实验表明，TinyDef-DETR在精度和召回率上均显著优于现有基线，尤其在小目标子集上表现突出，同时计算开销适中。在VisDrone基准测试上的进一步验证也证实了该方法的泛化能力。", "conclusion": "研究结果表明，整合细节保留降采样、边缘敏感表示、双域注意力以及难度自适应回归，为电力电网中基于无人机的小缺陷检测提供了一个实用且高效的解决方案。"}}
{"id": "2509.06838", "categories": ["cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06838", "abs": "https://arxiv.org/abs/2509.06838", "authors": ["Mohammad Reza Mirbagheri", "Mohammad Mahdi Mirkamali", "Zahra Motoshaker Arani", "Ali Javeri", "Amir Mahdi Sadeghzadeh", "Rasool Jalili"], "title": "EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs), trained on extensive datasets using advanced\ndeep learning architectures, have demonstrated remarkable performance across a\nwide range of language tasks, becoming a cornerstone of modern AI technologies.\nHowever, ensuring their trustworthiness remains a critical challenge, as\nreliability is essential not only for accurate performance but also for\nupholding ethical, cultural, and social values. Careful alignment of training\ndata and culturally grounded evaluation criteria are vital for developing\nresponsible AI systems. In this study, we introduce the EPT (Evaluation of\nPersian Trustworthiness) metric, a culturally informed benchmark specifically\ndesigned to assess the trustworthiness of LLMs across six key aspects:\ntruthfulness, safety, fairness, robustness, privacy, and ethical alignment. We\ncurated a labeled dataset and evaluated the performance of several leading\nmodels - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and\nQwen - using both automated LLM-based and human assessments. Our results reveal\nsignificant deficiencies in the safety dimension, underscoring the urgent need\nfor focused attention on this critical aspect of model behavior. Furthermore,\nour findings offer valuable insights into the alignment of these models with\nPersian ethical-cultural values and highlight critical gaps and opportunities\nfor advancing trustworthy and culturally responsible AI. The dataset is\npublicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.", "AI": {"tldr": "本研究引入了EPT（波斯语可信度评估）指标，这是一个文化敏感的基准，用于评估大型语言模型（LLMs）在真实性、安全性、公平性、鲁棒性、隐私和伦理对齐六个方面的可信度。通过对多个主流模型进行评估，发现安全性方面存在显著缺陷，并提供了关于模型与波斯伦理文化价值观对齐的见解。", "motivation": "大型语言模型（LLMs）性能卓越，但确保其可信度（包括准确性、伦理、文化和社会价值观）仍是关键挑战。开发负责任的AI系统需要仔细调整训练数据和基于文化的评估标准。本研究旨在解决LLMs在波斯文化背景下的可信度评估问题。", "method": "研究引入了EPT（波斯语可信度评估）指标，该指标专门设计用于评估LLMs在真实性、安全性、公平性、鲁棒性、隐私和伦理对齐六个关键方面的可信度。研究人员策划了一个带标签的数据集，并使用自动化LLM评估和人工评估两种方式，对包括ChatGPT、Claude、DeepSeek、Gemini、Grok、LLaMA、Mistral和Qwen在内的多个主流模型进行了性能评估。", "result": "评估结果揭示了模型在安全性维度上存在显著缺陷，凸显了对这一关键行为方面进行重点关注的紧迫性。此外，研究结果为这些模型与波斯伦理文化价值观的对齐提供了宝贵见解，并指出了在推进可信赖和文化负责任的AI方面存在的关键差距和机遇。", "conclusion": "本研究强调了LLMs安全性维度亟需关注，并为理解和改进LLMs在波斯伦理文化背景下的可信度和文化对齐提供了重要发现。所构建的数据集已公开，为未来研究奠定了基础，以开发更可信赖和文化负责任的AI系统。"}}
{"id": "2509.05952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05952", "abs": "https://arxiv.org/abs/2509.05952", "authors": ["Feng Wang", "Zihao Yu"], "title": "Coefficients-Preserving Sampling for Reinforcement Learning with Flow Matching", "comment": "work in progress", "summary": "Reinforcement Learning (RL) has recently emerged as a powerful technique for\nimproving image and video generation in Diffusion and Flow Matching models,\nspecifically for enhancing output quality and alignment with prompts. A\ncritical step for applying online RL methods on Flow Matching is the\nintroduction of stochasticity into the deterministic framework, commonly\nrealized by Stochastic Differential Equation (SDE). Our investigation reveals a\nsignificant drawback to this approach: SDE-based sampling introduces pronounced\nnoise artifacts in the generated images, which we found to be detrimental to\nthe reward learning process. A rigorous theoretical analysis traces the origin\nof this noise to an excess of stochasticity injected during inference. To\naddress this, we draw inspiration from Denoising Diffusion Implicit Models\n(DDIM) to reformulate the sampling process. Our proposed method,\nCoefficients-Preserving Sampling (CPS), eliminates these noise artifacts. This\nleads to more accurate reward modeling, ultimately enabling faster and more\nstable convergence for reinforcement learning-based optimizers like Flow-GRPO\nand Dance-GRPO. Code will be released at https://github.com/IamCreateAI/FlowCPS", "AI": {"tldr": "本文提出了一种名为系数保持采样（CPS）的新方法，用于解决在线强化学习应用于Flow Matching模型时，SDE采样引入的噪声伪影问题。CPS消除了这些噪声，从而实现了更准确的奖励建模，并加速了RL优化器的收敛。", "motivation": "强化学习（RL）在改善扩散和Flow Matching模型的图像/视频生成质量和与提示对齐方面表现出色。然而，将在线RL应用于Flow Matching需要引入随机性，通常通过随机微分方程（SDE）实现。研究发现SDE采样会引入明显的噪声伪影，严重损害奖励学习过程。", "method": "通过严格的理论分析，作者将噪声的来源追溯到推理过程中注入的过度随机性。受Denoising Diffusion Implicit Models（DDIM）的启发，本文重新制定了采样过程，并提出了系数保持采样（Coefficients-Preserving Sampling, CPS）方法，旨在消除这些噪声伪影。", "result": "所提出的CPS方法成功消除了生成的图像中的噪声伪影。这导致了更准确的奖励建模，并最终使得基于强化学习的优化器（如Flow-GRPO和Dance-GRPO）能够实现更快、更稳定的收敛。", "conclusion": "CPS方法有效解决了SDE采样在Flow Matching中引入的噪声问题，从而显著提升了强化学习在图像生成任务中的性能，实现了更稳定和快速的训练收敛。"}}
{"id": "2509.06040", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06040", "abs": "https://arxiv.org/abs/2509.06040", "authors": ["Yuming Li", "Yikai Wang", "Yuying Zhu", "Zhongyu Zhao", "Ming Lu", "Qi She", "Shanghang Zhang"], "title": "BranchGRPO: Stable and Efficient GRPO with Structured Branching in Diffusion Models", "comment": "12 pages, 6 figures", "summary": "Recent advancements in aligning image and video generative models via GRPO\nhave achieved remarkable gains in enhancing human preference alignment.\nHowever, these methods still face high computational costs from on-policy\nrollouts and excessive SDE sampling steps, as well as training instability due\nto sparse rewards. In this paper, we propose BranchGRPO, a novel method that\nintroduces a branch sampling policy updating the SDE sampling process. By\nsharing computation across common prefixes and pruning low-reward paths and\nredundant depths, BranchGRPO substantially lowers the per-update compute cost\nwhile maintaining or improving exploration diversity. This work makes three\nmain contributions: (1) a branch sampling scheme that reduces rollout and\ntraining cost; (2) a tree-based advantage estimator incorporating dense\nprocess-level rewards; and (3) pruning strategies exploiting path and depth\nredundancy to accelerate convergence and boost performance. Experiments on\nimage and video preference alignment show that BranchGRPO improves alignment\nscores by 16% over strong baselines, while cutting training time by 50%.", "AI": {"tldr": "BranchGRPO通过引入分支采样策略和树状优势估计器，显著降低了图像和视频生成模型对齐的计算成本和训练时间，同时提高了对齐分数。", "motivation": "现有的基于GRPO的图像和视频生成模型对齐方法面临高昂的计算成本（策略内采样、SDE采样步骤过多）和由于稀疏奖励导致的训练不稳定问题。", "method": "本文提出了BranchGRPO，一种新颖的方法，通过引入分支采样策略来更新SDE采样过程。它通过共享共同前缀的计算、修剪低奖励路径和冗余深度来大幅降低每次更新的计算成本。此外，它还采用了一个结合密集过程级奖励的树状优势估计器，以及利用路径和深度冗余的剪枝策略。", "result": "在图像和视频偏好对齐实验中，BranchGRPO将对齐分数比现有强大基线提高了16%，同时将训练时间缩短了50%。", "conclusion": "BranchGRPO通过其分支采样方案、树状优势估计器和剪枝策略，有效解决了现有GRPO方法的计算成本高和训练不稳定性问题，实现了更好的对齐性能和更快的收敛速度。"}}
{"id": "2509.06870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06870", "abs": "https://arxiv.org/abs/2509.06870", "authors": ["Wenting Zhao", "Pranjal Aggarwal", "Swarnadeep Saha", "Asli Celikyilmaz", "Jason Weston", "Ilia Kulikov"], "title": "The Majority is not always right: RL training for solution aggregation", "comment": null, "summary": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.", "AI": {"tldr": "本文提出了一种名为 AggLM 的方法，通过强化学习训练一个聚合器模型，将多个候选解决方案审查、协调并综合成最终正确答案，以提升大型语言模型在推理任务上的表现。", "motivation": "现有的大型语言模型（LLMs）在推理任务上通过生成多个独立解决方案并进行选择或聚合来提升性能，但多数方法（如简单多数投票或奖励模型排序）效果有限。", "method": "研究者将聚合视为一种显式的推理技能进行学习。他们训练了一个聚合器模型（AggLM），利用可验证的奖励进行强化学习，以审查、协调和综合候选解决方案。关键在于平衡简单和困难的训练样本，使模型既能恢复少数但正确的答案，也能处理简单的多数正确答案。", "result": "实验结果表明，AggLM 在多个基准测试中优于强大的基于规则和奖励模型的基线方法。此外，它能有效地泛化到来自不同模型的解决方案（包括比训练数据中更强的模型），同时比大量解决方案的多数投票法所需令牌少得多。", "conclusion": "通过强化学习将聚合作为一种显式推理技能进行训练，并精心平衡训练样本，可以显著提升大型语言模型在推理任务上的性能，且具有良好的泛化能力和更高的效率。"}}
{"id": "2509.05953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05953", "abs": "https://arxiv.org/abs/2509.05953", "authors": ["Jeonghyun Noh", "Wangsu Jeon", "Jinsun Park"], "title": "Dual Interaction Network with Cross-Image Attention for Medical Image Segmentation", "comment": "16pages", "summary": "Medical image segmentation is a crucial method for assisting professionals in\ndiagnosing various diseases through medical imaging. However, various factors\nsuch as noise, blurriness, and low contrast often hinder the accurate diagnosis\nof diseases. While numerous image enhancement techniques can mitigate these\nissues, they may also alter crucial information needed for accurate diagnosis\nin the original image. Conventional image fusion strategies, such as feature\nconcatenation can address this challenge. However, they struggle to fully\nleverage the advantages of both original and enhanced images while suppressing\nthe side effects of the enhancements. To overcome the problem, we propose a\ndual interactive fusion module (DIFM) that effectively exploits mutual\ncomplementary information from the original and enhanced images. DIFM employs\ncross-attention bidirectionally to simultaneously attend to corresponding\nspatial information across different images, subsequently refining the\ncomplementary features via global spatial attention. This interaction leverages\nlow- to high-level features implicitly associated with diverse structural\nattributes like edges, blobs, and object shapes, resulting in enhanced features\nthat embody important spatial characteristics. In addition, we introduce a\nmulti-scale boundary loss based on gradient extraction to improve segmentation\naccuracy at object boundaries. Experimental results on the ACDC and Synapse\ndatasets demonstrate the superiority of the proposed method quantitatively and\nqualitatively. Code available at: https://github.com/JJeong-Gari/DIN", "AI": {"tldr": "本文提出了一种双向交互融合模块（DIFM）和多尺度边界损失，用于有效融合原始和增强医学图像，以提高图像分割精度，尤其是在边界区域。", "motivation": "医学图像分割面临噪声、模糊和低对比度等挑战。虽然图像增强技术可以缓解这些问题，但也可能改变原始图像中的关键诊断信息。传统的图像融合策略（如特征拼接）难以充分利用原始和增强图像的优势，并抑制增强的副作用。", "method": "本文提出了一种双向交互融合模块（DIFM），通过双向交叉注意力机制同步关注不同图像中的空间信息，并通过全局空间注意力细化互补特征，从而有效利用原始和增强图像的互补信息。DIFM 利用从低级到高级的特征（如边缘、斑点、对象形状）。此外，还引入了一种基于梯度提取的多尺度边界损失，以提高对象边界的分割精度。", "result": "在ACDC和Synapse数据集上的实验结果表明，所提出的方法在定量和定性上都优于现有方法。", "conclusion": "所提出的双向交互融合模块和多尺度边界损失能够有效融合原始和增强医学图像，充分利用互补信息并抑制增强的副作用，从而显著提高了医学图像分割的准确性，尤其是在对象边界处。"}}
{"id": "2509.06122", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06122", "abs": "https://arxiv.org/abs/2509.06122", "authors": ["Tang Sui", "Songxi Yang", "Qunying Huang"], "title": "SpecSwin3D: Generating Hyperspectral Imagery from Multispectral Data via Transformer Networks", "comment": null, "summary": "Multispectral and hyperspectral imagery are widely used in agriculture,\nenvironmental monitoring, and urban planning due to their complementary spatial\nand spectral characteristics. A fundamental trade-off persists: multispectral\nimagery offers high spatial but limited spectral resolution, while\nhyperspectral imagery provides rich spectra at lower spatial resolution. Prior\nhyperspectral generation approaches (e.g., pan-sharpening variants, matrix\nfactorization, CNNs) often struggle to jointly preserve spatial detail and\nspectral fidelity. In response, we propose SpecSwin3D, a transformer-based\nmodel that generates hyperspectral imagery from multispectral inputs while\npreserving both spatial and spectral quality. Specifically, SpecSwin3D takes\nfive multispectral bands as input and reconstructs 224 hyperspectral bands at\nthe same spatial resolution. In addition, we observe that reconstruction errors\ngrow for hyperspectral bands spectrally distant from the input bands. To\naddress this, we introduce a cascade training strategy that progressively\nexpands the spectral range to stabilize learning and improve fidelity.\nMoreover, we design an optimized band sequence that strategically repeats and\norders the five selected multispectral bands to better capture pairwise\nrelations within a 3D shifted-window transformer framework. Quantitatively, our\nmodel achieves a PSNR of 35.82 dB, SAM of 2.40{\\deg}, and SSIM of 0.96,\noutperforming the baseline MHF-Net by +5.6 dB in PSNR and reducing ERGAS by\nmore than half. Beyond reconstruction, we further demonstrate the practical\nvalue of SpecSwin3D on two downstream tasks, including land use classification\nand burnt area segmentation.", "AI": {"tldr": "该研究提出了一种名为SpecSwin3D的基于Transformer的模型，能够从多光谱图像生成高空间和光谱质量的高光谱图像，并优于现有方法，在下游任务中也表现出色。", "motivation": "多光谱和高光谱图像在空间和光谱分辨率上存在固有的权衡（多光谱空间分辨率高但光谱有限，高光谱光谱丰富但空间分辨率低）。现有的高光谱生成方法（如全色锐化、矩阵分解、CNNs）难以同时保持空间细节和光谱保真度。", "method": "该研究提出了SpecSwin3D，一个基于Transformer的模型，以五个多光谱波段为输入，重建224个相同空间分辨率的高光谱波段。为解决重建误差随波段光谱距离增加的问题，引入了级联训练策略来逐步扩展光谱范围。此外，设计了一种优化的波段序列，战略性地重复和排列五个选定的多光谱波段，以更好地捕获3D shifted-window Transformer框架内的成对关系。", "result": "SpecSwin3D模型在定量评估中表现出色，实现了35.82 dB的PSNR、2.40°的SAM和0.96的SSIM，与基线MHF-Net相比，PSNR提高了+5.6 dB，ERGAS减少了一半以上。除了重建，该模型在土地利用分类和烧毁区域分割这两个下游任务中也展现了实用价值。", "conclusion": "SpecSwin3D模型有效解决了多光谱到高光谱图像生成中空间细节和光谱保真度难以兼顾的问题。通过创新的Transformer架构、级联训练策略和优化的波段序列，显著提升了高光谱图像的重建质量，并在实际应用中展现了其潜力。"}}
{"id": "2509.06883", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.06883", "abs": "https://arxiv.org/abs/2509.06883", "authors": ["Joe Wilder", "Nikhil Kadapala", "Benji Xu", "Mohammed Alsaadi", "Aiden Parsons", "Mitchell Rogers", "Palash Agarwal", "Adam Hassick", "Laura Dietz"], "title": "UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction", "comment": "16 pages,3 tables, CLEF 2025 Working Notes, 9-12 September 2025,\n  Madrid, Spain", "summary": "We participate in CheckThat! Task 2 English and explore various methods of\nprompting and in-context learning, including few-shot prompting and fine-tuning\nwith different LLM families, with the goal of extracting check-worthy claims\nfrom social media passages. Our best METEOR score is achieved by fine-tuning a\nFLAN-T5 model. However, we observe that higher-quality claims can sometimes be\nextracted using other methods, even when their METEOR scores are lower.", "AI": {"tldr": "该研究探索了多种大语言模型（LLM）提示和微调方法，旨在从社交媒体文本中提取值得核查的声明，发现微调FLAN-T5模型在METEOR分数上表现最佳，但其他方法有时能提取出更高质量的声明。", "motivation": "参与CheckThat! 任务2英语，目标是从社交媒体段落中提取值得核查的声明。", "method": "采用了多种提示方法（包括少样本提示和上下文学习）和不同LLM家族的微调方法。", "result": "微调FLAN-T5模型取得了最佳的METEOR分数。然而，观察到即使METEOR分数较低，其他方法有时也能提取出更高质量的声明。", "conclusion": "微调FLAN-T5在自动评估指标（如METEOR）上表现出色，但声明的实际质量评估可能需要更全面的考量，而不仅仅是单一指标。"}}
{"id": "2509.05954", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05954", "abs": "https://arxiv.org/abs/2509.05954", "authors": ["Weichao Wang", "Wendong Mao", "Zhongfeng Wang"], "title": "StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud", "comment": null, "summary": "The deployment of high-accuracy 3D object detection models from point cloud\nremains a significant challenge due to their substantial computational and\nmemory requirements. To address this, we introduce StripDet, a novel\nlightweight framework designed for on-device efficiency. First, we propose the\nnovel Strip Attention Block (SAB), a highly efficient module designed to\ncapture long-range spatial dependencies. By decomposing standard 2D\nconvolutions into asymmetric strip convolutions, SAB efficiently extracts\ndirectional features while reducing computational complexity from quadratic to\nlinear. Second, we design a hardware-friendly hierarchical backbone that\nintegrates SAB with depthwise separable convolutions and a simple multiscale\nfusion strategy, achieving end-to-end efficiency. Extensive experiments on the\nKITTI dataset validate StripDet's superiority. With only 0.65M parameters, our\nmodel achieves a 79.97% mAP for car detection, surpassing the baseline\nPointPillars with a 7x parameter reduction. Furthermore, StripDet outperforms\nrecent lightweight and knowledge distillation-based methods, achieving a\nsuperior accuracy-efficiency trade-off while establishing itself as a practical\nsolution for real-world 3D detection on edge devices.", "AI": {"tldr": "StripDet是一个轻量级3D目标检测框架，通过引入条带注意力模块和硬件友好型骨干网络，显著降低了计算和内存需求，实现了在边缘设备上高效准确的3D检测。", "motivation": "高精度3D点云目标检测模型通常需要大量的计算和内存资源，这严重阻碍了它们在边缘设备上的部署。", "method": "该研究提出了StripDet框架：1. 引入了条带注意力块（SAB），通过将标准2D卷积分解为非对称条带卷积，以线性复杂度高效捕获长距离空间依赖。2. 设计了一个硬件友好的分层骨干网络，将SAB与深度可分离卷积和简单的多尺度融合策略相结合，实现了端到端的高效性。", "result": "在KITTI数据集上，StripDet仅用0.65M参数就实现了79.97%的汽车检测mAP，参数量比PointPillars基线减少了7倍，同时性能更优。它在精度-效率权衡方面超越了现有的轻量级和基于知识蒸馏的方法。", "conclusion": "StripDet为边缘设备上的实时3D检测提供了一个实用的解决方案，在实现卓越准确性的同时，保持了极高的效率。"}}
{"id": "2509.06165", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06165", "abs": "https://arxiv.org/abs/2509.06165", "authors": ["Huy Le", "Nhat Chung", "Tung Kieu", "Jingkang Yang", "Ngan Le"], "title": "UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning", "comment": "11 pages, 7 figures", "summary": "Video Scene Graph Generation (VidSGG) aims to represent dynamic visual\ncontent by detecting objects and modeling their temporal interactions as\nstructured graphs. Prior studies typically target either coarse-grained\nbox-level or fine-grained panoptic pixel-level VidSGG, often requiring\ntask-specific architectures and multi-stage training pipelines. In this paper,\nwe present UNO (UNified Object-centric VidSGG), a single-stage, unified\nframework that jointly addresses both tasks within an end-to-end architecture.\nUNO is designed to minimize task-specific modifications and maximize parameter\nsharing, enabling generalization across different levels of visual granularity.\nThe core of UNO is an extended slot attention mechanism that decomposes visual\nfeatures into object and relation slots. To ensure robust temporal modeling, we\nintroduce object temporal consistency learning, which enforces consistent\nobject representations across frames without relying on explicit tracking\nmodules. Additionally, a dynamic triplet prediction module links relation slots\nto corresponding object pairs, capturing evolving interactions over time. We\nevaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Results\ndemonstrate that UNO not only achieves competitive performance across both\ntasks but also offers improved efficiency through a unified, object-centric\ndesign.", "AI": {"tldr": "UNO是一个统一的单阶段框架，通过扩展槽注意力、对象时间一致性学习和动态三元组预测，同时解决了粗粒度框级和细粒度像素级视频场景图生成（VidSGG）任务，实现了竞争性性能和更高的效率。", "motivation": "先前的视频场景图生成（VidSGG）研究通常针对粗粒度的框级或细粒度的像素级任务，并且通常需要特定任务的架构和多阶段训练流程。研究动机是开发一个统一的框架，能够同时处理这两种粒度，减少任务特定修改并最大化参数共享。", "method": "本文提出了UNO（UNified Object-centric VidSGG），一个单阶段、端到端的统一框架。其核心是一个扩展的槽注意力机制，将视觉特征分解为对象槽和关系槽。为确保鲁棒的时间建模，引入了对象时间一致性学习，无需显式跟踪模块即可强制跨帧的对象表示一致性。此外，一个动态三元组预测模块将关系槽链接到相应的对象对，以捕捉随时间演变的关系。", "result": "UNO在标准的框级和像素级VidSGG基准测试中均取得了竞争性性能。同时，通过其统一的、以对象为中心的设计，UNO还提高了效率。", "conclusion": "UNO成功地在一个统一的、单阶段、以对象为中心的架构中解决了框级和像素级VidSGG任务，不仅达到了有竞争力的性能，而且在效率上也有所提升，展示了其在不同视觉粒度上的泛化能力。"}}
{"id": "2509.06888", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06888", "abs": "https://arxiv.org/abs/2509.06888", "authors": ["Marc Marone", "Orion Weller", "William Fleshman", "Eugene Yang", "Dawn Lawrie", "Benjamin Van Durme"], "title": "mmBERT: A Modern Multilingual Encoder with Annealed Language Learning", "comment": null, "summary": "Encoder-only languages models are frequently used for a variety of standard\nmachine learning tasks, including classification and retrieval. However, there\nhas been a lack of recent research for encoder models, especially with respect\nto multilingual models. We introduce mmBERT, an encoder-only language model\npretrained on 3T tokens of multilingual text in over 1800 languages. To build\nmmBERT we introduce several novel elements, including an inverse mask ratio\nschedule and an inverse temperature sampling ratio. We add over 1700\nlow-resource languages to the data mix only during the decay phase, showing\nthat it boosts performance dramatically and maximizes the gains from the\nrelatively small amount of training data. Despite only including these\nlow-resource languages in the short decay phase we achieve similar\nclassification performance to models like OpenAI's o3 and Google's Gemini 2.5\nPro. Overall, we show that mmBERT significantly outperforms the previous\ngeneration of models on classification and retrieval tasks -- on both high and\nlow-resource languages.", "AI": {"tldr": "本文介绍了mmBERT，一个在超过1800种语言的3T多语言文本上预训练的编码器模型。它通过新颖的训练策略（如逆掩码比率和逆温度采样比率，并在衰减阶段加入低资源语言）显著提升了分类和检索任务的性能，超越了前代模型，并在低资源语言上表现出色。", "motivation": "现有研究中，编码器模型，特别是多语言编码器模型，缺乏最新进展和关注。", "method": "引入了mmBERT，一个在3T多语言文本（涵盖1800多种语言）上预训练的编码器模型。采用了创新的训练元素，包括逆掩码比率调度和逆温度采样比率。在训练的衰减阶段才加入超过1700种低资源语言，以最大化其性能增益。", "result": "在衰减阶段加入低资源语言显著提升了模型性能。mmBERT在分类任务上达到了与OpenAI的o3和Google的Gemini 2.5 Pro等模型相似的性能。在分类和检索任务上，mmBERT显著优于前代模型，无论是在高资源还是低资源语言上。", "conclusion": "mmBERT是一个高效的多语言编码器模型，通过创新的训练策略，在大量语言（包括低资源语言）上取得了卓越的分类和检索性能，超越了现有模型。"}}
{"id": "2509.05963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05963", "abs": "https://arxiv.org/abs/2509.05963", "authors": ["Rafal Karp", "Dawid Gruszka", "Tomasz Trzcinski"], "title": "Neural Bloom: A Deep Learning Approach to Real-Time Lighting", "comment": null, "summary": "We propose a novel method to generate bloom lighting effect in real time\nusing neural networks. Our solution generate brightness mask from given 3D\nscene view up to 30% faster than state-of-the-art methods. The existing\ntraditional techniques rely on multiple blur appliances and texture sampling,\nalso very often have existing conditional branching in its implementation.\nThese operations occupy big portion of the execution time. We solve this\nproblem by proposing two neural network-based bloom lighting methods, Neural\nBloom Lighting (NBL) and Fast Neural Bloom Lighting (FastNBL), focusing on\ntheir quality and performance. Both methods were tested on a variety of 3D\nscenes, with evaluations conducted on brightness mask accuracy and inference\nspeed. The main contribution of this work is that both methods produce\nhigh-quality bloom effects while outperforming the standard state-of-the-art\nbloom implementation, with FastNBL being faster by 28% and NBL faster by 12%.\nThese findings highlight that we can achieve realistic bloom lighting phenomena\nfaster, moving us towards more realism in real-time environments in the future.\nThis improvement saves computational resources, which is a major bottleneck in\nreal-time rendering. Furthermore, it is crucial for sustaining immersion and\nensuring smooth experiences in high FPS environments, while maintaining\nhigh-quality realism.", "AI": {"tldr": "本文提出两种基于神经网络的实时泛光（bloom）渲染方法（NBL和FastNBL），它们在保持高质量的同时，比现有技术分别快12%和28%，有效提升了实时渲染效率。", "motivation": "现有传统泛光渲染技术依赖多重模糊、纹理采样和条件分支，这些操作占据大量执行时间，成为实时渲染的瓶颈，影响了沉浸感和高帧率环境下的流畅体验。", "method": "本文提出两种基于神经网络的泛光渲染方法：Neural Bloom Lighting (NBL) 和 Fast Neural Bloom Lighting (FastNBL)。这些方法通过神经网络生成亮度掩码，以规避传统方法的计算开销，并在多种3D场景中测试其质量和推理速度。", "result": "NBL和FastNBL均能生成高质量的泛光效果。与现有最先进的泛光实现相比，FastNBL速度提升28%，NBL速度提升12%，尤其在亮度掩码生成方面，速度提升高达30%。", "conclusion": "研究表明，通过神经网络可以更快地实现逼真的泛光效果，从而节省计算资源，解决实时渲染中的主要瓶颈，并有助于在未来实时环境中实现更高的真实感，同时保持高帧率和沉浸式体验。"}}
{"id": "2509.06336", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.06336", "abs": "https://arxiv.org/abs/2509.06336", "authors": ["Jeongmin Yu", "Susang Kim", "Kisu Lee", "Taekyoung Kwon", "Won-Yong Shin", "Ha Young Kim"], "title": "Multi View Slot Attention Using Paraphrased Texts For Face Anti-Spoofing", "comment": "Accepted by ICCV 2025", "summary": "Recent face anti-spoofing (FAS) methods have shown remarkable cross-domain\nperformance by employing vision-language models like CLIP. However, existing\nCLIP-based FAS models do not fully exploit CLIP's patch embedding tokens,\nfailing to detect critical spoofing clues. Moreover, these models rely on a\nsingle text prompt per class (e.g., 'live' or 'fake'), which limits\ngeneralization. To address these issues, we propose MVP-FAS, a novel framework\nincorporating two key modules: Multi-View Slot attention (MVS) and Multi-Text\nPatch Alignment (MTPA). Both modules utilize multiple paraphrased texts to\ngenerate generalized features and reduce dependence on domain-specific text.\nMVS extracts local detailed spatial features and global context from patch\nembeddings by leveraging diverse texts with multiple perspectives. MTPA aligns\npatches with multiple text representations to improve semantic robustness.\nExtensive experiments demonstrate that MVP-FAS achieves superior generalization\nperformance, outperforming previous state-of-the-art methods on cross-domain\ndatasets. Code: https://github.com/Elune001/MVP-FAS.", "AI": {"tldr": "MVP-FAS是一种新颖的活体检测（FAS）框架，通过多视角槽注意力（MVS）和多文本补丁对齐（MTPA）模块，充分利用CLIP的补丁嵌入和多重文本提示，显著提升了跨域泛化性能。", "motivation": "现有的基于CLIP的FAS模型未能充分利用CLIP的补丁嵌入来检测关键欺骗线索，并且依赖于每个类别单一的文本提示（如“活体”或“假体”），这限制了其泛化能力。", "method": "本文提出了MVP-FAS框架，包含两个核心模块：多视角槽注意力（MVS）和多文本补丁对齐（MTPA）。这两个模块都利用多个释义文本来生成泛化特征并减少对特定领域文本的依赖。MVS通过利用多视角多样文本从补丁嵌入中提取局部详细空间特征和全局上下文。MTPA将补丁与多个文本表示对齐，以提高语义鲁棒性。", "result": "广泛的实验表明，MVP-FAS在跨域数据集上取得了卓越的泛化性能，超越了以往最先进的方法。", "conclusion": "MVP-FAS通过更有效地利用CLIP的补丁嵌入和引入多文本提示，显著提高了活体检测的跨域泛化能力。"}}
{"id": "2509.06902", "categories": ["cs.CL", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06902", "abs": "https://arxiv.org/abs/2509.06902", "authors": ["Aivin V. Solatorio"], "title": "Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification", "comment": null, "summary": "Large Language Models (LLMs) as stochastic systems may generate numbers that\ndeviate from available data, a failure known as \\emph{numeric hallucination}.\nExisting safeguards -- retrieval-augmented generation, citations, and\nuncertainty estimation -- improve transparency but cannot guarantee fidelity:\nfabricated or misquoted values may still be displayed as if correct. We propose\n\\textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that\nenforces numeric fidelity through mechanical verification. Under PCN, numeric\nspans are emitted as \\emph{claim-bound tokens} tied to structured claims, and a\nverifier checks each token under a declared policy (e.g., exact equality,\nrounding, aliases, or tolerance with qualifiers). Crucially, PCN places\nverification in the \\emph{renderer}, not the model: only claim-checked numbers\nare marked as verified, and all others default to unverified. This separation\nprevents spoofing and guarantees fail-closed behavior. We formalize PCN and\nprove soundness, completeness under honest tokens, fail-closed behavior, and\nmonotonicity under policy refinement. PCN is lightweight and model-agnostic,\nintegrates seamlessly into existing applications, and can be extended with\ncryptographic commitments. By enforcing verification as a mandatory step before\ndisplay, PCN establishes a simple contract for numerically sensitive settings:\n\\emph{trust is earned only by proof}, while the absence of a mark communicates\nuncertainty.", "AI": {"tldr": "针对大型语言模型（LLM）的数字幻觉问题，本文提出了“携带证明的数字”（PCN）协议，通过在呈现层进行机械验证来强制实现数字保真度，确保只有经过验证的数字才被标记为可信。", "motivation": "LLM作为随机系统可能生成与现有数据不符的数字（数字幻觉）。现有的保障措施（如检索增强生成、引用、不确定性估计）虽提高了透明度，但无法保证数字的准确性，仍可能显示伪造或引用错误的数值。", "method": "PCN是一种呈现层协议。数字片段被作为“声明绑定令牌”发出，并与结构化声明关联。一个验证器（位于渲染器而非模型中）根据预设策略（如精确相等、四舍五入、别名、带限定符的容差）检查每个令牌。只有经过声明检查的数字才被标记为已验证，否则默认为未验证。这种分离防止了欺骗并保证了故障关闭行为。", "result": "本文对PCN进行了形式化，并证明了其健全性、在诚实令牌下的完备性、故障关闭行为以及在策略细化下的单调性。PCN轻量级且与模型无关，可无缝集成到现有应用中，并可通过加密承诺进行扩展。", "conclusion": "通过将验证作为显示前的强制步骤，PCN为对数字敏感的设置建立了一个简单的契约：信任必须通过证明来赢得，而缺乏标记则表示不确定性，从而有效解决了LLM的数字幻觉问题。"}}
{"id": "2509.05967", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05967", "abs": "https://arxiv.org/abs/2509.05967", "authors": ["Yiqin Zhang", "Meiling Chen", "Zhengjie Zhang"], "title": "Spatial-Aware Self-Supervision for Medical 3D Imaging with Multi-Granularity Observable Tasks", "comment": null, "summary": "The application of self-supervised techniques has become increasingly\nprevalent within medical visualization tasks, primarily due to its capacity to\nmitigate the data scarcity prevalent in the healthcare sector. The majority of\ncurrent works are influenced by designs originating in the generic 2D visual\ndomain, which lack the intuitive demonstration of the model's learning process\nregarding 3D spatial knowledge. Consequently, these methods often fall short in\nterms of medical interpretability. We propose a method consisting of three\nsub-tasks to capture the spatially relevant semantics in medical 3D imaging.\nTheir design adheres to observable principles to ensure interpretability, and\nminimize the performance loss caused thereby as much as possible. By leveraging\nthe enhanced semantic depth offered by the extra dimension in 3D imaging, this\napproach incorporates multi-granularity spatial relationship modeling to\nmaintain training stability. Experimental findings suggest that our approach is\ncapable of delivering performance that is on par with current methodologies,\nwhile facilitating an intuitive understanding of the self-supervised learning\nprocess.", "AI": {"tldr": "该研究提出了一种针对3D医学影像的自监督学习方法，通过设计三个子任务来捕捉空间语义，旨在提高模型的可解释性，同时保持与现有方法相当的性能。", "motivation": "自监督学习在医疗可视化中日益普及，以缓解数据稀缺问题。然而，当前多数方法受2D视觉领域启发，缺乏对3D空间知识学习过程的直观展示，导致医学可解释性不足。", "method": "提出了一种包含三个子任务的方法，用于捕捉医学3D影像中与空间相关的语义。其设计遵循可观察原则以确保可解释性，并尽可能减少由此造成的性能损失。通过利用3D影像额外维度提供的增强语义深度，该方法融入了多粒度空间关系建模以维持训练稳定性。", "result": "实验结果表明，该方法能够提供与当前主流方法相当的性能，同时促进对自监督学习过程的直观理解。", "conclusion": "该方法成功地在3D医学自监督学习中提升了模型的空间知识理解和可解释性，且未牺牲性能，为医疗影像分析提供了更具洞察力的工具。"}}
{"id": "2509.06367", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06367", "abs": "https://arxiv.org/abs/2509.06367", "authors": ["Aswini Kumar Patra", "Lingaraj Sahoo"], "title": "MRD-LiNet: A Novel Lightweight Hybrid CNN with Gradient-Guided Unlearning for Improved Drought Stress Identification", "comment": "11 pages, 6 Figures, 3 Tables", "summary": "Drought stress is a major threat to global crop productivity, making its\nearly and precise detection essential for sustainable agricultural management.\nTraditional approaches, though useful, are often time-consuming and\nlabor-intensive, which has motivated the adoption of deep learning methods. In\nrecent years, Convolutional Neural Network (CNN) and Vision Transformer\narchitectures have been widely explored for drought stress identification;\nhowever, these models generally rely on a large number of trainable parameters,\nrestricting their use in resource-limited and real-time agricultural settings.\nTo address this challenge, we propose a novel lightweight hybrid CNN framework\ninspired by ResNet, DenseNet, and MobileNet architectures. The framework\nachieves a remarkable 15-fold reduction in trainable parameters compared to\nconventional CNN and Vision Transformer models, while maintaining competitive\naccuracy. In addition, we introduce a machine unlearning mechanism based on a\ngradient norm-based influence function, which enables targeted removal of\nspecific training data influence, thereby improving model adaptability. The\nmethod was evaluated on an aerial image dataset of potato fields with\nexpert-annotated healthy and drought-stressed regions. Experimental results\nshow that our framework achieves high accuracy while substantially lowering\ncomputational costs. These findings highlight its potential as a practical,\nscalable, and adaptive solution for drought stress monitoring in precision\nagriculture, particularly under resource-constrained conditions.", "AI": {"tldr": "本文提出了一种轻量级混合CNN框架，结合了ResNet、DenseNet和MobileNet的优点，用于精确且低成本地检测作物干旱胁迫，并引入了基于梯度范数影响函数的机器遗忘机制以提高模型适应性。", "motivation": "传统干旱检测方法耗时费力，而现有深度学习模型（CNN、Vision Transformer）参数量过大，不适用于资源受限和实时农业环境，因此需要开发轻量级、高效且适应性强的解决方案。", "method": "研究者提出了一种受ResNet、DenseNet和MobileNet启发的轻量级混合CNN框架。此外，他们引入了一种基于梯度范数影响函数的机器遗忘机制，以实现对特定训练数据影响的定向移除。该方法在专家标注的马铃薯田航空图像数据集上进行了评估。", "result": "该框架与传统CNN和Vision Transformer模型相比，可训练参数减少了15倍，同时保持了具有竞争力的准确性。实验结果表明，该框架在显著降低计算成本的同时实现了高精度。", "conclusion": "该框架为精准农业中的干旱胁迫监测提供了一个实用、可扩展和适应性强的解决方案，尤其适用于资源受限的条件。"}}
{"id": "2509.06948", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06948", "abs": "https://arxiv.org/abs/2509.06948", "authors": ["Liang Chen", "Xueting Han", "Li Shen", "Jing Bai", "Kam-Fai Wong"], "title": "Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning", "comment": null, "summary": "Reinforcement learning (RL) has proven effective in incentivizing the\nreasoning abilities of large language models (LLMs), but suffers from severe\nefficiency challenges due to its trial-and-error nature. While the common\npractice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this\ndecoupled two-stage approach limits interaction between SFT and RL, thereby\nconstraining overall effectiveness. This study introduces a novel method for\nlearning reasoning models that employs bilevel optimization to facilitate\nbetter cooperation between these training paradigms. By conditioning the SFT\nobjective on the optimal RL policy, our approach enables SFT to meta-learn how\nto guide RL's optimization process. During training, the lower level performs\nRL updates while simultaneously receiving SFT supervision, and the upper level\nexplicitly maximizes the cooperative gain-the performance advantage of joint\nSFT-RL training over RL alone. Empirical evaluations on five reasoning\nbenchmarks demonstrate that our method consistently outperforms baselines and\nachieves a better balance between effectiveness and efficiency.", "AI": {"tldr": "针对大型语言模型（LLM）推理能力的强化学习（RL）存在效率问题，现有SFT与RL的两阶段方法交互有限。本研究提出一种双层优化方法，通过让SFT目标依赖于最优RL策略，实现SFT对RL的元学习指导，从而提升了LLM推理的有效性和效率。", "motivation": "强化学习在激励LLM推理能力方面有效，但其试错性质导致效率低下。常见的监督微调（SFT）作为RL热身阶段的两阶段方法是解耦的，限制了SFT和RL之间的交互，从而制约了整体效果。", "method": "本研究提出了一种新颖的双层优化方法，以促进SFT和RL训练范式之间的更好协作。通过将SFT目标条件化于最优RL策略，使SFT能够元学习如何指导RL的优化过程。训练期间，下层执行RL更新并同时接收SFT监督，上层则明确最大化合作收益（联合SFT-RL训练相对于单独RL的性能优势）。", "result": "在五个推理基准上的实证评估表明，该方法始终优于基线，并在有效性和效率之间实现了更好的平衡。", "conclusion": "所提出的双层优化方法通过更好地整合SFT和RL，有效解决了LLM推理训练中的效率和效果问题，实现了卓越的性能和效率平衡。"}}
{"id": "2509.05970", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05970", "abs": "https://arxiv.org/abs/2509.05970", "authors": ["Ye Wang", "Zili Yi", "Yibo Zhang", "Peng Zheng", "Xuping Xie", "Jiang Lin", "Yilin Wang", "Rui Ma"], "title": "OmniStyle2: Scalable and High Quality Artistic Style Transfer Data Generation via Destylization", "comment": "Project Page: https://wangyephd.github.io/projects/omnistyle2.html", "summary": "OmniStyle2 introduces a novel approach to artistic style transfer by\nreframing it as a data problem. Our key insight is destylization, reversing\nstyle transfer by removing stylistic elements from artworks to recover natural,\nstyle-free counterparts. This yields DST-100K, a large-scale dataset that\nprovides authentic supervision signals by aligning real artistic styles with\ntheir underlying content. To build DST-100K, we develop (1) DST, a text-guided\ndestylization model that reconstructs stylefree content, and (2) DST-Filter, a\nmulti-stage evaluation model that employs Chain-of-Thought reasoning to\nautomatically discard low-quality pairs while ensuring content fidelity and\nstyle accuracy. Leveraging DST-100K, we train OmniStyle2, a simple feed-forward\nmodel based on FLUX.1-dev. Despite its simplicity, OmniStyle2 consistently\nsurpasses state-of-the-art methods across both qualitative and quantitative\nbenchmarks. Our results demonstrate that scalable data generation via\ndestylization provides a reliable supervision paradigm, overcoming the\nfundamental challenge posed by the lack of ground-truth data in artistic style\ntransfer.", "AI": {"tldr": "OmniStyle2通过将艺术风格迁移重新定义为数据问题，并引入“去风格化”方法，创建了大规模数据集DST-100K。利用此数据集，OmniStyle2（一个基于FLUX.1-dev的简单前馈模型）在定性和定量基准上均超越了现有最先进的方法。", "motivation": "艺术风格迁移领域面临的核心挑战是缺乏真实（ground-truth）的监督数据。传统的风格迁移方法难以获得艺术作品的原始、无风格内容，这限制了模型的训练和性能。", "method": "该研究提出了一种“去风格化”（destylization）方法，旨在从艺术作品中移除风格元素，恢复其自然、无风格的内容。基于此，构建了DST-100K数据集，该数据集通过以下两步生成：1) DST，一个文本引导的去风格化模型，用于重建无风格内容；2) DST-Filter，一个多阶段评估模型，利用思维链（Chain-of-Thought）推理自动筛选低质量配对，确保内容保真度和风格准确性。最后，利用DST-100K数据集训练了OmniStyle2，一个基于FLUX.1-dev的简单前馈模型。", "result": "尽管模型结构简单，OmniStyle2在定性和定量基准测试中均持续超越了现有最先进的艺术风格迁移方法。这证明了通过去风格化实现可扩展数据生成是一种可靠的监督范式。", "conclusion": "通过去风格化生成可扩展数据提供了一种可靠的监督范式，成功克服了艺术风格迁移中缺乏真实数据这一根本挑战，显著提升了风格迁移的性能。"}}
{"id": "2509.06415", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06415", "abs": "https://arxiv.org/abs/2509.06415", "authors": ["Jaemin Son", "Sujin Choi", "Inyong Yun"], "title": "Index-Preserving Lightweight Token Pruning for Efficient Document Understanding in Vision-Language Models", "comment": "Submitted to ICASSP 2026", "summary": "Recent progress in vision-language models (VLMs) has led to impressive\nresults in document understanding tasks, but their high computational demands\nremain a challenge. To mitigate the compute burdens, we propose a lightweight\ntoken pruning framework that filters out non-informative background regions\nfrom document images prior to VLM processing. A binary patch-level classifier\nremoves non-text areas, and a max-pooling refinement step recovers fragmented\ntext regions to enhance spatial coherence. Experiments on real-world document\ndatasets demonstrate that our approach substantially lowers computational\ncosts, while maintaining comparable accuracy.", "AI": {"tldr": "本文提出了一种轻量级token剪枝框架，通过在VLM处理前过滤文档图像中的非信息性背景区域，显著降低了文档理解任务的计算成本，同时保持了相似的准确性。", "motivation": "尽管视觉语言模型（VLMs）在文档理解任务中取得了显著进展，但其高计算需求仍然是一个挑战。", "method": "该方法包括一个轻量级token剪枝框架：首先，一个二进制补丁级分类器用于移除非文本区域；其次，一个最大池化细化步骤用于恢复碎片化的文本区域，以增强空间连贯性。", "result": "在真实世界文档数据集上的实验表明，该方法显著降低了计算成本，同时保持了可比的准确性。", "conclusion": "所提出的轻量级token剪枝框架能够有效缓解VLM在文档理解任务中的计算负担，且不牺牲性能。"}}
{"id": "2509.06949", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06949", "abs": "https://arxiv.org/abs/2509.06949", "authors": ["Yinjie Wang", "Ling Yang", "Bowen Li", "Ye Tian", "Ke Shen", "Mengdi Wang"], "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models", "comment": "Code and Models: https://github.com/Gen-Verse/dLLM-RL", "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL", "AI": {"tldr": "本文提出TraceRL，一个轨迹感知强化学习框架，用于扩散语言模型（DLM）的后训练。该框架通过整合偏好推理轨迹和扩散值模型，显著提升了DLM在复杂数学和编程任务上的推理性能，并导出了SOTA的TraDo系列DLM，其性能超越了同等规模的自回归模型。同时，还发布了一个全面的开源框架。", "motivation": "研究旨在改进扩散语言模型（DLM）的后训练过程，通过引入偏好推理轨迹来提高其在复杂推理任务上的表现，解决其跨架构适用性问题，并最终促进DLM的可复现研究和实际应用。", "method": "本文提出TraceRL框架，它是一种轨迹感知强化学习方法，将偏好的推理轨迹融入DLM的后训练中。该框架配备了一个基于扩散的值模型以增强训练稳定性，并可将特定块模型适应到更大的块以提高采样灵活性。此外，研究还通过课程学习开发了首个长CoT DLM。", "result": "1. 在复杂数学和编程任务上显著提高了推理性能。2. 导出了SOTA的TraDo扩散语言模型系列。3. TraDo-4B-Instruct在复杂数学推理任务上持续优于7B规模的自回归（AR）模型。4. TraDo-8B-Instruct在数学推理基准上，相对于Qwen2.5-7B-Instruct准确率提高6.1%，相对于Llama3.1-8B-Instruct提高51.3%。5. 导出了首个长CoT DLM，在MATH500上比Qwen2.5-7B-Instruct相对准确率提高18.1%。6. 发布了一个全面的开源框架，用于构建、训练和部署跨不同架构的扩散LLM，该框架集成了加速KV-cache技术和推理引擎，并包含多种监督微调和强化学习方法的实现。", "conclusion": "TraceRL是一个有效且通用的强化学习框架，能够显著提升扩散语言模型在复杂推理任务上的性能，并成功开发出超越现有自回归模型SOTA的TraDo系列DLM。该研究还通过开源框架促进了DLM领域的未来研究和实际应用。"}}
{"id": "2509.05992", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.05992", "abs": "https://arxiv.org/abs/2509.05992", "authors": ["Zekun Zhou", "Yanru Gong", "Liu Shi", "Qiegen Liu"], "title": "Multi-Strategy Guided Diffusion via Sparse Masking Temporal Reweighting Distribution Correction", "comment": null, "summary": "Diffusion models have demonstrated remarkable generative capabilities in\nimage processing tasks. We propose a Sparse condition Temporal Rewighted\nIntegrated Distribution Estimation guided diffusion model (STRIDE) for\nsparse-view CT reconstruction. Specifically, we design a joint training\nmechanism guided by sparse conditional probabilities to facilitate the model\neffective learning of missing projection view completion and global information\nmodeling. Based on systematic theoretical analysis, we propose a temporally\nvarying sparse condition reweighting guidance strategy to dynamically adjusts\nweights during the progressive denoising process from pure noise to the real\nimage, enabling the model to progressively perceive sparse-view information.\nThe linear regression is employed to correct distributional shifts between\nknown and generated data, mitigating inconsistencies arising during the\nguidance process. Furthermore, we construct a dual-network parallel\narchitecture to perform global correction and optimization across multiple\nsub-frequency components, thereby effectively improving the model capability in\nboth detail restoration and structural preservation, ultimately achieving\nhigh-quality image reconstruction. Experimental results on both public and real\ndatasets demonstrate that the proposed method achieves the best improvement of\n2.58 dB in PSNR, increase of 2.37\\% in SSIM, and reduction of 0.236 in MSE\ncompared to the best-performing baseline methods. The reconstructed images\nexhibit excellent generalization and robustness in terms of structural\nconsistency, detail restoration, and artifact suppression.", "AI": {"tldr": "本文提出了一种名为STRIDE的扩散模型，用于稀疏视角CT重建，通过联合训练、时变重加权、线性回归校正和双网络架构等创新策略，显著提升了重建图像的质量和鲁棒性。", "motivation": "扩散模型在图像生成方面表现出卓越能力，但稀疏视角CT重建仍面临挑战，需要有效处理缺失投影补全和全局信息建模，以实现高质量图像重建。", "method": "本文提出了STRIDE模型，具体方法包括：1) 设计了由稀疏条件概率引导的联合训练机制，以学习缺失投影补全和全局信息建模。2) 提出了一种时变稀疏条件重加权指导策略，在去噪过程中动态调整权重。3) 采用线性回归校正已知数据与生成数据之间的分布偏移。4) 构建了双网络并行架构，对多个子频率分量进行全局校正和优化，以改善细节恢复和结构保持。", "result": "实验结果表明，与现有最佳基线方法相比，STRIDE模型在PSNR上实现了2.58 dB的最佳提升，SSIM增加了2.37%，MSE减少了0.236。重建图像在结构一致性、细节恢复和伪影抑制方面表现出卓越的泛化性和鲁棒性。", "conclusion": "所提出的STRIDE模型在稀疏视角CT重建任务中取得了显著的性能提升，能够生成高质量的重建图像，并展现出优异的泛化性和鲁棒性，有效解决了稀疏视角CT重建中的关键挑战。"}}
{"id": "2509.06461", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06461", "abs": "https://arxiv.org/abs/2509.06461", "authors": ["Yuyao Ge", "Shenghua Liu", "Yiwei Wang", "Lingrui Mei", "Baolong Bi", "Xuanshan Zhou", "Jiayu Yao", "Jiafeng Guo", "Xueqi Cheng"], "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning", "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.", "AI": {"tldr": "本文提出了一种名为CARVE的免训练方法，通过像素级对比注意力来增强视觉语言模型（VLMs）在复杂视觉环境中的性能，实现了显著提升。", "motivation": "视觉语言模型（VLMs）在复杂视觉环境中性能下降，而现有增强方法需要额外训练、依赖外部工具或粒度较粗，且忽视了VLM内在的能力。", "method": "研究了VLM的注意力模式，发现视觉复杂性与注意力熵相关，且注意力从浅层全局扫描到深层聚焦收敛。理论上证明了通用查询和任务特定查询之间注意力图的对比能够将视觉信号分解为语义信号和视觉噪声。在此基础上，提出了CARVE（Contrastive Attention Refinement for Visual Enhancement），一种免训练方法，通过像素级注意力对比提取任务相关的视觉信号。", "result": "CARVE持续提升了性能，在开源模型上实现了高达75%的改进。", "conclusion": "该研究深入探讨了视觉复杂性与注意力机制之间的相互作用，并提供了一种通过对比注意力提高视觉推理效率的有效途径。"}}
{"id": "2509.06952", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.06952", "abs": "https://arxiv.org/abs/2509.06952", "authors": ["Linlu Qiu", "Cedegao E. Zhang", "Joshua B. Tenenbaum", "Yoon Kim", "Roger P. Levy"], "title": "On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts", "comment": "EMNLP 2025 (Main)", "summary": "Language use is shaped by pragmatics -- i.e., reasoning about communicative\ngoals and norms in context. As language models (LMs) are increasingly used as\nconversational agents, it becomes ever more important to understand their\npragmatic reasoning abilities. We propose an evaluation framework derived from\nWavelength, a popular communication game where a speaker and a listener\ncommunicate about a broad range of concepts in a granular manner. We study a\nrange of LMs on both language comprehension and language production using\ndirect and Chain-of-Thought (CoT) prompting, and further explore a Rational\nSpeech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM\ninference. We find that state-of-the-art LMs, but not smaller ones, achieve\nstrong performance on language comprehension, obtaining similar-to-human\naccuracy and exhibiting high correlations with human judgments even without CoT\nprompting or RSA. On language production, CoT can outperform direct prompting,\nand using RSA provides significant improvements over both approaches. Our study\nhelps identify the strengths and limitations in LMs' pragmatic reasoning\nabilities and demonstrates the potential for improving them with RSA, opening\nup future avenues for understanding conceptual representation, language\nunderstanding, and social reasoning in LMs and humans.", "AI": {"tldr": "本研究提出一个基于Wavelength游戏的评估框架，用于测试语言模型（LMs）的语用推理能力。结果显示，最先进的LMs在语言理解方面表现出色，但在语言生成方面，通过思维链（CoT）和理性言语行为（RSA）方法可以显著提升性能。", "motivation": "随着语言模型越来越多地被用作对话代理，理解它们的语用推理能力变得至关重要。语用学（即在语境中对沟通目标和规范的推理）塑造了语言使用。", "method": "研究采用了一个源自流行沟通游戏Wavelength的评估框架，该框架允许对广泛概念进行细粒度沟通。研究人员使用直接提示和思维链（CoT）提示，在语言理解和语言生成两方面评估了一系列语言模型。此外，他们还探索了将贝叶斯语用推理融入LM推理的理性言语行为（RSA）方法。", "result": "最先进的语言模型（而非小型模型）在语言理解方面表现出强大的性能，达到了与人类相似的准确性，并且即使没有CoT提示或RSA也与人类判断高度相关。在语言生成方面，CoT提示优于直接提示，而使用RSA则比这两种方法都提供了显著的改进。", "conclusion": "本研究有助于识别语言模型语用推理能力的优势和局限性，并证明了通过RSA提升这些能力的潜力。这为未来理解语言模型和人类在概念表征、语言理解和社会推理方面的研究开辟了新途径。"}}
{"id": "2509.06000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06000", "abs": "https://arxiv.org/abs/2509.06000", "authors": ["Jose Sosa", "Dan Pineau", "Arunkumar Rathinam", "Abdelrahman Shabayek", "Djamila Aouada"], "title": "Motion Aware ViT-based Framework for Monocular 6-DoF Spacecraft Pose Estimation", "comment": null, "summary": "Monocular 6-DoF pose estimation plays an important role in multiple\nspacecraft missions. Most existing pose estimation approaches rely on single\nimages with static keypoint localisation, failing to exploit valuable temporal\ninformation inherent to space operations. In this work, we adapt a deep\nlearning framework from human pose estimation to the spacecraft pose estimation\ndomain that integrates motion-aware heatmaps and optical flow to capture motion\ndynamics. Our approach combines image features from a Vision Transformer (ViT)\nencoder with motion cues from a pre-trained optical flow model to localise 2D\nkeypoints. Using the estimates, a Perspective-n-Point (PnP) solver recovers\n6-DoF poses from known 2D-3D correspondences. We train and evaluate our method\non the SPADES-RGB dataset and further assess its generalisation on real and\nsynthetic data from the SPARK-2024 dataset. Overall, our approach demonstrates\nimproved performance over single-image baselines in both 2D keypoint\nlocalisation and 6-DoF pose estimation. Furthermore, it shows promising\ngeneralisation capabilities when testing on different data distributions.", "AI": {"tldr": "本文提出了一种基于深度学习的单目6自由度航天器姿态估计算法，通过结合运动感知热图和光流，利用时间信息改进了2D关键点定位和6自由度姿态恢复，并在不同数据集上表现出优越的性能和泛化能力。", "motivation": "大多数现有的航天器姿态估计算法依赖于单张图像和静态关键点定位，未能利用太空操作中固有的宝贵时间信息，这限制了其在动态场景中的表现。", "method": "该方法将人类姿态估计的深度学习框架 адаптирован 到航天器姿态估计领域，集成了运动感知热图和光流来捕捉运动动态。它结合了来自Vision Transformer (ViT)编码器的图像特征和来自预训练光流模型的运动线索来定位2D关键点。随后，使用PnP（Perspective-n-Point）求解器从已知的2D-3D对应关系中恢复6自由度姿态。该方法在SPADES-RGB数据集上进行训练和评估，并在SPARK-2024数据集的真实和合成数据上评估了其泛化能力。", "result": "该方法在2D关键点定位和6自由度姿态估计方面均优于单图像基线方法。此外，在不同数据分布上进行测试时，它还展示了有前景的泛化能力。", "conclusion": "通过整合运动感知热图和光流等时间信息，本文提出的深度学习框架显著提升了单目6自由度航天器姿态估计的性能，并在关键点定位和姿态恢复上均取得了改进，同时展现出良好的跨数据分布泛化能力。"}}
{"id": "2509.06535", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06535", "abs": "https://arxiv.org/abs/2509.06535", "authors": ["Hua Chang Bakker", "Stan Fris", "Angela Madelon Bernardy", "Stan Deutekom"], "title": "On the Reproducibility of \"FairCLIP: Harnessing Fairness in Vision-Language Learning''", "comment": null, "summary": "We investigated the reproducibility of FairCLIP, proposed by Luo et al.\n(2024), for improving the group fairness of CLIP (Radford et al., 2021) by\nminimizing image-text similarity score disparities across sensitive groups\nusing the Sinkhorn distance. The experimental setup of Luo et al. (2024) was\nreproduced to primarily investigate the research findings for FairCLIP. The\nmodel description by Luo et al. (2024) was found to differ from the original\nimplementation. Therefore, a new implementation, A-FairCLIP, is introduced to\nexamine specific design choices. Furthermore, FairCLIP+ is proposed to extend\nthe FairCLIP objective to include multiple attributes. Additionally, the impact\nof the distance minimization on FairCLIP's fairness and performance was\nexplored. In alignment with the original authors, CLIP was found to be biased\ntowards certain demographics when applied to zero-shot glaucoma classification\nusing medical scans and clinical notes from the Harvard-FairVLMed dataset.\nHowever, the experimental results on two datasets do not support their claim\nthat FairCLIP improves the performance and fairness of CLIP. Although the\nregularization objective reduces Sinkhorn distances, both the official\nimplementation and the aligned implementation, A-FairCLIP, were not found to\nimprove performance nor fairness in zero-shot glaucoma classification.", "AI": {"tldr": "本文调查了FairCLIP的重现性及其在零样本青光眼分类中的表现，发现其未能如原论文所述提升CLIP的公平性和性能，尽管它能降低Sinkhorn距离。", "motivation": "Luo et al. (2024) 提出的FairCLIP旨在通过最小化敏感组间的图像-文本相似度分数差异来改善CLIP的群组公平性。本研究的动机是验证FairCLIP的重现性及其研究发现。", "method": "研究者首先重现了Luo et al. (2024) 的实验设置。由于发现原模型描述与实现不符，引入了新的实现A-FairCLIP以检查特定设计选择。此外，提出了FairCLIP+以扩展FairCLIP目标以包含多个属性。研究还探讨了距离最小化对FairCLIP公平性和性能的影响。", "result": "研究证实了CLIP在零样本青光眼分类中对某些人口统计学存在偏见。然而，在两个数据集上的实验结果不支持FairCLIP能提升CLIP性能和公平性的主张。尽管正则化目标确实降低了Sinkhorn距离，但无论是官方实现还是对齐实现A-FairCLIP，均未在零样本青光眼分类中观察到性能或公平性的改善。", "conclusion": "尽管FairCLIP的正则化目标能够减少Sinkhorn距离，但本研究未能重现其在零样本青光眼分类中改善CLIP性能和公平性的效果，无论是官方实现还是其修正版本A-FairCLIP均未能实现这一目标。"}}
{"id": "2509.06945", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06945", "abs": "https://arxiv.org/abs/2509.06945", "authors": ["Wenxuan Huang", "Shuang Chen", "Zheyong Xie", "Shaosheng Cao", "Shixiang Tang", "Yufan Shen", "Qingyu Yin", "Wenbo Hu", "Xiaoman Wang", "Yuntian Tang", "Junbo Qiao", "Yue Guo", "Yao Hu", "Zhenfei Yin", "Philip Torr", "Yu Cheng", "Wanli Ouyang", "Shaohui Lin"], "title": "Interleaving Reasoning for Better Text-to-Image Generation", "comment": null, "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .", "AI": {"tldr": "本文提出了一种名为交错推理生成（IRG）的新框架和训练方法（IRGL），通过在文本思考和图像合成之间交替进行，显著提升了文本到图像（T2I）生成在指令遵循和细节保留方面的能力。", "motivation": "尽管统一多模态模型在图像生成方面取得了显著进步，但在指令遵循和细节保留方面与GPT-4o等紧密耦合的系统仍有较大差距。受交错推理最新进展的启发，研究旨在探索这种推理能否进一步改善T2I生成。", "method": "研究引入了交错推理生成（IRG）框架，该框架在文本思考和图像合成之间交替进行：模型首先生成文本思考以指导初始图像，然后反思结果以在保持语义的同时，改进细节、视觉质量和美学。为了有效训练IRG，提出了交错推理生成学习（IRGL），旨在实现两个子目标：1) 强化初始思考和生成阶段以建立核心内容和基础质量；2) 实现高质量的文本反思并忠实地在后续图像中实现这些改进。为此，构建了IRGL-300K数据集，并采用两阶段训练：首先从一个统一的基础模型构建鲁棒的思考和反思能力，然后高效地在完整的思考-图像轨迹数据上微调IRG管道。", "result": "实验结果显示，该方法达到了最先进的性能（SoTA），在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN等基准测试上取得了5-10个百分点的绝对增益，并在视觉质量和精细保真度方面有显著提升。", "conclusion": "交错推理生成（IRG）框架通过在文本思考和图像合成之间交替进行，能够有效弥补现有统一多模态模型在指令遵循和细节保留上的不足，显著提升了文本到图像的生成质量和保真度。"}}
{"id": "2509.06010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06010", "abs": "https://arxiv.org/abs/2509.06010", "authors": ["Wanyin Cheng", "Zanxi Ruan"], "title": "BLaVe-CoT: Consistency-Aware Visual Question Answering for Blind and Low Vision Users", "comment": null, "summary": "Visual Question Answering (VQA) holds great potential for assisting Blind and\nLow Vision (BLV) users, yet real-world usage remains challenging. Due to visual\nimpairments, BLV users often take blurry or poorly framed photos and face\ndifficulty in articulating specific questions about what they cannot fully see.\nAs a result, their visual questions are frequently ambiguous, and different\nusers may interpret them in diverse ways. This leads to multiple valid answers,\neach grounded in different image regions-posing a mismatch with conventional\nVQA systems that assume a single answer and region. To bridge this gap, we\npresent BLaVe-CoT, a VQA framework designed to reason about answer consistency\nin the face of ambiguity. Our method proposes diverse candidate answers using a\nLoRA-tuned BLIP-2 model, then grounds each answer spatially using PolyFormer,\nand finally applies a chain-of-thought reasoning module to assess whether the\nanswers refer to the same or different regions. Evaluated on the\nVQA-AnswerTherapy benchmark, BLaVe-CoT outperforms previous methods and proves\nmore robust to the ambiguity and visual noise common in assistive settings.\nThis work highlights the need for VQA systems that can adapt to real human\nuncertainty and provide inclusive support for BLV users. To foster further\nresearch and accessibility applications, we have made the code publicly\navailable at https://github.com/Accecwan/BLaVe-CoT.", "AI": {"tldr": "针对盲人和低视力(BLV)用户在VQA中面临的模糊输入和多义性问题，本文提出了BLaVe-CoT框架。它通过生成多样化答案、空间定位和链式思维推理来处理答案一致性，并在基准测试中表现出更强的鲁棒性。", "motivation": "传统VQA系统假设单一答案和区域，无法适应BLV用户因视力障碍造成的模糊、低质量图像以及模糊问题。这导致BLV用户的视觉问题常有多重有效答案，且根植于不同图像区域，与现有系统不匹配。", "method": "BLaVe-CoT框架通过以下步骤处理歧义：1. 使用LoRA微调的BLIP-2模型生成多样化的候选答案。2. 使用PolyFormer对每个答案进行空间定位。3. 应用链式思维(Chain-of-Thought)推理模块评估答案是否指向相同或不同区域，从而判断答案一致性。", "result": "在VQA-AnswerTherapy基准测试中，BLaVe-CoT优于现有方法，并且在辅助环境中常见的歧义和视觉噪声面前表现出更高的鲁棒性。", "conclusion": "这项工作强调了VQA系统需要适应真实的人类不确定性，并为BLV用户提供包容性支持。作者已开源代码以促进进一步研究和无障碍应用。"}}
{"id": "2509.06625", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06625", "abs": "https://arxiv.org/abs/2509.06625", "authors": ["Aswini Kumar Patra"], "title": "Improved Classification of Nitrogen Stress Severity in Plants Under Combined Stress Conditions Using Spatio-Temporal Deep Learning Framework", "comment": "13 pages, 8 figures, 7 Tables", "summary": "Plants in their natural habitats endure an array of interacting stresses,\nboth biotic and abiotic, that rarely occur in isolation. Nutrient\nstress-particularly nitrogen deficiency-becomes even more critical when\ncompounded with drought and weed competition, making it increasingly difficult\nto distinguish and address its effects. Early detection of nitrogen stress is\ntherefore crucial for protecting plant health and implementing effective\nmanagement strategies. This study proposes a novel deep learning framework to\naccurately classify nitrogen stress severity in a combined stress environment.\nOur model uses a unique blend of four imaging modalities-RGB, multispectral,\nand two infrared wavelengths-to capture a wide range of physiological plant\nresponses from canopy images. These images, provided as time-series data,\ndocument plant health across three levels of nitrogen availability (low,\nmedium, and high) under varying water stress and weed pressures. The core of\nour approach is a spatio-temporal deep learning pipeline that merges a\nConvolutional Neural Network (CNN) for extracting spatial features from images\nwith a Long Short-Term Memory (LSTM) network to capture temporal dependencies.\nWe also devised and evaluated a spatial-only CNN pipeline for comparison. Our\nCNN-LSTM pipeline achieved an impressive accuracy of 98%, impressively\nsurpassing the spatial-only model's 80.45% and other previously reported\nmachine learning method's 76%. These results bring actionable insights based on\nthe power of our CNN-LSTM approach in effectively capturing the subtle and\ncomplex interactions between nitrogen deficiency, water stress, and weed\npressure. This robust platform offers a promising tool for the timely and\nproactive identification of nitrogen stress severity, enabling better crop\nmanagement and improved plant health.", "AI": {"tldr": "本研究提出了一种新颖的深度学习框架（CNN-LSTM），利用多模态时序图像数据，在高交互胁迫环境下（干旱、杂草）准确分类植物氮胁迫的严重程度，实现了98%的准确率，远超其他方法。", "motivation": "植物在自然环境中面临多种相互作用的生物和非生物胁迫，其中氮缺乏与干旱和杂草竞争复合时，其影响更难区分和解决。因此，早期检测氮胁迫对于保护植物健康和实施有效管理策略至关重要。", "method": "本研究提出了一种新颖的深度学习框架，用于在复合胁迫环境下准确分类氮胁迫严重程度。该模型融合了四种成像模态（RGB、多光谱和两种红外波长）来捕捉植物的广泛生理响应。这些图像作为时间序列数据，记录了在不同水分胁迫和杂草压力下，三种氮可用性水平（低、中、高）下的植物健康状况。核心方法是时空深度学习流程，结合卷积神经网络（CNN）提取图像空间特征和长短期记忆网络（LSTM）捕捉时间依赖性。研究还设计并评估了一个仅基于空间的CNN流程进行比较。", "result": "CNN-LSTM流程实现了98%的准确率，显著超越了仅基于空间模型的80.45%准确率以及其他先前报道的机器学习方法的76%准确率。", "conclusion": "CNN-LSTM方法能够有效捕捉氮缺乏、水分胁迫和杂草压力之间微妙而复杂的相互作用。这一鲁棒平台为及时主动识别氮胁迫严重程度提供了有前景的工具，从而实现更好的作物管理和改善植物健康。"}}
{"id": "2509.06011", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06011", "abs": "https://arxiv.org/abs/2509.06011", "authors": ["Zhenhai Weng", "Zhongliang Yu"], "title": "Cross-Modal Enhancement and Benchmark for UAV-based Open-Vocabulary Object Detection", "comment": null, "summary": "Open-Vocabulary Object Detection (OVD) has emerged as a pivotal technology\nfor applications involving Unmanned Aerial Vehicles (UAVs). However, the\nprevailing large-scale datasets for OVD pre-training are predominantly composed\nof ground-level, natural images. This creates a significant domain gap, causing\nmodels trained on them to exhibit a substantial drop in performance on UAV\nimagery. To address this limitation, we first propose a refined UAV-Label\nengine. Then we construct and introduce UAVDE-2M(contains over 2,000,000\ninstances and 1800 categories) and UAVCAP-15k(contains over 15,000 images).\nFurthermore, we propose a novel Cross-Attention Gated Enhancement Fusion (CAGE)\nmodule and integrate it into the YOLO-World-v2 architecture. Finally, extensive\nexperiments on the VisDrone and SIMD datasets verify the effectiveness of our\nproposed method for applications in UAV-based imagery and remote sensing.", "AI": {"tldr": "针对无人机开放词汇目标检测（OVD）中的领域鸿沟问题，本文提出了一种改进的UAV-Label引擎，构建了新的无人机数据集UAVDE-2M和UAVCAP-15k，并设计了跨注意力门控增强融合（CAGE）模块并集成到YOLO-World-v2中，有效提升了无人机图像的检测性能。", "motivation": "无人机开放词汇目标检测（OVD）技术至关重要，但现有的大规模OVD预训练数据集主要由地面自然图像组成，这与无人机图像存在显著的领域鸿沟，导致模型在无人机图像上的性能大幅下降。", "method": ["提出了一种改进的UAV-Label引擎。", "构建并引入了两个新的无人机数据集：UAVDE-2M（包含超过200万个实例和1800个类别）和UAVCAP-15k（包含超过15000张图像）。", "提出了一种新颖的跨注意力门控增强融合（CAGE）模块。", "将CAGE模块集成到YOLO-World-v2架构中。"], "result": "在VisDrone和SIMD数据集上进行了广泛的实验，验证了所提出方法在无人机图像和遥感应用中的有效性。", "conclusion": "通过改进标签引擎、构建特定数据集以及引入新的融合模块，本文有效解决了无人机开放词汇目标检测中的领域鸿沟问题，显著提升了模型在无人机图像和遥感应用中的性能。"}}
{"id": "2509.06690", "categories": ["cs.CV", "cs.AI", "cs.AR", "N/A", "I.2.9; I.2.10; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.06690", "abs": "https://arxiv.org/abs/2509.06690", "authors": ["Usman Haider", "Lukasz Szemet", "Daniel Kelly", "Vasileios Sergis", "Andrew C. Daly", "Karl Mason"], "title": "BioLite U-Net: Edge-Deployable Semantic Segmentation for In Situ Bioprinting Monitoring", "comment": "8 pages, 5 figures, conference-style submission (ICRA 2026). Includes\n  dataset description, BioLite U-Net architecture, benchmark results on edge\n  device (Raspberry Pi 4B)", "summary": "Bioprinting is a rapidly advancing field that offers a transformative\napproach to fabricating tissue and organ models through the precise deposition\nof cell-laden bioinks. Ensuring the fidelity and consistency of printed\nstructures in real-time remains a core challenge, particularly under\nconstraints imposed by limited imaging data and resource-constrained embedded\nhardware. Semantic segmentation of the extrusion process, differentiating\nbetween nozzle, extruded bioink, and surrounding background, enables in situ\nmonitoring critical to maintaining print quality and biological viability. In\nthis work, we introduce a lightweight semantic segmentation framework tailored\nfor real-time bioprinting applications. We present a novel, manually annotated\ndataset comprising 787 RGB images captured during the bioprinting process,\nlabeled across three classes: nozzle, bioink, and background. To achieve fast\nand efficient inference suitable for integration with bioprinting systems, we\npropose a BioLite U-Net architecture that leverages depthwise separable\nconvolutions to drastically reduce computational load without compromising\naccuracy. Our model is benchmarked against MobileNetV2 and MobileNetV3-based\nsegmentation baselines using mean Intersection over Union (mIoU), Dice score,\nand pixel accuracy. All models were evaluated on a Raspberry Pi 4B to assess\nreal-world feasibility. The proposed BioLite U-Net achieves an mIoU of 92.85%\nand a Dice score of 96.17%, while being over 1300x smaller than\nMobileNetV2-DeepLabV3+. On-device inference takes 335 ms per frame,\ndemonstrating near real-time capability. Compared to MobileNet baselines,\nBioLite U-Net offers a superior tradeoff between segmentation accuracy,\nefficiency, and deployability, making it highly suitable for intelligent,\nclosed-loop bioprinting systems.", "AI": {"tldr": "该研究提出了一种名为BioLite U-Net的轻量级语义分割框架，专为实时生物打印过程监控设计，实现了高精度和高效率，适用于资源受限的嵌入式硬件。", "motivation": "生物打印中，实时确保打印结构的一致性和保真度是一个核心挑战，尤其是在成像数据有限和嵌入式硬件资源受限的情况下。通过对挤出过程进行语义分割（区分喷嘴、挤出生物墨水和背景），可以实现原位监控，这对于维持打印质量和生物活力至关重要。", "method": "研究方法包括：1) 构建了一个包含787张手动标注RGB图像的新数据集，分为喷嘴、生物墨水和背景三类。2) 提出了一种BioLite U-Net架构，该架构利用深度可分离卷积来显著减少计算负荷，同时不牺牲精度。3) 将该模型与基于MobileNetV2和MobileNetV3的分割基线进行基准测试，评估指标包括mIoU、Dice分数和像素精度。4) 所有模型均在树莓派4B上进行评估，以测试实际可行性。", "result": "BioLite U-Net实现了92.85%的mIoU和96.17%的Dice分数，模型大小比MobileNetV2-DeepLabV3+小1300多倍。在设备上的推理时间为每帧335毫秒，展示了接近实时的能力。与MobileNet基线相比，BioLite U-Net在分割精度、效率和部署能力之间提供了更优的权衡。", "conclusion": "BioLite U-Net在生物打印应用中展现出卓越的性能，其在精度、效率和可部署性方面的优势使其非常适合集成到智能、闭环的生物打印系统中。"}}
{"id": "2509.06015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06015", "abs": "https://arxiv.org/abs/2509.06015", "authors": ["Zhiwen Shao", "Yifan Cheng", "Fan Zhang", "Xuehuai Shi", "Canlin Li", "Lizhuang Ma", "Dit-yan Yeung"], "title": "Micro-Expression Recognition via Fine-Grained Dynamic Perception", "comment": null, "summary": "Facial micro-expression recognition (MER) is a challenging task, due to the\ntransience, subtlety, and dynamics of micro-expressions (MEs). Most existing\nmethods resort to hand-crafted features or deep networks, in which the former\noften additionally requires key frames, and the latter suffers from small-scale\nand low-diversity training data. In this paper, we develop a novel fine-grained\ndynamic perception (FDP) framework for MER. We propose to rank frame-level\nfeatures of a sequence of raw frames in chronological order, in which the rank\nprocess encodes the dynamic information of both ME appearances and motions.\nSpecifically, a novel local-global feature-aware transformer is proposed for\nframe representation learning. A rank scorer is further adopted to calculate\nrank scores of each frame-level feature. Afterwards, the rank features from\nrank scorer are pooled in temporal dimension to capture dynamic representation.\nFinally, the dynamic representation is shared by a MER module and a dynamic\nimage construction module, in which the former predicts the ME category, and\nthe latter uses an encoder-decoder structure to construct the dynamic image.\nThe design of dynamic image construction task is beneficial for capturing\nfacial subtle actions associated with MEs and alleviating the data scarcity\nissue. Extensive experiments show that our method (i) significantly outperforms\nthe state-of-the-art MER methods, and (ii) works well for dynamic image\nconstruction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11%\nover the previous best results in terms of F1-score on the CASME II, SAMM,\nCAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available at\nhttps://github.com/CYF-cuber/FDP.", "AI": {"tldr": "本文提出了一种名为细粒度动态感知（FDP）的新型框架，用于微表情识别（MER），通过对帧级特征进行排序来编码动态信息，并结合动态图像构建任务以解决数据稀缺问题，显著优于现有SOTA方法。", "motivation": "微表情识别因其短暂性、微妙性和动态性而极具挑战。现有方法要么依赖手工特征（常需关键帧），要么深度网络受限于小规模和低多样性的训练数据。", "method": "本文提出了细粒度动态感知（FDP）框架。首先，对原始帧序列的帧级特征按时间顺序进行排序，以编码微表情的外观和运动动态信息。具体而言，采用了一种新颖的局部-全局特征感知Transformer进行帧表示学习，然后使用一个排序评分器计算每帧特征的排序分数。接着，将排序特征在时间维度上进行池化以捕获动态表示。最后，该动态表示被一个MER模块和一个动态图像构建模块共享，前者预测微表情类别，后者通过编解码器结构构建动态图像，此任务有助于捕捉面部细微动作并缓解数据稀缺问题。", "result": "实验结果表明，该方法（i）显著优于最先进的微表情识别方法，并且（ii）在动态图像构建方面表现良好。特别是在CASME II、SAMM、CAS(ME)^2和CAS(ME)^3数据集上，F1分数分别比之前的最佳结果提高了4.05%、2.50%、7.71%和2.11%。", "conclusion": "FDP框架通过其独特的动态信息编码和辅助动态图像构建任务，有效解决了微表情识别的挑战，显著提升了识别性能，并缓解了数据稀缺问题。"}}
{"id": "2509.06713", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06713", "abs": "https://arxiv.org/abs/2509.06713", "authors": ["Mustafa Yurdakul", "Şakir Taşdemir"], "title": "MRI-Based Brain Tumor Detection through an Explainable EfficientNetV2 and MLP-Mixer-Attention Architecture", "comment": null, "summary": "Brain tumors are serious health problems that require early diagnosis due to\ntheir high mortality rates. Diagnosing tumors by examining Magnetic Resonance\nImaging (MRI) images is a process that requires expertise and is prone to\nerror. Therefore, the need for automated diagnosis systems is increasing day by\nday. In this context, a robust and explainable Deep Learning (DL) model for the\nclassification of brain tumors is proposed. In this study, a publicly available\nFigshare dataset containing 3,064 T1-weighted contrast-enhanced brain MRI\nimages of three tumor types was used. First, the classification performance of\nnine well-known CNN architectures was evaluated to determine the most effective\nbackbone. Among these, EfficientNetV2 demonstrated the best performance and was\nselected as the backbone for further development. Subsequently, an\nattention-based MLP-Mixer architecture was integrated into EfficientNetV2 to\nenhance its classification capability. The performance of the final model was\ncomprehensively compared with basic CNNs and the methods in the literature.\nAdditionally, Grad-CAM visualization was used to interpret and validate the\ndecision-making process of the proposed model. The proposed model's performance\nwas evaluated using the five-fold cross-validation method. The proposed model\ndemonstrated superior performance with 99.50% accuracy, 99.47% precision,\n99.52% recall and 99.49% F1 score. The results obtained show that the model\noutperforms the studies in the literature. Moreover, Grad-CAM visualizations\ndemonstrate that the model effectively focuses on relevant regions of MRI\nimages, thus improving interpretability and clinical reliability. A robust deep\nlearning model for clinical decision support systems has been obtained by\ncombining EfficientNetV2 and attention-based MLP-Mixer, providing high accuracy\nand interpretability in brain tumor classification.", "AI": {"tldr": "提出了一种结合EfficientNetV2和注意力机制MLP-Mixer的深度学习模型，用于脑肿瘤的自动化、高精度和可解释性分类，实现了99.50%的准确率。", "motivation": "脑肿瘤死亡率高，早期诊断至关重要。通过MRI图像诊断需要专业知识且易出错，因此迫切需要自动化、鲁棒且可解释的诊断系统。", "method": "研究使用了包含3064张T1加权对比增强脑部MRI图像的Figshare公开数据集。首先评估了九种CNN架构，选择EfficientNetV2作为最佳主干网络。随后，将基于注意力的MLP-Mixer架构集成到EfficientNetV2中以增强分类能力。模型性能通过五折交叉验证进行评估，并与基本CNN和现有文献方法进行比较。同时，使用Grad-CAM可视化来解释模型的决策过程。", "result": "所提出的模型表现出卓越的性能，准确率为99.50%，精确率为99.47%，召回率为99.52%，F1分数为99.49%。这些结果优于现有文献中的研究。此外，Grad-CAM可视化证明模型能有效聚焦于MRI图像的相关区域，从而提高了可解释性和临床可靠性。", "conclusion": "通过结合EfficientNetV2和基于注意力的MLP-Mixer，成功构建了一个在脑肿瘤分类方面具有高准确性和可解释性的鲁棒深度学习模型，为临床决策支持系统提供了有力工具。"}}
{"id": "2509.06023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06023", "abs": "https://arxiv.org/abs/2509.06023", "authors": ["Mengmeng Liu", "Michael Ying Yang", "Jiuming Liu", "Yunpeng Zhang", "Jiangtao Li", "Sander Oude Elberink", "George Vosselman", "Hao Cheng"], "title": "DVLO4D: Deep Visual-Lidar Odometry with Sparse Spatial-temporal Fusion", "comment": "Accepted by ICRA 2025", "summary": "Visual-LiDAR odometry is a critical component for autonomous system\nlocalization, yet achieving high accuracy and strong robustness remains a\nchallenge. Traditional approaches commonly struggle with sensor misalignment,\nfail to fully leverage temporal information, and require extensive manual\ntuning to handle diverse sensor configurations. To address these problems, we\nintroduce DVLO4D, a novel visual-LiDAR odometry framework that leverages sparse\nspatial-temporal fusion to enhance accuracy and robustness. Our approach\nproposes three key innovations: (1) Sparse Query Fusion, which utilizes sparse\nLiDAR queries for effective multi-modal data fusion; (2) a Temporal Interaction\nand Update module that integrates temporally-predicted positions with current\nframe data, providing better initialization values for pose estimation and\nenhancing model's robustness against accumulative errors; and (3) a Temporal\nClip Training strategy combined with a Collective Average Loss mechanism that\naggregates losses across multiple frames, enabling global optimization and\nreducing the scale drift over long sequences. Extensive experiments on the\nKITTI and Argoverse Odometry dataset demonstrate the superiority of our\nproposed DVLO4D, which achieves state-of-the-art performance in terms of both\npose accuracy and robustness. Additionally, our method has high efficiency,\nwith an inference time of 82 ms, possessing the potential for the real-time\ndeployment.", "AI": {"tldr": "DVLO4D是一种新型的视觉-激光雷达里程计框架，通过稀疏时空融合提高了定位的准确性和鲁棒性，并实现了高效的实时部署潜力。", "motivation": "传统视觉-激光雷达里程计面临传感器未对准、未能充分利用时间信息以及需要大量手动调优以适应不同传感器配置的挑战，导致难以实现高精度和强鲁棒性。", "method": "本文提出了DVLO4D框架，包含三项关键创新：1) 稀疏查询融合，利用稀疏激光雷达查询实现高效的多模态数据融合；2) 时间交互与更新模块，将时间预测位置与当前帧数据结合，提供更好的位姿估计初始化并增强模型对累积误差的鲁棒性；3) 时间片段训练策略结合集体平均损失机制，聚合多帧损失以实现全局优化并减少长序列的尺度漂移。", "result": "在KITTI和Argoverse里程计数据集上的广泛实验表明，DVLO4D在位姿精度和鲁棒性方面均达到了最先进的性能。此外，该方法具有高效率，推理时间为82毫秒，具备实时部署的潜力。", "conclusion": "DVLO4D通过其创新的稀疏时空融合方法，显著提升了视觉-激光雷达里程计的准确性、鲁棒性和效率，为自动驾驶系统的定位提供了高性能的解决方案。"}}
{"id": "2509.06854", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06854", "abs": "https://arxiv.org/abs/2509.06854", "authors": ["Hajar Moradmand", "Lei Ren"], "title": "Automated Radiographic Total Sharp Score (ARTSS) in Rheumatoid Arthritis: A Solution to Reduce Inter-Intra Reader Variation and Enhancing Clinical Practice", "comment": null, "summary": "Assessing the severity of rheumatoid arthritis (RA) using the Total Sharp/Van\nDer Heijde Score (TSS) is crucial, but manual scoring is often time-consuming\nand subjective. This study introduces an Automated Radiographic Sharp Scoring\n(ARTSS) framework that leverages deep learning to analyze full-hand X-ray\nimages, aiming to reduce inter- and intra-observer variability. The research\nuniquely accommodates patients with joint disappearance and variable-length\nimage sequences. We developed ARTSS using data from 970 patients, structured\ninto four stages: I) Image pre-processing and re-orientation using ResNet50,\nII) Hand segmentation using UNet.3, III) Joint identification using YOLOv7, and\nIV) TSS prediction using models such as VGG16, VGG19, ResNet50, DenseNet201,\nEfficientNetB0, and Vision Transformer (ViT). We evaluated model performance\nwith Intersection over Union (IoU), Mean Average Precision (MAP), mean absolute\nerror (MAE), Root Mean Squared Error (RMSE), and Huber loss. The average TSS\nfrom two radiologists was used as the ground truth. Model training employed\n3-fold cross-validation, with each fold consisting of 452 training and 227\nvalidation samples, and external testing included 291 unseen subjects. Our\njoint identification model achieved 99% accuracy. The best-performing model,\nViT, achieved a notably low Huber loss of 0.87 for TSS prediction. Our results\ndemonstrate the potential of deep learning to automate RA scoring, which can\nsignificantly enhance clinical practice. Our approach addresses the challenge\nof joint disappearance and variable joint numbers, offers timesaving benefits,\nreduces inter- and intra-reader variability, improves radiologist accuracy, and\naids rheumatologists in making more informed decisions.", "AI": {"tldr": "本研究提出了一种名为ARTSS的深度学习框架，用于自动化分析全手X射线图像，以评估类风湿关节炎（RA）的严重程度，旨在减少手动评分的时间消耗和主观性。", "motivation": "评估类风湿关节炎（RA）的夏普/范德海德总分（TSS）至关重要，但手动评分通常耗时且主观，存在观察者间和观察者内变异性。", "method": "ARTSS框架利用深度学习，分为四个阶段：I）使用ResNet50进行图像预处理和重新定向；II）使用UNet.3进行手部分割；III）使用YOLOv7进行关节识别；IV）使用VGG16、VGG19、ResNet50、DenseNet201、EfficientNetB0和Vision Transformer（ViT）等模型进行TSS预测。研究使用了970名患者的数据，采用3折交叉验证进行模型训练，并包含291名未见过受试者的外部测试。地面真相为两位放射科医生的平均TSS。", "result": "关节识别模型达到了99%的准确率。在TSS预测方面，表现最佳的模型ViT实现了0.87的Huber损失。该方法成功处理了关节消失和可变长度图像序列的挑战。", "conclusion": "深度学习在自动化RA评分方面具有巨大潜力，可以显著改善临床实践。ARTSS方法能够节省时间、减少观察者间和观察者内变异性、提高放射科医生的准确性，并帮助风湿病专家做出更明智的决策。"}}
{"id": "2509.06033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06033", "abs": "https://arxiv.org/abs/2509.06033", "authors": ["Nadia Bakhsheshi", "Hamid Beigy"], "title": "Analysis of Blood Report Images Using General Purpose Vision-Language Models", "comment": "4 pages , 3 figures , This paper has been submitted to the\n  IEEE-affiliated ICBME Conference (Iran), 2025, and is currently under review.\n  DOR number: [20.1001.2.0425023682.1404.10.1.440.7]", "summary": "The reliable analysis of blood reports is important for health knowledge, but\nindividuals often struggle with interpretation, leading to anxiety and\noverlooked issues. We explore the potential of general-purpose Vision-Language\nModels (VLMs) to address this challenge by automatically analyzing blood report\nimages. We conduct a comparative evaluation of three VLMs: Qwen-VL-Max, Gemini\n2.5 Pro, and Llama 4 Maverick, determining their performance on a dataset of\n100 diverse blood report images. Each model was prompted with clinically\nrelevant questions adapted to each blood report. The answers were then\nprocessed using Sentence-BERT to compare and evaluate how closely the models\nresponded. The findings suggest that general-purpose VLMs are a practical and\npromising technology for developing patient-facing tools for preliminary blood\nreport analysis. Their ability to provide clear interpretations directly from\nimages can improve health literacy and reduce the limitations to understanding\ncomplex medical information. This work establishes a foundation for the future\ndevelopment of reliable and accessible AI-assisted healthcare applications.\nWhile results are encouraging, they should be interpreted cautiously given the\nlimited dataset size.", "AI": {"tldr": "本研究评估了通用视觉-语言模型（VLMs）自动分析血检报告图像的潜力，发现它们在提高健康素养方面具有实用性和前景。", "motivation": "个人在解读血检报告时常感困难，导致焦虑并可能忽视重要健康问题，因此需要一种可靠的自动化分析方法。", "method": "研究选取了Qwen-VL-Max、Gemini 2.5 Pro和Llama 4 Maverick三款VLM，在一个包含100张不同血检报告图像的数据集上进行对比评估。模型被提示回答与每份报告相关的临床问题，然后使用Sentence-BERT处理和比较模型的回答。", "result": "研究结果表明，通用VLM是开发面向患者的初步血检报告分析工具的实用且有前景的技术。它们能够直接从图像提供清晰的解读，有助于提高健康素养并减少理解复杂医疗信息的障碍。", "conclusion": "VLM为未来开发可靠且易于获取的AI辅助医疗应用奠定了基础。尽管结果令人鼓舞，但鉴于数据集规模有限，应谨慎解读。"}}
{"id": "2509.06885", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.06885", "abs": "https://arxiv.org/abs/2509.06885", "authors": ["Morteza Kiani Haftlang", "Mohammadhossein Malmir", "Foroutan Parand", "Umberto Michelucci", "Safouane El Ghazouali"], "title": "Barlow-Swin: Toward a novel siamese-based segmentation architecture using Swin-Transformers", "comment": null, "summary": "Medical image segmentation is a critical task in clinical workflows,\nparticularly for the detection and delineation of pathological regions. While\nconvolutional architectures like U-Net have become standard for such tasks,\ntheir limited receptive field restricts global context modeling. Recent efforts\nintegrating transformers have addressed this, but often result in deep,\ncomputationally expensive models unsuitable for real-time use. In this work, we\npresent a novel end-to-end lightweight architecture designed specifically for\nreal-time binary medical image segmentation. Our model combines a Swin\nTransformer-like encoder with a U-Net-like decoder, connected via skip pathways\nto preserve spatial detail while capturing contextual information. Unlike\nexisting designs such as Swin Transformer or U-Net, our architecture is\nsignificantly shallower and competitively efficient. To improve the encoder's\nability to learn meaningful features without relying on large amounts of\nlabeled data, we first train it using Barlow Twins, a self-supervised learning\nmethod that helps the model focus on important patterns by reducing unnecessary\nrepetition in the learned features. After this pretraining, we fine-tune the\nentire model for our specific task. Experiments on benchmark binary\nsegmentation tasks demonstrate that our model achieves competitive accuracy\nwith substantially reduced parameter count and faster inference, positioning it\nas a practical alternative for deployment in real-time and resource-limited\nclinical environments. The code for our method is available at Github\nrepository: https://github.com/mkianih/Barlow-Swin.", "AI": {"tldr": "本文提出了一种轻量级端到端架构，结合了类Swin Transformer编码器和类U-Net解码器，并通过Barlow Twins自监督预训练，实现了实时二值医学图像分割，在保持竞争性准确率的同时显著减少了参数量并加快了推理速度。", "motivation": "卷积网络（如U-Net）的感受野有限，难以建模全局上下文；现有Transformer集成模型计算成本高昂，不适合实时应用。临床工作流需要一种轻量级、实时且高效的医学图像分割模型。", "method": "提出了一种新颖的端到端轻量级架构，结合了类Swin Transformer编码器和类U-Net解码器，通过跳跃连接保留空间细节并捕获上下文信息。该模型比现有设计更浅、更高效。编码器首先通过Barlow Twins自监督学习方法进行预训练，以在数据量有限的情况下学习有意义的特征，然后对整个模型进行微调。", "result": "在基准二值分割任务上，该模型实现了竞争性的准确率，同时显著减少了参数数量并加快了推理速度。这使其成为在实时和资源受限的临床环境中部署的实用替代方案。", "conclusion": "所提出的轻量级架构通过结合自监督预训练和高效的编码器-解码器设计，为实时二值医学图像分割提供了一个实用且高性能的解决方案，克服了传统方法在全局上下文建模和计算效率方面的限制。"}}
{"id": "2509.06041", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06041", "abs": "https://arxiv.org/abs/2509.06041", "authors": ["Mohammad Ahangarkiasari", "Hassan Pouraria"], "title": "Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities", "comment": null, "summary": "Buoyancy-driven heat transfer in closed cavities serves as a canonical\ntestbed for thermal design High-fidelity CFD modelling yields accurate thermal\nfield solutions, yet its reliance on expert-crafted physics models, fine\nmeshes, and intensive computation limits rapid iteration. Recent developments\nin data-driven modeling, especially Graph Neural Networks (GNNs), offer new\nalternatives for learning thermal-fluid behavior directly from simulation data,\nparticularly on irregular mesh structures. However, conventional GNNs often\nstruggle to capture long-range dependencies in high-resolution graph\nstructures. To overcome this limitation, we propose a novel multi-stage GNN\narchitecture that leverages hierarchical pooling and unpooling operations to\nprogressively model global-to-local interactions across multiple spatial\nscales. We evaluate the proposed model on our newly developed CFD dataset\nsimulating natural convection within a rectangular cavities with varying aspect\nratios where the bottom wall is isothermal hot, the top wall is isothermal\ncold, and the two vertical walls are adiabatic. Experimental results\ndemonstrate that the proposed model achieves higher predictive accuracy,\nimproved training efficiency, and reduced long-term error accumulation compared\nto state-of-the-art (SOTA) GNN baselines. These findings underscore the\npotential of the proposed multi-stage GNN approach for modeling complex heat\ntransfer in mesh-based fluid dynamics simulations.", "AI": {"tldr": "针对传统GNN难以捕捉高分辨率网格中长距离依赖的问题，本文提出一种多阶段GNN架构，通过分层池化与反池化操作，有效提升了复杂传热模拟的精度和效率。", "motivation": "高精度CFD模拟计算资源消耗大，限制快速迭代；现有GNN在处理高分辨率网格数据时难以有效捕捉长距离依赖关系。", "method": "提出一种新颖的多阶段图神经网络（GNN）架构，通过分层池化（pooling）和反池化（unpooling）操作，逐步建模跨多个空间尺度的全局到局部交互。该模型在一个模拟矩形腔内自然对流的CFD数据集上进行了评估。", "result": "与现有最先进的GNN基线相比，所提出的模型实现了更高的预测精度、更高的训练效率，并减少了长期误差积累。", "conclusion": "提出的多阶段GNN方法在基于网格的流体动力学模拟中，对复杂传热建模展现出巨大潜力。"}}
{"id": "2509.06956", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06956", "abs": "https://arxiv.org/abs/2509.06956", "authors": ["Wenhao Li", "Mengyuan Liu", "Hong Liu", "Pichao Wang", "Shijian Lu", "Nicu Sebe"], "title": "H$_{2}$OT: Hierarchical Hourglass Tokenizer for Efficient Video Pose Transformers", "comment": "Accepted by TPAMI 2025, Open Sourced. arXiv admin note: substantial\n  text overlap with arXiv:2311.12028", "summary": "Transformers have been successfully applied in the field of video-based 3D\nhuman pose estimation. However, the high computational costs of these video\npose transformers (VPTs) make them impractical on resource-constrained devices.\nIn this paper, we present a hierarchical plug-and-play pruning-and-recovering\nframework, called Hierarchical Hourglass Tokenizer (H$_{2}$OT), for efficient\ntransformer-based 3D human pose estimation from videos. H$_{2}$OT begins with\nprogressively pruning pose tokens of redundant frames and ends with recovering\nfull-length sequences, resulting in a few pose tokens in the intermediate\ntransformer blocks and thus improving the model efficiency. It works with two\nkey modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module\n(TRM). TPM dynamically selects a few representative tokens to eliminate the\nredundancy of video frames, while TRM restores the detailed spatio-temporal\ninformation based on the selected tokens, thereby expanding the network output\nto the original full-length temporal resolution for fast inference. Our method\nis general-purpose: it can be easily incorporated into common VPT models on\nboth seq2seq and seq2frame pipelines while effectively accommodating different\ntoken pruning and recovery strategies. In addition, our H$_{2}$OT reveals that\nmaintaining the full pose sequence is unnecessary, and a few pose tokens of\nrepresentative frames can achieve both high efficiency and estimation accuracy.\nExtensive experiments on multiple benchmark datasets demonstrate both the\neffectiveness and efficiency of the proposed method. Code and models are\navailable at https://github.com/NationalGAILab/HoT.", "AI": {"tldr": "本文提出了一种名为H$_{2}$OT的分层即插即用剪枝-恢复框架，用于提高视频3D人体姿态估计中Transformer模型的效率，通过动态剪枝冗余帧的姿态标记并恢复完整序列，显著降低计算成本。", "motivation": "Transformer模型在视频3D人体姿态估计领域表现出色，但其高昂的计算成本使其在资源受限设备上难以实际应用。", "method": "本文提出了H$_{2}$OT框架，包含两个关键模块：标记剪枝模块（TPM）和标记恢复模块（TRM）。TPM动态选择少量代表性标记以消除视频帧的冗余，而TRM则根据这些选定标记恢复详细的时空信息，将网络输出扩展到原始的全长时间分辨率，实现快速推理。该方法是通用性的，可集成到seq2seq和seq2frame管道的常见VPT模型中。", "result": "H$_{2}$OT在中间Transformer块中仅使用少量姿态标记，显著提高了模型效率。研究表明，维持完整的姿态序列并非必要，少量代表性帧的姿态标记即可实现高效率和高估计精度。在多个基准数据集上的大量实验证明了所提出方法的有效性和效率。", "conclusion": "H$_{2}$OT提供了一种有效且高效的解决方案，通过分层剪枝和恢复姿态标记，解决了视频3D人体姿态估计中Transformer模型计算成本高的问题，使其更适用于资源受限的环境。"}}
{"id": "2509.06068", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06068", "abs": "https://arxiv.org/abs/2509.06068", "authors": ["Shih-Ying Yeh"], "title": "Home-made Diffusion Model from Scratch to Hatch", "comment": null, "summary": "We introduce Home-made Diffusion Model (HDM), an efficient yet powerful\ntext-to-image diffusion model optimized for training (and inferring) on\nconsumer-grade hardware. HDM achieves competitive 1024x1024 generation quality\nwhile maintaining a remarkably low training cost of $535-620 using four RTX5090\nGPUs, representing a significant reduction in computational requirements\ncompared to traditional approaches. Our key contributions include: (1)\nCross-U-Transformer (XUT), a novel U-shape transformer, Cross-U-Transformer\n(XUT), that employs cross-attention for skip connections, providing superior\nfeature integration that leads to remarkable compositional consistency; (2) a\ncomprehensive training recipe that incorporates TREAD acceleration, a novel\nshifted square crop strategy for efficient arbitrary aspect-ratio training, and\nprogressive resolution scaling; and (3) an empirical demonstration that smaller\nmodels (343M parameters) with carefully crafted architectures can achieve\nhigh-quality results and emergent capabilities, such as intuitive camera\ncontrol. Our work provides an alternative paradigm of scaling, demonstrating a\nviable path toward democratizing high-quality text-to-image generation for\nindividual researchers and smaller organizations with limited computational\nresources.", "AI": {"tldr": "本文介绍了Home-made Diffusion Model (HDM)，一个高效且强大的文本到图像扩散模型，专为消费级硬件优化，以极低的训练成本实现了高质量的图像生成。", "motivation": "传统文本到图像扩散模型计算成本高昂，限制了个人研究者和小型组织的使用。该研究旨在降低计算门槛，使高质量文本到图像生成民主化。", "method": "本文提出HDM模型，其核心方法包括：1) Cross-U-Transformer (XUT)，一种新型U形Transformer，利用交叉注意力进行跳跃连接，以实现卓越的特征整合和组合一致性；2) 综合训练方案，融合了TREAD加速、用于高效任意长宽比训练的移位方形裁剪策略，以及渐进式分辨率缩放；3) 采用较小模型（343M参数）和精心设计的架构。", "result": "HDM在1024x1024分辨率下实现了具有竞争力的生成质量，同时训练成本显著降低，使用四块RTX5090 GPU仅需$535-620。XUT带来了卓越的组合一致性，并且小模型展现出高质量结果和直观的相机控制等新兴能力。", "conclusion": "HDM提供了一种替代的扩展范式，证明了为计算资源有限的个人研究者和小型组织民主化高质量文本到图像生成的可行路径。"}}
{"id": "2509.06082", "categories": ["cs.CV", "cond-mat.mtrl-sci", "90C20, 94A08, 68U10"], "pdf": "https://arxiv.org/pdf/2509.06082", "abs": "https://arxiv.org/abs/2509.06082", "authors": ["Anuraag Mishra", "Andrea Gilch", "Benjamin Apeleo Zubiri", "Jan Rolfes", "Frauke Liers"], "title": "High-Quality Tomographic Image Reconstruction Integrating Neural Networks and Mathematical Optimization", "comment": "36 pages, 17 figures", "summary": "In this work, we develop a novel technique for reconstructing images from\nprojection-based nano- and microtomography. Our contribution focuses on\nenhancing reconstruction quality, particularly for specimen composed of\nhomogeneous material phases connected by sharp edges. This is accomplished by\ntraining a neural network to identify edges within subpictures. The trained\nnetwork is then integrated into a mathematical optimization model, to reduce\nartifacts from previous reconstructions. To this end, the optimization approach\nfavors solutions according to the learned predictions, however may also\ndetermine alternative solutions if these are strongly supported by the raw\ndata. Hence, our technique successfully incorporates knowledge about the\nhomogeneity and presence of sharp edges in the sample and thereby eliminates\nblurriness. Our results on experimental datasets show significant enhancements\nin interface sharpness and material homogeneity compared to benchmark\nalgorithms. Thus, our technique produces high-quality reconstructions,\nshowcasing its potential for advancing tomographic imaging techniques.", "AI": {"tldr": "开发了一种结合神经网络边缘识别与数学优化模型的新型断层扫描重建技术，旨在提高具有均匀材料相和锐利边缘样本的图像质量，有效消除模糊并增强界面清晰度。", "motivation": "现有的投影式纳米和微断层扫描重建技术在处理由均匀材料相和锐利边缘组成的样本时，可能存在伪影和模糊，导致重建质量不佳。", "method": "该方法首先训练一个神经网络来识别子图像中的边缘。然后，将训练好的神经网络集成到一个数学优化模型中，以减少先前重建中的伪影。优化方法倾向于根据神经网络学习的预测来寻找解决方案，但如果原始数据强烈支持，也会考虑替代方案。通过这种方式，该技术成功地结合了样本的均匀性和锐利边缘的先验知识。", "result": "在实验数据集上的结果表明，与基准算法相比，该技术在界面锐度和材料均匀性方面都有显著提升。", "conclusion": "该技术能够产生高质量的重建图像，展示了其在推进断层扫描成像技术方面的巨大潜力。"}}
{"id": "2509.06096", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06096", "abs": "https://arxiv.org/abs/2509.06096", "authors": ["Yiwen Ye", "Yicheng Wu", "Xiangde Luo", "He Zhang", "Ziyang Chen", "Ting Dang", "Yanning Zhang", "Yong Xia"], "title": "MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation", "comment": "10 pages, 5 figures", "summary": "Foundation models have become a promising paradigm for advancing medical\nimage analysis, particularly for segmentation tasks where downstream\napplications often emerge sequentially. Existing fine-tuning strategies,\nhowever, remain limited: parallel fine-tuning isolates tasks and fails to\nexploit shared knowledge, while multi-task fine-tuning requires simultaneous\naccess to all datasets and struggles with incremental task integration. To\naddress these challenges, we propose MedSeqFT, a sequential fine-tuning\nframework that progressively adapts pre-trained models to new tasks while\nrefining their representational capacity. MedSeqFT introduces two core\ncomponents: (1) Maximum Data Similarity (MDS) selection, which identifies\ndownstream samples most representative of the original pre-training\ndistribution to preserve general knowledge, and (2) Knowledge and\nGeneralization Retention Fine-Tuning (K&G RFT), a LoRA-based knowledge\ndistillation scheme that balances task-specific adaptation with the retention\nof pre-trained knowledge. Extensive experiments on two multi-task datasets\ncovering ten 3D segmentation tasks demonstrate that MedSeqFT consistently\noutperforms state-of-the-art fine-tuning strategies, yielding substantial\nperformance gains (e.g., an average Dice improvement of 3.0%). Furthermore,\nevaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFT\nenhances transferability, particularly for tumor segmentation. Visual analyses\nof loss landscapes and parameter variations further highlight the robustness of\nMedSeqFT. These results establish sequential fine-tuning as an effective,\nknowledge-retentive paradigm for adapting foundation models to evolving\nclinical tasks. Code will be released.", "AI": {"tldr": "MedSeqFT是一种用于医学图像分割的顺序微调框架，它通过最大数据相似性选择和基于LoRA的知识蒸馏，使预训练模型能够逐步适应新任务，同时保留通用知识和泛化能力。", "motivation": "现有的微调策略（并行微调和多任务微调）在医学图像分析中存在局限性：并行微调无法利用共享知识，多任务微调需要同时访问所有数据集且难以增量集成任务，这不适用于下游应用顺序出现的场景。", "method": "本文提出了MedSeqFT框架，包含两个核心组件：1) 最大数据相似性（MDS）选择，用于识别最能代表原始预训练分布的下游样本，以保留通用知识；2) 知识和泛化保留微调（K&G RFT），这是一种基于LoRA的知识蒸馏方案，旨在平衡任务特定适应与预训练知识的保留。", "result": "MedSeqFT在两个包含十个3D分割任务的多任务数据集上持续优于最先进的微调策略，取得了显著的性能提升（例如，平均Dice分数提高3.0%）。此外，在两个未见任务（COVID-19-20和Kidney）上的评估验证了MedSeqFT增强了可迁移性，特别是对于肿瘤分割。对损失景观和参数变化的视觉分析进一步强调了MedSeqFT的鲁棒性。", "conclusion": "MedSeqFT将顺序微调确立为一种有效且知识保留的范式，用于使基础模型适应不断演变的临床任务，从而在医学图像分析领域取得显著进展。"}}
{"id": "2509.06105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06105", "abs": "https://arxiv.org/abs/2509.06105", "authors": ["Yating Huang", "Ziyan Huang", "Lintao Xiang", "Qijun Yang", "Hujun Yin"], "title": "PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology", "comment": "Accept by EMNLP2025", "summary": "Accurate analysis of pathological images is essential for automated tumor\ndiagnosis but remains challenging due to high structural similarity and subtle\nmorphological variations in tissue images. Current vision-language (VL) models\noften struggle to capture the complex reasoning required for interpreting\nstructured pathological reports. To address these limitations, we propose\nPathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities in\nhierarchical semantic understanding and compositional reasoning within the\npathology domain. Results of this benchmark reveal that existing VL models fail\nto effectively model intricate cross-modal relationships, hence limiting their\napplicability in clinical setting. To overcome this, we further introduce a\npathology-specific VL training scheme that generates enhanced and perturbed\nsamples for multimodal contrastive learning. Experimental evaluations\ndemonstrate that our approach achieves state-of-the-art performance on\nPathoHR-Bench and six additional pathology datasets, highlighting its\neffectiveness in fine-grained pathology representation.", "AI": {"tldr": "该论文提出了PathoHR-Bench基准来评估视觉-语言（VL）模型在病理学中的分层语义理解和组合推理能力，并引入了一种病理学专用VL训练方案，实现了最先进的性能。", "motivation": "自动化肿瘤诊断需要精确分析病理图像，但由于结构相似性和细微形态变异而充满挑战。现有的视觉-语言模型难以捕捉解释结构化病理报告所需的复杂推理，限制了其在临床环境中的应用。", "method": "1. 提出了PathoHR-Bench基准，用于评估VL模型在病理领域的分层语义理解和组合推理能力。2. 引入了一种病理学专用的VL训练方案，通过生成增强和扰动样本进行多模态对比学习。", "result": "1. 现有VL模型未能有效建模复杂的跨模态关系，在PathoHR-Bench上表现不佳。2. 提出的方法在PathoHR-Bench和六个额外的病理学数据集上均达到了最先进的性能。", "conclusion": "该研究表明，现有的VL模型在病理学复杂推理方面存在局限性。提出的PathoHR-Bench基准和病理学专用训练方案能有效增强VL模型在细粒度病理学表示方面的能力，提升其在临床环境中的适用性。"}}
{"id": "2509.06116", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2509.06116", "abs": "https://arxiv.org/abs/2509.06116", "authors": ["Giulia Bonino", "Luca Alberto Rizzo"], "title": "CARDIE: clustering algorithm on relevant descriptors for image enhancement", "comment": null, "summary": "Automatic image clustering is a cornerstone of computer vision, yet its\napplication to image enhancement remains limited, primarily due to the\ndifficulty of defining clusters that are meaningful for this specific task. To\naddress this issue, we introduce CARDIE, an unsupervised algorithm that\nclusters images based on their color and luminosity content. In addition, we\nintroduce a method to quantify the impact of image enhancement algorithms on\nluminance distribution and local variance. Using this method, we demonstrate\nthat CARDIE produces clusters more relevant to image enhancement than those\nderived from semantic image attributes. Furthermore, we demonstrate that CARDIE\nclusters can be leveraged to resample image enhancement datasets, leading to\nimproved performance for tone mapping and denoising algorithms. To encourage\nadoption and ensure reproducibility, we publicly release CARDIE code on our\nGitHub.", "AI": {"tldr": "本文提出了一种名为CARDIE的无监督图像聚类算法，它基于颜色和亮度内容对图像进行聚类，并证明其聚类结果比语义属性更适用于图像增强任务，从而提高了色调映射和去噪算法的性能。", "motivation": "自动图像聚类在计算机视觉中至关重要，但其在图像增强中的应用受限，主要是因为难以定义对该特定任务有意义的聚类。", "method": "引入了CARDIE，一种基于颜色和亮度内容对图像进行聚类的无监督算法。同时，提出了一种量化图像增强算法对亮度分布和局部方差影响的方法。利用该方法，将CARDIE产生的聚类与语义图像属性产生的聚类进行比较，并利用CARDIE聚类对图像增强数据集进行重采样。", "result": "CARDIE产生的聚类比语义图像属性产生的聚类更与图像增强相关。利用CARDIE聚类对图像增强数据集进行重采样，可以提高色调映射和去噪算法的性能。", "conclusion": "CARDIE是一种有效的无监督图像聚类算法，能根据颜色和亮度内容生成对图像增强任务有意义的聚类，进而提升相关算法的性能。为促进应用和复现，CARDIE代码已公开。"}}
{"id": "2509.06142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06142", "abs": "https://arxiv.org/abs/2509.06142", "authors": ["Zhengquan Luo", "Chi Liu", "Dongfu Xiao", "Zhen Yu", "Yueye Wang", "Tianqing Zhu"], "title": "RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving", "comment": null, "summary": "The integration of AI with medical images enables the extraction of implicit\nimage-derived biomarkers for a precise health assessment. Recently, retinal\nage, a biomarker predicted from fundus images, is a proven predictor of\nsystemic disease risks, behavioral patterns, aging trajectory and even\nmortality. However, the capability to infer such sensitive biometric data\nraises significant privacy risks, where unauthorized use of fundus images could\nlead to bioinformation leakage, breaching individual privacy. In response, we\nformulate a new research problem of biometric privacy associated with medical\nimages and propose RetinaGuard, a novel privacy-enhancing framework that\nemploys a feature-level generative adversarial masking mechanism to obscure\nretinal age while preserving image visual quality and disease diagnostic\nutility. The framework further utilizes a novel multiple-to-one knowledge\ndistillation strategy incorporating a retinal foundation model and diverse\nsurrogate age encoders to enable a universal defense against black-box age\nprediction models. Comprehensive evaluations confirm that RetinaGuard\nsuccessfully obfuscates retinal age prediction with minimal impact on image\nquality and pathological feature representation. RetinaGuard is also flexible\nfor extension to other medical image derived biomarkers. RetinaGuard is also\nflexible for extension to other medical image biomarkers.", "AI": {"tldr": "该研究提出RetinaGuard框架，通过特征级生成对抗掩码和多对一知识蒸馏，在保留眼底图像视觉质量和疾病诊断效用的同时，模糊了从图像中预测的视网膜年龄这一敏感生物识别信息，以应对生物信息泄露的隐私风险。", "motivation": "人工智能与医学图像结合可提取隐式图像衍生生物标志物，如视网膜年龄，它能有效预测全身疾病风险、行为模式、衰老轨迹乃至死亡率。然而，这种敏感生物特征数据的推断能力带来了严重的隐私风险，未经授权使用眼底图像可能导致生物信息泄露，侵犯个人隐私。", "method": "研究提出RetinaGuard隐私增强框架，通过以下方法实现：1) 采用特征级生成对抗掩码机制来模糊视网膜年龄，同时保留图像视觉质量和疾病诊断效用；2) 利用新颖的多对一知识蒸馏策略，结合视网膜基础模型和多样化的替代年龄编码器，实现对黑盒年龄预测模型的通用防御。", "result": "综合评估证实，RetinaGuard成功地模糊了视网膜年龄预测，同时对图像质量和病理特征表示的影响最小。该框架还具有灵活性，可扩展应用于其他医学图像衍生的生物标志物。", "conclusion": "RetinaGuard提供了一个有效且通用的隐私增强解决方案，能够保护从医学图像中提取的敏感生物识别信息（如视网膜年龄），同时不牺牲图像的诊断价值，并具备扩展到其他生物标志物的潜力。"}}
{"id": "2509.06155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06155", "abs": "https://arxiv.org/abs/2509.06155", "authors": ["Duomin Wang", "Wei Zuo", "Aojie Li", "Ling-Hao Chen", "Xinyao Liao", "Deyu Zhou", "Zixin Yin", "Xili Dai", "Daxin Jiang", "Gang Yu"], "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts", "comment": "Project page: https://dorniwang.github.io/UniVerse-1/", "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.", "AI": {"tldr": "UniVerse-1是一个统一的音视频生成模型，通过“专家缝合”技术整合预训练模型，并利用在线标注流程解决数据对齐问题，实现协调的音视频和高质量的语音生成。", "motivation": "研究旨在提高音视频生成模型的训练效率，充分利用现有预训练模型的强大能力，并解决传统文本标注中常见的对齐问题导致的性能下降，以推进音视频生成领域的研究并缩小与最先进模型（如Veo3）的性能差距。", "method": "引入UniVerse-1模型，该模型类似于Veo-3，能够同步生成协调的音频和视频。采用“专家缝合”（Stitching of Experts, SoE）技术，深度融合预训练视频和音乐生成专家模型的对应模块，以提升训练效率。开发了在线标注流程，在训练过程中处理训练数据并生成准确且时间对齐的环境音和语音标签。模型在约7,600小时的音视频数据上进行微调。为评估方法，引入了新的基准数据集Verse-Bench。", "result": "经过微调后，UniVerse-1模型能生成与视频内容良好协调的环境音，并在语音生成方面展现出强大的对齐能力。", "conclusion": "UniVerse-1通过结合专家缝合技术和在线标注流程，有效地实现了协调的音视频生成和准确的语音对齐。为促进音视频生成研究并缩小与SOTA模型的差距，作者公开了模型、代码以及新的基准数据集Verse-Bench。"}}
{"id": "2509.06228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06228", "abs": "https://arxiv.org/abs/2509.06228", "authors": ["Amna Hassan", "Ilsa Afzaal", "Nouman Muneeb", "Aneeqa Batool", "Hamail Noor"], "title": "AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models", "comment": "https://github.com/Amna-Hassan04/Fracture-Detection-Using-X-Rays-with-CNN", "summary": "Bone fractures present a major global health challenge, often resulting in\npain, reduced mobility, and productivity loss, particularly in low-resource\nsettings where access to expert radiology services is limited. Conventional\nimaging methods suffer from high costs, radiation exposure, and dependency on\nspecialized interpretation. To address this, we developed an AI-based solution\nfor automated fracture detection from X-ray images using a custom Convolutional\nNeural Network (CNN) and benchmarked it against transfer learning models\nincluding EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted on\nthe publicly available FracAtlas dataset, comprising 4,083 anonymized\nmusculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94\nprecision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.\nAlthough transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)\nperformed poorly in this specific setup, these results should be interpreted in\nlight of class imbalance and data set limitations. This work highlights the\npromise of lightweight CNNs for detecting fractures in X-rays and underscores\nthe importance of fair benchmarking, diverse datasets, and external validation\nfor clinical translation", "AI": {"tldr": "该研究开发了一种基于定制卷积神经网络（CNN）的AI解决方案，用于X射线图像的自动骨折检测。在FracAtlas数据集上，定制CNN取得了95.96%的准确率，优于迁移学习模型，展示了轻量级CNN在骨折检测中的潜力。", "motivation": "骨折是一个重大的全球健康挑战，尤其是在资源有限地区，专家放射学服务稀缺。传统成像方法存在成本高、辐射暴露和依赖专业解读等缺点。因此，需要一种自动化、可及的骨折检测方案。", "method": "研究开发了一个基于定制卷积神经网络（CNN）的AI解决方案，用于从X射线图像中自动检测骨折。该方案在包含4,083张匿名肌肉骨骼X射线片的公开FracAtlas数据集上进行训练，并与EfficientNetB0、MobileNetV2和ResNet50等迁移学习模型进行了基准测试。", "result": "定制CNN在FracAtlas数据集上取得了95.96%的准确率、0.94的精确度、0.88的召回率和0.91的F1分数。虽然迁移学习模型在此特定设置中表现不佳，但这些结果应结合类别不平衡和数据集限制来解释。", "conclusion": "这项工作突出了轻量级CNN在X射线骨折检测方面的潜力，并强调了公平基准测试、多样化数据集和外部验证对于临床转化的重要性。"}}
{"id": "2509.06246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06246", "abs": "https://arxiv.org/abs/2509.06246", "authors": ["Lucas Wojcik", "Luiz Coelho", "Roger Granada", "David Menotti"], "title": "Exploring Light-Weight Object Recognition for Real-Time Document Detection", "comment": null, "summary": "Object Recognition and Document Skew Estimation have come a long way in terms\nof performance and efficiency. New models follow one of two directions:\nimproving performance using larger models, and improving efficiency using\nsmaller models. However, real-time document detection and rectification is a\nniche that is largely unexplored by the literature, yet it remains a vital step\nfor automatic information retrieval from visual documents. In this work, we\nstrive towards an efficient document detection pipeline that is satisfactory in\nterms of Optical Character Recognition (OCR) retrieval and faster than other\navailable solutions. We adapt IWPOD-Net, a license plate detection network, and\ntrain it for detection on NBID, a synthetic ID card dataset. We experiment with\ndata augmentation and cross-dataset validation with MIDV (another synthetic ID\nand passport document dataset) to find the optimal scenario for the model.\nOther methods from both the Object Recognition and Skew Estimation\nstate-of-the-art are evaluated for comparison with our approach. We use each\nmethod to detect and rectify the document, which is then read by an OCR system.\nThe OCR output is then evaluated using a novel OCR quality metric based on the\nLevenshtein distance. Since the end goal is to improve automatic information\nretrieval, we use the overall OCR quality as a performance metric. We observe\nthat with a promising model, document rectification does not have to be perfect\nto attain state-of-the-art performance scores. We show that our model is\nsmaller and more efficient than current state-of-the-art solutions while\nretaining a competitive OCR quality metric. All code is available at\nhttps://github.com/BOVIFOCR/iwpod-doc-corners.git", "AI": {"tldr": "本文提出了一种高效的文档检测和校正流水线，通过改进IWPOD-Net并在合成数据集上训练，实现了更小、更高效的模型，同时保持了与现有先进解决方案相当的OCR识别质量。", "motivation": "尽管物体识别和文档倾斜估计取得了显著进展，但实时文档检测和校正这一细分领域尚未得到充分探索，而它对于从视觉文档中自动检索信息至关重要。现有模型要么追求更大模型的性能，要么追求更小模型的效率，未能有效结合。", "method": "研究人员改编了IWPOD-Net（一个车牌检测网络），并在合成身份证数据集NBID上进行了训练。他们通过数据增强和使用另一个合成数据集MIDV进行跨数据集验证来优化模型。同时，他们将提出的方法与物体识别和倾斜估计领域的其他先进方法进行了比较。性能评估标准是基于Levenshtein距离的新颖OCR质量度量，因为最终目标是提高自动信息检索能力。", "result": "研究发现，即使文档校正不完美，也可以获得先进的性能分数。提出的模型比现有先进解决方案更小、更高效，同时保持了具有竞争力的OCR质量指标。", "conclusion": "本文成功开发了一个高效的文档检测流水线，在OCR检索方面表现令人满意，并且比现有解决方案更快。该模型在保持竞争性OCR质量的同时，展现出更小的体积和更高的效率，证明了文档校正无需完美也能达到先进性能。"}}
{"id": "2509.06266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06266", "abs": "https://arxiv.org/abs/2509.06266", "authors": ["Mohsen Gholami", "Ahmad Rezaei", "Zhou Weimin", "Yong Zhang", "Mohammad Akbari"], "title": "Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes", "comment": null, "summary": "Understanding 3D spatial relationships remains a major limitation of current\nVision-Language Models (VLMs). Prior work has addressed this issue by creating\nspatial question-answering (QA) datasets based on single images or indoor\nvideos. However, real-world embodied AI agents such as robots and self-driving\ncars typically rely on ego-centric, multi-view observations. To this end, we\nintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatial\nreasoning abilities of VLMs using ego-centric, multi-view outdoor data.\nEgo3D-Bench comprises over 8,600 QA pairs, created with significant involvement\nfrom human annotators to ensure quality and diversity. We benchmark 16 SOTA\nVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our results\nreveal a notable performance gap between human level scores and VLM\nperformance, highlighting that current VLMs still fall short of human level\nspatial understanding. To bridge this gap, we propose Ego3D-VLM, a\npost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLM\ngenerates cognitive map based on estimated global 3D coordinates, resulting in\n12% average improvement on multi-choice QA and 56% average improvement on\nabsolute distance estimation. Ego3D-VLM is modular and can be integrated with\nany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools for\nadvancing toward human level spatial understanding in real-world, multi-view\nenvironments.", "AI": {"tldr": "当前视觉-语言模型（VLMs）在3D空间关系理解方面存在局限，尤其是在以自我为中心的多视角户外数据中。本文提出了Ego3D-Bench基准来评估VLMs的空间推理能力，并引入了Ego3D-VLM后训练框架以显著提升其性能。", "motivation": "当前VLMs在理解3D空间关系方面存在主要局限。现有工作主要基于单张图像或室内视频创建空间问答数据集，但现实世界的具身AI（如机器人和自动驾驶汽车）依赖于以自我为中心、多视角观测的户外数据，这方面的评估和提升仍是挑战。", "method": "本文引入了Ego3D-Bench，一个包含8,600多个由人工标注的问答对的新基准，用于评估VLMs在以自我为中心、多视角户外数据上的空间推理能力。作者使用该基准测试了包括GPT-4o、Gemini1.5-Pro等在内的16个SOTA VLM。为弥补性能差距，作者提出了Ego3D-VLM，一个模块化的后训练框架，通过基于估计的全局3D坐标生成认知地图来增强VLMs的3D空间推理能力。", "result": "在Ego3D-Bench上的基准测试显示，人类水平分数与VLM性能之间存在显著差距，表明当前VLMs仍远未达到人类水平的空间理解能力。Ego3D-VLM框架在多项选择问答任务上平均提高了12%的性能，在绝对距离估计任务上平均提高了56%的性能。", "conclusion": "Ego3D-Bench和Ego3D-VLM为推动VLMs在现实世界多视角环境中的人类水平空间理解提供了宝贵的工具。当前VLMs在空间理解方面仍有巨大提升空间，而Ego3D-VLM提供了一个有效的解决方案来弥补这一差距。"}}
{"id": "2509.06282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06282", "abs": "https://arxiv.org/abs/2509.06282", "authors": ["Cecelia Soh", "Rizhao Cai", "Monalisha Paul", "Dennis Sng", "Alex Kot"], "title": "AI-driven Remote Facial Skin Hydration and TEWL Assessment from Selfie Images: A Systematic Solution", "comment": "Paper accepted by the journal of Machine Intelligence Research\n  (JCR-Q1). To be in press soon", "summary": "Skin health and disease resistance are closely linked to the skin barrier\nfunction, which protects against environmental factors and water loss. Two key\nphysiological indicators can quantitatively represent this barrier function:\nskin hydration (SH) and trans-epidermal water loss (TEWL). Measurement of SH\nand TEWL is valuable for the public to monitor skin conditions regularly,\ndiagnose dermatological issues, and personalize their skincare regimens.\nHowever, these measurements are not easily accessible to general users unless\nthey visit a dermatology clinic with specialized instruments. To tackle this\nproblem, we propose a systematic solution to estimate SH and TEWL from selfie\nfacial images remotely with smartphones. Our solution encompasses multiple\nstages, including SH/TEWL data collection, data preprocessing, and formulating\na novel Skin-Prior Adaptive Vision Transformer model for SH/TEWL regression.\nThrough experiments, we identified the annotation imbalance of the SH/TEWL data\nand proposed a symmetric-based contrastive regularization to reduce the model\nbias due to the imbalance effectively. This work is the first study to explore\nskin assessment from selfie facial images without physical measurements. It\nbridges the gap between computer vision and skin care research, enabling\nAI-driven accessible skin analysis for broader real-world applications.", "AI": {"tldr": "该研究提出了一种利用智能手机自拍面部图像远程估计皮肤水合度（SH）和经表皮水分流失（TEWL）的方法，以实现更便捷的皮肤健康监测。", "motivation": "皮肤健康评估（如SH和TEWL测量）对公众监测皮肤状况、诊断皮肤问题和个性化护肤至关重要。然而，这些测量通常需要专业仪器和皮肤科诊所，对普通用户而言不易获取，限制了其普及性。", "method": "该方案包括SH/TEWL数据收集、数据预处理，并提出了一种新颖的Skin-Prior Adaptive Vision Transformer模型用于SH/TEWL回归。为解决数据标注不平衡问题，研究还引入了基于对称的对比正则化来减少模型偏差。", "result": "实验发现SH/TEWL数据存在标注不平衡问题，并提出的基于对称的对比正则化能有效减少由不平衡引起的模型偏差。这是首次探索无需物理测量，仅通过自拍面部图像进行皮肤评估的研究。", "conclusion": "这项工作弥合了计算机视觉和皮肤护理研究之间的鸿沟，通过AI驱动的方式，使皮肤分析变得更加易于获取，有望在更广泛的实际应用中推广。"}}
{"id": "2509.06291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06291", "abs": "https://arxiv.org/abs/2509.06291", "authors": ["Jiangnan Xie", "Xiaolong Zheng", "Liang Zheng"], "title": "Prototype-Aware Multimodal Alignment for Open-Vocabulary Visual Grounding", "comment": null, "summary": "Visual Grounding (VG) aims to utilize given natural language queries to\nlocate specific target objects within images. While current transformer-based\napproaches demonstrate strong localization performance in standard scene (i.e,\nscenarios without any novel objects), they exhibit notable limitations in\nopen-vocabulary scene (i.e, both familiar and novel object categories during\ntesting). These limitations primarily stem from three key factors: (1)\nimperfect alignment between visual and linguistic modalities, (2) insufficient\ncross-modal feature fusion, and (3) ineffective utilization of semantic\nprototype information. To overcome these challenges, we present Prototype-Aware\nMultimodal Learning (PAML), an innovative framework that systematically\naddresses these issues through several key components: First, we leverage ALBEF\nto establish robust cross-modal alignment during initial feature encoding.\nSubsequently, our Visual Discriminative Feature Encoder selectively enhances\nsalient object representations while suppressing irrelevant visual context. The\nframework then incorporates a novel prototype discovering and inheriting\nmechanism that extracts and aggregates multi-neighbor semantic prototypes to\nfacilitate open-vocabulary recognition. These enriched features undergo\ncomprehensive multimodal integration through our Multi-stage Decoder before\nfinal bounding box regression. Extensive experiments across five benchmark\ndatasets validate our approach, showing competitive performance in standard\nscene while achieving state-of-the-art results in open-vocabulary scene. Our\ncode is available at https://github.com/plankXie/PAML.", "AI": {"tldr": "本文提出了原型感知多模态学习（PAML）框架，旨在解决视觉定位（VG）在开放词汇场景中因模态对齐不佳、特征融合不足和原型信息利用效率低而面临的挑战，并在开放词汇场景中取得了最先进的性能。", "motivation": "现有基于Transformer的视觉定位方法在标准场景中表现良好，但在开放词汇场景（包含新颖对象类别）中存在显著局限性。这主要源于三个关键因素：视觉和语言模态之间对齐不完善、跨模态特征融合不足，以及语义原型信息利用效率低下。", "method": "PAML框架通过以下几个关键组件系统地解决这些问题：1) 利用ALBEF在初始特征编码阶段建立鲁棒的跨模态对齐。2) 引入视觉判别特征编码器，选择性地增强显著对象表示并抑制不相关视觉上下文。3) 包含新颖的原型发现和继承机制，提取并聚合多邻居语义原型，以促进开放词汇识别。4) 这些丰富的特征通过多阶段解码器进行全面的多模态整合，最终进行边界框回归。", "result": "在五个基准数据集上的广泛实验验证了PAML方法的有效性，在标准场景中展现出具有竞争力的性能，并在开放词汇场景中取得了最先进（SOTA）的结果。", "conclusion": "PAML框架通过改进跨模态对齐、增强特征融合以及有效利用语义原型信息，成功克服了视觉定位在开放词汇场景中的挑战，显著提升了模型在该场景下的识别能力。"}}
{"id": "2509.06306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06306", "abs": "https://arxiv.org/abs/2509.06306", "authors": ["Zhang Jing", "Pu Nan", "Xie Yu Xiang", "Guo Yanming", "Lu Qianqi", "Zou Shiwei", "Yan Jie", "Chen Yan"], "title": "Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning", "comment": null, "summary": "Generalized Category Discovery (GCD) is an emerging and challenging\nopen-world problem that has garnered increasing attention in recent years. Most\nexisting GCD methods focus on discovering categories in static images. However,\nrelying solely on static visual content is often insufficient to reliably\ndiscover novel categories. To bridge this gap, we extend the GCD problem to the\nvideo domain and introduce a new setting, termed Video-GCD. Thus, effectively\nintegrating multi-perspective information across time is crucial for accurate\nVideo-GCD. To tackle this challenge, we propose a novel Memory-guided\nConsistency-aware Contrastive Learning (MCCL) framework, which explicitly\ncaptures temporal-spatial cues and incorporates them into contrastive learning\nthrough a consistency-guided voting mechanism. MCCL consists of two core\ncomponents: Consistency-Aware Contrastive Learning(CACL) and Memory-Guided\nRepresentation Enhancement (MGRE). CACL exploits multiperspective temporal\nfeatures to estimate consistency scores between unlabeled instances, which are\nthen used to weight the contrastive loss accordingly. MGRE introduces a\ndual-level memory buffer that maintains both feature-level and logit-level\nrepresentations, providing global context to enhance intra-class compactness\nand inter-class separability. This in turn refines the consistency estimation\nin CACL, forming a mutually reinforcing feedback loop between representation\nlearning and consistency modeling. To facilitate a comprehensive evaluation, we\nconstruct a new and challenging Video-GCD benchmark, which includes action\nrecognition and bird classification video datasets. Extensive experiments\ndemonstrate that our method significantly outperforms competitive GCD\napproaches adapted from image-based settings, highlighting the importance of\ntemporal information for discovering novel categories in videos. The code will\nbe publicly available.", "AI": {"tldr": "本文将广义类别发现（GCD）问题扩展到视频领域，提出了一种新的视频广义类别发现（Video-GCD）设置，并开发了记忆引导一致性感知对比学习（MCCL）框架，有效利用时空线索和多视角信息，显著优于现有图像方法。", "motivation": "大多数现有广义类别发现（GCD）方法仅关注静态图像，这不足以可靠地发现新类别。为了弥补这一不足，研究者将GCD问题扩展到视频领域，并引入Video-GCD新设置，旨在有效整合跨时间的多视角信息以实现准确的类别发现。", "method": "本文提出了一种名为记忆引导一致性感知对比学习（MCCL）的框架。MCCL包含两个核心组件：一致性感知对比学习（CACL）和记忆引导表示增强（MGRE）。CACL利用多视角时间特征来估计未标记实例之间的一致性分数，并以此加权对比损失。MGRE引入双层记忆缓冲区（特征级和logit级），提供全局上下文以增强类内紧凑性和类间可分离性，从而反过来细化CACL中的一致性估计，形成一个相互强化的反馈循环。此外，研究者构建了一个新的Video-GCD基准。", "result": "广泛的实验表明，本文提出的方法显著优于从基于图像的设置改编而来的竞争性GCD方法。这强调了时间信息对于在视频中发现新类别的重要性。", "conclusion": "MCCL框架通过显式捕获时空线索和多视角信息，并将其整合到对比学习中，有效解决了Video-GCD问题，实现了卓越的性能，证明了时间信息在视频类别发现中的关键作用。"}}
{"id": "2509.06321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06321", "abs": "https://arxiv.org/abs/2509.06321", "authors": ["Mengcheng Lan", "Chaofeng Chen", "Jiaxing Xu", "Zongrui Li", "Yiping Ke", "Xudong Jiang", "Yingchen Yu", "Yunqing Zhao", "Song Bai"], "title": "Text4Seg++: Advancing Image Segmentation via Generative Language Modeling", "comment": "Extended version of our conference paper arXiv:2410.09855", "summary": "Multimodal Large Language Models (MLLMs) have shown exceptional capabilities\nin vision-language tasks. However, effectively integrating image segmentation\ninto these models remains a significant challenge. In this work, we propose a\nnovel text-as-mask paradigm that casts image segmentation as a text generation\nproblem, eliminating the need for additional decoders and significantly\nsimplifying the segmentation process. Our key innovation is semantic\ndescriptors, a new textual representation of segmentation masks where each\nimage patch is mapped to its corresponding text label. We first introduce\nimage-wise semantic descriptors, a patch-aligned textual representation of\nsegmentation masks that integrates naturally into the language modeling\npipeline. To enhance efficiency, we introduce the Row-wise Run-Length Encoding\n(R-RLE), which compresses redundant text sequences, reducing the length of\nsemantic descriptors by 74% and accelerating inference by $3\\times$, without\ncompromising performance. Building upon this, our initial framework Text4Seg\nachieves strong segmentation performance across a wide range of vision tasks.\nTo further improve granularity and compactness, we propose box-wise semantic\ndescriptors, which localizes regions of interest using bounding boxes and\nrepresents region masks via structured mask tokens called semantic bricks. This\nleads to our refined model, Text4Seg++, which formulates segmentation as a\nnext-brick prediction task, combining precision, scalability, and generative\nefficiency. Comprehensive experiments on natural and remote sensing datasets\nshow that Text4Seg++ consistently outperforms state-of-the-art models across\ndiverse benchmarks without any task-specific fine-tuning, while remaining\ncompatible with existing MLLM backbones. Our work highlights the effectiveness,\nscalability, and generalizability of text-driven image segmentation within the\nMLLM framework.", "AI": {"tldr": "该研究提出了一种“文本即掩码”范式，将图像分割转化为文本生成问题，通过语义描述符（将图像块映射为文本标签）实现，并引入R-RLE和语义砖块进一步优化，最终模型Text4Seg++在多模态大语言模型（MLLM）框架下实现了领先的分割性能。", "motivation": "多模态大语言模型（MLLMs）在视觉-语言任务中表现出色，但将图像分割有效整合到这些模型中仍是一个重大挑战，通常需要额外的解码器。", "method": "研究提出了一种新颖的“文本即掩码”范式，将图像分割视为文本生成问题。核心创新是语义描述符，这是一种将每个图像块映射到其对应文本标签的文本表示。首先引入图像级语义描述符，并利用行级行程编码（R-RLE）压缩冗余文本序列以提高效率。在此基础上，提出了初始框架Text4Seg。为进一步提高粒度和紧凑性，引入了边界框级语义描述符，通过边界框定位感兴趣区域，并使用结构化掩码标记“语义砖块”表示区域掩码，从而形成了改进模型Text4Seg++。", "result": "R-RLE将语义描述符的长度减少了74%，并将推理速度提高了3倍，同时不影响性能。Text4Seg在广泛的视觉任务中实现了强大的分割性能。Text4Seg++在自然和遥感数据集上的综合实验表明，它在各种基准测试中始终优于现有最先进的模型，无需任何任务特定的微调，并且与现有MLLM骨干兼容。", "conclusion": "该工作突出了在多模态大语言模型（MLLM）框架内，文本驱动的图像分割方法的有效性、可扩展性和泛化性。"}}
{"id": "2509.06329", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.06329", "abs": "https://arxiv.org/abs/2509.06329", "authors": ["Ruiming Du", "Guangxun Zhai", "Tian Qiu", "Yu Jiang"], "title": "Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap", "comment": null, "summary": "The precise characterization of plant morphology provides valuable insights\ninto plant environment interactions and genetic evolution. A key technology for\nextracting this information is 3D segmentation, which delineates individual\nplant organs from complex point clouds. Despite significant progress in general\n3D computer vision domains, the adoption of 3D segmentation for plant\nphenotyping remains limited by three major challenges: i) the scarcity of\nlarge-scale annotated datasets, ii) technical difficulties in adapting advanced\ndeep neural networks to plant point clouds, and iii) the lack of standardized\nbenchmarks and evaluation protocols tailored to plant science. This review\nsystematically addresses these barriers by: i) providing an overview of\nexisting 3D plant datasets in the context of general 3D segmentation domains,\nii) systematically summarizing deep learning-based methods for point cloud\nsemantic and instance segmentation, iii) introducing Plant Segmentation Studio\n(PSS), an open-source framework for reproducible benchmarking, and iv)\nconducting extensive quantitative experiments to evaluate representative\nnetworks and sim-to-real learning strategies. Our findings highlight the\nefficacy of sparse convolutional backbones and transformer-based instance\nsegmentation, while also emphasizing the complementary role of modeling-based\nand augmentation-based synthetic data generation for sim-to-real learning in\nreducing annotation demands. In general, this study bridges the gap between\nalgorithmic advances and practical deployment, providing immediate tools for\nresearchers and a roadmap for developing data-efficient and generalizable deep\nlearning solutions in 3D plant phenotyping. Data and code are available at\nhttps://github.com/perrydoremi/PlantSegStudio.", "AI": {"tldr": "这篇综述系统性地解决了3D植物点云分割在数据、方法和基准方面的挑战，介绍了Plant Segmentation Studio (PSS)框架，评估了代表性网络和sim-to-real学习策略，并为3D植物表型分析提供了实用工具和发展路线图。", "motivation": "植物形态的精确表征对理解植物与环境的互作及遗传进化至关重要，而3D分割是提取这些信息的关键技术。然而，3D植物表型分析中的3D分割面临三大挑战：缺乏大规模标注数据集、难以将先进深度神经网络应用于植物点云、以及缺乏标准化基准和评估协议。", "method": "本研究通过以下方式解决上述障碍：1) 概述现有3D植物数据集；2) 系统总结基于深度学习的点云语义和实例分割方法；3) 引入开源框架Plant Segmentation Studio (PSS)用于可复现的基准测试；4) 进行广泛的定量实验，评估代表性网络和sim-to-real学习策略。", "result": "研究结果表明稀疏卷积骨干网络和基于Transformer的实例分割是有效的。同时，强调了基于模型和基于增强的合成数据生成（用于sim-to-real学习）在减少标注需求方面的互补作用。", "conclusion": "本研究弥合了算法进展与实际部署之间的鸿沟，为研究人员提供了即时工具，并为开发3D植物表型分析中数据高效且泛化性强的深度学习解决方案提供了路线图。"}}
{"id": "2509.06331", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06331", "abs": "https://arxiv.org/abs/2509.06331", "authors": ["Md Sultanul Islam Ovi", "Mainul Hossain", "Md Badsha Biswas"], "title": "Quantitative Currency Evaluation in Low-Resource Settings through Pattern Analysis to Assist Visually Impaired Users", "comment": "10 Pages, 9 Figures, 5 Tables", "summary": "Currency recognition systems often overlook usability and authenticity\nassessment, especially in low-resource environments where visually impaired\nusers and offline validation are common. While existing methods focus on\ndenomination classification, they typically ignore physical degradation and\nforgery, limiting their applicability in real-world conditions. This paper\npresents a unified framework for currency evaluation that integrates three\nmodules: denomination classification using lightweight CNN models, damage\nquantification through a novel Unified Currency Damage Index (UCDI), and\ncounterfeit detection using feature-based template matching. The dataset\nconsists of over 82,000 annotated images spanning clean, damaged, and\ncounterfeit notes. Our Custom_CNN model achieves high classification\nperformance with low parameter count. The UCDI metric provides a continuous\nusability score based on binary mask loss, chromatic distortion, and structural\nfeature loss. The counterfeit detection module demonstrates reliable\nidentification of forged notes across varied imaging conditions. The framework\nsupports real-time, on-device inference and addresses key deployment challenges\nin constrained environments. Results show that accurate, interpretable, and\ncompact solutions can support inclusive currency evaluation in practical\nsettings.", "AI": {"tldr": "本文提出了一个统一的货币评估框架，集成了面额分类、损伤量化和伪钞检测功能，适用于低资源环境下的实时、设备端部署。", "motivation": "现有货币识别系统常忽略可用性和真实性评估，特别是在视觉障碍用户和离线验证常见的低资源环境中，且未充分考虑物理磨损和伪造问题。", "method": "该框架包含三个模块：使用轻量级CNN模型（如Custom_CNN）进行面额分类；通过新颖的统一货币损伤指数（UCDI，基于二值掩码损失、色度失真和结构特征损失）进行损伤量化；以及使用基于特征的模板匹配进行伪钞检测。数据集包含超过82,000张带标注的清洁、受损和伪造钞票图像，支持实时、设备端推理。", "result": "Custom_CNN模型以低参数量实现了高分类性能；UCDI指标提供了连续的可用性评分；伪钞检测模块在不同成像条件下均能可靠识别伪造钞票。该框架支持实时、设备端推理，并在受限环境中解决了关键部署挑战。", "conclusion": "该研究表明，准确、可解释且紧凑的解决方案能够支持在实际应用场景中实现包容性的货币评估。"}}
{"id": "2509.06335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06335", "abs": "https://arxiv.org/abs/2509.06335", "authors": ["Tz-Ying Wu", "Sharath Nittur Sridhar", "Subarna Tripathi"], "title": "Harnessing Object Grounding for Time-Sensitive Video Understanding", "comment": null, "summary": "We propose to improve the time-sensitive video understanding (TSV) capability\nof video large language models (Video-LLMs) with grounded objects (GO). We\nhypothesize that TSV tasks can benefit from GO within frames, which is\nsupported by our preliminary experiments on LITA, a state-of-the-art Video-LLM\nfor reasoning temporal localization. While augmenting prompts with textual\ndescription of these object annotations improves the performance of LITA, it\nalso introduces extra token length and susceptibility to the noise in object\nlevel information. To address this, we propose GO-Tokenizer, a lightweight\nadd-on module for Video-LLMs leveraging off-the-shelf object detectors to\nencode compact object information on the fly. Experimental results demonstrate\nthat pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and its\ncounterpart utilizing textual description of objects in the prompt. The gain\ngeneralizes across different models, datasets and video understanding tasks\nsuch as reasoning temporal localization and dense captioning.", "AI": {"tldr": "本文提出GO-Tokenizer，一个轻量级模块，通过实时编码紧凑的地面物体（GO）信息，显著提升视频大语言模型（Video-LLMs）的时间敏感视频理解（TSV）能力，优于使用文本描述的方法。", "motivation": "时间敏感视频理解（TSV）任务可以从帧内的地面物体（GO）中受益，初步实验（在LITA上）也支持这一假设。然而，通过文本描述来增强提示会增加令牌长度，并容易受到物体级别信息中噪声的影响。", "method": "为了解决文本描述的局限性，本文提出了GO-Tokenizer，这是一个轻量级的Video-LLM附加模块。它利用现成的物体检测器来实时编码紧凑的物体信息，避免了冗长的文本描述。", "result": "实验结果表明，使用GO-Tokenizer进行预训练的模型，其性能优于原始的Video-LLM以及在提示中使用物体文本描述的对应模型。这种性能提升在不同的模型、数据集和视频理解任务（如推理时间定位和密集字幕）中都具有泛化性。", "conclusion": "GO-Tokenizer通过有效且紧凑地编码地面物体信息，成功提升了Video-LLMs的时间敏感视频理解能力。它在性能和泛化性方面均优于传统的文本描述方法，为视频理解任务提供了更鲁棒的解决方案。"}}
{"id": "2509.06351", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06351", "abs": "https://arxiv.org/abs/2509.06351", "authors": ["Krithik Ramesh", "Ritvik Koneru"], "title": "A Multi-Modal Deep Learning Framework for Colorectal Pathology Diagnosis: Integrating Histological and Colonoscopy Data in a Pilot Study", "comment": null, "summary": "Colorectal diseases, including inflammatory conditions and neoplasms, require\nquick, accurate care to be effectively treated. Traditional diagnostic\npipelines require extensive preparation and rely on separate, individual\nevaluations on histological images and colonoscopy footage, introducing\npossible variability and inefficiencies. This pilot study proposes a unified\ndeep learning network that uses convolutional neural networks (CN N s) to\nclassify both histopathological slides and colonoscopy video frames in one\npipeline. The pipeline integrates class-balancing learning, robust\naugmentation, and calibration methods to ensure accurate results. Static colon\nhistology images were taken from the PathMNIST dataset, and the lower\ngastrointestinal (colonoscopy) videos were drawn from the HyperKvasir dataset.\nThe CNN architecture used was ResNet-50. This study demonstrates an\ninterpretable and reproducible diagnostic pipeline that unifies multiple\ndiagnostic modalities to advance and ease the detection of colorectal diseases.", "AI": {"tldr": "本研究提出并验证了一个统一的深度学习诊断流程，该流程使用卷积神经网络（ResNet-50）同时对结肠组织病理图像和结肠镜视频帧进行分类，以提高结直肠疾病的诊断效率和准确性。", "motivation": "结直肠疾病需要快速准确的治疗。传统的诊断流程依赖于对组织学图像和结肠镜视频的独立评估，这导致了潜在的变异性和低效率，且需要大量准备。", "method": "本研究提出了一个统一的深度学习网络，该网络利用卷积神经网络（CNNs，具体为ResNet-50）在一个流程中对组织病理学切片和结肠镜视频帧进行分类。该流程整合了类别平衡学习、鲁棒数据增强和校准方法以确保结果的准确性。组织学图像取自PathMNIST数据集，结肠镜视频取自HyperKvasir数据集。", "result": "本研究成功展示了一个可解释且可重现的诊断流程，该流程统一了多种诊断模式，从而能够推进并简化结直肠疾病的检测。", "conclusion": "通过统一组织病理学和结肠镜视频分析，该深度学习诊断流程有望显著提升结直肠疾病诊断的效率、准确性和便捷性，克服传统方法的局限性。"}}
{"id": "2509.06387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06387", "abs": "https://arxiv.org/abs/2509.06387", "authors": ["Dongsik Yoon", "Jongeun Kim"], "title": "Your Super Resolution Model is not Enough for Tackling Real-World Scenarios", "comment": "To appear in Workshop on Efficient Computing under Limited Resources:\n  Visual Computing (ICCV 2025)", "summary": "Despite remarkable progress in Single Image Super-Resolution (SISR),\ntraditional models often struggle to generalize across varying scale factors,\nlimiting their real-world applicability. To address this, we propose a plug-in\nScale-Aware Attention Module (SAAM) designed to retrofit modern fixed-scale SR\nmodels with the ability to perform arbitrary-scale SR. SAAM employs\nlightweight, scale-adaptive feature extraction and upsampling, incorporating\nthe Simple parameter-free Attention Module (SimAM) for efficient guidance and\ngradient variance loss to enhance sharpness in image details. Our method\nintegrates seamlessly into multiple state-of-the-art SR backbones (e.g., SCNet,\nHiT-SR, OverNet), delivering competitive or superior performance across a wide\nrange of integer and non-integer scale factors. Extensive experiments on\nbenchmark datasets demonstrate that our approach enables robust multi-scale\nupscaling with minimal computational overhead, offering a practical solution\nfor real-world scenarios.", "AI": {"tldr": "本文提出了一种可插拔的尺度感知注意力模块（SAAM），旨在使现有固定尺度超分辨率模型能够执行任意尺度超分辨率，从而提高其泛化能力和实际应用性。", "motivation": "传统的单图像超分辨率（SISR）模型难以泛化到不同的尺度因子，限制了它们在现实世界中的应用。", "method": "SAAM模块采用轻量级、尺度自适应的特征提取和上采样，并结合了简单的无参数注意力模块（SimAM）以实现高效指导。此外，还引入了梯度方差损失来增强图像细节的清晰度。该方法可以无缝集成到多个最先进的超分辨率骨干网络中。", "result": "实验表明，SAAM在各种整数和非整数尺度因子下，都能在基准数据集上提供具有竞争力或更优的性能。该方法实现了鲁棒的多尺度上采样，且计算开销极小。", "conclusion": "SAAM为现实世界的任意尺度超分辨率场景提供了一个实用的解决方案，有效解决了传统模型泛化能力不足的问题。"}}
{"id": "2509.06396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06396", "abs": "https://arxiv.org/abs/2509.06396", "authors": ["Lorenz Achim Kuhn", "Daniel Abler", "Jonas Richiardi", "Andreas F. Hottinger", "Luis Schiappacasse", "Vincent Dunet", "Adrien Depeursinge", "Vincent Andrearczyk"], "title": "AI-based response assessment and prediction in longitudinal imaging for brain metastases treated with stereotactic radiosurgery", "comment": "Submitted and Accepted to the Learning with longitudinal medical\n  Images and Data workshop at the MICCAI 2025 Conference", "summary": "Brain Metastases (BM) are a large contributor to mortality of patients with\ncancer. They are treated with Stereotactic Radiosurgery (SRS) and monitored\nwith Magnetic Resonance Imaging (MRI) at regular follow-up intervals according\nto treatment guidelines. Analyzing and quantifying this longitudinal imaging\nrepresents an intractable workload for clinicians. As a result, follow-up\nimages are not annotated and merely assessed by observation. Response to\ntreatment in longitudinal imaging is being studied, to better understand growth\ntrajectories and ultimately predict treatment success or toxicity as early as\npossible. In this study, we implement an automated pipeline to curate a large\nlongitudinal dataset of SRS treatment data, resulting in a cohort of 896 BMs in\n177 patients who were monitored for >360 days at approximately two-month\nintervals at Lausanne University Hospital (CHUV). We use a data-driven\nclustering to identify characteristic trajectories. In addition, we predict 12\nmonths lesion-level response using classical as well as graph machine learning\nGraph Machine Learning (GML). Clustering revealed 5 dominant growth\ntrajectories with distinct final response categories. Response prediction\nreaches up to 0.90 AUC (CI95%=0.88-0.92) using only pre-treatment and first\nfollow-up MRI with gradient boosting. Similarly, robust predictive performance\nof up to 0.88 AUC (CI95%=0.86-0.90) was obtained using GML, offering more\nflexibility with a single model for multiple input time-points configurations.\nOur results suggest potential automation and increased precision for the\ncomprehensive assessment and prediction of BM response to SRS in longitudinal\nMRI. The proposed pipeline facilitates scalable data curation for the\ninvestigation of BM growth patterns, and lays the foundation for clinical\ndecision support systems aiming at optimizing personalized care.", "AI": {"tldr": "本研究开发了一个自动化流程来整理脑转移瘤(BM)的纵向影像数据，通过数据驱动的聚类识别了五种生长轨迹，并使用经典机器学习和图机器学习成功预测了立体定向放射外科(SRS)治疗后的病灶反应，准确率高达0.90 AUC。", "motivation": "脑转移瘤是癌症患者死亡率的重要原因。虽然立体定向放射外科(SRS)是主要治疗方法，但对治疗后定期MRI随访影像的分析和量化对临床医生来说是巨大的工作量，导致随访图像通常只进行观察性评估。因此，迫切需要更好地理解生长轨迹，并尽早预测治疗成功或毒性。", "method": "研究实施了一个自动化流程来整理大型纵向SRS治疗数据集，构建了一个包含177名患者中896个BMs的队列，这些患者在超过360天的时间内以大约两个月的间隔进行监测。方法包括：1) 使用数据驱动的聚类来识别特征性生长轨迹；2) 使用经典机器学习（如梯度提升）和图机器学习(GML)来预测12个月的病灶层面反应，其中经典机器学习使用治疗前和首次随访MRI数据，GML则能灵活处理多个输入时间点配置。", "result": "聚类分析揭示了5种主要的生长轨迹，它们具有不同的最终反应类别。使用治疗前和首次随访MRI数据，梯度提升模型预测反应的AUC高达0.90 (95%置信区间=0.88-0.92)。图机器学习(GML)也取得了高达0.88 AUC (95%置信区间=0.86-0.90)的稳健预测性能，并为多时间点输入配置提供了更大的灵活性。", "conclusion": "研究结果表明，在纵向MRI中自动化和提高对SRS治疗后脑转移瘤反应评估和预测的精度是可行的。所提出的流程有助于可扩展的数据整理，用于研究脑转移瘤的生长模式，并为旨在优化个性化护理的临床决策支持系统奠定了基础。"}}
{"id": "2509.06400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06400", "abs": "https://arxiv.org/abs/2509.06400", "authors": ["Matthieu Gendrin", "Stéphane Pateux", "Théo Ladune"], "title": "3DOF+Quantization: 3DGS quantization for large scenes with limited Degrees of Freedom", "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a major breakthrough in 3D scene\nreconstruction. With a number of views of a given object or scene, the\nalgorithm trains a model composed of 3D gaussians, which enables the production\nof novel views from arbitrary points of view. This freedom of movement is\nreferred to as 6DoF for 6 degrees of freedom: a view is produced for any\nposition (3 degrees), orientation of camera (3 other degrees). On large scenes,\nthough, the input views are acquired from a limited zone in space, and the\nreconstruction is valuable for novel views from the same zone, even if the\nscene itself is almost unlimited in size. We refer to this particular case as\n3DoF+, meaning that the 3 degrees of freedom of camera position are limited to\nsmall offsets around the central position. Considering the problem of\ncoordinate quantization, the impact of position error on the projection error\nin pixels is studied. It is shown that the projection error is proportional to\nthe squared inverse distance of the point being projected. Consequently, a new\nquantization scheme based on spherical coordinates is proposed. Rate-distortion\nperformance of the proposed method are illustrated on the well-known Garden\nscene.", "AI": {"tldr": "本文研究了3D Gaussian Splatting (3DGS) 在大场景“3DoF+”受限自由度下的坐标量化问题，分析了位置误差对投影误差的影响，并提出了一种基于球坐标的新量化方案。", "motivation": "3DGS在生成新视角方面取得了突破，但对于大场景，输入视图通常来自有限区域，导致相机位置自由度受限（3DoF+）。在这种情况下，坐标量化问题及其对像素投影误差的影响是一个值得关注的问题。", "method": "研究了坐标量化中位置误差对像素投影误差的影响，发现投影误差与被投影点距离的平方反比成正比。基于此分析，提出了一种新的基于球坐标的量化方案。", "result": "研究表明，投影误差与被投影点距离的平方反比成正比。所提出的基于球坐标的量化方法在著名的Garden场景上展示了其率失真性能。", "conclusion": "在3DGS的3DoF+场景中，坐标量化对投影误差有显著影响，且误差与距离平方反比相关。提出的球坐标量化方案能有效解决此问题，提升性能。"}}
{"id": "2509.06422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06422", "abs": "https://arxiv.org/abs/2509.06422", "authors": ["Hua Zhang", "Changjiang Luo", "Ruoyu Chen"], "title": "Phantom-Insight: Adaptive Multi-cue Fusion for Video Camouflaged Object Detection with Multimodal LLM", "comment": null, "summary": "Video camouflaged object detection (VCOD) is challenging due to dynamic\nenvironments. Existing methods face two main issues: (1) SAM-based methods\nstruggle to separate camouflaged object edges due to model freezing, and (2)\nMLLM-based methods suffer from poor object separability as large language\nmodels merge foreground and background. To address these issues, we propose a\nnovel VCOD method based on SAM and MLLM, called Phantom-Insight. To enhance the\nseparability of object edge details, we represent video sequences with temporal\nand spatial clues and perform feature fusion via LLM to increase information\ndensity. Next, multiple cues are generated through the dynamic foreground\nvisual token scoring module and the prompt network to adaptively guide and\nfine-tune the SAM model, enabling it to adapt to subtle textures. To enhance\nthe separability of objects and background, we propose a decoupled\nforeground-background learning strategy. By generating foreground and\nbackground cues separately and performing decoupled training, the visual token\ncan effectively integrate foreground and background information independently,\nenabling SAM to more accurately segment camouflaged objects in the video.\nExperiments on the MoCA-Mask dataset show that Phantom-Insight achieves\nstate-of-the-art performance across various metrics. Additionally, its ability\nto detect unseen camouflaged objects on the CAD2016 dataset highlights its\nstrong generalization ability.", "AI": {"tldr": "本文提出了一种名为Phantom-Insight的视频伪装目标检测（VCOD）方法，结合了SAM和MLLM的优势，通过增强边缘细节分离和解耦前景-背景学习来解决现有方法的局限性，实现了最先进的性能和强大的泛化能力。", "motivation": "现有VCOD方法在动态环境中面临挑战：基于SAM的方法因模型冻结难以分离伪装目标边缘；基于MLLM的方法因大语言模型合并前景和背景而导致目标可分离性差。本研究旨在解决这些问题。", "method": "提出Phantom-Insight方法，结合SAM和MLLM。为增强边缘细节可分离性，利用时空线索表示视频序列并通过LLM进行特征融合；通过动态前景视觉token评分模块和提示网络自适应指导和微调SAM。为增强前景-背景可分离性，提出解耦的前景-背景学习策略，分别生成前景和背景线索并进行解耦训练，使视觉token独立整合信息。", "result": "Phantom-Insight在MoCA-Mask数据集上取得了最先进的性能，并在CAD2016数据集上对未见过的伪装目标展现出强大的泛化能力。", "conclusion": "Phantom-Insight通过创新的特征融合和解耦学习策略，有效提升了SAM和MLLM在视频伪装目标检测中的表现，解决了边缘分离和前景-背景区分的难题，达到了卓越的性能和泛化能力。"}}
{"id": "2509.06427", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06427", "abs": "https://arxiv.org/abs/2509.06427", "authors": ["Rabin Dulal", "Lihong Zheng", "Muhammad Ashad Kabir"], "title": "When Language Model Guides Vision: Grounding DINO for Cattle Muzzle Detection", "comment": null, "summary": "Muzzle patterns are among the most effective biometric traits for cattle\nidentification. Fast and accurate detection of the muzzle region as the region\nof interest is critical to automatic visual cattle identification.. Earlier\napproaches relied on manual detection, which is labor-intensive and\ninconsistent. Recently, automated methods using supervised models like YOLO\nhave become popular for muzzle detection. Although effective, these methods\nrequire extensive annotated datasets and tend to be trained data-dependent,\nlimiting their performance on new or unseen cattle. To address these\nlimitations, this study proposes a zero-shot muzzle detection framework based\non Grounding DINO, a vision-language model capable of detecting muzzles without\nany task-specific training or annotated data. This approach leverages natural\nlanguage prompts to guide detection, enabling scalable and flexible muzzle\nlocalization across diverse breeds and environments. Our model achieves a mean\nAverage Precision (mAP)@0.5 of 76.8\\%, demonstrating promising performance\nwithout requiring annotated data. To our knowledge, this is the first research\nto provide a real-world, industry-oriented, and annotation-free solution for\ncattle muzzle detection. The framework offers a practical alternative to\nsupervised methods, promising improved adaptability and ease of deployment in\nlivestock monitoring applications.", "AI": {"tldr": "本研究提出了一种基于Grounding DINO的零样本牛鼻纹检测框架，无需标注数据即可实现高效检测，并在实际应用中表现出良好性能。", "motivation": "牛鼻纹是有效的生物识别特征。现有牛鼻纹检测方法存在局限性：手动检测费时费力且不一致；基于YOLO等监督模型的自动化方法需要大量标注数据，且对训练数据依赖性强，在新或未见过的牛只上性能受限。", "method": "本研究提出了一种基于Grounding DINO（一种视觉-语言模型）的零样本牛鼻纹检测框架。该方法利用自然语言提示引导检测，无需任何特定任务训练或标注数据，从而实现跨不同品种和环境的牛鼻纹定位。", "result": "该模型在无需标注数据的情况下，取得了76.8%的mAP@0.5（平均精度），展现出良好的性能。据作者所知，这是首个为牛鼻纹检测提供真实世界、面向行业且无需标注解决方案的研究。", "conclusion": "该框架为监督方法提供了一种实用的替代方案，有望提高牲畜监测应用的适应性和部署便利性。"}}
{"id": "2509.06456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06456", "abs": "https://arxiv.org/abs/2509.06456", "authors": ["Zongyi Xu", "Zhongpeng Lang", "Yilong Chen", "Shanshan Zhao", "Xiaoshui Huang", "Yifan Zuo", "Yan Zhang", "Qianni Zhang", "Xinbo Gao"], "title": "Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark", "comment": null, "summary": "Cross-source point cloud registration, which aims to align point cloud data\nfrom different sensors, is a fundamental task in 3D vision. However, compared\nto the same-source point cloud registration, cross-source registration faces\ntwo core challenges: the lack of publicly available large-scale real-world\ndatasets for training the deep registration models, and the inherent\ndifferences in point clouds captured by multiple sensors. The diverse patterns\ninduced by the sensors pose great challenges in robust and accurate point cloud\nfeature extraction and matching, which negatively influence the registration\naccuracy. To advance research in this field, we construct Cross3DReg, the\ncurrently largest and real-world multi-modal cross-source point cloud\nregistration dataset, which is collected by a rotating mechanical lidar and a\nhybrid semi-solid-state lidar, respectively. Moreover, we design an\noverlap-based cross-source registration framework, which utilizes unaligned\nimages to predict the overlapping region between source and target point\nclouds, effectively filtering out redundant points in the irrelevant regions\nand significantly mitigating the interference caused by noise in\nnon-overlapping areas. Then, a visual-geometric attention guided matching\nmodule is proposed to enhance the consistency of cross-source point cloud\nfeatures by fusing image and geometric information to establish reliable\ncorrespondences and ultimately achieve accurate and robust registration.\nExtensive experiments show that our method achieves state-of-the-art\nregistration performance. Our framework reduces the relative rotation error\n(RRE) and relative translation error (RTE) by $63.2\\%$ and $40.2\\%$,\nrespectively, and improves the registration recall (RR) by $5.4\\%$, which\nvalidates its effectiveness in achieving accurate cross-source registration.", "AI": {"tldr": "该研究构建了目前最大的真实世界多模态跨源点云配准数据集Cross3DReg，并提出了一种基于重叠区域预测和视觉-几何注意力引导匹配的跨源点云配准框架，显著提升了配准精度和鲁棒性。", "motivation": "跨源点云配准面临两大核心挑战：缺乏用于训练深度配准模型的公开大规模真实世界数据集，以及不同传感器捕获的点云固有的差异性，这些差异导致特征提取和匹配困难，影响配准精度。", "method": "1. 构建了Cross3DReg数据集，这是目前最大的真实世界多模态跨源点云配准数据集，由旋转机械激光雷达和混合固态激光雷达采集。2. 设计了一个基于重叠区域的跨源配准框架，利用未对齐图像预测点云重叠区域，过滤冗余点，减少非重叠区域噪声干扰。3. 提出了一个视觉-几何注意力引导匹配模块，融合图像和几何信息，增强跨源点云特征的一致性，建立可靠对应关系。", "result": "实验结果表明，所提出的方法实现了最先进的配准性能。相对旋转误差（RRE）降低了63.2%，相对平移误差（RTE）降低了40.2%，配准召回率（RR）提高了5.4%。", "conclusion": "该研究通过构建大规模真实世界数据集和提出创新的重叠区域预测与视觉-几何注意力引导匹配框架，有效解决了跨源点云配准中的核心挑战，实现了准确鲁棒的跨源配准，验证了其方法的有效性。"}}
{"id": "2509.06459", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06459", "abs": "https://arxiv.org/abs/2509.06459", "authors": ["Sebastian-Vasile Echim", "Andrei-Alexandru Preda", "Dumitru-Clementin Cercel", "Florin Pop"], "title": "IGAff: Benchmarking Adversarial Iterative and Genetic Affine Algorithms on Deep Neural Networks", "comment": "10 pages, 7 figures, Accepted at ECAI 2025 (28th European Conference\n  on Artificial Intelligence)", "summary": "Deep neural networks currently dominate many fields of the artificial\nintelligence landscape, achieving state-of-the-art results on numerous tasks\nwhile remaining hard to understand and exhibiting surprising weaknesses. An\nactive area of research focuses on adversarial attacks, which aim to generate\ninputs that uncover these weaknesses. However, this proves challenging,\nespecially in the black-box scenario where model details are inaccessible. This\npaper explores in detail the impact of such adversarial algorithms on\nResNet-18, DenseNet-121, Swin Transformer V2, and Vision Transformer network\narchitectures. Leveraging the Tiny ImageNet, Caltech-256, and Food-101\ndatasets, we benchmark two novel black-box iterative adversarial algorithms\nbased on affine transformations and genetic algorithms: 1) Affine\nTransformation Attack (ATA), an iterative algorithm maximizing our attack score\nfunction using random affine transformations, and 2) Affine Genetic Attack\n(AGA), a genetic algorithm that involves random noise and affine\ntransformations. We evaluate the performance of the models in the algorithm\nparameter variation, data augmentation, and global and targeted attack\nconfigurations. We also compare our algorithms with two black-box adversarial\nalgorithms, Pixle and Square Attack. Our experiments yield better results on\nthe image classification task than similar methods in the literature, achieving\nan accuracy improvement of up to 8.82%. We provide noteworthy insights into\nsuccessful adversarial defenses and attacks at both global and targeted levels,\nand demonstrate adversarial robustness through algorithm parameter variation.", "AI": {"tldr": "本论文提出了两种新颖的黑盒迭代对抗性攻击算法（ATA和AGA），它们基于仿射变换和遗传算法，旨在揭示深度神经网络的弱点。实验结果表明，这些算法在图像分类任务上优于现有方法，准确率提升高达8.82%。", "motivation": "深度神经网络在许多AI领域表现出色，但仍难以理解且存在意外的弱点。对抗性攻击旨在生成输入以揭示这些弱点，尤其是在模型细节不可访问的黑盒场景下，这更具挑战性。", "method": "研究详细探讨了对抗性算法对ResNet-18、DenseNet-121、Swin Transformer V2和Vision Transformer架构的影响。利用Tiny ImageNet、Caltech-256和Food-101数据集，基准测试了两种基于仿射变换和遗传算法的新型黑盒迭代对抗性算法：仿射变换攻击（ATA）和仿射遗传攻击（AGA）。评估了模型在算法参数变化、数据增强以及全局和目标攻击配置下的性能，并与Pixle和Square Attack两种黑盒对抗性算法进行了比较。", "result": "实验结果显示，提出的算法在图像分类任务上取得了比现有文献中类似方法更好的结果，准确率最高提升了8.82%。论文还为全局和目标层面的成功对抗性防御和攻击提供了有价值的见解，并通过算法参数变化展示了对抗性鲁棒性。", "conclusion": "本研究提出的两种基于仿射变换和遗传算法的黑盒对抗性攻击算法（ATA和AGA）在图像分类任务中表现优异，超越了现有方法，并为理解和提升深度神经网络的对抗性鲁棒性提供了新的视角和见解。"}}
{"id": "2509.06464", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06464", "abs": "https://arxiv.org/abs/2509.06464", "authors": ["Erez Posner", "Ore Shtalrid", "Oded Erell", "Daniel Noy", "Moshe Bouhnik"], "title": "A Statistical 3D Stomach Shape Model for Anatomical Analysis", "comment": null, "summary": "Realistic and parameterized 3D models of human anatomy have become invaluable\nin research, diagnostics, and surgical planning. However, the development of\ndetailed models for internal organs, such as the stomach, has been limited by\ndata availability and methodological challenges. In this paper, we propose a\nnovel pipeline for the generation of synthetic 3D stomach models, enabling the\ncreation of anatomically diverse morphologies informed by established studies\non stomach shape variability. Using this pipeline, we construct a dataset of\nsynthetic stomachs. Building on this dataset, we develop a 3D statistical shape\nmodel of the stomach, trained to capture natural anatomical variability in a\nlow-dimensional shape space. The model is further refined using CT meshes\nderived from publicly available datasets through a semi-supervised alignment\nprocess, enhancing its ability to generalize to unseen anatomical variations.\nWe evaluated the model on a held-out test set of real stomach CT scans,\ndemonstrating robust generalization and fit accuracy. We make the statistical\nshape model along with the synthetic dataset publicly available on GitLab:\nhttps://gitlab.com/Erez.Posner/stomach_pytorch to facilitate further research.\nThis work introduces the first statistical 3D shape model of the stomach, with\napplications ranging from surgical simulation and pre-operative planning to\nmedical education and computational modeling. By combining synthetic data\ngeneration, parametric modeling, and real-world validation, our approach\nrepresents a significant advancement in organ modeling and opens new\npossibilities for personalized healthcare solutions.", "AI": {"tldr": "本文提出了一种新颖的胃部合成3D模型生成流程，并基于此构建了首个胃部3D统计形状模型。该模型通过结合合成数据和真实CT扫描数据进行半监督对齐训练，展现出强大的泛化能力和拟合精度，并已公开供研究使用。", "motivation": "尽管3D人体解剖模型在研究、诊断和手术规划中价值巨大，但由于数据可得性和方法学挑战，内部器官（如胃）的详细模型开发受到限制。", "method": "本文提出了一种生成合成3D胃部模型的新颖流程，该流程基于已有的胃部形状变异性研究。利用此流程构建了合成胃部数据集，并在此基础上开发了一个3D统计形状模型，以在低维形状空间中捕获自然的解剖变异性。模型通过半监督对齐过程，使用来自公开数据集的CT网格进行了进一步完善。", "result": "本文成功构建了首个胃部3D统计形状模型，并生成了相应的合成胃部数据集。该模型在真实胃部CT扫描的保留测试集上进行了评估，结果显示出强大的泛化能力和拟合精度。该统计形状模型和合成数据集已公开可用。", "conclusion": "这项工作首次引入了胃部的统计3D形状模型，代表了器官建模领域的重大进步。它结合了合成数据生成、参数化建模和真实世界验证，为个性化医疗解决方案开辟了新的可能性，并可应用于手术模拟、术前规划、医学教育和计算建模等领域。"}}
{"id": "2509.06467", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06467", "abs": "https://arxiv.org/abs/2509.06467", "authors": ["Che Liu", "Yinda Chen", "Haoyuan Shi", "Jinpeng Lu", "Bailiang Jian", "Jiazhen Pan", "Linghan Cai", "Jiayi Wang", "Yundi Zhang", "Jun Li", "Cosmin I. Bercea", "Cheng Ouyang", "Chen Chen", "Zhiwei Xiong", "Benedikt Wiestler", "Christian Wachinger", "Daniel Rueckert", "Wenjia Bai", "Rossella Arcucci"], "title": "Does DINOv3 Set a New Medical Vision Standard?", "comment": "Technical Report", "summary": "The advent of large-scale vision foundation models, pre-trained on diverse\nnatural images, has marked a paradigm shift in computer vision. However, how\nthe frontier vision foundation models' efficacies transfer to specialized\ndomains remains such as medical imaging remains an open question. This report\ninvestigates whether DINOv3, a state-of-the-art self-supervised vision\ntransformer (ViT) that features strong capability in dense prediction tasks,\ncan directly serve as a powerful, unified encoder for medical vision tasks\nwithout domain-specific pre-training. To answer this, we benchmark DINOv3\nacross common medical vision tasks, including 2D/3D classification and\nsegmentation on a wide range of medical imaging modalities. We systematically\nanalyze its scalability by varying model sizes and input image resolutions. Our\nfindings reveal that DINOv3 shows impressive performance and establishes a\nformidable new baseline. Remarkably, it can even outperform medical-specific\nfoundation models like BiomedCLIP and CT-Net on several tasks, despite being\ntrained solely on natural images. However, we identify clear limitations: The\nmodel's features degrade in scenarios requiring deep domain specialization,\nsuch as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),\nand Positron Emission Tomography (PET). Furthermore, we observe that DINOv3\ndoes not consistently obey scaling law in the medical domain; performance does\nnot reliably increase with larger models or finer feature resolutions, showing\ndiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3\nas a strong baseline, whose powerful visual features can serve as a robust\nprior for multiple complex medical tasks. This opens promising future\ndirections, such as leveraging its features to enforce multiview consistency in\n3D reconstruction.", "AI": {"tldr": "本研究评估了DINOv3（一种基于自然图像训练的自监督视觉Transformer）在多种医学影像任务中的直接应用效果。结果显示其表现出色，甚至在某些任务上优于医学专用模型，但在深度专业化领域和缩放规律方面存在局限性，但仍可作为强大的医学视觉任务基线。", "motivation": "大规模视觉基础模型在计算机视觉领域取得了显著进展，但其在医学影像等专业领域的有效性仍是未解之谜。本研究旨在探讨前沿视觉基础模型（如DINOv3）能否在不进行领域特定预训练的情况下，直接作为医学视觉任务的强大统一编码器。", "method": "研究人员将DINOv3在各种医学影像模态（包括2D/3D分类和分割）的常见医学视觉任务上进行了基准测试。他们还通过改变模型大小和输入图像分辨率来系统分析其可扩展性。", "result": "DINOv3展现出令人印象深刻的性能，并在某些任务上超越了BioMedCLIP和CT-Net等医学专用基础模型。然而，研究也发现其在需要深度领域专业化的场景（如全玻片病理图像、电子显微镜和正电子发射断层扫描）中特征性能会下降。此外，DINOv3在医学领域并不总是遵循缩放定律，性能不会随着模型增大或分辨率提高而可靠提升，而是表现出多样化的缩放行为。", "conclusion": "本研究确立了DINOv3作为医学视觉任务的强大基线，其强大的视觉特征可作为多种复杂医学任务的稳健先验。这为未来的研究方向（如利用其特征增强3D重建中的多视角一致性）提供了有益的启示。"}}
{"id": "2509.06482", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06482", "abs": "https://arxiv.org/abs/2509.06482", "authors": ["Zhongxiang Xie", "Shuangxi Miao", "Yuhan Jiang", "Zhewei Zhang", "Jing Yao", "Xuecao Li", "Jianxi Huang", "Pedram Ghamisi"], "title": "FSG-Net: Frequency-Spatial Synergistic Gated Network for High-Resolution Remote Sensing Change Detection", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing\n  (TGRS). 13 pages, 9 figures", "summary": "Change detection from high-resolution remote sensing images lies as a\ncornerstone of Earth observation applications, yet its efficacy is often\ncompromised by two critical challenges. First, false alarms are prevalent as\nmodels misinterpret radiometric variations from temporal shifts (e.g.,\nillumination, season) as genuine changes. Second, a non-negligible semantic gap\nbetween deep abstract features and shallow detail-rich features tends to\nobstruct their effective fusion, culminating in poorly delineated boundaries.\nTo step further in addressing these issues, we propose the Frequency-Spatial\nSynergistic Gated Network (FSG-Net), a novel paradigm that aims to\nsystematically disentangle semantic changes from nuisance variations.\nSpecifically, FSG-Net first operates in the frequency domain, where a\nDiscrepancy-Aware Wavelet Interaction Module (DAWIM) adaptively mitigates\npseudo-changes by discerningly processing different frequency components.\nSubsequently, the refined features are enhanced in the spatial domain by a\nSynergistic Temporal-Spatial Attention Module (STSAM), which amplifies the\nsaliency of genuine change regions. To finally bridge the semantic gap, a\nLightweight Gated Fusion Unit (LGFU) leverages high-level semantics to\nselectively gate and integrate crucial details from shallow layers.\nComprehensive experiments on the CDD, GZ-CD, and LEVIR-CD benchmarks validate\nthe superiority of FSG-Net, establishing a new state-of-the-art with F1-scores\nof 94.16%, 89.51%, and 91.27%, respectively. The code will be made available at\nhttps://github.com/zxXie-Air/FSG-Net after a possible publication.", "AI": {"tldr": "本文提出FSG-Net，一个新颖的频率-空间协同门控网络，用于高分辨率遥感图像变化检测，旨在系统地分离语义变化和干扰变化，解决了伪变化和语义鸿沟问题，并取得了SOTA性能。", "motivation": "高分辨率遥感图像变化检测面临两大挑战：一是模型将辐射变化（如光照、季节）误解为真实变化，导致伪警报普遍存在；二是深度抽象特征和浅层细节丰富特征之间存在显著的语义鸿沟，阻碍了有效融合，导致边界描绘不佳。", "method": "FSG-Net首先在频率域运行，通过差异感知小波交互模块（DAWIM）自适应地处理不同频率分量，以减轻伪变化。随后，在空间域通过协同时空注意力模块（STSAM）增强精炼特征，放大真实变化区域的显著性。最后，通过轻量级门控融合单元（LGFU）利用高级语义选择性地门控和整合来自浅层的重要细节，以弥合语义鸿沟。", "result": "在CDD、GZ-CD和LEVIR-CD基准测试上进行的综合实验验证了FSG-Net的优越性，F1分数分别达到94.16%、89.51%和91.27%，建立了新的最先进水平（SOTA）。", "conclusion": "FSG-Net通过在频率和空间域协同处理，有效解决了高分辨率遥感图像变化检测中的伪变化和语义鸿沟问题，并在多个基准测试中取得了显著优于现有方法的性能，达到了最先进水平。"}}
{"id": "2509.06485", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06485", "abs": "https://arxiv.org/abs/2509.06485", "authors": ["Andrea Marelli", "Alberto Foresti", "Leonardo Pesce", "Giacomo Boracchi", "Mario Grosso"], "title": "WS$^2$: Weakly Supervised Segmentation using Before-After Supervision in Waste Sorting", "comment": "10 pages, 7 figures, ICCV 2025 - Workshops The WS$^2$ dataset is\n  publicly available for download at https://zenodo.org/records/14793518, all\n  the details are reported in the supplementary material", "summary": "In industrial quality control, to visually recognize unwanted items within a\nmoving heterogeneous stream, human operators are often still indispensable.\nWaste-sorting stands as a significant example, where operators on multiple\nconveyor belts manually remove unwanted objects to select specific materials.\nTo automate this recognition problem, computer vision systems offer great\npotential in accurately identifying and segmenting unwanted items in such\nsettings. Unfortunately, considering the multitude and the variety of sorting\ntasks, fully supervised approaches are not a viable option to address this\nchallange, as they require extensive labeling efforts. Surprisingly, weakly\nsupervised alternatives that leverage the implicit supervision naturally\nprovided by the operator in his removal action are relatively unexplored. In\nthis paper, we define the concept of Before-After Supervision, illustrating how\nto train a segmentation network by leveraging only the visual differences\nbetween images acquired \\textit{before} and \\textit{after} the operator. To\npromote research in this direction, we introduce WS$^2$ (Weakly Supervised\nsegmentation for Waste-Sorting), the first multiview dataset consisting of more\nthan 11 000 high-resolution video frames captured on top of a conveyor belt,\nincluding \"before\" and \"after\" images. We also present a robust end-to-end\npipeline, used to benchmark several state-of-the-art weakly supervised\nsegmentation methods on WS$^2$.", "AI": {"tldr": "本文提出了一种名为“前后监督”的弱监督方法，通过利用操作员移除物体前后图像的视觉差异来训练分割网络，以解决工业质量控制（如垃圾分类）中的自动化识别问题，并为此引入了首个多视角数据集WS$^2$及基准测试流程。", "motivation": "在工业质量控制中，人工操作员在识别和移除异物方面（例如垃圾分类）仍然不可或缺。为了实现自动化，计算机视觉系统具有巨大潜力，但由于任务的多样性和复杂性，全监督方法需要大量的标注工作，因此不可行。现有的弱监督方法，特别是利用操作员移除动作所提供的隐式监督，尚未得到充分探索。", "method": "本文定义了“前后监督”的概念，利用操作员移除物体前后图像之间的视觉差异来训练分割网络。为促进该方向研究，作者引入了WS$^2$数据集，这是首个包含11,000多帧高分辨率视频帧的多视角数据集，其中包括“前”和“后”图像。此外，还提出了一个鲁棒的端到端管道，用于在WS$^2$数据集上对几种最先进的弱监督分割方法进行基准测试。", "result": "本文定义了“前后监督”的概念，展示了如何仅利用操作员移除前后图像的视觉差异来训练分割网络。同时，成功构建了WS$^2$数据集，这是首个用于垃圾分类弱监督分割的多视角数据集。此外，还提供了一个端到端管道，用于评估和基准测试多种弱监督分割方法。", "conclusion": "本文提出了一种新颖的弱监督分割方法——“前后监督”，有效利用了操作员行为提供的隐式监督。通过引入WS$^2$数据集和基准测试管道，为工业质量控制（尤其是垃圾分类）领域的自动化识别和分割研究提供了重要的资源和工具，有望推动该领域弱监督学习的发展。"}}
{"id": "2509.06499", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06499", "abs": "https://arxiv.org/abs/2509.06499", "authors": ["Jibai Lin", "Bo Ma", "Yating Yang", "Rong Ma", "Turghun Osman", "Ahtamjan Ahmat", "Rui Dong", "Lei Wang", "Xi Zhou"], "title": "TIDE: Achieving Balanced Subject-Driven Image Generation via Target-Instructed Diffusion Enhancement", "comment": null, "summary": "Subject-driven image generation (SDIG) aims to manipulate specific subjects\nwithin images while adhering to textual instructions, a task crucial for\nadvancing text-to-image diffusion models. SDIG requires reconciling the tension\nbetween maintaining subject identity and complying with dynamic edit\ninstructions, a challenge inadequately addressed by existing methods. In this\npaper, we introduce the Target-Instructed Diffusion Enhancing (TIDE) framework,\nwhich resolves this tension through target supervision and preference learning\nwithout test-time fine-tuning. TIDE pioneers target-supervised triplet\nalignment, modelling subject adaptation dynamics using a (reference image,\ninstruction, target images) triplet. This approach leverages the Direct Subject\nDiffusion (DSD) objective, training the model with paired \"winning\" (balanced\npreservation-compliance) and \"losing\" (distorted) targets, systematically\ngenerated and evaluated via quantitative metrics. This enables implicit reward\nmodelling for optimal preservation-compliance balance. Experimental results on\nstandard benchmarks demonstrate TIDE's superior performance in generating\nsubject-faithful outputs while maintaining instruction compliance,\noutperforming baseline methods across multiple quantitative metrics. TIDE's\nversatility is further evidenced by its successful application to diverse\ntasks, including structural-conditioned generation, image-to-image generation,\nand text-image interpolation. Our code is available at\nhttps://github.com/KomJay520/TIDE.", "AI": {"tldr": "本文提出TIDE框架，用于解决主体驱动图像生成（SDIG）中主体身份保持与文本指令遵循之间的矛盾。TIDE通过目标监督和偏好学习，在无需测试时微调的情况下，实现了卓越的生成效果和广泛的应用性。", "motivation": "主体驱动图像生成（SDIG）需要精确操作图像中的特定主体并遵循文本指令，但现有方法在保持主体身份和遵守动态编辑指令之间存在难以调和的矛盾，未能有效解决这一挑战。", "method": "TIDE框架通过目标监督和偏好学习来解决矛盾，无需测试时微调。它首创目标监督三元组对齐，使用（参考图像、指令、目标图像）三元组建模主体适应动态。通过Direct Subject Diffusion (DSD) 目标，模型使用成对的“成功”（平衡了保持与遵循）和“失败”（扭曲）目标进行训练，这些目标通过量化指标系统生成和评估，从而实现隐式奖励建模以优化平衡。", "result": "实验结果表明，TIDE在标准基准测试中表现优越，能生成忠实于主体的输出，同时保持指令依从性，在多项量化指标上优于基线方法。TIDE还成功应用于结构条件生成、图像到图像生成和文本-图像插值等多样化任务，展现了其多功能性。", "conclusion": "TIDE框架有效解决了主体驱动图像生成中主体身份保持和指令遵循之间的核心矛盾，实现了卓越的性能和广泛的任务适用性，为文本到图像扩散模型的发展提供了重要进展。"}}
{"id": "2509.06511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06511", "abs": "https://arxiv.org/abs/2509.06511", "authors": ["Daniil Tikhonov", "Matheus Scatolin", "Mohor Banerjee", "Qiankun Ji", "Ahmed Jaheen", "Mostafa Salem", "Abdelrahman Elsayed", "Hu Wang", "Sarim Hashmi", "Mohammad Yaqub"], "title": "Predicting Brain Tumor Response to Therapy using a Hybrid Deep Learning and Radiomics Approach", "comment": "Submitted to the BraTS-Lighthouse 2025 Challenge (MICCAI 2025)", "summary": "Accurate evaluation of the response of glioblastoma to therapy is crucial for\nclinical decision-making and patient management. The Response Assessment in\nNeuro-Oncology (RANO) criteria provide a standardized framework to assess\npatients' clinical response, but their application can be complex and subject\nto observer variability. This paper presents an automated method for\nclassifying the intervention response from longitudinal MRI scans, developed to\npredict tumor response during therapy as part of the BraTS 2025 challenge. We\npropose a novel hybrid framework that combines deep learning derived feature\nextraction and an extensive set of radiomics and clinically chosen features.\nOur approach utilizes a fine-tuned ResNet-18 model to extract features from 2D\nregions of interest across four MRI modalities. These deep features are then\nfused with a rich set of more than 4800 radiomic and clinically driven\nfeatures, including 3D radiomics of tumor growth and shrinkage masks,\nvolumetric changes relative to the nadir, and tumor centroid shift. Using the\nfused feature set, a CatBoost classifier achieves a mean ROC AUC of 0.81 and a\nMacro F1 score of 0.50 in the 4-class response prediction task (Complete\nResponse, Partial Response, Stable Disease, Progressive Disease). Our results\nhighlight that synergizing learned image representations with domain-targeted\nradiomic features provides a robust and effective solution for automated\ntreatment response assessment in neuro-oncology.", "AI": {"tldr": "本文提出了一种结合深度学习和放射组学特征的自动化方法，用于从纵向MRI扫描中分类胶质母细胞瘤的治疗反应，旨在解决RANO标准应用复杂且存在观察者变异性的问题。", "motivation": "胶质母细胞瘤治疗反应的准确评估对于临床决策和患者管理至关重要。现有的神经肿瘤学反应评估（RANO）标准虽然提供了标准化框架，但其应用复杂且容易受到观察者变异性的影响。", "method": "研究提出了一种新颖的混合框架：首先，利用经过微调的ResNet-18模型从四种MRI模态的2D感兴趣区域中提取深度学习特征；然后，将这些深度特征与超过4800个丰富的放射组学和临床驱动特征（包括肿瘤生长和缩小掩模的3D放射组学、相对于最低点的体积变化以及肿瘤质心位移）进行融合；最后，使用CatBoost分类器对融合后的特征集进行4类（完全缓解、部分缓解、疾病稳定、疾病进展）反应预测。", "result": "在4类反应预测任务中，该方法实现了0.81的平均ROC AUC和0.50的Macro F1分数。", "conclusion": "研究结果强调，将学习到的图像表示与领域特定的放射组学特征相结合，为神经肿瘤学中自动化治疗反应评估提供了一个稳健且有效的解决方案。"}}
{"id": "2509.06536", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06536", "abs": "https://arxiv.org/abs/2509.06536", "authors": ["Senem Aktas", "Charles Markham", "John McDonald", "Rozenn Dahyot"], "title": "Benchmarking EfficientTAM on FMO datasets", "comment": null, "summary": "Fast and tiny object tracking remains a challenge in computer vision and in\nthis paper we first introduce a JSON metadata file associated with four open\nsource datasets of Fast Moving Objects (FMOs) image sequences. In addition, we\nextend the description of the FMOs datasets with additional ground truth\ninformation in JSON format (called FMOX) with object size information. Finally\nwe use our FMOX file to test a recently proposed foundational model for\ntracking (called EfficientTAM) showing that its performance compares well with\nthe pipelines originally taylored for these FMO datasets. Our comparison of\nthese state-of-the-art techniques on FMOX is provided with Trajectory\nIntersection of Union (TIoU) scores. The code and JSON is shared open source\nallowing FMOX to be accessible and usable for other machine learning pipelines\naiming to process FMO datasets.", "AI": {"tldr": "该论文引入了FMO数据集的JSON元数据文件和扩展的FMOX地面真值信息，并使用FMOX测试了EfficientTAM跟踪模型，结果显示其性能与专门为FMO定制的管道相当。", "motivation": "快速微小物体跟踪在计算机视觉领域仍然是一个挑战。", "method": "1. 引入了与四个快速移动物体(FMOs)图像序列开源数据集相关的JSON元数据文件。2. 使用JSON格式（称为FMOX）的额外地面真值信息（包含物体尺寸）扩展了FMOs数据集的描述。3. 使用FMOX文件测试了一个最近提出的基础跟踪模型（EfficientTAM）。4. 使用轨迹交并比（TIoU）分数对这些最先进的技术在FMOX上进行了比较。5. 开源了代码和JSON文件。", "result": "EfficientTAM模型在FMOX数据集上的性能与最初为这些FMO数据集量身定制的管道表现相当。", "conclusion": "FMOX提供了一个可访问和可用的资源，用于处理FMO数据集的机器学习管道，并且EfficientTAM模型在快速移动物体跟踪方面表现出色。"}}
{"id": "2509.06566", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06566", "abs": "https://arxiv.org/abs/2509.06566", "authors": ["Emil Demić", "Luka Čehovin Zajc"], "title": "Back To The Drawing Board: Rethinking Scene-Level Sketch-Based Image Retrieval", "comment": "Accepted to BMVC2025", "summary": "The goal of Scene-level Sketch-Based Image Retrieval is to retrieve natural\nimages matching the overall semantics and spatial layout of a free-hand sketch.\nUnlike prior work focused on architectural augmentations of retrieval models,\nwe emphasize the inherent ambiguity and noise present in real-world sketches.\nThis insight motivates a training objective that is explicitly designed to be\nrobust to sketch variability. We show that with an appropriate combination of\npre-training, encoder architecture, and loss formulation, it is possible to\nachieve state-of-the-art performance without the introduction of additional\ncomplexity. Extensive experiments on a challenging FS-COCO and widely-used\nSketchyCOCO datasets confirm the effectiveness of our approach and underline\nthe critical role of training design in cross-modal retrieval tasks, as well as\nthe need to improve the evaluation scenarios of scene-level SBIR.", "AI": {"tldr": "该研究通过强调草图的内在模糊性和噪声，提出了一种鲁棒的训练目标、预训练、编码器架构和损失函数组合，在不增加模型复杂性的情况下，实现了场景级草图图像检索的最新性能。", "motivation": "以往研究侧重于检索模型的架构增强，而本研究则强调真实世界草图中固有的模糊性和噪声，这促使研究人员设计一个能有效应对草图变异性的训练目标。", "method": "通过适当结合预训练、编码器架构和损失函数，明确设计了一个对草图变异性具有鲁棒性的训练目标，以实现跨模态检索。", "result": "在具有挑战性的FS-COCO和广泛使用的SketchyCOCO数据集上，该方法在不引入额外复杂性的情况下，达到了最先进的性能，并证实了其有效性。", "conclusion": "训练设计在跨模态检索任务中扮演着关键角色，同时，场景级草图图像检索的评估场景也需要改进。"}}
{"id": "2509.06570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06570", "abs": "https://arxiv.org/abs/2509.06570", "authors": ["Runqing Yang", "Yimin Fu", "Changyuan Wu", "Zhunga Liu"], "title": "Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition", "comment": "10 pages, 6 figures, 2025 IEEE/CVF International Conference on\n  Computer Vision Workshops", "summary": "Existing open set recognition (OSR) methods are typically designed for static\nscenarios, where models aim to classify known classes and identify unknown ones\nwithin fixed scopes. This deviates from the expectation that the model should\nincrementally identify newly emerging unknown classes from continuous data\nstreams and acquire corresponding knowledge. In such evolving scenarios, the\ndiscriminability of OSR decision boundaries is hard to maintain due to\nrestricted access to former training data, causing severe inter-class\nconfusion. To solve this problem, we propose retentive angular representation\nlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknown\nrepresentations are encouraged to align around inactive prototypes within an\nangular space constructed under the equiangular tight frame, thereby mitigating\nexcessive representation drift during knowledge updates. Specifically, we adopt\na virtual-intrinsic interactive (VII) training strategy, which compacts known\nrepresentations by enforcing clear inter-class margins through\nboundary-proximal virtual classes. Furthermore, a stratified rectification\nstrategy is designed to refine decision boundaries, mitigating representation\nbias and feature space distortion caused by imbalances between old/new and\npositive/negative class samples. We conduct thorough evaluations on CIFAR100\nand TinyImageNet datasets and establish a new benchmark for IOSR. Experimental\nresults across various task setups demonstrate that the proposed method\nachieves state-of-the-art performance.", "AI": {"tldr": "针对增量开放集识别（IOSR）中决策边界难以维持和类间混淆问题，本文提出了一种保留性角度表示学习（RARL）方法，通过虚拟-内在交互训练和分层校正策略，实现了最先进的性能。", "motivation": "现有开放集识别（OSR）方法主要为静态场景设计，无法有效应对模型需从连续数据流中增量识别新未知类并获取知识的动态演进场景。在此类场景中，由于难以访问旧训练数据，OSR决策边界的判别力难以维持，导致严重的类间混淆。", "method": "本文提出了用于增量开放集识别（IOSR）的保留性角度表示学习（RARL）。RARL在等角紧框架构建的角度空间中，鼓励未知表示围绕非活跃原型对齐，从而减轻知识更新时的过度表示漂移。具体包括：1) 虚拟-内在交互（VII）训练策略，通过边界邻近的虚拟类强制清晰的类间裕度，以压缩已知表示；2) 分层校正策略，用于精炼决策边界，缓解新旧类及正负样本不平衡导致的表示偏差和特征空间扭曲。", "result": "在CIFAR100和TinyImageNet数据集上进行了全面评估，为IOSR建立了新的基准。在各种任务设置下的实验结果表明，所提出的方法达到了最先进的性能。", "conclusion": "所提出的RARL方法有效解决了增量开放集识别中决策边界难以维持和类间混淆的问题。该方法在标准数据集上取得了最先进的性能，并为IOSR领域设定了新基准。"}}
{"id": "2509.06577", "categories": ["cs.CV", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2509.06577", "abs": "https://arxiv.org/abs/2509.06577", "authors": ["Marcos Eduardo Valle", "Santiago Velasco-Forero", "Joao Batista Florindo", "Gustavo Jesus Angulo"], "title": "Approximating Condorcet Ordering for Vector-valued Mathematical Morphology", "comment": "Submitted to the 4th International Conference on Discrete Geometry\n  and Mathematical Morphology (DGMM 2025)", "summary": "Mathematical morphology provides a nonlinear framework for image and spatial\ndata processing and analysis. Although there have been many successful\napplications of mathematical morphology to vector-valued images, such as color\nand hyperspectral images, there is still no consensus on the most suitable\nvector ordering for constructing morphological operators. This paper addresses\nthis issue by examining a reduced ordering approximating the Condorcet ranking\nderived from a set of vector orderings. Inspired by voting problems, the\nCondorcet ordering ranks elements from most to least voted, with voters\nrepresenting different orderings. In this paper, we develop a machine learning\napproach that learns a reduced ordering that approximates the Condorcet\nordering. Preliminary computational experiments confirm the effectiveness of\nlearning the reduced mapping to define vector-valued morphological operators\nfor color images.", "AI": {"tldr": "本文提出一种机器学习方法，通过学习近似孔多塞排序的简化排序，来解决向量值图像数学形态学操作中缺乏一致向量排序的问题，并在彩色图像上展示了其有效性。", "motivation": "尽管数学形态学在向量值图像处理中有许多成功应用，但对于构建形态学算子最合适的向量排序方式仍没有达成共识。", "method": "受投票问题中孔多塞排序（Condorcet ranking）的启发，本文开发了一种机器学习方法。该方法学习一个简化排序，以近似由一组向量排序导出的孔多塞排序，其中不同的排序被视为“投票者”。", "result": "初步计算实验证实，学习这种简化映射对于定义彩色图像的向量值形态学算子是有效的。", "conclusion": "通过机器学习获得的近似孔多塞排序的简化排序，为定义向量值数学形态学算子提供了一种有效方法，解决了该领域缺乏标准向量排序的挑战。"}}
{"id": "2509.06579", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06579", "abs": "https://arxiv.org/abs/2509.06579", "authors": ["Xin Kong", "Daniel Watson", "Yannick Strümpler", "Michael Niemeyer", "Federico Tombari"], "title": "CausNVS: Autoregressive Multi-view Diffusion for Flexible 3D Novel View Synthesis", "comment": null, "summary": "Multi-view diffusion models have shown promise in 3D novel view synthesis,\nbut most existing methods adopt a non-autoregressive formulation. This limits\ntheir applicability in world modeling, as they only support a fixed number of\nviews and suffer from slow inference due to denoising all frames\nsimultaneously. To address these limitations, we propose CausNVS, a multi-view\ndiffusion model in an autoregressive setting, which supports arbitrary\ninput-output view configurations and generates views sequentially. We train\nCausNVS with causal masking and per-frame noise, using pairwise-relative camera\npose encodings (CaPE) for precise camera control. At inference time, we combine\na spatially-aware sliding-window with key-value caching and noise conditioning\naugmentation to mitigate drift. Our experiments demonstrate that CausNVS\nsupports a broad range of camera trajectories, enables flexible autoregressive\nnovel view synthesis, and achieves consistently strong visual quality across\ndiverse settings. Project page: https://kxhit.github.io/CausNVS.html.", "AI": {"tldr": "CausNVS是一个自回归多视角扩散模型，用于新颖视图合成。它支持任意视图配置和顺序生成，解决了现有非自回归模型的视图数量固定和推理速度慢的问题，并能生成高质量的图像。", "motivation": "大多数现有多视角扩散模型采用非自回归形式，这限制了它们在世界建模中的应用，因为它们只支持固定数量的视图，并且由于同时去噪所有帧而导致推理缓慢。", "method": "本文提出了CausNVS，一个在自回归设置下的多视角扩散模型。它通过因果掩码和逐帧噪声进行训练，并使用成对相对相机姿态编码（CaPE）进行精确相机控制。在推理时，结合空间感知滑动窗口、键值缓存和噪声条件增强来缓解漂移问题。", "result": "CausNVS支持广泛的相机轨迹，实现了灵活的自回归新颖视图合成，并在各种设置下保持一致的强大视觉质量。", "conclusion": "CausNVS成功克服了现有非自回归多视角扩散模型的局限性，提供了一种灵活且高质量的自回归新颖视图合成方法，对世界建模具有重要意义。"}}
{"id": "2509.06585", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06585", "abs": "https://arxiv.org/abs/2509.06585", "authors": ["Ritwik Kulkarni", "WU Hanqin", "Enrico Di Minin"], "title": "Detection of trade in products derived from threatened species using machine learning and a smartphone", "comment": null, "summary": "Unsustainable trade in wildlife is a major threat to biodiversity and is now\nincreasingly prevalent in digital marketplaces and social media. With the sheer\nvolume of digital content, the need for automated methods to detect wildlife\ntrade listings is growing. These methods are especially needed for the\nautomatic identification of wildlife products, such as ivory. We developed\nmachine learning-based object recognition models that can identify wildlife\nproducts within images and highlight them. The data consists of images of\nelephant, pangolin, and tiger products that were identified as being sold\nillegally or that were confiscated by authorities. Specifically, the wildlife\nproducts included elephant ivory and skins, pangolin scales, and claws (raw and\ncrafted), and tiger skins and bones. We investigated various combinations of\ntraining strategies and two loss functions to identify the best model to use in\nthe automatic detection of these wildlife products. Models were trained for\neach species while also developing a single model to identify products from all\nthree species. The best model showed an overall accuracy of 84.2% with\naccuracies of 71.1%, 90.2% and 93.5% in detecting products derived from\nelephants, pangolins, and tigers, respectively. We further demonstrate that the\nmachine learning model can be made easily available to stakeholders, such as\ngovernment authorities and law enforcement agencies, by developing a\nsmartphone-based application that had an overall accuracy of 91.3%. The\napplication can be used in real time to click images and help identify\npotentially prohibited products of target species. Thus, the proposed method is\nnot only applicable for monitoring trade on the web but can also be used e.g.\nin physical markets for monitoring wildlife trade.", "AI": {"tldr": "该研究开发了一种基于机器学习的图像识别模型和智能手机应用，用于自动检测数字平台和实体市场中的非法野生动物产品，实现了较高的准确性。", "motivation": "不可持续的野生动物贸易对生物多样性构成重大威胁，且在数字市场和社交媒体上日益猖獗。由于数字内容量巨大，急需自动化方法来检测野生动物贸易信息，特别是自动识别象牙等野生动物产品。", "method": "研究开发了基于机器学习的物体识别模型，用于识别图像中的野生动物产品。数据包括被认定非法销售或被当局没收的大象（象牙、皮）、穿山甲（鳞片、爪）和老虎（皮、骨）产品图像。研究探索了不同的训练策略和两种损失函数，以确定最佳模型。模型既为每个物种单独训练，也开发了一个能识别所有三种物种产品的单一模型。此外，还开发了一个智能手机应用程序，以便政府机构和执法部门等利益相关者使用。", "result": "最佳模型在检测野生动物产品方面整体准确率达到84.2%，其中大象产品检测准确率为71.1%，穿山甲为90.2%，老虎为93.5%。开发的智能手机应用程序整体准确率为91.3%，可用于实时图像拍摄并帮助识别潜在的违禁目标物种产品。", "conclusion": "所提出的方法不仅适用于监测网络上的贸易，还可以应用于实体市场（例如通过智能手机应用）进行野生动物贸易监测。该技术为政府部门和执法机构等利益相关者提供了有效的工具，以打击非法野生动物贸易。"}}
{"id": "2509.06591", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06591", "abs": "https://arxiv.org/abs/2509.06591", "authors": ["Yichao Liu", "YueYang Teng"], "title": "Hybrid Swin Attention Networks for Simultaneously Low-Dose PET and CT Denoising", "comment": null, "summary": "Low-dose computed tomography (LDCT) and positron emission tomography (PET)\nhave emerged as safer alternatives to conventional imaging modalities by\nsignificantly reducing radiation exposure. However, this reduction often\nresults in increased noise and artifacts, which can compromise diagnostic\naccuracy. Consequently, denoising for LDCT/PET has become a vital area of\nresearch aimed at enhancing image quality while maintaining radiation safety.\nIn this study, we introduce a novel Hybrid Swin Attention Network (HSANet),\nwhich incorporates Efficient Global Attention (EGA) modules and a hybrid\nupsampling module. The EGA modules enhance both spatial and channel-wise\ninteraction, improving the network's capacity to capture relevant features,\nwhile the hybrid upsampling module mitigates the risk of overfitting to noise.\nWe validate the proposed approach using a publicly available LDCT/PET dataset.\nExperimental results demonstrate that HSANet achieves superior denoising\nperformance compared to existing methods, while maintaining a lightweight model\nsize suitable for deployment on GPUs with standard memory configurations. This\nmakes our approach highly practical for real-world clinical applications.", "AI": {"tldr": "本研究提出了一种新颖的混合Swin注意力网络（HSANet），用于低剂量CT/PET图像去噪，该网络在实现卓越去噪性能的同时保持了轻量级模型尺寸，适用于临床应用。", "motivation": "低剂量CT/PET虽然能显著减少辐射暴露，但会增加图像噪声和伪影，从而影响诊断准确性。因此，开发有效的去噪方法以提升图像质量同时保持辐射安全成为一项重要的研究领域。", "method": "本研究引入了一种混合Swin注意力网络（HSANet），该网络整合了高效全局注意力（EGA）模块，以增强空间和通道间的交互，从而提升网络捕获相关特征的能力；同时，还结合了一个混合上采样模块，以减轻模型对噪声过拟合的风险。", "result": "实验结果表明，在公开的LDCT/PET数据集上，HSANet的去噪性能优于现有方法。此外，该模型保持了轻量级的尺寸，适用于标准内存配置的GPU部署。", "conclusion": "HSANet在低剂量CT/PET图像去噪方面表现出卓越的性能和实用性，使其成为真实临床应用中极具前景的解决方案。"}}
{"id": "2509.06685", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06685", "abs": "https://arxiv.org/abs/2509.06685", "authors": ["Shengkai Zhang", "Yuhe Liu", "Guanjun Wu", "Jianhua He", "Xinggang Wang", "Mozi Chen", "Kezhong Liu"], "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes", "comment": null, "summary": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for\nnovel-view synthesis (NVS) in large scenes. GS typically requires accurate\ndepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited\ndepth sensing range makes it difficult for GS to work in large scenes.\nMonocular images, however, lack depth to guide the learning and lead to\ninferior NVS results. Although large foundation models (LFMs) for monocular\ndepth estimation are available, they suffer from cross-frame inconsistency,\ninaccuracy for distant scenes, and ambiguity in deceptive texture cues. This\npaper aims to generate dense, accurate depth images from monocular RGB inputs\nfor high-definite GS rendering. The key idea is to leverage the accurate but\nsparse depth from visual-inertial Structure-from-Motion (SfM) to refine the\ndense but coarse depth from LFMs. To bridge the sparse input and dense output,\nwe propose an object-segmented depth propagation algorithm that renders the\ndepth of pixels of structured objects. Then we develop a dynamic depth\nrefinement module to handle the crippled SfM depth of dynamic objects and\nrefine the coarse LFM depth. Experiments using public and customized datasets\ndemonstrate the superior rendering quality of VIM-GS in large scenes.", "AI": {"tldr": "VIM-GS是一个使用单目图像在大场景中进行新视角合成（NVS）的Gaussian Splatting（GS）框架。它通过结合视觉惯性SfM的稀疏但精确深度与大型基础模型（LFM）的密集但粗糙深度来解决单目GS的深度不足问题，并引入了物体分割深度传播和动态深度细化模块。", "motivation": "GS通常需要精确深度，但RGB-D/立体相机深度范围有限，难以应用于大场景。单目图像缺乏深度指导，导致NVS结果不佳。虽然存在用于单目深度估计的LFM，但它们存在跨帧不一致、远距离场景不准确以及纹理模糊性等问题。", "method": "该方法旨在从单目RGB输入生成密集、精确的深度图像，以实现高清晰度的GS渲染。核心思想是利用视觉惯性SfM的精确但稀疏深度来细化LFM的密集但粗糙深度。为此，提出了一个物体分割深度传播算法来渲染结构化物体像素的深度，以及一个动态深度细化模块来处理动态物体的受损SfM深度并细化LFM深度。", "result": "在公共和定制数据集上的实验表明，VIM-GS在大场景中展现出卓越的渲染质量。", "conclusion": "VIM-GS成功地通过结合SfM和LFM深度，克服了单目图像在大场景GS中的深度挑战，实现了高质量的新视角合成。"}}
{"id": "2509.06693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06693", "abs": "https://arxiv.org/abs/2509.06693", "authors": ["Xichen Xu", "Yanshu Wang", "Jinbao Wang", "Qunyi Zhang", "Xiaoning Lei", "Guoyang Xie", "Guannan Jiang", "Zhichao Lu"], "title": "STAGE: Segmentation-oriented Industrial Anomaly Synthesis via Graded Diffusion with Explicit Mask Alignment", "comment": null, "summary": "Segmentation-oriented Industrial Anomaly Synthesis (SIAS) plays a pivotal\nrole in enhancing the performance of downstream anomaly segmentation, as it\nprovides an effective means of expanding abnormal data. However, existing SIAS\nmethods face several critical limitations: (i) the synthesized anomalies often\nlack intricate texture details and fail to align precisely with the surrounding\nbackground, and (ii) they struggle to generate fine-grained, pixel-level\nanomalies. To address these challenges, we propose Segmentation-oriented\nAnomaly synthesis via Graded diffusion with Explicit mask alignment, termed\nSTAGE. STAGE introduces a novel anomaly inference strategy that incorporates\nclean background information as a prior to guide the denoising distribution,\nenabling the model to more effectively distinguish and highlight abnormal\nforegrounds. Furthermore, it employs a graded diffusion framework with an\nanomaly-only branch to explicitly record local anomalies during both the\nforward and reverse processes, ensuring that subtle anomalies are not\noverlooked. Finally, STAGE incorporates the explicit mask alignment (EMA)\nstrategy to progressively align the synthesized anomalies with the background,\nresulting in context-consistent and structurally coherent generations.\nExtensive experiments on the MVTec and BTAD datasets demonstrate that STAGE\nachieves state-of-the-art performance in SIAS, which in turn enhances\ndownstream anomaly segmentation.", "AI": {"tldr": "本文提出了一种名为STAGE（Segmentation-oriented Anomaly synthesis via Graded diffusion with Explicit mask alignment）的新型工业异常合成方法，通过引入背景先验、分级扩散框架和显式掩码对齐策略，解决了现有方法在纹理细节、背景对齐和细粒度异常生成方面的不足，并在异常分割任务中取得了最先进的性能。", "motivation": "现有的分割导向工业异常合成（SIAS）方法存在局限性：(i) 合成异常缺乏精细纹理细节，且与背景对齐不精确；(ii) 难以生成细粒度、像素级的异常。这些问题限制了合成数据对下游异常分割性能的提升。", "method": "本文提出了STAGE方法，包括：1) 新颖的异常推理策略，利用干净背景信息作为先验来引导去噪分布，从而更有效地突出异常前景。2) 分级扩散框架，引入一个仅处理异常的分支，在正向和反向过程中显式记录局部异常，以确保不遗漏细微异常。3) 显式掩码对齐（EMA）策略，逐步将合成异常与背景对齐，以生成上下文一致且结构连贯的异常。", "result": "在MVTec和BTAD数据集上进行的广泛实验表明，STAGE在SIAS任务中达到了最先进的性能，并有效提升了下游异常分割的效果。", "conclusion": "STAGE通过其创新的背景先验引导、分级扩散和显式掩码对齐策略，成功克服了现有工业异常合成方法的局限性，能够生成高质量、上下文一致且细粒度的异常，从而显著增强了下游异常分割的性能。"}}
{"id": "2509.06705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06705", "abs": "https://arxiv.org/abs/2509.06705", "authors": ["Mohamed Zayaan S"], "title": "Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention", "comment": "8 pages, 4 figures", "summary": "We present Cortex Synth, a novel end-to-end differentiable framework for\njoint 3D skeleton geometry and topology synthesis from single 2D images. Our\narchitecture introduces three key innovations: (1) A hierarchical graph\nattention mechanism with multi-scale skeletal refinement, (2) Differentiable\nspectral topology optimization via Laplacian eigen decomposition, and (3)\nAdversarial geometric consistency training for pose structure alignment. The\nframework integrates four synergistic modules: a pseudo 3D point cloud\ngenerator, an enhanced PointNet encoder, a skeleton coordinate decoder, and a\nnovel Differentiable Graph Construction Network (DGCN). Our experiments\ndemonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and\n27.3 percent in Graph Edit Distance on ShapeNet, while reducing topological\nerrors by 42 percent compared to previous approaches. The model's end-to-end\ndifferentiability enables applications in robotic manipulation, medical\nimaging, and automated character rigging.", "AI": {"tldr": "Cortex Synth是一个端到端可微分框架，能够从单张2D图像联合合成3D骨架的几何形状和拓扑结构。", "motivation": "现有方法在从单张2D图像合成3D骨架的几何形状和拓扑结构方面存在局限性，需要一个更有效、更精确的端到端解决方案。", "method": "该框架引入了三项关键创新：1) 带有多尺度骨架精修的分层图注意力机制；2) 通过拉普拉斯特征分解实现的可微分谱拓扑优化；3) 用于姿态结构对齐的对抗性几何一致性训练。它集成了四个协同模块：伪3D点云生成器、增强型PointNet编码器、骨架坐标解码器和新型可微分图构建网络（DGCN）。", "result": "在ShapeNet数据集上，该模型在MPJPE方面提高了18.7%，在图编辑距离方面提高了27.3%，并将拓扑错误比现有方法减少了42%，达到了最先进的性能。", "conclusion": "该模型的端到端可微分特性使其可应用于机器人操作、医学成像和自动化角色绑定等领域。"}}
{"id": "2509.06723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06723", "abs": "https://arxiv.org/abs/2509.06723", "authors": ["Ruicheng Zhang", "Jun Zhou", "Zunnan Xu", "Zihao Liu", "Jiehui Huang", "Mingyang Zhang", "Yu Sun", "Xiu Li"], "title": "Zero-shot 3D-Aware Trajectory-Guided image-to-video generation via Test-Time Training", "comment": null, "summary": "Trajectory-Guided image-to-video (I2V) generation aims to synthesize videos\nthat adhere to user-specified motion instructions. Existing methods typically\nrely on computationally expensive fine-tuning on scarce annotated datasets.\nAlthough some zero-shot methods attempt to trajectory control in the latent\nspace, they may yield unrealistic motion by neglecting 3D perspective and\ncreating a misalignment between the manipulated latents and the network's noise\npredictions. To address these challenges, we introduce Zo3T, a novel zero-shot\ntest-time-training framework for trajectory-guided generation with three core\ninnovations: First, we incorporate a 3D-Aware Kinematic Projection, leveraging\ninferring scene depth to derive perspective-correct affine transformations for\ntarget regions. Second, we introduce Trajectory-Guided Test-Time LoRA, a\nmechanism that dynamically injects and optimizes ephemeral LoRA adapters into\nthe denoising network alongside the latent state. Driven by a regional feature\nconsistency loss, this co-adaptation effectively enforces motion constraints\nwhile allowing the pre-trained model to locally adapt its internal\nrepresentations to the manipulated latent, thereby ensuring generative fidelity\nand on-manifold adherence. Finally, we develop Guidance Field Rectification,\nwhich refines the denoising evolutionary path by optimizing the conditional\nguidance field through a one-step lookahead strategy, ensuring efficient\ngenerative progression towards the target trajectory. Zo3T significantly\nenhances 3D realism and motion accuracy in trajectory-controlled I2V\ngeneration, demonstrating superior performance over existing training-based and\nzero-shot approaches.", "AI": {"tldr": "Zo3T 是一种新颖的零样本测试时训练框架，用于轨迹引导的图像到视频（I2V）生成，通过结合 3D 感知运动投影、轨迹引导的测试时 LoRA 和引导场修正，显著提升了 3D 真实感和运动精度。", "motivation": "现有轨迹引导的 I2V 生成方法通常依赖于计算昂贵的微调和稀缺的标注数据集。一些零样本方法在潜在空间中尝试轨迹控制，但由于忽略 3D 透视以及操纵的潜在变量与网络噪声预测之间的错位，可能导致不真实的运动。", "method": "Zo3T 框架包含三个核心创新：1. **3D 感知运动投影**：利用推断的场景深度为目标区域推导透视校正的仿射变换。2. **轨迹引导的测试时 LoRA**：动态注入并优化临时的 LoRA 适配器到去噪网络和潜在状态中，并通过区域特征一致性损失驱动，以强制执行运动约束并确保生成保真度。3. **引导场修正**：通过一步前瞻策略优化条件引导场，从而修正去噪演化路径，确保高效的生成进展。", "result": "Zo3T 显著增强了轨迹控制 I2V 生成中的 3D 真实感和运动精度，并展示出优于现有基于训练和零样本方法的性能。", "conclusion": "Zo3T 提供了一种有效的零样本测试时训练方法，通过其独特的三大创新，成功解决了现有方法在 3D 真实感和运动准确性方面的挑战，实现了高质量的轨迹引导图像到视频生成。"}}
{"id": "2509.06740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06740", "abs": "https://arxiv.org/abs/2509.06740", "authors": ["Qing Xu", "Wenting Duan", "Zhen Chen"], "title": "Co-Seg: Mutual Prompt-Guided Collaborative Learning for Tissue and Nuclei Segmentation", "comment": "Accepted to MICCAI 2025", "summary": "Histopathology image analysis is critical yet challenged by the demand of\nsegmenting tissue regions and nuclei instances for tumor microenvironment and\ncellular morphology analysis. Existing studies focused on tissue semantic\nsegmentation or nuclei instance segmentation separately, but ignored the\ninherent relationship between these two tasks, resulting in insufficient\nhistopathology understanding. To address this issue, we propose a Co-Seg\nframework for collaborative tissue and nuclei segmentation. Specifically, we\nintroduce a novel co-segmentation paradigm, allowing tissue and nuclei\nsegmentation tasks to mutually enhance each other. To this end, we first devise\na region-aware prompt encoder (RP-Encoder) to provide high-quality semantic and\ninstance region prompts as prior constraints. Moreover, we design a mutual\nprompt mask decoder (MP-Decoder) that leverages cross-guidance to strengthen\nthe contextual consistency of both tasks, collaboratively computing semantic\nand instance segmentation masks. Extensive experiments on the PUMA dataset\ndemonstrate that the proposed Co-Seg surpasses state-of-the-arts in the\nsemantic, instance and panoptic segmentation of tumor tissues and nuclei\ninstances. The source code is available at https://github.com/xq141839/Co-Seg.", "AI": {"tldr": "提出Co-Seg框架，通过协同分割范式同时处理组织区域和细胞核实例分割，以利用两者之间的内在关系，提高病理图像分析的准确性。", "motivation": "现有病理图像分析方法将组织语义分割和细胞核实例分割视为独立任务，忽略了它们之间的固有关系，导致对病理图像的理解不足。", "method": "Co-Seg框架引入了一种新颖的协同分割范式，使组织和细胞核分割任务能够相互增强。具体包括：1) 设计区域感知提示编码器（RP-Encoder），提供高质量的语义和实例区域提示作为先验约束；2) 设计相互提示掩码解码器（MP-Decoder），利用交叉指导加强两个任务的上下文一致性，协同计算语义和实例分割掩码。", "result": "在PUMA数据集上的大量实验表明，所提出的Co-Seg框架在肿瘤组织和细胞核实例的语义、实例和全景分割方面均超越了现有最先进的方法。", "conclusion": "Co-Seg框架通过协同分割策略，有效解决了组织和细胞核分割任务的挑战，显著提升了病理图像分析的性能，实现了对肿瘤组织和细胞核的更准确理解。"}}
{"id": "2509.06750", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06750", "abs": "https://arxiv.org/abs/2509.06750", "authors": ["Mang Hu", "Qianqian Xia"], "title": "Pothole Detection and Recognition based on Transfer Learning", "comment": null, "summary": "With the rapid development of computer vision and machine learning, automated\nmethods for pothole detection and recognition based on image and video data\nhave received significant attention. It is of great significance for social\ndevelopment to conduct an in-depth analysis of road images through feature\nextraction, thereby achieving automatic identification of the pothole condition\nin new images. Consequently, this is the main issue addressed in this study.\nBased on preprocessing techniques such as standardization, normalization, and\ndata augmentation applied to the collected raw dataset, we continuously\nimproved the network model based on experimental results. Ultimately, we\nconstructed a deep learning feature extraction network\nResNet50-EfficientNet-RegNet model based on transfer learning. This model\nexhibits high classification accuracy and computational efficiency. In terms of\nmodel evaluation, this study employed a comparative evaluation approach by\ncomparing the performance of the proposed transfer learning model with other\nmodels, including Random Forest, MLP, SVM, and LightGBM. The comparison\nanalysis was conducted based on metrics such as Accuracy, Recall, Precision,\nF1-score, and FPS, to assess the classification performance of the transfer\nlearning model proposed in this paper. The results demonstrate that our model\nexhibits high performance in terms of recognition speed and accuracy,\nsurpassing the performance of other models. Through careful parameter selection\nand model optimization, our transfer learning model achieved a classification\naccuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%\n(890/900) on the expanded test set.", "AI": {"tldr": "本研究提出了一种基于迁移学习的深度学习模型（ResNet50-EfficientNet-RegNet），用于自动检测路面坑洼，实现了高分类精度和计算效率，并优于传统机器学习模型。", "motivation": "随着计算机视觉和机器学习的快速发展，自动化的路面坑洼检测和识别方法受到了广泛关注。通过对道路图像进行特征提取，实现新图像中坑洼状况的自动识别对社会发展具有重要意义。", "method": "对原始数据集进行标准化、归一化和数据增强等预处理。基于实验结果持续改进网络模型，最终构建了一个基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet。通过与随机森林、MLP、SVM和LightGBM等其他模型进行比较评估，使用准确率、召回率、精确率、F1分数和FPS等指标来评估模型性能。", "result": "所提出的迁移学习模型在识别速度和准确性方面表现出高水平性能，超越了其他比较模型。在90个初始测试样本上实现了97.78%的分类准确率，在扩大的900个测试集上达到了98.89%的准确率。", "conclusion": "本研究构建的基于迁移学习的深度学习模型（ResNet50-EfficientNet-RegNet）能够高效、准确地自动识别路面坑洼，在性能上优于其他传统机器学习模型。"}}
{"id": "2509.06767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06767", "abs": "https://arxiv.org/abs/2509.06767", "authors": ["Zijie Ning", "Enmin Lin", "Sudarshan R. Iyengar", "Patrick Vandewalle"], "title": "Raw2Event: Converting Raw Frame Camera into Event Camera", "comment": "Submitted to IEEE Transactions on Robotics (Special Section on\n  Event-based Vision for Robotics), under review. This version is submitted for\n  peer review and may be updated upon acceptance", "summary": "Event cameras offer unique advantages such as high temporal resolution, low\nlatency, and high dynamic range, making them more and more popular for vision\ntasks under challenging light conditions. However, their high cost, limited\nresolution, and lack of features such as autofocus hinder their broad adoption,\nparticularly for early-stage development and prototyping. In this work, we\npresent Raw2Event, a complete hardware-software system that enables real-time\nevent generation from low-cost raw frame-based cameras. By leveraging direct\naccess to raw Bayer data and bypassing traditional image signal processors\n(ISP), our system is able to utilize the full potential of camera hardware,\ndelivering higher dynamic range, higher resolution, and more faithful output\nthan RGB-based frame-to-event converters.\n  Built upon the DVS-Voltmeter model, Raw2Event features a configurable\nsimulation framework optimized for deployment on embedded platforms. We further\ndesign a data acquisition pipeline that supports synchronized recording of raw,\nRGB, and event streams, facilitating downstream evaluation and dataset\ncreation. Experimental results show that Raw2Event can generate event streams\nclosely resembling those from real event cameras, while benefiting from higher\nresolution and autofocus capabilities. The system also supports user-intuitive\nparameter tuning, enabling flexible adaptation to various application\nrequirements. Finally, we deploy the system on a Raspberry Pi for real-time\noperation, providing a scalable and cost-effective solution for event-based\nvision research and early-stage system development.\n  The codes are available online:\nhttps://anonymous.4open.science/r/raw2event-BFF2/README.md.", "AI": {"tldr": "Raw2Event是一个软硬件系统，能从低成本的原始帧相机实时生成事件流，利用Bayer数据绕过ISP，提供比基于RGB的转换器更高动态范围和分辨率的输出，且支持实时嵌入式部署。", "motivation": "事件相机虽具有高时间分辨率、低延迟和高动态范围等优势，但其高成本、有限的分辨率和缺乏自动对焦等功能阻碍了其广泛应用，尤其是在早期开发和原型设计阶段。", "method": "本文提出了Raw2Event系统，通过直接访问原始Bayer数据并绕过传统图像信号处理器（ISP），利用DVS-Voltmeter模型构建可配置的仿真框架，并优化用于嵌入式平台。此外，设计了支持原始、RGB和事件流同步记录的数据采集管道。", "result": "Raw2Event能生成与真实事件相机高度相似的事件流，同时受益于更高的分辨率和自动对焦能力。系统支持用户友好的参数调整，并已在树莓派上实现实时部署，提供了一个可扩展且经济高效的解决方案。", "conclusion": "Raw2Event为事件视觉研究和早期系统开发提供了一个可扩展且经济高效的解决方案，克服了真实事件相机的高成本和功能限制，同时实现了实时事件生成，并具有更高分辨率和自动对焦功能。"}}
{"id": "2509.06771", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06771", "abs": "https://arxiv.org/abs/2509.06771", "authors": ["Sai Kartheek Reddy Kasu", "Mohammad Zia Ur Rehman", "Shahid Shafi Dar", "Rishi Bharat Junghare", "Dhanvin Sanjay Namboodiri", "Nagendra Kumar"], "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning", "comment": "Accepted at IEEE International Conference on Data Mining (ICDM) 2025", "summary": "Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning", "AI": {"tldr": "本研究引入了一个新的暗黑幽默模因数据集，并提出了一个基于推理增强的多模态框架，用于检测暗黑幽默、识别目标类别和预测强度，其性能优于现有基线。", "motivation": "在线模因中的暗黑幽默因其隐晦、敏感和文化语境化的线索而带来独特的挑战。目前缺乏用于检测多模态内容中暗黑幽默的资源和方法。", "method": "1. 构建了一个包含4,379个Reddit模因的新数据集，标注了暗黑幽默、目标类别（性别、心理健康、暴力、种族、残疾等）和三级强度（轻度、中度、重度）。2. 提出了一个推理增强框架：首先使用大型视觉-语言模型（VLM）生成结构化解释；然后通过“角色反转自循环”机制迭代细化解释；接着从OCR文本和自精炼推理中提取文本特征，并从图像中提取视觉特征。3. 最后，使用一个三流交叉推理网络（TCRNet）通过成对注意力机制融合文本、图像和推理这三股信息流，生成统一表示进行分类。", "result": "实验结果表明，本方法在暗黑幽默检测、目标识别和强度预测这三项任务上均优于强大的基线模型。", "conclusion": "本研究提出的方法在理解和检测多模态暗黑幽默方面表现出色。数据集、标注和代码的发布将促进多模态幽默理解和内容审核领域的进一步研究。"}}
{"id": "2509.06781", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06781", "abs": "https://arxiv.org/abs/2509.06781", "authors": ["Muhammad Shahbaz", "Shaurya Agarwal"], "title": "UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets", "comment": null, "summary": "This article presents UrbanTwin datasets - high-fidelity, realistic replicas\nof three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.\nEach UrbanTwin dataset contains 10K annotated frames corresponding to one of\nthe public datasets. Annotations include 3D bounding boxes, instance\nsegmentation labels, and tracking IDs for six object classes, along with\nsemantic segmentation labels for nine classes. These datasets are synthesized\nusing emulated lidar sensors within realistic digital twins, modeled based on\nsurrounding geometry, road alignment at lane level, and the lane topology and\nvehicle movement patterns at intersections of the actual locations\ncorresponding to each real dataset. Due to the precise digital twin modeling,\nthe synthetic datasets are well aligned with their real counterparts, offering\nstrong standalone and augmentative value for training deep learning models on\ntasks such as 3D object detection, tracking, and semantic and instance\nsegmentation. We evaluate the alignment of the synthetic replicas through\nstatistical and structural similarity analysis with real data, and further\ndemonstrate their utility by training 3D object detection models solely on\nsynthetic data and testing them on real, unseen data. The high similarity\nscores and improved detection performance, compared to the models trained on\nreal data, indicate that the UrbanTwin datasets effectively enhance existing\nbenchmark datasets by increasing sample size and scene diversity. In addition,\nthe digital twins can be adapted to test custom scenarios by modifying the\ndesign and dynamics of the simulations. To our knowledge, these are the first\ndigitally synthesized datasets that can replace in-domain real-world datasets\nfor lidar perception tasks. UrbanTwin datasets are publicly available at\nhttps://dataverse.harvard.edu/dataverse/ucf-ut.", "AI": {"tldr": "UrbanTwin数据集是高保真、逼真的合成激光雷达数据集，旨在复制现有公共数据集，并为3D目标检测、跟踪和分割等感知任务提供强大的独立或增强训练价值。", "motivation": "深度学习模型需要大量多样化的数据进行训练，而真实世界激光雷达数据的收集和标注成本高昂且耗时。该研究旨在创建能够替代或增强现有真实数据集的合成数据，以克服这些限制并提高模型性能。", "method": "通过在基于真实世界几何、车道对齐、车道拓扑和车辆运动模式建模的逼真数字孪生中模拟激光雷达传感器来合成数据集。每个数据集包含10K带注释的帧，包括六个对象类别的3D边界框、实例分割标签和跟踪ID，以及九个类别的语义分割标签。通过统计和结构相似性分析评估合成数据与真实数据的对齐程度，并通过仅使用合成数据训练3D目标检测模型并在未见过的真实数据上进行测试来验证其实用性。", "result": "UrbanTwin数据集与真实数据表现出高度相似性，并且使用合成数据训练的模型相比于仅使用真实数据训练的模型，在检测性能上有所提高。这表明UrbanTwin数据集通过增加样本量和场景多样性，有效增强了现有基准数据集。此外，数字孪生可以修改以测试自定义场景。据作者所知，这是首批能够替代领域内真实世界激光雷达感知任务数据集的数字合成数据集。", "conclusion": "UrbanTwin数据集是高质量的合成激光雷达数据，可以有效增强现有基准数据集，提高深度学习模型的训练效果，甚至可能在激光雷达感知任务中替代领域内的真实世界数据集，并支持自定义场景的测试。"}}
{"id": "2509.06784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06784", "abs": "https://arxiv.org/abs/2509.06784", "authors": ["Changfeng Ma", "Yang Li", "Xinhao Yan", "Jiachen Xu", "Yunhan Yang", "Chunshi Wang", "Zibo Zhao", "Yanwen Guo", "Zhuo Chen", "Chunchao Guo"], "title": "P3-SAM: Native 3D Part Segmentation", "comment": "Tech Report", "summary": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.", "AI": {"tldr": "本文提出了P3-SAM，一个原生的3D点提示式部件分割模型，旨在全自动、鲁棒地分割任何复杂3D对象的组成部分，并实现了最先进的性能。", "motivation": "3D资产分割对于提升3D理解、促进模型复用和支持部件生成等应用至关重要。然而，现有方法在处理复杂对象时鲁棒性差，且无法实现全自动化。", "method": "P3-SAM模型包含特征提取器、多个分割头和IoU预测器，支持交互式分割。同时，提出了一种算法来自动选择和合并模型预测的掩码，以实现部件实例分割。模型在一个包含近370万个带有合理分割标签的新构建数据集上进行训练。", "result": "P3-SAM在任何复杂对象上都实现了精确的分割结果和强大的鲁棒性，达到了最先进的性能。", "conclusion": "P3-SAM成功地解决了3D对象部件分割的自动化和鲁棒性问题，其性能优于现有方法，为3D理解和应用提供了有力支持。"}}
{"id": "2509.06793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06793", "abs": "https://arxiv.org/abs/2509.06793", "authors": ["George Ciubotariu", "Florin-Alexandru Vasluianu", "Zhuyun Zhou", "Nancy Mehta", "Radu Timofte", "Ke Wu", "Long Sun", "Lingshun Kong", "Zhongbao Yang", "Jinshan Pan", "Jiangxin Dong", "Jinhui Tang", "Hao Chen", "Yinghui Fang", "Dafeng Zhang", "Yongqi Song", "Jiangbo Guo", "Shuhua Jin", "Zeyu Xiao", "Rui Zhao", "Zhuoyuan Li", "Cong Zhang", "Yufeng Peng", "Xin Lu", "Zhijing Sun", "Chengjie Ge", "Zihao Li", "Zishun Liao", "Ziang Zhou", "Qiyu Kang", "Xueyang Fu", "Zheng-Jun Zha", "Yuqian Zhang", "Shuai Liu", "Jie Liu", "Zhuhao Zhang", "Lishen Qu", "Zhihao Liu", "Shihao Zhou", "Yaqi Luo", "Juncheng Zhou", "Jufeng Yang", "Qianfeng Yang", "Qiyuan Guan", "Xiang Chen", "Guiyue Jin", "Jiyu Jin"], "title": "AIM 2025 Challenge on High FPS Motion Deblurring: Methods and Results", "comment": "ICCVW AIM 2025", "summary": "This paper presents a comprehensive review of the AIM 2025 High FPS\nNon-Uniform Motion Deblurring Challenge, highlighting the proposed solutions\nand final results. The objective of this challenge is to identify effective\nnetworks capable of producing clearer and visually compelling images in diverse\nand challenging conditions, by learning representative visual cues for complex\naggregations of motion types. A total of 68 participants registered for the\ncompetition, and 9 teams ultimately submitted valid entries. This paper\nthoroughly evaluates the state-of-the-art advances in high-FPS single image\nmotion deblurring, showcasing the significant progress in the field, while\nleveraging samples of the novel dataset, MIORe, that introduces challenging\nexamples of movement patterns.", "AI": {"tldr": "本文回顾了AIM 2025高帧率非均匀运动去模糊挑战，总结了提出的解决方案、最终结果，并评估了该领域的最新进展。", "motivation": "该研究旨在通过挑战赛识别出能够学习复杂运动类型并生成更清晰、视觉上更吸引人的图像的有效网络，以应对多样且具有挑战性的去模糊条件。", "method": "本文回顾并评估了挑战赛中参赛团队提出的解决方案和最终结果，分析了高帧率单图像运动去模糊领域的最新进展，并利用了引入挑战性运动模式的新数据集MIORe的样本。", "result": "共有68名参赛者注册，9支队伍提交了有效参赛作品。本文展示了高帧率单图像运动去模糊领域取得的显著进展。", "conclusion": "高帧率单图像运动去模糊领域取得了显著进展，挑战赛成功识别了在复杂运动去模糊方面有效的网络和解决方案。"}}
{"id": "2509.06798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06798", "abs": "https://arxiv.org/abs/2509.06798", "authors": ["Zhengqing Chen", "Ruohong Mei", "Xiaoyang Guo", "Qingjie Wang", "Yubin Hu", "Wei Yin", "Weiqiang Ren", "Qian Zhang"], "title": "SynthDrive: Scalable Real2Sim2Real Sensor Simulation Pipeline for High-Fidelity Asset Generation and Driving Data Synthesis", "comment": "8 pages", "summary": "In the field of autonomous driving, sensor simulation is essential for\ngenerating rare and diverse scenarios that are difficult to capture in\nreal-world environments. Current solutions fall into two categories: 1)\nCG-based methods, such as CARLA, which lack diversity and struggle to scale to\nthe vast array of rare cases required for robust perception training; and 2)\nlearning-based approaches, such as NeuSim, which are limited to specific object\ncategories (vehicles) and require extensive multi-sensor data, hindering their\napplicability to generic objects. To address these limitations, we propose a\nscalable real2sim2real system that leverages 3D generation to automate asset\nmining, generation, and rare-case data synthesis.", "AI": {"tldr": "本文提出了一种可扩展的real2sim2real系统，利用3D生成技术自动化资产挖掘、生成和稀有案例数据合成，以克服现有自动驾驶传感器模拟方法在生成多样化和稀有场景方面的局限性。", "motivation": "自动驾驶领域中，传感器模拟对于生成难以在真实世界中捕获的稀有和多样化场景至关重要。然而，当前的解决方案（如基于CG的CARLA和基于学习的NeuSim）存在多样性不足、难以扩展到大量稀有案例、仅限于特定物体类别以及需要大量多传感器数据等问题，阻碍了其在通用物体上的应用。", "method": "本文提出了一种可扩展的real2sim2real系统。该系统利用3D生成技术，自动化资产挖掘、资产生成和稀有案例数据合成。", "result": "该系统旨在解决现有CG和基于学习的传感器模拟方法的局限性，实现稀有和多样化场景数据的自动化生成，从而支持鲁棒的感知训练。", "conclusion": "通过提出一个利用3D生成技术的real2sim2real系统，本文提供了一种可扩展的解决方案，以应对自动驾驶传感器模拟中生成稀有和多样化场景数据的挑战，从而提升感知系统的训练效果。"}}
{"id": "2509.06803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06803", "abs": "https://arxiv.org/abs/2509.06803", "authors": ["George Ciubotariu", "Zhuyun Zhou", "Zongwei Wu", "Radu Timofte"], "title": "MIORe & VAR-MIORe: Benchmarks to Push the Boundaries of Restoration", "comment": "ICCV 2025 Oral", "summary": "We introduce MIORe and VAR-MIORe, two novel multi-task datasets that address\ncritical limitations in current motion restoration benchmarks. Designed with\nhigh-frame-rate (1000 FPS) acquisition and professional-grade optics, our\ndatasets capture a broad spectrum of motion scenarios, which include complex\nego-camera movements, dynamic multi-subject interactions, and depth-dependent\nblur effects. By adaptively averaging frames based on computed optical flow\nmetrics, MIORe generates consistent motion blur, and preserves sharp inputs for\nvideo frame interpolation and optical flow estimation. VAR-MIORe further\nextends by spanning a variable range of motion magnitudes, from minimal to\nextreme, establishing the first benchmark to offer explicit control over motion\namplitude. We provide high-resolution, scalable ground truths that challenge\nexisting algorithms under both controlled and adverse conditions, paving the\nway for next-generation research of various image and video restoration tasks.", "AI": {"tldr": "本文介绍了MIORe和VAR-MIORe两个新型多任务数据集，旨在解决当前运动恢复基准的局限性，提供高帧率、复杂运动场景、深度相关模糊和可控运动幅度的真实数据。", "motivation": "现有运动恢复基准存在严重局限性，缺乏高帧率、复杂场景（如自我视角运动、多主体交互、深度依赖模糊）以及对运动幅度进行显式控制的数据集。", "method": "通过1000 FPS高帧率采集和专业级光学设备，捕捉广泛的运动场景。MIORe通过基于光流度量的自适应帧平均生成一致的运动模糊，并保留清晰输入用于视频帧插值和光流估计。VAR-MIORe在此基础上进一步扩展，涵盖了从最小到极端的变量运动幅度范围，首次实现了对运动幅度的显式控制。", "result": "成功构建了MIORe和VAR-MIORe两个多任务数据集，它们捕捉了复杂的自我视角运动、动态多主体交互和深度依赖模糊效应。MIORe生成一致的运动模糊并保留清晰输入，VAR-MIORe提供了对运动幅度的显式控制。数据集提供了高分辨率、可扩展的真实数据。", "conclusion": "这些数据集在受控和不利条件下对现有算法构成挑战，为图像和视频恢复任务的下一代研究铺平了道路。"}}
{"id": "2509.06818", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06818", "abs": "https://arxiv.org/abs/2509.06818", "authors": ["Yufeng Cheng", "Wenxu Wu", "Shaojin Wu", "Mengqi Huang", "Fei Ding", "Qian He"], "title": "UMO: Scaling Multi-Identity Consistency for Image Customization via Matching Reward", "comment": "Project page: https://bytedance.github.io/UMO/ Code and model:\n  https://github.com/bytedance/UMO", "summary": "Recent advancements in image customization exhibit a wide range of\napplication prospects due to stronger customization capabilities. However,\nsince we humans are more sensitive to faces, a significant challenge remains in\npreserving consistent identity while avoiding identity confusion with\nmulti-reference images, limiting the identity scalability of customization\nmodels. To address this, we present UMO, a Unified Multi-identity Optimization\nframework, designed to maintain high-fidelity identity preservation and\nalleviate identity confusion with scalability. With \"multi-to-multi matching\"\nparadigm, UMO reformulates multi-identity generation as a global assignment\noptimization problem and unleashes multi-identity consistency for existing\nimage customization methods generally through reinforcement learning on\ndiffusion models. To facilitate the training of UMO, we develop a scalable\ncustomization dataset with multi-reference images, consisting of both\nsynthesised and real parts. Additionally, we propose a new metric to measure\nidentity confusion. Extensive experiments demonstrate that UMO not only\nimproves identity consistency significantly, but also reduces identity\nconfusion on several image customization methods, setting a new\nstate-of-the-art among open-source methods along the dimension of identity\npreserving. Code and model: https://github.com/bytedance/UMO", "AI": {"tldr": "本文提出了UMO框架，通过将多身份生成重构为全局分配优化问题，并结合强化学习，显著提高了图像定制中多身份的一致性，减少了身份混淆，实现了可扩展的身份保留。", "motivation": "现有图像定制方法在处理多参考图像时，难以保持身份一致性并避免身份混淆，尤其是在对人脸敏感的场景中，这限制了定制模型的身份可扩展性。", "method": "UMO采用“多对多匹配”范式，将多身份生成重构为全局分配优化问题，并通过在扩散模型上应用强化学习来增强多身份一致性。此外，本文还开发了一个包含合成和真实数据的多参考图像可扩展定制数据集，并提出了一种新的身份混淆度量指标。", "result": "实验证明，UMO不仅显著提高了身份一致性，还减少了多种图像定制方法中的身份混淆，在身份保留方面超越了现有的开源方法，达到了新的技术水平。", "conclusion": "UMO是一个统一的多身份优化框架，有效解决了图像定制中多身份保留和混淆问题，具有良好的可扩展性，并为未来研究提供了新的数据集和评估指标。"}}
{"id": "2509.06826", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06826", "abs": "https://arxiv.org/abs/2509.06826", "authors": ["Dipta Neogi", "Nourash Azmine Chowdhury", "Muhammad Rafsan Kabir", "Mohammad Ashrafuzzaman Khan"], "title": "Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning", "comment": "12 pages, 9 figures", "summary": "The rapid growth of visual content consumption across platforms necessitates\nautomated video classification for age-suitability standards like the MPAA\nrating system (G, PG, PG-13, R). Traditional methods struggle with large\nlabeled data requirements, poor generalization, and inefficient feature\nlearning. To address these challenges, we employ contrastive learning for\nimproved discrimination and adaptability, exploring three frameworks: Instance\nDiscrimination, Contextual Contrastive Learning, and Multi-View Contrastive\nLearning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with a\nBahdanau attention mechanism, achieving state-of-the-art performance in the\nContextual Contrastive Learning framework, with 88% accuracy and an F1 score of\n0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,\nand attention mechanisms for dynamic frame prioritization, the model excels in\nfine-grained borderline distinctions, such as differentiating PG-13 and R-rated\ncontent. We evaluate the model's performance across various contrastive loss\nfunctions, including NT-Xent, NT-logistic, and Margin Triplet, demonstrating\nthe robustness of our proposed architecture. To ensure practical application,\nthe model is deployed as a web application for real-time MPAA rating\nclassification, offering an efficient solution for automated content compliance\nacross streaming platforms.", "AI": {"tldr": "该研究提出了一种结合对比学习、LRCN（CNN+LSTM）骨干网络和Bahdanau注意力机制的混合架构，用于MPAA年龄适宜性视频自动分类，在上下文对比学习框架下取得了最先进的性能，并已部署为实时Web应用。", "motivation": "随着视觉内容消费的快速增长，平台需要自动化的视频分类以符合MPAA等年龄适宜性标准。传统方法面临标记数据需求量大、泛化能力差和特征学习效率低等挑战。", "method": "该研究采用混合架构，结合了LRCN（CNN+LSTM）骨干网络和Bahdanau注意力机制。为提高判别力和适应性，探索了三种对比学习框架：实例判别、上下文对比学习和多视图对比学习。模型融合了CNN的空间特征提取、LSTM的时间建模和注意力机制的动态帧优先级，并评估了NT-Xent、NT-logistic和Margin Triplet等多种对比损失函数。最终，该模型被部署为一个Web应用程序用于实时分类。", "result": "在上下文对比学习框架下，该模型取得了最先进的性能，准确率达到88%，F1分数达到0.8815。该模型在精细的边界区分（如PG-13和R级内容）方面表现出色，并验证了其在不同对比损失函数下的鲁棒性。", "conclusion": "该研究提出的混合架构结合对比学习，为MPAA年龄适宜性视频的自动分类提供了一个高效、鲁棒且准确的解决方案，特别适用于流媒体平台上的内容合规性管理，并已成功部署为实时应用。"}}
{"id": "2509.06830", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06830", "abs": "https://arxiv.org/abs/2509.06830", "authors": ["Corentin Dancette", "Julien Khlaut", "Antoine Saporta", "Helene Philippe", "Elodie Ferreres", "Baptiste Callard", "Théo Danielou", "Léo Alberge", "Léo Machado", "Daniel Tordjman", "Julie Dupuis", "Korentin Le Floch", "Jean Du Terrail", "Mariam Moshiri", "Laurent Dercle", "Tom Boeken", "Jules Gregory", "Maxime Ronot", "François Legou", "Pascal Roux", "Marc Sapoval", "Pierre Manceron", "Paul Hérent"], "title": "Curia: A Multi-Modal Foundation Model for Radiology", "comment": null, "summary": "AI-assisted radiological interpretation is based on predominantly narrow,\nsingle-task models. This approach is impractical for covering the vast spectrum\nof imaging modalities, diseases, and radiological findings. Foundation models\n(FMs) hold the promise of broad generalization across modalities and in\nlow-data settings. However, this potential has remained largely unrealized in\nradiology. We introduce Curia, a foundation model trained on the entire\ncross-sectional imaging output of a major hospital over several years, which to\nour knowledge is the largest such corpus of real-world data-encompassing\n150,000 exams (130 TB). On a newly curated 19-task external validation\nbenchmark, Curia accurately identifies organs, detects conditions like brain\nhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.\nCuria meets or surpasses the performance of radiologists and recent foundation\nmodels, and exhibits clinically significant emergent properties in\ncross-modality, and low-data regimes. To accelerate progress, we release our\nbase model's weights at https://huggingface.co/raidium/curia.", "AI": {"tldr": "当前放射学AI模型狭窄，本文提出Curia，一个基于医院多年影像数据训练的放射学基础模型，在19项任务上表现优于放射科医生及现有模型，并展现出跨模态和低数据量下的显著性能。", "motivation": "现有AI辅助放射学解释主要基于狭窄的单任务模型，难以覆盖广泛的影像模态、疾病和发现。基础模型有望实现跨模态和低数据量下的广泛泛化，但在放射学领域尚未充分实现。", "method": "引入Curia，一个基础模型。该模型在一家大型医院数年来的全部横断面影像输出（15万次检查，130 TB）上进行训练，据称是最大的真实世界数据集。在一个新策划的19项任务外部验证基准上进行评估。", "result": "Curia能准确识别器官，检测脑出血和心肌梗死等疾病，并预测肿瘤分期结果。其性能达到或超越放射科医生和近期其他基础模型，并在跨模态和低数据量情况下展现出临床上显著的涌现特性。基础模型权重已发布。", "conclusion": "Curia证明了基础模型在放射学领域实现广泛泛化和高性能的潜力，通过在大量真实世界数据上训练，成功应对了多任务挑战，并超越了现有方法，有望加速放射学AI的发展。"}}
{"id": "2509.06831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06831", "abs": "https://arxiv.org/abs/2509.06831", "authors": ["Simon Pezold", "Jérôme A. Kurylec", "Jan S. Liechti", "Beat P. Müller", "Joël L. Lavanchy"], "title": "Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis", "comment": "13 pages, 3 figures; accepted at ML-CDS @ MICCAI 2025, Daejeon,\n  Republic of Korea", "summary": "We investigate how both the adaptation of a generic foundation model via\ntransfer learning and the integration of complementary modalities from the\noperating room (OR) can support surgical data science. To this end, we use\nV-JEPA as the single-modality foundation of a multimodal model for minimally\ninvasive surgery support. We analyze how the model's downstream performance can\nbenefit (a) from finetuning on unlabeled surgical video data and (b) from\nproviding additional time-resolved data streams from the OR in a multimodal\nsetup.\n  In an in-house dataset of liver surgery videos, we analyze the tasks of\npredicting hospital length of stay and postoperative complications. In videos\nof the public HeiCo dataset, we analyze the task of surgical phase recognition.\nAs a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it on\nunlabeled, held-out videos to investigate its change in performance after\ndomain adaptation. Following the idea of modular decision support networks, we\nintegrate additional data streams from the OR by training a separate encoder to\nform a shared representation space with V-JEPA's embeddings.\n  Our experiments show that finetuning on domain-specific data increases model\nperformance. On the in-house data, integrating additional time-resolved data\nlikewise benefits the model. On the HeiCo data, accuracy of the pretrained\nvideo-only, single-modality baseline setup is on par with the top-performing\nsubmissions of the EndoVis2017 challenge, while finetuning on domain-specific\ndata increases accuracy further. Our results thus demonstrate how surgical data\nscience can leverage public, generic foundation models. Likewise, they indicate\nthe potential of domain adaptation and of integrating suitable complementary\ndata streams from the OR. To support further research, we release our code and\nmodel weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.", "AI": {"tldr": "本研究探讨了如何通过迁移学习适应通用基础模型（V-JEPA）并整合手术室（OR）的互补模态数据来支持手术数据科学，结果表明领域适应和多模态数据集成可显著提升模型性能。", "motivation": "研究旨在调查通用基础模型（如V-JEPA）如何通过迁移学习进行适应，以及整合手术室（OR）的互补模态数据如何支持手术数据科学，以提升手术支持系统的性能。", "method": "本研究以V-JEPA作为单模态基础模型，构建多模态模型以支持微创手术。方法包括：(a) 在未标记的手术视频数据上进行微调，以评估领域适应对模型下游性能的益处；(b) 在多模态设置中，通过训练一个独立的编码器与V-JEPA嵌入形成共享表示空间，整合手术室中额外的时间分辨数据流。研究在内部肝脏手术视频数据集上分析住院时间预测和术后并发症预测任务，并在公共HeiCo数据集上分析手术阶段识别任务。", "result": "实验结果显示，在领域特定数据上进行微调能提高模型性能。在内部数据集上，整合额外的时间分辨数据同样有益于模型。在HeiCo数据集上，预训练的纯视频、单模态基线设置的准确性与EndoVis2017挑战赛的顶尖提交结果相当，而领域特定数据上的微调进一步提升了准确性。", "conclusion": "本研究结果表明，手术数据科学可以有效利用公共的通用基础模型。同时，领域适应和整合手术室中合适的互补数据流展现出巨大的潜力。为支持进一步研究，代码和模型权重已公开发布。"}}
{"id": "2509.06835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06835", "abs": "https://arxiv.org/abs/2509.06835", "authors": ["Nabeyou Tadessa", "Balaji Iyangar", "Mashrur Chowdhury"], "title": "Evaluating the Impact of Adversarial Attacks on Traffic Sign Classification using the LISA Dataset", "comment": null, "summary": "Adversarial attacks pose significant threats to machine learning models by\nintroducing carefully crafted perturbations that cause misclassification. While\nprior work has primarily focused on MNIST and similar datasets, this paper\ninvestigates the vulnerability of traffic sign classifiers using the LISA\nTraffic Sign dataset. We train a convolutional neural network to classify 47\ndifferent traffic signs and evaluate its robustness against Fast Gradient Sign\nMethod (FGSM) and Projected Gradient Descent (PGD) attacks. Our results show a\nsharp decline in classification accuracy as the perturbation magnitude\nincreases, highlighting the models susceptibility to adversarial examples. This\nstudy lays the groundwork for future exploration into defense mechanisms\ntailored for real-world traffic sign recognition systems.", "AI": {"tldr": "本研究使用LISA交通标志数据集，评估了卷积神经网络对47种交通标志的分类能力，并测试了其在FGSM和PGD对抗性攻击下的脆弱性，结果显示分类准确率随扰动幅度增加而急剧下降。", "motivation": "以往的对抗性攻击研究主要集中在MNIST等简单数据集上，但交通标志识别系统在现实世界中至关重要。本研究旨在探究交通标志分类器在对抗性攻击下的脆弱性，以填补这一空白。", "method": "研究训练了一个卷积神经网络（CNN）来分类LISA交通标志数据集中的47种不同交通标志。随后，使用快速梯度符号法（FGSM）和投影梯度下降（PGD）两种对抗性攻击方法，评估了模型在不同扰动幅度下的鲁棒性。", "result": "实验结果表明，随着对抗性扰动幅度的增加，交通标志分类器的分类准确率急剧下降，突显了模型对对抗性样本的敏感性。", "conclusion": "研究证实了交通标志分类器在对抗性攻击下的高度脆弱性。这项工作为未来探索针对真实世界交通标志识别系统的防御机制奠定了基础。"}}
{"id": "2509.06839", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.06839", "abs": "https://arxiv.org/abs/2509.06839", "authors": ["Matteo Muratori", "Joël Seytre"], "title": "ToonOut: Fine-tuned Background-Removal for Anime Characters", "comment": null, "summary": "While state-of-the-art background removal models excel at realistic imagery,\nthey frequently underperform in specialized domains such as anime-style\ncontent, where complex features like hair and transparency present unique\nchallenges. To address this limitation, we collected and annotated a custom\ndataset of 1,228 high-quality anime images of characters and objects, and\nfine-tuned the open-sourced BiRefNet model on this dataset. This resulted in\nmarked improvements in background removal accuracy for anime-style images,\nincreasing from 95.3% to 99.5% for our newly introduced Pixel Accuracy metric.\nWe are open-sourcing the code, the fine-tuned model weights, as well as the\ndataset at: https://github.com/MatteoKartoon/BiRefNet.", "AI": {"tldr": "针对动漫风格内容背景去除的挑战，本研究收集并标注了一个包含1,228张动漫图像的定制数据集，并在此数据集上对BiRefNet模型进行微调，显著提高了动漫图像背景去除的准确性，像素准确率从95.3%提升至99.5%。", "motivation": "现有最先进的背景去除模型在动漫风格内容等特殊领域表现不佳，因为动漫图像中复杂的特征（如头发和透明度）带来了独特的挑战。", "method": "研究人员收集并标注了一个包含1,228张高质量动漫角色和物体图像的定制数据集，然后使用此数据集对开源的BiRefNet模型进行了微调。", "result": "在动漫风格图像的背景去除准确性方面取得了显著提升，新引入的像素准确率指标从95.3%增加到99.5%。研究还开源了代码、微调后的模型权重以及数据集。", "conclusion": "通过在专门收集和标注的动漫数据集上对现有模型进行微调，可以有效解决动漫风格内容背景去除的挑战，显著提高其准确性。"}}
{"id": "2509.06862", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06862", "abs": "https://arxiv.org/abs/2509.06862", "authors": ["Aymen Merrouche", "Stefanie Wuhrer", "Edmond Boyer"], "title": "Matching Shapes Under Different Topologies: A Topology-Adaptive Deformation Guided Approach", "comment": null, "summary": "Non-rigid 3D mesh matching is a critical step in computer vision and computer\ngraphics pipelines. We tackle matching meshes that contain topological\nartefacts which can break the assumption made by current approaches. While\nFunctional Maps assume the deformation induced by the ground truth\ncorrespondences to be near-isometric, ARAP-like deformation-guided approaches\nassume the latter to be ARAP. Neither assumption holds in certain topological\nconfigurations of the input shapes. We are motivated by real-world scenarios\nsuch as per-frame multi-view reconstructions, often suffering from topological\nartefacts. To this end, we propose a topology-adaptive deformation model\nallowing changes in shape topology to align shape pairs under ARAP and\nbijective association constraints. Using this model, we jointly optimise for a\ntemplate mesh with adequate topology and for its alignment with the shapes to\nbe matched to extract correspondences. We show that, while not relying on any\ndata-driven prior, our approach applies to highly non-isometric shapes and\nshapes with topological artefacts, including noisy per-frame multi-view\nreconstructions, even outperforming methods trained on large datasets in 3D\nalignment quality.", "AI": {"tldr": "该研究提出了一种拓扑自适应变形模型，用于解决非刚性三维网格匹配中因拓扑伪影导致现有方法失效的问题，通过联合优化模板网格及其与待匹配形状的对齐，实现在ARAP和双射关联约束下处理拓扑变化，并取得了优于数据驱动方法的对齐质量。", "motivation": "现有非刚性三维网格匹配方法（如功能映射和ARAP类方法）依赖于近等距或ARAP变形假设。然而，在真实世界的场景中（例如每帧多视角重建），输入形状常包含拓扑伪影，导致这些假设失效，从而限制了现有方法的应用。", "method": "本文提出了一种拓扑自适应变形模型，该模型允许形状拓扑发生变化，以在ARAP和双射关联约束下对齐形状对。通过使用此模型，研究人员联合优化了一个具有适当拓扑的模板网格，并将其与待匹配的形状进行对齐，从而提取对应关系。", "result": "该方法无需依赖任何数据驱动先验，即可应用于高度非等距形状和包含拓扑伪影的形状（包括嘈杂的每帧多视角重建）。在三维对齐质量方面，其表现甚至优于在大数据集上训练的方法。", "conclusion": "所提出的拓扑自适应变形模型能够有效处理具有拓扑伪影和高度非等距变形的非刚性三维网格匹配问题，提供鲁棒且高质量的对齐结果，且无需数据驱动先验。"}}
{"id": "2509.06868", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06868", "abs": "https://arxiv.org/abs/2509.06868", "authors": ["Behnoud Shafiezadeh", "Amir Mashmool", "Farshad Eshghi", "Manoochehr Kelarestaghi"], "title": "A New Hybrid Model of Generative Adversarial Network and You Only Look Once Algorithm for Automatic License-Plate Recognition", "comment": null, "summary": "Automatic License-Plate Recognition (ALPR) plays a pivotal role in\nIntelligent Transportation Systems (ITS) as a fundamental element of Smart\nCities. However, due to its high variability, ALPR faces challenging issues\nmore efficiently addressed by deep learning techniques. In this paper, a\nselective Generative Adversarial Network (GAN) is proposed for deblurring in\nthe preprocessing step, coupled with the state-of-the-art You-Only-Look-Once\n(YOLO)v5 object detection architectures for License-Plate Detection (LPD), and\nthe integrated Character Segmentation (CS) and Character Recognition (CR)\nsteps. The selective preprocessing bypasses unnecessary and sometimes\ncounter-productive input manipulations, while YOLOv5 LPD/CS+CR delivers high\naccuracy and low computing cost. As a result, YOLOv5 achieves a detection time\nof 0.026 seconds for both LP and CR detection stages, facilitating real-time\napplications with exceptionally rapid responsiveness. Moreover, the proposed\nmodel achieves accuracy rates of 95\\% and 97\\% in the LPD and CR detection\nphases, respectively. Furthermore, the inclusion of the Deblur-GAN\npre-processor significantly improves detection accuracy by nearly 40\\%,\nespecially when encountering blurred License Plates (LPs).To train and test the\nlearning components, we generated and publicly released our blur and ALPR\ndatasets (using Iranian license plates as a use-case), which are more\nrepresentative of close-to-real-life ad-hoc situations. The findings\ndemonstrate that employing the state-of-the-art YOLO model results in excellent\noverall precision and detection time, making it well-suited for portable\napplications. Additionally, integrating the Deblur-GAN model as a preliminary\nprocessing step enhances the overall effectiveness of our comprehensive model,\nparticularly when confronted with blurred scenes captured by the camera as\ninput.", "AI": {"tldr": "本文提出了一种结合选择性Deblur-GAN预处理和YOLOv5模型的自动车牌识别（ALPR）系统，实现了高精度和低延迟，特别是在处理模糊车牌时性能显著提升。", "motivation": "自动车牌识别（ALPR）在智能交通系统（ITS）和智慧城市中至关重要，但由于其高变异性，面临诸多挑战，尤其是车牌模糊问题，需要更有效的深度学习技术来解决。", "method": "该研究提出了一种选择性生成对抗网络（GAN）用于预处理步骤中的去模糊，并结合了最先进的YOLOv5目标检测架构进行车牌检测（LPD）、字符分割（CS）和字符识别（CR）。选择性预处理避免了不必要且可能适得其反的输入操作。此外，作者生成并公开了一个包含模糊和ALPR数据的伊朗车牌数据集进行训练和测试。", "result": "YOLOv5模型在车牌和字符识别阶段的检测时间为0.026秒，实现了实时应用。在LPD和CR检测阶段分别达到了95%和97%的准确率。Deblur-GAN预处理器显著提高了检测精度近40%，特别是在遇到模糊车牌时。该模型在整体精度和检测时间上表现出色。", "conclusion": "采用最先进的YOLO模型实现了卓越的整体精度和检测时间，使其非常适合便携式应用。将Deblur-GAN模型作为预处理步骤集成，显著增强了综合模型的整体有效性，尤其是在处理相机捕获的模糊场景输入时。"}}
{"id": "2509.06904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06904", "abs": "https://arxiv.org/abs/2509.06904", "authors": ["Cem Eteke", "Alexander Griessel", "Wolfgang Kellerer", "Eckehard Steinbach"], "title": "BIR-Adapter: A Low-Complexity Diffusion Model Adapter for Blind Image Restoration", "comment": "20 pages, 14 figures", "summary": "This paper introduces BIR-Adapter, a low-complexity blind image restoration\nadapter for diffusion models. The BIR-Adapter enables the utilization of the\nprior of pre-trained large-scale diffusion models on blind image restoration\nwithout training any auxiliary feature extractor. We take advantage of the\nrobustness of pretrained models. We extract features from degraded images via\nthe model itself and extend the self-attention mechanism with these degraded\nfeatures. We introduce a sampling guidance mechanism to reduce hallucinations.\nWe perform experiments on synthetic and real-world degradations and demonstrate\nthat BIR-Adapter achieves competitive or better performance compared to\nstate-of-the-art methods while having significantly lower complexity.\nAdditionally, its adapter-based design enables integration into other diffusion\nmodels, enabling broader applications in image restoration tasks. We showcase\nthis by extending a super-resolution-only model to perform better under\nadditional unknown degradations.", "AI": {"tldr": "本文提出BIR-Adapter，一种低复杂度的盲图像修复适配器，利用预训练扩散模型的先验知识，无需训练辅助特征提取器，即可在盲图像修复任务上达到SOTA或更优性能。", "motivation": "研究动机在于利用预训练大型扩散模型的强大先验知识进行盲图像修复，同时寻求一种低复杂度、无需额外训练辅助特征提取器的方法，并利用预训练模型的鲁棒性处理退化图像。", "method": "BIR-Adapter通过模型自身从退化图像中提取特征，并用这些退化特征扩展自注意力机制。此外，引入了一种采样引导机制来减少幻觉。其设计为适配器形式，无需训练辅助特征提取器。", "result": "BIR-Adapter在合成和真实世界的退化图像上取得了与现有最先进方法相当或更优的性能，同时具有显著降低的复杂度。其适配器设计使其能够集成到其他扩散模型中，扩展了图像修复任务的应用范围，例如将仅支持超分辨率的模型扩展到处理未知退化。", "conclusion": "BIR-Adapter提供了一种高效、低复杂度的解决方案，成功地将预训练扩散模型的先验知识应用于盲图像修复。其适配器设计不仅实现了卓越性能，还增强了模型的可扩展性和通用性，使其能够应用于更广泛的图像修复场景。"}}
{"id": "2509.06907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.06907", "abs": "https://arxiv.org/abs/2509.06907", "authors": ["Bing Han", "Chen Zhu", "Dong Han", "Rui Yu", "Songliang Cao", "Jianhui Wu", "Scott Chapman", "Zijian Wang", "Bangyou Zheng", "Wei Guo", "Marie Weiss", "Benoit de Solan", "Andreas Hund", "Lukas Roth", "Kirchgessner Norbert", "Andrea Visioni", "Yufeng Ge", "Wenjuan Li", "Alexis Comar", "Dong Jiang", "Dejun Han", "Fred Baret", "Yanfeng Ding", "Hao Lu", "Shouyang Liu"], "title": "FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data", "comment": null, "summary": "Vision-driven field monitoring is central to digital agriculture, yet models\nbuilt on general-domain pretrained backbones often fail to generalize across\ntasks, owing to the interaction of fine, variable canopy structures with\nfluctuating field conditions. We present FoMo4Wheat, one of the first\ncrop-domain vision foundation model pretrained with self-supervision on\nImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5\nmillion high-resolution images collected over a decade at 30 global sites,\nspanning >2,000 genotypes and >500 environmental conditions). This\nwheat-specific pretraining yields representations that are robust for wheat and\ntransferable to other crops and weeds. Across ten in-field vision tasks at\ncanopy and organ levels, FoMo4Wheat models consistently outperform\nstate-of-the-art models pretrained on general-domain dataset. These results\ndemonstrate the value of crop-specific foundation models for reliable in-field\nperception and chart a path toward a universal crop foundation model with\ncross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheat\ndataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheat\nand https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:\nhttps://fomo4wheat.phenix-lab.com/.", "AI": {"tldr": "FoMo4Wheat是一个基于最大的小麦图像数据集ImAg4Wheat进行自监督预训练的作物领域视觉基础模型，它在多种田间视觉任务中显著优于通用领域模型，证明了作物特定基础模型在数字农业中的价值。", "motivation": "现有基于通用领域预训练骨干的模型在数字农业中难以泛化，原因在于精细多变的冠层结构与波动不定的田间条件相互作用，导致其在农业任务中表现不佳。", "method": "研究提出了FoMo4Wheat，一个作物领域视觉基础模型。该模型在ImAg4Wheat数据集上进行自监督预训练，该数据集是目前最大、最多样化的小麦图像数据集（包含250万张高分辨率图像，跨越十年、30个全球地点、2000多种基因型和500多种环境条件）。模型在冠层和器官级别的十项田间视觉任务上进行了性能评估。", "result": "FoMo4Wheat模型持续优于在通用领域数据集上预训练的现有最先进模型。它产生了对小麦鲁棒且可迁移到其他作物和杂草的表示。这些结果证明了作物特定基础模型在可靠田间感知中的价值。", "conclusion": "作物特定基础模型对于可靠的田间感知具有重要价值，并为开发具有跨物种和跨任务能力的通用作物基础模型指明了方向。"}}
