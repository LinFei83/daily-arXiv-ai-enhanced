<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 52]
- [cs.CV](#cs.CV) [Total: 130]
- [cs.CL](#cs.CL) [Total: 51]
- [cs.RO](#cs.RO) [Total: 35]
- [eess.SY](#eess.SY) [Total: 10]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.GR](#cs.GR) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Efficient Agents: Building Effective Agents While Reducing Cost](https://arxiv.org/abs/2508.02694)
*Ningning Wang,Xavier Hu,Pai Liu,He Zhu,Yue Hou,Heyuan Huang,Shengyu Zhang,Jian Yang,Jiaheng Liu,Ge Zhang,Changwang Zhang,Jun Wang,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.AI

TL;DR: 本研究系统性地探讨了LLM驱动的智能体系统在效率与有效性之间的权衡，并提出了一个名为“高效智能体”的新框架，显著降低了成本同时保持了高水平性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）驱动的智能体在处理复杂多步骤任务方面表现出色，但其不断上升的成本威胁到可扩展性和可访问性。因此，迫切需要设计具有成本效益且不牺牲性能的智能体系统。

Method: 本研究对现代智能体系统中的效率-有效性权衡进行了首次系统性研究。通过在GAIA基准上进行实证分析，评估了LLM骨干模型选择、智能体框架设计和测试时扩展策略的影响。使用“通过成本”（cost-of-pass）指标量化了这些维度上的效率-性能权衡。在此基础上，提出了一种名为“高效智能体”的新型智能体框架，旨在实现任务要求与复杂度的最优匹配。

Result: 研究回答了智能体任务固有的复杂性需求、附加模块的边际效益递减点以及通过高效框架设计可获得的效率提升等问题。结果表明，“高效智能体”框架在将操作成本从0.398美元降低到0.228美元的同时（通过成本提高了28.4%），仍保留了领先开源智能体框架OWL 96.7%的性能。

Conclusion: 本研究为设计高效、高性能的智能体系统提供了可操作的见解，从而提升了AI驱动解决方案的可访问性和可持续性。

Abstract: The remarkable capabilities of Large Language Model (LLM)-driven agents have
enabled sophisticated systems to tackle complex, multi-step tasks, but their
escalating costs threaten scalability and accessibility. This work presents the
first systematic study of the efficiency-effectiveness trade-off in modern
agent systems, addressing the critical need for cost-effective designs without
sacrificing performance. We investigate three key questions: (1) How much
complexity do agentic tasks inherently require? (2) When do additional modules
yield diminishing returns? (3) How much efficiency can be gained through the
design of efficient agent frameworks? Through an empirical analysis on the GAIA
benchmark, we evaluate the impact of LLM backbone selection, agent framework
designs, and test-time scaling strategies. Using the cost-of-pass metric, we
quantify the efficiency-performance trade-off across these dimensions. Our
findings inform the development of Efficient Agents , a novel agent framework
that has an optimal complexity to task requirements. Efficient Agents retains
96.7% of the performance of OWL, one leading open-source agent framework, while
reducing operational costs from $0.398 to $0.228, resulting in a 28.4%
improvement in cost-of-pass. Our work provides actionable insights for
designing efficient, high-performing agent systems, advancing the accessibility
and sustainability of AI-driven solutions.

</details>


### [2] [Planning with Dynamically Changing Domains](https://arxiv.org/abs/2508.02697)
*Mikhail Soutchanski,Yongmei Liu*

Main category: cs.AI

TL;DR: 该研究提出了一种在动态对象集（可创建/销毁对象）环境下，无需领域封闭假设（DCA）的规划方法，并证明了其完备性和正确性。


<details>
  <summary>Details</summary>
Motivation: 传统的经典规划和一致性规划都依赖于领域封闭假设（DCA），即预先给定有限的命名对象，且只有这些对象能参与动作和流变。然而，在实际规划问题中，对象集合会随着动作的执行而动态变化，例如创建新对象或销毁旧对象，DCA无法处理此类问题。

Method: 将规划问题形式化为一阶逻辑，假设初始理论是有限且一致的流变文字集合。讨论了何时能保证在每种情境下只有有限数量的可能动作。对计划长度施加有限整数限制，并提出在规划时对接地动作序列进行搜索。

Result: 该方法被证明是可靠且完备的。它能够解决属于序列广义规划（无感知动作）和一致性规划交集中的、无DCA的有限规划问题，但仅限于没有流变文字析取的特殊情况。论文还讨论了其规划器概念验证实现的细节。

Conclusion: 该方法为解决动态对象集下的规划问题提供了一种理论上健全且完备的解决方案，克服了传统规划中DCA的限制，并展示了其在特定问题类别中的有效性。

Abstract: In classical planning and conformant planning, it is assumed that there are
finitely many named objects given in advance, and only they can participate in
actions and in fluents. This is the Domain Closure Assumption (DCA). However,
there are practical planning problems where the set of objects changes
dynamically as actions are performed; e.g., new objects can be created, old
objects can be destroyed. We formulate the planning problem in first-order
logic, assume an initial theory is a finite consistent set of fluent literals,
discuss when this guarantees that in every situation there are only finitely
many possible actions, impose a finite integer bound on the length of the plan,
and propose to organize search over sequences of actions that are grounded at
planning time. We show the soundness and completeness of our approach. It can
be used to solve the bounded planning problems without DCA that belong to the
intersection of sequential generalized planning (without sensing actions) and
conformant planning, restricted to the case without the disjunction over fluent
literals. We discuss a proof-of-the-concept implementation of our planner.

</details>


### [3] [Recovering Individual-Level Activity Sequences from Location-Based Service Data Using a Novel Transformer-Based Model](https://arxiv.org/abs/2508.02734)
*Weiyu Luo,Chenfeng Xiong*

Main category: cs.AI

TL;DR: 该研究提出VSNIT模型，用于从高质量LBS数据中恢复个人层面的不完整活动序列，显著提升了恢复的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: LBS数据虽然能提供人类出行洞察，但其稀疏性常导致出行和活动序列不完整，难以准确推断。因此，研究旨在解决如何利用高质量LBS数据恢复不完整的个人活动序列。

Method: 本研究提出了一种新的解决方案：变数选择网络融合插入转换器（Variable Selection Network-fused Insertion Transformer, VSNIT）。该模型整合了插入转换器（Insertion Transformer）的灵活序列构建能力和变数选择网络（Variable Selection Network）的动态协变量处理能力，以恢复不完整的活动序列中的缺失片段，同时保留现有数据。

Result: 研究结果表明，VSNIT能够插入更多样化、更真实的活动模式，更接近现实世界的变异性，并更有效地恢复被中断的活动转换，使其与目标对齐。此外，VSNIT在所有指标上均显著优于基线模型。

Conclusion: 这些结果突显了VSNIT在活动序列恢复任务中卓越的准确性和多样性，证明了其在增强LBS数据效用以进行出行分析方面的潜力。该方法为未来的基于位置的研究和应用提供了一个有前景的框架。

Abstract: Location-Based Service (LBS) data provides critical insights into human
mobility, yet its sparsity often yields incomplete trip and activity sequences,
making accurate inferences about trips and activities difficult. We raise a
research problem: Can we use activity sequences derived from high-quality LBS
data to recover incomplete activity sequences at the individual level? This
study proposes a new solution, the Variable Selection Network-fused Insertion
Transformer (VSNIT), integrating the Insertion Transformer's flexible sequence
construction with the Variable Selection Network's dynamic covariate handling
capability, to recover missing segments in incomplete activity sequences while
preserving existing data. The findings show that VSNIT inserts more diverse,
realistic activity patterns, more closely matching real-world variability, and
restores disrupted activity transitions more effectively aligning with the
target. It also performs significantly better than the baseline model across
all metrics. These results highlight VSNIT's superior accuracy and diversity in
activity sequence recovery tasks, demonstrating its potential to enhance LBS
data utility for mobility analysis. This approach offers a promising framework
for future location-based research and applications.

</details>


### [4] [Large Language Model-based Data Science Agent: A Survey](https://arxiv.org/abs/2508.02744)
*Peiran Wang,Yaoning Yu,Ke Chen,Xianyang Zhan,Haohan Wang*

Main category: cs.AI

TL;DR: 这篇综述全面分析了用于数据科学任务的LLM（大型语言模型）代理，从代理设计和数据科学工作流双重角度进行了探讨。


<details>
  <summary>Details</summary>
Motivation: LLM的快速发展推动了新应用，其中LLM代理是重要的探索领域，尤其是在数据科学任务中展现出巨大潜力。

Method: 通过对近期研究的总结，本研究从代理视角讨论了关键设计原则（角色、执行、知识、反思方法），并从数据科学视角识别了LLM代理的关键流程（数据预处理、模型开发、评估、可视化等）。

Result: 成果包括：1) 对LLM代理应用于数据科学任务最新进展的全面综述；2) 一个连接通用代理设计原则与数据科学实际工作流的双视角框架。

Conclusion: 本工作提供了对LLM代理在数据科学领域应用的全面回顾，并提出了一个实用的双视角框架，有助于理解和开发LLM驱动的数据科学代理。

Abstract: The rapid advancement of Large Language Models (LLMs) has driven novel
applications across diverse domains, with LLM-based agents emerging as a
crucial area of exploration. This survey presents a comprehensive analysis of
LLM-based agents designed for data science tasks, summarizing insights from
recent studies. From the agent perspective, we discuss the key design
principles, covering agent roles, execution, knowledge, and reflection methods.
From the data science perspective, we identify key processes for LLM-based
agents, including data preprocessing, model development, evaluation,
visualization, etc. Our work offers two key contributions: (1) a comprehensive
review of recent developments in applying LLMbased agents to data science
tasks; (2) a dual-perspective framework that connects general agent design
principles with the practical workflows in data science.

</details>


### [5] [Cognitive Loop via In-Situ Optimization: Self-Adaptive Reasoning for Science](https://arxiv.org/abs/2508.02789)
*Newman Cheng,Gordon Broadbent,William Chappell*

Main category: cs.AI

TL;DR: 该研究提出了一种名为CLIO（认知循环通过原位优化）的新方法，使大型语言模型（LLMs）能够自我制定解决问题的方法，并在置信度低时调整行为，从而提高科学发现中的推理能力、透明度和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的AI开发模式（基于非推理模型的框架或抽象推理控制的推理模型）在科学发现中存在局限性。科学家不仅需要推理的准确性和透明度，还需要可控性，以最大限度地利用AI进行科学发现。

Method: CLIO通过“认知循环和原位优化”实现对推理过程的深度和精确控制。它允许LLMs自我制定解决问题的方法，在自我置信度低时调整行为，并最终提供一个最终的信念或答案。其开放式设计使科学家能够观察不确定性水平，通过图结构理解最终信念状态的形成，并进行干预纠正。

Result: 在没有额外后期训练的情况下，结合CLIO的OpenAI GPT-4.1在“人类的最后考试”（HLE）文本生物学和医学问题上达到了22.37%的准确率。这比基础GPT-4.1模型净提高了13.82%（相对提高了161.64%），并超越了OpenAI o3在高低推理努力模式下的表现。研究还发现，内部不确定性测量的振荡是决定CLIO结果准确性的关键。

Conclusion: CLIO的开放设计和内部机制能够为科学决策过程提供洞察和控制，证明了其在提高AI辅助科学发现能力方面的潜力。

Abstract: The capacity for artificial intelligence (AI) to formulate, evolve, and test
altered thought patterns under dynamic conditions indicates advanced cognition
that is crucial for scientific discovery. The existing AI development landscape
falls into two categories: 1) frameworks over non-reasoning models that
natively incorporate opinions on how humans think, and 2) reasoning models that
abstract precise control of the reasoning intuition away from end users. While
powerful, for scientists to maximize utility of AI in scientific discovery,
they not only require accuracy and transparency in reasoning, but also
steerability. Hence, we introduce an alternative approach that enables deep and
precise control over the reasoning process called: a cognitive loop via in-situ
optimization (CLIO). CLIO enables large language models (LLMs) to
self-formulate ways of approaching a problem, adapt behavior when
self-confidence is low, and ultimately provide scientists with a final belief
or answer. Through CLIO's open design, scientists can observe uncertainty
levels, understand how final belief states are formulated using graph
structures, and interject corrections. Without any further post-training,
OpenAI's GPT-4.1 with CLIO yields an accuracy of 22.37\% in text-based biology
and medicine questions on Humanity's Last Exam (HLE). This yields a 13.82\% net
or 161.64\% relative increase when compared to the base GPT-4.1 model and
surpasses OpenAI's o3 performance in high and low reasoning effort modes. We
further discovered that oscillations within internal uncertainty measures are
key in determining the accuracy of CLIO's results, revealing how its open
design and internal mechanisms can provide insight and control into scientific
decision-making processes.

</details>


### [6] [A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering](https://arxiv.org/abs/2508.02841)
*Ziruo Yi,Jinyu Liu,Ting Xiao,Mark V. Albert*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体系统（MAS），通过专门的智能体协同工作，解决了放射科视觉问答（RVQA）中基于多模态大语言模型（MLLMs）和检索增强生成（RAG）方法存在的准确性、幻觉和跨模态错位问题，显著提高了RVQA的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于MLLMs和RAG的方法在RVQA中取得了进展，但它们在事实准确性、幻觉和跨模态错位方面仍面临挑战。现有方法难以支持RVQA所需的复杂推理，从而限制了其在减轻放射科医生工作量方面的潜力。

Method: 引入了一个多智能体系统（MAS），该系统包含专门的智能体，分别负责上下文理解、多模态推理和答案验证。通过模型分歧过滤，构建了一个具有挑战性的RVQA数据集，用于评估系统。在该数据集上与强大的MLLM基线进行了广泛的实验。

Result: 实验结果表明，该系统在性能上优于强大的MLLM基线，并展现出卓越的有效性。案例研究进一步证明了其可靠性和可解释性。

Conclusion: 这项工作强调了多智能体方法在支持需要复杂推理的可解释和可信赖临床AI应用方面的巨大潜力。

Abstract: Radiology visual question answering (RVQA) provides precise answers to
questions about chest X-ray images, alleviating radiologists' workload. While
recent methods based on multimodal large language models (MLLMs) and
retrieval-augmented generation (RAG) have shown promising progress in RVQA,
they still face challenges in factual accuracy, hallucinations, and cross-modal
misalignment. We introduce a multi-agent system (MAS) designed to support
complex reasoning in RVQA, with specialized agents for context understanding,
multimodal reasoning, and answer validation. We evaluate our system on a
challenging RVQA set curated via model disagreement filtering, comprising
consistently hard cases across multiple MLLMs. Extensive experiments
demonstrate the superiority and effectiveness of our system over strong MLLM
baselines, with a case study illustrating its reliability and interpretability.
This work highlights the potential of multi-agent approaches to support
explainable and trustworthy clinical AI applications that require complex
reasoning.

</details>


### [7] [Seemingly Simple Planning Problems are Computationally Challenging: The Countdown Game](https://arxiv.org/abs/2508.02900)
*Michael Katz,Harsha Kokel,Sarath Sreedharan*

Main category: cs.AI

TL;DR: 该论文提出了一个基于“倒计时”游戏的新型规划基准，用于评估基础模型和智能体的长期规划能力，并证明其对现有LLM方法极具挑战性。


<details>
  <summary>Details</summary>
Motivation: 当前基础模型和智能体的主要限制是无法形成长期规划，而现有规划基准不足以真正衡量其规划能力。大多数现有基准要么难以形式化和验证，要么是为测试现有自动化规划器弱点而设计。

Method: 提出了一种基于“倒计时”（Countdown）游戏的规划基准创建程序。该游戏要求玩家通过算术运算从给定数字列表中形成目标数字。该方法被证明具有直观的自然语言描述、计算挑战性（NP完全）和丰富的实例空间（避免记忆化）。论文进行了广泛的理论分析，确立了计算复杂性，并演示了其实例生成程序相对于公共基准的优势。

Result: 理论分析证实了“倒计时”问题的计算复杂性（NP完全性）。实验结果表明，与24点游戏（倒计时特例）等其他领域不同，所提出的动态基准对现有基于LLM的规划方法仍然极具挑战性。

Conclusion: 基于“倒计时”游戏的新型规划基准能够有效且严苛地评估基础模型和智能体的长期规划能力，揭示了现有LLM辅助规划方法在复杂规划任务上的显著局限性。

Abstract: There is a broad consensus that the inability to form long-term plans is one
of the key limitations of current foundational models and agents. However, the
existing planning benchmarks remain woefully inadequate to truly measure their
planning capabilities. Most existing benchmarks either focus on loosely defined
tasks like travel planning or end up leveraging existing domains and problems
from international planning competitions. While the former tasks are hard to
formalize and verify, the latter were specifically designed to test and
challenge the weaknesses of existing automated planners. To address these
shortcomings, we propose a procedure for creating a planning benchmark centered
around the game called Countdown, where a player is expected to form a target
number from a list of input numbers through arithmetic operations. We discuss
how this problem meets many of the desiderata associated with an ideal
benchmark for planning capabilities evaluation. Specifically, the domain allows
for an intuitive, natural language description for each problem instance, it is
computationally challenging (NP-complete), and the instance space is rich
enough that we do not have to worry about memorization. We perform an extensive
theoretical analysis, establishing the computational complexity result and
demonstrate the advantage of our instance generation procedure over public
benchmarks. We evaluate a variety of existing LLM-assisted planning methods on
instances generated using our procedure. Our results show that, unlike other
domains like 24 Game (a special case of Countdown), our proposed dynamic
benchmark remains extremely challenging for existing LLM-based approaches.

</details>


### [8] [Enhancing Japanese Large Language Models with Reasoning Vectors](https://arxiv.org/abs/2508.02913)
*Carolina Minami Oguchi,Leo Wei,Koyo Kobayashi,Hsin-Tai Wu,Dipak Ghosal*

Main category: cs.AI

TL;DR: 针对资源受限的日语大型语言模型，提出一种从推理LLM中提取“推理向量”并应用于日语LLM的后训练方法，以提升其性能和推理能力。


<details>
  <summary>Details</summary>
Motivation: 主流LLM的后训练方法能提升性能和推理能力，但由于资源限制，对日语LLM而言难以实现。

Method: 受任务向量（提取训练前后权重变化）启发，从推理LLM中获取“推理向量”，并将其应用于日语LLM。

Result: 该方法为提升日语LLM性能提供了一种简单有效的方式，取得了显著的改进。

Conclusion: 该方法成功提升了日语LLM的性能，并有望为其他面临类似资源挑战的语言提供启发。

Abstract: Post-training methods have improved the performance and enhanced the
reasoning capability for mainstream large language models (LLMs), but the same
is challenging for Japanese LLMs to achieve due to the amount of resources
required. Inspired by task vectors that extract the change of weights before
and after training, specifically for a certain task, we obtain reasoning
vectors from reasoning LLMs and apply them to Japanese LLMs to boost their
performance. While the resources available present a challenge to improve
Japanese LLMs, we present a simple and effective way to obtain high improvement
and hope to inspire for other languages.

</details>


### [9] [PentestJudge: Judging Agent Behavior Against Operational Requirements](https://arxiv.org/abs/2508.02921)
*Shane Caldwell,Max Harley,Michael Kouremetis,Vincent Abruzzo,Will Pearce*

Main category: cs.AI

TL;DR: PentestJudge是一个基于LLM的评估系统，用于衡量渗透测试代理的操作，通过将复杂任务分解为可判定的标准，并与人类专家对比，发现其在评估AI安全代理过程质量方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以程序化地评估渗透测试代理的复杂操作，需要一种更实用、可扩展且全面的方法来评估AI安全代理的过程质量，以便在敏感生产环境中安全使用。

Method: 引入PentestJudge系统，利用大型语言模型（LLM）作为评判者，并赋予其工具访问权限以分析代理状态和工具调用历史。开发了树状结构的评估标准（rubrics），将渗透测试任务分解为层级化的子任务，直到叶节点为简单的“是/否”判断标准。这些标准涵盖操作目标、操作安全和技术技巧。将LLM的评分与人类领域专家的评分进行比较，使用F1分数等标准二分类指标评估其相对性能。

Result: 最佳的LLM评判模型达到了0.83的F1分数。发现工具使用能力更强的模型与人类专家表现更接近。即使总体分数相似，不同模型在不同类型的问题上仍存在差异。较弱和更便宜的模型能够评估更强大和昂贵模型执行的渗透测试轨迹。

Conclusion: 验证（评估）渗透测试任务可能比生成（执行）更容易。该方法有助于未来研究，以全面和可扩展地评估基于AI的信息安全代理的过程质量，从而使其能够自信地应用于敏感生产环境。

Abstract: We introduce PentestJudge, a system for evaluating the operations of
penetration testing agents. PentestJudge is a large language model
(LLM)-as-judge with access to tools that allow it to consume arbitrary
trajectories of agent states and tool call history to determine whether a
security agent's actions meet certain operating criteria that would be
impractical to evaluate programmatically. We develop rubrics that use a tree
structure to hierarchically collapse the penetration testing task for a
particular environment into smaller, simpler, and more manageable sub-tasks and
criteria until each leaf node represents simple yes-or-no criteria for
PentestJudge to evaluate. Task nodes are broken down into different categories
related to operational objectives, operational security, and tradecraft.
LLM-as-judge scores are compared to human domain experts as a ground-truth
reference, allowing us to compare their relative performance with standard
binary classification metrics, such as F1 scores. We evaluate several frontier
and open-source models acting as judge agents, with the best model reaching an
F1 score of 0.83. We find models that are better at tool-use perform more
closely to human experts. By stratifying the F1 scores by requirement type, we
find even models with similar overall scores struggle with different types of
questions, suggesting certain models may be better judges of particular
operating criteria. We find that weaker and cheaper models can judge the
trajectories of pentests performed by stronger and more expensive models,
suggesting verification may be easier than generation for the penetration
testing task. We share this methodology to facilitate future research in
understanding the ability of judges to holistically and scalably evaluate the
process quality of AI-based information security agents so that they may be
confidently used in sensitive production environments.

</details>


### [10] [AQUAH: Automatic Quantification and Unified Agent in Hydrology](https://arxiv.org/abs/2508.02936)
*Songkun Yan,Zhi Li,Siyu Zhu,Yixin Wen,Mofan Zhang,Mengye Chen,Jie Cao,Yang Hong*

Main category: cs.AI

TL;DR: AQUAH是首个端到端、基于语言的水文建模智能体，能根据自然语言指令自动完成数据检索、模型配置、模拟运行并生成自包含的PDF报告。


<details>
  <summary>Details</summary>
Motivation: 旨在简化复杂环境建模流程，降低地球观测数据、物理工具与决策者之间的使用门槛，实现水文模拟的自动化和智能化。

Method: AQUAH利用支持视觉的大型语言模型（LLM），解释地图和栅格数据，自主执行数据检索、水文模型配置、模拟运行，并生成PDF报告。LLM还指导关键决策，如出口选择、参数初始化和不确定性评估。

Result: 在美国多个流域的初步实验表明，AQUAH无需人工干预即可完成冷启动模拟并生成分析师可用的文档。水文学家评价其结果清晰、透明且符合物理规律。

Conclusion: 早期成果突出显示了以LLM为中心、以视觉为基础的智能体在简化复杂环境建模方面的巨大潜力，并能有效连接地球观测数据、物理工具和决策者，尽管仍需进一步校准和验证以实现操作部署。

Abstract: We introduce AQUAH, the first end-to-end language-based agent designed
specifically for hydrologic modeling. Starting from a simple natural-language
prompt (e.g., 'simulate floods for the Little Bighorn basin from 2020 to
2022'), AQUAH autonomously retrieves the required terrain, forcing, and gauge
data; configures a hydrologic model; runs the simulation; and generates a
self-contained PDF report. The workflow is driven by vision-enabled large
language models, which interpret maps and rasters on the fly and steer key
decisions such as outlet selection, parameter initialization, and uncertainty
commentary. Initial experiments across a range of U.S. basins show that AQUAH
can complete cold-start simulations and produce analyst-ready documentation
without manual intervention. The results are judged by hydrologists as clear,
transparent, and physically plausible. While further calibration and validation
are still needed for operational deployment, these early outcomes highlight the
promise of LLM-centered, vision-grounded agents to streamline complex
environmental modeling and lower the barrier between Earth observation data,
physics-based tools, and decision makers.

</details>


### [11] [MedBLINK: Probing Basic Perception in Multimodal Language Models for Medicine](https://arxiv.org/abs/2508.02951)
*Mahtab Bigverdi,Wisdom Ikezogwo,Kevin Zhang,Hyewon Jeong,Mingyu Lu,Sungjae Cho,Linda Shapiro,Ranjay Krishna*

Main category: cs.AI

TL;DR: 多模态语言模型在医疗领域有潜力，但Medblink基准测试显示它们在基本的医学图像感知任务上表现不佳，远低于人类水平。


<details>
  <summary>Details</summary>
Motivation: 临床医生对AI工具的采用非常谨慎，如果模型在简单的感知任务（如图像方向、CT增强识别）上出错，将难以被采纳。因此，需要评估这些模型的基本感知能力。

Method: 引入Medblink基准测试，包含8项临床有意义的任务，涵盖多种成像模态和解剖区域，共1429个多项选择题和1605张图像。评估了19个最先进的多模态语言模型（包括通用和领域专用模型），并与人类标注员的表现进行比较。

Result: 人类标注员的准确率达到96.4%，而表现最好的模型仅达到65%。这表明当前的多模态语言模型在常规感知检查中频繁失败。

Conclusion: 当前的多模态语言模型在基本的感知任务上表现不足，需要加强其视觉基础能力，以支持在临床环境中的实际应用和采纳。

Abstract: Multimodal language models (MLMs) show promise for clinical decision support
and diagnostic reasoning, raising the prospect of end-to-end automated medical
image interpretation. However, clinicians are highly selective in adopting AI
tools; a model that makes errors on seemingly simple perception tasks such as
determining image orientation or identifying whether a CT scan is
contrast-enhance are unlikely to be adopted for clinical tasks. We introduce
Medblink, a benchmark designed to probe these models for such perceptual
abilities. Medblink spans eight clinically meaningful tasks across multiple
imaging modalities and anatomical regions, totaling 1,429 multiple-choice
questions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including
general purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,
LLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the
best-performing model reaches only 65%. These results show that current MLMs
frequently fail at routine perceptual checks, suggesting the need to strengthen
their visual grounding to support clinical adoption. Data is available on our
project page.

</details>


### [12] [Polymath: A Self-Optimizing Agent with Dynamic Hierarchical Workflow](https://arxiv.org/abs/2508.02959)
*Chia-Tung Ho,Jing Gong,Xufeng Yao,Yunsheng Bai,Abhishek B Akkur,Haoxing Ren*

Main category: cs.AI

TL;DR: 本文提出Polymath，一个自优化智能体，利用动态分层工作流和无标签数据优化方法，显著提升了大型语言模型（LLMs）在多种任务上的表现，平均超越现有SOTA基线8.1%。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通过文本接口手动嵌入到代理系统（如Chain-of-Thought、ReACT）中，限制了可扩展性和效率。现有自动化工作流生成和优化方法依赖于标注数据集，这在缺乏标注数据的真实世界动态问题中是无效且不灵活的。

Method: 引入Polymath，一个具有动态分层工作流的自优化智能体。它利用任务流图的灵活性和代码表示工作流的表达能力。其优化方法整合了受多网格启发的图优化与自反思引导的进化算法，从而在无需标注数据的情况下优化工作流。

Result: 在编码、数学和多轮问答等六个基准数据集上的实验结果表明，Polymath比现有最先进的基线平均提高了8.1%。

Conclusion: Polymath通过其动态分层工作流和创新的无标注数据优化方法，成功解决了构建通用LLM代理的扩展性和效率限制，并在真实世界动态问题上展现出卓越的性能提升。

Abstract: Large language models (LLMs) excel at solving complex tasks by executing
agentic workflows composed of detailed instructions and structured operations.
Yet, building general-purpose agents by manually embedding foundation models
into agentic systems such as Chain-of-Thought, Self-Reflection, and ReACT
through text interfaces limits scalability and efficiency. Recently, many
researchers have sought to automate the generation and optimization of these
workflows through code-based representations. However, existing methods often
rely on labeled datasets to train and optimize workflows, making them
ineffective and inflexible for solving real-world, dynamic problems where
labeled data is unavailable. To address this challenge, we introduce Polymath,
a self-optimizing agent with dynamic hierarchical workflow that leverages the
flexibility of task flow graphs and the expressiveness of code-represented
workflows to solve a wide range of real-world, dynamic problems. The proposed
optimization methodology integrates multi-grid-inspired graph optimization with
a self-reflection-guided evolutionary algorithm to refine workflows without
labeled data. Experimental results on six benchmark datasets across coding,
math, and multi-turn QA tasks show that Polymath achieves 8.1% average
improvement over state-of-the-art baselines.

</details>


### [13] [Defend LLMs Through Self-Consciousness](https://arxiv.org/abs/2508.02961)
*Boshi Huang,Fabio Nonato de Paula*

Main category: cs.AI

TL;DR: 本文提出一种基于大语言模型（LLM）内在推理能力的“自我意识”防御机制，以对抗提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: 传统的提示注入防御方法依赖外部分类器，本文旨在利用LLM固有的推理能力实现自我保护，提高LLM的伦理安全性。

Method: 引入了一个包含元认知（Meta-Cognitive）和仲裁（Arbitration）模块的框架，使LLM能够自主评估和调节其输出。该方法在七个先进的LLM上，使用AdvBench和Prompt-Injection-Mixed-Techniques-2024两个数据集进行了评估。

Result: 实验结果表明，防御成功率显著提高，在增强模式下，部分模型达到了完美或接近完美的防御效果。研究还分析了防御成功率提升与计算开销之间的权衡。

Conclusion: 这种“自我意识”方法为增强LLM的伦理提供了轻量级、经济高效的解决方案，特别适用于各种平台上的生成式AI应用。

Abstract: This paper introduces a novel self-consciousness defense mechanism for Large
Language Models (LLMs) to combat prompt injection attacks. Unlike traditional
approaches that rely on external classifiers, our method leverages the LLM's
inherent reasoning capabilities to perform self-protection. We propose a
framework that incorporates Meta-Cognitive and Arbitration Modules, enabling
LLMs to evaluate and regulate their own outputs autonomously. Our approach is
evaluated on seven state-of-the-art LLMs using two datasets: AdvBench and
Prompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate
significant improvements in defense success rates across models and datasets,
with some achieving perfect and near-perfect defense in Enhanced Mode. We also
analyze the trade-off between defense success rate improvement and
computational overhead. This self-consciousness method offers a lightweight,
cost-effective solution for enhancing LLM ethics, particularly beneficial for
GenAI use cases across various platforms.

</details>


### [14] [Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling](https://arxiv.org/abs/2508.02979)
*Peng Ding,Rick Stevens*

Main category: cs.AI

TL;DR: 提出了一种统一的LLM工具集成方法，旨在解决现有生态系统的碎片化问题，并通过自动化和优化提升开发效率和执行性能。


<details>
  <summary>Details</summary>
Motivation: 当前的工具增强型大型语言模型（LLMs）生态系统碎片化严重，开发者需要处理多种协议、手动定义模式和复杂的执行流程。

Method: 提出了一种统一的、协议无关的工具集成方法，包括自动化模式生成、双模式并发执行以及无缝多源工具管理。

Result: 实验结果显示，集成场景下的代码量减少了60-80%，通过优化并发性能提升高达3.1倍，并与现有函数调用标准完全兼容。

Conclusion: 该工作为工具集成架构提供了理论见解，并为LLM应用开发提供了实用的解决方案。

Abstract: The proliferation of tool-augmented Large Language Models (LLMs) has created
a fragmented ecosystem where developers must navigate multiple protocols,
manual schema definitions, and complex execution workflows. We address this
challenge by proposing a unified approach to tool integration that abstracts
protocol differences while optimizing execution performance. Our solution
demonstrates how protocol-agnostic design principles can significantly reduce
development overhead through automated schema generation, dual-mode concurrent
execution, and seamless multi-source tool management. Experimental results show
60-80% code reduction across integration scenarios, performance improvements up
to 3.1x through optimized concurrency, and full compatibility with existing
function calling standards. This work contributes both theoretical insights
into tool integration architecture and practical solutions for real-world LLM
application development.

</details>


### [15] [When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs](https://arxiv.org/abs/2508.02994)
*Fangyi Yu*

Main category: cs.AI

TL;DR: 这篇综述探讨了利用AI代理作为评估者（“代理即法官”）来解决大型语言模型（LLMs）评估瓶颈的新范式，分析了其演变、优缺点、应用领域、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型能力和自主性的增强，尤其是在开放式和复杂任务中，评估其输出成为一个关键瓶颈。需要可扩展且细致的替代方案来取代人工评估。

Method: 本文作为一篇综述，首先定义了“代理即法官”的概念，追溯了其从单模型评估到动态多代理辩论框架的演变。随后，批判性地审视了这些方法的优缺点，并在可靠性、成本和人类对齐方面进行了比较，同时调查了其在医学、法律、金融和教育等领域的实际部署。最后，指出了面临的挑战（如偏见、鲁棒性和元评估），并展望了未来的研究方向。

Result: “代理即法官”方法利用LLM的推理和视角采纳能力，为LLM评估提供了可扩展和细致的替代方案。该范式已从单一模型发展到多代理辩论框架，并在多个实际领域得到应用。尽管存在挑战，但它能够补充（而非取代）人工监督。

Conclusion: 代理即法官”方法标志着LLM评估迈向可信赖、可扩展的一步，能够有效补充人类监督，但不能完全取代之。

Abstract: As large language models (LLMs) grow in capability and autonomy, evaluating
their outputs-especially in open-ended and complex tasks-has become a critical
bottleneck. A new paradigm is emerging: using AI agents as the evaluators
themselves. This "agent-as-a-judge" approach leverages the reasoning and
perspective-taking abilities of LLMs to assess the quality and safety of other
models, promising calable and nuanced alternatives to human evaluation. In this
review, we define the agent-as-a-judge concept, trace its evolution from
single-model judges to dynamic multi-agent debate frameworks, and critically
examine their strengths and shortcomings. We compare these approaches across
reliability, cost, and human alignment, and survey real-world deployments in
domains such as medicine, law, finance, and education. Finally, we highlight
pressing challenges-including bias, robustness, and meta evaluation-and outline
future research directions. By bringing together these strands, our review
demonstrates how agent-based judging can complement (but not replace) human
oversight, marking a step toward trustworthy, scalable evaluation for
next-generation LLMs.

</details>


### [16] [AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots](https://arxiv.org/abs/2508.02999)
*Xinjie Zhao,Moritz Blum,Fan Gao,Yingjian Chen,Boming Yang,Luis Marquez-Carpintero,Mónica Pina-Navarro,Yanran Fu,So Morikawa,Yusuke Iwasawa,Yutaka Matsuo,Chanjun Park,Irene Li*

Main category: cs.AI

TL;DR: AGENTiGraph是一个用户友好的代理驱动系统，通过自然语言操作知识图谱，使非技术用户能够直观地管理领域特定数据，并支持多轮对话和动态更新，无需专业查询语言。


<details>
  <summary>Details</summary>
Motivation: 现有知识库管理系统对非技术用户不友好，需要专业查询语言，难以实现直观交互、增量构建和动态更新。该研究旨在为非技术用户提供一个完整的、可视化的解决方案，以自然语言方式管理和精炼知识库。

Method: AGENTiGraph采用代理驱动设计，包含意图分类、任务规划和自动知识集成，以确保不同任务之间的无缝推理。它通过自然语言处理和知识图谱操作实现多轮对话和动态更新。

Result: 在教育场景中，系统在3500个查询的基准测试中表现优异，分类准确率达到95.12%，执行成功率达到90.45%，显著优于强大的零样本基线。这表明其在法律和医疗等合规性关键或多步骤查询领域具有潜在的可扩展性。

Conclusion: AGENTiGraph为多轮企业知识管理提供了一种强大的新范式，有效连接了大型语言模型（LLMs）和结构化图谱。其在教育场景的成功表现预示着在法律和医疗等复杂领域具有广泛应用前景，能够动态整合新信息。

Abstract: AGENTiGraph is a user-friendly, agent-driven system that enables intuitive
interaction and management of domain-specific data through the manipulation of
knowledge graphs in natural language. It gives non-technical users a complete,
visual solution to incrementally build and refine their knowledge bases,
allowing multi-round dialogues and dynamic updates without specialized query
languages. The flexible design of AGENTiGraph, including intent classification,
task planning, and automatic knowledge integration, ensures seamless reasoning
between diverse tasks. Evaluated on a 3,500-query benchmark within an
educational scenario, the system outperforms strong zero-shot baselines
(achieving 95.12% classification accuracy, 90.45% execution success),
indicating potential scalability to compliance-critical or multi-step queries
in legal and medical domains, e.g., incorporating new statutes or research on
the fly. Our open-source demo offers a powerful new paradigm for multi-turn
enterprise knowledge management that bridges LLMs and structured graphs.

</details>


### [17] [Beyond Policy Optimization: A Data Curation Flywheel for Sparse-Reward Long-Horizon Planning](https://arxiv.org/abs/2508.03018)
*Yutong Wang,Pengliang Ji,Kaixin Li,Baolong Bi,Tao Feng,Guillaume Sartoretti*

Main category: cs.AI

TL;DR: BPO是一个三阶段自改进框架，通过规划四元数、课程学习和奖励门控拒绝采样，解决了大型语言模型在稀疏奖励交互式环境中进行多轮智能体规划时面临的信用分配和计算开销问题，实现了SOTA性能和高token效率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在静态任务上表现出色，但在交互式环境中的多轮智能体规划面临两大挑战：1) 难以解决的信用分配问题导致传统强化学习在稀疏奖励设置下失效；2) 冗长、逐步的推理历史导致计算开销过大。

Method: 本文提出了BPO框架，包含三个阶段：1) 自举（bootstrapping）：使用规划四元数（planning quaternions）结合长短链式思考（long-short chain-of-thought fusion）来引导高效推理；2) 外推（extrapolation）：通过复杂度分层课程学习（complexity-stratified curriculum learning）将模型泛化到分布外任务；3) 精炼（refinement）：通过奖励门控拒绝采样（reward-gated rejection sampling）选择高质量经验进行迭代学习，实现模型自我改进。

Result: 在ALFWorld、ScienceWorld和WebShop上的实验表明，BPO方法实现了最先进的性能（state-of-the-art），并显著提高了token效率。

Conclusion: BPO为智能体规划中的推理模型提供了一种新的范式，有效解决了长周期、稀疏奖励环境中大型语言模型面临的挑战。

Abstract: Large Language Reasoning Models have demonstrated remarkable success on
static tasks, yet their application to multi-round agentic planning in
interactive environments faces two fundamental challenges. First, the
intractable credit assignment problem renders conventional reinforcement
learning ineffective in sparse-reward settings. Second, the computational
overhead of verbose, step-by-step reasoning histories is prohibitive. To
address these challenges, we propose BPO, a three-stage framework
(bootstrapping, extrapolation, and refinement) that establishes a
self-improving data flywheel to develop robust reasoning models for
long-horizon, sparse-reward environments. Our framework first bootstraps
efficient reasoning using the proposed planning quaternions with long-short
chain-of-thought fusion. It then extrapolates to out-of-distribution tasks
through complexity-stratified curriculum learning. Finally, the model
iteratively refines itself by learning exclusively on experiences selected via
reward-gated rejection sampling. Experiments on ALFWorld, ScienceWorld, and
WebShop demonstrate that our approach achieves state-of-the-art with
significant token efficiency, providing a new recipe for reasoning models in
agentic planning.

</details>


### [18] [Collab-Solver: Collaborative Solving Policy Learning for Mixed-Integer Linear Programming](https://arxiv.org/abs/2508.03030)
*Siyuan Li,Yifan Yu,Yanchen Deng,Zhihao Zhang,Mengjing Chen,Fangzhou Zhu,Tao Zhong,Jianye Hao,Peng Liu,Bo An*

Main category: cs.AI

TL;DR: 该论文提出了一种名为Collab-Solver的新型多智能体协作策略学习框架，用于优化混合整数线性规划（MILP）求解器中的不同模块（如割平面选择和分支），以解决现有学习方法忽略模块间相互依赖的问题，从而显著提高求解速度、质量和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 混合整数线性规划（MILP）是组合优化中的一个基本问题。传统的MILP求解器依赖于大量硬编码的启发式方法。尽管基于学习的MILP方法前景广阔，但现有工作独立地训练MILP求解器中各个模块的策略，忽略了它们之间的相互依赖性，严重影响了求解速度和质量。

Method: 本文提出了一种新颖的基于多智能体的MILP策略学习框架Collab-Solver，旨在协同优化多个模块的策略。具体而言，将MILP求解中割平面选择和分支的协作建模为Stackelberg博弈。在此框架下，开发了一个两阶段学习范式来稳定协作策略学习：第一阶段实现数据通信的策略预训练，第二阶段进一步协调各模块的策略学习。

Result: 联合学习的策略显著提高了在合成和大规模真实世界MILP数据集上的求解性能。此外，Collab-Solver学习到的策略在不同实例集上也表现出卓越的泛化能力。

Conclusion: Collab-Solver框架通过将MILP求解中的关键模块（如割平面选择和分支）的策略学习建模为协作博弈，并采用两阶段学习范式，有效解决了现有学习方法中模块间独立优化的问题，从而显著提升了MILP的求解性能和泛化能力。

Abstract: Mixed-integer linear programming (MILP) has been a fundamental problem in
combinatorial optimization. Previous works have designed a plethora of
hard-coded heuristics to accomplish challenging MILP solving with domain
knowledge. Driven by the high capability of neural networks, recent research is
devoted to replacing manually designed heuristics with learned policies.
Although learning-based MILP methods have shown great promise, existing
worksindependentlytreatthepolicylearningineachmoduleofMILPsolvers without
considering their interdependence, severely hurting the solving speed and
quality. To address this issue, we propose a novel multi-agent-based policy
learning framework for MILP (Collab-Solver), which can collaboratively optimize
the policies for multiple modules. Specifically, we formulate the collaboration
of cut selection and branching in MILP solving as a Stackelberg game. Under
this formulation, we develop a two-phase learning paradigm to stabilize the
collaborative policy learning, where the first phase achieves the
data-communicated policy pretraining and the second phase further orchestrates
the policy learning for various modules. The jointly learned policy
significantly improves the solving performance on both synthetic and
large-scale real-world MILP datasets. Moreover, the policies learned by
Collab-Solver have also demonstrated excellent generalization abilities across
different instance sets.

</details>


### [19] [From Text to Trajectories: GPT-2 as an ODE Solver via In-Context](https://arxiv.org/abs/2508.03031)
*Ziyang Ma,Baojian Zhou,Deqing Yang,Yanghua Xiao*

Main category: cs.AI

TL;DR: 研究了大型语言模型（LLMs）在语境学习（ICL）设置下解决常微分方程（ODEs）的能力，发现GPT-2能有效学习并展现出高精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 语境学习（ICL）在大型语言模型（LLMs）中的非线性行为及其底层机制尚不清楚，尤其是在自然语言处理（NLP）任务中。本文旨在通过让LLMs解决常微分方程（ODEs）来深入理解ICL的工作原理。

Method: 将标准常微分方程问题及其解格式化为顺序提示。使用GPT-2模型进行评估，并在两种类型的常微分方程上进行实验。

Result: GPT-2模型能够有效地学习一种元ODE算法，其收敛行为与欧拉方法相当或更优。随着演示数量的增加，模型精度呈指数级提高。此外，模型对分布外（OOD）问题也表现出良好的泛化能力和鲁棒的推断能力。

Conclusion: 这些实证发现为自然语言处理中语境学习的机制及其解决非线性数值问题的潜力提供了新的见解。

Abstract: In-Context Learning (ICL) has emerged as a new paradigm in large language
models (LLMs), enabling them to perform novel tasks by conditioning on a few
examples embedded in the prompt. Yet, the highly nonlinear behavior of ICL for
NLP tasks remains poorly understood. To shed light on its underlying
mechanisms, this paper investigates whether LLMs can solve ordinary
differential equations (ODEs) under the ICL setting. We formulate standard ODE
problems and their solutions as sequential prompts and evaluate GPT-2 models on
these tasks. Experiments on two types of ODEs show that GPT-2 can effectively
learn a meta-ODE algorithm, with convergence behavior comparable to, or better
than, the Euler method, and achieve exponential accuracy gains with increasing
numbers of demonstrations. Moreover, the model generalizes to
out-of-distribution (OOD) problems, demonstrating robust extrapolation
capabilities. These empirical findings provide new insights into the mechanisms
of ICL in NLP and its potential for solving nonlinear numerical problems.

</details>


### [20] [Tree-of-Reasoning: Towards Complex Medical Diagnosis via Multi-Agent Reasoning with Evidence Tree](https://arxiv.org/abs/2508.03038)
*Qi Peng,Jialin Cui,Jiayuan Xie,Yi Cai,Qing Li*

Main category: cs.AI

TL;DR: 针对LLMs在复杂医疗诊断中推理深度不足的问题，本文提出了Tree-of-Reasoning (ToR)多智能体框架，通过树状推理路径记录和交叉验证机制，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLMs）在处理复杂现实世界医疗诊断任务时表现不足，主要原因是缺乏足够的推理深度，导致处理大量专业医疗数据时信息丢失或逻辑跳跃，进而引发诊断错误。

Method: 提出了Tree-of-Reasoning (ToR)多智能体框架，旨在处理复杂医疗场景。ToR引入了一个树状结构，用于清晰记录LLMs的推理路径和相应的临床证据。同时，提出了一种交叉验证机制，以确保多智能体决策的一致性，从而提高在复杂医疗场景下的临床推理能力。

Result: 在真实世界医疗数据上的实验结果表明，该框架能够比现有基线方法取得更好的性能。

Conclusion: ToR框架通过增强推理深度和多智能体决策一致性，有效提升了LLMs在复杂医疗诊断任务中的临床推理能力和性能。

Abstract: Large language models (LLMs) have shown great potential in the medical
domain. However, existing models still fall short when faced with complex
medical diagnosis task in the real world. This is mainly because they lack
sufficient reasoning depth, which leads to information loss or logical jumps
when processing a large amount of specialized medical data, leading to
diagnostic errors. To address these challenges, we propose Tree-of-Reasoning
(ToR), a novel multi-agent framework designed to handle complex scenarios.
Specifically, ToR introduces a tree structure that can clearly record the
reasoning path of LLMs and the corresponding clinical evidence. At the same
time, we propose a cross-validation mechanism to ensure the consistency of
multi-agent decision-making, thereby improving the clinical reasoning ability
of multi-agents in complex medical scenarios. Experimental results on
real-world medical data show that our framework can achieve better performance
than existing baseline methods.

</details>


### [21] [Beyond Surface-Level Detection: Towards Cognitive-Driven Defense Against Jailbreak Attacks via Meta-Operations Reasoning](https://arxiv.org/abs/2508.03054)
*Rui Pu,Chaozhuo Li,Rui Ha,Litian Zhang,Lirong Qiu,Xi Zhang*

Main category: cs.AI

TL;DR: 本文提出认知驱动防御（CDD）框架，通过模拟人类认知推理链和应用元操作来防御大型语言模型（LLMs）的越狱攻击，结合监督微调和熵引导强化学习以实现对未知攻击的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法多依赖浅层模式匹配，难以泛化到新颖的越狱攻击策略，而LLMs的安全可靠部署需要更鲁棒的防御机制。

Method: CDD框架通过应用元操作（隐藏恶意意图的基本操作）来针对越狱提示的底层结构。它模拟人类认知推理，包括全局感知和局部分析以揭示隐藏操作。通过对结构化推理链进行监督微调，模型学习识别已知操作模式。为增强对未知威胁的泛化能力，引入熵引导强化学习算法（EG-GRPO）鼓励探索新类型的元操作。

Result: 实验证明，CDD能够实现最先进的防御性能，并对未见的越狱攻击表现出强大的泛化能力。

Conclusion: CDD框架通过其独特的认知推理和学习机制，为防御LLMs越狱攻击提供了一种高效且泛化能力强的解决方案。

Abstract: Defending large language models (LLMs) against jailbreak attacks is essential
for their safe and reliable deployment. Existing defenses often rely on shallow
pattern matching, which struggles to generalize to novel and unseen attack
strategies. To address this challenge, we propose the Cognitive-Driven Defense
(CDD) framework, which targets the underlying structure of jailbreak prompts by
applying meta-operations, defined as basic manipulations that conceal harmful
intent.CDD emulates human cognitive reasoning through a structured reasoning
chain. It begins with a global perception of the prompt and follows with a
localized analysis to uncover hidden manipulations. By applying supervised
fine-tuning on this structured chain, the model learns to identify and reason
about known manipulation patterns. To enhance generalization to unseen threats,
an entropy-guided reinforcement learning algorithm (EG-GRPO) is introduced to
encourage exploration of new types and variants of meta-operations. Experiments
demonstrate that CDD can achieve state-of-the-art defense performance and
exhibit strong generalization to unseen jailbreak attacks.

</details>


### [22] [ContractEval: Benchmarking LLMs for Clause-Level Legal Risk Identification in Commercial Contracts](https://arxiv.org/abs/2508.03080)
*Shuang Liu,Zelong Li,Ruoyun Ma,Haiyan Zhao,Mengnan Du*

Main category: cs.AI

TL;DR: 本研究引入ContractEval基准，评估开源大型语言模型在合同法律风险分析中与专有模型的性能差距，发现专有模型表现更优，开源模型虽有潜力但需针对性微调。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在法律风险分析等专业领域潜力未被充分探索。随着对本地部署开源LLM以保护数据机密性的兴趣日益增长，需要评估开源LLM在法律任务中是否能与专有LLM媲美。

Method: 引入ContractEval基准，首次全面评估LLM在识别商业合同中条款级法律风险的能力。使用合同理解Atticus数据集（CUAD），评估了4个专有LLM和15个开源LLM，分析了正确性、输出有效性、推理模式、无相关条款响应频率以及模型量化的影响。

Result: 1. 专有模型在正确性和输出有效性上优于开源模型，但部分开源模型在特定维度具有竞争力。2. 更大的开源模型通常表现更好，但性能提升随模型增大而放缓。3. 推理模式（“思考”模式）提高了输出有效性，但降低了正确性。4. 开源模型即使存在相关条款也更频繁地生成“无相关条款”响应。5. 模型量化加速了推理但牺牲了性能。

Conclusion: 大多数LLM的表现与初级法律助理相当，但开源模型需要有针对性的微调以确保在关键法律环境中的正确性和有效性。ContractEval为未来法律领域LLM的开发提供了坚实的基准。

Abstract: The potential of large language models (LLMs) in specialized domains such as
legal risk analysis remains underexplored. In response to growing interest in
locally deploying open-source LLMs for legal tasks while preserving data
confidentiality, this paper introduces ContractEval, the first benchmark to
thoroughly evaluate whether open-source LLMs could match proprietary LLMs in
identifying clause-level legal risks in commercial contracts. Using the
Contract Understanding Atticus Dataset (CUAD), we assess 4 proprietary and 15
open-source LLMs. Our results highlight five key findings: (1) Proprietary
models outperform open-source models in both correctness and output
effectiveness, though some open-source models are competitive in certain
specific dimensions. (2) Larger open-source models generally perform better,
though the improvement slows down as models get bigger. (3) Reasoning
("thinking") mode improves output effectiveness but reduces correctness, likely
due to over-complicating simpler tasks. (4) Open-source models generate "no
related clause" responses more frequently even when relevant clauses are
present. This suggests "laziness" in thinking or low confidence in extracting
relevant content. (5) Model quantization speeds up inference but at the cost of
performance drop, showing the tradeoff between efficiency and accuracy. These
findings suggest that while most LLMs perform at a level comparable to junior
legal assistants, open-source models require targeted fine-tuning to ensure
correctness and effectiveness in high-stakes legal settings. ContractEval
offers a solid benchmark to guide future development of legal-domain LLMs.

</details>


### [23] [EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design](https://arxiv.org/abs/2508.03082)
*Fei Liu,Yilu Liu,Qingfu Zhang,Xialiang Tong,Mingxuan Yuan*

Main category: cs.AI

TL;DR: 该研究提出了一种新的LLM驱动的自动化启发式设计方法（AHSD），旨在生成一个互补的启发式集合，以解决现有方法仅设计单一启发式导致泛化能力差的问题，并在实验中取得了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM驱动的自动化启发式设计（AHD）方法仅设计单个启发式来服务所有问题实例，导致在不同分布或设置下泛化能力差。

Method: 提出了自动化启发式集合设计（AHSD）的新范式，旨在自动生成一个小型互补启发式集合。证明了AHSD的目标函数是单调且超模的。在此基础上，提出了启发式集合进化（EoH-S）算法，包含互补种群管理和互补感知模因搜索两种新机制，以有效地生成高质量和互补的启发式集合。

Result: 在三个AHD任务上，针对不同大小和分布的多样化实例进行综合实验，结果表明EoH-S始终优于现有的最先进AHD方法，并实现了高达60%的性能提升。

Conclusion: AHSD（通过EoH-S实现）能够有效地生成一组高质量且互补的启发式方法，以应对多样化的优化问题实例，显著优于传统的单一启发式设计方法。

Abstract: Automated Heuristic Design (AHD) using Large Language Models (LLMs) has
achieved notable success in recent years. Despite the effectiveness of existing
approaches, they only design a single heuristic to serve all problem instances,
often inducing poor generalization across different distributions or settings.
To address this issue, we propose Automated Heuristic Set Design (AHSD), a new
formulation for LLM-driven AHD. The aim of AHSD is to automatically generate a
small-sized complementary heuristic set to serve diverse problem instances,
such that each problem instance could be optimized by at least one heuristic in
this set. We show that the objective function of AHSD is monotone and
supermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the
AHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary
population management and complementary-aware memetic search, EoH-S could
effectively generate a set of high-quality and complementary heuristics.
Comprehensive experimental results on three AHD tasks with diverse instances
spanning various sizes and distributions demonstrate that EoH-S consistently
outperforms existing state-of-the-art AHD methods and achieves up to 60\%
performance improvements.

</details>


### [24] [MissDDIM: Deterministic and Efficient Conditional Diffusion for Tabular Data Imputation](https://arxiv.org/abs/2508.03083)
*Youran Zhou,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.AI

TL;DR: 针对现有基于DDPM的扩散模型在表格数据插补中推理延迟高和输出变异性大的问题，本文提出了MissDDIM，一个基于DDIM的条件扩散框架，以实现更高效和稳定的插补。


<details>
  <summary>Details</summary>
Motivation: 现有的基于随机去噪扩散概率模型（DDPM）的扩散模型在缺失数据插补方面表现出色，但其高推理延迟和不稳定的输出限制了它们在真实世界表格数据场景中的应用。

Method: 提出了MissDDIM，一个条件扩散框架，该框架将去噪扩散隐式模型（DDIM）应用于表格数据插补。

Result: 抽象中未直接给出具体结果，但暗示MissDDIM旨在解决现有DDPM方法的推理延迟和输出变异性问题。它提到随机采样虽然能生成多样化补全，但引入的输出变异性会使下游处理复杂化。

Conclusion: MissDDIM通过采用DDIM来改进表格数据插补中的扩散模型，旨在克服现有DDPM方法的高延迟和输出变异性问题。

Abstract: Diffusion models have recently emerged as powerful tools for missing data
imputation by modeling the joint distribution of observed and unobserved
variables. However, existing methods, typically based on stochastic denoising
diffusion probabilistic models (DDPMs), suffer from high inference latency and
variable outputs, limiting their applicability in real-world tabular settings.
To address these deficiencies, we present in this paper MissDDIM, a conditional
diffusion framework that adapts Denoising Diffusion Implicit Models (DDIM) for
tabular imputation. While stochastic sampling enables diverse completions, it
also introduces output variability that complicates downstream processing.

</details>


### [25] [T2UE: Generating Unlearnable Examples from Text Descriptions](https://arxiv.org/abs/2508.03091)
*Xingjun Ma,Hanxun Huang,Tianwei Song,Ye Sun,Yifeng Gao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: T2UE是一种新型框架，允许用户仅使用文本描述生成不可学习示例（UEs），从而在不暴露原始图像数据的情况下保护个人数据，解决了现有UE生成方法中的隐私悖论。


<details>
  <summary>Details</summary>
Motivation: 大规模预训练模型（如CLIP）依赖于包含私人用户数据的网络爬取数据集，引发了滥用担忧。现有不可学习示例（UEs）生成方法通常需要同时优化图像和文本的不可学习噪声，但计算成本高昂，且依赖第三方服务，导致用户必须先暴露数据才能获得保护，形成隐私悖论。

Method: 本文提出了Text-to-Unlearnable Example (T2UE)框架，通过使用文本到图像（T2I）模型将文本描述映射到图像（噪声）空间，并结合误差最小化框架来生成有效的不可学习噪声。该方法仅依赖文本描述，无需原始图像数据。

Result: 实验表明，T2UE保护的数据显著降低了最先进模型在下游任务（如跨模态检索）中的性能。此外，这种保护效果能够泛化到不同的模型架构，甚至适用于监督学习设置。

Conclusion: 该工作证明了“零接触数据保护”的可行性，即仅基于文本描述即可保护个人数据，无需直接暴露数据，从而解决了数据保护中的隐私矛盾。

Abstract: Large-scale pre-training frameworks like CLIP have revolutionized multimodal
learning, but their reliance on web-scraped datasets, frequently containing
private user data, raises serious concerns about misuse. Unlearnable Examples
(UEs) have emerged as a promising countermeasure against unauthorized model
training, employing carefully crafted unlearnable noise to disrupt the learning
of meaningful representations from protected data. Current approaches typically
generate UEs by jointly optimizing unlearnable noise for both images and their
associated text descriptions (or labels). However, this optimization process is
often computationally prohibitive for on-device execution, forcing reliance on
external third-party services. This creates a fundamental privacy paradox:
users must initially expose their data to these very services to achieve
protection, thereby compromising privacy in the process. Such a contradiction
has severely hindered the development of practical, scalable data protection
solutions. To resolve this paradox, we introduce \textbf{Text-to-Unlearnable
Example (T2UE)}, a novel framework that enables users to generate UEs using
only text descriptions. T2UE circumvents the need for original image data by
employing a text-to-image (T2I) model to map text descriptions into the image
(noise) space, combined with an error-minimization framework to produce
effective unlearnable noise. Extensive experiments show that T2UE-protected
data substantially degrades performance in downstream tasks (e.g., cross-modal
retrieval) for state-of-the-art models. Notably, the protective effect
generalizes across diverse architectures and even to supervised learning
settings. Our work demonstrates the feasibility of "zero-contact data
protection", where personal data can be safeguarded based solely on their
textual descriptions, eliminating the need for direct data exposure.

</details>


### [26] [Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework](https://arxiv.org/abs/2508.03092)
*Zikun Cui,Tianyi Huang,Chia-En Chiang,Cuiqianhe Du*

Main category: cs.AI

TL;DR: 本文提出了一种创新的可验证大型语言模型（LLM）代理，用于检测虚假信息，该代理通过与网络源互动、评估来源可信度并提供可验证的推理过程，超越了传统的二元判断。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的普及，虚假信息检测变得日益重要和复杂，传统的真/假二元判断已不足以应对挑战。

Method: 研究设计了一个LLM代理架构，包含三个核心工具：精确网络搜索工具、来源可信度评估工具和数值声明验证工具。该代理能执行多步骤验证策略、维护证据日志并形成综合评估结论。通过在FakeNewsNet等标准虚假信息数据集上进行评估，并与传统机器学习模型和LLM进行比较，评估指标包括分类指标、推理过程质量和内容重写鲁棒性。

Result: 实验结果表明，该代理在虚假信息检测准确性、推理透明度和信息重写抵抗力方面均优于基线方法。

Conclusion: 该研究为可信赖的AI辅助事实核查提供了一个新范式。

Abstract: With the proliferation of Large Language Models (LLMs), the detection of
misinformation has become increasingly important and complex. This research
proposes an innovative verifiable misinformation detection LLM agent that goes
beyond traditional true/false binary judgments. The agent actively verifies
claims through dynamic interaction with diverse web sources, assesses
information source credibility, synthesizes evidence, and provides a complete
verifiable reasoning process. Our designed agent architecture includes three
core tools: precise web search tool, source credibility assessment tool and
numerical claim verification tool. These tools enable the agent to execute
multi-step verification strategies, maintain evidence logs, and form
comprehensive assessment conclusions. We evaluate using standard misinformation
datasets such as FakeNewsNet, comparing with traditional machine learning
models and LLMs. Evaluation metrics include standard classification metrics,
quality assessment of reasoning processes, and robustness testing against
rewritten content. Experimental results show that our agent outperforms
baseline methods in misinformation detection accuracy, reasoning transparency,
and resistance to information rewriting, providing a new paradigm for
trustworthy AI-assisted fact-checking.

</details>


### [27] [AgentSME for Simulating Diverse Communication Modes in Smart Education](https://arxiv.org/abs/2508.03109)
*Wen-Xi Yang,Tian-Fang Zhao*

Main category: cs.AI

TL;DR: 本文提出了AgentSME，一个基于LLM的统一生成式智能体框架，用于智能教育，考虑了Solo、Mono和Echo三种通信模式，并发现Echo模式准确率最高，DeepSeek多样性最佳。


<details>
  <summary>Details</summary>
Motivation: 智能教育中针对性强的生成式智能体模型仍不成熟，主要挑战在于教育背景的复杂性（学习者是具有不同认知行为的人类）以及教学核心在于个性化的人际交流。

Method: 提出了AgentSME，一个由LLM驱动的统一生成式智能体框架。考虑了Solo、Mono和Echo三种通信模式，分别反映不同的智能体自主性和交流互惠性。评估指标包括准确率和三个多样性指数（评估推理内容多样性）。测试了六个LLM（分为基础能力和高能力配置）以验证通信模式的鲁棒性。

Result: 结果表明，采用Echo通信模式的生成式智能体取得了最高的准确率分数，而DeepSeek模型表现出最大的多样性。

Conclusion: 本研究为提高智能体的学习能力和启发智能教育模型提供了有价值的信息。

Abstract: Generative agent models specifically tailored for smart education are
critical, yet remain relatively underdeveloped. A key challenge stems from the
inherent complexity of educational contexts: learners are human beings with
various cognitive behaviors, and pedagogy is fundamentally centered on
personalized human-to-human communication. To address this issue, this paper
proposes AgentSME, a unified generative agent framework powered by LLM. Three
directional communication modes are considered in the models, namely Solo,
Mono, and Echo, reflecting different types of agency autonomy and communicative
reciprocity. Accuracy is adopted as the primary evaluation metric, complemented
by three diversity indices designed to assess the diversity of reasoning
contents. Six widely used LLMs are tested to validate the robustness of
communication modes across different model tiers, which are equally divided
into base-capacity and high-capacity configurations. The results show that
generative agents that employ the Echo communication mode achieve the highest
accuracy scores, while DeepSeek exhibits the greatest diversity. This study
provides valuable information to improve agent learning capabilities and
inspire smart education models.

</details>


### [28] [Toward a Trustworthy Optimization Modeling Agent via Verifiable Synthetic Data Generation](https://arxiv.org/abs/2508.03117)
*Vinicius Lima,Dzung T. Phan,Jayant Kalagnanam,Dhaval Patel,Nianjun Zhou*

Main category: cs.AI

TL;DR: 该研究提出一个框架，通过可验证的合成数据生成流程来训练用于优化建模的可信赖大型语言模型（LLM）代理，并在标准基准测试中达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 构建可信赖的LLM代理以进行优化建模（特别是线性规划和混合整数线性规划）面临挑战，需要确保生成的数据和解决方案的准确性和可验证性。

Method: 该方法包括一个可验证的合成数据生成管道，该管道从结构化符号表示开始，系统地生成自然语言描述、数学公式和可执行求解器代码。通过编程方式构建具有已知最优解的实例，确保了完全可验证性并能自动过滤低质量的教师模型生成演示。基于此数据，通过监督微调优化了开源LLM。此外，引入了一个名为OptiTrust的模块化LLM代理，该代理利用分步演示、多语言推理和多数投票交叉验证，执行从自然语言到求解器就绪代码的多阶段翻译。

Result: OptiTrust代理在标准基准测试中取得了最先进的性能，在7个数据集中有6个达到了最高准确率，并且在其中3个数据集上比次优算法至少高出8个百分点。

Conclusion: 该方法为构建用于现实世界优化应用的可靠LLM代理提供了一条可扩展、可验证和有原则的途径。

Abstract: We present a framework for training trustworthy large language model (LLM)
agents for optimization modeling via a verifiable synthetic data generation
pipeline. Focusing on linear and mixed-integer linear programming, our approach
begins with structured symbolic representations and systematically produces
natural language descriptions, mathematical formulations, and solver-executable
code. By programmatically constructing each instance with known optimal
solutions, the pipeline ensures full verifiability and enables automatic
filtering of low-quality demonstrations generated by teacher models. Each
dataset instance includes a structured representation of the optimization
problem, a corresponding natural language description, the verified optimal
solution, and step-by-step demonstrations - generated by a teacher model - that
show how to model and solve the problem across multiple optimization modeling
languages. This enables supervised fine-tuning of open-source LLMs specifically
tailored to optimization tasks. To operationalize this pipeline, we introduce
OptiTrust, a modular LLM agent that performs multi-stage translation from
natural language to solver-ready code, leveraging stepwise demonstrations,
multi-language inference, and majority-vote cross-validation. Our agent
achieves state-of-the-art performance on standard benchmarks. Out of 7
datasets, it achieves the highest accuracy on six and outperforms the next-best
algorithm by at least 8 percentage on three of them. Our approach provides a
scalable, verifiable, and principled path toward building reliable LLM agents
for real-world optimization applications.

</details>


### [29] [Can Large Language Models Bridge the Gap in Environmental Knowledge?](https://arxiv.org/abs/2508.03149)
*Linda Smail,David Santandreu Calonge,Firuz Kamalov,Nur H. Orak*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型（LLMs）在弥合大学生环境知识鸿沟方面的潜力，发现AI模型拥有丰富的知识，但仍需人类专家验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 旨在探讨人工智能模型如何帮助弥补大学生在环境教育方面的知识空白。

Method: 使用标准化工具环境知识测试（EKT-19）和特定问题，比较大学生与GPT-3.5、GPT-4、GPT-4o、Gemini、Claude Sonnet和Llama 2等AI模型在环境知识方面的表现。

Result: 研究结果表明，AI模型拥有庞大、易获取且有效的知识库，有潜力赋能学生和教职员工。

Conclusion: 尽管AI模型潜力巨大，但仍可能需要环境科学领域的人类学科专家来验证其提供信息的准确性。

Abstract: This research investigates the potential of Artificial Intelligence (AI)
models to bridge the knowledge gap in environmental education among university
students. By focusing on prominent large language models (LLMs) such as
GPT-3.5, GPT-4, GPT-4o, Gemini, Claude Sonnet, and Llama 2, the study assesses
their effectiveness in conveying environmental concepts and, consequently,
facilitating environmental education. The investigation employs a standardized
tool, the Environmental Knowledge Test (EKT-19), supplemented by targeted
questions, to evaluate the environmental knowledge of university students in
comparison to the responses generated by the AI models. The results of this
study suggest that while AI models possess a vast, readily accessible, and
valid knowledge base with the potential to empower both students and academic
staff, a human discipline specialist in environmental sciences may still be
necessary to validate the accuracy of the information provided.

</details>


### [30] [Causal identification with $Y_0$](https://arxiv.org/abs/2508.03167)
*Charles Tapley Hoyt,Craig Bakker,Richard J. Callahan,Joseph Cottam,August George,Benjamin M. Gyori,Haley M. Hummel,Nathaniel Merrill,Sara Mohammad Taheri,Pruthvi Prakash Navada,Marc-Antoine Parent,Adam Rupe,Olga Vitek,Jeremy Zucker*

Main category: cs.AI

TL;DR: Y0是一个Python软件包，旨在帮助研究人员定性地识别因果关系，确定因果效应是否可从现有数据中估计，并指导将因果查询转化为可估计的符号表达式。


<details>
  <summary>Details</summary>
Motivation: 研究的动机是为了解决在进行因果效应量化估计之前，如何判断因果关系是否可估计以及如何将因果查询转化为可估计形式的问题，尤其是在面对来自不同来源（RCT、观察性研究或混合）的数据时。

Method: Y0通过实现多种因果识别算法，提供领域特定语言来表示因果查询和可估计量，支持表示带有未观测混杂因素的因果图模型（如ADMGs），并能处理来自随机对照试验、观察性研究或混合数据。

Result: Y0提供了一个可用的Python软件包，能够对干预性、反事实和可迁移性查询进行因果识别，帮助研究人员在量化估计前进行定性分析，并指导他们将因果查询转换为非参数可估计的符号表达式。

Conclusion: Y0软件包为因果推断领域提供了一个实用工具，使研究人员能够系统地探索因果关系的可估计性，并从不同类型的数据中推导出可估计的因果表达式，从而在定量分析之前奠定坚实的基础。

Abstract: We present the $Y_0$ Python package, which implements causal identification
algorithms that apply interventional, counterfactual, and transportability
queries to data from (randomized) controlled trials, observational studies, or
mixtures thereof. $Y_0$ focuses on the qualitative investigation of causation,
helping researchers determine whether a causal relationship can be estimated
from available data before attempting to estimate how strong that relationship
is. Furthermore, $Y_0$ provides guidance on how to transform the causal query
into a symbolic estimand that can be non-parametrically estimated from the
available data. $Y_0$ provides a domain-specific language for representing
causal queries and estimands as symbolic probabilistic expressions, tools for
representing causal graphical models with unobserved confounders, such as
acyclic directed mixed graphs (ADMGs), and implementations of numerous
identification algorithms from the recent causal inference literature. The
$Y_0$ source code can be found under the MIT License at
https://github.com/y0-causal-inference/y0 and it can be installed with pip
install y0.

</details>


### [31] [Geoint-R1: Formalizing Multimodal Geometric Reasoning with Dynamic Auxiliary Constructions](https://arxiv.org/abs/2508.03173)
*Jingxuan Wei,Caijun Jia,Qi Chen,Honghao He,Linzhuang Sun,Conghui He,Lijun Wu,Bihui Yu,Cheng Tan*

Main category: cs.AI

TL;DR: 该论文提出了Geoint-R1多模态推理框架和Geoint基准，旨在解决现有大模型在形式化几何推理中动态构造和验证辅助几何元素的不足。实验证明Geoint-R1在几何推理任务上，尤其在需要辅助元素构造的难题上，显著超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 数学几何推理对科学发现和教育发展至关重要，需要精确逻辑和严格的形式验证。然而，现有多模态大语言模型（MLLMs）在形式化几何推理，尤其是动态构造和验证辅助几何元素方面表现不足。

Method: 本文引入了Geoint-R1多模态推理框架，该框架能从文本描述和视觉图表中生成可形式化验证的几何解。Geoint-R1独特地整合了辅助元素构造、通过Lean4表示的形式化推理以及交互式可视化。为系统评估和推进形式化几何推理，还提出了Geoint基准，包含1,885个经过严格标注的几何问题，每个问题都包含结构化文本标注、精确的Lean4辅助构造代码和专家验证的详细解题步骤。

Result: 广泛实验表明，Geoint-R1显著超越了现有多模态和数学专用推理模型，特别是在需要明确辅助元素构造的挑战性问题上表现突出。

Conclusion: Geoint-R1框架和Geoint基准的提出，有效提升了多模态模型在形式化几何推理，特别是辅助元素构造方面的能力，为该领域的研究和发展提供了新的工具和评估标准。

Abstract: Mathematical geometric reasoning is essential for scientific discovery and
educational development, requiring precise logic and rigorous formal
verification. While recent advances in Multimodal Large Language Models (MLLMs)
have improved reasoning tasks, existing models typically struggle with formal
geometric reasoning, particularly when dynamically constructing and verifying
auxiliary geometric elements. To address these challenges, we introduce
Geoint-R1, a multimodal reasoning framework designed to generate formally
verifiable geometric solutions from textual descriptions and visual diagrams.
Geoint-R1 uniquely integrates auxiliary elements construction, formal reasoning
represented via Lean4, and interactive visualization. To systematically
evaluate and advance formal geometric reasoning, we propose the Geoint
benchmark, comprising 1,885 rigorously annotated geometry problems across
diverse topics such as plane, spatial, and solid geometry. Each problem
includes structured textual annotations, precise Lean4 code for auxiliary
constructions, and detailed solution steps verified by experts. Extensive
experiments demonstrate that Geoint-R1 significantly surpasses existing
multimodal and math-specific reasoning models, particularly on challenging
problems requiring explicit auxiliary element constructions.

</details>


### [32] [InqEduAgent: Adaptive AI Learning Partners with Gaussian Process Augmentation](https://arxiv.org/abs/2508.03174)
*Tian-Fang Zhao,Wen-Xi Yang*

Main category: cs.AI

TL;DR: 本文提出了一个名为InqEduAgent的LLM驱动智能体模型，用于模拟和选择探究式学习的理想学习伙伴，并通过实验验证了其在多数知识学习场景中的优越性能。


<details>
  <summary>Details</summary>
Motivation: 在探究式教育中，协作伙伴关系至关重要。然而，目前的学习伙伴选择方法要么依赖经验分配（缺乏科学规划），要么基于规则的机器助手（存在知识扩展困难和灵活性不足），因此需要更智能、自适应的解决方案。

Method: 研究提出了InqEduAgent模型，该模型包含：1) 生成式智能体，用于捕捉学习者在真实场景中的认知和评估特征；2) 一种自适应匹配算法，结合高斯过程增强，以识别先验知识中的模式，从而为面临不同练习的学习者提供最佳学习伙伴匹配。

Result: 实验结果表明，InqEduAgent在大多数知识学习场景以及不同能力水平的LLM环境中都表现出最佳性能。

Conclusion: 这项研究推动了基于人类学习伙伴的智能分配以及基于AI学习伙伴的构建，为探究式教育中的学习伙伴选择提供了新的智能化途径。

Abstract: Collaborative partnership matters in inquiry-oriented education. However,
most study partners are selected either rely on experience-based assignments
with little scientific planning or build on rule-based machine assistants,
encountering difficulties in knowledge expansion and inadequate flexibility.
This paper proposes an LLM-empowered agent model for simulating and selecting
learning partners tailored to inquiry-oriented learning, named InqEduAgent.
Generative agents are designed to capture cognitive and evaluative features of
learners in real-world scenarios. Then, an adaptive matching algorithm with
Gaussian process augmentation is formulated to identify patterns within prior
knowledge. Optimal learning-partner matches are provided for learners facing
different exercises. The experimental results show the optimal performance of
InqEduAgent in most knowledge-learning scenarios and LLM environment with
different levels of capabilities. This study promotes the intelligent
allocation of human-based learning partners and the formulation of AI-based
learning partners. The code, data, and appendix are publicly available at
https://github.com/InqEduAgent/InqEduAgent.

</details>


### [33] [Full-History Graphs with Edge-Type Decoupled Networks for Temporal Reasoning](https://arxiv.org/abs/2508.03251)
*Osama Mohammed,Jiaxin Pan,Mojtaba Nayyeri,Daniel Hernández,Steffen Staab*

Main category: cs.AI

TL;DR: 该论文提出了一种名为“全历史图”的新型时间图表示方法，以及一个名为ETDNet的神经网络模型，用于建模实体之间随时间演变的交互，并在驾驶员意图预测和金融欺诈检测任务上取得了显著优于基线的性能。


<details>
  <summary>Details</summary>
Motivation: 在许多现实世界任务中，建模实体之间不断演变的交互至关重要，例如预测驾驶员行为或检测金融欺诈。传统时间序列预测方法难以捕捉“谁在何时与谁交互”的复杂关系，需要一种能明确表示关系及其演变的时间图表示。

Method: 该研究引入了一种“全历史图”表示，为每个实体在每个时间步实例化一个节点，并分离两种边：(i) 时间步内边，捕捉单个帧内的关系；(ii) 时间步间边，连接实体在连续时间步的自身。在此图上，设计了“边类型解耦网络 (ETDNet)”，包含并行模块：一个图注意力模块处理时间步内边，一个多头时间注意力模块处理实体的时间步间历史，以及一个融合模块在每层之后结合两种信息。

Result: ETDNet在驾驶员意图预测（Waymo数据集）和比特币欺诈检测（Elliptic++数据集）任务上进行了评估。结果显示，ETDNet持续超越了强基线，将Waymo的联合准确率提升至75.6%（对比74.1%），并将Elliptic++的非法类别F1分数提高到88.1%（对比60.4%）。

Conclusion: 研究结果表明，在单个图中将结构关系和时间关系表示为不同的边，能够显著提升模型性能，证明了这种表示方法的有效性。

Abstract: Modeling evolving interactions among entities is critical in many real-world
tasks. For example, predicting driver maneuvers in traffic requires tracking
how neighboring vehicles accelerate, brake, and change lanes relative to one
another over consecutive frames. Likewise, detecting financial fraud hinges on
following the flow of funds through successive transactions as they propagate
through the network. Unlike classic time-series forecasting, these settings
demand reasoning over who interacts with whom and when, calling for a
temporal-graph representation that makes both the relations and their evolution
explicit. Existing temporal-graph methods typically use snapshot graphs to
encode temporal evolution. We introduce a full-history graph that instantiates
one node for every entity at every time step and separates two edge sets: (i)
intra-time-step edges that capture relations within a single frame and (ii)
inter-time-step edges that connect an entity to itself at consecutive steps. To
learn on this graph we design an Edge-Type Decoupled Network (ETDNet) with
parallel modules: a graph-attention module aggregates information along
intra-time-step edges, a multi-head temporal-attention module attends over an
entity's inter-time-step history, and a fusion module combines the two messages
after every layer. Evaluated on driver-intention prediction (Waymo) and Bitcoin
fraud detection (Elliptic++), ETDNet consistently surpasses strong baselines,
lifting Waymo joint accuracy to 75.6\% (vs. 74.1\%) and raising Elliptic++
illicit-class F1 to 88.1\% (vs. 60.4\%). These gains demonstrate the benefit of
representing structural and temporal relations as distinct edges in a single
graph.

</details>


### [34] [ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools](https://arxiv.org/abs/2508.03284)
*Shaofeng Yin,Ting Lei,Yang Liu*

Main category: cs.AI

TL;DR: 该研究引入了ToolVQA，一个包含2.3万实例的大规模多模态数据集，旨在弥补现有LFM工具使用基准在真实世界多步推理方面的不足。ToolVQA通过ToolEngine生成，并展示了在ToolVQA上微调的7B LFM在OOD数据集上超越GPT-3.5-turbo的强大泛化能力。


<details>
  <summary>Details</summary>
Motivation: 尽管现有研究在工具增强型视觉问答(VQA)中表现出色，但最近的基准测试揭示了大型基础模型(LFMs)在真实世界、功能多样、需要多步推理的多模态环境中，其工具使用能力存在显著差距。这促使了对更贴近真实用户交互的数据集的需求。

Method: 研究引入了ToolVQA数据集，该数据集包含2.3万个实例，采用真实世界视觉上下文和具有挑战性的隐式多步推理任务。为构建该数据集，提出了ToolEngine数据生成管道，该管道利用深度优先搜索(DFS)和动态上下文示例匹配机制来模拟人类般的工具使用推理。ToolVQA涵盖了7个不同任务领域的10种多模态工具，平均每个实例需要2.78个推理步骤。

Result: 在ToolVQA上微调的7B大型基础模型不仅在测试集上取得了令人印象深刻的性能，而且在各种域外(OOD)数据集上超越了大型闭源模型GPT-3.5-turbo。这表明了ToolVQA在模拟真实世界工具使用场景方面的强大泛化能力。

Conclusion: ToolVQA数据集有效弥补了现有LFM工具使用基准在真实世界多模态、多步推理方面的差距。基于ToolVQA训练的模型展示了强大的泛化能力，能够有效应对真实世界的工具使用场景，甚至超越了更大型的闭源模型。

Abstract: Integrating external tools into Large Foundation Models (LFMs) has emerged as
a promising approach to enhance their problem-solving capabilities. While
existing studies have demonstrated strong performance in tool-augmented Visual
Question Answering (VQA), recent benchmarks reveal significant gaps in
real-world tool-use proficiency, particularly in functionally diverse
multimodal settings requiring multi-step reasoning. In this work, we introduce
ToolVQA, a large-scale multimodal dataset comprising 23K instances, designed to
bridge this gap. Unlike previous datasets that rely on synthetic scenarios and
simplified queries, ToolVQA features real-world visual contexts and challenging
implicit multi-step reasoning tasks, better aligning with real user
interactions. To construct this dataset, we propose ToolEngine, a novel data
generation pipeline that employs Depth-First Search (DFS) with a dynamic
in-context example matching mechanism to simulate human-like tool-use
reasoning. ToolVQA encompasses 10 multimodal tools across 7 diverse task
domains, with an average inference length of 2.78 reasoning steps per instance.
The fine-tuned 7B LFMs on ToolVQA not only achieve impressive performance on
our test set but also surpass the large close-sourced model GPT-3.5-turbo on
various out-of-distribution (OOD) datasets, demonstrating strong
generalizability to real-world tool-use scenarios.

</details>


### [35] [Nemori: Self-Organizing Agent Memory Inspired by Cognitive Science](https://arxiv.org/abs/2508.03341)
*Jiayan Nan,Wenquan Ma,Wenlong Wu,Yize Chen*

Main category: cs.AI

TL;DR: Nemori是一种受人类认知启发的自组织记忆架构，通过双步对齐原则解决记忆粒度问题，并通过预测-校准原则实现自适应知识进化，显著提升了LLM在长语境下的长期记忆能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在长语境中缺乏持久记忆，限制了其作为自主代理的有效性。现有记忆系统存在记忆单元粒度任意性及知识提取机制被动、基于规则的问题，阻碍了真正的学习和进化。

Method: 本文提出了Nemori架构，其核心创新包括两点：1. 受事件分割理论启发的“双步对齐原则”，提供了一种从上而下的方法，将原始对话流组织成语义连贯的片段，解决了记忆粒度问题。2. 受自由能原理启发的“预测-校准原则”，使代理能够主动从预测偏差中学习，实现自适应知识进化。

Result: 在LoCoMo和LongMemEval基准测试中，Nemori的表现显著优于现有最先进系统，尤其在更长的语境中优势更为明显。

Conclusion: Nemori为处理自主代理的长期、动态工作流提供了一条可行的路径，有望克服LLM在长语境下记忆不足的限制。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities, yet their
inability to maintain persistent memory in long contexts limits their
effectiveness as autonomous agents in long-term interactions. While existing
memory systems have made progress, their reliance on arbitrary granularity for
defining the basic memory unit and passive, rule-based mechanisms for knowledge
extraction limits their capacity for genuine learning and evolution. To address
these foundational limitations, we present Nemori, a novel self-organizing
memory architecture inspired by human cognitive principles. Nemori's core
innovation is twofold: First, its Two-Step Alignment Principle, inspired by
Event Segmentation Theory, provides a principled, top-down method for
autonomously organizing the raw conversational stream into semantically
coherent episodes, solving the critical issue of memory granularity. Second,
its Predict-Calibrate Principle, inspired by the Free-energy Principle, enables
the agent to proactively learn from prediction gaps, moving beyond pre-defined
heuristics to achieve adaptive knowledge evolution. This offers a viable path
toward handling the long-term, dynamic workflows of autonomous agents.
Extensive experiments on the LoCoMo and LongMemEval benchmarks demonstrate that
Nemori significantly outperforms prior state-of-the-art systems, with its
advantage being particularly pronounced in longer contexts.

</details>


### [36] [Adaptive AI Agent Placement and Migration in Edge Intelligence Systems](https://arxiv.org/abs/2508.03345)
*Xingdan Wang,Jiayi He,Zhiqing Tang,Jianxiong Guo,Jiong Lou,Liping Qian,Tian Wang,Weijia Jia*

Main category: cs.AI

TL;DR: 该论文提出了一种在动态边缘环境中部署和管理基于LLM的AI智能体的系统解决方案，通过自适应框架和轻量级迁移来优化资源利用和QoS。


<details>
  <summary>Details</summary>
Motivation: 随着LLM（如ChatGPT）的兴起，对能实时处理任务的AI智能体的需求增加。传统上将数据密集型、多模态边缘工作负载迁移到云数据中心会导致显著延迟。在边缘部署AI智能体可以提高效率并降低延迟，但边缘环境资源有限且异构。为移动用户保持QoS需要智能体迁移，而AI智能体（协调LLM、任务规划、内存和外部工具）的复杂性使得迁移变得复杂。

Method: 本文提出了一个新颖的自适应框架，用于边缘智能系统中AI智能体的放置和迁移。该方法对资源限制、延迟和成本进行建模，利用蚁群算法和基于LLM的优化进行高效决策。它能自主放置智能体以优化资源利用率和QoS，并通过仅传输必要状态实现轻量级智能体迁移。该解决方案在分布式系统上使用AgentScope实现，并在全球分布式边缘服务器上进行了验证。

Result: 该解决方案显著降低了部署延迟和迁移成本。

Conclusion: 本文首次提出了一种在动态边缘环境中部署和管理基于LLM的AI智能体的系统解决方案，有效解决了资源受限和迁移复杂性问题，提高了效率并降低了成本。

Abstract: The rise of LLMs such as ChatGPT and Claude fuels the need for AI agents
capable of real-time task handling. However, migrating data-intensive,
multi-modal edge workloads to cloud data centers, traditionally used for agent
deployment, introduces significant latency. Deploying AI agents at the edge
improves efficiency and reduces latency. However, edge environments present
challenges due to limited and heterogeneous resources. Maintaining QoS for
mobile users necessitates agent migration, which is complicated by the
complexity of AI agents coordinating LLMs, task planning, memory, and external
tools. This paper presents the first systematic deployment and management
solution for LLM-based AI agents in dynamic edge environments. We propose a
novel adaptive framework for AI agent placement and migration in edge
intelligence systems. Our approach models resource constraints and
latency/cost, leveraging ant colony algorithms and LLM-based optimization for
efficient decision-making. It autonomously places agents to optimize resource
utilization and QoS and enables lightweight agent migration by transferring
only essential state. Implemented on a distributed system using AgentScope and
validated across globally distributed edge servers, our solution significantly
reduces deployment latency and migration costs.

</details>


### [37] [Compressing Chain-of-Thought in LLMs via Step Entropy](https://arxiv.org/abs/2508.03346)
*Zeju Li,Jianyuan Zhong,Ziyang Zheng,Xiangyu Wen,Zhijian Xu,Yingying Cheng,Fan Zhang,Qiang Xu*

Main category: cs.AI

TL;DR: 该研究提出了一种基于步长熵的思维链（CoT）压缩框架，并结合两阶段训练策略，使大型语言模型（LLMs）能够自主生成压缩的CoT，显著提高推理效率并保持准确性。


<details>
  <summary>Details</summary>
Motivation: LLMs使用CoT提示时会生成冗长且包含大量冗余的思维过程，这导致推理成本增加和效率降低。

Method: 1. 引入“步长熵”度量来量化每个推理步骤的信息贡献，以识别冗余。2. 基于步长熵进行低熵步骤的剪枝。3. 提出一种两阶段训练策略，结合监督微调（SFT）和组相对策略优化（GRPO）强化学习，使LLMs能通过战略性地加入[SKIP]标记来自主生成压缩的CoT。

Result: 1. 理论分析和实验验证表明，低熵步骤确实高度冗余。2. 在DeepSeek-R1-7B、14B和Qwen3-8B模型上，80%的低熵中间步骤可以在最终答案准确性仅有轻微下降的情况下被剪除。3. 这与随机剪枝或高熵剪枝形成鲜明对比，后者会严重损害推理性能。4. 所提出的方法显著提高了LLM的推理效率，同时严格保持了准确性。

Conclusion: 该方法通过实现高效、准确的压缩CoT生成，对LLM的实际部署和对推理结构的深入理解具有深远意义。

Abstract: Large Language Models (LLMs) using Chain-of-Thought (CoT) prompting excel at
complex reasoning but generate verbose thought processes with considerable
redundancy, leading to increased inference costs and reduced efficiency. We
introduce a novel CoT compression framework based on step entropy, a metric
that quantifies the informational contribution of individual reasoning steps to
identify redundancy. Through theoretical analysis and extensive empirical
validation on mathematical reasoning benchmarks, we demonstrate that steps with
low entropy are indeed highly redundant. Our experiments reveal that an
astonishing 80\% of low-entropy intermediate steps can be pruned with minor
degradation in the final answer accuracy across DeepSeek-R1-7B, 14B and
Qwen3-8B. This finding sharply contrasts with random or high-entropy pruning,
which severely impairs reasoning performance. Building on this, we propose a
novel two-stage training strategy combining Supervised Fine-Tuning (SFT) and
Group Relative Policy Optimization (GRPO) reinforcement learning. This approach
enables LLMs to autonomously learn to generate compressed COTs during inference
by strategically incorporating [SKIP] tokens. Our method significantly enhances
LLM inference efficiency while rigorously preserving accuracy, offering
profound implications for practical LLM deployment and a deeper understanding
of reasoning structures.

</details>


### [38] [CogBench: A Large Language Model Benchmark for Multilingual Speech-Based Cognitive Impairment Assessment](https://arxiv.org/abs/2508.03360)
*Feng Rui,Zhiyao Luo,Wei Wang,Yuting Song,Yong Liu,Tingting Zhu,Jianqing Li,Xingyao Wang*

Main category: cs.AI

TL;DR: 本研究提出了CogBench基准，用于评估大型语言模型(LLMs)在跨语言和跨中心语音认知障碍评估中的泛化能力，并发现LLMs通过思维链提示和LoRA微调能显著提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 自动语音认知障碍评估缺乏跨语言和跨临床环境的泛化能力，限制了其实用性。

Method: 提出了CogBench基准，采用统一的多模态流程，在英语和普通话的三个语音数据集（ADReSSo、NCMMSC2021-AD、CIR-E）上评估模型性能。研究了传统深度学习模型、结合思维链提示的LLMs以及通过LoRA进行轻量级微调的LLMs。

Result: 传统深度学习模型在跨领域迁移时性能显著下降；LLMs结合思维链提示表现出更好的适应性，但对提示设计敏感；通过LoRA轻量级微调LLMs显著提高了在目标领域的泛化能力。

Conclusion: 这些发现是构建临床有用且语言鲁棒的语音认知评估工具的关键一步。

Abstract: Automatic assessment of cognitive impairment from spontaneous speech offers a
promising, non-invasive avenue for early cognitive screening. However, current
approaches often lack generalizability when deployed across different languages
and clinical settings, limiting their practical utility. In this study, we
propose CogBench, the first benchmark designed to evaluate the cross-lingual
and cross-site generalizability of large language models (LLMs) for
speech-based cognitive impairment assessment. Using a unified multimodal
pipeline, we evaluate model performance on three speech datasets spanning
English and Mandarin: ADReSSo, NCMMSC2021-AD, and a newly collected test set,
CIR-E. Our results show that conventional deep learning models degrade
substantially when transferred across domains. In contrast, LLMs equipped with
chain-of-thought prompting demonstrate better adaptability, though their
performance remains sensitive to prompt design. Furthermore, we explore
lightweight fine-tuning of LLMs via Low-Rank Adaptation (LoRA), which
significantly improves generalization in target domains. These findings offer a
critical step toward building clinically useful and linguistically robust
speech-based cognitive assessment tools.

</details>


### [39] [A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning](https://arxiv.org/abs/2508.03366)
*Michael K. Chen*

Main category: cs.AI

TL;DR: 本文比较了两种神经符号AI方法（集成式和混合式）在通用逻辑推理方面的潜力，发现混合式方法更有前景，并提出了一个可泛化的框架。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在通用逻辑推理方面表现不佳，缺乏确定性和可解释性。神经符号AI旨在结合逻辑和神经网络来解决这一问题，但不同方法在领域无关的逻辑推理基准上的表现尚未得到充分研究和比较。

Method: 作者首先识别了两种主要的神经符号AI方法：集成式（逻辑推理包含在神经网络内部）和混合式（独立的符号求解器执行推理）。然后，选择两种代表性的领域无关模型作为案例研究：集成式方法的Logic Neural Network (LNN) 和混合式方法的LLM-Symbolic Solver (LLM-SS)，通过对比分析评估它们在发展通用逻辑推理方面的潜力。最后，提出了一个基于LLM-SS的模块化、模型无关、领域无关且几乎无需人工输入的通用框架。

Result: 分析结果表明，混合式方法在发展通用逻辑推理方面更具前景。主要原因有二：(i) 其推理链更具可解释性；(ii) 它保留了现有LLM的能力和优势。

Conclusion: 混合式神经符号AI方法在实现通用逻辑推理方面比集成式方法更有希望，因为它提供了更好的可解释性并能利用LLM的固有优势。为支持未来的研究，本文提出了一个基于LLM-SS的通用框架。

Abstract: General logical reasoning, defined as the ability to reason deductively on
domain-agnostic tasks, continues to be a challenge for large language models
(LLMs). Current LLMs fail to reason deterministically and are not
interpretable. As such, there has been a recent surge in interest in
neurosymbolic AI, which attempts to incorporate logic into neural networks. We
first identify two main neurosymbolic approaches to improving logical
reasoning: (i) the integrative approach comprising models where symbolic
reasoning is contained within the neural network, and (ii) the hybrid approach
comprising models where a symbolic solver, separate from the neural network,
performs symbolic reasoning. Both contain AI systems with promising results on
domain-specific logical reasoning benchmarks. However, their performance on
domain-agnostic benchmarks is understudied. To the best of our knowledge, there
has not been a comparison of the contrasting approaches that answers the
following question: Which approach is more promising for developing general
logical reasoning? To analyze their potential, the following best-in-class
domain-agnostic models are introduced: Logic Neural Network (LNN), which uses
the integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the
hybrid approach. Using both models as case studies and representatives of each
approach, our analysis demonstrates that the hybrid approach is more promising
for developing general logical reasoning because (i) its reasoning chain is
more interpretable, and (ii) it retains the capabilities and advantages of
existing LLMs. To support future works using the hybrid approach, we propose a
generalizable framework based on LLM-SS that is modular by design,
model-agnostic, domain-agnostic, and requires little to no human input.

</details>


### [40] [Board Game Arena: A Framework and Benchmark for Assessing Large Language Models via Strategic Play](https://arxiv.org/abs/2508.03368)
*Lucia Cipolina-Kun,Marianna Nezhurina,Jenia Jitsev*

Main category: cs.AI

TL;DR: Board Game Arena库是一个评估大型语言模型（LLM）在策略棋盘游戏（基于Google OpenSpiel）中决策能力的框架，支持多种代理类型和分布式执行，并提供LLM推理分析工具。


<details>
  <summary>Details</summary>
Motivation: 旨在系统地评估和比较LLM在策略棋盘游戏中的决策能力，理解其推理过程和博弈论行为，并与人类、强化学习等其他代理进行对比。

Method: 该框架通过封装Google OpenSpiel中的多种棋盘和矩阵游戏，支持LLM、随机、人类和强化学习等多种代理类型。它集成了LiteLLM进行模型API访问，vLLM进行本地模型部署，并利用Ray实现分布式执行。此外，还提供了针对LLM推理轨迹的广泛分析工具。

Result: 该论文概述了Board Game Arena库的结构、关键特性和动机，展示了其如何促进对LLM推理和博弈论行为的实证评估。

Conclusion: Board Game Arena库为在策略棋盘游戏中实证评估LLM的决策能力、推理过程和博弈论行为提供了一个全面的框架，有助于推动LLM研究进展。

Abstract: The Board Game Arena library provides a framework for evaluating the decision
making abilities of large language models (LLMs) through strategic board games
implemented in Google OpenSpiel library. The framework enables systematic
comparisons between LLM based agents and other agents (random, human,
reinforcement learning agents, etc.) in various game scenarios by wrapping
multiple board and matrix games and supporting different agent types. It
integrates API access to models via LiteLLM, local model deployment via vLLM,
and offers distributed execution through Ray. Additionally it provides
extensive analysis tools for the LLM reasoning traces. This paper summarizes
the structure, key characteristics, and motivation of the repository,
highlighting how it contributes to the empirical evaluation of the reasoning of
LLM and game-theoretic behavior

</details>


### [41] [Data Dependency Inference for Industrial Code Generation Based on UML Sequence Diagrams](https://arxiv.org/abs/2508.03379)
*Wenxin Mao,Zhitao Wang Long Wang,Sirong Chen,Cuiyun Gao,Luyang Cao,Ziming Liu,Qiming Zhang,Jun Zhou,Zhi Jin*

Main category: cs.AI

TL;DR: UML2Dep是一个分步代码生成框架，它通过引入增强的UML序列图（结合决策表和API规范）和显式数据依赖推理，解决了LLM从模糊自然语言生成复杂代码的挑战。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在从自然语言生成代码时，面临自然语言描述固有的模糊性，难以捕捉复杂的系统行为、条件逻辑、架构约束和隐式数据依赖等问题。

Method: 该研究提出了UML2Dep框架：首先，引入了针对服务导向架构的增强UML序列图，通过集成决策表和API规范来明确形式化业务逻辑和结构关系，消除语言歧义。其次，提出了数据依赖推理（DDI）任务，在代码合成前构建显式数据依赖图，并通过将DDI形式化为受约束的数学推理任务来提高可靠性，并利用静态解析和依赖剪枝减少上下文复杂性。

Result: 通过上述方法，UML2Dep旨在严格消除语言歧义，确保数据依赖推理的可靠性，并提升LLM在处理复杂规范时的推理准确性和效率。

Conclusion: 该框架通过利用明确的正式规范和显式数据依赖建模，成功弥合了自然语言描述与复杂代码生成之间的鸿沟，为LLM生成高质量代码提供了更可靠的途径。

Abstract: Large language models (LLMs) excel at generating code from natural language
(NL) descriptions. However, the plain textual descriptions are inherently
ambiguous and often fail to capture complex requirements like intricate system
behaviors, conditional logic, and architectural constraints; implicit data
dependencies in service-oriented architectures are difficult to infer and
handle correctly. To bridge this gap, we propose a novel step-by-step code
generation framework named UML2Dep by leveraging unambiguous formal
specifications of complex requirements. First, we introduce an enhanced Unified
Modeling Language (UML) sequence diagram tailored for service-oriented
architectures. This diagram extends traditional visual syntax by integrating
decision tables and API specifications, explicitly formalizing structural
relationships and business logic flows in service interactions to rigorously
eliminate linguistic ambiguity. Second, recognizing the critical role of data
flow, we introduce a dedicated data dependency inference (DDI) task. DDI
systematically constructs an explicit data dependency graph prior to actual
code synthesis. To ensure reliability, we formalize DDI as a constrained
mathematical reasoning task through novel prompting strategies, aligning with
LLMs' excellent mathematical strengths. Additional static parsing and
dependency pruning further reduce context complexity and cognitive load
associated with intricate specifications, thereby enhancing reasoning accuracy
and efficiency.

</details>


### [42] [Hide and Seek with LLMs: An Adversarial Game for Sneaky Error Generation and Self-Improving Diagnosis](https://arxiv.org/abs/2508.03396)
*Rui Zou,Mengqi Wei,Yutao Zhu,Jirong Wen,Xin Zhao,Jing Chen*

Main category: cs.AI

TL;DR: 该论文提出Hide and Seek Game (HSG)框架，通过对抗性训练提升大型语言模型(LLMs)对复杂推理错误的诊断能力，在数学推理任务上显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理和生成方面表现出色，但难以识别和诊断复杂错误。这主要是因为训练目标优先考虑正确答案，导致模型接触和学习错误的机会有限。现有错误信号多为浅层、静态错误，无法有效提升深层诊断能力。

Method: 提出Hide and Seek Game (HSG)，一个动态对抗性框架，用于错误生成和诊断。框架包含两个对抗角色：'Sneaky'（生成微妙、欺骗性的推理错误）和'Diagnosis'（准确检测这些错误）。通过对抗性共同演化，提升错误隐蔽性和诊断精度。在数学问题解决任务上进行评估。

Result: HSG显著提升了错误诊断能力，在多项数学推理任务上的准确率比GPT-4o等基线模型高出16.8%—31.4%。同时发布了一个具有挑战性的欺骗性错误和诊断标注数据集。

Conclusion: HSG框架通过动态对抗性共同演化，有效解决了LLMs在复杂错误诊断方面的不足，显著提升了其诊断精度，为未来研究提供了新的基准和数据集。

Abstract: Large Language Models (LLMs) excel in reasoning and generation across
domains, but still struggle with identifying and diagnosing complex errors.
This stems mainly from training objectives that prioritize correct answers,
limiting exposure to and learning from errors. While recent studies have begun
to address this by introducing error signals, most rely on shallow, static
errors, restricting improvement in deep diagnostic ability. To overcome this,
we propose Hide and Seek Game (HSG), a dynamic adversarial framework for error
generation and diagnosis, and evaluate it on mathematical problem-solving. HSG
involves two adversarial roles: Sneaky, which "hides" by generating subtle,
deceptive reasoning errors, and Diagnosis, which "seeks" to accurately detect
them. Through adversarial co-evolution, both error stealth and diagnostic
precision are enhanced. Experiments on several math reasoning tasks show that
HSG significantly boosts error diagnosis, achieving 16.8\%--31.4\% higher
accuracy than baselines like GPT-4o. We also release a challenging dataset of
deceptive errors and diagnostic annotations as a benchmark for future research.

</details>


### [43] [Multi-Objective Infeasibility Diagnosis for Routing Problems Using Large Language Models](https://arxiv.org/abs/2508.03406)
*Kai Li,Ruihao Zheng,Xinye Hao,Zhenkun Wang*

Main category: cs.AI

TL;DR: MOID结合大语言模型(LLM)和多目标优化，为不 S可行的路径规划问题提供多样的、可操作的诊断建议，以恢复模型可行性。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的路径规划问题常因用户需求冲突或不合理导致模型不可行，现有基于LLM的方法虽能诊断但未能考虑多种潜在的调整方案。

Method: MOID通过多目标优化（同时考虑路径成本和约束违反）生成一系列权衡解决方案，每种方案代表不同程度的模型调整。随后，利用LLM代理生成解决方案分析函数，对这些不同方案进行分析，从而诊断原始不可行模型，并提供多样化的诊断见解和建议。

Result: 在50种不可行路径规划问题上的比较结果表明，MOID在单次运行中能自动生成多个诊断建议，相较于现有方法，为恢复模型可行性和决策提供了更实用的见解。

Conclusion: MOID有效解决了不可行路径规划模型的诊断和修复问题，通过结合多目标优化和LLM代理，为用户提供了更全面、实用的诊断见解和决策支持。

Abstract: In real-world routing problems, users often propose conflicting or
unreasonable requirements, which result in infeasible optimization models due
to overly restrictive or contradictory constraints, leading to an empty
feasible solution set. Existing Large Language Model (LLM)-based methods
attempt to diagnose infeasible models, but modifying such models often involves
multiple potential adjustments that these methods do not consider. To fill this
gap, we introduce Multi-Objective Infeasibility Diagnosis (MOID), which
combines LLM agents and multi-objective optimization within an automatic
routing solver, to provide a set of representative actionable suggestions.
Specifically, MOID employs multi-objective optimization to consider both path
cost and constraint violation, generating a set of trade-off solutions, each
encompassing varying degrees of model adjustments. To extract practical
insights from these solutions, MOID utilizes LLM agents to generate a solution
analysis function for the infeasible model. This function analyzes these
distinct solutions to diagnose the original infeasible model, providing users
with diverse diagnostic insights and suggestions. Finally, we compare MOID with
several LLM-based methods on 50 types of infeasible routing problems. The
results indicate that MOID automatically generates multiple diagnostic
suggestions in a single run, providing more practical insights for restoring
model feasibility and decision-making compared to existing methods.

</details>


### [44] [Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction](https://arxiv.org/abs/2508.03438)
*Taine J. Elliott,Stephen P. Levitt,Ken Nixon,Martin Bekker*

Main category: cs.AI

TL;DR: 该研究提出一种利用大型语言模型（LLM）从医学摘要中提取信息并自动生成知识图谱（KG）的方法，以应对医学文献量激增的问题。


<details>
  <summary>Details</summary>
Motivation: 公开医学数据和科学文献的快速增长使临床医生和研究人员难以系统地回顾和理解最新知识，导致知识应用与文献量之间存在差距。

Method: 该方法通过一个LLM代理管道，将PubMed摘要分解为语义命题，并从中提取知识图谱三元组。这些三元组通过结合开放域和本体论信息提取方法进行增强，纳入本体类别，并添加上下文变量形成“四元组”。通过比较由增强三元组生成的自然语言句子与原始命题，验证LLM的提取准确性。此外，还探讨了LLM推断新关系和连接知识图谱中集群的能力。

Result: LLM提取的准确性验证表明，由增强三元组生成的句子与原始命题的平均余弦相似度达到0.874。上下文变量的引入提高了生成句子与原始命题的相似度。研究还表明LLM能够推断新关系并连接知识库中的集群。

Conclusion: 该方法为医学从业者提供了一个集中、实时更新且可持续的知识来源，有望成为各领域类似进步的基础。

Abstract: The rapid expansion of publicly-available medical data presents a challenge
for clinicians and researchers alike, increasing the gap between the volume of
scientific literature and its applications. The steady growth of studies and
findings overwhelms medical professionals at large, hindering their ability to
systematically review and understand the latest knowledge. This paper presents
an approach to information extraction and automatic knowledge graph (KG)
generation to identify and connect biomedical knowledge. Through a pipeline of
large language model (LLM) agents, the system decomposes 44 PubMed abstracts
into semantically meaningful proposition sentences and extracts KG triples from
these sentences. The triples are enhanced using a combination of open domain
and ontology-based information extraction methodologies to incorporate
ontological categories. On top of this, a context variable is included during
extraction to allow the triple to stand on its own - thereby becoming
`quadruples'. The extraction accuracy of the LLM is validated by comparing
natural language sentences generated from the enhanced triples to the original
propositions, achieving an average cosine similarity of 0.874. The similarity
for generated sentences of enhanced triples were compared with generated
sentences of ordinary triples showing an increase as a result of the context
variable. Furthermore, this research explores the ability for LLMs to infer new
relationships and connect clusters in the knowledge base of the knowledge
graph. This approach leads the way to provide medical practitioners with a
centralised, updated in real-time, and sustainable knowledge source, and may be
the foundation of similar gains in a wide variety of fields.

</details>


### [45] [Toward a Graph-Theoretic Model of Belief: Confidence, Credibility, and Structural Coherence](https://arxiv.org/abs/2508.03465)
*Saleh Nikooroo*

Main category: cs.AI

TL;DR: 本文提出一种将信念系统建模为有向加权图的新形式主义，以更好地表示信念的内部结构、区分可信度与置信度，并处理碎片化或矛盾的认知状态。


<details>
  <summary>Details</summary>
Motivation: 传统的信念系统表示（如全局一致的命题集或标量概率分布）往往掩盖了信念的内部结构，混淆了外部可信度与内部连贯性，并且无法建模碎片化或矛盾的认知状态。

Method: 该方法将信念系统表示为有向加权图。图中节点代表个体信念，边编码认知关系（如支持或矛盾）。同时，引入两个独立函数：一个分配信念的可信度（反映来源信任），另一个分配置信度（源自内部结构支持）。该模型是纯静态的，不包含推理或修订过程。

Result: 通过区分信念结构与信念强度，该形式主义能够实现比现有概率、逻辑或基于论证的方法更丰富的认知状态分类。它提供了一个分析信念系统内部组织（包括连贯性条件、认知张力及表示限制）的基础基质。

Conclusion: 所提出的有向加权图形式主义为分析信念系统的内部组织提供了一个基础框架，克服了传统模型在表示复杂认知状态方面的局限性，实现了对认知状态更细致的分类。

Abstract: Belief systems are often treated as globally consistent sets of propositions
or as scalar-valued probability distributions. Such representations tend to
obscure the internal structure of belief, conflate external credibility with
internal coherence, and preclude the modeling of fragmented or contradictory
epistemic states. This paper introduces a minimal formalism for belief systems
as directed, weighted graphs. In this framework, nodes represent individual
beliefs, edges encode epistemic relationships (e.g., support or contradiction),
and two distinct functions assign each belief a credibility (reflecting source
trust) and a confidence (derived from internal structural support). Unlike
classical probabilistic models, our approach does not assume prior coherence or
require belief updating. Unlike logical and argumentation-based frameworks, it
supports fine-grained structural representation without committing to binary
justification status or deductive closure. The model is purely static and
deliberately excludes inference or revision procedures. Its aim is to provide a
foundational substrate for analyzing the internal organization of belief
systems, including coherence conditions, epistemic tensions, and
representational limits. By distinguishing belief structure from belief
strength, this formalism enables a richer classification of epistemic states
than existing probabilistic, logical, or argumentation-based approaches.

</details>


### [46] [Semantic-aware Graph-guided Behavior Sequences Generation with Large Language Models for Smart Homes](https://arxiv.org/abs/2508.03484)
*Zhiyao Xu,Dan Zhao,Qingsong Zou,Qing Li,Yong Jiang,Yuhang Wang,Jingyu Xiao*

Main category: cs.AI

TL;DR: SmartGen是一个基于LLM的框架，通过合成上下文感知的用户行为数据，以支持智能家居模型在行为漂移下的持续适应。


<details>
  <summary>Details</summary>
Motivation: 智能家居模型通常在静态数据集上训练，但用户行为会因季节、生活方式变化等原因发生漂移，导致模型性能下降。重新收集新行为数据进行再训练成本高昂、耗时且涉及隐私问题，因此需要一种无需实际数据收集即可适应行为漂移的方法。

Method: SmartGen包含四个核心组件：1) 时间和语义感知分割模块，将长行为序列分解为有意义的子序列；2) 语义感知序列压缩，通过潜在空间聚类减少输入长度同时保留语义；3) 图引导序列合成，构建行为关系图并编码频繁转换，指导LLM生成符合上下文变化的数据；4) 两阶段异常值过滤器，识别并移除不合理或语义不一致的生成数据，以提高数据有效性。

Result: 在三个真实世界数据集上的实验表明，SmartGen显著提升了模型在行为漂移下的异常检测和行为预测任务性能。异常检测平均提升85.43%，行为预测平均提升70.51%。

Conclusion: SmartGen通过合成上下文感知的用户行为数据，有效解决了智能家居模型在行为漂移下的持续适应问题，显著提高了异常检测和行为预测的性能。

Abstract: As smart homes become increasingly prevalent, intelligent models are widely
used for tasks such as anomaly detection and behavior prediction. These models
are typically trained on static datasets, making them brittle to behavioral
drift caused by seasonal changes, lifestyle shifts, or evolving routines.
However, collecting new behavior data for retraining is often impractical due
to its slow pace, high cost, and privacy concerns. In this paper, we propose
SmartGen, an LLM-based framework that synthesizes context-aware user behavior
data to support continual adaptation of downstream smart home models. SmartGen
consists of four key components. First, we design a Time and Semantic-aware
Split module to divide long behavior sequences into manageable, semantically
coherent subsequences under dual time-span constraints. Second, we propose
Semantic-aware Sequence Compression to reduce input length while preserving
representative semantics by clustering behavior mapping in latent space. Third,
we introduce Graph-guided Sequence Synthesis, which constructs a behavior
relationship graph and encodes frequent transitions into prompts, guiding the
LLM to generate data aligned with contextual changes while retaining core
behavior patterns. Finally, we design a Two-stage Outlier Filter to identify
and remove implausible or semantically inconsistent outputs, aiming to improve
the factual coherence and behavioral validity of the generated sequences.
Experiments on three real-world datasets demonstrate that SmartGen
significantly enhances model performance on anomaly detection and behavior
prediction tasks under behavioral drift, with anomaly detection improving by
85.43% and behavior prediction by 70.51% on average. The code is available at
https://github.com/horizonsinzqs/SmartGen.

</details>


### [47] [VQA support to Arabic Language Learning Educational Tool](https://arxiv.org/abs/2508.03488)
*Khaled Bachir Delassi,Lakhdar Zeggane,Hadda Cherroun,Abdelhamid Haouhat,Kaoutar Bouzouad*

Main category: cs.AI

TL;DR: 本文设计并评估了一个AI驱动的阿拉伯语学习工具，通过生成交互式视觉测验（基于VQA、VLP和LLM）来解决现代教学工具的稀缺问题，并经验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏支持主动学习等现代教学模式的阿拉伯语学习工具，这阻碍了语言熟练度的提升。研究旨在填补这一空白，为非母语的初中级学习者提供一个AI赋能的解决方案。

Method: 开发了一个AI驱动的教育工具，主要活动是视觉问答（VQA），采用建构主义学习方法。系统利用视觉-语言预训练（VLP）模型生成图像描述，并结合大型语言模型（LLM）通过提示词生成定制化的阿拉伯语学习测验，侧重于词汇、语法和理解。工具的有效性通过包含1266个真实视觉测验的手动标注基准进行评估，并收集了人类参与者的反馈。

Result: 评估结果显示该工具具有合适的准确率，验证了其弥补阿拉伯语教育差距的潜力，并突显了其作为可靠、AI驱动的阿拉伯语学习资源的广阔前景，能够提供个性化和互动式的学习体验。

Conclusion: 该AI驱动的阿拉伯语学习工具是有效且有前景的，它通过利用先进的AI模型提供互动式、个性化的学习体验，有望成为阿拉伯语学习者的可靠资源，解决现有教育工具的不足。

Abstract: We address the problem of scarcity of educational Arabic Language Learning
tools that advocate modern pedagogical models such as active learning which
ensures language proficiency. In fact, we investigate the design and evaluation
of an AI-powered educational tool designed to enhance Arabic language learning
for non-native speakers with beginner-to-intermediate proficiency level. The
tool leverages advanced AI models to generate interactive visual quizzes,
deploying Visual Question Answering as the primary activity. Adopting a
constructivist learning approach, the system encourages active learning through
real-life visual quizzes, and image-based questions that focus on improving
vocabulary, grammar, and comprehension. The system integrates Vision-Language
Pretraining models to generate contextually relevant image description from
which Large Language Model generate assignments based on customized Arabic
language Learning quizzes thanks to prompting.
  The effectiveness of the tool is evaluated through a manual annotated
benchmark consisting of 1266 real-life visual quizzes, with human participants
providing feedback. The results show a suitable accuracy rates, validating the
tool's potential to bridge the gap in Arabic language education and
highlighting the tool's promise as a reliable, AI-powered resource for Arabic
learners, offering personalized and interactive learning experiences.

</details>


### [48] [Error Detection and Correction for Interpretable Mathematics in Large Language Models](https://arxiv.org/abs/2508.03500)
*Yijin Yang,Cristina Cornelio,Mario Leiva,Paulo Shakarian*

Main category: cs.AI

TL;DR: EDCIM是一种针对可解释数学任务中大型语言模型（LLM）推理错误的检测与纠正方法，通过结合符号错误检测和分层LLM使用，显著降低成本并保持或提高准确性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在多步推理中常出现中间步骤错误，导致最终预测不准确；同时存在幻觉问题，难以遵循特定输出格式（如数学表达式或源代码）。

Method: EDCIM使用LLM生成问题方程组，接着采用符号错误检测框架识别错误并提供有针对性的反馈以供LLM纠正。为优化效率，它结合了轻量级开源LLM和更强大的专有模型，并通过一个超参数平衡成本与准确性。

Result: 实验结果表明，EDCIM显著降低了计算和财务成本，同时在适当配置下保持甚至提高了预测准确性。

Conclusion: EDCIM成功解决了LLM在可解释数学任务中推理错误的问题，提供了一个成本效益高且准确的解决方案，通过结合LLM生成与符号错误检测，并灵活集成不同能力模型。

Abstract: Recent large language models (LLMs) have demonstrated the ability to perform
explicit multi-step reasoning such as chain-of-thought prompting. However,
their intermediate steps often contain errors that can propagate leading to
inaccurate final predictions. Additionally, LLMs still struggle with
hallucinations and often fail to adhere to prescribed output formats, which is
particularly problematic for tasks like generating mathematical expressions or
source code. This work introduces EDCIM (Error Detection and Correction for
Interpretable Mathematics), a method for detecting and correcting these errors
in interpretable mathematics tasks, where the model must generate the exact
functional form that explicitly solve the problem (expressed in natural
language) rather than a black-box solution. EDCIM uses LLMs to generate a
system of equations for a given problem, followed by a symbolic error-detection
framework that identifies errors and provides targeted feedback for LLM-based
correction. To optimize efficiency, EDCIM integrates lightweight, open-source
LLMs with more powerful proprietary models, balancing cost and accuracy. This
balance is controlled by a single hyperparameter, allowing users to control the
trade-off based on their cost and accuracy requirements. Experimental results
across different datasets show that EDCIM significantly reduces both
computational and financial costs, while maintaining, and even improving,
prediction accuracy when the balance is properly configured.

</details>


### [49] [Hidden Dynamics of Massive Activations in Transformer Training](https://arxiv.org/abs/2508.03616)
*Jorge Gallego-Feliciano,S. Aaron McClendon,Juan Morinelli,Stavros Zervoudakis,Antonios Saravanos*

Main category: cs.AI

TL;DR: 本文首次全面分析了Transformer模型训练过程中巨量激活（massive activations）的出现动态，发现其遵循可预测的数学模式，并开发了一个机器学习框架，仅通过架构规范即可预测这些模式的关键参数。


<details>
  <summary>Details</summary>
Motivation: 先前的研究已经表征了完全训练模型中的巨量激活现象，但其在训练过程中出现的时间动态仍不清楚。

Method: 研究使用Pythia模型家族作为测试平台，系统分析了不同模型尺寸在多个训练检查点上的巨量激活发展。开发了一个机器学习框架，用于仅从架构规范预测巨量激活出现模式的数学参数。

Result: 巨量激活的出现遵循可预测的数学模式，可以用一个五参数的指数调制对数函数精确建模。开发的机器学习框架在预测稳态行为方面实现了高精度，在预测出现时间和幅度方面实现了中等精度。

Conclusion: 巨量激活的出现受模型设计支配，可以在训练开始前预测甚至潜在地控制，这对模型的稳定性、训练周期长度、可解释性和优化具有重要意义。

Abstract: Massive activations are scalar values in transformer hidden states that
achieve values orders of magnitude larger than typical activations and have
been shown to be critical for model functionality. While prior work has
characterized these phenomena in fully trained models, the temporal dynamics of
their emergence during training remain poorly understood. We present the first
comprehensive analysis of massive activation development throughout transformer
training, using the Pythia model family as our testbed. Through systematic
analysis of various model sizes across multiple training checkpoints, we
demonstrate that massive activation emergence follows predictable mathematical
patterns that can be accurately modeled using an exponentially-modulated
logarithmic function with five key parameters. We develop a machine learning
framework to predict these mathematical parameters from architectural
specifications alone, achieving high accuracy for steady-state behavior and
moderate accuracy for emergence timing and magnitude. These findings enable
architects to predict and potentially control key aspects of massive activation
emergence through design choices, with significant implications for model
stability, training cycle length, interpretability, and optimization. Our
findings demonstrate that the emergence of massive activations is governed by
model design and can be anticipated, and potentially controlled, before
training begins.

</details>


### [50] [Refining Critical Thinking in LLM Code Generation: A Faulty Premise-based Evaluation Framework](https://arxiv.org/abs/2508.03622)
*Jialin Li,Jinzhe Li,Gengxu Li,Yi Chang,Yuan Wu*

Main category: cs.AI

TL;DR: 大语言模型在错误前提下生成代码时易出现幻觉，本文提出FPBench评估框架，发现模型推理和自查能力差，并揭示了不同错误前提对模型的影响。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）代码生成能力的提升，其对输入前提的依赖性增强。当用户提供包含错误前提的输入时，代码生成幻觉的概率显著增加，暴露出LLMs自查能力的不足。

Method: 提出了首个针对错误前提的代码生成评估框架FPBench。通过系统构建三类错误前提，并集成多维度评估指标，对15个代表性LLMs进行了深入评估。

Result: 1. 大多数模型在错误前提下表现出较差的推理能力和次优的代码生成性能，严重依赖显式提示进行错误检测，自查能力有限；2. 错误前提导致资源投入回报递减，盲目增加输入长度无法提高质量；3. 三类错误前提分别激活了模型不同的缺陷模式，揭示了代码生成模型认知机制的三重分离。

Conclusion: 本研究不仅强调了LLMs在代码生成中主动验证前提的迫切性，而且通过提出的FPBench框架和多维度评估系统，为开发可靠、以人为中心的代码生成模型提供了理论基础和实践途径。

Abstract: With the advancement of code generation capabilities in large language models
(LLMs), their reliance on input premises has intensified. When users provide
inputs containing faulty premises, the probability of code generation
hallucinations rises significantly, exposing deficiencies in their
self-scrutiny capabilities. This paper proposes Faulty Premises Bench
(FPBench), the first code generation evaluation framework targeting faulty
premises. By systematically constructing three categories of faulty premises
and integrating multi-dimensional evaluation metrics, it conducts in-depth
assessments of 15 representative LLMs. The key findings are as follows: (1)
Most models exhibit poor reasoning abilities and suboptimal code generation
performance under faulty premises, heavily relying on explicit prompts for
error detection, with limited self-scrutiny capabilities; (2) Faulty premises
trigger a point of diminishing returns in resource investment, leading to
blindly increasing length fails to enhance quality; (3) The three types of
faulty premises respectively activate distinct defect patterns in models,
revealing a triple dissociation in the cognitive mechanisms of code generation
models. This study not only highlights the urgent need for LLMs to proactively
verify premises in code generation but also, through the proposed FPBench
framework and multi-dimensional evaluation system, provides a theoretical
foundation and practical pathway for developing reliable, human-centric code
generation models.

</details>


### [51] [Automated Algorithmic Discovery for Gravitational-Wave Detection Guided by LLM-Informed Evolutionary Monte Carlo Tree Search](https://arxiv.org/abs/2508.03661)
*He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: 该论文提出了Evo-MCTS框架，结合蒙特卡洛树搜索、进化优化和大型语言模型启发式，用于引力波信号识别。该框架在性能上超越现有方法，并能生成可解释的算法路径，为计算科学领域的自动化算法发现提供了可迁移的方法。


<details>
  <summary>Details</summary>
Motivation: 计算科学发现，特别是在引力波信号识别中，面临挑战。现有算法如匹配滤波（MF）计算成本高昂且依赖预定义模板，而深度神经网络（DNN）是黑箱模型，缺乏可解释性并引入潜在偏差。因此，需要一种既能提升性能又能提供可解释性的新方法。

Method: 论文提出了进化蒙特卡洛树搜索（Evo-MCTS）框架。该方法结合了树状搜索、进化优化和大型语言模型（LLM）启发式，通过领域感知的物理约束指导算法空间探索，旨在创建可解释的算法解决方案。

Result: Evo-MCTS框架在MLGWSC-1基准数据集上，比现有最先进的引力波探测算法性能提升了20.2%。该框架生成了人类可解释的算法路径，揭示了独特的性能模式，并发现了新颖的算法组合。

Conclusion: Evo-MCTS框架不仅显著提升了引力波信号识别的性能，还通过生成可解释的算法路径解决了现有方法的局限性。它提供了一种可迁移的、用于计算科学领域自动化算法发现的新方法。

Abstract: Computational scientific discovery increasingly relies on algorithms to
process complex data and identify meaningful patterns - yet faces persistent
challenges in gravitational-wave signal identification. While existing
algorithmic approaches like matched filtering (MF) and deep neural networks
(DNNs) have achieved partial success, their limitations directly stem from
fundamental limitations: MF's excessive computational demands arise from its
reliance on predefined theoretical waveform templates, while DNNs' black-box
architectures obscure decision logic and introduce hidden biases. We propose
Evolutionary Monte Carlo Tree Search (Evo-MCTS), a framework that addresses
these limitations through systematic algorithm space exploration guided by
domain-aware physical constraints. Our approach combines tree-structured search
with evolutionary optimization and large language model heuristics to create
interpretable algorithmic solutions. Our Evo-MCTS framework demonstrates
substantial improvements, achieving a 20.2\% improvement over state-of-the-art
gravitational wave detection algorithms on the MLGWSC-1 benchmark dataset.
High-performing algorithm variants consistently exceed thresholds. The
framework generates human-interpretable algorithmic pathways that reveal
distinct performance patterns. Beyond performance improvements, our framework
discovers novel algorithmic combinations, thereby establishing a transferable
methodology for automated algorithmic discovery across computational science
domains.

</details>


### [52] [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680)
*Xufang Luo,Yuge Zhang,Zhiyuan He,Zilong Wang,Siyun Zhao,Dongsheng Li,Luna K. Qiu,Yuqing Yang*

Main category: cs.AI

TL;DR: Agent Lightning是一个灵活可扩展的框架，它将强化学习（RL）训练与大型语言模型（LLM）驱动的AI智能体执行完全解耦，实现对任意智能体的RL训练，并展示了稳定的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法将RL训练与智能体紧密耦合，或依赖于带掩码的序列拼接，导致难以与现有智能体集成。因此，需要一个能完全解耦智能体执行和RL训练的通用框架，以支持对任意AI智能体进行RL微调。

Method: Agent Lightning将智能体执行建模为马尔可夫决策过程（MDP），并定义了统一的数据接口。它提出了一个分层RL算法LightningRL，包含信用分配模块，能将任意智能体生成的轨迹分解为训练转换。系统设计上，引入了“训练-智能体解耦”架构，并将智能体可观测性框架引入运行时，提供标准化的智能体微调接口。

Result: 在text-to-SQL、检索增强生成（RAG）和数学工具使用等任务上，Agent Lightning展示了稳定、持续的性能改进。

Conclusion: Agent Lightning框架通过其解耦设计和分层RL算法，证明了在真实世界智能体训练和部署方面的巨大潜力，能够无缝集成并有效训练各种现有AI智能体。

Abstract: We present Agent Lightning, a flexible and extensible framework that enables
Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for
any AI agent. Unlike existing methods that tightly couple RL training with
agent or rely on sequence concatenation with masking, Agent Lightning achieves
complete decoupling between agent execution and training, allowing seamless
integration with existing agents developed via diverse ways (e.g., using
frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from
scratch) with almost ZERO code modifications. By formulating agent execution as
Markov decision process, we define an unified data interface and propose a
hierarchical RL algorithm, LightningRL, which contains a credit assignment
module, allowing us to decompose trajectories generated by ANY agents into
training transition. This enables RL to handle complex interaction logic, such
as multi-agent scenarios and dynamic workflows. For the system design, we
introduce a Training-Agent Disaggregation architecture, and brings agent
observability frameworks into agent runtime, providing a standardized agent
finetuning interface. Experiments across text-to-SQL, retrieval-augmented
generation, and math tool-use tasks demonstrate stable, continuous
improvements, showcasing the framework's potential for real-world agent
training and deployment.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [PyCAT4: A Hierarchical Vision Transformer-based Framework for 3D Human Pose Estimation](https://arxiv.org/abs/2508.02806)
*Zongyou Yang,Jonathan Loo*

Main category: cs.CV

TL;DR: 该研究通过引入Transformer特征提取、时序特征融合和空间金字塔结构，深度优化了PyMAF网络，提出了PyCAT4模型，显著提升了3D人体姿态估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有PyMAF网络结合CNN和金字塔网格对齐已取得显著进展，同时Transformer在计算机视觉时序分析中也实现了突破。为进一步提升3D人体姿态估计精度，本研究旨在优化和改进现有PyMAF架构。

Method: 1. 引入基于自注意力机制的Transformer特征提取网络层，以增强低层特征捕获。2. 通过特征时序融合技术，增强对视频序列中时序信号的理解和捕获。3. 实施空间金字塔结构，实现多尺度特征融合，平衡不同尺度特征表示差异。

Result: 在COCO和3DPW数据集上的实验结果表明，所提出的改进策略显著增强了网络在人体姿态估计中的检测能力。

Conclusion: 本研究提出的新PyCAT4模型通过结合Transformer、时序融合和空间金字塔结构，有效提升了人体姿态估计技术的性能，进一步推动了该领域的发展。

Abstract: Recently, a significant improvement in the accuracy of 3D human pose
estimation has been achieved by combining convolutional neural networks (CNNs)
with pyramid grid alignment feedback loops. Additionally, innovative
breakthroughs have been made in the field of computer vision through the
adoption of Transformer-based temporal analysis architectures. Given these
advancements, this study aims to deeply optimize and improve the existing Pymaf
network architecture. The main innovations of this paper include: (1)
Introducing a Transformer feature extraction network layer based on
self-attention mechanisms to enhance the capture of low-level features; (2)
Enhancing the understanding and capture of temporal signals in video sequences
through feature temporal fusion techniques; (3) Implementing spatial pyramid
structures to achieve multi-scale feature fusion, effectively balancing feature
representations differences across different scales. The new PyCAT4 model
obtained in this study is validated through experiments on the COCO and 3DPW
datasets. The results demonstrate that the proposed improvement strategies
significantly enhance the network's detection capability in human pose
estimation, further advancing the development of human pose estimation
technology.

</details>


### [54] [DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a Stage-Wise Diffusion Transformer Framework](https://arxiv.org/abs/2508.02807)
*Tongchun Zuo,Zaiyu Huang,Shuliang Ning,Ente Lin,Chao Liang,Zerong Zheng,Jianwen Jiang,Yuan Zhang,Mingyuan Gao,Xin Dong*

Main category: cs.CV

TL;DR: DreamVVT是一种基于Diffusion Transformers的两阶段视频虚拟试穿框架，旨在解决现有方法对配对数据集的依赖、细节保留不足和时间一致性差的问题，通过利用非配对数据和预训练模型生成高质量、时间一致的虚拟试穿视频。


<details>
  <summary>Details</summary>
Motivation: 现有端到端视频虚拟试穿方法严重依赖稀缺的配对服装中心数据集，未能有效利用先进视觉模型和测试时输入的先验知识，导致在无约束场景下难以准确保留精细服装细节和维持时间一致性。

Method: DreamVVT是一个两阶段框架：第一阶段，从输入视频中采样代表性帧，并利用集成视觉-语言模型（VLM）的多帧试穿模型合成高保真、语义一致的关键帧试穿图像，作为后续视频生成的补充外观指导。第二阶段，从输入内容中提取骨架图、精细运动和外观描述，连同关键帧试穿图像一起输入到通过LoRA适配器增强的预训练视频生成模型中，以确保未见区域的长期时间连贯性和高度逼真的动态运动。

Result: 广泛的定量和定性实验表明，DreamVVT在真实场景中超越了现有方法，在保留详细服装内容和时间稳定性方面表现更优。

Conclusion: DreamVVT通过创新的两阶段DiT框架，有效解决了视频虚拟试穿中细节保留和时间一致性的挑战，并通过利用非配对数据和预训练模型，显著提升了在真实世界场景中的适应性和性能。

Abstract: Video virtual try-on (VVT) technology has garnered considerable academic
interest owing to its promising applications in e-commerce advertising and
entertainment. However, most existing end-to-end methods rely heavily on scarce
paired garment-centric datasets and fail to effectively leverage priors of
advanced visual models and test-time inputs, making it challenging to
accurately preserve fine-grained garment details and maintain temporal
consistency in unconstrained scenarios. To address these challenges, we propose
DreamVVT, a carefully designed two-stage framework built upon Diffusion
Transformers (DiTs), which is inherently capable of leveraging diverse unpaired
human-centric data to enhance adaptability in real-world scenarios. To further
leverage prior knowledge from pretrained models and test-time inputs, in the
first stage, we sample representative frames from the input video and utilize a
multi-frame try-on model integrated with a vision-language model (VLM), to
synthesize high-fidelity and semantically consistent keyframe try-on images.
These images serve as complementary appearance guidance for subsequent video
generation. \textbf{In the second stage}, skeleton maps together with
fine-grained motion and appearance descriptions are extracted from the input
content, and these along with the keyframe try-on images are then fed into a
pretrained video generation model enhanced with LoRA adapters. This ensures
long-term temporal coherence for unseen regions and enables highly plausible
dynamic motions. Extensive quantitative and qualitative experiments demonstrate
that DreamVVT surpasses existing methods in preserving detailed garment content
and temporal stability in real-world scenarios. Our project page
https://virtu-lab.github.io/

</details>


### [55] [Elucidating the Role of Feature Normalization in IJEPA](https://arxiv.org/abs/2508.02829)
*Adam Colton*

Main category: cs.CV

TL;DR: 本文提出IJEPA中特征层归一化（LN）破坏了视觉token的自然能量层级，导致性能下降。通过用DynTanh激活替换LN，可以更好地保留token能量，消除伪影，并显著提升自监督学习表现。


<details>
  <summary>Details</summary>
Motivation: 标准IJEPA中，教师编码器输出的特征在作为学生编码器和预测器的蒸馏目标前会进行层归一化（LN）。作者认为这种特征归一化破坏了视觉token的自然能量层级（高能量token编码语义重要区域），导致所有特征L2范数相同，无法优先处理语义丰富的区域，并在损失图中产生棋盘状伪影。

Method: 将IJEPA中的特征层归一化（LN）替换为DynTanh激活函数。DynTanh能更好地保留token能量，允许高能量token对预测损失做出更大贡献。

Result: 使用DynTanh训练的IJEPA模型展现出更长的损失分布尾部，并消除了损失图中的棋盘状伪影。经验结果显示，ViT-Small模型在ImageNet线性探测准确率从38%提升至42.7%，在NYU Depth V2单目深度估计任务中RMSE降低了0.08。

Conclusion: 研究表明，保留视觉token的自然能量对于有效的自监督视觉表征学习至关重要。

Abstract: In the standard image joint embedding predictive architecture (IJEPA),
features at the output of the teacher encoder are layer normalized (LN) before
serving as a distillation target for the student encoder and predictor. We
propose that this feature normalization disrupts the natural energy hierarchy
of visual tokens, where high-energy tokens (those with larger L2 norms) encode
semantically important image regions. LN forces all features to have identical
L2 norms, effectively equalizing their energies and preventing the model from
prioritizing semantically rich regions. We find that IJEPA models trained with
feature LN exhibit loss maps with significant checkerboard-like artifacts. We
propose that feature LN be replaced with a DynTanh activation as the latter
better preserves token energies and allows high-energy tokens to greater
contribute to the prediction loss. We show that IJEPA trained with feature
DynTanh exhibits a longer-tailed loss distribution and fixes the checkerboard
artifacts in the loss map. Our empirical results show that our simple
modification improves ImageNet linear probe accuracy from 38% to 42.7% for
ViT-Small and reduces RMSE by 0.08 on NYU Depth V2 monocular depth estimation.
These results suggest that preserving natural token energies is crucial for
effective self-supervised visual representation learning.

</details>


### [56] [GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing](https://arxiv.org/abs/2508.02831)
*Mikołaj Zieliński,Krzysztof Byrski,Tomasz Szczepanik,Przemysław Spurek*

Main category: cs.CV

TL;DR: GENIE是一个混合模型，结合了NeRF的光真实感渲染质量与GS的可编辑结构化表示，实现了实时、局部感知的3D场景交互式编辑。


<details>
  <summary>Details</summary>
Motivation: NeRF在新视角合成方面表现出色但难以编辑和物理交互；GS虽然可实时渲染和编辑，但在渲染质量上可能不如NeRF。研究旨在弥合几何编辑与神经渲染之间的鸿沟，创建一个兼具两者优点的模型。

Method: GENIE是一个混合模型，为每个高斯体分配可训练的特征嵌入，而非球谐函数。这些嵌入用于根据查询点最近的k个高斯体来条件化NeRF网络。为提高效率，引入了Ray-Traced Gaussian Proximity Search (RT-GPS)进行快速最近高斯体搜索。同时，集成多分辨率哈希网格来初始化和更新高斯特征。

Result: GENIE实现了实时、局部感知的编辑，高斯基元的位置或修改能立即反映在渲染输出中。它支持直观的场景操作、动态交互，并与物理模拟兼容。

Conclusion: GENIE通过结合隐式（NeRF）和显式（GS）表示的优势，成功弥合了几何编辑和神经渲染之间的差距，提供了直观的场景操作、动态交互和物理模拟兼容性。

Abstract: Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently
transformed 3D scene representation and rendering. NeRF achieves high-fidelity
novel view synthesis by learning volumetric representations through neural
networks, but its implicit encoding makes editing and physical interaction
challenging. In contrast, GS represents scenes as explicit collections of
Gaussian primitives, enabling real-time rendering, faster training, and more
intuitive manipulation. This explicit structure has made GS particularly
well-suited for interactive editing and integration with physics-based
simulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural
Radiance Fields Interactive Editing), a hybrid model that combines the
photorealistic rendering quality of NeRF with the editable and structured
representation of GS. Instead of using spherical harmonics for appearance
modeling, we assign each Gaussian a trainable feature embedding. These
embeddings are used to condition a NeRF network based on the k nearest
Gaussians to each query point. To make this conditioning efficient, we
introduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest
Gaussian search based on a modified ray-tracing pipeline. We also integrate a
multi-resolution hash grid to initialize and update Gaussian features.
Together, these components enable real-time, locality-aware editing: as
Gaussian primitives are repositioned or modified, their interpolated influence
is immediately reflected in the rendered output. By combining the strengths of
implicit and explicit representations, GENIE supports intuitive scene
manipulation, dynamic interaction, and compatibility with physical simulation,
bridging the gap between geometry-based editing and neural rendering. The code
can be found under (https://github.com/MikolajZielinski/genie)

</details>


### [57] [RefineSeg: Dual Coarse-to-Fine Learning for Medical Image Segmentation](https://arxiv.org/abs/2508.02844)
*Anghong Du,Nay Aung,Theodoros N. Arvanitis,Stefan K. Piechnik,Joao A C Lima,Steffen E. Petersen,Le Zhang*

Main category: cs.CV

TL;DR: 该论文提出了一种新的粗到精分割框架，仅使用粗粒度标注（包括目标和补充绘图），通过引入转移矩阵来建模不准确和不完整的区域，并联合训练多组粗标注，以实现对精确标签的鲁棒近似。


<details>
  <summary>Details</summary>
Motivation: 医学图像的像素级标注成本高昂且需要医学专业知识，这限制了监督分割任务的应用。

Method: 提出了一种粗到精的分割框架，该框架完全依赖粗粒度标注。通过引入转移矩阵来建模粗标注中不准确和不完整的区域。通过联合训练多组粗标注，逐步优化网络输出并推断真实的分割分布，通过基于矩阵的建模实现精确标签的鲁棒近似。

Result: 在ACDC、MSCMRseg和UK Biobank三个心脏影像数据集上的实验结果表明，该方法超越了现有的弱监督方法，并接近全监督方法的性能。

Conclusion: 该方法证明了其灵活性和有效性，能够仅利用粗粒度标注，通过矩阵建模实现对精确标签的鲁棒近似，解决了医学图像分割中高成本标注的挑战。

Abstract: High-quality pixel-level annotations of medical images are essential for
supervised segmentation tasks, but obtaining such annotations is costly and
requires medical expertise. To address this challenge, we propose a novel
coarse-to-fine segmentation framework that relies entirely on coarse-level
annotations, encompassing both target and complementary drawings, despite their
inherent noise. The framework works by introducing transition matrices in order
to model the inaccurate and incomplete regions in the coarse annotations. By
jointly training on multiple sets of coarse annotations, it progressively
refines the network's outputs and infers the true segmentation distribution,
achieving a robust approximation of precise labels through matrix-based
modeling. To validate the flexibility and effectiveness of the proposed method,
we demonstrate the results on two public cardiac imaging datasets, ACDC and
MSCMRseg, and further evaluate its performance on the UK Biobank dataset.
Experimental results indicate that our approach surpasses the state-of-the-art
weakly supervised methods and closely matches the fully supervised approach.

</details>


### [58] [MIDAR: Mimicking LiDAR Detection for Traffic Applications with a Lightweight Plug-and-Play Model](https://arxiv.org/abs/2508.02858)
*Tianheng Zhu,Yiheng Feng*

Main category: cs.CV

TL;DR: MIDAR是一个LiDAR检测模拟模型，它通过使用微观交通模拟器中可用的车辆级特征，来近似逼真的LiDAR检测结果，从而弥合了可扩展交通模拟器和高保真感知模拟器之间的差距。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶技术中，合作感知数据对交通应用至关重要。然而，现有模拟器要么可扩展但缺乏感知建模（如SUMO），要么能生成高保真感知数据但难以扩展到多AV场景（如CARLA），导致大规模合作感知研究受限。

Method: 本文提出了MIDAR模型，它利用微观交通模拟器中易于获得的车辆级特征（空间布局、尺寸）来预测LiDAR检测结果中的真阳性（TPs）和假阴性（FNs）。具体方法包括构建精炼多跳视线（RM-LoS）图来编码车辆间的遮挡关系，并在此基础上采用GRU增强的APPNP架构来传播自我AV和遮挡车辆的特征，以预测目标检测结果。

Result: MIDAR在nuScenes AD数据集上，以0.909的AUC近似了主流3D LiDAR检测模型CenterPoint的检测结果。通过两个基于合作感知的交通应用进一步验证了这种逼真检测建模的必要性，尤其对于需要精确个体车辆观测（如位置、速度、车道索引）的任务。

Conclusion: MIDAR提供了一种有效的方法，将逼真的LiDAR检测建模引入到可扩展的交通模拟器和轨迹数据集中，从而弥合了模拟器之间的差距，对于大规模多AV场景下的合作感知研究具有重要意义，并将开源提供。

Abstract: As autonomous driving (AD) technology advances, increasing research has
focused on leveraging cooperative perception (CP) data collected from multiple
AVs to enhance traffic applications. Due to the impracticality of large-scale
real-world AV deployments, simulation has become the primary approach in most
studies. While game-engine-based simulators like CARLA generate high-fidelity
raw sensor data (e.g., LiDAR point clouds) which can be used to produce
realistic detection outputs, they face scalability challenges in multi-AV
scenarios. In contrast, microscopic traffic simulators such as SUMO scale
efficiently but lack perception modeling capabilities. To bridge this gap, we
propose MIDAR, a LiDAR detection mimicking model that approximates realistic
LiDAR detections using vehicle-level features readily available from
microscopic traffic simulators. Specifically, MIDAR predicts true positives
(TPs) and false negatives (FNs) from ideal LiDAR detection results based on the
spatial layouts and dimensions of surrounding vehicles. A Refined Multi-hop
Line-of-Sight (RM-LoS) graph is constructed to encode the occlusion
relationships among vehicles, upon which MIDAR employs a GRU-enhanced APPNP
architecture to propagate features from the ego AV and occluding vehicles to
the prediction target. MIDAR achieves an AUC of 0.909 in approximating the
detection results generated by CenterPoint, a mainstream 3D LiDAR detection
model, on the nuScenes AD dataset. Two CP-based traffic applications further
validate the necessity of such realistic detection modeling, particularly for
tasks requiring accurate individual vehicle observations (e.g., position,
speed, lane index). As demonstrated in the applications, MIDAR can be
seamlessly integrated into traffic simulators and trajectory datasets and will
be open-sourced upon publication.

</details>


### [59] [Evaluation and Analysis of Deep Neural Transformers and Convolutional Neural Networks on Modern Remote Sensing Datasets](https://arxiv.org/abs/2508.02871)
*J. Alex Hurt,Trevor M. Bajkowski,Grant J. Scott,Curt H. Davis*

Main category: cs.CV

TL;DR: 本文比较了Transformer和卷积神经网络在遥感图像目标检测中的性能，发现在高分辨率卫星图像上，Transformer模型展现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 深度卷积神经网络在计算机视觉领域取得了显著进展，但随着视觉Transformer的出现，需要大规模评估其在遥感数据上的性能，以了解其在卫星图像目标检测任务中的表现。

Method: 研究探索了基于Transformer的神经网络在高分辨率电光卫星图像中进行目标检测的应用。比较了11种不同的边界框检测和定位算法（其中7种发表于2020年之后，全部发表于2015年之后），包括5种基于Transformer的架构和6种卷积网络。这些模型在3个最先进的开源高分辨率遥感图像数据集上进行了训练和评估，共计33个深度神经网络模型。

Result: 研究结果表明，基于Transformer的架构在多种公开基准数据集上，针对高分辨率电光卫星图像的目标检测任务，展现了最先进的性能。

Conclusion: 基于Transformer的神经网络在高分辨率遥感图像目标检测方面表现出色，达到了最先进的水平，这表明它们是该领域极具潜力的选择。

Abstract: In 2012, AlexNet established deep convolutional neural networks (DCNNs) as
the state-of-the-art in CV, as these networks soon led in visual tasks for many
domains, including remote sensing. With the publication of Visual Transformers,
we are witnessing the second modern leap in computational vision, and as such,
it is imperative to understand how various transformer-based neural networks
perform on satellite imagery. While transformers have shown high levels of
performance in natural language processing and CV applications, they have yet
to be compared on a large scale to modern remote sensing data. In this paper,
we explore the use of transformer-based neural networks for object detection in
high-resolution electro-optical satellite imagery, demonstrating
state-of-the-art performance on a variety of publicly available benchmark data
sets. We compare eleven distinct bounding-box detection and localization
algorithms in this study, of which seven were published since 2020, and all
eleven since 2015. The performance of five transformer-based architectures is
compared with six convolutional networks on three state-of-the-art opensource
high-resolution remote sensing imagery datasets ranging in size and complexity.
Following the training and evaluation of thirty-three deep neural models, we
then discuss and analyze model performance across various feature extraction
methodologies and detection algorithms.

</details>


### [60] [VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction](https://arxiv.org/abs/2508.02890)
*Rongxin Jiang,Robert Long,Chenghao Gu,Mingrui Yan*

Main category: cs.CV

TL;DR: VisuCraft是一个新颖的框架，通过集成多模态结构化信息提取器和动态提示生成模块，显著提升了大型视觉-语言模型（LVLMs）在复杂视觉引导创意内容生成（如故事、诗歌）方面的视觉保真度、创造力和指令遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs在生成长文本时，难以保持高视觉保真度、真正的创造力以及精确遵循用户指令。

Method: VisuCraft包含两个核心模块：多模态结构化信息提取器（E），用于从输入图像中提取细粒度视觉属性并转化为结构化表示；动态提示生成模块（G），将提取的结构化信息与用户指令结合，为底层LVLMs（如LLaVA、InstructBLIP）生成高度优化的提示。

Result: 在自建的ImageStoryGen-500K数据集上，使用VisuGen指标（视觉关联度、创造力、指令遵循）评估，VisuCraft在故事生成和诗歌创作等任务上持续优于基线LVLMs，尤其在创造力和指令遵循方面表现出显著提升。

Conclusion: VisuCraft有效生成了富有想象力、视觉关联且符合用户需求的长篇创意文本，展示了其在复杂创意AI应用中解锁LVLMs新潜力的能力。

Abstract: This paper introduces VisuCraft, a novel framework designed to significantly
enhance the capabilities of Large Vision-Language Models (LVLMs) in complex
visual-guided creative content generation. Existing LVLMs often exhibit
limitations in maintaining high visual fidelity, genuine creativity, and
precise adherence to nuanced user instructions when generating long-form texts.
VisuCraft addresses these challenges by integrating a multimodal structured
information extractor (E) and a dynamic prompt generation module (G). The
extractor distills fine-grained visual attributes from input images into a
rich, structured representation, which the dynamic prompt module then combines
with user instructions to create highly optimized prompts for underlying LVLMs
(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed
ImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,
and Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs
across tasks like story generation and poetry composition. Our results
demonstrate remarkable improvements, particularly in creativity and instruction
adherence, validating VisuCraft's effectiveness in producing imaginative,
visually grounded, and user-aligned long-form creative text. This work unlocks
new potential for LVLMs in sophisticated creative AI applications.

</details>


### [61] [RDDPM: Robust Denoising Diffusion Probabilistic Model for Unsupervised Anomaly Segmentation](https://arxiv.org/abs/2508.02903)
*Mehrdad Moradi,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出了一种鲁棒去噪扩散概率模型（RDDPM），用于在仅有受污染（正常和异常混合）数据的情况下进行无监督异常分割，通过将DDPM重新解释为鲁棒回归问题，并取得了显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在无监督异常分割中表现出色，但它们通常需要纯净的正常数据进行训练，这限制了其在实际应用中的普适性，因为真实场景中数据往往是受污染的。

Method: 将数据最大似然估计重新表述为非线性回归问题，从而通过回归视角重新解释去噪扩散概率模型（DDPM）。在此基础上，利用鲁棒回归技术推导出鲁棒版本的DDPM，使其能够在只有受污染数据的情况下进行训练。

Result: 在仅有受污染数据可用的情况下，该方法在无监督异常分割任务上优于当前最先进的扩散模型。在MVTec数据集上，AUROC提高了8.08%，AUPRC提高了10.37%。

Conclusion: 该研究成功开发了一种能在受污染数据上进行训练的鲁棒扩散模型，显著提升了扩散模型在更现实、数据不纯净环境中的异常分割能力。

Abstract: Recent advancements in diffusion models have demonstrated significant success
in unsupervised anomaly segmentation. For anomaly segmentation, these models
are first trained on normal data; then, an anomalous image is noised to an
intermediate step, and the normal image is reconstructed through backward
diffusion. Unlike traditional statistical methods, diffusion models do not rely
on specific assumptions about the data or target anomalies, making them
versatile for use across different domains. However, diffusion models typically
assume access to normal data for training, limiting their applicability in
realistic settings. In this paper, we propose novel robust denoising diffusion
models for scenarios where only contaminated (i.e., a mix of normal and
anomalous) unlabeled data is available. By casting maximum likelihood
estimation of the data as a nonlinear regression problem, we reinterpret the
denoising diffusion probabilistic model through a regression lens. Using robust
regression, we derive a robust version of denoising diffusion probabilistic
models. Our novel framework offers flexibility in constructing various robust
diffusion models. Our experiments show that our approach outperforms current
state of the art diffusion models, for unsupervised anomaly segmentation when
only contaminated data is available. Our method outperforms existing
diffusion-based approaches, achieving up to 8.08\% higher AUROC and 10.37\%
higher AUPRC on MVTec datasets. The implementation code is available at:
https://github.com/mehrdadmoradi124/RDDPM

</details>


### [62] [How Would It Sound? Material-Controlled Multimodal Acoustic Profile Generation for Indoor Scenes](https://arxiv.org/abs/2508.02905)
*Mahnoor Fatima Saad,Ziad Al-Halah*

Main category: cs.CV

TL;DR: 该研究提出了一种材料控制的声学剖面生成任务，通过一个新颖的编码器-解码器模型，根据用户定义的材料配置生成目标房间脉冲响应（RIR），并创建了一个新的数据集来支持该任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以动态模拟室内场景中声音随不同材料配置的变化。研究旨在解决给定室内场景的音视频特征，根据用户在推理时定义的材料配置生成目标声学剖面（RIR）的问题。

Method: 采用新颖的编码器-解码器方法：编码器从音视频观察中编码场景的关键属性，解码器根据用户提供的材料规格生成目标RIR。为支持此任务，创建了新的基准数据集——Acoustic Wonderland Dataset，用于开发和评估材料感知RIR预测方法。

Result: 所提出的模型能够有效地编码材料信息并生成高保真RIR，性能优于多个基线方法和现有最先进的方法。

Conclusion: 该模型成功实现了材料控制的声学剖面生成，能够根据动态定义的材料配置生成多样化的RIR，为声学模拟和设计提供了有效工具。

Abstract: How would the sound in a studio change with a carpeted floor and acoustic
tiles on the walls? We introduce the task of material-controlled acoustic
profile generation, where, given an indoor scene with specific audio-visual
characteristics, the goal is to generate a target acoustic profile based on a
user-defined material configuration at inference time. We address this task
with a novel encoder-decoder approach that encodes the scene's key properties
from an audio-visual observation and generates the target Room Impulse Response
(RIR) conditioned on the material specifications provided by the user. Our
model enables the generation of diverse RIRs based on various material
configurations defined dynamically at inference time. To support this task, we
create a new benchmark, the Acoustic Wonderland Dataset, designed for
developing and evaluating material-aware RIR prediction methods under diverse
and challenging settings. Our results demonstrate that the proposed model
effectively encodes material information and generates high-fidelity RIRs,
outperforming several baselines and state-of-the-art methods.

</details>


### [63] [Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces](https://arxiv.org/abs/2508.02917)
*Vebjørn Haug Kåsene,Pierre Lison*

Main category: cs.CV

TL;DR: 本文探讨了预训练大型视觉语言模型（LVLMs）在视觉语言导航（VLN）任务中的应用潜力，并评估了它们在低级和全景动作空间下的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的VLN系统多依赖于专门设计和优化的模型，而现成的LVLMs的潜力尚未被充分探索。此外，VLN任务的动作空间已从低级原子动作转向离散可导航的全景视图，需要研究LVLMs如何支持这两种范式。

Method: 研究通过在Room-to-Room (R2R) 数据集上微调开源模型Qwen2.5-VL-3B-Instruct，并在低级和全景动作空间下评估其经验性能，过程中未进行架构修改或基于模拟器的训练。

Result: 最佳模型在R2R测试集上取得了41%的成功率，表明现成的LVLMs可以学习执行视觉语言导航任务。

Conclusion: 尽管现成的LVLMs能够学习并执行视觉语言导航，但它们的性能仍落后于专门为此任务设计的模型。

Abstract: Vision-and-Language Navigation (VLN) refers to the task of enabling
autonomous robots to navigate unfamiliar environments by following natural
language instructions. While recent Large Vision-Language Models (LVLMs) have
shown promise in this task, most current VLM systems rely on models
specifically designed and optimized for navigation, leaving the potential of
off-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used
low-level action spaces with egocentric views and atomic actions (such as "turn
left" or "move forward"), newer models tend to favor panoramic action spaces
with discrete navigable viewpoints. This paper investigates (1) whether
off-the-shelf LVLMs (fine-tuned without architectural modifications or
simulator-based training) can effectively support VLN tasks and (2) whether
such models can support both low-level and panoramic action paradigms. To this
end, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the
Room-to-Room (R2R) dataset and evaluate its empirical performance across both
low-level and panoramic action spaces. The best resulting model achieves a 41%
success rate on the R2R test set, demonstrating that while off-the-shelf LVLMs
can learn to perform Vision-and-Language Navigation, they still lag behind
models specifically designed for this task.

</details>


### [64] [How Diffusion Prior Landscapes Shape the Posterior in Blind Deconvolution](https://arxiv.org/abs/2508.02923)
*Minh-Hai Nguyen,Edouard Pauwels,Pierre Weiss*

Main category: cs.CV

TL;DR: 研究发现，在盲去卷积中，当与基于扩散的图像先验结合时，最大后验（MAP）估计器倾向于产生模糊结果，但后验分布的局部最小值能得到清晰的自然图像，这需要良好的局部初始化。


<details>
  <summary>Details</summary>
Motivation: MAP估计在盲去卷积中常用于恢复清晰图像，但当与稀疏性先验结合时，它已被证明偏向模糊解，限制了其有效性。本文旨在重新审视这一问题，并探索基于扩散的先验如何影响MAP估计。

Method: 通过对基于扩散的图像先验的似然景观进行经验性检查，揭示其特性；然后对盲去模糊后验进行理论分析；最后通过数值实验验证分析结果。

Result: 1. 模糊图像在基于扩散的先验下倾向于具有更高的似然值；2. 似然景观包含大量对应于自然图像的局部最小值；3. MAP估计器倾向于产生尖锐的模糊滤波器（接近狄拉克函数）和模糊的图像解；4. 后验分布的局部最小值（可通过梯度下降获得）对应于真实、自然的图像，有效解决了盲去卷积问题。

Conclusion: 要克服MAP估计的局限性，需要良好的局部初始化以找到后验景观中的局部最小值。这些发现对设计改进的先验和优化技术具有实际指导意义。

Abstract: The Maximum A Posteriori (MAP) estimation is a widely used framework in blind
deconvolution to recover sharp images from blurred observations. The estimated
image and blur filter are defined as the maximizer of the posterior
distribution. However, when paired with sparsity-promoting image priors, MAP
estimation has been shown to favors blurry solutions, limiting its
effectiveness. In this paper, we revisit this result using diffusion-based
priors, a class of models that capture realistic image distributions. Through
an empirical examination of the prior's likelihood landscape, we uncover two
key properties: first, blurry images tend to have higher likelihoods; second,
the landscape contains numerous local minimizers that correspond to natural
images. Building on these insights, we provide a theoretical analysis of the
blind deblurring posterior. This reveals that the MAP estimator tends to
produce sharp filters (close to the Dirac delta function) and blurry solutions.
However local minimizers of the posterior, which can be obtained with gradient
descent, correspond to realistic, natural images, effectively solving the blind
deconvolution problem. Our findings suggest that overcoming MAP's limitations
requires good local initialization to local minima in the posterior landscape.
We validate our analysis with numerical experiments, demonstrating the
practical implications of our insights for designing improved priors and
optimization techniques.

</details>


### [65] [Live Demonstration: Neuromorphic Radar for Gesture Recognition](https://arxiv.org/abs/2508.03324)
*Satyapreet Singh Yadav,Chandra Sekhar Seelamantula,Chetan Singh Thakur*

Main category: cs.CV

TL;DR: 该研究提出了一种基于事件驱动的神经形态雷达框架，通过异步sigma-delta编码将雷达信号转换为稀疏脉冲，并在低功耗微控制器上实现实时、低功耗的手势识别，显著降低了系统开销。


<details>
  <summary>Details</summary>
Motivation: 传统的雷达手势识别（HGR）系统持续采样和处理数据，导致高昂的内存、功耗和计算开销，促使研究人员寻求更高效、低功耗的解决方案。

Method: 该系统包含一个24 GHz多普勒雷达前端和一个定制的神经形态采样器，通过异步sigma-delta编码将中频（IF）信号转换为稀疏的脉冲表示。这些事件由部署在Cortex-M0微控制器上的轻量级神经网络直接处理，无需频谱图重建。系统仅在检测到有意义运动时激活，采用事件驱动的处理方式。

Result: 在包含七名用户五种手势的数据集上进行评估，该系统实现了超过85%的实时识别准确率，并显著降低了内存、功耗和计算开销。

Conclusion: 该工作首次将生物启发式异步sigma-delta编码和事件驱动处理框架应用于基于雷达的手势识别，为实时、低功耗的雷达手势识别提供了新的范例。

Abstract: We present a neuromorphic radar framework for real-time, low-power hand
gesture recognition (HGR) using an event-driven architecture inspired by
biological sensing. Our system comprises a 24 GHz Doppler radar front-end and a
custom neuromorphic sampler that converts intermediate-frequency (IF) signals
into sparse spike-based representations via asynchronous sigma-delta encoding.
These events are directly processed by a lightweight neural network deployed on
a Cortex-M0 microcontroller, enabling low-latency inference without requiring
spectrogram reconstruction. Unlike conventional radar HGR pipelines that
continuously sample and process data, our architecture activates only when
meaningful motion is detected, significantly reducing memory, power, and
computation overhead. Evaluated on a dataset of five gestures collected from
seven users, our system achieves > 85% real-time accuracy. To the best of our
knowledge, this is the first work that employs bio-inspired asynchronous
sigma-delta encoding and an event-driven processing framework for radar-based
HGR.

</details>


### [66] [Infrared Object Detection with Ultra Small ConvNets: Is ImageNet Pretraining Still Useful?](https://arxiv.org/abs/2508.02927)
*Srikanth Muralidharan,Heitor R. Medeiros,Masih Aminbeidokhti,Eric Granger,Marco Pedersoli*

Main category: cs.CV

TL;DR: 研究了ImageNet预训练对超小型模型（参数<1M）在红外目标检测任务中鲁棒性的影响，发现预训练仍有用，但超过一定容量阈值后，对域外鲁棒性的提升回报递减。


<details>
  <summary>Details</summary>
Motivation: 现实应用需要模型在有限硬件的嵌入式设备上运行，同时对不同操作条件和模态具有鲁棒性。虽然预训练对常规大小模型有益，但其对超小型模型的影响尚不明确。

Method: 通过缩放定律构建了两类超小型骨干网络（参数<1M），并系统研究了ImageNet预训练对它们在红外视觉模态下，下游目标检测任务鲁棒性的影响。实验在三个不同数据集上进行。

Result: ImageNet预训练仍然有用，但当模型容量超过某个阈值后，它在域外检测鲁棒性方面的回报会递减。

Conclusion: 建议实践者仍然使用预训练。同时，应避免使用过小的模型，因为它们虽然可能在域内问题上表现良好，但在工作条件不同时会变得脆弱。

Abstract: Many real-world applications require recognition models that are robust to
different operational conditions and modalities, but at the same time run on
small embedded devices, with limited hardware. While for normal size models,
pre-training is known to be very beneficial in accuracy and robustness, for
small models, that can be employed for embedded and edge devices, its effect is
not clear. In this work, we investigate the effect of ImageNet pretraining on
increasingly small backbone architectures (ultra-small models, with $<$1M
parameters) with respect to robustness in downstream object detection tasks in
the infrared visual modality. Using scaling laws derived from standard object
recognition architectures, we construct two ultra-small backbone families and
systematically study their performance. Our experiments on three different
datasets reveal that while ImageNet pre-training is still useful, beyond a
certain capacity threshold, it offers diminishing returns in terms of
out-of-distribution detection robustness. Therefore, we advise practitioners to
still use pre-training and, when possible avoid too small models as while they
might work well for in-domain problems, they are brittle when working
conditions are different.

</details>


### [67] [Sparsity and Total Variation Constrained Multilayer Linear Unmixing for Hyperspectral Imagery](https://arxiv.org/abs/2508.03403)
*Gang Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为STVMLU的新型高光谱解混方法，通过结合多层线性解混模型、全变分（TV）空间相似性约束和L1/2稀疏性约束，并利用ADMM算法优化，实现了更好的解混性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱解混是高光谱图像应用中的一个关键预处理步骤，旨在估计材料特征（端元）及其比例（丰度），提高解混精度是重要目标。

Method: 该方法基于多层矩阵分解模型。为提高解混精度，引入了全变分（TV）约束以考虑相邻空间相似性，并采用L1/2范数稀疏约束来有效表征丰度矩阵的稀疏性。模型优化采用交替方向乘子法（ADMM），可同时提取端元及其对应的丰度矩阵。

Result: 实验结果表明，所提出的STVMLU方法与其他算法相比，性能有所提升。

Conclusion: STVMLU方法通过结合空间相似性和稀疏性约束，有效提高了高光谱图像解混的准确性。

Abstract: Hyperspectral unmixing aims at estimating material signatures (known as
endmembers) and the corresponding proportions (referred to abundances), which
is a critical preprocessing step in various hyperspectral imagery applications.
This study develops a novel approach called sparsity and total variation (TV)
constrained multilayer linear unmixing (STVMLU) for hyperspectral imagery.
Specifically, based on a multilayer matrix factorization model, to improve the
accuracy of unmixing, a TV constraint is incorporated to consider adjacent
spatial similarity. Additionally, a L1/2-norm sparse constraint is adopted to
effectively characterize the sparsity of the abundance matrix. For optimizing
the STVMLU model, the method of alternating direction method of multipliers
(ADMM) is employed, which allows for the simultaneous extraction of endmembers
and their corresponding abundance matrix. Experimental results illustrate the
enhanced performance of the proposed STVMLU when compared to other algorithms.

</details>


### [68] [X-Actor: Emotional and Expressive Long-Range Portrait Acting from Audio](https://arxiv.org/abs/2508.02944)
*Chenxu Zhang,Zenan Li,Hongyi Xu,You Xie,Xiaochen Zhao,Tianpei Gu,Guoxian Song,Xin Chen,Chao Liang,Jianwen Jiang,Linjie Luo*

Main category: cs.CV

TL;DR: X-Actor是一个新颖的音频驱动肖像动画框架，能从单张图片和音频生成逼真、富有情感的长程说话人视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法侧重于唇部同步和短程视觉保真度，难以生成捕捉细微、动态演变情感的长时间、演员级肖像表演。

Method: 采用两阶段解耦生成流程：首先，一个音频条件自回归扩散模型在长时间上下文窗口内预测富有表现力但与身份无关的面部运动潜在令牌；然后，一个基于扩散的视频合成模块将这些运动转化为高保真视频动画。通过在紧凑的面部运动潜在空间中操作并采用扩散强制训练范式，有效捕捉音频与面部动态之间的长程关联，实现无限长度的情感丰富运动预测而无误差累积。

Result: X-Actor能生成引人入胜、电影风格的表演，超越了标准的说话人动画，并在长程、音频驱动的情感肖像表演方面达到了最先进的水平。

Conclusion: X-Actor成功解决了传统音频驱动肖像动画在长程情感表达和连贯性方面的不足，实现了高质量、富有情感的逼真视频生成。

Abstract: We present X-Actor, a novel audio-driven portrait animation framework that
generates lifelike, emotionally expressive talking head videos from a single
reference image and an input audio clip. Unlike prior methods that emphasize
lip synchronization and short-range visual fidelity in constrained speaking
scenarios, X-Actor enables actor-quality, long-form portrait performance
capturing nuanced, dynamically evolving emotions that flow coherently with the
rhythm and content of speech. Central to our approach is a two-stage decoupled
generation pipeline: an audio-conditioned autoregressive diffusion model that
predicts expressive yet identity-agnostic facial motion latent tokens within a
long temporal context window, followed by a diffusion-based video synthesis
module that translates these motions into high-fidelity video animations. By
operating in a compact facial motion latent space decoupled from visual and
identity cues, our autoregressive diffusion model effectively captures
long-range correlations between audio and facial dynamics through a
diffusion-forcing training paradigm, enabling infinite-length emotionally-rich
motion prediction without error accumulation. Extensive experiments demonstrate
that X-Actor produces compelling, cinematic-style performances that go beyond
standard talking head animations and achieves state-of-the-art results in
long-range, audio-driven emotional portrait acting.

</details>


### [69] [CloudBreaker: Breaking the Cloud Covers of Sentinel-2 Images using Multi-Stage Trained Conditional Flow Matching on Sentinel-1](https://arxiv.org/abs/2508.03608)
*Saleh Sakib Ahmed,Sara Nowreen,M. Sohel Rahman*

Main category: cs.CV

TL;DR: 提出CloudBreaker框架，利用Sentinel-1雷达数据生成高质量Sentinel-2多光谱图像（包括RGB、NDVI、NDWI），以克服云层和夜间条件对遥感数据的限制。


<details>
  <summary>Details</summary>
Motivation: 卫星遥感中，云层覆盖和夜间条件严重限制了多光谱图像的可用性，而Sentinel-1雷达图像不受这些条件影响，可提供持续数据。

Method: 提出CloudBreaker框架，通过Sentinel-1数据生成Sentinel-2多光谱信号（RGB、NDVI、NDWI）。采用基于条件潜在流匹配的新颖多阶段训练方法，并首次将余弦调度与流匹配结合。

Result: 生成的光学图像FID得分为0.7432，表明高保真度和真实感；NDWI的SSIM为0.6156，NDVI的SSIM为0.6874，表明高度结构相似性。

Conclusion: CloudBreaker为多光谱数据通常不可用或不可靠的遥感应用提供了一个有前景的解决方案。

Abstract: Cloud cover and nighttime conditions remain significant limitations in
satellite-based remote sensing, often restricting the availability and
usability of multi-spectral imagery. In contrast, Sentinel-1 radar images are
unaffected by cloud cover and can provide consistent data regardless of weather
or lighting conditions. To address the challenges of limited satellite imagery,
we propose CloudBreaker, a novel framework that generates high-quality
multi-spectral Sentinel-2 signals from Sentinel-1 data. This includes the
reconstruction of optical (RGB) images as well as critical vegetation and water
indices such as NDVI and NDWI.We employed a novel multi-stage training approach
based on conditional latent flow matching and, to the best of our knowledge,
are the first to integrate cosine scheduling with flow matching. CloudBreaker
demonstrates strong performance, achieving a Frechet Inception Distance (FID)
score of 0.7432, indicating high fidelity and realism in the generated optical
imagery. The model also achieved Structural Similarity Index Measure (SSIM) of
0.6156 for NDWI and 0.6874 for NDVI, indicating a high degree of structural
similarity. This establishes CloudBreaker as a promising solution for a wide
range of remote sensing applications where multi-spectral data is typically
unavailable or unreliable

</details>


### [70] [Towards Robust Image Denoising with Scale Equivariance](https://arxiv.org/abs/2508.02967)
*Dawei Zhang,Xiaojie Guo*

Main category: cs.CV

TL;DR: 本文提出一个基于尺度等变性的鲁棒盲去噪框架，通过异构归一化模块（HNM）和交互门控模块（IGM）处理空间变异噪声，显著提升了模型在分布外（OOD）条件下的泛化能力和去噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像去噪模型难以泛化到分布外（OOD）噪声模式，特别是面对空间变异噪声时，这一泛化差距是一个基本但未被充分探索的挑战。

Method: 引入“尺度等变性”作为核心归纳偏置以提升OOD鲁棒性。提出一个鲁棒盲去噪框架，包含两个关键组件：1) 异构归一化模块（HNM），用于稳定特征分布并动态校正不同噪声强度下的特征；2) 交互门控模块（IGM），通过信号和特征路径间的门控交互实现有效信息调制。

Result: 该模型在合成和真实世界基准测试中，特别是在空间异构噪声条件下，持续优于现有最先进的方法。

Conclusion: 通过整合尺度等变结构以及HNM和IGM模块，模型能够更好地从均匀噪声训练适应非均匀退化推理，显著提升了去噪模型在OOD和空间异构噪声条件下的鲁棒性和性能。

Abstract: Despite notable advances in image denoising, existing models often struggle
to generalize beyond in-distribution noise patterns, particularly when
confronted with out-of-distribution (OOD) conditions characterized by spatially
variant noise. This generalization gap remains a fundamental yet underexplored
challenge. In this work, we investigate \emph{scale equivariance} as a core
inductive bias for improving OOD robustness. We argue that incorporating
scale-equivariant structures enables models to better adapt from training on
spatially uniform noise to inference on spatially non-uniform degradations.
Building on this insight, we propose a robust blind denoising framework
equipped with two key components: a Heterogeneous Normalization Module (HNM)
and an Interactive Gating Module (IGM). HNM stabilizes feature distributions
and dynamically corrects features under varying noise intensities, while IGM
facilitates effective information modulation via gated interactions between
signal and feature paths. Extensive evaluations demonstrate that our model
consistently outperforms state-of-the-art methods on both synthetic and
real-world benchmarks, especially under spatially heterogeneous noise. Code
will be made publicly available.

</details>


### [71] [Diffusion Models with Adaptive Negative Sampling Without External Resources](https://arxiv.org/abs/2508.02973)
*Alakh Desai,Nuno Vasconcelos*

Main category: cs.CV

TL;DR: 本文提出了一种名为ANSWER的无训练方法，通过利用扩散模型对否定概念的内部理解，从单个提示中处理正负条件，从而在不使用外部负提示的情况下，显著提高了图像生成对提示的忠实度和质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量图像方面表现出色，但在提示依从性和图像质量上仍有显著差异。负向提示虽能提高依从性，但可能不完整且有损。本研究旨在开发一种更鲁棒的方法来提升提示依从性和图像质量，避免传统负向提示的局限性。

Method: 开发了ANSWER（Adaptive Negative Sampling Without External Resources）采样过程。该方法探索了负向提示与分类器自由引导（CFG）之间的关系，以从单个提示中同时处理正向和负向条件。它利用扩散模型对否定概念的内部理解来生成更忠实于提示的图像。ANSWER是一种无需训练的技术，适用于任何支持CFG的模型，并允许对图像概念进行负向接地，而无需显式的、有损且不完整的负向提示。

Result: 实验表明，将ANSWER添加到现有扩散模型中，其性能在多个基准测试上优于现有基线方法。此外，人类评估者对使用ANSWER生成图像的偏好度是其他方法的2倍。

Conclusion: ANSWER是一种有效的、无需训练的技术，通过利用扩散模型对否定概念的内部理解，从单个提示中处理正负条件，显著提高了图像生成对提示的忠实度和图像质量，并消除了对显式负向提示的需求。

Abstract: Diffusion models (DMs) have demonstrated an unparalleled ability to create
diverse and high-fidelity images from text prompts. However, they are also
well-known to vary substantially regarding both prompt adherence and quality.
Negative prompting was introduced to improve prompt compliance by specifying
what an image must not contain. Previous works have shown the existence of an
ideal negative prompt that can maximize the odds of the positive prompt. In
this work, we explore relations between negative prompting and classifier-free
guidance (CFG) to develop a sampling procedure, {\it Adaptive Negative Sampling
Without External Resources} (ANSWER), that accounts for both positive and
negative conditions from a single prompt. This leverages the internal
understanding of negation by the diffusion model to increase the odds of
generating images faithful to the prompt. ANSWER is a training-free technique,
applicable to any model that supports CFG, and allows for negative grounding of
image concepts without an explicit negative prompts, which are lossy and
incomplete. Experiments show that adding ANSWER to existing DMs outperforms the
baselines on multiple benchmarks and is preferred by humans 2x more over the
other methods.

</details>


### [72] [Separating Shared and Domain-Specific LoRAs for Multi-Domain Learning](https://arxiv.org/abs/2508.02978)
*Yusaku Takama,Ning Ding,Tatsuya Yokota,Toru Tamaki*

Main category: cs.CV

TL;DR: 本文提出一种多域学习方法，通过将共享LoRA和领域特定LoRA分别置于预训练权重的列空间和左零空间，确保它们存在于不同子空间，从而提升领域信息捕获能力，并在动作识别任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的多域学习架构中，共享LoRA和领域特定LoRA的组合未能明确有效地捕获领域特定信息，其结构是否有效仍不清楚。

Method: 提出一种新方法，确保共享LoRA和领域特定LoRA分别存在于预训练权重的列空间和左零空间这两个不同的子空间中。

Result: 将所提出的方法应用于动作识别任务，在UCF101、Kinetics400和HMDB51三个数据集上进行了实验，结果表明在某些情况下该方法有效，并对LoRA权重的维度进行了分析。

Conclusion: 通过将共享LoRA和领域特定LoRA分离到预训练权重的不同正交子空间中，可以更有效地捕获领域特定信息，从而提升多域学习的性能。

Abstract: Existing architectures of multi-domain learning have two types of adapters:
shared LoRA for all domains and domain-specific LoRA for each particular
domain. However, it remains unclear whether this structure effectively captures
domain-specific information. In this paper, we propose a method that ensures
that shared and domain-specific LoRAs exist in different subspaces;
specifically, the column and left null subspaces of the pre-trained weights. We
apply the proposed method to action recognition with three datasets (UCF101,
Kinetics400, and HMDB51) and demonstrate its effectiveness in some cases along
with the analysis of the dimensions of LoRA weights.

</details>


### [73] [MoExDA: Domain Adaptation for Edge-based Action Recognition](https://arxiv.org/abs/2508.02981)
*Takuya Sugimoto,Ning Ding,Toru Tamaki*

Main category: cs.CV

TL;DR: MoExDA是一种轻量级域适应方法，通过结合RGB和边缘帧来解决现代动作识别模型中的静态偏差问题，提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现代动作识别模型存在静态偏差问题，导致泛化性能下降。

Method: 提出了MoExDA方法，这是一种轻量级的域适应方案。它在RGB帧的基础上，额外使用边缘帧信息来对抗静态偏差。

Result: 该方法有效抑制了静态偏差，计算成本更低，并且实现了比以往方法更鲁棒的动作识别。

Conclusion: MoExDA通过利用边缘信息，以较低的计算成本有效解决了动作识别中的静态偏差问题，提高了模型的鲁棒性。

Abstract: Modern action recognition models suffer from static bias, leading to reduced
generalization performance. In this paper, we propose MoExDA, a lightweight
domain adaptation between RGB and edge information using edge frames in
addition to RGB frames to counter the static bias issue. Experiments
demonstrate that the proposed method effectively suppresses static bias with a
lower computational cost, allowing for more robust action recognition than
previous approaches.

</details>


### [74] [Adversarial Attention Perturbations for Large Object Detection Transformers](https://arxiv.org/abs/2508.02987)
*Zachary Yahn,Selim Furkan Tekin,Fatih Ilhan,Sihao Hu,Tiansheng Huang,Yichang Xu,Margaret Loper,Ling Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为AFOG的注意力聚焦对抗性梯度攻击方法，专门针对目标检测Transformer，但也能有效攻击基于CNN的检测器，通过可学习的注意力机制将扰动集中在脆弱区域，实现高效且隐蔽的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的目标检测对抗性扰动方法要么仅限于攻击基于CNN的检测器，要么对基于Transformer的检测器效果不佳，因此需要一种对Transformer模型同样有效的通用攻击方法。

Method: AFOG通过以下方式实现攻击：1) 利用可学习的注意力机制，将扰动集中在多框检测任务中图像的脆弱区域；2) 攻击损失通过两种特征损失的集成来制定，并通过可学习的注意力更新和对抗性扰动的迭代注入来优化；3) 该方法是神经架构无关的，对CNN和Transformer模型都有效。

Result: AFOG在COCO数据集上针对十二个大型检测Transformer进行了广泛实验，结果表明：1) 相比非注意力基线，其性能提升高达30.6%；2) AFOG在Transformer和CNN检测器上的攻击效果优于现有方法高达83%；3) AFOG攻击高效、隐蔽，能够以视觉上难以察觉的扰动使训练有素的模型失效，且具有更快的速度和更好的不可感知性。

Conclusion: AFOG是一种有效、高效且隐蔽的对抗性扰动方法，能够成功攻击包括大型Transformer在内的各种目标检测模型，并通过探测其弱点，导致模型失败，显著优于现有方法。

Abstract: Adversarial perturbations are useful tools for exposing vulnerabilities in
neural networks. Existing adversarial perturbation methods for object detection
are either limited to attacking CNN-based detectors or weak against
transformer-based detectors. This paper presents an Attention-Focused Offensive
Gradient (AFOG) attack against object detection transformers. By design, AFOG
is neural-architecture agnostic and effective for attacking both large
transformer-based object detectors and conventional CNN-based detectors with a
unified adversarial attention framework. This paper makes three original
contributions. First, AFOG utilizes a learnable attention mechanism that
focuses perturbations on vulnerable image regions in multi-box detection tasks,
increasing performance over non-attention baselines by up to 30.6%. Second,
AFOG's attack loss is formulated by integrating two types of feature loss
through learnable attention updates with iterative injection of adversarial
perturbations. Finally, AFOG is an efficient and stealthy adversarial
perturbation method. It probes the weak spots of detection transformers by
adding strategically generated and visually imperceptible perturbations which
can cause well-trained object detection models to fail. Extensive experiments
conducted with twelve large detection transformers on COCO demonstrate the
efficacy of AFOG. Our empirical results also show that AFOG outperforms
existing attacks on transformer-based and CNN-based object detectors by up to
83% with superior speed and imperceptibility. Code is available at
https://github.com/zacharyyahn/AFOG.

</details>


### [75] [Seeing It Before It Happens: In-Generation NSFW Detection for Diffusion-Based Text-to-Image Models](https://arxiv.org/abs/2508.03006)
*Fan Yang,Yihao Huang,Jiayi Zhu,Ling Shi,Geguang Pu,Jin Song Dong,Kailong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为In-Generation Detection (IGD)的新方法，通过利用扩散模型生成过程中预测的噪声作为内部信号，有效检测文本到图像（T2I）模型生成的NSFW（不安全工作内容）。


<details>
  <summary>Details</summary>
Motivation: 文本到图像（T2I）模型能生成高质量图像，但也存在滥用风险，特别是生成NSFW内容。现有检测方法主要集中于生成前过滤提示或生成后审核图像，而扩散模型的“生成中”阶段在NSFW检测方面仍未被充分探索。初步研究表明，预测噪声可能捕捉到区分NSFW和良性提示的语义线索，即使在对抗性提示下也如此。

Method: 引入了In-Generation Detection (IGD)方法，该方法利用扩散过程中预测的噪声作为内部信号来识别NSFW内容。

Result: 在七个NSFW类别上进行的实验表明，IGD在幼稚和对抗性NSFW提示上的平均检测准确率达到91.32%，优于七种基线方法。

Conclusion: IGD是一种简单而有效的在扩散模型生成过程中检测NSFW内容的方法，通过利用预测噪声作为内部信号，表现出卓越的检测性能。

Abstract: Diffusion-based text-to-image (T2I) models enable high-quality image
generation but also pose significant risks of misuse, particularly in producing
not-safe-for-work (NSFW) content. While prior detection methods have focused on
filtering prompts before generation or moderating images afterward, the
in-generation phase of diffusion models remains largely unexplored for NSFW
detection. In this paper, we introduce In-Generation Detection (IGD), a simple
yet effective approach that leverages the predicted noise during the diffusion
process as an internal signal to identify NSFW content. This approach is
motivated by preliminary findings suggesting that the predicted noise may
capture semantic cues that differentiate NSFW from benign prompts, even when
the prompts are adversarially crafted. Experiments conducted on seven NSFW
categories show that IGD achieves an average detection accuracy of 91.32% over
naive and adversarial NSFW prompts, outperforming seven baseline methods.

</details>


### [76] [Multi-Granularity Feature Calibration via VFM for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.03007)
*Xinhui Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: 本文提出多粒度特征校准（MGFC）框架，通过对视觉基础模型（VFMs）进行粗到细的特征对齐，以增强领域泛化语义分割（DGSS）在领域迁移下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的领域泛化语义分割方法在利用视觉基础模型时，主要关注全局特征微调，却忽略了对密集预测至关重要的跨特征层级适应，导致在未见领域泛化能力不足。

Method: 提出多粒度特征校准（MGFC）框架，通过分层和粒度感知的方式校准VFM特征。具体包括：首先校准粗粒度特征以捕获全局上下文语义和场景级结构；其次通过促进类别级特征判别性来优化中粒度特征；最后通过高频空间细节增强来校准细粒度特征。

Result: 在基准数据集上的广泛实验表明，MGFC方法优于现有最先进的DGSS方法。

Conclusion: 多粒度适应对于领域泛化语义分割任务是有效的，MGFC通过分层和粒度感知校准，成功将VFMs的泛化能力转移到DGSS任务中，显著提升了模型在领域迁移下的鲁棒性。

Abstract: Domain Generalized Semantic Segmentation (DGSS) aims to improve the
generalization ability of models across unseen domains without access to target
data during training. Recent advances in DGSS have increasingly exploited
vision foundation models (VFMs) via parameter-efficient fine-tuning strategies.
However, most existing approaches concentrate on global feature fine-tuning,
while overlooking hierarchical adaptation across feature levels, which is
crucial for precise dense prediction. In this paper, we propose
Multi-Granularity Feature Calibration (MGFC), a novel framework that performs
coarse-to-fine alignment of VFM features to enhance robustness under domain
shifts. Specifically, MGFC first calibrates coarse-grained features to capture
global contextual semantics and scene-level structure. Then, it refines
medium-grained features by promoting category-level feature discriminability.
Finally, fine-grained features are calibrated through high-frequency spatial
detail enhancement. By performing hierarchical and granularity-aware
calibration, MGFC effectively transfers the generalization strengths of VFMs to
the domain-specific task of DGSS. Extensive experiments on benchmark datasets
demonstrate that our method outperforms state-of-the-art DGSS approaches,
highlighting the effectiveness of multi-granularity adaptation for the semantic
segmentation task of domain generalization.

</details>


### [77] [Enhancing Long Video Question Answering with Scene-Localized Frame Grouping](https://arxiv.org/abs/2508.03009)
*Xuyi Yang,Wenhao Zhang,Hongbo Jin,Lin Liu,Hongbo Xu,Yongwei Nie,Fei Yu,Fei Ma*

Main category: cs.CV

TL;DR: 针对多模态大语言模型（MLLMs）在长视频理解中的不足，本文提出了一个新的场景问答任务SceneQA和数据集LVSQA，并引入了SLFG方法。SLFG通过将独立帧组合成语义连贯的场景帧，显著提升了现有MLLMs在长视频理解上的能力，且无需修改模型架构，即插即用。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在长视频理解方面表现不佳，主要原因是资源限制导致无法处理所有视频帧及其信息，难以高效提取相关信息。现有框架和评估任务侧重于识别包含核心对象的特定帧，与实际应用中的场景理解需求不符。

Method: 1. 提出了新的视频问答场景——SceneQA任务，强调基于场景的细节感知和推理能力。2. 开发了LVSQA数据集，用于支持SceneQA任务，该数据集基于LVBench精选视频构建，并包含新的问答对。3. 引入了SLFG方法，其核心思想是将单个帧组合成语义连贯的场景帧，通过场景定位和动态帧重组机制来增强理解能力。SLFG无需修改原始模型架构，具有即插即用的特性。

Result: SLFG方法显著增强了现有MLLMs在长视频中的理解能力，并在多个长视频基准测试中表现出色。

Conclusion: SceneQA任务、LVSQA数据集以及SLFG方法有效解决了MLLMs在长视频理解中的挑战，显著提升了其性能，并具有良好的实用性。

Abstract: Current Multimodal Large Language Models (MLLMs) often perform poorly in long
video understanding, primarily due to resource limitations that prevent them
from processing all video frames and their associated information. Efficiently
extracting relevant information becomes a challenging task. Existing frameworks
and evaluation tasks focus on identifying specific frames containing core
objects from a large number of irrelevant frames, which does not align with the
practical needs of real-world applications. To address this issue, we propose a
new scenario under the video question-answering task, SceneQA, which emphasizes
scene-based detail perception and reasoning abilities. And we develop the LVSQA
dataset to support the SceneQA task, which is built upon carefully selected
videos from LVBench and contains a new collection of question-answer pairs to
promote a more fair evaluation of MLLMs' scene perception abilities in long
videos. Inspired by human cognition, we introduce a novel method called SLFG.
The core idea of SLFG is to combine individual frames into semantically
coherent scene frames. By leveraging scene localization methods and dynamic
frame reassembly mechanisms, SLFG significantly enhances the understanding
capabilities of existing MLLMs in long videos. SLFG requires no modification to
the original model architecture and boasts excellent plug-and-play usability.
Experimental results show that this method performs exceptionally well in
several long video benchmark tests. Code and dataset will be released at
http://www.slfg.pkuzwh.cn.

</details>


### [78] [SA-3DGS: A Self-Adaptive Compression Method for 3D Gaussian Splatting](https://arxiv.org/abs/2508.03017)
*Liheng Zhang,Weihao Yu,Zubo Lu,Haozhi Gu,Jin Huang*

Main category: cs.CV

TL;DR: SA-3DGS是一种3D高斯溅射模型压缩方法，通过学习重要性分数进行有效剪枝、基于重要性的聚类压缩以及码本修复，显著降低存储需求（最高66倍压缩）同时保持或提升渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射模型需要大量高斯点，导致存储需求高，限制了实际部署。现有压缩方法难以有效识别不重要的高斯点，导致剪枝和压缩质量下降，影响渲染性能。

Method: 1. 学习一个重要性分数以自动识别场景重建中最不重要的高斯点，实现有效剪枝和冗余减少。 2. 采用重要性感知聚类模块更准确地将高斯属性压缩到码本中，提高码本表达能力并减小模型尺寸。 3. 设计码本修复模块，利用上下文场景信息修复码本，恢复原始高斯点属性，减轻信息损失导致的渲染质量下降。

Result: 在多个基准数据集上的实验结果表明，该方法在保持甚至提升渲染质量的同时，实现了高达66倍的压缩。所提出的高斯剪枝方法不仅适用于其他基于剪枝的方法（如LightGaussian），还能提升其性能，展现出卓越的性能和强大的泛化能力。

Conclusion: SA-3DGS通过创新的重要性感知剪枝和压缩策略，成功解决了3D高斯溅射模型存储成本高昂的问题，显著降低了模型大小，同时保持了高质量的渲染效果，并对现有方法具有良好的兼容性和提升作用。

Abstract: Recent advancements in 3D Gaussian Splatting have enhanced efficient and
high-quality novel view synthesis. However, representing scenes requires a
large number of Gaussian points, leading to high storage demands and limiting
practical deployment. The latest methods facilitate the compression of Gaussian
models but struggle to identify truly insignificant Gaussian points in the
scene, leading to a decline in subsequent Gaussian pruning, compression
quality, and rendering performance. To address this issue, we propose SA-3DGS,
a method that significantly reduces storage costs while maintaining rendering
quality. SA-3DGS learns an importance score to automatically identify the least
significant Gaussians in scene reconstruction, thereby enabling effective
pruning and redundancy reduction. Next, the importance-aware clustering module
compresses Gaussians attributes more accurately into the codebook, improving
the codebook's expressive capability while reducing model size. Finally, the
codebook repair module leverages contextual scene information to repair the
codebook, thereby recovering the original Gaussian point attributes and
mitigating the degradation in rendering quality caused by information loss.
Experimental results on several benchmark datasets show that our method
achieves up to 66x compression while maintaining or even improving rendering
quality. The proposed Gaussian pruning approach is not only adaptable to but
also improves other pruning-based methods (e.g., LightGaussian), showcasing
excellent performance and strong generalization ability.

</details>


### [79] [MoCA: Identity-Preserving Text-to-Video Generation via Mixture of Cross Attention](https://arxiv.org/abs/2508.03034)
*Qi Xie,Yongjia Ma,Donglin Di,Xuehao Gao,Xun Yang*

Main category: cs.CV

TL;DR: MoCA是一种基于Diffusion Transformer的新型视频扩散模型，通过引入混合交叉注意力机制和潜在视频感知损失，显著提升了文本到视频生成中人物身份的保留和时间一致性，并在新数据集CelebIPVid上表现优异。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型取得了进展，但ID-preserving的文本到视频（T2V）生成仍具挑战性，现有方法难以捕捉精细的面部动态或保持时间上的身份一致性。

Method: 本文提出了MoCA模型，该模型基于Diffusion Transformer (DiT) 主干，并引入了受专家混合（MoE）启发的多专家交叉注意力（MoCA）机制。MoCA层嵌入到每个DiT块中，其中分层时间池化（Hierarchical Temporal Pooling）捕获不同时间尺度上的身份特征，时间感知交叉注意力专家（Temporal-Aware Cross-Attention Experts）动态建模时空关系。此外，模型还引入了潜在视频感知损失（Latent Video Perceptual Loss）以增强视频帧间的身份连贯性和细节。为训练模型，作者收集了包含10,000个高分辨率视频的CelebIPVid数据集。

Result: 在CelebIPVid数据集上的大量实验表明，MoCA在人脸相似度方面优于现有T2V方法超过5%。

Conclusion: MoCA模型通过其创新的架构和训练策略，有效解决了文本到视频生成中身份保留和时间一致性的挑战，实现了更优的生成质量和身份保持能力。

Abstract: Achieving ID-preserving text-to-video (T2V) generation remains challenging
despite recent advances in diffusion-based models. Existing approaches often
fail to capture fine-grained facial dynamics or maintain temporal identity
coherence. To address these limitations, we propose MoCA, a novel Video
Diffusion Model built on a Diffusion Transformer (DiT) backbone, incorporating
a Mixture of Cross-Attention mechanism inspired by the Mixture-of-Experts
paradigm. Our framework improves inter-frame identity consistency by embedding
MoCA layers into each DiT block, where Hierarchical Temporal Pooling captures
identity features over varying timescales, and Temporal-Aware Cross-Attention
Experts dynamically model spatiotemporal relationships. We further incorporate
a Latent Video Perceptual Loss to enhance identity coherence and fine-grained
details across video frames. To train this model, we collect CelebIPVid, a
dataset of 10,000 high-resolution videos from 1,000 diverse individuals,
promoting cross-ethnicity generalization. Extensive experiments on CelebIPVid
show that MoCA outperforms existing T2V methods by over 5% across Face
similarity.

</details>


### [80] [VideoForest: Person-Anchored Hierarchical Reasoning for Cross-Video Question Answering](https://arxiv.org/abs/2508.03039)
*Yiran Meng,Junhong Ye,Wei Zhou,Guanghui Yue,Xudong Mao,Ruomei Wang,Baoquan Zhao*

Main category: cs.CV

TL;DR: 本文提出了VideoForest框架，通过以人物为中心的层次化推理解决跨视频问答的挑战，并构建了新的CrossVideoQA数据集，实验证明其在跨视频推理任务中表现优越。


<details>
  <summary>Details</summary>
Motivation: 传统单视频理解无法满足跨视频问答的需求，主要挑战在于如何建立跨视频流的有效连接，以及管理多源信息的复杂性。

Method: 引入VideoForest框架，采用以人物为中心的层次化推理，无需端到端训练。其核心创新包括：1) 人物锚定的特征提取机制，利用ReID和跟踪算法在多视频源间建立时空关系；2) 多粒度生成树结构，围绕人物轨迹层次化组织视觉内容；3) 多智能体推理框架，高效遍历该层次结构以回答复杂查询。此外，还开发了CrossVideoQA数据集用于评估。

Result: VideoForest在跨视频推理任务中表现出色，人物识别准确率达71.93%，行为分析达83.75%，总结和推理达51.67%，显著优于现有方法。

Conclusion: 该工作通过以人物级特征统一多个视频流，为跨视频理解建立了新范式，实现了分布式视觉信息间的复杂推理，同时保持了计算效率。

Abstract: Cross-video question answering presents significant challenges beyond
traditional single-video understanding, particularly in establishing meaningful
connections across video streams and managing the complexity of multi-source
information retrieval. We introduce VideoForest, a novel framework that
addresses these challenges through person-anchored hierarchical reasoning. Our
approach leverages person-level features as natural bridge points between
videos, enabling effective cross-video understanding without requiring
end-to-end training. VideoForest integrates three key innovations: 1) a
human-anchored feature extraction mechanism that employs ReID and tracking
algorithms to establish robust spatiotemporal relationships across multiple
video sources; 2) a multi-granularity spanning tree structure that
hierarchically organizes visual content around person-level trajectories; and
3) a multi-agent reasoning framework that efficiently traverses this
hierarchical structure to answer complex cross-video queries. To evaluate our
approach, we develop CrossVideoQA, a comprehensive benchmark dataset
specifically designed for person-centric cross-video analysis. Experimental
results demonstrate VideoForest's superior performance in cross-video reasoning
tasks, achieving 71.93% accuracy in person recognition, 83.75% in behavior
analysis, and 51.67% in summarization and reasoning, significantly
outperforming existing methods. Our work establishes a new paradigm for
cross-video understanding by unifying multiple video streams through
person-level features, enabling sophisticated reasoning across distributed
visual information while maintaining computational efficiency.

</details>


### [81] [Multi-human Interactive Talking Dataset](https://arxiv.org/abs/2508.03050)
*Zeyu Zhu,Weijia Wu,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 该研究引入了一个名为MIT的大规模数据集和基线模型CovOG，旨在解决现有谈话视频生成研究中多人类交互的局限性，专注于多人类谈话视频的生成。


<details>
  <summary>Details</summary>
Motivation: 现有的谈话视频生成研究主要集中于单人独白或孤立的面部动画，限制了其在真实多人类交互场景中的应用。研究旨在弥补这一空白。

Method: 该研究开发了一个自动化流程来收集和标注多人物对话视频，构建了MIT数据集，包含身体姿态和语音交互的细粒度标注。在此基础上，提出了基线模型CovOG，它集成了多人类姿态编码器（MPE）以处理不同数量的说话者，并通过交互式音频驱动器（IAD）根据特定说话者的音频特征调节头部动态。

Result: MIT数据集包含12小时的高分辨率素材，每段视频有2到4名说话者，并带有细致的身体姿态和语音交互标注，捕捉了多说话者场景中的自然对话动态。CovOG模型展示了生成真实多人类谈话视频的可行性和挑战。

Conclusion: MIT数据集为未来的多人类谈话视频生成研究提供了一个宝贵的基准，而CovOG模型则展示了该任务的潜力和挑战。

Abstract: Existing studies on talking video generation have predominantly focused on
single-person monologues or isolated facial animations, limiting their
applicability to realistic multi-human interactions. To bridge this gap, we
introduce MIT, a large-scale dataset specifically designed for multi-human
talking video generation. To this end, we develop an automatic pipeline that
collects and annotates multi-person conversational videos. The resulting
dataset comprises 12 hours of high-resolution footage, each featuring two to
four speakers, with fine-grained annotations of body poses and speech
interactions. It captures natural conversational dynamics in multi-speaker
scenario, offering a rich resource for studying interactive visual behaviors.
To demonstrate the potential of MIT, we furthur propose CovOG, a baseline model
for this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle
varying numbers of speakers by aggregating individual pose embeddings, and an
Interactive Audio Driver (IAD) to modulate head dynamics based on
speaker-specific audio features. Together, these components showcase the
feasibility and challenges of generating realistic multi-human talking videos,
establishing MIT as a valuable benchmark for future research. The code is
avalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.

</details>


### [82] [Uncertainty-Guided Face Matting for Occlusion-Aware Face Transformation](https://arxiv.org/abs/2508.03055)
*Hyebin Cho,Jaehyup Lee*

Main category: cs.CV

TL;DR: FaceMat是一个无需Trimap、感知不确定性的面部抠图框架，能有效分离遮挡物和面部区域，从而提升面部滤镜在复杂遮挡下的性能。


<details>
  <summary>Details</summary>
Motivation: 面部滤镜在存在遮挡（如手、头发、配饰）时性能会下降。现有方法难以处理这种情况，因此需要一种能精确分离遮挡物和面部区域的方法。

Method: 引入了“面部抠图”的新任务，旨在估计精细的alpha蒙版来分离遮挡物和面部区域。提出了FaceMat框架，该框架无需Trimap且感知不确定性。采用两阶段训练流程：教师模型联合估计alpha蒙版和像素级不确定性（使用NLL损失），然后利用此不确定性通过空间自适应知识蒸馏指导学生模型，使其专注于模糊或被遮挡区域。该方法无需辅助输入（如Trimap或分割掩码），并将皮肤视为前景，遮挡物视为背景。为此任务构建了大型合成数据集CelebAMat。

Result: FaceMat在多个基准测试中优于最先进的方法，显著提高了面部滤镜在真实、无约束视频场景中的视觉质量和鲁棒性。

Conclusion: FaceMat通过引入新颖、鲁棒且支持实时应用的面部抠图框架，并配合新数据集，有效解决了面部滤镜在遮挡情况下的性能下降问题，从而显著提升了面部滤镜的整体表现。

Abstract: Face filters have become a key element of short-form video content, enabling
a wide array of visual effects such as stylization and face swapping. However,
their performance often degrades in the presence of occlusions, where objects
like hands, hair, or accessories obscure the face. To address this limitation,
we introduce the novel task of face matting, which estimates fine-grained alpha
mattes to separate occluding elements from facial regions. We further present
FaceMat, a trimap-free, uncertainty-aware framework that predicts high-quality
alpha mattes under complex occlusions. Our approach leverages a two-stage
training pipeline: a teacher model is trained to jointly estimate alpha mattes
and per-pixel uncertainty using a negative log-likelihood (NLL) loss, and this
uncertainty is then used to guide the student model through spatially adaptive
knowledge distillation. This formulation enables the student to focus on
ambiguous or occluded regions, improving generalization and preserving semantic
consistency. Unlike previous approaches that rely on trimaps or segmentation
masks, our framework requires no auxiliary inputs making it well-suited for
real-time applications. In addition, we reformulate the matting objective by
explicitly treating skin as foreground and occlusions as background, enabling
clearer compositing strategies. To support this task, we newly constructed
CelebAMat, a large-scale synthetic dataset specifically designed for
occlusion-aware face matting. Extensive experiments show that FaceMat
outperforms state-of-the-art methods across multiple benchmarks, enhancing the
visual quality and robustness of face filters in real-world, unconstrained
video scenarios. The source code and CelebAMat dataset are available at
https://github.com/hyebin-c/FaceMat.git

</details>


### [83] [CHARM: Collaborative Harmonization across Arbitrary Modalities for Modality-agnostic Semantic Segmentation](https://arxiv.org/abs/2508.03060)
*Lekang Wen,Jing Xiao,Liang Liao,Jiajun Chen,Mi Wang*

Main category: cs.CV

TL;DR: 模态无关语义分割(MaSS)旨在实现跨模态鲁棒场景理解。现有方法通过显式特征对齐导致模态同质化并破坏互补性。本文提出CHARM框架，通过隐式对齐和双路径优化策略实现模态协同协调，在多个数据集上显著优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有模态无关语义分割(MaSS)方法通常依赖显式特征对齐来实现模态同质化，但这会稀释各模态的独特优势并破坏它们固有的互补性，无法实现真正的协同。

Method: 本文提出CHARM，一个新颖的互补学习框架，旨在隐式对齐内容同时保留模态特有优势。它包含两个核心组件：1) 互感知单元(MPU)，通过基于窗口的跨模态交互实现隐式对齐；2) 双路径优化策略，将训练解耦为用于互补融合学习的协同学习策略(CoL)和用于受保护模态特定优化的个体增强策略(InE)。

Result: 在多个数据集和骨干网络上的实验表明，CHARM始终优于基线方法，尤其在“脆弱”模态上取得了显著提升。

Conclusion: 本工作将研究焦点从模型同质化转向模态协调化，通过实现跨模态互补性，达到了多样性中的真正和谐。

Abstract: Modality-agnostic Semantic Segmentation (MaSS) aims to achieve robust scene
understanding across arbitrary combinations of input modality. Existing methods
typically rely on explicit feature alignment to achieve modal homogenization,
which dilutes the distinctive strengths of each modality and destroys their
inherent complementarity. To achieve cooperative harmonization rather than
homogenization, we propose CHARM, a novel complementary learning framework
designed to implicitly align content while preserving modality-specific
advantages through two components: (1) Mutual Perception Unit (MPU), enabling
implicit alignment through window-based cross-modal interaction, where
modalities serve as both queries and contexts for each other to discover
modality-interactive correspondences; (2) A dual-path optimization strategy
that decouples training into Collaborative Learning Strategy (CoL) for
complementary fusion learning and Individual Enhancement Strategy (InE) for
protected modality-specific optimization. Experiments across multiple datasets
and backbones indicate that CHARM consistently outperform the baselines, with
significant increment on the fragile modalities. This work shifts the focus
from model homogenization to harmonization, enabling cross-modal
complementarity for true harmony in diversity.

</details>


### [84] [CORE-ReID: Comprehensive Optimization and Refinement through Ensemble fusion in Domain Adaptation for person re-identification](https://arxiv.org/abs/2508.03064)
*Trinh Quoc Nguyen,Oky Dicky Ardiansyah Prima,Katsuyoshi Hotta*

Main category: cs.CV

TL;DR: 该研究提出了CORE-ReID框架，通过CycleGAN进行数据多样化，结合教师-学生网络、多级聚类和可学习的集成融合组件，解决了行人重识别中的无监督域适应问题，并取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别方法在无监督域适应（UDA）方面存在挑战，尤其是在处理不同摄像源的图像特征差异以及如何有效利用伪标签方面。

Method: CORE-ReID框架包括两个阶段：1) 预训练阶段，使用CycleGAN生成多样化数据以协调不同摄像源的图像特征；2) 微调阶段，基于教师-学生网络，整合多视图特征进行多级聚类以生成多样伪标签，并引入可学习的集成融合组件（Ensemble Fusion）来增强学习全面性，避免多伪标签的歧义。此外，还引入了高效通道注意力块（ECA）和双向均值特征归一化（BMFN）来减轻偏差效应并自适应融合全局和局部特征。

Result: 实验结果表明，该框架在三个常见的行人重识别UDA任务上显著优于现有最先进的方法，并在平均精度均值（mAP）、Top-1、Top-5和Top-10等指标上实现了高精度。

Conclusion: CORE-ReID框架为行人重识别中的无监督域适应提供了一个先进且有效的解决方案，它确保了融合特征的清晰性，避免了歧义，并达到了高准确率。

Abstract: This study introduces a novel framework, "Comprehensive Optimization and
Refinement through Ensemble Fusion in Domain Adaptation for Person
Re-identification (CORE-ReID)", to address an Unsupervised Domain Adaptation
(UDA) for Person Re-identification (ReID). The framework utilizes CycleGAN to
generate diverse data that harmonizes differences in image characteristics from
different camera sources in the pre-training stage. In the fine-tuning stage,
based on a pair of teacher-student networks, the framework integrates
multi-view features for multi-level clustering to derive diverse pseudo labels.
A learnable Ensemble Fusion component that focuses on fine-grained local
information within global features is introduced to enhance learning
comprehensiveness and avoid ambiguity associated with multiple pseudo-labels.
Experimental results on three common UDAs in Person ReID demonstrate
significant performance gains over state-of-the-art approaches. Additional
enhancements, such as Efficient Channel Attention Block and Bidirectional Mean
Feature Normalization mitigate deviation effects and adaptive fusion of global
and local features using the ResNet-based model, further strengthening the
framework. The proposed framework ensures clarity in fusion features, avoids
ambiguity, and achieves high ac-curacy in terms of Mean Average Precision,
Top-1, Top-5, and Top-10, positioning it as an advanced and effective solution
for the UDA in Person ReID. Our codes and models are available at
https://github.com/TrinhQuocNguyen/CORE-ReID.

</details>


### [85] [COFFEE: A Shadow-Resilient Real-Time Pose Estimator for Unknown Tumbling Asteroids using Sparse Neural Networks](https://arxiv.org/abs/2508.03132)
*Arion Zimmermann,Soon-Jo Chung,Fred Hadaegh*

Main category: cs.CV

TL;DR: COFFEE是一种实时小行星姿态估计框架，通过利用太阳相位角信息，提取对阴影运动不变的特征，并结合稀疏神经网络和图神经网络进行特征匹配，实现了无偏差、高精度且计算效率远超现有方法的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 空间未知天体（如空间碎片、小行星）的精确状态估计是关键挑战。现有方法存在问题：传统方法（SIFT、ORB、AKAZE）实时但精度不足；现代深度学习方法精度高但计算资源需求大，不适用于航天硬件；最重要的是，两者都无法有效处理物体上的自投阴影，阴影会导致姿态估计产生大的偏差，进而可能导致航天器状态估计器误判，甚至任务失败。

Method: 提出COFFEE（Celestial Occlusion Fast FEature Extractor）框架。该方法利用航天器上常见的太阳追踪传感器提供的太阳相位角先验信息，将显著轮廓与其投影阴影关联起来，检测出一组对阴影运动不变的稀疏特征。随后，一个稀疏神经网络与一个基于注意力的图神经网络特征匹配模型被联合训练，以提供连续帧之间的对应关系。

Result: 实验结果表明，COFFEE姿态估计算法是无偏差的，比经典姿态估计管道更准确，并且在合成数据以及翻滚小行星Apophis的渲染图上，比其他最先进的深度学习管道快一个数量级。

Conclusion: COFFEE框架成功解决了空间天体姿态估计中阴影导致的偏差问题，并在计算效率和精度上取得了显著提升，为小行星的实时、鲁棒、高精度姿态估计提供了有效的解决方案。

Abstract: The accurate state estimation of unknown bodies in space is a critical
challenge with applications ranging from the tracking of space debris to the
shape estimation of small bodies. A necessary enabler to this capability is to
find and track features on a continuous stream of images. Existing methods,
such as SIFT, ORB and AKAZE, achieve real-time but inaccurate pose estimates,
whereas modern deep learning methods yield higher quality features at the cost
of more demanding computational resources which might not be available on
space-qualified hardware. Additionally, both classical and data-driven methods
are not robust to the highly opaque self-cast shadows on the object of
interest. We show that, as the target body rotates, these shadows may lead to
large biases in the resulting pose estimates. For these objects, a bias in the
real-time pose estimation algorithm may mislead the spacecraft's state
estimator and cause a mission failure, especially if the body undergoes a
chaotic tumbling motion. We present COFFEE, the Celestial Occlusion Fast
FEature Extractor, a real-time pose estimation framework for asteroids designed
to leverage prior information on the sun phase angle given by sun-tracking
sensors commonly available onboard spacecraft. By associating salient contours
to their projected shadows, a sparse set of features are detected, invariant to
the motion of the shadows. A Sparse Neural Network followed by an
attention-based Graph Neural Network feature matching model are then jointly
trained to provide a set of correspondences between successive frames. The
resulting pose estimation pipeline is found to be bias-free, more accurate than
classical pose estimation pipelines and an order of magnitude faster than other
state-of-the-art deep learning pipelines on synthetic data as well as on
renderings of the tumbling asteroid Apophis.

</details>


### [86] [SSFMamba: Symmetry-driven Spatial-Frequency Feature Fusion for 3D Medical Image Segmentation](https://arxiv.org/abs/2508.03069)
*Bo Zhang,Yifan Zhang,Shuo Yan,Yu Bai,Zheng Zhang,Wu Liu,Xiuzhuang Zhou,Wendong Wang*

Main category: cs.CV

TL;DR: SSFMamba是一种基于Mamba的空间-频率特征融合网络，用于3D医学图像分割，通过双分支架构和新的扫描机制，有效融合空间和频率域信息，提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D医学图像分割方法在空间域中难以建模全局上下文，而频率域方法常忽略其特有属性（如共轭对称性）及与空间域的数据分布差异，导致频率域的互补优势未能充分发挥或被稀释。

Method: 提出SSFMamba网络，采用双分支架构分别从空间域和频率域提取特征。利用Mamba模块融合异构特征，以保留全局上下文并增强局部细节。在频率域分支中，结合Mamba的全局上下文提取能力和频率域特征的协同效应，进一步增强全局建模。设计了3D多方向扫描机制以强化局部和全局信息的融合。

Result: 在BraTS2020和BraTS2023数据集上的大量实验表明，SSFMamba在各种评估指标上均持续优于最先进的方法。

Conclusion: SSFMamba通过结合空间和频率域特征，并利用Mamba的强大能力和创新的扫描机制，有效解决了3D医学图像分割中全局上下文建模的挑战，实现了卓越的分割性能。

Abstract: In light of the spatial domain's limited capacity for modeling global context
in 3D medical image segmentation, emerging approaches have begun to incorporate
frequency domain representations. However, straightforward feature extraction
strategies often overlook the unique properties of frequency domain
information, such as conjugate symmetry. They also fail to account for the
fundamental differences in data distribution between the spatial and frequency
domains, which can ultimately dilute or obscure the complementary strengths
that frequency-based representations offer. In this paper, we propose SSFMamba,
a Mamba based Symmetry-driven Spatial-Frequency feature fusion network for 3D
medical image segmentation. SSFMamba employs a complementary dual-branch
architecture that extracts features from both the spatial and frequency
domains, and leverages a Mamba block to fuse these heterogeneous features to
preserve global context while reinforcing local details. In the frequency
domain branch, we harness Mamba's exceptional capability to extract global
contextual information in conjunction with the synergistic effect of frequency
domain features to further enhance global modeling. Moreover, we design a 3D
multi-directional scanning mechanism to strengthen the fusion of local and
global cues. Extensive experiments on the BraTS2020 and BraTS2023 datasets
demonstrate that our approach consistently outperforms state-of-the-art methods
across various evaluation metrics.

</details>


### [87] [LRDDv2: Enhanced Long-Range Drone Detection Dataset with Range Information and Comprehensive Real-World Challenges](https://arxiv.org/abs/2508.03331)
*Amirreza Rouhi,Sneh Patel,Noah McCarthy,Siddiqa Khan,Hadi Khorsand,Kaleb Lefkowitz,David K. Han*

Main category: cs.CV

TL;DR: 本文介绍了LRDDv2数据集，一个包含39,516张精心标注图像的无人机检测数据集，旨在解决长距离无人机检测和测距的挑战。


<details>
  <summary>Details</summary>
Motivation: 无人机使用量呈指数级增长，对在远距离检测无人机以确保安全操作的需求日益迫切，尤其是在人口密集区域。尽管深度学习在计算机视觉方面取得了巨大进步，但检测这些小型空中目标仍是一个巨大挑战。现有无人机检测数据集缺乏多样性和广度，特别是在不同环境条件下的长距离检测方面。

Method: 研究者通过细致标注39,516张图像，构建了LRDDv2数据集，作为先前发布的LRDDv1的增强版本。LRDDv2增加了图像的多样性，并首次包含了8,000多张图像的目标距离信息，以便开发无人机距离估计算法。该数据集主要包含在1080p分辨率下仅占50像素或更小的无人机图像，专为长距离空中目标检测设计。

Result: LRDDv2数据集包含39,516张精心标注的图像，提供了比LRDDv1更丰富和多样化的无人机检测资源。其中，超过8,000张图像包含目标距离信息，使得开发无人机距离估计算法成为可能。数据集中的大多数图像捕捉的是在1080p分辨率下像素尺寸小于或等于50的无人机，非常适合长距离空中目标检测研究。

Conclusion: LRDDv2数据集为长距离无人机检测研究提供了一个更广泛、更全面的资源，并通过包含目标距离信息，为无人机距离估计算法的开发开辟了新途径。这有助于推动计算机视觉在小型空中目标检测领域的进步，从而提高无人机操作的安全性。

Abstract: The exponential growth in Unmanned Aerial Vehicles (UAVs) usage underscores
the critical need of detecting them at extended distances to ensure safe
operations, especially in densely populated areas. Despite the tremendous
advances made in computer vision through deep learning, the detection of these
small airborne objects remains a formidable challenge. While several datasets
have been developed specifically for drone detection, the need for a more
extensive and diverse collection of drone image data persists, particularly for
long-range detection under varying environmental conditions. We introduce here
the Long Range Drone Detection (LRDD) Version 2 dataset, comprising 39,516
meticulously annotated images, as a second release of the LRDD dataset released
previously. The LRDDv2 dataset enhances the LRDDv1 by incorporating a greater
variety of images, providing a more diverse and comprehensive resource for
drone detection research. What sets LRDDv2 apart is its inclusion of target
range information for over 8,000 images, making it possible to develop
algorithms for drone range estimation. Tailored for long-range aerial object
detection, the majority of LRDDv2's dataset consists of images capturing drones
with 50 or fewer pixels in 1080p resolution. For access to the complete
Long-Range Drone Detection Dataset (LRDD)v2, please visit
https://research.coe.drexel.edu/ece/imaple/lrddv2/ .

</details>


### [88] [RobustGS: Unified Boosting of Feedforward 3D Gaussian Splatting under Low-Quality Conditions](https://arxiv.org/abs/2508.03077)
*Anran Wu,Long Peng,Xin Di,Xueyuan Dai,Chen Wu,Yang Wang,Xueyang Fu,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: RobustGS是一个多视图特征增强模块，显著提高了前向3D高斯泼溅（3DGS）方法在各种恶劣成像条件下的鲁棒性和重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有的前向3DGS方法假设输入多视图图像是干净且高质量的，但在现实世界中，图像常在噪声、低光或雨等挑战性条件下捕获，导致几何不准确和3D重建质量下降。

Method: 本文提出了RobustGS，一个通用且高效的多视图特征增强模块，可即插即用地集成到现有预训练管线中。具体而言，它引入了一个“广义退化学习器”来提取多种退化的通用表示，并提出了一个“语义感知状态空间模型”，该模型首先利用退化表示在特征空间增强受损输入，然后采用语义感知策略聚合跨视图的语义相似信息，以提取细粒度的跨视图对应关系。

Result: 当集成到现有方法中时，RobustGS在各种类型的退化条件下始终实现最先进的重建质量。

Conclusion: RobustGS显著提高了前向3DGS方法在恶劣成像条件下的鲁棒性，实现了高质量的3D重建。

Abstract: Feedforward 3D Gaussian Splatting (3DGS) overcomes the limitations of
optimization-based 3DGS by enabling fast and high-quality reconstruction
without the need for per-scene optimization. However, existing feedforward
approaches typically assume that input multi-view images are clean and
high-quality. In real-world scenarios, images are often captured under
challenging conditions such as noise, low light, or rain, resulting in
inaccurate geometry and degraded 3D reconstruction. To address these
challenges, we propose a general and efficient multi-view feature enhancement
module, RobustGS, which substantially improves the robustness of feedforward
3DGS methods under various adverse imaging conditions, enabling high-quality 3D
reconstruction. The RobustGS module can be seamlessly integrated into existing
pretrained pipelines in a plug-and-play manner to enhance reconstruction
robustness. Specifically, we introduce a novel component, Generalized
Degradation Learner, designed to extract generic representations and
distributions of multiple degradations from multi-view inputs, thereby
enhancing degradation-awareness and improving the overall quality of 3D
reconstruction. In addition, we propose a novel semantic-aware state-space
model. It first leverages the extracted degradation representations to enhance
corrupted inputs in the feature space. Then, it employs a semantic-aware
strategy to aggregate semantically similar information across different views,
enabling the extraction of fine-grained cross-view correspondences and further
improving the quality of 3D representations. Extensive experiments demonstrate
that our approach, when integrated into existing methods in a plug-and-play
manner, consistently achieves state-of-the-art reconstruction quality across
various types of degradations.

</details>


### [89] [OmniShape: Zero-Shot Multi-Hypothesis Shape and Pose Estimation in the Real World](https://arxiv.org/abs/2508.03669)
*Katherine Liu,Sergey Zakharov,Dian Chen,Takuya Ikeda,Greg Shakhnarovich,Adrien Gaidon,Rares Ambrus*

Main category: cs.CV

TL;DR: 提出OmniShape，一种从单次观测中概率性估计物体姿态和完整形状的新方法，无需已知3D模型或类别。


<details>
  <summary>Details</summary>
Motivation: 在没有已知3D模型或类别假设的情况下，从单次观测中估计物体的姿态和完整形状。

Method: OmniShape将形状补全解耦为两个多模态分布：测量如何投影到归一化对象参考系，以及对象几何形状的先验（表示为三平面神经场）。通过为这两个分布训练独立的条件扩散模型，实现从联合姿态和形状分布中采样多个假设。

Result: 在具有挑战性的真实世界数据集中展示了引人注目的性能。

Conclusion: OmniShape是首个实现从单次观测中概率性估计物体姿态和完整形状的方法，通过解耦的扩散模型能够生成多个假设，有效解决了无先验模型下的形状和姿态估计问题。

Abstract: We would like to estimate the pose and full shape of an object from a single
observation, without assuming known 3D model or category. In this work, we
propose OmniShape, the first method of its kind to enable probabilistic pose
and shape estimation. OmniShape is based on the key insight that shape
completion can be decoupled into two multi-modal distributions: one capturing
how measurements project into a normalized object reference frame defined by
the dataset and the other modelling a prior over object geometries represented
as triplanar neural fields. By training separate conditional diffusion models
for these two distributions, we enable sampling multiple hypotheses from the
joint pose and shape distribution. OmniShape demonstrates compelling
performance on challenging real world datasets. Project website:
https://tri-ml.github.io/omnishape

</details>


### [90] [Exploring Fairness across Fine-Grained Attributes in Large Vision-Language Models](https://arxiv.org/abs/2508.03079)
*Zaiying Zhao,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 研究发现大型视觉-语言模型（LVLMs）在更广泛的细粒度属性上存在偏见，且文化、环境和行为因素对其决策影响比传统人口属性更显著。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉-语言模型（LVLMs）应用迅速扩展，其公平性问题日益突出。现有研究主要关注种族和性别等人口统计属性，但对更广泛属性的公平性探索不足。

Method: 本研究利用大型语言模型（LLMs）构建了一个开放式偏见属性知识库，并评估了LVLMs在更细粒度属性上的公平性。

Result: 实验结果表明，LVLMs在多样化属性上表现出偏见输出。此外，文化、环境和行为因素对LVLM决策的影响比传统人口统计属性更为显著。

Conclusion: LVLMs的公平性问题不仅限于传统的人口统计属性，更受文化、环境和行为等细粒度因素的显著影响，这提示未来研究需关注更全面的偏见维度。

Abstract: The rapid expansion of applications using Large Vision-Language Models
(LVLMs), such as GPT-4o, has raised significant concerns about their fairness.
While existing studies primarily focus on demographic attributes such as race
and gender, fairness across a broader range of attributes remains largely
unexplored. In this study, we construct an open-set knowledge base of bias
attributes leveraging Large Language Models (LLMs) and evaluate the fairness of
LVLMs across finer-grained attributes. Our experimental results reveal that
LVLMs exhibit biased outputs across a diverse set of attributes and further
demonstrate that cultural, environmental, and behavioral factors have a more
pronounced impact on LVLM decision-making than traditional demographic
attributes.

</details>


### [91] [Veila: Panoramic LiDAR Generation from a Monocular RGB Image](https://arxiv.org/abs/2508.03690)
*Youquan Liu,Lingdong Kong,Weidong Yang,Ao Liang,Jianxiong Gao,Yang Wu,Xiang Xu,Xin Li,Linfeng Li,Runnan Chen,Ben Fei*

Main category: cs.CV

TL;DR: Veila是一个新颖的条件扩散框架，它利用单目RGB图像作为空间控制信号，生成逼真且可控的全景LiDAR数据，解决了现有方法在可控性和跨模态对齐方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶和机器人技术中，可扩展的3D感知需要逼真且可控的全景LiDAR数据生成。现有方法要么缺乏可控性，要么通过文本引导合成，缺乏精细的空间控制。利用单目RGB图像作为空间控制信号是一种可扩展且低成本的替代方案，但面临三个核心挑战：RGB语义和深度线索的空间变化性、RGB外观和LiDAR几何之间的模态差距，以及单目RGB和全景LiDAR之间结构一致性的维护。

Method: 本文提出了Veila框架，包含：1. 置信度感知条件机制（CACM），根据局部可靠性自适应平衡语义和深度线索，增强RGB条件作用；2. 几何跨模态对齐（GCMA），在噪声扩散下实现鲁棒的RGB-LiDAR对齐；3. 全景特征一致性（PFC），强制单目RGB和全景LiDAR之间的全局结构一致性。此外，引入了跨模态语义一致性和跨模态深度一致性两个新度量来评估对齐质量。

Result: 在nuScenes、SemanticKITTI和提出的KITTI-Weather基准测试中，Veila实现了最先进的生成保真度和跨模态一致性。实验还表明，通过生成数据增强，可以提高下游LiDAR语义分割的性能。

Conclusion: Veila成功地解决了单目RGB引导全景LiDAR数据生成中的关键挑战，实现了高保真度、强一致性的数据生成，并能有效提升下游3D感知任务的性能，为自动驾驶和机器人领域提供了有价值的数据增强工具。

Abstract: Realistic and controllable panoramic LiDAR data generation is critical for
scalable 3D perception in autonomous driving and robotics. Existing methods
either perform unconditional generation with poor controllability or adopt
text-guided synthesis, which lacks fine-grained spatial control. Leveraging a
monocular RGB image as a spatial control signal offers a scalable and low-cost
alternative, which remains an open problem. However, it faces three core
challenges: (i) semantic and depth cues from RGB are vary spatially,
complicating reliable conditioning generation; (ii) modality gaps between RGB
appearance and LiDAR geometry amplify alignment errors under noisy diffusion;
and (iii) maintaining structural coherence between monocular RGB and panoramic
LiDAR is challenging, particularly in non-overlap regions between images and
LiDAR. To address these challenges, we propose Veila, a novel conditional
diffusion framework that integrates: a Confidence-Aware Conditioning Mechanism
(CACM) that strengthens RGB conditioning by adaptively balancing semantic and
depth cues according to their local reliability; a Geometric Cross-Modal
Alignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a
Panoramic Feature Coherence (PFC) for enforcing global structural consistency
across monocular RGB and panoramic LiDAR. Additionally, we introduce two
metrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to
evaluate alignment quality across modalities. Experiments on nuScenes,
SemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila
achieves state-of-the-art generation fidelity and cross-modal consistency,
while enabling generative data augmentation that improves downstream LiDAR
semantic segmentation.

</details>


### [92] [Contrastive Cross-Bag Augmentation for Multiple Instance Learning-based Whole Slide Image Classification](https://arxiv.org/abs/2508.03081)
*Bo Zhang,Xu Xinan,Shuo Yan,Yu Bai,Zheng Zhang,Wufan Wang,Wendong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为$C^2Aug$的对比跨包增强方法，通过从同类所有包中采样实例来增加伪包多样性，并结合包级和组级对比学习，以提高多实例学习(MIL)在全玻片图像(WSI)分类上的性能，尤其是在肿瘤区域较小的测试幻灯片上。


<details>
  <summary>Details</summary>
Motivation: 现有的伪包增强方法从有限数量的包中采样实例，导致多样性受限；同时，伪包中关键实例（如肿瘤实例）数量的增加，限制了模型在肿瘤区域较小的测试幻灯片上的性能。

Method: 1. 提出对比跨包增强($C^2Aug$)，从所有同类包中采样实例，以增加伪包多样性。2. 引入包级和组级对比学习框架，增强具有不同语义含义特征的判别能力。

Result: 实验结果表明，$C^2Aug$在多个评估指标上持续优于现有最先进的方法。

Conclusion: $C^2Aug$通过增加伪包多样性和增强特征判别能力，有效提高了基于MIL的WSI分类性能，尤其在处理小肿瘤区域时表现出色。

Abstract: Recent pseudo-bag augmentation methods for Multiple Instance Learning
(MIL)-based Whole Slide Image (WSI) classification sample instances from a
limited number of bags, resulting in constrained diversity. To address this
issue, we propose Contrastive Cross-Bag Augmentation ($C^2Aug$) to sample
instances from all bags with the same class to increase the diversity of
pseudo-bags. However, introducing new instances into the pseudo-bag increases
the number of critical instances (e.g., tumor instances). This increase results
in a reduced occurrence of pseudo-bags containing few critical instances,
thereby limiting model performance, particularly on test slides with small
tumor areas. To address this, we introduce a bag-level and group-level
contrastive learning framework to enhance the discrimination of features with
distinct semantic meanings, thereby improving model performance. Experimental
results demonstrate that $C^2Aug$ consistently outperforms state-of-the-art
approaches across multiple evaluation metrics.

</details>


### [93] [La La LiDAR: Large-Scale Layout Generation from LiDAR Data](https://arxiv.org/abs/2508.03691)
*Youquan Liu,Lingdong Kong,Weidong Yang,Xin Li,Ao Liang,Runnan Chen,Ben Fei,Tongliang Liu*

Main category: cs.CV

TL;DR: 该论文提出了“La La LiDAR”模型，一个布局引导的生成框架，用于可控地生成逼真的激光雷达场景，解决了现有扩散模型在前景对象和空间关系控制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的激光雷达生成模型虽然保真度高，但缺乏对前景对象和空间关系的明确控制，这限制了它们在场景模拟和安全验证（如自动驾驶）中的应用。

Method: 本文提出了“La La LiDAR”模型，该模型通过引入语义增强的场景图扩散和关系感知的上下文条件来生成结构化的激光雷达布局，然后通过前景感知的控制注入完成整个场景的生成。为支持结构化激光雷达生成，论文还构建了Waymo-SG和nuScenes-SG两个大规模激光雷达场景图数据集，并提出了新的布局合成评估指标。

Result: La La LiDAR在激光雷达生成和下游感知任务中均达到了最先进的性能，为可控的3D场景生成设立了新的基准。

Conclusion: La La LiDAR通过其布局引导和前景感知控制机制，实现了对激光雷达场景的可定制控制，同时确保了空间和语义一致性，显著提升了可控3D场景生成的能力。

Abstract: Controllable generation of realistic LiDAR scenes is crucial for applications
such as autonomous driving and robotics. While recent diffusion-based models
achieve high-fidelity LiDAR generation, they lack explicit control over
foreground objects and spatial relationships, limiting their usefulness for
scenario simulation and safety validation. To address these limitations, we
propose Large-scale Layout-guided LiDAR generation model ("La La LiDAR"), a
novel layout-guided generative framework that introduces semantic-enhanced
scene graph diffusion with relation-aware contextual conditioning for
structured LiDAR layout generation, followed by foreground-aware control
injection for complete scene generation. This enables customizable control over
object placement while ensuring spatial and semantic consistency. To support
our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two
large-scale LiDAR scene graph datasets, along with new evaluation metrics for
layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves
state-of-the-art performance in both LiDAR generation and downstream perception
tasks, establishing a new benchmark for controllable 3D scene generation.

</details>


### [94] [Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts](https://arxiv.org/abs/2508.03094)
*Jiantao Tan,Peixian Ma,Kanghao Chen,Zhiming Dai,Ruixuan Wang*

Main category: cs.CV

TL;DR: 该研究提出一种新颖的持续学习框架，利用大型语言模型（LLMs）生成的视觉概念作为判别性语义指导，以增强医学图像分类的持续学习能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用文本模态信息进行持续学习时，仅依赖于简单的类名模板，忽视了更丰富的语义信息，限制了多模态信息融合的效果。

Method: 该方法动态构建一个视觉概念池，通过基于相似性的过滤机制避免冗余。然后，通过一个跨模态图像-概念注意力模块和注意力损失，将这些概念整合到持续学习过程中，从而利用相关视觉概念的语义知识，生成具有类别代表性的融合特征进行分类。

Result: 在医学图像和自然图像数据集上的实验表明，该方法实现了最先进的性能，证明了其有效性和优越性。

Conclusion: 该方法通过有效利用LLMs生成的视觉概念作为语义指导，显著提升了图像分类中持续学习的性能，尤其适用于动态变化的临床环境。

Abstract: Continual learning is essential for medical image classification systems to
adapt to dynamically evolving clinical environments. The integration of
multimodal information can significantly enhance continual learning of image
classes. However, while existing approaches do utilize textual modality
information, they solely rely on simplistic templates with a class name,
thereby neglecting richer semantic information. To address these limitations,
we propose a novel framework that harnesses visual concepts generated by large
language models (LLMs) as discriminative semantic guidance. Our method
dynamically constructs a visual concept pool with a similarity-based filtering
mechanism to prevent redundancy. Then, to integrate the concepts into the
continual learning process, we employ a cross-modal image-concept attention
module, coupled with an attention loss. Through attention, the module can
leverage the semantic knowledge from relevant visual concepts and produce
class-representative fused features for classification. Experiments on medical
and natural image datasets show our method achieves state-of-the-art
performance, demonstrating the effectiveness and superiority of our method. We
will release the code publicly.

</details>


### [95] [LiDARCrafter: Dynamic 4D World Modeling from LiDAR Sequences](https://arxiv.org/abs/2508.03692)
*Ao Liang,Youquan Liu,Yu Yang,Dongyue Lu,Linfeng Li,Lingdong Kong,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: LiDARCrafter是一个统一的4D LiDAR生成和编辑框架，通过自然语言指令生成和编辑具有时间一致性的LiDAR序列，解决了自动驾驶中LiDAR世界模型在可控性、时间连贯性和评估标准化方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶生成世界模型主要关注视频或占用栅格，忽略了LiDAR的独特属性。将LiDAR生成扩展到动态4D世界建模在可控性、时间连贯性和评估标准化方面面临挑战。

Method: LiDARCrafter框架将自然语言输入解析为以自我为中心的场景图，这些图作为条件驱动三分支扩散网络生成物体结构、运动轨迹和几何形状。一个自回归模块用于生成时间连贯的4D LiDAR序列。同时，建立了一个包含场景、物体和序列层面的综合评估基准。

Result: 在nuScenes数据集上，LiDARCrafter在保真度、可控性和时间一致性方面均达到了最先进的性能。代码和基准已发布。

Conclusion: LiDARCrafter为自动驾驶中的数据增强和模拟铺平了道路，通过提供一个统一的4D LiDAR生成和编辑框架，并在各项指标上取得了卓越表现。

Abstract: Generative world models have become essential data engines for autonomous
driving, yet most existing efforts focus on videos or occupancy grids,
overlooking the unique LiDAR properties. Extending LiDAR generation to dynamic
4D world modeling presents challenges in controllability, temporal coherence,
and evaluation standardization. To this end, we present LiDARCrafter, a unified
framework for 4D LiDAR generation and editing. Given free-form natural language
inputs, we parse instructions into ego-centric scene graphs, which condition a
tri-branch diffusion network to generate object structures, motion
trajectories, and geometry. These structured conditions enable diverse and
fine-grained scene editing. Additionally, an autoregressive module generates
temporally coherent 4D LiDAR sequences with smooth transitions. To support
standardized evaluation, we establish a comprehensive benchmark with diverse
metrics spanning scene-, object-, and sequence-level aspects. Experiments on
the nuScenes dataset using this benchmark demonstrate that LiDARCrafter
achieves state-of-the-art performance in fidelity, controllability, and
temporal consistency across all levels, paving the way for data augmentation
and simulation. The code and benchmark are released to the community.

</details>


### [96] [AVATAR: Reinforcement Learning to See, Hear, and Reason Over Video](https://arxiv.org/abs/2508.03100)
*Yogesh Kulkarni,Pooyan Fazli*

Main category: cs.CV

TL;DR: AVATAR是一个用于长时视频多模态推理的框架，通过离策略训练和时间优势塑形解决了现有方法的数据效率低下、优势消失和信用分配不均等问题，显著提升了性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GRPO）在长时视频多模态推理中存在三个主要限制：1) 在策略设计导致数据效率低下；2) 组内奖励相同或接近导致优势值消失，学习信号丧失；3) 统一的信用分配未能突出关键推理步骤。

Method: 本文提出了AVATAR（Audio-Video Agent for Alignment and Reasoning）框架，包含两个核心组件：1) 离策略训练架构，通过重用过去经验并增加奖励多样性，提高了样本效率并解决了优势消失问题；2) 时间优势塑形（Temporal Advantage Shaping, TAS），一种新颖的信用分配策略，在学习过程中加权关键推理阶段。

Result: AVATAR在多个基准测试中表现出色，相比Qwen2.5-Omni基线，在MMVU上性能提升+5.4，在OmniBench上提升+4.9，在Video-Holmes上提升+4.5，同时样本效率提高了35%以上。

Conclusion: AVATAR框架通过其创新的离策略训练和时间优势塑形策略，有效克服了长时视频多模态推理中的挑战，显著提升了模型性能和样本效率。

Abstract: Multimodal reasoning over long-horizon video is challenging due to the need
for precise spatiotemporal fusion and alignment across modalities. While recent
methods such as Group Relative Policy Optimization (GRPO) have shown promise in
this domain, they suffer from three key limitations: (1) data inefficiency from
their on-policy design, (2) a vanishing advantage problem, where identical or
near-identical rewards within a group eliminate the learning signal by
producing zero-valued advantages, and (3) uniform credit assignment that fails
to emphasize critical reasoning steps. We introduce AVATAR (Audio-Video Agent
for Alignment and Reasoning), a framework that addresses these limitations
through two core components: (1) an off-policy training architecture that
improves sample efficiency and resolves vanishing advantages by reusing past
experiences with greater reward diversity, and (2) Temporal Advantage Shaping
(TAS), a novel credit assignment strategy that upweights key reasoning phases
during learning. AVATAR achieves strong performance across various benchmarks,
outperforming the Qwen2.5-Omni baseline by +5.4on MMVU, +4.9 on OmniBench, and
+4.5 on Video-Holmes, while demonstrating over 35% higher sample efficiency.

</details>


### [97] [Causal Disentanglement and Cross-Modal Alignment for Enhanced Few-Shot Learning](https://arxiv.org/abs/2508.03102)
*Tianjiao Jiang,Zhen Zhang,Yuhang Liu,Javen Qinfeng Shi*

Main category: cs.CV

TL;DR: 本文提出Causal CLIP Adapter (CCA)，通过无监督独立成分分析(ICA)显式解耦CLIP视觉特征，并增强跨模态对齐，显著提升小样本学习性能和分布偏移鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有小样本学习(FSL)方法依赖纠缠表示，在有限监督下难以有效解耦并适应模型。尽管CLIP等方法能解耦潜在表示，但仍需显式利用以提高适应性并减少过拟合。

Method: CCA框架首先使用无监督独立成分分析(ICA)显式解耦从CLIP提取的视觉特征，减少了从有限标记数据中学习解耦过程的需求。为弥补ICA可能破坏CLIP模态对齐的问题，CCA通过微调基于CLIP的文本分类器进行单向增强，并通过跨注意力机制进行双向增强，丰富视觉和文本表示。最终，将单模态和跨模态分类输出线性组合以提高准确性。

Result: 在11个基准数据集上的大量实验表明，CCA在小样本性能和对分布偏移的鲁棒性方面持续优于现有最先进方法，同时保持了计算效率。

Conclusion: Causal CLIP Adapter (CCA)通过显式解耦视觉特征并增强CLIP的跨模态对齐，有效解决了小样本学习中表示纠缠的问题，从而在性能和鲁棒性上取得了显著提升。

Abstract: Few-shot learning (FSL) often requires effective adaptation of models using
limited labeled data. However, most existing FSL methods rely on entangled
representations, requiring the model to implicitly recover the unmixing process
to obtain disentangled representations using only limited supervision, which
hinders effective adaptation. Recent theoretical studies show that multimodal
contrastive learning methods, such as CLIP, can disentangle latent
representations up to linear transformations. In light of this, we propose the
Causal CLIP Adapter (CCA), a novel framework that explicitly disentangles
visual features extracted from CLIP using unsupervised Independent Component
Analysis (ICA). This removes the need to learn the unmixing process from the
labeled data, thereby reducing the number of trainable parameters and
mitigating overfitting. Taking a step further, while ICA can obtain visual
disentangled representations, it may also disrupt CLIP's intra- and inter-modal
alignment. To counteract this, CCA further leverages CLIP's inherent
cross-modal alignment by enhancing it in two ways: unidirectionally, through
fine-tuning a CLIP-based text classifier, and bidirectionally, via a
cross-attention mechanism that enriches visual and textual representations
through mutual interaction. Both unimodal and cross-modal classification
outputs can be effectively combined linearly to improve classification
accuracy. Extensive experiments on 11 benchmark datasets demonstrate that our
method consistently outperforms state-of-the-art approaches in terms of
few-shot performance and robustness to distributional shifts, while maintaining
computational efficiency. Code will be available at
https://github.com/tianjiao-j/CCA.

</details>


### [98] [H3R: Hybrid Multi-view Correspondence for Generalizable 3D Reconstruction](https://arxiv.org/abs/2508.03118)
*Heng Jia,Linchao Zhu,Na Zhao*

Main category: cs.CV

TL;DR: H3R提出了一种混合框架，结合了体素潜在融合和基于注意力的特征聚合，解决了多视角对应建模中几何精度与收敛速度的矛盾，实现了更快的收敛和更强的泛化能力，并在3D重建任务中达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 尽管前馈3D高斯泼溅技术取得了进展，但可泛化的3D重建仍然具有挑战性，尤其是在多视角对应建模方面。现有方法面临一个基本权衡：显式方法几何精度高但在模糊区域表现不佳，而隐式方法鲁棒性强但收敛缓慢。

Method: H3R是一个混合框架，通过整合体素潜在融合和基于注意力的特征聚合来解决上述限制。它包含两个互补组件：一个通过极线约束强制执行几何一致性的高效潜在体素，以及一个利用普吕克坐标进行自适应对应细化的相机感知Transformer。此外，研究发现空间对齐的基础模型（如SD-VAE）在空间重建方面显著优于语义对齐模型（如DINOv2）。

Result: 该方法在增强泛化能力的同时，收敛速度比现有方法快2倍。它支持可变数量和高分辨率的输入视图，并展示了强大的跨数据集泛化能力。在RealEstate10K、ACID和DTU数据集上，PSNR分别显著提升了0.59 dB、1.06 dB和0.22 dB，达到了最先进的性能。

Conclusion: H3R通过结合体素潜在融合和注意力机制，有效解决了3D重建中多视角对应建模的挑战，实现了高精度、快速收敛和强泛化的性能。同时，研究强调了空间对齐基础模型在空间重建任务中的优越性。

Abstract: Despite recent advances in feed-forward 3D Gaussian Splatting, generalizable
3D reconstruction remains challenging, particularly in multi-view
correspondence modeling. Existing approaches face a fundamental trade-off:
explicit methods achieve geometric precision but struggle with ambiguous
regions, while implicit methods provide robustness but suffer from slow
convergence. We present H3R, a hybrid framework that addresses this limitation
by integrating volumetric latent fusion with attention-based feature
aggregation. Our framework consists of two complementary components: an
efficient latent volume that enforces geometric consistency through epipolar
constraints, and a camera-aware Transformer that leverages Pl\"ucker
coordinates for adaptive correspondence refinement. By integrating both
paradigms, our approach enhances generalization while converging 2$\times$
faster than existing methods. Furthermore, we show that spatial-aligned
foundation models (e.g., SD-VAE) substantially outperform semantic-aligned
models (e.g., DINOv2), resolving the mismatch between semantic representations
and spatial reconstruction requirements. Our method supports variable-number
and high-resolution input views while demonstrating robust cross-dataset
generalization. Extensive experiments show that our method achieves
state-of-the-art performance across multiple benchmarks, with significant PSNR
improvements of 0.59 dB, 1.06 dB, and 0.22 dB on the RealEstate10K, ACID, and
DTU datasets, respectively. Code is available at
https://github.com/JiaHeng-DLUT/H3R.

</details>


### [99] [Landsat30-AU: A Vision-Language Dataset for Australian Landsat Imagery](https://arxiv.org/abs/2508.03127)
*Sai Ma,Zhuang Li,John A Taylor*

Main category: cs.CV

TL;DR: 该研究引入了Landsat30-AU，一个用于长期、低分辨率卫星图像的视觉语言数据集，旨在解决现有VLM在地球观测领域的数据局限性，并证明了在该数据集上进行微调可以显著提升VLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）及其数据集主要关注短期、高分辨率的卫星图像，忽略了对经济、无偏见的全球监测至关重要的低分辨率、多卫星、长期存档数据（如Landsat）。这限制了VLMs在地球观测领域的应用，无法加速专家工作流程、向非专业人士普及数据以及实现全球范围的自动化。

Method: 研究构建了一个名为Landsat30-AU的大规模视觉语言数据集，该数据集包含来自四颗Landsat卫星（5, 7, 8, 9）在澳大利亚上空超过36年间收集的30米分辨率图像。数据集包含两部分：Landsat30-AU-Cap（196,262对图像-标题）和Landsat30-AU-VQA（17,725个人工验证的视觉问答样本）。数据集通过一个自举流程构建，该流程利用通用VLMs进行迭代细化和人工验证以确保数据质量。研究还评估了八个VLM在该基准上的性能，并对Qwen2.5-VL-7B模型进行了轻量级微调。

Result: 评估结果显示，开箱即用的VLM难以理解卫星图像，例如开源遥感VLM EarthDial在图像标题生成方面SPIDEr得分仅为0.07，VQA准确率为0.48。然而，在Landsat30-AU上对Qwen2.5-VL-7B进行轻量级微调后，其标题生成性能从0.11 SPIDEr提升到0.31 SPIDEr，VQA准确率从0.74提升到0.87。

Conclusion: 当前VLM在理解长期、低分辨率卫星图像方面存在局限性。新创建的Landsat30-AU数据集填补了现有数据集的空白，并为该领域的研究提供了宝贵的资源。对现有VLM进行轻量级微调，能够显著提升其在卫星图像理解任务上的性能，为地球观测的民主化和自动化提供了新的途径。

Abstract: Vision language models (VLMs) that enable natural language interaction with
satellite imagery can democratize Earth observation by accelerating expert
workflows, making data accessible to non-specialists, and enabling planet-scale
automation. However, existing datasets focus mainly on short-term,
high-resolution imagery from a limited number of satellites, overlooking
low-resolution, multi-satellite, long-term archives, such as Landsat, that are
essential for affordable and bias-robust global monitoring. We address this gap
with Landsat30-AU, a large-scale vision-language dataset built from 30-meter
resolution imagery collected by four Landsat satellites (5, 7, 8, and 9) over
Australia, spanning more than 36 years. The dataset includes two components:
Landsat30-AU-Cap, containing 196,262 image-caption pairs, and Landsat30-AU-VQA,
comprising 17,725 human-verified visual question answering (VQA) samples across
eight remote sensing domains. Both datasets are curated through a bootstrapped
pipeline that leverages generic VLMs with iterative refinement and human
verification to ensure quality. Our evaluation of eight VLMs on our benchmark
reveals that off-the-shelf models struggle to understand satellite imagery. The
open-source remote-sensing VLM EarthDial achieves only 0.07 SPIDEr in
captioning and a VQA accuracy of 0.48, highlighting the limitations of current
approaches. Encouragingly, lightweight fine-tuning of Qwen2.5-VL-7B on
Landsat30-AU improves captioning performance from 0.11 to 0.31 SPIDEr and
boosts VQA accuracy from \textbf{0.74} to 0.87. Code and data are available at
https://github.com/papersubmit1/landsat30-au.

</details>


### [100] [Uint: Building Uint Detection Dataset](https://arxiv.org/abs/2508.03139)
*Haozhou Zhai,Yanzhe Gao,Tianjiang Hu*

Main category: cs.CV

TL;DR: 该研究引入了一个由无人机视角捕获的合成建筑单元火灾数据集，以解决现有火灾数据集中带注释建筑单元数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的火灾场景数据集，尤其是针对建筑单元的带注释数据，严重不足，这限制了计算机视觉模型在火灾预警和应急救援任务中的训练效果。

Method: 通过以下技术构建数据集：使用真实多层场景构建背景，结合运动模糊和亮度调整增强图像真实性，模拟不同条件下的无人机拍摄，并利用大型模型在不同位置生成火灾效果。

Result: 生成了一个包含1978张图像的合成数据集，涵盖了广泛的建筑场景。该数据集能有效提高火灾单元检测的泛化能力，提供多场景和可扩展数据，并降低收集真实火灾数据的风险和成本。

Conclusion: 所创建的合成数据集有效弥补了带注释建筑单元火灾数据的不足，为火灾单元检测提供了宝贵且可扩展的训练资源。

Abstract: Fire scene datasets are crucial for training robust computer vision models,
particularly in tasks such as fire early warning and emergency rescue
operations. However, among the currently available fire-related data, there is
a significant shortage of annotated data specifically targeting building
units.To tackle this issue, we introduce an annotated dataset of building units
captured by drones, which incorporates multiple enhancement techniques. We
construct backgrounds using real multi-story scenes, combine motion blur and
brightness adjustment to enhance the authenticity of the captured images,
simulate drone shooting conditions under various circumstances, and employ
large models to generate fire effects at different locations.The synthetic
dataset generated by this method encompasses a wide range of building
scenarios, with a total of 1,978 images. This dataset can effectively improve
the generalization ability of fire unit detection, providing multi-scenario and
scalable data while reducing the risks and costs associated with collecting
real fire data. The dataset is available at
https://github.com/boilermakerr/FireUnitData.

</details>


### [101] [UniEdit-I: Training-free Image Editing for Unified VLM via Iterative Understanding, Editing and Verifying](https://arxiv.org/abs/2508.03142)
*Chengyu Bai,Jintao Chen,Xiang Bai,Yilong Chen,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: 本文提出UniEdit-I，一个无需训练的框架，通过理解、编辑、验证三步迭代循环，为统一视觉-语言模型（VLM）赋能图像编辑能力。


<details>
  <summary>Details</summary>
Motivation: 尽管统一VLM在视觉理解和生成方面取得了快速进展，并且GPT-4o的生成管线（理解VLM->视觉特征->投影器->扩散模型->图像）显示出巨大潜力，但如何轻松实现图像编辑能力仍未被探索。

Method: UniEdit-I框架包含三个迭代步骤：1. 理解：通过结构化语义分析从源图像创建源提示，并根据编辑指令进行最小词替换以形成目标提示。2. 编辑：引入时间自适应偏移，在去噪过程中实现从粗到细的连贯编辑。3. 验证：检查目标提示与中间编辑图像的一致性，提供自动一致性分数和纠正反馈，并决定是否提前停止或继续编辑循环。该循环迭代直至收敛，实现无需训练的高保真编辑。

Result: 基于最新的BLIP3-o实现，UniEdit-I在GEdit-Bench基准测试上达到了最先进的性能（SOTA）。

Conclusion: UniEdit-I提供了一种无需训练的、高保真图像编辑方法，成功为统一VLM（如基于GPT-4o生成管线的模型）赋能了图像编辑能力，并通过理解、编辑、验证的迭代循环实现了卓越性能。

Abstract: In recent years, unified vision-language models (VLMs) have rapidly advanced,
effectively tackling both visual understanding and generation tasks within a
single design. While many unified VLMs have explored various design choices,
the recent hypothesis from OpenAI's GPT-4o suggests a promising generation
pipeline: Understanding VLM->Visual Feature->Projector->Diffusion Model->Image.
The understanding VLM is frozen, and only the generation-related modules are
trained. This pipeline maintains the strong capability of understanding VLM
while enabling the image generation ability of the unified VLM. Although this
pipeline has shown very promising potential for the future development of
unified VLM, how to easily enable image editing capability is still unexplored.
In this paper, we introduce a novel training-free framework named UniEdit-I to
enable the unified VLM with image editing capability via three iterative steps:
understanding, editing, and verifying. 1. The understanding step analyzes the
source image to create a source prompt through structured semantic analysis and
makes minimal word replacements to form the target prompt based on the editing
instruction. 2. The editing step introduces a time-adaptive offset, allowing
for coherent editing from coarse to fine throughout the denoising process. 3.
The verification step checks the alignment between the target prompt and the
intermediate edited image, provides automatic consistency scores and corrective
feedback, and determines whether to stop early or continue the editing loop.
This understanding, editing, and verifying loop iterates until convergence,
delivering high-fidelity editing in a training-free manner. We implemented our
method based on the latest BLIP3-o and achieved state-of-the-art (SOTA)
performance on the GEdit-Bench benchmark.

</details>


### [102] [SARD: Segmentation-Aware Anomaly Synthesis via Region-Constrained Diffusion with Discriminative Mask Guidance](https://arxiv.org/abs/2508.03143)
*Yanshu Wang,Xichen Xu,Xiaoning Lei,Guoyang Xie*

Main category: cs.CV

TL;DR: SARD是一种新型的基于扩散模型的异常合成框架，通过区域约束扩散和判别性掩码引导，实现了高空间精度和区域保真度的工业异常生成，提升了异常检测系统的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在生成复杂缺陷模式方面表现出色，但在空间可控性和保持精细区域保真度方面存在不足，导致生成的异常可能缺乏空间精确性或引入背景伪影，从而影响工业异常检测系统的鲁棒性。

Method: 本文提出了SARD框架，包含两个核心组件：1. 区域约束扩散（RCD）过程：在逆向去噪阶段冻结背景，仅选择性地更新前景异常区域，以减少背景伪影并保持背景完整性。2. 判别性掩码引导（DMG）模块：集成到判别器中，通过像素级掩码引导，联合评估全局真实性和局部异常保真度。

Result: 在MVTec-AD和BTAD数据集上的大量实验表明，SARD在分割精度和视觉质量方面均超越了现有方法，为像素级异常合成设定了新的最先进水平。

Conclusion: SARD通过其创新的区域约束扩散和判别性掩码引导机制，有效解决了现有扩散模型在异常合成中空间可控性和区域保真度不足的问题，实现了逼真且空间精确的异常生成，显著提升了工业异常检测系统的性能。

Abstract: Synthesizing realistic and spatially precise anomalies is essential for
enhancing the robustness of industrial anomaly detection systems. While recent
diffusion-based methods have demonstrated strong capabilities in modeling
complex defect patterns, they often struggle with spatial controllability and
fail to maintain fine-grained regional fidelity. To overcome these limitations,
we propose SARD (Segmentation-Aware anomaly synthesis via Region-constrained
Diffusion with discriminative mask Guidance), a novel diffusion-based framework
specifically designed for anomaly generation. Our approach introduces a
Region-Constrained Diffusion (RCD) process that preserves the background by
freezing it and selectively updating only the foreground anomaly regions during
the reverse denoising phase, thereby effectively reducing background artifacts.
Additionally, we incorporate a Discriminative Mask Guidance (DMG) module into
the discriminator, enabling joint evaluation of both global realism and local
anomaly fidelity, guided by pixel-level masks. Extensive experiments on the
MVTec-AD and BTAD datasets show that SARD surpasses existing methods in
segmentation accuracy and visual quality, setting a new state-of-the-art for
pixel-level anomaly synthesis.

</details>


### [103] [LORE: Latent Optimization for Precise Semantic Control in Rectified Flow-based Image Editing](https://arxiv.org/abs/2508.03144)
*Liangyang Ouyang,Jiafeng Mao*

Main category: cs.CV

TL;DR: 现有基于反演的文本驱动图像编辑方法存在语义偏差，导致编辑失败或非目标区域修改。本文提出LORE，一种无需训练的图像编辑方法，通过直接优化反演噪声，显著提升了概念替换的稳定性、可控性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前基于反演的整流流模型在文本驱动图像编辑中，存在结构性限制：反演噪声中编码的源概念语义偏向会抑制对目标概念的注意力。当源和目标语义不相似时，这尤其导致编辑失败或在非目标区域产生意外修改。

Method: 本文提出了LORE，一种无需训练且高效的图像编辑方法。LORE通过直接优化反演噪声来解决现有方法在泛化性和可控性方面的核心限制，从而实现稳定、可控、通用的概念替换，且无需修改架构或模型微调。

Result: 在PIEBench、SmartEdit和GapEdit三个挑战性基准测试中，LORE在语义对齐、图像质量和背景保真度方面显著优于现有强基线，证明了潜在空间优化在通用图像编辑中的有效性和可扩展性。

Conclusion: LORE通过直接优化反演噪声，有效解决了现有文本驱动图像编辑方法中存在的语义偏差和注意力抑制问题，证明了潜在空间优化对于通用图像编辑的有效性和可扩展性，实现了更稳定、可控和泛化的概念替换。

Abstract: Text-driven image editing enables users to flexibly modify visual content
through natural language instructions, and is widely applied to tasks such as
semantic object replacement, insertion, and removal. While recent
inversion-based editing methods using rectified flow models have achieved
promising results in image quality, we identify a structural limitation in
their editing behavior: the semantic bias toward the source concept encoded in
the inverted noise tends to suppress attention to the target concept. This
issue becomes particularly critical when the source and target semantics are
dissimilar, where the attention mechanism inherently leads to editing failure
or unintended modifications in non-target regions. In this paper, we
systematically analyze and validate this structural flaw, and introduce LORE, a
training-free and efficient image editing method. LORE directly optimizes the
inverted noise, addressing the core limitations in generalization and
controllability of existing approaches, enabling stable, controllable, and
general-purpose concept replacement, without requiring architectural
modification or model fine-tuning. We conduct comprehensive evaluations on
three challenging benchmarks: PIEBench, SmartEdit, and GapEdit. Experimental
results show that LORE significantly outperforms strong baselines in terms of
semantic alignment, image quality, and background fidelity, demonstrating the
effectiveness and scalability of latent-space optimization for general-purpose
image editing.

</details>


### [104] [ChartCap: Mitigating Hallucination of Dense Chart Captioning](https://arxiv.org/abs/2508.03164)
*Junyoung Lim,Jaewoo Ahn,Gunhee Kim*

Main category: cs.CV

TL;DR: 本文介绍了ChartCap，一个包含56.5万张真实世界图表图像及其类型特定、密集、无无关信息的描述的大规模数据集，旨在解决现有图表描述生成中幻觉和信息不足的问题。研究还提出了一种新的评估指标和构建方法，并证明了ChartCap能显著提升模型生成图表描述的准确性和信息量。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在为图表生成准确、信息丰富且无幻觉的描述方面面临挑战，主要原因是缺乏大规模、高质量的真实世界图表数据集。现有数据集存在包含图表无法推断的无关信息，并且未能充分捕捉结构元素和关键见解的问题。

Method: 本文提出了ChartCap数据集，包含56.5万张真实世界图表图像，并配有类型特定、密集、排除无关信息且详细突出结构元素和关键见解的描述。构建ChartCap采用了一个四阶段流水线，仅使用图表可识别的数据生成描述，并采用基于循环一致性的人工验证来加速质量控制。此外，还提出了一种新的评估指标——视觉一致性分数（Visual Consistency Score），通过测量从描述重建的图表与原始图表之间的相似性来评估描述质量。

Result: 广泛的实验证实，在ChartCap上微调的模型能够持续生成更准确、信息更丰富且幻觉更少的描述，其性能超越了开源模型、专有模型乃至人工标注的描述。

Conclusion: ChartCap数据集的引入及其构建方法和新的评估指标，显著提升了视觉语言模型在图表描述生成方面的能力，解决了现有数据集的局限性，为未来图表理解和描述生成研究奠定了基础。

Abstract: Generating accurate, informative, and hallucination-free captions for charts
remains challenging for vision language models, primarily due to the lack of
large-scale, high-quality datasets of real-world charts. However, existing
real-world chart datasets suffer from the inclusion of extraneous information
that cannot be inferred from the chart and failure to sufficiently capture
structural elements and key insights. Therefore, we introduce ChartCap, a
large-scale dataset of 565K real-world chart images paired with type-specific,
dense captions that exclude extraneous information and highlight both
structural elements and key insights in detail. To build ChartCap, we design a
four-stage pipeline that generates captions using only the discernible data
from the chart and employ a cycle consistency-based human verification, which
accelerates quality control without sacrificing accuracy. Additionally, we
propose a novel metric, the Visual Consistency Score, which evaluates caption
quality by measuring the similarity between the chart regenerated from a
caption and the original chart, independent of reference captions. Extensive
experiments confirms that models fine-tuned on ChartCap consistently generate
more accurate and informative captions with reduced hallucinations, surpassing
both open-source and proprietary models and even human-annotated captions.

</details>


### [105] [SAVER: Mitigating Hallucinations in Large Vision-Language Models via Style-Aware Visual Early Revision](https://arxiv.org/abs/2508.03177)
*Zhaoxu Li,Chenqi Kong,Yi Yu,Qiangqiang Wu,Xinghao Jiang,Ngai-Man Cheung,Bihan Wen,Alex Kot,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文发现大型视觉语言模型（LVLMs）在处理风格化图像时存在严重的幻觉问题，并提出了一个名为SAVER的新机制，通过利用早期层视觉注意力反馈来有效缓解这一问题。


<details>
  <summary>Details</summary>
Motivation: LVLMs在理解复杂视觉-文本上下文方面取得了显著进展，但幻觉问题限制了其在现实世界中的应用。现有缓解方法主要关注摄影图像，忽略了风格化图像带来的潜在风险，而风格化图像在游戏、艺术教育和医疗分析等关键场景中至关重要。

Method: 首先构建了一个包含摄影图像及其对应风格化版本并带有详细标注的数据集。其次，在该数据集上对13个先进的LVLMs进行了判别和生成任务的基准测试。最后，提出了Style-Aware Visual Early Revision (SAVER) 机制，该机制基于标记级别的视觉注意力模式，利用早期层反馈动态调整LVLMs的最终输出，以缓解风格化图像引起的幻觉。

Result: 研究发现，风格化图像比摄影图像更容易诱发显著更多的幻觉。大量实验表明，SAVER在各种模型、数据集和任务中都实现了最先进的幻觉缓解性能。

Conclusion: 风格化图像是导致LVLM幻觉的重要来源。SAVER通过利用早期层视觉注意力模式，能够有效缓解LVLMs在处理风格化图像时产生的幻觉问题，提升了模型的实际应用性。

Abstract: Large Vision-Language Models (LVLMs) recently achieve significant
breakthroughs in understanding complex visual-textual contexts. However,
hallucination issues still limit their real-world applicability. Although
previous mitigation methods effectively reduce hallucinations in photographic
images, they largely overlook the potential risks posed by stylized images,
which play crucial roles in critical scenarios such as game scene
understanding, art education, and medical analysis. In this work, we first
construct a dataset comprising photographic images and their corresponding
stylized versions with carefully annotated caption labels. We then conduct
head-to-head comparisons on both discriminative and generative tasks by
benchmarking 13 advanced LVLMs on the collected datasets. Our findings reveal
that stylized images tend to induce significantly more hallucinations than
their photographic counterparts. To address this issue, we propose Style-Aware
Visual Early Revision SAVER, a novel mechanism that dynamically adjusts LVLMs'
final outputs based on the token-level visual attention patterns, leveraging
early-layer feedback to mitigate hallucinations caused by stylized images.
Extensive experiments demonstrate that SAVER achieves state-of-the-art
performance in hallucination mitigation across various models, datasets, and
tasks.

</details>


### [106] [Advancing Precision in Multi-Point Cloud Fusion Environments](https://arxiv.org/abs/2508.03179)
*Ulugbek Alibekov,Vanessa Staderini,Philipp Schneider,Doris Antensteiner*

Main category: cs.CV

TL;DR: 该研究通过评估点云和多点云匹配方法，并引入合成数据集、距离度量和CloudCompare插件，以提高工业视觉检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 提高自动化工业检测系统的准确性和效率，特别是在视觉工业检测中对点云进行评估和匹配。

Method: 评估点云和多点云匹配方法；引入合成数据集用于配准方法和距离度量的定量评估；开发CloudCompare插件用于合并多点云和可视化表面缺陷。

Result: 提出了一种用于定量评估配准方法和距离度量的合成数据集；开发了一个新的CloudCompare插件，能够合并多点云并可视化表面缺陷。

Conclusion: 所提出的方法和工具（包括合成数据集和CloudCompare插件）能够提高自动化工业检测系统的准确性和效率。

Abstract: This research focuses on visual industrial inspection by evaluating point
clouds and multi-point cloud matching methods. We also introduce a synthetic
dataset for quantitative evaluation of registration method and various distance
metrics for point cloud comparison. Additionally, we present a novel
CloudCompare plugin for merging multiple point clouds and visualizing surface
defects, enhancing the accuracy and efficiency of automated inspection systems.

</details>


### [107] [Duplex-GS: Proxy-Guided Weighted Blending for Real-Time Order-Independent Gaussian Splatting](https://arxiv.org/abs/2508.03180)
*Weihang Liu,Yuke Li,Yuxuan Li,Jingyi Yu,Xin Lou*

Main category: cs.CV

TL;DR: Duplex-GS是一种双层级框架，结合代理高斯表示和顺序无关渲染（OIT）技术，显著提升了3D高斯泼溅（3DGS）的渲染效率和质量，尤其适用于资源受限平台。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法依赖计算昂贵的顺序alpha混合操作，导致显著开销，尤其在资源受限平台上性能不佳。

Method: 提出Duplex-GS，一个双层级框架，集成代理高斯表示与顺序无关渲染技术。引入单元代理管理局部高斯，并提出单元搜索光栅化以加速。与OIT结合，开发了物理启发式加权和渲染技术，消除“弹出”和“透明度”伪影。

Result: 实现了逼真的渲染效果和实时性能。在多种真实世界数据集上表现出鲁棒性，包括多尺度训练视图和大规模环境。与现有基于OIT的3DGS方法相比，渲染速度提升1.5到4倍，radix排序开销减少52.2%到86.9%，且无质量损失。

Conclusion: Duplex-GS验证了OIT渲染范式在高斯泼溅中的优势，通过解决效率和伪影问题，显著提升了渲染质量和性能。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have demonstrated remarkable
rendering fidelity and efficiency. However, these methods still rely on
computationally expensive sequential alpha-blending operations, resulting in
significant overhead, particularly on resource-constrained platforms. In this
paper, we propose Duplex-GS, a dual-hierarchy framework that integrates proxy
Gaussian representations with order-independent rendering techniques to achieve
photorealistic results while sustaining real-time performance. To mitigate the
overhead caused by view-adaptive radix sort, we introduce cell proxies for
local Gaussians management and propose cell search rasterization for further
acceleration. By seamlessly combining our framework with Order-Independent
Transparency (OIT), we develop a physically inspired weighted sum rendering
technique that simultaneously eliminates "popping" and "transparency"
artifacts, yielding substantial improvements in both accuracy and efficiency.
Extensive experiments on a variety of real-world datasets demonstrate the
robustness of our method across diverse scenarios, including multi-scale
training views and large-scale environments. Our results validate the
advantages of the OIT rendering paradigm in Gaussian Splatting, achieving
high-quality rendering with an impressive 1.5 to 4 speedup over existing OIT
based Gaussian Splatting approaches and 52.2% to 86.9% reduction of the radix
sort overhead without quality degradation.

</details>


### [108] [Monocular Depth Estimation with Global-Aware Discretization and Local Context Modeling](https://arxiv.org/abs/2508.03186)
*Heng Wu,Qian Zhang,Guixu Zhang*

Main category: cs.CV

TL;DR: 本文提出一种结合局部和全局线索的单目深度估计算法，通过Gated Large Kernel Attention Module (GLKAM)捕获局部多尺度信息，并通过Global Bin Prediction Module (GBPM)提供全局深度分布指导，在NYU-V2和KITTI数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 单目深度估计是一个具有挑战性的问题，因为从单一视图恢复3D结构本质上是一个病态问题，存在多种可能的深度配置产生相同的2D投影的模糊性。

Method: 1. 提出Gated Large Kernel Attention Module (GLKAM)，利用大核卷积和门控机制有效捕获多尺度局部结构信息。2. 引入Global Bin Prediction Module (GBPM)，估计深度bin的全局分布，为深度回归提供结构指导。

Result: 在NYU-V2和KITTI数据集上进行了广泛实验，结果表明所提出的方法达到了有竞争力的性能，并优于现有方法，验证了每个提出组件的有效性。

Conclusion: 所提出的结合局部和全局线索的单目深度估计算法及其组件是有效的，并在标准数据集上取得了领先的性能。

Abstract: Accurate monocular depth estimation remains a challenging problem due to the
inherent ambiguity that stems from the ill-posed nature of recovering 3D
structure from a single view, where multiple plausible depth configurations can
produce identical 2D projections. In this paper, we present a novel depth
estimation method that combines both local and global cues to improve
prediction accuracy. Specifically, we propose the Gated Large Kernel Attention
Module (GLKAM) to effectively capture multi-scale local structural information
by leveraging large kernel convolutions with a gated mechanism. To further
enhance the global perception of the network, we introduce the Global Bin
Prediction Module (GBPM), which estimates the global distribution of depth bins
and provides structural guidance for depth regression. Extensive experiments on
the NYU-V2 and KITTI dataset demonstrate that our method achieves competitive
performance and outperforms existing approaches, validating the effectiveness
of each proposed component.

</details>


### [109] [Unifying Locality of KANs and Feature Drift Compensation for Data-free Continual Face Forgery Detection](https://arxiv.org/abs/2508.03189)
*Tianshuo Zhang,Siran Peng,Li Gao,Haoyuan Zhang,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 本文提出了一种基于Kolmogorov-Arnold网络（KANs）的持续人脸伪造检测框架（KAN-CFD），以解决持续学习中灾难性遗忘问题，并通过改进KANs来处理高维图像输入和避免特征空间重叠。


<details>
  <summary>Details</summary>
Motivation: 人脸伪造技术快速发展，要求检测器持续适应新方法，这属于持续学习范畴。然而，现有检测器在学习新伪造类型时，对旧类型的性能会迅速下降，即灾难性遗忘。KANs因其局部可塑性激活函数理论上适合解决此问题，但其在处理高维图像和持续学习中特征重叠时存在局限性。

Method: 提出了KAN-based Continual Face Forgery Detection (KAN-CFD) 框架，包含两个核心组件：1) Domain-Group KAN Detector (DG-KD)，使KANs能够处理高维图像输入并保持局部性和局部可塑性；2) data-free replay Feature Separation strategy via KAN Drift Compensation Projection (FS-KDCP)，在不使用过往任务数据的情况下避免KAN输入空间的重叠。

Result: 实验结果表明，所提出的方法在持续人脸伪造检测中取得了卓越的性能，并显著减少了灾难性遗忘。

Conclusion: 所提出的KAN-CFD框架有效解决了持续人脸伪造检测中的灾难性遗忘问题，通过对KANs的创新改进，使其能够适应高维图像输入和避免特征空间重叠，从而在性能和遗忘减少方面均表现出色。

Abstract: The rapid advancements in face forgery techniques necessitate that detectors
continuously adapt to new forgery methods, thus situating face forgery
detection within a continual learning paradigm. However, when detectors learn
new forgery types, their performance on previous types often degrades rapidly,
a phenomenon known as catastrophic forgetting. Kolmogorov-Arnold Networks
(KANs) utilize locally plastic splines as their activation functions, enabling
them to learn new tasks by modifying only local regions of the functions while
leaving other areas unaffected. Therefore, they are naturally suitable for
addressing catastrophic forgetting. However, KANs have two significant
limitations: 1) the splines are ineffective for modeling high-dimensional
images, while alternative activation functions that are suitable for images
lack the essential property of locality; 2) in continual learning, when
features from different domains overlap, the mapping of different domains to
distinct curve regions always collapses due to repeated modifications of the
same regions. In this paper, we propose a KAN-based Continual Face Forgery
Detection (KAN-CFD) framework, which includes a Domain-Group KAN Detector
(DG-KD) and a data-free replay Feature Separation strategy via KAN Drift
Compensation Projection (FS-KDCP). DG-KD enables KANs to fit high-dimensional
image inputs while preserving locality and local plasticity. FS-KDCP avoids the
overlap of the KAN input spaces without using data from prior tasks.
Experimental results demonstrate that the proposed method achieves superior
performance while notably reducing forgetting.

</details>


### [110] [Neovascularization Segmentation via a Multilateral Interaction-Enhanced Graph Convolutional Network](https://arxiv.org/abs/2508.03197)
*Tao Chen,Dan Zhang,Da Chen,Huazhu Fu,Kai Jin,Shanshan Wang,Laurent D. Cohen,Yitian Zhao,Quanyong Yi,Jiong Zhang*

Main category: cs.CV

TL;DR: 该研究构建了首个公开的CNV数据集（CNVSeg），并提出了一个新颖的多边图卷积交互增强网络（MTG-Net），用于光学相干断层扫描血管造影（OCTA）图像中的脉络膜新生血管（CNV）区域和血管的精确分割，以解决不规则形状、伪影和数据集缺乏等挑战。


<details>
  <summary>Details</summary>
Motivation: 脉络膜新生血管（CNV）是湿性年龄相关性黄斑变性（湿性AMD）的主要特征，是全球失明的主要原因。在临床实践中，OCTA常用于研究CNV相关的病理变化，因此，准确分割CNV区域和血管对于湿性AMD的临床评估至关重要。然而，CNV形状不规则、成像限制（如投影伪影、噪声和边界模糊）以及缺乏公开数据集带来了挑战。

Method: 本研究构建了首个公开的CNV数据集（CNVSeg），并提出了一个新颖的多边图卷积交互增强CNV分割网络（MTG-Net）。MTG-Net整合了区域和血管的形态信息，在图域中探索语义和几何对偶约束。具体而言，MTG-Net包含一个多任务框架和两个基于图的跨任务模块：多边交互图推理（MIGR）和多边强化图推理（MRGR）。多任务框架编码病灶形状和表面的丰富几何特征，将图像解耦为三个任务特定的特征图。MIGR和MRGR通过图机制迭代推理跨任务的高阶关系，实现任务特定目标的互补优化。此外，还提出了一种不确定性加权损失，以减轻伪影和噪声对分割精度的影响。

Result: 实验结果表明，MTG-Net优于现有方法，在区域分割方面实现了87.21%的Dice分数，在血管分割方面实现了88.12%的Dice分数。

Conclusion: MTG-Net通过整合区域和血管信息、利用图推理机制以及引入不确定性加权损失，有效解决了CNV分割中的挑战，并在新构建的公开数据集上取得了优异的性能，为湿性AMD的临床评估提供了重要工具。

Abstract: Choroidal neovascularization (CNV), a primary characteristic of wet
age-related macular degeneration (wet AMD), represents a leading cause of
blindness worldwide. In clinical practice, optical coherence tomography
angiography (OCTA) is commonly used for studying CNV-related pathological
changes, due to its micron-level resolution and non-invasive nature. Thus,
accurate segmentation of CNV regions and vessels in OCTA images is crucial for
clinical assessment of wet AMD. However, challenges existed due to irregular
CNV shapes and imaging limitations like projection artifacts, noises and
boundary blurring. Moreover, the lack of publicly available datasets
constraints the CNV analysis. To address these challenges, this paper
constructs the first publicly accessible CNV dataset (CNVSeg), and proposes a
novel multilateral graph convolutional interaction-enhanced CNV segmentation
network (MTG-Net). This network integrates both region and vessel morphological
information, exploring semantic and geometric duality constraints within the
graph domain. Specifically, MTG-Net consists of a multi-task framework and two
graph-based cross-task modules: Multilateral Interaction Graph Reasoning (MIGR)
and Multilateral Reinforcement Graph Reasoning (MRGR). The multi-task framework
encodes rich geometric features of lesion shapes and surfaces, decoupling the
image into three task-specific feature maps. MIGR and MRGR iteratively reason
about higher-order relationships across tasks through a graph mechanism,
enabling complementary optimization for task-specific objectives. Additionally,
an uncertainty-weighted loss is proposed to mitigate the impact of artifacts
and noise on segmentation accuracy. Experimental results demonstrate that
MTG-Net outperforms existing methods, achieving a Dice socre of 87.21\% for
region segmentation and 88.12\% for vessel segmentation.

</details>


### [111] [AlignCAT: Visual-Linguistic Alignment of Category and Attributefor Weakly Supervised Visual Grounding](https://arxiv.org/abs/2508.03201)
*Yidan Wang,Chenyi Zhuang,Wutao Liu,Pan Gao,Nicu Sebe*

Main category: cs.CV

TL;DR: AlignCAT是一种新颖的弱监督视觉定位框架，通过粗粒度和细粒度对齐模块，有效解决文本描述中的类别和属性模糊性问题，提升跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有弱监督视觉定位方法在区分文本表达中的细微语义差异时，缺乏强大的跨模态推理能力，原因在于存在基于类别和基于属性的歧义。

Method: 本文提出AlignCAT，一个基于查询的语义匹配框架。它包含两个模块：1) 粗粒度对齐模块，利用类别信息和全局上下文减少不一致对象的干扰；2) 细粒度对齐模块，利用描述性信息和词级文本特征实现属性一致性。该方法通过充分利用语言线索，逐步过滤未对齐的视觉查询并增强对比学习效率。

Result: 在RefCOCO、RefCOCO+和RefCOCOg三个视觉定位基准测试上，AlignCAT在两项视觉定位任务中均表现出优于现有弱监督方法的性能。

Conclusion: AlignCAT通过其新颖的查询式语义匹配框架和渐进式粗细粒度对齐策略，有效解决了弱监督视觉定位中的类别和属性模糊性挑战，显著提升了视觉-语言对齐和跨模态推理能力。

Abstract: Weakly supervised visual grounding (VG) aims to locate objects in images
based on text descriptions. Despite significant progress, existing methods lack
strong cross-modal reasoning to distinguish subtle semantic differences in text
expressions due to category-based and attribute-based ambiguity. To address
these challenges, we introduce AlignCAT, a novel query-based semantic matching
framework for weakly supervised VG. To enhance visual-linguistic alignment, we
propose a coarse-grained alignment module that utilizes category information
and global context, effectively mitigating interference from
category-inconsistent objects. Subsequently, a fine-grained alignment module
leverages descriptive information and captures word-level text features to
achieve attribute consistency. By exploiting linguistic cues to their fullest
extent, our proposed AlignCAT progressively filters out misaligned visual
queries and enhances contrastive learning efficiency. Extensive experiments on
three VG benchmarks, namely RefCOCO, RefCOCO+, and RefCOCOg, verify the
superiority of AlignCAT against existing weakly supervised methods on two VG
tasks. Our code is available at: https://github.com/I2-Multimedia-Lab/AlignCAT.

</details>


### [112] [VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation](https://arxiv.org/abs/2508.03351)
*Yufei Xue,Yushi Huang,Jiawei Shao,Jun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为VLMQ的新型重要性感知后训练量化（PTQ）框架，专门针对视觉-语言模型（VLMs）的模态差异（文本令牌有限，视觉令牌冗余），解决了现有LLM PTQ方法在VLM上性能下降的问题，并在多项基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 后训练量化（PTQ）已被广泛应用于压缩大型语言模型（LLMs）并加速其推理，但在视觉-语言模型（VLMs）中的应用尚未得到充分探索。研究发现VLM存在模态差异（文本令牌有限vs视觉令牌冗余），而现有的基于Hessian的LLM PTQ方法在量化时平等对待所有令牌，导致在应用于VLM时性能严重下降。

Method: 本文提出了VLMQ框架。为解决视觉令牌冗余问题，VLMQ1）优化了一个重要性感知目标，生成带有令牌级重要性因子的增强Hessian，同时保持与并行权重更新的兼容性；2）通过单次轻量级块级反向传播计算这些因子，并结合了与令牌级扰动的理论联系，确保了效率和有效性。

Result: 在0.5B到32B的VLM上，通过8个基准测试的广泛评估表明，VLMQ实现了最先进的（SOTA）性能，尤其是在低比特设置下。例如，在2比特量化下，VLMQ在MME-RealWorld上实现了16.45%的显著提升。

Conclusion: VLMQ通过引入重要性感知机制，有效解决了VLM在PTQ中因模态差异导致的性能下降问题，在低比特量化下表现出卓越的性能，证明了其在VLM压缩和加速方面的巨大潜力。

Abstract: Post-training quantization (PTQ) has emerged as an effective approach for
compressing large models and accelerating their inference without retraining.
While PTQ has been extensively studied in the context of large language models
(LLMs), its applicability to vision-language models (VLMs) remains
underexplored. In this paper, we identify a modality discrepancy (\emph{i.e.},
limited text tokens \emph{vs.} excessive and redundant vision tokens) of VLMs.
However, existing Hessian-based LLM PTQ methods treat all tokens equally during
quantization, resulting in severe performance drops when applied to VLMs.
Motivated by this observation, we propose a novel importance-aware PTQ
framework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token
redundancy, VLMQ 1) optimizes an importance-aware objective that yields an
enhanced Hessian with token-level importance factors, while retaining
compatibility with parallelized weight updates, and 2) ensures efficiency and
effectiveness by computing these factors via a single lightweight block-wise
backward pass, guided by a theoretical connection to token-level perturbations.
Extensive evaluations on 8 benchmarks across 0.5B$\sim$32B VLMs demonstrate the
state-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit
settings. For example, it achieves a substantial \textbf{16.45\%} improvement
on MME-RealWorld under 2-bit quantization.

</details>


### [113] [Open-Vocabulary HOI Detection with Interaction-aware Prompt and Concept Calibration](https://arxiv.org/abs/2508.03207)
*Ting Lei,Shaofeng Yin,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出INP-CC，一个端到端的开放词汇人-物交互（HOI）检测器，通过交互感知提示和概念校准来解决现有方法在细粒度区域级检测和文本描述编码上的不足，并在多个数据集上显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前开放词汇HOI检测方法依赖视觉和语言模型（VLMs），但存在两个主要挑战：1) 图像编码器次优，因为图像级预训练与HOI所需的细粒度区域级检测不匹配；2) 有效编码视觉外观的文本描述仍很困难，限制了模型捕捉详细HOI关系的能力。

Method: 本文提出INteraction-aware Prompting with Concept Calibration (INP-CC)。具体方法包括：1) 一个交互感知提示生成器，根据输入场景动态生成紧凑提示集，实现相似交互间的选择性共享，从而引导模型关注关键交互模式。2) 通过语言模型引导的概念校准来提炼HOI概念表示，通过调查跨类别的视觉相似性来区分不同的HOI概念。3) 采用负采样策略来改善跨模态相似性建模，使模型能更好地区分视觉相似但语义不同的动作。

Result: INP-CC在SWIG-HOI和HICO-DET数据集上显著优于最先进的模型。

Conclusion: INP-CC通过引入交互感知提示和概念校准，有效解决了开放词汇HOI检测中图像编码器次优和文本描述编码困难的问题，显著提升了模型在泛化到新交互类别上的性能。

Abstract: Open Vocabulary Human-Object Interaction (HOI) detection aims to detect
interactions between humans and objects while generalizing to novel interaction
classes beyond the training set. Current methods often rely on Vision and
Language Models (VLMs) but face challenges due to suboptimal image encoders, as
image-level pre-training does not align well with the fine-grained region-level
interaction detection required for HOI. Additionally, effectively encoding
textual descriptions of visual appearances remains difficult, limiting the
model's ability to capture detailed HOI relationships. To address these issues,
we propose INteraction-aware Prompting with Concept Calibration (INP-CC), an
end-to-end open-vocabulary HOI detector that integrates interaction-aware
prompts and concept calibration. Specifically, we propose an interaction-aware
prompt generator that dynamically generates a compact set of prompts based on
the input scene, enabling selective sharing among similar interactions. This
approach directs the model's attention to key interaction patterns rather than
generic image-level semantics, enhancing HOI detection. Furthermore, we refine
HOI concept representations through language model-guided calibration, which
helps distinguish diverse HOI concepts by investigating visual similarities
across categories. A negative sampling strategy is also employed to improve
inter-modal similarity modeling, enabling the model to better differentiate
visually similar but semantically distinct actions. Extensive experimental
results demonstrate that INP-CC significantly outperforms state-of-the-art
models on the SWIG-HOI and HICO-DET datasets. Code is available at
https://github.com/ltttpku/INP-CC.

</details>


### [114] [Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.03481)
*Hyungjin Kim,Seokho Ahn,Young-Duk Seo*

Main category: cs.CV

TL;DR: DrUM是一种新型方法，通过用户画像和基于Transformer的适配器，在潜在空间进行条件级建模，实现个性化T2I生成，解决现有方法精度不足和令牌容量限制问题。


<details>
  <summary>Details</summary>
Motivation: 现有T2I扩散模型的个性化生成主要依赖于提示级建模，受限于模型输入令牌容量，导致个性化不准确。

Method: 提出DrUM方法，将用户画像与基于Transformer的适配器相结合，在潜在空间实现条件级建模，从而进行个性化生成。

Result: DrUM在大规模数据集上表现出色，可无缝集成到开源文本编码器中，兼容主流T2I基础模型，无需额外微调。

Conclusion: DrUM通过在潜在空间进行条件级建模，有效解决了T2I扩散模型个性化生成中现有方法的局限性，实现了更准确且兼容性强的个性化生成。

Abstract: Personalized generation in T2I diffusion models aims to naturally incorporate
individual user preferences into the generation process with minimal user
intervention. However, existing studies primarily rely on prompt-level modeling
with large-scale models, often leading to inaccurate personalization due to the
limited input token capacity of T2I diffusion models. To address these
limitations, we propose DrUM, a novel method that integrates user profiling
with a transformer-based adapter to enable personalized generation through
condition-level modeling in the latent space. DrUM demonstrates strong
performance on large-scale datasets and seamlessly integrates with open-source
text encoders, making it compatible with widely used foundation T2I models
without requiring additional fine-tuning.

</details>


### [115] [GeoShield: Safeguarding Geolocation Privacy from Vision-Language Models via Adversarial Perturbations](https://arxiv.org/abs/2508.03209)
*Xinwei Liu,Xiaojun Jia,Yuan Xun,Simeng Qin,Xiaochun Cao*

Main category: cs.CV

TL;DR: 针对视觉-语言模型（VLMs）通过图像推断地理位置造成的隐私风险，本文提出了GeoShield，一个新颖的对抗性扰动框架，旨在提供鲁棒的地理隐私保护，且对图像质量影响最小。


<details>
  <summary>Details</summary>
Motivation: GPT-4o等VLMs能够从公开共享图像中推断用户位置，对地理隐私构成重大风险。现有对抗性防御方法在处理高分辨率图像和低扰动预算时表现不佳，且可能引入不相关的语义内容，因此需要更有效的解决方案。

Method: GeoShield框架包含三个关键模块：1. 特征解耦模块：分离地理和非地理信息；2. 暴露元素识别模块：识别图像中透露地理信息的区域；3. 尺度自适应增强模块：在全局和局部层面共同优化扰动，以确保跨分辨率的有效性。

Result: 在具有挑战性的基准测试中，GeoShield在黑盒设置下持续超越现有方法，实现了强大的隐私保护，同时对视觉或语义质量的影响最小。

Conclusion: GeoShield是首次探索使用对抗性扰动来防御先进VLMs地理定位推断的工作，为日益增长的隐私问题提供了一个实用且有效的解决方案。

Abstract: Vision-Language Models (VLMs) such as GPT-4o now demonstrate a remarkable
ability to infer users' locations from public shared images, posing a
substantial risk to geoprivacy. Although adversarial perturbations offer a
potential defense, current methods are ill-suited for this scenario: they often
perform poorly on high-resolution images and low perturbation budgets, and may
introduce irrelevant semantic content. To address these limitations, we propose
GeoShield, a novel adversarial framework designed for robust geoprivacy
protection in real-world scenarios. GeoShield comprises three key modules: a
feature disentanglement module that separates geographical and non-geographical
information, an exposure element identification module that pinpoints
geo-revealing regions within an image, and a scale-adaptive enhancement module
that jointly optimizes perturbations at both global and local levels to ensure
effectiveness across resolutions. Extensive experiments on challenging
benchmarks show that GeoShield consistently surpasses prior methods in
black-box settings, achieving strong privacy protection with minimal impact on
visual or semantic quality. To our knowledge, this work is the first to explore
adversarial perturbations for defending against geolocation inference by
advanced VLMs, providing a practical and effective solution to escalating
privacy concerns.

</details>


### [116] [FastInit: Fast Noise Initialization for Temporally Consistent Video Generation](https://arxiv.org/abs/2506.16119)
*Chengyu Bai,Yuming Li,Zhongyu Zhao,Jintao Chen,Peidong Jia,Qi She,Ming Lu,Shanghang Zhang*

Main category: cs.CV

TL;DR: FastInit提出了一种快速噪声初始化方法，通过学习一个视频噪声预测网络（VNPNet），在单次前向传递中生成高质量且时间一致的视频，解决了现有迭代精炼方法的计算开销问题。


<details>
  <summary>Details</summary>
Motivation: 视频生成中，扩散模型虽然取得了进展，但时间一致性仍是挑战。FreeInit通过迭代精炼初始噪声解决了训练-推理差距，但计算成本高昂，因此需要一种更高效的方法。

Method: FastInit引入了一个视频噪声预测网络（VNPNet），该网络以随机噪声和文本提示为输入，通过单次前向传递生成精炼的噪声。为了训练VNPNet，研究人员构建了一个包含文本提示、随机噪声和精炼噪声对的大规模数据集。

Result: 在多种文本到视频模型上的广泛实验表明，FastInit方法能持续提高生成视频的质量和时间一致性。它显著提升了视频生成效率，同时保持了高时间一致性。

Conclusion: FastInit不仅显著改进了视频生成效果，还提供了一个实用的解决方案，可以直接应用于推理阶段，无需迭代精炼，大幅提升了效率和实用性。

Abstract: Video generation has made significant strides with the development of
diffusion models; however, achieving high temporal consistency remains a
challenging task. Recently, FreeInit identified a training-inference gap and
introduced a method to iteratively refine the initial noise during inference.
However, iterative refinement significantly increases the computational cost
associated with video generation. In this paper, we introduce FastInit, a fast
noise initialization method that eliminates the need for iterative refinement.
FastInit learns a Video Noise Prediction Network (VNPNet) that takes random
noise and a text prompt as input, generating refined noise in a single forward
pass. Therefore, FastInit greatly enhances the efficiency of video generation
while achieving high temporal consistency across frames. To train the VNPNet,
we create a large-scale dataset consisting of pairs of text prompts, random
noise, and refined noise. Extensive experiments with various text-to-video
models show that our method consistently improves the quality and temporal
consistency of the generated videos. FastInit not only provides a substantial
improvement in video generation but also offers a practical solution that can
be applied directly during inference. The code and dataset will be released.

</details>


### [117] [Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching](https://arxiv.org/abs/2508.03562)
*Muzhaffar Hazman,Susan McKeever,Josephine Griffith*

Main category: cs.CV

TL;DR: 本文提出了一种超越模板匹配的模因匹配新方法，旨在解决现有方法仅限于基于模板的模因的局限性。研究测试了传统相似性度量（包括分段计算）和基于多模态大语言模型的提示方法，发现准确匹配非模板模因仍是一个开放挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的模因匹配方法大多假设模因由共享视觉背景（模板）和叠加文本组成，从而将匹配限制为仅比较背景图像。这排除了许多非模板模因，限制了自动化模因分析的有效性，并且无法有效地将模因与当代网络模因词典关联起来。

Method: 研究引入了一种更广泛的模因匹配公式，超越了模板匹配。测试了传统的相似性度量（包括新颖的分段计算方法），并探索了使用预训练多模态大语言模型（MLLM）的提示式方法进行模因匹配。

Result: 传统相似性度量（包括分段计算）在匹配基于模板的模因方面表现出色，但在应用于非模板模因时效果不佳。然而，分段方法在匹配非模板模因方面始终优于整图度量。结果表明，通过共享视觉元素（不仅仅是背景模板）准确匹配模因仍然是一个开放挑战，需要更复杂的匹配技术。

Conclusion: 要实现通过共享视觉元素（而不仅仅是背景模板）的准确模因匹配，需要开发更复杂的匹配技术。现有方法在处理非模板模因方面存在局限性，这是一个仍待解决的重要问题。

Abstract: Internet memes, now a staple of digital communication, play a pivotal role in
how users engage within online communities and allow researchers to gain
insight into contemporary digital culture. These engaging user-generated
content are characterised by their reuse of visual elements also found in other
memes. Matching instances of memes via these shared visual elements, called
Meme Matching, is the basis of a wealth of meme analysis approaches. However,
most existing methods assume that every meme consists of a shared visual
background, called a Template, with some overlaid text, thereby limiting meme
matching to comparing the background image alone. Current approaches exclude
the many memes that are not template-based and limit the effectiveness of
automated meme analysis and would not be effective at linking memes to
contemporary web-based meme dictionaries. In this work, we introduce a broader
formulation of meme matching that extends beyond template matching. We show
that conventional similarity measures, including a novel segment-wise
computation of the similarity measures, excel at matching template-based memes
but fall short when applied to non-template-based meme formats. However, the
segment-wise approach was found to consistently outperform the whole-image
measures on matching non-template-based memes. Finally, we explore a
prompting-based approach using a pretrained Multimodal Large Language Model for
meme matching. Our results highlight that accurately matching memes via shared
visual elements, not just background templates, remains an open challenge that
requires more sophisticated matching techniques.

</details>


### [118] [The Power of Many: Synergistic Unification of Diverse Augmentations for Efficient Adversarial Robustness](https://arxiv.org/abs/2508.03213)
*Wang Yu-Hang,Shiwei Li,Jianxiang Liao,Li Bohan,Jian Liu,Wenfei Yin*

Main category: cs.CV

TL;DR: 本文提出了一种名为通用对抗增强器（UAA）的新框架，通过离线预计算通用变换来高效生成对抗扰动，实现了数据增强型对抗防御的最新技术水平，解决了对抗训练成本高和现有数据增强效果有限的问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型面临对抗扰动的严重威胁。现有的主要防御方法对抗训练（AT）存在计算成本高和标准性能下降的问题。而现有的数据增强技术要么鲁棒性增益有限，要么训练开销巨大。因此，开发一种既高效又鲁棒的防御机制至关重要。

Method: 首先，系统分析了现有数据增强技术，发现多种策略的协同作用对于增强鲁棒性至关重要。在此基础上，提出了通用对抗增强器（UAA）框架，其特点是即插即用和训练高效。UAA通过离线预计算一个通用变换，将昂贵的扰动生成过程与模型训练解耦，然后在训练期间高效地为每个样本生成独特的对抗扰动。

Result: 在多个基准测试上的大量实验验证了UAA的有效性。结果表明，UAA在基于数据增强的对抗防御策略中建立了新的最先进水平（SOTA），并且无需在训练期间在线生成对抗样本。

Conclusion: UAA框架为构建鲁棒模型提供了一条实用且高效的途径。

Abstract: Adversarial perturbations pose a significant threat to deep learning models.
Adversarial Training (AT), the predominant defense method, faces challenges of
high computational costs and a degradation in standard performance. While data
augmentation offers an alternative path, existing techniques either yield
limited robustness gains or incur substantial training overhead. Therefore,
developing a defense mechanism that is both highly efficient and strongly
robust is of paramount importance.In this work, we first conduct a systematic
analysis of existing augmentation techniques, revealing that the synergy among
diverse strategies -- rather than any single method -- is crucial for enhancing
robustness. Based on this insight, we propose the Universal Adversarial
Augmenter (UAA) framework, which is characterized by its plug-and-play nature
and training efficiency. UAA decouples the expensive perturbation generation
process from model training by pre-computing a universal transformation
offline, which is then used to efficiently generate unique adversarial
perturbations for each sample during training.Extensive experiments conducted
on multiple benchmarks validate the effectiveness of UAA. The results
demonstrate that UAA establishes a new state-of-the-art (SOTA) for
data-augmentation-based adversarial defense strategies , without requiring the
online generation of adversarial examples during training. This framework
provides a practical and efficient pathway for building robust models,Our code
is available in the supplementary materials.

</details>


### [119] [ActionSink: Toward Precise Robot Manipulation with Dynamic Integration of Action Flow](https://arxiv.org/abs/2508.03218)
*Shanshan Guo,Xiwen Liang,Junfan Lin,Yuzheng Zhuang,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: ActionSink是一种新型机器人操作框架，通过将机器人动作重构为“动作流”（由动作引起的光流），并结合粗到精的匹配器和动态集成器，显著提高了低级动作估计的精度，从而提升了机器人操作性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型预训练模型在高级感知和规划方面取得了进展，但低级动作估计的精度不足已成为机器人操作性能的关键限制因素。

Method: ActionSink框架将机器人动作自监督地重构为视频中的“动作流”（action flow），并利用其进行动作估计。它包含两个主要模块：1) 粗到精的动作流匹配器，通过迭代检索和去噪连续优化动作流精度；2) 动态动作流集成器，利用工作记忆池动态管理历史动作流，并通过多层融合模块整合直接估计和来自当前及记忆的动作流，实现高精度动作估计。

Result: ActionSink框架在LIBERO基准测试上将成功率提高了7.9%，在更具挑战性的LIBERO-Long长程视觉任务上获得了近8%的精度提升，超越了先前的SOTA方法。

Conclusion: ActionSink通过引入“动作流”概念和创新的匹配与集成机制，成功解决了低级动作估计精度不足的问题，为基于学习的机器人操作提供了精确的动作估计方法，并显著提升了机器人操作任务的性能。

Abstract: Language-instructed robot manipulation has garnered significant interest due
to the potential of learning from collected data. While the challenges in
high-level perception and planning are continually addressed along the progress
of general large pre-trained models, the low precision of low-level action
estimation has emerged as the key limiting factor in manipulation performance.
To this end, this paper introduces a novel robot manipulation framework, i.e.,
ActionSink, to pave the way toward precise action estimations in the field of
learning-based robot manipulation. As the name suggests, ActionSink
reformulates the actions of robots as action-caused optical flows from videos,
called "action flow", in a self-supervised manner, which are then used to be
retrieved and integrated to enhance the action estimation. Specifically,
ActionSink incorporates two primary modules. The first module is a
coarse-to-fine action flow matcher, which continuously refines the accuracy of
action flow via iterative retrieval and denoising process. The second module is
a dynamic action flow integrator, which employs a working memory pool that
dynamically and efficiently manages the historical action flows that should be
used to integrate to enhance the current action estimation. In this module, a
multi-layer fusion module is proposed to integrate direct estimation and action
flows from both the current and the working memory, achieving highly accurate
action estimation through a series of estimation-integration processes. Our
ActionSink framework outperformed prior SOTA on the LIBERO benchmark by a 7.9\%
success rate, and obtained nearly an 8\% accuracy gain on the challenging
long-horizon visual task LIBERO-Long.

</details>


### [120] [Trace3D: Consistent Segmentation Lifting via Gaussian Instance Tracing](https://arxiv.org/abs/2508.03227)
*Hongyu Shen,Junfeng Ni,Yixin Chen,Weishuo Li,Mingtao Pei,Siyuan Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为高斯实例追踪（GIT）的方法，用于解决高斯泼溅中2D到3D语义分割的挑战，通过引入实例权重矩阵和自适应密度控制来纠正视图不一致性和改善分割边界。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将2D视觉分割提升到3D高斯泼溅时，存在跨视角的2D掩码不一致以及产生的分割边界噪声问题，因为它们忽略了语义线索来优化学习到的高斯。

Method: 该方法通过以下方式解决问题：1) 增强标准高斯表示，增加一个跨输入视图的实例权重矩阵（GIT）。2) 利用高斯固有的3D一致性，使用该矩阵识别和纠正2D分割不一致性。3) 提出一种GIT引导的自适应密度控制机制，在训练期间分割和修剪模糊的高斯，以获得更清晰、更连贯的2D和3D分割边界。

Result: 实验结果表明，该方法能够提取干净的3D资产，并在在线（如自提示）和离线（如对比提升）设置中持续改进3D分割，从而实现分层分割、对象提取和场景编辑等应用。

Conclusion: GIT方法有效解决了高斯泼溅中2D到3D分割的挑战，通过引入实例权重矩阵和自适应密度控制，显著提高了3D分割的一致性和边界质量，并拓展了相关应用。

Abstract: We address the challenge of lifting 2D visual segmentation to 3D in Gaussian
Splatting. Existing methods often suffer from inconsistent 2D masks across
viewpoints and produce noisy segmentation boundaries as they neglect these
semantic cues to refine the learned Gaussians. To overcome this, we introduce
Gaussian Instance Tracing (GIT), which augments the standard Gaussian
representation with an instance weight matrix across input views. Leveraging
the inherent consistency of Gaussians in 3D, we use this matrix to identify and
correct 2D segmentation inconsistencies. Furthermore, since each Gaussian
ideally corresponds to a single object, we propose a GIT-guided adaptive
density control mechanism to split and prune ambiguous Gaussians during
training, resulting in sharper and more coherent 2D and 3D segmentation
boundaries. Experimental results show that our method extracts clean 3D assets
and consistently improves 3D segmentation in both online (e.g., self-prompting)
and offline (e.g., contrastive lifting) settings, enabling applications such as
hierarchical segmentation, object extraction, and scene editing.

</details>


### [121] [Zero-shot Shape Classification of Nanoparticles in SEM Images using Vision Foundation Models](https://arxiv.org/abs/2508.03235)
*Freida Barnatan,Emunah Goldstein,Einav Kalimian,Orchen Madar,Avi Huri,David Zitoun,Ya'akov Mandelbaum,Moshe Amitay*

Main category: cs.CV

TL;DR: 本研究提出了一种零样本分类流程，利用SAM进行图像分割和DINOv2进行特征嵌入，实现了纳米颗粒形态的高精度分类，无需大量标记数据和计算资源，为纳米材料研究提供了高效且易用的自动化显微图像分析工具。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习方法在纳米颗粒形态分类中需要大量标注数据集和计算密集型训练，这限制了其在纳米颗粒研究和工业实践中的可及性。因此，需要一种更高效、更易于使用的替代方案。

Method: 该研究引入了一个零样本分类流程，结合了两个视觉基础模型：使用Segment Anything Model (SAM) 进行目标分割，以及DINOv2进行特征嵌入。随后，将这些特征与一个轻量级分类器结合，实现了纳米颗粒的形态分类。

Result: 该方法在三个形态多样化的纳米颗粒数据集上实现了高精度形状分类，且无需大量参数微调。它优于经过微调的YOLOv11和ChatGPT o4-mini-high基线模型，并表现出对小数据集、细微形态变化以及从自然图像到科学图像的领域转移的鲁棒性。此外，还讨论了DINOv2特征在PCA图上的聚类指标，可用于评估化学合成的进展。

Conclusion: 这项工作突出了基础模型在推进自动化显微镜图像分析方面的潜力，为纳米颗粒研究中传统的深度学习流程提供了一种更高效且更易于用户使用的替代方案。

Abstract: Accurate and efficient characterization of nanoparticle morphology in
Scanning Electron Microscopy (SEM) images is critical for ensuring product
quality in nanomaterial synthesis and accelerating development. However,
conventional deep learning methods for shape classification require extensive
labeled datasets and computationally demanding training, limiting their
accessibility to the typical nanoparticle practitioner in research and
industrial settings. In this study, we introduce a zero-shot classification
pipeline that leverages two vision foundation models: the Segment Anything
Model (SAM) for object segmentation and DINOv2 for feature embedding. By
combining these models with a lightweight classifier, we achieve high-precision
shape classification across three morphologically diverse nanoparticle datasets
- without the need for extensive parameter fine-tuning. Our methodology
outperforms a fine-tuned YOLOv11 and ChatGPT o4-mini-high baselines,
demonstrating robustness to small datasets, subtle morphological variations,
and domain shifts from natural to scientific imaging. Quantitative clustering
metrics on PCA plots of the DINOv2 features are discussed as a means of
assessing the progress of the chemical synthesis. This work highlights the
potential of foundation models to advance automated microscopy image analysis,
offering an alternative to traditional deep learning pipelines in nanoparticle
research which is both more efficient and more accessible to the user.

</details>


### [122] [FFHQ-Makeup: Paired Synthetic Makeup Dataset with Facial Consistency Across Multiple Styles](https://arxiv.org/abs/2508.03241)
*Xingchao Yang,Shiori Ueda,Yuantian Huang,Tomoya Akiyama,Takafumi Taketomi*

Main category: cs.CV

TL;DR: 本文提出了FFHQ-Makeup，一个高质量的合成素颜-妆容配对数据集，通过解耦身份和妆容，解决了现有数据集在真实感和一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 素颜-妆容配对图像对于虚拟试妆、面部隐私保护和面部美学分析等美容相关任务至关重要。然而，收集大规模高质量的真实配对图像非常困难，而现有合成方法常存在真实感不足或素颜与妆容图像之间不一致（如面部几何扭曲、身份和表情改变）的问题。

Method: 基于多样化的FFHQ数据集，本文引入了一种改进的妆容迁移方法，该方法能够解耦身份和妆容，将现有数据集中的真实妆容风格迁移到1.8万个身份上。每个身份与5种不同的妆容风格配对。

Result: 创建了FFHQ-Makeup数据集，包含总计9万对高质量的素颜-妆容图像对（1.8万个身份，每个身份5种妆容风格），同时保留了面部身份和表情的一致性。这是第一个专门专注于构建妆容数据集的工作。

Conclusion: FFHQ-Makeup数据集填补了高质量素颜-妆容配对数据集的空白，有望成为未来美容相关研究的宝贵资源。

Abstract: Paired bare-makeup facial images are essential for a wide range of
beauty-related tasks, such as virtual try-on, facial privacy protection, and
facial aesthetics analysis. However, collecting high-quality paired makeup
datasets remains a significant challenge. Real-world data acquisition is
constrained by the difficulty of collecting large-scale paired images, while
existing synthetic approaches often suffer from limited realism or
inconsistencies between bare and makeup images. Current synthetic methods
typically fall into two categories: warping-based transformations, which often
distort facial geometry and compromise the precision of makeup; and
text-to-image generation, which tends to alter facial identity and expression,
undermining consistency. In this work, we present FFHQ-Makeup, a high-quality
synthetic makeup dataset that pairs each identity with multiple makeup styles
while preserving facial consistency in both identity and expression. Built upon
the diverse FFHQ dataset, our pipeline transfers real-world makeup styles from
existing datasets onto 18K identities by introducing an improved makeup
transfer method that disentangles identity and makeup. Each identity is paired
with 5 different makeup styles, resulting in a total of 90K high-quality
bare-makeup image pairs. To the best of our knowledge, this is the first work
that focuses specifically on constructing a makeup dataset. We hope that
FFHQ-Makeup fills the gap of lacking high-quality bare-makeup paired datasets
and serves as a valuable resource for future research in beauty-related tasks.

</details>


### [123] [V.I.P. : Iterative Online Preference Distillation for Efficient Video Diffusion Models](https://arxiv.org/abs/2508.03254)
*Jisoo Kim,Wooseok Seo,Junwan Kim,Seungho Park,Sooyeon Park,Youngjae Yu*

Main category: cs.CV

TL;DR: 该研究提出ReDPO蒸馏方法和V.I.P.数据策展框架，旨在降低文生视频（T2V）模型的计算成本，同时避免模式崩溃并保持甚至超越原始模型的性能。


<details>
  <summary>Details</summary>
Motivation: 文生视频（T2V）模型计算成本高昂，难以在资源受限环境中部署。现有蒸馏方法（如SFT）常导致模式崩溃，因为剪枝后的模型难以直接匹配教师模型输出，从而导致质量下降。

Method: 提出ReDPO蒸馏方法，结合DPO（直接偏好优化）和SFT（监督微调），使学生模型专注于恢复目标属性而非被动模仿教师模型，同时提升整体性能。此外，引入V.I.P.框架用于筛选和整理高质量配对数据集，并提出分步在线校准训练方法。

Result: 在VideoCrafter2和AnimateDiff两种主流T2V模型上验证了方法有效性，分别实现了36.2%和67.5%的参数缩减，同时性能保持或超越了完整模型。实验证明ReDPO和V.I.P.框架均能有效实现高效高质量的视频生成。

Conclusion: ReDPO和V.I.P.框架的结合，为在资源受限环境中部署高效且高质量的文生视频模型提供了有效的解决方案，成功解决了现有蒸馏方法导致的模式崩溃和性能下降问题。

Abstract: With growing interest in deploying text-to-video (T2V) models in
resource-constrained environments, reducing their high computational cost has
become crucial, leading to extensive research on pruning and knowledge
distillation methods while maintaining performance. However, existing
distillation methods primarily rely on supervised fine-tuning (SFT), which
often leads to mode collapse as pruned models with reduced capacity fail to
directly match the teacher's outputs, ultimately resulting in degraded quality.
To address this challenge, we propose an effective distillation method, ReDPO,
that integrates DPO and SFT. Our approach leverages DPO to guide the student
model to focus on recovering only the targeted properties, rather than
passively imitating the teacher, while also utilizing SFT to enhance overall
performance. We additionally propose V.I.P., a novel framework for filtering
and curating high-quality pair datasets, along with a step-by-step online
approach for calibrated training. We validate our method on two leading T2V
models, VideoCrafter2 and AnimateDiff, achieving parameter reduction of 36.2%
and 67.5% each, while maintaining or even surpassing the performance of full
models. Further experiments demonstrate the effectiveness of both ReDPO and
V.I.P. framework in enabling efficient and high-quality video generation. Our
code and videos are available at https://jiiiisoo.github.io/VIP.github.io/.

</details>


### [124] [MVTOP: Multi-View Transformer-based Object Pose-Estimation](https://arxiv.org/abs/2508.03243)
*Lukas Ranftl,Felix Brendel,Bertram Drost,Carsten Steger*

Main category: cs.CV

TL;DR: MVTOP是一种基于Transformer的多视角刚体姿态估计方法，通过早期特征融合和视线建模解决单视角或后处理无法解决的姿态模糊问题，并在挑战性数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的单视角或基于单视角后处理的方法无法解决某些姿态模糊问题，需要一种能够可靠解决此类模糊的整体多视角方法。

Method: MVTOP是一种基于Transformer的方法，通过早期融合视点特定特征来解决姿态模糊。它通过从相机中心发出的视线来建模多视角几何，并假设相机内参和相对方向已知（但每次推理可变）。该模型是端到端可训练的，不需要额外数据（如深度）。作者还提供了一个新的合成数据集，其中姿态无法通过单视角解决，以展示模型能力。

Result: MVTOP能够解决单视角或单视角姿态后处理无法解决的姿态模糊。在作者提供的合成数据集上，该方法优于单视角和所有现有多视角方法，并在YCB-V数据集上取得了有竞争力的结果。

Conclusion: MVTOP是一种新颖、通用且可靠的整体多视角姿态估计方法，能有效解决复杂的姿态模糊问题，并超越了现有技术，为多视角姿态估计领域树立了新标准。

Abstract: We present MVTOP, a novel transformer-based method for multi-view rigid
object pose estimation. Through an early fusion of the view-specific features,
our method can resolve pose ambiguities that would be impossible to solve with
a single view or with a post-processing of single-view poses. MVTOP models the
multi-view geometry via lines of sight that emanate from the respective camera
centers. While the method assumes the camera interior and relative orientations
are known for a particular scene, they can vary for each inference. This makes
the method versatile. The use of the lines of sight enables MVTOP to correctly
predict the correct pose with the merged multi-view information. To show the
model's capabilities, we provide a synthetic data set that can only be solved
with such holistic multi-view approaches since the poses in the dataset cannot
be solved with just one view. Our method outperforms single-view and all
existing multi-view approaches on our dataset and achieves competitive results
on the YCB-V dataset. To the best of our knowledge, no holistic multi-view
method exists that can resolve such pose ambiguities reliably. Our model is
end-to-end trainable and does not require any additional data, e.g., depth.

</details>


### [125] [BaroPoser: Real-time Human Motion Tracking from IMUs and Barometers in Everyday Devices](https://arxiv.org/abs/2508.03313)
*Libo Zhang,Xinyu Yi,Feng Xu*

Main category: cs.CV

TL;DR: BaroPoser是首个结合智能手机和智能手表IMU与气压数据，实时估计人体姿态和非平坦地形全局位移的方法，显著优于现有仅使用IMU的方法。


<details>
  <summary>Details</summary>
Motivation: 现有使用IMU的运动追踪方法存在传感器测量稀疏、缺乏非平坦地形数据集的问题，导致姿态估计精度不足且通常仅限于平坦地形。因此，需要一种能有效处理非平坦地形运动追踪的方法。

Method: BaroPoser结合了智能手机和智能手表记录的IMU和气压数据。通过气压数据估计传感器高度变化，为姿态估计和非平坦地形全局位移预测提供线索。此外，引入局部大腿坐标系以解耦局部和全局运动输入，优化姿态表示学习。

Result: 在公共基准数据集和真实世界记录上的定量和定性评估表明，BaroPoser在相同硬件配置下，性能优于现有仅使用IMU的最先进方法。

Conclusion: BaroPoser是第一个利用IMU和气压数据实现实时人体姿态和非平坦地形全局位移估计的方法，通过整合气压信息和创新的局部坐标系，显著提升了运动追踪的准确性和适用性。

Abstract: In recent years, tracking human motion using IMUs from everyday devices such
as smartphones and smartwatches has gained increasing popularity. However, due
to the sparsity of sensor measurements and the lack of datasets capturing human
motion over uneven terrain, existing methods often struggle with pose
estimation accuracy and are typically limited to recovering movements on flat
terrain only. To this end, we present BaroPoser, the first method that combines
IMU and barometric data recorded by a smartphone and a smartwatch to estimate
human pose and global translation in real time. By leveraging barometric
readings, we estimate sensor height changes, which provide valuable cues for
both improving the accuracy of human pose estimation and predicting global
translation on non-flat terrain. Furthermore, we propose a local thigh
coordinate frame to disentangle local and global motion input for better pose
representation learning. We evaluate our method on both public benchmark
datasets and real-world recordings. Quantitative and qualitative results
demonstrate that our approach outperforms the state-of-the-art (SOTA) methods
that use IMUs only with the same hardware configuration.

</details>


### [126] [Ultralight Polarity-Split Neuromorphic SNN for Event-Stream Super-Resolution](https://arxiv.org/abs/2508.03244)
*Chuanzhi Xu,Haoxian Zhou,Langyi Chen,Yuk Ying Chung,Qiang Qu*

Main category: cs.CV

TL;DR: 本文提出一种基于脉冲神经网络（SNNs）的超轻量级、流式事件到事件超分辨率方法，专为资源受限设备实时部署设计。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、低延迟和高动态范围的优势，但其有限的空间分辨率限制了精细感知任务的应用。

Method: 该方法采用超轻量级、流式事件到事件超分辨率，基于脉冲神经网络（SNNs）。引入了新颖的双向极性分离事件编码策略，将正负事件分离到共享SNN的不同前向路径中以减小模型尺寸。此外，提出了一种可学习的时空极性感知损失（LearnSTPLoss），通过可学习的不确定性权重自适应平衡时间、空间和极性一致性。

Result: 实验结果表明，该方法在多个数据集上实现了有竞争力的超分辨率性能，同时显著减小了模型尺寸和推理时间。

Conclusion: 该轻量化设计使得模块能够嵌入到事件相机中，或作为下游视觉任务的高效前端预处理模块使用。

Abstract: Event cameras offer unparalleled advantages such as high temporal resolution,
low latency, and high dynamic range. However, their limited spatial resolution
poses challenges for fine-grained perception tasks. In this work, we propose an
ultra-lightweight, stream-based event-to-event super-resolution method based on
Spiking Neural Networks (SNNs), designed for real-time deployment on
resource-constrained devices. To further reduce model size, we introduce a
novel Dual-Forward Polarity-Split Event Encoding strategy that decouples
positive and negative events into separate forward paths through a shared SNN.
Furthermore, we propose a Learnable Spatio-temporal Polarity-aware Loss
(LearnSTPLoss) that adaptively balances temporal, spatial, and polarity
consistency using learnable uncertainty-based weights. Experimental results
demonstrate that our method achieves competitive super-resolution performance
on multiple datasets while significantly reducing model size and inference
time. The lightweight design enables embedding the module into event cameras or
using it as an efficient front-end preprocessing for downstream vision tasks.

</details>


### [127] [SCFlow: Implicitly Learning Style and Content Disentanglement with Flow Models](https://arxiv.org/abs/2508.03402)
*Pingchuan Ma,Xiaopei Yang,Yusong Li,Ming Gui,Felix Krause,Johannes Schusterbauer,Björn Ommer*

Main category: cs.CV

TL;DR: SCFlow提出了一种流匹配框架，通过学习风格和内容的可逆合并来自然实现解缠，避免了传统显式解缠的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于语义重叠和人类感知的主观性，在视觉模型中显式解缠风格和内容仍然具有挑战性。现有方法面临概念交织的固有模糊性。本文旨在探索是否可以通过学习可逆地合并风格和内容来绕过显式解缠，从而使分离自然地出现。

Method: 本文提出了SCFlow，一个流匹配框架，用于学习缠绕和解缠表示之间的双向映射。其核心思想包括：1) 仅训练合并风格和内容（一个明确定义的任务）即可实现无需显式监督的可逆解缠；2) 流匹配可以在任意分布上进行，避免了扩散模型和归一化流的限制性高斯先验；3) 构建了一个包含51万个样本（51种风格 × 1万个内容样本）的合成数据集，通过系统性的风格-内容配对来模拟解缠。

Result: SCFlow在可控生成任务之外，还展示了其在零样本设置下对ImageNet-1k和WikiArt的泛化能力，并取得了有竞争力的性能，这突出表明解缠自然地从可逆合并过程中涌现出来。

Conclusion: 通过学习风格和内容的可逆合并，可以自然地实现解缠，这为解决风格与内容解缠的难题提供了一种新颖有效的方法。

Abstract: Explicitly disentangling style and content in vision models remains
challenging due to their semantic overlap and the subjectivity of human
perception. Existing methods propose separation through generative or
discriminative objectives, but they still face the inherent ambiguity of
disentangling intertwined concepts. Instead, we ask: Can we bypass explicit
disentanglement by learning to merge style and content invertibly, allowing
separation to emerge naturally? We propose SCFlow, a flow-matching framework
that learns bidirectional mappings between entangled and disentangled
representations. Our approach is built upon three key insights: 1) Training
solely to merge style and content, a well-defined task, enables invertible
disentanglement without explicit supervision; 2) flow matching bridges on
arbitrary distributions, avoiding the restrictive Gaussian priors of diffusion
models and normalizing flows; and 3) a synthetic dataset of 510,000 samples (51
styles $\times$ 10,000 content samples) was curated to simulate disentanglement
through systematic style-content pairing. Beyond controllable generation tasks,
we demonstrate that SCFlow generalizes to ImageNet-1k and WikiArt in zero-shot
settings and achieves competitive performance, highlighting that
disentanglement naturally emerges from the invertible merging process.

</details>


### [128] [Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion](https://arxiv.org/abs/2508.03252)
*Wentao Qu,Guofeng Mei,Jing Wang,Yujiao Wu,Xiaoshui Huang,Liang Xiao*

Main category: cs.CV

TL;DR: RSDNet是一个鲁棒的单阶段全稀疏3D目标检测网络，它利用可分离的潜空间去噪扩散模型（DDPMs），通过轻量级去噪网络和语义几何条件引导，实现了高效且最先进的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于DDPM的3D目标检测方法通常依赖于多步迭代推理，效率低下，这限制了它们在实际应用中的潜力。

Method: 本文提出了RSDNet，一个带有可分离潜空间框架（DLF）的DDPMs。具体方法包括：1) 在潜特征空间中通过多级去噪自编码器（DAEs）学习去噪过程；2) 重新设计DDPM的加噪和去噪机制，以构建多类型、多级噪声样本和目标，增强对多扰动的鲁棒性；3) 引入语义几何条件引导，解决稀疏表示中的中心特征缺失问题，实现全稀疏检测；4) DLF的可分离去噪网络设计使得单步推理成为可能，提升检测效率。

Result: 在公共基准测试上的大量实验表明，RSDNet性能优于现有方法，达到了最先进的检测水平。

Conclusion: RSDNet成功地将DDPM的鲁棒性与单阶段、全稀疏和高效的特性结合起来，为3D目标检测提供了一个高性能的解决方案。

Abstract: Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust
3D object detection tasks. Existing methods often rely on the score matching
from 3D boxes or pre-trained diffusion priors. However, they typically require
multi-step iterations in inference, which limits efficiency. To address this,
we propose a \textbf{R}obust single-stage fully \textbf{S}parse 3D object
\textbf{D}etection \textbf{Net}work with a Detachable Latent Framework (DLF) of
DDPMs, named RSDNet. Specifically, RSDNet learns the denoising process in
latent feature spaces through lightweight denoising networks like multi-level
denoising autoencoders (DAEs). This enables RSDNet to effectively understand
scene distributions under multi-level perturbations, achieving robust and
reliable detection. Meanwhile, we reformulate the noising and denoising
mechanisms of DDPMs, enabling DLF to construct multi-type and multi-level noise
samples and targets, enhancing RSDNet robustness to multiple perturbations.
Furthermore, a semantic-geometric conditional guidance is introduced to
perceive the object boundaries and shapes, alleviating the center feature
missing problem in sparse representations, enabling RSDNet to perform in a
fully sparse detection pipeline. Moreover, the detachable denoising network
design of DLF enables RSDNet to perform single-step detection in inference,
further enhancing detection efficiency. Extensive experiments on public
benchmarks show that RSDNet can outperform existing methods, achieving
state-of-the-art detection.

</details>


### [129] [Visual Document Understanding and Question Answering: A Multi-Agent Collaboration Framework with Test-Time Scaling](https://arxiv.org/abs/2508.03404)
*Xinlei Yu,Zhangquan Chen,Yudong Zhang,Shilin Lu,Ruolin Shen,Jiangning Zhang,Xiaobin Hu,Yanwei Fu,Shuicheng Yan*

Main category: cs.CV

TL;DR: MACT是一个多智能体协作框架，通过规划、执行、判断和回答智能体，并结合测试时缩放和混合奖励模型，显著提升了视觉文档理解和视觉问答性能，尤其在长视觉上下文和复杂推理任务中表现出色，且参数规模更小。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLM）受参数规模限制，缺乏鲁棒的自我纠正能力，且在涉及长视觉上下文和复杂推理的任务（特别是文档任务）中表现不佳。

Method: 提出MACT框架，包含四个小型智能体：规划、执行、判断和回答。判断智能体专门负责验证和重定向以进行修正。此外，引入混合奖励建模以平衡智能体特定能力和全局协作，并采用智能体级混合测试时缩放策略，根据各智能体功能定制缩放方案。

Result: MACT在文档和非文档基准测试中均表现出卓越性能，参数规模更小，且不牺牲通用和数学任务能力。在涉及长视觉上下文和复杂推理的基准测试中尤为突出。MACT的三个变体在平均得分中稳居前三，并在15个基准测试中的13个中领先。

Conclusion: MACT通过其独特的多智能体协作框架、强大的自我纠正机制和灵活的测试时缩放策略，有效克服了现有VLM的局限性，在视觉文档理解和视觉问答任务上取得了显著的性能提升，尤其是在处理复杂和长上下文视觉信息时表现优异。

Abstract: Existing vision-language models (VLMs), whether generalists or specialists,
remain constrained by their parameter scale, lack robust self-correction
capabilities, and underperform in tasks involving long visual contexts and
complex reasoning, resulting in suboptimal performance on document-based tasks.
To address this, we propose MACT, a Multi-Agent Collaboration framework with
Test-Time scaling, tailored for visual document understanding and visual
question answering (VQA). It comprises four distinct small-scale agents, i.e.,
planning, execution, judgment, and answer agents, with clearly defined roles
and effective collaboration. Notably, the judgment agent exclusively verifies
correctness and redirects to prior agents for revisions, outperforming
conventional correction strategies. To further expand the capability boundaries
of the framework, we propose mixed reward modeling that balances agent-specific
abilities and global collaboration, as well as agent-wise hybrid test-time
scaling, which customizes different scaling strategies for each agent based on
their functions. Evaluated on benchmarks spanning both document-based and
non-document-based settings, our MACT shows superior performance with a smaller
parameter scale without sacrificing the ability of general and mathematical
tasks. Especially, it stands out in benchmarks involving long visual contexts
and complicated reasoning. The three variants of MACT consistently hold the top
three positions in average scores, leading in 13 of the 15 benchmarks. Code
will be available at: https://github.com/YU-deep/MACT.git.

</details>


### [130] [Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line Generation](https://arxiv.org/abs/2508.03256)
*Gang Dai,Yifan Zhang,Yutao Qin,Qiangya Guo,Shuangping Huang,Shuicheng Yan*

Main category: cs.CV

TL;DR: DiffBrush是一种新颖的基于扩散模型，用于生成手写文本行，它通过内容解耦的风格学习和多尺度内容学习，在风格模仿和内容准确性方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有手写文本生成方法主要关注孤立的单词，但真实手写文本不仅需要考虑单个单词，还需要关注它们之间的关系（如垂直对齐和水平间距）。生成整个文本行更具挑战性，需要准确建模包含词内和词间关系的复杂风格模式，并保持多字符的内容准确性。

Method: 本文提出了DiffBrush，一个基于扩散模型的手写文本行生成模型。它采用两种关键策略：1) 内容解耦的风格学习，通过列向和行向掩码将风格与内容分离，以更好地捕捉词内和词间风格模式；2) 多尺度内容学习，利用行和词判别器确保文本内容的全局连贯性和局部准确性。

Result: 广泛的实验表明，DiffBrush在生成高质量文本行方面表现出色，特别是在风格再现和内容保持方面。

Conclusion: DiffBrush模型有效解决了手写文本行生成中的复杂风格模式建模和内容准确性维护的挑战，在风格模仿和内容准确性方面均取得了显著成效。

Abstract: Existing handwritten text generation methods primarily focus on isolated
words. However, realistic handwritten text demands attention not only to
individual words but also to the relationships between them, such as vertical
alignment and horizontal spacing. Therefore, generating entire text lines
emerges as a more promising and comprehensive task. However, this task poses
significant challenges, including the accurate modeling of complex style
patterns encompassing both intra- and inter-word relationships, and maintaining
content accuracy across numerous characters. To address these challenges, we
propose DiffBrush, a novel diffusion-based model for handwritten text-line
generation. Unlike existing methods, DiffBrush excels in both style imitation
and content accuracy through two key strategies: (1) content-decoupled style
learning, which disentangles style from content to better capture intra-word
and inter-word style patterns by using column- and row-wise masking; and (2)
multi-scale content learning, which employs line and word discriminators to
ensure global coherence and local accuracy of textual content. Extensive
experiments show that DiffBrush excels in generating high-quality text lines,
particularly in style reproduction and content preservation. Code is available
at https://github.com/dailenson/DiffBrush.

</details>


### [131] [SlotMatch: Distilling Temporally Consistent Object-Centric Representations for Unsupervised Video Segmentation](https://arxiv.org/abs/2508.03411)
*Diana-Nicoleta Grigore,Neelu Madan,Andreas Mogelmose,Thomas B. Moeslund,Radu Tudor Ionescu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SlotMatch的简单知识蒸馏框架，能将无监督视频分割中基于槽位注意力的大型模型（教师）的物体中心表示有效迁移到轻量级学生模型，在性能匹配甚至超越教师的同时显著减少参数量并提高运行速度。


<details>
  <summary>Details</summary>
Motivation: 无监督视频分割是一个具有挑战性的任务，缺乏监督信号且视觉场景复杂。现有的最先进模型（如基于槽位注意力的模型）往往需要庞大且计算昂贵的神经网络架构。

Method: 本文提出了一个名为SlotMatch的知识蒸馏框架。该框架通过余弦相似度对齐教师模型和学生模型中对应的槽位，无需额外的蒸馏目标或辅助监督。理论和经验证据表明，集成额外损失是多余的。

Result: 实验结果显示，基于SlotMatch的学生模型在性能上与最先进的教师模型SlotContrast匹配甚至超越，同时使用的参数量减少了3.6倍，运行速度提升了1.9倍。此外，该学生模型也超越了以往的无监督视频分割模型。

Conclusion: SlotMatch框架证明了通过简单的知识蒸馏，可以有效地将物体中心表示从大型模型迁移到轻量级模型，从而在无监督视频分割任务中实现更高的效率和竞争力，而无需额外的复杂性。

Abstract: Unsupervised video segmentation is a challenging computer vision task,
especially due to the lack of supervisory signals coupled with the complexity
of visual scenes. To overcome this challenge, state-of-the-art models based on
slot attention often have to rely on large and computationally expensive neural
architectures. To this end, we propose a simple knowledge distillation
framework that effectively transfers object-centric representations to a
lightweight student. The proposed framework, called SlotMatch, aligns
corresponding teacher and student slots via the cosine similarity, requiring no
additional distillation objectives or auxiliary supervision. The simplicity of
SlotMatch is confirmed via theoretical and empirical evidence, both indicating
that integrating additional losses is redundant. We conduct experiments on two
datasets to compare the state-of-the-art teacher model, SlotContrast, with our
distilled student. The results show that our student based on SlotMatch matches
and even outperforms its teacher, while using 3.6x less parameters and running
1.9x faster. Moreover, our student surpasses previous unsupervised video
segmentation models.

</details>


### [132] [EgoPrompt: Prompt Pool Learning for Egocentric Action Recognition](https://arxiv.org/abs/2508.03266)
*Huaihai Lyu,Chaofan Chen,Yuheng Ji,Changsheng Xu*

Main category: cs.CV

TL;DR: 针对第一视角动作识别中动词和名词组件独立处理的问题，本文提出EgoPrompt框架，通过提示学习和统一提示池空间，融合动词和名词表示，实现组件间交互，显著提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 增强现实和虚拟现实应用对第一视角动作识别的需求日益增长。现有方法将动作识别的动词（行为）和名词（对象）组件视为独立的分类任务，忽略了它们固有的语义和上下文关系，导致表示碎片化和泛化能力不足。

Method: 本文提出了基于提示学习的EgoPrompt框架。该框架在现有提示策略基础上，构建了一个统一提示池空间，以建立动词和名词两种组件表示之间的交互。具体而言，组件表示首先被分解为提示对形式的细粒度模式，然后通过基于注意力的机制融合这些模式级表示，以促进跨组件交互。为确保提示池的信息量，进一步引入了新颖的训练目标——多样化池标准（Diverse Pool Criteria），该目标从提示选择频率正则化和提示知识正交化两个角度实现。

Result: 在Ego4D、EPIC-Kitchens和EGTEA数据集上进行了广泛实验。结果一致表明，EgoPrompt在数据集内、跨数据集以及从基础到新颖的泛化基准上都取得了最先进的性能。

Conclusion: EgoPrompt通过统一处理第一视角动作识别中的动词和名词组件，并建立它们之间的语义和上下文关系，有效解决了现有方法的局限性。其提出的提示学习框架和多样化提示池机制，显著提升了模型的泛化能力，达到了当前最优水平。

Abstract: Driven by the increasing demand for applications in augmented and virtual
reality, egocentric action recognition has emerged as a prominent research
area. It is typically divided into two subtasks: recognizing the performed
behavior (i.e., verb component) and identifying the objects being acted upon
(i.e., noun component) from the first-person perspective. However, most
existing approaches treat these two components as independent classification
tasks, focusing on extracting component-specific knowledge while overlooking
their inherent semantic and contextual relationships, leading to fragmented
representations and sub-optimal generalization capability. To address these
challenges, we propose a prompt learning-based framework, EgoPrompt, to conduct
the egocentric action recognition task. Building on the existing prompting
strategy to capture the component-specific knowledge, we construct a Unified
Prompt Pool space to establish interaction between the two types of component
representations. Specifically, the component representations (from verbs and
nouns) are first decomposed into fine-grained patterns with the prompt pair
form. Then, these pattern-level representations are fused through an
attention-based mechanism to facilitate cross-component interaction. To ensure
the prompt pool is informative, we further introduce a novel training
objective, Diverse Pool Criteria. This objective realizes our goals from two
perspectives: Prompt Selection Frequency Regularization and Prompt Knowledge
Orthogonalization. Extensive experiments are conducted on the Ego4D,
EPIC-Kitchens, and EGTEA datasets. The results consistently show that EgoPrompt
achieves state-of-the-art performance across within-dataset, cross-dataset, and
base-to-novel generalization benchmarks.

</details>


### [133] [Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN](https://arxiv.org/abs/2508.03415)
*Shivangi Nigam,Adarsh Prasad Behera,Shekhar Verma,P. Nagabhushan*

Main category: cs.CV

TL;DR: Fd-CycleGAN是一个改进的图像到图像翻译框架，通过引入局部邻域编码和频率感知监督，并使用分布损失，实现了更优的感知质量、更快的收敛和更好的模式多样性，尤其在数据量较少时表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有图像到图像翻译方法在潜在表示学习和逼近真实数据分布方面存在不足，导致生成图像的局部语义和结构一致性欠佳。研究旨在提高生成图像的感知质量、收敛速度和模式多样性。

Method: Fd-CycleGAN在CycleGAN基础上，整合了局部邻域编码（LNE）和频率感知监督，以捕获细粒度的局部像素语义并保持结构连贯性。采用基于分布的损失度量（如KL/JS散度和基于对数的相似度）来量化真实和生成图像在空间和频率域的分布对齐。同时，与扩散模型进行了比较，以突出其轻量级对抗方法的优势。

Result: Fd-CycleGAN在Horse2Zebra、Monet2Photo和合成Strike-off数据集上进行了实验，结果表明其在感知质量、收敛速度和模式多样性方面优于基线CycleGAN和其他SOTA方法，尤其在低数据量情况下表现更佳。实现了视觉上更连贯、语义上更一致的翻译。

Conclusion: 频率引导的潜在学习显著改善了图像翻译任务的泛化能力，在文档修复、艺术风格迁移和医学图像合成等领域具有广阔应用前景。Fd-CycleGAN作为一种轻量级对抗方法，在训练效率和定性输出方面优于扩散生成模型。

Abstract: This paper presents Fd-CycleGAN, an image-to-image (I2I) translation
framework that enhances latent representation learning to approximate real data
distributions. Building upon the foundation of CycleGAN, our approach
integrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to
capture fine-grained local pixel semantics while preserving structural
coherence from the source domain. We employ distribution-based loss metrics,
including KL/JS divergence and log-based similarity measures, to explicitly
quantify the alignment between real and generated image distributions in both
spatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we
conduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a
synthetically augmented Strike-off dataset. Compared to baseline CycleGAN and
other state-of-the-art methods, our approach demonstrates superior perceptual
quality, faster convergence, and improved mode diversity, particularly in
low-data regimes. By effectively capturing local and global distribution
characteristics, Fd-CycleGAN achieves more visually coherent and semantically
consistent translations. Our results suggest that frequency-guided latent
learning significantly improves generalization in image translation tasks, with
promising applications in document restoration, artistic style transfer, and
medical image synthesis. We also provide comparative insights with
diffusion-based generative models, highlighting the advantages of our
lightweight adversarial approach in terms of training efficiency and
qualitative output.

</details>


### [134] [Efficient Multi-Slide Visual-Language Feature Fusion for Placental Disease Classification](https://arxiv.org/abs/2508.03277)
*Hang Guo,Qing Zhang,Zixuan Gao,Siyuan Yang,Shulin Peng,Xiang Tao,Ting Yu,Yan Wang,Qingli Li*

Main category: cs.CV

TL;DR: 针对胎盘疾病WSI分析中存在的计算挑战和全局上下文丢失问题，本文提出了EmmPD框架，通过两阶段补丁选择和混合多模态融合（结合自适应图学习和文本报告）实现了最先进的诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有WSI分类方法在胎盘疾病诊断中存在局限性：1) 补丁选择策略不足，导致性能或计算效率受损；2) 补丁级处理导致全局组织学上下文丢失。

Method: 本文提出了EmmPD框架：1) 两阶段补丁选择模块，结合无参数和可学习压缩策略，平衡计算效率与关键特征保留；2) 混合多模态融合模块，利用自适应图学习增强病理特征表示，并整合文本医学报告以丰富全局上下文理解。

Result: 在自建的患者级胎盘数据集和两个公共数据集上进行了广泛实验，结果表明该方法实现了最先进的诊断性能。

Conclusion: EmmPD框架通过创新的补丁选择和多模态融合策略，有效解决了WSI分析中的计算挑战和上下文丢失问题，在胎盘疾病诊断中展现出卓越的性能。

Abstract: Accurate prediction of placental diseases via whole slide images (WSIs) is
critical for preventing severe maternal and fetal complications. However, WSI
analysis presents significant computational challenges due to the massive data
volume. Existing WSI classification methods encounter critical limitations: (1)
inadequate patch selection strategies that either compromise performance or
fail to sufficiently reduce computational demands, and (2) the loss of global
histological context resulting from patch-level processing approaches. To
address these challenges, we propose an Efficient multimodal framework for
Patient-level placental disease Diagnosis, named EmmPD. Our approach introduces
a two-stage patch selection module that combines parameter-free and learnable
compression strategies, optimally balancing computational efficiency with
critical feature preservation. Additionally, we develop a hybrid multimodal
fusion module that leverages adaptive graph learning to enhance pathological
feature representation and incorporates textual medical reports to enrich
global contextual understanding. Extensive experiments conducted on both a
self-constructed patient-level Placental dataset and two public datasets
demonstrating that our method achieves state-of-the-art diagnostic performance.
The code is available at https://github.com/ECNU-MultiDimLab/EmmPD.

</details>


### [135] [R2GenKG: Hierarchical Multi-modal Knowledge Graph for LLM-based Radiology Report Generation](https://arxiv.org/abs/2508.03426)
*Futian Wang,Yuhan Qiao,Xiao Wang,Fuling Wang,Yuxiang Zhang,Dengdi Sun*

Main category: cs.CV

TL;DR: 本文构建了一个大规模多模态医学知识图谱（M3KG），并将其与视觉特征结合，通过大语言模型生成X射线医学报告，以解决现有方法中幻觉和诊断能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大基础模型显著提升了医学报告生成质量，但在X射线医学报告生成中，仍然存在幻觉（hallucination）和疾病诊断能力弱的挑战。

Method: 首先，利用GPT-4o从真实医学报告构建了一个大规模多模态医学知识图谱（M3KG），包含实体、关系、三元组和疾病感知视觉token。接着，对M3KG进行采样以获得多粒度语义图，并使用R-GCN编码器提取特征。对于输入的X射线图像，采用Swin-Transformer提取视觉特征，并通过交叉注意力与知识交互。视觉token通过Q-former和另一个交叉注意力检索疾病感知视觉token。最后，使用大语言模型将语义知识图谱、输入X射线图像和疾病感知视觉token映射为语言描述。

Result: 在多个数据集上进行的广泛实验充分验证了所提出的知识图谱和X射线报告生成框架的有效性。

Conclusion: 本文提出的基于知识图谱和多模态交互的X射线报告生成框架，有效提升了报告质量，解决了现有模型中幻觉和诊断能力不足的问题。

Abstract: X-ray medical report generation is one of the important applications of
artificial intelligence in healthcare. With the support of large foundation
models, the quality of medical report generation has significantly improved.
However, challenges such as hallucination and weak disease diagnostic
capability still persist. In this paper, we first construct a large-scale
multi-modal medical knowledge graph (termed M3KG) based on the ground truth
medical report using the GPT-4o. It contains 2477 entities, 3 kinds of
relations, 37424 triples, and 6943 disease-aware vision tokens for the CheXpert
Plus dataset. Then, we sample it to obtain multi-granularity semantic graphs
and use an R-GCN encoder for feature extraction. For the input X-ray image, we
adopt the Swin-Transformer to extract the vision features and interact with the
knowledge using cross-attention. The vision tokens are fed into a Q-former and
retrieved the disease-aware vision tokens using another cross-attention.
Finally, we adopt the large language model to map the semantic knowledge graph,
input X-ray image, and disease-aware vision tokens into language descriptions.
Extensive experiments on multiple datasets fully validated the effectiveness of
our proposed knowledge graph and X-ray report generation framework. The source
code of this paper will be released on
https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [136] [Zero Shot Domain Adaptive Semantic Segmentation by Synthetic Data Generation and Progressive Adaptation](https://arxiv.org/abs/2508.03300)
*Jun Luo,Zijing Zhao,Yang Liu*

Main category: cs.CV

TL;DR: SDGPA提出了一种零样本域适应语义分割方法，通过文本到图像扩散模型生成合成数据并采用渐进式适应策略来应对目标域无图像的挑战。


<details>
  <summary>Details</summary>
Motivation: 深度学习语义分割模型在训练和测试数据之间存在分布偏移时表现受限。在零样本域适应场景下，目标域没有可用的图像，仅提供文本描述，这进一步增加了挑战。

Method: 1. 利用预训练的文本到图像扩散模型，将源域图像转换为目标风格以生成训练图像。2. 针对合成数据中布局不准确的问题，提出裁剪源图像、单独编辑小块并重新合并的方法，以提高空间精度。3. 构建一个增强的中间域，通过简化适应子任务来稳定模型向目标域的适应。4. 设计渐进式适应策略，以减轻合成数据中噪声的影响，确保训练过程中的鲁棒学习。

Result: 该方法在零样本语义分割中取得了最先进的性能。

Conclusion: SDGPA通过创新的合成数据生成和渐进式适应策略，有效解决了零样本域适应语义分割的难题，即使在目标域仅有文本描述的情况下也能实现高性能表现。

Abstract: Deep learning-based semantic segmentation models achieve impressive results
yet remain limited in handling distribution shifts between training and test
data. In this paper, we present SDGPA (Synthetic Data Generation and
Progressive Adaptation), a novel method that tackles zero-shot domain adaptive
semantic segmentation, in which no target images are available, but only a text
description of the target domain's style is provided. To compensate for the
lack of target domain training data, we utilize a pretrained off-the-shelf
text-to-image diffusion model, which generates training images by transferring
source domain images to target style. Directly editing source domain images
introduces noise that harms segmentation because the layout of source images
cannot be precisely maintained. To address inaccurate layouts in synthetic
data, we propose a method that crops the source image, edits small patches
individually, and then merges them back together, which helps improve spatial
precision. Recognizing the large domain gap, SDGPA constructs an augmented
intermediate domain, leveraging easier adaptation subtasks to enable more
stable model adaptation to the target domain. Additionally, to mitigate the
impact of noise in synthetic data, we design a progressive adaptation strategy,
ensuring robust learning throughout the training process. Extensive experiments
demonstrate that our method achieves state-of-the-art performance in zero-shot
semantic segmentation. The code is available at
https://github.com/ROUJINN/SDGPA

</details>


### [137] [Spatial Imputation Drives Cross-Domain Alignment for EEG Classification](https://arxiv.org/abs/2508.03437)
*Hongjun Liu,Chao Yao,Yalan Zhang,Xiaokun wang,Xiaojuan Ban*

Main category: cs.CV

TL;DR: IMAC是一种新颖的基于通道依赖掩码和插补的自监督框架，通过将跨域EEG数据对齐转化为空间时间序列插补任务，有效解决了EEG分类中因异构电极配置、采集协议和硬件差异导致的数据分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: EEG信号分类面临挑战，原因在于不同域（如异构电极配置、采集协议、硬件差异）导致的数据分布偏移。现有方法难以有效处理这些跨域差异。

Method: IMAC框架首先采用3D到2D的位置统一映射策略，标准化不同电极布局以建立统一空间表示。然后，它引入通道依赖掩码和重建任务，将其构建为低分辨率到高分辨率的EEG空间插补问题，模拟跨域变化（如通道遗漏和时间不稳定性）。此外，IMAC采用解耦结构，独立建模EEG信号的时间和空间信息，以降低计算复杂性并增强灵活性和适应性。

Result: 在10个公开EEG数据集上的综合评估表明，IMAC在跨受试者和跨中心验证场景中均实现了最先进的分类精度。值得注意的是，IMAC在模拟和真实世界分布偏移下均表现出强大的鲁棒性，完整性分数比基线方法高出35%，同时保持了稳定的分类精度。

Conclusion: IMAC框架通过创新的空间时间序列插补方法，有效解决了跨域EEG数据对齐的挑战，显著提升了EEG分类的准确性和鲁棒性，在多种分布偏移条件下表现出色。

Abstract: Electroencephalogram (EEG) signal classification faces significant challenges
due to data distribution shifts caused by heterogeneous electrode
configurations, acquisition protocols, and hardware discrepancies across
domains. This paper introduces IMAC, a novel channel-dependent mask and
imputation self-supervised framework that formulates the alignment of
cross-domain EEG data shifts as a spatial time series imputation task. To
address heterogeneous electrode configurations in cross-domain scenarios, IMAC
first standardizes different electrode layouts using a 3D-to-2D positional
unification mapping strategy, establishing unified spatial representations.
Unlike previous mask-based self-supervised representation learning methods,
IMAC introduces spatio-temporal signal alignment. This involves constructing a
channel-dependent mask and reconstruction task framed as a low-to-high
resolution EEG spatial imputation problem. Consequently, this approach
simulates cross-domain variations such as channel omissions and temporal
instabilities, thus enabling the model to leverage the proposed imputer for
robust signal alignment during inference. Furthermore, IMAC incorporates a
disentangled structure that separately models the temporal and spatial
information of the EEG signals separately, reducing computational complexity
while enhancing flexibility and adaptability. Comprehensive evaluations across
10 publicly available EEG datasets demonstrate IMAC's superior performance,
achieving state-of-the-art classification accuracy in both cross-subject and
cross-center validation scenarios. Notably, IMAC shows strong robustness under
both simulated and real-world distribution shifts, surpassing baseline methods
by up to $35$\% in integrity scores while maintaining consistent classification
accuracy.

</details>


### [138] [Architectural Insights into Knowledge Distillation for Object Detection: A Comprehensive Review](https://arxiv.org/abs/2508.03317)
*Mahdi Golizadeh,Nassibeh Golizadeh,Mohammad Ali Keyvanrad,Hossein Shirazi*

Main category: cs.CV

TL;DR: 该综述提出了一种以架构为中心的知识蒸馏（KD）分类法，用于目标检测，涵盖CNN和Transformer模型，并评估了代表性方法，旨在指导高效可扩展检测系统的未来研究。


<details>
  <summary>Details</summary>
Motivation: 深度学习使目标检测精度显著提升，但计算成本也随之增加，限制了其在资源受限设备上的部署。知识蒸馏（KD）是有效解决方案，但将其应用于目标检测面临独特挑战，如分类与定位双重目标、前景-背景不平衡以及多尺度特征表示。

Method: 引入了一种新颖的、以架构为中心的KD方法分类法：针对CNN-based检测器，分为骨干级、颈部级、头部级和RPN/RoI级蒸馏；针对Transformer-based检测器，包括查询级、特征级和logit级蒸馏。此外，使用MS COCO和PASCAL VOC数据集，以mAP@0.5为性能指标，评估了代表性方法并进行了比较分析。

Result: 通过提出的分类法和评估，对各种代表性KD方法的有效性进行了比较分析，旨在阐明目标检测中KD的发展现状。

Conclusion: 所提出的分类法和分析旨在澄清目标检测中KD的演变格局，突出当前挑战，并指导未来研究，以实现高效和可扩展的检测系统。

Abstract: Object detection has achieved remarkable accuracy through deep learning, yet
these improvements often come with increased computational cost, limiting
deployment on resource-constrained devices. Knowledge Distillation (KD)
provides an effective solution by enabling compact student models to learn from
larger teacher models. However, adapting KD to object detection poses unique
challenges due to its dual objectives-classification and localization-as well
as foreground-background imbalance and multi-scale feature representation. This
review introduces a novel architecture-centric taxonomy for KD methods,
distinguishing between CNN-based detectors (covering backbone-level,
neck-level, head-level, and RPN/RoI-level distillation) and Transformer-based
detectors (including query-level, feature-level, and logit-level distillation).
We further evaluate representative methods using the MS COCO and PASCAL VOC
datasets with mAP@0.5 as performance metric, providing a comparative analysis
of their effectiveness. The proposed taxonomy and analysis aim to clarify the
evolving landscape of KD in object detection, highlight current challenges, and
guide future research toward efficient and scalable detection systems.

</details>


### [139] [VideoGuard: Protecting Video Content from Unauthorized Editing](https://arxiv.org/abs/2508.03480)
*Junjie Cao,Kaizhou Li,Xinchun Yu,Hongxiang Li,Xiaoping Zhang*

Main category: cs.CV

TL;DR: 本文提出VideoGuard方法，通过引入微小扰动来有效保护视频内容免受未经授权的生成式模型恶意编辑。


<details>
  <summary>Details</summary>
Motivation: 生成技术快速发展，可生成高保真数字内容并进行受控编辑，但存在被恶意滥用的风险。现有研究主要关注图片保护，视频内容编辑的保护存在显著不足。

Method: 提出VideoGuard方法，通过引入几乎不可察觉的微小扰动来干扰生成扩散模型的功能。为解决视频帧间冗余和帧间注意力机制问题，该方法采用联合帧优化（将所有视频帧视为一个优化实体），并提取视频运动信息融入优化目标，迫使模型产生不合理和不一致的输出。文中提供了扰动优化流程。

Result: 通过客观和主观指标评估，结果表明VideoGuard方法的保护性能优于所有基线方法。

Conclusion: VideoGuard方法能有效保护视频免受未经授权的恶意编辑，通过联合帧优化和融入视频运动信息，实现了优越的保护效果。

Abstract: With the rapid development of generative technology, current generative
models can generate high-fidelity digital content and edit it in a controlled
manner. However, there is a risk that malicious individuals might misuse these
capabilities for misleading activities. Although existing research has
attempted to shield photographic images from being manipulated by generative
models, there remains a significant disparity in the protection offered to
video content editing. To bridge the gap, we propose a protection method named
VideoGuard, which can effectively protect videos from unauthorized malicious
editing. This protection is achieved through the subtle introduction of nearly
unnoticeable perturbations that interfere with the functioning of the intended
generative diffusion models. Due to the redundancy between video frames, and
inter-frame attention mechanism in video diffusion models, simply applying
image-based protection methods separately to every video frame can not shield
video from unauthorized editing. To tackle the above challenge, we adopt joint
frame optimization, treating all video frames as an optimization entity.
Furthermore, we extract video motion information and fuse it into optimization
objectives. Thus, these alterations can effectively force the models to produce
outputs that are implausible and inconsistent. We provide a pipeline to
optimize this perturbation. Finally, we use both objective metrics and
subjective metrics to demonstrate the efficacy of our method, and the results
show that the protection performance of VideoGuard is superior to all the
baseline methods.

</details>


### [140] [Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding and Generation](https://arxiv.org/abs/2508.03320)
*Peiyu Wang,Yi Peng,Yimeng Gan,Liang Hu,Tianyidan Xie,Xiaokun Wang,Yichen Wei,Chuanxin Tang,Bo Zhu,Changshi Li,Hongyang Wei,Eric Li,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Skywork UniPic是一个15亿参数的自回归模型，在一个单一架构中统一了图像理解、文本到图像生成和图像编辑，无需任务特定适配器，并在消费级硬件上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态系统通常需要任务特定适配器或模块间连接器，且高保真多模态集成往往需要过高的资源。研究动机是开发紧凑且能在普通硬件上实现最先进性能的多模态系统。

Method: ['采用解耦编码策略：使用掩码自回归编码器进行合成，SigLIP2编码器进行理解，两者均输入共享的自回归解码器。', '设计了渐进式、分辨率感知的训练计划，从256x256扩展到1024x1024，并动态解冻参数以平衡容量和稳定性。', '构建了精心策划的亿级规模数据集，并辅以任务特定的奖励模型来优化生成和编辑目标。']

Result: ['GenEval得分达到0.86，超越了大多数现有统一模型。', 'DPG-Bench复杂生成任务创下85.5的新纪录。', '图像编辑方面，GEditBench-EN得分5.83，ImgEdit-Bench得分3.49。', '能在低于15 GB的GPU内存（如RTX 4090）下生成1024x1024的图像。']

Conclusion: Skywork UniPic证明了高保真多模态集成无需巨大的资源投入，为可部署的高保真多模态AI建立了一个实用的范例。

Abstract: We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model
that unifies image understanding, text-to-image generation, and image editing
within a single architecture-eliminating the need for task-specific adapters or
inter-module connectors-and demonstrate that compact multimodal systems can
achieve state-of-the-art performance on commodity hardware. Skywork UniPic
achieves a GenEval score of 0.86, surpassing most existing unified models; sets
a new DPG-Bench complex-generation record of 85.5; attains 5.83 on
GEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x
1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled
encoding strategy that leverages a masked autoregressive encoder for synthesis
and a SigLIP2 encoder for understanding, all feeding a shared autoregressive
decoder; (2) a progressive, resolution-aware training schedule scaling from 256
x 256 to 1024 x 1024 while dynamically unfreezing parameters to balance
capacity and stability; and (3) meticulously curated, 100 million-scale
datasets augmented with task-specific reward models to refine generation and
editing objectives. By demonstrating that high-fidelity multimodal integration
need not incur prohibitive resource demands, Skywork UniPic establishes a
practical paradigm for deployable, high-fidelity multimodal AI. Code and
weights are publicly available at
https://huggingface.co/Skywork/Skywork-UniPic-1.5B.

</details>


### [141] [When Cars Have Stereotypes: Auditing Demographic Bias in Objects from Text-to-Image Models](https://arxiv.org/abs/2508.03483)
*Dasol Choi Jihwan Lee,Minjae Lee,Minsuk Kahng*

Main category: cs.CV

TL;DR: 该研究引入SODA框架，系统测量文本到图像生成模型中物体（而非人类）的隐含人口统计学偏见，发现模型在生成物体时会根据人口学提示（如年龄、性别、种族）产生并强化刻板印象，呼吁更负责任的AI开发。


<details>
  <summary>Details</summary>
Motivation: 先前的研究主要关注文本到图像生成模型在人类描绘上的偏见，但研究者认为在生成的物体中也存在一种更微妙但普遍的人口统计学偏见，亟需系统性测量和揭示。

Method: 引入了SODA（Stereotyped Object Diagnostic Audit）框架，通过比较在带有特定人口统计学提示（如“为年轻人设计的”）和中性提示下生成的物体的视觉属性。研究使用了2700张由GPT Image-1、Imagen 4和Stable Diffusion三款主流模型生成的图像，涵盖五种物体类别，进行综合分析。

Result: 研究发现特定人口统计学群体与视觉属性之间存在强烈的关联，例如由性别或种族提示引起的重复颜色模式。这些模式不仅反映并强化了众所周知的刻板印象，也揭示了更微妙和不直观的偏见。此外，部分模型生成的输出多样性较低，这进一步放大了与中性提示相比的视觉差异。

Conclusion: 所提出的SODA审计框架为测试和揭示生成模型中仍然存在的刻板印象提供了一种实用方法。这被认为是迈向更系统化和负责任的AI开发的关键一步。

Abstract: While prior research on text-to-image generation has predominantly focused on
biases in human depictions, we investigate a more subtle yet pervasive
phenomenon: demographic bias in generated objects (e.g., cars). We introduce
SODA (Stereotyped Object Diagnostic Audit), a novel framework for
systematically measuring such biases. Our approach compares visual attributes
of objects generated with demographic cues (e.g., "for young people'') to those
from neutral prompts, across 2,700 images produced by three state-of-the-art
models (GPT Image-1, Imagen 4, and Stable Diffusion) in five object categories.
Through a comprehensive analysis, we uncover strong associations between
specific demographic groups and visual attributes, such as recurring color
patterns prompted by gender or ethnicity cues. These patterns reflect and
reinforce not only well-known stereotypes but also more subtle and unintuitive
biases. We also observe that some models generate less diverse outputs, which
in turn amplifies the visual disparities compared to neutral prompts. Our
proposed auditing framework offers a practical approach for testing, revealing
how stereotypes still remain embedded in today's generative models. We see this
as an essential step toward more systematic and responsible AI development.

</details>


### [142] [Macro-from-Micro Planning for High-Quality and Parallelized Autoregressive Long Video Generation](https://arxiv.org/abs/2508.03334)
*Xunzhi Xiang,Yabo Chen,Guiyu Zhang,Zhongyu Wang,Zhe Gao,Quanming Xiang,Gonghu Shang,Junqi Liu,Haibin Huang,Yang Gao,Chi Zhang,Qi Fan,Xuelong Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为MMPL（Macro-from-Micro Planning）的“规划-填充”框架，用于生成长视频，解决了现有自回归扩散模型在长视频生成中存在的时序漂移和并行化限制。


<details>
  <summary>Details</summary>
Motivation: 当前的自回归扩散模型在视频生成方面表现出色，但通常仅限于短时程视频。理论分析表明，自回归建模常因误差累积导致时序漂移，并阻碍长视频合成中的并行化。

Method: 本文提出了一种以MMPL为核心的“规划-填充”框架。MMPL通过两个层级阶段勾勒出整个视频的全局故事情节：微观规划（Micro Planning）在每个短视频段内预测稀疏的未来关键帧，提供运动和外观先验以指导高质量视频段生成；宏观规划（Macro Planning）通过微观计划的自回归链将段内关键帧规划扩展到整个视频，确保视频段之间的长期一致性。随后，基于MMPL的内容填充（Content Populating）在不同视频段之间并行生成所有中间帧，实现自回归生成的高效并行化。并行化通过自适应工作负载调度（Adaptive Workload Scheduling）进一步优化，以平衡GPU执行并加速自回归视频生成。

Result: 广泛的实验证实，该方法在质量和稳定性方面优于现有长视频生成模型。

Conclusion: MMPL框架通过分层规划和并行内容生成，有效解决了自回归模型在长视频生成中的局限性，实现了高质量和高稳定性的长视频生成。

Abstract: Current autoregressive diffusion models excel at video generation but are
generally limited to short temporal durations. Our theoretical analysis
indicates that the autoregressive modeling typically suffers from temporal
drift caused by error accumulation and hinders parallelization in long video
synthesis. To address these limitations, we propose a novel
planning-then-populating framework centered on Macro-from-Micro Planning (MMPL)
for long video generation. MMPL sketches a global storyline for the entire
video through two hierarchical stages: Micro Planning and Macro Planning.
Specifically, Micro Planning predicts a sparse set of future keyframes within
each short video segment, offering motion and appearance priors to guide
high-quality video segment generation. Macro Planning extends the in-segment
keyframes planning across the entire video through an autoregressive chain of
micro plans, ensuring long-term consistency across video segments.
Subsequently, MMPL-based Content Populating generates all intermediate frames
in parallel across segments, enabling efficient parallelization of
autoregressive generation. The parallelization is further optimized by Adaptive
Workload Scheduling for balanced GPU execution and accelerated autoregressive
video generation. Extensive experiments confirm that our method outperforms
existing long video generation models in quality and stability. Generated
videos and comparison results are in our project page.

</details>


### [143] [Retinal Lipidomics Associations as Candidate Biomarkers for Cardiovascular Health](https://arxiv.org/abs/2508.03538)
*Inamullah,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

TL;DR: 本研究首次在一个健康人群队列中，结合深度学习衍生的视网膜血管特征和脂质组学亚类，揭示了血清脂质亚类（如游离脂肪酸、胆固醇酯、甘油二酯和甘油三酯）与视网膜微血管特征之间的关联。


<details>
  <summary>Details</summary>
Motivation: 视网膜微血管成像作为评估全身血管和代谢健康的非侵入性方法日益受到认可，然而，脂质组学与视网膜血管之间的关联性研究不足。

Method: 研究在一个大型基于人群的队列中进行，使用Spearman相关分析检查脂质亚类与十种视网膜微血管特征之间的相互关系，并应用Benjamini-Hochberg假发现率（BH-FDR）进行统计显著性调整。视网膜特征通过深度学习（DL）技术获得。

Result: 结果显示，游离脂肪酸（FA）与视网膜血管的弯曲度相关；胆固醇酯（CE）与动脉和静脉的平均宽度相关。相反，甘油二酯（DAG）和甘油三酯（TAG）与小动脉和小静脉的宽度和复杂性呈负相关。

Conclusion: 这些发现表明视网膜血管结构反映了不同的循环脂质谱，支持其作为全身代谢健康非侵入性标志物的作用。本研究首次在一个健康队列中整合了深度学习衍生的视网膜特征与脂质组学亚类，为独立于疾病状态或治疗效果的微血管结构变化提供了见解。

Abstract: Retinal microvascular imaging is increasingly recognised as a non invasive
method for evaluating systemic vascular and metabolic health. However, the
association between lipidomics and retinal vasculature remains inadequate. This
study investigates the relationships between serum lipid subclasses, free fatty
acids (FA), diacylglycerols (DAG), triacylglycerols (TAG), and cholesteryl
esters (CE), and retinal microvascular characteristics in a large
population-based cohort. Using Spearman correlation analysis, we examined the
interconnection between lipid subclasses and ten retinal microvascular traits,
applying the Benjamini-Hochberg false discovery rate (BH-FDR) to adjust for
statistical significance.
  Results indicated that FA were linked to retinal vessel twistiness, while CE
correlated with the average widths of arteries and veins. Conversely, DAG and
TAG showed negative correlations with the width and complexity of arterioles
and venules. These findings suggest that retinal vascular architecture reflects
distinct circulating lipid profiles, supporting its role as a non-invasive
marker of systemic metabolic health. This study is the first to integrate deep
learning (DL)derived retinal traits with lipidomic subclasses in a healthy
cohort, thereby providing insights into microvascular structural changes
independent of disease status or treatment effects.

</details>


### [144] [Beyond Illumination: Fine-Grained Detail Preservation in Extreme Dark Image Restoration](https://arxiv.org/abs/2508.03336)
*Tongshun Zhang,Pingping Liu,Zixuan Zhong,Zijian Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 本文提出一种高效的双阶段方法，用于恢复极暗图像中的精细细节。第一阶段利用残差傅里叶引导模块恢复全局光照；第二阶段通过Mamba模块（Patch Mamba和Grad Mamba）精炼纹理结构和锐利边缘。


<details>
  <summary>Details</summary>
Motivation: 在极暗图像中恢复精细细节具有挑战性，因为存在严重的结构信息丢失和噪声污染。现有增强方法往往无法保留复杂的细节和锐利的边缘，从而限制了它们在文本和边缘检测等下游应用中的有效性。

Method: 该方法采用高效的双阶段方法：
1.  **第一阶段**：引入残差傅里叶引导模块（RFGM），在频域有效恢复全局光照。RFGM通过残差连接捕获阶段间和通道间依赖，提供鲁棒的先验信息，同时减轻误差累积。
2.  **第二阶段**：采用互补的Mamba模块进行纹理结构精炼。Patch Mamba在非下采样补丁上操作，建模像素级关联以增强精细细节。Grad Mamba专注于高梯度区域，重建锐利边缘和边界，并缓解状态空间模型中的状态衰减。

Result: 在多个基准数据集和下游应用上的广泛实验表明，该方法显著提高了细节恢复性能，同时保持了效率。所提出的模块轻量级，可以无缝集成到现有基于傅里叶的框架中，计算开销极小。

Conclusion: 本文提出的双阶段方法，结合RFGM和创新的Mamba模块，成功解决了极暗图像细节恢复的挑战，显著提升了图像质量和下游应用表现，且具有高效和轻量级的特点。

Abstract: Recovering fine-grained details in extremely dark images remains challenging
due to severe structural information loss and noise corruption. Existing
enhancement methods often fail to preserve intricate details and sharp edges,
limiting their effectiveness in downstream applications like text and edge
detection. To address these deficiencies, we propose an efficient dual-stage
approach centered on detail recovery for dark images. In the first stage, we
introduce a Residual Fourier-Guided Module (RFGM) that effectively restores
global illumination in the frequency domain. RFGM captures inter-stage and
inter-channel dependencies through residual connections, providing robust
priors for high-fidelity frequency processing while mitigating error
accumulation risks from unreliable priors. The second stage employs
complementary Mamba modules specifically designed for textural structure
refinement: (1) Patch Mamba operates on channel-concatenated non-downsampled
patches, meticulously modeling pixel-level correlations to enhance fine-grained
details without resolution loss. (2) Grad Mamba explicitly focuses on
high-gradient regions, alleviating state decay in state space models and
prioritizing reconstruction of sharp edges and boundaries. Extensive
experiments on multiple benchmark datasets and downstream applications
demonstrate that our method significantly improves detail recovery performance
while maintaining efficiency. Crucially, the proposed modules are lightweight
and can be seamlessly integrated into existing Fourier-based frameworks with
minimal computational overhead. Code is available at
https://github.com/bywlzts/RFGM.

</details>


### [145] [MetaScope: Optics-Driven Neural Network for Ultra-Micro Metalens Endoscopy](https://arxiv.org/abs/2508.03596)
*Wuyang Li,Wentao Pan,Xiaoyuan Liu,Zhendong Luo,Chenxin Li,Hengyu Liu,Din Ping Tsai,Mu Ku Chen,Yixuan Yuan*

Main category: cs.CV

TL;DR: 针对微型内窥镜中金属透镜成像的挑战，本文提出MetaScope神经网络，通过光学信息调整亮度和校正色差，显著提升了分割和复原性能。


<details>
  <summary>Details</summary>
Motivation: 现有微型内窥镜受限于传统凸透镜的物理尺寸（毫米级），难以实现微米级临床应用。虽然金属透镜（微米级）提供了有前景的解决方案，但其物理特性导致数据获取和算法研究存在巨大空白，亟需填补。

Method: 首先，建立了金属透镜内窥镜数据集并进行初步光学模拟，识别出两个强光学先验相关的问题。其次，提出了MetaScope，一个由物理光学驱动的新型神经网络。MetaScope包含两个新颖设计：光学信息强度调整（OIA）用于学习光学嵌入以校正强度衰减；光学信息色差校正（OCC）用于通过学习点扩散函数（PSF）分布指导的空间形变来减轻色差。此外，还部署了梯度引导蒸馏以自适应地从基础模型中转移知识，以增强联合学习。

Result: 大量实验表明，MetaScope不仅在金属透镜图像分割和复原方面均优于现有最先进方法，而且在真实生物医学场景中展现出出色的泛化能力。

Conclusion: 本研究成功填补了金属透镜内窥镜成像的算法空白，MetaScope为微米级内窥镜提供了高性能的图像处理解决方案，推动了微型内窥镜技术的发展。

Abstract: Miniaturized endoscopy has advanced accurate visual perception within the
human body. Prevailing research remains limited to conventional cameras
employing convex lenses, where the physical constraints with millimetre-scale
thickness impose serious impediments on the micro-level clinical. Recently,
with the emergence of meta-optics, ultra-micro imaging based on metalenses
(micron-scale) has garnered great attention, serving as a promising solution.
However, due to the physical difference of metalens, there is a large gap in
data acquisition and algorithm research. In light of this, we aim to bridge
this unexplored gap, advancing the novel metalens endoscopy. First, we
establish datasets for metalens endoscopy and conduct preliminary optical
simulation, identifying two derived optical issues that physically adhere to
strong optical priors. Second, we propose MetaScope, a novel optics-driven
neural network tailored for metalens endoscopy driven by physical optics.
MetaScope comprises two novel designs: Optics-informed Intensity Adjustment
(OIA), rectifying intensity decay by learning optical embeddings, and
Optics-informed Chromatic Correction (OCC), mitigating chromatic aberration by
learning spatial deformations informed by learned Point Spread Function (PSF)
distributions. To enhance joint learning, we further deploy a gradient-guided
distillation to transfer knowledge from the foundational model adaptively.
Extensive experiments demonstrate that MetaScope not only outperforms
state-of-the-art methods in both metalens segmentation and restoration but also
achieves impressive generalized ability in real biomedical scenes.

</details>


### [146] [Less is More: Token-Efficient Video-QA via Adaptive Frame-Pruning and Semantic Graph Integration](https://arxiv.org/abs/2508.03337)
*Shaoguang Wang,Jianxiang He,Yijie Xu,Ziyang Chen,Weiyu Guo,Hui Xiong*

Main category: cs.CV

TL;DR: 针对视频问答（Video-QA）中多模态大语言模型（MLLMs）因帧数过多导致的高昂token成本和性能下降问题，本文提出自适应帧剪枝（AFP）方法，通过智能合并冗余帧并结合轻量级语义图，大幅减少输入帧数和token，同时提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视频问答中的实际应用受限于处理大量视频帧导致的高昂token成本。研究发现，过度增加帧数反而会因上下文稀释而降低性能（“少即是多”现象）。此外，现有关键帧选择方法仍存在显著的时间冗余，即“视觉回声”。

Method: 提出自适应帧剪枝（AFP）作为关键帧的后处理方法。AFP在融合的ResNet-50和CLIP特征空间上应用自适应层次聚类算法，识别并合并视觉回声为单个代表帧。为弥补信息损失，引入一个轻量级的、基于文本的语义图，以最小的token开销提供关键上下文。

Result: 在LongVideoBench和VideoMME基准测试上，该方法使所需帧数最多减少86.9%，总输入token最多减少83.2%。通过提供简洁、高质量的帧集，该方法不仅提高了效率，而且在许多情况下比使用更多帧的基线模型取得了更高的准确性。

Conclusion: 自适应帧剪枝（AFP）及其配套的语义图机制有效解决了视频问答中MLLMs面临的高token成本和上下文稀释问题。通过智能地精简视频帧并提供必要的语义补充，该方法显著提升了MLLMs在视频问答任务上的效率和准确性。

Abstract: The practical application of Multimodal Large Language Models (MLLMs) to
Video Question Answering (Video-QA) is severely hindered by the high token cost
of processing numerous video frames. While increasing the number of sampled
frames is a common strategy, we observe a "less is more" phenomenon where
excessive frames can paradoxically degrade performance due to context dilution.
Concurrently, state-of-the-art keyframe selection methods, while effective,
still yield significant temporal redundancy, which we term 'visual echoes'. To
address these dual challenges, we propose Adaptive Frame-Pruning (AFP), a novel
post-processing method that intelligently prunes the selected keyframes. AFP
employs an adaptive hierarchical clustering algorithm on a fused ResNet-50 and
CLIP feature space to identify and merge these echoes into single
representatives. To compensate for information loss, we then introduce a
lightweight, text-based semantic graph that provides critical context with
minimal token overhead. Conducting extensive experiments on the LongVideoBench
and VideoMME benchmarks across multiple leading MLLMs, our full approach
demonstrates a drastic reduction in required frames by up to 86.9% and total
input tokens by up to 83.2%. Crucially, by providing a concise, high-quality
set of frames, our method not only enhances efficiency but often improves
accuracy over baselines that use more frames. The code will be released upon
publication.

</details>


### [147] [AttZoom: Attention Zoom for Better Visual Features](https://arxiv.org/abs/2508.03625)
*Daniel DeAlcala,Aythami Morales,Julian Fierrez,Ruben Tolosana*

Main category: cs.CV

TL;DR: Attention Zoom是一种模块化、模型无关的空间注意力机制，通过强调输入中的重要区域，在多种CNN骨干网络上持续提升分类精度，且架构开销极小。


<details>
  <summary>Details</summary>
Motivation: 传统的注意力机制需要与特定网络架构深度集成，限制了其通用性。本研究旨在开发一种独立、通用的空间注意力层，以改进CNN的特征提取。

Method: 提出了一种名为Attention Zoom的独立层，该层通过空间方式强调输入中的高重要性区域。在CIFAR-100和TinyImageNet数据集上，使用多个CNN骨干网络进行了评估。同时，采用Grad-CAM和空间扭曲进行视觉分析。

Result: 在Top-1和Top-5分类准确率上均显示出持续的提升。视觉分析表明，该方法能促进细粒度和多样化的注意力模式。

Conclusion: Attention Zoom层有效且通用，能以最小的架构开销显著改善卷积神经网络的性能。

Abstract: We present Attention Zoom, a modular and model-agnostic spatial attention
mechanism designed to improve feature extraction in convolutional neural
networks (CNNs). Unlike traditional attention approaches that require
architecture-specific integration, our method introduces a standalone layer
that spatially emphasizes high-importance regions in the input. We evaluated
Attention Zoom on multiple CNN backbones using CIFAR-100 and TinyImageNet,
showing consistent improvements in Top-1 and Top-5 classification accuracy.
Visual analyses using Grad-CAM and spatial warping reveal that our method
encourages fine-grained and diverse attention patterns. Our results confirm the
effectiveness and generality of the proposed layer for improving CCNs with
minimal architectural overhead.

</details>


### [148] [CIVQLLIE: Causal Intervention with Vector Quantization for Low-Light Image Enhancement](https://arxiv.org/abs/2508.03338)
*Tongshun Zhang,Pingping Liu,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种名为CIVQLLIE的新型低光照图像增强（LLIE）框架，通过向量量化（VQ）进行离散表示学习，并结合多级因果干预来解决现有方法在极暗条件下的局限性及可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 现有低光照图像增强方法存在显著挑战：数据驱动的端到端映射网络缺乏可解释性或依赖不可靠先验，在极端黑暗条件下表现不佳；基于物理的方法则依赖简化假设，在复杂真实场景中常常失效。

Method: 本文提出CIVQLLIE框架，利用向量量化（VQ）将连续图像特征映射到从高质量图像学习到的离散视觉令牌码本，该码本作为可靠先验。为解决低光照输入与码本之间的分布偏移，提出多级因果干预方法：1) 编码阶段的像素级因果干预（PCI）模块校正低级特征；2) 结合低频选择性注意力门控（LSAG）的特征感知因果干预（FCI）机制识别并增强受照度退化影响最严重的通道；3) 解码阶段的高频细节重建模块（HDRM）利用可变形卷积技术从匹配的码本表示中重建精细细节。

Result: 通过将连续图像特征转换为离散码本表示，并引入多级因果干预机制，本文提出的方法能够系统地校正低光照图像与学习码本之间的分布偏移，从而实现准确的码本令牌匹配，并有效重建高频细节，增强了编码器的泛化性能，克服了传统LLIE方法在极端黑暗和复杂场景下的局限性。

Conclusion: CIVQLLIE框架通过结合向量量化学习到的可靠离散先验和多级因果干预策略，有效解决了低光照图像增强中存在的可见性差、可解释性不足以及对复杂场景适应性差的问题，为极端低光照条件下的图像增强提供了一种新颖且鲁棒的解决方案。

Abstract: Images captured in nighttime scenes suffer from severely reduced visibility,
hindering effective content perception. Current low-light image enhancement
(LLIE) methods face significant challenges: data-driven end-to-end mapping
networks lack interpretability or rely on unreliable prior guidance, struggling
under extremely dark conditions, while physics-based methods depend on
simplified assumptions that often fail in complex real-world scenarios. To
address these limitations, we propose CIVQLLIE, a novel framework that
leverages the power of discrete representation learning through causal
reasoning. We achieve this through Vector Quantization (VQ), which maps
continuous image features to a discrete codebook of visual tokens learned from
large-scale high-quality images. This codebook serves as a reliable prior,
encoding standardized brightness and color patterns that are independent of
degradation. However, direct application of VQ to low-light images fails due to
distribution shifts between degraded inputs and the learned codebook.
Therefore, we propose a multi-level causal intervention approach to
systematically correct these shifts. First, during encoding, our Pixel-level
Causal Intervention (PCI) module intervenes to align low-level features with
the brightness and color distributions expected by the codebook. Second, a
Feature-aware Causal Intervention (FCI) mechanism with Low-frequency Selective
Attention Gating (LSAG) identifies and enhances channels most affected by
illumination degradation, facilitating accurate codebook token matching while
enhancing the encoder's generalization performance through flexible
feature-level intervention. Finally, during decoding, the High-frequency Detail
Reconstruction Module (HDRM) leverages structural information preserved in the
matched codebook representations to reconstruct fine details using deformable
convolution techniques.

</details>


### [149] [WaMo: Wavelet-Enhanced Multi-Frequency Trajectory Analysis for Fine-Grained Text-Motion Retrieval](https://arxiv.org/abs/2508.03343)
*Junlong Ren,Gangjian Zhang,Honghao Fu,Pengcheng Wu,Hao Wang*

Main category: cs.CV

TL;DR: WaMo是一种基于小波的多频特征提取框架，用于解决文本-动作检索（TMR）中3D动作与文本的精细对齐挑战，通过捕获身体部位特异性和时变运动细节，显著提升了检索性能。


<details>
  <summary>Details</summary>
Motivation: 现有文本-动作检索（TMR）方法在匹配3D动作与文本时面临挑战，主要原因是人体结构的复杂性和其时空动态性，现有方法常忽略这些复杂性，无法区分不同身体部位及其动态，导致语义对齐不精确。

Method: 本文提出了WaMo框架，包含三个关键组件：1) 轨迹小波分解：将运动信号分解为频率分量，同时保留局部运动学细节和全局运动语义。2) 轨迹小波重建：使用可学习的逆小波变换从提取的特征中重建原始关节轨迹，确保保留关键时空信息。3) 乱序运动序列预测：通过重新排序打乱的运动序列，改善对内在时间连贯性的学习，从而增强运动与文本的对齐。

Result: WaMo在HumanML3D和KIT-ML数据集上，Rsum指标分别取得了17.0%和18.2%的提升，优于现有最先进（SOTA）方法。

Conclusion: WaMo通过有效捕获身体部位特异性和时变运动细节，解决了文本-动作检索中的挑战，实现了运动与文本的精细对齐，并显著提升了检索性能。

Abstract: Text-Motion Retrieval (TMR) aims to retrieve 3D motion sequences semantically
relevant to text descriptions. However, matching 3D motions with text remains
highly challenging, primarily due to the intricate structure of human body and
its spatial-temporal dynamics. Existing approaches often overlook these
complexities, relying on general encoding methods that fail to distinguish
different body parts and their dynamics, limiting precise semantic alignment.
To address this, we propose WaMo, a novel wavelet-based multi-frequency feature
extraction framework. It fully captures part-specific and time-varying motion
details across multiple resolutions on body joints, extracting discriminative
motion features to achieve fine-grained alignment with texts. WaMo has three
key components: (1) Trajectory Wavelet Decomposition decomposes motion signals
into frequency components that preserve both local kinematic details and global
motion semantics. (2) Trajectory Wavelet Reconstruction uses learnable inverse
wavelet transforms to reconstruct original joint trajectories from extracted
features, ensuring the preservation of essential spatial-temporal information.
(3) Disordered Motion Sequence Prediction reorders shuffled motion sequences to
improve the learning of inherent temporal coherence, enhancing motion-text
alignment. Extensive experiments demonstrate WaMo's superiority, achieving
17.0\% and 18.2\% improvements in $Rsum$ on HumanML3D and KIT-ML datasets,
respectively, outperforming existing state-of-the-art (SOTA) methods.

</details>


### [150] [FedPromo: Federated Lightweight Proxy Models at the Edge Bring New Domains to Foundation Models](https://arxiv.org/abs/2508.03356)
*Matteo Caligiuri,Francesco Barbato,Donald Shenaj,Umberto Michieli,Pietro Zanuttigh*

Main category: cs.CV

TL;DR: FedPromo是一种新颖的联邦学习框架，通过在客户端优化轻量级代理模型并利用服务器端知识蒸馏，实现了在资源受限客户端上高效适应大型基础模型，同时保护隐私。


<details>
  <summary>Details</summary>
Motivation: 传统的联邦学习方法在客户端设备上训练大型深度学习模型时，需要大量的计算资源，这对于资源受限的客户端来说是不可行的。

Method: FedPromo采用两阶段方法：1. 服务器端知识蒸馏：将大型基础模型（如Transformer）的表示与紧凑型模型（如CNN）对齐。2. 客户端联邦学习：将紧凑型模型编码器部署到客户端，本地学习可训练分类器，然后聚合并无缝传回基础模型，实现个性化适应。通过新颖的正则化策略，实现分散式多领域学习。

Result: 在五个图像分类基准测试中，FedPromo在假设客户端资源有限的情况下，表现优于现有方法。

Conclusion: FedPromo框架在性能、隐私和资源效率之间取得了平衡，为在资源受限的联邦学习环境中适应大型基础模型提供了一种高效、私密且多领域适用的解决方案。

Abstract: Federated Learning (FL) is an established paradigm for training deep learning
models on decentralized data. However, as the size of the models grows,
conventional FL approaches often require significant computational resources on
client devices, which may not be feasible. We introduce FedPromo, a novel
framework that enables efficient adaptation of large-scale foundation models
stored on a central server to new domains encountered only by remote clients.
Instead of directly training the large model on client devices, FedPromo
optimizes lightweight proxy models via FL, significantly reducing computational
overhead while maintaining privacy. Our method follows a two-stage process:
first, server-side knowledge distillation aligns the representations of a
large-scale foundation model (e.g., a transformer) with those of a compact
counterpart (e.g., a CNN). Then, the compact model encoder is deployed to
client devices, where trainable classifiers are learned locally. These
classifiers are subsequently aggregated and seamlessly transferred back to the
foundation model, facilitating personalized adaptation without requiring direct
access to user data. Through novel regularization strategies, our framework
enables decentralized multi-domain learning, balancing performance, privacy,
and resource efficiency. Extensive experiments on five image classification
benchmarks demonstrate that FedPromo outperforms existing methods while
assuming limited-resource clients.

</details>


### [151] [Diffusion Once and Done: Degradation-Aware LoRA for Efficient All-in-One Image Restoration](https://arxiv.org/abs/2508.03373)
*Ni Tang,Xiaotong Luo,Zihan Cheng,Liangtai Zhou,Dongxiao Zhang,Yanyun Qu*

Main category: cs.CV

TL;DR: 本文提出了一种名为DOD（Diffusion Once and Done）的高效全能图像恢复（AiOIR）方法，通过一步Stable Diffusion采样实现卓越的恢复性能，解决了现有扩散模型恢复方法推理成本高和适应性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的全能图像恢复（AiOIR）方法通常面临高昂的推理成本和对多样退化类型适应性有限的问题。

Method: 该方法首先引入多退化特征调制来捕获不同的退化提示，然后通过参数高效的条件低秩适应（LoRA）将这些提示整合，以微调SD模型适应不同退化类型。此外，还在SD解码器中集成了一个高保真细节增强模块，以改善结构和纹理细节。最终实现一步采样。

Result: 实验证明，该方法在视觉质量和推理效率方面均优于现有基于扩散的恢复方法。

Conclusion: DOD方法通过一步采样实现了高效且高性能的图像恢复，有效解决了现有扩散模型恢复方法的局限性。

Abstract: Diffusion models have revealed powerful potential in all-in-one image
restoration (AiOIR), which is talented in generating abundant texture details.
The existing AiOIR methods either retrain a diffusion model or fine-tune the
pretrained diffusion model with extra conditional guidance. However, they often
suffer from high inference costs and limited adaptability to diverse
degradation types. In this paper, we propose an efficient AiOIR method,
Diffusion Once and Done (DOD), which aims to achieve superior restoration
performance with only one-step sampling of Stable Diffusion (SD) models.
Specifically, multi-degradation feature modulation is first introduced to
capture different degradation prompts with a pretrained diffusion model. Then,
parameter-efficient conditional low-rank adaptation integrates the prompts to
enable the fine-tuning of the SD model for adapting to different degradation
types. Besides, a high-fidelity detail enhancement module is integrated into
the decoder of SD to improve structural and textural details. Experiments
demonstrate that our method outperforms existing diffusion-based restoration
approaches in both visual quality and inference efficiency.

</details>


### [152] [GRASPing Anatomy to Improve Pathology Segmentation](https://arxiv.org/abs/2508.03374)
*Keyi Li,Alexander Jaus,Jens Kleesiek,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: GRASP是一个即插即用框架，通过伪标签集成和特征对齐，利用现有解剖结构分割模型增强病理分割，无需重新训练解剖组件，从而引入解剖学上下文。


<details>
  <summary>Details</summary>
Motivation: 大多数当前深度学习方法在病理分割中仅依赖模式识别，忽略了病理发生发展的解剖学上下文，而放射科医生在病理描绘中高度依赖解剖学理解。

Method: GRASP（病理分割引导表示对齐）是一个模块化即插即用框架。它通过以下方式增强病理分割模型：1. 伪标签集成（将解剖学伪标签作为输入通道）；2. 特征对齐（通过Transformer引导的解剖学特征融合）。该框架集成到标准病理优化方案中，无需重新训练现有的解剖学组件。

Result: GRASP在两个PET/CT数据集上，跨多种评估指标和不同架构，始终取得顶尖排名。其双重解剖学注入策略（结合解剖学伪标签作为输入通道和Transformer引导的解剖学特征融合）有效地融入了解剖学上下文。

Conclusion: GRASP通过其独特的双重解剖学注入策略，成功将解剖学上下文整合到病理分割模型中，提升了模型的性能，弥补了深度学习与放射科医生解剖学理解之间的差距。

Abstract: Radiologists rely on anatomical understanding to accurately delineate
pathologies, yet most current deep learning approaches use pure pattern
recognition and ignore the anatomical context in which pathologies develop. To
narrow this gap, we introduce GRASP (Guided Representation Alignment for the
Segmentation of Pathologies), a modular plug-and-play framework that enhances
pathology segmentation models by leveraging existing anatomy segmentation
models through pseudolabel integration and feature alignment. Unlike previous
approaches that obtain anatomical knowledge via auxiliary training, GRASP
integrates into standard pathology optimization regimes without retraining
anatomical components. We evaluate GRASP on two PET/CT datasets, conduct
systematic ablation studies, and investigate the framework's inner workings. We
find that GRASP consistently achieves top rankings across multiple evaluation
metrics and diverse architectures. The framework's dual anatomy injection
strategy, combining anatomical pseudo-labels as input channels with
transformer-guided anatomical feature fusion, effectively incorporates
anatomical context.

</details>


### [153] [GaitAdapt: Continual Learning for Evolving Gait Recognition](https://arxiv.org/abs/2508.03375)
*Jingjie Wang,Shunli Zhang,Xiang Wei,Senmao Tian*

Main category: cs.CV

TL;DR: 本文提出GaitAdapt持续步态识别任务，并开发了GaitAdapter非回放持续学习方法，通过知识聚合和距离稳定性来解决新旧数据集知识遗忘问题，有效提升模型识别能力。


<details>
  <summary>Details</summary>
Motivation: 现有步态识别方法在面对新数据集时需要重新训练，但重训模型难以保留旧知识，导致在先前测试集上的性能显著下降。这促使研究人员寻求一种能持续学习并避免知识遗忘的步态识别方案。

Method: 本文定义了GaitAdapt持续步态识别任务，并提出了GaitAdapter方法。该方法包含两个核心组件：1) GaitPartition Adaptive Knowledge (GPAK) 模块，利用图神经网络将当前数据中的通用步态模式聚合到图向量构成的知识库中，以增强新任务中步态特征的判别性。2) 基于负样本的欧氏距离稳定性方法 (EDSN)，确保来自不同类别的新增步态样本在先前和当前任务中保持相似的相对空间分布，从而减轻任务变化对原始域特征可区分性的影响。

Result: 广泛评估表明，GaitAdapter能有效保留从不同任务中获取的步态知识，与现有方法相比，表现出显著优越的判别能力。

Conclusion: GaitAdapter为持续步态识别提供了一种有效的非回放学习解决方案，成功解决了知识遗忘问题，并显著提升了模型在多任务环境下的识别性能和知识保留能力。

Abstract: Current gait recognition methodologies generally necessitate retraining when
encountering new datasets. Nevertheless, retrained models frequently encounter
difficulties in preserving knowledge from previous datasets, leading to a
significant decline in performance on earlier test sets. To tackle these
challenges, we present a continual gait recognition task, termed GaitAdapt,
which supports the progressive enhancement of gait recognition capabilities
over time and is systematically categorized according to various evaluation
scenarios. Additionally, we propose GaitAdapter, a non-replay continual
learning approach for gait recognition. This approach integrates the
GaitPartition Adaptive Knowledge (GPAK) module, employing graph neural networks
to aggregate common gait patterns from current data into a repository
constructed from graph vectors. Subsequently, this repository is used to
improve the discriminability of gait features in new tasks, thereby enhancing
the model's ability to effectively recognize gait patterns. We also introduce a
Euclidean Distance Stability Method (EDSN) based on negative pairs, which
ensures that newly added gait samples from different classes maintain similar
relative spatial distributions across both previous and current gait tasks,
thereby alleviating the impact of task changes on the distinguishability of
original domain features. Extensive evaluations demonstrate that GaitAdapter
effectively retains gait knowledge acquired from diverse tasks, exhibiting
markedly superior discriminative capability compared to alternative methods.

</details>


### [154] [Neutralizing Token Aggregation via Information Augmentation for Efficient Test-Time Adaptation](https://arxiv.org/abs/2508.03388)
*Yizhe Xiong,Zihan Zhou,Yiwen Liang,Hui Chen,Zijia Lin,Tianxiang Hao,Fan Zhang,Jungong Han,Guiguang Ding*

Main category: cs.CV

TL;DR: 本文提出NAVIA，一种高效的测试时间适应（ETTA）方法，通过信息增强来弥补ViT中令牌聚合导致的信息损失，显著提升性能并降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的ViT测试时间适应（TTA）方法计算开销大，不适用于资源受限场景。令牌聚合虽能降低推理成本，但与现有TTA结合时会造成显著的性能下降，主要原因是信息丢失。因此，需要一种在降低延迟的同时保持TTA适应能力的方法。

Method: 本文将问题形式化为高效测试时间适应（ETTA）。首先，从互信息角度理论分析令牌聚合导致信息损失，且传统TTA难以弥补。受此启发，提出NAVIA（通过信息增强中和令牌聚合）。具体地，NAVIA直接增强[CLS]令牌嵌入，并在ViT浅层为[CLS]令牌引入自适应偏差。理论证明，这些增强在通过熵最小化优化时，能恢复因令牌聚合而丢失的信息。

Result: 在各种分布外基准测试中，NAVIA的性能超越现有最先进方法超过2.5%，同时推理延迟降低超过20%。

Conclusion: NAVIA有效解决了ETTA挑战，在保持TTA适应能力的同时显著降低了推理延迟，实现了性能与效率的平衡。

Abstract: Test-Time Adaptation (TTA) has emerged as an effective solution for adapting
Vision Transformers (ViT) to distribution shifts without additional training
data. However, existing TTA methods often incur substantial computational
overhead, limiting their applicability in resource-constrained real-world
scenarios. To reduce inference cost, plug-and-play token aggregation methods
merge redundant tokens in ViTs to reduce total processed tokens. Albeit
efficient, it suffers from significant performance degradation when directly
integrated with existing TTA methods. We formalize this problem as Efficient
Test-Time Adaptation (ETTA), seeking to preserve the adaptation capability of
TTA while reducing inference latency. In this paper, we first provide a
theoretical analysis from a novel mutual information perspective, showing that
token aggregation inherently leads to information loss, which cannot be fully
mitigated by conventional norm-tuning-based TTA methods. Guided by this
insight, we propose to \textbf{N}eutralize Token \textbf{A}ggregation
\textbf{v}ia \textbf{I}nformation \textbf{A}ugmentation (\textbf{NAVIA}).
Specifically, we directly augment the [CLS] token embedding and incorporate
adaptive biases into the [CLS] token in shallow layers of ViTs. We
theoretically demonstrate that these augmentations, when optimized via entropy
minimization, recover the information lost due to token aggregation. Extensive
experiments across various out-of-distribution benchmarks demonstrate that
NAVIA significantly outperforms state-of-the-art methods by over 2.5\%, while
achieving an inference latency reduction of more than 20\%, effectively
addressing the ETTA challenge.

</details>


### [155] [DepthGait: Multi-Scale Cross-Level Feature Fusion of RGB-Derived Depth and Silhouette Sequences for Robust Gait Recognition](https://arxiv.org/abs/2508.03397)
*Xinzhu Li,Juepeng Zheng,Yikun Chen,Xudong Mao,Guanghui Yue,Wei Zhou,Chenlei Lv,Ruomei Wang,Fan Zhou,Baoquan Zhao*

Main category: cs.CV

TL;DR: 提出了一种名为DepthGait的新框架，通过结合RGB图像导出的深度图和剪影来增强步态识别，并设计了多尺度跨层融合方案。


<details>
  <summary>Details</summary>
Motivation: 现有的2D表示（如二值剪影和骨架）在处理视角变化和捕获步态的精细细节方面存在不足，无法提供足够的判别性特征。

Method: 引入DepthGait框架，利用RGB图像序列显式估计深度图作为新的模态，并结合传统剪影表示。同时，开发了一种新颖的多尺度和跨层融合方案，以弥合深度图和剪影之间的模态差距。

Result: 在标准基准测试中，所提出的DepthGait方法与同类方法相比达到了最先进的性能，并在挑战性数据集上取得了令人印象深刻的平均Rank-1准确率。

Conclusion: 结合深度图和剪影，并通过专门设计的融合方案，能够显著提升步态识别的性能，有效应对视角变化并捕获更丰富的步态信息。

Abstract: Robust gait recognition requires highly discriminative representations, which
are closely tied to input modalities. While binary silhouettes and skeletons
have dominated recent literature, these 2D representations fall short of
capturing sufficient cues that can be exploited to handle viewpoint variations,
and capture finer and meaningful details of gait. In this paper, we introduce a
novel framework, termed DepthGait, that incorporates RGB-derived depth maps and
silhouettes for enhanced gait recognition. Specifically, apart from the 2D
silhouette representation of the human body, the proposed pipeline explicitly
estimates depth maps from a given RGB image sequence and uses them as a new
modality to capture discriminative features inherent in human locomotion. In
addition, a novel multi-scale and cross-level fusion scheme has also been
developed to bridge the modality gap between depth maps and silhouettes.
Extensive experiments on standard benchmarks demonstrate that the proposed
DepthGait achieves state-of-the-art performance compared to peer methods and
attains an impressive mean rank-1 accuracy on the challenging datasets.

</details>


### [156] [MedCAL-Bench: A Comprehensive Benchmark on Cold-Start Active Learning with Foundation Models for Medical Image Analysis](https://arxiv.org/abs/2508.03441)
*Ning Zhu,Xiaochuan Ma,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: 该论文提出了MedCAL-Bench，首个针对医学图像分析中基于基础模型（FM）的冷启动主动学习（CSAL）的系统性基准，评估了多种FM和CSAL策略，并揭示了它们在不同任务和数据集上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有CSAL方法多依赖于自监督学习（SSL）进行特征提取，效率低下且特征表示有限。尽管预训练的基础模型（FM）在特征提取方面表现出色，但其在CSAL中的应用鲜有研究，且缺乏相应的比较基准。

Method: 提出了MedCAL-Bench基准，系统性地评估了14个基础模型和7种CSAL策略，涵盖7个医学图像数据集（包括分类和分割任务，来自不同医学模态），并在不同的标注预算下进行测试。该基准首次同时评估了特征提取和样本选择阶段。

Result: 实验结果显示：1) 大多数基础模型是有效的CSAL特征提取器，其中DINO系列在分割任务中表现最佳；2) 这些基础模型在分割任务中的性能差异较大，但在分类任务中差异较小；3) 在不同数据集上，CSAL应考虑不同的样本选择策略，其中ALPS在分割任务中表现最佳，而RepDiv在分类任务中表现领先。

Conclusion: 基础模型在医学图像分析的冷启动主动学习中展现出强大的潜力。本研究通过MedCAL-Bench基准，为FM在CSAL任务中的特征提取和样本选择提供了深入的性能洞察，并为未来的研究提供了指导。

Abstract: Cold-Start Active Learning (CSAL) aims to select informative samples for
annotation without prior knowledge, which is important for improving annotation
efficiency and model performance under a limited annotation budget in medical
image analysis. Most existing CSAL methods rely on Self-Supervised Learning
(SSL) on the target dataset for feature extraction, which is inefficient and
limited by insufficient feature representation. Recently, pre-trained
Foundation Models (FMs) have shown powerful feature extraction ability with a
potential for better CSAL. However, this paradigm has been rarely investigated,
with a lack of benchmarks for comparison of FMs in CSAL tasks. To this end, we
propose MedCAL-Bench, the first systematic FM-based CSAL benchmark for medical
image analysis. We evaluate 14 FMs and 7 CSAL strategies across 7 datasets
under different annotation budgets, covering classification and segmentation
tasks from diverse medical modalities. It is also the first CSAL benchmark that
evaluates both the feature extraction and sample selection stages. Our
experimental results reveal that: 1) Most FMs are effective feature extractors
for CSAL, with DINO family performing the best in segmentation; 2) The
performance differences of these FMs are large in segmentation tasks, while
small for classification; 3) Different sample selection strategies should be
considered in CSAL on different datasets, with Active Learning by Processing
Surprisal (ALPS) performing the best in segmentation while RepDiv leading for
classification. The code is available at
https://github.com/HiLab-git/MedCAL-Bench.

</details>


### [157] [RAAG: Ratio Aware Adaptive Guidance](https://arxiv.org/abs/2508.03442)
*Shangwen Zhu,Qianyu Peng,Yuting Hu,Zhantao Yang,Han Zhang,Zhao Pu,Ruili Feng,Fan Cheng*

Main category: cs.CV

TL;DR: 本文揭示了流生成模型在早期采样步骤中存在的指导不稳定问题，该问题源于条件与非条件预测的相对强度（RATIO）峰值。作者提出了一种RATIO感知的自适应指导策略，通过在早期步骤中衰减指导尺度来解决此问题，从而在保持或提高生成质量的同时，实现高达3倍的采样速度提升。


<details>
  <summary>Details</summary>
Motivation: 尽管流生成模型在图像和视频合成中取得了显著进展，且分类器无关指导（CFG）已成为高保真、可控生成的标准工具，但人们对其在采样过程中，特别是在现代流管道典型的快速、低步长方案中，如何与不同阶段交互知之甚少。研究发现，在早期反向步骤中存在一个根本性不稳定性，即对指导尺度的敏感性。

Method: 本文揭示并分析了早期反向步骤中条件预测与非条件预测相对强度（RATIO）的显著峰值，指出这是数据分布固有的，与模型架构无关，并在与强指导结合时导致指数级误差放大。为解决此问题，提出了一种简单、理论上合理且RATIO感知的自适应指导策略，该策略根据不断演变的RATIO，通过封闭形式的指数衰减，自动抑制早期步骤的指导尺度。该方法轻量级，无需额外推理开销，并兼容标准流框架。

Result: 实验证明，所提出的方法在最先进的图像（SD3.5, Lumina）和视频（WAN2.1）模型上，实现了高达3倍的采样速度提升，同时保持或提高了生成质量、鲁棒性和语义对齐。广泛的消融研究进一步证实了该策略在不同模型、数据集和超参数下的通用性和稳定性。

Conclusion: 研究结果强调了分步指导自适应在充分发挥快速流生成模型潜力中的关键作用。

Abstract: Flow-based generative models have recently achieved remarkable progress in
image and video synthesis, with classifier-free guidance (CFG) becoming the
standard tool for high-fidelity, controllable generation. However, despite
their practical success, little is known about how guidance interacts with
different stages of the sampling process-especially in the fast, low-step
regimes typical of modern flow-based pipelines. In this work, we uncover and
analyze a fundamental instability: the earliest reverse steps are acutely
sensitive to the guidance scale, owing to a pronounced spike in the relative
strength (RATIO) of conditional to unconditional predictions. Through rigorous
theoretical analysis and empirical validation, we show that this RATIO spike is
intrinsic to the data distribution, independent of the model architecture, and
causes exponential error amplification when paired with strong guidance. To
address this, we propose a simple, theoretically grounded, RATIO-aware adaptive
guidance schedule that automatically dampens the guidance scale at early steps
based on the evolving RATIO, using a closed-form exponential decay. Our method
is lightweight, requires no additional inference overhead, and is compatible
with standard flow frameworks. Experiments across state-of-the-art image
(SD3.5, Lumina) and video (WAN2.1) models demonstrate that our approach enables
up to 3x faster sampling while maintaining or improving generation quality,
robustness, and semantic alignment. Extensive ablation studies further confirm
the generality and stability of our schedule across models, datasets, and
hyperparameters. Our findings highlight the critical role of stepwise guidance
adaptation in unlocking the full potential of fast flow-based generative
models.

</details>


### [158] [CoPS: Conditional Prompt Synthesis for Zero-Shot Anomaly Detection](https://arxiv.org/abs/2508.03447)
*Qiyu Chen,Zhen Qu,Wei Luo,Haiming Yao,Yunkang Cao,Yuxin Jiang,Yinan Duan,Huiyuan Luo,Chengkan Lv,Zhengtao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为条件提示合成（CoPS）的新框架，通过合成动态提示并融入视觉特征，显著提升了零样本异常检测（ZSAD）的性能，解决了现有方法中静态提示和稀疏标签导致的泛化性差和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有零样本异常检测（ZSAD）中的提示学习面临挑战：(i) 静态可学习标记难以捕捉正常和异常状态的连续多样模式，限制了对未见类别的泛化能力；(ii) 固定文本标签提供过于稀疏的类别信息，使模型容易过拟合到特定语义子空间。

Method: 本文提出条件提示合成（CoPS）框架，通过以下方式合成动态提示：(i) 从细粒度补丁特征中提取代表性的正常和异常原型并明确注入提示，实现自适应状态建模；(ii) 利用变分自编码器（VAE）对语义图像特征进行建模，并将多样的类别标记隐式融合到提示中，以解决类别标签稀疏性问题；(iii) 整合空间感知对齐机制。

Result: CoPS在13个工业和医学数据集的分类和分割任务中，AUROC指标均超越现有最先进方法2.5%。

Conclusion: CoPS通过合成动态的、视觉特征条件化的提示，有效解决了零样本异常检测中提示学习的泛化性和过拟合问题，在多种数据集上实现了卓越的异常检测性能。

Abstract: Recently, large pre-trained vision-language models have shown remarkable
performance in zero-shot anomaly detection (ZSAD). With fine-tuning on a single
auxiliary dataset, the model enables cross-category anomaly detection on
diverse datasets covering industrial defects and medical lesions. Compared to
manually designed prompts, prompt learning eliminates the need for expert
knowledge and trial-and-error. However, it still faces the following
challenges: (i) static learnable tokens struggle to capture the continuous and
diverse patterns of normal and anomalous states, limiting generalization to
unseen categories; (ii) fixed textual labels provide overly sparse category
information, making the model prone to overfitting to a specific semantic
subspace. To address these issues, we propose Conditional Prompt Synthesis
(CoPS), a novel framework that synthesizes dynamic prompts conditioned on
visual features to enhance ZSAD performance. Specifically, we extract
representative normal and anomaly prototypes from fine-grained patch features
and explicitly inject them into prompts, enabling adaptive state modeling.
Given the sparsity of class labels, we leverage a variational autoencoder to
model semantic image features and implicitly fuse varied class tokens into
prompts. Additionally, integrated with our spatially-aware alignment mechanism,
extensive experiments demonstrate that CoPS surpasses state-of-the-art methods
by 2.5% AUROC in both classification and segmentation across 13 industrial and
medical datasets. Code will be available at https://github.com/cqylunlun/CoPS.

</details>


### [159] [Video Demoireing using Focused-Defocused Dual-Camera System](https://arxiv.org/abs/2508.03449)
*Xuan Dong,Xiangyuan Sun,Xia Wang,Jian Song,Ya Li,Weixin Li*

Main category: cs.CV

TL;DR: 本文提出一种双摄像头去摩尔纹框架，利用散焦视频辅助聚焦视频的去摩尔纹，以解决区分摩尔纹与真实纹理以及保持色调和时间一致性的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有单摄像头去摩尔纹方法面临两大挑战：1) 难以区分摩尔纹与视觉上相似的真实纹理；2) 在去除摩尔纹伪影的同时难以保持色调一致性和时间连贯性。

Method: 研究者提出一个双摄像头框架，同步捕捉同一场景的视频：一个聚焦视频（保留高质量纹理但可能有摩尔纹）和一个散焦视频（摩尔纹显著减少但纹理模糊）。散焦视频用于帮助区分摩尔纹和真实纹理，以指导聚焦视频的去摩尔纹。去摩尔纹流程是逐帧进行的，首先通过光流对齐聚焦和散焦帧；然后，利用对齐后的散焦帧，通过多尺度CNN和多维度训练损失来指导聚焦帧的去摩尔纹；最后，使用联合双边滤波器，以CNN的去摩尔纹结果为指导，对输入聚焦帧进行滤波，以保持色调和时间一致性。

Result: 实验结果表明，所提出的框架在很大程度上优于现有的图像和视频去摩尔纹方法。

Conclusion: 双摄像头框架通过利用散焦视频的辅助，有效解决了去摩尔纹中区分摩尔纹和真实纹理的难题，并在保持色调和时间一致性方面表现出色，取得了领先的去摩尔纹效果。

Abstract: Moire patterns, unwanted color artifacts in images and videos, arise from the
interference between spatially high-frequency scene contents and the spatial
discrete sampling of digital cameras. Existing demoireing methods primarily
rely on single-camera image/video processing, which faces two critical
challenges: 1) distinguishing moire patterns from visually similar real
textures, and 2) preserving tonal consistency and temporal coherence while
removing moire artifacts. To address these issues, we propose a dual-camera
framework that captures synchronized videos of the same scene: one in focus
(retaining high-quality textures but may exhibit moire patterns) and one
defocused (with significantly reduced moire patterns but blurred textures). We
use the defocused video to help distinguish moire patterns from real texture,
so as to guide the demoireing of the focused video. We propose a frame-wise
demoireing pipeline, which begins with an optical flow based alignment step to
address any discrepancies in displacement and occlusion between the focused and
defocused frames. Then, we leverage the aligned defocused frame to guide the
demoireing of the focused frame using a multi-scale CNN and a multi-dimensional
training loss. To maintain tonal and temporal consistency, our final step
involves a joint bilateral filter to leverage the demoireing result from the
CNN as the guide to filter the input focused frame to obtain the final output.
Experimental results demonstrate that our proposed framework largely
outperforms state-of-the-art image and video demoireing methods.

</details>


### [160] [AVPDN: Learning Motion-Robust and Scale-Adaptive Representations for Video-Based Polyp Detection](https://arxiv.org/abs/2508.03458)
*Zilin Chen,Shengnan Lu*

Main category: cs.CV

TL;DR: 提出了一种名为AVPDN的自适应视频息肉检测网络，通过特征交互增强和尺度感知上下文集成模块，有效解决了结肠镜视频中快速相机移动导致的背景噪声和误报问题，提高了息肉检测的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌的早期和中期诊断中，息肉的准确检测至关重要。动态结肠镜视频提供更全面的视觉信息，但快速的相机移动引入大量背景噪声，破坏场景结构，增加误报风险。现有方法难以有效处理这些挑战。

Method: 提出了自适应视频息肉检测网络（AVPDN），包含两个核心组件：自适应特征交互与增强（AFIA）模块和尺度感知上下文集成（SACI）模块。AFIA模块采用三分支架构，通过密集自注意力进行全局上下文建模，稀疏自注意力减轻低查询-键相似性影响，以及通道混洗操作促进分支间信息交换，以增强特征表示。SACI模块利用不同感受野的扩张卷积捕获多尺度上下文信息，增强模型去噪能力，从而加强多尺度特征集成。

Result: 在多个具有挑战性的公共基准数据集上进行的实验证明，所提出的方法具有有效性和泛化能力，在基于视频的息肉检测任务中取得了具有竞争力的性能。

Conclusion: AVPDN是一个鲁棒的框架，能够有效应对结肠镜视频中快速相机移动带来的挑战，实现了多尺度息肉的准确检测，为结直肠癌的早期诊断提供了有力的技术支持。

Abstract: Accurate detection of polyps is of critical importance for the early and
intermediate stages of colorectal cancer diagnosis. Compared to static images,
dynamic colonoscopy videos provide more comprehensive visual information, which
can facilitate the development of effective treatment plans. However, unlike
fixed-camera recordings, colonoscopy videos often exhibit rapid camera
movement, introducing substantial background noise that disrupts the structural
integrity of the scene and increases the risk of false positives. To address
these challenges, we propose the Adaptive Video Polyp Detection Network
(AVPDN), a robust framework for multi-scale polyp detection in colonoscopy
videos. AVPDN incorporates two key components: the Adaptive Feature Interaction
and Augmentation (AFIA) module and the Scale-Aware Context Integration (SACI)
module. The AFIA module adopts a triple-branch architecture to enhance feature
representation. It employs dense self-attention for global context modeling,
sparse self-attention to mitigate the influence of low query-key similarity in
feature aggregation, and channel shuffle operations to facilitate inter-branch
information exchange. In parallel, the SACI module is designed to strengthen
multi-scale feature integration. It utilizes dilated convolutions with varying
receptive fields to capture contextual information at multiple spatial scales,
thereby improving the model's denoising capability. Experiments conducted on
several challenging public benchmarks demonstrate the effectiveness and
generalization ability of the proposed method, achieving competitive
performance in video-based polyp detection tasks.

</details>


### [161] [IKOD: Mitigating Visual Attention Degradation in Large Vision-Language Models](https://arxiv.org/abs/2508.03469)
*Jiabing Yang,Chenhang Cui,Yiyang Zhou,Yixiang Chen,Peng Xia,Ying Wei,Tao Yu,Yan Huang,Liang Wang*

Main category: cs.CV

TL;DR: 本文研究了大型视觉-语言模型（LVLMs）中幻觉（hallucinations）随序列长度增加而加剧的现象，发现这是由于模型对视觉输入的注意力随生成序列增长而减弱所致。作者提出了IKOD（Image attention-guided Key-value merging cOllaborative Decoding）方法，通过键值合并（key-value merging）和协同解码来维持视觉注意力，有效抑制幻觉且成本较低。


<details>
  <summary>Details</summary>
Motivation: LVLMs在视觉和语言整合时常出现与图像不符的“幻觉”输出。现有解决幻觉的方法存在计算成本高或数据标注昂贵等局限性。研究发现LVLMs存在幻觉随序列长度增加而加剧的长期偏差，但其根本原因尚不清楚，这促使作者深入探究其与视觉注意力的关系。

Method: 通过对LVLMs中注意力机制的广泛研究，作者分析了长期偏差与视觉注意力的关系。他们发现，随着生成序列的增长，模型对视觉输入的注意力会持续减弱。基于此洞察，提出IKOD协同解码策略。该方法通过键值合并从具有更高图像注意力的短序列中获取逻辑值（logits），并将其与原始解码的逻辑值结合，从而有效缓解注意力衰减并抑制幻觉。

Result: 在幻觉和综合基准测试中，IKOD表现出卓越的有效性，能够显著减轻幻觉并提升LVLMs的综合能力。重要的是，IKOD无需额外训练或外部工具，是一个轻量且高效的框架，可适用于多种模型。

Conclusion: IKOD通过解决LVLMs中视觉注意力随序列增长而衰减的问题，提供了一种有效且低成本的方法来抑制幻觉并提升模型性能，无需额外的训练或工具，使其成为一个有前景的通用框架。

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated
significant progress across multiple domains. However, these models still face
the inherent challenge of integrating vision and language for collaborative
inference, which often leads to "hallucinations", outputs that are not grounded
in the corresponding images. Many efforts have been made to address these
issues, but each comes with its own limitations, such as high computational
cost or expensive dataset annotation. Recent research shows that LVLMs exhibit
a long-term bias where hallucinations increase as the sequence length grows,
yet the underlying cause remains poorly understood. Building on extensive
research into attention mechanisms in LVLMs, we analyze the relationship
between this long-term bias and visual attention. In our research, we identify
a consistent phenomenon in current LVLMs: the model's attention to visual input
diminishes as the generated sequence grows, which we hypothesize to be a key
factor contributing to observed increasing hallucinations. Based on these
insights, we propose Image attention-guided Key-value merging cOllaborative
Decoding (IKOD), a collaborative decoding strategy generating more
image-focused sequences. This method derives logits from shorter sequences with
higher image attention through key-value merging and combines them with those
from the original decoding, effectively mitigating attention degradation and
suppressing hallucinations while not incurring too much inference cost.
Extensive experiments on both hallucination and comprehensive benchmarks
demonstrate IKOD's superior effectiveness in mitigating hallucinations and
improving comprehensive capacities for LVLMs. Importantly, IKOD requires no
additional training or external tools, making it a lightweight and efficient
framework applicable to various models.

</details>


### [162] [LRQ-DiT: Log-Rotation Post-Training Quantization of Diffusion Transformers for Text-to-Image Generation](https://arxiv.org/abs/2508.03485)
*Lianwei Yang,Haokun Lin,Tianchen Zhao,Yichen Wu,Hongyu Zhu,Ruiqi Xie,Zhenan Sun,Yu Wang,Qingyi Gu*

Main category: cs.CV

TL;DR: 针对Diffusion Transformers (DiTs)模型在极低比特下的后训练量化(PTQ)性能下降问题，本文提出了LRQ-DiT框架，通过双对数量化(TLQ)解决权重长尾分布，并引入自适应旋转方案(ARS)处理激活异常值，实现了DiT模型的低比特量化且保持图像质量，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: DiT模型在文本到图像生成中表现出色，但计算成本高、参数量大，不适用于资源受限场景。PTQ是降低内存和加速推理的有效方案，但在极低比特设置下现有PTQ方法性能严重下降。主要障碍是：1) 模型权重呈高斯状长尾分布，导致均匀量化误差大；2) 激活存在两种异常值（轻微和显著），干扰激活量化。

Method: 提出LRQ-DiT高效准确的PTQ框架。引入双对数量化(Twin-Log Quantization, TLQ)作为基于对数的权重处理方法，更好地匹配权重分布并减少量化误差。提出自适应旋转方案(Adaptive Rotation Scheme, ARS)，根据激活波动动态应用Hadamard或异常值感知旋转，有效缓解两种激活异常值的影响。

Result: 在PixArt和FLUX模型上，以及COCO、MJHQ和sDCI数据集上进行了评估。LRQ-DiT实现了DiT模型的低比特量化，同时保持了图像质量，性能优于现有的PTQ基线方法。

Conclusion: LRQ-DiT是一个有效且准确的DiT模型PTQ框架，它通过创新的量化和异常值处理策略，成功解决了DiT模型在极低比特量化中的挑战，实现了性能与效率的平衡。

Abstract: Diffusion Transformers (DiTs) have achieved impressive performance in
text-to-image generation. However, their high computational cost and large
parameter sizes pose significant challenges for usage in resource-constrained
scenarios. Post-training quantization (PTQ) is a promising solution to reduce
memory usage and accelerate inference, but existing PTQ methods suffer from
severe performance degradation under extreme low-bit settings. We identify two
key obstacles to low-bit post-training quantization for DiT models: (1) model
weights follow a Gaussian-like distribution with long tails, causing uniform
quantization to poorly allocate intervals and leading to significant errors;
(2) two types of activation outliers: (i) Mild Outliers with slightly elevated
values, and (ii) Salient Outliers with large magnitudes concentrated in
specific channels, which disrupt activation quantization. To address these
issues, we propose LRQ-DiT, an efficient and accurate PTQ framework. We
introduce Twin-Log Quantization (TLQ), a log-based method that aligns well with
the weight distribution and reduces quantization errors. We also propose an
Adaptive Rotation Scheme (ARS) that dynamically applies Hadamard or
outlier-aware rotations based on activation fluctuation, effectively mitigating
the impact of both types of outliers. We evaluate LRQ-DiT on PixArt and FLUX
under various bit-width settings, and validate the performance on COCO, MJHQ,
and sDCI datasets. LRQ-DiT achieves low-bit quantization of DiT models while
preserving image quality, outperforming existing PTQ baselines.

</details>


### [163] [ParticleSAM: Small Particle Segmentation for Material Quality Monitoring in Recycling Processes](https://arxiv.org/abs/2508.03490)
*Yu Zhou,Pelle Thielmann,Ayush Chamoli,Bruno Mirbach,Didier Stricker,Jason Rambach*

Main category: cs.CV

TL;DR: 该论文提出了ParticleSAM，一个针对建筑材料中密集小颗粒图像的分割基础模型SAM的改进版本，并创建了一个新的密集多颗粒数据集，用于自动化视觉材料质量控制。


<details>
  <summary>Details</summary>
Motivation: 建筑行业资源消耗巨大，回收材料潜力高，但骨料质量监测仍依赖人工方法。现有视觉分割方法不适用于含有数百个小颗粒的图像，因此需要更快、更高效的解决方案。

Method: 提出ParticleSAM，将分割基础模型SAM适应于小而密集的物体图像。同时，通过自动化数据生成和标注流程，从孤立颗粒图像中模拟创建了一个新的密集多颗粒数据集。

Result: 实验结果表明，与原始SAM方法相比，ParticleSAM在定量和定性实验中都展现出优势。

Conclusion: ParticleSAM方法在建筑材料质量控制自动化方面具有潜在价值，并可应用于其他需要小颗粒分割的领域。新创建的数据集可作为视觉材料质量控制自动化的基准。

Abstract: The construction industry represents a major sector in terms of resource
consumption. Recycled construction material has high reuse potential, but
quality monitoring of the aggregates is typically still performed with manual
methods. Vision-based machine learning methods could offer a faster and more
efficient solution to this problem, but existing segmentation methods are by
design not directly applicable to images with hundreds of small particles. In
this paper, we propose ParticleSAM, an adaptation of the segmentation
foundation model to images with small and dense objects such as the ones often
encountered in construction material particles. Moreover, we create a new dense
multi-particle dataset simulated from isolated particle images with the
assistance of an automated data generation and labeling pipeline. This dataset
serves as a benchmark for visual material quality control automation while our
segmentation approach has the potential to be valuable in application areas
beyond construction where small-particle segmentation is needed. Our
experimental results validate the advantages of our method by comparing to the
original SAM method both in quantitative and qualitative experiments.

</details>


### [164] [Quality Versus Sparsity in Image Recovery by Dictionary Learning Using Iterative Shrinkage](https://arxiv.org/abs/2508.03492)
*Mohammadsadegh Khoshghiaferezaee,Moritz Krauth,Shima Shabani,Michael Breuß*

Main category: cs.CV

TL;DR: 该研究探讨了稀疏字典学习（SDL）在图像恢复中的应用，重点分析了不同优化方法下解的稀疏度，并指出高稀疏度通常不会损害恢复质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏字典学习（SDL）是图像处理（如图像恢复）中的基础技术，可被视为非光滑优化问题。迭代收缩方法是解决这类问题的有力算法。稀疏性对于高效处理或存储至关重要，但问题在于SDL中应强制执行何种程度的稀疏性才不会损害恢复质量。

Method: 本文关注使用多种优化方法获得的解的稀疏性，并分析了不同方法如何影响稀疏性。

Result: 研究发现，存在不同的稀疏度方案，这取决于所使用的优化方法。此外，高稀疏度通常不会损害恢复质量，即使恢复的图像与学习数据库差异很大。

Conclusion: 在稀疏字典学习中，可以通过多种优化方法实现不同的稀疏度，并且高稀疏度通常不会影响图像恢复的质量。

Abstract: Sparse dictionary learning (SDL) is a fundamental technique that is useful
for many image processing tasks. As an example we consider here image recovery,
where SDL can be cast as a nonsmooth optimization problem. For this kind of
problems, iterative shrinkage methods represent a powerful class of algorithms
that are subject of ongoing research. Sparsity is an important property of the
learned solutions, as exactly the sparsity enables efficient further processing
or storage. The sparsity implies that a recovered image is determined as a
combination of a number of dictionary elements that is as low as possible.
Therefore, the question arises, to which degree sparsity should be enforced in
SDL in order to not compromise recovery quality. In this paper we focus on the
sparsity of solutions that can be obtained using a variety of optimization
methods. It turns out that there are different sparsity regimes depending on
the method in use. Furthermore, we illustrate that high sparsity does in
general not compromise recovery quality, even if the recovered image is quite
different from the learning database.

</details>


### [165] [Prototype-Enhanced Confidence Modeling for Cross-Modal Medical Image-Report Retrieval](https://arxiv.org/abs/2508.03494)
*Shreyank N Gowda,Xiaobo Jin,Christian Wagner*

Main category: cs.CV

TL;DR: 该论文提出了原型增强置信度建模（PECM）框架，用于解决医学图像与报告的跨模态检索中数据歧义性和变异性问题，通过引入多级原型和双流置信度估计，显著提高了检索精度和一致性。


<details>
  <summary>Details</summary>
Motivation: 在医学跨模态检索任务中，准确对齐医学图像和文本报告至关重要，但由于医学数据固有的歧义性和变异性，现有模型难以捕捉放射学数据中细致的多级语义关系，导致检索结果不可靠。

Method: 本文提出了原型增强置信度建模（PECM）框架。该框架为每种模态引入了多级原型，以更好地捕捉语义变异性并增强检索鲁棒性。PECM采用双流置信度估计，利用原型相似性分布和自适应加权机制来控制高不确定性数据对检索排名的影响。

Result: 将所提出的方法应用于放射学图像-报告数据集，PECM在检索精度和一致性方面取得了显著改进，有效处理了数据歧义性，并提升了复杂临床场景中的可靠性。在多个不同数据集和任务（包括全监督和零样本检索）上，性能提升高达10.17%，建立了新的最先进水平。

Conclusion: PECM框架通过有效处理医学数据的歧义性和变异性，显著提高了跨模态检索的精度和可靠性，在复杂临床场景中表现出色，并达到了新的最先进性能。

Abstract: In cross-modal retrieval tasks, such as image-to-report and report-to-image
retrieval, accurately aligning medical images with relevant text reports is
essential but challenging due to the inherent ambiguity and variability in
medical data. Existing models often struggle to capture the nuanced,
multi-level semantic relationships in radiology data, leading to unreliable
retrieval results. To address these issues, we propose the Prototype-Enhanced
Confidence Modeling (PECM) framework, which introduces multi-level prototypes
for each modality to better capture semantic variability and enhance retrieval
robustness. PECM employs a dual-stream confidence estimation that leverages
prototype similarity distributions and an adaptive weighting mechanism to
control the impact of high-uncertainty data on retrieval rankings. Applied to
radiology image-report datasets, our method achieves significant improvements
in retrieval precision and consistency, effectively handling data ambiguity and
advancing reliability in complex clinical scenarios. We report results on
multiple different datasets and tasks including fully supervised and zero-shot
retrieval obtaining performance gains of up to 10.17%, establishing in new
state-of-the-art.

</details>


### [166] [EditGarment: An Instruction-Based Garment Editing Dataset Constructed with Automated MLLM Synthesis and Semantic-Aware Evaluation](https://arxiv.org/abs/2508.03497)
*Deqiang Yin,Junyi Guo,Huanda Lu,Fangyu Wu,Dongming Lu*

Main category: cs.CV

TL;DR: 该论文提出了一个自动化流程，用于构建高质量的指令式服装编辑数据集（EditGarment），解决了现有数据稀缺和多模态大模型（MLLMs）在时尚领域应用受限的问题。


<details>
  <summary>Details</summary>
Motivation: 指令式服装编辑在时尚设计和定制中有广泛应用，但其发展受限于高质量指令-图像对的稀缺性。手动标注成本高昂且难以扩展，而现有MLLMs在服装编辑数据合成方面存在指令建模不精确和缺乏时尚特定监督信号的问题。

Method: 首先，定义了六种与实际时尚工作流程对齐的编辑指令类别，以指导平衡和多样化的指令-图像三元组生成。其次，引入了“时尚编辑得分”（Fashion Edit Score），这是一个语义感知的评估指标，用于捕捉服装属性之间的语义依赖关系，并在数据构建过程中提供可靠的监督。

Result: 利用该自动化流程，共构建了52,257个候选三元组，并从中保留了20,596个高质量三元组，从而构建了EditGarment数据集。EditGarment是第一个专为独立服装编辑量身定制的指令式数据集。

Conclusion: 该论文通过提出的自动化流程，成功构建了高质量、指令式的服装编辑数据集EditGarment，有效解决了服装编辑领域数据稀缺的挑战，并为未来的研究提供了可靠的数据基础。

Abstract: Instruction-based garment editing enables precise image modifications via
natural language, with broad applications in fashion design and customization.
Unlike general editing tasks, it requires understanding garment-specific
semantics and attribute dependencies. However, progress is limited by the
scarcity of high-quality instruction-image pairs, as manual annotation is
costly and hard to scale. While MLLMs have shown promise in automated data
synthesis, their application to garment editing is constrained by imprecise
instruction modeling and a lack of fashion-specific supervisory signals. To
address these challenges, we present an automated pipeline for constructing a
garment editing dataset. We first define six editing instruction categories
aligned with real-world fashion workflows to guide the generation of balanced
and diverse instruction-image triplets. Second, we introduce Fashion Edit
Score, a semantic-aware evaluation metric that captures semantic dependencies
between garment attributes and provides reliable supervision during
construction. Using this pipeline, we construct a total of 52,257 candidate
triplets and retain 20,596 high-quality triplets to build EditGarment, the
first instruction-based dataset tailored to standalone garment editing. The
project page is https://yindq99.github.io/EditGarment-project/.

</details>


### [167] [MAUP: Training-free Multi-center Adaptive Uncertainty-aware Prompting for Cross-domain Few-shot Medical Image Segmentation](https://arxiv.org/abs/2508.03511)
*Yazhou Zhu,Haofeng Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MAUP的免训练跨域少样本医学图像分割（CD-FSMIS）模型，通过多中心自适应不确定性感知提示策略，将自然图像基础模型SAM应用于医学图像分割，并在DINOv2特征编码器辅助下实现了精确分割。


<details>
  <summary>Details</summary>
Motivation: 现有CD-FSMIS模型依赖于大量源域训练，降低了模型的通用性和部署便捷性。随着大型视觉模型在自然图像领域的进展，研究者希望探索一种无需额外训练即可适应CD-FSMIS任务的解决方案。

Method: 提出Multi-center Adaptive Uncertainty-aware Prompting (MAUP) 策略来适应基础模型SAM。MAUP包含三个关键创新点：1) 基于K-means聚类的多中心提示生成，实现全面的空间覆盖；2) 不确定性感知提示选择，聚焦于挑战区域；3) 自适应提示优化，根据目标区域复杂性动态调整。模型利用预训练的DINOv2特征编码器。

Result: MAUP在三个医学数据集上实现了精确的分割结果，且无需任何额外训练，性能优于多个传统CD-FSMIS模型和免训练FSMIS模型。

Conclusion: MAUP提供了一种创新的免训练方法，通过有效适应自然图像基础模型SAM，解决了CD-FSMIS中模型通用性和部署的挑战，为医学图像分割提供了高效且便捷的解决方案。

Abstract: Cross-domain Few-shot Medical Image Segmentation (CD-FSMIS) is a potential
solution for segmenting medical images with limited annotation using knowledge
from other domains. The significant performance of current CD-FSMIS models
relies on the heavily training procedure over other source medical domains,
which degrades the universality and ease of model deployment. With the
development of large visual models of natural images, we propose a
training-free CD-FSMIS model that introduces the Multi-center Adaptive
Uncertainty-aware Prompting (MAUP) strategy for adapting the foundation model
Segment Anything Model (SAM), which is trained with natural images, into the
CD-FSMIS task. To be specific, MAUP consists of three key innovations: (1)
K-means clustering based multi-center prompts generation for comprehensive
spatial coverage, (2) uncertainty-aware prompts selection that focuses on the
challenging regions, and (3) adaptive prompt optimization that can dynamically
adjust according to the target region complexity. With the pre-trained DINOv2
feature encoder, MAUP achieves precise segmentation results across three
medical datasets without any additional training compared with several
conventional CD-FSMIS models and training-free FSMIS model. The source code is
available at: https://github.com/YazhouZhu19/MAUP.

</details>


### [168] [Distribution-aware Knowledge Unification and Association for Non-exemplar Lifelong Person Re-identification](https://arxiv.org/abs/2508.03516)
*Shiben Liu,Mingyue Xu,Huijie Fan,Qiang Wang,Yandong Tang,Zhi Han*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的分布感知知识统一与关联（DKUA）框架，用于终身行人重识别（LReID），通过域风格建模、自适应知识整合、统一知识关联和基于分布的知识迁移，有效平衡旧知识的保留与新信息的适应，显著提升了抗遗忘和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有终身行人重识别（LReID）方法通常采用知识蒸馏来对齐表示，但忽略了特定分布感知和跨域统一知识学习这两个关键方面，导致在平衡旧知识保留和新信息适应方面面临挑战。

Method: 本文提出了分布感知知识统一与关联（DKUA）框架：
1.  **分布感知模型**：对每个实例进行域风格建模，将当前域的实例级表示转换为具有不同域风格的域特定表示，以保留学习到的知识。
2.  **自适应知识整合（AKC）**：动态生成统一表示作为跨域表示中心。
3.  **统一知识关联（UKA）**：利用统一表示作为桥梁，显式建模域间关联，减少域间差距，进一步缓解遗忘。
4.  **基于分布的知识迁移（DKT）**：防止当前域分布偏离跨域分布中心，提高适应能力。

Result: 实验结果表明，DKUA框架在抗遗忘和泛化能力方面，平均mAP和R@1分别比现有方法提高了7.6%和5.3%。

Conclusion: 所提出的DKUA框架通过其独特的分布感知和知识统一机制，有效解决了终身行人重识别中知识遗忘和泛化能力的挑战，显著优于现有方法，证明了其在平衡新旧知识方面的有效性。

Abstract: Lifelong person re-identification (LReID) encounters a key challenge:
balancing the preservation of old knowledge with adaptation to new information.
Existing LReID methods typically employ knowledge distillation to enforce
representation alignment. However, these approaches ignore two crucial aspects:
specific distribution awareness and cross-domain unified knowledge learning,
both of which are essential for addressing this challenge. To overcome these
limitations, we propose a novel distribution-aware knowledge unification and
association (DKUA) framework where domain-style modeling is performed for each
instance to propagate domain-specific representations, enhancing
anti-forgetting and generalization capacity. Specifically, we design a
distribution-aware model to transfer instance-level representations of the
current domain into the domain-specific representations with the different
domain styles, preserving learned knowledge without storing old samples. Next,
we propose adaptive knowledge consolidation (AKC) to dynamically generate the
unified representation as a cross-domain representation center. To further
mitigate forgetting, we develop a unified knowledge association (UKA)
mechanism, which explores the unified representation as a bridge to explicitly
model inter-domain associations, reducing inter-domain gaps. Finally,
distribution-based knowledge transfer (DKT) is proposed to prevent the current
domain distribution from deviating from the cross-domain distribution center,
improving adaptation capacity. Experimental results show our DKUA outperforms
the existing methods by 7.6%/5.3% average mAP/R@1 improvement on
anti-forgetting and generalization capacity, respectively. Our code will be
publicly released.

</details>


### [169] [Semantic Mosaicing of Histo-Pathology Image Fragments using Visual Foundation Models](https://arxiv.org/abs/2508.03524)
*Stefan Brandstätter,Maximilian Köller,Philipp Seeböck,Alissa Blessing,Felicitas Oberndorfer,Svitlana Pochepnia,Helmut Prosch,Georg Langs*

Main category: cs.CV

TL;DR: 针对组织病理学中大尺寸样本的拼接挑战，本文提出了SemanticStitcher，它利用视觉病理学基础模型的潜在特征表示进行语义匹配和鲁棒的姿态估计，从而实现全切片图像（WMS）的自动拼接，并优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 组织病理学中的组织样本通常大于标准显微镜载玻片，需要拼接多个碎片才能处理肿瘤等完整结构。自动化拼接是规模化分析的先决条件，但现有方法面临组织丢失、形态畸变、染色不一致、区域缺失或组织边缘磨损等挑战，限制了基于边界形状匹配的传统拼接方法重建人工全切片图像的能力。

Method: 本文引入了SemanticStitcher。它利用从视觉病理学基础模型中提取的潜在特征表示来识别不同碎片中的相邻区域。通过大量语义匹配候选点进行鲁棒的姿态估计，从而将多个碎片拼接成全切片图像（WMS）。

Result: 在三个不同的组织病理学数据集上的实验表明，SemanticStitcher能够生成鲁棒的全切片图像拼接，并且在正确的边界匹配方面始终优于现有技术。

Conclusion: SemanticStitcher通过利用来自视觉病理学基础模型的语义特征，提供了一种鲁棒且性能卓越的自动化全切片图像重建解决方案，有效克服了传统边界匹配方法的局限性，提升了组织病理学分析的效率和准确性。

Abstract: In histopathology, tissue samples are often larger than a standard microscope
slide, making stitching of multiple fragments necessary to process entire
structures such as tumors. Automated stitching is a prerequisite for scaling
analysis, but is challenging due to possible tissue loss during preparation,
inhomogeneous morphological distortion, staining inconsistencies, missing
regions due to misalignment on the slide, or frayed tissue edges. This limits
state-of-the-art stitching methods using boundary shape matching algorithms to
reconstruct artificial whole mount slides (WMS). Here, we introduce
SemanticStitcher using latent feature representations derived from a visual
histopathology foundation model to identify neighboring areas in different
fragments. Robust pose estimation based on a large number of semantic matching
candidates derives a mosaic of multiple fragments to form the WMS. Experiments
on three different histopathology datasets demonstrate that SemanticStitcher
yields robust WMS mosaicing and consistently outperforms the state of the art
in correct boundary matches.

</details>


### [170] [CoEmoGen: Towards Semantically-Coherent and Scalable Emotional Image Content Generation](https://arxiv.org/abs/2508.03535)
*Kaishen Yuan,Yuting Zhang,Shang Gao,Yijie Zhu,Wenshuo Chen,Yutao Yue*

Main category: cs.CV

TL;DR: 本文提出CoEmoGen，一个新颖的情感图像内容生成（EICG）框架，通过利用多模态大语言模型（MLLMs）提供高质量语义指导和设计分层低秩适应（HiLoRA）模块，解决了现有方法在生成抽象情感图像时存在的语义不连贯、模糊和可扩展性差的问题，并构建了大规模情感艺术图像数据集EmoArt。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在生成具象概念方面表现出色，但在处理抽象情感时面临挑战。专门为EICG设计的方法过度依赖词级属性标签，导致语义不连贯、模糊且可扩展性有限。

Method: CoEmoGen主要采用两种方法：1. 利用多模态大语言模型（MLLMs）构建高质量、专注于情感触发内容的图像描述，以提供丰富的上下文语义指导。2. 借鉴心理学洞察，设计分层低秩适应（HiLoRA）模块，协同建模情感极性共享的低级特征和情感特有的高级语义。

Result: CoEmoGen在情感忠实度和语义连贯性方面表现出卓越的性能，并通过定量、定性和用户研究得到了广泛验证。此外，本文还策划并发布了大规模情感艺术图像数据集EmoArt，以直观展示其可扩展性。

Conclusion: CoEmoGen通过创新的语义指导和特征建模方法，显著提升了情感图像内容生成的语义连贯性和可扩展性，有效克服了现有模型的局限性，并为情感驱动的艺术创作提供了宝贵资源。

Abstract: Emotional Image Content Generation (EICG) aims to generate semantically clear
and emotionally faithful images based on given emotion categories, with broad
application prospects. While recent text-to-image diffusion models excel at
generating concrete concepts, they struggle with the complexity of abstract
emotions. There have also emerged methods specifically designed for EICG, but
they excessively rely on word-level attribute labels for guidance, which suffer
from semantic incoherence, ambiguity, and limited scalability. To address these
challenges, we propose CoEmoGen, a novel pipeline notable for its semantic
coherence and high scalability. Specifically, leveraging multimodal large
language models (MLLMs), we construct high-quality captions focused on
emotion-triggering content for context-rich semantic guidance. Furthermore,
inspired by psychological insights, we design a Hierarchical Low-Rank
Adaptation (HiLoRA) module to cohesively model both polarity-shared low-level
features and emotion-specific high-level semantics. Extensive experiments
demonstrate CoEmoGen's superiority in emotional faithfulness and semantic
coherence from quantitative, qualitative, and user study perspectives. To
intuitively showcase scalability, we curate EmoArt, a large-scale dataset of
emotionally evocative artistic images, providing endless inspiration for
emotion-driven artistic creation. The dataset and code are available at
https://github.com/yuankaishen2001/CoEmoGen.

</details>


### [171] [Quality-Aware Language-Conditioned Local Auto-Regressive Anomaly Synthesis and Detection](https://arxiv.org/abs/2508.03539)
*Long Qian,Bingke Zhu,Yingying Chen,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: ARAS是一种语言条件自回归异常合成方法，通过潜在编辑精确注入局部缺陷，解决了现有方法的结构缺陷和效率问题。结合QARAD框架，它通过动态加权强调高质量合成样本，显著提升了异常检测性能和合成速度。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型和粗糙修复管道在异常合成方面存在结构缺陷，如微结构不连续、语义可控性有限和生成效率低下。

Method: 本文提出了ARAS（语言条件自回归异常合成方法），通过基于token的潜在编辑将文本指定的局部缺陷精确注入正常图像。ARAS利用硬门控自回归操作符和免训练的上下文保留掩码采样核。此外，在QARAD（质量感知重加权异常检测）框架中，提出了一种动态加权策略，通过双编码器模型计算图像-文本相似度分数，以强调高质量的合成样本。

Result: ARAS显著增强了缺陷的真实感，保留了精细的材料纹理，并提供了对合成异常的连续语义控制。QARAD在MVTec AD、VisA和BTAD三个基准数据集上，在图像级和像素级异常检测任务中均优于SOTA方法，实现了更高的准确性、鲁棒性，并且合成速度比基于扩散的方法快5倍。

Conclusion: ARAS和QARAD框架通过创新的异常合成和质量感知加权策略，有效克服了现有方法的局限性，大幅提升了异常检测的性能和效率，为异常合成和检测领域带来了显著进步。

Abstract: Despite substantial progress in anomaly synthesis methods, existing
diffusion-based and coarse inpainting pipelines commonly suffer from structural
deficiencies such as micro-structural discontinuities, limited semantic
controllability, and inefficient generation. To overcome these limitations, we
introduce ARAS, a language-conditioned, auto-regressive anomaly synthesis
approach that precisely injects local, text-specified defects into normal
images via token-anchored latent editing. Leveraging a hard-gated
auto-regressive operator and a training-free, context-preserving masked
sampling kernel, ARAS significantly enhances defect realism, preserves
fine-grained material textures, and provides continuous semantic control over
synthesized anomalies. Integrated within our Quality-Aware Re-weighted Anomaly
Detection (QARAD) framework, we further propose a dynamic weighting strategy
that emphasizes high-quality synthetic samples by computing an image-text
similarity score with a dual-encoder model. Extensive experiments across three
benchmark datasets-MVTec AD, VisA, and BTAD, demonstrate that our QARAD
outperforms SOTA methods in both image- and pixel-level anomaly detection
tasks, achieving improved accuracy, robustness, and a 5 times synthesis speedup
compared to diffusion-based alternatives. Our complete code and synthesized
dataset will be publicly available.

</details>


### [172] [Speech-to-LaTeX: New Models and Datasets for Converting Spoken Equations and Sentences](https://arxiv.org/abs/2508.03542)
*Dmitrii Korzh,Dmitrii Tarasov,Artyom Iudin,Elvir Karimov,Matvey Skripkin,Nikita Kuzmin,Andrey Kuznetsov,Oleg Y. Rogov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 该研究提出了首个大规模、开源、多语言（英俄）的数学语音转LaTeX数据集，并应用音频语言模型，显著提升了数学公式和句子识别的准确率，为多模态AI在数学内容识别方面奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 将口头数学表达式转换为结构化的符号表示（如LaTeX）是一项具有挑战性的任务，涉及语音转录和歧义处理。尽管ASR和LM取得了进展，但口头数学转LaTeX问题仍未被充分探索。现有工作存在局限性，如需要两次转录、仅关注孤立方程、测试集有限且缺乏训练数据和多语言覆盖。

Method: 构建了一个包含超过66,000个人工标注的数学公式和句子音频样本（英语和俄语）的大规模开放数据集。除了ASR后校正模型和少样本提示外，还应用了音频语言模型进行转换。

Result: 在MathSpeech基准测试上，公式转换的字符错误率（CER）与现有模型相当（28% vs 30%）。在提出的S2L-equations基准测试上，模型表现显著优于MathSpeech模型，领先超过40个百分点（27% vs 64%）。首次建立了数学句子识别的基准（S2L-sentences），并实现了40%的公式CER。

Conclusion: 这项工作为未来多模态AI，特别是数学内容识别领域的发展奠定了基础。

Abstract: Conversion of spoken mathematical expressions is a challenging task that
involves transcribing speech into a strictly structured symbolic representation
while addressing the ambiguity inherent in the pronunciation of equations.
Although significant progress has been achieved in automatic speech recognition
(ASR) and language models (LM), the problem of converting spoken mathematics
into LaTeX remains underexplored. This task directly applies to educational and
research domains, such as lecture transcription or note creation. Based on ASR
post-correction, prior work requires 2 transcriptions, focuses only on isolated
equations, has a limited test set, and provides neither training data nor
multilingual coverage. To address these issues, we present the first fully
open-source large-scale dataset, comprising over 66,000 human-annotated audio
samples of mathematical equations and sentences in both English and Russian,
drawn from diverse scientific domains. In addition to the ASR post-correction
models and few-shot prompting, we apply audio language models, demonstrating
comparable character error rate (CER) results on the MathSpeech benchmark (28%
vs. 30%) for the equations conversion. In contrast, on the proposed
S2L-equations benchmark, our models outperform the MathSpeech model by a
substantial margin of more than 40 percentage points, even after accounting for
LaTeX formatting artifacts (27% vs. 64%). We establish the first benchmark for
mathematical sentence recognition (S2L-sentences) and achieve an equation CER
of 40%. This work lays the groundwork for future advances in multimodal AI,
with a particular focus on mathematical content recognition.

</details>


### [173] [Advancing Wildlife Monitoring: Drone-Based Sampling for Roe Deer Density Estimation](https://arxiv.org/abs/2508.03545)
*Stephanie Wohlfahrt,Christoph Praschl,Horst Leitner,Wolfram Jantsch,Julia Konic,Silvio Schueler,Andreas Stöckl,David C. Schedl*

Main category: cs.CV

TL;DR: 本研究利用无人机结合热成像和RGB图像估算野生动物密度，并与相机陷阱数据进行比较，发现无人机是一种有前景且可扩展的野生动物密度估算方法。


<details>
  <summary>Details</summary>
Motivation: 传统的野生动物密度估算方法（如捕获-再捕获、距离采样或相机陷阱）劳动强度大或受空间限制。无人机提供了一种高效、非侵入性的动物计数方法。

Method: 研究在奥地利东南部无叶期（2024年10月和11月）的三个区域进行。使用无人机搭载热成像（IR）和RGB图像传感器，在预设的350米网格和算法定义的系统随机样带上飞行，飞行高度60米。记录的图像中动物被手动标注，并采用三种外推方法（朴素面积外推、自助法、零膨胀负二项式模型）估算每平方公里的密度。同时，使用飞行期间的相机陷阱数据计算随机相遇模型（REM）估算值进行比较。

Result: 无人机估算方法得出相似结果，且通常比随机相遇模型（REM）估算值更高，但10月份的一个区域除外。研究推测无人机估算的密度反映了白天在开放和森林区域的活动，而REM估算的是森林区域在更长时间内的平均活动。

Conclusion: 尽管无人机和相机陷阱提供了不同的野生动物存在视角，但研究结果表明无人机是一种有前景、可扩展的野生动物密度估算方法。

Abstract: We use unmanned aerial drones to estimate wildlife density in southeastern
Austria and compare these estimates to camera trap data. Traditional methods
like capture-recapture, distance sampling, or camera traps are well-established
but labour-intensive or spatially constrained. Using thermal (IR) and RGB
imagery, drones enable efficient, non-intrusive animal counting. Our surveys
were conducted during the leafless period on single days in October and
November 2024 in three areas of a sub-Illyrian hill and terrace landscape.
Flight transects were based on predefined launch points using a 350 m grid and
an algorithm that defined the direction of systematically randomized transects.
This setup allowed surveying large areas in one day using multiple drones,
minimizing double counts. Flight altitude was set at 60 m to avoid disturbing
roe deer (Capreolus capreolus) while ensuring detection. Animals were manually
annotated in the recorded imagery and extrapolated to densities per square
kilometer. We applied three extrapolation methods with increasing complexity:
naive area-based extrapolation, bootstrapping, and zero-inflated negative
binomial modelling. For comparison, a Random Encounter Model (REM) estimate was
calculated using camera trap data from the flight period. The drone-based
methods yielded similar results, generally showing higher densities than REM,
except in one area in October. We hypothesize that drone-based density reflects
daytime activity in open and forested areas, while REM estimates average
activity over longer periods within forested zones. Although both approaches
estimate density, they offer different perspectives on wildlife presence. Our
results show that drones offer a promising, scalable method for wildlife
density estimation.

</details>


### [174] [A Scalable Machine Learning Pipeline for Building Footprint Detection in Historical Maps](https://arxiv.org/abs/2508.03564)
*Annemarie McCarthy*

Main category: cs.CV

TL;DR: 该研究提出了一种可扩展且高效的机器学习管道，用于从历史农村地图中提取稀疏分布的建筑物，通过分层CNN方法显著提高了效率，并在爱尔兰地图上验证了其性能，成功识别了一个可能在大饥荒期间被废弃的定居点。


<details>
  <summary>Details</summary>
Motivation: 现有从历史地图中提取建筑物足迹的机器学习方法主要集中在城市区域，且计算密集，难以应用于需要分析广阔农村地区（建筑物稀疏）的研究问题，例如验证历史人口普查数据或定位废弃定居点。

Method: 本文提出了一种针对农村地图稀疏建筑物分布的、可扩展且高效的管道。该方法采用分层机器学习方法：首先使用卷积神经网络（CNN）分类器逐步过滤掉不太可能包含建筑物的地图区域，显著减少需要详细分析的面积；然后，使用CNN分割算法处理剩余的高概率区域以提取建筑物特征。

Result: 该管道在爱尔兰测绘局历史25英寸和6英寸系列地图的测试部分上进行了验证，结果表明其性能高，且与传统仅分割方法相比效率更高。将该技术应用于覆盖相同地理区域的两种地图系列，发现了一个约22栋建筑的定居点（位于戈尔韦郡图利），该定居点在1839年制作的6英寸地图中存在，但在1899年制作的25英寸地图中已消失。

Conclusion: 该管道在历史和考古发现方面具有巨大潜力。通过识别出在不同时期地图上消失的定居点，推测其可能在爱尔兰大饥荒期间被废弃，从而为历史研究提供了新的线索。

Abstract: Historical maps offer a valuable lens through which to study past landscapes
and settlement patterns. While prior research has leveraged machine learning
based techniques to extract building footprints from historical maps, such
approaches have largely focused on urban areas and tend to be computationally
intensive. This presents a challenge for research questions requiring analysis
across extensive rural regions, such as verifying historical census data or
locating abandoned settlements. In this paper, this limitation is addressed by
proposing a scalable and efficient pipeline tailored to rural maps with sparse
building distributions. The method described employs a hierarchical machine
learning based approach: convolutional neural network (CNN) classifiers are
first used to progressively filter out map sections unlikely to contain
buildings, significantly reducing the area requiring detailed analysis. The
remaining high probability sections are then processed using CNN segmentation
algorithms to extract building features. The pipeline is validated using test
sections from the Ordnance Survey Ireland historical 25 inch map series and 6
inch map series, demonstrating both high performance and improved efficiency
compared to conventional segmentation-only approaches. Application of the
technique to both map series, covering the same geographic region, highlights
its potential for historical and archaeological discovery. Notably, the
pipeline identified a settlement of approximately 22 buildings in Tully, Co.
Galway, present in the 6 inch map, produced in 1839, but absent from the 25
inch map, produced in 1899, suggesting it may have been abandoned during the
Great Famine period.

</details>


### [175] [SAM2-UNeXT: An Improved High-Resolution Baseline for Adapting Foundation Models to Downstream Segmentation Tasks](https://arxiv.org/abs/2508.03566)
*Xinyu Xiong,Zihuang Wu,Lei Zhang,Lei Lu,Ming Li,Guanbin Li*

Main category: cs.CV

TL;DR: 该论文提出了SAM2-UNeXT框架，通过集成DINOv2编码器和采用双分辨率策略，显著增强了SAM的编码器能力，并在多种下游分割任务中取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 尽管Segment Anything Model (SAM) 在下游任务中展现出巨大潜力，但构建一个更强大、更具泛化能力的编码器以进一步提升性能仍然是一个开放的挑战。

Method: 本文提出了SAM2-UNeXT，一个基于SAM2-UNet的高级框架。它通过集成辅助的DINOv2编码器来扩展SAM2的表示能力，并结合双分辨率策略和密集连接层，以简单的架构实现更精确的分割，从而减少了对复杂解码器设计的需求。

Result: 在二值图像分割、伪装物体检测、海洋动物分割和遥感显著性检测四个基准测试上进行的广泛实验表明，所提出的方法展现出卓越的性能。

Conclusion: 通过增强编码器能力和简化架构设计，SAM2-UNeXT在多种图像分割任务中实现了性能提升和更好的泛化能力，验证了其有效性和优越性。

Abstract: Recent studies have highlighted the potential of adapting the Segment
Anything Model (SAM) for various downstream tasks. However, constructing a more
powerful and generalizable encoder to further enhance performance remains an
open challenge. In this work, we propose SAM2-UNeXT, an advanced framework that
builds upon the core principles of SAM2-UNet while extending the
representational capacity of SAM2 through the integration of an auxiliary
DINOv2 encoder. By incorporating a dual-resolution strategy and a dense glue
layer, our approach enables more accurate segmentation with a simple
architecture, relaxing the need for complex decoder designs. Extensive
experiments conducted on four benchmarks, including dichotomous image
segmentation, camouflaged object detection, marine animal segmentation, and
remote sensing saliency detection, demonstrate the superior performance of our
proposed method. The code is available at
https://github.com/WZH0120/SAM2-UNeXT.

</details>


### [176] [RadProPoser: A Framework for Human Pose Estimation with Uncertainty Quantification from Raw Radar Data](https://arxiv.org/abs/2508.03578)
*Jonas Leo Mueller,Lukas Engel,Eva Dorschky,Daniel Krauss,Ingrid Ullmann,Martin Vossiek,Bjoern M. Eskofier*

Main category: cs.CV

TL;DR: RadProPoser是一种基于MIMO雷达的概率编码器-解码器架构，用于人体姿态估计，它能同时预测3D关节位置和异方差不确定性，并在新数据集上表现出色，且其不确定性可用于数据增强。


<details>
  <summary>Details</summary>
Motivation: 雷达人体姿态估计具有隐私保护和光照不变性优点，但面临测量噪声和多径效应的挑战。现有系统缺乏对每关节不确定性的显式建模和量化。

Method: 引入RadProPoser，一个概率编码器-解码器架构，处理来自3发4收MIMO雷达的复数值雷达张量。通过将变分推断整合到关键点回归中，联合预测26个3D关节位置和异方差不确定性。探索了使用高斯和拉普拉斯分布作为潜在先验和似然的不同概率公式。

Result: 在新发布的数据集上，RadProPoser的平均每关节位置误差（MPJPE）为6.425厘米，在45度视角下为5.678厘米。学习到的不确定性与实际姿态误差高度一致，并能被校准以产生可靠的预测区间，最佳配置的预期校准误差为0.021。通过潜在分布采样进行数据增强，在下游活动分类任务中F1分达到0.870。

Conclusion: RadProPoser是首个端到端基于雷达张量的人体姿态估计系统，能够从原始雷达张量数据中显式建模和量化每关节不确定性，为雷达应用中可解释和可靠的人体运动分析奠定了基础。

Abstract: Radar-based human pose estimation (HPE) provides a privacy-preserving,
illumination-invariant sensing modality but is challenged by noisy,
multipath-affected measurements. We introduce RadProPoser, a probabilistic
encoder-decoder architecture that processes complex-valued radar tensors from a
compact 3-transmitter, 4-receiver MIMO radar. By incorporating variational
inference into keypoint regression, RadProPoser jointly predicts 26
three-dimensional joint locations alongside heteroscedastic aleatoric
uncertainties and can be recalibrated to predict total uncertainty. We explore
different probabilistic formulations using both Gaussian and Laplace
distributions for latent priors and likelihoods. On our newly released dataset
with optical motion-capture ground truth, RadProPoser achieves an overall mean
per-joint position error (MPJPE) of 6.425 cm, with 5.678 cm at the 45 degree
aspect angle. The learned uncertainties exhibit strong alignment with actual
pose errors and can be calibrated to produce reliable prediction intervals,
with our best configuration achieving an expected calibration error of 0.021.
As an additional demonstration, sampling from these latent distributions
enables effective data augmentation for downstream activity classification,
resulting in an F1 score of 0.870. To our knowledge, this is the first
end-to-end radar tensor-based HPE system to explicitly model and quantify
per-joint uncertainty from raw radar tensor data, establishing a foundation for
explainable and reliable human motion analysis in radar applications.

</details>


### [177] [DyCAF-Net: Dynamic Class-Aware Fusion Network](https://arxiv.org/abs/2508.03598)
*Md Abrar Jahin,Shahriar Soudeep,M. F. Mridha,Nafiz Fahad,Md. Jakir Hossen*

Main category: cs.CV

TL;DR: 提出DyCAF-Net，通过动态类感知融合和注意力机制，解决现有目标检测在复杂场景中的局限性，显著提升性能并保持效率。


<details>
  <summary>Details</summary>
Motivation: 现有目标检测方法依赖的静态融合启发式和类别无关注意力机制，在动态场景（如遮挡、杂乱、类别不平衡）中性能受限。

Method: 提出DyCAF-Net，包含三项创新：1) 基于输入条件的平衡颈部，通过隐式不动点建模迭代优化多尺度特征；2) 双动态注意力机制，利用输入和类别依赖线索自适应调整通道和空间响应；3) 类感知特征自适应，调制特征以优先处理稀有类别的判别区域。

Result: DyCAF-Net在13个多样化基准（包括遮挡严重和长尾数据集）上，相对于YOLOv8及9个SOTA基线，在精度、mAP@50和mAP@50-95方面取得了显著提升。同时保持计算效率（约11.1M参数）和有竞争力的推理速度。

Conclusion: DyCAF-Net能适应尺度变化、语义重叠和类别不平衡，是一个鲁棒的解决方案，适用于医学影像、监控和自动驾驶系统等现实世界检测任务。

Abstract: Recent advancements in object detection rely on modular architectures with
multi-scale fusion and attention mechanisms. However, static fusion heuristics
and class-agnostic attention limit performance in dynamic scenes with
occlusions, clutter, and class imbalance. We introduce Dynamic Class-Aware
Fusion Network (DyCAF-Net) that addresses these challenges through three
innovations: (1) an input-conditioned equilibrium-based neck that iteratively
refines multi-scale features via implicit fixed-point modeling, (2) a dual
dynamic attention mechanism that adaptively recalibrates channel and spatial
responses using input- and class-dependent cues, and (3) class-aware feature
adaptation that modulates features to prioritize discriminative regions for
rare classes. Through comprehensive ablation studies with YOLOv8 and related
architectures, alongside benchmarking against nine state-of-the-art baselines,
DyCAF-Net achieves significant improvements in precision, mAP@50, and mAP@50-95
across 13 diverse benchmarks, including occlusion-heavy and long-tailed
datasets. The framework maintains computational efficiency ($\sim$11.1M
parameters) and competitive inference speeds, while its adaptability to scale
variance, semantic overlaps, and class imbalance positions it as a robust
solution for real-world detection tasks in medical imaging, surveillance, and
autonomous systems.

</details>


### [178] [evTransFER: A Transfer Learning Framework for Event-based Facial Expression Recognition](https://arxiv.org/abs/2508.03609)
*Rodrigo Verschae,Ignacio Bugueno-Cordova*

Main category: cs.CV

TL;DR: evTransFER是一个基于事件相机的面部表情识别框架，通过对抗生成方法进行迁移学习，显著提高了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、高时间分辨率和高动态范围的特点，能有效捕捉场景的时空动态，为面部表情识别提供了有价值的信息。

Method: 本文提出了evTransFER框架，其核心是一个特征提取器，通过在面部重建任务上训练对抗生成模型，然后将训练好的编码器权重迁移到面部表情识别系统。此外，还引入了LSTM来捕捉长期面部表情动态，并提出了一种新的事件表示TIE，以进一步提高性能。

Result: evTransFER在事件基面部表情数据库e-CK+上实现了93.6%的识别率，与现有最先进方法相比，准确率显著提高了25.9%或更多。

Conclusion: 所提出的迁移学习方法（evTransFER），结合LSTM和TIE事件表示，大大提高了使用事件相机进行面部表情识别的能力和准确性。

Abstract: Event-based cameras are bio-inspired vision sensors that asynchronously
capture per-pixel intensity changes with microsecond latency, high temporal
resolution, and high dynamic range, providing valuable information about the
spatio-temporal dynamics of the scene. In the present work, we propose
evTransFER, a transfer learning-based framework and architecture for face
expression recognition using event-based cameras. The main contribution is a
feature extractor designed to encode the spatio-temporal dynamics of faces,
built by training an adversarial generative method on a different problem
(facial reconstruction) and then transferring the trained encoder weights to
the face expression recognition system. We show that this proposed transfer
learning method greatly improves the ability to recognize facial expressions
compared to training a network from scratch. In addition, we propose an
architecture that incorporates an LSTM to capture longer-term facial expression
dynamics, and we introduce a new event-based representation, referred to as
TIE, both of which further improve the results. We evaluate the proposed
framework on the event-based facial expression database e-CK+ and compare it to
state-of-the-art methods. The results show that the proposed framework
evTransFER achieves a 93.6\% recognition rate on the e-CK+ database,
significantly improving the accuracy (25.9\% points or more) when compared to
state-of-the-art performance for similar problems.

</details>


### [179] [FPG-NAS: FLOPs-Aware Gated Differentiable Neural Architecture Search for Efficient 6DoF Pose Estimation](https://arxiv.org/abs/2508.03618)
*Nassim Ali Ousalah,Peyman Rostami,Anis Kacem,Enjie Ghorbel,Emmanuel Koumandakis,Djamila Aouada*

Main category: cs.CV

TL;DR: FPG-NAS是一种FLOPs感知的门控可微分神经架构搜索框架，用于高效的6DoF物体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 6DoF物体姿态估计计算量大，限制了其在资源受限场景中的应用。

Method: FPG-NAS提出了一种专门针对6DoF姿态估计的可微分NAS方法，具有任务特定的搜索空间和可微分门控机制，实现离散多候选操作符选择，增强架构多样性。此外，引入FLOPs正则化项以平衡精度和效率。该框架探索了约10^92种可能的架构。

Result: 在LINEMOD和SPEED+数据集上的实验表明，FPG-NAS派生模型在严格的FLOPs限制下优于现有方法。

Conclusion: FPG-NAS是首个专门为6DoF物体姿态估计设计的可微分NAS框架。

Abstract: We introduce FPG-NAS, a FLOPs-aware Gated Differentiable Neural Architecture
Search framework for efficient 6DoF object pose estimation. Estimating 3D
rotation and translation from a single image has been widely investigated yet
remains computationally demanding, limiting applicability in
resource-constrained scenarios. FPG-NAS addresses this by proposing a
specialized differentiable NAS approach for 6DoF pose estimation, featuring a
task-specific search space and a differentiable gating mechanism that enables
discrete multi-candidate operator selection, thus improving architectural
diversity. Additionally, a FLOPs regularization term ensures a balanced
trade-off between accuracy and efficiency. The framework explores a vast search
space of approximately 10\textsuperscript{92} possible architectures.
Experiments on the LINEMOD and SPEED+ datasets demonstrate that FPG-NAS-derived
models outperform previous methods under strict FLOPs constraints. To the best
of our knowledge, FPG-NAS is the first differentiable NAS framework
specifically designed for 6DoF object pose estimation.

</details>


### [180] [Uni3R: Unified 3D Reconstruction and Semantic Understanding via Generalizable Gaussian Splatting from Unposed Multi-View Images](https://arxiv.org/abs/2508.03643)
*Xiangyu Sun,Haoyi jiang,Liu Liu,Seungtae Nam,Gyeongjin Kang,Xinjie wang,Wei Sui,Zhizhong Su,Wenyu Liu,Xinggang Wang,Eunbyung Park*

Main category: cs.CV

TL;DR: Uni3R是一个前馈框架，能直接从无姿态多视角图像中联合重建统一的3D场景表示，并赋予开放词汇语义，实现高保真新视角合成、语义分割和深度预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法将语义理解与重建分离，或需要昂贵的逐场景优化，限制了可扩展性和泛化性。

Method: Uni3R采用跨视图Transformer整合多视角信息，然后回归一组带有语义特征场的3D高斯基元，实现单次前馈通过即可完成新视角合成、开放词汇3D语义分割和深度预测。

Result: Uni3R在多个基准测试中达到了新的最先进水平，包括RE10K上的25.07 PSNR和ScanNet上的55.84 mIoU。

Conclusion: Uni3R为可泛化、统一的3D场景重建和理解提供了一种新范式。

Abstract: Reconstructing and semantically interpreting 3D scenes from sparse 2D views
remains a fundamental challenge in computer vision. Conventional methods often
decouple semantic understanding from reconstruction or necessitate costly
per-scene optimization, thereby restricting their scalability and
generalizability. In this paper, we introduce Uni3R, a novel feed-forward
framework that jointly reconstructs a unified 3D scene representation enriched
with open-vocabulary semantics, directly from unposed multi-view images. Our
approach leverages a Cross-View Transformer to robustly integrate information
across arbitrary multi-view inputs, which then regresses a set of 3D Gaussian
primitives endowed with semantic feature fields. This unified representation
facilitates high-fidelity novel view synthesis, open-vocabulary 3D semantic
segmentation, and depth prediction, all within a single, feed-forward pass.
Extensive experiments demonstrate that Uni3R establishes a new state-of-the-art
across multiple benchmarks, including 25.07 PSNR on RE10K and 55.84 mIoU on
ScanNet. Our work signifies a novel paradigm towards generalizable, unified 3D
scene reconstruction and understanding. The code is available at
https://github.com/HorizonRobotics/Uni3R.

</details>


### [181] [LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation](https://arxiv.org/abs/2508.03694)
*Jianxiong Gao,Zhaoxi Chen,Xian Liu,Jianfeng Feng,Chenyang Si,Yanwei Fu,Yu Qiao,Ziwei Liu*

Main category: cs.CV

TL;DR: LongVie是一个端到端自回归框架，通过统一噪声初始化、全局控制信号归一化、多模态控制和退化感知训练，解决了可控超长视频生成中的时间不一致性和视觉退化问题，并引入了新的基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有方法在生成超长视频时存在时间不一致性和视觉退化问题，难以扩展。主要挑战源于：独立的噪声初始化、独立的控制信号归一化以及单模态指导的局限性。

Method: 本文提出了LongVie框架，核心设计包括：1) 统一的噪声初始化策略以保持片段间生成一致性；2) 全局控制信号归一化以确保整个视频在控制空间中的对齐；3) 集成密集（如深度图）和稀疏（如关键点）控制信号的多模态控制框架；4) 退化感知训练策略，自适应平衡模态贡献以保持视觉质量。此外，还引入了LongVGenBench，一个包含100个高分辨率、超过一分钟视频的综合基准测试。

Result: LongVie在长距离可控性、一致性和质量方面均实现了最先进的性能。

Conclusion: LongVie通过其创新的设计有效解决了可控超长视频生成中的关键挑战，并在性能上超越了现有方法，为该领域树立了新标杆。

Abstract: Controllable ultra-long video generation is a fundamental yet challenging
task. Although existing methods are effective for short clips, they struggle to
scale due to issues such as temporal inconsistency and visual degradation. In
this paper, we initially investigate and identify three key factors: separate
noise initialization, independent control signal normalization, and the
limitations of single-modality guidance. To address these issues, we propose
LongVie, an end-to-end autoregressive framework for controllable long video
generation. LongVie introduces two core designs to ensure temporal consistency:
1) a unified noise initialization strategy that maintains consistent generation
across clips, and 2) global control signal normalization that enforces
alignment in the control space throughout the entire video. To mitigate visual
degradation, LongVie employs 3) a multi-modal control framework that integrates
both dense (e.g., depth maps) and sparse (e.g., keypoints) control signals,
complemented by 4) a degradation-aware training strategy that adaptively
balances modality contributions over time to preserve visual quality. We also
introduce LongVGenBench, a comprehensive benchmark consisting of 100
high-resolution videos spanning diverse real-world and synthetic environments,
each lasting over one minute. Extensive experiments show that LongVie achieves
state-of-the-art performance in long-range controllability, consistency, and
quality.

</details>


### [182] [Trokens: Semantic-Aware Relational Trajectory Tokens for Few-Shot Action Recognition](https://arxiv.org/abs/2508.03695)
*Pulkit Kumar,Shuaiyi Huang,Matthew Walmer,Sai Saketh Rambhatla,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: Trokens通过将轨迹点转换为语义感知的关系令牌，并结合自适应采样和全面的运动建模，显著提升了少样本动作识别的性能，达到了当前最佳水平。


<details>
  <summary>Details</summary>
Motivation: 少样本动作识别需要有效建模运动和外观信息。尽管点跟踪技术有所进展，但选择信息丰富的跟踪点以及有效建模其运动模式仍是两大挑战。

Method: 提出了Trokens方法。首先，引入了一种语义感知采样策略，根据物体尺度和语义相关性自适应地分布跟踪点。其次，开发了一个运动建模框架，通过定向位移直方图（HoD）捕获轨迹内动态，并通过轨迹间关系建模复杂的动作模式。该方法将这些轨迹令牌与语义特征结合，用运动信息增强外观特征。

Result: 在六个不同的少样本动作识别基准测试（Something-Something-V2、Kinetics、UCF101、HMDB51和FineGym）上均实现了最先进的性能。

Conclusion: Trokens通过有效结合语义感知点采样和全面的运动建模，成功地将丰富的运动信息融入外观特征，显著提升了少样本动作识别的能力，达到了新的性能标杆。

Abstract: Video understanding requires effective modeling of both motion and appearance
information, particularly for few-shot action recognition. While recent
advances in point tracking have been shown to improve few-shot action
recognition, two fundamental challenges persist: selecting informative points
to track and effectively modeling their motion patterns. We present Trokens, a
novel approach that transforms trajectory points into semantic-aware relational
tokens for action recognition. First, we introduce a semantic-aware sampling
strategy to adaptively distribute tracking points based on object scale and
semantic relevance. Second, we develop a motion modeling framework that
captures both intra-trajectory dynamics through the Histogram of Oriented
Displacements (HoD) and inter-trajectory relationships to model complex action
patterns. Our approach effectively combines these trajectory tokens with
semantic features to enhance appearance features with motion information,
achieving state-of-the-art performance across six diverse few-shot action
recognition benchmarks: Something-Something-V2 (both full and small splits),
Kinetics, UCF101, HMDB51, and FineGym. For project page see
https://trokens-iccv25.github.io

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [183] [Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation](https://arxiv.org/abs/2508.02808)
*Radhika Dua,Young Joon,Kwon,Siddhant Dogra,Daniel Freedman,Diana Ruan,Motaz Nashawaty,Danielle Rigau,Daniel Alexander Alber,Kang Zhang,Kyunghyun Cho,Eric Karl Oermann*

Main category: cs.CL

TL;DR: ICARE是一个可解释的、基于大语言模型代理的放射报告自动评估框架，通过动态多项选择问答来衡量生成报告的临床准确性和一致性，并显著优于现有指标。


<details>
  <summary>Details</summary>
Motivation: 放射报告生成（RRG）模型的部署需要可靠的临床评估，但现有评估指标多依赖于表层相似性或缺乏可解释性，无法满足安全部署的要求。

Method: ICARE（Interpretable and Clinically-grounded Agent-based Report Evaluation）框架利用两个大语言模型代理：一个持有真实报告，另一个持有生成报告。它们互相生成并回答临床相关问题，通过答案的一致性来衡量发现的保留和一致性，作为临床准确性和召回率的代理指标。分数与问答对关联，实现透明评估。

Result: 临床医生研究表明，ICARE与专家判断的吻合度显著高于现有指标。扰动分析证实了其对临床内容的敏感性和可复现性。模型比较揭示了可解释的错误模式。

Conclusion: ICARE提供了一个透明、可解释且以临床为基础的放射报告评估框架，能够更准确地反映专家判断，并提供错误模式的洞察，从而促进RRG模型的安全部署。

Abstract: Radiological imaging is central to diagnosis, treatment planning, and
clinical decision-making. Vision-language foundation models have spurred
interest in automated radiology report generation (RRG), but safe deployment
requires reliable clinical evaluation of generated reports. Existing metrics
often rely on surface-level similarity or behave as black boxes, lacking
interpretability. We introduce ICARE (Interpretable and Clinically-grounded
Agent-based Report Evaluation), an interpretable evaluation framework
leveraging large language model agents and dynamic multiple-choice question
answering (MCQA). Two agents, each with either the ground-truth or generated
report, generate clinically meaningful questions and quiz each other. Agreement
on answers captures preservation and consistency of findings, serving as
interpretable proxies for clinical precision and recall. By linking scores to
question-answer pairs, ICARE enables transparent, and interpretable assessment.
Clinician studies show ICARE aligns significantly more with expert judgment
than prior metrics. Perturbation analyses confirm sensitivity to clinical
content and reproducibility, while model comparisons reveal interpretable error
patterns.

</details>


### [184] [Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives](https://arxiv.org/abs/2508.02853)
*Yinuo Xu,Veronica Derricks,Allison Earl,David Jurgens*

Main category: cs.CL

TL;DR: 该研究提出一种通过人口统计学感知的专家混合模型（DEM-MoE）和LLM生成的合成数据来建模主观NLP任务中标注者分歧的方法，以更好地表示多样化观点。


<details>
  <summary>Details</summary>
Motivation: 先前的模型难以有效表示标注者分歧中结构化的、群体层面的差异，并且存在人口统计学覆盖稀疏的问题，因此需要新的方法来提升对多样化视角的建模能力。

Method: 该研究采用两种创新方法：1) 架构创新：开发DEM-MoE模型，根据标注者人口统计学信息将输入路由到专家子网络；2) 数据中心创新：利用LLM通过零样本角色提示生成合成标注，用于数据插补和丰富，并评估了融合真实和合成数据的策略。

Result: DEM-MoE模型在不同人口统计学群体中表现出竞争力，尤其在标注者分歧度高的数据集上表现出色。LLM生成的合成判断与人类标注有中等程度的一致性，并提供了一种可扩展的数据丰富方式。研究还发现，最佳的数据融合策略取决于数据集结构。

Conclusion: 这些贡献共同提升了在主观NLP任务中对多样化视角的表示能力，有效解决了标注者分歧建模和数据稀疏性问题。

Abstract: We present an approach to modeling annotator disagreement in subjective NLP
tasks through both architectural and data-centric innovations. Our model,
DEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert
subnetworks based on annotator demographics, enabling it to better represent
structured, group-level variation compared to prior models. DEM-MoE
consistently performs competitively across demographic groups, and shows
especially strong results on datasets with high annotator disagreement. To
address sparse demographic coverage, we test whether LLM-generated synthetic
annotations via zero-shot persona prompting can be used for data imputation. We
show these synthetic judgments align moderately well with human annotations on
our data and offer a scalable way to potentially enrich training data. We then
propose and evaluate approaches for blending real and synthetic data using
strategies tailored to dataset structure. We find that the optimal strategies
depend on dataset structure. Together, these contributions improve the
representation of diverse perspectives.

</details>


### [185] [Highlight & Summarize: RAG without the jailbreaks](https://arxiv.org/abs/2508.02872)
*Giovanni Cherubin,Andrew Paverd*

Main category: cs.CL

TL;DR: 本文提出并评估了一种名为“高亮与摘要”（Highlight & Summarize, H&S）的新型检索增强生成（RAG）系统设计模式，旨在通过不向生成式LLM暴露用户问题来有效防止LLM越狱和模型劫持攻击。


<details>
  <summary>Details</summary>
Motivation: 防止大型语言模型（LLM）的越狱和模型劫持是一个重要但具有挑战性的任务。现有缓解措施（如强化系统提示或使用内容分类器）由于输入和输出空间巨大，很容易被绕过，导致LLM生成不期望的内容或执行偏离预期的任务。

Method: H&S设计模式将RAG管道拆分为两个核心组件：一个“高亮器”（highlighter），它接收用户问题并从检索到的文档中提取相关段落（“高亮内容”）；一个“摘要器”（summarizer），它接收高亮内容并将其总结为连贯的答案。这种方法确保生成式LLM永远不会直接看到用户的原始问题。

Result: 评估结果显示，当使用基于LLM的高亮器时，大多数H&S生成的响应在正确性、相关性和响应质量方面被判断为优于标准的RAG管道。

Conclusion: H&S是一种通过设计来有效防止LLM越狱和模型劫持攻击的新型RAG系统设计模式，并且在生成响应的质量上表现出超越传统RAG的潜力。

Abstract: Preventing jailbreaking and model hijacking of Large Language Models (LLMs)
is an important yet challenging task. For example, when interacting with a
chatbot, malicious users can input specially crafted prompts to cause the LLM
to generate undesirable content or perform a completely different task from its
intended purpose. Existing mitigations for such attacks typically rely on
hardening the LLM's system prompt or using a content classifier trained to
detect undesirable content or off-topic conversations. However, these
probabilistic approaches are relatively easy to bypass due to the very large
space of possible inputs and undesirable outputs. In this paper, we present and
evaluate Highlight & Summarize (H&S), a new design pattern for
retrieval-augmented generation (RAG) systems that prevents these attacks by
design. The core idea is to perform the same task as a standard RAG pipeline
(i.e., to provide natural language answers to questions, based on relevant
sources) without ever revealing the user's question to the generative LLM. This
is achieved by splitting the pipeline into two components: a highlighter, which
takes the user's question and extracts relevant passages ("highlights") from
the retrieved documents, and a summarizer, which takes the highlighted passages
and summarizes them into a cohesive answer. We describe several possible
instantiations of H&S and evaluate their generated responses in terms of
correctness, relevance, and response quality. Surprisingly, when using an
LLM-based highlighter, the majority of H&S responses are judged to be better
than those of a standard RAG pipeline.

</details>


### [186] [Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages](https://arxiv.org/abs/2508.02885)
*Elliot Murphy,Rohan Venkatesh,Edward Khokhlovich,Andrey Vyshedskiy*

Main category: cs.CL

TL;DR: 该研究通过行为学实验和聚类分析，发现语言学中的核心句法操作“合并”（Merge）产生的不同句法结构类型，可能由不同的认知机制支持，而非单一不可分割。


<details>
  <summary>Details</summary>
Motivation: 现代语言学中，“合并”操作被认为是句法的核心运算，但关于其是否是单一不可分割、在演化中一次性出现，以及不同合并结构（如动词短语、名词短语、介词短语）是否由不同神经认知机制支持，存在争议。本研究旨在系统性地探究这一问题。

Method: 研究系统地调查了参与者对不同句法复杂程度句子的理解。通过聚类分析（Clustering analyses）来揭示行为学证据中的结构类型。

Result: 聚类分析揭示了三种不同的句法结构类型，这些类型可能在不同的发展阶段出现，并可能受到选择性损伤。这表明即使“合并”作为一种抽象操作是统一的，其具体应用在认知处理上可能存在差异。

Conclusion: 尽管基于“合并”的句法能力可能在演化上突然出现，但不同类型的“合并”结构在处理过程中似乎由不同的认知机制支持。这挑战了“合并”是一个完全统一且不可分的神经认知操作的观点。

Abstract: In the modern language sciences, the core computational operation of syntax,
'Merge', is defined as an operation that combines two linguistic units (e.g.,
'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).
This can then be further combined with additional linguistic units based on
this categorial information, respecting non-associativity such that abstract
grouping is respected. Some linguists have embraced the view that Merge is an
elementary, indivisible operation that emerged in a single evolutionary step.
From a neurocognitive standpoint, different mental objects constructed by Merge
may be supported by distinct mechanisms: (1) simple command constructions
(e.g., "eat apples"); (2) the merging of adjectives and nouns ("red boat"); and
(3) the merging of nouns with spatial prepositions ("laptop behind the sofa").
Here, we systematically investigate participants' comprehension of sentences
with increasing levels of syntactic complexity. Clustering analyses revealed
behavioral evidence for three distinct structural types, which we discuss as
potentially emerging at different developmental stages and subject to selective
impairment. While a Merge-based syntax may still have emerged suddenly in
evolutionary time, responsible for the structured symbolic turn our species
took, different cognitive mechanisms seem to underwrite the processing of
various types of Merge-based objects.

</details>


### [187] [Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models](https://arxiv.org/abs/2508.02886)
*Wenjie Luo,Ruocheng Li,Shanshan Zhu,Julian Perry*

Main category: cs.CL

TL;DR: 提出Coherent Multimodal Reasoning Framework (CMRF)，通过迭代式自评估推理机制，显著提升大型视觉语言模型(LVLMs)在复杂多模态常识推理任务中的表现，模拟人类解决问题的方式。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型(LLMs)和视觉语言模型(LVLMs)在处理复杂、多步骤、跨模态的常识推理任务时表现不佳，缺乏“审慎思考”能力，倾向于依赖表面关联而非深层链式推理，尤其在整合视觉信息与抽象概念时。

Method: 提出Coherent Multimodal Reasoning Framework (CMRF)，模仿人类解决问题，通过迭代、自评估的推理机制增强LVLMs的常识推理能力。框架包含三个核心模块：推理分解单元(RDU)分解问题，上下文推理引擎(CIE)进行上下文推理，连贯性评估模块(CAM)评估逻辑一致性。结合自适应迭代优化策略，系统地改进推理路径。基于LLaVA-1.6-34B构建，并在新型多模态日常活动推理(MDAR)数据集上进行训练。

Result: CMRF在VCR、A-OKVQA和DailyLife-MRC等挑战性基准测试中，在开源LVLMs中取得了最先进的性能，平均准确率达到69.4%，比最佳开源基线高出2.4个百分点，在复杂推理场景中表现尤为突出。广泛的消融研究和人工评估证实了每个模块的关键贡献以及迭代优化的有效性。

Conclusion: CMRF通过其迭代自评估推理框架，显著提升了LVLMs在复杂多模态常识推理任务上的性能，证明了这种模仿人类思维的策略在解决复杂跨模态问题上的有效性。

Abstract: Despite significant advancements, current large language models (LLMs) and
vision-language models (LVLMs) continue to struggle with complex, multi-step,
cross-modal common sense reasoning tasks, often exhibiting a lack of
"deliberative thinking." They tend to rely on superficial associations rather
than deep, chained inference, particularly when integrating visual information
with abstract concepts. To address this, we propose the Coherent Multimodal
Reasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense
reasoning capabilities through an iterative, self-evaluating inference
mechanism. CMRF mimics human problem-solving by decomposing complex queries,
generating step-by-step inferences, and self-correcting errors. Our framework
integrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking
down problems into sub-questions, a Contextual Inference Engine (CIE) for
contextual inference, and a Coherence Assessment Module (CAM) for evaluating
logical consistency and confidence. Coupled with an Adaptive Iterative
Refinement strategy, CMRF systematically refines its reasoning paths. Built
upon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning
(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source
LVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It
attains an average accuracy of 69.4%, surpassing the best open-source baseline
by +2.4 percentage points, with particular strength in complex reasoning
scenarios. Extensive ablation studies and human evaluations confirm the
critical contributions of each module and the effectiveness of iterative
refinement in fostering more coherent and accurate reasoning.

</details>


### [188] [SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations](https://arxiv.org/abs/2508.02901)
*Osama Khalid,Sanvesh Srivastava,Padmini Srinivasan*

Main category: cs.CL

TL;DR: 该研究探索了感官语言与传统风格特征（如LIWC）的关系，提出了一种新的降秩岭回归（R4）方法来提取低维特征，并引入了精简可解释模型（SLIM-LLMs），证明其在大幅减少参数的同时能匹配全尺寸语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 感官语言在表达经验和感知方面扮演着基础角色。研究旨在探索感官语言与LIWC等传统文体特征之间的关系，并寻求更高效、可解释的模型来理解和预测这种关系。

Method: 1. 采用了一种新颖的降秩岭回归（Reduced-Rank Ridge Regression, R4）方法来从LIWC特征中提取低维潜在表示。2. 引入了文体精简可解释模型（Stylometrically Lean Interpretable Models, SLIM-LLMs），用于建模这些风格维度之间的非线性关系。

Result: 1. R4方法表明，24维的LIWC低维潜在表示能有效捕捉感官语言的文体信息，其预测效果与74维的完整特征集相当。2. SLIM-LLMs模型在五种文体上进行评估，使用低秩LIWC特征时，其性能与全尺寸语言模型匹配，同时将参数量减少了高达80%。

Conclusion: 该研究证明，通过使用降秩的文体特征和精简可解释的模型（SLIM-LLMs），可以高效且准确地预测感官语言，同时显著降低模型的复杂性和参数量，达到与大型模型相当的性能。

Abstract: Sensorial language -- the language connected to our senses including vision,
sound, touch, taste, smell, and interoception, plays a fundamental role in how
we communicate experiences and perceptions. We explore the relationship between
sensorial language and traditional stylistic features, like those measured by
LIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate
that low-dimensional latent representations of LIWC features r = 24 effectively
capture stylistic information for sensorial language prediction compared to the
full feature set (r = 74). We introduce Stylometrically Lean Interpretable
Models (SLIM-LLMs), which model non-linear relationships between these style
dimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features
match the performance of full-scale language models while reducing parameters
by up to 80%.

</details>


### [189] [Can LLMs Generate High-Quality Task-Specific Conversations?](https://arxiv.org/abs/2508.02931)
*Shengqi Li,Amarnath Gupta*

Main category: cs.CL

TL;DR: 本文提出一个参数化框架，通过九个参数控制大型语言模型（LLM）的对话质量，并实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有对话生成面临主题连贯性、知识进展、角色一致性和控制粒度等挑战，需要一种精确控制对话属性的方法。

Method: 引入一个参数化框架，探索了六个维度中的九个关键参数，以实现对话属性的精确指定。通过对最先进的LLM进行实验来验证参数化控制的效果。

Result: 参数化控制能够对生成的对话属性产生统计学上显著的差异，有效解决了对话生成中的多项挑战。

Conclusion: 该框架为对话质量控制提供了一种标准化方法，在教育、治疗、客户服务和娱乐等领域具有广泛应用前景。未来工作将侧重于通过架构修改实现更多参数和开发基准数据集。

Abstract: This paper introduces a parameterization framework for controlling
conversation quality in large language models. We explore nine key parameters
across six dimensions that enable precise specification of dialogue properties.
Through experiments with state-of-the-art LLMs, we demonstrate that
parameter-based control produces statistically significant differences in
generated conversation properties. Our approach addresses challenges in
conversation generation, including topic coherence, knowledge progression,
character consistency, and control granularity. The framework provides a
standardized method for conversation quality control with applications in
education, therapy, customer service, and entertainment. Future work will focus
on implementing additional parameters through architectural modifications and
developing benchmark datasets for evaluation.

</details>


### [190] [CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors](https://arxiv.org/abs/2508.02997)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 本文提出一种利用上下文共现矩阵和张量潜在空间特性检测大语言模型（LLM）越狱攻击的新方法，在数据稀缺环境下显著提升了检测性能和速度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）的复杂性和不透明性使其容易受到越狱攻击，从而产生有害响应。为了确保LLMs的安全可靠使用，迫切需要开发强大的检测方法来识别这些恶意提示。

Method: 本研究利用在数据稀缺环境中表现出色的上下文共现矩阵（Contextual Co-occurrence Matrix）来解决越狱提示检测问题。提出了一种新颖的方法，该方法利用上下文共现矩阵和张量的潜在空间特性，以有效识别对抗性提示和越狱提示。

Result: 该方法在仅使用0.5%的标注提示时，F1分数达到0.83，比基线模型提高了96.6%。此外，该方法显著加快了检测速度，与基线模型相比，速度提升了2.3到128.4倍。

Conclusion: 所提出的方法在LLM越狱攻击检测方面表现出卓越的性能，尤其是在标注数据稀缺的情况下，其学习模式的有效性得到显著体现，并且具有显著的计算效率优势，有助于LLMs的安全和可靠部署。

Abstract: The widespread use of Large Language Models (LLMs) in many applications marks
a significant advance in research and practice. However, their complexity and
hard-to-understand nature make them vulnerable to attacks, especially
jailbreaks designed to produce harmful responses. To counter these threats,
developing strong detection methods is essential for the safe and reliable use
of LLMs. This paper studies this detection problem using the Contextual
Co-occurrence Matrix, a structure recognized for its efficacy in data-scarce
environments. We propose a novel method leveraging the latent space
characteristics of Contextual Co-occurrence Matrices and Tensors for the
effective identification of adversarial and jailbreak prompts. Our evaluations
show that this approach achieves a notable F1 score of 0.83 using only 0.5% of
labeled prompts, which is a 96.6% improvement over baselines. This result
highlights the strength of our learned patterns, especially when labeled data
is scarce. Our method is also significantly faster, speedup ranging from 2.3 to
128.4 times compared to the baseline models. To support future research and
reproducibility, we have made our implementation publicly available.

</details>


### [191] [When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025](https://arxiv.org/abs/2508.03037)
*Ariya Mukherjee-Gandhi,Oliver Muellerklein*

Main category: cs.CL

TL;DR: 该研究分析了2013年至2025年间关于AI生成艺术的英语语篇，发现艺术家对同意、透明度和创意劳动的担忧常被主流媒体叙事和技术术语边缘化。


<details>
  <summary>Details</summary>
Motivation: 生成式AI正在重塑艺术生产和人类表达方式，直接影响艺术家生计，引发了对同意、透明度和创意劳动未来的担忧。然而，在主流公共和学术话语中，艺术家的声音常被边缘化。

Method: 该研究对2013年至2025年间439篇精选的500字英语语篇（包括评论文章、新闻报道、博客、法律文件和口语转录）进行了为期十二年的分析。采用可复现的方法（基于BERTopic），识别了五个稳定的主题集群。

Result: 研究识别出五个稳定的主题集群，并揭示了艺术家感知与主流媒体叙事之间的不一致。结果还强调了技术术语如何充当一种微妙的“看门人”形式，常常将艺术家认为最紧迫的问题边缘化。

Conclusion: 该工作提供了一种基于BERTopic的方法和多模态基线，以供未来研究使用，并明确呼吁在不断发展的AI创意领域中，更深入、更透明地关注艺术家的视角。

Abstract: As generative AI continues to reshape artistic production and alternate modes
of human expression, artists whose livelihoods are most directly affected have
raised urgent concerns about consent, transparency, and the future of creative
labor. However, the voices of artists are often marginalized in dominant public
and scholarly discourse. This study presents a twelve-year analysis, from 2013
to 2025, of English-language discourse surrounding AI-generated art. It draws
from 439 curated 500-word excerpts sampled from opinion articles, news reports,
blogs, legal filings, and spoken-word transcripts. Through a reproducible
methodology, we identify five stable thematic clusters and uncover a
misalignment between artists' perceptions and prevailing media narratives. Our
findings highlight how the use of technical jargon can function as a subtle
form of gatekeeping, often sidelining the very issues artists deem most urgent.
Our work provides a BERTopic-based methodology and a multimodal baseline for
future research, alongside a clear call for deeper, transparency-driven
engagement with artist perspectives in the evolving AI-creative landscape.

</details>


### [192] [Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03098)
*Haoran Wang,Xiongxiao Xu,Baixiang Huang,Kai Shu*

Main category: cs.CL

TL;DR: 针对检索增强生成（RAG）系统中敏感数据泄露的风险，本文提出了一种名为隐私感知解码（PAD）的轻量级推理时防御机制，通过自适应地注入校准高斯噪声来提供差分隐私保证，有效减少信息泄露并保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 检索增强生成（RAG）系统虽然能提升大型语言模型（LLM）的准确性，但在处理私有或敏感数据时，容易受到提取攻击，导致机密信息通过生成的响应泄露。

Method: 本文提出了隐私感知解码（PAD），这是一种在生成过程中向token logits自适应注入校准高斯噪声的推理时防御方法。PAD集成了基于置信度的筛选以选择性保护高风险token，高效的敏感性估计以最小化不必要的噪声，以及上下文感知的噪声校准以平衡隐私和生成质量。它使用Rényi差分隐私（RDP）会计师严格跟踪累积隐私损失，为敏感输出提供明确的每响应$(\varepsilon, \delta)$-DP保证。PAD是模型无关的，且仅在解码时操作，计算开销极小。

Result: 在三个真实世界数据集上的实验表明，PAD显著减少了私人信息泄露，同时保持了响应的实用性，并且优于现有的基于检索和后处理的防御方法。

Conclusion: PAD通过解码策略，在缓解RAG中的隐私风险方面迈出了重要一步，为敏感领域的通用和可扩展隐私解决方案铺平了道路。

Abstract: Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large
language models (LLMs) by conditioning outputs on external knowledge sources.
However, when retrieval involves private or sensitive data, RAG systems are
susceptible to extraction attacks that can leak confidential information
through generated responses. We propose Privacy-Aware Decoding (PAD), a
lightweight, inference-time defense that adaptively injects calibrated Gaussian
noise into token logits during generation. PAD integrates confidence-based
screening to selectively protect high-risk tokens, efficient sensitivity
estimation to minimize unnecessary noise, and context-aware noise calibration
to balance privacy with generation quality. A \renyi Differential Privacy (RDP)
accountant rigorously tracks cumulative privacy loss, enabling explicit
per-response $(\varepsilon, \delta)$-DP guarantees for sensitive outputs.
Unlike prior approaches requiring retraining or corpus-level filtering, PAD is
model-agnostic and operates entirely at decoding time with minimal
computational overhead. Experiments on three real-world datasets demonstrate
that PAD substantially reduces private information leakage while preserving
response utility, outperforming existing retrieval- and post-processing-based
defenses. Our work takes an important step toward mitigating privacy risks in
RAG via decoding strategies, paving the way for universal and scalable privacy
solutions in sensitive domains. Our code is available:
https://github.com/wang2226/PAD.

</details>


### [193] [Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation](https://arxiv.org/abs/2508.03110)
*Zizhong Li,Haopeng Zhang,Jiawei Zhang*

Main category: cs.CL

TL;DR: TPARAG是一种针对RAG系统的新型攻击框架，通过在token级别优化恶意内容，同时攻击检索和生成阶段，在白盒和黑盒场景下均有效，揭示了RAG的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在幻觉和知识过时问题。RAG框架通过外部知识增强LLMs，但引入了新的安全漏洞：外部数据库中的恶意内容可能被检索并用于操纵模型输出。现有针对RAG的攻击方法依赖于检索器访问或未能同时考虑检索和生成阶段，在黑盒场景下效果有限。

Method: 本文提出TPARAG（Token-level Precise Attack on the RAG），一个针对白盒和黑盒RAG系统的新型攻击框架。TPARAG利用一个轻量级白盒LLM作为攻击者，在token级别生成并迭代优化恶意段落，以确保其可检索性以及在生成阶段的高攻击成功率。

Result: 在开放域问答数据集上的大量实验表明，TPARAG在检索阶段和端到端攻击效果上均持续优于现有方法。

Conclusion: 这些结果进一步揭示了RAG管道中的关键漏洞，并为提高其鲁棒性提供了新的见解。

Abstract: While large language models (LLMs) have achieved remarkable success in
providing trustworthy responses for knowledge-intensive tasks, they still face
critical limitations such as hallucinations and outdated knowledge. To address
these issues, the retrieval-augmented generation (RAG) framework enhances LLMs
with access to external knowledge via a retriever, enabling more accurate and
real-time outputs about the latest events. However, this integration brings new
security vulnerabilities: the risk that malicious content in the external
database can be retrieved and used to manipulate model outputs. Although prior
work has explored attacks on RAG systems, existing approaches either rely
heavily on access to the retriever or fail to jointly consider both retrieval
and generation stages, limiting their effectiveness, particularly in black-box
scenarios. To overcome these limitations, we propose Token-level Precise Attack
on the RAG (TPARAG), a novel framework that targets both white-box and
black-box RAG systems. TPARAG leverages a lightweight white-box LLM as an
attacker to generate and iteratively optimize malicious passages at the token
level, ensuring both retrievability and high attack success in generation.
Extensive experiments on open-domain QA datasets demonstrate that TPARAG
consistently outperforms previous approaches in retrieval-stage and end-to-end
attack effectiveness. These results further reveal critical vulnerabilities in
RAG pipelines and offer new insights into improving their robustness.

</details>


### [194] [Cross-lingual Opinions and Emotions Mining in Comparable Documents](https://arxiv.org/abs/2508.03112)
*Motaz Saad,David Langlois,Kamel Smaili*

Main category: cs.CL

TL;DR: 本研究探讨了英阿可比文本中的情感和情绪差异，发现来自同一新闻机构的文章情感一致性较高，而来自不同机构的文章则存在分歧。


<details>
  <summary>Details</summary>
Motivation: 可比文本对于理解同一主题在不同语言中的讨论方式具有重要价值。此前研究尚未充分探索不同来源的可比文本在情感和情绪方面的差异，特别是跨语言情感和情绪的一致性问题。

Method: 研究首先对文本进行了情感和情绪标注。对于主观/客观意见分类，采用了不依赖机器翻译的跨语言方法。对于情绪（愤怒、厌恶、恐惧、喜悦、悲伤、惊讶）标注，手动将英语WordNet-Affect（WNA）词典翻译成阿拉伯语，创建了双语情感词典。随后，使用统计方法评估每对源-目标文档的情感和情绪一致性。研究使用了来自Euronews、BBC和Al-Jazeera（JSC）的英阿文档对。

Result: 结果显示，当文章来自同一新闻机构时，情感和情绪标注趋于一致；而当文章来自不同机构时，情感和情绪标注则出现分歧。

Conclusion: 所提出的方法是语言独立的，可推广到其他语言对。文档来源对跨语言可比文本的情感和情绪一致性具有显著影响。

Abstract: Comparable texts are topic-aligned documents in multiple languages that are
not direct translations. They are valuable for understanding how a topic is
discussed across languages. This research studies differences in sentiments and
emotions across English-Arabic comparable documents. First, texts are annotated
with sentiment and emotion labels. We apply a cross-lingual method to label
documents with opinion classes (subjective/objective), avoiding reliance on
machine translation. To annotate with emotions (anger, disgust, fear, joy,
sadness, surprise), we manually translate the English WordNet-Affect (WNA)
lexicon into Arabic, creating bilingual emotion lexicons used to label the
comparable corpora. We then apply a statistical measure to assess the agreement
of sentiments and emotions in each source-target document pair. This comparison
is especially relevant when the documents originate from different sources. To
our knowledge, this aspect has not been explored in prior literature. Our study
includes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera
(JSC). Results show that sentiment and emotion annotations align when articles
come from the same news agency and diverge when they come from different ones.
The proposed method is language-independent and generalizable to other language
pairs.

</details>


### [195] [Long Story Generation via Knowledge Graph and Literary Theory](https://arxiv.org/abs/2508.03137)
*Ge Shi,Kaiyu Huang,Guochen Feng*

Main category: cs.CL

TL;DR: 本文提出了一种多智能体故事生成器结构，通过引入记忆存储模型和故事主题障碍框架，并模拟写读者交互，以解决长篇故事生成中主题漂移和情节缺乏吸引力的问题，从而生成更高质量的故事。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大纲的多阶段长篇故事生成方法存在两个主要问题：由于对之前大纲的记忆丢失导致几乎不可避免的主题漂移，以及情节乏味、逻辑不连贯，对人类读者缺乏吸引力。

Method: 本文提出了一个多智能体故事生成器结构，以大型语言模型（LLMs）作为智能体的核心组件。为避免主题漂移，引入了一个记忆存储模型，包括识别重要记忆的长期记忆和保留最新大纲的短期记忆。为增加故事的吸引力，设计了一个基于文学叙事学理论的故事主题障碍框架，引入不确定因素和评估标准来生成大纲，通过构建知识图谱和整合新节点内容来增强故事吸引力。此外，建立了多智能体交互阶段，通过对话模拟写读者交互并根据反馈修改故事文本，以确保一致性和逻辑性。

Result: 与现有方法相比，本文提出的方法能够生成更高质量的长篇故事。

Conclusion: 通过多智能体结构、记忆存储模型、故事主题障碍框架以及模拟写读者交互，可以有效解决长篇故事生成中的主题漂移和情节吸引力不足的问题，显著提升生成故事的质量。

Abstract: The generation of a long story consisting of several thousand words is a
sub-task in the field of long text generation~(LTG). Previous research has
addressed this challenge through outline-based generation, which employs a
multi-stage method for generating outlines into stories. However, this approach
suffers from two common issues: almost inevitable theme drift caused by the
loss of memory of previous outlines, and tedious plots with incoherent logic
that are less appealing to human readers.
  In this paper, we propose the multi-agent Story Generator structure to
improve the multi-stage method, using large language models~(LLMs) as the core
components of agents. To avoid theme drift, we introduce a memory storage model
comprising two components: a long-term memory storage that identifies the most
important memories, thereby preventing theme drift; and a short-term memory
storage that retains the latest outlines from each generation round. To
incorporate engaging elements into the story, we design a story theme obstacle
framework based on literary narratology theory that introduces uncertain
factors and evaluation criteria to generate outline. This framework calculates
the similarity of the former storyline and enhances the appeal of the story by
building a knowledge graph and integrating new node content. Additionally, we
establish a multi-agent interaction stage to simulate writer-reader interaction
through dialogue and revise the story text according to feedback, to ensure it
remains consistent and logical. Evaluations against previous methods
demonstrate that our approach can generate higher-quality long stories.

</details>


### [196] [RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior](https://arxiv.org/abs/2508.03140)
*Junyao Yang,Jianwei Wang,Huiping Zhuang,Cen Chen,Ziqian Zeng*

Main category: cs.CL

TL;DR: 提出RCP-Merging方法，旨在以资源高效的方式将具有长CoT推理能力的LLM与特定领域知识结合，同时克服现有合并方法导致的推理能力下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型合并方法在合并长CoT模型和领域特定LLM时，会导致推理能力显著下降，甚至出现乱码或输出崩溃，因此需要一种能有效融合两者并保持性能的方法。

Method: RCP-Merging框架将推理模型权重视为基础先验，并利用推理能力指标来保留核心长CoT能力模型的权重，同时选择性地合并重要的领域特定权重，以实现双重能力的融合。

Result: 在生物医学和金融领域的Qwen2.5-7B、Llama3.1-8B和Qwen2.5-1.5B模型上的实验表明，RCP-Merging成功合并了推理模型和领域特定模型，使领域任务性能比现有最佳方法提高了9.5%和9.2%，同时没有显著损害原始的长CoT推理能力。

Conclusion: RCP-Merging是一种有效且资源高效的模型合并框架，能够将长CoT推理能力与领域特定知识融合到大型语言模型中，同时保持两种能力的高性能。

Abstract: Large Language Models (LLMs) with long chain-of-thought (CoT) capability,
termed Reasoning Models, demonstrate superior intricate problem-solving
abilities through multi-step long CoT reasoning. To create a dual-capability
model with long CoT capability and domain-specific knowledge without
substantial computational and data costs, model merging emerges as a highly
resource-efficient method. However, significant challenges lie in merging
domain-specific LLMs with long CoT ones since nowadays merging methods suffer
from reasoning capability degradation, even gibberish output and output
collapse. To overcome this, we introduce RCP-Merging: Merging Long
Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning
Capability as Prior, a novel merging framework designed to integrate
domain-specific LLMs with long CoT capability, meanwhile maintaining model
performance in the original domain. Treating reasoning model weights as
foundational prior, our method utilizes a reasoning capability indicator to
preserve core long CoT capability model weights while selectively merging
essential domain-specific weights. We conducted extensive experiments on
Qwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance
domains. Our results show that RCP-Merging successfully merges a reasoning
model with domain-specific ones, improving domain task performance by 9.5% and
9.2% over state-of-the-art methods, without significantly harming the original
long CoT reasoning capability.

</details>


### [197] [Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following](https://arxiv.org/abs/2508.03178)
*Chenyang Wang,Liang Wen,Shousheng Jia,Xiangzheng Zhang,Liang Xu*

Main category: cs.CL

TL;DR: 本文提出一个包含预览和自检的框架，通过数据筛选、Entropy-SFT和TEA-RL来解决LLM在复杂指令遵循中存在的“懒惰推理”问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在数学、编码和通用谜题方面表现出色，但在复杂指令遵循方面仍存在不一致性，主要归因于推理阶段的“懒惰推理”。

Method: 1. 识别“懒惰推理”为导致指令遵循不佳的主因。2. 提出一个包含预览和自检的严谨推理框架。3. 生成带有复杂约束的指令并筛选为hard、easy和pass三类数据集。4. 对pass提示进行拒绝采样，构建高质量冷启动数据集。5. 采用熵保持监督微调（Entropy-SFT）和基于规则密集奖励的逐令牌熵自适应强化学习（TEA-RL），以促进模型形成可泛化的预览和自检推理能力。

Result: 在指令遵循基准测试中，模型性能显著提升，且在各种模型规模上均有体现。Light-IF-32B模型甚至超越了更大的开源模型（如DeepSeek-R1）和闭源模型（如Doubao-1.6）。

Conclusion: 所提出的框架能有效缓解LLM的“懒惰推理”问题，通过促进严谨的推理过程（包括预览和自检），显著提高了模型遵循复杂指令的能力和推理的泛化性。

Abstract: While advancements in the reasoning abilities of LLMs have significantly
enhanced their performance in solving mathematical problems, coding tasks, and
general puzzles, their effectiveness in accurately adhering to instructions
remains inconsistent, particularly with more complex directives. Our
investigation identifies lazy reasoning during the thinking stage as the
primary factor contributing to poor instruction adherence. To mitigate this
issue, we propose a comprehensive framework designed to enable rigorous
reasoning processes involving preview and self-checking, essential for
satisfying strict instruction constraints. Specifically, we first generate
instructions with complex constraints and apply a filtering process to obtain
valid prompts, resulting in three distinct prompt datasets categorized as hard,
easy, and pass. Then, we employ rejection sampling on the pass prompts to
curate a small yet high-quality dataset, enabling a cold-start initialization
of the model and facilitating its adaptation to effective reasoning patterns.
Subsequently, we employ an entropy-preserving supervised fine-tuning
(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)
reinforcement learning guided by rule-based dense rewards. This approach
encourages the model to transform its reasoning mechanism, ultimately fostering
generalizable reasoning abilities that encompass preview and self-checking.
Extensive experiments conducted on instruction-following benchmarks demonstrate
remarkable performance improvements across various model scales. Notably, our
Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1
and closed-source models like Doubao-1.6.

</details>


### [198] [Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification](https://arxiv.org/abs/2508.03181)
*Lukas Pätz,Moritz Beyer,Jannik Späth,Lasse Bohlen,Patrick Zschech,Mathias Kraus,Julian Rosenberger*

Main category: cs.CL

TL;DR: 本研究利用机器学习模型分析了德国联邦议院近五年的约2.8万份演讲，以揭示政党间的话题趋势、情感分布和话语策略演变。


<details>
  <summary>Details</summary>
Motivation: 研究旨在深入探讨德国议会中的政治话语，解答关于话题演变、情感动态以及政党特定话语策略的关键问题。

Method: 分析了约28,000份议会演讲。开发并训练了两个机器学习模型，分别用于话题和情感分类，模型在手动标注数据集上进行训练。训练后的模型被应用于评估不同政党和不同时间段的话题趋势和情感分布。

Result: 模型表现出强大的分类性能（话题分类AUROC为0.94，情感分类AUROC为0.89）。分析揭示了政党与其议会角色之间的显著关系。观察到政党从执政党变为反对党时，其话语风格发生变化。意识形态立场和执政责任共同塑造了政治话语。

Conclusion: 德国联邦议院的政治话语不仅受意识形态影响，也受执政责任塑造，政党角色的转变会导致话语风格的变化。本研究直接回答了关于议题演变、情感动态和政党特定话语策略的关键问题。

Abstract: This study investigates political discourse in the German parliament, the
Bundestag, by analyzing approximately 28,000 parliamentary speeches from the
last five years. Two machine learning models for topic and sentiment
classification were developed and trained on a manually labeled dataset. The
models showed strong classification performance, achieving an area under the
receiver operating characteristic curve (AUROC) of 0.94 for topic
classification (average across topics) and 0.89 for sentiment classification.
Both models were applied to assess topic trends and sentiment distributions
across political parties and over time. The analysis reveals remarkable
relationships between parties and their role in parliament. In particular, a
change in style can be observed for parties moving from government to
opposition. While ideological positions matter, governing responsibilities also
shape discourse. The analysis directly addresses key questions about the
evolution of topics, sentiment dynamics, and party-specific discourse
strategies in the Bundestag.

</details>


### [199] [Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models](https://arxiv.org/abs/2508.03199)
*Muhammed Saeed,Shaina Raza,Ashmal Vayani,Muhammad Abdul-Mageed,Ali Emami,Shady Shehata*

Main category: cs.CL

TL;DR: 研究发现，文本到图像（T2I）模型中的语法性别会显著影响生成的视觉内容中的性别表示，导致超出刻板印象的偏差。


<details>
  <summary>Details</summary>
Motivation: 以往对T2I模型偏差的研究主要集中在人口统计学表示和刻板印象属性，忽略了语法性别如何跨语言影响视觉表示这一基本问题。

Method: 引入了一个跨语言基准，其中包含语法性别与刻板印象性别关联相矛盾的词语（例如，法语中语法为阴性但指代刻板印象为男性的“哨兵”）。数据集涵盖五种有性别语言（法语、西班牙语、德语、意大利语、俄语）和两种性别中立的对照语言（英语、中文），包含800个独特提示，在三种最先进的T2I模型上生成了28,800张图像。

Result: 分析显示，语法性别极大地影响图像生成：阳性语法标记平均将男性表示增加到73%（相比于性别中立英语的22%），而阴性语法标记将女性表示增加到38%（相比于英语的28%）。这些影响系统地因语言资源可用性和模型架构而异，高资源语言显示出更强的效果。

Conclusion: 研究结果表明，语言结构本身，而不仅仅是内容，塑造了AI生成的视觉输出，为理解多语言、多模态系统中的偏差和公平性引入了一个新维度。

Abstract: Research on bias in Text-to-Image (T2I) models has primarily focused on
demographic representation and stereotypical attributes, overlooking a
fundamental question: how does grammatical gender influence visual
representation across languages? We introduce a cross-linguistic benchmark
examining words where grammatical gender contradicts stereotypical gender
associations (e.g., ``une sentinelle'' - grammatically feminine in French but
referring to the stereotypically masculine concept ``guard''). Our dataset
spans five gendered languages (French, Spanish, German, Italian, Russian) and
two gender-neutral control languages (English, Chinese), comprising 800 unique
prompts that generated 28,800 images across three state-of-the-art T2I models.
Our analysis reveals that grammatical gender dramatically influences image
generation: masculine grammatical markers increase male representation to 73\%
on average (compared to 22\% with gender-neutral English), while feminine
grammatical markers increase female representation to 38\% (compared to 28\% in
English). These effects vary systematically by language resource availability
and model architecture, with high-resource languages showing stronger effects.
Our findings establish that language structure itself, not just content, shapes
AI-generated visual outputs, introducing a new dimension for understanding bias
and fairness in multilingual, multimodal systems.

</details>


### [200] [Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP](https://arxiv.org/abs/2508.03204)
*Abhirup Sinha,Pritilata Saha,Tithi Saha*

Main category: cs.CL

TL;DR: 本报告探讨了在大型语言模型训练数据中，为保护隐私而对文本数据进行匿名化的几种预处理方法。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型需要海量数据，其中常包含私人信息，且这些信息可能被提取，从而侵犯个人隐私权并违反如GDPR等数据保护法规。

Method: 报告聚焦于讨论几种现有的、领域无关的文本数据预处理方法，这些方法旨在对私人信息进行掩盖或假名化处理。

Result: 研究表明，私人信息可以从语言模型中被提取。本报告的成果是综述和探讨了多种用于文本数据匿名化的预处理方法。

Conclusion: 即使无法实现完全匿名化，通过预处理方法对文本数据中的私人信息进行掩盖或假名化对于保护数据隐私至关重要，尤其是在处理大型语言模型所需数据时。

Abstract: Privacy is a fundamental human right. Data privacy is protected by different
regulations, such as GDPR. However, modern large language models require a huge
amount of data to learn linguistic variations, and the data often contains
private information. Research has shown that it is possible to extract private
information from such language models. Thus, anonymizing such private and
sensitive information is of utmost importance. While complete anonymization may
not be possible, a number of different pre-processing approaches exist for
masking or pseudonymizing private information in textual data. This report
focuses on a few of such approaches for domain-agnostic NLP tasks.

</details>


### [201] [Probing Syntax in Large Language Models: Successes and Remaining Challenges](https://arxiv.org/abs/2508.03211)
*Pablo J. Diego-Simón,Emmanuel Chemla,Jean-Rémi King,Yair Lakretz*

Main category: cs.CL

TL;DR: 结构探针在评估大型语言模型（LLM）的句法表示时，存在距离偏见、难以处理深层结构和干扰词等问题，且不受词语可预测性影响。


<details>
  <summary>Details</summary>
Motivation: 现有的结构探针通常在未加区分的句子集上进行评估，导致不清楚结构和/或统计因素是否系统性地影响LLM的句法表示。本研究旨在深入分析此问题。

Method: 在三个受控基准上对结构探针进行了深入分析，以评估其性能和受到的影响。

Result: 研究发现三点：1) 结构探针受词语距离影响，距离越近越可能被认为是句法关联；2) 结构探针难以表示深层句法结构，并受交互名词或不合语法动词形式的干扰；3) 结构探针似乎不受单个词语可预测性的影响。

Conclusion: 这项工作揭示了结构探针当前面临的挑战，并强调了使用受控刺激基准来更好评估其性能的重要性。

Abstract: The syntactic structures of sentences can be readily read-out from the
activations of large language models (LLMs). However, the ``structural probes''
that have been developed to reveal this phenomenon are typically evaluated on
an indiscriminate set of sentences. Consequently, it remains unclear whether
structural and/or statistical factors systematically affect these syntactic
representations. To address this issue, we conduct an in-depth analysis of
structural probes on three controlled benchmarks. Our results are three-fold.
First, structural probes are biased by a superficial property: the closer two
words are in a sentence, the more likely structural probes will consider them
as syntactically linked. Second, structural probes are challenged by linguistic
properties: they poorly represent deep syntactic structures, and get interfered
by interacting nouns or ungrammatical verb forms. Third, structural probes do
not appear to be affected by the predictability of individual words. Overall,
this work sheds light on the current challenges faced by structural probes.
Providing a benchmark made of controlled stimuli to better evaluate their
performance.

</details>


### [202] [CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting](https://arxiv.org/abs/2508.03240)
*Mutaz Ayesh,Nicolás Gutiérrez-Rolón,Fernando Alva-Manchego*

Main category: cs.CL

TL;DR: 本文介绍了CardiffNLP团队在CLEARS西班牙语文本改编共享任务中的贡献，采用LLM提示方法，最终使用Gemma-3模型，在两个子任务中分别获得第三名和第二名。


<details>
  <summary>Details</summary>
Motivation: 参与IberLEF 2025主办的CLEARS西班牙语文本改编共享任务，并解决其包含的两个子任务。

Method: 采用LLM（大型语言模型）提示方法，并尝试了不同的提示变体。初期实验使用了LLaMA-3.2，但最终提交采用了Gemma-3模型。

Result: 在子任务1中获得第三名，在子任务2中获得第二名。论文详细描述了团队的多种提示变体、示例和实验结果。

Conclusion: 基于LLM提示的方法，特别是使用Gemma-3模型，在西班牙语文本改编任务中表现出色，取得了较高的排名。

Abstract: This paper details the CardiffNLP team's contribution to the CLEARS shared
task on Spanish text adaptation, hosted by IberLEF 2025. The shared task
contained two subtasks and the team submitted to both. Our team took an
LLM-prompting approach with different prompt variations. While we initially
experimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and
landed third place in Subtask 1 and second place in Subtask 2. We detail our
numerous prompt variations, examples, and experimental results.

</details>


### [203] [Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs](https://arxiv.org/abs/2508.03247)
*Shintaro Sakai,Jisun An,Migyeong Kang,Haewoon Kwak*

Main category: cs.CL

TL;DR: 研究发现，大型语言模型（LLMs）在模拟西方和东方抑郁症症状报告的文化差异方面表现不佳，即使使用文化角色提示。尽管使用东方语言提示有所改善，但模型对文化角色敏感度低且存在强烈的文化无关症状等级是主要原因，表明当前LLMs不具备安全的心理健康应用所需的文化意识能力。


<details>
  <summary>Details</summary>
Motivation: 先前的临床心理学研究表明，西方抑郁症患者倾向于报告心理症状，而东方患者则报告躯体症状。鉴于LLMs在心理健康领域日益增长的应用，研究旨在测试LLMs是否能再现这些文化模式。

Method: 通过使用西方或东方角色提示LLMs，并在英语以及主要东方语言（如中文、日语和印地语）中进行测试，以观察LLMs是否能复现抑郁症症状报告的文化模式。

Result: 结果显示，LLMs在英语提示下未能很好地复制这些模式。尽管使用主要东方语言提示时，在某些配置下有所改善，但LLMs未能完全对齐。分析指出失败的两个主要原因：模型对文化角色的敏感度低，以及存在一个强大的、文化不变的症状等级，该等级覆盖了文化线索。

Conclusion: 研究得出结论，虽然提示语言很重要，但当前的通用LLMs缺乏强大、具有文化意识的能力，而这些能力对于安全有效的心理健康应用至关重要。

Abstract: Prior clinical psychology research shows that Western individuals with
depression tend to report psychological symptoms, while Eastern individuals
report somatic ones. We test whether Large Language Models (LLMs), which are
increasingly used in mental health, reproduce these cultural patterns by
prompting them with Western or Eastern personas. Results show that LLMs largely
fail to replicate the patterns when prompted in English, though prompting in
major Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment
in several configurations. Our analysis pinpoints two key reasons for this
failure: the models' low sensitivity to cultural personas and a strong,
culturally invariant symptom hierarchy that overrides cultural cues. These
findings reveal that while prompt language is important, current
general-purpose LLMs lack the robust, culture-aware capabilities essential for
safe and effective mental health applications.

</details>


### [204] [RooseBERT: A New Deal For Political Language Modelling](https://arxiv.org/abs/2508.03250)
*Deborah Dore,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 本文提出了RooseBERT，一个专门用于政治话语分析的预训练语言模型，它在政治辩论分析任务上显著优于通用语言模型。


<details>
  <summary>Details</summary>
Motivation: 政治辩论和相关讨论日益增多，需要新的计算方法来自动分析这些内容，以帮助公民理解政治审议。然而，政治语言的特殊性及其论证形式（包含隐藏的沟通策略和隐含论点）使得即使是当前的通用预训练语言模型也难以有效处理。

Method: 研究者引入了RooseBERT，一个针对政治话语语言的预训练语言模型。该模型在大型英语政治辩论和演讲语料库（8K场辩论）上进行训练。为了评估其性能，研究者将其在四个政治辩论分析下游任务上进行了微调，包括命名实体识别、情感分析、论证组件检测和分类，以及论证关系预测和分类。

Result: RooseBERT在所有四个下游任务上均表现出比通用语言模型显著的性能提升，这表明领域特定的预训练能有效增强政治辩论分析的性能。

Conclusion: 领域特定的预训练能够显著提高政治辩论分析任务的性能。RooseBERT作为专门的政治话语语言模型，在相关任务上超越了通用模型。该模型已向研究社区发布。

Abstract: The increasing amount of political debates and politics-related discussions
calls for the definition of novel computational methods to automatically
analyse such content with the final goal of lightening up political
deliberation to citizens. However, the specificity of the political language
and the argumentative form of these debates (employing hidden communication
strategies and leveraging implicit arguments) make this task very challenging,
even for current general-purpose pre-trained Language Models. To address this
issue, we introduce a novel pre-trained Language Model for political discourse
language called RooseBERT. Pre-training a language model on a specialised
domain presents different technical and linguistic challenges, requiring
extensive computational resources and large-scale data. RooseBERT has been
trained on large political debate and speech corpora (8K debates, each composed
of several sub-debates on different topics) in English. To evaluate its
performances, we fine-tuned it on four downstream tasks related to political
debate analysis, i.e., named entity recognition, sentiment analysis, argument
component detection and classification, and argument relation prediction and
classification. Our results demonstrate significant improvements over
general-purpose Language Models on these four tasks, highlighting how
domain-specific pre-training enhances performance in political debate analysis.
We release the RooseBERT language model for the research community.

</details>


### [205] [Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition](https://arxiv.org/abs/2508.03259)
*Duzhen Zhang,Chenxing Li,Jiahua Dong,Qi Liu,Dong Yu*

Main category: cs.CL

TL;DR: 本文提出了一种名为稳定性-可塑性权衡（SPT）的方法，用于持续命名实体识别（CNER），旨在平衡模型在学习新实体类型时的旧知识保留（稳定性）和新知识获取（可塑性）。


<details>
  <summary>Details</summary>
Motivation: 以往的CNER方法主要依赖知识蒸馏（KD），导致模型过度稳定而可塑性不足，难以有效获取新知识。此外，CNER中非实体类型的语义漂移问题常被忽视。

Method: SPT方法从表示和权重两个层面平衡稳定性与可塑性。在表示层面，通过在KD中引入池化操作来增加可塑性；在权重层面，动态融合新旧模型权重，并采用权重引导的选择机制。此外，还开发了一种基于置信度的伪标签方法，利用旧模型预测当前非实体类型，以应对语义漂移。

Result: 在三个基准数据集的十种CNER设置上进行的广泛实验表明，SPT方法优于以前的CNER方法。

Conclusion: SPT方法有效实现了稳定性与可塑性的平衡，显著提升了CNER的性能。

Abstract: Continual Named Entity Recognition (CNER) is an evolving field that focuses
on sequentially updating an existing model to incorporate new entity types.
Previous CNER methods primarily utilize Knowledge Distillation (KD) to preserve
prior knowledge and overcome catastrophic forgetting, strictly ensuring that
the representations of old and new models remain consistent. Consequently, they
often impart the model with excessive stability (i.e., retention of old
knowledge) but limited plasticity (i.e., acquisition of new knowledge). To
address this issue, we propose a Stability-Plasticity Trade-off (SPT) method
for CNER that balances these aspects from both representation and weight
perspectives. From the representation perspective, we introduce a pooling
operation into the original KD, permitting a level of plasticity by
consolidating representation dimensions. From the weight perspective, we
dynamically merge the weights of old and new models, strengthening old
knowledge while maintaining new knowledge. During this fusion, we implement a
weight-guided selective mechanism to prioritize significant weights. Moreover,
we develop a confidence-based pseudo-labeling approach for the current
non-entity type, which predicts entity types using the old model to handle the
semantic shift of the non-entity type, a challenge specific to CNER that has
largely been ignored by previous methods. Extensive experiments across ten CNER
settings on three benchmark datasets demonstrate that our SPT method surpasses
previous CNER approaches, highlighting its effectiveness in achieving a
suitable stability-plasticity trade-off.

</details>


### [206] [Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?](https://arxiv.org/abs/2508.03262)
*Junhyuk Choi,Hyeonchu Park,Haemin Lee,Hyebeen Shin,Hyun Joung Jin,Bugeun Kim*

Main category: cs.CL

TL;DR: 本研究评估了大型语言模型（LLMs）使用真实人类数据预测个体经济决策的能力，发现在个体层面预测困难，但在群体层面表现合理，且复杂提示方法效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现有关于LLMs模拟人类行为的研究多依赖虚构角色而非真实人类数据，本研究旨在弥补这一局限性，评估LLMs在真实经济决策场景中的预测能力。

Method: 研究通过“随心付”（PWYW）定价实验，使用来自522名韩国参与者的真实详细个人信息，评估了三种先进的多模态LLMs预测文化消费场景中个体经济决策的能力。同时，探究了人格注入方法对预测性能的影响。

Result: 结果显示，LLMs难以进行精确的个体层面预测，但能合理地捕捉群体层面的行为趋势。此外，常用的提示技术（如个人叙事重构或检索增强生成）并未比简单提示方法带来显著性能提升。

Conclusion: 本研究首次全面评估了LLMs使用真实人类数据模拟经济行为的能力，为计算社会科学中基于角色的模拟提供了实证指导。

Abstract: Recent advances in Large Language Models (LLMs) have generated significant
interest in their capacity to simulate human-like behaviors, yet most studies
rely on fictional personas rather than actual human data. We address this
limitation by evaluating LLMs' ability to predict individual economic
decision-making using Pay-What-You-Want (PWYW) pricing experiments with real
522 human personas. Our study systematically compares three state-of-the-art
multimodal LLMs using detailed persona information from 522 Korean participants
in cultural consumption scenarios. We investigate whether LLMs can accurately
replicate individual human choices and how persona injection methods affect
prediction performance. Results reveal that while LLMs struggle with precise
individual-level predictions, they demonstrate reasonable group-level
behavioral tendencies. Also, we found that commonly adopted prompting
techniques are not much better than naive prompting methods; reconstruction of
personal narrative nor retrieval augmented generation have no significant gain
against simple prompting method. We believe that these findings can provide the
first comprehensive evaluation of LLMs' capabilities on simulating economic
behavior using real human data, offering empirical guidance for persona-based
simulation in computational social science.

</details>


### [207] [LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning](https://arxiv.org/abs/2508.03275)
*Jiahao Zhao*

Main category: cs.CL

TL;DR: LECTOR是一种基于LLM的自适应间隔重复算法，专为考试型学习设计，通过语义分析和个性化配置解决词汇学习中的语义混淆问题，在模拟测试中显著提高了成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的间隔重复系统在处理语义干扰和个性化适应方面存在不足，特别是在以成功率为核心的考试型学习场景（如语言考试）中，语义混淆是一个关键挑战。

Method: LECTOR利用大型语言模型（LLMs）进行语义分析和语义相似度评估，并结合个性化学习档案和既定的间隔重复原则。该算法旨在解决词汇学习中的语义混淆问题。

Result: 在针对100名模拟学习者进行100天的评估中，LECTOR的成功率为90.2%，优于表现最佳的基线算法（SSP-MMC，88.4%），相对提升了2.0%。该算法在处理语义相似概念方面表现出色，减少了由混淆引起的错误，同时保持了计算效率。

Conclusion: LECTOR为智能辅导系统和自适应学习平台提供了一个有前景的方向，证明了其在提高考试型学习成功率方面的潜力，尤其是在处理语义相似概念时。

Abstract: Spaced repetition systems are fundamental to efficient learning and memory
retention, but existing algorithms often struggle with semantic interference
and personalized adaptation. We present LECTOR (\textbf{L}LM-\textbf{E}nhanced
\textbf{C}oncept-based \textbf{T}est-\textbf{O}riented \textbf{R}epetition), a
novel adaptive scheduling algorithm specifically designed for test-oriented
learning scenarios, particularly language examinations where success rate is
paramount. LECTOR leverages large language models for semantic analysis while
incorporating personalized learning profiles, addressing the critical challenge
of semantic confusion in vocabulary learning by utilizing LLM-powered semantic
similarity assessment and integrating it with established spaced repetition
principles. Our comprehensive evaluation against six baseline algorithms
(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over
100 days demonstrates significant improvements: LECTOR achieves a 90.2\%
success rate compared to 88.4\% for the best baseline (SSP-MMC), representing a
2.0\% relative improvement. The algorithm shows particular strength in handling
semantically similar concepts, reducing confusion-induced errors while
maintaining computational efficiency. Our results establish LECTOR as a
promising direction for intelligent tutoring systems and adaptive learning
platforms.

</details>


### [208] [Do language models accommodate their users? A study of linguistic convergence](https://arxiv.org/abs/2508.03276)
*Terra Blevins,Susanne Schmalwieser,Benjamin Roth*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在对话中表现出语言趋同现象，但其趋同模式与人类不同，且通常存在过度拟合。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在生成语言方面表现出色，但它们在语言使用上与人类的相似度（特别是语言趋同这一核心语用元素）仍未被充分研究。本文旨在探究模型是否会像人类一样适应或趋同于用户的语言模式。

Method: 研究者系统地比较了16个语言模型在3个对话语料库中对现有对话的补全，并与原始人类响应进行了对比，使用了多种文体特征进行分析。

Result: 研究发现模型强烈趋同于对话风格，但相对于人类基线，这种趋同往往存在显著的过度拟合。趋同模式通常是特征特定的，并且在不同模型设置下存在一致的趋同变化，其中指令微调模型和大型模型的趋同程度低于其预训练对应模型。

Conclusion: 鉴于人类和模型趋同模式的差异，研究者推测这两种行为的底层机制可能非常不同。

Abstract: While large language models (LLMs) are generally considered proficient in
generating language, how similar their language usage is to that of humans
remains understudied. In this paper, we test whether models exhibit linguistic
convergence, a core pragmatic element of human language communication, asking:
do models adapt, or converge, to the linguistic patterns of their user? To
answer this, we systematically compare model completions of exisiting dialogues
to the original human responses across sixteen language models, three dialogue
corpora, and a variety of stylometric features. We find that models strongly
converge to the conversation's style, often significantly overfitting relative
to the human baseline. While convergence patterns are often feature-specific,
we observe consistent shifts in convergence across modeling settings, with
instruction-tuned and larger models converging less than their pretrained
counterparts. Given the differences between human and model convergence
patterns, we hypothesize that the underlying mechanisms for these behaviors are
very different.

</details>


### [209] [Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes](https://arxiv.org/abs/2508.03292)
*Shahed Masoudian,Gustavo Escobedo,Hannah Strauss,Markus Schedl*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）存在性别偏见，尤其是在生成长篇内容时。本研究通过引入基于心理学性别刻板印象的叙事生成任务和数据集，发现LLMs在无条件提示下偏向男性，但基于非性别刻板印象的属性进行条件生成可缓解偏见。结合多个男性刻板印象属性会加剧偏见，而女性刻板印象属性则能缓解偏见，且模型偏见与心理学真实情况一致，并随模型增大而增强。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）在各种应用中的普及，人们对其可能放大性别偏见的担忧日益增加。以往的研究通常使用显式性别线索或在短文本任务中探测偏见，这可能忽视了LLMs在生成更长内容时所嵌入的更隐性的偏见形式。

Method: 本研究通过开放式叙事生成任务，利用心理学中研究的性别刻板印象（例如攻击性或八卦）来调查LLMs中的性别偏见。为此，引入了一个名为StereoBias-Stories的新数据集，包含无条件或根据25个心理学刻板印象中的（一个、两个或六个）随机属性以及三个任务相关的故事结局进行条件生成的故事。分析了故事中性别贡献如何随这些属性的变化而变化。

Result: ['在无条件提示下，模型平均高度偏向男性，但基于与性别刻板印象无关的属性进行条件生成可缓解这种偏见。', '结合与相同性别刻板印象相关的多个属性会加剧模型行为：男性刻板印象属性会放大偏见，而女性刻板印象属性则会缓解偏见。', '模型偏见与心理学真实情况（用于分类的）相符，并且这种对齐强度随模型尺寸的增大而增强。']

Conclusion: 这些发现共同强调了对大型语言模型进行基于心理学原理的评估的重要性。

Abstract: As Large Language Models (LLMs) are increasingly used across different
applications, concerns about their potential to amplify gender biases in
various tasks are rising. Prior research has often probed gender bias using
explicit gender cues as counterfactual, or studied them in sentence completion
and short question answering tasks. These formats might overlook more implicit
forms of bias embedded in generative behavior of longer content. In this work,
we investigate gender bias in LLMs using gender stereotypes studied in
psychology (e.g., aggressiveness or gossiping) in an open-ended task of
narrative generation. We introduce a novel dataset called StereoBias-Stories
containing short stories either unconditioned or conditioned on (one, two, or
six) random attributes from 25 psychological stereotypes and three task-related
story endings. We analyze how the gender contribution in the overall story
changes in response to these attributes and present three key findings: (1)
While models, on average, are highly biased towards male in unconditioned
prompts, conditioning on attributes independent from gender stereotypes
mitigates this bias. (2) Combining multiple attributes associated with the same
gender stereotype intensifies model behavior, with male ones amplifying bias
and female ones alleviating it. (3) Model biases align with psychological
ground-truth used for categorization, and alignment strength increases with
model size. Together, these insights highlight the importance of
psychology-grounded evaluation of LLMs.

</details>


### [210] [NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty](https://arxiv.org/abs/2508.03294)
*Leonidas Zotos,Ivo Pascal de Jong,Matias Valdenegro-Toro,Andreea Ioana Sburlea,Malvina Nissim,Hedderik van Rijn*

Main category: cs.CL

TL;DR: 该研究比较了大型语言模型（LLM）和教授在估计考试题目难度方面的能力，发现基于LLM不确定性的监督学习方法表现最佳，可帮助教授提高评估质量。


<details>
  <summary>Details</summary>
Motivation: 开发高质量的考试需要准确估计试题难度，但教授们在这方面表现不佳。

Method: 研究将多种基于LLM的方法（直接询问Gemini 2.5、利用LLM不确定性进行监督学习）与三位教授进行比较。任务是估计学生在神经网络和机器学习领域的是非题上给出正确答案的百分比。监督学习仅使用了42个训练样本。

Result: 结果显示，教授们区分试题难度的能力有限，且表现不如直接询问Gemini 2.5。而利用LLM不确定性进行监督学习的方法，即使仅有42个训练样本，也取得了更好的结果。

Conclusion: 研究得出结论，利用LLM不确定性进行监督学习可以帮助教授更好地估计考试题目难度，从而提高评估质量。

Abstract: Estimating the difficulty of exam questions is essential for developing good
exams, but professors are not always good at this task. We compare various
Large Language Model-based methods with three professors in their ability to
estimate what percentage of students will give correct answers on True/False
exam questions in the areas of Neural Networks and Machine Learning. Our
results show that the professors have limited ability to distinguish between
easy and difficult questions and that they are outperformed by directly asking
Gemini 2.5 to solve this task. Yet, we obtained even better results using
uncertainties of the LLMs solving the questions in a supervised learning
setting, using only 42 training samples. We conclude that supervised learning
using LLM uncertainty can help professors better estimate the difficulty of
exam questions, improving the quality of assessment.

</details>


### [211] [Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling](https://arxiv.org/abs/2508.03296)
*Anqi Li,Wenwei Jin,Jintao Tong,Pengda Qin,Weijia Li,Guo Lu*

Main category: cs.CL

TL;DR: 本文提出了Hi-Guard，一个多模态内容审核框架，通过分层管道和与政策对齐的决策范式，提高了审核的准确性、泛化能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有内容审核系统主要依赖于嘈杂的标签驱动学习，缺乏与审核规则的对齐，并且产生的决策不透明，阻碍了人工审查，难以在大规模应用中同时保证效率、准确性和可解释性。

Method: Hi-Guard引入了新的政策对齐决策范式。其“分层”体现在两个方面：1) 分层审核管道，轻量级二元模型首先过滤安全内容，然后更强的模型处理细粒度风险分类；2) 第二阶段的分层分类法，模型对从粗到细粒度的分层分类法进行基于路径的分类。为确保与政策对齐，Hi-Guard将规则定义直接整合到模型提示中。此外，引入了多级软边距奖励并使用组相对策略优化（GRPO）进行优化，以惩罚语义相邻的错误分类并提高解释质量。

Result: 广泛的实验和实际部署表明，Hi-Guard在分类准确性、泛化能力和可解释性方面均表现出色。

Conclusion: Hi-Guard为构建可扩展、透明和值得信赖的内容安全系统铺平了道路。

Abstract: Social platforms have revolutionized information sharing, but also
accelerated the dissemination of harmful and policy-violating content. To
ensure safety and compliance at scale, moderation systems must go beyond
efficiency and offer accuracy and interpretability. However, current approaches
largely rely on noisy, label-driven learning, lacking alignment with moderation
rules and producing opaque decisions that hinder human review. Therefore, we
propose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that
introduces a new policy-aligned decision paradigm. The term "Hierarchical"
reflects two key aspects of our system design: (1) a hierarchical moderation
pipeline, where a lightweight binary model first filters safe content and a
stronger model handles fine-grained risk classification; and (2) a hierarchical
taxonomy in the second stage, where the model performs path-based
classification over a hierarchical taxonomy ranging from coarse to fine-grained
levels. To ensure alignment with evolving moderation policies, Hi-Guard
directly incorporates rule definitions into the model prompt. To further
enhance structured prediction and reasoning, we introduce a multi-level
soft-margin reward and optimize with Group Relative Policy Optimization (GRPO),
penalizing semantically adjacent misclassifications and improving explanation
quality. Extensive experiments and real-world deployment demonstrate that
Hi-Guard achieves superior classification accuracy, generalization, and
interpretability, paving the way toward scalable, transparent, and trustworthy
content safety systems. Code is available at:
https://github.com/lianqi1008/Hi-Guard.

</details>


### [212] [CTTS: Collective Test-Time Scaling](https://arxiv.org/abs/2508.03333)
*Zhende Song,Shengji Tang,Peng Ye,Jiayuan Fan,Tao Chen*

Main category: cs.CL

TL;DR: 本文探索了集体测试时缩放（CTTS）以提升LLM性能，提出并验证了三种范式，发现多智能体对多奖励模型（MA-MR）表现最佳。基于此，提出CTTS-MM框架，通过智能体协作搜索（ACS）和奖励模型混合（MoR）进一步增强推理效果。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放（TTS）无需额外训练即可提升大型语言模型（LLM）性能，但现有方法如Best-of-N和Self-Consistency受限于单智能体单奖励模型（SA-SR）范式。近期研究表明，集体智能体方法能突破单智能体系统的上限，因此有必要探索集体测试时缩放（CTTS）。

Method: 本文首次探索集体测试时缩放（CTTS），设计了三种主要范式来研究其最优模式：1) 单智能体对多奖励模型（SA-MR）；2) 多智能体对单奖励模型（MA-SR）；3) 多智能体对多奖励模型（MA-MR）。在此基础上，提出CTTS-MM框架，其中包含：多智能体协作的“智能体协作搜索（ACS）”模块，用于从候选池中选择最有效的LLM智能体组合；以及多奖励模型协作的“奖励模型混合（MoR）”模块，通过精心策划的问题池和“先验奖励模型集成选择（PRES）”以及“成对奖励排名（PRR）”指标来选择最佳奖励模型组合。

Result: 广泛实验表明，多智能体对多奖励模型（MA-MR）范式持续取得最佳性能。所提出的CTTS-MM框架在七个主流基准测试中均持续获得卓越的性能。

Conclusion: 集体测试时缩放（CTTS），特别是多智能体对多奖励模型（MA-MR）范式，是提升LLM性能的有效途径。所提出的CTTS-MM框架通过结合多智能体和多奖励模型协作，显著增强了LLM的推理能力。

Abstract: Test-time scaling (TTS) has emerged as a promising research field for
enhancing the effectiveness of large language models (LLMs) without extra
training. However, most existing approaches, e.g., Best-of-N and
Self-Consistency rely on a single agent interacting with a reward model
(SA-SR), constrained by limited capabilities of a single test-time scaling
(STTS) paradigm. On the other hand, recent works demonstrate that
collective-agent methods can break through the upper bound of single-agent
systems by orchestrating diverse models. Thus, in this paper, we take a first
step towards exploring Collective Test-Time Scaling (CTTS). Consider the
different interaction types of single and multiple models, we design three
primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent
to multiple reward models (SA-MR); (2) multiple agents to single reward model
(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive
experiments demonstrate that MA-MR consistently achieves the best performance.
Based on this, we propose a novel framework named CTTS-MM that effectively
leverages both multi-agent and multi-reward-model collaboration for enhanced
inference. Specifically, for multi-agent collaboration, we propose an Agent
Collaboration Search (ACS), which searches for the most effective combination
of LLM agents from a large candidate pool; for multi-reward-model
collaboration, we propose Mixture of Reword Models (MoR), which consists of a
curated question pool and a Prior Reward model Ensemble Selection (PRES) to
select the optimal combinations of reward models via Pair-wise Reward Ranking
(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that
the proposed CTTS-MM consistently obtains superior performance. Code will be
released at https://github.com/magent4aci/CTTS-MM.

</details>


### [213] [Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature](https://arxiv.org/abs/2508.03358)
*Tiago G Canário,Catarina Duarte,Flávio L. Pinheiro,João L. M. Pereira*

Main category: cs.CL

TL;DR: 本文提出了一个名为Taggus的流水线，用于从葡萄牙语文学作品中自动识别人物及其互动，并构建社交网络。该方法结合词性标注和启发式规则，在人物识别和共指消解方面F1分数达到94.1%，在互动检测方面达到75.9%，显著优于现有SOTA工具。


<details>
  <summary>Details</summary>
Motivation: 自动识别小说中的人物及其互动是一项复杂任务，现有NLP方法（如NER）在此任务中表现不佳，尤其是在葡萄牙语等数据标注较少的语言中，导致构建人物社交网络困难。

Method: 研究人员提出了Taggus流水线，该流水线不依赖于现成的NER工具或大型语言模型，而是利用词性标注（POS tagging）和一系列启发式规则来识别人物并解决共指问题，进而检测人物间的互动。

Result: Taggus流水线在人物识别和共指消解任务中取得了94.1%的平均F1分数，在互动检测任务中取得了75.9%的F1分数。与现有SOTA工具（包括NER工具和ChatGPT）相比，Taggus在人物识别和共指消解方面提升了50.7%，在互动检测方面提升了22.3%。

Conclusion: Taggus流水线在葡萄牙语文学作品的人物及互动提取方面表现出色，显著超越了现有最先进的工具。该研究为低资源语言的角色网络构建提供了有效解决方案，并已公开Taggus流水线以促进该领域的发展。未来工作将探索人物间关系的检测。

Abstract: Automatically identifying characters and their interactions from fiction
books is, arguably, a complex task that requires pipelines that leverage
multiple Natural Language Processing (NLP) methods, such as Named Entity
Recognition (NER) and Part-of-speech (POS) tagging. However, these methods are
not optimized for the task that leads to the construction of Social Networks of
Characters. Indeed, the currently available methods tend to underperform,
especially in less-represented languages, due to a lack of manually annotated
data for training. Here, we propose a pipeline, which we call Taggus, to
extract social networks from literary fiction works in Portuguese. Our results
show that compared to readily available State-of-the-Art tools -- off-the-shelf
NER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which
uses POS tagging and a combination of heuristics, achieves satisfying results
with an average F1-Score of $94.1\%$ in the task of identifying characters and
solving for co-reference and $75.9\%$ in interaction detection. These
represent, respectively, an increase of $50.7\%$ and $22.3\%$ on results
achieved by the readily available State-of-the-Art tools. Further steps to
improve results are outlined, such as solutions for detecting relationships
between characters. Limitations on the size and scope of our testing samples
are acknowledged. The Taggus pipeline is publicly available to encourage
development in this field for the Portuguese language.2

</details>


### [214] [Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models](https://arxiv.org/abs/2508.03363)
*Haotian Wu,Bo Xu,Yao Shu,Menglin Yang,Chengwei Qin*

Main category: cs.CL

TL;DR: 本文提出了一种名为JointThinking的新型上下文学习（ICL）范式，通过利用“思考”和“不思考”两种推理模式之间的结构化差异，显著提高了大型语言模型（RLLMs）的推理准确性和鲁棒性，同时保持了较低的延迟开销。


<details>
  <summary>Details</summary>
Motivation: 尽管推理型大型语言模型（RLLMs）在结构化和多步推理方面表现出色，但其在上下文学习（ICL）方面的潜力仍未得到充分探索，现有研究主要集中于改进训练和推理策略。

Method: 提出JointThinking范式：模型并行生成两个答案，一个在“思考”模式下，一个在“不思考”模式下。仅当两个初始回答不一致时（这种情况很少发生，如GSM8K中仅占6%），才触发第二轮“思考”，此时使用一个结合了原始问题和两个候选答案的单一提示。由于大多数情况下只需一轮推理，因此延迟开销极小。

Result: JointThinking在多个推理基准测试中显著优于少样本思维链（CoT）和多数投票，并提升了答案鲁棒性。其在同分布任务上表现与基于训练的SOTA方法相当，但在异分布任务上则显著优于SOTA。系统分析表明，利用不同推理模式能持续降低错误率，且第二轮思考中，实际与理想推理之间的性能差距随模型规模增大而缩小，显示出良好的可扩展性。

Conclusion: JointThinking是一种有效且可扩展的RLLM上下文学习方法，通过利用不同推理模式间的结构化差异，在提高推理准确性和鲁棒性方面表现出色，尤其在异分布任务上优势明显，并强调了结构化思维多样性的价值。本文也讨论了当前局限性并展望了未来ICL研究方向。

Abstract: Reasoning large language models (RLLMs) have recently demonstrated remarkable
capabilities through structured and multi-step reasoning. While prior research
has primarily focused on improving their training and inference strategies,
their potential for in-context learning (ICL) remains largely underexplored. To
fill this gap, we propose Thinking with Nothinking Calibration (JointThinking),
a new ICL paradigm that leverages the structured difference between two
reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.
Specifically, our method prompts the model to generate two answers in parallel:
one in Thinking mode and the other in Nothinking mode. A second round of
Thinking is triggered only when the two initial responses are inconsistent,
using a single prompt that incorporates the original question and both
candidate answers. Since such disagreement occurs infrequently (e.g., only 6\%
in GSM8K), our method performs just one round of reasoning in most cases,
resulting in minimal latency overhead. Extensive experiments across multiple
reasoning benchmarks demonstrate that JointThinking significantly outperforms
few-shot chain-of-thought (CoT) and majority voting with improved answer
robustness. Moreover, It achieves comparable in-distribution performance to
training-based SOTA method, while substantially outperforming on
out-of-distribution tasks. We further conduct a systematic analysis of the
calibration mechanism, showing that leveraging different reasoning modes
consistently lowers the error rate and highlights the value of structural
thinking diversity. Additionally, we observe that the performance gap between
actual and ideal reasoning narrows as model size increases in the second round
of thinking, indicating the strong scalability of our approach. Finally, we
discuss current limitations and outline promising directions for future ICL
research in RLLMs.

</details>


### [215] [ReDSM5: A Reddit Dataset for DSM-5 Depression Detection](https://arxiv.org/abs/2508.03399)
*Eliseo Bao,Anxo Pérez,Javier Parapar*

Main category: cs.CL

TL;DR: 该论文介绍了一个名为ReDSM5的新型Reddit语料库，通过心理学家对帖子进行句子级DSM-5抑郁症症状标注和临床解释，旨在提高社交媒体抑郁症检测的临床相关性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 抑郁症诊断存在传统临床障碍和污名化问题，导致许多病例未被确诊。现有社交媒体抑郁症计算方法通常只进行二元分类（抑郁/非抑郁），未能将语言与DSM-5的具体症状标准关联，缺乏临床相关性和可解释性。

Method: 构建ReDSM5语料库，包含1484篇长篇Reddit帖子，由持证心理学家在句子层面针对DSM-5的九种抑郁症症状进行详尽标注，并为每个标签提供简洁的临床理由。对语料库进行探索性分析，考察词汇、句法和情感模式。为多标签症状分类和解释生成建立了基准线。

Result: ReDSM5独特地结合了症状特异性监督和专家解释，促进了能够检测抑郁症并生成人类可解释推理的模型开发。通过探索性分析揭示了社交媒体叙事中症状表达的模式。为未来的检测和可解释性研究提供了基准结果。

Conclusion: ReDSM5语料库通过提供与DSM-5症状挂钩的专家标注和解释，弥补了现有方法的不足，为开发具有临床相关性和可解释性的抑郁症检测模型奠定了基础，并为后续研究提供了可参考的基准。

Abstract: Depression is a pervasive mental health condition that affects hundreds of
millions of individuals worldwide, yet many cases remain undiagnosed due to
barriers in traditional clinical access and pervasive stigma. Social media
platforms, and Reddit in particular, offer rich, user-generated narratives that
can reveal early signs of depressive symptomatology. However, existing
computational approaches often label entire posts simply as depressed or not
depressed, without linking language to specific criteria from the DSM-5, the
standard clinical framework for diagnosing depression. This limits both
clinical relevance and interpretability. To address this gap, we introduce
ReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each
exhaustively annotated at the sentence level by a licensed psychologist for the
nine DSM-5 depression symptoms. For each label, the annotator also provides a
concise clinical rationale grounded in DSM-5 methodology. We conduct an
exploratory analysis of the collection, examining lexical, syntactic, and
emotional patterns that characterize symptom expression in social media
narratives. Compared to prior resources, ReDSM5 uniquely combines
symptom-specific supervision with expert explanations, facilitating the
development of models that not only detect depression but also generate
human-interpretable reasoning. We establish baseline benchmarks for both
multi-label symptom classification and explanation generation, providing
reference results for future research on detection and interpretability.

</details>


### [216] [Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations](https://arxiv.org/abs/2508.03420)
*Bing Wang,Ximing Li,Yiming Wang,Changchun Li,Jiaxu Cui,Renchu Guan,Bo Yang*

Main category: cs.CL

TL;DR: 针对社交媒体上新闻真实性动态变化的挑战，本文提出MISDER框架，通过学习并预测动态社会环境表示来改进谣言检测，并验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的虚假信息泛滥且有害。主流的谣言检测方法将问题视为静态学习，但新闻的真实性在动态演变的社会环境中可能会波动，这与现实不符。

Method: 提出MISDER（Misinformation detection with Dynamic Environmental Representations）框架。核心思想是为每个时期学习一个社会环境表示，并使用时间模型预测未来时期的表示。具体时间模型包括LSTM、连续动力学方程和预训练动力学系统，形成MISDER-LSTM、MISDER-ODE和MISDER-PT三个变体。

Result: 在两个流行的数据集上与多种基线模型进行比较，实验结果表明所提出的MISDER模型是有效的。

Conclusion: MISDER框架通过捕捉新闻真实性的动态变化，有效提升了谣言检测的性能，解决了现有静态检测方法的局限性。

Abstract: The proliferation of misinformation across diverse social media platforms has
drawn significant attention from both academic and industrial communities due
to its detrimental effects. Accordingly, automatically distinguishing
misinformation, dubbed as Misinformation Detection (MD), has become an
increasingly active research topic. The mainstream methods formulate MD as a
static learning paradigm, which learns the mapping between the content, links,
and propagation of news articles and the corresponding manual veracity labels.
However, the static assumption is often violated, since in real-world
scenarios, the veracity of news articles may vacillate within the dynamically
evolving social environment. To tackle this problem, we propose a novel
framework, namely Misinformation detection with Dynamic Environmental
Representations (MISDER). The basic idea of MISDER lies in learning a social
environmental representation for each period and employing a temporal model to
predict the representation for future periods. In this work, we specify the
temporal model as the LSTM model, continuous dynamics equation, and pre-trained
dynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,
MISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,
we compare it to various MD baselines across 2 prevalent datasets, and the
experimental results can indicate the effectiveness of our proposed model.

</details>


### [217] [LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models](https://arxiv.org/abs/2508.03440)
*Junhong Wu,Jinliang Lu,Zixuan Ren,Ganqiang Hu,Zhi Wu,Dai Dai,Hua Wu*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）的“软思考”能力，发现其在连续概念空间推理中倾向于贪婪解码。通过引入随机性，特别是Gumbel-Softmax技巧，可以有效提升软思考的性能。


<details>
  <summary>Details</summary>
Motivation: 现有推理模型依赖生成离散标记，限制了表达能力。近期工作尝试让LLMs生成软、抽象标记以在连续概念空间进行推理，但其内部行为和效率有待深入探究。

Method: 使用一系列探查技术检查LLMs的内部行为，以理解其“软思考”能力。为解决发现的问题，探索了引入随机性的采样策略，包括Dirichlet重采样和Gumbel-Softmax技巧。

Result: 研究发现LLMs在“软思考”中主要依赖软输入中最具影响力的分量，这阻碍了不同推理路径的探索，使香草软思考退化为一种贪婪解码。引入随机性，尤其是Gumbel-Softmax技巧，能够缓解这些局限性，并在八个推理基准上实现卓越性能。

Conclusion: 香草“软思考”在LLMs中表现出贪婪解码的倾向，未能充分利用软标记传递的信息。通过引入适当的随机性（如Gumbel-Softmax技巧），可以有效克服这一限制，释放“软思考”在连续概念空间推理中的潜力。

Abstract: Human cognition naturally engages with abstract and fluid concepts, whereas
existing reasoning models often rely on generating discrete tokens, potentially
constraining their expressive capabilities. Recent advancements aim to address
this limitation by enabling large language models (LLMs) to generate soft,
abstract tokens, thus facilitating reasoning within a continuous concept space.
This paper explores the `Soft Thinking' capabilities of various LLMs by
examining the models' internal behavior using a suite of probing techniques.
Contrary to the common belief that Soft Thinking enables the simultaneous
exploration of diverse reasoning paths, our findings reveal that LLMs
predominantly rely on the most influential component of the soft inputs during
subsequent decoding steps. This reliance hinders the exploration of different
reasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,
obscuring the advantage of transmitting more information through Soft Tokens.
To tackle this issue, we explore sampling strategies to introduce
\emph{randomness}, employing methods such as Dirichlet resampling and the
Gumbel-Softmax trick. Our experiments demonstrate that incorporating randomness
can alleviate the limitations of vanilla approaches and unleash the potential
of Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate
randomness with controlled smoothness, resulting in superior performance across
eight reasoning benchmarks.

</details>


### [218] [Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings](https://arxiv.org/abs/2508.03453)
*Rita González-Márquez,Philipp Berens,Dmitry Kobak*

Main category: cs.CL

TL;DR: 该研究系统比较了两种自监督文本嵌入对比学习中的数据增强策略，发现裁剪增强优于基于Dropout的方法，且自监督微调在域内数据上能快速生成高质量嵌入。


<details>
  <summary>Details</summary>
Motivation: 文本嵌入在NLP中至关重要，但当前顶尖模型依赖大量监督微调。与计算机视觉中自监督学习的成功形成对比，本研究旨在探索自监督方法在文本嵌入生成中的潜力，特别是评估数据增强策略的效果。

Method: 研究系统比较了两种最知名的自监督数据增强策略（裁剪增强与基于Dropout的方法）用于对比学习中正样本对的生成。通过MTEB基准测试和额外的域内评估来评估嵌入质量。同时，分析了Transformer模型中不同层在微调过程中的变化，并测试了仅微调最后几层的效果。

Result: 研究发现，裁剪增强在性能上显著优于基于Dropout的方法。在域外数据上，自监督生成的嵌入质量低于监督SOTA模型；但在域内数据上，经过非常短的微调（有时仅略低于监督SOTA），自监督微调能产生高质量的文本嵌入。此外，表示质量随Transformer层数的增加而提高，最后几层在微调过程中变化最大，且仅微调这些最后层就足以达到相似的嵌入质量。

Conclusion: 自监督微调，特别是结合裁剪增强，是一种在文本嵌入领域有前景的方法，尤其适用于快速生成高质量域内嵌入。通过集中微调Transformer模型的最后几层，可以有效提升训练效率并保持高表示质量。

Abstract: Text embeddings, i.e. vector representations of entire texts, play an
important role in many NLP applications, such as retrieval-augmented
generation, sentiment analysis, clustering, or visualizing collections of texts
for data exploration. Currently, top-performing embedding models are derived
from pre-trained language models via extensive supervised fine-tuning using
curated text pairs. This contrasts with computer vision, where self-supervised
training based on data augmentations has demonstrated remarkable success. Here
we systematically compare the two most well-known augmentation strategies for
positive pair generation in contrastive learning of text embeddings. We assess
embedding quality on MTEB and additional in-domain evaluations and show that
cropping augmentation strongly outperforms the dropout-based approach. We find
that on out-of-domain data, the quality of resulting embeddings is below the
supervised SOTA models, but for in-domain data, self-supervised fine-tuning
produces high-quality text embeddings after very short fine-tuning, sometimes
only marginally below the supervised SOTA. Finally, we show that representation
quality increases towards the last transformer layers, which undergo the
largest change during fine-tuning; and that fine-tuning only those last layers
is sufficient to reach similar embedding quality.

</details>


### [219] [fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval](https://arxiv.org/abs/2508.03475)
*Pranshu Rastogi*

Main category: cs.CL

TL;DR: 该研究针对SemEval-2025任务7的多语言和跨语言事实核查声明检索，采用基于预训练Transformer的轻量级双编码器模型，通过学习排序任务实现了高成功率。


<details>
  <summary>Details</summary>
Motivation: 参与SemEval-2025任务7：多语言和跨语言事实核查声明检索，旨在开发有效的方法来检索跨语言的事实核查声明。

Method: 将任务视为学习排序问题，使用一个从预训练Transformer（针对句子相似性优化）微调的双编码器模型。多语言检索训练时使用源语言及其英语翻译，而跨语言检索仅使用英语翻译。模型参数少于5亿，在Kaggle T4 GPU上进行训练。

Result: 在多语言检索中实现了92%的Success@10，在跨语言检索中实现了80%的Success@10。在跨语言赛道中排名第5，在多语言赛道中排名第10。

Conclusion: 所提出的基于轻量级双编码器模型的方法，通过特定的多语言和跨语言训练策略，在事实核查声明检索任务中表现出色，取得了较高的成功率和良好的竞赛排名。

Abstract: SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim
Retrieval is approached as a Learning-to-Rank task using a bi-encoder model
fine-tuned from a pre-trained transformer optimized for sentence similarity.
Training used both the source languages and their English translations for
multilingual retrieval and only English translations for cross-lingual
retrieval. Using lightweight models with fewer than 500M parameters and
training on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual
and 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.

</details>


### [220] [CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation](https://arxiv.org/abs/2508.03489)
*Kaiwen Zhao,Bharathan Balaji,Stephen Lee*

Main category: cs.CL

TL;DR: 该论文旨在解决从PDF格式的产品可持续性报告中提取碳足迹信息并回答相关问题的挑战。为此，作者提出了一个新数据集CarbonPDF-QA，并开发了一种基于LLM的新技术CarbonPDF（通过微调Llama 3），该技术在处理非结构化和不一致文本方面表现优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 产品可持续性报告通常以PDF格式发布，包含表格和文本的混合内容，且缺乏标准化，导致难以从大量文档中提取和解释相关信息，特别是关于碳足迹的数据。

Method: 研究者创建了CarbonPDF-QA，一个包含1735份产品报告的问答对和人工标注答案的开源数据集。他们分析了GPT-4o在处理数据不一致性时的不足。在此基础上，提出了CarbonPDF，一种专门用于回答碳足迹问题的基于LLM的技术，通过使用其训练数据微调Llama 3模型实现。

Result: 分析显示GPT-4o在处理数据不一致性时表现不佳。所提出的CarbonPDF技术（通过微调Llama 3开发）在回答碳足迹问题上，优于包括在表格和文本数据上微调的现有最先进问答系统。

Conclusion: CarbonPDF是一种有效且性能卓越的LLM-based技术，能够解决从非结构化和不一致的PDF可持续性报告中提取碳足迹信息并回答相关问题的难题，显著优于现有SOTA方法。

Abstract: Product sustainability reports provide valuable insights into the
environmental impacts of a product and are often distributed in PDF format.
These reports often include a combination of tables and text, which complicates
their analysis. The lack of standardization and the variability in reporting
formats further exacerbate the difficulty of extracting and interpreting
relevant information from large volumes of documents. In this paper, we tackle
the challenge of answering questions related to carbon footprints within
sustainability reports available in PDF format. Unlike previous approaches, our
focus is on addressing the difficulties posed by the unstructured and
inconsistent nature of text extracted from PDF parsing. To facilitate this
analysis, we introduce CarbonPDF-QA, an open-source dataset containing
question-answer pairs for 1735 product report documents, along with
human-annotated answers. Our analysis shows that GPT-4o struggles to answer
questions with data inconsistencies. To address this limitation, we propose
CarbonPDF, an LLM-based technique specifically designed to answer carbon
footprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama
3 with our training data. Our results show that our technique outperforms
current state-of-the-art techniques, including question-answering (QA) systems
finetuned on table and text data.

</details>


### [221] [UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression](https://arxiv.org/abs/2508.03520)
*Md Rakibul Hasan,Md Zakir Hossain,Aneesh Krishna,Shafin Rahman,Tom Gedeon*

Main category: cs.CL

TL;DR: 本文提出了UPLME，一个不确定性感知的概率语言建模框架，用于解决同理心回归中自报告分数带噪声的问题，并在公共基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 同理心回归中的监督学习面临自报告同理心分数噪声大的挑战。尽管文本分类中处理噪声标签的算法很多，但回归任务中对噪声标签的处理相对未被充分探索。

Method: UPLME框架包含一个概率语言模型，该模型能预测同理心分数和异方差不确定性，并使用贝叶斯概念和变分模型集成进行训练。此外，引入了两个新颖的损失组件：一个惩罚退化的不确定性量化（UQ），另一个强制输入对之间的相似性。

Result: UPLME在两个带有标签噪声的公共基准上，性能达到了文献报道的最先进水平（Pearson相关系数：0.558→0.580和0.629→0.634）。通过合成标签噪声注入，UPLME能有效根据预测的不确定性区分噪声样本和干净样本。UPLME还优于最近的基于变分模型集成的回归UQ方法（校准误差：0.571→0.376）。

Conclusion: UPLME是一个有效且鲁棒的框架，能够处理同理心回归中带有噪声的标签，并通过其不确定性量化能力，在性能和噪声样本识别方面均表现出色。

Abstract: Supervised learning for empathy regression is challenged by noisy
self-reported empathy scores. While many algorithms have been proposed for
learning with noisy labels in textual classification problems, the regression
counterpart is relatively under-explored. We propose UPLME, an
uncertainty-aware probabilistic language modelling framework to capture label
noise in the regression setting of empathy detection. UPLME includes a
probabilistic language model that predicts both empathy score and
heteroscedastic uncertainty and is trained using Bayesian concepts with
variational model ensembling. We further introduce two novel loss components:
one penalises degenerate Uncertainty Quantification (UQ), and another enforces
the similarity between the input pairs on which we predict empathy. UPLME
provides state-of-the-art performance (Pearson Correlation Coefficient:
$0.558\rightarrow0.580$ and $0.629\rightarrow0.634$) in terms of the
performance reported in the literature in two public benchmarks, having label
noise. Through synthetic label noise injection, we show that UPLME is effective
in separating noisy and clean samples based on the predicted uncertainty. UPLME
further outperform (Calibration error: $0.571\rightarrow0.376$) a recent
variational model ensembling-based UQ method designed for regression problems.

</details>


### [222] [FilBench: Can LLMs Understand and Generate Filipino?](https://arxiv.org/abs/2508.03523)
*Lester James V. Miranda,Elyanah Aco,Conner Manuel,Jan Christian Blaise Cruz,Joseph Marvin Imperial*

Main category: cs.CL

TL;DR: 本文介绍了FilBench，一个旨在评估大型语言模型（LLMs）在菲律宾语、他加禄语和宿务语方面能力的基准测试，发现现有LLMs在菲律宾语阅读理解和翻译上表现不佳，即使是最佳模型GPT-4o也仅达到72.23%的得分。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在英语任务上表现出色，但其在特定语言（如菲律宾语）上的能力尚不明确。现有研究存在空白，缺乏针对菲律宾语系语言的专门基准测试。

Method: 研究者引入了FilBench，一个以菲律宾为中心的基准测试，旨在评估LLMs在菲律宾语、他加禄语和宿务语上的多种任务和能力。FilBench的任务经过精心策划，涵盖文化知识、经典自然语言处理、阅读理解和生成等领域。研究者在FilBench上评估了27个最先进的LLMs。

Result: 评估结果显示，许多LLMs在阅读理解和翻译能力方面表现不佳。FilBench具有挑战性，即使是表现最好的模型GPT-4o也只取得了72.23%的得分。此外，专门针对东南亚语言训练的模型在FilBench上表现往往不佳，其中表现最好的SEA-LION v3 70B模型仅获得61.07%的得分。

Conclusion: 这项工作证明了策划特定语言LLM基准测试的价值，有助于推动菲律宾自然语言处理的进展，并促进菲律宾语言在LLM开发中的包容性。

Abstract: Despite the impressive performance of LLMs on English-based tasks, little is
known about their capabilities in specific languages such as Filipino. In this
work, we address this gap by introducing FilBench, a Filipino-centric benchmark
designed to evaluate LLMs across a diverse set of tasks and capabilities in
Filipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to
reflect the priorities and trends of NLP research in the Philippines such as
Cultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By
evaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs
suffer from reading comprehension and translation capabilities. Our results
indicate that FilBench is challenging, with the best model, GPT-4o, achieving
only a score of 72.23%. Moreover, we also find that models trained specifically
for Southeast Asian languages tend to underperform on FilBench, with the
highest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.
Our work demonstrates the value of curating language-specific LLM benchmarks to
aid in driving progress on Filipino NLP and increasing the inclusion of
Philippine languages in LLM development.

</details>


### [223] [Marito: Structuring and Building Open Multilingual Terminologies for South African NLP](https://arxiv.org/abs/2508.03529)
*Vukosi Marivate,Isheanesu Dzingirai,Fiskani Banda,Richard Lastrucci,Thapelo Sindane,Keabetswe Madumo,Kayode Olaleye,Abiodun Modupe,Unarine Netshifhefhe,Herkulaas Combrink,Mohlatlego Nakeng,Matome Ledwaba*

Main category: cs.CL

TL;DR: Marito项目通过聚合、清洗和标准化南非官方语言的术语列表，创建了开放、可互操作的结构化数据集，显著提升了大型语言模型在特定领域英-特语机器翻译的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 南非官方语言缺乏结构化术语数据，现有政府和学术术语列表分散且为非机器可读格式，阻碍了多语言NLP的进展和计算研究与开发。

Method: 系统地聚合、清洗和标准化分散的术语资源，形成开放、可互操作的Marito数据集（基于NOODL框架发布）。将该术语数据集成到检索增强生成（RAG）管道中，并进行实验。

Result: 实验表明，将Marito术语数据集成到RAG管道后，大型语言模型在英-特语（Tshivenda）机器翻译的准确性和领域特定一致性方面有显著提升。

Conclusion: Marito为开发健壮和公平的NLP技术奠定了可扩展的基础，确保了南非丰富的语言多样性在数字时代得到代表。

Abstract: The critical lack of structured terminological data for South Africa's
official languages hampers progress in multilingual NLP, despite the existence
of numerous government and academic terminology lists. These valuable assets
remain fragmented and locked in non-machine-readable formats, rendering them
unusable for computational research and development. \emph{Marito} addresses
this challenge by systematically aggregating, cleaning, and standardising these
scattered resources into open, interoperable datasets. We introduce the
foundational \emph{Marito} dataset, released under the equitable,
Africa-centered NOODL framework. To demonstrate its immediate utility, we
integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.
Experiments show substantial improvements in the accuracy and domain-specific
consistency of English-to-Tshivenda machine translation for large language
models. \emph{Marito} provides a scalable foundation for developing robust and
equitable NLP technologies, ensuring South Africa's rich linguistic diversity
is represented in the digital age.

</details>


### [224] [EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models](https://arxiv.org/abs/2508.03533)
*Xiaoming Hou,Jiquan Zhang,Zibin Lin,DaCheng Tao,Shengli Zhang*

Main category: cs.CL

TL;DR: EmbedGrad是一种通过梯度优化文本提示嵌入的新框架，解耦训练与部署，显著提升基础模型在多任务上的适应性，尤其对小型模型在复杂任务上效果显著。


<details>
  <summary>Details</summary>
Motivation: 预训练基础模型在适应多样化任务时面临挑战。现有方法（离散提示工程和连续参数调整）存在局限：离散方法缺乏精度，参数方法增加复杂性并降低可解释性。

Method: 提出EmbedGrad框架，通过基于梯度的优化方法精炼文本提示嵌入。该方法独特之处在于训练与部署解耦：训练时，利用带标签数据指导精确嵌入调整并保持语义；推理时，仅将优化后的嵌入与用户查询结合。

Result: 在数学推理、情感分析和因果判断任务上的综合评估证明了EmbedGrad的有效性。例如，优化Qwen2.5-Math-1.5B的推理提示，数学问题准确率从14.74%提高到58.96%。在不同模型规模（0.5B-14B）和所有任务上都观察到持续改进，特别是小型模型在复杂问题（如因果判断）上获得了显著提升。

Conclusion: 该工作在不改变模型架构的情况下，弥合了提示工程和参数效率之间的鸿沟，将嵌入精炼确立为任务适应的一种强大新范式。

Abstract: Effectively adapting powerful pretrained foundation models to diverse tasks
remains a key challenge in AI deployment. Current approaches primarily follow
two paradigms:discrete optimization of text prompts through prompt engineering,
or continuous adaptation via additional trainable parameters. Both exhibit
limitations-discrete methods lack refinement precision while parameter-based
techniques increase complexity and reduce interpretability. To address these
constraints, we propose EmbedGrad, a novel framework that optimizes text prompt
embeddings through gradient-based refinement. Our approach uniquely decouples
training from deployment:during optimization,labeled examples guide precise
embedding adjustments while preserving semantic meaning; during inference, only
optimized embeddings integrate with user queries. This enables fine-grained
calibration impossible in text space, such as enhancing the reasoning
capability of prompts like please reason step by step. Comprehensive
evaluations across mathematical reasoning, sentiment analysis, and causal
judgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning
prompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% on
mathematical problems. Consistent improvements were observed across model
scales (0.5B-14B) and all tasks, with particularly significant gains for
smaller models on complex problems like causal judgment. By bridging prompt
engineering and parameter efficiency without architectural changes, our work
establishes embedding refinement as a powerful new paradigm for task
adaptation.

</details>


### [225] [Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations](https://arxiv.org/abs/2508.03550)
*Peng Lai,Jianjie Zheng,Sijie Cheng,Yun Chen,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: LAGER通过聚合LLM内部中间层的表示来提高“LLM即评委”与人类偏好的一致性，无需复杂提示或微调，实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 大规模评估任务中，“LLM即评委”的自动化评估范式被广泛采用，但如何在不使用复杂提示或微调的情况下提高其与人类偏好的一致性仍然是一个挑战。初步研究发现，LLM的中间层到上层编码的语义和任务相关表示通常比最终层更符合人类判断，这激发了本研究。

Method: 本文提出了LAGER框架，通过聚合跨层得分-token的logits，并从基于softmax的分布中计算预期得分，以生成细粒度的判断分数。该方法保持LLM骨干网络冻结，充分利用不同层之间的互补信息，克服了仅依赖最终层的局限性。

Result: LAGER在标准对齐基准（Flask、HelpSteer、BIGGen）上，使用Spearman相关性评估，相对于最佳基线实现了高达7.5%的改进。在不使用推理步骤的情况下，LAGER能够匹配或超越基于推理的方法。在数据选择和情感理解等下游应用中也验证了其有效性。

Conclusion: LAGER通过利用LLM的内部表示，有效增强了“LLM即评委”与人类评分的一致性，提供了一个轻量且高效的解决方案。它在多个基准测试中表现优异，甚至在没有推理步骤的情况下也能超越或匹配基于推理的方法，并在实际应用中展现了良好效果。

Abstract: The growing scale of evaluation tasks has led to the widespread adoption of
automated evaluation using large language models, a paradigm known as
"LLMas-a-judge." However, improving its alignment with human preferences
without complex prompts or fine-tuning remains challenging. In this work,
motivated by preliminary findings that middle-to-upper layers encode
semantically and taskrelevant representations that are often more aligned with
human judgments than the final layer, we propose LAGER, a lightweight and
efficient framework for enhancing LLM-as-a-Judge alignment with human scoring,
via internal representations. LAGER produces fine-grained judgment scores by
aggregating cross-layer scoretoken logits and computing the expected score from
a softmax-based distribution, with the LLM backbone kept frozen. LAGER fully
leverages the complementary information across different layers, overcoming the
limitations of relying solely on the final layer. We evaluate our method on the
standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman
correlation, and find that LAGER achieves improvements of up to 7.5% over the
best baseline across these benchmarks. Without reasoning steps, LAGER matches
or outperforms reasoning-based methods. Experiments on downstream applications,
such as data selection and emotional understanding, further show the
effectiveness of our method.

</details>


### [226] [Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation](https://arxiv.org/abs/2508.03571)
*Iing Muttakhiroh,Thomas Fevens*

Main category: cs.CL

TL;DR: KILO是一种新型的持续学习框架，通过结合动态知识图谱和指令微调，帮助大型语言模型在领域迁移中克服灾难性遗忘，同时提升新领域适应性和旧知识保留能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在面对领域迁移时，常因灾难性遗忘而导致性能下降。

Method: 本文提出了KILO（Knowledge-Instructed Learning for Continual Adaptation）框架，它将动态知识图谱与指令微调相结合。KILO在训练过程中利用检索到的特定领域知识作为指导，以增强对新领域的适应性和对先前知识的保留。模型在WikiText-103上进行预训练，并在BioASQ、SciQ、TweetEval和MIND四个不同目标领域进行顺序适应性评估。

Result: KILO在逆向迁移、正向迁移、F1分数、知识保留率和训练效率方面，持续优于强基线模型，包括持续微调、ERNIE 2.0和CPT。

Conclusion: 结合结构化知识检索和指令提示，能够有效克服持续学习场景中的领域迁移挑战。

Abstract: Large Language Models (LLMs) often suffer from performance degradation when
faced with domain shifts, primarily due to catastrophic forgetting. In this
work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),
a novel continual learning framework that integrates dynamic knowledge graphs
with instruction tuning. By leveraging retrieved domain-specific knowledge as
guidance during training, KILO enhances both adaptability to new domains and
retention of previously acquired knowledge. We pretrain our model on
WikiText-103 and evaluate sequential adaptation across four diverse target
domains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that
KILO consistently outperforms strong baselines, including continual
fine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward
transfer, F1 score, retention rate, and training efficiency. These results
highlight the effectiveness of combining structured knowledge retrieval and
instruction prompting to overcome domain shift challenges in continual learning
scenarios.

</details>


### [227] [Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?](https://arxiv.org/abs/2508.03644)
*Wenxuan Shen,Mingjia Wang,Yaochen Wang,Dongping Chen,Junjie Yang,Yao Wan,Weiwei Lin*

Main category: cs.CL

TL;DR: 本文提出了一个名为Double-Bench的大规模、多语言、多模态评估系统，用于全面评估文档RAG系统，以克服现有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的RAG系统评估基准存在不足，它们通常只关注文档RAG系统的特定部分，使用合成数据，且缺乏完整的真实标签和证据，未能反映现实世界的瓶颈和挑战，从而阻碍了多模态大型语言模型（MLLM）在复杂文档理解方面RAG系统的发展。

Method: 研究者引入了Double-Bench，一个包含3,276份文档（72,880页）和5,168个单跳及多跳查询的评估系统，涵盖6种语言和4种文档类型，并支持动态更新以解决潜在的数据污染。所有查询都基于详尽扫描的证据页面，并经过人工专家验证。研究者使用Double-Bench对9种最先进的嵌入模型、4种MLLM和4种端到端文档RAG框架进行了全面实验。

Result: 实验结果表明，文本和视觉嵌入模型之间的差距正在缩小，这凸显了构建更强大的文档检索模型的必要性。此外，研究发现当前的文档RAG框架存在过度自信的困境，即使没有证据支持也倾向于提供答案。

Conclusion: Double-Bench提供了一个严格的基础，有望推动未来先进文档RAG系统的研究。研究团队计划及时检索语料并每年发布新的基准。

Abstract: Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language
Models (MLLMs) show great promise for complex document understanding, yet their
development is critically hampered by inadequate evaluation. Current benchmarks
often focus on specific part of document RAG system and use synthetic data with
incomplete ground truth and evidence labels, therefore failing to reflect
real-world bottlenecks and challenges. To overcome these limitations, we
introduce Double-Bench: a new large-scale, multilingual, and multimodal
evaluation system that is able to produce fine-grained assessment to each
component within document RAG systems. It comprises 3,276 documents (72,880
pages) and 5,168 single- and multi-hop queries across 6 languages and 4
document types with streamlined dynamic update support for potential data
contamination issues. Queries are grounded in exhaustively scanned evidence
pages and verified by human experts to ensure maximum quality and completeness.
Our comprehensive experiments across 9 state-of-the-art embedding models, 4
MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text
and visual embedding models is narrowing, highlighting the need in building
stronger document retrieval models. Our findings also reveal the
over-confidence dilemma within current document RAG frameworks that tend to
provide answer even without evidence support. We hope our fully open-source
Double-Bench provide a rigorous foundation for future research in advanced
document RAG systems. We plan to retrieve timely corpus and release new
benchmarks on an annual basis.

</details>


### [228] [Can Large Vision-Language Models Understand Multimodal Sarcasm?](https://arxiv.org/abs/2508.03654)
*Xinyu Wang,Yue Zhang,Liqiang Jing*

Main category: cs.CL

TL;DR: 本文评估了大型视觉语言模型（LVLMs）在多模态讽刺分析（MSA）中的表现，发现其在视觉理解和概念知识方面的局限性，并提出了一种免训练框架，通过深度对象提取和外部概念知识来提升LVLMs的讽刺理解与解释能力。


<details>
  <summary>Details</summary>
Motivation: 讽刺是一种复杂的语言现象，对情感分析等任务构成挑战。传统讽刺检测方法主要侧重文本，尽管已有结合多模态信息的方法，但大型视觉语言模型（LVLMs）在多模态讽刺分析（MSA）中的应用仍未被充分探索。现有LVLMs在视觉理解和概念知识方面存在不足，亟需解决。

Method: 研究人员评估了LVLMs在多模态讽刺检测和多模态讽刺解释任务中的性能。他们通过实验识别了LVLMs的关键局限性（如视觉理解不足和概念知识缺乏）。为解决这些问题，提出了一种免训练框架，该框架整合了深度对象提取和外部概念知识，以增强模型解释多模态语境中讽刺的能力。

Result: 综合实验结果表明，LVLMs在多模态讽刺分析中存在视觉理解不足和概念知识缺乏等局限性。然而，所提出的免训练框架在多个模型上均显示出有效性，显著提升了模型解释和理解多模态讽刺的能力。

Conclusion: 大型视觉语言模型在多模态讽刺分析中面临视觉理解和概念知识的挑战。通过引入深度对象提取和外部概念知识的免训练框架，可以有效提升LVLMs在多模态语境中解释和理解讽刺的能力。

Abstract: Sarcasm is a complex linguistic phenomenon that involves a disparity between
literal and intended meanings, making it challenging for sentiment analysis and
other emotion-sensitive tasks. While traditional sarcasm detection methods
primarily focus on text, recent approaches have incorporated multimodal
information. However, the application of Large Visual Language Models (LVLMs)
in Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we
evaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm
Detection and Multimodal Sarcasm Explanation. Through comprehensive
experiments, we identify key limitations, such as insufficient visual
understanding and a lack of conceptual knowledge. To address these issues, we
propose a training-free framework that integrates in-depth object extraction
and external conceptual knowledge to improve the model's ability to interpret
and explain sarcasm in multimodal contexts. The experimental results on
multiple models show the effectiveness of our proposed framework. The code is
available at https://github.com/cp-cp/LVLM-MSA.

</details>


### [229] [CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction](https://arxiv.org/abs/2508.03668)
*Zixuan Li,Binzong Geng,Jing Xiong,Yong He,Yuxuan Hu,Jian Chen,Dingwei Chen,Xiyu Chang,Liang Zhang,Linjian Mo,Chengming Li,Chuan Yuan,Zhenan Sun*

Main category: cs.CL

TL;DR: 针对推荐系统中基于语言模型（LM）的点击率（CTR）预测中用户行为序列与自然语言不匹配导致的语义碎片化问题，本文提出了CTR-Sink框架，通过引入行为级注意力汇聚点（attention sinks）来引导LM的注意力，从而提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 尽管语言模型在语义理解和上下文建模方面能力强大，但用户行为序列（离散动作、语义空分隔符）与LM预训练的连贯自然语言存在根本性结构差异。这种不匹配导致语义碎片化，使得LM的注意力分散在无关标记上，未能有效关注有意义的行为边界和行为间关系，从而降低了预测性能。

Method: 本文提出了CTR-Sink框架，引入行为级注意力汇聚点：1) 在连续行为之间插入汇聚点标记（sink tokens），并融入推荐场景特有的信号（如时间距离）作为稳定的注意力汇聚点；2) 设计了两阶段训练策略，显式引导LM的注意力聚焦于汇聚点标记；3) 设计了注意力汇聚机制以增强汇聚点间的依赖关系，更好地捕捉行为间的关联性。

Result: 在工业数据集和两个开源数据集（MovieLens、Kuairec）上的实验，以及可视化结果，都验证了该方法在不同场景下的有效性。

Conclusion: CTR-Sink框架通过引入行为级注意力汇聚点，成功解决了基于语言模型的CTR预测中用户行为序列的语义碎片化问题，有效提升了预测性能，并在多种场景下展现出良好效果。

Abstract: Click-Through Rate (CTR) prediction, a core task in recommendation systems,
estimates user click likelihood using historical behavioral data. Modeling user
behavior sequences as text to leverage Language Models (LMs) for this task has
gained traction, owing to LMs' strong semantic understanding and contextual
modeling capabilities. However, a critical structural gap exists: user behavior
sequences consist of discrete actions connected by semantically empty
separators, differing fundamentally from the coherent natural language in LM
pre-training. This mismatch causes semantic fragmentation, where LM attention
scatters across irrelevant tokens instead of focusing on meaningful behavior
boundaries and inter-behavior relationships, degrading prediction performance.
To address this, we propose $\textit{CTR-Sink}$, a novel framework introducing
behavior-level attention sinks tailored for recommendation scenarios. Inspired
by attention sink theory, it constructs attention focus sinks and dynamically
regulates attention aggregation via external information. Specifically, we
insert sink tokens between consecutive behaviors, incorporating
recommendation-specific signals such as temporal distance to serve as stable
attention sinks. To enhance generality, we design a two-stage training strategy
that explicitly guides LM attention toward sink tokens and a attention sink
mechanism that amplifies inter-sink dependencies to better capture behavioral
correlations. Experiments on one industrial dataset and two open-source
datasets (MovieLens, Kuairec), alongside visualization results, validate the
method's effectiveness across scenarios.

</details>


### [230] [FairLangProc: A Python package for fairness in NLP](https://arxiv.org/abs/2508.03677)
*Arturo Pérez-Peralta,Sandra Benítez-Peña,Rosa E. Lillo*

Main category: cs.CL

TL;DR: 本文提出了FairLangProc，一个Python包，旨在为自然语言处理中的偏见缓解技术提供统一的实现，并与Hugging Face Transformers库兼容，以促进这些技术的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在决策场景（如组织公正或医疗保健）中的广泛应用引发了社会对模型公平性的担忧。尽管已提出多种数据集、度量和算法来衡量和缓解自然语言处理中的有害偏见，但它们的实现方式多样且缺乏集中性。

Method: 开发了一个名为FairLangProc的综合性Python包，它提供了自然语言处理中最新公平性进展的通用实现，并提供与Hugging Face Transformers库兼容的接口。

Result: FairLangProc包提供了一个统一的平台，整合了多种偏见缓解技术，并与流行的Hugging Face生态系统无缝集成，旨在降低使用门槛，促进偏见缓解技术的普及。

Conclusion: FairLangProc的推出有助于鼓励偏见缓解技术的广泛使用和民主化，从而解决大型语言模型在关键决策场景中的公平性问题。

Abstract: The rise in usage of Large Language Models to near ubiquitousness in recent
years has risen societal concern about their applications in decision-making
contexts, such as organizational justice or healthcare. This, in turn, poses
questions about the fairness of these models in critical settings, which leads
to the developement of different procedures to address bias in Natural Language
Processing. Although many datasets, metrics and algorithms have been proposed
to measure and mitigate harmful prejudice in Natural Language Processing, their
implementation is diverse and far from centralized. As a response, this paper
presents FairLangProc, a comprehensive Python package providing a common
implementation of some of the more recent advances in fairness in Natural
Language Processing providing an interface compatible with the famous Hugging
Face transformers library, aiming to encourage the widespread use and
democratization of bias mitigation techniques. The implementation can be found
on https://github.com/arturo-perez-peralta/FairLangProc.

</details>


### [231] [More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation](https://arxiv.org/abs/2508.03678)
*Yangtian Zi,Harshitha Menon,Arjun Guha*

Main category: cs.CL

TL;DR: 研究LLM在特定代码任务上表现不佳的原因，通过引入PartialOrderEval方法，系统性地探索提示细节对LLM代码生成能力的影响，发现提示特异性是关键因素。


<details>
  <summary>Details</summary>
Motivation: LLM在通用基准测试（如HumanEval）上表现优异，但在专业基准测试（如ParEval）上表现不佳。研究旨在探究这是否是由于LLM缺乏领域知识，还是因为提示细节不足。

Method: 引入PartialOrderEval方法，该方法能为任何代码生成基准测试增加从最小到最大详细程度的提示偏序。将此方法应用于HumanEval以及ParEval的串行和OpenMP子集，并使用Llama-3.x和Qwen2.5-Coder模型测量pass@1如何随提示特异性变化。

Result: 实验表明，不同任务对提示敏感度存在不同程度的差异。定性分析突出显示，明确的输入/输出规范、边缘情况处理和分步细化是提示细节改进的关键驱动因素。

Conclusion: LLM在代码生成任务上的表现，尤其是在专业领域，受到提示细节的显著影响。通过增加提示特异性，如明确I/O、处理边缘情况和提供逐步分解，可以有效提升模型性能，表明提示细节而非仅仅领域知识是关键因素。

Abstract: State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general
benchmarks like HumanEval but underperform on specialized suites such as
ParEval. Is this due to LLMs missing domain knowledge or insufficient prompt
detail is given? To answer this, we introduce PartialOrderEval, which augments
any code generation benchmark with a partial order of prompts from minimal to
maximally detailed. Applying it to HumanEval and both serial and OpenMP subsets
of ParEval, we measure how pass@1 scales with prompt specificity. Our
experiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of
prompt sensitivity across different tasks, and a qualitative analysis
highlights explicit I/O specifications, edge-case handling, and stepwise
breakdowns as the key drivers of prompt detail improvement.

</details>


### [232] [CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward](https://arxiv.org/abs/2508.03686)
*Shudong Liu,Hongwei Liu,Junnan Liu,Linchen Xiao,Songyang Gao,Chengqi Lyu,Yuzhe Gu,Wenwei Zhang,Derek F. Wong,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 本文提出CompassVerifier，一个轻量级、准确且鲁棒的答案验证模型，用于评估大型语言模型（LLM）并作为其优化奖励模型。同时发布VerifierBench基准测试，以系统评估验证能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM答案验证方法存在两点局限：1) 缺乏系统评估不同LLM验证能力的综合基准；2) 现有验证器开发尚处于早期，缺乏处理复杂边缘情况的鲁棒性和跨领域泛化能力，且依赖耗时的人工定制（如正则表达式或提示工程）。

Method: 开发了CompassVerifier，一个轻量级验证模型，旨在提高准确性和鲁棒性。同时构建了VerifierBench基准测试，其中包含从多个数据源收集的模型输出，并通过人工分析元错误模式进行增强，以提升CompassVerifier的性能。

Result: CompassVerifier在数学、知识和多样推理任务等多个领域展现出强大的能力，能够处理多子问题、公式和序列答案等多种答案类型，并有效识别异常/无效响应。VerifierBench则提供了一个系统评估验证能力的平台。

Conclusion: CompassVerifier和VerifierBench有望促进答案验证、评估协议和强化学习研究的发展。

Abstract: Answer verification is crucial not only for evaluating large language models
(LLMs) by matching their unstructured outputs against standard answers, but
also serves as the reward model to guide LLM optimization. Most evaluation
frameworks rely on regularized matching or employ general LLMs for answer
verification, which demands extensive, repetitive customization for regex rules
or evaluation prompts. Two fundamental limitations persist in current
methodologies: 1) the absence of comprehensive benchmarks that systematically
evaluate verification capabilities across different LLMs; and 2) the nascent
stage of verifier development, where existing approaches lack both the
robustness to handle complex edge cases and the generalizability across
different domains. In this work, we develop CompassVerifier, an accurate and
robust lightweight verifier model for evaluation and outcome reward. It
demonstrates multi-domain competency spanning math, knowledge, and diverse
reasoning tasks, with the capability to process various answer types, including
multi-subproblems, formulas, and sequence answers, while effectively
identifying abnormal/invalid responses. We introduce VerifierBench benchmark
comprising model outputs collected from multiple data sources, augmented
through manual analysis of metaerror patterns to enhance CompassVerifier. We
anticipate that CompassVerifier and VerifierBench will facilitate answer
verification, evaluation protocols, and reinforcement learning research. Code
and dataset are available at https://github.com/open-compass/CompassVerifier.

</details>


### [233] [Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025](https://arxiv.org/abs/2508.01263)
*Long S. T. Nguyen,Khang H. N. Vo,Thu H. A. Nguyen,Tuan C. Bui,Duc Q. Nguyen,Thanh-Tung Tran,Anh D. Nguyen,Minh L. Nguyen,Fabien Baldacci,Thang H. Bui,Emanuel Di Nardo,Angelo Ciaramella,Son H. Le,Ihsan Ullah,Lorenzo Di Rocco,Tho T. Quan*

Main category: cs.CL

TL;DR: 本文分析了XAI挑战赛2025，一个旨在教育领域推动可解释人工智能（XAI）的黑客马拉松，参赛者需构建基于轻量级LLM或混合系统的问答系统，为大学政策查询提供逻辑清晰的解释。


<details>
  <summary>Details</summary>
Motivation: 随着AI在教育中日益普及，对透明度和可解释性的需求增加。然而，很少有黑客马拉松直接在真实的教育场景中解决可解释AI（XAI）问题。

Method: 本文描述了XAI挑战赛2025，一个要求参赛者构建问答系统以回答学生关于大学政策的查询，并生成清晰、基于逻辑的自然语言解释的竞赛。系统需使用轻量级大型语言模型（LLMs）或混合LLM-符号系统。竞赛提供了一个通过逻辑模板构建、Z3验证并经专家学生审查的高质量数据集。

Result: 该挑战赛代表了一项新颖的努力，旨在将大型语言模型与符号推理相结合，以服务于可解释性。本文描述了挑战赛的动机、结构、数据集构建和评估协议。

Conclusion: 研究结果为未来以XAI为中心的教育系统和竞争性研究计划提供了可操作的见解。

Abstract: The growing integration of Artificial Intelligence (AI) into education has
intensified the need for transparency and interpretability. While hackathons
have long served as agile environments for rapid AI prototyping, few have
directly addressed eXplainable AI (XAI) in real-world educational contexts.
This paper presents a comprehensive analysis of the XAI Challenge 2025, a
hackathon-style competition jointly organized by Ho Chi Minh City University of
Technology (HCMUT) and the International Workshop on Trustworthiness and
Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International
Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked
participants with building Question-Answering (QA) systems capable of answering
student queries about university policies while generating clear, logic-based
natural language explanations. To promote transparency and trustworthiness,
solutions were required to use lightweight Large Language Models (LLMs) or
hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed
via logic-based templates with Z3 validation and refined through expert student
review to ensure alignment with real-world academic scenarios. We describe the
challenge's motivation, structure, dataset construction, and evaluation
protocol. Situating the competition within the broader evolution of AI
hackathons, we argue that it represents a novel effort to bridge LLMs and
symbolic reasoning in service of explainability. Our findings offer actionable
insights for future XAI-centered educational systems and competitive research
initiatives.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [234] [Learning User Interaction Forces using Vision for a Soft Finger Exosuit](https://arxiv.org/abs/2508.02870)
*Mohamed Irfan Refai,Abdulaziz Y. Alkayas,Anup Teejo Mathew,Federico Renda,Thomas George Thuruthel*

Main category: cs.RO

TL;DR: 本文提出了一种基于图像的机器学习框架，用于估计软体可穿戴设备（如手指外骨骼）与人体组织之间的分布式接触力，并证明其可作为闭环控制的替代力传感器。


<details>
  <summary>Details</summary>
Motivation: 可穿戴辅助设备日益软化，但其非线性顺应性使得物理建模和嵌入式传感都极具挑战性，难以捕捉动态辅助的力传输，因此需要一种新的方法来估计其与人体组织的界面力。

Method: 开发了一种基于图像的、学习型框架来估计手指外骨骼系统的分布式接触力。利用SoRoSim工具箱生成了多样化的外骨骼几何形状和驱动场景数据集用于训练。将该模型集成到反馈控制器中进行测试。

Result: 该方法能从低分辨率灰度图像中准确估计多个接触点的相互作用力，能够泛化到未见的形状和驱动水平，并在视觉噪声和对比度变化下保持鲁棒性。视觉估计器可作为闭环控制的替代力传感器。

Conclusion: 所提出的基于图像的学习方法可以作为外骨骼实时力估计的一种非侵入式替代方案，为软体可穿戴设备的控制提供了新的可能性。

Abstract: Wearable assistive devices are increasingly becoming softer. Modelling their
interface with human tissue is necessary to capture transmission of dynamic
assistance. However, their nonlinear and compliant nature makes both physical
modeling and embedded sensing challenging. In this paper, we develop a
image-based, learning-based framework to estimate distributed contact forces
for a finger-exosuit system. We used the SoRoSim toolbox to generate a diverse
dataset of exosuit geometries and actuation scenarios for training. The method
accurately estimated interaction forces across multiple contact locations from
low-resolution grayscale images, was able to generalize to unseen shapes and
actuation levels, and remained robust under visual noise and contrast
variations. We integrated the model into a feedback controller, and found that
the vision-based estimator functions as a surrogate force sensor for
closed-loop control. This approach could be used as a non-intrusive alternative
for real-time force estimation for exosuits.

</details>


### [235] [Tunable Leg Stiffness in a Monopedal Hopper for Energy-Efficient Vertical Hopping Across Varying Ground Profiles](https://arxiv.org/abs/2508.02873)
*Rongqian Chen,Jun Kwon,Kefan Wu,Wei-Hsi Chen*

Main category: cs.RO

TL;DR: HASTA是一个具有可调腿部刚度的垂直跳跃机器人，旨在通过实时调整腿部刚度，在不同地面条件下优化能量效率，实现最大跳跃高度。


<details>
  <summary>Details</summary>
Motivation: 在不同地面（具有不同刚度和阻尼）上，通过调整机器人腿部刚度来优化能量效率，以最大化跳跃高度。研究假设是，软腿在软阻尼地面表现更好，硬腿在硬低阻尼地面表现更好。

Method: 设计并实现了HASTA机器人，该机器人具有实时可调的腿部刚度。通过实验测试和仿真，寻找在不同地面条件下最佳的腿部刚度设置。

Result: 研究找到了在每种地面刚度和阻尼组合下，能使机器人在恒定能量输入下达到最大稳态跳跃高度的最佳腿部刚度。这些结果支持了可调刚度能提高能量效率的假设。此外，仿真为未来开发腿部刚度选择控制器提供了见解。

Conclusion: 可调刚度能够提高机器人在受控实验条件下的能量高效运动能力，并且仿真结果为未来控制器开发提供了指导。

Abstract: We present the design and implementation of HASTA (Hopper with Adjustable
Stiffness for Terrain Adaptation), a vertical hopping robot with real-time
tunable leg stiffness, aimed at optimizing energy efficiency across various
ground profiles (a pair of ground stiffness and damping conditions). By
adjusting leg stiffness, we aim to maximize apex hopping height, a key metric
for energy-efficient vertical hopping. We hypothesize that softer legs perform
better on soft, damped ground by minimizing penetration and energy loss, while
stiffer legs excel on hard, less damped ground by reducing limb deformation and
energy dissipation. Through experimental tests and simulations, we find the
best leg stiffness within our selection for each combination of ground
stiffness and damping, enabling the robot to achieve maximum steady-state
hopping height with a constant energy input. These results support our
hypothesis that tunable stiffness improves energy-efficient locomotion in
controlled experimental conditions. In addition, the simulation provides
insights that could aid in the future development of controllers for selecting
leg stiffness.

</details>


### [236] [Co-designing Zoomorphic Robot Concepts for Animal Welfare Education](https://arxiv.org/abs/2508.02898)
*Isobel Voysey,Lynne Baillie,Joanne Williams,Michael Herrmann*

Main category: cs.RO

TL;DR: 该研究通过与动物福利教育者和儿童的参与式设计工作坊，探讨了用于动物福利教育的拟动物机器人关键需求，并提出了设计挑战和新颖的参与式设计活动。


<details>
  <summary>Details</summary>
Motivation: 动物福利教育可通过定制机器人帮助儿童学习动物及其行为，从而促进积极、安全的儿童-动物互动。

Method: 与动物福利教育者和儿童进行了参与式设计工作坊，以从他们的角度识别拟动物机器人的关键要求。同时贡献了新颖的儿童参与式设计活动，包括受主题统觉测验和互动叙事启发的支线故事板。

Result: 研究结果涵盖了拟动物机器人的外观、行为和特征，以及围绕机器人的叙事概念。通过比较两组，发现以下重要性：儿童对不良行为的负面反应；利用面部特征和尾巴提供信号动物内部状态的线索；以及自然、毛茸茸的外观和质地。研究还反思了在各组间达成共识的一些关键设计挑战。

Conclusion: 为动物福利教育设计拟动物机器人时，应重视儿童对不良行为的反应、利用面部和尾部表达动物情绪、以及自然毛茸茸的外观。尽管设计概念有重叠，但在不同用户群体间达成共识仍是主要挑战。

Abstract: Animal welfare education could greatly benefit from customized robots to help
children learn about animals and their behavior, and thereby promote positive,
safe child-animal interactions. To this end, we ran Participatory Design
workshops with animal welfare educators and children to identify key
requirements for zoomorphic robots from their perspectives. Our findings
encompass a zoomorphic robot's appearance, behavior, and features, as well as
concepts for a narrative surrounding the robot. Through comparing and
contrasting the two groups, we find the importance of: negative reactions to
undesirable behavior from children; using the facial features and tail to
provide cues signaling an animal's internal state; and a natural, furry
appearance and texture. We also contribute some novel activities for
Participatory Design with children, including branching storyboards inspired by
thematic apperception tests and interactive narratives, and reflect on some of
the key design challenges of achieving consensus between the groups, despite
much overlap in their design concepts.

</details>


### [237] [Context-aware Risk Assessment and Its Application in Autonomous Driving](https://arxiv.org/abs/2508.02919)
*Boyang Tian,Weisong Shi*

Main category: cs.RO

TL;DR: 本文提出了一种名为CRI（Context-aware Risk Index）的轻量级模块化框架，用于自动驾驶中的实时风险评估和自适应行为调整，显著降低了碰撞率并提高了驾驶安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的风险评估方法存在局限性，包括输出粗略的全局场景指标、缺乏可解释性、指标未能具体整合到自动驾驶系统中，或仅关注特定驾驶场景。因此，需要一种能够提供精确、实时风险评估并实现自适应行为的解决方案，以确保自动驾驶的安全性。

Method: CRI框架通过以下方法量化定向风险并动态调整控制指令：1. 基于物体运动学和空间关系，在动态安全包络线内采用方向感知的空间划分（利用责任敏感安全RSS原则）。2. 采用混合概率-最大融合策略进行风险聚合。3. 采用自适应控制策略进行实时行为调制。该框架具有轻量级和模块化特点。

Result: 在包含220个安全关键场景的Bench2Drive基准测试中，使用先进的端到端模型Transfuser++在挑战性路线上评估了CRI。结果显示：每失败路线的车辆碰撞减少19%（p = 0.003），每公里碰撞减少20%（p = 0.004），综合驾驶得分提高17%（p = 0.016），罚分显著减少（p = 0.013）。此外，CRI的开销极低（每个决策周期3.6毫秒）。

Conclusion: CRI在复杂、高风险的环境中显著提高了自动驾驶的安全性与鲁棒性，同时保持了模块化和低运行时开销。这些结果证明了CRI在实际应用中的有效性。

Abstract: Ensuring safety in autonomous driving requires precise, real-time risk
assessment and adaptive behavior. Prior work on risk estimation either outputs
coarse, global scene-level metrics lacking interpretability, proposes
indicators without concrete integration into autonomous systems, or focuses
narrowly on specific driving scenarios. We introduce the Context-aware Risk
Index (CRI), a light-weight modular framework that quantifies directional risks
based on object kinematics and spatial relationships, dynamically adjusting
control commands in real time. CRI employs direction-aware spatial partitioning
within a dynamic safety envelope using Responsibility-Sensitive Safety (RSS)
principles, a hybrid probabilistic-max fusion strategy for risk aggregation,
and an adaptive control policy for real-time behavior modulation. We evaluate
CRI on the Bench2Drive benchmark comprising 220 safety-critical scenarios using
a state-of-the-art end-to-end model Transfuser++ on challenging routes. Our
collision-rate metrics show a 19\% reduction (p = 0.003) in vehicle collisions
per failed route, a 20\% reduction (p = 0.004) in collisions per kilometer, a
17\% increase (p = 0.016) in composed driving score, and a statistically
significant reduction in penalty scores (p = 0.013) with very low overhead (3.6
ms per decision cycle). These results demonstrate that CRI substantially
improves safety and robustness in complex, risk-intensive environments while
maintaining modularity and low runtime overhead.

</details>


### [238] [Model-agnostic Meta-learning for Adaptive Gait Phase and Terrain Geometry Estimation with Wearable Soft Sensors](https://arxiv.org/abs/2508.02930)
*Zenan Zhu,Wenxi Chen,Pei-Chun Kao,Janelle Clark,Lily Behnke,Rebecca Kramer-Bottiglio,Holly Yanco,Yan Gu*

Main category: cs.RO

TL;DR: 该论文提出了一个基于MAML（模型无关元学习）的框架，利用织物软传感器，实现了人体步态相位和地形几何的同步准确估计，对未见过个体和不同地形具有高效适应和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统刚性传感器舒适性差；织物软传感器虽舒适但存在滞后、放置误差和织物变形等非线性问题；此外，个体和地形间的巨大差异以及实际应用中校准数据有限，进一步增加了准确估计的难度。

Method: 提出的框架将MAML集成到深度学习架构中，以学习一个可泛化的模型初始化。该初始化能够捕捉与个体和地形无关的结构，从而使得模型能够仅通过少量校准数据和几次微调步骤即可高效适应新用户，并保持在不同个体和地形间的高估计精度。

Result: 实验结果表明，该框架在步态相位、运动模式和倾斜角度估计方面，性能优于基线方法，展现出卓越的准确性、适应效率和泛化能力。

Conclusion: 该MAML框架成功解决了织物软传感器引入的非线性问题、个体和地形差异以及校准数据有限的挑战，实现了对新用户的高效适应和在不同个体与地形间的强泛化能力，为可穿戴软传感器在步态和地形估计领域的应用提供了有效解决方案。

Abstract: This letter presents a model-agnostic meta-learning (MAML) based framework
for simultaneous and accurate estimation of human gait phase and terrain
geometry using a small set of fabric-based wearable soft sensors, with
efficient adaptation to unseen subjects and strong generalization across
different subjects and terrains. Compared to rigid alternatives such as
inertial measurement units, fabric-based soft sensors improve comfort but
introduce nonlinearities due to hysteresis, placement error, and fabric
deformation. Moreover, inter-subject and inter-terrain variability, coupled
with limited calibration data in real-world deployments, further complicate
accurate estimation. To address these challenges, the proposed framework
integrates MAML into a deep learning architecture to learn a generalizable
model initialization that captures subject- and terrain-invariant structure.
This initialization enables efficient adaptation (i.e., adaptation with only a
small amount of calibration data and a few fine-tuning steps) to new users,
while maintaining strong generalization (i.e., high estimation accuracy across
subjects and terrains). Experiments on nine participants walking at various
speeds over five terrain conditions demonstrate that the proposed framework
outperforms baseline approaches in estimating gait phase, locomotion mode, and
incline angle, with superior accuracy, adaptation efficiency, and
generalization.

</details>


### [239] [AeroSafe: Mobile Indoor Air Purification using Aerosol Residence Time Analysis and Robotic Cough Emulator Testbed](https://arxiv.org/abs/2508.02947)
*M Tanjid Hasan Tonmoy,Rahath Malladi,Kaustubh Singh,Forsad Al Hossain,Rajesh Gupta,Andrés E. Tejada-Martínez,Tauhidur Rahman*

Main category: cs.RO

TL;DR: 本文提出AeroSafe系统，通过机器人咳嗽模拟器和数字孪生技术分析气溶胶停留时间，以提高室内空气净化系统的效率，从而有效降低空气传播疾病风险。


<details>
  <summary>Details</summary>
Motivation: 现有便携式空气过滤器常忽略咳嗽产生的呼吸道气溶胶浓度，在高风险环境（如医疗机构和公共场所）构成潜在威胁。研究旨在解决这一问题，提升空气净化系统应对气溶胶传播疾病的有效性。

Method: 开发了AeroSafe系统，包含一个机器人双代理物理模拟器：一个模拟咳嗽事件的机械人体模型和一个自主响应气溶胶的便携式空气净化器。模拟器生成的数据用于训练数字孪生模型，该模型结合了基于物理的隔室模型与机器学习方法（长短期记忆网络和图卷积层）。

Result: 实验结果表明，该模型能够预测气溶胶浓度动态，平均停留时间预测误差在35秒以内。所提出的系统实时干预策略优于静态空气过滤器放置方法。

Conclusion: AeroSafe系统通过其创新的机器人模拟器和数字孪生分析，展示了在减轻空气传播病原体风险方面的巨大潜力，有效提升了室内空气净化系统的性能。

Abstract: Indoor air quality plays an essential role in the safety and well-being of
occupants, especially in the context of airborne diseases. This paper
introduces AeroSafe, a novel approach aimed at enhancing the efficacy of indoor
air purification systems through a robotic cough emulator testbed and a
digital-twins-based aerosol residence time analysis. Current portable air
filters often overlook the concentrations of respiratory aerosols generated by
coughs, posing a risk, particularly in high-exposure environments like
healthcare facilities and public spaces. To address this gap, we present a
robotic dual-agent physical emulator comprising a maneuverable mannequin
simulating cough events and a portable air purifier autonomously responding to
aerosols. The generated data from this emulator trains a digital twins model,
combining a physics-based compartment model with a machine learning approach,
using Long Short-Term Memory (LSTM) networks and graph convolution layers.
Experimental results demonstrate the model's ability to predict aerosol
concentration dynamics with a mean residence time prediction error within 35
seconds. The proposed system's real-time intervention strategies outperform
static air filter placement, showcasing its potential in mitigating airborne
pathogen risks.

</details>


### [240] [A novel autonomous microplastics surveying robot for beach environments](https://arxiv.org/abs/2508.02952)
*Hassan Iqbal,Kobiny Rex,Joseph Shirley,Carlos Baiz,Christian Claudel*

Main category: cs.RO

TL;DR: 本文介绍了一种新型机器人平台，能够自动检测并化学分析海滩表面的微塑料。


<details>
  <summary>Details</summary>
Motivation: 微塑料是普遍存在的环境污染物，在海滩堆积，但检测和绘制其浓度仍是主要挑战。

Method: 该系统是一个移动机械臂，使用末端执行器上的摄像头扫描区域并分割候选微塑料颗粒；然后利用近红外（NIR）光谱传感器，结合NIR和视觉反馈，实时对颗粒进行化学分析。

Result: 在实验室和海滩环境中进行的实验表明，该系统在操作控制方面实现了卓越的定位精度，并在微塑料分类方面达到了高准确度。

Conclusion: 该新型机器人平台能够有效地在海滩环境中自动检测和化学分析微塑料，解决了现有环境监测的挑战。

Abstract: Microplastics, defined as plastic particles smaller than 5 millimeters, have
become a pervasive environmental contaminant that accumulates on beaches due to
wind patterns and tidal forcing. Detecting microplastics and mapping their
concentration in the wild remains one of the primary challenges in addressing
this environmental issue. This paper introduces a novel robotic platform that
automatically detects and chemically analyzes microplastics on beach surfaces.
This mobile manipulator system scans areas for microplastics using a camera
mounted on the robotic arm's end effector. The system effectively segments
candidate microplastic particles on sand surfaces even in the presence of
organic matter such as leaves and clams. Once a candidate microplastic particle
is detected, the system steers a near-infrared (NIR) spectroscopic sensor onto
the particle using both NIR and visual feedback to chemically analyze it in
real-time. Through experiments in lab and beach environments, the system is
shown to achieve an excellent positional precision in manipulation control and
high microplastic classification accuracy.

</details>


### [241] [Optimal Trajectory Planning in a Vertically Undulating Snake Locomotion using Contact-implicit Optimization](https://arxiv.org/abs/2508.02953)
*Adarsh Salagame,Eric Sihite,Alireza Ramezani*

Main category: cs.RO

TL;DR: 本文提出了一种基于Moreau步进法的新型降阶模型，用于解决蛇形机器人运动中的复杂接触问题，并通过仿真和实验验证了其准确性。


<details>
  <summary>Details</summary>
Motivation: 现有蛇形机器人运动控制研究要么忽略接触复杂性，要么专注于非常复杂的交互（如挖洞），缺乏基于简单刚体动力学、能有效处理接触和控制分配问题的中间范式模型。

Method: 引入了一种基于微分包含数学中Moreau步进法的降阶模型。

Result: 通过仿真验证了模型的准确性，并通过实验进行了验证。

Conclusion: 该工作为蛇形机器人运动中的接触问题提供了一个有意义的贡献，提出了一个准确且经过实验验证的降阶模型。

Abstract: Contact-rich problems, such as snake robot locomotion, offer unexplored yet
rich opportunities for optimization-based trajectory and acyclic contact
planning. So far, a substantial body of control research has focused on
emulating snake locomotion and replicating its distinctive movement patterns
using shape functions that either ignore the complexity of interactions or
focus on complex interactions with matter (e.g., burrowing movements). However,
models and control frameworks that lie in between these two paradigms and are
based on simple, fundamental rigid body dynamics, which alleviate the
challenging contact and control allocation problems in snake locomotion, remain
absent. This work makes meaningful contributions, substantiated by simulations
and experiments, in the following directions: 1) introducing a reduced-order
model based on Moreau's stepping-forward approach from differential inclusion
mathematics, 2) verifying model accuracy, 3) experimental validation.

</details>


### [242] [Robot builds a robot's brain: AI generated drone command and control station hosted in the sky](https://arxiv.org/abs/2508.02962)
*Peter Burke*

Main category: cs.RO

TL;DR: 研究展示了一个完全由AI生成的无人机控制系统，AI自主编写了所有代码，并在真实和模拟无人机上成功部署和演示，显著加快了开发速度。


<details>
  <summary>Details</summary>
Motivation: 人工智能（AI）的进步，特别是大型语言模型（LLMs）和混合推理模型，为重新构想自主机器人（如无人机）的设计、开发和验证提供了新机遇。

Method: 研究采用AI模型，在最少的人类输入下，自主编写了实时、自托管的无人机指挥与控制平台的所有代码。该系统在真实无人机和云端模拟无人机上进行了部署和演示，实现了实时地图、飞行遥测、自主任务规划与执行以及安全协议，并通过无人机上托管的网页界面进行协调。

Result: 定量基准测试显示，AI生成的代码能够以比人类编码架构快几个数量级的开发周期交付功能完整的指挥与控制堆栈。尽管存在模型上下文窗口和推理深度相关的局限性，但研究揭示了AI驱动的机器人控制代码生成在当前模型规模下的实际边界、新兴优势和故障模式。

Conclusion: 这项工作为机器人控制系统的自主创建开创了先例，并更广泛地提出了一种新的机器人工程范式，即未来的机器人可能在很大程度上由人工智能共同设计、开发和验证。这是一项由机器人构建机器人大脑的初步工作。

Abstract: Advances in artificial intelligence (AI) including large language models
(LLMs) and hybrid reasoning models present an opportunity to reimagine how
autonomous robots such as drones are designed, developed, and validated. Here,
we demonstrate a fully AI-generated drone control system: with minimal human
input, an artificial intelligence (AI) model authored all the code for a
real-time, self-hosted drone command and control platform, which was deployed
and demonstrated on a real drone in flight as well as a simulated virtual drone
in the cloud. The system enables real-time mapping, flight telemetry,
autonomous mission planning and execution, and safety protocolsall orchestrated
through a web interface hosted directly on the drone itself. Not a single line
of code was written by a human. We quantitatively benchmark system performance,
code complexity, and development speed against prior, human-coded
architectures, finding that AI-generated code can deliver functionally complete
command-and-control stacks at orders-of-magnitude faster development cycles,
though with identifiable current limitations related to specific model context
window and reasoning depth. Our analysis uncovers the practical boundaries of
AI-driven robot control code generation at current model scales, as well as
emergent strengths and failure modes in AI-generated robotics code. This work
sets a precedent for the autonomous creation of robot control systems and, more
broadly, suggests a new paradigm for robotics engineeringone in which future
robots may be largely co-designed, developed, and verified by artificial
intelligence. In this initial work, a robot built a robot's brain.

</details>


### [243] [Physics-informed Neural Time Fields for Prehensile Object Manipulation](https://arxiv.org/abs/2508.02976)
*Hanwen Ren,Ruiqi Ni,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的多模态物理信息神经网络（PINN），用于解决机器人物体操作任务，无需专家数据，能高效学习和快速规划，并在复杂环境中实现高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的机器人物体操作方法存在效率低下（基于采样）、需要专家演示或通过试错学习等问题，不适用于实际场景中的复杂、杂乱环境。

Method: 提出了一种新颖的多模态物理信息神经网络（PINN），用于求解Eikonal方程。该方法无需专家数据即可学习，能快速找到物体操作轨迹，并在操作过程中响应式地重新规划机器人的抓取姿态，以达到期望的物体位姿。

Result: 在模拟和真实世界场景中进行了验证，并与现有SOTA方法进行比较。结果表明，该方法对各种物体均有效，与之前的学习方法相比训练更高效，并在规划时间、轨迹长度和成功率方面表现出卓越性能。

Conclusion: 所提出的多模态物理信息神经网络（PINN）是一种有效且高效的机器人物体操作方法，能够在复杂、杂乱的环境中实现高性能，且无需专家数据。

Abstract: Object manipulation skills are necessary for robots operating in various
daily-life scenarios, ranging from warehouses to hospitals. They allow the
robots to manipulate the given object to their desired arrangement in the
cluttered environment. The existing approaches to solving object manipulations
are either inefficient sampling based techniques, require expert
demonstrations, or learn by trial and error, making them less ideal for
practical scenarios. In this paper, we propose a novel, multimodal
physics-informed neural network (PINN) for solving object manipulation tasks.
Our approach efficiently learns to solve the Eikonal equation without expert
data and finds object manipulation trajectories fast in complex, cluttered
environments. Our method is multimodal as it also reactively replans the
robot's grasps during manipulation to achieve the desired object poses. We
demonstrate our approach in both simulation and real-world scenarios and
compare it against state-of-the-art baseline methods. The results indicate that
our approach is effective across various objects, has efficient training
compared to previous learning-based methods, and demonstrates high performance
in planning time, trajectory length, and success rates. Our demonstration
videos can be found at https://youtu.be/FaQLkTV9knI.

</details>


### [244] [Multimodal Human-Intent Modeling for Contextual Robot-to-Human Handovers of Arbitrary Objects](https://arxiv.org/abs/2508.02982)
*Lucas Chen,Guna Avula,Hanwen Ren,Zixing Wang,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 本文提出了一种统一的人机协作物体递交方法，该方法能根据人类的言语和非言语指令选择目标物体，并结合人类的隐性和显性偏好生成机器人抓取和顺从的递交动作序列，以实现更自然流畅的交互。


<details>
  <summary>Details</summary>
Motivation: 现有的人机物体递交方法依赖于预先选择的目标物体，且未能充分考虑人类在递交过程中的隐性和显性偏好（如目标物体选择和机器人抓取方式），这限制了人机交互的自然性和流畅性。这些偏好对于从杂乱环境中选择目标物体以及机器人如何抓取物体以方便人类后续抓取至关重要。

Method: 本文提出了一种统一的方法。该方法通过人类的言语和非言语指令选择远距离的目标物体，并通过情境化人类的隐性和显性偏好来生成机器人的抓取方式和顺从的递交运动序列。该集成框架及其组件通过真实世界实验和用户研究进行了评估。

Result: 评估结果表明，所提出的集成框架及其组件在处理物体递交任务方面是有效的，并且能够理解人类的偏好。

Conclusion: 该研究提出的管道能够通过理解人类偏好来有效处理人机物体递交任务，从而促进更自然和流畅的人机交互。

Abstract: Human-robot object handover is a crucial element for assistive robots that
aim to help people in their daily lives, including elderly care, hospitals, and
factory floors. The existing approaches to solving these tasks rely on
pre-selected target objects and do not contextualize human implicit and
explicit preferences for handover, limiting natural and smooth interaction
between humans and robots. These preferences can be related to the target
object selection from the cluttered environment and to the way the robot should
grasp the selected object to facilitate desirable human grasping during
handovers. Therefore, this paper presents a unified approach that selects
target distant objects using human verbal and non-verbal commands and performs
the handover operation by contextualizing human implicit and explicit
preferences to generate robot grasps and compliant handover motion sequences.
We evaluate our integrated framework and its components through real-world
experiments and user studies with arbitrary daily-life objects. The results of
these evaluations demonstrate the effectiveness of our proposed pipeline in
handling object handover tasks by understanding human preferences. Our
demonstration videos can be found at https://youtu.be/6z27B2INl-s.

</details>


### [245] [Estimation of Aerodynamics Forces in Dynamic Morphing Wing Flight](https://arxiv.org/abs/2508.02984)
*Bibek Gupta,Mintae Kim,Albert Park,Eric Sihite,Koushil Sreenath,Alireza Ramezani*

Main category: cs.RO

TL;DR: 本文研究了两种用于扑翼飞行机器人气动力的估算方法：基于物理的共轭动量观测器和基于神经网络的回归模型，并在系留飞行中进行了评估。


<details>
  <summary>Details</summary>
Motivation: 准确估计气动力对于具有动态变形能力的扑翼飞行机器人的控制、建模和设计至关重要，特别是为了实现闭环飞行控制。

Method: 1. 基于物理的观测器：利用哈密顿力学和共轭动量概念，从系统的降阶动力学模型和实时传感器数据推断外部气动力，无需训练数据。2. 基于神经网络的回归模型：采用多层感知器（MLP），学习关节运动学、扑翼频率和环境参数到气动力输出的映射。两种方法均通过六轴力传感器在高频数据采集设置下进行评估。

Result: 共轭动量观测器和回归模型在三个力分量（Fx、Fy、Fz）上表现出高度一致性。

Conclusion: 所提出的两种气动力估算方法（基于物理的观测器和神经网络模型）均能有效且准确地估算扑翼飞行机器人的气动力。

Abstract: Accurate estimation of aerodynamic forces is essential for advancing the
control, modeling, and design of flapping-wing aerial robots with dynamic
morphing capabilities. In this paper, we investigate two distinct methodologies
for force estimation on Aerobat, a bio-inspired flapping-wing platform designed
to emulate the inertial and aerodynamic behaviors observed in bat flight. Our
goal is to quantify aerodynamic force contributions during tethered flight, a
crucial step toward closed-loop flight control. The first method is a
physics-based observer derived from Hamiltonian mechanics that leverages the
concept of conjugate momentum to infer external aerodynamic forces acting on
the robot. This observer builds on the system's reduced-order dynamic model and
utilizes real-time sensor data to estimate forces without requiring training
data. The second method employs a neural network-based regression model,
specifically a multi-layer perceptron (MLP), to learn a mapping from joint
kinematics, flapping frequency, and environmental parameters to aerodynamic
force outputs. We evaluate both estimators using a 6-axis load cell in a
high-frequency data acquisition setup that enables fine-grained force
measurements during periodic wingbeats. The conjugate momentum observer and the
regression model demonstrate strong agreement across three force components
(Fx, Fy, Fz).

</details>


### [246] [GACL: Grounded Adaptive Curriculum Learning with Active Task and Performance Monitoring](https://arxiv.org/abs/2508.02988)
*Linji Wang,Zifan Xu,Peter Stone,Xuesu Xiao*

Main category: cs.RO

TL;DR: 本文提出了一种名为“接地自适应课程学习”（GACL）的新框架，用于自动化机器人任务的课程学习，旨在解决手动设计课程的局限性和现有自动化方法在复杂机器人领域面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的课程学习方法在机器人任务中主要依赖手动设计，这耗费大量工程精力且容易导致次优结果。自动化课程学习在简单领域表现良好，但在机器人任务中面临独特挑战，如处理复杂的任务空间和在目标领域分布部分未知的情况下保持相关性。

Method: 本文提出了GACL框架，包含三项关键创新：1) 一种能够一致处理复杂机器人任务设计的任务表示方法；2) 一种主动性能跟踪机制，可以根据机器人当前能力自适应地生成课程；3) 一种接地方法，通过在参考任务和合成任务之间交替采样来保持与目标领域的相关性。

Result: GACL在受限环境下的轮式导航和挑战性3D受限空间中的四足机器人运动任务上进行了验证，分别比现有最先进方法取得了6.8%和6.1%的成功率提升。

Conclusion: GACL框架成功解决了机器人课程学习中的关键挑战，通过创新的任务表示、自适应课程生成和领域接地方法，显著提高了复杂机器人任务的学习成功率，优于现有最先进方法。

Abstract: Curriculum learning has emerged as a promising approach for training complex
robotics tasks, yet current applications predominantly rely on manually
designed curricula, which demand significant engineering effort and can suffer
from subjective and suboptimal human design choices. While automated curriculum
learning has shown success in simple domains like grid worlds and games where
task distributions can be easily specified, robotics tasks present unique
challenges: they require handling complex task spaces while maintaining
relevance to target domain distributions that are only partially known through
limited samples. To this end, we propose Grounded Adaptive Curriculum Learning,
a framework specifically designed for robotics curriculum learning with three
key innovations: (1) a task representation that consistently handles complex
robot task design, (2) an active performance tracking mechanism that allows
adaptive curriculum generation appropriate for the robot's current
capabilities, and (3) a grounding approach that maintains target domain
relevance through alternating sampling between reference and synthetic tasks.
We validate GACL on wheeled navigation in constrained environments and
quadruped locomotion in challenging 3D confined spaces, achieving 6.8% and 6.1%
higher success rates, respectively, than state-of-the-art methods in each
domain.

</details>


### [247] [Thruster-Enhanced Locomotion: A Decoupled Model Predictive Control with Learned Contact Residuals](https://arxiv.org/abs/2508.03003)
*Chenghao Wang,Alireza Ramezani*

Main category: cs.RO

TL;DR: 针对Husky Carbon机器人低扭矩控制带宽问题，提出一种解耦控制架构：腿部采用Raibert型控制器，推力器使用增强了学习型接触残余动力学（CRD）的MPC，实现了更稳定的步态。


<details>
  <summary>Details</summary>
Motivation: 传统统一的模型预测控制（MPC）框架，虽然理论上能解决机器人姿态操纵与推力矢量控制的统一问题，但受限于系统轻量级执行器低扭矩控制带宽，实际应用可行性受限。

Method: 提出一种解耦控制架构：腿部运动采用基于位置的Raibert型控制器；推力器通过MPC进行调节，并辅以学习到的接触残余动力学（CRD）来补偿腿地冲击。这种分离绕过了扭矩控制速率瓶颈，同时通过学习到的残余量保留了推力器MPC以明确考虑腿地冲击动力学。

Result: 通过仿真和硬件实验验证了该方法。结果表明，与不带CRD的解耦控制器相比，带CRD的解耦控制架构在推力恢复和猫步式行走步态方面表现出更稳定的行为。

Conclusion: 解耦控制架构结合学习型接触残余动力学（CRD）能有效克服轻量级执行器低扭矩控制带宽的挑战，显著提高了机器人步态的稳定性和鲁棒性。

Abstract: Husky Carbon, a robot developed by Northeastern University, serves as a
research platform to explore unification of posture manipulation and thrust
vectoring. Unlike conventional quadrupeds, its joint actuators and thrusters
enable enhanced control authority, facilitating thruster-assisted narrow-path
walking. While a unified Model Predictive Control (MPC) framework optimizing
both ground reaction forces and thruster forces could theoretically address
this control problem, its feasibility is limited by the low torque-control
bandwidth of the system's lightweight actuators. To overcome this challenge, we
propose a decoupled control architecture: a Raibert-type controller governs
legged locomotion using position-based control, while an MPC regulates the
thrusters augmented by learned Contact Residual Dynamics (CRD) to account for
leg-ground impacts. This separation bypasses the torque-control rate bottleneck
while retaining the thruster MPC to explicitly account for leg-ground impact
dynamics through learned residuals. We validate this approach through both
simulation and hardware experiments, showing that the decoupled control
architecture with CRD performs more stable behavior in terms of push recovery
and cat-like walking gait compared to the decoupled controller without CRD.

</details>


### [248] [Aerobatic maneuvers in insect-scale flapping-wing aerial robots via deep-learned robust tube model predictive control](https://arxiv.org/abs/2508.03043)
*Yi-Hsuan Hsiao,Andrea Tagliabue,Owen Matteson,Suhan Kim,Tong Zhao,Jonathan P. How,YuFeng Chen*

Main category: cs.RO

TL;DR: 该研究通过设计深度学习的鲁棒管模型预测控制器和模仿学习，显著提升了750毫克扑翼机器人的飞行敏捷性和鲁棒性，使其能够执行昆虫般的机动动作。


<details>
  <summary>Details</summary>
Motivation: 昆虫具有高度敏捷的飞行能力，如急刹车、眼跳和翻身，而现有昆虫大小的飞行机器人受限于低惯性、快速动力学、气动不确定性和环境干扰，只能跟踪非激进轨迹。这种性能差距促使研究人员寻求生成激进飞行轨迹和高反馈率控制器的方法，以应对硬件限制和模型/环境不确定性。

Method: 研究设计了一种深度学习的鲁棒管模型预测控制器（MPC），用于跟踪激进飞行轨迹并应对干扰。为在计算受限的实时系统中实现高反馈率，研究采用了模仿学习方法，训练了一个双层全连接神经网络，其架构类似于昆虫的中央神经系统和运动神经元。

Result: 该机器人展示了昆虫般的眼跳运动，横向速度达到197厘米/秒（比之前结果提升447%），加速度达到11.7米/秒²（比之前结果提升255%）。机器人还能在160厘米/秒的风扰和较大的指令-力映射误差下执行眼跳机动。此外，它在11秒内连续完成了10次机身翻转，这是亚克级飞行器中最具挑战性的机动。

Conclusion: 这些成果代表了在实现昆虫尺度飞行敏捷性方面的一个里程碑，并为未来在传感和计算自主性方面的研究提供了启发。

Abstract: Aerial insects exhibit highly agile maneuvers such as sharp braking,
saccades, and body flips under disturbance. In contrast, insect-scale aerial
robots are limited to tracking non-aggressive trajectories with small body
acceleration. This performance gap is contributed by a combination of low robot
inertia, fast dynamics, uncertainty in flapping-wing aerodynamics, and high
susceptibility to environmental disturbance. Executing highly dynamic maneuvers
requires the generation of aggressive flight trajectories that push against the
hardware limit and a high-rate feedback controller that accounts for model and
environmental uncertainty. Here, through designing a deep-learned robust tube
model predictive controller, we showcase insect-like flight agility and
robustness in a 750-millgram flapping-wing robot. Our model predictive
controller can track aggressive flight trajectories under disturbance. To
achieve a high feedback rate in a compute-constrained real-time system, we
design imitation learning methods to train a two-layer, fully connected neural
network, which resembles insect flight control architecture consisting of
central nervous system and motor neurons. Our robot demonstrates insect-like
saccade movements with lateral speed and acceleration of 197 centimeters per
second and 11.7 meters per second square, representing 447$\%$ and 255$\%$
improvement over prior results. The robot can also perform saccade maneuvers
under 160 centimeters per second wind disturbance and large command-to-force
mapping errors. Furthermore, it performs 10 consecutive body flips in 11
seconds - the most challenging maneuver among sub-gram flyers. These results
represent a milestone in achieving insect-scale flight agility and inspire
future investigations on sensing and compute autonomy.

</details>


### [249] [LiGen: GAN-Augmented Spectral Fingerprinting for Indoor Positioning](https://arxiv.org/abs/2508.03024)
*Jie Lin,Hsun-Yu Lee,Ho-Ming Li,Fang-Jing Wu*

Main category: cs.RO

TL;DR: LiGen是一种新型室内定位系统，利用环境光的频谱强度模式作为指纹，并结合生成对抗网络（GANs）进行数据增强，实现了高精度和鲁棒性，优于传统Wi-Fi系统。


<details>
  <summary>Details</summary>
Motivation: 现有的Wi-Fi室内定位系统容易受到环境条件影响，不够稳定。研究需要一种更稳定、无需基础设施的替代方案。

Method: LiGen系统利用环境光的频谱强度模式作为指纹。为解决频谱数据有限的问题，设计了基于GANs的数据增强框架，包括PointGAN（根据坐标生成指纹）和FreeGAN（使用弱定位模型标记无条件样本）。定位模型采用多层感知机（MLP）在合成数据上训练。

Result: LiGen系统实现了亚米级定位精度，性能优于基于Wi-Fi的基线系统50%以上。该系统在杂乱环境中也表现出强大的鲁棒性。这是首个将频谱指纹与基于GAN的数据增强结合用于室内定位的系统。

Conclusion: LiGen系统通过利用环境光频谱指纹和GANs数据增强，提供了一种准确、鲁棒且稳定的室内定位解决方案，克服了传统Wi-Fi系统的局限性。

Abstract: Accurate and robust indoor localization is critical for smart building
applications, yet existing Wi-Fi-based systems are often vulnerable to
environmental conditions. This work presents a novel indoor localization
system, called LiGen, that leverages the spectral intensity patterns of ambient
light as fingerprints, offering a more stable and infrastructure-free
alternative to radio signals. To address the limited spectral data, we design a
data augmentation framework based on generative adversarial networks (GANs),
featuring two variants: PointGAN, which generates fingerprints conditioned on
coordinates, and FreeGAN, which uses a weak localization model to label
unconditioned samples. Our positioning model, leveraging a Multi-Layer
Perceptron (MLP) architecture to train on synthesized data, achieves
submeter-level accuracy, outperforming Wi-Fi-based baselines by over 50\%.
LiGen also demonstrates strong robustness in cluttered environments. To the
best of our knowledge, this is the first system to combine spectral
fingerprints with GAN-based data augmentation for indoor localization.

</details>


### [250] [UniFucGrasp: Human-Hand-Inspired Unified Functional Grasp Annotation Strategy and Dataset for Diverse Dexterous Hands](https://arxiv.org/abs/2508.03339)
*Haoran Lin,Wenrui Chen,Xianchi Chen,Fan Yang,Qiang Diao,Wenxin Xie,Sijie Wu,Kailun Yang,Maojun Li,Yaonan Wang*

Main category: cs.RO

TL;DR: 灵巧抓取数据集缺乏功能性抓取且成本高昂。本文提出UniFucGrasp，一个通用的多手型功能性抓取标注策略和数据集，提高了抓取精度、稳定性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的灵巧抓取数据集主要侧重抓取稳定性，忽略了任务所需的功能性抓取（如开瓶盖、握杯柄）。此外，大多数数据集依赖笨重、昂贵且难以控制的高自由度Shadow Hands。

Method: 受到欠驱动人手的启发，建立了UniFucGrasp，一个通用的功能性抓取标注策略和数据集。该方法基于仿生学，将自然人手运动映射到不同机械手结构，并利用基于几何的力闭合来确保功能性、稳定且类人抓取。支持低成本、高效的数据收集，并建立了首个多手型功能性抓取数据集，提供合成模型验证其有效性。

Result: 在UFG数据集、IsaacSim和复杂机器人任务上的实验表明，该方法提高了功能性操作精度和抓取稳定性，实现了在不同机器人手型上的高效泛化，并克服了灵巧抓取中的标注成本和泛化挑战。

Conclusion: UniFucGrasp策略和数据集通过提供多样化、高质量、功能性且可在多种机器人手上泛化的抓取，有效解决了现有灵巧抓取数据集的局限性，显著推动了具身智能的发展。

Abstract: Dexterous grasp datasets are vital for embodied intelligence, but mostly
emphasize grasp stability, ignoring functional grasps needed for tasks like
opening bottle caps or holding cup handles. Most rely on bulky, costly, and
hard-to-control high-DOF Shadow Hands. Inspired by the human hand's
underactuated mechanism, we establish UniFucGrasp, a universal functional grasp
annotation strategy and dataset for multiple dexterous hand types. Based on
biomimicry, it maps natural human motions to diverse hand structures and uses
geometry-based force closure to ensure functional, stable, human-like grasps.
This method supports low-cost, efficient collection of diverse, high-quality
functional grasps. Finally, we establish the first multi-hand functional grasp
dataset and provide a synthesis model to validate its effectiveness.
Experiments on the UFG dataset, IsaacSim, and complex robotic tasks show that
our method improves functional manipulation accuracy and grasp stability,
enables efficient generalization across diverse robotic hands, and overcomes
annotation cost and generalization challenges in dexterous grasping. The
project page is at https://haochen611.github.io/UFG.

</details>


### [251] [CogniPlan: Uncertainty-Guided Path Planning with Conditional Generative Layout Prediction](https://arxiv.org/abs/2508.03027)
*Yizhuo Wang,Haodong He,Jingsong Liang,Yuhong Cao,Ritabrata Chakraborty,Guillaume Sartoretti*

Main category: cs.RO

TL;DR: CogniPlan是一种新颖的路径规划框架，它利用条件生成修复模型预测的多个可能环境布局，从而在未知环境中实现高效的探索和导航。


<details>
  <summary>Details</summary>
Motivation: 移动机器人在未知环境中的路径规划（包括自主探索和目标导航）是一项关键但固有的挑战，需要机器人实时感知环境、更新信念并估计潜在信息增益以指导规划。

Method: 该研究提出了CogniPlan框架，它借鉴人类认知地图的导航方式，利用一个条件生成修复（COGNI）模型根据部分观测地图和布局条件向量来预测多个可信的潜在环境布局。这种方法将生成式图像布局预测与基于图注意力机制的路径规划相结合，使规划器能够在不确定性下有效推理。

Result: CogniPlan在探索和导航任务中均取得了显著的性能提升，结合了图表示的可扩展性与占据地图的保真度和预测性。在两个大型数据集（数百张地图和真实平面图）上，CogniPlan持续优于现有最先进的规划器，并在高保真模拟器和真实硬件上展示了其高质量的路径规划能力和实际应用价值。

Conclusion: CogniPlan通过利用生成模型预测环境布局，有效解决了未知环境中的路径规划挑战，展现出卓越的性能和实际应用潜力。

Abstract: Path planning in unknown environments is a crucial yet inherently challenging
capability for mobile robots, which primarily encompasses two coupled tasks:
autonomous exploration and point-goal navigation. In both cases, the robot must
perceive the environment, update its belief, and accurately estimate potential
information gain on-the-fly to guide planning. In this work, we propose
CogniPlan, a novel path planning framework that leverages multiple plausible
layouts predicted by a COnditional GeNerative Inpainting model, mirroring how
humans rely on cognitive maps during navigation. These predictions, based on
the partially observed map and a set of layout conditioning vectors, enable our
planner to reason effectively under uncertainty. We demonstrate strong synergy
between generative image-based layout prediction and graph-attention-based path
planning, allowing CogniPlan to combine the scalability of graph
representations with the fidelity and predictiveness of occupancy maps,
yielding notable performance gains in both exploration and navigation. We
extensively evaluate CogniPlan on two datasets (hundreds of maps and realistic
floor plans), consistently outperforming state-of-the-art planners. We further
deploy it in a high-fidelity simulator and on hardware, showcasing its
high-quality path planning and real-world applicability.

</details>


### [252] [Residual Neural Terminal Constraint for MPC-based Collision Avoidance in Dynamic Environments](https://arxiv.org/abs/2508.03428)
*Bojan Derajić,Mohamed-Khalil Bouzidi,Sebastian Bernhard,Wolfgang Hönig*

Main category: cs.RO

TL;DR: 本文提出了一种混合MPC局部规划器，通过学习近似时变安全集作为MPC终端约束，该安全集基于Hamilton-Jacobi可达性分析，并利用神经网络对残差项进行建模以实现实时估计，显著提升了规划成功率。


<details>
  <summary>Details</summary>
Motivation: Hamilton-Jacobi (HJ) 可达性分析计算出的价值函数可以表示为安全集，但其实时计算不可行，因此需要一种实时可行的学习方法来近似该安全集，作为模型预测控制（MPC）的终端约束，以确保规划器的安全性。

Method: 提出混合MPC局部规划器，将学习近似的时变安全集作为MPC终端约束。该安全集是HJ价值函数的零超水平集。利用HJ价值函数可表示为带符号距离函数（SDF）与非负残差函数之差的特性，将残差建模为具有非负输出的神经网络，并从SDF中减去以获得实时的价值函数估计。为提高实时性能和泛化能力，通过超网络参数化神经网络残差。

Result: 与三种最先进的方法进行仿真和硬件实验比较，结果显示所提方法成功率比最佳基线高出30%，同时计算开销相似，并能产生高质量（低旅行时间）的解决方案。

Conclusion: 所提出的混合MPC局部规划器，通过学习近似HJ可达性分析得到的安全集并将其作为终端约束，在实时性能和泛化能力方面表现优异，显著提升了规划成功率，并能生成高质量的路径，且计算成本可控。

Abstract: In this paper, we propose a hybrid MPC local planner that uses a
learning-based approximation of a time-varying safe set, derived from local
observations and applied as the MPC terminal constraint. This set can be
represented as a zero-superlevel set of the value function computed via
Hamilton-Jacobi (HJ) reachability analysis, which is infeasible in real-time.
We exploit the property that the HJ value function can be expressed as a
difference of the corresponding signed distance function (SDF) and a
non-negative residual function. The residual component is modeled as a neural
network with non-negative output and subtracted from the computed SDF,
resulting in a real-time value function estimate that is at least as safe as
the SDF by design. Additionally, we parametrize the neural residual by a
hypernetwork to improve real-time performance and generalization properties.
The proposed method is compared with three state-of-the-art methods in
simulations and hardware experiments, achieving up to 30\% higher success rates
compared to the best baseline while requiring a similar computational effort
and producing high-quality (low travel-time) solutions.

</details>


### [253] [SkeNa: Learning to Navigate Unseen Environments Based on Abstract Hand-Drawn Maps](https://arxiv.org/abs/2508.03053)
*Haojun Xu,Jiaqi Xiang,Wu Wei,Jinyu Chen,Linqing Zhong,Linjiang Huang,Hongyu Yang,Si Liu*

Main category: cs.RO

TL;DR: 该研究引入了基于手绘草图的视觉导航（SkeNa）任务，并发布了大规模数据集SoR，同时提出了SkeNavigator框架，该框架通过对齐视觉观测与草图来导航，并显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 受人类通过手绘草图指导导航的启发，现有具身导航任务缺乏使用手绘、可能抽象的草图作为指导，促使研究者探索一种更自然、符合人类习惯的导航方式。

Method: 引入了基于手绘草图的视觉导航（SkeNa）任务；构建了包含5.4万对轨迹和草图的大规模数据集SoR，包含两种抽象级别的验证集；开发了自动化草图生成流程；提出了SkeNavigator导航框架，包含射线地图描述器（RMD）用于增强草图特征表示，以及双地图对齐目标预测器（DAGP）用于对齐草图与探索地图特征以预测目标位置。

Result: SkeNavigator框架显著优于现有平面图导航方法，在高抽象度验证集上将SPL相对提高了105%。

Conclusion: 该研究成功定义并支持了基于手绘草图的具身导航任务SkeNa，提供了大规模数据集SoR，并提出了高效的SkeNavigator解决方案，证明了其在利用手绘草图进行导航方面的卓越性能。

Abstract: A typical human strategy for giving navigation guidance is to sketch route
maps based on the environmental layout. Inspired by this, we introduce Sketch
map-based visual Navigation (SkeNa), an embodied navigation task in which an
agent must reach a goal in an unseen environment using only a hand-drawn sketch
map as guidance. To support research for SkeNa, we present a large-scale
dataset named SoR, comprising 54k trajectory and sketch map pairs across 71
indoor scenes. In SoR, we introduce two navigation validation sets with varying
levels of abstraction in hand-drawn sketches, categorized based on their
preservation of spatial scales in the environment, to facilitate future
research. To construct SoR, we develop an automated sketch-generation pipeline
that efficiently converts floor plans into hand-drawn representations. To solve
SkeNa, we propose SkeNavigator, a navigation framework that aligns visual
observations with hand-drawn maps to estimate navigation targets. It employs a
Ray-based Map Descriptor (RMD) to enhance sketch map valid feature
representation using equidistant sampling points and boundary distances. To
improve alignment with visual observations, a Dual-Map Aligned Goal Predictor
(DAGP) leverages the correspondence between sketch map features and on-site
constructed exploration map features to predict goal position and guide
navigation. SkeNavigator outperforms prior floor plan navigation methods by a
large margin, improving SPL on the high-abstract validation set by 105%
relatively. Our code and dataset will be released.

</details>


### [254] [Hand-Eye Autonomous Delivery: Learning Humanoid Navigation, Locomotion and Reaching](https://arxiv.org/abs/2508.03068)
*Sirui Chen,Yufei Ye,Zi-Ang Cao,Jennifer Lew,Pei Xu,C. Karen Liu*

Main category: cs.RO

TL;DR: HEAD是一个框架，使类人机器人能直接从人类运动和视觉感知数据中学习导航、运动和抓取技能，采用高低层解耦的模块化方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是让类人机器人能从人类数据中学习复杂的导航、运动和抓取技能，以在为人类设计的复杂环境中有效操作。

Method: HEAD框架采用模块化方法：高层规划器指令手和眼睛的目标位置与方向，低层策略控制全身运动以实现这些目标。低层控制器从大规模人体动作捕捉数据中学习跟踪眼睛和双手三个点，高层策略则从Aria眼镜收集的人类数据中学习。这种方法将自我中心视觉感知与物理动作解耦。

Result: 该方法在模拟和现实世界中均进行了评估，展示了类人机器人在复杂环境中进行导航和抓取的能力。

Conclusion: HEAD框架通过模块化方法，实现了从人类数据中高效学习类人机器人的导航、运动和抓取技能，并具有良好的可扩展性，能适应新场景。

Abstract: We propose Hand-Eye Autonomous Delivery (HEAD), a framework that learns
navigation, locomotion, and reaching skills for humanoids, directly from human
motion and vision perception data. We take a modular approach where the
high-level planner commands the target position and orientation of the hands
and eyes of the humanoid, delivered by the low-level policy that controls the
whole-body movements. Specifically, the low-level whole-body controller learns
to track the three points (eyes, left hand, and right hand) from existing
large-scale human motion capture data while high-level policy learns from human
data collected by Aria glasses. Our modular approach decouples the ego-centric
vision perception from physical actions, promoting efficient learning and
scalability to novel scenes. We evaluate our method both in simulation and in
the real-world, demonstrating humanoid's capabilities to navigate and reach in
complex environments designed for humans.

</details>


### [255] [Optimizing Bipedal Locomotion for The 100m Dash With Comparison to Human Running](https://arxiv.org/abs/2508.03070)
*Devin Crowley,Jeremy Dao,Helei Duan,Kevin Green,Jonathan Hurst,Alan Fern*

Main category: cs.RO

TL;DR: 本文优化了双足机器人Cassie的跑步步态以实现极高速度，并将其与人类跑步力学进行比较，最终集成了控制器，使Cassie打破了双足机器人100米短跑的世界纪录。


<details>
  <summary>Details</summary>
Motivation: 研究旨在使双足机器人Cassie实现极高速奔跑，并探究优化后的机器人步态与已知高效的人类跑步力学之间的相似性。

Method: 1. 优化Cassie的步态效率以适应不同速度，旨在实现高速运行。2. 基于现有的人类生物力学研究，将优化后的机器人步态与人类跑步力学进行比较。3. 将优化后的跑步步态集成到一个完整的控制器中，使其能够完成从站立姿态启动和停止的100米短跑任务。

Result: 1. 成功优化了Cassie的跑步步态，实现了高速运行。2. 尽管Cassie与人类在形态上存在差异，但在广泛的速度范围内，其步态的关键特性与人类高度相似。3. 该控制器在硬件上得到验证，Cassie成功创造了双足机器人100米短跑的最快吉尼斯世界纪录。

Conclusion: 通过优化步态和集成控制器，双足机器人Cassie能够实现高效且类似人类的高速奔跑，并成功完成现实世界的100米短跑任务，创造了新的性能里程碑。

Abstract: In this paper, we explore the space of running gaits for the bipedal robot
Cassie. Our first contribution is to present an approach for optimizing gait
efficiency across a spectrum of speeds with the aim of enabling extremely
high-speed running on hardware. This raises the question of how the resulting
gaits compare to human running mechanics, which are known to be highly
efficient in comparison to quadrupeds. Our second contribution is to conduct
this comparison based on established human biomechanical studies. We find that
despite morphological differences between Cassie and humans, key properties of
the gaits are highly similar across a wide range of speeds. Finally, our third
contribution is to integrate the optimized running gaits into a full controller
that satisfies the rules of the real-world task of the 100m dash, including
starting and stopping from a standing position. We demonstrate this controller
on hardware to establish the Guinness World Record for Fastest 100m by a
Bipedal Robot.

</details>


### [256] [Point2Act: Efficient 3D Distillation of Multimodal LLMs for Zero-Shot Context-Aware Grasping](https://arxiv.org/abs/2508.03099)
*Sang Min Kim,Hyeongjun Heo,Junho Kim,Yonghyeon Lee,Young Min Kim*

Main category: cs.RO

TL;DR: Point2Act利用多模态大语言模型（MLLMs）直接从上下文描述中检索3D动作点，解决了现有方法在精确3D定位上的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型为通用机器人提供了零样本任务执行能力，但从2D图像和语言数据中获得的语义理解难以推断出精确的3D动作位置，存在2D区域模糊、几何模糊（如遮挡）和语言描述中的语义不确定性等问题。

Method: Point2Act绕过高维特征，通过“3D相关性场”高效地注入轻量级的2D点级引导，并结合多视图聚合来补偿几何和语义不确定性造成的错位。其完整堆栈流程包括捕获、MLLM查询、3D重建和抓取姿态提取。

Result: 该方法输出高度局部化的3D空间上下文，可直接转化为物理动作的精确位置，并在20秒内生成空间接地响应，促进了实际操作任务。

Conclusion: Point2Act提供了一个高效且实用的解决方案，能够将自然语言描述精确地映射到3D动作点，从而实现机器人对场景的精确物理交互。

Abstract: We propose Point2Act, which directly retrieves the 3D action point relevant
for a contextually described task, leveraging Multimodal Large Language Models
(MLLMs). Foundation models opened the possibility for generalist robots that
can perform a zero-shot task following natural language descriptions within an
unseen environment. While the semantics obtained from large-scale image and
language datasets provide contextual understanding in 2D images, the rich yet
nuanced features deduce blurry 2D regions and struggle to find precise 3D
locations for actions. Our proposed 3D relevancy fields bypass the
high-dimensional features and instead efficiently imbue lightweight 2D
point-level guidance tailored to the task-specific action. The multi-view
aggregation effectively compensates for misalignments due to geometric
ambiguities, such as occlusion, or semantic uncertainties inherent in the
language descriptions. The output region is highly localized, reasoning
fine-grained 3D spatial context that can directly transfer to an explicit
position for physical action at the on-the-fly reconstruction of the scene. Our
full-stack pipeline, which includes capturing, MLLM querying, 3D
reconstruction, and grasp pose extraction, generates spatially grounded
responses in under 20 seconds, facilitating practical manipulation tasks.
Project page: https://sangminkim-99.github.io/point2act/

</details>


### [257] [Safety-Aware Imitation Learning via MPC-Guided Disturbance Injection](https://arxiv.org/abs/2508.03129)
*Le Qiu,Yusuf Umut Ciftci,Somil Bansal*

Main category: cs.RO

TL;DR: MPC-SafeGIL通过在专家演示中注入对抗性扰动来增强模仿学习的安全性，使策略学习鲁棒的恢复行为。


<details>
  <summary>Details</summary>
Motivation: 模仿学习策略可能导致安全违规，限制了其在安全关键应用中的部署。

Method: 提出MPC-SafeGIL，一种设计时方法，在专家演示期间注入对抗性扰动，使专家暴露于更广泛的安全关键场景。该方法使用基于采样的模型预测控制（MPC）来近似最坏情况扰动，适用于高维和黑盒动态系统。它将安全考虑直接整合到数据收集中。

Result: 通过四足运动和视觉运动导航的广泛模拟以及四旋翼的真实世界实验验证了该方法，证明了安全性和任务性能均有所提高。

Conclusion: MPC-SafeGIL通过在数据收集阶段主动引入对抗性扰动，有效提升了模仿学习策略的安全性，并使其能学习到鲁棒的恢复行为。

Abstract: Imitation Learning has provided a promising approach to learning complex
robot behaviors from expert demonstrations. However, learned policies can make
errors that lead to safety violations, which limits their deployment in
safety-critical applications. We propose MPC-SafeGIL, a design-time approach
that enhances the safety of imitation learning by injecting adversarial
disturbances during expert demonstrations. This exposes the expert to a broader
range of safety-critical scenarios and allows the imitation policy to learn
robust recovery behaviors. Our method uses sampling-based Model Predictive
Control (MPC) to approximate worst-case disturbances, making it scalable to
high-dimensional and black-box dynamical systems. In contrast to prior work
that relies on analytical models or interactive experts, MPC-SafeGIL integrates
safety considerations directly into data collection. We validate our approach
through extensive simulations including quadruped locomotion and visuomotor
navigation and real-world experiments on a quadrotor, demonstrating
improvements in both safety and task performance. See our website here:
https://leqiu2003.github.io/MPCSafeGIL/

</details>


### [258] [Language as Cost: Proactive Hazard Mapping using VLM for Robot Navigation](https://arxiv.org/abs/2508.03138)
*Mintaek Oh,Chan Kim,Seung-Woo Seo,Seong-Woo Kim*

Main category: cs.RO

TL;DR: 该论文提出一种零样本“语言即成本”映射框架，利用视觉-语言模型（VLM）解释视觉场景并预测动态风险，从而使机器人能够主动规避潜在危险，而非被动反应。


<details>
  <summary>Details</summary>
Motivation: 传统机器人导航系统依赖静态地图，难以处理动态风险（如突然开门后出现的人），导致其在应对动态危险时通常是被动而非主动的。预训练大型语言模型和视觉-语言模型的发展为主动避险提供了新机遇。

Method: 提出一个零样本“语言即成本”映射框架。该框架利用视觉-语言模型（VLMs）来解释视觉场景，评估潜在的动态风险，并预先分配风险感知导航成本。这些语言成本图与几何障碍图结合，使机器人不仅识别现有障碍物，还能预测并主动规划规避环境动态产生的潜在危险。

Result: 在模拟和多样化的动态环境中进行的实验表明，与被动基线规划器相比，所提出的方法显著提高了导航成功率并减少了危险遭遇。

Conclusion: 通过整合基于语言的成本图和几何障碍图，机器人能够主动预测并规划规避潜在的动态环境危险，从而显著提升导航安全性和效率。

Abstract: Robots operating in human-centric or hazardous environments must proactively
anticipate and mitigate dangers beyond basic obstacle detection. Traditional
navigation systems often depend on static maps, which struggle to account for
dynamic risks, such as a person emerging from a suddenly opening door. As a
result, these systems tend to be reactive rather than anticipatory when
handling dynamic hazards. Recent advancements in pre-trained large language
models and vision-language models (VLMs) create new opportunities for proactive
hazard avoidance. In this work, we propose a zero-shot language-as-cost mapping
framework that leverages VLMs to interpret visual scenes, assess potential
dynamic risks, and assign risk-aware navigation costs preemptively, enabling
robots to anticipate hazards before they materialize. By integrating this
language-based cost map with a geometric obstacle map, the robot not only
identifies existing obstacles but also anticipates and proactively plans around
potential hazards arising from environmental dynamics. Experiments in simulated
and diverse dynamic environments demonstrate that the proposed method
significantly improves navigation success rates and reduces hazard encounters,
compared to reactive baseline planners. Code and supplementary materials are
available at https://github.com/Taekmino/LaC.

</details>


### [259] [CookBench: A Long-Horizon Embodied Planning Benchmark for Complex Cooking Scenarios](https://arxiv.org/abs/2508.03232)
*Muzhen Cai,Xiubo Chen,Yining An,Jiaxin Zhang,Xuesong Wang,Wang Xu,Weinan Zhang,Ting Liu*

Main category: cs.RO

TL;DR: CookBench是一个新的基准，旨在解决现有具身规划基准任务周期短、动作粒度粗的问题，通过在复杂的烹饪场景中提供长周期、细粒度的具身规划任务，并揭示了现有大型语言模型和视觉语言模型在此类任务中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的具身规划基准通常任务周期短且动作原语粗糙，无法有效模拟和解决复杂物理世界中的长周期任务挑战。

Method: 引入CookBench，一个基于Unity高保真模拟环境的烹饪场景基准。核心任务分为两阶段：意图识别和具身交互。动作粒度被细化到考虑关键操作信息的空间级别，同时抽象了低级机器人控制。提供了一个包含宏观操作和细粒度具身动作的统一API工具集。对最先进的闭源大型语言模型和视觉语言模型进行了深入分析。

Result: 通过CookBench的分析，揭示了最先进的闭源大型语言模型和视觉语言模型在处理复杂、长周期任务时存在主要缺陷和挑战。

Conclusion: CookBench为具身规划领域提供了一个具有挑战性的新基准，专注于长周期和细粒度任务，有助于推动具身智能研究，并揭示了当前AI模型在处理此类复杂任务时的局限性，从而促进未来的研究。

Abstract: Embodied Planning is dedicated to the goal of creating agents capable of
executing long-horizon tasks in complex physical worlds. However, existing
embodied planning benchmarks frequently feature short-horizon tasks and
coarse-grained action primitives. To address this challenge, we introduce
CookBench, a benchmark for long-horizon planning in complex cooking scenarios.
By leveraging a high-fidelity simulation environment built upon the powerful
Unity game engine, we define frontier AI challenges in a complex, realistic
environment. The core task in CookBench is designed as a two-stage process.
First, in Intention Recognition, an agent needs to accurately parse a user's
complex intent. Second, in Embodied Interaction, the agent should execute the
identified cooking goal through a long-horizon, fine-grained sequence of
physical actions. Unlike existing embodied planning benchmarks, we refine the
action granularity to a spatial level that considers crucial operational
information while abstracting away low-level robotic control. Besides, We
provide a comprehensive toolset that encapsulates the simulator. Its unified
API supports both macro-level operations, such as placing orders and purchasing
ingredients, and a rich set of fine-grained embodied actions for physical
interaction, enabling researchers to focus on high-level planning and
decision-making. Furthermore, we present an in-depth analysis of
state-of-the-art, closed-source Large Language Model and Vision-Language Model,
revealing their major shortcomings and challenges posed by complex,
long-horizon tasks. The full benchmark will be open-sourced to facilitate
future research.

</details>


### [260] [Force-Compliance MPC and Robot-User CBFs for Interactive Navigation and User-Robot Safety in Hexapod Guide Robots](https://arxiv.org/abs/2508.03246)
*Zehua Fan,Feng Gao,Zhijun Chen,Yunpeng Yin,Limin Yang,Qingxing Xi,En Yang,Xuefeng Luo*

Main category: cs.RO

TL;DR: 该研究提出了一种针对六足导盲机器人的力-顺从模型预测控制（FC-MPC）和机器人-用户控制障碍函数（CBFs）方法，以实现在复杂环境中与用户的双向交互、安全导航和避障，并采用高效的障碍物感知技术。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境中引导视障人士需要实时的双向交互和严格的安全保障。

Method: 该研究提出了以下方法：1) **力-顺从模型预测控制（FC-MPC）**：通过机器人动力学模型和递归最小二乘法（RLS）估计用户施加的力和力矩，并相应调整机器人运动，实现双向交互。2) **机器人-用户控制障碍函数（CBFs）**：确保用户和机器人安全，处理静态和动态障碍物，并采用加权松弛变量解决复杂动态环境中的可行性问题。3) **八向连接DBSCAN方法**：用于障碍物聚类，将计算复杂度从O(n²)降低到大约O(n)，实现资源有限的机载机器人计算机上的实时局部感知。4) **障碍物建模**：使用最小边界椭圆（MBEs）。5) **轨迹预测**：通过卡尔曼滤波进行。系统在HexGuide机器人上实现。

Result: 该系统无缝集成了力顺从性、自主导航和避障功能。实验结果表明，系统能够在复杂环境中根据用户力指令进行调整，同时保证用户和机器人的安全。

Conclusion: 所提出的系统通过结合力顺从性、安全保障和高效的实时障碍物处理，成功实现了在复杂环境中对视障人士的有效引导。

Abstract: Guiding the visually impaired in complex environments requires real-time
two-way interaction and safety assurance. We propose a Force-Compliance Model
Predictive Control (FC-MPC) and Robot-User Control Barrier Functions (CBFs) for
force-compliant navigation and obstacle avoidance in Hexapod guide robots.
FC-MPC enables two-way interaction by estimating user-applied forces and
moments using the robot's dynamic model and the recursive least squares (RLS)
method, and then adjusting the robot's movements accordingly, while Robot-User
CBFs ensure the safety of both the user and the robot by handling static and
dynamic obstacles, and employ weighted slack variables to overcome feasibility
issues in complex dynamic environments. We also adopt an Eight-Way Connected
DBSCAN method for obstacle clustering, reducing computational complexity from
O(n2) to approximately O(n), enabling real-time local perception on
resource-limited on-board robot computers. Obstacles are modeled using Minimum
Bounding Ellipses (MBEs), and their trajectories are predicted through Kalman
filtering. Implemented on the HexGuide robot, the system seamlessly integrates
force compliance, autonomous navigation, and obstacle avoidance. Experimental
results demonstrate the system's ability to adapt to user force commands while
guaranteeing user and robot safety simultaneously during navigation in complex
environments.

</details>


### [261] [Opti-Acoustic Scene Reconstruction in Highly Turbid Underwater Environments](https://arxiv.org/abs/2508.03408)
*Ivana Collado-Gonzalez,John McConnell,Paul Szenher,Brendan Englot*

Main category: cs.RO

TL;DR: 提出了一种实时光声融合的水下场景重建方法，专为浑浊水域优化，克服了单一视觉或声纳方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 水下机器人近距离导航需要场景重建能力。单目视觉方法在浑浊水域不可靠且缺乏深度尺度信息；声纳虽然耐受浑浊水和非均匀光照，但分辨率低且存在高程模糊性。

Method: 提出了一种实时光声场景重建方法。该方法避免识别视觉数据中的点特征，转而识别数据中的感兴趣区域，并将图像中的相关区域与对应的声纳数据进行匹配。通过利用声纳的距离数据和相机图像的高程数据来获得场景重建。

Result: 在不同浑浊度水平下，与其它基于视觉和基于声纳的方法进行了实验比较，并在码头环境中进行了现场测试，验证了所提出方法的有效性。

Conclusion: 所提出的光声融合方法能够有效克服传统单一视觉或声纳方法在浑浊水域进行水下场景重建时的局限性，实现了鲁棒且有效的重建。

Abstract: Scene reconstruction is an essential capability for underwater robots
navigating in close proximity to structures. Monocular vision-based
reconstruction methods are unreliable in turbid waters and lack depth scale
information. Sonars are robust to turbid water and non-uniform lighting
conditions, however, they have low resolution and elevation ambiguity. This
work proposes a real-time opti-acoustic scene reconstruction method that is
specially optimized to work in turbid water. Our strategy avoids having to
identify point features in visual data and instead identifies regions of
interest in the data. We then match relevant regions in the image to
corresponding sonar data. A reconstruction is obtained by leveraging range data
from the sonar and elevation data from the camera image. Experimental
comparisons against other vision-based and sonar-based approaches at varying
turbidity levels, and field tests conducted in marina environments, validate
the effectiveness of the proposed approach. We have made our code open-source
to facilitate reproducibility and encourage community engagement.

</details>


### [262] [Theatre in the Loop: A Rehearsal-Based, Collaborative Workflow for Expressive Robotic Behaviours](https://arxiv.org/abs/2508.03514)
*Pavlos Panagiotidis,Victor Zhi Heung Ngo,Sean Myatt,Roma Patel,Rachel Ramchurn,Alan Chamberlain,Ayse Kucukyilmaz*

Main category: cs.RO

TL;DR: 本文提出了“剧场循环”框架，通过导演指导的木偶操作工作流，为机器人艺术表演开发富有表现力的行为，并生成可重用的运动模板。


<details>
  <summary>Details</summary>
Motivation: 为机器人艺术表演创造具有情感表达的机器人行为，并探索人机协作生成社交表达行为的方法。

Method: 提出“剧场循环”框架，利用戏剧方法，由导演设定叙事目标，指导木偶操作师即兴生成表达特定情感的机器人姿态。这些即兴表演被捕捉并整理，构建可重用于未来自主表演的运动模板数据集。

Result: 初步试验表明该方法可行，能将机器人姿态精确塑造成连贯的情感弧线，同时也揭示了机器人机械限制带来的挑战。

Conclusion: 该实践主导的框架为跨学科团队创建社交表达机器人行为提供了一个模型，有助于将剧场作为人机交互的互动训练场，并促进人机之间的共同创造方法论。

Abstract: In this paper, we propose theatre-in-the-loop, a framework for developing
expressive robot behaviours tailored to artistic performance through a
director-guided puppeteering workflow. Leveraging theatrical methods, we use
narrative objectives to direct a puppeteer in generating improvised robotic
gestures that convey specific emotions. These improvisations are captured and
curated to build a dataset of reusable movement templates for standalone
playback in future autonomous performances. Initial trials demonstrate the
feasibility of this approach, illustrating how the workflow enables precise
sculpting of robotic gestures into coherent emotional arcs while revealing
challenges posed by the robot's mechanical constraints. We argue that this
practice-led framework provides a model for interdisciplinary teams creating
socially expressive robot behaviours, contributing to (1) theatre as an
interactive training ground for human-robot interaction and (2) co-creation
methodologies between humans and machines.

</details>


### [263] [CollaBot: Vision-Language Guided Simultaneous Collaborative Manipulation](https://arxiv.org/abs/2508.03526)
*Kun Song,Shentao Ma,Gaoming Chen,Ninglong Jin,Guangbao Zhao,Mingyu Ding,Zhenhua Xiong,Jia Pan*

Main category: cs.RO

TL;DR: CollaBot是一个通用的协同操作框架，用于多机器人同时搬运大型物体。


<details>
  <summary>Details</summary>
Motivation: 传统机器人研究主要关注小型物体操作，但工厂和家庭环境中常需移动大型物体（如桌子），这需要多机器人协作。现有研究缺乏可扩展且通用的框架。

Method: 该方法首先使用SEEM进行场景分割和目标物体点云提取；然后提出一个协同抓取框架，将其分解为局部抓取姿态生成和全局协作；最后设计一个两阶段规划模块，生成无碰撞轨迹。

Result: 实验表明，该框架在不同数量的机器人、物体和任务下，成功率达到52%，证明了其有效性。

Conclusion: CollaBot框架为同时协同操作提供了有效的解决方案，能够处理大型物体的搬运任务。

Abstract: A central research topic in robotics is how to use this system to interact
with the physical world. Traditional manipulation tasks primarily focus on
small objects. However, in factory or home environments, there is often a need
for the movement of large objects, such as moving tables. These tasks typically
require multi-robot systems to work collaboratively. Previous research lacks a
framework that can scale to arbitrary sizes of robots and generalize to various
kinds of tasks. In this work, we propose CollaBot, a generalist framework for
simultaneous collaborative manipulation. First, we use SEEM for scene
segmentation and point cloud extraction of the target object. Then, we propose
a collaborative grasping framework, which decomposes the task into local grasp
pose generation and global collaboration. Finally, we design a 2-stage planning
module that can generate collision-free trajectories to achieve this task.
Experiments show a success rate of 52% across different numbers of robots,
objects, and tasks, indicating the effectiveness of the proposed framework.

</details>


### [264] [Vision-based Perception System for Automated Delivery Robot-Pedestrians Interactions](https://arxiv.org/abs/2508.03541)
*Ergi Tushe,Bilal Farooq*

Main category: cs.RO

TL;DR: 该研究开发了一个基于单目视觉传感器的完整管道，用于在行人密集区域实现自动配送机器人（ADRs）的多行人检测、跟踪、姿态估计和深度感知，显著提升了行人轨迹预测和身份保持能力，并支持更具社会意识的机器人行为。


<details>
  <summary>Details</summary>
Motivation: 自动配送机器人（ADRs）在行人密集的城市空间中面临安全、高效和可被社会接受的导航挑战，尤其是在多行人场景下。

Method: 开发了一个基于单个视觉传感器的完整管道，包含多行人检测与跟踪、姿态估计和单目深度感知。利用真实世界的MOT17数据集序列，通过整合人体姿态估计和深度信息来增强行人轨迹预测和身份保持。

Result: 结果显示显著改进：身份保持（IDF1）提升高达10%，多目标跟踪准确度（MOTA）提升7%，即使在复杂场景下检测精度也始终高于85%。该系统还能识别脆弱行人群体，支持更具社会意识和包容性的机器人行为。

Conclusion: 将人体姿态估计和深度信息整合到单目视觉系统中，能够有效提升行人的轨迹预测和身份保持能力，从而使自动配送机器人在城市环境中实现更安全、更高效且更具社会意识的导航。

Abstract: The integration of Automated Delivery Robots (ADRs) into pedestrian-heavy
urban spaces introduces unique challenges in terms of safe, efficient, and
socially acceptable navigation. We develop the complete pipeline for a single
vision sensor based multi-pedestrian detection and tracking, pose estimation,
and monocular depth perception. Leveraging the real-world MOT17 dataset
sequences, this study demonstrates how integrating human-pose estimation and
depth cues enhances pedestrian trajectory prediction and identity maintenance,
even under occlusions and dense crowds. Results show measurable improvements,
including up to a 10% increase in identity preservation (IDF1), a 7%
improvement in multiobject tracking accuracy (MOTA), and consistently high
detection precision exceeding 85%, even in challenging scenarios. Notably, the
system identifies vulnerable pedestrian groups supporting more socially aware
and inclusive robot behaviour.

</details>


### [265] [Online Learning for Vibration Suppression in Physical Robot Interaction using Power Tools](https://arxiv.org/abs/2508.03559)
*Gokhan Solak,Arash Ajoudani*

Main category: cs.RO

TL;DR: 本文提出了一种名为“阻尼BMFLC”的新方法，用于协作机器人在嘈杂环境中主动抑制由外部源引起的振动，通过改进的自适应步长和阻尼机制，提高了收敛速度和抗噪能力。


<details>
  <summary>Details</summary>
Motivation: 在建筑工地等恶劣环境中部署的协作机器人需要具备振动抑制能力，特别是针对电动工具等外部源引起的振动，以提高其性能和稳定性。

Method: 本文采用带限多傅里叶线性组合器（BMFLC）算法在线学习振动并通过前馈力控制进行抵消。在此基础上，提出了“阻尼BMFLC”方法，引入了一种新颖的自适应步长方法以提高收敛时间和抗噪性。其基于逻辑函数的阻尼机制减少了噪声影响，并允许使用更大的学习率。

Result: 仿真实验表明，与原始BMFLC及其基于递归最小二乘和卡尔曼滤波的扩展相比，所提出的方法提高了抑制率，并且效率远高于后两者。此外，在实际抛光实验中也验证了该方法的有效性。

Conclusion: 所提出的阻尼BMFLC方法有效解决了协作机器人在外部振动源影响下的振动抑制问题，通过其创新的自适应步长和阻尼机制，显著提升了抑制性能、收敛速度和抗噪能力。

Abstract: Vibration suppression is an important capability for collaborative robots
deployed in challenging environments such as construction sites. We study the
active suppression of vibration caused by external sources such as power tools.
We adopt the band-limited multiple Fourier linear combiner (BMFLC) algorithm to
learn the vibration online and counter it by feedforward force control. We
propose the damped BMFLC method, extending BMFLC with a novel adaptive
step-size approach that improves the convergence time and noise resistance. Our
logistic function-based damping mechanism reduces the effect of noise and
enables larger learning rates. We evaluate our method on extensive simulation
experiments with realistic time-varying multi-frequency vibration and
real-world physical interaction experiments. The simulation experiments show
that our method improves the suppression rate in comparison to the original
BMFLC and its recursive least squares and Kalman filter-based extensions.
Furthermore, our method is far more efficient than the latter two. We further
validate the effectiveness of our method in real-world polishing experiments. A
supplementary video is available at https://youtu.be/ms6m-6JyVAI.

</details>


### [266] [Why Evolve When You Can Adapt? Post-Evolution Adaptation of Genetic Memory for On-the-Fly Control](https://arxiv.org/abs/2508.03600)
*Hamze Hammami,Eva Denisa Barbulescu,Talal Shaikh,Mouayad Aldada,Muhammad Saad Munawar*

Main category: cs.RO

TL;DR: 本文提出了一种结合遗传算法（GA）控制器与在线赫布（Hebbian）可塑性的新型零样本自适应机制，使机器人控制器能像人类突触一样动态调整以应对实时挑战。


<details>
  <summary>Details</summary>
Motivation: 目前的机器人控制器难以在实时面对不可预见的挑战时进行适应，而生物系统（如人类突触）具有动态重连的能力，启发了研究者开发一种类似自适应机制。

Method: 该方法将GA控制器与在线赫布可塑性结合，分离了学习（赫布更新）和记忆（基因型）。适应度函数被用作赫布学习的实时缩放因子，使机器人神经控制器能够实时调整突触权重，无需额外训练。任务结束后，机器人“遗忘”临时调整，恢复原始权重，保留核心知识。

Result: 该混合GA-赫布控制器在e-puck机器人的T型迷宫导航任务中得到了验证，该任务包含不断变化的光照条件和障碍物。

Conclusion: 所提出的混合GA-赫布控制器为机器人提供了一个动态的自适应层，使其能够在运行时处理意外的环境变化，同时保留核心知识。

Abstract: Imagine a robot controller with the ability to adapt like human synapses,
dynamically rewiring itself to overcome unforeseen challenges in real time.
This paper proposes a novel zero-shot adaptation mechanism for evolutionary
robotics, merging a standard Genetic Algorithm (GA) controller with online
Hebbian plasticity. Inspired by biological systems, the method separates
learning and memory, with the genotype acting as memory and Hebbian updates
handling learning. In our approach, the fitness function is leveraged as a live
scaling factor for Hebbian learning, enabling the robot's neural controller to
adjust synaptic weights on-the-fly without additional training. This adds a
dynamic adaptive layer that activates only during runtime to handle unexpected
environmental changes. After the task, the robot 'forgets' the temporary
adjustments and reverts to the original weights, preserving core knowledge. We
validate this hybrid GA-Hebbian controller on an e-puck robot in a T-maze
navigation task with changing light conditions and obstacles.

</details>


### [267] [DiWA: Diffusion Policy Adaptation with World Models](https://arxiv.org/abs/2508.03645)
*Akshay L Chandra,Iman Nematollahi,Chenguang Huang,Tim Welschehold,Wolfram Burgard,Abhinav Valada*

Main category: cs.RO

TL;DR: DiWA提出了一种新颖的框架，通过离线世界模型对基于扩散的机器人策略进行微调，显著提高了样本效率，解决了传统强化学习微调扩散策略时交互成本高昂的问题。


<details>
  <summary>Details</summary>
Motivation: 使用强化学习微调扩散策略面临两大挑战：一是长时间的去噪序列阻碍了奖励有效传播；二是标准强化学习方法需要数百万次真实世界交互，效率低下且不切实际。现有将去噪过程视为马尔可夫决策过程的方法仍高度依赖环境交互。

Method: DiWA框架利用一个世界模型，完全离线地使用强化学习微调基于扩散的机器人技能。该世界模型仅需在数十万次离线交互数据上训练一次。

Result: DiWA在CALVIN基准测试中，仅通过离线适应就显著提升了八项任务的性能，并且所需物理交互次数比无模型基线少几个数量级，极大地提高了样本效率。这是首次展示使用离线世界模型微调真实世界机器人技能的扩散策略。

Conclusion: DiWA通过引入离线世界模型，成功克服了强化学习微调扩散策略的挑战，显著提升了样本效率和实用性，为真实世界机器人学习提供了更可行、更安全的方法。

Abstract: Fine-tuning diffusion policies with reinforcement learning (RL) presents
significant challenges. The long denoising sequence for each action prediction
impedes effective reward propagation. Moreover, standard RL methods require
millions of real-world interactions, posing a major bottleneck for practical
fine-tuning. Although prior work frames the denoising process in diffusion
policies as a Markov Decision Process to enable RL-based updates, its strong
dependence on environment interaction remains highly inefficient. To bridge
this gap, we introduce DiWA, a novel framework that leverages a world model for
fine-tuning diffusion-based robotic skills entirely offline with reinforcement
learning. Unlike model-free approaches that require millions of environment
interactions to fine-tune a repertoire of robot skills, DiWA achieves effective
adaptation using a world model trained once on a few hundred thousand offline
play interactions. This results in dramatically improved sample efficiency,
making the approach significantly more practical and safer for real-world robot
learning. On the challenging CALVIN benchmark, DiWA improves performance across
eight tasks using only offline adaptation, while requiring orders of magnitude
fewer physical interactions than model-free baselines. To our knowledge, this
is the first demonstration of fine-tuning diffusion policies for real-world
robotic skills using an offline world model. We make the code publicly
available at https://diwa.cs.uni-freiburg.de.

</details>


### [268] [Inland-LOAM: Voxel-Based Structural Semantic Mapping for Inland Waterways](https://arxiv.org/abs/2508.03672)
*Zhongbi Luo,Yunjia Wang,Jan Swevers,Peter Slaets,Herman Bruyninckx*

Main category: cs.RO

TL;DR: 本文提出Inland-LOAM，一个针对内陆水域的激光雷达SLAM框架，通过改进特征提取和水面平面约束解决了传统方法在水域的垂直漂移问题，并生成结构化的2D语义地图和海岸线，提高了定位精度和态势感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有内陆水域电子航道图（IENC）缺乏实时细节，且传统激光雷达SLAM在水域环境中表现不佳，存在垂直漂移和生成非语义地图的问题，这些都阻碍了内陆水域自主航行。

Method: 该研究引入Inland-LOAM框架，采用改进的特征提取和水面平面约束来减轻垂直漂移。它通过基于体素的几何分析将3D点云转换为结构化的2D语义地图，实现实时计算航行参数（如桥梁净空）。此外，还包含一个自动化模块用于提取海岸线并导出为轻量级、兼容IENC的格式。

Result: 在真实世界数据集上的评估表明，Inland-LOAM在定位精度上优于现有先进方法。生成的语义地图和海岸线与真实条件高度吻合，为增强态势感知提供了可靠数据。

Conclusion: Inland-LOAM框架有效解决了内陆水域自主航行中高精度地理空间信息获取的挑战，通过提供卓越的定位精度和实用的语义地图，显著提升了内陆水域运输的安全性与自主性。

Abstract: Accurate geospatial information is crucial for safe, autonomous Inland
Waterway Transport (IWT), as existing charts (IENC) lack real-time detail and
conventional LiDAR SLAM fails in waterway environments. These challenges lead
to vertical drift and non-semantic maps, hindering autonomous navigation.
  This paper introduces Inland-LOAM, a LiDAR SLAM framework for waterways. It
uses an improved feature extraction and a water surface planar constraint to
mitigate vertical drift. A novel pipeline transforms 3D point clouds into
structured 2D semantic maps using voxel-based geometric analysis, enabling
real-time computation of navigational parameters like bridge clearances. An
automated module extracts shorelines and exports them into a lightweight,
IENC-compatible format.
  Evaluations on a real-world dataset show Inland-LOAM achieves superior
localization accuracy over state-of-the-art methods. The generated semantic
maps and shorelines align with real-world conditions, providing reliable data
for enhanced situational awareness. The code and dataset will be publicly
available

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [269] [State dimension reduction of recurrent equilibrium networks with contraction and robustness preservation](https://arxiv.org/abs/2508.02843)
*M. F. Shakib*

Main category: eess.SY

TL;DR: 本文提出了一种基于投影的方法，用于降低已训练的循环平衡网络（RENs）中线性时不变（LTI）组件的状态维度，以实现在资源受限设备上的部署，同时保持收缩性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 循环平衡网络（RENs）在学习复杂动力学系统方面表现出色，并具有经过认证的收缩性和鲁棒性。然而，将大规模RENs部署到资源受限设备上的实时应用中仍然是一个挑战。

Method: 该方法提出了一种基于投影的方法来减少已训练RENs中LTI组件的状态维度。其中一个投影矩阵专门用于利用已学习的REN收缩性证书来保持收缩性和鲁棒性。另一个投影矩阵则基于LTI模型降阶的必要h2-最优条件进行迭代更新，以提高降阶REN的准确性。

Result: 数值示例验证了该方法，表明在有限的精度损失下实现了显著的状态维度降低，同时保留了收缩性和鲁棒性。

Conclusion: 所提出的投影方法能够有效降低RENs的复杂性，使其适用于资源受限的设备，同时确保其关键性能（收缩性和鲁棒性）得以保持。

Abstract: Recurrent equilibrium networks (RENs) are effective for learning the dynamics
of complex dynamical systems with certified contraction and robustness
properties through unconstrained learning. While this opens the door to
learning large-scale RENs, deploying such large-scale RENs in real-time
applications on resource-limited devices remains challenging. Since a REN
consists of a feedback interconnection of linear time-invariant (LTI) dynamics
and static activation functions, this article proposes a projection-based
approach to reduce the state dimension of the LTI component of a trained REN.
One of the two projection matrices is dedicated to preserving contraction and
robustness by leveraging the already-learned REN contraction certificate. The
other projection matrix is iteratively updated to improve the accuracy of the
reduced-order REN based on necessary $h_2$-optimality conditions for LTI model
reduction. Numerical examples validate the approach, demonstrating significant
state dimension reduction with limited accuracy loss while preserving
contraction and robustness.

</details>


### [270] [Optimizing Preventive and Reactive Defense Resource Allocation with Uncertain Sensor Signals](https://arxiv.org/abs/2508.02881)
*Faezeh Shojaeighadikolaei,Shouhuai Xu,Keith Paarporn*

Main category: eess.SY

TL;DR: 研究在网络攻击防御中，如何在预防性防御和响应性防御之间分配资源，同时考虑传感器信号不确定性对防御策略和安全水平的影响。


<details>
  <summary>Details</summary>
Motivation: 现有网络防御决策框架侧重于阻止攻击成功，但忽略了成功攻击造成的清理成本。同时，攻击检测器不完美导致节点是否被入侵的传感器信号存在不确定性，这促使研究如何在预防和响应防御之间进行资源分配。

Method: 本文提出并建立了一个新的资源分配问题模型，要求防御者决定如何在旨在强化节点以抵御攻击的预防性防御和旨在快速清理受损节点的响应性防御之间分配投资，并考虑了与观察（传感器信号）相关的真伪不确定性。

Result: 研究表明，随着传感器质量的提高，对预防性资源的最佳投资会增加，从而减少对响应性资源的投资。此外，当攻击者只能实现较低的攻击成功概率时，防御者相对于不使用传感器的基线，其性能提升最大。

Conclusion: 传感器信号质量对防御者在预防性和响应性防御之间的战略投资选择以及最终能达到的安全水平有显著影响。在攻击成功概率较低的情况下，高质量传感器带来的防御性能提升最为显著。

Abstract: Cyber attacks continue to be a cause of concern despite advances in cyber
defense techniques. Although cyber attacks cannot be fully prevented, standard
decision-making frameworks typically focus on how to prevent them from
succeeding, without considering the cost of cleaning up the damages incurred by
successful attacks. This motivates us to investigate a new resource allocation
problem formulated in this paper: The defender must decide how to split its
investment between preventive defenses, which aim to harden nodes from attacks,
and reactive defenses, which aim to quickly clean up the compromised nodes.
This encounters a challenge imposed by the uncertainty associated with the
observation, or sensor signal, whether a node is truly compromised or not; this
uncertainty is real because attack detectors are not perfect. We investigate
how the quality of sensor signals impacts the defender's strategic investment
in the two types of defense, and ultimately the level of security that can be
achieved. In particular, we show that the optimal investment in preventive
resources increases, and thus reactive resource investment decreases, with
higher sensor quality. We also show that the defender's performance
improvement, relative to a baseline of no sensors employed, is maximal when the
attacker can only achieve low attack success probabilities.

</details>


### [271] [Modeling and Simulation of an Active Quarter Car Suspension with a Robust LQR Controller under Road Disturbance and Parameter Uncertainty](https://arxiv.org/abs/2508.02906)
*Mehmet Karahan*

Main category: eess.SY

TL;DR: 本研究比较了被动悬架、PID主动悬架和LQR主动悬架的性能，发现LQR主动悬架在舒适性和安全性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 车辆悬架对于乘客舒适性、减少振动冲击以及提高车辆抓地力、转弯安全性和降低事故风险至关重要。被动悬架结构简单成本低但无控制器，主动悬架虽复杂昂贵但更安全。PID控制器因其简单和易于调整而被广泛用于主动悬架，但存在进一步提升空间，因此研究旨在设计更鲁棒的LQR控制器主动悬架。

Method: 设计了一个LQR控制的主动悬架系统，并与被动悬架和PID控制的主动悬架进行了比较。对三种悬架系统进行了鲁棒性分析。在道路扰动以及道路扰动与参数不确定性同时存在的情况下，模拟了悬架行程、簧载质量加速度和簧载质量运动。通过获取悬架的上升时间、超调量和稳定时间数据进行了比较分析。

Result: LQR控制的主动悬架表现出最小的超调量和最短的稳定时间，优于被动悬架和PID控制的主动悬架。

Conclusion: LQR控制的主动悬架相比被动悬架和PID控制的主动悬架，能提供更舒适、更安全的驾驶体验。

Abstract: Vehicle suspension is important for passengers to travel comfortably and to
be less exposed to effects such as vibration and shock. A good suspension
system in-creases the road holding of vehicles, allows them to take turns
safely and reduces the risk of traffic accidents. Passive suspension system is
the most widely used suspension system in vehicles due to its simple structure
and low cost. Passive suspension systems do not have an actuator and therefore
do not have a controller. Active suspension systems have an actuator and a
controller. Although their structures are more complex and costly, they are
safer. PID controller is widely used in active suspension systems due to its
simple structure, reasonable cost and easy adjustment of coefficients. In this
study, a more robust LQR controlled active suspension was designed than passive
sus-pension and PID controlled active suspension. Robustness analyses were
performed for passive suspension, PID controlled active suspension and LQR
controlled active sus-pension. Suspension travel, sprung mass acceleration and
sprung mass motion simulations were performed for all 3 suspensions under road
disturbance and under simultaneous road disturbance and parameter uncertainty.
A comparative analysis was performed by obtaining the suspension rise time,
overshoot and settling time data. It was observed that the LQR controlled
active suspension showed the least overshoot and had the shortest settling
time. In this case, it was proven that the LQR controlled active suspension
provided a more comfortable and safe ride compared to the other two suspension
systems.

</details>


### [272] [Integrating Upstream Supply Chains into Generation Expansion Planning](https://arxiv.org/abs/2508.03001)
*Boyu Yao,Andrey Bernstein,Yury Dvorkin*

Main category: eess.SY

TL;DR: 该论文提出了一个考虑上游供应链限制（如材料、制造能力、交付周期和可用性）的多阶段电力系统发电扩张规划（SC-GEP）模型，并通过案例研究证明了其对技术选择、部署时间和成本的影响。


<details>
  <summary>Details</summary>
Motivation: 电力需求不断增长，但传统发电扩张规划模型常忽略上游供应链限制（材料、制造能力、交付周期、现场可用性），这可能导致计划资源延迟上线，威胁系统可靠性。

Method: 引入了一个多阶段供应链受限的发电扩张规划（SC-GEP）模型，该模型优化长期投资，并考虑材料可用性、生产限制、时空约束以及报废资产的材料再利用。使用分解算法高效求解由此产生的混合整数线性规划（MILP）问题。

Result: 马里兰州的案例研究表明，供应链约束会改变技术选择，加剧交付周期导致的部署延迟，并促使更早投资于交付周期短、材料强度低的选项。在低需求情景下，供应链约束使投资成本增加12亿美元。在高需求情景下，会出现持续的发电和备用短缺。

Conclusion: 研究强调了将上游供应链约束整合到长期发电规划中的必要性，以应对电力系统可靠性挑战和潜在的成本上升及短缺问题。

Abstract: Rising electricity demand underscores the need for secure and reliable
generation expansion planning that accounts for upstream supply chain
constraints. Traditional models often overlook limitations in materials,
manufacturing capacity, lead times for deployment, and field availability,
which can delay availability of planned resources and thus to threaten system
reliability. This paper introduces a multi-stage supply chain-constrained
generation expansion planning (SC-GEP) model that optimizes long-term
investments while capturing material availability, production limits, spatial
and temporal constraints, and material reuse from retired assets. A
decomposition algorithm efficiently solves the resulting MILP. A Maryland case
study shows that supply chain constraints shift technology choices, amplify
deployment delays caused by lead times, and prompt earlier investment in
shorter lead-time, low-material-intensity options. In the low-demand scenario,
supply chain constraints raise investment costs by $1.2 billion. Under high
demand, persistent generation and reserve shortfalls emerge, underscoring the
need to integrate upstream constraints into long-term planning.

</details>


### [273] [Power System Voltage Stability Boundary: Computational Results and Applications](https://arxiv.org/abs/2508.03119)
*Zhenyao Li,Yifan Yao,Deqiang Gan*

Main category: eess.SY

TL;DR: 本文报告了DAE稳定性边界的计算结果，旨在推进电力系统电压稳定性研究，提出新的正则化变换、伪鞍点计算方法及电压稳定裕度表达式，并验证了其准确性和有效性。


<details>
  <summary>Details</summary>
Motivation: 为了推进DAE理论在电力系统电压稳定性研究中的应用，需要报告DAE稳定性边界的计算结果。

Method: ['提出了一种新的标准微分代数方程（DAE）正则化变换。', '检验了电压稳定性边界上锚点的存在性。', '提出了一种计算控制伪鞍点的优化方法。', '给出了稳定性边界上伪鞍点稳定流形的局部表示。', '推导了电压稳定裕度表达式。']

Result: ['提出了一种新的DAE正则化变换。', '考察了锚点在电压稳定性边界上的存在性。', '提出了一种计算控制伪鞍点的优化方法。', '给出了稳定性边界上伪鞍点稳定流形的局部表示。', '获得了电压稳定裕度表达式。', '所提出的结果通过多个例子验证，证明了方法的准确性和有效性。']

Conclusion: 所提出的DAE稳定性边界计算方法，包括新的正则化变换、伪鞍点计算和稳定裕度表达式，在电力系统电压稳定性研究中具有准确性和有效性。

Abstract: The objective of this paper is to report some computational results for the
theory of DAE stability boundary, with the aim of advancing applications in
power system voltage stability studies. Firstly, a new regularization
transformation for standard differential-algebraic equations (DAEs) is
proposed. Then the existence of anchor points on voltage stability boundary is
examined, and an optimization method for computing the controlling
pseudo-saddle is suggested. Subsequently, a local representation of the stable
manifold of the pseudo-saddle on the stability boundary is presented, and a
voltage stability margin expression is obtained. Finally, the proposed results
are verified using several examples, demonstrating the accuracy and
effectiveness of the suggested methods.

</details>


### [274] [Filtering and 1/3 Power Law for Optimal Time Discretisation in Numerical Integration of Stochastic Differential Equations](https://arxiv.org/abs/2508.03135)
*Igor G. Vladimirov*

Main category: eess.SY

TL;DR: 本文研究了随机微分方程（SDEs）的数值积分，通过卡尔曼滤波和贝叶斯方法优化时间离散化，发现最优网格密度函数遵循1/3幂律。


<details>
  <summary>Details</summary>
Motivation: 旨在改进随机微分方程（SDEs）强解的数值积分方法，特别是通过优化时间离散化来减少均方误差，以更有效地估计隐藏系统状态。

Method: 将维纳过程替换为离散时间增量，采用滤波（贝叶斯）视角将SDE的近似强解视为隐藏系统状态的估计，并利用布朗桥处理中间时间间隔。对于多变量线性SDEs，数值解被组织成卡尔曼滤波器。通过对均匀网格进行平滑单调变换来定义时间离散化，研究了在精细网格下终端和积分均方误差泛函的渐近行为，并构建了时间离散化剖面的约束优化问题。

Result: 优化问题的解揭示了渐近最优网格密度函数遵循1/3幂律。以Ornstein-Uhlenbeck过程为例，验证了该结果。

Conclusion: 通过卡尔曼滤波方法对线性随机微分方程进行数值积分时，最优的时间离散化网格密度函数遵循1/3幂律，这为提高SDEs数值解的精度提供了理论指导。

Abstract: This paper is concerned with the numerical integration of stochastic
differential equations (SDEs) which govern diffusion processes driven by a
standard Wiener process. With the latter being replaced by a sequence of
increments at discrete moments of time, we revisit a filtering point of view on
the approximate strong solution of the SDE as an estimate of the hidden system
state whose conditional probability distribution is updated using a Bayesian
approach and Brownian bridges over the intermediate time intervals. For a class
of multivariable linear SDEs, where the numerical solution is organised as a
Kalman filter, we investigate the fine-grid asymptotic behaviour of terminal
and integral mean-square error functionals when the time discretisation is
specified by a sufficiently smooth monotonic transformation of a uniform grid.
This leads to constrained optimisation problems over the time discretisation
profile, and their solutions reveal a 1/3 power law for the asymptotically
optimal grid density functions. As a one-dimensional example, the results are
illustrated for the Ornstein-Uhlenbeck process.

</details>


### [275] [An Event-based State Estimation Approach for Positive Systems with Positive Observers](https://arxiv.org/abs/2508.03154)
*Bhargavi Chaudhary,Krishanu Nath,Subashish Datta,Indra Narayan Kar*

Main category: eess.SY

TL;DR: 本文提出了一种针对连续时间线性正网络系统的事件测量型正观测器设计，旨在带宽限制下确保状态估计的非负性和渐近稳定性，并避免Zeno行为。


<details>
  <summary>Details</summary>
Motivation: 在网络化系统中，通信带宽受限是一个常见问题。同时，对于某些物理系统，其状态估计（如浓度、液位等）必须始终保持非负性，这使得传统观测器不适用，需要设计专门的正观测器。

Method: 采用基于事件测量的方法，通过加权采样误差确定系统与观测器之间的采样序列。观测器动力学基于标准Luenberger结构，仅在事件发生时更新。利用线性矩阵不等式（LMI）推导了稳定性和正性条件，并确保了事件触发机制无Zeno行为。

Result: 成功建立了带有采样信息的观测器动力学的渐近稳定性。推导了保证稳定性和正性的充分条件。设计确保了事件触发架构无Zeno行为，保证了执行间隔时间的最小正界。通过三罐系统仿真验证了所提出方法的有效性。

Conclusion: 所提出的事件测量型正观测器设计，在满足网络带宽限制的同时，确保了状态估计的非负性和观测器的渐近稳定性，并有效避免了Zeno现象，为线性正网络系统的状态估计提供了有效解决方案。

Abstract: This article addresses the problem of state observer design for
continuous-time linear positive networked systems. Considering the bandwidth
constraint in the communication network, an event-measurement-based positive
observer design is proposed. The physical interpretation of a positive observer
differs from that of a general observer. Its primary goal is to ensure that all
state estimates remain non-negative at all times. Using output measurements, a
law with weighted sampling error is used to determine the sampling sequence
between the system and the observer. The observer dynamics are designed using
the standard Luenberger structure with the event-based sampled output
information, which is updated only when an event occurs. Assuming observability
and sufficient conditions for the positivity of the system, the asymptotic
stability of the observer dynamics with sampled information is established.
Sufficient conditions of stability and positivity are derived using linear
matrix inequalities. Moreover, the design ensures that the event-based
architecture is free from Zeno behavior, ensuring a positive minimum bound on
the inter-execution time. In addition, numerical simulations on a three-tank
system having variable cross-sections are used to demonstrate the efficacy of
the proposed event-based positive observer.

</details>


### [276] [Grid-Forming Vector Current Control FRT Modes Under Symmetrical and Asymmetrical Faults](https://arxiv.org/abs/2508.03389)
*Ognjen Stanojev,Orcun Karaca,Mario Schweizer*

Main category: eess.SY

TL;DR: 本文提出并分析了几种针对并网变换器网格形成矢量电流控制（GFVCC）方案的故障穿越（FRT）策略，以应对对称和非对称短路故障，同时保持电网同步和网格形成行为。


<details>
  <summary>Details</summary>
Motivation: 网格形成矢量电流控制（GFVCC）方案在并网变换器中展现出控制架构简单、模块化以及易于从锁相环（PLL）型网格跟随控制平滑过渡到网格形成控制等显著优势。然而，任何并网变换器控制策略的关键在于如何处理电网故障场景（如对称和非对称短路故障），同时保持其网格形成特性并尊重硬件限制。

Method: 本文提出了多种GFVCC的故障穿越（FRT）策略。这些策略通过模块化扩展控制方案，纳入负序环路，以应对对称和非对称故障。所提出的FRT策略旨在使变换器在提供故障电流的同时保持与电网同步，尊重硬件限制，并保留网格形成行为。

Result: 所提出的FRT策略使得变换器能够在故障情况下提供故障电流，保持与电网的同步，同时遵守变换器硬件限制并保持其网格形成行为。这些策略通过无限母线设置和多单元电网等案例研究进行了分析和验证。

Conclusion: 所提出的GFVCC故障穿越（FRT）策略能够有效处理对称和非对称电网故障，使变换器在故障期间保持电网同步、提供故障电流，同时满足硬件限制并维持其网格形成能力。

Abstract: Recent research has shown that operating grid-connected converters using the
grid-forming vector current control (GFVCC) scheme offers significant benefits,
including the simplicity and modularity of the control architecture, as well as
enabling a seamless transition from PLL-based grid-following control to
grid-forming. An important aspect of any grid-connected converter control
strategy is the handling of grid-fault scenarios such as symmetrical and
asymmetrical short-circuit faults. This paper presents several fault
ride-through (FRT) strategies for GFVCC that enable the converter to provide
fault current and stay synchronized to the grid while respecting the converter
hardware limitations and retaining grid-forming behavior. The converter control
scheme is extended in a modular manner to include negative-sequence loops, and
the proposed FRT strategies address both symmetrical and asymmetrical faults.
The proposed FRT strategies are analyzed through case studies, including
infinite-bus setups and multi-unit grids.

</details>


### [277] [A Robust Cooperative Vehicle Coordination Framework for Intersection Crossing](https://arxiv.org/abs/2508.03417)
*Haojie Bai,Jiping Luo,Huafu Li,Xiongwei Zhao,Yang Wang*

Main category: eess.SY

TL;DR: 针对无信号交叉口车辆协调中存在的状态不确定性和通信限制问题，本文提出了一个鲁棒的协调框架，包含鲁棒合作轨迹规划器和上下文感知状态更新调度器，旨在提高安全性的同时优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 现有的无信号交叉口车辆协调研究大多过于简化系统，假设车辆状态信息准确且更新过程理想，这在实际存在状态不确定性和通信约束时会带来驾驶风险。

Method: 本文提出了一个鲁棒且全面的交叉口协调框架，包括两个核心组件：1) 一个鲁棒的合作轨迹规划器，直接控制轨迹分布的演变，提供概率性安全保证；2) 一个上下文感知状态更新调度器，根据车辆的驾驶紧急程度动态优先排序状态更新，以适应带宽受限的实际条件。

Result: 仿真结果验证了所提协调框架的鲁棒性和有效性。与最先进的策略相比，该框架在保持可比协调效率的同时，显著降低了碰撞概率。此外，在实际不确定和带宽受限的条件下，该框架在无线资源利用方面表现出卓越的有效性。

Conclusion: 所提出的鲁棒协调框架能够有效应对无信号交叉口车辆协调中的状态不确定性和通信限制，显著提高安全性并优化无线资源利用，使其在实际应用中更具可行性。

Abstract: Cooperative vehicle coordination at unsignalized intersections has garnered
significant interest from both academia and industry in recent years,
highlighting its notable advantages in improving traffic throughput and fuel
efficiency. However, most existing studies oversimplify the coordination
system, assuming accurate vehicle state information and ideal state update
process. The oversights pose driving risks in the presence of state uncertainty
and communication constraint. To address this gap, we propose a robust and
comprehensive intersection coordination framework consisting of a robust
cooperative trajectory planner and a context-aware status update scheduler. The
trajectory planner directly controls the evolution of the trajectory
distributions during frequent vehicle interactions, thereby offering
probabilistic safety guarantees. To further align with coordination safety in
practical bandwidth-limited conditions, we propose a context-aware status
update scheduler that dynamically prioritizes the state updating order of
vehicles based on their driving urgency. Simulation results validate the
robustness and effectiveness of the proposed coordination framework, showing
that the collision probability can be significantly reduced while maintaining
comparable coordination efficiency to state-of-theart strategies. Moreover, our
proposed framework demonstrates superior effectiveness in utilizing wireless
resources in practical uncertain and bandwidth-limited conditions.

</details>


### [278] [Improving Q-Learning for Real-World Control: A Case Study in Series Hybrid Agricultural Tractors](https://arxiv.org/abs/2508.03647)
*Hend Abououf,Sidra Ghayour Bhatti,Qadeer Ahmed*

Main category: eess.SY

TL;DR: 本文评估了Q值强化学习算法在混合农业拖拉机动力系统控制中的性能，并引入了分段领域特定奖励整形策略和专家演示经验回放缓冲区初始化，以加速训练并提高燃油效率。


<details>
  <summary>Details</summary>
Motivation: 混合农业拖拉机负载需求多变且不可预测，导致难以设计最优的基于规则的能量管理策略。现有方法常依赖基本的燃油奖励，且未利用专家演示加速训练。

Method: 1. 评估了Q值强化学习算法（DQL、DQN、DDQN）在混合农业拖拉机动力系统控制中的性能，比较收敛速度和策略最优性。2. 引入分段领域特定奖励整形策略，以提高学习效率并引导智能体行为趋向发动机燃油高效运行区域。3. 检查经验回放缓冲区的设计，重点是使用专家演示初始化缓冲区，并分析不同类型专家策略对收敛动态和最终性能的影响。

Result: 1. 在此应用领域，DDQN比DQN的收敛速度快70%。2. 所提出的奖励整形方法有效促使学习策略产生燃油高效的结果。3. 使用结构化专家数据初始化回放缓冲区可使收敛速度提高33%。

Conclusion: Q值强化学习算法（特别是DDQN）结合领域特定奖励整形和专家演示初始化经验回放缓冲区，能显著提高混合农业拖拉机动力系统能量管理策略的学习效率和燃油经济性。

Abstract: The variable and unpredictable load demands in hybrid agricultural tractors
make it difficult to design optimal rule-based energy management strategies,
motivating the use of adaptive, learning-based control. However, existing
approaches often rely on basic fuel-based rewards and do not leverage expert
demonstrations to accelerate training. In this paper, first, the performance of
Q-value-based reinforcement learning algorithms is evaluated for powertrain
control in a hybrid agricultural tractor. Three algorithms, Double Q-Learning
(DQL), Deep Q-Networks (DQN), and Double DQN (DDQN), are compared in terms of
convergence speed and policy optimality. Second, a piecewise domain-specific
reward-shaping strategy is introduced to improve learning efficiency and steer
agent behavior toward engine fuel-efficient operating regions. Third, the
design of the experience replay buffer is examined, with a focus on the effects
of seeding the buffer with expert demonstrations and analyzing how different
types of expert policies influence convergence dynamics and final performance.
Experimental results demonstrate that (1) DDQN achieves 70\% faster convergence
than DQN in this application domain, (2) the proposed reward shaping method
effectively biases the learned policy toward fuel-efficient outcomes, and (3)
initializing the replay buffer with structured expert data leads to a 33\%
improvement in convergence speed.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [279] [MPCA-based Domain Adaptation for Transfer Learning in Ultrasonic Guided Waves](https://arxiv.org/abs/2508.02726)
*Lucio Pinello,Francesco Cadini,Luca Lomazzi*

Main category: eess.IV

TL;DR: 该研究提出了一种基于多线性主成分分析（MPCA）的迁移学习（TL）框架，用于超声导波（UGW）结构健康监测（SHM）中的损伤定位，以解决数据稀缺和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: UGW结合机器学习（ML）在薄壁结构SHM中应用受限于数据稀缺以及在不同材料和传感器配置下泛化能力有限。

Method: 首先训练一个用于回归的卷积神经网络（CNN）进行平板结构损伤定位。然后，结合MPCA和微调（fine-tuning），使CNN适应不同的平板结构。通过在源域和目标域联合应用MPCA提取共享潜在特征，实现有效的域适应，无需对维度进行预设。接着通过微调使预训练的CNN适应新域，无需大量训练数据。

Result: 该MPCA-based TL方法在12个涉及不同复合材料和传感器阵列的案例研究中进行了测试。结果显示，与标准TL技术相比，定位误差显著降低。

Conclusion: 所提出的方法是一个鲁棒、数据高效且基于统计的UGW-based SHM迁移学习框架，有效提升了损伤定位的准确性。

Abstract: Ultrasonic Guided Waves (UGWs) represent a promising diagnostic tool for
Structural Health Monitoring (SHM) in thin-walled structures, and their
integration with machine learning (ML) algorithms is increasingly being adopted
to enable real-time monitoring capabilities. However, the large-scale
deployment of UGW-based ML methods is constrained by data scarcity and limited
generalisation across different materials and sensor configurations. To address
these limitations, this work proposes a novel transfer learning (TL) framework
based on Multilinear Principal Component Analysis (MPCA). First, a
Convolutional Neural Network (CNN) for regression is trained to perform damage
localisation for a plated structure. Then, MPCA and fine-tuning are combined to
have the CNN work for a different plate. By jointly applying MPCA to the source
and target domains, the method extracts shared latent features, enabling
effective domain adaptation without requiring prior assumptions about
dimensionality. Following MPCA, fine-tuning enables adapting the pre-trained
CNN to a new domain without the need for a large training dataset. The proposed
MPCA-based TL method was tested against 12 case studies involving different
composite materials and sensor arrays. Statistical metrics were used to assess
domains alignment both before and after MPCA, and the results demonstrate a
substantial reduction in localisation error compared to standard TL techniques.
Hence, the proposed approach emerges as a robust, data-efficient, and
statistically based TL framework for UGW-based SHM.

</details>


### [280] [Spatial-Temporal-Spectral Mamba with Sparse Deformable Token Sequence for Enhanced MODIS Time Series Classification](https://arxiv.org/abs/2508.02839)
*Zack Dewis,Zhengsen Xu,Yimin Zhu,Motasem Alkayid,Mabel Heffring,Lincoln Linlin Xu*

Main category: eess.IV

TL;DR: 本文提出了一种新颖的空间-时间-光谱Mamba（STSMamba）模型，结合可变形令牌序列，用于增强MODIS时间序列数据的土地覆盖分类，解决了高维度、混合像素和耦合效应等挑战。


<details>
  <summary>Details</summary>
Motivation: MODIS时间序列数据对于动态、大规模土地覆盖分类至关重要，但由于其高时间维度、混合像素以及空间-时间-光谱耦合效应，难以捕捉细微的类别特征信息。

Method: 1. 设计了时间分组主干（TGS）模块，用于初始特征学习，以解耦时间-光谱特征耦合。2. 提出了稀疏可变形Mamba序列化（SDMS）方法，以减少Mamba序列中的信息冗余，提高Mamba建模的效率、适应性和可学习性。3. 基于SDMS，设计了新颖的空间-时间-光谱Mamba架构，包括稀疏可变形空间Mamba模块（SDSpaM）、稀疏可变形光谱Mamba模块（SDSpeM）和稀疏可变形时间Mamba模块（SDTM），以显式学习MODIS中的关键信息源。

Result: 与许多现有先进方法相比，所提出的方法在MODIS时间序列数据测试中取得了更高的分类精度，并降低了计算复杂度。

Conclusion: 所提出的STSMamba模型能够有效处理MODIS时间序列数据的复杂性，通过创新的模块设计实现更准确、更高效的土地覆盖分类。

Abstract: Although MODIS time series data are critical for supporting dynamic,
large-scale land cover land use classification, it is a challenging task to
capture the subtle class signature information due to key MODIS difficulties,
e.g., high temporal dimensionality, mixed pixels, and spatial-temporal-spectral
coupling effect. This paper presents a novel spatial-temporal-spectral Mamba
(STSMamba) with deformable token sequence for enhanced MODIS time series
classification, with the following key contributions. First, to disentangle
temporal-spectral feature coupling, a temporal grouped stem (TGS) module is
designed for initial feature learning. Second, to improve Mamba modeling
efficiency and accuracy, a sparse, deformable Mamba sequencing (SDMS) approach
is designed, which can reduce the potential information redundancy in Mamba
sequence and improve the adaptability and learnability of the Mamba sequencing.
Third, based on SDMS, to improve feature learning, a novel
spatial-temporal-spectral Mamba architecture is designed, leading to three
modules, i.e., a sparse deformable spatial Mamba module (SDSpaM), a sparse
deformable spectral Mamba module (SDSpeM), and a sparse deformable temporal
Mamba module (SDTM) to explicitly learn key information sources in MODIS. The
proposed approach is tested on MODIS time series data in comparison with many
state-of-the-art approaches, and the results demonstrate that the proposed
approach can achieve higher classification accuracy with reduced computational
complexity.

</details>


### [281] [Evaluation of 3D Counterfactual Brain MRI Generation](https://arxiv.org/abs/2508.02880)
*Pengwei Sun,Wei Peng,Lun Yu Li,Yixin Wang,Kilian M. Pohl*

Main category: eess.IV

TL;DR: 本文将六种生成模型转换为3D反事实方法，通过基于因果图的解剖引导框架生成逼真的大脑MRI，并对其进行基准测试，发现解剖学条件化能成功修改目标区域但难以保留非目标结构。


<details>
  <summary>Details</summary>
Motivation: 在医学影像中，反事实生成对于理解疾病机制和生成生理学上合理的数据具有潜力。然而，生成尊重解剖学和因果约束的真实3D大脑MRI具有挑战性，原因包括数据稀缺、结构复杂性以及缺乏标准化的评估协议。

Method: 将六种生成模型转换为3D反事实方法，方法是整合一个基于因果图的解剖引导框架，其中区域大脑体积作为直接条件输入。在ADNI的T1加权大脑MRI上，从构成、可逆性、真实性、有效性和最小性方面评估每个模型。此外，还在NCANDA的T1加权MRI上测试了模型的泛化能力。

Result: 解剖学上基于条件的生成成功修改了目标解剖区域；然而，它在保留非目标结构方面表现出局限性。

Conclusion: 这项工作为更具可解释性和临床相关性的大脑MRI生成建模奠定了基础，并强调了需要新的架构来更准确地捕捉解剖学上的相互依赖性。

Abstract: Counterfactual generation offers a principled framework for simulating
hypothetical changes in medical imaging, with potential applications in
understanding disease mechanisms and generating physiologically plausible data.
However, generating realistic structural 3D brain MRIs that respect anatomical
and causal constraints remains challenging due to data scarcity, structural
complexity, and the lack of standardized evaluation protocols. In this work, we
convert six generative models into 3D counterfactual approaches by
incorporating an anatomy-guided framework based on a causal graph, in which
regional brain volumes serve as direct conditioning inputs. Each model is
evaluated with respect to composition, reversibility, realism, effectiveness
and minimality on T1-weighted brain MRIs (T1w MRIs) from the Alzheimer's
Disease Neuroimaging Initiative (ADNI). In addition, we test the
generalizability of each model with respect to T1w MRIs of the National
Consortium on Alcohol and Neurodevelopment in Adolescence (NCANDA). Our results
indicate that anatomically grounded conditioning successfully modifies the
targeted anatomical regions; however, it exhibits limitations in preserving
non-targeted structures. Beyond laying the groundwork for more interpretable
and clinically relevant generative modeling of brain MRIs, this benchmark
highlights the need for novel architectures that more accurately capture
anatomical interdependencies.

</details>


### [282] [REFLECT: Rectified Flows for Efficient Brain Anomaly Correction Transport](https://arxiv.org/abs/2508.02889)
*Farzad Beizaee,Sina Hajimiri,Ismail Ben Ayed,Gregory Lodygensky,Christian Desrosiers,Jose Dolz*

Main category: eess.IV

TL;DR: REFLECT框架利用整流流实现脑部MR图像的单步直接校正，以进行无监督异常检测和精确异常定位，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 脑部图像无监督异常检测（UAD）在识别病理方面至关重要，但面临无标注数据、脑部解剖结构复杂和异常样本稀缺等挑战，导致异常定位困难。

Method: REFLECT框架引入整流流（rectified flows），学习一个直接、线性的单步传输映射，将异常MR图像校正到正常分布。通过比较异常输入和校正后的图像之间的差异来精确定位异常。与扩散模型不同，REFLECT实现单步推理。

Result: 在流行的UAD脑分割基准测试中，REFLECT的性能显著优于最先进的无监督异常检测方法。

Conclusion: REFLECT提供了一种基于整流流的高效、精确的单步无监督脑部异常检测和定位方法，超越了现有技术。

Abstract: Unsupervised anomaly detection (UAD) in brain imaging is crucial for
identifying pathologies without the need for labeled data. However, accurately
localizing anomalies remains challenging due to the intricate structure of
brain anatomy and the scarcity of abnormal examples. In this work, we introduce
REFLECT, a novel framework that leverages rectified flows to establish a
direct, linear trajectory for correcting abnormal MR images toward a normal
distribution. By learning a straight, one-step correction transport map, our
method efficiently corrects brain anomalies and can precisely localize
anomalies by detecting discrepancies between anomalous input and corrected
counterpart. In contrast to the diffusion-based UAD models, which require
iterative stochastic sampling, rectified flows provide a direct transport map,
enabling single-step inference. Extensive experiments on popular UAD brain
segmentation benchmarks demonstrate that REFLECT significantly outperforms
state-of-the-art unsupervised anomaly detection methods. The code is available
at https://github.com/farzad-bz/REFLECT.

</details>


### [283] [AMD-Mamba: A Phenotype-Aware Multi-Modal Framework for Robust AMD Prognosis](https://arxiv.org/abs/2508.02957)
*Puzhen Wu,Mingquan Lin,Qingyu Chen,Emily Y. Chew,Zhiyong Lu,Yifan Peng,Hexin Dong*

Main category: eess.IV

TL;DR: 该研究提出了AMD-Mamba，一个新颖的多模态框架，用于年龄相关性黄斑变性（AMD）的预后，并开发了一种新的AMD生物标志物。该框架整合了眼底图像、遗传变异和社会人口学变量，利用度量学习和Vision Mamba模型，显著提高了早期高风险AMD患者的检测能力。


<details>
  <summary>Details</summary>
Motivation: 年龄相关性黄斑变性（AMD）是导致不可逆视力丧失的主要原因，因此有效的预后对于及时干预至关重要。现有模型可能未能充分捕捉疾病进展模式，或仅关注局部信息。

Method: 本研究提出了AMD-Mamba多模态框架，整合了彩色眼底图像、基因变异和社会人口学变量。核心方法包括：1) 引入创新的度量学习策略，利用AMD严重程度评分作为先验知识，使学习到的特征与临床表型对齐；2) 应用Vision Mamba模型，同时融合局部和长程全局信息，区别于传统CNN只关注局部信息；3) 通过多尺度融合，结合不同分辨率的图像信息和临床变量。该模型在AREDS数据集上进行了评估。

Result: 实验结果表明，所提出的生物标志物是AMD进展最重要的生物标志物之一。将此生物标志物与其他现有变量结合，在早期检测高风险AMD患者方面取得了显著改善。

Conclusion: 该多模态框架及其新生物标志物具有潜力，能够促进更精确和主动的AMD管理。

Abstract: Age-related macular degeneration (AMD) is a leading cause of irreversible
vision loss, making effective prognosis crucial for timely intervention. In
this work, we propose AMD-Mamba, a novel multi-modal framework for AMD
prognosis, and further develop a new AMD biomarker. This framework integrates
color fundus images with genetic variants and socio-demographic variables. At
its core, AMD-Mamba introduces an innovative metric learning strategy that
leverages AMD severity scale score as prior knowledge. This strategy allows the
model to learn richer feature representations by aligning learned features with
clinical phenotypes, thereby improving the capability of conventional prognosis
methods in capturing disease progression patterns. In addition, unlike existing
models that use traditional CNN backbones and focus primarily on local
information, such as the presence of drusen, AMD-Mamba applies Vision Mamba and
simultaneously fuses local and long-range global information, such as vascular
changes. Furthermore, we enhance prediction performance through multi-scale
fusion, combining image information with clinical variables at different
resolutions. We evaluate AMD-Mamba on the AREDS dataset, which includes 45,818
color fundus photographs, 52 genetic variants, and 3 socio-demographic
variables from 2,741 subjects. Our experimental results demonstrate that our
proposed biomarker is one of the most significant biomarkers for the
progression of AMD. Notably, combining this biomarker with other existing
variables yields promising improvements in detecting high-risk AMD patients at
early stages. These findings highlight the potential of our multi-modal
framework to facilitate more precise and proactive management of AMD.

</details>


### [284] [ClinicalFMamba: Advancing Clinical Assessment using Mamba-based Multimodal Neuroimaging Fusion](https://arxiv.org/abs/2508.03008)
*Meng Zhou,Farzad Khalvati*

Main category: eess.IV

TL;DR: 本文提出了一种名为ClinicalFMamba的新型CNN-Mamba混合架构，用于2D和3D多模态医学图像融合。该方法结合了局部和全局特征建模，并通过三平面扫描策略处理3D数据，实现了卓越的实时融合性能，并在临床验证中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在多模态医学图像融合中存在局限：CNN擅长局部特征但缺乏全局上下文，Transformer能建模长程依赖但计算复杂度高，限制了临床应用。State Space Models (SSMs) 提供高效长程依赖建模，但其在3D数据上的扩展和临床验证尚待深入探索。

Method: 提出了ClinicalFMamba，一种端到端的CNN-Mamba混合架构，协同结合局部和全局特征建模，适用于2D和3D图像。为有效学习3D图像中的体积依赖性，设计了三平面扫描策略。

Result: 在三个数据集上的综合评估表明，该方法在多个定量指标上实现了卓越的融合性能，并达到了实时融合。在下游的2D/3D脑肿瘤分类任务中，该方法也优于基线方法，验证了其临床实用性。

Conclusion: 本文提出的方法为高效多模态医学图像融合建立了新范式，适用于实时临床部署。

Abstract: Multimodal medical image fusion integrates complementary information from
different imaging modalities to enhance diagnostic accuracy and treatment
planning. While deep learning methods have advanced performance, existing
approaches face critical limitations: Convolutional Neural Networks (CNNs)
excel at local feature extraction but struggle to model global context
effectively, while Transformers achieve superior long-range modeling at the
cost of quadratic computational complexity, limiting clinical deployment.
Recent State Space Models (SSMs) offer a promising alternative, enabling
efficient long-range dependency modeling in linear time through selective scan
mechanisms. Despite these advances, the extension to 3D volumetric data and the
clinical validation of fused images remains underexplored. In this work, we
propose ClinicalFMamba, a novel end-to-end CNN-Mamba hybrid architecture that
synergistically combines local and global feature modeling for 2D and 3D
images. We further design a tri-plane scanning strategy for effectively
learning volumetric dependencies in 3D images. Comprehensive evaluations on
three datasets demonstrate the superior fusion performance across multiple
quantitative metrics while achieving real-time fusion. We further validate the
clinical utility of our approach on downstream 2D/3D brain tumor classification
tasks, achieving superior performance over baseline methods. Our method
establishes a new paradigm for efficient multimodal medical image fusion
suitable for real-time clinical deployment.

</details>


### [285] [A Survey of Medical Point Cloud Shape Learning: Registration, Reconstruction and Variation](https://arxiv.org/abs/2508.03057)
*Tongxu Zhang,Zhiming Liang,Bei Wang*

Main category: eess.IV

TL;DR: 本文综述了2021年至2025年间基于深度学习的医学点云形状分析研究，重点关注配准、重建和变异建模，并讨论了挑战、趋势及未来方向。


<details>
  <summary>Details</summary>
Motivation: 点云作为一种紧凑且保留表面的3D医学图像表示方式日益重要，深度学习的进展推动了其在解剖形状分析中的应用。因此，需要对医学点云的形状分析进行全面系统的综述。

Method: 本文采用系统文献综述的方法，审查了2021年至2025年间的相关文献，总结了医学点云形状分析中配准、重建和变异建模这三个基本任务的代表性方法、数据集、评估指标、临床应用及独特挑战。

Result: 综述总结了该领域的方法、数据集和评估指标，并强调了临床应用和特有挑战（如数据稀缺、患者间变异性、对可解释和鲁棒解决方案的需求）。识别出关键趋势包括混合表示集成、大规模自监督模型和生成技术。

Conclusion: 文章讨论了当前局限性，并为推进医学影像中基于点云的形状学习指明了未来的发展方向。

Abstract: Point clouds have become an increasingly important representation for 3D
medical imaging, offering a compact, surface-preserving alternative to
traditional voxel or mesh-based approaches. Recent advances in deep learning
have enabled rapid progress in extracting, modeling, and analyzing anatomical
shapes directly from point cloud data. This paper provides a comprehensive and
systematic survey of learning-based shape analysis for medical point clouds,
focusing on three fundamental tasks: registration, reconstruction, and
variation modeling. We review recent literature from 2021 to 2025, summarize
representative methods, datasets, and evaluation metrics, and highlight
clinical applications and unique challenges in the medical domain. Key trends
include the integration of hybrid representations, large-scale self-supervised
models, and generative techniques. We also discuss current limitations, such as
data scarcity, inter-patient variability, and the need for interpretable and
robust solutions for clinical deployment. Finally, future directions are
outlined for advancing point cloud-based shape learning in medical imaging.

</details>


### [286] [Nexus-INR: Diverse Knowledge-guided Arbitrary-Scale Multimodal Medical Image Super-Resolution](https://arxiv.org/abs/2508.03073)
*Bo Zhang,JianFei Huo,Zheng Zhang,Wufan Wang,Hui Gao,Xiangyang Gong,Wendong Wang*

Main category: eess.IV

TL;DR: Nexus-INR是一个针对医学图像的任意分辨率超分辨率（ARSR）框架，通过整合多模态信息和下游任务，解决了传统方法在处理变分辨率和多模态图像时的局限性，并超越了现有技术。


<details>
  <summary>Details</summary>
Motivation: 传统基于CNN的超分辨率方法通常设计用于固定上采样因子，不适用于需要适应不同空间分辨率的任意分辨率超分辨率（ARSR）。虽然基于INR的方法克服了这一限制，但它们在有效处理和利用具有不同分辨率和细节的多模态图像方面仍存在困难。

Method: 本文提出了Nexus-INR框架，一个多样知识引导的ARSR框架，包含三个关键组件：1) 一个带有辅助分类任务的双分支编码器，用于有效解耦共享解剖结构和模态特定特征；2) 一个知识蒸馏模块，使用跨模态注意力以高分辨率参考指导低分辨率模态重建，并通过自监督一致性损失增强；3) 一个集成分割模块，嵌入解剖语义以改善重建质量和下游分割性能。

Result: 在BraTS2020数据集上进行的超分辨率和下游分割实验表明，Nexus-INR在各种指标上均优于现有最先进的方法。

Conclusion: Nexus-INR通过其独特的三组件设计，有效解决了医学图像ARSR中多模态和变分辨率处理的挑战，不仅提升了图像重建质量，也改善了下游分割任务的表现，为医学图像分析提供了关键的灵活性。

Abstract: Arbitrary-resolution super-resolution (ARSR) provides crucial flexibility for
medical image analysis by adapting to diverse spatial resolutions. However,
traditional CNN-based methods are inherently ill-suited for ARSR, as they are
typically designed for fixed upsampling factors. While INR-based methods
overcome this limitation, they still struggle to effectively process and
leverage multi-modal images with varying resolutions and details. In this
paper, we propose Nexus-INR, a Diverse Knowledge-guided ARSR framework, which
employs varied information and downstream tasks to achieve high-quality,
adaptive-resolution medical image super-resolution. Specifically, Nexus-INR
contains three key components. A dual-branch encoder with an auxiliary
classification task to effectively disentangle shared anatomical structures and
modality-specific features; a knowledge distillation module using cross-modal
attention that guides low-resolution modality reconstruction with
high-resolution reference, enhanced by self-supervised consistency loss; an
integrated segmentation module that embeds anatomical semantics to improve both
reconstruction quality and downstream segmentation performance. Experiments on
the BraTS2020 dataset for both super-resolution and downstream segmentation
demonstrate that Nexus-INR outperforms state-of-the-art methods across various
metrics.

</details>


### [287] [GL-LCM: Global-Local Latent Consistency Models for Fast High-Resolution Bone Suppression in Chest X-Ray Images](https://arxiv.org/abs/2508.03357)
*Yifei Sun,Zhanghao Chen,Hao Zheng,Yuqing Lu,Lixin Duan,Fenglei Fan,Ahmed Elazab,Xiang Wan,Changmiao Wang,Ruiquan Ge*

Main category: eess.IV

TL;DR: 本文提出了一种名为GL-LCM的全局-局部潜在一致性模型，用于在胸部X光片中快速高效地抑制骨骼结构，同时保留细节，以提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 胸部X光片中的骨骼结构会遮挡关键诊断细节。现有基于扩散模型的骨骼抑制方法难以在彻底抑制骨骼和保留局部纹理细节之间取得平衡，且计算成本高、处理时间长，阻碍了其在临床中的实际应用。

Method: 提出了一个全局-局部潜在一致性模型（GL-LCM）架构。该模型结合了肺部分割、双路径采样和全局-局部融合，实现了胸部X光片中快速高分辨率的骨骼抑制。为解决局部路径采样中潜在的边界伪影和细节模糊问题，进一步提出了局部增强引导（Local-Enhanced Guidance），无需额外训练即可解决这些问题。

Result: 在自收集数据集SZCH-X-Rays和公共数据集JSRT上的综合实验表明，GL-LCM在骨骼抑制和计算效率方面均表现出色，显著优于其他竞争方法。

Conclusion: GL-LCM模型成功解决了现有胸部X光片骨骼抑制方法的局限性，实现了快速、高分辨率的骨骼抑制，同时有效保留了图像细节，具有显著的临床应用潜力。

Abstract: Chest X-Ray (CXR) imaging for pulmonary diagnosis raises significant
challenges, primarily because bone structures can obscure critical details
necessary for accurate diagnosis. Recent advances in deep learning,
particularly with diffusion models, offer significant promise for effectively
minimizing the visibility of bone structures in CXR images, thereby improving
clarity and diagnostic accuracy. Nevertheless, existing diffusion-based methods
for bone suppression in CXR imaging struggle to balance the complete
suppression of bones with preserving local texture details. Additionally, their
high computational demand and extended processing time hinder their practical
use in clinical settings. To address these limitations, we introduce a
Global-Local Latent Consistency Model (GL-LCM) architecture. This model
combines lung segmentation, dual-path sampling, and global-local fusion,
enabling fast high-resolution bone suppression in CXR images. To tackle
potential boundary artifacts and detail blurring in local-path sampling, we
further propose Local-Enhanced Guidance, which addresses these issues without
additional training. Comprehensive experiments on a self-collected dataset
SZCH-X-Rays, and the public dataset JSRT, reveal that our GL-LCM delivers
superior bone suppression and remarkable computational efficiency,
significantly outperforming several competitive methods. Our code is available
at https://github.com/diaoquesang/GL-LCM.

</details>


### [288] [Evaluating the Predictive Value of Preoperative MRI for Erectile Dysfunction Following Radical Prostatectomy](https://arxiv.org/abs/2508.03461)
*Gideon N. L. Rouwendaal,Daniël Boeke,Inge L. Cox,Henk G. van der Poel,Margriet C. van Dijk-de Haan,Regina G. H. Beets-Tan,Thierry N. Boellaard,Wilson Silva*

Main category: eess.IV

TL;DR: 研究发现，术前MRI在预测根治性前列腺切除术后勃起功能障碍方面，未能超越现有临床特征的预测能力，但MRI模型确实关注了相关的解剖区域。


<details>
  <summary>Details</summary>
Motivation: 准确的术前预测勃起功能障碍（ED）对接受根治性前列腺切除术的患者咨询至关重要。尽管临床特征已是既定预测因子，但术前MRI的额外价值尚未得到充分探索。

Method: 研究评估了四种建模策略来预测术后12个月的ED：1) 仅基于临床特征的基线模型；2) 使用手工提取的MRI解剖特征的经典模型；3) 直接在MRI切片上训练的深度学习模型；4) 影像和临床输入的多模态融合模型。通过AUC、SHAP分析和显著性图评估模型性能。

Result: 影像学模型（最大AUC 0.569）略优于手工解剖特征方法（AUC 0.554），但均低于临床基线模型（AUC 0.663）。融合模型仅带来微小增益（AUC 0.586），未能超越仅临床特征的表现。SHAP分析证实临床特征对预测性能贡献最大。表现最佳的影像学模型的显著性图显示，其主要关注前列腺和神经血管束等解剖学上合理的区域。

Conclusion: 尽管基于MRI的模型未能提升对临床特征的预测性能，但研究结果表明它们试图捕捉相关解剖结构中的模式，未来可能在多模态方法中补充临床预测因子。

Abstract: Accurate preoperative prediction of erectile dysfunction (ED) is important
for counseling patients undergoing radical prostatectomy. While clinical
features are established predictors, the added value of preoperative MRI
remains underexplored. We investigate whether MRI provides additional
predictive value for ED at 12 months post-surgery, evaluating four modeling
strategies: (1) a clinical-only baseline, representing current
state-of-the-art; (2) classical models using handcrafted anatomical features
derived from MRI; (3) deep learning models trained directly on MRI slices; and
(4) multimodal fusion of imaging and clinical inputs. Imaging-based models
(maximum AUC 0.569) slightly outperformed handcrafted anatomical approaches
(AUC 0.554) but fell short of the clinical baseline (AUC 0.663). Fusion models
offered marginal gains (AUC 0.586) but did not exceed clinical-only
performance. SHAP analysis confirmed that clinical features contributed most to
predictive performance. Saliency maps from the best-performing imaging model
suggested a predominant focus on anatomically plausible regions, such as the
prostate and neurovascular bundles. While MRI-based models did not improve
predictive performance over clinical features, our findings suggest that they
try to capture patterns in relevant anatomical structures and may complement
clinical predictors in future multimodal approaches.

</details>


### [289] [CADD: Context aware disease deviations via restoration of brain images using normative conditional diffusion models](https://arxiv.org/abs/2508.03594)
*Ana Lawry Aguila,Ayodeji Ijishakin,Juan Eugenio Iglesias,Tomomi Takenaga,Yukihiro Nomura,Takeharu Yoshikawa,Osamu Abe,Shouhei Hanaoka*

Main category: eess.IV

TL;DR: CADD是首个用于3D图像规范建模的条件扩散模型，通过新颖的推理修复策略，在异构临床数据中实现了神经异常检测的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 将机器学习应用于真实世界医疗数据（如医院档案）以检测脑部疾病具有巨大潜力，但异构队列中的病理检测极具挑战。现有扩散模型在异常检测中未能整合临床信息，且对健康区域的修复不佳，导致重建质量和检测性能受限。

Method: 本文提出了CADD，首个用于3D图像规范建模的条件扩散模型。为指导健康区域的修复过程，CADD引入了一种新颖的推理修复策略，旨在平衡异常去除与保留受试者特定特征。

Result: CADD在三个具有挑战性的数据集（包括可能存在低对比度、厚切片和运动伪影的临床扫描）上进行了评估，在异构队列中检测神经异常方面取得了最先进的性能。

Conclusion: CADD通过结合条件扩散模型和优化的推理修复策略，有效解决了真实世界异构医学数据中神经异常检测的难题，显著提升了检测精度。

Abstract: Applying machine learning to real-world medical data, e.g. from hospital
archives, has the potential to revolutionize disease detection in brain images.
However, detecting pathology in such heterogeneous cohorts is a difficult
challenge. Normative modeling, a form of unsupervised anomaly detection, offers
a promising approach to studying such cohorts where the ``normal'' behavior is
modeled and can be used at subject level to detect deviations relating to
disease pathology. Diffusion models have emerged as powerful tools for anomaly
detection due to their ability to capture complex data distributions and
generate high-quality images. Their performance relies on image restoration;
differences between the original and restored images highlight potential
abnormalities. However, unlike normative models, these diffusion model
approaches do not incorporate clinical information which provides important
context to guide the disease detection process. Furthermore, standard
approaches often poorly restore healthy regions, resulting in poor
reconstructions and suboptimal detection performance. We present CADD, the
first conditional diffusion model for normative modeling in 3D images. To guide
the healthy restoration process, we propose a novel inference inpainting
strategy which balances anomaly removal with retention of subject-specific
features. Evaluated on three challenging datasets, including clinical scans,
which may have lower contrast, thicker slices, and motion artifacts, CADD
achieves state-of-the-art performance in detecting neurological abnormalities
in heterogeneous cohorts.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [290] [READ: Real-time and Efficient Asynchronous Diffusion for Audio-driven Talking Head Generation](https://arxiv.org/abs/2508.03457)
*Haotian Wang,Yuzhe Weng,Jun Du,Haoran Xu,Xiaoyan Wu,Shan He,Bing Yin,Cong Liu,Jianqing Gao,Qingfeng Liu*

Main category: cs.GR

TL;DR: 本文提出了READ，首个基于扩散-Transformer的实时音视频驱动说话人头部生成框架，通过压缩潜在空间和异步噪声调度器显著提升了推理速度和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在音视频驱动说话人头部生成方面取得了显著进展，但其极慢的推理速度严重限制了实际应用。

Method: 1. 通过时序VAE学习时空高度压缩的视频潜在空间，以减少token数量并加速生成。2. 提出预训练的语音自编码器（SpeechAE）生成与视频潜在空间对应的时序压缩语音潜在编码。3. 设计了音频到视频扩散Transformer（A2V-DiT）骨干网络来建模这些潜在表示。4. 提出了一种新颖的异步噪声调度器（ANS），用于训练和推理过程，通过异步加噪和异步运动引导生成确保视频片段的时序一致性并加速推理。

Result: 实验结果表明，READ在生成具有竞争力的说话人头部视频方面优于现有最先进的方法，显著缩短了运行时间，在质量和速度之间取得了最佳平衡，并能长时间生成并保持鲁棒的度量稳定性。

Conclusion: READ成功实现了实时音视频驱动说话人头部生成，在保持高质量的同时显著提升了推理速度，为该领域的实际应用提供了可行方案。

Abstract: The introduction of diffusion models has brought significant advances to the
field of audio-driven talking head generation. However, the extremely slow
inference speed severely limits the practical implementation of diffusion-based
talking head generation models. In this study, we propose READ, the first
real-time diffusion-transformer-based talking head generation framework. Our
approach first learns a spatiotemporal highly compressed video latent space via
a temporal VAE, significantly reducing the token count to accelerate
generation. To achieve better audio-visual alignment within this compressed
latent space, a pre-trained Speech Autoencoder (SpeechAE) is proposed to
generate temporally compressed speech latent codes corresponding to the video
latent space. These latent representations are then modeled by a carefully
designed Audio-to-Video Diffusion Transformer (A2V-DiT) backbone for efficient
talking head synthesis. Furthermore, to ensure temporal consistency and
accelerated inference in extended generation, we propose a novel asynchronous
noise scheduler (ANS) for both the training and inference process of our
framework. The ANS leverages asynchronous add-noise and asynchronous
motion-guided generation in the latent space, ensuring consistency in generated
video clips. Experimental results demonstrate that READ outperforms
state-of-the-art methods by generating competitive talking head videos with
significantly reduced runtime, achieving an optimal balance between quality and
speed while maintaining robust metric stability in long-time generation.

</details>
