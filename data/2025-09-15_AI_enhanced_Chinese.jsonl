{"id": "2509.09720", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.09720", "abs": "https://arxiv.org/abs/2509.09720", "authors": ["Akansel Cosgun", "Lachlan Chumbley", "Benjamin J. Meyer"], "title": "Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision", "comment": null, "summary": "This paper introduces the Australian Supermarket Object Set (ASOS), a\ncomprehensive dataset comprising 50 readily available supermarket items with\nhigh-quality 3D textured meshes designed for benchmarking in robotics and\ncomputer vision applications. Unlike existing datasets that rely on synthetic\nmodels or specialized objects with limited accessibility, ASOS provides a\ncost-effective collection of common household items that can be sourced from a\nmajor Australian supermarket chain. The dataset spans 10 distinct categories\nwith diverse shapes, sizes, and weights. 3D meshes are acquired by a\nstructure-from-motion techniques with high-resolution imaging to generate\nwatertight meshes. The dataset's emphasis on accessibility and real-world\napplicability makes it valuable for benchmarking object detection, pose\nestimation, and robotics applications.", "AI": {"tldr": "本文介绍了澳大利亚超市物品集（ASOS），一个包含50种常见超市商品的综合数据集，提供高质量的3D纹理网格，用于机器人和计算机视觉应用的基准测试。", "motivation": "现有数据集依赖于合成模型或专业对象，可访问性有限，缺乏真实世界的适用性，无法满足对常见物品进行基准测试的需求。", "method": "ASOS包含50种来自10个不同类别的常见超市商品。通过运动结构（structure-from-motion）技术结合高分辨率成像，获取高质量的3D纹理网格，生成水密网格。", "result": "ASOS提供了一个经济实惠、易于获取的常见家庭用品集合，具有高质量的3D纹理网格，适用于物体检测、姿态估计和机器人应用的基准测试。", "conclusion": "ASOS数据集强调可访问性和真实世界适用性，使其在机器人和计算机视觉领域的基准测试中具有重要价值。"}}
{"id": "2509.09721", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09721", "abs": "https://arxiv.org/abs/2509.09721", "authors": ["Jiayi Miao", "Dingxin Lu", "Zhuqi Wang"], "title": "A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval", "comment": null, "summary": "After natural disasters, accurate evaluations of damage to housing are\nimportant for insurance claims response and planning of resources. In this\nwork, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)\nframework. On top of classical RAG architecture, we further the framework to\ndevise a two-branch multimodal encoder structure that the image branch employs\na visual encoder composed of ResNet and Transformer to extract the\ncharacteristic of building damage after disaster, and the text branch harnesses\na BERT retriever for the text vectorization of posts as well as insurance\npolicies and for the construction of a retrievable restoration index. To impose\ncross-modal semantic alignment, the model integrates a cross-modal interaction\nmodule to bridge the semantic representation between image and text via\nmulti-head attention. Meanwhile, in the generation module, the introduced modal\nattention gating mechanism dynamically controls the role of visual evidence and\ntext prior information during generation. The entire framework takes end-to-end\ntraining, and combines the comparison loss, the retrieval loss and the\ngeneration loss to form multi-task optimization objectives, and achieves image\nunderstanding and policy matching in collaborative learning. The results\ndemonstrate superior performance in retrieval accuracy and classification index\non damage severity, where the Top-1 retrieval accuracy has been improved by\n9.6%.", "AI": {"tldr": "本文提出了一种新颖的多模态检索增强生成（MM-RAG）框架，用于自然灾害后房屋损害评估。该框架通过双分支编码器、跨模态交互和模态注意力门控机制，实现图像理解与保单匹配的协同学习，显著提高了检索准确性和损害严重性分类性能。", "motivation": "自然灾害后对房屋损害进行准确评估对于保险理赔响应和资源规划至关重要。", "method": "该研究引入了一个MM-RAG框架。它在经典RAG架构基础上，设计了一个双分支多模态编码器：图像分支采用ResNet和Transformer提取建筑损害特征；文本分支利用BERT检索器对帖子和保单进行文本向量化并构建可检索的修复索引。模型集成了一个跨模态交互模块（通过多头注意力）实现图像和文本的语义对齐。生成模块中，引入模态注意力门控机制动态控制视觉证据和文本先验信息。整个框架进行端到端训练，结合比较损失、检索损失和生成损失形成多任务优化目标，实现图像理解和保单匹配的协同学习。", "result": "结果表明，该框架在损害严重性的检索准确性和分类指标上表现优异，其中Top-1检索准确率提高了9.6%。", "conclusion": "所提出的MM-RAG框架通过整合多模态数据，有效提升了灾后房屋损害评估的准确性，在检索和损害严重性分类方面取得了显著改进，对保险和资源分配具有重要意义。"}}
{"id": "2509.09722", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09722", "abs": "https://arxiv.org/abs/2509.09722", "authors": ["Taylor Archibald", "Tony Martinez"], "title": "Improving MLLM Historical Record Extraction with Test-Time Image", "comment": null, "summary": "We present a novel ensemble framework that stabilizes LLM based text\nextraction from noisy historical documents. We transcribe multiple augmented\nvariants of each image with Gemini 2.0 Flash and fuse these outputs with a\ncustom Needleman Wunsch style aligner that yields both a consensus\ntranscription and a confidence score. We present a new dataset of 622\nPennsylvania death records, and demonstrate our method improves transcription\naccuracy by 4 percentage points relative to a single shot baseline. We find\nthat padding and blurring are the most useful for improving accuracy, while\ngrid warp perturbations are best for separating high and low confidence cases.\nThe approach is simple, scalable, and immediately deployable to other document\ncollections and transcription models.", "AI": {"tldr": "本文提出了一种新颖的集成框架，通过多重增强、LLM转录和自定义对齐器融合，稳定了从嘈杂历史文档中进行文本提取的过程，显著提高了转录准确性。", "motivation": "从嘈杂的历史文档中进行基于大型语言模型（LLM）的文本提取存在稳定性问题。", "method": "该方法包括：1) 使用Gemini 2.0 Flash转录每个图像的多个增强变体；2) 使用自定义的Needleman-Wunsch风格对齐器融合这些输出，以生成共识转录和置信度分数；3) 构建了一个包含622份宾夕法尼亚州死亡记录的新数据集进行评估。", "result": "相对于单次基线，该方法将转录准确性提高了4个百分点。发现填充和模糊对提高准确性最有效，而网格变形扰动最适合区分高置信度和低置信度情况。", "conclusion": "所提出的方法简单、可扩展，并且可以立即部署到其他文档集合和转录模型中。"}}
{"id": "2509.09730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09730", "abs": "https://arxiv.org/abs/2509.09730", "authors": ["Kaikai Zhao", "Zhaoxiang Liu", "Peng Wang", "Xin Wang", "Zhicheng Ma", "Yajun Xu", "Wenjing Zhang", "Yibing Nan", "Kai Wang", "Shiguo Lian"], "title": "MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance", "comment": "accepted by Image and Vision Computing", "summary": "General-domain large multimodal models (LMMs) have achieved significant\nadvances in various image-text tasks. However, their performance in the\nIntelligent Traffic Surveillance (ITS) domain remains limited due to the\nabsence of dedicated multimodal datasets. To address this gap, we introduce\nMITS (Multimodal Intelligent Traffic Surveillance), the first large-scale\nmultimodal benchmark dataset specifically designed for ITS. MITS includes\n170,400 independently collected real-world ITS images sourced from traffic\nsurveillance cameras, annotated with eight main categories and 24 subcategories\nof ITS-specific objects and events under diverse environmental conditions.\nAdditionally, through a systematic data generation pipeline, we generate\nhigh-quality image captions and 5 million instruction-following visual\nquestion-answer pairs, addressing five critical ITS tasks: object and event\nrecognition, object counting, object localization, background analysis, and\nevent reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream\nLMMs on this dataset, enabling the development of ITS-specific applications.\nExperimental results show that MITS significantly improves LMM performance in\nITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905\n(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to\n0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the\ndataset, code, and models as open-source, providing high-value resources to\nadvance both ITS and LMM research.", "AI": {"tldr": "本文介绍了MITS，首个大规模多模态智能交通监控（ITS）基准数据集，旨在解决通用大型多模态模型（LMMs）在ITS领域性能受限的问题。MITS包含真实交通监控图像和丰富的ITS特定标注，通过微调主流LMMs显著提升了它们在ITS应用中的表现。", "motivation": "尽管通用大型多模态模型（LMMs）在多种图像-文本任务中取得了显著进展，但由于缺乏专用的多模态数据集，它们在智能交通监控（ITS）领域的性能仍然有限。为了弥补这一空白，研究旨在创建一个专门针对ITS领域的大规模多模态基准数据集。", "method": "研究引入了MITS数据集，其中包含170,400张独立收集的真实世界ITS图像，这些图像来自交通监控摄像头，并标注了八个主要类别和24个子类别的ITS特定物体和事件。此外，通过系统的数据生成流程，研究生成了高质量的图像标题和500万个指令遵循的视觉问答对，涵盖了物体和事件识别、物体计数、物体定位、背景分析和事件推理这五项关键ITS任务。为验证MITS的有效性，研究人员使用该数据集对主流LMMs进行了微调。", "result": "实验结果表明，MITS显著提升了LMMs在ITS应用中的性能。例如，LLaVA-1.5的性能从0.494提升至0.905（增长83.2%），LLaVA-1.6从0.678提升至0.921（增长35.8%），Qwen2-VL从0.584提升至0.926（增长58.6%），Qwen2.5-VL从0.732提升至0.930（增长27.0%）。", "conclusion": "MITS数据集及其相关工作有效地解决了LMMs在ITS领域性能受限的问题，显著提升了主流LMMs在ITS应用中的表现。研究人员已将数据集、代码和模型开源，为ITS和LMM研究提供了宝贵的资源，有助于推动这两个领域的进一步发展。"}}
{"id": "2509.09769", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09769", "abs": "https://arxiv.org/abs/2509.09769", "authors": ["Rutav Shah", "Shuijing Liu", "Qi Wang", "Zhenyu Jiang", "Sateesh Kumar", "Mingyo Seo", "Roberto Martín-Martín", "Yuke Zhu"], "title": "MimicDroid: In-Context Learning for Humanoid Robot Manipulation from Human Play Videos", "comment": "11 pages, 9 figures, 5 tables", "summary": "We aim to enable humanoid robots to efficiently solve new manipulation tasks\nfrom a few video examples. In-context learning (ICL) is a promising framework\nfor achieving this goal due to its test-time data efficiency and rapid\nadaptability. However, current ICL methods rely on labor-intensive teleoperated\ndata for training, which restricts scalability. We propose using human play\nvideos -- continuous, unlabeled videos of people interacting freely with their\nenvironment -- as a scalable and diverse training data source. We introduce\nMimicDroid, which enables humanoids to perform ICL using human play videos as\nthe only training data. MimicDroid extracts trajectory pairs with similar\nmanipulation behaviors and trains the policy to predict the actions of one\ntrajectory conditioned on the other. Through this process, the model acquired\nICL capabilities for adapting to novel objects and environments at test time.\nTo bridge the embodiment gap, MimicDroid first retargets human wrist poses\nestimated from RGB videos to the humanoid, leveraging kinematic similarity. It\nalso applies random patch masking during training to reduce overfitting to\nhuman-specific cues and improve robustness to visual differences. To evaluate\nfew-shot learning for humanoids, we introduce an open-source simulation\nbenchmark with increasing levels of generalization difficulty. MimicDroid\noutperformed state-of-the-art methods and achieved nearly twofold higher\nsuccess rates in the real world. Additional materials can be found on:\nut-austin-rpl.github.io/MimicDroid", "AI": {"tldr": "MimicDroid使人形机器人能够利用人类自由玩耍视频（而非昂贵的遥操作数据）进行上下文学习，从而高效解决新的操作任务，并展现出强大的泛化能力。", "motivation": "现有上下文学习（ICL）方法依赖于耗时费力的遥操作数据进行训练，限制了其可扩展性。研究旨在使人形机器人能从少量视频示例中高效学习并解决新的操作任务。", "method": "提出MimicDroid框架，以人类自由玩耍视频作为唯一训练数据源。MimicDroid通过提取具有相似操作行为的轨迹对，训练策略以在给定一条轨迹的条件下预测另一条轨迹的动作。为弥合具身差距，它将从RGB视频中估计的人类手腕姿态重定向到人形机器人，并应用随机补丁遮蔽以减少对人类特定线索的过拟合，提高视觉差异下的鲁棒性。同时，引入了一个开源仿真基准来评估少样本学习能力。", "result": "MimicDroid在测试时获得了适应新物体和环境的ICL能力。在仿真中超越了现有最先进方法，并在真实世界中实现了近两倍的成功率。", "conclusion": "MimicDroid证明了利用人类自由玩耍视频作为训练数据，可以有效赋予人形机器人上下文学习能力，使其能够从少量示例中高效解决新的操作任务，并展现出卓越的泛化性能。"}}
{"id": "2509.09777", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.09777", "abs": "https://arxiv.org/abs/2509.09777", "authors": ["Alexander Von Moll", "Dipankar Maity", "Meir Pachter", "Daigo Shishika", "Michael Dorothy"], "title": "Target Defense Using a Turret and Mobile Defender Team", "comment": "Submitted to IEEE L-CSS and the 2026 ACC", "summary": "A scenario is considered wherein a stationary, turn constrained agent\n(Turret) and a mobile agent (Defender) cooperate to protect the former from an\nadversarial mobile agent (Attacker). The Attacker wishes to reach the Turret\nprior to getting captured by either the Defender or Turret, if possible.\nMeanwhile, the Defender and Turret seek to capture the Attacker as far from the\nTurret as possible. This scenario is formulated as a differential game and\nsolved using a geometric approach. Necessary and sufficient conditions for the\nTurret-Defender team winning and the Attacker winning are given. In the case of\nthe Turret-Defender team winning equilibrium strategies for the min max\nterminal distance of the Attacker to the Turret are given. Three cases arise\ncorresponding to solo capture by the Defender, solo capture by the Turret, and\ncapture simultaneously by both Turret and Defender.", "AI": {"tldr": "本文研究了一个差分博弈场景：一个固定炮塔（Turret）和一个移动防御者（Defender）合作，以保护炮塔免受移动攻击者（Attacker）的侵害。研究给出了双方获胜的充要条件和均衡策略。", "motivation": "研究的动机是分析一个防御系统，其中一个受转向限制的固定炮塔和一个移动防御者需要协同工作，以防止一个移动攻击者在被捕获前到达炮塔。防御方旨在尽可能远离炮塔捕获攻击者，而攻击者则试图在被捕获前到达炮塔。", "method": "该场景被建模为一个差分博弈（differential game），并采用几何方法（geometric approach）进行求解。", "result": "研究给出了炮塔-防御者团队获胜和攻击者获胜的充要条件。在炮塔-防御者团队获胜的情况下，还给出了使攻击者到炮塔的最终距离最小最大化的均衡策略。捕获情况分为三种：防御者单独捕获、炮塔单独捕获以及炮塔和防御者同时捕获。", "conclusion": "该研究成功地利用差分博弈和几何方法分析了固定炮塔、移动防御者与移动攻击者之间的对抗场景，明确了各方获胜的条件，并为防御方提供了最大化捕获距离的均衡策略，区分了不同的捕获模式。"}}
{"id": "2509.09880", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2509.09880", "abs": "https://arxiv.org/abs/2509.09880", "authors": ["Yaşar Utku Alçalar", "Junno Yun", "Mehmet Akçakaya"], "title": "Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining", "comment": "IEEE International Workshop on Computational Advances in Multi-Sensor\n  Adaptive Processing (CAMSAP), 2025", "summary": "Diffusion/score-based models have recently emerged as powerful generative\npriors for solving inverse problems, including accelerated MRI reconstruction.\nWhile their flexibility allows decoupling the measurement model from the\nlearned prior, their performance heavily depends on carefully tuned data\nfidelity weights, especially under fast sampling schedules with few denoising\nsteps. Existing approaches often rely on heuristics or fixed weights, which\nfail to generalize across varying measurement conditions and irregular timestep\nschedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling\n(ZADS), a test-time optimization method that adaptively tunes fidelity weights\nacross arbitrary noise schedules without requiring retraining of the diffusion\nprior. ZADS treats the denoising process as a fixed unrolled sampler and\noptimizes fidelity weights in a self-supervised manner using only undersampled\nmeasurements. Experiments on the fastMRI knee dataset demonstrate that ZADS\nconsistently outperforms both traditional compressed sensing and recent\ndiffusion-based methods, showcasing its ability to deliver high-fidelity\nreconstructions across varying noise schedules and acquisition settings.", "AI": {"tldr": "本文提出ZADS方法，通过在测试时自适应调整扩散模型的数据保真度权重，解决了快速MRI重建中现有方法泛化性差的问题，无需重新训练，并显著优于现有技术。", "motivation": "扩散/分数模型在逆问题（如加速MRI重建）中表现强大，但其性能高度依赖于数据保真度权重的精确调优，尤其是在快速采样和少量去噪步骤下。现有方法常依赖启发式或固定权重，导致在不同测量条件和不规则时间步调度下泛化能力不足。", "method": "本文提出“零样本自适应扩散采样”（ZADS），一种测试时优化方法。它将去噪过程视为一个固定的展开采样器，并以自监督方式，仅使用欠采样测量数据，自适应地调整任意噪声调度下的保真度权重，无需重新训练扩散先验模型。", "result": "在fastMRI膝盖数据集上的实验表明，ZADS持续优于传统的压缩感知和近期基于扩散的方法，展示了其在不同噪声调度和采集设置下提供高保真重建的能力。", "conclusion": "ZADS提供了一种无需重新训练即可在测试时自适应调整扩散模型保真度权重的方法，显著提高了加速MRI重建的性能和泛化性，克服了现有扩散模型在逆问题中面临的挑战。"}}
{"id": "2509.09699", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09699", "abs": "https://arxiv.org/abs/2509.09699", "authors": ["Mingyang Li", "Viktor Schlegel", "Tingting Mu", "Warren Del-Pinto", "Goran Nenadic"], "title": "Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs", "comment": null, "summary": "Mapping clinical documents to standardised clinical vocabularies is an\nimportant task, as it provides structured data for information retrieval and\nanalysis, which is essential to clinical research, hospital administration and\nimproving patient care. However, manual coding is both difficult and\ntime-consuming, making it impractical at scale. Automated coding can\npotentially alleviate this burden, improving the availability and accuracy of\nstructured clinical data. The task is difficult to automate, as it requires\nmapping to high-dimensional and long-tailed target spaces, such as the\nInternational Classification of Diseases (ICD). While external knowledge\nsources have been readily utilised to enhance output code representation, the\nuse of external resources for representing the input documents has been\nunderexplored. In this work, we compute a structured representation of the\ninput documents, making use of document-level knowledge graphs (KGs) that\nprovide a comprehensive structured view of a patient's condition. The resulting\nknowledge graph efficiently represents the patient-centred input documents with\n23\\% of the original text while retaining 90\\% of the information. We assess\nthe effectiveness of this graph for automated ICD-9 coding by integrating it\ninto the state-of-the-art ICD coding architecture PLM-ICD. Our experiments\nyield improved Macro-F1 scores by up to 3.20\\% on popular benchmarks, while\nimproving training efficiency. We attribute this improvement to different types\nof entities and relationships in the KG, and demonstrate the improved\nexplainability potential of the approach over the text-only baseline.", "AI": {"tldr": "本研究利用文档级知识图谱（KG）对临床文档进行结构化表示，以提高自动化ICD编码的准确性和训练效率，同时增强可解释性。", "motivation": "将临床文档映射到标准化词汇是重要但耗时且难以大规模手动完成的任务。自动化编码面临高维度和长尾目标空间的挑战。虽然外部知识源常用于增强输出代码表示，但将外部资源用于表示输入文档的研究不足。", "method": "研究人员计算了输入文档的结构化表示，利用文档级知识图谱（KG）提供患者病情的全面结构化视图。然后，将这种知识图谱集成到最先进的ICD编码架构PLM-ICD中，以评估其在自动化ICD-9编码中的有效性。", "result": "生成的知识图谱能高效表示以患者为中心的输入文档，仅用23%的原始文本保留了90%的信息。实验表明，在流行基准测试中，宏观F1分数提高了高达3.20%，同时提高了训练效率。这种改进归因于KG中不同类型的实体和关系，并且该方法比纯文本基线具有更好的可解释性潜力。", "conclusion": "利用文档级知识图谱对输入临床文档进行结构化表示，能显著提高自动化ICD编码的准确性和训练效率，并增强模型的可解释性，为大规模临床数据分析提供了有效途径。"}}
{"id": "2509.09738", "categories": ["cs.AI", "q-bio.QM", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.09738", "abs": "https://arxiv.org/abs/2509.09738", "authors": ["Umut Eser", "Yael Gozin", "L. Jay Stallons", "Ari Caroline", "Martin Preusse", "Brandon Rice", "Scott Wright", "Andrew Robertson"], "title": "Human-AI Collaboration Increases Efficiency in Regulatory Writing", "comment": null, "summary": "Background: Investigational New Drug (IND) application preparation is\ntime-intensive and expertise-dependent, slowing early clinical development.\nObjective: To evaluate whether a large language model (LLM) platform (AutoIND)\ncan reduce first-draft composition time while maintaining document quality in\nregulatory submissions. Methods: Drafting times for IND nonclinical written\nsummaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly\nrecorded. For comparison, manual drafting times for IND summaries previously\ncleared by the U.S. FDA were estimated from the experience of regulatory\nwriters ($\\geq$6 years) and used as industry-standard benchmarks. Quality was\nassessed by a blinded regulatory writing assessor using seven pre-specified\ncategories: correctness, completeness, conciseness, consistency, clarity,\nredundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a\npercentage. A critical regulatory error was defined as any misrepresentation or\nomission likely to alter regulatory interpretation (e.g., incorrect NOAEL,\nomission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced\ninitial drafting time by $\\sim$97% (from $\\sim$100 h to 3.7 h for 18,870\npages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).\nQuality scores were 69.6\\% and 77.9\\% for IND-1 and IND-2. No critical\nregulatory errors were detected, but deficiencies in emphasis, conciseness, and\nclarity were noted. Conclusions: AutoIND can dramatically accelerate IND\ndrafting, but expert regulatory writers remain essential to mature outputs to\nsubmission-ready quality. Systematic deficiencies identified provide a roadmap\nfor targeted model improvements.", "AI": {"tldr": "本研究评估了一个大型语言模型（LLM）平台AutoIND在IND（新药临床试验申请）非临床总结撰写中的效率和质量。结果显示，AutoIND能大幅缩短初稿撰写时间（约97%），且未发现关键监管错误，但仍需专家进行最终润色以达到提交标准。", "motivation": "IND申请的准备工作耗时且高度依赖专业知识，这减缓了早期临床开发进程。", "method": "研究直接记录了AutoIND生成IND非临床书面总结（eCTD模块2.6.2、2.6.4、2.6.6）的起草时间。作为对比，通过经验丰富的监管撰稿人（≥6年经验）估算了手动撰写FDA已批准IND总结所需的时间，作为行业基准。质量评估由一位盲审的监管撰稿评估员根据七个预设类别（正确性、完整性、简洁性、一致性、清晰度、冗余和重点）进行。每个子标准评分0-3并标准化为百分比。关键监管错误定义为任何可能改变监管解释的错误或遗漏。", "result": "AutoIND将初稿撰写时间缩短了约97%（对于IND-1，从约100小时减少到3.7小时；对于IND-2，减少到2.6小时）。IND-1和IND-2的质量得分分别为69.6%和77.9%。未检测到关键监管错误，但发现了在重点、简洁性和清晰度方面的不足。", "conclusion": "AutoIND可以显著加速IND的起草过程，但专业的监管撰稿人仍然必不可少，以将产出完善至可提交的质量。识别出的系统性缺陷为模型有针对性的改进提供了路线图。"}}
{"id": "2509.09732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09732", "abs": "https://arxiv.org/abs/2509.09732", "authors": ["Sary Elmansoury", "Islam Mesabah", "Gerrit Großmann", "Peter Neigel", "Raj Bhalwankar", "Daniel Kondermann", "Sebastian J. Vollmer"], "title": "Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs", "comment": null, "summary": "Vision language models (VLMs) excel at zero-shot visual classification, but\ntheir performance on fine-grained tasks and large hierarchical label spaces is\nunderstudied. This paper investigates whether structured, tree-based reasoning\ncan enhance VLM performance. We introduce a framework that decomposes\nclassification into interpretable decisions using decision trees and evaluates\nit on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the\nmodel achieves 98.2% accuracy in understanding the tree knowledge, tree-based\nreasoning consistently underperforms standard zero-shot prompting. We also\nexplore enhancing the tree prompts with LLM-generated classes and image\ndescriptions to improve alignment. The added description enhances the\nperformance of the tree-based and zero-shot methods. Our findings highlight\nlimitations of structured reasoning in visual classification and offer insights\nfor designing more interpretable VLM systems.", "AI": {"tldr": "本研究探讨了视觉语言模型（VLM）在细粒度分类中使用树状结构化推理的有效性，发现尽管模型能理解树知识，但树状推理表现不如标准零样本提示，不过LLM生成的描述能提升性能。", "motivation": "VLM在零样本视觉分类中表现出色，但其在细粒度任务和大型分层标签空间上的性能尚未得到充分研究。研究者希望探索结构化的、基于树的推理是否能增强VLM的性能。", "method": "引入了一个将分类分解为可解释决策的框架，并使用决策树进行实现。该框架在细粒度（GTSRB）和粗粒度（CIFAR-10）数据集上进行了评估。此外，还探索了使用大型语言模型（LLM）生成的类别和图像描述来增强树提示，以改善对齐。", "result": "模型在理解树知识方面达到了98.2%的准确率。然而，基于树的推理表现始终不如标准的零样本提示。通过添加LLM生成的描述，树状推理和零样本方法的性能都得到了提升。", "conclusion": "研究结果突出了结构化推理在视觉分类中的局限性，并为设计更具可解释性的VLM系统提供了见解。"}}
{"id": "2509.09805", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09805", "abs": "https://arxiv.org/abs/2509.09805", "authors": ["Francisco M. López", "Miles Lenz", "Marco G. Fedozzi", "Arthur Aubret", "Jochen Triesch"], "title": "MIMo grows! Simulating body and sensory development in a multimodal infant model", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 6 figures", "summary": "Infancy is characterized by rapid body growth and an explosive change of\nsensory and motor abilities. However, developmental robots and simulation\nplatforms are typically designed in the image of a specific age, which limits\ntheir ability to capture the changing abilities and constraints of developing\ninfants. To address this issue, we present MIMo v2, a new version of the\nmultimodal infant model. It includes a growing body with increasing actuation\nstrength covering the age range from birth to 24 months. It also features\nfoveated vision with developing visual acuity as well as sensorimotor delays\nmodeling finite signal transmission speeds to and from an infant's brain.\nFurther enhancements of this MIMo version include an inverse kinematics module,\na random environment generator and updated compatiblity with third-party\nsimulation and learning libraries. Overall, this new MIMo version permits\nincreased realism when modeling various aspects of sensorimotor development.\nThe code is available on the official repository\n(https://github.com/trieschlab/MIMo).", "AI": {"tldr": "MIMo v2是一个新的多模态婴儿模型，具有随年龄增长的身体、力量、视力以及感觉运动延迟，旨在提高对婴儿感觉运动发展的建模真实性。", "motivation": "现有开发机器人和仿真平台通常针对特定年龄设计，限制了它们捕捉婴儿不断变化的能力和约束，无法真实反映婴儿期的快速身体成长和感觉运动能力的爆炸性变化。", "method": "MIMo v2模型包含一个从出生到24个月的生长身体，其驱动强度随年龄增加。它还具有发展中视力的中央凹视觉，以及模拟信号传输速度有限的感觉运动延迟。此外，该版本还增强了逆运动学模块、随机环境生成器，并更新了与第三方仿真和学习库的兼容性。", "result": "新版本的MIMo模型允许在建模感觉运动发展的各个方面时获得更高的真实感。", "conclusion": "MIMo v2通过整合生长身体、发展视力、感觉运动延迟等特性，显著提高了对婴儿感觉运动发展的建模真实性，解决了现有平台无法捕捉婴儿不断变化的能力和约束的问题。"}}
{"id": "2509.09784", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.09784", "abs": "https://arxiv.org/abs/2509.09784", "authors": ["Amir Bahador Javadi", "Amin Kargarian", "Mort Naraghi-Pour"], "title": "Automatic Regression for Governing Equations with Control (ARGOSc)", "comment": null, "summary": "Learning the governing equations of dynamical systems from data has drawn\nsignificant attention across diverse fields, including physics, engineering,\nrobotics and control, economics, climate science, and healthcare. Sparse\nregression techniques, exemplified by the Automatic Regression for Governing\nEquations (ARGOS) framework, have demonstrated effectiveness in extracting\nparsimonious models from time series data. However, real-world dynamical\nsystems are driven by input control, external forces, or human interventions,\nwhich standard ARGOS does not accommodate. To address this, we introduce ARGOS\nwith control (ARGOSc), an extension of ARGOS that incorporates external control\ninputs into the system identification process. ARGOSc extends the sparse\nregression framework to infer governing equations while accounting for the\neffects of exogenous inputs, enabling robust identification of forcing dynamics\nin low- to medium-noise datasets. We demonstrate ARGOSc efficacy on benchmark\nsystems, including the Van der Pol oscillator, Lotka-Volterra, and the Lorenz\nsystem with forcing and feedback control, showing enhanced accuracy in\ndiscovering governing laws. Under the noisy conditions, ARGOSc outperforms the\nwidely used sparse identification of nonlinear dynamics with control (SINDYc),\nin accurately identifying the underlying forced dynamics. In some cases, SINDYc\nfails to capture the true system dynamics, whereas ARGOSc consistently\nsucceeds.", "AI": {"tldr": "本文提出ARGOSc，它是ARGOS框架的扩展，旨在从数据中学习包含外部控制输入的动力系统方程。ARGOSc在处理带控制的系统时表现出更高的准确性，尤其是在噪声条件下，优于SINDYc。", "motivation": "从数据中学习动力系统的控制方程是一个重要领域，但现有的稀疏回归技术（如标准ARGOS）无法处理真实世界系统中常见的外部控制输入、外力或人为干预。这促使研究人员开发一种能够整合这些控制输入的系统识别方法。", "method": "本文引入了带控制的ARGOS (ARGOSc)，它是ARGOS的扩展。ARGOSc将外部控制输入纳入系统识别过程中的稀疏回归框架，以推断控制方程，同时考虑外生输入的影响。", "result": "ARGOSc在基准系统（如Van der Pol振子、Lotka-Volterra和带强制与反馈控制的Lorenz系统）上表现出卓越的性能，显著提高了发现控制律的准确性。在噪声条件下，ARGOSc在准确识别潜在的受迫动力学方面优于广泛使用的SINDYc，在某些SINDYc失败的情况下，ARGOSc仍能成功。", "conclusion": "ARGOSc为包含外部控制输入的动力系统提供了鲁棒的控制方程识别方法，在低到中等噪声数据集中能够有效地识别强制动力学。与现有方法SINDYc相比，ARGOSc在噪声环境下表现出更高的准确性和稳定性。"}}
{"id": "2509.09894", "categories": ["eess.IV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09894", "abs": "https://arxiv.org/abs/2509.09894", "authors": ["Jiayun Wang", "Yousuf Aborahama", "Arya Khokhar", "Yang Zhang", "Chuwei Wang", "Karteekeya Sastry", "Julius Berner", "Yilin Luo", "Boris Bonev", "Zongyi Li", "Kamyar Azizzadenesheli", "Lihong V. Wang", "Anima Anandkumar"], "title": "Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators", "comment": null, "summary": "Photoacoustic computed tomography (PACT) combines optical contrast with\nultrasonic resolution, achieving deep-tissue imaging beyond the optical\ndiffusion limit. While three-dimensional PACT systems enable high-resolution\nvolumetric imaging for applications spanning transcranial to breast imaging,\ncurrent implementations require dense transducer arrays and prolonged\nacquisition times, limiting clinical translation. We introduce Pano (PACT\nimaging neural operator), an end-to-end physics-aware model that directly\nlearns the inverse acoustic mapping from sensor measurements to volumetric\nreconstructions. Unlike existing approaches (e.g. universal back-projection\nalgorithm), Pano learns both physics and data priors while also being agnostic\nto the input data resolution. Pano employs spherical discrete-continuous\nconvolutions to preserve hemispherical sensor geometry, incorporates Helmholtz\nequation constraints to ensure physical consistency and operates\nresolutionindependently across varying sensor configurations. We demonstrate\nthe robustness and efficiency of Pano in reconstructing high-quality images\nfrom both simulated and real experimental data, achieving consistent\nperformance even with significantly reduced transducer counts and limited-angle\nacquisition configurations. The framework maintains reconstruction fidelity\nacross diverse sparse sampling patterns while enabling real-time volumetric\nimaging capabilities. This advancement establishes a practical pathway for\nmaking 3D PACT more accessible and feasible for both preclinical research and\nclinical applications, substantially reducing hardware requirements without\ncompromising image reconstruction quality.", "AI": {"tldr": "Pano是一种端到端、物理感知的神经算子模型，用于光声计算机断层扫描（PACT）的三维重建，它显著减少了所需的传感器数量和采集时间，同时保持了高图像质量，从而提高了PACT的实用性和可及性。", "motivation": "现有的三维PACT系统需要密集的换能器阵列和长时间的采集，这限制了其临床转化和实际应用。", "method": "Pano（PACT成像神经算子）是一个端到端的物理感知模型，直接学习从传感器测量到体素重建的逆声学映射。它采用球形离散-连续卷积以保持半球形传感器几何形状，整合亥姆霍兹方程约束以确保物理一致性，并能独立于分辨率在不同传感器配置下运行。Pano学习物理和数据先验，并且对输入数据分辨率不可知。", "result": "Pano在模拟和真实实验数据中都能重建高质量图像，即使在显著减少换能器数量和有限角度采集配置下也能保持一致的性能。该框架在多样化的稀疏采样模式下保持了重建保真度，并实现了实时体素成像能力。", "conclusion": "这项进展为使3D PACT在临床前研究和临床应用中更易于获取和可行奠定了基础，它在不损害图像重建质量的前提下，大幅降低了硬件要求。"}}
{"id": "2509.09700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09700", "abs": "https://arxiv.org/abs/2509.09700", "authors": ["Malavika Suresh", "Rahaf Aljundi", "Ikechukwu Nkisi-Orji", "Nirmalie Wiratunga"], "title": "Cross-Layer Attention Probing for Fine-Grained Hallucination Detection", "comment": "To be published at the TRUST-AI workshop, ECAI 2025", "summary": "With the large-scale adoption of Large Language Models (LLMs) in various\napplications, there is a growing reliability concern due to their tendency to\ngenerate inaccurate text, i.e. hallucinations. In this work, we propose\nCross-Layer Attention Probing (CLAP), a novel activation probing technique for\nhallucination detection, which processes the LLM activations across the entire\nresidual stream as a joint sequence. Our empirical evaluations using five LLMs\nand three tasks show that CLAP improves hallucination detection compared to\nbaselines on both greedy decoded responses as well as responses sampled at\nhigher temperatures, thus enabling fine-grained detection, i.e. the ability to\ndisambiguate hallucinations and non-hallucinations among different sampled\nresponses to a given prompt. This allows us to propose a detect-then-mitigate\nstrategy using CLAP to reduce hallucinations and improve LLM reliability\ncompared to direct mitigation approaches. Finally, we show that CLAP maintains\nhigh reliability even when applied out-of-distribution.", "AI": {"tldr": "本文提出了一种名为CLAP的新型激活探测技术，用于检测大型语言模型中的幻觉，并在各种设置下显示出优于基线的性能，从而提高了LLM的可靠性。", "motivation": "由于大型语言模型（LLMs）在各种应用中的广泛采用，其生成不准确文本（即幻觉）的倾向引发了对其可靠性的日益增长的担忧。", "method": "本文提出了一种名为“跨层注意力探测”（Cross-Layer Attention Probing, CLAP）的新型激活探测技术。CLAP将LLM在整个残差流中的激活作为联合序列进行处理。", "result": "通过对五种LLM和三项任务的实证评估，CLAP在贪婪解码和高温度采样响应上的幻觉检测均优于基线。它实现了细粒度检测，并支持一种“先检测后缓解”的策略，与直接缓解方法相比，能有效减少幻觉并提高LLM可靠性。此外，CLAP在分布外应用时仍能保持高可靠性。", "conclusion": "CLAP是一种有效且鲁棒的幻觉检测技术，能够提高大型语言模型的可靠性，并支持通过检测来缓解幻觉的策略，即使在分布外场景下也能保持高性能。"}}
{"id": "2509.09775", "categories": ["cs.AI", "cs.CL", "cs.FL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.09775", "abs": "https://arxiv.org/abs/2509.09775", "authors": ["Aleksandr Boldachev"], "title": "Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture", "comment": "22 pages, 6 figures", "summary": "This paper presents boldsea, Boldachev's semantic-event approach -- an\narchitecture for modeling complex dynamic systems using executable ontologies\n-- semantic models that act as dynamic structures, directly controlling process\nexecution. We demonstrate that integrating event semantics with a dataflow\narchitecture addresses the limitations of traditional Business Process\nManagement (BPM) systems and object-oriented semantic technologies. The paper\npresents the formal BSL (boldsea Semantic Language), including its BNF grammar,\nand outlines the boldsea-engine's architecture, which directly interprets\nsemantic models as executable algorithms without compilation. It enables the\nmodification of event models at runtime, ensures temporal transparency, and\nseamlessly merges data and business logic within a unified semantic framework.", "AI": {"tldr": "本文提出了boldsea架构，它利用可执行本体（作为动态结构直接控制过程执行的语义模型）来建模复杂的动态系统。该方法通过将事件语义与数据流架构结合，解决了传统业务流程管理（BPM）系统和面向对象语义技术的局限性。", "motivation": "传统业务流程管理（BPM）系统和面向对象语义技术在建模复杂动态系统时存在局限性，促使研究人员寻求一种更有效的方法。", "method": "boldsea架构通过可执行本体（即作为动态结构直接控制过程执行的语义模型）来建模系统。它将事件语义与数据流架构集成，并提出了形式化的boldsea语义语言（BSL）及其BNF语法。boldsea引擎直接解释语义模型作为可执行算法，无需编译。", "result": "boldsea方法解决了传统BPM系统和面向对象语义技术的局限性。它支持在运行时修改事件模型，确保了时间透明性，并在统一的语义框架内无缝融合了数据和业务逻辑。", "conclusion": "boldsea提供了一种强大的架构，通过使用可执行本体和直接解释的语义模型，有效地建模复杂动态系统，克服了现有技术的缺点，并提供了运行时灵活性和数据与业务逻辑的统一性。"}}
{"id": "2509.09737", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09737", "abs": "https://arxiv.org/abs/2509.09737", "authors": ["Klemen Kotar", "Wanhee Lee", "Rahul Venkatesh", "Honglin Chen", "Daniel Bear", "Jared Watrous", "Simon Kim", "Khai Loong Aw", "Lilian Naing Chen", "Stefan Stojanov", "Kevin Feigelis", "Imran Thobani", "Alex Durango", "Khaled Jedoui", "Atlas Kazemian", "Dan Yamins"], "title": "World Modeling with Probabilistic Structure Integration", "comment": null, "summary": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.", "AI": {"tldr": "PSI（概率结构集成）是一个用于从数据中学习高度可控、灵活可提示的世界模型的系统。它通过概率预测、结构提取和集成三步循环，不断增强模型能力，并创建类似LLM的通用提示语言。", "motivation": "研究动机是开发一个能够从数据中学习出丰富可控、灵活可提示的世界模型，以更好地理解和预测复杂数据（如视频）。", "method": "PSI系统采用三步循环：\n1. 概率预测：构建一个随机访问自回归序列模型Psi，作为数据的概率图模型，支持学习任意变量间的条件分布。\n2. 结构提取：通过对Psi进行因果推断，以零样本方式提取数据中低维的“中间结构”。\n3. 集成：将提取的结构转换为新的令牌类型，作为条件信号和预测目标重新混合到训练数据中，从而增强Psi的能力。", "result": "研究人员在1.4万亿个互联网视频令牌上训练了Psi的一个实例。结果表明，Psi能够执行各种有用的视频预测和理解推理，并提取出最先进的光流、自监督深度和对象分割，并利用这些结构支持了预测能力的完整改进周期。", "conclusion": "PSI系统通过迭代地预测、提取和集成数据中的潜在结构，成功地学习了可控的世界模型。这种方法不仅提升了模型对底层数据的建模能力，还创建了新的控制句柄，实现了类似大型语言模型（LLM）的通用提示功能。"}}
{"id": "2509.09889", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09889", "abs": "https://arxiv.org/abs/2509.09889", "authors": ["Giulia Botta", "Marco Botta", "Cristina Gena", "Alessandro Mazzei", "Massimo Donini", "Alberto Lillo"], "title": "Using the Pepper Robot to Support Sign Language Communication", "comment": "paper presented at ICSR2025", "summary": "Social robots are increasingly experimented in public and assistive settings,\nbut their accessibility for Deaf users remains quite underexplored. Italian\nSign Language (LIS) is a fully-fledged natural language that relies on complex\nmanual and non-manual components. Enabling robots to communicate using LIS\ncould foster more inclusive human robot interaction, especially in social\nenvironments such as hospitals, airports, or educational settings. This study\ninvestigates whether a commercial social robot, Pepper, can produce\nintelligible LIS signs and short signed LIS sentences. With the help of a Deaf\nstudent and his interpreter, an expert in LIS, we co-designed and implemented\n52 LIS signs on Pepper using either manual animation techniques or a MATLAB\nbased inverse kinematics solver. We conducted a exploratory user study\ninvolving 12 participants proficient in LIS, both Deaf and hearing.\nParticipants completed a questionnaire featuring 15 single-choice video-based\nsign recognition tasks and 2 open-ended questions on short signed sentences.\nResults shows that the majority of isolated signs were recognized correctly,\nalthough full sentence recognition was significantly lower due to Pepper's\nlimited articulation and temporal constraints. Our findings demonstrate that\neven commercially available social robots like Pepper can perform a subset of\nLIS signs intelligibly, offering some opportunities for a more inclusive\ninteraction design. Future developments should address multi-modal enhancements\n(e.g., screen-based support or expressive avatars) and involve Deaf users in\nparticipatory design to refine robot expressivity and usability.", "AI": {"tldr": "本研究探讨了Pepper社交机器人生成意大利手语（LIS）手势和短句的可行性。结果显示，孤立手势可被识别，但完整句子的识别率较低，表明商用机器人有望实现更具包容性的人机交互。", "motivation": "社交机器人在公共和辅助环境中日益普及，但其对聋人用户的可访问性研究不足。意大利手语（LIS）是一种完整的自然语言，使机器人能够使用LIS进行交流可以促进更具包容性的人机交互，尤其是在医院、机场或教育等社会环境中。", "method": "与一名聋人学生及其LIS专家口译员合作，共同设计并实现了52个LIS手势在Pepper机器人上，使用了手动动画技术或基于MATLAB的逆运动学求解器。进行了一项探索性用户研究，涉及12名精通LIS的参与者（包括聋人和听力正常者）。参与者完成了一份问卷，包含15个基于视频的单选手势识别任务和2个关于短手语句子的开放式问题。", "result": "结果表明，大多数孤立手势被正确识别，但由于Pepper机器人的有限关节活动和时间限制，完整句子的识别率显著降低。研究发现，即使是Pepper这样的商用社交机器人也能清晰地执行部分LIS手势。", "conclusion": "本研究证明，商用社交机器人能够以可理解的方式执行部分LIS手势，为更具包容性的交互设计提供了机会。未来的发展应解决多模态增强（例如，基于屏幕的支持或富有表现力的虚拟形象）问题，并让聋人用户参与到参与式设计中，以完善机器人的表达能力和可用性。"}}
{"id": "2509.09789", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.09789", "abs": "https://arxiv.org/abs/2509.09789", "authors": ["Safa Mohammed Sali", "Hoach The Nguyen", "Ameena Saad Al-Sumaiti"], "title": "High-Gain Voltage-Multiplier Coupled Quadratic Boost Converter: A New Design for Small Scale PV Integration", "comment": null, "summary": "This paper introduces a single-switch high-gain voltage-multiplier coupled\nquadratic boost converter (HGVM-QBC), developed from the conventional quadratic\nboost converter (QBC). The proposed topology is designed to achieve higher\nvoltage gain, lower semiconductor voltage stress, and continuous current\noperation, making it particularly suitable for small-scale photovoltaic (PV)\nsystems. By incorporating a voltage multiplier cell into the QBC, the converter\nsignificantly improves voltage boosting capability while mitigating stress on\nswitching devices. In this configuration, the output voltage is obtained by\ncombining the voltages across multiple output capacitors, thereby enhancing the\noverall voltage level. A detailed comparative study with recently reported\nconverter topologies demonstrates the superior gain and reduced device stress\noffered by the HGVM-QBC. The design is validated through MATLAB/Simulink\nsimulations, which confirm improved performance in terms of gain and voltage\nstress. Furthermore, an experimental prototype achieves an output of 151 Vdc\nfrom a 12 Vdc input at a 55% duty cycle, corresponding to a gain of 12.59.\nThese results establish the HGVM-QBC as an efficient and reliable solution for\nPV applications that demand high voltage output from low input sources.", "AI": {"tldr": "本文介绍了一种单开关高增益倍压器耦合二次升压变换器（HGVM-QBC），专为小型光伏系统设计，具有高电压增益、低半导体电压应力及连续电流操作的特点。", "motivation": "传统二次升压变换器（QBC）在小型光伏系统应用中，需要更高的电压增益、更低的半导体电压应力以及连续电流操作能力。", "method": "通过将倍压器单元集成到传统二次升压变换器中，并结合多个输出电容器的电压以提高整体电压水平。通过MATLAB/Simulink仿真和实验原型验证其性能。", "result": "与现有变换器拓扑相比，HGVM-QBC展现出卓越的增益和更低的器件应力。仿真结果证实了增益和电压应力的改善。实验原型在55%占空比下，将12 Vdc输入升压至151 Vdc输出，增益达到12.59。", "conclusion": "HGVM-QBC为需要从低输入源获得高电压输出的光伏应用提供了一个高效可靠的解决方案。"}}
{"id": "2509.09972", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09972", "abs": "https://arxiv.org/abs/2509.09972", "authors": ["Mohammadreza Narimani", "Alireza Pourreza", "Ali Moghimi", "Mohsen Mesgaran", "Parastoo Farajpoor", "Hamid Jafarbiglu"], "title": "Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms", "comment": "Author-accepted version (no publisher header/footer). 10 pages +\n  presentation. Published in Proceedings of SPIE Defense + Commercial Sensing\n  2024, Vol. 13053, Paper 1305304. Event: National Harbor, Maryland, USA.\n  Official version: https://doi.org/10.1117/12.3021219", "summary": "This study addresses the escalating threat of branched broomrape (Phelipanche\nramosa) to California's tomato industry, which supplies over 90 percent of U.S.\nprocessing tomatoes. The parasite's largely underground life cycle makes early\ndetection difficult, while conventional chemical controls are costly,\nenvironmentally harmful, and often ineffective. To address this, we combined\ndrone-based multispectral imagery with Long Short-Term Memory (LSTM) deep\nlearning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)\nto handle class imbalance. Research was conducted on a known broomrape-infested\ntomato farm in Woodland, Yolo County, CA, across five key growth stages\ndetermined by growing degree days (GDD). Multispectral images were processed to\nisolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with\n79.09 percent overall accuracy and 70.36 percent recall without integrating\nlater stages. Incorporating sequential growth stages with LSTM improved\ndetection substantially. The best-performing scenario, which integrated all\ngrowth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy\nand 95.37 percent recall. These results demonstrate the strong potential of\ntemporal multispectral analysis and LSTM networks for early broomrape\ndetection. While further real-world data collection is needed for practical\ndeployment, this study shows that UAV-based multispectral sensing coupled with\ndeep learning could provide a powerful precision agriculture tool to reduce\nlosses and improve sustainability in tomato production.", "AI": {"tldr": "本研究结合无人机多光谱图像和LSTM深度学习网络，通过SMOTE技术处理类别不平衡问题，实现了对番茄作物中列当（Phelipanche ramosa）的早期高效检测，为精准农业提供了新工具。", "motivation": "加州番茄产业（供应美国90%以上加工番茄）正面临分枝列当日益严重的威胁。这种寄生植物的地下生命周期使其难以早期发现，而传统化学防治成本高昂、对环境有害且效果不佳，急需更有效的检测和控制方法。", "method": "研究在已知列当侵染的番茄农场进行，结合无人机多光谱图像和长短期记忆（LSTM）深度学习网络。采用合成少数过采样技术（SMOTE）处理类别不平衡问题。研究覆盖了由生长积温（GDD）确定的五个关键生长阶段，并对多光谱图像进行处理以分离番茄冠层反射率。", "result": "在897 GDD时，未整合后期阶段的列当检测准确率为79.09%，召回率为70.36%。通过LSTM整合顺序生长阶段显著提高了检测能力。最佳方案（整合所有生长阶段并结合SMOTE增强）达到了88.37%的总体准确率和95.37%的召回率。", "conclusion": "这些结果表明，时间多光谱分析和LSTM网络在早期列当检测方面具有巨大潜力。尽管需要进一步的实际数据收集才能进行实际部署，但这项研究证明了基于无人机的多光谱遥感结合深度学习可以为番茄生产提供强大的精准农业工具，以减少损失并提高可持续性。"}}
{"id": "2509.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09701", "abs": "https://arxiv.org/abs/2509.09701", "authors": ["JungHo Jung", "Junhyun Lee"], "title": "Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task", "comment": null, "summary": "End-to-end speech-to-text translation typically suffers from the scarcity of\npaired speech-text data. One way to overcome this shortcoming is to utilize the\nbitext data from the Machine Translation (MT) task and perform Multi-Task\nLearning (MTL). In this paper, we formulate MTL from a regularization\nperspective and explore how sequences can be regularized within and across\nmodalities. By thoroughly investigating the effect of consistency\nregularization (different modality) and R-drop (same modality), we show how\nthey respectively contribute to the total regularization. We also demonstrate\nthat the coefficient of MT loss serves as another source of regularization in\nthe MTL setting. With these three sources of regularization, we introduce the\noptimal regularization contour in the high-dimensional space, called the\nregularization horizon. Experiments show that tuning the hyperparameters within\nthe regularization horizon achieves near state-of-the-art performance on the\nMuST-C dataset.", "AI": {"tldr": "本文从正则化角度探讨了端到端语音到文本翻译中的多任务学习，通过分析跨模态和同模态正则化以及机器翻译损失系数，提出了正则化视界以优化模型性能，并在MuST-C数据集上取得了接近最先进的结果。", "motivation": "端到端语音到文本翻译面临配对语音-文本数据稀缺的问题。利用机器翻译任务的双语语料进行多任务学习是克服这一挑战的有效途径。", "method": "本文将多任务学习（MTL）从正则化视角进行建模，探讨了序列如何在模态内部和跨模态进行正则化。具体分析了跨模态的一致性正则化和同模态的R-drop正则化，并指出机器翻译损失系数也是MTL设置中的另一个正则化来源。在此基础上，引入了高维空间中的最优正则化轮廓——正则化视界。", "result": "实验结果表明，在正则化视界内调整超参数可以在MuST-C数据集上取得接近最先进的性能。", "conclusion": "一致性正则化、R-drop以及机器翻译损失系数共同构成了多任务学习中端到端语音到文本翻译的有效正则化来源。通过在正则化视界内进行超参数调优，可以显著提升模型性能，有效应对数据稀缺问题。"}}
{"id": "2509.09790", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09790", "abs": "https://arxiv.org/abs/2509.09790", "authors": ["Yuxuan Li", "Victor Zhong"], "title": "How well can LLMs provide planning feedback in grounded environments?", "comment": null, "summary": "Learning to plan in grounded environments typically requires carefully\ndesigned reward functions or high-quality annotated demonstrations. Recent\nworks show that pretrained foundation models, such as large language models\n(LLMs) and vision language models (VLMs), capture background knowledge helpful\nfor planning, which reduces the amount of reward design and demonstrations\nneeded for policy learning. We evaluate how well LLMs and VLMs provide feedback\nacross symbolic, language, and continuous control environments. We consider\nprominent types of feedback for planning including binary feedback, preference\nfeedback, action advising, goal advising, and delta action feedback. We also\nconsider inference methods that impact feedback performance, including\nin-context learning, chain-of-thought, and access to environment dynamics. We\nfind that foundation models can provide diverse high-quality feedback across\ndomains. Moreover, larger and reasoning models consistently provide more\naccurate feedback, exhibit less bias, and benefit more from enhanced inference\nmethods. Finally, feedback quality degrades for environments with complex\ndynamics or continuous state spaces and action spaces.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）和视觉语言模型（VLMs）在符号、语言和连续控制环境中提供规划反馈的能力。结果表明，基础模型能提供多样化的高质量反馈，且更大、更具推理能力的模型表现更优，但复杂环境会降低反馈质量。", "motivation": "在具身环境中学习规划通常需要精心设计的奖励函数或高质量的演示。最近研究表明，预训练的基础模型（如LLMs和VLMs）捕获了对规划有用的背景知识，这可以减少策略学习所需的奖励设计和演示量。", "method": "研究评估了LLMs和VLMs在符号、语言和连续控制环境中的反馈能力。考虑了多种规划反馈类型，包括二元反馈、偏好反馈、行动建议、目标建议和增量行动反馈。同时，还考虑了影响反馈性能的推理方法，包括上下文学习、思维链和对环境动态的访问。", "result": "研究发现，基础模型可以在不同领域提供多样化的高质量反馈。此外，更大、更具推理能力的模型始终能提供更准确的反馈，表现出更少的偏差，并从增强的推理方法中获益更多。然而，对于动态复杂或状态空间和行动空间连续的环境，反馈质量会下降。", "conclusion": "基础模型在为规划提供反馈方面具有潜力，其性能受到模型规模、推理能力、推理方法以及环境复杂性的影响。"}}
{"id": "2509.09742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09742", "abs": "https://arxiv.org/abs/2509.09742", "authors": ["Md Fazle Rasul", "Alanood Alqobaisi", "Bruhadeshwar Bezawada", "Indrakshi Ray"], "title": "Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning", "comment": null, "summary": "Federated learning (FL) allows multiple entities to train a shared model\ncollaboratively. Its core, privacy-preserving principle is that participants\nonly exchange model updates, such as gradients, and never their raw, sensitive\ndata. This approach is fundamental for applications in domains where privacy\nand confidentiality are important. However, the security of this very mechanism\nis threatened by gradient inversion attacks, which can reverse-engineer private\ntraining data directly from the shared gradients, defeating the purpose of FL.\nWhile the impact of these attacks is known for image, text, and tabular data,\ntheir effect on video data remains an unexamined area of research. This paper\npresents the first analysis of video data leakage in FL using gradient\ninversion attacks. We evaluate two common video classification approaches: one\nemploying pre-trained feature extractors and another that processes raw video\nframes with simple transformations. Our initial results indicate that the use\nof feature extractors offers greater resilience against gradient inversion\nattacks. We also demonstrate that image super-resolution techniques can enhance\nthe frames extracted through gradient inversion attacks, enabling attackers to\nreconstruct higher-quality videos. Our experiments validate this across\nscenarios where the attacker has access to zero, one, or more reference frames\nfrom the target environment. We find that although feature extractors make\nattacks more challenging, leakage is still possible if the classifier lacks\nsufficient complexity. We, therefore, conclude that video data leakage in FL is\na viable threat, and the conditions under which it occurs warrant further\ninvestigation.", "AI": {"tldr": "本文首次分析了联邦学习中视频数据通过梯度反演攻击泄露的风险，发现特征提取器能提供一定抵抗力，但简单分类器仍可能泄露，且超分辨率技术可提升攻击效果。", "motivation": "联邦学习（FL）的核心是保护隐私，但梯度反演攻击可能通过共享梯度逆向工程私有训练数据，从而威胁FL的隐私目标。虽然此类攻击对图像、文本和表格数据的影响已知，但其对视频数据的影响尚未被研究。", "method": "本文通过梯度反演攻击，对联邦学习中的视频数据泄露进行了首次分析。研究评估了两种常见的视频分类方法：一种使用预训练特征提取器，另一种处理带有简单变换的原始视频帧。此外，研究还展示了图像超分辨率技术如何增强通过梯度反演攻击提取的帧，以重建更高质量的视频，并在攻击者拥有零、一或多个目标环境参考帧的场景下验证了这一点。", "result": "初步结果表明，使用特征提取器能提供更大的梯度反演攻击抵抗力。研究还发现，图像超分辨率技术可以增强通过梯度反演攻击重建的视频帧，使攻击者能够重建更高质量的视频。尽管特征提取器使攻击更具挑战性，但如果分类器缺乏足够的复杂性，数据泄露仍然可能发生。", "conclusion": "研究得出结论，联邦学习中的视频数据泄露是一个可行的威胁，其发生的条件值得进一步调查。"}}
{"id": "2509.09893", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09893", "abs": "https://arxiv.org/abs/2509.09893", "authors": ["Hanbit Oh", "Masaki Murooka", "Tomohiro Motoda", "Ryoichi Nakajo", "Yukiyasu Domae"], "title": "Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision", "comment": "Under review", "summary": "Imitation learning is a promising paradigm for training robot agents;\nhowever, standard approaches typically require substantial data acquisition --\nvia numerous demonstrations or random exploration -- to ensure reliable\nperformance. Although exploration reduces human effort, it lacks safety\nguarantees and often results in frequent collisions -- particularly in\nclearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual\nenvironmental resets and imposing additional human burden. This study proposes\nSelf-Augmented Robot Trajectory (SART), a framework that enables policy\nlearning from a single human demonstration, while safely expanding the dataset\nthrough autonomous augmentation. SART consists of two stages: (1) human\nteaching only once, where a single demonstration is provided and precision\nboundaries -- represented as spheres around key waypoints -- are annotated,\nfollowed by one environment reset; (2) robot self-augmentation, where the robot\ngenerates diverse, collision-free trajectories within these boundaries and\nreconnects to the original demonstration. This design improves the data\ncollection efficiency by minimizing human effort while ensuring safety.\nExtensive evaluations in simulation and real-world manipulation tasks show that\nSART achieves substantially higher success rates than policies trained solely\non human-collected demonstrations. Video results available at\nhttps://sites.google.com/view/sart-il .", "AI": {"tldr": "SART是一种模仿学习框架，通过单次人类演示和机器人自主安全数据增强，显著提高了机器人策略的学习效率和成功率。", "motivation": "标准的模仿学习需要大量数据（演示或探索），而探索缺乏安全保障，常导致碰撞，特别是在精细操作任务中，这需要人工重置环境，增加了人类负担。", "method": "SART框架分为两个阶段：1) 人类教学一次：提供一次演示，并标注关键路径点周围的精度边界（球体），然后进行一次环境重置。2) 机器人自主增强：机器人在这些边界内生成多样化、无碰撞的轨迹，并重新连接到原始演示。", "result": "在仿真和真实世界的操作任务中，SART实现了比仅使用人类收集演示训练的策略显著更高的成功率。", "conclusion": "SART通过最小化人类工作量和确保安全性，提高了数据收集效率，从而实现了从单次演示中学习策略，并取得了更好的性能。"}}
{"id": "2509.09812", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.09812", "abs": "https://arxiv.org/abs/2509.09812", "authors": ["Xiuzhen Ye", "Wentao Tang"], "title": "EDMD-Based Robust Observer Synthesis for Nonlinear Systems", "comment": "6 pages, 3 figures. Submitted to IEEE CSS and ACC2026", "summary": "This paper presents a data driven Koopman operator based framework for\ndesigning robust state observers for nonlinear systems. Based on a finite\ndimensional surrogate of the Koopman generator, identified via an extended\ndynamic mode decomposition procedure, a tractable formulation of the observer\ndesign is enabled on the data driven model with conic uncertainties. The\nresulting problem is cast as a semidefinite program with linear matrix\ninequalities, guaranteeing exponential convergence of the observer with a\npredetermined rate in a probabilistic sense. The approach bridges the gap\nbetween statistical error tolerance and observer convergence certification, and\nenables an explicit use of linear systems theory for state observation via a\ndata driven linear surrogate model. Numerical studies demonstrate the\neffectiveness and flexibility of the proposed method.", "AI": {"tldr": "本文提出了一种基于数据驱动Koopman算子的框架，用于设计非线性系统的鲁棒状态观测器，通过半定规划确保观测器以概率方式指数收敛。", "motivation": "现有方法在非线性系统观测器设计中面临挑战，尤其是在弥合统计误差容忍度与观测器收敛性认证之间的差距方面。", "method": "该方法基于通过扩展动态模态分解（EDMD）识别的Koopman生成器的有限维替代模型，将观测器设计问题转化为一个带有锥形不确定性的数据驱动模型。最终问题被表述为一个包含线性矩阵不等式（LMI）的半定规划（SDP）。", "result": "该方法能够以概率方式保证观测器以预定速率指数收敛，弥合了统计误差容忍度与观测器收敛性认证之间的差距，并通过数据驱动的线性替代模型实现了线性系统理论在状态观测中的显式应用。数值研究证明了所提方法的有效性和灵活性。", "conclusion": "所提出的基于数据驱动Koopman算子的框架为非线性系统鲁棒状态观测器的设计提供了一种有效且灵活的方法，并通过半定规划实现了概率意义上的指数收敛保证。"}}
{"id": "2509.10098", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10098", "abs": "https://arxiv.org/abs/2509.10098", "authors": ["Muhamad Daniel Ariff Bin Abdul Rahman", "Yusuke Monno", "Masayuki Tanaka", "Masatoshi Okutomi"], "title": "Polarization Denoising and Demosaicking: Dataset and Baseline Method", "comment": "Published in ICIP2025; Project page:\n  http://www.ok.sc.e.titech.ac.jp/res/PolarDem/PDD.html", "summary": "A division-of-focal-plane (DoFP) polarimeter enables us to acquire images\nwith multiple polarization orientations in one shot and thus it is valuable for\nmany applications using polarimetric information. The image processing pipeline\nfor a DoFP polarimeter entails two crucial tasks: denoising and demosaicking.\nWhile polarization demosaicking for a noise-free case has increasingly been\nstudied, the research for the joint task of polarization denoising and\ndemosaicking is scarce due to the lack of a suitable evaluation dataset and a\nsolid baseline method. In this paper, we propose a novel dataset and method for\npolarization denoising and demosaicking. Our dataset contains 40 real-world\nscenes and three noise-level conditions, consisting of pairs of noisy mosaic\ninputs and noise-free full images. Our method takes a\ndenoising-then-demosaicking approach based on well-accepted signal processing\ncomponents to offer a reproducible method. Experimental results demonstrate\nthat our method exhibits higher image reconstruction performance than other\nalternative methods, offering a solid baseline.", "AI": {"tldr": "本文针对分焦平面(DoFP)偏振相机图像的去噪和去马赛克联合任务，提出了一种新的数据集和基于“先去噪后去马赛克”策略的基线方法，并验证了其优越性能。", "motivation": "分焦平面(DoFP)偏振相机在许多应用中具有重要价值，其图像处理流程包含去噪和去马赛克两个关键任务。尽管无噪声情况下的偏振去马赛克研究日益增多，但由于缺乏合适的评估数据集和可靠的基线方法，对偏振去噪和去马赛克联合任务的研究却非常稀缺。", "method": "本文提出了一个包含40个真实世界场景和三种噪声水平条件的新数据集，该数据集由噪声马赛克输入图像和无噪声完整图像对组成。同时，提出了一种基于“先去噪后去马赛克”策略的方法，该方法利用成熟的信号处理组件，旨在提供可复现的基线。", "result": "实验结果表明，本文提出的方法比其他替代方法展现出更高的图像重建性能，为该领域提供了一个坚实的基线。", "conclusion": "本文成功地为DoFP偏振图像的去噪和去马赛克联合任务提供了新的数据集和高性能的基线方法，填补了该领域研究的空白。"}}
{"id": "2509.09702", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.09702", "abs": "https://arxiv.org/abs/2509.09702", "authors": ["Ninad Bhat", "Kieran Browne", "Pip Bingemann"], "title": "Creativity Benchmark: A benchmark for marketing creativity for LLM models", "comment": "30 Pages, 14 figures", "summary": "We introduce Creativity Benchmark, an evaluation framework for large language\nmodels (LLMs) in marketing creativity. The benchmark covers 100 brands (12\ncategories) and three prompt types (Insights, Ideas, Wild Ideas). Human\npairwise preferences from 678 practising creatives over 11,012 anonymised\ncomparisons, analysed with Bradley-Terry models, show tightly clustered\nperformance with no model dominating across brands or prompt types: the\ntop-bottom spread is $\\Delta\\theta \\approx 0.45$, which implies a head-to-head\nwin probability of $0.61$; the highest-rated model beats the lowest only about\n$61\\%$ of the time. We also analyse model diversity using cosine distances to\ncapture intra- and inter-model variation and sensitivity to prompt reframing.\nComparing three LLM-as-judge setups with human rankings reveals weak,\ninconsistent correlations and judge-specific biases, underscoring that\nautomated judges cannot substitute for human evaluation. Conventional\ncreativity tests also transfer only partially to brand-constrained tasks.\nOverall, the results highlight the need for expert human evaluation and\ndiversity-aware workflows.", "AI": {"tldr": "该研究引入了“创意基准”来评估大型语言模型（LLM）在营销创意方面的表现。结果显示，LLM的性能差异不大，没有模型能完全主导，并且LLM作为评估者无法替代人类判断。因此，强调需要专家级人工评估和注重多样性的工作流程。", "motivation": "现有的大型语言模型评估框架未能充分覆盖营销创意这一特定领域，缺乏针对品牌和营销任务的专业评估基准。", "method": "研究引入了“创意基准”，包含100个品牌（12个类别）和三种提示类型（洞察、想法、狂野想法）。通过678名专业创意人员对11,012个匿名比较进行人工配对偏好评估，并使用Bradley-Terry模型进行分析。同时，使用余弦距离分析了模型的内部和模型间多样性，以及对提示重构的敏感性。此外，还将三种“LLM作为评判者”的设置与人类排名进行了比较。", "result": "LLM的性能表现高度集中，没有单一模型在所有品牌或提示类型中占据主导地位，最高和最低模型之间的胜率仅为61%。将LLM用作评判者时，与人类排名的相关性较弱且不一致，并存在评判者特有的偏差，表明LLM无法替代人类评估。传统的创意测试也只能部分适用于品牌受限的任务。", "conclusion": "研究结果强调，在评估大型语言模型的营销创意时，需要进行专业的专家级人工评估，并采纳注重多样性的工作流程。"}}
{"id": "2509.09794", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09794", "abs": "https://arxiv.org/abs/2509.09794", "authors": ["Jackson Eshbaugh", "Chetan Tiwari", "Jorge Silveyra"], "title": "A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes", "comment": "44 pages; 2 appendices; 9 figures; 1 table. Code available at\n  https://github.com/Lafayette-EshbaughSilveyra-Group/synthetic-homes", "summary": "Computational models have emerged as powerful tools for energy modeling\nresearch, touting scalability and quantitative results. However, these models\nrequire a plethora of data, some of which is inaccessible, expensive, or raises\nprivacy concerns. We introduce a modular multimodal framework to produce this\ndata from publicly accessible residential information and images using\ngenerative artificial intelligence (AI). Additionally, we provide a pipeline\ndemonstrating this framework, and we evaluate its generative AI components. Our\nexperiments show that our framework's use of AI avoids common issues with\ngenerative models. Our framework produces realistic, labeled data. By reducing\ndependence on costly or restricted data sources, we pave a path towards more\naccessible and reproducible research.", "AI": {"tldr": "本文提出一个模块化的多模态框架，利用生成式AI从公开住宅信息和图像中生成能源模型所需的数据，以解决数据获取困难的问题。", "motivation": "计算模型在能源建模研究中日益重要，但其需要大量数据，其中一些数据难以获取、成本高昂或涉及隐私问题。", "method": "引入一个模块化的多模态框架，利用生成式AI从公开可访问的住宅信息和图像中生成所需数据。同时提供一个演示该框架的管道，并评估其生成式AI组件。", "result": "实验表明，该框架使用AI避免了生成模型中的常见问题，并能生成逼真、带有标签的数据。", "conclusion": "通过减少对昂贵或受限数据源的依赖，该框架为更易访问和可复现的能源建模研究铺平了道路。"}}
{"id": "2509.09750", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09750", "abs": "https://arxiv.org/abs/2509.09750", "authors": ["Hossein Yazdanjouei", "Arash Mansouri", "Mohammad Shokouhifar"], "title": "A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images", "comment": null, "summary": "This study proposes a semi-supervised co-training framework for object\ndetection in densely packed retail environments, where limited labeled data and\ncomplex conditions pose major challenges. The framework combines Faster R-CNN\n(utilizing a ResNet backbone) for precise localization with YOLO (employing a\nDarknet backbone) for global context, enabling mutual pseudo-label exchange\nthat improves accuracy in scenes with occlusion and overlapping objects. To\nstrengthen classification, it employs an ensemble of XGBoost, Random Forest,\nand SVM, utilizing diverse feature representations for higher robustness.\nHyperparameters are optimized using a metaheuristic-driven algorithm, enhancing\nprecision and efficiency across models. By minimizing reliance on manual\nlabeling, the approach reduces annotation costs and adapts effectively to\nfrequent product and layout changes common in retail. Experiments on the\nSKU-110k dataset demonstrate strong performance, highlighting the scalability\nand practicality of the proposed framework for real-world retail applications\nsuch as automated inventory tracking, product monitoring, and checkout systems.", "AI": {"tldr": "本研究提出了一种半监督协同训练框架，用于密集零售环境中的目标检测，结合Faster R-CNN和YOLO进行伪标签交换，并使用集成分类器和元启发式优化，以减少手动标注并提高复杂场景下的准确性。", "motivation": "在密集零售环境中，标记数据有限、条件复杂（如遮挡和物体重叠）是主要挑战，且手动标注成本高昂，难以适应频繁的产品和布局变化。", "method": "该框架采用半监督协同训练方法。它结合了Faster R-CNN（使用ResNet骨干网络）进行精确本地化和YOLO（使用Darknet骨干网络）获取全局上下文，两者通过相互伪标签交换来提高准确性。为加强分类，采用XGBoost、Random Forest和SVM的集成模型，利用多样化的特征表示提高鲁棒性。超参数通过元启发式算法进行优化，以提高模型精度和效率。", "result": "在SKU-110k数据集上的实验表明，该框架表现出色，尤其在有遮挡和重叠物体的场景中提高了准确性。它显著减少了对人工标注的依赖，降低了标注成本，并能有效适应零售环境中频繁的产品和布局变化。", "conclusion": "所提出的框架具有良好的可扩展性和实用性，适用于自动化库存跟踪、产品监控和结账系统等现实世界零售应用。"}}
{"id": "2509.09953", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09953", "abs": "https://arxiv.org/abs/2509.09953", "authors": ["Mahfuzul I. Nissan", "Sharmin Aktar"], "title": "Detection of Anomalous Behavior in Robot Systems Based on Machine Learning", "comment": null, "summary": "Ensuring the safe and reliable operation of robotic systems is paramount to\nprevent potential disasters and safeguard human well-being. Despite rigorous\ndesign and engineering practices, these systems can still experience\nmalfunctions, leading to safety risks. In this study, we present a machine\nlearning-based approach for detecting anomalies in system logs to enhance the\nsafety and reliability of robotic systems. We collected logs from two distinct\nscenarios using CoppeliaSim and comparatively evaluated several machine\nlearning models, including Logistic Regression (LR), Support Vector Machine\n(SVM), and an Autoencoder. Our system was evaluated in a quadcopter context\n(Context 1) and a Pioneer robot context (Context 2). Results showed that while\nLR demonstrated superior performance in Context 1, the Autoencoder model proved\nto be the most effective in Context 2. This highlights that the optimal model\nchoice is context-dependent, likely due to the varying complexity of anomalies\nacross different robotic platforms. This research underscores the value of a\ncomparative approach and demonstrates the particular strengths of autoencoders\nfor detecting complex anomalies in robotic systems.", "AI": {"tldr": "本研究提出一种基于机器学习的方法，通过分析系统日志来检测机器人系统中的异常，以提高其安全性和可靠性。在两种不同的机器人场景中比较了多种模型，发现最佳模型选择取决于具体情境，其中自编码器在检测复杂异常方面表现出色。", "motivation": "确保机器人系统的安全可靠运行至关重要，以防止潜在灾难并保障人类福祉。尽管有严格的设计和工程实践，这些系统仍可能发生故障，导致安全风险。", "method": "采用基于机器学习的方法检测系统日志中的异常。使用CoppeliaSim从四旋翼飞行器（情境1）和Pioneer机器人（情境2）两种不同场景收集日志。比较评估了多种机器学习模型，包括逻辑回归（LR）、支持向量机（SVM）和自编码器。", "result": "结果显示，逻辑回归在情境1中表现优异，而自编码器模型在情境2中最为有效。这表明最佳模型选择是情境依赖的，可能因为不同机器人平台的异常复杂性各异。", "conclusion": "本研究强调了比较方法的价值，并证明了自编码器在检测机器人系统中复杂异常方面的特殊优势。"}}
{"id": "2509.09863", "categories": ["eess.SY", "cs.LG", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.09863", "abs": "https://arxiv.org/abs/2509.09863", "authors": ["Sarvan Gill", "Daniela Constantinescu"], "title": "Off Policy Lyapunov Stability in Reinforcement Learning", "comment": "Conference on Robot Learning (CORL) 2025", "summary": "Traditional reinforcement learning lacks the ability to provide stability\nguarantees. More recent algorithms learn Lyapunov functions alongside the\ncontrol policies to ensure stable learning. However, the current self-learned\nLyapunov functions are sample inefficient due to their on-policy nature. This\npaper introduces a method for learning Lyapunov functions off-policy and\nincorporates the proposed off-policy Lyapunov function into the Soft Actor\nCritic and Proximal Policy Optimization algorithms to provide them with a data\nefficient stability certificate. Simulations of an inverted pendulum and a\nquadrotor illustrate the improved performance of the two algorithms when\nendowed with the proposed off-policy Lyapunov function.", "AI": {"tldr": "本文提出了一种离策略Lyapunov函数学习方法，并将其集成到SAC和PPO算法中，以提供数据高效的稳定性保证，从而提高了算法性能。", "motivation": "传统的强化学习算法缺乏稳定性保证。虽然近期算法通过学习Lyapunov函数来确保稳定学习，但现有的自学习Lyapunov函数由于其在策略（on-policy）性质而导致样本效率低下。", "method": "本文引入了一种离策略（off-policy）学习Lyapunov函数的方法，并将其整合到Soft Actor Critic (SAC) 和 Proximal Policy Optimization (PPO) 算法中。", "result": "在倒立摆和四旋翼飞行器仿真中，当SAC和PPO算法配备了所提出的离策略Lyapunov函数后，其性能得到了显著提升。", "conclusion": "所提出的离策略Lyapunov函数能够为强化学习算法（如SAC和PPO）提供数据高效的稳定性证明，并有效提升其性能。"}}
{"id": "2509.10125", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.10125", "abs": "https://arxiv.org/abs/2509.10125", "authors": ["Madina Kojanazarova", "Sidady El Hadramy", "Jack Wilkie", "Georg Rauter", "Philippe C. Cattin"], "title": "Soft Tissue Simulation and Force Estimation from Heterogeneous Structures using Equivariant Graph Neural Networks", "comment": null, "summary": "Accurately simulating soft tissue deformation is crucial for surgical\ntraining, pre-operative planning, and real-time haptic feedback systems. While\nphysics-based models such as the finite element method (FEM) provide\nhigh-fidelity results, they are often computationally expensive and require\nextensive preprocessing. We propose a graph neural network (GNN) architecture\nthat predicts both tissue surface deformation and applied force from sparse\npoint clouds. The model incorporates internal anatomical information through\nbinary tissue profiles beneath each point and leverages E(n)-equivariant\nmessage passing to improve robustness. We collected experimental data that\ncomprises a real silicone and bone-like phantom, and complemented it with\nsynthetic simulations generated using FEM. Our model achieves a comparable\nperformance to a baseline GNN on standard test cases and significantly\noutperforms it in rotated and cross-resolution scenarios, showing a strong\ngeneralization to unseen orientations and point densities. It also achieves a\nsignificant speed improvement, offering a solution for real-time applications.\nWhen fine-tuned on experimental data, the model maintains sub-millimeter\ndeformation accuracy despite limited sample size and measurement noise. The\nresults demonstrate that our approach offers an efficient, data-driven\nalternative to traditional simulations, capable of generalizing across\nanatomical configurations and supporting interactive surgical environments.", "AI": {"tldr": "该研究提出了一种图神经网络（GNN）模型，用于从稀疏点云预测软组织形变和作用力，其性能与传统物理模型相当，但在实时性、泛化能力和鲁棒性方面表现更优，适用于交互式手术环境。", "motivation": "在手术训练、术前规划和实时触觉反馈系统中，准确模拟软组织形变至关重要。然而，有限元法（FEM）等基于物理的模型虽然精度高，但计算成本昂贵且需要大量预处理，难以满足实时应用的需求。", "method": "该研究提出了一种图神经网络（GNN）架构，能够从稀疏点云预测组织表面形变和作用力。该模型通过每个点下方的二元组织剖面整合内部解剖信息，并利用E(n)等变消息传递来提高鲁棒性。研究团队收集了真实的硅胶和类骨模型实验数据，并辅以有限元法生成的合成模拟数据进行训练和验证。", "result": "该模型在标准测试案例上达到了与基线GNN相当的性能，但在旋转和跨分辨率场景中显著优于基线模型，显示出对未知方向和点密度的强大泛化能力。它还实现了显著的速度提升，为实时应用提供了解决方案。当在有限的实验数据上进行微调时，尽管样本量有限且存在测量噪声，模型仍保持了亚毫米级的形变精度。", "conclusion": "该方法提供了一种高效、数据驱动的替代方案，可替代传统模拟方法，能够泛化到不同的解剖配置，并支持交互式手术环境，为实时应用提供了可行方案。"}}
{"id": "2509.09703", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09703", "abs": "https://arxiv.org/abs/2509.09703", "authors": ["Zhenhua Xu", "Xixiang Zhao", "Xubin Yue", "Shengwei Tian", "Changting Lin", "Meng Han"], "title": "CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor", "comment": "Accepted by EMNLP2025 MainConference", "summary": "The widespread deployment of large language models (LLMs) has intensified\nconcerns around intellectual property (IP) protection, as model theft and\nunauthorized redistribution become increasingly feasible. To address this,\nmodel fingerprinting aims to embed verifiable ownership traces into LLMs.\nHowever, existing methods face inherent trade-offs between stealthness,\nrobustness, and generalizability, being either detectable via distributional\nshifts, vulnerable to adversarial modifications, or easily invalidated once the\nfingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven\nfingerprinting framework that encodes contextual correlations across multiple\ndialogue turns, such as counterfactual, rather than relying on token-level or\nsingle-turn triggers. CTCC enables fingerprint verification under black-box\naccess while mitigating false positives and fingerprint leakage, supporting\ncontinuous construction under a shared semantic rule even if partial triggers\nare exposed. Extensive experiments across multiple LLM architectures\ndemonstrate that CTCC consistently achieves stronger stealth and robustness\nthan prior work. Our findings position CTCC as a reliable and practical\nsolution for ownership verification in real-world LLM deployment scenarios. Our\ncode and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.", "AI": {"tldr": "CTCC是一个新颖的规则驱动指纹框架，通过编码多轮对话中的上下文关联（如反事实）来保护大型语言模型的知识产权，解决了现有方法的隐蔽性、鲁棒性和泛化性权衡问题。", "motivation": "大型语言模型的广泛部署加剧了知识产权保护的担忧，因为模型盗窃和未经授权的再分发变得越来越容易。现有模型指纹方法在隐蔽性、鲁棒性和泛化性之间存在固有的权衡，容易被检测、易受攻击或一旦指纹暴露就会失效。", "method": "本文提出了CTCC框架，一个规则驱动的指纹方法。它通过编码跨多个对话轮次的上下文关联（例如反事实），而不是依赖于标记级或单轮触发器来嵌入指纹。CTCC支持在黑盒访问下进行指纹验证，同时减少误报和指纹泄露，并支持在部分触发器暴露的情况下，基于共享语义规则进行持续构建。", "result": "在多种LLM架构上的广泛实验表明，CTCC始终比现有工作实现更强的隐蔽性和鲁棒性。", "conclusion": "CTCC被定位为在实际LLM部署场景中进行所有权验证的可靠且实用的解决方案。"}}
{"id": "2509.09810", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09810", "abs": "https://arxiv.org/abs/2509.09810", "authors": ["Agnieszka Mensfelt", "David Tena Cucala", "Santiago Franco", "Angeliki Koutsoukou-Argyraki", "Vince Trencsenyi", "Kostas Stathis"], "title": "Towards a Common Framework for Autoformalization", "comment": null, "summary": "Autoformalization has emerged as a term referring to the automation of\nformalization - specifically, the formalization of mathematics using\ninteractive theorem provers (proof assistants). Its rapid development has been\ndriven by progress in deep learning, especially large language models (LLMs).\nMore recently, the term has expanded beyond mathematics to describe the broader\ntask of translating informal input into formal logical representations. At the\nsame time, a growing body of research explores using LLMs to translate informal\nlanguage into formal representations for reasoning, planning, and knowledge\nrepresentation - often without explicitly referring to this process as\nautoformalization. As a result, despite addressing similar tasks, the largely\nindependent development of these research areas has limited opportunities for\nshared methodologies, benchmarks, and theoretical frameworks that could\naccelerate progress. The goal of this paper is to review - explicit or implicit\n- instances of what can be considered autoformalization and to propose a\nunified framework, encouraging cross-pollination between different fields to\nadvance the development of next generation AI systems.", "AI": {"tldr": "本文回顾了自动形式化（广义）的概念，并提出了一个统一框架，旨在整合由大型语言模型驱动的不同研究领域，以加速人工智能的发展。", "motivation": "尽管有相似的任务，但将非正式输入转化为形式逻辑表示的不同研究领域（包括数学形式化和LLM驱动的推理/规划）独立发展，限制了方法、基准和理论框架的共享，从而阻碍了进展。", "method": "通过审查显性或隐性的自动形式化实例，并提出一个统一的框架。", "result": "论文提供了对自动形式化相关研究的综述，并提出了一个旨在促进跨领域交流的统一框架。", "conclusion": "统一这些研究领域将促进跨学科交流，加速下一代人工智能系统的发展。"}}
{"id": "2509.09785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09785", "abs": "https://arxiv.org/abs/2509.09785", "authors": ["Moslem Yazdanpanah", "Ali Bahri", "Mehrdad Noori", "Sahar Dastani", "Gustavo Adolfo Vargas Hakim", "David Osowiechi", "Ismail Ben Ayed", "Christian Desrosiers"], "title": "Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging", "comment": null, "summary": "Test-time adaptation (TTA) is crucial for mitigating performance degradation\ncaused by distribution shifts in 3D point cloud classification. In this work,\nwe introduce Token Purging (PG), a novel backpropagation-free approach that\nremoves tokens highly affected by domain shifts before they reach attention\nlayers. Unlike existing TTA methods, PG operates at the token level, ensuring\nrobust adaptation without iterative updates. We propose two variants: PG-SP,\nwhich leverages source statistics, and PG-SF, a fully source-free version\nrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,\nShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of\n+10.3\\% higher accuracy than state-of-the-art backpropagation-free methods,\nwhile PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is\n12.4 times faster and 5.5 times more memory efficient than our baseline, making\nit suitable for real-world deployment. Code is available at\n\\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}", "AI": {"tldr": "本文提出了一种名为Token Purging (PG) 的新型免反向传播方法，用于3D点云分类中的测试时自适应 (TTA)，通过在注意力层之前移除受领域偏移影响严重的tokens，显著提高了自适应性能和效率。", "motivation": "在3D点云分类中，由于分布偏移导致模型性能下降是一个关键问题，因此需要有效的测试时自适应 (TTA) 方法来缓解这一问题。", "method": "本文引入了Token Purging (PG)，这是一种无需反向传播的创新方法，它在tokens到达注意力层之前，识别并移除受领域偏移严重影响的tokens。PG在token层面操作，无需迭代更新。提出了两种变体：PG-SP（利用源域统计信息）和PG-SF（完全源域无关，依赖于CLS-token驱动的自适应）。", "result": "在ModelNet40-C、ShapeNet-C和ScanObjectNN-C上的广泛评估表明，PG-SP的平均准确率比现有最先进的免反向传播方法高出10.3%。PG-SF在源域无关自适应方面树立了新基准。此外，PG比基线方法快12.4倍，内存效率高5.5倍。", "conclusion": "PG是一种高效、内存友好且性能卓越的3D点云分类测试时自适应方法，能够有效应对分布偏移，适用于实际部署。"}}
{"id": "2509.10007", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10007", "abs": "https://arxiv.org/abs/2509.10007", "authors": ["Samuli Soutukorva", "Markku Suomalainen", "Martin Kollingbaum", "Tapio Heikkilä"], "title": "Gaussian path model library for intuitive robot motion programming by demonstration", "comment": null, "summary": "This paper presents a system for generating Gaussian path models from\nteaching data representing the path shape. In addition, methods for using these\npath models to classify human demonstrations of paths are introduced. By\ngenerating a library of multiple Gaussian path models of various shapes, human\ndemonstrations can be used for intuitive robot motion programming. A method for\nmodifying existing Gaussian path models by demonstration through geometric\nanalysis is also presented.", "AI": {"tldr": "本文提出了一种从示教数据生成高斯路径模型并用于分类人类路径演示的系统，同时介绍了通过几何分析修改现有模型的方法，旨在实现直观的机器人运动编程。", "motivation": "研究动机是希望通过人类演示实现直观的机器人运动编程，使机器人能学习并适应不同的路径形状。", "method": "该研究的方法包括：1) 从代表路径形状的示教数据生成高斯路径模型；2) 利用这些模型对人类路径演示进行分类；3) 构建一个包含多种形状高斯路径模型的库；4) 通过几何分析和演示来修改现有高斯路径模型。", "result": "研究结果是一个能够从示教数据生成高斯路径模型、利用这些模型分类人类路径演示，并通过演示和几何分析修改现有模型的系统，从而实现了通过人类演示进行直观机器人运动编程。", "conclusion": "该系统通过生成、分类和修改高斯路径模型，为机器人运动编程提供了一种直观且灵活的方法，能够根据人类演示学习和适应不同的路径形状。"}}
{"id": "2509.09937", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.09937", "abs": "https://arxiv.org/abs/2509.09937", "authors": ["Wenqi Cui", "Yiheng Xie", "Steven Low", "Adam Wierman", "Baosen Zhang"], "title": "Leveraging Predictions in Power System Voltage Control: An Adaptive Approach", "comment": null, "summary": "High variability of solar PV and sudden changes in load (e.g., electric\nvehicles and storage) can lead to large voltage fluctuations in the\ndistribution system. In recent years, a number of controllers have been\ndesigned to optimize voltage control. These controllers, however, almost always\nassume that the net load in the system remains constant over a sufficiently\nlong time, such that the control actions converge before the load changes\nagain. Given the intermittent and uncertain nature of renewable resources, it\nis becoming important to explicitly consider net load that is time-varying.\n  This paper proposes an adaptive approach to voltage control in power systems\nwith significant time-varying net load. We leverage advances in short-term load\nforecasting, where the net load in the system can be partially predicted using\nlocal measurements. We integrate these predictions into the design of adaptive\ncontrollers, and prove that the overall control architecture achieves\ninput-to-state stability in a decentralized manner. We optimize the control\npolicy through reinforcement learning. Case studies are conducted using\ntime-varying load data from a real-world distribution system.", "AI": {"tldr": "针对太阳能光伏高波动性和负荷突变导致的配电系统电压波动问题，本文提出了一种自适应电压控制方法。该方法结合短期负荷预测和强化学习，实现了去中心化输入-状态稳定性，有效应对时变净负荷。", "motivation": "太阳能光伏的高波动性以及电动汽车和储能等负荷的突然变化，会导致配电系统电压大幅波动。现有电压控制器通常假设系统净负荷在足够长的时间内保持不变，这与可再生能源的间歇性和不确定性不符，因此需要显式考虑时变净负荷。", "method": "本文提出了一种自适应电压控制方法，利用短期负荷预测（通过局部测量预测净负荷）将预测结果整合到自适应控制器设计中。该方法证明了整体控制架构能以去中心化方式实现输入-状态稳定性，并通过强化学习优化控制策略。通过真实配电系统的时变负荷数据进行案例研究。", "result": "所提出的整体控制架构以去中心化方式实现了输入-状态稳定性。", "conclusion": "本文提出了一种结合短期负荷预测和强化学习的自适应去中心化电压控制方法，有效应对了具有显著时变净负荷的电力系统中的电压控制挑战，并从理论上证明了其输入-状态稳定性。"}}
{"id": "2509.10348", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10348", "abs": "https://arxiv.org/abs/2509.10348", "authors": ["Yehudit Aperstein", "Amit Tzahar", "Alon Gottlib", "Tal Verber", "Ravit Shagan Damti", "Alexander Apartsin"], "title": "Multi-pathology Chest X-ray Classification with Rejection Mechanisms", "comment": "12 pages, 4 figures", "summary": "Overconfidence in deep learning models poses a significant risk in\nhigh-stakes medical imaging tasks, particularly in multi-label classification\nof chest X-rays, where multiple co-occurring pathologies must be detected\nsimultaneously. This study introduces an uncertainty-aware framework for chest\nX-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective\nprediction mechanisms: entropy-based rejection and confidence interval-based\nrejection. Both methods enable the model to abstain from uncertain predictions,\nimproving reliability by deferring ambiguous cases to clinical experts. A\nquantile-based calibration procedure is employed to tune rejection thresholds\nusing either global or class-specific strategies. Experiments conducted on\nthree large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)\ndemonstrate that selective rejection improves the trade-off between diagnostic\naccuracy and coverage, with entropy-based rejection yielding the highest\naverage AUC across all pathologies. These results support the integration of\nselective prediction into AI-assisted diagnostic workflows, providing a\npractical step toward safer, uncertainty-aware deployment of deep learning in\nclinical settings.", "AI": {"tldr": "本研究提出了一种基于DenseNet-121的胸部X射线不确定性感知诊断框架，通过引入熵和置信区间选择性拒绝机制，使模型能够放弃不确定预测，从而提高诊断的可靠性，并将模糊病例转交给临床专家处理。", "motivation": "深度学习模型在多标签胸部X射线分类等高风险医学影像任务中过度自信，对同时检测多种并发病理构成重大风险，因此需要提高模型的可靠性并处理不确定性。", "method": "研究采用基于DenseNet-121的骨干网络，并增强了两种选择性预测机制：基于熵的拒绝和基于置信区间的拒绝。通过分位数校准程序（全局或类别特定策略）调整拒绝阈值。实验在PadChest、NIH ChestX-ray14和MIMIC-CXR三个大型公共数据集上进行。", "result": "选择性拒绝改善了诊断准确性和覆盖率之间的权衡。其中，基于熵的拒绝在所有病理上产生了最高的平均AUC。", "conclusion": "研究结果支持将选择性预测集成到AI辅助诊断工作流程中，为在临床环境中安全、不确定性感知地部署深度学习提供了实用步骤。"}}
{"id": "2509.09704", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.09704", "abs": "https://arxiv.org/abs/2509.09704", "authors": ["Ali Mazyaki", "Mohammad Naghizadeh", "Samaneh Ranjkhah Zonouzaghi", "Hossein Setareh"], "title": "Temporal Preferences in Language Models for Long-Horizon Assistance", "comment": null, "summary": "We study whether language models (LMs) exhibit future- versus\npresent-oriented preferences in intertemporal choice and whether those\npreferences can be systematically manipulated. Using adapted human experimental\nprotocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them\nagainst a sample of human decision makers. We introduce an operational metric,\nthe Manipulability of Time Orientation (MTO), defined as the change in an LM's\nrevealed time preference between future- and present-oriented prompts. In our\ntests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)\nchoose later options under future-oriented prompts but only partially\npersonalize decisions across identities or geographies. Moreover, models that\ncorrectly reason about time orientation internalize a future orientation for\nthemselves as AI decision makers. We discuss design implications for AI\nassistants that should align with heterogeneous, long-horizon goals and outline\na research agenda on personalized contextual calibration and socially aware\ndeployment.", "AI": {"tldr": "本研究探讨了语言模型（LMs）在跨期选择中的未来导向或现在导向偏好，以及这些偏好是否可被系统性操纵。结果显示，推理型模型在未来导向提示下表现出未来偏好，并为自身内化了未来导向。", "motivation": "研究动机是理解语言模型在跨期选择中是偏向未来还是现在，以及它们的偏好是否可以被外部提示系统性地改变，这对于设计与人类长期目标一致的AI助手至关重要。", "method": "研究方法包括：1) 采用改编自人类实验协议的时间权衡任务来评估多个语言模型；2) 将语言模型的表现与人类决策者样本进行基准测试；3) 引入“时间导向可操纵性”（MTO）作为衡量指标，定义为模型在未来导向和现在导向提示下时间偏好的变化。", "result": "主要结果包括：1) 推理型模型（如DeepSeek-Reasoner和grok-3-mini）在未来导向提示下选择更晚的选项；2) 这些模型在不同身份或地理位置上的决策个性化程度有限；3) 能够正确理解时间导向的模型，会为自己（作为AI决策者）内化一种未来导向。", "conclusion": "本研究的结论是，语言模型表现出可被操纵的跨期偏好，推理型模型倾向于未来导向。这为AI助手的未来设计提供了启示，强调需要考虑如何使AI与异构的、长期目标保持一致，并提出了关于个性化上下文校准和社会意识部署的研究议程。"}}
{"id": "2509.09848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09848", "abs": "https://arxiv.org/abs/2509.09848", "authors": ["Nana Han", "Dong Liu", "Tomas Norton"], "title": "Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation", "comment": null, "summary": "Large language models (LLMs) are increasingly being recognised as valuable\nknowledge communication tools in many industries. However, their application in\nlivestock farming remains limited, being constrained by several factors not\nleast the availability, diversity and complexity of knowledge sources. This\nstudy introduces an intelligent knowledge assistant system designed to support\nhealth management in farmed goats. Leveraging the Retrieval-Augmented\nGeneration (RAG), two structured knowledge processing methods, table\ntextualization and decision-tree textualization, were proposed to enhance large\nlanguage models' (LLMs) understanding of heterogeneous data formats. Based on\nthese methods, a domain-specific goat farming knowledge base was established to\nimprove LLM's capacity for cross-scenario generalization. The knowledge base\nspans five key domains: Disease Prevention and Treatment, Nutrition Management,\nRearing Management, Goat Milk Management, and Basic Farming Knowledge.\nAdditionally, an online search module is integrated to enable real-time\nretrieval of up-to-date information. To evaluate system performance, six\nablation experiments were conducted to examine the contribution of each\ncomponent. The results demonstrated that heterogeneous knowledge fusion method\nachieved the best results, with mean accuracies of 87.90% on the validation set\nand 84.22% on the test set. Across the text-based, table-based, decision-tree\nbased Q&A tasks, accuracy consistently exceeded 85%, validating the\neffectiveness of structured knowledge fusion within a modular design. Error\nanalysis identified omission as the predominant error category, highlighting\nopportunities to further improve retrieval coverage and context integration. In\nconclusion, the results highlight the robustness and reliability of the\nproposed system for practical applications in goat farming.", "AI": {"tldr": "本研究开发了一个基于RAG的智能知识助手系统，通过表格文本化和决策树文本化等结构化知识处理方法，解决了大语言模型在山羊养殖健康管理中处理异构数据的局限性，并取得了高准确率。", "motivation": "大语言模型在许多行业中被认为是宝贵的知识交流工具，但在畜牧业，尤其是山羊养殖中应用有限，主要受限于知识来源的可用性、多样性和复杂性。", "method": "本研究引入了一个智能知识助手系统以支持山羊养殖的健康管理。该系统利用检索增强生成（RAG），并提出了表格文本化和决策树文本化两种结构化知识处理方法，以增强LLM对异构数据格式的理解。在此基础上，建立了特定领域的山羊养殖知识库（涵盖疾病防治、营养管理、饲养管理、羊奶管理和基础养殖知识五个领域），并集成了在线搜索模块以实现实时信息检索。通过六项消融实验评估了系统性能。", "result": "异构知识融合方法取得了最佳结果，在验证集上的平均准确率为87.90%，在测试集上为84.22%。在基于文本、表格和决策树的问答任务中，准确率始终超过85%，验证了模块化设计中结构化知识融合的有效性。错误分析表明遗漏是主要的错误类别。", "conclusion": "本研究结果突显了所提出的系统在山羊养殖实际应用中的鲁棒性和可靠性。"}}
{"id": "2509.09792", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09792", "abs": "https://arxiv.org/abs/2509.09792", "authors": ["Zimin Xia", "Chenghao Xu", "Alexandre Alahi"], "title": "Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors", "comment": null, "summary": "We propose an accurate and highly interpretable fine-grained cross-view\nlocalization method that estimates the 3 Degrees of Freedom pose of a\nground-level image by matching its local features with a reference aerial\nimage. Previous methods typically transform the ground image into a bird's-eye\nview (BEV) representation and then align it with the aerial image for\nlocalization. However, this transformation often leads to information loss due\nto perspective distortion or compression of height information, thereby\ndegrading alignment quality with the aerial view. In contrast, our method\ndirectly establishes correspondences between ground and aerial images and lifts\nonly the matched keypoints to BEV space using monocular depth prior. Notably,\nmodern depth predictors can provide reliable metric depth when the test samples\nare similar to the training data. When the depth distribution differs, they\nstill produce consistent relative depth, i.e., depth accurate up to an unknown\nscale. Our method supports both metric and relative depth. It employs a\nscale-aware Procrustes alignment to estimate the camera pose from the\ncorrespondences and optionally recover the scale when using relative depth.\nExperimental results demonstrate that, with only weak supervision on camera\npose, our method learns accurate local feature correspondences and achieves\nsuperior localization performance under challenging conditions, such as\ncross-area generalization and unknown orientation. Moreover, our method is\ncompatible with various relative depth models without requiring per-model\nfinetuning. This flexibility, combined with strong localization performance,\nmakes it well-suited for real-world deployment.", "AI": {"tldr": "该论文提出了一种精确且高度可解释的细粒度跨视图定位方法，通过直接匹配地面图像和航空图像的局部特征，并利用单目深度先验将匹配的关键点提升到鸟瞰图空间，来估计地面图像的3自由度姿态。", "motivation": "现有方法通常将地面图像转换为鸟瞰图(BEV)表示，然后与航空图像对齐进行定位。然而，这种转换常因透视失真或高度信息压缩而导致信息丢失，从而降低与航空视图的对齐质量。", "method": "该方法直接在地面图像和航空图像之间建立对应关系，并仅使用单目深度先验将匹配的关键点提升到鸟瞰图空间（支持度量深度和相对深度）。它采用了一种尺度感知的Procrustes对齐方法来从对应关系中估计相机姿态，并在使用相对深度时可选地恢复尺度。", "result": "实验结果表明，在仅有弱相机姿态监督的情况下，该方法学习到准确的局部特征对应关系，并在跨区域泛化和未知方向等挑战性条件下实现了卓越的定位性能。此外，该方法与各种相对深度模型兼容，无需为每个模型进行微调。", "conclusion": "该方法的灵活性和强大的定位性能使其非常适合实际部署，并且在挑战性环境下表现出色，解决了传统BEV转换方法的局限性。"}}
{"id": "2509.10012", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10012", "abs": "https://arxiv.org/abs/2509.10012", "authors": ["Richard Matthias Hartisch", "Alexander Rother", "Jörg Krüger", "Kevin Haninger"], "title": "Towards simulation-based optimization of compliant fingers for high-speed connector assembly", "comment": null, "summary": "Mechanical compliance is a key design parameter for dynamic contact-rich\nmanipulation, affecting task success and safety robustness over contact\ngeometry variation. Design of soft robotic structures, such as compliant\nfingers, requires choosing design parameters which affect geometry and\nstiffness, and therefore manipulation performance and robustness. Today, these\nparameters are chosen through either hardware iteration, which takes\nsignificant development time, or simplified models (e.g. planar), which can't\naddress complex manipulation task objectives. Improvements in dynamic\nsimulation, especially with contact and friction modeling, present a potential\ndesign tool for mechanical compliance. We propose a simulation-based design\ntool for compliant mechanisms which allows design with respect to task-level\nobjectives, such as success rate. This is applied to optimize design parameters\nof a structured compliant finger to reduce failure cases inside a tolerance\nwindow in insertion tasks. The improvement in robustness is then validated on a\nreal robot using tasks from the benchmark NIST task board. The finger stiffness\naffects the tolerance window: optimized parameters can increase tolerable\nranges by a factor of 2.29, with workpiece variation up to 8.6 mm being\ncompensated. However, the trends remain task-specific. In some tasks, the\nhighest stiffness yields the widest tolerable range, whereas in others the\nopposite is observed, motivating need for design tools which can consider\napplication-specific geometry and dynamics.", "AI": {"tldr": "本文提出了一种基于仿真的柔顺机构设计工具，用于优化柔顺手指的设计参数，以提高插入任务在公差范围内的成功率和鲁棒性，并通过真实机器人实验验证了其有效性。", "motivation": "机械柔顺性是动态接触式操作的关键设计参数，影响任务成功率和对接触几何变化的鲁棒性。目前，柔性机器人结构的设计参数选择依赖耗时的硬件迭代或简化的模型，无法有效处理复杂的操纵任务目标。动态仿真（特别是接触和摩擦建模）的进步为机械柔顺性设计提供了潜在工具。", "method": "研究者提出了一种基于仿真的柔顺机构设计工具，允许根据任务级别目标（如成功率）进行设计。该工具被应用于优化结构化柔顺手指的设计参数，以减少插入任务在公差窗口内的失败情况。优化后的鲁棒性通过在真实机器人上使用NIST任务板的基准任务进行验证。", "result": "优化后的参数可以将可容忍范围提高2.29倍，补偿高达8.6毫米的工件变化。然而，趋势是任务特定的：在某些任务中，最高刚度产生最宽的可容忍范围，而在其他任务中则观察到相反的情况。", "conclusion": "基于仿真的设计工具能够有效优化柔顺机构以满足任务级别目标，显著提高鲁棒性。由于最佳柔顺性是任务特定的，因此需要能够考虑应用特定几何形状和动态特性的设计工具。"}}
{"id": "2509.10029", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.10029", "abs": "https://arxiv.org/abs/2509.10029", "authors": ["Dennis Laurijssen", "Wouter Jansen", "Arne Aerts", "Walter Daems", "Jan Steckel"], "title": "Ruggedized Ultrasound Sensing in Harsh Conditions: eRTIS in the wild", "comment": null, "summary": "We present eRTIS, a rugged, embedded ultrasound sensing system for use in\nharsh industrial environments. The system features a broadband capacitive\ntransducer and a 32-element MEMS microphone array capable of 2D and 3D\nbeamforming. A modular hardware architecture separates sensing and processing\ntasks: a high-performance microcontroller handles excitation signal generation\nand data acquisition, while an NVIDIA Jetson module performs GPU-accelerated\nsignal processing. eRTIS supports external synchronization via a custom\ncontroller that powers and coordinates up to six devices, either simultaneously\nor in a defined sequence. Additional synchronization options include\nbidirectional triggering and in-band signal injection. A sealed, anodized\naluminum enclosure with passive cooling and IP-rated connectors ensures\nreliability in challenging conditions. Performance is demonstrated in three\nfield scenarios: harbor mooring, off-road robotics, and autonomous navigation\nin cluttered environments, demonstrates that eRTIS provides robust sensing in\nsituations where optical systems degrade.", "AI": {"tldr": "eRTIS是一种坚固耐用的嵌入式超声波传感系统，专为恶劣工业环境设计，采用宽带电容式换能器和MEMS麦克风阵列，支持2D/3D波束成形，并通过模块化硬件和多种同步选项实现，在实际场景中表现出超越光学系统的鲁棒传感能力。", "motivation": "需要在恶劣工业环境中进行可靠的传感，特别是光学系统性能下降的场景。", "method": "该系统名为eRTIS，集成宽带电容式换能器和32单元MEMS麦克风阵列，支持2D和3D波束成形。采用模块化硬件架构：高性能微控制器负责激励信号生成和数据采集，NVIDIA Jetson模块进行GPU加速信号处理。通过定制控制器（可同步多达六个设备）、双向触发和带内信号注入实现外部同步。系统采用密封、阳极氧化铝外壳，具有被动冷却和IP级连接器，确保在恶劣条件下的可靠性。", "result": "eRTIS在三个实际场景中展示了其性能：港口系泊、越野机器人和杂乱环境中的自主导航。结果表明，eRTIS在光学系统性能下降的情况下也能提供鲁棒的传感能力。", "conclusion": "eRTIS系统能够在光学系统受限的恶劣环境中提供可靠且鲁棒的超声波传感解决方案。"}}
{"id": "2509.10429", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2509.10429", "abs": "https://arxiv.org/abs/2509.10429", "authors": ["Giulia Bassani", "Emilio Maoddi", "Usman Asghar", "Carlo Alberto Avizzano", "Alessandro Filippeschi"], "title": "Human Body Segment Volume Estimation with Two RGB-D Cameras", "comment": "11 pages, 8 figures, 4 tables, to be submitted to IEEE Transactions\n  on Instrumentation and Measurement", "summary": "In the field of human biometry, accurately estimating the volume of the whole\nbody and its individual segments is of fundamental importance. Such\nmeasurements support a wide range of applications that include assessing\nhealth, optimizing ergonomic design, and customizing biomechanical models. In\nthis work, we presented a Body Segment Volume Estimation (BSV) system to\nautomatically compute whole-body and segment volumes using only two RGB-D\ncameras, thus limiting the system complexity. However, to maintain the accuracy\ncomparable to 3D laser scanners, we enhanced the As-Rigid-As-Possible (ARAP)\nnon-rigid registration techniques, disconnecting its energy from the single\ntriangle mesh. Thus, we improved the geometrical coherence of the reconstructed\nmesh, especially in the lateral gap areas. We evaluated BSV starting from the\nRGB-D camera performances, through the results obtained with FAUST dataset\nhuman body models, and comparing with a state-of-the-art work, up to real\nacquisitions. It showed superior ability in accurately estimating human body\nvolumes, and it allows evaluating volume ratios between proximal and distal\nbody segments, which are useful indices in many clinical applications.", "AI": {"tldr": "本文提出了一种名为BSV的身体分段体积估算系统，仅使用两台RGB-D相机，通过改进的ARAP非刚性配准技术，实现了与3D激光扫描仪相当的全身和分段体积估算精度。", "motivation": "在人体生物计量学中，准确估算全身及其各个分段的体积至关重要，这支持健康评估、人体工程学优化和生物力学模型定制等广泛应用。", "method": "该系统使用两台RGB-D相机以降低复杂性。为保持与3D激光扫描仪相当的精度，研究人员增强了As-Rigid-As-Possible (ARAP) 非刚性配准技术，将其能量与单一三角形网格分离，从而改善了重建网格的几何连贯性，尤其是在侧向间隙区域。", "result": "通过评估RGB-D相机性能、FAUST数据集人体模型、与现有先进工作的比较以及实际采集数据，BSV系统在准确估算人体体积方面表现出卓越的能力，并且能够评估近端和远端身体分段之间的体积比，这在许多临床应用中是重要的指标。", "conclusion": "BSV系统能够准确估算人体体积，并提供有用的分段体积比，为临床应用提供有价值的指标，同时通过简化硬件（两台RGB-D相机）保持了高精度，可作为复杂3D激光扫描仪的有效替代方案。"}}
{"id": "2509.09705", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09705", "abs": "https://arxiv.org/abs/2509.09705", "authors": ["Claudio Pinhanez", "Paulo Cavalin", "Cassia Sanctos", "Marcelo Grave", "Yago Primerano"], "title": "The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks", "comment": null, "summary": "This work explores the consistency of small LLMs (2B-8B parameters) in\nanswering multiple times the same question. We present a study on known,\nopen-source LLMs responding to 10 repetitions of questions from the\nmultiple-choice benchmarks MMLU-Redux and MedQA, considering different\ninference temperatures, small vs. medium models (50B-80B), finetuned vs. base\nmodels, and other parameters. We also look into the effects of requiring\nmulti-trial answer consistency on accuracy and the trade-offs involved in\ndeciding which model best provides both of them. To support those studies, we\npropose some new analytical and graphical tools. Results show that the number\nof questions which can be answered consistently vary considerably among models\nbut are typically in the 50%-80% range for small models at low inference\ntemperatures. Also, accuracy among consistent answers seems to reasonably\ncorrelate with overall accuracy. Results for medium-sized models seem to\nindicate much higher levels of answer consistency.", "AI": {"tldr": "本研究探讨了小型LLM（2B-8B）在多次回答相同问题时的一致性，发现其一致性通常在50%-80%之间，且与整体准确性相关，而中型模型的一致性更高。", "motivation": "研究动机是探索小型LLM在重复回答相同问题时的一致性，并分析不同参数（如推理温度、模型大小、是否微调）对一致性及准确性的影响，以及如何权衡两者。", "method": "研究方法包括让已知开源LLM（2B-8B）对MMLU-Redux和MedQA基准测试中的问题重复回答10次。实验考虑了不同的推理温度、小型与中型模型（50B-80B）、微调与基础模型等参数。为支持研究，还提出了新的分析和图形工具。", "result": "结果显示，小型模型在低推理温度下，能一致回答的问题数量差异很大，但通常在50%-80%之间。一致性回答的准确性与整体准确性之间存在合理关联。中型模型的结果表明其答案一致性水平显著更高。", "conclusion": "小型LLM在重复回答相同问题时具有一定的（50%-80%）但可变的答案一致性，且一致性与准确性正相关。中型模型在答案一致性方面表现更优，研究揭示了在模型选择中需权衡一致性和准确性。"}}
{"id": "2509.09867", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09867", "abs": "https://arxiv.org/abs/2509.09867", "authors": ["Yago Romano Matinez", "Jesse Roberts"], "title": "LLMs as Agentic Cooperative Players in Multiplayer UNO", "comment": null, "summary": "LLMs promise to assist humans -- not just by answering questions, but by\noffering useful guidance across a wide range of tasks. But how far does that\nassistance go? Can a large language model based agent actually help someone\naccomplish their goal as an active participant? We test this question by\nengaging an LLM in UNO, a turn-based card game, asking it not to win but\ninstead help another player to do so. We built a tool that allows decoder-only\nLLMs to participate as agents within the RLCard game environment. These models\nreceive full game-state information and respond using simple text prompts under\ntwo distinct prompting strategies. We evaluate models ranging from small (1B\nparameters) to large (70B parameters) and explore how model scale impacts\nperformance. We find that while all models were able to successfully outperform\na random baseline when playing UNO, few were able to significantly aid another\nplayer.", "AI": {"tldr": "本文研究了大型语言模型（LLM）作为活跃参与者协助人类完成任务的能力，特别是在UNO纸牌游戏中帮助另一名玩家获胜。结果显示，虽然所有模型都优于随机基线，但很少有模型能显著帮助另一名玩家。", "motivation": "大型语言模型（LLM）被寄予厚望，不仅能回答问题，还能在各种任务中提供有用的指导，甚至作为活跃参与者协助人类。本文旨在探究LLM代理作为活跃参与者，能否真正帮助他人实现目标。", "method": "研究人员让LLM参与UNO纸牌游戏，但其目标不是获胜，而是帮助另一名玩家获胜。他们开发了一个工具，使仅解码器LLM能作为代理参与RLCard游戏环境。这些模型接收完整的游戏状态信息，并使用两种不同的提示策略通过简单的文本提示进行响应。研究评估了从小型（10亿参数）到大型（700亿参数）的模型，并探讨了模型规模对性能的影响。", "result": "所有模型在玩UNO时都能成功超越随机基线。然而，只有少数模型能够显著帮助另一名玩家获胜。", "conclusion": "尽管大型语言模型在玩UNO时表现优于随机水平，但它们作为活跃参与者显著协助另一名玩家实现目标（即获胜）的能力有限。这表明LLM作为积极协作伙伴的潜力仍有待进一步探索和提升。"}}
{"id": "2509.09808", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09808", "abs": "https://arxiv.org/abs/2509.09808", "authors": ["Judith Massmann", "Alexander Lichtenstein", "Francisco M. López"], "title": "Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test", "comment": "Accepted at IEEE ICDL 2025. 6 pages, 7 figures, 2 tables", "summary": "Numerous visual impairments can be detected in red-eye reflex images from\nyoung children. The so-called Bruckner test is traditionally performed by\nophthalmologists in clinical settings. Thanks to the recent technological\nadvances in smartphones and artificial intelligence, it is now possible to\nrecreate the Bruckner test using a mobile device. In this paper, we present a\nfirst study conducted during the development of KidsVisionCheck, a free\napplication that can perform vision screening with a mobile device using\nred-eye reflex images. The underlying model relies on deep neural networks\ntrained on children's pupil images collected and labeled by an ophthalmologist.\nWith an accuracy of 90% on unseen test data, our model provides highly reliable\nperformance without the necessity of specialist equipment. Furthermore, we can\nidentify the optimal conditions for data collection, which can in turn be used\nto provide immediate feedback to the users. In summary, this work marks a first\nstep toward accessible pediatric vision screenings and early intervention for\nvision abnormalities worldwide.", "AI": {"tldr": "本文介绍了一项初步研究，开发了一款名为KidsVisionCheck的免费移动应用，该应用利用深度神经网络和红眼反射图像，实现了儿童视力筛查，准确率达到90%。", "motivation": "传统的Bruckner测试需要眼科医生在临床环境中进行，而智能手机和人工智能的最新技术进步使得通过移动设备重现该测试成为可能，从而实现更便捷的儿童视力筛查和早期干预。", "method": "该研究开发了一款名为KidsVisionCheck的移动应用，其核心模型基于深度神经网络。该网络使用由眼科医生收集和标记的儿童瞳孔红眼反射图像进行训练。", "result": "模型在未见过的测试数据上达到了90%的准确率，提供了高度可靠的性能，且无需专业设备。此外，研究还确定了最佳数据收集条件，可用于向用户提供即时反馈。", "conclusion": "这项工作标志着全球范围内普及儿科视力筛查和早期干预视力异常的第一步，使其更加便捷和可及。"}}
{"id": "2509.10032", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10032", "abs": "https://arxiv.org/abs/2509.10032", "authors": ["Marawan Khalil", "Fabian Arzberger", "Andreas Nüchter"], "title": "Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping", "comment": "6 Pages, 9 figures, International Workshop 3D-AdViCE in conjunction\n  with 12th ECMR 2025", "summary": "Spherical robots offer unique advantages for mapping applications in\nhazardous or confined environments, thanks to their protective shells and\nomnidirectional mobility. This work presents two complementary spherical\nmapping systems: a lightweight, non-actuated design and an actuated variant\nfeaturing internal pendulum-driven locomotion. Both systems are equipped with a\nLivox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)\nalgorithms on resource-constrained hardware. We assess the mapping accuracy of\nthese systems by comparing the resulting 3D point-clouds from the LIO\nalgorithms to a ground truth map. The results indicate that the performance of\nstate-of-the-art LIO algorithms deteriorates due to the high dynamic movement\nintroduced by the spherical locomotion, leading to globally inconsistent maps\nand sometimes unrecoverable drift.", "AI": {"tldr": "本文提出了两种球形机器人测绘系统（非驱动和摆锤驱动），它们配备LiDAR并运行LIO算法。研究发现，球形运动引入的高动态性导致LIO算法性能下降，产生不一致的地图和不可恢复的漂移。", "motivation": "球形机器人在危险或受限环境中进行测绘具有独特优势，得益于其保护性外壳和全向移动能力。", "method": "研究开发了两种互补的球形测绘系统：一种是轻量级、非驱动设计；另一种是带有内部摆锤驱动的执行版本。两种系统均配备Livox Mid-360固态LiDAR传感器，并在资源受限硬件上运行LiDAR-惯性里程计（LIO）算法。通过将LIO算法生成的3D点云与真实地图进行比较，评估了这些系统的测绘精度。", "result": "研究结果表明，由于球形运动引入的高动态性，最先进的LIO算法性能会下降，导致全局不一致的地图，有时还会出现不可恢复的漂移。", "conclusion": "球形机器人运动的高动态性对LIO算法的性能产生了负面影响，导致测绘精度降低，地图不一致且可能出现严重漂移。这表明LIO算法在处理此类动态运动时面临挑战。"}}
{"id": "2509.10044", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.10044", "abs": "https://arxiv.org/abs/2509.10044", "authors": ["Jorge Ventura", "Jaroslav Hrdina", "Aleš Návrat", "Marek Stodola", "Ahmad Eid", "Santiago Sanchez-Acevedo", "Francisco G. Montoya"], "title": "Understanding the Geometry of Faulted Power Systems under High Penetration of Inverter-Based Resources via Ellipse Fitting and Geometric Algebra", "comment": null, "summary": "Power systems with high penetration of inverter-based resources (IBR) present\nsignificant challenges for conventional protection schemes, with traditional\ndistance protection methods failing to detect line-to-line faults during\nasymmetric conditions. This paper presents a methodology for electrical fault\ndetection and classification using ellipse fitting and geometric algebra\napplied to voltage and current space curves. The approach characterizes\nelectrical faults by fitting ellipses to voltage vector data, enabling fault\ndetection with only a quarter-cycle. The method employs bivector components for\nline-to-ground fault classification, while ellipse parameters identify\nline-to-line and three-phase faults. The geometric representation preserves\nvoltage or current curve shapes in three-dimensional space, overcoming Clarke\ntransform limitations when zero-sequence components are present. Validation\nusing simulations and laboratory experiments demonstrates accurate fault\nidentification and magnitude estimation, providing enhanced power system\nprotection capabilities.", "AI": {"tldr": "本文提出了一种基于椭圆拟合和几何代数的新方法，用于在高逆变器并网资源（IBR）电力系统中，快速准确地检测和分类电气故障，克服了传统保护方案的局限性。", "motivation": "在高IBR渗透率的电力系统中，传统保护方案（特别是距离保护）在不对称条件下无法检测相间故障，这构成了重大挑战。", "method": "该方法通过将椭圆拟合到电压矢量数据来表征电气故障，仅需四分之一周期即可检测故障。它使用双矢量分量进行接地故障分类，而椭圆参数用于识别相间和三相故障。几何表示法保留了三维空间中的电压或电流曲线形状，克服了存在零序分量时Clarke变换的局限性。", "result": "通过仿真和实验室实验验证，该方法能够准确识别故障并估计其大小，且检测速度快（仅需四分之一周期）。", "conclusion": "该方法为电力系统保护提供了增强的能力，克服了传统保护方案在高IBR系统中的不足，实现了快速准确的故障检测和分类。"}}
{"id": "2509.10021", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.10021", "abs": "https://arxiv.org/abs/2509.10021", "authors": ["Jonas Kühne", "Christian Vogt", "Michele Magno", "Luca Benini"], "title": "Efficient and Accurate Downfacing Visual Inertial Odometry", "comment": "This article has been accepted for publication in the IEEE Internet\n  of Things Journal (IoT-J)", "summary": "Visual Inertial Odometry (VIO) is a widely used computer vision method that\ndetermines an agent's movement through a camera and an IMU sensor. This paper\npresents an efficient and accurate VIO pipeline optimized for applications on\nmicro- and nano-UAVs. The proposed design incorporates state-of-the-art feature\ndetection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and\nquantized for emerging RISC-V-based ultra-low-power parallel systems on chips\n(SoCs). Furthermore, by employing a rigid body motion model, the pipeline\nreduces estimation errors and achieves improved accuracy in planar motion\nscenarios. The pipeline's suitability for real-time VIO is assessed on an\nultra-low-power SoC in terms of compute requirements and tracking accuracy\nafter quantization. The pipeline, including the three feature tracking methods,\nwas implemented on the SoC for real-world validation. This design bridges the\ngap between high-accuracy VIO pipelines that are traditionally run on\ncomputationally powerful systems and lightweight implementations suitable for\nmicrocontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates\nan average reduction in RMSE of up to a factor of 3.65x over the baseline\npipeline when using the ORB feature tracker. The analysis of the computational\ncomplexity of the feature trackers further shows that PX4FLOW achieves on-par\ntracking accuracy with ORB at a lower runtime for movement speeds below 24\npixels/frame.", "AI": {"tldr": "本文提出了一种针对微型和纳米无人机优化的、高效且高精度的视觉惯性里程计（VIO）流水线。该流水线集成了SuperPoint、PX4FLOW和ORB等先进特征跟踪方法，并针对基于RISC-V的超低功耗片上系统（SoC）进行了优化和量化，显著降低了估计误差，实现了高精度VIO在资源受限平台上的应用。", "motivation": "现有高精度VIO流水线通常需要在计算能力强大的系统上运行，而微型和纳米无人机等微控制器平台对计算资源有严格限制。因此，研究人员旨在弥合高精度VIO与轻量级、低功耗实现之间的差距，使其能在超低功耗SoC上高效运行。", "method": "该研究采用以下方法：\n1. 设计了一个高效准确的VIO流水线。\n2. 集成了SuperPoint、PX4FLOW和ORB等先进的特征检测和跟踪方法。\n3. 对这些方法进行了优化和量化，以适应新兴的基于RISC-V的超低功耗并行片上系统（SoCs）。\n4. 采用刚体运动模型，以减少平面运动场景下的估计误差并提高精度。\n5. 在GAP9低功耗SoC上对流水线的计算需求和量化后的跟踪精度进行了评估和实时验证。", "result": "主要结果如下：\n1. 优化后的流水线在GAP9低功耗SoC上运行时，使用ORB特征跟踪器时，均方根误差（RMSE）平均降低了3.65倍。\n2. 对特征跟踪器的计算复杂度分析表明，当运动速度低于24像素/帧时，PX4FLOW在较低的运行时长下能达到与ORB相当的跟踪精度。", "conclusion": "该设计成功弥合了传统上运行在高性能系统上的高精度VIO流水线与适用于微控制器的轻量级实现之间的鸿沟。该优化的流水线在超低功耗SoC上实现了显著的精度提升，并为微型和纳米无人机等资源受限平台提供了实时、高精度的VIO解决方案。"}}
{"id": "2509.09708", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09708", "abs": "https://arxiv.org/abs/2509.09708", "authors": ["Nirmalendu Prakash", "Yeo Wei Jie", "Amir Abdullah", "Ranjan Satapathy", "Erik Cambria", "Roy Ka Wei Lee"], "title": "Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal", "comment": null, "summary": "Refusal on harmful prompts is a key safety behaviour in instruction-tuned\nlarge language models (LLMs), yet the internal causes of this behaviour remain\npoorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT\nand LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on\nresidual-stream activations. Given a harmful prompt, we search the SAE latent\nspace for feature sets whose ablation flips the model from refusal to\ncompliance, demonstrating causal influence and creating a jailbreak. Our search\nproceeds in three stages: (1) Refusal Direction: find a refusal-mediating\ndirection and collect SAE features near that direction; (2) Greedy Filtering:\nprune to a minimal set; and (3) Interaction Discovery: fit a factorization\nmachine (FM) that captures nonlinear interactions among the remaining active\nfeatures and the minimal set. This pipeline yields a broad set of\njailbreak-critical features, offering insight into the mechanistic basis of\nrefusal. Moreover, we find evidence of redundant features that remain dormant\nunless earlier features are suppressed. Our findings highlight the potential\nfor fine-grained auditing and targeted intervention in safety behaviours by\nmanipulating the interpretable latent space.", "AI": {"tldr": "本研究利用稀疏自编码器（SAEs）分析大型语言模型（LLMs）拒绝有害提示的内部机制，通过识别并消融关键特征来使模型从拒绝转变为顺从，从而实现“越狱”，并揭示了拒绝行为的深层机制。", "motivation": "尽管LLMs拒绝有害提示是其关键的安全行为，但这种行为的内部原因仍知之甚少。", "method": "研究使用了Gemma-2-2B-IT和LLaMA-3.1-8B-IT模型，并在残差流激活上训练了稀疏自编码器（SAEs）。通过在SAE潜在空间中搜索，寻找消融后能使模型从拒绝转变为顺从的特征集。该搜索分三阶段进行：1) 拒绝方向：找到介导拒绝的方向并收集附近SAE特征；2) 贪婪过滤：修剪至最小特征集；3) 交互发现：拟合分解机（FM）以捕获剩余活跃特征与最小特征集之间的非线性交互。", "result": "该方法识别出大量对“越狱”至关重要的特征，深入揭示了拒绝行为的机械基础。此外，研究发现存在冗余特征，这些特征在早期特征被抑制时才会激活。", "conclusion": "研究结果强调了通过操纵可解释的潜在空间，对LLMs安全行为进行细粒度审计和有针对性干预的潜力。"}}
{"id": "2509.09915", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2509.09915", "abs": "https://arxiv.org/abs/2509.09915", "authors": ["Woong Shin", "Renan Souza", "Daniel Rosendo", "Frédéric Suter", "Feiyi Wang", "Prasanna Balaprakash", "Rafael Ferreira da Silva"], "title": "The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science", "comment": null, "summary": "Modern scientific discovery increasingly requires coordinating distributed\nfacilities and heterogeneous resources, forcing researchers to act as manual\nworkflow coordinators rather than scientists. Advances in AI leading to AI\nagents show exciting new opportunities that can accelerate scientific discovery\nby providing intelligence as a component in the ecosystem. However, it is\nunclear how this new capability would materialize and integrate in the real\nworld. To address this, we propose a conceptual framework where workflows\nevolve along two dimensions which are intelligence (from static to intelligent)\nand composition (from single to swarm) to chart an evolutionary path from\ncurrent workflow management systems to fully autonomous, distributed scientific\nlaboratories. With these trajectories in mind, we present an architectural\nblueprint that can help the community take the next steps towards harnessing\nthe opportunities in autonomous science with the potential for 100x discovery\nacceleration and transformational scientific workflows.", "AI": {"tldr": "本文提出一个概念框架和架构蓝图，旨在利用AI智能体将科学工作流从手动协调转变为自主、分布式实验室，以加速科学发现。", "motivation": "现代科学发现需要协调分布式和异构资源，使研究人员耗费精力在工作流协调而非科学研究上。AI智能体带来了加速科学发现的新机遇，但其在实际中的实现和整合方式尚不明确。", "method": "作者提出了一个概念框架，描述了工作流在“智能性”（从静态到智能）和“组合性”（从单一到群体）两个维度上的演进路径。在此基础上，他们进一步提出了一个架构蓝图。", "result": "提出了一个从当前工作流管理系统到完全自主、分布式科学实验室的演进路径概念框架，以及一个具体的架构蓝图。该蓝图有望实现100倍的发现加速和变革性的科学工作流。", "conclusion": "该概念框架和架构蓝图为社区提供了下一步的方向，以利用自主科学的机遇，加速科学发现并实现变革性的科学工作流。"}}
{"id": "2509.09828", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.09828", "abs": "https://arxiv.org/abs/2509.09828", "authors": ["Tim Broedermannn", "Christos Sakaridis", "Luigi Piccinelli", "Wim Abbeloos", "Luc Van Gool"], "title": "DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception", "comment": "Code and models will be available at\n  https://github.com/timbroed/DGFusion", "summary": "Robust semantic perception for autonomous vehicles relies on effectively\ncombining multiple sensors with complementary strengths and weaknesses.\nState-of-the-art sensor fusion approaches to semantic perception often treat\nsensor data uniformly across the spatial extent of the input, which hinders\nperformance when faced with challenging conditions. By contrast, we propose a\nnovel depth-guided multimodal fusion method that upgrades condition-aware\nfusion by integrating depth information. Our network, DGFusion, poses\nmultimodal segmentation as a multi-task problem, utilizing the lidar\nmeasurements, which are typically available in outdoor sensor suites, both as\none of the model's inputs and as ground truth for learning depth. Our\ncorresponding auxiliary depth head helps to learn depth-aware features, which\nare encoded into spatially varying local depth tokens that condition our\nattentive cross-modal fusion. Together with a global condition token, these\nlocal depth tokens dynamically adapt sensor fusion to the spatially varying\nreliability of each sensor across the scene, which largely depends on depth. In\naddition, we propose a robust loss for our depth, which is essential for\nlearning from lidar inputs that are typically sparse and noisy in adverse\nconditions. Our method achieves state-of-the-art panoptic and semantic\nsegmentation performance on the challenging MUSES and DELIVER datasets. Code\nand models will be available at https://github.com/timbroed/DGFusion", "AI": {"tldr": "本文提出了一种名为DGFusion的深度引导多模态融合方法，通过整合深度信息来提升自动驾驶车辆在挑战性条件下的语义感知鲁棒性，并在多任务学习框架下实现动态的、空间可变的传感器融合。", "motivation": "自动驾驶车辆的鲁棒语义感知需要有效结合多种传感器，但现有的传感器融合方法通常对输入数据的空间范围进行统一处理，这在面临挑战性条件时会影响性能。", "method": "DGFusion将多模态分割视为一个多任务问题，利用激光雷达测量作为模型输入和深度学习的真值。它引入了一个辅助深度头来学习深度感知特征，这些特征被编码成空间可变的局部深度令牌，并结合一个全局条件令牌，动态地根据场景中传感器依赖于深度的空间可变可靠性来调整传感器融合。此外，还提出了一种鲁棒的深度损失函数，以应对稀疏且嘈杂的激光雷达输入。", "result": "DGFusion在具有挑战性的MUSES和DELIVER数据集上实现了最先进的全景和语义分割性能。", "conclusion": "通过引入深度引导的多模态融合，DGFusion能够动态适应传感器在场景中不同深度的可靠性，显著提升了自动驾驶车辆在复杂条件下的语义感知鲁棒性和准确性。"}}
{"id": "2509.10063", "categories": ["cs.RO", "cs.AI", "I.2.9"], "pdf": "https://arxiv.org/pdf/2509.10063", "abs": "https://arxiv.org/abs/2509.10063", "authors": ["Xiyan Huang", "Zhe Xu", "Chenxi Xiao"], "title": "TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model", "comment": "7 pages, 9 figures, 1 table, to be published in IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Robot skill acquisition processes driven by reinforcement learning often rely\non simulations to efficiently generate large-scale interaction data. However,\nthe absence of simulation models for tactile sensors has hindered the use of\ntactile sensing in such skill learning processes, limiting the development of\neffective policies driven by tactile perception. To bridge this gap, we present\nTwinTac, a system that combines the design of a physical tactile sensor with\nits digital twin model. Our hardware sensor is designed for high sensitivity\nand a wide measurement range, enabling high quality sensing data essential for\nobject interaction tasks. Building upon the hardware sensor, we develop the\ndigital twin model using a real-to-sim approach. This involves collecting\nsynchronized cross-domain data, including finite element method results and the\nphysical sensor's outputs, and then training neural networks to map simulated\ndata to real sensor responses. Through experimental evaluation, we\ncharacterized the sensitivity of the physical sensor and demonstrated the\nconsistency of the digital twin in replicating the physical sensor's output.\nFurthermore, by conducting an object classification task, we showed that\nsimulation data generated by our digital twin sensor can effectively augment\nreal-world data, leading to improved accuracy. These results highlight\nTwinTac's potential to bridge the gap in cross-domain learning tasks.", "AI": {"tldr": "本文提出TwinTac系统，结合高灵敏度物理触觉传感器及其数字孪生模型，旨在弥补触觉传感器仿真模型缺失的空白，以支持机器人强化学习中的触觉感知。", "motivation": "机器人技能习得的强化学习过程依赖仿真数据，但缺乏触觉传感器的仿真模型，阻碍了触觉感知在技能学习中的应用，限制了基于触觉的有效策略发展。", "method": "研究人员首先设计了一种高灵敏度和宽测量范围的物理触觉传感器。然后，采用“从真实到仿真”的方法构建数字孪生模型，通过收集同步的跨域数据（包括有限元分析结果和物理传感器输出），并训练神经网络将仿真数据映射到真实的传感器响应。", "result": "实验评估表明，物理传感器具有高灵敏度，数字孪生模型能够一致地复现物理传感器的输出。此外，在物体分类任务中，数字孪生传感器生成的仿真数据有效增强了真实世界数据，提高了分类准确性。", "conclusion": "TwinTac系统通过提供物理触觉传感器及其数字孪生模型，有效弥补了跨域学习任务中触觉感知的空白，有望推动基于触觉感知的机器人强化学习发展。"}}
{"id": "2509.10055", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.10055", "abs": "https://arxiv.org/abs/2509.10055", "authors": ["Xicheng Wang", "Yun. Feng", "Dmitry Grishchenko", "Pavel Kudinov", "Ruifeng Tian", "Sichao Tan"], "title": "Data-driven optimization of sparse sensor placement in thermal hydraulic experiments", "comment": null, "summary": "Thermal-Hydraulic (TH) experiments provide valuable insight into the physics\nof heat and mass transfer and qualified data for code development, calibration\nand validation. However, measurements are typically collected from sparsely\ndistributed sensors, offering limited coverage over the domain of interest and\nphenomena of interest. Determination of the spatial configuration of these\nsensors is crucial and challenging during the pre-test design stage. This paper\ndevelops a data-driven framework for optimizing sensor placement in TH\nexperiments, including (i) a sensitivity analysis to construct datasets, (ii)\nProper Orthogonal Decomposition (POD) for dimensionality reduction, and (iii)\nQR factorization with column pivoting to determine optimal sensor configuration\nunder spatial constraints. The framework is demonstrated on a test conducted in\nthe TALL-3D Lead-bismuth eutectic (LBE) loop. In this case, the utilization of\noptical techniques, such as Particle Image Velocimetry (PIV), are impractical.\nThereby the quantification of momentum and energy transport relies heavily on\nreadings from Thermocouples (TCs). The test section was previously instrumented\nwith many TCs determined through a manual process combining simulation results\nwith expert judgement. The proposed framework provides a systematic and\nautomated approach for sensor placement. The resulting TCs exhibit high\nsensitivity to the variation of uncertain input parameters and enable accurate\nfull field reconstruction while maintaining robustness against measurement\nnoise.", "AI": {"tldr": "本文提出了一种数据驱动框架，用于优化热工水力实验中的传感器（如热电偶）放置，以提高数据质量和全场重建精度。", "motivation": "热工水力实验中的传感器通常分布稀疏，覆盖范围有限，且传感器配置过程常依赖手动和专家判断，效率低且挑战大。因此，需要一种系统化的方法来优化传感器布局。", "method": "该框架包括三个主要步骤：(i) 通过敏感性分析构建数据集，(ii) 利用本征正交分解（POD）进行降维，以及 (iii) 结合列主元QR分解确定在空间约束下的最优传感器配置。", "result": "所提出的框架提供了一种系统化、自动化的传感器放置方法。在TALL-3D铅铋共晶（LBE）回路的案例研究中，优化后的热电偶对不确定输入参数的变化表现出高敏感性，能够实现准确的全场重建，并对测量噪声保持鲁棒性。", "conclusion": "该数据驱动框架为热工水力实验中的传感器优化放置提供了一个系统且自动化的解决方案，显著提升了传感器配置的效率和数据收集的质量，有助于更准确地理解和量化传热传质现象。"}}
{"id": "2509.09709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09709", "abs": "https://arxiv.org/abs/2509.09709", "authors": ["Jing Ren", "Weiqi Wang"], "title": "Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement", "comment": null, "summary": "Large language models (LLMs) like ChatGPT are increasingly used in academic\nwriting, yet issues such as incorrect or fabricated references raise ethical\nconcerns. Moreover, current content quality evaluations often rely on\nsubjective human judgment, which is labor-intensive and lacks objectivity,\npotentially compromising the consistency and reliability. In this study, to\nprovide a quantitative evaluation and enhance research proposal writing\ncapabilities of LLMs, we propose two key evaluation metrics--content quality\nand reference validity--and an iterative prompting method based on the scores\nderived from these two metrics. Our extensive experiments show that the\nproposed metrics provide an objective, quantitative framework for assessing\nChatGPT's writing performance. Additionally, iterative prompting significantly\nenhances content quality while reducing reference inaccuracies and\nfabrications, addressing critical ethical challenges in academic contexts.", "AI": {"tldr": "本研究提出两个量化指标（内容质量和引用有效性）和一种迭代提示方法，用于客观评估和提升大型语言模型在学术写作中的表现，有效解决了引用错误和内容质量问题。", "motivation": "大型语言模型在学术写作中日益普及，但存在引用错误和捏造等道德问题。此外，现有内容质量评估依赖主观人工判断，效率低且缺乏客观性，影响评估一致性和可靠性。", "method": "本研究提出两个关键评估指标：内容质量和引用有效性。基于这两个指标的分数，进一步提出一种迭代提示方法，以量化评估并增强大型语言模型的研究提案写作能力。", "result": "实验结果表明，所提出的指标为评估ChatGPT的写作表现提供了一个客观、量化的框架。此外，迭代提示显著提高了内容质量，同时减少了引用不准确和捏造的情况。", "conclusion": "本研究提出的量化评估框架和迭代提示方法，为大型语言模型在学术写作中的表现提供了客观评估工具，并有效解决了内容质量和引用准确性等关键伦理挑战。"}}
{"id": "2509.09919", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09919", "abs": "https://arxiv.org/abs/2509.09919", "authors": ["Franklin Yiu", "Mohan Lu", "Nina Li", "Kevin Joseph", "Tianxu Zhang", "Julian Togelius", "Timothy Merino", "Sam Earle"], "title": "A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments", "comment": null, "summary": "Procedural content generation often requires satisfying both\ndesigner-specified objectives and adjacency constraints implicitly imposed by\nthe underlying tile set. To address the challenges of jointly optimizing both\nconstraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a\nMarkov Decision Process (MDP), enabling external optimization algorithms to\nfocus exclusively on objective maximization while leveraging WFC's propagation\nmechanism to enforce constraint satisfaction. We empirically compare optimizing\nthis MDP to traditional evolutionary approaches that jointly optimize global\nmetrics and local tile placement. Across multiple domains with various\ndifficulties, we find that joint optimization not only struggles as task\ncomplexity increases, but consistently underperforms relative to optimization\nover the WFC-MDP, underscoring the advantages of decoupling local constraint\nsatisfaction from global objective optimization.", "AI": {"tldr": "该研究将WaveFunctionCollapse (WFC) 重构为马尔可夫决策过程 (MDP)，以解耦程序化内容生成中的局部约束满足与全局目标优化，并证明了其优于联合优化方法。", "motivation": "程序化内容生成需要同时满足设计师指定的目标和底层瓦片集隐含的邻接约束。联合优化这两种约束和目标具有挑战性。", "method": "将WaveFunctionCollapse (WFC) 重新表述为马尔可夫决策过程 (MDP)。WFC的传播机制用于强制满足约束，而外部优化算法则专注于目标最大化。", "result": "在不同难度和多个领域中，优化WFC-MDP的方法始终优于传统联合优化全局指标和局部瓦片放置的演化方法。随着任务复杂性增加，联合优化方法表现更差。", "conclusion": "将局部约束满足与全局目标优化解耦具有显著优势。"}}
{"id": "2509.09841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09841", "abs": "https://arxiv.org/abs/2509.09841", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework", "comment": null, "summary": "Rosacea, which is a chronic inflammatory skin condition that manifests with\nfacial redness, papules, and visible blood vessels, often requirs precise and\nearly detection for significantly improving treatment effectiveness. This paper\npresents new patch-based automatic rosacea detection strategies using the\nResNet-18 deep learning framework. The contributions of the proposed strategies\ncome from the following aspects. First, various image pateches are extracted\nfrom the facial images of people in different sizes, shapes, and locations.\nSecond, a number of investigation studies are carried out to evaluate how the\nlocalized visual information influences the deep learing model performance.\nThird, thorough experiments are implemented to reveal that several patch-based\nautomatic rosacea detection strategies achieve competitive or superior accuracy\nand sensitivity than the full-image based methods. And finally, the proposed\npatch-based strategies, which use only localized patches, inherently preserve\npatient privacy by excluding any identifiable facial features from the data.\nThe experimental results indicate that the proposed patch-based strategies\nguide the deep learning model to focus on clinically relevant regions, enhance\nrobustness and interpretability, and protect patient privacy. As a result, the\nproposed strategies offer practical insights for improving automated\ndermatological diagnostics.", "AI": {"tldr": "本文提出基于ResNet-18深度学习框架的斑块级自动酒渣鼻检测策略，通过提取不同大小、形状和位置的面部图像斑块，实现了比全图像方法更高的准确性和敏感性，同时增强了模型的可解释性并保护了患者隐私。", "motivation": "酒渣鼻是一种慢性炎症性皮肤病，早期和精准检测对于显著提高治疗效果至关重要。", "method": "研究采用ResNet-18深度学习框架，提出新的斑块级自动酒渣鼻检测策略。具体方法包括：1) 从面部图像中提取不同大小、形状和位置的各种图像斑块；2) 进行调查研究以评估局部视觉信息对深度学习模型性能的影响；3) 实施实验比较斑块级策略与全图像方法的性能。", "result": "实验结果表明，所提出的斑块级自动酒渣鼻检测策略在准确性和敏感性方面优于或与全图像方法相当。这些策略能引导深度学习模型关注临床相关区域，增强模型的鲁棒性和可解释性，并通过排除可识别的面部特征来保护患者隐私。", "conclusion": "所提出的斑块级策略为改进自动化皮肤病诊断提供了实用的见解。"}}
{"id": "2509.10065", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10065", "abs": "https://arxiv.org/abs/2509.10065", "authors": ["Hauzi Cao", "Jiahao Shen", "Zhengzhen Li", "Qinquan Ren", "Shiyu Zhao"], "title": "Prespecified-Performance Kinematic Tracking Control for Aerial Manipulation", "comment": null, "summary": "This paper studies the kinematic tracking control problem for aerial\nmanipulators. Existing kinematic tracking control methods, which typically\nemploy proportional-derivative feedback or tracking-error-based feedback\nstrategies, may fail to achieve tracking objectives within specified time\nconstraints. To address this limitation, we propose a novel control framework\ncomprising two key components: end-effector tracking control based on a\nuser-defined preset trajectory and quadratic programming-based reference\nallocation. Compared with state-of-the-art approaches, the proposed method has\nseveral attractive features. First, it ensures that the end-effector reaches\nthe desired position within a preset time while keeping the tracking error\nwithin a performance envelope that reflects task requirements. Second,\nquadratic programming is employed to allocate the references of the quadcopter\nbase and the Delta arm, while considering the physical constraints of the\naerial manipulator, thus preventing solutions that may violate physical\nlimitations. The proposed approach is validated through three experiments.\nExperimental results demonstrate the effectiveness of the proposed algorithm\nand its capability to guarantee that the target position is reached within the\npreset time.", "AI": {"tldr": "本文提出了一种针对空中机械手的运动学跟踪控制框架，通过预设时间跟踪和二次规划的参考分配，解决了现有方法无法在指定时间内完成跟踪的问题。", "motivation": "现有的空中机械手运动学跟踪控制方法（通常采用比例-微分反馈或基于跟踪误差的反馈策略）可能无法在指定的时间限制内实现跟踪目标。", "method": "本文提出了一种新的控制框架，包含两个主要组件：1) 基于用户定义的预设轨迹和性能包络的末端执行器跟踪控制，确保在预设时间内达到期望位置并控制误差；2) 基于二次规划的参考分配，用于分配四旋翼基座和Delta机械臂的参考，同时考虑空中机械手的物理约束以避免违反物理限制的解。", "result": "所提出的方法确保末端执行器在预设时间内到达期望位置，并使跟踪误差保持在反映任务要求的性能包络内。二次规划有效分配了参考并避免了物理限制。通过三个实验验证了算法的有效性及其在预设时间内达到目标位置的能力。", "conclusion": "实验结果表明，所提出的算法是有效的，并能够保证在预设时间内达到目标位置。"}}
{"id": "2509.10118", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.10118", "abs": "https://arxiv.org/abs/2509.10118", "authors": ["Jingyuan Zhou", "Haoze Wu", "Haokun Yu", "Kaidi Yang"], "title": "Scalable Synthesis and Verification of String Stable Neural Certificates for Interconnected Systems", "comment": null, "summary": "Ensuring string stability is critical for the safety and efficiency of\nlarge-scale interconnected systems. Although learning-based controllers (e.g.,\nthose based on reinforcement learning) have demonstrated strong performance in\ncomplex control scenarios, their black-box nature hinders formal guarantees of\nstring stability. To address this gap, we propose a novel verification and\nsynthesis framework that integrates discrete-time scalable input-to-state\nstability (sISS) with neural network verification to formally guarantee string\nstability in interconnected systems. Our contributions are four-fold. First, we\nestablish a formal framework for synthesizing and robustly verifying\ndiscrete-time scalable input-to-state stability (sISS) certificates for neural\nnetwork-based interconnected systems. Specifically, our approach extends the\nnotion of sISS to discrete-time settings, constructs neural sISS certificates,\nand introduces a verification procedure that ensures string stability while\nexplicitly accounting for discrepancies between the true dynamics and their\nneural approximations. Second, we establish theoretical foundations and\nalgorithms to scale the training and verification pipeline to large-scale\ninterconnected systems. Third, we extend the framework to handle systems with\nexternal control inputs, thereby allowing the joint synthesis and verification\nof neural certificates and controllers. Fourth, we validate our approach in\nscenarios of mixed-autonomy platoons, drone formations, and microgrids.\nNumerical simulations show that the proposed framework not only guarantees sISS\nwith minimal degradation in control performance but also efficiently trains and\nverifies controllers for large-scale interconnected systems under specific\npractical conditions.", "AI": {"tldr": "本文提出了一种验证和综合框架，将离散时间可伸缩输入到状态稳定性（sISS）与神经网络验证相结合，以形式化保证基于学习的互联系统（如车队、无人机编队、微电网）的串行稳定性。", "motivation": "学习型控制器（如强化学习）在复杂控制场景中表现出色，但其黑盒性质阻碍了串行稳定性的形式化保证，而这对于大规模互联系统的安全性和效率至关重要。", "method": "1. 建立了用于合成和鲁棒验证离散时间sISS证书的正式框架，将其扩展到离散时间设置，构建神经sISS证书，并引入考虑真实动力学与神经近似之间差异的验证程序。2. 建立了将训练和验证流程扩展到大规模互联系统的理论基础和算法。3. 将框架扩展到处理带有外部控制输入的系统，从而实现神经证书和控制器的联合合成与验证。", "result": "数值模拟表明，所提出的框架不仅在控制性能下降最小的情况下保证了sISS，而且能够高效地训练和验证大规模互联系统在特定实际条件下的控制器。该方法在混合自主车队、无人机编队和微电网场景中得到了验证。", "conclusion": "该框架成功地将离散时间sISS与神经网络验证结合，为基于学习的互联系统提供了形式化的串行稳定性保证，同时实现了可伸缩性和良好的控制性能，有效解决了黑盒控制器在安全性保证方面的挑战。"}}
{"id": "2509.09710", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09710", "abs": "https://arxiv.org/abs/2509.09710", "authors": ["Sepehr Golrokh Amin", "Devin Rhoads", "Fatemeh Fakhrmoosavi", "Nicholas E. Lownes", "John N. Ivan"], "title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data", "comment": null, "summary": "This study introduces a Large Language Model (LLM) scheme for generating\nindividual travel diaries in agent-based transportation models. While\ntraditional approaches rely on large quantities of proprietary household travel\nsurveys, the method presented in this study generates personas stochastically\nfrom open-source American Community Survey (ACS) and Smart Location Database\n(SLD) data, then synthesizes diaries through direct prompting. This study\nfeatures a novel one-to-cohort realism score: a composite of four metrics (Trip\nCount Score, Interval Score, Purpose Score, and Mode Score) validated against\nthe Connecticut Statewide Transportation Study (CSTS) diaries, matched across\ndemographic variables. The validation utilizes Jensen-Shannon Divergence to\nmeasure distributional similarities between generated and real diaries. When\ncompared to diaries generated with classical methods (Negative Binomial for\ntrip generation; Multinomial Logit for mode/purpose) calibrated on the\nvalidation set, LLM-generated diaries achieve comparable overall realism (LLM\nmean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and\ndemonstrates greater consistency (narrower realism score distribution), while\nclassical models lead in numerical estimates of trip count and activity\nduration. Aggregate validation confirms the LLM's statistical\nrepresentativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot\nviability and establishing a quantifiable metric of diary realism for future\nsynthetic diary evaluation systems.", "AI": {"tldr": "本研究提出了一种使用大型语言模型（LLM）从开放数据生成个体旅行日记的新方法，并在与传统方法比较后，证明其在模拟交通模型中的可行性与竞争力，尤其在出行目的判断和一致性方面表现突出。", "motivation": "传统的个体旅行日记生成方法高度依赖大量专有的家庭旅行调查数据，这限制了其应用范围和数据获取成本。本研究旨在开发一种利用开放数据和LLM生成旅行日记的新方案，以克服这些局限性。", "method": "该研究通过以下步骤生成旅行日记：1) 从开放的美国社区调查（ACS）和智能位置数据库（SLD）数据中随机生成人物画像（personas）。2) 通过直接提示（direct prompting）LLM合成旅行日记。3) 引入了一个新颖的“一对群组真实性评分”（one-to-cohort realism score），该评分由四个指标（出行次数得分、间隔得分、目的得分和模式得分）组成，并使用Jensen-Shannon散度进行验证，以衡量生成日记与康涅狄格州全州交通研究（CSTS）日记在人口统计学变量匹配下的分布相似性。4) 将LLM生成的结果与使用经典方法（负二项式模型用于出行生成，多项Logit模型用于模式/目的选择）生成的结果进行比较。", "result": "LLM生成的日记与经典方法生成的日记在总体真实性方面具有可比性（LLM平均：0.485 vs. 0.455）。LLM在确定出行目的方面表现出色，并显示出更高的一致性（真实性得分分布更窄）。而经典模型在出行次数和活动持续时间的数值估计方面表现更优。聚合验证证实了LLM的统计代表性（LLM平均：0.612 vs. 0.435），展示了LLM的零样本（zero-shot）可行性。", "conclusion": "本研究证明了LLM在代理交通模型中生成个体旅行日记的零样本可行性。LLM在出行目的判断和结果一致性方面具有优势，尽管在某些数值估计上不如经典模型。研究还建立了一个可量化的日记真实性指标，为未来合成日记评估系统提供了基础。"}}
{"id": "2509.09982", "categories": ["cs.AI", "I.2.4"], "pdf": "https://arxiv.org/pdf/2509.09982", "abs": "https://arxiv.org/abs/2509.09982", "authors": ["Stav Armoni-Friedmann", "Hana Chockler", "David A. Kelly"], "title": "Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae", "comment": "Accepted to ECAI-EXCD Workshop, 8 pages, 2 figures, 5 tables", "summary": "Evaluating explainable AI (XAI) approaches is a challenging task in general,\ndue to the subjectivity of explanations. In this paper, we focus on tabular\ndata and the specific use case of AI models predicting the values of Boolean\nfunctions. We extend the previous work in this domain by proposing a formal and\nprecise measure of importance of variables based on actual causality, and we\nevaluate state-of-the-art XAI tools against this measure. We also present a\nnovel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it\nis superior to other black-box XAI tools on a large-scale benchmark.\nSpecifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\\pm$ 0.012\non random 10-valued Boolean formulae", "AI": {"tldr": "本文提出了一种基于实际因果关系的形式化变量重要性度量，用于评估解释性AI（XAI）工具在表格数据和布尔函数预测上的表现。同时，引入了一种新的XAI工具B-ReX，并证明其在大型基准测试中优于其他黑盒XAI工具。", "motivation": "由于解释的主观性，评估解释性AI（XAI）方法通常具有挑战性。特别是在表格数据和AI模型预测布尔函数值的使用场景中，需要更客观、精确的评估方法。", "method": "研究人员提出了一个基于实际因果关系的变量重要性形式化度量。他们利用此度量来评估现有最先进的XAI工具。此外，他们基于现有工具ReX开发并提出了一个新的XAI工具B-ReX。", "result": "B-ReX在大型基准测试中表现优于其他黑盒XAI工具。具体而言，在随机的10值布尔函数上，B-ReX实现了0.072 ± 0.012的Jensen-Shannon散度。", "conclusion": "该研究成功地提出了一种基于实际因果关系的变量重要性度量，为XAI工具的评估提供了更客观的方法。同时，新提出的B-ReX工具在布尔函数预测任务上展现出优越的解释性能。"}}
{"id": "2509.09844", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09844", "abs": "https://arxiv.org/abs/2509.09844", "authors": ["Chengyu Yang", "Rishik Reddy Yesgari", "Chengjun Liu"], "title": "Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection", "comment": null, "summary": "Rosacea is a common but underdiagnosed inflammatory skin condition that\nprimarily affects the central face and presents with subtle redness, pustules,\nand visible blood vessels. Automated detection remains challenging due to the\ndiffuse nature of symptoms, the scarcity of labeled datasets, and privacy\nconcerns associated with using identifiable facial images. A novel\nprivacy-preserving automated rosacea detection method inspired by clinical\npriors and trained entirely on synthetic data is presented in this paper.\nSpecifically, the proposed method, which leverages the observation that rosacea\nmanifests predominantly through central facial erythema, first constructs a\nfixed redness-informed mask by selecting regions with consistently high red\nchannel intensity across facial images. The mask thus is able to focus on\ndiagnostically relevant areas such as the cheeks, nose, and forehead and\nexclude identity-revealing features. Second, the ResNet-18 deep learning\nmethod, which is trained on the masked synthetic images, achieves superior\nperformance over the full-face baselines with notable gains in terms of\naccuracy, recall and F1 score when evaluated using the real-world test data.\nThe experimental results demonstrate that the synthetic data and clinical\npriors can jointly enable accurate and ethical dermatological AI systems,\nespecially for privacy sensitive applications in telemedicine and large-scale\nscreening.", "AI": {"tldr": "本文提出了一种新颖的、隐私保护的酒渣鼻自动检测方法，该方法结合临床先验知识，通过红度信息掩码聚焦诊断区域，并完全使用合成数据训练ResNet-18模型，在真实世界数据上表现优异。", "motivation": "酒渣鼻是一种常见但诊断不足的炎症性皮肤病。由于症状弥漫性、标记数据集稀缺以及面部图像涉及隐私问题，自动化检测面临挑战。", "method": "该方法受临床先验知识（酒渣鼻主要表现为面部中央红斑）启发，首先构建一个固定的、基于红度信息的掩码，通过选择面部图像中红色通道强度持续较高的区域（如脸颊、鼻子、额头）来聚焦诊断相关区域，同时排除身份识别特征。其次，使用在这些掩码合成图像上训练的ResNet-18深度学习模型进行检测。", "result": "实验结果表明，该方法在真实世界测试数据上评估时，相较于全脸基线方法，在准确率、召回率和F1分数方面均取得了显著提升。", "conclusion": "合成数据和临床先验知识的结合能够实现准确且符合伦理的皮肤病AI系统，尤其适用于远程医疗和大规模筛查等隐私敏感应用。"}}
{"id": "2509.10096", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10096", "abs": "https://arxiv.org/abs/2509.10096", "authors": ["Saeed Saadatnejad", "Reyhaneh Hosseininejad", "Jose Barreiros", "Katherine M. Tsui", "Alexandre Alahi"], "title": "HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario", "comment": "Accepted to RA-L 2025", "summary": "The increasing labor shortage and aging population underline the need for\nassistive robots to support human care recipients. To enable safe and\nresponsive assistance, robots require accurate human motion prediction in\nphysical interaction scenarios. However, this remains a challenging task due to\nthe variability of assistive settings and the complexity of coupled dynamics in\nphysical interactions. In this work, we address these challenges through two\nkey contributions: (1) HHI-Assist, a dataset comprising motion capture clips of\nhuman-human interactions in assistive tasks; and (2) a conditional\nTransformer-based denoising diffusion model for predicting the poses of\ninteracting agents. Our model effectively captures the coupled dynamics between\ncaregivers and care receivers, demonstrating improvements over baselines and\nstrong generalization to unseen scenarios. By advancing interaction-aware\nmotion prediction and introducing a new dataset, our work has the potential to\nsignificantly enhance robotic assistance policies. The dataset and code are\navailable at: https://sites.google.com/view/hhi-assist/home", "AI": {"tldr": "该研究针对辅助机器人中人机交互场景下的人体运动预测难题，提出了一个名为HHI-Assist的人人交互运动捕捉数据集，并开发了一个基于条件Transformer的去噪扩散模型来预测交互代理的姿态，显著提升了预测准确性和泛化能力。", "motivation": "劳动力短缺和人口老龄化背景下，辅助机器人对人类护理对象的需求日益增长。为实现安全响应的辅助，机器人需在物理交互场景中准确预测人体运动。然而，由于辅助环境的多样性和物理交互中耦合动力学的复杂性，这仍是一个巨大挑战。", "method": "本研究通过两项关键贡献来应对挑战：1) HHI-Assist，一个包含辅助任务中人人交互运动捕捉片段的数据集；2) 一个基于条件Transformer的去噪扩散模型，用于预测交互代理的姿态。", "result": "所提出的模型能有效捕捉护理人员和护理接收者之间的耦合动力学，与基线模型相比有所改进，并对未见过的场景表现出强大的泛化能力。", "conclusion": "通过推动交互感知运动预测和引入新数据集，本工作有望显著增强机器人辅助策略，为辅助机器人领域带来重要进展。"}}
{"id": "2509.10154", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.10154", "abs": "https://arxiv.org/abs/2509.10154", "authors": ["Johannes van Randenborgh", "Moritz Schulze Darup"], "title": "MPC for Aquifer Thermal Energy Storage Systems Using ARX Models", "comment": "16th INDUSCON 2025 in Sao Sebastiao, Brazil", "summary": "An aquifer thermal energy storage (ATES) can mitigate CO2 emissions of\nheating, ventilation, and air conditioning (HVAC) systems for buildings. In\napplication, an ATES keeps large quantities of thermal energy in\ngroundwater-saturated aquifers. Normally, an ATES system comprises two (one for\nheat and one for cold) storages and supports the heating and cooling efforts of\nsimultaneously present HVAC system components. This way, the operation and\nemissions of installed and, usually, fossil fuel-based components are reduced.\n  The control of ATES systems is challenging, and various control schemes,\nincluding model predictive control (MPC), have been proposed. In this context,\nwe present a lightweight input-output-data-based autoregressive with exogenous\ninput (ARX) model of the hybrid ATES system dynamics. The ARX model allows the\ndesign of an output-based MPC scheme, resulting in an easy-to-solve quadratic\nprogram and avoiding challenging state estimations of ground temperatures. A\nnumerical study discusses the accuracy of the ARX predictor and controller\nperformance.", "AI": {"tldr": "本文提出了一种基于轻量级输入-输出数据的自回归外生输入（ARX）模型，用于控制地下水热能存储（ATES）系统，并设计了一个基于输出的模型预测控制（MPC）方案，以简化控制并避免复杂的地下温度状态估计。", "motivation": "地下水热能存储（ATES）系统能有效减少建筑供暖、通风和空调（HVAC）系统的二氧化碳排放。然而，ATES系统的控制具有挑战性，现有的模型预测控制（MPC）方案通常需要复杂的地下温度状态估计。", "method": "研究提出了一种轻量级的、基于输入-输出数据的自回归外生输入（ARX）模型来描述混合ATES系统的动态。利用该ARX模型设计了一个基于输出的模型预测控制（MPC）方案，该方案可转化为易于求解的二次规划问题，并避免了复杂的地下温度状态估计。", "result": "所提出的ARX模型能够支持基于输出的MPC方案设计，使得控制问题转化为易于求解的二次规划。该方法成功规避了地下温度等具有挑战性的状态估计问题。数值研究讨论了ARX预测器的准确性和控制器性能。", "conclusion": "通过引入轻量级ARX模型和基于输出的MPC方案，可以有效且简化地控制ATES系统，解决了传统MPC在ATES应用中面临的复杂状态估计难题，同时保持了良好的控制性能。"}}
{"id": "2509.09711", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09711", "abs": "https://arxiv.org/abs/2509.09711", "authors": ["Aya E. Fouda", "Abdelrahamn A. Hassan", "Radwa J. Hanafy", "Mohammed E. Fouda"], "title": "Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry", "comment": null, "summary": "Large language models (LLMs) hold great promise in enhancing psychiatric\npractice, from improving diagnostic accuracy to streamlining clinical\ndocumentation and therapeutic support. However, existing evaluation resources\nheavily rely on small clinical interview corpora, social media posts, or\nsynthetic dialogues, which limits their clinical validity and fails to capture\nthe full complexity of psychiatric reasoning. In this work, we introduce\nPsychiatryBench, a rigorously curated benchmark grounded exclusively in\nauthoritative, expert-validated psychiatric textbooks and casebooks.\nPsychiatryBench comprises eleven distinct question-answering tasks ranging from\ndiagnostic reasoning and treatment planning to longitudinal follow-up,\nmanagement planning, clinical approach, sequential case analysis, and\nmultiple-choice/extended matching formats totaling over 5,300 expert-annotated\nitems. We evaluate a diverse set of frontier LLMs (including Google Gemini,\nDeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models\n(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an\n\"LLM-as-judge\" similarity scoring framework. Our results reveal substantial\ngaps in clinical consistency and safety, particularly in multi-turn follow-up\nand management tasks, underscoring the need for specialized model tuning and\nmore robust evaluation paradigms. PsychiatryBench offers a modular, extensible\nplatform for benchmarking and improving LLM performance in high-stakes mental\nhealth applications.", "AI": {"tldr": "本文介绍了 PsychiatryBench，一个基于权威精神病学教科书和案例的基准测试集，用于评估大型语言模型在精神病学实践中的表现，并发现现有模型在临床一致性和安全性方面存在显著差距。", "motivation": "大型语言模型在精神病学实践中具有巨大潜力，但现有评估资源过度依赖小型临床访谈、社交媒体或合成对话，限制了其临床有效性，未能捕捉精神病学推理的复杂性。", "method": "研究引入了 PsychiatryBench，一个严格筛选的基准测试集，完全基于权威、专家验证的精神病学教科书和案例。它包含11个不同的问答任务，涵盖诊断推理、治疗计划、长期随访等，共计超过5300个专家标注条目。研究评估了多种前沿LLM和领先的开源医学模型，使用传统指标和“LLM作为评判”的相似性评分框架。", "result": "评估结果揭示了模型在临床一致性和安全性方面存在显著差距，尤其是在多轮随访和管理任务中。这强调了对专业模型微调和更稳健评估范式的需求。", "conclusion": "PsychiatryBench 提供了一个模块化、可扩展的平台，用于基准测试和改进LLM在高风险精神健康应用中的性能。研究结果表明，需要专门的模型调优和更强大的评估方法来提高LLM在精神病学领域的表现。"}}
{"id": "2509.10018", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10018", "abs": "https://arxiv.org/abs/2509.10018", "authors": ["Hailong Yang", "Renhuo Zhao", "Guanjin Wang", "Zhaohong Deng"], "title": "GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method", "comment": null, "summary": "With the rapid advancement of Large Language Model (LLM), LLM-based agents\nexhibit exceptional abilities in understanding and generating natural language,\nfacilitating human-like collaboration and information transmission in LLM-based\nMulti-Agent System (MAS). High-performance LLMs are often hosted on remote\nservers in public spaces. When tasks involve privacy data, MAS cannot securely\nutilize these LLMs without implementing privacy-preserving mechanisms. To\naddress this challenge, we propose a General Anonymizing Multi-Agent system\n(GAMA), which divides the agents' workspace into private and public spaces and\nprotects privacy through the anonymizing mechanism. In the private space,\nagents handle sensitive data, while in the public space, only anonymized data\nis utilized. GAMA incorporates two key modules to mitigate semantic loss caused\nby anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and\nDisproof-based Logic Enhancement (DLE). We evaluate GAMA on two public\nquestion-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The\nresults demonstrate that GAMA has superior performance compared to the\nstate-of-the-art models. To further assess its privacy-preserving capabilities,\nwe designed two new datasets: Knowledge Privacy Preservation and Logic Privacy\nPreservation. The final results highlight GAMA's exceptional effectiveness in\nboth task processing and privacy preservation.", "AI": {"tldr": "本文提出了一种名为GAMA的通用匿名化多智能体系统，旨在解决基于LLM的多智能体系统在处理隐私数据时无法安全使用远程LLM的问题。GAMA通过工作空间划分和匿名化机制保护隐私，并通过DRKE和DLE模块减少匿名化导致的语义损失，在任务处理和隐私保护方面均表现出色。", "motivation": "随着大型语言模型（LLM）的快速发展，LLM-based多智能体系统（MAS）在语言理解和生成方面展现出卓越能力。然而，高性能LLM通常部署在远程服务器上。当任务涉及隐私数据时，现有MAS无法在不实施隐私保护机制的情况下安全地利用这些LLM。", "method": "本文提出了通用匿名化多智能体系统（GAMA）。GAMA将智能体的工作空间划分为私有空间和公共空间，并通过匿名化机制保护隐私。在私有空间处理敏感数据，而在公共空间仅使用匿名化数据。为缓解匿名化造成的语义损失，GAMA集成了两个关键模块：基于领域规则的知识增强（DRKE）和基于反驳的逻辑增强（DLE）。", "result": "GAMA在两个公共问答数据集（Trivia Creative Writing和Logic Grid Puzzle）上进行了评估，结果表明GAMA的性能优于现有最先进的模型。为进一步评估其隐私保护能力，设计了两个新数据集（知识隐私保护和逻辑隐私保护），最终结果突出显示GAMA在任务处理和隐私保护方面均具有卓越的有效性。", "conclusion": "GAMA系统成功地解决了LLM-based多智能体系统在处理隐私数据时面临的安全挑战，通过创新的匿名化机制和语义增强模块，实现了在保持高性能任务处理能力的同时，有效保护用户隐私的目标。"}}
{"id": "2509.09849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09849", "abs": "https://arxiv.org/abs/2509.09849", "authors": ["Chengyu Yang", "Chengjun Liu"], "title": "Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking", "comment": null, "summary": "To rigorously assess the effectiveness and necessity of individual components\nwithin the recently proposed ULW framework for laparoscopic image desmoking,\nthis paper presents a comprehensive ablation study. The ULW approach combines a\nU-Net based backbone with a compound loss function that comprises mean squared\nerror (MSE), structural similarity index (SSIM) loss, and perceptual loss. The\nframework also incorporates a differentiable, learnable Wiener filter module.\nIn this study, each component is systematically ablated to evaluate its\nspecific contribution to the overall performance of the whole framework. The\nanalysis includes: (1) removal of the learnable Wiener filter, (2) selective\nuse of individual loss terms from the composite loss function. All variants are\nbenchmarked on a publicly available paired laparoscopic images dataset using\nquantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative\nvisual comparisons.", "AI": {"tldr": "本文对腹腔镜图像去烟雾ULW框架的各个组件进行了全面的消融研究，以评估其有效性和必要性。", "motivation": "ULW框架最近被提出用于腹腔镜图像去烟雾，但其内部各个组件（U-Net骨干、复合损失函数、可学习维纳滤波器）的有效性和必要性需要被严格评估。", "method": "研究方法是进行消融研究，系统性地移除ULW框架中的每个组件。具体包括：1) 移除可学习的维纳滤波器；2) 选择性地使用复合损失函数中的单个损失项（MSE、SSIM、感知损失）。所有变体都在公开的配对腹腔镜图像数据集上，使用定量指标（SSIM、PSNR、MSE和CIEDE-2000）和定性视觉比较进行基准测试。", "result": "通过系统性地消融每个组件，评估了其对整个框架整体性能的具体贡献。研究旨在量化分析每个部分对去烟雾效果的影响。", "conclusion": "该研究旨在通过消融实验，为ULW框架中各个组件的有效性和必要性提供严格的评估和理解，从而为未来的改进提供依据。"}}
{"id": "2509.10128", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10128", "abs": "https://arxiv.org/abs/2509.10128", "authors": ["Philip Arm", "Oliver Fischer", "Joseph Church", "Adrian Fuhrer", "Hendrik Kolvenbach", "Marco Hutter"], "title": "Efficient Learning-Based Control of a Legged Robot in Lunar Gravity", "comment": null, "summary": "Legged robots are promising candidates for exploring challenging areas on\nlow-gravity bodies such as the Moon, Mars, or asteroids, thanks to their\nadvanced mobility on unstructured terrain. However, as planetary robots' power\nand thermal budgets are highly restricted, these robots need energy-efficient\ncontrol approaches that easily transfer to multiple gravity environments. In\nthis work, we introduce a reinforcement learning-based control approach for\nlegged robots with gravity-scaled power-optimized reward functions. We use our\napproach to develop and validate a locomotion controller and a base pose\ncontroller in gravity environments from lunar gravity (1.62 m/s2) to a\nhypothetical super-Earth (19.62 m/s2). Our approach successfully scales across\nthese gravity levels for locomotion and base pose control with the\ngravity-scaled reward functions. The power-optimized locomotion controller\nreached a power consumption for locomotion of 23.4 W in Earth gravity on a\n15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.\nAdditionally, we designed a constant-force spring offload system that allowed\nus to conduct real-world experiments on legged locomotion in lunar gravity. In\nlunar gravity, the power-optimized control policy reached 12.2 W, 36 % less\nthan a baseline controller which is not optimized for power efficiency. Our\nmethod provides a scalable approach to developing power-efficient locomotion\ncontrollers for legged robots across multiple gravity levels.", "AI": {"tldr": "本文提出了一种基于强化学习的腿足机器人控制方法，通过重力缩放的功率优化奖励函数，实现了在多种重力环境下（从月球到超地球）的节能运动和姿态控制，并在实际月球重力环境中验证了其显著的能效提升。", "motivation": "腿足机器人在低重力星球（如月球、火星、小行星）探索中具有巨大潜力，但其功率和热量预算受限。因此，需要开发出能源效率高且能适应多种重力环境的控制方法。", "method": "研究人员提出了一种基于强化学习的腿足机器人控制方法，该方法使用重力缩放的功率优化奖励函数。他们开发并验证了运动控制器和基础姿态控制器，并设计了一个恒力弹簧卸载系统，用于在模拟月球重力环境下进行实际实验。", "result": "该方法成功地在从月球重力（1.62 m/s²）到假设的超地球重力（19.62 m/s²）的多种重力水平下实现了运动和姿态控制。在地球重力下，功率优化的运动控制器在15.65公斤、0.4米/秒的机器人上实现了23.4瓦的功耗，比基线策略提高了23%。在月球重力下，功率优化控制策略达到了12.2瓦的功耗，比未经功率优化的基线控制器低36%。", "conclusion": "该方法为开发适用于多种重力水平的腿足机器人节能运动控制器提供了一种可扩展的途径。"}}
{"id": "2509.10246", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.10246", "abs": "https://arxiv.org/abs/2509.10246", "authors": ["Amir Bahador Javadi", "Amin Kargarian", "Mort Naraghi-Pour"], "title": "Learning Constraint Surrogate Model for Two-stage Stochastic Unit Commitment", "comment": null, "summary": "The increasing penetration of renewable energy sources introduces significant\nuncertainty in power system operations, making traditional deterministic unit\ncommitment approaches computationally expensive. This paper presents a machine\nlearning surrogate modeling approach designed to reformulate the feasible\ndesign space of the two-stage stochastic unit commitment (TSUC) problem,\nreducing its computational complexity. The proposed method uses a support\nvector machine (SVM) to construct a surrogate model based on the governing\nequations of the learner. This model replaces the original 2|L| * |S|\ntransmission line flow constraints, where |S| is the number of uncertainty\nscenarios and |L| is the number of transmission lines with |S| much less than\n|L|, with a significantly reduced set of 1 * |S| linear inequality constraints.\nThe approach is theoretically grounded in the polyhedral structure of the\nfeasible region under the DC power flow approximation, enabling the\ntransformation of 2|L| line flow limit constraints into a single linear\nconstraint. The surrogate model is trained using data generated from\ncomputationally efficient DC optimal power flow simulations. Simulation results\non the IEEE 57-bus and 118-bus systems demonstrate SVM halfspace constraint\naccuracy of 99.72% and 99.88%, respectively, with TSUC computational time\nreductions of 46% and 31% and negligible generation cost increases (0.63% and\n0.88% on average for IEEE 57- and 118-bus systems, respectively). This shows\nthe effectiveness of the proposed approach for practical power system\noperations under renewable energy uncertainty.", "AI": {"tldr": "本文提出一种基于机器学习（支持向量机）的代理模型方法，通过重新构造两阶段随机机组组合（TSUC）问题的可行设计空间，显著减少了含可再生能源不确定性的电力系统运行的计算复杂性。", "motivation": "可再生能源渗透率的增加给电力系统运行带来了显著的不确定性，使得传统的确定性机组组合方法计算成本高昂。因此，需要一种能够降低计算复杂性的方法来有效处理这种不确定性。", "method": "该方法使用支持向量机（SVM）构建代理模型，以替代TSUC问题中原始的2|L| * |S|条输电线路潮流约束（其中|S|是场景数，|L|是线路数），将其简化为1 * |S|条线性不等式约束。该方法基于直流潮流近似下可行区域的多面体结构，并利用计算高效的直流最优潮流模拟生成数据来训练代理模型。", "result": "在IEEE 57和118节点系统上的仿真结果表明，SVM半空间约束精度分别达到99.72%和99.88%。TSUC计算时间分别减少了46%和31%，而发电成本的增加可忽略不计（对于IEEE 57和118节点系统，平均分别仅为0.63%和0.88%）。", "conclusion": "研究结果表明，所提出的方法在应对可再生能源不确定性下的实际电力系统运行中是有效的，能够显著降低计算复杂性，同时保持高精度和可接受的成本影响。"}}
{"id": "2509.09712", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09712", "abs": "https://arxiv.org/abs/2509.09712", "authors": ["Talha Tahir"], "title": "The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization", "comment": null, "summary": "Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral\ntherapy with emerging evidence of efficacy in several psychiatric conditions.\nThis study investigates the impact of post-training methodology and explicit\nreasoning on the ability of a small open-weight large language model (LLM) to\ndeliver ACT. Using 50 sets of synthetic ACT transcripts generated by\nMistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,\nsupervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each\nwith and without an explicit chain-of-thought (COT) reasoning step. Performance\nwas evaluated by comparing these four post-trained variants against the base\nInstruct model. These models were benchmarked in simulated therapy sessions,\nwith performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)\nand the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned\non human evaluations. Our findings demonstrate that the ORPO-trained models\nsignificantly outperformed both their SFT and Instruct counterparts on ACT\nfidelity ($\\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\\chi^2(5) =\n140.37, p < .001$). The effect of COT was conditional as it provided a\nsignificant benefit to SFT models, improving ACT-FM scores by an average of\n2.68 points ($p < .001$), while offering no discernible advantage to the\nsuperior ORPO or instruct-tuned variants. We posit that the superiority of ORPO\nstems from its ability to learn the therapeutic `process' over imitating\n`content,' a key aspect of ACT, while COT acts as a necessary scaffold for\nmodels trained only via imitation. This study establishes that\npreference-aligned policy optimization can effectively instill ACT competencies\nin small LLMs, and that the utility of explicit reasoning is highly dependent\non the underlying training paradigm.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2509.10054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10054", "abs": "https://arxiv.org/abs/2509.10054", "authors": ["Hailong Yang", "Mingxian Gu", "Jianqi Wang", "Guanjin Wang", "Zhaohong Deng"], "title": "XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph", "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nenhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans\nwith complex, real-world tasks. However, MAS still face challenges in effective\ntask planning when handling highly complex tasks with uncertainty, often\nresulting in misleading or incorrect outputs that hinder task execution. To\naddress this, we propose XAgents, a unified multi-agent cooperative framework\nbuilt on a multipolar task processing graph and IF-THEN rules. XAgents uses the\nmultipolar task processing graph to enable dynamic task planning and handle\ntask uncertainty. During subtask processing, it integrates domain-specific\nIF-THEN rules to constrain agent behaviors, while global rules enhance\ninter-agent collaboration. We evaluate the performance of XAgents across three\ndistinct datasets, demonstrating that it consistently surpasses\nstate-of-the-art single-agent and multi-agent approaches in both\nknowledge-typed and logic-typed question-answering tasks. The codes for XAgents\nare available at: https://github.com/AGI-FHBC/XAgents.", "AI": {"tldr": "XAgents是一个统一的多智能体协作框架，它利用多极任务处理图和IF-THEN规则来解决大型语言模型（LLM）增强的多智能体系统（MAS）在处理复杂、不确定任务时规划不当的问题，并在知识型和逻辑型问答任务中超越了现有技术。", "motivation": "尽管大型语言模型（LLMs）极大地提升了多智能体系统（MAS）的能力，但MAS在处理高度复杂且不确定的任务时，其任务规划能力仍面临挑战，常导致误导或错误的输出，从而阻碍任务执行。", "method": "本文提出了XAgents框架，它基于多极任务处理图和IF-THEN规则构建。该框架利用多极任务处理图实现动态任务规划和处理任务不确定性。在子任务处理过程中，它集成领域特定的IF-THEN规则来约束智能体行为，同时利用全局规则增强智能体之间的协作。", "result": "XAgents在三个不同数据集上的表现评估显示，它在知识型和逻辑型问答任务中，持续超越了最先进的单智能体和多智能体方法。", "conclusion": "XAgents通过结合多极任务处理图和IF-THEN规则，有效解决了多智能体系统在复杂不确定任务规划中的挑战，显著提升了任务执行的准确性和效率，展现出优越的性能。"}}
{"id": "2509.09859", "categories": ["cs.CV", "cs.LG", "68W99"], "pdf": "https://arxiv.org/pdf/2509.09859", "abs": "https://arxiv.org/abs/2509.09859", "authors": ["Razvan Stefanescu", "Ethan Oh", "Ruben Vazquez", "Chris Mesterharm", "Constantin Serban", "Ritu Chadha"], "title": "WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector", "comment": "11 pages, 11 figures", "summary": "We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and\nacoustic signals for robust real-life UAV object detection. Our approach fuses\nvisual and acoustic features in a unified object detector model relying on the\nDeformable DETR and Wav2Vec2 architectures, achieving strong performance under\nchallenging environmental conditions. Our work leverage the existing\nDrone-vs-Bird dataset and the newly generated ARDrone dataset containing more\nthan 7,500 synchronized images and audio segments. We show how the acoustic\ninformation is used to improve the performance of the Deformable DETR object\ndetector on the real ARDrone dataset. We developed, trained and tested four\ndifferent fusion configurations based on a gated mechanism, linear layer, MLP\nand cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi\nresolution feature mappings of the Deformable DETR and enhance the object\ndetection performance over all drones dimensions. The best performer is the\ngated fusion approach, which improves the mAP of the Deformable DETR object\ndetector on our in-distribution and out-of-distribution ARDrone datasets by\n11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.\nThe mAP scores for medium and large drones are also enhanced, with overall\ngains across all drone sizes ranging from 3.27% to 5.84%.", "AI": {"tldr": "本文提出WAVE-DETR多模态无人机检测器，结合可见光RGB和声学信号，融合Deformable DETR和Wav2Vec2架构，在挑战性环境下实现鲁棒的无人机目标检测，特别提升了小型无人机的检测性能。", "motivation": "在挑战性真实环境条件下，仅靠单一模态难以实现鲁棒的无人机目标检测，需要更有效的方法来提高检测性能。", "method": "该研究引入了WAVE-DETR多模态无人机检测器，将可见光RGB和声学信号结合。它将Deformable DETR的视觉特征和Wav2Vec2的声学特征在一个统一的模型中进行融合。研究探索了四种不同的融合配置：门控机制、线性层、MLP和交叉注意力。实验使用了现有的Drone-vs-Bird数据集和新生成的ARDrone数据集（包含超过7,500个同步图像和音频片段）。", "result": "声学信息显著提高了Deformable DETR目标检测器在真实ARDrone数据集上的性能。其中，门控融合方法表现最佳，将Deformable DETR在ARDrone数据集（包括同分布和异分布）上小型无人机的mAP提升了11.1%至15.3%（在IoU 0.5至0.9的所有阈值下）。中型和大型无人机的mAP也得到增强，所有无人机尺寸的总体增益范围为3.27%至5.84%。", "conclusion": "结合视觉和声学信号的多模态融合方法，特别是采用门控融合机制，能够显著提升无人机目标检测的性能，尤其对于小型无人机，在复杂环境下表现出更强的鲁棒性。"}}
{"id": "2509.10139", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10139", "abs": "https://arxiv.org/abs/2509.10139", "authors": ["Santiago Montiel-Marín", "Angel Llamazares", "Miguel Antunes-García", "Fabio Sánchez-García", "Luis M. Bergasa"], "title": "CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion", "comment": "4 pages, 2 figures", "summary": "Camera-radar fusion offers a robust and cost-effective alternative to\nLiDAR-based autonomous driving systems by combining complementary sensing\ncapabilities: cameras provide rich semantic cues but unreliable depth, while\nradar delivers sparse yet reliable position and motion information. We\nintroduce CaR1, a novel camera-radar fusion architecture for BEV vehicle\nsegmentation. Built upon BEVFusion, our approach incorporates a grid-wise radar\nencoding that discretizes point clouds into structured BEV features and an\nadaptive fusion mechanism that dynamically balances sensor contributions.\nExperiments on nuScenes demonstrate competitive segmentation performance (57.6\nIoU), on par with state-of-the-art methods. Code is publicly available\n\\href{https://www.github.com/santimontiel/car1}{online}.", "AI": {"tldr": "CaR1是一种新颖的相机-雷达融合架构，用于BEV车辆分割，通过网格化雷达编码和自适应融合机制改进了BEVFusion，在nuScenes上实现了与最先进方法相当的性能。", "motivation": "激光雷达自动驾驶系统成本高昂。相机提供丰富的语义信息但深度不可靠，雷达提供稀疏但可靠的位置和运动信息。通过结合两者的互补优势，可以为自动驾驶提供一个鲁棒且经济高效的替代方案，特别是在BEV车辆分割任务上。", "method": "该方法基于BEVFusion构建，并引入了两个关键改进：1. 网格化雷达编码：将点云离散化为结构化的BEV特征。2. 自适应融合机制：动态平衡不同传感器的贡献。", "result": "在nuScenes数据集上的实验表明，该方法在分割性能上具有竞争力（57.6 IoU），与最先进的方法持平。", "conclusion": "CaR1是一种新颖的相机-雷达融合架构，通过创新的编码和融合机制，在BEV车辆分割方面取得了与现有最佳方法相当的性能，为自动驾驶提供了一个有效的解决方案。"}}
{"id": "2509.10353", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.10353", "abs": "https://arxiv.org/abs/2509.10353", "authors": ["Davide Gorbani", "Mohamed Elobaid", "Giuseppe L'Erario", "Hosameldin Awadalla Omer Mohamed", "Daniele Pucci"], "title": "Data-fused Model Predictive Control with Guarantees: Application to Flying Humanoid Robots", "comment": "8 pages, 3 figures", "summary": "This paper introduces a Data-Fused Model Predictive Control (DFMPC) framework\nthat combines physics-based models with data-driven representations of unknown\ndynamics. Leveraging Willems' Fundamental Lemma and an artificial equilibrium\nformulation, the method enables tracking of changing, potentially unreachable\nsetpoints while explicitly handling measurement noise through slack variables\nand regularization. We provide guarantees of recursive feasibility and\npractical stability under input-output constraints for a specific class of\nreference signals. The approach is validated on the iRonCub flying humanoid\nrobot, integrating analytical momentum models with data-driven turbine\ndynamics. Simulations show improved tracking and robustness compared to a\npurely model-based MPC, while maintaining real-time feasibility.", "AI": {"tldr": "本文提出一种数据融合模型预测控制（DFMPC）框架，结合物理模型和数据驱动的未知动力学表示，实现对变化且可能无法达到的设定点的跟踪，同时通过松弛变量和正则化处理测量噪声。该方法在iRonCub飞行人形机器人上得到验证，显示出比纯模型MPC更好的跟踪和鲁棒性。", "motivation": "传统的模型预测控制（MPC）在面对未知动力学、需要跟踪变化且可能无法达到的设定点以及处理测量噪声时存在挑战。因此，需要一种能够结合物理知识和数据驱动方法来克服这些限制的控制框架。", "method": "该方法引入了数据融合模型预测控制（DFMPC）框架，该框架结合了基于物理的模型和数据驱动的未知动力学表示。它利用了Willems的基本引理和人工平衡公式。通过引入松弛变量和正则化，明确处理了测量噪声。该方法为特定类别的参考信号提供了递归可行性和实际稳定性的保证。", "result": "仿真结果表明，与纯粹基于模型的MPC相比，DFMPC框架在iRonCub飞行人形机器人上实现了更好的跟踪性能和鲁棒性，同时保持了实时可行性。该方法成功地将分析动量模型与数据驱动的涡轮动力学相结合。", "conclusion": "数据融合模型预测控制（DFMPC）框架能够有效地结合物理模型和数据驱动的未知动力学表示，实现对复杂系统中变化和可能无法达到的设定点的跟踪，并在存在测量噪声的情况下表现出良好的性能。该方法在实际应用中具有可行性，并能显著提升控制系统的性能和鲁棒性。"}}
{"id": "2509.09713", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09713", "abs": "https://arxiv.org/abs/2509.09713", "authors": ["Duolin Sun", "Dan Yang", "Yue Shen", "Yihan Jiao", "Zhehao Tan", "Jie Feng", "Lianzhen Zhong", "Jian Wang", "Peng Wei", "Jinjie Gu"], "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks.", "AI": {"tldr": "本文提出HANRAG框架，通过启发式路由、子查询分解和噪声过滤，解决现有RAG方法在多跳查询中效率低下和噪声积累的问题，显著提升了单跳和多跳问答性能。", "motivation": "现有检索增强生成（RAG）方法在处理多跳查询时面临挑战：过度依赖迭代检索导致步骤浪费；原始复杂查询检索未能捕捉子查询相关内容，引入噪声并导致噪声积累。", "method": "引入HANRAG，一个基于启发式的新颖框架。它由一个强大的“揭示器”（revelator）驱动，负责路由查询、将复杂查询分解为子查询，并过滤检索到的文档中的噪声，从而提高系统的适应性和抗噪能力。", "result": "HANRAG框架在各种基准测试中，与行业领先方法相比，在单跳和多跳问答任务中均取得了卓越的性能。", "conclusion": "HANRAG框架通过高效处理查询、分解子查询和过滤噪声，有效解决了RAG在多跳查询中的挑战，显著提升了系统的适应性和抗噪能力，并在多跳问答任务中展现出优越性。"}}
{"id": "2509.10104", "categories": ["cs.AI", "stat.ME"], "pdf": "https://arxiv.org/pdf/2509.10104", "abs": "https://arxiv.org/abs/2509.10104", "authors": ["Sofia Vei", "Paolo Giudici", "Pavlos Sermpezis", "Athena Vakali", "Adelaide Emma Bernardelli"], "title": "AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework", "comment": null, "summary": "The absolute dominance of Artificial Intelligence (AI) introduces\nunprecedented societal harms and risks. Existing AI risk assessment models\nfocus on internal compliance, often neglecting diverse stakeholder perspectives\nand real-world consequences. We propose a paradigm shift to a human-centric,\nharm-severity adaptive approach grounded in empirical incident data. We present\nAI Harmonics, which includes a novel AI harm assessment metric (AIH) that\nleverages ordinal severity data to capture relative impact without requiring\nprecise numerical estimates. AI Harmonics combines a robust, generalized\nmethodology with a data-driven, stakeholder-aware framework for exploring and\nprioritizing AI harms. Experiments on annotated incident data confirm that\npolitical and physical harms exhibit the highest concentration and thus warrant\nurgent mitigation: political harms erode public trust, while physical harms\npose serious, even life-threatening risks, underscoring the real-world\nrelevance of our approach. Finally, we demonstrate that AI Harmonics\nconsistently identifies uneven harm distributions, enabling policymakers and\norganizations to target their mitigation efforts effectively.", "AI": {"tldr": "该研究提出了一种以人为中心、危害严重性自适应的AI风险评估方法——AI Harmonics，它包含一个新的AI危害评估指标（AIH），利用序数严重性数据来捕捉相对影响，并基于经验事件数据识别和优先处理AI危害。", "motivation": "现有AI风险评估模型侧重内部合规，忽视了多样化的利益相关者视角和真实世界后果，而AI的绝对主导地位带来了前所未有的社会危害和风险。", "method": "提出了一种以人为中心、危害严重性自适应的方法（AI Harmonics），该方法基于经验事件数据。引入了一个新颖的AI危害评估指标（AIH），利用序数严重性数据来捕捉相对影响。结合了稳健的通用方法论与数据驱动、利益相关者感知的框架，用于探索和优先排序AI危害。", "result": "对标注事件数据的实验证实，政治和物理危害的集中度最高，因此需要紧急缓解：政治危害侵蚀公众信任，而物理危害构成严重甚至危及生命的风险。AI Harmonics能持续识别不均衡的危害分布。", "conclusion": "AI Harmonics方法具有现实世界相关性，能够帮助政策制定者和组织有效针对性地进行缓解工作，尤其是在识别不均衡危害分布和优先处理政治与物理危害方面。"}}
{"id": "2509.09869", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09869", "abs": "https://arxiv.org/abs/2509.09869", "authors": ["Yihao Liu", "Junyu Chen", "Lianrui Zuo", "Shuwen Wei", "Brian D. Boyd", "Carmen Andreescu", "Olusola Ajilore", "Warren D. Taylor", "Aaron Carass", "Bennett A. Landman"], "title": "Surrogate Supervision for Robust and Generalizable Deformable Image Registration", "comment": null, "summary": "Objective: Deep learning-based deformable image registration has achieved\nstrong accuracy, but remains sensitive to variations in input image\ncharacteristics such as artifacts, field-of-view mismatch, or modality\ndifference. We aim to develop a general training paradigm that improves the\nrobustness and generalizability of registration networks. Methods: We introduce\nsurrogate supervision, which decouples the input domain from the supervision\ndomain by applying estimated spatial transformations to surrogate images. This\nallows training on heterogeneous inputs while ensuring supervision is computed\nin domains where similarity is well defined. We evaluate the framework through\nthree representative applications: artifact-robust brain MR registration,\nmask-agnostic lung CT registration, and multi-modal MR registration. Results:\nAcross tasks, surrogate supervision demonstrated strong resilience to input\nvariations including inhomogeneity field, inconsistent field-of-view, and\nmodality differences, while maintaining high performance on well-curated data.\nConclusions: Surrogate supervision provides a principled framework for training\nrobust and generalizable deep learning-based registration models without\nincreasing complexity. Significance: Surrogate supervision offers a practical\npathway to more robust and generalizable medical image registration, enabling\nbroader applicability in diverse biomedical imaging scenarios.", "AI": {"tldr": "为解决深度学习形变图像配准对输入图像特性变化的敏感性，本文提出了“代理监督”训练范式，通过解耦输入域和监督域来提高配准网络的鲁棒性和泛化能力。", "motivation": "深度学习形变图像配准虽然精度高，但对输入图像特性（如伪影、视野不匹配、模态差异）的变化敏感。研究目标是开发一种通用的训练范式，以提高配准网络的鲁棒性和泛化能力。", "method": "引入“代理监督”方法，通过将估计的空间变换应用于代理图像，从而解耦输入域和监督域。这允许在异构输入上进行训练，同时确保监督在相似性定义明确的域中计算。通过三种代表性应用（抗伪影脑部MR配准、掩模无关肺部CT配准和多模态MR配准）评估了该框架。", "result": "在各项任务中，代理监督展示了对输入变异（包括不均匀场、不一致视野和模态差异）的强大弹性，同时在精心整理的数据上保持了高性能。", "conclusion": "代理监督为训练鲁棒且泛化性强的深度学习配准模型提供了一个原则性框架，且不增加复杂性。它为实现更鲁棒和泛化性更强的医学图像配准提供了一条实用途径，使其在多样化的生物医学成像场景中具有更广泛的适用性。"}}
{"id": "2509.10247", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10247", "abs": "https://arxiv.org/abs/2509.10247", "authors": ["Xinhong Zhang", "Runqing Wang", "Yunfan Ren", "Jian Sun", "Hao Fang", "Jie Chen", "Gang Wang"], "title": "DiffAero: A GPU-Accelerated Differentiable Simulation Framework for Efficient Quadrotor Policy Learning", "comment": "8 pages, 11 figures, 1 table", "summary": "This letter introduces DiffAero, a lightweight, GPU-accelerated, and fully\ndifferentiable simulation framework designed for efficient quadrotor control\npolicy learning. DiffAero supports both environment-level and agent-level\nparallelism and integrates multiple dynamics models, customizable sensor stacks\n(IMU, depth camera, and LiDAR), and diverse flight tasks within a unified,\nGPU-native training interface. By fully parallelizing both physics and\nrendering on the GPU, DiffAero eliminates CPU-GPU data transfer bottlenecks and\ndelivers orders-of-magnitude improvements in simulation throughput. In contrast\nto existing simulators, DiffAero not only provides high-performance simulation\nbut also serves as a research platform for exploring differentiable and hybrid\nlearning algorithms. Extensive benchmarks and real-world flight experiments\ndemonstrate that DiffAero and hybrid learning algorithms combined can learn\nrobust flight policies in hours on consumer-grade hardware. The code is\navailable at https://github.com/flyingbitac/diffaero.", "AI": {"tldr": "DiffAero是一个轻量级、GPU加速、完全可微分的四旋翼模拟框架，旨在通过消除CPU-GPU传输瓶颈和支持并行化来高效学习控制策略，并可作为可微分学习的研究平台。", "motivation": "现有模拟器在四旋翼控制策略学习中可能存在CPU-GPU数据传输瓶颈，导致模拟吞吐量低，且未能充分支持可微分和混合学习算法的研究。", "method": "DiffAero是一个GPU原生的训练接口，支持环境级和智能体级并行。它集成了多种动力学模型、可定制的传感器堆栈（IMU、深度相机、LiDAR）和多种飞行任务。通过在GPU上完全并行化物理和渲染，DiffAero消除了CPU-GPU数据传输瓶颈。", "result": "DiffAero在模拟吞吐量方面实现了数量级的提升。结合混合学习算法，它能够在消费级硬件上在数小时内学习到鲁棒的飞行策略。基准测试和真实世界飞行实验验证了其有效性。", "conclusion": "DiffAero不仅提供高性能模拟，还是探索可微分和混合学习算法的有效研究平台，能够高效且鲁棒地学习四旋翼飞行策略。"}}
{"id": "2509.10380", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2509.10380", "abs": "https://arxiv.org/abs/2509.10380", "authors": ["Yusheng Zheng", "Wenxue Liu", "Yunhong Che", "Ferdinand Grimm", "Jingyuan Zhao", "Xiaosong Hu", "Simona Onori", "Remus Teodorescu", "Gregory J. Offer"], "title": "Merging Physics-Based Synthetic Data and Machine Learning for Thermal Monitoring of Lithium-ion Batteries: The Role of Data Fidelity", "comment": null, "summary": "Since the internal temperature is less accessible than surface temperature,\nthere is an urgent need to develop accurate and real-time estimation algorithms\nfor better thermal management and safety. This work presents a novel framework\nfor resource-efficient and scalable development of accurate, robust, and\nadaptive internal temperature estimation algorithms by blending physics-based\nmodeling with machine learning, in order to address the key challenges in data\ncollection, model parameterization, and estimator design that traditionally\nhinder both approaches. In this framework, a physics-based model is leveraged\nto generate simulation data that includes different operating scenarios by\nsweeping the model parameters and input profiles. Such a cheap simulation\ndataset can be used to pre-train the machine learning algorithm to capture the\nunderlying mapping relationship. To bridge the simulation-to-reality gap\nresulting from imperfect modeling, transfer learning with unsupervised domain\nadaptation is applied to fine-tune the pre-trained machine learning model, by\nusing limited operational data (without internal temperature values) from\ntarget batteries. The proposed framework is validated under different operating\nconditions and across multiple cylindrical batteries with convective air\ncooling, achieving a root mean square error of 0.5 {\\deg}C when relying solely\non prior knowledge of battery thermal properties, and less than 0.1 {\\deg}C\nwhen using thermal parameters close to the ground truth. Furthermore, the role\nof the simulation data quality in the proposed framework has been\ncomprehensively investigated to identify promising ways of synthetic data\ngeneration to guarantee the performance of the machine learning model.", "AI": {"tldr": "本文提出了一种结合物理模型和机器学习的新框架，用于高效、可扩展地估计电池内部温度。该框架利用物理模型生成仿真数据进行预训练，并通过无监督域适应的迁移学习，利用有限的实际运行数据（无内部温度标签）进行微调，以弥合仿真与现实之间的差距，实现了高精度。", "motivation": "由于内部温度难以直接获取，迫切需要开发准确、实时的估计算法，以实现更好的热管理和安全。传统方法在数据收集、模型参数化和估计器设计方面面临挑战。", "method": "该方法将基于物理的模型与机器学习相结合：1) 利用物理模型通过扫掠模型参数和输入配置文件生成包含不同运行场景的仿真数据，用于预训练机器学习算法。2) 应用无监督域适应的迁移学习，利用目标电池有限的运行数据（不含内部温度值）微调预训练的机器学习模型，以弥合仿真与现实之间的差距。", "result": "该框架在不同运行条件和多个圆柱形电池上进行了验证。在仅依赖电池热性能先验知识的情况下，均方根误差为0.5°C；当使用接近真实值的热参数时，均方根误差小于0.1°C。此外，还全面研究了仿真数据质量对框架性能的影响。", "conclusion": "该框架成功地将物理建模与机器学习融合，提供了一种资源高效、可扩展的方法，用于开发准确、鲁棒和自适应的内部温度估计算法，有效解决了传统方法在数据、参数和设计上的挑战，并在有限真实数据下实现了高精度估计。"}}
{"id": "2509.09714", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09714", "abs": "https://arxiv.org/abs/2509.09714", "authors": ["Serge Lionel Nikiema", "Albérick Euraste Djire", "Abdoul Aziz Bonkoungou", "Micheline Bénédicte Moumoula", "Jordan Samhi", "Abdoul Kader Kabore", "Jacques Klein", "Tegawendé F. Bissyande"], "title": "How Small Transformation Expose the Weakness of Semantic Similarity Measures", "comment": null, "summary": "This research examines how well different methods measure semantic\nsimilarity, which is important for various software engineering applications\nsuch as code search, API recommendations, automated code reviews, and\nrefactoring tools. While large language models are increasingly used for these\nsimilarity assessments, questions remain about whether they truly understand\nsemantic relationships or merely recognize surface patterns.\n  The study tested 18 different similarity measurement approaches, including\nword-based methods, embedding techniques, LLM-based systems, and\nstructure-aware algorithms. The researchers created a systematic testing\nframework that applies controlled changes to text and code to evaluate how well\neach method handles different types of semantic relationships.\n  The results revealed significant issues with commonly used metrics. Some\nembedding-based methods incorrectly identified semantic opposites as similar up\nto 99.9 percent of the time, while certain transformer-based approaches\noccasionally rated opposite meanings as more similar than synonymous ones. The\nstudy found that embedding methods' poor performance often stemmed from how\nthey calculate distances; switching from Euclidean distance to cosine\nsimilarity improved results by 24 to 66 percent. LLM-based approaches performed\nbetter at distinguishing semantic differences, producing low similarity scores\n(0.00 to 0.29) for genuinely different meanings, compared to embedding methods\nthat incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.", "AI": {"tldr": "研究评估了18种语义相似度测量方法，发现常用的嵌入方法存在严重问题，高达99.9%的时间将语义相反的词识别为相似，而LLM方法在区分语义差异方面表现更好。距离计算方式对嵌入方法性能影响显著。", "motivation": "语义相似度测量对软件工程应用（如代码搜索、API推荐、自动化代码审查）至关重要。尽管大语言模型（LLM）日益普及，但它们是否真正理解语义关系或仅识别表面模式仍存疑问。", "method": "研究测试了18种不同的相似度测量方法，包括基于词语的方法、嵌入技术、基于LLM的系统和结构感知算法。研究人员创建了一个系统性测试框架，通过对文本和代码施加受控变化来评估每种方法处理不同类型语义关系的能力。", "result": "结果显示常用指标存在显著问题。一些基于嵌入的方法高达99.9%的时间将语义相反的词错误识别为相似，而某些基于Transformer的方法偶尔将相反的含义评为比同义词更相似。嵌入方法的糟糕表现常源于其距离计算方式；将欧几里得距离改为余弦相似度可将结果提高24%至66%。LLM方法在区分语义差异方面表现更好，对真正不同的含义给出较低的相似度分数（0.00至0.29），而嵌入方法则错误地为不相似内容分配高分（0.82至0.99）。", "conclusion": "许多现有的语义相似度测量方法，特别是基于嵌入的方法，在真正理解语义关系方面存在严重缺陷，常将相反或不相似的内容误判为高度相似。LLM方法在区分语义差异上表现出更好的能力，且嵌入方法的距离计算方式对性能至关重要。"}}
{"id": "2509.10147", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10147", "abs": "https://arxiv.org/abs/2509.10147", "authors": ["Nenad Tomasev", "Matija Franklin", "Joel Z. Leibo", "Julian Jacobs", "William A. Cunningham", "Iason Gabriel", "Simon Osindero"], "title": "Virtual Agent Economies", "comment": null, "summary": "The rapid adoption of autonomous AI agents is giving rise to a new economic\nlayer where agents transact and coordinate at scales and speeds beyond direct\nhuman oversight. We propose the \"sandbox economy\" as a framework for analyzing\nthis emergent system, characterizing it along two key dimensions: its origins\n(emergent vs. intentional) and its degree of separateness from the established\nhuman economy (permeable vs. impermeable). Our current trajectory points toward\na spontaneous emergence of a vast and highly permeable AI agent economy,\npresenting us with opportunities for an unprecedented degree of coordination as\nwell as significant challenges, including systemic economic risk and\nexacerbated inequality. Here we discuss a number of possible design choices\nthat may lead to safely steerable AI agent markets. In particular, we consider\nauction mechanisms for fair resource allocation and preference resolution, the\ndesign of AI \"mission economies\" to coordinate around achieving collective\ngoals, and socio-technical infrastructure needed to ensure trust, safety, and\naccountability. By doing this, we argue for the proactive design of steerable\nagent markets to ensure the coming technological shift aligns with humanity's\nlong-term collective flourishing.", "AI": {"tldr": "本文提出了“沙盒经济”框架来分析新兴的AI智能体经济，探讨其起源和与人类经济的独立程度。文章指出，当前趋势指向一个自发、庞大且高度渗透的AI智能体经济，这既带来前所未有的协作机会，也伴随系统性经济风险和不平等加剧等挑战。为此，作者讨论了旨在确保AI智能体市场安全可控的设计方案，强调需主动设计可控的智能体市场以促进人类福祉。", "motivation": "随着自主AI智能体的快速普及，一个新的经济层正在形成，其中智能体以超越人类直接监督的规模和速度进行交易和协调。这种新兴系统带来了巨大的机遇，但也伴随着系统性经济风险和不平等加剧等挑战，因此需要一个框架来分析并提出应对方案。", "method": "作者提出了“沙盒经济”作为分析新兴AI智能体经济的框架，并从“起源”（自发 vs. 有意）和“与既有经济的独立程度”（可渗透 vs. 不可渗透）两个维度对其进行描述。在此基础上，文章讨论了多种可能的设计选择，以引导AI智能体市场安全可控，包括：用于公平资源分配和偏好解决的拍卖机制、为实现集体目标而设计的AI“任务经济”，以及确保信任、安全和问责所需的社会技术基础设施。", "result": "研究发现，当前的轨迹正指向一个自发形成、庞大且高度渗透的AI智能体经济。这种发展趋势既提供了前所未有的协作机会，也带来了包括系统性经济风险和不平等加剧在内的重大挑战。", "conclusion": "为了确保即将到来的技术变革能够符合人类的长期集体繁荣，我们必须主动设计可控的智能体市场。通过采纳公平的资源分配机制、设计集体目标驱动的“任务经济”以及构建健全的社会技术基础设施，可以有效引导AI智能体经济，使其安全且有益。"}}
{"id": "2509.09911", "categories": ["cs.CV", "cs.AI", "68T07 (Primary)"], "pdf": "https://arxiv.org/pdf/2509.09911", "abs": "https://arxiv.org/abs/2509.09911", "authors": ["Barkin Buyukcakir", "Jannick De Tobel", "Patrick Thevissen", "Dirk Vandermeulen", "Peter Claes"], "title": "An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars", "comment": "21 pages, 11 figures, Scientific Reports", "summary": "The practical adoption of deep learning in high-stakes forensic applications,\nsuch as dental age estimation, is often limited by the 'black box' nature of\nthe models. This study introduces a framework designed to enhance both\nperformance and transparency in this context. We use a notable performance\ndisparity in the automated staging of mandibular second (tooth 37) and third\n(tooth 38) molars as a case study. The proposed framework, which combines a\nconvolutional autoencoder (AE) with a Vision Transformer (ViT), improves\nclassification accuracy for both teeth over a baseline ViT, increasing from\n0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond\nimproving performance, the framework provides multi-faceted diagnostic\ninsights. Analysis of the AE's latent space metrics and image reconstructions\nindicates that the remaining performance gap is data-centric, suggesting high\nintra-class morphological variability in the tooth 38 dataset is a primary\nlimiting factor. This work highlights the insufficiency of relying on a single\nmode of interpretability, such as attention maps, which can appear anatomically\nplausible yet fail to identify underlying data issues. By offering a\nmethodology that both enhances accuracy and provides evidence for why a model\nmay be uncertain, this framework serves as a more robust tool to support expert\ndecision-making in forensic age estimation.", "AI": {"tldr": "本研究针对牙齿年龄估计中深度学习模型的“黑箱”问题，提出了一种结合卷积自编码器（AE）和Vision Transformer（ViT）的框架。该框架不仅提高了性能，还能通过诊断性洞察揭示模型不确定性的数据中心原因，例如牙齿38数据集内部形态变异性高的问题，超越了单一解释模式的局限性。", "motivation": "深度学习在牙齿年龄估计等高风险法医应用中，由于其“黑箱”特性，实际采用受限。此外，在下颌第二磨牙（牙齿37）和第三磨牙（牙齿38）的自动分期中存在显著的性能差异，这促使研究人员寻求提升性能和透明度的方法。", "method": "本研究提出了一种结合卷积自编码器（AE）和Vision Transformer（ViT）的框架。该框架利用AE的潜在空间度量和图像重建来提供多方面的诊断性洞察，以识别模型性能的潜在限制因素。", "result": "该框架相较于基线ViT，提高了两种牙齿的分类准确率：牙齿37从0.712提升至0.815，牙齿38从0.462提升至0.543。通过分析AE的潜在空间和图像重建，研究发现剩余的性能差距是数据中心性的，牙齿38数据集中较高的类内形态变异性是主要限制因素。研究还指出，仅依赖注意力图等单一解释模式不足以识别潜在的数据问题。", "conclusion": "该框架通过同时提高准确性并提供模型不确定性原因的证据，为法医年龄估计中的专家决策提供了更强大的工具。它强调了多方面诊断洞察的重要性，以超越单一解释模式的局限性，从而识别并解决潜在的数据问题。"}}
{"id": "2509.10305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10305", "abs": "https://arxiv.org/abs/2509.10305", "authors": ["Yutong Shen", "Ruizhe Xia", "Bokai Yan", "Shunqi zhang", "Pengrui Xiang", "Sicheng He", "Yixin Xu"], "title": "GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning", "comment": "6 pages, 5 figures", "summary": "In dynamic and uncertain environments, robotic path planning demands accurate\nspatiotemporal environment understanding combined with robust decision-making\nunder partial observability. However, current deep reinforcement learning-based\npath planning methods face two fundamental limitations: (1) insufficient\nmodeling of multi-scale temporal dependencies, resulting in suboptimal\nadaptability in dynamic scenarios, and (2) inefficient exploration-exploitation\nbalance, leading to degraded path quality. To address these challenges, we\npropose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic Path\nPlanning. The framework comprises two key modules: (i) the Spatiotemporal\nPerception module, which hierarchically extracts multi-granularity spatial\nfeatures and multi-scale temporal dependencies ranging from instantaneous to\nextended time horizons, thereby improving perception accuracy in dynamic\nenvironments; and (ii) the Adaptive Policy Optimization module, which balances\nexploration and exploitation during training while optimizing for smoothness\nand collision probability through constrained policy updates. Experiments in\ndynamic environments demonstrate that GundamQ achieves a 15.3\\% improvement in\nsuccess rate and a 21.7\\% increase in overall path quality, significantly\noutperforming existing state-of-the-art methods.", "AI": {"tldr": "GundamQ提出了一种多尺度时空Q网络，通过改进时空感知和自适应策略优化，显著提升了机器人在动态不确定环境中的路径规划成功率和路径质量。", "motivation": "当前基于深度强化学习的路径规划方法存在两个主要局限性：(1) 对多尺度时间依赖建模不足，导致在动态场景中适应性差；(2) 探索-利用平衡效率低下，导致路径质量下降。", "method": "本文提出了GundamQ框架，包含两个关键模块：(i) 时空感知模块，分层提取多粒度空间特征和从瞬时到长期范围的多尺度时间依赖，以提高动态环境中的感知精度；(ii) 自适应策略优化模块，在训练过程中平衡探索与利用，并通过约束策略更新优化路径平滑度和碰撞概率。", "result": "在动态环境中的实验表明，GundamQ的成功率提高了15.3%，整体路径质量提高了21.7%，显著优于现有的最先进方法。", "conclusion": "GundamQ通过解决多尺度时间依赖建模和探索-利用平衡问题，有效提升了机器人在动态不确定环境中的路径规划性能，实现了更高的成功率和更好的路径质量。"}}
{"id": "2509.09715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09715", "abs": "https://arxiv.org/abs/2509.09715", "authors": ["Naveen Lamba", "Sanju Tiwari", "Manas Gaur"], "title": "Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA", "comment": null, "summary": "Hallucination in Large Language Models (LLMs) is a well studied problem.\nHowever, the properties that make LLM intrinsically vulnerable to\nhallucinations have not been identified and studied. This research identifies\nand characterizes the key properties, allowing us to pinpoint vulnerabilities\nwithin the model's internal mechanisms. To solidify on these properties, we\nutilized two established datasets, HaluEval and TruthfulQA and convert their\nexisting format of question answering into various other formats to narrow down\nthese properties as the reason for the hallucinations. Our findings reveal that\nhallucination percentages across symbolic properties are notably high for\nGemma-2-2B, averaging 79.0% across tasks and datasets. With increased model\nscale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,\nreflecting a 15 percentage point reduction overall. Although the hallucination\nrate decreases as the model size increases, a substantial amount of\nhallucination caused by symbolic properties still persists. This is especially\nevident for modifiers (ranging from 84.76% to 94.98%) and named entities\n(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.\nThese findings indicate that symbolic elements continue to confuse the models,\npointing to a fundamental weakness in how these LLMs process such\ninputs--regardless of their scale.", "AI": {"tldr": "本研究识别并量化了导致大型语言模型（LLMs）幻觉的关键内在属性。结果显示，Gemma模型对符号属性（特别是修饰语和命名实体）的幻觉率很高，尽管随着模型规模的增加幻觉率有所下降，但这种根本性弱点仍然存在。", "motivation": "幻觉是大型语言模型中一个被广泛研究的问题，但导致LLMs本质上容易产生幻觉的内在属性尚未被明确识别和研究。本研究旨在识别和表征这些关键属性，以找出模型内部机制中的脆弱点。", "method": "研究使用了HaluEval和TruthfulQA两个既有数据集，并将其问答格式转换为其他多种格式，以缩小并确定导致幻觉的特定属性。实验对象是Gemma-2系列模型（2B、9B和27B），通过评估它们在不同符号属性上的幻觉百分比来分析其表现。", "result": "Gemma-2-2B在各项任务和数据集上的幻觉率平均高达79.0%。随着模型规模的增加，幻觉率有所下降，Gemma-2-9B降至73.6%，Gemma-2-27B降至63.9%，总体降低了15个百分点。然而，由符号属性引起的幻觉仍大量存在，尤其是在修饰语（84.76%至94.98%）和命名实体（83.87%至93.96%）方面，所有Gemma模型和两个数据集都显示出高幻觉率。", "conclusion": "研究结果表明，符号元素持续困扰着大型语言模型，这指出了LLMs在处理此类输入时存在根本性弱点，且这种弱点与模型规模无关，即使模型变大，该问题依然显著存在。"}}
{"id": "2509.10162", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10162", "abs": "https://arxiv.org/abs/2509.10162", "authors": ["Tamir Shazman", "Idan Lev-Yehudi", "Ron Benchetit", "Vadim Indelman"], "title": "Online Robust Planning under Model Uncertainty: A Sample-Based Approach", "comment": null, "summary": "Online planning in Markov Decision Processes (MDPs) enables agents to make\nsequential decisions by simulating future trajectories from the current state,\nmaking it well-suited for large-scale or dynamic environments. Sample-based\nmethods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely\nadopted for their ability to approximate optimal actions using a generative\nmodel. However, in practical settings, the generative model is often learned\nfrom limited data, introducing approximation errors that can degrade\nperformance or lead to unsafe behaviors. To address these challenges, Robust\nMDPs (RMDPs) offer a principled framework for planning under model uncertainty,\nyet existing approaches are typically computationally intensive and not suited\nfor real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the\nfirst online planning algorithm for RMDPs with finite-sample theoretical\nperformance guarantees. Unlike Sparse Sampling, which estimates the nominal\nvalue function, RSS computes a robust value function by leveraging the\nefficiency and theoretical properties of Sample Average Approximation (SAA),\nenabling tractable robust policy computation in online settings. RSS is\napplicable to infinite or continuous state spaces, and its sample and\ncomputational complexities are independent of the state space size. We provide\ntheoretical performance guarantees and empirically show that RSS outperforms\nstandard Sparse Sampling in environments with uncertain dynamics.", "AI": {"tldr": "本文提出鲁棒稀疏采样（RSS），一种用于鲁棒马尔可夫决策过程（RMDPs）的在线规划算法，具有有限样本理论性能保证，能有效处理模型不确定性。", "motivation": "在线规划方法（如稀疏采样、MCTS）在模型通过有限数据学习时，会因近似误差导致性能下降或不安全行为。鲁棒MDPs虽能处理模型不确定性，但现有方法计算量大，不适用于实时应用。", "method": "引入鲁棒稀疏采样（RSS）算法，这是首个具有有限样本理论性能保证的RMDPs在线规划算法。RSS通过利用样本平均近似（SAA）的效率和理论特性，计算鲁棒价值函数，而非名义价值函数。它适用于无限或连续状态空间。", "result": "RSS算法具有有限样本理论性能保证，其样本和计算复杂度与状态空间大小无关。实验结果表明，在动态不确定的环境中，RSS优于标准的稀疏采样算法。", "conclusion": "RSS为RMDPs提供了一种可行的在线规划解决方案，通过计算鲁棒价值函数有效地应对模型不确定性，并具有理论性能保证和实际应用潜力。"}}
{"id": "2509.09935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09935", "abs": "https://arxiv.org/abs/2509.09935", "authors": ["Chirayu Agrawal", "Snehasis Mukherjee"], "title": "SCoDA: Self-supervised Continual Domain Adaptation", "comment": "Submitted to ICVGIP 2025", "summary": "Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a\nmodel to a target domain without access to the data of the source domain.\nPrevailing methods typically start with a source model pre-trained with full\nsupervision and distill the knowledge by aligning instance-level features.\nHowever, these approaches, relying on cosine similarity over L2-normalized\nfeature vectors, inadvertently discard crucial geometric information about the\nlatent manifold of the source model. We introduce Self-supervised Continual\nDomain Adaptation (SCoDA) to address these limitations. We make two key\ndepartures from standard practice: first, we avoid the reliance on supervised\npre-training by initializing the proposed framework with a teacher model\npre-trained entirely via self-supervision (SSL). Second, we adapt the principle\nof geometric manifold alignment to the SFDA setting. The student is trained\nwith a composite objective combining instance-level feature matching with a\nSpace Similarity Loss. To combat catastrophic forgetting, the teacher's\nparameters are updated via an Exponential Moving Average (EMA) of the student's\nparameters. Extensive experiments on benchmark datasets demonstrate that SCoDA\nsignificantly outperforms state-of-the-art SFDA methods.", "AI": {"tldr": "本文提出了一种名为SCoDA的无源持续域适应方法，通过自监督预训练和几何流形对齐来解决现有SFDA方法中几何信息丢失的问题，并显著优于现有最先进的方法。", "motivation": "现有无源域适应（SFDA）方法通常依赖于全监督预训练模型，并通过对L2归一化特征向量进行余弦相似度计算来对齐实例级特征。这种做法无意中丢弃了源模型潜在流形中关键的几何信息。", "method": "SCoDA方法做了两项关键改进：1) 避免依赖监督预训练，而是使用完全通过自监督学习（SSL）预训练的教师模型初始化框架。2) 将几何流形对齐原则应用于SFDA设置。学生模型通过结合实例级特征匹配和空间相似度损失的复合目标进行训练。为对抗灾难性遗忘，教师模型的参数通过学生模型参数的指数移动平均（EMA）进行更新。", "result": "在基准数据集上进行的广泛实验表明，SCoDA显著优于最先进的SFDA方法。", "conclusion": "SCoDA通过采用自监督预训练和几何流形对齐，有效解决了传统SFDA方法中几何信息丢失的局限性，实现了卓越的性能，并在无源域适应领域取得了重要进展。"}}
{"id": "2509.10317", "categories": ["cs.RO", "cs.LG", "93C85", "I.2.9; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2509.10317", "abs": "https://arxiv.org/abs/2509.10317", "authors": ["Elizaveta D. Moskovskaya", "Anton D. Moscowsky"], "title": "Robot guide with multi-agent control and automatic scenario generation with LLM", "comment": "14 pages, 5 figures, 2 tables, 1 demo-video and repository link", "summary": "The work describes the development of a hybrid control architecture for an\nanthropomorphic tour guide robot, combining a multi-agent resource management\nsystem with automatic behavior scenario generation based on large language\nmodels. The proposed approach aims to overcome the limitations of traditional\nsystems, which rely on manual tuning of behavior scenarios. These limitations\ninclude manual configuration, low flexibility, and lack of naturalness in robot\nbehavior. The process of preparing tour scenarios is implemented through a\ntwo-stage generation: first, a stylized narrative is created, then non-verbal\naction tags are integrated into the text. The multi-agent system ensures\ncoordination and conflict resolution during the execution of parallel actions,\nas well as maintaining default behavior after the completion of main\noperations, contributing to more natural robot behavior. The results obtained\nfrom the trial demonstrate the potential of the proposed approach for\nautomating and scaling social robot control systems.", "AI": {"tldr": "该研究开发了一种用于拟人化导游机器人的混合控制架构，结合了多智能体资源管理系统和基于大型语言模型的自动行为场景生成。", "motivation": "传统机器人系统依赖手动调整行为场景，存在手动配置、灵活性低和机器人行为不自然等局限性。本研究旨在克服这些限制。", "method": "提出了一种混合控制架构，其中：1) 利用大型语言模型进行两阶段行为场景生成（先创建风格化叙述，再整合非语言动作标签）；2) 采用多智能体系统进行并行动作的协调、冲突解决以及主要操作完成后的默认行为维护。", "result": "试验结果表明，所提出的方法在自动化和扩展社交机器人控制系统方面具有潜力，并有助于实现更自然的机器人行为。", "conclusion": "该混合控制方法成功实现了导游机器人行为场景的自动化生成和管理，提高了系统的灵活性和行为的自然性，为社交机器人控制系统的自动化和规模化提供了可行方案。"}}
{"id": "2509.09723", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ME", "I.2.6; J.4; I.5.1; H.3.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2509.09723", "abs": "https://arxiv.org/abs/2509.09723", "authors": ["Kai R. Larsen", "Sen Yan", "Roland Müller", "Lan Sang", "Mikko Rönkkö", "Ravi Starzl", "Donald Edmondson"], "title": "ALIGNS: Unlocking nomological networks in psychological measurement through a large language model", "comment": null, "summary": "Psychological measurement is critical to many disciplines. Despite advances\nin measurement, building nomological networks, theoretical maps of how concepts\nand measures relate to establish validity, remains a challenge 70 years after\nCronbach and Meehl proposed them as fundamental to validation. This limitation\nhas practical consequences: clinical trials may fail to detect treatment\neffects, and public policy may target the wrong outcomes. We introduce Analysis\nof Latent Indicators to Generate Nomological Structures (ALIGNS), a large\nlanguage model-based system trained with validated questionnaire measures.\nALIGNS provides three comprehensive nomological networks containing over\n550,000 indicators across psychology, medicine, social policy, and other\nfields. This represents the first application of large language models to solve\na foundational problem in measurement validation. We report classification\naccuracy tests used to develop the model, as well as three evaluations. In the\nfirst evaluation, the widely used NIH PROMIS anxiety and depression instruments\nare shown to converge into a single dimension of emotional distress. The second\nevaluation examines child temperament measures and identifies four potential\ndimensions not captured by current frameworks, and questions one existing\ndimension. The third evaluation, an applicability check, engages expert\npsychometricians who assess the system's importance, accessibility, and\nsuitability. ALIGNS is freely available at nomologicalnetwork.org,\ncomplementing traditional validation methods with large-scale nomological\nanalysis.", "AI": {"tldr": "ALIGNS是一个基于大型语言模型的系统，用于构建心理测量中的列联网络，以解决效度验证的长期挑战，并已应用于心理学、医学和社会政策等领域。", "motivation": "心理测量中的列联网络（nomological networks）构建是建立效度（validity）的关键，但自Cronbach和Meehl提出以来，70年来仍是一个挑战。这一局限性导致临床试验可能无法检测到治疗效果，公共政策可能针对错误的成果。", "method": "本文介绍了“分析潜在指标以生成列联结构”（ALIGNS）系统，这是一个基于大型语言模型（LLM）的系统，使用经过验证的问卷测量进行训练。ALIGNS旨在提供全面的列联网络。", "result": "ALIGNS提供了三个包含超过550,000个指标的综合列联网络，涵盖心理学、医学、社会政策等领域。这是大型语言模型首次应用于解决测量验证中的基础问题。模型经过分类准确性测试。三项评估结果显示：1) NIH PROMIS焦虑和抑郁工具趋向于单一的情绪困扰维度；2) 儿童气质测量识别出四个当前框架未捕捉到的潜在维度，并质疑一个现有维度；3) 专家心理计量师评估了系统的“重要性”、“可访问性”和“适用性”。", "conclusion": "ALIGNS通过大规模的列联分析，补充了传统的验证方法。该系统免费提供，有望解决测量验证中的长期挑战，并对多个学科产生实际影响。"}}
{"id": "2509.10210", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10210", "abs": "https://arxiv.org/abs/2509.10210", "authors": ["Marko Petković", "Vlado Menkovski", "Sofía Calero"], "title": "Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction", "comment": null, "summary": "Automated characterization of porous materials has the potential to\naccelerate materials discovery, but it remains limited by the complexity of\nsimulation setup and force field selection. We propose a multi-agent framework\nin which LLM-based agents can autonomously understand a characterization task,\nplan appropriate simulations, assemble relevant force fields, execute them and\ninterpret their results to guide subsequent steps. As a first step toward this\nvision, we present a multi-agent system for literature-informed force field\nextraction and automated RASPA simulation setup. Initial evaluations\ndemonstrate high correctness and reproducibility, highlighting this approach's\npotential to enable fully autonomous, scalable materials characterization.", "AI": {"tldr": "该论文提出一个基于LLM的多智能体框架，旨在自动化多孔材料的表征过程，通过自主理解任务、规划模拟、选择力场、执行并解释结果，以克服传统模拟设置和力场选择的复杂性。", "motivation": "多孔材料的自动化表征对于加速材料发现至关重要，但目前受限于模拟设置的复杂性和合适的力场选择的困难。", "method": "研究者提出了一个多智能体框架，其中基于大型语言模型（LLM）的智能体能够自主理解表征任务、规划适当的模拟、组装相关的力场、执行模拟并解释结果以指导后续步骤。作为第一步，他们实现了一个用于文献驱动的力场提取和自动化RASPA模拟设置的多智能体系统。", "result": "初步评估结果表明，该方法具有高度的正确性和可重复性。", "conclusion": "该方法有望实现完全自主、可扩展的材料表征，从而加速材料科学的发现过程。"}}
{"id": "2509.09943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09943", "abs": "https://arxiv.org/abs/2509.09943", "authors": ["Zhu Chen", "Mert Edgü", "Er Jin", "Johannes Stegmaier"], "title": "Segment Anything for Cell Tracking", "comment": null, "summary": "Tracking cells and detecting mitotic events in time-lapse microscopy image\nsequences is a crucial task in biomedical research. However, it remains highly\nchallenging due to dividing objects, low signal-tonoise ratios, indistinct\nboundaries, dense clusters, and the visually similar appearance of individual\ncells. Existing deep learning-based methods rely on manually labeled datasets\nfor training, which is both costly and time-consuming. Moreover, their\ngeneralizability to unseen datasets remains limited due to the vast diversity\nof microscopy data. To overcome these limitations, we propose a zero-shot cell\ntracking framework by integrating Segment Anything 2 (SAM2), a large foundation\nmodel designed for general image and video segmentation, into the tracking\npipeline. As a fully-unsupervised approach, our method does not depend on or\ninherit biases from any specific training dataset, allowing it to generalize\nacross diverse microscopy datasets without finetuning. Our approach achieves\ncompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos\nwhile eliminating the need for dataset-specific adaptation.", "AI": {"tldr": "本文提出了一种零样本细胞追踪框架，通过整合Segment Anything 2 (SAM2)实现了完全无监督的细胞追踪，解决了现有深度学习方法对人工标注的依赖和泛化性差的问题，并在2D和3D延时显微镜视频中取得了有竞争力的准确性。", "motivation": "细胞追踪和有丝分裂事件检测在生物医学研究中至关重要，但面临对象分裂、低信噪比、边界模糊、密集聚类和细胞外观相似等挑战。现有基于深度学习的方法依赖昂贵耗时的人工标注数据集进行训练，且对未见数据集的泛化能力有限。", "method": "提出了一种零样本细胞追踪框架，将通用图像和视频分割基础模型Segment Anything 2 (SAM2)整合到追踪流程中。这是一种完全无监督的方法，不依赖或继承任何特定训练数据集的偏差，因此无需微调即可泛化到不同的显微镜数据集。", "result": "该方法在2D和大规模3D延时显微镜视频中均取得了有竞争力的准确性，同时消除了对数据集特定适应的需求。", "conclusion": "通过整合SAM2，本文提出的零样本、完全无监督的细胞追踪框架克服了现有方法的局限性，实现了在多样化显微镜数据上的良好泛化能力，无需人工标注或微调，为细胞追踪提供了一种高效且普适的解决方案。"}}
{"id": "2509.10349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10349", "abs": "https://arxiv.org/abs/2509.10349", "authors": ["Weiyan Lu", "Huizhe Li", "Yuhao Fang", "Zhexuan Zhou", "Junda Wu", "Yude Li", "Youmin Gong", "Jie Mei"], "title": "Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) with suspended payloads offer significant\nadvantages for aerial transportation in complex and cluttered environments.\nHowever, existing systems face critical limitations, including unreliable\nperception of the cable-payload dynamics, inefficient planning in large-scale\nenvironments, and the inability to guarantee whole-body safety under cable\nbending and external disturbances. This paper presents Acetrans, an Autonomous,\nCorridor-based, and Efficient UAV suspended transport system that addresses\nthese challenges through a unified perception, planning, and control framework.\nA LiDAR-IMU fusion module is proposed to jointly estimate both payload pose and\ncable shape under taut and bent modes, enabling robust whole-body state\nestimation and real-time filtering of cable point clouds. To enhance planning\nscalability, we introduce the Multi-size-Aware Configuration-space Iterative\nRegional Inflation (MACIRI) algorithm, which generates safe flight corridors\nwhile accounting for varying UAV and payload geometries. A spatio-temporal,\ncorridor-constrained trajectory optimization scheme is then developed to ensure\ndynamically feasible and collision-free trajectories. Finally, a nonlinear\nmodel predictive controller (NMPC) augmented with cable-bending constraints\nprovides robust whole-body safety during execution. Simulation and experimental\nresults validate the effectiveness of Acetrans, demonstrating substantial\nimprovements in perception accuracy, planning efficiency, and control safety\ncompared to state-of-the-art methods.", "AI": {"tldr": "本文提出Acetrans，一个自主、基于走廊、高效的无人机悬挂运输系统，通过统一的感知、规划和控制框架，解决了现有系统在感知、规划效率和全身安全方面的挑战。", "motivation": "现有的无人机悬挂运输系统面临关键限制，包括：对缆绳-有效载荷动力学感知不可靠；在大规模环境中规划效率低下；以及在缆绳弯曲和外部扰动下无法保证全身安全。", "method": "本文提出了一个统一的感知、规划和控制框架：1. LiDAR-IMU融合模块，用于联合估计有效载荷姿态和缆绳形状（拉紧和弯曲模式），实现鲁棒的全身状态估计和缆绳点云实时滤波。2. MACIRI算法，生成考虑不同无人机和有效载荷几何形状的安全飞行走廊，增强规划可扩展性。3. 时空、走廊约束的轨迹优化方案，确保动态可行和无碰撞轨迹。4. 增强了缆绳弯曲约束的非线性模型预测控制器（NMPC），在执行过程中提供鲁棒的全身安全。", "result": "仿真和实验结果验证了Acetrans的有效性，与现有最先进方法相比，在感知精度、规划效率和控制安全性方面均有显著提高。", "conclusion": "Acetrans通过其统一的感知、规划和控制框架，成功解决了无人机悬挂运输面临的关键挑战，并展示了优越的性能。"}}
{"id": "2509.09724", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T09"], "pdf": "https://arxiv.org/pdf/2509.09724", "abs": "https://arxiv.org/abs/2509.09724", "authors": ["Wonyoung Kim", "Sujeong Seo", "Juhyun Lee"], "title": "DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model", "comment": "5 figures", "summary": "Technology opportunities are critical information that serve as a foundation\nfor advancements in technology, industry, and innovation. This paper proposes a\nframework based on the temporal relationships between technologies to identify\nemerging technology opportunities. The proposed framework begins by extracting\ntext from a patent dataset, followed by mapping text-based topics to discover\ninter-technology relationships. Technology opportunities are then identified by\ntracking changes in these topics over time. To enhance efficiency, the\nframework leverages a large language model to extract topics and employs a\nprompt for a chat-based language model to support the discovery of technology\nopportunities. The framework was evaluated using an artificial intelligence\npatent dataset provided by the United States Patent and Trademark Office. The\nexperimental results suggest that artificial intelligence technology is\nevolving into forms that facilitate everyday accessibility. This approach\ndemonstrates the potential of the proposed framework to identify future\ntechnology opportunities.", "AI": {"tldr": "本文提出一个基于技术间时间关系识别新兴技术机会的框架，该框架利用大型语言模型从专利数据中提取主题并追踪其随时间的变化，以发现技术机会。", "motivation": "技术机会是推动技术、产业和创新进步的关键信息，因此需要一种有效的方法来识别它们。", "method": "该框架首先从专利数据集中提取文本，然后将基于文本的主题映射以发现技术间关系。通过追踪这些主题随时间的变化来识别技术机会。为提高效率，框架利用大型语言模型提取主题，并使用基于聊天的语言模型提示来支持技术机会的发现。该框架已在美国专利商标局提供的AI专利数据集上进行了评估。", "result": "实验结果表明，人工智能技术正朝着促进日常可及性的方向发展。该方法展示了所提出框架在识别未来技术机会方面的潜力。", "conclusion": "所提出的基于技术时间关系的框架能够有效识别未来的技术机会，并具有实际应用潜力。"}}
{"id": "2509.10222", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10222", "abs": "https://arxiv.org/abs/2509.10222", "authors": ["Maël Jullien", "Lei Xu", "Marco Valentino", "André Freitas"], "title": "Compartmentalised Agentic Reasoning for Clinical NLI", "comment": null, "summary": "A common assumption holds that scaling data and parameters yields\nincreasingly structured, generalisable internal representations. We interrogate\nthis assumption in clinical natural language inference (NLI) by adopting a\nbenchmark decomposed into four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction,\nand introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI\nthat separates knowledge access from principled inference. CARENLI routes each\npremise, statement pair to a family specific solver and enforces auditable\nprocedures via a planner, verifier, and refiner.\n  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching\n98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag\nviolations with near-ceiling reliability, while refiners correct a substantial\nshare of epistemic errors. Remaining failures cluster in routing, identifying\nfamily classification as the main bottleneck. These results show that LLMs\noften retain relevant facts but default to heuristics when inference is\nunderspecified, a dissociation CARENLI makes explicit while offering a\nframework for safer, auditable reasoning.", "AI": {"tldr": "本研究质疑了在临床自然语言推理（NLI）中，数据和参数扩展能带来更结构化、泛化性强的内部表示的假设。为此，我们引入了CARENLI，一个将知识获取与推理分离的系统，它通过特定家族的求解器和可审计的程序，显著提高了推理的准确性，并揭示了大型语言模型（LLMs）在推理不足时倾向于使用启发式方法。", "motivation": "普遍的假设认为，扩展数据和参数能使内部表示更结构化、更具泛化性。本研究旨在在临床自然语言推理（NLI）领域质疑这一假设，并寻求提高推理的准确性和可审计性。", "method": "本研究引入了CARENLI（临床NLI分层智能推理系统），它将临床NLI分解为四个推理家族：因果归因、组合基础、认知验证和风险状态抽象。CARENLI将知识获取与原则性推理分离，将每个前提-陈述对路由到特定家族的求解器，并通过规划器、验证器和修正器强制执行可审计的程序。", "result": "在四种大型语言模型上，CARENLI将推理准确性提高了高达42个百分点，在因果归因中达到98.0%，在风险状态抽象中达到81.2%。验证器在标记违规方面显示出接近上限的可靠性，修正器纠正了大量认知错误。剩余的失败集中在路由环节，表明家族分类是主要的瓶颈。结果表明，LLMs通常保留相关事实，但在推理不足时默认采用启发式方法。", "conclusion": "CARENLI明确揭示了大型语言模型在保留事实和默认使用启发式方法之间的分离，并为更安全、可审计的推理提供了一个框架。这表明仅靠扩展规模并不能保证在临床NLI中获得结构化的内部表示。"}}
{"id": "2509.09946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09946", "abs": "https://arxiv.org/abs/2509.09946", "authors": ["Vu-Minh Le", "Thao-Anh Tran", "Duc Huy Do", "Xuan Canh Do", "Huong Ninh", "Hai Tran"], "title": "Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation", "comment": "Accepted at ICCVW 2025", "summary": "Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision\ntask for automating large-scale surveillance. With camera calibration and depth\ninformation, the targets in the scene can be projected into 3D space, offering\nunparalleled levels of automatic perception of a 3D environment. However,\ntracking in the 3D space requires replacing all 2D tracking components from the\nground up, which may be infeasible for existing MTMC systems. In this paper, we\npresent an approach for extending any online 2D multi-camera tracking system\ninto 3D space by utilizing depth information to reconstruct a target in\npoint-cloud space, and recovering its 3D box through clustering and yaw\nrefinement following tracking. We also introduced an enhanced online data\nassociation mechanism that leverages the target's local ID consistency to\nassign global IDs across frames. The proposed framework is evaluated on the\n2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the\nleaderboard.", "AI": {"tldr": "本文提出了一种将任何在线2D多摄像头多目标跟踪系统扩展到3D空间的方法，通过利用深度信息重建目标并恢复其3D边界框，同时引入了增强的在线数据关联机制。", "motivation": "多目标多摄像头跟踪（MTMC）是自动化大规模监控的关键任务。尽管3D空间跟踪能提供无与伦比的3D环境感知能力，但它需要彻底替换所有2D跟踪组件，这对于现有MTMC系统来说可能不切实际。", "method": "该方法通过以下步骤将2D系统扩展到3D：1) 利用深度信息在点云空间中重建目标；2) 通过聚类和偏航角细化恢复目标的3D边界框；3) 引入了一种增强的在线数据关联机制，该机制利用目标的局部ID一致性来跨帧分配全局ID。", "result": "所提出的框架在2025年AI City Challenge的3D MTMC数据集上进行了评估，并在排行榜上取得了第三名的成绩。", "conclusion": "该研究成功地提出了一种将现有在线2D多摄像头跟踪系统扩展到3D空间的可行方法，并通过引入改进的数据关联机制提升了跟踪性能，在相关挑战赛中表现出色。"}}
{"id": "2509.10405", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10405", "abs": "https://arxiv.org/abs/2509.10405", "authors": ["Nicholas Carlotti", "Mirko Nava", "Alessandro Giusti"], "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels By Classifying LED States", "comment": "accepted at CoRL 2025", "summary": "We introduce a model for monocular RGB relative pose estimation of a ground\nrobot that trains from scratch without pose labels nor prior knowledge about\nthe robot's shape or appearance. At training time, we assume: (i) a robot\nfitted with multiple LEDs, whose states are independent and known at each\nframe; (ii) knowledge of the approximate viewing direction of each LED; and\n(iii) availability of a calibration image with a known target distance, to\naddress the ambiguity of monocular depth estimation. Training data is collected\nby a pair of robots moving randomly without needing external infrastructure or\nhuman supervision. Our model trains on the task of predicting from an image the\nstate of each LED on the robot. In doing so, it learns to predict the position\nof the robot in the image, its distance, and its relative bearing. At inference\ntime, the state of the LEDs is unknown, can be arbitrary, and does not affect\nthe pose estimation performance. Quantitative experiments indicate that our\napproach: is competitive with SoA approaches that require supervision from pose\nlabels or a CAD model of the robot; generalizes to different domains; and\nhandles multi-robot pose estimation.", "AI": {"tldr": "本文提出了一种用于地面机器人单目RGB相对姿态估计的模型，该模型无需姿态标签或机器人CAD模型即可从零开始训练，利用机器人上的LED灯状态作为监督信号。", "motivation": "现有的单目机器人姿态估计算法通常需要姿态标签或机器人CAD模型进行监督，这限制了其应用场景和部署便利性。本研究旨在开发一种无需这些先验知识的训练方法。", "method": "该模型在训练时假设：(i) 机器人配备多个LED灯，其状态独立且在每帧已知；(ii) 每个LED灯的近似视线方向已知；(iii) 可用一张已知目标距离的校准图像来解决单目深度模糊性。训练数据由一对机器人随机移动收集，无需外部基础设施或人工监督。模型训练的目标是从图像中预测每个LED灯的状态，从而间接学习机器人在图像中的位置、距离和相对方位。在推理时，LED灯的状态未知且任意，不影响姿态估计性能。", "result": "实验结果表明，该方法：与需要姿态标签或机器人CAD模型的现有最先进方法具有竞争力；能够泛化到不同领域；并且支持多机器人姿态估计。", "conclusion": "本研究成功引入了一种新颖的单目RGB地面机器人相对姿态估计模型，该模型在无需姿态标签或CAD模型的情况下，通过利用机器人上的LED灯状态进行自监督训练，实现了与有监督方法相当的性能，并展现出良好的泛化能力和多机器人处理能力。"}}
{"id": "2509.09725", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09725", "abs": "https://arxiv.org/abs/2509.09725", "authors": ["Chunyu Li", "Xindi Zheng", "Siqi Liu"], "title": "BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025", "comment": null, "summary": "Entity linking (EL) for biomedical text is typically benchmarked on\nEnglish-only corpora with flat mentions, leaving the more realistic scenario of\nnested and multilingual mentions largely unexplored. We present our system for\nthe BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task\n(English & Russian), closing this gap with a lightweight pipeline that keeps\nthe original EL model intact and modifies only three task-aligned components:\nTwo-stage retrieval-ranking. We leverage the same base encoder model in both\nstages: the retrieval stage uses the original pre-trained model, while the\nranking stage applies domain-specific fine-tuning. Boundary cues. In the\nranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing\nthe encoder with an explicit, language-agnostic span before robustness to\noverlap and nesting. Dataset augmentation. We also automatically expand the\nranking training corpus with three complementary data sources, enhancing\ncoverage without extra manual annotation. On the BioNNE 2025 leaderboard, our\ntwo stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual\ntrack, demonstrating the effectiveness and competitiveness of these minimal yet\nprincipled modifications. Code are publicly available at\nhttps://github.com/Kaggle-Competitions-Code/BioNNE-L.", "AI": {"tldr": "本文介绍了一种名为BIBERT-Pipe的轻量级系统，用于解决生物医学文本中的多语言嵌套实体链接问题，通过两阶段检索-排序、边界提示和数据增强，在BioNNE 2025多语言赛道中排名第三。", "motivation": "目前的生物医学实体链接（EL）基准测试主要集中于英语语料库和扁平化提及，忽略了更具挑战性的嵌套和多语言提及的现实场景。", "method": "该系统采用轻量级管道，保留了原始EL模型，并修改了三个任务对齐的组件：1) 两阶段检索-排序，在检索阶段使用原始预训练模型，在排序阶段应用领域特定微调。2) 边界提示，在排序阶段用可学习的[Ms]/[Me]标签包裹每个提及，提供明确、与语言无关的跨度信息。3) 数据增强，自动扩展排序训练语料库，使用三个互补数据源以增强覆盖范围。", "result": "在BioNNE 2025排行榜上，该系统（BIBERT-Pipe）在多语言赛道中排名第三。", "conclusion": "这些最小而有原则的修改（两阶段检索-排序、边界提示和数据增强）在处理多语言嵌套生物医学实体链接方面表现出有效性和竞争力。"}}
{"id": "2509.10249", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10249", "abs": "https://arxiv.org/abs/2509.10249", "authors": ["Hanna Abi Akl"], "title": "Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering", "comment": "accepted for the International Joint Conference on Rules and\n  Reasoning (RuleML+RR) 2025", "summary": "Recent advances in Language Models (LMs) have failed to mask their\nshortcomings particularly in the domain of reasoning. This limitation impacts\nseveral tasks, most notably those involving ontology engineering. As part of a\nPhD research, we investigate the consequences of incorporating formal methods\non the performance of Small Language Models (SLMs) on reasoning tasks.\nSpecifically, we aim to orient our work toward using SLMs to bootstrap ontology\nconstruction and set up a series of preliminary experiments to determine the\nimpact of expressing logical problems with different grammars on the\nperformance of SLMs on a predefined reasoning task. Our findings show that it\nis possible to substitute Natural Language (NL) with a more compact logical\nlanguage while maintaining a strong performance on reasoning tasks and hope to\nuse these results to further refine the role of SLMs in ontology engineering.", "AI": {"tldr": "本研究旨在通过引入形式化方法，提升小型语言模型（SLMs）在推理任务上的表现，特别是在本体工程领域，并发现使用紧凑的逻辑语言可保持强大的推理性能。", "motivation": "大型语言模型（LMs）在推理领域存在不足，尤其是在本体工程任务中表现受限。", "method": "研究将形式化方法融入小型语言模型（SLMs）的后果，目标是利用SLMs引导本体构建。通过一系列初步实验，比较使用不同语法（包括自然语言和更紧凑的逻辑语言）表达逻辑问题对SLMs在预定义推理任务上性能的影响。", "result": "研究发现，在推理任务中，可以用更紧凑的逻辑语言替代自然语言，同时保持强大的性能。", "conclusion": "这些结果有望进一步明确SLMs在本体工程中的作用，表明形式化方法可以有效提升SLMs的推理能力。"}}
{"id": "2509.09958", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09958", "abs": "https://arxiv.org/abs/2509.09958", "authors": ["Jeffrey Liu", "Rongbin Hu"], "title": "Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification", "comment": null, "summary": "Referring Expression Comprehension (REC) is usually addressed with\ntask-trained grounding models. We show that a zero-shot workflow, without any\nREC-specific training, can achieve competitive or superior performance. Our\napproach reformulates REC as box-wise visual-language verification: given\nproposals from a COCO-clean generic detector (YOLO-World), a general-purpose\nVLM independently answers True/False queries for each region. This simple\nprocedure reduces cross-box interference, supports abstention and multiple\nmatches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our\nmethod not only surpasses a zero-shot GroundingDINO baseline but also exceeds\nreported results for GroundingDINO trained on REC and GroundingDINO+CRG.\nControlled studies with identical proposals confirm that verification\nsignificantly outperforms selection-based prompting, and results hold with open\nVLMs. Overall, we show that workflow design, rather than task-specific\npretraining, drives strong zero-shot REC performance.", "AI": {"tldr": "本文提出了一种零样本参照表达理解（REC）方法，将REC重构为逐框视觉-语言验证任务，无需特定训练即可达到甚至超越经过训练的模型性能。", "motivation": "传统的REC方法通常依赖于任务特定的接地模型训练。作者旨在探索一种零样本工作流，在不进行任何REC特定训练的情况下，能否实现有竞争力甚至更优的性能。", "method": "该方法将REC重新表述为逐框视觉-语言验证。首先，使用COCO-clean通用检测器（如YOLO-World）生成候选框。然后，一个通用视觉-语言模型（VLM）独立地对每个区域回答是/否查询。这种简单流程减少了跨框干扰，支持弃权和多重匹配，且无需微调。", "result": "在RefCOCO、RefCOCO+和RefCOCOg数据集上，该方法不仅超越了零样本GroundingDINO基线，还超过了报告中经过REC训练的GroundingDINO以及GroundingDINO+CRG的性能。受控研究证实，验证方法显著优于基于选择的提示方法，并且结果在开放VLM中也成立。", "conclusion": "研究表明，工作流设计而非任务特定预训练是实现强大零样本REC性能的关键。"}}
{"id": "2509.10416", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10416", "abs": "https://arxiv.org/abs/2509.10416", "authors": ["Ze Fu", "Pinhao Song", "Yutong Hu", "Renaud Detry"], "title": "TASC: Task-Aware Shared Control for Teleoperated Manipulation", "comment": null, "summary": "We present TASC, a Task-Aware Shared Control framework for teleoperated\nmanipulation that infers task-level user intent and provides assistance\nthroughout the task. To support everyday tasks without predefined knowledge,\nTASC constructs an open-vocabulary interaction graph from visual input to\nrepresent functional object relationships, and infers user intent accordingly.\nA shared control policy then provides rotation assistance during both grasping\nand object interaction, guided by spatial constraints predicted by a\nvision-language model. Our method addresses two key challenges in\ngeneral-purpose, long-horizon shared control: (1) understanding and inferring\ntask-level user intent, and (2) generalizing assistance across diverse objects\nand tasks. Experiments in both simulation and the real world demonstrate that\nTASC improves task efficiency and reduces user input effort compared to prior\nmethods. To the best of our knowledge, this is the first shared control\nframework that supports everyday manipulation tasks with zero-shot\ngeneralization. The code that supports our experiments is publicly available at\nhttps://github.com/fitz0401/tasc.", "AI": {"tldr": "TASC是一个任务感知的共享控制框架，通过构建开放词汇交互图和视觉-语言模型，推断用户意图并提供旋转辅助，实现了对日常遥控操作任务的零样本泛化，提高了效率并减少了用户输入。", "motivation": "现有通用、长周期共享控制面临两大挑战：1) 理解和推断任务级别的用户意图；2) 将辅助泛化到各种物体和任务，尤其是在没有预定义知识的情况下支持日常任务。", "method": "TASC框架通过视觉输入构建开放词汇交互图，以表示物体功能关系并推断用户意图。一个共享控制策略在抓取和物体交互过程中提供旋转辅助，其指导来源于视觉-语言模型预测的空间约束。", "result": "在仿真和现实世界实验中，TASC与现有方法相比，显著提高了任务效率并减少了用户输入工作量。它是首个支持日常操作任务并具有零样本泛化能力的共享控制框架。", "conclusion": "TASC成功解决了通用、长周期共享控制中的关键挑战，即理解任务级用户意图和泛化辅助，从而在日常操作任务中实现了零样本泛化，并带来了性能提升。"}}
{"id": "2509.09726", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09726", "abs": "https://arxiv.org/abs/2509.09726", "authors": ["Seiji Hattori", "Takuya Matsuzaki", "Makoto Fujiwara"], "title": "Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure", "comment": "Submitted to INLG 2025 (accepted)", "summary": "This paper proposes a natural language translation method for\nmachine-verifiable formal proofs that leverages the informalization\n(verbalization of formal language proof steps) and summarization capabilities\nof LLMs. For evaluation, it was applied to formal proof data created in\naccordance with natural language proofs taken from an undergraduate-level\ntextbook, and the quality of the generated natural language proofs was analyzed\nin comparison with the original natural language proofs. Furthermore, we will\ndemonstrate that this method can output highly readable and accurate natural\nlanguage proofs by applying it to existing formal proof library of the Lean\nproof assistant.", "AI": {"tldr": "本文提出一种利用大型语言模型（LLMs）的非形式化和摘要能力，将机器可验证的形式化证明翻译成自然语言的方法。", "motivation": "将机器可验证的形式化证明转化为人类可读的自然语言形式，以提高其可理解性。", "method": "利用LLMs对形式化证明步骤进行非形式化（口头化）和摘要。通过将该方法应用于根据本科教材自然语言证明创建的形式化证明数据进行评估，并与原始自然语言证明进行质量比较。此外，还将其应用于Lean证明助手的现有形式化证明库。", "result": "该方法能够输出高度可读且准确的自然语言证明。", "conclusion": "该方法能够有效且高质量地将形式化证明转换为自然语言证明。"}}
{"id": "2509.10297", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10297", "abs": "https://arxiv.org/abs/2509.10297", "authors": ["Eoin O'Doherty", "Nicole Weinrauch", "Andrew Talone", "Uri Klempner", "Xiaoyuan Yi", "Xing Xie", "Yi Zeng"], "title": "The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis", "comment": "Work in progress", "summary": "Artificial intelligence (AI) is advancing at a pace that raises urgent\nquestions about how to align machine decision-making with human moral values.\nThis working paper investigates how leading AI systems prioritize moral\noutcomes and what this reveals about the prospects for human-AI symbiosis. We\naddress two central questions: (1) What moral values do state-of-the-art large\nlanguage models (LLMs) implicitly favour when confronted with dilemmas? (2) How\ndo differences in model architecture, cultural origin, and explainability\naffect these moral preferences? To explore these questions, we conduct a\nquantitative experiment with six LLMs, ranking and scoring outcomes across 18\ndilemmas representing five moral frameworks. Our findings uncover strikingly\nconsistent value biases. Across all models, Care and Virtue values outcomes\nwere rated most moral, while libertarian choices were consistently penalized.\nReasoning-enabled models exhibited greater sensitivity to context and provided\nricher explanations, whereas non-reasoning models produced more uniform but\nopaque judgments. This research makes three contributions: (i) Empirically, it\ndelivers a large-scale comparison of moral reasoning across culturally distinct\nLLMs; (ii) Theoretically, it links probabilistic model behaviour with\nunderlying value encodings; (iii) Practically, it highlights the need for\nexplainability and cultural awareness as critical design principles to guide AI\ntoward a transparent, aligned, and symbiotic future.", "AI": {"tldr": "本研究通过对六个大型语言模型（LLMs）进行定量实验，发现它们在道德困境中表现出惊人一致的价值观偏见，普遍偏爱关怀和美德，而惩罚自由主义选择。具有推理能力的模型对上下文更敏感且解释性更强。", "motivation": "随着人工智能的快速发展，如何使机器决策与人类道德价值观保持一致成为一个紧迫问题。本研究旨在探究领先的AI系统如何优先考虑道德结果，以及这如何揭示人机共生的前景。", "method": "研究通过一项定量实验进行，使用了六个大型语言模型（LLMs）。实验中，模型需要对代表五种道德框架的18个困境中的结果进行排名和评分，以评估其道德偏好。", "result": "研究发现模型之间存在惊人一致的价值观偏见：所有模型都将“关怀”和“美德”价值观的结果评为最道德，而自由主义选择则始终受到惩罚。具有推理能力的模型对上下文表现出更大的敏感性，并提供了更丰富的解释，而非推理模型则产生更统一但更不透明的判断。", "conclusion": "本研究强调了可解释性和文化意识作为关键设计原则的重要性，以指导AI走向透明、对齐和共生的未来。它为跨文化LLM的道德推理提供了大规模比较，并理论上将概率模型行为与潜在的价值编码联系起来。"}}
{"id": "2509.09961", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09961", "abs": "https://arxiv.org/abs/2509.09961", "authors": ["Tianqi Wei", "Xin Yu", "Zhi Chen", "Scott Chapman", "Zi Huang"], "title": "Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation", "comment": null, "summary": "Accurate segmentation of foliar diseases and insect damage in wheat is\ncrucial for effective crop management and disease control. However, the insect\ndamage typically occupies only a tiny fraction of annotated pixels. This\nextreme pixel-level imbalance poses a significant challenge to the segmentation\nperformance, which can result in overfitting to common classes and insufficient\nlearning of rare classes, thereby impairing overall performance. In this paper,\nwe propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to\naddress the pixel imbalance problem. Specifically, we extract rare\ninsect-damage patches from annotated training images and apply random geometric\ntransformations to simulate variations. The transformed patches are then pasted\nin appropriate regions while avoiding overlaps with lesions or existing damaged\nregions. In addition, we apply a random projection filter to the pasted\nregions, refining local features and ensuring a natural blend with the new\nbackground. Experiments show that our method substantially improves\nsegmentation performance on the insect damage class, while maintaining or even\nslightly enhancing accuracy on other categories. Our results highlight the\neffectiveness of targeted augmentation in mitigating extreme pixel imbalance,\noffering a straightforward yet effective solution for agricultural segmentation\nproblems.", "AI": {"tldr": "本文提出了一种名为随机投影复制粘贴（RPCP）的数据增强技术，以解决小麦叶部病害和虫害分割中极度像素不平衡的问题，尤其针对稀有虫害类别。", "motivation": "小麦叶部病害和虫害分割对于作物管理至关重要。然而，虫害通常只占标注像素的极小部分，导致像素级极度不平衡。这使得模型容易过拟合常见类别，对稀有类别学习不足，从而影响整体分割性能。", "method": "本文提出了随机投影复制粘贴（RPCP）增强技术。具体方法是：从标注训练图像中提取稀有虫害斑块，应用随机几何变换模拟变异，然后将变换后的斑块粘贴到适当区域（避免与病变或现有受损区域重叠）。此外，对粘贴区域应用随机投影滤波器，以细化局部特征并确保与新背景自然融合。", "result": "实验结果表明，该方法显著提高了虫害类别的分割性能，同时保持甚至略微提升了其他类别的准确性。", "conclusion": "研究结果强调了目标增强在缓解极端像素不平衡方面的有效性，为农业分割问题提供了一个直接而有效的解决方案。"}}
{"id": "2509.10426", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.10426", "abs": "https://arxiv.org/abs/2509.10426", "authors": ["Jianxin Shi", "Zengqi Peng", "Xiaolong Chen", "Tianyu Wo", "Jun Ma"], "title": "DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training", "comment": null, "summary": "Trajectory prediction is a critical component of autonomous driving,\nessential for ensuring both safety and efficiency on the road. However,\ntraditional approaches often struggle with the scarcity of labeled data and\nexhibit suboptimal performance in multi-agent prediction scenarios. To address\nthese challenges, we introduce a disentangled context-aware pre-training\nframework for multi-agent motion prediction, named DECAMP. Unlike existing\nmethods that entangle representation learning with pretext tasks, our framework\ndecouples behavior pattern learning from latent feature reconstruction,\nprioritizing interpretable dynamics and thereby enhancing scene representation\nfor downstream prediction. Additionally, our framework incorporates\ncontext-aware representation learning alongside collaborative spatial-motion\npretext tasks, which enables joint optimization of structural and intentional\nreasoning while capturing the underlying dynamic intentions. Our experiments on\nthe Argoverse 2 benchmark showcase the superior performance of our method, and\nthe results attained underscore its effectiveness in multi-agent motion\nforecasting. To the best of our knowledge, this is the first context\nautoencoder framework for multi-agent motion forecasting in autonomous driving.\nThe code and models will be made publicly available.", "AI": {"tldr": "提出了一种名为DECAMP的解耦上下文感知预训练框架，用于多智能体运动预测，通过解耦行为模式学习和潜在特征重建，并结合上下文感知表示学习和协作空间-运动预训练任务，显著提高了自动驾驶中多智能体轨迹预测的性能。", "motivation": "传统方法在标签数据稀缺和多智能体预测场景中表现不佳，且现有方法常将表示学习与预训练任务纠缠在一起，导致可解释性不足。", "method": "引入了DECAMP框架，它解耦了行为模式学习与潜在特征重建，优先考虑可解释的动态性；同时，结合了上下文感知表示学习和协作空间-运动预训练任务，以联合优化结构和意图推理，并捕捉潜在的动态意图。", "result": "在Argoverse 2基准测试中，DECAMP展现出卓越的性能，并证明了其在多智能体运动预测中的有效性。", "conclusion": "DECAMP是首个用于自动驾驶中多智能体运动预测的上下文自编码器框架，有效解决了现有方法的挑战，提高了预测准确性和场景表示能力。"}}
{"id": "2509.09727", "categories": ["cs.CL", "cs.CE"], "pdf": "https://arxiv.org/pdf/2509.09727", "abs": "https://arxiv.org/abs/2509.09727", "authors": ["Andy Zhu", "Yingjun Du"], "title": "A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs", "comment": "8 pages, 6 figures, Underreview", "summary": "Question answering (QA) plays a central role in financial education, yet\nexisting large language model (LLM) approaches often fail to capture the\nnuanced and specialized reasoning required for financial problem-solving. The\nfinancial domain demands multistep quantitative reasoning, familiarity with\ndomain-specific terminology, and comprehension of real-world scenarios. We\npresent a multi-agent framework that leverages role-based prompting to enhance\nperformance on domain-specific QA. Our framework comprises a Base Generator, an\nEvidence Retriever, and an Expert Reviewer agent that work in a single-pass\niteration to produce a refined answer. We evaluated our framework on a set of\n3,532 expert-designed finance education questions from Study.com, an online\nlearning platform. We leverage retrieval-augmented generation (RAG) for\ncontextual evidence from 6 finance textbooks and prompting strategies for a\ndomain-expert reviewer. Our experiments indicate that critique-based refinement\nimproves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,\nwith the highest performance from Gemini-2.0-Flash. Furthermore, our method\nenables GPT-4o-mini to achieve performance comparable to the finance-tuned\nFinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to\nenhancing financial QA and offer insights for further research in multi-agent\nfinancial LLM systems.", "AI": {"tldr": "本文提出一个多智能体框架，通过角色提示和 RAG 提升金融教育问答的准确性，超越零样本 CoT 基线，并使小型 LLM 达到与专业模型相当的性能。", "motivation": "现有大型语言模型（LLM）在金融领域问答中表现不佳，难以处理多步量化推理、专业术语和真实世界场景理解等金融问题所需的细致和专业推理。", "method": "开发了一个多智能体框架，利用基于角色的提示来增强领域特定问答的性能。该框架包含一个基础生成器（Base Generator）、一个证据检索器（Evidence Retriever）和一个专家评审员（Expert Reviewer），以单次迭代方式协作生成优化答案。研究利用 RAG 从 6 本金融教科书获取上下文证据，并为领域专家评审员设计提示策略。该框架在 Study.com 的 3,532 个专家设计的金融教育问题上进行了评估。", "result": "基于批判性反馈的答案优化比零样本思维链（Chain-of-Thought）基线提高了 6.6-8.3% 的准确率，其中 Gemini-2.0-Flash 表现最佳。此外，该方法使得 GPT-4o-mini 达到了与金融领域微调模型 FinGPT-mt_Llama3-8B_LoRA 相当的性能。", "conclusion": "该研究提供了一种经济有效的方法来增强金融问答系统，并为多智能体金融 LLM 系统的进一步研究提供了见解。"}}
{"id": "2509.10326", "categories": ["cs.AI", "cs.LO", "03G27 (Primary) 68W30, 68T27 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.10326", "abs": "https://arxiv.org/abs/2509.10326", "authors": ["Dmitry Lesnik", "Tobias Schäfer"], "title": "State Algebra for Propositional Logic", "comment": "47 pages", "summary": "This paper presents State Algebra, a novel framework designed to represent\nand manipulate propositional logic using algebraic methods. The framework is\nstructured as a hierarchy of three representations: Set, Coordinate, and Row\nDecomposition. These representations anchor the system in well-known semantics\nwhile facilitating the computation using a powerful algebraic engine. A key\naspect of State Algebra is its flexibility in representation. We show that\nalthough the default reduction of a state vector is not canonical, a unique\ncanonical form can be obtained by applying a fixed variable order during the\nreduction process. This highlights a trade-off: by foregoing guaranteed\ncanonicity, the framework gains increased flexibility, potentially leading to\nmore compact representations of certain classes of problems. We explore how\nthis framework provides tools to articulate both search-based and knowledge\ncompilation algorithms and discuss its natural extension to probabilistic logic\nand Weighted Model Counting.", "AI": {"tldr": "本文提出了状态代数（State Algebra），一个用于表示和操作命题逻辑的代数框架，具有分层表示、计算灵活性，并可扩展到概率逻辑。", "motivation": "旨在通过代数方法表示和操作命题逻辑，提供一个强大的代数计算引擎和灵活的表示形式。", "method": "引入了状态代数框架，该框架由集合（Set）、坐标（Coordinate）和行分解（Row Decomposition）三个层次的表示构成。研究了默认规约的非规范性与通过固定变量顺序实现唯一规范形式之间的权衡。", "result": "状态代数提供了灵活的表示，默认规约虽非规范，但通过固定变量顺序可获得唯一的规范形式。这种灵活性可能导致某些问题更紧凑的表示。该框架为基于搜索和知识编译算法提供了工具，并可自然扩展到概率逻辑和加权模型计数。", "conclusion": "状态代数是一个灵活且强大的命题逻辑框架，在规范性和表示紧凑性之间取得平衡，并具有扩展到概率逻辑和加权模型计数的潜力。"}}
{"id": "2509.09962", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09962", "abs": "https://arxiv.org/abs/2509.09962", "authors": ["Anne Marthe Sophie Ngo Bibinbe", "Chiron Bang", "Patrick Gagnon", "Jamie Ahloy-Dallaire", "Eric R. Paquet"], "title": "An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock", "comment": "13 pages, 7 figures, 1 table, accepted at CVPR animal workshop 2024,\n  submitted to IJCV", "summary": "The need for long-term multi-object tracking (MOT) is growing due to the\ndemand for analyzing individual behaviors in videos that span several minutes.\nUnfortunately, due to identity switches between objects, the tracking\nperformance of existing MOT approaches decreases over time, making them\ndifficult to apply for long-term tracking. However, in many real-world\napplications, such as in the livestock sector, it is possible to obtain\nsporadic identifications for some of the animals from sources like feeders. To\naddress the challenges of long-term MOT, we propose a new framework that\ncombines both uncertain identities and tracking using a Hidden Markov Model\n(HMM) formulation. In addition to providing real-world identities to animals,\nour HMM framework improves the F1 score of ByteTrack, a leading MOT approach\neven with re-identification, on a 10 minute pig tracking dataset with 21\nidentifications at the pen's feeding station. We also show that our approach is\nrobust to the uncertainty of identifications, with performance increasing as\nidentities are provided more frequently. The improved performance of our HMM\nframework was also validated on the MOT17 and MOT20 benchmark datasets using\nboth ByteTrack and FairMOT. The code for this new HMM framework and the new\n10-minute pig tracking video dataset are available at:\nhttps://github.com/ngobibibnbe/uncertain-identity-aware-tracking", "AI": {"tldr": "本文提出了一种基于隐马尔可夫模型（HMM）的新框架，通过整合不确定的零星身份信息，解决了长期多目标跟踪（MOT）中身份切换导致性能下降的问题，并在猪跟踪数据集及MOT基准上取得了显著改进。", "motivation": "由于现有MOT方法在长时间跟踪中易发生身份切换，导致性能随时间下降，难以应用于需要分析个体行为的长期视频。然而，在许多实际应用中（如畜牧业），可以从喂食器等来源获取零星的动物身份信息，这为解决长期MOT挑战提供了机会。", "method": "作者提出了一个新框架，该框架利用隐马尔可夫模型（HMM）将不确定的身份信息与跟踪过程相结合。该方法旨在利用零星的真实世界身份来提高长期跟踪的准确性。", "result": "该HMM框架在10分钟的猪跟踪数据集上，即使仅有21个来自喂食站的识别信息，也能提高领先MOT方法ByteTrack（包含重识别功能）的F1分数。研究还表明，该方法对识别信息的不确定性具有鲁棒性，且性能随识别频率的增加而提高。此外，该HMM框架在MOT17和MOT20基准数据集上，结合ByteTrack和FairMOT，也验证了其性能改进。", "conclusion": "所提出的基于HMM的框架通过有效整合不确定的零星身份信息，成功解决了长期多目标跟踪中的身份切换问题。该方法不仅提高了跟踪性能和鲁棒性，而且在实际应用场景和标准基准数据集上均得到了验证，为长期个体行为分析提供了新的解决方案。"}}
{"id": "2509.10444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10444", "abs": "https://arxiv.org/abs/2509.10444", "authors": ["Chaerim Moon", "Joohyung Kim"], "title": "Coordinated Motion Planning of a Wearable Multi-Limb System for Enhanced Human-Robot Interaction", "comment": "Presented in IROS 2023 Workshop (Multilimb Coordination in Human\n  Neuroscience and Robotics: Classical and Learning Perspectives)", "summary": "Supernumerary Robotic Limbs (SRLs) can enhance human capability within close\nproximity. However, as a wearable device, the generated moment from its\noperation acts on the human body as an external torque. When the moments\nincrease, more muscle units are activated for balancing, and it can result in\nreduced muscular null space. Therefore, this paper suggests a concept of a\nmotion planning layer that reduces the generated moment for enhanced\nHuman-Robot Interaction. It modifies given trajectories with desirable angular\nacceleration and position deviation limits. Its performance to reduce the\nmoment is demonstrated through the simulation, which uses simplified human and\nrobotic system models.", "AI": {"tldr": "该论文提出了一种运动规划层，用于减少外骨骼机器人（SRLs）在操作过程中对人体产生的力矩，从而增强人机交互。", "motivation": "可穿戴的外骨骼机器人（SRLs）在操作时会对人体产生外部力矩，导致肌肉激活增加和肌肉零空间减少，从而影响人机交互和人体能力。", "method": "提出了一种运动规划层，通过限制期望的角加速度和位置偏差来修改给定的轨迹，以达到减少生成力矩的目的。", "result": "通过使用简化的人体和机器人系统模型进行的仿真，证明了该方法在减少力矩方面的有效性。", "conclusion": "该运动规划层能够通过减少外骨骼机器人产生的力矩来增强人机交互体验。"}}
{"id": "2509.09728", "categories": ["cs.CL", "cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2509.09728", "abs": "https://arxiv.org/abs/2509.09728", "authors": ["Elena Rohde", "Jonas Klingwort", "Christian Borgs"], "title": "A meta-analysis on the performance of machine-learning based language models for sentiment analysis", "comment": null, "summary": "This paper presents a meta-analysis evaluating ML performance in sentiment\nanalysis for Twitter data. The study aims to estimate the average performance,\nassess heterogeneity between and within studies, and analyze how study\ncharacteristics influence model performance. Using PRISMA guidelines, we\nsearched academic databases and selected 195 trials from 20 studies with 12\nstudy features. Overall accuracy, the most reported performance metric, was\nanalyzed using double arcsine transformation and a three-level random effects\nmodel. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,\n0.84]. This paper provides two key insights: 1) Overall accuracy is widely used\nbut often misleading due to its sensitivity to class imbalance and the number\nof sentiment classes, highlighting the need for normalization. 2) Standardized\nreporting of model performance, including reporting confusion matrices for\nindependent test sets, is essential for reliable comparisons of ML classifiers\nacross studies, which seems far from common practice.", "AI": {"tldr": "本研究对Twitter数据的情感分析中机器学习模型性能进行了荟萃分析，发现平均准确率为0.80，并强调了标准化报告和规范化性能指标的重要性。", "motivation": "评估机器学习在Twitter情感分析中的表现，估计平均性能，评估研究内部和研究间的异质性，并分析研究特征如何影响模型性能。", "method": "采用PRISMA指南进行荟萃分析，检索学术数据库，从20项研究中选择了195个试验，并提取了12个研究特征。使用双反正弦变换和三级随机效应模型分析了最常报告的性能指标——总体准确率。", "result": "AIC优化模型的平均总体准确率为0.80 [0.76, 0.84]。研究发现，总体准确率因类别不平衡和情感类别数量而易产生误导，需要进行规范化。此外，标准化报告模型性能（包括独立测试集的混淆矩阵）对于可靠比较机器学习分类器至关重要，但目前并不常见。", "conclusion": "总体准确率在情感分析中广泛使用但常具误导性，需要规范化。为了实现机器学习分类器之间可靠的跨研究比较，标准化报告模型性能（特别是混淆矩阵）是必不可少的。"}}
{"id": "2509.10401", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10401", "abs": "https://arxiv.org/abs/2509.10401", "authors": ["Alva West", "Yixuan Weng", "Minjun Zhu", "Zhen Lin", "Yue Zhang"], "title": "Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems", "comment": null, "summary": "Failure attribution in multi-agent systems -- pinpointing the exact step\nwhere a decisive error occurs -- is a critical yet unsolved challenge. Current\nmethods treat this as a pattern recognition task over long conversation logs,\nleading to critically low step-level accuracy (below 17\\%), which renders them\nimpractical for debugging complex systems. Their core weakness is a fundamental\ninability to perform robust counterfactual reasoning: to determine if\ncorrecting a single action would have actually averted the task failure. To\nbridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)\nScaffolding, a novel agent framework that transforms failure attribution from\npattern recognition into a structured causal inference task. A2P explicitly\nguides a large language model through a formal three-step reasoning process\nwithin a single inference pass: (1) Abduction, to infer the hidden root causes\nbehind an agent's actions; (2) Action, to define a minimal corrective\nintervention; and (3) Prediction, to simulate the subsequent trajectory and\nverify if the intervention resolves the failure. This structured approach\nleverages the holistic context of the entire conversation while imposing a\nrigorous causal logic on the model's analysis. Our extensive experiments on the\nWho\\&When benchmark demonstrate its efficacy. On the Algorithm-Generated\ndataset, A2P achieves 47.46\\% step-level accuracy, a 2.85$\\times$ improvement\nover the 16.67\\% of the baseline. On the more complex Hand-Crafted dataset, it\nachieves 29.31\\% step accuracy, a 2.43$\\times$ improvement over the baseline's\n12.07\\%. By reframing the problem through a causal lens, A2P Scaffolding\nprovides a robust, verifiable, and significantly more accurate solution for\nautomated failure attribution.", "AI": {"tldr": "本文提出A2P Scaffolding框架，将多智能体系统中的故障归因从模式识别转变为结构化因果推理任务。通过溯因、行动、预测三步推理，显著提高了故障归因的步骤级准确性，解决了现有方法反事实推理能力不足的问题。", "motivation": "多智能体系统中的故障归因（即精确定位导致失败的关键错误步骤）是一个未解决的难题。现有方法将其视为对长对话日志的模式识别任务，导致步骤级准确率极低（低于17%），不适用于复杂系统的调试。其核心弱点在于无法进行稳健的反事实推理，即无法确定纠正单一行动是否能真正避免任务失败。", "method": "本文引入了Abduct-Act-Predict (A2P) Scaffolding，这是一个新颖的智能体框架，它将故障归因从模式识别任务转化为结构化的因果推理任务。A2P明确引导大型语言模型在一次推理过程中完成正式的三步推理：(1) 溯因 (Abduction)，推断智能体行动背后的隐藏根源；(2) 行动 (Action)，定义最小的纠正干预措施；(3) 预测 (Prediction)，模拟后续轨迹并验证干预是否解决了故障。这种结构化方法利用了整个对话的整体上下文，并对模型的分析施加了严格的因果逻辑。", "result": "在Who&When基准测试上进行了广泛实验。在算法生成的数据集上，A2P实现了47.46%的步骤级准确率，比基线（16.67%）提高了2.85倍。在更复杂的手工制作数据集上，A2P实现了29.31%的步骤准确率，比基线（12.07%）提高了2.43倍。", "conclusion": "通过从因果角度重新定义问题，A2P Scaffolding为自动化故障归因提供了一个稳健、可验证且显著更准确的解决方案。"}}
{"id": "2509.09971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09971", "abs": "https://arxiv.org/abs/2509.09971", "authors": ["Aupendu Kar", "Vishnu Raj", "Guan-Ming Su"], "title": "Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey", "comment": null, "summary": "Event camera sensors are bio-inspired sensors which asynchronously capture\nper-pixel brightness changes and output a stream of events encoding the\npolarity, location and time of these changes. These systems are witnessing\nrapid advancements as an emerging field, driven by their low latency, reduced\npower consumption, and ultra-high capture rates. This survey explores the\nevolution of fusing event-stream captured with traditional frame-based capture,\nhighlighting how this synergy significantly benefits various video restoration\nand 3D reconstruction tasks. The paper systematically reviews major deep\nlearning contributions to image/video enhancement and restoration, focusing on\ntwo dimensions: temporal enhancement (such as frame interpolation and motion\ndeblurring) and spatial enhancement (including super-resolution, low-light and\nHDR enhancement, and artifact reduction). This paper also explores how the 3D\nreconstruction domain evolves with the advancement of event driven fusion.\nDiverse topics are covered, with in-depth discussions on recent works for\nimproving visual quality under challenging conditions. Additionally, the survey\ncompiles a comprehensive list of openly available datasets, enabling\nreproducible research and benchmarking. By consolidating recent progress and\ninsights, this survey aims to inspire further research into leveraging event\ncamera systems, especially in combination with deep learning, for advanced\nvisual media restoration and enhancement.", "AI": {"tldr": "这篇综述探讨了事件相机与传统帧相机融合的演变，重点关注其在视频恢复和3D重建中的应用，特别是在深度学习背景下，并涵盖了相关数据集。", "motivation": "事件相机具有低延迟、低功耗和超高捕获率的优势。将事件流与传统帧捕获融合，能显著提升各种视频恢复和3D重建任务的性能，因此需要对该新兴领域进行系统性探索。", "method": "本文通过系统回顾深度学习在图像/视频增强和恢复方面的主要贡献，重点关注时间增强（如帧插值、运动去模糊）和空间增强（如超分辨率、低光/HDR增强、伪影消除）。此外，还探讨了事件驱动融合如何促进3D重建领域的发展，并汇编了公开数据集。", "result": "综述强调了事件-帧融合在视频恢复（时间与空间增强）和3D重建中的显著优势，深入讨论了在挑战性条件下改善视觉质量的最新工作，并提供了一个全面的开放数据集列表，以促进可复现研究和基准测试。", "conclusion": "通过整合最新进展和见解，本综述旨在激发未来研究，以利用事件相机系统，特别是结合深度学习，来实现先进的视觉媒体恢复和增强。"}}
{"id": "2509.10454", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10454", "abs": "https://arxiv.org/abs/2509.10454", "authors": ["Hang Yin", "Haoyu Wei", "Xiuwei Xu", "Wenxuan Guo", "Jie Zhou", "Jiwen Lu"], "title": "GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation", "comment": "Accepted to CoRL 2025. Project page: [this https\n  URL](https://bagh2178.github.io/GC-VLN/)", "summary": "In this paper, we propose a training-free framework for vision-and-language\nnavigation (VLN). Existing zero-shot VLN methods are mainly designed for\ndiscrete environments or involve unsupervised training in continuous simulator\nenvironments, which makes it challenging to generalize and deploy them in\nreal-world scenarios. To achieve a training-free framework in continuous\nenvironments, our framework formulates navigation guidance as graph constraint\noptimization by decomposing instructions into explicit spatial constraints. The\nconstraint-driven paradigm decodes spatial semantics through constraint\nsolving, enabling zero-shot adaptation to unseen environments. Specifically, we\nconstruct a spatial constraint library covering all types of spatial\nrelationship mentioned in VLN instructions. The human instruction is decomposed\ninto a directed acyclic graph, with waypoint nodes, object nodes and edges,\nwhich are used as queries to retrieve the library to build the graph\nconstraints. The graph constraint optimization is solved by the constraint\nsolver to determine the positions of waypoints, obtaining the robot's\nnavigation path and final goal. To handle cases of no solution or multiple\nsolutions, we construct a navigation tree and the backtracking mechanism.\nExtensive experiments on standard benchmarks demonstrate significant\nimprovements in success rate and navigation efficiency compared to\nstate-of-the-art zero-shot VLN methods. We further conduct real-world\nexperiments to show that our framework can effectively generalize to new\nenvironments and instruction sets, paving the way for a more robust and\nautonomous navigation framework.", "AI": {"tldr": "本文提出了一种无需训练的视觉-语言导航（VLN）框架，通过将指令分解为空间约束并利用图约束优化，实现在连续环境中对未知环境的零样本适应。", "motivation": "现有零样本VLN方法主要针对离散环境或涉及在连续模拟器中进行无监督训练，这使得它们在真实世界场景中的泛化和部署面临挑战。", "method": "该框架将导航指导表述为图约束优化，通过将指令分解为明确的空间约束。它构建了一个涵盖所有空间关系类型的空间约束库，将人类指令分解为有向无环图（包含路点、对象节点和边），并用作查询来构建图约束。图约束优化通过约束求解器确定路点位置，从而获得机器人导航路径和最终目标。为处理无解或多解情况，引入了导航树和回溯机制。", "result": "在标准基准测试中，与最先进的零样本VLN方法相比，成功率和导航效率显著提高。真实世界实验表明，该框架能有效泛化到新环境和指令集。", "conclusion": "该框架为更稳健和自主的导航框架铺平了道路，尤其适用于连续环境中的零样本VLN，并能有效泛化到真实世界场景。"}}
{"id": "2509.09729", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.09729", "abs": "https://arxiv.org/abs/2509.09729", "authors": ["Gerard Sant", "Zifan Jiang", "Carlos Escolano", "Amit Moryossef", "Mathias Müller", "Rico Sennrich", "Sarah Ebling"], "title": "MultimodalHugs: Enabling Sign Language Processing in Hugging Face", "comment": null, "summary": "In recent years, sign language processing (SLP) has gained importance in the\ngeneral field of Natural Language Processing. However, compared to research on\nspoken languages, SLP research is hindered by complex ad-hoc code,\ninadvertently leading to low reproducibility and unfair comparisons. Existing\ntools that are built for fast and reproducible experimentation, such as Hugging\nFace, are not flexible enough to seamlessly integrate sign language\nexperiments. This view is confirmed by a survey we conducted among SLP\nresearchers.\n  To address these challenges, we introduce MultimodalHugs, a framework built\non top of Hugging Face that enables more diverse data modalities and tasks,\nwhile inheriting the well-known advantages of the Hugging Face ecosystem. Even\nthough sign languages are our primary focus, MultimodalHugs adds a layer of\nabstraction that makes it more widely applicable to other use cases that do not\nfit one of the standard templates of Hugging Face. We provide quantitative\nexperiments to illustrate how MultimodalHugs can accommodate diverse modalities\nsuch as pose estimation data for sign languages, or pixel data for text\ncharacters.", "AI": {"tldr": "MultimodalHugs是一个基于Hugging Face的框架，旨在解决手语处理（SLP）研究中存在的低复现性和不公平比较问题，通过支持更多样化的数据模态和任务，同时保留Hugging Face的优势。", "motivation": "手语处理（SLP）研究受到复杂、临时性代码的阻碍，导致复现性低和比较不公平。现有的工具（如Hugging Face）不够灵活，无法无缝集成手语实验，这一点已通过对SLP研究人员的调查得到证实。", "method": "引入MultimodalHugs框架，它建立在Hugging Face之上，增加了一个抽象层，使其能够支持更多样化的数据模态和任务，同时继承了Hugging Face生态系统的优势。", "result": "MultimodalHugs能够适应多种模态，例如用于手语的姿态估计数据或用于文本字符的像素数据。定量实验证明了其处理多样化模态的能力。", "conclusion": "MultimodalHugs提供了一个灵活且可复现的解决方案，以应对手语处理及其他不符合Hugging Face标准模板的多模态用例所面临的挑战，从而促进这些领域的研究进展。"}}
{"id": "2509.10423", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.10423", "abs": "https://arxiv.org/abs/2509.10423", "authors": ["Cameron Reid", "Wael Hafez", "Amirhossein Nazeri"], "title": "Mutual Information Tracks Policy Coherence in Reinforcement Learning", "comment": "10 pages, 4 figures, 1 table", "summary": "Reinforcement Learning (RL) agents deployed in real-world environments face\ndegradation from sensor faults, actuator wear, and environmental shifts, yet\nlack intrinsic mechanisms to detect and diagnose these failures. We present an\ninformation-theoretic framework that reveals both the fundamental dynamics of\nRL and provides practical methods for diagnosing deployment-time anomalies.\nThrough analysis of state-action mutual information patterns in a robotic\ncontrol task, we first demonstrate that successful learning exhibits\ncharacteristic information signatures: mutual information between states and\nactions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing\nstate entropy, indicating that agents develop increasingly selective attention\nto task-relevant patterns. Intriguingly, states, actions and next states joint\nmutual information, MI(S,A;S'), follows an inverted U-curve, peaking during\nearly learning before declining as the agent specializes suggesting a\ntransition from broad exploration to efficient exploitation. More immediately\nactionable, we show that information metrics can differentially diagnose system\nfailures: observation-space, i.e., states noise (sensor faults) produces broad\ncollapses across all information channels with pronounced drops in state-action\ncoupling, while action-space noise (actuator faults) selectively disrupts\naction-outcome predictability while preserving state-action relationships. This\ndifferential diagnostic capability demonstrated through controlled perturbation\nexperiments enables precise fault localization without architectural\nmodifications or performance degradation. By establishing information patterns\nas both signatures of learning and diagnostic for system health, we provide the\nfoundation for adaptive RL systems capable of autonomous fault detection and\npolicy adjustment based on information-theoretic principles.", "AI": {"tldr": "本文提出一个信息论框架，利用互信息模式来揭示强化学习的动态特征，并为部署时期的异常（如传感器和执行器故障）提供诊断方法，实现了无需修改架构的故障定位。", "motivation": "部署在真实世界环境中的强化学习智能体面临传感器故障、执行器磨损和环境变化等问题，但缺乏内在机制来检测和诊断这些故障。", "method": "本文采用信息论框架，通过分析机器人控制任务中状态-动作互信息（MI(S,A)）和状态、动作、下一状态联合互信息（MI(S,A;S')）的模式。通过受控扰动实验来模拟传感器（观测空间噪声）和执行器（动作空间噪声）故障，以评估信息度量的诊断能力。", "result": "1. 成功的学习表现出特征信息签名：状态-动作互信息（MI(S,A)）从0.84增加到2.83比特（增长238%），表明智能体对任务相关模式的注意力越来越有选择性。2. 状态、动作和下一状态的联合互信息（MI(S,A;S')）呈现倒U形曲线，在学习早期达到峰值，随后下降，表明从广泛探索到高效利用的转变。3. 信息度量可以区分诊断系统故障：观测空间噪声（传感器故障）导致所有信息通道的广泛崩溃，状态-动作耦合显著下降；而动作空间噪声（执行器故障）则选择性地破坏动作-结果的可预测性，同时保持状态-动作关系。这种差异化诊断能力实现了精确的故障定位，无需修改架构或降低性能。", "conclusion": "信息模式既是学习的特征签名，也是系统健康的诊断指标，为基于信息论原则的自适应强化学习系统奠定了基础，使其能够自主检测故障和调整策略。"}}
{"id": "2509.09977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.09977", "abs": "https://arxiv.org/abs/2509.09977", "authors": ["Siying Liu", "Zikai Wang", "Hanle Zheng", "Yifan Hu", "Xilin Wang", "Qingkai Yang", "Jibin Wu", "Hao Guo", "Lei Deng"], "title": "ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking", "comment": "15 pages, 8 figures", "summary": "RGB-Event tracking has become a promising trend in visual object tracking to\nleverage the complementary strengths of both RGB images and dynamic spike\nevents for improved performance. However, existing artificial neural networks\n(ANNs) struggle to fully exploit the sparse and asynchronous nature of event\nstreams. Recent efforts toward hybrid architectures combining ANNs and spiking\nneural networks (SNNs) have emerged as a promising solution in RGB-Event\nperception, yet effectively fusing features across heterogeneous paradigms\nremains a challenge. In this work, we propose ISTASTrack, the first\ntransformer-based \\textbf{A}NN-\\textbf{S}NN hybrid \\textbf{Track}er equipped\nwith \\textbf{ISTA} adapters for RGB-Event tracking. The two-branch model\nemploys a vision transformer to extract spatial context from RGB inputs and a\nspiking transformer to capture spatio-temporal dynamics from event streams. To\nbridge the modality and paradigm gap between ANN and SNN features, we\nsystematically design a model-based ISTA adapter for bidirectional feature\ninteraction between the two branches, derived from sparse representation theory\nby unfolding the iterative shrinkage thresholding algorithm. Additionally, we\nincorporate a temporal downsampling attention module within the adapter to\nalign multi-step SNN features with single-step ANN features in the latent\nspace, improving temporal fusion. Experimental results on RGB-Event tracking\nbenchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that\nISTASTrack achieves state-of-the-art performance while maintaining high energy\nefficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN\ndesigns for robust visual tracking. The code is publicly available at\nhttps://github.com/lsying009/ISTASTrack.git.", "AI": {"tldr": "ISTASTrack是首个基于Transformer的ANN-SNN混合RGB-事件跟踪器，通过ISTA适配器实现RGB图像和事件流之间的有效特征融合，在保持高能效的同时达到了最先进的跟踪性能。", "motivation": "现有的人工神经网络（ANN）难以充分利用事件流的稀疏和异步特性。虽然结合ANN和脉冲神经网络（SNN）的混合架构在RGB-事件感知中前景广阔，但如何有效融合异构范式（ANN和SNN）的特征仍然是一个挑战。", "method": "本文提出了ISTASTrack，一个基于Transformer的ANN-SNN混合跟踪器。它包含一个用于RGB输入的视觉Transformer和一个用于事件流的脉冲Transformer。为了弥合ANN和SNN特征之间的模态和范式鸿沟，设计了一种基于模型（源自迭代收缩阈值算法ISTA）的ISTA适配器，用于双向特征交互。此外，适配器中还整合了一个时间下采样注意力模块，以对齐多步SNN特征和单步ANN特征，从而改善时间融合。", "result": "ISTASTrack在FE240hz、VisEvent、COESOT和FELT等RGB-事件跟踪基准测试中取得了最先进的性能，同时保持了高能效。", "conclusion": "混合ANN-SNN设计对于鲁棒的视觉跟踪是有效且实用的，ISTASTrack的成功证明了其潜力。"}}
{"id": "2509.09731", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09731", "abs": "https://arxiv.org/abs/2509.09731", "authors": ["Haiyang Yu", "Yuchuan Wu", "Fan Shi", "Lei Liao", "Jinghui Lu", "Xiaodong Ge", "Han Wang", "Minghan Zhuo", "Xuecheng Wu", "Xiang Fei", "Hao Feng", "Guozhi Tang", "An-Lan Wang", "Hanshen Zhu", "Yangfan He", "Quanhuan Liang", "Liyuan Meng", "Chao Feng", "Can Huang", "Jingqun Tang", "Bin Li"], "title": "Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning", "comment": null, "summary": "Chinese ancient documents, invaluable carriers of millennia of Chinese\nhistory and culture, hold rich knowledge across diverse fields but face\nchallenges in digitization and understanding, i.e., traditional methods only\nscan images, while current Vision-Language Models (VLMs) struggle with their\nvisual and linguistic complexity. Existing document benchmarks focus on English\nprinted texts or simplified Chinese, leaving a gap for evaluating VLMs on\nancient Chinese documents. To address this, we present AncientDoc, the first\nbenchmark for Chinese ancient documents, designed to assess VLMs from OCR to\nknowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular\ntranslation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and\ncovers 14 document types, over 100 books, and about 3,000 pages. Based on\nAncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by\na human-aligned large language model for scoring.", "AI": {"tldr": "该论文提出了AncientDoc，首个用于评估视觉-语言模型（VLMs）在中国古代文献处理能力上的基准，涵盖从光学字符识别（OCR）到知识推理的多个任务。", "motivation": "中国古代文献蕴含丰富的历史文化知识，但其数字化和理解面临挑战。传统方法仅扫描图像，而现有VLMs难以处理其视觉和语言复杂性。当前文档基准主要关注英文印刷文本或简化中文，缺乏针对古代中文文献的评估工具。", "method": "研究者创建了AncientDoc基准，包含五项任务（页面级OCR、白话文翻译、基于推理的问答、基于知识的问答、语言变体问答），覆盖14种文献类型、100多本书籍和约3000页内容。他们使用多项指标评估了主流VLMs，并辅以与人类对齐的大型语言模型进行评分。", "result": "AncientDoc是首个针对中国古代文献的基准，旨在评估VLMs从OCR到知识推理的能力。它包含多样化的任务和广泛的文献覆盖，并已用于评估主流VLMs。", "conclusion": "AncientDoc基准填补了中国古代文献领域VLM评估的空白，为全面评估VLMs处理复杂古代中文文献的视觉和语言能力提供了一个重要工具。"}}
{"id": "2509.09988", "categories": ["cs.CV", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2509.09988", "abs": "https://arxiv.org/abs/2509.09988", "authors": ["Yusuke Takagi", "Shunya Nagashima", "Komei Sugiura"], "title": "FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction", "comment": "Accepted for presentation at ICONIP2025", "summary": "Accurate and reliable solar flare predictions are essential to mitigate\npotential impacts on critical infrastructure. However, the current performance\nof solar flare forecasting is insufficient. In this study, we address the task\nof predicting the class of the largest solar flare expected to occur within the\nnext 72 hours. Existing methods often fail to adequately address the severe\nclass imbalance across flare classes. To address this issue, we propose a solar\nflare prediction model based on multiple deep state space models. In addition,\nwe introduce the frequency & local-boundary-aware reliability loss (FLARE loss)\nto improve predictive performance and reliability under class imbalance.\nExperiments were conducted on a multi-wavelength solar image dataset covering a\nfull 11-year solar activity cycle. As a result, our method outperformed\nbaseline approaches in terms of both the Gandin-Murphy-Gerrity score and the\ntrue skill statistic, which are standard metrics in terms of the performance\nand reliability.", "AI": {"tldr": "该研究提出了一种基于多个深度状态空间模型的太阳耀斑预测模型，并引入了FLARE损失函数，以解决现有方法在预测未来72小时最大耀斑类别时面临的类别不平衡问题，并在标准指标上优于基线方法。", "motivation": "准确可靠的太阳耀斑预测对于减轻对关键基础设施的潜在影响至关重要。然而，目前的太阳耀斑预测性能不足，并且现有方法未能充分解决耀斑类别之间严重的类别不平衡问题。", "method": "提出了一种基于多个深度状态空间模型的太阳耀斑预测模型。此外，引入了频率与局部边界感知可靠性损失（FLARE损失）以提高类别不平衡条件下的预测性能和可靠性。", "result": "在涵盖一个完整11年太阳活动周期的多波长太阳图像数据集上进行实验，结果表明该方法在Gandin-Murphy-Gerrity分数和真实技能统计量（性能和可靠性的标准指标）方面均优于基线方法。", "conclusion": "所提出的基于深度状态空间模型和FLARE损失的太阳耀斑预测模型，有效解决了类别不平衡问题，显著提升了预测性能和可靠性，优于现有基线方法。"}}
{"id": "2509.09734", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.09734", "abs": "https://arxiv.org/abs/2509.09734", "authors": ["Zikang Guo", "Benfeng Xu", "Chiwei Zhu", "Wentao Hong", "Xiaorui Wang", "Zhendong Mao"], "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools", "comment": null, "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems.", "AI": {"tldr": "本文介绍了MCP-AgentBench，一个专门用于评估在Model Context Protocol (MCP) 标准下语言代理工具交互能力的综合基准，旨在解决现有基准在评估MCP环境下的代理性能方面的不足。", "motivation": "尽管Model Context Protocol (MCP) 作为代理-工具集成和互操作性的开放标准正在迅速普及，但现有基准未能准确捕捉MCP范式下代理的真实世界性能，导致对其操作价值的感知失真，并难以可靠地区分代理能力。", "method": "本文提出了MCP-AgentBench，其核心贡献包括：1) 建立了包含33个操作服务器和188种不同工具的强大MCP测试平台；2) 开发了一个包含600个系统设计查询的基准，这些查询分布在6个不同交互复杂度的类别中；3) 引入了MCP-Eval，一种以结果为导向的新型评估方法，优先考虑真实世界的任务成功。", "result": "通过对领先语言代理进行广泛的实证评估，MCP-AgentBench提供了基础性见解。", "conclusion": "MCP-AgentBench旨在为研究社区提供一个标准化且可靠的框架，以构建、验证和推进能够充分利用MCP变革性优势的代理，从而加速实现真正有能力和可互操作的AI系统。"}}
{"id": "2509.09801", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T07, 68T50, 68T05", "I.2.7; I.2.6; C.4"], "pdf": "https://arxiv.org/pdf/2509.09801", "abs": "https://arxiv.org/abs/2509.09801", "authors": ["Brennen Hill"], "title": "HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning", "comment": null, "summary": "The adaptation of large language models (LLMs) to specialized reasoning tasks\nis fundamentally constrained by computational resources. Parameter-Efficient\nFine-Tuning (PEFT) methods have emerged as a powerful solution, yet the\nlandscape of these techniques is diverse, with distinct methods operating in\neither the model's weight space or its representation space. This paper\ninvestigates the hypothesis that a synergistic combination of these paradigms\ncan unlock superior performance and efficiency. We introduce HEFT (Hierarchical\nEfficient Fine-Tuning), a novel hierarchical adaptation strategy that composes\ntwo distinct PEFT methods in a coarse-to-fine manner: first, a broad,\nfoundational adaptation in the weight space using Low-Rank Adaptation (LoRA),\nfollowed by a precise, surgical refinement of internal activations using\nRepresentation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a\nLlama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential\nreasoning. Our results reveal a profound synergistic effect. A model fine-tuned\nfor only three epochs with our HEFT strategy achieves an accuracy of 85.17\\%,\nexceeding the performance of models trained for 20 epochs with either LoRA-only\n(85.05\\%) or ReFT-only (83.36\\%) methodologies. This work demonstrates that the\nthoughtful composition of PEFT methods is a potent algorithmic innovation,\noffering a more efficient and effective path toward advancing the reasoning\ncapabilities of language models. By achieving superior results with a fraction\nof the computational budget, our findings present a principled approach to\novercoming the obstacles inherent in adapting large-scale models for complex\ncognitive tasks.", "AI": {"tldr": "本文提出HEFT，一种结合权重空间（LoRA）和表示空间（ReFT）PEFT方法的层次化策略，在推理任务上以更少计算资源实现更优性能。", "motivation": "大型语言模型（LLMs）在专业推理任务上的适应性受计算资源限制。现有的参数高效微调（PEFT）方法多样，分别作用于模型的权重空间或表示空间。本文旨在探索这些范式协同组合能否带来更卓越的性能和效率。", "method": "本文引入HEFT（Hierarchical Efficient Fine-Tuning），一种新颖的层次化适应策略。它以从粗到细的方式组合两种不同的PEFT方法：首先，使用低秩适应（LoRA）在权重空间进行广泛的基础适应；随后，使用表示微调（ReFT）对内部激活进行精确的微调。该方法在Llama-2-7B模型上，针对BoolQ推理基准进行评估。", "result": "实验结果显示出显著的协同效应。使用HEFT策略仅微调三个epoch的模型达到了85.17%的准确率，超过了单独使用LoRA（85.05%）或ReFT（83.36%）方法训练20个epoch的模型性能。", "conclusion": "研究表明，深思熟虑地组合PEFT方法是一种强大的算法创新，为提升语言模型推理能力提供了一条更高效、更有效的途径。通过以更少的计算预算实现卓越结果，HEFT提供了一种克服大规模模型适应复杂认知任务固有障碍的原则性方法。"}}
{"id": "2509.10005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10005", "abs": "https://arxiv.org/abs/2509.10005", "authors": ["Xiaodong Guo", "Tong Liu", "Yike Li", "Zi'ang Lin", "Zhihong Deng"], "title": "TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion", "comment": null, "summary": "RGB-thermal (RGB-T) semantic segmentation improves the environmental\nperception of autonomous platforms in challenging conditions. Prevailing models\nemploy encoders pre-trained on RGB images to extract features from both RGB and\ninfrared inputs, and design additional modules to achieve cross-modal feature\nfusion. This results in limited thermal feature extraction and suboptimal\ncross-modal fusion, while the redundant encoders further compromises the\nmodel's real-time efficiency. To address the above issues, we propose TUNI,\nwith an RGB-T encoder consisting of multiple stacked blocks that simultaneously\nperform multi-modal feature extraction and cross-modal fusion. By leveraging\nlarge-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder\nlearns to integrate feature extraction and fusion in a unified manner. By\nslimming down the thermal branch, the encoder achieves a more compact\narchitecture. Moreover, we introduce an RGB-T local module to strengthen the\nencoder's capacity for cross-modal local feature fusion. The RGB-T local module\nemploys adaptive cosine similarity to selectively emphasize salient consistent\nand distinct local features across RGB-T modalities. Experimental results show\nthat TUNI achieves competitive performance with state-of-the-art models on FMB,\nPST900 and CART, with fewer parameters and lower computational cost. Meanwhile,\nit achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its\nreal-time capability in deployment. Codes are available at\nhttps://github.com/xiaodonguo/TUNI.", "AI": {"tldr": "本文提出TUNI模型，通过统一的RGB-T编码器和RGB-T局部模块，解决了RGB-T语义分割中热特征提取受限和跨模态融合不佳的问题。TUNI实现了高效的特征提取和融合，参数更少，计算成本更低，并在边缘设备上展现出实时推理能力。", "motivation": "现有的RGB-T语义分割模型普遍存在以下问题：1) 依赖RGB预训练编码器处理两种模态，导致热特征提取能力有限；2) 跨模态特征融合效果不理想；3) 冗余编码器降低了模型的实时效率。", "method": "本文提出了TUNI模型，主要包括：1) 一个RGB-T编码器，由多个堆叠块组成，能够同时进行多模态特征提取和跨模态融合。该编码器通过RGB和伪热数据进行大规模预训练，实现特征提取和融合的统一学习，并通过精简热分支来获得更紧凑的架构。2) 一个RGB-T局部模块，用于增强编码器在跨模态局部特征融合方面的能力。该模块采用自适应余弦相似度，选择性地强调RGB-T模态间显著的一致和不同局部特征。", "result": "实验结果表明，TUNI在FMB、PST900和CART数据集上取得了与最先进模型相当的性能，同时具有更少的参数和更低的计算成本。此外，TUNI在Jetson Orin NX上实现了27 FPS的推理速度，展示了其在部署中的实时能力。", "conclusion": "TUNI通过其统一的RGB-T编码器和RGB-T局部模块，有效解决了RGB-T语义分割中热特征提取和跨模态融合的局限性，并提高了模型的实时效率。它在性能、效率和部署实时性方面均表现出色，为挑战性环境下的自主平台环境感知提供了有效方案。"}}
{"id": "2509.09735", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09735", "abs": "https://arxiv.org/abs/2509.09735", "authors": ["Willem Huijzer", "Jieying Chen"], "title": "Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation", "comment": "7 pages", "summary": "The rapid integration of Large Language Models (LLMs) into various domains\nraises concerns about societal inequalities and information bias. This study\nexamines biases in LLMs related to background, gender, and age, with a focus on\ntheir impact on decision-making and summarization tasks. Additionally, the\nresearch examines the cross-lingual propagation of these biases and evaluates\nthe effectiveness of prompt-instructed mitigation strategies. Using an adapted\nversion of the dataset by Tamkin et al. (2023) translated into Dutch, we\ncreated 151,200 unique prompts for the decision task and 176,400 for the\nsummarisation task. Various demographic variables, instructions, salience\nlevels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed\nthat both models were significantly biased during decision-making, favouring\nfemale gender, younger ages, and certain backgrounds such as the\nAfrican-American background. In contrast, the summarisation task showed minimal\nevidence of bias, though significant age-related differences emerged for\nGPT-3.5 in English. Cross-lingual analysis showed that bias patterns were\nbroadly similar between English and Dutch, though notable differences were\nobserved across specific demographic categories. The newly proposed mitigation\ninstructions, while unable to eliminate biases completely, demonstrated\npotential in reducing them. The most effective instruction achieved a 27\\% mean\nreduction in the gap between the most and least favorable demographics.\nNotably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts\nin English, indicating the specific potential for prompt-based mitigation\nwithin newer models. This research underscores the importance of cautious\nadoption of LLMs and context-specific bias testing, highlighting the need for\ncontinued development of effective mitigation strategies to ensure responsible\ndeployment of AI.", "AI": {"tldr": "本研究调查了大型语言模型（LLMs）在决策和摘要任务中存在的背景、性别和年龄偏见，以及这些偏见的跨语言传播，并评估了提示指令缓解策略的有效性。", "motivation": "LLMs的快速整合引发了对社会不平等和信息偏见的担忧，促使研究人员探讨这些模型中存在的偏见及其影响。", "method": "研究人员使用了Tamkin等人（2023）数据集的改编版本，并将其翻译成荷兰语，创建了151,200个决策任务提示和176,400个摘要任务提示。他们通过测试不同的人口统计变量、指令、显著性水平和语言，对GPT-3.5和GPT-4o模型进行了评估。", "result": "分析显示，在决策任务中，两个模型都存在显著偏见，倾向于女性、年轻年龄和某些背景（如非洲裔美国人）。摘要任务中偏见较少，但GPT-3.5在英语中显示出显著的年龄相关差异。跨语言分析表明，英语和荷兰语的偏见模式大致相似，但在特定人口类别中存在显著差异。新提出的缓解指令虽然未能完全消除偏见，但显示出减少偏见的潜力，最有效的指令使最有利和最不利人口群体之间的差距平均减少了27%。值得注意的是，与GPT-3.5相反，GPT-4o在所有英语提示中都显示出偏见减少，表明较新模型中基于提示的缓解具有特定潜力。", "conclusion": "本研究强调了谨慎采用LLMs和进行特定情境偏见测试的重要性，并指出需要持续开发有效的缓解策略，以确保AI的负责任部署。"}}
{"id": "2509.09871", "categories": ["cs.CL", "cs.AI", "68T50 (Primary) 91F10 (Secondary)"], "pdf": "https://arxiv.org/pdf/2509.09871", "abs": "https://arxiv.org/abs/2509.09871", "authors": ["Bastián González-Bustamante", "Nando Verelst", "Carla Cisternas"], "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case", "comment": "Working paper: 18 pages, 4 tables, 2 figures", "summary": "Large Language Models (LLMs) offer promising avenues for methodological and\napplied innovations in survey research by using synthetic respondents to\nemulate human answers and behaviour, potentially mitigating measurement and\nrepresentation errors. However, the extent to which LLMs recover aggregate item\ndistributions remains uncertain and downstream applications risk reproducing\nsocial stereotypes and biases inherited from training data. We evaluate the\nreliability of LLM-generated synthetic survey responses against ground-truth\nhuman responses from a Chilean public opinion probabilistic survey.\nSpecifically, we benchmark 128 prompt-model-question triplets, generating\n189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,\nprecision, recall, and F1-score) in a meta-analysis across 128\nquestion-subsample pairs to test for biases along key sociodemographic\ndimensions. The evaluation spans OpenAI's GPT family and o-series reasoning\nmodels, as well as Llama and Qwen checkpoints. Three results stand out. First,\nsynthetic responses achieve excellent performance on trust items (F1-score and\naccuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform\ncomparably on this task. Third, synthetic-human alignment is highest among\nrespondents aged 45-59. Overall, LLM-based synthetic samples approximate\nresponses from a probabilistic sample, though with substantial item-level\nheterogeneity. Capturing the full nuance of public opinion remains challenging\nand requires careful calibration and additional distributional tests to ensure\nalgorithmic fidelity and reduce errors.", "AI": {"tldr": "该研究评估了大型语言模型（LLM）在生成合成调查问卷回复方面的可靠性，发现它们能近似真实概率样本，但在捕捉公众意见细微差别方面仍面临挑战，且存在项目层面的异质性。", "motivation": "LLM在调查研究中通过合成受访者模拟人类回答和行为，有望减少测量和代表性误差。然而，LLM恢复聚合项目分布的程度以及下游应用重现社会刻板印象和偏见的风险尚不明确，这促使研究者对LLM生成回复的可靠性进行评估。", "method": "研究人员将LLM生成的合成调查回复与智利公众意见概率调查的真实人类回复进行对比评估。具体方法包括：基准测试128个提示-模型-问题三联体，生成189,696个合成配置文件；通过元分析汇总128个问题-子样本对的性能指标（准确率、精确率、召回率和F1分数），以检测关键社会人口维度上的偏差；评估了OpenAI的GPT系列、o系列推理模型以及Llama和Qwen模型。", "result": "研究得出三个主要结果：1. 合成回复在信任项目上表现出色（F1分数和准确率 > 0.90）。2. GPT-4o、GPT-4o-mini和Llama 4 Maverick在该任务上表现相当。3. 合成回复与人类回复的一致性在45-59岁受访者中最高。总体而言，基于LLM的合成样本能近似概率样本的回复，但存在显著的项目层面异质性。", "conclusion": "LLM生成的合成样本可以近似概率样本的调查回复，但要捕捉公众意见的全部细微差别仍然具有挑战性。这需要仔细的校准和额外的分布测试，以确保算法的忠实度和减少误差。"}}
{"id": "2509.10006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10006", "abs": "https://arxiv.org/abs/2509.10006", "authors": ["Masaki Akiba", "Shumpei Takezaki", "Daichi Haraguchi", "Seiichi Uchida"], "title": "Few-Part-Shot Font Generation", "comment": "ICDAR 2025 Workshop on Machine Learning", "summary": "This paper proposes a novel model of few-part-shot font generation, which\ndesigns an entire font based on a set of partial design elements, i.e., partial\nshapes. Unlike conventional few-shot font generation, which requires entire\ncharacter shapes for a couple of character classes, our approach only needs\npartial shapes as input. The proposed model not only improves the efficiency of\nfont creation but also provides insights into how partial design details\ninfluence the entire structure of the individual characters.", "AI": {"tldr": "本文提出了一种新颖的“少部分样本”字体生成模型，仅使用部分设计元素（即部分形状）来生成完整字体。", "motivation": "传统的少样本字体生成需要完整的字符形状作为输入，而本文旨在通过仅使用部分形状来提高字体创建效率，并深入理解部分设计细节如何影响整体字符结构。", "method": "本文提出了一种新颖的“少部分样本字体生成”模型，该模型以部分形状作为输入，进而设计出完整的字体。", "result": "该模型不仅提高了字体创建的效率，而且揭示了部分设计细节如何影响单个字符的整体结构。", "conclusion": "所提出的模型通过仅使用部分形状输入，实现了高效的字体生成，并为理解部分设计元素对整体字符结构的影响提供了新的视角。"}}
{"id": "2509.09804", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09804", "abs": "https://arxiv.org/abs/2509.09804", "authors": ["Helen de Andrade Abreu", "Tiago Timponi Torrent", "Ely Edison da Silva Matos"], "title": "Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization", "comment": "Paper submitted to Language Sciences Journal", "summary": "This paper proposes a framework for modeling multimodal conversational turn\norganization via the proposition of correlations between language and\ninteractive gestures, based on analysis as to how pragmatic frames are\nconceptualized and evoked by communicators. As a means to provide evidence for\nthe analysis, we developed an annotation methodology to enrich a multimodal\ndataset (annotated for semantic frames) with pragmatic frames modeling\nconversational turn organization. Although conversational turn organization has\nbeen studied by researchers from diverse fields, the specific strategies,\nespecially gestures used by communicators, had not yet been encoded in a\ndataset that can be used for machine learning. To fill this gap, we enriched\nthe Frame2 dataset with annotations of gestures used for turn organization. The\nFrame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo\nMundo annotated for semantic frames evoked in both video and text. This dataset\nallowed us to closely observe how communicators use interactive gestures\noutside a laboratory, in settings, to our knowledge, not previously recorded in\nrelated literature. Our results have confirmed that communicators involved in\nface-to-face conversation make use of gestures as a tool for passing, taking\nand keeping conversational turns, and also revealed variations of some gestures\nthat had not been documented before. We propose that the use of these gestures\narises from the conceptualization of pragmatic frames, involving mental spaces,\nblending and conceptual metaphors. In addition, our data demonstrate that the\nannotation of pragmatic frames contributes to a deeper understanding of human\ncognition and language.", "AI": {"tldr": "本文提出一个多模态对话轮次组织建模框架，通过语言与交互手势的相关性，基于语用框架的构思与激活。研究团队开发了手势标注方法，丰富了现有数据集，并证实了手势在对话轮次管理中的作用，揭示了未曾记录的手势变体。", "motivation": "尽管对话轮次组织已被广泛研究，但缺乏包含手势策略且适用于机器学习的数据集。研究旨在填补这一空白，深入理解交流者如何构思和激活语用框架。", "method": "本文提出了一个框架，通过语言和交互手势之间的关联来建模多模态对话轮次组织，其基础是对语用框架如何被概念化和激活的分析。研究团队开发了一种标注方法，用语用框架（建模对话轮次组织）和手势丰富了Frame2多模态数据集（包含巴西电视剧集，已标注语义框架）。", "result": "研究结果证实，面对面交流中的沟通者确实利用手势作为传递、获取和保持对话轮次的工具，并揭示了一些此前未被记录的手势变体。数据还表明，语用框架的标注有助于更深入地理解人类认知和语言。", "conclusion": "手势在对话轮次组织中扮演着关键角色，其使用源于语用框架（涉及心理空间、融合和概念隐喻）的概念化。新提出的框架、标注方法和丰富的数据集为机器学习提供了资源，并加深了对人类认知和语言的理解。"}}
{"id": "2509.09969", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.09969", "abs": "https://arxiv.org/abs/2509.09969", "authors": ["Zhitian Hou", "Zihan Ye", "Nanli Zeng", "Tianyong Hao", "Kun Zeng"], "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey", "comment": null, "summary": "Large Language Models (LLMs) have significantly advanced the development of\nLegal Artificial Intelligence (Legal AI) in recent years, enhancing the\nefficiency and accuracy of legal tasks. To advance research and applications of\nLLM-based approaches in legal domain, this paper provides a comprehensive\nreview of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and\nalso gather 15 benchmarks and 29 datasets to evaluate different legal\ncapabilities. Additionally, we analyse the challenges and discuss future\ndirections for LLM-based approaches in the legal domain. We hope this paper\nprovides a systematic introduction for beginners and encourages future research\nin this field. Resources are available at\nhttps://github.com/ZhitianHou/LLMs4LegalAI.", "AI": {"tldr": "本文对法律人工智能领域中的大语言模型（LLM）进行了全面综述，涵盖了模型、框架、基准和数据集，并探讨了面临的挑战与未来方向。", "motivation": "大语言模型显著推动了法律人工智能的发展，提高了法律任务的效率和准确性。本文旨在推进基于LLM的法律领域研究和应用。", "method": "本文综述了16个法律LLM系列和47个用于法律任务的LLM框架，并收集了15个基准和29个数据集以评估不同的法律能力。此外，还分析了基于LLM方法在法律领域面临的挑战并讨论了未来方向。", "result": "本文提供了对法律LLM系列、LLM法律任务框架、评估基准和数据集的全面回顾，并深入分析了当前挑战和未来发展方向。", "conclusion": "本论文为初学者提供了系统的介绍，并旨在鼓励该领域未来的研究。"}}
{"id": "2509.10024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10024", "abs": "https://arxiv.org/abs/2509.10024", "authors": ["Danling Cao"], "title": "Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images", "comment": "This work was completed during the author's MPhil studies at the\n  University of Manchester", "summary": "Recovering 3D face models from 2D in-the-wild images has gained considerable\nattention in the computer vision community due to its wide range of potential\napplications. However, the lack of ground-truth labeled datasets and the\ncomplexity of real-world environments remain significant challenges. In this\nchapter, we propose a convolutional neural network-based approach, the\nHierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face\nmodels from single in-the-wild images. Our model predicts detailed facial\ngeometry, texture, pose, and illumination parameters from a single image.\nSpecifically, we employ a pre-trained hierarchical backbone network and\nintroduce multi-level attention mechanisms at different stages of 2D face image\nfeature extraction. A semi-supervised training strategy is employed,\nincorporating 3D Morphable Model (3DMM) parameters from publicly available\ndatasets along with a differentiable renderer, enabling an end-to-end training\nprocess. Extensive experiments, including both comparative and ablation\nstudies, were conducted on two benchmark datasets, AFLW2000-3D and MICC\nFlorence, focusing on 3D face reconstruction and 3D face alignment tasks. The\neffectiveness of the proposed method was evaluated both quantitatively and\nqualitatively.", "AI": {"tldr": "本文提出了一种名为MLANet的卷积神经网络方法，用于从单张野外2D图像中重建详细的3D人脸模型，通过多级注意力机制和半监督训练策略实现了端到端训练，并在基准数据集上取得了有效的结果。", "motivation": "从野外2D图像中恢复3D人脸模型在计算机视觉领域受到广泛关注，但缺乏地面真实标注数据集以及真实世界环境的复杂性是主要挑战。", "method": "本文提出了一种基于卷积神经网络的层次多级注意力网络（MLANet）。该模型利用预训练的层次骨干网络，并在2D人脸图像特征提取的不同阶段引入多级注意力机制。采用半监督训练策略，结合公开数据集中的3D可变形模型（3DMM）参数和可微分渲染器，实现了端到端训练。模型能够预测人脸几何、纹理、姿态和光照参数。", "result": "在AFLW2000-3D和MICC Florence两个基准数据集上进行了广泛的比较和消融实验，专注于3D人脸重建和3D人脸对齐任务。定量和定性评估均表明所提出的方法是有效的。", "conclusion": "所提出的MLANet方法能够有效克服现有挑战，从单张野外2D图像中重建详细的3D人脸模型，并在多个任务和基准数据集上表现出优异的性能。"}}
{"id": "2509.09852", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09852", "abs": "https://arxiv.org/abs/2509.09852", "authors": ["Chuyuan Li", "Austin Xu", "Shafiq Joty", "Giuseppe Carenini"], "title": "Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization", "comment": null, "summary": "A key challenge in Multi-Document Summarization (MDS) is effectively\nintegrating information from multiple sources while maintaining coherence and\ntopical relevance. While Large Language Models have shown impressive results in\nsingle-document summarization, their performance on MDS still leaves room for\nimprovement. In this paper, we propose a topic-guided reinforcement learning\napproach to improve content selection in MDS. We first show that explicitly\nprompting models with topic labels enhances the informativeness of the\ngenerated summaries. Building on this insight, we propose a novel topic reward\nwithin the Group Relative Policy Optimization (GRPO) framework to measure topic\nalignment between the generated summary and source documents. Experimental\nresults on the Multi-News and Multi-XScience datasets demonstrate that our\nmethod consistently outperforms strong baselines, highlighting the\neffectiveness of leveraging topical cues in MDS.", "AI": {"tldr": "本文提出一种主题引导的强化学习方法，通过引入主题奖励和显式主题提示，改进多文档摘要（MDS）中的内容选择。", "motivation": "多文档摘要（MDS）面临有效整合多源信息并保持连贯性和主题相关性的挑战。尽管大型语言模型在单文档摘要中表现出色，但在MDS上的性能仍有提升空间。", "method": "首先，研究表明显式地用主题标签提示模型可以提高生成摘要的信息量。在此基础上，本文在Group Relative Policy Optimization (GRPO)框架内提出了一种新颖的主题奖励，用于衡量生成摘要与源文档之间的主题一致性。", "result": "在Multi-News和Multi-XScience数据集上的实验结果表明，该方法持续优于强大的基线模型。", "conclusion": "研究强调了在MDS中利用主题线索的有效性，证明了其在内容选择方面的改进。"}}
{"id": "2509.10004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10004", "abs": "https://arxiv.org/abs/2509.10004", "authors": ["Ponhvoan Srey", "Xiaobao Wu", "Anh Tuan Luu"], "title": "Unsupervised Hallucination Detection by Inspecting Reasoning Processes", "comment": "To appear in EMNLP 2025", "summary": "Unsupervised hallucination detection aims to identify hallucinated content\ngenerated by large language models (LLMs) without relying on labeled data.\nWhile unsupervised methods have gained popularity by eliminating\nlabor-intensive human annotations, they frequently rely on proxy signals\nunrelated to factual correctness. This misalignment biases detection probes\ntoward superficial or non-truth-related aspects, limiting generalizability\nacross datasets and scenarios. To overcome these limitations, we propose IRIS,\nan unsupervised hallucination detection framework, leveraging internal\nrepresentations intrinsic to factual correctness. IRIS prompts the LLM to\ncarefully verify the truthfulness of a given statement, and obtain its\ncontextualized embedding as informative features for training. Meanwhile, the\nuncertainty of each response is considered a soft pseudolabel for truthfulness.\nExperimental results demonstrate that IRIS consistently outperforms existing\nunsupervised methods. Our approach is fully unsupervised, computationally low\ncost, and works well even with few training data, making it suitable for\nreal-time detection.", "AI": {"tldr": "IRIS是一种无监督幻觉检测框架，它利用大型语言模型（LLM）的内部表征和响应不确定性来识别幻觉内容，克服了现有无监督方法依赖非事实性代理信号的局限性。", "motivation": "现有无监督幻觉检测方法依赖与事实准确性无关的代理信号，导致检测偏差和泛化能力受限；而有监督方法需要大量人工标注，成本高昂。", "method": "IRIS框架通过提示LLM仔细验证给定陈述的真实性，并获取其语境化嵌入作为信息特征进行训练。同时，将每个响应的不确定性视为真实性的软伪标签。", "result": "实验结果表明，IRIS持续优于现有的无监督方法。", "conclusion": "IRIS方法是完全无监督的，计算成本低，即使在少量训练数据下也能表现良好，使其适用于实时检测。"}}
{"id": "2509.10026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10026", "abs": "https://arxiv.org/abs/2509.10026", "authors": ["Jing Huang", "Zhiya Tan", "Shutao Gong", "Fanwei Zeng", "Jianshu Li"], "title": "LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA", "comment": "12 Pages, 12 Figures, 2 Tables", "summary": "As large vision language models (VLMs) advance, their capabilities in\nmultilingual visual question answering (mVQA) have significantly improved.\nChain-of-thought (CoT) reasoning has been proven to enhance interpretability\nand complex reasoning. However, most existing approaches rely primarily on\ntextual CoT and provide limited support for multilingual multimodal reasoning,\nconstraining their deployment in real-world applications. To address this gap,\nwe introduce \\textbf{LaV-CoT}, the first Language-aware Visual CoT framework\nwith Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable\nmulti-stage reasoning pipeline consisting of Text Summary with Bounding Box\n(BBox), Language Identification, Spatial Object-level Captioning, and\nStep-by-step Logical Reasoning. Following this reasoning pipeline, we design an\nautomated data curation method that generates multilingual CoT annotations\nthrough iterative generation, correction, and refinement, enabling scalable and\nhigh-quality training data. To improve reasoning and generalization, LaV-CoT\nadopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)\nwith Language-aware Group Relative Policy Optimization (GRPO), guided by\nverifiable multi-aspect rewards including language consistency, structural\naccuracy, and semantic alignment. Extensive evaluations on public datasets\nincluding MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up\nto \\(\\sim\\)9.5\\% accuracy improvements over open-source baselines of similar\nsize and even surpasses models with 2$\\times$ larger scales by \\(\\sim\\)2.6\\%.\nMoreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513\nand Gemini-2.5-flash. We further conducted an online A/B test to validate our\nmethod on real-world data, highlighting its effectiveness for industrial\ndeployment. Our code is available at this link:\n\\href{https://github.com/HJNVR/LaV-CoT}", "AI": {"tldr": "LaV-CoT是首个语言感知视觉思维链（CoT）框架，结合多方面奖励优化，显著提升了多语言多模态视觉问答（mVQA）的性能，超越了现有开源及专有模型。", "motivation": "尽管大型视觉语言模型（VLMs）在mVQA方面有所进步，但现有CoT方法主要依赖文本CoT，对多语言多模态推理的支持有限，阻碍了其在实际应用中的部署。", "method": "LaV-CoT采用可解释的多阶段推理流程（包括带边界框的文本摘要、语言识别、空间对象级字幕和逐步逻辑推理）。设计了通过迭代生成、修正和细化来自动生成多语言CoT注释的数据 Curating 方法。训练范式结合了监督微调（SFT）和语言感知组相对策略优化（GRPO），并由语言一致性、结构准确性和语义对齐等可验证的多方面奖励指导。", "result": "在MMMB、Multilingual MMBench和MTVQA等公开数据集上，LaV-CoT比同等规模的开源基线模型准确率提升高达9.5%，甚至超过2倍规模的模型约2.6%。同时，其性能优于GPT-4o-0513和Gemini-2.5-flash等先进专有模型。在线A/B测试进一步验证了其在实际数据上的有效性。", "conclusion": "LaV-CoT成功解决了现有mVQA模型的局限性，通过语言感知的视觉CoT和多方面奖励优化，实现了卓越的性能和泛化能力，对工业部署具有重要意义。"}}
{"id": "2509.09990", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.09990", "abs": "https://arxiv.org/abs/2509.09990", "authors": ["Guixian Xu", "Zeli Su", "Ziyin Zhang", "Jianing Liu", "XU Han", "Ting Zhang", "Yushuang Dong"], "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China", "comment": null, "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks.", "AI": {"tldr": "该研究创建了一个针对藏语、维吾尔语和蒙语的少数民族语言标题生成数据集（CMHG），并提供了一个高质量的测试集，以解决这些语言语料库稀缺的问题。", "motivation": "中国少数民族语言（如藏语、维吾尔语、蒙语）的书写系统独特，与国际标准不同，导致在标题生成等监督任务中缺乏相关语料库。", "method": "引入了一个名为“中国少数民族标题生成（CMHG）”的新数据集，包含10万条藏语条目和各5万条维吾尔语、蒙语条目。此外，还提出了一个由母语使用者标注的高质量测试集，旨在作为该领域未来研究的基准。", "result": "成功构建了CMHG数据集，为藏语、维吾尔语和蒙语的标题生成任务提供了大量语料，并创建了一个由母语使用者标注的基准测试集。", "conclusion": "该数据集有望成为推动中国少数民族语言标题生成研究和相关基准发展的重要资源。"}}
{"id": "2509.10059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10059", "abs": "https://arxiv.org/abs/2509.10059", "authors": ["Yue Zhou", "Litong Feng", "Mengcheng Lan", "Xue Yang", "Qingyun Li", "Yiping Ke", "Xue Jiang", "Wayne Zhang"], "title": "Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration", "comment": "17 pages, 16 figures", "summary": "Mathematical reasoning is critical for tasks such as precise distance and\narea computations, trajectory estimations, and spatial analysis in unmanned\naerial vehicle (UAV) based remote sensing, yet current vision-language models\n(VLMs) have not been adequately tested in this domain. To address this gap, we\nintroduce AVI-Math, the first benchmark to rigorously evaluate multimodal\nmathematical reasoning in aerial vehicle imagery, moving beyond simple counting\ntasks to include domain-specific knowledge in areas such as geometry, logic,\nand algebra. The dataset comprises 3,773 high-quality vehicle-related questions\ncaptured from UAV views, covering 6 mathematical subjects and 20 topics. The\ndata, collected at varying altitudes and from multiple UAV angles, reflects\nreal-world UAV scenarios, ensuring the diversity and complexity of the\nconstructed mathematical problems. In this paper, we benchmark 14 prominent\nVLMs through a comprehensive evaluation and demonstrate that, despite their\nsuccess on previous multimodal benchmarks, these models struggle with the\nreasoning tasks in AVI-Math. Our detailed analysis highlights significant\nlimitations in the mathematical reasoning capabilities of current VLMs and\nsuggests avenues for future research. Furthermore, we explore the use of\nChain-of-Thought prompting and fine-tuning techniques, which show promise in\naddressing the reasoning challenges in AVI-Math. Our findings not only expose\nthe limitations of VLMs in mathematical reasoning but also offer valuable\ninsights for advancing UAV-based trustworthy VLMs in real-world applications.\nThe code, and datasets will be released at\nhttps://github.com/VisionXLab/avi-math", "AI": {"tldr": "本文引入了AVI-Math，首个用于评估无人机图像中多模态数学推理能力的基准数据集，并发现现有视觉-语言模型（VLMs）在此类任务上表现不佳，但链式思考提示和微调技术显示出潜力。", "motivation": "数学推理对于无人机遥感中的精确距离、面积计算、轨迹估计和空间分析等任务至关重要。然而，当前的视觉-语言模型（VLMs）尚未在该领域得到充分测试，且现有基准未能涵盖几何、逻辑和代数等领域特定的数学推理任务。", "method": "研究者构建了AVI-Math数据集，包含3,773个从无人机视角捕获的高质量车辆相关问题，涵盖6个数学科目和20个主题，数据采集自不同高度和多个无人机角度，以反映真实世界场景的复杂性。他们对14个主流VLM进行了全面评估，并探索了链式思考（Chain-of-Thought）提示和微调技术在解决推理挑战中的应用。", "result": "尽管在之前的多模态基准测试中取得了成功，但现有VLM在AVI-Math的推理任务中表现不佳，表明其数学推理能力存在显著局限性。然而，链式思考提示和微调技术在解决AVI-Math中的推理挑战方面显示出前景。", "conclusion": "研究不仅揭示了当前VLM在数学推理方面的局限性，而且为未来研究提供了方向，并为推进基于无人机的可信VLM在实际应用中的发展提供了宝贵见解。"}}
{"id": "2509.10058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10058", "abs": "https://arxiv.org/abs/2509.10058", "authors": ["Sung-Lin Tsai", "Bo-Lun Huang", "Yu Ting Shen", "Cheng Yu Yeo", "Chiang Tseng", "Bo-Kai Ruan", "Wen-Sheng Lien", "Hong-Han Shuai"], "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation", "comment": "Accepted to ACM Multimedia 2025 (MM '25)", "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.", "AI": {"tldr": "本文提出了一种无需训练的框架，利用大型语言模型（LLM）来消歧文本到图像（T2I）生成中的模糊颜色描述，并通过在文本嵌入空间中指导颜色混合操作，从而提高颜色保真度，且不影响图像质量。", "motivation": "当前扩散模型在处理微妙和复合颜色术语（如“蒂芙尼蓝”、“酸橙绿”）时，难以准确对齐颜色，导致生成的图像与人类意图不符。现有方法（如交叉注意力操纵、参考图像或微调）未能系统地解决模糊颜色描述问题，而准确的颜色对齐对于时尚、产品可视化等应用至关重要。", "method": "该方法是一个无需训练的框架：首先，利用大型语言模型（LLM）解析文本提示中模糊的颜色术语；然后，根据解析出的颜色术语在CIELAB颜色空间中的空间关系，直接在文本嵌入空间中细化文本嵌入，并指导颜色混合操作。与以往方法不同，该方法无需额外训练或外部参考图像。", "result": "实验结果表明，该框架在不损害图像质量的前提下，显著提高了颜色对齐的准确性。它成功弥合了文本语义与视觉生成之间的差距。", "conclusion": "该研究提供了一种有效的、无需训练的方法，通过结合LLM的语义理解能力和CIELAB颜色空间的结构化信息，解决了T2I模型在处理模糊颜色描述时的挑战，从而显著提升了生成图像的颜色保真度。"}}
{"id": "2509.10010", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.10010", "abs": "https://arxiv.org/abs/2509.10010", "authors": ["Adnan Ahmad", "Philine Kowol", "Stefan Hillmann", "Sebastian Möller"], "title": "Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs", "comment": null, "summary": "In this paper, we provide an extensive analysis of multi-label intent\nclassification using Large Language Models (LLMs) that are open-source,\npublicly available, and can be run in consumer hardware. We use the MultiWOZ\n2.1 dataset, a benchmark in the dialogue system domain, to investigate the\nefficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,\nMistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot\nsetup, giving 20 examples in the prompt with some instructions. Our approach\nfocuses on the differences in performance of these models across several\nperformance metrics by methodically assessing these models on multi-label\nintent classification tasks. Additionally, we compare the performance of the\ninstruction-based fine-tuning approach with supervised learning using the\nsmaller transformer model BertForSequenceClassification as a baseline. To\nevaluate the performance of the models, we use evaluation metrics like\naccuracy, precision, and recall as well as micro, macro, and weighted F1 score.\nWe also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1\noutperforms two other generative models on 11 intent classes out of 14 in terms\nof F-Score, with a weighted average of 0.50. It also has relatively lower\nHumming Loss and higher Jaccard Similarity, making it the winning model in the\nfew-shot setting. We find BERT based supervised classifier having superior\nperformance compared to the best performing few-shot generative LLM. The study\nprovides a framework for small open-source LLMs in detecting complex\nmulti-intent dialogues, enhancing the Natural Language Understanding aspect of\ntask-oriented chatbots.", "AI": {"tldr": "本研究对Llama2-7B、Mistral-7B和Yi-6B等开源LLM在多标签意图分类任务上的性能进行了广泛分析，使用MultiWOZ 2.1数据集进行少量样本学习。结果显示，Mistral-7B在LLM中表现最佳，但基于BERT的监督学习分类器整体性能优于表现最好的少量样本LLM。", "motivation": "研究开源、可在消费级硬件上运行的大语言模型（LLMs）在多标签意图分类任务中的有效性，以提升面向任务型聊天机器人的自然语言理解能力。", "method": "使用MultiWOZ 2.1数据集，评估了Llama2-7B-hf、Mistral-7B-v0.1和Yi-6B三款开源预训练LLM。采用少量样本（few-shot）设置，在提示中提供20个示例和指令。同时，将LLM的性能与使用小型Transformer模型BertForSequenceClassification进行监督学习（微调）的基线进行比较。评估指标包括准确率、精确率、召回率、微观/宏观/加权F1分数、Humming Loss、Jaccard Similarity，并报告推理时间和VRAM需求。", "result": "Mistral-7B-v0.1在14个意图类别中的11个上，F-Score（加权平均0.50）优于其他两款生成模型。它还具有相对较低的Humming Loss和较高的Jaccard Similarity，使其成为少量样本设置中的最佳模型。然而，研究发现基于BERT的监督分类器在性能上优于表现最佳的少量样本生成式LLM。", "conclusion": "本研究为小型开源LLM在检测复杂多意图对话方面提供了一个框架，有助于增强面向任务型聊天机器人的自然语言理解能力。尽管开源LLM在少量样本设置中表现出潜力，但经过监督学习微调的BERT模型在该特定任务上仍具有更优异的性能。"}}
{"id": "2509.10078", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10078", "abs": "https://arxiv.org/abs/2509.10078", "authors": ["Dongmin Choi", "Woojung Song", "Jongwook Han", "Eun-Ju Lee", "Yohan Jo"], "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models", "comment": "17 pages, 4 figures", "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI,\nPVQ) to measure the personality traits and values reflected in the responses of\nLarge Language Models (LLMs). However, concerns have been raised about applying\nthese human-designed questionnaires to LLMs. One such concern is their lack of\necological validity--the extent to which survey questions adequately reflect\nand resemble real-world contexts in which LLMs generate texts in response to\nuser queries. However, it remains unclear how established questionnaires and\necologically valid questionnaires differ in their outcomes, and what insights\nthese differences may provide. In this paper, we conduct a comprehensive\ncomparative analysis of the two types of questionnaires. Our analysis reveals\nthat established questionnaires (1) yield substantially different profiles of\nLLMs from ecologically valid ones, deviating from the psychological\ncharacteristics expressed in the context of user queries, (2) suffer from\ninsufficient items for stable measurement, (3) create misleading impressions\nthat LLMs possess stable constructs, and (4) yield exaggerated profiles for\npersona-prompted LLMs. Overall, our work cautions against the use of\nestablished psychological questionnaires for LLMs. Our code will be released\nupon publication.", "AI": {"tldr": "本文比较了用于测量大型语言模型（LLM）心理特征的传统问卷与生态有效问卷，发现传统问卷存在多方面问题，并建议谨慎使用。", "motivation": "研究人员已将人类心理测量问卷应用于LLM，但对其生态有效性存在担忧，即这些问卷是否能真实反映LLM在实际用户查询情境下的文本生成。目前尚不清楚传统问卷与生态有效问卷在结果上有何差异，以及这些差异能提供何种见解。", "method": "本文对两种类型的问卷（传统心理测量问卷和生态有效问卷）进行了全面的比较分析。", "result": "分析显示，传统问卷：(1) 得出的LLM画像与生态有效问卷显著不同，偏离了LLM在用户查询情境中表达的心理特征；(2) 缺乏足够的测量项目以实现稳定测量；(3) 错误地暗示LLM拥有稳定的心理结构；(4) 对于角色提示的LLM会产生夸大的画像。", "conclusion": "研究结果总体上警示不应将传统心理学问卷用于测量大型语言模型。"}}
{"id": "2509.10080", "categories": ["cs.CV", "I.2.9; I.4.8"], "pdf": "https://arxiv.org/pdf/2509.10080", "abs": "https://arxiv.org/abs/2509.10080", "authors": ["Minsang Kong", "Myeongjun Kim", "Sang Gu Kang", "Sang Hun Lee"], "title": "BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals", "comment": "Submitted to IEEE Transactions on Intelligent Transportation Systems\n  (under review)", "summary": "In autonomous driving, trajectory prediction is essential for ensuring safe\nand efficient navigation. To improve prediction accuracy, recent approaches\noften rely on pre-built high-definition (HD) maps or real-time local map\nconstruction modules to incorporate static environmental information. However,\npre-built HD maps are limited to specific regions and cannot adapt to transient\nchanges. In addition, local map construction modules, which recognize only\npredefined elements, may fail to capture critical scene details or introduce\nerrors that degrade prediction performance. To overcome these limitations, we\npropose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory\nprediction framework that operates directly in the bird's-eye view (BEV) space\nutilizing real-time sensor data without relying on any pre-built maps. The\nBEVTraj leverages deformable attention to efficiently extract relevant context\nfrom dense BEV features. Furthermore, we introduce a Sparse Goal Candidate\nProposal (SGCP) module, which enables full end-to-end prediction without\nrequiring any post-processing steps. Extensive experiments demonstrate that the\nBEVTraj achieves performance comparable to state-of-the-art HD map-based models\nwhile offering greater flexibility by eliminating the dependency on pre-built\nmaps. The source code is available at https://github.com/Kongminsang/bevtraj.", "AI": {"tldr": "BEVTraj是一种新颖的鸟瞰图（BEV）轨迹预测框架，它直接利用实时传感器数据在BEV空间中运行，无需预构建高精地图，并实现了与现有先进模型相当的性能。", "motivation": "自动驾驶中的轨迹预测依赖高精地图或实时局部地图构建，但高精地图受限于特定区域且无法适应瞬态变化，而局部地图构建可能遗漏关键细节或引入误差，从而影响预测性能。", "method": "BEVTraj直接在BEV空间中利用实时传感器数据进行轨迹预测，不依赖预构建地图。它利用可变形注意力机制高效提取BEV特征中的相关上下文，并引入稀疏目标候选提案（SGCP）模块实现完全端到端预测，无需后处理。", "result": "广泛实验表明，BEVTraj在性能上与最先进的基于高精地图的模型相当，同时通过消除对预构建地图的依赖提供了更大的灵活性。", "conclusion": "BEVTraj提供了一种灵活且无需预构建地图的轨迹预测解决方案，其性能可与基于高精地图的现有最佳模型媲美，解决了传统方法的局限性。"}}
{"id": "2509.10035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10035", "abs": "https://arxiv.org/abs/2509.10035", "authors": ["Laurin Plank", "Armin Zlomuzica"], "title": "Linguistic trajectories of bipolar disorder on social media", "comment": "Pre-print", "summary": "Language provides valuable markers of affective disorders such as bipolar\ndisorder (BD), yet clinical assessments remain limited in scale. In response,\nanalyses of social media (SM) language have gained prominence due to their high\ntemporal resolution and longitudinal scope. Here, we introduce a method to\ndetermine the timing of users' diagnoses and apply it to study language\ntrajectories from 3 years before to 21 years after BD diagnosis - contrasted\nwith uses reporting unipolar depression (UD) and non-affected users (HC). We\nshow that BD diagnosis is accompanied by pervasive linguistic alterations\nreflecting mood disturbance, psychiatric comorbidity, substance abuse,\nhospitalization, medical comorbidities, unusual thought content, and\ndisorganized thought. We further observe recurring mood-related language\nchanges across two decades after the diagnosis, with a pronounced 12-month\nperiodicity suggestive of seasonal mood episodes. Finally, trend-level evidence\nsuggests an increased periodicity in users estimated to be female. In sum, our\nfindings provide evidence for language alterations in the acute and chronic\nphase of BD. This validates and extends recent efforts leveraging SM for\nscalable monitoring of mental health.", "AI": {"tldr": "该研究利用社交媒体语言分析了双相情感障碍（BD）患者在诊断前后长达24年的语言变化轨迹，发现广泛的语言改变反映了情绪障碍、共病和季节性情绪发作，验证了社交媒体在精神健康监测中的潜力。", "motivation": "语言是情感障碍（如双相情感障碍）的重要标志，但临床评估规模有限。社交媒体语言分析因其高时间分辨率和纵向范围而受到关注，可弥补临床评估的不足。", "method": "研究开发了一种确定用户诊断时间的方法，并将其应用于分析双相情感障碍用户在诊断前3年到诊断后21年的社交媒体语言轨迹。结果与单相抑郁症（UD）用户和未受影响的对照组（HC）用户进行了对比。", "result": "双相情感障碍诊断伴随着广泛的语言改变，反映了情绪障碍、精神共病、药物滥用、住院、躯体共病、异常思维内容和思维混乱。诊断后二十年内，观察到反复出现的情绪相关语言变化，具有明显的12个月周期性，提示季节性情绪发作。此外，趋势层面的证据表明，估计为女性的用户周期性增加。", "conclusion": "研究结果为双相情感障碍急性期和慢性期的语言改变提供了证据。这验证并扩展了利用社交媒体进行可扩展精神健康监测的现有努力。"}}
{"id": "2509.10122", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10122", "abs": "https://arxiv.org/abs/2509.10122", "authors": ["Zongliang Wu", "Siming Zheng", "Peng-Tao Jiang", "Xin Yuan"], "title": "Realism Control One-step Diffusion for Real-World Image Super-Resolution", "comment": null, "summary": "Pre-trained diffusion models have shown great potential in real-world image\nsuper-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.\nWhile one-step diffusion (OSD) methods significantly improve efficiency\ncompared to traditional multi-step approaches, they still have limitations in\nbalancing fidelity and realism across diverse scenarios. Since the OSDs for SR\nare usually trained or distilled by a single timestep, they lack flexible\ncontrol mechanisms to adaptively prioritize these competing objectives, which\nare inherently manageable in multi-step methods through adjusting sampling\nsteps. To address this challenge, we propose a Realism Controlled One-step\nDiffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping\nstrategy that enables explicit control over fidelity-realism trade-offs during\nthe noise prediction phase with minimal training paradigm modifications and\noriginal training data. A degradation-aware sampling strategy is also\nintroduced to align distillation regularization with the grouping strategy and\nenhance the controlling of trade-offs. Moreover, a visual prompt injection\nmodule is used to replace conventional text prompts with degradation-aware\nvisual tokens, enhancing both restoration accuracy and semantic consistency.\nOur method achieves superior fidelity and perceptual quality while maintaining\ncomputational efficiency. Extensive experiments demonstrate that RCOD\noutperforms state-of-the-art OSD methods in both quantitative metrics and\nvisual qualities, with flexible realism control capabilities in the inference\nstage. The code will be released.", "AI": {"tldr": "本文提出了一种名为RCOD的真实世界图像超分辨率（Real-ISR）一步扩散模型，通过潜在域分组策略、降级感知采样和视觉提示注入，解决了现有一步扩散模型在保真度和真实感之间平衡的挑战，实现了灵活控制、卓越性能和高效率。", "motivation": "预训练扩散模型在Real-ISR任务中潜力巨大，但一步扩散（OSD）方法虽然提高了效率，却难以在不同场景下平衡保真度和真实感。这是因为OSD通常通过单一时间步训练或蒸馏，缺乏像多步方法那样通过调整采样步数来灵活控制这些竞争目标的能力。", "method": "本文提出了一个真实感控制一步扩散（RCOD）框架：1. 引入了潜在域分组策略，在噪声预测阶段实现对保真度-真实感权衡的显式控制。2. 提出了降级感知采样策略，使蒸馏正则化与分组策略对齐，并增强权衡控制。3. 使用视觉提示注入模块，以降级感知视觉Token取代传统文本提示，提高恢复精度和语义一致性。", "result": "RCOD在保持计算效率的同时，实现了卓越的保真度和感知质量。广泛的实验表明，RCOD在定量指标和视觉质量上均优于最先进的一步扩散方法，并在推理阶段提供了灵活的真实感控制能力。", "conclusion": "RCOD框架通过创新的潜在域分组、降级感知采样和视觉提示注入，有效解决了Real-ISR中一步扩散模型在保真度和真实感平衡方面的局限性，提供了高效、高性能且可灵活控制的超分辨率解决方案。"}}
{"id": "2509.10093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10093", "abs": "https://arxiv.org/abs/2509.10093", "authors": ["Laura Bragagnolo", "Matteo Terreran", "Leonardo Barcellona", "Stefano Ghidoni"], "title": "Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing", "comment": "ICIAP 2025", "summary": "Multi-human parsing is the task of segmenting human body parts while\nassociating each part to the person it belongs to, combining instance-level and\npart-level information for fine-grained human understanding. In this work, we\ndemonstrate that, while state-of-the-art approaches achieved notable results on\npublic datasets, they struggle considerably in segmenting people with\noverlapping bodies. From the intuition that overlapping people may appear\nseparated from a different point of view, we propose a novel training framework\nexploiting multi-view information to improve multi-human parsing models under\nocclusions. Our method integrates such knowledge during the training process,\nintroducing a novel approach based on weak supervision on human instances and a\nmulti-view consistency loss. Given the lack of suitable datasets in the\nliterature, we propose a semi-automatic annotation strategy to generate human\ninstance segmentation masks from multi-view RGB+D data and 3D human skeletons.\nThe experiments demonstrate that the approach can achieve up to a 4.20\\%\nrelative improvement on human parsing over the baseline model in occlusion\nscenarios.", "AI": {"tldr": "该研究针对多人解析中人体遮挡问题，提出了一种利用多视角信息的新型训练框架，结合弱监督和多视角一致性损失，并在遮挡场景下实现了显著性能提升。同时，还提出了一种半自动标注策略来生成多视角数据集。", "motivation": "现有最先进的多人解析方法在处理人体相互重叠（即遮挡）的场景时表现不佳。", "method": "1. 提出了一种利用多视角信息的新型训练框架，以改善遮挡下的多人解析模型。2. 在训练过程中引入了基于人体实例弱监督的新方法。3. 引入了多视角一致性损失。4. 针对缺乏合适数据集的问题，提出了一种半自动标注策略，从多视角RGB+D数据和3D人体骨架生成人体实例分割掩码。", "result": "在遮挡场景下，该方法在人体解析方面比基线模型取得了高达4.20%的相对改进。", "conclusion": "该研究提出的多视角训练框架，结合弱监督和一致性损失，能有效提高多人解析模型在人体遮挡场景下的性能，并通过半自动标注解决了数据集稀缺问题。"}}
{"id": "2509.10040", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10040", "abs": "https://arxiv.org/abs/2509.10040", "authors": ["Mohamed Basem", "Mohamed Younes", "Seif Ahmed", "Abdelrahman Moustafa"], "title": "!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment", "comment": "10 Pages , 8 figures , ArabicNLP 2025 , Co-located with EMNLP 2025", "summary": "We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained\nArabic readability assessment, achieving first place in six of six tracks. Our\napproach is a confidence-weighted ensemble of four complementary transformer\nmodels (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with\ndistinct loss functions to capture diverse readability signals. To tackle\nsevere class imbalance and data scarcity, we applied weighted training,\nadvanced preprocessing, SAMER corpus relabeling with our strongest model, and\nsynthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level\nsamples. A targeted post-processing step corrected prediction distribution\nskew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system\nreached 87.5 percent QWK at the sentence level and 87.4 percent at the document\nlevel, demonstrating the power of model and loss diversity, confidence-informed\nfusion, and intelligent augmentation for robust Arabic readability prediction.", "AI": {"tldr": "该论文介绍了MSA在BAREC 2025细粒度阿拉伯语可读性评估共享任务中获得六个赛道第一名的系统，该系统结合了多样化的Transformer模型、损失函数、数据增强和后处理技术。", "motivation": "参与BAREC 2025共享任务，解决细粒度阿拉伯语可读性评估中的严重类别不平衡和数据稀缺问题。", "method": "采用四种互补的Transformer模型（AraBERTv2, AraELECTRA, MARBERT, CAMeLBERT）的置信度加权集成，每种模型使用不同的损失函数进行微调。为解决数据问题，使用了加权训练、高级预处理、对SAMER语料库进行重新标注，并通过Gemini 2.5 Flash生成了约10,000个稀有级别的合成数据。此外，还应用了目标后处理步骤来纠正预测分布偏差。", "result": "在BAREC 2025共享任务的所有六个赛道中均获得第一名。在句子级别达到87.5%的二次加权Kappa (QWK)，在文档级别达到87.4%的QWK。后处理步骤带来了6.3%的QWK提升。", "conclusion": "该研究证明了模型和损失函数多样性、基于置信度的融合以及智能数据增强对于实现鲁棒的阿拉伯语可读性预测的强大作用。"}}
{"id": "2509.10127", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10127", "abs": "https://arxiv.org/abs/2509.10127", "authors": ["Zhengyu Hu", "Zheyuan Xiao", "Max Xiong", "Yuxuan Lei", "Tianfu Wang", "Jianxun Lian", "Kaize Ding", "Ziang Xiao", "Nicholas Jing Yuan", "Xing Xie"], "title": "Population-Aligned Persona Generation for LLM-based Social Simulation", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled human-like\nsocial simulations at unprecedented scale and fidelity, offering new\nopportunities for computational social science. A key challenge, however, is\nthe construction of persona sets that authentically represent the diversity and\ndistribution of real-world populations. Most existing LLM-based social\nsimulation studies focus primarily on designing agentic frameworks and\nsimulation environments, often overlooking the complexities of persona\ngeneration and the potential biases introduced by unrepresentative persona\nsets. In this paper, we propose a systematic framework for synthesizing\nhigh-quality, population-aligned persona sets for LLM-driven social simulation.\nOur approach begins by leveraging LLMs to generate narrative personas from\nlong-term social media data, followed by rigorous quality assessment to filter\nout low-fidelity profiles. We then apply importance sampling to achieve global\nalignment with reference psychometric distributions, such as the Big Five\npersonality traits. To address the needs of specific simulation contexts, we\nfurther introduce a task-specific module that adapts the globally aligned\npersona set to targeted subpopulations. Extensive experiments demonstrate that\nour method significantly reduces population-level bias and enables accurate,\nflexible social simulation for a wide range of research and policy\napplications.", "AI": {"tldr": "本文提出一个系统框架，用于为LLM驱动的社会模拟合成高质量、与人口分布对齐的角色集，以解决现有方法中角色生成复杂性和代表性不足的问题，从而显著减少偏差并提高模拟准确性。", "motivation": "尽管大型语言模型（LLMs）使大规模社会模拟成为可能，但现有的LLM社会模拟研究主要关注代理框架和模拟环境设计，往往忽视角色生成（persona generation）的复杂性以及不具代表性的角色集可能引入的偏见。因此，需要一个能真实反映人口多样性和分布的角色集构建方法。", "method": "该方法首先利用LLMs从长期社交媒体数据生成叙事性角色；接着进行严格的质量评估以过滤低保真度档案；然后应用重要性抽样（importance sampling）实现与参考心理测量分布（如大五人格特质）的全局对齐；最后，引入一个任务特定模块，将全局对齐的角色集适应于目标子人群，以满足特定模拟情境的需求。", "result": "广泛的实验证明，该方法显著减少了人口层面的偏见，并能为广泛的研究和政策应用实现准确、灵活的社会模拟。", "conclusion": "该论文提出的系统框架成功解决了LLM驱动社会模拟中构建高质量、与人口对齐的角色集的关键挑战，从而能够进行更准确、更灵活的社会模拟。"}}
{"id": "2509.10105", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10105", "abs": "https://arxiv.org/abs/2509.10105", "authors": ["Young-rok Cha", "Jeongho Ju", "SunYoung Park", "Jong-Hyeon Lee", "Younghyun Yu", "Youngjune Kim"], "title": "VARCO-VISION-2.0 Technical Report", "comment": "19 pages, 1 figure, 14 tables. Technical report for VARCO-VISION-2.0,\n  a Korean-English bilingual VLM in 14B and 1.7B variants. Key features:\n  multi-image understanding, OCR with text localization, improved Korean\n  capabilities", "summary": "We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model\n(VLM) for Korean and English with improved capabilities compared to the\nprevious model VARCO-VISION-14B. The model supports multi-image understanding\nfor complex inputs such as documents, charts, and tables, and delivers\nlayoutaware OCR by predicting both textual content and its spatial location.\nTrained with a four-stage curriculum with memory-efficient techniques, the\nmodel achieves enhanced multimodal alignment, while preserving core language\nabilities and improving safety via preference optimization. Extensive benchmark\nevaluations demonstrate strong spatial grounding and competitive results for\nboth languages, with the 14B model achieving 8th place on the OpenCompass VLM\nleaderboard among models of comparable scale. Alongside the 14B-scale model, we\nrelease a 1.7B version optimized for on-device deployment. We believe these\nmodels advance the development of bilingual VLMs and their practical\napplications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a\nfull-scale 14B model and a lightweight 1.7B model.", "AI": {"tldr": "本文介绍了VARCO-VISION-2.0，一个改进的韩语和英语双语视觉-语言模型（VLM），它支持多图像理解、布局感知OCR，并通过多阶段训练和偏好优化实现了增强的多模态对齐和安全性，并在基准测试中表现出色。", "motivation": "研究动机是为了改进前代模型VARCO-VISION-14B，提升双语VLM在韩语和英语上的能力，支持文档、图表、表格等复杂输入的多图像理解，并提供布局感知的OCR功能，同时保持核心语言能力和提高安全性。", "method": "模型采用四阶段课程训练，结合内存高效技术以实现增强的多模态对齐。通过偏好优化来保持核心语言能力并提高安全性。它还支持多图像理解，并能预测文本内容及其空间位置以实现布局感知OCR。", "result": "VARCO-VISION-2.0实现了增强的多模态对齐，同时保留了核心语言能力并提高了安全性。在广泛的基准评估中，模型在两种语言上都展现出强大的空间定位能力和竞争力，其中14B模型在OpenCompass VLM排行榜上同等规模模型中排名第8。此外，还发布了针对设备部署优化的1.7B版本。", "conclusion": "VARCO-VISION-2.0模型（包括14B和1.7B版本）的发布，标志着双语VLM及其实际应用发展的重要进展。"}}
{"id": "2509.10087", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10087", "abs": "https://arxiv.org/abs/2509.10087", "authors": ["Mustapha Adamu", "Qi Zhang", "Huitong Pan", "Longin Jan Latecki", "Eduard C. Dragut"], "title": "Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery", "comment": "ACM SIGIR 2025 Workshop MANILA", "summary": "The growing complexity and volume of climate science literature make it\nincreasingly difficult for researchers to find relevant information across\nmodels, datasets, regions, and variables. This paper introduces a\ndomain-specific Knowledge Graph (KG) built from climate publications and\nbroader scientific texts, aimed at improving how climate knowledge is accessed\nand used. Unlike keyword based search, our KG supports structured, semantic\nqueries that help researchers discover precise connections such as which models\nhave been validated in specific regions or which datasets are commonly used\nwith certain teleconnection patterns. We demonstrate how the KG answers such\nquestions using Cypher queries, and outline its integration with large language\nmodels in RAG systems to improve transparency and reliability in\nclimate-related question answering. This work moves beyond KG construction to\nshow its real world value for climate researchers, model developers, and others\nwho rely on accurate, contextual scientific information.", "AI": {"tldr": "该论文介绍了一个从气候科学文献构建的领域特定知识图谱（KG），旨在通过支持结构化语义查询，提高气候知识的获取和使用效率，并可与大型语言模型集成。", "motivation": "气候科学文献日益复杂和庞大，研究人员难以在模型、数据集、区域和变量等多个维度上找到相关信息。", "method": "构建了一个从气候出版物和更广泛科学文本中提取信息的领域特定知识图谱（KG）。通过Cypher查询演示了KG如何回答具体问题，并概述了其与大型语言模型（LLM）在RAG系统中的集成，以提高气候相关问答的透明度和可靠性。", "result": "该知识图谱支持结构化、语义化的查询，帮助研究人员发现精确的连接，例如哪些模型在特定区域经过验证，或哪些数据集常与某些遥相关模式一起使用。它能够改进气候相关问答的透明度和可靠性。", "conclusion": "这项工作超越了知识图谱的构建，展示了其对气候研究人员、模型开发者以及其他依赖准确、情境化科学信息的人的实际价值。"}}
{"id": "2509.10179", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10179", "abs": "https://arxiv.org/abs/2509.10179", "authors": ["Jiří Milička", "Anna Marklová", "Václav Cvrček"], "title": "Benchmark of stylistic variation in LLM-generated texts", "comment": null, "summary": "This study investigates the register variation in texts written by humans and\ncomparable texts produced by large language models (LLMs). Biber's\nmultidimensional analysis (MDA) is applied to a sample of human-written texts\nand AI-created texts generated to be their counterparts to find the dimensions\nof variation in which LLMs differ most significantly and most systematically\nfrom humans. As textual material, a new LLM-generated corpus AI-Brown is used,\nwhich is comparable to BE-21 (a Brown family corpus representing contemporary\nBritish English). Since all languages except English are underrepresented in\nthe training data of frontier LLMs, similar analysis is replicated on Czech\nusing AI-Koditex corpus and Czech multidimensional model. Examined were 16\nfrontier models in various settings and prompts, with emphasis placed on the\ndifference between base models and instruction-tuned models. Based on this, a\nbenchmark is created through which models can be compared with each other and\nranked in interpretable dimensions.", "AI": {"tldr": "本研究使用Biber的多维分析（MDA）方法，调查了人类和大型语言模型（LLM）生成的文本在语域变异方面的差异，并创建了一个模型评估基准。", "motivation": "了解LLM生成的文本在语域特征上与人类文本有何不同，特别是LLM在哪些维度上与人类存在最显著和最系统的差异，以及这些差异在不同语言（英语和捷克语）和不同模型类型（基础模型与指令调优模型）之间如何体现。", "method": "采用Biber的多维分析（MDA）方法；使用新的LLM生成语料库AI-Brown（与BE-21相当）进行英语文本分析；使用AI-Koditex语料库和捷克语多维模型进行捷克语文本分析；考察了16个前沿LLM（包括基础模型和指令调优模型），并使用了多种设置和提示；基于分析结果创建了一个模型比较和排序的基准。", "result": "LLM在语域变异的某些维度上与人类文本存在最显著和最系统的差异；创建了一个可解释的基准，用于比较和评估不同LLM模型。", "conclusion": "该研究揭示了LLM生成文本在语域上与人类文本的系统性差异，并提供了一个有效工具来评估和排名LLM在生成具有特定语域特征文本方面的表现。"}}
{"id": "2509.10114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10114", "abs": "https://arxiv.org/abs/2509.10114", "authors": ["MohammadAli Hamidi", "Hadi Amirpour", "Luigi Atzori", "Christian Timmerer"], "title": "A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss", "comment": null, "summary": "Face image quality assessment (FIQA) plays a critical role in face\nrecognition and verification systems, especially in uncontrolled, real-world\nenvironments. Although several methods have been proposed, general-purpose\nno-reference image quality assessment techniques often fail to capture\nface-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be\ncomputationally intensive, limiting their practical applicability. We propose a\nlightweight and efficient method for FIQA, designed for the perceptual\nevaluation of face images in the wild. Our approach integrates an ensemble of\ntwo compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,\nwith prediction-level fusion via simple averaging. To enhance alignment with\nhuman perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),\ncombining mean squared error (MSE) with a Pearson correlation regularizer. Our\nmethod achieves a strong balance between accuracy and computational cost,\nmaking it suitable for real-world deployment. Experiments on the VQualA FIQA\nbenchmark demonstrate that our model achieves a Spearman rank correlation\ncoefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient\n(PLCC) of 0.9894, remaining within competition efficiency constraints.", "AI": {"tldr": "本文提出了一种轻量高效的人脸图像质量评估（FIQA）方法，通过集成紧凑型CNN模型和结合皮尔逊相关系数的损失函数，在保证准确性的同时降低了计算成本。", "motivation": "现有的通用无参考图像质量评估技术无法有效捕捉人脸特有的退化，而最先进的FIQA模型则计算量大，限制了其实际应用，尤其是在不受控的真实世界环境中。", "method": "该方法集成了两个紧凑型卷积神经网络（MobileNetV3-Small和ShuffleNetV2），通过简单平均进行预测级融合。为增强与人类感知判断的一致性，采用了相关感知损失（MSECorrLoss），该损失结合了均方误差（MSE）和皮尔逊相关正则化器。", "result": "在VQualA FIQA基准测试中，该模型实现了0.9829的Spearman秩相关系数（SRCC）和0.9894的Pearson线性相关系数（PLCC），并在效率约束范围内保持了高性能，达到了准确性和计算成本之间的良好平衡。", "conclusion": "所提出的方法在准确性和计算成本之间取得了强大的平衡，使其适用于真实世界的部署，解决了现有FIQA模型在捕获人脸特有退化和计算效率方面的局限性。"}}
{"id": "2509.10095", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10095", "abs": "https://arxiv.org/abs/2509.10095", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Ammar Mohammed"], "title": "Arabic Large Language Models for Medical Text Generation", "comment": "Published in 2025 4th International Conference on Computer\n  Technologies (ICCTech)", "summary": "Efficient hospital management systems (HMS) are critical worldwide to address\nchallenges such as overcrowding, limited resources, and poor availability of\nurgent health care. Existing methods often lack the ability to provide\naccurate, real-time medical advice, particularly for irregular inputs and\nunderrepresented languages. To overcome these limitations, this study proposes\nan approach that fine-tunes large language models (LLMs) for Arabic medical\ntext generation. The system is designed to assist patients by providing\naccurate medical advice, diagnoses, drug recommendations, and treatment plans\nbased on user input. The research methodology required the collection of a\nunique dataset from social media platforms, capturing real-world medical\nconversations between patients and doctors. The dataset, which includes patient\ncomplaints together with medical advice, was properly cleaned and preprocessed\nto account for multiple Arabic dialects. Fine-tuning state-of-the-art\ngenerative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2\nMedium, optimized the system's ability to generate reliable medical text.\nResults from evaluations indicate that the fine-tuned Mistral-7B model\noutperformed the other models, achieving average BERT (Bidirectional Encoder\nRepresentations from Transformers) Score values in precision, recall, and\nF1-scores of 68.5\\%, 69.08\\%, and 68.5\\%, respectively. Comparative\nbenchmarking and qualitative assessments validate the system's ability to\nproduce coherent and relevant medical replies to informal input. This study\nhighlights the potential of generative artificial intelligence (AI) in\nadvancing HMS, offering a scalable and adaptable solution for global healthcare\nchallenges, especially in linguistically and culturally diverse environments.", "AI": {"tldr": "本研究通过微调大型语言模型（LLMs）以生成阿拉伯语医疗文本，旨在改进医院管理系统（HMS），提供准确的实时医疗建议，尤其针对非标准输入和代表性不足的语言。微调后的Mistral-7B模型表现最佳。", "motivation": "现有的医院管理系统在处理过度拥挤、资源有限和紧急医疗服务不足等挑战时效率低下。现有方法难以提供准确的实时医疗建议，尤其对于非标准输入和代表性不足的语言（如阿拉伯语）。", "method": "本研究提出一种通过微调大型语言模型（LLMs）来生成阿拉伯语医疗文本的方法。具体步骤包括：1. 收集来自社交媒体的独特数据集，包含患者与医生之间的真实医疗对话，并进行清洗和预处理以处理多种阿拉伯语方言。2. 微调最先进的生成模型，包括Mistral-7B-Instruct-v0.2、LLaMA-2-7B和GPT-2 Medium。", "result": "评估结果显示，微调后的Mistral-7B模型表现优于其他模型，在BERT得分的精确率、召回率和F1分数上分别达到68.5%、69.08%和68.5%。对比基准测试和定性评估证实了该系统能够对非正式输入生成连贯且相关的医疗回复。", "conclusion": "本研究强调了生成式人工智能在推进医院管理系统方面的潜力，为全球医疗挑战，特别是在语言和文化多样性环境中，提供了一个可扩展和适应性强的解决方案。"}}
{"id": "2509.10208", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10208", "abs": "https://arxiv.org/abs/2509.10208", "authors": ["Shengqiang Fu"], "title": "SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning", "comment": null, "summary": "Large Language Models often generate unfaithful responses in knowledge\nintensive tasks due to knowledge conflict,that is,a preference for relying on\ninternal parametric knowledge rather than the provided context.To address this\nissue,we propose a novel self improving framework,Self Improving Faithfulness\nAware Contrastive Tuning.The framework uses a self instruct mechanism that\nallows the base LLM to automatically generate high quality,structured\ncontrastive learning data,including anchor samples,semantically equivalent\npositive samples,and negative samples simulating unfaithful scenarios.This\napproach significantly reduces the cost of manual\nannotation.Subsequently,contrastive learning is applied to train the\nmodel,enabling it to pull faithful responses closer and push unfaithful\nresponses farther apart in the representation space.Experiments on knowledge\nconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT\nmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%\nover the best baseline method,while significantly reducing dependence on\ninternal memory.The results indicate that SI FACT provides strong effectiveness\nand high data efficiency in enhancing the contextual faithfulness of\nLLMs,offering a practical pathway toward building more proactive and\ntrustworthy language models.", "AI": {"tldr": "为解决大型语言模型（LLMs）在知识密集型任务中因知识冲突导致的不忠实响应问题，本文提出了一种名为SI FACT的自改进框架，该框架通过自指令机制自动生成对比学习数据，并利用对比调优来增强LLMs的上下文忠实性，显著提高了上下文召回率并降低了对内部记忆的依赖。", "motivation": "大型语言模型在知识密集型任务中常产生不忠实的响应，原因在于它们倾向于依赖内部参数化知识而非提供的上下文，即存在知识冲突。", "method": "本文提出了一个名为“Self Improving Faithfulness Aware Contrastive Tuning (SI FACT)”的自改进框架。该框架利用自指令机制，使基础LLM能够自动生成高质量、结构化的对比学习数据，包括锚点样本、语义等效的正样本以及模拟不忠实场景的负样本，从而大幅减少手动标注成本。随后，通过对比学习训练模型，使其在表示空间中将忠实响应拉近，将不忠实响应推远。", "result": "在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法将上下文召回率提高了6.2%，同时显著降低了对内部记忆的依赖。", "conclusion": "SI FACT在增强LLMs上下文忠实性方面表现出强大的有效性和高数据效率，为构建更主动、更值得信赖的语言模型提供了一条实用的途径。"}}
{"id": "2509.10134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10134", "abs": "https://arxiv.org/abs/2509.10134", "authors": ["Rini Smita Thakur", "Rajeev Ranjan Dwivedi", "Vinod K Kurmi"], "title": "Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment", "comment": "Accepted in BMVC 2025", "summary": "Accurate segmentation of the optic disc and cup is critical for the early\ndiagnosis and management of ocular diseases such as glaucoma. However,\nsegmentation models trained on one dataset often suffer significant performance\ndegradation when applied to target data acquired under different imaging\nprotocols or conditions. To address this challenge, we propose\n\\textbf{Grad-CL}, a novel source-free domain adaptation framework that\nleverages a pre-trained source model and unlabeled target data to robustly\nadapt segmentation performance without requiring access to the original source\ndata. Grad-CL combines a gradient-guided pseudolabel refinement module with a\ncosine similarity-based contrastive learning strategy. In the first stage,\nsalient class-specific features are extracted via a gradient-based mechanism,\nenabling more accurate uncertainty quantification and robust prototype\nestimation for refining noisy pseudolabels. In the second stage, a contrastive\nloss based on cosine similarity is employed to explicitly enforce inter-class\nseparability between the gradient-informed features of the optic cup and disc.\nExtensive experiments on challenging cross-domain fundus imaging datasets\ndemonstrate that Grad-CL outperforms state-of-the-art unsupervised and\nsource-free domain adaptation methods, achieving superior segmentation accuracy\nand improved boundary delineation. Project and code are available at\nhttps://visdomlab.github.io/GCL/.", "AI": {"tldr": "Grad-CL是一个新颖的无源域适应框架，结合梯度引导的伪标签细化和对比学习，用于在不访问原始源数据的情况下，鲁棒地适应视盘和视杯的分割模型，以解决跨域性能下降问题。", "motivation": "由于成像协议或条件差异，在一个数据集上训练的分割模型在应用于目标数据时性能会显著下降。准确的视盘和视杯分割对于青光眼等眼部疾病的早期诊断和管理至关重要，因此需要解决跨域性能下降的问题。", "method": "本文提出了Grad-CL框架，它利用预训练的源模型和未标记的目标数据进行适应。该方法结合了两个阶段：1) 梯度引导的伪标签细化模块，通过梯度机制提取显著的类别特定特征，实现更准确的不确定性量化和鲁棒的原型估计，以细化噪声伪标签；2) 基于余弦相似度的对比学习策略，明确强制视杯和视盘的梯度信息特征之间的类间可分离性。", "result": "在具有挑战性的跨域眼底图像数据集上的大量实验表明，Grad-CL优于最先进的无监督和无源域适应方法，实现了卓越的分割精度和改进的边界描绘。", "conclusion": "Grad-CL是一个有效的无源域适应框架，能够鲁棒地适应视盘和视杯的分割模型，显著提高在不同成像协议或条件下的目标数据上的性能，而无需访问原始源数据。"}}
{"id": "2509.10108", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10108", "abs": "https://arxiv.org/abs/2509.10108", "authors": ["Abdulrahman Allam", "Seif Ahmed", "Ali Hamdi", "Khaled Shaban"], "title": "Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records", "comment": "Accepted in AICCSA 2025", "summary": "The development of medical chatbots in Arabic is significantly constrained by\nthe scarcity of large-scale, high-quality annotated datasets. While prior\nefforts compiled a dataset of 20,000 Arabic patient-doctor interactions from\nsocial media to fine-tune large language models (LLMs), model scalability and\ngeneralization remained limited. In this study, we propose a scalable synthetic\ndata augmentation strategy to expand the training corpus to 100,000 records.\nUsing advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated\n80,000 contextually relevant and medically coherent synthetic question-answer\npairs grounded in the structure of the original dataset. These synthetic\nsamples were semantically filtered, manually validated, and integrated into the\ntraining pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,\nand evaluated their performance using BERTScore metrics and expert-driven\nqualitative assessments. To further analyze the effectiveness of synthetic\nsources, we conducted an ablation study comparing ChatGPT-4o and\nGemini-generated data independently. The results showed that ChatGPT-4o data\nconsistently led to higher F1-scores and fewer hallucinations across all\nmodels. Overall, our findings demonstrate the viability of synthetic\naugmentation as a practical solution for enhancing domain-specific language\nmodels in-low resource medical NLP, paving the way for more inclusive,\nscalable, and accurate Arabic healthcare chatbot systems.", "AI": {"tldr": "本研究提出了一种可扩展的合成数据增强策略，将阿拉伯语医疗聊天机器人数据集从2万条扩展到10万条，显著提高了低资源医疗NLP领域LLM的性能和泛化能力。", "motivation": "阿拉伯语医疗聊天机器人的开发受到大规模、高质量标注数据集稀缺的严重限制。尽管现有工作已编译了一个2万条社交媒体医患互动数据集，但模型的扩展性和泛化能力仍然有限。", "method": "研究提出了一种可扩展的合成数据增强策略，利用ChatGPT-4o和Gemini 2.5 Pro等先进生成式AI系统生成了8万条上下文相关且医学上连贯的合成问答对，并将其整合到现有数据集中，使训练语料库扩展到10万条记录。这些合成样本经过语义过滤和人工验证。随后，研究微调了包括Mistral-7B和AraGPT2在内的五种大型语言模型，并使用BERTScore指标和专家驱动的定性评估进行性能评估。为分析合成数据源的有效性，还进行了消融研究，独立比较了ChatGPT-4o和Gemini生成的数据。", "result": "结果显示，ChatGPT-4o生成的数据在所有模型上都持续带来更高的F1分数和更少的幻觉。总的来说，研究发现合成数据增强在提高低资源医疗NLP领域特定领域语言模型方面是可行的解决方案。", "conclusion": "合成数据增强是提高低资源医疗NLP领域特定领域语言模型的实用解决方案，为开发更具包容性、可扩展和准确的阿拉伯语医疗聊天机器人系统铺平了道路。"}}
{"id": "2509.10266", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10266", "abs": "https://arxiv.org/abs/2509.10266", "authors": ["Wenfang Wu", "Tingting Yuan", "Yupeng Li", "Daling Wang", "Xiaoming Fu"], "title": "SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion", "comment": null, "summary": "Sign language translation (SLT) aims to translate natural language from sign\nlanguage videos, serving as a vital bridge for inclusive communication. While\nrecent advances leverage powerful visual backbones and large language models,\nmost approaches mainly focus on manual signals (hand gestures) and tend to\noverlook non-manual cues like mouthing. In fact, mouthing conveys essential\nlinguistic information in sign languages and plays a crucial role in\ndisambiguating visually similar signs. In this paper, we propose SignClip, a\nnovel framework to improve the accuracy of sign language translation. It fuses\nmanual and non-manual cues, specifically spatial gesture and lip movement\nfeatures. Besides, SignClip introduces a hierarchical contrastive learning\nframework with multi-level alignment objectives, ensuring semantic consistency\nacross sign-lip and visual-text modalities. Extensive experiments on two\nbenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our\napproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip\nsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from\n24.32 to 24.71, and ROUGE from 46.57 to 48.38.", "AI": {"tldr": "SignClip通过融合手势和唇部动作等非手动线索，并采用分层对比学习框架，显著提升了手语翻译的准确性，超越了现有最先进模型。", "motivation": "现有的手语翻译方法主要关注手动信号（手势），却忽略了唇部动作等非手动线索，而这些线索在手语中传递着重要的语言信息，并有助于区分视觉上相似的符号。", "method": "本文提出了SignClip框架，它融合了空间手势和唇部动作特征（即手动和非手动线索）。此外，SignClip引入了一个分层对比学习框架，具有多级对齐目标，以确保手语-唇部和视觉-文本模态之间的语义一致性。", "result": "在PHOENIX14T和How2Sign两个基准数据集上进行了广泛实验，证明了该方法的优越性。例如，在PHOENIX14T数据集的无Gloss设置下，SignClip的BLEU-4从24.32提高到24.71，ROUGE从46.57提高到48.38，超越了之前的最先进模型SpaMo。", "conclusion": "SignClip通过有效融合手动和非手动线索，并利用分层对比学习，显著提高了手语翻译的准确性，为包容性交流提供了重要的技术支持。"}}
{"id": "2509.10140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10140", "abs": "https://arxiv.org/abs/2509.10140", "authors": ["Yifan Chang", "Jie Qin", "Limeng Qiao", "Xiaofeng Wang", "Zheng Zhu", "Lin Ma", "Xingang Wang"], "title": "Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization", "comment": null, "summary": "Vector quantization (VQ) is a key component in discrete tokenizers for image\ngeneration, but its training is often unstable due to straight-through\nestimation bias, one-step-behind updates, and sparse codebook gradients, which\nlead to suboptimal reconstruction performance and low codebook usage. In this\nwork, we analyze these fundamental challenges and provide a simple yet\neffective solution. To maintain high codebook usage in VQ networks (VQN) during\nlearning annealing and codebook size expansion, we propose VQBridge, a robust,\nscalable, and efficient projector based on the map function method. VQBridge\noptimizes code vectors through a compress-process-recover pipeline, enabling\nstable and effective codebook training. By combining VQBridge with learning\nannealing, our VQN achieves full (100%) codebook usage across diverse codebook\nconfigurations, which we refer to as FVQ (FullVQ). Through extensive\nexperiments, we demonstrate that FVQ is effective, scalable, and generalizable:\nit attains 100% codebook usage even with a 262k-codebook, achieves\nstate-of-the-art reconstruction performance, consistently improves with larger\ncodebooks, higher vector channels, or longer training, and remains effective\nacross different VQ variants. Moreover, when integrated with LlamaGen, FVQ\nsignificantly enhances image generation performance, surpassing visual\nautoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,\nhighlighting the importance of high-quality tokenizers for strong\nautoregressive image generation.", "AI": {"tldr": "向量量化（VQ）训练因不稳定性导致性能不佳和码本利用率低。本文提出了VQBridge结合学习退火，实现了100%码本利用率（FVQ），显著提升了重建质量和图像生成性能，超越了现有模型。", "motivation": "向量量化（VQ）作为图像生成离散分词器的关键组件，其训练常因直通估计偏差、一步滞后更新和稀疏码本梯度而变得不稳定，导致重建性能不理想和码本利用率低下。", "method": "本文提出VQBridge，一种基于映射函数方法的鲁棒、可扩展、高效的投影器，通过“压缩-处理-恢复”管道优化码向量，实现稳定的码本训练。通过将VQBridge与学习退火相结合，实现了100%码本利用率的VQN，称之为FVQ（FullVQ）。", "result": "FVQ在各种码本配置下均实现了100%的码本利用率（包括262k码本），达到了最先进的重建性能，并随着码本增大、向量通道增加或训练时间延长而持续改进。它对不同的VQ变体都有效。此外，与LlamaGen结合后，FVQ显著提升了图像生成性能，在rFID指标上超越了视觉自回归模型（VAR）0.5和扩散模型（DiT）0.2。", "conclusion": "FVQ通过解决VQ训练的不稳定性问题，证明了高质量分词器对于强大的自回归图像生成至关重要，并能显著提升生成模型的性能。"}}
{"id": "2509.10116", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.10116", "abs": "https://arxiv.org/abs/2509.10116", "authors": ["Julian Linke", "Barbara Schuppler"], "title": "Prominence-aware automatic speech recognition for conversational speech", "comment": null, "summary": "This paper investigates prominence-aware automatic speech recognition (ASR)\nby combining prominence detection and speech recognition for conversational\nAustrian German. First, prominence detectors were developed by fine-tuning\nwav2vec2 models to classify word-level prominence. The detector was then used\nto automatically annotate prosodic prominence in a large corpus. Based on those\nannotations, we trained novel prominence-aware ASR systems that simultaneously\ntranscribe words and their prominence levels. The integration of prominence\ninformation did not change performance compared to our baseline ASR system,\nwhile reaching a prominence detection accuracy of 85.53% for utterances where\nthe recognized word sequence was correct. This paper shows that\ntransformer-based models can effectively encode prosodic information and\nrepresents a novel contribution to prosody-enhanced ASR, with potential\napplications for linguistic research and prosody-informed dialogue systems.", "AI": {"tldr": "本文研究了奥地利德语的重音感知自动语音识别（ASR），通过微调wav2vec2模型开发重音检测器，并将其集成到ASR系统中。结果显示，该方法能有效编码韵律信息并实现高重音检测准确率，但未提升ASR性能。", "motivation": "研究重音感知的ASR，以期在语言学研究和韵律感知对话系统中获得潜在应用。", "method": "首先，通过微调wav2vec2模型开发了词级重音检测器。然后，使用该检测器自动标注了大型语料库中的韵律重音。最后，基于这些标注训练了新的重音感知ASR系统，该系统能同时转录词语及其重音级别。", "result": "在识别词序列正确的语料中，重音检测准确率达到85.53%。与基线ASR系统相比，重音信息的整合并未改变ASR性能。研究表明，基于Transformer的模型可以有效编码韵律信息。", "conclusion": "该研究对韵律增强ASR做出了新颖贡献，表明Transformer模型能有效编码韵律信息，并为语言学研究和韵律感知对话系统提供了潜在应用。尽管ASR性能未提升，但重音检测的准确性及其集成方法仍具价值。"}}
{"id": "2509.10334", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10334", "abs": "https://arxiv.org/abs/2509.10334", "authors": ["Jordan Sassoon", "Michal Szczepanski", "Martyna Poreba"], "title": "I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation", "comment": null, "summary": "Vision Transformers (ViTs) have recently achieved strong results in semantic\nsegmentation, yet their deployment on resource-constrained devices remains\nlimited due to their high memory footprint and computational cost. Quantization\noffers an effective strategy to improve efficiency, but ViT-based segmentation\nmodels are notoriously fragile under low precision, as quantization errors\naccumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the\nfirst fully integer-only ViT segmentation framework. Building on the Segmenter\narchitecture, I-Segmenter systematically replaces floating-point operations\nwith integer-only counterparts. To further stabilize both training and\ninference, we propose $\\lambda$-ShiftGELU, a novel activation function that\nmitigates the limitations of uniform quantization in handling long-tailed\nactivation distributions. In addition, we remove the L2 normalization layer and\nreplace bilinear interpolation in the decoder with nearest neighbor upsampling,\nensuring integer-only execution throughout the computational graph. Extensive\nexperiments show that I-Segmenter achieves accuracy within a reasonable margin\nof its FP32 baseline (5.1 % on average), while reducing model size by up to\n3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,\neven in one-shot PTQ with a single calibration image, I-Segmenter delivers\ncompetitive accuracy, underscoring its practicality for real-world deployment.", "AI": {"tldr": "ViT分割模型在资源受限设备上部署困难，I-Segmenter是首个全整数ViT分割框架，通过系统地替换浮点运算、引入新型激活函数和优化层，显著降低了模型大小和计算成本，同时保持了合理的精度。", "motivation": "Vision Transformers (ViTs) 在语义分割中表现出色，但其高内存占用和计算成本限制了它们在资源受限设备上的部署。量化是提高效率的有效策略，但ViT分割模型在低精度下表现脆弱，因为量化误差会在深层编解码器管道中累积。", "method": "本文引入了I-Segmenter，这是首个全整数ViT分割框架。该方法基于Segmenter架构，系统地将浮点运算替换为全整数运算。为稳定训练和推理，提出了一种新型激活函数 $\\lambda$-ShiftGELU，以缓解均匀量化在处理长尾激活分布时的局限性。此外，移除了L2归一化层，并将解码器中的双线性插值替换为最近邻上采样，以确保整个计算图的全整数执行。", "result": "I-Segmenter在精度上与FP32基线模型相比，平均下降5.1%，但在可接受范围内。模型大小减少了高达3.8倍，通过优化的运行时，推理速度提高了1.2倍。值得注意的是，即使在单次PTQ（Post-Training Quantization）使用单个校准图像的情况下，I-Segmenter也能提供有竞争力的精度。", "conclusion": "I-Segmenter作为首个全整数ViT分割框架，成功解决了ViT分割模型在资源受限设备上部署的挑战。通过全面的整数化策略和创新方法，它在保持合理精度的同时，显著提升了模型效率和在实际部署中的实用性。"}}
{"id": "2509.10156", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10156", "abs": "https://arxiv.org/abs/2509.10156", "authors": ["Goker Erdogan", "Nikhil Parthasarathy", "Catalin Ionescu", "Drew Hudson", "Alexander Lerchner", "Andrew Zisserman", "Mehdi Sajjadi", "Joao Carreira"], "title": "LayerLock: Non-collapsing Representation Learning with Progressive Freezing", "comment": "ICCV 2025", "summary": "We introduce LayerLock, a simple yet effective approach for self-supervised\nvisual representation learning, that gradually transitions from pixel to latent\nprediction through progressive layer freezing. First, we make the observation\nthat during training of video masked-autoencoding (MAE) models, ViT layers\nconverge in the order of their depth: shallower layers converge early, deeper\nlayers converge late. We then show that this observation can be exploited to\naccelerate standard MAE by progressively freezing the model according to an\nexplicit schedule, throughout training. Furthermore, this same schedule can be\nused in a simple and scalable approach to latent prediction that does not\nsuffer from \"representation collapse\". We apply our proposed approach,\nLayerLock, to large models of up to 4B parameters with results surpassing those\nof non-latent masked prediction on the 4DS perception suite.", "AI": {"tldr": "LayerLock是一种通过渐进式层冻结，从像素预测过渡到潜在预测的自监督视觉表示学习方法，它能加速MAE训练并避免表示崩溃。", "motivation": "研究发现，在视频掩码自编码器（MAE）模型训练中，ViT层收敛顺序与其深度相关：浅层先收敛，深层后收敛。这一发现启发了利用层收敛顺序来改进训练过程。", "method": "LayerLock方法在训练过程中根据预设的时间表，逐步冻结模型层。同时，该时间表也被用于潜在预测，以避免“表示崩溃”问题。", "result": "LayerLock方法能够加速标准MAE模型的训练。在4DS感知套件上，使用LayerLock训练的4B参数大型模型，其结果超越了非潜在掩码预测方法。", "conclusion": "LayerLock是一种简单而有效的自监督视觉表示学习方法，通过渐进式层冻结，不仅能加速MAE训练，还能以可扩展的方式实现稳定的潜在预测，避免表示崩溃，并取得了优异的性能。"}}
{"id": "2509.10129", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.10129", "abs": "https://arxiv.org/abs/2509.10129", "authors": ["Alessio Chen", "Simone Giovannini", "Andrea Gemelli", "Fabio Coppini", "Simone Marinai"], "title": "Towards Reliable and Interpretable Document Question Answering via VLMs", "comment": null, "summary": "Vision-Language Models (VLMs) have shown strong capabilities in document\nunderstanding, particularly in identifying and extracting textual information\nfrom complex documents. Despite this, accurately localizing answers within\ndocuments remains a major challenge, limiting both interpretability and\nreal-world applicability. To address this, we introduce\n\\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that\ndecouples answer generation from spatial localization. This design makes it\napplicable to existing VLMs, including proprietary systems where fine-tuning is\nnot feasible. Through systematic evaluation, we provide quantitative insights\ninto the gap between textual accuracy and spatial grounding, showing that\ncorrect answers often lack reliable localization. Our standardized framework\nhighlights these shortcomings and establishes a benchmark for future research\ntoward more interpretable and robust document information extraction VLMs.", "AI": {"tldr": "本文提出DocExplainerV0，一个即插即用的边界框预测模块，旨在解决视觉语言模型（VLMs）在文档理解中答案空间定位不准确的问题，从而提高可解释性和实际应用性。", "motivation": "尽管VLMs在文档理解和文本信息提取方面表现出色，但它们在文档中准确地定位答案（即空间接地）仍然是一个重大挑战，这限制了模型的可解释性和实际应用价值。", "method": "研究人员引入了DocExplainerV0，这是一个即插即用的边界框预测模块。它的设计理念是将答案生成与空间定位解耦，使其能够应用于现有的VLM，包括那些无法进行微调的专有系统。", "result": "通过系统评估，研究发现文本准确性与空间接地之间存在显著差距，即模型给出的正确答案往往缺乏可靠的定位。所建立的标准化框架突出了这些不足。", "conclusion": "DocExplainerV0模块解决了VLM在文档理解中答案定位的难题，并提供了关于文本准确性和空间接地之间差距的量化见解。该研究建立了一个基准，为未来开发更具可解释性和鲁棒性的文档信息提取VLM指明了方向。"}}
{"id": "2509.10344", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10344", "abs": "https://arxiv.org/abs/2509.10344", "authors": ["Yuexi Du", "Lihui Chen", "Nicha C. Dvornek"], "title": "GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography", "comment": "Accepted by MICCAI 2025", "summary": "Mammography screening is an essential tool for early detection of breast\ncancer. The speed and accuracy of mammography interpretation have the potential\nto be improved with deep learning methods. However, the development of a\nfoundation visual language model (VLM) is hindered by limited data and domain\ndifferences between natural and medical images. Existing mammography VLMs,\nadapted from natural images, often ignore domain-specific characteristics, such\nas multi-view relationships in mammography. Unlike radiologists who analyze\nboth views together to process ipsilateral correspondence, current methods\ntreat them as independent images or do not properly model the multi-view\ncorrespondence learning, losing critical geometric context and resulting in\nsuboptimal prediction. We propose GLAM: Global and Local Alignment for\nMulti-view mammography for VLM pretraining using geometry guidance. By\nleveraging the prior knowledge about the multi-view imaging process of\nmammograms, our model learns local cross-view alignments and fine-grained local\nfeatures through joint global and local, visual-visual, and visual-language\ncontrastive learning. Pretrained on EMBED [14], one of the largest open\nmammography datasets, our model outperforms baselines across multiple datasets\nunder different settings.", "AI": {"tldr": "本文提出GLAM模型，一个用于乳腺钼靶多视图视觉语言模型预训练的新方法，通过几何指导进行全局和局部对齐，解决了现有模型忽视多视图关系的问题，显著提高了乳腺癌筛查的准确性。", "motivation": "乳腺钼靶筛查的解读速度和准确性可通过深度学习提升，但现有视觉语言模型（VLM）受限于数据、自然图像与医学图像的领域差异，且未能有效处理乳腺钼靶的多视图关系（如将不同视图视为独立图像），导致关键几何上下文丢失和次优预测。", "method": "提出GLAM模型，通过几何指导进行多视图乳腺钼靶VLM预训练。该模型利用多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉以及视觉-语言对比学习，学习局部跨视图对齐和细粒度局部特征。", "result": "GLAM模型在最大的开放乳腺钼靶数据集EMBED上进行预训练后，在多种数据集和不同设置下均优于现有基线模型。", "conclusion": "GLAM通过有效整合多视图几何信息和对比学习，显著提升了乳腺钼靶视觉语言模型的性能，有望改进乳腺癌的早期检测。"}}
{"id": "2509.10241", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10241", "abs": "https://arxiv.org/abs/2509.10241", "authors": ["Elias De Smijter", "Renaud Detry", "Christophe De Vleeschouwer"], "title": "On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints", "comment": "9 pages, 3 figures, to be presented at ASTRA25,", "summary": "We present the first systematic comparison of implicit and explicit Novel\nView Synthesis methods for space-based 3D object reconstruction, evaluating the\nrole of appearance embeddings. While embeddings improve photometric fidelity by\nmodeling lighting variation, we show they do not translate into meaningful\ngains in geometric accuracy - a critical requirement for space robotics\napplications. Using the SPEED+ dataset, we compare K-Planes, Gaussian\nSplatting, and Convex Splatting, and demonstrate that embeddings primarily\nreduce the number of primitives needed for explicit methods rather than\nenhancing geometric fidelity. Moreover, convex splatting achieves more compact\nand clutter-free representations than Gaussian splatting, offering advantages\nfor safety-critical applications such as interaction and collision avoidance.\nOur findings clarify the limits of appearance embeddings for geometry-centric\ntasks and highlight trade-offs between reconstruction quality and\nrepresentation efficiency in space scenarios.", "AI": {"tldr": "本文首次系统比较了用于空间三维物体重建的隐式和显式新视角合成（NVS）方法，并评估了外观嵌入的作用，发现外观嵌入主要提高光度保真度而非几何精度，且凸散列比高斯散列更紧凑。", "motivation": "三维物体重建对于空间机器人应用至关重要，需要深入理解外观嵌入在提高几何精度方面的实际效果，以满足空间机器人对高几何精度的关键需求。", "method": "本文在SPEED+数据集上，系统比较了K-Planes（隐式）、Gaussian Splatting（显式）和Convex Splatting（显式）三种NVS方法，并评估了外观嵌入对这些方法性能的影响。", "result": "研究发现，外观嵌入能提高光度保真度（建模光照变化），但并未显著提升几何精度。对于显式方法，外观嵌入主要减少了所需基元的数量，而非增强几何保真度。此外，凸散列（Convex Splatting）比高斯散列（Gaussian Splatting）实现了更紧凑、更简洁的表示。", "conclusion": "研究结果明确了外观嵌入在以几何为中心任务中的局限性，并强调了在空间场景中重建质量与表示效率之间的权衡，这对安全关键应用（如交互和避碰）具有重要意义。"}}
{"id": "2509.10184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10184", "abs": "https://arxiv.org/abs/2509.10184", "authors": ["Leen Almajed", "Abeer ALdayel"], "title": "Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations", "comment": "This paper is under review", "summary": "In emotionally supportive conversations, well-intended positivity can\nsometimes misfire, leading to responses that feel dismissive, minimizing, or\nunrealistically optimistic. We examine this phenomenon of incongruent\npositivity as miscalibrated expressions of positive support in both human and\nLLM generated responses. To this end, we collected real user-assistant\ndialogues from Reddit across a range of emotional intensities and generated\nadditional responses using large language models for the same context. We\ncategorize these conversations by intensity into two levels: Mild, which covers\nrelationship tension and general advice, and Severe, which covers grief and\nanxiety conversations. This level of categorization enables a comparative\nanalysis of how supportive responses vary across lower and higher stakes\ncontexts. Our analysis reveals that LLMs are more prone to unrealistic\npositivity through dismissive and minimizing tone, particularly in high-stakes\ncontexts. To further study the underlying dimensions of this phenomenon, we\nfinetune LLMs on datasets with strong and weak emotional reactions. Moreover,\nwe developed a weakly supervised multilabel classifier ensemble (DeBERTa and\nMentalBERT) that shows improved detection of incongruent positivity types\nacross two sorts of concerns (Mild and Severe). Our findings shed light on the\nneed to move beyond merely generating generic positive responses and instead\nstudy the congruent support measures to balance positive affect with emotional\nacknowledgment. This approach offers insights into aligning large language\nmodels with affective expectations in the online supportive dialogue, paving\nthe way toward context-aware and trust preserving online conversation systems.", "AI": {"tldr": "本文研究了在情感支持对话中，人类和LLM生成的不恰当积极回应（如轻视、最小化或不切实际的乐观）现象。通过分析Reddit对话和LLM生成的回应，发现LLM在高风险情境下更容易出现这种问题。研究还开发了检测该现象的分类器，强调了平衡积极情绪与情感认可的重要性。", "motivation": "在情感支持对话中，善意的积极回应有时会适得其反，导致被支持者感到被轻视、被最小化或过于乐观，尤其是在大型语言模型（LLM）生成的回应中，这种不协调的积极性（incongruent positivity）可能更普遍。研究旨在理解和解决这一问题，以构建更具情境感知和信任度的在线对话系统。", "method": "研究收集了Reddit上真实的用户-助手对话，并使用LLM为相同情境生成了额外回应。对话按情感强度分为“轻微”（关系紧张、一般建议）和“严重”（悲伤、焦虑）两类。为深入研究，还在具有强烈和微弱情感反应的数据集上对LLM进行了微调。此外，开发了一个弱监督多标签分类器集成（DeBERTa和MentalBERT）来检测不协调的积极性类型。", "result": "分析显示，LLM更容易表现出不切实际的积极性，通过轻视和最小化的语气，尤其是在高风险情境中。开发的弱监督多标签分类器集成（DeBERTa和MentalBERT）在检测“轻微”和“严重”两种关注类型中的不协调积极性方面表现出改进。", "conclusion": "研究结果表明，需要超越仅仅生成通用的积极回应，转而研究如何提供协调一致的支持，以平衡积极情感与情感认可。这种方法为使大型语言模型与在线支持对话中的情感预期保持一致提供了见解，为构建情境感知和信任维护的在线对话系统铺平了道路。"}}
{"id": "2509.10345", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10345", "abs": "https://arxiv.org/abs/2509.10345", "authors": ["Georgios Pantazopoulos", "Eda B. Özyiğit"], "title": "Towards Understanding Visual Grounding in Visual Language Models", "comment": null, "summary": "Visual grounding refers to the ability of a model to identify a region within\nsome visual input that matches a textual description. Consequently, a model\nequipped with visual grounding capabilities can target a wide range of\napplications in various domains, including referring expression comprehension,\nanswering questions pertinent to fine-grained details in images or videos,\ncaption visual context by explicitly referring to entities, as well as low and\nhigh-level control in simulated and real environments. In this survey paper, we\nreview representative works across the key areas of research on modern\ngeneral-purpose vision language models (VLMs). We first outline the importance\nof grounding in VLMs, then delineate the core components of the contemporary\nparadigm for developing grounded models, and examine their practical\napplications, including benchmarks and evaluation metrics for grounded\nmultimodal generation. We also discuss the multifaceted interrelations among\nvisual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,\nwe analyse the challenges inherent to visual grounding and suggest promising\ndirections for future research.", "AI": {"tldr": "这篇综述论文审查了现代通用视觉语言模型（VLMs）中视觉定位（Visual Grounding）的关键研究领域，涵盖其重要性、核心组成、实际应用、评估指标、与多模态思维链和推理的关系，并分析了挑战和未来方向。", "motivation": "视觉定位能力使模型能够识别视觉输入中与文本描述匹配的区域，从而支持广泛的应用，如指代表达理解、细粒度问答、实体引用图像描述以及模拟和真实环境中的控制。", "method": "本文采用综述方法，回顾了现代通用视觉语言模型（VLMs）中视觉定位的代表性工作。具体包括：概述视觉定位在VLM中的重要性、描绘开发定位模型的核心组件、审视其实际应用（包括基准和评估指标）、讨论视觉定位、多模态思维链和VLM中推理之间的多方面相互关系。", "result": "论文提供了对视觉定位在VLM中全面审视，包括其核心范式、广泛的应用场景、用于多模态生成的基准和评估指标，以及其与多模态思维链和推理的复杂关联。", "conclusion": "论文分析了视觉定位固有的挑战，并提出了未来研究的有前景方向。"}}
{"id": "2509.10250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10250", "abs": "https://arxiv.org/abs/2509.10250", "authors": ["Haozhen Yan", "Yan Hong", "Suning Lang", "Jiahui Zhan", "Yikun Ji", "Yujie Gao", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Jianfu Zhang"], "title": "GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection", "comment": "11 pages, 5 figures", "summary": "With generative models becoming increasingly sophisticated and diverse,\ndetecting AI-generated images has become increasingly challenging. While\nexisting AI-genereted Image detectors achieve promising performance on\nin-distribution generated images, their generalization to unseen generative\nmodels remains limited. This limitation is largely attributed to their reliance\non generation-specific artifacts, such as stylistic priors and compression\npatterns. To address these limitations, we propose GAMMA, a novel training\nframework designed to reduce domain bias and enhance semantic alignment. GAMMA\nintroduces diverse manipulation strategies, such as inpainting-based\nmanipulation and semantics-preserving perturbations, to ensure consistency\nbetween manipulated and authentic content. We employ multi-task supervision\nwith dual segmentation heads and a classification head, enabling pixel-level\nsource attribution across diverse generative domains. In addition, a reverse\ncross-attention mechanism is introduced to allow the segmentation heads to\nguide and correct biased representations in the classification branch. Our\nmethod achieves state-of-the-art generalization performance on the GenImage\nbenchmark, imporving accuracy by 5.8%, but also maintains strong robustness on\nnewly released generative model such as GPT-4o.", "AI": {"tldr": "现有AI生成图像检测器在泛化到未见过模型时表现不佳，因其依赖特定生成伪影。本文提出GAMMA框架，通过多样化操作、多任务监督和逆向交叉注意力机制，减少领域偏差并增强语义对齐，显著提升了泛化性能和鲁棒性。", "motivation": "现有AI生成图像检测器在处理分布内图像时表现良好，但对未见过的生成模型泛化能力有限。这主要是因为它们过度依赖生成模型特有的伪影，如风格先验和压缩模式。", "method": "本文提出GAMMA训练框架，旨在减少领域偏差并增强语义对齐。主要方法包括：1) 引入多样化操作策略（如基于修复的操作和语义保留扰动），以确保操作内容与真实内容之间的一致性。2) 采用带有双分割头和分类头的多任务监督，实现跨不同生成域的像素级源归因。3) 引入逆向交叉注意力机制，使分割头能够引导和纠正分类分支中存在的偏差表示。", "result": "GAMMA方法在GenImage基准测试上取得了最先进的泛化性能，准确率提高了5.8%。此外，该方法对GPT-4o等新发布的生成模型也保持了强大的鲁棒性。", "conclusion": "通过引入多样化操作策略、多任务监督和逆向交叉注意力机制，GAMMA框架有效解决了AI生成图像检测器在泛化到未见模型时的局限性，显著提升了检测的准确性和对新模型的鲁棒性。"}}
{"id": "2509.10199", "categories": ["cs.CL", "I.7; I.2; J.4"], "pdf": "https://arxiv.org/pdf/2509.10199", "abs": "https://arxiv.org/abs/2509.10199", "authors": ["Miklós Sebők", "Viktor Kovács", "Martin Bánóczy", "Daniel Møller Eriksen", "Nathalie Neptune", "Philippe Roussille"], "title": "Beyond Token Limits: Assessing Language Model Performance on Long Text Classification", "comment": null, "summary": "The most widely used large language models in the social sciences (such as\nBERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text\nlength that they can process to produce predictions. This is a particularly\npressing issue for some classification tasks, where the aim is to handle long\ninput texts. One such area deals with laws and draft laws (bills), which can\nhave a length of multiple hundred pages and, therefore, are not particularly\namenable for processing with models that can only handle e.g. 512 tokens. In\nthis paper, we show results from experiments covering 5 languages with\nXLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass\nclassification task of the Comparative Agendas Project, which has a codebook of\n21 policy topic labels from education to health care. Results show no\nparticular advantage for the Longformer model, pre-trained specifically for the\npurposes of handling long inputs. The comparison between the GPT variants and\nthe best-performing open model yielded an edge for the latter. An analysis of\nclass-level factors points to the importance of support and substance overlaps\nbetween specific categories when it comes to performance on long text inputs.", "AI": {"tldr": "本研究评估了多种大型语言模型（包括Longformer、GPT-3.5、GPT-4等）在处理跨语言长文本多类别分类任务时的性能，发现专门为长文本设计的Longformer并无明显优势，而最佳开源模型表现优于GPT变体，并强调了类别支持度和内容重叠对长文本分类性能的重要性。", "motivation": "社会科学领域广泛使用的大型语言模型（如BERT、RoBERTa）在输入文本长度上存在限制（如512个token），这对于需要处理长文本的分类任务（如法律文件，可能长达数百页）是一个紧迫的问题，这些模型无法有效处理。", "method": "研究使用了XLM-RoBERTa、Longformer、GPT-3.5和GPT-4模型，在五种语言上进行了实验。任务是比较议程项目（Comparative Agendas Project）的多类别分类，该项目包含21个政策主题标签（从教育到医疗保健）。", "result": "实验结果显示，专门为处理长输入而预训练的Longformer模型没有表现出特别的优势。GPT变体与表现最佳的开源模型相比，后者略胜一筹。对类别层面因素的分析表明，在长文本输入上，特定类别之间的支持度（support）和实质内容重叠（substance overlaps）对性能至关重要。", "conclusion": "对于长文本分类任务，专门为长输入设计的模型（如Longformer）不一定比其他模型有优势。最佳开源模型在某些情况下可能优于GPT变体。在处理长文本时，类别之间的支持度和内容重叠是影响分类性能的关键因素。"}}
{"id": "2509.10408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.10408", "abs": "https://arxiv.org/abs/2509.10408", "authors": ["Iacopo Curti", "Pierluigi Zama Ramirez", "Alioscia Petrelli", "Luigi Di Stefano"], "title": "Multimodal SAM-adapter for Semantic Segmentation", "comment": null, "summary": "Semantic segmentation, a key task in computer vision with broad applications\nin autonomous driving, medical imaging, and robotics, has advanced\nsubstantially with deep learning. Nevertheless, current approaches remain\nvulnerable to challenging conditions such as poor lighting, occlusions, and\nadverse weather. To address these limitations, multimodal methods that\nintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,\nproviding complementary information that enhances robustness. In this work, we\npresent MM SAM-adapter, a novel framework that extends the capabilities of the\nSegment Anything Model (SAM) for multimodal semantic segmentation. The proposed\nmethod employs an adapter network that injects fused multimodal features into\nSAM's rich RGB features. This design enables the model to retain the strong\ngeneralization ability of RGB features while selectively incorporating\nauxiliary modalities only when they contribute additional cues. As a result, MM\nSAM-adapter achieves a balanced and efficient use of multimodal information. We\nevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,\nwhere MM SAM-adapter delivers state-of-the-art performance. To further analyze\nmodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard\nsubsets. Results consistently demonstrate that our framework outperforms\ncompeting methods in both favorable and adverse conditions, highlighting the\neffectiveness of multimodal adaptation for robust scene understanding. The code\nis available at the following link:\nhttps://github.com/iacopo97/Multimodal-SAM-Adapter.", "AI": {"tldr": "本文提出了MM SAM-adapter框架，通过适配器网络将融合的多模态特征注入到SAM的RGB特征中，从而扩展SAM以实现多模态语义分割，并在挑战性基准测试中取得了最先进的性能。", "motivation": "语义分割在恶劣条件（如光照不足、遮挡、恶劣天气）下表现不佳。多模态方法通过整合辅助传感器数据（如LiDAR、红外）来增强鲁棒性，但需要一种有效的方式来将这些信息与像SAM这样强大的模型结合。", "method": "MM SAM-adapter框架通过一个适配器网络工作，该网络将融合的多模态特征注入到SAM的RGB特征中。这种设计允许模型保留RGB特征强大的泛化能力，同时仅在辅助模态提供额外线索时选择性地整合它们，从而实现多模态信息的平衡和高效利用。", "result": "MM SAM-adapter在DeLiVER、FMB和MUSES三个具有挑战性的基准测试中取得了最先进的性能。在DeLiVER和FMB的RGB-easy和RGB-hard子集中，该框架始终优于竞争方法，证明了其在有利和不利条件下都具有出色的表现。", "conclusion": "MM SAM-adapter通过有效地整合多模态信息，增强了SAM的语义分割能力，特别是在挑战性条件下实现了鲁棒的场景理解。该方法在多个基准测试中表现出最先进的性能，证明了多模态适应在提升场景理解方面的有效性。"}}
{"id": "2509.10257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10257", "abs": "https://arxiv.org/abs/2509.10257", "authors": ["Ema Masterl", "Tina Vipotnik Vesnaver", "Žiga Špiclin"], "title": "Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI", "comment": "Accepted at the PIPPI Workshop of MICCAI 2025", "summary": "Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce\nmotion artifacts caused by fetal movement. However, these stacks are typically\nlow resolution, may suffer from motion corruption, and do not adequately\ncapture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to\naddress these limitations by combining slice-to-volume registration and\nsuper-resolution techniques to generate high-resolution (HR) 3D volumes. While\nseveral SRR methods have been proposed, their comparative performance -\nparticularly in pathological cases - and their influence on downstream\nvolumetric analysis and diagnostic tasks remain underexplored. In this study,\nwe applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to\n140 fetal brain MRI scans, including both healthy controls (HC) and\npathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was\nsegmented using the BoUNTi algorithm to extract volumes of nine principal brain\nstructures. We evaluated visual quality, SRR success rates, volumetric\nmeasurement agreement, and diagnostic classification performance. NeSVoR\ndemonstrated the highest and most consistent reconstruction success rate (>90%)\nacross both HC and PC groups. Although significant differences in volumetric\nestimates were observed between SRR methods, classification performance for VM\nwas not affected by the choice of SRR method. These findings highlight NeSVoR's\nrobustness and the resilience of diagnostic performance despite SRR-induced\nvolumetric variability.", "AI": {"tldr": "本研究比较了三种最先进的胎儿脑部MRI超分辨率重建（SRR）方法（NiftyMIC, SVRTK, NeSVoR），发现在健康和病理病例中，NeSVoR的重建成功率最高且最稳定。尽管不同SRR方法导致体积测量存在显著差异，但诊断分类性能并未受SRR方法选择的影响。", "motivation": "胎儿脑部MRI通常采用快速多视角2D切片采集，以减少胎儿运动引起的伪影，但这些切片分辨率低，可能受运动损坏，且未能充分捕捉3D解剖结构。超分辨率重建（SRR）方法旨在通过结合切片到体积配准和超分辨率技术来生成高分辨率（HR）3D体积，但其比较性能（尤其是在病理病例中）及其对后续体积分析和诊断任务的影响仍未得到充分探索。", "method": "研究将NiftyMIC、SVRTK和NeSVoR这三种最先进的SRR方法应用于140例胎儿脑部MRI扫描，其中包括健康对照组（HC）和伴有脑室扩张（VM）的病理病例组（PC）。每个高分辨率重建体都使用BoUNTi算法进行分割，以提取九个主要脑结构的体积。评估了视觉质量、SRR成功率、体积测量一致性和诊断分类性能。", "result": "NeSVoR在HC和PC两组中均表现出最高且最一致的重建成功率（>90%）。尽管在体积估算方面，不同SRR方法之间存在显著差异，但VM的诊断分类性能并未受SRR方法选择的影响。", "conclusion": "这些发现突出了NeSVoR的鲁棒性，以及尽管SRR方法引起的体积变异性，诊断性能仍具有韧性。"}}
{"id": "2509.10377", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10377", "abs": "https://arxiv.org/abs/2509.10377", "authors": ["Yixiao Zhou", "Ziyu Zhao", "Dongzhou Cheng", "zhiliang wu", "Jie Gui", "Yi Yang", "Fei Wu", "Yu Cheng", "Hehe Fan"], "title": "Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs", "comment": "Accepted to EMNLP2025", "summary": "Sparse Mixture-of-Experts (SMoE) architectures are widely used in large\nlanguage models (LLMs) due to their computational efficiency. However, though\nonly a few experts are activated for each token, SMoE still requires loading\nall expert parameters, leading to high memory usage and challenges in\ndeployment. Previous work has tried to reduce the overhead by pruning and\nmerging experts, but primarily focused on expert-level operations, leaving\nneuron-level structure underexplored. We propose DERN (Dropping Experts,\nRecombining Neurons), a task-agnostic and retraining-free framework for expert\npruning and reconstruction. We observe that experts are often misaligned and\ncontain semantic conflicts at the neuron level, which poses challenges for\ndirect merging. To solve this, DERN works in three steps: it first prunes\nredundant experts using router statistics; then it decomposes them into\nneuron-level expert segments, assigning each segment to its most compatible\nretained expert; and finally, it merges segments within each retained expert to\nbuild a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE\nmodels show that DERN improves performance by more than 5% on commonsense\nreasoning and MMLU benchmarks under 50% expert sparsity, without extra\ntraining. It also greatly reduces the number of experts and memory usage,\nmaking SMoE LLMs easier to deploy in practice.", "AI": {"tldr": "稀疏专家混合模型（SMoE）在大型语言模型中因内存占用高而部署困难。DERN提出了一种无需再训练的框架，通过修剪冗余专家并在神经元层面重组，显著减少了内存占用，并在多项基准测试中提升了性能。", "motivation": "SMoE架构在大型语言模型中因计算效率高而被广泛使用，但其需要加载所有专家参数，导致内存占用高和部署困难。现有方法主要集中在专家层面的操作，对神经元层面的结构探索不足。", "method": "DERN（Dropping Experts, Recombining Neurons）是一个任务无关、无需再训练的专家修剪和重建框架，分为三步：1. 利用路由统计数据修剪冗余专家；2. 将修剪后的专家分解为神经元级别的专家片段，并将每个片段分配给最兼容的保留专家；3. 在每个保留专家内部合并这些片段以构建紧凑的表示。", "result": "在Mixtral、Qwen和DeepSeek SMoE模型上，DERN在50%专家稀疏度下，无需额外训练，将常识推理和MMLU基准测试的性能提高了5%以上。它还大大减少了专家数量和内存使用，使SMoE大型语言模型在实践中更容易部署。", "conclusion": "DERN通过修剪冗余专家和智能重组神经元，有效解决了SMoE大型语言模型的高内存占用和部署挑战，在不进行额外训练的情况下，实现了性能提升和效率优化。"}}
{"id": "2509.10414", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10414", "abs": "https://arxiv.org/abs/2509.10414", "authors": ["Adrian de Wynter"], "title": "Is In-Context Learning Learning?", "comment": "Director's cut", "summary": "In-context learning (ICL) allows some autoregressive models to solve tasks\nvia next-token prediction and without needing further training. This has led to\nclaims about these model's ability to solve (learn) unseen tasks with only a\nfew shots (exemplars) in the prompt. However, deduction does not always imply\nlearning, as ICL does not explicitly encode a given observation. Instead, the\nmodels rely on their prior knowledge and the exemplars given, if any. We argue\nthat, mathematically, ICL does constitute learning, but its full\ncharacterisation requires empirical work. We then carry out a large-scale\nanalysis of ICL ablating out or accounting for memorisation, pretraining,\ndistributional shifts, and prompting style and phrasing. We find that ICL is an\neffective learning paradigm, but limited in its ability to learn and generalise\nto unseen tasks. We note that, in the limit where exemplars become more\nnumerous, accuracy is insensitive to exemplar distribution, model, prompt\nstyle, and the input's linguistic features. Instead, it deduces patterns from\nregularities in the prompt, which leads to distributional sensitivity,\nespecially in prompting styles such as chain-of-thought. Given the varied\naccuracies on formally similar tasks, we conclude that autoregression's ad-hoc\nencoding is not a robust mechanism, and suggests limited all-purpose\ngeneralisability.", "AI": {"tldr": "本文认为上下文学习（ICL）在数学上构成学习，但通过大规模实证分析发现其在学习和泛化到未见任务方面存在局限性，且其效果更多依赖于提示中的规律性而非鲁棒的编码机制。", "motivation": "自回归模型通过上下文学习（ICL）无需额外训练即可解决任务，被宣称能以少量示例学习未见任务。然而，ICL不显式编码观测值，其能力是“学习”还是“推导”存在争议，这促使作者对ICL的本质及其泛化能力进行深入探究。", "method": "首先，从数学角度论证ICL构成学习。然后，进行大规模实证分析，通过消融或考虑记忆、预训练、分布偏移以及提示风格和措辞等因素，全面评估ICL的表现。", "result": "研究发现ICL是一种有效的学习范式，但在学习和泛化到未见任务方面存在局限性。当示例数量充足时，准确性对示例分布、模型、提示风格和输入语言特征不敏感，而是从提示中的规律性推导模式。这导致了对分布的敏感性，尤其是在思维链等提示风格中。此外，在形式相似的任务上表现出不同的准确性。", "conclusion": "自回归模型的即兴编码机制不够鲁棒，表明其通用泛化能力有限。尽管ICL在数学上是学习，但其在泛化到未见任务方面的能力受限，且其有效性高度依赖于提示中的规律性，而非普适的鲁棒机制。"}}
{"id": "2509.10259", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10259", "abs": "https://arxiv.org/abs/2509.10259", "authors": ["Hua Yuan", "Jin Yuan", "Yicheng Jiang", "Yao Zhang", "Xin Geng", "Yong Rui"], "title": "Mask Consistency Regularization in Object Removal", "comment": null, "summary": "Object removal, a challenging task within image inpainting, involves\nseamlessly filling the removed region with content that matches the surrounding\ncontext. Despite advancements in diffusion models, current methods still face\ntwo critical challenges. The first is mask hallucination, where the model\ngenerates irrelevant or spurious content inside the masked region, and the\nsecond is mask-shape bias, where the model fills the masked area with an object\nthat mimics the mask's shape rather than surrounding content. To address these\nissues, we propose Mask Consistency Regularization (MCR), a novel training\nstrategy designed specifically for object removal tasks. During training, our\napproach introduces two mask perturbations: dilation and reshape, enforcing\nconsistency between the outputs of these perturbed branches and the original\nmask. The dilated masks help align the model's output with the surrounding\ncontent, while reshaped masks encourage the model to break the mask-shape bias.\nThis combination of strategies enables MCR to produce more robust and\ncontextually coherent inpainting results. Our experiments demonstrate that MCR\nsignificantly reduces hallucinations and mask-shape bias, leading to improved\nperformance in object removal.", "AI": {"tldr": "本文提出了一种名为MCR（Mask Consistency Regularization）的新训练策略，通过引入掩码扰动来解决图像修复中物体移除任务中存在的掩码幻觉和掩码形状偏差问题。", "motivation": "尽管扩散模型有所进步，但当前的物体移除方法仍面临两个关键挑战：一是掩码幻觉（模型在掩码区域生成不相关或虚假内容），二是掩码形状偏差（模型生成与掩码形状相似而非与周围内容匹配的物体）。", "method": "本文提出Mask Consistency Regularization (MCR) 训练策略。在训练过程中，引入两种掩码扰动：膨胀（dilation）和重塑（reshape）。膨胀掩码有助于使模型输出与周围内容对齐，而重塑掩码则鼓励模型打破掩码形状偏差。MCR通过强制这些扰动分支的输出与原始掩码之间保持一致性来工作。", "result": "实验表明，MCR显著减少了幻觉和掩码形状偏差，从而提高了物体移除的性能。", "conclusion": "MCR策略能够生成更鲁棒、上下文更连贯的图像修复结果，有效解决了物体移除任务中的掩码幻觉和掩码形状偏差问题。"}}
{"id": "2509.10417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10417", "abs": "https://arxiv.org/abs/2509.10417", "authors": ["Christopher Ormerod", "Gitit Kehat"], "title": "Long Context Automated Essay Scoring with Language Models", "comment": "8 pages, 2 figures, 2 tables", "summary": "Transformer-based language models are architecturally constrained to process\ntext of a fixed maximum length. Essays written by higher-grade students\nfrequently exceed the maximum allowed length for many popular open-source\nmodels. A common approach to addressing this issue when using these models for\nAutomated Essay Scoring is to truncate the input text. This raises serious\nvalidity concerns as it undermines the model's ability to fully capture and\nevaluate organizational elements of the scoring rubric, which requires long\ncontexts to assess. In this study, we evaluate several models that incorporate\narchitectural modifications of the standard transformer architecture to\novercome these length limitations using the Kaggle ASAP 2.0 dataset. The models\nconsidered in this study include fine-tuned versions of XLNet, Longformer,\nModernBERT, Mamba, and Llama models.", "AI": {"tldr": "针对Transformer模型处理长文本（如学生作文）时的固定长度限制，本研究评估了多种通过修改架构来克服这一限制的模型，以提高自动作文评分的有效性，避免因截断文本而损害对组织结构元素的评估。", "motivation": "Transformer模型在处理长文本时有固定长度限制，导致在自动作文评分中需要截断输入。这种截断会严重损害模型评估作文评分标准中需要长上下文的组织结构元素的能力，从而引发有效性问题。", "method": "本研究评估了多种对标准Transformer架构进行修改以克服长度限制的模型。这些模型包括经过微调的XLNet、Longformer、ModernBERT、Mamba和Llama模型。研究使用了Kaggle ASAP 2.0数据集进行评估。", "result": "抽象中未提供具体的实验结果。", "conclusion": "抽象中未提供具体的结论。"}}
{"id": "2509.10260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10260", "abs": "https://arxiv.org/abs/2509.10260", "authors": ["Jia Wang", "Jie Hu", "Xiaoqi Ma", "Hanghang Ma", "Yanbing Zeng", "Xiaoming Wei"], "title": "MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation", "comment": null, "summary": "Text-to-image (T2I) generation has achieved remarkable progress in\ninstruction following and aesthetics. However, a persistent challenge is the\nprevalence of physical artifacts, such as anatomical and structural flaws,\nwhich severely degrade perceptual quality and limit application. Given the\ndiversity and complexity of these artifacts, a systematic and fine-grained\nevaluation framework is required, which is lacking in current benchmarks. To\nfill this gap, we introduce MagicMirror, a comprehensive framework for\nartifacts assessment. We first establish a detailed taxonomy of generated image\nartifacts. Guided by this taxonomy, we manually annotate MagicData340K, the\nfirst human-annotated large-scale dataset of 340K generated images with\nfine-grained artifact labels. Building on this dataset, we train MagicAssessor,\na Vision-Language Model (VLM) that provides detailed assessments and\ncorresponding labels. To overcome challenges like class imbalance and reward\nhacking, we design a novel data sampling strategy and a multi-level reward\nsystem for Group Relative Policy Optimization (GRPO). Finally, we leverage\nMagicAssessor to construct MagicBench, an automated benchmark for evaluating\nthe image artifacts of current T2I models. Our evaluation with MagicBench\nreveals that despite their widespread adoption, even top-tier models like\nGPT-image-1 are consistently plagued by significant artifacts, highlighting\nartifact reduction as a critical frontier for future T2I development. Project\npage: https://wj-inf.github.io/MagicMirror-page/.", "AI": {"tldr": "本文介绍了MagicMirror，一个用于评估文本到图像（T2I）生成模型中物理伪影的综合框架。该框架包括一个伪影分类法、一个大规模人工标注数据集MagicData340K、一个视觉-语言模型（VLM）评估器MagicAssessor，以及一个自动化基准MagicBench，揭示了顶级T2I模型中普遍存在的伪影问题。", "motivation": "尽管文本到图像（T2I）生成在指令遵循和美学方面取得了显著进展，但仍普遍存在物理伪影（如解剖和结构缺陷），严重降低了感知质量并限制了应用。现有基准缺乏系统化、细粒度的伪影评估框架来应对这些多样且复杂的伪影。", "method": "研究首先建立了一个详细的生成图像伪影分类法。以此为指导，人工标注了MagicData340K，这是首个包含34万张生成图像及细粒度伪影标签的大规模数据集。在此基础上，训练了一个视觉-语言模型（VLM）MagicAssessor，用于提供详细评估和相应标签。为解决类别不平衡和奖励作弊等挑战，设计了一种新颖的数据采样策略和用于群组相对策略优化（GRPO）的多级奖励系统。最后，利用MagicAssessor构建了MagicBench，一个用于自动化评估T2I模型图像伪影的基准。", "result": "通过MagicBench进行的评估显示，尽管被广泛采用，即使是像GPT-image-1这样的顶级T2I模型也持续受到显著伪影的困扰。", "conclusion": "伪影减少是未来T2I发展的一个关键前沿领域。"}}
{"id": "2509.10436", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10436", "abs": "https://arxiv.org/abs/2509.10436", "authors": ["Shadikur Rahman", "Aroosa Hameed", "Gautam Srivastava", "Syed Muhammad Danish"], "title": "RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment", "comment": "12 pages, 5 figures, submitted to IEEE Transactions on Services\n  Computing", "summary": "To optimize the reasoning and problem-solving capabilities of Large Language\nModels (LLMs), we propose a novel cloud-edge collaborative architecture that\nenables a structured, multi-agent prompting framework. This framework comprises\nthree specialized components: GuideLLM, a lightweight model deployed at the\nedge to provide methodological guidance; SolverLLM, a more powerful model\nhosted in the cloud responsible for generating code solutions; and JudgeLLM, an\nautomated evaluator for assessing solution correctness and quality. To evaluate\nand demonstrate the effectiveness of this architecture in realistic settings,\nwe introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate\nand enhance the performance of Large Language Models (LLMs) across multi-domain\ncoding tasks. Motivated by the limitations of existing benchmarks,\nRefactorCoderQA systematically covers various technical domains, including\nSoftware Engineering, Data Science, Machine Learning, and Natural Language\nProcessing, using authentic coding challenges from Stack Overflow. Extensive\nexperiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves\nstate-of-the-art performance, significantly outperforming leading open-source\nand commercial baselines with an overall accuracy of 76.84%. Human evaluations\nfurther validate the interpretability, accuracy, and practical relevance of the\ngenerated solutions. In addition, we evaluate system-level metrics, such as\nthroughput and latency, to gain deeper insights into the performance\ncharacteristics and trade-offs of the proposed architecture.", "AI": {"tldr": "本文提出了一种新颖的云边协同多智能体架构，旨在优化大型语言模型（LLM）的推理和问题解决能力。该架构包含GuideLLM、SolverLLM和JudgeLLM三个组件，并引入了RefactorCoderQA基准。通过实验，RefactorCoder-MoE模型在多领域编码任务上实现了最先进的性能。", "motivation": "研究动机是为了优化大型语言模型（LLM）的推理和问题解决能力，并解决现有基准的局限性。", "method": "本文提出了一种云边协同架构，该架构包含一个结构化的多智能体提示框架：边缘部署的轻量级GuideLLM提供方法论指导；云端强大的SolverLLM负责生成代码解决方案；以及用于评估解决方案正确性和质量的自动化JudgeLLM。此外，引入了RefactorCoderQA基准，用于评估和增强LLM在软件工程、数据科学、机器学习和自然语言处理等多领域编码任务中的性能。通过对RefactorCoder-MoE模型进行微调，并评估系统级指标（如吞吐量和延迟）来深入了解架构的性能特征。", "result": "微调后的RefactorCoder-MoE模型实现了76.84%的总体准确率，显著超越了领先的开源和商业基线，达到了最先进的性能。人工评估进一步验证了生成解决方案的可解释性、准确性和实用相关性。系统级指标评估提供了对所提出架构性能特征和权衡的深入见解。", "conclusion": "所提出的云边协同多智能体架构及其RefactorCoder-MoE模型有效增强了LLM在编码任务中的推理和问题解决能力，实现了最先进的性能，并具有实际相关性。"}}
{"id": "2509.10278", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10278", "abs": "https://arxiv.org/abs/2509.10278", "authors": ["Vidit Vidit", "Pavel Korshunov", "Amir Mohammadi", "Christophe Ecabert", "Ketan Kotwal", "Sébastien Marcel"], "title": "Detecting Text Manipulation in Images using Vision Language Models", "comment": "Accepted in Synthetic Realities and Biometric Security Workshop\n  BMVC-2025. For paper page see https://www.idiap.ch/paper/textvlmdet/", "summary": "Recent works have shown the effectiveness of Large Vision Language Models\n(VLMs or LVLMs) in image manipulation detection. However, text manipulation\ndetection is largely missing in these studies. We bridge this knowledge gap by\nanalyzing closed- and open-source VLMs on different text manipulation datasets.\nOur results suggest that open-source models are getting closer, but still\nbehind closed-source ones like GPT- 4o. Additionally, we benchmark image\nmanipulation detection-specific VLMs for text manipulation detection and show\nthat they suffer from the generalization problem. We benchmark VLMs for\nmanipulations done on in-the-wild scene texts and on fantasy ID cards, where\nthe latter mimic a challenging real-world misuse.", "AI": {"tldr": "本文分析了大型视觉语言模型（VLMs）在文本篡改检测方面的表现，比较了开源和闭源模型，并评估了图像篡改检测专用模型的泛化能力。", "motivation": "现有研究主要关注VLMs在图像篡改检测方面的有效性，而对文本篡改检测的探讨不足，存在知识空白。", "method": "研究分析了闭源和开源VLMs在不同文本篡改数据集上的表现。同时，评估了专门用于图像篡改检测的VLMs在文本篡改检测上的泛化能力。测试场景包括真实世界场景文本和模拟现实世界滥用的虚构身份证件。", "result": "结果显示，开源模型正在进步，但仍落后于GPT-4o等闭源模型。此外，图像篡改检测专用VLMs在文本篡改检测方面存在泛化问题。", "conclusion": "VLMs在文本篡改检测方面有待提升，闭源模型表现更优，而图像篡改专用模型在文本任务上泛化能力不足，凸显了该领域进一步研究的必要性。"}}
{"id": "2509.10446", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.10446", "abs": "https://arxiv.org/abs/2509.10446", "authors": ["Rui Lu", "Zhenyu Hou", "Zihan Wang", "Hanchen Zhang", "Xiao Liu", "Yujiang Li", "Shi Feng", "Jie Tang", "Yuxiao Dong"], "title": "DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL", "comment": null, "summary": "Augmenting large language models (LLMs) with browsing tools substantially\nimproves their potential as deep search agents to solve complex, real-world\ntasks. Yet, open LLMs still perform poorly in such settings due to limited\nlong-horizon reasoning capacity with browsing tools and the lack of\nsufficiently difficult supervised data. To address these challenges, we present\nDeepDive to advance deep search agents. First, we propose a strategy to\nautomatically synthesize complex, difficult, and hard-to-find questions from\nopen knowledge graphs. Second, we apply end-to-end multi-turn reinforcement\nlearning (RL) to enhance LLMs' long-horizon reasoning with deep search.\nExperiments show that DeepDive-32B achieves a new open-source competitive\nresult on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and\nSearch-o1. We demonstrate that multi-turn RL training improves deep search\nability and significantly contributes to the performance improvements across\nmultiple benchmarks. We observe that DeepDive enables test-time scaling of tool\ncalls and parallel sampling. All datasets, models, and code are publicly\navailable at https://github.com/THUDM/DeepDive.", "AI": {"tldr": "DeepDive通过自动生成复杂问题和多轮强化学习，显著提升了大型语言模型作为深度搜索代理的能力，并在BrowseComp等基准测试中取得了领先的开源性能。", "motivation": "开放式大型语言模型（LLMs）在结合浏览工具进行深度搜索时表现不佳，原因在于其长周期推理能力有限，且缺乏足够难度高的监督数据。", "method": ["提出一种策略，从开放知识图谱中自动合成复杂、困难且难以找到的问题。", "应用端到端多轮强化学习（RL）来增强LLMs在深度搜索中的长周期推理能力。"], "result": ["DeepDive-32B在BrowseComp上取得了新的开源竞争性结果，超越了WebSailor、DeepSeek-R1-Browse和Search-o1。", "多轮强化学习训练显著提高了深度搜索能力，并对多个基准测试的性能改进做出了贡献。", "DeepDive支持测试时工具调用和并行采样的扩展。"], "conclusion": "DeepDive通过解决数据稀缺和推理能力限制，极大地推进了深度搜索代理的发展，并展示了通过问题合成和多轮强化学习实现最先进开源性能的有效性。"}}
{"id": "2509.10282", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10282", "abs": "https://arxiv.org/abs/2509.10282", "authors": ["Gang Li", "Tianjiao Chen", "Mingle Zhou", "Min Li", "Delong Han", "Jin Wan"], "title": "MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection", "comment": "Page 14, 5 pictures", "summary": "Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects\nwithout relying on labeled training data, making it especially valuable in\nscenarios constrained by data scarcity, privacy, or high annotation cost.\nHowever, most existing methods focus exclusively on point clouds, neglecting\nthe rich semantic cues available from complementary modalities such as RGB\nimages and texts priors. This paper introduces MCL-AD, a novel framework that\nleverages multimodal collaboration learning across point clouds, RGB images,\nand texts semantics to achieve superior zero-shot 3D anomaly detection.\nSpecifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that\nenhances the intra-modal representation capability and inter-modal\ncollaborative learning by introducing an object-agnostic decoupled text prompt\nand a multimodal contrastive loss. In addition, a collaborative modulation\nmechanism (CMM) is proposed to fully leverage the complementary representations\nof point clouds and RGB images by jointly modulating the RGB image-guided and\npoint cloud-guided branches. Extensive experiments demonstrate that the\nproposed MCL-AD framework achieves state-of-the-art performance in ZS-3D\nanomaly detection.", "AI": {"tldr": "MCL-AD是一种新颖的零样本3D异常检测框架，通过点云、RGB图像和文本语义的多模态协作学习，实现了卓越的检测性能。", "motivation": "现有的零样本3D（ZS-3D）异常检测方法主要关注点云，忽略了RGB图像和文本先验等互补模态中丰富的语义信息。在数据稀缺、隐私或高标注成本的场景中，ZS-3D异常检测具有重要价值。", "method": "本文提出了MCL-AD框架。具体来说，引入了多模态提示学习机制（MPLM），通过解耦的文本提示和多模态对比损失来增强模态内表示能力和模态间协作学习。此外，还提出了协作调制机制（CMM），通过联合调制RGB图像引导和点云引导分支，充分利用点云和RGB图像的互补表示。", "result": "广泛的实验证明，所提出的MCL-AD框架在零样本3D异常检测中取得了最先进的性能。", "conclusion": "MCL-AD通过整合多模态协作学习、多模态提示学习机制和协作调制机制，有效解决了现有ZS-3D异常检测方法忽视多模态语义的问题，显著提升了检测能力。"}}
{"id": "2509.10452", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10452", "abs": "https://arxiv.org/abs/2509.10452", "authors": ["Akshat Pandey", "Karun Kumar", "Raphael Tang"], "title": "WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers", "comment": "5 pages, 2 figures", "summary": "Pretrained automatic speech recognition (ASR) models such as Whisper perform\nwell but still need domain adaptation to handle unseen vocabulary and parlance.\nIn many real-world settings, collecting speech data is impractical,\nnecessitating text-only adaptation. We propose WhisTLE, a deeply supervised,\ntext-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE\ntrains a variational autoencoder (VAE) to model encoder outputs from text and\nfine-tunes the decoder using the learned text-to-latent encoder, optionally\ncombined with text-to-speech (TTS) adaptation. At inference, the original\nencoder is restored, incurring no extra runtime cost. Across four out-of-domain\ndatasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by\n12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines\nin 27 of 32 scenarios.", "AI": {"tldr": "WhisTLE是一种深度监督的纯文本域适应方法，用于预训练的ASR模型，通过训练VAE从文本建模编码器输出并微调解码器，显著降低了域外数据集的词错误率。", "motivation": "预训练的ASR模型（如Whisper）在处理未见词汇和口语时仍需域适应，但在许多实际场景中收集语音数据不切实际，因此需要纯文本的适应方法。", "method": "提出WhisTLE方法，它是一种深度监督的纯文本域适应方法，用于预训练的编码器-解码器ASR模型。该方法训练一个变分自编码器（VAE）来从文本建模编码器输出，并使用学习到的文本到潜在编码器微调解码器，可选地结合文本到语音（TTS）适应。推理时恢复原始编码器，不增加运行时成本。", "result": "在四个域外数据集和四个ASR模型上，结合TTS的WhisTLE相对于仅使用TTS的适应方法，相对降低了12.3%的词错误率（WER），并在32个场景中的27个中优于所有非WhisTLE基线方法。", "conclusion": "WhisTLE是一种有效的纯文本域适应方法，能够显著提高预训练ASR模型在未见域上的性能，且不增加推理成本。"}}
{"id": "2509.10298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10298", "abs": "https://arxiv.org/abs/2509.10298", "authors": ["Laith Nayal", "Mahmoud Mousatat", "Bader Rasheed"], "title": "Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks", "comment": "8 pages, 2 tables", "summary": "Deep neural networks and Vision Transformers achieve state-of-the-art\nperformance in computer vision but are highly vulnerable to adversarial\nperturbations. Standard defenses often incur high computational cost or lack\nformal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)\nmethod, where drop probabilities increase with depth to control the effective\nLipschitz constant of the network. This approach regularizes deeper layers,\nimproving robustness while preserving clean accuracy and reducing computation.\nExperiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent\nschedule maintains near-baseline clean accuracy, enhances robustness under\nFGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to\nbaseline and linear DropPath schedules.", "AI": {"tldr": "本文提出了一种Lipschitz引导的随机深度（DropPath）方法，通过增加深层网络的丢弃概率来控制有效Lipschitz常数，从而在保持准确性的同时提高对抗鲁棒性并降低计算成本。", "motivation": "深度神经网络和Vision Transformers易受对抗性扰动攻击，而现有防御方法通常计算成本高昂或缺乏形式保证。", "method": "提出了一种Lipschitz引导的随机深度（DropPath）方法，其中丢弃概率随网络深度增加，以有效控制网络的Lipschitz常数。这种方法正则化了更深层，并采用定制的深度依赖调度。", "result": "在CIFAR-10和ViT-Tiny上的实验表明，该方法在保持接近基线准确性的同时，显著增强了在FGSM、PGD-20和AutoAttack下的鲁棒性，并且与基线和线性DropPath调度相比，显著降低了FLOPs。", "conclusion": "所提出的Lipschitz引导的随机深度方法能够有效提高深度神经网络的对抗鲁棒性，同时保持干净准确性并降低计算开销。"}}
{"id": "2509.10310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10310", "abs": "https://arxiv.org/abs/2509.10310", "authors": ["Evan Murphy", "Marco Viola", "Vladimir A. Krylov"], "title": "A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments", "comment": "Accepted for publication in the Proceedings of the 27th Irish Machine\n  Vision and Image Processing Conference (IMVIP 2025)", "summary": "In this paper we address the problem of precise geolocation of street\nfurniture in complex urban environments, which is a critical task for effective\nmonitoring and maintenance of public infrastructure by local authorities and\nprivate stakeholders. To this end, we propose a probabilistic framework based\non energy maps that encode the spatial likelihood of object locations.\nRepresenting the energy in a map-based geopositioned format allows the\noptimisation process to seamlessly integrate external geospatial information,\nsuch as GIS layers, road maps, or placement constraints, which improves\ncontextual awareness and localisation accuracy. A stochastic birth-and-death\noptimisation algorithm is introduced to infer the most probable configuration\nof assets. We evaluate our approach using a realistic simulation informed by a\ngeolocated dataset of street lighting infrastructure in Dublin city centre,\ndemonstrating its potential for scalable and accurate urban asset mapping. The\nimplementation of the algorithm will be made available in the GitHub repository\nhttps://github.com/EMurphy0108/SBD_Street_Furniture.", "AI": {"tldr": "本文提出了一种基于能量图的概率框架和随机生灭优化算法，用于在复杂城市环境中精确地理定位街道设施，以实现可扩展和准确的城市资产测绘。", "motivation": "在复杂城市环境中，精确地理定位街道设施是地方当局和私人利益相关者有效监测和维护公共基础设施的关键任务。", "method": "该研究提出一个基于能量图的概率框架，用于编码物体位置的空间可能性。通过地图式地理定位格式表示能量，该优化过程可以无缝整合外部地理空间信息（如GIS图层、道路地图、放置约束）。此外，引入了一种随机生灭优化算法来推断最可能的资产配置。", "result": "该方法通过基于都柏林市中心街道照明基础设施地理定位数据集的真实模拟进行评估，证明了其在可扩展和准确的城市资产测绘方面的潜力。", "conclusion": "该研究提出的方法能够实现城市街道设施的可扩展且准确的测绘，对于公共基础设施的监测和维护具有重要意义。"}}
{"id": "2509.10312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10312", "abs": "https://arxiv.org/abs/2509.10312", "authors": ["Zhixin Zheng", "Xinyu Wang", "Chang Zou", "Shaobo Wang", "Linfeng Zhang"], "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching", "comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration", "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.", "AI": {"tldr": "本文提出了一种名为ClusCa的聚类驱动特征缓存方法，通过在每个时间步进行空间聚类来显著减少扩散Transformer中的令牌数量（超过90%），从而加速迭代去噪过程，且无需重新训练，在图像和视频生成任务中表现出显著的加速效果和甚至更高的质量。", "motivation": "扩散Transformer在生成高质量图像和视频方面表现出色，但其迭代去噪过程导致巨大的计算成本。现有的特征缓存方法仅利用扩散模型的时间相似性来加速，而忽略了空间维度上的相似性。", "method": "引入聚类驱动特征缓存（ClusCa）作为现有特征缓存的补充方法。ClusCa在每个时间步对令牌进行空间聚类，每个簇只计算一个令牌，并将其信息传播到该簇中的所有其他令牌，从而将令牌数量减少90%以上。该方法可直接应用于任何扩散Transformer，无需训练。", "result": "ClusCa能够将令牌数量减少90%以上。在DiT、FLUX和HunyuanVideo上进行了广泛实验，验证了其在文本到图像和文本到视频生成中的有效性。例如，ClusCa在FLUX上实现了4.96倍的加速，ImageReward达到99.49%，比原始模型高出0.51%。", "conclusion": "ClusCa是一种有效且互补的加速扩散Transformer的方法，它通过利用空间相似性进行聚类，显著降低了计算成本，同时无需重新训练，甚至可能提升生成质量。该方法可广泛应用于各种扩散Transformer模型。"}}
{"id": "2509.10341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10341", "abs": "https://arxiv.org/abs/2509.10341", "authors": ["Botond Fazekas", "Thomas Pinetz", "Guilherme Aresta", "Taha Emre", "Hrvoje Bogunovic"], "title": "GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT", "comment": null, "summary": "Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing\nand monitoring retinal diseases. However, OCT images are inherently degraded by\nspeckle noise, which obscures fine details and hinders accurate interpretation.\nWhile numerous denoising methods exist, many struggle to balance noise\nreduction with the preservation of crucial anatomical structures. This paper\nintroduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel\ndeep learning approach for OCT image despeckling that leverages the strengths\nof diffusion probabilistic models. Unlike conventional diffusion models that\nassume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more\naccurately reflect the statistical properties of speckle. Furthermore, we\nintroduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,\nless-noisy image to guide the denoising process. This crucial addition prevents\nthe reintroduction of high-frequency noise. We accelerate the inference process\nby adapting the Denoising Diffusion Implicit Model framework to our Gamma-based\nmodel. Experiments on a dataset with paired noisy and less-noisy OCT B-scans\ndemonstrate that GARD significantly outperforms traditional denoising methods\nand state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.\nQualitative results confirm that GARD produces sharper edges and better\npreserves fine anatomical details.", "AI": {"tldr": "本文提出GARD（基于Gamma的解剖结构恢复和去噪），一种新颖的深度学习方法，利用Gamma扩散概率模型和噪声降低保真项对光学相干断层扫描（OCT）图像进行散斑去噪，实现了卓越的去噪效果和细节保留。", "motivation": "OCT图像固有的散斑噪声会模糊精细细节并阻碍准确诊断。现有去噪方法难以在降噪和保留关键解剖结构之间取得平衡。", "method": "GARD方法采用去噪扩散Gamma模型（DDGM），以更准确地反映散斑的统计特性，而非传统的假设高斯噪声的扩散模型。此外，引入了一个噪声降低保真项，利用预处理的低噪声图像来指导去噪过程，防止高频噪声的重新引入。通过调整去噪扩散隐式模型（DDIM）框架，加速了推理过程。", "result": "在配对的噪声和低噪声OCT B扫描数据集上的实验表明，GARD在PSNR、SSIM和MSE方面显著优于传统去噪方法和最先进的深度学习模型。定性结果证实GARD能产生更锐利的边缘并更好地保留精细的解剖细节。", "conclusion": "GARD是一种有效且先进的OCT图像散斑去噪深度学习方法，它通过创新的Gamma扩散模型和噪声降低保真项，在去噪的同时显著提升了图像质量和解剖细节的保留。"}}
{"id": "2509.10359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10359", "abs": "https://arxiv.org/abs/2509.10359", "authors": ["Matteo Trippodo", "Federico Becattini", "Lorenzo Seidenari"], "title": "Immunizing Images from Text to Image Editing via Adversarial Cross-Attention", "comment": "Accepted as Regular Paper at ACM Multimedia 2025", "summary": "Recent advances in text-based image editing have enabled fine-grained\nmanipulation of visual content guided by natural language. However, such\nmethods are susceptible to adversarial attacks. In this work, we propose a\nnovel attack that targets the visual component of editing methods. We introduce\nAttention Attack, which disrupts the cross-attention between a textual prompt\nand the visual representation of the image by using an automatically generated\ncaption of the source image as a proxy for the edit prompt. This breaks the\nalignment between the contents of the image and their textual description,\nwithout requiring knowledge of the editing method or the editing prompt.\nReflecting on the reliability of existing metrics for immunization success, we\npropose two novel evaluation strategies: Caption Similarity, which quantifies\nsemantic consistency between original and adversarial edits, and semantic\nIntersection over Union (IoU), which measures spatial layout disruption via\nsegmentation masks. Experiments conducted on the TEDBench++ benchmark\ndemonstrate that our attack significantly degrades editing performance while\nremaining imperceptible.", "AI": {"tldr": "本文提出了一种名为“注意力攻击”的新型对抗性攻击，旨在通过破坏文本提示与图像视觉表示之间的交叉注意力，降低文本图像编辑方法的性能，同时保持攻击的不可感知性。", "motivation": "文本图像编辑方法容易受到对抗性攻击。现有攻击可能需要了解编辑方法或编辑提示，这限制了其普适性。", "method": "我们引入了“注意力攻击”，它通过使用源图像的自动生成标题作为编辑提示的代理，来扰乱文本提示和图像视觉表示之间的交叉注意力。这打破了图像内容与其文本描述之间的对齐，且无需了解编辑方法或编辑提示。此外，我们提出了两种新的评估策略：标题相似度（量化语义一致性）和语义交并比（IoU，通过分割掩码测量空间布局中断）。", "result": "在TEDBench++基准测试上进行的实验表明，我们的攻击显著降低了编辑性能，同时保持了不可感知性。", "conclusion": "我们提出了一种有效且不可感知的注意力攻击，揭示了文本图像编辑方法的脆弱性。同时，我们提出了更可靠的评估策略，以更好地衡量免疫成功率。"}}
{"id": "2509.10366", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10366", "abs": "https://arxiv.org/abs/2509.10366", "authors": ["Fabien Allemand", "Attilio Fiandrotti", "Sumanta Chaudhuri", "Alaa Eddine Mazouz"], "title": "Efficient Learned Image Compression Through Knowledge Distillation", "comment": "19 pages, 21 figures", "summary": "Learned image compression sits at the intersection of machine learning and\nimage processing. With advances in deep learning, neural network-based\ncompression methods have emerged. In this process, an encoder maps the image to\na low-dimensional latent space, which is then quantized, entropy-coded into a\nbinary bitstream, and transmitted to the receiver. At the receiver end, the\nbitstream is entropy-decoded, and a decoder reconstructs an approximation of\nthe original image. Recent research suggests that these models consistently\noutperform conventional codecs. However, they require significant processing\npower, making them unsuitable for real-time use on resource-constrained\nplatforms, which hinders their deployment in mainstream applications. This\nstudy aims to reduce the resource requirements of neural networks used for\nimage compression by leveraging knowledge distillation, a training paradigm\nwhere smaller neural networks, partially trained on the outputs of larger, more\ncomplex models, can achieve better performance than when trained independently.\nOur work demonstrates that knowledge distillation can be effectively applied to\nimage compression tasks: i) across various architecture sizes, ii) to achieve\ndifferent image quality/bit rate tradeoffs, and iii) to save processing and\nenergy resources. This approach introduces new settings and hyperparameters,\nand future research could explore the impact of different teacher models, as\nwell as alternative loss functions. Knowledge distillation could also be\nextended to transformer-based models. The code is publicly available at:\nhttps://github.com/FABallemand/PRIM .", "AI": {"tldr": "深度学习图像压缩模型性能优异但资源消耗大。本研究利用知识蒸馏技术，显著降低了这些模型的资源需求，使其更适用于资源受限平台。", "motivation": "基于神经网络的图像压缩方法已超越传统编码器，但其高计算需求限制了在资源受限平台上的实时应用，阻碍了其主流部署。", "method": "本研究利用知识蒸馏（Knowledge Distillation）范式，通过让较小的神经网络学习大型复杂模型的输出，来训练用于图像压缩的神经网络，从而降低其资源需求。", "result": "研究表明，知识蒸馏可以有效地应用于图像压缩任务：i) 适用于不同架构尺寸的模型；ii) 实现不同的图像质量/比特率权衡；iii) 节省处理和能源资源。", "conclusion": "知识蒸馏是降低神经网络图像压缩模型资源需求的一种有效方法，使其更适用于实际应用。未来的研究可探索不同的教师模型、替代损失函数以及将其扩展到基于Transformer的模型。"}}
{"id": "2509.10388", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10388", "abs": "https://arxiv.org/abs/2509.10388", "authors": ["Zeqing Leo Yuan", "Mani Ramanagopal", "Aswin C. Sankaranarayanan", "Srinivasa G. Narasimhan"], "title": "Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition", "comment": null, "summary": "Decomposing an image into its intrinsic photometric factors--shading and\nreflectance--is a long-standing challenge due to the lack of extensive\nground-truth data for real-world scenes. Recent methods rely on synthetic data\nor sparse annotations for limited indoor and even fewer outdoor scenes. We\nintroduce a novel training-free approach for intrinsic image decomposition\nusing only a pair of visible and thermal images. We leverage the principle that\nlight not reflected from an opaque surface is absorbed and detected as heat by\na thermal camera. This allows us to relate the ordinalities between visible and\nthermal image intensities to the ordinalities of shading and reflectance, which\ncan densely self-supervise an optimizing neural network to recover shading and\nreflectance. We perform quantitative evaluations with known reflectance and\nshading under natural and artificial lighting, and qualitative experiments\nacross diverse outdoor scenes. The results demonstrate superior performance\nover recent learning-based models and point toward a scalable path to curating\nreal-world ordinal supervision, previously infeasible via manual labeling.", "AI": {"tldr": "本文提出一种无需训练的方法，利用可见光和热成像图像对进行内在图像分解，通过光吸收原理自监督优化神经网络，在真实世界场景中表现优于现有学习模型。", "motivation": "内在图像分解（将图像分解为阴影和反射）长期以来面临挑战，原因在于缺乏真实世界场景的广泛地面真值数据。现有方法依赖合成数据或有限的稀疏标注。", "method": "该方法是一种新颖的无需训练的内在图像分解方法，仅使用一对可见光和热成像图像。它利用了不从不透明表面反射的光会被吸收并被热像仪检测为热量的原理。这使得可见光和热成像图像强度之间的序数关系能够与阴影和反射的序数关系相关联，从而密集地自监督一个优化神经网络来恢复阴影和反射。", "result": "在自然光和人工光照下，通过已知反射和阴影进行的定量评估，以及在各种室外场景中进行的定性实验，结果表明该方法优于最近基于学习的模型，并为获取真实世界序数监督提供了一条可扩展的途径，这在以前通过手动标注是不可行的。", "conclusion": "利用可见光和热成像图像对，该方法能够有效地进行内在图像分解，表现出优异的性能，并为解决真实世界场景中地面真值数据稀缺问题提供了一种可扩展的自监督方案。"}}
{"id": "2509.10407", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10407", "abs": "https://arxiv.org/abs/2509.10407", "authors": ["Xiem HoangVan", "Dang BuiDinh", "Sang NguyenQuang", "Wen-Hsiao Peng"], "title": "Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards", "comment": null, "summary": "Compressed video quality enhancement (CVQE) is crucial for improving user\nexperience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.\nWhile deep learning based CVQE has driven significant progress, existing\nsurveys still suffer from limitations: lack of systematic classification\nlinking methods to specific standards and artifacts, insufficient comparative\nanalysis of architectural paradigms across coding types, and underdeveloped\nbenchmarking practices. To address these gaps, this paper presents three key\ncontributions. First, it introduces a novel taxonomy classifying CVQE methods\nacross architectural paradigms, coding standards, and compressed-domain feature\nutilization. Second, it proposes a unified benchmarking framework integrating\nmodern compression protocols and standard test sequences for fair\nmulti-criteria evaluation. Third, it provides a systematic analysis of the\ncritical trade-offs between reconstruction performance and computational\ncomplexity observed in state-of-the-art methods and highlighting promising\ndirections for future research. This comprehensive review aims to establish a\nfoundation for consistent assessment and informed model selection in CVQE\nresearch and deployment.", "AI": {"tldr": "本文对基于深度学习的压缩视频质量增强（CVQE）进行了全面综述，提出了新的分类法、统一的基准测试框架，并分析了性能与计算复杂度的权衡，旨在为CVQE研究提供一致的评估基础。", "motivation": "现有关于深度学习CVQE的综述存在局限性，包括缺乏系统分类（未能将方法与特定标准和伪影关联）、对不同编码类型的架构范式比较不足，以及基准测试实践不完善。", "method": "该研究提出了三项主要贡献：1. 引入了一种新颖的CVQE方法分类法，涵盖架构范式、编码标准和压缩域特征利用。2. 提出了一个统一的基准测试框架，整合现代压缩协议和标准测试序列以进行公平的多准则评估。3. 系统分析了现有先进方法在重建性能和计算复杂度之间的关键权衡。", "result": "本文通过提供新的分类法、统一的基准测试框架以及对性能-复杂度权衡的系统分析，为CVQE研究和部署中的一致性评估和模型选择奠定了基础，并指出了未来研究的有前景方向。", "conclusion": "这项全面的综述旨在为压缩视频质量增强研究和部署中的一致性评估和知情模型选择建立基础，并突出未来研究的有前景方向。"}}
{"id": "2509.10441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.10441", "abs": "https://arxiv.org/abs/2509.10441", "authors": ["Tao Han", "Wanghan Xu", "Junchao Gong", "Xiaoyu Yue", "Song Guo", "Luping Zhou", "Lei Bai"], "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis", "comment": "Accepted by ICCV 2025", "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\n\\textbf{InfGen}, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.", "AI": {"tldr": "InfGen提出了一种新的单步生成器，用于替代潜在扩散模型中的VAE解码器，从而在不重新训练扩散模型的情况下，从固定大小的潜在表示生成任意分辨率的图像，显著降低了计算复杂性，并将4K图像生成时间缩短至10秒以内。", "motivation": "当前扩散模型生成任意分辨率图像时，计算需求随分辨率呈二次方增长，导致4K图像生成耗时超过100秒，影响了用户体验和应用效率。", "method": "InfGen基于潜在扩散模型，将扩散模型生成的固定潜在表示视为内容表征。它提出用一个新的“单步生成器”来解码任意分辨率的图像，该生成器取代了VAE解码器。这种方法无需重新训练扩散模型，并且可以应用于使用相同潜在空间的任何模型。", "result": "实验表明，InfGen能够将许多模型提升到任意高分辨率时代，同时将4K图像的生成时间缩短到10秒以下。", "conclusion": "InfGen提供了一种简化且计算效率高的方法，能够从固定大小的潜在表示生成任意高分辨率图像，解决了现有扩散模型在高分辨率生成方面的效率问题，具有广泛的应用潜力。"}}
{"id": "2509.10453", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.10453", "abs": "https://arxiv.org/abs/2509.10453", "authors": ["Emily Kaczmarek", "Justin Szeto", "Brennan Nichyporuk", "Tal Arbel"], "title": "SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets", "comment": null, "summary": "Alzheimer's disease is a progressive, neurodegenerative disorder that causes\nmemory loss and cognitive decline. While there has been extensive research in\napplying deep learning models to Alzheimer's prediction tasks, these models\nremain limited by lack of available labeled data, poor generalization across\ndatasets, and inflexibility to varying numbers of input scans and time\nintervals between scans. In this study, we adapt three state-of-the-art\ntemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,\nand add novel extensions designed to handle variable-length inputs and learn\nrobust spatial features. We aggregate four publicly available datasets\ncomprising 3,161 patients for pre-training, and show the performance of our\nmodel across multiple Alzheimer's prediction tasks including diagnosis\nclassification, conversion detection, and future conversion prediction.\nImportantly, our SSL model implemented with temporal order prediction and\ncontrastive learning outperforms supervised learning on six out of seven\ndownstream tasks. It demonstrates adaptability and generalizability across\ntasks and number of input images with varying time intervals, highlighting its\ncapacity for robust performance across clinical applications. We release our\ncode and model publicly at https://github.com/emilykaczmarek/SSL-AD.", "AI": {"tldr": "本研究针对阿尔茨海默病预测中深度学习模型面临的数据稀缺、泛化性差和输入灵活性不足等问题，提出了一种适应性强的时序自监督学习（SSL）方法，通过新颖的扩展处理可变长度输入，并在多项任务中显著优于监督学习。", "motivation": "现有的深度学习模型在阿尔茨海默病（AD）预测任务中受限于标注数据不足、跨数据集泛化能力差，以及对不同数量的输入扫描和扫描时间间隔缺乏灵活性。", "method": "本研究将三种最先进的时序自监督学习（SSL）方法应用于3D脑部MRI分析，并添加了新颖的扩展，旨在处理可变长度输入并学习鲁棒的空间特征。研究聚合了四个公开数据集（共3,161名患者）进行预训练，并使用了时序顺序预测和对比学习。", "result": "实施了时序顺序预测和对比学习的SSL模型在七项下游阿尔茨海默病预测任务（包括诊断分类、转换检测和未来转换预测）中的六项上优于监督学习。该模型在任务和不同时间间隔的输入图像数量方面均表现出适应性和泛化性。", "conclusion": "所提出的时序自监督学习模型在阿尔茨海默病预测中展现出强大的性能，克服了现有深度学习方法的关键局限性，并具有在各种临床应用中实现鲁棒性能的潜力。"}}
