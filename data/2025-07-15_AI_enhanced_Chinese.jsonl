{"id": "2507.08849", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.08849", "abs": "https://arxiv.org/abs/2507.08849", "authors": ["Emilio Carrizosa", "Martina Fischetti", "Roshell Haaker", "Juan Miguel Morales"], "title": "Counterfactual optimization for fault prevention in complex wind energy systems", "comment": null, "summary": "Machine Learning models are increasingly used in businesses to detect faults\nand anomalies in complex systems. In this work, we take this approach a step\nfurther: beyond merely detecting anomalies, we aim to identify the optimal\ncontrol strategy that restores the system to a safe state with minimal\ndisruption. We frame this challenge as a counterfactual problem: given a\nMachine Learning model that classifies system states as either good or\nanomalous, our goal is to determine the minimal adjustment to the system's\ncontrol variables (i.e., its current status) that is necessary to return it to\nthe good state. To achieve this, we leverage a mathematical model that finds\nthe optimal counterfactual solution while respecting system specific\nconstraints. Notably, most counterfactual analysis in the literature focuses on\nindividual cases where a person seeks to alter their status relative to a\ndecision made by a classifier, such as for loan approval or medical diagnosis.\nOur work addresses a fundamentally different challenge: optimizing\ncounterfactuals for a complex energy system, specifically an offshore wind\nturbine oil type transformer. This application not only advances counterfactual\noptimization in a new domain but also opens avenues for broader research in\nthis area. Our tests on real world data provided by our industrial partner show\nthat our methodology easily adapts to user preferences and brings savings in\nthe order of 3 million euros per year in a typical farm.", "AI": {"tldr": "论文提出了一种基于机器学习的方法，不仅检测系统异常，还优化控制策略以最小化恢复成本，应用于风电变压器系统，年节省约300万欧元。", "motivation": "现有方法仅检测异常，未提供恢复策略。本文旨在填补这一空白，优化系统恢复的最小调整。", "method": "将问题建模为反事实优化问题，利用数学模型在系统约束下找到最优解。", "result": "在风电变压器系统中，方法适应性强，年节省约300万欧元。", "conclusion": "该方法不仅拓展了反事实优化的应用领域，还为复杂系统的恢复策略提供了新思路。"}}
{"id": "2507.09000", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09000", "abs": "https://arxiv.org/abs/2507.09000", "authors": ["Arshia Rafieioskouei", "Kenneth Rogale", "Borzoo Bonakdarpour"], "title": "Efficient Discovery of Actual Causality with Uncertainty", "comment": null, "summary": "Identifying the actual cause of events in engineered systems is a fundamental\nchallenge in system analysis. Finding such causes becomes more challenging in\nthe presence of noise and uncertainty in real-world systems. In this paper, we\nadopt the notion of probabilistic actual causality by Fenton-Glynn, which is a\nprobabilistic extension of Halpern and Pearl's actual causality, and propose a\nnovel method to formally reason about causal effect of events in systems\nsubject to uncertainty. We (1) formulate the discovery of probabilistic actual\ncauses in computing systems as an SMT problem, and (2) address the scalability\nchallenges by introducing an abstraction-refinement technique that\nsignificantly improves efficiency. We demonstrate the effectiveness of our\napproach through three case studies, identifying probabilistic causes of safety\nviolations in (1) the Mountain Car problem, (2) the Lunar Lander benchmark, and\n(3) MPC controller for an F-16 autopilot simulator.", "AI": {"tldr": "论文提出了一种基于概率实际因果关系的SMT问题求解方法，用于在不确定系统中识别事件的实际原因，并通过抽象-细化技术提高效率。", "motivation": "在噪声和不确定性存在的现实系统中，识别事件的实际原因具有挑战性。", "method": "采用Fenton-Glynn的概率实际因果关系概念，将问题建模为SMT问题，并引入抽象-细化技术提升效率。", "result": "在三个案例研究中验证了方法的有效性，包括Mountain Car问题、Lunar Lander基准和F-16自动驾驶模拟器的MPC控制器。", "conclusion": "该方法能够有效识别不确定系统中的概率实际原因，并具有较高的可扩展性。"}}
{"id": "2507.09115", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09115", "abs": "https://arxiv.org/abs/2507.09115", "authors": ["Sampson E. Nwachukwu"], "title": "Modelling and Control of a Buck Converter Using State-Space Averaging and Classical Feedback Techniques", "comment": "8 Pages", "summary": "This study presents the modeling, control design, and performance analysis of\na DC-DC buck converter using state-space averaging techniques. Buck converters\nare essential in modern power electronics for regulating DC voltages in\nrenewable energy and electric vehicle systems. The paper first introduces the\nbasic operation of buck converters and emphasizes the need for voltage\nregulation through closed-loop control systems. A state-space averaged model is\nderived to simplify the nonlinear switched dynamics, enabling a more effective\nanalysis and controller design. The small-signal transfer function from the\nduty cycle to the output voltage is obtained to support control development. In\naddition, the Proportional-Integral (PI) control based on the frequency-domain\nmethod was explored. The PI controller was tuned to achieve various phase\nmargins and is evaluated through Bode plots, step responses, and performance\nmetrics, revealing trade-offs between overshoot, settling time, and\nsteady-state error. A complete simulation of the controlled buck converter\nverifies its ability to maintain a stable output voltage across wide input\nvoltage variations. The results validate the effectiveness of state-space\naveraging in control design and highlight the robustness of feedback systems in\npower electronic converters.", "AI": {"tldr": "本文通过状态空间平均技术对DC-DC降压转换器进行建模、控制设计和性能分析，验证了其在宽输入电压变化下保持稳定输出电压的能力。", "motivation": "降压转换器在可再生能源和电动汽车系统中对直流电压调节至关重要，需通过闭环控制系统实现电压调节。", "method": "推导状态空间平均模型以简化非线性开关动态，获取小信号传递函数，并基于频域方法设计PI控制器。", "result": "通过Bode图、阶跃响应和性能指标评估PI控制器，验证其在宽输入电压变化下的稳定性和鲁棒性。", "conclusion": "状态空间平均技术在控制设计中有效，反馈系统在电力电子转换器中表现出鲁棒性。"}}
{"id": "2507.08855", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08855", "abs": "https://arxiv.org/abs/2507.08855", "authors": ["Yang Ming", "Jiang Shi Zhong", "Zhou Su Juan"], "title": "Multi-omic Prognosis of Alzheimer's Disease with Asymmetric Cross-Modal Cross-Attention Network", "comment": null, "summary": "Alzheimer's Disease (AD) is an irreversible neurodegenerative disease\ncharacterized by progressive cognitive decline as its main symptom. In the\nresearch field of deep learning-assisted diagnosis of AD, traditional\nconvolutional neural networks and simple feature concatenation methods fail to\neffectively utilize the complementary information between multimodal data, and\nthe simple feature concatenation approach is prone to cause the loss of key\ninformation during the process of modal fusion. In recent years, the\ndevelopment of deep learning technology has brought new possibilities for\nsolving the problem of how to effectively fuse multimodal features. This paper\nproposes a novel deep learning algorithm framework to assist medical\nprofessionals in AD diagnosis. By fusing medical multi-view information such as\nbrain fluorodeoxyglucose positron emission tomography (PET), magnetic resonance\nimaging (MRI), genetic data, and clinical data, it can accurately detect the\npresence of AD, Mild Cognitive Impairment (MCI), and Cognitively Normal (CN).\nThe innovation of the algorithm lies in the use of an asymmetric cross-modal\ncross-attention mechanism, which can effectively capture the key information\nfeatures of the interactions between different data modal features. This paper\ncompares the asymmetric cross-modal cross-attention mechanism with the\ntraditional algorithm frameworks of unimodal and multimodal deep learning\nmodels for AD diagnosis, and evaluates the importance of the asymmetric\ncross-modal cross-attention mechanism. The algorithm model achieves an accuracy\nof 94.88% on the test set.", "AI": {"tldr": "本文提出了一种新型深度学习算法框架，通过融合多模态医学数据，利用不对称跨模态交叉注意力机制，显著提升了阿尔茨海默病的诊断准确性。", "motivation": "传统卷积神经网络和简单特征拼接方法在多模态数据融合中效果不佳，无法充分利用互补信息，导致关键信息丢失。", "method": "采用不对称跨模态交叉注意力机制，融合脑PET、MRI、遗传数据和临床数据等多模态信息。", "result": "算法在测试集上达到94.88%的准确率。", "conclusion": "不对称跨模态交叉注意力机制在多模态数据融合中表现优异，为AD诊断提供了新思路。"}}
{"id": "2507.09134", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09134", "abs": "https://arxiv.org/abs/2507.09134", "authors": ["Shu Zhang", "James Y. Z. Liu", "Dominic Liao-McPherson"], "title": "Integrating Planning and Predictive Control Using the Path Feasibility Governor", "comment": "14 pages, 7 figures, submitted to IEEE Transactions on Automatic\n  Control", "summary": "The motion planning problem of generating dynamically feasible,\ncollision-free trajectories in non-convex environments is a fundamental\nchallenge for autonomous systems. Decomposing the problem into path planning\nand path tracking improves tractability, but integrating these components in a\ntheoretically sound and computationally efficient manner is challenging. We\npropose the Path Feasibility Governor (PathFG), a framework for integrating\npath planners with nonlinear Model Predictive Control (MPC). The PathFG\nmanipulates the reference passed to the MPC controller, guiding it along a path\nwhile ensuring constraint satisfaction, stability, and recursive feasibility.\nThe PathFG is modular, compatible with replanning, and improves computational\nefficiency and reliability by reducing the need for long prediction horizons.\nWe prove safety and asymptotic stability with a significantly expanded region\nof attraction, and validate its real-time performance through a simulated case\nstudy of quadrotor navigation in a cluttered environment.", "AI": {"tldr": "提出PathFG框架，将路径规划与非线性MPC结合，确保动态可行性和安全性，提高计算效率。", "motivation": "解决非凸环境中动态可行、无碰撞轨迹生成的挑战，改进路径规划与跟踪的集成问题。", "method": "PathFG框架通过操纵MPC参考路径，确保约束满足、稳定性和递归可行性，模块化设计支持重规划。", "result": "证明安全性和渐近稳定性，显著扩大吸引域，模拟实验验证实时性能。", "conclusion": "PathFG高效可靠，适用于复杂环境中的自主系统导航。"}}
{"id": "2507.08952", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08952", "abs": "https://arxiv.org/abs/2507.08952", "authors": ["Silas Nyboe Ørting", "Kristina Miger", "Anne Sophie Overgaard Olesen", "Mikael Ploug Boesen", "Michael Brun Andersen", "Jens Petersen", "Olav W. Nielsen", "Marleen de Bruijne"], "title": "Interpretable Artificial Intelligence for Detecting Acute Heart Failure on Acute Chest CT Scans", "comment": "34 pages, 11 figures, Submitted to \"Radiology AI\"", "summary": "Introduction: Chest CT scans are increasingly used in dyspneic patients where\nacute heart failure (AHF) is a key differential diagnosis. Interpretation\nremains challenging and radiology reports are frequently delayed due to a\nradiologist shortage, although flagging such information for emergency\nphysicians would have therapeutic implication. Artificial intelligence (AI) can\nbe a complementary tool to enhance the diagnostic precision. We aim to develop\nan explainable AI model to detect radiological signs of AHF in chest CT with an\naccuracy comparable to thoracic radiologists.\n  Methods: A single-center, retrospective study during 2016-2021 at Copenhagen\nUniversity Hospital - Bispebjerg and Frederiksberg, Denmark. A Boosted Trees\nmodel was trained to predict AHF based on measurements of segmented cardiac and\npulmonary structures from acute thoracic CT scans. Diagnostic labels for\ntraining and testing were extracted from radiology reports. Structures were\nsegmented with TotalSegmentator. Shapley Additive explanations values were used\nto explain the impact of each measurement on the final prediction.\n  Results: Of the 4,672 subjects, 49% were female. The final model incorporated\ntwelve key features of AHF and achieved an area under the ROC of 0.87 on the\nindependent test set. Expert radiologist review of model misclassifications\nfound that 24 out of 64 (38%) false positives and 24 out of 61 (39%) false\nnegatives were actually correct model predictions, with the errors originating\nfrom inaccuracies in the initial radiology reports.\n  Conclusion: We developed an explainable AI model with strong discriminatory\nperformance, comparable to thoracic radiologists. The AI model's stepwise,\ntransparent predictions may support decision-making.", "AI": {"tldr": "开发了一种可解释的AI模型，用于检测胸部CT中的急性心力衰竭（AHF）放射学特征，性能与胸科放射科医生相当。", "motivation": "由于放射科医生短缺，胸部CT扫描在急性心力衰竭诊断中的应用受到限制，AI可作为辅助工具提高诊断精度。", "method": "采用Boosted Trees模型，基于胸部CT扫描中分割的心脏和肺部结构测量数据预测AHF，并使用Shapley Additive explanations解释预测。", "result": "模型在独立测试集上ROC曲线下面积为0.87，部分误分类源于初始放射学报告的不准确。", "conclusion": "该AI模型具有强判别性能，透明化的预测步骤可支持临床决策。"}}
{"id": "2507.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08851", "abs": "https://arxiv.org/abs/2507.08851", "authors": ["Simon Schwaiger", "Stefan Thalhammer", "Wilfried Wöber", "Gerald Steinbauer-Wagner"], "title": "OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation", "comment": null, "summary": "Understanding open-world semantics is critical for robotic planning and\ncontrol, particularly in unstructured outdoor environments. Current\nvision-language mapping approaches rely on object-centric segmentation priors,\nwhich often fail outdoors due to semantic ambiguities and indistinct semantic\nclass boundaries. We propose OTAS - an Open-vocabulary Token Alignment method\nfor Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary\nsegmentation models by extracting semantic structure directly from the output\ntokens of pretrained vision models. By clustering semantically similar\nstructures across single and multiple views and grounding them in language,\nOTAS reconstructs a geometrically consistent feature field that supports\nopen-vocabulary segmentation queries. Our method operates zero-shot, without\nscene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor\nIoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on\nthe Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU\nimprovement over open-vocabulary mapping methods in 3D segmentation on\nTartanAir. Real-world reconstructions demonstrate OTAS' applicability to\nrobotic applications. The code and ROS node will be made publicly available\nupon paper acceptance.", "AI": {"tldr": "OTAS是一种开放词汇的户外分割方法，通过从预训练视觉模型的输出令牌中提取语义结构，克服了现有方法的局限性，支持零样本运行和几何一致的特征场重建。", "motivation": "在非结构化户外环境中，现有的视觉-语言映射方法依赖对象中心分割先验，常因语义模糊和边界不清而失败，因此需要一种更有效的方法。", "method": "OTAS通过聚类单视图和多视图中的语义相似结构，并将其与语言对齐，重建几何一致的特征场，支持开放词汇分割查询。", "result": "OTAS在零样本条件下运行，速度达17 fps，在Off-Road Freespace Detection数据集上表现优于微调和开放词汇2D分割方法，在TartanAir上3D分割性能提升151%。", "conclusion": "OTAS适用于机器人应用，代码和ROS节点将在论文接受后公开。"}}
{"id": "2507.08865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "提出了一种基于Transformer的模型Spatial ModernBERT，用于从复杂财务文档中提取表格和键值对，通过空间嵌入和多头分类任务实现高精度。", "motivation": "财务文档中的表格和键值对提取对审计、数据分析和自动化发票处理等业务流程至关重要。", "method": "使用Spatial ModernBERT模型，结合空间嵌入，通过三个分类头（标签、列、行）进行标记分类，并在PubTables-1M数据集上预训练后微调。", "result": "模型在财务文档中表现出色，能够有效利用文本和空间线索实现高精度提取。", "conclusion": "Spatial ModernBERT为财务文档中的表格和键值对提取提供了高效解决方案。"}}
{"id": "2507.08831", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08831", "abs": "https://arxiv.org/abs/2507.08831", "authors": ["Josh Qixuan Sun", "Xiaoying Xing", "Huaiyuan Weng", "Chul Min Yeum", "Mark Crowley"], "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "comment": "Under review", "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent\nfollows instructions and moves freely to reach a destination, is a key research\nproblem in embodied AI. However, most navigation policies are sensitive to\nviewpoint changes, i.e., variations in camera height and viewing angle that\nalter the agent's observation. In this paper, we introduce a generalized\nscenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View\nInvariant Learning), a view-invariant post-training strategy that enhances the\nrobustness of existing navigation policies to changes in camera viewpoint. VIL\nemploys a contrastive learning framework to learn sparse and view-invariant\nfeatures. Additionally, we introduce a teacher-student framework for the\nWaypoint Predictor Module, a core component of most VLNCE baselines, where a\nview-dependent teacher model distills knowledge into a view-invariant student\nmodel. We employ an end-to-end training paradigm to jointly optimize these\ncomponents, thus eliminating the cost for individual module training. Empirical\nresults show that our method outperforms state-of-the-art approaches on\nV2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets\nR2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE\nsetting and find that, despite being trained for varied viewpoints, it often\nstill improves performance. On the more challenging RxR-CE dataset, our method\nalso achieved state-of-the-art performance across all metrics when compared to\nother map-free methods. This suggests that adding VIL does not diminish the\nstandard viewpoint performance and can serve as a plug-and-play post-training\nmethod.", "AI": {"tldr": "论文提出了一种名为VIL的视角不变学习策略，用于增强视觉语言导航在连续环境中的鲁棒性，通过对比学习和师生框架实现，显著提升了性能。", "motivation": "解决视觉语言导航中导航策略对视角变化敏感的问题，提出V2-VLNCE场景和VIL方法。", "method": "采用对比学习框架学习稀疏且视角不变的特征，并引入师生框架优化Waypoint Predictor Module，通过端到端训练联合优化。", "result": "在V2-VLNCE上性能提升8-15%，在标准VLNCE设置下也表现优异，尤其在RxR-CE数据集上达到最优。", "conclusion": "VIL是一种即插即用的后训练方法，既能提升视角变化的鲁棒性，又不影响标准视角性能。"}}
{"id": "2507.08806", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent.", "AI": {"tldr": "论文提出了一种通过去除推理过程中的冗余注意力来提高大语言模型性能的方法，显著提升了数学竞赛等推理密集型任务的准确性。", "motivation": "观察到当前大语言模型在长链推理中存在注意力分散和冗余问题，尤其是在错误答案中表现更明显。", "method": "通过测量特殊结束标记的注意力分数识别冗余，提出结构感知剪枝方法去除低贡献推理块，随后恢复推理生成。", "result": "方法在无需训练的情况下显著提升了推理密集型任务的准确性，尤其在数学竞赛基准测试中表现突出。", "conclusion": "去除推理冗余能有效提升模型性能，尤其在复杂推理任务中效果显著。"}}
{"id": "2507.09280", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09280", "abs": "https://arxiv.org/abs/2507.09280", "authors": ["Xuan He", "Yuxin Pan", "Yize Chen", "Danny H. K. Tsang"], "title": "Vertex-Guided Redundant Constraints Identification for Unit Commitment", "comment": null, "summary": "Power systems Unit Commitment (UC) problem determines the generator\ncommitment schedule and dispatch decisions to realize the reliable and economic\noperation of power networks. The growing penetration of stochastic renewables\nand demand behaviors makes it necessary to solve the UC problem timely. It is\npossible to derive lightweight, faster-to-solve UC models via constraint\nscreening to eliminate redundant constraints. However, the screening process\nremains computationally cumbersome due to the need of solving numerous linear\nprogramming (LP) problems. To reduce the number of LPs to solve, we introduce a\nnovel perspective on such classic LP-based screening. Our key insights lie in\nthe principle that redundant constraints will be satisfied by all vertices of\nthe screened feasible region. Using the UC decision variables' bounds tightened\nby solving much fewer LPs, we build an outer approximation for the UC feasible\nregion as the screened region. A matrix operation is then designed and applied\nto the outer approximation's vertices to identify all redundant constraints\non-the-fly. Adjustments for the outer approximation are further explored to\nimprove screening efficiency by considering the load operating range and\ncutting planes derived from UC cost and discrete unit status prediction.\nExtensive simulations are performed on a set of testbeds up to 2,383 buses to\nsubstantiate the effectiveness of the proposed schemes. Compared to classic\nLP-based screening, our schemes can achieve up to 8.8x acceleration while\nfinding the same redundant constraints.", "AI": {"tldr": "论文提出了一种基于线性规划（LP）的约束筛选新方法，通过减少需要解决的LP问题数量，显著提高了电力系统机组组合（UC）问题的求解效率。", "motivation": "随着随机可再生能源和需求行为的增加，需要快速解决UC问题。传统约束筛选方法因需要解决大量LP问题而计算负担重。", "method": "通过利用冗余约束在可行区域顶点均满足的特性，构建UC可行区域的外逼近，并通过矩阵操作动态识别冗余约束。进一步优化外逼近以提高筛选效率。", "result": "在多达2,383个总线的测试平台上验证了方法的有效性，相比传统LP筛选方法，加速比可达8.8倍。", "conclusion": "提出的方法显著减少了计算负担，同时保持了与传统方法相同的约束筛选效果。"}}
{"id": "2507.08982", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08982", "abs": "https://arxiv.org/abs/2507.08982", "authors": ["Hanene F. Z. Brachemi Meftah", "Wassim Hamidouche", "Sid Ahmed Fezza", "Olivier Déforges"], "title": "VIP: Visual Information Protection through Adversarial Attacks on Vision-Language Models", "comment": null, "summary": "Recent years have witnessed remarkable progress in developing Vision-Language\nModels (VLMs) capable of processing both textual and visual inputs. These\nmodels have demonstrated impressive performance, leading to their widespread\nadoption in various applications. However, this widespread raises serious\nconcerns regarding user privacy, particularly when models inadvertently process\nor expose private visual information. In this work, we frame the preservation\nof privacy in VLMs as an adversarial attack problem. We propose a novel attack\nstrategy that selectively conceals information within designated Region Of\nInterests (ROIs) in an image, effectively preventing VLMs from accessing\nsensitive content while preserving the semantic integrity of the remaining\nimage. Unlike conventional adversarial attacks that often disrupt the entire\nimage, our method maintains high coherence in unmasked areas. Experimental\nresults across three state-of-the-art VLMs namely LLaVA, Instruct-BLIP, and\nBLIP2-T5 demonstrate up to 98% reduction in detecting targeted ROIs, while\nmaintaining global image semantics intact, as confirmed by high similarity\nscores between clean and adversarial outputs. We believe that this work\ncontributes to a more privacy conscious use of multimodal models and offers a\npractical tool for further research, with the source code publicly available\nat: https://github.com/hbrachemi/Vlm_defense-attack.", "AI": {"tldr": "论文提出了一种针对视觉语言模型（VLMs）的新型隐私保护方法，通过选择性隐藏图像中的敏感区域（ROIs），防止模型泄露隐私信息，同时保持图像的语义完整性。", "motivation": "随着视觉语言模型的广泛应用，隐私泄露问题日益严重，需要一种有效的方法保护用户隐私。", "method": "将隐私保护问题建模为对抗攻击，提出了一种选择性隐藏ROIs的策略，避免破坏图像整体语义。", "result": "实验表明，该方法在三种先进VLMs（LLaVA、Instruct-BLIP和BLIP2-T5）上实现了高达98%的敏感区域隐藏效果，同时保持了图像的全局语义。", "conclusion": "该研究为多模态模型的隐私保护提供了实用工具，并推动了隐私意识的提升。"}}
{"id": "2507.08885", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08885", "abs": "https://arxiv.org/abs/2507.08885", "authors": ["Baining Zhao", "Rongze Tang", "Mingyuan Jia", "Ziyou Wang", "Fanghang Man", "Xin Zhang", "Yu Shang", "Weichen Zhang", "Chen Gao", "Wei Wu", "Xin Wang", "Xinlei Chen", "Yong Li"], "title": "AirScape: An Aerial Generative World Model with Motion Controllability", "comment": null, "summary": "How to enable robots to predict the outcomes of their own motion intentions\nin three-dimensional space has been a fundamental problem in embodied\nintelligence. To explore more general spatial imagination capabilities, here we\npresent AirScape, the first world model designed for six-degree-of-freedom\naerial agents. AirScape predicts future observation sequences based on current\nvisual inputs and motion intentions. Specifically, we construct an dataset for\naerial world model training and testing, which consists of 11k video-intention\npairs. This dataset includes first-person-view videos capturing diverse drone\nactions across a wide range of scenarios, with over 1,000 hours spent\nannotating the corresponding motion intentions. Then we develop a two-phase\ntraining schedule to train a foundation model -- initially devoid of embodied\nspatial knowledge -- into a world model that is controllable by motion\nintentions and adheres to physical spatio-temporal constraints.", "AI": {"tldr": "AirScape是一个为六自由度空中代理设计的首个世界模型，通过视觉输入和运动意图预测未来观测序列。", "motivation": "解决机器人预测自身运动意图在三维空间中的结果这一基础问题，探索更通用的空间想象能力。", "method": "构建包含11k视频-意图对的数据集，开发两阶段训练计划，将无空间知识的基础模型训练为可控的世界模型。", "result": "成功训练出可预测未来观测序列的世界模型，符合物理时空约束。", "conclusion": "AirScape为空中代理提供了有效的空间想象能力，推动了具身智能的发展。"}}
{"id": "2507.08898", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "SEALGuard是一种多语言护栏，旨在解决现有护栏在多语言安全对齐上的不足，通过LoRA技术改进多语言不安全提示的检测性能。", "motivation": "现有护栏（如LlamaGuard）在多语言不安全输入（尤其是低资源语言）上表现不佳，导致LLM系统易受攻击。", "method": "采用低秩适应（LoRA）技术将通用多语言模型转化为多语言护栏，并构建了包含26万条多语言提示的SEALSBench数据集。", "result": "SEALGuard在多语言不安全提示检测上显著优于LlamaGuard，DSR提升48%，并在DSR、精确率和F1分数上表现最佳。", "conclusion": "SEALGuard通过有效的多语言护栏技术，显著提升了LLM系统的安全对齐能力。"}}
{"id": "2507.08917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08917", "abs": "https://arxiv.org/abs/2507.08917", "authors": ["Justin D. Norman", "Hany Farid"], "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "comment": "10 pages, 3 figures, 3 tables", "summary": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.", "AI": {"tldr": "提出了一种利用面部生物特征异常模式检测深度伪造视频的新方法。", "motivation": "深度伪造技术容易被用于欺诈、诈骗和政治虚假信息，亟需有效的检测手段。", "method": "开发了一种基于面部生物特征异常模式的机器学习技术。", "result": "在大规模数据集上评估了该技术，并验证了其对视频篡改的鲁棒性和泛化能力。", "conclusion": "该方法能有效检测深度伪造视频，具有实际应用潜力。"}}
{"id": "2507.08875", "categories": ["cs.AI", "90B50, 90C29, 90C08, 91A80, 91B06"], "pdf": "https://arxiv.org/pdf/2507.08875", "abs": "https://arxiv.org/abs/2507.08875", "authors": ["Fuh-Hwa Franklin Liu", "Su-Chuan Shih"], "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data", "comment": "38 pages, 6 figures, 5 table. A practice applicable method for\n  multi-criteria assessments using cardinal and ordinal data", "summary": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment\nAnalysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria\nDecision-Making (MCDM), are utilized to appraise a collection of\nDecision-Making Units (DMUs), also known as alternatives, based on several\ncriteria. These methodologies inherently rely on assumptions and can be\ninfluenced by subjective judgment to effectively tackle the complex evaluation\nchallenges in various fields. In real-world scenarios, it is essential to\nincorporate both quantitative and qualitative criteria as they consist of\ncardinal and ordinal data. Despite the inherent variability in the criterion\nvalues of different alternatives, the homogeneity assumption is often employed,\nsignificantly affecting evaluations. To tackle these challenges and determine\nthe most appropriate alternative, we propose a novel MCA approach that combines\ntwo Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear\nprogramming, is pivotal in the MCA methodology. This approach improves\nefficiency and fairness, ensuring that evaluations are both comprehensive and\ndependable, thus offering a strong and adaptive solution. Two comprehensive\nnumerical examples demonstrate the accuracy and transparency of our proposed\nmethod. The goal is to encourage continued advancement and stimulate progress\nin automated decision systems and decision support systems.", "AI": {"tldr": "提出了一种结合两种虚拟差距分析（VGA）模型的新型多准则评估（MCA）方法，以提高评估的效率和公平性。", "motivation": "解决现有多准则评估方法（如DEA、SFA、MCDM）中依赖假设和主观判断的问题，并整合定量与定性标准。", "method": "结合两种VGA模型，基于线性编程框架，提出新的MCA方法。", "result": "通过两个数值示例验证了方法的准确性和透明度。", "conclusion": "该方法为自动化决策系统提供了全面可靠的解决方案，推动了决策支持系统的进步。"}}
{"id": "2507.09311", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09311", "abs": "https://arxiv.org/abs/2507.09311", "authors": ["Matteo Cederle", "Marco Fabris", "Gian Antonio Susto"], "title": "A Fairness-Oriented Multi-Objective Reinforcement Learning approach for Autonomous Intersection Management", "comment": "6 pages, 5 figures, accepted at the 1st Joint Conference on\n  Computers, Cognition and Communication, Padua, Italy, Sep. 15-18, 2025", "summary": "This study introduces a novel multi-objective reinforcement learning (MORL)\napproach for autonomous intersection management, aiming to balance traffic\nefficiency and environmental sustainability across electric and internal\ncombustion vehicles. The proposed method utilizes MORL to identify\nPareto-optimal policies, with a post-hoc fairness criterion guiding the\nselection of the final policy. Simulation results in a complex intersection\nscenario demonstrate the approach's effectiveness in optimizing traffic\nefficiency and emissions reduction while ensuring fairness across vehicle\ncategories. We believe that this criterion can lay the foundation for ensuring\nequitable service, while fostering safe, efficient, and sustainable practices\nin smart urban mobility.", "AI": {"tldr": "提出一种多目标强化学习方法，用于平衡交通效率和环境可持续性，通过后验公平准则选择最优策略。", "motivation": "解决智能城市交通中交通效率和排放减少的平衡问题，同时确保车辆类别的公平性。", "method": "利用多目标强化学习（MORL）识别Pareto最优策略，并通过后验公平准则选择最终策略。", "result": "在复杂交叉路口场景的模拟中，该方法有效优化了交通效率和排放减少，同时确保公平性。", "conclusion": "该方法为智能城市交通中的公平、高效和可持续实践奠定了基础。"}}
{"id": "2507.09158", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09158", "abs": "https://arxiv.org/abs/2507.09158", "authors": ["Sunil Munthumoduku Krishna Murthy", "Kumar Rajamani", "Srividya Tirunellai Rajamani", "Yupei Li", "Qiyang Sun", "Bjoern W. Schuller"], "title": "Automatic Contouring of Spinal Vertebrae on X-Ray using a Novel Sandwich U-Net Architecture", "comment": null, "summary": "In spinal vertebral mobility disease, accurately extracting and contouring\nvertebrae is essential for assessing mobility impairments and monitoring\nvariations during flexion-extension movements. Precise vertebral contouring\nplays a crucial role in surgical planning; however, this process is\ntraditionally performed manually by radiologists or surgeons, making it\nlabour-intensive, time-consuming, and prone to human error. In particular,\nmobility disease analysis requires the individual contouring of each vertebra,\nwhich is both tedious and susceptible to inconsistencies. Automated methods\nprovide a more efficient alternative, enabling vertebra identification,\nsegmentation, and contouring with greater accuracy and reduced time\nconsumption. In this study, we propose a novel U-Net variation designed to\naccurately segment thoracic vertebrae from anteroposterior view on X-Ray\nimages. Our proposed approach, incorporating a ``sandwich\" U-Net structure with\ndual activation functions, achieves a 4.1\\% improvement in Dice score compared\nto the baseline U-Net model, enhancing segmentation accuracy while ensuring\nreliable vertebral contour extraction.", "AI": {"tldr": "提出了一种改进的U-Net模型，用于从X射线图像中准确分割胸椎，提高了分割精度和效率。", "motivation": "脊柱活动性疾病中，手动椎骨轮廓提取耗时且易出错，自动化方法能提高效率和准确性。", "method": "采用改进的U-Net结构（带双重激活函数的“三明治”结构），用于胸椎分割。", "result": "相比基线U-Net模型，Dice分数提高了4.1%，分割精度更高。", "conclusion": "该方法在椎骨轮廓提取中表现出更高的准确性和可靠性。"}}
{"id": "2507.08901", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08901", "abs": "https://arxiv.org/abs/2507.08901", "authors": ["Zebang Feng", "Miao Fan", "Bao Liu", "Shengtong Xu", "Haoyi Xiong"], "title": "End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles", "comment": "Accepted by ITSC'25", "summary": "High-precision vectorized maps are indispensable for autonomous driving, yet\ntraditional LiDAR-based creation is costly and slow, while single-vehicle\nperception methods lack accuracy and robustness, particularly in adverse\nconditions. This paper introduces EGC-VMAP, an end-to-end framework that\novercomes these limitations by generating accurate, city-scale vectorized maps\nthrough the aggregation of data from crowdsourced vehicles. Unlike prior\napproaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements\nperceived onboard vehicles using a novel Trip-Aware Transformer architecture\nwithin a unified learning process. Combined with hierarchical matching for\nefficient training and a multi-objective loss, our method significantly\nenhances map accuracy and structural robustness compared to single-vehicle\nbaselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP\ndemonstrates superior performance, enabling a scalable, cost-effective solution\nfor city-wide mapping with a reported 90\\% reduction in manual annotation\ncosts.", "AI": {"tldr": "EGC-VMAP是一个端到端框架，通过众包车辆数据生成高精度城市级矢量化地图，显著提升地图精度和鲁棒性。", "motivation": "传统LiDAR制图成本高且速度慢，单车感知方法在恶劣条件下精度和鲁棒性不足。", "method": "采用Trip-Aware Transformer架构，融合多车多时态地图元素，结合分层匹配和多目标损失函数。", "result": "在大规模多城市数据集上验证，性能优于单车基线，手动标注成本降低90%。", "conclusion": "EGC-VMAP为城市级地图提供了一种可扩展、经济高效的解决方案。"}}
{"id": "2507.08916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "论文分析了当前大型语言模型（LLMs）在医学问答中的局限性，重点关注评估数据集的质量，并提出标准化框架的需求。", "motivation": "评估LLMs在医学问答中的表现时，现有数据集缺乏临床真实性、透明性和验证过程，需要更严谨的工具。", "method": "通过审查MedQA、MedMCQA、PubMedQA和MMLU等常用基准数据集，以及医学期刊的挑战问题，分析其优缺点。", "result": "现有数据集普遍缺乏临床现实性和透明度，挑战问题虽有益但规模小、范围窄且可能被LLM训练暴露。", "conclusion": "需要标准化框架和多方合作，以确保数据集和方法严谨、无偏见且反映临床复杂性。"}}
{"id": "2507.08979", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08979", "abs": "https://arxiv.org/abs/2507.08979", "authors": ["Mahdiyar Molahasani", "Azadeh Motamedi", "Michael Greenspan", "Il-Min Kim", "Ali Etemad"], "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection", "comment": "Accepted to ICCV 2025", "summary": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.", "AI": {"tldr": "PRISM是一种无需数据且任务无关的方法，用于减少视觉语言模型（如CLIP）中的隐式偏差，通过两阶段方法生成场景描述并学习投影以减少偏差。", "motivation": "视觉语言模型（VLMs）常因训练数据中的偏差而产生偏斜预测，PRISM旨在无需额外数据或预定义偏差类别的情况下解决这一问题。", "method": "PRISM分两阶段：1）用LLM生成包含虚假相关性的场景描述；2）使用对比式去偏差损失学习投影，将嵌入映射到减少虚假相关性的潜在空间。", "result": "PRISM在Waterbirds和CelebA数据集上优于现有去偏差方法。", "conclusion": "PRISM提供了一种有效且通用的去偏差解决方案，代码已公开。"}}
{"id": "2507.08892", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08892", "abs": "https://arxiv.org/abs/2507.08892", "authors": ["Alexander Sasha Vezhnevets", "Jayd Matyas", "Logan Cross", "Davide Paglieri", "Minsuk Chang", "William A. Cunningham", "Simon Osindero", "William S. Isaac", "Joel Z. Leibo"], "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine", "comment": "13 pages", "summary": "Generative AI can be used in multi-actor environments with purposes ranging\nfrom social science modeling to interactive narrative and AI evaluation.\nSupporting this diversity of use cases -- which we classify as Simulationist,\nDramatist, and Evaluationist -- demands a flexible scenario definition\nframework. We argue here that a good approach is to take inspiration from\ntabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible\nfor the environment and generates all parts of the story not directly\ndetermined by the voluntary actions of player characters. We argue that the\nEntity-Component architectural pattern is useful here. In such a system, the GM\nis not a hardcoded computer game but is itself a configurable entity, composed\nof components just like any other actor. By design, the approach allows for a\nseparation between the underlying implementation details handled by an\nengineer, the creation of reusable components, and their composition and\nconfiguration managed by a designer who constructs entities from the\ncomponents. This separation of concerns is instrumental for achieving rapid\niteration, maintaining modularity, and ultimately to ensure scalability. We\ndescribe the ongoing evolution of the Concordia library in terms of this\nphilosophy, demonstrating how it allows users to effectively configure\nscenarios that align with their specific goals.", "AI": {"tldr": "论文提出了一种基于桌游角色扮演游戏（TTRPGs）的灵活场景定义框架，用于支持生成式AI在多角色环境中的多样化应用。", "motivation": "为了满足生成式AI在模拟、叙事和评估等多样化应用中的需求，需要一个灵活的框架来定义场景。", "method": "采用实体-组件架构模式，将游戏管理员（GM）设计为可配置的实体，由组件构成，实现工程师与设计师的分工协作。", "result": "通过Concordia库的实践，展示了该框架如何支持用户快速配置符合特定目标的场景。", "conclusion": "该框架通过分离关注点，实现了快速迭代、模块化和可扩展性，适用于多样化的生成式AI应用。"}}
{"id": "2507.09503", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09503", "abs": "https://arxiv.org/abs/2507.09503", "authors": ["Zhentong Shao", "Jingtao Qin", "Nanpeng Yu"], "title": "Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem", "comment": "Submitted to IEEE Transactions on Power Systems", "summary": "This paper proposes a neural stochastic optimization method for efficiently\nsolving the two-stage stochastic unit commitment (2S-SUC) problem under\nhigh-dimensional uncertainty scenarios. The proposed method approximates the\nsecond-stage recourse problem using a deep neural network trained to map\ncommitment decisions and uncertainty features to recourse costs. The trained\nnetwork is subsequently embedded into the first-stage UC problem as a\nmixed-integer linear program (MILP), allowing for explicit enforcement of\noperational constraints while preserving the key uncertainty characteristics. A\nscenario-embedding network is employed to enable dimensionality reduction and\nfeature aggregation across arbitrary scenario sets, serving as a data-driven\nscenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and\n118-bus systems demonstrate that the proposed neural two-stage stochastic\noptimization method achieves solutions with an optimality gap of less than 1%,\nwhile enabling orders-of-magnitude speedup compared to conventional MILP\nsolvers and decomposition-based methods. Moreover, the model's size remains\nconstant regardless of the number of scenarios, offering significant\nscalability for large-scale stochastic unit commitment problems.", "AI": {"tldr": "提出了一种神经随机优化方法，用于高效解决高维不确定性下的两阶段随机机组组合问题。", "motivation": "传统方法在处理高维不确定性时效率低下，难以满足大规模问题的需求。", "method": "使用深度神经网络近似第二阶段问题，并将其嵌入到第一阶段问题中，形成混合整数线性规划。", "result": "在IEEE测试系统上，方法的最优性差距小于1%，且计算速度显著提升。", "conclusion": "该方法具有高效性和可扩展性，适用于大规模随机机组组合问题。"}}
{"id": "2507.09227", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09227", "abs": "https://arxiv.org/abs/2507.09227", "authors": ["Sanyam Jain", "Bruna Neves de Freitas", "Andreas Basse-OConnor", "Alexandros Iosifidis", "Ruben Pauwels"], "title": "PanoDiff-SR: Synthesizing Dental Panoramic Radiographs using Diffusion and Super-resolution", "comment": null, "summary": "There has been increasing interest in the generation of high-quality,\nrealistic synthetic medical images in recent years. Such synthetic datasets can\nmitigate the scarcity of public datasets for artificial intelligence research,\nand can also be used for educational purposes. In this paper, we propose a\ncombination of diffusion-based generation (PanoDiff) and Super-Resolution (SR)\nfor generating synthetic dental panoramic radiographs (PRs). The former\ngenerates a low-resolution (LR) seed of a PR (256 X 128) which is then\nprocessed by the SR model to yield a high-resolution (HR) PR of size 1024 X\n512. For SR, we propose a state-of-the-art transformer that learns local-global\nrelationships, resulting in sharper edges and textures. Experimental results\ndemonstrate a Frechet inception distance score of 40.69 between 7243 real and\nsynthetic images (in HR). Inception scores were 2.55, 2.30, 2.90 and 2.98 for\nreal HR, synthetic HR, real LR and synthetic LR images, respectively. Among a\ndiverse group of six clinical experts, all evaluating a mixture of 100\nsynthetic and 100 real PRs in a time-limited observation, the average accuracy\nin distinguishing real from synthetic images was 68.5% (with 50% corresponding\nto random guessing).", "AI": {"tldr": "提出了一种结合扩散生成（PanoDiff）和超分辨率（SR）的方法，用于生成高质量合成牙科全景X光片（PRs）。实验结果显示合成图像质量接近真实图像。", "motivation": "解决医学影像数据稀缺问题，支持AI研究和教育用途。", "method": "使用PanoDiff生成低分辨率种子图像，再通过SR模型提升分辨率。SR模型采用先进的transformer学习局部-全局关系。", "result": "合成图像与真实图像的FID得分为40.69，专家区分准确率为68.5%（接近随机猜测）。", "conclusion": "该方法能生成高质量的合成医学影像，可用于研究和教育。"}}
{"id": "2507.08903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08903", "abs": "https://arxiv.org/abs/2507.08903", "authors": ["Zhongzhang Chen", "Miao Fan", "Shengtong Xu", "Mengmeng Yang", "Kun Jiang", "Xiangzeng Liu", "Haoyi Xiong"], "title": "Multimodal HD Mapping for Intersections by Intelligent Roadside Units", "comment": "Accepted by ITSC'25", "summary": "High-definition (HD) semantic mapping of complex intersections poses\nsignificant challenges for traditional vehicle-based approaches due to\nocclusions and limited perspectives. This paper introduces a novel camera-LiDAR\nfusion framework that leverages elevated intelligent roadside units (IRUs).\nAdditionally, we present RS-seq, a comprehensive dataset developed through the\nsystematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes\nprecisely labelled camera imagery and LiDAR point clouds collected from\nroadside installations, along with vectorized maps for seven intersections\nannotated with detailed features such as lane dividers, pedestrian crossings,\nand stop lines. This dataset facilitates the systematic investigation of\ncross-modal complementarity for HD map generation using IRU data. The proposed\nfusion framework employs a two-stage process that integrates modality-specific\nfeature extraction and cross-modal semantic integration, capitalizing on camera\nhigh-resolution texture and precise geometric data from LiDAR. Quantitative\nevaluations using the RS-seq dataset demonstrate that our multimodal approach\nconsistently surpasses unimodal methods. Specifically, compared to unimodal\nbaselines evaluated on the RS-seq dataset, the multimodal approach improves the\nmean Intersection-over-Union (mIoU) for semantic segmentation by 4\\% over the\nimage-only results and 18\\% over the point cloud-only results. This study\nestablishes a baseline methodology for IRU-based HD semantic mapping and\nprovides a valuable dataset for future research in infrastructure-assisted\nautonomous driving systems.", "AI": {"tldr": "本文提出了一种基于摄像头-LiDAR融合的高清语义地图生成框架，利用智能路边单元（IRUs）解决传统车辆方法的遮挡和视角限制问题，并发布了RS-seq数据集。", "motivation": "传统车辆方法在复杂交叉路口的高清语义地图生成中存在遮挡和视角限制问题，需要更高效的解决方案。", "method": "采用两阶段融合框架，结合摄像头的高分辨率纹理和LiDAR的精确几何数据，利用RS-seq数据集进行验证。", "result": "多模态方法在语义分割上的mIoU比单模态方法分别提高了4%（图像）和18%（点云）。", "conclusion": "本研究为基于IRU的高清语义地图生成提供了基准方法和数据集，推动了基础设施辅助自动驾驶系统的发展。"}}
{"id": "2507.08924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "论文介绍了两个韩语专家级基准测试KMMLU-Redux和KMMLU-Pro，用于评估大语言模型在韩国工业领域的适用性。", "motivation": "开发能够全面评估大语言模型在学术和工业领域适用性的基准测试。", "method": "重构KMMLU为KMMLU-Redux，并基于韩国国家专业执照考试创建KMMLU-Pro。", "result": "实验表明这两个基准测试能全面代表韩国工业知识。", "conclusion": "公开数据集，为评估大语言模型在韩国工业领域的适用性提供了有效工具。"}}
{"id": "2507.08981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08981", "abs": "https://arxiv.org/abs/2507.08981", "authors": ["Hanbyel Cho", "Jaesung Ahn", "Yooshin Cho", "Junmo Kim"], "title": "Video Inference for Human Mesh Recovery with Vision Transformer", "comment": "Accepted to IEEE FG 2023", "summary": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.", "AI": {"tldr": "提出了一种结合时空和运动学信息的HMR-ViT方法，通过构建时空-运动学特征图像并使用Vision Transformer，显著提升了人体网格恢复的准确性。", "motivation": "现有HMR方法仅利用时空或运动学信息，缺乏同时利用两者的方法，导致性能受限。", "method": "构建时空-运动学特征图像，使用通道重排矩阵（CRM）优化特征排列，通过Vision Transformer和回归网络推断SMPL参数。", "result": "在3DPW和Human3.6M数据集上表现优异，验证了方法的有效性。", "conclusion": "HMR-ViT通过结合时空和运动学信息，显著提升了人体网格恢复的准确性。"}}
{"id": "2507.09080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09080", "abs": "https://arxiv.org/abs/2507.09080", "authors": ["Athanasios Trantas", "Martino Mensio", "Stylianos Stasinos", "Sebastian Gribincea", "Taimur Khan", "Damian Podareanu", "Aliene van der Veen"], "title": "BioAnalyst: A Foundation Model for Biodiversity", "comment": null, "summary": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges.", "AI": {"tldr": "BioAnalyst是一个基于Transformer架构的基础模型，专为生物多样性分析和保护规划设计，通过多模态数据预训练，适用于多种下游任务，如物种分布建模和栖息地评估。", "motivation": "生物多样性快速丧失对生态研究和保护策略提出严峻挑战，需要新的监测和预测工具。AI基础模型在科学领域的成功应用为生物多样性保护提供了新思路。", "method": "BioAnalyst采用Transformer架构，预训练于多模态数据集（物种记录、遥感数据、气候变量等），可微调用于多种下游任务。", "result": "在数据稀缺场景下，BioAnalyst在两种用例中表现出优于现有方法的泛化能力，为生态预测设立了新的准确性基准。", "conclusion": "BioAnalyst的开放发布旨在促进生物多样性建模的合作，推动AI解决生态挑战。"}}
{"id": "2507.09646", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09646", "abs": "https://arxiv.org/abs/2507.09646", "authors": ["Lucian Cristian Iacob", "Máté Szécsi", "Gerben Izaak Beintema", "Maarten Schoukens", "Roland Tóth"], "title": "Learning Koopman Models From Data Under General Noise Conditions", "comment": "Submitted to SIAM Journal on Applied Dynamical Systems (SIADS)", "summary": "This paper presents a novel identification approach of Koopman models of\nnonlinear systems with inputs under rather general noise conditions. The method\nuses deep state-space encoders based on the concept of state reconstructability\nand an efficient multiple-shooting formulation of the squared loss of the\nprediction error to estimate the dynamics and the lifted state from\ninput-output data. Furthermore, the Koopman model structure includes an\ninnovation noise term that is used to handle process and measurement noise. It\nis shown that the proposed approach is statistically consistent and\ncomputationally efficient due to the multiple-shooting formulation where, on\nsubsections of the data, multi-step prediction errors can be calculated in\nparallel. The latter allows for efficient batch optimization of the network\nparameters and, at the same time, excellent long-term prediction capabilities\nof the obtained models. The performance of the approach is illustrated by\nnonlinear benchmark examples.", "AI": {"tldr": "提出了一种基于深度状态空间编码器和多重射击优化的Koopman模型识别方法，适用于非线性系统输入噪声条件下的动态估计。", "motivation": "解决非线性系统在噪声条件下的动态建模问题，提升模型的一致性和计算效率。", "method": "使用深度状态空间编码器结合状态重构性和多重射击优化方法，通过输入-输出数据估计动态和提升状态。", "result": "方法在统计上一致且计算高效，具有优秀的长期预测能力，并通过非线性基准测试验证了性能。", "conclusion": "该方法为非线性系统噪声条件下的建模提供了一种高效且一致的解决方案。"}}
{"id": "2507.09236", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09236", "abs": "https://arxiv.org/abs/2507.09236", "authors": ["Eric Bezzam", "Martin Vetterli"], "title": "Encryption and Authentication with a Lensless Camera Based on a Programmable Mask", "comment": "6 pages main, 3 pages supplementary, accepted to ICME 2025", "summary": "Lensless cameras replace traditional optics with thin masks, leading to\nhighly multiplexed measurements akin to encryption. However, static masks in\nconventional designs leave systems vulnerable to simple attacks. This work\nexplores the use of programmable masks to enhance security by dynamically\nvarying the mask patterns. We perform our experiments with a low-cost system\n(around 100 USD) based on a liquid crystal display. Experimental results\ndemonstrate that variable masks successfully block a variety of attacks while\nenabling high-quality recovery for legitimate users. The system's encryption\nstrength exceeds AES-256, achieving effective key lengths over 2'500 bits.\nAdditionally, we demonstrate how a programmable mask enables robust\nauthentication and verification, as each mask pattern leaves a unique\nfingerprint on the image. When combined with a lensed system, lensless\nmeasurements can serve as analog certificates, providing a novel solution for\nverifying image authenticity and combating deepfakes.", "AI": {"tldr": "论文提出了一种使用可编程掩模增强无透镜相机安全性的方法，通过动态变化掩模图案抵御攻击，同时实现高质量图像恢复。", "motivation": "传统无透镜相机的静态掩模易受攻击，需要增强安全性。", "method": "采用基于液晶显示器的低成本系统，动态变化掩模图案。", "result": "实验证明可变掩模能有效抵御攻击，加密强度超过AES-256，密钥长度超2500位。", "conclusion": "可编程掩模不仅提升安全性，还可用于图像认证和防伪，为对抗深度伪造提供新方案。"}}
{"id": "2507.09117", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09117", "abs": "https://arxiv.org/abs/2507.09117", "authors": ["Gagan Khandate"], "title": "Towards Human-level Dexterity via Robot Learning", "comment": "PhD thesis", "summary": "Dexterous intelligence -- the ability to perform complex interactions with\nmulti-fingered hands -- is a pinnacle of human physical intelligence and\nemergent higher-order cognitive skills. However, contrary to Moravec's paradox,\ndexterous intelligence in humans appears simple only superficially. Many\nmillion years were spent co-evolving the human brain and hands including rich\ntactile sensing. Achieving human-level dexterity with robotic hands has long\nbeen a fundamental goal in robotics and represents a critical milestone toward\ngeneral embodied intelligence. In this pursuit, computational sensorimotor\nlearning has made significant progress, enabling feats such as arbitrary\nin-hand object reorientation. However, we observe that achieving higher levels\nof dexterity requires overcoming very fundamental limitations of computational\nsensorimotor learning.\n  I develop robot learning methods for highly dexterous multi-fingered\nmanipulation by directly addressing these limitations at their root cause.\nChiefly, through key studies, this disseration progressively builds an\neffective framework for reinforcement learning of dexterous multi-fingered\nmanipulation skills. These methods adopt structured exploration, effectively\novercoming the limitations of random exploration in reinforcement learning. The\ninsights gained culminate in a highly effective reinforcement learning that\nincorporates sampling-based planning for direct exploration. Additionally, this\nthesis explores a new paradigm of using visuo-tactile human demonstrations for\ndexterity, introducing corresponding imitation learning techniques.", "AI": {"tldr": "论文探讨了如何通过改进强化学习方法实现机器人多指灵巧操作，克服了传统随机探索的局限性，并结合视觉触觉人类示范提升灵巧性。", "motivation": "人类灵巧智能是多指操作与高阶认知能力的结合，但机器人实现类似能力面临根本性挑战。", "method": "采用结构化探索和基于采样的规划强化学习，并结合视觉触觉人类示范的模仿学习技术。", "result": "开发了高效的强化学习框架，显著提升了机器人多指操作的灵巧性。", "conclusion": "通过改进探索方法和引入人类示范，为机器人灵巧操作提供了有效解决方案。"}}
{"id": "2507.08967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "SIMS是一种无需外部监督的自改进模型引导框架，通过自主生成和优化对比样本，显著提升了大型语言模型（LLM）的引导效果和适应性。", "motivation": "传统模型引导方法依赖外部标注数据，限制了适应性和效果。SIMS旨在通过自改进框架解决这一问题。", "method": "SIMS通过迭代自改进循环生成和优化对比样本，并结合提示排序和对比采样等策略提升引导效果。", "result": "实验表明，SIMS在多种LLM和基准测试中显著优于现有方法。", "conclusion": "自改进模型引导是未来LLM推理对齐研究的有前景方向。"}}
{"id": "2507.09005", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.09005", "abs": "https://arxiv.org/abs/2507.09005", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "comment": null, "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.", "AI": {"tldr": "提出了一种结合NeRF和MPM的新框架，通过视觉观测推断颗粒材料特性，摩擦角估计误差在2度以内。", "motivation": "解决在无法直接测量的情况下，通过视觉观测表征颗粒材料特性的问题。", "method": "生成合成实验数据，用NeRF重建3D几何，MPM模拟未知摩擦角，通过贝叶斯优化最小化图像损失。", "result": "摩擦角估计误差在2度以内，验证了方法的有效性。", "conclusion": "该方法为颗粒材料特性表征提供了实用解决方案。"}}
{"id": "2507.09089", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "pdf": "https://arxiv.org/pdf/2507.09089", "abs": "https://arxiv.org/abs/2507.09089", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "comment": "50 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design.", "AI": {"tldr": "研究发现，尽管开发者预期AI工具能缩短任务完成时间，但实际使用AI工具反而增加了19%的完成时间，与经济学和ML专家的预测相反。", "motivation": "探讨AI工具在现实软件开发中对开发者生产力的实际影响。", "method": "通过随机对照试验（RCT），16名有中等AI经验的开发者在成熟项目中完成246项任务，随机允许或禁止使用2025年的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）。", "result": "使用AI工具导致任务完成时间增加19%，与开发者预期的20%缩短和专家预测的38-39%缩短相反。", "conclusion": "AI工具在实际开发中可能并未如预期提升生产力，甚至可能减缓开发速度，这一现象需要进一步研究以排除实验设计的影响。"}}
{"id": "2507.09685", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09685", "abs": "https://arxiv.org/abs/2507.09685", "authors": ["Yutong Li", "Ilya Kolmanovsky"], "title": "Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control", "comment": "6 pages, 5 figures", "summary": "Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid\ndisorders but carry significant risks when administered chronically at high\ndoses. Precise long-term control of gastric acidity is challenged by the\nimpracticality of invasive gastric acid monitoring beyond 72 hours and wide\ninter-patient variability. We propose a noninvasive, symptom-based framework\nthat tailors PPI dosing solely on patient-reported reflux and digestive symptom\npatterns. A Bayesian Neural Network prediction model learns to predict patient\nsymptoms and quantifies its uncertainty from historical symptom scores, meal,\nand PPIs intake data. These probabilistic forecasts feed a chance-constrained\nModel Predictive Control (MPC) algorithm that dynamically computes future PPI\ndoses to minimize drug usage while enforcing acid suppression with high\nconfidence - without any direct acid measurement. In silico studies over\ndiverse dietary schedules and virtual patient profiles demonstrate that our\nlearning-augmented MPC reduces total PPI consumption by 65 percent compared to\nstandard fixed regimens, while maintaining acid suppression with at least 95\npercent probability. The proposed approach offers a practical path to\npersonalized PPI therapy, minimizing treatment burden and overdose risk without\ninvasive sensors.", "AI": {"tldr": "提出了一种基于症状的非侵入性PPI剂量调整框架，通过贝叶斯神经网络和模型预测控制算法，减少65%的PPI用量，同时保持95%以上的抑酸效果。", "motivation": "长期高剂量PPI治疗存在风险，但传统侵入性胃酸监测不实用，且患者间差异大。", "method": "使用贝叶斯神经网络预测症状，结合模型预测控制算法动态调整PPI剂量。", "result": "模拟研究表明，该方法比固定剂量方案减少65%的PPI用量，且抑酸效果保持在95%以上。", "conclusion": "该方法为个性化PPI治疗提供了可行方案，减少治疗负担和过量风险。"}}
{"id": "2507.09393", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09393", "abs": "https://arxiv.org/abs/2507.09393", "authors": ["Necmettin Bayar", "Isin Erer", "Deniz Kumlu"], "title": "Deep Image Prior Assisted ISAR Imaging for Missing Data Case", "comment": null, "summary": "In Inverse Synthetic Aperture Radar (ISAR), random missing entries of the\nreceived radar echo matrix deteriorate the imaging quality, compromising target\ndistinction from the background. Compressive sensing techniques or matrix\ncompletion prior to conventional imaging have been used in recent years to\nsolve this issue. However, while the former techniques fail to preserve target\ncontinuity due to the sparsity constraint, the latter fails for high missing\nratios. This paper proposes to use deep image prior (DIP) to complete the\ncomplex radar data and then obtain the radar image by conventional Fourier\nimaging. Real and imaginary parts are separately completed by independent deep\nstructures and then put together for the imaging part. The proposed DIP based\nimaging method has been compared with IALM, 2D-SL0 and NNM methods visually and\nquantitatively for both simulated and real data. The results demonstrate an\nincrease of 100% for some extreme cases in terms of RMSE, 50% increase on\nCorrelation and 30% increase on IC metrics quantitatively.", "AI": {"tldr": "论文提出了一种基于深度图像先验（DIP）的方法，用于完成ISAR中的缺失雷达数据，并通过传统傅里叶成像提高成像质量。", "motivation": "ISAR中雷达回波矩阵的随机缺失会降低成像质量，现有方法（如压缩感知或矩阵补全）在高缺失率或稀疏约束下表现不佳。", "method": "使用独立的深度结构分别补全雷达数据的实部和虚部，再通过傅里叶成像生成雷达图像。", "result": "与IALM、2D-SL0和NNM方法相比，DIP方法在RMSE、相关性和IC指标上分别提高了100%、50%和30%。", "conclusion": "DIP方法在极端情况下显著提升了ISAR成像质量，优于现有技术。"}}
{"id": "2507.09123", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09123", "abs": "https://arxiv.org/abs/2507.09123", "authors": ["Ziyan Gao", "Lijun Wang", "Yuntao Kong", "Nak Young Chong"], "title": "Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning", "comment": null, "summary": "The Online Bin Packing Problem (OBPP) is a sequential decision-making task in\nwhich each item must be placed immediately upon arrival, with no knowledge of\nfuture arrivals. Although recent deep-reinforcement-learning methods achieve\nsuperior volume utilization compared with classical heuristics, the learned\npolicies cannot ensure the structural stability of the bin and lack mechanisms\nfor safely reconfiguring the bin when a new item cannot be placed directly. In\nthis work, we propose a novel framework that integrates packing policy with\nstructural stability validation and heuristic planning to overcome these\nlimitations. Specifically, we introduce the concept of Load Bearable Convex\nPolygon (LBCP), which provides a computationally efficient way to identify\nstable loading positions that guarantee no bin collapse. Additionally, we\npresent Stable Rearrangement Planning (SRP), a module that rearranges existing\nitems to accommodate new ones while maintaining overall stability. Extensive\nexperiments on standard OBPP benchmarks demonstrate the efficiency and\ngeneralizability of our LBCP-based stability validation, as well as the\nsuperiority of SRP in finding the effort-saving rearrangement plans. Our method\noffers a robust and practical solution for automated packing in real-world\nindustrial and logistics applications.", "AI": {"tldr": "提出了一种结合包装策略、结构稳定性验证和启发式规划的新框架，解决在线装箱问题中的结构稳定性和安全重新配置问题。", "motivation": "现有深度强化学习方法在在线装箱问题中虽能提高体积利用率，但无法保证箱子的结构稳定性，且缺乏安全重新配置机制。", "method": "引入负载可承载凸多边形（LBCP）概念进行稳定性验证，并提出稳定重排规划（SRP）模块以安全重新配置箱子。", "result": "在标准OBPP基准测试中，LBCP验证高效且通用，SRP在节省重排成本方面表现优越。", "conclusion": "该方法为工业与物流中的自动化装箱提供了鲁棒且实用的解决方案。"}}
{"id": "2507.08969", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "研究发现，电子健康记录（EHR）中存在对特定患者群体的污名化语言，尤其是黑人、低收入患者和某些疾病患者。", "motivation": "探讨EHR中污名化语言的普遍性及其对不同患者群体的影响。", "method": "通过扩展词典匹配和监督学习分类器识别MIMIC-III EHR中的怀疑标记和污名化标签，并使用泊松回归模型分析预测因素。", "result": "黑人、低收入患者和某些疾病患者的污名化标签率更高；护士和社会工作者的污名化语言使用率较高。", "conclusion": "污名化语言在历史上被污名化的患者中更为普遍，且由多种医疗提供者使用。"}}
{"id": "2507.09008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09008", "abs": "https://arxiv.org/abs/2507.09008", "authors": ["Xiwei Xuan", "Xiaoqi Wang", "Wenbin He", "Jorge Piazentin Ono", "Liang Gou", "Kwan-Liu Ma", "Liu Ren"], "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels", "comment": "IEEE Transactions on Visualization and Computer Graphics (2025)", "summary": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.", "AI": {"tldr": "VISTA是一个视觉分析框架，旨在提升多模态基础模型生成标签的质量，通过结合多阶段数据验证策略和人类专家知识，解决现有方法在数据质量验证上的不足。", "motivation": "现有方法多关注数据量而非质量，缺乏对多模态基础模型生成标签质量的全面研究，验证大规模无标注数据存在挑战。", "method": "提出VISTA框架，结合多阶段数据验证策略和人类专家知识，识别和修正标签中的潜在问题。", "result": "在开放词汇图像分割领域的两个基准数据集上，通过定量和定性分析验证了VISTA的有效性。", "conclusion": "VISTA通过提升数据质量，显著增强了多模态模型在下游任务中的表现。"}}
{"id": "2507.09179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09179", "abs": "https://arxiv.org/abs/2507.09179", "authors": ["Ronghua Shi", "Yiou Liu", "Xinyu Ying", "Yang Tan", "Yuchun Feng", "Lynn Ai", "Bill Shi", "Xuhui Wang", "Zhuang Liu"], "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System", "comment": null, "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.", "AI": {"tldr": "提出了一种基于多智能体强化学习（MARL）的框架，用于检测去中心化金融（DeFi）中的市场操纵行为，通过动态对抗游戏建模操纵者与检测器的交互。", "motivation": "DeFi的无许可创新带来了市场操纵问题，缺乏中心化监管导致恶意行为频发。", "method": "采用MARL框架，结合GRPO优化、理论驱动的奖励函数和多模态智能体管道，整合语义特征、社交图信号和链上数据。", "result": "在真实数据和对抗模拟中验证，Hide-and-Shill系统在检测准确性和因果归因方面表现优异。", "conclusion": "该研究为去中心化市场情报提供了新范式，推动了多智能体系统与金融监管的结合。"}}
{"id": "2507.09726", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09726", "abs": "https://arxiv.org/abs/2507.09726", "authors": ["Boyou Chen", "Kaihan Zhang", "Austin Moore", "Bochen Jia", "Mengqiu Cao"], "title": "Electric Vehicle Public Charging Equity Considerations: A Systematic Review", "comment": null, "summary": "Public electric vehicle (EV) charging infrastructure is crucial for\naccelerating EV adoption and reducing transportation emissions; however,\ndisparities in infrastructure access have raised significant equity concerns.\nThis systematic review synthesizes existing knowledge and identifies gaps\nregarding equity in EV public charging research. Following structured review\nprotocols, 91 peer-reviewed studies from Scopus and Google Scholar were\nanalyzed, focusing explicitly on equity considerations. The findings indicate\nthat current research on EV public charging equity mainly adopted geographic\ninformation systems (GIS), network optimization, behavioral modeling, and\nhybrid analytical frameworks, yet lacks consistent normative frameworks for\nassessing equity outcomes. Equity assessments highlight four key dimensions:\nspatial accessibility, cost burdens, reliability and usability, and user\nawareness and trust. Socio-economic disparities, particularly income, housing\ntenure, and ethnicity, frequently exacerbate inequitable access,\ndisproportionately disadvantaging low-income, renter, and minority populations.\nAdditionally, infrastructure-specific choices, including charger reliability,\nstrategic location, and pricing strategies, significantly influence adoption\npatterns and equity outcomes. However, existing literature primarily reflects\nNorth American, European, and Chinese contexts, revealing substantial\ngeographical and methodological limitations. This review suggests the need for\nmore robust normative evaluations of equity, comprehensive demographic data\nintegration, and advanced methodological frameworks, thereby guiding targeted,\ninclusive, and context-sensitive infrastructure planning and policy\ninterventions.", "AI": {"tldr": "本文系统综述了电动汽车公共充电基础设施的公平性问题，总结了现有研究的方法和差距，并提出了未来研究方向。", "motivation": "加速电动汽车普及和减少交通排放需要公平的充电基础设施，但现有研究缺乏一致的公平评估框架。", "method": "通过结构化综述协议分析了91篇同行评审研究，重点关注GIS、网络优化、行为建模和混合分析框架。", "result": "研究发现充电公平性涉及空间可达性、成本负担、可靠性和用户信任等维度，社会经济差异加剧了不公平。", "conclusion": "未来研究需加强公平性评估、整合人口数据并开发更先进的方法框架，以指导包容性政策。"}}
{"id": "2507.09608", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09608", "abs": "https://arxiv.org/abs/2507.09608", "authors": ["Mehmet Onurcan Kaya", "Figen S. Oktem"], "title": "prNet: Data-Driven Phase Retrieval via Stochastic Refinement", "comment": null, "summary": "We propose a novel framework for phase retrieval that leverages Langevin\ndynamics to enable efficient posterior sampling, yielding reconstructions that\nexplicitly balance distortion and perceptual quality. Unlike conventional\napproaches that prioritize pixel-wise accuracy, our method navigates the\nperception-distortion tradeoff through a principled combination of stochastic\nsampling, learned denoising, and model-based updates. The framework comprises\nthree variants of increasing complexity, integrating theoretically grounded\nLangevin inference, adaptive noise schedule learning, parallel reconstruction\nsampling, and warm-start initialization from classical solvers. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\nacross multiple benchmarks, both in terms of fidelity and perceptual quality.", "AI": {"tldr": "提出了一种基于Langevin动力学的新型相位检索框架，通过高效后验采样平衡失真和感知质量。", "motivation": "传统方法过于关注像素级精度，而忽略了感知质量与失真之间的权衡。", "method": "结合随机采样、学习去噪和基于模型的更新，提出三种复杂度递增的变体，包括Langevin推断、自适应噪声调度学习和并行重建采样。", "result": "在多个基准测试中实现了最先进的性能，兼顾保真度和感知质量。", "conclusion": "该框架为相位检索提供了一种高效且平衡的方法，显著提升了重建质量。"}}
{"id": "2507.09160", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09160", "abs": "https://arxiv.org/abs/2507.09160", "authors": ["Jialei Huang", "Shuo Wang", "Fanqi Lin", "Yihang Hu", "Chuan Wen", "Yang Gao"], "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown remarkable achievements,\ndriven by the rich implicit knowledge of their vision-language components.\nHowever, achieving generalist robotic agents demands precise grounding into\nphysical interactions, especially in contact-rich scenarios where fine-grained\nforce control is essential. We advance VLAs' implicit knowledge beyond\nidentifying what to do, towards guiding how to physically interact with real\nworld. This paper introduces Tactile-VLA, a novel framework that deeply fuses\nvision, language, action, and tactile sensing. This framework incorporates a\nhybrid position-force controller to translate the model's intentions into\nprecise physical actions and a reasoning module that allows the robot to adapt\nits strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's\neffectiveness and generalizability in three key aspects: (1) enabling\ntactile-aware instruction following, (2) utilizing tactile-relevant\ncommonsense, and (3) facilitating adaptive tactile-involved reasoning. A key\nfinding is that the VLM's prior knowledge already contains semantic\nunderstanding of physical interaction; by connecting it to the robot's tactile\nsensors with only a few demonstrations, we can activate this prior knowledge to\nachieve zero-shot generalization in contact-rich tasks.", "AI": {"tldr": "Tactile-VLA框架融合视觉、语言、动作和触觉感知，通过混合力-位置控制器和触觉反馈推理模块，实现精确物理交互和自适应策略。实验证明其在触觉感知指令跟随、触觉常识利用和自适应推理方面的有效性。", "motivation": "提升Vision-Language-Action（VLA）模型在物理交互中的精确性，尤其是在需要精细力控制的接触丰富场景中。", "method": "提出Tactile-VLA框架，结合视觉、语言、动作和触觉感知，采用混合力-位置控制器和触觉反馈推理模块。", "result": "实验显示框架在触觉感知指令跟随、触觉常识利用和自适应推理方面表现优异，且能通过少量演示激活VLM的先验知识实现零样本泛化。", "conclusion": "Tactile-VLA成功将VLA模型的隐式知识扩展到物理交互领域，为通用机器人代理的发展提供了重要支持。"}}
{"id": "2507.09011", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "研究通过Ganzflicker诱导的视觉幻觉，探讨了不同视觉想象能力个体在幻觉内容上的差异。强想象者描述复杂自然内容，弱想象者报告简单几何模式。视觉语言模型能更好地捕捉这些差异。", "motivation": "探讨视觉想象能力（从无想象到典型想象再到生动想象）如何影响Ganzflicker诱导的幻觉内容。", "method": "使用自然语言处理工具分析4000多名参与者对幻觉的自由文本描述，比较不同想象能力个体的描述差异。", "result": "强想象者描述复杂自然内容，弱想象者报告简单几何模式；视觉语言模型比纯文本模型更能捕捉这些差异。", "conclusion": "研究结果可能反映了早期视觉区域与高阶区域在协调上的个体差异，与想象能力谱相关。"}}
{"id": "2507.09036", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09036", "abs": "https://arxiv.org/abs/2507.09036", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Hendrik Möller", "Ilhem Isra Mekki", "Josef A. Buchner", "Anton Schmick", "Arianna Pfiffer", "Eva Oswald", "Lucas Zimmer", "Ezequiel de la Rosa", "Sarthak Pati", "Julian Canisius", "Arianna Piffer", "Ujjwal Baid", "Mahyar Valizadeh", "Akis Linardos", "Jan C. Peeken", "Surprosanna Shit", "Felix Steinbauer", "Daniel Rueckert", "Rolf Heckemann", "Spyridon Bakas", "Jan Kirschke", "Constantin von See", "Ivan Ezhov", "Marie Piraud", "Benedikt Wiestler", "Bjoern Menze"], "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis", "comment": "16p, 3f", "summary": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.", "AI": {"tldr": "BrainLesion Suite是一个用于构建模块化脑部病变图像分析管道的Python工具包，旨在简化复杂工作流程的开发。", "motivation": "为临床和科研实践提供高效、低认知负担的脑部病变图像分析工具。", "method": "基于Pythonic原则设计，包含预处理模块（如配准、去颅骨等），并利用BraTS挑战赛算法合成缺失模态、修复病变及生成肿瘤分割。", "result": "支持量化分割模型性能，适用于脑部病变（如胶质瘤、转移瘤和多发性硬化）及其他生物医学图像分析。", "conclusion": "BrainLesion Suite是一个灵活的工具包，可通过GitHub获取其模块和教程。"}}
{"id": "2507.09329", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09329", "abs": "https://arxiv.org/abs/2507.09329", "authors": ["Matous Kozak", "Roshanak Zilouchian Moghaddam", "Siva Sivaraman"], "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents", "comment": "15 pages", "summary": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents.", "AI": {"tldr": "论文首次系统评估了基于LLM的编程代理的安全性，发现21%的操作存在安全隐患，并提出了检测系统和缓解策略。", "motivation": "理解基于LLM的编程代理在软件开发中的安全影响，填补现有研究的空白。", "method": "分析了12,000多个操作，涉及5种先进模型（如GPT-4o、GPT-4.1等）在93个实际软件任务中的表现。", "result": "21%的操作存在安全隐患，信息泄露（CWE-200）最常见；GPT-4.1的缓解成功率高达96.8%。", "conclusion": "研究为评估编程代理安全性提供了框架，并强调下一代LLM编程代理需注重安全设计。"}}
{"id": "2507.09755", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09755", "abs": "https://arxiv.org/abs/2507.09755", "authors": ["Amir Farakhor", "Iman Askari", "Di Wu", "Huazhen Fang"], "title": "Optimal Power Management of Battery Energy Storage Systems via Ensemble Kalman Inversion", "comment": null, "summary": "Optimal power management of battery energy storage systems (BESS) is crucial\nfor their safe and efficient operation. Numerical optimization techniques are\nfrequently utilized to solve the optimal power management problems. However,\nthese techniques often fall short of delivering real-time solutions for\nlarge-scale BESS due to their computational complexity. To address this issue,\nthis paper proposes a computationally efficient approach. We introduce a new\nset of decision variables called power-sharing ratios corresponding to each\ncell, indicating their allocated power share from the output power demand. We\nthen formulate an optimal power management problem to minimize the system-wide\npower losses while ensuring compliance with safety, balancing, and power\nsupply-demand match constraints. To efficiently solve this problem, a\nparameterized control policy is designed and leveraged to transform the optimal\npower management problem into a parameter estimation problem. We then implement\nthe ensemble Kalman inversion to estimate the optimal parameter set. The\nproposed approach significantly reduces computational requirements due to 1)\nthe much lower dimensionality of the decision parameters and 2) the estimation\ntreatment of the optimal power management problem. Finally, we conduct\nextensive simulations to validate the effectiveness of the proposed approach.\nThe results show promise in accuracy and computation time compared with\nexplored numerical optimization techniques.", "AI": {"tldr": "提出一种计算高效的方法，通过功率分配比和参数化控制策略优化电池储能系统的功率管理，显著降低计算复杂度。", "motivation": "解决传统数值优化技术在大规模电池储能系统中实时性不足的问题。", "method": "引入功率分配比作为决策变量，设计参数化控制策略，将优化问题转化为参数估计问题，并使用集成卡尔曼反演求解。", "result": "仿真验证了该方法在精度和计算时间上的优越性。", "conclusion": "该方法为大规模电池储能系统的实时功率管理提供了高效解决方案。"}}
{"id": "2507.09609", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09609", "abs": "https://arxiv.org/abs/2507.09609", "authors": ["Mehmet Onurcan Kaya", "Figen S. Oktem"], "title": "I2I-PR: Deep Iterative Refinement for Phase Retrieval using Image-to-Image Diffusion Models", "comment": null, "summary": "Phase retrieval involves recovering a signal from intensity-only\nmeasurements, crucial in many fields such as imaging, holography, optical\ncomputing, crystallography, and microscopy. Although there are several\nwell-known phase retrieval algorithms, including classical iterative solvers,\nthe reconstruction performance often remains sensitive to initialization and\nmeasurement noise. Recently, image-to-image diffusion models have gained\ntraction in various image reconstruction tasks, yielding significant\ntheoretical insights and practical breakthroughs. In this work, we introduce a\nnovel phase retrieval approach based on an image-to-image diffusion framework\ncalled Inversion by Direct Iteration. Our method begins with an enhanced\ninitialization stage that leverages a hybrid iterative technique, combining the\nHybrid Input-Output and Error Reduction methods and incorporating a novel\nacceleration mechanism to obtain a robust crude estimate. Then, it iteratively\nrefines this initial crude estimate using the learned image-to-image pipeline.\nOur method achieves substantial improvements in both training efficiency and\nreconstruction quality. Furthermore, our approach utilizes aggregation\ntechniques to refine quality metrics and demonstrates superior results compared\nto both classical and contemporary techniques. This highlights its potential\nfor effective and efficient phase retrieval across various applications.", "AI": {"tldr": "提出了一种基于图像到图像扩散框架的新型相位检索方法，结合混合迭代技术和加速机制，显著提升了重建质量和训练效率。", "motivation": "传统相位检索算法对初始化和测量噪声敏感，扩散模型在图像重建任务中表现出色，因此探索其在相位检索中的应用。", "method": "采用混合输入-输出和误差减少方法的混合迭代技术，结合加速机制生成初始估计，再通过图像到图像扩散框架迭代优化。", "result": "在训练效率和重建质量上均有显著提升，优于传统和现代方法。", "conclusion": "该方法在相位检索中表现出高效性和有效性，具有广泛的应用潜力。"}}
{"id": "2507.09167", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09167", "abs": "https://arxiv.org/abs/2507.09167", "authors": ["Michal Vavrecka", "Radoslav Skoviera", "Gabriela Sejnova", "Karla Stepanova"], "title": "PRAG: Procedural Action Generator", "comment": null, "summary": "We present a novel approach for the procedural construction of multi-step\ncontact-rich manipulation tasks in robotics. Our generator takes as input\nuser-defined sets of atomic actions, objects, and spatial predicates and\noutputs solvable tasks of a given length for the selected robotic environment.\nThe generator produces solvable tasks by constraining all possible\n(nonsolvable) combinations by symbolic and physical validation. The symbolic\nvalidation checks each generated sequence for logical and operational\nconsistency, and also the suitability of object-predicate relations. Physical\nvalidation checks whether tasks can be solved in the selected robotic\nenvironment. Only the tasks that passed both validators are retained. The\noutput from the generator can be directly interfaced with any existing\nframework for training robotic manipulation tasks, or it can be stored as a\ndataset of curated robotic tasks with detailed information about each task.\nThis is beneficial for RL training as there are dense reward functions and\ninitial and goal states paired with each subgoal. It allows the user to measure\nthe semantic similarity of all generated tasks. We tested our generator on\nsequences of up to 15 actions resulting in millions of unique solvable\nmulti-step tasks.", "AI": {"tldr": "提出了一种用于机器人多步接触丰富操作任务程序化构建的新方法，通过符号和物理验证生成可解任务。", "motivation": "解决机器人操作任务中多步接触丰富任务的程序化生成问题，为训练提供密集奖励和详细任务信息。", "method": "输入原子动作、对象和空间谓词，通过符号和物理验证生成可解任务序列。", "result": "生成了数百万个独特的可解多步任务，适用于现有机器人训练框架。", "conclusion": "该方法能高效生成可解任务，为机器人训练提供丰富且语义可衡量的数据集。"}}
{"id": "2507.09025", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "Lizard是一个线性化框架，将预训练的Transformer大语言模型转化为支持无限上下文生成的次二次复杂度架构，解决了内存和计算瓶颈问题。", "motivation": "Transformer大语言模型在长上下文生成时面临内存和计算瓶颈，Lizard旨在通过次二次复杂度注意力机制解决这一问题。", "method": "Lizard引入次二次注意力机制，结合门控模块和混合机制（全局压缩与局部滑动窗口注意力），并采用硬件感知算法加速训练。", "result": "实验表明，Lizard在标准语言建模任务中几乎无损恢复教师模型性能，在MMLU基准上提升18分，并在关联召回任务中表现显著提升。", "conclusion": "Lizard通过灵活的架构设计和高效的注意力机制，显著提升了长上下文生成的能力和性能。"}}
{"id": "2507.09052", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09052", "abs": "https://arxiv.org/abs/2507.09052", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "comment": "20 pages, 11 figures", "summary": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.", "AI": {"tldr": "论文提出两种对比损失函数，提升类别不平衡扩散模型中尾部类别的图像多样性，同时保持头部类别的保真度和多样性。", "motivation": "类别不平衡数据导致尾部类别图像合成多样性不足，需在不影响头部类别的情况下提升尾部类别多样性。", "method": "引入无监督InfoNCE损失和MSE损失，通过对比学习和条件-无条件对齐增强尾部类别多样性。", "result": "方法在多个数据集（如CIFAR10/100-LT等）上优于标准DDPM和其他替代方法。", "conclusion": "对比学习框架简单有效，成功应用于类别不平衡扩散模型，显著提升尾部类别多样性。"}}
{"id": "2507.09369", "categories": ["cs.AI", "68T01", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.09369", "abs": "https://arxiv.org/abs/2507.09369", "authors": ["Andrew Critch", "Jacob Tsimerman"], "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "comment": null, "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI.", "AI": {"tldr": "本文提出了AI可能导致的人类灭绝事件的分类和示例，旨在通过公开讨论支持预防措施。", "motivation": "通过公开讨论AI可能带来的灾难性风险，争取公众支持以推动预防措施。", "method": "提出分类和具体示例，说明AI可能导致的人类灭绝情景。", "result": "明确了AI潜在灾难性风险的可能性，强调了预防的重要性。", "conclusion": "公开讨论AI风险有助于推动预防措施，减少潜在灾难。"}}
{"id": "2507.09794", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09794", "abs": "https://arxiv.org/abs/2507.09794", "authors": ["Minjae Jeon", "Lang Tong", "Qing Zhao"], "title": "Joint Scheduling of Deferrable and Nondeferrable Demand with Colocated Stochastic Supply", "comment": null, "summary": "We address the problem of optimal joint scheduling of deferrable and\nnondeferrable demand involving colocated stochastic supply. Deferrable demand\ncan be delayed within its service deadline, whereas nondeferrable demand must\nbe scheduled immediately. Under a finite-horizon stochastic dynamic programming\nformulation, we show that the optimal scheduling policy is a ``procrastination\npolicy'' that delays scheduling as much as possible and is characterized by\nthree procrastination parameters. Exploiting the low-dimensional\nparameterization of the optimal policy, we propose a Procrastination Threshold\nReinforcement Learning algorithm. Numerical experiments based on real-world\ntest data confirm that the threshold-learning algorithm closely approximates\nthe optimal policy and outperforms standard benchmarks.", "AI": {"tldr": "论文提出了一种针对可延迟和不可延迟需求的联合调度问题，采用拖延策略和强化学习算法，实验证明其优于基准方法。", "motivation": "解决在随机供应下可延迟和不可延迟需求的最优联合调度问题。", "method": "基于有限时域随机动态规划，提出拖延策略和拖延阈值强化学习算法。", "result": "实验表明算法接近最优策略且优于标准基准。", "conclusion": "拖延策略和阈值学习算法在联合调度问题中高效且实用。"}}
{"id": "2507.09731", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09731", "abs": "https://arxiv.org/abs/2507.09731", "authors": ["Robby Hoover", "Nelly Elsayed", "Zag ElSayed", "Chengcheng Li"], "title": "Pre-trained Under Noise: A Framework for Robust Bone Fracture Detection in Medical Imaging", "comment": "7 pages, under review", "summary": "Medical Imagings are considered one of the crucial diagnostic tools for\ndifferent bones-related diseases, especially bones fractures. This paper\ninvestigates the robustness of pre-trained deep learning models for classifying\nbone fractures in X-ray images and seeks to address global healthcare disparity\nthrough the lens of technology. Three deep learning models have been tested\nunder varying simulated equipment quality conditions. ResNet50, VGG16 and\nEfficientNetv2 are the three pre-trained architectures which are compared.\nThese models were used to perform bone fracture classification as images were\nprogressively degraded using noise. This paper specifically empirically studies\nhow the noise can affect the bone fractures detection and how the pre-trained\nmodels performance can be changes due to the noise that affect the quality of\nthe X-ray images. This paper aims to help replicate real world challenges\nexperienced by medical imaging technicians across the world. Thus, this paper\nestablishes a methodological framework for assessing AI model degradation using\ntransfer learning and controlled noise augmentation. The findings provide\npractical insight into how robust and generalizable different pre-trained deep\nlearning powered computer vision models can be when used in different contexts.", "AI": {"tldr": "本文研究了预训练深度学习模型在X射线图像中分类骨骨折的鲁棒性，通过模拟不同设备质量条件测试了ResNet50、VGG16和EfficientNetv2的性能，旨在解决全球医疗技术差异问题。", "motivation": "探讨预训练模型在骨骨折分类中的表现，尤其是在图像质量受噪声影响时的鲁棒性，以应对全球医疗资源不均的挑战。", "method": "使用ResNet50、VGG16和EfficientNetv2三种预训练模型，通过逐步添加噪声模拟图像质量退化，评估其分类性能。", "result": "研究发现噪声对模型性能有显著影响，不同模型在噪声条件下的表现差异明显，为实际应用提供了参考。", "conclusion": "本文提出了一种评估AI模型在噪声条件下性能的方法框架，为医疗影像技术的全球推广提供了实用见解。"}}
{"id": "2507.09176", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09176", "abs": "https://arxiv.org/abs/2507.09176", "authors": ["Han Ye", "Yuqiang Jin", "Jinyuan Liu", "Tao Li", "Wen-An Zhang", "Minglei Fu"], "title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "comment": "9 pages,14 figures", "summary": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})", "AI": {"tldr": "提出了一种无需目标的多LiDAR外参标定框架，通过LiDAR束调整优化和自适应加权机制，显著提高了标定精度和鲁棒性。", "motivation": "多LiDAR系统的精确外参标定对3D地图重建至关重要，传统方法依赖重叠视场或人工标注，限制了应用场景。", "method": "结合LiDAR束调整优化和滑动窗口技术，构建参考点云地图，并通过自适应加权机制实现抗异常值的参数估计。", "result": "在CARLA仿真和真实场景中，平均平移误差5 mm，旋转误差0.2°，初始误差容忍度达0.4 m/30°。", "conclusion": "该方法无需基础设施或人工调参，开源且性能优越，适用于非重叠传感器配置。"}}
{"id": "2507.09037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "ALIGN系统通过提示对齐实现LLM动态个性化，支持多属性分析和模块化算法集成。", "motivation": "用户多样化需求和偏好需要LLM个性化对齐方法，现有工具仅关注基准任务。", "method": "ALIGN系统采用提示对齐、结构化输出生成和可替换LLM核心算法。", "result": "系统支持定性和定量分析，并在公共意见调查和医疗分诊中验证对齐方法。", "conclusion": "ALIGN开源框架为可靠、负责和个性化的LLM决策提供新研究方向。"}}
{"id": "2507.09068", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09068", "abs": "https://arxiv.org/abs/2507.09068", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "title": "Infinite Video Understanding", "comment": null, "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "AI": {"tldr": "论文探讨了无限视频理解（Infinite Video Understanding）作为多媒体研究的新前沿，旨在解决长视频处理中的计算、内存和时序一致性等挑战。", "motivation": "当前大型语言模型（LLMs）和多模态扩展（MLLMs）在视频理解方面取得进展，但处理超长视频时仍面临计算、内存和时序一致性问题。", "method": "提出将无限视频理解作为研究方向，强调需要创新的流式架构、持久内存机制、分层表示和事件中心推理。", "result": "论文未提供具体实验结果，但提出了未来研究的核心挑战和方向。", "conclusion": "无限视频理解是多媒体和AI研究的重要目标，需跨领域合作推动技术创新。"}}
{"id": "2507.09374", "categories": ["cs.AI", "I.2.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.09374", "abs": "https://arxiv.org/abs/2507.09374", "authors": ["Chenglin Zhu", "Tao Zhang", "Chong Li", "Mingan Lin", "Zenan Zhou", "Jian Xie"], "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique", "comment": "14 pages,4 figures", "summary": "Multimodal large language models (MLLMs) still perform poorly on scientific\ntasks, particularly those requiring multi-step and interpretable reasoning.\nTheir limitations include insufficient scientific reasoning patterns, lack of\nglobal coherence in multi-step inference, and the absence of reflective\nself-correction, making them unreliable in structured scientific contexts. We\nintroduce EduFlow, the first end-to-end framework that covers the full pipeline\nof educational scientific reasoning, including data selection, MCTS-based\ntrajectory construction, model training, and output optimization. At its core\nis EduPRM, a process-aware reward model that critiques reasoning steps with\ntags and justifications. EduPRM is trained via curriculum learning on three\ncomplementary supervision sources: MCTS-guided trajectories, error-injected\ncritiques, and teacher-student dialogues, enabling dynamic adaptation to\nmulti-stage problem solving and iterative refinement during inference. We\nfurther propose EduMCTS, a domain-adapted search framework that introduces\nbootstrapping actions specifically designed for educational reasoning, such as\na self-reflection mechanism that promotes reflective error correction. It\nfurther leverages EduPRM's fine-grained feedback to guide the search toward\nhigher-quality reasoning trajectories. By applying self-consistency and\nrejection sampling, we constructed EduMCTS-160K, a large-scale dataset of\neducational reasoning trajectories. Extensive experiments demonstrate that\nEduFlow enhances reasoning consistency and coherence. Code, data, and models\nwill be released.", "AI": {"tldr": "EduFlow是一个端到端框架，旨在提升多模态大语言模型（MLLMs）在科学任务中的推理能力，通过EduPRM和EduMCTS实现多步推理和自校正。", "motivation": "MLLMs在科学任务中表现不佳，缺乏多步推理和自校正能力，EduFlow旨在解决这些问题。", "method": "EduFlow结合EduPRM（过程感知奖励模型）和EduMCTS（领域适应搜索框架），通过课程学习和自反射机制优化推理。", "result": "实验表明EduFlow显著提升了推理的一致性和连贯性。", "conclusion": "EduFlow为科学教育任务提供了一种有效的端到端解决方案，未来将公开代码、数据和模型。"}}
{"id": "2507.09864", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09864", "abs": "https://arxiv.org/abs/2507.09864", "authors": ["Hossein Nejatbakhsh Esfahani", "Javad Mohammadpour Velni"], "title": "Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO", "comment": null, "summary": "Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a\nstructured and interpretable alternative to Deep Neural Network (DNN)-based RL\nmethods, with lower computational complexity and greater transparency. However,\nstandard MPC-RL approaches often suffer from slow convergence, suboptimal\npolicy learning due to limited parameterization, and safety issues during\nonline adaptation. To address these challenges, we propose a novel framework\nthat integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The\nproposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its\ngradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)\napproach, and incorporates them into a MOBO algorithm using the Expected\nHypervolume Improvement (EHVI) acquisition function. This fusion enables\nefficient and safe tuning of the MPC parameters to achieve improved closed-loop\nperformance, even under model imperfections. A numerical example demonstrates\nthe effectiveness of the proposed approach in achieving sample-efficient,\nstable, and high-performance learning for control systems.", "AI": {"tldr": "提出了一种结合MPC-RL与MOBO的新框架，通过CDPG和EHVI优化MPC参数，提升控制系统的性能与安全性。", "motivation": "解决传统MPC-RL方法收敛慢、策略学习次优及在线适应安全性不足的问题。", "method": "集成MPC-RL与MOBO，利用CDPG估计RL阶段成本及其梯度，并通过EHVI算法优化MPC参数。", "result": "在数值实验中展示了该方法的高效性、稳定性和高性能。", "conclusion": "MPC-RL-MOBO框架能有效提升控制系统的学习效率与安全性。"}}
{"id": "2507.09759", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09759", "abs": "https://arxiv.org/abs/2507.09759", "authors": ["Abdul Manaf", "Nimra Mughal"], "title": "AI-Enhanced Pediatric Pneumonia Detection: A CNN-Based Approach Using Data Augmentation and Generative Adversarial Networks (GANs)", "comment": null, "summary": "Pneumonia is a leading cause of mortality in children under five, requiring\naccurate chest X-ray diagnosis. This study presents a machine learning-based\nPediatric Chest Pneumonia Classification System to assist healthcare\nprofessionals in diagnosing pneumonia from chest X-ray images. The CNN-based\nmodel was trained on 5,863 labeled chest X-ray images from children aged 0-5\nyears from the Guangzhou Women and Children's Medical Center. To address\nlimited data, we applied augmentation techniques (rotation, zooming, shear,\nhorizontal flipping) and employed GANs to generate synthetic images, addressing\nclass imbalance. The system achieved optimal performance using combined\noriginal, augmented, and GAN-generated data, evaluated through accuracy and F1\nscore metrics. The final model was deployed via a Flask web application,\nenabling real-time classification with probability estimates. Results\ndemonstrate the potential of deep learning and GANs in improving diagnostic\naccuracy and efficiency for pediatric pneumonia classification, particularly\nvaluable in resource-limited clinical settings\nhttps://github.com/AbdulManaf12/Pediatric-Chest-Pneumonia-Classification", "AI": {"tldr": "该研究开发了一个基于机器学习的儿科胸部肺炎分类系统，通过CNN模型和GAN生成合成图像，提高了肺炎诊断的准确性和效率。", "motivation": "肺炎是五岁以下儿童死亡的主要原因，需要准确的胸部X光诊断。研究旨在通过深度学习技术辅助医疗专业人员诊断。", "method": "使用5,863张标记的儿童胸部X光图像训练CNN模型，结合数据增强和GAN生成合成图像以解决数据不足和类别不平衡问题。", "result": "系统通过结合原始、增强和GAN生成的数据实现了最佳性能，并通过Flask应用实现了实时分类。", "conclusion": "研究表明深度学习和GAN在提高儿科肺炎诊断准确性和效率方面具有潜力，尤其适用于资源有限的临床环境。"}}
{"id": "2507.09309", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09309", "abs": "https://arxiv.org/abs/2507.09309", "authors": ["Peng Xie", "Johannes Betz", "Amr Alanwar"], "title": "Informed Hybrid Zonotope-based Motion Planning Algorithm", "comment": null, "summary": "Optimal path planning in nonconvex free spaces is notoriously challenging, as\nformulating such problems as mixed-integer linear programs (MILPs) is NP-hard.\nWe propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an\nalternative approach that decomposes the obstacle-free space and performs\nlow-dimensional face sampling guided by an ellipsotope heuristic, enabling\nfocused exploration along promising transit regions. This structured\nexploration eliminates the excessive, unreachable sampling that degrades\nexisting informed planners such as AIT* and EIT* in narrow gaps or boxed-goal\nscenarios. We prove that HZ-MP is probabilistically complete and asymptotically\noptimal. It converges to near-optimal trajectories in finite time and scales to\nhigh-dimensional cluttered scenes.", "AI": {"tldr": "HZ-MP是一种基于混合Zonotope的运动规划器，通过分解自由空间和低维面采样，优化路径规划，解决了非凸空间中的NP难题。", "motivation": "非凸自由空间中的路径规划问题复杂且NP难，现有方法如AIT*和EIT*在狭窄空间或目标场景中表现不佳。", "method": "HZ-MP采用混合Zonotope分解自由空间，结合低维面采样和启发式引导，减少无效采样。", "result": "HZ-MP在有限时间内收敛到接近最优轨迹，适用于高维复杂场景，并具有概率完备性和渐近最优性。", "conclusion": "HZ-MP是一种高效且可扩展的运动规划方法，优于现有技术。"}}
{"id": "2507.09075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09075", "abs": "https://arxiv.org/abs/2507.09075", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.", "AI": {"tldr": "OpenCodeReasoning-II数据集包含250万问答-解决方案-评论三元组，用于代码生成和评论的联合训练，显著提升了模型性能。", "motivation": "大规模高质量数据集对代码生成和评论的进展至关重要，现有数据集规模不足。", "method": "采用两阶段监督微调策略，首阶段专注于代码生成，次阶段联合训练代码生成和评论模型。", "result": "微调后的Qwen2.5-Instruct模型在代码生成上表现优异，联合模型显著提升竞赛编程性能。", "conclusion": "OpenCodeReasoning-II数据集和联合训练方法为代码推理任务提供了有效解决方案，并扩展了LiveCodeBench基准以支持C++。"}}
{"id": "2507.09071", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.09071", "abs": "https://arxiv.org/abs/2507.09071", "authors": ["Tharun Adithya Srikrishnan", "Deval Shah", "Steven K. Reinhardt"], "title": "BlindSight: Harnessing Sparsity for Efficient VLMs", "comment": null, "summary": "Large vision-language models (VLMs) enable the joint processing of text and\nimages. However, the inclusion of vision data significantly expands the prompt\nlength. Along with the quadratic complexity of the attention computation, this\nresults in a longer prefill duration. An approach to mitigate this bottleneck\nis to leverage the inherent sparsity in the attention computation. In our\nanalysis of attention patterns in VLMs, we observe that a substantial portion\nof layers exhibit minimal cross-image attention, except through attention-sink\ntokens per image. These sparse attention patterns fall into distinct\ncategories: sink-only, document mask and a hybrid document-sink mask. Based on\nthis, we propose BlindSight: a training-free approach to optimize VLM inference\nusing a input template-aware attention sparsity mask. We utilize samples from a\ndataset to derive a prompt-agnostic sparsity categorization for every attention\nhead. We evaluate the proposed technique using VLMs such as Qwen2-VL,\nQwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on\naverage with -2%-+2% accuracy compared to the original model in most evaluated\nmulti-image understanding benchmarks.", "AI": {"tldr": "论文提出BlindSight方法，通过利用注意力计算的稀疏性优化视觉语言模型（VLM）推理，减少计算量（FLOPs）32%-41%，同时保持准确率。", "motivation": "视觉数据的加入显著增加了提示长度，导致注意力计算的二次复杂度问题，延长预填充时间。", "method": "分析VLM中的注意力模式，发现稀疏性，提出基于输入模板的注意力稀疏掩码BlindSight，无需训练即可优化推理。", "result": "在Qwen2-VL等模型上测试，FLOPs减少32%-41%，准确率变化在-2%到+2%之间。", "conclusion": "BlindSight是一种高效且无需训练的方法，显著减少VLM推理的计算开销。"}}
{"id": "2507.09389", "categories": ["cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09389", "abs": "https://arxiv.org/abs/2507.09389", "authors": ["Chris Davis Jaldi", "Anmol Saini", "Elham Ghiasi", "O. Divine Eziolise", "Cogan Shimizu"], "title": "Knowledge Conceptualization Impacts RAG Efficacy", "comment": null, "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.", "AI": {"tldr": "论文探讨了可解释性和适应性在AI系统中的重要性，特别是针对神经符号AI系统的设计与评估。", "motivation": "研究如何结合可解释性和适应性，设计可迁移且可解释的神经符号AI系统。", "method": "系统评估了知识表示的结构和复杂性对LLM查询三元组存储的影响。", "result": "结果表明，不同知识表示方法对查询效果有显著影响。", "conclusion": "研究为设计更高效的Agentic Retrieval-Augmented Generation系统提供了见解。"}}
{"id": "2507.09938", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09938", "abs": "https://arxiv.org/abs/2507.09938", "authors": ["Chito A. Petilla"], "title": "A Case Study on Data Acquisition Systems: Relevance to Renewable Energy Technologies", "comment": null, "summary": "Multiple advantages had been identified with the integration of data\nacquisition into any existing system configuration and implementation. Using\ndata acquisition as a support into a monitoring system has not only improved\nits overall performance and reliability but also lowered its operational and\nmaintenance cost because of its real-time data collection from node sensors.\n  As renewable energy needs to be sustainable for it to fully support the\nenergy demand of communities, its management and control still needs to be\nimproved and enhanced. Smart systems are considered the next generation\ntechnological improvement of any system that exists. It is the prelude to\nautonomous systems from industrial applications to home automation. Data\nacquisition is only a part of these smart systems that help in the remote\nmanagement and control of these devices. Remote monitoring functionality\nenhances the operation and reliability which help in making proactive decisions\nduring critical situations and circumstances.\n  Even with data acquisition enhancements, there is still room for improving\nits implementation regarding data security and privacy and accuracy of\ninformation being exchanged between nodes. Current technological advancements\nhave already shown promising results and have widen its utilization spectrum by\ncovering almost any field of specialization. With increasing implementation and\ndesign complexity that comes with its enhancements, challenges and issues are\nalso faced that needs to be addressed and considered to mitigate the effects of\nsuch.", "AI": {"tldr": "论文探讨了数据采集在系统集成中的优势及其在可再生能源管理中的应用，同时指出了数据安全和隐私方面的改进空间。", "motivation": "研究旨在通过数据采集提升监控系统的性能和可靠性，并降低运营成本，同时推动可再生能源管理的智能化发展。", "method": "通过实时数据采集和远程监控功能，结合智能系统技术，优化系统管理和控制。", "result": "数据采集显著提升了系统性能和可靠性，但数据安全和隐私问题仍需解决。", "conclusion": "数据采集是智能系统的重要组成部分，未来需进一步解决其实现中的挑战，如数据安全和隐私问题。"}}
{"id": "2507.09872", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09872", "abs": "https://arxiv.org/abs/2507.09872", "authors": ["Shengjie Liu", "Lu Zhang", "Siqin Wang"], "title": "Resolution Revolution: A Physics-Guided Deep Learning Framework for Spatiotemporal Temperature Reconstruction", "comment": "ICCV 2025 Workshop SEA -- International Conference on Computer Vision\n  2025 Workshop on Sustainability with Earth Observation and AI", "summary": "Central to Earth observation is the trade-off between spatial and temporal\nresolution. For temperature, this is especially critical because real-world\napplications require high spatiotemporal resolution data. Current technology\nallows for hourly temperature observations at 2 km, but only every 16 days at\n100 m, a gap further exacerbated by cloud cover. Earth system models offer\ncontinuous hourly temperature data, but at a much coarser spatial resolution\n(9-31 km). Here, we present a physics-guided deep learning framework for\ntemperature data reconstruction that integrates these two data sources. The\nproposed framework uses a convolutional neural network that incorporates the\nannual temperature cycle and includes a linear term to amplify the coarse Earth\nsystem model output into fine-scale temperature values observed from\nsatellites. We evaluated this framework using data from two satellites, GOES-16\n(2 km, hourly) and Landsat (100 m, every 16 days), and demonstrated effective\ntemperature reconstruction with hold-out and in situ data across four datasets.\nThis physics-guided deep learning framework opens new possibilities for\ngenerating high-resolution temperature data across spatial and temporal scales,\nunder all weather conditions and globally.", "AI": {"tldr": "提出了一种物理引导的深度学习框架，用于整合高时空分辨率温度数据，填补现有观测与模型之间的分辨率差距。", "motivation": "地球观测中空间与时间分辨率的权衡是关键问题，尤其是温度数据的高分辨率需求。现有技术无法同时满足高时空分辨率，且受云层影响。", "method": "采用卷积神经网络，结合年温度周期和线性项，将粗分辨率地球系统模型输出放大为卫星观测的精细尺度温度值。", "result": "通过GOES-16和Landsat数据验证，框架在四个数据集上实现了有效的温度重建。", "conclusion": "该框架为全球全天候高分辨率温度数据生成提供了新可能。"}}
{"id": "2507.09340", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09340", "abs": "https://arxiv.org/abs/2507.09340", "authors": ["Hongyu Nie", "Xingyu Li", "Xu Liu", "Zhaotong Tan", "Sen Mei", "Wenbo Su"], "title": "Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics", "comment": "Submitted to IEEE Transactions on Robotics (TRO) in July 2025", "summary": "Autonomous navigation in mobile robots, reliant on perception and planning,\nfaces major hurdles in large-scale, complex environments. These include heavy\ncomputational burdens for mapping, sensor occlusion failures for UAVs, and\ntraversal challenges on irregular terrain for UGVs, all compounded by a lack of\nperception-aware strategies. To address these challenges, we introduce Random\nMapping and Random Projection (RMRP). This method constructs a lightweight\nlinear parametric map by first mapping data to a high-dimensional space,\nfollowed by a sparse random projection for dimensionality reduction. Our novel\nResidual Energy Preservation Theorem provides theoretical guarantees for this\nprocess, ensuring critical geometric properties are preserved. Based on this\nmap, we propose the RPATR (Robust Perception-Aware Trajectory Planner)\nframework. For UAVs, our method unifies grid and Euclidean Signed Distance\nField (ESDF) maps. The front-end uses an analytical occupancy gradient to\nrefine initial paths for safety and smoothness, while the back-end uses a\nclosed-form ESDF for trajectory optimization. Leveraging the trained RMRP\nmodel's generalization, the planner predicts unobserved areas for proactive\nnavigation. For UGVs, the model characterizes terrain and provides closed-form\ngradients, enabling online planning to circumvent large holes. Validated in\ndiverse scenarios, our framework demonstrates superior mapping performance in\ntime, memory, and accuracy, and enables computationally efficient, safe\nnavigation for high-speed UAVs and UGVs. The code will be released to foster\ncommunity collaboration.", "AI": {"tldr": "提出了一种名为RMRP的轻量级线性参数化地图构建方法，结合RPATR框架，解决了大规模复杂环境中移动机器人自主导航的感知与规划问题。", "motivation": "解决大规模复杂环境中移动机器人自主导航的挑战，包括计算负担、传感器遮挡和不规则地形等问题。", "method": "使用RMRP方法构建轻量级地图，结合RPATR框架进行轨迹规划，包括前端路径优化和后端轨迹优化。", "result": "在多种场景中验证了框架的高效性，展示了在时间、内存和准确性上的优越性能。", "conclusion": "RMRP和RPATR框架为高速无人机和地面车辆提供了计算高效且安全的导航解决方案。"}}
{"id": "2507.09076", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "AI": {"tldr": "本文提出了一种动态参数记忆（DPM）机制，用于解决语音大语言模型（SLLM）在长音频序列情感识别中的限制，显著提升了性能。", "motivation": "语音模态的高帧率限制了SLLM的信号处理和理解能力，现有方法忽视了情感在对话中的连续性和惯性。", "method": "提出DPM机制，结合上下文语义和句子级情感编码，通过临时LoRA模块逐步记忆上下文信息。", "result": "在IEMOCAP数据集上，DPM显著提升了SLLM处理长音频序列的情感识别能力，达到最优性能。", "conclusion": "DPM机制有效解决了SLLM在长音频处理中的限制，为情感识别任务提供了新思路。"}}
{"id": "2507.09081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09081", "abs": "https://arxiv.org/abs/2507.09081", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Hua Wang", "Pei Wang", "Junyi Chen", "Kun Wang"], "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion", "comment": null, "summary": "Quantitative remote sensing inversion aims to estimate continuous surface\nvariables-such as biomass, vegetation indices, and evapotranspiration-from\nsatellite observations, supporting applications in ecosystem monitoring, carbon\naccounting, and land management. With the evolution of remote sensing systems\nand artificial intelligence, traditional physics-based paradigms are giving way\nto data-driven and foundation model (FM)-based approaches. This paper\nsystematically reviews the methodological evolution of inversion techniques,\nfrom physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods\n(e.g., deep learning, multimodal fusion), and further to foundation models\n(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application\nscenarios, and limitations of each paradigm, with emphasis on recent FM\nadvances in self-supervised pretraining, multi-modal integration, and\ncross-task adaptation. We also highlight persistent challenges in physical\ninterpretability, domain generalization, limited supervision, and uncertainty\nquantification. Finally, we envision the development of next-generation\nfoundation models for remote sensing inversion, emphasizing unified modeling\ncapacity, cross-domain generalization, and physical interpretability.", "AI": {"tldr": "综述了定量遥感反演方法从物理模型到数据驱动和基础模型的演变，比较了各范式的假设、应用和局限，并展望了下一代基础模型的发展方向。", "motivation": "探讨遥感反演技术的演变，以支持生态系统监测、碳核算和土地管理等应用。", "method": "系统回顾物理模型（如PROSPECT）、机器学习（如深度学习）和基础模型（如SatMAE）的方法论，比较其假设、应用和局限。", "result": "总结了各范式的优缺点，并强调基础模型在自监督预训练、多模态集成和跨任务适应方面的进展。", "conclusion": "展望下一代基础模型的发展，强调统一建模能力、跨域泛化和物理可解释性。"}}
{"id": "2507.09407", "categories": ["cs.AI", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09407", "abs": "https://arxiv.org/abs/2507.09407", "authors": ["Quanyan Zhu"], "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "comment": null, "summary": "We introduce the framework of LLM-Stackelberg games, a class of sequential\ndecision-making models that integrate large language models (LLMs) into\nstrategic interactions between a leader and a follower. Departing from\nclassical Stackelberg assumptions of complete information and rational agents,\nour formulation allows each agent to reason through structured prompts,\ngenerate probabilistic behaviors via LLMs, and adapt their strategies through\ninternal cognition and belief updates. We define two equilibrium concepts:\nreasoning and behavioral equilibrium, which aligns an agent's internal\nprompt-based reasoning with observable behavior, and conjectural reasoning\nequilibrium, which accounts for epistemic uncertainty through parameterized\nmodels over an opponent's response. These layered constructs capture bounded\nrationality, asymmetric information, and meta-cognitive adaptation. We\nillustrate the framework through a spearphishing case study, where a sender and\na recipient engage in a deception game using structured reasoning prompts. This\nexample highlights the cognitive richness and adversarial potential of\nLLM-mediated interactions. Our results show that LLM-Stackelberg games provide\na powerful paradigm for modeling decision-making in domains such as\ncybersecurity, misinformation, and recommendation systems.", "AI": {"tldr": "LLM-Stackelberg游戏框架将大型语言模型（LLM）融入领导者与追随者的战略互动中，通过结构化提示和概率行为生成，定义了两类均衡概念，展示了在网络安全等领域的应用潜力。", "motivation": "传统Stackelberg游戏假设完全信息和理性行为，而现实中的决策者往往受限于信息不对称和有限理性，因此需要一种新框架来模拟更真实的战略互动。", "method": "提出LLM-Stackelberg游戏框架，利用LLM生成行为并通过结构化提示进行推理，定义推理与行为均衡及推测推理均衡。", "result": "通过钓鱼攻击案例展示了框架的认知丰富性和对抗潜力，验证了其在网络安全等领域的适用性。", "conclusion": "LLM-Stackelberg游戏为建模有限理性、信息不对称和认知适应的决策问题提供了新范式。"}}
{"id": "2507.09960", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09960", "abs": "https://arxiv.org/abs/2507.09960", "authors": ["Subin Shin", "Seongkyu Jung", "Jinseok Choi", "Jeonghun Park"], "title": "Efficient RF Chain Selection for MIMO Integrated Sensing and Communications: A Greedy Approach", "comment": null, "summary": "In multiple-input multiple-output integrated sensing and communication (MIMO\nISAC) systems, radio frequency chain (i.e., RF chain) selection plays a vital\nrole in reducing hardware cost, power consumption, and computational\ncomplexity. However, designing an effective RF chain selection strategy is\nchallenging due to the disparity in performance metrics between communication\nand sensing-mutual information (MI) versus beam-pattern mean-squared error\n(MSE) or the Cram\\'er-Rao lower bound (CRLB). To overcome this, we propose a\nlow-complexity greedy RF chain selection framework maximizing a unified\nMI-based performance metric applicable to both functions. By decomposing the\ntotal MI into individual contributions of each RF chain, we introduce two\napproaches: greedy eigen-based selection (GES) and greedy cofactor-based\nselection (GCS), which iteratively identify and remove the RF chains with the\nlowest contribution. We further extend our framework to beam selection for\nbeamspace MIMO ISAC systems, introducing diagonal beam selection (DBS) as a\nsimplified solution. Simulation results show that our proposed methods achieve\nnear-optimal performance with significantly lower complexity than exhaustive\nsearch, demonstrating their practical effectiveness for MIMO ISAC systems.", "AI": {"tldr": "提出了一种低复杂度的贪婪RF链选择框架，通过统一基于互信息的性能指标优化MIMO ISAC系统中的RF链选择。", "motivation": "解决MIMO ISAC系统中RF链选择因通信与感知性能指标差异（互信息与波束模式均方误差或CRLB）带来的设计挑战。", "method": "将总互信息分解为各RF链的贡献，提出贪婪特征选择（GES）和贪婪余子式选择（GCS）方法，迭代移除贡献最低的RF链。", "result": "仿真结果表明，所提方法在显著降低复杂度的同时实现了接近最优的性能。", "conclusion": "该方法为MIMO ISAC系统提供了一种实用且高效的RF链选择解决方案。"}}
{"id": "2507.09898", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09898", "abs": "https://arxiv.org/abs/2507.09898", "authors": ["Alireza Golkarieha", "Kiana Kiashemshakib", "Sajjad Rezvani Boroujenic", "Nasibeh Asadi Isakand"], "title": "Advanced U-Net Architectures with CNN Backbones for Automated Lung Cancer Detection and Segmentation in Chest CT Images", "comment": "This manuscript has 20 pages and 10 figures. It is submitted to the\n  Journal 'Scientific Reports'", "summary": "This study investigates the effectiveness of U-Net architectures integrated\nwith various convolutional neural network (CNN) backbones for automated lung\ncancer detection and segmentation in chest CT images, addressing the critical\nneed for accurate diagnostic tools in clinical settings. A balanced dataset of\n832 chest CT images (416 cancerous and 416 non-cancerous) was preprocessed\nusing Contrast Limited Adaptive Histogram Equalization (CLAHE) and resized to\n128x128 pixels. U-Net models were developed with three CNN backbones: ResNet50,\nVGG16, and Xception, to segment lung regions. After segmentation, CNN-based\nclassifiers and hybrid models combining CNN feature extraction with traditional\nmachine learning classifiers (Support Vector Machine, Random Forest, and\nGradient Boosting) were evaluated using 5-fold cross-validation. Metrics\nincluded accuracy, precision, recall, F1-score, Dice coefficient, and ROC-AUC.\nU-Net with ResNet50 achieved the best performance for cancerous lungs (Dice:\n0.9495, Accuracy: 0.9735), while U-Net with VGG16 performed best for\nnon-cancerous segmentation (Dice: 0.9532, Accuracy: 0.9513). For\nclassification, the CNN model using U-Net with Xception achieved 99.1 percent\naccuracy, 99.74 percent recall, and 99.42 percent F1-score. The hybrid\nCNN-SVM-Xception model achieved 96.7 percent accuracy and 97.88 percent\nF1-score. Compared to prior methods, our framework consistently outperformed\nexisting models. In conclusion, combining U-Net with advanced CNN backbones\nprovides a powerful method for both segmentation and classification of lung\ncancer in CT scans, supporting early diagnosis and clinical decision-making.", "AI": {"tldr": "研究探讨了结合不同CNN骨干网络的U-Net架构在胸部CT图像中自动检测和分割肺癌的效果，结果显示U-Net与ResNet50结合在癌症分割中表现最佳，而U-Net与Xception结合的分类模型准确率最高。", "motivation": "解决临床环境中对准确诊断工具的需求，提升肺癌检测和分割的自动化水平。", "method": "使用CLAHE预处理832张胸部CT图像，构建U-Net模型（结合ResNet50、VGG16和Xception骨干网络）进行分割，随后评估CNN分类器和混合模型的性能。", "result": "U-Net与ResNet50在癌症分割中表现最佳（Dice: 0.9495，准确率: 0.9735），U-Net与Xception的分类模型准确率达99.1%。", "conclusion": "结合U-Net与先进CNN骨干网络的方法在肺癌CT扫描的分割和分类中表现优异，支持早期诊断和临床决策。"}}
{"id": "2507.09344", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09344", "abs": "https://arxiv.org/abs/2507.09344", "authors": ["Daniel Engelsman", "Itzik Klein"], "title": "C-ZUPT: Stationarity-Aided Aerial Hovering", "comment": "14 Pages, 16 Figures, 9 Tables", "summary": "Autonomous systems across diverse domains have underscored the need for\ndrift-resilient state estimation. Although satellite-based positioning and\ncameras are widely used, they often suffer from limited availability in many\nenvironments. As a result, positioning must rely solely on inertial sensors,\nleading to rapid accuracy degradation over time due to sensor biases and noise.\nTo counteract this, alternative update sources-referred to as information\naiding-serve as anchors of certainty. Among these, the zero-velocity update\n(ZUPT) is particularly effective in providing accurate corrections during\nstationary intervals, though it is restricted to surface-bound platforms. This\nwork introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and\ncontrol, independent of surface contact. By defining an uncertainty threshold,\nC-ZUPT identifies quasi-static equilibria to deliver precise velocity updates\nto the estimation filter. Extensive validation confirms that these\nopportunistic, high-quality updates significantly reduce inertial drift and\ncontrol effort. As a result, C-ZUPT mitigates filter divergence and enhances\nnavigation stability, enabling more energy-efficient hovering and substantially\nextending sustained flight-key advantages for resource-constrained aerial\nsystems.", "AI": {"tldr": "论文提出了一种名为C-ZUPT的方法，用于空中导航和控制，通过不确定性阈值识别准静态平衡，显著减少惯性漂移和控制能耗。", "motivation": "卫星定位和摄像头在许多环境中可用性有限，导致依赖惯性传感器时精度快速下降。需要一种不依赖地面接触的漂移恢复方法。", "method": "引入控制零速度更新（C-ZUPT）方法，通过不确定性阈值识别准静态平衡，为估计滤波器提供精确速度更新。", "result": "C-ZUPT显著减少了惯性漂移和控制能耗，增强了导航稳定性，延长了飞行时间。", "conclusion": "C-ZUPT方法有效解决了空中系统的漂移问题，提高了能源效率和飞行稳定性。"}}
{"id": "2507.09104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "AI": {"tldr": "CompassJudger-2是一个新型通用评判模型，通过任务驱动的多领域数据策略提升评判能力，优于现有模型。", "motivation": "现有评判模型存在专业狭窄和鲁棒性不足的问题，无法全面评估大语言模型。", "method": "采用任务驱动、多领域数据策略，结合可验证奖励和监督学习，通过拒绝采样和边际策略梯度损失优化模型。", "result": "CompassJudger-2在多个评判和奖励基准测试中表现优异，7B模型与更大模型竞争。", "conclusion": "CompassJudger-2提升了评判模型的鲁棒性和可扩展性，并提出了新的评估标准JudgerBenchV2。"}}
{"id": "2507.09082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09082", "abs": "https://arxiv.org/abs/2507.09082", "authors": ["Seungwoo Kim", "Khai Loong Aw", "Klemen Kotar", "Cristobal Eyzaguirre", "Wanhee Lee", "Yunong Liu", "Jared Watrous", "Stefan Stojanov", "Juan Carlos Niebles", "Jiajun Wu", "Daniel L. K. Yamins"], "title": "Taming generative video models for zero-shot optical flow extraction", "comment": "Project webpage: https://neuroailab.github.io/projects/kl_tracing", "summary": "Extracting optical flow from videos remains a core computer vision problem.\nMotivated by the success of large general-purpose models, we ask whether frozen\nself-supervised video models trained only for future frame prediction can be\nprompted, without fine-tuning, to output flow. Prior work reading out depth or\nillumination from video generators required fine-tuning, which is impractical\nfor flow where labels are scarce and synthetic datasets suffer from a\nsim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,\nwhich can obtain point-wise correspondences by injecting a small tracer\nperturbation into a next-frame predictor and tracking its propagation, we\nextend this idea to generative video models. We explore several popular\narchitectures and find that successful zero-shot flow extraction in this manner\nis aided by three model properties: (1) distributional prediction of future\nframes (avoiding blurry or noisy outputs); (2) factorized latents that treat\neach spatio-temporal patch independently; and (3) random-access decoding that\ncan condition on any subset of future pixels. These properties are uniquely\npresent in the recent Local Random Access Sequence (LRAS) architecture.\nBuilding on LRAS, we propose KL-tracing: a novel test-time procedure that\ninjects a localized perturbation into the first frame, rolls out the model one\nstep, and computes the Kullback-Leibler divergence between perturbed and\nunperturbed predictive distributions. Without any flow-specific fine-tuning,\nour method outperforms state-of-the-art models on real-world TAP-Vid DAVIS\ndataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid\nKubric (4.7% relative improvement). Our results indicate that counterfactual\nprompting of controllable generative video models is a scalable and effective\nalternative to supervised or photometric-loss approaches for high-quality flow.", "AI": {"tldr": "论文提出了一种无需微调的零样本方法，通过扰动生成视频模型提取光流，性能优于现有方法。", "motivation": "探索是否可以通过冻结的自监督视频模型（仅用于未来帧预测）直接提取光流，避免因标签稀缺和合成数据差距导致的微调问题。", "method": "基于CWM范式，提出KL-tracing方法，通过局部扰动和KL散度计算提取光流，并利用LRAS架构的特性。", "result": "在TAP-Vid DAVIS和Kubric数据集上分别实现了16.6%和4.7%的相对性能提升。", "conclusion": "研究表明，通过可控生成视频模型的因果提示是一种高效且可扩展的光流提取方法。"}}
{"id": "2507.09495", "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09495", "abs": "https://arxiv.org/abs/2507.09495", "authors": ["Hang Wang", "Junshan Zhang"], "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective", "comment": "Position paper", "summary": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks.", "AI": {"tldr": "论文提出从反应式到主动式多智能体强化学习的范式转变，利用生成式AI实现前瞻性决策和协作。", "motivation": "传统方法在多智能体强化学习中面临指数级增长的动作空间、非静态环境和部分可观测性等挑战，无法应对新场景。", "method": "通过生成式AI重构智能体为生成模型，预测环境演变和其他智能体行为，生成协调动作序列，实现战略推理。", "result": "该方法利用生成式AI的识别与生成能力，实现主动决策、无缝协调和动态适应。", "conclusion": "这一范式转变有望解决传统框架难以处理的协作问题，推动分布式智能和集体行为的发展。"}}
{"id": "2507.09997", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09997", "abs": "https://arxiv.org/abs/2507.09997", "authors": ["Venkatraman Renganathan", "Sabyasachi Mondal", "Antonios Tsourdos"], "title": "Predictive & Trust-based Multi-Agent Coordination", "comment": null, "summary": "This paper presents a trust-based predictive multi-agent consensus protocol\nthat analyses neighbours' anticipation data and makes coordination decisions.\nAgents in the network share their future predicted data over a finite\nlook-ahead horizon with their neighbours and update their predictions in a\nrolling-horizon fashion. The prediction data is then used by agents to learn\nboth the trust and the commitment traits exhibited by their neighbours over\ntime. The proposed protocol is named as the Anticipatory Distributed\nCoordination (ADC) protocol. Lyapunov theory-based agreement convergence\nbetween agents is provided, followed by demonstrations using numerical\nsimulations.", "AI": {"tldr": "本文提出了一种基于信任的多智能体共识协议（ADC），通过分析邻居的预测数据做出协调决策，并利用Lyapunov理论证明收敛性。", "motivation": "研究多智能体网络中如何通过预测数据和信任学习实现高效协调。", "method": "智能体共享未来预测数据，滚动更新预测，并学习邻居的信任和承诺特征，提出ADC协议。", "result": "通过数值模拟验证了协议的收敛性和有效性。", "conclusion": "ADC协议能有效实现多智能体的协调决策，具有理论和实践价值。"}}
{"id": "2507.09923", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09923", "abs": "https://arxiv.org/abs/2507.09923", "authors": ["Sejin Park", "Sangmin Lee", "Kyong Hwan Jin", "Seung-Won Jung"], "title": "IM-LUT: Interpolation Mixing Look-Up Tables for Image Super-Resolution", "comment": "ICCV 2025", "summary": "Super-resolution (SR) has been a pivotal task in image processing, aimed at\nenhancing image resolution across various applications. Recently, look-up table\n(LUT)-based approaches have attracted interest due to their efficiency and\nperformance. However, these methods are typically designed for fixed scale\nfactors, making them unsuitable for arbitrary-scale image SR (ASISR). Existing\nASISR techniques often employ implicit neural representations, which come with\nconsiderable computational cost and memory demands. To address these\nlimitations, we propose Interpolation Mixing LUT (IM-LUT), a novel framework\nthat operates ASISR by learning to blend multiple interpolation functions to\nmaximize their representational capacity. Specifically, we introduce IM-Net, a\nnetwork trained to predict mixing weights for interpolation functions based on\nlocal image patterns and the target scale factor. To enhance efficiency of\ninterpolation-based methods, IM-Net is transformed into IM-LUT, where LUTs are\nemployed to replace computationally expensive operations, enabling lightweight\nand fast inference on CPUs while preserving reconstruction quality.\nExperimental results on several benchmark datasets demonstrate that IM-LUT\nconsistently achieves a superior balance between image quality and efficiency\ncompared to existing methods, highlighting its potential as a promising\nsolution for resource-constrained applications.", "AI": {"tldr": "论文提出了一种名为IM-LUT的新框架，用于任意尺度图像超分辨率（ASISR），通过混合插值函数提高效率和质量。", "motivation": "现有基于查找表（LUT）的方法仅适用于固定尺度，而现有ASISR方法计算成本高且内存需求大。", "method": "提出IM-LUT框架，通过IM-Net预测插值函数的混合权重，并利用LUT替换计算密集型操作。", "result": "实验表明IM-LUT在图像质量和效率上优于现有方法。", "conclusion": "IM-LUT是一种适用于资源受限应用的高效解决方案。"}}
{"id": "2507.09371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09371", "abs": "https://arxiv.org/abs/2507.09371", "authors": ["Kehan Wen", "Chenhao Li", "Junzhe He", "Marco Hutter"], "title": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality", "comment": "This paper is under review", "summary": "Learning from demonstration has proven effective in robotics for acquiring\nnatural behaviors, such as stylistic motions and lifelike agility, particularly\nwhen explicitly defining style-oriented reward functions is challenging.\nSynthesizing stylistic motions for real-world tasks usually requires balancing\ntask performance and imitation quality. Existing methods generally depend on\nexpert demonstrations closely aligned with task objectives. However, practical\ndemonstrations are often incomplete or unrealistic, causing current methods to\nboost style at the expense of task performance. To address this issue, we\npropose formulating the problem as a constrained Markov Decision Process\n(CMDP). Specifically, we optimize a style-imitation objective with constraints\nto maintain near-optimal task performance. We introduce an adaptively\nadjustable Lagrangian multiplier to guide the agent to imitate demonstrations\nselectively, capturing stylistic nuances without compromising task performance.\nWe validate our approach across multiple robotic platforms and tasks,\ndemonstrating both robust task performance and high-fidelity style learning. On\nANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile\ngait pattern, showcasing real-world benefits.", "AI": {"tldr": "论文提出了一种基于约束马尔可夫决策过程（CMDP）的方法，用于在机器人任务中平衡任务性能和风格模仿质量，通过自适应调整拉格朗日乘数实现选择性模仿。", "motivation": "现有方法依赖与任务目标紧密对齐的专家示范，但实际示范往往不完整或不现实，导致风格模仿以牺牲任务性能为代价。", "method": "将问题建模为CMDP，优化风格模仿目标并约束任务性能，引入自适应拉格朗日乘数选择性模仿示范。", "result": "在多种机器人平台和任务中验证，任务性能稳定且风格模仿高保真，ANYmal-D硬件上机械能耗降低14.5%，步态更敏捷。", "conclusion": "该方法能有效平衡任务性能和风格模仿，适用于现实场景。"}}
{"id": "2507.09155", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "AI": {"tldr": "OPENXRD是一个用于晶体学问答的开卷管道，结合GPT-4.5生成的简明支持内容，显著提升了小模型在X射线衍射（XRD）任务中的准确性。", "motivation": "解决传统扫描教材可能带来的版权问题，同时填补小模型在晶体学领域的知识空白。", "method": "通过GPT-4.5生成紧凑的领域特定参考内容，并在217个专家级XRD问题上评估不同视觉语言模型的性能。", "result": "使用GPT-4.5生成摘要的模型在准确性上有显著提升，尤其是在晶体学训练有限的情况下。", "conclusion": "OPENXRD展示了专业开卷系统在材料科学中的潜力，并为科学领域的自然语言处理工具奠定了基础。"}}
{"id": "2507.09092", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09092", "abs": "https://arxiv.org/abs/2507.09092", "authors": ["Ram S Iyer", "Narayan S Iyer", "Rugmini Ammal P"], "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks", "comment": "12 pages, 10 figures", "summary": "With the intervention of machine vision in our crucial day to day necessities\nincluding healthcare and automated power plants, attention has been drawn to\nthe internal mechanisms of convolutional neural networks, and the reason why\nthe network provides specific inferences. This paper proposes a novel post-hoc\nvisual explanation method called MI CAM based on activation mapping. Differing\nfrom previous class activation mapping based approaches, MI CAM produces\nsaliency visualizations by weighing each feature map through its mutual\ninformation with the input image and the final result is generated by a linear\ncombination of weights and activation maps. It also adheres to producing causal\ninterpretations as validated with the help of counterfactual analysis. We aim\nto exhibit the visual performance and unbiased justifications for the model\ninferencing procedure achieved by MI CAM. Our approach works at par with all\nstate-of-the-art methods but particularly outperforms some in terms of\nqualitative and quantitative measures. The implementation of proposed method\ncan be found on https://anonymous.4open.science/r/MI-CAM-4D27", "AI": {"tldr": "本文提出了一种名为MI CAM的新型后验视觉解释方法，基于激活映射，通过互信息加权特征图生成显著性可视化，优于现有方法。", "motivation": "随着机器视觉在医疗和自动化电厂等关键领域的应用，理解卷积神经网络的内部机制及其推理原因变得至关重要。", "method": "MI CAM通过互信息加权特征图，生成线性组合的显著图，并通过反事实分析验证因果解释。", "result": "MI CAM在定性和定量指标上优于现有方法，提供无偏见的模型推理解释。", "conclusion": "MI CAM是一种有效的视觉解释方法，能够清晰展示模型推理过程，性能与现有方法相当甚至更优。"}}
{"id": "2507.09534", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09534", "abs": "https://arxiv.org/abs/2507.09534", "authors": ["Guanquan Wang", "Takuya Hiraoka", "Yoshimasa Tsuruoka"], "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning", "comment": null, "summary": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline\nmodel-based reinforcement learning method that leverages the recently proposed\nConsistency Trajectory Model (CTM) for efficient trajectory optimization. While\nprior work applying diffusion models to planning has demonstrated strong\nperformance, it often suffers from high computational costs due to iterative\nsampling procedures. CTP supports fast, single-step trajectory generation\nwithout significant degradation in policy quality. We evaluate CTP on the D4RL\nbenchmark and show that it consistently outperforms existing diffusion-based\nplanning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves\nhigher normalized returns while using significantly fewer denoising steps. In\nparticular, CTP achieves comparable performance with over $120\\times$ speedup\nin inference time, demonstrating its practicality and effectiveness for\nhigh-performance, low-latency offline planning.", "AI": {"tldr": "CTP是一种基于一致性轨迹模型的离线强化学习方法，通过单步轨迹生成提升效率，显著减少计算成本，并在D4RL基准测试中表现优于现有扩散规划方法。", "motivation": "现有扩散模型在规划中计算成本高，CTP旨在通过单步轨迹生成解决这一问题。", "method": "利用一致性轨迹模型（CTM）进行高效轨迹优化，支持快速单步生成。", "result": "在D4RL测试中表现优异，性能接近且推理速度快120倍。", "conclusion": "CTP高效实用，适用于高性能、低延迟的离线规划。"}}
{"id": "2507.10004", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10004", "abs": "https://arxiv.org/abs/2507.10004", "authors": ["Taouba Jouini", "Jan Wachter", "Sophie An", "Veit Hagenmeyer"], "title": "Hardware test and validation of the angular droop control: Analysis and experiments", "comment": null, "summary": "The angular droop control is a grid-forming control strategy that exploits\nthe idea of power-to-angle droop to achieve exact frequency synchronization\nwith no stringent separation between primary and secondary frequency control.\nIn this work, we conduct hardware experiments in the Smart Energy System\nControl Laboratory at Karlsruhe Institute of Technology (KIT) to test and\nvalidate the angular droop control for low voltage power grids in two different\ntest scenarios. First, we verify its grid-forming capabilities after a major\nevent, e.g., following a blackout, demonstrated via power-to-angle droop\nbehavior. For this, we propose two implementation schemes that rely either on\ndirect or indirect actuation of the modulation signal and draw a comparison\nbetween them. Second, we investigate the plug-and-play capabilities, i.e.,\nlocal stability and power sharing for a two-converter system and provide\nsuitable tuning for the control gains. Our experimental findings illustrate the\nusefulness of hardware test and validation for DC/AC converter control, the\npractical challenges entailed and the proposed remedies.", "AI": {"tldr": "论文研究了角下垂控制策略，通过硬件实验验证其在低压电网中的网格形成和即插即用能力，并提出了两种实现方案和增益调优方法。", "motivation": "探索角下垂控制在低压电网中的实际应用，验证其频率同步能力和稳定性。", "method": "在KIT实验室进行硬件实验，测试两种实现方案（直接和间接调制信号驱动），并分析双转换器系统的稳定性与功率分配。", "result": "实验验证了角下垂控制的网格形成能力及即插即用性能，提出了增益调优方案。", "conclusion": "硬件实验证实了角下垂控制的实用性，同时揭示了实际挑战并提供了解决方案。"}}
{"id": "2507.09966", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09966", "abs": "https://arxiv.org/abs/2507.09966", "authors": ["Mingda Zhang"], "title": "A Brain Tumor Segmentation Method Based on CLIP and 3D U-Net with Cross-Modal Semantic Guidance and Multi-Level Feature Fusion", "comment": "13 pages,6 figures", "summary": "Precise segmentation of brain tumors from magnetic resonance imaging (MRI) is\nessential for neuro-oncology diagnosis and treatment planning. Despite advances\nin deep learning methods, automatic segmentation remains challenging due to\ntumor morphological heterogeneity and complex three-dimensional spatial\nrelationships. Current techniques primarily rely on visual features extracted\nfrom MRI sequences while underutilizing semantic knowledge embedded in medical\nreports. This research presents a multi-level fusion architecture that\nintegrates pixel-level, feature-level, and semantic-level information,\nfacilitating comprehensive processing from low-level data to high-level\nconcepts. The semantic-level fusion pathway combines the semantic understanding\ncapabilities of Contrastive Language-Image Pre-training (CLIP) models with the\nspatial feature extraction advantages of 3D U-Net through three mechanisms:\n3D-2D semantic bridging, cross-modal semantic guidance, and semantic-based\nattention mechanisms. Experimental validation on the BraTS 2020 dataset\ndemonstrates that the proposed model achieves an overall Dice coefficient of\n0.8567, representing a 4.8% improvement compared to traditional 3D U-Net, with\na 7.3% Dice coefficient increase in the clinically important enhancing tumor\n(ET) region.", "AI": {"tldr": "提出一种多级融合架构，结合像素级、特征级和语义级信息，用于脑肿瘤MRI分割，性能优于传统3D U-Net。", "motivation": "解决脑肿瘤形态异质性和复杂空间关系导致的自动分割难题，充分利用医学报告中的语义知识。", "method": "采用多级融合架构，结合CLIP模型的语义理解能力和3D U-Net的空间特征提取，通过3D-2D语义桥接、跨模态语义引导和基于语义的注意力机制。", "result": "在BraTS 2020数据集上，整体Dice系数达0.8567，比传统3D U-Net提升4.8%，增强肿瘤区域提升7.3%。", "conclusion": "多级融合架构显著提升脑肿瘤分割性能，尤其在临床关键区域表现突出。"}}
{"id": "2507.09383", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09383", "abs": "https://arxiv.org/abs/2507.09383", "authors": ["Wondmgezahu Teshome", "Kian Behzad", "Octavia Camps", "Michael Everett", "Milad Siami", "Mario Sznaier"], "title": "Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields", "comment": "Accepted to IEEE RA-L 2025", "summary": "Motivated by the problem of pursuit-evasion, we present a motion planning\nframework that combines energy-based diffusion models with artificial potential\nfields for robust real time trajectory generation in complex environments. Our\napproach processes obstacle information directly from point clouds, enabling\nefficient planning without requiring complete geometric representations. The\nframework employs classifier-free guidance training and integrates local\npotential fields during sampling to enhance obstacle avoidance. In dynamic\nscenarios, the system generates initial trajectories using the diffusion model\nand continuously refines them through potential field-based adaptation,\ndemonstrating effective performance in pursuit-evasion scenarios with partial\npursuer observability.", "AI": {"tldr": "结合能量扩散模型与人工势场，提出一种实时轨迹规划框架，适用于复杂环境中的追逃问题。", "motivation": "解决追逃问题中的实时轨迹规划需求，特别是在部分可观测的动态环境中。", "method": "利用点云直接处理障碍物信息，结合无分类器引导训练和局部势场采样增强避障能力。", "result": "在动态场景中生成初始轨迹并通过势场调整持续优化，表现良好。", "conclusion": "该框架在部分可观测的追逃场景中表现出高效且鲁棒的轨迹规划能力。"}}
{"id": "2507.09157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09157", "abs": "https://arxiv.org/abs/2507.09157", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "AI": {"tldr": "论文提出了一种轻量级模型PU-Lie，结合冻结BERT嵌入和PU学习目标，用于检测战略对话中的欺骗信息，在Diplomacy数据集上取得最佳性能。", "motivation": "战略对话中的欺骗检测因语言微妙性和类别不平衡而复杂且高风险，传统方法难以应对。", "method": "结合冻结BERT嵌入、可解释的语言和游戏特定特征，采用PU学习目标，针对少量标注的欺骗信息进行建模。", "result": "模型在Diplomacy数据集上达到0.60的宏F1，同时减少650倍可训练参数。", "conclusion": "PU学习、语言可解释性和说话者感知表示在此任务中具有重要价值，准确检测欺骗比识别真实信息更关键。"}}
{"id": "2507.09097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09097", "abs": "https://arxiv.org/abs/2507.09097", "authors": ["Yunsoo Kim", "Jinge Wu", "Honghan Wu"], "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated promising performance\nin chest X-ray (CXR) analysis. To enhance human-computer interaction, several\nstudies have incorporated radiologists' eye gaze, typically through heatmaps or\ntextual prompts. However, these methods often overlook the sequential order of\neye movements, which could provide valuable insights by highlighting both the\nareas of interest and the order in which they are examined. In this work, we\npropose a novel approach called RadEyeVideo that integrates radiologists'\neye-fixation data as a video sequence, capturing both the temporal and spatial\ndynamics of their gaze. We evaluate this method in CXR report generation and\ndisease diagnosis using three general-domain, open-source LVLMs with video\ninput capabilities. When prompted with eye-gaze videos, model performance\nimproves by up to 24.6% in the report generation task and on average 15.2% for\nboth tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an\nopen-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs\nsuch as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work\nhighlights that domain expert's knowledge (eye-gaze information in this case),\nwhen effectively integrated with LVLMs, can significantly enhance\ngeneral-domain models' capabilities in clinical tasks. RadEyeVideo is a step\ntoward a scalable human-centered approach of utilizing LVLMs in medical image\nanalytics.", "AI": {"tldr": "RadEyeVideo通过整合放射科医生的眼动视频序列，显著提升了大型视觉语言模型（LVLMs）在胸部X光报告生成和疾病诊断中的性能。", "motivation": "现有方法忽略了眼动顺序的潜在价值，RadEyeVideo旨在通过捕捉眼动的时空动态来提升模型性能。", "method": "提出RadEyeVideo方法，将眼动数据作为视频序列输入LVLMs，评估其在报告生成和疾病诊断中的效果。", "result": "模型性能提升高达24.6%（报告生成）和平均15.2%（两项任务），甚至超越特定医学LVLMs。", "conclusion": "有效整合领域专家知识（如眼动信息）可显著增强通用模型在临床任务中的能力，RadEyeVideo为医学图像分析提供了可扩展的人本方法。"}}
{"id": "2507.09540", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09540", "abs": "https://arxiv.org/abs/2507.09540", "authors": ["Ali Safa", "Farida Mohsen", "Ali Al-Zawqari"], "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling", "comment": null, "summary": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes.", "AI": {"tldr": "提出了一种基于Metropolis-Hastings采样的框架，用于训练SNN在RL任务中，无需依赖梯度方法，表现优于传统方法。", "motivation": "SNN在实时控制系统中具有生物启发和高效能优势，但其训练因脉冲通信的不可微分性而面临挑战。", "method": "采用Metropolis-Hastings采样，通过迭代提议和概率接受参数更新，绕过反向传播限制。", "result": "在AcroBot和CartPole基准测试中，该方法在累积奖励和资源效率上优于传统DQL和SNN方法。", "conclusion": "该框架为SNN在RL任务中的训练提供了高效且资源优化的解决方案。"}}
{"id": "2507.10010", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.10010", "abs": "https://arxiv.org/abs/2507.10010", "authors": ["Venkatraman Renganathan"], "title": "Probabilistic Robustness in the Gap Metric", "comment": null, "summary": "Uncertainties influencing the dynamical systems pose a significant challenge\nin estimating the achievable performance of a controller aiming to control such\nuncertain systems. When the uncertainties are of stochastic nature, obtaining\nhard guarantees for the robustness of a controller aiming to hedge against the\nuncertainty is not possible. This issue set the platform for the development of\nprobabilistic robust control approaches. In this work, we utilise the gap\nmetric between the known nominal model and the unknown perturbed model of the\nuncertain system as a tool to gauge the robustness of a controller and\nformulate the gap as a random variable in the setting with stochastic\nuncertainties. Main results of this paper includes giving probabilistic bound\non the gap exceeding a known threshold followed by bounds on the expected gap\nvalue and probabilistic robust stability in terms of the gap metric. Further,\nwe also provide a probabilistic controller performance certification under gap\nuncertainty and probabilistic guarantee on the achievable\n$\\mathcal{H}_{\\infty}$ robustness. Numerical simulations are provided at many\nplaces to demonstrate the proposed approach.", "AI": {"tldr": "本文提出了一种基于间隙度量的概率鲁棒控制方法，用于处理随机不确定性对控制系统性能的影响，并提供了概率界限和稳定性保证。", "motivation": "由于随机不确定性难以提供硬性鲁棒性保证，因此需要开发概率鲁棒控制方法来评估控制器的性能。", "method": "利用已知名义模型与未知扰动模型之间的间隙度量作为工具，将间隙建模为随机变量，并给出概率界限和期望值。", "result": "提供了间隙超过阈值的概率界限、期望间隙值、概率鲁棒稳定性以及控制器性能的概率认证。", "conclusion": "通过数值模拟验证了所提方法的有效性，为随机不确定性下的控制系统提供了实用的鲁棒性分析工具。"}}
{"id": "2507.09995", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09995", "abs": "https://arxiv.org/abs/2507.09995", "authors": ["Guohao Huo", "Ruiting Dai", "Hao Tang"], "title": "Graph-based Multi-Modal Interaction Lightweight Network for Brain Tumor Segmentation (GMLN-BTS) in Edge Iterative MRI Lesion Localization System (EdgeIMLocSys)", "comment": null, "summary": "Brain tumor segmentation plays a critical role in clinical diagnosis and\ntreatment planning, yet the variability in imaging quality across different MRI\nscanners presents significant challenges to model generalization. To address\nthis, we propose the Edge Iterative MRI Lesion Localization System\n(EdgeIMLocSys), which integrates Continuous Learning from Human Feedback to\nadaptively fine-tune segmentation models based on clinician feedback, thereby\nenhancing robustness to scanner-specific imaging characteristics. Central to\nthis system is the Graph-based Multi-Modal Interaction Lightweight Network for\nBrain Tumor Segmentation (GMLN-BTS), which employs a Modality-Aware Adaptive\nEncoder (M2AE) to extract multi-scale semantic features efficiently, and a\nGraph-based Multi-Modal Collaborative Interaction Module (G2MCIM) to model\ncomplementary cross-modal relationships via graph structures. Additionally, we\nintroduce a novel Voxel Refinement UpSampling Module (VRUM) that\nsynergistically combines linear interpolation and multi-scale transposed\nconvolutions to suppress artifacts while preserving high-frequency details,\nimproving segmentation boundary accuracy. Our proposed GMLN-BTS model achieves\na Dice score of 85.1% on the BraTS2017 dataset with only 4.58 million\nparameters, representing a 98% reduction compared to mainstream 3D Transformer\nmodels, and significantly outperforms existing lightweight approaches. This\nwork demonstrates a synergistic breakthrough in achieving high-accuracy,\nresource-efficient brain tumor segmentation suitable for deployment in\nresource-constrained clinical environments.", "AI": {"tldr": "提出EdgeIMLocSys系统，结合GMLN-BTS网络和VRUM模块，实现高效、轻量的脑肿瘤分割，适应不同MRI扫描仪。", "motivation": "解决MRI成像质量差异对模型泛化能力的挑战，提升脑肿瘤分割的鲁棒性和临床适用性。", "method": "1. 使用GMLN-BTS网络，包含M2AE编码器和G2MCIM模块；2. 引入VRUM模块优化分割边界；3. 结合人类反馈持续学习。", "result": "在BraTS2017数据集上Dice分数达85.1%，参数量仅4.58M，比主流3D Transformer模型减少98%。", "conclusion": "GMLN-BTS在资源受限环境中实现了高精度、轻量化的脑肿瘤分割。"}}
{"id": "2507.09463", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09463", "abs": "https://arxiv.org/abs/2507.09463", "authors": ["Anoop Kiran", "Nora Ayanian", "Kenneth Breuer"], "title": "Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems", "comment": "Accepted for publication in Robotics: Science and Systems (RSS) 2025,\n  12 pages, 16 figures", "summary": "Flying multiple quadrotors in close proximity presents a significant\nchallenge due to complex aerodynamic interactions, particularly downwash\neffects that are known to destabilize vehicles and degrade performance.\nTraditionally, multi-quadrotor systems rely on conservative strategies, such as\ncollision avoidance zones around the robot volume, to circumvent this effect.\nThis restricts their capabilities by requiring a large volume for the operation\nof a multi-quadrotor system, limiting their applicability in dense\nenvironments. This work provides a comprehensive, data-driven analysis of the\ndownwash effect, with a focus on characterizing, analyzing, and understanding\nforces, moments, and velocities in both single and multi-quadrotor\nconfigurations. We use measurements of forces and torques to characterize\nvehicle interactions, and particle image velocimetry (PIV) to quantify the\nspatial features of the downwash wake for a single quadrotor and an interacting\npair of quadrotors. This data can be used to inform physics-based strategies\nfor coordination, leverage downwash for optimized formations, expand the\nenvelope of operation, and improve the robustness of multi-quadrotor control.", "AI": {"tldr": "该论文研究了多旋翼无人机近距离飞行时的下洗效应，通过数据驱动的方法分析力和力矩，提出优化编队和控制的策略。", "motivation": "多旋翼无人机在密集环境中飞行时，下洗效应会降低性能和稳定性，传统保守策略限制了其应用范围。", "method": "使用力和力矩测量以及粒子图像测速技术（PIV）量化单机和多机配置中的下洗效应。", "result": "数据可用于优化编队、扩展操作范围并提高多旋翼控制的鲁棒性。", "conclusion": "研究为多旋翼系统的协调和控制提供了物理基础策略，提升了密集环境中的适用性。"}}
{"id": "2507.09174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09174", "abs": "https://arxiv.org/abs/2507.09174", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "AI": {"tldr": "RAMA是一个检索增强的多代理框架，用于验证多媒体虚假信息，通过精确查询、跨验证证据聚合和多代理架构，显著提升性能。", "motivation": "多模态虚假信息的快速扩散对自动化事实核查系统提出了挑战，尤其是当声明模糊或缺乏上下文时。", "method": "RAMA采用三种创新：战略查询制定、跨验证证据聚合和多代理集成架构。", "result": "在基准数据集上表现优异，尤其在解决模糊或不合理声明时表现突出。", "conclusion": "整合基于网络的证据和多代理推理对可信的多媒体验证至关重要，为更可靠的事实核查解决方案铺平道路。"}}
{"id": "2507.09102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09102", "abs": "https://arxiv.org/abs/2507.09102", "authors": ["Yiyang Chen", "Shanshan Zhao", "Lunhao Duan", "Changxing Ding", "Dacheng Tao"], "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning", "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based models, widely used in text-to-image generation, have proven\neffective in 2D representation learning. Recently, this framework has been\nextended to 3D self-supervised learning by constructing a conditional point\ngenerator for enhancing 3D representations. However, its performance remains\nconstrained by the 3D diffusion model, which is trained on the available 3D\ndatasets with limited size. We hypothesize that the robust capabilities of\ntext-to-image diffusion models, particularly Stable Diffusion (SD), which is\ntrained on large-scale datasets, can help overcome these limitations. To\ninvestigate this hypothesis, we propose PointSD, a framework that leverages the\nSD model for 3D self-supervised learning. By replacing the SD model's text\nencoder with a 3D encoder, we train a point-to-image diffusion model that\nallows point clouds to guide the denoising of rendered noisy images. With the\ntrained point-to-image diffusion model, we use noise-free images as the input\nand point clouds as the condition to extract SD features. Next, we train a 3D\nbackbone by aligning its features with these SD features, thereby facilitating\ndirect semantic learning. Comprehensive experiments on downstream point cloud\ntasks and ablation studies demonstrate that the SD model can enhance point\ncloud self-supervised learning. Code is publicly available at\nhttps://github.com/wdttt/PointSD.", "AI": {"tldr": "PointSD框架利用Stable Diffusion模型增强3D点云自监督学习，通过点云引导去噪图像并提取特征，提升下游任务性能。", "motivation": "现有3D扩散模型受限于小规模数据集，而文本到图像扩散模型（如Stable Diffusion）在大规模数据上表现优异，可能弥补这一不足。", "method": "提出PointSD框架，将Stable Diffusion的文本编码器替换为3D编码器，训练点云到图像的扩散模型，并提取特征对齐3D主干网络。", "result": "实验表明，PointSD能有效提升点云自监督学习性能。", "conclusion": "利用大规模预训练的扩散模型可以显著增强3D点云表示学习。"}}
{"id": "2507.09588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09588", "abs": "https://arxiv.org/abs/2507.09588", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation", "comment": null, "summary": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance.", "AI": {"tldr": "eSapiens是一个面向企业的AIaaS平台，结合专有数据、工作流程和主流LLM，提供数据安全和自动化支持。", "motivation": "解决企业在AI应用中面临的数据安全和知识保留问题，同时提升团队效率。", "method": "平台集成结构化文档处理、混合向量检索和无代码编排，支持多种LLM，并通过THOR Agent处理SQL查询。", "result": "实验显示，512令牌分块检索精度最高（Top-3准确率91.3%），生成质量提升23%。", "conclusion": "eSapiens在高风险领域（如法律和金融）中实现了可信、可审计的AI工作流程。"}}
{"id": "2507.10011", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10011", "abs": "https://arxiv.org/abs/2507.10011", "authors": ["Juan A. Martinez-Velasco", "Alexandre Serrano-Fontova", "Ricard Bosch-Tous", "Pau Casals-Torrens"], "title": "Survey on Methods for Detection, Classification and Location of Faults in Power Systems Using Artificial Intelligence", "comment": null, "summary": "Components of electrical power systems are susceptible to failures caused by\nlightning strikes, aging or human errors. These faults can cause equipment\ndamage, affect system reliability, and results in expensive repair costs. As\nelectric power systems are becoming more complex, traditional protection\nmethods face limitations and shortcomings. Faults in power systems can occur at\nanytime and anywhere, can be caused by a natural disaster or an accident, and\ntheir occurrence can be hardly predicted or avoided; therefore, it is crucial\nto accurately estimate the fault location and quickly restore service. The\ndevelopment of methods capable of accurately detecting, locating and removing\nfaults is essential (i.e. fast isolation of faults is necessary to maintain the\nsystem stability at transmission levels; accurate and fast detection and\nlocation of faults are essential for increasing reliability and customer\nsatisfaction at distribution levels). This has motivated the development of new\nand more efficient methods. Methods developed to detect and locate faults in\npower systems can be divided into two categories, conventional and artificial\nintelligence-based techniques. Although the utilization of artificial\nintelligence (AI) techniques offer tremendous potential, they are challenging\nand time consuming (i.e. many AI techniques require training data for\nprocessing). This paper presents a survey of the application of AI techniques\nto fault diagnosis (detection, classification and location of faults) of lines\nand cables of power systems at both transmission and distribution levels. The\npaper provides a short introduction to AI concepts, a brief summary of the\napplication of AI techniques to power system analysis and design, and a\ndiscussion on AI-based fault diagnosis methods.", "AI": {"tldr": "本文综述了人工智能（AI）技术在电力系统故障诊断中的应用，包括故障检测、分类和定位，并探讨了传统方法与AI方法的优缺点。", "motivation": "电力系统组件易受雷击、老化或人为错误导致的故障影响，传统保护方法存在局限性，需要更高效的方法来快速准确地诊断和隔离故障。", "method": "论文分为两部分：介绍AI概念及其在电力系统分析设计中的应用，以及讨论基于AI的故障诊断方法。", "result": "AI技术在电力系统故障诊断中显示出巨大潜力，但其应用仍面临挑战，如需要大量训练数据。", "conclusion": "AI技术为电力系统故障诊断提供了新的可能性，但仍需进一步研究以克服其局限性。"}}
{"id": "2507.10250", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10250", "abs": "https://arxiv.org/abs/2507.10250", "authors": ["Ashkan Shakarami", "Lorenzo Nicole", "Rocco Cappellesso", "Angelo Paolo Dei Tos", "Stefano Ghidoni"], "title": "DepViT-CAD: Deployable Vision Transformer-Based Cancer Diagnosis in Histopathology", "comment": "25 pages, 15 figures", "summary": "Accurate and timely cancer diagnosis from histopathological slides is vital\nfor effective clinical decision-making. This paper introduces DepViT-CAD, a\ndeployable AI system for multi-class cancer diagnosis in histopathology. At its\ncore is MAViT, a novel Multi-Attention Vision Transformer designed to capture\nfine-grained morphological patterns across diverse tumor types. MAViT was\ntrained on expert-annotated patches from 1008 whole-slide images, covering 11\ndiagnostic categories, including 10 major cancers and non-tumor tissue.\nDepViT-CAD was validated on two independent cohorts: 275 WSIs from The Cancer\nGenome Atlas and 50 routine clinical cases from pathology labs, achieving\ndiagnostic sensitivities of 94.11% and 92%, respectively. By combining\nstate-of-the-art transformer architecture with large-scale real-world\nvalidation, DepViT-CAD offers a robust and scalable approach for AI-assisted\ncancer diagnostics. To support transparency and reproducibility, software and\ncode will be made publicly available at GitHub.", "AI": {"tldr": "DepViT-CAD是一种可部署的AI系统，用于病理切片的多类癌症诊断，核心是MAViT（多注意力视觉Transformer），在真实世界验证中表现优异。", "motivation": "提高癌症诊断的准确性和及时性，支持临床决策。", "method": "采用MAViT（多注意力视觉Transformer）从病理切片中提取细粒度形态特征，训练于1008张全切片图像的专家标注数据。", "result": "在两个独立队列中验证，诊断敏感性分别为94.11%和92%。", "conclusion": "DepViT-CAD结合先进Transformer架构和大规模验证，为AI辅助癌症诊断提供了可靠且可扩展的解决方案。"}}
{"id": "2507.09464", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09464", "abs": "https://arxiv.org/abs/2507.09464", "authors": ["Azfar Azdi Arfakhsyad", "Aufa Nasywa Rahman", "Larasati Kinanti", "Ahmad Ataka Awwalur Rizqi", "Hannan Nur Muhammad"], "title": "Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm", "comment": "7 pages, 13 figures. Accepted to IEEE ICITEE 2023", "summary": "Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving\nthe demand for accurate modeling to support developmental testing. This paper\nproposes data-driven modeling software for UAV. Emphasizes the utilization of\ncost-effective sensors to obtain orientation and location data subsequently\nprocessed through the application of data filtering algorithms and sensor\nfusion techniques to improve the data quality to make a precise model\nvisualization on the software. UAV's orientation is obtained using processed\nInertial Measurement Unit (IMU) data and represented using Quaternion\nRepresentation to avoid the gimbal lock problem. The UAV's location is\ndetermined by combining data from the Global Positioning System (GPS), which\nprovides stable geographic coordinates but slower data update frequency, and\nthe accelerometer, which has higher data update frequency but integrating it to\nget position data is unstable due to its accumulative error. By combining data\nfrom these two sensors, the software is able to calculate and continuously\nupdate the UAV's real-time position during its flight operations. The result\nshows that the software effectively renders UAV orientation and position with\nhigh degree of accuracy and fluidity", "AI": {"tldr": "本文提出了一种基于数据驱动的无人机建模软件，利用低成本传感器和传感器融合技术，通过IMU和GPS数据结合，实现高精度的无人机姿态和位置可视化。", "motivation": "无人机（UAV）作为多功能平台，需要精确建模支持开发测试，而低成本传感器和数据融合技术能够提升建模质量。", "method": "使用IMU数据（四元数表示避免万向节锁问题）和GPS数据（结合加速度计以弥补各自缺点），通过数据滤波和传感器融合技术处理数据。", "result": "软件能够高精度、流畅地实时渲染无人机的姿态和位置。", "conclusion": "该软件通过低成本传感器和高效数据处理，实现了无人机建模的高精度和实时性。"}}
{"id": "2507.09185", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09185", "abs": "https://arxiv.org/abs/2507.09185", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "AI": {"tldr": "提出了一种通过剪枝神经元来提升大语言模型泛化能力的微调方法。", "motivation": "大语言模型（LLMs）常依赖数据集特定的机制，导致在新任务或分布上性能下降，因此需要提升其泛化能力。", "method": "使用Integrated Gradients量化神经元对高置信度预测的影响，剪枝与数据集特定机制相关的神经元。", "result": "在多选题基准测试中，该方法显著提升了性能，超越了之前的非剪枝适应方法。", "conclusion": "通过剪枝数据集特定神经元，模型能更依赖泛化表示，提升性能。"}}
{"id": "2507.09105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09105", "abs": "https://arxiv.org/abs/2507.09105", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "comment": null, "summary": "Earlier Sign Language Production (SLP) models typically relied on\nautoregressive methods that generate output tokens one by one, which inherently\nprovide temporal alignment. Although techniques like Teacher Forcing can\nprevent model collapse during training, they still cannot solve the problem of\nerror accumulation during inference, since ground truth is unavailable at that\nstage. In contrast, more recent approaches based on diffusion models leverage\nstep-by-step denoising to enable high-quality generation. However, the\niterative nature of these models and the requirement to denoise entire\nsequences limit their applicability in real-time tasks like SLP. To address it,\nwe apply a hybrid approach combining autoregressive and diffusion models to SLP\nfor the first time, leveraging the strengths of both models in sequential\ndependency modeling and output refinement. To capture fine-grained body\nmovements, we design a Multi-Scale Pose Representation module that separately\nextracts detailed features from distinct articulators and integrates them via a\nMulti-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal\nAttention mechanism that utilizes joint-level confidence scores to dynamically\nguide the pose generation process, improving accuracy and robustness. Extensive\nexperiments on the PHOENIX14T and How2Sign datasets demonstrate the\neffectiveness of our method in both generation quality and real-time streaming\nefficiency.", "AI": {"tldr": "论文提出了一种结合自回归和扩散模型的混合方法，用于手语生成（SLP），解决了传统方法的误差累积和实时性限制问题。", "motivation": "传统自回归方法在推理阶段无法避免误差累积，而扩散模型因迭代性质不适用于实时任务。", "method": "采用混合自回归和扩散模型的方法，设计多尺度姿态表示模块和置信感知因果注意力机制。", "result": "在PHOENIX14T和How2Sign数据集上验证了方法的生成质量和实时流效率。", "conclusion": "混合方法有效结合了两种模型的优势，提升了手语生成的准确性和实时性。"}}
{"id": "2507.09611", "categories": ["cs.AI", "cs.CY", "68T01"], "pdf": "https://arxiv.org/pdf/2507.09611", "abs": "https://arxiv.org/abs/2507.09611", "authors": ["Jenis Winsta"], "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development", "comment": "5 pages, 3 figures", "summary": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future.", "AI": {"tldr": "AI的快速发展带来了环境和伦理问题，包括能源消耗、电子废物、计算资源不平等和网络安全能耗。本文呼吁可持续、透明和公平的AI发展。", "motivation": "探讨AI在性能之外的环境和伦理挑战，推动负责任AI的发展。", "method": "通过综述近期研究和机构报告，分析AI在能源、废物、资源分配和网络安全方面的系统性影响。", "result": "揭示了AI发展中的高排放、硬件更新快、全球资源不平等和网络安全能耗等问题。", "conclusion": "AI的进步需与伦理责任和环境保护结合，以实现可持续和包容的技术未来。"}}
{"id": "2507.10123", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.10123", "abs": "https://arxiv.org/abs/2507.10123", "authors": ["Ruotong Sun", "Ermin Wei", "Lihui Yi"], "title": "Optimal Battery Placement in Power Grid", "comment": "10 pages, 2 figures", "summary": "We study the optimal placement of an unlimited-capacity battery in power\ngrids under a centralized market model, where the independent system operator\n(ISO) aims to minimize total generation costs through load shifting. The\noptimal battery placement is not well understood by the existing literature,\nespecially regarding the influence of network topology on minimizing generation\ncosts. Our work starts with decomposing the Mixed-Integer Linear Programming\n(MILP) problem into a series of Linear Programming (LP) formulations. For power\ngrids with sufficiently large generation capacity or tree topologies, we derive\nanalytical cost expressions demonstrating that, under reasonable assumptions,\nthe weighted degree is the only topological factor for optimal battery\nplacement. We also discuss the minor impact of higher-order topological\nconditions on tree-topology networks. To find the localized nature of a single\nbattery's impact, we establish that the relative cost-saving benefit of a\nsingle battery decreases as the network scales. Furthermore, we design a\nlow-complexity algorithm for weakly-cyclic networks. Numerical experiments show\nthat our algorithm is not only approximately 100 times faster than commercial\nsolvers but also maintains high accuracy even when some theoretical assumptions\nare relaxed.", "AI": {"tldr": "研究了在集中市场模型下无限容量电池在电网中的最优放置问题，通过分解MILP问题为LP问题，发现加权度是影响最优放置的唯一拓扑因素，并提出了高效算法。", "motivation": "现有文献对电网拓扑如何影响电池最优放置以最小化发电成本的理解不足，本研究旨在填补这一空白。", "method": "将MILP问题分解为LP问题，推导出加权度是唯一拓扑因素，并设计低复杂度算法。", "result": "在树状拓扑下，加权度是唯一影响因素；算法比商业求解器快约100倍且保持高精度。", "conclusion": "加权度是电网中电池最优放置的关键拓扑因素，提出的算法高效且实用。"}}
{"id": "2507.09111", "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09111", "abs": "https://arxiv.org/abs/2507.09111", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Rainer Stiefelhagen"], "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI.", "AI": {"tldr": "论文提出了首个针对人-物交互（HOI）检测的鲁棒性基准RoHOI，并提出了语义感知掩码渐进学习（SAMPL）策略以提升模型在复杂环境下的性能。", "motivation": "现有HOI检测模型在真实世界中的性能因环境干扰（如遮挡、噪声等）而下降，缺乏针对鲁棒性的评估标准。", "method": "提出了RoHOI基准，包含20种干扰类型，并设计了SAMPL策略，通过动态调整模型优化来学习鲁棒特征。", "result": "实验表明SAMPL策略显著提升了模型在干扰条件下的性能，优于现有方法。", "conclusion": "RoHOI为HOI检测的鲁棒性评估提供了新标准，SAMPL策略有效提升了模型在复杂环境中的表现。"}}
{"id": "2507.09469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09469", "abs": "https://arxiv.org/abs/2507.09469", "authors": ["Haoyang Wang", "Jingao Xu", "Xinyu Luo", "Ting Zhang", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Yunhao Liu", "Jianfeng Zheng", "Weijie Hong", "Xinlei Chen"], "title": "mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization", "comment": "17 pages, 34 figures. arXiv admin note: substantial text overlap with\n  arXiv:2502.14992", "summary": "For precise, efficient, and safe drone landings, ground platforms should\nreal-time, accurately locate descending drones and guide them to designated\nspots. While mmWave sensing combined with cameras improves localization\naccuracy, lower sampling frequency of traditional frame cameras compared to\nmmWave radar creates bottlenecks in system throughput. In this work, we upgrade\ntraditional frame camera with event camera, a novel sensor that harmonizes in\nsampling frequency with mmWave radar within ground platform setup, and\nintroduce mmE-Loc, a high-precision, low-latency ground localization system\ndesigned for precise drone landings. To fully exploit the \\textit{temporal\nconsistency} and \\textit{spatial complementarity} between these two modalities,\nwe propose two innovative modules: \\textit{(i)} the Consistency-instructed\nCollaborative Tracking module, which further leverages the drone's physical\nknowledge of periodic micro-motions and structure for accurate measurements\nextraction, and \\textit{(ii)} the Graph-informed Adaptive Joint Optimization\nmodule, which integrates drone motion information for efficient sensor fusion\nand drone localization. Real-world experiments conducted in landing scenarios\nwith a drone delivery company demonstrate that mmE-Loc significantly\noutperforms state-of-the-art methods in both accuracy and latency.", "AI": {"tldr": "论文提出了一种结合事件相机与毫米波雷达的高精度低延迟地面定位系统mmE-Loc，用于无人机精准降落。", "motivation": "传统帧相机采样频率低于毫米波雷达，限制了系统吞吐量，需改进以实现更高效的无人机定位与降落。", "method": "引入事件相机以匹配毫米波雷达采样频率，并提出两个创新模块：一致性协作跟踪模块和图引导的自适应联合优化模块。", "result": "实际测试表明，mmE-Loc在精度和延迟上显著优于现有方法。", "conclusion": "mmE-Loc系统通过多模态融合和优化模块，实现了无人机精准降落的高效解决方案。"}}
{"id": "2507.09205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09205", "abs": "https://arxiv.org/abs/2507.09205", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "AI": {"tldr": "论文提出了Banzhida，一个针对藏语的多语言大模型，通过构建最大的藏语预训练语料库，显著提升了藏语生成能力。", "motivation": "藏语作为低资源语言在现有模型中代表性不足，缺乏高质量训练语料。", "method": "构建最大的藏语预训练语料库，通过专门的数据清洗和处理流程，继续预训练多语言基础模型。", "result": "Banzhida在多种任务中显著优于类似规模的开源模型和针对藏语的模型。", "conclusion": "Banzhida为藏语生成AI提供了重要进展，填补了低资源语言的空白。"}}
{"id": "2507.09118", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09118", "abs": "https://arxiv.org/abs/2507.09118", "authors": ["Linlan Huang", "Xusheng Cao", "Haori Lu", "Yifan Meng", "Fei Yang", "Xialei Liu"], "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning", "comment": "Accepted at ICCV 2025", "summary": "Continual learning aims to enable models to learn sequentially from\ncontinuously incoming data while retaining performance on previously learned\ntasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting\nstrong capabilities across various downstream tasks, there has been growing\ninterest in leveraging CLIP for continual learning in such scenarios. Most\nexisting works overlook the inherent modality gap in CLIP, a key factor in its\ngeneralization and adaptability. In this paper, we analyze the variations in\nthe modality gap during the fine-tuning of vision-language pre-trained models.\nOur observations reveal that the modality gap effectively reflects the extent\nto which pre-trained knowledge is preserved. Based on these insights, we\npropose a simple yet effective method, MG-CLIP, that improves CLIP's\nperformance in class-incremental learning. Our approach leverages modality gap\npreservation to mitigate forgetting and modality gap compensation to enhance\nthe capacity for new data, introducing a novel modality-gap-based perspective\nfor continual learning. Extensive experiments on multiple benchmarks\ndemonstrate that our method outperforms existing approaches without requiring\nadditional replay data. Our code is available at\nhttps://github.com/linlany/MindtheGap.", "AI": {"tldr": "论文提出MG-CLIP方法，通过分析CLIP模型中的模态间隙变化，改进其在持续学习中的表现。", "motivation": "利用CLIP模型在持续学习中的潜力，并解决现有方法忽视模态间隙的问题。", "method": "提出MG-CLIP方法，通过保持模态间隙减少遗忘，并通过补偿模态间隙增强新数据学习能力。", "result": "在多个基准测试中表现优于现有方法，且无需额外回放数据。", "conclusion": "模态间隙是持续学习中的关键因素，MG-CLIP方法简单有效。"}}
{"id": "2507.09617", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09617", "abs": "https://arxiv.org/abs/2507.09617", "authors": ["Margherita Martorana", "Francesca Urgese", "Mark Adamik", "Ilaria Tiddi"], "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs", "comment": null, "summary": "Personal service robots are deployed to support daily living in domestic\nenvironments, particularly for elderly and individuals requiring assistance.\nThese robots must perceive complex and dynamic surroundings, understand tasks,\nand execute context-appropriate actions. However, current systems rely on\nproprietary, hard-coded solutions tied to specific hardware and software,\nresulting in siloed implementations that are difficult to adapt and scale\nacross platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to\nenable interoperability across systems, through structured and standardized\nrepresentations of knowledge and reasoning. However, symbolic systems such as\nKGs and ontologies struggle with raw and noisy sensory input. In contrast,\nmultimodal language models are well suited for interpreting input such as\nimages and natural language, but often lack transparency, consistency, and\nknowledge grounding. In this work, we propose a neurosymbolic framework that\ncombines the perceptual strengths of multimodal language models with the\nstructured representations provided by KGs and ontologies, with the aim of\nsupporting interoperability in robotic applications. Our approach generates\nontology-compliant KGs that can inform robot behavior in a platform-independent\nmanner. We evaluated this framework by integrating robot perception data,\nontologies, and five multimodal models (three LLaMA and two GPT models), using\ndifferent modes of neural-symbolic interaction. We assess the consistency and\neffectiveness of the generated KGs across multiple runs and configurations, and\nperform statistical analyzes to evaluate performance. Results show that GPT-o1\nand LLaMA 4 Maverick consistently outperform other models. However, our\nfindings also indicate that newer models do not guarantee better results,\nhighlighting the critical role of the integration strategy in generating\nontology-compliant KGs.", "AI": {"tldr": "提出了一种结合多模态语言模型与知识图谱的神经符号框架，用于提升服务机器人的互操作性和适应性。", "motivation": "当前服务机器人系统依赖专有硬编码方案，难以跨平台适配和扩展，而知识图谱和多模态语言模型各有局限。", "method": "结合多模态语言模型的感知能力和知识图谱的结构化表示，生成符合本体论的知识图谱以指导机器人行为。", "result": "GPT-o1和LLaMA 4 Maverick表现最优，但新模型不一定更好，集成策略对生成符合本体论的知识图谱至关重要。", "conclusion": "神经符号框架有效支持机器人互操作性，但模型选择和集成策略是关键因素。"}}
{"id": "2507.10205", "categories": ["eess.SY", "cs.NA", "cs.SY", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.10205", "abs": "https://arxiv.org/abs/2507.10205", "authors": ["Friedemann Kemm"], "title": "A new time-stepping strategy and boundary treatment to improve recent 2d traffic model", "comment": null, "summary": "We show how a recently published 2d model for traffic flow can be further\nimproved. Besides other improvements and simplifications, we present not only a\nmethod to compute the necessary time step restrictions, but also a subcycling\nfor the inflow and outflow. This drastically reduces computational cost on\nlarge domains with coarse grids, i.\\,e.\\ for simulations of a whole region\ninstead of a small part of a city or town.", "AI": {"tldr": "改进了一种2D交通流模型，通过时间步长限制计算和子循环技术显著降低了计算成本。", "motivation": "提升现有2D交通流模型的效率和适用性，特别是在大区域模拟中。", "method": "提出了时间步长限制计算方法和子循环技术，用于流入和流出处理。", "result": "大幅降低了在大区域或粗网格模拟中的计算成本。", "conclusion": "改进后的模型更适合大规模交通流模拟，计算效率显著提高。"}}
{"id": "2507.09305", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09305", "abs": "https://arxiv.org/abs/2507.09305", "authors": ["Zhiwei Xu"], "title": "DAA*: Deep Angular A Star for Image-based Path Planning", "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable.", "AI": {"tldr": "提出了一种名为DAA*的新方法，通过引入路径角度自由度（PAF）改进A*算法，提升路径平滑性和相似性，在多个数据集上显著优于现有方法。", "motivation": "路径平滑性在模仿学习中被忽视，需要一种方法在路径相似性和平滑性之间找到平衡。", "method": "结合PAF的DAA*算法，通过联合优化路径缩短和平滑性（分别对应启发式距离和PAF）来改进路径最优性。", "result": "在7个数据集上验证，DAA*在路径相似性上显著优于神经A*和TransPath，SPR、ASIM和PSIM指标分别提升9.0%、6.9%和3.9%。", "conclusion": "DAA*在路径最优性和搜索效率之间取得平衡，显著提升了模仿学习中的路径生成质量。"}}
{"id": "2507.09505", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09505", "abs": "https://arxiv.org/abs/2507.09505", "authors": ["Tenghui Xie", "Zhiying Song", "Fuxi Wen", "Jun Li", "Guangzhao Liu", "Zijian Zhao"], "title": "TruckV2X: A Truck-Centered Perception Dataset", "comment": null, "summary": "Autonomous trucking offers significant benefits, such as improved safety and\nreduced costs, but faces unique perception challenges due to trucks' large size\nand dynamic trailer movements. These challenges include extensive blind spots\nand occlusions that hinder the truck's perception and the capabilities of other\nroad users. To address these limitations, cooperative perception emerges as a\npromising solution. However, existing datasets predominantly feature light\nvehicle interactions or lack multi-agent configurations for heavy-duty vehicle\nscenarios. To bridge this gap, we introduce TruckV2X, the first large-scale\ntruck-centered cooperative perception dataset featuring multi-modal sensing\n(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and\nRSUs). We further investigate how trucks influence collaborative perception\nneeds, establishing performance benchmarks while suggesting research priorities\nfor heavy vehicle perception. The dataset provides a foundation for developing\ncooperative perception systems with enhanced occlusion handling capabilities,\nand accelerates the deployment of multi-agent autonomous trucking systems. The\nTruckV2X dataset is available at\nhttps://huggingface.co/datasets/XieTenghu1/TruckV2X.", "AI": {"tldr": "论文介绍了TruckV2X数据集，首个以卡车为中心的大规模协同感知数据集，旨在解决卡车感知中的盲区和遮挡问题。", "motivation": "卡车自动驾驶面临独特的感知挑战，如盲区和动态拖车运动，现有数据集缺乏针对重型车辆的多智能体配置。", "method": "提出TruckV2X数据集，包含多模态感知（LiDAR和摄像头）和多智能体合作（牵引车、拖车、CAV和RSU）。", "result": "数据集为开发协同感知系统提供了基础，增强了遮挡处理能力，并加速了多智能体自动驾驶卡车系统的部署。", "conclusion": "TruckV2X填补了重型车辆协同感知数据集的空白，为未来研究提供了基准和方向。"}}
{"id": "2507.09225", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "AI": {"tldr": "研究分析了视觉隐喻（如融化的冰川表现为融化的冰手榴弹）在气候变化传播中的效果，发现其虽更难理解但更具美感，且能引发更积极的认知体验。", "motivation": "探讨视觉隐喻在环境传播中的实际效果，填补相关研究空白。", "method": "构建MetaClimage数据库，收集人类评分和标签数据，结合自然语言处理分析语义和情感变量。", "result": "视觉隐喻比直白图像更难理解但更美观，未在效果和情感唤起上显著差异，但能引发更多标签和积极情感。", "conclusion": "视觉隐喻虽增加认知负担，但能促进更深层次的认知加工和积极体验，为环境传播提供权衡参考。"}}
{"id": "2507.09122", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09122", "abs": "https://arxiv.org/abs/2507.09122", "authors": ["Chuan Guo", "Inwoo Hwang", "Jian Wang", "Bing Zhou"], "title": "SnapMoGen: Human Motion Generation from Expressive Texts", "comment": "Project Webpage: https://snap-research.github.io/SnapMoGen/", "summary": "Text-to-motion generation has experienced remarkable progress in recent\nyears. However, current approaches remain limited to synthesizing motion from\nshort or general text prompts, primarily due to dataset constraints. This\nlimitation undermines fine-grained controllability and generalization to unseen\nprompts. In this paper, we introduce SnapMoGen, a new text-motion dataset\nfeaturing high-quality motion capture data paired with accurate, expressive\ntextual annotations. The dataset comprises 20K motion clips totaling 44 hours,\naccompanied by 122K detailed textual descriptions averaging 48 words per\ndescription (vs. 12 words of HumanML3D). Importantly, these motion clips\npreserve original temporal continuity as they were in long sequences,\nfacilitating research in long-term motion generation and blending. We also\nimprove upon previous generative masked modeling approaches. Our model,\nMoMask++, transforms motion into multi-scale token sequences that better\nexploit the token capacity, and learns to generate all tokens using a single\ngenerative masked transformer. MoMask++ achieves state-of-the-art performance\non both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the\nability to process casual user prompts by employing an LLM to reformat inputs\nto align with the expressivity and narration style of SnapMoGen. Project\nwebpage: https://snap-research.github.io/SnapMoGen/", "AI": {"tldr": "SnapMoGen是一个新的文本-动作数据集，包含高质量动作捕捉数据和详细的文本标注，支持长期动作生成研究。MoMask++模型通过多尺度标记序列和生成掩码变换器，实现了最先进的性能。", "motivation": "现有文本到动作生成方法受限于短文本或通用提示，缺乏细粒度控制和泛化能力。", "method": "提出SnapMoGen数据集，包含20K动作片段和122K详细文本描述；改进MoMask++模型，使用多尺度标记序列和单一生成掩码变换器。", "result": "MoMask++在HumanML3D和SnapMoGen基准测试中达到最优性能，并能处理用户输入的随意提示。", "conclusion": "SnapMoGen和MoMask++为文本到动作生成提供了更细粒度的控制和更强的泛化能力。"}}
{"id": "2507.09626", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09626", "abs": "https://arxiv.org/abs/2507.09626", "authors": ["Rodion Nazarov", "Anthony Quinn", "Robert Shorten", "Jakub Marecek"], "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems", "comment": null, "summary": "Artificial intelligence (AI) systems often interact with multiple agents. The\nregulation of such AI systems often requires that {\\em a priori\\/} guarantees\nof fairness and robustness be satisfied. With stochastic models of agents'\nresponses to the outputs of AI systems, such {\\em a priori\\/} guarantees\nrequire non-trivial reasoning about the corresponding stochastic systems. Here,\nwe present an open-source PyTorch-based toolkit for the use of stochastic\ncontrol techniques in modelling interconnections of AI systems and properties\nof their repeated uses. It models robustness and fairness desiderata in a\nclosed-loop fashion, and provides {\\em a priori\\/} guarantees for these\ninterconnections. The PyTorch-based toolkit removes much of the complexity\nassociated with the provision of fairness guarantees for closed-loop models of\nmulti-agent systems.", "AI": {"tldr": "介绍了一个基于PyTorch的工具包，用于通过随机控制技术建模多代理AI系统的交互，并提供公平性和鲁棒性的先验保证。", "motivation": "AI系统在与多代理交互时需要满足公平性和鲁棒性的先验保证，但现有方法对此的建模和保证较为复杂。", "method": "开发了一个基于PyTorch的工具包，利用随机控制技术建模AI系统与代理的交互，并以闭环方式评估公平性和鲁棒性。", "result": "工具包简化了多代理系统闭环模型中公平性保证的复杂性，并提供了先验保证。", "conclusion": "该工具包为AI系统在多代理环境中的公平性和鲁棒性提供了有效的建模和保证手段。"}}
{"id": "2507.10280", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10280", "abs": "https://arxiv.org/abs/2507.10280", "authors": ["Haomiaomiao Wang", "Conor Fennell", "Swati Poojary", "Mingming Liu"], "title": "A SUMO-Based Digital Twin for Evaluation of Conventional and Electric Vehicle Networks", "comment": "The paper has been accepted by the IEEE Energy Conversion Congress &\n  Expo (ECCE) Europe 2025 conference", "summary": "Digital twins are increasingly applied in transportation modelling to\nreplicate real-world traffic dynamics and evaluate mobility and energy\nefficiency. This study presents a SUMO-based digital twin that simulates mixed\nICEV-EV traffic on a major motorway segment, leveraging multi-sensor data\nfusion from inductive loops, GPS probes, and toll records. The model is\nvalidated under both complete and partial information scenarios, achieving\n93.1% accuracy in average speed estimation and 97.1% in average trip length\nestimation. Statistical metrics, including KL Divergence and Wasserstein\nDistance, demonstrate strong alignment between simulated and observed traffic\npatterns. Furthermore, CO2 emissions were overestimated by only 0.8-2.4%, and\nEV power consumption underestimated by 1.0-5.4%, highlighting the model's\nrobustness even with incomplete vehicle classification information.", "AI": {"tldr": "该研究提出了一种基于SUMO的数字孪生模型，用于模拟混合燃油车和电动车的交通动态，并通过多传感器数据融合验证了其高精度和鲁棒性。", "motivation": "数字孪生在交通建模中的应用日益广泛，本研究旨在通过多源数据融合提高模拟的准确性和鲁棒性。", "method": "利用SUMO平台构建数字孪生模型，融合感应线圈、GPS探测和收费记录等多传感器数据，验证了完整和部分信息场景下的性能。", "result": "模型在平均速度和行程长度估计中分别达到93.1%和97.1%的准确率，CO2排放和EV能耗误差较小。", "conclusion": "该数字孪生模型在混合交通模拟中表现出高精度和鲁棒性，适用于交通管理和能源效率评估。"}}
{"id": "2507.09681", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09681", "abs": "https://arxiv.org/abs/2507.09681", "authors": ["Osher Rafaeli", "Tal Svoray", "Ariel Nahlieli"], "title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model", "comment": "18 pages", "summary": "High-resolution elevation estimations are essential to understand catchment\nand hillslope hydrology, study urban morphology and dynamics, and monitor the\ngrowth, decline, and mortality of terrestrial ecosystems. Various deep learning\napproaches (e.g., super-resolution techniques, monocular depth estimation) have\nbeen developed to create high-resolution Digital Elevation Models (DEMs).\nHowever, super-resolution techniques are limited by the upscaling factor, and\nmonocular depth estimation lacks global elevation context, making its\nconversion to a seamless DEM restricted. The recently introduced technique of\nprompt-based monocular depth estimation has opened new opportunities to extract\nestimates of absolute elevation in a global context. We present here a\nframework for the estimation of high-resolution DEMs as a new paradigm for\nabsolute global elevation mapping. It is exemplified using low-resolution\nShuttle Radar Topography Mission (SRTM) elevation data as prompts and\nhigh-resolution RGB imagery from the National Agriculture Imagery Program\n(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived\nDEMs and employs a versatile prompting strategy, enabling tasks such as DEM\nestimation, void filling, and updating. Our framework achieves a 100x\nresolution gain (from 30-m to 30-cm), surpassing prior methods by an order of\nmagnitude. Evaluations across three diverse U.S. landscapes show robust\ngeneralization, capturing urban structures and fine-scale terrain features with\n< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological\nanalysis confirms suitability for hazard and environmental studies. We\ndemonstrate scalability by applying the framework to large regions in the U.S.\nand Israel. All code and pretrained models are publicly available at:\nhttps://osherr1996.github.io/prompt2dem_propage/.", "AI": {"tldr": "论文提出了一种基于提示的单目深度估计框架，用于生成高分辨率数字高程模型（DEM），显著提升了分辨率（从30米到30厘米），并在多种地形中表现出色。", "motivation": "高分辨率高程估计对水文、城市形态和生态系统研究至关重要，但现有方法（如超分辨率技术和单目深度估计）存在局限性。", "method": "利用低分辨率SRTM数据作为提示，结合高分辨率RGB图像，通过微调视觉变换器编码器实现DEM估计、填补和更新。", "result": "框架实现了100倍分辨率提升，误差低于5米，优于SRTM达18%，适用于水文和环境研究。", "conclusion": "该框架具有强泛化能力和可扩展性，为全球高程测绘提供了新范式。"}}
{"id": "2507.09537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09537", "abs": "https://arxiv.org/abs/2507.09537", "authors": ["Yangang Ren", "Guojian Zhan", "Chen Lv", "Jun Li", "Fenghua Liang", "Keqiang Li"], "title": "Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles", "comment": null, "summary": "Predicting the future of surrounding agents and accordingly planning a safe,\ngoal-directed trajectory are crucial for automated vehicles. Current methods\ntypically rely on imitation learning to optimize metrics against the ground\ntruth, often overlooking how scene understanding could enable more holistic\ntrajectories. In this paper, we propose Plan-MAE, a unified pretraining\nframework for prediction and planning that capitalizes on masked autoencoders.\nPlan-MAE fuses critical contextual understanding via three dedicated tasks:\nreconstructing masked road networks to learn spatial correlations, agent\ntrajectories to model social interactions, and navigation routes to capture\ndestination intents. To further align vehicle dynamics and safety constraints,\nwe incorporate a local sub-planning task predicting the ego-vehicle's near-term\ntrajectory segment conditioned on earlier segment. This pretrained model is\nsubsequently fine-tuned on downstream tasks to jointly generate the prediction\nand planning trajectories. Experiments on large-scale datasets demonstrate that\nPlan-MAE outperforms current methods on the planning metrics by a large margin\nand can serve as an important pre-training step for learning-based motion\nplanner.", "AI": {"tldr": "Plan-MAE是一个基于掩码自编码器的预测与规划统一预训练框架，通过三个任务学习场景理解，显著提升自动驾驶车辆的轨迹规划性能。", "motivation": "当前方法依赖模仿学习，忽视了场景理解对生成更全面轨迹的作用。Plan-MAE旨在通过预训练框架提升预测与规划的整体性能。", "method": "Plan-MAE通过三个任务学习场景理解：重建掩码道路网络（空间相关性）、重建代理轨迹（社交交互）、重建导航路线（目的地意图）。此外，加入局部子规划任务以对齐车辆动态与安全约束。", "result": "在大规模数据集上，Plan-MAE在规划指标上显著优于现有方法。", "conclusion": "Plan-MAE可作为基于学习的运动规划器的重要预训练步骤，显著提升自动驾驶车辆的轨迹规划能力。"}}
{"id": "2507.09245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09245", "abs": "https://arxiv.org/abs/2507.09245", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "AI": {"tldr": "Swa-bhasha Resource Hub提供2020-2025年间开发的罗马化僧伽罗语转僧伽罗语的数据资源和算法，推动了僧伽罗语NLP研究，并公开了数据集和工具。", "motivation": "为僧伽罗语NLP研究提供资源支持，特别是罗马化僧伽罗语的转写模型和应用开发。", "method": "收集并公开数据集和算法，进行现有转写应用的比较分析。", "result": "资源中心为研究提供了重要支持，并公开了相关工具和数据集。", "conclusion": "该资源中心显著推动了僧伽罗语NLP领域的研究和应用。"}}
{"id": "2507.09139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09139", "abs": "https://arxiv.org/abs/2507.09139", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment", "comment": "Preprint", "summary": "Human pose estimation traditionally relies on architectures that encode\nkeypoint priors, limiting their generalization to novel poses or unseen\nkeypoints. Recent language-guided approaches like LocLLM reformulate keypoint\nlocalization as a vision-language task, enabling zero-shot generalization\nthrough textual descriptions. However, LocLLM's linear projector fails to\ncapture complex spatial-textual interactions critical for high-precision\nlocalization. To address this, we propose PoseLLM, the first Large Language\nModel (LLM)-based pose estimation framework that replaces the linear projector\nwith a nonlinear MLP vision-language connector. This lightweight two-layer MLP\nwith GELU activation enables hierarchical cross-modal feature transformation,\nenhancing the fusion of visual patches and textual keypoint descriptions.\nTrained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO\nvalidation set, outperforming LocLLM by +0.4 AP, while maintaining strong\nzero-shot generalization on Human-Art and MPII. Our work demonstrates that a\nsimple yet powerful nonlinear connector significantly boosts localization\naccuracy without sacrificing generalization, advancing the state-of-the-art in\nlanguage-guided pose estimation. Code is available at\nhttps://github.com/Ody-trek/PoseLLM.", "AI": {"tldr": "PoseLLM提出了一种基于大型语言模型的姿态估计框架，通过非线性MLP视觉语言连接器提升定位精度，优于现有方法。", "motivation": "传统姿态估计方法依赖关键点先验，泛化能力有限；语言引导方法如LocLLM的线性投影器无法捕捉复杂空间-文本交互。", "method": "PoseLLM用非线性MLP替换线性投影器，实现跨模态特征分层转换，增强视觉与文本描述的融合。", "result": "在COCO验证集上达到77.8 AP，优于LocLLM 0.4 AP，同时在Human-Art和MPII上保持强零样本泛化能力。", "conclusion": "非线性连接器显著提升定位精度且不牺牲泛化能力，推动了语言引导姿态估计的先进水平。"}}
{"id": "2507.09662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09662", "abs": "https://arxiv.org/abs/2507.09662", "authors": ["Jason Zhu", "Hongyu Li"], "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.", "AI": {"tldr": "大型推理模型（LRMs）在复杂任务上表现优异，但存在推理链冗长的问题，导致资源浪费和响应延迟。本文综述了如何实现简洁和自适应推理以优化LRMs的效率。", "motivation": "LRMs在复杂推理任务中表现出色，但对简单问题生成冗长推理链，浪费资源且影响实际应用。需研究如何缩短推理链并实现自适应推理。", "method": "综述了近期关于简洁和自适应推理的研究进展，包括方法、基准和未来挑战。", "result": "总结了优化LRMs推理效率的现有方法，并指出了未来研究方向。", "conclusion": "本文为研究者提供了该领域的全面概述，旨在启发新的自适应推理方法，以更好地利用LRMs。"}}
{"id": "2507.10352", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10352", "abs": "https://arxiv.org/abs/2507.10352", "authors": ["Alvaro Detailleur", "Guillaume Ducard", "Christopher Onder"], "title": "Improved Sum-of-Squares Stability Verification of Neural-Network-Based Controllers", "comment": null, "summary": "This work presents several improvements to the closed-loop stability\nverification framework using semialgebraic sets and convex semidefinite\nprogramming to examine neural-network-based control systems regulating\nnonlinear dynamical systems. First, the utility of the framework is greatly\nexpanded: two semialgebraic functions mimicking common, smooth activation\nfunctions are presented and compatibility with control systems incorporating\nRecurrent Equilibrium Networks (RENs) and thereby Recurrent Neural Networks\n(RNNs) is established. Second, the validity of the framework's state-of-the-art\nstability analyses is established via an alternate proof. Third, based on this\nproof, two new optimization problems simplifying the analysis of local\nstability properties are presented. To simplify the analysis of a closed-loop\nsystem's Region of Attraction (RoA), the first problem explicitly parameterizes\na class of candidate Lyapunov functions larger than in previous works. The\nsecond problem utilizes the unique guarantees available under the condition of\ninvariance to further expand the set of candidate Lyapunov functions and\ndirectly determine whether an invariant set forms part of the system's RoA.\nThese contributions are successfully demonstrated in two numerical examples and\nsuggestions for future research are provided.", "AI": {"tldr": "论文提出了对基于神经网络的非线性动态系统控制闭环稳定性验证框架的改进，包括扩展框架适用性、验证稳定性分析有效性及提出新的优化问题。", "motivation": "提高神经网络控制系统的稳定性验证能力，扩展框架的适用性，并简化局部稳定性分析。", "method": "使用半代数集和凸半定规划，提出两种半代数函数模拟激活函数，兼容RENs和RNNs，并通过新证明验证稳定性分析。", "result": "成功展示两种数值示例，验证了框架的有效性，并提出了未来研究方向。", "conclusion": "改进的框架扩展了适用性，简化了稳定性分析，为未来研究提供了方向。"}}
{"id": "2507.10222", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10222", "abs": "https://arxiv.org/abs/2507.10222", "authors": ["Mingzhi Xu", "Yizhe Zhang"], "title": "Spatial Lifting for Dense Prediction", "comment": "Preprint. Under review", "summary": "We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision.", "AI": {"tldr": "提出了一种名为Spatial Lifting（SL）的新方法，通过将输入提升到更高维度（如3D）并使用相应网络处理，在密集预测任务中表现优异，同时减少模型参数和推理成本。", "motivation": "传统密集预测方法通常计算成本高且模型参数多，SL旨在通过维度提升实现高效、准确的预测。", "method": "将2D输入（如图像）提升到更高维度（如3D），并使用对应维度的网络（如3D U-Net）处理，生成结构化输出。", "result": "在19个基准数据集（13个语义分割和6个深度估计）上验证，模型参数减少98%以上，推理成本降低，性能仍具竞争力。", "conclusion": "SL为密集预测任务提供了一种高效、准确且可靠的新范式。"}}
{"id": "2507.09538", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09538", "abs": "https://arxiv.org/abs/2507.09538", "authors": ["Zainab Ali", "Lujayn Al-Amir", "Ali Safa"], "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks", "comment": null, "summary": "Using neuromorphic computing for robotics applications has gained much\nattention in recent year due to the remarkable ability of Spiking Neural\nNetworks (SNNs) for high-precision yet low memory and compute complexity\ninference when implemented in neuromorphic hardware. This ability makes SNNs\nwell-suited for autonomous robot applications (such as in drones and rovers)\nwhere battery resources and payload are typically limited. Within this context,\nthis paper studies the use of SNNs for performing direct robot navigation and\nobstacle avoidance from LIDAR data. A custom robot platform equipped with a\nLIDAR is set up for collecting a labeled dataset of LIDAR sensing data together\nwith the human-operated robot control commands used for obstacle avoidance.\nCrucially, this paper provides what is, to the best of our knowledge, a first\nfocused study about the importance of neuron membrane leakage on the SNN\nprecision when processing LIDAR data for obstacle avoidance. It is shown that\nby carefully tuning the membrane potential leakage constant of the spiking\nLeaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to\nachieve on-par robot control precision compared to the use of a non-spiking\nConvolutional Neural Network (CNN). Finally, the LIDAR dataset collected during\nthis work is released as open-source with the hope of benefiting future\nresearch.", "AI": {"tldr": "论文研究了利用脉冲神经网络（SNN）处理激光雷达数据实现机器人导航和避障，并首次探讨了神经元膜泄漏对SNN精度的影响。", "motivation": "由于SNN在神经形态硬件中具有高精度、低内存和计算复杂度的优势，适合用于资源有限的自主机器人应用。", "method": "搭建了配备激光雷达的机器人平台，收集带标签的激光雷达数据及人工操作的控制命令，并研究了神经元膜泄漏对SNN精度的影响。", "result": "通过调整LIF神经元的膜电位泄漏常数，SNN的机器人控制精度可与非脉冲CNN媲美。", "conclusion": "论文首次揭示了神经元膜泄漏对SNN处理激光雷达数据的重要性，并开源了数据集以促进未来研究。"}}
{"id": "2507.09259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09259", "abs": "https://arxiv.org/abs/2507.09259", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "title": "Psychology-Driven Enhancement of Humour Translation", "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "AI": {"tldr": "提出了一种心理学启发的幽默分解机制（HDM），结合Chain-of-Thought（CoT）和幽默理论，显著提升了大型语言模型在幽默翻译中的表现。", "motivation": "现有大型语言模型在幽默翻译中存在语言干扰和缺乏幽默感的问题，需要改进。", "method": "采用心理学启发的HDM，结合CoT模仿人类思维过程，并融入幽默理论。", "result": "在开源幽默数据集上的实验显示，幽默、流畅性和连贯性分别平均提升7.75%、2.81%和6.13%。", "conclusion": "HDM有效提升了幽默翻译的质量，为跨文化沟通提供了更好的工具。"}}
{"id": "2507.09144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09144", "abs": "https://arxiv.org/abs/2507.09144", "authors": ["Zhimin Liao", "Ping Wei", "Ruijie Zhang", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", "comment": null, "summary": "Forecasting the evolution of 3D scenes and generating unseen scenarios via\noccupancy-based world models offers substantial potential for addressing corner\ncases in autonomous driving systems. While tokenization has revolutionized\nimage and video generation, efficiently tokenizing complex 3D scenes remains a\ncritical challenge for 3D world models. To address this, we propose\n$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method\ndecouples scene tokenization into intra-scene and inter-scene tokenizers. The\nintra-scene tokenizer employs a multi-scale residual quantization strategy to\nhierarchically compress 3D scenes while preserving spatial details. The\ninter-scene tokenizer residually aggregates temporal dependencies across\ntimesteps. This dual design preserves the compactness of 3D tokenizers while\nretaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only\nGPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder\narchitecture. The encoder aggregates spatial context from the current scene and\npredicts a transformation matrix to enable high-level control over scene\ngeneration. The decoder, conditioned on this matrix and historical tokens,\nensures temporal consistency during generation. Experiments demonstrate that\n$I^{2}$-World achieves state-of-the-art performance, outperforming existing\nmethods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while\nexhibiting exceptional computational efficiency: it requires merely 2.9 GB of\ntraining memory and achieves real-time inference at 37.0 FPS. Our code is\navailable on https://github.com/lzzzzzm/II-World.", "AI": {"tldr": "提出了一种名为$I^{2}$-World的高效4D占用预测框架，通过解耦场景标记化实现动态3D场景建模，性能显著优于现有方法。", "motivation": "解决自动驾驶系统中3D场景演化的预测和生成问题，尤其是处理复杂3D场景的标记化挑战。", "method": "采用双标记器设计（场景内和场景间），结合编码器-解码器架构，实现高效4D占用预测。", "result": "在4D占用预测任务中，mIoU和IoU分别提升25.1%和36.9%，训练内存仅需2.9GB，推理速度达37.0FPS。", "conclusion": "$I^{2}$-World在性能和效率上均表现出色，为自动驾驶系统提供了强大的场景建模工具。"}}
{"id": "2507.09742", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09742", "abs": "https://arxiv.org/abs/2507.09742", "authors": ["Xiaofeng Xiao", "Bo Shen", "Xubo Yue"], "title": "Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations", "comment": null, "summary": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume\nof data streams requiring real-time monitoring continues to grow. However, due\nto limited resources, it is impractical to place sensors at every location to\ndetect unexpected shifts. Therefore, it is necessary to develop an optimal\nsensor placement strategy that enables partial observability of the system\nwhile detecting anomalies as quickly as possible. Numerous approaches have been\nproposed to address this challenge; however, most existing methods consider\nonly variable correlations and neglect a crucial factor: Causality. Moreover,\nalthough a few techniques incorporate causal analysis, they rely on\ninterventions-artificially creating anomalies-to identify causal effects, which\nis impractical and might lead to catastrophic losses. In this paper, we\nintroduce a causality-informed deep Q-network (Causal DQ) approach for\npartially observable sensor placement in anomaly detection. By integrating\ncausal information at each stage of Q-network training, our method achieves\nfaster convergence and tighter theoretical error bounds. Furthermore, the\ntrained causal-informed Q-network significantly reduces the detection time for\nanomalies under various settings, demonstrating its effectiveness for sensor\nplacement in large-scale, real-world data streams. Beyond the current\nimplementation, our technique's fundamental insights can be applied to various\nreinforcement learning problems, opening up new possibilities for real-world\ncausality-informed machine learning methods in engineering applications.", "AI": {"tldr": "提出了一种基于因果关系的深度Q网络（Causal DQ）方法，用于部分可观测的传感器布局，以快速检测异常。", "motivation": "由于资源限制，无法在所有位置部署传感器，且现有方法忽视因果关系或依赖不切实际的干预。", "method": "结合因果信息训练深度Q网络，实现快速收敛和更紧的理论误差界。", "result": "Causal DQ显著减少了异常检测时间，适用于大规模实时数据流。", "conclusion": "该方法不仅适用于传感器布局，还可推广到其他强化学习问题，为工程应用中的因果机器学习提供新思路。"}}
{"id": "2507.09714", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09714", "abs": "https://arxiv.org/abs/2507.09714", "authors": ["Yifan Zeng", "Yihan Li", "Suiyi He", "Koushil Sreenath", "Jun Zeng"], "title": "IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance", "comment": null, "summary": "This paper presents a unified planning-control strategy for competing with\nother racing cars called IteraOptiRacing in autonomous racing environments.\nThis unified strategy is proposed based on Iterative Linear Quadratic Regulator\nfor Iterative Tasks (i2LQR), which can improve lap time performance in the\npresence of surrounding racing obstacles. By iteratively using the ego car's\nhistorical data, both obstacle avoidance for multiple moving cars and time cost\noptimization are considered in this unified strategy, resulting in\ncollision-free and time-optimal generated trajectories. The algorithm's\nconstant low computation burden and suitability for parallel computing enable\nreal-time operation in competitive racing scenarios. To validate its\nperformance, simulations in a high-fidelity simulator are conducted with\nmultiple randomly generated dynamic agents on the track. Results show that the\nproposed strategy outperforms existing methods across all randomly generated\nautonomous racing scenarios, enabling enhanced maneuvering for the ego racing\ncar.", "AI": {"tldr": "提出了一种基于i2LQR的统一规划控制策略IteraOptiRacing，用于自动驾驶赛车环境中的多车竞争，优化圈速并避免碰撞。", "motivation": "在自动驾驶赛车环境中，需要一种能够同时优化圈速并避免与其他赛车碰撞的策略。", "method": "基于i2LQR的统一策略，利用历史数据迭代优化轨迹，实现多车避障和圈速优化。", "result": "仿真结果表明，该策略在所有随机生成的赛车场景中均优于现有方法，实现了实时操作。", "conclusion": "IteraOptiRacing策略在自动驾驶赛车中表现出色，能够生成无碰撞且时间最优的轨迹。"}}
{"id": "2507.10461", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10461", "abs": "https://arxiv.org/abs/2507.10461", "authors": ["Tao Tang", "Chengxu Yang"], "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening", "comment": "To appear in the proceedings of the 6th International Conference on\n  Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5\n  pages, 6 figures", "summary": "Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.", "AI": {"tldr": "RAPNet提出了一种基于内容自适应卷积的遥感图像融合方法，通过RAPConv和PAN-DFF模块提升空间细节和光谱保真度，性能优于现有方法。", "motivation": "传统CNN在遥感图像融合中因卷积核均匀应用而忽略局部内容变化，限制了性能。", "method": "RAPNet采用RAPConv生成空间自适应卷积核，并结合PAN-DFF模块通过注意力机制平衡空间细节与光谱保真度。", "result": "在公开数据集上，RAPNet在定量和定性评估中均优于现有方法，消融实验验证了自适应组件的有效性。", "conclusion": "RAPNet通过自适应卷积和动态特征融合，显著提升了遥感图像融合的精度和效果。"}}
{"id": "2507.09725", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09725", "abs": "https://arxiv.org/abs/2507.09725", "authors": ["Gabriel G. Gattaux", "Julien R. Serres", "Franck Ruffier", "Antoine Wystrach"], "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks", "comment": "Published by Springer Nature with the 14th bioinspired and biohybrid\n  systems conference in Sheffield, and presented at the conference in July 2025", "summary": "Ants achieve robust visual homing with minimal sensory input and only a few\nlearning walks, inspiring biomimetic solutions for autonomous navigation. While\nMushroom Body (MB) models have been used in robotic route following, they have\nnot yet been applied to visual homing. We present the first real-world\nimplementation of a lateralized MB architecture for visual homing onboard a\ncompact autonomous car-like robot. We test whether the sign of the angular path\nintegration (PI) signal can categorize panoramic views, acquired during\nlearning walks and encoded in the MB, into \"goal on the left\" and \"goal on the\nright\" memory banks, enabling robust homing in natural outdoor settings. We\nvalidate this approach through four incremental experiments: (1) simulation\nshowing attractor-like nest dynamics; (2) real-world homing after decoupled\nlearning walks, producing nest search behavior; (3) homing after random walks\nusing noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal\nbehavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to\ncontrol velocity. This mimics the accurate homing behavior of ants and\nfunctionally resembles waypoint-based position control in robotics, despite\nrelying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with\n32x32 pixel views and a memory footprint under 9 kB, our system offers a\nbiologically grounded, resource-efficient solution for autonomous visual\nhoming.", "AI": {"tldr": "该论文提出了一种基于蚂蚁视觉归巢行为的生物启发方法，首次在真实世界中实现了一种侧向化蘑菇体（MB）架构，用于紧凑型自动驾驶车辆的视觉归巢。", "motivation": "蚂蚁仅需少量感官输入和学习行走即可实现稳健的视觉归巢，这启发了自主导航的生物仿生解决方案。", "method": "通过四个渐进实验验证了角路径积分（PI）信号的符号是否可以将全景视图分类为“目标在左”和“目标在右”记忆库，从而实现自然户外环境中的稳健归巢。", "result": "实验表明，该方法在真实世界中实现了类似蚂蚁的精确归巢行为，且仅依赖视觉输入，资源占用极低。", "conclusion": "该系统为自主视觉归巢提供了一种生物基础且资源高效的解决方案。"}}
{"id": "2507.09282", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09282", "abs": "https://arxiv.org/abs/2507.09282", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "title": "ClaritySpeech: Dementia Obfuscation in Speech", "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "AI": {"tldr": "提出ClaritySpeech框架，通过ASR、文本混淆和零样本TTS技术改善痴呆症患者的语音，提升隐私和可访问性。", "motivation": "痴呆症影响语音模式，现有语音技术难以处理，需解决隐私和可访问性问题。", "method": "结合ASR、文本混淆和零样本TTS，无需微调即可修正语音并保留说话者身份。", "result": "在ADReSS和ADReSSo数据集上，F1分数下降16%和10%，但WER显著改善，语音质量提升。", "conclusion": "ClaritySpeech有效提升痴呆症语音的可访问性和隐私保护，适用于低数据环境。"}}
{"id": "2507.09168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09168", "abs": "https://arxiv.org/abs/2507.09168", "authors": ["Haiming Zhu", "Yangyang Xu", "Chenshu Xu", "Tingrui Shen", "Wenxi Liu", "Yong Du", "Jun Yu", "Shengfeng He"], "title": "Stable Score Distillation", "comment": null, "summary": "Text-guided image and 3D editing have advanced with diffusion-based models,\nyet methods like Delta Denoising Score often struggle with stability, spatial\ncontrol, and editing strength. These limitations stem from reliance on complex\nauxiliary structures, which introduce conflicting optimization signals and\nrestrict precise, localized edits. We introduce Stable Score Distillation\n(SSD), a streamlined framework that enhances stability and alignment in the\nediting process by anchoring a single classifier to the source prompt.\nSpecifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves\ncross-prompt alignment, and introduces a constant term null-text branch to\nstabilize the optimization process. This approach preserves the original\ncontent's structure and ensures that editing trajectories are closely aligned\nwith the source prompt, enabling smooth, prompt-specific modifications while\nmaintaining coherence in surrounding regions. Additionally, SSD incorporates a\nprompt enhancement branch to boost editing strength, particularly for style\ntransformations. Our method achieves state-of-the-art results in 2D and 3D\nediting tasks, including NeRF and text-driven style edits, with faster\nconvergence and reduced complexity, providing a robust and efficient solution\nfor text-guided editing.", "AI": {"tldr": "提出了一种名为稳定分数蒸馏（SSD）的新框架，通过简化优化过程提升文本引导图像和3D编辑的稳定性和对齐性。", "motivation": "现有方法如Delta Denoising Score在稳定性、空间控制和编辑强度方面存在不足，主要由于依赖复杂的辅助结构导致优化信号冲突和局部编辑受限。", "method": "SSD通过固定一个分类器到源提示，利用Classifier-Free Guidance（CFG）方程实现跨提示对齐，并引入常数项空文本分支稳定优化过程。此外，还加入了提示增强分支以提升编辑强度。", "result": "SSD在2D和3D编辑任务（如NeRF和文本驱动风格编辑）中取得了最先进的结果，具有更快的收敛速度和更低的复杂度。", "conclusion": "SSD为文本引导编辑提供了一个高效且稳健的解决方案，能够保持原始内容结构并实现平滑的提示特定修改。"}}
{"id": "2507.09751", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09751", "abs": "https://arxiv.org/abs/2507.09751", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.", "AI": {"tldr": "提出一种将大语言模型（LLM）整合到形式语义解释函数中的方法，以解决其逻辑不一致性问题。", "motivation": "尽管LLM在自然语言理解和生成方面表现出色，但其输出存在逻辑不一致问题，需要一种方法在形式推理中利用其广泛知识。", "method": "将LLM直接整合到一种矛盾容忍逻辑的形式语义解释函数中，并通过实验验证其可行性。", "result": "实验证明该方法可行，且不同于以往工作，它提供了一个理论框架，既能利用LLM知识，又能保持逻辑的可靠性和完备性。", "conclusion": "该方法为神经符号推理提供了一个理论框架，解决了LLM的逻辑不一致问题。"}}
{"id": "2507.09822", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09822", "abs": "https://arxiv.org/abs/2507.09822", "authors": ["Darshan Gadginmath", "Farhad Nawaz", "Minjun Sung", "Faizan M Tariq", "Sangjae Bae", "David Isele", "Fabio Pasqualetti", "Jovin Dsa"], "title": "Active Probing with Multimodal Predictions for Motion Planning", "comment": "To appear at IROS '25. 8 pages. 3 tables. 6 figures", "summary": "Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.", "AI": {"tldr": "论文提出了一种结合轨迹规划、多模态预测和主动探测的统一框架，用于增强不确定性下的决策能力，并通过实验验证了其有效性。", "motivation": "动态环境中导航需要处理其他代理行为的不确定性，现有方法难以同时处理多模态预测和主动探测。", "method": "开发了一种新的风险度量，通过混合模型整合多模态预测不确定性，并结合主动探测机制减少预测模糊性。", "result": "在MetaDrive仿真环境中验证了框架的有效性，成功处理了复杂交通场景中的不确定性预测。", "conclusion": "该框架在多模态预测和主动探测方面表现出色，适用于现实世界的自主导航挑战。"}}
{"id": "2507.09836", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09836", "abs": "https://arxiv.org/abs/2507.09836", "authors": ["Vindula Jayawardana", "Sirui Li", "Yashar Farid", "Cathy Wu"], "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems", "comment": null, "summary": "Autonomous vehicles (AVs) are becoming increasingly popular, with their\napplications now extending beyond just a mode of transportation to serving as\nmobile actuators of a traffic flow to control flow dynamics. This contrasts\nwith traditional fixed-location actuators, such as traffic signals, and is\nreferred to as Lagrangian traffic control. However, designing effective\nLagrangian traffic control policies for AVs that generalize across traffic\nscenarios introduces a major challenge. Real-world traffic environments are\nhighly diverse, and developing policies that perform robustly across such\ndiverse traffic scenarios is challenging. It is further compounded by the joint\ncomplexity of the multi-agent nature of traffic systems, mixed motives among\nparticipants, and conflicting optimization objectives subject to strict\nphysical and external constraints. To address these challenges, we introduce\nMulti-Residual Mixture of Expert Learning (MRMEL), a novel framework for\nLagrangian traffic control that augments a given suboptimal nominal policy with\na learned residual while explicitly accounting for the structure of the traffic\nscenario space. In particular, taking inspiration from residual reinforcement\nlearning, MRMEL augments a suboptimal nominal AV control policy by learning a\nresidual correction, but at the same time dynamically selects the most suitable\nnominal policy from a pool of nominal policies conditioned on the traffic\nscenarios and modeled as a mixture of experts. We validate MRMEL using a case\nstudy in cooperative eco-driving at signalized intersections in Atlanta, Dallas\nFort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.\nThe results show that MRMEL consistently yields superior performance-achieving\nan additional 4%-9% reduction in aggregate vehicle emissions relative to the\nstrongest baseline in each setting.", "AI": {"tldr": "论文提出了一种名为MRMEL的新框架，用于设计自动驾驶车辆的拉格朗日交通控制策略，通过动态选择最优策略并结合残差学习，显著提升了交通流控制的性能。", "motivation": "自动驾驶车辆作为移动执行器在交通流控制中具有潜力，但传统固定执行器（如交通信号）的局限性以及交通场景的多样性使得设计通用且鲁棒的控制策略具有挑战性。", "method": "提出了多残差专家混合学习（MRMEL）框架，通过结合残差强化学习和动态选择最优策略，优化自动驾驶车辆的控制策略。", "result": "在真实交通场景的案例研究中，MRMEL实现了比基线方法额外4%-9%的车辆排放减少。", "conclusion": "MRMEL框架在复杂多变的交通环境中表现出色，为自动驾驶车辆的交通控制提供了有效的解决方案。"}}
{"id": "2507.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09424", "abs": "https://arxiv.org/abs/2507.09424", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "AI": {"tldr": "DATE-LM是一个用于评估数据归因方法的统一基准，通过三个关键任务衡量归因质量，并发现现有方法在不同任务中存在权衡。", "motivation": "当前缺乏对数据归因方法的系统性评估，尤其是在语言模型（LLM）中的应用。", "method": "引入DATE-LM基准，通过训练数据选择、毒性/偏见过滤和事实归因三个任务评估归因方法。", "result": "研究发现，没有单一方法在所有任务中表现最佳，且方法性能受任务设计影响。", "conclusion": "DATE-LM为未来LLM数据归因研究提供了基础，并发布了公开排行榜以促进社区参与。"}}
{"id": "2507.09180", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09180", "abs": "https://arxiv.org/abs/2507.09180", "authors": ["Zichun Xu", "Yuntao Li", "Zhaomin Wang", "Lei Zhuang", "Guocai Yang", "Jingdong Zhao"], "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning", "comment": null, "summary": "Depth information is robust to scene appearance variations and inherently\ncarries 3D spatial details. In this paper, a visual backbone based on the\nvision transformer is proposed to fuse RGB and depth modalities for enhancing\ngeneralization. Different modalities are first processed by separate CNN stems,\nand the combined convolutional features are delivered to the scalable vision\ntransformer to obtain visual representations. Moreover, a contrastive\nunsupervised learning scheme is designed with masked and unmasked tokens to\naccelerate the sample efficiency during the reinforcement learning progress.\nFor sim2real transfer, a flexible curriculum learning schedule is developed to\ndeploy domain randomization over training processes.", "AI": {"tldr": "提出了一种基于视觉Transformer的视觉主干网络，融合RGB和深度模态以增强泛化能力，结合对比学习和课程学习优化训练。", "motivation": "深度信息对场景外观变化具有鲁棒性且携带3D空间细节，因此融合RGB和深度模态可提升模型的泛化能力。", "method": "分别通过CNN处理不同模态，将卷积特征输入可扩展的视觉Transformer获取视觉表示；设计对比无监督学习方案，结合掩码和非掩码令牌提升样本效率；采用灵活的课程学习策略进行域随机化。", "result": "提出的方法能够有效融合多模态信息，并通过对比学习和课程学习优化训练过程。", "conclusion": "该方法在多模态融合和泛化能力提升方面具有潜力，适用于sim2real迁移任务。"}}
{"id": "2507.09801", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09801", "abs": "https://arxiv.org/abs/2507.09801", "authors": ["Peter Barnett", "Aaron Scher", "David Abecassis"], "title": "Technical Requirements for Halting Dangerous AI Activities", "comment": null, "summary": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans.", "AI": {"tldr": "论文探讨了AI快速发展带来的风险，并提出了通过技术干预实现协调暂停危险AI活动的方案。", "motivation": "AI的快速发展带来了失控、滥用、地缘政治不稳定和权力集中等前所未有的风险，需要政府采取行动以避免最坏结果。", "method": "提出了关键的技术干预措施，这些措施可以协调暂停危险的AI开发和部署。", "result": "这些干预措施能够限制多种危险的AI活动，并为潜在的AI治理计划提供技术基础。", "conclusion": "通过技术干预实现协调暂停危险AI活动，是应对AI风险的有效途径。"}}
{"id": "2507.10075", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10075", "abs": "https://arxiv.org/abs/2507.10075", "authors": ["Jie Pan", "Tianyi Wang", "Yangyang Wang", "Junfeng Jiao", "Christian Claudel"], "title": "TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic", "comment": "6 pages, 7 figures, accepted for IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "summary": "Automated vehicles (AVs) face a critical need to adopt socially compatible\nbehaviors and cooperate effectively with human-driven vehicles (HVs) in\nheterogeneous traffic environment. However, most existing lane-changing\nframeworks overlook HVs' dynamic trust levels, limiting their ability to\naccurately predict human driver behaviors. To address this gap, this study\nproposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.\nFirst, we formulate a multi-vehicle coalition game, incorporating fully\ncooperative interactions among AVs and partially cooperative behaviors from HVs\ninformed by real-time trust evaluations. Second, we develop an online trust\nevaluation method to dynamically estimate HVs' trust levels during\nlane-changing interactions, guiding AVs to select context-appropriate\ncooperative maneuvers. Lastly, social compatibility objectives are considered\nby minimizing disruption to surrounding vehicles and enhancing the\npredictability of AV behaviors, thereby ensuring human-friendly and\ncontext-adaptive lane-changing strategies. A human-in-the-loop experiment\nconducted in a highway on-ramp merging scenario validates our TGLD approach.\nResults show that AVs can effectively adjust strategies according to different\nHVs' trust levels and driving styles. Moreover, incorporating a trust mechanism\nsignificantly improves lane-changing efficiency, maintains safety, and\ncontributes to transparent and adaptive AV-HV interactions.", "AI": {"tldr": "本文提出了一种信任感知的博弈论换道决策（TGLD）框架，通过动态评估人类驾驶车辆的信任水平，优化自动驾驶车辆的换道策略，提高效率与安全性。", "motivation": "自动驾驶车辆（AVs）需要与人类驾驶车辆（HVs）在异构交通环境中有效协作，但现有换道框架忽视了HVs的动态信任水平，导致行为预测不准确。", "method": "提出TGLD框架，包括多车辆联盟博弈建模、实时信任评估方法，以及考虑社会兼容性目标的换道策略优化。", "result": "实验表明，AVs能根据HVs的信任水平和驾驶风格调整策略，显著提高换道效率并确保安全。", "conclusion": "TGLD框架通过信任机制实现了透明、自适应的AV-HV交互，为异构交通环境中的协作提供了有效解决方案。"}}
{"id": "2507.09857", "categories": ["cs.RO", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09857", "abs": "https://arxiv.org/abs/2507.09857", "authors": ["Xiaofei Wang", "Mingliang Han", "Tianyu Hao", "Cegang Li", "Yunbo Zhao", "Keke Tang"], "title": "AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective", "comment": "IJCAI'2025", "summary": "Adversarial attacks on robotic grasping provide valuable insights into\nevaluating and improving the robustness of these systems. Unlike studies that\nfocus solely on neural network predictions while overlooking the physical\nprinciples of grasping, this paper introduces AdvGrasp, a framework for\nadversarial attacks on robotic grasping from a physical perspective.\nSpecifically, AdvGrasp targets two core aspects: lift capability, which\nevaluates the ability to lift objects against gravity, and grasp stability,\nwhich assesses resistance to external disturbances. By deforming the object's\nshape to increase gravitational torque and reduce stability margin in the\nwrench space, our method systematically degrades these two key grasping\nmetrics, generating adversarial objects that compromise grasp performance.\nExtensive experiments across diverse scenarios validate the effectiveness of\nAdvGrasp, while real-world validations demonstrate its robustness and practical\napplicability", "AI": {"tldr": "AdvGrasp框架从物理角度对机器人抓取进行对抗攻击，通过变形物体形状降低抓取性能。", "motivation": "研究机器人抓取系统的鲁棒性，填补现有研究忽视物理原理的空白。", "method": "通过变形物体形状增加重力扭矩并减少稳定性裕度，系统性地降低抓取能力。", "result": "实验验证了AdvGrasp的有效性，并在实际应用中展示了其鲁棒性。", "conclusion": "AdvGrasp为机器人抓取的对抗攻击提供了物理视角的解决方案，具有实际应用价值。"}}
{"id": "2507.09470", "categories": ["cs.CL", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "AI": {"tldr": "研究优化了DRAGON Longformer模型用于临床文本分类，通过调整超参数、预处理和架构，显著提升了性能。", "motivation": "探索如何优化预训练模型以提高临床文本分类的准确性和效率。", "method": "使用400例临床案例训练模型，调整序列长度、学习率、训练轮次，并加入医学术语。", "result": "优化后模型性能显著提升：准确率从72.0%升至85.2%，F1分数从71.0%升至85.2%。", "conclusion": "优化模型在临床NLP中表现优异，具有广泛医疗应用潜力。"}}
{"id": "2507.09183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09183", "abs": "https://arxiv.org/abs/2507.09183", "authors": ["Yongwei Jiang", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning", "comment": "Accepted to ICCV 2025, 11 pages", "summary": "Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data\nscarcity and incremental learning in real-world scenarios. While pool-based\nprompting methods have demonstrated success in traditional incremental\nlearning, their effectiveness in FSCIL settings remains unexplored. This paper\npresents the first study of current prompt pool methods in FSCIL tasks,\nrevealing an unanticipated performance degradation in incremental sessions.\nThrough comprehensive analysis, we identify that this phenomenon stems from\ntoken-dimension saturation: with limited data, excessive prompts compete for\ntask-relevant information, leading to model overfitting. Based on this finding,\nwe propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively\nshifts pool-based prompt learning from the token dimension to the spatial\ndimension. LGSP-Prompt generates spatial prompts by synergistically combining\nlocal spatial features and global frequency-domain representations to highlight\nkey patterns in input images. We construct two spatial prompt pools enabling\ndynamic prompt selection to maintain acquired knowledge while effectively\nlearning novel sessions. Extensive experiments demonstrate that our approach\nachieves state-of-the-art performance across multiple FSCIL benchmarks, showing\nsignificant advantages in both base knowledge preservation and incremental\nlearning. Our implementation is available at\nhttps://github.com/Jywsuperman/LGSP.", "AI": {"tldr": "本文研究了Few-Shot Class-Incremental Learning（FSCIL）中的性能下降问题，并提出了一种新的空间提示方法LGSP-Prompt，显著提升了性能。", "motivation": "FSCIL面临数据稀缺和增量学习的双重挑战，现有提示池方法在FSCIL中效果不佳，需探索新方法。", "method": "提出LGSP-Prompt，将提示学习从token维度转向空间维度，结合局部空间特征和全局频域表示生成空间提示。", "result": "实验表明LGSP-Prompt在多个FSCIL基准测试中达到最优性能，显著优于现有方法。", "conclusion": "LGSP-Prompt通过空间提示解决了FSCIL中的性能下降问题，为增量学习提供了新思路。"}}
{"id": "2507.09850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09850", "abs": "https://arxiv.org/abs/2507.09850", "authors": ["Wei Du", "Branislav Kisacanin", "George Armstrong", "Shubham Toshniwal", "Ivan Moshkov", "Alexan Ayrapetyan", "Sadegh Mahdavi", "Dan Zhao", "Shizhe Diao", "Dragan Masulovic", "Marius Stanean", "Advaith Avadhanam", "Max Wang", "Ashmit Dutta", "Shitij Govil", "Sri Yanamandara", "Mihir Tandon", "Sriram Ananthakrishnan", "Vedant Rathi", "David Zhang", "Joonseok Kang", "Leon Luo", "Titu Andreescu", "Boris Ginsburg", "Igor Gitman"], "title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)", "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.", "AI": {"tldr": "通过少量高质量的长链思维（CoT）示例微调基础模型，可以显著提升其推理能力，甚至超越更大规模的模型。", "motivation": "探索是否仅通过提示或最小微调即可在基础模型中诱导长链思维推理能力。", "method": "使用20个来自推理模型的长链思维示例微调基础模型，并尝试其他来源的CoT数据。", "result": "微调后的模型性能超越更大规模的模型，但非推理模型或人工标注的CoT数据效果较差。", "conclusion": "高质量的专家CoT数据难以复制，但少量精心设计的人工标注数据仍能激活基础模型的推理能力。"}}
{"id": "2507.10204", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10204", "abs": "https://arxiv.org/abs/2507.10204", "authors": ["Abdelhakim Amer", "Mohit Mehindratta", "Yury Brodskiy", "Bilal Wehbe", "Erdal Kayacan"], "title": "REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles", "comment": null, "summary": "Inspection of complex underwater structures with tethered underwater vehicles\nis often hindered by the risk of tether entanglement. We propose REACT\n(real-time entanglement-aware coverage path planning for tethered underwater\nvehicles), a framework designed to overcome this limitation. REACT comprises a\nfast geometry-based tether model using the signed distance field (SDF) map for\naccurate, real-time simulation of taut tether configurations around arbitrary\nstructures in 3D. This model enables an efficient online replanning strategy by\nenforcing a maximum tether length constraint, thereby actively preventing\nentanglement. By integrating REACT into a coverage path planning framework, we\nachieve safe and optimal inspection paths, previously challenging due to tether\nconstraints. The complete REACT framework's efficacy is validated in a pipe\ninspection scenario, demonstrating safe, entanglement-free navigation and\nfull-coverage inspection. Simulation results show that REACT achieves complete\ncoverage while maintaining tether constraints and completing the total mission\n20% faster than conventional planners, despite a longer inspection time due to\nproactive avoidance of entanglement that eliminates extensive post-mission\ndisentanglement. Real-world experiments confirm these benefits, where REACT\ncompletes the full mission, while the baseline planner fails due to physical\ntether entanglement.", "AI": {"tldr": "REACT框架通过实时几何模型和路径规划，解决了水下缆线缠绕问题，提高了任务效率和安全性。", "motivation": "解决水下缆线缠绕问题，提升水下结构检查的效率和安全性。", "method": "使用基于几何的快速缆线模型（SDF）和实时路径规划策略，避免缠绕。", "result": "在管道检查场景中，REACT实现了无缠绕导航和全覆盖检查，任务完成时间缩短20%。", "conclusion": "REACT框架显著提升了水下检查任务的安全性和效率，优于传统方法。"}}
{"id": "2507.09858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09858", "abs": "https://arxiv.org/abs/2507.09858", "authors": ["Shuaikang Wang", "Tiecheng Guo", "Meng Guo"], "title": "Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths", "comment": "accepted to IEEE RA-L", "summary": "Safe navigation within a workspace is a fundamental skill for autonomous\nrobots to accomplish more complex tasks. Harmonic potentials are artificial\npotential fields that are analytical, globally convergent and provably free of\nlocal minima. Thus, it has been widely used for generating safe and reliable\nrobot navigation control policies. However, most existing methods do not allow\ncustomization of the harmonic potential fields nor the resulting paths,\nparticularly regarding their topological properties. In this paper, we propose\na novel method that automatically finds homotopy classes of paths that can be\ngenerated by valid harmonic potential fields. The considered complex workspaces\ncan be as general as forest worlds consisting of numerous overlapping\nstar-obstacles. The method is based on a hybrid optimization algorithm that\nsearches over homotopy classes, selects the structure of each tree-of-stars\nwithin the forest, and optimizes over the continuous weight parameters for each\npurged tree via the projected gradient descent. The key insight is to transform\nthe forest world to the unbounded point world via proper diffeomorphic\ntransformations. It not only facilitates a simpler design of the\nmulti-directional D-signature between non-homotopic paths, but also retain the\nsafety and convergence properties. Extensive simulations and hardware\nexperiments are conducted for non-trivial scenarios, where the navigation\npotentials are customized for desired homotopic properties. Project page:\nhttps://shuaikang-wang.github.io/CustFields.", "AI": {"tldr": "提出一种新方法，通过混合优化算法自动生成具有特定拓扑特性的谐波势场路径，适用于复杂工作空间。", "motivation": "现有谐波势场方法无法定制路径的拓扑特性，限制了其应用灵活性。", "method": "采用混合优化算法，搜索路径同伦类，选择星形障碍物结构，并通过投影梯度下降优化权重参数。", "result": "方法成功生成具有定制拓扑特性的安全导航路径，并在仿真和硬件实验中验证。", "conclusion": "该方法为复杂工作空间中的机器人导航提供了灵活且安全的解决方案。"}}
{"id": "2507.09474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09474", "abs": "https://arxiv.org/abs/2507.09474", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.", "AI": {"tldr": "本文介绍了CoNLL-2013共享任务的定义、数据集、评估指标及评分工具，并总结了参与团队的方法和评估结果。", "motivation": "共享任务旨在推动语法错误纠正领域的研究，提供统一的任务定义和评估标准。", "method": "介绍了任务的数据集、评估指标和评分工具，并概述了参与团队采用的不同方法。", "result": "总结了参与团队在语法错误纠正任务中的评估结果。", "conclusion": "通过共享任务，促进了语法错误纠正技术的发展，并提供了未来研究的参考。"}}
{"id": "2507.09184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09184", "abs": "https://arxiv.org/abs/2507.09184", "authors": ["Qiyan Zhao", "Xiaofeng Zhang", "Yiheng Li", "Yun Xing", "Xiaosong Yuan", "Feilong Tang", "Sinan Fan", "Xuhang Chen", "Xuyao Zhang", "Dahan Wang"], "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models", "comment": "Accepted in ACM MM 2025", "summary": "Hallucinations pose a significant challenge in Large Vision Language Models\n(LVLMs), with misalignment between multimodal features identified as a key\ncontributing factor. This paper reveals the negative impact of the long-term\ndecay in Rotary Position Encoding (RoPE), used for positional modeling in\nLVLMs, on multimodal alignment. Concretely, under long-term decay, instruction\ntokens exhibit uneven perception of image tokens located at different positions\nwithin the two-dimensional space: prioritizing image tokens from the\nbottom-right region since in the one-dimensional sequence, these tokens are\npositionally closer to the instruction tokens. This biased perception leads to\ninsufficient image-instruction interaction and suboptimal multimodal alignment.\nWe refer to this phenomenon as image alignment bias. To enhance instruction's\nperception of image tokens at different spatial locations, we propose\nMCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a\ntwo-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the\none-dimensional sequence order and two-dimensional spatial position of image\ntokens for positional modeling, mitigating hallucinations by alleviating image\nalignment bias. Experimental results of MCA-LLaVA across various hallucination\nand general benchmarks demonstrate its effectiveness and generality. The code\ncan be accessed in https://github.com/ErikZ719/MCA-LLaVA.", "AI": {"tldr": "论文提出MCA-LLaVA方法，通过改进RoPE的长距离衰减问题，缓解LVLMs中的图像对齐偏差，从而减少幻觉现象。", "motivation": "大型视觉语言模型（LVLMs）中的幻觉问题主要由多模态特征未对齐引起，而RoPE的长距离衰减加剧了这一问题。", "method": "提出基于曼哈顿距离的MCA-LLaVA，将一维序列顺序与二维空间位置结合建模，改善图像-指令交互。", "result": "实验表明MCA-LLaVA在幻觉和通用基准测试中表现优异。", "conclusion": "MCA-LLaVA通过优化空间位置建模，有效缓解了图像对齐偏差，提升了模型性能。"}}
{"id": "2507.09854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09854", "abs": "https://arxiv.org/abs/2507.09854", "authors": ["Aniruddha Chattopadhyay", "Raj Dandekar", "Kaushik Roy"], "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems", "comment": "Accepted as paper in 19th International Conference on Neurosymbolic\n  Learning and Reasoning,NeSy 2025", "summary": "Neurosymbolic artificial intelligence (AI) systems combine neural network and\nclassical symbolic AI mechanisms to exploit the complementary strengths of\nlarge scale, generalizable learning and robust, verifiable reasoning. Numerous\nclassifications of neurosymbolic AI illustrate how these two components can be\nintegrated in distinctly different ways. In this work, we propose\nreinterpreting instruction tuned large language models as model grounded\nsymbolic AI systems where natural language serves as the symbolic layer and\ngrounding is achieved through the models internal representation space. Within\nthis framework, we investigate and develop novel learning and reasoning\napproaches that preserve structural similarities to traditional learning and\nreasoning paradigms. Preliminary evaluations across axiomatic deductive\nreasoning procedures of varying complexity provide insights into the\neffectiveness of our approach in improving learning efficiency and reasoning\nreliability.", "AI": {"tldr": "该论文提出将指令调优的大型语言模型重新解释为基于模型的符号AI系统，利用自然语言作为符号层，并通过模型内部表示空间实现接地。", "motivation": "结合神经网络的通用学习能力与符号AI的可验证推理能力，探索新型学习与推理方法。", "method": "将自然语言作为符号层，利用模型内部表示空间实现接地，开发与传统学习推理范式结构相似的新方法。", "result": "初步评估表明，该方法在提高学习效率和推理可靠性方面有效。", "conclusion": "该框架为神经符号AI系统提供了一种新的实现方式，具有潜力提升学习与推理能力。"}}
{"id": "2507.10310", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10310", "abs": "https://arxiv.org/abs/2507.10310", "authors": ["Michael Schröder", "Eric Schöneberg", "Daniel Görges", "Hans D. Schotten"], "title": "Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic", "comment": null, "summary": "In practice, navigation of mobile robots in confined environments is often\ndone using a spatially discrete cost-map to represent obstacles. Path following\nis a typical use case for model predictive control (MPC), but formulating\nconstraints for obstacle avoidance is challenging in this case. Typically the\ncost and constraints of an MPC problem are defined as closed-form functions and\ntypical solvers work best with continuously differentiable functions. This is\ncontrary to spatially discrete occupancy grid maps, in which a grid's value\ndefines the cost associated with occupancy. This paper presents a way to\novercome this compatibility issue by re-formulating occupancy grid maps to\ncontinuously differentiable functions to be embedded into the MPC scheme as\nconstraints. Each obstacle is defined as a polygon -- an intersection of\nhalf-spaces. Any half-space is a linear inequality representing one edge of a\npolygon. Using AND and OR operators, the combined set of all obstacles and\ntherefore the obstacle avoidance constraints can be described. The key\ncontribution of this paper is the use of fuzzy logic to re-formulate such\nconstraints that include logical operators as inequality constraints which are\ncompatible with standard MPC formulation. The resulting MPC-based trajectory\nplanner is successfully tested in simulation. This concept is also applicable\noutside of navigation tasks to implement logical or verbal constraints in MPC.", "AI": {"tldr": "论文提出了一种将离散的占用网格地图转化为连续可微函数的方法，以便在模型预测控制（MPC）中嵌入障碍物避障约束。", "motivation": "解决MPC中障碍物避障约束与离散网格地图不兼容的问题。", "method": "使用模糊逻辑将多边形障碍物的逻辑运算符（AND/OR）转化为不等式约束，使其适用于标准MPC。", "result": "在仿真中成功测试了基于MPC的轨迹规划器。", "conclusion": "该方法不仅适用于导航任务，还可用于MPC中实现逻辑或语言约束。"}}
{"id": "2507.09985", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09985", "abs": "https://arxiv.org/abs/2507.09985", "authors": ["Samson Yu", "Kelvin Lin", "Harold Soh"], "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model", "comment": "Published at R:SS 2025", "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.", "AI": {"tldr": "Octopi-1.5是一个视觉-触觉-语言模型，支持多部位触觉信号处理和检索增强生成（RAG），通过手持触觉接口TMI实现交互，用于物体识别和操作建议。", "motivation": "触觉对人类和机器人至关重要，尤其在灵巧操作、材料识别和视觉遮挡场景中。Octopi-1.5旨在提升触觉模型的性能和应用范围。", "method": "Octopi-1.5结合多部位触觉信号处理和RAG模块，通过TMI手持触觉接口（配备GelSight和TAC-02传感器）实现交互。", "result": "模型能通过触觉输入和常识知识解决推理任务（如物体识别和操作建议），并支持实时学习新物体。", "conclusion": "Octopi-1.5展示了视觉-触觉-语言模型的进展，同时揭示了其局限性，推动了该领域的研究兴趣。"}}
{"id": "2507.09477", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "AI": {"tldr": "本文综述了检索增强生成（RAG）与推理的结合，提出统一的推理-检索视角，分析两者如何相互增强，并展望未来研究方向。", "motivation": "解决RAG在多步推理中的不足以及纯推理方法的事实错误问题，推动更有效的知识密集型系统。", "method": "通过分析推理如何优化RAG的各阶段（Reasoning-Enhanced RAG），以及检索知识如何支持复杂推理（RAG-Enhanced Reasoning），并介绍协同框架（Synergized RAG-Reasoning）。", "result": "展示了结合推理与检索的协同框架在知识密集型任务中的先进性能，并总结了方法、数据集和挑战。", "conclusion": "未来研究方向包括更有效、多模态适应、可信赖且以人为中心的RAG-推理系统。"}}
{"id": "2507.09200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09200", "abs": "https://arxiv.org/abs/2507.09200", "authors": ["Trong-Thuan Nguyen", "Pha Nguyen", "Jackson Cothren", "Alper Yilmaz", "Minh-Triet Tran", "Khoa Luu"], "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage", "comment": null, "summary": "The rapid proliferation of video in applications such as autonomous driving,\nsurveillance, and sports analytics necessitates robust methods for dynamic\nscene understanding. Despite advances in static scene graph generation and\nearly attempts at video scene graph generation, previous methods often suffer\nfrom fragmented representations, failing to capture fine-grained spatial\ndetails and long-range temporal dependencies simultaneously. To address these\nlimitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)\napproach, which synergistically integrates hierarchical feature aggregation\nwith cyclic temporal refinement to address these limitations. In particular,\nTHYME effectively models multi-scale spatial context and enforces temporal\nconsistency across frames, yielding more accurate and coherent scene graphs. In\naddition, we present AeroEye-v1.0, a novel aerial video dataset enriched with\nfive types of interactivity that overcome the constraints of existing datasets\nand provide a comprehensive benchmark for dynamic scene graph generation.\nEmpirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that\nthe proposed THYME approach outperforms state-of-the-art methods, offering\nimproved scene understanding in ground-view and aerial scenarios.", "AI": {"tldr": "论文提出了一种名为THYME的时空层次循环场景图方法，用于动态场景理解，解决了现有方法在细粒度空间细节和长时序依赖上的不足，并在新数据集AeroEye-v1.0上验证了其优越性。", "motivation": "动态场景理解在自动驾驶、监控和体育分析等应用中需求迫切，但现有方法在空间细节和时序一致性上表现不足。", "method": "提出THYME方法，结合层次特征聚合和循环时序优化，建模多尺度空间上下文并保持时序一致性。", "result": "在ASPIRe和AeroEye-v1.0数据集上，THYME优于现有方法，提升了场景理解的准确性。", "conclusion": "THYME方法有效解决了动态场景图生成的挑战，为复杂场景理解提供了新思路。"}}
{"id": "2507.09884", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09884", "abs": "https://arxiv.org/abs/2507.09884", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu", "Yongzhen Guo", "Wentao Zhang"], "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains", "comment": "Preprint, Under review", "summary": "Large language models (LLMs) increasingly rely on reinforcement learning (RL)\nto enhance their reasoning capabilities through feedback. A critical challenge\nis verifying the consistency of model-generated responses and reference\nanswers, since these responses are often lengthy, diverse, and nuanced.\nRule-based verifiers struggle with complexity, prompting the use of model-based\nverifiers. However, specialized verifiers lack flexibility, while general LLM\njudges can be inconsistent. Existing research primarily focuses on building\nbetter verifiers, yet a systematic evaluation of different types of verifiers'\nperformance across domains remains lacking, severely constraining the reliable\ndevelopment of Reinforcement Learning with Verifiable Reward (RLVR). To address\nthis, we propose VerifyBench--a cross-domain comprehensive benchmark for\nsystematically evaluating verifiers. We construct 4,000 expert-level questions\ncovering mathematics, physics, chemistry, and biology. Each question is\nequipped with reference answers and diverse responses. The reliability of the\nevaluation is ensured through a rigorous annotation process conducted by a\nmultidisciplinary expert team. We design a four-dimensional experimental\nframework to comprehensively compare the performance boundaries of specialized\nverifiers and general LLMs under combined conditions of extracted answers vs.\ncomplete responses, and short vs. long outputs. Our evaluation uncovers\nfundamental trade-offs in verifiers: while specialized verifiers achieve\nleading accuracy, they exhibit deficiencies in recall; general models show\nstronger inclusivity but unstable precision. More importantly, we discover\nverifiers' high sensitivity to input structure and inherent limitations in\ncross-domain generalization, providing critical insights into the bottlenecks\nof current verifier technology.", "AI": {"tldr": "论文提出了VerifyBench，一个跨领域综合基准，用于系统评估验证器性能，揭示了专业验证器和通用LLM在精度与召回率之间的权衡。", "motivation": "当前验证器在复杂、多样化的模型生成响应中表现不一致，缺乏系统性评估，限制了RLVR的可靠发展。", "method": "构建包含4,000个专家级问题的VerifyBench，设计四维实验框架比较专业验证器和通用LLM的性能。", "result": "专业验证器精度高但召回率低，通用模型包容性强但精度不稳定，验证器对输入结构敏感且跨领域泛化能力有限。", "conclusion": "验证器技术存在瓶颈，需进一步优化输入结构和跨领域能力，为RLVR发展提供关键见解。"}}
{"id": "2507.10003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10003", "abs": "https://arxiv.org/abs/2507.10003", "authors": ["Mohit Singh", "Mihir Dharmadhikari", "Kostas Alexis"], "title": "Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "This work presents a vision-based underwater exploration and inspection\nautonomy solution integrated into Ariel, a custom vision-driven underwater\nrobot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a\nrefraction-aware multi-camera visual-inertial state estimation method aided by\na learning-based proprioceptive robot velocity prediction method that enhances\nrobustness against visual degradation. Furthermore, our previously developed\nand extensively field-verified autonomous exploration and general visual\ninspection solution is integrated on Ariel, providing aerial drone-level\nautonomy underwater. The proposed system is field-tested in a submarine dry\ndock in Trondheim under challenging visual conditions. The field demonstration\nshows the robustness of the state estimation solution and the generalizability\nof the path planning techniques across robot embodiments.", "AI": {"tldr": "该论文提出了一种基于视觉的水下探索与检查自主解决方案，集成到定制的水下机器人Ariel中，通过多摄像头视觉-惯性状态估计和学习型机器人速度预测方法增强鲁棒性。", "motivation": "解决水下环境中视觉退化问题，提升自主探索和检查的鲁棒性。", "method": "采用折射感知的多摄像头视觉-惯性状态估计方法，并结合学习型机器人速度预测方法。", "result": "在潜艇干船坞的实地测试中展示了状态估计的鲁棒性和路径规划技术的通用性。", "conclusion": "该系统在水下环境中实现了类似空中无人机的自主性，适用于复杂视觉条件。"}}
{"id": "2507.09482", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "论文提出了M2SaG数据集和ViSP框架，用于多模态讽刺生成，通过PPO和对比学习提升生成质量。", "motivation": "现有讽刺生成研究依赖文本模态且忽视视觉线索，导致图像内容与讽刺意图不匹配。", "method": "提出ViSP框架，结合PPO和对比学习，利用DIP奖励分数优化讽刺文本生成。", "result": "ViSP在五个指标上超越基线模型，生成文本的讽刺分数（0.898 vs. 0.770）和事实不一致性（0.768 vs. 0.739）更高。", "conclusion": "ViSP能生成更高质量的讽刺内容，数据集和代码将公开。"}}
{"id": "2507.09207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09207", "abs": "https://arxiv.org/abs/2507.09207", "authors": ["Alexander C. Ogren", "Berthy T. Feng", "Jihoon Ahn", "Katherine L. Bouman", "Chiara Daraio"], "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves", "comment": "ICCV 2025", "summary": "Wave propagation on the surface of a material contains information about\nphysical properties beneath its surface. We propose a method for inferring the\nthickness and stiffness of a structure from just a video of waves on its\nsurface. Our method works by extracting a dispersion relation from the video\nand then solving a physics-based optimization problem to find the best-fitting\nthickness and stiffness parameters. We validate our method on both simulated\nand real data, in both cases showing strong agreement with ground-truth\nmeasurements. Our technique provides a proof-of-concept for at-home health\nmonitoring of medically-informative tissue properties, and it is further\napplicable to fields such as human-computer interaction.", "AI": {"tldr": "通过视频分析表面波的传播特性，推断材料厚度和刚度的方法。", "motivation": "表面波的传播特性可以反映材料内部的物理性质，为健康监测和人机交互提供新方法。", "method": "从视频中提取波的色散关系，通过物理优化问题求解厚度和刚度参数。", "result": "在模拟和真实数据中验证，结果与地面真实测量高度一致。", "conclusion": "该方法为家庭健康监测和人机交互领域提供了概念验证。"}}
{"id": "2507.09955", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09955", "abs": "https://arxiv.org/abs/2507.09955", "authors": ["Luolin Xiong", "Haofen Wang", "Xi Chen", "Lu Sheng", "Yun Xiong", "Jingping Liu", "Yanghua Xiao", "Huajun Chen", "Qing-Long Han", "Yang Tang"], "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models", "comment": null, "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning.", "AI": {"tldr": "DeepSeek发布了V3和R1系列模型，因其低成本、高性能和开源优势引起全球关注。论文回顾了大模型的发展，介绍了DeepSeek的创新算法和工程突破，并分析了其对AI竞争格局的影响。", "motivation": "探讨DeepSeek模型的技术创新及其对AI领域的影响，为大模型的未来发展提供见解。", "method": "回顾大模型发展历程，介绍DeepSeek的MLA、MoE、MTP和GRPO等新算法，分析其工程优化和系统架构。", "result": "DeepSeek模型在性能和成本上具有竞争力，推动了AI领域的技术进步。", "conclusion": "DeepSeek的创新为AI大模型的发展提供了新方向，未来需关注数据、训练和推理的优化。"}}
{"id": "2507.10030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10030", "abs": "https://arxiv.org/abs/2507.10030", "authors": ["Marco Calì", "Alberto Sinigaglia", "Niccolò Turcato", "Ruggero Carli", "Gian Antonio Susto"], "title": "Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots", "comment": null, "summary": "Deep Reinforcement Learning (RL) has emerged as a powerful method for\naddressing complex control problems, particularly those involving underactuated\nrobotic systems. However, in some cases, policies may require refinement to\nachieve optimal performance and robustness aligned with specific task\nobjectives. In this paper, we propose an approach for fine-tuning Deep RL\npolicies using Evolutionary Strategies (ES) to enhance control performance for\nunderactuated robots. Our method involves initially training an RL agent with\nSoft-Actor Critic (SAC) using a surrogate reward function designed to\napproximate complex specific scoring metrics. We subsequently refine this\nlearned policy through a zero-order optimization step employing the Separable\nNatural Evolution Strategy (SNES), directly targeting the original score.\nExperimental evaluations conducted in the context of the 2nd AI Olympics with\nRealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning\nsignificantly improves agent performance while maintaining high robustness. The\nresulting controllers outperform established baselines, achieving competitive\nscores for the competition tasks.", "AI": {"tldr": "提出一种结合深度强化学习和进化策略的方法，用于优化欠驱动机器人的控制策略。", "motivation": "深度强化学习在处理复杂控制问题时表现优异，但策略可能需要进一步优化以实现特定任务目标。", "method": "先使用SAC训练RL代理，再通过SNES进化策略直接优化原始评分。", "result": "实验表明，该方法显著提升了性能，同时保持了高鲁棒性，优于基线方法。", "conclusion": "进化微调是一种有效的策略优化方法，适用于欠驱动机器人控制任务。"}}
{"id": "2507.09485", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09485", "abs": "https://arxiv.org/abs/2507.09485", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.", "AI": {"tldr": "本文提出了一种基于大语言模型（LLM）的ABSA方法，通过数据增强生成平衡的训练数据，并结合强化学习优化数据质量，显著提升了性能。", "motivation": "现有ABSA研究因短文本和小规模不平衡数据难以充分学习上下文信息，数据增强虽可行但质量难以保证。", "method": "利用LLM生成增强数据以平衡标签分布，并通过强化学习优化数据质量。", "result": "在英文ABSA基准数据集上表现优于现有方法。", "conclusion": "所提方法通过数据增强和强化学习显著提升了ABSA任务的性能。"}}
{"id": "2507.09209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09209", "abs": "https://arxiv.org/abs/2507.09209", "authors": ["Xiao Liang", "Di Wang", "Zhicheng Jiao", "Ronghan Li", "Pengfei Yang", "Quan Wang", "Tat-Seng Chua"], "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models", "comment": null, "summary": "The rapid advancements in Vision Language Models (VLMs) have prompted the\ndevelopment of multi-modal medical assistant systems. Despite this progress,\ncurrent models still have inherent probabilistic uncertainties, often producing\nerroneous or unverified responses-an issue with serious implications in medical\napplications. Existing methods aim to enhance the performance of Medical Vision\nLanguage Model (MedVLM) by adjusting model structure, fine-tuning with\nhigh-quality data, or through preference fine-tuning. However, these\ntraining-dependent strategies are costly and still lack sufficient alignment\nwith clinical expertise. To address these issues, we propose an\nexpert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance\n(Expert-CFG) to align MedVLM with clinical expertise without additional\ntraining. This framework introduces an uncertainty estimation strategy to\nidentify unreliable outputs. It then retrieves relevant references to assist\nexperts in highlighting key terms and applies classifier-free guidance to\nrefine the token embeddings of MedVLM, ensuring that the adjusted outputs are\ncorrect and align with expert highlights. Evaluations across three medical\nvisual question answering benchmarks demonstrate that the proposed Expert-CFG,\nwith 4.2B parameters and limited expert annotations, outperforms\nstate-of-the-art models with 13B parameters. The results demonstrate the\nfeasibility of deploying such a system in resource-limited settings for\nclinical use.", "AI": {"tldr": "提出了一种名为Expert-CFG的专家参与框架，无需额外训练即可将医学视觉语言模型与临床专业知识对齐，显著提升了性能。", "motivation": "当前医学视觉语言模型存在概率不确定性，可能产生错误或未经核实的回答，影响医疗应用。现有方法依赖训练，成本高且临床对齐不足。", "method": "引入不确定性估计策略识别不可靠输出，检索相关参考资料辅助专家标注关键术语，并应用无分类器指导调整模型输出。", "result": "在三个医学视觉问答基准测试中，Expert-CFG以4.2B参数和有限专家标注优于13B参数的最先进模型。", "conclusion": "Expert-CFG展示了在资源有限环境中部署临床系统的可行性。"}}
{"id": "2507.09989", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09989", "abs": "https://arxiv.org/abs/2507.09989", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Sheng Han"], "title": "Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient", "comment": null, "summary": "In heterogeneous multi-agent reinforcement learning (MARL), achieving\nmonotonic improvement plays a pivotal role in enhancing performance. The HAPPO\nalgorithm proposes a feasible solution by introducing a sequential update\nscheme, which requires independent learning with No Parameter-sharing (NoPS).\nHowever, heterogeneous MARL generally requires Partial Parameter-sharing\n(ParPS) based on agent grouping to achieve high cooperative performance. Our\nexperiments prove that directly combining ParPS with the sequential update\nscheme leads to the policy updating baseline drift problem, thereby failing to\nachieve improvement. To solve the conflict between monotonic improvement and\nParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)\nalgorithm. First, we replace the sequentially computed $Q_{\\psi}^s(s,a_{1:i})$\nwith the Optimal Marginal Q (OMQ) function $\\phi_{\\psi}^*(s,a_{1:i})$ derived\nfrom Q-functions. This maintains MAAD's monotonic improvement while eliminating\nthe conflict through optimal joint action sequences instead of sequential\npolicy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)\nas the critic function, employing pessimistic uncertainty-constrained loss to\noptimize different Q-value estimations. This provides the required Q-values for\nOMQ computation and stable baselines for actor updates. Finally, we implement a\nCentralized Critic Grouped Actor (CCGA) architecture that simultaneously\nachieves ParPS in local policy networks and accurate global Q-function\ncomputation. Experimental results in SMAC and MAMuJoCo environments demonstrate\nthat OMDPG outperforms various state-of-the-art MARL baselines.", "AI": {"tldr": "OMDPG算法通过引入最优边际Q函数和广义Q批评器，解决了异构多智能体强化学习中单调改进与部分参数共享的冲突，实验证明其性能优于现有方法。", "motivation": "异构多智能体强化学习（MARL）中，单调改进对性能提升至关重要，但部分参数共享（ParPS）与现有方法（如HAPPO）存在冲突，导致性能下降。", "method": "提出OMDPG算法：1) 用最优边际Q函数替代顺序计算的Q函数；2) 引入广义Q批评器优化Q值估计；3) 采用集中式批评器分组执行器架构（CCGA）。", "result": "在SMAC和MAMuJoCo环境中，OMDPG优于多种最先进的MARL基线方法。", "conclusion": "OMDPG成功解决了单调改进与ParPS的冲突，为异构MARL提供了高效解决方案。"}}
{"id": "2507.10047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10047", "abs": "https://arxiv.org/abs/2507.10047", "authors": ["Marc Kaufeld", "Mattia Piccinini", "Johannes Betz"], "title": "MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks", "comment": "8 pages, Submitted to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025), Australia", "summary": "This research introduces MP-RBFN, a novel formulation leveraging Radial Basis\nFunction Networks for efficiently learning Motion Primitives derived from\noptimal control problems for autonomous driving. While traditional motion\nplanning approaches based on optimization are highly accurate, they are often\ncomputationally prohibitive. In contrast, sampling-based methods demonstrate\nhigh performance but impose constraints on the geometric shape of trajectories.\nMP-RBFN combines the strengths of both by coupling the high-fidelity trajectory\ngeneration of sampling-based methods with an accurate description of vehicle\ndynamics. Empirical results show compelling performance compared to previous\nmethods, achieving a precise description of motion primitives at low inference\ntimes. MP-RBFN yields a seven times higher accuracy in generating optimized\nmotion primitives compared to existing semi-analytic approaches. We demonstrate\nthe practical applicability of MP-RBFN for motion planning by integrating the\nmethod into a sampling-based trajectory planner. MP-RBFN is available as\nopen-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.", "AI": {"tldr": "MP-RBFN是一种利用径向基函数网络高效学习自动驾驶最优控制问题中运动基元的新方法，结合了采样方法的高性能和优化方法的高精度。", "motivation": "传统优化方法计算量大，采样方法对轨迹几何形状有限制，MP-RBFN旨在结合两者优势。", "method": "MP-RBFN通过径向基函数网络耦合采样方法的高保真轨迹生成与车辆动力学的精确描述。", "result": "实验显示MP-RBFN性能优越，生成优化运动基元的精度是现有半解析方法的七倍，且推理时间短。", "conclusion": "MP-RBFN在运动规划中具有实际应用价值，已开源集成到采样轨迹规划器中。"}}
{"id": "2507.09497", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09497", "abs": "https://arxiv.org/abs/2507.09497", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.", "AI": {"tldr": "GoalfyMax是一个协议驱动的多智能体协作框架，通过标准化通信层和分层记忆系统提升协调性和适应性。", "motivation": "传统单用途AI系统在协调性、记忆重用和任务分解方面存在不足，难以满足复杂动态任务需求。", "method": "采用Agent-to-Agent通信层（基于MCP协议）和Experience Pack分层记忆系统，支持异步交互和知识保留。", "result": "在复杂任务编排基准测试中，GoalfyMax展现出优于基线框架的适应性、协调性和经验重用能力。", "conclusion": "GoalfyMax为多智能体系统提供了可扩展的未来解决方案。"}}
{"id": "2507.09214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09214", "abs": "https://arxiv.org/abs/2507.09214", "authors": ["Shiyi Mu", "Zichong Gu", "Hanqi Lyu", "Yilin Gao", "Shugong Xu"], "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline", "comment": "under review", "summary": "3D detection technology is widely used in the field of autonomous driving,\nwith its application scenarios gradually expanding from enclosed highways to\nopen conventional roads. For rare anomaly categories that appear on the road,\n3D detection models trained on closed sets often misdetect or fail to detect\nanomaly objects. To address this risk, it is necessary to enhance the\ngeneralization ability of 3D detection models for targets of arbitrary shapes\nand to possess the capability to filter out anomalies. The generalization of 3D\ndetection is limited by two factors: the coupled training of 2D and 3D, and the\ninsufficient diversity in the scale distribution of training samples. This\npaper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,\nwhich decouples the training strategy of 3D and 2D to release the\ngeneralization ability for arbitrary 3D foreground detection, and proposes an\nanomaly scoring algorithm based on foreground confidence prediction, achieving\ntarget-level anomaly scoring. In order to further verify and enhance the\ngeneralization of anomaly detection, we use a 3D rendering method to synthesize\ntwo augmented reality binocular stereo 3D detection datasets which named\nKITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k\npairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories\nas extra training data to address the sparse sample distribution issue.\nAdditionally, 58 rare categories form the KITTI-AR-OoD subset, which are not\nused in training to simulate zero-shot scenarios in real-world settings, solely\nfor evaluating 3D anomaly detection. Finally, the performance of the algorithm\nand the dataset is verified in the experiments. (Code and dataset can be\nobtained at https://github.com/xxxx/xxx).", "AI": {"tldr": "论文提出了一种基于立体视觉的3D异常物体检测算法（S3AD），通过解耦2D和3D训练策略提升泛化能力，并设计了基于前景置信度预测的异常评分算法。同时，构建了KITTI-AR数据集以验证算法性能。", "motivation": "解决3D检测模型在开放道路中对罕见异常物体的误检或漏检问题，提升模型对任意形状目标的泛化能力和异常过滤能力。", "method": "提出S3AD算法，解耦2D和3D训练策略，设计基于前景置信度的异常评分算法。构建KITTI-AR数据集，包含新增的97个类别，用于验证算法。", "result": "实验验证了算法和数据集的有效性，提升了3D异常检测的泛化能力。", "conclusion": "S3AD算法和KITTI-AR数据集为3D异常检测提供了有效的解决方案，增强了模型在开放场景中的适用性。"}}
{"id": "2507.10000", "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "pdf": "https://arxiv.org/pdf/2507.10000", "abs": "https://arxiv.org/abs/2507.10000", "authors": ["Mark Burgess"], "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.", "AI": {"tldr": "论文探讨了意图和意向性在科技领域的实际意义，提出了一种基于Promise Theory的低成本方法，通过多尺度异常检测和时空一致性来识别数据中的潜在意向性。", "motivation": "由于意图和意向性在科技领域的研究较少，论文旨在提供一种低成本、实用的方法来理解和识别数据中的潜在意向性。", "method": "利用Promise Theory的语义时空模型，通过多尺度异常检测和时空一致性分析，将内容分为“意图”和“背景”。", "result": "该方法能以极低的计算成本识别数据中的潜在意向性，适用于基本生物体，无需大规模人工概率批量处理。", "conclusion": "论文提出了一种简单但实用的方法，为理解数据中的意向性提供了新的视角，但其概念形成能力受限于代理的记忆容量。"}}
{"id": "2507.10055", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10055", "abs": "https://arxiv.org/abs/2507.10055", "authors": ["Muhtadin", "I Wayan Agus Darmawan", "Muhammad Hilmi Rusydiansyah", "I Ketut Eddy Purnama", "Chastine Fatichah", "Mauridhi Hery Purnomo"], "title": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems", "comment": null, "summary": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.", "AI": {"tldr": "提出了一种轻量级深度学习手势识别系统，用于自然控制协作机器人，模型仅需1,103参数和22KB大小，准确率达93.5%。通过量化和剪枝优化后，模型缩小至7KB，成功在UR5机器人上实时测试。", "motivation": "实现无需额外设备的自然人机交互，提升协作机器人的直观控制。", "method": "采用轻量级深度学习模型识别八种手势，并利用TensorFlow Lite进行量化和剪枝优化。", "result": "模型准确率93.5%，优化后仅7KB，成功在UR5机器人上实时运行。", "conclusion": "极轻量级模型也能实现准确、响应迅速的手势控制，为受限环境中的自然人机交互提供新可能。"}}
{"id": "2507.09506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09506", "abs": "https://arxiv.org/abs/2507.09506", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.", "AI": {"tldr": "论文提出了Ref-Long基准，用于评估长上下文语言模型（LCLMs）的长上下文引用能力，发现现有模型（包括GPT-4o）在此任务上表现不佳。", "motivation": "长上下文引用是LCLMs的关键任务，但尚未充分研究，因此需要开发专门的评估基准。", "method": "设计Ref-Long基准，包含三个子集（合成到现实场景），要求模型识别引用特定关键词的文档索引。", "result": "实验显示13个LCLMs在长上下文引用任务中存在显著不足，即使高级模型如GPT-4o也表现不佳。", "conclusion": "Ref-Long揭示了LCLMs在长上下文引用任务中的挑战，为未来研究提供了重要参考。"}}
{"id": "2507.09216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09216", "abs": "https://arxiv.org/abs/2507.09216", "authors": ["Jingguo Liu", "Han Yu", "Shigang Li", "Jianfeng Li"], "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models", "comment": "This paper is accecpted by ICMEW 2025", "summary": "Due to the current lack of large-scale datasets at the million-scale level,\ntasks involving panoramic images predominantly rely on existing two-dimensional\npre-trained image benchmark models as backbone networks. However, these\nnetworks are not equipped to recognize the distortions and discontinuities\ninherent in panoramic images, which adversely affects their performance in such\ntasks. In this paper, we introduce a novel spherical sampling method for\npanoramic images that enables the direct utilization of existing pre-trained\nmodels developed for two-dimensional images. Our method employs spherical\ndiscrete sampling based on the weights of the pre-trained models, effectively\nmitigating distortions while achieving favorable initial training values.\nAdditionally, we apply the proposed sampling method to panoramic image\nsegmentation, utilizing features obtained from the spherical model as masks for\nspecific channel attentions, which yields commendable results on commonly used\nindoor datasets, Stanford2D3D.", "AI": {"tldr": "提出了一种新的球形采样方法，用于全景图像，直接利用现有的二维预训练模型，减少失真并提升性能。", "motivation": "由于缺乏大规模全景图像数据集，现有任务依赖二维预训练模型，但这些模型无法处理全景图像的失真和间断问题。", "method": "采用基于预训练模型权重的球形离散采样方法，减少失真并获得良好初始训练值；应用于全景图像分割任务。", "result": "在Stanford2D3D室内数据集上取得了良好效果。", "conclusion": "球形采样方法有效解决了全景图像处理中的失真问题，并提升了任务性能。"}}
{"id": "2507.10007", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10007", "abs": "https://arxiv.org/abs/2507.10007", "authors": ["Zijun Chen", "Wenbo Hu", "Richang Hong"], "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning", "comment": null, "summary": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential.", "AI": {"tldr": "本文提出了一种通过利用模型内在的真实性编码来校准链式思维（CoT）推理准确性的新方法，显著提升了推理的可靠性。", "motivation": "链式思维推理在大型语言模型和多模态模型中表现出强大的深度推理能力，但其可靠性常因中间步骤错误的累积而受到影响。", "method": "通过识别特定注意力头激活反映推理步骤的真实性，训练置信度预测器动态选择最优推理路径。", "result": "实验表明，该方法在数学、符号和常识推理任务中显著优于现有基线方法，且在单模态和多模态场景下均表现出更高的准确性和可靠性。", "conclusion": "本研究为链式思维推理提供了一种新颖的可靠性改进路径，具有广泛的应用潜力。"}}
{"id": "2507.10082", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10082", "abs": "https://arxiv.org/abs/2507.10082", "authors": ["Amit Levy", "Itzik Klein"], "title": "Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications", "comment": "5 pages, 2 figures", "summary": "The unscented Kalman filter is a nonlinear estimation algorithm commonly used\nin navigation applications. The prediction of the mean and covariance matrix is\ncrucial to the stable behavior of the filter. This prediction is done by\npropagating the sigma points according to the dynamic model at hand. In this\npaper, we introduce an innovative method to propagate the sigma points\naccording to the nonlinear dynamic model of the navigation error state vector.\nThis improves the filter accuracy and navigation performance. We demonstrate\nthe benefits of our proposed approach using real sensor data recorded by an\nautonomous underwater vehicle during several scenarios.", "AI": {"tldr": "提出了一种改进的无迹卡尔曼滤波方法，通过非线性动态模型传播sigma点，提高了导航精度。", "motivation": "无迹卡尔曼滤波在导航应用中广泛使用，但均值和协方差矩阵的预测对滤波器稳定性至关重要。", "method": "引入创新方法，根据导航误差状态向量的非线性动态模型传播sigma点。", "result": "实验证明，该方法提高了滤波器精度和导航性能。", "conclusion": "改进的方法在真实传感器数据中表现出优越性。"}}
{"id": "2507.09509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09509", "abs": "https://arxiv.org/abs/2507.09509", "authors": ["Patrícia Schmidtová", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina Hämmerl", "Vilém Zouhar"], "title": "How Important is `Perfect' English for Machine Translation Prompts?", "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.", "AI": {"tldr": "研究探讨了提示质量对大型语言模型（LLM）在机器翻译和翻译评估任务中表现的影响，发现错误类型和数量对性能有显著差异。", "motivation": "大型语言模型在机器翻译中表现优异，但对提示中的错误敏感，研究旨在系统评估这种敏感性。", "method": "通过定量和定性分析，评估不同类型和程度的提示错误对LLM在翻译和翻译评估任务中的影响。", "result": "提示质量显著影响翻译性能，字符级和组合噪声比短语扰动更严重；LLM在极高噪声下仍能翻译。", "conclusion": "提示错误主要影响指令遵循而非翻译质量本身，LLM对噪声的鲁棒性超出人类预期。"}}
{"id": "2507.09217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09217", "abs": "https://arxiv.org/abs/2507.09217", "authors": ["Görkay Aydemir"], "title": "Online Long-term Point Tracking in the Foundation Model Era", "comment": "arXiv admin note: substantial text overlap with arXiv:2501.18487", "summary": "Point tracking aims to identify the same physical point across video frames\nand serves as a geometry-aware representation of motion. This representation\nsupports a wide range of applications, from robotics to augmented reality, by\nenabling accurate modeling of dynamic environments. Most existing long-term\ntracking approaches operate in an offline setting, where future frames are\navailable to refine predictions and recover from occlusions. However,\nreal-world scenarios often demand online predictions: the model must operate\ncausally, using only current and past frames. This constraint is critical in\nstreaming video and embodied AI, where decisions must be made immediately based\non past observations. Under such constraints, viewpoint invariance becomes\nessential. Visual foundation models, trained on diverse large-scale datasets,\noffer the potential for robust geometric representations. While they lack\ntemporal reasoning on their own, they can be integrated into tracking pipelines\nto enrich spatial features. In this thesis, we address the problem of long-term\npoint tracking in an online setting, where frames are processed sequentially\nwithout access to future information or sliding windows. We begin by evaluating\nthe suitability of visual foundation models for this task and find that they\ncan serve as useful initializations and be integrated into tracking pipelines.\nHowever, to enable long-term tracking in an online setting, a dedicated design\nis still required. In particular, maintaining coherence over time in this\ncausal regime requires memory to propagate appearance and context across\nframes. To address this, we introduce Track-On, a transformer-based model that\ntreats each tracked point as a query and processes video frames one at a time.\nTrack-On sets a new state of the art across seven public benchmarks,\ndemonstrating the feasibility of long-term tracking without future access.", "AI": {"tldr": "论文提出了一种在线点跟踪方法Track-On，利用视觉基础模型和Transformer架构，在无需未来帧信息的情况下实现长期跟踪。", "motivation": "现实场景需要在线预测，但现有方法多为离线处理，无法满足实时需求。视觉基础模型虽缺乏时序推理，但能提供稳健的空间特征。", "method": "结合视觉基础模型和Transformer架构，提出Track-On模型，将每个跟踪点视为查询，逐帧处理视频。", "result": "Track-On在七个公开基准测试中达到新最优性能，证明了无需未来帧信息的长期跟踪可行性。", "conclusion": "Track-On为在线点跟踪提供了有效解决方案，展示了视觉基础模型与Transformer结合在时序任务中的潜力。"}}
{"id": "2507.10045", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10045", "abs": "https://arxiv.org/abs/2507.10045", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.", "AI": {"tldr": "研究评估了三种大型语言模型（LLM）在SPARQL查询翻译中的表现，发现模型和提示策略对性能影响显著，且翻译方向影响结果。", "motivation": "解决知识图谱（KG）互操作性研究中SPARQL查询自动翻译的空白。", "method": "使用三种LLM（Llama-3-8B、DeepSeek-R1-Distill-Llama-70B、Mistral-Large-Instruct-2407），通过零样本、少样本和思维链变体测试，在两个基准数据集（DBpedia-Wikidata和DBLP-OpenAlex）上评估性能。", "result": "模型性能因模型和提示策略差异显著，Wikidata到DBpedia的翻译效果优于反向。", "conclusion": "LLM在SPARQL翻译中表现不一，翻译方向对结果有重要影响。"}}
{"id": "2507.10087", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10087", "abs": "https://arxiv.org/abs/2507.10087", "authors": ["Muhammad Tayyab Khan", "Ammar Waheed"], "title": "Foundation Model Driven Robotics: A Comprehensive Review", "comment": null, "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.", "AI": {"tldr": "本文综述了基础模型（如LLMs和VLMs）在机器人领域的应用，强调其语义理解、推理和多模态泛化能力，并分析了集成系统级策略的可行性及实际挑战。", "motivation": "基础模型的快速发展为机器人技术带来了新的可能性，但其在实际环境中的应用仍面临诸多挑战，需要系统性的评估和整合。", "method": "通过结构化综述，分类总结了基础模型在机器人领域的应用（如仿真设计、开放世界执行等），并讨论了关键趋势和瓶颈。", "result": "基础模型在机器人领域展现出强大的潜力，但也存在局限性（如数据不足、安全性问题等），需进一步研究解决。", "conclusion": "未来研究应致力于结合语义推理与物理智能，开发更鲁棒、可解释和具身化的模型。"}}
{"id": "2507.09536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09536", "abs": "https://arxiv.org/abs/2507.09536", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.", "AI": {"tldr": "论文探讨了如何将现有定义建模模型适配到白俄罗斯语，并提出了一个包含43,150条定义的新数据集。实验表明适配所需数据量少，但自动评估指标存在不足。", "motivation": "支持更多语言和方言的定义建模，以辅助词典编纂工作。", "method": "提出一个白俄罗斯语的新数据集，并适配现有定义建模系统。", "result": "适配模型所需数据量少，但自动评估指标未能完全捕捉模型性能。", "conclusion": "定义建模模型适配新语言可行，但需改进评估方法。"}}
{"id": "2507.09222", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09222", "abs": "https://arxiv.org/abs/2507.09222", "authors": ["Behraj Khan", "Tahir Syed"], "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift", "comment": null, "summary": "Foundation models like CLIP and SAM have transformed computer vision and\nmedical imaging via low-shot transfer learning. However, deployment of these\nmodels hindered by two key challenges: \\textit{distribution shift} between\ntraining and test data, and \\textit{confidence misalignment} that leads to\noverconfident incorrect predictions. These issues manifest differently in\nvision-language classification and medical segmentation tasks, yet existing\nsolutions remain domain-specific. We propose \\textit{StaRFM}, a unified\nframework addressing both challenges. It introduces a Fisher information\npenalty (FIP), extended to 3D medical data via patch-wise regularization, to\nreduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence\nmisalignment penalty (CMP), reformulated for voxel-level predictions,\ncalibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes\nbounds showing FIP controls generalization via the Fisher-Rao norm, while CMP\nminimizes calibration error through Brier score optimization. StaRFM shows\nconsistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19\nvision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in\nmedical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain\nperformance gap compared to prior benchmarking methods. The framework is\nplug-and-play, requiring minimal architectural changes for seamless integration\nwith foundation models. Code and models will be released at\nhttps://anonymous.4open.science/r/StaRFM-C0CD/README.md", "AI": {"tldr": "StaRFM是一个统一框架，通过Fisher信息惩罚和置信度对齐惩罚，解决了基础模型在视觉分类和医学分割任务中的分布偏移和置信度不匹配问题，显著提升了性能。", "motivation": "基础模型（如CLIP和SAM）在低样本迁移学习中表现出色，但面临训练与测试数据分布偏移和置信度不匹配问题，现有解决方案多为领域特定。", "method": "提出StaRFM框架，包含Fisher信息惩罚（FIP）和置信度不匹配惩罚（CMP），分别用于减少分布偏移和校准不确定性。", "result": "在19个视觉数据集和医学分割任务中，StaRFM显著提升了性能（如准确率+3.5%，DSC 84.7%），并降低了跨域性能差距。", "conclusion": "StaRFM是一个即插即用的通用框架，可无缝集成到基础模型中，解决分布偏移和置信度问题。"}}
{"id": "2507.10076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10076", "abs": "https://arxiv.org/abs/2507.10076", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "title": "On Gradual Semantics for Assumption-Based Argumentation", "comment": null, "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence.", "AI": {"tldr": "本文提出了一种新的渐进语义家族，用于为假设赋予辩证强度，填补了假设基础论证（ABA）中渐进语义研究的空白。", "motivation": "渐进语义在计算论证中是一种细粒度的替代方案，但尚未在ABA中得到充分研究，尽管ABA是一种流行的结构化论证形式。", "method": "通过将双极集基础论证框架作为ABA框架的抽象，并扩展了最先进的模块化渐进语义，提出了一种新的渐进ABA语义。", "result": "实验表明，渐进ABA语义满足平衡性和单调性等理想性质，并通过合成ABA框架验证了其收敛性。", "conclusion": "本文填补了ABA中渐进语义的研究空白，并验证了新语义的有效性和实用性。"}}
{"id": "2507.10105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10105", "abs": "https://arxiv.org/abs/2507.10105", "authors": ["Ines Sorrentino", "Giulio Romualdi", "Lorenzo Moretti", "Silvio Traversaro", "Daniele Pucci"], "title": "Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots", "comment": null, "summary": "This paper presents a novel framework for whole-body torque control of\nhumanoid robots without joint torque sensors, designed for systems with\nelectric motors and high-ratio harmonic drives. The approach integrates\nPhysics-Informed Neural Networks (PINNs) for friction modeling and Unscented\nKalman Filtering (UKF) for joint torque estimation, within a real-time torque\ncontrol architecture. PINNs estimate nonlinear static and dynamic friction from\njoint and motor velocity readings, capturing effects like motor actuation\nwithout joint movement. The UKF utilizes PINN-based friction estimates as\ndirect measurement inputs, improving torque estimation robustness. Experimental\nvalidation on the ergoCub humanoid robot demonstrates improved torque tracking\naccuracy, enhanced energy efficiency, and superior disturbance rejection\ncompared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using\na dynamic balancing experiment. The framework's scalability is shown by\nconsistent performance across robots with similar hardware but different\nfriction characteristics, without re-identification. Furthermore, a comparative\nanalysis with position control highlights the advantages of the proposed torque\ncontrol approach. The results establish the method as a scalable and practical\nsolution for sensorless torque control in humanoid robots, ensuring torque\ntracking, adaptability, and stability in dynamic environments.", "AI": {"tldr": "提出了一种无需关节扭矩传感器的人形机器人全身扭矩控制框架，结合物理信息神经网络（PINNs）和无迹卡尔曼滤波（UKF），在实时扭矩控制中提升了精度和效率。", "motivation": "解决人形机器人在缺乏关节扭矩传感器情况下的扭矩控制问题，提高扭矩跟踪精度和能量效率。", "method": "使用PINNs建模非线性摩擦，UKF估计关节扭矩，并在ergoCub机器人上进行实验验证。", "result": "实验显示该方法在扭矩跟踪、能量效率和抗干扰能力上优于现有RNEA算法，且具有硬件适应性。", "conclusion": "该方法为无传感器扭矩控制提供了可扩展且实用的解决方案，适用于动态环境中的稳定控制。"}}
{"id": "2507.09601", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "AI": {"tldr": "NMIXX模型通过跨语言金融嵌入和KorFinSTS基准测试，提升了金融领域语义捕捉能力，尤其在韩语等低资源语言中表现突出。", "motivation": "通用句子嵌入模型难以捕捉金融领域的专业语义，特别是在低资源语言（如韩语）中，存在领域术语、时间语义变化和双语词汇不对齐等问题。", "method": "提出NMIXX模型，通过18.8K高质量三元组（领域内释义、语义变化负样本和韩英精确翻译）进行微调，并发布KorFinSTS基准测试。", "result": "NMIXX在英语FinSTS和韩语KorFinSTS上分别提升0.10和0.22的Spearman's rho，优于其他基线模型，但通用STS性能略有下降。", "conclusion": "NMIXX和KorFinSTS为金融领域的跨语言表示学习提供了有效工具，强调了分词设计在低资源语言中的重要性。"}}
{"id": "2507.09230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09230", "abs": "https://arxiv.org/abs/2507.09230", "authors": ["G. Kutay Türkoglu", "Julian Tanke", "Iheb Belgacem", "Lev Markhasin"], "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views", "comment": "10 pages, 5 figures", "summary": "An ideal digital telepresence experience requires accurate replication of a\nperson's body, clothing, and movements. To capture and transfer these movements\ninto virtual reality, the egocentric (first-person) perspective can be adopted,\nwhich enables the use of a portable and cost-effective device without\nfront-view cameras. However, this viewpoint introduces challenges such as\nocclusions and distorted body proportions.\n  There are few works reconstructing human appearance from egocentric views,\nand none use a generative prior-based approach. Some methods create avatars\nfrom a single egocentric image during inference, but still rely on multi-view\ndatasets during training. To our knowledge, this is the first study using a\ngenerative backbone to reconstruct animatable avatars from egocentric inputs.\nBased on Stable Diffusion, our method reduces training burden and improves\ngeneralizability.\n  Inspired by methods such as SiTH and MagicMan, which perform 360-degree\nreconstruction from a frontal image, we introduce a pipeline that generates\nrealistic frontal views from occluded top-down images using ControlNet and a\nStable Diffusion backbone.\n  Our goal is to convert a single top-down egocentric image into a realistic\nfrontal representation and feed it into an image-to-motion model. This enables\ngeneration of avatar motions from minimal input, paving the way for more\naccessible and generalizable telepresence systems.", "AI": {"tldr": "该论文提出了一种基于生成先验的方法，从第一人称视角重建可动画化虚拟形象，利用Stable Diffusion减少训练负担并提高泛化能力。", "motivation": "解决从第一人称视角重建虚拟形象时的遮挡和身体比例失真问题，同时减少对多视角数据集的依赖。", "method": "使用ControlNet和Stable Diffusion框架，从遮挡的俯视图像生成真实的正视图，再输入到图像到动作模型中生成虚拟形象动作。", "result": "实现了从单张俯视图像生成真实正视图，并进一步生成虚拟形象动作，为更易用和泛化的远程呈现系统铺平道路。", "conclusion": "该方法首次利用生成先验从第一人称视角重建虚拟形象，显著减少了训练负担并提高了泛化能力。"}}
{"id": "2507.10106", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10106", "abs": "https://arxiv.org/abs/2507.10106", "authors": ["Harshal Nandigramwar", "Syed Qutub", "Kay-Ulrich Scholl"], "title": "BlueGlass: A Framework for Composite AI Safety", "comment": "Accepted at ICML 2025 [Actionable Interpretability Workshop]", "summary": "As AI systems become increasingly capable and ubiquitous, ensuring the safety\nof these systems is critical. However, existing safety tools often target\ndifferent aspects of model safety and cannot provide full assurance in\nisolation, highlighting a need for integrated and composite methodologies. This\npaper introduces BlueGlass, a framework designed to facilitate composite AI\nsafety workflows by providing a unified infrastructure enabling the integration\nand composition of diverse safety tools that operate across model internals and\noutputs. Furthermore, to demonstrate the utility of this framework, we present\nthree safety-oriented analyses on vision-language models for the task of object\ndetection: (1) distributional evaluation, revealing performance trade-offs and\npotential failure modes across distributions; (2) probe-based analysis of layer\ndynamics highlighting shared hierarchical learning via phase transition; and\n(3) sparse autoencoders identifying interpretable concepts. More broadly, this\nwork contributes foundational infrastructure and findings for building more\nrobust and reliable AI systems.", "AI": {"tldr": "本文介绍了BlueGlass框架，旨在通过整合多样化的安全工具来提升AI系统的安全性，并通过三项视觉语言模型分析验证其效用。", "motivation": "随着AI系统的能力增强和普及，确保其安全性变得至关重要，但现有工具无法单独提供全面保障，需要集成化的方法。", "method": "提出了BlueGlass框架，提供统一基础设施以整合和组合多样化的安全工具，覆盖模型内部和输出。", "result": "通过三项分析验证了框架的实用性：分布评估、基于探针的层次动态分析和稀疏自编码器识别可解释概念。", "conclusion": "BlueGlass为构建更稳健可靠的AI系统提供了基础架构和发现。"}}
{"id": "2507.10121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10121", "abs": "https://arxiv.org/abs/2507.10121", "authors": ["Seung Hyun Kim", "Jiamiao Guo", "Arman Tekinalp", "Heng-Sheng Chang", "Ugur Akcal", "Tixian Wang", "Darren Biskup", "Benjamin Walt", "Girish Chowdhary", "Girish Krishnan", "Prashant G. Mehta", "Mattia Gazzola"], "title": "Simulations and experiments with assemblies of fiber-reinforced soft actuators", "comment": "8 pages, 4 figures This work has been submitted to the IEEE for\n  possible publication", "summary": "Soft continuum arms (SCAs) promise versatile manipulation through mechanical\ncompliance, for assistive devices, agriculture, search applications, or\nsurgery. However, SCAs' real-world use is challenging, partly due to their\nhard-to-control non-linear behavior. Here, a simulation framework for SCAs\nmodularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is\ndeveloped and integrated with a video-tracking system for experimental testing\nand control design.", "AI": {"tldr": "开发了一种用于软连续臂（SCAs）的仿真框架，结合视频跟踪系统进行实验测试和控制设计。", "motivation": "软连续臂（SCAs）因其机械顺应性在多个领域具有潜力，但其非线性行为难以控制，限制了实际应用。", "method": "开发了模块化组装的纤维增强弹性体外壳（FREEs）仿真框架，并整合视频跟踪系统。", "result": "框架成功用于实验测试和控制设计。", "conclusion": "该仿真框架为SCAs的实际应用提供了可行的控制和测试解决方案。"}}
{"id": "2507.09628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09628", "abs": "https://arxiv.org/abs/2507.09628", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.", "AI": {"tldr": "SpreadPy是一个用于模拟认知单层和多层网络中激活传播的Python库，支持结构-功能关系的数值模拟研究。", "motivation": "研究认知、心理和临床现象中激活动态与网络结构的关系。", "method": "通过数值模拟和案例研究（数学焦虑、创造力任务、失语症）验证工具的有效性。", "result": "SpreadPy能区分不同认知状态（如高/低数学焦虑）、任务难度对激活轨迹的影响，以及失语症患者的错误类型与网络结构的相关性。", "conclusion": "SpreadPy为心理学、神经科学和教育研究提供了可重复的研究工具，支持理论和实证网络的建模。"}}
{"id": "2507.09242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09242", "abs": "https://arxiv.org/abs/2507.09242", "authors": ["Shiqi Jiang", "Xinpeng Li", "Xi Mao", "Changbo Wang", "Chenhui Li"], "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process", "comment": "ACM International Conference on Multimedia 2025", "summary": "Artistic image assessment has become a prominent research area in computer\nvision. In recent years, the field has witnessed a proliferation of datasets\nand methods designed to evaluate the aesthetic quality of paintings. However,\nmost existing approaches focus solely on static final images, overlooking the\ndynamic and multi-stage nature of the artistic painting process. To address\nthis gap, we propose a novel framework for human-aligned assessment of painting\nprocesses. Specifically, we introduce the Painting Process Assessment Dataset\n(PPAD), the first large-scale dataset comprising real and synthetic painting\nprocess images, annotated by domain experts across eight detailed attributes.\nFurthermore, we present PPJudge (Painting Process Judge), a Transformer-based\nmodel enhanced with temporally-aware positional encoding and a heterogeneous\nmixture-of-experts architecture, enabling effective assessment of the painting\nprocess. Experimental results demonstrate that our method outperforms existing\nbaselines in accuracy, robustness, and alignment with human judgment, offering\nnew insights into computational creativity and art education.", "AI": {"tldr": "提出了一种新框架PPJudge，用于评估绘画过程，填补了现有方法仅关注静态图像的不足。", "motivation": "现有艺术图像评估方法仅关注静态最终图像，忽略了绘画过程的动态性和多阶段性。", "method": "构建了PPAD数据集，并提出基于Transformer的PPJudge模型，采用时间感知位置编码和混合专家架构。", "result": "实验表明，PPJudge在准确性、鲁棒性和与人类判断的一致性上优于现有基线。", "conclusion": "该方法为计算创造力和艺术教育提供了新见解。"}}
{"id": "2507.10119", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10119", "abs": "https://arxiv.org/abs/2507.10119", "authors": ["Sadig Gojayev", "Ahmad Anaqreh", "Carolina Fortuna"], "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration", "comment": null, "summary": "Application migration in edge-cloud system enables high QoS and cost\neffective service delivery. However, automatically orchestrating such migration\nis typically solved with heuristic approaches. Starting from the Markov\nDecision Process (MDP), in this paper, we identify, analyze and compare\nselected state-of-the-art Artificial Intelligence (AI) planning and\nReinforcement Learning (RL) approaches for solving the class of edge-cloud\napplication migration problems that can be modeled as Towers of Hanoi (ToH)\nproblems. We introduce a new classification based on state space definition and\nanalyze the compared models also through this lense. The aim is to understand\navailable techniques capable of orchestrating such application migration in\nemerging computing continuum environments.", "AI": {"tldr": "论文探讨了边缘-云系统中应用迁移的自动编排问题，通过MDP框架比较了AI规划和强化学习方法，并基于状态空间定义提出了新分类。", "motivation": "研究旨在理解如何在计算连续环境中高效编排应用迁移，以提升服务质量和成本效益。", "method": "从MDP出发，分析并比较了AI规划和强化学习方法，特别关注可建模为汉诺塔问题的迁移场景，提出基于状态空间的新分类。", "result": "研究总结了现有技术的能力，为边缘-云环境中的应用迁移提供了方法比较和新视角。", "conclusion": "论文为计算连续环境中的应用迁移编排提供了技术分析和分类框架，有助于未来研究和实践。"}}
{"id": "2507.10131", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10131", "abs": "https://arxiv.org/abs/2507.10131", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints", "comment": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)", "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.", "AI": {"tldr": "GUIDER是一个双阶段概率框架，用于机器人推断人类意图，在导航和操作阶段均表现优异。", "motivation": "提高人机协作中意图推断的准确性，避免冲突并保持人类控制。", "method": "GUIDER采用双阶段方法：导航阶段结合控制器速度和占用网格，操作阶段结合显著性检测和几何可行性测试。", "result": "在25次试验中，导航稳定性达93-100%，操作稳定性达94-100%，显著优于基线方法。", "conclusion": "GUIDER在移动操作任务中显著提升了意图推断能力。"}}
{"id": "2507.09629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09629", "abs": "https://arxiv.org/abs/2507.09629", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "title": "An Exploration of Knowledge Editing for Arabic", "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.", "AI": {"tldr": "本文首次研究了阿拉伯语知识编辑（KE），评估了四种方法在阿拉伯语翻译数据集上的表现，发现参数化方法在跨语言泛化上表现不佳，而指令调优方法更稳健。", "motivation": "探索知识编辑在形态丰富的语言（如阿拉伯语）中的行为，填补了该领域的研究空白。", "method": "评估了四种方法（ROME、MEMIT、ICE和LTE）在阿拉伯语翻译数据集上的表现，并扩展了LTE到多语言环境。", "result": "参数化方法在跨语言泛化上表现不佳，而指令调优方法更稳健；多语言联合训练提升了编辑和迁移能力。", "conclusion": "阿拉伯语知识编辑的研究为未来工作提供了基准和多语言训练数据，支持进一步探索。"}}
{"id": "2507.09248", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09248", "abs": "https://arxiv.org/abs/2507.09248", "authors": ["Varsha Devi", "Amine Bohi", "Pardeep Kumar"], "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition", "comment": "13 Pages, 4 figures, 2 tables ICIAP 2025", "summary": "Context-aware emotion recognition (CAER) enhances affective computing in\nreal-world scenarios, but traditional methods often suffer from context\nbias-spurious correlation between background context and emotion labels (e.g.\nassociating ``garden'' with ``happy''). In this paper, we propose\n\\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces\n\\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the\nConvNeXt backbone by integrating Spatial Transformer Network and\nSqueeze-and-Excitation layers for enhanced feature recalibration. At the core\nof AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),\nwhich applies causal theory, perturbs context features, isolates spurious\ncorrelations, and performs an attention-driven correction guided by face\nfeatures to mitigate context bias. Experimental results on the CAER-S dataset\ndemonstrate the effectiveness of AGCD-Net, achieving state-of-the-art\nperformance and highlighting the importance of causal debiasing for robust\nemotion recognition in complex settings.", "AI": {"tldr": "AGCD-Net通过注意力引导的上下文去偏方法，结合因果干预模块，解决了情感识别中的上下文偏差问题，并在CAER-S数据集上取得了最优性能。", "motivation": "传统情感识别方法存在上下文偏差问题（如将背景与情感标签错误关联），影响了真实场景中的情感计算效果。", "method": "提出AGCD-Net模型，采用Hybrid ConvNeXt编码器（结合Spatial Transformer和Squeeze-and-Excitation层），并设计AG-CIM模块（基于因果理论扰动上下文特征，通过注意力校正减少偏差）。", "result": "在CAER-S数据集上实现了最先进的性能，验证了因果去偏对复杂场景下情感识别的有效性。", "conclusion": "AGCD-Net通过因果干预和注意力校正，显著提升了情感识别的鲁棒性，为实际应用提供了新思路。"}}
{"id": "2507.10124", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10124", "abs": "https://arxiv.org/abs/2507.10124", "authors": ["Thomas T. Hills"], "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making", "comment": "12 pages, 3 figures", "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making.", "AI": {"tldr": "论文探讨了利用人类心理学中的元认知提示（如“你可能是错的吗？”）来减少大型语言模型（LLM）中的偏见，展示了这种方法的有效性。", "motivation": "由于LLM仍在发展中，当前的偏见问题可能随时间变化，因此需要通用的去偏见策略。人类决策中的去偏见方法为此提供了灵感。", "method": "采用人类决策文献中的元认知提示（如“你可能是错的吗？”）来引导LLM反思其回答中的偏见和错误。", "result": "元认知提示能有效引导LLM揭示初始回答中未提及的偏见、错误和矛盾信息，改善模型输出。", "conclusion": "人类心理学为提示工程提供了新思路，利用元认知提示可显著提升LLM的自我反思和去偏见能力。"}}
{"id": "2507.10164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10164", "abs": "https://arxiv.org/abs/2507.10164", "authors": ["Egor Maslennikov", "Eduard Zaliaev", "Nikita Dudorov", "Oleg Shamanin", "Karanov Dmitry", "Gleb Afanasev", "Alexey Burkov", "Egor Lygin", "Simeon Nedelchev", "Evgeny Ponomarev"], "title": "Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains", "comment": null, "summary": "Developing robust locomotion controllers for bipedal robots with closed\nkinematic chains presents unique challenges, particularly since most\nreinforcement learning (RL) approaches simplify these parallel mechanisms into\nserial models during training. We demonstrate that this simplification\nsignificantly impairs sim-to-real transfer by failing to capture essential\naspects such as joint coupling, friction dynamics, and motor-space control\ncharacteristics. In this work, we present an RL framework that explicitly\nincorporates closed-chain dynamics and validate it on our custom-built robot\nTopA. Our approach enhances policy robustness through symmetry-aware loss\nfunctions, adversarial training, and targeted network regularization.\nExperimental results demonstrate that our integrated approach achieves stable\nlocomotion across diverse terrains, significantly outperforming methods based\non simplified kinematic models.", "AI": {"tldr": "提出了一种强化学习框架，显式结合闭链动力学，显著提升了双足机器人的运动控制鲁棒性。", "motivation": "现有强化学习方法将双足机器人的并行机制简化为串行模型，导致仿真到现实的迁移效果不佳，无法捕捉关节耦合、摩擦动力学等关键特性。", "method": "采用对称感知损失函数、对抗训练和目标网络正则化，显式结合闭链动力学。", "result": "实验表明，该方法在多样化地形上实现了稳定的运动控制，性能显著优于基于简化运动学模型的方法。", "conclusion": "显式结合闭链动力学的强化学习框架能有效提升双足机器人运动控制的鲁棒性和仿真到现实的迁移效果。"}}
{"id": "2507.09638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09638", "abs": "https://arxiv.org/abs/2507.09638", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.", "AI": {"tldr": "论文提出了一种基于GRPO的方法，显著提升了泰语法律问答系统中LLM的法律引用准确性和回答质量，同时降低了计算成本。", "motivation": "泰语法律问答系统中RAG的性能有限，尤其是在需要复杂法律推理的问题上。", "method": "采用Group-Relative Policy Optimization (GRPO)方法，结合BGE-M3嵌入作为语义相似性奖励，显著降低计算成本。", "result": "在NitiBench基准测试中，GRPO实现了90%的引用-F1提升和31%的联合质量指标提升，且在复杂法律推理任务中表现更稳健。", "conclusion": "该方法为提升泰语法律LLM提供了一种高效且资源节约的解决方案。"}}
{"id": "2507.09256", "categories": ["cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09256", "abs": "https://arxiv.org/abs/2507.09256", "authors": ["Junyu Chen", "Yihua Gao", "Mingyuan Ge", "Mingyong Li"], "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching", "comment": "Accepted by the Knowledge-Based Systems(KBS), 2025", "summary": "Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .", "AI": {"tldr": "本文提出了一种名为AAHR的框架，通过动态聚类原型对比学习解决图像-文本匹配中的语义模糊和高阶关联问题，显著提升了匹配性能。", "motivation": "现有方法在处理高阶关联和语义模糊时存在不足，尤其是对软正样本和软负样本的区分能力有限，且未能充分利用训练批次中样本的邻域关系。", "method": "AAHR通过动态聚类原型对比学习构建统一表示空间，结合全局和局部特征提取机制、自适应聚合网络、GNN增强语义交互，以及动量对比学习扩展负样本集。", "result": "在Flickr30K、MSCOCO和ECCV Caption数据集上，AAHR优于现有方法，显著提高了图像-文本匹配的准确性和效率。", "conclusion": "AAHR通过多策略结合有效解决了语义模糊和高阶关联问题，为图像-文本匹配提供了更优的解决方案。"}}
{"id": "2507.10134", "categories": ["cs.AI", "53-01", "C.2"], "pdf": "https://arxiv.org/pdf/2507.10134", "abs": "https://arxiv.org/abs/2507.10134", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida"], "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "comment": "8 pages, 8 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.", "AI": {"tldr": "论文提出了一种基于LLM的在线飞行资源分配方案（FRSICL），用于无人机辅助的野火监测系统，实时优化飞行控制和数据收集调度，以最小化信息年龄（AoI）。", "motivation": "无人机在野火监测中至关重要，但现有的深度强化学习方法（如DRL）存在采样效率低、仿真与现实的差距以及训练复杂等问题，不适合时间敏感的应用。", "method": "FRSICL利用自然语言任务描述和环境反馈，动态生成数据收集计划和飞行速度控制，无需大量重新训练。", "result": "仿真结果显示，FRSICL在最小化平均AoI方面优于PPO和最近邻基线方法。", "conclusion": "FRSICL是一种高效且适应性强的解决方案，适用于无人机辅助的野火监测系统。"}}
{"id": "2507.10284", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10284", "abs": "https://arxiv.org/abs/2507.10284", "authors": ["Venkat Margapuri"], "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning", "comment": null, "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.", "AI": {"tldr": "论文提出Prompt-Informed Reinforcement Learning (PIRL)，结合大型语言模型（LLM）的零样本推理能力和好奇心驱动的强化学习（RL），以动态调整奖励函数，优化无人机视觉覆盖路径规划。", "motivation": "传统RL方法依赖环境特定的奖励设计，缺乏语义适应性。PIRL旨在通过LLM的语义反馈动态调整奖励函数，提升覆盖率和效率。", "method": "PIRL结合GPT-3.5的语义反馈与PPO算法，动态调整无人机位置和摄像头控制。训练在OpenAI Gym中进行，并在Webots模拟器中测试。", "result": "PIRL在视觉覆盖率（OpenAI Gym提升14%，Webots提升27%）、电池效率（提升25%）和冗余降低（18%）方面均优于基线方法。", "conclusion": "LLM引导的奖励设计在复杂空间探索任务中有效，为自然语言与RL结合提供了新方向。"}}
{"id": "2507.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09701", "abs": "https://arxiv.org/abs/2507.09701", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.", "AI": {"tldr": "MCEval是一个多语言文化评估框架，用于评估大型语言模型的文化偏见和跨文化理解能力，覆盖13种文化和语言，揭示性能差异和公平性问题。", "motivation": "大型语言模型存在文化偏见和跨文化理解能力不足的问题，尤其是在服务全球多样化用户时。", "method": "采用动态文化问题构建和反事实重述、混杂重述进行因果分析，评估文化意识和文化偏见。", "result": "实验结果显示性能差异与训练数据分布和语言-文化对齐相关，并揭示了公平性问题。", "conclusion": "MCEval是首个全面的多语言文化评估框架，为LLMs的文化理解提供了深入见解。"}}
{"id": "2507.09266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09266", "abs": "https://arxiv.org/abs/2507.09266", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation", "comment": "Accepted in International Conference on Computer Vision (ICCV)\n  Workshops", "summary": "Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving\nstrong performances without relying on gloss annotations. However, these gains\nhave often come with increased model complexity and high computational demands,\nraising concerns about scalability, especially as large-scale sign language\ndatasets become more common. We propose a segment-aware visual tokenization\nframework that leverages sign segmentation to convert continuous video into\ndiscrete, sign-informed visual tokens. This reduces input sequence length by up\nto 50% compared to prior methods, resulting in up to 2.67x lower memory usage\nand better scalability on larger datasets. To bridge the visual and linguistic\nmodalities, we introduce a token-to-token contrastive alignment objective,\nalong with a dual-level supervision that aligns both language embeddings and\nintermediate hidden states. This improves fine-grained cross-modal alignment\nwithout relying on gloss-level supervision. Our approach notably exceeds the\nperformance of state-of-the-art methods on the PHOENIX14T benchmark, while\nsignificantly reducing sequence length. Further experiments also demonstrate\nour improved performance over prior work under comparable sequence-lengths,\nvalidating the potential of our tokenization and alignment strategies.", "AI": {"tldr": "提出了一种无注释的手语翻译方法，通过分段感知的视觉标记化框架减少输入序列长度和内存使用，同时引入对比对齐目标和双重监督提升跨模态对齐性能。", "motivation": "现有无注释手语翻译方法虽性能提升，但模型复杂度和计算需求高，难以扩展到大尺度数据集。", "method": "采用分段感知视觉标记化框架将连续视频转换为离散标记，减少序列长度；引入标记间对比对齐目标和双重监督（语言嵌入和中间隐藏状态对齐）。", "result": "在PHOENIX14T基准测试中性能超越现有方法，序列长度减少50%，内存使用降低2.67倍，且在相同序列长度下表现更优。", "conclusion": "提出的标记化和对齐策略显著提升了无注释手语翻译的性能和可扩展性。"}}
{"id": "2507.10142", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10142", "abs": "https://arxiv.org/abs/2507.10142", "authors": ["Siyi Hu", "Mohamad A Hady", "Jianglin Qiao", "Jimmy Cao", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.", "AI": {"tldr": "论文提出多智能体强化学习（MARL）在动态现实环境中的适应性评估框架，以解决其实际部署的挑战。", "motivation": "现实多智能体系统（MAS）的复杂性和动态性限制了MARL的实际应用，需评估其适应变化的能力。", "method": "提出适应性概念，并构建包含学习适应性、策略适应性和场景驱动适应性的三维框架。", "result": "通过适应性视角，支持更系统的MARL性能评估，超越传统基准测试。", "conclusion": "该框架有助于开发更适合动态现实MAS的MARL算法。"}}
{"id": "2507.10290", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10290", "abs": "https://arxiv.org/abs/2507.10290", "authors": ["Jiajun Yu", "Nanhe Chen", "Guodong Liu", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity", "comment": "8 pages, submitted to RA-L", "summary": "Optimization has been widely used to generate smooth trajectories for motion\nplanning. However, existing trajectory optimization methods show weakness when\ndealing with large-scale long trajectories. Recent advances in parallel\ncomputing have accelerated optimization in some fields, but how to efficiently\nsolve trajectory optimization via parallelism remains an open question. In this\npaper, we propose a novel trajectory optimization framework based on the\nConsensus Alternating Direction Method of Multipliers (CADMM) algorithm, which\ndecomposes the trajectory into multiple segments and solves the subproblems in\nparallel. The proposed framework reduces the time complexity to O(1) per\niteration to the number of segments, compared to O(N) of the state-of-the-art\n(SOTA) approaches. Furthermore, we introduce a closed-form solution that\nintegrates convex linear and quadratic constraints to speed up the\noptimization, and we also present numerical solutions for general inequality\nconstraints. A series of simulations and experiments demonstrate that our\napproach outperforms the SOTA approach in terms of efficiency and smoothness.\nEspecially for a large-scale trajectory, with one hundred segments, achieving\nover a tenfold speedup. To fully explore the potential of our algorithm on\nmodern parallel computing architectures, we deploy our framework on a GPU and\nshow high performance with thousands of segments.", "AI": {"tldr": "提出了一种基于CADMM算法的轨迹优化框架，通过并行计算解决大规模长轨迹问题，显著提升效率。", "motivation": "现有轨迹优化方法在处理大规模长轨迹时效率不足，并行计算的应用尚未充分探索。", "method": "采用CADMM算法分解轨迹为多段并行求解，引入闭式解和数值解处理约束。", "result": "相比SOTA方法，每迭代时间复杂度降至O(1)，实验显示效率和平滑性显著提升。", "conclusion": "该框架在GPU上部署后，能高效处理数千段轨迹，展示了并行计算的潜力。"}}
{"id": "2507.09709", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09709", "abs": "https://arxiv.org/abs/2507.09709", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.", "AI": {"tldr": "研究发现大型语言模型（LLM）的潜在空间中，高级语义信息集中在低维子空间，且在不同领域中线性可分。这种可分性在深层和结构化推理提示下更明显，支持几何感知工具的开发。", "motivation": "探索LLM内部语义表示的组织方式，以提升模型解释性和对齐性。", "method": "对11个基于Transformer的LLM进行大规模实证研究，分析6个科学主题和12层的隐藏状态。", "result": "语义信息在低维子空间中线性可分，深层和结构化推理提示下更明显，支持简单因果干预。", "conclusion": "几何感知工具可直接操作潜在表示，用于检测和防御有害内容，如轻量级MLP分类器作为潜在空间护栏。"}}
{"id": "2507.09269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09269", "abs": "https://arxiv.org/abs/2507.09269", "authors": ["Shuhan Ye", "Yuanbin Qian", "Chong Wang", "Sunqi Lin", "Jiazhen Xu", "Jiangbo Qian", "Yuqi Li"], "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks", "comment": "This paper has been accepted by ICME2025", "summary": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in\ncomputer vision domain due to their high biological plausibility, event-driven\ncharacteristic and energy-saving efficiency. Still, limited annotated\nevent-based datasets and immature SNN architectures result in their performance\ninferior to that of Artificial Neural Networks (ANNs). To enhance the\nperformance of SNNs on their optimal data format, DVS data, we explore using\nRGB data and well-performing ANNs to implement knowledge distillation. In this\ncase, solving cross-modality and cross-architecture challenges is necessary. In\nthis paper, we propose cross knowledge distillation (CKD), which not only\nleverages semantic similarity and sliding replacement to mitigate the\ncross-modality challenge, but also uses an indirect phased knowledge\ndistillation to mitigate the cross-architecture challenge. We validated our\nmethod on main-stream neuromorphic datasets, including N-Caltech101 and\nCEP-DVS. The experimental results show that our method outperforms current\nState-of-the-Art methods. The code will be available at\nhttps://github.com/ShawnYE618/CKD", "AI": {"tldr": "论文提出了一种跨知识蒸馏（CKD）方法，通过语义相似性和滑动替换解决跨模态问题，间接分阶段知识蒸馏解决跨架构问题，提升了SNN在DVS数据上的性能。", "motivation": "SNN在计算机视觉领域潜力巨大，但受限于标注数据和架构不成熟，性能不如ANN。论文旨在通过知识蒸馏提升SNN在DVS数据上的表现。", "method": "提出CKD方法，利用语义相似性和滑动替换解决跨模态问题，间接分阶段知识蒸馏解决跨架构问题。", "result": "在主流神经形态数据集（如N-Caltech101和CEP-DVS）上验证，性能优于当前最优方法。", "conclusion": "CKD方法有效解决了跨模态和跨架构挑战，显著提升了SNN的性能。"}}
{"id": "2507.10156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10156", "abs": "https://arxiv.org/abs/2507.10156", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Stavroula Mougiakakou"], "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation", "comment": "10 pages, 2 Figures, 7 tables", "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating.", "AI": {"tldr": "论文介绍了瑞士食品知识图谱（SwissFKG），整合食谱、成分、替代品、营养数据及饮食限制，并利用LLM增强图谱内容，为下一代饮食评估工具奠定基础。", "motivation": "现有自动饮食评估系统常忽略非视觉因素（如成分替代对营养的影响）和个性化需求（如过敏、文化习惯）。瑞士缺乏整合相关营养信息的集中资源。", "method": "构建SwissFKG，结合LLM增强图谱内容，并评估四种LLM的表现。开发Graph-RAG应用展示图谱在回答用户营养查询中的作用。", "result": "LLM能有效丰富图谱营养信息。SwissFKG提供成分级信息（如过敏原）和营养指南，支持个性化查询。", "conclusion": "SwissFKG为融合视觉、上下文和文化维度的下一代饮食评估工具提供了基础。"}}
{"id": "2507.10376", "categories": ["cs.RO", "I.2"], "pdf": "https://arxiv.org/pdf/2507.10376", "abs": "https://arxiv.org/abs/2507.10376", "authors": ["Mohammadhossein Talebi", "Pragyan Dahal", "Davide Possenti", "Stefano Arrigoni", "Francesco Braghin"], "title": "Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions", "comment": "8 pages", "summary": "Autonomous driving systems are highly dependent on sensors like cameras,\nLiDAR, and inertial measurement units (IMU) to perceive the environment and\nestimate their motion. Among these sensors, perception-based sensors are not\nprotected from harsh weather and technical failures. Although existing methods\nshow robustness against common technical issues like rotational misalignment\nand disconnection, they often degrade when faced with dynamic environmental\nfactors like weather conditions. To address these problems, this research\nintroduces a novel deep learning-based motion estimator that integrates visual,\ninertial, and millimeter-wave radar data, utilizing each sensor strengths to\nimprove odometry estimation accuracy and reliability under adverse\nenvironmental conditions such as snow, rain, and varying light. The proposed\nmodel uses advanced sensor fusion techniques that dynamically adjust the\ncontributions of each sensor based on the current environmental condition, with\nradar compensating for visual sensor limitations in poor visibility. This work\nexplores recent advancements in radar-based odometry and highlights that radar\nrobustness in different weather conditions makes it a valuable component for\npose estimation systems, specifically when visual sensors are degraded.\nExperimental results, conducted on the Boreas dataset, showcase the robustness\nand effectiveness of the model in both clear and degraded environments.", "AI": {"tldr": "提出了一种基于深度学习的运动估计器，融合视觉、惯性和毫米波雷达数据，以提高恶劣环境下的里程估计准确性和可靠性。", "motivation": "现有方法在动态环境因素（如天气条件）下性能下降，需要一种更鲁棒的解决方案。", "method": "利用先进的传感器融合技术，动态调整各传感器的贡献，雷达补偿视觉传感器在低能见度下的不足。", "result": "在Boreas数据集上的实验表明，模型在清晰和恶劣环境下均表现出鲁棒性和有效性。", "conclusion": "雷达在不同天气条件下的鲁棒性使其成为姿态估计系统的有价值组件，特别是在视觉传感器性能下降时。"}}
{"id": "2507.09758", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.09758", "abs": "https://arxiv.org/abs/2507.09758", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Schütze"], "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.", "AI": {"tldr": "提出了一种自适应的课程学习范式，利用预训练语言模型预测难度分数，优化微调顺序，实验表明其优于随机采样。", "motivation": "现有课程学习方法依赖人工定义的难度指标（如文本长度），可能无法准确反映模型视角，因此提出自适应的难度评分方法。", "method": "基于预训练语言模型预测的难度分数，探索了从易到难、从难到易及混合采样的不同训练策略。", "result": "在四个自然语言理解数据集上验证，该方法比随机采样收敛更快且性能更优。", "conclusion": "自适应的课程学习范式能有效提升模型训练效率和性能。"}}
{"id": "2507.09279", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09279", "abs": "https://arxiv.org/abs/2507.09279", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.", "AI": {"tldr": "Prompt4Trust是一个针对多模态大语言模型（MLLMs）的强化学习框架，旨在通过上下文感知的辅助提示改善模型置信度校准，提升医疗领域的安全性和可信度。", "motivation": "MLLMs在医疗等安全关键领域的应用受限，因其对提示设计敏感且可能高置信度生成错误答案，影响临床决策的可靠性。", "method": "提出Prompt4Trust框架，通过轻量级LLM生成辅助提示，优化下游任务MLLM的置信度校准和任务准确性。", "result": "在PMC-VQA基准测试中达到最先进的医学视觉问答性能，并展示了对更大MLLM的零样本泛化能力。", "conclusion": "Prompt4Trust通过自动化且符合人类需求的提示工程，显著提升了MLLMs在安全关键场景中的可信度。"}}
{"id": "2507.10174", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10174", "abs": "https://arxiv.org/abs/2507.10174", "authors": ["Yumi Omori", "Zixuan Dong", "Keith Ross"], "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?", "comment": "Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)", "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?", "AI": {"tldr": "本文通过实验比较了决策变换器（DT）和基于MLP的过滤行为克隆（FBC）在稀疏奖励环境中的表现，发现FBC性能更优，且更简单高效。", "motivation": "探讨决策变换器（DT）在稀疏奖励环境中的适用性，并验证更简单的FBC方法是否更具优势。", "method": "在Robomimic和D4RL基准上实验比较DT和FBC的性能，FBC通过过滤低质量轨迹后进行行为克隆。", "result": "FBC在稀疏奖励环境中表现优于DT，且更高效。", "conclusion": "DT在稀疏奖励环境中并非优选，其适用性受到质疑。"}}
{"id": "2507.10500", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10500", "abs": "https://arxiv.org/abs/2507.10500", "authors": ["Kyungtae Han", "Yitao Chen", "Rohit Gupta", "Onur Altintas"], "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance", "comment": null, "summary": "While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance.", "AI": {"tldr": "SC-ADAS是一个基于生成式AI的模块化框架，通过结合视觉、语言模型和结构化功能调用，实现实时、可解释的自适应驾驶辅助系统。", "motivation": "当前ADAS系统缺乏场景理解和自然语言交互能力，难以适应动态环境或驾驶员意图。", "method": "SC-ADAS整合了大型语言模型、视觉到文本解释和结构化功能调用，支持基于视觉和传感器上下文的多轮对话。", "result": "在CARLA模拟器中实现，系统能够执行用户确认的ADAS命令，无需模型微调，但存在延迟和对话历史增长的权衡。", "conclusion": "SC-ADAS展示了结合对话推理、场景感知和模块化ADAS控制的可行性，为下一代智能驾驶辅助系统提供了方向。"}}
{"id": "2507.09777", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.09777", "abs": "https://arxiv.org/abs/2507.09777", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "title": "Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "AI": {"tldr": "论文重新定义了点击诱饵，提出好奇心差距是其核心特征，并创建了首个西班牙语点击诱饵检测开源数据集TA1C。", "motivation": "当前对点击诱饵的定义缺乏共识，研究旨在明确其与类似现象的区别。", "method": "提出新定义，优化数据集标注标准，创建并发布TA1C数据集，实现高标注一致性。", "result": "TA1C数据集包含3,500条推文，标注一致性达0.825，基线模型F1分数为0.84。", "conclusion": "研究为点击诱饵检测提供了更清晰的定义和高质量数据集，推动了相关领域的发展。"}}
{"id": "2507.09285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09285", "abs": "https://arxiv.org/abs/2507.09285", "authors": ["Chenhao Ding", "Jiangtao Zhang", "Zongsheng Yue", "Hui Wang", "Qian Zhao", "Deyu Meng"], "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring", "comment": null, "summary": "Deep prior-based approaches have demonstrated remarkable success in blind\nmotion deblurring (BMD) recently. These methods, however, are often limited by\nthe high non-convexity of the underlying optimization process in BMD, which\nleads to extreme sensitivity to the initial blur kernel. To address this issue,\nwe propose a novel framework for BMD that leverages a deep generative model to\nencode the kernel prior and induce a better initialization for the blur kernel.\nSpecifically, we pre-train a kernel generator based on a generative adversarial\nnetwork (GAN) to aptly characterize the kernel's prior distribution, as well as\na kernel initializer to provide a well-informed and high-quality starting point\nfor kernel estimation. By combining these two components, we constrain the BMD\nsolution within a compact latent kernel manifold, thus alleviating the\naforementioned sensitivity for kernel initialization. Notably, the kernel\ngenerator and initializer are designed to be easily integrated with existing\nBMD methods in a plug-and-play manner, enhancing their overall performance.\nFurthermore, we extend our approach to tackle blind non-uniform motion\ndeblurring without the need for additional priors, achieving state-of-the-art\nperformance on challenging benchmark datasets. The source code is available at\nhttps://github.com/dch0319/GLKM-Deblur.", "AI": {"tldr": "提出了一种基于深度生成模型的盲运动去模糊（BMD）新框架，通过预训练的GAN生成器和初始化器，优化模糊核的初始估计，提升去模糊性能。", "motivation": "现有深度先验方法在盲运动去模糊中因优化过程的高非凸性对初始模糊核极度敏感，限制了性能。", "method": "预训练基于GAN的模糊核生成器和初始化器，约束解空间于紧凑的潜在核流形，并与现有方法无缝集成。", "result": "在挑战性基准数据集上实现了最先进的性能，并能扩展到盲非均匀运动去模糊。", "conclusion": "提出的框架有效缓解了初始模糊核敏感性问题，提升了盲运动去模糊的整体性能。"}}
{"id": "2507.10208", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10208", "abs": "https://arxiv.org/abs/2507.10208", "authors": ["Hamzah Ziadeh", "Hendrik Knoche"], "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks", "comment": null, "summary": "Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design.", "AI": {"tldr": "论文提出了一种基于‘什么、为什么、谁’三个维度对可解释人工智能（XAI）研究进行分类和比较的方法，以解决当前研究中任务描述不足、脱离上下文和用户测试不足的问题。", "motivation": "当前XAI研究存在大量矛盾，缺乏具体设计建议，主要源于对需要AI辅助的任务理解不足。", "method": "结合可视化分析、认知科学和仪表板设计等多领域，提出分类和比较XAI研究的三维框架。", "result": "研究发现主要问题包括任务描述不足、脱离上下文的研究和用户测试不足。建议研究应明确报告用户的领域、AI和数据分析专业知识。", "conclusion": "论文提出的框架和指南有助于研究者更好地识别相关研究、发现研究空白，并处理XAI设计中的矛盾结果。"}}
{"id": "2507.10543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10543", "abs": "https://arxiv.org/abs/2507.10543", "authors": ["Juyi Sheng", "Ziyi Wang", "Peiming Li", "Mengyuan Liu"], "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation", "comment": null, "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/.", "AI": {"tldr": "MP1是一种结合3D点云输入和MeanFlow范式的机器人学习方法，通过单次网络评估生成动作轨迹，避免了扩散模型的慢速采样和Flow-based方法的架构限制。", "motivation": "解决机器人学习中生成模型在慢速采样和架构限制之间的权衡问题，提高轨迹生成效率和精度。", "method": "采用MeanFlow Identity直接学习区间平均速度，避免一致性约束，并引入Dispersive Loss提升泛化能力。", "result": "在Adroit和Meta-World基准测试中，MP1的平均任务成功率优于DP3和FlowPolicy，推理速度快19倍。", "conclusion": "MP1通过高效的单次评估和轻量级损失函数，显著提升了机器人学习的性能和效率。"}}
{"id": "2507.09875", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "AI": {"tldr": "论文研究了大型语言模型通过上下文学习执行未见任务的能力，聚焦于‘错位加法’任务，揭示了模型内部的可重用和可组合结构如何支持任务级泛化。", "motivation": "探索大型语言模型在未见任务中表现优异的内部机制，特别是通过‘错位加法’任务分析其泛化能力。", "method": "使用路径修补等电路式可解释性技术，分析模型在‘错位加法’任务中的内部计算过程。", "result": "发现了一种函数归纳机制，揭示了模型从标准加法泛化到错位加法的能力，并展示了该机制在更广泛任务中的可重用性。", "conclusion": "研究揭示了语言模型内部的可重用和可组合结构如何支持任务级泛化，为理解模型能力提供了新视角。"}}
{"id": "2507.09291", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09291", "abs": "https://arxiv.org/abs/2507.09291", "authors": ["Yuval Grader", "Hadar Averbuch-Elor"], "title": "Supercharging Floorplan Localization with Semantic Rays", "comment": "Accepted at ICCV 2025", "summary": "Floorplans provide a compact representation of the building's structure,\nrevealing not only layout information but also detailed semantics such as the\nlocations of windows and doors. However, contemporary floorplan localization\ntechniques mostly focus on matching depth-based structural cues, ignoring the\nrich semantics communicated within floorplans. In this work, we introduce a\nsemantic-aware localization framework that jointly estimates depth and semantic\nrays, consolidating over both for predicting a structural-semantic probability\nvolume. Our probability volume is constructed in a coarse-to-fine manner: We\nfirst sample a small set of rays to obtain an initial low-resolution\nprobability volume. We then refine these probabilities by performing a denser\nsampling only in high-probability regions and process the refined values for\npredicting a 2D location and orientation angle. We conduct an evaluation on two\nstandard floorplan localization benchmarks. Our experiments demonstrate that\nour approach substantially outperforms state-of-the-art methods, achieving\nsignificant improvements in recall metrics compared to prior works. Moreover,\nwe show that our framework can easily incorporate additional metadata such as\nroom labels, enabling additional gains in both accuracy and efficiency.", "AI": {"tldr": "提出了一种语义感知的定位框架，联合估计深度和语义光线，显著优于现有方法。", "motivation": "现有楼层平面图定位技术主要依赖深度结构线索，忽略了丰富的语义信息。", "method": "通过粗到细的方式构建结构-语义概率体积，先采样少量光线生成低分辨率概率体积，再在高概率区域密集采样进行细化。", "result": "在两个标准基准测试中显著优于现有方法，召回率显著提升，并可轻松整合额外元数据（如房间标签）以进一步提高精度和效率。", "conclusion": "该框架通过结合语义信息，显著提升了楼层平面图定位的性能和灵活性。"}}
{"id": "2507.10281", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10281", "abs": "https://arxiv.org/abs/2507.10281", "authors": ["Jiaming Tian", "Liyao Li", "Wentao Ye", "Haobo Wang", "Lingxin Wang", "Lihua Yu", "Zujie Ren", "Gang Chen", "Junbo Zhao"], "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence", "comment": null, "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.", "AI": {"tldr": "该论文综述了基于LLM的表格代理在现实场景中的应用，定义了五个核心能力，并分析了当前方法的局限性，提出了改进建议。", "motivation": "现实中的表格任务常涉及噪声和复杂性，而现有研究多针对干净数据集，因此需要探索更鲁棒的自动化解决方案。", "method": "通过定义五个核心能力（C1-C5）分析比较现有方法，并以Text-to-SQL代理为例揭示性能差距。", "result": "发现开源模型在真实场景中表现不佳，提出了提升鲁棒性、泛化性和效率的建议。", "conclusion": "论文为LLM表格代理的实际应用提供了改进方向，强调需解决现实场景中的挑战。"}}
{"id": "2507.09294", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09294", "abs": "https://arxiv.org/abs/2507.09294", "authors": ["Rui Tang", "Haochen Yin", "Guankun Wang", "Long Bai", "An Wang", "Huxin Gao", "Jiazheng Wang", "Hongliang Ren"], "title": "Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection", "comment": "IEEE ICIA 2025", "summary": "Surgical phase recognition plays a critical role in developing intelligent\nassistance systems for minimally invasive procedures such as Endoscopic\nSubmucosal Dissection (ESD). However, the high visual similarity across\ndifferent phases and the lack of structural cues in RGB images pose significant\nchallenges. Depth information offers valuable geometric cues that can\ncomplement appearance features by providing insights into spatial relationships\nand anatomical structures. In this paper, we pioneer the use of depth\ninformation for surgical phase recognition and propose Geo-RepNet, a\ngeometry-aware convolutional framework that integrates RGB image and depth\ninformation to enhance recognition performance in complex surgical scenes.\nBuilt upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the\nDepth-Guided Geometric Prior Generation (DGPG) module that extracts geometry\npriors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention\n(GEMA) to inject spatial guidance through geometry-aware cross-attention and\nefficient multi-scale aggregation. To evaluate the effectiveness of our\napproach, we construct a nine-phase ESD dataset with dense frame-level\nannotations from real-world ESD videos. Extensive experiments on the proposed\ndataset demonstrate that Geo-RepNet achieves state-of-the-art performance while\nmaintaining robustness and high computational efficiency under complex and\nlow-texture surgical environments.", "AI": {"tldr": "Geo-RepNet利用深度信息增强手术阶段识别性能，通过几何感知框架整合RGB和深度数据，在复杂手术场景中表现优异。", "motivation": "手术阶段识别对智能辅助系统至关重要，但RGB图像缺乏结构线索且视觉相似性高，深度信息可提供几何补充。", "method": "提出Geo-RepNet框架，结合RGB和深度信息，采用DGPG模块提取几何先验，GEMA模块通过几何感知注意力增强空间引导。", "result": "在九阶段ESD数据集上，Geo-RepNet实现最优性能，保持高计算效率和鲁棒性。", "conclusion": "深度信息显著提升手术阶段识别性能，Geo-RepNet为复杂手术场景提供有效解决方案。"}}
{"id": "2507.09935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09935", "abs": "https://arxiv.org/abs/2507.09935", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.", "AI": {"tldr": "提出了一种改进RAG系统的新框架，通过分层文本分割和聚类生成更有语义的块，提升了检索的精确性和上下文相关性。", "motivation": "传统分块方法未能充分捕捉语义，因未考虑文本结构，导致检索效果不佳。", "method": "整合分层文本分割和聚类，生成语义连贯的块，并在推理时结合段级和簇级向量表示进行检索。", "result": "在NarrativeQA、QuALITY和QASPER数据集上表现优于传统分块方法。", "conclusion": "新框架显著提升了RAG系统的检索效果，生成更有意义的块。"}}
{"id": "2507.09299", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09299", "abs": "https://arxiv.org/abs/2507.09299", "authors": ["Abdulvahap Mutlu", "Şengül Doğan", "Türker Tuncer"], "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation", "comment": "All codes are available at\n  https://github.com/abdulvahapmutlu/vit-protonet", "summary": "The remarkable representational power of Vision Transformers (ViTs) remains\nunderutilized in few-shot image classification. In this work, we introduce\nViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical\nNetwork framework. By averaging class conditional token embeddings from a\nhandful of support examples, ViT-ProtoNet constructs robust prototypes that\ngeneralize to novel categories under 5-shot settings. We conduct an extensive\nempirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,\nCUB-200, and CIFAR-FS, including overlapped support variants to assess\nrobustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based\nprototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot\naccuracy and demonstrating superior feature separability in latent space.\nFurthermore, it outperforms or is competitive with transformer-based\ncompetitors using a more lightweight backbone. Comprehensive ablations examine\nthe impact of transformer depth, patch size, and fine-tuning strategy. To\nfoster reproducibility, we release code and pretrained weights. Our results\nestablish ViT-ProtoNet as a powerful, flexible approach for few-shot\nclassification and set a new baseline for transformer-based meta-learners.", "AI": {"tldr": "ViT-ProtoNet结合ViT-Small和原型网络，在少样本图像分类中表现优异，优于CNN原型网络和部分Transformer方法。", "motivation": "Vision Transformers（ViTs）在少样本图像分类中的潜力未充分挖掘，ViT-ProtoNet旨在利用ViT-Small提升原型网络的性能。", "method": "通过平均少量支持样本的类别条件令牌嵌入构建鲁棒原型，并在5-shot设置下评估。", "result": "在多个基准测试中，ViT-ProtoNet表现优于CNN原型网络，5-shot准确率提升3.2%，且特征可分性更强。", "conclusion": "ViT-ProtoNet为少样本分类提供了强大且灵活的方法，并为基于Transformer的元学习设定了新基准。"}}
{"id": "2507.10397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10397", "abs": "https://arxiv.org/abs/2507.10397", "authors": ["Alessandra M. M. M. Gouvêa", "Nuno Paulos", "Eduardo Uchoa e Mariá C. V. Nascimento"], "title": "Instance space analysis of the capacitated vehicle routing problem", "comment": null, "summary": "This paper seeks to advance CVRP research by addressing the challenge of\nunderstanding the nuanced relationships between instance characteristics and\nmetaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a\nvaluable tool that allows for a new perspective on the field. By combining the\nISA methodology with a dataset from the DIMACS 12th Implementation Challenge on\nVehicle Routing, our research enabled the identification of 23 relevant\ninstance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,\nwhich employ dimensionality reduction and machine learning methods, allowed us\nto create a two-dimensional projection of the instance space to understand how\nthe structure of instances affect the behavior of MHs. A key contribution of\nour work is that we provide a projection matrix, which makes it straightforward\nto incorporate new instances into this analysis and allows for a new method for\ninstance analysis in the CVRP field.", "AI": {"tldr": "本文通过实例空间分析（ISA）方法，结合DIMACS数据集，研究了CVRP中实例特征与元启发式算法性能的关系，并提供了投影矩阵以支持新实例分析。", "motivation": "解决CVRP研究中实例特征与元启发式算法性能之间复杂关系的问题。", "method": "采用实例空间分析（ISA）方法，结合PRELIM、SIFTED和PILOT阶段，利用降维和机器学习技术，构建二维实例空间投影。", "result": "识别了23个相关实例特征，并提供了投影矩阵，便于新实例的分析。", "conclusion": "ISA为CVRP领域提供了一种新的实例分析方法，有助于理解实例结构对算法行为的影响。"}}
{"id": "2507.09420", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09420", "abs": "https://arxiv.org/abs/2507.09420", "authors": ["Timothy Chase Jr", "Karthik Dantu"], "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data", "comment": "Presented at the RSS Space Robotics Workshop 2025. Poster available\n  online at https://tjchase34.github.io/assets/pdfs/rss_poster.pdf", "summary": "The detection and tracking of celestial surface terrain features are crucial\nfor autonomous spaceflight applications, including Terrain Relative Navigation\n(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data\ncollection. Traditional photoclinometry-based pipelines often rely on extensive\na priori imaging and offline processing, constrained by the computational\nlimitations of radiation-hardened systems. While historically effective, these\napproaches typically increase mission costs and duration, operate at low\nprocessing rates, and have limited generalization. Recently, learning-based\ncomputer vision has gained popularity to enhance spacecraft autonomy and\novercome these limitations. While promising, emerging techniques frequently\nimpose computational demands exceeding the capabilities of typical spacecraft\nhardware for real-time operation and are further challenged by the scarcity of\nlabeled training data for diverse extraterrestrial environments. In this work,\nwe present novel formulations for in-situ landmark tracking via detection and\ndescription. We utilize lightweight, computationally efficient neural network\narchitectures designed for real-time execution on current-generation spacecraft\nflight processors. For landmark detection, we propose improved domain\nadaptation methods that enable the identification of celestial terrain features\nwith distinct, cheaply acquired training data. Concurrently, for landmark\ndescription, we introduce a novel attention alignment formulation that learns\nrobust feature representations that maintain correspondence despite significant\nlandmark viewpoint variations. Together, these contributions form a unified\nsystem for landmark tracking that demonstrates superior performance compared to\nexisting state-of-the-art techniques.", "AI": {"tldr": "论文提出了一种轻量级神经网络方法，用于实时检测和描述天体表面地形特征，解决了传统方法计算量大和数据稀缺的问题。", "motivation": "传统的光度测量方法计算成本高、处理速度慢，且泛化能力有限，而现有学习技术又难以在航天器硬件上实时运行。", "method": "采用轻量级神经网络架构，结合改进的域适应方法和注意力对齐机制，实现实时地形特征检测与描述。", "result": "提出的系统在性能上优于现有技术，适用于实时天体地形跟踪。", "conclusion": "该方法为航天器自主导航提供了高效、实时的解决方案。"}}
{"id": "2507.09973", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09973", "abs": "https://arxiv.org/abs/2507.09973", "authors": ["Sarah Pan"], "title": "Tiny Reward Models", "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.", "AI": {"tldr": "TinyRM是一种小型双向掩码语言模型，参数仅4亿，性能媲美更大模型，适用于奖励建模任务。", "motivation": "随着奖励模型在测试时策略中的部署增加，其推理成本成为问题，需要更高效的解决方案。", "method": "结合FLAN风格提示、定向低秩适应（DoRA）和层冻结技术，优化小型模型性能。", "result": "TinyRM在RewardBench上表现优异，资源消耗显著减少。", "conclusion": "小型模型在特定领域调优中表现突出，双向架构有望成为高效、可扩展的偏好建模方案。"}}
{"id": "2507.09308", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09308", "abs": "https://arxiv.org/abs/2507.09308", "authors": ["Zile Wang", "Hao Yu", "Jiabo Zhan", "Chun Yuan"], "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning", "comment": null, "summary": "Recent advances in latent diffusion models have achieved remarkable results\nin high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress\nand reconstruct pixel data at low computational cost. However, the generation\nof transparent or layered content (RGBA image) remains largely unexplored, due\nto the lack of large-scale benchmarks. In this work, we propose ALPHA, the\nfirst comprehensive RGBA benchmark that adapts standard RGB metrics to\nfour-channel images via alpha blending over canonical backgrounds. We further\nintroduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB\nVAE by incorporating a dedicated alpha channel. The model is trained with a\ncomposite objective that combines alpha-blended pixel reconstruction,\npatch-level fidelity, perceptual consistency, and dual KL divergence\nconstraints to ensure latent fidelity across both RGB and alpha\nrepresentations. Our RGBA VAE, trained on only 8K images in contrast to 1M used\nby prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase\nin SSIM over LayerDiffuse in reconstruction. It also enables superior\ntransparent image generation when fine-tuned within a latent diffusion\nframework. Our code, data, and models are released on\nhttps://github.com/o0o0o00o0/AlphaVAE for reproducibility.", "AI": {"tldr": "ALPHA 是一个针对 RGBA 图像的基准测试，ALPHAVAE 是一个端到端的 RGBA VAE 模型，通过扩展预训练的 RGB VAE 并引入专用 alpha 通道，显著提升了透明图像生成的质量。", "motivation": "目前缺乏针对透明或分层内容（RGBA 图像）的大规模基准测试和生成方法，限制了相关研究的发展。", "method": "提出 ALPHA 基准测试，并开发 ALPHAVAE 模型，结合 alpha 混合、补丁级保真度、感知一致性和双重 KL 散度约束进行训练。", "result": "ALPHAVAE 在仅使用 8K 图像训练的情况下，PSNR 提升 4.9 dB，SSIM 提升 3.2%，且透明图像生成效果更优。", "conclusion": "ALPHAVAE 为 RGBA 图像生成提供了高效解决方案，并在透明图像生成任务中表现出色。"}}
{"id": "2507.10421", "categories": ["cs.AI", "cs.ET", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10421", "abs": "https://arxiv.org/abs/2507.10421", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning", "comment": "International Conference on Education and New Learning Technologies\n  (2025)", "summary": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance", "AI": {"tldr": "论文提出了一种结合BERT情感分析和XGBoost特征选择的新模型，用于预测远程学习中的学生辍学风险，准确率达84%。", "motivation": "远程学习中的学生辍学问题严重，早期预测对干预和提升学生坚持性至关重要。", "method": "结合BERT对学生的评论进行情感分析，并通过XGBoost整合社会人口和行为数据，进行特征选择和模型训练。", "result": "模型在未见数据上达到84%的准确率，优于基线模型的82%，且在精确率和F1分数上表现更优。", "conclusion": "该模型可作为个性化策略开发的重要工具，有效降低辍学率并鼓励学生坚持学习。"}}
{"id": "2507.09459", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09459", "abs": "https://arxiv.org/abs/2507.09459", "authors": ["Zhihan Kang", "Boyu Wang"], "title": "SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation", "comment": "Undergraduate Theis; 12 pages, 6 figures", "summary": "We propose SegVec3D, a novel framework for 3D point cloud instance\nsegmentation that integrates attention mechanisms, embedding learning, and\ncross-modal alignment. The approach builds a hierarchical feature extractor to\nenhance geometric structure modeling and enables unsupervised instance\nsegmentation via contrastive clustering. It further aligns 3D data with natural\nlanguage queries in a shared semantic space, supporting zero-shot retrieval.\nCompared to recent methods like Mask3D and ULIP, our method uniquely unifies\ninstance segmentation and multimodal understanding with minimal supervision and\npractical deployability.", "AI": {"tldr": "SegVec3D是一个新颖的3D点云实例分割框架，结合了注意力机制、嵌入学习和跨模态对齐，支持无监督实例分割和零样本检索。", "motivation": "解决3D点云实例分割中几何结构建模和多模态理解的统一问题，减少监督需求并提升实用性。", "method": "采用分层特征提取器增强几何建模，通过对比聚类实现无监督实例分割，并在共享语义空间中对齐3D数据与自然语言查询。", "result": "优于Mask3D和ULIP等方法，统一了实例分割和多模态理解，具有低监督需求和实际部署能力。", "conclusion": "SegVec3D为3D点云实例分割和多模态理解提供了一种高效且实用的解决方案。"}}
{"id": "2507.09982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09982", "abs": "https://arxiv.org/abs/2507.09982", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.", "AI": {"tldr": "TextOmics是一个开创性基准，通过建立组学表达与分子文本描述的一对一对应关系，促进分子生成。提出的ToDi框架结合组学和文本描述生成生物相关、化学有效的分子，优于现有方法。", "motivation": "解决靶向药物发现中缺乏异构数据和统一框架的问题。", "method": "引入TextOmics基准和ToDi框架，利用组学和文本编码器（OmicsEn和TextEn）及条件扩散（DiffGen）生成分子。", "result": "实验证明TextOmics有效，ToDi优于现有方法，并在零样本分子生成中表现优异。", "conclusion": "TextOmics和ToDi为靶向药物发现提供了新工具，展示了生成治疗潜力分子的潜力。"}}
{"id": "2507.09313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09313", "abs": "https://arxiv.org/abs/2507.09313", "authors": ["Yueqian Wang", "Xiaojun Meng", "Yifan Wang", "Huishuai Zhang", "Dongyan Zhao"], "title": "ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models", "comment": null, "summary": "With the growing research focus on multimodal dialogue systems, the\ncapability for proactive interaction is gradually gaining recognition. As an\nalternative to conventional turn-by-turn dialogue, users increasingly expect\nmultimodal systems to be more initiative, for example, by autonomously\ndetermining the timing of multi-turn responses in real time during video\nplayback. To facilitate progress in this emerging area, we introduce\nProactiveBench, the first comprehensive benchmark to evaluate a system's\nability to engage in proactive interaction. Since model responses are generated\nat varying timestamps, we further propose PAUC, the first metric that accounts\nfor the temporal dynamics of model responses. This enables a more accurate\nevaluation of systems operating in proactive settings. Through extensive\nbenchmarking of various baseline systems on ProactiveBench and a user study of\nhuman preferences, we show that PAUC is in better agreement with human\npreferences than traditional evaluation metrics, which typically only consider\nthe textual content of responses. These findings demonstrate that PAUC provides\na more faithful assessment of user experience in proactive interaction\nscenarios. Project homepage:\nhttps://github.com/yellow-binary-tree/ProactiveBench", "AI": {"tldr": "论文介绍了ProactiveBench，首个评估多模态对话系统主动交互能力的基准，并提出了PAUC指标以考虑响应时间动态性。", "motivation": "随着多模态对话系统研究的深入，用户期望系统能更主动交互，如实时决定多轮响应时机。", "method": "提出ProactiveBench基准和PAUC指标，评估系统在主动交互中的表现。", "result": "PAUC比传统指标更符合人类偏好，能更准确评估用户体验。", "conclusion": "PAUC为主动交互场景提供了更可靠的评估方法。"}}
{"id": "2507.10446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10446", "abs": "https://arxiv.org/abs/2507.10446", "authors": ["Sudarshan Babu"], "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures", "comment": "arXiv admin note: text overlap with arXiv:2310.17075", "summary": "The ability to transfer knowledge from prior experiences to novel tasks\nstands as a pivotal capability of intelligent agents, including both humans and\ncomputational models. This principle forms the basis of transfer learning,\nwhere large pre-trained neural networks are fine-tuned to adapt to downstream\ntasks. Transfer learning has demonstrated tremendous success, both in terms of\ntask adaptation speed and performance. However there are several domains where,\ndue to lack of data, training such large pre-trained models or foundational\nmodels is not a possibility - computational chemistry, computational\nimmunology, and medical imaging are examples. To address these challenges, our\nwork focuses on designing architectures to enable efficient acquisition of\npriors when large amounts of data are unavailable. In particular, we\ndemonstrate that we can use neural memory to enable adaptation on\nnon-stationary distributions with only a few samples. Then we demonstrate that\nour hypernetwork designs (a network that generates another network) can acquire\nmore generalizable priors than standard networks when trained with Model\nAgnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene\ngeneration, demonstrating that they can acquire priors efficiently on just a\nhandful of training scenes, thereby leading to faster text-to-3D generation. We\nthen extend our hypernetwork framework to perform 3D segmentation on novel\nscenes with limited data by efficiently transferring priors from earlier viewed\nscenes. Finally, we repurpose an existing molecular generative method as a\npre-training framework that facilitates improved molecular property prediction,\naddressing critical challenges in computational immunology", "AI": {"tldr": "论文提出了一种在数据稀缺领域（如计算化学、医学成像）中高效获取先验知识的架构，包括神经记忆和超网络设计，并展示了其在3D场景生成和分子属性预测中的应用。", "motivation": "解决在数据稀缺领域无法训练大型预训练模型的问题，设计高效获取先验知识的架构。", "method": "使用神经记忆适应非平稳分布，超网络结合MAML获取更通用的先验，并应用于3D场景生成和分子属性预测。", "result": "超网络在少量训练场景下高效获取先验，实现快速文本到3D生成；分子生成方法改进分子属性预测。", "conclusion": "提出的方法在数据稀缺领域高效获取先验知识，为计算免疫学等领域的应用提供了新思路。"}}
{"id": "2507.10034", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10034", "abs": "https://arxiv.org/abs/2507.10034", "authors": ["Xianghong Zou", "Jianping Li", "Zhe Chen", "Zhen Cao", "Zhen Dong", "Qiegen Liu", "Bisheng Yang"], "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning", "comment": null, "summary": "Point cloud place recognition (PCPR) plays a crucial role in photogrammetry\nand robotics applications such as autonomous driving, intelligent\ntransportation, and augmented reality. In real-world large-scale deployments of\na positioning system, PCPR models must continuously acquire, update, and\naccumulate knowledge to adapt to diverse and dynamic environments, i.e., the\nability known as continual learning (CL). However, existing PCPR models often\nsuffer from catastrophic forgetting, leading to significant performance\ndegradation in previously learned scenes when adapting to new environments or\nsensor types. This results in poor model scalability, increased maintenance\ncosts, and system deployment difficulties, undermining the practicality of\nPCPR. To address these issues, we propose LifelongPR, a novel continual\nlearning framework for PCPR, which effectively extracts and fuses knowledge\nfrom sequential point cloud data. First, to alleviate the knowledge loss, we\npropose a replay sample selection method that dynamically allocates sample\nsizes according to each dataset's information quantity and selects spatially\ndiverse samples for maximal representativeness. Second, to handle domain\nshifts, we design a prompt learning-based CL framework with a lightweight\nprompt module and a two-stage training strategy, enabling domain-specific\nfeature adaptation while minimizing forgetting. Comprehensive experiments on\nlarge-scale public and self-collected datasets are conducted to validate the\neffectiveness of the proposed method. Compared with state-of-the-art (SOTA)\nmethods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in\nmR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly\navailable at https://github.com/zouxianghong/LifelongPR.", "AI": {"tldr": "论文提出了一种名为LifelongPR的持续学习框架，用于解决点云地点识别中的灾难性遗忘问题，通过动态样本选择和提示学习提升性能。", "motivation": "点云地点识别（PCPR）在自动驾驶等领域至关重要，但现有模型在持续学习时易受灾难性遗忘影响，导致性能下降。", "method": "提出LifelongPR框架，包括动态样本选择方法和基于提示学习的轻量级模块，分两阶段训练以减少遗忘。", "result": "实验表明，该方法在mIR@1和mR@1上分别提升6.50%和7.96%，遗忘率降低8.95%。", "conclusion": "LifelongPR有效解决了PCPR中的持续学习问题，提升了模型的实用性和可扩展性。"}}
{"id": "2507.10008", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10008", "abs": "https://arxiv.org/abs/2507.10008", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.", "AI": {"tldr": "该研究提出了一种新颖的框架，通过联合学习风险因素和保护因素对自杀风险的动态影响，预测后续自杀风险。", "motivation": "现有研究多关注静态自杀风险检测，忽略了动态风险波动和保护因素的作用，限制了预测能力。", "method": "构建了一个包含风险和保护因素的新型数据集，并提出了动态因素影响学习方法。", "result": "模型在三个数据集上显著优于现有方法，并提供可解释的权重。", "conclusion": "该框架为临床干预提供了更精准和可解释的工具。"}}
{"id": "2507.09323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09323", "abs": "https://arxiv.org/abs/2507.09323", "authors": ["Kaixuan Cong", "Yifan Wang", "Rongkun Xue", "Yuyang Jiang", "Yiming Feng", "Jing Yang"], "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition", "comment": null, "summary": "Humans do not understand individual events in isolation; rather, they\ngeneralize concepts within classes and compare them to others. Existing\naudio-video pre-training paradigms only focus on the alignment of the overall\naudio-video modalities, without considering the reinforcement of distinguishing\neasily confused classes through cognitive induction and contrast during\ntraining. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder\n(DICCAE), an encoder that aligns audio-video representations at a fine-grained,\ncategory-level. DICCAE addresses category confusion by dynamically adjusting\nthe confusion loss based on inter-class confusion degrees, thereby enhancing\nthe model's ability to distinguish between similar activities. To further\nextend the application of DICCAE, we also introduce a novel training framework\nthat incorporates both audio and video modalities, as well as their fusion. To\nmitigate the scarcity of audio-video data in the human activity recognition\ntask, we propose a cluster-guided audio-video self-supervised pre-training\nstrategy for DICCAE. DICCAE achieves near state-of-the-art performance on the\nVGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its\nfeature representation quality through extensive ablation studies, validating\nthe necessity of each module.", "AI": {"tldr": "论文提出了一种动态调整类间混淆损失的音频-视频预训练编码器（DICCAE），通过细粒度类别对齐和自监督预训练策略，显著提升了模型对相似活动的区分能力。", "motivation": "现有音频-视频预训练方法仅关注整体模态对齐，忽略了通过认知归纳和对比强化易混淆类别的区分能力。", "method": "提出DICCAE编码器，动态调整类间混淆损失，并结合音频、视频及其融合模态的训练框架；采用聚类引导的自监督预训练策略缓解数据稀缺问题。", "result": "在VGGSound数据集上达到65.5%的top-1准确率，接近最先进水平，并通过消融实验验证了各模块的必要性。", "conclusion": "DICCAE通过细粒度类别对齐和动态混淆损失，显著提升了音频-视频模型的性能，为易混淆活动的识别提供了有效解决方案。"}}
{"id": "2507.10522", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10522", "abs": "https://arxiv.org/abs/2507.10522", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.", "AI": {"tldr": "DeepResearch$^{\\text{Eco}}$是一种基于LLM的自动化科学合成系统，支持递归、深度和广度可控的探索，显著提升文献检索的多样性和细致度。", "motivation": "解决传统检索增强生成管道在科学文献合成中的局限性，提供用户可控的合成和透明推理。", "method": "通过参数驱动的可配置性，实现高吞吐量的领域证据整合，同时保持分析严谨性。", "result": "在49个生态研究问题中，源整合量提升21倍，每千字整合源提升14.9倍，高参数设置下达到专家级分析深度和多样性。", "conclusion": "DeepResearch$^{\\text{Eco}}$在科学文献合成中表现出色，具有高灵活性和分析深度。"}}
{"id": "2507.10474", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10474", "abs": "https://arxiv.org/abs/2507.10474", "authors": ["Seyed Alireza Rahimi Azghadi", "Truong-Thanh-Hung Nguyen", "Helene Fournier", "Monica Wachowicz", "Rene Richard", "Francis Palma", "Hung Cao"], "title": "Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation", "comment": null, "summary": "The aging population is growing rapidly, and so is the danger of falls in\nolder adults. A major cause of injury is falling, and detection in time can\ngreatly save medical expenses and recovery time. However, to provide timely\nintervention and avoid unnecessary alarms, detection systems must be effective\nand reliable while addressing privacy concerns regarding the user. In this\nwork, we propose a framework for detecting falls using several complementary\nsystems: a semi-supervised federated learning-based fall detection system\n(SF2D), an indoor localization and navigation system, and a vision-based human\nfall recognition system. A wearable device and an edge device identify a fall\nscenario in the first system. On top of that, the second system uses an indoor\nlocalization technique first to localize the fall location and then navigate a\nrobot to inspect the scenario. A vision-based detection system running on an\nedge device with a mounted camera on a robot is used to recognize fallen\npeople. Each of the systems of this proposed framework achieves different\naccuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to\n99.19% accuracy, while the vision-based fallen people detection achieves 96.3%\naccuracy. However, when we combine the accuracy of these two systems with the\naccuracy of the navigation system (95% success rate), our proposed framework\ncreates a highly reliable performance for fall detection, with an overall\naccuracy of 99.99%. Not only is the proposed framework safe for older adults,\nbut it is also a privacy-preserving solution for detecting falls.", "AI": {"tldr": "提出了一种结合半监督联邦学习、室内定位导航和视觉识别的跌倒检测框架，具有高准确性和隐私保护特性。", "motivation": "老龄化人口增长导致跌倒风险增加，及时检测可降低医疗成本和恢复时间，同时需解决隐私问题。", "method": "采用SF2D（半监督联邦学习）、室内定位导航系统和视觉识别系统，结合可穿戴设备和边缘设备。", "result": "SF2D准确率99.19%，视觉识别准确率96.3%，导航系统成功率95%，综合准确率达99.99%。", "conclusion": "该框架高效可靠，兼顾隐私保护，适用于老年人跌倒检测。"}}
{"id": "2507.10059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10059", "abs": "https://arxiv.org/abs/2507.10059", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.", "AI": {"tldr": "GeLaCo是一种基于进化的LLM压缩方法，通过层折叠和模块相似性评估，高效探索压缩解空间，支持单目标和多目标优化，性能优于现有方法。", "motivation": "大型语言模型（LLM）计算需求高，部署和使用受限，模型压缩是解决这一问题的关键。现有方法如结构化剪枝成本高且可能忽略更优解。", "method": "GeLaCo采用进化方法，通过层折叠和基于种群的搜索，结合模块相似性适应度函数（注意力、前馈和隐藏状态表示），支持单目标和多目标压缩搜索。", "result": "GeLaCo在困惑度和生成评估中优于现有方法，首次建立了压缩与质量之间的帕累托前沿。", "conclusion": "GeLaCo为LLM压缩提供了一种高效且性能优越的解决方案，支持更灵活的压缩目标。"}}
{"id": "2507.09334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09334", "abs": "https://arxiv.org/abs/2507.09334", "authors": ["Wencan Huang", "Daizong Liu", "Wei Hu"], "title": "Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding", "comment": "Accepted by ACM MM 2025", "summary": "While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable\nscene understanding capabilities, their practical deployment faces critical\nchallenges due to computational inefficiency. The key bottleneck stems from\nprocessing excessive object-centric visual tokens required for comprehensive 3D\nscene representation. Although visual token pruning has shown promise in\naccelerating 2D MLLMs, its applicability to 3D domains remains largely\nunexplored due to fundamental disparities in token structures. In this paper,\nwe reveal two critical insights: (1) Significant redundancy exists in\nobject-level 3D token representations, analogous to patch-level redundancy in\n2D systems; (2) Global attention patterns exhibit strong predictive power for\nidentifying non-essential tokens in 3D contexts. Building on these\nobservations, we propose Fast3D, a plug-and-play visual token pruning framework\nfor 3D MLLMs featuring two technical innovations: (1) Global Attention\nPrediction (GAP), where a lightweight neural network learns to predict the\nglobal attention distributions of the target model, enabling efficient token\nimportance estimation for precise pruning guidance; (2) Sample-Adaptive visual\ntoken Pruning (SAP), which introduces dynamic token budgets through\nattention-based complexity assessment, automatically adjusting layer-wise\npruning ratios based on input characteristics. Both of these two techniques\noperate without modifying the parameters of the target model. Extensive\nevaluations across five benchmarks validate the effectiveness of Fast3D,\nparticularly under high visual token pruning ratios. Code is available at\nhttps://github.com/wencan25/Fast3D", "AI": {"tldr": "Fast3D是一个用于3D多模态大语言模型（MLLMs）的视觉令牌剪枝框架，通过全局注意力预测和样本自适应剪枝技术提高计算效率。", "motivation": "3D MLLMs在场景理解方面表现优异，但计算效率低下限制了其实际部署，主要原因是处理过多的对象中心视觉令牌。", "method": "提出Fast3D框架，包含全局注意力预测（GAP）和样本自适应剪枝（SAP）技术，无需修改目标模型参数。", "result": "在五个基准测试中验证了Fast3D的有效性，尤其在高视觉令牌剪枝比率下表现突出。", "conclusion": "Fast3D为3D MLLMs提供了一种高效且无需修改模型参数的视觉令牌剪枝解决方案。"}}
{"id": "2412.11407", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2412.11407", "abs": "https://arxiv.org/abs/2412.11407", "authors": ["TianZhu Liu", "BangYan Hu", "YanFeng Gu", "Xian Li", "Aleksandra Pižurica"], "title": "An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds", "comment": "16 pages, 9 figures, 5 tables", "summary": "Multispectral point cloud (MPC) captures 3D spatial-spectral information from\nthe observed scene, which can be used for scene understanding and has a wide\nrange of applications. However, most of the existing classification methods\nwere extensively tested on indoor datasets, and when applied to outdoor\ndatasets they still face problems including sparse labeled targets, differences\nin land-covers scales, and long-tailed distributions. To address the above\nissues, an enhanced classification method based on adaptive multi-scale fusion\nfor MPCs with long-tailed distributions is proposed. In the training set\ngeneration stage, a grid-balanced sampling strategy is designed to reliably\ngenerate training samples from sparse labeled datasets. In the feature learning\nstage, a multi-scale feature fusion module is proposed to fuse shallow features\nof land-covers at different scales, addressing the issue of losing fine\nfeatures due to scale variations in land-covers. In the classification stage,\nan adaptive hybrid loss module is devised to utilize multi-classification heads\nwith adaptive weights to balance the learning ability of different classes,\nimproving the classification performance of small classes due to various-scales\nand long-tailed distributions in land-covers. Experimental results on three MPC\ndatasets demonstrate the effectiveness of the proposed method compared with the\nstate-of-the-art methods.", "AI": {"tldr": "提出了一种基于自适应多尺度融合的多光谱点云分类方法，解决了稀疏标注、尺度差异和长尾分布问题。", "motivation": "现有分类方法主要针对室内数据集，应用于室外数据集时面临稀疏标注、尺度差异和长尾分布等问题。", "method": "采用网格平衡采样策略生成训练样本，提出多尺度特征融合模块和自适应混合损失模块。", "result": "在三个多光谱点云数据集上验证了方法的有效性，优于现有方法。", "conclusion": "该方法显著提升了多光谱点云在复杂场景下的分类性能。"}}
{"id": "2507.10073", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10073", "abs": "https://arxiv.org/abs/2507.10073", "authors": ["Simon Münker"], "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "AI": {"tldr": "研究发现大型语言模型（LLMs）无法代表多样文化道德框架，尽管其语言能力强。模型在19种文化背景下与人类道德直觉存在显著差距，且模型规模增大并未改善文化代表性。", "motivation": "探讨AI系统是否能真正代表人类多样化的道德价值观，而非简单地平均化。", "method": "应用道德基础问卷（Moral Foundations Questionnaire）在19种文化背景下比较LLMs与人类道德直觉的差异。", "result": "LLMs系统性地同质化道德多样性，模型规模增大未显著提升文化代表性。", "conclusion": "当前AI对齐方法存在根本性局限，需开发更接地气的对齐目标和评估指标，以确保AI系统能代表多样化的人类价值观。"}}
{"id": "2507.09338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09338", "abs": "https://arxiv.org/abs/2507.09338", "authors": ["Svetlana Orlova", "Tommie Kerssies", "Brunó B. Englert", "Gijs Dubbelman"], "title": "Simplifying Traffic Anomaly Detection with Video Foundation Models", "comment": "ICCVW 2025 accepted. Code: https://github.com/tue-mps/simple-tad", "summary": "Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on\ncomplex multi-stage or multi-representation fusion architectures, yet it\nremains unclear whether such complexity is necessary. Recent findings in visual\nperception suggest that foundation models, enabled by advanced pre-training,\nallow simple yet flexible architectures to outperform specialized designs.\nTherefore, in this work, we investigate an architecturally simple encoder-only\napproach using plain Video Vision Transformers (Video ViTs) and study how\npre-training enables strong TAD performance. We find that: (i) strong\npre-training enables simple encoder-only models to match or even surpass the\nperformance of specialized state-of-the-art TAD methods, while also being\nsignificantly more efficient; (ii) although weakly- and fully-supervised\npre-training are advantageous on standard benchmarks, we find them less\neffective for TAD. Instead, self-supervised Masked Video Modeling (MVM)\nprovides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on\nunlabeled driving videos further improves downstream performance, without\nrequiring anomalous examples. Our findings highlight the importance of\npre-training and show that effective, efficient, and scalable TAD models can be\nbuilt with minimal architectural complexity. We release our code,\ndomain-adapted encoders, and fine-tuned models to support future work:\nhttps://github.com/tue-mps/simple-tad.", "AI": {"tldr": "本文研究了基于预训练的视频视觉变换器（Video ViTs）的简单编码器方法在交通异常检测（TAD）中的表现，发现强预训练可使简单模型性能超越复杂专用方法。", "motivation": "探讨复杂架构是否必要，基于视觉感知研究，利用预训练基础模型简化架构并提升性能。", "method": "使用简单的编码器架构和Video ViTs，研究不同预训练方法（弱监督、全监督、自监督MVM）及领域自适应预训练（DAPT）的效果。", "result": "强预训练使简单模型性能超越专用方法；自监督MVM表现最佳；DAPT进一步提升性能。", "conclusion": "预训练是关键，简单架构结合有效预训练可实现高效、可扩展的TAD模型。"}}
{"id": "2507.10085", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10085", "abs": "https://arxiv.org/abs/2507.10085", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.", "AI": {"tldr": "提出了Critical Representation Fine-Tuning (CRFT)，通过信息流分析识别和优化关键表示，显著提升复杂推理任务的性能。", "motivation": "直接使用原生ReFT方法在复杂推理任务中表现不佳，因为固定位置的表示对输出的影响不确定。", "method": "CRFT通过信息流分析识别关键表示，并在低秩线性子空间中动态优化这些表示，同时冻结基础模型。", "result": "在八个算术和常识推理基准测试中验证了CRFT的有效性和效率，并在少样本设置中提升了16.4%的单次准确率。", "conclusion": "CRFT展示了表示级优化在复杂推理任务中的潜力，为传统PEFT方法提供了轻量且强大的替代方案。"}}
{"id": "2507.09375", "categories": ["cs.CV", "I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.09375", "abs": "https://arxiv.org/abs/2507.09375", "authors": ["Sourish Suri", "Yifei Shao"], "title": "Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture", "comment": "29 pages, 10 figures, 1 table. Code available at:\n  https://github.com/Sourish85/CNN-CROP-DIS-DETECTOR", "summary": "Crop diseases present a significant barrier to agricultural productivity and\nglobal food security, especially in large-scale farming where early\nidentification is often delayed or inaccurate. This research introduces a\nConvolutional Neural Network (CNN)-based image classification system designed\nto automate the detection and classification of eight common crop diseases\nusing leaf imagery. The methodology involves a complete deep learning pipeline:\nimage acquisition from a large, labeled dataset, preprocessing via resizing,\nnormalization, and augmentation, and model training using TensorFlow with\nKeras' Sequential API. The CNN architecture comprises three convolutional\nlayers with increasing filter sizes and ReLU activations, followed by max\npooling, flattening, and fully connected layers, concluding with a softmax\noutput for multi-class classification. The system achieves high training\naccuracy (~90%) and demonstrates reliable performance on unseen data, although\na validation accuracy of ~60% suggests minor overfitting. Notably, the model\nintegrates a treatment recommendation module, providing actionable guidance by\nmapping each detected disease to suitable pesticide or fungicide interventions.\nFurthermore, the solution is deployed on an open-source, mobile-compatible\nplatform, enabling real-time image-based diagnostics for farmers in remote\nareas. This research contributes a scalable and accessible tool to the field of\nprecision agriculture, reducing reliance on manual inspection and promoting\nsustainable disease management practices. By merging deep learning with\npractical agronomic support, this work underscores the potential of CNNs to\ntransform crop health monitoring and enhance food production resilience on a\nglobal scale.", "AI": {"tldr": "本文提出了一种基于CNN的图像分类系统，用于自动检测和分类八种常见作物病害，并通过移动平台为农民提供实时诊断和治疗建议。", "motivation": "作物病害对农业生产和全球粮食安全构成重大威胁，尤其是在大规模农业中，早期识别往往延迟或不准确。", "method": "采用深度学习流程，包括图像采集、预处理（调整大小、归一化和增强）、模型训练（使用TensorFlow和Keras），CNN架构包含三个卷积层、最大池化层和全连接层。", "result": "系统训练准确率约90%，验证准确率约60%，表明存在轻微过拟合，但整体性能可靠。", "conclusion": "该研究为精准农业提供了可扩展且易用的工具，通过结合深度学习和实际农艺支持，展示了CNN在作物健康监测和全球粮食生产中的潜力。"}}
{"id": "2507.10098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10098", "abs": "https://arxiv.org/abs/2507.10098", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.", "AI": {"tldr": "提出了一种结合LLMs和Transformer的混合架构，以融合语义和时间信息，提升时间序列预测性能。", "motivation": "现有LLM方法在时间序列预测中表现不佳，而传统Transformer难以捕捉高级语义模式，因此需要结合两者优势。", "method": "设计了一种混合Transformer架构，融合LLM的语义表示和Transformer的时间动态信息。", "result": "实验证明该方法在基准数据集上优于现有方法。", "conclusion": "通过结合LLMs和Transformer，模型能够更准确地预测未来值。"}}
{"id": "2507.09410", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09410", "abs": "https://arxiv.org/abs/2507.09410", "authors": ["Bernie Boscoe", "Shawn Johnson", "Andrea Osborn", "Chandler Campbell", "Karen Mager"], "title": "GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups", "comment": "This is the preprint version of the paper in Practice and Experience\n  in Advanced Research Computing, PEARC25", "summary": "Camera traps have long been used by wildlife researchers to monitor and study\nanimal behavior, population dynamics, habitat use, and species diversity in a\nnon-invasive and efficient manner. While data collection from the field has\nincreased with new tools and capabilities, methods to develop, process, and\nmanage the data, especially the adoption of ML/AI tools, remain challenging.\nThese challenges include the sheer volume of data generated, the need for\naccurate labeling and annotation, variability in environmental conditions\naffecting data quality, and the integration of ML/AI tools into existing\nworkflows that often require domain-specific customization and computational\nresources. This paper provides a guide to a low-resource pipeline to process\ncamera trap data on-premise, incorporating ML/AI capabilities tailored for\nsmall research groups with limited resources and computational expertise. By\nfocusing on practical solutions, the pipeline offers accessible approaches for\ndata transmission, inference, and evaluation, enabling researchers to discover\nmeaningful insights from their ever-increasing camera trap datasets.", "AI": {"tldr": "本文提出了一种低资源处理相机陷阱数据的流程，结合ML/AI技术，为资源有限的小型研究团队提供实用解决方案。", "motivation": "相机陷阱数据量大、标注复杂、环境多变，且现有ML/AI工具难以集成到资源有限的工作流程中。", "method": "开发了一种低资源流程，支持本地数据处理，包括数据传输、推理和评估，并针对小型研究团队优化。", "result": "该流程为资源有限的研究团队提供了处理和分析相机陷阱数据的可行方法。", "conclusion": "通过实用解决方案，研究者能够从日益增长的相机陷阱数据中提取有价值的信息。"}}
{"id": "2507.10155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10155", "abs": "https://arxiv.org/abs/2507.10155", "authors": ["Khouloud Saadi", "Di Wang"], "title": "Task-Based Flexible Feature Distillation for LLMs", "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.", "AI": {"tldr": "提出一种基于任务的特征蒸馏方法，无需引入新参数即可在不同隐藏层维度的师生模型间实现知识迁移。", "motivation": "传统特征蒸馏方法假设师生模型隐藏层维度相同，限制了学生模型的灵活性；线性投影方法虽能解决此问题，但会引入额外参数并降低性能。", "method": "通过识别教师模型中与任务最相关的隐藏单元，直接将其激活蒸馏到学生模型，无需新参数。", "result": "在分类、指令跟随和摘要等任务上表现优于基线方法，性能提升达3%。", "conclusion": "该方法灵活且高效，能显著提升知识蒸馏性能，适用于多种任务。"}}
{"id": "2507.09446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09446", "abs": "https://arxiv.org/abs/2507.09446", "authors": ["Yuanhong Zheng", "Ruixuan Yu", "Jian Sun"], "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions", "comment": "ICCV 2025", "summary": "3D multi-person motion prediction is a highly complex task, primarily due to\nthe dependencies on both individual past movements and the interactions between\nagents. Moreover, effectively modeling these interactions often incurs\nsubstantial computational costs. In this work, we propose a computationally\nefficient model for multi-person motion prediction by simplifying spatial and\ntemporal interactions. Our approach begins with the design of lightweight dual\nbranches that learn local and global representations for individual and\nmultiple persons separately. Additionally, we introduce a novel cross-level\ninteraction block to integrate the spatial and temporal representations from\nboth branches. To further enhance interaction modeling, we explicitly\nincorporate the spatial inter-person distance embedding. With above efficient\ntemporal and spatial design, we achieve state-of-the-art performance for\nmultiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while\nsignificantly reducing the computational cost. Code is available at\nhttps://github.com/Yuanhong-Zheng/EMPMP.", "AI": {"tldr": "提出了一种计算高效的多人体运动预测模型，通过简化时空交互，结合轻量级双分支设计和跨层级交互块，显著降低了计算成本并实现了最优性能。", "motivation": "多人体运动预测因依赖个体历史运动和交互关系而复杂且计算成本高，需高效建模方法。", "method": "设计轻量级双分支分别学习局部和全局表示，引入跨层级交互块整合时空表示，并加入空间人际距离嵌入。", "result": "在CMU-Mocap、MuPoTS-3D和3DPW数据集上实现最优性能，同时显著降低计算成本。", "conclusion": "通过高效时空设计，模型在多人体运动预测中表现出色且计算高效。"}}
{"id": "2507.09487", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09487", "abs": "https://arxiv.org/abs/2507.09487", "authors": ["Changli Wang", "Fang Yin", "Jiafeng Liu", "Rui Wu"], "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space", "comment": null, "summary": "Visual and semantic concepts are often structured in a hierarchical manner.\nFor instance, textual concept `cat' entails all images of cats. A recent study,\nMERU, successfully adapts multimodal learning techniques from Euclidean space\nto hyperbolic space, effectively capturing the visual-semantic hierarchy.\nHowever, a critical question remains: how can we more efficiently train a model\nto capture and leverage this hierarchy? In this paper, we propose the\n\\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel\nand efficient method that integrates Masked Image Modeling (MIM) and knowledge\ndistillation techniques within hyperbolic space. To the best of our knowledge,\nthis is the first approach to leverage MIM and knowledge distillation in\nhyperbolic space to train highly efficient models. In addition, we introduce a\ndistillation loss function specifically designed to facilitate effective\nknowledge transfer in hyperbolic space. Our experiments demonstrate that MIM\nand knowledge distillation techniques in hyperbolic space can achieve the same\nremarkable success as in Euclidean space. Extensive evaluations show that our\nmethod excels across a wide range of downstream tasks, significantly\noutperforming existing models like MERU and CLIP in both image classification\nand retrieval.", "AI": {"tldr": "HMID-Net提出了一种在双曲空间中结合掩码图像建模（MIM）和知识蒸馏的新方法，显著提升了视觉-语义层次结构的捕捉效率。", "motivation": "现有方法MERU虽成功将多模态学习技术从欧几里得空间扩展到双曲空间，但如何更高效地训练模型以利用这种层次结构仍是一个关键问题。", "method": "提出HMID-Net，在双曲空间中整合MIM和知识蒸馏技术，并设计了一种新的蒸馏损失函数以优化知识传递。", "result": "实验表明，该方法在下游任务中表现优异，显著优于MERU和CLIP等现有模型。", "conclusion": "HMID-Net证明了在双曲空间中应用MIM和知识蒸馏的可行性，为高效模型训练提供了新思路。"}}
{"id": "2507.10177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10177", "abs": "https://arxiv.org/abs/2507.10177", "authors": ["Rohitash Chandra", "Jiyong Choi"], "title": "Abusive text transformation using LLMs", "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.", "AI": {"tldr": "研究探讨了大型语言模型（LLMs）在识别和转换辱骂性文本为无辱骂版本的能力，评估了Gemini、GPT-4o、DeepSeek和Groq等模型的表现，发现Groq与其他模型差异显著。", "motivation": "探索LLMs在辱骂性文本分类和转换中的有效性，以保留文本意图的同时去除不当内容。", "method": "使用LLMs识别并转换辱骂性文本（推文和评论），通过情感和语义分析评估原始与转换后文本。", "result": "Groq的表现与其他模型差异显著，GPT-4o与DeepSeek-V3表现相似。", "conclusion": "LLMs在辱骂性文本转换中具有潜力，但不同模型表现差异较大，需进一步研究。"}}
{"id": "2507.09471", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09471", "abs": "https://arxiv.org/abs/2507.09471", "authors": ["Lingfeng He", "De Cheng", "Zhiheng Ma", "Huaijie Wang", "Dingwen Zhang", "Nannan Wang", "Xinbo Gao"], "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning", "comment": null, "summary": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods.", "AI": {"tldr": "CKAA框架通过双级知识对齐和任务信心引导的适配器混合，解决了PEFT-based CL方法在误导任务ID下的模糊决策问题。", "motivation": "由于独立训练的子模块导致特征子空间不对齐，现有方法在误导任务ID下表现不佳。", "method": "提出CKAA框架，包括双级知识对齐（DKA）和任务信心引导的适配器混合（TC-MoA）。", "result": "实验表明CKAA优于现有PEFT-based CL方法。", "conclusion": "CKAA通过知识对齐和自适应聚合提升了模型对误导任务ID的鲁棒性。"}}
{"id": "2507.09492", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09492", "abs": "https://arxiv.org/abs/2507.09492", "authors": ["Fuyin Ye", "Erwen Yao", "Jianyong Chen", "Fengmei He", "Junxiang Zhang", "Lihao Ni"], "title": "SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification", "comment": "4 pages, 2 figures", "summary": "Hyperspectral image classification plays a pivotal role in precision\nagriculture, providing accurate insights into crop health monitoring, disease\ndetection, and soil analysis. However, traditional methods struggle with\nhigh-dimensional data, spectral-spatial redundancy, and the scarcity of labeled\nsamples, often leading to suboptimal performance. To address these challenges,\nwe propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines\ntensor decomposition with regularization mechanisms to dynamically adjust\ntensor ranks, ensuring optimal feature representation tailored to the\ncomplexity of the data. Building upon SDTN, we propose the Tensor-Regularized\nNetwork (TRN), which integrates the features extracted by SDTN into a\nlightweight network capable of capturing spectral-spatial features at multiple\nscales. This approach not only maintains high classification accuracy but also\nsignificantly reduces computational complexity, making the framework highly\nsuitable for real-time deployment in resource-constrained environments.\nExperiments on PaviaU datasets demonstrate significant improvements in accuracy\nand reduced model parameters compared to state-of-the-art methods.", "AI": {"tldr": "提出了一种结合张量分解和正则化的自适应网络SDTN，并进一步提出轻量级网络TRN，用于高光谱图像分类，显著提升精度并降低计算复杂度。", "motivation": "传统方法在处理高维数据、光谱-空间冗余和标记样本稀缺时表现不佳，需要更高效的方法。", "method": "结合张量分解与正则化机制的自适应网络SDTN动态调整张量秩，TRN集成SDTN提取的特征，实现多尺度光谱-空间特征捕获。", "result": "在PaviaU数据集上实验表明，相比现有方法，精度显著提升且模型参数减少。", "conclusion": "SDTN和TRN框架在资源受限环境中具有实时部署潜力，适用于精准农业的高光谱图像分类。"}}
{"id": "2507.10216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10216", "abs": "https://arxiv.org/abs/2507.10216", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.", "AI": {"tldr": "论文介绍了Absher基准，用于评估大语言模型在沙特阿拉伯方言中的表现，揭示了其在文化和上下文理解上的不足。", "motivation": "评估大语言模型对阿拉伯语方言和文化细微差别的理解能力，特别是在沙特阿拉伯这样的语言多样化环境中。", "method": "开发了包含18,000多个多选题的Absher基准，涵盖六类任务，评估多种先进的大语言模型。", "result": "结果显示模型在需要文化推断或上下文理解的任务中存在显著性能差距。", "conclusion": "强调了方言感知训练和文化对齐评估方法的紧迫性，以提升大语言模型在阿拉伯语应用中的表现。"}}
{"id": "2507.09491", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09491", "abs": "https://arxiv.org/abs/2507.09491", "authors": ["Yiyang Zhou", "Linjie Li", "Shi Qiu", "Zhengyuan Yang", "Yuyang Zhao", "Siwei Han", "Yangfan He", "Kangqi Li", "Haonian Ji", "Zihao Zhao", "Haibo Tong", "Lijuan Wang", "Huaxiu Yao"], "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?", "comment": "15 pages, 10 figures", "summary": "Existing video benchmarks often resemble image-based benchmarks, with\nquestion types like \"What actions does the person perform throughout the\nvideo?\" or \"What color is the woman's dress in the video?\" For these, models\ncan often answer by scanning just a few key frames, without deep temporal\nreasoning. This limits our ability to assess whether large vision-language\nmodels (LVLMs) can truly think with videos rather than perform superficial\nframe-level analysis. To address this, we introduce GLIMPSE, a benchmark\nspecifically designed to evaluate whether LVLMs can genuinely think with\nvideos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video\nunderstanding beyond static image cues. It consists of 3,269 videos and over\n4,342 highly visual-centric questions across 11 categories, including\nTrajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions\nare carefully crafted by human annotators and require watching the entire video\nand reasoning over full video context-this is what we mean by thinking with\nvideo. These questions cannot be answered by scanning selected frames or\nrelying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,\nbut current LVLMs face significant challenges. Even the best-performing model,\nGPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move\nbeyond surface-level reasoning to truly think with videos.", "AI": {"tldr": "GLIMPSE是一个新的视频理解基准，旨在评估大型视觉语言模型（LVLMs）是否能真正进行视频推理，而非仅依赖关键帧或文本。", "motivation": "现有视频基准测试多基于静态图像，无法评估模型是否具备深层次的时间推理能力，限制了LVLMs的真实视频理解能力评估。", "method": "GLIMPSE包含3,269个视频和4,342个视觉中心问题，涵盖11个类别，所有问题需观看完整视频并进行上下文推理。", "result": "人类评估准确率达94.82%，而最佳LVLM模型（GPT-o3）仅达66.43%，显示模型在视频推理上仍有显著挑战。", "conclusion": "GLIMPSE揭示了LVLMs在视频理解上的局限性，强调了开发更高级时间推理模型的必要性。"}}
{"id": "2507.09514", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09514", "abs": "https://arxiv.org/abs/2507.09514", "authors": ["Tien-Yu Chi", "Hung-Yueh Chiang", "Diana Marculescu", "Kai-Chiang Wu"], "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models", "comment": "Accepted by Efficient Systems for Foundation Models Workshop at the\n  International Conference on Machine Learning (ICML) 2025", "summary": "State space models (SSMs) reduce the quadratic complexity of transformers by\nleveraging linear recurrence. Recently, VMamba has emerged as a strong\nSSM-based vision backbone, yet remains bottlenecked by spatial redundancy in\nits four-directional scan. We propose QuarterMap, a post-training activation\npruning method that removes redundant spatial activations before scanning and\nrestores dimensions via nearest-neighbor upsampling. Our method improves\nthroughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%\nspeedup on VMamba with less than 0.9% accuracy drop, and yields similar gains\non ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a\ndomain-specific model that shares the same four-directional scanning structure,\nwhere it consistently improves throughput while preserving accuracy across\nmultiple medical imaging tasks. Compared to token merging methods like ToMe,\nQuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our\nmethod offers a plug-and-play tool for deployment-time efficiency without\ncompromising transferability.", "AI": {"tldr": "QuarterMap是一种后训练激活剪枝方法，通过移除冗余空间激活并恢复维度，提升VMamba等SSM模型的吞吐量，同时保持准确性。", "motivation": "解决VMamba等基于状态空间模型（SSM）的视觉骨干网络中空间冗余问题，提升效率。", "method": "提出QuarterMap方法，通过剪枝冗余空间激活并在扫描前通过最近邻上采样恢复维度，无需重新训练。", "result": "在ImageNet-1K上实现11%的速度提升，精度下降小于0.9%；在ADE20K和MedMamba上也表现良好。", "conclusion": "QuarterMap是一种即插即用的高效部署工具，适用于SSM模型，无需牺牲迁移性。"}}
{"id": "2507.10326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10326", "abs": "https://arxiv.org/abs/2507.10326", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.", "AI": {"tldr": "提出了一种基于进化搜索的自动离散提示优化方法，用于解决复杂任务和小型语言模型的提示设计问题。", "motivation": "现有自动化提示工程方法主要针对简单任务和大型语言模型，而复杂任务和小型模型对提示设计更敏感，需要更高效的优化方法。", "method": "采用两阶段进化搜索：第一阶段使用语法引导的遗传编程合成提示创建程序；第二阶段通过局部搜索进一步优化性能。", "result": "在三个小型通用语言模型和四个领域特定任务中，该方法优于PromptWizard、OPRO和RL-Prompt等现有方法。", "conclusion": "该方法在复杂任务和小型模型中表现优异，性能提升显著，且退化情况极少。"}}
{"id": "2507.09500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09500", "abs": "https://arxiv.org/abs/2507.09500", "authors": ["Yiwen Liang", "Hui Chen", "Yizhe Xiong", "Zihan Zhou", "Mengyao Lyu", "Zijia Lin", "Shuaicheng Niu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations", "comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)", "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.", "AI": {"tldr": "论文提出了一种可靠的测试时适应方法（ReTA），通过一致性感知熵重加权（CER）和多样性驱动的分布校准（DDC）解决现有缓存方法在分布偏移下的不可靠性问题。", "motivation": "视觉语言模型（VLMs）在零样本任务中表现优异，但在分布偏移下性能下降，现有测试时适应（TTA）方法因熵不可靠和决策边界僵化而面临挑战。", "method": "ReTA结合CER（利用一致性约束加权熵）和DDC（通过多变量高斯分布建模类文本嵌入），提升缓存质量和预测准确性。", "result": "实验表明，ReTA在真实分布偏移下显著优于现有方法。", "conclusion": "ReTA通过双重策略有效提升了测试时适应的可靠性，为分布偏移下的视觉语言任务提供了实用解决方案。"}}
{"id": "2507.09531", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09531", "abs": "https://arxiv.org/abs/2507.09531", "authors": ["Son Nguyen", "Giang Nguyen", "Hung Dao", "Thao Do", "Daeyoung Kim"], "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization", "comment": "Under Review", "summary": "Key Information Extraction (KIE) underpins the understanding of visual\ndocuments (e.g., receipts and contracts) by extracting precise semantic content\nand accurately capturing spatial structure. Yet existing multimodal large\nlanguage models (MLLMs) often perform poorly on dense documents and rely on\nvision tokenization approaches that scale with image size, leading to redundant\ncomputation and memory inefficiency. To address these challenges, we introduce\nVDInstruct, an MLLM that separates spatial region detection from semantic\nfeature extraction. Central to our model is a content-aware tokenization\nstrategy: rather than fragmenting the entire image uniformly, it generates\ntokens in proportion to document complexity, preserving critical structure\nwhile eliminating wasted tokens. Leveraging a three-stage training paradigm,\nour model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching\nor exceeding the accuracy of leading approaches while reducing the number of\nimage tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses\nstrong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its\nrobustness to unseen documents. These findings show that content-aware\ntokenization combined with explicit layout modeling offers a promising\ndirection forward for document understanding. Data, source code, and model\nweights will be made publicly available.", "AI": {"tldr": "VDInstruct是一种多模态大语言模型（MLLM），通过内容感知的标记化策略和显式布局建模，显著提升了密集文档的关键信息提取（KIE）性能，同时减少了计算冗余。", "motivation": "现有MLLM在密集文档上表现不佳，且视觉标记化方法效率低下，导致计算冗余和内存浪费。", "method": "提出VDInstruct模型，分离空间区域检测与语义特征提取，采用内容感知标记化策略，根据文档复杂度生成标记，并结合三阶段训练范式。", "result": "在KIE基准测试中达到SOTA，零样本评估中F1分数提升5.5点，图像标记数量减少约3.6倍。", "conclusion": "内容感知标记化与显式布局建模为文档理解提供了有前景的方向。"}}
{"id": "2507.10330", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10330", "abs": "https://arxiv.org/abs/2507.10330", "authors": ["Mohammed Bouri", "Adnane Saoud"], "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "AI": {"tldr": "论文提出了一种基于Growth Bound Matrices（GBM）的新正则化技术，用于提升NLP模型对对抗攻击的鲁棒性，特别是在LSTM、S4和CNN架构中。", "motivation": "尽管NLP领域有所进步，但模型仍易受对抗攻击（如同义词替换）的影响，尤其是循环网络和现代状态空间模型（SSMs）的鲁棒性研究不足。", "method": "通过计算GBM，减少输入扰动对模型输出的影响，重点关注LSTM、S4和CNN三种架构。", "result": "实验表明，该方法在对抗鲁棒性上比现有基线提升高达8.8%，并在对抗防御中优于多种先进方法。", "conclusion": "GBM方法有效提升了NLP模型的鲁棒性，填补了SSM（S4）鲁棒性研究的空白。"}}
{"id": "2507.09512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09512", "abs": "https://arxiv.org/abs/2507.09512", "authors": ["Pengyu Liu", "Kun Li", "Fei Wang", "Yanyan Wei", "Junhui She", "Dan Guo"], "title": "Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention", "comment": "11 pages, 4 figures", "summary": "In this paper, we introduce the latest solution developed by our team,\nHFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA\nChallenge. The Micro-gesture Online Recognition task is a highly challenging\nproblem that aims to locate the temporal positions and recognize the categories\nof multiple micro-gesture instances in untrimmed videos. Compared to\ntraditional temporal action detection, this task places greater emphasis on\ndistinguishing between micro-gesture categories and precisely identifying the\nstart and end times of each instance. Moreover, micro-gestures are typically\nspontaneous human actions, with greater differences than those found in other\nhuman actions. To address these challenges, we propose hand-crafted data\naugmentation and spatial-temporal attention to enhance the model's ability to\nclassify and localize micro-gestures more accurately. Our solution achieved an\nF1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a\nresult, our method ranked first in the Micro-gesture Online Recognition track.", "AI": {"tldr": "HFUT-VUT团队提出了一种用于微手势在线识别的新方法，结合数据增强和时空注意力机制，显著提升了性能，并在IJCAI 2025 MiGA挑战赛中排名第一。", "motivation": "微手势在线识别任务具有挑战性，需精确定位时间位置并区分类别，传统方法难以应对。", "method": "采用手工数据增强和时空注意力机制，提升分类和定位能力。", "result": "F1分数达到38.03，比之前最优方法提升37.9%。", "conclusion": "该方法在微手势在线识别任务中表现优异，成为新的最优解决方案。"}}
{"id": "2507.09562", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09562", "abs": "https://arxiv.org/abs/2507.09562", "authors": ["Yidong Jiang"], "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges", "comment": null, "summary": "The Segment Anything Model (SAM) has revolutionized image segmentation\nthrough its innovative prompt-based approach, yet the critical role of prompt\nengineering in its success remains underexplored. This paper presents the first\ncomprehensive survey focusing specifically on prompt engineering techniques for\nSAM and its variants. We systematically organize and analyze the rapidly\ngrowing body of work in this emerging field, covering fundamental\nmethodologies, practical applications, and key challenges. Our review reveals\nhow prompt engineering has evolved from simple geometric inputs to\nsophisticated multimodal approaches, enabling SAM's adaptation across diverse\ndomains including medical imaging and remote sensing. We identify unique\nchallenges in prompt optimization and discuss promising research directions.\nThis survey fills an important gap in the literature by providing a structured\nframework for understanding and advancing prompt engineering in foundation\nmodels for segmentation.", "AI": {"tldr": "本文综述了Segment Anything Model (SAM)中提示工程的技术、应用及挑战，填补了该领域的研究空白。", "motivation": "探索提示工程在SAM及其变体中的关键作用，填补现有研究的不足。", "method": "系统整理和分析提示工程的技术、应用及挑战，涵盖基础方法和多模态应用。", "result": "揭示了提示工程从简单几何输入到多模态方法的演变，并识别了优化挑战。", "conclusion": "为分割基础模型中的提示工程提供了结构化框架，并指出了未来研究方向。"}}
{"id": "2507.10342", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10342", "abs": "https://arxiv.org/abs/2507.10342", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "title": "Using AI to replicate human experimental results: a motion study", "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.", "AI": {"tldr": "论文探讨了大型语言模型（LLM）在语言学研究中作为可靠分析工具的潜力，发现LLM与人类在情感意义判断上高度一致。", "motivation": "验证LLM是否能复制人类在语言学任务中的细微判断，以扩展研究规模和效率。", "method": "通过四项心理语言学实验（涉及情感意义、情绪语境中的动词选择等），比较人类与LLM的反应。", "result": "人类与LLM的反应高度相关（Spearman's rho = .73-.96），表明LLM可作为有效补充工具。", "conclusion": "LLM在语言学研究中具有可信度和信息价值，能增强传统实验并支持假设生成。"}}
{"id": "2507.09524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09524", "abs": "https://arxiv.org/abs/2507.09524", "authors": ["Yunwei Lan", "Zhigao Cui", "Xin Luo", "Chang Liu", "Nian Wang", "Menglin Zhang", "Yanzhao Su", "Dong Liu"], "title": "When Schrödinger Bridge Meets Real-World Image Dehazing with Unpaired Training", "comment": "Accepted by ICCV2025", "summary": "Recent advancements in unpaired dehazing, particularly those using GANs, show\npromising performance in processing real-world hazy images. However, these\nmethods tend to face limitations due to the generator's limited transport\nmapping capability, which hinders the full exploitation of their effectiveness\nin unpaired training paradigms. To address these challenges, we propose\nDehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger\nBridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges\nthe distributions between hazy and clear images. This enables optimal transport\nmappings from hazy to clear images in fewer steps, thereby generating\nhigh-quality results. To ensure the consistency of structural information and\ndetails in the restored images, we introduce detail-preserving regularization,\nwhich enforces pixel-level alignment between hazy inputs and dehazed outputs.\nFurthermore, we propose a novel prompt learning to leverage pre-trained CLIP\nmodels in distinguishing hazy images and clear ones, by learning a haze-aware\nvision-language alignment. Extensive experiments on multiple real-world\ndatasets demonstrate our method's superiority. Code:\nhttps://github.com/ywxjm/DehazeSB.", "AI": {"tldr": "DehazeSB是一种基于Schrödinger Bridge的新型无配对去雾框架，通过最优传输理论直接桥接雾图和清晰图的分布，生成高质量结果。", "motivation": "现有基于GAN的无配对去雾方法因生成器的传输映射能力有限而效果受限，需改进。", "method": "利用最优传输理论桥接分布，引入细节保留正则化和基于CLIP的提示学习。", "result": "在多个真实数据集上表现优越。", "conclusion": "DehazeSB通过优化传输映射和细节保留，显著提升了无配对去雾效果。"}}
{"id": "2507.09574", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09574", "abs": "https://arxiv.org/abs/2507.09574", "authors": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Liang Chen", "Jiuxiang Gu", "Wen Xiao", "Junjie Hu"], "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models", "comment": "24 pages,12 figures", "summary": "Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR", "AI": {"tldr": "MENTOR是一个新颖的自回归框架，通过两阶段训练实现多模态输入与图像输出的细粒度对齐，提升了生成控制性和效率。", "motivation": "解决现有文本到图像模型在多模态输入平衡、精确视觉控制和复杂多模态图像生成训练需求方面的局限性。", "method": "结合自回归图像生成器和两阶段训练范式：多模态对齐阶段和指令调优阶段，无需额外适配器或交叉注意力模块。", "result": "在DreamBench++基准测试中表现优异，优于基线模型，概念保留和提示跟随能力更强，图像重建保真度高且训练效率提升。", "conclusion": "MENTOR在多模态图像生成中实现了高效、可控和高质量的生成，优于基于扩散的方法。"}}
{"id": "2507.10354", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10354", "abs": "https://arxiv.org/abs/2507.10354", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.", "AI": {"tldr": "论文提出了一种分层隐喻处理模型，将隐喻意义分为内容分析、概念融合和语用意三个层次，为计算系统提供了更丰富的隐喻解释框架。", "motivation": "隐喻意义并非简单的概念映射，而是一个复杂的认知现象，需要多层次解释。现有方法难以全面捕捉隐喻的深度和语境敏感性。", "method": "提出三层模型：内容分析（标注基本概念元素）、概念融合（建模概念组合）、语用意（引入语用词汇捕捉意图和语境效果）。", "result": "模型为计算系统提供了更丰富、更认知基础的隐喻解释方法，支持更深层次和语境敏感的推理。", "conclusion": "通过统一多层框架，该模型为计算隐喻理解奠定了基础，超越了表面关联，实现了更深层次的意义表达。"}}
{"id": "2507.09541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09541", "abs": "https://arxiv.org/abs/2507.09541", "authors": ["Zihao Xiong", "Fei Zhou", "Fengyi Wu", "Shuai Yuan", "Maixia Fu", "Zhenming Peng", "Jian Yang", "Yimian Dai"], "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection", "comment": "Accepted by TGRS", "summary": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.", "AI": {"tldr": "提出了一种动态RPCA网络（DRPCA-Net），通过结合稀疏先验和深度学习，提高了红外小目标检测的准确性和泛化能力。", "motivation": "现有深度学习方法在红外小目标检测中过于复杂且缺乏可解释性，忽视了目标的稀疏性先验。", "method": "基于RPCA模型，设计了一个动态展开网络（DRPCA-Net），通过轻量级超网络动态生成参数，并引入动态残差组（DRG）模块优化背景建模。", "result": "在多个公开红外数据集上，DRPCA-Net显著优于现有方法。", "conclusion": "DRPCA-Net通过动态机制和稀疏先验的结合，实现了高效且鲁棒的红外小目标检测。"}}
{"id": "2507.09630", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09630", "abs": "https://arxiv.org/abs/2507.09630", "authors": ["Shomukh Qari", "Maha A. Thafar"], "title": "Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI", "comment": "5 figures", "summary": "Stroke is one of the leading causes of death globally, making early and\naccurate diagnosis essential for improving patient outcomes, particularly in\nemergency settings where timely intervention is critical. CT scans are the key\nimaging modality because of their speed, accessibility, and cost-effectiveness.\nThis study proposed an artificial intelligence framework for multiclass stroke\nclassification (ischemic, hemorrhagic, and no stroke) using CT scan images from\na dataset provided by the Republic of Turkey's Ministry of Health. The proposed\nmethod adopted MaxViT, a state-of-the-art Vision Transformer, as the primary\ndeep learning model for image-based stroke classification, with additional\ntransformer variants (vision transformer, transformer-in-transformer, and\nConvNext). To enhance model generalization and address class imbalance, we\napplied data augmentation techniques, including synthetic image generation. The\nMaxViT model trained with augmentation achieved the best performance, reaching\nan accuracy and F1-score of 98.00%, outperforming all other evaluated models\nand the baseline methods. The primary goal of this study was to distinguish\nbetween stroke types with high accuracy while addressing crucial issues of\ntransparency and trust in artificial intelligence models. To achieve this,\nExplainable Artificial Intelligence (XAI) was integrated into the framework,\nparticularly Grad-CAM++. It provides visual explanations of the model's\ndecisions by highlighting relevant stroke regions in the CT scans and\nestablishing an accurate, interpretable, and clinically applicable solution for\nearly stroke detection. This research contributed to the development of a\ntrustworthy AI-assisted diagnostic tool for stroke, facilitating its\nintegration into clinical practice and enhancing access to timely and optimal\nstroke diagnosis in emergency departments, thereby saving more lives.", "AI": {"tldr": "提出了一种基于MaxViT的多类别中风分类AI框架，结合数据增强和XAI技术，在CT扫描图像上达到98%的准确率和F1分数。", "motivation": "中风是全球主要死因之一，早期准确诊断对急诊至关重要，CT扫描因其快速、可及和成本效益成为关键影像手段。", "method": "采用MaxViT作为主模型，结合其他Transformer变体，通过数据增强（包括合成图像生成）解决类别不平衡问题，并集成XAI技术（如Grad-CAM++）提高模型透明度。", "result": "MaxViT模型在增强数据训练下表现最佳，准确率和F1分数均为98%，优于其他模型和基线方法。", "conclusion": "研究开发了一种可信赖的AI辅助诊断工具，有助于临床实践中的中风早期检测，提升急诊诊断效率，挽救更多生命。"}}
{"id": "2507.10435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10435", "abs": "https://arxiv.org/abs/2507.10435", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.", "AI": {"tldr": "大型语言模型（LLMs）能够通过文本描述理解图结构，本文提出诱导子结构过滤（ISF）视角，解释Transformer如何识别子结构，并验证其在多种图类型中的能力。", "motivation": "探索仅解码器Transformer架构如何理解图结构，特别是通过子结构提取任务分析其内部机制。", "method": "提出诱导子结构过滤（ISF）视角，结合实证和理论分析，验证其在多层Transformer中的一致性。", "result": "发现LLMs能有效提取属性图（如分子图）的子结构，展示其在复杂模式提取中的能力。", "conclusion": "为序列Transformer处理图数据提供了新见解，展示了其在子结构提取任务中的潜力。"}}
{"id": "2507.09556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09556", "abs": "https://arxiv.org/abs/2507.09556", "authors": ["Ximeng Zhai", "Bohan Xu", "Yaohong Chen", "Hao Wang", "Kehua Guo", "Yimian Dai"], "title": "SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing", "comment": "Accepted by TGRS", "summary": "Due to the limitation of the optical lens focal length and the resolution of\nthe infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)\ngroups typically appear as mixing spots in the infrared image. In this paper,\nwe propose a novel task, Sequential CSIST Unmixing, namely detecting all\ntargets in the form of sub-pixel localization from a highly dense CSIST group.\nHowever, achieving such precise detection is an extremely difficult challenge.\nIn addition, the lack of high-quality public datasets has also restricted the\nresearch progress. To this end, firstly, we contribute an open-source\necosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit\nthat provides objective evaluation metrics for this special task, along with\nthe implementation of 23 relevant methods. Furthermore, we propose the\nDeformable Refinement Network (DeRefNet), a model-driven deep learning\nframework that introduces a Temporal Deformable Feature Alignment (TDFA) module\nenabling adaptive inter-frame information aggregation. To the best of our\nknowledge, this work is the first endeavor to address the CSIST Unmixing task\nwithin a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate\nthat our method outperforms the state-of-the-art approaches with mean Average\nPrecision (mAP) metric improved by 5.3\\%. Our dataset and toolkit are available\nfrom https://github.com/GrokCV/SeqCSIST.", "AI": {"tldr": "论文提出了一种新任务——顺序CSIST解混，旨在从高密度CSIST群中检测所有目标，并贡献了一个开源生态系统和DeRefNet模型，性能优于现有方法。", "motivation": "由于光学镜头焦距和红外探测器分辨率的限制，远距离CSIST群在红外图像中通常表现为混合斑点，缺乏高质量数据集限制了研究进展。", "method": "提出了DeRefNet模型，包含TDFA模块，用于自适应帧间信息聚合，并贡献了SeqCSIST数据集和工具包。", "result": "在SeqCSIST数据集上，DeRefNet的mAP指标比现有方法提高了5.3%。", "conclusion": "本研究首次在多帧范式中解决CSIST解混任务，开源数据集和工具包将推动相关研究。"}}
{"id": "2507.09861", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09861", "abs": "https://arxiv.org/abs/2507.09861", "authors": ["Yihao Ding", "Siwen Luo", "Yue Dai", "Yanbei Jiang", "Zechuan Li", "Geoffrey Martin", "Yifan Peng"], "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends", "comment": "Work in progress", "summary": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field,\ndriven by the need to automatically process documents containing complex\nvisual, textual, and layout information. Recently, Multimodal Large Language\nModels (MLLMs) have shown remarkable potential in this domain, leveraging both\nOptical Character Recognition (OCR)-dependent and OCR-free frameworks to\nextract and interpret information in document images. This survey reviews\nrecent advancements in MLLM-based VRDU, highlighting three core components: (1)\nmethods for encoding and fusing textual, visual, and layout features; (2)\ntraining paradigms, including pretraining strategies, instruction-response\ntuning, and the trainability of different model modules; and (3) datasets\nutilized for pretraining, instruction-tuning, and supervised fine-tuning.\nFinally, we discuss the challenges and opportunities in this evolving field and\npropose future directions to advance the efficiency, generalizability, and\nrobustness of VRDU systems.", "AI": {"tldr": "该论文综述了基于多模态大语言模型（MLLMs）的视觉丰富文档理解（VRDU）的最新进展，包括特征编码与融合方法、训练范式及数据集，并探讨了未来研究方向。", "motivation": "视觉丰富文档理解（VRDU）的需求推动了自动处理复杂文档的研究，多模态大语言模型（MLLMs）在此领域展现出潜力。", "method": "论文综述了MLLMs在VRDU中的应用，包括文本、视觉和布局特征的编码与融合方法，训练范式（如预训练、指令响应调优），以及相关数据集的使用。", "result": "总结了MLLMs在VRDU中的核心组件和当前研究进展，提出了未来提升系统效率、泛化性和鲁棒性的方向。", "conclusion": "VRDU领域仍面临挑战，但MLLMs的应用为未来发展提供了重要机遇。"}}
{"id": "2507.10445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10445", "abs": "https://arxiv.org/abs/2507.10445", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.", "AI": {"tldr": "研究探讨了LLMs在任务导向对话中提出澄清问题的能力，发现人类与LLMs在模糊性处理上相关性低，且LLMs的提问能力可能与推理能力相关。", "motivation": "探索LLMs在异步任务导向对话中提出澄清问题的能力，并比较其与人类行为的差异。", "method": "结合Minecraft Dialogue Corpus的两种标注，构建新语料库，对比人类与LLMs在模糊性情况下的行为。", "result": "人类很少因指代模糊提问，而LLMs更多；人类更多因任务不确定性提问，而LLMs较少。推理能力可能提升LLMs提问频率和相关性。", "conclusion": "LLMs的澄清问题能力与推理能力相关，但与人类行为差异显著。"}}
{"id": "2507.09560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09560", "abs": "https://arxiv.org/abs/2507.09560", "authors": ["Bolun Zheng", "Xinjie Liu", "Qianyu Zhang", "Canjin Wang", "Fangni Chen", "Mingen Xu"], "title": "EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation", "comment": null, "summary": "3D hand pose estimation has garnered great attention in recent years due to\nits critical applications in human-computer interaction, virtual reality, and\nrelated fields. The accurate estimation of hand joints is essential for\nhigh-quality hand pose estimation. However, existing methods neglect the\nimportance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints\noverall and often fail to account for the phenomenon of error accumulation for\ndistal joints in gesture estimation, which can cause certain joints to incur\nlarger errors, resulting in misalignments and artifacts in the pose estimation\nand degrading the overall reconstruction quality. To address this challenge, we\npropose a novel segmented architecture for enhanced hand pose estimation\n(EHPE). We perform local extraction of TIP and wrist, thus alleviating the\neffect of error accumulation on TIP prediction and further reduce the\npredictive errors for all joints on this basis. EHPE consists of two key\nstages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions\nof the TIP and wrist joints are estimated to provide an initial accurate joint\nconfiguration; In the Prior Guided Joints Estimation stage (PG-stage), a\ndual-branch interaction network is employed to refine the positions of the\nremaining joints. Extensive experiments on two widely used benchmarks\ndemonstrate that EHPE achieves state-of-the-arts performance. Code is available\nat https://github.com/SereinNout/EHPE.", "AI": {"tldr": "论文提出了一种分段架构EHPE，通过局部提取指尖和手腕关节，减少误差累积，提升手部姿态估计精度。", "motivation": "现有方法忽视指尖和手腕关节的重要性，且未解决远端关节误差累积问题，导致姿态估计质量下降。", "method": "EHPE分为两个阶段：TW-stage提取指尖和手腕关节；PG-stage通过双分支交互网络优化其余关节位置。", "result": "在两个广泛使用的基准测试中，EHPE实现了最先进的性能。", "conclusion": "EHPE通过分段架构显著提升了手部姿态估计的精度，解决了误差累积问题。"}}
{"id": "2507.09876", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09876", "abs": "https://arxiv.org/abs/2507.09876", "authors": ["Yongheng Zhang", "Xu Liu", "Ruihan Tao", "Qiguang Chen", "Hao Fei", "Wanxiang Che", "Libo Qin"], "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models", "comment": "Accepted by ACM MM 2025", "summary": "Video understanding plays a vital role in bridging low-level visual signals\nwith high-level cognitive reasoning, and is fundamental to applications such as\nautonomous driving, embodied AI, and the broader pursuit of AGI. The rapid\ndevelopment of large language models (LLMs), particularly those utilizing\nChain-of-Thought (CoT) technology, has significantly advanced video reasoning\ncapabilities. However, current approaches primarily depend on textual\ninformation for reasoning, overlooking the visual modality in the actual video\nreasoning process. In contrast, humans naturally re-examine visual content\nwhile reasoning. Motivated by this, we introduce a novel video reasoning\nparadigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive\nand cognitively aligned reasoning. To the end, first, we construct the\nVideo-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for\nkey-video selection and manually verified. Furthermore, we extensively explore\nthe potential of the ViTCoT paradigm in the video understanding field.\nExtensive experiments demonstrate that ViTCoT significantly enhances\nperformance compared to the traditional text-only CoT paradigm and effectively\nactivates more neuron values in MLLMs.", "AI": {"tldr": "论文提出了一种新的视频推理范式ViTCoT，结合视觉和文本信息，显著提升了视频理解性能。", "motivation": "现有方法主要依赖文本信息进行推理，忽视了视觉模态，而人类在推理时会自然结合视觉内容。", "method": "构建了Video-Text Interleaved Benchmark (ViTIB)，并探索了ViTCoT范式在视频理解中的应用。", "result": "ViTCoT显著优于传统文本CoT范式，并激活了更多MLLMs神经元。", "conclusion": "ViTCoT为视频推理提供了一种更直观且认知对齐的方法。"}}
{"id": "2507.10468", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10468", "abs": "https://arxiv.org/abs/2507.10468", "authors": ["Ariadna Mon", "Saúl Fenollosa", "Jon Lecumberri"], "title": "From BERT to Qwen: Hate Detection across architectures", "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).", "AI": {"tldr": "研究比较了经典编码器和新一代LLM在仇恨言论检测上的表现，验证了更大规模的模型是否实际提升效果。", "motivation": "在线平台难以在不过度审查合法言论的情况下遏制仇恨言论，需要验证更大规模的LLM是否真正提升检测能力。", "method": "通过基准测试比较经典双向Transformer编码器和超大型自回归LLM在仇恨言论检测任务上的表现。", "result": "研究结果揭示了LLM在实际仇恨言论检测中的效果，但具体表现未明确。", "conclusion": "研究为验证LLM在仇恨言论检测中的实用性提供了实证基础。"}}
{"id": "2507.09573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09573", "abs": "https://arxiv.org/abs/2507.09573", "authors": ["Zhe Wang", "Jingbo Zhang", "Tianyi Wei", "Wanchao Su", "Can Wang"], "title": "WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending", "comment": "14 pages, 16 figures", "summary": "Artistic typography aims to stylize input characters with visual effects that\nare both creative and legible. Traditional approaches rely heavily on manual\ndesign, while recent generative models, particularly diffusion-based methods,\nhave enabled automated character stylization. However, existing solutions\nremain limited in interactivity, lacking support for localized edits, iterative\nrefinement, multi-character composition, and open-ended prompt interpretation.\nWe introduce WordCraft, an interactive artistic typography system that\nintegrates diffusion models to address these limitations. WordCraft features a\ntraining-free regional attention mechanism for precise, multi-region generation\nand a noise blending that supports continuous refinement without compromising\nvisual quality. To support flexible, intent-driven generation, we incorporate a\nlarge language model to parse and structure both concrete and abstract user\nprompts. These components allow our framework to synthesize high-quality,\nstylized typography across single- and multi-character inputs across multiple\nlanguages, supporting diverse user-centered workflows. Our system significantly\nenhances interactivity in artistic typography synthesis, opening up creative\npossibilities for artists and designers.", "AI": {"tldr": "WordCraft是一个交互式艺术字体系统，利用扩散模型支持局部编辑、迭代优化和多字符组合，结合大语言模型解析用户提示，提升艺术字体的交互性和创造力。", "motivation": "传统艺术字体设计依赖手工，现有生成模型缺乏交互性，无法满足局部编辑、多字符组合等需求。", "method": "WordCraft采用无训练的区域注意力机制和噪声混合技术，结合大语言模型解析用户提示。", "result": "系统能生成高质量、多样化的艺术字体，支持多语言和多字符输入，显著提升交互性。", "conclusion": "WordCraft为艺术家和设计师提供了更灵活、高效的创作工具，拓展了艺术字体的可能性。"}}
{"id": "2507.09950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09950", "abs": "https://arxiv.org/abs/2507.09950", "authors": ["Shubham Shukla", "Kunal Sonalkar"], "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis", "comment": "11 pages, 2 figures", "summary": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction.", "AI": {"tldr": "论文评估了GPT-4o-mini和Gemini 2.0 Flash在细粒度时尚属性识别中的零样本性能，发现Gemini 2.0 Flash表现更优，并探讨了其在电商产品属性标注中的应用潜力。", "motivation": "时尚零售业务依赖产品属性的理解，而现有大语言模型在细粒度时尚属性识别中的表现尚未充分探索。", "method": "使用DeepFashion-MultiModal数据集，以图像为唯一输入，评估GPT-4o-mini和Gemini 2.0 Flash在18类时尚属性上的表现。", "result": "Gemini 2.0 Flash的宏F1得分为56.79%，优于GPT-4o-mini的43.28%。", "conclusion": "Gemini 2.0 Flash在零样本任务中表现更优，但需领域微调；为时尚AI和多模态属性提取研究奠定基础。"}}
{"id": "2507.10472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10472", "abs": "https://arxiv.org/abs/2507.10472", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.", "AI": {"tldr": "MLAR是一种基于RPA框架的新型ATS系统，利用LLM技术优化招聘流程，显著提高简历处理效率。", "motivation": "传统招聘流程在简历筛选和候选人短名单上存在时间和资源瓶颈，MLAR旨在解决这些问题。", "method": "MLAR采用三层LLM技术：提取职位关键特征、解析简历信息、进行相似度匹配，并通过语义算法高效匹配候选人。", "result": "MLAR在2400份简历处理中，平均每份耗时5.4秒，比Automation Anywhere和UiPath分别快16.9%和17.1%。", "conclusion": "MLAR为现代招聘需求提供了高效、准确且可扩展的解决方案，具有显著潜力。"}}
{"id": "2507.09577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09577", "abs": "https://arxiv.org/abs/2507.09577", "authors": ["Ming Yin", "Fu Wang", "Xujiong Ye", "Yanda Meng", "Zeyu Fu"], "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation", "comment": null, "summary": "Surgical video segmentation is a critical task in computer-assisted surgery,\nessential for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has demonstrated remarkable\nadvancements in both image and video segmentation. However, the inherent\nlimitations of SAM2's greedy selection memory design are amplified by the\nunique properties of surgical videos-rapid instrument movement, frequent\nocclusion, and complex instrument-tissue interaction-resulting in diminished\nperformance in the segmentation of complex, long videos. To address these\nchallenges, we introduce Memory Augmented (MA)-SAM2, a training-free video\nobject segmentation strategy, featuring novel context-aware and\nocclusion-resilient memory models. MA-SAM2 exhibits strong robustness against\nocclusions and interactions arising from complex instrument movements while\nmaintaining accuracy in segmenting objects throughout videos. Employing a\nmulti-target, single-loop, one-prompt inference further enhances the efficiency\nof the tracking process in multi-instrument videos. Without introducing any\nadditional parameters or requiring further training, MA-SAM2 achieved\nperformance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and\nEndoVis2018 datasets, respectively, demonstrating its potential for practical\nsurgical applications.", "AI": {"tldr": "MA-SAM2是一种无需训练的视频对象分割策略，通过上下文感知和遮挡弹性记忆模型，提升了在复杂手术视频中的分割性能。", "motivation": "解决SAM2在手术视频中因快速器械移动、频繁遮挡和复杂交互导致的性能下降问题。", "method": "引入上下文感知和遮挡弹性记忆模型，采用多目标、单循环、单提示推理。", "result": "在EndoVis2017和EndoVis2018数据集上分别比SAM2提升4.36%和6.1%。", "conclusion": "MA-SAM2在无需额外参数或训练的情况下，显著提升了手术视频分割的鲁棒性和准确性。"}}
{"id": "2507.10015", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10015", "abs": "https://arxiv.org/abs/2507.10015", "authors": ["Jaisidh Singh", "Diganta Misra", "Boris Knyazev", "Antonio Orvieto"], "title": "(Almost) Free Modality Stitching of Foundation Models", "comment": "Pre-print", "summary": "Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\nautoregressive text model. This stitching process is performed by training a\nconnector module that aims to align the representation-representation or\nrepresentation-input spaces of these uni-modal models. However, given the\ncomplexity of training such connectors on large scale web-based datasets\ncoupled with the ever-increasing number of available pretrained uni-modal\nmodels, the task of uni-modal models selection and subsequent connector module\ntraining becomes computationally demanding. To address this under-studied\ncritical problem, we propose Hypernetwork Model Alignment (Hyma), a novel\nall-in-one solution for optimal uni-modal model selection and connector\ntraining by leveraging hypernetworks. Specifically, our framework utilizes the\nparameter prediction capability of a hypernetwork to obtain jointly trained\nconnector modules for $N \\times M$ combinations of uni-modal models. In our\nexperiments, Hyma reduces the optimal uni-modal model pair search cost by\n$10\\times$ (averaged across all experiments), while matching the ranking and\ntrained connector performance obtained via grid search across a suite of\ndiverse multi-modal benchmarks.", "AI": {"tldr": "Hyma提出了一种基于超网络的多模态模型对齐方法，显著降低了最优单模态模型选择和连接器训练的计算成本。", "motivation": "多模态基础模型通常通过拼接多个预训练单模态模型实现，但选择和训练连接器模块的计算成本高昂。", "method": "利用超网络的参数预测能力，为N×M种单模态模型组合联合训练连接器模块。", "result": "实验表明，Hyma将最优单模态模型对的搜索成本降低10倍，同时性能与网格搜索相当。", "conclusion": "Hyma是一种高效的多模态模型对齐解决方案，显著提升了计算效率。"}}
{"id": "2507.10475", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.10475", "abs": "https://arxiv.org/abs/2507.10475", "authors": ["İsmail Tarım", "Aytuğ Onan"], "title": "Can You Detect the Difference?", "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.", "AI": {"tldr": "论文比较了扩散生成文本（LLaDA）和自回归生成文本（LLaMA）的检测效果，发现LLaDA更接近人类文本，导致现有检测器失效，需开发针对扩散模型的检测方法。", "motivation": "大型语言模型（LLMs）的快速发展引发了对AI生成文本检测的担忧，现有方法对扩散模型的效果未知。", "method": "使用2000个样本，比较LLaDA和LLaMA在困惑度、突发性、词汇多样性、可读性及BLEU/ROUGE分数上的表现。", "result": "LLaDA在困惑度和突发性上接近人类文本，导致检测器高假阴性率；LLaMA困惑度低但词汇保真度差。单一指标无法区分扩散输出与人类写作。", "conclusion": "需开发针对扩散模型的检测方法，如混合模型、扩散特定特征或鲁棒水印。"}}
{"id": "2507.09595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09595", "abs": "https://arxiv.org/abs/2507.09595", "authors": ["Or Greenberg"], "title": "Demystifying Flux Architecture", "comment": null, "summary": "FLUX.1 is a diffusion-based text-to-image generation model developed by Black\nForest Labs, designed to achieve faithful text-image alignment while\nmaintaining high image quality and diversity. FLUX is considered\nstate-of-the-art in text-to-image generation, outperforming popular models such\nas Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly\navailable as open source, the authors have not released official technical\ndocumentation detailing the model's architecture or training setup. This report\nsummarizes an extensive reverse-engineering effort aimed at demystifying FLUX's\narchitecture directly from its source code, to support its adoption as a\nbackbone for future research and development. This document is an unofficial\ntechnical report and is not published or endorsed by the original developers or\ntheir affiliated institutions.", "AI": {"tldr": "FLUX.1是一种基于扩散的文本到图像生成模型，性能优于主流模型，但缺乏官方技术文档。本报告通过逆向工程解析其架构。", "motivation": "FLUX.1在文本到图像生成领域表现优异，但缺乏公开的技术细节，阻碍了其进一步研究和应用。", "method": "通过逆向工程分析FLUX.1的源代码，解析其架构和训练设置。", "result": "报告提供了FLUX.1的非官方技术细节，支持其作为未来研究的骨干模型。", "conclusion": "FLUX.1的逆向工程解析为研究和开发提供了基础，但需注意其非官方性质。"}}
{"id": "2507.10056", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10056", "abs": "https://arxiv.org/abs/2507.10056", "authors": ["A. K. M. Shoriful Islam", "Md. Rakib Hassan", "Macbah Uddin", "Md. Shahidur Rahman"], "title": "Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning", "comment": null, "summary": "Poultry farming is a vital component of the global food supply chain, yet it\nremains highly vulnerable to infectious diseases such as coccidiosis,\nsalmonellosis, and Newcastle disease. This study proposes a lightweight machine\nlearning-based approach to detect these diseases by analyzing poultry fecal\nimages. We utilize multi-color space feature extraction (RGB, HSV, LAB) and\nexplore a wide range of color, texture, and shape-based descriptors, including\ncolor histograms, local binary patterns (LBP), wavelet transforms, and edge\ndetectors. Through a systematic ablation study and dimensionality reduction\nusing PCA and XGBoost feature selection, we identify a compact global feature\nset that balances accuracy and computational efficiency. An artificial neural\nnetwork (ANN) classifier trained on these features achieved 95.85% accuracy\nwhile requiring no GPU and only 638 seconds of execution time in Google Colab.\nCompared to deep learning models such as Xception and MobileNetV3, our proposed\nmodel offers comparable accuracy with drastically lower resource usage. This\nwork demonstrates a cost-effective, interpretable, and scalable alternative to\ndeep learning for real-time poultry disease detection in low-resource\nagricultural settings.", "AI": {"tldr": "提出了一种基于轻量级机器学习的家禽疾病检测方法，通过分析粪便图像实现高效、低成本诊断。", "motivation": "家禽养殖易受传染病影响，传统方法资源消耗大，需一种高效、低成本的解决方案。", "method": "采用多颜色空间特征提取（RGB、HSV、LAB）和多种描述符（颜色直方图、LBP、小波变换等），结合PCA和XGBoost降维，训练ANN分类器。", "result": "模型准确率达95.85%，无需GPU，执行时间仅638秒，资源消耗显著低于深度学习模型。", "conclusion": "该方法为低资源农业环境提供了一种高效、可解释且可扩展的疾病检测方案。"}}
{"id": "2507.10524", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10524", "abs": "https://arxiv.org/abs/2507.10524", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.", "AI": {"tldr": "Mixture-of-Recursions (MoR) 是一种结合参数共享和自适应计算的高效框架，通过递归Transformer实现，显著降低计算和内存需求。", "motivation": "解决语言模型扩展时计算和内存需求高的问题，同时实现参数共享和自适应计算。", "method": "MoR 通过共享层和轻量级路由器动态分配递归深度，选择性缓存键值对，并提出KV共享变体以减少预填充延迟。", "result": "在135M到1.7B参数范围内，MoR在相同训练FLOPs下显著降低验证困惑度，提高少样本准确率，并提升吞吐量。", "conclusion": "MoR 是实现高质量大模型而不增加成本的有效路径。"}}
{"id": "2507.09612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09612", "abs": "https://arxiv.org/abs/2507.09612", "authors": ["You Huang", "Lichao Chen", "Jiayi Ji", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive", "comment": "Accepted by ICCV 2025", "summary": "Interactive segmentation (IS) improves annotation efficiency by segmenting\ntarget regions from user prompts, with widespread applications in real-world\nscenarios. Current approaches face a critical trade-off: dense-token methods\nachieve superior accuracy and detail preservation but suffer from prohibitively\nslow processing on CPU devices, while the Segment Anything Model (SAM) advances\nthe field with sparse prompt tokens for fast inference but compromises\nsegmentation quality. In this paper, we propose Inter2Former to address this\nchallenge by optimizing computation allocation in dense-token processing, which\nintroduces four key enhancements. First, we propose Dynamic Prompt Embedding\n(DPE) that adaptively processes only regions of interest while avoiding\nadditional overhead from background tokens. Second, we introduce Dynamic Hybrid\nAttention (DHA), which leverages previous segmentation masks to route tokens\nthrough either full attention (O(N2)) for boundary regions or our proposed\nefficient BSQ attention (O(N)) for non-boundary regions. Third, we develop\nHybrid Mixture of Experts (HMoE), which applies similar adaptive computation\nstrategies in FFN modules with CPU-optimized parallel processing. Finally, we\npresent Dynamic Local Upsampling (DLU), a reverse operation of DPE, which\nlocalizes objects with a lightweight MLP and performs fine-grained upsampling\nonly in detected regions. Experimental results on high-precision IS benchmarks\ndemonstrate that Inter2Former achieves SOTA performance with high efficiency on\nCPU devices.", "AI": {"tldr": "Inter2Former通过优化密集令牌处理的计算分配，提出四种关键改进，实现了在CPU设备上的高效高精度交互式分割。", "motivation": "当前密集令牌方法精度高但速度慢，SAM模型速度快但精度不足，Inter2Former旨在解决这一权衡问题。", "method": "提出动态提示嵌入（DPE）、动态混合注意力（DHA）、混合专家（HMoE）和动态局部上采样（DLU）四种改进。", "result": "在高精度交互式分割基准测试中，Inter2Former实现了SOTA性能，同时在CPU设备上保持高效。", "conclusion": "Inter2Former通过计算优化，成功平衡了交互式分割的精度与效率，适用于实际应用场景。"}}
{"id": "2507.10127", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10127", "abs": "https://arxiv.org/abs/2507.10127", "authors": ["Md Abulkalam Azad", "John Nyberg", "Håvard Dalen", "Bjørnar Grenne", "Lasse Lovstakken", "Andreas Østvik"], "title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion", "comment": "Accepted to CVAMD workshop at ICCV 2025", "summary": "Accurate motion estimation for tracking deformable tissues in\nechocardiography is essential for precise cardiac function measurements. While\ntraditional methods like block matching or optical flow struggle with intricate\ncardiac motion, modern point tracking approaches remain largely underexplored\nin this domain. This work investigates the potential of state-of-the-art (SOTA)\npoint tracking methods for ultrasound, with a focus on echocardiography.\nAlthough these novel approaches demonstrate strong performance in general\nvideos, their effectiveness and generalizability in echocardiography remain\nlimited. By analyzing cardiac motion throughout the heart cycle in real B-mode\nultrasound videos, we identify that a directional motion bias across different\nviews is affecting the existing training strategies. To mitigate this, we\nrefine the training procedure and incorporate a set of tailored augmentations\nto reduce the bias and enhance tracking robustness and generalization through\nimpartial cardiac motion. We also propose a lightweight network leveraging\nmulti-scale cost volumes from spatial context alone to challenge the advanced\nspatiotemporal point tracking models. Experiments demonstrate that fine-tuning\nwith our strategies significantly improves models' performances over their\nbaselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker\nboosts overall position accuracy by 60.7% and reduces median trajectory error\nby 61.5% across heart cycle phases. Interestingly, several point tracking\nmodels fail to outperform our proposed simple model in terms of tracking\naccuracy and generalization, reflecting their limitations when applied to\nechocardiography. Nevertheless, clinical evaluation reveals that these methods\nimprove GLS measurements, aligning more closely with expert-validated,\nsemi-automated tools and thus demonstrating better reproducibility in\nreal-world applications.", "AI": {"tldr": "该论文探讨了在超声心动图中应用先进的点跟踪方法进行运动估计的潜力，通过改进训练策略和提出轻量级网络，显著提升了跟踪精度和泛化能力。", "motivation": "传统方法在复杂心脏运动估计中表现不佳，而现代点跟踪方法在超声心动图领域的应用尚未充分探索。", "method": "分析了心脏运动的方向性偏差，改进了训练策略并引入定制化增强方法，提出了一种轻量级网络，利用多尺度成本体积提升性能。", "result": "实验表明，改进后的方法在位置精度和轨迹误差上分别提升了60.7%和61.5%，并在临床评估中表现出更好的可重复性。", "conclusion": "研究展示了改进的点跟踪方法在超声心动图中的潜力，尤其是在提升测量精度和泛化能力方面。"}}
{"id": "2507.10535", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10535", "abs": "https://arxiv.org/abs/2507.10535", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.", "AI": {"tldr": "论文介绍了CodeJudgeBench，一个评估LLM作为代码任务裁判的基准，发现思考型模型表现更优，但存在判断随机性和顺序敏感性问题。", "motivation": "探索LLM作为裁判在代码任务中的有效性，填补缺乏专用基准的空白。", "method": "引入CodeJudgeBench基准，评估26个LLM模型在代码生成、修复和单元测试生成任务中的表现。", "result": "思考型模型优于非思考型模型，但所有模型存在判断随机性和顺序敏感性。", "conclusion": "LLM作为裁判在代码任务中表现不稳定，需优化提示策略以提高可靠性。"}}
{"id": "2507.09615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09615", "abs": "https://arxiv.org/abs/2507.09615", "authors": ["Eman Ali", "Sathira Silva", "Chetan Arora", "Muhammad Haris Khan"], "title": "Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score", "comment": null, "summary": "Vision-language models (VLMs) like CLIP excel in zero-shot learning by\naligning image and text representations through contrastive pretraining.\nExisting approaches to unsupervised adaptation (UA) for fine-grained\nclassification with VLMs either rely on fixed alignment scores that cannot\ncapture evolving, subtle class distinctions or use computationally expensive\npseudo-labeling strategies that limit scalability. In contrast, we show that\nmodeling fine-grained cross-modal interactions during adaptation produces more\naccurate, class-discriminative pseudo-labels and substantially improves\nperformance over state-of-the-art (SOTA) methods. We introduce Fine-grained\nAlignment and Interaction Refinement (FAIR), an innovative approach that\ndynamically aligns localized image features with descriptive language\nembeddings through a set of Class Description Anchors (CDA). This enables the\ndefinition of a Learned Alignment Score (LAS), which incorporates CDA as an\nadaptive classifier, facilitating cross-modal interactions to improve\nself-training in unsupervised adaptation. Furthermore, we propose a\nself-training weighting mechanism designed to refine pseudo-labels in the\npresence of inter-class ambiguities. Our approach, FAIR, delivers a substantial\nperformance boost in fine-grained unsupervised adaptation, achieving a notable\noverall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.", "AI": {"tldr": "FAIR方法通过动态对齐图像和文本特征，改进了无监督适应中的伪标签生成，显著提升了细粒度分类性能。", "motivation": "现有方法在无监督适应中无法捕捉细粒度类别差异或计算成本高，FAIR旨在解决这些问题。", "method": "提出FAIR方法，通过Class Description Anchors动态对齐特征，并使用Learned Alignment Score改进伪标签生成。", "result": "在13个细粒度数据集上，FAIR比现有方法平均提升2.78%。", "conclusion": "FAIR通过动态对齐和交互优化，显著提升了细粒度无监督适应的性能。"}}
{"id": "2507.10202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10202", "abs": "https://arxiv.org/abs/2507.10202", "authors": ["Jaeseong Lee", "Yeeun Choi", "Heechan Choi", "Hanjung Kim", "Seonjoo Kim"], "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images", "comment": "Accepted at CVPR 2025 Workshop on Emergent Visual Abilities and\n  Limits of Foundation Models", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language understanding, reasoning, and generation.\nHowever, they struggle with tasks requiring fine-grained localization and\nreasoning in high-resolution images. This constraint stems from the fact that\nMLLMs are fine-tuned with fixed image resolution to align with the pre-trained\nimage encoder used in MLLM. Consequently, feeding high-resolution images\ndirectly into MLLMs leads to poor generalization due to a train-test resolution\ndiscrepancy, while downsampling these images-although ensuring\nconsistency-compromises fine-grained visual details and ultimately degrades\nperformance. To address this challenge, we propose Extract Candidate then\nPredict (ECP), a novel training-free, task-agnostic two-stage framework\ndesigned to enhance MLLM performance on high-resolution images. The key\nintuition behind ECP is that while MLLMs struggle with high-resolution images,\ntheir predictions on downsampled images still contain implicit localization\ncues. By first identifying candidate region using the coarse prediction and\nthen predicting the final output based on candidate region, ECP effectively\npreserves fine-grained details while mitigating the challenges posed by\nhigh-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K\nMLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared\nto baseline respectively, demonstrating its effectiveness. Code is available at\nhttps://github.com/yenncye/ECP.", "AI": {"tldr": "ECP是一个无需训练、任务无关的两阶段框架，旨在提升多模态大语言模型（MLLMs）在高分辨率图像上的性能，通过提取候选区域并预测最终输出。", "motivation": "MLLMs在高分辨率图像上的细粒度定位和推理能力不足，原因是训练和测试分辨率不一致。", "method": "提出ECP框架，先通过低分辨率预测提取候选区域，再基于候选区域进行最终预测。", "result": "在4K GUI grounding和4K、8K MLLM感知任务上，分别实现了21.3%、5.8%、5.2%的绝对性能提升。", "conclusion": "ECP有效解决了MLLMs在高分辨率图像上的性能问题，同时保留了细粒度视觉细节。"}}
{"id": "2507.10541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10541", "abs": "https://arxiv.org/abs/2507.10541", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.", "AI": {"tldr": "REST框架通过同时测试多个问题，评估大型推理模型在压力下的表现，揭示现有基准测试的局限性。", "motivation": "现有评估方法局限于单问题测试，无法反映真实世界的多上下文压力需求。", "method": "提出REST框架，同时暴露模型于多个问题，评估其上下文优先级分配、跨问题干扰抵抗和动态认知负载管理能力。", "result": "即使SOTA模型在压力测试下表现显著下降，REST显示出更强的区分能力。", "conclusion": "REST是一种高效、未来兼容的评估范式，减少对人类标注的依赖，更贴近实际需求。"}}
{"id": "2507.09619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09619", "abs": "https://arxiv.org/abs/2507.09619", "authors": ["Yilin Lu", "Jianghang Lin", "Linhuang Xie", "Kai Zhao", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection", "comment": null, "summary": "Anomaly inspection plays a vital role in industrial manufacturing, but the\nscarcity of anomaly samples significantly limits the effectiveness of existing\nmethods in tasks such as localization and classification. While several anomaly\nsynthesis approaches have been introduced for data augmentation, they often\nstruggle with low realism, inaccurate mask alignment, and poor generalization.\nTo overcome these limitations, we propose Generate Aligned Anomaly (GAA), a\nregion-guided, few-shot anomaly image-mask pair generation framework. GAA\nleverages the strong priors of a pretrained latent diffusion model to generate\nrealistic, diverse, and semantically aligned anomalies using only a small\nnumber of samples. The framework first employs Localized Concept Decomposition\nto jointly model the semantic features and spatial information of anomalies,\nenabling flexible control over the type and location of anomalies. It then\nutilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained\nsemantic clustering of anomaly concepts, thereby enhancing the consistency of\nanomaly representations. Subsequently, a region-guided mask generation strategy\nensures precise alignment between anomalies and their corresponding masks,\nwhile a low-quality sample filtering module is introduced to further improve\nthe overall quality of the generated samples. Extensive experiments on the\nMVTec AD and LOCO datasets demonstrate that GAA achieves superior performance\nin both anomaly synthesis quality and downstream tasks such as localization and\nclassification.", "AI": {"tldr": "GAA是一个基于预训练潜在扩散模型的少样本异常图像-掩膜对生成框架，通过区域引导和语义对齐生成高质量异常样本，解决了现有方法在真实感、掩膜对齐和泛化能力上的不足。", "motivation": "工业制造中异常样本稀缺限制了异常检测方法的性能，现有异常合成方法存在真实感低、掩膜对齐不准确和泛化能力差的问题。", "method": "GAA利用预训练潜在扩散模型的强先验，通过局部概念分解联合建模异常语义特征和空间信息，结合自适应多轮异常聚类和区域引导掩膜生成策略，确保异常与掩膜精确对齐。", "result": "在MVTec AD和LOCO数据集上的实验表明，GAA在异常合成质量和下游任务（如定位和分类）中表现优异。", "conclusion": "GAA通过少样本生成高质量异常样本，显著提升了异常检测任务的性能。"}}
{"id": "2507.10223", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10223", "abs": "https://arxiv.org/abs/2507.10223", "authors": ["Xiangyu Yin", "Boyuan Yang", "Weichen Liu", "Qiyao Xue", "Abrar Alamri", "Goeran Fiedler", "Wei Gao"], "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users", "comment": "Accepted by ICCV'25", "summary": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing\nindividuals with lower-limb amputations the ability to regain mobility and\nimprove their quality of life. Gait analysis is fundamental for optimizing\nprosthesis design and alignment, directly impacting the mobility and life\nquality of individuals with lower-limb amputations. Vision-based machine\nlearning (ML) methods offer a scalable and non-invasive solution to gait\nanalysis, but face challenges in correctly detecting and analyzing prosthesis,\ndue to their unique appearances and new movement patterns. In this paper, we\naim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,\nto support multiple vision tasks including Video Object Segmentation, 2D Human\nPose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from\nfour above-knee amputees when testing multiple newly-fitted prosthetic legs\nthrough walking trials, and depicts the presence, contours, poses, and gait\npatterns of human subjects with transfemoral prosthetic legs. Alongside the\ndataset itself, we also present benchmark tasks and fine-tuned baseline models\nto illustrate the practical application and performance of the ProGait dataset.\nWe compared our baseline models against pre-trained vision models,\ndemonstrating improved generalizability when applying the ProGait dataset for\nprosthesis-specific tasks. Our code is available at\nhttps://github.com/pittisl/ProGait and dataset at\nhttps://huggingface.co/datasets/ericyxy98/ProGait.", "AI": {"tldr": "论文介绍了ProGait数据集，用于支持基于视觉的机器学习方法在假肢步态分析中的应用，并提供了基准任务和基线模型。", "motivation": "假肢步态分析对优化假肢设计和改善患者生活质量至关重要，但现有视觉方法在检测和分析假肢时面临挑战。", "method": "提出ProGait数据集，包含412个视频片段，涵盖假肢的轮廓、姿态和步态模式，并提供了基准任务和微调基线模型。", "result": "基线模型在假肢特定任务中表现出更好的泛化能力。", "conclusion": "ProGait数据集为假肢步态分析提供了实用工具，并展示了其在视觉任务中的潜力。"}}
{"id": "2507.10013", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10013", "abs": "https://arxiv.org/abs/2507.10013", "authors": ["Tom Kouwenhoven", "Kiana Shahrasbi", "Tessa Verhoef"], "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect", "comment": null, "summary": "Recent advances in multimodal models have raised questions about whether\nvision-and-language models (VLMs) integrate cross-modal information in ways\nthat reflect human cognition. One well-studied test case in this domain is the\nbouba-kiki effect, where humans reliably associate pseudowords like \"bouba\"\nwith round shapes and \"kiki\" with jagged ones. Given the mixed evidence found\nin prior studies for this effect in VLMs, we present a comprehensive\nre-evaluation focused on two variants of CLIP, ResNet and Vision Transformer\n(ViT), given their centrality in many state-of-the-art VLMs. We apply two\ncomplementary methods closely modelled after human experiments: a prompt-based\nevaluation that uses probabilities as model preference, and we use Grad-CAM as\na novel way to interpret visual attention in shape-word matching tasks. Our\nfindings show that these models do not consistently exhibit the bouba-kiki\neffect. While ResNet shows a preference for round shapes, overall performance\nacross both models lacks the expected associations. Moreover, direct comparison\nwith prior human data on the same task shows that the models' responses fall\nmarkedly short of the robust, modality-integrated behaviour characteristic of\nhuman cognition. These results contribute to the ongoing debate about the\nextent to which VLMs truly understand cross-modal concepts, highlighting\nlimitations in their internal representations and alignment with human\nintuitions.", "AI": {"tldr": "研究重新评估了两种CLIP变体（ResNet和ViT）是否表现出bouba-kiki效应，发现模型表现不一致且与人类认知差异显著。", "motivation": "探讨视觉-语言模型（VLMs）是否以类似人类认知的方式整合跨模态信息，特别是bouba-kiki效应。", "method": "使用基于提示的概率评估和Grad-CAM视觉注意力分析，对比人类实验数据。", "result": "模型未一致表现bouba-kiki效应，ResNet偏好圆形但整体表现与人类认知不符。", "conclusion": "VLMs在跨模态概念理解上存在局限性，与人类直觉不一致。"}}
{"id": "2507.09640", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09640", "abs": "https://arxiv.org/abs/2507.09640", "authors": ["Leonor Fernandes", "Tiago Gonçalves", "João Matos", "Luis Filipe Nakayama", "Jaime S. Cardoso"], "title": "Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams", "comment": "10 pages. Under review", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss in working-age\nadults. While screening reduces the risk of blindness, traditional imaging is\noften costly and inaccessible. Artificial intelligence (AI) algorithms present\na scalable diagnostic solution, but concerns regarding fairness and\ngeneralization persist. This work evaluates the fairness and performance of\nimage-trained models in DR prediction, as well as the impact of disentanglement\nas a bias mitigation technique, using the diverse mBRSET fundus dataset. Three\nmodels, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to\npredict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness\nwas assessed between subgroups of SAs, and disentanglement was applied to\nreduce bias. All models achieved high DR prediction performance in diagnosing\n(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%\nAUROC, respectively). Fairness assessment suggests disparities, such as a 10%\nAUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction\nhad varying results, depending on the model selected. Disentanglement improved\nDINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2\nand Swin V2 (7% and 3%, respectively). These findings highlight the complexity\nof disentangling fine-grained features in fundus imaging and emphasize the\nimportance of fairness in medical imaging AI to ensure equitable and reliable\nhealthcare solutions.", "AI": {"tldr": "研究评估了AI模型在糖尿病视网膜病变（DR）预测中的公平性和性能，探讨了解缠技术对减少偏见的有效性。", "motivation": "糖尿病视网膜病变是导致工作年龄成年人视力丧失的主要原因，传统筛查方法成本高且难以普及，AI算法提供了可扩展的解决方案，但公平性和泛化性仍是问题。", "method": "使用mBRSET眼底数据集，训练了ConvNeXt V2、DINOv2和Swin V2三种模型，预测DR及敏感属性（如年龄和性别），并评估公平性和解缠技术效果。", "result": "所有模型在DR预测中表现优异（最高94% AUROC），但公平性评估显示存在差异（如DINOv2中年龄组间10% AUROC差距）。解缠技术对不同模型效果不一。", "conclusion": "研究强调了医学影像AI中公平性的重要性，解缠技术效果因模型而异，需进一步优化以实现公平可靠的医疗解决方案。"}}
{"id": "2507.10300", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10300", "abs": "https://arxiv.org/abs/2507.10300", "authors": ["Hatef Otroshi Shahreza", "Sébastien Marcel"], "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding", "comment": "Accepted in ICCV 2025 workshops", "summary": "Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.", "AI": {"tldr": "FaceLLM是一种针对面部图像理解的多模态大语言模型，通过ChatGPT生成高质量问答对训练数据，显著提升了面部相关任务的性能。", "motivation": "现有MLLMs在通用数据集上训练，缺乏对领域特定视觉线索（如面部图像）的推理能力，尤其是面部结构、表情、情感等细节理解任务。", "method": "提出弱监督流程，利用ChatGPT生成基于FairFace数据集的问答对（FairFaceGPT），训练FaceLLM模型。", "result": "FaceLLM在多种面部中心任务中表现优异，达到SOTA性能。", "conclusion": "FaceLLM展示了语言模型合成监督在构建领域专用MLLMs中的潜力，为可信赖、以人为本的多模态AI系统树立了先例。"}}
{"id": "2507.10398", "categories": ["cs.CV", "cs.AI", "cs.CL", "14J60", "I.2.7; I.4; I.5; I.7.5"], "pdf": "https://arxiv.org/pdf/2507.10398", "abs": "https://arxiv.org/abs/2507.10398", "authors": ["Diksha Mehta", "Prateek Mehta"], "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network", "comment": "9 pages, 6 figures", "summary": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.", "AI": {"tldr": "本文提出了一种基于深度卷积神经网络的德瓦纳加里手写字符识别方法，实现了高准确率。", "motivation": "德瓦纳加里文字缺乏数字化工具，研究旨在通过自动化方法提高识别效率。", "method": "使用两层深度卷积神经网络，基于德瓦纳加里手写字符数据集（DHCD）进行训练和测试。", "result": "测试准确率为96.36%，训练准确率为99.55%。", "conclusion": "该方法在德瓦纳加里手写文本识别中表现出色，具有实际应用潜力。"}}
{"id": "2507.09649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09649", "abs": "https://arxiv.org/abs/2507.09649", "authors": ["Zhengyuan Peng", "Jianqing Xu", "Shen Li", "Jiazhen Ji", "Yuge Huang", "Jingyun Zhang", "Jinmin Li", "Shouhong Ding", "Rizen Guo", "Xin Tan", "Lizhuang Ma"], "title": "EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR", "comment": "Accepted to IJCAI", "summary": "Human-machine interaction through augmented reality (AR) and virtual reality\n(VR) is increasingly prevalent, requiring accurate and efficient gaze\nestimation which hinges on the accuracy of eye segmentation to enable smooth\nuser experiences. We introduce EyeSeg, a novel eye segmentation framework\ndesigned to overcome key challenges that existing approaches struggle with:\nmotion blur, eyelid occlusion, and train-test domain gaps. In these situations,\nexisting models struggle to extract robust features, leading to suboptimal\nperformance. Noting that these challenges can be generally quantified by\nuncertainty, we design EyeSeg as an uncertainty-aware eye segmentation\nframework for AR/VR wherein we explicitly model the uncertainties by performing\nBayesian uncertainty learning of a posterior under the closed set prior.\nTheoretically, we prove that a statistic of the learned posterior indicates\nsegmentation uncertainty levels and empirically outperforms existing methods in\ndownstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score\nand the segmentation result, weighting and fusing multiple gaze estimates for\nrobustness, which proves to be effective especially under motion blur, eyelid\nocclusion and cross-domain challenges. Moreover, empirical results suggest that\nEyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing\nprevious approaches. The code is publicly available at\nhttps://github.com/JethroPeng/EyeSeg.", "AI": {"tldr": "EyeSeg是一种不确定性感知的眼部分割框架，用于AR/VR中的眼部分割，通过贝叶斯不确定性学习建模后验分布，显著提升了在运动模糊、眼睑遮挡和跨域挑战下的性能。", "motivation": "现有眼部分割方法在运动模糊、眼睑遮挡和训练-测试域差异等挑战下表现不佳，需要一种更鲁棒的解决方案。", "method": "设计了一个不确定性感知框架EyeSeg，通过贝叶斯不确定性学习建模后验分布，输出分割结果和不确定性分数，并加权融合多个注视估计以提高鲁棒性。", "result": "在MIoU、E1、F1和ACC等指标上优于现有方法，尤其在运动模糊、眼睑遮挡和跨域挑战下表现突出。", "conclusion": "EyeSeg通过不确定性建模显著提升了眼部分割的鲁棒性和准确性，适用于AR/VR中的眼部分割任务。"}}
{"id": "2507.10449", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10449", "abs": "https://arxiv.org/abs/2507.10449", "authors": ["Hongyong Han", "Wei Wang", "Gaowei Zhang", "Mingjie Li", "Yi Wang"], "title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding", "comment": null, "summary": "Coral reefs are vital yet vulnerable ecosystems that require continuous\nmonitoring to support conservation. While coral reef images provide essential\ninformation in coral monitoring, interpreting such images remains challenging\ndue to the need for domain expertise. Visual Question Answering (VQA), powered\nby Large Vision-Language Models (LVLMs), has great potential in user-friendly\ninteraction with coral reef images. However, applying VQA to coral imagery\ndemands a dedicated dataset that addresses two key challenges: domain-specific\nannotations and multidimensional questions. In this work, we introduce\nCoralVQA, the first large-scale VQA dataset for coral reef analysis. It\ncontains 12,805 real-world coral images from 67 coral genera collected from 3\noceans, along with 277,653 question-answer pairs that comprehensively assess\necological and health-related conditions. To construct this dataset, we develop\na semi-automatic data construction pipeline in collaboration with marine\nbiologists to ensure both scalability and professional-grade data quality.\nCoralVQA presents novel challenges and provides a comprehensive benchmark for\nstudying vision-language reasoning in the context of coral reef images. By\nevaluating several state-of-the-art LVLMs, we reveal key limitations and\nopportunities. These insights form a foundation for future LVLM development,\nwith a particular emphasis on supporting coral conservation efforts.", "AI": {"tldr": "CoralVQA是首个用于珊瑚礁分析的大规模视觉问答数据集，包含12,805张真实珊瑚图像和277,653个问答对，旨在解决珊瑚监测中的领域专业性和多维问题挑战。", "motivation": "珊瑚礁是重要但脆弱的生态系统，需要持续监测以支持保护。现有珊瑚图像解读依赖专业知识，而视觉问答（VQA）技术有望提供用户友好的交互方式。", "method": "开发了半自动数据构建流程，结合海洋生物学家确保数据质量和可扩展性，构建了包含多维问题的CoralVQA数据集。", "result": "评估多个先进的大视觉语言模型（LVLM），揭示了关键局限和机遇，为未来模型开发提供基础。", "conclusion": "CoralVQA为珊瑚礁图像中的视觉语言推理提供了全面基准，支持珊瑚保护工作。"}}
{"id": "2507.10403", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.10403", "abs": "https://arxiv.org/abs/2507.10403", "authors": ["Daniele Rege Cambrin", "Lorenzo Vaiani", "Giuseppe Gallipoli", "Luca Cagliero", "Paolo Garza"], "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "comment": null, "summary": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.", "AI": {"tldr": "论文提出CrisisLandMark数据集和CLOSP框架，通过文本对齐光学与SAR图像，提升检索性能54%，并整合地理坐标优化任务表现。", "motivation": "现有文本-图像检索系统多限于RGB数据，未能充分利用多传感器（如SAR和多光谱）的物理信息。", "method": "构建CrisisLandMark数据集，提出CLOSP框架，通过对比学习对齐光学与SAR图像嵌入空间，并引入GeoCLOSP整合地理坐标。", "result": "CLOSP提升检索性能54%，GeoCLOSP在特定地理任务中表现更优。", "conclusion": "多传感器数据与地理背景的整合对遥感档案的潜力释放至关重要。"}}
{"id": "2507.09672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09672", "abs": "https://arxiv.org/abs/2507.09672", "authors": ["Xinyu Zhang", "Zhonghao Ye", "Jingwei Zhang", "Xiang Tian", "Zhisheng Liang", "Shipeng Yu"], "title": "VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation", "comment": "8 pages, 7 figures, 8 tables. WiFi CSI, VST-Pose framework +\n  ViSTA-Former dual-stream attention backbone. Code:\n  https://github.com/CarmenQing/VST-Pose", "summary": "WiFi-based human pose estimation has emerged as a promising non-visual\nalternative approaches due to its pene-trability and privacy advantages. This\npaper presents VST-Pose, a novel deep learning framework for accurate and\ncontinuous pose estimation using WiFi channel state information. The proposed\nmethod introduces ViSTA-Former, a spatiotemporal attention backbone with\ndual-stream architecture that adopts a dual-stream architecture to separately\ncapture temporal dependencies and structural relationships among body joints.\nTo enhance sensitivity to subtle human motions, a velocity modeling branch is\nintegrated into the framework, which learns short-term keypoint dis-placement\npatterns and improves fine-grained motion representation. We construct a 2D\npose dataset specifically designed for smart home care scenarios and\ndemonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,\noutperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.\nFurther evaluation on the public MMFi dataset confirms the model's robustness\nand effectiveness in 3D pose estimation tasks. The proposed system provides a\nreliable and privacy-aware solution for continuous human motion analysis in\nindoor environments. Our codes are available in\nhttps://github.com/CarmenQing/VST-Pose.", "AI": {"tldr": "VST-Pose是一种基于WiFi信道状态信息的深度学习框架，用于准确连续的人体姿态估计，通过双流时空注意力架构和速度建模分支提升性能。", "motivation": "WiFi姿态估计因其穿透性和隐私优势成为非视觉替代方案，但现有方法在精度和连续性上仍有不足。", "method": "提出ViSTA-Former双流架构，分别捕捉时间依赖和关节结构关系，并集成速度建模分支增强细微动作敏感性。", "result": "在自建数据集上PCK@50达92.2%，优于现有方法8.3%；在公共MMFi数据集上验证了3D姿态估计的鲁棒性。", "conclusion": "VST-Pose为室内连续运动分析提供了可靠且隐私保护的解决方案。"}}
{"id": "2507.10492", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10492", "abs": "https://arxiv.org/abs/2507.10492", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Zhanli Hu", "Jing Qin"], "title": "BenchReAD: A systematic benchmark for retinal anomaly detection", "comment": "MICCAI 2025", "summary": "Retinal anomaly detection plays a pivotal role in screening ocular and\nsystemic diseases. Despite its significance, progress in the field has been\nhindered by the absence of a comprehensive and publicly available benchmark,\nwhich is essential for the fair evaluation and advancement of methodologies.\nDue to this limitation, previous anomaly detection work related to retinal\nimages has been constrained by (1) a limited and overly simplistic set of\nanomaly types, (2) test sets that are nearly saturated, and (3) a lack of\ngeneralization evaluation, resulting in less convincing experimental setups.\nFurthermore, existing benchmarks in medical anomaly detection predominantly\nfocus on one-class supervised approaches (training only with negative samples),\noverlooking the vast amounts of labeled abnormal data and unlabeled data that\nare commonly available in clinical practice. To bridge these gaps, we introduce\na benchmark for retinal anomaly detection, which is comprehensive and\nsystematic in terms of data and algorithm. Through categorizing and\nbenchmarking previous methods, we find that a fully supervised approach\nleveraging disentangled representations of abnormalities (DRA) achieves the\nbest performance but suffers from significant drops in performance when\nencountering certain unseen anomalies. Inspired by the memory bank mechanisms\nin one-class supervised learning, we propose NFM-DRA, which integrates DRA with\na Normal Feature Memory to mitigate the performance degradation, establishing a\nnew SOTA. The benchmark is publicly available at\nhttps://github.com/DopamineLcy/BenchReAD.", "AI": {"tldr": "该论文提出了一个全面的视网膜异常检测基准，解决了现有方法在数据、算法和评估上的局限性，并提出了新的方法NFM-DRA以提升性能。", "motivation": "现有视网膜异常检测领域缺乏公开、全面的基准，导致方法评估不公平且实验设置不够严谨。此外，现有方法多关注单类监督学习，忽略了临床中常见的异常数据和未标记数据。", "method": "论文引入了一个系统性基准，并提出了NFM-DRA方法，该方法结合了异常解耦表示（DRA）和正常特征记忆机制，以应对未见异常的性能下降问题。", "result": "实验表明，NFM-DRA方法在性能上优于现有方法，成为新的SOTA。", "conclusion": "该研究填补了视网膜异常检测领域的空白，为未来方法评估和进步提供了可靠基准。"}}
{"id": "2507.10548", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10548", "abs": "https://arxiv.org/abs/2507.10548", "authors": ["Mingxian Lin", "Wei Huang", "Yitang Li", "Chengjie Jiang", "Kui Wu", "Fangwei Zhong", "Shengju Qian", "Xin Wang", "Xiaojuan Qi"], "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments", "comment": "Project page: https://mxllc.github.io/EmbRACE-3K/", "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.", "AI": {"tldr": "论文介绍了EmRACE-3K数据集，用于评估和改进视觉语言模型在具身环境中的推理能力，现有模型表现不佳，但通过微调显著提升。", "motivation": "现有视觉语言模型在被动任务中表现优秀，但在具身环境中的在线交互和主动场景理解能力有限，需要新的数据集和方法来提升。", "method": "提出EmRACE-3K数据集，包含3,000多个语言指导任务，涵盖导航、物体操作和多阶段目标执行，并使用监督学习和强化学习微调Qwen2.5-VL-7B模型。", "result": "零样本设置下模型成功率低于20%，微调后性能显著提升，验证了数据集的有效性。", "conclusion": "EmRACE-3K为具身推理能力提供了重要基准，微调方法展示了改进模型的潜力。"}}
{"id": "2507.09693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09693", "abs": "https://arxiv.org/abs/2507.09693", "authors": ["Jiali Chen", "Yujie Jia", "Zihan Wu", "Jinyu Yang", "Jianpeng Chen", "Xusen Hei", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments", "comment": "Accepted by ACM MM 2025", "summary": "Experiment commentary is crucial in describing the experimental procedures,\ndelving into underlying scientific principles, and incorporating\ncontent-related safety guidelines. In practice, human teachers rely heavily on\nsubject-specific expertise and invest significant time preparing such\ncommentary. To address this challenge, we introduce the task of automatic\ncommentary generation across multi-discipline scientific experiments. While\nrecent progress in large multimodal models (LMMs) has demonstrated promising\ncapabilities in video understanding and reasoning, their ability to generate\nfine-grained and insightful experiment commentary remains largely\nunderexplored. In this paper, we make the following contributions: (i) We\nconstruct \\textit{ExpInstruct}, the first dataset tailored for experiment\ncommentary generation, featuring over 7\\textit{K} step-level commentaries\nacross 21 scientific subjects from 3 core disciplines (\\ie, science, healthcare\nand engineering). Each sample includes procedural descriptions along with\npotential scientific principles (\\eg, chemical equations and physical laws) and\nsafety guidelines. (ii) We propose ExpStar, an automatic experiment commentary\ngeneration model that leverages a retrieval-augmented mechanism to adaptively\naccess, evaluate, and utilize external knowledge. (iii) Extensive experiments\nshow that our ExpStar substantially outperforms 14 leading LMMs, which\nhighlights the superiority of our dataset and model. We believe that ExpStar\nholds great potential for advancing AI-assisted scientific experiment\ninstruction.", "AI": {"tldr": "论文提出自动生成多学科科学实验评论的任务，构建了首个数据集ExpInstruct，并提出模型ExpStar，显著优于现有大型多模态模型。", "motivation": "解决人工教师依赖专业知识和时间投入的问题，探索大型多模态模型在生成细粒度实验评论方面的潜力。", "method": "构建ExpInstruct数据集，提出检索增强模型ExpStar，结合外部知识生成评论。", "result": "ExpStar在实验中显著优于14种领先的大型多模态模型。", "conclusion": "ExpStar在AI辅助科学实验教学中具有巨大潜力。"}}
{"id": "2507.10496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10496", "abs": "https://arxiv.org/abs/2507.10496", "authors": ["Ruilong Li", "Brent Yi", "Junchen Liu", "Hang Gao", "Yi Ma", "Angjoo Kanazawa"], "title": "Cameras as Relative Positional Encoding", "comment": "Project Page: https://www.liruilong.cn/prope/", "summary": "Transformers are increasingly prevalent for multi-view computer vision tasks,\nwhere geometric relationships between viewpoints are critical for 3D\nperception. To leverage these relationships, multi-view transformers must use\ncamera geometry to ground visual tokens in 3D space. In this work, we compare\ntechniques for conditioning transformers on cameras: token-level raymap\nencodings, attention-level relative pose encodings, and a new relative encoding\nwe propose -- Projective Positional Encoding (PRoPE) -- that captures complete\ncamera frustums, both intrinsics and extrinsics, as a relative positional\nencoding. Our experiments begin by showing how relative camera conditioning\nimproves performance in feedforward novel view synthesis, with further gains\nfrom PRoPE. This holds across settings: scenes with both shared and varying\nintrinsics, when combining token- and attention-level conditioning, and for\ngeneralization to inputs with out-of-distribution sequence lengths and camera\nintrinsics. We then verify that these benefits persist for different tasks,\nstereo depth estimation and discriminative spatial cognition, as well as larger\nmodel sizes.", "AI": {"tldr": "论文比较了多种方法在多视角计算机视觉任务中利用相机几何信息，提出了一种新的相对位置编码（PRoPE），并在多个任务中验证了其有效性。", "motivation": "多视角视觉任务中相机几何关系对3D感知至关重要，但现有方法未能充分利用这些关系。", "method": "比较了三种相机几何编码方法：token级光线图编码、attention级相对位姿编码，以及新提出的PRoPE（投影位置编码）。", "result": "实验表明，PRoPE在多种任务（如新视角合成、立体深度估计）中表现优越，尤其在泛化性和模型规模扩展时效果显著。", "conclusion": "PRoPE能有效捕捉相机几何信息，提升多视角视觉任务的性能，具有广泛适用性。"}}
{"id": "2507.09702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09702", "abs": "https://arxiv.org/abs/2507.09702", "authors": ["Phat Nguyen", "Ngai-Man Cheung"], "title": "Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI", "comment": null, "summary": "Token compression techniques have recently emerged as powerful tools for\naccelerating Vision Transformer (ViT) inference in computer vision. Due to the\nquadratic computational complexity with respect to the token sequence length,\nthese methods aim to remove less informative tokens before the attention layers\nto improve inference throughput. While numerous studies have explored various\naccuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.\nFirst, there is a lack of unified survey that systematically categorizes and\ncompares token compression approaches based on their core strategies (e.g.,\npruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.\nplug-in). Second, most benchmarks are limited to standard ViT models (e.g.,\nViT-B, ViT-L), leaving open the question of whether such methods remain\neffective when applied to structurally compressed transformers, which are\nincreasingly deployed on resource-constrained edge devices. To address these\ngaps, we present the first systematic taxonomy and comparative study of token\ncompression methods, and we evaluate representative techniques on both standard\nand compact ViT architectures. Our experiments reveal that while token\ncompression methods are effective for general-purpose ViTs, they often\nunderperform when directly applied to compact designs. These findings not only\nprovide practical insights but also pave the way for future research on\nadapting token optimization techniques to compact transformer-based networks\nfor edge AI and AI agent applications.", "AI": {"tldr": "论文系统分类和比较了视觉Transformer（ViT）中的令牌压缩技术，并评估了其在标准和紧凑ViT架构上的表现。", "motivation": "填补令牌压缩技术缺乏统一分类和在紧凑ViT架构上有效性研究的空白。", "method": "提出系统分类法，并在标准和紧凑ViT架构上评估代表性令牌压缩技术。", "result": "令牌压缩技术对标准ViT有效，但在紧凑ViT上表现不佳。", "conclusion": "研究为未来紧凑Transformer网络的令牌优化技术提供了方向。"}}
{"id": "2507.10552", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10552", "abs": "https://arxiv.org/abs/2507.10552", "authors": ["Vladimir Iashin", "Horace Lee", "Dan Schofield", "Andrew Zisserman"], "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder", "comment": "Accepted for publication. Project page, code and weights:\n  https://www.robots.ox.ac.uk/~vgg/research/ChimpUFE/", "summary": "Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.", "AI": {"tldr": "提出了一种完全自监督的方法，利用DINOv2框架从无标签的相机陷阱视频中学习黑猩猩面部嵌入，无需身份标签即可实现高性能的开放集重识别。", "motivation": "解决野生动物监测中手动识别个体动物的瓶颈问题，推动非侵入性种群研究的可扩展性。", "method": "基于DINOv2框架，利用自动挖掘的面部裁剪训练视觉变换器，完全自监督，无需标签数据。", "result": "在Bossou等挑战性基准测试中，开放集重识别性能优于监督基线方法。", "conclusion": "自监督学习在生物多样性监测中具有巨大潜力，为可扩展的非侵入性种群研究提供了新途径。"}}
{"id": "2507.09748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09748", "abs": "https://arxiv.org/abs/2507.09748", "authors": ["Yu Lei", "Bingde Liu", "Qingsong Xie", "Haonan Lu", "Zhijie Deng"], "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation", "comment": "Accepted by ICCV 2025", "summary": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion\nmodels has gained increasing interest, with variational score distillation\n(VSD) as a remarkable example. VSD proves that vanilla score distillation can\nbe improved by introducing an extra score-based model, which characterizes the\ndistribution of images rendered from 3D models, to correct the distillation\ngradient. Despite the theoretical foundations, VSD, in practice, is likely to\nsuffer from slow and sometimes ill-posed convergence. In this paper, we perform\nan in-depth investigation of the interplay between the introduced score model\nand the 3D model, and find that there exists a mismatching problem between LoRA\nand 3D distributions in practical implementation. We can simply adjust their\noptimization order to improve the generation quality. By doing so, the score\nmodel looks ahead to the current 3D state and hence yields more reasonable\ncorrections. Nevertheless, naive lookahead VSD may suffer from unstable\ntraining in practice due to the potential over-fitting. To address this, we\npropose to use a linearized variant of the model for score distillation, giving\nrise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).\n$L^2$-VSD can be realized efficiently with forward-mode autodiff\nfunctionalities of existing deep learning libraries. Extensive experiments\nvalidate the efficacy of $L^2$-VSD, revealing its clear superiority over prior\nscore distillation-based methods. We also show that our method can be\nseamlessly incorporated into any other VSD-based text-to-3D framework.", "AI": {"tldr": "论文提出了一种改进的变分分数蒸馏方法（$L^2$-VSD），通过线性化模型和前瞻优化顺序，解决了传统VSD方法收敛慢和不稳定的问题。", "motivation": "传统变分分数蒸馏（VSD）方法在实践中存在收敛慢和不稳定的问题，论文旨在通过优化分数模型与3D模型的交互来提升生成质量。", "method": "提出线性化前瞻变分分数蒸馏（$L^2$-VSD），通过调整优化顺序和使用线性化模型，提高分数蒸馏的稳定性和效率。", "result": "实验证明$L^2$-VSD在生成质量和稳定性上优于现有方法，并能无缝集成到其他VSD框架中。", "conclusion": "$L^2$-VSD通过前瞻和线性化优化，显著提升了文本到3D生成的效率和效果。"}}
{"id": "2507.09767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09767", "abs": "https://arxiv.org/abs/2507.09767", "authors": ["Ofir Itzhak Shahar", "Gur Elkin", "Ohad Ben-Shahar"], "title": "Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments", "comment": null, "summary": "Pairwise compatibility calculation is at the core of most\nfragments-reconstruction algorithms, in particular those designed to solve\ndifferent types of the jigsaw puzzle problem. However, most existing approaches\nfail, or aren't designed to deal with fragments of realistic geometric\nproperties one encounters in real-life puzzles. And in all other cases,\ncompatibility methods rely strongly on the restricted shapes of the fragments.\nIn this paper, we propose an efficient hybrid (geometric and pictorial)\napproach for computing the optimal alignment for pairs of fragments, without\nany assumptions about their shapes, dimensions, or pictorial content. We\nintroduce a new image fragments dataset generated via a novel method for image\nfragmentation and a formal erosion model that mimics real-world archaeological\nerosion, along with evaluation metrics for the compatibility task. We then\nembed our proposed compatibility into an archaeological puzzle-solving\nframework and demonstrate state-of-the-art neighborhood-level precision and\nrecall on the RePAIR 2D dataset, directly reflecting compatibility performance\nimprovements.", "AI": {"tldr": "本文提出了一种高效的混合（几何和图像）方法，用于计算碎片对的最佳对齐，无需假设其形状、尺寸或图像内容。", "motivation": "现有方法难以处理现实拼图中碎片的几何特性，且通常依赖于碎片的限制形状。", "method": "采用混合几何和图像的方法，引入新的图像碎片数据集和侵蚀模型，并嵌入考古拼图解决框架。", "result": "在RePAIR 2D数据集上实现了最先进的邻域级精度和召回率。", "conclusion": "提出的兼容性方法显著提升了性能，适用于现实拼图问题。"}}
{"id": "2507.09795", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09795", "abs": "https://arxiv.org/abs/2507.09795", "authors": ["Amirhossein Ansari", "Ke Wang", "Pulei Xiong"], "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection", "comment": "Accepted to ICCV 2025", "summary": "Recent advancements in Vision-Language Models like CLIP have enabled\nzero-shot OOD detection by leveraging both image and textual label information.\nAmong these, negative label-based methods such as NegLabel and CSP have shown\npromising results by utilizing a lexicon of words to define negative labels for\ndistinguishing OOD samples. However, these methods suffer from detecting\nin-distribution samples as OOD due to negative labels that are subcategories of\nin-distribution labels or proper nouns. They also face limitations in handling\nimages that match multiple in-distribution and negative labels. We propose\nNegRefine, a novel negative label refinement framework for zero-shot OOD\ndetection. By introducing a filtering mechanism to exclude subcategory labels\nand proper nouns from the negative label set and incorporating a\nmulti-matching-aware scoring function that dynamically adjusts the\ncontributions of multiple labels matching an image, NegRefine ensures a more\nrobust separation between in-distribution and OOD samples. We evaluate\nNegRefine on large-scale benchmarks, including ImageNet-1K. Source code is\navailable at https://github.com/ah-ansari/NegRefine.", "AI": {"tldr": "NegRefine提出了一种改进的负标签细化框架，用于零样本OOD检测，通过过滤子类别标签和专有名词，并引入多匹配感知评分函数，提高了检测的鲁棒性。", "motivation": "现有基于负标签的方法（如NegLabel和CSP）在区分OOD样本时存在误判问题，尤其是当负标签是分布内标签的子类别或专有名词时。此外，这些方法难以处理匹配多个标签的图像。", "method": "NegRefine通过过滤机制排除负标签集中的子类别标签和专有名词，并采用多匹配感知评分函数动态调整多个标签对图像的贡献。", "result": "在ImageNet-1K等大规模基准测试中，NegRefine表现优于现有方法，显著减少了误判。", "conclusion": "NegRefine通过改进负标签选择和评分机制，显著提升了零样本OOD检测的性能和鲁棒性。"}}
{"id": "2507.09815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09815", "abs": "https://arxiv.org/abs/2507.09815", "authors": ["Younggun Kim", "Ahmed S. Abdelrahman", "Mohamed Abdel-Aty"], "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding", "comment": "22 pages, 11 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.", "AI": {"tldr": "论文提出了VRU-Accident基准，用于评估多模态大语言模型在涉及弱势道路使用者的高风险交通场景中的推理能力。", "motivation": "解决自动驾驶系统中弱势道路使用者（如行人、骑行者）安全问题，现有模型缺乏标准化评估基准。", "method": "构建包含1K事故视频、6K多选问答对和1K密集场景描述的VRU-Accident基准，评估17种先进模型。", "result": "模型在视觉属性上表现良好，但在事故原因、类型和可预防性推理方面存在显著挑战。", "conclusion": "VRU-Accident为评估模型在安全关键场景中的能力提供了标准化工具，揭示了现有模型的不足。"}}
{"id": "2507.09830", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09830", "abs": "https://arxiv.org/abs/2507.09830", "authors": ["Shuhao Fu", "Philip J. Kellman", "Hongjing Lu"], "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models", "comment": null, "summary": "Both humans and deep learning models can recognize objects from 3D shapes\ndepicted with sparse visual information, such as a set of points randomly\nsampled from the surfaces of 3D objects (termed a point cloud). Although deep\nlearning models achieve human-like performance in recognizing objects from 3D\nshapes, it remains unclear whether these models develop 3D shape\nrepresentations similar to those used by human vision for object recognition.\nWe hypothesize that training with 3D shapes enables models to form\nrepresentations of local geometric structures in 3D shapes. However, their\nrepresentations of global 3D object shapes may be limited. We conducted two\nhuman experiments systematically manipulating point density and object\norientation (Experiment 1), and local geometric structure (Experiment 2).\nHumans consistently performed well across all experimental conditions. We\ncompared two types of deep learning models, one based on a convolutional neural\nnetwork (DGCNN) and the other on visual transformers (point transformer), with\nhuman performance. We found that the point transformer model provided a better\naccount of human performance than the convolution-based model. The advantage\nmainly results from the mechanism in the point transformer model that supports\nhierarchical abstraction of 3D shapes.", "AI": {"tldr": "论文探讨了深度学习模型与人类在识别3D物体形状时的表现差异，发现视觉变换器模型更接近人类表现。", "motivation": "研究深度学习模型是否形成与人类相似的3D形状表征，以理解模型的局限性。", "method": "通过两个人类实验（点密度、物体方向和局部几何结构）比较卷积神经网络和视觉变换器模型的表现。", "result": "视觉变换器模型（point transformer）比卷积模型（DGCNN）更接近人类表现，因其支持3D形状的层次抽象。", "conclusion": "视觉变换器模型在3D形状表征上更接近人类，表明其机制更适合处理全局形状信息。"}}
{"id": "2507.09862", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09862", "abs": "https://arxiv.org/abs/2507.09862", "authors": ["Youliang Zhang", "Zhaoyang Li", "Duomin Wang", "Jiahe Zhang", "Deyu Zhou", "Zixin Yin", "Xili Dai", "Gang Yu", "Xiu Li"], "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation", "comment": null, "summary": "The rapid development of large-scale models has catalyzed significant\nbreakthroughs in the digital human domain. These advanced methodologies offer\nhigh-fidelity solutions for avatar driving and rendering, leading academia to\nfocus on the next major challenge: audio-visual dyadic interactive virtual\nhuman. To facilitate research in this emerging area, we present SpeakerVid-5M\ndataset, the first large-scale, high-quality dataset designed for audio-visual\ndyadic interactive virtual human generation. Totaling over 8,743 hours,\nSpeakerVid-5M contains more than 5.2 million video clips of human portraits. It\ncovers diverse scales and interaction types, including monadic talking,\nlistening, and dyadic conversations. Crucially, the dataset is structured along\ntwo key dimensions: interaction type and data quality. First, it is categorized\ninto four types (dialogue branch, single branch, listening branch and\nmulti-turn branch) based on the interaction scenario. Second, it is stratified\ninto a large-scale pre-training subset and a curated, high-quality subset for\nSupervised Fine-Tuning (SFT). This dual structure accommodates a wide array of\n2D virtual human tasks. In addition, we provide an autoregressive (AR)-based\nvideo chat baseline trained on this data, accompanied by a dedicated set of\nmetrics and test data to serve as a benchmark VidChatBench for future work.\nBoth the dataset and the corresponding data processing code will be publicly\nreleased. Project page: https://dorniwang.github.io/SpeakerVid-5M/", "AI": {"tldr": "论文介绍了SpeakerVid-5M数据集，首个为音频-视觉双模态交互虚拟人生成设计的大规模高质量数据集，包含超过520万视频片段，并提供了基于自回归的视频聊天基线模型和评估基准。", "motivation": "随着大规模模型的快速发展，数字人领域取得重大突破，但音频-视觉双模态交互虚拟人成为新的研究挑战，需高质量数据集支持。", "method": "构建了SpeakerVid-5M数据集，包含四种交互类型（对话分支、单分支、倾听分支和多轮分支），并分为大规模预训练子集和高质量监督微调子集。同时提供了基于自回归的视频聊天基线模型和评估基准VidChatBench。", "result": "数据集包含超过520万视频片段，总时长8,743小时，覆盖多种交互类型和质量层次，为2D虚拟人任务提供了广泛支持。", "conclusion": "SpeakerVid-5M数据集和基线模型为音频-视觉双模态交互虚拟人的研究提供了重要资源，未来工作可在此基础上进一步推进。"}}
{"id": "2507.09880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09880", "abs": "https://arxiv.org/abs/2507.09880", "authors": ["Keito Suzuki", "Bang Du", "Runfa Blark Li", "Kunyao Chen", "Lei Wang", "Peng Liu", "Ning Bi", "Truong Nguyen"], "title": "OpenHuman4D: Open-Vocabulary 4D Human Parsing", "comment": null, "summary": "Understanding dynamic 3D human representation has become increasingly\ncritical in virtual and extended reality applications. However, existing human\npart segmentation methods are constrained by reliance on closed-set datasets\nand prolonged inference times, which significantly restrict their\napplicability. In this paper, we introduce the first 4D human parsing framework\nthat simultaneously addresses these challenges by reducing the inference time\nand introducing open-vocabulary capabilities. Building upon state-of-the-art\nopen-vocabulary 3D human parsing techniques, our approach extends the support\nto 4D human-centric video with three key innovations: 1) We adopt mask-based\nvideo object tracking to efficiently establish spatial and temporal\ncorrespondences, avoiding the necessity of segmenting all frames. 2) A novel\nMask Validation module is designed to manage new target identification and\nmitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating\nmemory-conditioned attention and logits equalization for robust embedding\nfusion. Extensive experiments demonstrate the effectiveness and flexibility of\nthe proposed method on 4D human-centric parsing tasks, achieving up to 93.3%\nacceleration compared to the previous state-of-the-art method, which was\nlimited to parsing fixed classes.", "AI": {"tldr": "本文提出了一种4D人体解析框架，通过减少推理时间和引入开放词汇能力，解决了现有方法依赖封闭数据集和推理时间长的问题。", "motivation": "动态3D人体表示在虚拟和扩展现实应用中日益重要，但现有方法受限于封闭数据集和长推理时间，限制了其适用性。", "method": "基于最先进的开放词汇3D人体解析技术，提出三项创新：1）采用基于掩码的视频对象跟踪；2）设计掩码验证模块；3）提出4D掩码融合模块。", "result": "实验表明，该方法在4D人体解析任务中有效且灵活，推理速度比现有方法快93.3%。", "conclusion": "该框架显著提升了4D人体解析的效率和灵活性，适用于开放词汇场景。"}}
{"id": "2507.09881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09881", "abs": "https://arxiv.org/abs/2507.09881", "authors": ["Yiran Qiao", "Disheng Liu", "Yiren Lu", "Yu Yin", "Mengnan Du", "Jing Ma"], "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering", "comment": null, "summary": "Recent work on counterfactual visual explanations has contributed to making\nartificial intelligence models more explainable by providing visual\nperturbation to flip the prediction. However, these approaches neglect the\ncausal relationships and the spurious correlations behind the image generation\nprocess, which often leads to unintended alterations in the counterfactual\nimages and renders the explanations with limited quality. To address this\nchallenge, we introduce a novel framework CECAS, which first leverages a\ncausally-guided adversarial method to generate counterfactual explanations. It\ninnovatively integrates a causal perspective to avoid unwanted perturbations on\nspurious factors in the counterfactuals. Extensive experiments demonstrate that\nour method outperforms existing state-of-the-art approaches across multiple\nbenchmark datasets and ultimately achieves a balanced trade-off among various\naspects of validity, sparsity, proximity, and realism.", "AI": {"tldr": "论文提出CECAS框架，通过因果引导的对抗方法生成反事实解释，避免虚假相关性，提升解释质量。", "motivation": "现有反事实视觉解释方法忽视因果关系和虚假相关性，导致解释质量受限。", "method": "提出CECAS框架，结合因果视角生成反事实解释，避免虚假因素的干扰。", "result": "在多个基准数据集上优于现有方法，实现有效性、稀疏性、接近性和真实性的平衡。", "conclusion": "CECAS通过因果引导显著提升了反事实解释的质量和实用性。"}}
{"id": "2507.09885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09885", "abs": "https://arxiv.org/abs/2507.09885", "authors": ["Zhanjiang Yang", "Lijun Sun", "Jiawei Dong", "Xiaoxin An", "Yang Liu", "Meng Li"], "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention", "comment": null, "summary": "Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective\nsolution for various vision-based applications. However, most existing\nlearning-based hyperspectral reconstruction methods directly learn the\nRGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent\nchallenge of transitioning from low-dimensional to high-dimensional\ninformation. To address this limitation, we propose a two-stage approach, MCGA,\nwhich first learns spectral patterns before estimating the mapping. In the\nfirst stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI\ndatasets, extracting a Mixture of Codebooks (MoC). In the second stage, the\nRGB-to-HSI mapping is refined by querying features from the MoC to replace\nlatent HSI representations, incorporating prior knowledge rather than forcing a\ndirect high-dimensional transformation. To further enhance reconstruction\nquality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,\nwhich adaptively adjust feature map intensities to meet hyperspectral\nreconstruction requirements. This physically motivated attention mechanism\nensures lightweight and efficient HSI recovery. Moreover, we propose an\nentropy-based Test-Time Adaptation strategy to improve robustness in real-world\nscenarios. Extensive experiments demonstrate that our method, MCGA, achieves\nstate-of-the-art performance. The code and models will be released at\nhttps://github.com/Fibonaccirabbit/MCGA", "AI": {"tldr": "MCGA提出了一种两阶段方法，通过先学习光谱模式再估计映射，解决了RGB到HSI转换的挑战，并引入注意力机制提升重建质量。", "motivation": "现有方法直接学习RGB到HSI的映射，忽略了低维到高维转换的固有挑战。", "method": "两阶段方法：1) 多尺度VQ-VAE学习光谱模式；2) 通过查询特征优化映射。引入灰度感知注意力和量化自注意力。", "result": "MCGA在实验中表现优异，达到最新技术水平。", "conclusion": "MCGA通过结合先验知识和注意力机制，高效且轻量地实现了高质量的HSI重建。"}}
{"id": "2507.09896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09896", "abs": "https://arxiv.org/abs/2507.09896", "authors": ["Xiuyu Wu", "Xinhao Wang", "Xiubin Zhu", "Lan Yang", "Jiyuan Liu", "Xingchen Hu"], "title": "Measuring the Impact of Rotation Equivariance on Aerial Object Detection", "comment": "Accepted by ICCV 2025", "summary": "Due to the arbitrary orientation of objects in aerial images, rotation\nequivariance is a critical property for aerial object detectors. However,\nrecent studies on rotation-equivariant aerial object detection remain scarce.\nMost detectors rely on data augmentation to enable models to learn\napproximately rotation-equivariant features. A few detectors have constructed\nrotation-equivariant networks, but due to the breaking of strict rotation\nequivariance by typical downsampling processes, these networks only achieve\napproximately rotation-equivariant backbones. Whether strict rotation\nequivariance is necessary for aerial image object detection remains an open\nquestion. In this paper, we implement a strictly rotation-equivariant backbone\nand neck network with a more advanced network structure and compare it with\napproximately rotation-equivariant networks to quantitatively measure the\nimpact of rotation equivariance on the performance of aerial image detectors.\nAdditionally, leveraging the inherently grouped nature of rotation-equivariant\nfeatures, we propose a multi-branch head network that reduces the parameter\ncount while improving detection accuracy. Based on the aforementioned\nimprovements, this study proposes the Multi-branch head rotation-equivariant\nsingle-stage Detector (MessDet), which achieves state-of-the-art performance on\nthe challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an\nexceptionally low parameter count.", "AI": {"tldr": "论文提出了一种严格旋转等变的目标检测器MessDet，通过改进网络结构和多分支头设计，在低参数量下实现了高性能。", "motivation": "航空图像中目标方向任意，旋转等变性对检测器至关重要，但现有方法多为近似等变，严格等变是否必要尚不明确。", "method": "实现严格旋转等变的骨干和颈部网络，并提出多分支头网络以减少参数并提升精度。", "result": "在DOTA-v1.0、DOTA-v1.5和DIOR-R数据集上达到SOTA性能，且参数量极低。", "conclusion": "严格旋转等变对航空目标检测性能有显著提升，多分支头设计有效优化了模型效率。"}}
{"id": "2507.09910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09910", "abs": "https://arxiv.org/abs/2507.09910", "authors": ["Yadong Qu", "Shancheng Fang", "Yuxin Wang", "Xiaorui Wang", "Zhineng Chen", "Hongtao Xie", "Yongdong Zhang"], "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation", "comment": "ICCV 2025", "summary": "Graphic design visually conveys information and data by creating and\ncombining text, images and graphics. Two-stage methods that rely primarily on\nlayout generation lack creativity and intelligence, making graphic design still\nlabor-intensive. Existing diffusion-based methods generate non-editable graphic\ndesign files at image level with poor legibility in visual text rendering,\nwhich prevents them from achieving satisfactory and practical automated graphic\ndesign. In this paper, we propose Instructional Graphic Designer (IGD) to\nswiftly generate multimodal layers with editable flexibility with only natural\nlanguage instructions. IGD adopts a new paradigm that leverages parametric\nrendering and image asset generation. First, we develop a design platform and\nestablish a standardized format for multi-scenario design files, thus laying\nthe foundation for scaling up data. Second, IGD utilizes the multimodal\nunderstanding and reasoning capabilities of MLLM to accomplish attribute\nprediction, sequencing and layout of layers. It also employs a diffusion model\nto generate image content for assets. By enabling end-to-end training, IGD\narchitecturally supports scalability and extensibility in complex graphic\ndesign tasks. The superior experimental results demonstrate that IGD offers a\nnew solution for graphic design.", "AI": {"tldr": "IGD是一种基于自然语言指令快速生成可编辑多模态层的图形设计方法，结合参数化渲染和图像资产生成，解决了现有方法缺乏创造力和可编辑性的问题。", "motivation": "现有图形设计方法依赖布局生成或扩散模型，缺乏创造力和可编辑性，导致设计过程仍依赖人工。", "method": "IGD利用多模态大语言模型（MLLM）进行属性预测、排序和布局，结合扩散模型生成图像资产，并通过端到端训练支持复杂任务。", "result": "实验结果表明，IGD在图形设计中表现优越，提供了新的解决方案。", "conclusion": "IGD为自动化图形设计提供了可扩展且实用的新范式。"}}
{"id": "2507.09915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09915", "abs": "https://arxiv.org/abs/2507.09915", "authors": ["Siyue Yao", "Mingjie Sun", "Eng Gee Lim", "Ran Yi", "Baojiang Zhong", "Moncef Gabbouj"], "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios", "comment": null, "summary": "The scarcity of data in various scenarios, such as medical, industry and\nautonomous driving, leads to model overfitting and dataset imbalance, thus\nhindering effective detection and segmentation performance. Existing studies\nemploy the generative models to synthesize more training samples to mitigate\ndata scarcity. However, these synthetic samples are repetitive or simplistic\nand fail to provide \"crucial information\" that targets the downstream model's\nweaknesses. Additionally, these methods typically require separate training for\ndifferent objects, leading to computational inefficiencies. To address these\nissues, we propose Crucial-Diff, a domain-agnostic framework designed to\nsynthesize crucial samples. Our method integrates two key modules. The Scene\nAgnostic Feature Extractor (SAFE) utilizes a unified feature extractor to\ncapture target information. The Weakness Aware Sample Miner (WASM) generates\nhard-to-detect samples using feedback from the detection results of downstream\nmodel, which is then fused with the output of SAFE module. Together, our\nCrucial-Diff framework generates diverse, high-quality training data, achieving\na pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,\nCrucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be\nreleased after acceptance.", "AI": {"tldr": "论文提出Crucial-Diff框架，通过生成关键样本解决数据稀缺问题，提升检测和分割性能。", "motivation": "数据稀缺导致模型过拟合和数据集不平衡，现有生成方法合成的样本重复或简单，无法针对下游模型的弱点。", "method": "结合SAFE模块提取目标信息，WASM模块根据下游模型反馈生成难检测样本，融合生成高质量数据。", "result": "在MVTec上达到83.63%的AP和78.12%的F1-MAX，息肉数据集上mIoU为81.64%，mDice为87.69%。", "conclusion": "Crucial-Diff能高效生成多样且高质量的样本，显著提升下游任务性能。"}}
{"id": "2507.09953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09953", "abs": "https://arxiv.org/abs/2507.09953", "authors": ["Zifei Wang", "Zian Mao", "Xiaoya He", "Xi Huang", "Haoran Zhang", "Chun Cheng", "Shufen Chu", "Tingzheng Hou", "Xiaoqin Zeng", "Yujun Xie"], "title": "4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion", "comment": null, "summary": "While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.", "AI": {"tldr": "该论文提出了一种基于多图像超分辨率（MISR）和卷积神经网络（CNN）的方法，用于在超低剂量条件下实现原子级分辨率的电子显微镜成像，适用于对辐射敏感的材料。", "motivation": "电子显微镜在原子分辨率下研究材料的结构-性能关系时，辐射损伤限制了其在敏感材料（如蛋白质和二维材料）中的应用。", "method": "通过结合多图像超分辨率技术和卷积神经网络，融合多个低分辨率、亚像素位移的图像，并利用合成多角度观测的特征增强重建。", "result": "开发了一种双路径、注意力引导的网络，实现了在超低剂量条件下原子级分辨率的成像，适用于非晶、半晶和晶体材料。", "conclusion": "该方法扩展了4D-STEM的能力，为辐射敏感材料的结构分析提供了一种通用且高效的新方法。"}}
{"id": "2507.09980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09980", "abs": "https://arxiv.org/abs/2507.09980", "authors": ["Zhipeng Xue", "Yan Zhang", "Ming Li", "Chun Li", "Yue Liu", "Fei Yu"], "title": "Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures", "comment": null, "summary": "Existing multi-view classification and clustering methods typically improve\ntask accuracy by leveraging and fusing information from different views.\nHowever, ensuring the reliability of multi-view integration and final decisions\nis crucial, particularly when dealing with noisy or corrupted data. Current\nmethods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty\nof network predictions, ignoring domain gaps between different modalities. To\naddress this issue, KPHD-Net, based on H\\\"older divergence, is proposed for\nmulti-view classification and clustering tasks. Generally, our KPHD-Net employs\na variational Dirichlet distribution to represent class probability\ndistributions, models evidences from different views, and then integrates it\nwith Dempster-Shafer evidence theory (DST) to improve uncertainty estimation\neffects. Our theoretical analysis demonstrates that Proper H\\\"older divergence\noffers a more effective measure of distribution discrepancies, ensuring\nenhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence\ntheory, recognized for its superior performance in multi-view fusion tasks, is\nintroduced and combined with the Kalman filter to provide future state\nestimations. This integration further enhances the reliability of the final\nfusion results. Extensive experiments show that the proposed KPHD-Net\noutperforms the current state-of-the-art methods in both classification and\nclustering tasks regarding accuracy, robustness, and reliability, with\ntheoretical guarantees.", "AI": {"tldr": "KPHD-Net提出了一种基于Hölder散度的多视图分类和聚类方法，通过结合Dempster-Shafer证据理论和变分Dirichlet分布，提高了不确定性和多视图融合的可靠性。", "motivation": "现有方法通常使用KL散度估计不确定性，忽略了模态间的领域差异，导致多视图融合的可靠性不足。", "method": "KPHD-Net利用变分Dirichlet分布表示类别概率分布，结合Hölder散度和Dempster-Shafer证据理论，并通过Kalman滤波器增强融合结果的可靠性。", "result": "实验表明，KPHD-Net在分类和聚类任务中，在准确性、鲁棒性和可靠性方面优于现有方法。", "conclusion": "KPHD-Net通过改进的不确定性估计和多视图融合方法，显著提升了多视图学习的性能。"}}
{"id": "2507.09984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09984", "abs": "https://arxiv.org/abs/2507.09984", "authors": ["Junho Lee", "Jeongwoo Shin", "Hyungwook Choi", "Joonseok Lee"], "title": "Latent Diffusion Models with Masked AutoEncoders", "comment": null, "summary": "In spite of remarkable potential of the Latent Diffusion Models (LDMs) in\nimage generation, the desired properties and optimal design of the autoencoders\nhave been underexplored. In this work, we analyze the role of autoencoders in\nLDMs and identify three key properties: latent smoothness, perceptual\ncompression quality, and reconstruction quality. We demonstrate that existing\nautoencoders fail to simultaneously satisfy all three properties, and propose\nVariational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical\nfeatures maintained by Masked AutoEncoder. We integrate VMAEs into the LDM\nframework, introducing Latent Diffusion Models with Masked AutoEncoders\n(LDMAEs). Through comprehensive experiments, we demonstrate significantly\nenhanced image generation quality and computational efficiency.", "AI": {"tldr": "论文分析了潜在扩散模型（LDMs）中自编码器的关键属性，并提出了一种新型自编码器VMAE，显著提升了图像生成质量和计算效率。", "motivation": "尽管潜在扩散模型在图像生成方面具有巨大潜力，但自编码器的理想属性和最优设计尚未充分研究。", "method": "通过分析自编码器的三个关键属性（潜在平滑性、感知压缩质量和重建质量），提出了一种基于掩码自编码器的新型自编码器VMAE，并将其整合到LDM框架中，形成LDMAE。", "result": "实验表明，LDMAE显著提升了图像生成质量和计算效率。", "conclusion": "VMAE和LDMAE为潜在扩散模型中的自编码器设计提供了新的方向，具有重要的实际应用价值。"}}
{"id": "2507.09993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09993", "abs": "https://arxiv.org/abs/2507.09993", "authors": ["Yixun Zhang", "Lizhi Wang", "Junjun Zhao", "Wending Zhao", "Feng Zhou", "Yonghao Dang", "Jianqin Yin"], "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving", "comment": "Submitted to WACV 2026", "summary": "Camera-based object detection systems play a vital role in autonomous\ndriving, yet they remain vulnerable to adversarial threats in real-world\nenvironments. While existing 2D and 3D physical attacks typically optimize\ntexture, they often struggle to balance physical realism and attack robustness.\nIn this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel\nadversarial object generation framework that leverages the full 14-dimensional\nparameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry\nand appearance in physically realizable ways. Unlike prior works that rely on\npatches or texture, 3DGAA jointly perturbs both geometric attributes (shape,\nscale, rotation) and appearance attributes (color, opacity) to produce\nphysically realistic and transferable adversarial objects. We further introduce\na physical filtering module to preserve geometric fidelity, and a physical\naugmentation module to simulate complex physical scenarios, thus enhancing\nattack generalization under real-world conditions. We evaluate 3DGAA on both\nvirtual benchmarks and physical-world setups using miniature vehicle models.\nExperimental results show that 3DGAA achieves to reduce the detection mAP from\n87.21% to 7.38%, significantly outperforming existing 3D physical attacks.\nMoreover, our method maintains high transferability across different physical\nconditions, demonstrating a new state-of-the-art in physically realizable\nadversarial attacks. These results validate 3DGAA as a practical attack\nframework for evaluating the safety of perception systems in autonomous\ndriving.", "AI": {"tldr": "3DGAA是一种基于3D高斯泼溅的对抗攻击框架，通过联合优化几何和外观属性，生成物理上可实现的对抗物体，显著降低了目标检测性能。", "motivation": "现有2D和3D物理攻击在平衡物理真实性和攻击鲁棒性方面存在不足，3DGAA旨在解决这一问题。", "method": "利用3D高斯泼溅的14维参数化，联合优化几何（形状、尺度、旋转）和外观（颜色、透明度）属性，并引入物理过滤和增强模块。", "result": "在虚拟和物理实验中，3DGAA将检测mAP从87.21%降至7.38%，显著优于现有方法，并具有高迁移性。", "conclusion": "3DGAA是一种实用的对抗攻击框架，可用于评估自动驾驶感知系统的安全性。"}}
{"id": "2507.09996", "categories": ["cs.CV", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09996", "abs": "https://arxiv.org/abs/2507.09996", "authors": ["Quentin Dessain", "Nicolas Delinte", "Bernard Hanseeuw", "Laurence Dricot", "Benoît Macq"], "title": "Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI", "comment": null, "summary": "Objective: This study aims to support early diagnosis of Alzheimer's disease\nand detection of amyloid accumulation by leveraging the microstructural\ninformation available in multi-shell diffusion MRI (dMRI) data, using a vision\ntransformer-based deep learning framework.\n  Methods: We present a classification pipeline that employs the Swin\nTransformer, a hierarchical vision transformer model, on multi-shell dMRI data\nfor the classification of Alzheimer's disease and amyloid presence. Key metrics\nfrom DTI and NODDI were extracted and projected onto 2D planes to enable\ntransfer learning with ImageNet-pretrained models. To efficiently adapt the\ntransformer to limited labeled neuroimaging data, we integrated Low-Rank\nAdaptation. We assessed the framework on diagnostic group prediction\n(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)\nand amyloid status classification.\n  Results: The framework achieved competitive classification results within the\nscope of multi-shell dMRI-based features, with the best balanced accuracy of\n95.2% for distinguishing cognitively normal individuals from those with\nAlzheimer's disease dementia using NODDI metrics. For amyloid detection, it\nreached 77.2% balanced accuracy in distinguishing amyloid-positive mild\ncognitive impairment/Alzheimer's disease dementia subjects from\namyloid-negative cognitively normal subjects, and 67.9% for identifying\namyloid-positive individuals among cognitively normal subjects. Grad-CAM-based\nexplainability analysis identified clinically relevant brain regions, including\nthe parahippocampal gyrus and hippocampus, as key contributors to model\npredictions.\n  Conclusion: This study demonstrates the promise of diffusion MRI and\ntransformer-based architectures for early detection of Alzheimer's disease and\namyloid pathology, supporting biomarker-driven diagnostics in data-limited\nbiomedical settings.", "AI": {"tldr": "该研究利用多壳扩散MRI数据和视觉变换器深度学习框架，支持阿尔茨海默病的早期诊断和淀粉样蛋白积累检测。", "motivation": "旨在通过多壳扩散MRI数据的微观结构信息，结合深度学习技术，实现阿尔茨海默病的早期诊断和淀粉样蛋白检测。", "method": "采用Swin Transformer模型，结合DTI和NODDI关键指标，通过低秩适应技术优化模型，用于阿尔茨海默病和淀粉样蛋白状态的分类。", "result": "在阿尔茨海默病痴呆与认知正常个体的分类中达到95.2%的平衡准确率，淀粉样蛋白检测的平衡准确率为77.2%和67.9%。", "conclusion": "研究表明扩散MRI和变换器架构在阿尔茨海默病早期检测中具有潜力，支持数据有限的生物医学环境中的生物标志物驱动诊断。"}}
{"id": "2507.10006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10006", "abs": "https://arxiv.org/abs/2507.10006", "authors": ["Guanghai Ding", "Yihua Ren", "Yuting Liu", "Qijun Zhao", "Shuiwang Li"], "title": "Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges", "comment": null, "summary": "With the rapid advancement of UAV technology and its extensive application in\nvarious fields such as military reconnaissance, environmental monitoring, and\nlogistics, achieving efficient and accurate Anti-UAV tracking has become\nessential. The importance of Anti-UAV tracking is increasingly prominent,\nespecially in scenarios such as public safety, border patrol, search and\nrescue, and agricultural monitoring, where operations in complex environments\ncan provide enhanced security. Current mainstream Anti-UAV tracking\ntechnologies are primarily centered around computer vision techniques,\nparticularly those that integrate multi-sensor data fusion with advanced\ndetection and tracking algorithms. This paper first reviews the characteristics\nand current challenges of Anti-UAV detection and tracking technologies. Next,\nit investigates and compiles several publicly available datasets, providing\naccessible links to support researchers in efficiently addressing related\nchallenges. Furthermore, the paper analyzes the major vision-based and\nvision-fusion-based Anti-UAV detection and tracking algorithms proposed in\nrecent years. Finally, based on the above research, this paper outlines future\nresearch directions, aiming to provide valuable insights for advancing the\nfield.", "AI": {"tldr": "论文综述了反无人机跟踪技术的重要性、当前挑战、数据集、主流算法及未来研究方向。", "motivation": "随着无人机技术的快速发展和广泛应用，反无人机跟踪在公共安全、边境巡逻等复杂环境中变得至关重要。", "method": "回顾了反无人机检测与跟踪技术的特性与挑战，整理了公开数据集，并分析了近年来基于视觉和视觉融合的算法。", "result": "提供了数据集链接和算法分析，为研究者提供了解决相关挑战的资源。", "conclusion": "论文总结了未来研究方向，旨在推动反无人机跟踪领域的进步。"}}
{"id": "2507.10009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10009", "abs": "https://arxiv.org/abs/2507.10009", "authors": ["Geyou Zhang", "Kai Liu", "Ce Zhu"], "title": "Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry", "comment": null, "summary": "Phase shifting profilometry (PSP) is widely used in high-precision 3D\nscanning due to its high accuracy, robustness, and pixel-wise handling.\nHowever, a fundamental assumption of PSP that the object should remain static\ndoes not hold in dynamic measurement, making PSP susceptible to object motion.\nTo address this challenge, our proposed solution, phase-sequential binomial\nself-compensation (P-BSC), sums successive motion-affected phase frames\nweighted by binomial coefficients. This approach exponentially reduces the\nmotion error in a pixel-wise and frame-wise loopable manner. Despite its\nefficacy, P-BSC suffers from high computational overhead and error accumulation\ndue to its reliance on multi-frame phase calculations and weighted summations.\nInspired by P-BSC, we propose an image-sequential binomial self-compensation\n(I-BSC) to weight sum the homogeneous fringe images instead of successive phase\nframes, which generalizes the BSC concept from phase sequences to image\nsequences. I-BSC computes the arctangent function only once, resolving both\nlimitations in P-BSC. Extensive analysis, simulations, and experiments show\nthat 1) the proposed BSC outperforms existing methods in reducing motion error\nwhile achieving a quasi-single-shot frame rate, i.e., depth map frame rate\nequals to the camera's acquisition rate, enabling 3D reconstruction with high\npixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the\ncomputational complexity by one polynomial order, thereby accelerating the\ncomputational frame rate by several to dozen times, while also reaching faster\nmotion error convergence.", "AI": {"tldr": "提出了一种图像序列二项式自补偿（I-BSC）方法，通过加权求和同质条纹图像而非相位帧，显著降低动态测量中的运动误差，同时减少计算复杂度。", "motivation": "解决相位移动轮廓测量（PSP）在动态测量中因物体运动导致的误差问题。", "method": "I-BSC方法对同质条纹图像进行加权求和，仅需计算一次反正切函数，降低了计算复杂度和误差累积。", "result": "I-BSC在减少运动误差的同时实现了准单帧速率，计算帧速率提升数倍至数十倍，且运动误差收敛更快。", "conclusion": "I-BSC是一种高效的方法，适用于高精度动态3D扫描，具有高像素-深度-时间分辨率。"}}
{"id": "2507.10029", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10029", "abs": "https://arxiv.org/abs/2507.10029", "authors": ["Seokeon Choi", "Sunghyun Park", "Hyoungwoo Park", "Jeongho Kim", "Sungrack Yun"], "title": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies", "comment": null, "summary": "Memory-efficient personalization is critical for adapting text-to-image\ndiffusion models while preserving user privacy and operating within the limited\ncomputational resources of edge devices. To this end, we propose a selective\noptimization framework that adaptively chooses between backpropagation on\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\nimages (ZO-high), guided by the characteristics of the diffusion process. As\nobserved in our experiments, BP-low efficiently adapts the model to\ntarget-specific features, but suffers from structural distortions due to\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\nminimal memory overhead but faces slow convergence when applied without prior\nadaptation. By complementing both methods, our framework leverages BP-low for\neffective personalization while using ZO-high to maintain structural\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\nprobabilistic function that dynamically selects the appropriate optimization\nstrategy based on diffusion timesteps. This function mitigates the overfitting\nfrom BP-low at high timesteps, where structural information is critical, while\nensuring ZO-high is applied more effectively as training progresses.\nExperimental results demonstrate that our method achieves competitive\nperformance while significantly reducing memory consumption, enabling scalable,\nhigh-quality on-device personalization without increasing inference latency.", "AI": {"tldr": "提出了一种选择性优化框架，结合低分辨率反向传播（BP-low）和高分辨率零阶优化（ZO-high），以实现内存高效且高质量的文本到图像扩散模型个性化。", "motivation": "解决在边缘设备上适应文本到图像扩散模型时的内存效率和隐私保护问题。", "method": "选择性优化框架，动态选择BP-low和ZO-high，结合时间步感知概率函数。", "result": "在显著降低内存消耗的同时，实现了竞争性性能，支持高质量设备端个性化。", "conclusion": "该方法为内存高效的模型个性化提供了有效解决方案，适用于资源受限的设备。"}}
{"id": "2507.10053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10053", "abs": "https://arxiv.org/abs/2507.10053", "authors": ["Marc Serra Ortega", "Emanuele Vivoli", "Artemis Llabrés", "Dimosthenis Karatzas"], "title": "CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books", "comment": null, "summary": "This paper introduces CoSMo, a novel multimodal Transformer for Page Stream\nSegmentation (PSS) in comic books, a critical task for automated content\nunderstanding, as it is a necessary first stage for many downstream tasks like\ncharacter analysis, story indexing, or metadata enrichment. We formalize PSS\nfor this unique medium and curate a new 20,800-page annotated dataset. CoSMo,\ndeveloped in vision-only and multimodal variants, consistently outperforms\ntraditional baselines and significantly larger general-purpose vision-language\nmodels across F1-Macro, Panoptic Quality, and stream-level metrics. Our\nfindings highlight the dominance of visual features for comic PSS\nmacro-structure, yet demonstrate multimodal benefits in resolving challenging\nambiguities. CoSMo establishes a new state-of-the-art, paving the way for\nscalable comic book analysis.", "AI": {"tldr": "CoSMo是一种新型多模态Transformer，用于漫画书的页面流分割（PSS），在视觉和多模态版本中均优于传统基线模型，并在多个指标上表现优异。", "motivation": "漫画书的页面流分割是自动化内容理解的关键任务，为下游任务（如角色分析、故事索引）提供基础。", "method": "提出CoSMo模型，并构建了一个20,800页的标注数据集，开发了视觉和多模态版本。", "result": "CoSMo在F1-Macro、Panoptic Quality等指标上显著优于基线模型，视觉特征对宏观结构起主导作用，多模态在解决模糊性方面有优势。", "conclusion": "CoSMo为漫画书分析提供了新的最先进方法，支持可扩展的自动化分析。"}}
{"id": "2507.10065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10065", "abs": "https://arxiv.org/abs/2507.10065", "authors": ["Chenguo Lin", "Yuchen Lin", "Panwang Pan", "Yifan Yu", "Honglei Yan", "Katerina Fragkiadaki", "Yadong Mu"], "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second", "comment": "Project page: https://chenguolin.github.io/projects/MoVieS", "summary": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic\nnovel views from monocular videos in one second. MoVieS represents dynamic 3D\nscenes using pixel-aligned grids of Gaussian primitives, explicitly supervising\ntheir time-varying motion. This allows, for the first time, the unified\nmodeling of appearance, geometry and motion, and enables view synthesis,\nreconstruction and 3D point tracking within a single learning-based framework.\nBy bridging novel view synthesis with dynamic geometry reconstruction, MoVieS\nenables large-scale training on diverse datasets with minimal dependence on\ntask-specific supervision. As a result, it also naturally supports a wide range\nof zero-shot applications, such as scene flow estimation and moving object\nsegmentation. Extensive experiments validate the effectiveness and efficiency\nof MoVieS across multiple tasks, achieving competitive performance while\noffering several orders of magnitude speedups.", "AI": {"tldr": "MoVieS是一种新颖的前馈模型，能够在一秒内从单目视频合成4D动态新视角。它通过像素对齐的高斯基元网格表示动态3D场景，并显式监督其时间变化运动，首次实现了外观、几何和运动的统一建模。", "motivation": "传统方法难以统一建模动态场景的外观、几何和运动，且依赖任务特定的监督。MoVieS旨在通过一个学习框架实现多任务支持，并减少对特定监督的依赖。", "method": "MoVieS使用像素对齐的高斯基元网格表示动态3D场景，显式监督其时间变化运动，实现外观、几何和运动的统一建模。", "result": "MoVieS在多个任务中表现出色，支持零样本应用（如场景流估计和移动物体分割），并在速度上实现数量级的提升。", "conclusion": "MoVieS通过统一建模动态场景的多方面特性，实现了高效、多功能的动态新视角合成，为相关领域提供了新的解决方案。"}}
{"id": "2507.10072", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10072", "abs": "https://arxiv.org/abs/2507.10072", "authors": ["Meng Yu", "Kun Zhan"], "title": "Frequency Regulation for Exposure Bias Mitigation in Diffusion Models", "comment": "ACM Multimedia 2025 accepted!", "summary": "Diffusion models exhibit impressive generative capabilities but are\nsignificantly impacted by exposure bias. In this paper, we make a key\nobservation: the energy of the predicted noisy images decreases during the\ndiffusion process. Building on this, we identify two important findings: 1) The\nreduction in energy follows distinct patterns in the low-frequency and\nhigh-frequency subbands; 2) This energy reduction results in amplitude\nvariations between the network-reconstructed clean data and the real clean\ndata. Based on the first finding, we introduce a frequency-domain regulation\nmechanism utilizing wavelet transforms, which separately adjusts the low- and\nhigh-frequency subbands. Leveraging the second insight, we provide a more\naccurate analysis of exposure bias in the two subbands. Our method is\ntraining-free and plug-and-play, significantly improving the generative quality\nof various diffusion models and providing a robust solution to exposure bias\nacross different model architectures. The source code is available at\nhttps://github.com/kunzhan/wpp.", "AI": {"tldr": "本文通过分析扩散模型中预测噪声图像能量的变化，提出了一种基于小波变换的频率域调节机制，显著提升了生成质量并解决了曝光偏差问题。", "motivation": "扩散模型在生成能力上表现优异，但受曝光偏差影响较大。本文通过观察噪声图像能量的变化规律，试图解决这一问题。", "method": "利用小波变换分别调节低频和高频子带，并基于能量变化分析曝光偏差。方法无需训练，即插即用。", "result": "显著提升了多种扩散模型的生成质量，并提供了跨模型架构的曝光偏差解决方案。", "conclusion": "提出的频率域调节机制有效解决了扩散模型的曝光偏差问题，且具有通用性和实用性。"}}
{"id": "2507.10084", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10084", "abs": "https://arxiv.org/abs/2507.10084", "authors": ["Haonan Chen", "Xin Tong"], "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area", "comment": "13 pages, 6 figures, 2 tables", "summary": "To address the prevalent challenges of domain shift and small sample sizes in\nremote sensing image water body segmentation, this study proposes and validates\na two-stage transfer learning strategy based on the SegFormer model. The\napproach begins by training a foundational segmentation model on a diverse\nsource domain, where it achieves an Intersection over Union (IoU) of 68.80% on\nits validation set, followed by fine-tuning on data from the distinct target\ndomain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by\nhighly complex topography and spectral features -- the experimental results\ndemonstrate that this strategy significantly boosts the IoU for the water body\nsegmentation task from 25.50% (for direct transfer) to 64.84%. This not only\neffectively resolves the model performance degradation caused by domain\ndiscrepancy but also provides an effective technical paradigm for\nhigh-precision thematic information extraction in data-scarce and\nenvironmentally unique remote sensing scenarios.", "AI": {"tldr": "提出一种基于SegFormer的两阶段迁移学习策略，显著提升遥感图像水体分割的IoU。", "motivation": "解决遥感图像水体分割中的领域偏移和小样本问题。", "method": "先在大规模源域数据上训练基础模型，再在目标域数据上微调。", "result": "IoU从25.50%提升至64.84%。", "conclusion": "策略有效解决了领域差异导致的性能下降，为数据稀缺场景提供了技术范例。"}}
{"id": "2507.10095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10095", "abs": "https://arxiv.org/abs/2507.10095", "authors": ["Bingchao Wang", "Zhiwei Ning", "Jianyu Ding", "Xuanang Gao", "Yin Li", "Dongsheng Jiang", "Jie Yang", "Wei Liu"], "title": "FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text", "comment": null, "summary": "CLIP has shown promising performance across many short-text tasks in a\nzero-shot manner. However, limited by the input length of the text encoder,\nCLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To\nremedy this issue, we propose FIX-CLIP which includes three novel modules: (1)\nA dual-branch training pipeline that aligns short and long texts with masked\nand raw images respectively, which boosts the long-text representation while\npreserving the short-text ability. (2) Multiple learnable regional prompts with\nunidirectional masks in Transformer layers for regional information extraction.\n(3) A hierarchical feature alignment module in the intermediate encoder layers\nto promote the consistency of multi-scale features. Furthermore, we collect 30M\nimages and utilize existing MLLMs to synthesize long-text captions for\ntraining. Extensive experiments show that FIX-CLIP achieves state-of-the-art\nperformance on both long-text and short-text retrieval benchmarks. For\ndownstream applications, we reveal that FIX-CLIP's text encoder delivers\npromising performance in a plug-and-play manner for diffusion models with\nlong-text input.", "AI": {"tldr": "FIX-CLIP通过双分支训练、区域提示和层次特征对齐模块，解决了CLIP在长文本任务中的限制，并在长文本和短文本检索中表现优异。", "motivation": "CLIP在短文本任务中表现良好，但在长文本输入（>77个token）时受限。", "method": "提出FIX-CLIP，包括双分支训练、多区域提示和层次特征对齐模块，并使用合成数据训练。", "result": "在长文本和短文本检索任务中达到最优性能，并在扩散模型中表现良好。", "conclusion": "FIX-CLIP有效解决了CLIP的长文本限制，具有广泛的应用潜力。"}}
{"id": "2507.10115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10115", "abs": "https://arxiv.org/abs/2507.10115", "authors": ["Hamidreza Hashempoor"], "title": "Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association", "comment": null, "summary": "We propose a multi-camera multi-target (MCMT) tracking framework that ensures\nconsistent global identity assignment across views using trajectory and\nappearance cues. The pipeline starts with BoT-SORT-based single-camera\ntracking, followed by an initial glance phase to initialize global IDs via\ntrajectory-feature matching. In later frames, new tracklets are matched to\nexisting global identities through a prioritized global matching strategy. New\nglobal IDs are only introduced when no sufficiently similar trajectory or\nfeature match is found. 3D positions are estimated using depth maps and\ncalibration for spatial validation.", "AI": {"tldr": "提出了一种多摄像头多目标跟踪框架，通过轨迹和外观特征实现跨视角的全局身份一致性分配。", "motivation": "解决多摄像头场景下目标跟踪的全局身份一致性分配问题。", "method": "采用BoT-SORT单摄像头跟踪，通过轨迹特征匹配初始化全局ID，后续帧使用优先全局匹配策略。", "result": "实现了跨视角的全局身份一致性分配，并通过3D位置估计进行空间验证。", "conclusion": "该框架在多摄像头多目标跟踪中表现高效且准确。"}}
{"id": "2507.10118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10118", "abs": "https://arxiv.org/abs/2507.10118", "authors": ["Ivan Martinović", "Josip Šarić", "Marin Oršić", "Matej Kristan", "Siniša Šegvić"], "title": "DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation", "comment": "ICCV 2025 Findings Workshop", "summary": "Pixel-level annotation is expensive and time-consuming. Semi-supervised\nsegmentation methods address this challenge by learning models on few labeled\nimages alongside a large corpus of unlabeled images. Although foundation models\ncould further account for label scarcity, effective mechanisms for their\nexploitation remain underexplored. We address this by devising a novel\nsemi-supervised panoptic approach fueled by two dedicated foundation models. We\nenhance recognition by complementing unsupervised mask-transformer consistency\nwith zero-shot classification of CLIP features. We enhance localization by\nclass-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting\ndecoupled enhancement of recognition and localization (DEARLi) particularly\nexcels in the most challenging semi-supervised scenarios with large taxonomies\nand limited labeled data. Moreover, DEARLi outperforms the state of the art in\nsemi-supervised semantic segmentation by a large margin while requiring 8x less\nGPU memory, in spite of being trained only for the panoptic objective. We\nobserve 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The\nsource code is available at https://github.com/helen1c/DEARLi.", "AI": {"tldr": "论文提出了一种名为DEARLi的半监督全景分割方法，通过结合两个专用基础模型，显著提升了识别和定位能力，在标签稀缺的情况下表现优异。", "motivation": "像素级标注成本高昂且耗时，半监督分割方法通过利用少量标注图像和大量未标注图像学习模型，但基础模型的潜力尚未充分挖掘。", "method": "提出DEARLi方法，结合无监督掩码变换器一致性和CLIP特征的零样本分类增强识别，通过SAM伪标签的类无关解码器预热增强定位。", "result": "在ADE20K数据集上仅用158张标注图像，实现了29.9 PQ和38.9 mIoU，性能显著优于现有半监督语义分割方法，且GPU内存需求降低8倍。", "conclusion": "DEARLi在标签稀缺和大分类场景下表现优异，为半监督分割提供了高效解决方案。"}}
{"id": "2507.10143", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10143", "abs": "https://arxiv.org/abs/2507.10143", "authors": ["David Calhas", "Arlindo L. Oliveira"], "title": "Deep Recurrence for Dynamical Segmentation Models", "comment": "12 pages", "summary": "While biological vision systems rely heavily on feedback connections to\niteratively refine perception, most artificial neural networks remain purely\nfeedforward, processing input in a single static pass. In this work, we propose\na predictive coding inspired feedback mechanism that introduces a recurrent\nloop from output to input, allowing the model to refine its internal state over\ntime. We implement this mechanism within a standard U-Net architecture and\nintroduce two biologically motivated operations, softmax projection and\nexponential decay, to ensure stability of the feedback loop. Through controlled\nexperiments on a synthetic segmentation task, we show that the feedback model\nsignificantly outperforms its feedforward counterpart in noisy conditions and\ngeneralizes more effectively with limited supervision. Notably, feedback\nachieves above random performance with just two training examples, while the\nfeedforward model requires at least four. Our findings demonstrate that\nfeedback enhances robustness and data efficiency, and offer a path toward more\nadaptive and biologically inspired neural architectures. Code is available at:\ngithub.com/DCalhas/feedback_segmentation.", "AI": {"tldr": "论文提出了一种受预测编码启发的反馈机制，通过循环连接从输出到输入，提升模型在噪声条件下的性能和数据效率。", "motivation": "生物视觉系统依赖反馈连接迭代优化感知，而大多数人工神经网络仍为纯前馈结构。本文旨在探索反馈机制在提升模型鲁棒性和适应性方面的潜力。", "method": "在标准U-Net架构中引入反馈循环，结合软最大投影和指数衰减操作以确保稳定性。通过合成分割任务进行实验验证。", "result": "反馈模型在噪声条件下显著优于前馈模型，且仅需两个训练样本即可实现高于随机性能，而前馈模型至少需要四个样本。", "conclusion": "反馈机制增强了模型的鲁棒性和数据效率，为更适应性和生物启发的神经网络架构提供了方向。"}}
{"id": "2507.10171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10171", "abs": "https://arxiv.org/abs/2507.10171", "authors": ["Youngmin Kim", "Giyeong Oh", "Kwangsoo Youm", "Youngjae Yu"], "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis", "comment": null, "summary": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, a the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance.", "AI": {"tldr": "SlumpGuard是一个基于AI的视频系统，用于实时自动评估混凝土的工作性，解决了传统坍落度测试的不足。", "motivation": "传统坍落度测试手动、耗时且不一致，无法满足实时监测需求。", "method": "开发了AI驱动的视频系统SlumpGuard，自动分析混凝土流动，无需人工干预。", "result": "系统提高了质量控制精度和效率，实际部署验证了其有效性。", "conclusion": "SlumpGuard是混凝土质量保证的实用解决方案。"}}
{"id": "2507.10195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10195", "abs": "https://arxiv.org/abs/2507.10195", "authors": ["Shuyu Yang", "Yaxiong Wang", "Yongrui Li", "Li Zhu", "Zhedong Zheng"], "title": "Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval", "comment": null, "summary": "In this work, we focus on text-based person retrieval, which aims to identify\nindividuals based on textual descriptions. Given the significant privacy issues\nand the high cost associated with manual annotation, synthetic data has become\na popular choice for pretraining models, leading to notable advancements.\nHowever, the considerable domain gap between synthetic pretraining datasets and\nreal-world target datasets, characterized by differences in lighting, color,\nand viewpoint, remains a critical obstacle that hinders the effectiveness of\nthe pretrain-finetune paradigm. To bridge this gap, we introduce a unified\ntext-based person retrieval pipeline considering domain adaptation at both\nimage and region levels. In particular, it contains two primary components,\ni.e., Domain-aware Diffusion (DaD) for image-level adaptation and\nMulti-granularity Relation Alignment (MRA) for region-level adaptation. As the\nname implies, Domain-aware Diffusion is to migrate the distribution of images\nfrom the pretraining dataset domain to the target real-world dataset domain,\ne.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level\nalignment by establishing correspondences between visual regions and their\ndescriptive sentences, thereby addressing disparities at a finer granularity.\nExtensive experiments show that our dual-level adaptation method has achieved\nstate-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,\noutperforming existing methodologies. The dataset, model, and code are\navailable at https://github.com/Shuyu-XJTU/MRA.", "AI": {"tldr": "本文提出了一种基于文本的人物检索方法，通过图像和区域级别的域适应技术，解决了合成数据与真实数据之间的域差距问题，并在多个数据集上取得了最优效果。", "motivation": "由于隐私问题和手动标注成本高，合成数据被广泛用于预训练模型，但合成数据与真实数据之间的域差距（如光照、颜色和视角差异）限制了模型效果。", "method": "提出了一种统一的文本人物检索流程，包含两个主要组件：图像级适应的Domain-aware Diffusion（DaD）和区域级适应的Multi-granularity Relation Alignment（MRA）。", "result": "在CUHK-PEDES、ICFG-PEDES和RSTPReid数据集上实现了最先进的性能。", "conclusion": "通过双级别域适应方法，有效缩小了合成与真实数据之间的差距，提升了检索效果。"}}
{"id": "2507.10203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10203", "abs": "https://arxiv.org/abs/2507.10203", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "title": "Improving Multimodal Learning via Imbalanced Learning", "comment": "Accepted to ICCV2025", "summary": "Multimodal learning often encounters the under-optimized problem and may\nperform worse than unimodal learning. Existing approaches attribute this issue\nto imbalanced learning across modalities and tend to address it through\ngradient balancing. However, this paper argues that balanced learning is not\nthe optimal setting for multimodal learning. With bias-variance analysis, we\nprove that imbalanced dependency on each modality obeying the inverse ratio of\ntheir variances contributes to optimal performance. To this end, we propose the\nAsymmetric Representation Learning(ARL) strategy to assist multimodal learning\nvia imbalanced optimization. ARL introduces auxiliary regularizers for each\nmodality encoder to calculate their prediction variance. ARL then calculates\ncoefficients via the unimodal variance to re-weight the optimization of each\nmodality, forcing the modality dependence ratio to be inversely proportional to\nthe modality variance ratio. Moreover, to minimize the generalization error,\nARL further introduces the prediction bias of each modality and jointly\noptimizes them with multimodal loss. Notably, all auxiliary regularizers share\nparameters with the multimodal model and rely only on the modality\nrepresentation. Thus the proposed ARL strategy introduces no extra parameters\nand is independent of the structures and fusion methods of the multimodal\nmodel. Finally, extensive experiments on various datasets validate the\neffectiveness and versatility of ARL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}", "AI": {"tldr": "论文提出了一种非对称表示学习（ARL）策略，通过不平衡优化解决多模态学习中的欠优化问题，证明模态依赖比例与模态方差比例成反比时性能最优。", "motivation": "多模态学习常因欠优化问题表现不如单模态学习，现有方法通过梯度平衡解决，但本文认为不平衡学习才是最优设置。", "method": "提出ARL策略，通过辅助正则器计算模态预测方差，并基于方差比例重新加权优化，同时引入预测偏差联合优化。", "result": "在多个数据集上的实验验证了ARL的有效性和通用性。", "conclusion": "ARL无需额外参数，独立于多模态模型结构，能显著提升多模态学习性能。"}}
{"id": "2507.10209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10209", "abs": "https://arxiv.org/abs/2507.10209", "authors": ["Huai-Qian Khor", "Yante Li", "Xingxun Jiang", "Guoying Zhao"], "title": "Is Micro-expression Ethnic Leaning?", "comment": null, "summary": "How much does ethnicity play its part in emotional expression? Emotional\nexpression and micro-expression research probe into understanding human\npsychological responses to emotional stimuli, thereby revealing substantial\nhidden yet authentic emotions that can be useful in the event of diagnosis and\ninterviews. While increased attention had been provided to micro-expression\nanalysis, the studies were done under Ekman's assumption of emotion\nuniversality, where emotional expressions are identical across cultures and\nsocial contexts. Our computational study uncovers some of the influences of\nethnic background in expression analysis, leading to an argument that the\nemotional universality hypothesis is an overgeneralization from the perspective\nof manual psychological analysis. In this research, we propose to investigate\nthe level of influence of ethnicity in a simulated micro-expression scenario.\nWe construct a cross-cultural micro-expression database and algorithmically\nannotate the ethnic labels to facilitate the investigation. With the ethnically\nannotated dataset, we perform a prima facie study to compare mono-ethnicity and\nstereo-ethnicity in a controlled environment, which uncovers a certain\ninfluence of ethnic bias via an experimental way. Building on this finding, we\npropose a framework that integrates ethnic context into the emotional feature\nlearning process, yielding an ethnically aware framework that recognises\nethnicity differences in micro-expression recognition. For improved\nunderstanding, qualitative analyses have been done to solidify the preliminary\ninvestigation into this new realm of research. Code is publicly available at\nhttps://github.com/IcedDoggie/ICMEW2025_EthnicMER", "AI": {"tldr": "该研究探讨了种族背景对情绪表达的影响，挑战了情绪普遍性假设，并提出了一种考虑种族差异的微表情识别框架。", "motivation": "研究动机是验证情绪表达是否真的如Ekman假设的那样具有普遍性，还是受到种族背景的影响。", "method": "通过构建跨文化微表情数据库并算法标注种族标签，研究比较了单一种族和混合种族环境下的情绪表达差异，并提出了一种考虑种族背景的情绪特征学习框架。", "result": "研究发现种族背景对情绪表达有显著影响，表明情绪普遍性假设存在过度概括。", "conclusion": "结论指出，情绪表达分析应考虑种族差异，提出的种族感知框架在微表情识别中表现更优。"}}
{"id": "2507.10213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10213", "abs": "https://arxiv.org/abs/2507.10213", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "title": "Boosting Multimodal Learning via Disentangled Gradient Learning", "comment": "Accepted to ICCV2025", "summary": "Multimodal learning often encounters the under-optimized problem and may have\nworse performance than unimodal learning. Existing methods attribute this\nproblem to the imbalanced learning between modalities and rebalance them\nthrough gradient modulation. However, they fail to explain why the dominant\nmodality in multimodal models also underperforms that in unimodal learning. In\nthis work, we reveal the optimization conflict between the modality encoder and\nmodality fusion module in multimodal models. Specifically, we prove that the\ncross-modal fusion in multimodal models decreases the gradient passed back to\neach modality encoder compared with unimodal models. Consequently, the\nperformance of each modality in the multimodal model is inferior to that in the\nunimodal model. To this end, we propose a disentangled gradient learning (DGL)\nframework to decouple the optimization of the modality encoder and modality\nfusion module in the multimodal model. DGL truncates the gradient\nback-propagated from the multimodal loss to the modality encoder and replaces\nit with the gradient from unimodal loss. Besides, DGL removes the gradient\nback-propagated from the unimodal loss to the modality fusion module. This\nhelps eliminate the gradient interference between the modality encoder and\nmodality fusion module while ensuring their respective optimization processes.\nFinally, extensive experiments on multiple types of modalities, tasks, and\nframeworks with dense cross-modal interaction demonstrate the effectiveness and\nversatility of the proposed DGL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}", "AI": {"tldr": "论文提出了一种解耦梯度学习（DGL）框架，解决多模态学习中模态编码器和融合模块的优化冲突问题。", "motivation": "多模态学习中，模态编码器和融合模块的优化冲突导致性能不如单模态学习，现有方法未能解决这一问题。", "method": "DGL通过截断多模态损失的梯度并替换为单模态损失的梯度，消除梯度干扰。", "result": "实验证明DGL在多种模态、任务和框架中有效且通用。", "conclusion": "DGL成功解决了多模态学习中的优化冲突问题，提升了性能。"}}
{"id": "2507.10217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10217", "abs": "https://arxiv.org/abs/2507.10217", "authors": ["Jeongho Kim", "Sunghyun Park", "Hyoungwoo Park", "Sungrack Yun", "Jaegul Choo", "Seokeon Cho"], "title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation", "comment": "10 pages, 8 figures", "summary": "Recent diffusion models achieve personalization by learning specific\nsubjects, allowing learned attributes to be integrated into generated images.\nHowever, personalized human image generation remains challenging due to the\nneed for precise and consistent attribute preservation (e.g., identity,\nclothing details). Existing subject-driven image generation methods often\nrequire either (1) inference-time fine-tuning with few images for each new\nsubject or (2) large-scale dataset training for generalization. Both approaches\nare computationally expensive and impractical for real-time applications. To\naddress these limitations, we present Wardrobe Polyptych LoRA, a novel\npart-level controllable model for personalized human image generation. By\ntraining only LoRA layers, our method removes the computational burden at\ninference while ensuring high-fidelity synthesis of unseen subjects. Our key\nidea is to condition the generation on the subject's wardrobe and leverage\nspatial references to reduce information loss, thereby improving fidelity and\nconsistency. Additionally, we introduce a selective subject region loss, which\nencourages the model to disregard some of reference images during training. Our\nloss ensures that generated images better align with text prompts while\nmaintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no\nadditional parameters at the inference stage and performs generation using a\nsingle model trained on a few training samples. We construct a new dataset and\nbenchmark tailored for personalized human image generation. Extensive\nexperiments show that our approach significantly outperforms existing\ntechniques in fidelity and consistency, enabling realistic and\nidentity-preserving full-body synthesis.", "AI": {"tldr": "提出了一种名为Wardrobe Polyptych LoRA的新方法，通过部分级可控模型实现个性化人体图像生成，避免了推理时的计算负担，同时提高了生成图像的保真度和一致性。", "motivation": "个性化人体图像生成面临精确和一致属性保存的挑战，现有方法计算成本高且不适用于实时应用。", "method": "训练仅LoRA层，利用空间参考减少信息丢失，并引入选择性主题区域损失以优化生成。", "result": "实验表明，该方法在保真度和一致性上显著优于现有技术，实现了真实且身份保持的全身合成。", "conclusion": "Wardrobe Polyptych LoRA无需推理时额外参数，仅需少量训练样本即可高效生成高质量图像。"}}
{"id": "2507.10218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10218", "abs": "https://arxiv.org/abs/2507.10218", "authors": ["Jimin Dai", "Jiexi Yan", "Jian Yang", "Lei Luo"], "title": "Straighten Viscous Rectified Flow via Noise Optimization", "comment": null, "summary": "The Reflow operation aims to straighten the inference trajectories of the\nrectified flow during training by constructing deterministic couplings between\nnoises and images, thereby improving the quality of generated images in\nsingle-step or few-step generation. However, we identify critical limitations\nin Reflow, particularly its inability to rapidly generate high-quality images\ndue to a distribution gap between images in its constructed deterministic\ncouplings and real images. To address these shortcomings, we propose a novel\nalternative called Straighten Viscous Rectified Flow via Noise Optimization\n(VRFNO), which is a joint training framework integrating an encoder and a\nneural velocity field. VRFNO introduces two key innovations: (1) a historical\nvelocity term that enhances trajectory distinction, enabling the model to more\naccurately predict the velocity of the current trajectory, and (2) the noise\noptimization through reparameterization to form optimized couplings with real\nimages which are then utilized for training, effectively mitigating errors\ncaused by Reflow's limitations. Comprehensive experiments on synthetic data and\nreal datasets with varying resolutions show that VRFNO significantly mitigates\nthe limitations of Reflow, achieving state-of-the-art performance in both\none-step and few-step generation tasks.", "AI": {"tldr": "VRFNO改进Reflow，通过噪声优化和速度场增强，提升单步/少步生成图像质量。", "motivation": "Reflow在快速生成高质量图像时存在分布差距问题，VRFNO旨在解决这一问题。", "method": "VRFNO结合编码器和神经速度场，引入历史速度项和噪声优化，优化耦合训练。", "result": "VRFNO在合成和真实数据集上表现优异，显著优于Reflow。", "conclusion": "VRFNO通过创新设计有效解决了Reflow的局限性，成为单步/少步生成的新标杆。"}}
{"id": "2507.10225", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10225", "abs": "https://arxiv.org/abs/2507.10225", "authors": ["Jinglun Li", "Kaixun Jiang", "Zhaoyu Chen", "Bo Lin", "Yao Tang", "Weifeng Ge", "Wenqiang Zhang"], "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection", "comment": null, "summary": "Pre-trained vision-language models have exhibited remarkable abilities in\ndetecting out-of-distribution (OOD) samples. However, some challenging OOD\nsamples, which lie close to in-distribution (InD) data in image feature space,\ncan still lead to misclassification. The emergence of foundation models like\ndiffusion models and multimodal large language models (MLLMs) offers a\npotential solution to this issue. In this work, we propose SynOOD, a novel\napproach that harnesses foundation models to generate synthetic, challenging\nOOD data for fine-tuning CLIP models, thereby enhancing boundary-level\ndiscrimination between InD and OOD samples. Our method uses an iterative\nin-painting process guided by contextual prompts from MLLMs to produce nuanced,\nboundary-aligned OOD samples. These samples are refined through noise\nadjustments based on gradients from OOD scores like the energy score,\neffectively sampling from the InD/OOD boundary. With these carefully\nsynthesized images, we fine-tune the CLIP image encoder and negative label\nfeatures derived from the text encoder to strengthen connections between\nnear-boundary OOD samples and a set of negative labels. Finally, SynOOD\nachieves state-of-the-art performance on the large-scale ImageNet benchmark,\nwith minimal increases in parameters and runtime. Our approach significantly\nsurpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by\n11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.", "AI": {"tldr": "SynOOD利用基础模型生成合成OOD数据，通过微调CLIP模型提升InD与OOD样本的边界区分能力，显著优于现有方法。", "motivation": "解决现有预训练视觉语言模型在处理接近InD数据的OOD样本时易误分类的问题。", "method": "利用扩散模型和多模态大语言模型生成合成OOD数据，通过迭代修复和噪声调整优化样本，微调CLIP模型。", "result": "在ImageNet基准测试中，AUROC提升2.80%，FPR95降低11.13%。", "conclusion": "SynOOD通过合成边界对齐的OOD样本，显著提升了OOD检测性能，且计算开销低。"}}
{"id": "2507.10236", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10236", "abs": "https://arxiv.org/abs/2507.10236", "authors": ["Despina Konstantinidou", "Dimitrios Karageorgiou", "Christos Koutlis", "Olga Papadopoulou", "Emmanouil Schinas", "Symeon Papadopoulos"], "title": "Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?", "comment": "35 pages, 4 figures", "summary": "The rapid advancement of generative technologies presents both unprecedented\ncreative opportunities and significant challenges, particularly in maintaining\nsocial trust and ensuring the integrity of digital information. Following these\nconcerns, the challenge of AI-Generated Image Detection (AID) becomes\nincreasingly critical. As these technologies become more sophisticated, the\nquality of AI-generated images has reached a level that can easily deceive even\nthe most discerning observers. Our systematic evaluation highlights a critical\nweakness in current AI-Generated Image Detection models: while they perform\nexceptionally well on controlled benchmark datasets, they struggle\nsignificantly with real-world variations. To assess this, we introduce ITW-SM,\na new dataset of real and AI-generated images collected from major social media\nplatforms. In this paper, we identify four key factors that influence AID\nperformance in real-world scenarios: backbone architecture, training data\ncomposition, pre-processing strategies and data augmentation combinations. By\nsystematically analyzing these components, we shed light on their impact on\ndetection efficacy. Our modifications result in an average AUC improvement of\n26.87% across various AID models under real-world conditions.", "AI": {"tldr": "论文探讨了AI生成图像检测（AID）在现实世界中的挑战，提出了ITW-SM数据集，并分析了影响检测性能的四个关键因素，最终将AID模型的AUC平均提高了26.87%。", "motivation": "随着生成技术的快速发展，AI生成图像的质量已足以欺骗人类，因此检测其真实性变得至关重要。然而，现有检测模型在现实场景中表现不佳，亟需改进。", "method": "作者引入了ITW-SM数据集，并系统分析了影响AID性能的四个因素：主干架构、训练数据组成、预处理策略和数据增强组合。通过优化这些组件，提升了检测效果。", "result": "优化后的AID模型在现实条件下的平均AUC提高了26.87%，显著提升了检测性能。", "conclusion": "研究表明，通过系统优化关键因素，可以显著提高AI生成图像检测在现实场景中的准确性，为未来研究提供了重要方向。"}}
{"id": "2507.10239", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10239", "abs": "https://arxiv.org/abs/2507.10239", "authors": ["Ben Hamscher", "Edgar Heinert", "Annika Mütze", "Kira Maag", "Matthias Rottmann"], "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks", "comment": "accepted at ECAI 2025", "summary": "Recent research has investigated the shape and texture biases of deep neural\nnetworks (DNNs) in image classification which influence their generalization\ncapabilities and robustness. It has been shown that, in comparison to regular\nDNN training, training with stylized images reduces texture biases in image\nclassification and improves robustness with respect to image corruptions. In an\neffort to advance this line of research, we examine whether style transfer can\nlikewise deliver these two effects in semantic segmentation. To this end, we\nperform style transfer with style varying across artificial image areas. Those\nrandom areas are formed by a chosen number of Voronoi cells. The resulting\nstyle-transferred data is then used to train semantic segmentation DNNs with\nthe objective of reducing their dependence on texture cues while enhancing\ntheir reliance on shape-based features. In our experiments, it turns out that\nin semantic segmentation, style transfer augmentation reduces texture bias and\nstrongly increases robustness with respect to common image corruptions as well\nas adversarial attacks. These observations hold for convolutional neural\nnetworks and transformer architectures on the Cityscapes dataset as well as on\nPASCAL Context, showing the generality of the proposed method.", "AI": {"tldr": "研究探讨了风格迁移在语义分割中减少纹理偏差并提升鲁棒性的效果。", "motivation": "探索风格迁移是否能在语义分割中减少纹理偏差并增强形状特征依赖，从而提高模型鲁棒性。", "method": "通过Voronoi细胞生成随机区域进行风格迁移，用风格化数据训练语义分割DNN。", "result": "风格迁移显著减少纹理偏差，提升对图像损坏和对抗攻击的鲁棒性，适用于多种架构和数据集。", "conclusion": "风格迁移是一种通用方法，可有效减少纹理偏差并增强语义分割模型的鲁棒性。"}}
{"id": "2507.10265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10265", "abs": "https://arxiv.org/abs/2507.10265", "authors": ["Xinlong Ding", "Hongwei Yu", "Jiawei Li", "Feifan Li", "Yu Shang", "Bochao Zou", "Huimin Ma", "Jiansheng Chen"], "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures", "comment": "Accepted at ICCV 2025. Project page is available at\n  https://wakuwu.github.io/KBA", "summary": "Camera pose estimation is a fundamental computer vision task that is\nessential for applications like visual localization and multi-view stereo\nreconstruction. In the object-centric scenarios with sparse inputs, the\naccuracy of pose estimation can be significantly influenced by background\ntextures that occupy major portions of the images across different viewpoints.\nIn light of this, we introduce the Kaleidoscopic Background Attack (KBA), which\nuses identical segments to form discs with multi-fold radial symmetry. These\ndiscs maintain high similarity across different viewpoints, enabling effective\nattacks on pose estimation models even with natural texture segments.\nAdditionally, a projected orientation consistency loss is proposed to optimize\nthe kaleidoscopic segments, leading to significant enhancement in the attack\neffectiveness. Experimental results show that optimized adversarial\nkaleidoscopic backgrounds can effectively attack various camera pose estimation\nmodels.", "AI": {"tldr": "论文提出了一种名为Kaleidoscopic Background Attack (KBA)的方法，通过使用对称纹理背景攻击相机位姿估计模型，并提出了投影方向一致性损失优化攻击效果。", "motivation": "在稀疏输入的物体中心场景中，背景纹理可能显著影响相机位姿估计的准确性，因此需要一种有效的攻击方法。", "method": "使用多折叠径向对称的相同片段形成圆盘，提出投影方向一致性损失优化这些片段。", "result": "实验表明，优化的对抗性背景能有效攻击多种相机位姿估计模型。", "conclusion": "KBA方法通过对称背景和优化损失显著提升了攻击效果。"}}
{"id": "2507.10283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10283", "abs": "https://arxiv.org/abs/2507.10283", "authors": ["Muyi Bao", "Changyu Zeng", "Yifan Wang", "Zhengni Yang", "Zimu Wang", "Guangliang Cheng", "Jun Qi", "Wei Wang"], "title": "FTCFormer: Fuzzy Token Clustering Transformer for Image Classification", "comment": null, "summary": "Transformer-based deep neural networks have achieved remarkable success\nacross various computer vision tasks, largely attributed to their long-range\nself-attention mechanism and scalability. However, most transformer\narchitectures embed images into uniform, grid-based vision tokens, neglecting\nthe underlying semantic meanings of image regions, resulting in suboptimal\nfeature representations. To address this issue, we propose Fuzzy Token\nClustering Transformer (FTCFormer), which incorporates a novel clustering-based\ndownsampling module to dynamically generate vision tokens based on the semantic\nmeanings instead of spatial positions. It allocates fewer tokens to less\ninformative regions and more to represent semantically important regions,\nregardless of their spatial adjacency or shape irregularity. To further enhance\nfeature extraction and representation, we propose a Density Peak\nClustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center\ndetermination, a Spatial Connectivity Score (SCS) for token assignment, and a\nchannel-wise merging (Cmerge) strategy for token merging. Extensive experiments\non 32 datasets across diverse domains validate the effectiveness of FTCFormer\non image classification, showing consistent improvements over the TCFormer\nbaseline, achieving gains of improving 1.43% on five fine-grained datasets,\n1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%\non four remote sensing datasets. The code is available at:\nhttps://github.com/BaoBao0926/FTCFormer/tree/main.", "AI": {"tldr": "FTCFormer是一种基于模糊聚类的Transformer模型，通过动态生成语义相关的视觉标记，优化图像特征表示。", "motivation": "传统Transformer模型将图像嵌入为均匀的网格标记，忽略了语义信息，导致特征表示不理想。", "method": "提出FTCFormer，结合聚类下采样模块、DPC-FKNN机制、SCS评分和Cmerge策略，动态生成语义标记。", "result": "在32个数据集上验证，FTCFormer在图像分类任务中表现优于基线，提升1.43%（细粒度数据集）、1.09%（自然图像）、0.97%（医学图像）和0.55%（遥感图像）。", "conclusion": "FTCFormer通过语义驱动的标记生成，显著提升了图像分类性能。"}}
{"id": "2507.10293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10293", "abs": "https://arxiv.org/abs/2507.10293", "authors": ["Wenkang Han", "Wang Lin", "Yiyun Zhou", "Qi Liu", "Shulei Wang", "Chang Yao", "Jingyuan Chen"], "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration", "comment": "Accepted by MM 2025", "summary": "Face Video Restoration (FVR) aims to recover high-quality face videos from\ndegraded versions. Traditional methods struggle to preserve fine-grained,\nidentity-specific features when degradation is severe, often producing\naverage-looking faces that lack individual characteristics. To address these\nchallenges, we introduce IP-FVR, a novel method that leverages a high-quality\nreference face image as a visual prompt to provide identity conditioning during\nthe denoising process. IP-FVR incorporates semantically rich identity\ninformation from the reference image using decoupled cross-attention\nmechanisms, ensuring detailed and identity consistent results. For intra-clip\nidentity drift (within 24 frames), we introduce an identity-preserving feedback\nlearning method that combines cosine similarity-based reward signals with\nsuffix-weighted temporal aggregation. This approach effectively minimizes drift\nwithin sequences of frames. For inter-clip identity drift, we develop an\nexponential blending strategy that aligns identities across clips by\niteratively blending frames from previous clips during the denoising process.\nThis method ensures consistent identity representation across different clips.\nAdditionally, we enhance the restoration process with a multi-stream negative\nprompt, guiding the model's attention to relevant facial attributes and\nminimizing the generation of low-quality or incorrect features. Extensive\nexperiments on both synthetic and real-world datasets demonstrate that IP-FVR\noutperforms existing methods in both quality and identity preservation,\nshowcasing its substantial potential for practical applications in face video\nrestoration.", "AI": {"tldr": "IP-FVR是一种新方法，利用高质量参考图像作为视觉提示，通过解耦交叉注意力机制和反馈学习策略，解决人脸视频恢复中的身份漂移问题。", "motivation": "传统方法在严重退化情况下难以保留细粒度和身份特征，导致结果缺乏个体特性。", "method": "IP-FVR采用参考图像提供身份条件，结合解耦交叉注意力、反馈学习和指数混合策略，确保身份一致性。", "result": "在合成和真实数据集上，IP-FVR在质量和身份保留方面优于现有方法。", "conclusion": "IP-FVR在面部视频恢复中具有实际应用潜力。"}}
{"id": "2507.10302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10302", "abs": "https://arxiv.org/abs/2507.10302", "authors": ["Jiahe Zhao", "Rongkun Zheng", "Yi Wang", "Helin Wang", "Hengshuang Zhao"], "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs", "comment": "ICCV 2025", "summary": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo.", "AI": {"tldr": "DisCo是一种新颖的视觉封装方法，通过结合视觉概念判别器和时间焦点校准器，解决了视频MLLM中的语义模糊和时间不连贯问题。", "motivation": "线性投影器在视频MLLM中导致语义模糊和时间不连贯，而现有方法未能有效解决这些问题。", "method": "DisCo包含两个模块：视觉概念判别器（VCD）和时间焦点校准器（TFC），分别用于增强语义区分和时间一致性。", "result": "实验表明，DisCo在多个视频理解基准上显著优于现有方法，并提高了标记效率。", "conclusion": "DisCo为视频MLLM提供了一种高效且性能优越的视觉封装解决方案。"}}
{"id": "2507.10306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10306", "abs": "https://arxiv.org/abs/2507.10306", "authors": ["Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation", "comment": "Accepted at 9th Workshop on Sign Language Translation and Avatar\n  Technologies (SLTAT), will be held in conjunction with IVA'25", "summary": "Sign Language Translation (SLT) aims to convert sign language videos into\nspoken or written text. While early systems relied on gloss annotations as an\nintermediate supervision, such annotations are costly to obtain and often fail\nto capture the full complexity of continuous signing. In this work, we propose\na two-phase, dual visual encoder framework for gloss-free SLT, leveraging\ncontrastive visual-language pretraining. During pretraining, our approach\nemploys two complementary visual backbones whose outputs are jointly aligned\nwith each other and with sentence-level text embeddings via a contrastive\nobjective. During the downstream SLT task, we fuse the visual features and\ninput them into an encoder-decoder model. On the Phoenix-2014T benchmark, our\ndual encoder architecture consistently outperforms its single stream variants\nand achieves the highest BLEU-4 score among existing gloss-free SLT approaches.", "AI": {"tldr": "提出了一种基于双视觉编码器的无注释手语翻译框架，通过对比视觉-语言预训练提升性能。", "motivation": "传统方法依赖昂贵且不完整的注释，无法捕捉连续手语的复杂性。", "method": "采用双视觉编码器进行对比预训练，下游任务中融合视觉特征输入编码器-解码器模型。", "result": "在Phoenix-2014T基准测试中，性能优于单流变体，达到无注释方法的最高BLEU-4分数。", "conclusion": "双编码器框架在无注释手语翻译中表现优越，验证了对比预训练的有效性。"}}
{"id": "2507.10318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10318", "abs": "https://arxiv.org/abs/2507.10318", "authors": ["Yuhan Liu", "Jingwen Fu", "Yang Wu", "Kangyi Wu", "Pengna Li", "Jiayi Wu", "Sanping Zhou", "Jingmin Xin"], "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching", "comment": "Accepted by ICCV 2025", "summary": "Leveraging the vision foundation models has emerged as a mainstream paradigm\nthat improves the performance of image feature matching. However, previous\nworks have ignored the misalignment when introducing the foundation models into\nfeature matching. The misalignment arises from the discrepancy between the\nfoundation models focusing on single-image understanding and the cross-image\nunderstanding requirement of feature matching. Specifically, 1) the embeddings\nderived from commonly used foundation models exhibit discrepancies with the\noptimal embeddings required for feature matching; 2) lacking an effective\nmechanism to leverage the single-image understanding ability into cross-image\nunderstanding. A significant consequence of the misalignment is they struggle\nwhen addressing multi-instance feature matching problems. To address this, we\nintroduce a simple but effective framework, called IMD (Image feature Matching\nwith a pre-trained Diffusion model) with two parts: 1) Unlike the dominant\nsolutions employing contrastive-learning based foundation models that emphasize\nglobal semantics, we integrate the generative-based diffusion models to\neffectively capture instance-level details. 2) We leverage the prompt mechanism\nin generative model as a natural tunnel, propose a novel cross-image\ninteraction prompting module to facilitate bidirectional information\ninteraction between image pairs. To more accurately measure the misalignment,\nwe propose a new benchmark called IMIM, which focuses on multi-instance\nscenarios. Our proposed IMD establishes a new state-of-the-art in commonly\nevaluated benchmarks, and the superior improvement 12% in IMIM indicates our\nmethod efficiently mitigates the misalignment.", "AI": {"tldr": "论文提出了一种名为IMD的框架，通过结合生成式扩散模型和跨图像交互提示模块，解决了视觉基础模型在特征匹配中的对齐问题，并在多实例场景中取得了显著改进。", "motivation": "现有研究在将视觉基础模型引入特征匹配时忽略了单图像理解与跨图像理解之间的不对齐问题，导致在多实例特征匹配中表现不佳。", "method": "IMD框架包括两部分：1）使用生成式扩散模型捕捉实例级细节；2）提出跨图像交互提示模块，促进图像对之间的双向信息交互。", "result": "IMD在常用基准测试中达到新SOTA，在多实例基准IMIM上提升了12%。", "conclusion": "IMD通过解决不对齐问题，显著提升了特征匹配性能，尤其是在多实例场景中。"}}
{"id": "2507.10340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10340", "abs": "https://arxiv.org/abs/2507.10340", "authors": ["Hongjae Lee", "Myungjun Son", "Dongjea Kang", "Seung-Won Jung"], "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models", "comment": "ICCV 2025", "summary": "Despite the success of diffusion models in image generation tasks such as\ntext-to-image, the enormous computational complexity of diffusion models limits\ntheir use in resource-constrained environments. To address this, network\nquantization has emerged as a promising solution for designing efficient\ndiffusion models. However, existing diffusion model quantization methods do not\nconsider input conditions, such as text prompts, as an essential source of\ninformation for quantization. In this paper, we propose a novel quantization\nmethod dubbed Quantization of Language-to-Image diffusion models using text\nPrompts (QLIP). QLIP leverages text prompts to guide the selection of bit\nprecision for every layer at each time step. In addition, QLIP can be\nseamlessly integrated into existing quantization methods to enhance\nquantization efficiency. Our extensive experiments demonstrate the\neffectiveness of QLIP in reducing computational complexity and improving the\nquality of the generated images across various datasets.", "AI": {"tldr": "QLIP是一种新的量化方法，利用文本提示为扩散模型选择每层和每个时间步的比特精度，降低计算复杂度并提升生成图像质量。", "motivation": "扩散模型在图像生成任务中表现出色，但计算复杂度高，限制了其在资源受限环境中的应用。现有量化方法未考虑输入条件（如文本提示）的重要性。", "method": "提出QLIP方法，利用文本提示指导每层和时间步的比特精度选择，并可无缝集成到现有量化方法中。", "result": "实验表明，QLIP能有效降低计算复杂度，并在多个数据集上提升生成图像质量。", "conclusion": "QLIP通过结合文本提示优化量化过程，为高效扩散模型设计提供了新思路。"}}
{"id": "2507.10343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10343", "abs": "https://arxiv.org/abs/2507.10343", "authors": ["Hugo Norrby", "Gabriel Färm", "Kevin Hernandez-Diaz", "Fernando Alonso-Fernandez"], "title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans", "comment": "Accepted at International Workshop on Artificial Intelligence and\n  Pattern Recognition, IWAIPR 2025", "summary": "We introduce FGSSNet, a novel multi-headed feature-guided semantic\nsegmentation (FGSS) architecture designed to improve the generalization ability\nof wall segmentation on floorplans. FGSSNet features a U-Net segmentation\nbackbone with a multi-headed dedicated feature extractor used to extract\ndomain-specific feature maps which are injected into the latent space of U-Net\nto guide the segmentation process. This dedicated feature extractor is trained\nas an encoder-decoder with selected wall patches, representative of the walls\npresent in the input floorplan, to produce a compressed latent representation\nof wall patches while jointly trained to predict the wall width. In doing so,\nwe expect that the feature extractor encodes texture and width features of wall\npatches that are useful to guide the wall segmentation process. Our experiments\nshow increased performance by the use of such injected features in comparison\nto the vanilla U-Net, highlighting the validity of the proposed approach.", "AI": {"tldr": "FGSSNet是一种多头部特征引导的语义分割架构，旨在提高平面图中墙壁分割的泛化能力。", "motivation": "改进平面图中墙壁分割的泛化能力。", "method": "使用U-Net分割主干和多头部专用特征提取器，提取领域特定特征图并注入U-Net潜在空间以指导分割过程。特征提取器通过训练编码-解码结构和预测墙壁宽度来压缩墙壁补丁的潜在表示。", "result": "实验表明，注入特征的使用相比普通U-Net提高了性能。", "conclusion": "FGSSNet通过特征引导有效提升了墙壁分割的准确性。"}}
{"id": "2507.10355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10355", "abs": "https://arxiv.org/abs/2507.10355", "authors": ["Bo Jiang", "Xueyang Ze", "Beibei Wang", "Xixi Wang", "Xixi Wan", "Bin Luo"], "title": "Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter", "comment": null, "summary": "Textual adapter-based tuning methods have shown significant potential in\ntransferring knowledge from pre-trained Vision-Language Models (VLMs) to\ndownstream tasks. Existing works generally employ the deterministic textual\nfeature adapter to refine each category textual representation. However, due to\ninherent factors such as different attributes and contexts, there exists\nsignificant diversity in textual descriptions for each category. Such\ndescription diversity offers rich discriminative semantic knowledge that can\nbenefit downstream visual learning tasks. Obviously, traditional deterministic\nadapter model cannot adequately capture this varied semantic information. Also,\nit is desirable to exploit the inter-class relationships in VLM adapter. To\naddress these issues, we propose to exploit random graph model into VLM adapter\nand develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first\nmodels the inherent diverse descriptions of each category and inter-class\nrelationships of different categories simultaneously by leveraging a Vertex\nRandom Knowledge Graph (VRKG) model. Then, it employs probabilistic message\npropagation on VRKG to learn context-aware distribution representation for each\nclass node. Finally, it adopts a reparameterized sampling function to achieve\ntextual adapter learning. Note that, VRGAdapter provides a more general adapter\nsolution that encompasses traditional graph-based adapter as a special case. In\naddition, to enable more robust performance for downstream tasks, we also\nintroduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that\ndynamically integrates multiple pre-trained models for ensemble prediction.\nExtensive experiments on multiple benchmark datasets demonstrate the\neffectiveness of our approach.", "AI": {"tldr": "提出了一种基于随机图模型的文本适配器（VRGAdapter），通过顶点随机知识图（VRKG）建模类别多样性和类间关系，并结合不确定性引导的多分支融合（UMF）提升下游任务性能。", "motivation": "现有确定性文本适配器无法充分捕捉类别描述的多样性和类间关系，限制了视觉语言模型（VLM）在下游任务中的潜力。", "method": "利用顶点随机知识图（VRKG）建模类别多样性和类间关系，通过概率消息传播学习上下文感知分布表示，并采用重参数化采样实现适配器学习。结合UMF动态集成多个预训练模型。", "result": "在多个基准数据集上的实验验证了VRGAdapter的有效性。", "conclusion": "VRGAdapter提供了一种更通用的适配器解决方案，显著提升了视觉语言模型在下游任务中的性能。"}}
{"id": "2507.10358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10358", "abs": "https://arxiv.org/abs/2507.10358", "authors": ["Hongxu Ma", "Chenbo Zhang", "Lu Zhang", "Jiaogen Zhou", "Jihong Guan", "Shuigeng Zhou"], "title": "Fine-Grained Zero-Shot Object Detection", "comment": "Accepted by ACM MM'25", "summary": "Zero-shot object detection (ZSD) aims to leverage semantic descriptions to\nlocalize and recognize objects of both seen and unseen classes. Existing ZSD\nworks are mainly coarse-grained object detection, where the classes are\nvisually quite different, thus are relatively easy to distinguish. However, in\nreal life we often have to face fine-grained object detection scenarios, where\nthe classes are too similar to be easily distinguished. For example, detecting\ndifferent kinds of birds, fishes, and flowers.\n  In this paper, we propose and solve a new problem called Fine-Grained\nZero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of\ndifferent classes with minute differences in details under the ZSD paradigm. We\ndevelop an effective method called MSHC for the FG-ZSD task, which is based on\nan improved two-stage detector and employs a multi-level semantics-aware\nembedding alignment loss, ensuring tight coupling between the visual and\nsemantic spaces. Considering that existing ZSD datasets are not suitable for\nthe new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,\nwhich contains 148,820 images falling into 36 orders, 140 families, 579 genera\nand 1432 species. Extensive experiments on FGZSD-Birds show that our method\noutperforms existing ZSD models.", "AI": {"tldr": "本文提出并解决了细粒度零样本目标检测（FG-ZSD）问题，开发了基于改进两阶段检测器的MSHC方法，并构建了首个FG-ZSD基准数据集FGZSD-Birds。", "motivation": "现实中的细粒度目标检测场景（如鸟类、鱼类、花卉分类）需要区分视觉差异极小的类别，而现有零样本目标检测方法主要针对粗粒度任务，难以胜任。", "method": "提出MSHC方法，基于改进的两阶段检测器，采用多级语义感知嵌入对齐损失，确保视觉与语义空间的紧密耦合。", "result": "在构建的FGZSD-Birds数据集上，MSHC方法优于现有零样本目标检测模型。", "conclusion": "FG-ZSD是一个重要且具有挑战性的新问题，MSHC方法为解决该问题提供了有效方案。"}}
{"id": "2507.10375", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10375", "abs": "https://arxiv.org/abs/2507.10375", "authors": ["Utkarsh Singhal", "Ryan Feng", "Stella X. Yu", "Atul Prakash"], "title": "Test-Time Canonicalization by Foundation Models for Robust Perception", "comment": "Published at ICML 2025", "summary": "Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal.", "AI": {"tldr": "FOCAL是一种测试时、数据驱动的框架，利用基础模型的互联网规模视觉先验，通过生成和优化候选变换来实现鲁棒感知，无需重新训练或架构更改。", "motivation": "当前方法依赖专用架构或预定义增强训练，限制了泛化能力，FOCAL旨在通过视觉先验提升鲁棒性。", "method": "生成并优化候选变换，使其趋向视觉典型的“规范”视图，利用基础模型的先验知识。", "result": "实验显示FOCAL提升了CLIP和SAM在2D/3D旋转、光照变化和昼夜变化等挑战性变换中的鲁棒性。", "conclusion": "FOCAL挑战了变换特定训练的必要性，提供了一种可扩展的鲁棒性实现路径。"}}
{"id": "2507.10381", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10381", "abs": "https://arxiv.org/abs/2507.10381", "authors": ["Aaryam Sharma"], "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks", "comment": "9 pages, 8 figures", "summary": "Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.", "AI": {"tldr": "论文提出了一种将拓扑数据分析（TDA）特征与深度学习模型结合的方法，显著提升了遥感图像分类的准确性。", "motivation": "卷积神经网络（CNNs）在图像分类中偏向于纹理特征，而TDA能捕捉几何信息，弥补这一不足。", "method": "设计了一个TDA特征工程流程，并将其与ResNet18结合，用于遥感图像分类。", "result": "在EuroSAT数据集上达到99.33%准确率，提升1.44%；在RESISC45数据集上比基线高1.82%。", "conclusion": "TDA特征能与深度学习结合，扩展了TDA的适用性，即使在没有明显拓扑结构的数据集上。"}}
{"id": "2507.10407", "categories": ["cs.CV", "cs.SC", "math.AG", "68W30"], "pdf": "https://arxiv.org/pdf/2507.10407", "abs": "https://arxiv.org/abs/2507.10407", "authors": ["Timothy Duff"], "title": "Numerically Computing Galois Groups of Minimal Problems", "comment": "abstract accompanying invited tutorial at ISSAC 2025; 10 pages w/\n  references", "summary": "I discuss a seemingly unlikely confluence of topics in algebra, numerical\ncomputation, and computer vision. The motivating problem is that of solving\nmultiples instances of a parametric family of systems of algebraic (polynomial\nor rational function) equations. No doubt already of interest to ISSAC\nattendees, this problem arises in the context of robust model-fitting paradigms\ncurrently utilized by the computer vision community (namely \"Random Sampling\nand Consensus\", aka \"RanSaC\".) This talk will give an overview of work in the\nlast 5+ years that aspires to measure the intrinsic difficulty of solving such\nparametric systems, and makes strides towards practical solutions.", "AI": {"tldr": "论文探讨了代数、数值计算和计算机视觉中参数化代数方程组的求解问题，特别是与RanSaC模型相关的应用。", "motivation": "研究参数化代数方程组的求解问题，源于计算机视觉中RanSaC模型的实际需求。", "method": "通过过去五年的研究，分析了求解此类系统的内在难度，并提出了实用解决方案。", "result": "研究在衡量求解难度和提出实用方法方面取得了进展。", "conclusion": "论文为参数化代数方程组的求解提供了理论支持和实用方法，对相关领域有重要意义。"}}
{"id": "2507.10432", "categories": ["cs.CV", "I.4.7"], "pdf": "https://arxiv.org/pdf/2507.10432", "abs": "https://arxiv.org/abs/2507.10432", "authors": ["Qiang Li", "Qingsen Yan", "Haojian Huang", "Peng Wu", "Haokui Zhang", "Yanning Zhang"], "title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment", "comment": "9 pages, 5 figures, Accepted at ACMMM 2025", "summary": "With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.", "AI": {"tldr": "提出了一种名为SC-AGIQA的统一框架，通过文本-视觉语义约束提升AI生成图像的质量评估，解决了语义对齐和细节感知缺失的问题。", "motivation": "现有方法在评估AI生成图像时存在语义不对齐和细节感知缺失的问题，需要一种更全面的评估方法。", "method": "结合文本辅助语义对齐模块（TSAM）和频域细粒度退化感知模块（FFDPM），利用多模态大语言模型和频域分析提升评估效果。", "result": "在多个基准数据集上的实验表明，SC-AGIQA优于现有最先进方法。", "conclusion": "SC-AGIQA通过语义约束和频域分析，显著提升了AI生成图像的质量评估效果。"}}
{"id": "2507.10437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10437", "abs": "https://arxiv.org/abs/2507.10437", "authors": ["Shanshan Zhong", "Jiawei Peng", "Zehan Zheng", "Zhongzhan Huang", "Wufei Ma", "Guofeng Zhang", "Qihao Liu", "Alan Yuille", "Jieneng Chen"], "title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos", "comment": null, "summary": "Existing methods for reconstructing animatable 3D animals from videos\ntypically rely on sparse semantic keypoints to fit parametric models. However,\nobtaining such keypoints is labor-intensive, and keypoint detectors trained on\nlimited animal data are often unreliable. To address this, we propose\n4D-Animal, a novel framework that reconstructs animatable 3D animals from\nvideos without requiring sparse keypoint annotations. Our approach introduces a\ndense feature network that maps 2D representations to SMAL parameters,\nenhancing both the efficiency and stability of the fitting process.\nFurthermore, we develop a hierarchical alignment strategy that integrates\nsilhouette, part-level, pixel-level, and temporal cues from pre-trained 2D\nvisual models to produce accurate and temporally coherent reconstructions\nacross frames. Extensive experiments demonstrate that 4D-Animal outperforms\nboth model-based and model-free baselines. Moreover, the high-quality 3D assets\ngenerated by our method can benefit other 3D tasks, underscoring its potential\nfor large-scale applications. The code is released at\nhttps://github.com/zhongshsh/4D-Animal.", "AI": {"tldr": "4D-Animal是一种无需稀疏关键点注释即可从视频重建可动画3D动物的新框架，通过密集特征网络和分层对齐策略提高效率和稳定性。", "motivation": "现有方法依赖稀疏语义关键点，获取成本高且不可靠，因此提出无需关键点注释的解决方案。", "method": "引入密集特征网络将2D表示映射到SMAL参数，并结合分层对齐策略（轮廓、部分级、像素级和时间线索）。", "result": "实验表明4D-Animal优于基于模型和无模型的基线方法，生成的高质量3D资产可用于其他任务。", "conclusion": "4D-Animal具有大规模应用潜力，代码已开源。"}}
{"id": "2507.10470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10470", "abs": "https://arxiv.org/abs/2507.10470", "authors": ["Zhicun Yin", "Junjie Chen", "Ming Liu", "Zhixin Wang", "Fan Li", "Renjing Pei", "Xiaoming Li", "Rynson W. H. Lau", "Wangmeng Zuo"], "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction", "comment": null, "summary": "Blind facial image restoration is highly challenging due to unknown complex\ndegradations and the sensitivity of humans to faces. Although existing methods\nintroduce auxiliary information from generative priors or high-quality\nreference images, they still struggle with identity preservation problems,\nmainly due to improper feature introduction on detailed textures. In this\npaper, we focus on effectively incorporating appropriate features from\nhigh-quality reference images, presenting a novel blind facial image\nrestoration method that considers reference selection, transfer, and\nreconstruction (RefSTAR). In terms of selection, we construct a reference\nselection (RefSel) module. For training the RefSel module, we construct a\nRefSel-HQ dataset through a mask generation pipeline, which contains annotating\nmasks for 10,000 ground truth-reference pairs. As for the transfer, due to the\ntrivial solution in vanilla cross-attention operations, a feature fusion\nparadigm is designed to force the features from the reference to be integrated.\nFinally, we propose a reference image reconstruction mechanism that further\nensures the presence of reference image features in the output image. The cycle\nconsistency loss is also redesigned in conjunction with the mask. Extensive\nexperiments on various backbone models demonstrate superior performance,\nshowing better identity preservation ability and reference feature transfer\nquality. Source code, dataset, and pre-trained models are available at\nhttps://github.com/yinzhicun/RefSTAR.", "AI": {"tldr": "本文提出了一种名为RefSTAR的盲人脸图像恢复方法，通过参考图像的选择、转移和重建，解决了现有方法在身份保持和纹理细节上的不足。", "motivation": "盲人脸图像恢复因未知的复杂退化和人类对脸部的敏感性而极具挑战性，现有方法在身份保持上存在问题。", "method": "提出RefSTAR方法，包括参考选择模块（RefSel）、特征融合范式和参考图像重建机制，并设计了循环一致性损失。", "result": "实验表明，RefSTAR在身份保持和参考特征转移质量上表现优异。", "conclusion": "RefSTAR通过有效整合参考图像特征，显著提升了盲人脸图像恢复的性能。"}}
{"id": "2507.10473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10473", "abs": "https://arxiv.org/abs/2507.10473", "authors": ["David G. Shatwell", "Ishan Rajendrakumar Dave", "Sirnam Swetha", "Mubarak Shah"], "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space", "comment": "Accepted in ICCV2025", "summary": "Timestamp prediction aims to determine when an image was captured using only\nvisual information, supporting applications such as metadata correction,\nretrieval, and digital forensics. In outdoor scenarios, hourly estimates rely\non cues like brightness, hue, and shadow positioning, while seasonal changes\nand weather inform date estimation. However, these visual cues significantly\ndepend on geographic context, closely linking timestamp prediction to\ngeo-localization. To address this interdependence, we introduce GT-Loc, a novel\nretrieval-based method that jointly predicts the capture time (hour and month)\nand geo-location (GPS coordinates) of an image. Our approach employs separate\nencoders for images, time, and location, aligning their embeddings within a\nshared high-dimensional feature space. Recognizing the cyclical nature of time,\ninstead of conventional contrastive learning with hard positives and negatives,\nwe propose a temporal metric-learning objective providing soft targets by\nmodeling pairwise time differences over a cyclical toroidal surface. We present\nnew benchmarks demonstrating that our joint optimization surpasses previous\ntime prediction methods, even those using the ground-truth geo-location as an\ninput during inference. Additionally, our approach achieves competitive results\non standard geo-localization tasks, and the unified embedding space facilitates\ncompositional and text-based image retrieval.", "AI": {"tldr": "GT-Loc是一种新颖的检索方法，联合预测图像的拍摄时间（小时和月份）和地理位置（GPS坐标），通过共享高维特征空间对齐嵌入，优于现有时间预测方法。", "motivation": "解决时间戳预测与地理定位的相互依赖问题，支持元数据校正、检索和数字取证等应用。", "method": "采用图像、时间和位置的独立编码器，在共享特征空间中对齐嵌入，提出基于循环环形表面的时间度量学习目标。", "result": "GT-Loc在时间预测上超越现有方法，即使后者使用真实地理位置输入，同时在地理定位任务中表现优异。", "conclusion": "GT-Loc通过联合优化时间与地理定位，实现了高效的时间预测和地理定位，并支持组合和基于文本的图像检索。"}}
{"id": "2507.10490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10490", "abs": "https://arxiv.org/abs/2507.10490", "authors": ["Tugberk Erol", "Tuba Caglikantar", "Duygu Sarikaya"], "title": "The Power of Certainty: How Confident Models Lead to Better Segmentation", "comment": "9 pages, 3 figures", "summary": "Deep learning models have been proposed for automatic polyp detection and\nprecise segmentation of polyps during colonoscopy procedures. Although these\nstate-of-the-art models achieve high performance, they often require a large\nnumber of parameters. Their complexity can make them prone to overfitting,\nparticularly when trained on biased datasets, and can result in poor\ngeneralization across diverse datasets. Knowledge distillation and\nself-distillation are proposed as promising strategies to mitigate the\nlimitations of large, over-parameterized models. These approaches, however, are\nresource-intensive, often requiring multiple models and significant memory\nduring training. We propose a confidence-based self-distillation approach that\noutperforms state-of-the-art models by utilizing only previous iteration data\nstorage during training, without requiring extra computation or memory usage\nduring testing. Our approach calculates the loss between the previous and\ncurrent iterations within a batch using a dynamic confidence coefficient. To\nevaluate the effectiveness of our approach, we conduct comprehensive\nexperiments on the task of polyp segmentation. Our approach outperforms\nstate-of-the-art models and generalizes well across datasets collected from\nmultiple clinical centers. The code will be released to the public once the\npaper is accepted.", "AI": {"tldr": "提出了一种基于置信度的自蒸馏方法，用于改进结肠镜检查中的息肉分割，减少资源消耗并提升性能。", "motivation": "现有深度学习模型参数量大，易过拟合且泛化能力差，知识蒸馏方法资源消耗高。", "method": "利用动态置信系数计算批次内前后迭代的损失，仅需存储前一次迭代数据，无需额外计算或内存。", "result": "在息肉分割任务上表现优于现有最优模型，并在多临床中心数据集上泛化良好。", "conclusion": "提出的方法高效且性能优越，代码将公开。"}}
{"id": "2507.10499", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10499", "abs": "https://arxiv.org/abs/2507.10499", "authors": ["Philippe Rufin", "Pauline Lucie Hammer", "Leon-Friedrich Thomas", "Sá Nogueira Lisboa", "Natasha Ribeiro", "Almeida Sitoe", "Patrick Hostert", "Patrick Meyfroidt"], "title": "National level satellite-based crop field inventories in smallholder landscapes", "comment": null, "summary": "The design of science-based policies to improve the sustainability of\nsmallholder agriculture is challenged by a limited understanding of fundamental\nsystem properties, such as the spatial distribution of active cropland and\nfield size. We integrate very high spatial resolution (1.5 m) Earth observation\ndata and deep transfer learning to derive crop field delineations in complex\nagricultural systems at the national scale, while maintaining minimum reference\ndata requirements and enhancing transferability. We provide the first\nnational-level dataset of 21 million individual fields for Mozambique (covering\n~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural\nland use with an overall accuracy of 93% and balanced omission and commission\nerrors. Field-level spatial agreement reached median intersection over union\n(IoU) scores of 0.81, advancing the state-of-the-art in large-area field\ndelineation in complex smallholder systems. The active cropland maps capture\nfragmented rural regions with low cropland shares not yet identified in global\nland cover or cropland maps. These regions are mostly located in agricultural\nfrontier regions which host 7-9% of the Mozambican population. Field size in\nMozambique is very low overall, with half of the fields being smaller than 0.16\nha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial\nresolution (0.05{\\deg}) is 0.32 ha, but it varies strongly across gradients of\naccessibility, population density, and net forest cover change. This variation\nreflects a diverse set of actors, ranging from semi-subsistence smallholder\nfarms to medium-scale commercial farming, and large-scale farming operations.\nOur results highlight that field size is a key indicator relating to\nsocio-economic and environmental outcomes of agriculture (e.g., food\nproduction, livelihoods, deforestation, biodiversity), as well as their\ntrade-offs.", "AI": {"tldr": "该研究利用高分辨率地球观测数据和深度学习技术，首次为莫桑比克提供了全国范围的农田分布和地块大小数据，揭示了小农农业系统的复杂性及其与经济社会环境的关系。", "motivation": "设计基于科学的政策以提升小农农业的可持续性，但缺乏对农田分布和地块大小等基础系统属性的理解。", "method": "整合高分辨率（1.5米）地球观测数据和深度迁移学习，生成全国尺度的农田边界图，同时减少参考数据需求并提升可迁移性。", "result": "提供了莫桑比克2023年2100万块农田的数据集，总体精度达93%，地块级空间一致性中位数IoU为0.81。揭示了农田碎片化和地块大小的多样性及其与经济社会环境的关系。", "conclusion": "地块大小是农业社会经济和环境结果（如粮食生产、生计、森林砍伐）及其权衡的关键指标。"}}
{"id": "2507.10547", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10547", "abs": "https://arxiv.org/abs/2507.10547", "authors": ["Borui Zhang", "Qihang Rao", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "title": "Quantize-then-Rectify: Efficient VQ-VAE Training", "comment": null, "summary": "Visual tokenizers are pivotal in multimodal large models, acting as bridges\nbetween continuous inputs and discrete tokens. Nevertheless, training\nhigh-compression-rate VQ-VAEs remains computationally demanding, often\nnecessitating thousands of GPU hours. This work demonstrates that a pre-trained\nVAE can be efficiently transformed into a VQ-VAE by controlling quantization\nnoise within the VAE's tolerance threshold. We present\n\\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs\nto enable rapid VQ-VAE training with minimal computational overhead. By\nintegrating \\textbf{channel multi-group quantization} to enlarge codebook\ncapacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ\ncompresses ImageNet images into at most 512 tokens while sustaining competitive\nreconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training\ncosts by over two orders of magnitude relative to state-of-the-art approaches:\nReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,\nwhereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental\nresults show that ReVQ achieves superior efficiency-reconstruction trade-offs.", "AI": {"tldr": "ReVQ框架通过预训练的VAE快速训练高压缩率的VQ-VAE，显著降低计算成本。", "motivation": "训练高压缩率的VQ-VAE通常需要大量计算资源，ReVQ旨在解决这一问题。", "method": "利用预训练的VAE，结合通道多组量化和后矫正器，减少量化误差。", "result": "ReVQ在ImageNet上仅需512个token，重建质量保持竞争力（rFID=1.06），训练成本降低两个数量级。", "conclusion": "ReVQ在效率和重建质量之间取得了优越的平衡，显著降低了训练成本。"}}
