{"id": "2507.06261", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06261", "abs": "https://arxiv.org/abs/2507.06261", "authors": ["Gheorghe Comanici", "Eric Bieber", "Mike Schaekermann", "Ice Pasupat", "Noveen Sachdeva", "Inderjit Dhillon", "Marcel Blistein", "Ori Ram", "Dan Zhang", "Evan Rosen", "Luke Marris", "Sam Petulla", "Colin Gaffney", "Asaf Aharoni", "Nathan Lintz", "Tiago Cardal Pais", "Henrik Jacobsson", "Idan Szpektor", "Nan-Jiang Jiang", "Krishna Haridasan", "Ahmed Omran", "Nikunj Saunshi", "Dara Bahri", "Gaurav Mishra", "Eric Chu", "Toby Boyd", "Brad Hekman", "Aaron Parisi", "Chaoyi Zhang", "Kornraphop Kawintiranon", "Tania Bedrax-Weiss", "Oliver Wang", "Ya Xu", "Ollie Purkiss", "Uri Mendlovic", "Ilaï Deutel", "Nam Nguyen", "Adam Langley", "Flip Korn", "Lucia Rossazza", "Alexandre Ramé", "Sagar Waghmare", "Helen Miller", "Vaishakh Keshava", "Ying Jian", "Xiaofan Zhang", "Raluca Ada Popa", "Kedar Dhamdhere", "Blaž Bratanič", "Kyuyeun Kim", "Terry Koo", "Ferran Alet", "Yi-ting Chen", "Arsha Nagrani", "Hannah Muckenhirn", "Zhiyuan Zhang", "Corbin Quick", "Filip Pavetić", "Duc Dung Nguyen", "Joao Carreira", "Michael Elabd", "Haroon Qureshi", "Fabian Mentzer", "Yao-Yuan Yang", "Danielle Eisenbud", "Anmol Gulati", "Ellie Talius", "Eric Ni", "Sahra Ghalebikesabi", "Edouard Yvinec", "Alaa Saade", "Thatcher Ulrich", "Lorenzo Blanco", "Dan A. Calian", "Muhuan Huang", "Aäron van den Oord", "Naman Goyal", "Terry Chen", "Praynaa Rawlani", "Christian Schallhart", "Swachhand Lokhande", "Xianghong Luo", "Jyn Shan", "Ceslee Montgomery", "Victoria Krakovna", "Federico Piccinini", "Omer Barak", "Jingyu Cui", "Yiling Jia", "Mikhail Dektiarev", "Alexey Kolganov", "Shiyu Huang", "Zhe Chen", "Xingyu Wang", "Jessica Austin", "Peter de Boursac", "Evgeny Sluzhaev", "Frank Ding", "Huijian Li", "Surya Bhupatiraju", "Mohit Agarwal", "Sławek Kwasiborski", "Paramjit Sandhu", "Patrick Siegler", "Ahmet Iscen", "Eyal Ben-David", "Shiraz Butt", "Miltos Allamanis", "Seth Benjamin", "Robert Busa-Fekete", "Felix Hernandez-Campos", "Sasha Goldshtein", "Matt Dibb", "Weiyang Zhang", "Annie Marsden", "Carey Radebaugh", "Stephen Roller", "Abhishek Nayyar", "Jacob Austin", "Tayfun Terzi", "Bhargav Kanagal Shamanna", "Pete Shaw", "Aayush Singh", "Florian Luisier", "Artur Mendonça", "Vaibhav Aggarwal", "Larisa Markeeva", "Claudio Fantacci", "Sergey Brin", "HyunJeong Choe", "Guanyu Wang", "Hartwig Adam", "Avigail Dabush", "Tatsuya Kiyono", "Eyal Marcus", "Jeremy Cole", "Theophane Weber", "Hongrae Lee", "Ronny Huang", "Alex Muzio", "Leandro Kieliger", "Maigo Le", "Courtney Biles", "Long Le", "Archit Sharma", "Chengrun Yang", "Avery Lamp", "Dave Dopson", "Nate Hurley", "Katrina", "Xu", "Zhihao Shan", "Shuang Song", "Jiewen Tan", "Alexandre Senges", "George Zhang", "Chong You", "Yennie Jun", "David Raposo", "Susanna Ricco", "Xuan Yang", "Weijie Chen", "Prakhar Gupta", "Arthur Szlam", "Kevin Villela", "Chun-Sung Ferng", "Daniel Kasenberg", "Chen Liang", "Rui Zhu", "Arunachalam Narayanaswamy", "Florence Perot", "Paul Pucciarelli", "Anna Shekhawat", "Alexey Stern", "Rishikesh Ingale", "Stefani Karp", "Sanaz Bahargam", "Adrian Goedeckemeyer", "Jie Han", "Sicheng Li", "Andrea Tacchetti", "Dian Yu", "Abhishek Chakladar", "Zhiying Zhang", "Mona El Mahdy", "Xu Gao", "Dale Johnson", "Samrat Phatale", "AJ Piergiovanni", "Hyeontaek Lim", "Clement Farabet", "Carl Lebsack", "Theo Guidroz", "John Blitzer", "Nico Duduta", "David Madras", "Steve Li", "Daniel von Dincklage", "Xin Li", "Mahdis Mahdieh", "George Tucker", "Ganesh Jawahar", "Owen Xiao", "Danny Tarlow", "Robert Geirhos", "Noam Velan", "Daniel Vlasic", "Kalesha Bullard", "SK Park", "Nishesh Gupta", "Kellie Webster", "Ayal Hitron", "Jieming Mao", "Julian Eisenschlos", "Laurel Prince", "Nina D'Souza", "Kelvin Zheng", "Sara Nasso", "Gabriela Botea", "Carl Doersch", "Caglar Unlu", "Chris Alberti", "Alexey Svyatkovskiy", "Ankita Goel", "Krzysztof Choromanski", "Pan-Pan Jiang", "Richard Nguyen", "Four Flynn", "Daria Ćurko", "Peter Chen", "Nicholas Roth", "Kieran Milan", "Caleb Habtegebriel", "Shashi Narayan", "Michael Moffitt", "Jake Marcus", "Thomas Anthony", "Brendan McMahan", "Gowoon Cheon", "Ruibo Liu", "Megan Barnes", "Lukasz Lew", "Rebeca Santamaria-Fernandez", "Mayank Upadhyay", "Arjun Akula", "Arnar Mar Hrafnkelsson", "Alvaro Caceres", "Andrew Bunner", "Michal Sokolik", "Subha Puttagunta", "Lawrence Moore", "Berivan Isik", "Weilun Chen", "Jay Hartford", "Lawrence Chan", "Pradeep Shenoy", "Dan Holtmann-Rice", "Jane Park", "Fabio Viola", "Alex Salcianu", "Sujeevan Rajayogam", "Ian Stewart-Binks", "Zelin Wu", "Richard Everett", "Xi Xiong", "Pierre-Antoine Manzagol", "Gary Leung", "Carl Saroufim", "Bo Pang", "Dawid Wegner", "George Papamakarios", "Jennimaria Palomaki", "Helena Pankov", "Guangda Lai", "Guilherme Tubone", "Shubin Zhao", "Theofilos Strinopoulos", "Seth Neel", "Mingqiu Wang", "Joe Kelley", "Li Li", "Pingmei Xu", "Anitha Vijayakumar", "Andrea D'olimpio", "Omer Levy", "Massimo Nicosia", "Grigory Rozhdestvenskiy", "Ni Lao", "Sirui Xie", "Yash Katariya", "Jon Simon", "Sanjiv Kumar", "Florian Hartmann", "Michael Kilgore", "Jinhyuk Lee", "Aroma Mahendru", "Roman Ring", "Tom Hennigan", "Fiona Lang", "Colin Cherry", "David Steiner", "Dawsen Hwang", "Ray Smith", "Pidong Wang", "Jeremy Chen", "Ming-Hsuan Yang", "Sam Kwei", "Philippe Schlattner", "Donnie Kim", "Ganesh Poomal Girirajan", "Nikola Momchev", "Ayushi Agarwal", "Xingyi Zhou", "Ilkin Safarli", "Zachary Garrett", "AJ Pierigiovanni", "Sarthak Jauhari", "Alif Raditya Rochman", "Shikhar Vashishth", "Quan Yuan", "Christof Angermueller", "Jon Blanton", "Xinying Song", "Nitesh Bharadwaj Gundavarapu", "Thi Avrahami", "Maxine Deines", "Subhrajit Roy", "Manish Gupta", "Christopher Semturs", "Shobha Vasudevan", "Aditya Srikanth Veerubhotla", "Shriya Sharma", "Josh Jacob", "Zhen Yang", "Andreas Terzis", "Dan Karliner", "Auriel Wright", "Tania Rojas-Esponda", "Ashley Brown", "Abhijit Guha Roy", "Pawan Dogra", "Andrei Kapishnikov", "Peter Young", "Wendy Kan", "Vinodh Kumar Rajendran", "Maria Ivanova", "Salil Deshmukh", "Chia-Hua Ho", "Mike Kwong", "Stav Ginzburg", "Annie Louis", "KP Sawhney", "Slav Petrov", "Jing Xie", "Yunfei Bai", "Georgi Stoyanov", "Alex Fabrikant", "Rajesh Jayaram", "Yuqi Li", "Joe Heyward", "Justin Gilmer", "Yaqing Wang", "Radu Soricut", "Luyang Liu", "Qingnan Duan", "Jamie Hayes", "Maura O'Brien", "Gaurav Singh Tomar", "Sivan Eiger", "Bahar Fatemi", "Jeffrey Hui", "Catarina Barros", "Adaeze Chukwuka", "Alena Butryna", "Saksham Thakur", "Austin Huang", "Zhufeng Pan", "Haotian Tang", "Serkan Cabi", "Tulsee Doshi", "Michiel Bakker", "Sumit Bagri", "Ruy Ley-Wild", "Adam Lelkes", "Jennie Lees", "Patrick Kane", "David Greene", "Shimu Wu", "Jörg Bornschein", "Gabriela Surita", "Sarah Hodkinson", "Fangtao Li", "Chris Hidey", "Sébastien Pereira", "Sean Ammirati", "Phillip Lippe", "Adam Kraft", "Pu Han", "Sebastian Gerlach", "Zifeng Wang", "Liviu Panait", "Feng Han", "Brian Farris", "Yingying Bi", "Hannah DeBalsi", "Miaosen Wang", "Gladys Tyen", "James Cohan", "Susan Zhang", "Jarred Barber", "Da-Woon Chung", "Jaeyoun Kim", "Markus Kunesch", "Steven Pecht", "Nami Akazawa", "Abe Friesen", "James Lyon", "Ali Eslami", "Junru Wu", "Jie Tan", "Yue Song", "Ravi Kumar", "Chris Welty", "Ilia Akolzin", "Gena Gibson", "Sean Augenstein", "Arjun Pillai", "Nancy Yuen", "Du Phan", "Xin Wang", "Iain Barr", "Heiga Zen", "Nan Hua", "Casper Liu", "Jilei", "Wang", "Tanuj Bhatia", "Hao Xu", "Oded Elyada", "Pushmeet Kohli", "Mirek Olšák", "Ke Chen", "Azalia Mirhoseini", "Noam Shazeer", "Shoshana Jakobovits", "Maggie Tran", "Nolan Ramsden", "Tarun Bharti", "Fred Alcober", "Yunjie Li", "Shilpa Shetty", "Jing Chen", "Dmitry Kalashnikov", "Megha Nawhal", "Sercan Arik", "Hanwen Chen", "Michiel Blokzijl", "Shubham Gupta", "James Rubin", "Rigel Swavely", "Sophie Bridgers", "Ian Gemp", "Chen Su", "Arun Suggala", "Juliette Pluto", "Mary Cassin", "Alain Vaucher", "Kaiyang Ji", "Jiahao Cai", "Andrew Audibert", "Animesh Sinha", "David Tian", "Efrat Farkash", "Amy Hua", "Jilin Chen", "Duc-Hieu Tran", "Edward Loper", "Nicole Brichtova", "Lara McConnaughey", "Ballie Sandhu", "Robert Leland", "Doug DeCarlo", "Andrew Over", "James Huang", "Xing Wu", "Connie Fan", "Eric Li", "Yun Lei", "Deepak Sharma", "Cosmin Paduraru", "Luo Yu", "Matko Bošnjak", "Phuong Dao", "Min Choi", "Sneha Kudugunta", "Jakub Adamek", "Carlos Guía", "Ali Khodaei", "Jie Feng", "Wenjun Zeng", "David Welling", "Sandeep Tata", "Christina Butterfield", "Andrey Vlasov", "Seliem El-Sayed", "Swaroop Mishra", "Tara Sainath", "Shentao Yang", "RJ Skerry-Ryan", "Jeremy Shar", "Robert Berry", "Arunkumar Rajendran", "Arun Kandoor", "Andrea Burns", "Deepali Jain", "Tom Stone", "Wonpyo Park", "Shibo Wang", "Albin Cassirer", "Guohui Wang", "Hayato Kobayashi", "Sergey Rogulenko", "Vineetha Govindaraj", "Mikołaj Rybiński", "Nadav Olmert", "Colin Evans", "Po-Sen Huang", "Kelvin Xu", "Premal Shah", "Terry Thurk", "Caitlin Sikora", "Mu Cai", "Jin Xie", "Elahe Dabir", "Saloni Shah", "Norbert Kalb", "Carrie Zhang", "Shruthi Prabhakara", "Amit Sabne", "Artiom Myaskovsky", "Vikas Raunak", "Blanca Huergo", "Behnam Neyshabur", "Jon Clark", "Ye Zhang", "Shankar Krishnan", "Eden Cohen", "Dinesh Tewari", "James Lottes", "Yumeya Yamamori", "Hui", "Li", "Mohamed Elhawaty", "Ada Maksutaj Oflazer", "Adrià Recasens", "Sheryl Luo", "Duy Nguyen", "Taylor Bos", "Kalyan Andra", "Ana Salazar", "Ed Chi", "Jeongwoo Ko", "Matt Ginsberg", "Anders Andreassen", "Anian Ruoss", "Todor Davchev", "Elnaz Davoodi", "Chenxi Liu", "Min Kim", "Santiago Ontanon", "Chi Ming To", "Dawei Jia", "Rosemary Ke", "Jing Wang", "Anna Korsun", "Moran Ambar", "Ilya Kornakov", "Irene Giannoumis", "Toni Creswell", "Denny Zhou", "Yi Su", "Ishaan Watts", "Aleksandr Zaks", "Evgenii Eltyshev", "Ziqiang Feng", "Sidharth Mudgal", "Alex Kaskasoli", "Juliette Love", "Kingshuk Dasgupta", "Sam Shleifer", "Richard Green", "Sungyong Seo", "Chansoo Lee", "Dale Webster", "Prakash Shroff", "Ganna Raboshchuk", "Isabel Leal", "James Manyika", "Sofia Erell", "Daniel Murphy", "Zhisheng Xiao", "Anton Bulyenov", "Julian Walker", "Mark Collier", "Matej Kastelic", "Nelson George", "Sushant Prakash", "Sailesh Sidhwani", "Alexey Frolov", "Steven Hansen", "Petko Georgiev", "Tiberiu Sosea", "Chris Apps", "Aishwarya Kamath", "David Reid", "Emma Cooney", "Charlotte Magister", "Oriana Riva", "Alec Go", "Pu-Chin Chen", "Sebastian Krause", "Nir Levine", "Marco Fornoni", "Ilya Figotin", "Nick Roy", "Parsa Mahmoudieh", "Vladimir Magay", "Mukundan Madhavan", "Jin Miao", "Jianmo Ni", "Yasuhisa Fujii", "Ian Chou", "George Scrivener", "Zak Tsai", "Siobhan Mcloughlin", "Jeremy Selier", "Sandra Lefdal", "Jeffrey Zhao", "Abhijit Karmarkar", "Kushal Chauhan", "Shivanker Goel", "Zhaoyi Zhang", "Vihan Jain", "Parisa Haghani", "Mostafa Dehghani", "Jacob Scott", "Erin Farnese", "Anastasija Ilić", "Steven Baker", "Julia Pawar", "Li Zhong", "Josh Camp", "Yoel Zeldes", "Shravya Shetty", "Anand Iyer", "Vít Listík", "Jiaxian Guo", "Luming Tang", "Mark Geller", "Simon Bucher", "Yifan Ding", "Hongzhi Shi", "Carrie Muir", "Dominik Grewe", "Ramy Eskander", "Octavio Ponce", "Boqing Gong", "Derek Gasaway", "Samira Khan", "Umang Gupta", "Angelos Filos", "Weicheng Kuo", "Klemen Kloboves", "Jennifer Beattie", "Christian Wright", "Leon Li", "Alicia Jin", "Sandeep Mariserla", "Miteyan Patel", "Jens Heitkaemper", "Dilip Krishnan", "Vivek Sharma", "David Bieber", "Christian Frank", "John Lambert", "Paul Caron", "Martin Polacek", "Mai Giménez", "Himadri Choudhury", "Xing Yu", "Sasan Tavakkol", "Arun Ahuja", "Franz Och", "Rodolphe Jenatton", "Wojtek Skut", "Bryan Richter", "David Gaddy", "Andy Ly", "Misha Bilenko", "Megh Umekar", "Ethan Liang", "Martin Sevenich", "Mandar Joshi", "Hassan Mansoor", "Rebecca Lin", "Sumit Sanghai", "Abhimanyu Singh", "Xiaowei Li", "Sudheendra Vijayanarasimhan", "Zaheer Abbas", "Yonatan Bitton", "Hansa Srinivasan", "Manish Reddy Vuyyuru", "Alexander Frömmgen", "Yanhua Sun", "Ralph Leith", "Alfonso Castaño", "DJ Strouse", "Le Yan", "Austin Kyker", "Satish Kambala", "Mary Jasarevic", "Thibault Sellam", "Chao Jia", "Alexander Pritzel", "Raghavender R", "Huizhong Chen", "Natalie Clay", "Sudeep Gandhe", "Sean Kirmani", "Sayna Ebrahimi", "Hannah Kirkwood", "Jonathan Mallinson", "Chao Wang", "Adnan Ozturel", "Kuo Lin", "Shyam Upadhyay", "Vincent Cohen-Addad", "Sean Purser-haskell", "Yichong Xu", "Ebrahim Songhori", "Babi Seal", "Alberto Magni", "Almog Gueta", "Tingting Zou", "Guru Guruganesh", "Thais Kagohara", "Hung Nguyen", "Khalid Salama", "Alejandro Cruzado Ruiz", "Justin Frye", "Zhenkai Zhu", "Matthias Lochbrunner", "Simon Osindero", "Wentao Yuan", "Lisa Lee", "Aman Prasad", "Lam Nguyen Thiet", "Daniele Calandriello", "Victor Stone", "Qixuan Feng", "Han Ke", "Maria Voitovich", "Geta Sampemane", "Lewis Chiang", "Ling Wu", "Alexander Bykovsky", "Matt Young", "Luke Vilnis", "Ishita Dasgupta", "Aditya Chawla", "Qin Cao", "Bowen Liang", "Daniel Toyama", "Szabolcs Payrits", "Anca Stefanoiu", "Dimitrios Vytiniotis", "Ankesh Anand", "Tianxiao Shen", "Blagoj Mitrevski", "Michael Tschannen", "Sreenivas Gollapudi", "Aishwarya P S", "José Leal", "Zhe Shen", "Han Fu", "Wei Wang", "Arvind Kannan", "Doron Kukliansky", "Sergey Yaroshenko", "Svetlana Grant", "Umesh Telang", "David Wood", "Alexandra Chronopoulou", "Alexandru Ţifrea", "Tao Zhou", "Tony", "Nguy\\~ên", "Muge Ersoy", "Anima Singh", "Meiyan Xie", "Emanuel Taropa", "Woohyun Han", "Eirikur Agustsson", "Andrei Sozanschi", "Hui Peng", "Alex Chen", "Yoel Drori", "Efren Robles", "Yang Gao", "Xerxes Dotiwalla", "Ying Chen", "Anudhyan Boral", "Alexei Bendebury", "John Nham", "Chris Tar", "Luis Castro", "Jiepu Jiang", "Canoee Liu", "Felix Halim", "Jinoo Baek", "Andy Wan", "Jeremiah Liu", "Yuan Cao", "Shengyang Dai", "Trilok Acharya", "Ruoxi Sun", "Fuzhao Xue", "Saket Joshi", "Morgane Lustman", "Yongqin Xian", "Rishabh Joshi", "Deep Karkhanis", "Nora Kassner", "Jamie Hall", "Xiangzhuo Ding", "Gan Song", "Gang Li", "Chen Zhu", "Yana Kulizhskaya", "Bin Ni", "Alexey Vlaskin", "Solomon Demmessie", "Lucio Dery", "Salah Zaiem", "Yanping Huang", "Cindy Fan", "Felix Gimeno", "Ananth Balashankar", "Koji Kojima", "Hagai Taitelbaum", "Maya Meng", "Dero Gharibian", "Sahil Singla", "Wei Chen", "Ambrose Slone", "Guanjie Chen", "Sujee Rajayogam", "Max Schumacher", "Suyog Kotecha", "Rory Blevins", "Qifei Wang", "Mor Hazan Taege", "Alex Morris", "Xin Liu", "Fayaz Jamil", "Richard Zhang", "Pratik Joshi", "Ben Ingram", "Tyler Liechty", "Ahmed Eleryan", "Scott Baird", "Alex Grills", "Gagan Bansal", "Shan Han", "Kiran Yalasangi", "Shawn Xu", "Majd Al Merey", "Isabel Gao", "Felix Weissenberger", "Igor Karpov", "Robert Riachi", "Ankit Anand", "Gautam Prasad", "Kay Lamerigts", "Reid Hayes", "Jamie Rogers", "Mandy Guo", "Ashish Shenoy", "Qiong", "Hu", "Kyle He", "Yuchen Liu", "Polina Zablotskaia", "Sagar Gubbi", "Yifan Chang", "Jay Pavagadhi", "Kristian Kjems", "Archita Vadali", "Diego Machado", "Yeqing Li", "Renshen Wang", "Dipankar Ghosh", "Aahil Mehta", "Dana Alon", "George Polovets", "Alessio Tonioni", "Nate Kushman", "Joel D'sa", "Lin Zhuo", "Allen Wu", "Rohin Shah", "John Youssef", "Jiayu Ye", "Justin Snyder", "Karel Lenc", "Senaka Buthpitiya", "Matthew Tung", "Jichuan Chang", "Tao Chen", "David Saxton", "Jenny Lee", "Lydia Lihui Zhang", "James Qin", "Prabakar Radhakrishnan", "Maxwell Chen", "Piotr Ambroszczyk", "Metin Toksoz-Exley", "Yan Zhong", "Nitzan Katz", "Brendan O'Donoghue", "Tamara von Glehn", "Adi Gerzi Rosenthal", "Aga Świetlik", "Xiaokai Zhao", "Nick Fernando", "Jinliang Wei", "Jieru Mei", "Sergei Vassilvitskii", "Diego Cedillo", "Pranjal Awasthi", "Hui Zheng", "Koray Kavukcuoglu", "Itay Laish", "Joseph Pagadora", "Marc Brockschmidt", "Christopher A. Choquette-Choo", "Arunkumar Byravan", "Yifeng Lu", "Xu Chen", "Mia Chen", "Kenton Lee", "Rama Pasumarthi", "Sijal Bhatnagar", "Aditya Shah", "Qiyin Wu", "Zhuoyuan Chen", "Zack Nado", "Bartek Perz", "Zixuan Jiang", "David Kao", "Ganesh Mallya", "Nino Vieillard", "Lantao Mei", "Sertan Girgin", "Mandy Jordan", "Yeongil Ko", "Alekh Agarwal", "Yaxin Liu", "Yasemin Altun", "Raoul de Liedekerke", "Anastasios Kementsietsidis", "Daiyi Peng", "Dangyi Liu", "Utku Evci", "Peter Humphreys", "Austin Tarango", "Xiang Deng", "Yoad Lewenberg", "Kevin Aydin", "Chengda Wu", "Bhavishya Mittal", "Tsendsuren Munkhdalai", "Kleopatra Chatziprimou", "Rodrigo Benenson", "Uri First", "Xiao Ma", "Jinning Li", "Armand Joulin", "Hamish Tomlinson", "Tingnan Zhang", "Milad Nasr", "Zhi Hong", "Michaël Sander", "Lisa Anne Hendricks", "Anuj Sharma", "Andrew Bolt", "Eszter Vértes", "Jiri Simsa", "Tomer Levinboim", "Olcan Sercinoglu", "Divyansh Shukla", "Austin Wu", "Craig Swanson", "Danny Vainstein", "Fan Bu", "Bo Wang", "Ryan Julian", "Charles Yoon", "Sergei Lebedev", "Antonious Girgis", "Bernd Bandemer", "David Du", "Todd Wang", "Xi Chen", "Ying Xiao", "Peggy Lu", "Natalie Ha", "Vlad Ionescu", "Simon Rowe", "Josip Matak", "Federico Lebron", "Andreas Steiner", "Lalit Jain", "Manaal Faruqui", "Nicolas Lacasse", "Georgie Evans", "Neesha Subramaniam", "Dean Reich", "Giulia Vezzani", "Aditya Pandey", "Joe Stanton", "Tianhao Zhou", "Liam McCafferty", "Henry Griffiths", "Verena Rieser", "Soheil Hassas Yeganeh", "Eleftheria Briakou", "Lu Huang", "Zichuan Wei", "Liangchen Luo", "Erik Jue", "Gabby Wang", "Victor Cotruta", "Myriam Khan", "Jongbin Park", "Qiuchen Guo", "Peiran Li", "Rong Rong", "Diego Antognini", "Anastasia Petrushkina", "Chetan Tekur", "Eli Collins", "Parul Bhatia", "Chester Kwak", "Wenhu Chen", "Arvind Neelakantan", "Immanuel Odisho", "Sheng Peng", "Vincent Nallatamby", "Vaibhav Tulsyan", "Fabian Pedregosa", "Peng Xu", "Raymond Lin", "Yulong Wang", "Emma Wang", "Sholto Douglas", "Reut Tsarfaty", "Elena Gribovskaya", "Renga Aravamudhan", "Manu Agarwal", "Mara Finkelstein", "Qiao Zhang", "Elizabeth Cole", "Phil Crone", "Sarmishta Velury", "Anil Das", "Chris Sauer", "Luyao Xu", "Danfeng Qin", "Chenjie Gu", "Dror Marcus", "CJ Zheng", "Wouter Van Gansbeke", "Sobhan Miryoosefi", "Haitian Sun", "YaGuang Li", "Charlie Chen", "Jae Yoo", "Pavel Dubov", "Alex Tomala", "Adams Yu", "Paweł Wesołowski", "Alok Gunjan", "Eddie Cao", "Jiaming Luo", "Nikhil Sethi", "Arkadiusz Socala", "Laura Graesser", "Tomas Kocisky", "Arturo BC", "Minmin Chen", "Edward Lee", "Sophie Wang", "Weize Kong", "Qiantong Xu", "Nilesh Tripuraneni", "Yiming Li", "Xinxin Yu", "Allen Porter", "Paul Voigtlaender", "Biao Zhang", "Arpi Vezer", "Sarah York", "Qing Wei", "Geoffrey Cideron", "Mark Kurzeja", "Seungyeon Kim", "Benny Li", "Angéline Pouget", "Hyo Lee", "Kaspar Daugaard", "Yang Li", "Dave Uthus", "Aditya Siddhant", "Paul Cavallaro", "Sriram Ganapathy", "Maulik Shah", "Rolf Jagerman", "Jeff Stanway", "Piermaria Mendolicchio", "Li Xiao", "Kayi Lee", "Tara Thompson", "Shubham Milind Phal", "Jason Chase", "Sun Jae Lee", "Adrian N Reyes", "Disha Shrivastava", "Zhen Qin", "Roykrong Sukkerd", "Seth Odoom", "Lior Madmoni", "John Aslanides", "Jonathan Herzig", "Elena Pochernina", "Sheng Zhang", "Parker Barnes", "Daisuke Ikeda", "Qiujia Li", "Shuo-yiin Chang", "Shakir Mohamed", "Jim Sproch", "Richard Powell", "Bidisha Samanta", "Domagoj Ćevid", "Anton Kovsharov", "Shrestha Basu Mallick", "Srinivas Tadepalli", "Anne Zheng", "Kareem Ayoub", "Andreas Noever", "Christian Reisswig", "Zhuo Xu", "Junhyuk Oh", "Martin Matysiak", "Tim Blyth", "Shereen Ashraf", "Julien Amelot", "Boone Severson", "Michele Bevilacqua", "Motoki Sano", "Ethan Dyer", "Ofir Roval", "Anu Sinha", "Yin Zhong", "Sagi Perel", "Tea Sabolić", "Johannes Mauerer", "Willi Gierke", "Mauro Verzetti", "Rodrigo Cabrera", "Alvin Abdagic", "Steven Hemingray", "Austin Stone", "Jong Lee", "Farooq Ahmad", "Karthik Raman", "Lior Shani", "Jonathan Lai", "Orhan Firat", "Nathan Waters", "Eric Ge", "Mo Shomrat", "Himanshu Gupta", "Rajeev Aggarwal", "Tom Hudson", "Bill Jia", "Simon Baumgartner", "Palak Jain", "Joe Kovac", "Junehyuk Jung", "Ante Žužul", "Will Truong", "Morteza Zadimoghaddam", "Songyou Peng", "Marco Liang", "Rachel Sterneck", "Balaji Lakshminarayanan", "Machel Reid", "Oliver Woodman", "Tong Zhou", "Jianling Wang", "Vincent Coriou", "Arjun Narayanan", "Jay Hoover", "Yenai Ma", "Apoorv Jindal", "Clayton Sanford", "Doug Reid", "Swaroop Ramaswamy", "Alex Kurakin", "Roland Zimmermann", "Yana Lunts", "Dragos Dena", "Zalán Borsos", "Vered Cohen", "Shujian Zhang", "Will Grathwohl", "Robert Dadashi", "Morgan Redshaw", "Joshua Kessinger", "Julian Odell", "Silvano Bonacina", "Zihang Dai", "Grace Chen", "Ayush Dubey", "Pablo Sprechmann", "Mantas Pajarskas", "Wenxuan Zhou", "Niharika Ahuja", "Tara Thomas", "Martin Nikoltchev", "Matija Kecman", "Bharath Mankalale", "Andrey Ryabtsev", "Jennifer She", "Christian Walder", "Jiaming Shen", "Lu Li", "Carolina Parada", "Sheena Panthaplackel", "Okwan Kwon", "Matt Lawlor", "Utsav Prabhu", "Yannick Schroecker", "Marc'aurelio Ranzato", "Pete Blois", "Iurii Kemaev", "Ting Yu", "Dmitry", "Lepikhin", "Hao Xiong", "Sahand Sharifzadeh", "Oleaser Johnson", "Jeremiah Willcock", "Rui Yao", "Greg Farquhar", "Sujoy Basu", "Hidetoshi Shimokawa", "Nina Anderson", "Haiguang Li", "Khiem Pham", "Yizhong Liang", "Sebastian Borgeaud", "Alexandre Moufarek", "Hideto Kazawa", "Blair Kutzman", "Marcin Sieniek", "Sara Smoot", "Ruth Wang", "Natalie Axelsson", "Nova Fallen", "Prasha Sundaram", "Yuexiang Zhai", "Varun Godbole", "Petros Maniatis", "Alek Wang", "Ilia Shumailov", "Santhosh Thangaraj", "Remi Crocker", "Nikita Gupta", "Gang Wu", "Phil Chen", "Gellért Weisz", "Celine Smith", "Mojtaba Seyedhosseini", "Boya Fang", "Xiyang Luo", "Roey Yogev", "Zeynep Cankara", "Andrew Hard", "Helen Ran", "Rahul Sukthankar", "George Necula", "Gaël Liu", "Honglong Cai", "Praseem Banzal", "Daniel Keysers", "Sanjay Ghemawat", "Connie Tao", "Emma Dunleavy", "Aditi Chaudhary", "Wei Li", "Maciej Mikuła", "Chen-Yu Lee", "Tiziana Refice", "Krishna Somandepalli", "Alexandre Fréchette", "Dan Bahir", "John Karro", "Keith Rush", "Sarah Perrin", "Bill Rosgen", "Xiaomeng Yang", "Clara Huiyi Hu", "Mahmoud Alnahlawi", "Justin Mao-Jones", "Roopal Garg", "Hoang Nguyen", "Bat-Orgil Batsaikhan", "Iñaki Iturrate", "Anselm Levskaya", "Avi Singh", "Ashyana Kachra", "Tony Lu", "Denis Petek", "Zheng Xu", "Mark Graham", "Lukas Zilka", "Yael Karov", "Marija Kostelac", "Fangyu Liu", "Yaohui Guo", "Weiyue Wang", "Bernd Bohnet", "Emily Pitler", "Tony Bruguier", "Keisuke Kinoshita", "Chrysovalantis Anastasiou", "Nilpa Jha", "Ting Liu", "Jerome Connor", "Phil Wallis", "Philip Pham", "Eric Bailey", "Shixin Li", "Heng-Tze Cheng", "Sally Ma", "Haiqiong Li", "Akanksha Maurya", "Kate Olszewska", "Manfred Warmuth", "Christy Koh", "Dominik Paulus", "Siddhartha Reddy Jonnalagadda", "Enrique Piqueras", "Ali Elqursh", "Geoff Brown", "Hadar Shemtov", "Loren Maggiore", "Fei Xia", "Ryan Foley", "Beka Westberg", "George van den Driessche", "Livio Baldini Soares", "Arjun Kar", "Michael Quinn", "Siqi Zuo", "Jialin Wu", "Kyle Kastner", "Anna Bortsova", "Aijun Bai", "Ales Mikhalap", "Luowei Zhou", "Jennifer Brennan", "Vinay Ramasesh", "Honglei Zhuang", "John Maggs", "Johan Schalkwyk", "Yuntao Xu", "Hui Huang", "Andrew Howard", "Sasha Brown", "Linting Xue", "Gloria Shen", "Brian Albert", "Neha Jha", "Daniel Zheng", "Varvara Krayvanova", "Spurthi Amba Hombaiah", "Olivier Lacombe", "Gautam Vasudevan", "Dan Graur", "Tian Xie", "Meet Gandhi", "Bangju Wang", "Dustin Zelle", "Harman Singh", "Dahun Kim", "Sébastien Cevey", "Victor Ungureanu", "Natasha Noy", "Fei Liu", "Annie Xie", "Fangxiaoyu Feng", "Katerina Tsihlas", "Daniel Formoso", "Neera Vats", "Quentin Wellens", "Yinan Wang", "Niket Kumar Bhumihar", "Samrat Ghosh", "Matt Hoffman", "Tom Lieber", "Oran Lang", "Kush Bhatia", "Tom Paine", "Aroonalok Pyne", "Ronny Votel", "Madeleine Clare Elish", "Benoit Schillings", "Alex Panagopoulos", "Haichuan Yang", "Adam Raveret", "Zohar Yahav", "Shuang Liu", "Warren Chen", "Dalia El Badawy", "Nishant Agrawal", "Mohammed Badawi", "Mahdi Mirzazadeh", "Carla Bromberg", "Fan Ye", "Chang Liu", "Tatiana Sholokhova", "George-Cristian Muraru", "Gargi Balasubramaniam", "Jonathan Malmaud", "Alen Carin", "Danilo Martins", "Irina Jurenka", "Pankil Botadra", "Dave Lacey", "Richa Singh", "Mariano Schain", "Dan Zheng", "Isabelle Guyon", "Victor Lavrenko", "Seungji Lee", "Xiang Zhou", "Demis Hassabis", "Jeshwanth Challagundla", "Derek Cheng", "Nikhil Mehta", "Matthew Mauger", "Michela Paganini", "Pushkar Mishra", "Kate Lee", "Zhang Li", "Lexi Baugher", "Ondrej Skopek", "Max Chang", "Amir Zait", "Gaurav Menghani", "Lizzetth Bellot", "Guangxing Han", "Jean-Michel Sarr", "Sharat Chikkerur", "Himanshu Sahni", "Rohan Anil", "Arun Narayanan", "Chandu Thekkath", "Daniele Pighin", "Hana Strejček", "Marko Velic", "Fred Bertsch", "Manuel Tragut", "Keran Rong", "Alicia Parrish", "Kai Bailey", "Jiho Park", "Isabela Albuquerque", "Abhishek Bapna", "Rajesh Venkataraman", "Alec Kosik", "Johannes Griesser", "Zhiwei Deng", "Alek Andreev", "Qingyun Dou", "Kevin Hui", "Fanny Wei", "Xiaobin Yu", "Lei Shu", "Avia Aharon", "David Barker", "Badih Ghazi", "Sebastian Flennerhag", "Chris Breaux", "Yuchuan Liu", "Matthew Bilotti", "Josh Woodward", "Uri Alon", "Stephanie Winkler", "Tzu-Kuo Huang", "Kostas Andriopoulos", "João Gabriel Oliveira", "Penporn Koanantakool", "Berkin Akin", "Michael Wunder", "Cicero Nogueira dos Santos", "Mohammad Hossein Bateni", "Lin Yang", "Dan Horgan", "Beer Changpinyo", "Keyvan Amiri", "Min Ma", "Dayeong Lee", "Lihao Liang", "Anirudh Baddepudi", "Tejasi Latkar", "Raia Hadsell", "Jun Xu", "Hairong Mu", "Michael Han", "Aedan Pope", "Snchit Grover", "Frank Kim", "Ankit Bhagatwala", "Guan Sun", "Yamini Bansal", "Amir Globerson", "Alireza Nazari", "Samira Daruki", "Hagen Soltau", "Jane Labanowski", "Laurent El Shafey", "Matt Harvey", "Yanif Ahmad", "Elan Rosenfeld", "William Kong", "Etienne Pot", "Yi-Xuan Tan", "Aurora Wei", "Victoria Langston", "Marcel Prasetya", "Petar Veličković", "Richard Killam", "Robin Strudel", "Darren Ni", "Zhenhai Zhu", "Aaron Archer", "Kavya Kopparapu", "Lynn Nguyen", "Emilio Parisotto", "Hussain Masoom", "Sravanti Addepalli", "Jordan Grimstad", "Hexiang Hu", "Joss Moore", "Avinatan Hassidim", "Le Hou", "Mukund Raghavachari", "Jared Lichtarge", "Adam R. Brown", "Hilal Dib", "Natalia Ponomareva", "Justin Fu", "Yujing Zhang", "Altaf Rahman", "Joana Iljazi", "Edouard Leurent", "Gabriel Dulac-Arnold", "Cosmo Du", "Chulayuth Asawaroengchai", "Larry Jin", "Ela Gruzewska", "Ziwei Ji", "Benigno Uria", "Daniel De Freitas", "Paul Barham", "Lauren Beltrone", "Víctor Campos", "Jun Yan", "Neel Kovelamudi", "Arthur Nguyen", "Elinor Davies", "Zhichun Wu", "Zoltan Egyed", "Kristina Toutanova", "Nithya Attaluri", "Hongliang Fei", "Peter Stys", "Siddhartha Brahma", "Martin Izzard", "Siva Velusamy", "Scott Lundberg", "Vincent Zhuang", "Kevin Sequeira", "Adam Santoro", "Ehsan Amid", "Ophir Aharoni", "Shuai Ye", "Mukund Sundararajan", "Lijun Yu", "Yu-Cheng Ling", "Stephen Spencer", "Hugo Song", "Josip Djolonga", "Christo Kirov", "Sonal Gupta", "Alessandro Bissacco", "Clemens Meyer", "Mukul Bhutani", "Andrew Dai", "Weiyi Wang", "Siqi Liu", "Ashwin Sreevatsa", "Qijun Tan", "Maria Wang", "Lucy Kim", "Yicheng Wang", "Alex Irpan", "Yang Xiao", "Stanislav Fort", "Yifan He", "Alex Gurney", "Bryan Gale", "Yue Ma", "Monica Roy", "Viorica Patraucean", "Taylan Bilal", "Golnaz Ghiasi", "Anahita Hosseini", "Melvin Johnson", "Zhuowan Li", "Yi Tay", "Benjamin Beyret", "Katie Millican", "Josef Broder", "Mayank Lunayach", "Danny Swisher", "Eugen Vušak", "David Parkinson", "MH Tessler", "Adi Mayrav Gilady", "Richard Song", "Allan Dafoe", "Yves Raimond", "Masa Yamaguchi", "Itay Karo", "Elizabeth Nielsen", "Kevin Kilgour", "Mike Dusenberry", "Rajiv Mathews", "Jiho Choi", "Siyuan Qiao", "Harsh Mehta", "Sahitya Potluri", "Chris Knutsen", "Jialu Liu", "Tat Tan", "Kuntal Sengupta", "Keerthana Gopalakrishnan", "Abodunrinwa Toki", "Mencher Chiang", "Mike Burrows", "Grace Vesom", "Zafarali Ahmed", "Ilia Labzovsky", "Siddharth Vashishtha", "Preeti Singh", "Ankur Sharma", "Ada Ma", "Jinyu Xie", "Pranav Talluri", "Hannah Forbes-Pollard", "Aarush Selvan", "Joel Wee", "Loic Matthey", "Tom Funkhouser", "Parthasarathy Gopavarapu", "Lev Proleev", "Cheng Li", "Matt Thomas", "Kashyap Kolipaka", "Zhipeng Jia", "Ashwin Kakarla", "Srinivas Sunkara", "Joan Puigcerver", "Suraj Satishkumar Sheth", "Emily Graves", "Chen Wang", "Sadh MNM Khan", "Kai Kang", "Shyamal Buch", "Fred Zhang", "Omkar Savant", "David Soergel", "Kevin Lee", "Linda Friso", "Xuanyi Dong", "Rahul Arya", "Shreyas Chandrakaladharan", "Connor Schenck", "Greg Billock", "Tejas Iyer", "Anton Bakalov", "Leslie Baker", "Alex Ruiz", "Angad Chandorkar", "Trieu Trinh", "Matt Miecnikowski", "Yanqi Zhou", "Yangsibo Huang", "Jiazhong Nie", "Ali Shah", "Ashish Thapliyal", "Sam Haves", "Lun Wang", "Uri Shaham", "Patrick Morris-Suzuki", "Soroush Radpour", "Leonard Berrada", "Thomas Strohmann", "Chaochao Yan", "Jingwei Shen", "Sonam Goenka", "Tris Warkentin", "Petar Dević", "Dan Belov", "Albert Webson", "Madhavi Yenugula", "Puranjay Datta", "Jerry Chang", "Nimesh Ghelani", "Aviral Kumar", "Vincent Perot", "Jessica Lo", "Yang Song", "Herman Schmit", "Jianmin Chen", "Vasilisa Bashlovkina", "Xiaoyue Pan", "Diana Mincu", "Paul Roit", "Isabel Edkins", "Andy Davis", "Yujia Li", "Ben Horn", "Xinjian Li", "Pradeep Kumar S", "Eric Doi", "Wanzheng Zhu", "Sri Gayatri Sundara Padmanabhan", "Siddharth Verma", "Jasmine Liu", "Heng Chen", "Mihajlo Velimirović", "Malcolm Reynolds", "Priyanka Agrawal", "Nick Sukhanov", "Abhinit Modi", "Siddharth Goyal", "John Palowitch", "Nima Khajehnouri", "Wing Lowe", "David Klinghoffer", "Sharon Silver", "Vinh Tran", "Candice Schumann", "Francesco Piccinno", "Xi Liu", "Mario Lučić", "Xiaochen Yang", "Sandeep Kumar", "Ajay Kannan", "Ragha Kotikalapudi", "Mudit Bansal", "Fabian Fuchs", "Javad Hosseini", "Abdelrahman Abdelhamed", "Dawn Bloxwich", "Tianhe Yu", "Ruoxin Sang", "Gregory Thornton", "Karan Gill", "Yuchi Liu", "Virat Shejwalkar", "Jason Lin", "Zhipeng Yan", "Kehang Han", "Thomas Buschmann", "Michael Pliskin", "Zhi Xing", "Susheel Tatineni", "Junlin Zhang", "Sissie Hsiao", "Gavin Buttimore", "Marcus Wu", "Zefei Li", "Geza Kovacs", "Legg Yeung", "Tao Huang", "Aaron Cohen", "Bethanie Brownfield", "Averi Nowak", "Mikel Rodriguez", "Tianze Shi", "Hado van Hasselt", "Kevin Cen", "Deepanway Ghoshal", "Kushal Majmundar", "Weiren Yu", "Warren", "Chen", "Danila Sinopalnikov", "Hao Zhang", "Vlado Galić", "Di Lu", "Zeyu Zheng", "Maggie Song", "Gary Wang", "Gui Citovsky", "Swapnil Gawde", "Isaac Galatzer-Levy", "David Silver", "Ivana Balazevic", "Dipanjan Das", "Kingshuk Majumder", "Yale Cong", "Praneet Dutta", "Dustin Tran", "Hui Wan", "Junwei Yuan", "Daniel Eppens", "Alanna Walton", "Been Kim", "Harry Ragan", "James Cobon-Kerr", "Lu Liu", "Weijun Wang", "Bryce Petrini", "Jack Rae", "Rakesh Shivanna", "Yan Xiong", "Chace Lee", "Pauline Coquinot", "Yiming Gu", "Lisa Patel", "Blake Hechtman", "Aviel Boag", "Orion Jankowski", "Alex Wertheim", "Alex Lee", "Paul Covington", "Hila Noga", "Sam Sobell", "Shanthal Vasanth", "William Bono", "Chirag Nagpal", "Wei Fan", "Xavier Garcia", "Kedar Soparkar", "Aybuke Turker", "Nathan Howard", "Sachit Menon", "Yuankai Chen", "Vikas Verma", "Vladimir Pchelin", "Harish Rajamani", "Valentin Dalibard", "Ana Ramalho", "Yang Guo", "Kartikeya Badola", "Seojin Bang", "Nathalie Rauschmayr", "Julia Proskurnia", "Sudeep Dasari", "Xinyun Chen", "Mikhail Sushkov", "Anja Hauth", "Pauline Sho", "Abhinav Singh", "Bilva Chandra", "Allie Culp", "Max Dylla", "Olivier Bachem", "James Besley", "Heri Zhao", "Timothy Lillicrap", "Wei Wei", "Wael Al Jishi", "Ning Niu", "Alban Rrustemi", "Raphaël Lopez Kaufman", "Ryan Poplin", "Jewel Zhao", "Minh Truong", "Shikhar Bharadwaj", "Ester Hlavnova", "Eli Stickgold", "Cordelia Schmid", "Georgi Stephanov", "Zhaoqi Leng", "Frederick Liu", "Léonard Hussenot", "Shenil Dodhia", "Juliana Vicente Franco", "Lesley Katzen", "Abhanshu Sharma", "Sarah Cogan", "Zuguang Yang", "Aniket Ray", "Sergi Caelles", "Shen Yan", "Ravin Kumar", "Daniel Gillick", "Renee Wong", "Joshua Ainslie", "Jonathan Hoech", "Séb Arnold", "Dan Abolafia", "Anca Dragan", "Ben Hora", "Grace Hu", "Alexey Guseynov", "Yang Lu", "Chas Leichner", "Jinmeng Rao", "Abhimanyu Goyal", "Nagabhushan Baddi", "Daniel Hernandez Diaz", "Tim McConnell", "Max Bain", "Jake Abernethy", "Qiqi Yan", "Rylan Schaeffer", "Paul Vicol", "Will Thompson", "Montse Gonzalez Arenas", "Mathias Bellaiche", "Pablo Barrio", "Stefan Zinke", "Riccardo Patana", "Pulkit Mehta", "JK Kearns", "Avraham Ruderman", "Scott Pollom", "David D'Ambrosio", "Cath Hope", "Yang Yu", "Andrea Gesmundo", "Kuang-Huei Lee", "Aviv Rosenberg", "Yiqian Zhou", "Yaoyiran Li", "Drew Garmon", "Yonghui Wu", "Safeen Huda", "Gil Fidel", "Martin Baeuml", "Jian Li", "Phoebe Kirk", "Rhys May", "Tao Tu", "Sara Mc Carthy", "Toshiyuki Fukuzawa", "Miranda Aperghis", "Chih-Kuan Yeh", "Toshihiro Yoshino", "Bo Li", "Austin Myers", "Kaisheng Yao", "Ben Limonchik", "Changwan Ryu", "Rohun Saxena", "Alex Goldin", "Ruizhe Zhao", "Rocky Rhodes", "Tao Zhu", "Divya Tyam", "Heidi Howard", "Nathan Byrd", "Hongxu Ma", "Yan Wu", "Ryan Mullins", "Qingze Wang", "Aida Amini", "Sebastien Baur", "Yiran Mao", "Subhashini Venugopalan", "Will Song", "Wen Ding", "Paul Collins", "Sashank Reddi", "Megan Shum", "Andrei Rusu", "Luisa Zintgraf", "Kelvin Chan", "Sheela Goenka", "Mathieu Blondel", "Michael Collins", "Renke Pan", "Marissa Giustina", "Nikolai Chinaev", "Christian Schuler", "Ce Zheng", "Jonas Valfridsson", "Alyssa Loo", "Alex Yakubovich", "Jamie Smith", "Tao Jiang", "Rich Munoz", "Gabriel Barcik", "Rishabh Bansal", "Mingyao Yang", "Yilun Du", "Pablo Duque", "Mary Phuong", "Alexandra Belias", "Kunal Lad", "Zeyu Liu", "Tal Schuster", "Karthik Duddu", "Jieru Hu", "Paige Kunkle", "Matthew Watson", "Jackson Tolins", "Josh Smith", "Denis Teplyashin", "Garrett Bingham", "Marvin Ritter", "Marco Andreetto", "Divya Pitta", "Mohak Patel", "Shashank Viswanadha", "Trevor Strohman", "Catalin Ionescu", "Jincheng Luo", "Yogesh Kalley", "Jeremy Wiesner", "Dan Deutsch", "Derek Lockhart", "Peter Choy", "Rumen Dangovski", "Chawin Sitawarin", "Cat Graves", "Tanya Lando", "Joost van Amersfoort", "Ndidi Elue", "Zhouyuan Huo", "Pooya Moradi", "Jean Tarbouriech", "Henryk Michalewski", "Wenting Ye", "Eunyoung Kim", "Alex Druinsky", "Florent Altché", "Xinyi Chen", "Artur Dwornik", "Da-Cheng Juan", "Rivka Moroshko", "Horia Toma", "Jarrod Kahn", "Hai Qian", "Maximilian Sieb", "Irene Cai", "Roman Goldenberg", "Praneeth Netrapalli", "Sindhu Raghuram", "Yuan Gong", "Lijie Fan", "Evan Palmer", "Yossi Matias", "Valentin Gabeur", "Shreya Pathak", "Tom Ouyang", "Don Metzler", "Geoff Bacon", "Srinivasan Venkatachary", "Sridhar Thiagarajan", "Alex Cullum", "Eran Ofek", "Vytenis Sakenas", "Mohamed Hammad", "Cesar Magalhaes", "Mayank Daswani", "Oscar Chang", "Ashok Popat", "Ruichao Li", "Komal Jalan", "Yanhan Hou", "Josh Lipschultz", "Antoine He", "Wenhao Jia", "Pier Giuseppe Sessa", "Prateek Kolhar", "William Wong", "Sumeet Singh", "Lukas Haas", "Jay Whang", "Hanna Klimczak-Plucińska", "Georges Rotival", "Grace Chung", "Yiqing Hua", "Anfal Siddiqui", "Nicolas Serrano", "Dongkai Chen", "Billy Porter", "Libin Bai", "Keshav Shivam", "Sho Arora", "Partha Talukdar", "Tom Cobley", "Sangnie Bhardwaj", "Evgeny Gladchenko", "Simon Green", "Kelvin Guu", "Felix Fischer", "Xiao Wu", "Eric Wang", "Achintya Singhal", "Tatiana Matejovicova", "James Martens", "Hongji Li", "Roma Patel", "Elizabeth Kemp", "Jiaqi Pan", "Lily Wang", "Blake JianHang Chen", "Jean-Baptiste Alayrac", "Navneet Potti", "Erika Gemzer", "Eugene Ie", "Kay McKinney", "Takaaki Saeki", "Edward Chou", "Pascal Lamblin", "SQ Mah", "Zach Fisher", "Martin Chadwick", "Jon Stritar", "Obaid Sarvana", "Andrew Hogue", "Artem Shtefan", "Hadi Hashemi", "Yang Xu", "Jindong Gu", "Sharad Vikram", "Chung-Ching Chang", "Sabela Ramos", "Logan Kilpatrick", "Weijuan Xi", "Jenny Brennan", "Yinghao Sun", "Abhishek Jindal", "Ionel Gog", "Dawn Chen", "Felix Wu", "Jason Lee", "Sudhindra Kopalle", "Srinadh Bhojanapalli", "Oriol Vinyals", "Natan Potikha", "Burcu Karagol Ayan", "Yuan Yuan", "Michael Riley", "Piotr Stanczyk", "Sergey Kishchenko", "Bing Wang", "Dan Garrette", "Antoine Yang", "Vlad Feinberg", "CJ Carey", "Javad Azizi", "Viral Shah", "Erica Moreira", "Chongyang Shi", "Josh Feldman", "Elizabeth Salesky", "Thomas Lampe", "Aneesh Pappu", "Duhyeon Kim", "Jonas Adler", "Avi Caciularu", "Brian Walker", "Yunhan Xu", "Yochai Blau", "Dylan Scandinaro", "Terry Huang", "Sam El-Husseini", "Abhishek Sinha", "Lijie Ren", "Taylor Tobin", "Patrik Sundberg", "Tim Sohn", "Vikas Yadav", "Mimi Ly", "Emily Xue", "Jing Xiong", "Afzal Shama Soudagar", "Sneha Mondal", "Nikhil Khadke", "Qingchun Ren", "Ben Vargas", "Stan Bileschi", "Sarah Chakera", "Cindy Wang", "Boyu Wang", "Yoni Halpern", "Joe Jiang", "Vikas Sindhwani", "Petre Petrov", "Pranavaraj Ponnuramu", "Sanket Vaibhav Mehta", "Yu Watanabe", "Betty Chan", "Matheus Wisniewski", "Trang Pham", "Jingwei Zhang", "Conglong Li", "Dario de Cesare", "Art Khurshudov", "Alex Vasiloff", "Melissa Tan", "Zoe Ashwood", "Bobak Shahriari", "Maryam Majzoubi", "Garrett Tanzer", "Olga Kozlova", "Robin Alazard", "James Lee-Thorp", "Nguyet Minh Phu", "Isaac Tian", "Junwhan Ahn", "Andy Crawford", "Lauren Lax", "Yuan", "Shangguan", "Iftekhar Naim", "David Ross", "Oleksandr Ferludin", "Tongfei Guo", "Andrea Banino", "Hubert Soyer", "Xiaoen Ju", "Dominika Rogozińska", "Ishaan Malhi", "Marcella Valentine", "Daniel Balle", "Apoorv Kulshreshtha", "Maciej Kula", "Yiwen Song", "Sophia Austin", "John Schultz", "Roy Hirsch", "Arthur Douillard", "Apoorv Reddy", "Michael Fink", "Summer Yue", "Khyatti Gupta", "Adam Zhang", "Norman Rink", "Daniel McDuff", "Lei Meng", "András György", "Yasaman Razeghi", "Ricky Liang", "Kazuki Osawa", "Aviel Atias", "Matan Eyal", "Tyrone Hill", "Nikolai Grigorev", "Zhengdong Wang", "Nitish Kulkarni", "Rachel Soh", "Ivan Lobov", "Zachary Charles", "Sid Lall", "Kazuma Hashimoto", "Ido Kessler", "Victor Gomes", "Zelda Mariet", "Danny Driess", "Alessandro Agostini", "Canfer Akbulut", "Jingcao Hu", "Marissa Ikonomidis", "Emily Caveness", "Kartik Audhkhasi", "Saurabh Agrawal", "Ioana Bica", "Evan Senter", "Jayaram Mudigonda", "Kelly Chen", "Jingchen Ye", "Xuanhui Wang", "James Svensson", "Philipp Fränken", "Josh Newlan", "Li Lao", "Eva Schnider", "Sami Alabed", "Joseph Kready", "Jesse Emond", "Afief Halumi", "Tim Zaman", "Chengxi Ye", "Naina Raisinghani", "Vilobh Meshram", "Bo Chang", "Ankit Singh Rawat", "Axel Stjerngren", "Sergey Levi", "Rui Wang", "Xiangzhu Long", "Mitchelle Rasquinha", "Steven Hand", "Aditi Mavalankar", "Lauren Agubuzu", "Sudeshna Roy", "Junquan Chen", "Jarek Wilkiewicz", "Hao Zhou", "Michal Jastrzebski", "Qiong Hu", "Agustin Dal Lago", "Ramya Sree Boppana", "Wei-Jen Ko", "Jennifer Prendki", "Yao Su", "Zhi Li", "Eliza Rutherford", "Girish Ramchandra Rao", "Ramona Comanescu", "Adrià Puigdomènech", "Qihang Chen", "Dessie Petrova", "Christine Chan", "Vedrana Milutinovic", "Felipe Tiengo Ferreira", "Chin-Yi Cheng", "Ming Zhang", "Tapomay Dey", "Sherry Yang", "Ramesh Sampath", "Quoc Le", "Howard Zhou", "Chu-Cheng Lin", "Hoi Lam", "Christine Kaeser-Chen", "Kai Hui", "Dean Hirsch", "Tom Eccles", "Basil Mustafa", "Shruti Rijhwani", "Morgane Rivière", "Yuanzhong Xu", "Junjie Wang", "Xinyang Geng", "Xiance Si", "Arjun Khare", "Cheolmin Kim", "Vahab Mirrokni", "Kamyu Lee", "Khuslen Baatarsukh", "Nathaniel Braun", "Lisa Wang", "Pallavi LV", "Richard Tanburn", "Yuvein", "Zhu", "Fangda Li", "Setareh Ariafar", "Dan Goldberg", "Ken Burke", "Daniil Mirylenka", "Meiqi Guo", "Olaf Ronneberger", "Hadas Natalie Vogel", "Liqun Cheng", "Nishita Shetty", "Johnson Jia", "Thomas Jimma", "Corey Fry", "Ted Xiao", "Martin Sundermeyer", "Ryan Burnell", "Yannis Assael", "Mario Pinto", "JD Chen", "Rohit Sathyanarayana", "Donghyun Cho", "Jing Lu", "Rishabh Agarwal", "Sugato Basu", "Lucas Gonzalez", "Dhruv Shah", "Meng Wei", "Dre Mahaarachchi", "Rohan Agrawal", "Tero Rissa", "Yani Donchev", "Ramiro Leal-Cavazos", "Adrian Hutter", "Markus Mircea", "Alon Jacovi", "Faruk Ahmed", "Jiageng Zhang", "Shuguang Hu", "Bo-Juen Chen", "Jonni Kanerva", "Guillaume Desjardins", "Andrew Lee", "Nikos Parotsidis", "Asier Mujika", "Tobias Weyand", "Jasper Snoek", "Jo Chick", "Kai Chen", "Paul Chang", "Ethan Mahintorabi", "Zi Wang", "Tolly Powell", "Orgad Keller", "Abhirut Gupta", "Claire Sha", "Kanav Garg", "Nicolas Heess", "Ágoston Weisz", "Cassidy Hardin", "Bartek Wydrowski", "Ben Coleman", "Karina Zainullina", "Pankaj Joshi", "Alessandro Epasto", "Terry Spitz", "Binbin Xiong", "Kai Zhao", "Arseniy Klimovskiy", "Ivy Zheng", "Johan Ferret", "Itay Yona", "Waleed Khawaja", "Jean-Baptiste Lespiau", "Maxim Krikun", "Siamak Shakeri", "Timothee Cour", "Bonnie Li", "Igor Krivokon", "Dan Suh", "Alex Hofer", "Jad Al Abdallah", "Nikita Putikhin", "Oscar Akerlund", "Silvio Lattanzi", "Anurag Kumar", "Shane Settle", "Himanshu Srivastava", "Folawiyo Campbell-Ajala", "Edouard Rosseel", "Mihai Dorin Istin", "Nishanth Dikkala", "Anand Rao", "Nick Young", "Kate Lin", "Dhruva Bhaswar", "Yiming Wang", "Jaume Sanchez Elias", "Kritika Muralidharan", "James Keeling", "Dayou Du", "Siddharth Gopal", "Gregory Dibb", "Charles Blundell", "Manolis Delakis", "Jacky Liang", "Marco Tulio Ribeiro", "Georgi Karadzhov", "Guillermo Garrido", "Ankur Bapna", "Jiawei Cao", "Adam Sadovsky", "Pouya Tafti", "Arthur Guez", "Coline Devin", "Yixian Di", "Jinwei Xing", "Chuqiao", "Xu", "Hanzhao Lin", "Chun-Te Chu", "Sameera Ponda", "Wesley Helmholz", "Fan Yang", "Yue Gao", "Sara Javanmardi", "Wael Farhan", "Alex Ramirez", "Ricardo Figueira", "Khe Chai Sim", "Yuval Bahat", "Ashwin Vaswani", "Liangzhe Yuan", "Gufeng Zhang", "Leland Rechis", "Hanjun Dai", "Tayo Oguntebi", "Alexandra Cordell", "Eugénie Rives", "Kaan Tekelioglu", "Naveen Kumar", "Bing Zhang", "Aurick Zhou", "Nikolay Savinov", "Andrew Leach", "Alex Tudor", "Sanjay Ganapathy", "Yanyan Zheng", "Mirko Rossini", "Vera Axelrod", "Arnaud Autef", "Yukun Zhu", "Zheng Zheng", "Mingda Zhang", "Baochen Sun", "Jie Ren", "Nenad Tomasev", "Nithish Kannan", "Amer Sinha", "Charles Chen", "Louis O'Bryan", "Alex Pak", "Aditya Kusupati", "Weel Yang", "Deepak Ramachandran", "Patrick Griffin", "Seokhwan Kim", "Philipp Neubeck", "Craig Schiff", "Tammo Spalink", "Mingyang Ling", "Arun Nair", "Ga-Young Joung", "Linda Deng", "Avishkar Bhoopchand", "Lora Aroyo", "Tom Duerig", "Jordan Griffith", "Gabe Barth-Maron", "Jake Ades", "Alex Haig", "Ankur Taly", "Yunting Song", "Paul Michel", "Dave Orr", "Dean Weesner", "Corentin Tallec", "Carrie Grimes Bostock", "Paul Niemczyk", "Andy Twigg", "Mudit Verma", "Rohith Vallu", "Henry Wang", "Marco Gelmi", "Kiranbir Sodhia", "Aleksandr Chuklin", "Omer Goldman", "Jasmine George", "Liang Bai", "Kelvin Zhang", "Petar Sirkovic", "Efrat Nehoran", "Golan Pundak", "Jiaqi Mu", "Alice Chen", "Alex Greve", "Paulo Zacchello", "David Amos", "Heming Ge", "Eric Noland", "Colton Bishop", "Jeffrey Dudek", "Youhei Namiki", "Elena Buchatskaya", "Jing Li", "Dorsa Sadigh", "Masha Samsikova", "Dan Malkin", "Damien Vincent", "Robert David", "Rob Willoughby", "Phoenix Meadowlark", "Shawn Gao", "Yan Li", "Raj Apte", "Amit Jhindal", "Stein Xudong Lin", "Alex Polozov", "Zhicheng Wang", "Tomas Mery", "Anirudh GP", "Varun Yerram", "Sage Stevens", "Tianqi Liu", "Noah Fiedel", "Charles Sutton", "Matthew Johnson", "Xiaodan Song", "Kate Baumli", "Nir Shabat", "Muqthar Mohammad", "Hao Liu", "Marco Selvi", "Yichao Zhou", "Mehdi Hafezi Manshadi", "Chu-ling Ko", "Anthony Chen", "Michael Bendersky", "Jorge Gonzalez Mendez", "Nisarg Kothari", "Amir Zandieh", "Yiling Huang", "Daniel Andor", "Ellie Pavlick", "Idan Brusilovsky", "Jitendra Harlalka", "Sally Goldman", "Andrew Lampinen", "Guowang Li", "Asahi Ushio", "Somit Gupta", "Lei Zhang", "Chuyuan Kelly Fu", "Madhavi Sewak", "Timo Denk", "Jed Borovik", "Brendan Jou", "Avital Zipori", "Prateek Jain", "Junwen Bai", "Thang Luong", "Jonathan Tompson", "Alice Li", "Li Liu", "George Powell", "Jiajun Shen", "Alex Feng", "Grishma Chole", "Da Yu", "Yinlam Chow", "Tongxin Yin", "Eric Malmi", "Kefan Xiao", "Yash Pande", "Shachi Paul", "Niccolò Dal Santo", "Adil Dostmohamed", "Sergio Guadarrama", "Aaron Phillips", "Thanumalayan Sankaranarayana Pillai", "Gal Yona", "Amin Ghafouri", "Preethi Lahoti", "Benjamin Lee", "Dhruv Madeka", "Eren Sezener", "Simon Tokumine", "Adrian Collister", "Nicola De Cao", "Richard Shin", "Uday Kalra", "Parker Beak", "Emily Nottage", "Ryo Nakashima", "Ivan Jurin", "Vikash Sehwag", "Meenu Gaba", "Junhao Zeng", "Kevin R. McKee", "Fernando Pereira", "Tamar Yakar", "Amayika Panda", "Arka Dhar", "Peilin Zhong", "Daniel Sohn", "Mark Brand", "Lars Lowe Sjoesund", "Viral Carpenter", "Sharon Lin", "Shantanu Thakoor", "Marcus Wainwright", "Ashwin Chaugule", "Pranesh Srinivasan", "Muye Zhu", "Bernett Orlando", "Jack Weber", "Ayzaan Wahid", "Gilles Baechler", "Apurv Suman", "Jovana Mitrović", "Gabe Taubman", "Honglin Yu", "Helen King", "Josh Dillon", "Cathy Yip", "Dhriti Varma", "Tomas Izo", "Levent Bolelli", "Borja De Balle Pigem", "Julia Di Trapani", "Fotis Iliopoulos", "Adam Paszke", "Nishant Ranka", "Joe Zou", "Francesco Pongetti", "Jed McGiffin", "Alex Siegman", "Rich Galt", "Ross Hemsley", "Goran Žužić", "Victor Carbune", "Tao Li", "Myle Ott", "Félix de Chaumont Quitry", "David Vilar Torres", "Yuri Chervonyi", "Tomy Tsai", "Prem Eruvbetine", "Samuel Yang", "Matthew Denton", "Jake Walker", "Slavica Andačić", "Idan Heimlich Shtacher", "Vittal Premachandran", "Harshal Tushar Lehri", "Cip Baetu", "Damion Yates", "Lampros Lamprou", "Mariko Iinuma", "Ioana Mihailescu", "Ben Albrecht", "Shachi Dave", "Susie Sargsyan", "Bryan Perozzi", "Lucas Manning", "Chiyuan Zhang", "Denis Vnukov", "Igor Mordatch", "Raia Hadsell Wolfgang Macherey", "Ryan Kappedal", "Jim Stephan", "Aditya Tripathi", "Klaus Macherey", "Jun Qian", "Abhishek Bhowmick", "Shekoofeh Azizi", "Rémi Leblond", "Shiva Mohan Reddy Garlapati", "Timothy Knight", "Matthew Wiethoff", "Wei-Chih Hung", "Anelia Angelova", "Georgios Evangelopoulos", "Pawel Janus", "Dimitris Paparas", "Matthew Rahtz", "Ken Caluwaerts", "Vivek Sampathkumar", "Daniel Jarrett", "Shadi Noghabi", "Antoine Miech", "Chak Yeung", "Geoff Clark", "Henry Prior", "Fei Zheng", "Jean Pouget-Abadie", "Indro Bhattacharya", "Kalpesh Krishna", "Will Bishop", "Zhe Yuan", "Yunxiao Deng", "Ashutosh Sathe", "Kacper Krasowiak", "Ciprian Chelba", "Cho-Jui Hsieh", "Kiran Vodrahalli", "Buhuang Liu", "Thomas Köppe", "Amr Khalifa", "Lubo Litchev", "Pichi Charoenpanit", "Reed Roberts", "Sachin Yadav", "Yasumasa Onoe", "Desi Ivanov", "Megha Mohabey", "Vighnesh Birodkar", "Nemanja Rakićević", "Pierre Sermanet", "Vaibhav Mehta", "Krishan Subudhi", "Travis Choma", "Will Ng", "Luheng He", "Kathie Wang", "Tasos Kementsietsidis", "Shane Gu", "Mansi Gupta", "Andrew Nystrom", "Mehran Kazemi", "Timothy Chung", "Nacho Cano", "Nikhil Dhawan", "Yufei Wang", "Jiawei Xia", "Trevor Yacovone", "Eric Jia", "Mingqing Chen", "Simeon Ivanov", "Ashrith Sheshan", "Sid Dalmia", "Paweł Stradomski", "Pengcheng Yin", "Salem Haykal", "Congchao Wang", "Dennis Duan", "Neslihan Bulut", "Greg Kochanski", "Liam MacDermed", "Namrata Godbole", "Shitao Weng", "Jingjing Chen", "Rachana Fellinger", "Ramin Mehran", "Daniel Suo", "Hisham Husain", "Tong He", "Kaushal Patel", "Joshua Howland", "Randall Parker", "Kelvin Nguyen", "Sharath Maddineni", "Chris Rawles", "Mina Khan", "Shlomi Cohen-Ganor", "Amol Mandhane", "Xinyi Wu", "Chenkai Kuang", "Iulia Comşa", "Ramya Ganeshan", "Hanie Sedghi", "Adam Bloniarz", "Nuo Wang Pierse", "Anton Briukhov", "Petr Mitrichev", "Anita Gergely", "Serena Zhan", "Allan Zhou", "Nikita Saxena", "Eva Lu", "Josef Dean", "Ashish Gupta", "Nicolas Perez-Nieves", "Renjie Wu", "Cory McLean", "Wei Liang", "Disha Jindal", "Anton Tsitsulin", "Wenhao Yu", "Kaiz Alarakyia", "Tom Schaul", "Piyush Patil", "Peter Sung", "Elijah Peake", "Hongkun Yu", "Feryal Behbahani", "JD Co-Reyes", "Alan Ansell", "Sean Sun", "Clara Barbu", "Jonathan Lee", "Seb Noury", "James Allingham", "Bilal Piot", "Mohit Sharma", "Christopher Yew", "Ivan Korotkov", "Bibo Xu", "Demetra Brady", "Goran Petrovic", "Shibl Mourad", "Claire Cui", "Aditya Gupta", "Parker Schuh", "Saarthak Khanna", "Anna Goldie", "Abhinav Arora", "Vadim Zubov", "Amy Stuart", "Mark Epstein", "Yun Zhu", "Jianqiao Liu", "Yury Stuken", "Ziyue Wang", "Karolis Misiunas", "Dee Guo", "Ashleah Gill", "Ale Hartman", "Zaid Nabulsi", "Aurko Roy", "Aleksandra Faust", "Jason Riesa", "Ben Withbroe", "Mengchao Wang", "Marco Tagliasacchi", "Andreea Marzoca", "James Noraky", "Serge Toropov", "Malika Mehrotra", "Bahram Raad", "Sanja Deur", "Steve Xu", "Marianne Monteiro", "Zhongru Wu", "Yi Luan", "Sam Ritter", "Nick Li", "Håvard Garnes", "Yanzhang He", "Martin Zlocha", "Jifan Zhu", "Matteo Hessel", "Will Wu", "Spandana Raj Babbula", "Chizu Kawamoto", "Yuanzhen Li", "Mehadi Hassen", "Yan Wang", "Brian Wieder", "James Freedman", "Yin Zhang", "Xinyi Bai", "Tianli Yu", "David Reitter", "XiangHai Sheng", "Mateo Wirth", "Aditya Kini", "Dima Damen", "Mingcen Gao", "Rachel Hornung", "Michael Voznesensky", "Brian Roark", "Adhi Kuncoro", "Yuxiang Zhou", "Rushin Shah", "Anthony Brohan", "Kuangyuan Chen", "James Wendt", "David Rim", "Paul Kishan Rubenstein", "Jonathan Halcrow", "Michelle Liu", "Ty Geri", "Yunhsuan Sung", "Jane Shapiro", "Shaan Bijwadia", "Chris Duvarney", "Christina Sorokin", "Paul Natsev", "Reeve Ingle", "Pramod Gupta", "Young Maeng", "Ndaba Ndebele", "Kexin Zhu", "Valentin Anklin", "Katherine Lee", "Yuan Liu", "Yaroslav Akulov", "Shaleen Gupta", "Guolong Su", "Flavien Prost", "Tianlin Liu", "Vitaly Kovalev", "Pol Moreno", "Martin Scholz", "Sam Redmond", "Zongwei Zhou", "Alex Castro-Ros", "André Susano Pinto", "Dia Kharrat", "Michal Yarom", "Rachel Saputro", "Jannis Bulian", "Ben Caine", "Ji Liu", "Abbas Abdolmaleki", "Shariq Iqbal", "Tautvydas Misiunas", "Mikhail Sirotenko", "Shefali Garg", "Guy Bensky", "Huan Gui", "Xuezhi Wang", "Raphael Koster", "Mike Bernico", "Da Huang", "Romal Thoppilan", "Trevor Cohn", "Ben Golan", "Wenlei Zhou", "Andrew Rosenberg", "Markus Freitag", "Tynan Gangwani", "Vincent Tsang", "Anand Shukla", "Xiaoqi Ren", "Minh Giang", "Chi Zou", "Andre Elisseeff", "Charline Le Lan", "Dheeru Dua", "Shuba Lall", "Pranav Shyam", "Frankie Garcia", "Sarah Nguyen", "Michael Guzman", "AJ Maschinot", "Marcello Maggioni", "Ming-Wei Chang", "Karol Gregor", "Lotte Weerts", "Kumaran Venkatesan", "Bogdan Damoc", "Leon Liu", "Jan Wassenberg", "Lewis Ho", "Becca Roelofs", "Majid Hadian", "François-Xavier Aubet", "Yu Liang", "Sami Lachgar", "Danny Karmon", "Yong Cheng", "Amelio Vázquez-Reina", "Angie Chen", "Zhuyun Dai", "Andy Brock", "Shubham Agrawal", "Chenxi Pang", "Peter Garst", "Mariella Sanchez-Vargas", "Ivor Rendulic", "Aditya Ayyar", "Andrija Ražnatović", "Olivia Ma", "Roopali Vij", "Neha Sharma", "Ashwin Balakrishna", "Bingyuan Liu", "Ian Mackinnon", "Sorin Baltateanu", "Petra Poklukar", "Gabriel Ibagon", "Colin Ji", "Hongyang Jiao", "Isaac Noble", "Wojciech Stokowiec", "Zhihao Li", "Jeff Dean", "David Lindner", "Mark Omernick", "Kristen Chiafullo", "Mason Dimarco", "Vitor Rodrigues", "Vittorio Selo", "Garrett Honke", "Xintian", "Wu", "Wei He", "Adam Hillier", "Anhad Mohananey", "Vihari Piratla", "Chang Ye", "Chase Malik", "Sebastian Riedel", "Samuel Albanie", "Zi Yang", "Kenny Vassigh", "Maria Bauza", "Sheng Li", "Yiqing Tao", "Nevan Wichers", "Andrii Maksai", "Abe Ittycheriah", "Ross Mcilroy", "Bryan Seybold", "Noah Goodman", "Romina Datta", "Steven M. Hernandez", "Tian Shi", "Yony Kochinski", "Anna Bulanova", "Ken Franko", "Mikita Sazanovich", "Nicholas FitzGerald", "Praneeth Kacham", "Shubha Srinivas Raghvendra", "Vincent Hellendoorn", "Alexander Grushetsky", "Julian Salazar", "Angeliki Lazaridou", "Jason Chang", "Jan-Thorsten Peter", "Sushant Kafle", "Yann Dauphin", "Abhishek Rao", "Filippo Graziano", "Izhak Shafran", "Yuguo Liao", "Tianli Ding", "Geng Yan", "Grace Chu", "Zhao Fu", "Vincent Roulet", "Gabriel Rasskin", "Duncan Williams", "Shahar Drath", "Alex Mossin", "Raphael Hoffmann", "Jordi Orbay", "Francesco Bertolini", "Hila Sheftel", "Justin Chiu", "Siyang Xue", "Yuheng Kuang", "Ferjad Naeem", "Swaroop Nath", "Nana Nti", "Phil Culliton", "Kashyap Krishnakumar", "Michael Isard", "Pei Sun", "Ayan Chakrabarti", "Nathan Clement", "Regev Cohen", "Arissa Wongpanich", "GS Oh", "Ashwin Murthy", "Hao Zheng", "Jessica Hamrick", "Oskar Bunyan", "Suhas Ganesh", "Nitish Gupta", "Roy Frostig", "John Wieting", "Yury Malkov", "Pierre Marcenac", "Zhixin", "Lai", "Xiaodan Tang", "Mohammad Saleh", "Fedir Zubach", "Chinmay Kulkarni", "Huanjie Zhou", "Vicky Zayats", "Nan Ding", "Anshuman Tripathi", "Arijit Pramanik", "Patrik Zochbauer", "Harish Ganapathy", "Vedant Misra", "Zach Behrman", "Hugo Vallet", "Mingyang Zhang", "Mukund Sridhar", "Ye Jin", "Mohammad Babaeizadeh", "Siim Põder", "Megha Goel", "Divya Jain", "Tajwar Nasir", "Shubham Mittal", "Tim Dozat", "Diego Ardila", "Aliaksei Severyn", "Fabio Pardo", "Sammy Jerome", "Siyang Qin", "Louis Rouillard", "Amir Yazdanbakhsh", "Zizhao Zhang", "Shivani Agrawal", "Kaushik Shivakumar", "Caden Lu", "Praveen Kallakuri", "Rachita Chhaparia", "Kanishka Rao", "Charles Kwong", "Asya Fadeeva", "Shitij Nigam", "Yan Virin", "Yuan Zhang", "Balaji Venkatraman", "Beliz Gunel", "Marc Wilson", "Huiyu Wang", "Abhinav Gupta", "Xiaowei Xu", "Adrien Ali Taïga", "Kareem Mohamed", "Doug Fritz", "Daniel Rodriguez", "Zoubin Ghahramani", "Harry Askham", "Lior Belenki", "James Zhao", "Rahul Gupta", "Krzysztof Jastrzębski", "Takahiro Kosakai", "Kaan Katircioglu", "Jon Schneider", "Rina Panigrahy", "Konstantinos Bousmalis", "Peter Grabowski", "Prajit Ramachandran", "Chaitra Hegde", "Mihaela Rosca", "Angelo Scorza Scarpati", "Kyriakos Axiotis", "Ying Xu", "Zach Gleicher", "Assaf Hurwitz Michaely", "Mandar Sharma", "Sanil Jain", "Christoph Hirnschall", "Tal Marian", "Xuhui Jia", "Kevin Mather", "Kilol Gupta", "Linhai Qiu", "Nigamaa Nayakanti", "Lucian Ionita", "Steven Zheng", "Lucia Loher", "Kurt Shuster", "Igor Petrovski", "Roshan Sharma", "Rahma Chaabouni", "Angel Yeh", "James An", "Arushi Gupta", "Steven Schwarcz", "Seher Ellis", "Sam Conway-Rahman", "Javier Snaider", "Alex Zhai", "James Atwood", "Daniel Golovin", "Liqian Peng", "Te I", "Vivian Xia", "Salvatore Scellato", "Mahan Malihi", "Arthur Bražinskas", "Vlad-Doru Ion", "Younghoon Jun", "James Swirhun", "Soroosh Mariooryad", "Jiao Sun", "Steve Chien", "Rey Coaguila", "Ariel Brand", "Yi Gao", "Tom Kwiatkowski", "Roee Aharoni", "Cheng-Chun Lee", "Mislav Žanić", "Yichi Zhang", "Dan Ethier", "Vitaly Nikolaev", "Pranav Nair", "Yoav Ben Shalom", "Hen Fitoussi", "Jai Gupta", "Hongbin Liu", "Dee Cattle", "Tolga Bolukbasi", "Ben Murdoch", "Fantine Huot", "Yin Li", "Chris Hahn"], "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities", "comment": "72 pages, 17 figures", "summary": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and\nGemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite\nmodels. Gemini 2.5 Pro is our most capable model yet, achieving SoTA\nperformance on frontier coding and reasoning benchmarks. In addition to its\nincredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that\nexcels at multimodal understanding and it is now able to process up to 3 hours\nof video content. Its unique combination of long context, multimodal and\nreasoning capabilities can be combined to unlock new agentic workflows. Gemini\n2.5 Flash provides excellent reasoning abilities at a fraction of the compute\nand latency requirements and Gemini 2.0 Flash and Flash-Lite provide high\nperformance at low latency and cost. Taken together, the Gemini 2.X model\ngeneration spans the full Pareto frontier of model capability vs cost, allowing\nusers to explore the boundaries of what is possible with complex agentic\nproblem solving.", "AI": {"tldr": "Gemini 2.X模型家族包括Gemini 2.5 Pro和2.5 Flash，以及早期的2.0 Flash和Flash-Lite。2.5 Pro在多模态理解和长上下文处理上表现卓越，支持3小时视频内容处理；2.5 Flash以低计算需求提供优秀推理能力；2.0 Flash和Flash-Lite则以低延迟和低成本提供高性能。", "motivation": "开发Gemini 2.X模型家族旨在覆盖模型能力与成本的全帕累托前沿，支持复杂代理问题解决。", "method": "通过改进模型架构和优化计算资源，Gemini 2.5 Pro实现了长上下文和多模态理解能力，2.5 Flash则专注于低计算需求下的高效推理。", "result": "Gemini 2.5 Pro在编码和推理基准测试中达到SoTA性能，2.5 Flash和2.0系列在低成本和低延迟下表现优异。", "conclusion": "Gemini 2.X模型家族为用户提供了从高性能到低成本的全方位选择，推动了复杂代理工作流的发展。"}}
{"id": "2507.06306", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.06306", "abs": "https://arxiv.org/abs/2507.06306", "authors": ["Neil Rathi", "Dan Jurafsky", "Kaitlyn Zhou"], "title": "Humans overrely on overconfident language models, across languages", "comment": "10 pages main text, to appear at COLM 2025", "summary": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Previous work has shown that LLMs are\nlinguistically overconfident in English, leading users to overrely on confident\ngenerations. However, the usage and interpretation of epistemic markers (e.g.,\n'It's definitely,' 'I think') can differ sharply across languages. Here, we\nstudy the risks of multilingual linguistic (mis)calibration, overconfidence,\nand overreliance across five languages to evaluate the safety of LLMs in a\nglobal context.\n  We find that overreliance risks are high across all languages. We first\nanalyze the distribution of LLM-generated epistemic markers, and observe that\nwhile LLMs are cross-linguistically overconfident, they are also sensitive to\ndocumented linguistic variation. For example, models generate the most markers\nof uncertainty in Japanese and the most markers of certainty in German and\nMandarin. We then measure human reliance rates across languages, finding that\nwhile users strongly rely on confident LLM generations in all languages,\nreliance behaviors differ cross-linguistically: for example, users rely\nsignificantly more on expressions of uncertainty in Japanese than in English.\nTaken together, these results indicate high risk of reliance on overconfident\nmodel generations across languages. Our findings highlight the challenges of\nmultilingual linguistic calibration and stress the importance of culturally and\nlinguistically contextualized model safety evaluations.", "AI": {"tldr": "研究发现大型语言模型（LLM）在多语言环境中存在过度自信和用户过度依赖的问题，不同语言中不确定性标记的使用和解读差异显著。", "motivation": "评估LLM在全球范围内的安全性，特别是其在不同语言中的校准问题，以避免用户对模型输出的过度依赖。", "method": "分析五种语言中LLM生成的认识标记分布，并测量用户在不同语言中对模型输出的依赖行为。", "result": "LLM在所有语言中均表现出过度自信，但能敏感捕捉语言差异；用户在所有语言中均高度依赖自信输出，但依赖行为因语言而异。", "conclusion": "多语言校准面临挑战，需结合文化和语言背景进行模型安全性评估。"}}
{"id": "2507.06313", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06313", "abs": "https://arxiv.org/abs/2507.06313", "authors": ["Kiarash Zahirnia", "Zahra Golpayegani", "Walid Ahmad", "Yang Liu"], "title": "ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time", "comment": null, "summary": "Transformer-based Language Models' computation and memory overhead increase\nquadratically as a function of sequence length. The quadratic cost poses\nchallenges when employing LLMs for processing long sequences. In this work, we\nintroduce \\ourmodelacronym~(Extend at Test-Time), method for extending the\ncontext length of short context Transformer-based LLMs, with constant memory\nrequirement and linear computation overhead. ETT enable the extension of the\ncontext length at test-time by efficient fine-tuning the model's parameters on\nthe input context, chunked into overlapping small subsequences. We evaluate ETT\non LongBench by extending the context length of GPT-Large and Phi-2 up to 32\ntimes, increasing from 1k to 32k tokens. This results in up to a 30 percent\nimprovement in the model's accuracy. We also study how context can be stored in\nLLM's weights effectively and efficiently. Through a detailed ablation study,\nwe examine which Transformer modules are most beneficial to fine-tune at\ntest-time. Interestingly, we find that fine-tuning the second layer of the FFNs\nis more effective than full fine-tuning, leading to a further improvement in\nthe models' accuracy.", "AI": {"tldr": "论文提出ETT方法，通过高效微调短上下文Transformer模型的参数，以线性计算开销和恒定内存需求扩展上下文长度，提升模型在长序列任务中的表现。", "motivation": "Transformer模型的计算和内存开销随序列长度呈二次增长，限制了其在长序列任务中的应用。", "method": "ETT方法通过在测试时对输入上下文的子序列进行微调，扩展上下文长度，同时保持计算和内存效率。", "result": "在LongBench上，ETT将GPT-Large和Phi-2的上下文长度从1k扩展到32k，准确率提升30%。", "conclusion": "ETT有效扩展了上下文长度，且仅微调部分模块（如FFN的第二层）比全微调效果更好。"}}
{"id": "2507.06335", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06335", "abs": "https://arxiv.org/abs/2507.06335", "authors": ["Casey Kennington", "David Schlangen"], "title": "Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?", "comment": "9 pages", "summary": "Formal, Distributional, and Grounded theories of computational semantics each\nhave their uses and their drawbacks. There has been a shift to ground models of\nlanguage by adding visual knowledge, and there has been a call to enrich models\nof language with symbolic methods to gain the benefits from formal,\ndistributional, and grounded theories. In this paper, we attempt to make the\ncase that one potential path forward in unifying all three semantic fields is\npaved with the words-as-classifier model, a model of word-level grounded\nsemantics that has been incorporated into formalisms and distributional\nlanguage models in the literature, and it has been well-tested within\ninteractive dialogue settings. We review that literature, motivate the\nwords-as-classifiers model with an appeal to recent work in cognitive science,\nand describe a small experiment. Finally, we sketch a model of semantics\nunified through words-as-classifiers.", "AI": {"tldr": "本文探讨了如何通过“词作为分类器”模型统一形式、分布和接地语义理论，结合视觉知识和符号方法，并展示了其在对话环境中的应用。", "motivation": "动机在于结合形式、分布和接地语义理论的优点，以弥补各自的不足，并通过视觉知识和符号方法丰富语言模型。", "method": "方法包括回顾相关文献，借鉴认知科学的最新研究，进行小规模实验，并提出基于“词作为分类器”的统一语义模型。", "result": "结果表明“词作为分类器”模型在对话环境中表现良好，并能有效结合多种语义理论。", "conclusion": "结论认为“词作为分类器”模型是统一形式、分布和接地语义理论的有前景的路径。"}}
{"id": "2507.06352", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06352", "abs": "https://arxiv.org/abs/2507.06352", "authors": ["Senol Gulgonul"], "title": "Revisiting Chien-Hrones-Reswick Method for an Analytical Solution", "comment": "7 pages, 3 figures, 1 table. This work is licensed under CC BY-NC-ND\n  4.0. For commercial licensing, contact the author", "summary": "This study presents an analytical method for tuning PI controllers in\nFirst-Order with Time Delay (FOTD) systems, leveraging the Lambert W function.\nThe Lambert W function enables exact pole placement, yielding analytical\nexpressions for PI gains. The proposed approach identifies a critical condition\nthat achieves a step response without overshoot with minimum settling time,\nwhile also providing explicit tuning rules for systems where controlled\novershoot is specified. The method demonstrates strong agreement with\nestablished empirical Chien-Hrones-Reswick tuning rules for both\nnon-overshooting and overshooting cases, bridging the gap between theoretical\nanalysis and empirical results.", "AI": {"tldr": "提出了一种基于Lambert W函数的PI控制器调参方法，用于一阶时滞系统，实现精确极点配置和无超调响应。", "motivation": "解决一阶时滞系统中PI控制器调参的理论与经验差距问题。", "method": "利用Lambert W函数进行极点配置，推导PI增益的解析表达式。", "result": "方法在无超调和有超调情况下均与Chien-Hrones-Reswick经验规则高度一致。", "conclusion": "该方法填补了理论分析与经验结果之间的空白，适用于多种调参需求。"}}
{"id": "2507.06346", "categories": ["cs.RO", "stat.CO"], "pdf": "https://arxiv.org/pdf/2507.06346", "abs": "https://arxiv.org/abs/2507.06346", "authors": ["Li Zhou", "Elvan Ceyhan"], "title": "Solving the Constrained Random Disambiguation Path Problem via Lagrangian Relaxation and Graph Reduction", "comment": null, "summary": "We study a resource-constrained variant of the Random Disambiguation Path\n(RDP) problem, a generalization of the Stochastic Obstacle Scene (SOS) problem,\nin which a navigating agent must reach a target in a spatial environment\npopulated with uncertain obstacles. Each ambiguous obstacle may be\ndisambiguated at a (possibly) heterogeneous resource cost, subject to a global\ndisambiguation budget. We formulate this constrained planning problem as a\nWeight-Constrained Shortest Path Problem (WCSPP) with risk-adjusted edge costs\nthat incorporate probabilistic blockage and traversal penalties. To solve it,\nwe propose a novel algorithmic framework-COLOGR-combining Lagrangian relaxation\nwith a two-phase vertex elimination (TPVE) procedure. The method prunes\ninfeasible and suboptimal paths while provably preserving the optimal solution,\nand leverages dual bounds to guide efficient search. We establish correctness,\nfeasibility guarantees, and surrogate optimality under mild assumptions. Our\nanalysis also demonstrates that COLOGR frequently achieves zero duality gap and\noffers improved computational complexity over prior constrained path-planning\nmethods. Extensive simulation experiments validate the algorithm's robustness\nacross varying obstacle densities, sensor accuracies, and risk models,\nconsistently outperforming greedy baselines and approaching offline-optimal\nbenchmarks. The proposed framework is broadly applicable to stochastic network\ndesign, mobility planning, and constrained decision-making under uncertainty.", "AI": {"tldr": "本文研究了资源受限的随机消歧路径（RDP）问题，提出了一种结合拉格朗日松弛和两阶段顶点消除（TPVE）的新算法框架COLOGR，以解决约束规划问题。", "motivation": "在空间环境中，导航代理需要在不确定障碍物的情况下到达目标，而消歧每个障碍物的资源成本可能不同且受全局预算限制。", "method": "将问题建模为带权约束的最短路径问题（WCSPP），提出COLOGR算法框架，结合拉格朗日松弛和TPVE，修剪不可行和次优路径。", "result": "COLOGR在多种障碍物密度、传感器精度和风险模型下表现稳健，优于贪婪基线并接近离线最优基准。", "conclusion": "COLOGR框架在随机网络设计、移动规划和不确定性下的约束决策中具有广泛适用性。"}}
{"id": "2507.06264", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06264", "abs": "https://arxiv.org/abs/2507.06264", "authors": ["Weronika Hryniewska-Guzik", "Przemyslaw Biecek"], "title": "X-ray transferable polyrepresentation learning", "comment": "part of Weronika's PhD thesis", "summary": "The success of machine learning algorithms is inherently related to the\nextraction of meaningful features, as they play a pivotal role in the\nperformance of these algorithms. Central to this challenge is the quality of\ndata representation. However, the ability to generalize and extract these\nfeatures effectively from unseen datasets is also crucial. In light of this, we\nintroduce a novel concept: the polyrepresentation. Polyrepresentation\nintegrates multiple representations of the same modality extracted from\ndistinct sources, for example, vector embeddings from the Siamese Network,\nself-supervised models, and interpretable radiomic features. This approach\nyields better performance metrics compared to relying on a single\nrepresentation. Additionally, in the context of X-ray images, we demonstrate\nthe transferability of the created polyrepresentation to a smaller dataset,\nunderscoring its potential as a pragmatic and resource-efficient approach in\nvarious image-related solutions. It is worth noting that the concept of\npolyprepresentation on the example of medical data can also be applied to other\ndomains, showcasing its versatility and broad potential impact.", "AI": {"tldr": "论文提出了一种名为“多表示法”（polyrepresentation）的新概念，通过整合来自不同来源的同一模态的多种表示（如Siamese Network的向量嵌入、自监督模型和可解释的放射组学特征），提升了机器学习算法的性能。", "motivation": "机器学习算法的成功与特征提取密切相关，而数据表示的质量和从未见过的数据集中有效提取特征的能力是关键挑战。", "method": "提出多表示法，整合同一模态的多种表示（如Siamese Network、自监督模型和放射组学特征）。", "result": "多表示法在性能指标上优于单一表示法，并在X射线图像数据上展示了其可迁移性和资源效率。", "conclusion": "多表示法不仅适用于医学数据，还具有广泛的潜在应用领域，展示了其多功能性和实用性。"}}
{"id": "2507.06234", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06234", "abs": "https://arxiv.org/abs/2507.06234", "authors": ["Jiangzhong Cao", "Zekai Zeng", "Xu Zhang", "Huan Zhang", "Chunling Fan", "Gangyi Jiang", "Weisi Lin"], "title": "Unveiling the Underwater World: CLIP Perception Model-Guided Underwater Image Enhancement", "comment": "10 pages, 7 figures;Accepted to PR 2025;The source code is available\n  at https://github.com/Ave001025/UIE_CLIP", "summary": "High-quality underwater images are essential for both machine vision tasks\nand viewers with their aesthetic appeal.However, the quality of underwater\nimages is severely affected by light absorption and scattering. Deep\nlearning-based methods for Underwater Image Enhancement (UIE) have achieved\ngood performance. However, these methods often overlook considering human\nperception and lack sufficient constraints within the solution space.\nConsequently, the enhanced images often suffer from diminished perceptual\nquality or poor content restoration.To address these issues, we propose a UIE\nmethod with a Contrastive Language-Image Pre-Training (CLIP) perception loss\nmodule and curriculum contrastive regularization. Above all, to develop a\nperception model for underwater images that more aligns with human visual\nperception, the visual semantic feature extraction capability of the CLIP model\nis leveraged to learn an appropriate prompt pair to map and evaluate the\nquality of underwater images. This CLIP perception model is then incorporated\nas a perception loss module into the enhancement network to improve the\nperceptual quality of enhanced images. Furthermore, the CLIP perception model\nis integrated with the curriculum contrastive regularization to enhance the\nconstraints imposed on the enhanced images within the CLIP perceptual space,\nmitigating the risk of both under-enhancement and over-enhancement.\nSpecifically, the CLIP perception model is employed to assess and categorize\nthe learning difficulty level of negatives in the regularization process,\nensuring comprehensive and nuanced utilization of distorted images and\nnegatives with varied quality levels. Extensive experiments demonstrate that\nour method outperforms state-of-the-art methods in terms of visual quality and\ngeneralization ability.", "AI": {"tldr": "提出了一种结合CLIP感知损失模块和课程对比正则化的水下图像增强方法，以提升感知质量和内容恢复。", "motivation": "水下图像质量受光吸收和散射影响，现有深度学习方法忽视人类感知且缺乏解空间约束，导致增强图像感知质量下降或内容恢复不佳。", "method": "利用CLIP模型的视觉语义特征提取能力学习提示对映射和评估图像质量，并将其作为感知损失模块融入增强网络；结合课程对比正则化增强CLIP感知空间约束。", "result": "实验表明，该方法在视觉质量和泛化能力上优于现有技术。", "conclusion": "结合CLIP感知损失和课程对比正则化能有效提升水下图像增强的感知质量和内容恢复效果。"}}
{"id": "2507.06373", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.06373", "abs": "https://arxiv.org/abs/2507.06373", "authors": ["Jeremy Fischer", "Ram Krishnamoorthy", "Vishal Kumar", "Mahdi Al-Husseini"], "title": "Digital Wargames to Enhance Military Medical Evacuation Decision-Making", "comment": null, "summary": "Medical evacuation is one of the United States Army's most storied and\ncritical mission sets, responsible for efficiently and expediently evacuating\nthe battlefield ill and injured. Medical evacuation planning involves designing\na robust network of medical platforms and facilities capable of moving and\ntreating large numbers of casualties. Until now, there has not been a medium to\nsimulate these networks in a classroom setting and evaluate both offline\nplanning and online decision-making performance. This work describes the\nMedical Evacuation Wargaming Initiative (MEWI), a three-dimensional multiplayer\nsimulation developed in Unity that replicates battlefield constraints and\nuncertainties. MEWI accurately models patient interactions at casualty\ncollection points, ambulance exchange points, medical treatment facilities, and\nevacuation platforms. Two operational scenarios are introduced: an amphibious\nisland assault in the Pacific and a Eurasian conflict across a sprawling road\nand river network. These scenarios pit students against the clock to save as\nmany casualties as possible while adhering to doctrinal lessons learned during\ndidactic training. We visualize performance data collected from two iterations\nof the MEWI Pacific scenario executed in the United States Army's Medical\nEvacuation Doctrine Course. We consider post-wargame Likert survey data from\nstudent participants and external observer notes to identify key planning\ndecision points, document medical evacuation lessons learned, and quantify\ngeneral utility. Results indicate that MEWI participation substantially\nimproves uptake of medical evacuation lessons learned and co-operative\ndecision-making. MEWI is a substantial step forward in the field of\nhigh-fidelity training tools for medical education, and our study findings\noffer critical insights into improving medical evacuation education and\noperations across the joint force.", "AI": {"tldr": "MEWI是一个基于Unity开发的3D多人模拟工具，用于模拟战场医疗撤离网络，提升学生决策能力和学习效果。", "motivation": "缺乏在课堂环境中模拟医疗撤离网络并评估规划和决策性能的工具。", "method": "开发MEWI模拟器，模拟战场约束和不确定性，包括患者交互和设施网络。", "result": "MEWI显著提高了学生对医疗撤离知识的学习效果和协作决策能力。", "conclusion": "MEWI是医疗教育高保真培训工具的重要进步，为联合部队提供了改进医疗撤离教育和操作的关键见解。"}}
{"id": "2507.06378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06378", "abs": "https://arxiv.org/abs/2507.06378", "authors": ["Catherine Arnett", "Marisa Hudspeth", "Brendan O'Connor"], "title": "Evaluating Morphological Alignment of Tokenizers in 70 Languages", "comment": "6 pages, 3 figures. Accepted to the Tokenization Workshop at ICML\n  2025", "summary": "While tokenization is a key step in language modeling, with effects on model\ntraining and performance, it remains unclear how to effectively evaluate\ntokenizer quality. One proposed dimension of tokenizer quality is the extent to\nwhich tokenizers preserve linguistically meaningful subwords, aligning token\nboundaries with morphological boundaries within a word. We expand MorphScore\n(Arnett & Bergen, 2025), which previously covered 22 languages, to support a\ntotal of 70 languages. The updated MorphScore offers more flexibility in\nevaluation and addresses some of the limitations of the original version. We\nthen correlate our alignment scores with downstream task performance for five\npre-trained languages models on seven tasks, with at least one task in each of\nthe languages in our sample. We find that morphological alignment does not\nexplain very much variance in model performance, suggesting that morphological\nalignment alone does not measure dimensions of tokenization quality relevant to\nmodel performance.", "AI": {"tldr": "论文扩展了MorphScore以评估70种语言的tokenizer质量，发现形态对齐对模型性能影响有限。", "motivation": "研究如何有效评估tokenizer质量，特别是其是否保留语言学的有意义子词。", "method": "扩展MorphScore至70种语言，并分析形态对齐分数与下游任务性能的相关性。", "result": "形态对齐对模型性能的方差解释有限，表明其单独不足以衡量tokenizer质量。", "conclusion": "形态对齐并非衡量tokenizer质量的关键维度，需进一步探索其他因素。"}}
{"id": "2507.06383", "categories": ["eess.SY", "cs.CE", "cs.LO", "cs.SY", "I.2.1"], "pdf": "https://arxiv.org/pdf/2507.06383", "abs": "https://arxiv.org/abs/2507.06383", "authors": ["Mustafa Shabani", "Alireza Nasiri", "Hassan Nafardi"], "title": "Forex Trading Robot Using Fuzzy Logic", "comment": null, "summary": "In this study, we propose a fuzzy system for conducting short-term\ntransactions in the forex market. The system is designed to enhance common\nstrategies in the forex market using fuzzy logic, thereby improving the\naccuracy of transactions. Traditionally, technical strategies based on\noscillator indicators have relied on predefined ranges for indicators such as\nRelative Strength Index (RSI), Commodity Channel Indicator (CCI), and\nStochastic to determine entry points for trades. However, the use of these\nclassic indicators has yielded suboptimal results due to the changing nature of\nthe market over time. In our proposed approach, instead of employing classical\nindicators, we introduce a fuzzy Mamdani system for each indicator. The results\nobtained from these systems are then combined through voting to design a\ntrading robot. Our findings demonstrate a considerable increase in the\nprofitability factor compared to three other methods. Additionally, net profit,\ngross profit, and maximum capital reduction are calculated and compared across\nall approaches.", "AI": {"tldr": "提出了一种基于模糊逻辑的外汇市场短期交易系统，通过模糊Mamdani系统改进传统技术指标，显著提高了交易盈利性。", "motivation": "传统技术指标（如RSI、CCI、Stochastic）因市场变化导致效果不佳，需改进以提高交易准确性。", "method": "为每个指标引入模糊Mamdani系统，并通过投票机制设计交易机器人。", "result": "相比其他三种方法，盈利因子显著提升，净利、毛利和最大资本回撤均有比较。", "conclusion": "模糊逻辑系统能有效改进外汇交易策略，提高盈利性。"}}
{"id": "2507.06397", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06397", "abs": "https://arxiv.org/abs/2507.06397", "authors": ["Michalis Chatzispyrou", "Luke Horgan", "Hyunkil Hwang", "Harish Sathishchandra", "Monika Roznere", "Alberto Quattrini Li", "Philippos Mordohai", "Ioannis Rekleitis"], "title": "Mapping the Catacombs: An Underwater Cave Segment of the Devil's Eye System", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "This paper presents a framework for mapping underwater caves. Underwater\ncaves are crucial for fresh water resource management, underwater archaeology,\nand hydrogeology. Mapping the cave's outline and dimensions, as well as\ncreating photorealistic 3D maps, is critical for enabling a better\nunderstanding of this underwater domain. In this paper, we present the mapping\nof an underwater cave segment (the catacombs) of the Devil's Eye cave system at\nGinnie Springs, FL. We utilized a set of inexpensive action cameras in\nconjunction with a dive computer to estimate the trajectories of the cameras\ntogether with a sparse point cloud. The resulting reconstructions are utilized\nto produce a one-dimensional retract of the cave passages in the form of the\naverage trajectory together with the boundaries (top, bottom, left, and right).\nThe use of the dive computer enables the observability of the z-dimension in\naddition to the roll and pitch in a visual/inertial framework (SVIn2). In\naddition, the keyframes generated by SVIn2 together with the estimated camera\nposes for select areas are used as input to a global optimization (bundle\nadjustment) framework -- COLMAP -- in order to produce a dense reconstruction\nof those areas. The same cave segment is manually surveyed using the MNemo V2\ninstrument, providing an additional set of measurements validating the proposed\napproach. It is worth noting that with the use of action cameras, the primary\ncomponents of a cave map can be constructed. Furthermore, with the utilization\nof a global optimization framework guided by the results of VI-SLAM package\nSVIn2, photorealistic dense 3D representations of selected areas can be\nreconstructed.", "AI": {"tldr": "本文提出了一种利用低成本运动相机和潜水电脑绘制水下洞穴地图的框架，结合视觉/惯性框架和全局优化技术，生成洞穴的一维轨迹和密集3D重建。", "motivation": "水下洞穴对淡水资源管理、水下考古和水文地质学至关重要，但传统测绘方法成本高且复杂。本文旨在开发一种低成本、高效的方法来绘制洞穴轮廓和生成逼真3D地图。", "method": "使用低成本运动相机和潜水电脑估计相机轨迹并生成稀疏点云，结合SVIn2框架和COLMAP全局优化技术，生成洞穴的一维轨迹和密集3D重建。", "result": "实验验证了该方法能够生成洞穴的一维轨迹和密集3D重建，并通过手动测量验证了其准确性。", "conclusion": "低成本运动相机结合视觉/惯性框架和全局优化技术，能够有效绘制水下洞穴地图，并为特定区域生成逼真3D模型。"}}
{"id": "2507.06325", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.06325", "abs": "https://arxiv.org/abs/2507.06325", "authors": ["Nastaran Pourshab"], "title": "Optimization of Fractal Image Compression", "comment": null, "summary": "Fractal Image Compression (FIC) is a lossy image compression technique that\nleverages self-similarity within an image to achieve high compression ratios.\nHowever, the process of compressing the image is computationally expensive.\nThis paper investigates optimization techniques to improve the efficiency of\nFIC, focusing on increasing compression ratio and reducing computational time.\nThe paper explores a novel approach named the Box Counting Method for\nestimating fractal dimensions, which is very simple to integrate into FIC\ncompared to other algorithms. The results show that implementing these\noptimization techniques enhances both the compression ratio and the compression\ntime.", "AI": {"tldr": "论文研究了优化分形图像压缩（FIC）的方法，提出了一种名为Box Counting Method的新方法，以提高压缩比并减少计算时间。", "motivation": "分形图像压缩虽然能实现高压缩比，但计算成本高，因此需要优化以提高效率。", "method": "采用Box Counting Method估计分形维度，并将其集成到FIC中。", "result": "优化技术显著提升了压缩比和压缩时间。", "conclusion": "Box Counting Method是一种简单有效的优化方法，可显著改善FIC的性能。"}}
{"id": "2507.06265", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06265", "abs": "https://arxiv.org/abs/2507.06265", "authors": ["Ali Nasiri-Sarvi", "Hassan Rivaz", "Mahdi S. Hosseini"], "title": "SPARC: Concept-Aligned Sparse Autoencoders for Cross-Model and Cross-Modal Interpretability", "comment": null, "summary": "Understanding how different AI models encode the same high-level concepts,\nsuch as objects or attributes, remains challenging because each model typically\nproduces its own isolated representation. Existing interpretability methods\nlike Sparse Autoencoders (SAEs) produce latent concepts individually for each\nmodel, resulting in incompatible concept spaces and limiting cross-model\ninterpretability. To address this, we introduce SPARC (Sparse Autoencoders for\nAligned Representation of Concepts), a new framework that learns a single,\nunified latent space shared across diverse architectures and modalities (e.g.,\nvision models like DINO, and multimodal models like CLIP). SPARC's alignment is\nenforced through two key innovations: (1) a Global TopK sparsity mechanism,\nensuring all input streams activate identical latent dimensions for a given\nconcept; and (2) a Cross-Reconstruction Loss, which explicitly encourages\nsemantic consistency between models. On Open Images, SPARC dramatically\nimproves concept alignment, achieving a Jaccard similarity of 0.80, more than\ntripling the alignment compared to previous methods. SPARC creates a shared\nsparse latent space where individual dimensions often correspond to similar\nhigh-level concepts across models and modalities, enabling direct comparison of\nhow different architectures represent identical concepts without requiring\nmanual alignment or model-specific analysis. As a consequence of this aligned\nrepresentation, SPARC also enables practical applications such as text-guided\nspatial localization in vision-only models and cross-model/cross-modal\nretrieval. Code and models are available at\nhttps://github.com/AtlasAnalyticsLab/SPARC.", "AI": {"tldr": "SPARC框架通过统一潜在空间和跨模型语义一致性，显著提高了不同AI模型间概念对齐的能力。", "motivation": "解决现有方法因模型间概念空间不兼容而导致的跨模型解释性受限问题。", "method": "引入SPARC框架，采用Global TopK稀疏机制和跨重建损失，实现跨模型和模态的统一潜在空间。", "result": "在Open Images数据集上，SPARC将概念对齐的Jaccard相似度提升至0.80，远超之前方法。", "conclusion": "SPARC不仅实现了跨模型概念对齐，还支持文本引导的空间定位和跨模态检索等应用。"}}
{"id": "2507.06396", "categories": ["cs.AI", "cs.LG", "cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06396", "abs": "https://arxiv.org/abs/2507.06396", "authors": ["Mandana Vaziri", "Louis Mandel", "Yuji Watanabe", "Hirokuni Kitahara", "Martin Hirzel", "Anca Sailer"], "title": "Representing Prompting Patterns with PDL: Compliance Agent Case Study", "comment": "ICML 2025 Workshop on Programmatic Representations for Agent Learning", "summary": "Prompt engineering for LLMs remains complex, with existing frameworks either\nhiding complexity behind restrictive APIs or providing inflexible canned\npatterns that resist customization -- making sophisticated agentic programming\nchallenging. We present the Prompt Declaration Language (PDL), a novel approach\nto prompt representation that tackles this fundamental complexity by bringing\nprompts to the forefront, enabling manual and automatic prompt tuning while\ncapturing the composition of LLM calls together with rule-based code and\nexternal tools. By abstracting away the plumbing for such compositions, PDL\naims at improving programmer productivity while providing a declarative\nrepresentation that is amenable to optimization. This paper demonstrates PDL's\nutility through a real-world case study of a compliance agent. Tuning the\nprompting pattern of this agent yielded up to 4x performance improvement\ncompared to using a canned agent and prompt pattern.", "AI": {"tldr": "论文提出了一种名为Prompt Declaration Language (PDL)的新方法，用于解决LLM提示工程的复杂性，通过提升提示的灵活性和可优化性，显著提高了性能。", "motivation": "现有框架要么通过限制性API隐藏复杂性，要么提供难以自定义的固定模式，使得高级代理编程变得困难。", "method": "提出了PDL，一种新颖的提示表示方法，将提示置于核心位置，支持手动和自动调优，并整合了LLM调用、规则代码和外部工具的组合。", "result": "通过实际案例研究（合规代理），PDL的提示调优实现了高达4倍的性能提升。", "conclusion": "PDL通过抽象化组合的复杂性，提高了程序员效率，并提供了可优化的声明式表示。"}}
{"id": "2507.06393", "categories": ["cs.CL", "math.QA", "math.RA", "91F20, 18M60, 18M80, 16T05, 68Q70"], "pdf": "https://arxiv.org/pdf/2507.06393", "abs": "https://arxiv.org/abs/2507.06393", "authors": ["Matilde Marcolli", "Riny Huijbregts", "Richard K. Larson"], "title": "Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles", "comment": "LaTeX, 48 pages", "summary": "We show that head functions on syntactic objects extend the magma structure\nto a hypermagma, with the c-command relation compatible with the magma\noperation and the m-command relation with the hypermagma. We then show that the\nstructure of head and complement and specifier, additional modifier positions,\nand the structure of phases in the Extended Projection can be formulated as a\nbud generating system of a colored operad, in a form similar to the structure\nof theta roles. We also show that, due to the special form of the colored\noperad generators, the filtering of freely generated syntactic objects by these\ncoloring rules can be equivalently formulated as a filtering in the course of\nstructure formation via a colored Merge, which can in turn be related to the\nhypermagma structure. The rules on movement by Internal Merge with respect to\nphases, the Extended Projection Principle, Empty Category Principle, and Phase\nImpenetrability Condition are all subsumed into the form of the colored operad\ngenerators. Movement compatibilities between the phase structure and the theta\nroles assignments can then be formulated in terms of the respective colored\noperads and a transduction of colored operads.", "AI": {"tldr": "论文展示了句法对象的头部函数如何将岩浆结构扩展到超岩浆结构，并通过彩色操作ad的形式描述了句法结构的生成和过滤。", "motivation": "研究句法结构的数学形式化，特别是头部函数和操作ad在句法生成中的作用。", "method": "使用超岩浆结构和彩色操作ad来描述句法对象的生成和过滤，并分析其与句法规则的关系。", "result": "证明了句法结构和规则可以通过彩色操作ad的形式化表示，并统一了多种句法原则。", "conclusion": "彩色操作ad为句法结构的数学建模提供了统一框架，能够涵盖多种句法现象和规则。"}}
{"id": "2507.06389", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06389", "abs": "https://arxiv.org/abs/2507.06389", "authors": ["Giacomo Baggio", "Marco Fabris"], "title": "How Complex is a Complex Network? Insights from Linear Systems Theory", "comment": "6 pages, 2 figures, 1 table, published on Control Systems Letters\n  (L-CSS), to be presented at the 64th IEEE Conference on Decision and Control,\n  IEEE Control Systems Letters (2025)", "summary": "This paper leverages linear systems theory to propose a principled measure of\ncomplexity for network systems. We focus on a network of first-order scalar\nlinear systems interconnected through a directed graph. By locally filtering\nout the effect of nodal dynamics in the interconnected system, we propose a new\nquantitative index of network complexity rooted in the notion of McMillan\ndegree of a linear system. First, we show that network systems with the same\ninterconnection structure share the same complexity index for almost all\nchoices of their interconnection weights. Then, we investigate the dependence\nof the proposed index on the topology of the network and the pattern of\nheterogeneity of the nodal dynamics. Specifically, we find that the index\ndepends on the matching number of subgraphs identified by nodal dynamics of\ndifferent nature, highlighting the joint impact of network architecture and\ncomponent diversity on overall system complexity.", "AI": {"tldr": "提出了一种基于线性系统理论的网络系统复杂性度量方法，重点关注一阶标量线性系统的有向图互联网络。", "motivation": "研究网络系统的复杂性，通过局部过滤节点动态的影响，提出一种新的复杂性指标。", "method": "利用McMillan度的概念，提出复杂性指标，并分析其对网络拓扑和节点动态异质性的依赖性。", "result": "发现复杂性指标几乎对所有互联权重选择相同，且依赖于子图的匹配数和节点动态的异质性。", "conclusion": "网络架构和组件多样性共同影响系统复杂性。"}}
{"id": "2507.06404", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06404", "abs": "https://arxiv.org/abs/2507.06404", "authors": ["Matteo Tiezzi", "Tommaso Apicella", "Carlos Cardenas-Perez", "Giovanni Fregonese", "Stefano Dafarra", "Pietro Morerio", "Daniele Pucci", "Alessio Del Bue"], "title": "Learning to Evaluate Autonomous Behaviour in Human-Robot Interaction", "comment": null, "summary": "Evaluating and comparing the performance of autonomous Humanoid Robots is\nchallenging, as success rate metrics are difficult to reproduce and fail to\ncapture the complexity of robot movement trajectories, critical in Human-Robot\nInteraction and Collaboration (HRIC). To address these challenges, we propose a\ngeneral evaluation framework that measures the quality of Imitation Learning\n(IL) methods by focusing on trajectory performance. We devise the Neural Meta\nEvaluator (NeME), a deep learning model trained to classify actions from robot\njoint trajectories. NeME serves as a meta-evaluator to compare the performance\nof robot control policies, enabling policy evaluation without requiring human\ninvolvement in the loop. We validate our framework on ergoCub, a humanoid\nrobot, using teleoperation data and comparing IL methods tailored to the\navailable platform. The experimental results indicate that our method is more\naligned with the success rate obtained on the robot than baselines, offering a\nreproducible, systematic, and insightful means for comparing the performance of\nmultimodal imitation learning approaches in complex HRI tasks.", "AI": {"tldr": "提出了一种基于轨迹性能的通用评估框架NeME，用于比较模仿学习方法的性能，无需人工参与。", "motivation": "由于成功率指标难以复现且无法捕捉机器人运动轨迹的复杂性，评估人形机器人性能具有挑战性。", "method": "设计了NeME（神经元评估器），通过深度学习模型从机器人关节轨迹中分类动作，作为比较控制策略性能的元评估器。", "result": "实验表明，该方法比基线更符合机器人的实际成功率，提供了可复现且系统化的评估手段。", "conclusion": "NeME为复杂人机交互任务中的模仿学习方法提供了有效的性能比较工具。"}}
{"id": "2507.06363", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06363", "abs": "https://arxiv.org/abs/2507.06363", "authors": ["Szymon Płotka", "Maciej Chrabaszcz", "Gizem Mert", "Ewa Szczurek", "Arkadiusz Sitek"], "title": "Mamba Goes HoME: Hierarchical Soft Mixture-of-Experts for 3D Medical Image Segmentation", "comment": null, "summary": "In recent years, artificial intelligence has significantly advanced medical\nimage segmentation. However, challenges remain, including efficient 3D medical\nimage processing across diverse modalities and handling data variability. In\nthis work, we introduce Hierarchical Soft Mixture-of-Experts (HoME), a\ntwo-level token-routing layer for efficient long-context modeling, specifically\ndesigned for 3D medical image segmentation. Built on the Mamba state-space\nmodel (SSM) backbone, HoME enhances sequential modeling through sparse,\nadaptive expert routing. The first stage employs a Soft Mixture-of-Experts\n(SMoE) layer to partition input sequences into local groups, routing tokens to\nspecialized per-group experts for localized feature extraction. The second\nstage aggregates these outputs via a global SMoE layer, enabling cross-group\ninformation fusion and global context refinement. This hierarchical design,\ncombining local expert routing with global expert refinement improves\ngeneralizability and segmentation performance, surpassing state-of-the-art\nresults across datasets from the three most commonly used 3D medical imaging\nmodalities and data quality.", "AI": {"tldr": "提出了一种名为HoME的分层软专家混合方法，用于高效处理3D医学图像分割，通过局部和全局专家路由提升性能。", "motivation": "解决3D医学图像处理中的模态多样性和数据变异性问题，提升分割效率和准确性。", "method": "基于Mamba状态空间模型，采用两阶段的分层软专家混合（HoME）设计：局部专家路由提取特征，全局专家融合优化上下文。", "result": "在三种常用3D医学影像模态和数据质量上，HoME超越了现有最佳方法。", "conclusion": "HoME的分层设计显著提升了3D医学图像分割的泛化能力和性能。"}}
{"id": "2507.06269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06269", "abs": "https://arxiv.org/abs/2507.06269", "authors": ["Rushil Desai", "Frederik Warburg", "Trevor Darrell", "Marissa Ramirez de Chanlatte"], "title": "A Probabilistic Approach to Uncertainty Quantification Leveraging 3D Geometry", "comment": "ICCV 2025 Workshops (8 Pages, 6 Figures, 2 Tables)", "summary": "Quantifying uncertainty in neural implicit 3D representations, particularly\nthose utilizing Signed Distance Functions (SDFs), remains a substantial\nchallenge due to computational inefficiencies, scalability issues, and\ngeometric inconsistencies. Existing methods typically neglect direct geometric\nintegration, leading to poorly calibrated uncertainty maps. We introduce\nBayesSDF, a novel probabilistic framework for uncertainty quantification in\nneural implicit SDF models, motivated by scientific simulation applications\nwith 3D environments (e.g., forests) such as modeling fluid flow through\nforests, where precise surface geometry and awareness of fidelity surface\ngeometric uncertainty are essential. Unlike radiance-based models such as NeRF\nor 3D Gaussian splatting, which lack explicit surface formulations, SDFs define\ncontinuous and differentiable geometry, making them better suited for physical\nmodeling and analysis. BayesSDF leverages a Laplace approximation to quantify\nlocal surface instability via Hessian-based metrics, enabling computationally\nefficient, surface-aware uncertainty estimation. Our method shows that\nuncertainty predictions correspond closely with poorly reconstructed geometry,\nproviding actionable confidence measures for downstream use. Extensive\nevaluations on synthetic and real-world datasets demonstrate that BayesSDF\noutperforms existing methods in both calibration and geometric consistency,\nestablishing a strong foundation for uncertainty-aware 3D scene reconstruction,\nsimulation, and robotic decision-making.", "AI": {"tldr": "BayesSDF是一种新的概率框架，用于量化神经隐式SDF模型中的不确定性，解决了现有方法在几何一致性和计算效率上的不足。", "motivation": "科学模拟应用（如森林环境中的流体流动）需要精确的表面几何和对不确定性的感知，而现有方法缺乏直接几何集成，导致不确定性图校准不佳。", "method": "BayesSDF利用拉普拉斯近似，通过基于Hessian的指标量化局部表面不稳定性，实现高效且表面感知的不确定性估计。", "result": "实验表明，BayesSDF在校准和几何一致性上优于现有方法，为下游应用提供了可靠的不确定性度量。", "conclusion": "BayesSDF为不确定性感知的3D场景重建、模拟和机器人决策奠定了坚实基础。"}}
{"id": "2507.06398", "categories": ["cs.AI", "cs.CY", "68T01, 91B26, 93C15"], "pdf": "https://arxiv.org/pdf/2507.06398", "abs": "https://arxiv.org/abs/2507.06398", "authors": ["David Orban"], "title": "Jolting Technologies: Superexponential Acceleration in AI Capabilities and Implications for AGI", "comment": "13 pages, 2 figures. Revised following peer review", "summary": "This paper investigates the Jolting Technologies Hypothesis, which posits\nsuperexponential growth (increasing acceleration, or a positive third\nderivative) in the development of AI capabilities. We develop a theoretical\nframework and validate detection methodologies through Monte Carlo simulations,\nwhile acknowledging that empirical validation awaits suitable longitudinal\ndata. Our analysis focuses on creating robust tools for future empirical\nstudies and exploring the potential implications should the hypothesis prove\nvalid. The study examines how factors such as shrinking idea-to-action\nintervals and compounding iterative AI improvements drive this jolting pattern.\nBy formalizing jolt dynamics and validating detection methods through\nsimulation, this work provides the mathematical foundation necessary for\nunderstanding potential AI trajectories and their consequences for AGI\nemergence, offering insights for research and policy.", "AI": {"tldr": "论文研究了Jolting Technologies假说，提出AI能力发展可能呈现超指数增长（即加速增长或正三阶导数）。通过蒙特卡洛模拟验证检测方法，但实证验证仍需纵向数据。", "motivation": "探讨AI能力发展的超指数增长模式及其潜在影响，为未来实证研究提供工具。", "method": "开发理论框架，通过蒙特卡洛模拟验证检测方法，分析缩短的创意到行动间隔和迭代AI改进的复合效应。", "result": "通过模拟验证了检测方法的有效性，为理解AI发展轨迹及其对通用人工智能（AGI）的影响提供了数学基础。", "conclusion": "研究为AI发展轨迹的潜在影响提供了理论支持，对研究和政策制定具有启示意义。"}}
{"id": "2507.06415", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06415", "abs": "https://arxiv.org/abs/2507.06415", "authors": ["Zeming Chen", "Angelika Romanou", "Gail Weiss", "Antoine Bosselut"], "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning", "comment": "10 pages, 7 figures", "summary": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.", "AI": {"tldr": "PERK提出了一种参数高效的方法，通过梯度更新轻量级适配器在测试时编码长上下文，显著提升了长上下文推理任务的性能。", "motivation": "解决现有元学习方法在长上下文推理中内存消耗过大的问题。", "method": "采用双嵌套优化循环，内循环快速编码上下文到低秩适配器，外循环学习利用适配器进行推理。", "result": "在多个长上下文任务中显著优于基线，性能提升高达90%（小模型）和27%（大模型）。", "conclusion": "PERK在训练时内存消耗大，但在推理时更高效，且对推理复杂性、长度外推和信息位置更鲁棒。"}}
{"id": "2507.06392", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06392", "abs": "https://arxiv.org/abs/2507.06392", "authors": ["Federico Chiariotti", "Marco Fabris"], "title": "VoI-aware Scheduling Schemes for Multi-Agent Formation Control", "comment": "6 pages, 4 figures, 2 tables, accepted at the 1st Joint Conference on\n  Computers, Cognition and Communication, Padua, Italy, Sep. 15-18, 2025", "summary": "Formation control allows agents to maintain geometric patterns using local\ninformation, but most existing methods assume ideal communication. This paper\nintroduces a goal-oriented framework combining control, cooperative\npositioning, and communication scheduling for first-order formation tracking.\nEach agent estimates its position using 6G network-based triangulation, and the\nscheduling of information updates is governed by Age of Information (AoI) and\nValue of Information (VoI) metrics. We design three lightweight, signaling-free\nscheduling policies and assess their impact on formation quality. Simulation\nresults demonstrate the effectiveness of the proposed approach in maintaining\naccurate formations with no additional communication overhead, showing that\nworst-case formation adherence increases by 20%.", "AI": {"tldr": "论文提出了一种基于6G网络的目标导向框架，结合控制、协作定位和通信调度，用于一阶编队跟踪，并通过轻量级调度策略提升编队质量。", "motivation": "现有编队控制方法通常假设理想通信条件，而实际通信受限会影响编队质量。本文旨在解决这一问题。", "method": "结合6G网络三角定位、AoI和VoI指标设计通信调度策略，提出三种轻量级无信号调度策略。", "result": "仿真结果表明，该方法能有效维持编队精度，且无额外通信开销，最坏情况下编队一致性提升20%。", "conclusion": "该框架在通信受限条件下显著提升了编队控制性能，具有实际应用潜力。"}}
{"id": "2507.06426", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06426", "abs": "https://arxiv.org/abs/2507.06426", "authors": ["Devin Crowley", "Whitney G. Cole", "Christina M. Hospodar", "Ruiting Shen", "Karen E. Adolph", "Alan Fern"], "title": "Evaluating Robots Like Human Infants: A Case Study of Learned Bipedal Locomotion", "comment": "7 pages, 4 figures, accepted into ICDL 2025 as a contributed paper", "summary": "Typically, learned robot controllers are trained via relatively unsystematic\nregimens and evaluated with coarse-grained outcome measures such as average\ncumulative reward. The typical approach is useful to compare learning\nalgorithms but provides limited insight into the effects of different training\nregimens and little understanding about the richness and complexity of learned\nbehaviors. Likewise, human infants and other animals are \"trained\" via\nunsystematic regimens, but in contrast, developmental psychologists evaluate\ntheir performance in highly-controlled experiments with fine-grained measures\nsuch as success, speed of walking, and prospective adjustments. However, the\nstudy of learned behavior in human infants is limited by the practical\nconstraints of training and testing babies. Here, we present a case study that\napplies methods from developmental psychology to study the learned behavior of\nthe simulated bipedal robot Cassie. Following research on infant walking, we\nsystematically designed reinforcement learning training regimens and tested the\nresulting controllers in simulated environments analogous to those used for\nbabies--but without the practical constraints. Results reveal new insights into\nthe behavioral impact of different training regimens and the development of\nCassie's learned behaviors relative to infants who are learning to walk. This\ninterdisciplinary baby-robot approach provides inspiration for future research\ndesigned to systematically test effects of training on the development of\ncomplex learned robot behaviors.", "AI": {"tldr": "论文提出了一种将发展心理学方法应用于机器人行为学习的研究，通过系统设计训练方案，揭示了训练对机器人行为发展的影响。", "motivation": "传统机器人控制器的训练和评估方法缺乏系统性和精细度，无法深入理解训练方案对行为的影响。发展心理学的方法可以提供更细致的评估，但受限于实际约束。", "method": "采用发展心理学的方法，系统设计强化学习训练方案，并在模拟环境中测试双足机器人Cassie的行为。", "result": "研究揭示了不同训练方案对机器人行为的具体影响，并与婴儿学步行为进行了比较。", "conclusion": "跨学科的婴儿-机器人研究方法为未来系统测试训练对复杂机器人行为发展的影响提供了新思路。"}}
{"id": "2507.06384", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06384", "abs": "https://arxiv.org/abs/2507.06384", "authors": ["Emerson P. Grabke", "Babak Taati", "Masoom A. Haider"], "title": "Mitigating Multi-Sequence 3D Prostate MRI Data Scarcity through Domain Adaptation using Locally-Trained Latent Diffusion Models for Prostate Cancer Detection", "comment": "BT and MAH are co-senior authors on the work. This work has been\n  submitted to the IEEE for possible publication", "summary": "Objective: Latent diffusion models (LDMs) could mitigate data scarcity\nchallenges affecting machine learning development for medical image\ninterpretation. The recent CCELLA LDM improved prostate cancer detection\nperformance using synthetic MRI for classifier training but was limited to the\naxial T2-weighted (AxT2) sequence, did not investigate inter-institutional\ndomain shift, and prioritized radiology over histopathology outcomes. We\npropose CCELLA++ to address these limitations and improve clinical utility.\nMethods: CCELLA++ expands CCELLA for simultaneous biparametric prostate MRI\n(bpMRI) generation, including the AxT2, high b-value diffusion series (HighB)\nand apparent diffusion coefficient map (ADC). Domain adaptation was\ninvestigated by pretraining classifiers on real or LDM-generated synthetic data\nfrom an internal institution, followed with fine-tuning on progressively\nsmaller fractions of an out-of-distribution, external dataset. Results:\nCCELLA++ improved 3D FID for HighB and ADC but not AxT2 (0.013, 0.012, 0.063\nrespectively) sequences compared to CCELLA (0.060). Classifier pretraining with\nCCELLA++ bpMRI outperformed real bpMRI in AP and AUC for all domain adaptation\nscenarios. CCELLA++ pretraining achieved highest classifier performance below\n50% (n=665) external dataset volume. Conclusion: Synthetic bpMRI generated by\nour method can improve downstream classifier generalization and performance\nbeyond real bpMRI or CCELLA-generated AxT2-only images. Future work should seek\nto quantify medical image sample quality, balance multi-sequence LDM training,\nand condition the LDM with additional information. Significance: The proposed\nCCELLA++ LDM can generate synthetic bpMRI that outperforms real data for domain\nadaptation with a limited target institution dataset. Our code is available at\nhttps://github.com/grabkeem/CCELLA-plus-plus", "AI": {"tldr": "CCELLA++扩展了CCELLA，生成多序列bpMRI，并通过合成数据提升分类器在域适应中的性能。", "motivation": "解决CCELLA仅支持AxT2序列、未研究域转移及未关注病理结果的局限性，提升临床实用性。", "method": "扩展CCELLA以生成bpMRI（包括AxT2、HighB和ADC），研究域适应策略，通过合成数据预训练分类器。", "result": "CCELLA++在HighB和ADC序列上FID提升，分类器性能优于真实数据，尤其在外部数据量少时表现更佳。", "conclusion": "合成bpMRI能提升分类器性能，未来需优化图像质量、多序列训练及信息条件化。"}}
{"id": "2507.06272", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06272", "abs": "https://arxiv.org/abs/2507.06272", "authors": ["Zhang Li", "Biao Yang", "Qiang Liu", "Shuo Zhang", "Zhiyin Ma", "Shuo Zhang", "Liang Yin", "Linger Deng", "Yabo Sun", "Yuliang Liu", "Xiang Bai"], "title": "LIRA: Inferring Segmentation in Large Multi-modal Models with Local Interleaved Region Assistance", "comment": null, "summary": "While large multi-modal models (LMMs) demonstrate promising capabilities in\nsegmentation and comprehension, they still struggle with two limitations:\ninaccurate segmentation and hallucinated comprehension. These challenges stem\nprimarily from constraints in weak visual comprehension and a lack of\nfine-grained perception. To alleviate these limitations, we propose LIRA, a\nframework that capitalizes on the complementary relationship between visual\ncomprehension and segmentation via two key components: (1) Semantic-Enhanced\nFeature Extractor (SEFE) improves object attribute inference by fusing semantic\nand pixel-level features, leading to more accurate segmentation; (2)\nInterleaved Local Visual Coupling (ILVC) autoregressively generates local\ndescriptions after extracting local features based on segmentation masks,\noffering fine-grained supervision to mitigate hallucinations. Furthermore, we\nfind that the precision of object segmentation is positively correlated with\nthe latent related semantics of the <seg> token. To quantify this relationship\nand the model's potential semantic inferring ability, we introduce the\nAttributes Evaluation (AttrEval) dataset. Our experiments show that LIRA\nachieves state-of-the-art performance in both segmentation and comprehension\ntasks. Code will be available at https://github.com/echo840/LIRA.", "AI": {"tldr": "LIRA框架通过结合语义增强特征提取器和交错局部视觉耦合，解决了大型多模态模型在分割和视觉理解中的不准确和幻觉问题，并在实验中表现出色。", "motivation": "大型多模态模型在分割和理解任务中存在不准确分割和幻觉理解的局限性，主要源于视觉理解能力不足和细粒度感知缺失。", "method": "提出LIRA框架，包含语义增强特征提取器（SEFE）和交错局部视觉耦合（ILVC），分别提升分割准确性和减少幻觉理解。", "result": "LIRA在分割和理解任务中达到最先进性能，并发现分割精度与语义相关性正相关。", "conclusion": "LIRA通过结合语义和局部特征，有效提升了多模态模型的分割和理解能力。"}}
{"id": "2507.06798", "categories": ["cs.AI", "math.LO"], "pdf": "https://arxiv.org/pdf/2507.06798", "abs": "https://arxiv.org/abs/2507.06798", "authors": ["Uri Andrews", "Luca San Mauro"], "title": "Comparing Dialectical Systems: Contradiction and Counterexample in Belief Change (Extended Version)", "comment": "25 pages, accepted at JELIA 2025", "summary": "Dialectical systems are a mathematical formalism for modeling an agent\nupdating a knowledge base seeking consistency. Introduced in the 1970s by\nRoberto Magari, they were originally conceived to capture how a working\nmathematician or a research community refines beliefs in the pursuit of truth.\nDialectical systems also serve as natural models for the belief change of an\nautomated agent, offering a unifying, computable framework for dynamic belief\nmanagement.\n  The literature distinguishes three main models of dialectical systems:\n(d-)dialectical systems based on revising beliefs when they are seen to be\ninconsistent, p-dialectical systems based on revising beliefs based on finding\na counterexample, and q-dialectical systems which can do both. We answer an\nopen problem in the literature by proving that q-dialectical systems are\nstrictly more powerful than p-dialectical systems, which are themselves known\nto be strictly stronger than (d-)dialectical systems. This result highlights\nthe complementary roles of counterexample and contradiction in automated belief\nrevision, and thus also in the reasoning processes of mathematicians and\nresearch communities.", "AI": {"tldr": "论文证明了q-辩证系统比p-辩证系统更强大，而后者又比d-辩证系统更强，强调了反例和矛盾在自动信念修正中的互补作用。", "motivation": "研究辩证系统的目的是为了建模代理如何通过更新知识库来寻求一致性，并统一动态信念管理的可计算框架。", "method": "区分了三种辩证系统模型：d-、p-和q-辩证系统，并通过理论证明比较它们的表达能力。", "result": "证明了q-辩证系统严格强于p-辩证系统，而p-辩证系统又严格强于d-辩证系统。", "conclusion": "研究结果突出了反例和矛盾在自动信念修正和数学推理中的互补作用。"}}
{"id": "2507.06419", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06419", "abs": "https://arxiv.org/abs/2507.06419", "authors": ["Pankayaraj Pathmanathan", "Furong Huang"], "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling", "comment": null, "summary": "Reward modeling (RM), which captures human preferences to align large\nlanguage models (LLMs), is increasingly employed in tasks such as model\nfinetuning, response filtering, and ranking. However, due to the inherent\ncomplexity of human preferences and the limited coverage of available datasets,\nreward models often fail under distributional shifts or adversarial\nperturbations. Existing approaches for identifying such failure modes typically\nrely on prior knowledge about preference distributions or failure attributes,\nlimiting their practicality in real-world settings where such information is\nunavailable. In this work, we propose a tractable, preference-distribution\nagnostic method for discovering reward model failure modes via reward guided\ncontrolled decoding. Building on this, we introduce REFORM, a self-improving\nreward modeling framework that enhances robustness by using the reward model\nitself to guide the generation of falsely scored responses. These adversarial\nexamples are then used to augment the training data and patch the reward\nmodel's misaligned behavior. We evaluate REFORM on two widely used preference\ndatasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate\nthat it significantly improves robustness without sacrificing reward quality.\nNotably, REFORM preserves performance both in direct evaluation and in\ndownstream policy training, and further improves alignment quality by removing\nspurious correlations.", "AI": {"tldr": "提出了一种名为REFORM的自我改进奖励建模框架，通过奖励引导的解码方法发现奖励模型的失败模式，并利用生成的对抗样本增强训练数据，提升模型的鲁棒性。", "motivation": "由于人类偏好的复杂性和数据集的有限覆盖，奖励模型在分布偏移或对抗扰动下容易失效。现有方法依赖先验知识，限制了实际应用。", "method": "提出了一种不依赖偏好分布的先验知识的方法，通过奖励引导的解码发现失败模式，并利用REFORM框架生成对抗样本以改进训练数据。", "result": "在Anthropic HH和PKU Beavertails数据集上，REFORM显著提升了鲁棒性，同时保持了奖励质量，并在下游策略训练中表现良好。", "conclusion": "REFORM通过自我改进机制有效提升了奖励模型的鲁棒性和对齐质量，消除了虚假相关性。"}}
{"id": "2507.06399", "categories": ["eess.SY", "cs.AI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06399", "abs": "https://arxiv.org/abs/2507.06399", "authors": ["Doyeong Lim", "Yang Liu", "Zavier Ndum Ndum", "Christian Young", "Yassin Hassan"], "title": "An AI-Driven Thermal-Fluid Testbed for Advanced Small Modular Reactors: Integration of Digital Twin and Large Language Models", "comment": null, "summary": "This paper presents a multipurpose artificial intelligence (AI)-driven\nthermal-fluid testbed designed to advance Small Modular Reactor technologies by\nseamlessly integrating physical experimentation with advanced computational\nintelligence. The platform uniquely combines a versatile three-loop\nthermal-fluid facility with a high-fidelity digital twin and sophisticated AI\nframeworks for real-time prediction, control, and operational assistance.\nMethodologically, the testbed's digital twin, built upon the System Analysis\nModule code, is coupled with a Gated Recurrent Unit (GRU) neural network. This\nmachine learning model, trained on experimental data, enables\nfaster-than-real-time simulation, providing predictive insights into the\nsystem's dynamic behavior. The practical application of this AI integration is\nshowcased through case studies. An AI-driven control framework where the GRU\nmodel accurately forecasts future system states and the corresponding control\nactions required to meet operational demands. Furthermore, an intelligent\nassistant, powered by a large language model, translates complex sensor data\nand simulation outputs into natural language, offering operators actionable\nanalysis and safety recommendations. Comprehensive validation against\nexperimental transients confirms the platform's high fidelity, with the GRU\nmodel achieving a temperature prediction root mean square error of 1.42 K. This\nwork establishes an integrated research environment at the intersection of AI\nand thermal-fluid science, showcasing how AI-driven methodologies in modeling,\ncontrol, and operator support can accelerate the innovation and deployment of\nnext-generation nuclear systems.", "AI": {"tldr": "本文介绍了一种多功能AI驱动的热流体测试平台，用于推进小型模块化反应堆技术，结合物理实验与计算智能。", "motivation": "通过集成AI和数字孪生技术，提升热流体系统的实时预测、控制和操作支持能力，加速下一代核系统的创新与部署。", "method": "测试平台采用基于系统分析模块的数字孪生，结合门控循环单元（GRU）神经网络，利用实验数据训练模型，实现超实时模拟。", "result": "GRU模型的温度预测均方根误差为1.42 K，验证了平台的高保真度，并通过案例展示了AI驱动的控制框架和智能助手。", "conclusion": "该研究为AI与热流体科学的交叉领域提供了集成环境，展示了AI在建模、控制和操作支持中的潜力。"}}
{"id": "2507.06519", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06519", "abs": "https://arxiv.org/abs/2507.06519", "authors": ["Yuhan Liu", "Xinyu Zhang", "Haonan Chang", "Abdeslam Boularias"], "title": "Failure Forecasting Boosts Robustness of Sim2Real Rhythmic Insertion Policies", "comment": "Accepted at IROS2025. Project website:\n  https://jaysparrow.github.io/rit", "summary": "This paper addresses the challenges of Rhythmic Insertion Tasks (RIT), where\na robot must repeatedly perform high-precision insertions, such as screwing a\nnut into a bolt with a wrench. The inherent difficulty of RIT lies in achieving\nmillimeter-level accuracy and maintaining consistent performance over multiple\nrepetitions, particularly when factors like nut rotation and friction introduce\nadditional complexity. We propose a sim-to-real framework that integrates a\nreinforcement learning-based insertion policy with a failure forecasting\nmodule. By representing the wrench's pose in the nut's coordinate frame rather\nthan the robot's frame, our approach significantly enhances sim-to-real\ntransferability. The insertion policy, trained in simulation, leverages\nreal-time 6D pose tracking to execute precise alignment, insertion, and\nrotation maneuvers. Simultaneously, a neural network predicts potential\nexecution failures, triggering a simple recovery mechanism that lifts the\nwrench and retries the insertion. Extensive experiments in both simulated and\nreal-world environments demonstrate that our method not only achieves a high\none-time success rate but also robustly maintains performance over long-horizon\nrepetitive tasks.", "AI": {"tldr": "论文提出了一种解决Rhythmic Insertion Tasks（RIT）挑战的sim-to-real框架，结合强化学习插入策略和失败预测模块，显著提升了毫米级精度和长期重复任务的稳定性。", "motivation": "RIT任务（如用扳手拧螺母）需要毫米级精度和长期稳定性，但螺母旋转和摩擦等因素增加了复杂性。", "method": "提出sim-to-real框架，包括基于强化学习的插入策略和失败预测模块；通过将扳手姿态表示为螺母坐标系而非机器人坐标系，提升仿真到现实的迁移能力。", "result": "在仿真和现实环境中，该方法不仅单次成功率高，还能在长期重复任务中保持稳定性能。", "conclusion": "该方法有效解决了RIT任务的挑战，实现了高精度和长期稳定性。"}}
{"id": "2507.06410", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06410", "abs": "https://arxiv.org/abs/2507.06410", "authors": ["Peyman Sharifian", "Xiaotong Hong", "Alireza Karimian", "Mehdi Amini", "Hossein Arabi"], "title": "Attention-Enhanced Deep Learning Ensemble for Breast Density Classification in Mammography", "comment": "2025 IEEE Nuclear Science Symposium, Medical Imaging Conference and\n  Room Temperature Semiconductor Detector Conference", "summary": "Breast density assessment is a crucial component of mammographic\ninterpretation, with high breast density (BI-RADS categories C and D)\nrepresenting both a significant risk factor for developing breast cancer and a\ntechnical challenge for tumor detection. This study proposes an automated deep\nlearning system for robust binary classification of breast density (low: A/B\nvs. high: C/D) using the VinDr-Mammo dataset. We implemented and compared four\nadvanced convolutional neural networks: ResNet18, ResNet50, EfficientNet-B0,\nand DenseNet121, each enhanced with channel attention mechanisms. To address\nthe inherent class imbalance, we developed a novel Combined Focal Label\nSmoothing Loss function that integrates focal loss, label smoothing, and\nclass-balanced weighting. Our preprocessing pipeline incorporated advanced\ntechniques, including contrast-limited adaptive histogram equalization (CLAHE)\nand comprehensive data augmentation. The individual models were combined\nthrough an optimized ensemble voting approach, achieving superior performance\n(AUC: 0.963, F1-score: 0.952) compared to any single model. This system\ndemonstrates significant potential to standardize density assessments in\nclinical practice, potentially improving screening efficiency and early cancer\ndetection rates while reducing inter-observer variability among radiologists.", "AI": {"tldr": "本文提出了一种基于深度学习的自动化系统，用于乳腺密度的二元分类（低密度A/B vs 高密度C/D），通过集成多个卷积神经网络和新型损失函数，显著提升了分类性能。", "motivation": "乳腺密度评估对乳腺癌筛查至关重要，但高密度乳腺既增加了患癌风险，又增加了肿瘤检测的技术难度。自动化系统有助于标准化评估并减少人为差异。", "method": "使用四种卷积神经网络（ResNet18、ResNet50、EfficientNet-B0、DenseNet121）结合通道注意力机制，并提出了一种结合焦点损失、标签平滑和类平衡权重的新型损失函数。预处理包括CLAHE和数据增强，最终通过集成投票方法优化性能。", "result": "系统在AUC（0.963）和F1分数（0.952）上表现优异，优于单一模型。", "conclusion": "该系统有望在临床实践中标准化乳腺密度评估，提高筛查效率和早期癌症检出率，同时减少放射科医生之间的差异。"}}
{"id": "2507.06275", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06275", "abs": "https://arxiv.org/abs/2507.06275", "authors": ["Yassin Hussein Rassul", "Aram M. Ahmed", "Polla Fattah", "Bryar A. Hassan", "Arwaa W. Abdulkareem", "Tarik A. Rashid", "Joan Lu"], "title": "Advancing Offline Handwritten Text Recognition: A Systematic Review of Data Augmentation and Generation Techniques", "comment": null, "summary": "Offline Handwritten Text Recognition (HTR) systems play a crucial role in\napplications such as historical document digitization, automatic form\nprocessing, and biometric authentication. However, their performance is often\nhindered by the limited availability of annotated training data, particularly\nfor low-resource languages and complex scripts. This paper presents a\ncomprehensive survey of offline handwritten data augmentation and generation\ntechniques designed to improve the accuracy and robustness of HTR systems. We\nsystematically examine traditional augmentation methods alongside recent\nadvances in deep learning, including Generative Adversarial Networks (GANs),\ndiffusion models, and transformer-based approaches. Furthermore, we explore the\nchallenges associated with generating diverse and realistic handwriting\nsamples, particularly in preserving script authenticity and addressing data\nscarcity. This survey follows the PRISMA methodology, ensuring a structured and\nrigorous selection process. Our analysis began with 1,302 primary studies,\nwhich were filtered down to 848 after removing duplicates, drawing from key\nacademic sources such as IEEE Digital Library, Springer Link, Science Direct,\nand ACM Digital Library. By evaluating existing datasets, assessment metrics,\nand state-of-the-art methodologies, this survey identifies key research gaps\nand proposes future directions to advance the field of handwritten text\ngeneration across diverse linguistic and stylistic landscapes.", "AI": {"tldr": "本文综述了离线手写文本识别（HTR）中的数据增强与生成技术，分析了传统方法与深度学习方法（如GANs、扩散模型和基于Transformer的方法）的优缺点，并探讨了生成多样且真实手写样本的挑战。", "motivation": "离线HTR系统在历史文档数字化等领域至关重要，但标注数据稀缺限制了其性能，尤其是对低资源语言和复杂脚本。本文旨在通过综述数据增强与生成技术，提升HTR系统的准确性和鲁棒性。", "method": "采用PRISMA方法，系统筛选了1,302篇文献，最终纳入848篇，涵盖IEEE、Springer等主要学术资源，分析传统与深度学习方法。", "result": "综述揭示了现有技术的局限性，如脚本真实性和数据稀缺问题，并提出了未来研究方向。", "conclusion": "本文为手写文本生成领域的未来研究提供了方向，强调了跨语言和风格多样性的重要性。"}}
{"id": "2507.06852", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06852", "abs": "https://arxiv.org/abs/2507.06852", "authors": ["Uri Andrews", "Luca San Mauro"], "title": "SCC-recursiveness in infinite argumentation (extended version)", "comment": "26 pages, accepted at JELIA 2025", "summary": "Argumentation frameworks (AFs) are a foundational tool in artificial\nintelligence for modeling structured reasoning and conflict. SCC-recursiveness\nis a well-known design principle in which the evaluation of arguments is\ndecomposed according to the strongly connected components (SCCs) of the attack\ngraph, proceeding recursively from \"higher\" to \"lower\" components. While\nSCC-recursive semantics such as \\cft and \\stgt have proven effective for finite\nAFs, Baumann and Spanring showed the failure of SCC-recursive semantics to\ngeneralize reliably to infinite AFs due to issues with well-foundedness.\n  We propose two approaches to extending SCC-recursiveness to the infinite\nsetting. We systematically evaluate these semantics using Baroni and Giacomin's\nestablished criteria, showing in particular that directionality fails in\ngeneral. We then examine these semantics' behavior in finitary frameworks,\nwhere we find some of our semantics satisfy directionality. These results\nadvance the theory of infinite argumentation and lay the groundwork for\nreasoning systems capable of handling unbounded or evolving domains.", "AI": {"tldr": "论文探讨了如何将SCC递归语义扩展到无限论证框架中，提出了两种方法，并评估了其性能。", "motivation": "解决SCC递归语义在无限论证框架中因缺乏良基性而失效的问题。", "method": "提出两种扩展SCC递归语义的方法，并基于Baroni和Giacomin的标准进行系统评估。", "result": "发现方向性在一般情况下失效，但在有限框架中部分语义满足方向性。", "conclusion": "研究为无限论证理论奠定了基础，支持处理无界或动态领域的推理系统。"}}
{"id": "2507.06427", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06427", "abs": "https://arxiv.org/abs/2507.06427", "authors": ["Shun Wang", "Tyler Loakman", "Youbo Lei", "Yi Liu", "Bohao Yang", "Yuting Zhao", "Dong Yang", "Chenghua Lin"], "title": "Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders", "comment": null, "summary": "Large Language Models (LLMs) are traditionally viewed as black-box\nalgorithms, therefore reducing trustworthiness and obscuring potential\napproaches to increasing performance on downstream tasks. In this work, we\napply an effective LLM decomposition method using a dictionary-learning\napproach with sparse autoencoders. This helps extract monosemantic features\nfrom polysemantic LLM neurons. Remarkably, our work identifies model-internal\nmisunderstanding, allowing the automatic reformulation of the prompts with\nadditional annotations to improve the interpretation by LLMs. Moreover, this\napproach demonstrates a significant performance improvement in downstream\ntasks, such as mathematical reasoning and metaphor detection.", "AI": {"tldr": "论文提出了一种基于字典学习和稀疏自编码器的大语言模型分解方法，提取单义特征并改进模型性能。", "motivation": "传统大语言模型被视为黑箱，缺乏透明性，影响了信任度和下游任务性能的提升。", "method": "采用字典学习和稀疏自编码器的方法分解LLM，提取单义特征并自动优化提示。", "result": "方法显著提高了数学推理和隐喻检测等下游任务的性能。", "conclusion": "该方法通过分解LLM提升了模型透明性和任务性能，为改进模型理解提供了新途径。"}}
{"id": "2507.06416", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06416", "abs": "https://arxiv.org/abs/2507.06416", "authors": ["Yize Chen", "Baosen Zhang"], "title": "Voltage Regulation in Distribution Systems with Data Center Loads", "comment": "Code available at\n  https://github.com/chennnnnyize/voltage-regulation-with-data-centers", "summary": "Recent boom in foundation models and AI computing have raised growing\nconcerns on the power and energy trajectories of large-scale data centers. This\npaper focuses on the voltage issues caused by volatile and intensity of data\ncenter power demand, which also aligns with recent observations of more\nfrequent voltage disturbances in power grids. To address these data center\nintegration challenges, we propose a dynamic voltage control scheme by\nharnessing data center's load regulation capabilities. By taking local voltage\nmeasurements and adjusting power injections at each data center buses through\nthe dynamic voltage and frequency scaling (DVFS) scheme, we are able to\nmaintain safe voltage magnitude in a distributed fashion with higher data\ncenter computing load. Simulations using real large language model (LLM)\ninference load validate the effectiveness of our proposed mechanism. Both the\nLLM power data and proposed control scheme are open sourced.", "AI": {"tldr": "本文提出了一种动态电压控制方案，通过利用数据中心的负载调节能力，解决由数据中心电力需求波动引起的电压问题。", "motivation": "随着基础模型和AI计算的快速发展，大规模数据中心的能源需求引发了对其电力与能源轨迹的担忧，尤其是电压问题。", "method": "通过动态电压和频率调整（DVFS）方案，利用本地电压测量调整数据中心总线的功率注入，以分布式方式维持安全电压。", "result": "使用真实大型语言模型（LLM）推理负载的模拟验证了该方案的有效性。", "conclusion": "该动态电压控制方案能有效应对数据中心集成中的电压问题，相关数据和方案已开源。"}}
{"id": "2507.06562", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06562", "abs": "https://arxiv.org/abs/2507.06562", "authors": ["Keita Yoneda", "Kento Kawaharazuka", "Temma Suzuki", "Takahiro Hattori", "Kei Okada"], "title": "KLEIYN : A Quadruped Robot with an Active Waist for Both Locomotion and Wall Climbing", "comment": "Accepted at IROS2025, website -\n  https://keitayoneda.github.io/kleiyn-chimney-climbing/, YouTube -\n  https://www.youtube.com/watch?v=vDmSfkazAvI", "summary": "In recent years, advancements in hardware have enabled quadruped robots to\noperate with high power and speed, while robust locomotion control using\nreinforcement learning (RL) has also been realized. As a result, expectations\nare rising for the automation of tasks such as material transport and\nexploration in unknown environments. However, autonomous locomotion in rough\nterrains with significant height variations requires vertical movement, and\nrobots capable of performing such movements stably, along with their control\nmethods, have not yet been fully established. In this study, we developed the\nquadruped robot KLEIYN, which features a waist joint, and aimed to expand\nquadruped locomotion by enabling chimney climbing through RL. To facilitate the\nlearning of vertical motion, we introduced Contact-Guided Curriculum Learning\n(CGCL). As a result, KLEIYN successfully climbed walls ranging from 800 mm to\n1000 mm in width at an average speed of 150 mm/s, 50 times faster than\nconventional robots. Furthermore, we demonstrated that the introduction of a\nwaist joint improves climbing performance, particularly enhancing tracking\nability on narrow walls.", "AI": {"tldr": "研究开发了具有腰部关节的四足机器人KLEIYN，通过强化学习实现垂直攀爬，并引入接触引导课程学习（CGCL）方法，显著提升了攀爬速度和性能。", "motivation": "四足机器人在复杂地形中的自主运动能力尚未完全解决，尤其是垂直运动。研究旨在通过改进机器人设计和控制方法，实现稳定的垂直攀爬。", "method": "开发了具有腰部关节的四足机器人KLEIYN，采用强化学习（RL）和接触引导课程学习（CGCL）方法训练机器人攀爬。", "result": "KLEIYN成功攀爬宽度800至1000毫米的墙壁，平均速度达150毫米/秒，比传统机器人快50倍。腰部关节显著提升了窄墙攀爬的跟踪能力。", "conclusion": "通过腰部关节设计和CGCL方法，四足机器人的垂直攀爬能力得到显著提升，为复杂地形应用提供了新可能。"}}
{"id": "2507.06417", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06417", "abs": "https://arxiv.org/abs/2507.06417", "authors": ["Laura Pituková", "Peter Sinčák", "László József Kovács"], "title": "Capsule-ConvKAN: A Hybrid Neural Approach to Medical Image Classification", "comment": "Preprint version. Accepted to IEEE SMC 2025", "summary": "This study conducts a comprehensive comparison of four neural network\narchitectures: Convolutional Neural Network, Capsule Network, Convolutional\nKolmogorov--Arnold Network, and the newly proposed Capsule--Convolutional\nKolmogorov--Arnold Network. The proposed Capsule-ConvKAN architecture combines\nthe dynamic routing and spatial hierarchy capabilities of Capsule Network with\nthe flexible and interpretable function approximation of Convolutional\nKolmogorov--Arnold Networks. This novel hybrid model was developed to improve\nfeature representation and classification accuracy, particularly in challenging\nreal-world biomedical image data. The architectures were evaluated on a\nhistopathological image dataset, where Capsule-ConvKAN achieved the highest\nclassification performance with an accuracy of 91.21\\%. The results demonstrate\nthe potential of the newly introduced Capsule-ConvKAN in capturing spatial\npatterns, managing complex features, and addressing the limitations of\ntraditional convolutional models in medical image classification.", "AI": {"tldr": "比较四种神经网络架构，提出新的Capsule-ConvKAN模型，在生物医学图像分类中表现最佳。", "motivation": "改进特征表示和分类准确性，特别是在具有挑战性的生物医学图像数据中。", "method": "结合Capsule Network的动态路由和空间层次能力与ConvKAN的灵活可解释函数逼近，提出Capsule-ConvKAN。", "result": "在组织病理学图像数据集上，Capsule-ConvKAN分类准确率达91.21%，表现最优。", "conclusion": "Capsule-ConvKAN在捕捉空间模式、处理复杂特征及克服传统卷积模型局限性方面具有潜力。"}}
{"id": "2507.06321", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06321", "abs": "https://arxiv.org/abs/2507.06321", "authors": ["Joon Tai Kim", "Tianle Chen", "Ziyu Dong", "Nishanth Kunchala", "Alexander Guller", "Daniel Ospina Acero", "Roger Williams", "Mrinal Kumar"], "title": "Centralized Copy-Paste: Enhanced Data Augmentation Strategy for Wildland Fire Semantic Segmentation", "comment": "21 pages, 5 figures, and under review for AIAA SciTech 2026", "summary": "Collecting and annotating images for the purpose of training segmentation\nmodels is often cost prohibitive. In the domain of wildland fire science, this\nchallenge is further compounded by the scarcity of reliable public datasets\nwith labeled ground truth. This paper presents the Centralized Copy-Paste Data\nAugmentation (CCPDA) method, for the purpose of assisting with the training of\ndeep-learning multiclass segmentation models, with special focus on improving\nsegmentation outcomes for the fire-class. CCPDA has three main steps: (i)\nidentify fire clusters in the source image, (ii) apply a centralization\ntechnique to focus on the core of the fire area, and (iii) paste the refined\nfire clusters onto a target image. This method increases dataset diversity\nwhile preserving the essential characteristics of the fire class. The\neffectiveness of this augmentation technique is demonstrated via numerical\nanalysis and comparison against various other augmentation methods using a\nweighted sum-based multi-objective optimization approach. This approach helps\nelevate segmentation performance metrics specific to the fire class, which\ncarries significantly more operational significance than other classes (fuel,\nash, or background). Numerical performance assessment validates the efficacy of\nthe presented CCPDA method in alleviating the difficulties associated with\nsmall, manually labeled training datasets. It also illustrates that CCPDA\noutperforms other augmentation strategies in the application scenario\nconsidered, particularly in improving fire-class segmentation performance.", "AI": {"tldr": "论文提出了一种名为CCPDA的数据增强方法，用于改善野火科学中多类分割模型的训练效果，特别是针对火灾类别的分割性能。", "motivation": "由于收集和标注图像用于训练分割模型成本高昂，且野火科学领域缺乏可靠的公开数据集，因此需要一种有效的数据增强方法来缓解这一问题。", "method": "CCPDA方法包括三个步骤：(i) 识别源图像中的火灾簇，(ii) 应用中心化技术聚焦火灾核心区域，(iii) 将精炼后的火灾簇粘贴到目标图像上。", "result": "通过数值分析和多目标优化比较，CCPDA显著提升了火灾类别的分割性能，优于其他增强方法。", "conclusion": "CCPDA有效解决了小规模手动标注数据集的训练难题，并在火灾类别分割中表现出色。"}}
{"id": "2507.06968", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06968", "abs": "https://arxiv.org/abs/2507.06968", "authors": ["Li Du", "Hanyu Zhao", "Yiming Ju", "Tengfei Pan"], "title": "Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report", "comment": null, "summary": "Instruction tuning has become a foundation for unlocking the capabilities of\nlarge-scale pretrained models and improving their performance on complex tasks.\nThus, the construction of high-quality instruction datasets is crucial for\nenhancing model performance and generalizability. Although current instruction\ndatasets have reached tens of millions of samples, models finetuned on them may\nstill struggle with complex instruction following and tasks in rare domains.\nThis is primarily due to limited expansion in both ``coverage'' (coverage of\ntask types and knowledge areas) and ``depth'' (instruction complexity) of the\ninstruction set. To address this issue, we propose a systematic instruction\ndata construction framework, which integrates a hierarchical labeling system,\nan informative seed selection algorithm, an evolutionary data synthesis\nprocess, and a model deficiency diagnosis with targeted data generation. These\ncomponents form an iterative closed-loop to continuously enhance the coverage\nand depth of instruction data. Based on this framework, we construct\nInfinityInstruct-Subject, a high-quality dataset containing ~1.5 million\ninstructions. Experiments on multiple foundation models and benchmark tasks\ndemonstrate its effectiveness in improving instruction-following capabilities.\nFurther analyses suggest that InfinityInstruct-Subject shows enlarged coverage\nand depth compared to comparable synthesized instruction datasets. Our work\nlays a theoretical and practical foundation for the efficient, continuous\nevolution of instruction datasets, moving from data quantity expansion to\nqualitative improvement.", "AI": {"tldr": "提出了一种系统化的指令数据构建框架，通过分层标注、种子选择、数据合成和模型诊断生成高质量指令数据集，提升模型性能。", "motivation": "当前指令数据集在覆盖范围和深度上有限，导致模型在复杂任务和罕见领域表现不佳，需要系统性改进。", "method": "采用分层标注系统、种子选择算法、进化数据合成和模型诊断生成，形成闭环迭代优化指令数据。", "result": "构建了InfinityInstruct-Subject数据集（150万指令），实验证明其能显著提升模型指令遵循能力。", "conclusion": "该框架为指令数据集从数量扩张到质量提升提供了理论和实践基础。"}}
{"id": "2507.06435", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06435", "abs": "https://arxiv.org/abs/2507.06435", "authors": ["Rafiu Adekoya Badekale", "Adewale Akinfaderin"], "title": "Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling", "comment": "10 pages, 7 figures. Code and data available at\n  https://github.com/AdeTheBade/TACPD.git", "summary": "Understanding how policy language evolves over time is critical for assessing\nglobal responses to complex challenges such as climate change. Temporal\nanalysis helps stakeholders, including policymakers and researchers, to\nevaluate past priorities, identify emerging themes, design governance\nstrategies, and develop mitigation measures. Traditional approaches, such as\nmanual thematic coding, are time-consuming and limited in capturing the\ncomplex, interconnected nature of global policy discourse. With the increasing\nrelevance of unsupervised machine learning, these limitations can be addressed,\nparticularly under high-volume, complex, and high-dimensional data conditions.\nIn this work, we explore a novel approach that applies the dynamic embedded\ntopic model (DETM) to analyze the evolution of global climate policy discourse.\nA probabilistic model designed to capture the temporal dynamics of topics over\ntime. We collected a corpus of United Nations Framework Convention on Climate\nChange (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the\npostponement of COP26 as a result of the COVID-19 pandemic. The model reveals\nshifts from early emphases on greenhouse gases and international conventions to\nrecent focuses on implementation, technical collaboration, capacity building,\nfinance, and global agreements. Section 3 presents the modeling pipeline,\nincluding preprocessing, model training, and visualization of temporal word\ndistributions. Our results show that DETM is a scalable and effective tool for\nanalyzing the evolution of global policy discourse. Section 4 discusses the\nimplications of these findings and we concluded with future directions and\nrefinements to extend this approach to other policy domains.", "AI": {"tldr": "论文提出了一种动态嵌入主题模型（DETM）来分析全球气候政策话语的演变，揭示了从早期关注温室气体到近期聚焦实施与合作的转变。", "motivation": "评估全球对复杂挑战（如气候变化）的响应需要理解政策语言的演变，传统方法效率低且难以捕捉复杂性。", "method": "应用动态嵌入主题模型（DETM）分析1995-2023年UNFCCC政策决策的语料库，排除2020年数据。", "result": "模型显示政策主题从温室气体转向实施、技术合作、资金等，DETM被证明是有效的分析工具。", "conclusion": "DETM可扩展用于其他政策领域，未来将进一步优化和扩展。"}}
{"id": "2507.06436", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06436", "abs": "https://arxiv.org/abs/2507.06436", "authors": ["Xinyu Huang", "Yixiao Zhang", "Yingying Pei", "Jianzhe Xue", "Xuemin Shen"], "title": "Experience-Centric Resource Management in ISAC Networks: A Digital Agent-Assisted Approach", "comment": null, "summary": "In this paper, we propose a digital agent (DA)-assisted resource management\nscheme for enhanced user quality of experience (QoE) in integrated sensing and\ncommunication (ISAC) networks. Particularly, user QoE is a comprehensive metric\nthat integrates quality of service (QoS), user behavioral dynamics, and\nenvironmental complexity. The novel DA module includes a user status prediction\nmodel, a QoS factor selection model, and a QoE fitting model, which analyzes\nhistorical user status data to construct and update user-specific QoE models.\nUsers are clustered into different groups based on their QoE models. A\nCram\\'er-Rao bound (CRB) model is utilized to quantify the impact of allocated\ncommunication resources on sensing accuracy. A joint optimization problem of\ncommunication and computing resource management is formulated to maximize\nlong-term user QoE while satisfying CRB and resource constraints. A two-layer\ndata-model-driven algorithm is developed to solve the formulated problem, where\nthe top layer utilizes an advanced deep reinforcement learning algorithm to\nmake group-level decisions, and the bottom layer uses convex optimization\ntechniques to make user-level decisions. Simulation results based on a\nreal-world dataset demonstrate that the proposed DA-assisted resource\nmanagement scheme outperforms benchmark schemes in terms of user QoE.", "AI": {"tldr": "提出了一种数字代理（DA）辅助的资源管理方案，用于提升集成感知与通信（ISAC）网络中的用户体验质量（QoE）。", "motivation": "用户QoE是一个综合指标，结合了服务质量（QoS）、用户行为动态和环境复杂性，但现有方法难以全面优化。", "method": "DA模块包括用户状态预测、QoS因子选择及QoE拟合模型，通过聚类用户并利用CRB模型量化资源分配对感知精度的影响，采用双层数据-模型驱动算法进行资源优化。", "result": "仿真结果表明，该方案在用户QoE方面优于基准方案。", "conclusion": "DA辅助的资源管理方案能有效提升ISAC网络中的用户QoE。"}}
{"id": "2507.06564", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06564", "abs": "https://arxiv.org/abs/2507.06564", "authors": ["Tianshun Li", "Tianyi Huai", "Zhen Li", "Yichun Gao", "Haoang Li", "Xinhu Zheng"], "title": "SkyVLN: Vision-and-Language Navigation and NMPC Control for UAVs in Urban Environments", "comment": "8 pages, 9 figures, has been accepted by IROS 2025", "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as versatile tools across\nvarious sectors, driven by their mobility and adaptability. This paper\nintroduces SkyVLN, a novel framework integrating vision-and-language navigation\n(VLN) with Nonlinear Model Predictive Control (NMPC) to enhance UAV autonomy in\ncomplex urban environments. Unlike traditional navigation methods, SkyVLN\nleverages Large Language Models (LLMs) to interpret natural language\ninstructions and visual observations, enabling UAVs to navigate through dynamic\n3D spaces with improved accuracy and robustness. We present a multimodal\nnavigation agent equipped with a fine-grained spatial verbalizer and a history\npath memory mechanism. These components allow the UAV to disambiguate spatial\ncontexts, handle ambiguous instructions, and backtrack when necessary. The\nframework also incorporates an NMPC module for dynamic obstacle avoidance,\nensuring precise trajectory tracking and collision prevention. To validate our\napproach, we developed a high-fidelity 3D urban simulation environment using\nAirSim, featuring realistic imagery and dynamic urban elements. Extensive\nexperiments demonstrate that SkyVLN significantly improves navigation success\nrates and efficiency, particularly in new and unseen environments.", "AI": {"tldr": "SkyVLN框架结合视觉语言导航与非线性模型预测控制，提升无人机在复杂城市环境中的自主导航能力。", "motivation": "无人机在复杂城市环境中的导航需求日益增长，传统方法难以应对动态3D空间和自然语言指令的挑战。", "method": "SkyVLN利用大型语言模型解析自然语言指令和视觉观察，结合细粒度空间语言化器和历史路径记忆机制，并通过NMPC模块实现动态避障。", "result": "实验表明，SkyVLN显著提高了导航成功率和效率，尤其是在新环境中。", "conclusion": "SkyVLN为无人机在复杂城市环境中的自主导航提供了高效、鲁棒的解决方案。"}}
{"id": "2507.06575", "categories": ["eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.06575", "abs": "https://arxiv.org/abs/2507.06575", "authors": ["Chia-Hsiang Lin", "Jui-Ting Chen", "Zi-Chao Leng", "Jhao-Ting Lin"], "title": "COS2A: Conversion from Sentinel-2 to AVIRIS Hyperspectral Data Using Interpretable Algorithm With Spectral-Spatial Duality", "comment": "15 pages, 7 figures", "summary": "The Sentinel-2 satellite, launched by the European Space Agency (ESA), offers\nextensive spatial coverage and has become indispensable in a wide range of\nremote sensing applications. However, it just has 12 spectral bands, making\nsubstances/objects identification less effective, not mentioning the varying\nspatial resolutions (10/20/60 m) across the 12 bands. If such a\nmulti-resolution 12-band image can be computationally converted into a\nhyperspectral image with uniformly high resolution (i.e., 10 m), it\nsignificantly facilitates remote identification tasks. Though there are some\nspectral super-resolution methods, they did not address the multi-resolution\nissue on one hand, and, more seriously, they mostly focused on the CAVE-level\nhyperspectral image reconstruction (involving only 31 visible bands) on the\nother hand, greatly limiting their applicability in real-world remote sensing\nscenarios. We ambitiously aim to convert Sentinel-2 data directly into NASA's\nAVIRIS-level hyperspectral image (encompassing up to 172 visible and\nnear-infrared (NIR) bands, after ignoring those absorption/corruption ones).\nFor the first time, this paper solves this specific super-resolution problem\n(highly ill-posed), allowing all historical Sentinel-2 data to have their\ncorresponding high-standard AVIRIS counterparts. We achieve so by customizing a\nnovel algorithm that introduces deep unfolding regularization and\nQ-quadratic-norm regularization into the so-called convex/deep (CODE)\nsmall-data learning criterion. Based on the derived spectral-spatial duality,\nthe proposed interpretable COS2A algorithm demonstrates superior spectral\nsuper-resolution results across diverse land cover types, as validated through\nextensive experiments.", "AI": {"tldr": "该论文提出了一种新算法（COS2A），将Sentinel-2的多分辨率12波段图像转换为高分辨率AVIRIS级高光谱图像（172波段），解决了现有方法在多分辨率问题和波段范围上的局限性。", "motivation": "Sentinel-2卫星的12波段图像在多分辨率和波段数量上限制了物质识别的效果，现有超分辨率方法未能解决多分辨率问题且仅适用于有限波段范围。", "method": "通过结合深度展开正则化和Q-二次范数正则化的凸/深度（CODE）小数据学习准则，提出了一种可解释的COS2A算法。", "result": "实验验证表明，COS2A算法在多种土地覆盖类型上表现出优异的超分辨率效果。", "conclusion": "该研究首次解决了Sentinel-2数据到AVIRIS级高光谱图像的转换问题，为历史数据的利用提供了新途径。"}}
{"id": "2507.06332", "categories": ["cs.CV", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.06332", "abs": "https://arxiv.org/abs/2507.06332", "authors": ["Fuyuan Zhang", "Qichen Wang", "Jianjun Zhao"], "title": "AR2: Attention-Guided Repair for the Robustness of CNNs Against Common Corruptions", "comment": null, "summary": "Deep neural networks suffer from significant performance degradation when\nexposed to common corruptions such as noise, blur, weather, and digital\ndistortions, limiting their reliability in real-world applications. In this\npaper, we propose AR2 (Attention-Guided Repair for Robustness), a simple yet\neffective method to enhance the corruption robustness of pretrained CNNs. AR2\noperates by explicitly aligning the class activation maps (CAMs) between clean\nand corrupted images, encouraging the model to maintain consistent attention\neven under input perturbations. Our approach follows an iterative repair\nstrategy that alternates between CAM-guided refinement and standard\nfine-tuning, without requiring architectural changes. Extensive experiments\nshow that AR2 consistently outperforms existing state-of-the-art methods in\nrestoring robustness on standard corruption benchmarks (CIFAR-10-C, CIFAR-100-C\nand ImageNet-C), achieving a favorable balance between accuracy on clean data\nand corruption robustness. These results demonstrate that AR2 provides a robust\nand scalable solution for enhancing model reliability in real-world\nenvironments with diverse corruptions.", "AI": {"tldr": "AR2通过对齐干净和损坏图像的类激活图（CAMs）来提升预训练CNN的鲁棒性，无需架构更改，在多个基准测试中表现优异。", "motivation": "深度神经网络在常见损坏（如噪声、模糊等）下性能显著下降，限制了其在实际应用中的可靠性。", "method": "AR2采用CAM对齐策略，通过迭代修复（CAM引导的细化和标准微调交替）增强模型鲁棒性。", "result": "AR2在CIFAR-10-C等基准测试中优于现有方法，平衡了干净数据准确性和损坏鲁棒性。", "conclusion": "AR2为提升模型在多样化损坏环境中的可靠性提供了有效且可扩展的解决方案。"}}
{"id": "2507.06993", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06993", "abs": "https://arxiv.org/abs/2507.06993", "authors": ["Jieren Deng", "Aleksandar Cvetkovic", "Pak Kiu Chung", "Dragomir Yankov", "Chiqun Zhang"], "title": "The User-Centric Geo-Experience: An LLM-Powered Framework for Enhanced Planning, Navigation, and Dynamic Adaptation", "comment": null, "summary": "Traditional travel-planning systems are often static and fragmented, leaving\nthem ill-equipped to handle real-world complexities such as evolving\nenvironmental conditions and unexpected itinerary disruptions. In this paper,\nwe identify three gaps between existing service providers causing frustrating\nuser experience: intelligent trip planning, precision \"last-100-meter\"\nnavigation, and dynamic itinerary adaptation. We propose three cooperative\nagents: a Travel Planning Agent that employs grid-based spatial grounding and\nmap analysis to help resolve complex multi-modal user queries; a Destination\nAssistant Agent that provides fine-grained guidance for the final navigation\nleg of each journey; and a Local Discovery Agent that leverages image\nembeddings and Retrieval-Augmented Generation (RAG) to detect and respond to\ntrip plan disruptions. With evaluations and experiments, our system\ndemonstrates substantial improvements in query interpretation, navigation\naccuracy, and disruption resilience, underscoring its promise for applications\nfrom urban exploration to emergency response.", "AI": {"tldr": "论文提出三个协作代理解决传统旅行规划系统的不足，提升查询解释、导航精度和抗干扰能力。", "motivation": "传统旅行规划系统静态且碎片化，无法应对现实世界的复杂性和突发变化。", "method": "提出三个代理：旅行规划代理（空间网格和地图分析）、目的地助手代理（精细导航）、本地发现代理（图像嵌入和RAG技术）。", "result": "系统显著提升了查询解释、导航精度和抗干扰能力。", "conclusion": "该系统在从城市探索到应急响应等领域具有广泛应用前景。"}}
{"id": "2507.06448", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06448", "abs": "https://arxiv.org/abs/2507.06448", "authors": ["Zhenhailong Wang", "Xuehang Guo", "Sofia Stoica", "Haiyang Xu", "Hongru Wang", "Hyeonjeong Ha", "Xiusi Chen", "Yangyi Chen", "Ming Yan", "Fei Huang", "Heng Ji"], "title": "Perception-Aware Policy Optimization for Multimodal Reasoning", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.", "AI": {"tldr": "论文提出了一种名为PAPO的方法，通过改进RLVR框架，增强了多模态推理任务中的视觉感知能力。", "motivation": "当前RLVR在多模态推理任务中表现不佳，主要问题在于视觉输入的感知能力不足。", "method": "提出PAPO方法，通过引入隐式感知损失（KL散度项）优化GRPO目标，无需额外数据或外部模型。", "result": "PAPO在多模态基准测试中提升了4.4%，视觉依赖任务中提升近8.0%，感知错误减少30.5%。", "conclusion": "PAPO为RLVR框架提供了更深的感知监督，为视觉推理奠定了基础。"}}
{"id": "2507.06441", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06441", "abs": "https://arxiv.org/abs/2507.06441", "authors": ["Shanting Wang", "Panagiotis Typaldos", "Chenjun Li", "Andreas A. Malikopoulos"], "title": "VisioPath: Vision-Language Enhanced Model Predictive Control for Safe Autonomous Navigation in Mixed Traffic", "comment": null, "summary": "In this paper, we introduce VisioPath, a novel framework combining\nvision-language models (VLMs) with model predictive control (MPC) to enable\nsafe autonomous driving in dynamic traffic environments. The proposed approach\nleverages a bird's-eye view video processing pipeline and zero-shot VLM\ncapabilities to obtain structured information about surrounding vehicles,\nincluding their positions, dimensions, and velocities. Using this rich\nperception output, we construct elliptical collision-avoidance potential fields\naround other traffic participants, which are seamlessly integrated into a\nfinite-horizon optimal control problem for trajectory planning. The resulting\ntrajectory optimization is solved via differential dynamic programming with an\nadaptive regularization scheme and is embedded in an event-triggered MPC loop.\nTo ensure collision-free motion, a safety verification layer is incorporated in\nthe framework that provides an assessment of potential unsafe trajectories.\nExtensive simulations in Simulation of Urban Mobility (SUMO) demonstrate that\nVisioPath outperforms conventional MPC baselines across multiple metrics. By\ncombining modern AI-driven perception with the rigorous foundation of optimal\ncontrol, VisioPath represents a significant step forward in safe trajectory\nplanning for complex traffic systems.", "AI": {"tldr": "VisioPath结合视觉语言模型和模型预测控制，实现动态交通环境中的安全自动驾驶。", "motivation": "解决动态交通环境中自动驾驶的安全轨迹规划问题。", "method": "利用鸟瞰视频处理和零样本VLM能力获取车辆信息，构建椭圆避碰势场，结合最优控制进行轨迹规划。", "result": "在SUMO模拟中表现优于传统MPC方法。", "conclusion": "VisioPath为复杂交通系统的安全轨迹规划提供了重要进展。"}}
{"id": "2507.06574", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06574", "abs": "https://arxiv.org/abs/2507.06574", "authors": ["Thomas Touma", "Ersin Daş", "Erica Tevere", "Martin Feather", "Ksenia Kolcio", "Maurice Prather", "Alberto Candela", "Ashish Goel", "Erik Kramer", "Hari Nayar", "Lorraine Fesq", "Joel W. Burdick"], "title": "AI Space Cortex: An Experimental System for Future Era Space Exploration", "comment": null, "summary": "Our Robust, Explainable Autonomy for Scientific Icy Moon Operations (REASIMO)\neffort contributes to NASA's Concepts for Ocean worlds Life Detection\nTechnology (COLDTech) program, which explores science platform technologies for\nocean worlds such as Europa and Enceladus. Ocean world missions pose\nsignificant operational challenges. These include long communication lags,\nlimited power, and lifetime limitations caused by radiation damage and hostile\nconditions. Given these operational limitations, onboard autonomy will be vital\nfor future Ocean world missions. Besides the management of nominal lander\noperations, onboard autonomy must react appropriately in the event of\nanomalies. Traditional spacecraft rely on a transition into 'safe-mode' in\nwhich non-essential components and subsystems are powered off to preserve\nsafety and maintain communication with Earth. For a severely time-limited Ocean\nworld mission, resolutions to these anomalies that can be executed without\nEarth-in-the-loop communication and associated delays are paramount for\ncompletion of the mission objectives and science goals. To address these\nchallenges, the REASIMO effort aims to demonstrate a robust level of\nAI-assisted autonomy for such missions, including the ability to detect and\nrecover from anomalies, and to perform missions based on pre-trained behaviors\nrather than hard-coded, predetermined logic like all prior space missions. We\ndeveloped an AI-assisted, personality-driven, intelligent framework for control\nof an Ocean world mission by combining a mix of advanced technologies. To\ndemonstrate the capabilities of the framework, we perform tests of autonomous\nsampling operations on a lander-manipulator testbed at the NASA Jet Propulsion\nLaboratory, approximating possible surface conditions such a mission might\nencounter.", "AI": {"tldr": "REASIMO项目旨在为NASA的COLDTech计划开发AI辅助的自主系统，以应对海洋世界任务中的通信延迟、能源限制和恶劣环境等挑战，实现异常检测与恢复。", "motivation": "海洋世界任务（如欧罗巴和恩塞拉多斯）面临通信延迟、能源限制和辐射等极端条件，传统安全模式无法满足任务需求，需要自主性更强的解决方案。", "method": "开发AI辅助、个性驱动的智能框架，结合先进技术，通过预训练行为而非硬编码逻辑实现自主任务执行和异常恢复。", "result": "在NASA喷气推进实验室的测试平台上，成功演示了自主采样操作，模拟了可能的任务环境。", "conclusion": "REASIMO框架为未来海洋世界任务提供了强大的自主能力，能够在不依赖地球通信的情况下完成任务目标和科学目标。"}}
{"id": "2507.06581", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06581", "abs": "https://arxiv.org/abs/2507.06581", "authors": ["Qibiao Wu", "Yagang Wang", "Qian Zhang"], "title": "Airway Segmentation Network for Enhanced Tubular Feature Extraction", "comment": null, "summary": "Manual annotation of airway regions in computed tomography images is a\ntime-consuming and expertise-dependent task. Automatic airway segmentation is\ntherefore a prerequisite for enabling rapid bronchoscopic navigation and the\nclinical deployment of bronchoscopic robotic systems. Although convolutional\nneural network methods have gained considerable attention in airway\nsegmentation, the unique tree-like structure of airways poses challenges for\nconventional and deformable convolutions, which often fail to focus on fine\nairway structures, leading to missed segments and discontinuities. To address\nthis issue, this study proposes a novel tubular feature extraction network,\nnamed TfeNet. TfeNet introduces a novel direction-aware convolution operation\nthat first applies spatial rotation transformations to adjust the sampling\npositions of linear convolution kernels. The deformed kernels are then\nrepresented as line segments or polylines in 3D space. Furthermore, a tubular\nfeature fusion module (TFFM) is designed based on asymmetric convolution and\nresidual connection strategies, enhancing the network's focus on subtle airway\nstructures. Extensive experiments conducted on one public dataset and two\ndatasets used in airway segmentation challenges demonstrate that the proposed\nTfeNet achieves more accuracy and continuous airway structure predictions\ncompared with existing methods. In particular, TfeNet achieves the highest\noverall score of 94.95% on the current largest airway segmentation dataset,\nAirway Tree Modeling(ATM22), and demonstrates advanced performance on the lung\nfibrosis dataset(AIIB23). The code is available at\nhttps://github.com/QibiaoWu/TfeNet.", "AI": {"tldr": "提出了一种名为TfeNet的新型管状特征提取网络，用于解决传统卷积神经网络在气道分割中难以捕捉细微气道结构的问题。", "motivation": "气道区域的手动标注耗时且依赖专业知识，自动气道分割是快速支气管镜导航和临床部署机器人系统的关键。", "method": "TfeNet引入方向感知卷积操作，通过空间旋转变换调整卷积核采样位置，并结合管状特征融合模块（TFFM）增强对细微气道结构的关注。", "result": "在多个数据集上验证，TfeNet在气道分割任务中表现优于现有方法，尤其在ATM22数据集上达到94.95%的最高分。", "conclusion": "TfeNet通过创新的卷积操作和特征融合策略，显著提升了气道分割的准确性和连续性。"}}
{"id": "2507.06400", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06400", "abs": "https://arxiv.org/abs/2507.06400", "authors": ["Weiran Li", "Yeqiang Liu", "Qiannan Guo", "Yijie Wei", "Hwa Liang Leo", "Zhenbo Li"], "title": "When Trackers Date Fish: A Benchmark and Framework for Underwater Multiple Fish Tracking", "comment": null, "summary": "Multiple object tracking (MOT) technology has made significant progress in\nterrestrial applications, but underwater tracking scenarios remain\nunderexplored despite their importance to marine ecology and aquaculture. We\npresent Multiple Fish Tracking Dataset 2025 (MFT25), the first comprehensive\ndataset specifically designed for underwater multiple fish tracking, featuring\n15 diverse video sequences with 408,578 meticulously annotated bounding boxes\nacross 48,066 frames. Our dataset captures various underwater environments,\nfish species, and challenging conditions including occlusions, similar\nappearances, and erratic motion patterns. Additionally, we introduce\nScale-aware and Unscented Tracker (SU-T), a specialized tracking framework\nfeaturing an Unscented Kalman Filter (UKF) optimized for non-linear fish\nswimming patterns and a novel Fish-Intersection-over-Union (FishIoU) matching\nthat accounts for the unique morphological characteristics of aquatic species.\nExtensive experiments demonstrate that our SU-T baseline achieves\nstate-of-the-art performance on MFT25, with 34.1 HOTA and 44.6 IDF1, while\nrevealing fundamental differences between fish tracking and terrestrial object\ntracking scenarios. MFT25 establishes a robust foundation for advancing\nresearch in underwater tracking systems with important applications in marine\nbiology, aquaculture monitoring, and ecological conservation. The dataset and\ncodes are released at https://vranlee.github.io/SU-T/.", "AI": {"tldr": "论文提出了首个水下多鱼跟踪数据集MFT25，并开发了专门用于非线性和形态特征的跟踪框架SU-T，性能优于现有方法。", "motivation": "水下多目标跟踪在海洋生态和水产养殖中很重要，但相关研究不足，现有技术主要针对陆地场景。", "method": "提出了MFT25数据集，包含多样化的水下视频序列；开发了SU-T框架，结合UKF和FishIoU匹配方法。", "result": "SU-T在MFT25上表现优异，HOTA为34.1，IDF1为44.6，揭示了水下与陆地跟踪的差异。", "conclusion": "MFT25和SU-T为水下跟踪研究提供了基础，对海洋生物学和水产养殖有重要应用价值。"}}
{"id": "2507.07017", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07017", "abs": "https://arxiv.org/abs/2507.07017", "authors": ["Tianyu Zheng", "Tianshun Xing", "Qingshui Gu", "Taoran Liang", "Xingwei Qu", "Xin Zhou", "Yizhi Li", "Zhoufutu Wen", "Chenghua Lin", "Wenhao Huang", "Qian Liu", "Ge Zhang", "Zejun Ma"], "title": "First Return, Entropy-Eliciting Explore", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.", "AI": {"tldr": "FR3E框架通过结构化探索提升LLM推理能力，解决RLVR的不稳定探索问题。", "motivation": "RLVR在提升LLM推理能力时存在探索不稳定的问题，需要更有效的探索方法。", "method": "提出FR3E框架，识别高不确定性决策点并进行针对性探索，构建语义基础的中期反馈。", "result": "在数学推理基准测试中，FR3E提升了训练稳定性、生成长且连贯的响应，并增加完全正确轨迹的比例。", "conclusion": "FR3E通过结构化探索有效提升LLM推理能力。"}}
{"id": "2507.06450", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06450", "abs": "https://arxiv.org/abs/2507.06450", "authors": ["Xin Su", "Sungduk Yu", "Phillip Howard", "Steven Bethard"], "title": "A Semantic Parsing Framework for End-to-End Time Normalization", "comment": null, "summary": "Time normalization is the task of converting natural language temporal\nexpressions into machine-readable representations. It underpins many downstream\napplications in information retrieval, question answering, and clinical\ndecision-making. Traditional systems based on the ISO-TimeML schema limit\nexpressivity and struggle with complex constructs such as compositional,\nevent-relative, and multi-span time expressions. In this work, we introduce a\nnovel formulation of time normalization as a code generation task grounded in\nthe SCATE framework, which defines temporal semantics through symbolic and\ncompositional operators. We implement a fully executable SCATE Python library\nand demonstrate that large language models (LLMs) can generate executable SCATE\ncode. Leveraging this capability, we develop an automatic data augmentation\npipeline using LLMs to synthesize large-scale annotated data with code-level\nvalidation. Our experiments show that small, locally deployable models trained\non this augmented data can achieve strong performance, outperforming even their\nLLM parents and enabling practical, accurate, and interpretable time\nnormalization.", "AI": {"tldr": "论文提出了一种将时间规范化任务转化为代码生成任务的新方法，基于SCATE框架，利用大语言模型生成可执行代码，并通过数据增强提升小模型的性能。", "motivation": "传统基于ISO-TimeML的系统表达能力有限，难以处理复杂时间表达式，如组合式、事件相对和多跨度表达式。", "method": "将时间规范化任务重新定义为基于SCATE框架的代码生成任务，开发了可执行的SCATE Python库，并利用大语言模型生成可执行代码，通过数据增强生成大规模标注数据。", "result": "实验表明，基于增强数据训练的小型模型性能优于其父模型（大语言模型），实现了高效、准确且可解释的时间规范化。", "conclusion": "该方法为时间规范化提供了一种实用且高效的解决方案，克服了传统方法的局限性。"}}
{"id": "2507.06446", "categories": ["eess.SY", "cs.SY", "93C40 (Primary), 93B27 (Secondary)"], "pdf": "https://arxiv.org/pdf/2507.06446", "abs": "https://arxiv.org/abs/2507.06446", "authors": ["Erick Mejia Uzeda", "Mireille E. Broucke"], "title": "On Regular Regressors in Adaptive Control", "comment": "6 pages", "summary": "This paper addresses a shortcoming in adaptive control, that the property of\na regressor being persistently exciting (PE) is not well-behaved. One can\nconstruct regressors that upend the commonsense notion that excitation should\nnot be created out of nothing. To amend the situation, a notion of regularity\nof regressors is needed. We are naturally led to a broad class of regular\nregressors that enjoy the property that their excitation is always confined to\na subspace, a foundational result called the PE decomposition. A geometric\ncharacterization of regressor excitation opens up new avenues for adaptive\ncontrol, as we demonstrate by formulating a number of new adaptive control\nproblems.", "AI": {"tldr": "论文提出了一种解决自适应控制中回归器激励问题的正则性概念，通过PE分解将激励限制在子空间中，并展示了新的自适应控制问题。", "motivation": "解决自适应控制中回归器激励（PE）行为不佳的问题，避免激励凭空产生的不合理现象。", "method": "引入正则回归器的概念，提出PE分解，将激励限制在特定子空间内。", "result": "通过几何表征回归器激励，为自适应控制开辟了新途径。", "conclusion": "正则回归器和PE分解为自适应控制提供了新的理论基础和应用方向。"}}
{"id": "2507.06605", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06605", "abs": "https://arxiv.org/abs/2507.06605", "authors": ["Xinyu Wu"], "title": "Growing Trees with an Agent: Accelerating RRTs with Learned, Multi-Step Episodic Exploration", "comment": null, "summary": "Classical sampling-based motion planners like the RRTs suffer from\ninefficiencies, particularly in cluttered or high-dimensional spaces, due to\ntheir reliance on undirected, random sampling. This paper introduces the\nEpisodic RRT, a novel hybrid planning framework that replaces the primitive of\na random point with a learned, multi-step \"exploratory episode\" generated by a\nDeep Reinforcement Learning agent. By making the DRL agent the engine of\nexploration, ERRT transforms the search process from a diffuse, volumetric\nexpansion into a directed, branch-like growth. This paradigm shift yields key\nadvantages: it counters the curse of dimensionality with focused exploration,\nminimizes expensive collision checks by proactively proposing locally valid\npaths, and improves connectivity by generating inherently connected path\nsegments. We demonstrate through extensive empirical evaluation across 2D, 3D,\nand 6D environments that ERRT and its variants consistently and significantly\noutperform their classical counterparts. In a challenging 6D robotic arm\nscenario, ERRT achieves a 98% success rate compared to 19% for RRT, is up to\n107x faster, reduces collision checks by over 99.6%, and finds initial paths\nthat are nearly 50% shorter. Furthermore, its asymptotically optimal variant,\nERRT*, demonstrates vastly superior anytime performance, refining solutions to\nnear-optimality up to 29x faster than standard RRT* in 3D environments. Code:\nhttps://xinyuwuu.github.io/Episodic_RRT/.", "AI": {"tldr": "Episodic RRT (ERRT) 是一种结合深度强化学习的混合规划框架，通过多步探索替代随机采样，显著提升了运动规划的效率。", "motivation": "传统基于采样的运动规划器（如RRT）在高维或复杂环境中效率低下，主要依赖随机采样导致探索不集中。", "method": "ERRT 使用深度强化学习（DRL）生成多步探索片段，取代随机点采样，实现有向的路径搜索。", "result": "在2D、3D和6D环境中，ERRT及其变体表现优于传统方法，6D机械臂场景中成功率98%，速度提升107倍，碰撞检测减少99.6%。", "conclusion": "ERRT通过有向探索显著提升了运动规划的效率，其渐进最优变体ERRT*在优化性能上表现更优。"}}
{"id": "2507.06684", "categories": ["eess.IV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06684", "abs": "https://arxiv.org/abs/2507.06684", "authors": ["Matéo Ducastel", "David Tschumperlé", "Yvain Quéau"], "title": "Photometric Stereo using Gaussian Splatting and inverse rendering", "comment": "in French language. GRETSI 2025, Association GRETSI, Aug 2025,\n  Strasbourg, France", "summary": "Recent state-of-the-art algorithms in photometric stereo rely on neural\nnetworks and operate either through prior learning or inverse rendering\noptimization. Here, we revisit the problem of calibrated photometric stereo by\nleveraging recent advances in 3D inverse rendering using the Gaussian Splatting\nformalism. This allows us to parameterize the 3D scene to be reconstructed and\noptimize it in a more interpretable manner. Our approach incorporates a\nsimplified model for light representation and demonstrates the potential of the\nGaussian Splatting rendering engine for the photometric stereo problem.", "AI": {"tldr": "利用高斯泼溅形式重新审视光度立体问题，通过3D逆渲染优化场景参数化。", "motivation": "探索高斯泼溅渲染引擎在光度立体问题中的潜力，提供更可解释的优化方式。", "method": "结合简化的光照模型，使用高斯泼溅形式参数化3D场景并进行优化。", "result": "展示了高斯泼溅渲染引擎在光度立体问题中的应用潜力。", "conclusion": "高斯泼溅形式为光度立体问题提供了一种新的、可解释的优化方法。"}}
{"id": "2507.06405", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06405", "abs": "https://arxiv.org/abs/2507.06405", "authors": ["Lala Shakti Swarup Ray", "Mengxi Liu", "Deepika Gurung", "Bo Zhou", "Sungho Suh", "Paul Lukowicz"], "title": "SImpHAR: Advancing impedance-based human activity recognition using 3D simulation and text-to-motion models", "comment": null, "summary": "Human Activity Recognition (HAR) with wearable sensors is essential for\napplications in healthcare, fitness, and human-computer interaction.\nBio-impedance sensing offers unique advantages for fine-grained motion capture\nbut remains underutilized due to the scarcity of labeled data. We introduce\nSImpHAR, a novel framework addressing this limitation through two core\ncontributions. First, we propose a simulation pipeline that generates realistic\nbio-impedance signals from 3D human meshes using shortest-path estimation,\nsoft-body physics, and text-to-motion generation serving as a digital twin for\ndata augmentation. Second, we design a two-stage training strategy with\ndecoupled approach that enables broader activity coverage without requiring\nlabel-aligned synthetic data. We evaluate SImpHAR on our collected ImpAct\ndataset and two public benchmarks, showing consistent improvements over\nstate-of-the-art methods, with gains of up to 22.3% and 21.8%, in terms of\naccuracy and macro F1 score, respectively. Our results highlight the promise of\nsimulation-driven augmentation and modular training for impedance-based HAR.", "AI": {"tldr": "SImpHAR框架通过模拟生物阻抗信号和两阶段训练策略，显著提升了基于生物阻抗的人体活动识别性能。", "motivation": "生物阻抗传感在人体活动识别中潜力巨大，但标记数据稀缺限制了其应用。", "method": "提出模拟管道生成逼真生物阻抗信号，并采用两阶段训练策略。", "result": "在多个数据集上性能提升显著，准确率和F1分数分别提高22.3%和21.8%。", "conclusion": "模拟驱动增强和模块化训练为基于生物阻抗的活动识别提供了新思路。"}}
{"id": "2507.05116", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.05116", "abs": "https://arxiv.org/abs/2507.05116", "authors": ["Juyi Lin", "Amir Taherin", "Arash Akbari", "Arman Akbari", "Lei Lu", "Guangyu Chen", "Taskin Padir", "Xiaomeng Yang", "Weiwei Chen", "Yiqian Li", "Xue Lin", "David Kaeli", "Pu Zhao", "Yanzhi Wang"], "title": "VOTE: Vision-Language-Action Optimization with Trajectory Ensemble Voting", "comment": null, "summary": "Recent large-scale Vision Language Action (VLA) models have shown superior\nperformance in robotic manipulation tasks guided by natural language. However,\ntheir generalization remains limited when applied to novel objects or\nunfamiliar environments that lie outside the training distribution. To address\nthis, many existing approaches integrate additional components such as depth\nestimation, segmentation, or even diffusion to improve generalization, at the\ncost of adding significant computation overhead, resulting in low efficiency.\nThis motivates the exploration of efficient action prediction methods, which\nare independent of additional high-level visual representations or diffusion\ntechniques. In this work, we propose VOTE, an efficient and general framework\nfor the optimization and acceleration of VLA models. In details, we propose a\nnovel tokenizer-free fine-tuning approach for parallel accurate action\nprediction, which reduces computational overhead and accelerates inference\nspeed. Additionally, we adopt an ensemble voting strategy for the action\nsampling, which significantly improves model performance and enhances\ngeneralization. Experimental results show that our method achieves\nstate-of-the-art performance with 35$\\times$ faster inference and 145 Hz\nthroughput. All the details and codes will be open-sourced.", "AI": {"tldr": "提出了一种高效的VLA模型优化框架VOTE，通过免分词器的微调方法和集成投票策略，显著提升了推理速度和泛化能力。", "motivation": "现有VLA模型在新对象或陌生环境中的泛化能力有限，且附加组件增加了计算开销，因此需要探索高效的动作预测方法。", "method": "采用免分词器的微调方法进行并行动作预测，并结合集成投票策略优化动作采样。", "result": "实验表明，该方法实现了35倍推理加速和145 Hz的吞吐量，达到最优性能。", "conclusion": "VOTE框架高效且泛化能力强，代码将开源。"}}
{"id": "2507.06457", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06457", "abs": "https://arxiv.org/abs/2507.06457", "authors": ["Dustin Wang", "Rui-Jie Zhu", "Steven Abreu", "Yong Shan", "Taylor Kergan", "Yuqi Pan", "Yuhong Chou", "Zheng Li", "Ge Zhang", "Wenhao Huang", "Jason Eshraghian"], "title": "A Systematic Analysis of Hybrid Linear Attention", "comment": null, "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.", "AI": {"tldr": "论文研究了线性注意力机制在Transformer中的应用，发现优秀的独立线性模型在混合架构中未必表现最佳，并推荐了高效的混合架构比例。", "motivation": "解决Transformer在处理长序列时的二次复杂性和内存问题，同时提升线性注意力模型的召回性能。", "method": "系统评估了多种线性注意力模型，包括独立和混合架构，训练并开源了72个模型进行对比分析。", "result": "语言建模性能稳定，但召回性能随全注意力层比例增加而提升，推荐3:1至6:1的线性-全注意力比例。", "conclusion": "选择性门控、分层递归和可控遗忘对混合模型至关重要，推荐HGRN-2或GatedDeltaNet架构。"}}
{"id": "2507.06492", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06492", "abs": "https://arxiv.org/abs/2507.06492", "authors": ["Jiajun Shen", "Hao Tu", "Fengjun Li", "Morteza Hashemi", "Di Wu", "Huazhen Fang"], "title": "Dual State-space Fidelity Blade (D-STAB): A Novel Stealthy Cyber-physical Attack Paradigm", "comment": "accepted by 2025 American Control Conference", "summary": "This paper presents a novel cyber-physical attack paradigm, termed the Dual\nState-Space Fidelity Blade (D-STAB), which targets the firmware of core\ncyber-physical components as a new class of attack surfaces. The D-STAB attack\nexploits the information asymmetry caused by the fidelity gap between\nhigh-fidelity and low-fidelity physical models in cyber-physical systems. By\ndesigning precise adversarial constraints based on high-fidelity state-space\ninformation, the attack induces deviations in high-fidelity states that remain\nundetected by defenders relying on low-fidelity observations. The effectiveness\nof D-STAB is demonstrated through a case study in cyber-physical battery\nsystems, specifically in an optimal charging task governed by a Battery\nManagement System (BMS).", "AI": {"tldr": "提出了一种新型网络物理攻击范式D-STAB，通过利用高保真与低保真模型之间的信息不对称性，攻击核心网络物理组件的固件。", "motivation": "针对网络物理系统中高保真与低保真模型之间的保真度差距，探索了一种新的攻击表面。", "method": "设计基于高保真状态空间信息的精确对抗约束，诱导高保真状态偏差，而依赖低保真观测的防御者无法检测。", "result": "通过电池管理系统（BMS）中的最优充电任务案例研究，验证了D-STAB的有效性。", "conclusion": "D-STAB为网络物理系统的安全研究提供了新的攻击范式，揭示了模型保真度差距带来的潜在威胁。"}}
{"id": "2507.06625", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06625", "abs": "https://arxiv.org/abs/2507.06625", "authors": ["Shizhe Cai", "Jayadeep Jacob", "Zeya Yin", "Fabio Ramos"], "title": "Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic", "comment": "9 pages, 10 figures", "summary": "Deep reinforcement learning has shown remarkable success in continuous\ncontrol tasks, yet often requires extensive training data, struggles with\ncomplex, long-horizon planning, and fails to maintain safety constraints during\noperation. Meanwhile, Model Predictive Control (MPC) offers explainability and\nconstraint satisfaction, but typically yields only locally optimal solutions\nand demands careful cost function design. This paper introduces the Q-guided\nSTein variational model predictive Actor-Critic (Q-STAC), a novel framework\nthat bridges these approaches by integrating Bayesian MPC with actor-critic\nreinforcement learning through constrained Stein Variational Gradient Descent\n(SVGD). Our method optimizes control sequences directly using learned Q-values\nas objectives, eliminating the need for explicit cost function design while\nleveraging known system dynamics to enhance sample efficiency and ensure\ncontrol signals remain within safe boundaries. Extensive experiments on 2D\nnavigation and robotic manipulation tasks demonstrate that Q-STAC achieves\nsuperior sample efficiency, robustness, and optimality compared to\nstate-of-the-art algorithms, while maintaining the high expressiveness of\npolicy distributions. Experiment videos are available on our website:\nhttps://sites.google.com/view/q-stac", "AI": {"tldr": "Q-STAC结合贝叶斯MPC与演员-评论家强化学习，通过约束Stein变分梯度下降，提升样本效率与安全性。", "motivation": "解决深度强化学习在连续控制任务中数据需求大、长时规划难、安全性不足的问题，同时弥补MPC的局部最优和成本函数设计复杂的缺陷。", "method": "整合贝叶斯MPC与演员-评论家强化学习，利用Q值优化控制序列，避免显式成本函数设计，通过SVGD确保安全边界。", "result": "在2D导航和机器人操作任务中，Q-STAC表现出更高的样本效率、鲁棒性和最优性。", "conclusion": "Q-STAC成功结合了两种方法的优势，实现了高效、安全且最优的控制。"}}
{"id": "2507.06717", "categories": ["eess.IV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.06717", "abs": "https://arxiv.org/abs/2507.06717", "authors": ["Xuyang Chen", "Chong Huang", "Daquan Feng", "Lei Luo", "Yao Sun", "Xiang-Gen Xia"], "title": "QoE Optimization for Semantic Self-Correcting Video Transmission in Multi-UAV Networks", "comment": "13 pages", "summary": "Real-time unmanned aerial vehicle (UAV) video streaming is essential for\ntime-sensitive applications, including remote surveillance, emergency response,\nand environmental monitoring. However, it faces challenges such as limited\nbandwidth, latency fluctuations, and high packet loss. To address these issues,\nwe propose a novel semantic self-correcting video transmission framework with\nultra-fine bitrate granularity (SSCV-G). In SSCV-G, video frames are encoded\ninto a compact semantic codebook space, and the transmitter adaptively sends a\nsubset of semantic indices based on bandwidth availability, enabling\nfine-grained bitrate control for improved bandwidth efficiency. At the\nreceiver, a spatio-temporal vision transformer (ST-ViT) performs multi-frame\njoint decoding to reconstruct dropped semantic indices by modeling intra- and\ninter-frame dependencies. To further improve performance under dynamic network\nconditions, we integrate a multi-user proximal policy optimization (MUPPO)\nreinforcement learning scheme that jointly optimizes communication resource\nallocation and semantic bitrate selection to maximize user Quality of\nExperience (QoE). Extensive experiments demonstrate that the proposed SSCV-G\nsignificantly outperforms state-of-the-art video codecs in coding efficiency,\nbandwidth adaptability, and packet loss robustness. Moreover, the proposed\nMUPPO-based QoE optimization consistently surpasses existing benchmarks.", "AI": {"tldr": "提出了一种基于语义自校正的视频传输框架（SSCV-G），通过细粒度比特率控制和多用户强化学习优化，显著提升了无人机视频流的带宽效率和用户体验。", "motivation": "无人机实时视频流在带宽受限、延迟波动和高丢包率等挑战下，难以满足时间敏感应用的需求。", "method": "SSCV-G将视频帧编码为语义码本空间，自适应发送语义索引；接收端使用ST-ViT进行多帧联合解码；结合MUPPO强化学习优化资源分配和比特率选择。", "result": "SSCV-G在编码效率、带宽适应性和丢包鲁棒性上优于现有视频编解码器；MUPPO优化显著提升用户体验。", "conclusion": "SSCV-G和MUPPO的结合为无人机视频流提供了一种高效、自适应的解决方案。"}}
{"id": "2507.06411", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06411", "abs": "https://arxiv.org/abs/2507.06411", "authors": ["Hayat Ullah", "Arslan Munir", "Oliver Nina"], "title": "Hierarchical Multi-Stage Transformer Architecture for Context-Aware Temporal Action Localization", "comment": "17 pages, 6 figures,", "summary": "Inspired by the recent success of transformers and multi-stage architectures\nin video recognition and object detection domains. We thoroughly explore the\nrich spatio-temporal properties of transformers within a multi-stage\narchitecture paradigm for the temporal action localization (TAL) task. This\nexploration led to the development of a hierarchical multi-stage transformer\narchitecture called PCL-Former, where each subtask is handled by a dedicated\ntransformer module with a specialized loss function. Specifically, the\nProposal-Former identifies candidate segments in an untrimmed video that may\ncontain actions, the Classification-Former classifies the action categories\nwithin those segments, and the Localization-Former precisely predicts the\ntemporal boundaries (i.e., start and end) of the action instances. To evaluate\nthe performance of our method, we have conducted extensive experiments on three\nchallenging benchmark datasets: THUMOS-14, ActivityNet-1.3, and HACS Segments.\nWe also conducted detailed ablation experiments to assess the impact of each\nindividual module of our PCL-Former. The obtained quantitative results validate\nthe effectiveness of the proposed PCL-Former, outperforming state-of-the-art\nTAL approaches by 2.8%, 1.2%, and 4.8% on THUMOS14, ActivityNet-1.3, and HACS\ndatasets, respectively.", "AI": {"tldr": "提出了一种名为PCL-Former的多阶段Transformer架构，用于时间动作定位任务，通过三个专用模块分别处理候选段识别、动作分类和时间边界预测，在多个数据集上表现优于现有方法。", "motivation": "受Transformer和多阶段架构在视频识别和目标检测领域的成功启发，探索其在时间动作定位任务中的潜力。", "method": "设计了PCL-Former，包含Proposal-Former、Classification-Former和Localization-Former三个模块，每个模块配备专用损失函数。", "result": "在THUMOS14、ActivityNet-1.3和HACS数据集上分别提升2.8%、1.2%和4.8%，优于现有方法。", "conclusion": "PCL-Former通过模块化设计和专用损失函数，显著提升了时间动作定位任务的性能。"}}
{"id": "2507.06489", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06489", "abs": "https://arxiv.org/abs/2507.06489", "authors": ["Stephen Obadinma", "Xiaodan Zhu"], "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks", "comment": null, "summary": "Robust verbal confidence generated by large language models (LLMs) is crucial\nfor the deployment of LLMs to ensure transparency, trust, and safety in\nhuman-AI interactions across many high-stakes applications. In this paper, we\npresent the first comprehensive study on the robustness of verbal confidence\nunder adversarial attacks. We introduce a novel framework for attacking verbal\nconfidence scores through both perturbation and jailbreak-based methods, and\nshow that these attacks can significantly jeopardize verbal confidence\nestimates and lead to frequent answer changes. We examine a variety of\nprompting strategies, model sizes, and application domains, revealing that\ncurrent confidence elicitation methods are vulnerable and that commonly used\ndefence techniques are largely ineffective or counterproductive. Our findings\nunderscore the urgent need to design more robust mechanisms for confidence\nexpression in LLMs, as even subtle semantic-preserving modifications can lead\nto misleading confidence in responses.", "AI": {"tldr": "研究首次全面探讨了大型语言模型（LLM）在对抗攻击下口头置信度的鲁棒性，提出了一种攻击框架，并发现现有方法易受攻击且防御无效。", "motivation": "确保LLM在人类-AI交互中的透明性、信任和安全性，特别是在高风险应用中。", "method": "引入基于扰动和越狱方法的攻击框架，测试不同提示策略、模型规模和应用领域。", "result": "攻击显著危害置信度估计并导致频繁答案更改，现有防御技术无效或适得其反。", "conclusion": "亟需设计更鲁棒的置信度表达机制，因为即使是微小的语义保留修改也可能导致误导性置信度。"}}
{"id": "2507.06595", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06595", "abs": "https://arxiv.org/abs/2507.06595", "authors": ["Lane D. Smith", "Daniel S. Kirschen"], "title": "Effects of Net Metering Policies on Distributed Energy Resource Valuation and Operation", "comment": "Accepted to the 2025 IEEE Power and Energy Society (PES) General\n  Meeting", "summary": "Net energy metering has been a successful policy for increasing solar\ngeneration installations and reducing the costs of photovoltaic arrays for\nconsumers. However, increased maturity of solar technologies and concerns over\ncost shifts created by net energy metering have recently caused the policy to\nchange its incentives. What once favored behind-the-meter solar generation now\nis focused on compensating flexible operation. This paper explores the impacts\nthat different net energy metering policies have on commercial consumers with\nvarious distributed energy resources. We show that the newest iteration of net\nenergy metering is less beneficial for consumers with only solar generation and\ninstead favors those that pair energy storage with solar. Though shiftable\nflexible demand offers consumers the ability to operate flexibly, the export\nprices offered by the latest net energy metering policy provide limited value\nto flexible demand.", "AI": {"tldr": "本文探讨了不同净计量政策对商业消费者的影响，发现最新政策对仅使用太阳能的消费者不利，而更有利于搭配储能的消费者。", "motivation": "研究净计量政策变化对商业消费者的影响，尤其是对太阳能和储能结合使用的激励效果。", "method": "分析不同净计量政策下商业消费者的能源使用情况，重点关注太阳能和储能结合的效果。", "result": "最新净计量政策对仅使用太阳能的消费者不利，但对搭配储能的消费者更有利；灵活需求的出口价格价值有限。", "conclusion": "政策变化更倾向于支持储能与太阳能结合使用，而非单独的太阳能发电。"}}
{"id": "2507.06690", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06690", "abs": "https://arxiv.org/abs/2507.06690", "authors": ["Guobin Zhu", "Rui Zhou", "Wenkang Ji", "Hongyin Zhang", "Donglin Wang", "Shiyu Zhao"], "title": "Multi-Task Multi-Agent Reinforcement Learning via Skill Graphs", "comment": "Conditionally accepted by IEEE Robotics and Automation Letters", "summary": "Multi-task multi-agent reinforcement learning (MT-MARL) has recently gained\nattention for its potential to enhance MARL's adaptability across multiple\ntasks. However, it is challenging for existing multi-task learning methods to\nhandle complex problems, as they are unable to handle unrelated tasks and\npossess limited knowledge transfer capabilities. In this paper, we propose a\nhierarchical approach that efficiently addresses these challenges. The\nhigh-level module utilizes a skill graph, while the low-level module employs a\nstandard MARL algorithm. Our approach offers two contributions. First, we\nconsider the MT-MARL problem in the context of unrelated tasks, expanding the\nscope of MTRL. Second, the skill graph is used as the upper layer of the\nstandard hierarchical approach, with training independent of the lower layer,\neffectively handling unrelated tasks and enhancing knowledge transfer\ncapabilities. Extensive experiments are conducted to validate these advantages\nand demonstrate that the proposed method outperforms the latest hierarchical\nMAPPO algorithms. Videos and code are available at\nhttps://github.com/WindyLab/MT-MARL-SG", "AI": {"tldr": "本文提出了一种分层方法，通过技能图解决多任务多智能体强化学习中的无关任务处理和知识迁移问题。", "motivation": "现有多任务学习方法难以处理复杂问题，尤其是无关任务和有限的知识迁移能力。", "method": "采用分层方法，高层模块使用技能图，低层模块采用标准MARL算法。", "result": "实验表明，该方法优于最新的分层MAPPO算法，有效处理无关任务并提升知识迁移能力。", "conclusion": "该方法扩展了多任务强化学习的范围，通过技能图显著提升了性能。"}}
{"id": "2507.06764", "categories": ["eess.IV", "cs.CV", "cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.06764", "abs": "https://arxiv.org/abs/2507.06764", "authors": ["Guixian Xu", "Jinglai Li", "Junqi Tang"], "title": "Fast Equivariant Imaging: Acceleration for Unsupervised Learning via Augmented Lagrangian and Auxiliary PnP Denoisers", "comment": null, "summary": "We propose Fast Equivariant Imaging (FEI), a novel unsupervised learning\nframework to efficiently train deep imaging networks without ground-truth data.\nFrom the perspective of reformulating the Equivariant Imaging based\noptimization problem via the method of Lagrange multipliers and utilizing\nplug-and-play denoisers, this novel unsupervised scheme shows superior\nefficiency and performance compared to vanilla Equivariant Imaging paradigm. In\nparticular, our PnP-FEI scheme achieves an order-of-magnitude (10x)\nacceleration over standard EI on training U-Net with CT100 dataset for X-ray CT\nreconstruction, with improved generalization performance.", "AI": {"tldr": "提出了一种名为Fast Equivariant Imaging (FEI)的无监督学习框架，通过拉格朗日乘数法和即插即用去噪器优化问题，显著提升了训练效率和性能。", "motivation": "解决传统Equivariant Imaging范式在无监督学习中的效率低下问题。", "method": "利用拉格朗日乘数法重新构建优化问题，并结合即插即用去噪器。", "result": "在CT100数据集上训练U-Net时，FEI比标准EI快10倍，且泛化性能更好。", "conclusion": "FEI是一种高效且性能优越的无监督学习框架，适用于深度成像网络训练。"}}
{"id": "2507.06442", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06442", "abs": "https://arxiv.org/abs/2507.06442", "authors": ["Soroush Shahi", "Farzad Shahabi", "Rama Nabulsi", "Glenn Fernandes", "Aggelos Katsaggelos", "Nabil Alshurafa"], "title": "THOR: Thermal-guided Hand-Object Reasoning via Adaptive Vision Sampling", "comment": null, "summary": "Wearable cameras are increasingly used as an observational and interventional\ntool for human behaviors by providing detailed visual data of hand-related\nactivities. This data can be leveraged to facilitate memory recall for logging\nof behavior or timely interventions aimed at improving health. However,\ncontinuous processing of RGB images from these cameras consumes significant\npower impacting battery lifetime, generates a large volume of unnecessary video\ndata for post-processing, raises privacy concerns, and requires substantial\ncomputational resources for real-time analysis. We introduce THOR, a real-time\nadaptive spatio-temporal RGB frame sampling method that leverages thermal\nsensing to capture hand-object patches and classify them in real-time. We use\nlow-resolution thermal camera data to identify moments when a person switches\nfrom one hand-related activity to another, and adjust the RGB frame sampling\nrate by increasing it during activity transitions and reducing it during\nperiods of sustained activity. Additionally, we use the thermal cues from the\nhand to localize the region of interest (i.e., the hand-object interaction) in\neach RGB frame, allowing the system to crop and process only the necessary part\nof the image for activity recognition. We develop a wearable device to validate\nour method through an in-the-wild study with 14 participants and over 30\nactivities, and further evaluate it on Ego4D (923 participants across 9\ncountries, totaling 3,670 hours of video). Our results show that using only 3%\nof the original RGB video data, our method captures all the activity segments,\nand achieves hand-related activity recognition F1-score (95%) comparable to\nusing the entire RGB video (94%). Our work provides a more practical path for\nthe longitudinal use of wearable cameras to monitor hand-related activities and\nhealth-risk behaviors in real time.", "AI": {"tldr": "THOR是一种基于热感应的实时自适应RGB帧采样方法，用于高效捕捉和分类手部活动，显著减少数据量和计算资源需求。", "motivation": "解决穿戴式相机连续处理RGB图像时的高功耗、大数据量、隐私问题和计算资源需求。", "method": "利用低分辨率热感数据识别手部活动切换，动态调整RGB帧采样率，并结合热感线索定位手部交互区域以减少图像处理范围。", "result": "仅需3%的原始RGB数据即可捕捉全部活动段，手部活动识别F1分数达95%，与全RGB视频（94%）相当。", "conclusion": "THOR为穿戴式相机实时监测手部活动和健康风险行为提供了更实用的解决方案。"}}
{"id": "2507.06459", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06459", "abs": "https://arxiv.org/abs/2507.06459", "authors": ["Riadul Islam", "Joey Mulé", "Dhandeep Challagundla", "Shahmir Rizvi", "Sean Carson"], "title": "EA: An Event Autoencoder for High-Speed Vision Sensing", "comment": null, "summary": "High-speed vision sensing is essential for real-time perception in\napplications such as robotics, autonomous vehicles, and industrial automation.\nTraditional frame-based vision systems suffer from motion blur, high latency,\nand redundant data processing, limiting their performance in dynamic\nenvironments. Event cameras, which capture asynchronous brightness changes at\nthe pixel level, offer a promising alternative but pose challenges in object\ndetection due to sparse and noisy event streams. To address this, we propose an\nevent autoencoder architecture that efficiently compresses and reconstructs\nevent data while preserving critical spatial and temporal features. The\nproposed model employs convolutional encoding and incorporates adaptive\nthreshold selection and a lightweight classifier to enhance recognition\naccuracy while reducing computational complexity. Experimental results on the\nexisting Smart Event Face Dataset (SEFD) demonstrate that our approach achieves\ncomparable accuracy to the YOLO-v4 model while utilizing up to $35.5\\times$\nfewer parameters. Implementations on embedded platforms, including Raspberry Pi\n4B and NVIDIA Jetson Nano, show high frame rates ranging from 8 FPS up to 44.8\nFPS. The proposed classifier exhibits up to 87.84x better FPS than the\nstate-of-the-art and significantly improves event-based vision performance,\nmaking it ideal for low-power, high-speed applications in real-time edge\ncomputing.", "AI": {"tldr": "提出了一种基于事件自动编码器的高效事件数据压缩与重建方法，显著提升了事件相机的物体检测性能，适用于实时边缘计算。", "motivation": "传统帧式视觉系统在动态环境中存在运动模糊、高延迟和冗余数据处理问题，事件相机虽有潜力但面临稀疏和噪声事件流的挑战。", "method": "采用卷积编码的事件自动编码器架构，结合自适应阈值选择和轻量级分类器，优化识别精度并降低计算复杂度。", "result": "在SEFD数据集上，模型精度与YOLO-v4相当，但参数减少35.5倍；在嵌入式平台上实现8-44.8 FPS的高帧率。", "conclusion": "该方法显著提升了事件相机的性能，适用于低功耗、高速实时边缘计算应用。"}}
{"id": "2507.06506", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.06506", "abs": "https://arxiv.org/abs/2507.06506", "authors": ["Russell Taylor", "Benjamin Herbert", "Michael Sana"], "title": "Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings", "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Translating wordplay across languages presents unique challenges that have\nlong confounded both professional human translators and machine translation\nsystems. This research proposes a novel approach for translating puns from\nEnglish to French by combining state-of-the-art large language models with\nspecialized techniques for wordplay generation.\n  Our methodology employs a three-stage approach. First, we establish a\nbaseline using multiple frontier large language models with feedback based on a\nnew contrastive learning dataset. Second, we implement a guided\nchain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we\nimplement a multi-agent generator-discriminator framework for evaluating and\nregenerating puns with feedback.\n  Moving beyond the limitations of literal translation, our methodology's\nprimary objective is to capture the linguistic creativity and humor of the\nsource text wordplay, rather than simply duplicating its vocabulary. Our best\nruns earned first and second place in the CLEF JOKER 2025 Task 2 competition\nwhere they were evaluated manually by expert native French speakers.\n  This research addresses a gap between translation studies and computational\nlinguistics by implementing linguistically-informed techniques for wordplay\ntranslation, advancing our understanding of how language models can be\nleveraged to handle the complex interplay between semantic ambiguity, phonetic\nsimilarity, and the implicit cultural and linguistic awareness needed for\nsuccessful humor.", "AI": {"tldr": "提出了一种结合大型语言模型和专门技术的英语到法语双关语翻译新方法，并在CLEF JOKER 2025竞赛中取得优异成绩。", "motivation": "解决跨语言双关语翻译的挑战，捕捉源文本的语言创造力和幽默，而非简单词汇复制。", "method": "采用三阶段方法：基线模型、引导思维链管道和多代理生成-判别框架。", "result": "在CLEF JOKER 2025竞赛中获第一和第二名。", "conclusion": "填补了翻译研究与计算语言学之间的空白，推进了语言模型处理语义模糊性和文化意识的能力。"}}
{"id": "2507.06617", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06617", "abs": "https://arxiv.org/abs/2507.06617", "authors": ["Xiaokan Yang", "Wei Chen", "Li Qiu"], "title": "The Small Phase Condition is Necessary for Symmetric Systems", "comment": "Under review at Automatica", "summary": "In this paper, we show that the small phase condition is both sufficient and\nnecessary to ensure the feedback stability when the interconnected systems are\nsymmetric. Such symmetric systems arise in diverse applications. The key lies\nin that, for a complex symmetric and semi-sectorial matrix, the transformation\nmatrix in its generalized sectorial decomposition can be taken to be real. Such\na result fills the gap of phase based necessary condition for the feedback\nstability of symmetric systems, and serves as a counterpart of the necessity\nresult for small gain condition. Moreover, we explore the necessity of small\nphase condition for general asymmetric systems. Some insightful results are\npresented, which help to clarify the main challenge in the general case.", "AI": {"tldr": "论文证明了小相位条件对对称互联系统的反馈稳定性是充分且必要的，填补了相位条件在对称系统中的必要性空白，并探讨了非对称系统的挑战。", "motivation": "对称系统在多种应用中常见，但相位条件对其反馈稳定性的必要性尚未明确。本文旨在填补这一理论空白。", "method": "通过分析复对称半扇形矩阵的广义扇形分解，证明变换矩阵可为实数。", "result": "小相位条件对对称系统是充分且必要的，同时探讨了非对称系统的必要性。", "conclusion": "研究为对称系统的反馈稳定性提供了完整的相位条件理论，并揭示了非对称系统的主要挑战。"}}
{"id": "2507.06700", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.06700", "abs": "https://arxiv.org/abs/2507.06700", "authors": ["Pranav Pandey", "Ramviyas Parasuraman", "Prashant Doshi"], "title": "Integrating Perceptions: A Human-Centered Physical Safety Model for Human-Robot Interaction", "comment": "Accepted to IEEE RO-MAN 2025 Conference", "summary": "Ensuring safety in human-robot interaction (HRI) is essential to foster user\ntrust and enable the broader adoption of robotic systems. Traditional safety\nmodels primarily rely on sensor-based measures, such as relative distance and\nvelocity, to assess physical safety. However, these models often fail to\ncapture subjective safety perceptions, which are shaped by individual traits\nand contextual factors. In this paper, we introduce and analyze a parameterized\ngeneral safety model that bridges the gap between physical and perceived safety\nby incorporating a personalization parameter, $\\rho$, into the safety\nmeasurement framework to account for individual differences in safety\nperception. Through a series of hypothesis-driven human-subject studies in a\nsimulated rescue scenario, we investigate how emotional state, trust, and robot\nbehavior influence perceived safety. Our results show that $\\rho$ effectively\ncaptures meaningful individual differences, driven by affective responses,\ntrust in task consistency, and clustering into distinct user types.\nSpecifically, our findings confirm that predictable and consistent robot\nbehavior as well as the elicitation of positive emotional states, significantly\nenhance perceived safety. Moreover, responses cluster into a small number of\nuser types, supporting adaptive personalization based on shared safety models.\nNotably, participant role significantly shapes safety perception, and repeated\nexposure reduces perceived safety for participants in the casualty role,\nemphasizing the impact of physical interaction and experiential change. These\nfindings highlight the importance of adaptive, human-centered safety models\nthat integrate both psychological and behavioral dimensions, offering a pathway\ntoward more trustworthy and effective HRI in safety-critical domains.", "AI": {"tldr": "论文提出了一种参数化通用安全模型，通过个性化参数ρ结合物理安全和主观安全感知，研究情感状态、信任和机器人行为对安全感知的影响。", "motivation": "传统安全模型仅依赖传感器数据，忽略了个体差异和主观安全感知。本文旨在填补这一空白。", "method": "通过模拟救援场景的人体实验，分析情感、信任和机器人行为对安全感知的影响，并引入参数ρ。", "result": "ρ能有效捕捉个体差异，机器人行为的可预测性和一致性以及积极情感状态显著提升安全感知。", "conclusion": "研究强调需要结合心理和行为维度的自适应安全模型，以提升人机交互的信任和效果。"}}
{"id": "2507.06828", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06828", "abs": "https://arxiv.org/abs/2507.06828", "authors": ["Xuesong Li", "Nassir Navab", "Zhongliang Jiang"], "title": "Speckle2Self: Self-Supervised Ultrasound Speckle Reduction Without Clean Data", "comment": null, "summary": "Image denoising is a fundamental task in computer vision, particularly in\nmedical ultrasound (US) imaging, where speckle noise significantly degrades\nimage quality. Although recent advancements in deep neural networks have led to\nsubstantial improvements in denoising for natural images, these methods cannot\nbe directly applied to US speckle noise, as it is not purely random. Instead,\nUS speckle arises from complex wave interference within the body\nmicrostructure, making it tissue-dependent. This dependency means that\nobtaining two independent noisy observations of the same scene, as required by\npioneering Noise2Noise, is not feasible. Additionally, blind-spot networks also\ncannot handle US speckle noise due to its high spatial dependency. To address\nthis challenge, we introduce Speckle2Self, a novel self-supervised algorithm\nfor speckle reduction using only single noisy observations. The key insight is\nthat applying a multi-scale perturbation (MSP) operation introduces\ntissue-dependent variations in the speckle pattern across different scales,\nwhile preserving the shared anatomical structure. This enables effective\nspeckle suppression by modeling the clean image as a low-rank signal and\nisolating the sparse noise component. To demonstrate its effectiveness,\nSpeckle2Self is comprehensively compared with conventional filter-based\ndenoising algorithms and SOTA learning-based methods, using both realistic\nsimulated US images and human carotid US images. Additionally, data from\nmultiple US machines are employed to evaluate model generalization and\nadaptability to images from unseen domains. \\textit{Code and datasets will be\nreleased upon acceptance.", "AI": {"tldr": "Speckle2Self是一种自监督算法，用于仅使用单次噪声观测减少超声图像中的斑点噪声。", "motivation": "超声图像中的斑点噪声具有组织依赖性，传统方法无法直接处理，需要一种新的自监督方法。", "method": "通过多尺度扰动（MSP）操作引入组织依赖性变化，将干净图像建模为低秩信号并分离稀疏噪声。", "result": "Speckle2Self在模拟和真实超声图像上表现优于传统滤波器和最先进的学习方法。", "conclusion": "Speckle2Self为超声斑点噪声去除提供了一种有效的自监督解决方案，并展示了良好的泛化能力。"}}
{"id": "2507.06485", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06485", "abs": "https://arxiv.org/abs/2507.06485", "authors": ["Ziyang Wang", "Jaehong Yoon", "Shoubin Yu", "Md Mohaiminul Islam", "Gedas Bertasius", "Mohit Bansal"], "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning", "comment": "The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/", "summary": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and finetuning remain significant\nchallenges. These methods often rely on large-scale supervised fine-tuning\n(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,\nmaking them costly and hard to scale. To address this, we present Video-RTS, a\nnew approach to improve video reasoning capability with drastically improved\ndata efficiency by combining data-efficient RL with a video-adaptive test-time\nscaling (TTS) strategy. Based on observations about the data scaling of RL\nsamples, we skip the resource-intensive SFT step and employ efficient pure-RL\ntraining with output-based rewards, requiring no additional annotations or\nextensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by an average of 2.4% in accuracy\nusing only 3.6% training samples. For example, Video-RTS achieves a 4.2%\nimprovement on Video-Holmes, a recent and challenging video reasoning\nbenchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and\nadaptive video TTS offer complementary strengths, enabling Video-RTS's strong\nreasoning performance.", "AI": {"tldr": "Video-RTS通过结合数据高效的强化学习和视频自适应测试时间缩放策略，显著提升了视频推理能力，减少了数据需求和计算资源。", "motivation": "现有基于强化学习和大型语言模型的视频推理方法需要大量监督微调和长链思维标注，成本高且难以扩展。", "method": "跳过资源密集的监督微调步骤，采用纯强化学习训练和稀疏到密集的视频测试时间缩放策略。", "result": "在多个视频推理基准测试中，Video-RTS平均准确率提升2.4%，仅需3.6%的训练样本。", "conclusion": "Video-RTS通过纯强化学习和自适应视频缩放策略，显著提升了视频推理的效率和性能。"}}
{"id": "2507.06528", "categories": ["cs.CL", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06528", "abs": "https://arxiv.org/abs/2507.06528", "authors": ["Huisheng Wang", "Zhuoshi Pan", "Hangjing Zhang", "Mingxiao Liu", "Hanqing Gao", "H. Vicky Zhao"], "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior", "comment": null, "summary": "Aligning Large Language Models (LLMs) with investor decision-making processes\nunder herd behavior is a critical challenge in behavioral finance, which\ngrapples with a fundamental limitation: the scarcity of real-user data needed\nfor Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM\noutputs and human behavioral patterns, its reliance on massive authentic data\nimposes substantial collection costs and privacy risks. We propose InvestAlign,\na novel framework that constructs high-quality SFT datasets by leveraging\ntheoretical solutions to similar and simple optimal investment problems rather\nthan complex scenarios. Our theoretical analysis demonstrates that training\nLLMs with InvestAlign-generated data achieves faster parameter convergence than\nusing real-user data, suggesting superior learning efficiency. Furthermore, we\ndevelop InvestAgent, an LLM agent fine-tuned with InvestAlign, which\ndemonstrates significantly closer alignment to real-user data than pre-SFT\nmodels in both simple and complex investment problems. This highlights our\nproposed InvestAlign as a promising approach with the potential to address\ncomplex optimal investment problems and align LLMs with investor\ndecision-making processes under herd behavior. Our code is publicly available\nat https://github.com/thu-social-network-research-group/InvestAlign.", "AI": {"tldr": "提出InvestAlign框架，通过理论解决方案生成高质量SFT数据集，解决LLMs在投资者从众行为中的对齐问题，避免真实数据的高成本和隐私风险。", "motivation": "解决行为金融中LLMs与投资者从众行为对齐的挑战，尤其是真实用户数据稀缺导致的SFT困难。", "method": "利用理论解决方案构建高质量SFT数据集，开发InvestAgent（基于InvestAlign微调的LLM代理）。", "result": "InvestAlign生成的数据训练LLMs参数收敛更快，InvestAgent在简单和复杂投资问题中更接近真实用户数据。", "conclusion": "InvestAlign是解决复杂投资问题和LLMs对齐的有前景方法，代码已开源。"}}
{"id": "2507.06517", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06517", "abs": "https://arxiv.org/abs/2507.06517", "authors": ["Zicong Tang", "Shi Luohe", "Zuchao Li", "Baoyuan Qi", "Guoming Liu", "Lefei Zhang", "Ping Wang"], "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers", "comment": "Accepted by ACL 2025 main", "summary": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.", "AI": {"tldr": "SpindleKV是一种新型的KV缓存减少方法，通过平衡浅层和深层的处理策略，有效减少了LLMs推理时的内存消耗。", "motivation": "LLMs推理时KV缓存的内存消耗问题日益突出，现有方法在浅层缓存减少上效果不足。", "method": "SpindleKV结合了基于注意力权重的深层缓存淘汰方法和基于代码本的浅层缓存替换方法，解决了GQA问题。", "result": "实验表明，SpindleKV在减少KV缓存的同时保持了模型性能，优于基线方法。", "conclusion": "SpindleKV为LLMs推理系统的内存优化提供了有效解决方案。"}}
{"id": "2507.06713", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06713", "abs": "https://arxiv.org/abs/2507.06713", "authors": ["Saif Ahmad", "Seifeddine Ben Elghali", "Hafiz Ahmed"], "title": "Coordinated Fast Frequency Regulation in Dynamic Virtual Power Plants via Disturbance Estimation", "comment": null, "summary": "In the context of dynamic virtual power plants (DVPPs), the integration of\nfrequency containment reserve (FCR) and fast frequency control (FFC) enabled\nvia local compensation of power imbalance represents a significant advancement\nin decentralized frequency regulation. However, they still have to cope with\nthe limited power and energy capacities associated with commonly available\nstorage solutions. This work combines a disturbance estimation based\ndecentralized local control with distributed imbalance compensation in the\nevent of local shortfall. The layered architecture facilitates fast local\ncorrections in power setpoints while enabling coordination between neighbouring\nDVPP nodes to leverage the aggregated capacity, ensuring scalable and efficient\noperation suitable for renewable-heavy future grids. The proposed approach is\nvalidated on an illustrative 4-bus system with a high percentage of renewables.", "AI": {"tldr": "提出了一种结合分布式局部控制和分布式不平衡补偿的分层架构，用于动态虚拟电厂（DVPP）中的频率调节，解决了储能容量限制问题。", "motivation": "解决动态虚拟电厂中频率调节的储能容量限制问题，提高可再生能源为主电网的可扩展性和效率。", "method": "采用基于扰动估计的分布式局部控制与分布式不平衡补偿相结合的分层架构。", "result": "在含高比例可再生能源的4总线系统中验证了方法的有效性。", "conclusion": "该方法为未来可再生能源为主的电网提供了可扩展且高效的频率调节解决方案。"}}
{"id": "2507.06710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06710", "abs": "https://arxiv.org/abs/2507.06710", "authors": ["Zhenyang Liu", "Yikai Wang", "Kuanning Wang", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "title": "Spatial-Temporal Aware Visuomotor Diffusion Policy Learning", "comment": null, "summary": "Visual imitation learning is effective for robots to learn versatile tasks.\nHowever, many existing methods rely on behavior cloning with supervised\nhistorical trajectories, limiting their 3D spatial and 4D spatiotemporal\nawareness. Consequently, these methods struggle to capture the 3D structures\nand 4D spatiotemporal relationships necessary for real-world deployment. In\nthis work, we propose 4D Diffusion Policy (DP4), a novel visual imitation\nlearning method that incorporates spatiotemporal awareness into diffusion-based\npolicies. Unlike traditional approaches that rely on trajectory cloning, DP4\nleverages a dynamic Gaussian world model to guide the learning of 3D spatial\nand 4D spatiotemporal perceptions from interactive environments. Our method\nconstructs the current 3D scene from a single-view RGB-D observation and\npredicts the future 3D scene, optimizing trajectory generation by explicitly\nmodeling both spatial and temporal dependencies. Extensive experiments across\n17 simulation tasks with 173 variants and 3 real-world robotic tasks\ndemonstrate that the 4D Diffusion Policy (DP4) outperforms baseline methods,\nimproving the average simulation task success rate by 16.4% (Adroit), 14%\n(DexArt), and 6.45% (RLBench), and the average real-world robotic task success\nrate by 8.6%.", "AI": {"tldr": "提出了一种名为4D Diffusion Policy (DP4)的新方法，通过引入时空感知改进视觉模仿学习，显著提升了任务成功率。", "motivation": "现有视觉模仿学习方法依赖历史轨迹的行为克隆，缺乏3D空间和4D时空感知能力，限制了实际应用效果。", "method": "DP4利用动态高斯世界模型从交互环境中学习3D空间和4D时空感知，通过单视角RGB-D观测构建当前3D场景并预测未来场景，优化轨迹生成。", "result": "在17个仿真任务和3个真实机器人任务中，DP4表现优于基线方法，仿真任务成功率提升16.4%、14%和6.45%，真实任务提升8.6%。", "conclusion": "DP4通过时空感知显著提升了视觉模仿学习的性能，适用于复杂任务的实际部署。"}}
{"id": "2507.06937", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.06937", "abs": "https://arxiv.org/abs/2507.06937", "authors": ["Yuli Wang", "Victoria R. Shi", "Liwei Zhou", "Richard Chin", "Yuwei Dai", "Yuanyun Hu", "Cheng-Yi Li", "Haoyue Guan", "Jiashu Cheng", "Yu Sun", "Cheng Ting Lin", "Ihab Kamel", "Premal Trivedi", "Pamela Johnson", "John Eng", "Harrison Bai"], "title": "Dataset and Benchmark for Enhancing Critical Retained Foreign Object Detection", "comment": null, "summary": "Critical retained foreign objects (RFOs), including surgical instruments like\nsponges and needles, pose serious patient safety risks and carry significant\nfinancial and legal implications for healthcare institutions. Detecting\ncritical RFOs using artificial intelligence remains challenging due to their\nrarity and the limited availability of chest X-ray datasets that specifically\nfeature critical RFOs cases. Existing datasets only contain non-critical RFOs,\nlike necklace or zipper, further limiting their utility for developing\nclinically impactful detection algorithms. To address these limitations, we\nintroduce \"Hopkins RFOs Bench\", the first and largest dataset of its kind,\ncontaining 144 chest X-ray images of critical RFO cases collected over 18 years\nfrom the Johns Hopkins Health System. Using this dataset, we benchmark several\nstate-of-the-art object detection models, highlighting the need for enhanced\ndetection methodologies for critical RFO cases. Recognizing data scarcity\nchallenges, we further explore image synthetic methods to bridge this gap. We\nevaluate two advanced synthetic image methods, DeepDRR-RFO, a physics-based\nmethod, and RoentGen-RFO, a diffusion-based method, for creating realistic\nradiographs featuring critical RFOs. Our comprehensive analysis identifies the\nstrengths and limitations of each synthetic method, providing insights into\neffectively utilizing synthetic data to enhance model training. The Hopkins\nRFOs Bench and our findings significantly advance the development of reliable,\ngeneralizable AI-driven solutions for detecting critical RFOs in clinical chest\nX-rays.", "AI": {"tldr": "论文介绍了首个针对关键遗留异物（RFOs）的胸部X射线数据集Hopkins RFOs Bench，并评估了多种目标检测模型和合成图像方法，以提升检测算法的性能。", "motivation": "关键RFOs对患者安全构成严重威胁，但现有数据集仅包含非关键RFOs，限制了检测算法的开发。", "method": "构建了包含144张关键RFOs胸部X射线图像的数据集，并测试了多种目标检测模型和两种合成图像方法（DeepDRR-RFO和RoentGen-RFO）。", "result": "Hopkins RFOs Bench数据集填补了关键RFOs检测的数据空白，并分析了合成图像方法的优缺点。", "conclusion": "该研究为开发可靠的关键RFOs检测AI解决方案提供了重要数据和见解。"}}
{"id": "2507.06486", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06486", "abs": "https://arxiv.org/abs/2507.06486", "authors": ["Yuechen Xie", "Haobo Jiang", "Jin Xie"], "title": "Mask6D: Masked Pose Priors For 6D Object Pose Estimation", "comment": "Accepted at ICASSP 2024. 4 figures, 3 tables", "summary": "Robust 6D object pose estimation in cluttered or occluded conditions using\nmonocular RGB images remains a challenging task. One reason is that current\npose estimation networks struggle to extract discriminative, pose-aware\nfeatures using 2D feature backbones, especially when the available RGB\ninformation is limited due to target occlusion in cluttered scenes. To mitigate\nthis, we propose a novel pose estimation-specific pre-training strategy named\nMask6D. Our approach incorporates pose-aware 2D-3D correspondence maps and\nvisible mask maps as additional modal information, which is combined with RGB\nimages for the reconstruction-based model pre-training. Essentially, this 2D-3D\ncorrespondence maps a transformed 3D object model to 2D pixels, reflecting the\npose information of the target in camera coordinate system. Meanwhile, the\nintegrated visible mask map can effectively guide our model to disregard\ncluttered background information. In addition, an object-focused pre-training\nloss function is designed to further facilitate our network to remove the\nbackground interference. Finally, we fine-tune our pre-trained pose prior-aware\nnetwork via conventional pose training strategy to realize the reliable pose\nprediction. Extensive experiments verify that our method outperforms previous\nend-to-end pose estimation methods.", "AI": {"tldr": "提出了一种名为Mask6D的新型预训练策略，通过结合2D-3D对应图和可见掩码图，提升了单目RGB图像在复杂或遮挡条件下的6D物体姿态估计性能。", "motivation": "当前姿态估计网络在复杂场景中难以提取具有区分性的姿态感知特征，尤其是在目标被遮挡时RGB信息有限的情况下。", "method": "引入2D-3D对应图和可见掩码图作为额外模态信息，结合RGB图像进行基于重建的模型预训练，并设计了专注于物体的预训练损失函数。", "result": "实验表明，该方法优于以往的端到端姿态估计方法。", "conclusion": "Mask6D通过预训练策略显著提升了姿态估计的鲁棒性和准确性。"}}
{"id": "2507.06623", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06623", "abs": "https://arxiv.org/abs/2507.06623", "authors": ["James Stewart-Evans", "Emma Wilson", "Tessa Langley", "Andrew Prayle", "Angela Hands", "Karen Exley", "Jo Leonardi-Bee"], "title": "Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review", "comment": "44 pages, 4 figures", "summary": "The data extraction stages of reviews are resource-intensive, and researchers\nmay seek to expediate data extraction using online (large language models) LLMs\nand review protocols. Claude 3.5 Sonnet was used to trial two approaches that\nused a review protocol to prompt data extraction from 10 evidence sources\nincluded in a case study scoping review. A protocol-based approach was also\nused to review extracted data. Limited performance evaluation was undertaken\nwhich found high accuracy for the two extraction approaches (83.3% and 100%)\nwhen extracting simple, well-defined citation details; accuracy was lower (9.6%\nand 15.8%) when extracting more complex, subjective data items. Considering all\ndata items, both approaches had precision >90% but low recall (<25%) and F1\nscores (<40%). The context of a complex scoping review, open response types and\nmethodological approach likely impacted performance due to missed and\nmisattributed data. LLM feedback considered the baseline extraction accurate\nand suggested minor amendments: four of 15 (26.7%) to citation details and 8 of\n38 (21.1%) to key findings data items were considered to potentially add value.\nHowever, when repeating the process with a dataset featuring deliberate errors,\nonly 2 of 39 (5%) errors were detected. Review-protocol-based methods used for\nexpediency require more robust performance evaluation across a range of LLMs\nand review contexts with comparison to conventional prompt engineering\napproaches. We recommend researchers evaluate and report LLM performance if\nusing them similarly to conduct data extraction or review extracted data. LLM\nfeedback contributed to protocol adaptation and may assist future review\nprotocol drafting.", "AI": {"tldr": "论文探讨了使用Claude 3.5 Sonnet和审查协议从证据源中提取数据的两种方法，发现其在简单数据上表现良好，但在复杂数据上准确性较低。", "motivation": "数据提取阶段资源密集，研究者希望通过在线大语言模型（LLM）和审查协议加速这一过程。", "method": "使用Claude 3.5 Sonnet和审查协议对10个证据源进行数据提取，并对提取的数据进行审查。", "result": "简单数据提取准确率高（83.3%和100%），复杂数据提取准确率低（9.6%和15.8%）。总体精度>90%，但召回率和F1分数较低。", "conclusion": "需要更全面的性能评估，建议研究者在类似场景中评估并报告LLM表现，同时LLM反馈有助于协议改进。"}}
{"id": "2507.06539", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06539", "abs": "https://arxiv.org/abs/2507.06539", "authors": ["Yunyang Cao", "Yanjun Li", "Silong Dai"], "title": "Large Language Model for Extracting Complex Contract Information in Industrial Scenes", "comment": null, "summary": "This paper proposes a high-quality dataset construction method for complex\ncontract information extraction tasks in industrial scenarios and fine-tunes a\nlarge language model based on this dataset. Firstly, cluster analysis is\nperformed on industrial contract texts, and GPT-4 and GPT-3.5 are used to\nextract key information from the original contract data, obtaining high-quality\ndata annotations. Secondly, data augmentation is achieved by constructing new\ntexts, and GPT-3.5 generates unstructured contract texts from randomly combined\nkeywords, improving model robustness. Finally, the large language model is\nfine-tuned based on the high-quality dataset. Experimental results show that\nthe model achieves excellent overall performance while ensuring high field\nrecall and precision and considering parsing efficiency. LoRA, data balancing,\nand data augmentation effectively enhance model accuracy and robustness. The\nproposed method provides a novel and efficient solution for industrial contract\ninformation extraction tasks.", "AI": {"tldr": "提出了一种工业场景中复杂合同信息提取任务的高质量数据集构建方法，并基于此微调大语言模型。", "motivation": "工业合同信息提取任务需要高质量数据集，传统方法难以满足需求。", "method": "通过聚类分析、GPT-4/3.5标注关键信息、数据增强（生成新文本）构建高质量数据集，并微调大语言模型。", "result": "模型在保证高召回率和精度的同时，解析效率优秀；LoRA、数据平衡和数据增强提升了准确性和鲁棒性。", "conclusion": "该方法为工业合同信息提取任务提供了新颖高效的解决方案。"}}
{"id": "2507.06736", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06736", "abs": "https://arxiv.org/abs/2507.06736", "authors": ["Jonas Schweiger", "Ruaridh Macdonald"], "title": "Techno-economic analysis of decarbonized backup power systems using scenario-based stochastic optimization", "comment": "30 pages, 2 tables, 13 figures", "summary": "In the context of growing concerns about power disruptions, grid reliability\nand the need for decarbonization, this study evaluates a broad range of clean\nbackup power systems (BPSs) to replace traditional emergency diesel generators.\nA scenario-based stochastic optimization framework using actual load profiles\nand outage probabilities is proposed to assess the most promising options from\na pool of 27 technologies. This framework allows a comparison of\ncost-effectiveness and environmental impact of individual technologies and\nhybrid BPSs across various scenarios. The results highlight the trade-off\nbetween total annual system cost and emissions. Significant emission reductions\ncan be achieved at moderate cost increases but deep decarbonization levels\nincur higher costs. Primary and secondary batteries are included in optimal\nclean fuel-based systems across all decarbonization levels, combining\ncost-effective power delivery and long-term storage benefits. The findings\nhighlight the often-overlooked importance of fuel replacement on both emissions\nand costs. Among the assessed technologies, ammonia generators and hydrogen\nfuel cells combined with secondary iron-air batteries emerge as cost-effective\nsolutions for achieving decarbonization goals. To ensure a broad range of\napplicability, the study outlines the impact of emergency fuel purchases,\nvarying demand patterns and demand response options on the optimal BPS. The\nresearch findings are valuable for optimizing the design of clean BPSs to\neconomically meet the needs of many facility types and decarbonization targets.", "AI": {"tldr": "研究评估了多种清洁备用电源系统（BPSs）以替代传统柴油发电机，提出了一种基于场景的随机优化框架，比较了27种技术的成本效益和环境效益。结果表明，氨发电机和氢燃料电池结合铁空气电池是经济有效的脱碳解决方案。", "motivation": "随着对电力中断、电网可靠性和脱碳需求的关注增加，研究旨在评估清洁备用电源系统以替代传统柴油发电机。", "method": "采用基于场景的随机优化框架，结合实际负载曲线和停电概率，评估27种技术的成本效益和环境效益。", "result": "研究发现，氨发电机和氢燃料电池结合铁空气电池是经济有效的脱碳解决方案，同时强调了燃料替代对排放和成本的重要性。", "conclusion": "研究为优化清洁备用电源系统设计提供了有价值的见解，以经济高效地满足多种设施类型和脱碳目标的需求。"}}
{"id": "2507.06747", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06747", "abs": "https://arxiv.org/abs/2507.06747", "authors": ["Daojie Peng", "Jiahang Cao", "Qiang Zhang", "Jun Ma"], "title": "LOVON: Legged Open-Vocabulary Object Navigator", "comment": "9 pages, 10 figures; Project Page:\n  https://daojiepeng.github.io/LOVON/", "summary": "Object navigation in open-world environments remains a formidable and\npervasive challenge for robotic systems, particularly when it comes to\nexecuting long-horizon tasks that require both open-world object detection and\nhigh-level task planning. Traditional methods often struggle to integrate these\ncomponents effectively, and this limits their capability to deal with complex,\nlong-range navigation missions. In this paper, we propose LOVON, a novel\nframework that integrates large language models (LLMs) for hierarchical task\nplanning with open-vocabulary visual detection models, tailored for effective\nlong-range object navigation in dynamic, unstructured environments. To tackle\nreal-world challenges including visual jittering, blind zones, and temporary\ntarget loss, we design dedicated solutions such as Laplacian Variance Filtering\nfor visual stabilization. We also develop a functional execution logic for the\nrobot that guarantees LOVON's capabilities in autonomous navigation, task\nadaptation, and robust task completion. Extensive evaluations demonstrate the\nsuccessful completion of long-sequence tasks involving real-time detection,\nsearch, and navigation toward open-vocabulary dynamic targets. Furthermore,\nreal-world experiments across different legged robots (Unitree Go2, B2, and\nH1-2) showcase the compatibility and appealing plug-and-play feature of LOVON.", "AI": {"tldr": "LOVON是一个结合大型语言模型（LLMs）和开放词汇视觉检测模型的新框架，用于动态非结构化环境中的长距离物体导航。", "motivation": "解决开放世界环境中机器人系统在长时程任务中整合物体检测和高级任务规划的挑战。", "method": "提出LOVON框架，结合LLMs进行分层任务规划和开放词汇视觉检测，并设计视觉稳定化和功能执行逻辑。", "result": "实验证明LOVON能成功完成涉及实时检测、搜索和导航的长序列任务，并在不同机器人上展示兼容性。", "conclusion": "LOVON在动态环境中实现了高效的物体导航，具有广泛的适用性和鲁棒性。"}}
{"id": "2507.06955", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06955", "abs": "https://arxiv.org/abs/2507.06955", "authors": ["Kaveh Moradkhani", "R Jarrett Rushmore", "Sylvain Bouix"], "title": "SimCortex: Collision-free Simultaneous Cortical Surfaces Reconstruction", "comment": null, "summary": "Accurate cortical surface reconstruction from magnetic resonance imaging\n(MRI) data is crucial for reliable neuroanatomical analyses. Current methods\nhave to contend with complex cortical geometries, strict topological\nrequirements, and often produce surfaces with overlaps, self-intersections, and\ntopological defects. To overcome these shortcomings, we introduce SimCortex, a\ndeep learning framework that simultaneously reconstructs all brain surfaces\n(left/right white-matter and pial) from T1-weighted(T1w) MRI volumes while\npreserving topological properties. Our method first segments the T1w image into\na nine-class tissue label map. From these segmentations, we generate\nsubject-specific, collision-free initial surface meshes. These surfaces serve\nas precise initializations for subsequent multiscale diffeomorphic\ndeformations. Employing stationary velocity fields (SVFs) integrated via\nscaling-and-squaring, our approach ensures smooth, topology-preserving\ntransformations with significantly reduced surface collisions and\nself-intersections. Evaluations on standard datasets demonstrate that SimCortex\ndramatically reduces surface overlaps and self-intersections, surpassing\ncurrent methods while maintaining state-of-the-art geometric accuracy.", "AI": {"tldr": "SimCortex是一种深度学习框架，用于从T1加权MRI数据中重建大脑皮层表面，解决了现有方法中的拓扑缺陷和表面重叠问题。", "motivation": "现有方法在重建复杂皮层几何时存在拓扑缺陷和表面重叠问题，影响了神经解剖分析的可靠性。", "method": "SimCortex首先将T1w图像分割为九类组织标签图，生成无碰撞的初始表面网格，然后通过多尺度微分同胚变形进行优化，使用SVFs确保拓扑保持。", "result": "在标准数据集上的评估显示，SimCortex显著减少了表面重叠和自交，同时保持了最先进的几何精度。", "conclusion": "SimCortex提供了一种高效且拓扑保持的大脑皮层表面重建方法，优于现有技术。"}}
{"id": "2507.06510", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06510", "abs": "https://arxiv.org/abs/2507.06510", "authors": ["Yupeng Hu", "Changxing Ding", "Chang Sun", "Shaoli Huang", "Xiangmin Xu"], "title": "Bilateral Collaboration with Large Vision-Language Models for Open Vocabulary Human-Object Interaction Detection", "comment": "ICCV 2025", "summary": "Open vocabulary Human-Object Interaction (HOI) detection is a challenging\ntask that detects all <human, verb, object> triplets of interest in an image,\neven those that are not pre-defined in the training set. Existing approaches\ntypically rely on output features generated by large Vision-Language Models\n(VLMs) to enhance the generalization ability of interaction representations.\nHowever, the visual features produced by VLMs are holistic and coarse-grained,\nwhich contradicts the nature of detection tasks. To address this issue, we\npropose a novel Bilateral Collaboration framework for open vocabulary HOI\ndetection (BC-HOI). This framework includes an Attention Bias Guidance (ABG)\ncomponent, which guides the VLM to produce fine-grained instance-level\ninteraction features according to the attention bias provided by the HOI\ndetector. It also includes a Large Language Model (LLM)-based Supervision\nGuidance (LSG) component, which provides fine-grained token-level supervision\nfor the HOI detector by the LLM component of the VLM. LSG enhances the ability\nof ABG to generate high-quality attention bias. We conduct extensive\nexperiments on two popular benchmarks: HICO-DET and V-COCO, consistently\nachieving superior performance in the open vocabulary and closed settings. The\ncode will be released in Github.", "AI": {"tldr": "提出了一种双边协作框架（BC-HOI），用于开放词汇人-物交互检测，通过注意力偏差引导（ABG）和基于大语言模型的监督引导（LSG）提升性能。", "motivation": "现有方法依赖视觉-语言模型（VLM）生成的整体粗粒度特征，与检测任务的细粒度需求矛盾。", "method": "BC-HOI框架包含ABG和LSG组件，分别通过注意力偏差和细粒度监督提升交互特征质量。", "result": "在HICO-DET和V-COCO基准测试中表现优异，开放和封闭场景均优于现有方法。", "conclusion": "BC-HOI框架有效解决了开放词汇HOI检测中的细粒度特征问题，性能显著提升。"}}
{"id": "2507.06639", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06639", "abs": "https://arxiv.org/abs/2507.06639", "authors": ["Myungjang Pyeon", "Janghyeon Lee", "Minsoo Lee", "Juseung Yun", "Hwanil Choi", "Jonghyun Kim", "Jiwon Kim", "Yi Hu", "Jongseong Jang", "Soonyoung Lee"], "title": "EXAONE Path 2.0: Pathology Foundation Model with End-to-End Supervision", "comment": "EXAONE Path 2.0 technical report", "summary": "In digital pathology, whole-slide images (WSIs) are often difficult to handle\ndue to their gigapixel scale, so most approaches train patch encoders via\nself-supervised learning (SSL) and then aggregate the patch-level embeddings\nvia multiple instance learning (MIL) or slide encoders for downstream tasks.\nHowever, patch-level SSL may overlook complex domain-specific features that are\nessential for biomarker prediction, such as mutation status and molecular\ncharacteristics, as SSL methods rely only on basic augmentations selected for\nnatural image domains on small patch-level area. Moreover, SSL methods remain\nless data efficient than fully supervised approaches, requiring extensive\ncomputational resources and datasets to achieve competitive performance. To\naddress these limitations, we present EXAONE Path 2.0, a pathology foundation\nmodel that learns patch-level representations under direct slide-level\nsupervision. Using only 37k WSIs for training, EXAONE Path 2.0 achieves\nstate-of-the-art average performance across 10 biomarker prediction tasks,\ndemonstrating remarkable data efficiency.", "AI": {"tldr": "EXAONE Path 2.0提出了一种基于全幻灯片级别监督的病理学基础模型，解决了传统自监督学习方法在生物标志物预测中的局限性，仅用37k WSIs即达到领先性能。", "motivation": "传统自监督学习（SSL）方法在数字病理学中因依赖小区域补丁级训练和自然图像增强而忽略复杂领域特征，且数据效率低。", "method": "EXAONE Path 2.0通过直接在全幻灯片级别监督下学习补丁级表示，提升了数据效率和性能。", "result": "在10项生物标志物预测任务中达到最优平均性能，仅需37k WSIs训练。", "conclusion": "EXAONE Path 2.0显著提高了病理学任务的数据效率和预测性能，为领域提供了更高效的基础模型。"}}
{"id": "2507.06565", "categories": ["cs.CL", "cs.LG", "68T01, 60J10, 91D30, 05C82, 68T50, 68W20, 94A15", "I.2.7; I.2.11; G.3"], "pdf": "https://arxiv.org/pdf/2507.06565", "abs": "https://arxiv.org/abs/2507.06565", "authors": ["Juan B. Gutiérrez"], "title": "The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production", "comment": "27 pages, 3 figures, 4 tables, 1 algorithm, 28 references", "summary": "Large-language models turn writing into a live exchange between humans and\nsoftware. We capture this new medium with a discursive-network model that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. Broadening the focus from isolated hallucinations, we define\ninvalidation (any factual, logical, or structural breach) and show it follows\nfour hazards: drift from truth, self-repair, fresh fabrication, and external\ndetection. A general mathematical model of discursive networks is developed to\nprovide valuable insights: A network governed only by drift and self-repair\nstabilizes at a modest error rate; adding fabrication reproduces the high rates\nseen in current LLMs. Giving each false claim even a small chance of peer\nreview shifts the system to a truth-dominant state. We operationalize peer\nreview with the open-source \\emph{Flaws-of-Others (FOO) algorithm}: a\nconfigurable loop in which any set of agents critique one another while a\nharmoniser merges their verdicts. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nwiring imperfect ones into networks that keep each other honest.", "AI": {"tldr": "论文提出了一种将人类与大型语言模型（LLMs）视为平等节点的讨论网络模型，分析了四种无效性风险，并通过数学模型和开源算法展示了如何通过同行评审提升系统可靠性。", "motivation": "研究人类与LLMs交互中的无效性问题，探索如何通过网络化设计提升信息可靠性。", "method": "开发了讨论网络模型，定义四种无效性风险（漂移、自我修复、新编造、外部检测），并提出开源算法FOO实现同行评审。", "result": "网络仅依赖漂移和自我修复时，错误率较低；加入编造后错误率升高；引入同行评审可显著提升系统真实性。", "conclusion": "信息可靠性依赖于将不完美的模型网络化，通过相互监督保持诚实。"}}
{"id": "2507.06796", "categories": ["eess.SY", "cond-mat.mtrl-sci", "cs.SY", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.06796", "abs": "https://arxiv.org/abs/2507.06796", "authors": ["Matthias Derez", "Alexander Hoogsteyn", "Erik Delarue"], "title": "Optimisation of Electrolyser Operation: Integrating External Heat", "comment": null, "summary": "Integrating external heat into electrolysers can reduce the electrical power\ndemand for carbon-neutral hydrogen production. Efficient operation requires\ndetailed models that incorporate heat availability and its effect on startup\ncosts. This paper advances existing operational models by endogenously\nmodelling startup costs and direct heat integration, based on a piecewise\nlinear approximation of the electrochemical equations. We analyse the impact of\nlow- and high-temperature heat integration on the efficiency and profitability\nof hydrogen production for solid oxide and proton exchange membrane\nelectrolysis technologies.", "AI": {"tldr": "论文提出了一种整合外部热源到电解槽的方法，以减少碳中性制氢的电力需求，并通过内建模启动成本和直接热集成优化效率与盈利性。", "motivation": "为了减少碳中性制氢的电力需求，研究如何通过整合外部热源优化电解槽的运行效率与经济效益。", "method": "基于分段线性近似电化学方程，内建模启动成本和直接热集成，分析低温和高温热集成对制氢效率和盈利性的影响。", "result": "分析了低温和高温热集成对固体氧化物和质子交换膜电解技术的效率和盈利性的具体影响。", "conclusion": "整合外部热源可以有效降低电力需求，提升制氢效率和盈利性，为碳中性制氢提供优化方案。"}}
{"id": "2507.06750", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06750", "abs": "https://arxiv.org/abs/2507.06750", "authors": ["Tohid Kargar Tasooji", "Ramviyas Parasuraman"], "title": "Distributed Fault-Tolerant Multi-Robot Cooperative Localization in Adversarial Environments", "comment": "Accepted to IROS 2025 Conference", "summary": "In multi-robot systems (MRS), cooperative localization is a crucial task for\nenhancing system robustness and scalability, especially in GPS-denied or\ncommunication-limited environments. However, adversarial attacks, such as\nsensor manipulation, and communication jamming, pose significant challenges to\nthe performance of traditional localization methods. In this paper, we propose\na novel distributed fault-tolerant cooperative localization framework to\nenhance resilience against sensor and communication disruptions in adversarial\nenvironments. We introduce an adaptive event-triggered communication strategy\nthat dynamically adjusts communication thresholds based on real-time sensing\nand communication quality. This strategy ensures optimal performance even in\nthe presence of sensor degradation or communication failure. Furthermore, we\nconduct a rigorous analysis of the convergence and stability properties of the\nproposed algorithm, demonstrating its resilience against bounded adversarial\nzones and maintaining accurate state estimation. Robotarium-based experiment\nresults show that our proposed algorithm significantly outperforms traditional\nmethods in terms of localization accuracy and communication efficiency,\nparticularly in adversarial settings. Our approach offers improved scalability,\nreliability, and fault tolerance for MRS, making it suitable for large-scale\ndeployments in real-world, challenging environments.", "AI": {"tldr": "提出了一种分布式容错协同定位框架，通过自适应事件触发通信策略提升多机器人系统在对抗环境中的鲁棒性。", "motivation": "在GPS缺失或通信受限的环境中，传统定位方法易受对抗攻击（如传感器操纵和通信干扰）影响，亟需提升系统的容错能力。", "method": "采用自适应事件触发通信策略，动态调整通信阈值，并结合实时感知和通信质量优化性能。", "result": "实验证明该算法在对抗环境中显著优于传统方法，定位精度和通信效率更高。", "conclusion": "该框架提高了多机器人系统的可扩展性、可靠性和容错性，适用于现实世界中的大规模部署。"}}
{"id": "2507.07011", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07011", "abs": "https://arxiv.org/abs/2507.07011", "authors": ["Daniel Onah", "Ravish Desai"], "title": "Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning", "comment": "9 pages, 14 figures, 4 tables. To be submitted to a conference", "summary": "In recent years, deep learning has shown great promise in the automated\ndetection and classification of brain tumors from MRI images. However,\nachieving high accuracy and computational efficiency remains a challenge. In\nthis research, we propose Deep Brain Net, a novel deep learning system designed\nto optimize performance in the detection of brain tumors. The model integrates\nthe strengths of two advanced neural network architectures which are\nEfficientNetB0 and ResNet50, combined with transfer learning to improve\ngeneralization and reduce training time. The EfficientNetB0 architecture\nenhances model efficiency by utilizing mobile inverted bottleneck blocks, which\nincorporate depth wise separable convolutions. This design significantly\nreduces the number of parameters and computational cost while preserving the\nability of models to learn complex feature representations. The ResNet50\narchitecture, pre trained on large scale datasets like ImageNet, is fine tuned\nfor brain tumor classification. Its use of residual connections allows for\ntraining deeper networks by mitigating the vanishing gradient problem and\navoiding performance degradation. The integration of these components ensures\nthat the proposed system is both computationally efficient and highly accurate.\nExtensive experiments performed on publicly available MRI datasets demonstrate\nthat Deep Brain Net consistently outperforms existing state of the art methods\nin terms of classification accuracy, precision, recall, and computational\nefficiency. The result is an accuracy of 88 percent, a weighted F1 score of\n88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates\nthe robustness and clinical potential of Deep Brain Net in assisting\nradiologists with brain tumor diagnosis.", "AI": {"tldr": "提出了一种名为Deep Brain Net的新型深度学习系统，结合EfficientNetB0和ResNet50架构，通过迁移学习优化脑肿瘤检测性能，显著提升了分类准确率和计算效率。", "motivation": "尽管深度学习在脑肿瘤MRI图像的自动检测和分类中表现出潜力，但高准确率和计算效率仍是挑战。", "method": "整合EfficientNetB0（利用移动倒置瓶颈块减少参数和计算成本）和ResNet50（通过残差连接训练更深网络），并结合迁移学习。", "result": "在公开MRI数据集上，Deep Brain Net在分类准确率（88%）、加权F1分数（88.75%）和宏AUC ROC分数（98.17%）上优于现有方法。", "conclusion": "Deep Brain Net展示了在辅助放射科医生进行脑肿瘤诊断中的鲁棒性和临床潜力。"}}
{"id": "2507.06513", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06513", "abs": "https://arxiv.org/abs/2507.06513", "authors": ["Yaoqi Huang", "Julie Stephany Berrio", "Mao Shan", "Stewart Worrall"], "title": "What Demands Attention in Urban Street Scenes? From Scene Understanding towards Road Safety: A Survey of Vision-driven Datasets and Studies", "comment": "45 pages, 52 figures, 2 large tables (divided into 5), 73 datatsets,\n  35 tasks", "summary": "Advances in vision-based sensors and computer vision algorithms have\nsignificantly improved the analysis and understanding of traffic scenarios. To\nfacilitate the use of these improvements for road safety, this survey\nsystematically categorizes the critical elements that demand attention in\ntraffic scenarios and comprehensively analyzes available vision-driven tasks\nand datasets. Compared to existing surveys that focus on isolated domains, our\ntaxonomy categorizes attention-worthy traffic entities into two main groups\nthat are anomalies and normal but critical entities, integrating ten categories\nand twenty subclasses. It establishes connections between inherently related\nfields and provides a unified analytical framework. Our survey highlights the\nanalysis of 35 vision-driven tasks and comprehensive examinations and\nvisualizations of 73 available datasets based on the proposed taxonomy. The\ncross-domain investigation covers the pros and cons of each benchmark with the\naim of providing information on standards unification and resource\noptimization. Our article concludes with a systematic discussion of the\nexisting weaknesses, underlining the potential effects and promising solutions\nfrom various perspectives. The integrated taxonomy, comprehensive analysis, and\nrecapitulatory tables serve as valuable contributions to this rapidly evolving\nfield by providing researchers with a holistic overview, guiding strategic\nresource selection, and highlighting critical research gaps.", "AI": {"tldr": "该论文综述了基于视觉的交通场景分析，提出了一种分类方法，整合了35个视觉驱动任务和73个数据集，并探讨了现有弱点和潜在解决方案。", "motivation": "通过系统分类和综合分析，促进基于视觉的交通场景分析在道路安全中的应用。", "method": "提出了一种分类法，将交通实体分为异常和正常但关键的两大类，整合了10个类别和20个子类，并分析了35个视觉驱动任务和73个数据集。", "result": "建立了统一的分类框架，提供了资源优化和标准统一的信息，并总结了现有弱点和潜在解决方案。", "conclusion": "该综述为研究者提供了全面的概览，指导资源选择，并突出了关键研究空白。"}}
{"id": "2507.06654", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06654", "abs": "https://arxiv.org/abs/2507.06654", "authors": ["Naoya Sogi", "Takashi Shibata", "Makoto Terao", "Masanori Suganuma", "Takayuki Okatani"], "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval", "comment": "IJCAI 2025. Code: https://github.com/NEC-N-SOGI/msdpp", "summary": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp.", "AI": {"tldr": "本文提出了一种名为CDR-CA的新任务，通过多源DPP方法优化多属性多样性，以适应不同应用场景。", "motivation": "传统结果多样化方法仅关注图像外观多样性，而忽略了应用场景对多样性需求的变化，限制了其应用范围。", "method": "提出多源DPP方法，扩展DPP至多源，并引入切线归一化以反映上下文。", "result": "实验证明该方法在优化多属性多样性方面有效。", "conclusion": "CDR-CA任务及多源DPP方法为结果多样化提供了更灵活的解决方案。"}}
{"id": "2507.06571", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06571", "abs": "https://arxiv.org/abs/2507.06571", "authors": ["Srihari K B", "Pushpak Bhattacharyya"], "title": "Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis", "comment": null, "summary": "We propose a unified food-domain QA framework that combines a large-scale\nmultimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000\nrecipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate\n40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint\nfine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves\nBERTScore by 16.2\\%, reduces FID by 37.8\\%, and boosts CLIP alignment by\n31.1\\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\\% to 7.3\\%) and\nLLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid\nretrieval-generation strategy achieves 94.1\\% accurate image reuse and 85\\%\nadequacy in synthesis. Our results demonstrate that structured knowledge and\nmultimodal generation together enhance reliability and diversity in food QA.", "AI": {"tldr": "提出了一种结合多模态知识图谱（MMKG）与生成式AI的食品领域QA框架，显著提升了性能指标。", "motivation": "通过结合结构化知识和多模态生成，提高食品问答的可靠性和多样性。", "method": "构建包含13,000食谱、3,000食材的MMKG，生成40,000 QA对，联合微调LLaMA和Stable Diffusion。", "result": "BERTScore提升16.2%，FID降低37.8%，CLIP对齐提升31.1%，图像重用准确率94.1%。", "conclusion": "结构化知识与多模态生成结合显著提升了食品QA的性能和多样性。"}}
{"id": "2507.06890", "categories": ["eess.SY", "cs.AI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06890", "abs": "https://arxiv.org/abs/2507.06890", "authors": ["Yifan Wang"], "title": "A Single-Point Measurement Framework for Robust Cyber-Attack Diagnosis in Smart Microgrids Using Dual Fractional-Order Feature Analysis", "comment": "8 pages, 10 figures", "summary": "Cyber-attacks jeopardize the safe operation of smart microgrids. At the same\ntime, existing diagnostic methods either depend on expensive multi-point\ninstrumentation or stringent modelling assumptions that are untenable under\nsingle-sensor constraints. This paper proposes a Fractional-Order\nMemory-Enhanced Attack-Diagnosis Scheme (FO-MADS) that achieves low-latency\nfault localisation and cyber-attack detection using only one VPQ\n(Voltage-Power-Reactive-power) sensor. FO-MADS first constructs a dual\nfractional-order feature library by jointly applying Caputo and\nGr\\\"unwald-Letnikov derivatives, thereby amplifying micro-perturbations and\nslow drifts in the VPQ signal. A two-stage hierarchical classifier then\npinpoints the affected inverter and isolates the faulty IGBT switch,\neffectively alleviating class imbalance. Robustness is further strengthened\nthrough Progressive Memory-Replay Adversarial Training (PMR-AT), whose\nattack-aware loss is dynamically re-weighted via Online Hard Example Mining\n(OHEM) to prioritise the most challenging samples. Experiments on a\nfour-inverter microgrid testbed comprising 1 normal and 24 fault classes under\nfour attack scenarios demonstrate diagnostic accuracies of 96.6 % (bias), 94.0\n% (noise), 92.8 % (data replacement), and 95.7 % (replay), while sustaining\n96.7 % under attack-free conditions. These results establish FO-MADS as a\ncost-effective and readily deployable solution that markedly enhances the\ncyber-physical resilience of smart microgrids.", "AI": {"tldr": "提出了一种基于单传感器的分数阶记忆增强攻击诊断方案（FO-MADS），用于智能微电网的低延迟故障定位和网络攻击检测。", "motivation": "智能微电网面临网络攻击威胁，现有诊断方法依赖昂贵设备或严格假设，不适用于单传感器场景。", "method": "采用Caputo和Grünwald-Letnikov导数构建双分数阶特征库，结合两阶段分层分类器和渐进记忆重放对抗训练（PMR-AT）。", "result": "在四种攻击场景下，诊断准确率分别为96.6%（偏置）、94.0%（噪声）、92.8%（数据替换）和95.7%（重放），无攻击时达96.7%。", "conclusion": "FO-MADS是一种成本低、易部署的方案，显著提升了智能微电网的网络安全韧性。"}}
{"id": "2507.06787", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.06787", "abs": "https://arxiv.org/abs/2507.06787", "authors": ["Sean Smith", "Emmanuel Witrant", "Ya-Jun Pan"], "title": "Stream Function-Based Navigation for Complex Quadcopter Obstacle Avoidance", "comment": null, "summary": "This article presents a novel stream function-based navigational control\nsystem for obstacle avoidance, where obstacles are represented as\ntwo-dimensional (2D) rigid surfaces in inviscid, incompressible flows. The\napproach leverages the vortex panel method (VPM) and incorporates safety\nmargins to control the stream function and flow properties around virtual\nsurfaces, enabling navigation in complex, partially observed environments using\nreal-time sensing. To address the limitations of the VPM in managing relative\ndistance and avoiding rapidly accelerating obstacles at close proximity, the\nsystem integrates a model predictive controller (MPC) based on higher-order\ncontrol barrier functions (HOCBF). This integration incorporates VPM trajectory\ngeneration, state estimation, and constraint handling into a receding-horizon\noptimization problem. The 2D rigid surfaces are enclosed using minimum bounding\nellipses (MBEs), while an adaptive Kalman filter (AKF) captures and predicts\nobstacle dynamics, propagating these estimates into the MPC-HOCBF for rapid\navoidance maneuvers. Evaluation is conducted using a PX4-powered Clover drone\nGazebo simulator and real-time experiments involving a COEX Clover quadcopter\nequipped with a 360 degree LiDAR sensor.", "AI": {"tldr": "提出了一种基于流函数的导航控制系统，用于避障，结合涡流面板法和模型预测控制，实现了复杂环境中的实时避障。", "motivation": "解决在部分观测环境中避障的挑战，尤其是处理快速移动障碍物时的局限性。", "method": "结合涡流面板法（VPM）和基于高阶控制屏障函数（HOCBF）的模型预测控制器（MPC），利用最小包围椭圆（MBE）和自适应卡尔曼滤波（AKF）处理障碍物动态。", "result": "通过仿真和真实实验验证了系统的有效性，实现了复杂环境中的实时避障。", "conclusion": "该方法在复杂环境中表现出高效的避障能力，尤其适用于动态障碍物场景。"}}
{"id": "2507.06593", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.06593", "abs": "https://arxiv.org/abs/2507.06593", "authors": ["Qianyu Zhang", "Bolun Zheng", "Hangjia Pan", "Lingyu Zhu", "Zunjie Zhu", "Zongpeng Li", "Shiqi Wang"], "title": "Capturing Stable HDR Videos Using a Dual-Camera System", "comment": null, "summary": "In HDR video reconstruction, exposure fluctuations in reference images from\nalternating exposure methods often result in flickering. To address this issue,\nwe propose a dual-camera system (DCS) for HDR video acquisition, where one\ncamera is assigned to capture consistent reference sequences, while the other\nis assigned to capture non-reference sequences for information supplementation.\nTo tackle the challenges posed by video data, we introduce an exposure-adaptive\nfusion network (EAFNet) to achieve more robust results. EAFNet introduced a\npre-alignment subnetwork to explore the influence of exposure, selectively\nemphasizing the valuable features across different exposure levels. Then, the\nenhanced features are fused by the asymmetric cross-feature fusion subnetwork,\nwhich explores reference-dominated attention maps to improve image fusion by\naligning cross-scale features and performing cross-feature fusion. Finally, the\nreconstruction subnetwork adopts a DWT-based multiscale architecture to reduce\nghosting artifacts and refine features at different resolutions. Extensive\nexperimental evaluations demonstrate that the proposed method achieves\nstate-of-the-art performance on different datasets, validating the great\npotential of the DCS in HDR video reconstruction. The codes and data captured\nby DCS will be available at https://github.com/zqqqyu/DCS.", "AI": {"tldr": "提出了一种双摄像头系统（DCS）和曝光自适应融合网络（EAFNet）来解决HDR视频重建中的闪烁问题，通过参考和非参考序列的融合实现更鲁棒的结果。", "motivation": "HDR视频重建中，交替曝光方法导致的曝光波动常引起闪烁问题，需要一种更稳定的解决方案。", "method": "使用双摄像头系统（DCS）分别捕获参考和非参考序列，并设计EAFNet网络进行特征对齐、融合和重建。", "result": "在多个数据集上实现了最先进的性能，验证了DCS在HDR视频重建中的潜力。", "conclusion": "提出的DCS和EAFNet方法有效解决了闪烁问题，为HDR视频重建提供了新思路。"}}
{"id": "2507.06523", "categories": ["cs.CV", "cs.CL", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.06523", "abs": "https://arxiv.org/abs/2507.06523", "authors": ["Liqiang Jing", "Viet Lai", "Seunghyun Yoon", "Trung Bui", "Xinya Du"], "title": "FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation", "comment": null, "summary": "Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable\nprogress in both Video-to-Text and Text-to-Video tasks. However, they often\nsuffer fro hallucinations, generating content that contradicts the visual\ninput. Existing evaluation methods are limited to one task (e.g., V2T) and also\nfail to assess hallucinations in open-ended, free-form responses. To address\nthis gap, we propose FIFA, a unified FaIthFulness evAluation framework that\nextracts comprehensive descriptive facts, models their semantic dependencies\nvia a Spatio-Temporal Semantic Dependency Graph, and verifies them using\nVideoQA models. We further introduce Post-Correction, a tool-based correction\nframework that revises hallucinated content. Extensive experiments demonstrate\nthat FIFA aligns more closely with human judgment than existing evaluation\nmethods, and that Post-Correction effectively improves factual consistency in\nboth text and video generation.", "AI": {"tldr": "论文提出FIFA框架，用于评估和改进视频多模态大语言模型（VideoMLLMs）中的幻觉问题，并通过实验验证其有效性。", "motivation": "现有评估方法局限于单一任务且无法评估开放自由形式响应中的幻觉问题，因此需要一种更全面的评估框架。", "method": "提出FIFA框架，提取描述性事实，构建时空语义依赖图，并通过VideoQA模型验证；同时引入基于工具的后修正框架Post-Correction。", "result": "FIFA比现有评估方法更符合人类判断，Post-Correction有效提高了文本和视频生成的事实一致性。", "conclusion": "FIFA和Post-Correction为解决VideoMLLMs中的幻觉问题提供了有效工具，提升了模型的事实一致性。"}}
{"id": "2507.06658", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06658", "abs": "https://arxiv.org/abs/2507.06658", "authors": ["Gennadii Iakovlev"], "title": "Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models", "comment": null, "summary": "This project introduces a new measure of elite polarization via actor and\nsubject detection using artificial intelligence. I identify when politicians\nmention one another in parliamentary speeches, note who is speaking and who is\nbeing addressed, and assess the emotional temperature behind these evaluations.\nThis maps how elites evaluate their various out-parties, allowing us to create\nan index of mutual out-party hostility, that is, elite polarization. While I\nanalyzed polarization data over the past four decades for the UK, and two\ndecades for Hungary and Italy, my approach lays the groundwork for a\ntwenty-year, EU-wide time-series dataset on elite polarization. I obtain the\nresults that can be aggregated by party and quarter. The resulting index\ndemonstrates a good face validity: it reacts to events such as electoral\ncampaigns, country- and party-level crises, and to parties losing and assuming\npower.", "AI": {"tldr": "该研究通过AI技术分析议会演讲中的政治人物互动，提出了一种新的精英极化测量方法，并构建了一个欧盟范围内的精英极化时间序列数据集。", "motivation": "研究旨在量化政治精英之间的相互敌意（精英极化），以更好地理解政治动态。", "method": "利用AI检测议会演讲中的发言者和被提及者，并评估情感温度，构建精英极化指数。", "result": "研究结果显示，该指数对选举活动、政党危机等事件反应良好，并可按政党和季度汇总。", "conclusion": "该方法为长期监测欧盟范围内的精英极化提供了基础，且指数具有较高的表面效度。"}}
{"id": "2507.06607", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06607", "abs": "https://arxiv.org/abs/2507.06607", "authors": ["Liliang Ren", "Congcong Chen", "Haoran Xu", "Young Jin Kim", "Adam Atkinson", "Zheng Zhan", "Jiankai Sun", "Baolin Peng", "Liyuan Liu", "Shuohang Wang", "Hao Cheng", "Jianfeng Gao", "Weizhu Chen", "Yelong Shen"], "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation", "comment": null, "summary": "Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.", "AI": {"tldr": "论文提出了一种名为Gated Memory Unit (GMU)的机制，用于在SSM层之间高效共享内存，并基于此构建了SambaY架构，显著提升了解码效率和长上下文性能。", "motivation": "探索SSM层之间表示共享的效率潜力，以提升序列建模的性能和效率。", "method": "引入GMU机制，构建SambaY架构，结合Samba和YOCO的优势，实现内存共享和高效解码。", "result": "SambaY显著提升了解码效率，保持了线性预填充时间复杂度，并在长上下文任务中表现优异。最大模型在推理任务上性能优于基线，且解码吞吐量提升10倍。", "conclusion": "GMU和SambaY架构为高效序列建模提供了新思路，展示了在大规模计算环境下的优越性能扩展性。"}}
{"id": "2507.06935", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06935", "abs": "https://arxiv.org/abs/2507.06935", "authors": ["Karin Festl", "Michael Stolz"], "title": "A nonlinear dead-time compensation method for path tracking control", "comment": "Springer book on Recent Advances in Autonomous Vehicle Technology", "summary": "In the realm of autonomous vehicle technologies and advanced driver\nassistance systems, precise and reliable path tracking controllers are vital\nfor safe and efficient navigation. However the presence of dead time in the\nvehicle control systems poses a challenge to real-world systems. Input and\noutput delays are caused by factors like sensor processing and mechanical\nresponse and can range up to a few hundred milliseconds. This chapter addresses\nthe problem of dead time in path tracking control and proposes a method to\ncompensate the dead time. The proposed solution involves a nonlinear prediction\nmodel, in a structure similar to the Smith predictor, but incorporating the\nkinematic behavior of the vehicle plant system. The implementation avoids\nnumeric integration or optimization, enabling a fast execution. Simulation\ntests with various controllers and disturbances, including dead-time\nuncertainty, demonstrate the efficacy of the dead-time compensation method.\nResults indicate improved control performance in all tested scenarios.", "AI": {"tldr": "论文提出了一种用于补偿自动驾驶车辆路径跟踪控制中死区时间的方法，通过非线性预测模型改进控制性能。", "motivation": "自动驾驶系统中死区时间（如传感器处理和机械响应延迟）对路径跟踪控制造成挑战，需解决以提高导航安全性和效率。", "method": "采用类似Smith预测器的非线性预测模型，结合车辆动力学行为，避免数值积分或优化，实现快速执行。", "result": "仿真测试表明，该方法在各种控制器和干扰（包括死区时间不确定性）下均能提升控制性能。", "conclusion": "提出的死区时间补偿方法有效改善了路径跟踪控制的性能，适用于实际系统。"}}
{"id": "2507.06822", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06822", "abs": "https://arxiv.org/abs/2507.06822", "authors": ["Wei Xu", "Yanchao Zhao", "Weichao Guo", "Xinjun Sheng"], "title": "Hierarchical Reinforcement Learning for Articulated Tool Manipulation with Multifingered Hand", "comment": "Accepted by 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025). copyright 2025 IEEE. Final version to appear\n  in IEEE Xplore", "summary": "Manipulating articulated tools, such as tweezers or scissors, has rarely been\nexplored in previous research. Unlike rigid tools, articulated tools change\ntheir shape dynamically, creating unique challenges for dexterous robotic\nhands. In this work, we present a hierarchical, goal-conditioned reinforcement\nlearning (GCRL) framework to improve the manipulation capabilities of\nanthropomorphic robotic hands using articulated tools. Our framework comprises\ntwo policy layers: (1) a low-level policy that enables the dexterous hand to\nmanipulate the tool into various configurations for objects of different sizes,\nand (2) a high-level policy that defines the tool's goal state and controls the\nrobotic arm for object-picking tasks. We employ an encoder, trained on\nsynthetic pointclouds, to estimate the tool's affordance states--specifically,\nhow different tool configurations (e.g., tweezer opening angles) enable\ngrasping of objects of varying sizes--from input point clouds, thereby enabling\nprecise tool manipulation. We also utilize a privilege-informed heuristic\npolicy to generate replay buffer, improving the training efficiency of the\nhigh-level policy. We validate our approach through real-world experiments,\nshowing that the robot can effectively manipulate a tweezer-like tool to grasp\nobjects of diverse shapes and sizes with a 70.8 % success rate. This study\nhighlights the potential of RL to advance dexterous robotic manipulation of\narticulated tools.", "AI": {"tldr": "提出了一种分层目标条件强化学习框架，用于提升仿人机器人手对铰接工具的操控能力，实验成功率达70.8%。", "motivation": "铰接工具（如镊子或剪刀）的动态形状变化为机器人操控带来独特挑战，此前研究较少涉及。", "method": "采用分层策略：低层策略控制工具配置以适应不同尺寸物体，高层策略定义工具目标状态并控制机械臂。使用基于合成点云的编码器估计工具功能状态，并通过启发式策略生成回放缓冲区。", "result": "实验验证了机器人能有效操控镊子类工具抓取不同形状和大小的物体，成功率为70.8%。", "conclusion": "研究表明强化学习在提升机器人对铰接工具的灵巧操控方面具有潜力。"}}
{"id": "2507.06806", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.06806", "abs": "https://arxiv.org/abs/2507.06806", "authors": ["Eya Cherif", "Arthur Ouaknine", "Luke A. Brown", "Phuong D. Dao", "Kyle R. Kovach", "Bing Lu", "Daniel Mederer", "Hannes Feilhauer", "Teja Kattenborn", "David Rolnick"], "title": "GreenHyperSpectra: A multi-source hyperspectral dataset for global vegetation trait prediction", "comment": null, "summary": "Plant traits such as leaf carbon content and leaf mass are essential\nvariables in the study of biodiversity and climate change. However,\nconventional field sampling cannot feasibly cover trait variation at\necologically meaningful spatial scales. Machine learning represents a valuable\nsolution for plant trait prediction across ecosystems, leveraging hyperspectral\ndata from remote sensing. Nevertheless, trait prediction from hyperspectral\ndata is challenged by label scarcity and substantial domain shifts (\\eg across\nsensors, ecological distributions), requiring robust cross-domain methods.\nHere, we present GreenHyperSpectra, a pretraining dataset encompassing\nreal-world cross-sensor and cross-ecosystem samples designed to benchmark trait\nprediction with semi- and self-supervised methods. We adopt an evaluation\nframework encompassing in-distribution and out-of-distribution scenarios. We\nsuccessfully leverage GreenHyperSpectra to pretrain label-efficient\nmulti-output regression models that outperform the state-of-the-art supervised\nbaseline. Our empirical analyses demonstrate substantial improvements in\nlearning spectral representations for trait prediction, establishing a\ncomprehensive methodological framework to catalyze research at the intersection\nof representation learning and plant functional traits assessment. All code and\ndata are available at: https://github.com/echerif18/HyspectraSSL.", "AI": {"tldr": "论文提出GreenHyperSpectra数据集，用于植物性状预测的跨域方法，通过半监督和自监督学习提升模型性能。", "motivation": "传统野外采样无法覆盖生态尺度上的植物性状变异，机器学习结合高光谱数据为解决此问题提供了可能，但面临标签稀缺和域偏移挑战。", "method": "提出GreenHyperSpectra数据集，采用半监督和自监督学习方法，评估框架包括分布内和分布外场景。", "result": "预训练的标签高效多输出回归模型优于现有监督基线，显著提升了光谱表征学习能力。", "conclusion": "GreenHyperSpectra为植物功能性状评估与表征学习的交叉研究提供了方法论框架和数据支持。"}}
{"id": "2507.06526", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06526", "abs": "https://arxiv.org/abs/2507.06526", "authors": ["Chaoshuo Zhang", "Chenhao Lin", "Zhengyu Zhao", "Le Yang", "Qian Wang", "Chao Shen"], "title": "Concept Unlearning by Modeling Key Steps of Diffusion Process", "comment": null, "summary": "Text-to-image diffusion models (T2I DMs), represented by Stable Diffusion,\nwhich generate highly realistic images based on textual input, have been widely\nused. However, their misuse poses serious security risks. While existing\nconcept unlearning methods aim to mitigate these risks, they struggle to\nbalance unlearning effectiveness with generative retainability.To overcome this\nlimitation, we innovatively propose the Key Step Concept Unlearning (KSCU)\nmethod, which ingeniously capitalizes on the unique stepwise sampling\ncharacteristic inherent in diffusion models during the image generation\nprocess. Unlike conventional approaches that treat all denoising steps equally,\nKSCU strategically focuses on pivotal steps with the most influence over the\nfinal outcome by dividing key steps for different concept unlearning tasks and\nfine-tuning the model only at those steps. This targeted approach reduces the\nnumber of parameter updates needed for effective unlearning, while maximizing\nthe retention of the model's generative capabilities.Through extensive\nbenchmark experiments, we demonstrate that KSCU effectively prevents T2I DMs\nfrom generating undesirable images while better retaining the model's\ngenerative capabilities.Our code will be released.", "AI": {"tldr": "论文提出了一种名为KSCU的新方法，通过针对扩散模型的关键步骤进行概念遗忘，平衡了遗忘效果与生成能力的保留。", "motivation": "现有概念遗忘方法难以平衡遗忘效果与生成能力，导致扩散模型的安全风险。", "method": "KSCU方法利用扩散模型的逐步采样特性，仅对关键步骤进行微调，减少参数更新。", "result": "实验表明，KSCU能有效阻止不良图像生成，同时保留模型生成能力。", "conclusion": "KSCU是一种高效的概念遗忘方法，适用于文本到图像扩散模型。"}}
{"id": "2507.06715", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06715", "abs": "https://arxiv.org/abs/2507.06715", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs", "comment": "12 pages, 4 figures", "summary": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.", "AI": {"tldr": "CLI-RAG框架通过分层分块和双阶段检索机制，解决了临床文本生成中数据非结构化和语义密度高的挑战，显著提升了生成文本的时序和语义对齐。", "motivation": "解决临床文本生成中数据非结构化和语义密度高的问题，确保生成内容与临床需求一致。", "method": "引入分层分块策略和双阶段检索机制（全局阶段识别相关笔记类型，局部阶段提取高价值内容）。", "result": "在MIMIC-III数据集上，生成的结构化进度笔记平均对齐分数达87.7%，优于临床医生撰写的笔记（80.7%）。", "conclusion": "CLI-RAG框架在临床文本生成中表现出色，具有高一致性和可靠性，适合临床应用。"}}
{"id": "2507.06622", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06622", "abs": "https://arxiv.org/abs/2507.06622", "authors": ["Boshko Koloski", "Senja Pollak", "Roberto Navigli", "Blaž Škrlj"], "title": "FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation", "comment": null, "summary": "Building on the success of Large Language Models (LLMs), LLM-based\nrepresentations have dominated the document representation landscape, achieving\ngreat performance on the document embedding benchmarks. However, the\nhigh-dimensional, computationally expensive embeddings from LLMs tend to be\neither too generic or inefficient for domain-specific applications. To address\nthese limitations, we introduce FuDoBa a Bayesian optimisation-based method\nthat integrates LLM-based embeddings with domain-specific structured knowledge,\nsourced both locally and from external repositories like WikiData. This fusion\nproduces low-dimensional, task-relevant representations while reducing training\ncomplexity and yielding interpretable early-fusion weights for enhanced\nclassification performance. We demonstrate the effectiveness of our approach on\nsix datasets in two domains, showing that when paired with robust AutoML-based\nclassifiers, our proposed representation learning approach performs on par\nwith, or surpasses, those produced solely by the proprietary LLM-based\nembedding baselines.", "AI": {"tldr": "FuDoBa是一种基于贝叶斯优化的方法，结合LLM嵌入与领域特定知识，生成低维、任务相关的表示，提升分类性能。", "motivation": "解决LLM生成的高维嵌入在领域特定应用中过于通用或低效的问题。", "method": "通过贝叶斯优化整合LLM嵌入与本地及外部知识（如WikiData），生成低维表示。", "result": "在六个数据集上表现优于或与专有LLM嵌入基线相当。", "conclusion": "FuDoBa能有效提升分类性能，同时降低训练复杂度并增强可解释性。"}}
{"id": "2507.06946", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.06946", "abs": "https://arxiv.org/abs/2507.06946", "authors": ["Mohammad Reza Fasihi", "Brian L. Mark"], "title": "Device-to-Device Communication in 5G/6G: Architectural Foundations and Convergence with Enabling Technologies", "comment": "47 Pages, 16 Figures", "summary": "Device-to-Device (D2D) communication is a promising solution to meet the\ngrowing demands of 5G and future 6G networks by enabling direct communication\nbetween user devices. It enhances spectral efficiency (SE) and energy\nefficiency (EE), reduces latency, and supports proximity-based services. As\nwireless systems evolve toward 5G and 6G paradigms, the integration of D2D with\nadvanced cellular technologies introduces new opportunities and challenges.\nThis survey paper reviews the architectural foundations of D2D communication\nand explores its integration with key 5G/6G enabling technologies. We review\nstandardization efforts, analyze core challenges, and highlight future research\ndirections to unlock the full potential of D2D in next-generation wireless\nnetworks.", "AI": {"tldr": "本文综述了设备间（D2D）通信在5G/6G网络中的潜力、挑战及未来研究方向。", "motivation": "D2D通信能提升频谱和能源效率、降低延迟，并支持基于邻近的服务，是5G/6G网络的重要解决方案。", "method": "通过回顾D2D的架构基础及其与5G/6G关键技术的集成，分析标准化进展和核心挑战。", "result": "总结了D2D在下一代无线网络中的机遇与挑战。", "conclusion": "未来研究需进一步挖掘D2D在5G/6G网络中的潜力。"}}
{"id": "2507.06824", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06824", "abs": "https://arxiv.org/abs/2507.06824", "authors": ["Gabriel Arslan Waltersson", "Yiannis Karayiannidis"], "title": "Friction Estimation for In-Hand Planar Motion", "comment": null, "summary": "This paper presents a method for online estimation of contact properties\nduring in-hand sliding manipulation with a parallel gripper. We estimate the\nstatic and Coulomb friction as well as the contact radius from tactile\nmeasurements of contact forces and sliding velocities. The method is validated\nin both simulation and real-world experiments. Furthermore, we propose a\nheuristic to deal with fast slip-stick dynamics which can adversely affect the\nestimation.", "AI": {"tldr": "提出了一种在线估计平行夹持器滑动操作中接触特性的方法，包括静摩擦、库仑摩擦和接触半径，并通过仿真和实验验证。", "motivation": "研究目的是为了在滑动操作中实时估计接触特性，以提升夹持器的控制精度和适应性。", "method": "通过触觉测量接触力和滑动速度，估计静摩擦、库仑摩擦和接触半径，并提出启发式方法处理快速滑移-粘附动态。", "result": "方法在仿真和实际实验中均得到验证，能够有效估计接触特性。", "conclusion": "该方法为滑动操作中的接触特性估计提供了实用解决方案，并提出了处理动态问题的启发式方法。"}}
{"id": "2507.06971", "categories": ["cs.CV", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.06971", "abs": "https://arxiv.org/abs/2507.06971", "authors": ["Fei Teng", "Kai Luo", "Sheng Wu", "Siyu Li", "Pujun Guo", "Jiale Wei", "Kunyu Peng", "Jiaming Zhang", "Kailun Yang"], "title": "Hallucinating 360°: Panoramic Street-View Generation via Local Scenes Diffusion and Probabilistic Prompting", "comment": "The source code will be publicly available at\n  https://github.com/Bryant-Teng/Percep360", "summary": "Panoramic perception holds significant potential for autonomous driving,\nenabling vehicles to acquire a comprehensive 360{\\deg} surround view in a\nsingle shot. However, autonomous driving is a data-driven task. Complete\npanoramic data acquisition requires complex sampling systems and annotation\npipelines, which are time-consuming and labor-intensive. Although existing\nstreet view generation models have demonstrated strong data regeneration\ncapabilities, they can only learn from the fixed data distribution of existing\ndatasets and cannot achieve high-quality, controllable panoramic generation. In\nthis paper, we propose the first panoramic generation method Percep360 for\nautonomous driving. Percep360 enables coherent generation of panoramic data\nwith control signals based on the stitched panoramic data. Percep360 focuses on\ntwo key aspects: coherence and controllability. Specifically, to overcome the\ninherent information loss caused by the pinhole sampling process, we propose\nthe Local Scenes Diffusion Method (LSDM). LSDM reformulates the panorama\ngeneration as a spatially continuous diffusion process, bridging the gaps\nbetween different data distributions. Additionally, to achieve the controllable\ngeneration of panoramic images, we propose a Probabilistic Prompting Method\n(PPM). PPM dynamically selects the most relevant control cues, enabling\ncontrollable panoramic image generation. We evaluate the effectiveness of the\ngenerated images from three perspectives: image quality assessment (i.e.,\nno-reference and with reference), controllability, and their utility in\nreal-world Bird's Eye View (BEV) segmentation. Notably, the generated data\nconsistently outperforms the original stitched images in no-reference quality\nmetrics and enhances downstream perception models. The source code will be\npublicly available at https://github.com/Bryant-Teng/Percep360.", "AI": {"tldr": "论文提出了一种名为Percep360的全景生成方法，用于自动驾驶，解决了现有方法在数据分布固定和可控性上的不足。", "motivation": "自动驾驶需要全景感知，但数据采集和标注复杂耗时。现有生成模型无法实现高质量可控的全景生成。", "method": "提出了局部场景扩散方法（LSDM）和概率提示方法（PPM），分别解决信息丢失和可控生成问题。", "result": "生成图像在无参考质量指标上优于原始拼接图像，并提升了下游感知模型的性能。", "conclusion": "Percep360在自动驾驶全景生成中表现出色，具有高质量和可控性，代码将开源。"}}
{"id": "2507.06530", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06530", "abs": "https://arxiv.org/abs/2507.06530", "authors": ["Kazi Mahathir Rahman", "Naveed Imtiaz Nafis", "Md. Farhan Sadik", "Mohammad Al Rafi", "Mehedi Hasan Shahed"], "title": "Speak2Sign3D: A Multi-modal Pipeline for English Speech to American Sign Language Animation", "comment": "11 pages, 12 figures", "summary": "Helping deaf and hard-of-hearing people communicate more easily is the main\ngoal of Automatic Sign Language Translation. Although most past research has\nfocused on turning sign language into text, doing the reverse, turning spoken\nEnglish into sign language animations, has been largely overlooked. That's\nbecause it involves multiple steps, such as understanding speech, translating\nit into sign-friendly grammar, and generating natural human motion. In this\nwork, we introduce a complete pipeline that converts English speech into\nsmooth, realistic 3D sign language animations. Our system starts with Whisper\nto translate spoken English into text. Then, we use a MarianMT machine\ntranslation model to translate that text into American Sign Language (ASL)\ngloss, a simplified version of sign language that captures meaning without\ngrammar. This model performs well, reaching BLEU scores of 0.7714 and 0.8923.\nTo make the gloss translation more accurate, we also use word embeddings such\nas Word2Vec and FastText to understand word meanings. Finally, we animate the\ntranslated gloss using a 3D keypoint-based motion system trained on\nSign3D-WLASL, a dataset we created by extracting body, hand, and face key\npoints from real ASL videos in the WLASL dataset. To support the gloss\ntranslation stage, we also built a new dataset called BookGlossCorpus-CG, which\nturns everyday English sentences from the BookCorpus dataset into ASL gloss\nusing grammar rules. Our system stitches everything together by smoothly\ninterpolating between signs to create natural, continuous animations. Unlike\nprevious works like How2Sign and Phoenix-2014T that focus on recognition or use\nonly one type of data, our pipeline brings together audio, text, and motion in\na single framework that goes all the way from spoken English to lifelike 3D\nsign language animation.", "AI": {"tldr": "该论文提出了一种完整的流水线系统，将英语语音转换为流畅的3D手语动画，结合语音识别、文本翻译和动作生成技术。", "motivation": "帮助聋人和听力障碍者更轻松地沟通，填补了将口语转换为手语动画的研究空白。", "method": "使用Whisper进行语音转文本，MarianMT模型将文本翻译为ASL gloss，结合Word2Vec和FastText优化翻译，并通过3D关键点系统生成动画。", "result": "系统表现良好，BLEU分数达到0.7714和0.8923，并创建了新的数据集Sign3D-WLASL和BookGlossCorpus-CG。", "conclusion": "该研究提供了一个从语音到3D手语动画的完整框架，优于以往专注于单一任务的系统。"}}
{"id": "2507.06738", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06738", "abs": "https://arxiv.org/abs/2507.06738", "authors": ["Xinyu Xie", "Weifeng Cao", "Jun Shi", "Yangyang Hu", "Hui Liang", "Wanyong Liang", "Xiaoliang Qian"], "title": "DIFFUMA: High-Fidelity Spatio-Temporal Video Prediction via Dual-Path Mamba and Diffusion Enhancement", "comment": null, "summary": "Spatio-temporal video prediction plays a pivotal role in critical domains,\nranging from weather forecasting to industrial automation. However, in\nhigh-precision industrial scenarios such as semiconductor manufacturing, the\nabsence of specialized benchmark datasets severely hampers research on modeling\nand predicting complex processes. To address this challenge, we make a twofold\ncontribution.First, we construct and release the Chip Dicing Lane Dataset\n(CHDL), the first public temporal image dataset dedicated to the semiconductor\nwafer dicing process. Captured via an industrial-grade vision system, CHDL\nprovides a much-needed and challenging benchmark for high-fidelity process\nmodeling, defect detection, and digital twin development.Second, we propose\nDIFFUMA, an innovative dual-path prediction architecture specifically designed\nfor such fine-grained dynamics. The model captures global long-range temporal\ncontext through a parallel Mamba module, while simultaneously leveraging a\ndiffusion module, guided by temporal features, to restore and enhance\nfine-grained spatial details, effectively combating feature degradation.\nExperiments demonstrate that on our CHDL benchmark, DIFFUMA significantly\noutperforms existing methods, reducing the Mean Squared Error (MSE) by 39% and\nimproving the Structural Similarity (SSIM) from 0.926 to a near-perfect 0.988.\nThis superior performance also generalizes to natural phenomena datasets. Our\nwork not only delivers a new state-of-the-art (SOTA) model but, more\nimportantly, provides the community with an invaluable data resource to drive\nfuture research in industrial AI.", "AI": {"tldr": "论文提出了首个半导体晶圆切割过程的公开数据集CHDL，并设计了双路径预测模型DIFFUMA，显著提升了预测性能。", "motivation": "解决高精度工业场景（如半导体制造）中缺乏专业基准数据集的问题，推动复杂过程建模与预测的研究。", "method": "构建CHDL数据集，并提出DIFFUMA模型，结合并行Mamba模块和扩散模块，分别捕捉全局时间上下文和增强细粒度空间细节。", "result": "DIFFUMA在CHDL基准上表现优异，MSE降低39%，SSIM从0.926提升至0.988，且泛化能力良好。", "conclusion": "研究不仅提供了新的SOTA模型，还为工业AI领域贡献了宝贵的数据资源。"}}
{"id": "2507.06722", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06722", "abs": "https://arxiv.org/abs/2507.06722", "authors": ["Sunwoo Kim", "Haneul Yoo", "Alice Oh"], "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics", "comment": "Accepted to Actionable Interpretability Workshop - ICML 2025", "summary": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.", "AI": {"tldr": "论文研究了大型语言模型（LLMs）在内部如何处理不确定性，发现不确定性和确定性预测在层间概率轨迹上高度一致，挑战了简单检测不确定性的可行性。", "motivation": "理解LLMs如何内部表示和处理不确定性，以检测不确定性并防止幻觉。", "method": "使用Tuned Lens（Logit Lens的变体）分析11个数据集和5个模型的层间概率轨迹，比较确定性和不确定性预测的动态。", "result": "确定性和不确定性预测的层间概率轨迹高度一致，不确定性并未显著影响推理动态。", "conclusion": "研究结果表明，简单方法检测不确定性可能不可行，并展示了可解释性方法在研究不确定性影响推理中的应用。"}}
{"id": "2507.06884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06884", "abs": "https://arxiv.org/abs/2507.06884", "authors": ["Dong Bi", "Yongqi Zhao", "Zhengguo Gu", "Tomislav Mihalj", "Jia Hu", "Arno Eichberger"], "title": "Toward a Full-Stack Co-Simulation Platform for Testing of Automated Driving Systems", "comment": "IEEE International Conference on Intelligent Transportation Systems\n  (ITSC) 2025", "summary": "Virtual testing has emerged as an effective approach to accelerate the\ndeployment of automated driving systems. Nevertheless, existing simulation\ntoolchains encounter difficulties in integrating rapid, automated scenario\ngeneration with simulation environments supporting advanced automated driving\ncapabilities. To address this limitation, a full-stack toolchain is presented,\nenabling automatic scenario generation from real-world datasets and efficient\nvalidation through a co-simulation platform based on CarMaker, ROS, and Apollo.\nThe simulation results demonstrate the effectiveness of the proposed toolchain.\nA demonstration video showcasing the toolchain is available at the provided\nlink: https://youtu.be/taJw_-CmSiY.", "AI": {"tldr": "提出了一种全栈工具链，用于从真实数据自动生成场景，并通过基于CarMaker、ROS和Apollo的协同仿真平台进行高效验证。", "motivation": "虚拟测试是加速自动驾驶系统部署的有效方法，但现有仿真工具链难以集成快速自动场景生成与支持高级自动驾驶功能的仿真环境。", "method": "开发了一种全栈工具链，结合自动场景生成与协同仿真平台（CarMaker、ROS、Apollo）。", "result": "仿真结果证明了该工具链的有效性。", "conclusion": "该工具链解决了现有仿真工具链的局限性，为自动驾驶系统的虚拟测试提供了高效解决方案。"}}
{"id": "2507.07105", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.07105", "abs": "https://arxiv.org/abs/2507.07105", "authors": ["Yushen Zuo", "Qi Zheng", "Mingyang Wu", "Xinrui Jiang", "Renjie Li", "Jian Wang", "Yide Zhang", "Gengchen Mai", "Lihong V. Wang", "James Zou", "Xiaoyu Wang", "Ming-Hsuan Yang", "Zhengzhong Tu"], "title": "4KAgent: Agentic Any Image to 4K Super-Resolution", "comment": "Project page: https://4kagent.github.io", "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.", "AI": {"tldr": "4KAgent是一个统一的超分辨率通用系统，能够将任何图像提升至4K分辨率，甚至更高。它通过三个核心组件实现：Profiling、Perception Agent和Restoration Agent，并在多个领域表现出色。", "motivation": "解决低分辨率图像恢复问题，特别是在严重退化的情况下，提供高质量的超分辨率输出。", "method": "系统包含三个核心模块：Profiling（定制化流程）、Perception Agent（分析输入图像并制定恢复计划）和Restoration Agent（执行计划并优化输出）。", "result": "在11个任务类别和26个基准测试中表现优异，覆盖多种图像领域，并在感知和保真度指标上达到新水平。", "conclusion": "4KAgent为低层次视觉任务建立了新的代理范式，有望推动视觉自主代理的广泛研究和创新。"}}
{"id": "2507.06531", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06531", "abs": "https://arxiv.org/abs/2507.06531", "authors": ["Mingjin Zeng", "Nan Ouyang", "Wenkang Wan", "Lei Ao", "Qing Cai", "Kai Sheng"], "title": "ILNet: Trajectory Prediction with Inverse Learning Attention for Enhancing Intention Capture", "comment": null, "summary": "Trajectory prediction for multi-agent interaction scenarios is a crucial\nchallenge. Most advanced methods model agent interactions by efficiently\nfactorized attention based on the temporal and agent axes. However, this static\nand foward modeling lacks explicit interactive spatio-temporal coordination,\ncapturing only obvious and immediate behavioral intentions. Alternatively, the\nmodern trajectory prediction framework refines the successive predictions by a\nfixed-anchor selection strategy, which is difficult to adapt in different\nfuture environments. It is acknowledged that human drivers dynamically adjust\ninitial driving decisions based on further assumptions about the intentions of\nsurrounding vehicles. Motivated by human driving behaviors, this paper proposes\nILNet, a multi-agent trajectory prediction method with Inverse Learning (IL)\nattention and Dynamic Anchor Selection (DAS) module. IL Attention employs an\ninverse learning paradigm to model interactions at neighboring moments,\nintroducing proposed intentions to dynamically encode the spatio-temporal\ncoordination of interactions, thereby enhancing the model's ability to capture\ncomplex interaction patterns. Then, the learnable DAS module is proposed to\nextract multiple trajectory change keypoints as anchors in parallel with almost\nno increase in parameters. Experimental results show that the ILNet achieves\nstate-of-the-art performance on the INTERACTION and Argoverse motion\nforecasting datasets. Particularly, in challenged interaction scenarios, ILNet\nachieves higher accuracy and more multimodal distributions of trajectories over\nfewer parameters. Our codes are available at https://github.com/mjZeng11/ILNet.", "AI": {"tldr": "ILNet提出了一种多智能体轨迹预测方法，结合逆向学习注意力机制和动态锚点选择模块，显著提升了复杂交互场景下的预测性能。", "motivation": "受人类驾驶行为的启发，旨在解决现有方法在动态交互时空协调和适应性方面的不足。", "method": "采用逆向学习注意力机制（IL Attention）建模交互意图，并引入动态锚点选择模块（DAS）提取关键轨迹点。", "result": "在INTERACTION和Argoverse数据集上达到最优性能，尤其在复杂交互场景中表现更佳。", "conclusion": "ILNet通过动态建模交互意图和自适应锚点选择，显著提升了轨迹预测的准确性和多模态分布能力。"}}
{"id": "2507.06753", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.06753", "abs": "https://arxiv.org/abs/2507.06753", "authors": ["Ye Kyaw Thu", "Thura Aung", "Thazin Myint Oo", "Thepchai Supnithi"], "title": "KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution", "comment": "10 pages, 3 figures, 4 tables", "summary": "This paper presents the first application of Kolmogorov-Arnold Convolution\nfor Text (KAConvText) in sentence classification, addressing three tasks:\nimbalanced binary hate speech detection, balanced multiclass news\nclassification, and imbalanced multiclass ethnic language identification. We\ninvestigate various embedding configurations, comparing random to fastText\nembeddings in both static and fine-tuned settings, with embedding dimensions of\n100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs\nand CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we\ninvestigated KAConvText with different classification heads - MLP and KAN,\nwhere using KAN head supports enhanced interpretability. Results show that\nKAConvText-MLP with fine-tuned fastText embeddings achieves the best\nperformance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,\n92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%\naccuracy (F1-score = 0.9982) for language identification.", "AI": {"tldr": "论文首次将Kolmogorov-Arnold卷积（KAConvText）应用于句子分类，涵盖仇恨言论检测、新闻分类和语言识别任务，结果显示KAConvText-MLP结合微调fastText嵌入表现最佳。", "motivation": "探索KAConvText在句子分类任务中的应用，特别是在处理不平衡数据集时，同时比较不同嵌入配置的效果。", "method": "使用KAConvText结合不同嵌入（随机和fastText）和分类头（MLP和KAN），并与标准CNN和CNN-KAN基线对比。", "result": "KAConvText-MLP在仇恨言论检测、新闻分类和语言识别任务中分别达到91.23%、92.66%和99.82%的准确率。", "conclusion": "KAConvText-MLP结合微调fastText嵌入在句子分类任务中表现优异，且KAN分类头提升了模型的可解释性。"}}
{"id": "2507.06774", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06774", "abs": "https://arxiv.org/abs/2507.06774", "authors": ["Mohammad Ghiasvand Mohammadkhani", "Hamid Beigy"], "title": "Checklist Engineering Empowers Multilingual LLM Judges", "comment": null, "summary": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model.", "AI": {"tldr": "本文提出了一种无需训练的框架CE-Judge，利用清单直觉进行多语言评估，性能优于基线模型并与GPT-4o相当。", "motivation": "自动化文本评估是NLP的核心问题，但现有方法在跨语言任务中成本高且效率低。", "method": "提出CE-Judge框架，基于开源模型和清单直觉，无需训练即可进行多语言评估。", "result": "在多种语言和基准数据集上的实验表明，CE-Judge性能优于基线模型，与GPT-4o相当。", "conclusion": "CE-Judge为多语言文本评估提供了一种高效、低成本且无需训练的解决方案。"}}
{"id": "2507.06905", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06905", "abs": "https://arxiv.org/abs/2507.06905", "authors": ["Wandong Sun", "Luying Feng", "Baoshi Cao", "Yang Liu", "Yaochu Jin", "Zongwu Xie"], "title": "ULC: A Unified and Fine-Grained Controller for Humanoid Loco-Manipulation", "comment": null, "summary": "Loco-Manipulation for humanoid robots aims to enable robots to integrate\nmobility with upper-body tracking capabilities. Most existing approaches adopt\nhierarchical architectures that decompose control into isolated upper-body\n(manipulation) and lower-body (locomotion) policies. While this decomposition\nreduces training complexity, it inherently limits coordination between\nsubsystems and contradicts the unified whole-body control exhibited by humans.\nWe demonstrate that a single unified policy can achieve a combination of\ntracking accuracy, large workspace, and robustness for humanoid\nloco-manipulation. We propose the Unified Loco-Manipulation Controller (ULC), a\nsingle-policy framework that simultaneously tracks root velocity, root height,\ntorso rotation, and dual-arm joint positions in an end-to-end manner, proving\nthe feasibility of unified control without sacrificing performance. We achieve\nthis unified control through key technologies: sequence skill acquisition for\nprogressive learning complexity, residual action modeling for fine-grained\ncontrol adjustments, command polynomial interpolation for smooth motion\ntransitions, random delay release for robustness to deploy variations, load\nrandomization for generalization to external disturbances, and\ncenter-of-gravity tracking for providing explicit policy gradients to maintain\nstability. We validate our method on the Unitree G1 humanoid robot with 3-DOF\n(degrees-of-freedom) waist. Compared with strong baselines, ULC shows better\ntracking performance to disentangled methods and demonstrating larger workspace\ncoverage. The unified dual-arm tracking enables precise manipulation under\nexternal loads while maintaining coordinated whole-body control for complex\nloco-manipulation tasks.", "AI": {"tldr": "提出统一运动-操作控制器（ULC），通过单策略框架实现人形机器人全身协调控制，提升跟踪精度、工作范围和鲁棒性。", "motivation": "现有分层控制方法限制了子系统间的协调，无法实现类似人类的全身统一控制。", "method": "采用序列技能获取、残差动作建模、命令多项式插值等技术，实现端到端的统一控制。", "result": "在Unitree G1机器人上验证，ULC在跟踪性能和任务覆盖范围上优于基线方法。", "conclusion": "ULC证明了统一控制的可行性，且不牺牲性能，适用于复杂运动-操作任务。"}}
{"id": "2507.06537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06537", "abs": "https://arxiv.org/abs/2507.06537", "authors": ["Thi Thu Thuy Nguyen", "Duc Thanh Nguyen"], "title": "A model-agnostic active learning approach for animal detection from camera traps", "comment": null, "summary": "Smart data selection is becoming increasingly important in data-driven\nmachine learning. Active learning offers a promising solution by allowing\nmachine learning models to be effectively trained with optimal data including\nthe most informative samples from large datasets. Wildlife data captured by\ncamera traps are excessive in volume, requiring tremendous effort in data\nlabelling and animal detection models training. Therefore, applying active\nlearning to optimise the amount of labelled data would be a great aid in\nenabling automated wildlife monitoring and conservation. However, existing\nactive learning techniques require that a machine learning model (i.e., an\nobject detector) be fully accessible, limiting the applicability of the\ntechniques. In this paper, we propose a model-agnostic active learning approach\nfor detection of animals captured by camera traps. Our approach integrates\nuncertainty and diversity quantities of samples at both the object-based and\nimage-based levels into the active learning sample selection process. We\nvalidate our approach in a benchmark animal dataset. Experimental results\ndemonstrate that, using only 30% of the training data selected by our approach,\na state-of-the-art animal detector can achieve a performance of equal or\ngreater than that with the use of the complete training dataset.", "AI": {"tldr": "提出了一种模型无关的主动学习方法，用于优化相机陷阱捕获的野生动物数据标注，结合了对象和图像层面的不确定性与多样性，仅需30%的训练数据即可达到或超过完整数据集的检测性能。", "motivation": "野生动物数据标注和模型训练成本高，现有主动学习方法需要完全访问模型，限制了其应用。", "method": "提出模型无关的主动学习方法，结合对象和图像层面的不确定性与多样性进行样本选择。", "result": "实验表明，仅使用30%的训练数据，动物检测器性能可达到或超过完整数据集的效果。", "conclusion": "该方法显著降低了数据标注成本，为自动化野生动物监测提供了有效工具。"}}
{"id": "2507.06763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06763", "abs": "https://arxiv.org/abs/2507.06763", "authors": ["Saif Ur Rehman Khan", "Muhammad Nabeel Asim", "Sebastian Vollmer", "Andreas Dengel"], "title": "FOLC-Net: A Federated-Optimized Lightweight Architecture for Enhanced MRI Disease Diagnosis across Axial, Coronal, and Sagittal Views", "comment": null, "summary": "The framework is designed to improve performance in the analysis of combined\nas well as single anatomical perspectives for MRI disease diagnosis. It\nspecifically addresses the performance degradation observed in state-of-the-art\n(SOTA) models, particularly when processing axial, coronal, and sagittal\nanatomical planes. The paper introduces the FOLC-Net framework, which\nincorporates a novel federated-optimized lightweight architecture with\napproximately 1.217 million parameters and a storage requirement of only 0.9\nMB. FOLC-Net integrates Manta-ray foraging optimization (MRFO) mechanisms for\nefficient model structure generation, global model cloning for scalable\ntraining, and ConvNeXt for enhanced client adaptability. The model was\nevaluated on combined multi-view data as well as individual views, such as\naxial, coronal, and sagittal, to assess its robustness in various medical\nimaging scenarios. Moreover, FOLC-Net tests a ShallowFed model on different\ndata to evaluate its ability to generalize beyond the training dataset. The\nresults show that FOLC-Net outperforms existing models, particularly in the\nchallenging sagittal view. For instance, FOLC-Net achieved an accuracy of\n92.44% on the sagittal view, significantly higher than the 88.37% accuracy of\nstudy method (DL + Residual Learning) and 88.95% of DL models. Additionally,\nFOLC-Net demonstrated improved accuracy across all individual views, providing\na more reliable and robust solution for medical image analysis in decentralized\nenvironments. FOLC-Net addresses the limitations of existing SOTA models by\nproviding a framework that ensures better adaptability to individual views\nwhile maintaining strong performance in multi-view settings. The incorporation\nof MRFO, global model cloning, and ConvNeXt ensures that FOLC-Net performs\nbetter in real-world medical applications.", "AI": {"tldr": "FOLC-Net是一种新型联邦优化的轻量级框架，用于提升MRI疾病诊断中多视角和单视角的性能，显著优于现有方法。", "motivation": "解决现有SOTA模型在处理轴向、冠状和矢状解剖平面时性能下降的问题。", "method": "提出FOLC-Net框架，结合MRFO优化、全局模型克隆和ConvNeXt，参数少且存储需求低。", "result": "在矢状视角下准确率达92.44%，优于其他方法，并在多视角中表现稳健。", "conclusion": "FOLC-Net为分散式环境中的医学图像分析提供了更可靠和高效的解决方案。"}}
{"id": "2507.06795", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06795", "abs": "https://arxiv.org/abs/2507.06795", "authors": ["Seonwu Kim", "Yohan Na", "Kihun Kim", "Hanhee Cho", "Geun Lim", "Mintae Kim", "Seongik Park", "Ki Hyun Kim", "Youngsub Han", "Byoung-Ki Jeon"], "title": "Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining: Method, Evaluation and Applications", "comment": "under review", "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.", "AI": {"tldr": "研究验证了基于DACP的方法在小型LLMs上的有效性，证明其能显著提升目标领域性能，同时保持通用能力，为企业部署提供高效解决方案。", "motivation": "开源大型语言模型（LLMs）的兴起为企业应用提供了机会，但许多组织缺乏部署和维护大规模模型的基础设施，因此小型LLMs（sLLMs）成为实用替代方案。然而，领域自适应持续预训练（DACP）在商业应用中的实用性尚未充分研究。", "method": "通过在不同基础模型和服务领域应用基于DACP的方法，进行广泛实验和实际评估。", "result": "实验表明，应用DACP的sLLMs在目标领域性能上有显著提升，同时保持了通用能力。", "conclusion": "DACP方法为小型LLMs提供了一种成本高效且可扩展的企业级部署解决方案。"}}
{"id": "2507.06960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06960", "abs": "https://arxiv.org/abs/2507.06960", "authors": ["Samuel Matloob", "Ayan Dutta", "O. Patrick Kreidl", "Swapnonel Roy", "Ladislau Bölöni"], "title": "Bounomodes: the grazing ox algorithm for exploration of clustered anomalies", "comment": null, "summary": "A common class of algorithms for informative path planning (IPP) follows\nboustrophedon (\"as the ox turns\") patterns, which aim to achieve uniform area\ncoverage. However, IPP is often applied in scenarios where anomalies, such as\nplant diseases, pollution, or hurricane damage, appear in clusters. In such\ncases, prioritizing the exploration of anomalous regions over uniform coverage\nis beneficial. This work introduces a class of algorithms referred to as\nbounom\\=odes (\"as the ox grazes\"), which alternates between uniform\nboustrophedon sampling and targeted exploration of detected anomaly clusters.\nWhile uniform sampling can be designed using geometric principles, close\nexploration of clusters depends on the spatial distribution of anomalies and\nmust be learned. In our implementation, the close exploration behavior is\nlearned using deep reinforcement learning algorithms. Experimental evaluations\ndemonstrate that the proposed approach outperforms several established\nbaselines.", "AI": {"tldr": "论文提出了一种名为“bounom=odes”的算法，结合均匀覆盖和异常区域探索，优于传统方法。", "motivation": "传统均匀覆盖算法在异常集群场景中效率不足，需优先探索异常区域。", "method": "交替使用均匀采样和基于深度强化学习的异常集群探索。", "result": "实验表明，该方法优于多个基线算法。", "conclusion": "结合均匀覆盖与针对性探索能有效提升路径规划性能。"}}
{"id": "2507.06543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06543", "abs": "https://arxiv.org/abs/2507.06543", "authors": ["Taekyung Kim", "Dongyoon Han", "Byeongho Heo", "Jeongeun Park", "Sangdoo Yun"], "title": "Token Bottleneck: One Token to Remember Dynamics", "comment": "17 pages, 9 figures, 8 tables, project page:\n  https://token-bottleneck.github.io, code: https://github.com/naver-ai/tobo", "summary": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales.", "AI": {"tldr": "ToBo是一种自监督学习框架，通过压缩场景为瓶颈令牌并预测后续场景，学习动态场景的紧凑表示。", "motivation": "动态场景的紧凑和时间感知表示对视觉跟踪和机器人操作等任务至关重要。", "method": "ToBo通过压缩步骤将参考场景编码为瓶颈令牌，在扩展步骤中利用少量目标补丁预测目标场景。", "result": "ToBo在视频标签传播和机器人操作等任务中表现优于基线，并在实际机器人环境中验证了其鲁棒性。", "conclusion": "ToBo能够有效学习动态场景的时序依赖关系，并在不同模型规模上展现出可扩展性。"}}
{"id": "2507.06803", "categories": ["cs.CL", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.06803", "abs": "https://arxiv.org/abs/2507.06803", "authors": ["Matthew Anderson Hendricks", "Alice Cicirello"], "title": "Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams", "comment": null, "summary": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.", "AI": {"tldr": "提出一种利用领域和专家知识自动生成动态系统计算模型的策略，通过SysML图和NLP技术提升效率。", "motivation": "加速工程动态系统的设计与部署，减少人工建模的复杂性。", "method": "分五步实现，结合SysML图、NLP和LLMs提取信息并生成模型。", "result": "通过案例研究验证，生成的计算模型性能优于仅用LLMs的结果。", "conclusion": "该方法通用性强，适用于多种系统和领域，显著提升建模效率。"}}
{"id": "2507.06829", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06829", "abs": "https://arxiv.org/abs/2507.06829", "authors": ["Zenan Xu", "Zexuan Qiu", "Guanhua Huang", "Kun Li", "Siheng Li", "Chenchen Zhang", "Kejiao Li", "Qi Yi", "Yuhao Jiang", "Bo Zhou", "Fengzong Lian", "Zhanhui Kang"], "title": "Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework", "comment": "13 pages, 5 fiures", "summary": "Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy...", "AI": {"tldr": "论文提出了一种结合顺序推理和并行推理优势的协作推理框架，并引入语义熵（SE）作为动态控制推理质量的指标。", "motivation": "现有顺序推理和并行推理方法存在效率低或缺乏协调的问题，需设计更灵活的推理框架。", "method": "提出协作推理框架，利用语义熵（SE）评估推理质量，实现动态控制和早期终止。", "result": "语义熵（SE）与准确性呈强负相关，可作为推理质量的鲁棒指标。", "conclusion": "该框架结合了顺序和并行推理的优势，通过SE实现了高效的动态推理控制。"}}
{"id": "2507.06662", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06662", "abs": "https://arxiv.org/abs/2507.06662", "authors": ["Yifan Yang", "Peili Song", "Enfan Lan", "Dong Liu", "Jingtai Liu"], "title": "MK-Pose: Category-Level Object Pose Estimation via Multimodal-Based Keypoint Learning", "comment": null, "summary": "Category-level object pose estimation, which predicts the pose of objects\nwithin a known category without prior knowledge of individual instances, is\nessential in applications like warehouse automation and manufacturing. Existing\nmethods relying on RGB images or point cloud data often struggle with object\nocclusion and generalization across different instances and categories. This\npaper proposes a multimodal-based keypoint learning framework (MK-Pose) that\nintegrates RGB images, point clouds, and category-level textual descriptions.\nThe model uses a self-supervised keypoint detection module enhanced with\nattention-based query generation, soft heatmap matching and graph-based\nrelational modeling. Additionally, a graph-enhanced feature fusion module is\ndesigned to integrate local geometric information and global context. MK-Pose\nis evaluated on CAMERA25 and REAL275 dataset, and is further tested for\ncross-dataset capability on HouseCat6D dataset. The results demonstrate that\nMK-Pose outperforms existing state-of-the-art methods in both IoU and average\nprecision without shape priors. Codes will be released at\n\\href{https://github.com/yangyifanYYF/MK-Pose}{https://github.com/yangyifanYYF/MK-Pose}.", "AI": {"tldr": "MK-Pose是一种多模态关键点学习框架，结合RGB图像、点云和类别级文本描述，通过自监督关键点检测和注意力机制提升性能，在多个数据集上表现优于现有方法。", "motivation": "解决现有方法在物体遮挡和跨类别泛化上的不足，提升类别级物体姿态估计的准确性。", "method": "提出MK-Pose框架，整合RGB、点云和文本描述，采用自监督关键点检测、注意力查询生成、软热图匹配和图关系建模，并设计图增强特征融合模块。", "result": "在CAMERA25、REAL275和HouseCat6D数据集上表现优异，IoU和平均精度均优于现有方法。", "conclusion": "MK-Pose通过多模态融合和图增强特征显著提升了类别级物体姿态估计的性能，具有广泛的应用潜力。"}}
{"id": "2507.06547", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06547", "abs": "https://arxiv.org/abs/2507.06547", "authors": ["Yonghyun Park", "Chieh-Hsin Lai", "Satoshi Hayakawa", "Yuhta Takida", "Naoki Murata", "Wei-Hsiang Liao", "Woosung Choi", "Kin Wai Cheuk", "Junghyun Koo", "Yuki Mitsufuji"], "title": "Concept-TRAK: Understanding how diffusion models learn concepts through concept-level attribution", "comment": "Preprint", "summary": "While diffusion models excel at image generation, their growing adoption\nraises critical concerns around copyright issues and model transparency.\nExisting attribution methods identify training examples influencing an entire\nimage, but fall short in isolating contributions to specific elements, such as\nstyles or objects, that matter most to stakeholders. To bridge this gap, we\nintroduce \\emph{concept-level attribution} via a novel method called\n\\emph{Concept-TRAK}. Concept-TRAK extends influence functions with two key\ninnovations: (1) a reformulated diffusion training loss based on diffusion\nposterior sampling, enabling robust, sample-specific attribution; and (2) a\nconcept-aware reward function that emphasizes semantic relevance. We evaluate\nConcept-TRAK on the AbC benchmark, showing substantial improvements over prior\nmethods. Through diverse case studies--ranging from identifying IP-protected\nand unsafe content to analyzing prompt engineering and compositional\nlearning--we demonstrate how concept-level attribution yields actionable\ninsights for responsible generative AI development and governance.", "AI": {"tldr": "论文提出了一种名为Concept-TRAK的新方法，用于解决扩散模型在图像生成中的版权和透明度问题，通过概念级归因分析特定元素（如风格或对象）的影响。", "motivation": "扩散模型在图像生成中表现优异，但其广泛应用引发了版权和透明度问题。现有方法无法分析特定元素（如风格或对象）的贡献，因此需要一种更精细的归因方法。", "method": "提出了Concept-TRAK方法，扩展了影响函数，包括基于扩散后验采样的训练损失和概念感知奖励函数，以实现样本特定的归因和语义相关性。", "result": "在AbC基准测试中，Concept-TRAK显著优于现有方法，并通过案例研究展示了其在识别受保护内容、分析提示工程等方面的实用性。", "conclusion": "Concept-TRAK为生成式AI的负责任开发和治理提供了可操作的见解，解决了概念级归因的需求。"}}
{"id": "2507.06812", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06812", "abs": "https://arxiv.org/abs/2507.06812", "authors": ["Xu Yang", "Shaoli Huang", "Shenbo Xie", "Xuelin Chen", "Yifei Liu", "Changxing Ding"], "title": "Democratizing High-Fidelity Co-Speech Gesture Video Generation", "comment": "ICCV 2025", "summary": "Co-speech gesture video generation aims to synthesize realistic,\naudio-aligned videos of speakers, complete with synchronized facial expressions\nand body gestures. This task presents challenges due to the significant\none-to-many mapping between audio and visual content, further complicated by\nthe scarcity of large-scale public datasets and high computational demands. We\npropose a lightweight framework that utilizes 2D full-body skeletons as an\nefficient auxiliary condition to bridge audio signals with visual outputs. Our\napproach introduces a diffusion model conditioned on fine-grained audio\nsegments and a skeleton extracted from the speaker's reference image,\npredicting skeletal motions through skeleton-audio feature fusion to ensure\nstrict audio coordination and body shape consistency. The generated skeletons\nare then fed into an off-the-shelf human video generation model with the\nspeaker's reference image to synthesize high-fidelity videos. To democratize\nresearch, we present CSG-405-the first public dataset with 405 hours of\nhigh-resolution videos across 71 speech types, annotated with 2D skeletons and\ndiverse speaker demographics. Experiments show that our method exceeds\nstate-of-the-art approaches in visual quality and synchronization while\ngeneralizing across speakers and contexts.", "AI": {"tldr": "提出了一种轻量级框架，利用2D全身骨架作为辅助条件，通过扩散模型生成与音频同步的说话者视频。", "motivation": "解决语音手势视频生成中音频与视觉内容的一对多映射问题，以及数据集稀缺和高计算需求的挑战。", "method": "采用扩散模型，结合细粒度音频片段和参考图像的骨架，预测骨骼运动，并通过现成的人类视频生成模型合成高保真视频。", "result": "提出的方法在视觉质量和同步性上优于现有技术，并能泛化到不同说话者和语境。", "conclusion": "该方法高效且通用，同时发布了首个公开数据集CSG-405，促进相关研究。"}}
{"id": "2507.06838", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.06838", "abs": "https://arxiv.org/abs/2507.06838", "authors": ["Dahyun Lee", "Yongrae Jo", "Haeju Park", "Moontae Lee"], "title": "Shifting from Ranking to Set Selection for Retrieval Augmented Generation", "comment": "Accepted to ACL 2025 Oral", "summary": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR", "AI": {"tldr": "SETR通过集合式段落选择方法，利用Chain-of-Thought推理明确查询需求，显著提升了多跳问答中的检索质量。", "motivation": "现有方法仅基于段落个体相关性重新排序，难以满足复杂查询的需求。", "method": "提出SETR方法，通过Chain-of-Thought推理识别查询需求，并选择最优段落集合。", "result": "在多跳RAG基准测试中，SETR在答案正确性和检索质量上优于现有方法。", "conclusion": "SETR为RAG系统提供了一种高效且有效的替代方案。"}}
{"id": "2507.06687", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06687", "abs": "https://arxiv.org/abs/2507.06687", "authors": ["Marcel Vosshans", "Omar Ait-Aider", "Youcef Mezouar", "Markus Enzweiler"], "title": "StixelNExT++: Lightweight Monocular Scene Segmentation and Representation for Collective Perception", "comment": null, "summary": "This paper presents StixelNExT++, a novel approach to scene representation\nfor monocular perception systems. Building on the established Stixel\nrepresentation, our method infers 3D Stixels and enhances object segmentation\nby clustering smaller 3D Stixel units. The approach achieves high compression\nof scene information while remaining adaptable to point cloud and\nbird's-eye-view representations. Our lightweight neural network, trained on\nautomatically generated LiDAR-based ground truth, achieves real-time\nperformance with computation times as low as 10 ms per frame. Experimental\nresults on the Waymo dataset demonstrate competitive performance within a\n30-meter range, highlighting the potential of StixelNExT++ for collective\nperception in autonomous systems.", "AI": {"tldr": "StixelNExT++是一种用于单目感知系统的新型场景表示方法，通过聚类3D Stixel单元增强对象分割，实现高压缩场景信息，并在Waymo数据集上表现出竞争力。", "motivation": "改进现有的Stixel表示方法，以更高效地表示场景信息并适应点云和鸟瞰图表示。", "method": "使用轻量级神经网络，基于LiDAR生成的自动标注数据进行训练，实现实时性能（每帧10毫秒）。", "result": "在Waymo数据集上，30米范围内表现出竞争力，适用于自动驾驶系统的集体感知。", "conclusion": "StixelNExT++在场景表示和实时性能方面具有潜力，适用于自动驾驶应用。"}}
{"id": "2507.06560", "categories": ["cs.CV", "cs.LG", "68T07, 62H12", "I.2.6; I.4.8; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.06560", "abs": "https://arxiv.org/abs/2507.06560", "authors": ["Jae Hyoung Jeon", "Cheolsu Lim", "Myungjoo Kang"], "title": "Divergence-Based Similarity Function for Multi-View Contrastive Learning", "comment": "9 pages, 5 figures", "summary": "Recent success in contrastive learning has sparked growing interest in more\neffectively leveraging multiple augmented views of an instance. While prior\nmethods incorporate multiple views at the loss or feature level, they primarily\ncapture pairwise relationships and fail to model the joint structure across all\nviews. In this work, we propose a divergence-based similarity function (DSF)\nthat explicitly captures the joint structure by representing each set of\naugmented views as a distribution and measuring similarity as the divergence\nbetween distributions. Extensive experiments demonstrate that DSF consistently\nimproves performance across various tasks, including kNN classification and\nlinear evaluation, while also offering greater efficiency compared to other\nmulti-view methods. Furthermore, we establish a theoretical connection between\nDSF and cosine similarity, and show that, unlike cosine similarity, DSF\noperates effectively without requiring a temperature hyperparameter.", "AI": {"tldr": "提出了一种基于分布差异的相似性函数（DSF），通过将多视图表示为分布并测量分布间的差异来捕捉联合结构，优于现有方法。", "motivation": "现有方法主要捕捉成对关系，未能建模多视图的联合结构。", "method": "提出DSF，将多视图表示为分布并测量分布间的差异。", "result": "DSF在kNN分类和线性评估等任务中表现优异，且效率更高。", "conclusion": "DSF无需温度超参数即可有效运行，优于余弦相似度。"}}
{"id": "2507.06830", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06830", "abs": "https://arxiv.org/abs/2507.06830", "authors": ["Tao Feng", "Xianbing Zhao", "Zhenhua Chen", "Tien Tsin Wong", "Hamid Rezatofighi", "Gholamreza Haffari", "Lizhen Qu"], "title": "Physics-Grounded Motion Forecasting via Equation Discovery for Trajectory-Guided Image-to-Video Generation", "comment": null, "summary": "Recent advances in diffusion-based and autoregressive video generation models\nhave achieved remarkable visual realism. However, these models typically lack\naccurate physical alignment, failing to replicate real-world dynamics in object\nmotion. This limitation arises primarily from their reliance on learned\nstatistical correlations rather than capturing mechanisms adhering to physical\nlaws. To address this issue, we introduce a novel framework that integrates\nsymbolic regression (SR) and trajectory-guided image-to-video (I2V) models for\nphysics-grounded video forecasting. Our approach extracts motion trajectories\nfrom input videos, uses a retrieval-based pre-training mechanism to enhance\nsymbolic regression, and discovers equations of motion to forecast physically\naccurate future trajectories. These trajectories then guide video generation\nwithout requiring fine-tuning of existing models. Evaluated on scenarios in\nClassical Mechanics, including spring-mass, pendulums, and projectile motions,\nour method successfully recovers ground-truth analytical equations and improves\nthe physical alignment of generated videos over baseline methods.", "AI": {"tldr": "论文提出了一种结合符号回归和轨迹引导的视频生成框架，以解决现有模型物理对齐不足的问题。", "motivation": "现有扩散和自回归视频生成模型在视觉上表现优异，但缺乏物理准确性，无法模拟真实世界的物体运动。", "method": "通过提取输入视频的运动轨迹，利用检索式预训练增强符号回归，发现运动方程以预测物理准确的未来轨迹，并引导视频生成。", "result": "在经典力学场景（如弹簧-质量系统、摆锤和抛体运动）中，该方法成功恢复了真实运动方程，并提升了生成视频的物理对齐性。", "conclusion": "该框架无需微调现有模型，即可显著提高视频生成的物理准确性。"}}
{"id": "2507.06893", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06893", "abs": "https://arxiv.org/abs/2507.06893", "authors": ["Alexandra Abbas", "Celia Waggoner", "Justin Olive"], "title": "Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights", "comment": null, "summary": "AI evaluations have become critical tools for assessing large language model\ncapabilities and safety. This paper presents practical insights from eight\nmonths of maintaining $inspect\\_evals$, an open-source repository of 70+\ncommunity-contributed AI evaluations. We identify key challenges in\nimplementing and maintaining AI evaluations and develop solutions including:\n(1) a structured cohort management framework for scaling community\ncontributions, (2) statistical methodologies for optimal resampling and\ncross-model comparison with uncertainty quantification, and (3) systematic\nquality control processes for reproducibility. Our analysis reveals that AI\nevaluation requires specialized infrastructure, statistical rigor, and\ncommunity coordination beyond traditional software development practices.", "AI": {"tldr": "论文总结了维护开源AI评估工具的经验，提出了解决挑战的三种方法，并强调AI评估需要专门的基础设施和统计严谨性。", "motivation": "评估大型语言模型的能力和安全性至关重要，但现有工具在实施和维护上面临挑战。", "method": "提出了三种解决方案：(1) 结构化群组管理框架，(2) 统计方法优化重采样和跨模型比较，(3) 系统性质量控制流程。", "result": "通过实践验证了这些方法的有效性，并发现AI评估需要超越传统软件开发的基础设施和协作。", "conclusion": "AI评估需要专门的基础设施、统计严谨性和社区协作，以支持其复杂性和规模。"}}
{"id": "2507.06719", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.06719", "abs": "https://arxiv.org/abs/2507.06719", "authors": ["Zhenyang Liu", "Sixiao Zheng", "Siyu Chen", "Cairong Zhao", "Longfei Liang", "Xiangyang Xue", "Yanwei Fu"], "title": "A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding", "comment": null, "summary": "Open-vocabulary 3D visual grounding aims to localize target objects based on\nfree-form language queries, which is crucial for embodied AI applications such\nas autonomous navigation, robotics, and augmented reality. Learning 3D language\nfields through neural representations enables accurate understanding of 3D\nscenes from limited viewpoints and facilitates the localization of target\nobjects in complex environments. However, existing language field methods\nstruggle to accurately localize instances using spatial relations in language\nqueries, such as ``the book on the chair.'' This limitation mainly arises from\ninadequate reasoning about spatial relations in both language queries and 3D\nscenes. In this work, we propose SpatialReasoner, a novel neural\nrepresentation-based framework with large language model (LLM)-driven spatial\nreasoning that constructs a visual properties-enhanced hierarchical feature\nfield for open-vocabulary 3D visual grounding. To enable spatial reasoning in\nlanguage queries, SpatialReasoner fine-tunes an LLM to capture spatial\nrelations and explicitly infer instructions for the target, anchor, and spatial\nrelation. To enable spatial reasoning in 3D scenes, SpatialReasoner\nincorporates visual properties (opacity and color) to construct a hierarchical\nfeature field. This field represents language and instance features using\ndistilled CLIP features and masks extracted via the Segment Anything Model\n(SAM). The field is then queried using the inferred instructions in a\nhierarchical manner to localize the target 3D instance based on the spatial\nrelation in the language query. Extensive experiments show that our framework\ncan be seamlessly integrated into different neural representations,\noutperforming baseline models in 3D visual grounding while empowering their\nspatial reasoning capability.", "AI": {"tldr": "论文提出SpatialReasoner框架，通过LLM驱动的空间推理改进开放词汇3D视觉定位，解决现有方法在空间关系推理上的不足。", "motivation": "现有语言场方法在基于空间关系的语言查询（如“椅子上的书”）中定位实例时表现不佳，主要因缺乏对语言查询和3D场景中空间关系的推理能力。", "method": "提出SpatialReasoner框架，包括：1）微调LLM以捕获空间关系并推断目标、锚点和空间关系指令；2）结合视觉属性构建分层特征场，使用CLIP特征和SAM掩码表示语言和实例特征。", "result": "实验表明，该框架可无缝集成到不同神经表示中，显著提升3D视觉定位性能，并增强空间推理能力。", "conclusion": "SpatialReasoner通过LLM驱动的空间推理和分层特征场，有效解决了开放词汇3D视觉定位中的空间关系推理问题。"}}
{"id": "2507.06569", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06569", "abs": "https://arxiv.org/abs/2507.06569", "authors": ["Hao Shu"], "title": "Edge-Boundary-Texture Loss: A Tri-Class Generalization of Weighted Binary Cross-Entropy for Enhanced Edge Detection", "comment": "10 pages", "summary": "Edge detection (ED) remains a fundamental task in computer vision, yet its\nperformance is often hindered by the ambiguous nature of non-edge pixels near\nobject boundaries. The widely adopted Weighted Binary Cross-Entropy (WBCE) loss\ntreats all non-edge pixels uniformly, overlooking the structural nuances around\nedges and often resulting in blurred predictions. In this paper, we propose the\nEdge-Boundary-Texture (EBT) loss, a novel objective that explicitly divides\npixels into three categories, edge, boundary, and texture, and assigns each a\ndistinct supervisory weight. This tri-class formulation enables more structured\nlearning by guiding the model to focus on both edge precision and contextual\nboundary localization. We theoretically show that the EBT loss generalizes the\nWBCE loss, with the latter becoming a limit case. Extensive experiments across\nmultiple benchmarks demonstrate the superiority of the EBT loss both\nquantitatively and perceptually. Furthermore, the consistent use of unified\nhyperparameters across all models and datasets, along with robustness to their\nmoderate variations, indicates that the EBT loss requires minimal fine-tuning\nand is easily deployable in practice.", "AI": {"tldr": "论文提出了一种新的损失函数EBT，通过将像素分为边缘、边界和纹理三类，改进了传统WBCE损失在边缘检测中的模糊问题。", "motivation": "传统WBCE损失在处理非边缘像素时忽略了结构差异，导致预测模糊。", "method": "提出EBT损失，将像素分为三类并分配不同权重，以更结构化地学习边缘和边界。", "result": "实验证明EBT损失在多个基准测试中表现优异，且超参数鲁棒性强。", "conclusion": "EBT损失易于部署且性能优越，是边缘检测的有效改进。"}}
{"id": "2507.06856", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06856", "abs": "https://arxiv.org/abs/2507.06856", "authors": ["Subrat Kishore Dutta", "Xiao Zhang"], "title": "IAP: Invisible Adversarial Patch Attack through Perceptibility-Aware Localization and Perturbation Optimization", "comment": "Published in ICCV 2025", "summary": "Despite modifying only a small localized input region, adversarial patches\ncan drastically change the prediction of computer vision models. However, prior\nmethods either cannot perform satisfactorily under targeted attack scenarios or\nfail to produce contextually coherent adversarial patches, causing them to be\neasily noticeable by human examiners and insufficiently stealthy against\nautomatic patch defenses. In this paper, we introduce IAP, a novel attack\nframework that generates highly invisible adversarial patches based on\nperceptibility-aware localization and perturbation optimization schemes.\nSpecifically, IAP first searches for a proper location to place the patch by\nleveraging classwise localization and sensitivity maps, balancing the\nsusceptibility of patch location to both victim model prediction and human\nvisual system, then employs a perceptibility-regularized adversarial loss and a\ngradient update rule that prioritizes color constancy for optimizing invisible\nperturbations. Comprehensive experiments across various image benchmarks and\nmodel architectures demonstrate that IAP consistently achieves competitive\nattack success rates in targeted settings with significantly improved patch\ninvisibility compared to existing baselines. In addition to being highly\nimperceptible to humans, IAP is shown to be stealthy enough to render several\nstate-of-the-art patch defenses ineffective.", "AI": {"tldr": "IAP是一种新的对抗性补丁攻击框架，通过感知感知定位和扰动优化方案生成高度不可见的对抗性补丁，显著提升了攻击成功率和隐蔽性。", "motivation": "现有方法在针对性攻击场景中表现不佳或生成的补丁不连贯，容易被人类或自动防御系统发现。", "method": "IAP结合类定位和敏感度图选择补丁位置，并采用感知正则化对抗损失和颜色恒常性梯度更新规则优化扰动。", "result": "IAP在多种图像基准和模型架构中表现出高攻击成功率和隐蔽性，能有效绕过现有防御系统。", "conclusion": "IAP在对抗性补丁攻击中实现了高效性和隐蔽性，为防御系统提出了新挑战。"}}
{"id": "2507.06895", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06895", "abs": "https://arxiv.org/abs/2507.06895", "authors": ["Luca Mariotti", "Veronica Guidetti", "Federica Mandreoli"], "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN", "comment": null, "summary": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.", "AI": {"tldr": "SCoRE是一个模块化、低成本的句子级关系抽取系统，结合对比学习和贝叶斯kNN分类器，在低监督环境下表现优异，并提出了新的评估指标。", "motivation": "解决知识图谱（KG）增强中对噪声鲁棒且适应性强的关系抽取（RE）需求，尤其是在低监督环境下。", "method": "采用监督对比学习和贝叶斯kNN分类器进行多标签分类，无需微调预训练语言模型（PLM）。", "result": "在五个基准测试中，SCoRE性能优于或匹配现有方法，同时显著降低能耗。", "conclusion": "SCoRE因其高效、模块化和可扩展性，成为实际RE应用的理想选择。"}}
{"id": "2507.06590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06590", "abs": "https://arxiv.org/abs/2507.06590", "authors": ["Yin Wang", "Mu li", "Zhiying Leng", "Frederick W. B. Li", "Xiaohui Liang"], "title": "MOST: Motion Diffusion Model for Rare Text via Temporal Clip Banzhaf Interaction", "comment": null, "summary": "We introduce MOST, a novel motion diffusion model via temporal clip Banzhaf\ninteraction, aimed at addressing the persistent challenge of generating human\nmotion from rare language prompts. While previous approaches struggle with\ncoarse-grained matching and overlook important semantic cues due to motion\nredundancy, our key insight lies in leveraging fine-grained clip relationships\nto mitigate these issues. MOST's retrieval stage presents the first formulation\nof its kind - temporal clip Banzhaf interaction - which precisely quantifies\ntextual-motion coherence at the clip level. This facilitates direct,\nfine-grained text-to-motion clip matching and eliminates prevalent redundancy.\nIn the generation stage, a motion prompt module effectively utilizes retrieved\nmotion clips to produce semantically consistent movements. Extensive\nevaluations confirm that MOST achieves state-of-the-art text-to-motion\nretrieval and generation performance by comprehensively addressing previous\nchallenges, as demonstrated through quantitative and qualitative results\nhighlighting its effectiveness, especially for rare prompts.", "AI": {"tldr": "MOST是一种新型的运动扩散模型，通过时间片段Banzhaf交互解决从罕见语言提示生成人类运动的挑战。", "motivation": "现有方法在粗粒度匹配和语义线索忽视方面存在问题，MOST通过利用细粒度片段关系来解决这些问题。", "method": "MOST采用时间片段Banzhaf交互量化文本-运动一致性，并通过运动提示模块生成语义一致的运动。", "result": "MOST在文本到运动的检索和生成方面达到最先进水平，尤其在罕见提示上表现突出。", "conclusion": "MOST通过细粒度匹配和消除冗余，显著提升了文本到运动生成的效果。"}}
{"id": "2507.06899", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06899", "abs": "https://arxiv.org/abs/2507.06899", "authors": ["Ziang Ye", "Yang Zhang", "Wentao Shi", "Xiaoyu You", "Fuli Feng", "Tat-Seng Chua"], "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation", "comment": null, "summary": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents.", "AI": {"tldr": "GUI代理（基于大型视觉语言模型）存在视觉定位漏洞，易受新型后门攻击（如VisualTrap），仅需5%污染数据即可生效，且攻击具有跨环境泛化能力。", "motivation": "GUI代理的视觉定位功能可能引入安全漏洞，尤其是后门攻击，目前研究不足。", "method": "提出VisualTrap方法，通过污染预训练数据误导视觉定位，攻击具有隐蔽性和泛化性。", "result": "实验证明，仅需5%污染数据即可高效攻击，且攻击可跨GUI环境迁移。", "conclusion": "GUI代理的后门攻击风险亟需进一步研究。"}}
{"id": "2507.06908", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06908", "abs": "https://arxiv.org/abs/2507.06908", "authors": ["Ziyan Liu", "Chunxiao Fan", "Haoran Lou", "Yuexin Wu", "Kaiwei Deng"], "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection", "comment": "ACL 2025", "summary": "The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.", "AI": {"tldr": "提出了一种名为MIND的多智能体框架，用于零样本有害模因检测，无需依赖标注数据，通过检索相似模因、双向洞察机制和多智能体辩论实现高效检测。", "motivation": "社交媒体的模因快速传播，传统数据驱动方法难以检测新模因，亟需不依赖标注数据的解决方案。", "method": "MIND框架包含三个策略：检索相似模因提供上下文、双向洞察机制提取全面理解、多智能体辩论确保稳健决策。", "result": "在三个模因数据集上，MIND优于现有零样本方法，且在不同模型架构和参数规模下表现优异。", "conclusion": "MIND为有害模因检测提供了可扩展的解决方案，代码已开源。"}}
{"id": "2507.06592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06592", "abs": "https://arxiv.org/abs/2507.06592", "authors": ["Yang Chen", "Yueqi Duan", "Haowen Sun", "Jiwen Lu", "Yap-Peng Tan"], "title": "Ambiguity-aware Point Cloud Segmentation by Adaptive Margin Contrastive Learning", "comment": "This article has been accepted for publication in IEEE Transactions\n  on Multimedia. arXiv admin note: text overlap with arXiv:2502.04111", "summary": "This paper proposes an adaptive margin contrastive learning method for 3D\nsemantic segmentation on point clouds. Most existing methods use equally\npenalized objectives, which ignore the per-point ambiguities and less\ndiscriminated features stemming from transition regions. However, as highly\nambiguous points may be indistinguishable even for humans, their manually\nannotated labels are less reliable, and hard constraints over these points\nwould lead to sub-optimal models. To address this, we first design\nAMContrast3D, a method comprising contrastive learning into an ambiguity\nestimation framework, tailored to adaptive objectives for individual points\nbased on ambiguity levels. As a result, our method promotes model training,\nwhich ensures the correctness of low-ambiguity points while allowing mistakes\nfor high-ambiguity points. As ambiguities are formulated based on position\ndiscrepancies across labels, optimization during inference is constrained by\nthe assumption that all unlabeled points are uniformly unambiguous, lacking\nambiguity awareness. Inspired by the insight of joint training, we further\npropose AMContrast3D++ integrating with two branches trained in parallel, where\na novel ambiguity prediction module concurrently learns point ambiguities from\ngenerated embeddings. To this end, we design a masked refinement mechanism that\nleverages predicted ambiguities to enable the ambiguous embeddings to be more\nreliable, thereby boosting segmentation performance and enhancing robustness.\nExperimental results on 3D indoor scene datasets, S3DIS and ScanNet,\ndemonstrate the effectiveness of the proposed method. Code is available at\nhttps://github.com/YangChenApril/AMContrast3D.", "AI": {"tldr": "提出了一种自适应边距对比学习方法AMContrast3D++，用于点云的3D语义分割，通过模糊度感知优化模型性能。", "motivation": "现有方法对模糊区域的点使用均等惩罚目标，导致模型性能不佳。模糊点的标签不可靠，硬约束会降低模型效果。", "method": "设计AMContrast3D++，结合对比学习和模糊度估计框架，通过并行训练分支和掩码细化机制优化模糊点嵌入。", "result": "在S3DIS和ScanNet数据集上验证了方法的有效性，提升了分割性能和鲁棒性。", "conclusion": "自适应模糊度感知方法显著优于传统均等惩罚方法，代码已开源。"}}
{"id": "2507.06909", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06909", "abs": "https://arxiv.org/abs/2507.06909", "authors": ["Xiao Wang", "Jiahuan Pei", "Diancheng Shui", "Zhiguang Han", "Xin Sun", "Dawei Zhu", "Xiaoyu Shen"], "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction", "comment": "Accepted by NLPCC 2025", "summary": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.", "AI": {"tldr": "论文探讨了法律判决预测中多被告和多罪名是否应分开处理的问题，并提出了MPMCP数据集，评估了四种场景下不同法律大语言模型的性能。", "motivation": "研究多被告和多罪名在法律判决预测中的影响，填补现有研究的空白。", "method": "引入MPMCP数据集，评估四种法律场景下LLMs在罪名预测和刑期预测任务中的表现。", "result": "多被告多罪名场景（S4）最具挑战性，不同模型表现差异显著。", "conclusion": "多被告和多罪名的复杂性对法律判决预测有显著影响，需进一步研究。"}}
{"id": "2507.06910", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.06910", "abs": "https://arxiv.org/abs/2507.06910", "authors": ["Fareya Ikram", "Alexander Scarlatos", "Andrew Lan"], "title": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues", "comment": "Published in BEA 2025: 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications", "summary": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task.", "AI": {"tldr": "论文探讨了现代LLMs（如Llama 3和GPT-4o）在预测辅导对话中导师策略和学生表现的能力，发现现有模型仍有不足。", "motivation": "在线学习普及和AI辅导能力提升背景下，导师策略对学生表现的影响显著，但预测导师策略的研究较少。", "method": "使用两种数学辅导对话数据集，评估Llama 3和GPT-4o预测导师策略和学生表现的能力。", "result": "即使最先进的LLMs也难以准确预测导师策略，但导师策略对学生表现有显著影响。", "conclusion": "需要更强大的方法来预测导师策略及其对学生表现的影响。"}}
{"id": "2507.06603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06603", "abs": "https://arxiv.org/abs/2507.06603", "authors": ["Xu Shaowu", "Jia Xibin", "Gao Junyu", "Sun Qianmei", "Chang Jing", "Fan Chao"], "title": "Cross-Modal Dual-Causal Learning for Long-Term Action Recognition", "comment": null, "summary": "Long-term action recognition (LTAR) is challenging due to extended temporal\nspans with complex atomic action correlations and visual confounders. Although\nvision-language models (VLMs) have shown promise, they often rely on\nstatistical correlations instead of causal mechanisms. Moreover, existing\ncausality-based methods address modal-specific biases but lack cross-modal\ncausal modeling, limiting their utility in VLM-based LTAR. This paper proposes\n\\textbf{C}ross-\\textbf{M}odal \\textbf{D}ual-\\textbf{C}ausal \\textbf{L}earning\n(CMDCL), which introduces a structural causal model to uncover causal\nrelationships between videos and label texts.\n  CMDCL addresses cross-modal biases in text embeddings via textual causal\nintervention and removes confounders inherent in the visual modality through\nvisual causal intervention guided by the debiased text.\n  These dual-causal interventions enable robust action representations to\naddress LTAR challenges. Experimental results on three benchmarks including\nCharades, Breakfast and COIN, demonstrate the effectiveness of the proposed\nmodel. Our code is available at https://github.com/xushaowu/CMDCL.", "AI": {"tldr": "该论文提出了一种跨模态双因果学习（CMDCL）方法，通过结构因果模型揭示视频与标签文本之间的因果关系，以解决长期动作识别中的挑战。", "motivation": "长期动作识别（LTAR）因时间跨度长、动作关联复杂及视觉干扰而具有挑战性。现有方法多依赖统计相关性而非因果机制，且缺乏跨模态因果建模。", "method": "CMDCL通过文本因果干预解决文本嵌入中的跨模态偏差，并通过视觉因果干预去除视觉模态中的干扰因素。", "result": "在Charades、Breakfast和COIN三个基准测试中，CMDCL表现出色，验证了其有效性。", "conclusion": "CMDCL通过双因果干预实现了鲁棒的动作表示，为长期动作识别提供了有效解决方案。"}}
{"id": "2507.06959", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06959", "abs": "https://arxiv.org/abs/2507.06959", "authors": ["Xiao Liang", "Jiawei Hu", "Di Wang", "Zhi Ma", "Lin Zhao", "Ronghan Li", "Bo Wan", "Quan Wang"], "title": "CheXPO: Preference Optimization for Chest X-ray VLMs with Counterfactual Rationale", "comment": null, "summary": "Vision-language models (VLMs) are prone to hallucinations that critically\ncompromise reliability in medical applications. While preference optimization\ncan mitigate these hallucinations through clinical feedback, its implementation\nfaces challenges such as clinically irrelevant training samples, imbalanced\ndata distributions, and prohibitive expert annotation costs. To address these\nchallenges, we introduce CheXPO, a Chest X-ray Preference Optimization strategy\nthat combines confidence-similarity joint mining with counterfactual rationale.\nOur approach begins by synthesizing a unified, fine-grained multi-task chest\nX-ray visual instruction dataset across different question types for supervised\nfine-tuning (SFT). We then identify hard examples through token-level\nconfidence analysis of SFT failures and use similarity-based retrieval to\nexpand hard examples for balancing preference sample distributions, while\nsynthetic counterfactual rationales provide fine-grained clinical preferences,\neliminating the need for additional expert input. Experiments show that CheXPO\nachieves 8.93% relative performance gain using only 5% of SFT samples, reaching\nstate-of-the-art performance across diverse clinical tasks and providing a\nscalable, interpretable solution for real-world radiology applications.", "AI": {"tldr": "CheXPO通过结合置信度-相似性联合挖掘与反事实推理，优化胸部X光偏好，减少幻觉问题，显著提升性能。", "motivation": "视觉语言模型在医学应用中易产生幻觉，影响可靠性，而现有偏好优化方法面临样本无关、数据分布不平衡和专家标注成本高的问题。", "method": "提出CheXPO策略，通过多任务视觉指令数据集进行监督微调，利用置信度分析和相似性检索扩展困难样本，结合反事实推理提供临床偏好。", "result": "实验显示，CheXPO仅用5%的监督微调样本即实现8.93%的性能提升，达到最先进水平。", "conclusion": "CheXPO为放射学应用提供了可扩展、可解释的解决方案。"}}
{"id": "2507.06920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06920", "abs": "https://arxiv.org/abs/2507.06920", "authors": ["Zihan Ma", "Taolin Zhang", "Maosong Cao", "Wenwei Zhang", "Minnan Luo", "Songyang Zhang", "Kai Chen"], "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing", "comment": null, "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.", "AI": {"tldr": "论文提出了一种名为SAGA的人机协作方法，通过多维度指标和TCGBench评估测试用例生成任务，显著提升了测试用例的覆盖率和质量。", "motivation": "现有代码生成评估基准（如HumanEval和LiveCodeBench）的测试用例数量有限且同质化，导致性能测量失真和奖励估计不准确。", "method": "提出多维度指标量化测试套件全面性，并开发SAGA方法结合人类编程经验和LLM推理能力生成高质量测试用例。", "result": "SAGA在TCGBench上实现了90.62%的检测率和32.58%的验证准确率，优于LiveCodeBench-v6。", "conclusion": "SAGA为可靠的LLM代码评估提供了可扩展基础，推动了代码生成中RLVR的发展。"}}
{"id": "2507.06606", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06606", "abs": "https://arxiv.org/abs/2507.06606", "authors": ["Qing Zhang", "Guoquan Pei", "Yan Wang"], "title": "Omni-Fusion of Spatial and Spectral for Hyperspectral Image Segmentation", "comment": null, "summary": "Medical Hyperspectral Imaging (MHSI) has emerged as a promising tool for\nenhanced disease diagnosis, particularly in computational pathology, offering\nrich spectral information that aids in identifying subtle biochemical\nproperties of tissues. Despite these advantages, effectively fusing both\nspatial-dimensional and spectral-dimensional information from MHSIs remains\nchallenging due to its high dimensionality and spectral redundancy inherent\ncharacteristics. To solve the above challenges, we propose a novel\nspatial-spectral omni-fusion network for hyperspectral image segmentation,\nnamed as Omni-Fuse. Here, we introduce abundant cross-dimensional feature\nfusion operations, including a cross-dimensional enhancement module that\nrefines both spatial and spectral features through bidirectional attention\nmechanisms, a spectral-guided spatial query selection to select the most\nspectral-related spatial feature as the query, and a two-stage\ncross-dimensional decoder which dynamically guide the model to focus on the\nselected spatial query. Despite of numerous attention blocks, Omni-Fuse remains\nefficient in execution. Experiments on two microscopic hyperspectral image\ndatasets show that our approach can significantly improve the segmentation\nperformance compared with the state-of-the-art methods, with over 5.73 percent\nimprovement in DSC. Code available at:\nhttps://github.com/DeepMed-Lab-ECNU/Omni-Fuse.", "AI": {"tldr": "提出了一种名为Omni-Fuse的新型空间-光谱全融合网络，用于高光谱图像分割，显著提升了分割性能。", "motivation": "医学高光谱成像（MHSI）在疾病诊断中潜力巨大，但高维度和光谱冗余特性使其空间和光谱信息融合具有挑战性。", "method": "设计了跨维度特征融合操作，包括双向注意力机制、光谱引导的空间查询选择和两阶段跨维度解码器。", "result": "在两个显微高光谱图像数据集上的实验表明，该方法在DSC指标上比现有方法提升了5.73%。", "conclusion": "Omni-Fuse网络在保持高效执行的同时，显著提升了高光谱图像的分割性能。"}}
{"id": "2507.06992", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06992", "abs": "https://arxiv.org/abs/2507.06992", "authors": ["Qilong Xing", "Zikai Song", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "title": "MCA-RG: Enhancing LLMs with Medical Concept Alignment for Radiology Report Generation", "comment": "MICCAI 2025", "summary": "Despite significant advancements in adapting Large Language Models (LLMs) for\nradiology report generation (RRG), clinical adoption remains challenging due to\ndifficulties in accurately mapping pathological and anatomical features to\ntheir corresponding text descriptions. Additionally, semantic agnostic feature\nextraction further hampers the generation of accurate diagnostic reports. To\naddress these challenges, we introduce Medical Concept Aligned Radiology Report\nGeneration (MCA-RG), a knowledge-driven framework that explicitly aligns visual\nfeatures with distinct medical concepts to enhance the report generation\nprocess. MCA-RG utilizes two curated concept banks: a pathology bank containing\nlesion-related knowledge, and an anatomy bank with anatomical descriptions. The\nvisual features are aligned with these medical concepts and undergo tailored\nenhancement. We further propose an anatomy-based contrastive learning procedure\nto improve the generalization of anatomical features, coupled with a matching\nloss for pathological features to prioritize clinically relevant regions.\nAdditionally, a feature gating mechanism is employed to filter out low-quality\nconcept features. Finally, the visual features are corresponding to individual\nmedical concepts, and are leveraged to guide the report generation process.\nExperiments on two public benchmarks (MIMIC-CXR and CheXpert Plus) demonstrate\nthat MCA-RG achieves superior performance, highlighting its effectiveness in\nradiology report generation.", "AI": {"tldr": "MCA-RG是一种知识驱动的框架，通过将视觉特征与医学概念对齐，提升放射学报告生成的准确性。", "motivation": "现有大型语言模型在放射学报告生成中难以准确映射病理和解剖特征到文本描述，且特征提取语义不明确，阻碍了临床采用。", "method": "MCA-RG利用病理库和解剖库对齐视觉特征，提出基于解剖的对比学习和匹配损失优化特征，并采用特征门控机制过滤低质量特征。", "result": "在MIMIC-CXR和CheXpert Plus基准测试中，MCA-RG表现优异。", "conclusion": "MCA-RG通过知识驱动的方法显著提升了放射学报告生成的准确性和临床适用性。"}}
{"id": "2507.06956", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06956", "abs": "https://arxiv.org/abs/2507.06956", "authors": ["Sezen Perçin", "Xin Su", "Qutub Sha Syed", "Phillip Howard", "Aleksei Kuvshinov", "Leo Schwinn", "Kay-Ulrich Scholl"], "title": "Investigating the Robustness of Retrieval-Augmented Generation at the Query Level", "comment": "Accepted to Generation, Evaluation & Metrics (GEM) Workshop at ACL\n  2025", "summary": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed.", "AI": {"tldr": "论文研究了检索增强生成（RAG）系统对查询扰动的敏感性，发现常用检索器性能在轻微查询变化下显著下降，并提出评估框架和改进建议。", "motivation": "大型语言模型（LLMs）更新成本高且效率低，RAG虽能动态整合外部知识，但依赖查询质量，因此需研究其鲁棒性。", "method": "分析RAG管道中各组件对查询扰动的敏感性，研究模块独立及端到端问答效果，提出评估框架并进行大量实验。", "result": "检索器性能在轻微查询变化下显著下降，实验结果为RAG系统改进提供了依据。", "conclusion": "RAG系统对查询扰动敏感，需优化检索模块，提出的评估框架为实践者提供了改进方向。"}}
{"id": "2507.06618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06618", "abs": "https://arxiv.org/abs/2507.06618", "authors": ["Yang Chen", "Yueqi Duan", "Haowen Sun", "Ziwei Wang", "Jiwen Lu", "Yap-Peng Tan"], "title": "PointVDP: Learning View-Dependent Projection by Fireworks Rays for 3D Point Cloud Segmentation", "comment": null, "summary": "In this paper, we propose view-dependent projection (VDP) to facilitate point\ncloud segmentation, designing efficient 3D-to-2D mapping that dynamically\nadapts to the spatial geometry from view variations. Existing projection-based\nmethods leverage view-independent projection in complex scenes, relying on\nstraight lines to generate direct rays or upward curves to reduce occlusions.\nHowever, their view independence provides projection rays that are limited to\npre-defined parameters by human settings, restricting point awareness and\nfailing to capture sufficient projection diversity across different view\nplanes. Although multiple projections per view plane are commonly used to\nenhance spatial variety, the projected redundancy leads to excessive\ncomputational overhead and inefficiency in image processing. To address these\nlimitations, we design a framework of VDP to generate data-driven projections\nfrom 3D point distributions, producing highly informative single-image inputs\nby predicting rays inspired by the adaptive behavior of fireworks. In addition,\nwe construct color regularization to optimize the framework, which emphasizes\nessential features within semantic pixels and suppresses the non-semantic\nfeatures within black pixels, thereby maximizing 2D space utilization in a\nprojected image. As a result, our approach, PointVDP, develops lightweight\nprojections in marginal computation costs. Experiments on S3DIS and ScanNet\nbenchmarks show that our approach achieves competitive results, offering a\nresource-efficient solution for semantic understanding.", "AI": {"tldr": "提出了一种基于视点依赖投影（VDP）的点云分割方法，通过动态适应空间几何生成高效3D到2D映射，解决了现有投影方法因固定参数导致的多样性和计算效率问题。", "motivation": "现有投影方法依赖固定参数，无法适应不同视点的几何变化，导致投影多样性不足和计算冗余。", "method": "设计了VDP框架，通过数据驱动生成自适应投影，并结合颜色正则化优化特征提取。", "result": "在S3DIS和ScanNet基准测试中表现优异，计算成本低。", "conclusion": "PointVDP提供了一种资源高效的语义理解解决方案。"}}
{"id": "2507.06994", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.06994", "abs": "https://arxiv.org/abs/2507.06994", "authors": ["Qilong Xing", "Zikai Song", "Bingxin Gong", "Lian Yang", "Junqing Yu", "Wei Yang"], "title": "Cross-Modality Masked Learning for Survival Prediction in ICI Treated NSCLC Patients", "comment": "MICCAI 2025", "summary": "Accurate prognosis of non-small cell lung cancer (NSCLC) patients undergoing\nimmunotherapy is essential for personalized treatment planning, enabling\ninformed patient decisions, and improving both treatment outcomes and quality\nof life. However, the lack of large, relevant datasets and effective\nmulti-modal feature fusion strategies pose significant challenges in this\ndomain. To address these challenges, we present a large-scale dataset and\nintroduce a novel framework for multi-modal feature fusion aimed at enhancing\nthe accuracy of survival prediction. The dataset comprises 3D CT images and\ncorresponding clinical records from NSCLC patients treated with immune\ncheckpoint inhibitors (ICI), along with progression-free survival (PFS) and\noverall survival (OS) data. We further propose a cross-modality masked learning\napproach for medical feature fusion, consisting of two distinct branches, each\ntailored to its respective modality: a Slice-Depth Transformer for extracting\n3D features from CT images and a graph-based Transformer for learning node\nfeatures and relationships among clinical variables in tabular data. The fusion\nprocess is guided by a masked modality learning strategy, wherein the model\nutilizes the intact modality to reconstruct missing components. This mechanism\nimproves the integration of modality-specific features, fostering more\neffective inter-modality relationships and feature interactions. Our approach\ndemonstrates superior performance in multi-modal integration for NSCLC survival\nprediction, surpassing existing methods and setting a new benchmark for\nprognostic models in this context.", "AI": {"tldr": "提出了一种用于非小细胞肺癌（NSCLC）免疫治疗患者生存预测的多模态特征融合框架，结合3D CT图像和临床数据，通过跨模态掩码学习策略提升预测准确性。", "motivation": "精准预测NSCLC患者的预后对个性化治疗至关重要，但缺乏大规模数据集和有效的多模态特征融合方法。", "method": "构建了一个包含3D CT图像和临床记录的数据集，提出了一种跨模态掩码学习框架，包括Slice-Depth Transformer和基于图的Transformer分支。", "result": "该方法在多模态整合中表现优异，超越了现有方法，为NSCLC预后模型设定了新基准。", "conclusion": "该框架显著提升了NSCLC生存预测的准确性，为个性化治疗提供了有力工具。"}}
{"id": "2507.06974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.06974", "abs": "https://arxiv.org/abs/2507.06974", "authors": ["Artur Muratov", "Hana Fatima Shaikh", "Vanshikaa Jani", "Tarek Mahmoud", "Zhuohan Xie", "Daniil Orel", "Aaryamonvikram Singh", "Yuxia Wang", "Aadi Joshi", "Hasan Iqbal", "Ming Shan Hee", "Dhruv Sahnan", "Nikolaos Nikolaidis", "Purificação Silvano", "Dimitar Dimitrov", "Roman Yangarber", "Ricardo Campos", "Alípio Jorge", "Nuno Guimarães", "Elisa Sartori", "Nicolas Stefanovitch", "Giovanni Da San Martino", "Jakub Piskorski", "Preslav Nakov"], "title": "FRaN-X: FRaming and Narratives-eXplorer", "comment": "19 pages, 13 figures, submitted to EMNLP 2025 - Demo Track", "summary": "We present FRaN-X, a Framing and Narratives Explorer that automatically\ndetects entity mentions and classifies their narrative roles directly from raw\ntext. FRaN-X comprises a two-stage system that combines sequence labeling with\nfine-grained role classification to reveal how entities are portrayed as\nprotagonists, antagonists, or innocents, using a unique taxonomy of 22\nfine-grained roles nested under these three main categories. The system\nsupports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)\nand two domains (the Russia-Ukraine Conflict and Climate Change). It provides\nan interactive web interface for media analysts to explore and compare framing\nacross different sources, tackling the challenge of automatically detecting and\nlabeling how entities are framed. Our system allows end users to focus on a\nsingle article as well as analyze up to four articles simultaneously. We\nprovide aggregate level analysis including an intuitive graph visualization\nthat highlights the narrative a group of articles are pushing. Our system\nincludes a search feature for users to look up entities of interest, along with\na timeline view that allows analysts to track an entity's role transitions\nacross different contexts within the article. The FRaN-X system and the trained\nmodels are licensed under an MIT License. FRaN-X is publicly accessible at\nhttps://fran-x.streamlit.app/ and a video demonstration is available at\nhttps://youtu.be/VZVi-1B6yYk.", "AI": {"tldr": "FRaN-X是一个自动检测实体提及并分类其叙事角色的工具，支持五种语言和两种领域，提供交互式网络界面和可视化分析。", "motivation": "解决自动检测和标记实体如何被框架化的挑战，帮助媒体分析师探索和比较不同来源的框架。", "method": "两阶段系统，结合序列标记和细粒度角色分类，使用22种细粒度角色分类。", "result": "支持多语言和多领域分析，提供交互式界面、搜索功能和时间线视图。", "conclusion": "FRaN-X是一个功能强大的工具，可用于媒体框架分析，并已公开提供使用。"}}
{"id": "2507.06643", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06643", "abs": "https://arxiv.org/abs/2507.06643", "authors": ["Farahdiba Zarin", "Riccardo Oliva", "Vinkle Srivastav", "Armine Vardazaryan", "Andrea Rosati", "Alice Zampolini Faustini", "Giovanni Scambia", "Anna Fagotti", "Pietro Mascagni", "Nicolas Padoy"], "title": "Learning from Sparse Point Labels for Dense Carcinosis Localization in Advanced Ovarian Cancer Assessment", "comment": null, "summary": "Learning from sparse labels is a challenge commonplace in the medical domain.\nThis is due to numerous factors, such as annotation cost, and is especially\ntrue for newly introduced tasks. When dense pixel-level annotations are needed,\nthis becomes even more unfeasible. However, being able to learn from just a few\nannotations at the pixel-level, while extremely difficult and underutilized,\ncan drive progress in studies where perfect annotations are not immediately\navailable. This work tackles the challenge of learning the dense prediction\ntask of keypoint localization from a few point annotations in the context of 2d\ncarcinosis keypoint localization from laparoscopic video frames for diagnostic\nplanning of advanced ovarian cancer patients. To enable this, we formulate the\nproblem as a sparse heatmap regression from a few point annotations per image\nand propose a new loss function, called Crag and Tail loss, for efficient\nlearning. Our proposed loss function effectively leverages positive sparse\nlabels while minimizing the impact of false negatives or missed annotations.\nThrough an extensive ablation study, we demonstrate the effectiveness of our\napproach in achieving accurate dense localization of carcinosis keypoints,\nhighlighting its potential to advance research in scenarios where dense\nannotations are challenging to obtain.", "AI": {"tldr": "论文提出了一种从稀疏标注中学习密集预测任务的方法，针对医学图像中的关键点定位问题，设计了新的损失函数Crag and Tail loss，有效利用稀疏标注并减少假阴性影响。", "motivation": "医学领域中标注成本高，尤其是密集像素级标注难以获取，但稀疏标注的学习能力对研究进展至关重要。", "method": "将问题建模为稀疏热图回归，提出Crag and Tail损失函数，优化稀疏标注的学习效率。", "result": "通过实验验证了该方法在2D腹腔镜视频帧中准确实现癌变关键点定位的能力。", "conclusion": "该方法在稀疏标注场景下表现出色，为医学图像分析提供了新思路。"}}
{"id": "2507.07024", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07024", "abs": "https://arxiv.org/abs/2507.07024", "authors": ["Weijia Shi", "Akshita Bhagia", "Kevin Farhat", "Niklas Muennighoff", "Pete Walsh", "Jacob Morrison", "Dustin Schwenk", "Shayne Longpre", "Jake Poznanski", "Allyson Ettinger", "Daogao Liu", "Margaret Li", "Dirk Groeneveld", "Mike Lewis", "Wen-tau Yih", "Luca Soldaini", "Kyle Lo", "Noah A. Smith", "Luke Zettlemoyer", "Pang Wei Koh", "Hannaneh Hajishirzi", "Ali Farhadi", "Sewon Min"], "title": "FlexOlmo: Open Language Models for Flexible Data Use", "comment": null, "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.", "AI": {"tldr": "FlexOlmo是一种新型语言模型，支持分布式训练和数据灵活推理，无需共享数据或额外训练。", "motivation": "解决在受监管行业中敏感数据无法共享的问题，同时允许数据所有者保持数据本地化并控制访问。", "method": "采用混合专家（MoE）架构，专家独立训练于封闭数据集，通过领域感知路由集成。", "result": "在31个任务上表现优异，平均提升41%，优于现有模型合并方法和标准MoE。", "conclusion": "FlexOlmo为数据所有者和研究者提供了兼顾数据隐私和性能的解决方案。"}}
{"id": "2507.07030", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07030", "abs": "https://arxiv.org/abs/2507.07030", "authors": ["Fengran Mo", "Yifan Gao", "Chuan Meng", "Xin Liu", "Zhuofeng Wu", "Kelong Mao", "Zhengyang Wang", "Pei Chen", "Zheng Li", "Xian Li", "Bing Yin", "Meng Jiang"], "title": "UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations", "comment": "Accepted by ACL 2025 (main)", "summary": "The rapid advancement of conversational search systems revolutionizes how\ninformation is accessed by enabling the multi-turn interaction between the user\nand the system. Existing conversational search systems are usually built with\ntwo different models. This separation restricts the system from leveraging the\nintrinsic knowledge of the models simultaneously, which cannot ensure the\neffectiveness of retrieval benefiting the generation. The existing studies for\ndeveloping unified models cannot fully address the aspects of understanding\nconversational context, managing retrieval independently, and generating\nresponses. In this paper, we explore how to unify dense retrieval and response\ngeneration for large language models in conversation. We conduct joint\nfine-tuning with different objectives and design two mechanisms to reduce the\ninconsistency risks while mitigating data discrepancy. The evaluations on five\nconversational search datasets demonstrate that our unified model can mutually\nimprove both tasks and outperform the existing baselines.", "AI": {"tldr": "论文提出了一种统一密集检索和响应生成的方法，通过联合微调和设计机制减少不一致风险，实验表明该方法优于现有基线。", "motivation": "现有对话搜索系统通常使用两个独立模型，限制了知识共享和检索生成的有效性，需要统一模型解决这一问题。", "method": "联合微调不同目标，设计两种机制以减少不一致风险和数据差异。", "result": "在五个对话搜索数据集上评估，统一模型能同时提升检索和生成任务，优于现有基线。", "conclusion": "统一密集检索和响应生成的方法有效，能显著提升对话搜索系统的性能。"}}
{"id": "2507.06647", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06647", "abs": "https://arxiv.org/abs/2507.06647", "authors": ["Chengkun Li", "Yuqi Tong", "Kai Chen", "Zhenya Yang", "Ruiyang Li", "Shi Qiu", "Jason Ying-Kuen Chan", "Pheng-Ann Heng", "Qi Dou"], "title": "ClipGS: Clippable Gaussian Splatting for Interactive Cinematic Visualization of Volumetric Medical Data", "comment": "Early accepted by MICCAI 2025. Project is available at:\n  https://med-air.github.io/ClipGS", "summary": "The visualization of volumetric medical data is crucial for enhancing\ndiagnostic accuracy and improving surgical planning and education. Cinematic\nrendering techniques significantly enrich this process by providing\nhigh-quality visualizations that convey intricate anatomical details, thereby\nfacilitating better understanding and decision-making in medical contexts.\nHowever, the high computing cost and low rendering speed limit the requirement\nof interactive visualization in practical applications. In this paper, we\nintroduce ClipGS, an innovative Gaussian splatting framework with the clipping\nplane supported, for interactive cinematic visualization of volumetric medical\ndata. To address the challenges posed by dynamic interactions, we propose a\nlearnable truncation scheme that automatically adjusts the visibility of\nGaussian primitives in response to the clipping plane. Besides, we also design\nan adaptive adjustment model to dynamically adjust the deformation of Gaussians\nand refine the rendering performance. We validate our method on five volumetric\nmedical data (including CT and anatomical slice data), and reach an average\n36.635 PSNR rendering quality with 156 FPS and 16.1 MB model size,\noutperforming state-of-the-art methods in rendering quality and efficiency.", "AI": {"tldr": "ClipGS是一个支持裁剪平面的高斯样条框架，用于医学体积数据的交互式电影渲染，通过可学习的截断方案和自适应调整模型提升渲染性能。", "motivation": "医学体积数据的可视化对诊断和手术规划至关重要，但现有方法因计算成本高和渲染速度慢而难以实现交互式可视化。", "method": "提出ClipGS框架，结合可学习的截断方案和自适应调整模型，动态调整高斯基元的可见性和变形。", "result": "在五种医学体积数据上验证，平均PSNR为36.635，帧率为156 FPS，模型大小为16.1 MB，优于现有方法。", "conclusion": "ClipGS在渲染质量和效率上表现优异，适用于医学数据的交互式电影渲染。"}}
{"id": "2507.07029", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.9; H.3.1"], "pdf": "https://arxiv.org/pdf/2507.07029", "abs": "https://arxiv.org/abs/2507.07029", "authors": ["Parshva Dhilankumar Patel"], "title": "Design and Implementation of an OCR-Powered Pipeline for Table Extraction from Invoices", "comment": "17 pages, 23 figures, submitted to arXiv in July 2025", "summary": "This paper presents the design and development of an OCR-powered pipeline for\nefficient table extraction from invoices. The system leverages Tesseract OCR\nfor text recognition and custom post-processing logic to detect, align, and\nextract structured tabular data from scanned invoice documents. Our approach\nincludes dynamic preprocessing, table boundary detection, and row-column\nmapping, optimized for noisy and non-standard invoice formats. The resulting\npipeline significantly improves data extraction accuracy and consistency,\nsupporting real-world use cases such as automated financial workflows and\ndigital archiving.", "AI": {"tldr": "设计了一个基于OCR的流程，用于从发票中高效提取表格数据，结合Tesseract OCR和自定义后处理逻辑，显著提升了数据提取的准确性和一致性。", "motivation": "解决从非标准和噪声较多的发票中提取结构化表格数据的挑战，支持自动化财务流程和数字存档等实际应用。", "method": "采用动态预处理、表格边界检测和行列映射技术，结合Tesseract OCR进行文本识别和自定义后处理。", "result": "显著提高了数据提取的准确性和一致性，适用于实际应用场景。", "conclusion": "提出的OCR流程有效解决了发票表格提取问题，具有实际应用价值。"}}
{"id": "2507.07050", "categories": ["cs.CL", "cs.LG", "stat.ML", "68T50 (Primary) 68Q32, 60J27 (Secondary)", "G.3"], "pdf": "https://arxiv.org/pdf/2507.07050", "abs": "https://arxiv.org/abs/2507.07050", "authors": ["Ashen Weligalle"], "title": "Discrete Diffusion Models for Language Generation", "comment": "pdfLaTeX, 69 pages with 21 figures, Licentiate Thesis", "summary": "Diffusion models have emerged as a powerful class of generative models,\nachieving state-of-the-art results in continuous data domains such as image and\nvideo generation. Their core mechanism involves a forward diffusion process\nthat gradually transforms structured data into a Gaussian-like distribution,\nfollowed by a learned reverse process to reconstruct the data. While successful\nin continuous modalities, applying this framework to discrete data-particularly\nnatural language-remains challenging due to token dependency complexities and\nthe lack of a defined generation order.This thesis investigates the feasibility\nand performance of discrete diffusion models for natural language generation.\nSpecifically, we evaluate the Discrete Denoising Diffusion Probabilistic Model\n(D3PM) and compare it with traditional autoregressive (AR) language models. To\nassess generative performance, we use Bits Per Token (BPT), Negative\nLog-Likelihood (NLL), Perplexity (PPL), and Batch Processing Speed.\n  Results show the best-performing D3PM model achieves a BPT of 5.72, with a\nmean of 8.05. The AR model outperforms in compression with a lower mean BPT of\n4.59, but D3PM achieves higher processing speed, reaching up to 3.97 batches\nper sec., indicating potential for parallel generation.All evaluations were\nconducted under consistent conditions-generating 100,000 tokens per model with\na fixed batch size of four-for fair comparison. This research presents a\ndetailed analysis of diffusion-based vs. autoregressive models, highlighting\ntrade-offs in generative quality and efficiency. Findings emphasize both the\npromise and limitations of diffusion models for discrete data, supporting\nfuture work in non-autoregressive language generation.", "AI": {"tldr": "扩散模型在连续数据领域表现出色，但在离散数据（如自然语言）上应用仍具挑战。本文研究了离散扩散模型（D3PM）在自然语言生成中的表现，并与传统自回归模型（AR）进行了比较。结果显示D3PM在并行生成速度上占优，但AR在压缩性能上更佳。", "motivation": "探索扩散模型在离散数据（如自然语言）上的可行性和性能，以解决传统自回归模型的局限性。", "method": "评估离散去噪扩散概率模型（D3PM）与传统自回归模型（AR），使用Bits Per Token（BPT）、Negative Log-Likelihood（NLL）、Perplexity（PPL）和Batch Processing Speed等指标。", "result": "D3PM在并行生成速度上表现优异（3.97 batches/sec），但AR在压缩性能上更优（BPT均值4.59 vs D3PM的8.05）。", "conclusion": "扩散模型在离散数据生成中具有潜力，尤其在并行生成方面，但需进一步优化以提升生成质量。"}}
{"id": "2507.06651", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06651", "abs": "https://arxiv.org/abs/2507.06651", "authors": ["Juncheng Mu", "Chengwei Ren", "Weixiang Zhang", "Liang Pan", "Xiao-Ping Zhang", "Yue Gao"], "title": "Diff$^2$I2P: Differentiable Image-to-Point Cloud Registration with Diffusion Prior", "comment": "ICCV 2025", "summary": "Learning cross-modal correspondences is essential for image-to-point cloud\n(I2P) registration. Existing methods achieve this mostly by utilizing metric\nlearning to enforce feature alignment across modalities, disregarding the\ninherent modality gap between image and point data. Consequently, this paradigm\nstruggles to ensure accurate cross-modal correspondences. To this end, inspired\nby the cross-modal generation success of recent large diffusion models, we\npropose Diff$^2$I2P, a fully Differentiable I2P registration framework,\nleveraging a novel and effective Diffusion prior for bridging the modality gap.\nSpecifically, we propose a Control-Side Score Distillation (CSD) technique to\ndistill knowledge from a depth-conditioned diffusion model to directly optimize\nthe predicted transformation. However, the gradients on the transformation fail\nto backpropagate onto the cross-modal features due to the non-differentiability\nof correspondence retrieval and PnP solver. To this end, we further propose a\nDeformable Correspondence Tuning (DCT) module to estimate the correspondences\nin a differentiable way, followed by the transformation estimation using a\ndifferentiable PnP solver. With these two designs, the Diffusion model serves\nas a strong prior to guide the cross-modal feature learning of image and point\ncloud for forming robust correspondences, which significantly improves the\nregistration. Extensive experimental results demonstrate that Diff$^2$I2P\nconsistently outperforms SoTA I2P registration methods, achieving over 7%\nimprovement in registration recall on the 7-Scenes benchmark.", "AI": {"tldr": "Diff$^2$I2P提出了一种基于扩散模型的图像到点云（I2P）配准框架，通过控制侧分数蒸馏（CSD）和可变形对应调整（DCT）模块，解决了跨模态特征对齐的挑战，显著提升了配准性能。", "motivation": "现有方法通过度量学习实现跨模态特征对齐，但忽视了图像与点云数据之间的固有模态差异，导致跨模态对应关系不准确。", "method": "提出Diff$^2$I2P框架，利用扩散模型作为先验，结合CSD技术优化变换预测，并通过DCT模块实现可微的对应关系估计和变换求解。", "result": "在7-Scenes基准测试中，Diff$^2$I2P比现有最优方法提高了7%以上的配准召回率。", "conclusion": "Diff$^2$I2P通过扩散模型和可微设计，显著提升了跨模态配准性能，为I2P配准提供了新思路。"}}
{"id": "2507.07073", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07073", "abs": "https://arxiv.org/abs/2507.07073", "authors": ["Yulin An", "Enrique del Castillo"], "title": "An AI Approach for Learning the Spectrum of the Laplace-Beltrami Operator", "comment": "18 pages, 9 figures, submitted for publication", "summary": "The spectrum of the Laplace-Beltrami (LB) operator is central in geometric\ndeep learning tasks, capturing intrinsic properties of the shape of the object\nunder consideration. The best established method for its estimation, from a\ntriangulated mesh of the object, is based on the Finite Element Method (FEM),\nand computes the top k LB eigenvalues with a complexity of O(Nk), where N is\nthe number of points. This can render the FEM method inefficient when\nrepeatedly applied to databases of CAD mechanical parts, or in quality control\napplications where part metrology is acquired as large meshes and decisions\nabout the quality of each part are needed quickly and frequently. As a solution\nto this problem, we present a geometric deep learning framework to predict the\nLB spectrum efficiently given the CAD mesh of a part, achieving significant\ncomputational savings without sacrificing accuracy, demonstrating that the LB\nspectrum is learnable. The proposed Graph Neural Network architecture uses a\nrich set of part mesh features - including Gaussian curvature, mean curvature,\nand principal curvatures. In addition to our trained network, we make\navailable, for repeatability, a large curated dataset of real-world mechanical\nCAD models derived from the publicly available ABC dataset used for training\nand testing. Experimental results show that our method reduces computation time\nof the LB spectrum by approximately 5 times over linear FEM while delivering\ncompetitive accuracy.", "AI": {"tldr": "提出了一种基于几何深度学习的框架，用于高效预测CAD网格的Laplace-Beltrami谱，显著节省计算时间且保持准确性。", "motivation": "传统有限元方法（FEM）在计算Laplace-Beltrami谱时效率较低，尤其在需要频繁处理大型网格数据库或实时质量控制的场景中。", "method": "采用图神经网络架构，结合高斯曲率、平均曲率和主曲率等丰富的网格特征，预测Laplace-Beltrami谱。", "result": "实验表明，该方法比线性FEM节省约5倍计算时间，同时保持竞争力准确性。", "conclusion": "证明了Laplace-Beltrami谱的可学习性，并提供了公开数据集以支持可重复性。"}}
{"id": "2507.06999", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06999", "abs": "https://arxiv.org/abs/2507.06999", "authors": ["Yahan Yu", "Yuyang Dong", "Masafumi Oyamada"], "title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs", "comment": "Work in progress", "summary": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.", "AI": {"tldr": "D2I框架通过规则奖励提升多模态LLMs的推理能力，无需额外标注。", "motivation": "解决多模态推理研究中模态对齐和训练成本高的问题。", "method": "提出D2I框架，训练时使用规则奖励增强模态对齐，评估时转为直觉推理。", "result": "D2I在多个基准测试中优于基线方法。", "conclusion": "D2I展示了规则奖励对提升多模态LLMs推理能力的作用，并启发了训练与测试推理深度的解耦。"}}
{"id": "2507.06656", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.06656", "abs": "https://arxiv.org/abs/2507.06656", "authors": ["Hongjie Wu", "Mingqin Zhang", "Linchao He", "Ji-Zhe Zhou", "Jiancheng Lv"], "title": "Enhancing Diffusion Model Stability for Image Restoration via Gradient Management", "comment": "Accepted to ACM Multimedia 2025. Preprint version", "summary": "Diffusion models have shown remarkable promise for image restoration by\nleveraging powerful priors. Prominent methods typically frame the restoration\nproblem within a Bayesian inference framework, which iteratively combines a\ndenoising step with a likelihood guidance step. However, the interactions\nbetween these two components in the generation process remain underexplored. In\nthis paper, we analyze the underlying gradient dynamics of these components and\nidentify significant instabilities. Specifically, we demonstrate conflicts\nbetween the prior and likelihood gradient directions, alongside temporal\nfluctuations in the likelihood gradient itself. We show that these\ninstabilities disrupt the generative process and compromise restoration\nperformance. To address these issues, we propose Stabilized Progressive\nGradient Diffusion (SPGD), a novel gradient management technique. SPGD\nintegrates two synergistic components: (1) a progressive likelihood warm-up\nstrategy to mitigate gradient conflicts; and (2) adaptive directional momentum\n(ADM) smoothing to reduce fluctuations in the likelihood gradient. Extensive\nexperiments across diverse restoration tasks demonstrate that SPGD\nsignificantly enhances generation stability, leading to state-of-the-art\nperformance in quantitative metrics and visually superior results. Code is\navailable at \\href{https://github.com/74587887/SPGD}{here}.", "AI": {"tldr": "本文分析了扩散模型中先验和似然梯度方向的不稳定性，提出了SPGD方法，通过渐进式似然预热和自适应方向动量平滑来解决这些问题，显著提升了图像恢复性能。", "motivation": "扩散模型在图像恢复中表现出色，但先验和似然梯度方向的冲突及似然梯度的波动导致生成过程不稳定，影响恢复效果。", "method": "提出SPGD方法，包括渐进式似然预热策略和自适应方向动量平滑，以解决梯度冲突和波动问题。", "result": "实验表明，SPGD显著提升了生成稳定性，在定量指标和视觉效果上均达到最优性能。", "conclusion": "SPGD通过梯度管理技术有效解决了扩散模型中的不稳定性问题，为图像恢复任务提供了新的解决方案。"}}
{"id": "2507.06671", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06671", "abs": "https://arxiv.org/abs/2507.06671", "authors": ["Boyuan Tian", "Qizhe Gao", "Siran Xianyu", "Xiaotong Cui", "Minjia Zhang"], "title": "FlexGaussian: Flexible and Cost-Effective Training-Free Compression for 3D Gaussian Splatting", "comment": "To appear at ACM MM 2025", "summary": "3D Gaussian splatting has become a prominent technique for representing and\nrendering complex 3D scenes, due to its high fidelity and speed advantages.\nHowever, the growing demand for large-scale models calls for effective\ncompression to reduce memory and computation costs, especially on mobile and\nedge devices with limited resources. Existing compression methods effectively\nreduce 3D Gaussian parameters but often require extensive retraining or\nfine-tuning, lacking flexibility under varying compression constraints.\n  In this paper, we introduce FlexGaussian, a flexible and cost-effective\nmethod that combines mixed-precision quantization with attribute-discriminative\npruning for training-free 3D Gaussian compression. FlexGaussian eliminates the\nneed for retraining and adapts easily to diverse compression targets.\nEvaluation results show that FlexGaussian achieves up to 96.4% compression\nwhile maintaining high rendering quality (<1 dB drop in PSNR), and is\ndeployable on mobile devices. FlexGaussian delivers high compression ratios\nwithin seconds, being 1.7-2.1x faster than state-of-the-art training-free\nmethods and 10-100x faster than training-involved approaches. The code is being\nprepared and will be released soon at:\nhttps://github.com/Supercomputing-System-AI-Lab/FlexGaussian", "AI": {"tldr": "FlexGaussian是一种无需重新训练的高效3D高斯压缩方法，结合混合精度量化和属性判别剪枝，实现高压缩比（96.4%）和快速部署。", "motivation": "大规模3D模型对压缩的需求增加，现有方法缺乏灵活性且需重新训练。", "method": "采用混合精度量化和属性判别剪枝，无需重新训练。", "result": "压缩比达96.4%，渲染质量损失小（PSNR下降<1 dB），速度比现有方法快1.7-2.1倍。", "conclusion": "FlexGaussian是一种灵活、高效的压缩方法，适用于移动设备。"}}
{"id": "2507.06679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06679", "abs": "https://arxiv.org/abs/2507.06679", "authors": ["Miaojing Shi", "Xiaowen Zhang", "Zijie Yue", "Yong Luo", "Cairong Zhao", "Li Li"], "title": "Text-promptable Object Counting via Quantity Awareness Enhancement", "comment": "13 pages, 5 figures", "summary": "Recent advances in large vision-language models (VLMs) have shown remarkable\nprogress in solving the text-promptable object counting problem. Representative\nmethods typically specify text prompts with object category information in\nimages. This however is insufficient for training the model to accurately\ndistinguish the number of objects in the counting task. To this end, we propose\nQUANet, which introduces novel quantity-oriented text prompts with a\nvision-text quantity alignment loss to enhance the model's quantity awareness.\nMoreover, we propose a dual-stream adaptive counting decoder consisting of a\nTransformer stream, a CNN stream, and a number of Transformer-to-CNN\nenhancement adapters (T2C-adapters) for density map prediction. The\nT2C-adapters facilitate the effective knowledge communication and aggregation\nbetween the Transformer and CNN streams. A cross-stream quantity ranking loss\nis proposed in the end to optimize the ranking orders of predictions from the\ntwo streams. Extensive experiments on standard benchmarks such as FSC-147,\nCARPK, PUCPR+, and ShanghaiTech demonstrate our model's strong generalizability\nfor zero-shot class-agnostic counting. Code is available at\nhttps://github.com/viscom-tongji/QUANet", "AI": {"tldr": "QUANet通过引入数量导向的文本提示和视觉-文本数量对齐损失，提升了模型在计数任务中的数量感知能力，并通过双流自适应计数解码器实现了零样本类无关计数。", "motivation": "现有方法在文本提示中仅提供对象类别信息，不足以训练模型准确区分计数任务中的对象数量。", "method": "提出QUANet，包括数量导向的文本提示、视觉-文本数量对齐损失、双流自适应计数解码器（Transformer流、CNN流和T2C适配器）以及跨流数量排序损失。", "result": "在FSC-147、CARPK、PUCPR+和ShanghaiTech等标准基准测试中表现出强大的零样本类无关计数泛化能力。", "conclusion": "QUANet通过增强数量感知和双流解码器设计，显著提升了计数任务的性能。"}}
{"id": "2507.06689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06689", "abs": "https://arxiv.org/abs/2507.06689", "authors": ["Hao Tang", "Ling Shao", "Zhenyu Zhang", "Luc Van Gool", "Nicu Sebe"], "title": "Spatial-Temporal Graph Mamba for Music-Guided Dance Video Synthesis", "comment": "Accepted to TPAMI 2025", "summary": "We propose a novel spatial-temporal graph Mamba (STG-Mamba) for the\nmusic-guided dance video synthesis task, i.e., to translate the input music to\na dance video. STG-Mamba consists of two translation mappings:\nmusic-to-skeleton translation and skeleton-to-video translation. In the\nmusic-to-skeleton translation, we introduce a novel spatial-temporal graph\nMamba (STGM) block to effectively construct skeleton sequences from the input\nmusic, capturing dependencies between joints in both the spatial and temporal\ndimensions. For the skeleton-to-video translation, we propose a novel\nself-supervised regularization network to translate the generated skeletons,\nalong with a conditional image, into a dance video. Lastly, we collect a new\nskeleton-to-video translation dataset from the Internet, containing 54,944\nvideo clips. Extensive experiments demonstrate that STG-Mamba achieves\nsignificantly better results than existing methods.", "AI": {"tldr": "提出了一种新颖的空间-时间图Mamba（STG-Mamba）模型，用于音乐引导的舞蹈视频合成任务，通过两个映射步骤实现：音乐到骨架的转换和骨架到视频的转换。", "motivation": "解决从音乐生成舞蹈视频的任务，通过捕捉空间和时间维度的依赖关系，提升合成效果。", "method": "1. 音乐到骨架转换：引入空间-时间图Mamba（STGM）块构建骨架序列；2. 骨架到视频转换：提出自监督正则化网络，结合条件图像生成视频。", "result": "实验表明STG-Mamba显著优于现有方法。", "conclusion": "STG-Mamba在音乐到舞蹈视频的合成任务中表现出色，通过创新的空间-时间建模和自监督方法提升了效果。"}}
{"id": "2507.06732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06732", "abs": "https://arxiv.org/abs/2507.06732", "authors": ["Sobhan Asasi", "Mohamed Ilyes Lakhal", "Richard Bowden"], "title": "Hierarchical Feature Alignment for Gloss-Free Sign Language Translation", "comment": "Accepted in SLTAT", "summary": "Sign Language Translation (SLT) attempts to convert sign language videos into\nspoken sentences. However, many existing methods struggle with the disparity\nbetween visual and textual representations during end-to-end learning.\nGloss-based approaches help to bridge this gap by leveraging structured\nlinguistic information. While, gloss-free methods offer greater flexibility and\nremove the burden of annotation, they require effective alignment strategies.\nRecent advances in Large Language Models (LLMs) have enabled gloss-free SLT by\ngenerating text-like representations from sign videos. In this work, we\nintroduce a novel hierarchical pre-training strategy inspired by the structure\nof sign language, incorporating pseudo-glosses and contrastive video-language\nalignment. Our method hierarchically extracts features at frame, segment, and\nvideo levels, aligning them with pseudo-glosses and the spoken sentence to\nenhance translation quality. Experiments demonstrate that our approach improves\nBLEU-4 and ROUGE scores while maintaining efficiency.", "AI": {"tldr": "提出了一种基于伪注释和对比视频-语言对齐的分层预训练策略，用于提升手语翻译质量。", "motivation": "解决现有方法在视觉与文本表示之间的差异问题，同时避免对注释的依赖。", "method": "采用分层特征提取（帧、片段、视频级）并与伪注释和口语对齐。", "result": "实验表明，该方法提高了BLEU-4和ROUGE分数，同时保持高效性。", "conclusion": "分层预训练策略有效提升了手语翻译的性能和灵活性。"}}
{"id": "2507.06733", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06733", "abs": "https://arxiv.org/abs/2507.06733", "authors": ["Mahshid Shiri", "Cigdem Beyan", "Vittorio Murino"], "title": "MADPOT: Medical Anomaly Detection with CLIP Adaptation and Partial Optimal Transport", "comment": "Accepted to ICIAP 2025 (this version is not peer-reviewed; it is the\n  submitted version). ICIAP 2025 proceedings DOI will appear here", "summary": "Medical anomaly detection (AD) is challenging due to diverse imaging\nmodalities, anatomical variations, and limited labeled data. We propose a novel\napproach combining visual adapters and prompt learning with Partial Optimal\nTransport (POT) and contrastive learning (CL) to improve CLIP's adaptability to\nmedical images, particularly for AD. Unlike standard prompt learning, which\noften yields a single representation, our method employs multiple prompts\naligned with local features via POT to capture subtle abnormalities. CL further\nenforces intra-class cohesion and inter-class separation. Our method achieves\nstate-of-the-art results in few-shot, zero-shot, and cross-dataset scenarios\nwithout synthetic data or memory banks. The code is available at\nhttps://github.com/mahshid1998/MADPOT.", "AI": {"tldr": "提出一种结合视觉适配器、提示学习、部分最优传输和对比学习的新方法，提升CLIP在医学图像异常检测中的适应性。", "motivation": "医学异常检测面临成像模态多样、解剖变异大和标记数据有限等挑战，需要更高效的方法。", "method": "采用多提示学习与局部特征对齐的部分最优传输（POT）及对比学习（CL），捕捉细微异常并增强类内凝聚和类间分离。", "result": "在少样本、零样本和跨数据集场景中取得最优结果，无需合成数据或记忆库。", "conclusion": "该方法显著提升了医学图像异常检测的性能，代码已开源。"}}
{"id": "2507.06735", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.06735", "abs": "https://arxiv.org/abs/2507.06735", "authors": ["Guan Zheng", "Xue Wang", "Wenhua Qian", "Peng Liu", "Runzhuo Ma"], "title": "Residual Prior-driven Frequency-aware Network for Image Fusion", "comment": null, "summary": "Image fusion aims to integrate complementary information across modalities to\ngenerate high-quality fused images, thereby enhancing the performance of\nhigh-level vision tasks. While global spatial modeling mechanisms show\npromising results, constructing long-range feature dependencies in the spatial\ndomain incurs substantial computational costs. Additionally, the absence of\nground-truth exacerbates the difficulty of capturing complementary features\neffectively. To tackle these challenges, we propose a Residual Prior-driven\nFrequency-aware Network, termed as RPFNet. Specifically, RPFNet employs a\ndual-branch feature extraction framework: the Residual Prior Module (RPM)\nextracts modality-specific difference information from residual maps, thereby\nproviding complementary priors for fusion; the Frequency Domain Fusion Module\n(FDFM) achieves efficient global feature modeling and integration through\nfrequency-domain convolution. Additionally, the Cross Promotion Module (CPM)\nenhances the synergistic perception of local details and global structures\nthrough bidirectional feature interaction. During training, we incorporate an\nauxiliary decoder and saliency structure loss to strengthen the model's\nsensitivity to modality-specific differences. Furthermore, a combination of\nadaptive weight-based frequency contrastive loss and SSIM loss effectively\nconstrains the solution space, facilitating the joint capture of local details\nand global features while ensuring the retention of complementary information.\nExtensive experiments validate the fusion performance of RPFNet, which\neffectively integrates discriminative features, enhances texture details and\nsalient objects, and can effectively facilitate the deployment of the\nhigh-level vision task.", "AI": {"tldr": "RPFNet通过残差先验驱动的频率感知网络解决图像融合中的计算成本高和互补特征捕获难问题，采用双分支框架和频域卷积实现高效全局建模。", "motivation": "解决图像融合中长距离特征依赖的高计算成本和缺乏真实数据导致的互补特征捕获困难。", "method": "提出RPFNet，包括残差先验模块（RPM）、频域融合模块（FDFM）和交叉促进模块（CPM），结合辅助解码器和显著性结构损失优化训练。", "result": "实验验证RPFNet能有效融合判别特征，增强纹理细节和显著目标，提升高层视觉任务性能。", "conclusion": "RPFNet通过频域建模和残差先验，高效解决了图像融合中的关键问题，提升了融合质量和任务性能。"}}
{"id": "2507.06739", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06739", "abs": "https://arxiv.org/abs/2507.06739", "authors": ["Zishen Huang", "Chunyu Yang", "Mengyuan Ren"], "title": "PromptTea: Let Prompts Tell TeaCache the Optimal Threshold", "comment": null, "summary": "Despite recent progress in video generation, inference speed remains a major\nbottleneck. A common acceleration strategy involves reusing model outputs via\ncaching mechanisms at fixed intervals. However, we find that such\nfixed-frequency reuse significantly degrades quality in complex scenes, while\nmanually tuning reuse thresholds is inefficient and lacks robustness. To\naddress this, we propose Prompt-Complexity-Aware (PCA) caching, a method that\nautomatically adjusts reuse thresholds based on scene complexity estimated\ndirectly from the input prompt. By incorporating prompt-derived semantic cues,\nPCA enables more adaptive and informed reuse decisions than conventional\ncaching methods. We also revisit the assumptions behind TeaCache and identify a\nkey limitation: it suffers from poor input-output relationship modeling due to\nan oversimplified prior. To overcome this, we decouple the noisy input, enhance\nthe contribution of meaningful textual information, and improve the model's\npredictive accuracy through multivariate polynomial feature expansion. To\nfurther reduce computational cost, we replace the static CFGCache with\nDynCFGCache, a dynamic mechanism that selectively reuses classifier-free\nguidance (CFG) outputs based on estimated output variations. This allows for\nmore flexible reuse without compromising output quality. Extensive experiments\ndemonstrate that our approach achieves significant acceleration-for example,\n2.79x speedup on the Wan2.1 model-while maintaining high visual fidelity across\na range of scenes.", "AI": {"tldr": "提出了一种基于提示复杂度的自适应缓存方法（PCA缓存），通过动态调整重用阈值和优化输入输出关系建模，显著提升了视频生成的推理速度，同时保持高质量输出。", "motivation": "固定频率的缓存机制在复杂场景中会导致质量下降，手动调整阈值效率低且缺乏鲁棒性。", "method": "提出PCA缓存，根据输入提示估计场景复杂度自动调整重用阈值；改进TeaCache的输入输出关系建模；引入动态CFGCache机制。", "result": "在Wan2.1模型上实现了2.79倍的加速，同时保持高视觉保真度。", "conclusion": "PCA缓存和动态CFGCache的结合有效解决了推理速度瓶颈，同时保证了生成质量。"}}
{"id": "2507.06744", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.06744", "abs": "https://arxiv.org/abs/2507.06744", "authors": ["Yafei Zhang", "Yongle Shang", "Huafeng Li"], "title": "Dual-Granularity Cross-Modal Identity Association for Weakly-Supervised Text-to-Person Image Matching", "comment": null, "summary": "Weakly supervised text-to-person image matching, as a crucial approach to\nreducing models' reliance on large-scale manually labeled samples, holds\nsignificant research value. However, existing methods struggle to predict\ncomplex one-to-many identity relationships, severely limiting performance\nimprovements. To address this challenge, we propose a local-and-global\ndual-granularity identity association mechanism. Specifically, at the local\nlevel, we explicitly establish cross-modal identity relationships within a\nbatch, reinforcing identity constraints across different modalities and\nenabling the model to better capture subtle differences and correlations. At\nthe global level, we construct a dynamic cross-modal identity association\nnetwork with the visual modality as the anchor and introduce a confidence-based\ndynamic adjustment mechanism, effectively enhancing the model's ability to\nidentify weakly associated samples while improving overall sensitivity.\nAdditionally, we propose an information-asymmetric sample pair construction\nmethod combined with consistency learning to tackle hard sample mining and\nenhance model robustness. Experimental results demonstrate that the proposed\nmethod substantially boosts cross-modal matching accuracy, providing an\nefficient and practical solution for text-to-person image matching.", "AI": {"tldr": "提出了一种局部与全局双粒度身份关联机制，通过跨模态身份关系和动态调整机制提升文本到人物图像匹配的准确性。", "motivation": "解决现有方法在复杂一对多身份关系预测上的不足，减少对大规模手动标注样本的依赖。", "method": "局部层面显式建立跨模态身份关系，全局层面构建动态跨模态身份关联网络，并结合信息不对称样本对构建与一致性学习。", "result": "实验结果表明，该方法显著提升了跨模态匹配的准确性。", "conclusion": "该方法为文本到人物图像匹配提供了高效实用的解决方案。"}}
{"id": "2507.06761", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06761", "abs": "https://arxiv.org/abs/2507.06761", "authors": ["Yan Hon Michael Chung", "Donghyeok Choi"], "title": "Finetuning Vision-Language Models as OCR Systems for Low-Resource Languages: A Case Study of Manchu", "comment": null, "summary": "Manchu, a critically endangered language essential for understanding early\nmodern Eastern Eurasian history, lacks effective OCR systems that can handle\nreal-world historical documents. This study develops high-performing OCR\nsystems by fine-tuning three open-source vision-language models (LLaMA-3.2-11B,\nQwen2.5-VL-7B, Qwen2.5-VL-3B) on 60,000 synthetic Manchu word images using\nparameter-efficient training. LLaMA-3.2-11B achieved exceptional performance\nwith 98.3\\% word accuracy and 0.0024 character error rate on synthetic data,\nwhile crucially maintaining 93.1\\% accuracy on real-world handwritten\ndocuments. Comparative evaluation reveals substantial advantages over\ntraditional approaches: while a CRNN baseline achieved 99.8\\% synthetic\naccuracy, it suffered severe degradation to 72.5\\% on real documents. Our\napproach demonstrates effective synthetic-to-real domain transfer, providing a\ncost-effective solution deployable on accessible infrastructure. This work\nestablishes a transferable framework for endangered language OCR that removes\ntechnical and financial barriers in digital humanities, enabling historians and\nlinguists to process historical archives without specialized computing\nresources. Code and model weights are available at\nhttps://github.com/mic7ch1/ManchuAI-OCR.", "AI": {"tldr": "该研究开发了高性能的OCR系统，用于识别濒危语言满文的历史文档，通过微调开源视觉语言模型，实现了高准确率。", "motivation": "满文作为濒危语言，对理解早期现代东亚历史至关重要，但缺乏有效的OCR系统处理真实历史文档。", "method": "研究通过参数高效训练，在6万张合成的满文单词图像上微调了三个开源视觉语言模型（LLaMA-3.2-11B、Qwen2.5-VL-7B、Qwen2.5-VL-3B）。", "result": "LLaMA-3.2-11B在合成数据上表现优异（98.3%单词准确率），在真实手写文档上保持93.1%准确率，远超传统方法。", "conclusion": "该研究为濒危语言OCR提供了可转移的框架，降低了技术和财务门槛，使历史学家和语言学家无需专业计算资源即可处理历史档案。"}}
{"id": "2507.06797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06797", "abs": "https://arxiv.org/abs/2507.06797", "authors": ["Antonella Barisic Kulas", "Andreja Jurasovic", "Stjepan Bogdan"], "title": "Unlocking Thermal Aerial Imaging: Synthetic Enhancement of UAV Datasets", "comment": "Preprint. Accepted at ECMR 2025", "summary": "Thermal imaging from unmanned aerial vehicles (UAVs) holds significant\npotential for applications in search and rescue, wildlife monitoring, and\nemergency response, especially under low-light or obscured conditions. However,\nthe scarcity of large-scale, diverse thermal aerial datasets limits the\nadvancement of deep learning models in this domain, primarily due to the high\ncost and logistical challenges of collecting thermal data. In this work, we\nintroduce a novel procedural pipeline for generating synthetic thermal images\nfrom an aerial perspective. Our method integrates arbitrary object classes into\nexisting thermal backgrounds by providing control over the position, scale, and\norientation of the new objects, while aligning them with the viewpoints of the\nbackground. We enhance existing thermal datasets by introducing new object\ncategories, specifically adding a drone class in urban environments to the\nHIT-UAV dataset and an animal category to the MONET dataset. In evaluating\nthese datasets for object detection task, we showcase strong performance across\nboth new and existing classes, validating the successful expansion into new\napplications. Through comparative analysis, we show that thermal detectors\noutperform their visible-light-trained counterparts and highlight the\nimportance of replicating aerial viewing angles. Project page:\nhttps://github.com/larics/thermal_aerial_synthetic.", "AI": {"tldr": "提出了一种生成合成热成像数据的流程，用于扩展无人机热成像数据集，提升深度学习模型性能。", "motivation": "无人机热成像在搜索救援等领域潜力巨大，但缺乏大规模多样化数据集限制了模型发展。", "method": "通过将任意物体类别集成到现有热背景中，控制位置、比例和方向，生成合成热图像。", "result": "在HIT-UAV和MONET数据集中新增类别，验证了合成数据在物体检测任务中的有效性。", "conclusion": "合成热成像数据能有效扩展应用场景，热检测器性能优于可见光模型，且需模拟空中视角。"}}
{"id": "2507.06814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06814", "abs": "https://arxiv.org/abs/2507.06814", "authors": ["Qingsen Yan", "Kangbiao Shi", "Yixu Feng", "Tao Hu", "Peng Wu", "Guansong Pang", "Yanning Zhang"], "title": "HVI-CIDNet+: Beyond Extreme Darkness for Low-Light Image Enhancement", "comment": "14 pages", "summary": "Low-Light Image Enhancement (LLIE) aims to restore vivid content and details\nfrom corrupted low-light images. However, existing standard RGB (sRGB) color\nspace-based LLIE methods often produce color bias and brightness artifacts due\nto the inherent high color sensitivity. While Hue, Saturation, and Value (HSV)\ncolor space can decouple brightness and color, it introduces significant red\nand black noise artifacts. To address this problem, we propose a new color\nspace for LLIE, namely Horizontal/Vertical-Intensity (HVI), defined by the HV\ncolor map and learnable intensity. The HV color map enforces small distances\nfor the red coordinates to remove red noise artifacts, while the learnable\nintensity compresses the low-light regions to remove black noise artifacts.\nAdditionally, we introduce the Color and Intensity Decoupling Network+\n(HVI-CIDNet+), built upon the HVI color space, to restore damaged content and\nmitigate color distortion in extremely dark regions. Specifically, HVI-CIDNet+\nleverages abundant contextual and degraded knowledge extracted from low-light\nimages using pre-trained vision-language models, integrated via a novel\nPrior-guided Attention Block (PAB). Within the PAB, latent semantic priors can\npromote content restoration, while degraded representations guide precise color\ncorrection, both particularly in extremely dark regions through the\nmeticulously designed cross-attention fusion mechanism. Furthermore, we\nconstruct a Region Refinement Block that employs convolution for\ninformation-rich regions and self-attention for information-scarce regions,\nensuring accurate brightness adjustments. Comprehensive results from benchmark\nexperiments demonstrate that the proposed HVI-CIDNet+ outperforms the\nstate-of-the-art methods on 10 datasets.", "AI": {"tldr": "提出了一种新的颜色空间HVI和网络HVI-CIDNet+，用于低光图像增强，解决了现有方法的颜色偏差和噪声问题。", "motivation": "现有基于sRGB和HSV颜色空间的低光图像增强方法存在颜色偏差和噪声问题，需要一种更优的解决方案。", "method": "提出HVI颜色空间和HVI-CIDNet+网络，利用HV颜色图和可学习强度减少噪声，并通过预训练的视觉语言模型提取上下文知识。", "result": "在10个基准数据集上，HVI-CIDNet+优于现有方法。", "conclusion": "HVI-CIDNet+有效解决了低光图像增强中的颜色失真和噪声问题，性能显著提升。"}}
{"id": "2507.06848", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06848", "abs": "https://arxiv.org/abs/2507.06848", "authors": ["Joelle Hanna", "Damian Borth"], "title": "Know Your Attention Maps: Class-specific Token Masking for Weakly Supervised Semantic Segmentation", "comment": null, "summary": "Weakly Supervised Semantic Segmentation (WSSS) is a challenging problem that\nhas been extensively studied in recent years. Traditional approaches often rely\non external modules like Class Activation Maps to highlight regions of interest\nand generate pseudo segmentation masks. In this work, we propose an end-to-end\nmethod that directly utilizes the attention maps learned by a Vision\nTransformer (ViT) for WSSS. We propose training a sparse ViT with multiple\n[CLS] tokens (one for each class), using a random masking strategy to promote\n[CLS] token - class assignment. At inference time, we aggregate the different\nself-attention maps of each [CLS] token corresponding to the predicted labels\nto generate pseudo segmentation masks. Our proposed approach enhances the\ninterpretability of self-attention maps and ensures accurate class assignments.\nExtensive experiments on two standard benchmarks and three specialized datasets\ndemonstrate that our method generates accurate pseudo-masks, outperforming\nrelated works. Those pseudo-masks can be used to train a segmentation model\nwhich achieves results comparable to fully-supervised models, significantly\nreducing the need for fine-grained labeled data.", "AI": {"tldr": "提出一种基于Vision Transformer（ViT）的端到端弱监督语义分割方法，利用多[CLS]令牌的自注意力图生成伪分割掩码，性能优于现有方法。", "motivation": "传统弱监督语义分割方法依赖外部模块（如CAM），而ViT的自注意力图具有潜力直接用于分割任务，因此提出利用ViT的自注意力图提升分割性能。", "method": "训练稀疏ViT，每个类别对应一个[CLS]令牌，采用随机掩码策略促进令牌与类别关联。推理时聚合不同[CLS]令牌的自注意力图生成伪分割掩码。", "result": "在两个标准基准和三个专用数据集上表现优异，生成的伪掩码可用于训练分割模型，性能接近全监督模型。", "conclusion": "该方法提升了自注意力图的可解释性和类别分配的准确性，显著减少了对细粒度标注数据的需求。"}}
{"id": "2507.06858", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06858", "abs": "https://arxiv.org/abs/2507.06858", "authors": ["Mathias Schulz", "Alexander Spenke", "Pia Funk", "Florian Blümel", "Markus Rohde", "Ralph Breithaupt", "Gerd Nolden", "Norbert Jung", "Robert Lange"], "title": "Longitudinal Study of Facial Biometrics at the BEZ: Temporal Variance Analysis", "comment": "11 pages, 10 figures, 8 tables", "summary": "This study presents findings from long-term biometric evaluations conducted\nat the Biometric Evaluation Center (bez). Over the course of two and a half\nyears, our ongoing research with over 400 participants representing diverse\nethnicities, genders, and age groups were regularly assessed using a variety of\nbiometric tools and techniques at the controlled testing facilities. Our\nfindings are based on the General Data Protection Regulation-compliant local\nbez database with more than 238.000 biometric data sets categorized into\nmultiple biometric modalities such as face and finger. We used state-of-the-art\nface recognition algorithms to analyze long-term comparison scores. Our results\nshow that these scores fluctuate more significantly between individual days\nthan over the entire measurement period. These findings highlight the\nimportance of testing biometric characteristics of the same individuals over a\nlonger period of time in a controlled measurement environment and lays the\ngroundwork for future advancements in biometric data analysis.", "AI": {"tldr": "长期生物特征评估显示，个体间的日间分数波动比整个测量期间更大，强调了长期测试的重要性。", "motivation": "研究旨在评估长期生物特征数据的稳定性，为生物特征分析提供更可靠的基础。", "method": "使用最先进的人脸识别算法，分析超过238,000个生物特征数据集的长期比较分数。", "result": "个体日间分数波动显著，而整个测量期间的波动较小。", "conclusion": "长期受控环境下的生物特征测试对技术进步至关重要。"}}
{"id": "2507.06906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06906", "abs": "https://arxiv.org/abs/2507.06906", "authors": ["Matthias Zeller", "Daniel Casado Herraez", "Bengisu Ayan", "Jens Behley", "Michael Heidingsfeld", "Cyrill Stachniss"], "title": "SemRaFiner: Panoptic Segmentation in Sparse and Noisy Radar Point Clouds", "comment": "Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)", "summary": "Semantic scene understanding, including the perception and classification of\nmoving agents, is essential to enabling safe and robust driving behaviours of\nautonomous vehicles. Cameras and LiDARs are commonly used for semantic scene\nunderstanding. However, both sensor modalities face limitations in adverse\nweather and usually do not provide motion information. Radar sensors overcome\nthese limitations and directly offer information about moving agents by\nmeasuring the Doppler velocity, but the measurements are comparably sparse and\nnoisy. In this paper, we address the problem of panoptic segmentation in sparse\nradar point clouds to enhance scene understanding. Our approach, called\nSemRaFiner, accounts for changing density in sparse radar point clouds and\noptimizes the feature extraction to improve accuracy. Furthermore, we propose\nan optimized training procedure to refine instance assignments by incorporating\na dedicated data augmentation. Our experiments suggest that our approach\noutperforms state-of-the-art methods for radar-based panoptic segmentation.", "AI": {"tldr": "论文提出了一种名为SemRaFiner的方法，用于稀疏雷达点云的泛光分割，以提升场景理解能力。", "motivation": "自动驾驶车辆需要语义场景理解，但现有传感器（如摄像头和LiDAR）在恶劣天气下受限且无法提供运动信息。雷达传感器虽能克服这些限制，但数据稀疏且噪声多。", "method": "SemRaFiner方法优化了稀疏雷达点云的特征提取，并提出了改进的训练流程，包括专门的数据增强。", "result": "实验表明，该方法在雷达泛光分割任务上优于现有技术。", "conclusion": "SemRaFiner通过优化特征提取和训练流程，显著提升了雷达点云的场景理解能力。"}}
{"id": "2507.06928", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06928", "abs": "https://arxiv.org/abs/2507.06928", "authors": ["Qiyuan Dai", "Hanzhuo Huang", "Yu Wu", "Sibei Yang"], "title": "Adaptive Part Learning for Fine-Grained Generalized Category Discovery: A Plug-and-Play Enhancement", "comment": "Accepted to CVPR 2025", "summary": "Generalized Category Discovery (GCD) aims to recognize unlabeled images from\nknown and novel classes by distinguishing novel classes from known ones, while\nalso transferring knowledge from another set of labeled images with known\nclasses. Existing GCD methods rely on self-supervised vision transformers such\nas DINO for representation learning. However, focusing solely on the global\nrepresentation of the DINO CLS token introduces an inherent trade-off between\ndiscriminability and generalization. In this paper, we introduce an adaptive\npart discovery and learning method, called APL, which generates consistent\nobject parts and their correspondences across different similar images using a\nset of shared learnable part queries and DINO part priors, without requiring\nany additional annotations. More importantly, we propose a novel all-min\ncontrastive loss to learn discriminative yet generalizable part representation,\nwhich adaptively highlights discriminative object parts to distinguish similar\ncategories for enhanced discriminability while simultaneously sharing other\nparts to facilitate knowledge transfer for improved generalization. Our APL can\neasily be incorporated into different GCD frameworks by replacing their CLS\ntoken feature with our part representations, showing significant enhancements\non fine-grained datasets.", "AI": {"tldr": "论文提出了一种自适应部分发现和学习方法（APL），通过共享可学习部分查询和DINO部分先验，生成一致的对象部分及其对应关系，无需额外标注。该方法通过新颖的all-min对比损失学习判别性且可泛化的部分表示，显著提升了GCD框架在细粒度数据集上的性能。", "motivation": "现有GCD方法依赖DINO的全局表示，导致判别性与泛化性之间存在固有权衡。APL旨在通过自适应部分发现和学习解决这一问题。", "method": "APL利用共享可学习部分查询和DINO部分先验生成一致的对象部分及其对应关系，并提出all-min对比损失学习判别性且可泛化的部分表示。", "result": "APL在细粒度数据集上显著提升了GCD框架的性能。", "conclusion": "APL通过自适应部分发现和学习，有效平衡了判别性与泛化性，为GCD任务提供了新思路。"}}
{"id": "2507.06948", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06948", "abs": "https://arxiv.org/abs/2507.06948", "authors": ["Yixin Zhao", "Yuyi Zhang", "Lianwen Jin"], "title": "MCCD: A Multi-Attribute Chinese Calligraphy Character Dataset Annotated with Script Styles, Dynasties, and Calligraphers", "comment": "17 pages, 8 figures, 9 tables, accepted by the 19th International\n  Conference on Document Analysis and Recognition (ICDAR 2025)", "summary": "Research on the attribute information of calligraphy, such as styles,\ndynasties, and calligraphers, holds significant cultural and historical value.\nHowever, the styles of Chinese calligraphy characters have evolved dramatically\nthrough different dynasties and the unique touches of calligraphers, making it\nhighly challenging to accurately recognize these different characters and their\nattributes. Furthermore, existing calligraphic datasets are extremely scarce,\nand most provide only character-level annotations without additional attribute\ninformation. This limitation has significantly hindered the in-depth study of\nChinese calligraphy. To fill this gap, we present a novel Multi-Attribute\nChinese Calligraphy Character Dataset (MCCD). The dataset encompasses 7,765\ncategories with a total of 329,715 isolated image samples of Chinese\ncalligraphy characters, and three additional subsets were extracted based on\nthe attribute labeling of the three types of script styles (10 types),\ndynasties (15 periods) and calligraphers (142 individuals). The rich\nmulti-attribute annotations render MCCD well-suited diverse research tasks,\nincluding calligraphic character recognition, writer identification, and\nevolutionary studies of Chinese characters. We establish benchmark performance\nthrough single-task and multi-task recognition experiments across MCCD and all\nof its subsets. The experimental results demonstrate that the complexity of the\nstroke structure of the calligraphic characters, and the interplay between\ntheir different attributes, leading to a substantial increase in the difficulty\nof accurate recognition. MCCD not only fills a void in the availability of\ndetailed calligraphy datasets but also provides valuable resources for\nadvancing research in Chinese calligraphy and fostering advancements in\nmultiple fields. The dataset is available at\nhttps://github.com/SCUT-DLVCLab/MCCD.", "AI": {"tldr": "论文提出了一个多属性的中国书法字符数据集（MCCD），填补了现有数据集稀缺且缺乏属性信息的空白，支持多种研究任务。", "motivation": "中国书法字符的属性信息（如风格、朝代和书法家）具有重要文化和历史价值，但现有数据集稀缺且缺乏属性标注，阻碍了深入研究。", "method": "构建了包含7,765类329,715个书法字符图像的MCCD数据集，并提取了基于脚本风格、朝代和书法家的三个子集。", "result": "实验表明，书法字符的笔画结构和属性间的相互作用增加了准确识别的难度。", "conclusion": "MCCD填补了详细书法数据集的空白，为书法研究和多领域发展提供了宝贵资源。"}}
{"id": "2507.06949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06949", "abs": "https://arxiv.org/abs/2507.06949", "authors": ["Sebastian Fajardo", "Sina Mohammadi", "Jonas Gregorio de Souza", "César Ardila", "Alan Tapscott Baltar", "Shaddai Heidgen", "Maria Isabel Mayorga Hernández", "Sylvia Mota de Oliveira", "Fernando Montejo", "Marco Moderato", "Vinicius Peripato", "Katy Puche", "Carlos Reina", "Juan Carlos Vargas", "Frank W. Takes", "Marco Madella"], "title": "Pre-Columbian Settlements Shaped Palm Clusters in the Sierra Nevada de Santa Marta, Colombia", "comment": null, "summary": "Ancient populations markedly transformed Neotropical forests, yet\nunderstanding the long-term effects of ancient human management, particularly\nat high-resolution scales, remains challenging. In this work we propose a new\napproach to investigate archaeological areas of influence based on vegetation\nsignatures. It consists of a deep learning model trained on satellite imagery\nto identify palm trees, followed by a clustering algorithm to identify palm\nclusters, which are then used to estimate ancient management areas. To assess\nthe palm distribution in relation to past human activity, we applied the\nproposed approach to unique high-resolution satellite imagery data covering 765\nkm2 of the Sierra Nevada de Santa Marta, Colombia. With this work, we also\nrelease a manually annotated palm tree dataset along with estimated locations\nof archaeological sites from ground-surveys and legacy records. Results\ndemonstrate how palms were significantly more abundant near archaeological\nsites showing large infrastructure investment. The extent of the largest palm\ncluster indicates that ancient human-managed areas linked to major\ninfrastructure sites may be up to two orders of magnitude bigger than indicated\nby archaeological evidence alone. Our findings suggest that pre-Columbian\npopulations influenced local vegetation fostering conditions conducive to palm\nproliferation, leaving a lasting ecological footprint. This may have lowered\nthe logistical costs of establishing infrastructure-heavy settlements in\notherwise less accessible locations. Overall, this study demonstrates the\npotential of integrating artificial intelligence approaches with new ecological\nand archaeological data to identify archaeological areas of interest through\nvegetation patterns, revealing fine-scale human-environment interactions.", "AI": {"tldr": "提出了一种结合深度学习和聚类算法的方法，通过卫星图像识别棕榈树分布，揭示古代人类管理对植被的影响。", "motivation": "研究古代人类对热带森林的长期影响，特别是在高分辨率尺度上，传统方法难以实现。", "method": "使用深度学习模型识别卫星图像中的棕榈树，结合聚类算法分析棕榈树集群，进而估计古代人类管理区域。", "result": "棕榈树在考古遗址附近显著更多，表明古代人类管理区域可能比考古证据显示的大两个数量级。", "conclusion": "该方法展示了人工智能与生态、考古数据结合揭示人类环境互动的潜力，为考古研究提供了新工具。"}}
{"id": "2507.06966", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.06966", "abs": "https://arxiv.org/abs/2507.06966", "authors": ["Sudharsan Madhavan", "Chengcheng Gui", "Lando Bosma", "Josiah Simeth", "Jue Jiang", "Nicolas Cote", "Nima Hassan Rezaeian", "Himanshu Nagar", "Victoria Brennan", "Neelam Tyagi", "Harini Veeraraghavan"], "title": "Segmentation Regularized Training for Multi-Domain Deep Learning Registration applied to MR-Guided Prostate Cancer Radiotherapy", "comment": "Preprint in preparation for submission", "summary": "Background: Accurate deformable image registration (DIR) is required for\ncontour propagation and dose accumulation in MR-guided adaptive radiotherapy\n(MRgART). This study trained and evaluated a deep learning DIR method for\ndomain invariant MR-MR registration. Methods: A progressively refined\nregistration and segmentation (ProRSeg) method was trained with 262 pairs of 3T\nMR simulation scans from prostate cancer patients using weighted segmentation\nconsistency loss. ProRSeg was tested on same- (58 pairs), cross- (72 1.5T MR\nLinac pairs), and mixed-domain (42 MRSim-MRL pairs) datasets for contour\npropagation accuracy of clinical target volume (CTV), bladder, and rectum. Dose\naccumulation was performed for 42 patients undergoing 5-fraction MRgART.\nResults: ProRSeg demonstrated generalization for bladder with similar Dice\nSimilarity Coefficients across domains (0.88, 0.87, 0.86). For rectum and CTV,\nperformance was domain-dependent with higher accuracy on cross-domain MRL\ndataset (DSCs 0.89) versus same-domain data. The model's strong cross-domain\nperformance prompted us to study the feasibility of using it for dose\naccumulation. Dose accumulation showed 83.3% of patients met CTV coverage (D95\n>= 40.0 Gy) and bladder sparing (D50 <= 20.0 Gy) constraints. All patients\nachieved minimum mean target dose (>40.4 Gy), but only 9.5% remained under\nupper limit (<42.0 Gy). Conclusions: ProRSeg showed reasonable multi-domain\nMR-MR registration performance for prostate cancer patients with preliminary\nfeasibility for evaluating treatment compliance to clinical constraints.", "AI": {"tldr": "研究开发了一种名为ProRSeg的深度学习图像配准方法，用于多领域MR-MR配准，并在前列腺癌患者的MR引导自适应放疗中验证了其性能。", "motivation": "精确的可变形图像配准（DIR）在MR引导自适应放疗（MRgART）中至关重要，用于轮廓传播和剂量累积。本研究旨在开发一种能够跨领域工作的深度学习方法。", "method": "采用逐步细化的配准和分割（ProRSeg）方法，使用加权分割一致性损失训练262对3T MR模拟扫描数据，并在同领域、跨领域和混合领域数据集上测试性能。", "result": "ProRSeg在膀胱配准中表现出跨领域的泛化能力（DSC 0.88-0.86），而在直肠和CTV中性能依赖领域。剂量累积结果显示83.3%的患者满足CTV覆盖和膀胱保护约束。", "conclusion": "ProRSeg在多领域MR-MR配准中表现合理，初步展示了其在评估临床约束治疗依从性中的可行性。"}}
{"id": "2507.06972", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06972", "abs": "https://arxiv.org/abs/2507.06972", "authors": ["Johanna Orsholm", "John Quinto", "Hannu Autto", "Gaia Banelyte", "Nicolas Chazot", "Jeremy deWaard", "Stephanie deWaard", "Arielle Farrell", "Brendan Furneaux", "Bess Hardwick", "Nao Ito", "Amlan Kar", "Oula Kalttopää", "Deirdre Kerdraon", "Erik Kristensen", "Jaclyn McKeown", "Tommi Mononen", "Ellen Nein", "Hanna Rogers", "Tomas Roslin", "Paula Schmitz", "Jayme Sones", "Maija Sujala", "Amy Thompson", "Evgeny V. Zakharov", "Iuliia Zarubiieva", "Akshita Gupta", "Scott C. Lowe", "Graham W. Taylor"], "title": "A multi-modal dataset for insect biodiversity with imagery and DNA at the trap and individual level", "comment": "13 pages, 6 figures, submitted to Scientific Data", "summary": "Insects comprise millions of species, many experiencing severe population\ndeclines under environmental and habitat changes. High-throughput approaches\nare crucial for accelerating our understanding of insect diversity, with DNA\nbarcoding and high-resolution imaging showing strong potential for automatic\ntaxonomic classification. However, most image-based approaches rely on\nindividual specimen data, unlike the unsorted bulk samples collected in\nlarge-scale ecological surveys. We present the Mixed Arthropod Sample\nSegmentation and Identification (MassID45) dataset for training automatic\nclassifiers of bulk insect samples. It uniquely combines molecular and imaging\ndata at both the unsorted sample level and the full set of individual\nspecimens. Human annotators, supported by an AI-assisted tool, performed two\ntasks on bulk images: creating segmentation masks around each individual\narthropod and assigning taxonomic labels to over 17 000 specimens. Combining\nthe taxonomic resolution of DNA barcodes with precise abundance estimates of\nbulk images holds great potential for rapid, large-scale characterization of\ninsect communities. This dataset pushes the boundaries of tiny object detection\nand instance segmentation, fostering innovation in both ecological and machine\nlearning research.", "AI": {"tldr": "MassID45数据集结合分子和成像数据，用于训练批量昆虫样本的自动分类器，推动生态和机器学习研究的创新。", "motivation": "昆虫多样性研究面临种群下降和环境变化的挑战，需要高效方法加速分类。", "method": "结合DNA条形码和高分辨率成像，创建包含分割掩码和分类标签的数据集，支持AI辅助标注。", "result": "数据集包含17,000多个标本的标注，为昆虫群落快速大规模表征提供潜力。", "conclusion": "MassID45数据集在微小物体检测和实例分割方面推动技术进步，促进生态与机器学习研究。"}}
{"id": "2507.06973", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06973", "abs": "https://arxiv.org/abs/2507.06973", "authors": ["Qiyuan Dai", "Sibei Yang"], "title": "Free on the Fly: Enhancing Flexibility in Test-Time Adaptation with Online EM", "comment": "Accepted to CVPR 2025", "summary": "Vision-Language Models (VLMs) have become prominent in open-world image\nrecognition for their strong generalization abilities. Yet, their effectiveness\nin practical applications is compromised by domain shifts and distributional\nchanges, especially when test data distributions diverge from training data.\nTherefore, the paradigm of test-time adaptation (TTA) has emerged, enabling the\nuse of online off-the-shelf data at test time, supporting independent sample\npredictions, and eliminating reliance on test annotations. Traditional TTA\nmethods, however, often rely on costly training or optimization processes, or\nmake unrealistic assumptions about accessing or storing historical training and\ntest data. Instead, this study proposes FreeTTA, a training-free and\nuniversally available method that makes no assumptions, to enhance the\nflexibility of TTA. More importantly, FreeTTA is the first to explicitly model\nthe test data distribution, enabling the use of intrinsic relationships among\ntest samples to enhance predictions of individual samples without simultaneous\naccess--a direction not previously explored. FreeTTA achieves these advantages\nby introducing an online EM algorithm that utilizes zero-shot predictions from\nVLMs as priors to iteratively compute the posterior probabilities of each\nonline test sample and update parameters. Experiments demonstrate that FreeTTA\nachieves stable and significant improvements compared to state-of-the-art\nmethods across 15 datasets in both cross-domain and out-of-distribution\nsettings.", "AI": {"tldr": "FreeTTA是一种无需训练且通用的测试时适应方法，通过在线EM算法利用视觉语言模型的零样本预测作为先验，显著提升了跨域和分布外场景下的性能。", "motivation": "视觉语言模型在开放世界图像识别中表现优异，但在实际应用中因域偏移和分布变化而受限。传统测试时适应方法依赖昂贵训练或不现实的假设，FreeTTA旨在解决这些问题。", "method": "FreeTTA提出了一种无需训练的在线EM算法，利用视觉语言模型的零样本预测作为先验，迭代计算测试样本的后验概率并更新参数，无需同时访问历史数据。", "result": "在15个数据集的跨域和分布外实验中，FreeTTA相比现有方法实现了稳定且显著的性能提升。", "conclusion": "FreeTTA首次明确建模测试数据分布，利用样本间内在关系提升预测性能，为测试时适应提供了灵活且高效的解决方案。"}}
{"id": "2507.06976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.06976", "abs": "https://arxiv.org/abs/2507.06976", "authors": ["Sven Teufel", "Dominique Mayer", "Jörg Gamerdinger", "Oliver Bringmann"], "title": "DenoiseCP-Net: Efficient Collective Perception in Adverse Weather via Joint LiDAR-Based 3D Object Detection and Denoising", "comment": null, "summary": "While automated vehicles hold the potential to significantly reduce traffic\naccidents, their perception systems remain vulnerable to sensor degradation\ncaused by adverse weather and environmental occlusions. Collective perception,\nwhich enables vehicles to share information, offers a promising approach to\novercoming these limitations. However, to this date collective perception in\nadverse weather is mostly unstudied. Therefore, we conduct the first study of\nLiDAR-based collective perception under diverse weather conditions and present\na novel multi-task architecture for LiDAR-based collective perception under\nadverse weather. Adverse weather conditions can not only degrade perception\ncapabilities, but also negatively affect bandwidth requirements and latency due\nto the introduced noise that is also transmitted and processed. Denoising prior\nto communication can effectively mitigate these issues. Therefore, we propose\nDenoiseCP-Net, a novel multi-task architecture for LiDAR-based collective\nperception under adverse weather conditions. DenoiseCP-Net integrates\nvoxel-level noise filtering and object detection into a unified sparse\nconvolution backbone, eliminating redundant computations associated with\ntwo-stage pipelines. This design not only reduces inference latency and\ncomputational cost but also minimizes communication overhead by removing\nnon-informative noise. We extended the well-known OPV2V dataset by simulating\nrain, snow, and fog using our realistic weather simulation models. We\ndemonstrate that DenoiseCP-Net achieves near-perfect denoising accuracy in\nadverse weather, reduces the bandwidth requirements by up to 23.6% while\nmaintaining the same detection accuracy and reducing the inference latency for\ncooperative vehicles.", "AI": {"tldr": "研究提出了一种名为DenoiseCP-Net的多任务架构，用于恶劣天气下的LiDAR集体感知，通过噪声过滤和对象检测的统一设计，显著降低了带宽需求和延迟。", "motivation": "自动驾驶车辆的感知系统在恶劣天气下易受传感器性能下降的影响，集体感知虽能缓解这一问题，但相关研究仍不足。", "method": "提出DenoiseCP-Net，结合体素级噪声过滤和对象检测，采用稀疏卷积骨干网络，避免冗余计算。", "result": "在模拟的雨雪雾条件下，DenoiseCP-Net实现了近乎完美的去噪效果，带宽需求降低23.6%，同时保持检测精度和减少延迟。", "conclusion": "DenoiseCP-Net为恶劣天气下的集体感知提供了高效解决方案，显著提升了系统性能。"}}
{"id": "2507.07006", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07006", "abs": "https://arxiv.org/abs/2507.07006", "authors": ["S M Taslim Uddin Raju", "Md. Milon Islam", "Md Rezwanul Haque", "Hamdi Altaheri", "Fakhri Karray"], "title": "GNN-ViTCap: GNN-Enhanced Multiple Instance Learning with Vision Transformers for Whole Slide Image Classification and Captioning", "comment": null, "summary": "Microscopic assessment of histopathology images is vital for accurate cancer\ndiagnosis and treatment. Whole Slide Image (WSI) classification and captioning\nhave become crucial tasks in computer-aided pathology. However, microscopic WSI\nface challenges such as redundant patches and unknown patch positions due to\nsubjective pathologist captures. Moreover, generating automatic pathology\ncaptions remains a significant challenge. To address these issues, we introduce\na novel GNN-ViTCap framework for classification and caption generation from\nhistopathological microscopic images. First, a visual feature extractor\ngenerates patch embeddings. Redundant patches are then removed by dynamically\nclustering these embeddings using deep embedded clustering and selecting\nrepresentative patches via a scalar dot attention mechanism. We build a graph\nby connecting each node to its nearest neighbors in the similarity matrix and\napply a graph neural network to capture both local and global context. The\naggregated image embeddings are projected into the language model's input space\nthrough a linear layer and combined with caption tokens to fine-tune a large\nlanguage model. We validate our method on the BreakHis and PatchGastric\ndatasets. GNN-ViTCap achieves an F1 score of 0.934 and an AUC of 0.963 for\nclassification, along with a BLEU-4 score of 0.811 and a METEOR score of 0.569\nfor captioning. Experimental results demonstrate that GNN-ViTCap outperforms\nstate of the art approaches, offering a reliable and efficient solution for\nmicroscopy based patient diagnosis.", "AI": {"tldr": "提出了一种名为GNN-ViTCap的新框架，用于病理图像的分类和描述生成，解决了冗余补丁和未知位置的问题，并在多个指标上优于现有方法。", "motivation": "病理图像的微观评估对癌症诊断和治疗至关重要，但现有方法面临冗余补丁和未知位置等挑战，且自动生成病理描述仍是一个难题。", "method": "通过视觉特征提取器生成补丁嵌入，动态聚类去除冗余补丁，构建图结构并应用图神经网络捕获上下文，结合语言模型生成描述。", "result": "在BreakHis和PatchGastric数据集上，分类F1分数为0.934，AUC为0.963；描述生成BLEU-4为0.811，METEOR为0.569。", "conclusion": "GNN-ViTCap在病理图像分类和描述生成任务中表现优异，为基于显微镜的患者诊断提供了可靠高效的解决方案。"}}
{"id": "2507.07013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07013", "abs": "https://arxiv.org/abs/2507.07013", "authors": ["Yutong Sun", "Sichen Zhu", "Peng Qiu"], "title": "Integrating Pathology Foundation Models and Spatial Transcriptomics for Cellular Decomposition from Histology Images", "comment": null, "summary": "The rapid development of digital pathology and modern deep learning has\nfacilitated the emergence of pathology foundation models that are expected to\nsolve general pathology problems under various disease conditions in one\nunified model, with or without fine-tuning. In parallel, spatial\ntranscriptomics has emerged as a transformative technology that enables the\nprofiling of gene expression on hematoxylin and eosin (H&E) stained histology\nimages. Spatial transcriptomics unlocks the unprecedented opportunity to dive\ninto existing histology images at a more granular, cellular level. In this\nwork, we propose a lightweight and training-efficient approach to predict\ncellular composition directly from H&E-stained histology images by leveraging\ninformation-enriched feature embeddings extracted from pre-trained pathology\nfoundation models. By training a lightweight multi-layer perceptron (MLP)\nregressor on cell-type abundances derived via cell2location, our method\nefficiently distills knowledge from pathology foundation models and\ndemonstrates the ability to accurately predict cell-type compositions from\nhistology images, without physically performing the costly spatial\ntranscriptomics. Our method demonstrates competitive performance compared to\nexisting methods such as Hist2Cell, while significantly reducing computational\ncomplexity.", "AI": {"tldr": "提出了一种轻量级且训练高效的方法，利用预训练病理基础模型的特征嵌入，直接从H&E染色组织学图像预测细胞组成，避免了昂贵的空间转录组学实验。", "motivation": "数字病理学和深度学习的快速发展为病理基础模型的出现提供了条件，这些模型有望解决多种疾病条件下的通用病理问题。同时，空间转录组学技术的出现为从H&E染色图像中分析基因表达提供了新机会。本研究旨在利用病理基础模型的特征嵌入，高效预测细胞组成，减少对空间转录组学的依赖。", "method": "通过预训练的病理基础模型提取信息丰富的特征嵌入，并训练一个轻量级多层感知器（MLP）回归器，基于cell2location生成的细胞类型丰度数据，预测细胞组成。", "result": "该方法在预测细胞组成方面表现出与现有方法（如Hist2Cell）相当的竞争力，同时显著降低了计算复杂度。", "conclusion": "研究表明，利用病理基础模型的特征嵌入可以高效且准确地预测细胞组成，为组织学图像分析提供了一种低成本、高效的替代方案。"}}
{"id": "2507.07015", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.07015", "abs": "https://arxiv.org/abs/2507.07015", "authors": ["Hui Li", "Pengfei Yang", "Juanyang Chen", "Le Dong", "Yanxin Chen", "Quan Wang"], "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge Distillation", "comment": "Accepted to ACM MM 2025 (The 33rd ACM International Conference on\n  Multimedia)", "summary": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill.", "AI": {"tldr": "MST-Distill提出了一种新型跨模态知识蒸馏框架，通过混合专家教师模型和动态路由网络解决传统方法的局限性，显著提升了跨模态蒸馏性能。", "motivation": "传统知识蒸馏方法在跨模态场景中因数据和统计异质性难以利用跨模态教师模型的互补先验知识。", "method": "提出MST-Distill框架，结合跨模态和多模态教师模型，采用动态路由网络和掩码模块抑制模态差异。", "result": "在五个多模态数据集上的实验表明，MST-Distill显著优于现有方法。", "conclusion": "MST-Distill通过动态教师模型和掩码模块有效解决了知识漂移问题，提升了跨模态知识蒸馏效果。"}}
{"id": "2507.07048", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07048", "abs": "https://arxiv.org/abs/2507.07048", "authors": ["Bruce Coburn", "Jiangpeng He", "Megan E. Rollo", "Satvinder S. Dhaliwal", "Deborah A. Kerr", "Fengqing Zhu"], "title": "Evaluating Large Multimodal Models for Nutrition Analysis: A Benchmark Enriched with Contextual Metadata", "comment": null, "summary": "Large Multimodal Models (LMMs) are increasingly applied to meal images for\nnutrition analysis. However, existing work primarily evaluates proprietary\nmodels, such as GPT-4. This leaves the broad range of LLMs underexplored.\nAdditionally, the influence of integrating contextual metadata and its\ninteraction with various reasoning modifiers remains largely uncharted. This\nwork investigates how interpreting contextual metadata derived from GPS\ncoordinates (converted to location/venue type), timestamps (transformed into\nmeal/day type), and the food items present can enhance LMM performance in\nestimating key nutritional values. These values include calories,\nmacronutrients (protein, carbohydrates, fat), and portion sizes. We also\nintroduce ACETADA, a new food-image dataset slated for public release. This\nopen dataset provides nutrition information verified by the dietitian and\nserves as the foundation for our analysis. Our evaluation across eight LMMs\n(four open-weight and four closed-weight) first establishes the benefit of\ncontextual metadata integration over straightforward prompting with images\nalone. We then demonstrate how this incorporation of contextual information\nenhances the efficacy of reasoning modifiers, such as Chain-of-Thought,\nMultimodal Chain-of-Thought, Scale Hint, Few-Shot, and Expert Persona.\nEmpirical results show that integrating metadata intelligently, when applied\nthrough straightforward prompting strategies, can significantly reduce the Mean\nAbsolute Error (MAE) and Mean Absolute Percentage Error (MAPE) in predicted\nnutritional values. This work highlights the potential of context-aware LMMs\nfor improved nutrition analysis.", "AI": {"tldr": "研究探讨了如何通过整合上下文元数据（如GPS、时间戳和食物信息）提升大型多模态模型（LMMs）在营养分析中的表现，并引入了新数据集ACETADA。实验表明，元数据整合能显著降低预测误差。", "motivation": "现有研究主要评估专有模型（如GPT-4），忽略了其他LLMs的潜力，且上下文元数据与推理修饰符的交互影响尚未充分探索。", "method": "通过整合GPS、时间戳和食物信息等元数据，结合多种推理修饰符（如Chain-of-Thought），评估八种LMMs在营养分析中的表现。", "result": "整合元数据能显著降低预测营养值的平均绝对误差（MAE）和平均绝对百分比误差（MAPE）。", "conclusion": "上下文感知的LMMs在营养分析中具有显著潜力，未来可进一步探索其应用。"}}
{"id": "2507.07077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07077", "abs": "https://arxiv.org/abs/2507.07077", "authors": ["Yimu Pan", "Manas Mehta", "Gwen Sincerbeaux", "Jeffery A. Goldstein", "Alison D. Gernand", "James Z. Wang"], "title": "Reading a Ruler in the Wild", "comment": null, "summary": "Accurately converting pixel measurements into absolute real-world dimensions\nremains a fundamental challenge in computer vision and limits progress in key\napplications such as biomedicine, forensics, nutritional analysis, and\ne-commerce. We introduce RulerNet, a deep learning framework that robustly\ninfers scale \"in the wild\" by reformulating ruler reading as a unified\nkeypoint-detection problem and by representing the ruler with\ngeometric-progression parameters that are invariant to perspective\ntransformations. Unlike traditional methods that rely on handcrafted thresholds\nor rigid, ruler-specific pipelines, RulerNet directly localizes centimeter\nmarks using a distortion-invariant annotation and training strategy, enabling\nstrong generalization across diverse ruler types and imaging conditions while\nmitigating data scarcity. We also present a scalable synthetic-data pipeline\nthat combines graphics-based ruler generation with ControlNet to add\nphotorealistic context, greatly increasing training diversity and improving\nperformance. To further enhance robustness and efficiency, we propose DeepGP, a\nlightweight feed-forward network that regresses geometric-progression\nparameters from noisy marks and eliminates iterative optimization, enabling\nreal-time scale estimation on mobile or edge devices. Experiments show that\nRulerNet delivers accurate, consistent, and efficient scale estimates under\nchallenging real-world conditions. These results underscore its utility as a\ngeneralizable measurement tool and its potential for integration with other\nvision components for automated, scale-aware analysis in high-impact domains. A\nlive demo is available at https://huggingface.co/spaces/ymp5078/RulerNet-Demo.", "AI": {"tldr": "RulerNet是一种深度学习框架，通过将标尺读数统一为关键点检测问题，并结合几何级数参数表示，解决了像素测量转换为实际尺寸的挑战。", "motivation": "在计算机视觉中，将像素测量转换为实际尺寸是一个基础性挑战，限制了生物医学、法医学、营养分析和电子商务等关键应用的发展。", "method": "RulerNet通过关键点检测和几何级数参数表示标尺，采用抗畸变标注和训练策略，并结合合成数据增强训练多样性。", "result": "实验表明，RulerNet在多样化标尺类型和成像条件下表现出色，能够高效、准确地估计尺度。", "conclusion": "RulerNet作为一种通用测量工具，具有广泛的应用潜力，并可与其他视觉组件集成，实现自动化、尺度感知的分析。"}}
{"id": "2507.07079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07079", "abs": "https://arxiv.org/abs/2507.07079", "authors": ["Ziyue Liu", "Federico Girella", "Yiming Wang", "Davide Talon"], "title": "Evaluating Attribute Confusion in Fashion Text-to-Image Generation", "comment": "Accepted to ICIAP25. Project page: site\n  [https://intelligolabs.github.io/L-VQAScore/\\", "summary": "Despite the rapid advances in Text-to-Image (T2I) generation models, their\nevaluation remains challenging in domains like fashion, involving complex\ncompositional generation. Recent automated T2I evaluation methods leverage\npre-trained vision-language models to measure cross-modal alignment. However,\nour preliminary study reveals that they are still limited in assessing rich\nentity-attribute semantics, facing challenges in attribute confusion, i.e.,\nwhen attributes are correctly depicted but associated to the wrong entities. To\naddress this, we build on a Visual Question Answering (VQA) localization\nstrategy targeting one single entity at a time across both visual and textual\nmodalities. We propose a localized human evaluation protocol and introduce a\nnovel automatic metric, Localized VQAScore (L-VQAScore), that combines visual\nlocalization with VQA probing both correct (reflection) and miss-localized\n(leakage) attribute generation. On a newly curated dataset featuring\nchallenging compositional alignment scenarios, L-VQAScore outperforms\nstate-of-the-art T2I evaluation methods in terms of correlation with human\njudgments, demonstrating its strength in capturing fine-grained\nentity-attribute associations. We believe L-VQAScore can be a reliable and\nscalable alternative to subjective evaluations.", "AI": {"tldr": "论文提出了一种新的自动评估方法L-VQAScore，用于解决文本到图像生成模型中实体-属性语义评估的局限性，通过结合视觉定位和VQA技术，显著提升了评估的准确性。", "motivation": "现有文本到图像生成模型的评估方法在复杂组合生成（如时尚领域）中存在局限性，尤其是在实体-属性语义的评估上，容易出现属性混淆问题。", "method": "提出了一种基于视觉问答（VQA）定位策略的方法，结合视觉定位和VQA技术，开发了新的自动评估指标L-VQAScore，专注于单个实体的属性生成评估。", "result": "在新构建的数据集上，L-VQAScore在与人评估的相关性上优于现有方法，能够更准确地捕捉细粒度的实体-属性关联。", "conclusion": "L-VQAScore是一种可靠且可扩展的评估方法，有望替代主观评估，提升文本到图像生成模型的评估效果。"}}
{"id": "2507.07095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07095", "abs": "https://arxiv.org/abs/2507.07095", "authors": ["Ke Fan", "Shunlin Lu", "Minyue Dai", "Runyi Yu", "Lixing Xiao", "Zhiyang Dou", "Junting Dong", "Lizhuang Ma", "Jingbo Wang"], "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data", "comment": "Project Page: https://vankouf.github.io/MotionMillion/", "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.", "AI": {"tldr": "论文提出了一种基于文本描述生成多样化自然人体运动序列的方法，通过构建大规模数据集和评估框架，实现了零样本泛化能力。", "motivation": "当前方法在零样本泛化能力上存在不足，且缺乏全面的评估框架，阻碍了任务的发展。", "method": "开发高效标注流程，构建MotionMillion数据集（最大人体运动数据集），提出MotionMillion-Eval评估基准，并采用可扩展架构训练7B参数模型。", "result": "模型在MotionMillion-Eval上表现出色，能够泛化到域外和复杂组合运动。", "conclusion": "该方法为零样本人体运动生成迈出了重要一步，代码已开源。"}}
{"id": "2507.07104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07104", "abs": "https://arxiv.org/abs/2507.07104", "authors": ["Tiezheng Zhang", "Yitong Li", "Yu-cheng Chou", "Jieneng Chen", "Alan Yuille", "Chen Wei", "Junfei Xiao"], "title": "Vision-Language-Vision Auto-Encoder: Scalable Knowledge Distillation from Diffusion Models", "comment": null, "summary": "Building state-of-the-art Vision-Language Models (VLMs) with strong\ncaptioning capabilities typically necessitates training on billions of\nhigh-quality image-text pairs, requiring millions of GPU hours. This paper\nintroduces the Vision-Language-Vision (VLV) auto-encoder framework, which\nstrategically leverages key pretrained components: a vision encoder, the\ndecoder of a Text-to-Image (T2I) diffusion model, and subsequently, a Large\nLanguage Model (LLM). Specifically, we establish an information bottleneck by\nregularizing the language representation space, achieved through freezing the\npretrained T2I diffusion decoder. Our VLV pipeline effectively distills\nknowledge from the text-conditioned diffusion model using continuous\nembeddings, demonstrating comprehensive semantic understanding via high-quality\nreconstructions. Furthermore, by fine-tuning a pretrained LLM to decode the\nintermediate language representations into detailed descriptions, we construct\na state-of-the-art (SoTA) captioner comparable to leading models like GPT-4o\nand Gemini 2.0 Flash. Our method demonstrates exceptional cost-efficiency and\nsignificantly reduces data requirements; by primarily utilizing single-modal\nimages for training and maximizing the utility of existing pretrained models\n(image encoder, T2I diffusion model, and LLM), it circumvents the need for\nmassive paired image-text datasets, keeping the total training expenditure\nunder $1,000 USD.", "AI": {"tldr": "提出了一种名为VLV的自动编码器框架，通过利用预训练组件（视觉编码器、T2I扩散模型解码器和LLM）减少训练成本，实现高效视觉语言模型。", "motivation": "传统视觉语言模型需要大量高质量图像-文本对和GPU资源，成本高昂。VLV框架旨在通过利用现有预训练模型减少数据需求和训练成本。", "method": "通过冻结预训练的T2I扩散解码器建立信息瓶颈，利用连续嵌入从扩散模型中提取知识，并微调LLM生成详细描述。", "result": "VLV框架在保持高性能的同时，显著降低了训练成本（低于1000美元），并减少了对大规模配对数据的需求。", "conclusion": "VLV框架提供了一种高效、低成本的视觉语言模型训练方法，性能媲美主流模型如GPT-4o和Gemini 2.0 Flash。"}}
{"id": "2507.07106", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07106", "abs": "https://arxiv.org/abs/2507.07106", "authors": ["Vatsal Agarwal", "Matthew Gwilliam", "Gefen Kohavi", "Eshan Verma", "Daniel Ulbricht", "Abhinav Shrivastava"], "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware Feature Extractor", "comment": "Website: see https://vatsalag99.github.io/mustafar/", "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.", "AI": {"tldr": "该论文探讨了使用预训练的文本到图像扩散模型作为视觉编码器的潜力，以弥补CLIP在捕捉细粒度细节上的不足，并提出了一种融合策略来提升视觉问答任务的性能。", "motivation": "现有基于CLIP的多模态大语言模型（MLLMs）在视觉问答中常忽略细粒度细节，因此研究扩散模型是否能作为更有效的视觉编码器。", "method": "分析扩散模型的内部表示，利用其语义丰富性和图像-文本对齐能力，结合文本条件聚焦问题相关区域，并与大语言模型对齐。提出了一种融合CLIP和扩散特征的策略。", "result": "在通用VQA和专用MLLM基准测试中，扩散模型表现出色，尤其在需要空间和组合推理的视觉中心任务中。", "conclusion": "扩散模型在视觉理解任务中具有潜力，特别是在需要细粒度分析的场景中，融合策略能有效提升性能。"}}
