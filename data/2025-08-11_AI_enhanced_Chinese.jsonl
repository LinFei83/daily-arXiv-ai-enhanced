{"id": "2508.06137", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06137", "abs": "https://arxiv.org/abs/2508.06137", "authors": ["Ojonugwa Oluwafemi Ejiga Peter", "Daniel Emakporuena", "Bamidele Dayo Tunde", "Maryam Abdulkarim", "Abdullahi Bn Umar"], "title": "Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework", "comment": null, "summary": "Breast cancer detection through mammography interpretation remains difficult\nbecause of the minimal nature of abnormalities that experts need to identify\nalongside the variable interpretations between readers. The potential of CNNs\nfor medical image analysis faces two limitations: they fail to process both\nlocal information and wide contextual data adequately, and do not provide\nexplainable AI (XAI) operations that doctors need to accept them in clinics.\nThe researcher developed the MammoFormer framework, which unites\ntransformer-based architecture with multi-feature enhancement components and\nXAI functionalities within one framework. Seven different architectures\nconsisting of CNNs, Vision Transformer, Swin Transformer, and ConvNext were\ntested alongside four enhancement techniques, including original images,\nnegative transformation, adaptive histogram equalization, and histogram of\noriented gradients. The MammoFormer framework addresses critical clinical\nadoption barriers of AI mammography systems through: (1) systematic\noptimization of transformer architectures via architecture-specific feature\nenhancement, achieving up to 13% performance improvement, (2) comprehensive\nexplainable AI integration providing multi-perspective diagnostic\ninterpretability, and (3) a clinically deployable ensemble system combining CNN\nreliability with transformer global context modeling. The combination of\ntransformer models with suitable feature enhancements enables them to achieve\nequal or better results than CNN approaches. ViT achieves 98.3% accuracy\nalongside AHE while Swin Transformer gains a 13.0% advantage through HOG\nenhancements", "AI": {"tldr": "MammoFormer框架结合Transformer、多特征增强和可解释AI，旨在提高乳腺癌乳腺X线图像诊断的准确性和临床可接受性。", "motivation": "乳腺X线图像判读因异常微小和判读差异而困难；现有CNN模型在处理局部与全局信息及提供可解释AI方面存在局限，阻碍了其在临床中的应用。", "method": "研究者开发了MammoFormer框架，该框架整合了基于Transformer的架构、多特征增强组件和可解释AI功能。测试了七种不同架构（CNNs, Vision Transformer, Swin Transformer, ConvNext）和四种图像增强技术（原始图像、负片变换、自适应直方图均衡化、方向梯度直方图）。", "result": "MammoFormer通过架构特定的特征增强系统优化Transformer架构，性能提升高达13%；集成了多视角诊断可解释AI；构建了结合CNN可靠性和Transformer全局上下文建模的临床可部署集成系统。Transformer模型与合适的特征增强结合，可达到或超越CNN方法。ViT在AHE增强下达到98.3%准确率，Swin Transformer在HOG增强下性能提升13.0%。", "conclusion": "MammoFormer框架通过系统优化Transformer架构、集成可解释AI和构建集成系统，有效解决了AI乳腺X线系统在临床应用中的关键障碍。Transformer模型结合适当的特征增强，在乳腺癌检测中能取得与CNN相当或更优的结果。"}}
{"id": "2508.06182", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06182", "abs": "https://arxiv.org/abs/2508.06182", "authors": ["Chiara Baldini", "Kaisar Kushibar", "Richard Osuala", "Simone Balocco", "Oliver Diaz", "Karim Lekadir", "Leonardo S. Mattos"], "title": "Clinically-guided Data Synthesis for Laryngeal Lesion Detection", "comment": null, "summary": "Although computer-aided diagnosis (CADx) and detection (CADe) systems have\nmade significant progress in various medical domains, their application is\nstill limited in specialized fields such as otorhinolaryngology. In the latter,\ncurrent assessment methods heavily depend on operator expertise, and the high\nheterogeneity of lesions complicates diagnosis, with biopsy persisting as the\ngold standard despite its substantial costs and risks. A critical bottleneck\nfor specialized endoscopic CADx/e systems is the lack of well-annotated\ndatasets with sufficient variability for real-world generalization. This study\nintroduces a novel approach that exploits a Latent Diffusion Model (LDM)\ncoupled with a ControlNet adapter to generate laryngeal endoscopic\nimage-annotation pairs, guided by clinical observations. The method addresses\ndata scarcity by conditioning the diffusion process to produce realistic,\nhigh-quality, and clinically relevant image features that capture diverse\nanatomical conditions. The proposed approach can be leveraged to expand\ntraining datasets for CADx/e models, empowering the assessment process in\nlaryngology. Indeed, during a downstream task of detection, the addition of\nonly 10% synthetic data improved the detection rate of laryngeal lesions by 9%\nwhen the model was internally tested and 22.1% on out-of-domain external data.\nAdditionally, the realism of the generated images was evaluated by asking 5\nexpert otorhinolaryngologists with varying expertise to rate their confidence\nin distinguishing synthetic from real images. This work has the potential to\naccelerate the development of automated tools for laryngeal disease diagnosis,\noffering a solution to data scarcity and demonstrating the applicability of\nsynthetic data in real-world scenarios.", "AI": {"tldr": "本研究提出了一种利用潜在扩散模型（LDM）结合ControlNet生成喉内窥镜图像-标注对的方法，以解决耳鼻喉科CADx/e系统数据稀缺问题，并显著提高了病灶检测率。", "motivation": "耳鼻喉科的计算机辅助诊断/检测（CADx/e）系统应用受限，主要原因是当前评估高度依赖专家经验，病灶异质性高，活检成本高昂且有风险，以及缺乏足够多变且高质量标注的数据集。", "method": "该研究引入了一种新方法，利用潜在扩散模型（LDM）与ControlNet适配器相结合，在临床观察指导下生成喉内窥镜图像-标注对。通过条件化扩散过程，生成逼真、高质量且具有临床相关特征的图像，以捕获多样化的解剖条件，从而扩充训练数据集。", "result": "在下游检测任务中，仅添加10%的合成数据，内部测试时喉部病灶检测率提高了9%，在域外外部数据上则提高了22.1%。此外，通过邀请5位耳鼻喉科专家评估，验证了生成图像的真实性。", "conclusion": "这项工作为喉部疾病诊断的自动化工具开发提供了解决方案，有效应对了数据稀缺问题，并展示了合成数据在真实世界场景中的实用性和潜力，有望加速该领域的发展。"}}
{"id": "2508.06281", "categories": ["eess.IV", "65", "G.1.8; I.4.5; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.06281", "abs": "https://arxiv.org/abs/2508.06281", "authors": ["Alexander Denker", "Fabio Margotti", "Jianfeng Ning", "Kim Knudsen", "Derick Nganyu Tanyu", "Bangti Jin", "Andreas Hauptmann", "Peter Maass"], "title": "Deep Learning Based Reconstruction Methods for Electrical Impedance Tomography", "comment": null, "summary": "Electrical Impedance Tomography (EIT) is a powerful imaging modality widely\nused in medical diagnostics, industrial monitoring, and environmental studies.\nThe EIT inverse problem is about inferring the internal conductivity\ndistribution of the concerned object from the voltage measurements taken on its\nboundary. This problem is severely ill-posed, and requires advanced\ncomputational approaches for accurate and reliable image reconstruction. Recent\ninnovations in both model-based reconstruction and deep learning have driven\nsignificant progress in the field. In this review, we explore learned\nreconstruction methods that employ deep neural networks for solving the EIT\ninverse problem. The discussion focuses on the complete electrode model, one\npopular mathematical model for real-world applications of EIT. We compare a\nwide variety of learned approaches, including fully-learned, post-processing\nand learned iterative methods, with several conventional model-based\nreconstruction techniques, e.g., sparsity regularization, regularized\nGauss-Newton iteration and level set method. The evaluation is based on three\ndatasets: a simulated dataset of ellipses, an out-of-distribution simulated\ndataset, and the KIT4 dataset, including real-world measurements. Our results\ndemonstrate that learned methods outperform model-based methods for\nin-distribution data but face challenges in generalization, where hybrid\nmethods exhibit a good balance of accuracy and adaptability.", "AI": {"tldr": "本文综述了使用深度学习方法解决电气阻抗断层扫描（EIT）逆问题，并与传统模型方法进行比较，发现学习方法在同分布数据上表现更好，但在泛化性上存在挑战，而混合方法则在准确性和适应性之间取得了良好平衡。", "motivation": "电气阻抗断层扫描（EIT）逆问题是一个严重不适定问题，需要先进的计算方法来实现准确可靠的图像重建。近年来，模型重建和深度学习领域的创新推动了该领域的显著进展。", "method": "本综述探讨了利用深度神经网络解决EIT逆问题的学习重建方法，重点关注完全电极模型。比较了多种学习方法（包括全学习、后处理和学习迭代方法）与几种传统模型方法（如稀疏正则化、正则化高斯-牛顿迭代和水平集方法）。评估基于三个数据集：模拟椭圆数据集、域外模拟模拟数据集和包含真实世界测量的KIT4数据集。", "result": "研究结果表明，学习方法在同分布数据上优于模型方法，但在泛化性方面面临挑战。混合方法在准确性和适应性之间表现出良好的平衡。", "conclusion": "学习方法在特定数据分布上表现出色，但泛化能力有待提高。混合方法在解决EIT逆问题时，能够在准确性和适应性之间取得良好平衡，是未来研究的一个有前景的方向。"}}
{"id": "2508.06287", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06287", "abs": "https://arxiv.org/abs/2508.06287", "authors": ["Mobarak Abumohsen", "Enrique Costa-Montenegro", "Silvia García-Méndez", "Amani Yousef Owda", "Majdi Owda"], "title": "Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification", "comment": null, "summary": "Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one\nof the most common causes of death for men and women worldwide. Computed\nTomography (CT) images are the most preferred diagnosis method because of their\nlow cost and their faster processing times. Many researchers have proposed\nvarious ways of identifying lung cancer using CT images. However, such\ntechniques suffer from significant false positives, leading to low accuracy.\nThe fundamental reason results from employing a small and imbalanced dataset.\nThis paper introduces an innovative approach for LC detection and\nclassification from CT images based on the DenseNet201 model. Our approach\ncomprises several advanced methods such as Focal Loss, data augmentation, and\nregularization to overcome the imbalanced data issue and overfitting challenge.\nThe findings show the appropriateness of the proposal, attaining a promising\nperformance of 98.95% accuracy.", "AI": {"tldr": "该论文提出了一种基于DenseNet201模型结合Focal Loss、数据增强和正则化的肺癌CT图像检测与分类方法，旨在解决数据不平衡和过拟合问题，并取得了98.95%的准确率。", "motivation": "肺癌是全球最常见的癌症之一和主要死因。CT图像是首选诊断方法，但现有技术因数据集小且不平衡而导致假阳性高、准确率低。", "method": "该研究引入了一种基于DenseNet201模型的肺癌检测和分类方法，并结合了Focal Loss、数据增强和正则化等先进方法来克服数据不平衡和过拟合挑战。", "result": "该方法在肺癌检测和分类中取得了98.95%的准确率，表现出良好的性能。", "conclusion": "研究结果表明所提出的方法对于肺癌CT图像的检测和分类是合适的且有效的。"}}
{"id": "2508.05827", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.05827", "abs": "https://arxiv.org/abs/2508.05827", "authors": ["Tony Kinchen", "Panagiotis Typaldos", "Andreas A. Malikopoulos"], "title": "A United Framework for Planning Electric Vehicle Charging Accessibility", "comment": null, "summary": "The shift towards electric vehicles (EVs) is crucial for establishing\nsustainable and low-emission urban transportation systems. However, the success\nof this transition depends on the strategic placement of the charging\ninfrastructure. This paper addresses the challenge of optimizing charging\nstation locations in dense urban environments while balancing efficiency with\nspatial accessibility. We propose an optimization framework that integrates\ntraffic simulation, energy consumption modeling, and a mobility equity measure\nto evaluate the social reach of each potential charging station. Using New York\nCity as a case study, we demonstrate consistent improvements in accessibility\n(15-20% reduction in travel time variability). Our results provide a scalable\nmethodology for incorporating equity considerations into EV infrastructure\nplanning, although economic factors and grid integration remain important areas\nfor future development.", "AI": {"tldr": "该研究提出了一个优化框架，用于在城市环境中平衡效率和空间可达性，以战略性地部署电动汽车充电站，并考虑了出行公平性。", "motivation": "电动汽车转型对可持续城市交通至关重要，但其成功取决于充电基础设施的战略布局。在密集的城市环境中，如何优化充电站位置以平衡效率和空间可达性是一个挑战。", "method": "本文提出了一个优化框架，该框架整合了交通模拟、能源消耗建模和出行公平性衡量指标，以评估每个潜在充电站的社会覆盖范围。研究以纽约市作为案例研究。", "result": "结果表明，该方法能够持续改善可达性（出行时间变异性减少15-20%）。", "conclusion": "研究提供了一种可扩展的方法，将公平性考虑纳入电动汽车基础设施规划。未来的发展方向包括纳入经济因素和电网集成考量。"}}
{"id": "2508.05773", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05773", "abs": "https://arxiv.org/abs/2508.05773", "authors": ["Keyvan Majd", "Hardik Parwana", "Bardh Hoxha", "Steven Hong", "Hideki Okamoto", "Georgios Fainekos"], "title": "GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems", "comment": "Accepted to IEEE ITSC 2025", "summary": "Articulated vehicles such as tractor-trailers, yard trucks, and similar\nplatforms must often reverse and maneuver in cluttered spaces where pedestrians\nare present. We present how Barrier-Rate guided Model Predictive Path Integral\n(BR-MPPI) control can solve navigation in such challenging environments.\nBR-MPPI embeds Control Barrier Function (CBF) constraints directly into the\npath-integral update. By steering the importance-sampling distribution toward\ncollision-free, dynamically feasible trajectories, BR-MPPI enhances the\nexploration strength of MPPI and improves robustness of resulting trajectories.\nThe method is evaluated in the high-fidelity CarMaker simulator on a 12 [m]\ntractor-trailer tasked with reverse and forward parking in a parking lot.\nBR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for\nscenarios with eight obstacles) and maintains better parking clearance than a\nstandard MPPI baseline and an MPPI with collision cost baseline.", "AI": {"tldr": "本文提出了一种名为BR-MPPI（Barrier-Rate guided Model Predictive Path Integral）的控制方法，用于在复杂环境中引导铰接式车辆（如半挂车）进行倒车和机动，该方法通过嵌入控制障碍函数（CBF）来增强路径探索和轨迹鲁棒性。", "motivation": "铰接式车辆（如半挂车、场内卡车）经常需要在行人存在的拥挤空间中进行倒车和机动，这对其导航能力提出了严峻挑战。", "method": "该研究采用了BR-MPPI控制方法，将控制障碍函数（CBF）约束直接嵌入到路径积分更新中。通过将重要性采样分布引导至无碰撞、动态可行的轨迹，BR-MPPI增强了MPPI的探索能力并提高了所得轨迹的鲁棒性。", "result": "该方法在CarMaker高保真模拟器中对一辆12米半挂车进行了评估，任务是在停车场进行倒车和前进停车。结果显示，BR-MPPI在单个GPU上以超过100Hz的频率计算控制输入（在八个障碍物的场景下），并且比标准的MPPI基线和带有碰撞成本的MPPI基线保持了更好的停车间隙。", "conclusion": "BR-MPPI控制方法能够有效解决铰接式车辆在复杂环境中的导航问题，通过结合CBF增强了路径探索和轨迹鲁棒性，并在计算效率和停车间隙方面表现优于现有方法。"}}
{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.", "AI": {"tldr": "本文提出AEPO框架，通过多答案生成和自适应探索奖励机制，解决了MLLM在GUI代理中语义对齐的探索效率低下问题，显著提升了自然语言指令的接地能力，并创造了新的SOTA。", "motivation": "多模态大语言模型（MLLMs）在基于纯视觉输入的GUI代理中，面临自然语言指令鲁棒接地的根本挑战。现有方法（如RLVR）在提高空间对齐方面有效，但探索效率低下限制了语义对齐的学习，尤其难以学习复杂的语义关联。", "method": "提出自适应探索策略优化（AEPO）框架。AEPO采用多答案生成策略以强制进行更广泛的探索，并由基于效率第一性原理（η=U/C）推导出的自适应探索奖励（AER）函数进行引导。", "result": "经过AEPO训练的模型InfiGUI-G1-3B和InfiGUI-G1-7B在多个挑战性GUI接地基准测试中取得了新的SOTA结果，在旨在测试泛化和语义理解的基准测试中，相对于朴素的RLVR基线，实现了高达9.0%的显著相对改进。", "conclusion": "AEPO框架有效解决了MLLM在GUI代理中语义对齐的探索效率问题，通过改进探索策略和奖励机制，显著提升了模型学习复杂语义关联的能力，从而在GUI接地任务上取得了最先进的性能。"}}
{"id": "2508.05689", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05689", "abs": "https://arxiv.org/abs/2508.05689", "authors": ["Jinjia Peng", "Zeze Tao", "Huibing Wang", "Meng Wang", "Yang Wang"], "title": "Boosting Adversarial Transferability via Residual Perturbation Attack", "comment": "Accepted to ieee/cvf international conference on computer vision\n  (ICCV2025)", "summary": "Deep neural networks are susceptible to adversarial examples while suffering\nfrom incorrect predictions via imperceptible perturbations. Transfer-based\nattacks create adversarial examples for surrogate models and transfer these\nexamples to target models under black-box scenarios. Recent studies reveal that\nadversarial examples in flat loss landscapes exhibit superior transferability\nto alleviate overfitting on surrogate models. However, the prior arts overlook\nthe influence of perturbation directions, resulting in limited transferability.\nIn this paper, we propose a novel attack method, named Residual Perturbation\nAttack (ResPA), relying on the residual gradient as the perturbation direction\nto guide the adversarial examples toward the flat regions of the loss function.\nSpecifically, ResPA conducts an exponential moving average on the input\ngradients to obtain the first moment as the reference gradient, which\nencompasses the direction of historical gradients. Instead of heavily relying\non the local flatness that stems from the current gradients as the perturbation\ndirection, ResPA further considers the residual between the current gradient\nand the reference gradient to capture the changes in the global perturbation\ndirection. The experimental results demonstrate the better transferability of\nResPA than the existing typical transfer-based attack methods, while the\ntransferability can be further improved by combining ResPA with the current\ninput transformation methods. The code is available at\nhttps://github.com/ZezeTao/ResPA.", "AI": {"tldr": "本文提出了一种名为ResPA的新型迁移攻击方法，通过利用残差梯度作为扰动方向，将对抗样本引导至损失函数的平坦区域，从而显著提升了对抗样本的迁移性。", "motivation": "深度神经网络容易受到对抗样本攻击，而现有的迁移攻击方法在黑盒场景下存在迁移性不足的问题，尤其是在选择扰动方向时，过度依赖局部平坦性而忽略了扰动方向的影响。", "method": "ResPA方法通过对输入梯度进行指数移动平均来获取作为参考梯度的第一动量（包含历史梯度方向），然后考虑当前梯度与参考梯度之间的残差作为扰动方向。这种方法旨在捕获全局扰动方向的变化，引导对抗样本进入损失函数的平坦区域。", "result": "实验结果表明，ResPA比现有典型的迁移攻击方法具有更好的迁移性。此外，将ResPA与当前的输入转换方法结合使用时，迁移性可以得到进一步提升。", "conclusion": "ResPA通过引入残差梯度作为扰动方向，成功克服了现有迁移攻击方法在迁移性上的局限性，有效提升了对抗样本的迁移能力，为黑盒对抗攻击提供了新的有效途径。"}}
{"id": "2508.05722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05722", "abs": "https://arxiv.org/abs/2508.05722", "authors": ["Rania Al-Sabbagh"], "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare", "comment": null, "summary": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible.", "AI": {"tldr": "本文介绍了PEACH，一个包含医疗保健文本的英阿平行语料库，共51,671个句对，是用于对比语言学、翻译研究和自然语言处理的黄金标准资源。", "motivation": "需要高质量、领域特定的平行语料库来支持医疗保健文本的对比语言学、翻译研究和自然语言处理（特别是机器翻译和词典构建）等研究。", "method": "通过人工对齐句子，构建了一个句子对齐的英阿平行语料库，涵盖病人信息手册和教育材料。", "result": "PEACH语料库包含51,671个平行句对，约590,517个英文词元和567,707个阿拉伯文词元。句子平均长度在9.52到11.83词之间。作为一个手动对齐的语料库，PEACH是黄金标准资源，并已公开。", "conclusion": "PEACH语料库是一个宝贵的黄金标准资源，可用于派生双语词典、调整大型语言模型以进行领域特定机器翻译、评估医疗保健领域机器翻译的用户感知、评估病人信息手册和教育材料的可读性和通俗性，并作为翻译研究的教育资源。"}}
{"id": "2508.06490", "categories": ["eess.IV", "cs.CV", "cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06490", "abs": "https://arxiv.org/abs/2508.06490", "authors": ["Stanislas Ducotterd", "Michael Unser"], "title": "Multivariate Fields of Experts", "comment": null, "summary": "We introduce the multivariate fields of experts, a new framework for the\nlearning of image priors. Our model generalizes existing fields of experts\nmethods by incorporating multivariate potential functions constructed via\nMoreau envelopes of the $\\ell_\\infty$-norm. We demonstrate the effectiveness of\nour proposal across a range of inverse problems that include image denoising,\ndeblurring, compressed-sensing magnetic-resonance imaging, and computed\ntomography. The proposed approach outperforms comparable univariate models and\nachieves performance close to that of deep-learning-based regularizers while\nbeing significantly faster, requiring fewer parameters, and being trained on\nsubstantially fewer data. In addition, our model retains a relatively high\nlevel of interpretability due to its structured design.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2508.05887", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.05887", "abs": "https://arxiv.org/abs/2508.05887", "authors": ["Apostolos I. Rikos", "Nicola Bastianello", "Themistoklis Charalambous", "Karl H. Johansson"], "title": "Distributed Optimization and Learning for Automated Stepsize Selection with Finite Time Coordination", "comment": null, "summary": "Distributed optimization and learning algorithms are designed to operate over\nlarge scale networks enabling processing of vast amounts of data effectively\nand efficiently. One of the main challenges for ensuring a smooth learning\nprocess in gradient-based methods is the appropriate selection of a learning\nstepsize. Most current distributed approaches let individual nodes adapt their\nstepsizes locally. However, this may introduce stepsize heterogeneity in the\nnetwork, thus disrupting the learning process and potentially leading to\ndivergence. In this paper, we propose a distributed learning algorithm that\nincorporates a novel mechanism for automating stepsize selection among nodes.\nOur main idea relies on implementing a finite time coordination algorithm for\neliminating stepsize heterogeneity among nodes. We analyze the operation of our\nalgorithm and we establish its convergence to the optimal solution. We conclude\nour paper with numerical simulations for a linear regression problem,\nshowcasing that eliminating stepsize heterogeneity enhances convergence speed\nand accuracy against current approaches.", "AI": {"tldr": "本文提出一种新的分布式学习算法，通过有限时间协调机制消除节点间的学习步长异质性，以提高收敛速度和准确性。", "motivation": "分布式优化和学习算法在大规模网络中处理海量数据时，梯度下降法的学习步长选择是一个主要挑战。现有方法允许节点本地调整步长，但这可能导致步长异质性，从而扰乱学习过程甚至导致发散。", "method": "提出一种分布式学习算法，包含一种新颖的自动化步长选择机制。核心思想是实现一个有限时间协调算法，以消除节点间的步长异质性。", "result": "分析表明所提出的算法能收敛到最优解。线性回归问题的数值模拟结果显示，消除步长异质性能够显著提高收敛速度和准确性，优于现有方法。", "conclusion": "通过引入有限时间协调机制来消除分布式学习中的步长异质性，可以有效提升算法的收敛性能和准确性。"}}
{"id": "2508.05838", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY", "68T07, 68T40, 90C40, 93E35", "I.2.6; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.05838", "abs": "https://arxiv.org/abs/2508.05838", "authors": ["Ahmad Farooq", "Kamran Iqbal"], "title": "Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction", "comment": "Published in the Proceedings of the 2025 3rd International Conference\n  on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 figures, 1\n  table", "summary": "This paper presents a novel approach that integrates vision foundation models\nwith reinforcement learning to enhance object interaction capabilities in\nsimulated environments. By combining the Segment Anything Model (SAM) and\nYOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the\nAI2-THOR simulation environment, we enable the agent to perceive and interact\nwith objects more effectively. Our comprehensive experiments, conducted across\nfour diverse indoor kitchen settings, demonstrate significant improvements in\nobject interaction success rates and navigation efficiency compared to a\nbaseline agent without advanced perception. The results show a 68% increase in\naverage cumulative reward, a 52.5% improvement in object interaction success\nrate, and a 33% increase in navigation efficiency. These findings highlight the\npotential of integrating foundation models with reinforcement learning for\ncomplex robotic tasks, paving the way for more sophisticated and capable\nautonomous agents.", "AI": {"tldr": "本文提出将视觉基础模型与强化学习结合，以提升模拟环境中智能体的物体交互能力，实验证明显著提高了交互成功率和导航效率。", "motivation": "现有智能体在模拟环境中进行物体交互时，其感知和交互能力有待提升。研究旨在通过整合先进的视觉基础模型来增强这些能力。", "method": "该研究将Segment Anything Model (SAM) 和YOLOv5这两个视觉基础模型与Proximal Policy Optimization (PPO) 强化学习智能体相结合，并在AI2-THOR模拟环境中进行操作，以实现更有效的物体感知和交互。", "result": "在四种不同的室内厨房场景中进行的实验表明，与没有高级感知的基线智能体相比，该方法使平均累积奖励增加了68%，物体交互成功率提高了52.5%，导航效率提高了33%。", "conclusion": "研究结果强调了将基础模型与强化学习相结合在复杂机器人任务中的巨大潜力，为开发更复杂、更强大的自主智能体铺平了道路。"}}
{"id": "2508.05766", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.05766", "abs": "https://arxiv.org/abs/2508.05766", "authors": ["Bo Wen"], "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference", "comment": null, "summary": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures.", "AI": {"tldr": "本文提出了一个结合主动推理原理和大语言模型（LLMs）的新框架，旨在开发本质安全的通用人工智能（AGI）。", "motivation": "传统的AI安全方法，如事后可解释性和奖励工程，存在根本性局限，无法有效确保AGI的安全性。", "method": "该框架通过将安全保障集成到系统核心设计中，利用透明信念表示和分层价值对齐。它将自然语言作为表示和操作信念的媒介，实现直接的人类监督。架构实现了一个多智能体系统，智能体根据主动推理原则自组织，偏好和安全约束通过分层马尔可夫毯传递。具体的安全机制包括：1) 在自然语言中明确分离信念和偏好；2) 通过资源感知自由能最小化实现有限理性；3) 通过模块化智能体结构实现组合安全性。", "result": "提出了一个能够开发本质上更安全的AGI的路径，而非事后附加安全措施。该框架通过核心设计保障安全性，支持人类直接监督和多智能体自组织。", "conclusion": "该研究为AGI开发提供了一条内在安全的新路径。论文最后提出了一个以抽象推理语料库（ARC）基准为中心的研究议程，旨在通过实验验证该框架的安全特性。"}}
{"id": "2508.05732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05732", "abs": "https://arxiv.org/abs/2508.05732", "authors": ["Pinxuan Li", "Bing Cao", "Changqing Zhang", "Qinghua Hu"], "title": "Generalized Few-Shot Out-of-Distribution Detection", "comment": null, "summary": "Few-shot Out-of-Distribution (OOD) detection has emerged as a critical\nresearch direction in machine learning for practical deployment. Most existing\nFew-shot OOD detection methods suffer from insufficient generalization\ncapability for the open world. Due to the few-shot learning paradigm, the OOD\ndetection ability is often overfit to the limited training data itself, thus\ndegrading the performance on generalized data and performing inconsistently\nacross different scenarios. To address this challenge, we proposed a\nGeneralized Few-shot OOD Detection (GOOD) framework, which empowers the general\nknowledge of the OOD detection model with an auxiliary General Knowledge Model\n(GKM), instead of directly learning from few-shot data. We proceed to reveal\nthe few-shot OOD detection from a generalization perspective and theoretically\nderive the Generality-Specificity balance (GS-balance) for OOD detection, which\nprovably reduces the upper bound of generalization error with a general\nknowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)\nmechanism to adaptively modulate the guidance of general knowledge. KDE\ndynamically aligns the output distributions of the OOD detection model to the\ngeneral knowledge model based on the Generalized Belief (G-Belief) of GKM,\nthereby boosting the GS-balance. Experiments on real-world OOD benchmarks\ndemonstrate our superiority. Codes will be available.", "AI": {"tldr": "本文提出了一个广义小样本OOD检测（GOOD）框架，通过引入通用知识模型（GKM）和知识动态嵌入（KDE）机制，解决了现有小样本OOD检测方法泛化能力不足的问题，并在理论上推导了泛化-特异性平衡（GS-balance）。", "motivation": "现有的小样本OOD检测方法泛化能力不足，容易过拟合于有限的训练数据，导致在泛化数据上性能下降，并且在不同场景下表现不稳定。", "method": "提出了广义小样本OOD检测（GOOD）框架，利用辅助通用知识模型（GKM）赋予OOD检测模型通用知识。从泛化角度揭示了小样本OOD检测问题，并理论推导了泛化-特异性平衡（GS-balance），证明其能有效降低泛化误差上界。设计了知识动态嵌入（KDE）机制，根据GKM的广义置信度（G-Belief）自适应地调节通用知识的引导，以提升GS-balance。", "result": "在真实世界的OOD基准测试中，实验结果证明了所提出方法的优越性。", "conclusion": "GOOD框架通过利用通用知识并实现泛化-特异性平衡，显著提升了小样本OOD检测模型的泛化能力和性能稳定性。"}}
{"id": "2508.05775", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05775", "abs": "https://arxiv.org/abs/2508.05775", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation", "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies.", "AI": {"tldr": "该综述系统性审查了大型语言模型（LLMs）的安全性问题，包括无意毒性、对抗性越狱攻击和内容审核技术，并提出了统一的危害与防御分类法，分析了缓解策略。", "motivation": "大型语言模型在内容创作中展现出巨大潜力，但也可能无意或有意地生成有毒、冒犯性或有偏见的内容。LLMs的这种双重作用（强大工具与潜在危害源）构成了紧迫的社会技术挑战。", "method": "本文通过系统性回顾近期研究，涵盖无意毒性、对抗性越狱攻击和内容审核技术。具体方法包括提出LLM相关危害和防御的统一分类法，分析新兴的多模态和LLM辅助越狱策略，并评估包括RLHF、提示工程和安全对齐在内的缓解措施。", "result": "研究结果包括：提出了LLM相关危害和防御的统一分类法；分析了新兴的多模态和LLM辅助越狱策略；评估了RLHF、提示工程和安全对齐等缓解措施；强调了LLM安全领域的发展态势；指出了当前评估方法的局限性；并概述了未来的研究方向。", "conclusion": "本研究旨在指导鲁棒且符合伦理的语言技术发展，以应对LLMs带来的安全挑战和风险，确保其在带来益处的同时最小化潜在危害。"}}
{"id": "2508.06407", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06407", "abs": "https://arxiv.org/abs/2508.06407", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery", "comment": null, "summary": "High-resolution imagery plays a critical role in improving the performance of\nvisual recognition tasks such as classification, detection, and segmentation.\nIn many domains, including remote sensing and surveillance, low-resolution\nimages can limit the accuracy of automated analysis. To address this,\nsuper-resolution (SR) techniques have been widely adopted to attempt to\nreconstruct high-resolution images from low-resolution inputs. Related\ntraditional approaches focus solely on enhancing image quality based on\npixel-level metrics, leaving the relationship between super-resolved image\nfidelity and downstream classification performance largely underexplored. This\nraises a key question: can integrating classification objectives directly into\nthe super-resolution process further improve classification accuracy? In this\npaper, we try to respond to this question by investigating the relationship\nbetween super-resolution and classification through the deployment of a\nspecialised algorithmic strategy. We propose a novel methodology that increases\nthe resolution of synthetic aperture radar imagery by optimising loss functions\nthat account for both image quality and classification performance. Our\napproach improves image quality, as measured by scientifically ascertained\nimage quality indicators, while also enhancing classification accuracy.", "AI": {"tldr": "本文提出一种新颖的超分辨率方法，通过将图像质量和分类性能同时纳入损失函数进行优化，以提高合成孔径雷达（SAR）图像的质量和下游分类精度。", "motivation": "低分辨率图像限制了视觉识别任务的准确性。传统超分辨率方法只关注像素级图像质量，而忽视了超分辨图像保真度与下游分类性能之间的关系。因此，研究将分类目标直接整合到超分辨率过程中能否进一步提高分类精度成为关键问题。", "method": "提出一种专门的算法策略，通过优化同时考虑图像质量和分类性能的损失函数，来提高合成孔径雷达（SAR）图像的分辨率。", "result": "该方法在提高图像质量（通过科学鉴定的图像质量指标衡量）的同时，也提升了分类准确性。", "conclusion": "将分类目标直接整合到超分辨率过程中，可以有效提高分类准确性，同时改善图像质量。"}}
{"id": "2508.05895", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.05895", "abs": "https://arxiv.org/abs/2508.05895", "authors": ["Jiaqi Hu", "Karl H. Johansson", "Apostolos I. Rikos"], "title": "Distributed Quantized Average Consensus in Open Multi-Agent Systems with Dynamic Communication Links", "comment": null, "summary": "In this paper, we focus on the distributed quantized average consensus\nproblem in open multi-agent systems consisting of communication links that\nchange dynamically over time. Open multi-agent systems exhibiting the\naforementioned characteristic are referred to as \\textit{open dynamic\nmulti-agent systems} in this work. We present a distributed algorithm that\nenables active nodes in the open dynamic multi-agent system to calculate the\nquantized average of their initial states. Our algorithm consists of the\nfollowing advantages: (i) ensures efficient communication by enabling nodes to\nexchange quantized valued messages, and (ii) exhibits finite time convergence\nto the desired solution. We establish the correctness of our algorithm and we\npresent necessary and sufficient topological conditions for it to successfully\nsolve the quantized average consensus problem in an open dynamic multi-agent\nsystem. Finally, we illustrate the performance of our algorithm with numerical\nsimulations.", "AI": {"tldr": "本文提出了一种分布式算法，用于在开放动态多智能体系统中实现量化平均共识，该算法通过交换量化值消息实现高效通信，并在有限时间内收敛，同时给出了成功解决问题的拓扑条件。", "motivation": "研究旨在解决开放多智能体系统中动态变化的通信链路下，如何实现分布式量化平均共识的问题，特别是针对“开放动态多智能体系统”。", "method": "提出了一种分布式算法，使活跃节点能够计算其初始状态的量化平均值。该算法通过交换量化值消息以实现高效通信，并具有有限时间收敛特性。同时，建立了算法正确性以及成功解决量化平均共识问题所需的必要和充分拓扑条件。", "result": "所提出的算法能够确保节点间高效通信，并在有限时间内收敛到期望的量化平均解。研究确立了算法的正确性，并给出了算法在开放动态多智能体系统中成功解决量化平均共识问题的必要和充分拓扑条件。数值仿真验证了算法的性能。", "conclusion": "本文成功提出并验证了一种适用于开放动态多智能体系统的分布式量化平均共识算法，该算法具有通信效率高、有限时间收敛的优点，并明确了其成功的拓扑条件。"}}
{"id": "2508.05936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05936", "abs": "https://arxiv.org/abs/2508.05936", "authors": ["Haohui Pan", "Takuya Kiyokawa", "Tomoki Ishikura", "Shingo Hamada", "Genichiro Matsuda", "Kensuke Harada"], "title": "Modular Vacuum-Based Fixturing System for Adaptive Disassembly Workspace Integration", "comment": "8 pages, 9 figures", "summary": "The disassembly of small household appliances poses significant challenges\ndue to their complex and curved geometries, which render traditional rigid\nfixtures inadequate. In this paper, we propose a modular vacuum-based fixturing\nsystem that leverages commercially available balloon-type soft grippers to\nconform to arbitrarily shaped surfaces and provide stable support during\nscrew-removal tasks. To enable a reliable deployment of the system, we develop\na stability-aware planning framework that samples the bottom surface of the\ntarget object, filters candidate contact points based on geometric continuity,\nand evaluates support configurations using convex hull-based static stability\ncriteria. We compare the quality of object placement under different numbers\nand configurations of balloon hands. In addition, real-world experiments were\nconducted to compare the success rates of traditional rigid fixtures with our\nproposed system. The results demonstrate that our method consistently achieves\nhigher success rates and superior placement stability during screw removal\ntasks.", "AI": {"tldr": "本文提出了一种模块化真空吸附夹具系统，利用气囊式软抓手适应复杂曲面，并通过稳定性规划提高小型家电拆卸时的固定稳定性。", "motivation": "传统刚性夹具难以有效固定具有复杂和弯曲几何形状的小型家电，导致拆卸（如拧螺丝）任务面临挑战。", "method": "开发了一种基于模块化真空吸附的夹具系统，采用市售气囊式软抓手适应任意形状表面。设计了稳定性感知规划框架，该框架通过采样目标物体底部表面、基于几何连续性过滤候选接触点，并使用基于凸包的静态稳定性标准评估支撑配置。", "result": "实验结果表明，与传统刚性夹具相比，所提出的系统在螺丝拆卸任务中持续获得更高的成功率和卓越的放置稳定性。研究还比较了不同数量和配置的气囊手对物体放置质量的影响。", "conclusion": "该模块化真空吸附夹具系统能有效解决复杂几何形状物体拆卸时的固定难题，显著提高了操作的成功率和稳定性。"}}
{"id": "2508.05776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05776", "abs": "https://arxiv.org/abs/2508.05776", "authors": ["Thomas L. Griffiths", "Brenden M. Lake", "R. Thomas McCoy", "Ellie Pavlick", "Taylor W. Webb"], "title": "Whither symbols in the era of advanced neural networks?", "comment": null, "summary": "Some of the strongest evidence that human minds should be thought about in\nterms of symbolic systems has been the way they combine ideas, produce novelty,\nand learn quickly. We argue that modern neural networks -- and the artificial\nintelligence systems built upon them -- exhibit similar abilities. This\nundermines the argument that the cognitive processes and representations used\nby human minds are symbolic, although the fact that these neural networks are\ntypically trained on data generated by symbolic systems illustrates that such\nsystems play an important role in characterizing the abstract problems that\nhuman minds have to solve. This argument leads us to offer a new agenda for\nresearch on the symbolic basis of human thought.", "AI": {"tldr": "本文认为现代神经网络展现出与人类思维相似的能力（结合思想、产生新颖性、快速学习），这削弱了人类认知过程和表征是纯粹符号系统的论点，但同时指出符号系统在定义人类解决的抽象问题中仍扮演重要角色，并提出了关于人类思维符号基础的新研究议程。", "motivation": "以往，人类思维结合思想、产生新颖性、快速学习的能力被视为支持其是符号系统的强有力证据。然而，现代神经网络也展现出类似能力，这促使作者重新审视人类心智是否必须以符号系统来解释。", "method": "本文通过概念性论证，比较了现代神经网络与人类思维在特定认知能力上的表现，并基于此对传统符号主义观点提出了质疑和修正。", "result": "现代神经网络展现出与人类思维相似的结合思想、产生新颖性和快速学习的能力，这削弱了人类认知过程和表征是符号系统的论点。尽管如此，神经网络通常在由符号系统生成的数据上训练，表明符号系统在刻画人类思维需要解决的抽象问题方面仍发挥着重要作用。", "conclusion": "人类思维的认知过程和表征不应仅仅被视为符号系统，因为非符号的神经网络也能展现出类似的关键能力。然而，符号系统在定义和理解人类思维所处理的抽象问题上依然至关重要。这促使需要为人类思维的符号基础研究提出一个新的议程。"}}
{"id": "2508.05755", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05755", "abs": "https://arxiv.org/abs/2508.05755", "authors": ["Agnieszka Polowczyk", "Alicja Polowczyk", "Dawid Malarz", "Artur Kasymov", "Marcin Mazur", "Jacek Tabor", "Przemysław Spurek"], "title": "UnGuide: Learning to Forget with LoRA-Guided Diffusion Models", "comment": null, "summary": "Recent advances in large-scale text-to-image diffusion models have heightened\nconcerns about their potential misuse, especially in generating harmful or\nmisleading content. This underscores the urgent need for effective machine\nunlearning, i.e., removing specific knowledge or concepts from pretrained\nmodels without compromising overall performance. One possible approach is\nLow-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models\nfor targeted unlearning. However, LoRA often inadvertently alters unrelated\ncontent, leading to diminished image fidelity and realism. To address this\nlimitation, we introduce UnGuide -- a novel approach which incorporates\nUnGuidance, a dynamic inference mechanism that leverages Classifier-Free\nGuidance (CFG) to exert precise control over the unlearning process. UnGuide\nmodulates the guidance scale based on the stability of a few first steps of\ndenoising processes, enabling selective unlearning by LoRA adapter. For prompts\ncontaining the erased concept, the LoRA module predominates and is\ncounterbalanced by the base model; for unrelated prompts, the base model\ngoverns generation, preserving content fidelity. Empirical results demonstrate\nthat UnGuide achieves controlled concept removal and retains the expressive\npower of diffusion models, outperforming existing LoRA-based methods in both\nobject erasure and explicit content removal tasks.", "AI": {"tldr": "UnGuide是一种新方法，通过引入动态推理机制UnGuidance，利用CFG在扩散模型中实现对LoRA适配器的选择性遗忘，解决了LoRA遗忘时对无关内容产生负面影响的问题，同时保持了模型性能。", "motivation": "大规模文本到图像扩散模型可能被滥用生成有害或误导性内容，因此迫切需要有效的机器遗忘技术。现有LoRA方法在遗忘特定知识时，常无意中改变无关内容，导致图像保真度和真实感下降。", "method": "引入UnGuide方法，其核心是UnGuidance动态推理机制。该机制利用Classifier-Free Guidance (CFG) 对遗忘过程进行精确控制。UnGuide根据去噪过程前几步的稳定性来调节引导尺度，使LoRA适配器能够进行选择性遗忘。对于包含已擦除概念的提示，LoRA模块占主导地位并由基础模型平衡；对于无关提示，基础模型控制生成，从而保持内容保真度。", "result": "UnGuide实现了受控的概念移除，并保留了扩散模型的表达能力。在物体擦除和显式内容移除任务中，其性能均优于现有的基于LoRA的方法。", "conclusion": "UnGuide通过其创新的UnGuidance机制，有效解决了LoRA在机器遗忘中对无关内容的影响问题，实现了选择性遗忘，同时保持了扩散模型的整体性能和内容保真度。"}}
{"id": "2508.05782", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05782", "abs": "https://arxiv.org/abs/2508.05782", "authors": ["Xiangyan Chen", "Yufeng Li", "Yujian Gan", "Arkaitz Zubiaga", "Matthew Purver"], "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification", "comment": null, "summary": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub.", "AI": {"tldr": "本文提出了FineDialFact基准，用于细粒度对话事实核查，通过验证从对话回复中提取的原子事实来解决LLM幻觉检测中现有方法的粗粒度问题。", "motivation": "大型语言模型（LLMs）存在幻觉问题，这给自然语言处理应用带来了挑战。现有幻觉检测方法主要关注事实一致性，但对话回复通常混杂准确、不准确或无法验证的事实，导致单一的事实标签过于粗糙和简化。", "method": "引入了FineDialFact基准，用于细粒度对话事实核查，即验证从对话回复中提取的原子事实。为此，构建了一个基于公开对话数据集的数据集，并使用多种基线方法（包括思维链CoT推理）进行了评估。", "result": "实验结果表明，结合思维链（CoT）推理的方法可以提高对话事实核查的性能。然而，在开放域对话数据集HybriDialogue上，最佳F1分数仅为0.75，这表明该基准对于未来的研究仍是一项具有挑战性的任务。", "conclusion": "细粒度对话事实核查是一个具有挑战性的任务，尽管CoT方法能提升性能，但仍有很大提升空间。本文提出的FineDialFact基准和数据集将为未来研究提供支持。"}}
{"id": "2508.06079", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06079", "abs": "https://arxiv.org/abs/2508.06079", "authors": ["Tzu-Chien Hsueh", "Bill Lin", "Zijun Chen", "Yeshaiahu Fainman"], "title": "Panel-Scale Reconfigurable Photonic Interconnects for Scalable AI Computation", "comment": "16 pages, 9 figures, 2 tables", "summary": "Panel-scale reconfigurable photonic interconnects on a glass substrate up to\n500-mm x 500-mm or larger are envisioned by proposing a novel photonic switch\nfabric that enables all directional panel-edge-to-panel-edge reach without the\nneed for active repeaters while offering high communication bandwidth,\nplanar-direction reconfigurability, low energy consumption, and compelling data\nbandwidth density for heterogeneous integration of an in-package AI computing\nsystem on a single glass-substrate photonic interposer exceeding thousands of\ncentimeters square. The proposed approach focuses on reconfigurable photonic\ninterconnects, which are integration-compatible with commercial processor\nchiplets and 3D high-bandwidth memory (HBM) stacks on a large-area glass\nsubstrate, to create a novel panel-scale heterogeneously integrated interposer\nor package enabling low-energy and high-capacity\nwavelength-division-multiplexing (WDM) optical data links using advanced\nhigh-speed optical modulators, broadband photodetectors, novel optical crossbar\nswitches with multi-layer waveguides, and in-package frequency comb sources.", "AI": {"tldr": "该研究提出一种新型光子开关结构，用于在500x500毫米或更大的玻璃基板上实现面板级可重构光互连，以支持高带宽、低能耗的封装内AI计算系统。", "motivation": "现有AI计算系统需要更高带宽、更低能耗的互连方案，尤其是在大型异构集成封装内，传统电互连面临瓶颈。因此，需要一种能够实现面板级（千平方厘米以上）全向通信且无需中继的光子互连解决方案。", "method": "该方法提出一种新型光子开关结构，利用多层波导、新型光交叉开关、先进高速光调制器、宽带光电探测器和封装内频率梳光源，实现波分复用（WDM）光数据链路。该方案旨在与商用处理器芯片和3D高带宽存储器（HBM）堆栈在大型玻璃基板上兼容集成。", "result": "该方案能够实现面板边缘到面板边缘的全向通信，无需主动中继器；提供高通信带宽、平面方向可重构性、低能耗和高数据带宽密度；并能与处理器芯片和小芯片、HBM堆栈进行异构集成，形成新型面板级集成中介层或封装。", "conclusion": "该研究提出了一种在大型玻璃基板上实现面板级可重构光子互连的创新方法，为封装内AI计算系统提供了低能耗、高容量的光学数据链路，有望彻底改变未来高性能计算系统的集成方式。"}}
{"id": "2508.05937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05937", "abs": "https://arxiv.org/abs/2508.05937", "authors": ["Gen Sako", "Takuya Kiyokawa", "Kensuke Harada", "Tomoki Ishikura", "Naoya Miyaji", "Genichiro Matsuda"], "title": "Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts", "comment": "6 pages, 9 figures", "summary": "Robotic non-destructive disassembly of mating parts remains challenging due\nto the need for flexible manipulation and the limited visibility of internal\nstructures. This study presents an affordance-guided teleoperation system that\nenables intuitive human demonstrations for dual-arm fix-and-disassemble tasks\nfor mating parts. The system visualizes feasible grasp poses and disassembly\ndirections in a virtual environment, both derived from the object's geometry,\nto address occlusions and structural complexity. To prevent excessive position\ntracking under load when following the affordance, we integrate a hybrid\ncontroller that combines position and impedance control into the teleoperated\ndisassembly arm. Real-world experiments validate the effectiveness of the\nproposed system, showing improved task success rates and reduced object pose\ndeviation.", "AI": {"tldr": "本研究提出了一种基于可供性引导的遥操作系统，用于机器人对配合零件进行无损拆卸，通过虚拟环境可视化和混合控制器，提高了任务成功率并减少了物体姿态偏差。", "motivation": "机器人对配合零件进行无损拆卸仍具挑战性，原因在于需要灵活操作且内部结构可见性有限。", "method": "本研究提出一个可供性引导的遥操作系统，用于双臂固定和拆卸任务。该系统在虚拟环境中可视化可行的抓取姿态和拆卸方向（均源自物体几何形状），以解决遮挡和结构复杂性问题。为防止在负载下过度位置跟踪，拆卸臂集成了结合位置和阻抗控制的混合控制器。", "result": "真实世界实验验证了所提系统的有效性，显示任务成功率提高，物体姿态偏差减少。", "conclusion": "所提出的可供性引导遥操作系统能有效解决配合零件的机器人无损拆卸挑战，显著提升了操作性能。"}}
{"id": "2508.05792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05792", "abs": "https://arxiv.org/abs/2508.05792", "authors": ["Kausik Lakkaraju", "Siva Likitha Valluru", "Biplav Srivastava"], "title": "Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making", "comment": null, "summary": "Current eXplainable AI (XAI) methods largely serve developers, often focusing\non justifying model outputs rather than supporting diverse stakeholder needs. A\nrecent shift toward Evaluative AI reframes explanation as a tool for hypothesis\ntesting, but still focuses primarily on operational organizations. We introduce\nHolistic-XAI (H-XAI), a unified framework that integrates causal rating methods\nwith traditional XAI methods to support explanation as an interactive,\nmulti-method process. H-XAI allows stakeholders to ask a series of questions,\ntest hypotheses, and compare model behavior against automatically constructed\nrandom and biased baselines. It combines instance-level and global\nexplanations, adapting to each stakeholder's goals, whether understanding\nindividual decisions, assessing group-level bias, or evaluating robustness\nunder perturbations. We demonstrate the generality of our approach through two\ncase studies spanning six scenarios: binary credit risk classification and\nfinancial time-series forecasting. H-XAI fills critical gaps left by existing\nXAI methods by combining causal ratings and post-hoc explanations to answer\nstakeholder-specific questions at both the individual decision level and the\noverall model level.", "AI": {"tldr": "Holistic-XAI (H-XAI)是一个统一框架，整合因果评级与传统可解释AI方法，支持多利益相关者的交互式解释过程，以测试假设并适应不同解释目标。", "motivation": "现有可解释AI（XAI）方法主要服务于开发者，侧重于模型输出的合理性，而非满足多样化的利益相关者需求；评估性AI虽将解释视为假设检验工具，但仍主要关注运营组织，未能充分解决广泛的利益相关者需求。", "method": "引入Holistic-XAI (H-XAI)框架，该框架整合了因果评级方法与传统XAI方法，支持交互式、多方法解释过程。它允许利益相关者提出问题、测试假设，并将模型行为与自动构建的随机和偏置基线进行比较。H-XAI结合了实例级和全局解释，能适应利益相关者的不同目标，如理解个体决策、评估群体偏见或评估扰动下的鲁棒性。", "result": "通过两个案例研究（二元信用风险分类和金融时间序列预测）的六种场景，证明了该方法的通用性。", "conclusion": "H-XAI通过结合因果评级和事后解释，弥补了现有XAI方法的关键空白，能够回答个体决策层面和整体模型层面的利益相关者特定问题。"}}
{"id": "2508.05769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05769", "abs": "https://arxiv.org/abs/2508.05769", "authors": ["Seyed Hadi Seyed", "Ayberk Cansever", "David Hart"], "title": "Improving Masked Style Transfer using Blended Partial Convolution", "comment": null, "summary": "Artistic style transfer has long been possible with the advancements of\nconvolution- and transformer-based neural networks. Most algorithms apply the\nartistic style transfer to the whole image, but individual users may only need\nto apply a style transfer to a specific region in the image. The standard\npractice is to simply mask the image after the stylization. This work shows\nthat this approach tends to improperly capture the style features in the region\nof interest. We propose a partial-convolution-based style transfer network that\naccurately applies the style features exclusively to the region of interest.\nAdditionally, we present network-internal blending techniques that account for\nimperfections in the region selection. We show that this visually and\nquantitatively improves stylization using examples from the SA-1B dataset. Code\nis publicly available at https://github.com/davidmhart/StyleTransferMasked.", "AI": {"tldr": "该研究提出了一种基于偏卷积（partial-convolution）的风格迁移网络，能精确地将艺术风格应用于图像的特定区域，并引入内部融合技术以优化区域选择，解决了传统全图风格迁移后遮罩处理区域风格捕获不准确的问题。", "motivation": "现有艺术风格迁移算法多应用于整个图像，但用户可能只需要对图像的特定区域进行风格迁移。传统的做法是先对全图进行风格化再进行遮罩，但这会导致目标区域的风格特征捕获不准确。", "method": "提出了一种基于偏卷积的风格迁移网络，用于精确地将风格特征应用于感兴趣区域。此外，引入了网络内部融合技术，以解决区域选择中的不完美问题。", "result": "所提出的方法能够准确地将风格特征仅应用于感兴趣区域，并通过SA-1B数据集的示例证明了其在视觉和定量上的风格化改进。", "conclusion": "该研究成功开发了一种针对图像特定区域的艺术风格迁移方法，通过偏卷积和内部融合技术有效解决了传统方法的局限性，显著提高了区域风格化的准确性和视觉质量。"}}
{"id": "2508.05803", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.05803", "abs": "https://arxiv.org/abs/2508.05803", "authors": ["Abishek Thamma", "Micha Heilbron"], "title": "Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models", "comment": null, "summary": "Human memory is fleeting. As words are processed, the exact wordforms that\nmake up incoming sentences are rapidly lost. Cognitive scientists have long\nbelieved that this limitation of memory may, paradoxically, help in learning\nlanguage - an idea supported by classic connectionist modelling work. The rise\nof Transformers appears to challenge this idea, as these models can learn\nlanguage effectively, despite lacking memory limitations or other architectural\nrecency biases. Here, we investigate the hypothesized benefit of fleeting\nmemory for language learning in tightly controlled experiments on transformer\nlanguage models. Training transformers with and without fleeting memory on a\ndevelopmentally realistic training set, we find that fleeting memory\nconsistently improves language learning (as quantified by both overall language\nmodelling performance and targeted syntactic evaluation) but, unexpectedly,\nimpairs surprisal-based prediction of human reading times. Interestingly,\nfollow up analyses revealed that this discrepancy - better language modeling,\nyet worse reading time prediction - could not be accounted for by prior\nexplanations of why better language models sometimes fit human reading time\nworse. Together, these results support a benefit of memory limitations on\nneural network language learning - but not on predicting behavior.", "AI": {"tldr": "研究发现，短暂记忆有助于Transformer模型学习语言，但会损害其对人类阅读时间的预测。", "motivation": "认知科学家长期认为，记忆的短暂性（即对词形快速遗忘）可能有助于语言学习，经典联结主义模型也支持此观点。然而，Transformer模型在缺乏此类记忆限制的情况下也能有效学习语言，这挑战了这一传统观念。因此，本研究旨在通过受控实验探究短暂记忆对Transformer语言模型学习语言的潜在益处。", "method": "研究通过在Transformer语言模型上进行严格控制的实验，在发育逼真的训练集上训练带有和不带有短暂记忆的模型。通过整体语言建模性能、目标句法评估以及基于Surprisal的人类阅读时间预测来量化和比较模型的语言学习效果。", "result": "结果显示，短暂记忆持续改进了Transformer的语言学习能力（包括整体语言建模性能和目标句法评估），但出乎意料的是，它损害了基于Surprisal的人类阅读时间预测。进一步分析表明，这种“更好语言模型但更差阅读时间预测”的矛盾现象无法用先前解释来解释。", "conclusion": "这些结果支持了记忆限制对神经网络语言学习的益处，但这种益处并未体现在预测人类行为（如阅读时间）上。"}}
{"id": "2508.06326", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06326", "abs": "https://arxiv.org/abs/2508.06326", "authors": ["Nathaniel Virgo", "Martin Biehl", "Manuel Baltieri", "Matteo Capucci"], "title": "A \"good regulator theorem\" for embodied agents", "comment": "Accepted at the Artificial Life conference 2025 (ALife 2025). 10\n  pages, 1 figure", "summary": "In a classic paper, Conant and Ashby claimed that \"every good regulator of a\nsystem must be a model of that system.\" Artificial Life has produced many\nexamples of systems that perform tasks with apparently no model in sight; these\nsuggest Conant and Ashby's theorem doesn't easily generalise beyond its\nrestricted setup. Nevertheless, here we show that a similar intuition can be\nfleshed out in a different way: whenever an agent is able to perform a\nregulation task, it is possible for an observer to interpret it as having\n\"beliefs\" about its environment, which it \"updates\" in response to sensory\ninput. This notion of belief updating provides a notion of model that is more\nsophisticated than Conant and Ashby's, as well as a theorem that is more\nbroadly applicable. However, it necessitates a change in perspective, in that\nthe observer plays an essential role in the theory: models are not a mere\nproperty of the system but are imposed on it from outside. Our theorem holds\nregardless of whether the system is regulating its environment in a classic\ncontrol theory setup, or whether it's regulating its own internal state; the\nmodel is of its environment either way. The model might be trivial, however,\nand this is how the apparent counterexamples are resolved.", "AI": {"tldr": "本文重新诠释了Conant和Ashby的“好调节器”定理，提出任何成功的调节器都可以被观察者解读为拥有并更新关于其环境的“信念”（一种模型），从而通过强调观察者的作用和模型可能微不足道来解决表观反例。", "motivation": "Conant和Ashby的经典定理“任何好的调节器都必须是其系统的模型”在人工生命领域面临泛化困难，因为许多系统在没有明显模型的情况下也能执行任务，这表明原定理的适用范围受限。", "method": "本文提出一种新的“模型”概念：当一个智能体能够执行调节任务时，观察者可以将其解释为拥有关于其环境的“信念”，并根据感官输入“更新”这些信念。这种方法强调了观察者在理论中的关键作用，即模型是从外部赋予而非系统固有的。", "result": "建立了一个更广泛适用的定理：无论系统是调节其环境还是自身内部状态，只要它能成功调节，观察者就可以将其解释为拥有并更新关于环境的模型（信念）。通过承认这种模型可能微不足道，解决了与Conant和Ashby定理相悖的表观反例。", "conclusion": "本文提供了一种比Conant和Ashby更复杂且适用范围更广的“模型”概念，即通过观察者赋予和更新的“信念”来理解调节器。这改变了对模型的视角，从系统固有属性转变为外部解释，从而克服了原始定理的局限性。"}}
{"id": "2508.05941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05941", "abs": "https://arxiv.org/abs/2508.05941", "authors": ["Zhanyi Sun", "Shuran Song"], "title": "Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution", "comment": null, "summary": "Visuomotor policies trained via behavior cloning are vulnerable to covariate\nshift, where small deviations from expert trajectories can compound into\nfailure. Common strategies to mitigate this issue involve expanding the\ntraining distribution through human-in-the-loop corrections or synthetic data\naugmentation. However, these approaches are often labor-intensive, rely on\nstrong task assumptions, or compromise the quality of imitation. We introduce\nLatent Policy Barrier, a framework for robust visuomotor policy learning.\nInspired by Control Barrier Functions, LPB treats the latent embeddings of\nexpert demonstrations as an implicit barrier separating safe, in-distribution\nstates from unsafe, out-of-distribution (OOD) ones. Our approach decouples the\nrole of precise expert imitation and OOD recovery into two separate modules: a\nbase diffusion policy solely on expert data, and a dynamics model trained on\nboth expert and suboptimal policy rollout data. At inference time, the dynamics\nmodel predicts future latent states and optimizes them to stay within the\nexpert distribution. Both simulated and real-world experiments show that LPB\nimproves both policy robustness and data efficiency, enabling reliable\nmanipulation from limited expert data and without additional human correction\nor annotation.", "AI": {"tldr": "LPB框架通过将专家演示的潜在嵌入作为隐式屏障，提高了视觉运动策略的鲁棒性和数据效率，解决了行为克隆中的协变量偏移问题，无需人工干预。", "motivation": "行为克隆训练的视觉运动策略易受协变量偏移影响，导致小偏差累积为失败。现有缓解策略（如人工纠正、合成数据增强）通常劳动密集、依赖强任务假设或损害模仿质量。", "method": "引入潜在策略屏障（LPB）框架。受控制屏障函数启发，LPB将专家演示的潜在嵌入视为隐式屏障，区分安全（分布内）和不安全（分布外）状态。该方法将精确专家模仿和OOD恢复解耦为两个模块：基于专家数据的扩散策略和基于专家及次优策略rollout数据的动力学模型。推理时，动力学模型预测未来潜在状态并优化以保持在专家分布内。", "result": "仿真和真实世界实验表明，LPB显著提高了策略鲁棒性和数据效率，仅用有限专家数据即可实现可靠操作，且无需额外人工纠正或标注。", "conclusion": "LPB提供了一种无需大量人工干预即可有效解决行为克隆中协变量偏移问题的方法，提升了视觉运动策略的性能和实用性。"}}
{"id": "2508.05855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05855", "abs": "https://arxiv.org/abs/2508.05855", "authors": ["Zixia Wang", "Jia Hu", "Ronghui Mu"], "title": "Safety of Embodied Navigation: A Survey", "comment": null, "summary": "As large language models (LLMs) continue to advance and gain influence, the\ndevelopment of embodied AI has accelerated, drawing significant attention,\nparticularly in navigation scenarios. Embodied navigation requires an agent to\nperceive, interact with, and adapt to its environment while moving toward a\nspecified target in unfamiliar settings. However, the integration of embodied\nnavigation into critical applications raises substantial safety concerns. Given\ntheir deployment in dynamic, real-world environments, ensuring the safety of\nsuch systems is critical. This survey provides a comprehensive analysis of\nsafety in embodied navigation from multiple perspectives, encompassing attack\nstrategies, defense mechanisms, and evaluation methodologies. Beyond conducting\na comprehensive examination of existing safety challenges, mitigation\ntechnologies, and various datasets and metrics that assess effectiveness and\nrobustness, we explore unresolved issues and future research directions in\nembodied navigation safety. These include potential attack methods, mitigation\nstrategies, more reliable evaluation techniques, and the implementation of\nverification frameworks. By addressing these critical gaps, this survey aims to\nprovide valuable insights that can guide future research toward the development\nof safer and more reliable embodied navigation systems. Furthermore, the\nfindings of this study have broader implications for enhancing societal safety\nand increasing industrial efficiency.", "AI": {"tldr": "该论文全面综述了具身导航系统中的安全问题，涵盖攻击策略、防御机制和评估方法，并探讨了未解决的问题和未来研究方向。", "motivation": "随着大型语言模型（LLMs）的进步，具身AI（特别是导航）发展迅速，但在动态、真实世界的环境中部署具身导航系统面临重大的安全隐患，因此确保其安全性至关重要。", "method": "本文采用综述方法，从攻击策略、防御机制和评估方法等多个角度对具身导航的安全性进行了全面分析。它审查了现有安全挑战、缓解技术、数据集和评估指标，并探讨了未解决的问题和未来的研究方向。", "result": "该综述全面审视了具身导航的现有安全挑战、缓解技术以及用于评估有效性和鲁棒性的各种数据集和指标。同时，它指出了潜在的攻击方法、缓解策略、更可靠的评估技术和验证框架等未解决问题和未来研究方向。", "conclusion": "本综述旨在为未来开发更安全、更可靠的具身导航系统提供有价值的见解，并对提升社会安全和工业效率具有更广泛的意义。"}}
{"id": "2508.05772", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05772", "abs": "https://arxiv.org/abs/2508.05772", "authors": ["Can Zhao", "Pengfei Guo", "Dong Yang", "Yucheng Tang", "Yufan He", "Benjamin Simon", "Mason Belue", "Stephanie Harmon", "Baris Turkbey", "Daguang Xu"], "title": "MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss", "comment": null, "summary": "Medical image synthesis is an important topic for both clinical and research\napplications. Recently, diffusion models have become a leading approach in this\narea. Despite their strengths, many existing methods struggle with (1) limited\ngeneralizability that only work for specific body regions or voxel spacings,\n(2) slow inference, which is a common issue for diffusion models, and (3) weak\nalignment with input conditions, which is a critical issue for medical imaging.\nMAISI, a previously proposed framework, addresses generalizability issues but\nstill suffers from slow inference and limited condition consistency. In this\nwork, we present MAISI-v2, the first accelerated 3D medical image synthesis\nframework that integrates rectified flow to enable fast and high quality\ngeneration. To further enhance condition fidelity, we introduce a novel\nregion-specific contrastive loss to enhance the sensitivity to region of\ninterest. Our experiments show that MAISI-v2 can achieve SOTA image quality\nwith $33 \\times$ acceleration for latent diffusion model. We also conducted a\ndownstream segmentation experiment to show that the synthetic images can be\nused for data augmentation. We release our code, training details, model\nweights, and a GUI demo to facilitate reproducibility and promote further\ndevelopment within the community.", "AI": {"tldr": "MAISI-v2是一个加速的3D医学图像合成框架，通过结合整流流和区域特异性对比损失，解决了现有扩散模型在通用性、推理速度和条件对齐方面的不足，实现了SOTA图像质量和显著加速。", "motivation": "现有医学图像合成方法（特别是扩散模型）面临三大挑战：1) 通用性受限，无法适应不同身体部位或体素间距；2) 推理速度慢；3) 输入条件对齐性弱，这在医学成像中尤为关键。之前的MAISI框架解决了通用性问题，但仍存在推理慢和条件一致性差的问题。", "method": "本文提出了MAISI-v2。为实现快速高质量生成，它集成了整流流（rectified flow）。为进一步增强条件保真度，引入了一种新颖的区域特异性对比损失（region-specific contrastive loss），以提高对感兴趣区域的敏感性。", "result": "MAISI-v2在图像质量上达到了SOTA水平，并使潜在扩散模型加速了33倍。此外，下游分割实验表明，合成图像可用于数据增强。", "conclusion": "MAISI-v2是首个加速的3D医学图像合成框架，有效解决了现有扩散模型在速度和条件对齐方面的痛点，生成了高质量且可用于数据增强的图像。该工作通过代码和模型发布，促进了社区的复现和进一步发展。"}}
{"id": "2508.05830", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05830", "abs": "https://arxiv.org/abs/2508.05830", "authors": ["Tong Li", "Rasiq Hussain", "Mehak Gupta", "Joshua R. Oltmanns"], "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated", "comment": "39 pages, 9 figures", "summary": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment.", "AI": {"tldr": "本研究比较了抑郁症评估中“镜像模型”（使用与评估内容直接相关的语言数据）与“非镜像模型”（使用不直接相关的语言数据）的性能，发现镜像模型存在判据污染导致的效应量虚高，非镜像模型更具泛化性。", "motivation": "现有研究中，许多LLM预测抑郁症评估分数时直接使用与评估内容相关的语言响应（“镜像模型”），这导致了“判据污染”和人工效应量膨胀，降低了模型的泛化能力。本研究旨在比较镜像模型与非镜像模型的性能差异。", "method": "研究招募了110名参与者，完成了结构化诊断访谈和生活史访谈。使用GPT-4、GPT-4o和LLaMA3-70B分别从这两种访谈文本中预测结构化诊断访谈的抑郁症分数。将使用结构化诊断数据的模型称为“镜像模型”，使用生活史数据的模型称为“非镜像模型”。最后，将两种模型预测的分数与自我报告的抑郁症状进行关联分析，并进行主题建模。", "result": "镜像模型（使用结构化诊断数据）表现出非常大的效应量（R2 = 0.80）。非镜像模型（使用生活史数据）的效应量较小但仍相对较大（R2 = 0.27）。当镜像模型和非镜像模型预测的抑郁症分数与自我报告的抑郁症状相关联时，两者表现相同（r ≈ 0.54），这表明镜像模型可能包含由于判据污染引起的偏差。主题建模识别了镜像模型和非镜像模型之间以及真阳性预测和假阳性预测之间的聚类。", "conclusion": "在本次比较研究中，抑郁症的镜像语言AI模型显示出人工膨胀的效应量和较低的泛化性。随着抑郁症语言AI模型的发展，整合非镜像模型可能有助于识别可解释、可泛化的语义特征，这些特征在实际心理评估中具有独特的效用。"}}
{"id": "2508.05946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05946", "abs": "https://arxiv.org/abs/2508.05946", "authors": ["Nello Balossino", "Rossana Damiano", "Cristina Gena", "Alberto Lillo", "Anna Maria Marras", "Claudio Mattutino", "Antonio Pizzo", "Alessia Prin", "Fabiana Vernero"], "title": "Social and Telepresence Robots for Accessibility and Inclusion in Small Museums", "comment": null, "summary": "There are still many museums that present accessibility barriers,\nparticularly regarding perceptual, cultural, and cognitive aspects. This is\nespecially evident in low-density population areas. The aim of the ROBSO-PM\nproject is to improve the accessibility of small museums through the use of\nsocial robots and social telepresence robots, focusing on three museums as case\nstudies: the Museum of the Holy Shroud in Turin, a small but globally known\ninstitution, and two lesser known mountain museums: the Museum of the Champlas\ndu Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and\nTraditions. The project explores two main applications for robots: as guides\nsupporting inclusive visits for foreign or disabled visitors, and as\ntelepresence tools allowing people with limited mobility to access museums\nremotely. From a research perspective, key topics include storytelling, robot\npersonality, empathy, personalization, and, in the case of telepresence,\ncollaboration between the robot and the person, with clearly defined roles and\nautonomy.", "AI": {"tldr": "ROBSO-PM项目旨在通过社交机器人和社交临场感机器人，改善小型博物馆的可访问性，尤其关注外国游客、残疾人及行动不便者。", "motivation": "许多博物馆，特别是人口密度低地区的博物馆，存在可访问性障碍，尤其是在感知、文化和认知方面。", "method": "ROBSO-PM项目使用社交机器人和社交临场感机器人作为主要工具，以三个博物馆（都灵圣裹尸布博物馆、尚普拉斯杜科尔狂欢节博物馆、普拉杰拉托高山民族服饰和传统博物馆）作为案例研究。机器人主要有两种应用：作为导游支持外国或残疾游客的包容性参观；作为临场感工具，允许行动不便者远程访问博物馆。研究重点包括故事讲述、机器人个性、同理心、个性化以及机器人与人的协作。", "result": "本摘要描述了项目目标和方法，尚未呈现具体研究结果，项目仍在进行中。", "conclusion": "该项目旨在利用机器人技术提升小型博物馆的可访问性和包容性，并通过深入研究人机交互的关键要素，为未来博物馆体验提供新的可能性。"}}
{"id": "2508.05888", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05888", "abs": "https://arxiv.org/abs/2508.05888", "authors": ["Sahil Bansal", "Sai Shruthi Sistla", "Aarti Arikatala", "Sebastian Schreiber"], "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning", "comment": null, "summary": "Effective tool retrieval is essential for AI agents to select from a vast\narray of tools when identifying and planning actions in the context of complex\nuser queries. Despite its central role in planning, this aspect remains\nunderexplored in the literature. Traditional approaches rely primarily on\nsimilarities between user queries and tool descriptions, which significantly\nlimits retrieval accuracy, specifically when handling multi-step user requests.\nTo address these limitations, we propose a Knowledge Graph (KG)-based tool\nretrieval framework that captures the semantic relationships between tools and\ntheir functional dependencies. Our retrieval algorithm leverages ensembles of\n1-hop ego tool graphs to model direct and indirect connections between tools,\nenabling more comprehensive and contextual tool selection for multi-step tasks.\nWe evaluate our approach on a synthetically generated internal dataset across\nsix defined user classes, extending previous work on coherent dialogue\nsynthesis and too retrieval benchmarks. Results demonstrate that our tool\ngraph-based method achieves 91.85% tool coverage on the micro-average Complete\nRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid\nretrieval, the strongest non-KG baseline in our experiments. These findings\nsupport our hypothesis that the structural information in the KG provides\ncomplementary signals to pure similarity matching, particularly for queries\nrequiring sequential tool composition.", "AI": {"tldr": "本文提出了一种基于知识图谱（KG）的工具检索框架，用于提高AI代理在处理复杂多步用户请求时工具选择的准确性，通过捕捉工具间的语义关系和功能依赖性，显著优于传统基于相似性的方法。", "motivation": "在AI代理中，有效的工具检索对于复杂用户查询的行动规划至关重要。然而，现有方法主要依赖于用户查询与工具描述之间的相似性，这在处理多步用户请求时严重限制了检索准确性，导致该领域研究不足。", "method": "研究者提出了一个基于知识图谱（KG）的工具检索框架，该框架能够捕获工具之间的语义关系和功能依赖性。其检索算法利用1跳自我工具图（1-hop ego tool graphs）的集成来建模工具之间的直接和间接连接。该方法在一个包含六种用户类别的合成内部数据集上进行了评估。", "result": "该工具图谱方法在微平均完全召回率（micro-average Complete Recall）指标上实现了91.85%的工具覆盖率。相比之下，实验中最强的非KG基线（重排序语义-词汇混合检索）的覆盖率为89.26%。", "conclusion": "研究结果支持了知识图谱中的结构信息为纯相似性匹配提供了补充信号的假设，这对于需要顺序工具组合的查询尤其有效。"}}
{"id": "2508.05783", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05783", "abs": "https://arxiv.org/abs/2508.05783", "authors": ["Mengyu Li", "Guoyao Shen", "Chad W. Farris", "Xin Zhang"], "title": "Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks", "comment": "30 pages, 8 figures, 7 tables", "summary": "Machine learning using transformers has shown great potential in medical\nimaging, but its real-world applicability remains limited due to the scarcity\nof annotated data. In this study, we propose a practical framework for the\nfew-shot deployment of pretrained MRI transformers in diverse brain imaging\ntasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a\nlarge-scale, multi-cohort brain MRI dataset comprising over 31 million slices,\nwe obtain highly transferable latent representations that generalize well\nacross tasks and datasets. For high-level tasks such as classification, a\nfrozen MAE encoder combined with a lightweight linear head achieves\nstate-of-the-art accuracy in MRI sequence identification with minimal\nsupervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a\nhybrid architecture that fuses multiscale CNN features with pretrained MAE\nembeddings. This model consistently outperforms other strong baselines in both\nskull stripping and multi-class anatomical segmentation under data-limited\nconditions. With extensive quantitative and qualitative evaluations, our\nframework demonstrates efficiency, stability, and scalability, suggesting its\nsuitability for low-resource clinical environments and broader neuroimaging\napplications.", "AI": {"tldr": "本研究提出了一个实用的框架，用于在数据稀缺条件下，将预训练的MRI Transformer模型（基于MAE）应用于多种脑部影像任务，并在分类和分割任务中取得了优异表现。", "motivation": "Transformer模型在医学影像领域潜力巨大，但由于带标注数据稀缺，其在现实世界的应用受到限制。", "method": "研究采用Masked Autoencoder (MAE) 预训练策略，在包含超过3100万切片的大规模多队列脑部MRI数据集上进行预训练，以获得可高度迁移的潜在表示。对于分类等高级任务，结合冻结的MAE编码器和轻量级线性头。对于分割等低级任务，提出MAE-FUnet混合架构，该架构融合了多尺度CNN特征和预训练的MAE嵌入。", "result": "该框架获得了高度可迁移的潜在表示。在MRI序列识别等高级分类任务中，以极少监督实现了最先进的准确性。在数据受限条件下，MAE-FUnet在颅骨剥离和多类别解剖分割等低级任务中持续优于其他强基线模型。", "conclusion": "该框架通过广泛的定量和定性评估，展示了其效率、稳定性和可扩展性，表明其适用于资源匮乏的临床环境和更广泛的神经影像应用。"}}
{"id": "2508.05843", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05843", "abs": "https://arxiv.org/abs/2508.05843", "authors": ["Miles Gilberti", "Shane Storks", "Huteng Dai"], "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication", "comment": null, "summary": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes.", "AI": {"tldr": "该研究通过引入小词汇量和模拟屈折形态学，重新诠释了新兴通信设置，发现模拟语音约束促进了粘着形态学，且新兴语言再现了自然语言融合语法属性的趋势。", "motivation": "当前新兴通信（EmCom）研究主要关注属性与字符的一一对应和句法组合，但对人类语言的本质理解有限。研究旨在探索更接近自然语言的特性，如双重发音和屈折形态学。", "method": "研究重新诠释了常见的属性-值重建博弈，通过施加小词汇量约束来模拟双重发音。同时，提出了一种新的、类似于自然屈折形态学的设置，并开发了新的度量标准。实验探索了受屈折形态学真实属性（如粘着性和融合性）启发的博弈变体。", "result": "实验发现，模拟的语音约束鼓励了粘着形态学的出现，并且新兴语言复制了自然语言融合语法属性的趋势。", "conclusion": "该研究表明，通过模拟语音约束和探索屈折形态学，新兴通信模型能够揭示自然语言中双重发音和形态学特性的出现机制，特别是粘着性和融合性如何受到约束的影响。"}}
{"id": "2508.05972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05972", "abs": "https://arxiv.org/abs/2508.05972", "authors": ["Shaoting Liu", "Zhou Liu"], "title": "Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles", "comment": null, "summary": "Air-land bimodal vehicles provide a promising solution for navigating complex\nenvironments by combining the flexibility of aerial locomotion with the energy\nefficiency of ground mobility. To enhance the robustness of trajectory planning\nunder environmental disturbances, this paper presents a disturbance-aware\nplanning framework that incorporates real-time disturbance estimation into both\npath searching and trajectory optimization. A key component of the framework is\na disturbance-adaptive safety boundary adjustment mechanism, which dynamically\nmodifies the vehicle's feasible dynamic boundaries based on estimated\ndisturbances to ensure trajectory feasibility. Leveraging the dynamics model of\nthe bimodal vehicle, the proposed approach achieves adaptive and reliable\nmotion planning across different terrains and operating conditions. A series of\nreal-world experiments and benchmark comparisons on a custom-built platform\nvalidate the effectiveness and robustness of the method, demonstrating\nimprovements in tracking accuracy, task efficiency, and energy performance\nunder both ground and aerial disturbances.", "AI": {"tldr": "本文提出了一种扰动感知规划框架，通过实时估计环境扰动并动态调整安全边界，提高了空地两用车辆在复杂环境下的轨迹规划鲁棒性。", "motivation": "空地两用车辆结合了空中机动的灵活性和地面移动的能效，为复杂环境导航提供了前景。然而，在环境扰动下，需要增强轨迹规划的鲁棒性。", "method": "提出了一种扰动感知规划框架，将实时扰动估计融入路径搜索和轨迹优化中。核心机制是扰动自适应安全边界调整，根据估计的扰动动态修改车辆的可行动态边界，以确保轨迹可行性。该方法利用双模式车辆的动力学模型。", "result": "通过一系列真实世界实验和基准比较，验证了该方法的有效性和鲁棒性。结果表明，在地面和空中扰动下，跟踪精度、任务效率和能源性能均有所提高。", "conclusion": "所提出的方法实现了在不同地形和操作条件下的自适应和可靠运动规划，显著提升了空地两用车辆在存在扰动情况下的轨迹规划性能和鲁棒性。"}}
{"id": "2508.05996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05996", "abs": "https://arxiv.org/abs/2508.05996", "authors": ["Kaitao Chen", "Mianxin Liu", "Daoming Zong", "Chaoyue Ding", "Shaohao Rui", "Yankai Jiang", "Mu Zhou", "Xiaosong Wang"], "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making", "comment": "14 pages, 4 figures", "summary": "Complex medical decision-making involves cooperative workflows operated by\ndifferent clinicians. Designing AI multi-agent systems can expedite and augment\nhuman-level clinical decision-making. Existing multi-agent researches primarily\nfocus on language-only tasks, yet their extension to multimodal scenarios\nremains challenging. A blind combination of diverse vision-language models\n(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are\nless capable in instruction following and importantly self-reflection, compared\nto large language models (LLMs) of comparable sizes. This disparity largely\nconstrains VLMs' ability in cooperative workflows. In this study, we propose\nMedOrch, a mediator-guided multi-agent collaboration framework for medical\nmultimodal decision-making. MedOrch employs an LLM-based mediator agent that\nenables multiple VLM-based expert agents to exchange and reflect on their\noutputs towards collaboration. We utilize multiple open-source general-purpose\nand domain-specific VLMs instead of costly GPT-series models, revealing the\nstrength of heterogeneous models. We show that the collaboration within\ndistinct VLM-based agents can surpass the capabilities of any individual agent.\nWe validate our approach on five medical vision question answering benchmarks,\ndemonstrating superior collaboration performance without model training. Our\nfindings underscore the value of mediator-guided multi-agent collaboration in\nadvancing medical multimodal intelligence. Our code will be made publicly\navailable.", "AI": {"tldr": "MedOrch是一个用于医疗多模态决策的调解器引导多智能体协作框架，它利用一个基于LLM的调解器来协调多个基于VLM的专家智能体，以实现超越单一智能体的性能。", "motivation": "复杂的医疗决策需要多方协作；现有多智能体研究主要集中在纯语言任务，难以扩展到多模态场景；盲目组合视觉-语言模型（VLMs）可能放大错误；与大型语言模型（LLMs）相比，VLMs在指令遵循和自我反思能力上较弱，限制了其在协作工作流中的应用。", "method": "提出MedOrch框架，采用一个基于LLM的调解器智能体，使其能够引导多个基于VLM的专家智能体交换和反思其输出，从而实现协作。该方法使用多个开源的通用和领域特定VLM，而非昂贵的GPT系列模型，且无需模型训练。", "result": "不同VLM-based智能体之间的协作能力超越了任何单个智能体。在五个医疗视觉问答基准上验证了该方法，展示了卓越的协作性能。", "conclusion": "调解器引导的多智能体协作在推进医疗多模态智能方面具有重要价值。"}}
{"id": "2508.05813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05813", "abs": "https://arxiv.org/abs/2508.05813", "authors": ["Raphael Du Sablon", "David Hart"], "title": "Optimization-Free Style Transfer for 3D Gaussian Splats", "comment": null, "summary": "The task of style transfer for 3D Gaussian splats has been explored in many\nprevious works, but these require reconstructing or fine-tuning the splat while\nincorporating style information or optimizing a feature extraction network on\nthe splat representation. We propose a reconstruction- and optimization-free\napproach to stylizing 3D Gaussian splats. This is done by generating a graph\nstructure across the implicit surface of the splat representation. A\nfeed-forward, surface-based stylization method is then used and interpolated\nback to the individual splats in the scene. This allows for any style image and\n3D Gaussian splat to be used without any additional training or optimization.\nThis also allows for fast stylization of splats, achieving speeds under 2\nminutes even on consumer-grade hardware. We demonstrate the quality results\nthis approach achieves and compare to other 3D Gaussian splat style transfer\nmethods. Code is publicly available at\nhttps://github.com/davidmhart/FastSplatStyler.", "AI": {"tldr": "该论文提出了一种无需重建或优化的方法，通过在3D高斯泼溅的隐式表面上构建图结构并应用前馈表面风格化，实现快速的3D高斯泼溅风格迁移。", "motivation": "以往的3D高斯泼溅风格迁移方法需要重建、微调泼溅或优化特征提取网络，这些方法耗时且计算成本高。", "method": "通过在3D高斯泼溅表示的隐式表面上生成图结构，然后使用基于表面的前馈风格化方法，并将风格信息插值回单个泼溅，实现风格迁移。", "result": "该方法实现了快速的泼溅风格化，在消费级硬件上也能在2分钟内完成；无需额外的训练或优化即可应用于任何风格图像和3D高斯泼溅；展示了高质量的风格化结果，并优于其他现有方法。", "conclusion": "本研究提出了一种新颖、高效且高质量的3D高斯泼溅风格迁移方法，该方法避免了传统方法的重建和优化需求，显著提高了速度和便捷性。"}}
{"id": "2508.05880", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05880", "abs": "https://arxiv.org/abs/2508.05880", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available.", "AI": {"tldr": "该研究超越了大型语言模型（LLMs）对情感的表面识别，通过认知评估理论探究LLMs如何对情感进行认知推理，并引入了大规模基准测试CoRE来评估其内部认知结构。", "motivation": "现有LLMs情感研究多采用监督学习，局限于离散情感标签和表面任务（如情感识别）。研究者认为需要深入探究LLMs如何进行情感推理，而非仅仅识别情感。", "method": "基于认知评估理论，引入了大规模基准测试CoRE（Cognitive Reasoning for Emotions），旨在评估LLMs在处理情感刺激时是否能产生连贯且合理的认知推理，并探究其隐式使用的内部认知结构。通过大量实验，研究回答了LLMs是否依赖特定认知评估维度、哪些维度对特定情感重要，以及LLMs内部情感表征是否能通过认知评估维度解释等问题。", "result": "实验结果和分析揭示了不同LLMs在情感推理上表现出多样化的推理模式。", "conclusion": "本研究提供了一个新的基准测试，深入分析了LLMs通过认知维度进行情感推理的能力，并揭示了不同模型间的情感推理模式差异。相关基准和代码将公开。"}}
{"id": "2508.06053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06053", "abs": "https://arxiv.org/abs/2508.06053", "authors": ["Kaixuan Wu", "Yuanzhuo Xu", "Zejun Zhang", "Weiping Zhu", "Steve Drew", "Xiaoguang Niu"], "title": "ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference", "comment": null, "summary": "Pedestrian inertial localization is key for mobile and IoT services because\nit provides infrastructure-free positioning. Yet most learning-based methods\ndepend on fixed sliding-window integration, struggle to adapt to diverse motion\nscales and cadences, and yield inconsistent uncertainty, limiting real-world\nuse. We present ReNiL, a Bayesian deep-learning framework for accurate,\nefficient, and uncertainty-aware pedestrian localization. ReNiL introduces\nInertial Positioning Demand Points (IPDPs) to estimate motion at contextually\nmeaningful waypoints instead of dense tracking, and supports inference on IMU\nsequences at any scale so cadence can match application needs. It couples a\nmotion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a\ndual-task network that blends patch-based self-supervision with Bayesian\nregression. By modeling displacements with a Laplace distribution, ReNiL\nprovides homogeneous Euclidean uncertainty that integrates cleanly with other\nsensors. A Bayesian inference chain links successive IPDPs into consistent\ntrajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor\nmotion from 28 participants, ReNiL achieves state-of-the-art displacement\naccuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN\nvariants while reducing computation. Application studies further show\nrobustness and practicality for mobile and IoT localization, making ReNiL a\nscalable, uncertainty-aware foundation for next-generation positioning.", "AI": {"tldr": "ReNiL是一个基于贝叶斯深度学习的行人惯性定位框架，通过引入IPDPs和ASLE，实现了高精度、高效且具有不确定性感知的定位，克服了现有方法的局限性，适用于各种运动尺度和节奏。", "motivation": "大多数基于学习的行人惯性定位方法依赖于固定的滑动窗口，难以适应不同的运动尺度和节奏，并且产生不一致的不确定性，这限制了它们在现实世界中的应用。", "method": "ReNiL是一个贝叶斯深度学习框架。它引入了惯性定位需求点（IPDPs）来在情境上有意义的航路点估计运动，而非密集跟踪；支持任意尺度的IMU序列推理。它将运动感知方向滤波器与任意尺度拉普拉斯估计器（ASLE）相结合，ASLE是一个双任务网络，融合了基于补丁的自监督和贝叶斯回归。通过拉普拉斯分布建模位移，ReNiL提供均匀的欧几里得不确定性，并通过贝叶斯推理链将连续的IPDPs链接成一致的轨迹。", "result": "在RoNIN-ds和新的WUDataset（涵盖28名参与者的室内外运动）上，ReNiL实现了最先进的位移精度和不确定性一致性，优于TLIO、CTIN、iMoT和RoNIN变体，同时降低了计算量。应用研究进一步表明其在移动和物联网定位中的鲁棒性和实用性。", "conclusion": "ReNiL为下一代定位提供了一个可扩展、不确定性感知的坚实基础，解决了现有方法的不足，并为移动和物联网定位提供了鲁棒且实用的解决方案。"}}
{"id": "2508.06042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06042", "abs": "https://arxiv.org/abs/2508.06042", "authors": ["Daechul Ahn", "San Kim", "Jonghyun Choi"], "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning", "comment": "COLM 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive action\nsequence prediction capabilities but often struggle with dynamic, long-horizon\ntasks such as real-time strategic games. In a game such as StarCraftII (SC2),\nagents need to manage resource constraints and adapt to evolving battlefield\nsituations in a partially observable environment. This often overwhelms\nexisiting LLM-based approaches. To address these challenges, we propose a\nhierarchical multi-agent framework that employs specialized imitation learning\nagents under a meta-controller called Strategic Planner (SP). By expert\ndemonstrations, each specialized agent learns a distinctive strategy, such as\naerial support or defensive maneuvers, and produces coherent, structured\nmultistep action sequences. The SP then orchestrates these proposals into a\nsingle, environmentally adaptive plan that ensures local decisions aligning\nwith long-term strategies. We call this HIMA (Hierarchical Imitation\nMulti-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that\nencompasses all race match combinations in SC2. Our empirical results show that\nHIMA outperforms state of the arts in strategic clarity, adaptability, and\ncomputational efficiency, underscoring the potential of combining specialized\nimitation modules with meta-level orchestration to develop more robust,\ngeneral-purpose AI agents.", "AI": {"tldr": "提出HIMA（分层模仿多智能体）框架，结合专用模仿学习智能体和元控制器，用于解决星际争霸II等动态、长时程战略游戏中的挑战，并超越现有技术。", "motivation": "大型语言模型（LLMs）在动态、长时程任务（如星际争霸II）中表现不佳，难以管理资源约束和适应部分可观察环境中的战场变化，这使得现有基于LLM的方法不堪重负。", "method": "提出HIMA框架，采用分层的多智能体方法：1) 专用模仿学习智能体通过专家演示学习特定策略（如空中支援或防御机动），生成连贯的多步动作序列。2) 一个名为“战略规划器”（SP）的元控制器协调这些提议，形成适应环境的单一计划，确保局部决策与长期战略保持一致。同时，提出了一个全面的星际争霸II测试平台TEXTSCII-ALL，包含所有种族对战组合。", "result": "HIMA在战略清晰度、适应性和计算效率方面均优于现有最先进方法，证明了结合专用模仿模块与元级编排的潜力。", "conclusion": "将专用模仿模块与元级编排相结合，是开发更鲁棒、通用AI智能体的有效途径，尤其适用于像星际争霸II这样复杂的动态战略游戏。"}}
{"id": "2508.05819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05819", "abs": "https://arxiv.org/abs/2508.05819", "authors": ["Jong-Ik Park", "Carlee Joe-Wong", "Gary K. Fedder"], "title": "MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses", "comment": null, "summary": "Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from\nmultiple 2D images, even those taken with unknown camera poses. However, they\nstill miss the fine-detailed structures that matter in industrial inspection,\ne.g., detecting sub-micron defects on a production line or analyzing chips with\nScanning Electron Microscopy (SEM). In these scenarios, the sensor resolution\nis fixed and compute budgets are tight, so the only way to expose fine\nstructure is to add zoom-in images; yet, this breaks the multi-view consistency\nthat pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF\n(MZEN), the first NeRF framework that natively handles multi-zoom image sets.\nMZEN (i) augments the pin-hole camera model with an explicit, learnable zoom\nscalar that scales the focal length, and (ii) introduces a novel pose strategy:\nwide-field images are solved first to establish a global metric frame, and\nzoom-in images are then pose-primed to the nearest wide-field counterpart via a\nzoom-consistent crop-and-match procedure before joint refinement. Across eight\nforward-facing scenes$\\unicode{x2013}$synthetic TCAD models, real SEM of\nmicro-structures, and BLEFF objects$\\unicode{x2013}$MZEN consistently\noutperforms pose-free baselines and even high-resolution variants, boosting\nPSNR by up to $28 \\%$, SSIM by $10 \\%$, and reducing LPIPS by up to $222 \\%$.\nMZEN, therefore, extends NeRF to real-world factory settings, preserving global\naccuracy while capturing the micron-level details essential for industrial\ninspection.", "AI": {"tldr": "MZEN是首个原生处理多变焦图像集的NeRF框架，通过引入可学习的变焦标量和新颖的姿态策略，解决了传统NeRF在工业检测中无法捕捉微米级细节的问题，同时保持了全局精度。", "motivation": "现有NeRF方法在工业检测场景（如亚微米缺陷检测）中，无法捕获精细结构。当加入变焦图像时，会破坏多视角一致性，使得依赖无姿态训练的NeRF失效，而传感器分辨率和计算预算是固定的。", "method": "MZEN通过以下两点解决问题：(i) 扩展针孔相机模型，引入一个显式且可学习的变焦标量来缩放焦距；(ii) 引入新颖的姿态策略：首先解决广角图像以建立全局度量框架，然后通过变焦一致的裁剪匹配过程，将变焦图像的姿态预对准到最近的广角对应物，最后进行联合优化。", "result": "在八个前向场景（包括合成TCAD模型、微结构的真实SEM图像和BLEFF物体）中，MZEN始终优于无姿态基线甚至高分辨率变体，PSNR提升高达28%，SSIM提升10%，LPIPS降低高达222%。", "conclusion": "MZEN将NeRF扩展到真实的工厂设置，在保持全局精度的同时，捕获了工业检测中必不可少的微米级细节，解决了传统NeRF在处理多变焦图像和精细结构重建方面的局限性。"}}
{"id": "2508.05909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05909", "abs": "https://arxiv.org/abs/2508.05909", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Siya Qi", "Yulan He", "Hanqi Yan", "Lin Gui"], "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation", "comment": null, "summary": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation.", "AI": {"tldr": "该研究提出SPS（Spectrum Projection Score），一个轻量级、无监督的度量标准，用于评估检索到的摘要与LLM隐藏表示的语义对齐，并在此基础上开发了xCompress框架，用于动态采样、排序和压缩检索候选摘要，从而提升了检索增强生成（RAG）的性能。", "motivation": "现有的检索增强生成（RAG）评估方法通常是整体性的，难以单独衡量检索器的真实贡献，并且忽略了大型语言模型（LLM）作为阅读器对提示的敏感性。", "method": "引入SPS（Spectrum Projection Score），一个轻量级、无监督的度量标准，通过比较摘要生成标记形成的区域与阅读器子空间主方向，来衡量检索到的摘要与模型隐藏表示的语义对齐程度和相关性。基于SPS，提出了xCompress，一个推理时控制器框架，用于动态采样、排序和压缩检索摘要候选。", "result": "在五个问答基准和四个开源LLM上进行的广泛实验表明，SPS不仅在各种任务中提升了性能，而且为检索与生成之间的交互提供了原则性的视角。", "conclusion": "SPS（Spectrum Projection Score）提升了检索增强生成（RAG）的性能，并为理解检索和生成之间的互动提供了新的见解。"}}
{"id": "2508.06095", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06095", "abs": "https://arxiv.org/abs/2508.06095", "authors": ["Mitchell Abrams", "Thies Oelerich", "Christian Hartl-Nesic", "Andreas Kugi", "Matthias Scheutz"], "title": "Incremental Language Understanding for Online Motion Planning of Robot Manipulators", "comment": "8 pages, 9 figures, accepted at IROS 2025", "summary": "Human-robot interaction requires robots to process language incrementally,\nadapting their actions in real-time based on evolving speech input. Existing\napproaches to language-guided robot motion planning typically assume fully\nspecified instructions, resulting in inefficient stop-and-replan behavior when\ncorrections or clarifications occur. In this paper, we introduce a novel\nreasoning-based incremental parser which integrates an online motion planning\nalgorithm within the cognitive architecture. Our approach enables continuous\nadaptation to dynamic linguistic input, allowing robots to update motion plans\nwithout restarting execution. The incremental parser maintains multiple\ncandidate parses, leveraging reasoning mechanisms to resolve ambiguities and\nrevise interpretations when needed. By combining symbolic reasoning with online\nmotion planning, our system achieves greater flexibility in handling speech\ncorrections and dynamically changing constraints. We evaluate our framework in\nreal-world human-robot interaction scenarios, demonstrating online adaptions of\ngoal poses, constraints, or task objectives. Our results highlight the\nadvantages of integrating incremental language understanding with real-time\nmotion planning for natural and fluid human-robot collaboration. The\nexperiments are demonstrated in the accompanying video at\nwww.acin.tuwien.ac.at/42d5.", "AI": {"tldr": "本文提出一种新颖的基于推理的增量解析器，结合在线运动规划，使机器人能够持续适应动态语言输入，无需中断执行即可实时更新运动计划，从而实现更自然流畅的人机协作。", "motivation": "现有语言引导的机器人运动规划方法通常假设指令是完全指定的，导致在出现纠正或澄清时，机器人需要停止并重新规划，效率低下，无法满足人机交互中实时适应语言输入的需求。", "method": "引入一个基于推理的增量解析器，并将其集成到认知架构中的在线运动规划算法中。该解析器维护多个候选解析，并利用推理机制解决歧义和修正解释。通过结合符号推理和在线运动规划，实现对动态语言输入的连续适应。", "result": "该框架使机器人能够在不中断执行的情况下更新运动计划，并能灵活处理语音纠正和动态变化的约束。在真实人机交互场景中，成功展示了对目标姿态、约束或任务目标的在线适应能力。", "conclusion": "将增量语言理解与实时运动规划相结合，对于实现自然流畅的人机协作具有显著优势，能够有效处理动态变化的语言输入和任务需求。"}}
{"id": "2508.06060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06060", "abs": "https://arxiv.org/abs/2508.06060", "authors": ["Sankarshan Damle", "Boi Faltings"], "title": "LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences", "comment": "Published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence (ECAI 2025)", "summary": "Large Language Models (LLMs) are increasingly expected to handle complex\ndecision-making tasks, yet their ability to perform structured resource\nallocation remains underexplored. Evaluating their reasoning is also difficult\ndue to data contamination and the static nature of existing benchmarks. We\npresent a dual-purpose framework leveraging Participatory Budgeting (PB) both\nas (i) a practical setting for LLM-based resource allocation and (ii) an\nadaptive benchmark for evaluating their reasoning capabilities. We task LLMs\nwith selecting project subsets under feasibility (e.g., budget) constraints via\nthree prompting strategies: greedy selection, direct optimization, and a\nhill-climbing-inspired refinement. We benchmark LLMs' allocations against a\nutility-maximizing oracle. Interestingly, we also test whether LLMs can infer\nstructured preferences from natural-language voter input or metadata, without\nexplicit votes. By comparing allocations based on inferred preferences to those\nfrom ground-truth votes, we evaluate LLMs' ability to extract preferences from\nopen-ended input. Our results underscore the role of prompt design and show\nthat LLMs hold promise for mechanism design with unstructured inputs.", "AI": {"tldr": "该研究利用参与式预算（PB）框架评估大型语言模型（LLMs）在结构化资源分配方面的能力，并将其作为一种自适应基准，同时探讨LLMs从非结构化输入中推断偏好的能力。", "motivation": "LLMs在复杂决策任务中的应用日益增长，但其在结构化资源分配方面的能力尚未充分探索。此外，现有基准存在数据污染和静态性问题，难以有效评估LLMs的推理能力。", "method": "提出一个双重目的框架，将参与式预算（PB）作为LLM资源分配的实际场景和评估其推理能力的自适应基准。通过三种提示策略（贪婪选择、直接优化、受爬山算法启发的优化）让LLMs在预算约束下选择项目子集。将LLMs的分配结果与效用最大化的预言机进行基准测试。此外，还测试LLMs是否能从自然语言投票输入或元数据中推断出结构化偏好，并比较基于推断偏好和真实投票的分配结果。", "result": "研究结果强调了提示设计的重要性，并表明LLMs在处理非结构化输入进行机制设计方面具有潜力。", "conclusion": "LLMs在结构化资源分配和从非结构化输入中提取偏好方面表现出前景，尤其是在精心设计的提示下，它们有望应用于机制设计领域。"}}
{"id": "2508.05829", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05829", "abs": "https://arxiv.org/abs/2508.05829", "authors": ["Guoping Xu", "Hua-Chieh Shao", "You Zhang"], "title": "TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios", "comment": "23 pages, 5 figures", "summary": "Promptable video object segmentation and tracking (VOST) has seen significant\nadvances with the emergence of foundation models like Segment Anything Model 2\n(SAM2); however, their application in surgical video analysis remains\nchallenging due to complex motion dynamics and the redundancy of memory that\nimpedes effective learning. In this work, we propose TSMS-SAM2, a novel\nframework that enhances promptable VOST in surgical videos by addressing\nchallenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2\nintroduces two key strategies: multi-temporal-scale video sampling augmentation\nto improve robustness against motion variability, and a memory splitting and\npruning mechanism that organizes and filters past frame features for more\nefficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018\ndatasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,\nrespectively, outperforming prior SAM-based and task-specific methods.\nExtensive ablation studies confirm the effectiveness of multiscale temporal\naugmentation and memory splitting, highlighting the framework's potential for\nrobust, efficient segmentation in complex surgical scenarios. Our source code\nwill be available at https://github.com/apple1986/TSMS-SAM2.", "AI": {"tldr": "TSMS-SAM2是一个新颖的框架，通过多时间尺度视频采样增强和内存分割剪枝机制，提升了SAM2在手术视频中可提示视频对象分割与跟踪的性能，解决了快速运动和内存冗余问题。", "motivation": "尽管SAM2等基础模型在可提示视频对象分割与跟踪（VOST）方面取得了显著进展，但由于复杂运动动力学和阻碍有效学习的内存冗余，它们在手术视频分析中的应用仍面临挑战。", "method": "TSMS-SAM2引入了两种关键策略：1. 多时间尺度视频采样增强，以提高对运动变异性的鲁棒性。2. 内存分割与剪枝机制，用于组织和过滤过去帧特征，实现更高效和准确的分割。", "result": "TSMS-SAM2在EndoVis2017和EndoVis2018数据集上分别取得了95.24和86.73的最高平均Dice分数，优于先前的基于SAM和特定任务的方法。广泛的消融研究证实了多尺度时间增强和内存分割的有效性。", "conclusion": "TSMS-SAM2框架在复杂手术场景中具有实现鲁棒、高效分割的潜力，通过解决快速物体运动和内存冗余问题，显著提升了SAM2在手术视频VOST中的性能。"}}
{"id": "2508.05938", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "pdf": "https://arxiv.org/pdf/2508.05938", "abs": "https://arxiv.org/abs/2508.05938", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks.", "AI": {"tldr": "本文提出了一种三阶段的实用管道，用于可扩展、高精度的文本亲社会内容分类，最大限度地减少人工标注工作和推理成本。", "motivation": "检测文本中的亲社会性（旨在肯定、支持或改善他人行为的交流）是信任和安全系统面临的一个新兴且日益重要的挑战。与有害内容检测不同，亲社会性缺乏明确的定义和标注数据，需要新的标注和部署方法。", "method": "该方法包括三个阶段：1) 使用少量人工标注的种子集识别最佳的LLM（大型语言模型）标注策略。2) 引入人机协作细化循环，通过审查GPT-4和人类之间的高分歧案例，迭代地澄清和扩展任务定义。3) 利用GPT-4合成1万个高质量标签，并训练一个两阶段推理系统：轻量级分类器处理高置信度预测，而约35%的模糊实例才升级到GPT-4o处理。", "result": "该管道合成了1万个高质量标签，并将推理成本降低了约70%，同时实现了高精度（约0.90）。", "conclusion": "该管道展示了有针对性的人机交互、细致的任务制定和面向部署的架构设计如何为新型负责任的AI任务提供可扩展的解决方案。"}}
{"id": "2508.06096", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06096", "abs": "https://arxiv.org/abs/2508.06096", "authors": ["Eric Jing", "Abdeslam Boularias"], "title": "Bounding Distributional Shifts in World Modeling through Novelty Detection", "comment": "7 pages, 6 figures", "summary": "Recent work on visual world models shows significant promise in latent state\ndynamics obtained from pre-trained image backbones. However, most of the\ncurrent approaches are sensitive to training quality, requiring near-complete\ncoverage of the action and state space during training to prevent divergence\nduring inference. To make a model-based planning algorithm more robust to the\nquality of the learned world model, we propose in this work to use a\nvariational autoencoder as a novelty detector to ensure that proposed action\ntrajectories during planning do not cause the learned model to deviate from the\ntraining data distribution. To evaluate the effectiveness of this approach, a\nseries of experiments in challenging simulated robot environments was carried\nout, with the proposed method incorporated into a model-predictive control\npolicy loop extending the DINO-WM architecture. The results clearly show that\nthe proposed method improves over state-of-the-art solutions in terms of data\nefficiency.", "AI": {"tldr": "本文提出使用变分自编码器作为新颖性检测器，以增强基于视觉世界模型的规划算法的鲁棒性，使其在动作轨迹规划时避免偏离训练数据分布，从而提高数据效率。", "motivation": "当前的视觉世界模型对训练质量敏感，需要近乎完全覆盖动作和状态空间才能防止推理时发散。因此，需要使基于模型的规划算法对所学世界模型的质量更具鲁棒性。", "method": "本研究提出使用变分自编码器（VAE）作为新颖性检测器，以确保规划过程中提议的动作轨迹不会导致所学模型偏离训练数据分布。该方法被整合到扩展DINO-WM架构的模型预测控制（MPC）策略循环中，并在模拟机器人环境中进行评估。", "result": "实验结果表明，所提出的方法在数据效率方面优于现有最先进的解决方案。", "conclusion": "通过引入VAE作为新颖性检测器，本方法有效提高了基于视觉世界模型的规划算法的鲁棒性和数据效率，解决了模型对训练数据覆盖度敏感的问题。"}}
{"id": "2508.06062", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2508.06062", "abs": "https://arxiv.org/abs/2508.06062", "authors": ["Evgenii E. Vityaev", "Andrei Mantsivoda"], "title": "Don't Forget Imagination!", "comment": "14 pages, 2 figures", "summary": "Cognitive imagination is a type of imagination that plays a key role in human\nthinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to\nmentally visualize coherent and holistic systems of concepts and causal links\nthat serve as semantic contexts for reasoning, decision making and prediction.\nOur position is that the role of cognitive imagination is still greatly\nunderestimated, and this creates numerous problems and diminishes the current\ncapabilities of AI. For instance, when reasoning, humans rely on imaginary\ncontexts to retrieve background info. They also constantly return to the\ncontext for semantic verification that their reasoning is still reasonable.\nThus, reasoning without imagination is blind. This paper is a call for greater\nattention to cognitive imagination as the next promising breakthrough in\nartificial intelligence. As an instrument for simulating cognitive imagination,\nwe propose semantic models -- a new approach to mathematical models that can\nlearn, like neural networks, and are based on probabilistic causal\nrelationships. Semantic models can simulate cognitive imagination because they\nensure the consistency of imaginary contexts and implement a glass-box approach\nthat allows the context to be manipulated as a holistic and coherent system of\ninterrelated facts glued together with causal relations.", "AI": {"tldr": "本文定义了认知想象力，强调其在人类思维中的关键作用，并指出当前AI对其重视不足导致的能力限制。文章呼吁AI领域应更多关注认知想象力，并提出语义模型作为模拟认知想象力的工具，以期实现AI的突破。", "motivation": "人类的认知想象力在推理、决策和预测中扮演着核心角色，但目前AI对其作用的认识和利用严重不足，导致AI在推理等方面的能力受限（例如，“没有想象力的推理是盲目的”）。研究旨在解决这一问题，提升AI的能力。", "method": "本文提出“语义模型”作为模拟认知想象力的方法。语义模型是一种新型的数学模型，它能够像神经网络一样学习，并基于概率因果关系。其特点是能确保想象上下文的一致性，并采用“玻璃盒”方法，允许将上下文作为一个连贯的、相互关联的事实系统进行操作。", "result": "本文的主要成果是提出了一个概念框架和一种潜在的模拟认知想象力的方法。语义模型被提出能够模拟认知想象力，因为它们可以确保想象上下文的连贯性和一致性，并允许对作为整体和连贯系统的上下文进行操作和理解。", "conclusion": "认知想象力对人类思维至关重要，是AI未来突破的关键方向。研究呼吁AI领域应给予认知想象力更多关注，并提出语义模型作为模拟认知想象力的有效工具，有望推动人工智能的重大进展。"}}
{"id": "2508.05851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05851", "abs": "https://arxiv.org/abs/2508.05851", "authors": ["Ka-Wai Yung", "Felix J. S. Bragman", "Jialang Xu", "Imanol Luengo", "Danail Stoyanov", "Evangelos B. Mazomenos"], "title": "Temporal Cluster Assignment for Efficient Real-Time Video Segmentation", "comment": null, "summary": "Vision Transformers have substantially advanced the capabilities of\nsegmentation models across both image and video domains. Among them, the Swin\nTransformer stands out for its ability to capture hierarchical, multi-scale\nrepresentations, making it a popular backbone for segmentation in videos.\nHowever, despite its window-attention scheme, it still incurs a high\ncomputational cost, especially in larger variants commonly used for dense\nprediction in videos. This remains a major bottleneck for real-time,\nresource-constrained applications. Whilst token reduction methods have been\nproposed to alleviate this, the window-based attention mechanism of Swin\nrequires a fixed number of tokens per window, limiting the applicability of\nconventional pruning techniques. Meanwhile, training-free token clustering\napproaches have shown promise in image segmentation while maintaining window\nconsistency. Nevertheless, they fail to exploit temporal redundancy, missing a\nkey opportunity to further optimize video segmentation performance. We\nintroduce Temporal Cluster Assignment (TCA), a lightweight and effective,\nfine-tuning-free strategy that enhances token clustering by leveraging temporal\ncoherence across frames. Instead of indiscriminately dropping redundant tokens,\nTCA refines token clusters using temporal correlations, thereby retaining\nfine-grained details while significantly reducing computation. Extensive\nevaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical\nvideo dataset show that TCA consistently boosts the accuracy-speed trade-off of\nexisting clustering-based methods. Our results demonstrate that TCA generalizes\ncompetently across both natural and domain-specific videos.", "AI": {"tldr": "本文提出TCA（Temporal Cluster Assignment），一种轻量级、免微调的策略，通过利用帧间时间相关性优化Swin Transformer的token聚类，显著降低视频分割的计算成本，同时保持细粒度细节，提升了现有聚类方法的精度-速度权衡。", "motivation": "尽管Swin Transformer在视频分割中表现出色，但其计算成本高昂，尤其是在用于密集预测的大型变体中，成为实时、资源受限应用的瓶颈。现有token削减方法（如剪枝和聚类）未能充分利用视频中的时间冗余，且Swin的窗口注意力机制限制了传统剪枝技术的应用。", "method": "本文引入TCA（Temporal Cluster Assignment），一种轻量级、免微调的策略。TCA通过利用帧间的时间连贯性来增强token聚类，而非简单丢弃冗余token。它通过时间相关性精炼token簇，从而在显著减少计算量的同时保留了细粒度细节。", "result": "在YouTube-VIS 2019、YouTube-VIS 2021、OVIS和私有外科视频数据集上的广泛评估表明，TCA持续提升了现有基于聚类方法的精度-速度权衡。结果表明TCA在自然视频和特定领域视频中均表现出良好的泛化能力。", "conclusion": "TCA通过利用时间相干性有效优化了视频分割中Swin Transformer的token聚类，显著降低了计算成本并保持了性能。它为实时、资源受限的视频应用提供了有前景的解决方案，提升了现有聚类方法的效率。"}}
{"id": "2508.05987", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05987", "abs": "https://arxiv.org/abs/2508.05987", "authors": ["Chunyun Zhang", "Hongyan Zhao", "Chaoran Cui", "Qilong Song", "Zhiqing Lu", "Shuai Gong", "Kailin Liu"], "title": "Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring", "comment": null, "summary": "Cross-topic automated essay scoring (AES) aims to develop a transferable\nmodel capable of effectively evaluating essays on a target topic. A significant\nchallenge in this domain arises from the inherent discrepancies between topics.\nWhile existing methods predominantly focus on extracting topic-shared features\nthrough distribution alignment of source and target topics, they often neglect\ntopic-specific features, limiting their ability to assess critical traits such\nas topic adherence. To address this limitation, we propose an Adversarial\nTOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns\ntopic-shared and topic-specific features to improve cross-topic AES. ATOP\nachieves this by optimizing a learnable topic-aware prompt--comprising both\nshared and specific components--to elicit relevant knowledge from pre-trained\nlanguage models (PLMs). To enhance the robustness of topic-shared prompt\nlearning and mitigate feature scale sensitivity introduced by topic alignment,\nwe incorporate adversarial training within a unified regression and\nclassification framework. In addition, we employ a neighbor-based classifier to\nmodel the local structure of essay representations and generate pseudo-labels\nfor target-topic essays. These pseudo-labels are then used to guide the\nsupervised learning of topic-specific prompts tailored to the target topic.\nExtensive experiments on the publicly available ASAP++ dataset demonstrate that\nATOP significantly outperforms existing state-of-the-art methods in both\nholistic and multi-trait essay scoring. The implementation of our method is\npublicly available at: https://anonymous.4open.science/r/ATOP-A271.", "AI": {"tldr": "本文提出了一种名为ATOP的新方法，通过对抗性主题感知提示微调，联合学习主题共享和主题特定特征，以提高跨主题自动作文评分（AES）的准确性。", "motivation": "现有跨主题AES方法主要关注通过分布对齐提取主题共享特征，但忽略了主题特定特征，这限制了它们评估作文关键特质（如主题依从性）的能力，因为不同主题之间存在固有的差异。", "method": "ATOP通过优化一个可学习的主题感知提示（包含共享和特定组件）来从预训练语言模型中提取相关知识，从而联合学习主题共享和主题特定特征。为增强主题共享提示学习的鲁棒性并减少特征尺度敏感性，模型在一个统一的回归和分类框架中融入了对抗性训练。此外，采用基于邻居的分类器对作文表示的局部结构进行建模，并为目标主题作文生成伪标签，这些伪标签用于指导主题特定提示的监督学习。", "result": "在公开的ASAP++数据集上进行的广泛实验表明，ATOP在整体和多维度作文评分方面均显著优于现有最先进的方法。", "conclusion": "ATOP通过同时建模主题共享和主题特定特征，有效克服了现有跨主题AES方法的局限性，显著提高了跨主题作文评分的性能和鲁棒性。"}}
{"id": "2508.06181", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06181", "abs": "https://arxiv.org/abs/2508.06181", "authors": ["Jan Węgrzynowski", "Piotr Kicki", "Grzegorz Czechmanowski", "Maciej Krupka", "Krzysztof Walas"], "title": "Beyond Constant Parameters: Hyper Prediction Models and HyperMPC", "comment": null, "summary": "Model Predictive Control (MPC) is among the most widely adopted and reliable\nmethods for robot control, relying critically on an accurate dynamics model.\nHowever, existing dynamics models used in the gradient-based MPC are limited by\ncomputational complexity and state representation. To address this limitation,\nwe propose the Hyper Prediction Model (HyperPM) - a novel approach in which we\nproject the unmodeled dynamics onto a time-dependent dynamics model. This\ntime-dependency is captured through time-varying model parameters, whose\nevolution over the MPC prediction horizon is learned using a neural network.\nSuch formulation preserves the computational efficiency and robustness of the\nbase model while equipping it with the capacity to anticipate previously\nunmodeled phenomena. We evaluated the proposed approach on several challenging\nsystems, including real-world F1TENTH autonomous racing, and demonstrated that\nit significantly reduces long-horizon prediction errors. Moreover, when\nintegrated within the MPC framework (HyperMPC), our method consistently\noutperforms existing state-of-the-art techniques.", "AI": {"tldr": "该论文提出HyperPM模型，通过神经网络学习时变模型参数来预测未建模动力学，从而提高模型预测控制（MPC）的长期预测精度和控制性能。", "motivation": "梯度下降MPC中现有动力学模型受限于计算复杂性和状态表示，导致模型精度不足，无法有效处理未建模现象，影响控制性能。", "method": "提出超预测模型（HyperPM），将未建模动力学投影到时变动力学模型上。通过神经网络学习模型参数在MPC预测范围内的时变演化，从而捕捉时间依赖性。该方法保持了基础模型的计算效率和鲁棒性，并能预测之前未建模的现象。集成到MPC框架中称为HyperMPC。", "result": "在包括真实世界F1TENTH自动驾驶赛车在内的多个挑战性系统中，显著减少了长时域预测误差。当集成到MPC框架（HyperMPC）中时，该方法持续优于现有最先进技术。", "conclusion": "HyperPM方法通过学习时变未建模动力学，有效增强了MPC基础模型的预测能力，同时保持了计算效率和鲁棒性，并在实际应用中展现出卓越的性能。"}}
{"id": "2508.06064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06064", "abs": "https://arxiv.org/abs/2508.06064", "authors": ["Harold Silvère Kiossou", "Siegfried Nijssen", "Pierre Schaus"], "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree", "comment": null, "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.", "AI": {"tldr": "本文提出了一种名为CA-DL8.5的通用、完整且随时可用的束搜索算法，用于精确决策树学习，该算法扩展了DL8.5框架并统一了现有的一些随时可用策略。通过严格的实验比较，发现使用LDS（有限差异）的CA-DL8.5在随时可用性能上表现最佳。", "motivation": "寻找最小化分类错误的最佳决策树是NP-hard问题。虽然基于MILP、CP、SAT或动态规划的精确算法能保证最优性，但它们通常存在随时可用性（anytime behavior）差的问题，即在搜索未完成时难以快速找到高质量的决策树。现有的一些精确方法随时可用扩展（如LDS-DL8.5、Top-k-DL8.5和Blossom）尚未得到系统比较，难以评估其相对有效性。", "method": "本文提出了CA-DL8.5，一个通用、完整且随时可用的束搜索算法，它扩展了DL8.5框架并统一了现有的一些随时可用策略（如LDS-DL8.5和Top-k-DL8.5）。CA-DL8.5通过模块化设计，允许集成各种启发式和松弛机制。该算法重用了DL8.5高效的分支定界剪枝和Trie树缓存，并结合了基于重启的束搜索，逐步放松剪枝标准以随时间提高解决方案质量。通过使用原始间隙积分（primal gap integral）作为随时可用评估指标，对CA-DL8.5的几种实例化（基于Purity、Gain、Discrepancy和Top-k启发式）与Blossom算法进行了严格的实证比较。", "result": "实验结果表明，在标准分类基准测试中，使用LDS（有限差异）的CA-DL8.5始终提供最佳的随时可用性能，优于其他CA-DL8.5变体和Blossom算法，同时保持了完整性和最优性保证。", "conclusion": "CA-DL8.5提供了一个用于精确和随时可用决策树学习的通用框架，能够整合多样化的启发式和搜索策略。其中，结合LDS策略的CA-DL8.5在随时可用性能方面表现出色，是有效的精确决策树学习器。"}}
{"id": "2508.05852", "categories": ["cs.CV", "I.5.4"], "pdf": "https://arxiv.org/pdf/2508.05852", "abs": "https://arxiv.org/abs/2508.05852", "authors": ["Kaiser Hamid", "Khandakar Ashrafi Akbar", "Nade Liang"], "title": "VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments", "comment": null, "summary": "Driver visual attention prediction is a critical task in autonomous driving\nand human-computer interaction (HCI) research. Most prior studies focus on\nestimating attention allocation at a single moment in time, typically using\nstatic RGB images such as driving scene pictures. In this work, we propose a\nvision-language framework that models the changing landscape of drivers' gaze\nthrough natural language, using few-shot and zero-shot learning on single RGB\nimages. We curate and refine high-quality captions from the BDD-A dataset using\nhuman-in-the-loop feedback, then fine-tune LLaVA to align visual perception\nwith attention-centric scene understanding. Our approach integrates both\nlow-level cues and top-down context (e.g., route semantics, risk anticipation),\nenabling language-based descriptions of gaze behavior. We evaluate performance\nacross training regimes (few shot, and one-shot) and introduce domain-specific\nmetrics for semantic alignment and response diversity. Results show that our\nfine-tuned model outperforms general-purpose VLMs in attention shift detection\nand interpretability. To our knowledge, this is among the first attempts to\ngenerate driver visual attention allocation and shifting predictions in natural\nlanguage, offering a new direction for explainable AI in autonomous driving.\nOur approach provides a foundation for downstream tasks such as behavior\nforecasting, human-AI teaming, and multi-agent coordination.", "AI": {"tldr": "该研究提出了一个视觉-语言框架，通过自然语言描述来预测驾驶员的视觉注意力分配和转移，利用少量样本和零样本学习，并结合低级视觉线索和高级语境信息。", "motivation": "现有研究大多只在单一时刻预测驾驶员注意力，且主要基于静态RGB图像。本研究旨在通过自然语言建模驾驶员注视点的动态变化，以提供更具解释性的注意力预测。", "method": "该方法构建了一个视觉-语言框架，通过人工反馈精炼BDD-A数据集中的高质量图像描述，然后微调LLaVA模型，使其将视觉感知与以注意力为中心的场景理解对齐。它整合了低级线索和高级语境（如路线语义、风险预期）。评估在少样本和单样本训练机制下进行，并引入了领域特定的语义对齐和响应多样性指标。", "result": "微调后的模型在注意力转移检测和可解释性方面优于通用视觉-语言模型。这是首次尝试用自然语言生成驾驶员视觉注意力分配和转移预测。", "conclusion": "该研究为自动驾驶领域的可解释人工智能提供了一个新方向，通过自然语言预测驾驶员注意力，为行为预测、人机协作和多智能体协调等下游任务奠定了基础。"}}
{"id": "2508.06016", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06016", "abs": "https://arxiv.org/abs/2508.06016", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models.", "AI": {"tldr": "研究发现，在Transformer模型中引入结构化注意力稀疏性，不仅能提高计算效率，还能显著提升模型精度，这与传统认知相反。", "motivation": "自注意力机制的二次计算成本是Transformer模型扩展的主要挑战。尽管注意力稀疏性被广泛研究以提高效率，但普遍认为这会牺牲模型精度。", "method": "在对DistilBERT模型进行SST-2情感分析任务微调时，引入了结构化的、事后（post-hoc）的注意力稀疏性。", "result": "引入80%注意力稀疏性的模型，在验证集上达到91.59%的准确率，比密集基线模型绝对提升了0.97%。作者推测，稀疏性作为一种强大的隐式正则化器，通过强制模型使用更受约束和鲁棒的特征集进行预测，从而防止过拟合。", "conclusion": "注意力稀疏性不仅是提高计算效率的工具，也是提升Transformer模型泛化能力和性能的潜在方法。"}}
{"id": "2508.06206", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06206", "abs": "https://arxiv.org/abs/2508.06206", "authors": ["Hanqing Wang", "Shaoyang Wang", "Yiming Zhong", "Zemin Yang", "Jiamin Wang", "Zhiqing Cui", "Jiahao Yuan", "Yifan Han", "Mingyu Liu", "Yuexin Ma"], "title": "Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model", "comment": null, "summary": "Affordance grounding focuses on predicting the specific regions of objects\nthat are associated with the actions to be performed by robots. It plays a\nvital role in the fields of human-robot interaction, human-object interaction,\nembodied manipulation, and embodied perception. Existing models often neglect\nthe affordance shared among different objects because they lack the\nChain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)\ngeneralization and explicit reasoning capabilities. To address these\nchallenges, we propose Affordance-R1, the first unified affordance grounding\nframework that integrates cognitive CoT guided Group Relative Policy\nOptimization (GRPO) within a reinforcement learning paradigm. Specifically, we\ndesigned a sophisticated affordance function, which contains format,\nperception, and cognition rewards to effectively guide optimization directions.\nFurthermore, we constructed a high-quality affordance-centric reasoning\ndataset, ReasonAff, to support training. Trained exclusively via reinforcement\nlearning with GRPO and without explicit reasoning data, Affordance-R1 achieves\nrobust zero-shot generalization and exhibits emergent test-time reasoning\ncapabilities. Comprehensive experiments demonstrate that our model outperforms\nwell-established methods and exhibits open-world generalization. To the best of\nour knowledge, Affordance-R1 is the first to integrate GRPO-based RL with\nreasoning into affordance reasoning. The code of our method and our dataset is\nreleased on https://github.com/hq-King/Affordance-R1.", "AI": {"tldr": "本文提出了Affordance-R1，首个将认知思维链(CoT)引导的群组相对策略优化(GRPO)集成到强化学习范式中的统一可供性基础框架，以解决现有模型泛化能力不足的问题，并实现了零样本泛化和 emergent 推理能力。", "motivation": "现有可供性接地模型缺乏思维链(CoT)推理能力，导致它们忽略不同物体间共享的可供性，限制了域外(OOD)泛化和显式推理能力。", "method": "提出了Affordance-R1框架，将CoT引导的GRPO集成到强化学习中。设计了包含格式、感知和认知奖励的复杂可供性函数来指导优化。构建了高质量的以可供性为中心的推理数据集ReasonAff来支持训练。模型纯粹通过GRPO强化学习训练，无需显式推理数据。", "result": "Affordance-R1实现了鲁棒的零样本泛化能力，并展现出测试时的涌现推理能力。实验证明其性能优于现有方法，并具有开放世界泛化能力。", "conclusion": "Affordance-R1是首次将基于GRPO的强化学习与推理集成到可供性推理中的方法，有效解决了现有模型在泛化和推理方面的局限性。"}}
{"id": "2508.06074", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06074", "abs": "https://arxiv.org/abs/2508.06074", "authors": ["Siyi Lu", "Run Liu", "Dongsheng Yang", "Lei He"], "title": "ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception", "comment": null, "summary": "Autonomous driving systems face significant challenges in perceiving complex\nenvironments and making real-time decisions. Traditional modular approaches,\nwhile offering interpretability, suffer from error propagation and coordination\nissues, whereas end-to-end learning systems can simplify the design but face\ncomputational bottlenecks. This paper presents a novel approach to autonomous\ndriving using deep reinforcement learning (DRL) that integrates bird's-eye view\n(BEV) perception for enhanced real-time decision-making. We introduce the\n\\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction\nnetwork that combines BEV-based perception with the Mamba framework for\ntemporal feature modeling. This integration allows the system to encode vehicle\nsurroundings and road features in a unified coordinate system and accurately\nmodel long-range dependencies. Building on this, we propose the\n\\texttt{ME$^3$-BEV} framework, which utilizes the \\texttt{Mamba-BEV} model as a\nfeature input for end-to-end DRL, achieving superior performance in dynamic\nurban driving scenarios. We further enhance the interpretability of the model\nby visualizing high-dimensional features through semantic segmentation,\nproviding insight into the learned representations. Extensive experiments on\nthe CARLA simulator demonstrate that \\texttt{ME$^3$-BEV} outperforms existing\nmodels across multiple metrics, including collision rate and trajectory\naccuracy, offering a promising solution for real-time autonomous driving.", "AI": {"tldr": "提出一种结合BEV感知和Mamba-BEV特征提取的端到端深度强化学习（DRL）自动驾驶框架ME$^3$-BEV，在CARLA模拟器中实现了卓越的实时驾驶性能。", "motivation": "自动驾驶系统在感知复杂环境和实时决策方面面临挑战。传统模块化方法存在误差传播和协调问题，而端到端学习系统则面临计算瓶颈。", "method": "该研究提出一种结合鸟瞰图（BEV）感知和深度强化学习（DRL）的新方法。引入Mamba-BEV模型作为高效时空特征提取网络，它将BEV感知与Mamba框架结合，用于建模长期时间依赖。在此基础上，构建ME$^3$-BEV框架，将Mamba-BEV的特征作为端到端DRL的输入，并通过语义分割可视化高维特征以增强模型可解释性。", "result": "在CARLA模拟器上的广泛实验表明，ME$^3$-BEV在多项指标上（包括碰撞率和轨迹精度）均优于现有模型，并在动态城市驾驶场景中表现出色。", "conclusion": "ME$^3$-BEV为实时自动驾驶提供了一种有前景的解决方案。"}}
{"id": "2508.05857", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05857", "abs": "https://arxiv.org/abs/2508.05857", "authors": ["Qiaomu Miao", "Vivek Raju Golani", "Jingyi Xu", "Progga Paromita Dutta", "Minh Hoai", "Dimitris Samaras"], "title": "Multi-view Gaze Target Estimation", "comment": "Accepted to ICCV 2025", "summary": "This paper presents a method that utilizes multiple camera views for the gaze\ntarget estimation (GTE) task. The approach integrates information from\ndifferent camera views to improve accuracy and expand applicability, addressing\nlimitations in existing single-view methods that face challenges such as face\nocclusion, target ambiguity, and out-of-view targets. Our method processes a\npair of camera views as input, incorporating a Head Information Aggregation\n(HIA) module for leveraging head information from both views for more accurate\ngaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the\nmost reliable gaze output, and an Epipolar-based Scene Attention (ESA) module\nfor cross-view background information sharing. This approach significantly\noutperforms single-view baselines, especially when the second camera provides a\nclear view of the person's face. Additionally, our method can estimate the gaze\ntarget in the first view using the image of the person in the second view only,\na capability not possessed by single-view GTE methods. Furthermore, the paper\nintroduces a multi-view dataset for developing and evaluating multi-view GTE\nmethods. Data and code are available at\nhttps://www3.cs.stonybrook.edu/~cvl/multiview_gte.html", "AI": {"tldr": "本文提出了一种利用多视角（双摄像头）信息进行凝视目标估计（GTE）的方法，通过整合头部信息、基于不确定性的凝视选择和跨视角场景注意力，显著提高了估计精度并解决了单视角方法的局限性，并发布了新的多视角数据集。", "motivation": "现有单视角凝视目标估计方法存在局限性，如面部遮挡、目标模糊和目标超出视野等，限制了其准确性和适用性。", "method": "该方法以一对摄像头视图作为输入，包含三个核心模块：1) 头部信息聚合（HIA）模块，用于整合两个视角的头部信息以提高凝视估计精度；2) 基于不确定性的凝视选择（UGS）模块，用于识别最可靠的凝视输出；3) 基于对极几何的场景注意力（ESA）模块，用于实现跨视角背景信息共享。此外，本文还引入了一个新的多视角数据集用于开发和评估多视角GTE方法。", "result": "该方法显著优于单视角基线方法，尤其当第二个摄像头提供清晰的人脸视图时表现更佳。此外，该方法仅利用第二个视角的人物图像即可估计第一个视角中的凝视目标，这是单视角GTE方法不具备的能力。", "conclusion": "多视角方法能有效提高凝视目标估计的准确性和适用性，克服了单视角方法的固有局限性。本文提出的方法为多视角GTE任务提供了有效解决方案，并发布了相关数据集和代码以促进后续研究。"}}
{"id": "2508.06026", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06026", "abs": "https://arxiv.org/abs/2508.06026", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.", "AI": {"tldr": "本文提出“时间自奖励语言模型”（Temporal Self-Rewarding Language Models），通过战略性地协调模型过去、现在和未来的生成，解决了现有自奖励范式中因选择和拒绝样本差异缩小而导致的偏好学习效率低下问题，显著提升了模型性能和泛化能力。", "motivation": "现有自奖励语言模型（Self-Rewarding Language Models）在迭代直接偏好优化（DPO）过程中，选择和拒绝响应的同步改进导致对比样本之间的表征差异逐渐缩小，从而削弱了有效的偏好学习。", "method": "提出双阶段框架：1) 锚定拒绝（Anchored Rejection）：使用过去初始模型的输出固定被拒绝的响应。2) 未来引导选择（Future-Guided Chosen）：使用下一代模型的预测动态策划选择样本。通过这两种机制，维持学习信号。", "result": "在Llama、Qwen、Mistral三个模型家族和不同模型尺寸（Llama3B/8B/70B）上的大量实验表明，与使用相同计算资源的自奖励基线相比，本文方法显著提高了性能。例如，Llama3.1-8B在AlpacaEval 2.0上的胜率从19.69提高到29.44。此外，在数学推理（GSM8K）、知识问答（ARC, TruthfulQA）和代码生成（HumanEval）等任务上，即使没有专门收集训练数据，也表现出卓越的域外泛化能力。", "conclusion": "时间自奖励语言模型通过有效解决自奖励范式中样本差异缩小的问题，显著提升了大型语言模型的生成能力和跨任务泛化能力，为自奖励式模型训练提供了新的有效策略。"}}
{"id": "2508.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06207", "abs": "https://arxiv.org/abs/2508.06207", "authors": ["Andrea Dal Prete", "Seyram Ofori", "Chan Yon Sin", "Ashwin Narayan", "Francesco Braghin", "Marta Gandolla", "Haoyong Yu"], "title": "Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization", "comment": null, "summary": "Back exoskeletons can reduce musculoskeletal strain, but their effectiveness\ndepends on support modulation and adaptive control. This study addresses two\nchallenges: defining optimal support strategies and developing adaptive control\nbased on payload estimation. We introduce an optimization space based on muscle\nactivity reduction, perceived discomfort, and user preference, constructing\nfunctions to identify optimal strategies. Experiments with 12 subjects revealed\noptimal operating regions, highlighting the need for dynamic modulation. Based\non these insights, we developed a vision-based adaptive control pipeline that\nestimates payloads in real-time by enhancing exoskeleton contextual\nunderstanding, minimising latency and enabling support adaptation within the\ndefined optimisation space. Validation with 12 more subjects showed over 80%\naccuracy and improvements across all metrics. Compared to static control,\nadaptive modulation reduced peak back muscle activation by up to 23% while\npreserving user preference and minimising discomfort. These findings validate\nthe proposed framework and highlight the potential of intelligent,\ncontext-aware control in industrial exoskeletons.", "AI": {"tldr": "本研究提出了一种基于肌肉活动减少、不适感和用户偏好的优化空间，用于定义最佳背部外骨骼支撑策略，并开发了一种基于视觉的自适应控制系统，通过实时有效载荷估计来动态调节支撑，显著降低了肌肉激活并提高了用户体验。", "motivation": "背部外骨骼在减轻肌肉骨骼劳损方面的有效性受限于缺乏最佳支撑策略和基于有效载荷估计的自适应控制。", "method": "研究定义了一个基于肌肉活动减少、感知不适和用户偏好的优化空间来识别最佳支撑策略。随后，开发了一个基于视觉的自适应控制流程，通过增强外骨骼的上下文理解来实时估计有效载荷，从而实现支撑的自适应调节。通过两组各12名受试者的实验进行了验证。", "result": "实验揭示了外骨骼的最佳操作区域，并强调了动态调制的必要性。所开发的视觉自适应控制系统在有效载荷估计方面实现了超过80%的准确性，并在所有指标上均有改进。与静态控制相比，自适应调制将峰值背部肌肉激活降低了高达23%，同时保持了用户偏好并最大程度地减少了不适。", "conclusion": "研究验证了所提出的智能、上下文感知控制框架的有效性，突出了其在工业外骨骼中通过动态调节支撑来提高性能的潜力。"}}
{"id": "2508.06091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06091", "abs": "https://arxiv.org/abs/2508.06091", "authors": ["Stan P Hauke", "Przemysław Andrzej Wałęga"], "title": "Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2", "comment": "18 pages", "summary": "In recent years, there has been growing interest in understanding the\nexpressive power of graph neural networks (GNNs) by relating them to logical\nlanguages. This research has been been initialised by an influential result of\nBarcel\\'o et al. (2020), who showed that the graded modal logic (or a guarded\nfragment of the logic C2), characterises the logical expressiveness of\naggregate-combine GNNs. As a ``challenging open problem'' they left the\nquestion whether full C2 characterises the logical expressiveness of\naggregate-combine-readout GNNs. This question has remained unresolved despite\nseveral attempts. In this paper, we solve the above open problem by proving\nthat the logical expressiveness of aggregate-combine-readout GNNs strictly\nexceeds that of C2. This result holds over both undirected and directed graphs.\nBeyond its implications for GNNs, our work also leads to purely logical\ninsights on the expressive power of infinitary logics.", "AI": {"tldr": "本文解决了关于GNN表达能力的一个长期开放问题，证明了带有readout机制的聚合-组合GNN（aggregate-combine-readout GNNs）的逻辑表达能力严格超过了C2逻辑。", "motivation": "Barceló等人（2020）的研究将聚合-组合GNN的表达能力与分级模态逻辑（或C2逻辑的受保护片段）联系起来，并提出了一个“具有挑战性的开放问题”：完整的C2逻辑是否能刻画聚合-组合-readout GNNs的逻辑表达能力。这个问题尽管经过多次尝试，但仍未解决。", "method": "本文通过数学证明来解决上述开放问题。", "result": "研究结果表明，聚合-组合-readout GNNs的逻辑表达能力严格超过C2逻辑。此结果适用于无向图和有向图。此外，该工作还对无穷逻辑的表达能力产生了纯粹的逻辑洞察。", "conclusion": "带有readout机制的GNN比C2逻辑更具表达力。这项工作不仅对GNNs的表达能力有重要意义，也为无穷逻辑的表达能力提供了新的逻辑见解。"}}
{"id": "2508.05898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05898", "abs": "https://arxiv.org/abs/2508.05898", "authors": ["Hamidreza Dastmalchi", "Aijun An", "Ali cheraghian"], "title": "ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates", "comment": "BMVC2025", "summary": "Pretrained vision-language models (VLMs) like CLIP show strong zero-shot\nperformance but struggle with generalization under distribution shifts.\nTest-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test\ndata in new domains. While some TTA methods rely on prompt-tuning,\ntraining-free cache-based approaches are preferred for efficiency. However,\ncurrent cache-based TTA models store only a limited set of high-confidence\nsamples, restricting the decision boundary to these samples and ignoring the\ninfluence of other incoming test data. To address this, we propose Efficient\nTest-Time Adaptation (ETTA), introducing a Recursive Updating module that\nintegrates all incoming test samples, progressively refining the decision\nboundary. This strategy mimics an unbounded cache, dynamically updating\ncontextual embeddings for improved accuracy with minimal memory and\ncomputational overhead. ETTA also includes an Adaptive Ensemble module to\nreduce prompt dependency in image-to-text scores by dynamically selecting\noptimal prompts for each class. Furthermore, ETTA adaptively combines scores\nfrom both modules based on confidence levels, leveraging their complementary\nstrengths. Extensive experiments on two benchmarks confirm that ETTA surpasses\nthe state-of-the-art TTA models in computational complexity and accuracy,\nsetting a new standard for effective, efficient test-time adaptation. The code\nhas been released at https://github.com/hamidreza-dastmalchi/ETTA.", "AI": {"tldr": "提出了一种高效的测试时自适应（ETTA）方法，通过递归更新和自适应集成模块，提升了预训练视觉-语言模型（VLMs）在分布偏移下的泛化能力和效率。", "motivation": "预训练视觉-语言模型（如CLIP）在零样本表现良好，但在分布偏移下泛化能力不足。现有的测试时自适应（TTA）方法，尤其是基于缓存的方法，受限于有限的缓存大小，忽略了其他测试数据的影响，并且存在对提示（prompt）的依赖性。", "method": "ETTA包含两个核心模块：1. 递归更新模块：整合所有传入的测试样本，逐步优化决策边界，模拟无限缓存，以最小的内存和计算开销动态更新上下文嵌入。2. 自适应集成模块：动态选择每个类别的最佳提示，减少图像到文本分数对提示的依赖，并根据置信度自适应地结合两个模块的分数，利用它们的互补优势。", "result": "在两个基准测试中，ETTA在计算复杂度和准确性方面均超越了现有最先进的TTA模型，取得了新的标准。", "conclusion": "ETTA为有效、高效的测试时自适应设定了新标准，显著提升了VLMs在分布偏移下的泛化能力和效率。"}}
{"id": "2508.06030", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06030", "abs": "https://arxiv.org/abs/2508.06030", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek.", "AI": {"tldr": "PEEK（Proxy Embeddings to Estimate Knowledge）是一种利用预训练嵌入模型（如文本或图嵌入）作为代理来高效估计大型语言模型（LLMs）知识的方法，避免了昂贵的LLM前向传播。", "motivation": "LLMs在预训练中获取了大量知识，但由于其随机性，难以预测它们具体获得了什么知识。现有的知识探测方法（如调查隐藏表示、构建特定提示、策划样本等）需要LLM进行前向传播，计算成本高且耗时。", "method": "该研究提出了PEEK方法。首先，通过各种探测策略识别LLMs已知事实的训练集。然后，将预训练的嵌入模型（文本或图嵌入）作为LLM的代理，并使用一个线性解码器层来预测LLM的输出，从而适应嵌入模型以估计LLM的知识。", "result": "在3个Wikipedia数据集、4个LLMs和7个嵌入模型上的综合评估表明，该方法可以在未见过的数据集上以高达90%的准确率预测LLM的知识。此外，研究发现句子嵌入模型比图嵌入模型更适合预测LLM知识，这为LLM内部事实知识的表示提供了启示。", "conclusion": "知识适应的嵌入模型可以大规模地识别LLMs的知识空白，并能更深入地揭示LLMs的内部归纳偏置。"}}
{"id": "2508.06229", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06229", "abs": "https://arxiv.org/abs/2508.06229", "authors": ["Zihao Xu", "Ce Hao", "Chunzheng Wang", "Kuankuan Sima", "Fan Shi", "Jin Song Dong"], "title": "REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance", "comment": null, "summary": "Dynamic obstacle avoidance (DOA) is critical for quadrupedal robots operating\nin environments with moving obstacles or humans. Existing approaches typically\nrely on navigation-based trajectory replanning, which assumes sufficient\nreaction time and leading to fails when obstacles approach rapidly. In such\nscenarios, quadrupedal robots require reflexive evasion capabilities to perform\ninstantaneous, low-latency maneuvers. This paper introduces Reflexive Evasion\nRobot (REBot), a control framework that enables quadrupedal robots to achieve\nreal-time reflexive obstacle avoidance. REBot integrates an avoidance policy\nand a recovery policy within a finite-state machine. With carefully designed\nlearning curricula and by incorporating regularization and adaptive rewards,\nREBot achieves robust evasion and rapid stabilization in instantaneous DOA\ntasks. We validate REBot through extensive simulations and real-world\nexperiments, demonstrating notable improvements in avoidance success rates,\nenergy efficiency, and robustness to fast-moving obstacles. Videos and appendix\nare available on https://rebot-2025.github.io/.", "AI": {"tldr": "REBot是一个用于四足机器人实时反射性避障的控制框架，通过集成避障和恢复策略，实现对快速移动障碍物的鲁棒规避和快速稳定。", "motivation": "现有避障方法依赖轨迹重新规划，需要足够反应时间，在障碍物快速接近时会失效。四足机器人需要即时、低延迟的反射性规避能力。", "method": "引入REBot控制框架，将避障策略和恢复策略集成在有限状态机中。通过精心设计的学习课程、正则化和自适应奖励，实现即时动态避障。", "result": "在仿真和实际实验中，REBot显著提高了避障成功率、能源效率和对快速移动障碍物的鲁棒性。", "conclusion": "REBot为四足机器人提供了在动态环境中进行即时、鲁棒且快速的反射性避障能力。"}}
{"id": "2508.06110", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06110", "abs": "https://arxiv.org/abs/2508.06110", "authors": ["Yiran Rex Ma"], "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "comment": "Accepted at IJCNN 2025", "summary": "Table reasoning, including tabular QA and fact verification, often depends on\nannotated data or complex data augmentation, limiting flexibility and\ngeneralization. LLMs, despite their versatility, often underperform compared to\nsimple supervised models. To approach these issues, we introduce PanelTR, a\nframework utilizing LLM agent scientists for robust table reasoning through a\nstructured scientific approach. PanelTR's workflow involves agent scientists\nconducting individual investigations, engaging in self-review, and\nparticipating in collaborative peer-review discussions. This process, driven by\nfive scientist personas, enables semantic-level transfer without relying on\ndata augmentation or parametric optimization. Experiments across four\nbenchmarks show that PanelTR outperforms vanilla LLMs and rivals fully\nsupervised models, all while remaining independent of training data. Our\nfindings indicate that structured scientific methodology can effectively handle\ncomplex tasks beyond table reasoning with flexible semantic understanding in a\nzero-shot context.", "AI": {"tldr": "PanelTR是一个利用LLM智能体科学家进行表格推理的框架，它通过结构化的科学方法，在无需数据增强或训练的情况下，实现了超越普通LLM并媲美监督模型的性能。", "motivation": "表格推理（如表格问答和事实验证）通常依赖大量标注数据或复杂数据增强，这限制了灵活性和泛化能力。尽管LLM通用性强，但在这些任务上表现往往不如简单的监督模型。", "method": "PanelTR框架采用LLM智能体科学家，通过结构化的科学方法进行表格推理。工作流程包括：个体调查、自我审查和协作式同行评审讨论。该过程由五种科学家角色驱动，实现了语义层面的迁移，且不依赖于数据增强或参数优化。", "result": "在四个基准测试中，PanelTR的表现优于普通LLM，并可与完全监督模型相媲美，同时完全独立于训练数据。", "conclusion": "研究结果表明，结构化的科学方法能够有效处理复杂任务（不仅限于表格推理），在零样本情境下实现灵活的语义理解。"}}
{"id": "2508.05899", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.05899", "abs": "https://arxiv.org/abs/2508.05899", "authors": ["Zixuan Bian", "Ruohan Ren", "Yue Yang", "Chris Callison-Burch"], "title": "HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing", "comment": null, "summary": "3D scene generation plays a crucial role in gaming, artistic creation,\nvirtual reality and many other domains. However, current 3D scene design still\nrelies heavily on extensive manual effort from creators, and existing automated\nmethods struggle to generate open-domain scenes or support flexible editing. As\na result, generating 3D worlds directly from text has garnered increasing\nattention. In this paper, we introduce HOLODECK 2.0, an advanced\nvision-language-guided framework for 3D world generation with support for\ninteractive scene editing based on human feedback. HOLODECK 2.0 can generate\ndiverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and\ncyberpunk styles) that exhibit high semantic fidelity to fine-grained input\ndescriptions, suitable for both indoor and open-domain environments. HOLODECK\n2.0 leverages vision-language models (VLMs) to identify and parse the objects\nrequired in a scene and generates corresponding high-quality assets via\nstate-of-the-art 3D generative models. It then iteratively applies spatial\nconstraints derived from the VLMs to achieve semantically coherent and\nphysically plausible layouts. Human evaluations and CLIP-based assessments\ndemonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely\naligned with detailed textual descriptions, consistently outperforming\nbaselines across indoor and open-domain scenarios. Additionally, we provide\nediting capabilities that flexibly adapt to human feedback, supporting layout\nrefinement and style-consistent object edits. Finally, we present a practical\napplication of HOLODECK 2.0 in procedural game modeling, generating visually\nrich and immersive environments, potentially boosting efficiency.", "AI": {"tldr": "HOLODECK 2.0是一个先进的视觉-语言引导框架，用于从文本生成3D世界，并支持基于人类反馈的交互式场景编辑。", "motivation": "当前3D场景设计严重依赖人工，现有自动化方法难以生成开放域场景或支持灵活编辑，因此直接从文本生成3D世界受到关注。", "method": "HOLODECK 2.0利用视觉-语言模型（VLM）识别和解析场景所需对象，通过最先进的3D生成模型生成高质量资产，然后迭代应用从VLM导出的空间约束以实现语义连贯和物理合理的布局。它还支持布局细化和风格一致的对象编辑。", "result": "HOLODECK 2.0能生成多样化、风格丰富的3D场景（如逼真、卡通、动漫、赛博朋克），对细粒度输入描述具有高语义保真度，适用于室内和开放域环境。人类评估和CLIP评估表明其有效生成高质量场景，性能优于基线。此外，它提供了灵活的编辑能力，并成功应用于程序化游戏建模。", "conclusion": "HOLODECK 2.0能高效生成与详细文本描述高度一致的高质量3D场景，并提供灵活的编辑功能，在游戏建模等领域具有提高效率的潜力。"}}
{"id": "2508.06046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06046", "abs": "https://arxiv.org/abs/2508.06046", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach.", "AI": {"tldr": "本文提出了EvolvR框架，通过自演化配对推理提升大型语言模型在故事评估方面的能力，并能作为奖励模型指导故事生成。", "motivation": "尽管LLM作为评判者的有效性已被验证，但在开放式任务（尤其是故事评估）中的表现仍有限。现有方法存在困境：闭源模型的提示工程适应性差，而开源模型的微调方法缺乏故事评估所需的严谨推理能力。", "method": "提出自演化配对推理（EvolvR）框架。首先，通过多角色策略自合成得分对齐的思维链（CoT）数据；其次，利用多智能体对原始CoT进行自过滤以确保逻辑严谨性和鲁棒性；最后，将基于精炼数据训练的评估器部署为奖励模型，以指导故事生成任务。", "result": "EvolvR框架在StoryER、HANNA和OpenMEVA三个评估基准上实现了最先进（SOTA）的性能。此外，作为奖励模型时，它显著提升了生成故事的质量。", "conclusion": "EvolvR的自演化方法在故事评估和指导故事生成方面表现出卓越的优越性。"}}
{"id": "2508.06266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06266", "abs": "https://arxiv.org/abs/2508.06266", "authors": ["Zezeng Li", "Rui Yang", "Ruochen Chen", "ZhongXuan Luo", "Liming Chen"], "title": "ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints", "comment": null, "summary": "Diffusion policies have recently emerged as a powerful class of visuomotor\ncontrollers for robot manipulation, offering stable training and expressive\nmulti-modal action modeling. However, existing approaches typically treat\naction generation as an unconstrained denoising process, ignoring valuable a\npriori knowledge about geometry and control structure. In this work, we propose\nthe Adaptive Diffusion Policy (ADP), a test-time adaptation method that\nintroduces two key inductive biases into the diffusion. First, we embed a\ngeometric manifold constraint that aligns denoising updates with task-relevant\nsubspaces, leveraging the fact that the relative pose between the end-effector\nand target scene provides a natural gradient direction, and guiding denoising\nalong the geodesic path of the manipulation manifold. Then, to reduce\nunnecessary exploration and accelerate convergence, we propose an analytically\nguided initialization: rather than sampling from an uninformative prior, we\ncompute a rough registration between the gripper and target scenes to propose a\nstructured initial noisy action. ADP is compatible with pre-trained diffusion\npolicies and requires no retraining, enabling test-time adaptation that tailors\nthe policy to specific tasks, thereby enhancing generalization across novel\ntasks and environments. Experiments on RLBench, CALVIN, and real-world dataset\nshow that ADPro, an implementation of ADP, improves success rates,\ngeneralization, and sampling efficiency, achieving up to 25% faster execution\nand 9% points over strong diffusion baselines.", "AI": {"tldr": "本文提出自适应扩散策略（ADP），通过引入几何流形约束和分析引导初始化，在测试时自适应地增强机器人操作扩散策略，无需重新训练，显著提升成功率、泛化能力和采样效率。", "motivation": "现有扩散策略将动作生成视为无约束去噪过程，忽略了几何和控制结构等先验知识，导致探索效率低下和性能受限。", "method": "本文提出自适应扩散策略（ADP），一种测试时自适应方法，引入两个关键归纳偏置：1) 几何流形约束：将去噪更新与任务相关子空间对齐，利用末端执行器与目标场景的相对姿态作为梯度方向，引导去噪沿操作流形的测地路径进行。2) 分析引导初始化：通过计算夹持器与目标场景的粗略配准，提出结构化的初始噪声动作，而非从无信息先验中采样。ADP兼容预训练策略，无需重新训练。", "result": "在RLBench、CALVIN和真实世界数据集上的实验表明，ADP的实现ADPro提高了成功率、泛化能力和采样效率，实现了高达25%的执行速度提升和比现有强扩散基线高出9个百分点的性能提升。", "conclusion": "ADP通过在测试时引入几何流形约束和分析引导初始化，有效地利用了机器人操作的几何和控制结构先验知识，显著提升了扩散策略的性能、泛化能力和效率，且无需重新训练。"}}
{"id": "2508.06111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06111", "abs": "https://arxiv.org/abs/2508.06111", "authors": ["Dewi S. W. Gould", "Bruno Mlodozeniec", "Samuel F. Brown"], "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "comment": "7 pages and appendices", "summary": "Evaluating the capabilities and risks of foundation models is paramount, yet\ncurrent methods demand extensive domain expertise, hindering their scalability\nas these models rapidly evolve. We introduce SKATE: a novel evaluation\nframework in which large language models (LLMs) compete by generating and\nsolving verifiable tasks for one another. Our core insight is to treat\nevaluation as a game: models act as both task-setters and solvers, incentivized\nto create questions which highlight their own strengths while exposing others'\nweaknesses. SKATE offers several key advantages, balancing scalability,\nopen-endedness, and objectivity. It is fully automated, data-free, and\nscalable, requiring no human input or domain expertise. By using verifiable\ntasks rather than LLM judges, scoring is objective. Unlike domain-limited\nprogrammatically-generated benchmarks (e.g. chess-playing or spatial\nreasoning), having LLMs creatively pose challenges enables open-ended and\nscalable evaluation. As a proof of concept, we introduce LLM-set\ncode-output-prediction (COP) challenges as a verifiable and extensible\nframework in which to test our approach. Using a TrueSkill-based ranking\nsystem, we evaluate six frontier LLMs and find that: (1) weaker models can\nreliably differentiate and score stronger ones, (2) LLM-based systems are\ncapable of self-preferencing behavior, generating questions that align with\ntheir own capabilities, and (3) SKATE automatically surfaces fine-grained\ncapability differences between models. Our findings are an important step\ntowards general, scalable evaluation frameworks which can keep pace with LLM\nprogress.", "AI": {"tldr": "SKATE是一个新颖的LLM评估框架，通过让LLM相互生成和解决可验证任务进行竞争性评估，实现自动化、可扩展和客观的评估。", "motivation": "当前评估基础模型的方法需要大量领域专业知识，难以随着模型快速发展而扩展。", "method": "SKATE将评估视为一场游戏，LLM既是任务出题者也是解决者，被激励去创建突出自己优势并暴露他人弱点的题目。它完全自动化、无需数据、可扩展，通过可验证任务而非LLM判断实现客观评分。以LLM设定的代码输出预测（COP）挑战作为概念验证，并使用基于TrueSkill的排名系统进行评估。", "result": "1) 较弱的模型能够可靠地区分并评分较强的模型；2) 基于LLM的系统能够表现出自偏好行为，生成与自身能力相符的问题；3) SKATE自动揭示了模型之间细粒度的能力差异。", "conclusion": "SKATE的发现是迈向能够跟上LLM进展的通用、可扩展评估框架的重要一步。"}}
{"id": "2508.05903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05903", "abs": "https://arxiv.org/abs/2508.05903", "authors": ["Lang Nie", "Yuan Mei", "Kang Liao", "Yunqiu Xu", "Chunyu Lin", "Bin Xiao"], "title": "Robust Image Stitching with Optimal Plane", "comment": "* Equal contribution", "summary": "We present \\textit{RopStitch}, an unsupervised deep image stitching framework\nwith both robustness and naturalness. To ensure the robustness of\n\\textit{RopStitch}, we propose to incorporate the universal prior of content\nperception into the image stitching model by a dual-branch architecture. It\nseparately captures coarse and fine features and integrates them to achieve\nhighly generalizable performance across diverse unseen real-world scenes.\nConcretely, the dual-branch model consists of a pretrained branch to capture\nsemantically invariant representations and a learnable branch to extract\nfine-grained discriminative features, which are then merged into a whole by a\ncontrollable factor at the correlation level. Besides, considering that content\nalignment and structural preservation are often contradictory to each other, we\npropose a concept of virtual optimal planes to relieve this conflict. To this\nend, we model this problem as a process of estimating homography decomposition\ncoefficients, and design an iterative coefficient predictor and minimal\nsemantic distortion constraint to identify the optimal plane. This scheme is\nfinally incorporated into \\textit{RopStitch} by warping both views onto the\noptimal plane bidirectionally. Extensive experiments across various datasets\ndemonstrate that \\textit{RopStitch} significantly outperforms existing methods,\nparticularly in scene robustness and content naturalness. The code is available\nat {\\color{red}https://github.com/MmelodYy/RopStitch}.", "AI": {"tldr": "RopStitch是一个无监督深度图像拼接框架，通过双分支架构和虚拟最优平面概念，实现了鲁棒性和自然性兼顾的图像拼接，显著优于现有方法。", "motivation": "现有图像拼接方法在鲁棒性和自然性方面存在不足，尤其是在未见过的真实场景中，并且内容对齐和结构保持之间常有冲突，促使研究者寻求更通用和高质量的拼接方案。", "method": "RopStitch采用双分支架构捕捉粗粒度（语义不变）和细粒度（判别性）特征，并通过可控因子融合。为解决对齐与结构冲突，提出了虚拟最优平面概念，将其建模为单应分解系数估计问题，设计迭代系数预测器和最小语义畸变约束，将双视图双向扭曲到最优平面上。", "result": "在各种数据集上的广泛实验表明，RopStitch显著优于现有方法，特别是在场景鲁棒性和内容自然性方面表现突出。", "conclusion": "RopStitch通过创新的双分支特征学习和虚拟最优平面策略，有效解决了图像拼接中的鲁棒性和自然性问题，为图像拼接领域提供了一个高性能的无监督解决方案。"}}
{"id": "2508.06094", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06094", "abs": "https://arxiv.org/abs/2508.06094", "authors": ["Morris Alper", "Moran Yanuka", "Raja Giryes", "Gašper Beguš"], "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline", "comment": "Project page: https://conlangcrafter.github.io", "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise.", "AI": {"tldr": "本文提出ConlangCrafter，一个利用大型语言模型（LLMs）端到端创建人工语言（conlangs）的多阶段管道。", "motivation": "人工语言在艺术、哲学和国际交流中扮演重要角色，而大型基础模型在文本生成方面取得了革命性进展。研究旨在利用LLMs的计算创造力辅助人工语言的自动化创建。", "method": "ConlangCrafter是一个多跳管道，将语言设计分解为模块化阶段：音系、形态学、句法、词汇生成和翻译。在每个阶段，该方法利用LLMs的元语言推理能力，注入随机性以鼓励多样性，并利用自我完善反馈以确保生成语言描述的一致性。", "result": "ConlangCrafter在衡量连贯性和类型学多样性的指标上进行了评估，结果表明它能够在无需人类语言学专业知识的情况下，生成连贯且多样化的人工语言。", "conclusion": "ConlangCrafter展示了LLMs在无需人类语言学专业知识的情况下，生成连贯且多样化人工语言的潜力，为计算创意在语言设计领域的应用开辟了新途径。"}}
{"id": "2508.06276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06276", "abs": "https://arxiv.org/abs/2508.06276", "authors": ["Juan Heredia", "Christian Schlette", "Mikkel Baun Kjærgaard"], "title": "EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators", "comment": null, "summary": "Existing literature proposes models for estimating the electrical power of\nmanipulators, yet two primary limitations prevail. First, most models are\npredominantly tested using traditional industrial robots. Second, these models\noften lack accuracy. To address these issues, we introduce an open source\nMatlab-based library designed to automatically generate \\ac{ec} models for\nmanipulators. The necessary inputs for the library are Denavit-Hartenberg\nparameters, link masses, and centers of mass. Additionally, our model is\ndata-driven and requires real operational data, including joint positions,\nvelocities, accelerations, electrical power, and corresponding timestamps. We\nvalidated our methodology by testing on four lightweight robots sourced from\nthree distinct manufacturers: Universal Robots, Franka Emika, and Kinova. The\nmodel underwent testing, and the results demonstrated an RMSE ranging from 1.42\nW to 2.80 W for the training dataset and from 1.45 W to 5.25 W for the testing\ndataset.", "AI": {"tldr": "本文提出了一个开源的基于Matlab的库，用于自动生成机械臂的电功耗（EC）模型，并通过轻量级机器人进行了数据驱动的验证。", "motivation": "现有机械臂电功耗估算模型主要针对传统工业机器人且精度不足，缺乏开放性。", "method": "开发了一个开源的Matlab库，利用Denavit-Hartenberg参数、连杆质量和质心作为输入，并结合关节位置、速度、加速度、电功耗及时间戳等真实运行数据，构建数据驱动的电功耗模型。", "result": "该方法在四款来自不同制造商（Universal Robots, Franka Emika, Kinova）的轻量级机器人上进行了验证。训练数据集的RMSE范围为1.42 W至2.80 W，测试数据集的RMSE范围为1.45 W至5.25 W。", "conclusion": "所提出的数据驱动模型和Matlab库能够有效且较准确地估算机械臂，特别是轻量级机器人的电功耗。"}}
{"id": "2508.06129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06129", "abs": "https://arxiv.org/abs/2508.06129", "authors": ["Bachtiar Herdianto", "Romain Billot", "Flavien Lucas", "Marc Sevaux"], "title": "Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem", "comment": "22 pages, 14 figures", "summary": "The Vehicle Routing Problem (VRP) is a complex optimization problem with\nnumerous real-world applications, mostly solved using metaheuristic algorithms\ndue to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely\non human-crafted designs developed through empirical studies. However, recent\nresearch shows that machine learning methods can be used the structural\ncharacteristics of solutions in combinatorial optimization, thereby aiding in\ndesigning more efficient algorithms, particularly for solving VRP. Building on\nthis advancement, this study extends the previous research by conducting a\nsensitivity analysis using multiple classifier models that are capable of\npredicting the quality of VRP solutions. Hence, by leveraging explainable AI,\nthis research is able to extend the understanding of how these models make\ndecisions. Finally, our findings indicate that while feature importance varies,\ncertain features consistently emerge as strong predictors. Furthermore, we\npropose a unified framework able of ranking feature impact across different\nscenarios to illustrate this finding. These insights highlight the potential of\nfeature importance analysis as a foundation for developing a guidance mechanism\nof metaheuristic algorithms for solving the VRP.", "AI": {"tldr": "本研究利用机器学习分类模型和可解释AI来预测车辆路径问题（VRP）解的质量，并通过特征重要性分析，为元启发式算法的设计提供指导机制。", "motivation": "传统的VRP元启发式算法依赖于人工设计，效率提升受限。近期研究表明机器学习可利用组合优化解的结构特性来设计更高效的算法，本研究旨在扩展此方向，通过预测VRP解的质量来辅助算法设计。", "method": "研究采用多种分类模型进行敏感性分析，以预测VRP解的质量。同时，利用可解释AI（XAI）来理解模型决策过程。此外，提出一个统一框架，用于在不同场景下对特征影响进行排名。", "result": "研究发现，虽然特征重要性有所不同，但某些特征始终是强预测因子。所提出的统一框架能够有效地展示这些发现，即不同场景下特征影响的排名。", "conclusion": "特征重要性分析具有巨大潜力，可以作为开发VRP元启发式算法指导机制的基础。"}}
{"id": "2508.05907", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05907", "abs": "https://arxiv.org/abs/2508.05907", "authors": ["Ilya Chugunov"], "title": "Neural Field Representations of Mobile Computational Photography", "comment": "PhD thesis", "summary": "Over the past two decades, mobile imaging has experienced a profound\ntransformation, with cell phones rapidly eclipsing all other forms of digital\nphotography in popularity. Today's cell phones are equipped with a diverse\nrange of imaging technologies - laser depth ranging, multi-focal camera arrays,\nand split-pixel sensors - alongside non-visual sensors such as gyroscopes,\naccelerometers, and magnetometers. This, combined with on-board integrated\nchips for image and signal processing, makes the cell phone a versatile\npocket-sized computational imaging platform. Parallel to this, we have seen in\nrecent years how neural fields - small neural networks trained to map\ncontinuous spatial input coordinates to output signals - enable the\nreconstruction of complex scenes without explicit data representations such as\npixel arrays or point clouds. In this thesis, I demonstrate how carefully\ndesigned neural field models can compactly represent complex geometry and\nlighting effects. Enabling applications such as depth estimation, layer\nseparation, and image stitching directly from collected in-the-wild mobile\nphotography data. These methods outperform state-of-the-art approaches without\nrelying on complex pre-processing steps, labeled ground truth data, or machine\nlearning priors. Instead, they leverage well-constructed, self-regularized\nmodels that tackle challenging inverse problems through stochastic gradient\ndescent, fitting directly to raw measurements from a smartphone.", "AI": {"tldr": "该论文利用精心设计的神经场模型，将智能手机作为计算成像平台，直接从原始移动摄影数据中实现深度估计、图层分离和图像拼接等应用，性能超越现有技术。", "motivation": "智能手机已成为主流的计算成像平台，集成了多种成像和非视觉传感器以及强大的板载处理能力。同时，神经场模型在复杂场景重建方面展现出无需显式数据表示的潜力。本研究旨在结合两者，解决移动摄影中具有挑战性的逆问题，实现高级成像应用。", "method": "核心方法是使用精心设计的神经场模型，将连续空间输入坐标映射到输出信号，以紧凑地表示复杂的几何和光照效果。通过随机梯度下降，将这些自正则化模型直接拟合到智能手机的原始测量数据，解决逆问题，无需复杂的预处理、标注的真值数据或机器学习先验。", "result": "所提出的方法能够实现深度估计、图层分离和图像拼接等应用，并且在这些任务上优于现有最先进的方法。其优势在于无需复杂的预处理步骤、标记的地面真实数据或机器学习先验，而是直接利用智能手机的原始测量数据。", "conclusion": "本论文证明了精心设计的神经场模型能够紧凑地表示复杂的几何和光照效果，并能直接从智能手机收集的原始数据中有效地解决计算成像中的挑战性逆问题，从而在移动摄影应用中取得卓越性能。"}}
{"id": "2508.06103", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06103", "abs": "https://arxiv.org/abs/2508.06103", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Ammar Mohammed"], "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs", "comment": "6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,\n  https://imsa.msa.edu.eg/", "summary": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks.", "AI": {"tldr": "本论文提出了两种针对古兰经抽取式问答的有效方法，其中一种利用指令微调的大语言模型和专门的阿拉伯语提示框架，并结合后处理系统，显著提升了问答性能。", "motivation": "解决古兰经文本中复杂的语言、独特的术语和深层含义给问答任务带来的挑战。", "method": "提出了两种方法，其中一种利用指令微调的大语言模型（如Gemini和DeepSeek）进行小样本提示，并开发了专门的阿拉伯语提示框架进行文本跨度提取。此外，还集成了一个强大的后处理系统，包括子词对齐、重叠抑制和语义过滤，以提高精度并减少幻觉。", "result": "评估显示，使用阿拉伯语指令的大语言模型表现优于传统的微调模型。最佳配置达到了0.637的pAP10分数。", "conclusion": "研究结果证实，对于低资源、语义丰富的问答任务，基于提示的指令微调方法是有效的。"}}
{"id": "2508.06278", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06278", "abs": "https://arxiv.org/abs/2508.06278", "authors": ["Petr Novak", "Stefan Biffl", "Marek Obitko", "Petr Kadera"], "title": "Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs", "comment": "3 pages, 1 figure", "summary": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction.", "AI": {"tldr": "本文提出了一种名为PPR-AKG（产品-过程-资源资产知识图谱）的语义模型，用于分析和缓解柔性工业网络物理生产系统（CPPS）中的不良状况，并通过与大型语言模型（LLM）结合，提供直观的自然语言交互界面。", "motivation": "工业4.0的灵活性使得传统质量保证机制面临挑战，导致机器人工作单元组成的CPPS难以分析不良状况。传统的CPPS模型驱动工程在不良状况和错误处理表示方面存在不足。", "method": "基于成熟的PPR模型（源自ISA-95和VDI-3682），开发了PPR-AKG语义模型，并利用OWL本体论解决了传统模型驱动工程的缺点。此外，将语义技术与大型语言模型（LLM）集成，为工厂操作员、生产计划员和工程师提供了自然语言交互界面。", "result": "通过电动汽车电池再制造的用例评估表明，PPR-AKG方法能有效支持基于明确表示能力的资源分配，以及生产中不良状况的识别和缓解。主要贡献包括：(1) 一个捕获多维度生产知识的整体PPR-AKG模型；(2) PPR-AKG与基于LLM的聊天机器人相结合，实现人机交互。", "conclusion": "PPR-AKG模型，特别是与LLM结合后，为柔性CPPS中不良状况的分析、缓解和资源分配提供了有效支持，并显著提升了人机交互体验。"}}
{"id": "2508.06145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06145", "abs": "https://arxiv.org/abs/2508.06145", "authors": ["Byeonghun Bang", "Jongsuk Yoon", "Dong-Jin Chang", "Seho Park", "Yong Oh Lee"], "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications", "comment": null, "summary": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information.", "AI": {"tldr": "本研究通过集成检索增强生成（RAG）管道，显著提升了大型语言模型（LLMs）在处理药物禁忌症信息方面的准确性。", "motivation": "尽管大型语言模型（LLMs）应用广泛，但在医疗保健领域，尤其是在需要准确可靠信息的药物禁忌症方面，其应用面临挑战。", "method": "本研究利用OpenAI的GPT-4o-mini作为基础模型，text-embedding-3-small进行嵌入，并结合Langchain构建了一个混合检索系统，辅以重排序功能。该系统整合了来自公共数据库的药物利用审查（DUR）数据，重点关注特定年龄组、怀孕和药物联用禁忌症。数据集包含300个问答对，分为三类。", "result": "在整合RAG管道后，模型准确率显著提高。对于年龄组相关的禁忌症，准确率从0.49提升至0.94；对于怀孕相关禁忌症，从0.57提升至0.87；对于药物联用禁忌症，从0.57提升至0.89。", "conclusion": "研究结果表明，通过RAG框架增强LLMs的能力，可以大幅减少处方和用药决策中的不确定性，提供更精确可靠的药物禁忌症信息。"}}
{"id": "2508.05922", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05922", "abs": "https://arxiv.org/abs/2508.05922", "authors": ["Sri Ramana Saketh Vasanthawada", "Pengkun Liu", "Pingbo Tang"], "title": "Enhancing Construction Site Analysis and Understanding with 3D Segmentation", "comment": null, "summary": "Monitoring construction progress is crucial yet resource-intensive, prompting\nthe exploration of computer-vision-based methodologies for enhanced efficiency\nand scalability. Traditional data acquisition methods, primarily focusing on\nindoor environments, falter in construction site's complex, cluttered, and\ndynamically changing conditions. This paper critically evaluates the\napplication of two advanced 3D segmentation methods, Segment Anything Model\n(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained\ninitially on indoor datasets, both models' adaptability and performance are\nassessed in real-world construction settings, highlighting the gap in current\nsegmentation approaches due to the absence of benchmarks for outdoor scenarios.\nThrough a comparative analysis, this study not only showcases the relative\neffectiveness of SAM and Mask3D but also addresses the critical need for\ntailored segmentation workflows capable of extracting actionable insights from\nconstruction site data, thereby advancing the field towards more automated and\nprecise monitoring techniques.", "AI": {"tldr": "本文评估了SAM和Mask3D两种3D分割模型在复杂施工现场（室内外）进行进度监控的适用性，并强调了户外场景基准缺失的问题。", "motivation": "施工进度监控至关重要但资源密集，传统数据采集方法在复杂、动态的施工现场表现不佳，需要计算机视觉方法提高效率和可扩展性。", "method": "研究评估了两种先进的3D分割方法——Segment Anything Model (SAM) 和 Mask3D，在具有挑战性的户外和室内施工条件下进行应用和性能测试，并进行比较分析。", "result": "评估了SAM和Mask3D在实际施工环境中的适应性和性能，揭示了当前分割方法在户外场景中缺乏基准的不足，并展示了两种模型的相对有效性。", "conclusion": "研究强调了为施工现场数据提取可操作洞察力而定制分割工作流的迫切需求，从而推动自动化和精确监控技术的发展。"}}
{"id": "2508.06105", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06105", "abs": "https://arxiv.org/abs/2508.06105", "authors": ["Shengyuan Chen", "Chuang Zhou", "Zheng Yuan", "Qinggang Zhang", "Zeyang Cui", "Hao Chen", "Yilin Xiao", "Jiannong Cao", "Xiao Huang"], "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures", "comment": null, "summary": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines.", "AI": {"tldr": "LogicRAG是一种无需预构建图谱的逻辑感知检索增强生成框架，它在推理时动态提取推理结构以指导自适应检索，有效解决了现有GraphRAG的高成本和不灵活性问题。", "motivation": "大型语言模型（LLMs）存在幻觉问题。检索增强生成（RAG）通过检索相关上下文来解决此问题。现有基于图的RAG（GraphRAG）虽然表现出色，但其图谱构建成本高昂、更新延迟大，且预构建图谱可能无法适应实时查询类型和复杂性所需的逻辑结构，导致检索效率低下。", "method": "LogicRAG在推理时动态提取推理结构。它首先将输入查询分解为子问题并构建一个有向无环图（DAG）来建模其逻辑依赖。然后，使用拓扑排序将图线性化以支持连贯的多步推理。此外，LogicRAG还应用图剪枝减少冗余检索，并使用上下文剪枝过滤不相关上下文，从而显著降低整体token成本。", "result": "广泛的实验表明，LogicRAG与最先进的基线相比，在性能和效率方面均取得了卓越表现。", "conclusion": "LogicRAG提供了一种无需预构建图谱的动态、高效且逻辑感知的RAG方法，有效提升了LLM在复杂任务中的推理能力和准确性，同时显著降低了成本。"}}
{"id": "2508.06283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06283", "abs": "https://arxiv.org/abs/2508.06283", "authors": ["Saad Ejaz", "Marco Giberna", "Muhammad Shaheer", "Jose Andres Millan-Romera", "Ali Tourani", "Paul Kremer", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Situationally-aware Path Planning Exploiting 3D Scene Graphs", "comment": null, "summary": "3D Scene Graphs integrate both metric and semantic information, yet their\nstructure remains underutilized for improving path planning efficiency and\ninterpretability. In this work, we present S-Path, a situationally-aware path\nplanner that leverages the metric-semantic structure of indoor 3D Scene Graphs\nto significantly enhance planning efficiency. S-Path follows a two-stage\nprocess: it first performs a search over a semantic graph derived from the\nscene graph to yield a human-understandable high-level path. This also\nidentifies relevant regions for planning, which later allows the decomposition\nof the problem into smaller, independent subproblems that can be solved in\nparallel. We also introduce a replanning mechanism that, in the event of an\ninfeasible path, reuses information from previously solved subproblems to\nupdate semantic heuristics and prioritize reuse to further improve the\nefficiency of future planning attempts. Extensive experiments on both\nreal-world and simulated environments show that S-Path achieves average\nreductions of 5.7x in planning time while maintaining comparable path\noptimality to classical sampling-based planners and surpassing them in complex\nscenarios, making it an efficient and interpretable path planner for\nenvironments represented by indoor 3D Scene Graphs.", "AI": {"tldr": "S-Path利用3D场景图的度量-语义结构，通过两阶段规划和重规划机制，显著提升室内路径规划的效率和可解释性。", "motivation": "3D场景图虽然整合了度量和语义信息，但其结构在路径规划中未被充分利用，导致规划效率和可解释性不足。", "method": "S-Path采用两阶段规划：首先，在从场景图派生的语义图上进行搜索，生成人类可理解的高层路径，并识别相关规划区域，从而将问题分解为可并行解决的独立子问题。其次，引入重规划机制，当路径不可行时，重用先前已解决子问题的信息来更新语义启发式，并优先重用以提高未来规划尝试的效率。", "result": "在真实和模拟环境中，S-Path平均规划时间减少了5.7倍，同时保持了与传统基于采样规划器相当的路径最优性，并在复杂场景中表现更优。", "conclusion": "S-Path是一种高效且可解释的路径规划器，特别适用于由室内3D场景图表示的环境。"}}
{"id": "2508.06225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06225", "abs": "https://arxiv.org/abs/2508.06225", "authors": ["Zailong Tian", "Zhuoheng Han", "Yanzhe Chen", "Haozhe Xu", "Xi Yang", "richeng xuan", "Hongfeng Wang", "Lizi Liao"], "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution", "comment": null, "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.", "AI": {"tldr": "现有大型语言模型（LLM）作为自动评判员存在过度自信现象，本研究提出TH-Score量化置信度与准确性的一致性，并引入LLM-as-a-Fuser集成框架以提升其校准和可靠性。", "motivation": "LLM作为评判员的实用价值依赖于准确性和可信赖的、风险感知的判断，但现有方法主要关注准确性，忽视了对适应性和可靠评估至关重要的校准置信度。LLM评判员普遍存在“过度自信现象”，即预测置信度远高于实际正确性。", "method": "系统性地识别并量化了LLM评判员中的“过度自信现象”；引入了新的度量指标TH-Score来衡量置信度与准确性的一致性；提出了LLM-as-a-Fuser，一个将LLM转化为可靠、风险感知评估器的集成框架。", "result": "实验证明，所提出的方法显著改善了校准，并实现了自适应、置信度驱动的评估流程，与现有基线相比，取得了卓越的可靠性和准确性。", "conclusion": "强调从以准确性为中心的评估转向以置信度驱动、风险感知的LLM评判系统，认为良好的校准置信度对于可信赖和自适应评估至关重要，而LLM-as-a-Fuser有效实现了这一目标。"}}
{"id": "2508.05950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05950", "abs": "https://arxiv.org/abs/2508.05950", "authors": ["Yanxing Liang", "Yinghui Wang", "Jinlong Yang", "Wei Li"], "title": "A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image", "comment": null, "summary": "The lack of spatial dimensional information remains a challenge in normal\nestimation from a single image. Recent diffusion-based methods have\ndemonstrated significant potential in 2D-to-3D implicit mapping, they rely on\ndata-driven statistical priors and miss the explicit modeling of light-surface\ninteraction, leading to multi-view normal direction conflicts. Moreover, the\ndiscrete sampling mechanism of diffusion models causes gradient discontinuity\nin differentiable rendering reconstruction modules, preventing 3D geometric\nerrors from being backpropagated to the normal generation network, thereby\nforcing existing methods to depend on dense normal annotations. This paper\nproposes SINGAD, a novel Self-supervised framework from a single Image for\nNormal estimation via 3D GAussian splatting guided Diffusion. By integrating\nphysics-driven light-interaction modeling and a differentiable rendering-based\nreprojection strategy, our framework directly converts 3D geometric errors into\nnormal optimization signals, solving the challenges of multi-view geometric\ninconsistency and data dependency. Specifically, the framework constructs a\nlight-interaction-driven 3DGS reparameterization model to generate multi-scale\ngeometric features consistent with light transport principles, ensuring\nmulti-view normal consistency. A cross-domain feature fusion module is designed\nwithin a conditional diffusion model, embedding geometric priors to constrain\nnormal generation while maintaining accurate geometric error propagation.\nFurthermore, a differentiable 3D reprojection loss strategy is introduced for\nself-supervised optimization that minimizes geometric error between the\nreconstructed and input image, eliminating dependence on annotated normal\ndatasets. Quantitative evaluations on the Google Scanned Objects dataset\ndemonstrate that our method outperforms state-of-the-art approaches across\nmultiple metrics.", "AI": {"tldr": "本文提出SINGAD，一个通过3D高斯泼溅引导扩散的单图像自监督法线估计框架，解决了多视角几何不一致和对密集法线标注的依赖问题。", "motivation": "单图像法线估计缺乏空间维度信息。现有基于扩散的方法依赖数据驱动统计先验，但忽略光照-表面交互，导致多视角法线方向冲突。此外，扩散模型的离散采样机制导致可微分渲染模块中梯度不连续，阻碍3D几何误差反向传播，使得现有方法依赖于密集的法线标注。", "method": "SINGAD通过整合物理驱动的光照交互建模和基于可微分渲染的重投影策略，将3D几何误差直接转换为法线优化信号。具体而言，它构建了一个光照交互驱动的3DGS重参数化模型，生成符合光传输原理的多尺度几何特征，确保多视角法线一致性。在条件扩散模型中设计了跨域特征融合模块，嵌入几何先验以约束法线生成并保持几何误差的准确传播。最后，引入了可微分3D重投影损失策略进行自监督优化，最小化重建图像与输入图像之间的几何误差。", "result": "在Google Scanned Objects数据集上的定量评估表明，SINGAD在多项指标上优于现有最先进的方法。", "conclusion": "SINGAD成功解决了单图像法线估计中的多视角几何不一致和数据依赖性挑战，通过自监督优化实现了高性能的法线估计。"}}
{"id": "2508.06124", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06124", "abs": "https://arxiv.org/abs/2508.06124", "authors": ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"], "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models", "comment": null, "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.", "AI": {"tldr": "AURA框架通过引入过程奖励模型（PRMs）和多层评估，解决了大型语言模型（LLMs）在处理基于行为促成物（affordance-based）安全风险方面的不足，显著提升了模型的逻辑完整性和安全性。", "motivation": "现有LLMs面临基于行为促成物的安全风险（输出可能无意中促成有害行为），而传统安全方案（如标量奖励模型、参数调整、启发式解码）缺乏在细微推理步骤中可靠检测和干预所需的粒度和主动性。", "method": "引入AURA多层框架，核心是过程奖励模型（PRMs），提供逻辑一致性和安全意识的步骤级评估。该框架结合了内省式自我批判、细粒度PRM评估和自适应安全感知解码，以动态主动地引导模型走向更安全的推理轨迹。", "result": "实证证据表明，AURA方法显著超越了现有方法，显著提高了模型输出的逻辑完整性和对行为促成物敏感的安全性。", "conclusion": "这项研究代表了迈向更安全、更负责任、更具上下文意识的AI的关键一步，为对齐敏感型应用设定了新的基准。"}}
{"id": "2508.06291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06291", "abs": "https://arxiv.org/abs/2508.06291", "authors": ["Christian Rauch", "Björn Ellensohn", "Linus Nwankwo", "Vedant Dave", "Elmar Rueckert"], "title": "Real-Time 3D Vision-Language Embedding Mapping", "comment": null, "summary": "A metric-accurate semantic 3D representation is essential for many robotic\ntasks. This work proposes a simple, yet powerful, way to integrate the 2D\nembeddings of a Vision-Language Model in a metric-accurate 3D representation at\nreal-time. We combine a local embedding masking strategy, for a more distinct\nembedding distribution, with a confidence-weighted 3D integration for more\nreliable 3D embeddings. The resulting metric-accurate embedding representation\nis task-agnostic and can represent semantic concepts on a global multi-room, as\nwell as on a local object-level. This enables a variety of interactive robotic\napplications that require the localisation of objects-of-interest via natural\nlanguage. We evaluate our approach on a variety of real-world sequences and\ndemonstrate that these strategies achieve a more accurate object-of-interest\nlocalisation while improving the runtime performance in order to meet our\nreal-time constraints. We further demonstrate the versatility of our approach\nin a variety of interactive handheld, mobile robotics and manipulation tasks,\nrequiring only raw image data.", "AI": {"tldr": "该研究提出一种实时、度量精确的方法，将视觉-语言模型（VLM）的2D嵌入集成到3D表示中，以实现基于自然语言的对象定位。", "motivation": "许多机器人任务需要度量精确的语义3D表示，特别是通过自然语言定位感兴趣物体。", "method": "结合了局部嵌入掩蔽策略（用于更清晰的嵌入分布）和置信度加权的3D集成（用于更可靠的3D嵌入）。", "result": "实现了更准确的感兴趣对象定位，提高了运行时性能以满足实时约束，并在多种交互式手持、移动机器人和操作任务中展现了多功能性，仅需原始图像数据。", "conclusion": "所提出的方法能够实时构建度量精确的3D嵌入表示，该表示与任务无关，可在全局和局部层面表示语义概念，从而支持各种需要通过自然语言定位对象的交互式机器人应用。"}}
{"id": "2508.06226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06226", "abs": "https://arxiv.org/abs/2508.06226", "authors": ["Yumeng Fu", "Jiayin Zhu", "Lingling Zhang", "Bo Zhao", "Shaoxuan Ma", "Yushun Zhang", "Yanrui Wu", "Wenjun Wu"], "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines", "comment": null, "summary": "Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released.", "AI": {"tldr": "现有MLLM几何基准忽略辅助线和细粒度过程评估，本文提出GeoLaux基准，包含2186个几何问题（平均6.51步，41.8%需辅助线），并设计五维评估策略。实验发现MLLM在长推理步、证明题和辅助线方面表现差。", "motivation": "几何问题求解（GPS）需要模型掌握图表理解、逻辑推理、知识应用、数值计算和辅助线构造，对多模态大语言模型（MLLMs）构成巨大挑战。然而，现有评估MLLM几何能力的基准忽视了辅助线构造，且缺乏细粒度过程评估，不足以评估MLLM的长步推理能力。", "method": "提出了GeoLaux基准，包含2186个几何问题，涵盖计算和证明题，平均需要6.51个推理步骤，最长达24步，其中41.8%的问题需要辅助线构造。在此数据集基础上，设计了新颖的五维评估策略，评估答案正确性、过程正确性、过程质量、辅助线影响和错误原因。对13个领先的MLLM（包括思维模型和非思维模型）进行了广泛实验。", "result": "首先，模型在扩展推理步骤中表现出显著的性能下降（九个模型性能下降超过50%）。其次，与计算问题相比，MLLM在解决证明问题时倾向于采取捷径。第三，模型缺乏辅助线意识，增强此能力对整体几何推理能力提升特别有益。", "conclusion": "GeoLaux基准既可以用于评估MLLM在包含辅助线的长步几何推理方面的能力，也为能力提升提供了指导。数据集和代码将随补充材料发布。"}}
{"id": "2508.05954", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05954", "abs": "https://arxiv.org/abs/2508.05954", "authors": ["Han Lin", "Jaemin Cho", "Amir Zadeh", "Chuan Li", "Mohit Bansal"], "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents", "comment": "Project Page: https://bifrost-1.github.io", "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.", "AI": {"tldr": "Bifrost-1是一个统一框架，通过使用与多模态大语言模型（MLLM）视觉编码器对齐的块级CLIP图像嵌入作为潜在变量，高效地将预训练MLLM与扩散模型结合，实现了高保真可控图像生成。", "motivation": "现有将高保真视觉合成能力整合到大语言模型（LLM）中的方法，由于LLM在预训练时未见过图像表示，通常导致训练成本高昂，或损害其强大的推理能力。", "method": "Bifrost-1通过使用块级CLIP图像嵌入作为潜在变量，连接预训练的多模态大语言模型（MLLM）和扩散模型。这些嵌入与MLLM的CLIP视觉编码器原生对齐，并通过轻量级适配的ControlNet集成到扩散模型中。为保持MLLM原有的多模态推理能力，在预测块级图像嵌入时，为MLLM配备了一个从原始MLLM参数初始化的视觉生成分支。", "result": "Bifrost-1在视觉保真度和多模态理解方面，实现了与现有方法相当或更优的性能，同时显著降低了训练计算量。全面的消融研究也证明了其设计选择的有效性。", "conclusion": "Bifrost-1通过无缝整合预训练的MLLM和扩散模型，利用块级CLIP潜在变量，实现了高效、高保真和可控的图像生成，同时保留了MLLM的推理能力。"}}
{"id": "2508.06135", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06135", "abs": "https://arxiv.org/abs/2508.06135", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.", "AI": {"tldr": "本文提出了一种名为SRD（Selective Reflection Distillation）的新型数据精选框架，通过利用学生模型的反馈来系统地优化训练数据，从而在不修改现有知识蒸馏（KD）算法的情况下，显著提升大型语言模型（LLM）蒸馏的效率和性能。", "motivation": "现有的白盒知识蒸馏方法主要侧重于平衡真实标签和学生模型生成响应，但却忽视了两个关键因素：训练数据质量和学生模型兼容性，这限制了蒸馏的效果和效率。", "method": "SRD框架通过学生模型的反射来动态评估和选择提示-响应对，将真实数据与学生模型输出进行比较，根据难度自动排名，从而精选出高质量、与学生模型兼容的训练实例。此外，在数据选择后，采用课程调度策略，以固定间隔将这些精选子集逐步引入蒸馏过程。SRD作为一个即插即用模块，不修改底层的KD算法。", "result": "SRD在多种语言模型基准测试中，持续提升了蒸馏模型的性能，并在不同KD方法和模型家族下，将训练运行时间显著缩短了高达39%。它能够一致地改善各种白盒KD方法和模型架构的蒸馏结果，并显著降低计算成本。", "conclusion": "数据质量和兼容性对于大型语言模型的高效蒸馏至关重要，SRD提供了一个原则性的框架来实现这两点。这项工作加深了对知识蒸馏中数据中心因素的理解，并为增强压缩LLM的能力和效率提供了实用的见解。"}}
{"id": "2508.06295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06295", "abs": "https://arxiv.org/abs/2508.06295", "authors": ["Juan Heredia", "Emil Stubbe Kolvig-Raun", "Sune Lundo Sorensen", "Mikkel Baun Kjaergaard"], "title": "Evaluating Robot Program Performance with Power Consumption Driven Metrics in Lightweight Industrial Robots", "comment": null, "summary": "The code performance of industrial robots is typically analyzed through CPU\nmetrics, which overlook the physical impact of code on robot behavior. This\nstudy introduces a novel framework for assessing robot program performance from\nan embodiment perspective by analyzing the robot's electrical power profile.\nOur approach diverges from conventional CPU based evaluations and instead\nleverages a suite of normalized metrics, namely, the energy utilization\ncoefficient, the energy conversion metric, and the reliability coefficient, to\ncapture how efficiently and reliably energy is used during task execution.\nComplementing these metrics, the established robot wear metric provides further\ninsight into long term reliability. Our approach is demonstrated through an\nexperimental case study in machine tending, comparing four programs with\ndiverse strategies using a UR5e robot. The proposed metrics directly compare\nand categorize different robot programs, regardless of the specific task, by\nlinking code performance to its physical manifestation through power\nconsumption patterns. Our results reveal the strengths and weaknesses of each\nstrategy, offering actionable insights for optimizing robot programming\npractices. Enhancing energy efficiency and reliability through this embodiment\ncentric approach not only improves individual robot performance but also\nsupports broader industrial objectives such as sustainable manufacturing and\ncost reduction.", "AI": {"tldr": "该研究提出一种新颖的机器人程序性能评估框架，通过分析机器人电能消耗而非传统CPU指标，引入能量利用、能量转换和可靠性系数等指标，以体现代码对机器人物理行为的影响。", "motivation": "传统的机器人代码性能分析主要依赖CPU指标，忽略了代码对机器人物理行为的实际影响。需要一种从“具身”角度评估机器人程序性能的方法。", "method": "引入了一套归一化指标：能量利用系数、能量转换指标和可靠性系数，并结合现有的机器人磨损指标，通过分析机器人的电能曲线来评估任务执行期间的能量效率和可靠性。通过一个UR5e机器人在机床上下料任务中的实验案例进行验证，比较了四种不同策略的程序。", "result": "所提出的指标能够直接比较和分类不同的机器人程序，通过将代码性能与其物理表现（电能消耗模式）联系起来，揭示了每种策略的优缺点，为优化机器人编程实践提供了可操作的见解。", "conclusion": "这种以“具身”为中心的评估方法不仅能提高单个机器人的性能，增强能源效率和可靠性，还支持可持续制造和成本降低等更广泛的工业目标。"}}
{"id": "2508.06230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06230", "abs": "https://arxiv.org/abs/2508.06230", "authors": ["Ruben Sharma", "Sebastijan Dumančić", "Ross D. King", "Andrew Cropper"], "title": "Learning Logical Rules using Minimum Message Length", "comment": null, "summary": "Unifying probabilistic and logical learning is a key challenge in AI. We\nintroduce a Bayesian inductive logic programming approach that learns minimum\nmessage length programs from noisy data. Our approach balances hypothesis\ncomplexity and data fit through priors, which explicitly favour more general\nprograms, and a likelihood that favours accurate programs. Our experiments on\nseveral domains, including game playing and drug design, show that our method\nsignificantly outperforms previous methods, notably those that learn minimum\ndescription length programs. Our results also show that our approach is\ndata-efficient and insensitive to example balance, including the ability to\nlearn from exclusively positive examples.", "AI": {"tldr": "该研究提出了一种贝叶斯归纳逻辑编程（ILP）方法，通过最小消息长度（MML）原则从噪声数据中学习程序，有效平衡了假设复杂度和数据拟合，并在多个领域超越了现有方法，尤其擅长处理数据效率和示例不平衡问题。", "motivation": "将概率学习和逻辑学习统一是人工智能领域的一个关键挑战。", "method": "引入了一种贝叶斯归纳逻辑编程方法，该方法从噪声数据中学习最小消息长度（MML）程序。它通过先验（明确偏爱更通用的程序）和似然（偏爱更准确的程序）来平衡假设复杂度和数据拟合。", "result": "在游戏和药物设计等多个领域，该方法显著优于以前的方法，特别是那些学习最小描述长度（MDL）程序的方法。结果还表明，该方法数据高效，对示例平衡不敏感，包括能够仅从正例中学习。", "conclusion": "所提出的贝叶斯归纳逻辑编程方法在统一概率和逻辑学习方面取得了显著进展，能够从噪声数据中有效学习程序，并且在性能、数据效率和处理示例不平衡方面表现出色。"}}
{"id": "2508.05976", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05976", "abs": "https://arxiv.org/abs/2508.05976", "authors": ["Zhihao Zhu", "Yifan Zheng", "Siyu Pan", "Yaohui Jin", "Yao Mu"], "title": "PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation", "comment": "Accepted to ICCV 2025. 8 pages main paper, 8 figures, plus\n  supplementary material", "summary": "The fragmentation between high-level task semantics and low-level geometric\nfeatures remains a persistent challenge in robotic manipulation. While\nvision-language models (VLMs) have shown promise in generating affordance-aware\nvisual representations, the lack of semantic grounding in canonical spaces and\nreliance on manual annotations severely limit their ability to capture dynamic\nsemantic-affordance relationships. To address these, we propose Primitive-Aware\nSemantic Grounding (PASG), a closed-loop framework that introduces: (1)\nAutomatic primitive extraction through geometric feature aggregation, enabling\ncross-category detection of keypoints and axes; (2) VLM-driven semantic\nanchoring that dynamically couples geometric primitives with functional\naffordances and task-relevant description; (3) A spatial-semantic reasoning\nbenchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's\neffectiveness in practical robotic manipulation tasks across diverse scenarios,\nachieving performance comparable to manual annotations. PASG achieves a\nfiner-grained semantic-affordance understanding of objects, establishing a\nunified paradigm for bridging geometric primitives with task semantics in\nrobotic manipulation.", "AI": {"tldr": "提出PASG框架，通过自动几何基元提取和VLM驱动的语义锚定，弥合机器人操作中高层任务语义与低层几何特征之间的鸿沟。", "motivation": "机器人操作中，高层任务语义与低层几何特征之间的分离是一个持续的挑战。尽管视觉-语言模型（VLMs）在生成功能感知视觉表示方面显示出潜力，但它们缺乏在规范空间中的语义接地，且依赖手动标注，严重限制了其捕获动态语义-功能关系的能力。", "method": "本文提出了一个名为“基元感知语义接地”（PASG）的闭环框架，该框架包括：1) 通过几何特征聚合实现自动基元提取，从而实现关键点和轴的跨类别检测；2) 由VLM驱动的语义锚定，动态地将几何基元与功能可供性及任务相关描述相结合；3) 建立了一个空间-语义推理基准，并微调了一个VLM（Qwen2.5VL-PA）。", "result": "PASG在多样化的实际机器人操作任务中展示了其有效性，取得了与手动标注相媲美的性能。", "conclusion": "PASG实现了对物体更细粒度的语义-功能理解，为机器人操作中连接几何基元与任务语义建立了一个统一的范式。"}}
{"id": "2508.06149", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06149", "abs": "https://arxiv.org/abs/2508.06149", "authors": ["Gunhee Cho", "Yun-Gyung Cheong"], "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts", "comment": null, "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents.", "AI": {"tldr": "Big5-Scaler是一个基于提示的框架，能够通过将数值特质嵌入自然语言提示中，在无需额外训练的情况下，实现对大型语言模型（LLMs）大五人格特质的细粒度控制。", "motivation": "旨在解决LLMs在生成对话时缺乏可控人格特质的问题，并提供一种无需额外训练即可赋予LLMs细粒度人格控制能力的方法，以构建更具人格的对话代理。", "method": "开发了Big5-Scaler框架，通过将大五人格特质的数值嵌入到自然语言提示中，来调节LLMs的行为。该方法无需对模型进行额外训练。", "result": "实验结果表明，Big5-Scaler能够在不同LLMs中诱导出一致且可区分的人格特质。性能表现因提示类型和特质强度而异，其中简洁的提示和较低的特质强度表现出更高的效率和有效性。", "conclusion": "Big5-Scaler为构建具有人格感知能力的对话代理提供了一种高效且无需额外训练的方法，通过简单的提示工程即可实现LLMs的人格控制。"}}
{"id": "2508.06313", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06313", "abs": "https://arxiv.org/abs/2508.06313", "authors": ["Amir Hossein Barjini", "Mohammad Bahari", "Mahdi Hejrati", "Jouni Mattila"], "title": "Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators", "comment": "This is submitted to IEEE T-ASE", "summary": "This paper presents a unified system-level modeling and control framework for\nan all-electric heavy-duty robotic manipulator (HDRM) driven by\nelectromechanical linear actuators (EMLAs). A surrogate-enhanced actuator\nmodel, combining integrated electromechanical dynamics with a neural network\ntrained on a dedicated testbed, is integrated into an extended virtual\ndecomposition control (VDC) architecture augmented by a natural adaptation law.\nThe derived analytical HDRM model supports a hierarchical control structure\nthat seamlessly maps high-level force and velocity objectives to real-time\nactuator commands, accompanied by a Lyapunov-based stability proof. In\nmulti-domain simulations of both cubic and a custom planar triangular\ntrajectory, the proposed adaptive modular controller achieves sub-centimeter\nCartesian tracking accuracy. Experimental validation of the same 1-DoF platform\nunder realistic load emulation confirms the efficacy of the proposed control\nstrategy. These findings demonstrate that a surrogate-enhanced EMLA model\nembedded in the VDC approach can enable modular, real-time control of an\nall-electric HDRM, supporting its deployment in next-generation mobile working\nmachines.", "AI": {"tldr": "本文提出了一种用于由机电直线执行器驱动的全电动重型机械臂的统一系统级建模和控制框架。", "motivation": "旨在为全电动重型机械臂（HDRM）提供一种模块化、实时且高精度的控制策略，以支持其在下一代移动工作机器中的部署。", "method": "开发了一种替代增强型执行器模型，该模型结合了集成的机电动力学和一个在专用测试台上训练的神经网络。该模型被集成到一个扩展的虚拟分解控制（VDC）架构中，并辅以自然自适应律。所推导的HDRM分析模型支持分层控制结构，能够将高层力/速度目标无缝映射到实时执行器命令，并附有基于Lyapunov的稳定性证明。", "result": "在立方体和自定义平面三角形轨迹的多域仿真中，所提出的自适应模块化控制器实现了亚厘米级的笛卡尔跟踪精度。在具有实际负载模拟的1自由度平台上的实验验证也证实了所提出控制策略的有效性。", "conclusion": "研究结果表明，嵌入VDC方法中的替代增强型机电直线执行器（EMLA）模型能够实现全电动重型机械臂的模块化、实时控制，支持其在下一代移动工作机器中的部署。"}}
{"id": "2508.06263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06263", "abs": "https://arxiv.org/abs/2508.06263", "authors": ["Andrew Cropper", "David M. Cerna", "Matti Järvisalo"], "title": "Symmetry breaking for inductive logic programming", "comment": null, "summary": "The goal of inductive logic programming is to search for a hypothesis that\ngeneralises training data and background knowledge. The challenge is searching\nvast hypothesis spaces, which is exacerbated because many logically equivalent\nhypotheses exist. To address this challenge, we introduce a method to break\nsymmetries in the hypothesis space. We implement our idea in answer set\nprogramming. Our experiments on multiple domains, including visual reasoning\nand game playing, show that our approach can reduce solving times from over an\nhour to just 17 seconds.", "AI": {"tldr": "该论文提出了一种在归纳逻辑编程（ILP）中打破假设空间对称性的方法，并使用答案集编程（ASP）实现，显著缩短了求解时间。", "motivation": "归纳逻辑编程在庞大的假设空间中搜索通用假设时面临巨大挑战，尤其因为存在许多逻辑等价的假设，这进一步加剧了搜索难度。", "method": "引入了一种打破假设空间对称性的方法，并通过答案集编程（ASP）实现了该思想。", "result": "在视觉推理和游戏等多个领域的实验表明，该方法能将求解时间从一小时以上缩短到仅17秒。", "conclusion": "通过在归纳逻辑编程中打破假设空间的对称性，可以大幅提高搜索效率和求解速度。"}}
{"id": "2508.05982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05982", "abs": "https://arxiv.org/abs/2508.05982", "authors": ["Qingyang Liu", "Bingjie Gao", "Weiheng Huang", "Jun Zhang", "Zhongqian Sun", "Yang Wei", "Zelin Peng", "Qianli Ma", "Shuai Yang", "Zhaohe Liao", "Haonan Zhao", "Li Niu"], "title": "AnimateScene: Camera-controllable Animation in Any Scene", "comment": null, "summary": "3D scene reconstruction and 4D human animation have seen rapid progress and\nbroad adoption in recent years. However, seamlessly integrating reconstructed\nscenes with 4D human animation to produce visually engaging results remains\nchallenging. One key difficulty lies in placing the human at the correct\nlocation and scale within the scene while avoiding unrealistic\ninterpenetration. Another challenge is that the human and the background may\nexhibit different lighting and style, leading to unrealistic composites. In\naddition, appealing character motion videos are often accompanied by camera\nmovements, which means that the viewpoints need to be reconstructed along a\nspecified trajectory. We present AnimateScene, which addresses the above issues\nin a unified framework. First, we design an accurate placement module that\nautomatically determines a plausible 3D position for the human and prevents any\ninterpenetration within the scene during motion. Second, we propose a\ntraining-free style alignment method that adapts the 4D human representation to\nmatch the background's lighting and style, achieving coherent visual\nintegration. Finally, we design a joint post-reconstruction method for both the\n4D human and the 3D scene that allows camera trajectories to be inserted,\nenabling the final rendered video to feature visually appealing camera\nmovements. Extensive experiments show that AnimateScene generates dynamic scene\nvideos with high geometric detail and spatiotemporal coherence across various\ncamera and action combinations.", "AI": {"tldr": "AnimateScene是一个统一框架，用于解决4D人体动画与3D场景重建无缝集成中的挑战，包括精确放置、风格对齐和动态摄像机移动，从而生成逼真的动态场景视频。", "motivation": "将重建的3D场景与4D人体动画无缝集成以生成视觉吸引人的结果面临挑战：1) 人体在场景中的位置和比例不正确，且容易发生不真实的穿透；2) 人体与背景的光照和风格可能不匹配，导致合成不真实；3) 缺乏伴随角色动作的摄像机移动，影响视频吸引力。", "method": "本文提出了AnimateScene框架，包含三个模块：1) 精确放置模块，自动确定人体可信的3D位置并防止运动中的穿透；2) 免训练的风格对齐方法，使4D人体表示与背景的光照和风格匹配；3) 针对4D人体和3D场景的联合后重建方法，允许插入摄像机轨迹以实现吸引人的摄像机运动。", "result": "AnimateScene能够生成具有高几何细节和时空一致性的动态场景视频，适用于各种摄像机和动作组合。实验证明了其有效性。", "conclusion": "AnimateScene成功地解决了4D人体动画与3D场景重建集成中的关键问题，实现了精确放置、视觉风格统一和动态摄像机视角，从而生成了高质量的动态场景视频。"}}
{"id": "2508.06155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06155", "abs": "https://arxiv.org/abs/2508.06155", "authors": ["Renhan Zhang", "Lian Lian", "Zhen Qi", "Guiran Liu"], "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach", "comment": null, "summary": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired.", "AI": {"tldr": "本文提出了一种可解释的偏见检测方法，用于识别大型语言模型生成内容中隐性社会偏见，并通过结合嵌套语义表示和上下文对比机制，分析模型敏感性以揭示偏见形成路径。", "motivation": "大型语言模型在生成过程中可能产生隐性刻板印象，这些语义倾向难以通过显性语言特征捕捉，因此需要一种可解释的方法来识别和分析这些隐藏的社会偏见。", "method": "该方法结合了嵌套语义表示和上下文对比机制，从模型输出的向量空间结构中提取潜在偏见特征。通过注意力权重扰动，分析模型对特定社会属性词的敏感性，从而揭示偏见形成的语义路径。使用StereoSet数据集进行验证，评估偏见检测准确性、语义一致性和上下文敏感性。", "result": "实验结果表明，该方法在多个维度上（性别、职业、宗教、种族）均表现出强大的检测性能，能准确识别语义相似文本间的偏见差异，同时保持高语义对齐和输出稳定性。该方法在结构设计上具有高可解释性。", "conclusion": "该方法有助于揭示语言模型内部的偏见关联机制，为偏见检测提供了更透明和可靠的技术基础，适用于需要高可信度生成内容的实际应用场景。"}}
{"id": "2508.06319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06319", "abs": "https://arxiv.org/abs/2508.06319", "authors": ["Sagar Parekh", "Heramb Nemlekar", "Dylan P. Losey"], "title": "Towards Balanced Behavior Cloning from Imbalanced Datasets", "comment": null, "summary": "Robots should be able to learn complex behaviors from human demonstrations.\nIn practice, these human-provided datasets are inevitably imbalanced: i.e., the\nhuman demonstrates some subtasks more frequently than others. State-of-the-art\nmethods default to treating each element of the human's dataset as equally\nimportant. So if -- for instance -- the majority of the human's data focuses on\nreaching a goal, and only a few state-action pairs move to avoid an obstacle,\nthe learning algorithm will place greater emphasis on goal reaching. More\ngenerally, misalignment between the relative amounts of data and the importance\nof that data causes fundamental problems for imitation learning approaches. In\nthis paper we analyze and develop learning methods that automatically account\nfor mixed datasets. We formally prove that imbalanced data leads to imbalanced\npolicies when each state-action pair is weighted equally; these policies\nemulate the most represented behaviors, and not the human's complex, multi-task\ndemonstrations. We next explore algorithms that rebalance offline datasets\n(i.e., reweight the importance of different state-action pairs) without human\noversight. Reweighting the dataset can enhance the overall policy performance.\nHowever, there is no free lunch: each method for autonomously rebalancing\nbrings its own pros and cons. We formulate these advantages and disadvantages,\nhelping other researchers identify when each type of approach is most\nappropriate. We conclude by introducing a novel meta-gradient rebalancing\nalgorithm that addresses the primary limitations behind existing approaches.\nOur experiments show that dataset rebalancing leads to better downstream\nlearning, improving the performance of general imitation learning algorithms\nwithout requiring additional data collection. See our project website:\nhttps://collab.me.vt.edu/data_curation/.", "AI": {"tldr": "本文研究并开发了解决机器人模仿学习中人类演示数据不平衡问题的学习方法。研究发现数据不平衡会导致策略偏差，并提出了一种新的元梯度再平衡算法，显著提升了模仿学习的性能。", "motivation": "机器人应能从人类演示中学习复杂行为，但人类提供的数据集往往不平衡，某些子任务的演示频率远高于其他。现有方法默认平等对待所有数据，导致学习算法过度强调数据量大的行为，而非人类演示的复杂多任务行为，从而影响模仿学习的效果。", "method": "首先，形式化证明了当每个状态-动作对权重相等时，不平衡数据会导致不平衡策略。其次，探讨了无需人工干预即可对离线数据集进行再平衡（即重新加权不同状态-动作对的重要性）的算法，并分析了这些方法的优缺点。最后，引入了一种新颖的元梯度再平衡算法，旨在解决现有方法的局限性。", "result": "研究证明，不平衡数据确实会导致模仿学习产生偏向于数据量大的行为的策略。数据集再平衡能够增强整体策略性能，改善下游学习效果，并在不额外收集数据的情况下提升通用模仿学习算法的表现。", "conclusion": "数据集再平衡对于提升模仿学习性能至关重要。通过对现有再平衡方法的分析和新提出的元梯度再平衡算法，可以有效解决人类演示数据不平衡带来的问题，使机器人更好地学习复杂的、多任务的行为。"}}
{"id": "2508.06296", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06296", "abs": "https://arxiv.org/abs/2508.06296", "authors": ["Pierre Peigné - Lefebvre", "Quentin Feuillade-Montixi", "Tom David", "Nicolas Miailhe"], "title": "LLM Robustness Leaderboard v1 --Technical report", "comment": null, "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.", "AI": {"tldr": "该报告介绍了PRISM Eval BET工具，通过动态对抗优化对大型语言模型（LLMs）进行自动化红队测试，实现了极高的攻击成功率，并提出了细粒度的鲁棒性评估指标。", "motivation": "评估并量化LLM的鲁棒性，识别其在生成有害内容方面的脆弱性，并提供一种自动化的红队测试方法。", "method": "引入PRISM Eval行为诱导工具（BET），通过动态对抗优化进行自动化红队测试。提出了一个细粒度的鲁棒性指标，估计诱导有害行为所需的平均尝试次数。进行了原语级别（primitive-level）的漏洞分析，以识别对特定危害类别最有效的越狱技术。与AI安全网络的第三方进行协作评估。", "result": "PRISM Eval BET对41个最先进的LLM中的37个实现了100%的攻击成功率（ASR）。尽管模型普遍存在漏洞，但攻击难度在模型之间差异超过300倍。识别了针对特定危害类别最有效的越狱技术。展示了分布式鲁棒性评估的实际途径。", "conclusion": "PRISM Eval BET是一种高效的自动化红队工具，能够揭示LLM普遍存在的漏洞，并提供了一种细粒度的鲁棒性评估方法。该研究为社区协作评估LLM鲁棒性提供了实用路径。"}}
{"id": "2508.05989", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05989", "abs": "https://arxiv.org/abs/2508.05989", "authors": ["Younjoon Chung", "Hyoungseob Park", "Patrick Rim", "Xiaoran Zhang", "Jihe He", "Ziyao Zeng", "Safa Cicek", "Byung-Woo Hong", "James S. Duncan", "Alex Wong"], "title": "ETA: Energy-based Test-time Adaptation for Depth Completion", "comment": null, "summary": "We propose a method for test-time adaptation of pretrained depth completion\nmodels. Depth completion models, trained on some ``source'' data, often predict\nerroneous outputs when transferred to ``target'' data captured in novel\nenvironmental conditions due to a covariate shift. The crux of our method lies\nin quantifying the likelihood of depth predictions belonging to the source data\ndistribution. The challenge is in the lack of access to out-of-distribution\n(target) data prior to deployment. Hence, rather than making assumptions\nregarding the target distribution, we utilize adversarial perturbations as a\nmechanism to explore the data space. This enables us to train an energy model\nthat scores local regions of depth predictions as in- or out-of-distribution.\nWe update the parameters of pretrained depth completion models at test time to\nminimize energy, effectively aligning test-time predictions to those of the\nsource distribution. We call our method ``Energy-based Test-time Adaptation'',\nor ETA for short. We evaluate our method across three indoor and three outdoor\ndatasets, where ETA improve over the previous state-of-the-art method by an\naverage of 6.94% for outdoors and 10.23% for indoors. Project Page:\nhttps://fuzzythecat.github.io/eta.", "AI": {"tldr": "该论文提出了一种名为ETA（基于能量的测试时间适应）的方法，用于在测试时自适应预训练的深度补全模型，以解决协变量偏移问题，通过训练一个能量模型来量化深度预测属于源数据分布的可能性，并最小化能量来调整模型参数。", "motivation": "预训练的深度补全模型在转移到新环境（目标数据）时，由于协变量偏移，往往会产生错误的输出。挑战在于部署前无法访问目标数据。", "method": "该方法的核心是量化深度预测属于源数据分布的可能性。由于无法预先访问目标数据，论文利用对抗性扰动来探索数据空间。在此基础上，训练一个能量模型来评估深度预测的局部区域是分布内还是分布外。在测试时，更新预训练深度补全模型的参数以最小化能量，从而使测试时的预测与源分布对齐。", "result": "该方法在三个室内和三个室外数据集上进行了评估，ETA在室外数据集上比现有最先进方法平均提高了6.94%，在室内数据集上平均提高了10.23%。", "conclusion": "ETA方法有效地解决了深度补全模型在面对新环境时的协变量偏移问题，通过在测试时自适应模型参数，显著提高了深度预测的准确性，超越了现有技术水平。"}}
{"id": "2508.06163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06163", "abs": "https://arxiv.org/abs/2508.06163", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging.", "AI": {"tldr": "本文提出了TADrop，一种自适应稀疏化策略，通过根据参数张量的分布特性分配定制的稀疏度，以优化模型合并中的参数剪枝，显著提升了多任务学习性能。", "motivation": "现有模型合并中的稀疏化方法采用“一刀切”的策略，即统一的稀疏比，忽略了模型参数固有的结构和统计异质性。这导致关键参数被错误剪枝，而冗余参数被保留，从而影响合并性能。", "method": "TADrop（Tensor-wise Adaptive Drop）是一种自适应稀疏化策略。它不采用全局稀疏比，而是根据每个参数张量的分布特性为其分配定制的稀疏度。核心思想是，对于分布更密集、更冗余的张量进行更激进的剪枝，而对于更稀疏、更关键的张量则予以保留。TADrop作为一个即插即用的模块，可以与现有模型合并方法集成。", "result": "通过将TADrop与基础、经典和最先进的合并方法相结合，并在视觉、语言和多模态等多种任务以及ViT、BEiT等模型上进行广泛实验，结果表明TADrop始终显著提升了它们的性能。例如，在增强领先的合并方法时，它在8个ViT-B/32任务上平均性能提升了2.0%。", "conclusion": "TADrop通过根据模型的结构调整稀疏化，提供了一种更有效的方法来减轻参数干扰，为高性能模型合并提供了一个新的基线。"}}
{"id": "2508.06330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06330", "abs": "https://arxiv.org/abs/2508.06330", "authors": ["Baorun Li", "Chengrui Zhu", "Siyi Du", "Bingran Chen", "Jie Ren", "Wenfei Wang", "Yong Liu", "Jiajun Lv"], "title": "L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience", "comment": "IROS2025", "summary": "Extrinsic calibration is essential for multi-sensor fusion, existing methods\nrely on structured targets or fully-excited data, limiting real-world\napplicability. Online calibration further suffers from weak excitation, leading\nto unreliable estimates. To address these limitations, we propose a\nreinforcement learning (RL)-based extrinsic calibration framework that\nformulates extrinsic calibration as a decision-making problem, directly\noptimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach\nleverages a probabilistic Bingham distribution to model 3D rotations, ensuring\nstable optimization while inherently retaining quaternion symmetry. A\ntrajectory alignment reward mechanism enables robust calibration without\nstructured targets by quantitatively evaluating estimated tightly-coupled\ntrajectory against a reference trajectory. Additionally, an automated data\nselection module filters uninformative samples, significantly improving\nefficiency and scalability for large-scale datasets. Extensive experiments on\nUAVs, UGVs, and handheld platforms demonstrate that our method outperforms\ntraditional optimization-based approaches, achieving high-precision calibration\neven under weak excitation conditions. Our framework simplifies deployment on\ndiverse robotic platforms by eliminating the need for high-quality initial\nextrinsics and enabling calibration from routine operating data. The code is\navailable at https://github.com/APRIL-ZJU/learn-to-calibrate.", "AI": {"tldr": "提出了一种基于强化学习（RL）的外参校准框架，将校准视为决策问题，直接优化SE(3)以提高里程计精度，无需结构化目标或强激励数据，实现高精度校准。", "motivation": "现有外参校准方法依赖于结构化目标或完全激励数据，限制了实际应用；在线校准在弱激励下不可靠，导致估计不稳定。", "method": "将外参校准建模为决策问题，通过RL直接优化SE(3)外参；使用概率Bingham分布建模3D旋转，确保稳定优化并保留四元数对称性；采用轨迹对齐奖励机制，通过评估估计轨迹与参考轨迹的匹配程度，实现无需结构化目标的鲁棒校准；引入自动化数据选择模块，过滤无信息样本，提高效率和可扩展性。", "result": "在无人机、无人车和手持平台上的实验表明，该方法优于传统的基于优化的方法，即使在弱激励条件下也能实现高精度校准；无需高质量的初始外参，可使用日常操作数据进行校准，简化了在不同机器人平台上的部署。", "conclusion": "该基于RL的外参校准框架克服了传统方法的局限性，实现了在弱激励和无结构化目标条件下的高精度、鲁棒校准，显著简化了多传感器系统在各种机器人平台上的部署和校准过程。"}}
{"id": "2508.06348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06348", "abs": "https://arxiv.org/abs/2508.06348", "authors": ["Mille Mei Zhen Loo", "Gert Luzkov", "Paolo Burelli"], "title": "AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games", "comment": null, "summary": "Cheating in online video games compromises the integrity of gaming\nexperiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face\nsignificant challenges in keeping pace with evolving cheating methods without\nimposing invasive measures on users' systems. This paper presents\nAntiCheatPT\\_256, a transformer-based machine learning model designed to detect\ncheating behaviour in Counter-Strike 2 using gameplay data. To support this, we\nintroduce and publicly release CS2CD: A labelled dataset of 795 matches. Using\nthis dataset, 90,707 context windows were created and subsequently augmented to\naddress class imbalance. The transformer model, trained on these windows,\nachieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test\nset. This approach emphasizes reproducibility and real-world applicability,\noffering a robust baseline for future research in data-driven cheat detection.", "AI": {"tldr": "本文提出了AntiCheatPT_256，一个基于Transformer的机器学习模型，用于利用游戏数据检测《反恐精英2》中的作弊行为，并发布了CS2CD数据集。", "motivation": "在线视频游戏中的作弊行为损害了游戏体验的完整性。现有的反作弊系统（如VAC）在不侵犯用户系统隐私的前提下，难以跟上不断演变的作弊方法。", "method": "研究开发了一个名为AntiCheatPT_256的基于Transformer的机器学习模型。为支持该模型，创建并公开发布了包含795场比赛的CS2CD标记数据集。利用该数据集生成了90,707个上下文窗口，并对其进行了数据增强以解决类别不平衡问题，然后用这些窗口训练了Transformer模型。", "result": "在未经增强的测试集上，训练后的Transformer模型达到了89.17%的准确率和93.36%的AUC（受试者工作特征曲线下面积）。", "conclusion": "该方法强调可复现性和实际应用性，为未来数据驱动的作弊检测研究提供了一个稳健的基准。"}}
{"id": "2508.05990", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05990", "abs": "https://arxiv.org/abs/2508.05990", "authors": ["Haichao Wang", "Xinyue Xi", "Jiangtao Wen", "Yuxing Han"], "title": "Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision", "comment": null, "summary": "The efficiency of video computer vision system remains a challenging task due\nto the high temporal redundancy inside a video. Existing works have been\nproposed for efficient vision computer vision. However, they do not fully\nreduce the temporal redundancy and neglect the front end computation overhead.\nIn this paper, we propose an efficient video computer vision system. First,\nimage signal processor is removed and Bayer-format data is directly fed into\nvideo computer vision models, thus saving the front end computation. Second,\ninstead of optical flow models and video codecs, a fast block matching-based\nmotion estimation algorithm is proposed specifically for efficient video\ncomputer vision, with a MV refinement module. To correct the error,\ncontext-aware block refinement network is introduced to refine regions with\nlarge error. To further balance the accuracy and efficiency, a frame selection\nstrategy is employed. Experiments on multiple video computer vision tasks\ndemonstrate that our method achieves significant acceleration with slight\nperformance loss.", "AI": {"tldr": "该论文提出一种高效视频计算机视觉系统，通过移除ISP、直接处理Bayer数据、采用快速块匹配运动估计、上下文感知块细化网络和帧选择策略，显著降低了时间冗余和前端计算开销，从而实现系统加速并保持性能。", "motivation": "视频计算机视觉系统由于高时间冗余而效率低下，现有方法未能充分减少时间冗余并忽略了前端计算开销。", "method": "1. 移除图像信号处理器（ISP），将Bayer格式数据直接输入视频计算机视觉模型，节省前端计算。2. 提出一种基于块匹配的快速运动估计算法，并包含运动矢量（MV）细化模块，替代光流模型和视频编解码器。3. 引入上下文感知块细化网络，纠正误差较大的区域。4. 采用帧选择策略，进一步平衡精度和效率。", "result": "在多项视频计算机视觉任务上的实验表明，所提出的方法在性能损失轻微的情况下，实现了显著的系统加速。", "conclusion": "该论文成功设计并实现了一个高效的视频计算机视觉系统，有效解决了视频时间冗余和前端计算开销问题，为视频视觉任务提供了更快的处理速度和可接受的性能。"}}
{"id": "2508.06165", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06165", "abs": "https://arxiv.org/abs/2508.06165", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.", "AI": {"tldr": "提出UR2框架，通过难度感知课程训练和混合知识访问策略，将检索增强生成（RAG）和基于可验证奖励的强化学习（RLVR）统一起来，显著提升LLM在多任务上的性能，超越现有RAG和RL方法。", "motivation": "现有大型语言模型（LLMs）的检索增强生成（RAG）和基于可验证奖励的强化学习（RLVR）能力通常是独立开发的，或仅在狭窄范围内有限统一，这限制了其泛化能力和在更广泛领域中的应用。", "method": "提出UR2（统一RAG和推理）框架，通过强化学习整合检索与推理。主要包括两点：1. 难度感知课程训练，仅对有挑战性的问题触发检索；2. 混合知识访问策略，结合领域特定离线语料库和LLM生成的摘要。这些旨在实现检索与推理的动态协调。", "result": "在开放域问答、MMLU-Pro、医学和数学推理等多种任务上的实验表明，UR2（基于Qwen2.5-3/7B和LLaMA-3.1-8B构建）显著优于现有RAG和RL方法，并在多个基准测试中达到与GPT-4o-mini和GPT-4.1-mini相当的性能。", "conclusion": "UR2框架成功地统一了LLMs的检索和推理能力，通过创新的训练和知识访问策略，显著提升了模型在多样化任务上的适应性和性能，为更通用的RAG-RL方法奠定了基础。"}}
{"id": "2508.06404", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06404", "abs": "https://arxiv.org/abs/2508.06404", "authors": ["Abdullah Zareh Andaryan", "Michael G. H. Bell", "Mohsen Ramezani", "Glenn Geers"], "title": "V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicle navigation in structured environments requires planners\ncapable of generating time-optimal, collision-free trajectories that satisfy\ndynamic and kinematic constraints. We introduce V*, a graph-based motion\nplanner that represents speed and direction as explicit state variables within\na discretised space-time-velocity lattice. Unlike traditional methods that\ndecouple spatial search from dynamic feasibility or rely on post-hoc smoothing,\nV* integrates both motion dimensions directly into graph construction through\ndynamic graph generation during search expansion. To manage the complexity of\nhigh-dimensional search, we employ a hexagonal discretisation strategy and\nprovide formal mathematical proofs establishing optimal waypoint spacing and\nminimal node redundancy under constrained heading transitions for\nvelocity-aware motion planning. We develop a mathematical formulation for\ntransient steering dynamics in the kinematic bicycle model, modelling steering\nangle convergence with exponential behaviour, and deriving the relationship for\nconvergence rate parameters. This theoretical foundation, combined with\ngeometric pruning strategies that eliminate expansions leading to infeasible\nsteering configurations, enables V* to evaluate dynamically admissible\nmanoeuvres, ensuring each trajectory is physically realisable without further\nrefinement. We further demonstrate V*'s performance in simulation studies with\ncluttered and dynamic environments involving moving obstacles, showing its\nability to avoid conflicts, yield proactively, and generate safe, efficient\ntrajectories with temporal reasoning capabilities for waiting behaviours and\ndynamic coordination.", "AI": {"tldr": "V*是一种基于图的运动规划器，通过在离散时空速度格中显式表示速度和方向，生成满足动力学和运动学约束的时间最优、无碰撞轨迹，无需后期平滑。", "motivation": "在结构化环境中，自动驾驶车辆需要规划器能够生成时间最优、无碰撞且满足动力学和运动学约束的轨迹。传统方法通常将空间搜索与动态可行性解耦或依赖后期平滑，导致生成的轨迹可能不物理可行。", "method": "V*通过以下方法实现：1) 将速度和方向作为显式状态变量，在离散化的时空速度格中进行图搜索。2) 在搜索扩展过程中动态生成图，直接整合运动维度。3) 采用六边形离散化策略，并提供数学证明以优化路径点间距和减少节点冗余。4) 建立运动学自行车模型的瞬态转向动力学数学公式，模拟转向角收敛行为并推导收敛速率参数。5) 结合几何剪枝策略，消除导致不可行转向配置的扩展，确保轨迹物理可实现。", "result": "在模拟研究中，V*在拥挤和动态环境中（包含移动障碍物）表现出色，能够避免冲突、主动避让，并生成安全、高效的轨迹。它还具备时间推理能力，可以处理等待行为和动态协调。", "conclusion": "V*是一种有效的运动规划器，能够为自动驾驶车辆在复杂动态环境中生成物理可行的、时间最优的、无碰撞的轨迹，且无需额外的后处理。"}}
{"id": "2508.06352", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06352", "abs": "https://arxiv.org/abs/2508.06352", "authors": ["Christian Meske", "Justin Brenne", "Erdi Uenal", "Sabahat Oelcer", "Ayseguel Doganguen"], "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI", "comment": null, "summary": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts.", "AI": {"tldr": "该论文提出了“解释性AI”（Explanatory AI），作为可解释AI（XAI）的补充范式，利用生成式AI能力，通过叙事交流、自适应个性化和渐进式披露来促进人类理解，而非仅仅揭示算法透明度。", "motivation": "当前的XAI方法过于侧重算法透明度，提供的解释抽象且非自适应，往往无法支持终端用户的真正理解，未能有效支持社会技术环境中的人类决策。", "method": "引入“解释性AI”范式，并定义了一个系统的八维概念模型，通过叙事沟通、自适应个性化和渐进式披露原则来区分解释性AI。通过与医疗专业人员进行快速情境设计（Rapid Contextual Design）方法进行实证验证。", "result": "实证结果表明，用户一致偏好情境敏感、多模态的解释，而非技术透明度。研究揭示了设计AI系统以支持人类理解而非算法内省的实际紧迫性。", "conclusion": "该研究为推进以用户为中心的AI解释方法在不同领域和文化背景下的发展，确立了一个全面的研究议程，强调了为人类理解而设计AI系统的紧迫性。"}}
{"id": "2508.05991", "categories": ["cs.CV", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05991", "abs": "https://arxiv.org/abs/2508.05991", "authors": ["Juewen Hu", "Yexin Li", "Jiulin Li", "Shuo Chen", "Pring Wong"], "title": "ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge", "comment": null, "summary": "Emotion recognition plays a vital role in enhancing human-computer\ninteraction. In this study, we tackle the MER-SEMI challenge of the MER2025\ncompetition by proposing a novel multimodal emotion recognition framework. To\naddress the issue of data scarcity, we leverage large-scale pre-trained models\nto extract informative features from visual, audio, and textual modalities.\nSpecifically, for the visual modality, we design a dual-branch visual encoder\nthat captures both global frame-level features and localized facial\nrepresentations. For the textual modality, we introduce a context-enriched\nmethod that employs large language models to enrich emotional cues within the\ninput text. To effectively integrate these multimodal features, we propose a\nfusion strategy comprising two key components, i.e., self-attention mechanisms\nfor dynamic modality weighting, and residual connections to preserve original\nrepresentations. Beyond architectural design, we further refine noisy labels in\nthe training set by a multi-source labeling strategy. Our approach achieves a\nsubstantial performance improvement over the official baseline on the\nMER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to\n78.63%, thereby validating the effectiveness of the proposed framework.", "AI": {"tldr": "该研究提出了一种新颖的多模态情感识别框架，通过利用预训练模型、设计双分支视觉编码器、上下文丰富文本方法、创新的融合策略以及多源标签精炼来解决数据稀缺和噪声标签问题，并在MER2025-SEMI数据集上取得了显著性能提升。", "motivation": "情感识别在增强人机交互中扮演着重要角色。本研究旨在解决MER2025竞赛中MER-SEMI挑战的数据稀缺问题，并提高情感识别的准确性。", "method": "1. 利用大规模预训练模型从视觉、音频和文本模态中提取特征。2. 视觉模态：设计双分支视觉编码器，捕获全局帧级特征和局部面部表示。3. 文本模态：引入上下文丰富方法，使用大型语言模型增强文本中的情感线索。4. 多模态融合：采用自注意力机制进行动态模态加权，并使用残差连接保留原始表示。5. 标签精炼：通过多源标注策略优化训练集中的噪声标签。", "result": "该方法在MER2025-SEMI数据集上取得了显著优于官方基线的性能提升，加权F分数从78.63%提高到87.49%。", "conclusion": "所提出的多模态情感识别框架有效且性能优越，成功验证了其在解决数据稀缺和噪声标签问题上的有效性。"}}
{"id": "2508.06167", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06167", "abs": "https://arxiv.org/abs/2508.06167", "authors": ["Vít Gvoždiak"], "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.", "AI": {"tldr": "论文重新定义了语用学，使其成为语言作为社会工具的动态接口，并探讨了在大型语言模型（LLMs）背景下语用学理论需要如何调整，提出了新的框架并分析了现有问题。", "motivation": "随着大型语言模型（LLMs）在交流语境中的出现，传统的语用学理解和方法论需要被重新审视和完善，因为现有理论难以适应LLMs的特性。", "method": "论文通过以下方式展开论证：1) 挑战传统符号学三元论，提出人机交流（HMC）框架；2) 审视以人为中心的语用学理论与以机器为中心的LLMs之间的张力，推荐概率语用学（特别是理性言语行为框架）；3) 探讨替代主义的三种形式及其带来的人类中心偏见；4) 引入“语境挫败”概念，描述LLM交流中语境理解的悖论。", "result": "论文指出：连接主义LLM架构颠覆了既定的意义层级；传统语用学不适用于预测系统如LLMs；概率语用学更契合LLMs的优化目标；人类中心偏见扭曲了LLM评估；用户被迫与模型共同构建语用条件，导致语境理解的崩溃（语境挫败）。", "conclusion": "语用学理论可能需要调整或扩展，以更好地解释涉及生成式人工智能的交流。"}}
{"id": "2508.06426", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06426", "abs": "https://arxiv.org/abs/2508.06426", "authors": ["Youguang Xing", "Xu Luo", "Junlin Xie", "Lianli Gao", "Hengtao Shen", "Jingkuan Song"], "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation", "comment": "CoRL 2025", "summary": "Generalist robot policies trained on large-scale datasets such as Open\nX-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.\nHowever, they often struggle to generalize beyond the distribution of their\ntraining data. In this paper, we investigate the underlying cause of this\nlimited generalization capability. We identify shortcut learning -- the\nreliance on task-irrelevant features -- as a key impediment to generalization.\nThrough comprehensive theoretical and empirical analysis, we uncover two\nprimary contributors to shortcut learning: (1) limited diversity within\nindividual sub-datasets, and (2) significant distributional disparities across\nsub-datasets, leading to dataset fragmentation. These issues arise from the\ninherent structure of large-scale datasets like OXE, which are typically\ncomposed of multiple sub-datasets collected independently across varied\nenvironments and embodiments. Our findings provide critical insights into\ndataset collection strategies that can reduce shortcut learning and enhance the\ngeneralization ability of generalist robot policies. Moreover, in scenarios\nwhere acquiring new large-scale data is impractical, we demonstrate that\ncarefully selected robotic data augmentation strategies can effectively reduce\nshortcut learning in existing offline datasets, thereby improving\ngeneralization capabilities of generalist robot policies, e.g., $\\pi_0$, in\nboth simulation and real-world environments. More information at\nhttps://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.", "AI": {"tldr": "通用机器人策略因捷径学习导致泛化能力受限，其根源在于大型数据集中子数据集的多样性不足和分布差异。论文提出改进数据集收集策略，并证明数据增强可有效缓解此问题。", "motivation": "通用机器人策略在大型数据集上表现良好，但难以泛化到训练数据分布之外。本研究旨在探究其泛化能力受限的根本原因。", "method": "通过全面的理论和实证分析，识别并研究了捷径学习（即依赖任务无关特征）作为泛化障碍。分析了大型数据集（如OXE）的内在结构，特别是其由独立收集的子数据集组成的问题。此外，还探索并展示了精心选择的机器人数据增强策略如何减少现有离线数据集中的捷径学习。", "result": "识别出捷径学习是限制泛化能力的关键障碍。发现捷径学习的两个主要原因：1) 单个子数据集内部多样性有限；2) 子数据集之间存在显著的分布差异，导致数据集碎片化。研究表明，数据增强可以有效减少现有离线数据集中的捷径学习，从而提高通用机器人策略的泛化能力。", "conclusion": "本研究为减少捷径学习和增强通用机器人策略泛化能力的数据集收集策略提供了重要见解。同时，在无法获取新大规模数据的情况下，证明了数据增强是一种有效的解决方案，可以改善策略在模拟和真实世界环境中的泛化性能。"}}
{"id": "2508.06368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06368", "abs": "https://arxiv.org/abs/2508.06368", "authors": ["Claudia dAmato", "Giuseppe Rubini", "Francesco Didio", "Donato Francioso", "Fatima Zahra Amara", "Nicola Fanizzi"], "title": "Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned", "comment": null, "summary": "Legal decision-making process requires the availability of comprehensive and\ndetailed legislative background knowledge and up-to-date information on legal\ncases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a\nvaluable tool to facilitate access to legal information, to be queried and\nexploited for the purpose, and to enable advanced reasoning and machine\nlearning applications. Indeed, legal KGs may act as knowledge intensive\ncomponent to be used by pre-dictive machine learning solutions supporting the\ndecision process of the legal expert. Nevertheless, a few KGs can be found in\nthe legal domain. To fill this gap, we developed a legal KG targeting legal\ncases of violence against women, along with clear adopted methodologies.\nSpecifically, the paper introduces two complementary approaches for automated\nlegal KG construction; a systematic bottom-up approach, customized for the\nlegal domain, and a new solution leveraging Large Language Models. Starting\nfrom legal sentences publicly available from the European Court of Justice, the\nsolutions integrate structured data extraction, ontology development, and\nsemantic enrichment to produce KGs tailored for legal cases involving violence\nagainst women. After analyzing and comparing the results of the two approaches,\nthe developed KGs are validated via suitable competency questions. The obtained\nKG may be impactful for multiple purposes: can improve the accessibility to\nlegal information both to humans and machine, can enable complex queries and\nmay constitute an important knowledge component to be possibly exploited by\nmachine learning tools tailored for predictive justice.", "AI": {"tldr": "本文开发了针对侵害女性案件的法律知识图谱（KGs），结合了系统性自下而上方法和大型语言模型（LLMs）两种自动化构建方法，旨在提升法律信息的可访问性和支持预测性司法。", "motivation": "法律决策过程需要全面的法律背景知识和最新的案例信息，但法律领域缺乏可用的知识图谱。构建法律KG可以促进法律信息的获取、查询、高级推理和机器学习应用，作为预测性机器学习解决方案的知识密集型组件。", "method": "研究采用了两种互补的自动化法律KG构建方法：1) 针对法律领域定制的系统性自下而上方法；2) 利用大型语言模型的新解决方案。数据来源于欧洲法院公开的法律判决，整合了结构化数据提取、本体开发和语义丰富化。构建的KG通过适用的能力问题进行验证，并对两种方法的结果进行了分析和比较。", "result": "成功开发了针对侵害女性案件的法律知识图谱，并对两种自动化构建方法的成果进行了分析和比较。通过能力问题验证了所构建的KG。", "conclusion": "所构建的法律知识图谱具有多重影响力：能提高人类和机器访问法律信息的能力，支持复杂的查询，并可作为预测性司法机器学习工具的重要知识组件。"}}
{"id": "2508.05994", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05994", "abs": "https://arxiv.org/abs/2508.05994", "authors": ["Huadong Wu", "Yi Fu", "Yunhao Li", "Yuan Gao", "Kang Du"], "title": "EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad", "comment": null, "summary": "Facial makeup editing aims to realistically transfer makeup from a reference\nto a target face. Existing methods often produce low-quality results with\ncoarse makeup details and struggle to preserve both identity and makeup\nfidelity, mainly due to the lack of structured paired data -- where source and\nresult share identity, and reference and result share identical makeup. To\naddress this, we introduce MakeupQuad, a large-scale, high-quality dataset with\nnon-makeup faces, references, edited results, and textual makeup descriptions.\nBuilding on this, we propose EvoMakeup, a unified training framework that\nmitigates image degradation during multi-stage distillation, enabling iterative\nimprovement of both data and model quality. Although trained solely on\nsynthetic data, EvoMakeup generalizes well and outperforms prior methods on\nreal-world benchmarks. It supports high-fidelity, controllable, multi-task\nmakeup editing -- including full-face and partial reference-based editing, as\nwell as text-driven makeup editing -- within a single model. Experimental\nresults demonstrate that our method achieves superior makeup fidelity and\nidentity preservation, effectively balancing both aspects. Code and dataset\nwill be released upon acceptance.", "AI": {"tldr": "本文提出了一种名为MakeupQuad的大规模高质量数据集和名为EvoMakeup的统一训练框架，用于解决现有面部化妆编辑方法中细节粗糙、身份和妆容保真度难以兼顾的问题，实现了高保真、可控的多任务化妆编辑。", "motivation": "现有面部化妆编辑方法常产生低质量结果，妆容细节粗糙，并且难以同时保持身份和妆容的保真度。这主要是由于缺乏结构化的配对数据，即源图像与结果共享身份，参考图像与结果共享相同妆容的数据。", "method": "为了解决数据缺乏问题，作者引入了MakeupQuad数据集，其中包含非化妆脸、参考图、编辑结果和文本描述。在此基础上，提出了EvoMakeup，一个统一的训练框架，通过缓解多阶段蒸馏过程中的图像退化，实现了数据和模型质量的迭代改进。", "result": "尽管仅在合成数据上训练，EvoMakeup在真实世界基准测试中表现出色并优于现有方法。它在一个模型中支持高保真、可控的多任务化妆编辑，包括全脸和部分基于参考的编辑，以及文本驱动的化妆编辑。实验结果表明，该方法在妆容保真度和身份保留方面均达到卓越水平，有效平衡了这两个方面。", "conclusion": "该研究通过引入新的数据集和训练框架，成功解决了面部化妆编辑中妆容细节、身份和保真度难以兼顾的挑战，实现了卓越的化妆编辑效果，并在真实场景中展现出强大的泛化能力。"}}
{"id": "2508.06178", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06178", "abs": "https://arxiv.org/abs/2508.06178", "authors": ["Hugo Abonizio", "Thales Almeida", "Roberto Lotufo", "Rodrigo Nogueira"], "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime", "comment": null, "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.", "AI": {"tldr": "该研究探讨了向大型语言模型（LLMs）注入少量非结构化信息，并解决灾难性遗忘问题。通过合成数据增强，特别是多样化提示生成的数据，显著提高了模型在新知识获取方面的能力，并揭示了小数据量下知识学习与遗忘的平衡。", "motivation": "大型语言模型通常需要大量文本才能有效获取新知识。虽然在大语料上持续预训练或使用检索增强生成（RAG）已成功，但用少量（几千或几百万）tokens更新LLM仍具挑战性，且与灾难性遗忘现象相关。", "method": "研究使用近期新闻数据集（确保与模型预训练数据无重叠）来评估知识获取。从持续预训练基线开始，探索了不同的增强算法来生成合成数据，特别是通过多样化提示诱导更大变异性的方法。同时，也将结果与RAG方法进行了比较。", "result": "仅在有限数据上持续预训练改进有限；而让模型接触多样化的文本变体（特别是通过多样化提示诱导的变体）显著提高了新事实的学习能力。在小数据量下存在遗忘现象，学习新内容与保留现有能力之间存在微妙平衡。RAG方法在知识注入上对控制数据集的性能下降比参数化方法更敏感。模型可以生成有效的合成训练数据，表明了自我改进模型更新的潜力。", "conclusion": "通过生成多样化的合成数据，可以有效提高LLMs在小数据量下获取新知识的能力，同时需要平衡新知识学习与现有能力的保持。RAG方法在小数据量知识注入中可能更易导致性能下降。模型自我生成有效训练数据为未来自我改进的模型更新提供了途径。"}}
{"id": "2508.06113", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06113", "abs": "https://arxiv.org/abs/2508.06113", "authors": ["Jian Wang", "Chaokang Jiang", "Haitao Xu"], "title": "GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving", "comment": "7 pages, 4 figures", "summary": "Diffusion-based models are redefining the state-of-the-art in end-to-end\nautonomous driving, yet their performance is increasingly hampered by a\nreliance on transformer-based fusion. These architectures face fundamental\nlimitations: quadratic computational complexity restricts the use of\nhigh-resolution features, and a lack of spatial priors prevents them from\neffectively modeling the inherent structure of Bird's Eye View (BEV)\nrepresentations. This paper introduces GMF-Drive (Gated Mamba Fusion for\nDriving), an end-to-end framework that overcomes these challenges through two\nprincipled innovations. First, we supersede the information-limited\nhistogram-based LiDAR representation with a geometrically-augmented pillar\nformat encoding shape descriptors and statistical features, preserving critical\n3D geometric details. Second, we propose a novel hierarchical gated mamba\nfusion (GM-Fusion) architecture that substitutes an expensive transformer with\na highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM\nleverages directional sequencing and adaptive fusion mechanisms to capture\nlong-range dependencies with linear complexity, while explicitly respecting the\nunique spatial properties of the driving scene. Extensive experiments on the\nchallenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new\nstate-of-the-art performance, significantly outperforming DiffusionDrive.\nComprehensive ablation studies validate the efficacy of each component,\ndemonstrating that task-specific SSMs can surpass a general-purpose transformer\nin both performance and efficiency for autonomous driving.", "AI": {"tldr": "GMF-Drive提出了一种新的激光雷达表示和基于Mamba的状态空间模型融合架构，以解决自动驾驶中扩散模型受限于Transformer融合的计算复杂性和空间先验不足问题，实现了SOTA性能。", "motivation": "当前自动驾驶中基于扩散的模型依赖于Transformer融合，但Transformer存在计算复杂度高（二次方）限制高分辨率特征使用，以及缺乏空间先验无法有效建模BEV表示结构的问题。", "method": "1. 引入几何增强的柱状激光雷达表示（GMF-Drive），编码形状描述符和统计特征，替代信息受限的直方图表示。2. 提出一种新型分层门控Mamba融合（GM-Fusion）架构，用高效且空间感知的状态空间模型（SSM）替代Transformer。其核心BEV-SSM利用定向序列和自适应融合机制，以线性复杂度捕获长距离依赖，并显式尊重驾驶场景的空间特性。", "result": "GMF-Drive在NAVSIM基准测试中取得了新的SOTA性能，显著优于DiffusionDrive。全面的消融研究验证了每个组件的有效性。", "conclusion": "针对特定任务的SSM（如GMF-Drive中的GM-Fusion）在自动驾驶领域可以超越通用Transformer，在性能和效率上均表现更优。"}}
{"id": "2508.06443", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06443", "abs": "https://arxiv.org/abs/2508.06443", "authors": ["Debabrota Basu", "Udvas Das"], "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time", "comment": null, "summary": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment.", "AI": {"tldr": "本文提出了一种名为“公平博弈”的动态机制，利用强化学习在机器学习算法周围构建一个由审计器和去偏算法组成的循环，以实现预测的公平性并随时间适应社会互动。", "motivation": "现有的公平机器学习偏见定义通常是观察性的、冲突的，且需要先验知识或事后部署才能应用，这与在动态社会环境中实现公平的目标存在差距。因此，需要一种更灵活和自适应的机制来确保公平性。", "method": "提出“公平博弈”机制，将一个审计器（Auditor）和一个去偏算法（Debiasing algorithm）通过强化学习（RL）循环围绕机器学习算法。审计器量化偏见并向去偏算法提供反馈，去偏算法据此调整机器学习算法的预测。", "result": "“公平博弈”框架允许公平目标随时间动态调整，只需修改审计器及其量化的偏见类型。它模拟了社会中伦理和法律框架的演变，从而实现了机器学习系统在部署前和部署后都具有灵活性和适应性的公平性。", "conclusion": "“公平博弈”提供了一个独特且灵活的框架，通过动态的审计和去偏循环，利用强化学习使公平机器学习系统能够适应不断变化的社会环境和公平目标，解决了现有静态公平定义在实际应用中的局限性。"}}
{"id": "2508.06009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06009", "abs": "https://arxiv.org/abs/2508.06009", "authors": ["Jun Feng", "Zixin Wang", "Zhentao Zhang", "Yue Guo", "Zhihan Zhou", "Xiuyi Chen", "Zhenyang Li", "Dawei Yin"], "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models", "comment": "29 pages, 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.", "AI": {"tldr": "本文引入MathReal数据集，评估多模态大语言模型（MLLMs）在真实世界K-12数学图像上的推理能力，发现现有MLLMs在实际教育场景中面临显著挑战。", "motivation": "现有的MLLM数学推理基准主要基于干净或经过处理的多模态输入，未能充分考虑真实世界K-12教育用户手持设备拍摄的图像所固有的复杂性（如图像质量退化、透视变化和无关内容干扰），导致无法准确评估MLLMs在实际应用中的表现。", "method": "构建了MathReal数据集，包含2000道由手持移动设备在真实场景中拍摄的数学问题图像。对真实图像进行了系统分类，划分为3个主要类别和14个子类别（图像质量退化、透视变化、无关内容干扰）。数据集还涵盖5个核心知识和能力类别、3种问题类型以及3个难度级别。设计了6种实验设置，以全面评估当前最先进MLLMs在真实世界场景下的多模态数学推理能力。", "result": "通过广泛实验发现，现有MLLMs在真实教育环境下的问题解决能力受到显著挑战。研究深入分析了它们的性能和错误模式，揭示了模型在识别、理解和推理能力方面的不足。", "conclusion": "现有MLLMs在处理真实世界数学图像时表现不佳，需要进一步改进其识别、理解和推理能力。本研究为未来提升MLLMs在复杂现实场景中的数学推理能力指明了方向。"}}
{"id": "2508.06186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06186", "abs": "https://arxiv.org/abs/2508.06186", "authors": ["Ali Sarabadani", "Maryam Abdollahi Shamami", "Hamidreza Sadeghsalehi", "Borhan Asadi", "Saba Hesaraki"], "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration", "comment": null, "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.", "AI": {"tldr": "DKG-LLM框架结合动态知识图谱与Grok 3大语言模型，通过自适应语义融合算法，实现了高精度的医疗诊断和个性化治疗推荐。", "motivation": "大语言模型在语言处理任务上表现出色，但医疗领域需要更精确和动态的知识整合。本研究旨在利用LLM的强大能力，结合动态知识图谱，解决医疗诊断和治疗推荐中的复杂性和数据噪声问题。", "method": "提出DKG-LLM框架，将动态知识图谱（DKG）与Grok 3大语言模型集成。使用自适应语义融合算法（ASFA）处理异构医疗数据（临床报告、PubMed文章、患者记录），动态生成包含15,964个节点和127,392条边的知识图谱。ASFA利用概率模型、贝叶斯推理和图优化提取语义信息，并动态更新图谱，同时保持可扩展性。使用MIMIC-III和PubMed真实世界数据集进行评估。", "result": "DKG-LLM在诊断准确率上达到84.19%，治疗推荐准确率达到89.63%，语义覆盖率达到93.48%。模型能够处理噪声数据和复杂多症状疾病。", "conclusion": "DKG-LLM是一个可靠且具有变革性的工具，能够有效处理嘈杂数据和复杂多症状疾病，并通过医生反馈进行学习，在医疗诊断和个性化治疗推荐方面表现出色。"}}
{"id": "2508.06177", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06177", "abs": "https://arxiv.org/abs/2508.06177", "authors": ["Dominik Brämer", "Diana Kleingarn", "Oliver Urbann"], "title": "Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor", "comment": "Accepted at 28th RoboCup International Symposium, Salvador, Brasil", "summary": "Accurate localization represents a fundamental challenge in\n  robotic navigation. Traditional methodologies, such as Lidar or QR-code based\nsystems, suffer from inherent scalability and adaptability con straints,\nparticularly in complex environments. In this work, we propose\n  an innovative localization framework that harnesses flooring characteris tics\nby employing graph-based representations and Graph Convolutional\n  Networks (GCNs). Our method uses graphs to represent floor features,\n  which helps localize the robot more accurately (0.64cm error) and more\n  efficiently than comparing individual image features. Additionally, this\n  approach successfully addresses the kidnapped robot problem in every\n  frame without requiring complex filtering processes. These advancements\n  open up new possibilities for robotic navigation in diverse environments.", "AI": {"tldr": "提出一种基于地板特征和图卷积网络（GCN）的机器人定位框架，实现高精度和高效的定位，并能解决“被绑架机器人”问题。", "motivation": "传统的激光雷达或二维码定位方法在复杂环境中存在可扩展性和适应性限制，机器人导航中的精确定位仍是一个基本挑战。", "method": "该方法利用图表示来描述地板特征，并结合图卷积网络（GCN）进行机器人定位。它通过比较图表示而非单个图像特征来提高定位精度和效率。", "result": "该方法实现了0.64厘米的定位误差，比比较单个图像特征更高效。此外，它在每一帧中成功解决了“被绑架机器人”问题，无需复杂的滤波过程。", "conclusion": "这些进展为机器人在各种环境中的导航开辟了新的可能性。"}}
{"id": "2508.06454", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06454", "abs": "https://arxiv.org/abs/2508.06454", "authors": ["Joshua Caiata", "Ben Armstrong", "Kate Larson"], "title": "What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting", "comment": "41 pages", "summary": "Committee-selection problems arise in many contexts and applications, and\nthere has been increasing interest within the social choice research community\non identifying which properties are satisfied by different multi-winner voting\nrules. In this work, we propose a data-driven framework to evaluate how\nfrequently voting rules violate axioms across diverse preference distributions\nin practice, shifting away from the binary perspective of axiom satisfaction\ngiven by worst-case analysis. Using this framework, we analyze the relationship\nbetween multi-winner voting rules and their axiomatic performance under several\npreference distributions. We then show that neural networks, acting as voting\nrules, can outperform traditional rules in minimizing axiom violations. Our\nresults suggest that data-driven approaches to social choice can inform the\ndesign of new voting systems and support the continuation of data-driven\nresearch in social choice.", "AI": {"tldr": "本文提出一个数据驱动框架，评估多赢者投票规则在不同偏好分布下违反公理的频率，并发现神经网络投票规则在最小化公理违反方面优于传统规则。", "motivation": "传统的最坏情况分析对公理满足度提供的是二元视角，无法反映投票规则在实践中违反公理的频率。因此，需要一个数据驱动的方法来评估投票规则在不同偏好分布下的实际表现。", "method": "提出一个数据驱动框架，用于评估投票规则在多样偏好分布下违反公理的频率。利用此框架分析多赢者投票规则及其在多种偏好分布下的公理性能。然后，将神经网络作为投票规则，并评估其性能。", "result": "神经网络作为投票规则时，在最小化公理违反方面表现优于传统规则。", "conclusion": "数据驱动的社会选择方法可以为设计新的投票系统提供信息，并支持社会选择领域数据驱动研究的持续发展。"}}
{"id": "2508.06014", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06014", "abs": "https://arxiv.org/abs/2508.06014", "authors": ["Minsu Kim", "Subin Jeon", "In Cho", "Mijin Yoo", "Seon Joo Kim"], "title": "ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors", "comment": "10 pages, 6 Figures, ICCV 2025", "summary": "Recent advances in novel view synthesis (NVS) have enabled real-time\nrendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle\nwith artifacts and missing regions when rendering from viewpoints that deviate\nfrom the training trajectory, limiting seamless scene exploration. To address\nthis, we propose a 3DGS-based pipeline that generates additional training views\nto enhance reconstruction. We introduce an information-gain-driven virtual\ncamera placement strategy to maximize scene coverage, followed by video\ndiffusion priors to refine rendered results. Fine-tuning 3D Gaussians with\nthese enhanced views significantly improves reconstruction quality. To evaluate\nour method, we present Wild-Explore, a benchmark designed for challenging scene\nexploration. Experiments demonstrate that our approach outperforms existing\n3DGS-based methods, enabling high-quality, artifact-free rendering from\narbitrary viewpoints.\n  https://exploregs.github.io", "AI": {"tldr": "该论文提出了一种基于3D Gaussian Splatting (3DGS) 的新颖视图合成方法，通过生成额外的训练视图并结合信息增益驱动的虚拟相机放置策略和视频扩散先验来提升重建质量，从而解决现有方法在偏离训练轨迹视角下的伪影和缺失区域问题。", "motivation": "现有基于3DGS的新颖视图合成方法在渲染偏离训练轨迹的视角时，会出现伪影和区域缺失，这限制了无缝的场景探索体验。", "method": "该方法提出一个基于3DGS的流程，通过生成额外的训练视图来增强重建。具体包括：1) 引入信息增益驱动的虚拟相机放置策略，以最大化场景覆盖；2) 利用视频扩散先验来优化渲染结果；3) 使用这些增强后的视图对3D高斯进行微调。此外，还提出了Wild-Explore基准来评估该方法。", "result": "实验结果表明，该方法显著提高了重建质量，优于现有的基于3DGS的方法，能够从任意视角实现高质量、无伪影的渲染。", "conclusion": "该研究成功解决了3DGS在偏离训练轨迹视角渲染时的局限性，通过生成和优化额外的训练视图，实现了任意视角的无伪影高质量渲染，提升了场景探索体验。"}}
{"id": "2508.06194", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06194", "abs": "https://arxiv.org/abs/2508.06194", "authors": ["Lai Jiang", "Yuekang Li", "Xiaohan Zhang", "Youtao Ding", "Li Pan"], "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation", "comment": null, "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.", "AI": {"tldr": "该研究提出SceneJailEval，一个创新的场景自适应多维度框架，用于精确评估LLM越狱行为，解决了现有方法在越狱评估中缺乏危害强度量化和场景特异性不匹配的问题。", "motivation": "当前的越狱评估方法存在局限性：二元分类（如字符串匹配、毒性文本分类器）无法量化危害强度，仅提供“是/否”结果；现有多维度框架（如安全违规、相对真实性）采用统一评估标准，导致在不同场景下出现不匹配，降低了评估精度。", "method": "该研究引入了SceneJailEval，其主要贡献包括：1) 一个开创性的场景自适应多维度越狱评估框架，克服了现有方法的“一刀切”限制，并具有强大的可扩展性；2) 一个包含14个场景的综合数据集，涵盖多样化的越狱变体和区域案例，填补了高质量、整体性基准的空白。", "result": "SceneJailEval在全场景数据集上实现了0.917的F1分数（比现有SOTA提升6%），在JBB数据集上实现了0.995的F1分数（比现有SOTA提升3%），超越了现有评估方法在异构场景中的准确性限制，证实了其优势。", "conclusion": "SceneJailEval提供了一种更精确、更具适应性的越狱评估方法，通过场景自适应的多维度框架和全面的数据集，显著提升了LLM越狱评估的性能和精度。"}}
{"id": "2508.06227", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06227", "abs": "https://arxiv.org/abs/2508.06227", "authors": ["Md Sazidur Rahman", "David Cabecinhas", "Ricard Marxer"], "title": "Depth Jitter: Seeing through the Depth", "comment": null, "summary": "Depth information is essential in computer vision, particularly in underwater\nimaging, robotics, and autonomous navigation. However, conventional\naugmentation techniques overlook depth aware transformations, limiting model\nrobustness in real world depth variations. In this paper, we introduce\nDepth-Jitter, a novel depth-based augmentation technique that simulates natural\ndepth variations to improve generalization. Our approach applies adaptive depth\noffsetting, guided by depth variance thresholds, to generate synthetic depth\nperturbations while preserving structural integrity. We evaluate Depth-Jitter\non two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on\nmodel stability under diverse depth conditions. Extensive experiments compare\nDepth-Jitter against traditional augmentation strategies such as ColorJitter,\nanalyzing performance across varying learning rates, encoders, and loss\nfunctions. While Depth-Jitter does not always outperform conventional methods\nin absolute performance, it consistently enhances model stability and\ngeneralization in depth-sensitive environments. These findings highlight the\npotential of depth-aware augmentation for real-world applications and provide a\nfoundation for further research into depth-based learning strategies. The\nproposed technique is publicly available to support advancements in depth-aware\naugmentation. The code is publicly available on\n\\href{https://github.com/mim-team/Depth-Jitter}{github}.", "AI": {"tldr": "提出了一种名为Depth-Jitter的深度感知数据增强技术，通过模拟自然深度变化来提高模型在深度敏感环境中的稳定性和泛化能力。", "motivation": "传统的数据增强技术忽略了深度感知变换，限制了模型在现实世界深度变化中的鲁棒性，而深度信息在水下成像、机器人和自主导航等计算机视觉领域至关重要。", "method": "引入了Depth-Jitter，一种基于深度的新型增强技术。该方法通过深度方差阈值引导，应用自适应深度偏移来生成合成深度扰动，同时保持结构完整性。在FathomNet和UTDAC2020两个基准数据集上进行了评估，并与ColorJitter等传统增强策略进行了比较，分析了其在不同学习率、编码器和损失函数下的性能。", "result": "Depth-Jitter在绝对性能上不总是优于传统方法，但它在深度敏感环境中持续增强了模型的稳定性和泛化能力。", "conclusion": "研究结果突出了深度感知增强在实际应用中的潜力，并为未来基于深度的学习策略研究奠定了基础。所提出的技术已公开可用。"}}
{"id": "2508.06021", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06021", "abs": "https://arxiv.org/abs/2508.06021", "authors": ["Utku Ozbulak", "Michaela Cohrs", "Hristo L. Svilenov", "Joris Vankerschaver", "Wesley De Neve"], "title": "Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis", "comment": null, "summary": "Sub-visible particle analysis using flow imaging microscopy combined with\ndeep learning has proven effective in identifying particle types, enabling the\ndistinction of harmless components such as silicone oil from protein particles.\nHowever, the scarcity of available data and severe imbalance between particle\ntypes within datasets remain substantial hurdles when applying multi-class\nclassifiers to such problems, often forcing researchers to rely on less\neffective methods. The aforementioned issue is particularly challenging for\nparticle types that appear unintentionally and in lower numbers, such as\nsilicone oil and air bubbles, as opposed to protein particles, where obtaining\nlarge numbers of images through controlled settings is comparatively\nstraightforward. In this work, we develop a state-of-the-art diffusion model to\naddress data imbalance by generating high-fidelity images that can augment\ntraining datasets, enabling the effective training of multi-class deep neural\nnetworks. We validate this approach by demonstrating that the generated samples\nclosely resemble real particle images in terms of visual quality and structure.\nTo assess the effectiveness of using diffusion-generated images in training\ndatasets, we conduct large-scale experiments on a validation dataset comprising\n500,000 protein particle images and demonstrate that this approach improves\nclassification performance with no negligible downside. Finally, to promote\nopen research and reproducibility, we publicly release both our diffusion\nmodels and the trained multi-class deep neural network classifiers, along with\na straightforward interface for easy integration into future studies, at\nhttps://github.com/utkuozbulak/svp-generative-ai.", "AI": {"tldr": "该研究开发了一种先进的扩散模型，用于生成亚可见颗粒图像，以解决数据稀缺和类别不平衡问题，从而有效训练多类别深度神经网络进行颗粒类型识别。", "motivation": "流式成像显微镜结合深度学习在亚可见颗粒分析中识别颗粒类型（如区分硅油和蛋白质颗粒）已被证明有效。然而，数据稀缺以及数据集中颗粒类型之间的严重不平衡（特别是对于非故意出现且数量较少的颗粒类型如硅油和气泡）是应用多类别分类器的主要障碍，常常导致研究人员不得不依赖效果较差的方法。", "method": "本研究开发了一个最先进的扩散模型，通过生成高保真图像来扩充训练数据集，从而解决数据不平衡问题，并有效训练多类别深度神经网络。作者通过证明生成的样本在视觉质量和结构上与真实颗粒图像非常相似来验证了该方法。此外，他们发布了扩散模型、训练好的多类别深度神经网络分类器以及易于集成的接口，以促进开放研究和可复现性。", "result": "生成的样本在视觉质量和结构上与真实颗粒图像高度相似。通过在包含500,000张蛋白质颗粒图像的验证数据集上进行大规模实验，证明了使用扩散模型生成的图像来训练数据集可以提高分类性能，且没有明显的负面影响。", "conclusion": "扩散模型能够有效解决亚可见颗粒分析中数据不平衡的挑战，通过生成高质量合成图像来增强训练数据，从而显著提高多类别深度神经网络的分类性能。这项工作为未来的研究提供了可复现的工具和资源。"}}
{"id": "2508.06196", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06196", "abs": "https://arxiv.org/abs/2508.06196", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.", "AI": {"tldr": "该研究提出了一个针对LLM的情商（EI）四层分类法，并引入了EICAP-Bench基准来评估LLM的EI能力。结果显示现有模型在深度情感推理方面存在局限性，仅“评估”层在通用微调中有所改善，强调需要专门的数据和策略来提升LLM的EI。", "motivation": "情商（EI）是开发与人类对齐的大型语言模型（LLM）的关键但未充分探索的维度，现有LLM在情感推理方面存在不足。", "method": "1. 提出了一个心理学基础的、针对LLM的四层EI分类法，包括情感追踪、原因推断、评估和情感适当响应生成。 2. 构建了EICAP-Bench，一个多轮选择题式基准，用于评估开源LLM在不同语言和文化背景下的EI能力。 3. 在EICAP-Bench上评估了六个LLM（LLaMA3、Gemma、Qwen2.5及其指令版本）。 4. 使用LoRA适配器，在UltraChat（UC）数据集上对Qwen2.5-Base和Qwen2.5-Instruct进行了英阿双语微调。 5. 对微调结果进行了统计分析。", "result": "1. Qwen2.5-Instruct被确定为最强的基线模型。 2. 统计分析表明，在五个EI层中，只有“评估”（Appraisal）层通过基于UltraChat的微调获得了显著改善。", "conclusion": "现有预训练和指令微调范式在赋予LLM深度情感推理能力方面存在局限性。为了实现全面的EI对齐，需要专门的目标数据和建模策略。"}}
{"id": "2508.06032", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06032", "abs": "https://arxiv.org/abs/2508.06032", "authors": ["Kiran Chhatre", "Christopher Peters", "Srikrishna Karanam"], "title": "Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts", "comment": "16 pages, 11 figures", "summary": "Existing methods for human parsing into body parts and clothing often use\nfixed mask categories with broad labels that obscure fine-grained clothing\ntypes. Recent open-vocabulary segmentation approaches leverage pretrained\ntext-to-image (T2I) diffusion model features for strong zero-shot transfer, but\ntypically group entire humans into a single person category, failing to\ndistinguish diverse clothing or detailed body parts. To address this, we\npropose Spectrum, a unified network for part-level pixel parsing (body parts\nand clothing) and instance-level grouping. While diffusion-based\nopen-vocabulary models generalize well across tasks, their internal\nrepresentations are not specialized for detailed human parsing. We observe\nthat, unlike diffusion models with broad representations, image-driven 3D\ntexture generators maintain faithful correspondence to input images, enabling\nstronger representations for parsing diverse clothing and body parts. Spectrum\nintroduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --\nobtained by fine-tuning a T2I model on 3D human texture maps -- for improved\nalignment with body parts and clothing. From an input image, we extract\nhuman-part internal features via the I2Tx diffusion model and generate\nsemantically valid masks aligned to diverse clothing categories through\nprompt-guided grounding. Once trained, Spectrum produces semantic segmentation\nmaps for every visible body part and clothing category, ignoring standalone\ngarments or irrelevant objects, for any number of humans in the scene. We\nconduct extensive cross-dataset experiments -- separately assessing body parts,\nclothing parts, unseen clothing categories, and full-body masks -- and\ndemonstrate that Spectrum consistently outperforms baseline methods in\nprompt-based segmentation.", "AI": {"tldr": "Spectrum是一个统一的网络，通过重新利用图像到纹理（I2Tx）扩散模型，实现了对人体部位和精细服装的像素级解析和实例级分组，解决了现有方法在细粒度人体解析上的局限性。", "motivation": "现有的人体解析方法通常使用固定且宽泛的掩码类别，无法区分细粒度服装类型；而最新的开放词汇分割方法虽然利用预训练的文生图（T2I）扩散模型，但通常将整个人体归为单一类别，无法区分多样化的服装或详细身体部位。扩散模型内部表示也未专门针对详细人体解析进行优化。", "method": "提出Spectrum网络，其核心是重新利用一个图像到纹理（I2Tx）扩散模型（通过在3D人体纹理图上微调T2I模型获得）。该模型能够更好地对齐身体部位和服装。从输入图像中，通过I2Tx扩散模型提取人体部位的内部特征，并通过提示引导的接地生成与多样化服装类别对齐的语义有效掩码。", "result": "Spectrum训练后能为场景中任意数量的人生成每个可见身体部位和服装类别的语义分割图，并忽略独立服装或无关物体。在跨数据集实验中，包括身体部位、服装部位、未见过的服装类别和全身掩码的评估，Spectrum在基于提示的分割方面始终优于基线方法。", "conclusion": "Spectrum通过创新性地利用专门为人体部位和服装对齐的I2Tx扩散模型，有效解决了细粒度人体解析的挑战，并在多样化的身体部位和服装分割任务中展现出卓越的性能。"}}
{"id": "2508.06204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06204", "abs": "https://arxiv.org/abs/2508.06204", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "title": "Classification is a RAG problem: A case study on hate speech detection", "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.", "AI": {"tldr": "该研究提出使用检索增强生成（RAG）将传统分类任务转变为基于上下文知识的评估，以实现内容审核中政策的动态适应和鲁棒性。", "motivation": "传统内容审核分类系统在政策演变时需要昂贵的再训练，无法快速适应。研究旨在解决这一问题，使分类系统能灵活响应不断变化的政策。", "method": "引入了基于RAG的分类方法，将分类任务从“这是什么类别？”转变为“这是否违反了政策？”。具体实现为一个名为“上下文政策引擎（CPE）”的代理式RAG系统，该系统在推理时检索相关上下文知识进行评估。", "result": "CPE系统表现出三个主要优势：1) 分类准确性与领先的商业系统相当；2) 通过检索到的政策片段提供固有的可解释性；3) 无需模型再训练即可动态更新政策。实验证明了强大的基线性能，并能通过正确调整对特定身份群体的保护来实现细粒度政策控制，且不影响整体性能。", "conclusion": "研究表明，RAG能够将分类过程转化为一个更灵活、透明和适应性强的过程，适用于内容审核和更广泛的分类问题。"}}
{"id": "2508.06038", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06038", "abs": "https://arxiv.org/abs/2508.06038", "authors": ["Huanyu Wang", "Jushi Kai", "Haoli Bai", "Lu Hou", "Bo Jiang", "Ziwei He", "Zhouhan Lin"], "title": "Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models", "comment": "12 pages, 4 figures", "summary": "Vision-Language Models (VLMs) typically replace the predefined image\nplaceholder token (<image>) in textual instructions with visual features from\nan image encoder, forming the input to a backbone Large Language Model (LLM).\nHowever, the large number of vision tokens significantly increases the context\nlength, leading to high computational overhead and inference latency. While\nprevious efforts mitigate this by selecting only important visual features or\nleveraging learnable queries to reduce token count, they often compromise\nperformance or introduce substantial extra costs. In response, we propose\nFourier-VLM, a simple yet efficient method that compresses visual\nrepresentations in the frequency domain. Our approach is motivated by the\nobservation that vision features output from the vision encoder exhibit\nconcentrated energy in low-frequency components. Leveraging this, we apply a\nlow-pass filter to the vision features using a two-dimentional Discrete Cosine\nTransform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier\nTransform (FFT) operator with a time complexity of $\\mathcal{O}(n\\log n)$,\nminimizing the extra computational cost while introducing no additional\nparameters. Extensive experiments across various image-based benchmarks\ndemonstrate that Fourier-VLM achieves competitive performance with strong\ngeneralizability across both LLaVA and Qwen-VL architectures. Crucially, it\nreduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%\ncompared to LLaVA-v1.5, highlighting the superior efficiency and practicality.", "AI": {"tldr": "Fourier-VLM通过在频域压缩视觉特征，显著降低了视觉语言模型（VLM）的计算开销和推理延迟，同时保持了竞争力。", "motivation": "现有视觉语言模型（VLM）中，大量视觉tokens导致上下文长度增加，计算开销和推理延迟高。尽管有方法尝试减少tokens，但通常会牺牲性能或引入额外成本。", "method": "提出Fourier-VLM，利用视觉编码器输出特征在低频分量中能量集中的特性，通过二维离散余弦变换（DCT）应用低通滤波器来压缩视觉表示。DCT通过快速傅里叶变换（FFT）高效计算，额外计算成本低且不引入额外参数。", "result": "在多个图像基准测试中，Fourier-VLM在LLaVA和Qwen-VL架构上均表现出有竞争力的性能和强大的泛化能力。与LLaVA-v1.5相比，推理FLOPs降低高达83.8%，生成速度提升31.2%。", "conclusion": "Fourier-VLM是一种简单而高效的方法，通过频域压缩显著提升了视觉语言模型的效率和实用性，且不牺牲性能。"}}
{"id": "2508.06033", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06033", "abs": "https://arxiv.org/abs/2508.06033", "authors": ["Yiming Gong", "Zhen Zhu", "Minjia Zhang"], "title": "InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow", "comment": "ICCV 2025", "summary": "We propose a fast text-guided image editing method called InstantEdit based\non the RectifiedFlow framework, which is structured as a few-step editing\nprocess that preserves critical content while following closely to textual\ninstructions. Our approach leverages the straight sampling trajectories of\nRectifiedFlow by introducing a specialized inversion strategy called PerRFI. To\nmaintain consistent while editable results for RectifiedFlow model, we further\npropose a novel regeneration method, Inversion Latent Injection, which\neffectively reuses latent information obtained during inversion to facilitate\nmore coherent and detailed regeneration. Additionally, we propose a\nDisentangled Prompt Guidance technique to balance editability with detail\npreservation, and integrate a Canny-conditioned ControlNet to incorporate\nstructural cues and suppress artifacts. Evaluation on the PIE image editing\ndataset demonstrates that InstantEdit is not only fast but also achieves better\nqualitative and quantitative results compared to state-of-the-art few-step\nediting methods.", "AI": {"tldr": "InstantEdit是一种基于RectifiedFlow框架的快速文本引导图像编辑方法，通过创新性的反演、再生和引导技术，实现了在保持内容一致性的同时，快速高质量地进行图像编辑。", "motivation": "现有文本引导图像编辑方法在速度、内容保持和细节生成方面存在挑战，需要一种既快速又能生成高质量、一致性结果的编辑方案。", "method": "该方法基于RectifiedFlow框架，引入了以下关键技术：1. PerRFI（一种专门的反演策略）利用RectifiedFlow的直线采样轨迹；2. Inversion Latent Injection（反演潜空间注入）用于有效重用反演信息以实现连贯细致的再生；3. Disentangled Prompt Guidance（解耦提示引导）平衡可编辑性和细节保留；4. 整合Canny条件ControlNet引入结构线索并抑制伪影。", "result": "在PIE图像编辑数据集上的评估表明，InstantEdit不仅速度快，而且与最先进的少步编辑方法相比，在定性和定量方面都取得了更好的结果。", "conclusion": "InstantEdit是一种高效且高性能的文本引导图像编辑方法，它通过结合RectifiedFlow的优势和多项创新技术，显著提升了编辑速度和结果质量。"}}
{"id": "2508.06220", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06220", "abs": "https://arxiv.org/abs/2508.06220", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.", "AI": {"tldr": "该研究引入了InfoCausalQA基准，用于评估视觉-语言模型（VLMs）在结合图表和文本的多模态环境中的因果推理能力，结果显示现有VLMs在此方面表现有限，与人类存在显著差距。", "motivation": "尽管视觉-语言模型（VLMs）在感知和推理方面取得了显著进展，但其在多模态情境下的因果推理能力（人类认知核心）仍未被充分探索。", "method": "研究引入了InfoCausalQA基准，包含两个任务：基于推断数值趋势的定量因果推理和涉及五种因果关系（原因、结果、干预、反事实、时间）的语义因果推理。数据集包含494对图表-文本，由GPT-4o生成1482个多项选择QA对，并经人工修订以确保问题需要真正的视觉理解而非表面线索。", "result": "实验结果表明，当前VLMs在计算推理方面能力有限，在语义因果推理方面局限性更明显。与人类相比，它们的表现显著较低，这表明在利用图表信息进行因果推断方面存在巨大差距。", "conclusion": "InfoCausalQA基准突显了当前多模态AI系统在因果推理能力方面的不足，迫切需要提升其在理解和利用基于图表信息进行因果推断的能力。"}}
{"id": "2508.06072", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06072", "abs": "https://arxiv.org/abs/2508.06072", "authors": ["Zijian Chen", "Lirong Deng", "Zhengyu Chen", "Kaiwei Zhang", "Qi Jia", "Yuan Tian", "Yucheng Zhu", "Guangtao Zhai"], "title": "Can Large Models Fool the Eye? A New Turing Test for Biological Animation", "comment": "24 pages, 10 figures", "summary": "Evaluating the abilities of large models and manifesting their gaps are\nchallenging. Current benchmarks adopt either ground-truth-based score-form\nevaluation on static datasets or indistinct textual chatbot-style human\npreferences collection, which may not provide users with immediate, intuitive,\nand perceptible feedback on performance differences. In this paper, we\nintroduce BioMotion Arena, a novel framework for evaluating large language\nmodels (LLMs) and multimodal large language models (MLLMs) via visual\nanimation. Our methodology draws inspiration from the inherent visual\nperception of motion patterns characteristic of living organisms that utilizes\npoint-light source imaging to amplify the performance discrepancies between\nmodels. Specifically, we employ a pairwise comparison evaluation and collect\nmore than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion\nvariants. Data analyses show that the crowd-sourced human votes are in good\nagreement with those of expert raters, demonstrating the superiority of our\nBioMotion Arena in offering discriminative feedback. We also find that over\n90\\% of evaluated models, including the cutting-edge open-source InternVL3 and\nproprietary Claude-4 series, fail to produce fundamental humanoid point-light\ngroups, much less smooth and biologically plausible motions. This enables\nBioMotion Arena to serve as a challenging benchmark for performance\nvisualization and a flexible evaluation framework without restrictions on\nground-truth.", "AI": {"tldr": "本文提出了BioMotion Arena，一个通过视觉动画评估大型语言模型（LLMs）和多模态大型语言模型（MLLMs）的新框架，以直观地展示模型能力差异和不足。", "motivation": "当前的评估基准，无论是基于静态数据集的得分制评估，还是模糊的聊天机器人式人类偏好收集，都无法为用户提供即时、直观、可感知的模型性能差异反馈。", "method": "BioMotion Arena框架利用生物体运动模式的固有视觉感知特性，通过点光源成像来放大模型间的性能差异。该方法采用成对比较评估，收集了53个主流LLM和MLLM在90种生物运动变体上的超过4.5万次众包投票。", "result": "数据分析表明，众包的人类投票与专家评估高度一致，证明了BioMotion Arena在提供判别性反馈方面的优越性。研究还发现，超过90%的被评估模型，包括尖端的开源模型InternVL3和专有模型Claude-4系列，未能生成基本的人形点光源组，更不用说平滑且符合生物学规律的运动。", "conclusion": "BioMotion Arena可以作为一个具有挑战性的性能可视化基准，以及一个灵活且不受限于真实标签的评估框架，揭示了当前LLM和MLLM在生成生物运动方面的显著差距。"}}
{"id": "2508.06036", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06036", "abs": "https://arxiv.org/abs/2508.06036", "authors": ["Jun Xie", "Yingjian Zhu", "Feng Chen", "Zhenghao Zhang", "Xiaohui Fan", "Hongzhu Yi", "Xinming Wang", "Chen Yu", "Yue Bi", "Zhaoran Zhao", "Xiongjun Guan", "Zhepeng Wang"], "title": "More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment", "comment": null, "summary": "In this paper, we present our solution for the semi-supervised learning track\n(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the\nprinciple that \"more is better,\" to construct a robust Mixture of Experts (MoE)\nemotion recognition system. Our approach integrates a diverse range of input\nmodalities as independent experts, including novel signals such as knowledge\nfrom large Vision-Language Models (VLMs) and temporal Action Unit (AU)\ninformation. To effectively utilize unlabeled data, we introduce a\nconsensus-based pseudo-labeling strategy, generating high-quality labels from\nthe agreement between a baseline model and Gemini, which are then used in a\ntwo-stage training paradigm. Finally, we employ a multi-expert voting ensemble\ncombined with a rule-based re-ranking process to correct prediction bias and\nbetter align the outputs with human preferences. Evaluated on the MER2025-SEMI\nchallenge dataset, our method achieves an F1-score of 0.8772 on the test set,\nranking 2nd in the track. Our code is available at\nhttps://github.com/zhuyjan/MER2025-MRAC25.", "AI": {"tldr": "该论文提出了一种基于“越多越好”原则的混合专家（MoE）情感识别系统，用于MER2025半监督学习赛道。该系统结合了多种输入模态、共识伪标签策略和多专家投票集成，在测试集上取得了0.8772的F1分数，排名第二。", "motivation": "为MER2025情感识别半监督学习赛道（MER-SEMI）提供解决方案，构建一个鲁棒的情感识别系统，有效利用未标注数据。", "method": "提出一个基于“越多越好”原则的MoE框架，整合多种输入模态作为独立专家（包括来自大型视觉-语言模型（VLM）的知识和时间动作单元（AU）信息）。引入基于共识的伪标签策略，通过基线模型和Gemini的共识生成高质量标签，并采用两阶段训练范式。最后，使用多专家投票集成结合基于规则的重排序过程来校正预测偏差。", "result": "在MER2025-SEMI挑战数据集的测试集上，该方法取得了0.8772的F1分数，在该赛道中排名第二。", "conclusion": "所提出的多模态混合专家系统结合半监督学习策略，在情感识别任务上表现出色，尤其在利用未标注数据方面效果显著，并达到了竞赛前列水平。"}}
{"id": "2508.06277", "categories": ["cs.CL", "cs.LG", "cs.SD"], "pdf": "https://arxiv.org/pdf/2508.06277", "abs": "https://arxiv.org/abs/2508.06277", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.", "AI": {"tldr": "该研究通过结合微调的Whisper ASR和大型语言模型（LLM）生成的合成数据，显著提升了针对德国老年人语音命令的意图识别性能，并证明了生成式AI在低资源领域弥补数据差距的有效性。", "motivation": "现有的语音命令意图识别方法主要局限于短命令且多为英语，对于德国老年人语音等低资源领域存在显著局限性，因此需要一种能克服这些限制的新方法。", "method": "提出了一种结合方法：使用在德国老年人语音（SVC-de）上微调的Whisper ASR模型，并结合基于Transformer的语言模型。这些语言模型在由LeoLM、Llama3和ChatGPT等大型语言模型生成的合成文本数据集上进行训练。通过文本到语音模型生成合成语音，并进行广泛的跨数据集测试以评估鲁棒性。", "result": "研究结果表明，由LLM生成的合成数据显著提升了分类性能，并增强了对不同说话风格和未见词汇的鲁棒性。值得注意的是，较小的、特定领域的13B LLM LeoLM在德国意图识别的数据质量方面超越了更大的175B ChatGPT。", "conclusion": "该研究证明了生成式AI能够有效弥补低资源领域的数据鸿沟。所提出的方法及其数据生成和训练过程均提供了详细文档，确保了透明度和可复现性。"}}
{"id": "2508.06076", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06076", "abs": "https://arxiv.org/abs/2508.06076", "authors": ["Michael Wehrli", "Alicia Durrer", "Paul Friedrich", "Sidaty El Hadramy", "Edwin Li", "Luana Brahaj", "Carol C. Hasler", "Philippe C. Cattin"], "title": "Towards MR-Based Trochleoplasty Planning", "comment": "Accepted at MICCAI COLAS Workshop 2025. Code:\n  https://wehrlimi.github.io/sr-3d-planning/", "summary": "To treat Trochlear Dysplasia (TD), current approaches rely mainly on\nlow-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.\nThe surgeries are planned based on surgeons experience, have limited adoption\nof minimally invasive techniques, and lead to inconsistent outcomes. We propose\na pipeline that generates super-resolved, patient-specific 3D pseudo-healthy\ntarget morphologies from conventional clinical MR scans. First, we compute an\nisotropic super-resolved MR volume using an Implicit Neural Representation\n(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label\ncustom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to\ngenerate pseudo-healthy target morphologies of the trochlear region. In\ncontrast to prior work producing pseudo-healthy low-resolution 3D MR images,\nour approach enables the generation of sub-millimeter resolved 3D shapes\ncompatible for pre- and intraoperative use. These can serve as preoperative\nblueprints for reshaping the femoral groove while preserving the native patella\narticulation. Furthermore, and in contrast to other work, we do not require a\nCT for our pipeline - reducing the amount of radiation. We evaluated our\napproach on 25 TD patients and could show that our target morphologies\nsignificantly improve the sulcus angle (SA) and trochlear groove depth (TGD).\nThe code and interactive visualization are available at\nhttps://wehrlimi.github.io/sr-3d-planning/.", "AI": {"tldr": "该研究提出了一种基于隐式神经表示（INR）和小波扩散模型（WDM）的管道，用于从常规临床MR扫描生成超分辨、患者特异性的三维伪健康滑车形态，以改进滑车发育不良（TD）的治疗规划。", "motivation": "目前滑车发育不良（TD）的治疗主要依赖低分辨率临床MR扫描和手术直觉，导致手术规划基于经验、微创技术应用有限且结果不一致。研究旨在提供一种更精确、个性化的术前规划工具。", "method": "1. 使用隐式神经表示（INR）计算各向同性超分辨MR体积。2. 使用定制的多标签网络分割股骨、胫骨、髌骨和腓骨。3. 训练小波扩散模型（WDM）生成滑车区域的伪健康目标形态。该方法能生成亚毫米分辨率的三维形状，且无需CT扫描。", "result": "该方法能够生成亚毫米分辨率的三维形状，可用于术前和术中规划，作为重塑股骨沟的蓝图，同时保留髌骨的自然关节。与现有工作相比，该方法无需CT，减少了辐射。在25名TD患者上评估，结果显示目标形态显著改善了股骨沟角（SA）和滑车沟深度（TGD）。", "conclusion": "所提出的管道能够从MR扫描生成超分辨、患者特异性的三维伪健康滑车形态，为滑车发育不良的治疗提供了精确的术前蓝图，有望提高手术一致性和结果，同时减少患者的辐射暴露。"}}
{"id": "2508.06044", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06044", "abs": "https://arxiv.org/abs/2508.06044", "authors": ["Huimin Wu", "Xiaojian Ma", "Haozhe Zhao", "Yanpeng Zhao", "Qing Li"], "title": "NEP: Autoregressive Image Editing via Next Editing Token Prediction", "comment": "The project page is: https://nep-bigai.github.io/", "summary": "Text-guided image editing involves modifying a source image based on a\nlanguage instruction and, typically, requires changes to only small local\nregions. However, existing approaches generate the entire target image rather\nthan selectively regenerate only the intended editing areas. This results in\n(1) unnecessary computational costs and (2) a bias toward reconstructing\nnon-editing regions, which compromises the quality of the intended edits. To\nresolve these limitations, we propose to formulate image editing as Next\nEditing-token Prediction (NEP) based on autoregressive image generation, where\nonly regions that need to be edited are regenerated, thus avoiding unintended\nmodification to the non-editing areas. To enable any-region editing, we propose\nto pre-train an any-order autoregressive text-to-image (T2I) model. Once\ntrained, it is capable of zero-shot image editing and can be easily adapted to\nNEP for image editing, which achieves a new state-of-the-art on widely used\nimage editing benchmarks. Moreover, our model naturally supports test-time\nscaling (TTS) through iteratively refining its generation in a zero-shot\nmanner. The project page is: https://nep-bigai.github.io/", "AI": {"tldr": "该论文提出了一种名为“下一编辑令牌预测（NEP）”的图像编辑方法，通过仅重新生成需要编辑的区域来解决现有方法效率低和编辑质量受损的问题，并实现了零样本编辑和迭代优化。", "motivation": "现有文本引导图像编辑方法会生成整个目标图像，而非选择性地仅重新生成编辑区域。这导致了不必要的计算成本，并对非编辑区域的重建产生偏差，从而损害了预期编辑的质量。", "method": "将图像编辑表述为基于自回归图像生成的“下一编辑令牌预测（NEP）”，仅重新生成需要编辑的区域，避免对非编辑区域的意外修改。为此，预训练了一个任意顺序的自回归文本到图像（T2I）模型，使其能够进行零样本图像编辑，并可轻松适应NEP。", "result": "该模型在广泛使用的图像编辑基准上实现了新的最先进性能。此外，模型自然支持通过零样本方式迭代细化其生成的测试时间缩放（TTS）功能。", "conclusion": "所提出的NEP方法通过仅再生编辑区域，有效解决了现有图像编辑方法的计算效率低下和编辑质量受损问题。预训练的任意顺序T2I模型支持零样本编辑和测试时间缩放，显著提升了图像编辑的性能和实用性。"}}
{"id": "2508.06309", "categories": ["cs.CL", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.06309", "abs": "https://arxiv.org/abs/2508.06309", "authors": ["Ruichong Zhang"], "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "comment": null, "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.", "AI": {"tldr": "本文提出了一种名为MDIR（Matrix-Driven Instant Review）的新方法，利用矩阵分析和大偏差理论，有效且高效地检测大型语言模型（LLM）的抄袭行为，解决了现有方法在权重对应重建、统计显著性计算和误报方面的不足。", "motivation": "近年来，大型语言模型（LLM）的知识产权（IP）问题日益突出。抄袭其他LLM并声称原创，未正确归因，可能对原始开发者造成严重的经济和声誉损害。然而，现有的LLM抄袭检测方法存在局限性，包括无法准确重建权重对应关系、缺乏p值等统计显著性度量，以及可能错误地将基于相似数据训练的模型标记为相关。", "method": "本文提出MDIR方法，该方法利用矩阵分析和大偏差理论。MDIR专注于权重相似性，无需完整的模型推理，能够准确重建权重关系并提供严格的p值估计。", "result": "实验结果表明，MDIR即使在经过大量转换（如随机置换和数万亿token的持续预训练）后，也能可靠地检测抄袭。此外，所有检测都可以在单个PC上在一小时内完成，证明MDIR既高效又易于使用。", "conclusion": "MDIR是一种有效且高效的LLM抄袭检测方法，它通过解决现有方法的关键局限性，为保护LLM的知识产权提供了可行的解决方案，即使在模型经过复杂转换后也能准确检测抄袭。"}}
{"id": "2508.06107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06107", "abs": "https://arxiv.org/abs/2508.06107", "authors": ["Shree Mitra", "Ritabrata Chakraborty", "Nilkanta Sahu"], "title": "Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention", "comment": null, "summary": "Recognizing handwritten mathematical expressions (HMER) is a challenging task\ndue to the inherent two-dimensional structure, varying symbol scales, and\ncomplex spatial relationships among symbols. In this paper, we present a\nself-supervised learning (SSL) framework for HMER that eliminates the need for\nexpensive labeled data. Our approach begins by pretraining an image encoder\nusing a combination of global and local contrastive loss, enabling the model to\nlearn both holistic and fine-grained representations. A key contribution of\nthis work is a novel self-supervised attention network, which is trained using\na progressive spatial masking strategy. This attention mechanism is designed to\nlearn semantically meaningful focus regions, such as operators, exponents, and\nnested mathematical notation, without requiring any supervision. The\nprogressive masking curriculum encourages the network to become increasingly\nrobust to missing or occluded visual information, ultimately improving\nstructural understanding. Our complete pipeline consists of (1) self-supervised\npretraining of the encoder, (2) self-supervised attention learning, and (3)\nsupervised fine-tuning with a transformer decoder to generate LATEX sequences.\nExtensive experiments on CROHME benchmarks demonstrate that our method\noutperforms existing SSL and fully supervised baselines, validating the\neffectiveness of our progressive attention mechanism in enhancing HMER\nperformance. Our codebase can be found here.", "AI": {"tldr": "提出一个自监督学习（SSL）框架用于手写数学表达式识别（HMER），通过预训练编码器和新颖的自监督注意力网络，有效减少对昂贵标注数据的依赖并提升性能。", "motivation": "手写数学表达式识别（HMER）面临二维结构、符号尺度变化和复杂空间关系等挑战，且需要大量标注数据，本研究旨在消除对昂贵标注数据的需求。", "method": "1. 使用全局和局部对比损失自监督预训练图像编码器，学习整体和细粒度表示。2. 提出一种新颖的自监督注意力网络，通过渐进式空间遮蔽策略学习语义焦点区域（如运算符、指数等）。3. 使用Transformer解码器进行有监督微调以生成LATEX序列。", "result": "在CROHME基准测试中，该方法优于现有自监督和全监督基线，验证了渐进式注意力机制在增强HMER性能方面的有效性。", "conclusion": "所提出的自监督学习框架，特别是其渐进式注意力机制，能有效提升手写数学表达式识别的性能，并显著减少对大量标注数据的依赖。"}}
{"id": "2508.06051", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06051", "abs": "https://arxiv.org/abs/2508.06051", "authors": ["Linhan Cao", "Wei Sun", "Weixia Zhang", "Xiangyang Zhu", "Jun Jia", "Kaiwei Zhang", "Dandan Zhu", "Guangtao Zhai", "Xiongkuo Min"], "title": "VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning", "comment": null, "summary": "Video quality assessment (VQA) aims to objectively quantify perceptual\nquality degradation in alignment with human visual perception. Despite recent\nadvances, existing VQA models still suffer from two critical limitations:\n\\textit{poor generalization to out-of-distribution (OOD) videos} and\n\\textit{limited explainability}, which restrict their applicability in\nreal-world scenarios. To address these challenges, we propose\n\\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large\nmultimodal models (LMMs) with reinforcement learning to jointly model video\nquality understanding and scoring, emulating human perceptual decision-making.\nSpecifically, we adopt group relative policy optimization (GRPO), a rule-guided\nreinforcement learning algorithm that enables reasoning over video quality\nunder score-level supervision, and introduce three VQA-specific rewards: (1) a\n\\textbf{bell-shaped regression reward} that increases rapidly as the prediction\nerror decreases and becomes progressively less sensitive near the ground truth;\n(2) a \\textbf{pairwise ranking reward} that guides the model to correctly\ndetermine the relative quality between video pairs; and (3) a \\textbf{temporal\nconsistency reward} that encourages the model to prefer temporally coherent\nvideos over their perturbed counterparts. Extensive experiments demonstrate\nthat VQAThinker achieves state-of-the-art performance on both in-domain and OOD\nVQA benchmarks, showing strong generalization for video quality scoring.\nFurthermore, evaluations on video quality understanding tasks validate its\nsuperiority in distortion attribution and quality description compared to\nexisting explainable VQA models and LMMs. These findings demonstrate that\nreinforcement learning offers an effective pathway toward building\ngeneralizable and explainable VQA models solely with score-level supervision.", "AI": {"tldr": "VQAThinker是一个基于推理的视频质量评估（VQA）框架，利用大型多模态模型（LMMs）和强化学习来解决现有VQA模型泛化能力差和可解释性有限的问题，通过引入特定奖励机制，在域内和域外VQA任务上均达到了最先进的性能，并增强了质量理解能力。", "motivation": "现有的视频质量评估（VQA）模型存在两个主要局限性：对域外（OOD）视频的泛化能力差，以及可解释性有限，这限制了它们在实际场景中的应用。", "method": "本文提出了VQAThinker，一个基于推理的VQA框架，它利用大型多模态模型（LMMs）和强化学习来联合建模视频质量理解和评分，以模拟人类的感知决策过程。具体而言，它采用了规则引导的强化学习算法——组相对策略优化（GRPO），并在分数级别监督下进行推理。此外，引入了三个VQA特定的奖励机制：1) 钟形回归奖励，预测误差减小时快速增加，接近真实值时敏感度降低；2) 成对排序奖励，指导模型正确判断视频对的相对质量；3) 时间一致性奖励，鼓励模型偏好时间连贯的视频。", "result": "VQAThinker在域内和域外VQA基准测试上均取得了最先进的性能，显示出强大的视频质量评分泛化能力。此外，在视频质量理解任务上的评估表明，与现有可解释VQA模型和LMMs相比，VQAThinker在失真归因和质量描述方面表现出优越性。", "conclusion": "研究结果表明，强化学习为仅通过分数级监督构建可泛化和可解释的VQA模型提供了一条有效途径。"}}
{"id": "2508.06345", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06345", "abs": "https://arxiv.org/abs/2508.06345", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy", "AI": {"tldr": "针对大型多模态模型（LMMs）在图问答（Graph QA）中单一图表示形式的不足，本文提出了DynamicTRF框架。该框架通过动态选择最合适的图拓扑表示形式（TRF），显著提升了零样本图问答的准确性和简洁性。", "motivation": "现有LMMs在图问答任务中通常只使用单一类型的图表示形式（TRF），这种“一刀切”的方法未能考虑不同模型或任务的特定偏好，导致回答不准确或冗长。", "method": "首先，分析现有TRF的特点和弱点，并设计一套为零样本图问答定制的TRF集合（$F_{ZS}$）。其次，引入新的度量标准“图响应效率（GRE）”来衡量性能与简洁性的平衡。在此基础上，开发了DynamicTRF框架：构建一个TRF偏好（TRFP）数据集，根据GRE分数对TRF进行排名以探究问题特定的TRF偏好；然后，在该数据集上训练一个TRF路由器，在推理阶段为每个问题自适应地分配最佳TRF。", "result": "在7个域内算法图问答任务和2个域外下游任务上的大量实验表明，DynamicTRF显著提高了LMMs在零样本图问答方面的准确性。", "conclusion": "DynamicTRF框架通过动态选择和适应性地分配最佳图拓扑表示形式，有效解决了LMMs在零样本图问答中单一表示形式的局限性，从而在提高准确性的同时兼顾了响应的简洁性。"}}
{"id": "2508.06109", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06109", "abs": "https://arxiv.org/abs/2508.06109", "authors": ["Zhibo Zhu", "Renyu Huang", "Lei He"], "title": "FMCE-Net++: Feature Map Convergence Evaluation and Training", "comment": null, "summary": "Deep Neural Networks (DNNs) face interpretability challenges due to their\nopaque internal representations. While Feature Map Convergence Evaluation\n(FMCE) quantifies module-level convergence via Feature Map Convergence Scores\n(FMCS), it lacks experimental validation and closed-loop integration. To\naddress this limitation, we propose FMCE-Net++, a novel training framework that\nintegrates a pretrained, frozen FMCE-Net as an auxiliary head. This module\ngenerates FMCS predictions, which, combined with task labels, jointly supervise\nbackbone optimization through a Representation Auxiliary Loss. The RAL\ndynamically balances the primary classification loss and feature convergence\noptimization via a tunable \\Representation Abstraction Factor. Extensive\nexperiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100\ndemonstrate that FMCE-Net++ consistently enhances model performance without\narchitectural modifications or additional data. Key experimental outcomes\ninclude accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp\n(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate\nstate-of-the-art performance ceilings.", "AI": {"tldr": "FMCE-Net++是一种新型训练框架，通过引入辅助头来监督特征图收敛，从而在不修改网络架构或增加数据的情况下提升深度神经网络的性能。", "motivation": "深度神经网络因其不透明的内部表示而面临可解释性挑战。现有的特征图收敛评估（FMCE）方法缺乏实验验证和闭环集成。", "method": "提出FMCE-Net++框架，将一个预训练且冻结的FMCE-Net作为辅助头集成。该模块生成特征图收敛分数（FMCS），这些分数与任务标签结合，通过表示辅助损失（RAL）共同监督主干网络的优化。RAL通过一个可调的“表示抽象因子”动态平衡主分类损失和特征收敛优化。", "result": "在MNIST、CIFAR-10、FashionMNIST和CIFAR-100数据集上的广泛实验表明，FMCE-Net++持续提升模型性能，无需修改架构或额外数据。关键实验结果包括ResNet-50/CIFAR-10上准确率提升1.16个百分点，ShuffleNet v2/CIFAR-100上提升1.08个百分点。", "conclusion": "FMCE-Net++通过有效整合特征图收敛监督，能够提升现有最先进模型的性能上限。"}}
{"id": "2508.06055", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06055", "abs": "https://arxiv.org/abs/2508.06055", "authors": ["Wonjung Park", "Suhyun Ahn", "Jinah Park"], "title": "LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing", "comment": null, "summary": "Lateral ventricle (LV) shape analysis holds promise as a biomarker for\nneurological diseases; however, challenges remain due to substantial shape\nvariability across individuals and segmentation difficulties arising from\nlimited MRI resolution. We introduce LV-Net, a novel framework for producing\nindividualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint\nLV-hippocampus template mesh. By incorporating anatomical relationships\nembedded within the joint template, LV-Net reduces boundary segmentation\nartifacts and improves reconstruction robustness. In addition, by classifying\nthe vertices of the template mesh based on their anatomical adjacency, our\nmethod enhances point correspondence across subjects, leading to more accurate\nLV shape statistics. We demonstrate that LV-Net achieves superior\nreconstruction accuracy, even in the presence of segmentation imperfections,\nand delivers more reliable shape descriptors across diverse datasets. Finally,\nwe apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that\nshow significantly associations with the disease relative to cognitively normal\ncontrols. The codes for LV shape modeling are available at\nhttps://github.com/PWonjung/LV_Shape_Modeling.", "AI": {"tldr": "LV-Net是一个新颖的框架，通过形变联合LV-海马模板网格，从脑部MRI生成个体化的3D侧脑室（LV）网格，旨在提高LV形状分析的准确性和鲁棒性，并应用于阿尔茨海默病研究。", "motivation": "侧脑室（LV）形状分析作为神经系统疾病的生物标志物具有前景，但存在个体间形状差异大以及MRI分辨率限制导致的分割困难等挑战。", "method": "LV-Net通过形变一个解剖学感知的联合LV-海马模板网格来生成个体化的3D LV网格。它通过整合联合模板中嵌入的解剖关系来减少边界分割伪影并提高重建鲁棒性。此外，通过根据解剖邻近性对模板网格的顶点进行分类，增强了不同受试者间的点对应，从而获得更准确的LV形状统计。", "result": "LV-Net即使在分割不完善的情况下也能实现卓越的重建精度，并在不同数据集中提供更可靠的形状描述符。将其应用于阿尔茨海默病分析时，LV-Net识别出与认知正常对照组相比，与疾病显著相关的LV亚区域。", "conclusion": "LV-Net是一个有效且鲁棒的框架，用于从脑部MRI重建个体化3D侧脑室网格并进行形状分析，能够克服传统方法的挑战，并在神经系统疾病（如阿尔茨海默病）的生物标志物发现中显示出潜力。"}}
{"id": "2508.06360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06360", "abs": "https://arxiv.org/abs/2508.06360", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.", "AI": {"tldr": "本研究探索将攻击性检测作为辅助任务，通过丰富提示词管道方法，显著提升大型语言模型在网络欺凌检测中的性能和泛化能力。", "t": "本研究探索将攻击性检测作为辅助任务，通过丰富提示词管道方法，显著提升大型语言模型在网络欺凌检测中的性能和泛化能力。", "motivation": "由于网络欺凌表达方式的微妙性和多样性，在社交媒体上检测网络欺凌仍然是一个严峻挑战。", "method": "研究采用指令调优的大型语言模型，在五个攻击性数据集和一个网络欺凌数据集上进行实验。评估了零样本、少样本、独立LoRA微调和多任务学习（MTL）等策略。针对MTL结果的不一致性，提出了一种“丰富提示词管道”方法，将攻击性预测嵌入到网络欺凌检测的提示词中以提供上下文增强。", "result": "初步结果显示，丰富提示词管道方法持续优于标准的LoRA微调，表明攻击性信息上下文显著提升了网络欺凌检测的性能。", "conclusion": "本研究强调了辅助任务（如攻击性检测）在提高大型语言模型在社交网络安全关键应用中泛化能力的潜力。"}}
{"id": "2508.06136", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06136", "abs": "https://arxiv.org/abs/2508.06136", "authors": ["YoungChan Choi", "HengFei Wang", "YiHua Cheng", "Boeun Kim", "Hyung Jin Chang", "YoungGeun Choi", "Sang-Il Choi"], "title": "Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation", "comment": "9 pages, 5 figures, ACM Multimeida 2025 accepted", "summary": "We propose a novel 3D gaze redirection framework that leverages an explicit\n3D eyeball structure. Existing gaze redirection methods are typically based on\nneural radiance fields, which employ implicit neural representations via volume\nrendering. Unlike these NeRF-based approaches, where the rotation and\ntranslation of 3D representations are not explicitly modeled, we introduce a\ndedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian\nSplatting (3DGS). Our method generates photorealistic images that faithfully\nreproduce the desired gaze direction by explicitly rotating and translating the\n3D eyeball structure. In addition, we propose an adaptive deformation module\nthat enables the replication of subtle muscle movements around the eyes.\nThrough experiments conducted on the ETH-XGaze dataset, we demonstrate that our\nframework is capable of generating diverse novel gaze images, achieving\nsuperior image quality and gaze estimation accuracy compared to previous\nstate-of-the-art methods.", "AI": {"tldr": "本文提出一种新颖的3D凝视重定向框架，利用显式3D眼球结构和3D Gaussian Splatting，并通过自适应变形模块模拟肌肉运动，生成逼真且凝视方向准确的图像。", "motivation": "现有的基于神经辐射场（NeRF）的凝视重定向方法，其3D表示（如旋转和平移）是隐式建模的，未能显式地表示眼球的3D结构，导致生成图像的真实性和凝视精度受限。", "method": "该方法引入了一个专用的显式3D眼球结构，使用3D Gaussian Splatting (3DGS) 来表示眼球。通过显式地旋转和翻译3D眼球结构来实现凝视重定向。此外，还提出了一个自适应变形模块，以复制眼睛周围细微的肌肉运动。", "result": "在ETH-XGaze数据集上的实验表明，该框架能够生成多样化的新颖凝视图像，实现了卓越的图像质量和凝视估计准确性，优于以往的最新方法。", "conclusion": "所提出的基于显式3D眼球结构和自适应变形模块的3D凝视重定向框架，能够生成高质量、高精度的凝视图像，显著优于现有的隐式神经表示方法。"}}
{"id": "2508.06057", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06057", "abs": "https://arxiv.org/abs/2508.06057", "authors": ["Mojtaba Valipour", "Kelly Zheng", "James Lowman", "Spencer Szabados", "Mike Gartner", "Bobby Braswell"], "title": "AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?", "comment": "Accepted in IGARSS 2025!", "summary": "Artificial General Intelligence (AGI) is closer than ever to becoming a\nreality, sparking widespread enthusiasm in the research community to collect\nand work with various modalities, including text, image, video, and audio.\nDespite recent efforts, satellite spectral imagery, as an additional modality,\nhas yet to receive the attention it deserves. This area presents unique\nchallenges, but also holds great promise in advancing the capabilities of AGI\nin understanding the natural world. In this paper, we argue why Earth\nObservation data is useful for an intelligent model, and then we review\nexisting benchmarks and highlight their limitations in evaluating the\ngeneralization ability of foundation models in this domain. This paper\nemphasizes the need for a more comprehensive benchmark to evaluate earth\nobservation models. To facilitate this, we propose a comprehensive set of tasks\nthat a benchmark should encompass to effectively assess a model's ability to\nunderstand and interact with Earth observation data.", "AI": {"tldr": "本文强调卫星光谱图像作为多模态数据对通用人工智能（AGI）的重要性，指出现有地球观测（EO）基准的局限性，并提出了一个更全面的EO基准任务集。", "motivation": "通用人工智能（AGI）的进步需要处理多种模态数据，但卫星光谱图像这一重要模态尚未得到充分关注。现有地球观测（EO）基准在评估基础模型的泛化能力方面存在局限性，阻碍了AGI在理解自然世界方面的能力提升。", "method": "本文首先论证了地球观测数据对智能模型的实用性；其次，回顾了现有基准并指出了它们在评估基础模型泛化能力方面的不足；最后，提出了一个包含全面任务的基准，以有效评估模型理解和处理地球观测数据的能力。", "result": "研究强调了地球观测数据对智能模型的价值，揭示了当前地球观测基准在评估基础模型泛化能力方面的局限性，并提出了一个更全面的基准任务集，以促进该领域模型能力的评估和发展。", "conclusion": "卫星光谱图像是发展AGI理解自然世界能力的关键但未被充分利用的模态。为有效评估模型对地球观测数据的理解和交互能力，迫切需要一个更全面、多任务的地球观测基准。"}}
{"id": "2508.06374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06374", "abs": "https://arxiv.org/abs/2508.06374", "authors": ["Anubhav Jangra", "Bahareh Sarrafzadeh", "Adrian de Wynter", "Silviu Cucerzan", "Sujay Kumar Jauhar"], "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions", "comment": null, "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.", "AI": {"tldr": "本文探讨了低资源作者风格个性化文本生成的评估方法，质疑了传统度量标准的有效性，并提出了采用风格嵌入和LLM作为评判者的集成评估范式。", "motivation": "现有研究在低资源作者风格个性化文本生成评估方面探索有限，且广泛采用的BLEU和ROUGE等评估指标的有效性存疑。", "method": "研究探索了风格嵌入和“LLM作为评判者”等评估范式，并在一个涵盖八项写作任务和三种设置（领域判别、作者归因、LLM个性化与非个性化判别）的风格判别基准上，评估了这些指标及其集成。", "result": "研究提供了确凿证据，表明应采用多样化评估指标的集成方法来有效评估风格个性化文本生成。", "conclusion": "为有效评估风格个性化文本生成，应采用多种评估指标的集成方法。"}}
{"id": "2508.06169", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06169", "abs": "https://arxiv.org/abs/2508.06169", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Changting Lin", "Jianfeng Dong", "Chaochao Chen", "Xun Zhou", "Meng Han"], "title": "UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting", "comment": null, "summary": "Underwater 3D scene reconstruction faces severe challenges from light\nabsorption, scattering, and turbidity, which degrade geometry and color\nfidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF\nextensions such as SeaThru-NeRF incorporate physics-based models, their MLP\nreliance limits efficiency and spatial resolution in hazy environments. We\nintroduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for\nrobust underwater reconstruction. Key innovations include: (1) a plug-and-play\nlearnable underwater image formation module using voxel-based regression for\nspatially varying attenuation and backscatter; and (2) a Physics-Aware\nUncertainty Pruning (PAUP) branch that adaptively removes noisy floating\nGaussians via uncertainty scoring, ensuring artifact-free geometry. The\npipeline operates in training and rendering stages. During training, noisy\nGaussians are optimized end-to-end with underwater parameters, guided by PAUP\npruning and scattering modeling. In rendering, refined Gaussians produce clean\nUnattenuated Radiance Images (URIs) free from media effects, while learned\nphysics enable realistic Underwater Images (UWIs) with accurate light\ntransport. Experiments on SeaThru-NeRF and UWBundle datasets show superior\nperformance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on\nSeaThru-NeRF, with ~65% reduction in floating artifacts.", "AI": {"tldr": "该论文提出了UW-3DGS，一个基于3D高斯散射（3DGS）的新框架，用于鲁棒的水下3D场景重建。它通过引入可学习的水下图像形成模块和物理感知不确定性剪枝（PAUP）分支，有效克服了水下光照退化和传统方法（如NeRF）的局限性，显著提高了重建质量并减少了浮动伪影。", "motivation": "水下3D场景重建面临光吸收、散射和浊度带来的严峻挑战，这些因素会严重损害传统方法（如神经辐射场NeRF）的几何和色彩保真度。尽管NeRF的扩展（如SeaThru-NeRF）融入了基于物理的模型，但它们对MLP的依赖限制了在浑浊环境中的效率和空间分辨率。", "method": "本文提出了UW-3DGS框架，该框架将3D Gaussian Splatting (3DGS) 适应于水下重建。主要创新包括：1) 一个即插即用的可学习水下图像形成模块，使用基于体素的回归来处理空间变化的衰减和反向散射；2) 一个物理感知不确定性剪枝（PAUP）分支，通过不确定性评分自适应地移除嘈杂的浮动高斯，确保无伪影的几何形状。该流程在训练和渲染阶段运行：训练时，嘈杂的高斯与水下参数进行端到端优化，并由PAUP剪枝和散射建模引导；渲染时，精炼后的高斯生成无介质效应的干净未衰减辐射图像（URIs），同时学习到的物理模型能生成具有准确光传输的逼真水下图像（UWIs）。", "result": "在SeaThru-NeRF和UWBundle数据集上的实验表明，UW-3DGS表现优越。在SeaThru-NeRF数据集上，PSNR达到27.604，SSIM达到0.868，LPIPS达到0.104，同时浮动伪影减少了约65%。", "conclusion": "UW-3DGS通过将3DGS与创新的水下图像形成模块和物理感知不确定性剪枝相结合，成功解决了水下3D场景重建中的关键挑战，显著提升了几何和色彩保真度，减少了伪影，并实现了更高效和鲁棒的水下重建。"}}
{"id": "2508.06058", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06058", "abs": "https://arxiv.org/abs/2508.06058", "authors": ["Shiyang Zhou", "Haijin Zeng", "Yunfan Lu", "Yongyong Chen", "Jie Liu", "Jingyong Su"], "title": "Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention", "comment": null, "summary": "Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera\ncapture brightness changes as asynchronous \"events\" instead of frames, offering\nadvanced application on mobile photography. However, challenges arise from\ncombining a Quad Bayer Color Filter Array (CFA) sensor with event pixels\nlacking color information, resulting in aliasing and artifacts on the\ndemosaicing process before downstream application. Current methods struggle to\naddress these issues, especially on resource-limited mobile devices. In\nresponse, we introduce \\textbf{TSANet}, a lightweight \\textbf{T}wo-stage\nnetwork via \\textbf{S}tate space augmented cross-\\textbf{A}ttention, which can\nhandle event pixels inpainting and demosaicing separately, leveraging the\nbenefits of dividing complex tasks into manageable subtasks. Furthermore, we\nintroduce a lightweight Cross-Swin State Block that uniquely utilizes\npositional prior for demosaicing and enhances global dependencies through the\nstate space model with linear complexity. In summary, TSANet demonstrates\nexcellent demosaicing performance on both simulated and real data of HybridEVS\nwhile maintaining a lightweight model, averaging better results than the\nprevious state-of-the-art method DemosaicFormer across seven diverse datasets\nin both PSNR and SSIM, while respectively reducing parameter and computation\ncosts by $1.86\\times$ and $3.29\\times$. Our approach presents new possibilities\nfor efficient image demosaicing on mobile devices. Code is available in the\nsupplementary materials.", "AI": {"tldr": "本文提出了TSANet，一个轻量级的两阶段网络，用于解决混合事件视觉传感器（HybridEVS）相机中事件像素的去马赛克问题，尤其适用于资源受限的移动设备。", "motivation": "HybridEVS相机结合了Quad Bayer彩色滤光阵列（CFA）传感器与缺乏颜色信息的事件像素，导致在去马赛克过程中产生混叠和伪影。现有方法难以在资源有限的移动设备上有效解决这些问题。", "method": "引入TSANet，一个轻量级两阶段网络，通过状态空间增强的交叉注意力机制，将事件像素修复和去马赛克任务分开处理。此外，设计了轻量级Cross-Swin状态块，利用位置先验进行去马赛克，并通过具有线性复杂度的状态空间模型增强全局依赖性。", "result": "TSANet在模拟和真实HybridEVS数据上均表现出卓越的去马赛克性能，在七个多样化数据集上，PSNR和SSIM均优于现有最佳方法DemosaicFormer，同时参数和计算成本分别降低了1.86倍和3.29倍。", "conclusion": "该方法为移动设备上高效的图像去马赛克提供了新的可能性。"}}
{"id": "2508.06388", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06388", "abs": "https://arxiv.org/abs/2508.06388", "authors": ["Lanlan Qiu", "Xiao Pu", "Yeqi Feng", "Tianxing He"], "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing", "comment": "21 pages, 17 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.", "AI": {"tldr": "本研究旨在弥合大型语言模型（LLMs）在角色扮演和情感支持能力结合上的空白，通过构建首个情感支持角色扮演（ESRP）数据集ChatAnime，并评估LLMs在该任务上的表现。", "motivation": "大型语言模型在角色扮演对话和提供情感支持方面已展现出强大能力，但将这两种能力结合起来，实现与虚拟角色的情感支持性互动仍存在显著研究空白。", "method": "研究选择20个顶级动漫角色并设计60个以情感为中心的真实场景问题。招募40名对特定角色有深入了解且有丰富角色扮演经验的中国动漫爱好者。系统地收集了10个LLMs和这些爱好者两轮对话数据，构建了ChatAnime数据集，包含2,400条人类编写和24,000条LLM生成的回应，以及超过132,000条人工标注。设计了一个用户体验导向的评估系统，包含9个细粒度指标（基本对话、角色扮演、情感支持）和一个响应多样性指标。", "result": "实验结果表明，表现最佳的LLMs在角色扮演和情感支持方面超越了人类粉丝，但在响应多样性方面人类仍领先。", "conclusion": "本工作为未来优化LLMs在情感支持角色扮演（ESRP）方面的研究提供了宝贵的资源和见解。"}}
{"id": "2508.06170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06170", "abs": "https://arxiv.org/abs/2508.06170", "authors": ["Ojonugwa Oluwafemi Ejiga Peter", "Akingbola Oluwapemiisin", "Amalahu Chetachi", "Adeniran Opeyemi", "Fahmi Khalifa", "Md Mahmudur Rahman"], "title": "Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation", "comment": null, "summary": "Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,\nwhich is one of the main causes of cancer-related mortality globally; hence, it\nis deemed an essential technique for the prevention and early detection of\ncolorectal cancer. The research introduces a unique multidirectional\narchitectural framework to automate polyp detection within colonoscopy images\nwhile helping resolve limited healthcare dataset sizes and annotation\ncomplexities. The research implements a comprehensive system that delivers\nsynthetic data generation through Stable Diffusion enhancements together with\ndetection and segmentation algorithms. This detection approach combines Faster\nR-CNN for initial object localization while the Segment Anything Model (SAM)\nrefines the segmentation masks. The faster R-CNN detection algorithm achieved a\nrecall of 93.08% combined with a precision of 88.97% and an F1 score of\n90.98%.SAM is then used to generate the image mask. The research evaluated five\nstate-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,\nand MANet using ResNet34 as a base model. The results demonstrate the superior\nperformance of FPN with the highest scores of PSNR (7.205893) and SSIM\n(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced\nperformance in IoU (64.20%) and Dice score (77.53%).", "AI": {"tldr": "该研究提出了一种多向架构框架，结合Stable Diffusion生成合成数据，并利用Faster R-CNN进行息肉检测和SAM进行分割掩码细化，以解决结肠镜图像中息肉自动检测的医疗数据集有限和标注复杂性问题。", "motivation": "结肠镜检查是结直肠癌早期诊断的关键工具，但有限的医疗数据集和复杂的标注工作阻碍了息肉自动检测技术的发展。", "method": "研究采用多向架构框架，包括：1) 通过Stable Diffusion增强生成合成数据；2) 结合Faster R-CNN进行初始目标定位；3) 使用Segment Anything Model (SAM) 细化分割掩码；4) 评估了U-Net、PSPNet、FPN、LinkNet和MANet等五种最先进的分割模型，以ResNet34为基础模型。", "result": "Faster R-CNN检测算法实现了93.08%的召回率、88.97%的精确率和90.98%的F1分数。在评估的分割模型中，FPN在PSNR (7.205893) 和SSIM (0.492381) 上表现最佳，U-Net在召回率 (84.85%) 上表现出色，LinkNet在IoU (64.20%) 和Dice分数 (77.53%) 上表现均衡。", "conclusion": "该研究提出的多向架构框架，结合合成数据生成和先进的检测分割算法，有效解决了医疗数据限制，并在结肠镜图像息肉自动检测中取得了高精度和高性能。"}}
{"id": "2508.06063", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06063", "abs": "https://arxiv.org/abs/2508.06063", "authors": ["Chao Hao", "Zitong Yu", "Xin Liu", "Yuhao Wang", "Weicheng Xie", "Jingang Shi", "Huanjing Yue", "Jingyu Yang"], "title": "Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection", "comment": null, "summary": "Salient object detection (SOD) and camouflaged object detection (COD) are two\nclosely related but distinct computer vision tasks. Although both are\nclass-agnostic segmentation tasks that map from RGB space to binary space, the\nformer aims to identify the most salient objects in the image, while the latter\nfocuses on detecting perfectly camouflaged objects that blend into the\nbackground in the image. These two tasks exhibit strong contradictory\nattributes. Previous works have mostly believed that joint learning of these\ntwo tasks would confuse the network, reducing its performance on both tasks.\nHowever, here we present an opposite perspective: with the correct approach to\nlearning, the network can simultaneously possess the capability to find both\nsalient and camouflaged objects, allowing both tasks to benefit from joint\nlearning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,\nassuming that the decoding processes of SOD and COD have different distribution\ncharacteristics. The key to our method is to learn the respective means and\nvariances of the decoding processes for both tasks by inserting a minimal\namount of task-specific learnable parameters within a fully shared network\nstructure, thereby decoupling the contradictory attributes of the two tasks at\na minimal cost. Furthermore, we propose a saliency-based sampling strategy\n(SBSS) to sample the training set of the SOD task to balance the training set\nsizes of the two tasks. In addition, SBSS improves the training set quality and\nshortens the training time. Based on the proposed SCJoint and SBSS, we train a\npowerful generalist network, named JoNet, which has the ability to\nsimultaneously capture both ``salient\" and ``camouflaged\". Extensive\nexperiments demonstrate the competitive performance and effectiveness of our\nproposed method. The code is available at https://github.com/linuxsino/JoNet.", "AI": {"tldr": "本文提出了一种名为SCJoint的联合学习方案，用于同时进行显著目标检测（SOD）和伪装目标检测（COD），颠覆了以往认为这两种任务联合学习会降低性能的观点，并通过引入少量任务特定参数和采样策略，实现了两种任务的共同受益。", "motivation": "SOD和COD是密切相关但又相互矛盾的计算机视觉任务。传统观点认为联合学习会混淆网络，降低两者的性能。本文旨在挑战这一观点，探索在正确的方法下，网络能否同时具备检测显著和伪装目标的能力，并使两者从联合学习中受益。", "method": "本文提出了SCJoint联合学习方案，其核心在于假设SOD和COD的解码过程具有不同的分布特性。通过在完全共享的网络结构中插入少量任务特定的可学习参数，来学习各自解码过程的均值和方差，从而以最小的代价解耦了两个任务的矛盾属性。此外，还提出了基于显著性的采样策略（SBSS）来平衡SOD任务的训练集大小，提高训练集质量并缩短训练时间。基于SCJoint和SBSS，训练了一个名为JoNet的通用网络。", "result": "广泛的实验证明，所提出的方法具有竞争力且有效。训练出的JoNet网络能够同时捕获“显著”和“伪装”目标，实现了两种任务的良好性能。", "conclusion": "通过SCJoint联合学习方案和SBSS采样策略，可以成功地使网络同时具备检测显著和伪装目标的能力，并且两种任务都能从联合学习中受益，这与先前认为联合学习会损害性能的观点相反。"}}
{"id": "2508.06418", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06418", "abs": "https://arxiv.org/abs/2508.06418", "authors": ["Haoran Shi", "Hongwei Yao", "Shuo Shao", "Shaopeng Jiao", "Ziqi Peng", "Zhan Qin", "Cong Wang"], "title": "Quantifying Conversation Drift in MCP via Latent Polytope", "comment": null, "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.", "AI": {"tldr": "该研究提出了SecMCP框架，通过在潜在多面体空间中建模LLM激活向量来检测和量化对话漂移，从而解决Model Context Protocol (MCP)在集成外部工具时面临的工具中毒和间接提示注入等安全风险。", "motivation": "Model Context Protocol (MCP) 虽然通过集成外部工具增强了LLM的功能，但其非隔离的执行上下文引入了严重的安全和隐私风险，如工具中毒和间接提示注入，导致对话劫持、错误信息传播或数据泄露。现有防御措施因依赖静态签名、计算效率低下以及无法量化对话劫持而不足。", "method": "SecMCP通过在潜在多面体空间中建模LLM的激活向量，识别对话动态中的异常漂移（即由对抗性外部知识引起的潜在空间轨迹偏差），从而实现对话劫持、误导和数据泄露的主动检测。", "result": "SecMCP在Llama3、Vicuna和Mistral三种先进LLM上，使用MS MARCO、HotpotQA、FinQA等基准数据集进行了评估，结果显示其检测能力强大，AUROC分数超过0.915，同时保持了系统可用性。", "conclusion": "SecMCP提供了一种安全的框架，能够有效检测和量化对话漂移，从而解决了MCP的安全威胁。该研究贡献包括对MCP安全威胁的系统分类、一种新颖的基于潜在多面体的对话漂移量化方法，以及SecMCP有效性的实证验证。"}}
{"id": "2508.06202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06202", "abs": "https://arxiv.org/abs/2508.06202", "authors": ["Chang Che", "Ziqi Wang", "Pengwan Yang", "Qi Wang", "Hui Ma", "Zenglin Shi"], "title": "LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning", "comment": null, "summary": "Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language\nModels (MLLMs) to incrementally learn new tasks over time. However, this\nprocess is challenged by catastrophic forgetting, where performance on\npreviously learned tasks deteriorates as the model adapts to new ones. A common\napproach to mitigate forgetting is architecture expansion, which introduces\ntask-specific modules to prevent interference. Yet, existing methods often\nexpand entire layers for each task, leading to significant parameter overhead\nand poor scalability. To overcome these issues, we introduce LoRA in LoRA\n(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in\nMLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,\napplies an additional low-rank decomposition to matrix B to minimize\ntask-specific parameters, and incorporates a cosine-regularized stability loss\nto preserve consistency in shared representations over time. Extensive\nexperiments on a diverse CVIT benchmark show that LiLoRA consistently achieves\nsuperior performance in sequential task learning while significantly improving\nparameter efficiency compared to existing approaches.", "AI": {"tldr": "LiLoRA是一种高效的架构扩展方法，用于缓解多模态大语言模型（MLLMs）在持续视觉指令微调（CVIT）中面临的灾难性遗忘问题，它通过共享LoRA矩阵A、对矩阵B进行低秩分解以及引入余弦正则化稳定性损失，显著提高了参数效率和顺序任务学习性能。", "motivation": "CVIT使MLLMs能够增量学习新任务，但存在灾难性遗忘问题，即模型在适应新任务时对先前任务的性能下降。现有缓解遗忘的架构扩展方法（如为每个任务扩展整个层）导致参数开销大且可扩展性差。", "method": "LiLoRA是一种为CVIT量身定制的高效架构扩展方法。它通过以下方式减少冗余和最小化任务特定参数：1. 在任务间共享LoRA矩阵A。2. 对矩阵B应用额外的低秩分解。3. 引入余弦正则化稳定性损失，以保持共享表示随时间的一致性。", "result": "在多样化的CVIT基准测试中，LiLoRA在顺序任务学习中持续取得卓越性能，并且与现有方法相比，显著提高了参数效率。", "conclusion": "LiLoRA是一种高效且有效的架构扩展方法，能够解决MLLMs在CVIT中的灾难性遗忘问题，同时大大降低了参数开销，提升了模型学习新任务的能力和效率。"}}
{"id": "2508.06080", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06080", "abs": "https://arxiv.org/abs/2508.06080", "authors": ["Bin Xia", "Jiyang Liu", "Yuechen Zhang", "Bohao Peng", "Ruihang Chu", "Yitong Wang", "Xinglong Wu", "Bei Yu", "Jiaya Jia"], "title": "DreamVE: Unified Instruction-based Image and Video Editing", "comment": null, "summary": "Instruction-based editing holds vast potential due to its simple and\nefficient interactive editing format. However, instruction-based editing,\nparticularly for video, has been constrained by limited training data,\nhindering its practical application. To this end, we introduce DreamVE, a\nunified model for instruction-based image and video editing. Specifically, We\npropose a two-stage training strategy: first image editing, then video editing.\nThis offers two main benefits: (1) Image data scales more easily, and models\nare more efficient to train, providing useful priors for faster and better\nvideo editing training. (2) Unifying image and video generation is natural and\naligns with current trends. Moreover, we present comprehensive training data\nsynthesis pipelines, including collage-based and generative model-based data\nsynthesis. The collage-based data synthesis combines foreground objects and\nbackgrounds to generate diverse editing data, such as object manipulation,\nbackground changes, and text modifications. It can easily generate billions of\naccurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE\non extensive collage-based data to achieve strong performance in key editing\ntypes and enhance generalization and transfer capabilities. However,\ncollage-based data lacks some attribute editing cases, leading to a relative\ndrop in performance. In contrast, the generative model-based pipeline, despite\nbeing hard to scale up, offers flexibility in handling attribute editing cases.\nTherefore, we use generative model-based data to further fine-tune DreamVE.\nBesides, we design an efficient and powerful editing framework for DreamVE. We\nbuild on the SOTA T2V model and use a token concatenation with early drop\napproach to inject source image guidance, ensuring strong consistency and\neditability. The codes and models will be released.", "AI": {"tldr": "本文提出了DreamVE，一个统一的指令式图像和视频编辑模型，通过两阶段训练策略和全面的数据合成管道（包括拼贴式和生成模型式），解决了视频编辑中训练数据稀缺的问题，实现了强大的编辑能力和一致性。", "motivation": "指令式编辑因其简单高效的交互格式而具有巨大潜力，但其应用，特别是视频编辑，受到训练数据有限的严重制约。", "method": "1. 提出了DreamVE，一个统一的指令式图像和视频编辑模型。2. 采用两阶段训练策略：首先进行图像编辑训练，然后进行视频编辑训练。3. 设计了全面的训练数据合成管道，包括：a) 拼贴式数据合成：结合前景和背景生成多样化的编辑数据（如对象操作、背景更改、文本修改），可大规模生成准确、一致、真实且多样的数据。b) 生成模型式数据合成：用于处理拼贴式数据缺乏的属性编辑案例。4. 构建了一个高效强大的编辑框架，基于SOTA T2V模型，并采用令牌拼接与早期丢弃方法注入源图像指导，以确保强一致性和可编辑性。", "result": "1. 在大规模拼贴式数据上预训练DreamVE，实现了在关键编辑类型上的强大性能，并增强了泛化和迁移能力。2. 使用生成模型式数据进一步微调DreamVE，弥补了拼贴式数据在属性编辑方面的不足。3. 模型展现出强大的编辑能力和一致性。", "conclusion": "DreamVE通过创新的两阶段训练策略和全面的数据合成方法，有效解决了指令式图像和视频编辑领域的数据稀缺问题，提供了一个统一、高效且功能强大的编辑解决方案，实现了良好的编辑性能和一致性。"}}
{"id": "2508.06433", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06433", "abs": "https://arxiv.org/abs/2508.06433", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "title": "Memp: Exploring Agent Procedural Memory", "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.", "AI": {"tldr": "本文提出Memp框架，旨在为大型语言模型（LLM）代理提供可学习、可更新的终身程序记忆，通过从过往轨迹中提炼指令和脚本，显著提升代理在类似任务上的成功率和效率，并支持记忆迁移。", "motivation": "现有LLM代理的程序记忆脆弱，通常是手动工程或固化在静态参数中，缺乏学习、更新和终身演进的能力。", "method": "本文提出了Memp框架，将代理的过往轨迹提炼成细粒度的分步指令和高层次的脚本抽象。研究了程序记忆的构建（Build）、检索（Retrieval）和更新（Update）策略，并结合动态机制持续更新、纠正和废弃记忆内容，使其随新经验演进。", "result": "在TravelPlanner和ALFWorld任务上的实证评估表明，随着记忆库的完善，代理在类似任务上取得了持续更高的成功率和效率。此外，由更强模型构建的程序记忆具有保留价值，将其迁移到较弱模型上也能带来显著的性能提升。", "conclusion": "Memp框架通过提供可学习、可更新的程序记忆，有效克服了LLM代理在记忆方面的局限性，显著提高了代理的性能和效率，并展示了记忆的迁移能力。"}}
{"id": "2508.06259", "categories": ["cs.CV", "cs.AI", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.06259", "abs": "https://arxiv.org/abs/2508.06259", "authors": ["Zhangquan Chen", "Ruihui Zhao", "Chuwei Luo", "Mingze Sun", "Xinlei Yu", "Yangyang Kang", "Ruqi Huang"], "title": "SIFThinker: Spatially-Aware Image Focus for Visual Reasoning", "comment": "15 pages, 13 figures", "summary": "Current multimodal large language models (MLLMs) still face significant\nchallenges in complex visual tasks (e.g., spatial understanding, fine-grained\nperception). Prior methods have tried to incorporate visual reasoning, however,\nthey fail to leverage attention correction with spatial cues to iteratively\nrefine their focus on prompt-relevant regions. In this paper, we introduce\nSIFThinker, a spatially-aware \"think-with-images\" framework that mimics human\nvisual perception. Specifically, SIFThinker enables attention correcting and\nimage region focusing by interleaving depth-enhanced bounding boxes and natural\nlanguage. Our contributions are twofold: First, we introduce a\nreverse-expansion-forward-inference strategy that facilitates the generation of\ninterleaved image-text chains of thought for process-level supervision, which\nin turn leads to the construction of the SIF-50K dataset. Besides, we propose\nGRPO-SIF, a reinforced training paradigm that integrates depth-informed visual\ngrounding into a unified reasoning pipeline, teaching the model to dynamically\ncorrect and focus on prompt-relevant regions. Extensive experiments demonstrate\nthat SIFThinker outperforms state-of-the-art methods in spatial understanding\nand fine-grained visual perception, while maintaining strong general\ncapabilities, highlighting the effectiveness of our method.", "AI": {"tldr": "SIFThinker是一个模仿人类视觉感知的空间感知“图像思考”框架，通过深度增强的边界框和自然语言交错，实现注意力校正和图像区域聚焦，显著提升了多模态大语言模型在复杂视觉任务（如空间理解和细粒度感知）上的表现。", "motivation": "当前的多模态大语言模型（MLLMs）在复杂的视觉任务（如空间理解、细粒度感知）中面临显著挑战。现有方法未能有效利用空间线索进行注意力校正，以迭代地聚焦于与提示相关的区域。", "method": "本文提出了SIFThinker框架，通过交错使用深度增强的边界框和自然语言，实现注意力校正和图像区域聚焦。具体包括：1) 引入了“逆向扩展-正向推理”策略，生成交错的图像-文本思维链，用于过程级监督，并构建了SIF-50K数据集。2) 提出了GRPO-SIF，一种强化训练范式，将深度感知视觉定位集成到统一的推理流程中，训练模型动态纠正和聚焦于相关区域。", "result": "SIFThinker在空间理解和细粒度视觉感知方面超越了现有最先进的方法，同时保持了强大的通用能力，证明了该方法的有效性。", "conclusion": "SIFThinker通过模拟人类视觉感知，利用空间感知注意力校正和深度感知视觉定位，有效提升了多模态大语言模型在复杂视觉任务中的性能。"}}
{"id": "2508.06082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06082", "abs": "https://arxiv.org/abs/2508.06082", "authors": ["Yanxiao Sun", "Jiafu Wu", "Yun Cao", "Chengming Xu", "Yabiao Wang", "Weijian Cao", "Donghao Luo", "Chengjie Wang", "Yanwei Fu"], "title": "SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment", "comment": null, "summary": "Diffusion-based or flow-based models have achieved significant progress in\nvideo synthesis but require multiple iterative sampling steps, which incurs\nsubstantial computational overhead. While many distillation methods that are\nsolely based on trajectory-preserving or distribution-matching have been\ndeveloped to accelerate video generation models, these approaches often suffer\nfrom performance breakdown or increased artifacts under few-step settings. To\naddress these limitations, we propose \\textbf{\\emph{SwiftVideo}}, a unified and\nstable distillation framework that combines the advantages of\ntrajectory-preserving and distribution-matching strategies. Our approach\nintroduces continuous-time consistency distillation to ensure precise\npreservation of ODE trajectories. Subsequently, we propose a dual-perspective\nalignment that includes distribution alignment between synthetic and real data\nalong with trajectory alignment across different inference steps. Our method\nmaintains high-quality video generation while substantially reducing the number\nof inference steps. Quantitative evaluations on the OpenVid-1M benchmark\ndemonstrate that our method significantly outperforms existing approaches in\nfew-step video generation.", "AI": {"tldr": "SwiftVideo是一个统一且稳定的蒸馏框架，结合了轨迹保持和分布匹配策略，旨在显著加速扩散/流模型在少步视频生成中的效率和质量。", "motivation": "扩散/流模型在视频合成中表现出色，但需要大量迭代采样步骤，导致计算开销大。现有的蒸馏方法（仅基于轨迹保持或分布匹配）在少步设置下性能下降或产生更多伪影。", "method": "提出SwiftVideo框架，结合轨迹保持和分布匹配策略。引入连续时间一致性蒸馏以精确保留ODE轨迹。提出双视角对齐，包括合成数据与真实数据之间的分布对齐以及不同推理步骤间的轨迹对齐。", "result": "在显著减少推理步骤的同时，保持了高质量的视频生成。在OpenVid-1M基准测试上，在少步视频生成方面显著优于现有方法。", "conclusion": "SwiftVideo成功克服了现有蒸馏方法在加速视频生成方面的局限性，实现了高效且高质量的少步视频合成。"}}
{"id": "2508.06435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06435", "abs": "https://arxiv.org/abs/2508.06435", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.", "AI": {"tldr": "研究发现，通过少量语言微调的大语言模型（LLMs）能可靠地在未见语言中进行跨语言主题分类，并能有效纠正预训练偏见，证明了轻量级干预对LLM跨语言能力的有效性。", "motivation": "大语言模型正在改变社会科学研究，但其知识在微调后能否转移到预训练中出现但未在微调中出现的语言中，以及如何纠正预训练中的语言偏见，是亟待解决的问题。", "method": "研究使用轻量级LLaMA 3.2-3B模型，在单语、双语或多语数据集上进行微调，以分类13种语言中关于移民的推文。评估了少量语言特异性微调是否能实现跨语言主题检测，以及添加目标语言是否能纠正预训练偏见。同时发布了4位量化、LoRA微调的模型。", "result": "结果显示，在一到两种语言上微调的LLM能可靠地分类未见语言中的移民相关内容。然而，识别推文是支持或反对移民的立场时，多语言微调效果更佳。预训练偏向主导语言，但即使在微调中对代表性不足的语言进行极少量（原始预训练token量的$9.62\times10^{-11}$）的接触，也能带来显著提升。", "conclusion": "这些发现挑战了跨语言掌握需要大量多语言训练的假设：有限的语言覆盖足以实现主题层面的泛化，并且可以通过轻量级干预纠正结构性偏见。通过发布开源模型，为可扩展、包容性的研究提供了经济高效（推理速度快35倍，成本仅为GPT-4o的0.00000989%）的替代方案。"}}
{"id": "2508.06318", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06318", "abs": "https://arxiv.org/abs/2508.06318", "authors": ["Giacomo D'Amicantonio", "Snehashis Majhi", "Quan Kong", "Lorenzo Garattoni", "Gianpiero Francesca", "François Bremond", "Egor Bondarev"], "title": "Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection", "comment": null, "summary": "Video Anomaly Detection (VAD) is a challenging task due to the variability of\nanomalous events and the limited availability of labeled data. Under the\nWeakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided\nduring training, while predictions are made at the frame level. Although\nstate-of-the-art models perform well on simple anomalies (e.g., explosions),\nthey struggle with complex real-world events (e.g., shoplifting). This\ndifficulty stems from two key issues: (1) the inability of current models to\naddress the diversity of anomaly types, as they process all categories with a\nshared model, overlooking category-specific features; and (2) the weak\nsupervision signal, which lacks precise temporal information, limiting the\nability to capture nuanced anomalous patterns blended with normal events. To\naddress these challenges, we propose Gaussian Splatting-guided Mixture of\nExperts (GS-MoE), a novel framework that employs a set of expert models, each\nspecialized in capturing specific anomaly types. These experts are guided by a\ntemporal Gaussian splatting loss, enabling the model to leverage temporal\nconsistency and enhance weak supervision. The Gaussian splatting approach\nencourages a more precise and comprehensive representation of anomalies by\nfocusing on temporal segments most likely to contain abnormal events. The\npredictions from these specialized experts are integrated through a\nmixture-of-experts mechanism to model complex relationships across diverse\nanomaly patterns. Our approach achieves state-of-the-art performance, with a\n91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on\nXD-Violence and MSAD datasets. By leveraging category-specific expertise and\ntemporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.", "AI": {"tldr": "本文提出了一种名为GS-MoE的新型框架，通过结合专业专家模型和高斯溅射损失，解决了弱监督视频异常检测中复杂异常类型多样性和弱监督信号不足的问题，实现了最先进的性能。", "motivation": "视频异常检测（VAD）面临异常事件多样性和标注数据稀缺的挑战，尤其是在弱监督VAD（WSVAD）范式下，仅提供视频级标签。现有模型在处理简单异常时表现良好，但在复杂真实世界事件（如入店行窃）上表现不佳，原因在于：1) 模型无法处理异常类型的多样性，因为它们使用共享模型处理所有类别，忽略了类别特定特征；2) 弱监督信号缺乏精确的时间信息，限制了捕捉与正常事件混合的细微异常模式的能力。", "method": "本文提出了高斯溅射引导的专家混合模型（GS-MoE），一个新颖的框架。该框架采用一组专家模型，每个专家专注于捕捉特定异常类型。这些专家通过时间高斯溅射损失进行引导，使模型能够利用时间一致性并增强弱监督信号。高斯溅射方法通过关注最可能包含异常事件的时间段，鼓励更精确和全面的异常表示。来自这些专业专家的预测通过专家混合机制进行整合，以模拟不同异常模式之间的复杂关系。", "result": "GS-MoE方法在UCF-Crime数据集上实现了91.58%的AUC，达到了最先进的性能，并在XD-Violence和MSAD数据集上表现出卓越的结果。", "conclusion": "通过利用类别特定的专业知识和时间指导，GS-MoE为弱监督VAD设定了新的基准，有效解决了复杂异常检测中的关键挑战。"}}
{"id": "2508.06084", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06084", "abs": "https://arxiv.org/abs/2508.06084", "authors": ["Weichen Zhang", "Zhui Zhu", "Ningbo Li", "Kebin Liu", "Yunhao Liu"], "title": "AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance", "comment": null, "summary": "Vision-language models (VLMs) have achieved impressive performance on\nmultimodal reasoning tasks such as visual question answering (VQA), but their\ninference cost remains a significant challenge due to the large number of\nvision tokens processed during the prefill stage. Existing pruning methods\noften rely on directly using the attention patterns or static text prompt\nguidance, failing to exploit the dynamic internal signals generated during\ninference. To address these issues, we propose AdaptInfer, a plug-and-play\nframework for adaptive vision token pruning in VLMs. First, we introduce a\nfine-grained, dynamic text-guided pruning mechanism that reuses layer-wise\ntext-to-text attention maps to construct soft priors over text-token\nimportance, allowing more informed scoring of vision tokens at each stage.\nSecond, we perform an offline analysis of cross-modal attention shifts and\nidentify consistent inflection locations in inference, which inspire us to\npropose a more principled and efficient pruning schedule. Our method is\nlightweight and plug-and-play, also generalizable across multi-modal tasks.\nExperimental results have verified the effectiveness of the proposed method.\nFor example, it reduces CUDA latency by 61.3\\% while maintaining an average\naccuracy of 92.9\\% on vanilla LLaVA-1.5-7B. Under the same token budget,\nAdaptInfer surpasses SOTA in accuracy.", "AI": {"tldr": "AdaptInfer是一个即插即用的视觉token剪枝框架，通过动态文本引导和优化的剪枝策略，显著降低了视觉语言模型的推理成本，同时保持了高精度。", "motivation": "视觉语言模型（VLMs）在多模态推理任务上表现出色，但其推理成本高昂，尤其是在预填充阶段需要处理大量视觉token。现有剪枝方法未能有效利用推理过程中产生的动态内部信号。", "method": "本文提出了AdaptInfer框架。首先，引入了一种细粒度的动态文本引导剪枝机制，该机制重用了层级文本到文本的注意力图，以构建文本token重要性的软先验，从而在每个阶段更明智地评估视觉token。其次，通过对跨模态注意力转移进行离线分析，识别出推理中的一致拐点位置，从而提出了一种更原则且高效的剪枝策略。", "result": "AdaptInfer显著降低了推理成本，例如在香草LLaVA-1.5-7B模型上，它将CUDA延迟降低了61.3%，同时保持了92.9%的平均精度。在相同的token预算下，AdaptInfer在精度方面超越了现有最佳方法（SOTA）。", "conclusion": "AdaptInfer是一种有效、轻量级、即插即用且可泛化到多种多模态任务的视觉token剪枝方法，能显著降低VLMs的推理成本，同时保持或提高性能。"}}
{"id": "2508.06445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06445", "abs": "https://arxiv.org/abs/2508.06445", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.", "AI": {"tldr": "研究发现，生成式AI（特别是LLM）在新闻文章中的使用显著增加，尤其在地方和大学媒体中，AI内容常用于文章开头，提升了词汇丰富度和可读性但降低了正式性并导致写作风格趋于统一。", "motivation": "生成式AI（GenAI），特别是大型语言模型（LLMs）的迅速崛起，对新闻业的诚信和作者身份构成了担忧。", "method": "研究分析了来自主要、地方和大学新闻媒体的40,000多篇新闻文章，涵盖多种媒体格式。使用了三种先进的AI文本检测器（如Binoculars、Fast-Detect GPT和GPTZero），并进行了句子级别和语言学分析。", "result": "近年来GenAI的使用量大幅增加，尤其是在地方和大学新闻中。句子级别分析显示，LLMs常用于新闻的引言部分，而结论通常是手动撰写。语言学分析表明，GenAI提升了词汇丰富度和可读性，但降低了正式性，导致写作风格更加统一，特别是在地方媒体中。", "conclusion": "生成式AI在新闻领域的应用正在显著增长，并对新闻内容创作流程（如引言自动生成）和语言风格（如可读性提升但正式性降低、风格统一）产生了具体影响，尤其在地方和大学新闻中表现更为突出。"}}
{"id": "2508.06357", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06357", "abs": "https://arxiv.org/abs/2508.06357", "authors": ["Aman Bhatta", "Maria Dhakal", "Michael C. King", "Kevin W. Bowyer"], "title": "Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd", "comment": null, "summary": "A central problem in one-to-many facial identification is that the person in\nthe probe image may or may not have enrolled image(s) in the gallery; that is,\nmay be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one\nresult is Out-of-gallery have mostly focused on finding a suitable threshold on\nthe similarity score. We take a new approach, using the additional enrolled\nimages of the identity with the rank-one result to predict if the rank-one\nresult is In-gallery / Out-of-gallery. Given a gallery of identities and\nimages, we generate In-gallery and Out-of-gallery training data by extracting\nthe ranks of additional enrolled images corresponding to the rank-one identity.\nWe then train a classifier to utilize this feature vector to predict whether a\nrank-one result is In-gallery or Out-of-gallery. Using two different datasets\nand four different matchers, we present experimental results showing that our\napproach is viable for mugshot quality probe images, and also, importantly, for\nprobes degraded by blur, reduced resolution, atmospheric turbulence and\nsunglasses. We also analyze results across demographic groups, and show that\nIn-gallery / Out-of-gallery classification accuracy is similar across\ndemographics. Our approach has the potential to provide an objective estimate\nof whether a one-to-many facial identification is Out-of-gallery, and thereby\nto reduce false positive identifications, wrongful arrests, and wasted\ninvestigative time. Interestingly, comparing the results of older deep\nCNN-based face matchers with newer ones suggests that the effectiveness of our\nOut-of-gallery detection approach emerges only with matchers trained using\nadvanced margin-based loss functions.", "AI": {"tldr": "针对一对多人脸识别中查询图像是否在库（In-gallery/Out-of-gallery）的问题，提出一种新方法。该方法利用排名第一的身份的额外注册图像来训练分类器，以预测查询结果是否在库，有效降低误报。", "motivation": "一对多人脸识别的核心问题是查询图像中的人可能不在注册库中。以往检测Out-of-gallery的方法主要依赖于相似度分数阈值，存在局限性。", "method": "本研究提出一种新方法，利用与排名第一的身份相关的额外注册图像来预测查询结果是In-gallery还是Out-of-gallery。具体而言，通过提取这些额外注册图像的排名信息作为特征向量，并训练一个分类器来利用这些特征向量进行分类预测。", "result": "实验结果表明，该方法对高质量（如证件照）和受损（如模糊、低分辨率、大气湍流、戴墨镜）的探头图像均有效。此外，In-gallery/Out-of-gallery分类精度在不同人口统计群体之间相似。值得注意的是，该Out-of-gallery检测方法的有效性仅在使用了高级基于边距损失函数训练的较新深度CNN人脸匹配器上才能体现。", "conclusion": "该方法有潜力提供一种客观估计，判断一对多面部识别结果是否为Out-of-gallery，从而有助于减少误报识别、错误逮捕和浪费的调查时间。其有效性与所使用的面部匹配器类型（需基于高级边距损失函数训练）密切相关。"}}
{"id": "2508.06092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06092", "abs": "https://arxiv.org/abs/2508.06092", "authors": ["Yachun Mi", "Yu Li", "Yanting Li", "Shixin Sun", "Chen Hui", "Tong Zhang", "Yuanyuan Liu", "Chenyue Song", "Shaohui Liu"], "title": "Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation", "comment": null, "summary": "Accurate and efficient Video Quality Assessment (VQA) has long been a key\nresearch challenge. Current mainstream VQA methods typically improve\nperformance by pretraining on large-scale classification datasets (e.g.,\nImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this\nstrategy presents two significant challenges: (1) merely transferring semantic\nknowledge learned from pretraining is insufficient for VQA, as video quality\ndepends on multiple factors (e.g., semantics, distortion, motion, aesthetics);\n(2) pretraining on large-scale datasets demands enormous computational\nresources, often dozens or even hundreds of times greater than training\ndirectly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown\nremarkable generalization capabilities across a wide range of visual tasks, and\nhave begun to demonstrate promising potential in quality assessment. In this\nwork, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP\nenhances both visual and textual representations through a Shared Cross-Modal\nAdapter (SCMA), which contains only a minimal number of trainable parameters\nand is the only component that requires training. This design significantly\nreduces computational cost. In addition, we introduce a set of five learnable\nquality-level prompts to guide the VLMs in perceiving subtle quality\nvariations, thereby further enhancing the model's sensitivity to video quality.\nFurthermore, we investigate the impact of different frame sampling strategies\non VQA performance, and find that frame-difference-based sampling leads to\nbetter generalization performance across datasets. Extensive experiments\ndemonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.", "AI": {"tldr": "Q-CLIP是首个完全基于视觉-语言模型（VLMs）的视频质量评估（VQA）框架，通过引入共享跨模态适配器（SCMA）和可学习的质量级别提示，显著降低了计算成本并提升了对视频质量变化的感知能力，在多个VQA数据集上表现出色。", "motivation": "当前主流VQA方法依赖于大规模分类数据集预训练，但存在两大挑战：1) 预训练学到的语义知识不足以应对VQA中多因素（语义、失真、运动、美学）的复杂性；2) 大规模预训练需要巨大的计算资源。近期VLMs在视觉任务中展现出强大的泛化能力，为VQA提供了新的方向。", "method": "本文提出了Q-CLIP，一个基于VLMs的VQA框架。它通过一个共享跨模态适配器（SCMA）增强视觉和文本表示，SCMA仅包含少量可训练参数且是唯一需要训练的组件，从而显著降低了计算成本。此外，引入了五组可学习的质量级别提示，以引导VLMs感知细微的质量变化。研究还发现，基于帧差异的采样策略能带来更好的跨数据集泛化性能。", "result": "Q-CLIP在多个VQA数据集上展现出卓越的性能。基于帧差异的采样策略有助于模型在不同数据集上获得更好的泛化能力。", "conclusion": "Q-CLIP成功地将VLMs应用于VQA，克服了传统预训练方法的局限性，实现了高效且高精度的视频质量评估，为未来的VQA研究开辟了新途径。"}}
{"id": "2508.06447", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06447", "abs": "https://arxiv.org/abs/2508.06447", "authors": ["Lingkun Long", "Rubing Yang", "Yushi Huang", "Desheng Hui", "Ao Zhou", "Jianlei Yang"], "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning", "comment": null, "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.", "AI": {"tldr": "SlimInfer是一种新的框架，通过在前向传播过程中直接修剪不重要的提示词元来加速长上下文LLM推理，并利用信息扩散现象实现动态细粒度修剪和异步KV缓存管理。", "motivation": "长上下文LLM推理受到高计算需求的严重限制，尽管现有方法优化了注意力计算，但它们仍处理每层完整的隐藏状态，限制了整体效率。", "method": "SlimInfer在前向传播中直接修剪不重要的提示词元。其核心在于信息扩散现象：关键词元的信息在层间传播时会扩散到整个序列，即使修剪这些词元也能保持语义完整性。它引入了动态细粒度修剪机制，在中间层精确移除隐藏状态的冗余词元，并支持异步KV缓存管理器，预取所需词元块以减少内存和I/O成本。", "result": "在单个RTX 4090上，SlimInfer在LLaMA3.1-8B-Instruct模型上实现了高达2.53倍的首次词元生成时间(TTFT)加速和1.88倍的端到端延迟降低，同时在LongBench上保持了性能。", "conclusion": "SlimInfer通过直接修剪隐藏状态中的冗余词元，显著加速了长上下文LLM的推理过程，在不牺牲性能的情况下提高了效率，并有效降低了计算和内存开销。"}}
{"id": "2508.06429", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06429", "abs": "https://arxiv.org/abs/2508.06429", "authors": ["Guido Manni", "Clemente Lauretti", "Loredana Zollo", "Paolo Soda"], "title": "SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation", "comment": null, "summary": "Deep learning has revolutionized medical imaging, but its effectiveness is\nseverely limited by insufficient labeled training data. This paper introduces a\nnovel GAN-based semi-supervised learning framework specifically designed for\nlow labeled-data regimes, evaluated across settings with 5 to 50 labeled\nsamples per class. Our approach integrates three specialized neural networks --\na generator for class-conditioned image translation, a discriminator for\nauthenticity assessment and classification, and a dedicated classifier --\nwithin a three-phase training framework. The method alternates between\nsupervised training on limited labeled data and unsupervised learning that\nleverages abundant unlabeled images through image-to-image translation rather\nthan generation from noise. We employ ensemble-based pseudo-labeling that\ncombines confidence-weighted predictions from the discriminator and classifier\nwith temporal consistency through exponential moving averaging, enabling\nreliable label estimation for unlabeled data. Comprehensive evaluation across\neleven MedMNIST datasets demonstrates that our approach achieves statistically\nsignificant improvements over six state-of-the-art GAN-based semi-supervised\nmethods, with particularly strong performance in the extreme 5-shot setting\nwhere the scarcity of labeled data is most challenging. The framework maintains\nits superiority across all evaluated settings (5, 10, 20, and 50 shots per\nclass). Our approach offers a practical solution for medical imaging\napplications where annotation costs are prohibitive, enabling robust\nclassification performance even with minimal labeled data. Code is available at\nhttps://github.com/GuidoManni/SPARSE.", "AI": {"tldr": "本文提出了一种新颖的基于GAN的半监督学习框架，专门针对医疗影像领域标记数据极度稀缺的问题（每类5-50个样本），通过集成三个专业神经网络和三阶段训练策略，在多个MedMNIST数据集上显著优于现有SOTA方法。", "motivation": "深度学习在医学影像中受到标记训练数据不足的严重限制，因为医学图像标注成本高昂且耗时，这促使研究者寻求在极低标记数据下仍能有效分类的方法。", "method": "该方法采用一个GAN-based的半监督学习框架，包含三个专用神经网络：用于类别条件图像转换的生成器、用于真实性评估和分类的判别器，以及一个独立的分类器。训练分三阶段交替进行：有限标记数据的监督学习和利用大量未标记图像的无监督学习（通过图像到图像转换而非从噪声生成）。此外，采用基于集成（判别器和分类器置信度加权预测）和时间一致性（指数移动平均）的伪标签技术，为未标记数据生成可靠标签。", "result": "在11个MedMNIST数据集上的综合评估表明，该方法在统计学上显著优于六种最先进的基于GAN的半监督方法，尤其在极端的5样本设置中表现出特别强的性能。该框架在所有评估设置（每类5、10、20和50样本）中均保持其优越性。", "conclusion": "该方法为标记成本高昂的医疗影像应用提供了一个实用的解决方案，即使在标记数据极少的情况下也能实现鲁棒的分类性能。"}}
{"id": "2508.06093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06093", "abs": "https://arxiv.org/abs/2508.06093", "authors": ["Chen Zhu", "Buzhen Huang", "Zijing Wu", "Binghui Zuo", "Yangang Wang"], "title": "E-React: Towards Emotionally Controlled Synthesis of Human Reactions", "comment": null, "summary": "Emotion serves as an essential component in daily human interactions.\nExisting human motion generation frameworks do not consider the impact of\nemotions, which reduces naturalness and limits their application in interactive\ntasks, such as human reaction synthesis. In this work, we introduce a novel\ntask: generating diverse reaction motions in response to different emotional\ncues. However, learning emotion representation from limited motion data and\nincorporating it into a motion generation framework remains a challenging\nproblem. To address the above obstacles, we introduce a semi-supervised emotion\nprior in an actor-reactor diffusion model to facilitate emotion-driven reaction\nsynthesis. Specifically, based on the observation that motion clips within a\nshort sequence tend to share the same emotion, we first devise a\nsemi-supervised learning framework to train an emotion prior. With this prior,\nwe further train an actor-reactor diffusion model to generate reactions by\nconsidering both spatial interaction and emotional response. Finally, given a\nmotion sequence of an actor, our approach can generate realistic reactions\nunder various emotional conditions. Experimental results demonstrate that our\nmodel outperforms existing reaction generation methods. The code and data will\nbe made publicly available at https://ereact.github.io/", "AI": {"tldr": "该研究提出了一种新颖的任务：生成多样化的情感驱动反应动作，并通过引入半监督情感先验到演员-反应者扩散模型中，解决了从有限数据中学习情感表示并生成自然反应的挑战。", "motivation": "现有的人体动作生成框架未考虑情感因素，导致生成的动作不自然，并限制了其在人类反应合成等交互任务中的应用。从有限的动作数据中学习情感表示并将其整合到动作生成框架中是一个难题。", "method": "引入了生成多样化情感驱动反应动作的新任务。提出了一种半监督情感先验（semi-supervised emotion prior），利用短动作序列内情感一致的观察来训练该先验。随后，利用此先验训练了一个演员-反应者扩散模型（actor-reactor diffusion model），该模型同时考虑了空间交互和情感响应来生成反应。", "result": "该方法能够根据演员的动作序列，在各种情感条件下生成逼真的反应动作。实验结果表明，该模型优于现有的反应生成方法。", "conclusion": "通过结合半监督情感先验和演员-反应者扩散模型，成功实现了情感驱动的、自然且多样化的人体反应动作合成，显著提升了交互任务中动作生成的真实感和应用潜力。"}}
{"id": "2508.06471", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06471", "abs": "https://arxiv.org/abs/2508.06471", "authors": ["GLM-4. 5 Team", ":", "Aohan Zeng", "Xin Lv", "Qinkai Zheng", "Zhenyu Hou", "Bin Chen", "Chengxing Xie", "Cunxiang Wang", "Da Yin", "Hao Zeng", "Jiajie Zhang", "Kedong Wang", "Lucen Zhong", "Mingdao Liu", "Rui Lu", "Shulin Cao", "Xiaohan Zhang", "Xuancheng Huang", "Yao Wei", "Yean Cheng", "Yifan An", "Yilin Niu", "Yuanhao Wen", "Yushi Bai", "Zhengxiao Du", "Zihan Wang", "Zilin Zhu", "Bohan Zhang", "Bosi Wen", "Bowen Wu", "Bowen Xu", "Can Huang", "Casey Zhao", "Changpeng Cai", "Chao Yu", "Chen Li", "Chendi Ge", "Chenghua Huang", "Chenhui Zhang", "Chenxi Xu", "Chenzheng Zhu", "Chuang Li", "Congfeng Yin", "Daoyan Lin", "Dayong Yang", "Dazhi Jiang", "Ding Ai", "Erle Zhu", "Fei Wang", "Gengzheng Pan", "Guo Wang", "Hailong Sun", "Haitao Li", "Haiyang Li", "Haiyi Hu", "Hanyu Zhang", "Hao Peng", "Hao Tai", "Haoke Zhang", "Haoran Wang", "Haoyu Yang", "He Liu", "He Zhao", "Hongwei Liu", "Hongxi Yan", "Huan Liu", "Huilong Chen", "Ji Li", "Jiajing Zhao", "Jiamin Ren", "Jian Jiao", "Jiani Zhao", "Jianyang Yan", "Jiaqi Wang", "Jiayi Gui", "Jiayue Zhao", "Jie Liu", "Jijie Li", "Jing Li", "Jing Lu", "Jingsen Wang", "Jingwei Yuan", "Jingxuan Li", "Jingzhao Du", "Jinhua Du", "Jinxin Liu", "Junkai Zhi", "Junli Gao", "Ke Wang", "Lekang Yang", "Liang Xu", "Lin Fan", "Lindong Wu", "Lintao Ding", "Lu Wang", "Man Zhang", "Minghao Li", "Minghuan Xu", "Mingming Zhao", "Mingshu Zhai", "Pengfan Du", "Qian Dong", "Shangde Lei", "Shangqing Tu", "Shangtong Yang", "Shaoyou Lu", "Shijie Li", "Shuang Li", "Shuang-Li", "Shuxun Yang", "Sibo Yi", "Tianshu Yu", "Wei Tian", "Weihan Wang", "Wenbo Yu", "Weng Lam Tam", "Wenjie Liang", "Wentao Liu", "Xiao Wang", "Xiaohan Jia", "Xiaotao Gu", "Xiaoying Ling", "Xin Wang", "Xing Fan", "Xingru Pan", "Xinyuan Zhang", "Xinze Zhang", "Xiuqing Fu", "Xunkai Zhang", "Yabo Xu", "Yandong Wu", "Yida Lu", "Yidong Wang", "Yilin Zhou", "Yiming Pan", "Ying Zhang", "Yingli Wang", "Yingru Li", "Yinpei Su", "Yipeng Geng", "Yitong Zhu", "Yongkun Yang", "Yuhang Li", "Yuhao Wu", "Yujiang Li", "Yunan Liu", "Yunqing Wang", "Yuntao Li", "Yuxuan Zhang", "Zezhen Liu", "Zhen Yang", "Zhengda Zhou", "Zhongpei Qiao", "Zhuoer Feng", "Zhuorui Liu", "Zichen Zhang", "Zihan Wang", "Zijun Yao", "Zikang Wang", "Ziqiang Liu", "Ziwei Chai", "Zixuan Li", "Zuodong Zhao", "Wenguang Chen", "Jidong Zhai", "Bin Xu", "Minlie Huang", "Hongning Wang", "Juanzi Li", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models", "comment": null, "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.", "AI": {"tldr": "GLM-4.5是一个开源的混合专家（MoE）大语言模型，总参数355B，激活参数32B，采用混合推理方法，在智能体、推理和编码（ARC）任务上表现出色，且参数量少于同类竞品。", "motivation": "旨在开发一个高性能的开源大语言模型，尤其是在智能体、推理和编码（ARC）任务上表现优异，并提高模型效率。", "method": "采用MoE架构，总参数355B，激活参数32B；支持思考和直接响应两种模式的混合推理方法；经过23T tokens的多阶段训练；通过专家模型迭代和强化学习进行全面的后期训练。", "result": "在ARC任务上表现强劲，TAU-Bench得分70.1%，AIME 24得分91.0%，SWE-bench Verified得分64.2%。参数量远少于多个竞争对手，但总体排名第三，在智能体基准测试中排名第二。同时发布了GLM-4.5（355B参数）和紧凑版GLM-4.5-Air（106B参数）。", "conclusion": "GLM-4.5展示了在推理和智能体AI系统方面的强大性能和效率，并通过开源模型推进了相关研究。"}}
{"id": "2508.06434", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06434", "abs": "https://arxiv.org/abs/2508.06434", "authors": ["Shengzhu Yang", "Jiawei Du", "Shuai Lu", "Weihang Zhang", "Ningli Wang", "Huiqi Li"], "title": "CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment", "comment": null, "summary": "Large-scale natural image-text datasets, especially those automatically\ncollected from the web, often suffer from loose semantic alignment due to weak\nsupervision, while medical datasets tend to have high cross-modal correlation\nbut low content diversity. These properties pose a common challenge for\ncontrastive language-image pretraining (CLIP): they hinder the model's ability\nto learn robust and generalizable representations. In this work, we propose\nCLIPin, a unified non-contrastive plug-in that can be seamlessly integrated\ninto CLIP-style architectures to improve multimodal semantic alignment,\nproviding stronger supervision and enhancing alignment robustness. Furthermore,\ntwo shared pre-projectors are designed for image and text modalities\nrespectively to facilitate the integration of contrastive and non-contrastive\nlearning in a parameter-compromise manner. Extensive experiments on diverse\ndownstream tasks demonstrate the effectiveness and generality of CLIPin as a\nplug-and-play component compatible with various contrastive frameworks. Code is\navailable at https://github.com/T6Yang/CLIPin.", "AI": {"tldr": "本文提出了CLIPin，一个统一的非对比式插件，可无缝集成到CLIP风格的架构中，以改善多模态语义对齐并增强对齐鲁棒性。", "motivation": "大规模自然图像-文本数据集（如网络自动收集的）通常存在语义对齐松散的问题，而医学数据集虽然跨模态关联性高但内容多样性低。这些特性共同阻碍了对比语言-图像预训练（CLIP）模型学习鲁棒和可泛化的表示。", "method": "提出CLIPin，一个统一的非对比式插件，可无缝集成到CLIP风格的架构中，以提供更强的监督和增强对齐鲁棒性。此外，设计了两个共享的预投影器分别用于图像和文本模态，以参数折衷的方式促进对比学习和非对比学习的集成。", "result": "在各种下游任务上的大量实验证明了CLIPin作为即插即用组件的有效性和通用性，它兼容多种对比框架。", "conclusion": "CLIPin通过引入非对比学习和优化的模态集成，显著改善了CLIP风格模型的多模态语义对齐和鲁棒性，是提升CLIP性能的有效且通用的插件。"}}
{"id": "2508.06101", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06101", "abs": "https://arxiv.org/abs/2508.06101", "authors": ["Yachun Mi", "Xingyang He", "Shixin Sun", "Yu Li", "Yanting Li", "Zhixuan Li", "Jian Jin", "Chen Hui", "Shaohui Liu"], "title": "UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization", "comment": null, "summary": "In the digital age, advanced image editing tools pose a serious threat to the\nintegrity of visual content, making image forgery detection and localization a\nkey research focus. Most existing Image Manipulation Localization (IML) methods\nrely on discriminative learning and require large, high-quality annotated\ndatasets. However, current datasets lack sufficient scale and diversity,\nlimiting model performance in real-world scenarios. To overcome this, recent\nstudies have explored Constrained IML (CIML), which generates pixel-level\nannotations through algorithmic supervision. However, existing CIML approaches\noften depend on complex multi-stage pipelines, making the annotation process\ninefficient. In this work, we propose a novel generative framework based on\ndiffusion models, named UGD-IML, which for the first time unifies both IML and\nCIML tasks within a single framework. By learning the underlying data\ndistribution, generative diffusion models inherently reduce the reliance on\nlarge-scale labeled datasets, allowing our approach to perform effectively even\nunder limited data conditions. In addition, by leveraging a class embedding\nmechanism and a parameter-sharing design, our model seamlessly switches between\nIML and CIML modes without extra components or training overhead. Furthermore,\nthe end-to-end design enables our model to avoid cumbersome steps in the data\nannotation process. Extensive experimental results on multiple datasets\ndemonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and\n4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the\nproposed method also excels in uncertainty estimation, visualization and\nrobustness.", "AI": {"tldr": "本文提出了一种名为UGD-IML的生成式扩散模型框架，首次将图像篡改定位（IML）和受限图像篡改定位（CIML）任务统一在一个框架内，显著减少了对大规模标注数据集的依赖，并提高了性能和效率。", "motivation": "当前数字时代，图像编辑工具对视觉内容完整性构成严重威胁，使图像篡改检测成为关键研究领域。现有IML方法依赖大量高质量标注数据，但数据集规模和多样性不足限制了模型在实际场景中的表现。虽然CIML通过算法监督生成像素级标注，但现有CIML方法通常依赖复杂的多阶段流程，导致标注过程效率低下。", "method": "提出了一种基于扩散模型的生成式框架UGD-IML，首次将IML和CIML任务统一。通过学习底层数据分布，该模型固有地减少了对大规模标注数据集的依赖，使其在有限数据条件下也能有效工作。此外，通过利用类别嵌入机制和参数共享设计，模型无需额外组件或训练开销即可在IML和CIML模式间无缝切换。其端到端设计也避免了繁琐的数据标注步骤。", "result": "在多个数据集上的广泛实验结果表明，UGD-IML在IML和CIML任务的F1指标上分别平均优于SOTA方法9.66和4.36。此外，所提出的方法在不确定性估计、可视化和鲁棒性方面也表现出色。", "conclusion": "UGD-IML是一个新颖的、基于扩散模型的生成式框架，成功地统一了图像篡改定位和受限图像篡改定位任务。它显著降低了对大规模标注数据的需求，提高了任务性能和数据标注效率，并在多项指标上超越了现有最佳方法。"}}
{"id": "2508.06475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06475", "abs": "https://arxiv.org/abs/2508.06475", "authors": ["Guimin Hu", "Daniel Hershcovich", "Hasti Seifi"], "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning", "comment": null, "summary": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data.", "AI": {"tldr": "HapticLLaMA是一种多模态感官语言模型，能将触觉振动信号转化为自然语言描述，并在虚拟现实、辅助功能和康复应用中展现出潜力。", "motivation": "以往的多模态研究主要集中在视觉和听觉信号，而触觉信号（如振动）的自然语言描述生成仍未被充分探索，存在研究空白。", "method": "该研究将触觉字幕生成任务形式化，并提出了HapticLLaMA模型。模型采用两种触觉分词器（基于频率和基于EnCodec）将触觉信号转换为离散单元。训练分为两个阶段：(1) 使用LoRA适应的LLaMA架构进行监督微调；(2) 通过人类反馈强化学习(RLHF)进行微调。", "result": "HapticLLaMA在解释触觉振动信号方面表现出色，METEOR得分为59.98，BLEU-4得分为32.06。超过61%的生成字幕在7分制中获得3.5分以上的人类评价，RLHF使整体评分分布提高了10%，表明与人类触觉感知更强的对齐。", "conclusion": "研究结果突出了大型语言模型处理和适应感官数据的潜力，特别是触觉数据，为虚拟现实、辅助功能和康复应用提供了新的方向。"}}
{"id": "2508.06453", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06453", "abs": "https://arxiv.org/abs/2508.06453", "authors": ["Ruida Cheng", "Tejas Sudharshan Mathai", "Pritam Mukherjee", "Benjamin Hou", "Qingqing Zhu", "Zhiyong Lu", "Matthew McAuliffe", "Ronald M. Summers"], "title": "Text Embedded Swin-UMamba for DeepLesion Segmentation", "comment": null, "summary": "Segmentation of lesions on CT enables automatic measurement for clinical\nassessment of chronic diseases (e.g., lymphoma). Integrating large language\nmodels (LLMs) into the lesion segmentation workflow offers the potential to\ncombine imaging features with descriptions of lesion characteristics from the\nradiology reports. In this study, we investigate the feasibility of integrating\ntext into the Swin-UMamba architecture for the task of lesion segmentation. The\npublicly available ULS23 DeepLesion dataset was used along with short-form\ndescriptions of the findings from the reports. On the test dataset, a high Dice\nScore of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for\nlesion segmentation. The proposed Text-Swin-UMamba model outperformed prior\napproaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <\n0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by\n1.74% and 0.22%, respectively. The dataset and code can be accessed at\nhttps://github.com/ruida/LLM-Swin-UMamba", "AI": {"tldr": "研究了将文本信息（放射报告描述）整合到Swin-UMamba架构中进行CT病灶分割的可行性，并取得了优于现有方法的性能。", "motivation": "CT影像上的病灶分割对于慢性疾病（如淋巴瘤）的临床评估和自动测量至关重要。将大型语言模型（LLMs）整合到病灶分割流程中，有望结合影像特征和放射报告中的病灶描述，从而提高分割精度。", "method": "将文本信息整合到Swin-UMamba架构中进行病灶分割。使用了公开的ULS23 DeepLesion数据集，并结合了报告中的简短发现描述。模型性能通过Dice分数和Hausdorff距离进行评估，并与LLM驱动的LanGuideMedSeg以及纯图像的xLSTM-UNet和nnUNet模型进行比较。", "result": "在测试数据集上，病灶分割取得了82%的高Dice分数和6.58像素的低Hausdorff距离。所提出的Text-Swin-UMamba模型优于先前的LLM驱动的LanGuideMedSeg模型（提高37%，p < 0.001），并分别超越了纯图像的xLSTM-UNet和nnUNet模型1.74%和0.22%。", "conclusion": "将文本信息整合到Swin-UMamba架构中进行CT病灶分割是可行的，并且能够显著提升分割性能，优于纯图像和LLM驱动的现有方法。"}}
{"id": "2508.06104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06104", "abs": "https://arxiv.org/abs/2508.06104", "authors": ["Gui Zou", "Chaofan Gan", "Chern Hong Lim", "Supavadee Aramvith", "Weiyao Lin"], "title": "MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment", "comment": "ICMEW 2025", "summary": "With the increasing availability of 2D and 3D data, significant advancements\nhave been made in the field of cross-modal retrieval. Nevertheless, the\nexistence of imperfect annotations presents considerable challenges, demanding\nrobust solutions for 2D-3D cross-modal retrieval in the presence of noisy label\nconditions. Existing methods generally address the issue of noise by dividing\nsamples independently within each modality, making them susceptible to\noverfitting on corrupted labels. To address these issues, we propose a robust\n2D-3D \\textbf{M}ulti-level cross-modal adaptive \\textbf{C}orrection and\n\\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal\nJoint label Correction (MJC) mechanism that leverages multimodal historical\nself-predictions to jointly model the modality prediction consistency, enabling\nreliable label refinement. Additionally, we propose a Multi-level Adaptive\nAlignment (MAA) strategy to effectively enhance cross-modal feature semantics\nand discrimination across different levels. Extensive experiments demonstrate\nthe superiority of our method, MCA, which achieves state-of-the-art performance\non both conventional and realistic noisy 3D benchmarks, highlighting its\ngenerality and effectiveness.", "AI": {"tldr": "针对2D-3D跨模态检索中存在的噪声标签问题，本文提出了MCA框架，通过多模态联合标签校正和多层级自适应对齐策略，实现了在噪声基准数据集上的SOTA性能。", "motivation": "现有2D-3D跨模态检索方法在不完善标注（噪声标签）下表现不佳，且易因独立处理各模态样本而过拟合损坏标签，因此需要鲁棒的解决方案。", "method": "提出MCA（多层级跨模态自适应校正与对齐框架）。具体包括：1. 多模态联合标签校正（MJC）机制，利用多模态历史自预测共同建模模态预测一致性，实现可靠的标签精炼。2. 多层级自适应对齐（MAA）策略，有效增强跨模态特征语义和不同层级间的判别能力。", "result": "MCA方法在传统和现实噪声3D基准测试中均实现了最先进的性能，验证了其通用性和有效性。", "conclusion": "所提出的MCA框架在处理噪声标签条件下的2D-3D跨模态检索方面表现出卓越的性能、通用性和有效性。"}}
{"id": "2508.06482", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06482", "abs": "https://arxiv.org/abs/2508.06482", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "title": "Post-training for Efficient Communication via Convention Formation", "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.", "AI": {"tldr": "针对LLMs无法自然形成沟通习惯的问题，本文提出一种后训练微调方法，并通过两个新基准评估，显著提升了LLMs形成约定俗成的能力。", "motivation": "人类在多轮交互中通过适应语言和形成临时约定来提高沟通效率，而现有的大语言模型（LLMs）不具备这种自然行为。", "method": "采用一种后训练流程，通过对启发式识别的约定形成示例进行有针对性的微调来培养此能力。评估使用两个新基准：1. 一个以认知为导向的交互基准，能一致地引发人类的强约定形成趋势。2. 一个基于文档的引用完成任务，反映实际的约定形成行为。", "result": "经过后训练的LLMs在两种评估方法中都显示出显著提升的约定形成能力。", "conclusion": "通过后训练微调过程，LLMs成功发展出更强的约定形成能力，弥补了与人类在多轮交互中沟通效率的差距。"}}
{"id": "2508.06485", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06485", "abs": "https://arxiv.org/abs/2508.06485", "authors": ["Sofiane Bouaziz", "Adel Hafiane", "Raphael Canals", "Rachid Nedjai"], "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion", "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing\n  (TGRS)", "summary": "Urbanization, climate change, and agricultural stress are increasing the\ndemand for precise and timely environmental monitoring. Land Surface\nTemperature (LST) is a key variable in this context and is retrieved from\nremote sensing satellites. However, these systems face a trade-off between\nspatial and temporal resolution. While spatio-temporal fusion methods offer\npromising solutions, few have addressed the estimation of daily LST at 10 m\nresolution. In this study, we present WGAST, a Weakly-Supervised Generative\nNetwork for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra\nMODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning\nframework designed for this task. It adopts a conditional generative\nadversarial architecture, with a generator composed of four stages: feature\nextraction, fusion, LST reconstruction, and noise suppression. The first stage\nemploys a set of encoders to extract multi-level latent representations from\nthe inputs, which are then fused in the second stage using cosine similarity,\nnormalization, and temporal attention mechanisms. The third stage decodes the\nfused features into high-resolution LST, followed by a Gaussian filter to\nsuppress high-frequency noise. Training follows a weakly supervised strategy\nbased on physical averaging principles and reinforced by a PatchGAN\ndiscriminator. Experiments demonstrate that WGAST outperforms existing methods\nin both quantitative and qualitative evaluations. Compared to the\nbest-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves\nSSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and\neffectively captures fine-scale thermal patterns, as validated against 33\nground-based sensors. The code is available at\nhttps://github.com/Sofianebouaziz1/WGAST.git.", "AI": {"tldr": "该研究提出了WGAST，一个弱监督生成网络，用于通过融合Terra MODIS、Landsat 8和Sentinel-2数据，实现每日10米地表温度（LST）的估计，并在性能上超越了现有方法。", "motivation": "城市化、气候变化和农业压力增加了对精确及时环境监测的需求。地表温度（LST）是关键变量，但遥感卫星系统在空间和时间分辨率之间存在权衡。尽管时空融合方法有前景，但很少有方法能解决每日10米分辨率LST的估计问题。", "method": "本研究提出了WGAST，一个用于每日10米LST估计的弱监督生成网络。WGAST是第一个针对此任务的端到端深度学习框架，采用条件生成对抗架构。其生成器包含四个阶段：特征提取（使用编码器提取多级潜在表示）、融合（使用余弦相似性、归一化和时间注意力机制）、LST重建（解码融合特征为高分辨率LST）和噪声抑制（高斯滤波器）。训练采用基于物理平均原理的弱监督策略，并由PatchGAN判别器强化。", "result": "实验证明，WGAST在定量和定性评估方面均优于现有方法。与表现最佳的基线相比，WGAST平均将RMSE降低了17.18%，SSIM提高了11.00%。此外，WGAST对云层引起的地表温度具有鲁棒性，并能有效捕捉精细尺度的热模式，这已通过33个地面传感器验证。", "conclusion": "WGAST是首个用于通过时空融合估计每日10米LST的端到端深度学习框架，它显著优于现有方法，并在捕捉精细热模式和处理云影响方面表现出鲁棒性。"}}
{"id": "2508.06115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06115", "abs": "https://arxiv.org/abs/2508.06115", "authors": ["Weichen Zhang", "Kebin Liu", "Fan Dang", "Zhui Zhu", "Xikai Sun", "Yunhao Liu"], "title": "SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation", "comment": null, "summary": "Semantic segmentation in open-vocabulary scenarios presents significant\nchallenges due to the wide range and granularity of semantic categories.\nExisting weakly-supervised methods often rely on category-specific supervision\nand ill-suited feature construction methods for contrastive learning, leading\nto semantic misalignment and poor performance. In this work, we propose a novel\nweakly-supervised approach, SynSeg, to address the challenges. SynSeg performs\nMulti-Category Contrastive Learning (MCCL) as a stronger training signal with a\nnew feature reconstruction framework named Feature Synergy Structure (FSS).\nSpecifically, MCCL strategy robustly combines both intra- and inter-category\nalignment and separation in order to make the model learn the knowledge of\ncorrelations from different categories within the same image. Moreover, FSS\nreconstructs discriminative features for contrastive learning through prior\nfusion and semantic-activation-map enhancement, effectively avoiding the\nforeground bias introduced by the visual encoder. In general, SynSeg\neffectively improves the abilities in semantic localization and discrimination\nunder weak supervision. Extensive experiments on benchmarks demonstrate that\nour method outperforms state-of-the-art (SOTA) performance. For instance,\nSynSeg achieves higher accuracy than SOTA baselines by 4.5\\% on VOC, 8.9\\% on\nContext, 2.6\\% on Object and 2.0\\% on City.", "AI": {"tldr": "SynSeg是一种新颖的弱监督语义分割方法，通过多类别对比学习（MCCL）和特征协同结构（FSS）解决了开放词汇场景下的挑战，显著提升了性能并超越了现有SOTA。", "motivation": "开放词汇场景下的语义分割面临类别范围广、粒度细的挑战。现有弱监督方法常依赖类别特定监督和不适合对比学习的特征构建，导致语义错位和性能不佳。", "method": "本文提出SynSeg方法。它采用多类别对比学习（MCCL）作为更强的训练信号，结合新的特征协同结构（FSS）。MCCL策略结合了类别内和类别间的对齐与分离，学习不同类别间的相关性。FSS通过先验融合和语义激活图增强重建判别性特征，有效避免了视觉编码器引入的前景偏差。", "result": "SynSeg在多个基准测试中超越了现有最先进（SOTA）性能。例如，在VOC上比SOTA基线高4.5%，Context上高8.9%，Object上高2.6%，City上高2.0%。", "conclusion": "SynSeg在弱监督下有效提升了语义定位和判别能力，并在开放词汇语义分割任务中取得了SOTA性能。"}}
{"id": "2508.06492", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06492", "abs": "https://arxiv.org/abs/2508.06492", "authors": ["Yuwei Yang", "Zeyu Zhang", "Yunzhong Hou", "Zhuowan Li", "Gaowen Liu", "Ali Payani", "Yuan-Sen Ting", "Liang Zheng"], "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding", "comment": "Accepted by ICCV 2025 (poster). 26 pages, 17 figures", "summary": "Being able to effectively read scientific plots, or chart understanding, is a\ncentral part toward building effective agents for science. However, existing\nmultimodal large language models (MLLMs), especially open-source ones, are\nstill falling behind with a typical success rate of 30%-50% on challenging\nbenchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are\noften restricted by their inadequate similarity to the real charts, which could\ncompromise model training and performance on complex real-world charts. In this\nstudy, we show that modularizing chart generation and diversifying visual\ndetails improves chart understanding capabilities. In particular, we design a\nfive-step data synthesis pipeline, where we separate data and function creation\nfor single plot generation, condition the generation of later subplots on\nearlier ones for multi-subplot figures, visually diversify the generated\nfigures, filter out low quality data, and finally generate the question-answer\n(QA) pairs with GPT-4o. This approach allows us to streamline the generation of\nfine-tuning datasets and introduce the effective chart dataset (ECD), which\ncontains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring\n250+ chart type combinations with high visual complexity. We show that ECD\nconsistently improves the performance of various MLLMs on a range of real-world\nand synthetic test sets. Code, data and models are available at:\nhttps://github.com/yuweiyang-anu/ECD.", "AI": {"tldr": "本研究提出了一种模块化和多样化的数据合成方法，用于生成高质量的图表理解训练数据集（ECD），显著提升了多模态大语言模型（MLLMs）的图表理解能力。", "motivation": "现有MLLMs（特别是开源模型）在图表理解方面表现不佳，在挑战性基准测试中成功率仅为30%-50%。此前用于微调MLLMs的合成图表与真实图表相似度不足，影响了模型在复杂真实世界图表上的训练和性能。", "method": "设计了一个五步数据合成流程：1. 分离单图表的数据和功能创建；2. 多子图表生成时，后续子图表的生成基于先前的子图表；3. 多样化生成图表的视觉细节；4. 过滤低质量数据；5. 使用GPT-4o生成问答对。通过此方法构建了ECD数据集，包含1万多张图表图像和30万多对问答，涵盖25个主题和250多种高视觉复杂度的图表类型组合。", "result": "ECD数据集能够持续提升各种MLLMs在一系列真实世界和合成测试集上的性能。", "conclusion": "模块化图表生成和多样化视觉细节能有效提高图表理解能力。新提出的ECD数据集是提升MLLMs图表理解性能的有效资源。"}}
{"id": "2508.06122", "categories": ["cs.CV", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2508.06122", "abs": "https://arxiv.org/abs/2508.06122", "authors": ["Ting-Shuo Yo", "Shih-Hao Su", "Chien-Ming Wu", "Wei-Ting Chen", "Jung-Lien Chu", "Chiao-Wei Chang", "Hung-Chi Kuo"], "title": "Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events", "comment": "37 pages, 6 figures, 3 tables", "summary": "This study applied representation learning algorithms to satellite images and\nevaluated the learned latent spaces with classifications of various weather\nevents. The algorithms investigated include the classical linear\ntransformation, i.e., principal component analysis (PCA), state-of-the-art deep\nlearning method, i.e., convolutional autoencoder (CAE), and a residual network\npre-trained with large image datasets (PT). The experiment results indicated\nthat the latent space learned by CAE consistently showed higher threat scores\nfor all classification tasks. The classifications with PCA yielded high hit\nrates but also high false-alarm rates. In addition, the PT performed\nexceptionally well at recognizing tropical cyclones but was inferior in other\ntasks. Further experiments suggested that representations learned from\nhigher-resolution datasets are superior in all classification tasks for\ndeep-learning algorithms, i.e., CAE and PT. We also found that smaller latent\nspace sizes had minor impact on the classification task's hit rate. Still, a\nlatent space dimension smaller than 128 caused a significantly higher false\nalarm rate. Though the CAE can learn latent spaces effectively and efficiently,\nthe interpretation of the learned representation lacks direct connections to\nphysical attributions. Therefore, developing a physics-informed version of CAE\ncan be a promising outlook for the current work.", "AI": {"tldr": "本研究将表示学习算法应用于卫星图像，评估了PCA、CAE和预训练残差网络在各种天气事件分类中的潜在空间效果，发现CAE表现最佳，并指出其物理可解释性是未来研究方向。", "motivation": "研究旨在评估不同的表示学习算法（包括传统统计方法和深度学习方法）在卫星图像上学习到的潜在空间，以期提高对各种天气事件的分类性能。", "method": "研究采用了主成分分析（PCA）、卷积自编码器（CAE）以及在大型图像数据集上预训练的残差网络（PT）等表示学习算法。通过对各种天气事件进行分类来评估学习到的潜在空间，并比较了威胁评分、命中率和虚警率。此外，还探讨了更高分辨率数据集和不同潜在空间维度对分类性能的影响。", "result": "CAE学习到的潜在空间在所有分类任务中都表现出持续更高的威胁评分。PCA虽然命中率高，但虚警率也高。PT在识别热带气旋方面表现出色，但在其他任务中表现不佳。深度学习算法（CAE和PT）从更高分辨率数据集中学习到的表示在所有分类任务中都更优。较小的潜在空间尺寸对命中率影响不大，但小于128的潜在空间维度会显著提高虚警率。", "conclusion": "CAE能够有效且高效地学习潜在空间，但其学习到的表示缺乏与物理属性的直接联系。因此，开发物理信息增强的CAE版本是未来研究的一个有前景的方向。"}}
{"id": "2508.06125", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06125", "abs": "https://arxiv.org/abs/2508.06125", "authors": ["Lin Zhang", "Xianfang Zeng", "Kangcong Li", "Gang Yu", "Tao Chen"], "title": "SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning", "comment": "ICCV 2025", "summary": "We propose SC-Captioner, a reinforcement learning framework that enables the\nself-correcting capability of image caption models. Our crucial technique lies\nin the design of the reward function to incentivize accurate caption\ncorrections. Specifically, the predicted and reference captions are decomposed\ninto object, attribute, and relation sets using scene-graph parsing algorithms.\nWe calculate the set difference between sets of initial and self-corrected\ncaptions to identify added and removed elements. These elements are matched\nagainst the reference sets to calculate correctness bonuses for accurate\nrefinements and mistake punishments for wrong additions and removals, thereby\nforming the final reward. For image caption quality assessment, we propose a\nset of metrics refined from CAPTURE that alleviate its incomplete precision\nevaluation and inefficient relation matching problems. Furthermore, we collect\na fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K\ndiverse images from COCO dataset. Experiments show that applying SC-Captioner\non large visual-language models can generate better image captions across\nvarious scenarios, significantly outperforming the direct preference\noptimization training strategy.", "AI": {"tldr": "SC-Captioner是一个基于强化学习的框架，通过设计奖励函数实现图像字幕模型的自校正能力，并引入新的评估指标和数据集，显著提升了字幕生成质量。", "motivation": "现有图像字幕模型缺乏自校正能力，难以生成高度准确的字幕。研究旨在通过强化学习框架赋予模型自校正能力，以生成更精确的图像描述。", "method": "提出SC-Captioner强化学习框架，核心在于设计奖励函数以激励准确的字幕修正。该奖励函数通过场景图解析将预测和参考字幕分解为对象、属性和关系集合，计算初始与修正字幕之间的集合差异，并根据与参考集合的匹配度给予正确修正奖励和错误增删惩罚。此外，提出了一套改进的图像字幕质量评估指标，并构建了RefinedCaps细粒度标注数据集。", "result": "实验表明，将SC-Captioner应用于大型视觉语言模型能生成在各种场景下更好的图像字幕，其性能显著优于直接偏好优化训练策略。", "conclusion": "SC-Captioner是一个有效的强化学习框架，能够赋予图像字幕模型自校正能力，通过精心设计的奖励函数和新的评估方法，显著提升了字幕生成质量，并超越了DPO等现有训练策略。"}}
{"id": "2508.06127", "categories": ["cs.CV", "I.4.9"], "pdf": "https://arxiv.org/pdf/2508.06127", "abs": "https://arxiv.org/abs/2508.06127", "authors": ["Yi Qin", "Rui Wang", "Tao Huang", "Tong Xiao", "Liping Jing"], "title": "SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures", "comment": "8 pages,recived by ICCV2025", "summary": "While the Segment Anything Model (SAM) transforms interactive segmentation\nwith zero-shot abilities, its inherent vulnerabilities present a single-point\nrisk, potentially leading to the failure of numerous downstream applications.\nProactively evaluating these transferable vulnerabilities is thus imperative.\nPrior adversarial attacks on SAM often present limited transferability due to\ninsufficient exploration of common weakness across domains. To address this, we\npropose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that\nleverages only the encoder of SAM for generating transferable adversarial\nexamples. Specifically, it achieves this by explicitly characterizing the\nshared vulnerable regions between SAM and downstream models through a\nparametric simplicial complex. Our goal is to identify such complexes within\nadversarially potent regions by iterative vertex-wise refinement. A lightweight\ndomain re-adaptation strategy is introduced to bridge domain divergence using\nminimal reference data during the initialization of simplicial complex.\nUltimately, VeSCA generates consistently transferable adversarial examples\nthrough random simplicial complex sampling. Extensive experiments demonstrate\nthat VeSCA achieves performance improved by 12.7% compared to state-of-the-art\nmethods across three downstream model categories across five domain-specific\ndatasets. Our findings further highlight the downstream model risks posed by\nSAM's vulnerabilities and emphasize the urgency of developing more robust\nfoundation models.", "AI": {"tldr": "本文提出VeSCA，一种针对SAM（Segment Anything Model）的新型攻击方法，通过利用SAM的编码器和参数化单纯复形，生成可迁移的对抗性样本，以评估其对下游应用的潜在风险。", "motivation": "SAM虽然在零样本交互式分割方面表现出色，但其固有的脆弱性可能导致大量下游应用失败。现有对抗性攻击的迁移性有限，未能充分探索跨领域的共同弱点，因此迫切需要主动评估这些可迁移的漏洞。", "method": "本文提出Vertex-Refining Simplicial Complex Attack (VeSCA) 方法。该方法仅利用SAM的编码器生成可迁移的对抗性样本，通过参数化单纯复形明确表征SAM与下游模型之间的共享脆弱区域，并通过迭代的顶点细化来识别这些区域。此外，引入轻量级领域再适应策略，在单纯复形初始化时使用最少参考数据来弥合领域差异。最终，通过随机单纯复形采样生成一致可迁移的对抗性样本。", "result": "VeSCA在三个下游模型类别和五个特定领域数据集上，与现有最先进方法相比，性能提高了12.7%。研究结果进一步凸显了SAM的漏洞给下游模型带来的风险。", "conclusion": "SAM的漏洞对下游模型构成风险，这强调了开发更鲁棒的基础模型的紧迫性。"}}
{"id": "2508.06139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06139", "abs": "https://arxiv.org/abs/2508.06139", "authors": ["Shaohua Pan", "Xinyu Yi", "Yan Zhou", "Weihua Jian", "Yuan Zhang", "Pengfei Wan", "Feng Xu"], "title": "DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera", "comment": null, "summary": "Combining sparse IMUs and a monocular camera is a new promising setting to\nperform real-time human motion capture. This paper proposes a diffusion-based\nsolution to learn human motion priors and fuse the two modalities of signals\ntogether seamlessly in a unified framework. By delicately considering the\ncharacteristics of the two signals, the sequential visual information is\nconsidered as a whole and transformed into a condition embedding, while the\ninertial measurement is concatenated with the noisy body pose frame by frame to\nconstruct a sequential input for the diffusion model. Firstly, we observe that\nthe visual information may be unavailable in some frames due to occlusions or\nsubjects moving out of the camera view. Thus incorporating the sequential\nvisual features as a whole to get a single feature embedding is robust to the\noccasional degenerations of visual information in those frames. On the other\nhand, the IMU measurements are robust to occlusions and always stable when\nsignal transmission has no problem. So incorporating them frame-wisely could\nbetter explore the temporal information for the system. Experiments have\ndemonstrated the effectiveness of the system design and its state-of-the-art\nperformance in pose estimation compared with the previous works. Our codes are\navailable for research at https://shaohua-pan.github.io/diffcap-page.", "AI": {"tldr": "本文提出一种基于扩散模型的方法，结合稀疏IMU和单目相机，实现鲁棒的实时人体运动捕捉，并通过巧妙融合两种模态信号提升性能。", "motivation": "在实时人体运动捕捉中，稀疏IMU和单目相机结合是一种有前景的方法。然而，需要一种统一的框架来学习人体运动先验并无缝融合这两种模态信号，同时解决视觉信息可能因遮挡或移出视野而不可用的问题，以及充分利用IMU测量的稳定性。", "method": "提出一个扩散模型解决方案。将序列视觉信息作为一个整体转换为条件嵌入，以增强对偶尔视觉信息退化的鲁棒性。将惯性测量与每一帧的噪声身体姿态连接起来，构建扩散模型的序列输入，以更好地探索时间信息。这种设计充分考虑了两种信号的特性。", "result": "实验证明了该系统设计的有效性，并且在姿态估计方面达到了与现有工作相比最先进的性能。", "conclusion": "该研究成功地利用扩散模型，通过巧妙地融合稀疏IMU和单目相机数据，实现了高效、鲁棒且高精度的实时人体运动捕捉，解决了多模态融合中的关键挑战。"}}
{"id": "2508.06142", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06142", "abs": "https://arxiv.org/abs/2508.06142", "authors": ["Hanqing Wang", "Yuan Tian", "Mingyu Liu", "Zhenhao Zhang", "Xiangyang Zhu"], "title": "SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models", "comment": null, "summary": "In the rapidly evolving landscape of Multimodal Large Language Models\n(MLLMs), the safety concerns of their outputs have earned significant\nattention. Although numerous datasets have been proposed, they may become\noutdated with MLLM advancements and are susceptible to data contamination\nissues. To address these problems, we propose \\textbf{SDEval}, the\n\\textit{first} safety dynamic evaluation framework to controllably adjust the\ndistribution and complexity of safety benchmarks. Specifically, SDEval mainly\nadopts three dynamic strategies: text, image, and text-image dynamics to\ngenerate new samples from original benchmarks. We first explore the individual\neffects of text and image dynamics on model safety. Then, we find that\ninjecting text dynamics into images can further impact safety, and conversely,\ninjecting image dynamics into text also leads to safety risks. SDEval is\ngeneral enough to be applied to various existing safety and even capability\nbenchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and\ncapability benchmarks, MMBench and MMVet, show that SDEval significantly\ninfluences safety evaluation, mitigates data contamination, and exposes safety\nlimitations of MLLMs. Code is available at https://github.com/hq-King/SDEval", "AI": {"tldr": "SDEval是首个针对多模态大语言模型（MLLMs）安全的动态评估框架，通过文本、图像和图文动态策略生成新样本，解决现有安全基准过时和数据污染问题，并揭示MLLMs的安全局限性。", "motivation": "随着MLLMs的快速发展，其输出的安全问题日益受到关注。现有的安全评估数据集可能因模型进步而过时，且容易受到数据污染的影响，因此需要一种更动态、鲁棒的评估方法。", "method": "提出SDEval框架，通过三种动态策略可控地调整安全基准的分布和复杂性：文本动态（text dynamics）、图像动态（image dynamics）和图文动态（text-image dynamics）。这些策略从原始基准中生成新样本，并可应用于现有安全和能力基准。", "result": "实验结果表明，SDEval显著影响安全评估，有效缓解了数据污染问题，并揭示了MLLMs的安全局限性。研究发现，单独的文本和图像动态都会影响模型安全，而将文本动态注入图像或将图像动态注入文本则会进一步带来安全风险。", "conclusion": "SDEval是一个通用且有效的动态安全评估框架，能够应对MLLMs快速发展带来的评估挑战，提供更准确、抗污染的安全评估，并帮助识别模型的安全漏洞。"}}
{"id": "2508.06146", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06146", "abs": "https://arxiv.org/abs/2508.06146", "authors": ["Yuchen Guan", "Chong Sun", "Canmiao Fu", "Zhipeng Huang", "Chun Yuan", "Chen Li"], "title": "Text-guided Visual Prompt DINO for Generic Segmentation", "comment": null, "summary": "Recent advancements in multimodal vision models have highlighted limitations\nin late-stage feature fusion and suboptimal query selection for hybrid prompts\nopen-world segmentation, alongside constraints from caption-derived\nvocabularies. To address these challenges, we propose Prompt-DINO, a\ntext-guided visual Prompt DINO framework featuring three key innovations.\nFirst, we introduce an early fusion mechanism that unifies text/visual prompts\nand backbone features at the initial encoding stage, enabling deeper\ncross-modal interactions to resolve semantic ambiguities. Second, we design\norder-aligned query selection for DETR-based architectures, explicitly\noptimizing the structural alignment between text and visual queries during\ndecoding to enhance semantic-spatial consistency. Third, we develop a\ngenerative data engine powered by the Recognize Anything via Prompting (RAP)\nmodel, which synthesizes 0.5B diverse training instances through a dual-path\ncross-verification pipeline, reducing label noise by 80.5% compared to\nconventional approaches. Extensive experiments demonstrate that Prompt-DINO\nachieves state-of-the-art performance on open-world detection benchmarks while\nsignificantly expanding semantic coverage beyond fixed-vocabulary constraints.\nOur work establishes a new paradigm for scalable multimodal detection and data\ngeneration in open-world scenarios. Data&Code are available at\nhttps://github.com/WeChatCV/WeVisionOne.", "AI": {"tldr": "Prompt-DINO是一个文本引导的视觉提示DINO框架，通过早期融合、对齐查询选择和生成式数据引擎，解决了开放世界分割中多模态模型晚期特征融合、次优查询选择以及词汇限制的问题，实现了最先进的性能和语义覆盖。", "motivation": "现有多模态视觉模型在开放世界分割中存在局限性，包括晚期特征融合效率低下、混合提示的查询选择不理想，以及受限于从字幕派生出的固定词汇表。", "method": "1. 早期融合机制：在初始编码阶段统一文本/视觉提示和骨干特征，实现更深层次的跨模态交互以解决语义歧义。2. 顺序对齐查询选择：为DETR架构设计，明确优化解码过程中文本和视觉查询之间的结构对齐，增强语义-空间一致性。3. 生成式数据引擎：利用RAP模型合成5亿个多样化训练实例，通过双路径交叉验证流程，将标签噪声降低80.5%。", "result": "Prompt-DINO在开放世界检测基准测试中实现了最先进的性能，并显著扩展了语义覆盖范围，超越了固定词汇表的限制。", "conclusion": "该工作为开放世界场景中的可扩展多模态检测和数据生成建立了一个新范式。"}}
{"id": "2508.06147", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06147", "abs": "https://arxiv.org/abs/2508.06147", "authors": ["Xuanyu Liu", "Bonan An"], "title": "DSConv: Dynamic Splitting Convolution for Pansharpening", "comment": null, "summary": "Aiming to obtain a high-resolution image, pansharpening involves the fusion\nof a multi-spectral image (MS) and a panchromatic image (PAN), the low-level\nvision task remaining significant and challenging in contemporary research.\nMost existing approaches rely predominantly on standard convolutions, few\nmaking the effort to adaptive convolutions, which are effective owing to the\ninter-pixel correlations of remote sensing images. In this paper, we propose a\nnovel strategy for dynamically splitting convolution kernels in conjunction\nwith attention, selecting positions of interest, and splitting the original\nconvolution kernel into multiple smaller kernels, named DSConv. The proposed\nDSConv more effectively extracts features of different positions within the\nreceptive field, enhancing the network's generalization, optimization, and\nfeature representation capabilities. Furthermore, we innovate and enrich\nconcepts of dynamic splitting convolution and provide a novel network\narchitecture for pansharpening capable of achieving the tasks more efficiently,\nbuilding upon this methodology. Adequate fair experiments illustrate the\neffectiveness and the state-of-the-art performance attained by\nDSConv.Comprehensive and rigorous discussions proved the superiority and\noptimal usage conditions of DSConv.", "AI": {"tldr": "本文提出了一种名为DSConv的新型动态分裂卷积策略，结合注意力机制，用于高分辨率全色锐化任务，旨在更有效地提取特征并提升性能。", "motivation": "全色锐化是遥感图像处理中的一项重要且具有挑战性的低级视觉任务。现有方法多依赖标准卷积，而自适应卷积能更有效利用遥感图像的像素间关联性，但应用较少。", "method": "提出DSConv（动态分裂卷积），通过结合注意力机制动态地将原始卷积核分裂成多个更小的核，从而选择感兴趣的位置。在此基础上，构建了一个新的全色锐化网络架构。", "result": "DSConv能更有效地提取感受野内不同位置的特征，增强网络的泛化、优化和特征表示能力。实验证明其达到了先进的性能。", "conclusion": "DSConv是一种优越且有效的新型全色锐化方法，其优越性和最佳使用条件得到了充分验证。"}}
{"id": "2508.06152", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06152", "abs": "https://arxiv.org/abs/2508.06152", "authors": ["Kaiyuan Jiang", "Ruoxi Sun", "Ying Cao", "Yuqi Xu", "Xinran Zhang", "Junyan Guo", "ChengSheng Deng"], "title": "VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation", "comment": "17 pages,8 figures", "summary": "We present VISTAR, a user-centric, multi-dimensional benchmark for\ntext-to-image (T2I) evaluation that addresses the limitations of existing\nmetrics. VISTAR introduces a two-tier hybrid paradigm: it employs\ndeterministic, scriptable metrics for physically quantifiable attributes (e.g.,\ntext rendering, lighting) and a novel Hierarchical Weighted P/N Questioning\n(HWPQ) scheme that uses constrained vision-language models to assess abstract\nsemantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study\nwith 120 experts, we defined seven user roles and nine evaluation angles to\nconstruct the benchmark, which comprises 2,845 prompts validated by over 15,000\nhuman pairwise comparisons. Our metrics achieve high human alignment (>75%),\nwith the HWPQ scheme reaching 85.9% accuracy on abstract semantics,\nsignificantly outperforming VQA baselines. Comprehensive evaluation of\nstate-of-the-art models reveals no universal champion, as role-weighted scores\nreorder rankings and provide actionable guidance for domain-specific\ndeployment. All resources are publicly released to foster reproducible T2I\nassessment.", "AI": {"tldr": "VISTAR：一个用户中心、多维度的文本到图像评估基准，结合可量化指标和基于视觉语言模型的抽象语义评估，对现有T2I模型进行全面评估。", "motivation": "现有文本到图像（T2I）评估指标存在局限性，无法全面、用户中心地评估生成图像的质量。", "method": "提出VISTAR，采用双层混合范式：1) 对物理可量化属性（如文本渲染、光照）使用确定性、可脚本化指标；2) 引入分层加权P/N提问（HWPQ）方案，利用受限视觉语言模型评估抽象语义（如风格融合、文化忠实度）。该基准基于120位专家的德尔菲研究，定义了七种用户角色和九个评估角度，包含2,845个提示，并经过超过15,000次人工配对比较验证。", "result": "VISTAR指标与人类评估高度一致（>75%），其中HWPQ方案在抽象语义评估上达到85.9%的准确率，显著优于现有VQA基线。对最先进T2I模型的综合评估显示没有普适的“冠军”模型，角色加权得分能重新排序模型并为特定领域部署提供指导。", "conclusion": "VISTAR为T2I评估提供了一个用户中心、多维度的统一框架，其高人类对齐度及其资源公开，将促进可复现的T2I评估和特定领域模型的部署。"}}
{"id": "2508.06157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06157", "abs": "https://arxiv.org/abs/2508.06157", "authors": ["Xiaoxiao Yang", "Meiliang Liu", "Yunfang Xu", "Zijin Li", "Zhengye Si", "Xinyue Yang", "Zhiwen Zhao"], "title": "An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis", "comment": null, "summary": "Alzheimer's disease (AD) is a progressive neurodegenerative disorder that\nseverely impairs cognitive function and quality of life. Timely intervention in\nAD relies heavily on early and precise diagnosis, which remains challenging due\nto the complex and subtle structural changes in the brain. Most existing deep\nlearning methods focus only on a single plane of structural magnetic resonance\nimaging (sMRI) and struggle to accurately capture the complex and nonlinear\nrelationships among pathological regions of the brain, thus limiting their\nability to precisely identify atrophic features. To overcome these limitations,\nwe propose an innovative framework, MPF-KANSC, which integrates multi-plane\nfusion (MPF) for combining features from the coronal, sagittal, and axial\nplanes, and a Kolmogorov-Arnold Network-guided spatial-channel attention\nmechanism (KANSC) to more effectively learn and represent sMRI atrophy\nfeatures. Specifically, the proposed model enables parallel feature extraction\nfrom multiple anatomical planes, thus capturing more comprehensive structural\ninformation. The KANSC attention mechanism further leverages a more flexible\nand accurate nonlinear function approximation technique, facilitating precise\nidentification and localization of disease-related abnormalities. Experiments\non the ADNI dataset confirm that the proposed MPF-KANSC achieves superior\nperformance in AD diagnosis. Moreover, our findings provide new evidence of\nright-lateralized asymmetry in subcortical structural changes during AD\nprogression, highlighting the model's promising interpretability.", "AI": {"tldr": "该研究提出了一种名为MPF-KANSC的新型深度学习框架，通过整合多平面MRI特征融合和基于Kolmogorov-Arnold网络的空间-通道注意力机制，以提高阿尔茨海默病（AD）的早期诊断精度和可解释性。", "motivation": "阿尔茨海默病早期诊断面临挑战，现有深度学习方法多专注于单一平面MRI，难以准确捕捉大脑复杂非线性病理关系和识别细微萎缩特征，限制了诊断精度。", "method": "提出MPF-KANSC框架：1) 多平面融合（MPF）模块，并行提取并结合冠状面、矢状面和轴向面sMRI特征，获取更全面的结构信息。2) Kolmogorov-Arnold网络引导的空间-通道注意力机制（KANSC），利用更灵活准确的非线性函数逼近技术，精确识别和定位疾病相关异常。", "result": "在ADNI数据集上的实验证明，MPF-KANSC在AD诊断中取得了优越性能。此外，研究结果提供了AD进展中皮层下结构变化右侧偏侧不对称的新证据，显示了模型的良好可解释性。", "conclusion": "MPF-KANSC模型有效提升了AD诊断的准确性，并通过其可解释性揭示了AD进展中大脑结构变化的潜在生物学见解，为AD的早期干预提供了新工具。"}}
{"id": "2508.06160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06160", "abs": "https://arxiv.org/abs/2508.06160", "authors": ["Zhenbang Du", "Yonggan Fu", "Lifu Wang", "Jiayi Qian", "Xiao Luo", "Yingyan", "Lin"], "title": "Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment", "comment": "Accepted by ICCV 2025", "summary": "Diffusion models have shown remarkable success across generative tasks, yet\ntheir high computational demands challenge deployment on resource-limited\nplatforms. This paper investigates a critical question for compute-optimal\ndiffusion model deployment: Under a post-training setting without fine-tuning,\nis it more effective to reduce the number of denoising steps or to use a\ncheaper per-step inference? Intuitively, reducing the number of denoising steps\nincreases the variability of the distributions across steps, making the model\nmore sensitive to compression. In contrast, keeping more denoising steps makes\nthe differences smaller, preserving redundancy, and making post-training\ncompression more feasible. To systematically examine this, we propose PostDiff,\na training-free framework for accelerating pre-trained diffusion models by\nreducing redundancy at both the input level and module level in a post-training\nmanner. At the input level, we propose a mixed-resolution denoising scheme\nbased on the insight that reducing generation resolution in early denoising\nsteps can enhance low-frequency components and improve final generation\nfidelity. At the module level, we employ a hybrid module caching strategy to\nreuse computations across denoising steps. Extensive experiments and ablation\nstudies demonstrate that (1) PostDiff can significantly improve the\nfidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to\nboost efficiency while maintaining decent generation fidelity, reducing\nper-step inference cost is often more effective than reducing the number of\ndenoising steps. Our code is available at\nhttps://github.com/GATECH-EIC/PostDiff.", "AI": {"tldr": "本文提出PostDiff框架，用于在不重新训练的情况下加速扩散模型，通过优化输入分辨率和模块计算来降低推理成本，并发现降低每步推理成本比减少去噪步数更有效。", "motivation": "扩散模型计算成本高昂，难以部署在资源受限平台。研究旨在解决一个关键问题：在不微调的后训练设置下，是减少去噪步数更有效，还是使用更便宜的每步推理更有效？", "method": "提出PostDiff，一个免训练的框架，通过减少冗余来加速预训练扩散模型。在输入层面，采用混合分辨率去噪方案，在早期去噪步骤降低生成分辨率以增强低频分量。在模块层面，采用混合模块缓存策略，在去噪步骤间重用计算。", "result": "PostDiff显著改善了最先进扩散模型的保真度-效率权衡。为了提高效率并保持良好的生成保真度，降低每步推理成本通常比减少去噪步数更有效。", "conclusion": "PostDiff通过在输入和模块层面优化计算，为扩散模型提供了有效的后训练加速。研究表明，在追求效率时，降低每步推理成本通常优于减少去噪步数。"}}
{"id": "2508.06189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06189", "abs": "https://arxiv.org/abs/2508.06189", "authors": ["Cheng Liu", "Daou Zhang", "Tingxu Liu", "Yuhan Wang", "Jinyang Chen", "Yuexuan Li", "Xinying Xiao", "Chenbo Xin", "Ziru Wang", "Weichao Wu"], "title": "MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration", "comment": null, "summary": "With the acceleration of urbanization, criminal behavior in public scenes\nposes an increasingly serious threat to social security. Traditional anomaly\ndetection methods based on feature recognition struggle to capture high-level\nbehavioral semantics from historical information, while generative approaches\nbased on Large Language Models (LLMs) often fail to meet real-time\nrequirements. To address these challenges, we propose MA-CBP, a criminal\nbehavior prediction framework based on multi-agent asynchronous collaboration.\nThis framework transforms real-time video streams into frame-level semantic\ndescriptions, constructs causally consistent historical summaries, and fuses\nadjacent image frames to perform joint reasoning over long- and short-term\ncontexts. The resulting behavioral decisions include key elements such as event\nsubjects, locations, and causes, enabling early warning of potential criminal\nactivity. In addition, we construct a high-quality criminal behavior dataset\nthat provides multi-scale language supervision, including frame-level,\nsummary-level, and event-level semantic annotations. Experimental results\ndemonstrate that our method achieves superior performance on multiple datasets\nand offers a promising solution for risk warning in urban public safety\nscenarios.", "AI": {"tldr": "该论文提出了MA-CBP框架，一个基于多智能体异步协作的犯罪行为预测系统，通过实时视频流分析实现公共场景下的早期预警，并构建了高质量的犯罪行为数据集。", "motivation": "随着城市化进程加速，公共场所犯罪行为对社会安全构成严重威胁。传统异常检测方法难以捕捉高层行为语义，而基于大语言模型的生成方法无法满足实时性要求。", "method": "MA-CBP框架将实时视频流转换为帧级语义描述，构建因果一致的历史摘要，并融合相邻图像帧以对长期和短期上下文进行联合推理。该框架采用多智能体异步协作，并构建了一个包含帧级、摘要级和事件级语义标注的高质量犯罪行为数据集。", "result": "实验结果表明，该方法在多个数据集上取得了优越的性能。", "conclusion": "MA-CBP为城市公共安全场景中的风险预警提供了一个有前景的解决方案。"}}
{"id": "2508.06191", "categories": ["cs.CV", "68T45, 92C55", "I.4.6; I.5.4; J.3"], "pdf": "https://arxiv.org/pdf/2508.06191", "abs": "https://arxiv.org/abs/2508.06191", "authors": ["Ruixiang Tang", "Jianglong Qin", "Mingda Zhang", "Yan Song", "Yi Wu", "Wei Wu"], "title": "A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet", "comment": "12 pages, 6 figures, 2 tables", "summary": "Pleural effusion semantic segmentation can significantly enhance the accuracy\nand timeliness of clinical diagnosis and treatment by precisely identifying\ndisease severity and lesion areas. Currently, semantic segmentation of pleural\neffusion CT images faces multiple challenges. These include similar gray levels\nbetween effusion and surrounding tissues, blurred edges, and variable\nmorphology. Existing methods often struggle with diverse image variations and\ncomplex edges, primarily because direct feature concatenation causes semantic\ngaps. To address these challenges, we propose the Dual-Branch Interactive\nFusion Attention model (DBIF-AUNet). This model constructs a densely nested\nskip-connection network and innovatively refines the Dual-Domain Feature\nDisentanglement module (DDFD). The DDFD module orthogonally decouples the\nfunctions of dual-domain modules to achieve multi-scale feature complementarity\nand enhance characteristics at different levels. Concurrently, we design a\nBranch Interaction Attention Fusion module (BIAF) that works synergistically\nwith the DDFD. This module dynamically weights and fuses global, local, and\nfrequency band features, thereby improving segmentation robustness.\nFurthermore, we implement a nested deep supervision mechanism with hierarchical\nadaptive hybrid loss to effectively address class imbalance. Through validation\non 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet\nachieved IoU and Dice scores of 80.1% and 89.0% respectively. These results\noutperform state-of-the-art medical image segmentation models U-Net++ and\nSwin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant\noptimization in segmentation accuracy for complex pleural effusion CT images.", "AI": {"tldr": "该论文提出DBIF-AUNet模型，通过双域特征解耦和分支交互注意力融合，显著提升了胸腔积液CT图像的语义分割精度，优于现有先进模型。", "motivation": "胸腔积液CT图像语义分割面临多重挑战，包括积液与周围组织灰度相似、边缘模糊、形态多变，以及现有方法直接特征拼接导致的语义鸿沟，难以处理复杂的图像变异和边缘。", "method": "该研究提出了双分支交互融合注意力模型（DBIF-AUNet）。该模型构建了密集嵌套跳跃连接网络，并创新性地引入了双域特征解耦模块（DDFD）以正交解耦双域功能，实现多尺度特征互补。同时，设计了分支交互注意力融合模块（BIAF），动态加权融合全局、局部和频带特征以增强分割鲁棒性。此外，还实现了带有分层自适应混合损失的嵌套深度监督机制，以有效解决类别不平衡问题。", "result": "在西南医院的1622张胸腔积液CT图像数据集上，DBIF-AUNet的IoU和Dice分数分别达到80.1%和89.0%。这些结果优于U-Net++和Swin-UNet等现有先进医学图像分割模型，IoU和Dice分别提高了5.7%/2.7%和2.2%/1.5%。", "conclusion": "DBIF-AUNet模型在复杂胸腔积液CT图像分割中实现了显著的准确性优化，有效克服了现有挑战，有望提升临床诊断和治疗的精准性。"}}
{"id": "2508.06203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06203", "abs": "https://arxiv.org/abs/2508.06203", "authors": ["Zhaopeng Gu", "Bingke Zhu", "Guibo Zhu", "Yingying Chen", "Wei Ge", "Ming Tang", "Jinqiao Wang"], "title": "AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection", "comment": null, "summary": "Anomaly detection is a critical task across numerous domains and modalities,\nyet existing methods are often highly specialized, limiting their\ngeneralizability. These specialized models, tailored for specific anomaly types\nlike textural defects or logical errors, typically exhibit limited performance\nwhen deployed outside their designated contexts. To overcome this limitation,\nwe propose AnomalyMoE, a novel and universal anomaly detection framework based\non a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the\ncomplex anomaly detection problem into three distinct semantic hierarchies:\nlocal structural anomalies, component-level semantic anomalies, and global\nlogical anomalies. AnomalyMoE correspondingly employs three dedicated expert\nnetworks at the patch, component, and global levels, and is specialized in\nreconstructing features and identifying deviations at its designated semantic\nlevel. This hierarchical design allows a single model to concurrently\nunderstand and detect a wide spectrum of anomalies. Furthermore, we introduce\nan Expert Information Repulsion (EIR) module to promote expert diversity and an\nExpert Selection Balancing (ESB) module to ensure the comprehensive utilization\nof all experts. Experiments on 8 challenging datasets spanning industrial\nimaging, 3D point clouds, medical imaging, video surveillance, and logical\nanomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art\nperformance, significantly outperforming specialized methods in their\nrespective domains.", "AI": {"tldr": "AnomalyMoE是一个基于MoE的新型通用异常检测框架，通过将异常分解为局部结构、组件级语义和全局逻辑三个语义层次，并使用相应的专家网络进行检测，同时引入了专家信息排斥和专家选择平衡模块，实现了跨领域异常检测的SOTA性能。", "motivation": "现有异常检测方法高度专业化，针对特定异常类型（如纹理缺陷或逻辑错误）定制，导致其泛化能力受限，在指定上下文之外部署时性能不佳。研究旨在克服这一限制，开发一个更通用、能处理多种异常的框架。", "method": "提出AnomalMoE框架，基于混合专家（MoE）架构。核心思想是将复杂的异常检测问题分解为三个语义层次：局部结构异常、组件级语义异常和全局逻辑异常。AnomalMoE相应地采用三个专用专家网络，分别在补丁、组件和全局级别上，专门重建特征并识别其指定语义级别的偏差。此外，引入专家信息排斥（EIR）模块以促进专家多样性，以及专家选择平衡（ESB）模块以确保所有专家的全面利用。", "result": "在工业成像、3D点云、医学成像、视频监控和逻辑异常检测等8个具有挑战性的数据集上进行实验，AnomalMoE取得了新的最先进性能，显著优于各自领域的专业化方法。", "conclusion": "AnomalMoE通过其分层设计和MoE架构，成功地提供了一个单一模型，能够同时理解和检测广泛的异常类型，并在多个领域建立了新的SOTA性能，证明了其通用性和有效性。"}}
{"id": "2508.06205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06205", "abs": "https://arxiv.org/abs/2508.06205", "authors": ["Ruiyan Wang", "Lin Zuo", "Zonghao Lin", "Qiang Wang", "Zhengxue Cheng", "Rong Xie", "Jun Ling", "Li Song"], "title": "PA-HOI: A Physics-Aware Human and Object Interaction Dataset", "comment": null, "summary": "The Human-Object Interaction (HOI) task explores the dynamic interactions\nbetween humans and objects in physical environments, providing essential\nbiomechanical and cognitive-behavioral foundations for fields such as robotics,\nvirtual reality, and human-computer interaction. However, existing HOI data\nsets focus on details of affordance, often neglecting the influence of physical\nproperties of objects on human long-term motion. To bridge this gap, we\nintroduce the PA-HOI Motion Capture dataset, which highlights the impact of\nobjects' physical attributes on human motion dynamics, including human posture,\nmoving velocity, and other motion characteristics. The dataset comprises 562\nmotion sequences of human-object interactions, with each sequence performed by\nsubjects of different genders interacting with 35 3D objects that vary in size,\nshape, and weight. This dataset stands out by significantly extending the scope\nof existing ones for understanding how the physical attributes of different\nobjects influence human posture, speed, motion scale, and interacting\nstrategies. We further demonstrate the applicability of the PA-HOI dataset by\nintegrating it with existing motion generation methods, validating its capacity\nto transfer realistic physical awareness.", "AI": {"tldr": "引入了PA-HOI运动捕捉数据集，专注于物体物理属性（如尺寸、形状、重量）对人类长期运动（包括姿态、速度、运动尺度）的影响，填补了现有HOI数据集的空白。", "motivation": "现有的人机交互（HOI）数据集主要关注物体的可供性细节，但忽视了物体物理属性对人类长期运动模式（如姿态、移动速度）的深远影响，限制了机器人、虚拟现实等领域对真实世界交互的理解。", "method": "构建了PA-HOI运动捕捉数据集，包含562个运动序列。这些序列由不同性别的受试者与35个在尺寸、形状和重量上各异的3D物体进行交互时捕捉。数据集突出显示了物体物理属性对人类姿态、移动速度及其他运动特征的影响。", "result": "PA-HOI数据集显著扩展了现有数据集的范围，有助于理解不同物体的物理属性如何影响人类的姿态、速度、运动尺度和交互策略。通过与现有运动生成方法结合，该数据集被证明能够传递真实的物理感知。", "conclusion": "PA-HOI数据集为深入理解物体物理属性如何影响人类运动动力学提供了关键数据，为生成更具物理真实感的HOI模型奠定了基础，对机器人、虚拟现实和人机交互等领域具有重要意义。"}}
{"id": "2508.06218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06218", "abs": "https://arxiv.org/abs/2508.06218", "authors": ["Zhiyan Bo", "Laura C. Coates", "Bartlomiej W. Papiez"], "title": "Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning", "comment": "Accepted by MICCAI AMAI Workshop 2025", "summary": "The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials\nto quantify radiographic damage in Rheumatoid Arthritis (RA), but its\ncomplexity has limited its adoption in routine clinical practice. To address\nthe inefficiency of manual scoring, this work proposes a two-stage pipeline for\ninterpretable image-level SvdH score prediction using dual-hand radiographs.\nOur approach extracts disease-relevant image regions and integrates them using\nattention-based multiple instance learning to generate image-level features for\nprediction. We propose two region extraction schemes: 1) sampling image tiles\nmost likely to contain abnormalities, and 2) cropping patches containing\ndisease-relevant joints. With Scheme 2, our best individual score prediction\nmodel achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root\nmean squared error (RMSE) of 15.73. Ensemble learning further boosted\nprediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving\nstate-of-the-art performance that is comparable to that of experienced\nradiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively\nidentified and made decisions based on anatomical structures which clinicians\nconsider relevant to RA progression.", "AI": {"tldr": "本研究提出了一个两阶段、可解释的自动化流程，用于量化类风湿关节炎（RA）的放射学损伤，通过预测Sharp/van der Heijde (SvdH) 评分，其性能可与经验丰富的放射科医生相媲美。", "motivation": "SvdH评分在RA临床试验中广泛使用，但其复杂性限制了在日常临床实践中的应用。手动评分效率低下，因此需要一种自动化的解决方案。", "method": "本研究提出一个两阶段的管道，用于使用双手X光片进行可解释的图像级SvdH评分预测。首先，提取与疾病相关的图像区域；其次，使用基于注意力的多实例学习整合这些区域以生成图像级特征进行预测。提出了两种区域提取方案：1) 采样最可能包含异常的图像块；2) 裁剪包含相关关节的补丁。", "result": "在方案2下，最佳个体评分预测模型实现了0.943的皮尔逊相关系数（PCC）和15.73的均方根误差（RMSE）。集成学习进一步将PCC提高到0.945，RMSE降低到15.57，达到了最先进的性能，与经验丰富的放射科医生（PCC=0.97，RMSE=18.75）的表现相当。此外，该流程能有效识别并基于临床医生认为与RA进展相关的解剖结构做出决策。", "conclusion": "该研究成功开发了一个自动化、高精度且可解释的SvdH评分预测系统，其性能可与经验丰富的放射科医生媲美。这有望解决手动评分的效率问题，并促进SvdH评分在日常临床实践中的应用。"}}
{"id": "2508.06224", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06224", "abs": "https://arxiv.org/abs/2508.06224", "authors": ["Guoyu Zhou", "Jing Zhang", "Yi Yan", "Hui Zhang", "Li Zhuo"], "title": "TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images", "comment": "Submitted to GRSL", "summary": "Semantic segmentation of urban remote sensing images (URSIs) is crucial for\napplications such as urban planning and environmental monitoring. However,\ngeospatial objects often exhibit subtle texture differences and similar spatial\nstructures, which can easily lead to semantic ambiguity and misclassification.\nMoreover, challenges such as irregular object shapes, blurred boundaries, and\noverlapping spatial distributions of semantic objects contribute to complex and\ndiverse edge morphologies, further complicating accurate segmentation. To\ntackle these issues, we propose a texture-aware and edge-guided Transformer\n(TEFormer) that integrates texture awareness and edge-guidance mechanisms for\nsemantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is\ndesigned to capture fine-grained texture differences between visually similar\ncategories to enhance semantic discrimination. Then, an edge-guided tri-branch\ndecoder (Eg3Head) is constructed to preserve local edges and details for\nmultiscale context-awareness. Finally, an edge-guided feature fusion module\n(EgFFM) is to fuse contextual and detail information with edge information to\nrealize refined semantic segmentation. Extensive experiments show that TEFormer\nachieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and\nLoveDA datasets, respectively, shows the effectiveness in URSI semantic\nsegmentation.", "AI": {"tldr": "本文提出了一种名为TEFormer的纹理感知和边缘引导Transformer模型，用于解决城市遥感图像语义分割中因纹理相似性和复杂边缘形态导致的语义模糊和误分类问题，通过集成纹理感知模块和边缘引导解码器，实现了更精确的分割。", "motivation": "城市遥感图像（URSIs）语义分割在城市规划和环境监测中至关重要，但面临挑战：地理空间对象常表现出细微的纹理差异和相似的空间结构，导致语义模糊和误分类；此外，不规则的物体形状、模糊的边界和语义对象的空间重叠分布导致复杂的边缘形态，进一步加剧了精确分割的难度。", "method": "本文提出了一种纹理感知和边缘引导Transformer（TEFormer）。在编码器中，设计了一个纹理感知模块（TaM）以捕获视觉相似类别之间的细粒度纹理差异，增强语义判别能力。然后，构建了一个边缘引导三分支解码器（Eg3Head），用于保留局部边缘和细节，实现多尺度上下文感知。最后，一个边缘引导特征融合模块（EgFFM）被用来融合上下文、细节信息与边缘信息，实现精细化的语义分割。", "result": "TEFormer在Potsdam、Vaihingen和LoveDA数据集上分别取得了88.57%、81.46%和53.55%的mIoU（平均交并比），实验结果表明了其在城市遥感图像语义分割中的有效性。", "conclusion": "TEFormer模型通过整合纹理感知和边缘引导机制，有效解决了城市遥感图像语义分割中的语义模糊和复杂边缘问题，在多个数据集上展现出卓越的性能，证明了其在URSI语义分割领域的有效性。"}}
{"id": "2508.06228", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06228", "abs": "https://arxiv.org/abs/2508.06228", "authors": ["Daniel Feijoo", "Paula Garrido-Mellado", "Jaesung Rim", "Alvaro Garcia", "Marcos V. Conde"], "title": "Towards Unified Image Deblurring using a Mixture-of-Experts Decoder", "comment": "Preprint. Under review", "summary": "Image deblurring, removing blurring artifacts from images, is a fundamental\ntask in computational photography and low-level computer vision. Existing\napproaches focus on specialized solutions tailored to particular blur types,\nthus, these solutions lack generalization. This limitation in current methods\nimplies requiring multiple models to cover several blur types, which is not\npractical in many real scenarios. In this paper, we introduce the first\nall-in-one deblurring method capable of efficiently restoring images affected\nby diverse blur degradations, including global motion, local motion, blur in\nlow-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)\ndecoding module, which dynamically routes image features based on the\nrecognized blur degradation, enabling precise and efficient restoration in an\nend-to-end manner. Our unified approach not only achieves performance\ncomparable to dedicated task-specific models, but also demonstrates remarkable\nrobustness and generalization capabilities on unseen blur degradation\nscenarios.", "AI": {"tldr": "本文提出首个“一体化”图像去模糊方法，能有效处理多种模糊类型（全局运动、局部运动、弱光模糊、散焦模糊），解决了现有方法泛化性差的问题。", "motivation": "现有图像去模糊方法通常专注于特定模糊类型，导致缺乏泛化能力，在实际应用中需要多个模型来覆盖不同模糊，这很不实用。", "method": "引入了一个混合专家（MoE）解码模块，该模块能根据识别出的模糊类型动态路由图像特征，实现端到端、精确且高效的图像恢复。", "result": "所提出的统一方法在性能上与专门的任务特定模型相当，并且在未曾见过的模糊场景下展现出卓越的鲁棒性和泛化能力。", "conclusion": "该“一体化”去模糊方法不仅能处理多种复杂模糊，而且在效率和性能上均表现出色，为图像去模糊领域提供了一个更通用和实用的解决方案。"}}
{"id": "2508.06248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06248", "abs": "https://arxiv.org/abs/2508.06248", "authors": ["Andrii Yermakov", "Jan Cech", "Jiri Matas", "Mario Fritz"], "title": "Deepfake Detection that Generalizes Across Benchmarks", "comment": null, "summary": "The generalization of deepfake detectors to unseen manipulation techniques\nremains a challenge for practical deployment. Although many approaches adapt\nfoundation models by introducing significant architectural complexity, this\nwork demonstrates that robust generalization is achievable through a\nparameter-efficient adaptation of a pre-trained CLIP vision encoder. The\nproposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters\n(0.03% of the total) and enhances generalization by enforcing a hyperspherical\nfeature manifold using L2 normalization and latent space augmentations.\n  We conducted an extensive evaluation on 13 benchmark datasets spanning from\n2019 to 2025. The proposed method achieves state-of-the-art performance,\noutperforming more complex, recent approaches in average cross-dataset AUROC.\nOur analysis yields two primary findings for the field: 1) training on paired\nreal-fake data from the same source video is essential for mitigating shortcut\nlearning and improving generalization, and 2) detection difficulty on academic\ndatasets has not strictly increased over time, with models trained on older,\ndiverse datasets showing strong generalization capabilities.\n  This work delivers a computationally efficient and reproducible method,\nproving that state-of-the-art generalization is attainable by making targeted,\nminimal changes to a pre-trained CLIP model. The code will be made publicly\navailable upon acceptance.", "AI": {"tldr": "本文提出一种参数高效的深度伪造检测方法LNCLIP-DF，通过微调预训练CLIP模型的Layer Normalization参数，结合L2归一化和潜在空间增强，实现了对未知操作技术的强大泛化能力，并在多个基准数据集上达到最先进性能。", "motivation": "深度伪造检测器在实际部署中面临的挑战是难以泛化到未知的伪造技术。现有方法通常通过引入复杂的架构来适应基础模型，但效果不佳。", "method": "所提出的LNCLIP-DF方法仅微调预训练CLIP视觉编码器的Layer Normalization参数（占总参数的0.03%），并通过L2归一化和潜在空间增强来强制执行超球面特征流形，从而增强泛化能力。", "result": "该方法在涵盖2019年至2025年的13个基准数据集上进行了广泛评估，实现了最先进的性能，在平均跨数据集AUROC方面优于更复杂、最新的方法。研究发现：1) 使用来自相同源视频的真实-伪造配对数据对于缓解快捷学习和提高泛化能力至关重要；2) 学术数据集上的检测难度并未严格随时间增加，在较旧、多样化数据集上训练的模型显示出强大的泛化能力。", "conclusion": "这项工作提供了一种计算高效且可复现的方法，证明通过对预训练CLIP模型进行有针对性的、最小的修改，可以实现最先进的泛化能力。"}}
{"id": "2508.06256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06256", "abs": "https://arxiv.org/abs/2508.06256", "authors": ["Barış Büyüktaş", "Jonas Klotz", "Begüm Demir"], "title": "FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing", "comment": null, "summary": "Federated learning (FL) enables the collaborative training of deep neural\nnetworks across decentralized data archives (i.e., clients), where each client\nstores data locally and only shares model updates with a central server. This\nmakes FL a suitable learning paradigm for remote sensing (RS) image\nclassification tasks, where data centralization may be restricted due to legal\nand privacy constraints. However, a key challenge in applying FL to RS tasks is\nthe communication overhead caused by the frequent exchange of large model\nupdates between clients and the central server. To address this issue, in this\npaper we propose a novel strategy (denoted as FedX) that uses\nexplanation-guided pruning to reduce communication overhead by minimizing the\nsize of the transmitted models without compromising performance. FedX leverages\nbackpropagation-based explanation methods to estimate the task-specific\nimportance of model components and prunes the least relevant ones at the\ncentral server. The resulting sparse global model is then sent to clients,\nsubstantially reducing communication overhead. We evaluate FedX on multi-label\nscene classification using the BigEarthNet-S2 dataset and single-label scene\nclassification using the EuroSAT dataset. Experimental results show the success\nof FedX in significantly reducing the number of shared model parameters while\nenhancing the generalization capability of the global model, compared to both\nunpruned model and state-of-the-art pruning methods. The code of FedX will be\navailable at https://git.tu-berlin.de/rsim/FedX.", "AI": {"tldr": "本文提出FedX策略，通过解释引导剪枝减少联邦学习（FL）在遥感（RS）图像分类任务中的通信开销，同时提升模型泛化能力。", "motivation": "遥感图像数据因法律和隐私限制难以集中，联邦学习是合适的范式。然而，FL存在客户端与服务器之间频繁交换大型模型更新导致的通信开销问题。", "method": "FedX策略利用基于反向传播的解释方法，估计模型组件对任务的重要性，并在中央服务器修剪最不相关的部分。生成的稀疏全局模型随后发送给客户端，从而显著减少通信开销。", "result": "在BigEarthNet-S2多标签场景分类和EuroSAT单标签场景分类数据集上评估，FedX显著减少了共享模型参数数量，并增强了全局模型的泛化能力，优于未剪枝模型和现有最先进的剪枝方法。", "conclusion": "FedX通过解释引导剪枝成功解决了联邦学习在遥感任务中的通信开销问题，同时提升了模型性能和泛化能力。"}}
{"id": "2508.06258", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06258", "abs": "https://arxiv.org/abs/2508.06258", "authors": ["Byunghyun Ko", "Anning Tian", "Jeongkyu Lee"], "title": "XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation", "comment": "Accepted at the 2025 International Conference on Artificial\n  Intelligence, Computer, Data Sciences and Applications (ACDSA). This is the\n  preprint version of the paper", "summary": "Accurate segmentation of femur structures from Magnetic Resonance Imaging\n(MRI) is critical for orthopedic diagnosis and surgical planning but remains\nchallenging due to the limitations of existing 2D and 3D deep learning-based\nsegmentation approaches. In this study, we propose XAG-Net, a novel 2.5D\nU-Net-based architecture that incorporates pixel-wise cross-slice attention\n(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice\ncontextual modeling and intra-slice feature refinement. Unlike previous\nCSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent\nslices at each spatial location for fine-grained inter-slice modeling.\nExtensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and\n3D U-Net models in femur segmentation accuracy while maintaining computational\nefficiency. Ablation studies further validate the critical role of the CSA and\nAG modules, establishing XAG-Net as a promising framework for efficient and\naccurate femur MRI segmentation.", "AI": {"tldr": "提出了一种名为XAG-Net的2.5D U-Net模型，通过引入像素级跨层注意力（CSA）和跳跃注意力门控（AG）机制，显著提高了股骨MRI分割的准确性和计算效率。", "motivation": "股骨MRI的精确分割对骨科诊断和手术规划至关重要，但现有2D和3D深度学习分割方法存在局限性，导致分割仍具挑战性。", "method": "本研究提出XAG-Net，一个基于U-Net的2.5D架构。它整合了像素级跨层注意力（CSA）机制以增强层间上下文建模，以及跳跃注意力门控（AG）机制以细化层内特征。XAG-Net的CSA在每个空间位置对相邻切片应用像素级softmax注意力，实现更精细的层间建模。", "result": "广泛评估表明，XAG-Net在股骨分割精度上超越了基线2D、2.5D和3D U-Net模型，同时保持了计算效率。消融研究进一步验证了CSA和AG模块的关键作用。", "conclusion": "XAG-Net被确立为一个用于高效、准确股骨MRI分割的有前景框架。"}}
{"id": "2508.06317", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06317", "abs": "https://arxiv.org/abs/2508.06317", "authors": ["Jian Hu", "Zixu Cheng", "Shaogang Gong", "Isabel Guan", "Jianye Hao", "Jun Wang", "Kun Shao"], "title": "Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding", "comment": null, "summary": "Video Temporal Grounding (TG) aims to temporally locate video segments\nmatching a natural language description (a query) in a long video. While\nVision-Language Models (VLMs) are effective at holistic semantic matching, they\noften struggle with fine-grained temporal localisation. Recently, Group\nRelative Policy Optimisation (GRPO) reformulates the inference process as a\nreinforcement learning task, enabling fine-grained grounding and achieving\nstrong in-domain performance. However, GRPO relies on labelled data, making it\nunsuitable in unlabelled domains. Moreover, because videos are large and\nexpensive to store and process, performing full-scale adaptation introduces\nprohibitive latency and computational overhead, making it impractical for\nreal-time deployment. To overcome both problems, we introduce a Data-Efficient\nUnlabelled Cross-domain Temporal Grounding method, from which a model is first\ntrained on a labelled source domain, then adapted to a target domain using only\na small number of unlabelled videos from the target domain. This approach\neliminates the need for target annotation and keeps both computational and\nstorage overhead low enough to run in real time. Specifically, we introduce.\nUncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain\nknowledge transfer in learning video temporal grounding without target labels.\nURPA generates multiple candidate predictions using GRPO rollouts, averages\nthem to form a pseudo label, and estimates confidence from the variance across\nthese rollouts. This confidence then weights the training rewards, guiding the\nmodel to focus on reliable supervision. Experiments on three datasets across\nsix cross-domain settings show that URPA generalises well using only a few\nunlabelled target videos. Codes will be released once published.", "AI": {"tldr": "本文提出了一种数据高效的无标签跨域视频时间定位方法URPA，通过量化不确定性的策略适应，仅使用少量无标签目标域视频即可实现模型泛化。", "motivation": "现有视频时间定位方法存在局限：视觉-语言模型（VLMs）在细粒度定位上表现不佳；Group Relative Policy Optimisation (GRPO) 依赖于有标签数据，不适用于无标签领域；全尺度适应视频数据会导致过高的延迟和计算开销，不适合实时部署。", "method": "本文提出了不确定性量化回滚策略适应（URPA）方法。该方法首先在有标签源域训练模型，然后仅用少量无标签目标域视频进行适应。URPA通过GRPO回滚生成多个候选预测，平均形成伪标签，并从回滚结果的方差估计置信度。此置信度用于加权训练奖励，引导模型关注可靠的监督信号。", "result": "在三个数据集和六种跨域设置上的实验表明，URPA仅使用少量无标签目标视频就能很好地泛化。", "conclusion": "URPA克服了传统方法对目标域标签的依赖以及高计算和存储开销的问题，实现了数据高效、可实时部署的无标签跨域视频时间定位。"}}
{"id": "2508.06327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06327", "abs": "https://arxiv.org/abs/2508.06327", "authors": ["Xin Ci Wong", "Duygu Sarikaya", "Kieran Zucker", "Marc De Kamps", "Nishant Ravikumar"], "title": "Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?", "comment": "ICONIP 2025", "summary": "Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain\nshift due to variations in imaging devices and acquisition protocols. This\nchallenge limits the deployment of trained AI models in real-world scenarios,\nwhere performance degrades on unseen domains. Traditional solutions involve\nincreasing the size of the dataset through ad-hoc image augmentation or\nadditional online training/transfer learning, which have several limitations.\nSynthetic data offers a promising alternative, but anatomical/structural\nconsistency constraints limit the effectiveness of generative models in\ncreating image-label pairs. To address this, we propose a diffusion model (DM)\ntrained on a source domain that generates synthetic cardiac MR images that\nresemble a given reference. The synthetic data maintains spatial and structural\nfidelity, ensuring similarity to the source domain and compatibility with the\nsegmentation mask. We assess the utility of our generative approach in\nmulti-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and\nvanilla U-Net segmentation networks. We explore domain generalisation, where,\ndomain-invariant segmentation models are trained on synthetic source domain\ndata, and domain adaptation, where, we shift target domain data towards the\nsource domain using the DM. Both strategies significantly improved segmentation\nperformance on data from an unseen target domain, in terms of surface-based\nmetrics (Welch's t-test, p < 0.01), compared to training segmentation models on\nreal data alone. The proposed method ameliorates the need for transfer learning\nor online training to address domain shift challenges in cardiac MR image\nanalysis, especially useful in data-scarce settings.", "AI": {"tldr": "该研究提出一种基于扩散模型的方法，通过生成合成心脏MR图像来解决医学图像分析中的领域漂移问题，显著提高了多中心心脏MR图像分割的性能。", "motivation": "医学MR成像（包括心脏MR）因设备和采集协议差异容易产生领域漂移，导致训练好的AI模型在未见领域性能下降，限制了实际部署。传统解决方案如数据增强或在线训练/迁移学习存在局限性。合成数据有潜力，但生成模型在生成具有解剖一致性的图像-标签对方面面临挑战。", "method": "提出一个在源域上训练的扩散模型（DM），用于生成与给定参考相似的合成心脏MR图像，同时保持空间和结构保真度。该方法用于两种策略：1) 领域泛化，即在合成源域数据上训练领域不变的分割模型；2) 领域适应，即使用DM将目标域数据向源域迁移。评估使用了2D nnU-Net、3D nnU-Net和U-Net分割网络。", "result": "与仅使用真实数据训练分割模型相比，领域泛化和领域适应两种策略均显著提高了在未见目标域数据上的分割性能（基于表面度量，Welch's t-test, p < 0.01）。", "conclusion": "所提出的方法有效解决了心脏MR图像分析中的领域漂移挑战，减少了对迁移学习或在线训练的需求，在数据稀缺的环境中尤其有用。"}}
{"id": "2508.06335", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06335", "abs": "https://arxiv.org/abs/2508.06335", "authors": ["Patrick Takenaka", "Johannes Maucher", "Marco F. Huber"], "title": "ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction", "comment": "Published in 2025 International Joint Conference on Neural Networks\n  (IJCNN)", "summary": "Predicting future video frames is a challenging task with many downstream\napplications. Previous work has shown that procedural knowledge enables deep\nmodels for complex dynamical settings, however their model ViPro assumed a\ngiven ground truth initial symbolic state. We show that this approach led to\nthe model learning a shortcut that does not actually connect the observed\nenvironment with the predicted symbolic state, resulting in the inability to\nestimate states given an observation if previous states are noisy. In this\nwork, we add several improvements to ViPro that enables the model to correctly\ninfer states from observations without providing a full ground truth state in\nthe beginning. We show that this is possible in an unsupervised manner, and\nextend the original Orbits dataset with a 3D variant to close the gap to real\nworld scenarios.", "AI": {"tldr": "本文改进了ViPro模型，使其能够在无初始真值符号状态的情况下，以无监督方式从观测中正确推断状态，从而更好地进行视频帧预测，并扩展了数据集以适应更真实的场景。", "motivation": "之前的视频帧预测模型（如ViPro）依赖于给定的初始真值符号状态，这导致模型学习到捷径，无法将观测环境与预测的符号状态有效关联，从而在之前状态有噪声时无法从观测中准确估计状态。", "method": "对ViPro模型进行了多项改进，使其能够在不提供完整初始真值状态的情况下，从观测中正确推断状态。该方法以无监督方式实现，并扩展了原始Orbits数据集，增加了3D变体以缩小与现实场景的差距。", "result": "改进后的模型能够以无监督方式，在没有初始真值状态的情况下，从观测中正确推断出符号状态，解决了现有模型的问题。", "conclusion": "通过对ViPro的改进，证明了在复杂动态环境中，无需初始真值状态也能从观测中有效推断符号状态进行视频预测，且该方法可实现无监督学习，提升了模型在真实世界场景中的鲁棒性。"}}
{"id": "2508.06342", "categories": ["cs.CV", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.06342", "abs": "https://arxiv.org/abs/2508.06342", "authors": ["Kieran Elrod", "Katherine Flanigan", "Mario Bergés"], "title": "Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities", "comment": null, "summary": "Designing socially active streets has long been a goal of urban planning, yet\nexisting quantitative research largely measures pedestrian volume rather than\nthe quality of social interactions. We hypothesize that street view imagery --\nan inexpensive data source with global coverage -- contains latent social\ninformation that can be extracted and interpreted through established social\nscience theory. As a proof of concept, we analyzed 2,998 street view images\nfrom 15 cities using a multimodal large language model guided by Mehta's\ntaxonomy of passive, fleeting, and enduring sociability -- one illustrative\nexample of a theory grounded in urban design that could be substituted or\ncomplemented by other sociological frameworks. We then used linear regression\nmodels, controlling for factors like weather, time of day, and pedestrian\ncounts, to test whether the inferred sociability measures correlate with\ncity-level place attachment scores from the World Values Survey and with\nenvironmental predictors (e.g., green, sky, and water view indices) derived\nfrom individual street view images. Results aligned with long-standing urban\nplanning theory: the sky view index was associated with all three sociability\ntypes, the green view index predicted enduring sociability, and place\nattachment was positively associated with fleeting sociability. These results\nprovide preliminary evidence that street view images can be used to infer\nrelationships between specific types of social interactions and built\nenvironment variables. Further research could establish street view imagery as\na scalable, privacy-preserving tool for studying urban sociability, enabling\ncross-cultural theory testing and evidence-based design of socially vibrant\ncities.", "AI": {"tldr": "本研究利用街景图像和多模态大型语言模型，结合社会科学理论，量化街道的社交活跃度，并发现其与城市环境因素和地方依恋度相关联，为城市规划提供了可扩展的新工具。", "motivation": "城市规划长期以来致力于设计社交活跃的街道，但现有量化研究主要衡量行人数量而非社会互动质量。研究者假设街景图像这一廉价且全球覆盖的数据源，蕴含潜在的社会信息，可通过既有社会科学理论进行提取和解读。", "method": "研究以概念验证形式，分析了来自15个城市的2,998张街景图像。使用多模态大型语言模型，并以Mehta的社交性分类（被动、短暂、持久）作为指导，提取图像中的社交信息。随后，采用线性回归模型，在控制天气、时间、行人数量等因素后，检验推断的社交性指标是否与世界价值观调查中的城市级地方依恋分数以及从街景图像中提取的环境预测因子（如绿色、天空、水体视野指数）相关。", "result": "研究结果与长期存在的城市规划理论相符：天空视野指数与所有三种社交性类型均相关；绿色视野指数预测了持久性社交性；地方依恋度与短暂性社交性呈正相关。", "conclusion": "这些结果提供了初步证据，表明街景图像可用于推断特定社会互动类型与建成环境变量之间的关系。未来的研究可将街景图像确立为一种可扩展、保护隐私的城市社交性研究工具，从而促进跨文化理论检验和基于证据的社交活力城市设计。"}}
{"id": "2508.06350", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06350", "abs": "https://arxiv.org/abs/2508.06350", "authors": ["Yingxian Chen", "Jiahui Liu", "Ruifan Di", "Yanwei Li", "Chirui Chang", "Shizhen Zhao", "Wilton W. T. Fok", "Xiaojuan Qi", "Yik-Chung Wu"], "title": "Aligning Effective Tokens with Video Anomaly in Large Language Models", "comment": null, "summary": "Understanding abnormal events in videos is a vital and challenging task that\nhas garnered significant attention in a wide range of applications. Although\ncurrent video understanding Multi-modal Large Language Models (MLLMs) are\ncapable of analyzing general videos, they often struggle to handle anomalies\ndue to the spatial and temporal sparsity of abnormal events, where the\nredundant information always leads to suboptimal outcomes. To address these\nchallenges, exploiting the representation and generalization capabilities of\nVison Language Models (VLMs) and Large Language Models (LLMs), we propose\nVA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in\nvarious videos. Our approach efficiently aligns effective tokens between visual\nencoders and LLMs through two key proposed modules: Spatial Effective Token\nSelection (SETS) and Temporal Effective Token Generation (TETG). These modules\nenable our model to effectively capture and analyze both spatial and temporal\ninformation associated with abnormal events, resulting in more accurate\nresponses and interactions. Furthermore, we construct an instruction-following\ndataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a\ncross-domain evaluation benchmark based on XD-Violence dataset. Our proposed\nmethod outperforms existing state-of-the-art methods on various benchmarks.", "AI": {"tldr": "本文提出VA-GPT，一个新型多模态大语言模型（MLLM），用于视频中异常事件的总结和定位，通过空间有效令牌选择（SETS）和时间有效令牌生成（TETG）模块，有效处理异常事件的空间和时间稀疏性。", "motivation": "当前视频理解的多模态大语言模型（MLLM）在处理异常事件时面临挑战，因为异常事件的空间和时间稀疏性导致冗余信息影响性能。", "method": "作者提出了VA-GPT，一个利用视觉语言模型（VLM）和大语言模型（LLM）能力的新型MLLM。它包含两个关键模块：空间有效令牌选择（SETS）和时间有效令牌生成（TETG），以有效对齐视觉编码器和LLM之间的有效令牌。此外，构建了一个专门用于微调视频异常感知MLLM的指令遵循数据集，并基于XD-Violence数据集引入了一个跨域评估基准。", "result": "所提出的方法在各种基准测试中均优于现有的最先进方法。", "conclusion": "VA-GPT通过其独特的模块和训练策略，能有效捕捉和分析与异常事件相关的空间和时间信息，从而实现更准确的响应和交互，显著提升了异常事件视频理解的性能。"}}
{"id": "2508.06351", "categories": ["cs.CV", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06351", "abs": "https://arxiv.org/abs/2508.06351", "authors": ["Olakunle S. Abawonse", "Günay Doğan"], "title": "An Implemention of Two-Phase Image Segmentation using the Split Bregman Method", "comment": "15 pages", "summary": "In this paper, we describe an implementation of the two-phase image\nsegmentation algorithm proposed by Goldstein, Bresson, Osher in\n\\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into\nforeground and background regions, and each pixel of the image is assigned\nmembership to one of these two regions. The underlying assumption for the\nsegmentation model is that the pixel values of the input image can be\nsummarized by two distinct average values, and that the region boundaries are\nsmooth. Accordingly, the model is defined as an energy in which the variable is\na region membership function to assign pixels to either region, originally\nproposed by Chan and Vese in \\cite{chan:vese}. This energy is the sum of image\ndata terms in the regions and a length penalty for region boundaries.\nGoldstein, Bresson, Osher modify the energy of Chan-Vese in \\cite{gold:bre} so\nthat their new energy can be minimized efficiently using the split Bregman\nmethod to produce an equivalent two-phase segmentation. We provide a detailed\nimplementation of this method \\cite{gold:bre}, and document its performance\nwith several images over a range of algorithm parameters.", "AI": {"tldr": "本文描述并实现了Goldstein、Bresson、Osher提出的两阶段图像分割算法，该算法通过分体Bregman方法高效最小化Chan-Vese能量函数的变体。", "motivation": "Goldstein、Bresson、Osher提出的两阶段图像分割算法能够利用分体Bregman方法高效地最小化能量函数，其基本假设是图像像素值可由两个不同的平均值概括，且区域边界平滑。本文旨在提供该方法的详细实现并记录其性能。", "method": "该方法基于Chan-Vese模型，将图像分割为前景和背景区域，为每个像素分配区域归属。能量函数由区域内图像数据项和区域边界长度惩罚项组成。Goldstein、Bresson、Osher对Chan-Vese能量进行了修改，使其能通过分体Bregman方法高效最小化，从而实现等效的两阶段分割。本文提供了该方法的详细实现。", "result": "本文提供了Goldstein、Bresson、Osher方法的详细实现，并记录了该算法在多张图像和不同算法参数范围下的性能。", "conclusion": "成功实现并验证了Goldstein、Bresson、Osher提出的基于分体Bregman方法的两阶段图像分割算法，并展示了其在不同图像上的性能。"}}
{"id": "2508.06382", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06382", "abs": "https://arxiv.org/abs/2508.06382", "authors": ["Xiangyu Wu", "Feng Yu", "Yang Yang", "Jianfeng Lu"], "title": "Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning", "comment": "Accepted for publication at ACMMM 2025", "summary": "The integration of prompt tuning with multimodal learning has shown\nsignificant generalization abilities for various downstream tasks. Despite\nadvancements, existing methods heavily depend on massive modality-specific\nlabeled data (e.g., video, audio, and image), or are customized for a single\nmodality. In this study, we present Text as Any-Modality by Consistent Prompt\nTuning (TaAM-CPT), a scalable approach for constructing a general\nrepresentation model toward unlimited modalities using solely text data.\nTaAM-CPT comprises modality prompt pools, text construction, and\nmodality-aligned text encoders from pre-trained models, which allows for\nextending new modalities by simply adding prompt pools and modality-aligned\ntext encoders. To harmonize the learning across different modalities, TaAM-CPT\ndesigns intra- and inter-modal learning objectives, which can capture category\ndetails within modalities while maintaining semantic consistency across\ndifferent modalities. Benefiting from its scalable architecture and pre-trained\nmodels, TaAM-CPT can be seamlessly extended to accommodate unlimited\nmodalities. Remarkably, without any modality-specific labeled data, TaAM-CPT\nachieves leading results on diverse datasets spanning various modalities,\nincluding video classification, image classification, and audio classification.\nThe code is available at https://github.com/Jinx630/TaAM-CPT.", "AI": {"tldr": "TaAM-CPT是一种可扩展的通用表示模型，仅使用文本数据即可学习无限模态的表示，无需大量特定模态的标注数据，并在多模态任务上取得了领先结果。", "motivation": "现有多模态学习方法过度依赖大量特定模态的标注数据（如视频、音频、图像），或仅针对单一模态定制，限制了其泛化能力和扩展性。", "method": "TaAM-CPT（Text as Any-Modality by Consistent Prompt Tuning）通过模态提示池、文本构建和来自预训练模型的模态对齐文本编码器来构建通用表示。它设计了模态内和模态间学习目标，以捕获模态内细节并保持跨模态语义一致性。该架构可无缝扩展以适应无限模态。", "result": "TaAM-CPT在不使用任何特定模态标注数据的情况下，在多种数据集上（包括视频分类、图像分类和音频分类）取得了领先结果，证明了其优越的泛化能力和可扩展性。", "conclusion": "TaAM-CPT提供了一种有效且可扩展的通用多模态表示学习方法，克服了传统方法对大量模态特定标注数据的依赖，仅通过文本数据即可实现对无限模态的支持。"}}
{"id": "2508.06392", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06392", "abs": "https://arxiv.org/abs/2508.06392", "authors": ["Wenbin Teng", "Gonglin Chen", "Haiwei Chen", "Yajie Zhao"], "title": "FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation", "comment": null, "summary": "Recent progress in 3D reconstruction has enabled realistic 3D models from\ndense image captures, yet challenges persist with sparse views, often leading\nto artifacts in unseen areas. Recent works leverage Video Diffusion Models\n(VDMs) to generate dense observations, filling the gaps when only sparse views\nare available for 3D reconstruction tasks. A significant limitation of these\nmethods is their slow sampling speed when using VDMs. In this paper, we present\nFVGen, a novel framework that addresses this challenge by enabling fast novel\nview synthesis using VDMs in as few as four sampling steps. We propose a novel\nvideo diffusion model distillation method that distills a multi-step denoising\nteacher model into a few-step denoising student model using Generative\nAdversarial Networks (GANs) and softened reverse KL-divergence minimization.\nExtensive experiments on real-world datasets show that, compared to previous\nworks, our framework generates the same number of novel views with similar (or\neven better) visual quality while reducing sampling time by more than 90%.\nFVGen significantly improves time efficiency for downstream reconstruction\ntasks, particularly when working with sparse input views (more than 2) where\npre-trained VDMs need to be run multiple times to achieve better spatial\ncoverage.", "AI": {"tldr": "FVGen是一个新颖的框架，通过蒸馏视频扩散模型（VDM）实现快速新颖视图合成，显著加速了稀疏视图下的3D重建任务。", "motivation": "3D重建在稀疏视图下仍面临挑战，现有基于视频扩散模型（VDM）的方法虽然能生成密集观测，但采样速度慢是其主要限制。", "method": "FVGen提出了一种新颖的视频扩散模型蒸馏方法，将多步去噪教师模型蒸馏成几步去噪学生模型。该方法结合了生成对抗网络（GANs）和软化逆KL散度最小化。", "result": "相比于现有工作，FVGen在相似（或更好）的视觉质量下生成相同数量的新颖视图，同时采样时间减少了90%以上。这显著提高了下游重建任务的时间效率，尤其是在稀疏输入视图（超过2个）的情况下。", "conclusion": "FVGen通过加速VDM在新颖视图合成中的应用，极大地提升了稀疏视图下3D重建的时间效率和实用性。"}}
{"id": "2508.06420", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06420", "abs": "https://arxiv.org/abs/2508.06420", "authors": ["Ch Muhammad Awais", "Marco Reggiannini", "Davide Moroni", "Oktay Karakus"], "title": "Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification", "comment": "Accepted and presented at IGARSS", "summary": "SAR ship classification faces the challenge of long-tailed datasets, which\ncomplicates the classification of underrepresented classes. Oversampling\nmethods have proven effective in addressing class imbalance in optical data. In\nthis paper, we evaluated the effect of oversampling in the feature space for\nSAR ship classification. We propose two novel algorithms inspired by the\nMajor-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two\npublic datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three\nstate-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.\nAdditionally, we also analyzed the impact of oversampling methods on different\nclass sizes. The results demonstrated the effectiveness of our novel methods\nover the original M2m and baselines, with an average F1-score increase of 8.82%\nfor FuSARShip and 4.44% for OpenSARShip.", "AI": {"tldr": "该研究针对SAR舰船分类中的长尾数据问题，提出并评估了两种新的特征空间过采样算法M2m_f和M2m_u，在两个公开数据集上显著提升了分类性能。", "motivation": "SAR舰船分类面临长尾数据集挑战，导致难以对代表性不足的类别进行有效分类。光学数据中过采样方法已被证明有效，因此希望将其应用于SAR数据的特征空间以解决此类不平衡问题。", "method": "研究评估了在特征空间进行过采样对SAR舰船分类的影响。提出了两种受M2m方法启发的算法M2m_f和M2m_u。在OpenSARShip（6类）和FuSARShip（9类）两个公开数据集上进行测试，使用ViT、VGG16和ResNet50三种先进模型作为特征提取器。此外，还分析了过采样方法对不同类别规模的影响。", "result": "实验结果表明，所提出的新型方法M2m_f和M2m_u优于原始M2m方法和基线模型。FuSARShip数据集的平均F1分数提高了8.82%，OpenSARShip数据集提高了4.44%。", "conclusion": "所提出的新型特征空间过采样算法M2m_f和M2m_u在解决SAR舰船分类的长尾问题上表现出显著的有效性，能够有效提升对代表性不足类别的分类性能。"}}
{"id": "2508.06430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06430", "abs": "https://arxiv.org/abs/2508.06430", "authors": ["Om Patil", "Jinesh Modi", "Suryabha Mukhopadhyay", "Meghaditya Giri", "Chhavi Malhotra"], "title": "MotionSwap", "comment": "8 pages, 7 figures, 5 tables. This is a student research submission\n  from BITS Pilani, Hyderabad Campus. Our implementation enhances SimSwap with\n  attention modules and dynamic training strategies", "summary": "Face swapping technology has gained significant attention in both academic\nresearch and commercial applications. This paper presents our implementation\nand enhancement of SimSwap, an efficient framework for high fidelity face\nswapping. We introduce several improvements to the original model, including\nthe integration of self and cross-attention mechanisms in the generator\narchitecture, dynamic loss weighting, and cosine annealing learning rate\nscheduling. These enhancements lead to significant improvements in identity\npreservation, attribute consistency, and overall visual quality.\n  Our experimental results, spanning 400,000 training iterations, demonstrate\nprogressive improvements in generator and discriminator performance. The\nenhanced model achieves better identity similarity, lower FID scores, and\nvisibly superior qualitative results compared to the baseline. Ablation studies\nconfirm the importance of each architectural and training improvement. We\nconclude by identifying key future directions, such as integrating StyleGAN3,\nimproving lip synchronization, incorporating 3D facial modeling, and\nintroducing temporal consistency for video-based applications.", "AI": {"tldr": "本文对SimSwap换脸框架进行了改进和增强，引入了注意力机制、动态损失权重和学习率调度，显著提升了换脸质量和身份保持性。", "motivation": "换脸技术在学术和商业领域受到广泛关注，作者旨在改进现有高效的SimSwap框架，以实现更高保真度的换脸效果。", "method": "在SimSwap生成器架构中集成了自注意力与交叉注意力机制，引入了动态损失权重，并采用了余弦退火学习率调度策略。", "result": "实验结果显示，改进后的模型在身份保持、属性一致性和整体视觉质量上显著提升。与基线模型相比，实现了更好的身份相似度、更低的FID分数和肉眼可见的优质结果。消融研究证实了各项改进的重要性。", "conclusion": "所提出的增强方法有效提升了SimSwap的性能。未来工作方向包括整合StyleGAN3、改进唇形同步、引入3D面部建模以及增强视频应用的时间一致性。"}}
{"id": "2508.06452", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06452", "abs": "https://arxiv.org/abs/2508.06452", "authors": ["Mattia Litrico", "Mario Valerio Giuffrida", "Sebastiano Battiato", "Devis Tuia"], "title": "TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation", "comment": null, "summary": "Recent unsupervised domain adaptation (UDA) methods have shown great success\nin addressing classical domain shifts (e.g., synthetic-to-real), but they still\nsuffer under complex shifts (e.g. geographical shift), where both the\nbackground and object appearances differ significantly across domains. Prior\nworks showed that the language modality can help in the adaptation process,\nexhibiting more robustness to such complex shifts. In this paper, we introduce\nTRUST, a novel UDA approach that exploits the robustness of the language\nmodality to guide the adaptation of a vision model. TRUST generates\npseudo-labels for target samples from their captions and introduces a novel\nuncertainty estimation strategy that uses normalised CLIP similarity scores to\nestimate the uncertainty of the generated pseudo-labels. Such estimated\nuncertainty is then used to reweight the classification loss, mitigating the\nadverse effects of wrong pseudo-labels obtained from low-quality captions. To\nfurther increase the robustness of the vision model, we propose a multimodal\nsoft-contrastive learning loss that aligns the vision and language feature\nspaces, by leveraging captions to guide the contrastive training of the vision\nmodel on target images. In our contrastive loss, each pair of images acts as\nboth a positive and a negative pair and their feature representations are\nattracted and repulsed with a strength proportional to the similarity of their\ncaptions. This solution avoids the need for hardly determining positive and\nnegative pairs, which is critical in the UDA setting. Our approach outperforms\nprevious methods, setting the new state-of-the-art on classical (DomainNet) and\ncomplex (GeoNet) domain shifts. The code will be available upon acceptance.", "AI": {"tldr": "本文提出了一种名为TRUST的无监督域适应（UDA）方法，通过利用语言模态的鲁棒性来指导视觉模型的适应，并引入了基于CLIP相似度的不确定性估计策略和多模态软对比学习损失，以应对复杂域偏移。", "motivation": "现有无监督域适应方法在复杂域偏移（如地理偏移）下表现不佳，因为背景和物体外观差异显著。先前的研究表明，语言模态对这种复杂偏移表现出更强的鲁棒性，因此可以帮助域适应过程。", "method": "TRUST方法通过以下方式实现：1) 利用图像标题为目标样本生成伪标签；2) 引入一种新颖的不确定性估计策略，使用归一化的CLIP相似度分数来评估伪标签的不确定性，并用此不确定性重加权分类损失，以减轻错误伪标签的影响；3) 提出一种多模态软对比学习损失，通过利用标题引导视觉模型在目标图像上的对比训练，对齐视觉和语言特征空间。该对比损失允许图像对同时作为正负对，吸引和排斥强度与标题相似度成比例，避免了在UDA中难以确定正负对的问题。", "result": "该方法在经典域偏移数据集（DomainNet）和复杂域偏移数据集（GeoNet）上均超越了现有方法，达到了新的最先进水平（state-of-the-art）。", "conclusion": "TRUST成功地利用语言模态的鲁棒性，通过改进伪标签生成和引入创新的多模态软对比学习，显著提升了视觉模型在复杂无监督域适应任务中的性能。"}}
{"id": "2508.06494", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06494", "abs": "https://arxiv.org/abs/2508.06494", "authors": ["Yehonathan Litman", "Fernando De la Torre", "Shubham Tulsiani"], "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion", "comment": "ICCV 2025, Project page & Code:\n  https://yehonathanlitman.github.io/light_switch/", "summary": "Recent approaches for 3D relighting have shown promise in integrating 2D\nimage relighting generative priors to alter the appearance of a 3D\nrepresentation while preserving the underlying structure. Nevertheless,\ngenerative priors used for 2D relighting that directly relight from an input\nimage do not take advantage of intrinsic properties of the subject that can be\ninferred or cannot consider multi-view data at scale, leading to subpar\nrelighting. In this paper, we propose Lightswitch, a novel finetuned\nmaterial-relighting diffusion framework that efficiently relights an arbitrary\nnumber of input images to a target lighting condition while incorporating cues\nfrom inferred intrinsic properties. By using multi-view and material\ninformation cues together with a scalable denoising scheme, our method\nconsistently and efficiently relights dense multi-view data of objects with\ndiverse material compositions. We show that our 2D relighting prediction\nquality exceeds previous state-of-the-art relighting priors that directly\nrelight from images. We further demonstrate that LightSwitch matches or\noutperforms state-of-the-art diffusion inverse rendering methods in relighting\nsynthetic and real objects in as little as 2 minutes.", "AI": {"tldr": "Lightswitch是一个新颖的微调材质-重照明扩散框架，它能高效地将任意数量的输入图像重照明到目标光照条件，同时结合推断的内在属性和多视角信息，实现了卓越的2D重照明和3D逆渲染效果。", "motivation": "现有的3D重照明方法虽然能结合2D图像重照明生成先验，但由于未利用主体的内在属性或无法大规模处理多视角数据，导致重照明效果不佳。", "method": "Lightswitch提出了一种新颖的微调材质-重照明扩散框架，该框架结合了推断的内在属性、多视角和材质信息，并采用可扩展的去噪方案，以一致且高效的方式对具有不同材质组成的密集多视角对象数据进行重照明。", "result": "Lightswitch在2D重照明预测质量上超越了直接从图像进行重照明的现有最先进先验。此外，它在短短2分钟内就能匹配或超越最先进的扩散逆渲染方法，在合成和真实对象的重照明方面表现出色。", "conclusion": "Lightswitch通过结合内在属性和多视角信息，提供了一种高效且高质量的3D重照明解决方案，显著提升了2D重照明和3D逆渲染的性能。"}}
