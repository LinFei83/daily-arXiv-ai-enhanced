{"id": "2507.14249", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14249", "abs": "https://arxiv.org/abs/2507.14249", "authors": ["Yuejiao Xie", "Maonan Wang", "Di Zhou", "Man-On Pun", "Zhu Han"], "title": "Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach", "comment": null, "summary": "Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions\nto alleviate urban congestion, with path planning becoming a key focus area.\nUnlike ground transportation, UAM trajectory planning has to prioritize\ncommunication quality for accurate location tracking in constantly changing\nenvironments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,\nrequires adaptive planning to respond to real-time passenger requests,\nespecially in ride-sharing scenarios where passenger demands are unpredictable\nand dynamic. However, conventional trajectory planning strategies based on\npredefined routes lack the flexibility to meet varied passenger ride demands.\nTo address these challenges, this work first proposes constructing a radio map\nto evaluate the communication quality of urban airspace. Building on this, we\nintroduce a novel Multi-Source Hybrid Attention Reinforcement Learning\n(MSHA-RL) framework for the challenge of effectively focusing on passengers and\nUAM locations, which arises from the significant dimensional disparity between\nthe representations. This model first generates the alignment among diverse\ndata sources with large gap dimensions before employing hybrid attention to\nbalance global and local insights, thereby facilitating responsive, real-time\npath planning. Extensive experimental results demonstrate that the approach\nenables communication-compliant trajectory planning, reducing travel time and\nenhancing operational efficiency while prioritizing passenger safety.", "AI": {"tldr": "针对城市空中交通（UAM）路径规划中通信质量和实时乘客需求的挑战，本文提出构建无线电地图以评估通信质量，并引入多源混合注意力强化学习（MSHA-RL）框架，以实现安全、高效且灵活的UAM轨迹规划。", "motivation": "城市空中交通（UAM）系统在路径规划方面面临挑战：UAM轨迹规划需要优先确保通信质量以保障安全；作为空中出租车，UAM系统需自适应响应实时、动态的乘客需求（尤其在拼车场景中），而传统的预定义路线规划策略缺乏满足这些多变需求的灵活性。", "method": "首先，构建无线电地图以评估城市空域的通信质量。其次，提出一种新颖的多源混合注意力强化学习（MSHA-RL）框架，以解决乘客和UAM位置表示维度差异大的问题。该模型通过在不同数据源之间生成对齐，并采用混合注意力机制平衡全局和局部洞察，从而实现响应式、实时的路径规划。", "result": "实验结果表明，该方法能够实现符合通信要求的轨迹规划，显著减少了旅行时间，提高了运营效率，同时优先保障了乘客安全。", "conclusion": "本研究提出的结合无线电地图和MSHA-RL框架的方法，能够有效解决UAM路径规划中通信质量保障和实时乘客需求响应的挑战，从而实现安全、高效且灵活的UAM轨迹规划。"}}
{"id": "2507.14274", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.14274", "abs": "https://arxiv.org/abs/2507.14274", "authors": ["Andreas Mueller", "Shivesh Kumar", "Thomas Kordik"], "title": "A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators", "comment": null, "summary": "Series elastic actuators (SEA) were introduced for serial robotic arms. Their\nmodel-based trajectory tracking control requires the second time derivatives of\nthe inverse dynamics solution, for which algorithms were proposed. Trajectory\ncontrol of parallel kinematics manipulators (PKM) equipped with SEAs has not\nyet been pursued. Key element for this is the computationally efficient\nevaluation of the second time derivative of the inverse dynamics solution. This\nhas not been presented in the literature, and is addressed in the present paper\nfor the first time. The special topology of PKM is exploited reusing the\nrecursive algorithms for evaluating the inverse dynamics of serial robots. A\nLie group formulation is used and all relations are derived within this\nframework. Numerical results are presented for a 6-DOF Gough-Stewart platform\n(as part of an exoskeleton), and for a planar PKM when a flatness-based control\nscheme is applied.", "AI": {"tldr": "本文首次提出并解决了串联弹性驱动器（SEA）驱动的并联机器人（PKM）轨迹跟踪控制中，逆动力学解的二阶时间导数的高效计算问题。", "motivation": "串联弹性驱动器（SEA）已被用于串联机器人臂的轨迹跟踪控制，这需要逆动力学解的二阶时间导数。然而，对于配备SEA的并联机器人（PKM）的轨迹控制尚未被研究，其关键在于高效计算逆动力学解的二阶时间导数，这在现有文献中是空白。", "method": "利用PKM的特殊拓扑结构，重用串联机器人逆动力学评估的递归算法。所有关系均在李群（Lie group）框架内推导。", "result": "提出了计算并联机器人逆动力学解的二阶时间导数的高效方法。通过对一个6自由度Gough-Stewart平台（作为外骨骼的一部分）和一个平面PKM应用基于平坦度的控制方案，展示了数值结果。", "conclusion": "本文首次解决了配备SEA的并联机器人轨迹控制中逆动力学解二阶时间导数的高效计算问题，为未来PKM的轨迹控制奠定了基础。"}}
{"id": "2507.14412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14412", "abs": "https://arxiv.org/abs/2507.14412", "authors": ["Mengxue Fu", "Zhonghao Shi", "Minyu Huang", "Siqi Liu", "Mina Kian", "Yirui Song", "Maja J. Matarić"], "title": "Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support", "comment": null, "summary": "Socially assistive robots (SARs) have shown great potential for supplementing\nwell-being support. However, prior studies have found that existing dialogue\npipelines for SARs remain limited in real-time latency, back-channeling, and\npersonalized speech dialogue. Toward addressing these limitations, we propose\nusing integrated end-to-end speech-language models (SLMs) with SARs. This work\n1) evaluated the usability of an SLM-enabled SAR dialogue system through a\nsmall user study, and 2) identified remaining limitations through study user\nfeedback to inform future improvements. We conducted a small within-participant\nuser study with university students (N = 11) whose results showed that\nparticipants perceived an SLM-enabled SAR system as capable of providing\nempathetic feedback, natural turn-taking, back-channeling, and adaptive\nresponses. We also found that participants reported the robot's nonverbal\nbehaviors as lacking variability and synchronization with conversation, and the\nSLM's verbal feedback as generic and repetitive. These findings highlighted the\nneed for real-time robot movement synchronized with conversation, improved\nprompting or fine-tuning to generate outputs better aligned with mental health\npractices, and more expressive, adaptive vocal generation.", "AI": {"tldr": "该研究评估了集成端到端语音语言模型（SLM）的社交辅助机器人（SAR）对话系统，发现其在同理心和自然对话方面表现良好，但非语言行为和语言反馈仍需改进。", "motivation": "现有社交辅助机器人（SAR）的对话系统在实时延迟、背景信息反馈和个性化语音对话方面存在局限性。", "method": "本研究提出将集成端到端语音语言模型（SLM）应用于SAR。通过一项小规模的参与者内部用户研究（N=11名大学生），评估了启用SLM的SAR对话系统的可用性，并通过用户反馈识别了现有局限性。", "result": "研究结果显示，参与者认为启用SLM的SAR系统能够提供富有同理心的反馈、自然的轮流发言、背景信息反馈和自适应响应。然而，参与者也指出机器人的非语言行为缺乏多样性和与对话的同步性，且SLM的语言反馈过于通用和重复。", "conclusion": "研究强调需要改进机器人的实时动作与对话同步，优化SLM的提示或微调以更好地符合心理健康实践，并生成更具表现力、适应性更强的语音。"}}
{"id": "2507.14455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14455", "abs": "https://arxiv.org/abs/2507.14455", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking", "comment": null, "summary": "Time-delay embedding is a technique that uses snapshots of state history over\ntime to build a linear state space model of a nonlinear smooth system. We\ndemonstrate that periodic non-smooth or hybrid system can also be modeled as a\nlinear state space system using this approach as long as its behavior is\nconsistent in modes and timings. We extended time-delay embeddings to generate\na linear model of two periodic hybrid systems: the bouncing pendulum and the\nsimplest walker with control inputs. This leads to a novel state history\naugmented linear quadratic regulator (LQR) which uses current and past state\nhistory for feedback control.", "AI": {"tldr": "本文提出将时间延迟嵌入技术扩展到周期性非光滑或混合系统，以构建线性状态空间模型，并开发了一种基于状态历史的线性二次调节器（LQR）进行反馈控制。", "motivation": "传统的时延嵌入技术主要用于光滑非线性系统。本研究旨在探索该技术是否能应用于周期性非光滑或混合系统，并在此基础上实现有效的控制。", "method": "扩展了时间延迟嵌入方法，将其应用于周期性非光滑或混合系统（如弹跳摆和带控制输入的简易步行机器人），并基于此构建了一种新颖的、利用当前和过去状态历史进行反馈控制的线性二次调节器（LQR）。", "result": "只要行为在模式和时间上保持一致，周期性非光滑或混合系统也可以通过扩展的时间延迟嵌入方法建模为线性状态空间系统。成功地为弹跳摆和简易步行机器人建立了线性模型，并开发了基于状态历史的LQR。", "conclusion": "时间延迟嵌入技术可以有效应用于周期性非光滑或混合系统，以建立线性模型，从而实现基于状态历史增强的线性二次调节器（LQR）控制。"}}
{"id": "2507.14300", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14300", "abs": "https://arxiv.org/abs/2507.14300", "authors": ["Marcelo Jacinto", "Pedro Trindade", "Francisco Rego", "Rita Cunha"], "title": "Distributed consensus-based observer design for target state estimation with bearing measurements", "comment": null, "summary": "This paper introduces a novel distributed consensus-based observer design\nthat enables a group of agents in an undirected communication network to solve\nthe problem of target tracking, where the target is modeled as a chain of\nintegrators of arbitrary order. Each agent is assumed to know its own position\nand simultaneously measure bearing vectors relative to the target. We start by\nintroducing a general continuous time observer design tailored to systems whose\nstate dynamics are modeled as chains of integrators and whose measurement model\nfollows a particular nonlinear but observer-suited form. This design leverages\na correction term that combines innovation and consensus components, allowing\neach agent to broadcast only a part of the state estimate to its neighbours,\nwhich effectively reduces the data flowing across the network. To provide\nuniform exponential stability guarantees, a novel result for a class of\nnonlinear closed-loop systems in a generalized observer form is introduced and\nsubsequently used as the main tool to derive stability conditions on the\nobserver gains. Then, by exploring the properties of orthogonal projection\nmatrices, the proposed design is used to solve the distributed target tracking\nproblem and provide explicit stability conditions that depend on the\ntarget-agents geometric formation. Practical examples are derived for a target\nmodeled as first-, second-, and third-order integrator dynamics, highlighting\nthe design procedure and the stability conditions imposed. Finally, numerical\nresults showcase the properties of the proposed algorithm.", "AI": {"tldr": "本文提出了一种新颖的分布式一致性观测器设计，使代理组能够在无向通信网络中解决目标跟踪问题，目标被建模为任意阶积分器链，并利用轴承向量测量实现通信效率和指数稳定性。", "motivation": "解决分布式目标跟踪问题，其中目标被建模为积分器链，代理仅知道自身位置并测量相对于目标的轴承向量，同时需要减少网络数据流量并保证稳定性。", "method": "引入一种通用的连续时间观测器设计，适用于状态动态为积分器链且测量模型为特定非线性形式的系统。该设计利用结合了创新和一致性分量的校正项，允许每个代理仅广播部分状态估计以减少数据流。为提供一致指数稳定性保证，引入了一种针对广义观测器形式的非线性闭环系统的新颖稳定性结果。通过探索正交投影矩阵的特性，将该设计应用于解决分布式目标跟踪问题，并推导出依赖于目标-代理几何构型的显式稳定性条件。", "result": "所提出的设计能够解决分布式目标跟踪问题，并提供了一致的指数稳定性保证。导出的稳定性条件明确依赖于目标-代理的几何构型。文章通过一阶、二阶和三阶积分器动力学目标实例，展示了设计过程和稳定性条件。数值结果验证了所提算法的特性。", "conclusion": "该论文成功提出了一种新颖的分布式一致性观测器设计，有效解决了基于轴承测量的目标跟踪问题，实现了通信效率和一致指数稳定性，且稳定性条件与目标-代理的几何构型紧密相关。"}}
{"id": "2507.14189", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14189", "abs": "https://arxiv.org/abs/2507.14189", "authors": ["Song Mao", "Lejun Cheng", "Pinlong Cai", "Guohang Yan", "Ding Wang", "Botian Shi"], "title": "DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base", "comment": "work in process", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nvarious applications. However, their use as writing assistants in specialized\ndomains like finance, medicine, and law is often hampered by a lack of deep\ndomain-specific knowledge and a tendency to hallucinate. Existing solutions,\nsuch as Retrieval-Augmented Generation (RAG), can suffer from inconsistency\nacross multiple retrieval steps, while online search-based methods often\ndegrade quality due to unreliable web content. To address these challenges, we\nintroduce DeepWriter, a customizable, multimodal, long-form writing assistant\nthat operates on a curated, offline knowledge base. DeepWriter leverages a\nnovel pipeline that involves task decomposition, outline generation, multimodal\nretrieval, and section-by-section composition with reflection. By deeply mining\ninformation from a structured corpus and incorporating both textual and visual\nelements, DeepWriter generates coherent, factually grounded, and\nprofessional-grade documents. We also propose a hierarchical knowledge\nrepresentation to enhance retrieval efficiency and accuracy. Our experiments on\nfinancial report generation demonstrate that DeepWriter produces high-quality,\nverifiable articles that surpasses existing baselines in factual accuracy and\ngenerated content quality.", "AI": {"tldr": "DeepWriter是一种可定制的多模态长篇写作助手，它利用精选的离线知识库和新颖的管道，为金融等专业领域生成连贯、事实准确且专业的文档，解决了LLM在专业领域知识不足和幻觉问题。", "motivation": "大型语言模型（LLMs）在金融、医学、法律等专业领域作为写作助手时，存在缺乏深度领域知识和容易产生幻觉的问题。现有解决方案如RAG可能在多步检索中出现不一致，而在线搜索方法则常因不可靠的网络内容导致质量下降。", "method": "DeepWriter采用一个基于精选离线知识库的可定制、多模态、长篇写作助手。其核心方法包括任务分解、大纲生成、多模态检索以及逐节带反思的撰写。它还通过分层知识表示来提高检索效率和准确性，并深度挖掘结构化语料库中的文本和视觉信息。", "result": "在金融报告生成任务上的实验表明，DeepWriter能够生成高质量、可验证的文章，在事实准确性和生成内容质量方面均超越了现有基线方法。", "conclusion": "DeepWriter通过利用精心策划的离线知识库和创新的写作管道，成功解决了LLMs在专业领域写作中知识不足和幻觉的问题，能够生成事实准确、专业且高质量的长篇文档。"}}
{"id": "2507.14268", "categories": ["cs.CV", "cond-mat.mtrl-sci", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14268", "abs": "https://arxiv.org/abs/2507.14268", "authors": ["Andreas Alpers", "Orkun Furat", "Christian Jung", "Matthias Neumann", "Claudia Redenbach", "Aigerim Saken", "Volker Schmidt"], "title": "Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data", "comment": "31 pages, 16 figures, 8 tables", "summary": "This paper presents a comparative analysis of algorithmic strategies for\nfitting tessellation models to 3D image data of materials such as polycrystals\nand foams. In this steadily advancing field, we review and assess\noptimization-based methods -- including linear and nonlinear programming,\nstochastic optimization via the cross-entropy method, and gradient descent --\nfor generating Voronoi, Laguerre, and generalized balanced power diagrams\n(GBPDs) that approximate voxelbased grain structures. The quality of fit is\nevaluated on real-world datasets using discrepancy measures that quantify\ndifferences in grain volume, surface area, and topology. Our results highlight\ntrade-offs between model complexity, the complexity of the optimization\nroutines involved, and the quality of approximation, providing guidance for\nselecting appropriate methods based on data characteristics and application\nneeds.", "AI": {"tldr": "本文比较分析了将细分模型拟合到三维材料图像数据的各种算法策略，评估了优化方法的性能和权衡。", "motivation": "在材料三维图像数据拟合细分模型这一不断发展的领域中，需要对现有优化方法的性能进行回顾和评估。", "method": "采用基于优化的方法（包括线性/非线性规划、通过交叉熵法的随机优化、梯度下降）来生成逼近体素化晶粒结构的Voronoi、Laguerre和广义平衡功率图(GBPDs)。通过测量晶粒体积、表面积和拓扑结构差异的度量标准，在真实世界数据集上评估拟合质量。", "result": "研究结果揭示了模型复杂性、优化例程复杂性与近似质量之间的权衡关系。", "conclusion": "根据数据特征和应用需求，为选择合适的拟合方法提供了指导。"}}
{"id": "2507.14154", "categories": ["cs.AI", "cs.LG", "68T05, 81P68", "I.2.6; I.2.0; F.1.2"], "pdf": "https://arxiv.org/pdf/2507.14154", "abs": "https://arxiv.org/abs/2507.14154", "authors": ["Rahul Kabali"], "title": "The Free Will Equation: Quantum Field Analogies for AGI", "comment": "22 pages, 5 figures. Submitted as an arXiv preprint. All code and\n  experiment details included in appendix", "summary": "Artificial General Intelligence (AGI) research traditionally focuses on\nalgorithms that optimize for specific goals under deterministic rules. Yet,\nhuman-like intelligence exhibits adaptive spontaneity - an ability to make\nunexpected choices or free decisions not strictly dictated by past data or\nimmediate reward. This trait, often dubbed \"free will\" in a loose sense, might\nbe crucial for creativity, robust adaptation, and avoiding ruts in\nproblem-solving. This paper proposes a theoretical framework, called the Free\nWill Equation, that draws analogies from quantum field theory to endow AGI\nagents with a form of adaptive, controlled stochasticity in their\ndecision-making process. The core idea is to treat an AI agent's cognitive\nstate as a superposition of potential actions or thoughts, which collapses\nprobabilistically into a concrete action when a decision is made - much like a\nquantum wavefunction collapsing upon measurement. By incorporating mechanisms\nanalogous to quantum fields, along with intrinsic motivation terms, we aim to\nimprove an agent's ability to explore novel strategies and adapt to unforeseen\nchanges. Experiments in a non-stationary multi-armed bandit environment\ndemonstrate that agents using this framework achieve higher rewards and policy\ndiversity compared to baseline methods.", "AI": {"tldr": "本文提出“自由意志方程”框架，借鉴量子场论，为AGI引入自适应的、受控的随机性，以增强其决策中的创造性、适应性和探索性。", "motivation": "传统的AGI研究侧重于确定性规则下的目标优化，但人类智能展现出“自适应自发性”或“自由意志”，这对于创造力、鲁棒适应性以及避免陷入僵局至关重要，而现有AGI缺乏此特性。", "method": "该研究提出了“自由意志方程”理论框架，借鉴量子场论，将AI智能体的认知状态视为潜在行动或思想的叠加，决策时概率性地“塌缩”为具体行动。通过引入类似于量子场的机制和内在动机项，赋予AGI代理决策过程中的自适应、受控随机性。", "result": "在非平稳多臂赌博机环境中的实验表明，使用该框架的智能体比基线方法获得了更高的奖励和策略多样性。", "conclusion": "该框架通过引入类似量子场的机制和内在动机，显著提高了AGI代理探索新策略和适应不可预见变化的能力。"}}
{"id": "2507.14271", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14271", "abs": "https://arxiv.org/abs/2507.14271", "authors": ["Refik Samet", "Nooshin Nemati", "Emrah Hancer", "Serpil Sak", "Bilge Ayca Kirmizi", "Zeynep Yildirim"], "title": "MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images", "comment": null, "summary": "The MiDeSeC dataset is created through H&E stained invasive breast carcinoma,\nno special type (NST) slides of 25 different patients captured at 40x\nmagnification from the Department of Medical Pathology at Ankara University.\nThe slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and\nOlympus BX50 microscope. As several possible mitosis shapes exist, it is\ncrucial to have a large dataset to cover all the cases. Accordingly, a total of\n50 regions is selected from glass slides for 25 patients, each of regions with\na size of 1024*1024 pixels. There are more than 500 mitoses in total in these\n50 regions. Two-thirds of the regions are reserved for training, the other\nthird for testing.", "AI": {"tldr": "该论文介绍了MiDeSeC数据集的创建过程，该数据集包含来自25名乳腺癌患者的H&E染色切片，用于有丝分裂检测。", "motivation": "由于有丝分裂存在多种形态，需要一个大型数据集来覆盖所有可能的情况，以确保模型的泛化能力。", "method": "MiDeSeC数据集来源于安卡拉大学医学病理学系的25名浸润性乳腺癌（NST）患者的H&E染色切片，以40倍放大率捕获。切片通过3D Histech Panoramic p250 Flash-3扫描仪和Olympus BX50显微镜扫描。共选择了50个区域，每个区域大小为1024x1024像素，总计包含超过500个有丝分裂。数据集的2/3用于训练，1/3用于测试。", "result": "成功创建了MiDeSeC数据集，其中包含来自50个1024x1024像素区域的超过500个有丝分裂实例，并进行了训练集和测试集的划分。", "conclusion": "该研究通过创建MiDeSeC数据集，为乳腺癌有丝分裂检测提供了一个大型且多样化的资源，有助于解决有丝分裂形态多样性带来的挑战。"}}
{"id": "2507.14538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14538", "abs": "https://arxiv.org/abs/2507.14538", "authors": ["Jin Chai", "Xiang Yao", "Mengfan Hou", "Yanghong Li", "Erbao Dong"], "title": "A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0", "comment": null, "summary": "CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid\ntendon-driven actuation system that combines shape memory alloys (SMAs) and DC\nmotors. The hand employs high-strength fishing line as artificial tendons and\nuses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal\nand tendon-muscle structure of the human hand. A linear motor-driven module\ncontrols finger flexion, while an SMA-based module enables finger extension and\nlateral abduction. These modules are integrated into a compact hybrid actuation\nunit mounted on a custom rear support structure. Mechanical and kinematic\nexperiments, conducted under an Arduino Mega 2560-based control system,\nvalidate the effectiveness of the design and demonstrate its biomimetic\ndexterity.", "AI": {"tldr": "CYJ Hand-0是一种21自由度的人形灵巧手，采用形状记忆合金（SMA）和直流电机结合的混合腱驱动系统，具有3D打印金属骨架和仿生结构，实验验证了其设计有效性和仿生灵巧性。", "motivation": "旨在复制人手的骨骼和肌腱-肌肉结构，并实现紧凑、有效的仿生灵巧性。", "method": "CYJ Hand-0具有21个自由度，采用高强度鱼线作为人工肌腱。其骨架由全3D打印的AlSi10Mg金属制成，模仿人手骨骼和肌腱-肌肉结构。手指屈曲由线性电机驱动模块控制，而手指伸展和侧向外展则由基于SMA的模块实现。这些模块集成在一个紧凑的混合驱动单元中，安装在定制的后部支撑结构上。控制系统基于Arduino Mega 2560，并进行了机械和运动学实验。", "result": "机械和运动学实验验证了设计的高效性，并展示了其仿生灵巧性。", "conclusion": "CYJ Hand-0通过其混合驱动系统和仿生结构，实现了有效的设计和仿生灵巧性。"}}
{"id": "2507.14347", "categories": ["eess.SY", "cs.SE", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14347", "abs": "https://arxiv.org/abs/2507.14347", "authors": ["Ole Hans", "Benedikt Walter"], "title": "Remote Assistance or Remote Driving: The Impact of Operational Design Domains on ADS-Supporting Systems Selection", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "High level Automated Driving Systems (ADS) can handle many situations, but\nthey still encounter situations where human intervention is required. In\nsystems where a physical driver is present in the vehicle, typically SAE Level\n3 systems, this intervention is relatively straightforward and is handled by\nthe in-vehicle driver. However, the complexity increases for Level 4 systems,\nwhere, in most cases, no physical driver remains in the vehicle. The two common\nindustry solutions for this challenge are the integration of a remote support\nsystem, such as a Remote Driving System (RDS) or Remote Assistance System\n(RAS). While it is clear that ADS will require one of these systems, it is less\nclear how the suitability of either system for a particular ADS application\nshould be evaluated. Currently, the selection process often focuses on system\narchitecture as well as its design and integration challenges. Furthermore,\nsince many ADS developers choose to develop remote system solutions in-house,\nit is advantageous to select the simpler approach to streamline development and\nintegration efforts. While these decision points are certainly relevant, this\napproach overlooks the most critical factors: the use cases and the\ncomplementarity of the ADS and the remote support system within the context of\nthe Operational Design Design Domain (ODD). This paper proposes a structured\napproach for selecting between RDS and RAS as an ADS support system, based on\nthe defined ODD and use case analysis. To achieve this, the paper applies the\nPEGASUS framework to systematically describe and analyze the ODD. A structured\nframework is introduced to evaluate and select the most suitable remote support\nsystem for an ADS based on clearly defined criteria.", "AI": {"tldr": "本文提出了一种基于操作设计域（ODD）和用例分析的结构化方法，用于为L4级自动驾驶系统（ADS）选择合适的远程支持系统（远程驾驶系统RDS或远程辅助系统RAS）。", "motivation": "L4级自动驾驶系统（ADS）在车内通常没有物理驾驶员，当遇到需要人类干预的复杂情况时，需要远程支持系统。目前业界常采用RDS或RAS，但如何评估和选择最适合特定ADS应用的系统尚不明确，现有选择过程往往侧重于系统架构和开发集成挑战，忽略了最关键的因素：用例和ADS与远程支持系统在ODD背景下的互补性。", "method": "本文提出了一种结构化方法，通过应用PEGASUS框架系统地描述和分析ODD，并引入一个结构化框架，根据明确定义的标准来评估和选择最适合ADS的远程支持系统。", "result": "提出了一种基于ODD和用例分析的结构化方法，用于在RDS和RAS之间进行选择，并引入了一个结构化框架来评估和选择最合适的远程支持系统。", "conclusion": "通过系统地分析ODD和用例，可以为L4级自动驾驶系统选择最合适的远程支持系统（RDS或RAS），从而优化开发和集成工作，并确保系统在复杂情况下的有效运行。"}}
{"id": "2507.14198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14198", "abs": "https://arxiv.org/abs/2507.14198", "authors": ["Fufang Wen", "Shichang Zhang"], "title": "Retention analysis of edited knowledge after fine-tuning", "comment": null, "summary": "Large language models (LLMs) store vast amounts of knowledge, which often\nrequires updates to correct factual errors, incorporate newly acquired\ninformation, or adapt model behavior. Model editing methods have emerged as\nefficient solutions for such updates, offering localized and precise knowledge\nmodification at significantly lower computational cost than continual training.\nIn parallel, LLMs are frequently fine-tuned for a wide range of downstream\ntasks. However, the effect of fine-tuning on previously edited knowledge\nremains poorly understood. In this work, we systematically investigate how\ndifferent fine-tuning objectives interact with various model editing\ntechniques. Our findings show that edited knowledge is substantially more\nsusceptible to forgetting during fine-tuning than intrinsic knowledge acquired\nthrough pre-training. This analysis highlights a key limitation of current\nediting approaches and suggests that evaluating edit robustness under\ndownstream fine-tuning is critical for their practical deployment. We further\nfind that freezing layers associated with edited content can significantly\nimprove knowledge retention, offering insight into how future editing methods\nmight be made more robust.", "AI": {"tldr": "研究发现，大语言模型中通过模型编辑修改的知识在后续微调过程中比预训练获得的知识更容易被遗忘，而冻结相关层可以显著提高知识保留。", "motivation": "大语言模型需要更新知识（纠错、新增信息、调整行为），模型编辑是高效的解决方案。然而，微调对先前编辑的知识有何影响，目前尚不清楚，这限制了模型编辑的实际应用。", "method": "系统性地研究了不同微调目标与各种模型编辑技术之间的相互作用，以评估微调对已编辑知识的影响。", "result": "研究发现，通过模型编辑获得的知识在微调过程中比通过预训练获得的内在知识更容易被遗忘。此外，冻结与编辑内容相关的模型层可以显著提高知识的保留率。", "conclusion": "当前模型编辑方法的一个关键局限是其在后续微调中的鲁棒性不足。在实际部署中，评估编辑在下游微调下的鲁棒性至关重要。冻结相关层为未来构建更鲁棒的编辑方法提供了方向。"}}
{"id": "2507.14303", "categories": ["cs.CV", "I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14303", "abs": "https://arxiv.org/abs/2507.14303", "authors": ["Ehsan Rassekh"], "title": "Semantic Segmentation based Scene Understanding in Autonomous Vehicles", "comment": "74 pages, 35 figures, Master's Thesis, Institute for Advanced Studies\n  in Basic Sciences (IASBS), Zanjan, Iran, 2023", "summary": "In recent years, the concept of artificial intelligence (AI) has become a\nprominent keyword because it is promising in solving complex tasks. The need\nfor human expertise in specific areas may no longer be needed because machines\nhave achieved successful results using artificial intelligence and can make the\nright decisions in critical situations. This process is possible with the help\nof deep learning (DL), one of the most popular artificial intelligence\ntechnologies. One of the areas in which the use of DL is used is in the\ndevelopment of self-driving cars, which is very effective and important. In\nthis work, we propose several efficient models to investigate scene\nunderstanding through semantic segmentation. We use the BDD100k dataset to\ninvestigate these models. Another contribution of this work is the usage of\nseveral Backbones as encoders for models. The obtained results show that\nchoosing the appropriate backbone has a great effect on the performance of the\nmodel for semantic segmentation. Better performance in semantic segmentation\nallows us to understand better the scene and the environment around the agent.\nIn the end, we analyze and evaluate the proposed models in terms of accuracy,\nmean IoU, and loss function, and the results show that these metrics are\nimproved.", "AI": {"tldr": "本文提出并评估了多种深度学习模型，通过语义分割技术提升自动驾驶汽车的场景理解能力，并强调了选择合适骨干网络的重要性。", "motivation": "人工智能（AI）和深度学习（DL）在解决复杂任务和辅助决策方面展现出巨大潜力，尤其在自动驾驶领域，场景理解对安全至关重要。", "method": "本文提出了多种高效的深度学习模型用于语义分割，以实现场景理解。研究使用了BDD100k数据集，并探索了多种不同的骨干网络（Backbones）作为模型的编码器。", "result": "研究结果表明，选择合适的骨干网络对语义分割模型的性能有显著影响。所提出的模型在准确率、平均交并比（mean IoU）和损失函数方面均有所改善，从而提高了场景和环境的理解能力。", "conclusion": "通过优化深度学习模型和选择合适的骨干网络，可以有效提升自动驾驶汽车的语义分割性能和场景理解能力，这在关键指标上得到了验证。"}}
{"id": "2507.14267", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.14267", "abs": "https://arxiv.org/abs/2507.14267", "authors": ["Ziqi Wang", "Hongshuo Huang", "Hancheng Zhao", "Changwen Xu", "Shang Zhu", "Jan Janssen", "Venkatasubramanian Viswanathan"], "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation", "comment": "34 pages, 28 pages of Supporting Information", "summary": "Materials discovery relies on high-throughput, high-fidelity simulation\ntechniques such as Density Functional Theory (DFT), which require years of\ntraining, extensive parameter fine-tuning and systematic error handling. To\naddress these challenges, we introduce the DFT-based Research Engine for\nAgentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for\nDFT simulation that combines a central Large Language Model (LLM) planner agent\nwith domain-specific LLM agents for atomistic structure generation, systematic\nDFT convergence testing, High-Performance Computing (HPC) scheduling, and error\nhandling. In addition, a shared canvas helps the LLM agents to structure their\ndiscussions, preserve context and prevent hallucination. We validate DREAMS\ncapabilities on the Sol27LC lattice-constant benchmark, achieving average\nerrors below 1\\% compared to the results of human DFT experts. Furthermore, we\napply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating\nits long-term and complex problem-solving capabilities. The framework again\nreproduces expert-level literature adsorption-energy differences. Finally,\nDREAMS is employed to quantify functional-driven uncertainties with Bayesian\nensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at\nthe Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS\napproaches L3-level automation - autonomous exploration of a defined design\nspace - and significantly reduces the reliance on human expertise and\nintervention, offering a scalable path toward democratized, high-throughput,\nhigh-fidelity computational materials discovery.", "AI": {"tldr": "DREAMS是一个基于LLM的多智能体框架，旨在自动化密度泛函理论（DFT）模拟，显著减少对人类专业知识的依赖，从而加速材料发现。", "motivation": "材料发现依赖于高通量、高保真的DFT模拟技术，但这些技术通常需要多年的训练、大量的参数调优和系统的错误处理，效率低下。", "method": "DREAMS（基于DFT的代理材料筛选研究引擎）是一个分层的多智能体框架。它结合了一个中央大型语言模型（LLM）规划代理，以及多个领域特定的LLM代理，分别负责原子结构生成、系统性DFT收敛测试、高性能计算（HPC）调度和错误处理。此外，框架还通过共享画布帮助LLM代理组织讨论、保留上下文并防止幻觉。", "result": "DREAMS在Sol27LC晶格常数基准测试中表现出色，平均误差低于1%，与人类DFT专家的结果相当。它成功解决了CO/Pt(111)吸附难题，再现了专家级别的吸附能差异。DREAMS还通过贝叶斯集成采样量化了功能驱动的不确定性，并确认了在GGA DFT级别下FCC位点的偏好。", "conclusion": "DREAMS实现了L3级自动化（即在定义设计空间内的自主探索），显著减少了对人类专业知识和干预的依赖。它为高通量、高保真的计算材料发现提供了一条可扩展且民主化的途径。"}}
{"id": "2507.14272", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14272", "abs": "https://arxiv.org/abs/2507.14272", "authors": ["Refik Samet", "Nooshin Nemati", "Emrah Hancer", "Serpil Sak", "Bilge Ayca Kirmizi"], "title": "NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images", "comment": null, "summary": "The NuSeC dataset is created by selecting 4 images with the size of 1024*1024\npixels from the slides of each patient among 25 patients. Therefore, there are\na total of 100 images in the NuSeC dataset. To carry out a consistent\ncomparative analysis between the methods that will be developed using the NuSeC\ndataset by the researchers in the future, we divide the NuSeC dataset 75% as\nthe training set and 25% as the testing set. In detail, an image is randomly\nselected from 4 images of each patient among 25 patients to build the testing\nset, and then the remaining images are reserved for the training set. While the\ntraining set includes 75 images with around 30000 nuclei structures, the\ntesting set includes 25 images with around 6000 nuclei structures.", "AI": {"tldr": "本文介绍了NuSeC数据集的构建及其训练集与测试集的划分方法，旨在为未来的细胞核分割研究提供一个一致的比较分析平台。", "motivation": "为了确保未来研究者在使用NuSeC数据集开发方法时，能够进行一致的比较分析，需要创建一个标准化的数据集并进行明确的划分。", "method": "从25名患者的玻片中，每位患者选择4张1024x1024像素的图像，共计100张图像。数据集被划分为75%的训练集和25%的测试集。具体地，从每位患者的4张图像中随机选择1张作为测试集，其余图像作为训练集。", "result": "NuSeC数据集包含100张图像，其中训练集包含75张图像（约30000个细胞核结构），测试集包含25张图像（约6000个细胞核结构）。", "conclusion": "NuSeC数据集已成功创建并划分为训练集和测试集，为未来的细胞核分割方法研究提供了统一且一致的比较分析基础。"}}
{"id": "2507.14582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14582", "abs": "https://arxiv.org/abs/2507.14582", "authors": ["Zezhi Liu", "Shizhen Wu", "Hanqian Luo", "Deyun Qin", "Yongchun Fang"], "title": "BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives", "comment": "11 pages, 8 figures", "summary": "In the field of Learning from Demonstration (LfD), enabling robots to\ngeneralize learned manipulation skills to novel scenarios for long-horizon\ntasks remains challenging. Specifically, it is still difficult for robots to\nadapt the learned skills to new environments with different task and motion\nrequirements, especially in long-horizon, multi-stage scenarios with intricate\nconstraints. This paper proposes a novel hierarchical framework, called\nBT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and\nDynamical Movement Primitives (DMPs) to address this problem. Within this\nframework, Signal Temporal Logic (STL) is employed to formally specify complex,\nlong-horizon task requirements and constraints. These STL specifications are\nsystematically transformed to generate reactive and modular BTs for high-level\ndecision-making task structure. An STL-constrained DMP optimization method is\nproposed to optimize the DMP forcing term, allowing the learned motion\nprimitives to adapt flexibly while satisfying intricate spatiotemporal\nrequirements and, crucially, preserving the essential dynamics learned from\ndemonstrations. The framework is validated through simulations demonstrating\ngeneralization capabilities under various STL constraints and real-world\nexperiments on several long-horizon robotic manipulation tasks. The results\ndemonstrate that the proposed framework effectively bridges the symbolic-motion\ngap, enabling more reliable and generalizable autonomous manipulation for\ncomplex robotic tasks.", "AI": {"tldr": "该论文提出了一种名为BT-TL-DMPs的层级框架，结合行为树、时序逻辑和动态运动基元，旨在解决机器人从示教学习中泛化长周期操作技能到新环境的挑战。", "motivation": "在示教学习领域，机器人将学习到的操作技能泛化到新颖场景和长周期任务中仍然面临挑战。具体来说，机器人难以将学习到的技能适应到具有不同任务和运动要求的新环境中，尤其是在具有复杂约束的长周期、多阶段场景中。", "method": "本研究提出了一种名为BT-TL-DMPs的层级框架，它整合了行为树（BT）、时序逻辑（TL）和动态运动基元（DMPs）。在该框架内，利用信号时序逻辑（STL）来正式指定复杂、长周期的任务要求和约束。这些STL规范被系统地转换为生成反应式和模块化的行为树，用于高层决策任务结构。此外，提出了一种STL约束的DMP优化方法来优化DMP的强制项，使学习到的运动基元能够灵活适应，同时满足复杂的时空要求并保留从示教中学习到的基本动力学。", "result": "该框架通过模拟验证了在各种STL约束下的泛化能力，并通过在多个长周期机器人操作任务上的真实世界实验进行了验证。结果表明，所提出的框架有效地弥合了符号与运动之间的鸿沟，使复杂机器人任务的自主操作更加可靠和可泛化。", "conclusion": "该框架成功地将符号逻辑与运动控制相结合，显著提高了机器人在复杂长周期任务中学习技能的泛化能力和可靠性，有效解决了机器人操作的符号-运动鸿沟问题。"}}
{"id": "2507.14385", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14385", "abs": "https://arxiv.org/abs/2507.14385", "authors": ["Hongliang Li", "Herschel C. Pangborn", "Ilya Kovalenko"], "title": "Bi-level Model Predictive Control for Energy-aware Integrated Product Pricing and Production Scheduling", "comment": null, "summary": "The manufacturing industry is under growing pressure to enhance\nsustainability while preserving economic competitiveness. As a result,\nmanufacturers have been trying to determine how to integrate onsite renewable\nenergy and real-time electricity pricing into manufacturing schedules without\ncompromising profitability. To address this challenge, we propose a bi-level\nmodel predictive control framework that jointly optimizes product prices and\nproduction scheduling with explicit consideration of renewable energy\navailability. The higher level determines the product price to maximize revenue\nand renewable energy usage. The lower level controls production scheduling in\nruntime to minimize operational costs and respond to the product demand. Price\nelasticity is incorporated to model market response, allowing the system to\nincrease demand by lowering the product price during high renewable energy\ngeneration. Results from a lithium-ion battery pack manufacturing system case\nstudy demonstrate that our approach enables manufacturers to reduce grid energy\ncosts while increasing profit.", "AI": {"tldr": "该研究提出了一个双层模型预测控制框架，用于在考虑可再生能源可用性和实时电价的情况下，联合优化制造系统的产品定价和生产调度，以降低成本并提高利润。", "motivation": "制造业面临着在保持经济竞争力的同时提高可持续性的压力，制造商需要找到方法将现场可再生能源和实时电价整合到生产计划中而不损害盈利能力。", "method": "提出了一种双层模型预测控制（MPC）框架：上层模型确定产品价格以最大化收入和可再生能源使用；下层模型在运行时控制生产调度以最小化运营成本并响应产品需求。该模型还引入了价格弹性，以模拟市场响应，允许在可再生能源高发电量时通过降低产品价格来增加需求。", "result": "通过一个锂离子电池组制造系统的案例研究表明，该方法使制造商能够降低电网能源成本，同时增加利润。", "conclusion": "该双层优化方法有效地帮助制造商在整合可再生能源和应对实时电价的同时，实现了成本降低和利润增长。"}}
{"id": "2507.14200", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14200", "abs": "https://arxiv.org/abs/2507.14200", "authors": ["Shengji Tang", "Jianjian Cao", "Weihao Lin", "Jiale Hong", "Bo Zhang", "Shuyue Hu", "Lei Bai", "Tao Chen", "Wanli Ouyang", "Peng Ye"], "title": "Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System", "comment": null, "summary": "This paper aims to demonstrate the potential and strengths of open-source\ncollectives. It leads to a promising question: Can we harness multiple\nopen-source LLMs to match or even beat the closed-source LLMs? To answer this,\nwe propose SMACS, a scalable multi-agent collaboration system (MACS) framework\nwith high performance. Specifically, for continuous integration of new LLMs and\ngeneralization to diverse questions, we first propose a Retrieval-based Prior\nSelection (RPS), which assigns a proxy performance score to each LLM to select\nthe Top-k LLMs at the instance level for any given question. Then, we propose\nan Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the\ngeneration of diverse responses through prior dropping and selecting the\nhigh-quality response via a hybrid posterior score. Experiments on eight\nmainstream benchmarks validate the effectiveness of our SMACS: by integrating\nfifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,\ne.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)\nacross multiple tasks. Remarkably, it even exceeds the average of best results\nof different datasets from both open-source LLMs (+2.86%) and closed-source\nLLMs (+2.04%), pushing the upper bound of intelligence. Code will be released\nat https://github.com/magent4aci/SMACS.", "AI": {"tldr": "该论文提出SMACS框架，通过整合多个开源大型语言模型（LLMs），在多个基准测试中超越了领先的闭源LLMs。", "motivation": "探讨是否能利用多个开源LLMs来匹配甚至超越闭源LLMs的性能。", "method": "提出SMACS（可扩展多智能体协作系统）框架。具体包括：1. 检索式先验选择（RPS）：为每个LLM分配代理性能分数，根据问题选择Top-k LLMs。2. 探索-利用驱动的后验增强（EPE）：通过先验丢弃鼓励生成多样化响应，并通过混合后验分数选择高质量响应。", "result": "SMACS整合15个开源LLMs，在8个主流基准测试中表现优异，超越了Claude-3.7-Sonnet (+12.73%)、GPT-4.1 (+5.36%)和GPT-o3-mini (+5.28%)等领先的闭源LLMs。它甚至超过了开源LLMs (+2.86%)和闭源LLMs (+2.04%)在不同数据集上的最佳结果平均值。", "conclusion": "开源LLM集合具有巨大潜力，SMACS有效提升了LLM的智能上限，证明了通过协作可以超越现有最佳模型。"}}
{"id": "2507.14312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14312", "abs": "https://arxiv.org/abs/2507.14312", "authors": ["Marc Lafon", "Gustavo Adolfo Vargas Hakim", "Clément Rambour", "Christian Desrosier", "Nicolas Thome"], "title": "CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation", "comment": null, "summary": "Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities\nbut often fail to generalize under distribution shifts. Test-time adaptation\n(TTA) allows models to update at inference time without labeled data, typically\nvia entropy minimization. However, this objective is fundamentally misaligned\nwith the contrastive image-text training of VLMs, limiting adaptation\nperformance and introducing failure modes such as pseudo-label drift and class\ncollapse. We propose CLIPTTA, a new gradient-based TTA method for\nvision-language models that leverages a soft contrastive loss aligned with\nCLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's\ngradients, showing how its batch-aware design mitigates the risk of collapse.\nWe further extend CLIPTTA to the open-set setting, where both in-distribution\n(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier\nContrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75\ndatasets spanning diverse distribution shifts, CLIPTTA consistently outperforms\nentropy-based objectives and is highly competitive with state-of-the-art TTA\nmethods, outperforming them on a large number of datasets and exhibiting more\nstable performance across diverse shifts.", "AI": {"tldr": "本文提出CLIPTTA，一种新的基于梯度的测试时自适应（TTA）方法，专为视觉-语言模型（VLM）设计，通过采用与VLM预训练目标一致的软对比损失，有效解决了VLM在分布偏移下的泛化问题及传统TTA方法的局限性。", "motivation": "视觉-语言模型（如CLIP）在零样本能力方面表现出色，但在面临分布偏移时泛化能力不足。现有的测试时自适应（TTA）方法（通常通过熵最小化实现）与VLM的对比图像-文本训练目标不符，导致自适应性能受限，并引入伪标签漂移和类别崩溃等失败模式。", "method": "本文提出CLIPTTA，一种新的基于梯度的TTA方法。它利用与CLIP预训练目标对齐的软对比损失进行模型更新。研究还提供了CLIPTTA梯度理论分析，阐明其批次感知设计如何减轻崩溃风险。此外，CLIPTTA被扩展到开放集设置，通过使用异常值对比暴露（OCE）损失来提高域外（OOD）检测能力。", "result": "在涵盖75个不同分布偏移数据集的评估中，CLIPTTA持续优于基于熵的目标，并与最先进的TTA方法相比具有高度竞争力，在大量数据集上表现更优，并在各种偏移下展现出更稳定的性能。", "conclusion": "CLIPTTA通过采用与视觉-语言模型预训练目标一致的软对比损失，提供了一种更有效、更稳定的测试时自适应方法，显著提升了模型在分布偏移下的泛化能力，并解决了传统TTA方法的局限性。"}}
{"id": "2507.14293", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14293", "abs": "https://arxiv.org/abs/2507.14293", "authors": ["Boyuan Zheng", "Zeyi Liao", "Scott Salisbury", "Zeyuan Liu", "Michael Lin", "Qinyuan Zheng", "Zifan Wang", "Xiang Deng", "Dawn Song", "Huan Sun", "Yu Su"], "title": "WebGuard: Building a Generalizable Guardrail for Web Agents", "comment": "We publicly release WebGuard, along with its annotation tools and\n  fine-tuned models, to facilitate open-source research on monitoring and\n  safeguarding web agents. All resources are available at\n  https://github.com/OSU-NLP-Group/WebGuard", "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.", "AI": {"tldr": "随着大语言模型驱动的自主网络代理的快速发展，其潜在的意外或有害行为带来了风险。本文引入了WebGuard数据集来评估这些风险并开发安全防护措施，并发现即使经过微调的模型，在关键部署场景下仍需要更高的可靠性。", "motivation": "LLM驱动的自主网络代理在提升效率的同时，也带来了执行意外或有害动作的风险，因此迫切需要有效的安全防护措施，类似于人类用户的访问控制。", "method": "本文提出了WebGuard数据集，这是首个用于评估网络代理动作风险和开发防护措施的综合数据集。WebGuard包含4,939个人工标注的状态改变动作，来自193个网站（22个领域），并采用SAFE、LOW、HIGH三层风险分类。数据集包含训练和测试集以支持泛化评估。研究首先评估了前沿LLM在此任务上的表现，随后通过WebGuard数据集对专门的防护模型（如Qwen2.5VL-7B）进行了微调，并在多种泛化设置下进行了全面评估。", "result": "初步评估显示，前沿LLM在预测动作结果方面的准确率低于60%，对高风险动作的召回率也低于60%。通过WebGuard微调的Qwen2.5VL-7B模型性能显著提升，准确率从37%提高到80%，高风险动作召回率从20%提高到76%。", "conclusion": "当前一代代理在没有专用防护措施的情况下部署存在显著风险。尽管微调模型带来了显著改进，但其性能仍未能达到高风险部署所需的接近完美的可靠性水平。"}}
{"id": "2507.14308", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14308", "abs": "https://arxiv.org/abs/2507.14308", "authors": ["Jingjia Chen", "Haoyang Pei", "Christoph Maier", "Mary Bruno", "Qiuting Wen", "Seon-Hi Shin", "William Moore", "Hersh Chandarana", "Li Feng"], "title": "Self-Supervised Joint Reconstruction and Denoising of T2-Weighted PROPELLER MRI of the Lungs at 0.55T", "comment": null, "summary": "Purpose: This study aims to improve 0.55T T2-weighted PROPELLER lung MRI\nthrough a self-supervised joint reconstruction and denoising model.\n  Methods: T2-weighted 0.55T lung MRI dataset including 44 patients with\nprevious covid infection were used. A self-supervised learning framework was\ndeveloped, where each blade of the PROPELLER acquisition was split along the\nreadout direction into two partitions. One subset trains the unrolled\nreconstruction network, while the other subset is used for loss calculation,\nenabling self-supervised training without clean targets and leveraging matched\nnoise statistics for denoising. For comparison, Marchenko-Pastur Principal\nComponent Analysis (MPPCA) was performed along the coil dimension, followed by\nconventional parallel imaging reconstruction. The quality of the reconstructed\nlung MRI was assessed visually by two experienced radiologists independently.\n  Results: The proposed self-supervised model improved the clarity and\nstructural integrity of the lung images. For cases with available CT scans, the\nreconstructed images demonstrated strong alignment with corresponding CT\nimages. Additionally, the proposed model enables further scan time reduction by\nrequiring only half the number of blades. Reader evaluations confirmed that the\nproposed method outperformed MPPCA-denoised images across all categories\n(Wilcoxon signed-rank test, p<0.001), with moderate inter-reader agreement\n(weighted Cohen's kappa=0.55; percentage of exact and within +/-1 point\nagreement=91%).\n  Conclusion: By leveraging intrinsic structural redundancies between two\ndisjoint splits of k-space subsets, the proposed self-supervised learning model\neffectively reconstructs the image while suppressing the noise for 0.55T\nT2-weighted lung MRI with PROPELLER sampling.", "AI": {"tldr": "本研究提出了一种自监督联合重建与去噪模型，显著提升了0.55T T2加权PROPELLER肺部MRI的图像质量，并可缩短扫描时间。", "motivation": "本研究旨在改进0.55T T2加权PROPELLER肺部MRI的图像质量，以提高其在低场强下的诊断价值。", "method": "研究使用了包含44名既往COVID感染患者的0.55T T2加权肺部MRI数据集。开发了一个自监督学习框架，将PROPELLER采集的每个叶片沿读出方向分成两部分：一部分用于训练展开的重建网络，另一部分用于计算损失，从而实现无需干净目标数据的自监督训练，并利用匹配的噪声统计数据进行去噪。对比方法是沿线圈维度进行Marchenko-Pastur主成分分析（MPPCA），然后进行传统并行成像重建。重建图像质量由两位经验丰富的放射科医生独立进行视觉评估。", "result": "所提出的自监督模型显著改善了肺部图像的清晰度和结构完整性。对于有CT扫描的病例，重建图像与相应CT图像高度一致。此外，该模型仅需一半的叶片数量即可进一步缩短扫描时间。读者评估证实，该方法在所有类别中均优于MPPCA去噪图像（Wilcoxon符号秩检验，p<0.001），且读者间一致性中等（加权Cohen's kappa=0.55）。", "conclusion": "通过利用k空间子集两个不相交部分之间的内在结构冗余，所提出的自监督学习模型能有效重建图像，同时抑制0.55T T2加权PROPELLER采样肺部MRI的噪声。"}}
{"id": "2507.14605", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14605", "abs": "https://arxiv.org/abs/2507.14605", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition", "comment": null, "summary": "Online optimal control of quadrupedal robots would enable them to plan their\nmovement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged\nas a practical approach for real-time control. In LMPC, an optimization problem\nwith a quadratic cost and linear constraints is formulated over a finite\nhorizon and solved on the fly. However, LMPC relies on linearizing the\nequations of motion (EOM), which may lead to poor solution quality. In this\npaper, we use Koopman operator theory and the Extended Dynamic Mode\nDecomposition (EDMD) to create a linear model of the system in high dimensional\nspace, thus retaining the nonlinearity of the EOM. We model the aerial phase\nand ground contact phases using different linear models. Then, using LMPC, we\ndemonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait\ntransitions in level and rough terrains. The main novelty is the use of Koopman\noperator theory to create hybrid models of a quadrupedal system and demonstrate\nthe online generation of multiple gaits and gaits transitions.", "AI": {"tldr": "本文提出了一种基于Koopman算子理论和扩展动态模态分解（EDMD）的方法，为四足机器人创建高维线性模型，以在保留非线性的情况下，结合线性模型预测控制（LMPC）实现多种步态和步态转换的在线生成。", "motivation": "传统的线性模型预测控制（LMPC）在实时控制四足机器人方面表现出色，但其依赖于运动方程（EOM）的线性化，可能导致解的质量不佳，无法充分捕捉系统的非线性特性。", "method": "研究人员利用Koopman算子理论和扩展动态模态分解（EDMD）在更高维度空间中创建了系统的线性模型，从而保留了原始运动方程的非线性。他们针对空中阶段和地面接触阶段分别建立了不同的线性模型，然后将这些模型应用于线性模型预测控制（LMPC）。", "result": "该方法成功地在平坦和崎岖地形中展示了跳跃（bounding）、小跑（trotting）等多种步态，以及跳跃到小跑、小跑到跳跃的步态转换的在线生成。主要的新颖之处在于利用Koopman算子理论创建了四足机器人的混合模型，并实现了多种步态及步态转换的在线生成。", "conclusion": "通过结合Koopman算子理论和LMPC，本文成功解决了传统LMPC在线性化运动方程时面临的精度问题，实现了四足机器人复杂非线性动力学下的多步态在线控制和步态转换，为四足机器人在复杂环境中的运动规划提供了新的有效方法。"}}
{"id": "2507.14409", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14409", "abs": "https://arxiv.org/abs/2507.14409", "authors": ["Max L. Gardenswartz", "Brandon C. Fallin", "Cristian F. Nino", "Warren E. Dixon"], "title": "Collaborative Indirect Influencing and Control on Graphs using Graph Neural Networks", "comment": "arXiv admin note: substantial text overlap with arXiv:2503.15360", "summary": "This paper presents a novel approach to solving the indirect influence\nproblem in networked systems, in which cooperative nodes must regulate a target\nnode with uncertain dynamics to follow a desired trajectory. We leverage the\nmessage-passing structure of a graph neural network (GNN), allowing nodes to\ncollectively learn the unknown target dynamics in real time. We develop a novel\nGNN-based backstepping control strategy with formal stability guarantees\nderived from a Lyapunov-based analysis. Numerical simulations are included to\ndemonstrate the performance of the developed controller.", "AI": {"tldr": "本文提出一种新颖的基于图神经网络（GNN）的回步控制策略，用于解决网络系统中合作节点对具有不确定动力学特性的目标节点的间接影响问题，并提供形式化的稳定性保证。", "motivation": "在网络系统中，合作节点需要调节一个具有不确定动力学特性的目标节点，使其遵循期望轨迹，这是一个具有挑战性的间接影响问题。", "method": "利用图神经网络（GNN）的消息传递结构，使节点能够实时地集体学习未知的目标动力学。开发了一种新颖的基于GNN的回步控制策略，并使用Lyapunov分析推导了形式化的稳定性保证。", "result": "数值仿真结果证明了所开发控制器的性能。", "conclusion": "该研究成功开发了一种基于GNN的控制器，能够有效解决网络系统中的间接影响问题，并具有严格的稳定性保证。"}}
{"id": "2507.14214", "categories": ["cs.CL", "cs.CR", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14214", "abs": "https://arxiv.org/abs/2507.14214", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "title": "Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale", "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites despite claiming\notherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that\nassists users with personalized privacy policy analysis. PoliAnalyzer uses\nNatural Language Processing (NLP) to extract formal representations of data\nusage practices from policy texts. In favor of deterministic, logical inference\nis applied to compare user preferences with the formal privacy policy\nrepresentation and produce a compliance report. To achieve this, we extend an\nexisting formal Data Terms of Use policy language to model privacy policies as\napp policies and user preferences as data policies. In our evaluation using our\nenriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated\nhigh accuracy in identifying relevant data usage practices, achieving F1-score\nof 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can\nmodel diverse user data-sharing preferences, derived from prior research as 23\nuser profiles, and perform compliance analysis against the top 100 most-visited\nwebsites. This analysis revealed that, on average, 95.2% of a privacy policy's\nsegments do not conflict with the analyzed user preferences, enabling users to\nconcentrate on understanding the 4.8% (636 / 13205) that violates preferences,\nsignificantly reducing cognitive burden. Further, we identified common\npractices in privacy policies that violate user expectations - such as the\nsharing of location data with 3rd parties. This paper demonstrates that\nPoliAnalyzer can support automated personalized privacy policy analysis at\nscale using off-the-shelf NLP tools. This sheds light on a pathway to help\nindividuals regain control over their data and encourage societal discussions\non platform data practices to promote a fairer power dynamic.", "AI": {"tldr": "PoliAnalyzer是一个神经符号系统，利用NLP和逻辑推理帮助用户个性化分析隐私政策，高准确率识别数据使用实践，显著降低用户理解负担，并揭示了常见的违规行为。", "motivation": "现代人拥有大量在线账户，但极少阅读服务条款或隐私政策，导致用户对自身数据使用情况缺乏了解和控制。", "method": "PoliAnalyzer是一个神经符号系统。它使用自然语言处理（NLP）从政策文本中提取数据使用实践的形式化表示，并扩展了现有数据使用条款策略语言来建模隐私政策和用户偏好。然后，通过确定性逻辑推理将用户偏好与形式化隐私政策进行比较，生成合规报告。系统在法律专家整理的PolicyIE数据集上进行评估。", "result": "PoliAnalyzer在识别相关数据使用实践方面表现出高精度，在大多数任务中F1分数达到90-100%。对前100个最常访问网站的分析显示，平均95.2%的隐私政策段落不与用户偏好冲突，使用户只需关注理解4.8%（636/13205）的违规部分，显著降低了认知负担。研究还识别出常见的违反用户期望的隐私政策实践，例如将位置数据共享给第三方。", "conclusion": "PoliAnalyzer展示了利用现成NLP工具大规模支持自动化个性化隐私政策分析的能力。这为帮助个人重新掌控数据、鼓励社会讨论平台数据实践以促进更公平的权力动态提供了途径。"}}
{"id": "2507.14315", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14315", "abs": "https://arxiv.org/abs/2507.14315", "authors": ["Qiyu Xu", "Zhanxuan Hu", "Yu Duan", "Ercheng Pei", "Yonghang Tai"], "title": "A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention", "comment": null, "summary": "Generalized Category Discovery (GCD) aims to classify unlabeled data from\nboth known and unknown categories by leveraging knowledge from labeled known\ncategories. While existing methods have made notable progress, they often\noverlook a hidden stumbling block in GCD: distracted attention. Specifically,\nwhen processing unlabeled data, models tend to focus not only on key objects in\nthe image but also on task-irrelevant background regions, leading to suboptimal\nfeature extraction. To remove this stumbling block, we propose Attention\nFocusing (AF), an adaptive mechanism designed to sharpen the model's focus by\npruning non-informative tokens. AF consists of two simple yet effective\ncomponents: Token Importance Measurement (TIME) and Token Adaptive Pruning\n(TAP), working in a cascade. TIME quantifies token importance across multiple\nscales, while TAP prunes non-informative tokens by utilizing the multi-scale\nimportance scores provided by TIME. AF is a lightweight, plug-and-play module\nthat integrates seamlessly into existing GCD methods with minimal computational\noverhead. When incorporated into one prominent GCD method, SimGCD, AF achieves\nup to 15.4% performance improvement over the baseline with minimal\ncomputational overhead. The implementation code is provided in\nhttps://github.com/Afleve/AFGCD.", "AI": {"tldr": "该研究提出了一种名为“注意力聚焦”（AF）的自适应机制，通过修剪非信息性标记来解决广义类别发现（GCD）中模型因关注无关背景而导致的“注意力分散”问题，从而显著提升了性能。", "motivation": "现有广义类别发现（GCD）方法在处理未标记数据时存在“注意力分散”问题，即模型不仅关注图像中的关键对象，还会关注与任务无关的背景区域，导致特征提取不佳。", "method": "提出“注意力聚焦”（AF）机制，通过修剪非信息性标记来提高模型关注度。AF包含两个组件：标记重要性测量（TIME），用于量化多尺度标记重要性；以及标记自适应修剪（TAP），利用TIME提供的多尺度重要性分数修剪非信息性标记。AF是一个轻量级、即插即用的模块。", "result": "当集成到现有GCD方法SimGCD中时，AF在基线上实现了高达15.4%的性能提升，且计算开销极小。", "conclusion": "注意力聚焦（AF）机制通过有效修剪非信息性标记，成功解决了GCD中的注意力分散问题，显著提升了模型性能，且具有轻量级和易于集成的优点。"}}
{"id": "2507.14306", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14306", "abs": "https://arxiv.org/abs/2507.14306", "authors": ["Samarth P", "Vyoman Jain", "Shiva Golugula", "Motamarri Sai Sathvik"], "title": "Manimator: Transforming Research Papers into Visual Explanations", "comment": null, "summary": "Understanding complex scientific and mathematical concepts, particularly\nthose presented in dense research papers, poses a significant challenge for\nlearners. Dynamic visualizations can greatly enhance comprehension, but\ncreating them manually is time-consuming and requires specialized knowledge and\nskills. We introduce manimator, an open-source system that leverages Large\nLanguage Models to transform research papers and natural language prompts into\nexplanatory animations using the Manim engine. Manimator employs a pipeline\nwhere an LLM interprets the input text or research paper PDF to generate a\nstructured scene description outlining key concepts, mathematical formulas, and\nvisual elements and another LLM translates this description into executable\nManim Python code. We discuss its potential as an educational tool for rapidly\ncreating engaging visual explanations for complex STEM topics, democratizing\nthe creation of high-quality educational content.", "AI": {"tldr": "Manimator是一个开源系统，利用大型语言模型将科研论文和自然语言提示转换为Manim动画，以提高复杂概念的理解。", "motivation": "学习者理解复杂的科学和数学概念（尤其是在密集研究论文中呈现的）面临巨大挑战；手动创建动态可视化耗时且需要专业知识和技能。", "method": "Manimator系统采用LLM管道：一个LLM解释输入文本或研究论文PDF，生成包含关键概念、数学公式和视觉元素的结构化场景描述；另一个LLM将此描述翻译成可执行的Manim Python代码。", "result": "成功开发了Manimator系统，能够将研究论文和自然语言提示自动化地转换为解释性动画。", "conclusion": "该系统有望成为一种教育工具，用于快速创建复杂STEM主题的引人入胜的视觉解释，从而普及高质量教育内容的创作。"}}
{"id": "2507.14378", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14378", "abs": "https://arxiv.org/abs/2507.14378", "authors": ["Shrunal Pothagoni", "Benjamin Schweinhart"], "title": "Classification of Histopathology Slides with Persistence Homology Convolutions", "comment": null, "summary": "Convolutional neural networks (CNNs) are a standard tool for computer vision\ntasks such as image classification. However, typical model architectures may\nresult in the loss of topological information. In specific domains such as\nhistopathology, topology is an important descriptor that can be used to\ndistinguish between disease-indicating tissue by analyzing the shape\ncharacteristics of cells. Current literature suggests that reintroducing\ntopological information using persistent homology can improve medical\ndiagnostics; however, previous methods utilize global topological summaries\nwhich do not contain information about the locality of topological features. To\naddress this gap, we present a novel method that generates local persistent\nhomology-based data using a modified version of the convolution operator called\nPersistent Homology Convolutions. This method captures information about the\nlocality and translation invariance of topological features. We perform a\ncomparative study using various representations of histopathology slides and\nfind that models trained with persistent homology convolutions outperform\nconventionally trained models and are less sensitive to hyperparameters. These\nresults indicate that persistent homology convolutions extract meaningful\ngeometric information from the histopathology slides.", "AI": {"tldr": "该研究提出一种名为“持久同源卷积”（Persistent Homology Convolutions）的新方法，通过修改卷积操作符来捕获局部拓扑信息，以改进卷积神经网络在组织病理学图像分类中的表现，并发现其优于传统方法。", "motivation": "卷积神经网络（CNNs）在计算机视觉任务中很常用，但可能导致拓扑信息丢失。在组织病理学等特定领域，拓扑信息是区分疾病组织的重要描述符。现有研究尝试通过持久同源性（persistent homology）重新引入拓扑信息，但这些方法通常使用全局拓扑摘要，缺乏局部拓扑特征的信息。", "method": "研究提出了一种新颖的“持久同源卷积”方法，通过修改传统的卷积操作符，生成基于局部持久同源性的数据。这种方法旨在捕获拓扑特征的局部性和平移不变性。", "result": "通过对组织病理学切片进行比较研究，发现使用持久同源卷积训练的模型优于传统训练的模型，并且对超参数的敏感性较低。", "conclusion": "这些结果表明，持久同源卷积能够从组织病理学切片中提取有意义的几何信息，有效解决了传统CNNs丢失拓扑信息的问题，并提升了模型性能。"}}
{"id": "2507.14694", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14694", "abs": "https://arxiv.org/abs/2507.14694", "authors": ["Yue Ma", "Kanglei Zhou", "Fuyang Yu", "Frederick W. B. Li", "Xiaohui Liang"], "title": "Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks", "comment": null, "summary": "3D human motion forecasting aims to enable autonomous applications.\nEstimating uncertainty for each prediction (i.e., confidence based on\nprobability density or quantile) is essential for safety-critical contexts like\nhuman-robot collaboration to minimize risks. However, existing diverse motion\nforecasting approaches struggle with uncertainty quantification due to implicit\nprobabilistic representations hindering uncertainty modeling. We propose\nProbHMI, which introduces invertible networks to parameterize poses in a\ndisentangled latent space, enabling probabilistic dynamics modeling. A\nforecasting module then explicitly predicts future latent distributions,\nallowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI\nachieves strong performance for both deterministic and diverse prediction while\nvalidating uncertainty calibration, critical for risk-aware decision making.", "AI": {"tldr": "本文提出ProbHMI，利用可逆网络在解耦潜在空间中建模人体姿态，并显式预测未来潜在分布，从而实现3D人体运动预测中的不确定性量化。", "motivation": "现有3D人体运动预测方法在不确定性量化方面存在困难，而这对于人机协作等安全关键应用至关重要，以降低风险。", "method": "ProbHMI引入可逆网络将姿态参数化到解耦的潜在空间，从而实现概率动力学建模。然后，一个预测模块显式预测未来的潜在分布，以有效量化不确定性。", "result": "在基准测试中，ProbHMI在确定性预测和多样性预测方面均表现出色，并验证了不确定性校准，这对于风险感知决策至关重要。", "conclusion": "ProbHMI通过显式预测未来潜在分布，有效解决了3D人体运动预测中的不确定性量化问题，对风险感知决策具有重要意义。"}}
{"id": "2507.14450", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14450", "abs": "https://arxiv.org/abs/2507.14450", "authors": ["Jin Lu", "Linhan Fang", "Fan Jiang", "Xingpeng Li"], "title": "A Black Start Strategy for Hydrogen-integrated Renewable Grids with Energy Storage Systems", "comment": "6 pages, 5 figures", "summary": "With the increasing integration of renewable energy, the reliability and\nresilience of modern power systems are of vital significance. However,\nlarge-scale blackouts caused by natural disasters or equipment failures remain\na significant threat, necessitating effective restoration strategies. This\nstudy proposes novel black start models for modern power systems that integrate\nfuel cells and battery storage, recognizing their distinct characteristics and\ncontributions to grid resilience. These models specifically address the\nrestoration of electrical grids, including the energization paths and time of\nthe transmission network, while accounting for the unique power output traits\nof fuel cells and the energy storage capacity of batteries as black start\nresources. Black start simulations, comparing the generator startup sequence\n(GSUS) with fuel cell versus battery systems, are performed on the IEEE 39-bus\nsystem. We conduct sensitivity analyses on fuel cell capacity, battery storage\ncapacity, initial state of charge (SOC), and resource locations to identify\noptimal scenarios for black start operations.", "AI": {"tldr": "本研究提出了针对现代电力系统的黑启动新模型，该模型集成了燃料电池和电池储能，并考虑了它们在电网恢复中的独特特性和贡献。", "motivation": "随着可再生能源的日益整合，现代电力系统的可靠性和弹性至关重要。然而，自然灾害或设备故障引起的大规模停电仍然是重大威胁，因此需要有效的恢复策略。", "method": "研究提出了新的黑启动模型，整合了燃料电池和电池储能，并考虑了它们的独特出力特性和储能容量。模型特别关注电网恢复，包括输电网络的带电路径和时间。通过在IEEE 39节点系统上进行黑启动仿真，比较了发电机启动顺序（GSUS）与燃料电池和电池系统的表现，并对燃料电池容量、电池储能容量、初始荷电状态（SOC）和资源位置进行了敏感性分析。", "result": "通过在IEEE 39节点系统上进行黑启动仿真，比较了发电机启动顺序（GSUS）与燃料电池和电池系统的表现。敏感性分析识别了燃料电池容量、电池储能容量、初始荷电状态（SOC）和资源位置对黑启动操作的最佳情景。", "conclusion": "研究提出的黑启动模型和敏感性分析有助于识别使用燃料电池和电池储能作为黑启动资源的最佳操作情景，从而提高现代电力系统的可靠性和弹性。"}}
{"id": "2507.14231", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14231", "abs": "https://arxiv.org/abs/2507.14231", "authors": ["Khalid Hasan", "Jamil Saquer"], "title": "Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media", "comment": "The 37th International Conference on Software Engineering & Knowledge\n  Engineering, SEKE 2025 (camera-ready)", "summary": "Bipolar disorder is a chronic mental illness frequently underdiagnosed due to\nsubtle early symptoms and social stigma. This paper explores the advanced\nnatural language processing (NLP) models for recognizing signs of bipolar\ndisorder based on user-generated social media text. We conduct a comprehensive\nevaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,\nDistilBERT) and Long Short Term Memory (LSTM) models based on contextualized\n(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed\non a large, annotated dataset of Reddit posts after confirming their validity\nthrough sentiment variance and judgmental analysis. Our results demonstrate\nthat RoBERTa achieves the highest performance among transformer models with an\nF1 score of ~98% while LSTM models using BERT embeddings yield nearly identical\nresults. In contrast, LSTMs trained on static embeddings fail to capture\nmeaningful patterns, scoring near-zero F1. These findings underscore the\ncritical role of contextual language modeling in detecting bipolar disorder. In\naddition, we report model training times and highlight that DistilBERT offers\nan optimal balance between efficiency and accuracy. In general, our study\noffers actionable insights for model selection in mental health NLP\napplications and validates the potential of contextualized language models to\nsupport early bipolar disorder screening.", "AI": {"tldr": "本研究利用先进的自然语言处理模型，特别是基于Transformer的模型，从社交媒体文本中识别双相情感障碍的迹象，并强调了上下文语言建模的重要性。", "motivation": "双相情感障碍因早期症状不明显和社会污名而常被漏诊，这促使研究探索通过用户生成的文本进行早期识别的可能性。", "method": "研究评估了多种基于Transformer的模型（BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT）和长短期记忆（LSTM）模型，后者结合了上下文（BERT）和静态（GloVe, Word2Vec）词嵌入。实验在一个大型、经过标注的Reddit帖子数据集上进行，并确认了数据的有效性。", "result": "RoBERTa在Transformer模型中表现最佳，F1分数达到约98%。使用BERT嵌入的LSTM模型也取得了相似的结果。相比之下，使用静态嵌入训练的LSTM模型未能捕捉到有效模式，F1分数接近零。结果表明上下文语言建模在检测双相情感障碍中至关重要。此外，DistilBERT在效率和准确性之间提供了最佳平衡。", "conclusion": "本研究为精神健康NLP应用中的模型选择提供了实用见解，并验证了上下文语言模型在支持双相情感障碍早期筛查方面的潜力。"}}
{"id": "2507.14367", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14367", "abs": "https://arxiv.org/abs/2507.14367", "authors": ["Weiming Ren", "Raghav Goyal", "Zhiming Hu", "Tristan Ty Aumentado-Armstrong", "Iqbal Mohomed", "Alex Levinshtein"], "title": "Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution", "comment": "12 pages, 17 figures and 7 tables", "summary": "Generative super-resolution (GSR) currently sets the state-of-the-art in\nterms of perceptual image quality, overcoming the \"regression-to-the-mean\" blur\nof prior non-generative models. However, from a human perspective, such models\ndo not fully conform to the optimal balance between quality and fidelity.\nInstead, a different class of artifacts, in which generated details fail to\nperceptually match the low resolution image (LRI) or ground-truth image (GTI),\nis a critical but under studied issue in GSR, limiting its practical\ndeployments. In this work, we focus on measuring, analyzing, and mitigating\nthese artifacts (i.e., \"hallucinations\"). We observe that hallucinations are\nnot well-characterized with existing image metrics or quality models, as they\nare orthogonal to both exact fidelity and no-reference quality. Instead, we\ntake advantage of a multimodal large language model (MLLM) by constructing a\nprompt that assesses hallucinatory visual elements and generates a\n\"Hallucination Score\" (HS). We find that our HS is closely aligned with human\nevaluations, and also provides complementary insights to prior image metrics\nused for super-resolution (SR) models. In addition, we find certain deep\nfeature distances have strong correlations with HS. We therefore propose to\nalign the GSR models by using such features as differentiable reward functions\nto mitigate hallucinations.", "AI": {"tldr": "生成式超分辨率（GSR）模型存在幻觉伪影，现有指标无法有效衡量。本文提出使用多模态大语言模型（MLLM）评估幻觉，并利用深度特征距离作为可微分奖励函数来减轻这些伪影。", "motivation": "生成式超分辨率（GSR）在感知质量上表现出色，但存在生成细节与低分辨率或真实图像不符的“幻觉”伪影。这是一个关键但研究不足的问题，限制了GSR的实际应用。现有图像指标或质量模型未能很好地表征这些幻觉。", "method": "1. 利用多模态大语言模型（MLLM）构建提示，评估视觉幻觉元素，并生成“幻觉分数”（HS）。2. 发现某些深度特征距离与HS强相关。3. 提出使用这些深度特征作为可微分奖励函数，对GSR模型进行对齐，以减轻幻觉。", "result": "1. 提出的幻觉分数（HS）与人类评估高度一致。2. HS为超分辨率模型的现有图像指标提供了补充性洞察。3. 某些深度特征距离与HS有很强的相关性。4. 通过将这些特征用作可微分奖励函数，可以减轻GSR模型中的幻觉。", "conclusion": "幻觉是GSR中一个未被现有指标充分解决的关键问题。MLLM能够有效测量幻觉，并且利用与幻觉分数相关的深度特征作为奖励函数，可以成功地减轻GSR模型中的幻觉伪影。"}}
{"id": "2507.14334", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14334", "abs": "https://arxiv.org/abs/2507.14334", "authors": ["Hui Yang", "Jiaoyan Chen", "Yuan He", "Yongsheng Gao", "Ian Horrocks"], "title": "Language Models as Ontology Encoders", "comment": null, "summary": "OWL (Web Ontology Language) ontologies which are able to formally represent\ncomplex knowledge and support semantic reasoning have been widely adopted\nacross various domains such as healthcare and bioinformatics. Recently,\nontology embeddings have gained wide attention due to its potential to infer\nplausible new knowledge and approximate complex reasoning. However, existing\nmethods face notable limitations: geometric model-based embeddings typically\noverlook valuable textual information, resulting in suboptimal performance,\nwhile the approaches that incorporate text, which are often based on language\nmodels, fail to preserve the logical structure. In this work, we propose a new\nontology embedding method OnT, which tunes a Pretrained Language Model (PLM)\nvia geometric modeling in a hyperbolic space for effectively incorporating\ntextual labels and simultaneously preserving class hierarchies and other\nlogical relationships of Description Logic EL. Extensive experiments on four\nreal-world ontologies show that OnT consistently outperforms the baselines\nincluding the state-of-the-art across both tasks of prediction and inference of\naxioms. OnT also demonstrates strong potential in real-world applications,\nindicated by its robust transfer learning abilities and effectiveness in real\ncases of constructing a new ontology from SNOMED CT. Data and code are\navailable at https://github.com/HuiYang1997/OnT.", "AI": {"tldr": "本文提出了一种名为OnT的本体嵌入方法，它结合了预训练语言模型和双曲几何建模，以同时利用文本信息并保留本体的逻辑结构，在本体推理和预测任务上表现优异。", "motivation": "现有的本体嵌入方法存在局限性：基于几何模型的嵌入通常忽略文本信息导致性能不佳，而结合文本（通常基于语言模型）的方法又未能保留本体的逻辑结构。", "method": "OnT方法通过在双曲空间中进行几何建模来微调预训练语言模型（PLM）。这种方法旨在有效整合文本标签，并同时保留描述逻辑EL的类层次结构及其他逻辑关系。", "result": "在四个真实世界本体上的广泛实验表明，OnT在公理预测和推理两项任务上均持续优于包括最先进方法在内的基线。OnT还在实际应用中展现出强大潜力，体现在其强大的迁移学习能力和从SNOMED CT构建新本体的实际案例中的有效性。", "conclusion": "OnT是一种有效的新型本体嵌入方法，它成功地结合了文本信息和逻辑结构，克服了现有方法的局限性，并在多项任务和实际应用中取得了卓越的性能。"}}
{"id": "2507.14429", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14429", "abs": "https://arxiv.org/abs/2507.14429", "authors": ["Rodrigo A. Lobos", "Xiaokai Wang", "Rex T. L. Fung", "Yongli He", "David Frey", "Dinank Gupta", "Zhongming Liu", "Jeffrey A. Fessler", "Douglas C. Noll"], "title": "Spatiotemporal Maps for Dynamic MRI Reconstruction", "comment": "13 pages, 8 figures", "summary": "The partially separable functions (PSF) model is commonly adopted in dynamic\nMRI reconstruction, as is the underlying signal model in many reconstruction\nmethods including the ones relying on low-rank assumptions. Even though the PSF\nmodel offers a parsimonious representation of the dynamic MRI signal in several\napplications, its representation capabilities tend to decrease in scenarios\nwhere voxels present different temporal/spectral characteristics at different\nspatial locations. In this work we account for this limitation by proposing a\nnew model, called spatiotemporal maps (STMs), that leverages autoregressive\nproperties of (k, t)-space. The STM model decomposes the spatiotemporal MRI\nsignal into a sum of components, each one consisting of a product between a\nspatial function and a temporal function that depends on the spatial location.\nThe proposed model can be interpreted as an extension of the PSF model whose\ntemporal functions are independent of the spatial location. We show that\nspatiotemporal maps can be efficiently computed from autocalibration data by\nusing advanced signal processing and randomized linear algebra techniques,\nenabling STMs to be used as part of many reconstruction frameworks for\naccelerated dynamic MRI. As proof-of-concept illustrations, we show that STMs\ncan be used to reconstruct both 2D single-channel animal gastrointestinal MRI\ndata and 3D multichannel human functional MRI data.", "AI": {"tldr": "提出了一种新的时空映射（STMs）模型，通过允许时域函数依赖于空间位置来解决现有部分可分离函数（PSF）模型在动态MRI重建中对不同空间位置处不同时间/频谱特性的表示能力不足的问题。", "motivation": "动态MRI重建中常用的部分可分离函数（PSF）模型，在体素呈现不同空间位置处具有不同时间/频谱特性的场景中，其表示能力会下降。", "method": "提出时空映射（STMs）模型，将时空MRI信号分解为多个分量的总和，每个分量是空间函数与依赖于空间位置的时间函数之间的乘积。该模型利用(k, t)空间自回归特性，并可通过先进的信号处理和随机线性代数技术从自校准数据中高效计算得到。", "result": "时空映射（STMs）可以从自校准数据中高效计算，并能作为多种加速动态MRI重建框架的一部分。概念验证表明，STMs可用于重建2D单通道动物胃肠道MRI数据和3D多通道人类功能MRI数据。", "conclusion": "STMs模型是PSF模型的扩展，通过引入依赖于空间位置的时域函数，有效解决了PSF模型在处理具有空间变异时间/频谱特性的动态MRI信号时的局限性，并能高效计算和应用于多种重建框架。"}}
{"id": "2507.14700", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14700", "abs": "https://arxiv.org/abs/2507.14700", "authors": ["Nicholas Mohammad", "Nicola Bezzo"], "title": "Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation", "comment": "To be presented in the 64th IEEE Conference on Decision and Control\n  (CDC 25)", "summary": "Safe navigation in unknown and cluttered environments remains a challenging\nproblem in robotics. Model Predictive Contour Control (MPCC) has shown promise\nfor performant obstacle avoidance by enabling precise and agile trajectory\ntracking, however, existing methods lack formal safety assurances. To address\nthis issue, we propose a general Control Lyapunov Function (CLF) and Control\nBarrier Function (CBF) enabled MPCC framework that enforces safety constraints\nderived from a free-space corridor around the planned trajectory. To enhance\nfeasibility, we dynamically adapt the CBF parameters at runtime using a Soft\nActor-Critic (SAC) policy. The approach is validated with extensive simulations\nand an experiment on mobile robot navigation in unknown cluttered environments.", "AI": {"tldr": "本文提出一个结合CLF和CBF的MPCC框架，通过自由空间走廊实现未知复杂环境下的安全导航，并利用SAC动态调整CBF参数以提高可行性。", "motivation": "机器人Slam问题中，现有模型预测等高线控制（MPCC）在避障方面表现出色，但缺乏形式化的安全保证。", "method": "提出一个通用的、由控制李雅普诺夫函数（CLF）和控制障碍函数（CBF）驱动的MPCC框架，通过规划轨迹周围的自由空间走廊来强制执行安全约束。为增强可行性，使用软演员-评论家（SAC）策略在运行时动态调整CBF参数。", "result": "该方法通过大量的仿真和在移动机器人未知杂乱环境导航实验中得到验证。", "conclusion": "该框架为MPCC在复杂环境下的导航提供了形式化的安全保证，并提高了可行性。"}}
{"id": "2507.14595", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14595", "abs": "https://arxiv.org/abs/2507.14595", "authors": ["Tongxin Li"], "title": "Learning-Augmented Control: Adaptively Confidence Learning for Competitive MPC", "comment": "13 pages, 4 figures", "summary": "We introduce Learning-Augmented Control (LAC), an approach that integrates\nuntrusted machine learning predictions into the control of constrained,\nnonlinear dynamical systems. LAC is designed to achieve the\n\"best-of-both-worlds\" guarantees, i.e, near-optimal performance when\npredictions are accurate, and robust, safe performance when they are not. The\ncore of our approach is a delayed confidence learning procedure that optimizes\na confidence parameter online, adaptively balancing between ML and nominal\npredictions. We establish formal competitive ratio bounds for general nonlinear\nsystems under standard MPC regularity assumptions. For the linear quadratic\ncase, we derive a competitive ratio bound that is provably tight, thereby\ncharacterizing the fundamental limits of this learning-augmented approach. The\neffectiveness of LAC is demonstrated in numerical studies, where it maintains\nstability and outperforms standard methods under adversarial prediction errors.", "AI": {"tldr": "引入学习增强控制（LAC），将不可信的机器学习预测集成到受约束非线性动力系统控制中，旨在实现预测准确时近最优，预测不准确时鲁棒安全的“两全其美”性能。", "motivation": "现有控制方法难以在集成机器学习预测时，同时保证预测准确时的优异性能和预测不准确时的鲁棒安全性；需要一种方法来有效处理不可信的机器学习预测，并在受约束的非线性动力系统中提供可靠控制。", "method": "核心方法是“延迟置信度学习过程”，在线优化置信度参数，以自适应平衡机器学习预测与标称预测。为一般非线性系统在标准MPC正则性假设下建立了形式化的竞争比界限，并对线性二次（LQ）情况推导了可证明的紧密竞争比界限。", "result": "建立了针对一般非线性系统的形式化竞争比界限。针对线性二次情况，推导出了可证明的紧密竞争比界限，揭示了该学习增强方法的根本限制。数值研究表明LAC在对抗性预测误差下能保持系统稳定性并优于传统方法。", "conclusion": "LAC成功地将不可信的机器学习预测集成到控制系统中，实现了在预测准确时接近最优性能，在预测不准确时保持鲁棒和安全性能的“两全其美”保证。通过自适应的延迟置信度学习过程，LAC有效平衡了ML和标称预测。理论分析和数值实验证明了LAC在处理预测误差情况下的有效性和优越性。"}}
{"id": "2507.14238", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14238", "abs": "https://arxiv.org/abs/2507.14238", "authors": ["Matthew Kearney", "Reuben Binns", "Yarin Gal"], "title": "Language Models Change Facts Based on the Way You Talk", "comment": null, "summary": "Large language models (LLMs) are increasingly being used in user-facing\napplications, from providing medical consultations to job interview advice.\nRecent research suggests that these models are becoming increasingly proficient\nat inferring identity information about the author of a piece of text from\nlinguistic patterns as subtle as the choice of a few words. However, little is\nknown about how LLMs use this information in their decision-making in\nreal-world applications. We perform the first comprehensive analysis of how\nidentity markers present in a user's writing bias LLM responses across five\ndifferent high-stakes LLM applications in the domains of medicine, law,\npolitics, government benefits, and job salaries. We find that LLMs are\nextremely sensitive to markers of identity in user queries and that race,\ngender, and age consistently influence LLM responses in these applications. For\ninstance, when providing medical advice, we find that models apply different\nstandards of care to individuals of different ethnicities for the same\nsymptoms; we find that LLMs are more likely to alter answers to align with a\nconservative (liberal) political worldview when asked factual questions by\nolder (younger) individuals; and that LLMs recommend lower salaries for\nnon-White job applicants and higher salaries for women compared to men. Taken\ntogether, these biases mean that the use of off-the-shelf LLMs for these\napplications may cause harmful differences in medical care, foster wage gaps,\nand create different political factual realities for people of different\nidentities. Beyond providing an analysis, we also provide new tools for\nevaluating how subtle encoding of identity in users' language choices impacts\nmodel decisions. Given the serious implications of these findings, we recommend\nthat similar thorough assessments of LLM use in user-facing applications are\nconducted before future deployment.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在医疗、法律、政治、政府福利和薪资等高风险应用中，会根据用户的种族、性别和年龄等身份信息，产生显著且有害的偏见。", "motivation": "LLMs日益被用于面向用户的应用，并能从语言模式中推断出作者的身份信息。然而，目前对于LLMs如何利用这些身份信息进行决策知之甚少，尤其是在真实世界的应用中。", "method": "对用户文本中存在的身份标记如何影响LLM响应进行了首次全面的分析。研究涵盖了医学、法律、政治、政府福利和工作薪资五个高风险LLM应用领域，并探究了种族、性别和年龄对LLM响应的一致性影响。此外，还提供了评估用户语言中身份编码对模型决策影响的新工具。", "result": "LLMs对用户查询中的身份标记极其敏感，种族、性别和年龄持续影响LLM的响应。例如，在提供医疗建议时，模型对不同族裔的个体采用不同的护理标准；在回答事实性问题时，LLMs更倾向于根据年龄（年长者偏向保守，年轻者偏向自由）调整答案以符合政治世界观；推荐非白人求职者的薪资较低，而女性求职者的薪资高于男性。", "conclusion": "这些偏见意味着在这些应用中直接使用LLMs可能导致医疗服务差异、加剧工资差距，并为不同身份的人创造不同的政治事实。鉴于这些发现的严重性，建议在未来部署面向用户的LLM应用之前，进行类似的彻底评估。"}}
{"id": "2507.14368", "categories": ["cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.14368", "abs": "https://arxiv.org/abs/2507.14368", "authors": ["Praneeth Namburi", "Roger Pallarès-López", "Jessica Rosendorf", "Duarte Folgado", "Brian W. Anthony"], "title": "DUSTrack: Semi-automated point tracking in ultrasound videos", "comment": null, "summary": "Ultrasound technology enables safe, non-invasive imaging of dynamic tissue\nbehavior, making it a valuable tool in medicine, biomechanics, and sports\nscience. However, accurately tracking tissue motion in B-mode ultrasound\nremains challenging due to speckle noise, low edge contrast, and out-of-plane\nmovement. These challenges complicate the task of tracking anatomical landmarks\nover time, which is essential for quantifying tissue dynamics in many clinical\nand research applications. This manuscript introduces DUSTrack (Deep learning\nand optical flow-based toolkit for UltraSound Tracking), a semi-automated\nframework for tracking arbitrary points in B-mode ultrasound videos. We combine\ndeep learning with optical flow to deliver high-quality and robust tracking\nacross diverse anatomical structures and motion patterns. The toolkit includes\na graphical user interface that streamlines the generation of high-quality\ntraining data and supports iterative model refinement. It also implements a\nnovel optical-flow-based filtering technique that reduces high-frequency\nframe-to-frame noise while preserving rapid tissue motion. DUSTrack\ndemonstrates superior accuracy compared to contemporary zero-shot point\ntrackers and performs on par with specialized methods, establishing its\npotential as a general and foundational tool for clinical and biomechanical\nresearch. We demonstrate DUSTrack's versatility through three use cases:\ncardiac wall motion tracking in echocardiograms, muscle deformation analysis\nduring reaching tasks, and fascicle tracking during ankle plantarflexion. As an\nopen-source solution, DUSTrack offers a powerful, flexible framework for point\ntracking to quantify tissue motion from ultrasound videos. DUSTrack is\navailable at https://github.com/praneethnamburi/DUSTrack.", "AI": {"tldr": "DUSTrack是一个半自动化的超声图像点跟踪框架，结合深度学习和光流法，能克服超声图像挑战，实现高精度、鲁棒的组织运动量化，并已开源。", "motivation": "超声技术虽能安全无创成像动态组织行为，但B模式超声图像中的斑点噪声、低边缘对比度和平面外运动使得准确跟踪组织运动和解剖标志物变得困难，这限制了在临床和研究中量化组织动力学的应用。", "method": "本文提出DUSTrack，一个结合深度学习和光流法的半自动化工具包，用于跟踪B模式超声视频中的任意点。它包含一个图形用户界面，用于生成高质量训练数据和支持模型迭代优化。此外，还实现了一种新颖的基于光流的滤波技术，以减少高频帧间噪声同时保留快速组织运动。", "result": "DUSTrack在准确性方面优于现有零样本点跟踪器，并与专业方法表现相当。其通用性通过三个用例得到验证：超声心动图中心脏壁运动跟踪、抓取任务中肌肉变形分析以及踝关节跖屈时肌束跟踪。", "conclusion": "DUSTrack是一个通用且基础的工具，为临床和生物力学研究提供了强大、灵活的超声视频组织运动量化解决方案，且作为开源工具可广泛应用。"}}
{"id": "2507.14335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14335", "abs": "https://arxiv.org/abs/2507.14335", "authors": ["Nicolas Wischermann", "Claudio Mayrink Verdun", "Gabriel Poesia", "Francesco Noseda"], "title": "ProofCompass: Enhancing Specialized Provers with LLM Guidance", "comment": "19 pages, 7 figures. Accepted at the 2nd AI for MATH Workshop at the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "Language models have become increasingly powerful tools for formal\nmathematical reasoning. However, most existing approaches rely exclusively on\neither large general-purpose models or smaller specialized models, each with\ndistinct limitations, while training specialized large models still requires\nsignificant computational resources. This paper introduces ProofCompass, a\nnovel hybrid methodology that achieves remarkable computational efficiency by\nstrategically guiding existing specialized prover methods, such as\nDeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without\nrequiring additional model training. The LLM provides natural language proof\nstrategies and analyzes failed attempts to select intermediate lemmas, enabling\neffective problem decomposition. On the miniF2F benchmark, ProofCompass\ndemonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\\%\n\\rightarrow 55.3\\%$) while using 25x fewer attempts ($3200 \\rightarrow 128$).\nOur synergistic approach paves the way for simultaneously improving\ncomputational efficiency and accuracy in formal theorem proving.", "AI": {"tldr": "ProofCompass是一种混合方法，利用大型语言模型（LLM）指导现有专业证明器进行形式数学推理，显著提高了计算效率和准确性。", "motivation": "现有的形式数学推理方法，无论是大型通用模型还是小型专业模型，都存在局限性，且训练大型专业模型需要大量计算资源。本文旨在解决这一效率和资源消耗问题。", "method": "本文提出了ProofCompass，一种无需额外模型训练的混合方法。它利用LLM提供自然语言证明策略并分析失败尝试，以选择中间引理，从而有效分解问题。LLM战略性地指导现有的专业证明器（如DeepSeek-Prover-v1.5-RL）。", "result": "在miniF2F基准测试中，ProofCompass在证明成功率上略优于DSP-v1.5（从54.9%提升至55.3%），同时将尝试次数减少了25倍（从3200次减少到128次），展现出显著的资源效率。", "conclusion": "ProofCompass的协同方法为形式定理证明同时提高了计算效率和准确性，为该领域开辟了新途径。"}}
{"id": "2507.14760", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14760", "abs": "https://arxiv.org/abs/2507.14760", "authors": ["Cassandra Tong Ye", "Shamus Li", "Tyler King", "Kristina Monakhova"], "title": "QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems", "comment": null, "summary": "Deep learning models often hallucinate, producing realistic artifacts that\nare not truly present in the sample. This can have dire consequences for\nscientific and medical inverse problems, such as MRI and microscopy denoising,\nwhere accuracy is more important than perceptual quality. Uncertainty\nquantification techniques, such as conformal prediction, can pinpoint outliers\nand provide guarantees for image regression tasks, improving reliability.\nHowever, existing methods utilize a linear constant scaling factor to calibrate\nuncertainty bounds, resulting in larger, less informative bounds. We propose\nQUTCC, a quantile uncertainty training and calibration technique that enables\nnonlinear, non-uniform scaling of quantile predictions to enable tighter\nuncertainty estimates. Using a U-Net architecture with a quantile embedding,\nQUTCC enables the prediction of the full conditional distribution of quantiles\nfor the imaging task. During calibration, QUTCC generates uncertainty bounds by\niteratively querying the network for upper and lower quantiles, progressively\nrefining the bounds to obtain a tighter interval that captures the desired\ncoverage. We evaluate our method on several denoising tasks as well as\ncompressive MRI reconstruction. Our method successfully pinpoints\nhallucinations in image estimates and consistently achieves tighter uncertainty\nintervals than prior methods while maintaining the same statistical coverage.", "AI": {"tldr": "本文提出QUTCC，一种分位数不确定性训练和校准技术，旨在为科学和医学图像逆问题中的深度学习模型提供更紧凑、信息更丰富的不确定性估计，以有效识别模型幻觉并提高可靠性。", "motivation": "深度学习模型在科学和医学图像逆问题（如MRI和显微镜去噪）中容易产生“幻觉”，生成不真实但逼真的伪影，这在需要高精度的领域是不可接受的。现有不确定性量化方法（如共形预测）使用线性常数缩放因子校准不确定性边界，导致边界过大且信息量不足。", "method": "本文提出QUTCC（Quantile Uncertainty Training and Calibration Technique），它通过结合U-Net架构和分位数嵌入，实现对图像任务中分位数完整条件分布的预测。在校准阶段，QUTCC通过迭代查询网络获取上下分位数，逐步优化不确定性边界，以获得更紧凑的区间并捕获所需的覆盖率。该方法允许对分位数预测进行非线性、非均匀缩放。", "result": "QUTCC在多个去噪任务和压缩MRI重建中进行了评估。结果表明，该方法成功识别了图像估计中的幻觉，并且在保持相同统计覆盖率的同时，始终比现有方法获得更紧凑的不确定性区间。", "conclusion": "QUTCC提供了一种有效的方法来量化深度学习模型在科学和医学图像逆问题中的不确定性。通过实现更紧凑、信息量更大的不确定性估计，它能够更好地识别模型幻觉，从而显著提高这些关键应用中的模型可靠性和准确性。"}}
{"id": "2507.14721", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14721", "abs": "https://arxiv.org/abs/2507.14721", "authors": ["Keita Kobashi", "Masayoshi Tomizuka"], "title": "Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls", "comment": "7 pages, 7 figures", "summary": "This study addresses the problem of occluded grasping, where primary grasp\nconfigurations of an object are not available due to occlusion with\nenvironment. Simple parallel grippers often struggle with such tasks due to\nlimited dexterity and actuation constraints. Prior works have explored object\npose reorientation such as pivoting by utilizing extrinsic contacts between an\nobject and an environment feature like a wall, to make the object graspable.\nHowever, such works often assume the presence of a short wall, and this\nassumption may not always hold in real-world scenarios. If the wall available\nfor interaction is too large or too tall, the robot may still fail to grasp the\nobject even after pivoting, and the robot must combine different types of\nactions to grasp. To address this, we propose a hierarchical reinforcement\nlearning (RL) framework. We use Q-learning to train a high-level policy that\nselects the type of action expected to yield the highest reward. The selected\nlow-level skill then samples a specific robot action in continuous space. To\nguide the robot to an appropriate location for executing the selected action,\nwe adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on\nthe object point cloud and the skill ID, enabling it to infer a suitable\nlocation based on the object geometry and the selected skill. To promote\ngeneralization, we apply domain randomization during the training of low-level\nskills. The RL policy is trained entirely in simulation with a box-like object\nand deployed to six objects in real world. We conduct experiments to evaluate\nour method and demonstrate both its generalizability and robust sim-to-real\ntransfer performance with promising success rates.", "AI": {"tldr": "本研究提出了一种分层强化学习框架，结合条件变分自编码器（CVAE），以解决简单夹持器在存在遮挡且环境特征（如墙壁）过高时难以抓取物体的问题，并实现了良好的仿真到真实世界迁移。", "motivation": "简单的平行夹持器在存在遮挡的抓取任务中表现不佳，因为其灵活性和驱动限制。现有通过利用物体与环境特征（如墙壁）的外部接触来重新定位物体（如枢转）的工作，通常假设墙壁较短。然而，在真实世界场景中，如果可用的墙壁过大或过高，即使经过枢转，机器人也可能无法抓取物体，需要结合不同类型的动作来完成抓取。", "method": "本研究提出了一个分层强化学习（RL）框架。高层策略使用Q-learning训练，用于选择预期回报最高的动作类型。选定的低层技能则在连续空间中采样具体的机器人动作。为了引导机器人到执行所选动作的适当位置，采用了条件变分自编码器（CVAE），该CVAE以物体点云和技能ID为条件，根据物体几何形状和所选技能推断合适的位置。为了促进泛化，在低层技能训练期间应用了域随机化。RL策略完全在仿真中使用盒状物体进行训练。", "result": "该方法在真实世界中部署到六个物体上进行评估。实验结果表明，该方法具有良好的泛化能力和鲁棒的仿真到真实世界迁移性能，并取得了可喜的成功率。", "conclusion": "所提出的分层强化学习框架有效地解决了简单夹持器在复杂遮挡环境下的抓取问题，通过结合高层动作选择和低层动作执行，并利用CVAE进行位置推断，实现了对多种物体的有效抓取，展现了良好的泛化能力和仿真到真实世界的迁移性能。"}}
{"id": "2507.14601", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14601", "abs": "https://arxiv.org/abs/2507.14601", "authors": ["Giacomo Oliveri", "Francesco Zardi", "Aaron Angel Salas Sancez", "Andrea Massa"], "title": "One-Time Programmable Passive Electromagnetic Skins", "comment": null, "summary": "The implementation of simple, inexpensive, and mass-production-oriented\nsolutions for smart electromagnetic environments (SEMEs) is dealt with by\nintroducing the concept of \"one-time programmable\" electromagnetic skins\n(OTP-EMSs). The simultaneous achievement of modular fabrication, (one-time)\nconfigurable reflection properties, passive-static operation, and zero\nmaintenance is yielded by integrating expendable components at the atomic level\nof EMSs. Towards this end, an OTP meta-atom structure is properly defined and\noptimized to build EMSs featuring the desired scenario-dependent EM wave\nmanipulation functionalities. In order to illustrate the features as well as to\npoint out the potentialities of OTP-EMSs, a representative set of analytical,\nnumerical, and experimental results is reported by considering different\napertures, illuminations, and EM wave manipulation requirements.", "AI": {"tldr": "本文提出一次性可编程电磁皮肤（OTP-EMS）概念，为智能电磁环境（SEMEs）提供简单、廉价、可大规模生产的解决方案，实现一次性可配置的电磁波操控。", "motivation": "旨在为智能电磁环境（SEMEs）提供简单、廉价、可大规模生产的解决方案，同时实现模块化制造、可配置反射特性、被动静态操作和零维护。", "method": "引入“一次性可编程”电磁皮肤（OTP-EMS）概念，通过在电磁皮肤的原子层面集成一次性组件实现。定义并优化OTP超原子结构，以构建具有特定场景依赖电磁波操控功能的电磁皮肤。", "result": "OTP-EMS实现了模块化制造、一次性可配置反射特性、被动静态操作和零维护。通过分析、数值和实验结果，展示了其在不同孔径、照明和电磁波操控需求下的特性和潜力。", "conclusion": "OTP-EMS提供了一种简单、廉价、可大规模生产的智能电磁环境解决方案，能够实现场景依赖的电磁波操控功能。"}}
{"id": "2507.14239", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14239", "abs": "https://arxiv.org/abs/2507.14239", "authors": ["Weihua Zheng", "Roy Ka-Wei Lee", "Zhengyuan Liu", "Kui Wu", "AiTi Aw", "Bowei Zou"], "title": "CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation", "comment": null, "summary": "Multilingual Large Language Models(MLLMs) demonstrate strong generalization\nacross languages, yet they remain prone to hallucinations, especially in\nlow-resource languages, due to training data imbalances. These hallucinations,\nwhich include inaccurate or fabricated outputs, are particularly problematic in\ndomain-specific generation tasks (Chataigner et al., 2024). To address this\nchallenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based\nCross-lingual Chain-of-Thought), a two-stage fine-tuning framework for\nmitigating hallucination in MLLMs. Our approach first enhances cross-lingual\nsemantic alignment through curriculum-based contrastive learning combined with\nnext-token prediction during continued pre-training. Building on this\nfoundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting\nstrategy during instruction fine-tuning, which guides the model to reason in a\nhigh-resource language before generating answers in the target low-resource\nlanguage. Experimental results show that CCL-XCoT reduces hallucination rates\nby up to 62% and substantially improves factual knowledge transfer across\nlanguage pairs, without relying on external retrieval or multi-model ensembles.", "AI": {"tldr": "本文提出CCL-XCoT框架，通过两阶段微调（课程对比学习增强跨语言对齐和跨语言思维链提示策略）来显著降低多语言大模型在低资源语言中的幻觉，并提升事实知识迁移。", "motivation": "多语言大模型（MLLMs）在低资源语言中易产生幻觉，尤其是在领域特定生成任务中，这主要源于训练数据的不平衡，导致输出不准确或虚构。", "method": "CCL-XCoT是一个两阶段微调框架：1. 在持续预训练阶段，结合课程式对比学习和下一词预测，增强跨语言语义对齐。2. 在指令微调阶段，引入跨语言思维链（XCoT）提示策略，引导模型先用高资源语言推理，再用目标低资源语言生成答案。", "result": "实验结果表明，CCL-XCoT将幻觉率降低了高达62%，并显著改善了跨语言对的事实知识迁移，且无需依赖外部检索或多模型集成。", "conclusion": "CCL-XCoT通过其独特的两阶段微调策略，有效缓解了多语言大模型在低资源语言中的幻觉问题，并提升了跨语言知识迁移能力。"}}
{"id": "2507.14426", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14426", "abs": "https://arxiv.org/abs/2507.14426", "authors": ["Zhou Chen", "Joe Lin", "Sathyanarayanan N. Aakur"], "title": "CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding", "comment": "Accepted to NeSy 2025", "summary": "We introduce CRAFT, a neuro-symbolic framework for interpretable affordance\ngrounding, which identifies the objects in a scene that enable a given action\n(e.g., \"cut\"). CRAFT integrates structured commonsense priors from ConceptNet\nand language models with visual evidence from CLIP, using an energy-based\nreasoning loop to refine predictions iteratively. This process yields\ntransparent, goal-driven decisions to ground symbolic and perceptual\nstructures. Experiments in multi-object, label-free settings demonstrate that\nCRAFT enhances accuracy while improving interpretability, providing a step\ntoward robust and trustworthy scene understanding.", "AI": {"tldr": "CRAFT是一种神经符号框架，通过整合常识知识、语言模型和视觉证据，并利用基于能量的推理循环，实现可解释的物体功能（affordance）识别，提升场景理解的准确性和可信度。", "motivation": "研究旨在识别场景中支持特定动作的物体（即功能接地），并解决现有方法在可解释性、鲁棒性和可信度方面的不足。", "method": "引入CRAFT框架，该框架结合了来自ConceptNet的结构化常识先验知识、语言模型以及来自CLIP的视觉证据。它采用基于能量的推理循环来迭代优化预测。", "result": "在多物体、无标签设置下的实验表明，CRAFT提升了准确性并增强了可解释性，能够提供透明、目标驱动的决策来连接符号和感知结构。", "conclusion": "CRAFT为实现鲁棒和可信赖的场景理解迈出了重要一步，通过提高功能接地的准确性和可解释性来达成这一目标。"}}
{"id": "2507.14393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14393", "abs": "https://arxiv.org/abs/2507.14393", "authors": ["Humza Sami", "Mubashir ul Islam", "Pierre-Emmanuel Gaillardon", "Valerio Tenace"], "title": "Adaptive Multi-Agent Reasoning via Automated Workflow Generation", "comment": null, "summary": "The rise of Large Reasoning Models (LRMs) promises a significant leap forward\nin language model capabilities, aiming to tackle increasingly sophisticated\ntasks with unprecedented efficiency and accuracy. However, despite their\nimpressive performance, recent studies have highlighted how current reasoning\nmodels frequently fail to generalize to novel, unseen problems, often resorting\nto memorized solutions rather than genuine inferential reasoning. Such behavior\nunderscores a critical limitation in modern LRMs, i.e., their tendency toward\noverfitting, which in turn results in poor generalization in problem-solving\ncapabilities.\n  In this paper, we introduce Nexus Architect, an enhanced iteration of our\nmulti-agent system framework, Nexus, equipped with a novel automated workflow\nsynthesis mechanism. Given a user's prompt and a small set of representative\nexamples, the Architect autonomously generates a tailored reasoning workflow by\nselecting suitable strategies, tool integrations, and adversarial techniques\nfor a specific problem class. Furthermore, the Architect includes an iterative\nprompt refinement mechanism that fine-tunes agents' system prompts to maximize\nperformance and improve the generalization capabilities of the system.\n  We empirically evaluate Nexus Architect by employing an off-the-shelf,\nnon-reasoning model on a custom dataset of challenging logical questions and\ncompare its performance against state-of-the-art LRMs. Results show that Nexus\nArchitect consistently outperforms existing solutions, achieving up to a 66%\nincrease in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against\nClaude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.", "AI": {"tldr": "本文提出了Nexus Architect，一个增强型多智能体系统，通过自动化工作流合成和迭代提示词优化，显著提升了大型推理模型（LRMs）的泛化能力，在逻辑推理任务上超越了现有SOTA模型。", "motivation": "当前的大型推理模型（LRMs）尽管性能出色，但普遍存在泛化能力差的问题，常通过记忆而非真正的推理来解决新问题，表现出过拟合倾向，导致问题解决能力受限。", "method": "引入了Nexus Architect，作为其多智能体系统框架Nexus的增强版。它包含一个新颖的自动化工作流合成机制，能根据用户提示和少量示例，自主生成定制的推理工作流（选择策略、工具集成和对抗技术）。此外，它还包含一个迭代提示词优化机制，用于微调智能体的系统提示词，以最大化性能并提高系统泛化能力。实验中，使用一个现成的非推理模型，在自定义的挑战性逻辑问题数据集上评估了Nexus Architect。", "result": "Nexus Architect在实验中持续优于现有解决方案。与Gemini 2.5 Flash Preview相比，通过率提高了66%；与Claude Sonnet 4和DeepSeek-R1相比，通过率提高了近2.5倍；与Llama 4 Scout相比，通过率提高了3倍以上。", "conclusion": "Nexus Architect通过其独特的自动化工作流合成和迭代提示词优化机制，有效解决了大型推理模型过拟合和泛化能力不足的问题，显著提升了模型在复杂逻辑推理任务上的性能，并超越了目前最先进的LRMs。"}}
{"id": "2507.15078", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2507.15078", "abs": "https://arxiv.org/abs/2507.15078", "authors": ["Fumio Hashimoto", "Kuang Gong"], "title": "PET Image Reconstruction Using Deep Diffusion Image Prior", "comment": "11 pages, 11 figures", "summary": "Diffusion models have shown great promise in medical image denoising and\nreconstruction, but their application to Positron Emission Tomography (PET)\nimaging remains limited by tracer-specific contrast variability and high\ncomputational demands. In this work, we proposed an anatomical prior-guided PET\nimage reconstruction method based on diffusion models, inspired by the deep\ndiffusion image prior (DDIP) framework. The proposed method alternated between\ndiffusion sampling and model fine-tuning guided by the PET sinogram, enabling\nthe reconstruction of high-quality images from various PET tracers using a\nscore function pretrained on a dataset of another tracer. To improve\ncomputational efficiency, the half-quadratic splitting (HQS) algorithm was\nadopted to decouple network optimization from iterative PET reconstruction. The\nproposed method was evaluated using one simulation and two clinical datasets.\nFor the simulation study, a model pretrained on [$^{18}$F]FDG data was tested\non amyloid-negative PET data to assess out-of-distribution (OOD) performance.\nFor the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one\n[$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from\nanother tracer. Experiment results show that the proposed PET reconstruction\nmethod can generalize robustly across tracer distributions and scanner types,\nproviding an efficient and versatile reconstruction framework for low-dose PET\nimaging.", "AI": {"tldr": "该研究提出了一种基于扩散模型的解剖先验引导PET图像重建方法，旨在解决示踪剂特异性对比度变异性和高计算成本问题，并实现了跨示踪剂分布和扫描仪类型的鲁棒泛化。", "motivation": "扩散模型在医学图像去噪和重建方面潜力巨大，但在PET成像中的应用受限于示踪剂特异性对比度变异性及高计算需求。", "method": "受深度扩散图像先验（DDIP）框架启发，提出了一种解剖先验引导的PET图像重建方法。该方法在扩散采样和由PET正弦图引导的模型微调之间交替进行。为提高计算效率，采用了半二次分裂（HQS）算法来解耦网络优化与迭代PET重建。模型可以在一个示踪剂数据集上预训练，并应用于其他示踪剂的重建。", "result": "在模拟研究和两个临床数据集（包括低剂量[18F]FDG和[18F]Florbetapir数据）上进行了评估。结果表明，所提出的PET重建方法能够稳健地泛化到不同的示踪剂分布和扫描仪类型。", "conclusion": "该研究为低剂量PET成像提供了一个高效且多功能的重建框架，能够有效应对示踪剂特异性对比度变异性和计算效率挑战。"}}
{"id": "2507.14731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14731", "abs": "https://arxiv.org/abs/2507.14731", "authors": ["Haitong Wang", "Aaron Hao Tan", "Angus Fung", "Goldie Nejat"], "title": "X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots", "comment": null, "summary": "Existing navigation methods are primarily designed for specific robot\nembodiments, limiting their generalizability across diverse robot platforms. In\nthis paper, we introduce X-Nav, a novel framework for end-to-end\ncross-embodiment navigation where a single unified policy can be deployed\nacross various embodiments for both wheeled and quadrupedal robots. X-Nav\nconsists of two learning stages: 1) multiple expert policies are trained using\ndeep reinforcement learning with privileged observations on a wide range of\nrandomly generated robot embodiments; and 2) a single general policy is\ndistilled from the expert policies via navigation action chunking with\ntransformer (Nav-ACT). The general policy directly maps visual and\nproprioceptive observations to low-level control commands, enabling\ngeneralization to novel robot embodiments. Simulated experiments demonstrated\nthat X-Nav achieved zero-shot transfer to both unseen embodiments and\nphotorealistic environments. A scalability study showed that the performance of\nX-Nav improves when trained with an increasing number of randomly generated\nembodiments. An ablation study confirmed the design choices of X-Nav.\nFurthermore, real-world experiments were conducted to validate the\ngeneralizability of X-Nav in real-world environments.", "AI": {"tldr": "X-Nav是一个新颖的跨机器人形态导航框架，通过专家策略蒸馏，使单一通用策略能应用于轮式和四足机器人，实现零样本泛化。", "motivation": "现有导航方法主要针对特定机器人形态设计，导致其在不同机器人平台上的泛化能力受限。", "method": "X-Nav包含两个学习阶段：1) 使用深度强化学习在大量随机生成的机器人形态上训练多个专家策略，并利用特权观测；2) 通过导航动作分块结合Transformer（Nav-ACT）从专家策略中蒸馏出一个单一的通用策略。该通用策略直接将视觉和本体感受观测映射到低级控制指令。", "result": "模拟实验表明，X-Nav实现了对未见形态和真实感环境的零样本迁移。可扩展性研究显示，随着训练形态数量的增加，X-Nav的性能有所提升。消融研究证实了X-Nav的设计选择。此外，真实世界实验也验证了X-Nav在实际环境中的泛化能力。", "conclusion": "X-Nav是一个有效的端到端跨形态导航框架，其单一通用策略能够成功部署在各种轮式和四足机器人上，并在模拟和真实世界环境中展现出强大的泛化能力。"}}
{"id": "2507.14727", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14727", "abs": "https://arxiv.org/abs/2507.14727", "authors": ["Jiayu Ding", "Benjamin Seleb", "Saad Bhamla", "Zhenyu Gan"], "title": "Gait Transitions in Load-Pulling Quadrupeds: Insights from Sled Dogs and a Minimal SLIP Model", "comment": null, "summary": "Quadrupedal animals employ diverse galloping strategies to optimize speed,\nstability, and energy efficiency. However, the biomechanical mechanisms that\nenable adaptive gait transitions during high-speed locomotion under load remain\npoorly understood. In this study, we present new empirical and modeling\ninsights into the biomechanics of load-pulling quadrupeds, using sprint sled\ndogs as a model system. High-speed video and force recordings reveal that sled\ndogs often switch between rotary and transverse galloping gaits within just a\nfew strides and without any observable changes in speed, stride duration, or\nterrain, providing clear evidence of locomotor multistability during high-speed\nload-pulling. To investigate the mechanical basis of these transitions, a\nphysics-based quadrupedal Spring-Loaded Inverted Pendulum model with hybrid\ndynamics and prescribed footfall sequences to reproduce the asymmetric\ngalloping patterns observed in racing sled dogs. Through trajectory\noptimization, we replicate experimentally observed gait sequences and identify\nswing-leg stiffness modulation as a key control mechanism for inducing\ntransitions. This work provides a much-needed biomechanical perspective on\nhigh-speed animal draft and establishes a modeling framework for studying\nlocomotion in pulling quadrupeds, with implications for both biological\nunderstanding and the design of adaptive legged systems.", "AI": {"tldr": "研究发现雪橇犬在高速负重奔跑时能快速切换奔跑步态（旋转式和横向式），且不受速度或地形影响，表明存在运动多稳态性。建模揭示摆动腿刚度调节是步态转换的关键控制机制。", "motivation": "目前对四足动物在高速负重运动中，如何进行适应性步态转换的生物力学机制了解甚少。", "method": "本研究结合了经验数据和建模分析。经验数据通过高速视频和力记录采集自雪橇犬。建模方面，构建了一个基于物理的四足弹簧负载倒立摆（SLIP）模型，该模型具有混合动力学和预设的足部着地顺序，并通过轨迹优化来复现实验观察到的步态序列。", "result": "实验观察到雪橇犬在高速负重牵引时，能在几步之内频繁切换旋转式和横向奔跑步态，且速度、步幅持续时间或地形无明显变化，这提供了高速负重运动中存在运动多稳态性的明确证据。模型分析进一步识别出摆动腿刚度调节是诱导步态转换的关键控制机制。", "conclusion": "本研究为高速动物牵引提供了重要的生物力学视角，并建立了一个研究牵引四足动物运动的建模框架，对生物学理解和自适应腿式系统设计均具有重要意义。"}}
{"id": "2507.14240", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14240", "abs": "https://arxiv.org/abs/2507.14240", "authors": ["Mohammad Shahedur Rahman", "Peng Gao", "Yuede Ji"], "title": "HuggingGraph: Understanding the Supply Chain of LLM Ecosystem", "comment": "10 pages, 5 figures", "summary": "Large language models (LLMs) leverage deep learning to process and predict\nsequences of words from context, enabling them to perform various NLP tasks,\nsuch as translation, summarization, question answering, and content generation.\nHowever, the growing size and complexity of developing, training, and deploying\nadvanced LLMs require extensive computational resources and large datasets.\nThis creates a barrier for users. As a result, platforms that host models and\ndatasets are widely used. For example, Hugging Face, one of the most popular\nplatforms, hosted 1.8 million models and 450K datasets by June 2025, with no\nsign of slowing down. Since many LLMs are built from base models, pre-trained\nmodels, and external datasets, they can inherit vulnerabilities, biases, or\nmalicious components from earlier models or datasets. Therefore, it is critical\nto understand the origin and development of these components to better detect\npotential risks, improve model fairness, and ensure compliance. Motivated by\nthis, our project aims to study the relationships between models and datasets,\nwhich are core components of the LLM supply chain. First, we design a method to\nsystematically collect LLM supply chain data. Using this data, we build a\ndirected heterogeneous graph to model the relationships between models and\ndatasets, resulting in a structure with 397,376 nodes and 453,469 edges. We\nthen perform various analyses and uncover several findings, such as: (i) the\nLLM supply chain graph is large, sparse, and follows a power-law degree\ndistribution; (ii) it features a densely connected core and a fragmented\nperiphery; (iii) datasets play pivotal roles in training; (iv) strong\ninterdependence exists between models and datasets; and (v) the graph is\ndynamic, with daily updates reflecting the ecosystem's ongoing evolution.", "AI": {"tldr": "该研究旨在通过构建并分析一个异构图，系统地理解大型语言模型（LLM）供应链中模型与数据集之间的复杂关系，以识别潜在风险、提高公平性并确保合规性。", "motivation": "LLMs的日益增长的规模和复杂性导致其开发、训练和部署需要大量计算资源和数据集，形成了用户障碍。由于许多LLMs是基于基础模型、预训练模型和外部数据集构建的，它们可能继承漏洞、偏见或恶意组件。因此，理解这些组件的来源和发展对于检测潜在风险、提高模型公平性和确保合规性至关重要。", "method": "项目设计了一种系统收集LLM供应链数据的方法。利用这些数据，构建了一个有向异构图（包含397,376个节点和453,469条边），以建模模型和数据集之间的关系。随后对该图进行了多方面分析。", "result": "分析结果揭示了多项发现：(i) LLM供应链图巨大、稀疏，并遵循幂律度分布；(ii) 它具有一个密集连接的核心和碎片化的外围；(iii) 数据集在训练中扮演关键角色；(iv) 模型和数据集之间存在强烈的相互依赖性；(v) 该图是动态的，每日更新反映了生态系统的持续演变。", "conclusion": "通过对LLM供应链中模型与数据集关系的图分析，研究揭示了其规模、结构特征、核心组件（数据集）的关键作用、组件间的相互依赖性以及其动态演变，这对于风险检测、公平性和合规性具有重要意义。"}}
{"id": "2507.14432", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14432", "abs": "https://arxiv.org/abs/2507.14432", "authors": ["Han Gong", "Qiyue Li", "Zhi Liu", "Hao Zhou", "Peng Yuan Zhou", "Zhu Li", "Jie Li"], "title": "Adaptive 3D Gaussian Splatting Video Streaming", "comment": null, "summary": "The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the\nquality of volumetric video representation. Meanwhile, in contrast to\nconventional volumetric video, 3DGS video poses significant challenges for\nstreaming due to its substantially larger data volume and the heightened\ncomplexity involved in compression and transmission. To address these issues,\nwe introduce an innovative framework for 3DGS volumetric video streaming.\nSpecifically, we design a 3DGS video construction method based on the Gaussian\ndeformation field. By employing hybrid saliency tiling and differentiated\nquality modeling of 3DGS video, we achieve efficient data compression and\nadaptation to bandwidth fluctuations while ensuring high transmission quality.\nThen we build a complete 3DGS video streaming system and validate the\ntransmission performance. Through experimental evaluation, our method\ndemonstrated superiority over existing approaches in various aspects, including\nvideo quality, compression effectiveness, and transmission rate.", "AI": {"tldr": "本文提出一个创新的3DGS体视频流传输框架，通过高斯形变场构建、混合显著性分块和差异化质量建模，实现高效压缩和带宽自适应，提升传输质量。", "motivation": "3DGS体视频相较传统体视频数据量大、压缩传输复杂，对流媒体构成挑战。", "method": "设计基于高斯形变场的3DGS视频构建方法；采用混合显著性分块和差异化质量建模实现高效数据压缩和带宽自适应；构建完整的3DGS视频流传输系统。", "result": "实验评估表明，该方法在视频质量、压缩效率和传输速率方面优于现有方法。", "conclusion": "所提出的框架有效解决了3DGS体视频流传输中的挑战，实现了高质量、高效率的传输。"}}
{"id": "2507.14406", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14406", "abs": "https://arxiv.org/abs/2507.14406", "authors": ["Michael J. Zellinger", "Matt Thomson"], "title": "Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering", "comment": "8 pages, 5 figures", "summary": "State-of-the-art reasoning LLMs are powerful problem solvers, but they still\noccasionally make mistakes. However, adopting AI models in risk-sensitive\ndomains often requires error rates near 0%. To address this gap, we propose\ncollaboration between a reasoning model and a human expert who resolves queries\nthe model cannot confidently answer. We find that quantifying the uncertainty\nof a reasoning model through the length of its reasoning trace yields an\neffective basis for deferral to a human, e.g., cutting the error rate of Qwen3\n235B-A22B on difficult MATH problems from 3% to less than 1% when deferring\n7.5% of queries. However, the high latency of reasoning models still makes them\nchallenging to deploy on use cases with high query volume. To address this\nchallenge, we explore fronting a reasoning model with a large non-reasoning\nmodel. We call this modified human-in-the-loop system \"Fail Fast, or Ask\",\nsince the non-reasoning model may defer difficult queries to the human expert\ndirectly (\"failing fast\"), without incurring the reasoning model's higher\nlatency. We show that this approach yields around 40% latency reduction and\nabout 50% cost savings for DeepSeek R1 while maintaining 90+% area under the\naccuracy-rejection curve. However, we observe that latency savings are lower\nthan expected because of \"latency drag\", the phenomenon that processing easier\nqueries with a non-reasoning model pushes the reasoning model's latency\ndistribution towards longer latencies. Broadly, our results suggest that the\ndeficiencies of state-of-the-art reasoning models -- nontrivial error rates and\nhigh latency -- can be substantially mitigated through black-box systems\nengineering, without requiring access to LLM internals.", "AI": {"tldr": "本文提出通过人类与推理LLM协作来降低错误率，并引入“快速失败或提问”（Fail Fast, or Ask）系统，利用非推理模型作为前端来减少推理LLM的延迟和成本，同时保持高准确率，无需访问LLM内部。", "motivation": "尽管推理LLM功能强大，但仍会犯错，而风险敏感领域需要接近零的错误率。此外，推理模型的延迟较高，难以应对高查询量用例。", "method": "1. 通过推理痕迹长度量化推理模型的置信度，作为将查询转交给人类专家的依据。2. 提出“快速失败或提问”系统：使用大型非推理模型作为前端，可以直接将困难查询转交给人类专家（“快速失败”），避免推理模型的高延迟。", "result": "1. 通过将7.5%的查询转交人类，Qwen3 235B-A22B在MATH难题上的错误率从3%降至不到1%。2. “快速失败或提问”方法为DeepSeek R1节省了约40%的延迟和约50%的成本，同时保持了90%以上的准确率-拒绝曲线下面积。3. 观察到“延迟拖拽”现象：非推理模型处理简单查询反而使推理模型的延迟分布偏向更长。", "conclusion": "推理LLM的缺陷（非零错误率和高延迟）可以通过黑盒系统工程显著缓解，而无需访问LLM内部结构。"}}
{"id": "2507.15151", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15151", "abs": "https://arxiv.org/abs/2507.15151", "authors": ["Sebastian A. Cruz Romero", "Wilfredo E. Lugo Beauchamp"], "title": "Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection", "comment": "Accepted at International Symposium on Intelligent Computing &\n  Networks 2025", "summary": "Anemia is a widespread global health issue, particularly among young children\nin low-resource settings. Traditional methods for anemia detection often\nrequire expensive equipment and expert knowledge, creating barriers to early\nand accurate diagnosis. To address these challenges, we explore the use of deep\nlearning models for detecting anemia through conjunctival pallor, focusing on\nthe CP-AnemiC dataset, which includes 710 images from children aged 6-59\nmonths. The dataset is annotated with hemoglobin levels, gender, age and other\ndemographic data, enabling the development of machine learning models for\naccurate anemia detection. We use the MobileNet architecture as a backbone,\nknown for its efficiency in mobile and embedded vision applications, and\nfine-tune our model end-to-end using data augmentation techniques and a\ncross-validation strategy. Our model implementation achieved an accuracy of\n0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong\nperformance on the dataset. To optimize the model for deployment on edge\ndevices, we performed post-training quantization, evaluating the impact of\ndifferent bit-widths (FP32, FP16, INT8, and INT4) on model performance.\nPreliminary results suggest that while FP16 quantization maintains high\naccuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive\nquantization (INT8 and INT4) leads to significant performance degradation.\nOverall, our study supports further exploration of quantization schemes and\nhardware optimizations to assess trade-offs between model size, inference time,\nand diagnostic accuracy in mobile healthcare applications.", "AI": {"tldr": "本研究探索使用深度学习模型（基于MobileNet）通过结膜苍白检测贫血，并在CP-AnemiC数据集上取得了高精度，同时评估了不同量化策略对模型性能的影响，以优化在边缘设备上的部署。", "motivation": "贫血在全球范围内普遍存在，尤其在低资源地区，儿童贫血问题突出。传统的贫血检测方法昂贵且需要专业知识，阻碍了早期和准确诊断。", "method": "研究采用MobileNet架构作为骨干网络，在包含710张儿童结膜图像的CP-AnemiC数据集上进行端到端微调，并结合数据增强和交叉验证策略。为优化模型在边缘设备上的部署，研究还进行了训练后量化，评估了FP32、FP16、INT8和INT4等不同位宽对模型性能的影响。", "result": "模型实现了0.9313的准确率、0.9374的精确率和0.9773的F1分数。初步结果表明，FP16量化能保持较高的性能（准确率0.9250，精确率0.9370，F1分数0.9377），而INT8和INT4等更激进的量化会导致显著的性能下降。", "conclusion": "本研究支持进一步探索量化方案和硬件优化，以评估模型大小、推理时间和诊断准确性在移动医疗应用中的权衡。深度学习结合FP16量化在移动贫血检测中显示出巨大潜力。"}}
{"id": "2507.14820", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14820", "abs": "https://arxiv.org/abs/2507.14820", "authors": ["Bingran Chen", "Baorun Li", "Jian Yang", "Yong Liu", "Guangyao Zhai"], "title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning", "comment": null, "summary": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.", "AI": {"tldr": "KGN-Pro是一种新型的机器人抓取网络，它结合了2D关键点估计和概率PnP层，实现了端到端的6自由度抓取估计，并通过3D优化提高了抓取性能。", "motivation": "现有的机器人抓取方法存在局限性：直接从点云生成抓取难以处理小物体和传感器噪声；从RGB图像推断3D信息需要昂贵的标注和离散化问题；基于2D关键点和PnP的方法则面临不可微分和仅依赖2D监督，未能充分利用丰富的3D信息的问题。", "method": "KGN-Pro网络接收RGB-D图像作为输入，生成关键点图（Keypoint Map）和一个2D置信度图，用于在重投影误差最小化过程中加权关键点的贡献。通过概率性地建模加权平方重投影误差之和，网络能够有效地将3D监督信息传递给其2D关键点预测，从而实现端到端学习，并通过概率PnP层集成直接的3D优化。", "result": "在模拟和真实世界平台上的实验表明，KGN-Pro在抓取覆盖率和成功率方面均优于现有方法。", "conclusion": "KGN-Pro通过引入概率PnP层实现直接3D优化，成功地将2D关键点估计的效率和精细抓取能力与3D监督相结合，为高水平机器人操作任务提供了更鲁棒和高效的6自由度抓取估计方法。"}}
{"id": "2507.14728", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14728", "abs": "https://arxiv.org/abs/2507.14728", "authors": ["Maryam Salamatmoghadasi", "Metin Ozturk", "Halim Yanikomeroglu"], "title": "Enhancing Sustainability in HAPS-Assisted 6G Networks: Load Estimation Aware Cell Switching", "comment": "6 pages, 5 figures, PIMRC", "summary": "This study introduces and addresses the critical challenge of traffic load\nestimation in cell switching within vertical heterogeneous networks. The\neffectiveness of cell switching is significantly limited by the lack of\naccurate traffic load data for small base stations (SBSs) in sleep mode, making\nmany load-dependent energy-saving approaches impractical, as they assume\nperfect knowledge of traffic loads, an assumption that is unrealistic when SBSs\nare inactive. In other words, when SBSs are in sleep mode, their traffic loads\ncannot be directly known and can only be estimated, inevitably with\ncorresponding errors. Rather than proposing a new switching algorithm, we focus\non eliminating this foundational barrier by exploring effective prediction\ntechniques. A novel vertical heterogeneous network model is considered,\nintegrating a high-altitude platform station (HAPS) as a super macro base\nstation (SMBS). We investigate both spatial and temporal load estimation\napproaches, including three spatial interpolation schemes, random neighboring\nselection, distance based selection, and multi level clustering (MLC),\nalongside a temporal deep learning method based on long short-term memory\n(LSTM) networks. Using a real world dataset for empirical validation, our\nresults show that both spatial and temporal methods significantly improve\nestimation accuracy, with the MLC and LSTM approaches demonstrating\nparticularly strong performance.", "AI": {"tldr": "本研究旨在解决垂直异构网络中休眠小基站（SBS）的流量负载估计问题，通过探索空间插值和时间深度学习（LSTM）方法，以提高小区切换的效率和节能策略的实用性。", "motivation": "现有小区切换中的节能方法依赖于精确的流量负载数据，但当小基站处于休眠模式时，其流量负载无法直接获取，导致许多依赖负载的节能方法无法实际应用，限制了小区切换的有效性。", "method": "本研究不提出新的切换算法，而是专注于消除基础障碍，探索有效的预测技术。它考虑了一个包含高空平台站（HAPS）作为超级宏基站（SMBS）的新型垂直异构网络模型。研究了空间（随机邻居选择、基于距离选择、多级聚类MLC）和时间（基于长短期记忆LSTM网络的深度学习）负载估计方法，并使用真实世界数据集进行验证。", "result": "研究结果表明，空间和时间方法都能显著提高估计精度，其中多级聚类（MLC）和LSTM方法表现尤为出色。", "conclusion": "有效的预测技术（特别是MLC和LSTM）可以克服休眠小基站流量负载未知这一根本障碍，从而使依赖负载的节能方法在垂直异构网络中变得更加实用和有效。"}}
{"id": "2507.14241", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14241", "abs": "https://arxiv.org/abs/2507.14241", "authors": ["Rithesh Murthy", "Ming Zhu", "Liangwei Yang", "Jielin Qiu", "Juntao Tan", "Shelby Heinecke", "Caiming Xiong", "Silvio Savarese", "Huan Wang"], "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.", "AI": {"tldr": "Promptomatix是一个自动提示优化框架，能将自然语言任务描述转化为高质量提示，无需人工干预或专业知识。", "motivation": "当前大型语言模型（LLMs）的提示工程依赖手动、缺乏一致性且非专家难以操作，限制了LLMs的效能和可及性。", "method": "Promptomatix包含轻量级元提示优化器和基于DSPy的编译器，设计模块化。它通过分析用户意图、生成合成训练数据、选择提示策略并使用成本感知目标来优化提示。", "result": "Promptomatix在5类任务中表现出与现有库相当或更优的性能，同时显著减少了提示长度和计算开销。", "conclusion": "Promptomatix使提示优化变得可扩展、高效，且对非专家用户更易用，提升了LLMs的应用潜力。"}}
{"id": "2507.14449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14449", "abs": "https://arxiv.org/abs/2507.14449", "authors": ["Zhe Cao", "Jin Zhang", "Ruiheng Zhang"], "title": "IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark", "comment": "11 pages, 7 figures. This paper is accepted by ICCV 2025", "summary": "Real-world infrared imagery presents unique challenges for vision-language\nmodels due to the scarcity of aligned text data and domain-specific\ncharacteristics. Although existing methods have advanced the field, their\nreliance on synthetic infrared images generated through style transfer from\nvisible images, which limits their ability to capture the unique\ncharacteristics of the infrared modality. To address this, we propose IRGPT,\nthe first multi-modal large language model for real-world infrared images,\nbuilt upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K\nauthentic image-text pairs. The proposed IR-TD dataset contains real infrared\nimages paired with meticulously handcrafted texts, where the initial drafts\noriginated from two complementary processes: (1) LLM-generated descriptions of\nvisible images, and (2) rule-based descriptions of annotations. Furthermore, we\nintroduce a bi-cross-modal curriculum transfer learning strategy that\nsystematically transfers knowledge from visible to infrared domains by\nconsidering the difficulty scores of both infrared-visible and infrared-text.\nEvaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT\nachieves state-of-the-art performance even compared with larger-scale models.", "AI": {"tldr": "IRGPT是首个针对真实红外图像的多模态大型语言模型，通过构建大规模红外文本数据集（IR-TD）和引入双跨模态课程迁移学习策略，在多项任务上超越现有模型，达到最先进水平。", "motivation": "现有红外图像视觉-语言模型面临两大挑战：一是缺乏对齐的文本数据；二是现有方法依赖可见光图像合成的红外图像，无法捕捉真实红外模态的独特特性。", "method": "本文提出IRGPT模型，并构建了包含超过26万对真实图像-文本对的大规模IR-TD数据集。IR-TD中的文本通过两种互补方式生成：(1) 大型语言模型生成的可见光图像描述；(2) 基于规则的标注描述。此外，本文引入了一种双跨模态课程迁移学习策略，通过考虑红外-可见光和红外-文本的难度分数，系统地将知识从可见光领域迁移到红外领域。", "result": "IRGPT在9项基准任务（如识别、接地）上进行了评估，即使与更大规模的模型相比，也取得了最先进的性能。", "conclusion": "IRGPT成功解决了真实世界红外图像视觉-语言模型的挑战，通过其创新的数据构建和学习策略，显著提升了该领域的性能，并为未来的研究奠定了基础。"}}
{"id": "2507.14417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14417", "abs": "https://arxiv.org/abs/2507.14417", "authors": ["Aryo Pradipta Gema", "Alexander Hägele", "Runjin Chen", "Andy Arditi", "Jacob Goldman-Wetzler", "Kit Fraser-Taliente", "Henry Sleight", "Linda Petrini", "Julian Michael", "Beatrice Alex", "Pasquale Minervini", "Yanda Chen", "Joe Benton", "Ethan Perez"], "title": "Inverse Scaling in Test-Time Compute", "comment": null, "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.", "AI": {"tldr": "大型推理模型（LRMs）在某些评估任务中，延长推理长度反而会导致性能下降，表现出测试时计算量与准确性之间的反向缩放关系。", "motivation": "研究旨在探究延长大型推理模型（LRMs）的推理长度对其性能的影响，并识别可能出现的失败模式。", "method": "研究构建了四类评估任务：带有干扰项的简单计数、带有虚假特征的回归、需要约束跟踪的演绎以及高级AI风险。通过在这些任务上延长模型推理长度来观察其行为。", "result": "研究发现延长推理长度会使模型性能恶化，并识别出五种主要失败模式：Claude模型易受无关信息干扰；OpenAI o-系列模型虽抗干扰但易过拟合；模型从合理先验转向虚假关联；所有模型在复杂演绎任务中难以保持专注；以及延长推理可能放大问题行为（如Claude Sonnet 4的自我保护倾向增强）。", "conclusion": "虽然测试时计算扩展有望提升模型能力，但它也可能无意中强化有问题的推理模式。因此，在不同推理长度下评估模型对于识别和解决大型推理模型中的这些失败模式至关重要。"}}
{"id": "2507.15193", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15193", "abs": "https://arxiv.org/abs/2507.15193", "authors": ["Tanjin Taher Toma", "Tejas Sudharshan Mathai", "Bikash Santra", "Pritam Mukherjee", "Jianfei Liu", "Wesley Jong", "Darwish Alabyad", "Vivek Batheja", "Abhishek Jha", "Mayank Patel", "Darko Pucar", "Jayadira del Rivero", "Karel Pacak", "Ronald M. Summers"], "title": "A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT", "comment": null, "summary": "Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is\nessential for tumor burden estimation, prognosis, and treatment planning. It\nmay also help infer genetic clusters, reducing reliance on expensive testing.\nThis study systematically evaluates anatomical priors to identify\nconfigurations that improve deep learning-based PCC segmentation. We employed\nthe nnU-Net framework to evaluate eleven annotation strategies for accurate 3D\nsegmentation of pheochromocytoma, introducing a set of novel multi-class\nschemes based on organ-specific anatomical priors. These priors were derived\nfrom adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,\nkidney, aorta, adrenal gland, and pancreas), and were compared against a broad\nbody-region prior used in previous work. The framework was trained and tested\non 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.\nPerformance was measured using Dice Similarity Coefficient (DSC), Normalized\nSurface Distance (NSD), and instance-wise F1 score. Among all strategies, the\nTumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation\naccuracy, significantly outperforming the previously used Tumor + Body (TB)\nannotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%\nimprovement at an IoU threshold of 0.5), measured on a 70-30 train-test split.\nThe TKA model also showed superior tumor burden quantification (R^2 = 0.968)\nand strong segmentation across all genetic subtypes. In five-fold\ncross-validation, TKA consistently outperformed TB across IoU thresholds (0.1\nto 0.5), reinforcing its robustness and generalizability. These findings\nhighlight the value of incorporating relevant anatomical context in deep\nlearning models to achieve precise PCC segmentation, supporting clinical\nassessment and longitudinal monitoring.", "AI": {"tldr": "本研究评估了在深度学习中利用解剖学先验知识改进嗜铬细胞瘤（PCC）CT图像分割的方法，发现结合肿瘤、肾脏和主动脉的先验信息能显著提高分割精度。", "motivation": "准确分割腹部CT扫描中的嗜铬细胞瘤（PCC）对于评估肿瘤负荷、预后和治疗计划至关重要。此外，它还有助于推断遗传亚型，从而减少对昂贵基因检测的依赖。", "method": "研究采用nnU-Net框架，评估了11种注释策略，并引入了一系列基于邻近器官（如肝脏、脾脏、肾脏、主动脉、肾上腺和胰腺）的新型多类别解剖学先验方案。这些方案与之前工作中使用的广泛身体区域先验进行了比较。模型在NIH临床中心的105例增强CT扫描数据上进行训练和测试。性能通过Dice相似系数（DSC）、归一化表面距离（NSD）和实例级F1分数进行衡量。", "result": "在所有策略中，肿瘤+肾脏+主动脉（TKA）注释方案实现了最高的分割精度，在DSC（p = 0.0097）、NSD（p = 0.0110）和F1分数（IoU阈值为0.5时提高25.84%）方面显著优于先前使用的肿瘤+身体（TB）注释方案。TKA模型还显示出卓越的肿瘤负荷量化能力（R^2 = 0.968），并在所有遗传亚型中均表现出强大的分割能力。在五折交叉验证中，TKA在不同IoU阈值（0.1至0.5）下始终优于TB，证明了其鲁棒性和泛化性。", "conclusion": "研究结果强调了在深度学习模型中融入相关解剖学上下文（如TKA方案）对于实现精确PCC分割的价值，这有助于临床评估和长期监测。"}}
{"id": "2507.14903", "categories": ["cs.RO", "I.2.9; I.2.10; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.14903", "abs": "https://arxiv.org/abs/2507.14903", "authors": ["Pan Hu"], "title": "CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning", "comment": "8 pages, 5 figures", "summary": "Autonomous driving demands reliable and efficient solutions to closely\nrelated problems such as decision-making and motion planning. In this work,\ndecision-making refers specifically to highway lane selection, while motion\nplanning involves generating control commands (such as speed and steering) to\nreach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),\nachieving both flexible and safe lane selection alongside precise trajectory\nexecution remains a significant challenge. This paper proposes a framework\ncalled Cohesive Decision-Guided Motion Planning (CDGMP), which tightly\nintegrates decision-making and motion planning using a Mixture of Experts (MoE)\ninspired architecture combined with multi-policy reinforcement learning. By\ncoordinating multiple specialized sub-networks through a gating mechanism, the\nmethod decomposes the complex driving task into modular components. Each\nsub-network focuses on a specific aspect of driving, improving efficiency by\nactivating only the most relevant modules during inference. This design also\nenhances safety through modular specialization. CDGMP improves the adaptability\nand robustness of CAVs across diverse traffic scenarios, offering a scalable\nsolution to real-world autonomy challenges. The architectural principles behind\nCDGMP, especially the use of MoE, also provide a strong foundation for other\nhigh-dimensional decision and control tasks. Simulation results (available at\nhttps://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane\nselection and motion planning.", "AI": {"tldr": "本文提出Cohesive Decision-Guided Motion Planning (CDGMP) 框架，通过结合专家混合（MoE）架构和多策略强化学习，紧密整合自动驾驶中的决策（车道选择）和运动规划，以提高联网自动驾驶车辆（CAVs）在复杂交通场景下的适应性和鲁棒性。", "motivation": "自动驾驶需要可靠高效的决策制定和运动规划解决方案。在联网自动驾驶车辆（CAVs）背景下，实现灵活安全的车道选择和精确的轨迹执行仍然是一个重大挑战。", "method": "本文提出了Cohesive Decision-Guided Motion Planning (CDGMP) 框架，该框架采用受专家混合（MoE）启发的架构，并结合多策略强化学习，紧密整合了决策制定（高速公路车道选择）和运动规划（生成控制指令）。通过门控机制协调多个专业子网络，将复杂的驾驶任务分解为模块化组件，每个子网络专注于特定驾驶方面，从而提高推理效率并通过模块化专业化增强安全性。", "result": "CDGMP提高了联网自动驾驶车辆（CAVs）在不同交通场景下的适应性和鲁棒性，为现实世界的自动驾驶挑战提供了一个可扩展的解决方案。仿真结果表明其在车道选择和运动规划方面均表现出可靠的性能。", "conclusion": "CDGMP框架通过其MoE启发式架构和多策略强化学习方法，成功地紧密整合了自动驾驶中的决策和运动规划，为联网自动驾驶车辆提供了灵活、安全且高效的解决方案。其架构原则也为其他高维决策和控制任务奠定了坚实基础。"}}
{"id": "2507.14765", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14765", "abs": "https://arxiv.org/abs/2507.14765", "authors": ["Debadrita Banerjee", "Debjani Mitra", "Rajesh Dey", "Mudassir Khan", "Lalan Kumar"], "title": "Multi Target Observability", "comment": null, "summary": "In this paper, we mainly focus on the problem of multi-target observability,\nfocusing on the unique state estimation criteria for multiple targets. We\nderive the condition which is necessary as well as sufficient for observability\nusing bearing angles with multiple higher-order dynamics observed by a single\nobserver. We then establish an alternative notion of observability by analyzing\nambiguous target trajectories and deriving the condition which is NECNDSUF\n(Nec. and Suff.) for multi-target observability, considering three types of\nmeasurements: Doppler-only, bearing-only, and combined Doppler and bearing\nmeasurements, which offers insights that can improve target distinguishability,\ntrajectory reconstruction, and overall tracking accuracy.", "AI": {"tldr": "本文主要研究单观测器对多目标进行观测时，利用方位角、多普勒或组合测量推导多目标可观测性的充分必要条件。", "motivation": "解决多目标可观测性问题，特别是多目标唯一状态估计的判据。", "method": "1. 针对单观测器观测多高阶动力学目标，推导了使用方位角进行可观测性的充分必要条件。2. 通过分析模糊目标轨迹，建立了另一种可观测性概念。3. 针对纯多普勒、纯方位角以及多普勒和方位角组合三种测量类型，推导了多目标可观测性的充分必要条件。", "result": "1. 推导了使用方位角对多高阶动力学目标进行可观测性的充分必要条件。2. 推导了针对纯多普勒、纯方位角及组合测量下多目标可观测性的充分必要条件。", "conclusion": "所推导的条件为提高目标可区分性、轨迹重建和整体跟踪精度提供了见解。"}}
{"id": "2507.14298", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14298", "abs": "https://arxiv.org/abs/2507.14298", "authors": ["Wan-Cyuan Fan", "Yen-Chun Chen", "Mengchen Liu", "Alexander Jacobson", "Lu Yuan", "Leonid Sigal"], "title": "In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding", "comment": "arXiv admin note: substantial text overlap with arXiv:2407.14506", "summary": "Recent methods for customizing Large Vision Language Models (LVLMs) for\ndomain-specific tasks have shown promising results in scientific chart\ncomprehension. However, existing approaches face two major limitations: First,\nthey rely on paired data from only a few chart types, limiting generalization\nto wide range of chart types. Secondly, they lack targeted pre-training for\nchart-data alignment, which hampers the model's understanding of underlying\ndata. In this paper, we introduce ChartScope, an LVLM optimized for in-depth\nchart comprehension across diverse chart types. We propose an efficient data\ngeneration pipeline that synthesizes paired data for a wide range of chart\ntypes, along with a novel Dual-Path training strategy that enabling the model\nto succinctly capture essential data details while preserving robust reasoning\ncapabilities by incorporating reasoning over the underlying data. Lastly, we\nestablish ChartDQA, a new benchmark for evaluating not only question-answering\nat different levels but also underlying data understanding. Experimental\nresults demonstrate that ChartScope significantly enhances comprehension on a\nwide range of chart types. The code and data are available at\nhttps://davidhalladay.github.io/chartscope_demo.", "AI": {"tldr": "ChartScope是一个优化的LVLM，通过高效数据生成和双路径训练策略，显著提升了对多样图表类型的深度理解能力，并引入了新的评估基准ChartDQA。", "motivation": "现有方法在科学图表理解方面存在两大局限：1) 依赖少数图表类型的配对数据，泛化能力受限；2) 缺乏针对图表-数据对齐的预训练，影响模型对底层数据的理解。", "method": "本文提出了ChartScope，包括：1) 一个高效的数据生成管道，用于合成大量多样图表类型的配对数据；2) 一种新颖的双路径训练策略，使模型能够捕获关键数据细节并保持鲁棒推理能力；3) 建立了ChartDQA新基准，用于评估问答和底层数据理解。", "result": "实验结果表明，ChartScope显著提升了对各种图表类型的理解能力。", "conclusion": "ChartScope通过解决数据多样性和数据对齐问题，有效地提高了LVLM在深度图表理解任务上的性能。"}}
{"id": "2507.14452", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14452", "abs": "https://arxiv.org/abs/2507.14452", "authors": ["Weikang Gu", "Mingyue Han", "Li Xue", "Heng Dong", "Changcai Yang", "Riqing Chen", "Lifang Wei"], "title": "GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration", "comment": "9 pages, 4 figures. Accepted to IJCAI 2025", "summary": "The accurate identification of high-quality correspondences is a prerequisite\ntask in feature-based point cloud registration. However, it is extremely\nchallenging to handle the fusion of local and global features due to feature\nredundancy and complex spatial relationships. Given that Gestalt principles\nprovide key advantages in analyzing local and global relationships, we propose\na novel Gestalt-guided Parallel Interaction Network via orthogonal geometric\nconsistency (GPI-Net) in this paper. It utilizes Gestalt principles to\nfacilitate complementary communication between local and global information.\nSpecifically, we introduce an orthogonal integration strategy to optimally\nreduce redundant information and generate a more compact global structure for\nhigh-quality correspondences. To capture geometric features in correspondences,\nwe leverage a Gestalt Feature Attention (GFA) block through a hybrid\nutilization of self-attention and cross-attention mechanisms. Furthermore, to\nfacilitate the integration of local detail information into the global\nstructure, we design an innovative Dual-path Multi-Granularity parallel\ninteraction aggregation (DMG) block to promote information exchange across\ndifferent granularities. Extensive experiments on various challenging tasks\ndemonstrate the superior performance of our proposed GPI-Net in comparison to\nexisting methods. The code will be released at https://github.com/gwk/GPI-Net.", "AI": {"tldr": "本文提出GPI-Net，一个基于格式塔原理的并行交互网络，用于在点云配准中准确识别高质量对应点，有效融合局部和全局特征。", "motivation": "在特征点云配准中，准确识别高质量对应点是关键，但由于特征冗余和复杂的空间关系，融合局部和全局特征极具挑战性。", "method": "本文提出了GPI-Net（Gestalt-guided Parallel Interaction Network），利用格式塔原理促进局部和全局信息的互补通信。具体方法包括：引入正交集成策略以减少冗余信息并生成更紧凑的全局结构；通过结合自注意力与交叉注意力的格式塔特征注意力（GFA）模块捕获几何特征；设计双路径多粒度并行交互聚合（DMG）模块，促进不同粒度间的信息交换，将局部细节信息整合到全局结构中。", "result": "在各种具有挑战性的任务中，所提出的GPI-Net与现有方法相比，表现出卓越的性能。", "conclusion": "GPI-Net通过结合格式塔原理和创新的网络结构，有效解决了点云配准中局部与全局特征融合的挑战，实现了高质量对应点的准确识别。"}}
{"id": "2507.14447", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14447", "abs": "https://arxiv.org/abs/2507.14447", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "comment": "26 pages, 8 figures, 5 tables", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "AI": {"tldr": "Routine是一个多步骤智能体规划框架，通过明确结构和指令，显著提高了企业环境中多步骤工具调用任务的执行精度和稳定性，并能有效蒸馏领域知识以提升模型适应性。", "motivation": "企业环境中智能体系统部署面临挑战：现有模型缺乏领域特定过程知识，导致规划混乱、工具缺失和执行稳定性差。", "method": "本文提出Routine框架，它是一个具有清晰结构、明确指令和无缝参数传递的多步骤智能体规划框架，旨在指导智能体执行模块进行多步骤工具调用任务。通过构建Routine遵循训练数据集和基于Routine的蒸馏数据集来微调模型。", "result": "在真实企业场景评估中，Routine显著提高了模型工具调用的执行精度：GPT-4o从41.1%提升至96.3%，Qwen3-14B从32.6%提升至83.3%。通过Routine遵循数据集微调Qwen3-14B，精度提升至88.2%。通过Routine蒸馏数据集微调后，模型精度提升至95.5%，接近GPT-4o的性能。", "conclusion": "Routine提供了一种实用且易于实现的方法来构建稳定的智能体工作流，加速了智能体系统在企业环境中的部署和应用。它有效蒸馏了领域特定的工具使用模式，增强了模型对新场景的适应性，推动了“AI for Process”的技术愿景。"}}
{"id": "2507.15194", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15194", "abs": "https://arxiv.org/abs/2507.15194", "authors": ["Yilin Lyu", "Fan Yang", "Xiaoyue Liu", "Zichen Jiang", "Joshua Dillon", "Debbie Zhao", "Martyn Nash", "Charlene Mauger", "Alistair Young", "Ching-Hui Sia", "Mark YY Chan", "Lei Li"], "title": "Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling", "comment": "11 pages", "summary": "Accurate representation of myocardial infarct geometry is crucial for\npatient-specific cardiac modeling in MI patients. While Late gadolinium\nenhancement (LGE) MRI is the clinical gold standard for infarct detection, it\nrequires contrast agents, introducing side effects and patient discomfort.\nMoreover, infarct reconstruction from LGE often relies on sparsely sampled 2D\nslices, limiting spatial resolution and accuracy. In this work, we propose a\nnovel framework for automatically reconstructing high-fidelity 3D myocardial\ninfarct geometry from 2D clinically standard cine MRI, eliminating the need for\ncontrast agents. Specifically, we first reconstruct the 4D biventricular mesh\nfrom multi-view cine MRIs via an automatic deep shape fitting model, biv-me.\nThen, we design a infarction reconstruction model, CMotion2Infarct-Net, to\nexplicitly utilize the motion patterns within this dynamic geometry to localize\ninfarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our\nmethod shows reasonable agreement with manual delineation. This study\ndemonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct\nreconstruction, paving the way for efficient digital twin of MI.", "AI": {"tldr": "该研究提出了一种新颖的框架，利用临床标准的2D电影MRI自动重建高保真3D心肌梗死几何形状，无需造影剂。", "motivation": "传统的LGE MRI是心梗检测的金标准，但需要造影剂（有副作用和不适），且依赖稀疏采样的2D切片，限制了空间分辨率和准确性。研究旨在解决这些局限性，提供一种无造影剂、高精度的3D心梗重建方法。", "method": "1. 首先，通过一个名为“biv-me”的自动深度形状拟合模型，从多视图电影MRI中重建4D双心室网格。2. 其次，设计了一个名为“CMotion2Infarct-Net”的心梗重建模型，明确利用动态几何中的运动模式来定位心梗区域。", "result": "该方法在126名心梗患者的205次电影MRI扫描上进行了评估，结果显示与手动描绘具有合理的一致性。", "conclusion": "本研究证明了无造影剂、由心脏运动驱动的3D心肌梗死重建的可行性，为心梗患者的数字孪生模型铺平了道路。"}}
{"id": "2507.14914", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14914", "abs": "https://arxiv.org/abs/2507.14914", "authors": ["Zhexuan Xu", "Jie Wang", "Siyuan Xu", "Zijie Geng", "Mingxuan Yuan", "Feng Wu"], "title": "One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner", "comment": null, "summary": "Floorplanning determines the shapes and locations of modules on a chip canvas\nand plays a critical role in optimizing the chip's Power, Performance, and Area\n(PPA) metrics. However, existing floorplanning approaches often fail to\nintegrate with subsequent physical design stages, leading to suboptimal\nin-module component placement and excessive inter-module feedthrough. To tackle\nthis challenge, we propose Flora, a three-stage feedthrough and placement aware\nrectilinear floorplanner. In the first stage, Flora employs wiremask and\nposition mask techniques to achieve coarse-grained optimization of HPWL and\nfeedthrough. In the second stage, under the constraint of a fixed outline,\nFlora achieves a zero-whitespace layout by locally resizing module shapes,\nthereby performing fine-grained optimization of feedthrough and improving\ncomponent placement. In the third stage, Flora utilizes a fast tree\nsearch-based method to efficiently place components-including macros and\nstandard cells-within each module, subsequently adjusting module boundaries\nbased on the placement results to enable cross-stage optimization. Experimental\nresults show that Flora outperforms recent state-of-the-art floorplanning\napproaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,\n29.15% in FTmod, and a 14% improvement in component placement performance.", "AI": {"tldr": "Flora是一种三阶段的馈通和布局感知整流器平面规划器，通过跨阶段优化显著改善了芯片的PPA指标，解决了现有方法与后续物理设计阶段集成不足的问题。", "motivation": "现有平面规划方法未能与后续物理设计阶段有效集成，导致模块内组件布局次优和模块间馈通（feedthrough）过多，从而影响芯片的功耗、性能和面积（PPA）指标。", "method": "Flora是一个三阶段的馈通和布局感知整流器平面规划器：1. 第一阶段：利用线掩模（wiremask）和位置掩模（position mask）技术实现HPWL和馈通的粗粒度优化。2. 第二阶段：在固定轮廓约束下，通过局部调整模块形状实现零空白布局，进行馈通的细粒度优化并改善组件布局。3. 第三阶段：采用快速树搜索方法高效地在每个模块内部放置组件（包括宏单元和标准单元），并根据布局结果调整模块边界，实现跨阶段优化。", "result": "实验结果表明，Flora优于现有最先进的平面规划方法：HPWL平均减少6%，FTpin平均减少5.16%，FTmod平均减少29.15%，组件布局性能提高14%。", "conclusion": "Flora通过在平面规划中集成馈通和布局感知，并实现跨阶段优化，有效解决了现有方法与后续物理设计阶段集成不足的问题，从而显著提升了芯片的PPA指标。"}}
{"id": "2507.14800", "categories": ["eess.SY", "cs.AI", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14800", "abs": "https://arxiv.org/abs/2507.14800", "authors": ["Xu Yang", "Chenhui Lin", "Haotian Liu", "Qi Wang", "Wenchuan Wu"], "title": "Large Language Model as An Operator: An Experience-Driven Solution for Distribution Network Voltage Control", "comment": null, "summary": "With the advanced reasoning and information analysis capabilities, large\nlanguage models (LLMs) can offer a novel approach for the autonomous generation\nof dispatch strategies in power systems. This letter proposes an LLM-based\nexperience-driven voltage control solution for distribution networks, which\nenables the self-evolution of LLM-based voltage control strategies through the\ncollaboration and interaction of multiple modules-specifically, experience\nstorage, experience retrieval, experience generation, and experience\nmodification. Comprehensive experimental results validate the effectiveness of\nthe proposed method and highlight the applicability of LLM in addressing power\nsystem dispatch challenges.", "AI": {"tldr": "本文提出了一种基于LLM的经验驱动型配电网电压控制解决方案，通过多模块协作实现策略的自演化。", "motivation": "大型语言模型（LLMs）先进的推理和信息分析能力为电力系统调度策略的自主生成提供了新颖途径。", "method": "提出了一种基于LLM的经验驱动型配电网电压控制方案，通过经验存储、经验检索、经验生成和经验修改等多个模块的协作与交互，实现LLM电压控制策略的自演化。", "result": "全面的实验结果验证了所提方法的有效性，并突出了LLM在解决电力系统调度挑战中的适用性。", "conclusion": "所提出的基于LLM的经验驱动型电压控制方案在配电网中是有效的，并展示了LLM在应对电力系统调度挑战方面的潜力。"}}
{"id": "2507.14304", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14304", "abs": "https://arxiv.org/abs/2507.14304", "authors": ["Rakesh Paul", "Anusha Kamath", "Kanishk Singla", "Raviraj Joshi", "Utkarsh Vaidya", "Sanjay Singh Chauhan", "Niranjan Wartikar"], "title": "Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study", "comment": null, "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.", "AI": {"tldr": "本文提出了一种基于LLM的选择性翻译技术，用于为低资源语言生成对齐数据，以弥补多语言大型语言模型在非英语语言上的性能差距。", "motivation": "多语言LLM在英语和非英语语言之间存在性能差距，尤其是在低资源语境下。为低资源语言对齐模型至关重要但面临高质量数据有限的挑战。标准翻译方法无法有效保留代码、数学表达式和JSON等结构化格式中的关键元素。", "method": "研究者提出并研究了基于LLM的选择性翻译技术，该技术仅翻译文本中可翻译的部分，同时保留不可翻译的内容和句子结构。他们系统地研究了该方法与普通翻译的有效性、过滤噪声输出的重要性以及在对齐过程中混合翻译样本与原始英语数据的好处。实验以低资源印度语言印地语为例，比较了Google Cloud Translation (GCP) 和 Llama-3.1-405B 生成的翻译。", "result": "实验结果表明，选择性翻译是一种有前景且有效的方法，可以改善LLM的多语言对齐。", "conclusion": "选择性翻译是一种实用且有效的方法，能够显著改善多语言大型语言模型的对齐效果，尤其适用于低资源语言环境。"}}
{"id": "2507.14454", "categories": ["cs.CV", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14454", "abs": "https://arxiv.org/abs/2507.14454", "authors": ["Han Gong", "Qiyue Li", "Jie Li", "Zhi Liu"], "title": "Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation", "comment": null, "summary": "3D Gaussian splatting video (3DGS) streaming has recently emerged as a\nresearch hotspot in both academia and industry, owing to its impressive ability\nto deliver immersive 3D video experiences. However, research in this area is\nstill in its early stages, and several fundamental challenges, such as tiling,\nquality assessment, and bitrate adaptation, require further investigation. In\nthis paper, we tackle these challenges by proposing a comprehensive set of\nsolutions. Specifically, we propose an adaptive 3DGS tiling technique guided by\nsaliency analysis, which integrates both spatial and temporal features. Each\ntile is encoded into versions possessing dedicated deformation fields and\nmultiple quality levels for adaptive selection. We also introduce a novel\nquality assessment framework for 3DGS video that jointly evaluates\nspatial-domain degradation in 3DGS representations during streaming and the\nquality of the resulting 2D rendered images. Additionally, we develop a\nmeta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS\nvideo streaming, achieving optimal performance across varying network\nconditions. Extensive experiments demonstrate that our proposed approaches\nsignificantly outperform state-of-the-art methods.", "AI": {"tldr": "本文提出了一套全面的解决方案，以应对3D高斯泼溅(3DGS)视频流传输中的分块、质量评估和比特率自适应等核心挑战，显著提升了流传输性能。", "motivation": "3DGS视频流传输因其沉浸式体验而成为研究热点，但仍处于早期阶段，面临分块、质量评估和比特率自适应等基本挑战，亟需深入研究。", "method": "本文提出了一系列方法：1) 基于显著性分析（结合时空特征）的自适应3DGS分块技术，每个分块编码为包含专用形变场和多质量级别的版本；2) 一种新颖的3DGS视频质量评估框架，同时评估流传输过程中3DGS表示的空间域退化和2D渲染图像的质量；3) 一种基于元学习的自适应比特率算法，专为3DGS视频流传输设计。", "result": "广泛的实验证明，本文提出的方法显著优于现有最先进的方法。", "conclusion": "本文提出的综合解决方案有效解决了3DGS视频流传输中的关键挑战，实现了在不同网络条件下的最佳性能。"}}
{"id": "2507.14468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14468", "abs": "https://arxiv.org/abs/2507.14468", "authors": ["Yitong Lin", "Jiaying He", "Jiahe Chen", "Xinnan Zhu", "Jianwei Zheng", "Tao Bo"], "title": "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning", "comment": "Accepted by Bioinformatics on July 11th", "summary": "Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery\nand disease understanding, yet their completion and reasoning are challenging.\nKnowledge Embedding (KE) methods capture global semantics but struggle with\ndynamic structural integration, while Graph Neural Networks (GNNs) excel\nlocally but often lack semantic understanding. Even ensemble approaches,\nincluding those leveraging language models, often fail to achieve a deep,\nadaptive, and synergistic co-evolution between semantic comprehension and\nstructural learning. Addressing this critical gap in fostering continuous,\nreciprocal refinement between these two aspects in complex biomedical KGs is\nparamount.\n  Results: We introduce BioGraphFusion, a novel framework for deeply\nsynergistic semantic and structural learning. BioGraphFusion establishes a\nglobal semantic foundation via tensor decomposition, guiding an LSTM-driven\nmechanism to dynamically refine relation embeddings during graph propagation.\nThis fosters adaptive interplay between semantic understanding and structural\nlearning, further enhanced by query-guided subgraph construction and a hybrid\nscoring mechanism. Experiments across three key biomedical tasks demonstrate\nBioGraphFusion's superior performance over state-of-the-art KE, GNN, and\nensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)\nhighlights its ability to unveil biologically meaningful pathways.\n  Availability and Implementation: Source code and all training data are freely\navailable for download at https://github.com/Y-TARL/BioGraphFusion.\n  Supplementary information: Supplementary data are available at Bioinformatics\nonline.", "AI": {"tldr": "BioGraphFusion是一个新颖的框架，通过深度协同的语义和结构学习，解决了生物医学知识图谱补全和推理中现有知识嵌入和图神经网络方法的局限性，并在多项生物医学任务中表现出卓越性能。", "motivation": "生物医学知识图谱对药物发现和疾病理解至关重要，但其补全和推理面临挑战。现有知识嵌入（KE）方法难以处理动态结构集成，图神经网络（GNNs）缺乏语义理解。即使是结合语言模型的集成方法，也未能实现语义理解与结构学习之间的深度、自适应和协同共进化。本研究旨在弥补在复杂生物医学知识图谱中促进这两方面持续、相互细化之间的关键空白。", "method": "本文提出了BioGraphFusion框架。它通过张量分解建立全局语义基础，并利用一个由LSTM驱动的机制在图传播过程中动态细化关系嵌入。此外，该框架还通过查询引导的子图构建和混合评分机制，进一步增强了语义理解与结构学习之间的自适应交互。", "result": "BioGraphFusion在三个关键生物医学任务中，表现优于现有最先进的知识嵌入、图神经网络和集成模型。在皮肤恶性黑色素瘤1（CMM1）的案例研究中，该框架成功揭示了具有生物学意义的通路。", "conclusion": "BioGraphFusion成功实现了语义和结构学习的深度协同，有效解决了生物医学知识图谱补全和推理中的核心挑战，并能揭示有意义的生物学通路，为药物发现和疾病理解提供了强大的新工具。"}}
{"id": "2507.15203", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15203", "abs": "https://arxiv.org/abs/2507.15203", "authors": ["Xiaoyue Liu", "Xicheng Sheng", "Xiahai Zhuang", "Vicente Grau", "Mark YY Chan", "Ching-Hui Sia", "Lei Li"], "title": "Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins", "comment": null, "summary": "Cardiac digital twins (CDTs) provide personalized in-silico cardiac\nrepresentations and hold great potential for precision medicine in cardiology.\nHowever, whole-heart CDT models that simulate the full organ-scale\nelectromechanics of all four heart chambers remain limited. In this work, we\npropose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh\ndirectly from multi-view 2D cardiac cine MRIs. This is achieved by learning a\nself-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the\ngeneration of personalized heart models that closely correspond to input cine\nMRIs. The resulting 4D heart meshes can facilitate the automatic extraction of\nkey cardiac variables, including ejection fraction and dynamic chamber volume\nchanges with high temporal resolution. It demonstrates the feasibility of\ninferring personalized 4D heart models from cardiac MRIs, paving the way for an\nefficient CDT platform for precision medicine. The code will be publicly\nreleased once the manuscript is accepted.", "AI": {"tldr": "提出一种弱监督学习模型，从多视图2D心脏电影MRI重建4D心脏网格，以支持个性化心脏数字孪生。", "motivation": "现有的模拟全器官尺度电机械的完整心脏数字孪生模型（包含所有四个心腔）仍存在局限性。", "method": "提出一个弱监督学习模型，直接从多视图2D心脏电影MRI重建4D（3D+t）心脏网格。该方法通过学习电影MRI和4D心脏网格之间的自监督映射来实现。", "result": "生成的4D心脏网格能够自动提取关键心脏变量（如射血分数和动态心腔容积变化），并具有高时间分辨率。", "conclusion": "该研究证明了从心脏MRI推断个性化4D心脏模型的可行性，为高效的心脏数字孪生平台用于精准医疗铺平了道路。"}}
{"id": "2507.14929", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14929", "abs": "https://arxiv.org/abs/2507.14929", "authors": ["Tero Kaarlela", "Sami Salo", "Jose Outeiro"], "title": "Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly", "comment": null, "summary": "Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a\nsustainable transition to electric vehicles by enabling a closed-loop supply\nchain. Currently, the manual disassembly process exposes workers to hazards,\nincluding electrocution and toxic chemicals. We propose a teleoperated system\nfor the safe disassembly and sorting of EVBs. A human-in-the-loop can create\nand save disassembly sequences for unknown EVB types, enabling future\nautomation. An RGB camera aligns the physical and digital twins of the EVB, and\nthe digital twin of the robot is based on the Robot Operating System (ROS)\nmiddleware. This hybrid approach combines teleoperation and automation to\nimprove safety, adaptability, and efficiency in EVB disassembly and sorting.\nThe economic contribution is realized by reducing labor dependency and\nincreasing throughput in battery recycling. An online pilot study was set up to\nevaluate the usability of the presented approach, and the results demonstrate\nthe potential as a user-friendly solution.", "AI": {"tldr": "本文提出一种人机协作的远程操作系统，用于安全地拆卸和分类电动汽车电池（EVB），以提高安全性、适应性和效率，并支持可持续的闭环供应链。", "motivation": "当前EVB的人工拆卸过程存在触电和有毒化学品暴露等安全隐患，且不利于电动汽车的闭环供应链和可持续发展。", "method": "开发了一个远程操作系统，结合了远程操作和自动化。系统利用RGB相机对EVB的物理和数字孪生进行对齐，机器人数字孪生基于ROS中间件。通过人机协作，操作员可以创建并保存未知EVB类型的拆卸序列，为未来的自动化奠定基础。", "result": "该混合方法提高了EVB拆卸和分类的安全性、适应性和效率。经济效益体现在降低了劳动力依赖并增加了电池回收的吞吐量。在线试点研究表明，该方法具有用户友好的潜力。", "conclusion": "所提出的远程操作系统是EVB安全拆卸和分类的有效且用户友好的解决方案，能够促进电动汽车产业的可持续发展和电池回收效率的提升。"}}
{"id": "2507.14857", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14857", "abs": "https://arxiv.org/abs/2507.14857", "authors": ["Hassan Osseily"], "title": "Grid Stability and Power Factor Dynamics in Solar Farms Integration", "comment": "6 pages, 16 figures, African Scientific", "summary": "This paper examines the impact of solar farm fluctuations on grid stability,\nfocusing on maintaining an optimal power factor. ETAP-based simulations and\ncase studies are used to analyze real-time grid performance under solar\nvariability. Reactive power control strategies and advanced inverter functions\nare proposed for stabilization. Theoretical analysis and simulation results\nhighlight effective integration techniques. Artificial intelligence is trailed\nfor controlling the SVC in adaptive reactive power compensation. The study\nprovides practical solutions for improving reliability in renewable-integrated\npower systems.", "AI": {"tldr": "本文研究了太阳能电场波动对电网稳定性的影响，重点是维持最佳功率因数，并通过ETAP仿真、无功功率控制策略和AI辅助SVC控制，提出了提高可再生能源集成电力系统可靠性的实用解决方案。", "motivation": "太阳能电场波动对电网稳定性构成挑战，尤其是在维持最佳功率因数方面，因此需要开发策略以提高可再生能源集成电力系统的可靠性。", "method": "采用基于ETAP的仿真和案例研究来分析太阳能波动下的实时电网性能；提出无功功率控制策略和先进逆变器功能；进行理论分析；尝试使用人工智能控制SVC进行自适应无功功率补偿。", "result": "仿真结果突出了有效的集成技术；研究提供了提高可再生能源集成电力系统可靠性的实用解决方案。", "conclusion": "该研究通过解决太阳能电场波动和维持最佳功率因数，为提高可再生能源集成电力系统的可靠性提供了实用解决方案。"}}
{"id": "2507.14307", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14307", "abs": "https://arxiv.org/abs/2507.14307", "authors": ["Karin de Langis", "Jong Inn Park", "Andreas Schramm", "Bin Hu", "Khanh Chi Le", "Michael Mensink", "Ahn Thu Tong", "Dongyeop Kang"], "title": "How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs", "comment": null, "summary": "Large language models (LLMs) exhibit increasingly sophisticated linguistic\ncapabilities, yet the extent to which these behaviors reflect human-like\ncognition versus advanced pattern recognition remains an open question. In this\nstudy, we investigate how LLMs process the temporal meaning of linguistic\naspect in narratives that were previously used in human studies. Using an\nExpert-in-the-Loop probing pipeline, we conduct a series of targeted\nexperiments to assess whether LLMs construct semantic representations and\npragmatic inferences in a human-like manner. Our findings show that LLMs\nover-rely on prototypicality, produce inconsistent aspectual judgments, and\nstruggle with causal reasoning derived from aspect, raising concerns about\ntheir ability to fully comprehend narratives. These results suggest that LLMs\nprocess aspect fundamentally differently from humans and lack robust narrative\nunderstanding. Beyond these empirical findings, we develop a standardized\nexperimental framework for the reliable assessment of LLMs' cognitive and\nlinguistic capabilities.", "AI": {"tldr": "研究发现大型语言模型（LLMs）在处理叙事中的时间意义（语言体）时，与人类的处理方式存在根本性差异，缺乏鲁棒的叙事理解能力。", "motivation": "探讨LLMs日益复杂的语言能力是源于类人认知还是高级模式识别，特别是它们如何处理叙事中的时间意义。", "method": "采用“专家在环（Expert-in-the-Loop）”探测流程，进行了一系列有针对性的实验，评估LLMs是否以类人的方式构建语义表征和语用推断，使用了先前用于人类研究的叙事材料。", "result": "LLMs过度依赖原型性，产生不一致的体判断，并且难以进行源自体的因果推理，这表明它们对叙事的完全理解存在问题。研究还开发了一个用于可靠评估LLMs认知和语言能力的标准化实验框架。", "conclusion": "LLMs处理语言体的方式与人类根本不同，缺乏鲁棒的叙事理解能力，对其是否具备类人认知提出了质疑。"}}
{"id": "2507.14456", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14456", "abs": "https://arxiv.org/abs/2507.14456", "authors": ["Chi Wan", "Yixin Cui", "Jiatong Du", "Shuo Yang", "Yulong Bai", "Yanjun Huang"], "title": "GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving", "comment": null, "summary": "End-to-end autonomous driving requires adaptive and robust handling of\ncomplex and diverse traffic environments. However, prevalent single-mode\nplanning methods attempt to learn an overall policy while struggling to acquire\ndiversified driving skills to handle diverse scenarios. Therefore, this paper\nproposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework\nfeaturing a Global Expert, a Scene-Adaptive Experts Group, and equipped with a\nDual-aware Router. Specifically, the Global Expert is trained on the overall\ndataset, possessing robust performance. The Scene-Adaptive Experts are trained\non corresponding scene subsets, achieving adaptive performance. The Dual-aware\nRouter simultaneously considers scenario-level features and routing uncertainty\nto dynamically activate expert modules. Through the effective coupling of the\nGlobal Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,\nGEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS\noutperforms existing methods in the Bench2Drive closed-loop benchmark and\nachieves state-of-the-art performance in Driving Score and Success Rate, even\nwith only monocular vision input. Furthermore, ablation studies demonstrate\nsignificant improvements over the original single-expert baseline: 7.67% in\nDriving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The\ncode will be available at https://github.com/newbrains1/GEMINUS.", "AI": {"tldr": "本文提出GEMINUS，一个端到端自动驾驶的专家混合框架，通过结合全局专家、场景自适应专家组和双感知路由器，实现在复杂多样交通环境下的自适应和鲁棒驾驶。", "motivation": "现有的单一模式规划方法难以学习多样化的驾驶技能以应对复杂多样的交通场景，导致性能受限。", "method": "GEMINUS框架包含：1) 全局专家：在整体数据集上训练，提供鲁棒性能；2) 场景自适应专家组：在对应场景子集上训练，实现自适应性能；3) 双感知路由器：同时考虑场景级特征和路由不确定性，动态激活专家模块。通过双感知路由器有效耦合全局专家和场景自适应专家组。", "result": "GEMINUS在Bench2Drive闭环基准测试中超越现有方法，在驾驶得分和成功率上达到最先进水平（即使仅使用单目视觉输入）。消融研究显示，相对于原始单专家基线，驾驶得分提升7.67%，成功率提升22.06%，多能力平均得分提升19.41%。", "conclusion": "GEMINUS通过有效地结合全局专家和场景自适应专家，并通过双感知路由器进行动态管理，在多样化场景中实现了自适应和鲁棒的性能，显著优于现有方法和单一专家基线。"}}
{"id": "2507.14513", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14513", "abs": "https://arxiv.org/abs/2507.14513", "authors": ["Hongyi Yang", "Yue Pan", "Jiayi Xu", "Kelsen Liu"], "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy", "comment": null, "summary": "Recent advances in large language models (LLMs) and autonomous agents have\nenabled systems capable of performing complex tasks across domains such as\nhuman-computer interaction, planning, and web navigation. However, many\nexisting frameworks struggle in real-world or resource-constrained environments\ndue to their reliance on cloud-based computation, limited robustness in dynamic\ncontexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous\nagents optimized for embedded systems. Written in Rust for safety and\nperformance, Amico supports reactive, persistent agents that operate\nefficiently across embedded platforms and browser environments via WebAssembly.\nIt provides clean abstractions for event handling, state management, behavior\nexecution, and integration with reasoning modules. Amico delivers a unified\ninfrastructure for constructing resilient, interactive agents suitable for\ndeployment in settings with limited compute and intermittent connectivity.", "AI": {"tldr": "Amico是一个模块化、事件驱动的框架，用于构建针对嵌入式系统优化的自主代理，通过Rust和WebAssembly实现，支持在资源受限和连接不稳定的环境中高效、持久地运行。", "motivation": "现有的大型语言模型和自主代理框架在真实世界或资源受限环境中表现不佳，原因在于它们依赖云端计算、动态上下文中的鲁棒性有限以及缺乏持久自主性和环境感知能力。", "method": "Amico框架采用模块化、事件驱动的设计，使用Rust语言编写以确保安全性和性能，并通过WebAssembly支持在嵌入式平台和浏览器环境中高效运行。它提供清晰的事件处理、状态管理、行为执行和推理模块集成抽象。", "result": "Amico提供了一个统一的基础设施，用于构建具有弹性、交互性强的代理，适用于计算能力有限和连接间歇性中断的部署环境。", "conclusion": "Amico通过其优化设计和技术选择，克服了现有自主代理框架在资源受限环境中的挑战，实现了在嵌入式和浏览器环境中高效、持久运行的智能代理。"}}
{"id": "2507.15292", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15292", "abs": "https://arxiv.org/abs/2507.15292", "authors": ["An Wanga", "Rulin Zhou", "Mengya Xu", "Yiru Ye", "Longfei Gou", "Yiting Chang", "Hao Chen", "Chwee Ming Lim", "Jiankun Wang", "Hongliang Ren"], "title": "EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro", "comment": null, "summary": "Visualizing subtle vascular motions in endoscopic surgery is crucial for\nsurgical precision and decision-making, yet remains challenging due to the\ncomplex and dynamic nature of surgical scenes. To address this, we introduce\nEndoControlMag, a training-free, Lagrangian-based framework with\nmask-conditioned vascular motion magnification tailored to endoscopic\nenvironments. Our approach features two key modules: a Periodic Reference\nResetting (PRR) scheme that divides videos into short overlapping clips with\ndynamically updated reference frames to prevent error accumulation while\nmaintaining temporal coherence, and a Hierarchical Tissue-aware Magnification\n(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores\nusing a pretrained visual tracking model to maintain accurate localization\ndespite occlusions and view changes. It then applies one of two adaptive\nsoftening strategies to surrounding tissues: motion-based softening that\nmodulates magnification strength proportional to observed tissue displacement,\nor distance-based exponential decay that simulates biomechanical force\nattenuation. This dual-mode approach accommodates diverse surgical\nscenarios-motion-based softening excels with complex tissue deformations while\ndistance-based softening provides stability during unreliable optical flow\nconditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four\ndifferent surgery types and various challenging scenarios, including\nocclusions, instrument disturbance, view changes, and vessel deformations.\nQuantitative metrics, visual assessments, and expert surgeon evaluations\ndemonstrate that EndoControlMag significantly outperforms existing methods in\nboth magnification accuracy and visual quality while maintaining robustness\nacross challenging surgical conditions. The code, dataset, and video results\nare available at https://szupc.github.io/EndoControlMag/.", "AI": {"tldr": "该论文提出了EndoControlMag，一个无需训练的、基于拉格朗日的框架，用于在内窥镜手术中放大血管的微小运动，以提高手术精度和决策。", "motivation": "在内窥镜手术中，可视化微小的血管运动对于手术精度和决策至关重要，但由于手术场景的复杂性和动态性，这仍然是一个挑战。", "method": "本文引入了EndoControlMag框架，该框架是无训练的、基于拉格朗日的，并结合了掩膜条件下的血管运动放大技术。它包含两个核心模块：1) 周期性参考重置（PRR）方案，将视频分割成短的重叠片段，并动态更新参考帧以防止误差累积并保持时间连贯性；2) 分层组织感知放大（HTM）框架，采用双模态掩膜膨胀策略，首先使用预训练的视觉跟踪模型跟踪血管核心，然后对周围组织应用两种自适应软化策略：基于运动的软化（放大强度与组织位移成比例）或基于距离的指数衰减（模拟生物力学衰减）。", "result": "EndoControlMag在作者构建的EndoVMM24数据集（涵盖四种不同手术类型和各种挑战性场景，包括遮挡、器械干扰、视角变化和血管变形）上进行了评估。定量指标、视觉评估和专家外科医生评估表明，EndoControlMag在放大精度和视觉质量方面显著优于现有方法，同时在挑战性的手术条件下保持了鲁棒性。", "conclusion": "EndoControlMag框架能够准确、鲁棒地放大内窥镜手术中的微小血管运动，从而有助于提高手术精度和决策。"}}
{"id": "2507.14931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14931", "abs": "https://arxiv.org/abs/2507.14931", "authors": ["Qiaoqiao Ren", "Remko Proesmans", "Arend Pissens", "Lara Dehandschutter", "William Denecker", "Lotte Rouckhout", "Joke Carrette", "Peter Vanhopplinus", "Tony Belpaeme", "Francis wyffels"], "title": "Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry", "comment": null, "summary": "Forensic mental health care involves the treatment of individuals with severe\nmental disorders who have committed violent offences. These settings are often\ncharacterized by high levels of bureaucracy, risk avoidance, and restricted\nautonomy. Patients frequently experience a profound loss of control over their\nlives, leading to heightened psychological stress-sometimes resulting in\nisolation as a safety measure. In this study, we explore how co-design can be\nused to collaboratively develop a companion robot that helps monitor and\nregulate stress while maintaining tracking of the patients' interaction\nbehaviours for long-term intervention. We conducted four co-design workshops in\na forensic psychiatric clinic with patients, caregivers, and therapists. Our\nprocess began with the presentation of an initial speculative prototype to\ntherapists, enabling reflection on shared concerns, ethical risks, and\ndesirable features. This was followed by a creative ideation session with\npatients, a third workshop focused on defining desired functions and emotional\nresponses, and we are planning a final prototype demo to gather direct patient\nfeedback. Our findings emphasize the importance of empowering patients in the\ndesign process and adapting proposals based on their current emotional state.\nThe goal was to empower the patient in the design process and ensure each\npatient's voice was heard.", "AI": {"tldr": "本研究探讨了在法医精神卫生环境中，如何通过协同设计开发一款陪伴机器人，用于监测和调节患者压力，并追踪其互动行为。", "motivation": "法医精神卫生患者常因高度官僚化、规避风险和自主权受限而经历严重的心理压力和失控感，有时导致隔离。因此，需要工具来帮助监测和调节他们的压力。", "method": "研究在一家法医精神病诊所与患者、护理人员和治疗师进行了四次协同设计工作坊。过程包括：向治疗师展示初始原型以收集反馈，与患者进行创意构思，定义所需功能和情感反应，并计划最终原型演示以获取患者直接反馈。", "result": "研究结果强调了在设计过程中赋予患者权力，并根据他们当前的情绪状态调整提案的重要性。目标是确保每位患者的声音都被听到。", "conclusion": "协同设计是一种有效的方法，可以在敏感的法医精神卫生环境中开发支持工具，并强调了在设计过程中赋能患者和响应其情绪状态的重要性。"}}
{"id": "2507.14863", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14863", "abs": "https://arxiv.org/abs/2507.14863", "authors": ["Hampei Sasahara"], "title": "Adversarial Destabilization Attacks to Direct Data-Driven Control", "comment": "15 pages", "summary": "This study investigates the vulnerability of direct data-driven control\nmethods, specifically for the linear quadratic regulator problem, to\nadversarial perturbations in collected data used for controller synthesis. We\nconsider stealthy attacks that subtly manipulate offline-collected data to\ndestabilize the resulting closed-loop system while evading detection. To\ngenerate such perturbations, we propose the Directed Gradient Sign Method\n(DGSM) and its iterative variant (I-DGSM), adaptations of the fast gradient\nsign method originally developed for neural networks, which align perturbations\nwith the gradient of the spectral radius of the closed-loop matrix to reduce\nstability. A key contribution is an efficient gradient computation technique\nbased on implicit differentiation through the Karush-Kuhn-Tucker conditions of\nthe underlying semidefinite program, enabling scalable and exact gradient\nevaluation without repeated optimization computations. To defend against these\nattacks, we propose two defense strategies: a regularization-based approach\nthat enhances robustness by suppressing controller sensitivity to data\nperturbations and a robust data-driven control approach that guarantees\nclosed-loop stability within bounded perturbation sets. Extensive numerical\nexperiments on benchmark systems show that adversarial perturbations with\nmagnitudes up to ten times smaller than random noise can destabilize\ncontrollers trained on corrupted data and that the proposed defense strategies\neffectively mitigate attack success rates while maintaining control\nperformance. Additionally, we evaluate attack transferability under partial\nknowledge scenarios, highlighting the practical importance of protecting\ntraining data confidentiality.", "AI": {"tldr": "本研究揭示了数据驱动控制（特别是LQR）对数据中对抗性扰动的脆弱性，并提出了DGSM/I-DGSM攻击方法和两种防御策略，实验证明小扰动即可导致系统失稳，而防御策略有效。", "motivation": "随着数据驱动控制方法的兴起，研究其在控制器合成中使用的收集数据对对抗性扰动的脆弱性变得至关重要，特别是那些旨在秘密操纵数据以使闭环系统失稳的隐蔽攻击。", "method": "1. **攻击生成**: 提出了定向梯度符号法（DGSM）及其迭代变体（I-DGSM），通过将扰动与闭环矩阵谱半径的梯度对齐来降低系统稳定性。\n2. **高效梯度计算**: 基于基础半定规划的Karush-Kuhn-Tucker（KKT）条件的隐式微分，实现可扩展且精确的梯度评估，无需重复优化计算。\n3. **防御策略**: 提出了两种防御方法：基于正则化的方法（通过抑制控制器对数据扰动的敏感性来增强鲁棒性）和鲁棒数据驱动控制方法（在有界扰动集合内保证闭环稳定性）。", "result": "1. 即使是比随机噪声小十倍的对抗性扰动，也能使在受损数据上训练的控制器失稳。\n2. 所提出的防御策略能有效降低攻击成功率，同时保持控制性能。\n3. 在部分知识场景下评估了攻击的可迁移性，强调了保护训练数据机密性的重要性。", "conclusion": "数据驱动控制方法对数据中的隐蔽对抗性扰动非常脆弱，即使是微小扰动也可能导致系统失稳。本研究提出的DGSM/I-DGSM攻击方法有效，而提出的正则化和鲁棒控制防御策略能够有效缓解这些攻击，同时保护训练数据机密性对于实际应用至关重要。"}}
{"id": "2507.14314", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14314", "abs": "https://arxiv.org/abs/2507.14314", "authors": ["Marija Anđedelić", "Dominik Šipek", "Laura Majer", "Jan Šnajder"], "title": "What Makes You CLIC: Detection of Croatian Clickbait Headlines", "comment": "Accepted at Slavic NLP 2025", "summary": "Online news outlets operate predominantly on an advertising-based revenue\nmodel, compelling journalists to create headlines that are often scandalous,\nintriguing, and provocative -- commonly referred to as clickbait. Automatic\ndetection of clickbait headlines is essential for preserving information\nquality and reader trust in digital media and requires both contextual\nunderstanding and world knowledge. For this task, particularly in\nless-resourced languages, it remains unclear whether fine-tuned methods or\nin-context learning (ICL) yield better results. In this paper, we compile CLIC,\na novel dataset for clickbait detection of Croatian news headlines spanning a\n20-year period and encompassing mainstream and fringe outlets. We fine-tune the\nBERTi\\'c model on this task and compare its performance to LLM-based ICL\nmethods with prompts both in Croatian and English. Finally, we analyze the\nlinguistic properties of clickbait. We find that nearly half of the analyzed\nheadlines contain clickbait, and that finetuned models deliver better results\nthan general LLMs.", "AI": {"tldr": "本文构建了一个克罗地亚语点击诱饵检测数据集CLIC，并比较了微调模型和LLM上下文学习在点击诱饵检测任务上的表现，发现微调模型效果更优。", "motivation": "在线新闻媒体依赖广告收入，导致记者创作点击诱饵标题，这损害了信息质量和读者信任。自动检测点击诱饵对于维护数字媒体的健康至关重要，尤其是在资源匮乏的语言中，尚不清楚微调方法和上下文学习何者更优。", "method": "研究者构建了一个包含20年克罗地亚新闻标题的点击诱饵检测数据集CLIC。他们微调了BERTić模型，并将其性能与基于LLM的上下文学习（ICL）方法进行比较，提示语包括克罗地亚语和英语。此外，论文还分析了点击诱饵的语言特性。", "result": "研究发现，近一半的分析标题包含点击诱饵。在点击诱饵检测任务中，微调模型（如BERTić）的表现优于通用大型语言模型（LLM）。", "conclusion": "对于点击诱饵检测任务，特别是对于资源匮乏的语言，微调模型比通用LLM的上下文学习方法能提供更好的结果。点击诱饵在新闻标题中普遍存在。"}}
{"id": "2507.14459", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14459", "abs": "https://arxiv.org/abs/2507.14459", "authors": ["Huayuan Ye", "Juntong Chen", "Shenzhuo Zhang", "Yipeng Zhang", "Changbo Wang", "Chenhui Li"], "title": "VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval", "comment": "9 pages, IEEE VIS 2025", "summary": "The dissemination of visualizations is primarily in the form of raster\nimages, which often results in the loss of critical information such as source\ncode, interactive features, and metadata. While previous methods have proposed\nembedding metadata into images to facilitate Visualization Image Data Retrieval\n(VIDR), most existing methods lack practicability since they are fragile to\ncommon image tampering during online distribution such as cropping and editing.\nTo address this issue, we propose VisGuard, a tamper-resistant VIDR framework\nthat reliably embeds metadata link into visualization images. The embedded data\nlink remains recoverable even after substantial tampering upon images. We\npropose several techniques to enhance robustness, including repetitive data\ntiling, invertible information broadcasting, and an anchor-based scheme for\ncrop localization. VisGuard enables various applications, including interactive\nchart reconstruction, tampering detection, and copyright protection. We conduct\ncomprehensive experiments on VisGuard's superior performance in data retrieval\naccuracy, embedding capacity, and security against tampering and steganalysis,\ndemonstrating VisGuard's competence in facilitating and safeguarding\nvisualization dissemination and information conveyance.", "AI": {"tldr": "VisGuard是一个抗篡改的可视化图像数据检索（VIDR）框架，它能可靠地将元数据链接嵌入到可视化图像中，即使图像经过大幅篡改也能恢复，从而解决现有VIDR方法在在线分发中易受图像篡改影响的问题。", "motivation": "可视化作品以栅格图像形式传播时，会丢失源代码、交互功能和元数据等关键信息。虽然已有方法尝试将元数据嵌入图像以实现可视化图像数据检索（VIDR），但大多数现有方法不实用，因为它们在在线分发过程中对常见的图像篡改（如裁剪和编辑）非常脆弱。", "method": "本文提出了VisGuard框架，一个抗篡改的VIDR框架。它通过以下技术增强鲁棒性：重复数据平铺（repetitive data tiling）、可逆信息广播（invertible information broadcasting）以及基于锚点（anchor-based scheme）的裁剪定位方案。", "result": "VisGuard在数据检索准确性、嵌入容量以及对抗篡改和隐写分析的安全性方面表现出卓越的性能。它支持交互式图表重建、篡改检测和版权保护等多种应用。", "conclusion": "VisGuard能够有效地促进和保护可视化内容的传播和信息传递，在保障可视化信息交流方面具有竞争力。"}}
{"id": "2507.14520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14520", "abs": "https://arxiv.org/abs/2507.14520", "authors": ["Xinyi Chen", "Yifei Yuan", "Jiaang Li", "Serge Belongie", "Maarten de Rijke", "Anders Søgaard"], "title": "What if Othello-Playing Language Models Could See?", "comment": "ICML 2025 Assessing World Models Workshop", "summary": "Language models are often said to face a symbol grounding problem. While some\nargue that world understanding can emerge from text alone, others suggest\ngrounded learning is more efficient. We explore this through Othello, where the\nboard state defines a simplified, rule-based world. Building on prior work, we\nintroduce VISOTHELLO, a multi-modal model trained on move histories and board\nimages. Using next-move prediction, we compare it to mono-modal baselines and\ntest robustness to semantically irrelevant perturbations. We find that\nmulti-modal training improves both performance and the robustness of internal\nrepresentations. These results suggest that grounding language in visual input\nhelps models infer structured world representations.", "AI": {"tldr": "本文通过奥赛罗棋游戏，探讨了语言模型中的符号接地问题。研究发现，多模态（文本加图像）训练的模型在性能和内部表示的鲁棒性上优于单模态模型，表明视觉输入有助于模型构建结构化的世界表示。", "motivation": "解决语言模型面临的符号接地问题，即关于世界理解是否仅凭文本即可获得，还是需要接地学习（grounded learning）效率更高这一争议。", "method": "使用奥赛罗棋作为简化的、基于规则的世界；引入多模态模型VISOTHELLO，该模型同时接受棋步历史（文本）和棋盘图像（视觉）作为输入；通过预测下一步棋来评估其性能，并与单模态基线进行比较；测试模型对语义无关扰动的鲁棒性。", "result": "多模态训练显著提高了模型的性能，并增强了内部表示的鲁棒性。", "conclusion": "将语言与视觉输入相结合有助于模型推断出结构化的世界表示，支持了接地学习的有效性。"}}
{"id": "2507.15340", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15340", "abs": "https://arxiv.org/abs/2507.15340", "authors": ["Marc Boubnovski Martell", "Kristofer Linton-Reid", "Mitchell Chen", "Sumeet Hindocha", "Benjamin Hunter", "Marco A. Calzado", "Richard Lee", "Joram M. Posma", "Eric O. Aboagye"], "title": "MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis", "comment": null, "summary": "High-resolution volumetric computed tomography (CT) is essential for accurate\ndiagnosis and treatment planning in thoracic diseases; however, it is limited\nby radiation dose and hardware costs. We present the Transformer Volumetric\nSuper-Resolution Network (\\textbf{TVSRN-V2}), a transformer-based\nsuper-resolution (SR) framework designed for practical deployment in clinical\nlung CT analysis. Built from scalable components, including Through-Plane\nAttention Blocks (TAB) and Swin Transformer V2 -- our model effectively\nreconstructs fine anatomical details in low-dose CT volumes and integrates\nseamlessly with downstream analysis pipelines. We evaluate its effectiveness on\nthree critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis\n-- across multiple clinical cohorts. To enhance robustness across variable\nacquisition protocols, we introduce pseudo-low-resolution augmentation,\nsimulating scanner diversity without requiring private data. TVSRN-V2\ndemonstrates a significant improvement in segmentation accuracy (+4\\% Dice),\nhigher radiomic feature reproducibility, and enhanced predictive performance\n(+0.06 C-index and AUC). These results indicate that SR-driven recovery of\nstructural detail significantly enhances clinical decision support, positioning\nTVSRN-V2 as a well-engineered, clinically viable system for dose-efficient\nimaging and quantitative analysis in real-world CT workflows.", "AI": {"tldr": "TVSRN-V2是一个基于Transformer的超分辨率网络，旨在提高低剂量肺部CT的图像质量，从而增强肺癌诊断和治疗规划中的临床分析任务。", "motivation": "高分辨率体层CT对于胸部疾病的准确诊断和治疗计划至关重要，但其应用受限于辐射剂量和硬件成本。", "method": "本文提出了TVSRN-V2，一个基于Transformer的超分辨率（SR）框架，利用可扩展组件，包括穿平面注意力块（TAB）和Swin Transformer V2。为了增强对不同采集协议的鲁棒性，引入了伪低分辨率增强技术。", "result": "TVSRN-V2在肺癌的三个关键任务（肺叶分割、影像组学和预后）上表现出显著改进：分割准确性提高（+4% Dice），影像组学特征可重复性更高，预测性能增强（+0.06 C-index和AUC）。", "conclusion": "这些结果表明，通过SR驱动的结构细节恢复显著增强了临床决策支持，使TVSRN-V2成为一个设计精良、临床可行的系统，适用于真实世界CT工作流中的剂量高效成像和定量分析。"}}
{"id": "2507.14967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14967", "abs": "https://arxiv.org/abs/2507.14967", "authors": ["Pratik Ingle", "Kasper Støy", "Andres Faiña"], "title": "Heterogeneous object manipulation on nonlinear soft surface through linear controller", "comment": "8 pages, 3 figures", "summary": "Manipulation surfaces indirectly control and reposition objects by actively\nmodifying their shape or properties rather than directly gripping objects.\nThese surfaces, equipped with dense actuator arrays, generate dynamic\ndeformations. However, a high-density actuator array introduces considerable\ncomplexity due to increased degrees of freedom (DOF), complicating control\ntasks. High DOF restrict the implementation and utilization of manipulation\nsurfaces in real-world applications as the maintenance and control of such\nsystems exponentially increase with array/surface size. Learning-based control\napproaches may ease the control complexity, but they require extensive training\nsamples and struggle to generalize for heterogeneous objects. In this study, we\nintroduce a simple, precise and robust PID-based linear close-loop feedback\ncontrol strategy for heterogeneous object manipulation on MANTA-RAY\n(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation\ndensity). Our approach employs a geometric transformation-driven PID\ncontroller, directly mapping tilt angle control outputs(1D/2D) to actuator\ncommands to eliminate the need for extensive black-box training. We validate\nthe proposed method through simulations and experiments on a physical system,\nsuccessfully manipulating objects with diverse geometries, weights and\ntextures, including fragile objects like eggs and apples. The outcomes\ndemonstrate that our approach is highly generalized and offers a practical and\nreliable solution for object manipulation on soft robotic manipulation,\nfacilitating real-world implementation without prohibitive training demands.", "AI": {"tldr": "本文提出了一种基于PID的线性闭环反馈控制策略，用于在低密度柔性操纵表面（MANTA-RAY）上精确、鲁棒地操纵异质物体，无需大量训练。", "motivation": "传统的操纵表面由于高密度执行器阵列引入了巨大的复杂性（高自由度），使得控制任务复杂且难以维护。基于学习的方法虽然可以简化控制，但需要大量训练样本且泛化能力差，限制了其实际应用。", "method": "研究引入了一种简单、精确、鲁棒的基于PID的线性闭环反馈控制策略。该方法采用几何变换驱动的PID控制器，将倾斜角度控制输出（1D/2D）直接映射到执行器命令，从而避免了对大量黑盒训练的需求。", "result": "通过仿真和物理系统实验验证了所提出的方法，成功操纵了具有不同几何形状、重量和纹理的物体，包括鸡蛋和苹果等易碎物品。结果表明该方法具有高度泛化性，为软体机器人操纵提供了实用可靠的解决方案。", "conclusion": "该研究提供了一种实用且可靠的软体机器人物体操纵解决方案，无需大量的训练需求，从而促进了操纵表面在实际世界中的应用。"}}
{"id": "2507.14952", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14952", "abs": "https://arxiv.org/abs/2507.14952", "authors": ["Mahyar Mahinzaeim", "Kamyar Mehran"], "title": "An approach to the LQG/LTR design problem with specifications for finite-dimensional SISO control systems", "comment": null, "summary": "This is an expository paper which discusses an approach to the LQG/LTR design\nproblem for finite-dimensional SISO control systems. The approach is based on\nthe utilisation of weighting augmentation for incorporating design\nspecifications into the framework of the LTR technique for LQG compensator\ndesign. The LQG compensator is to simultaneously meet given analytical low- and\nhigh-frequency design specifications expressed in terms of desirable\nsensitivity and controller noise sensitivity functions. The paper is aimed at\nnonspecialists and, in particular, practitioners in finite-dimensional LQG\ntheory interested in the design of feedback compensators for closed-loop\nperformance and robustness shaping of SISO control systems in realistic\nsituations. The proposed approach is illustrated by a detailed numerical\nexample: the torque control of a current-controlled DC motor with an\nelastically mounted rotor.", "AI": {"tldr": "本文探讨了一种基于加权增强的LQG/LTR设计方法，旨在帮助有限维SISO控制系统满足预设的频率设计规范。", "motivation": "研究动机在于为有限维SISO控制系统设计LQG补偿器，使其能够同时满足低频和高频的设计规范（如期望的灵敏度函数和控制器噪声灵敏度函数），并提升闭环系统的性能和鲁棒性，特别针对实际应用场景。", "method": "该方法基于加权增强技术，将其整合到LQG补偿器设计的LTR（Loop Transfer Recovery）框架中，以纳入设计规格。通过这种方式，LQG补偿器能够同时满足给定的分析性低频和高频设计要求。", "result": "提出了一种LQG/LTR设计方法，并通过一个详细的数值例子（例如，具有弹性安装转子的电流控制直流电机的扭矩控制）进行了具体说明和验证。", "conclusion": "该论文为非专业人士和LQG理论的实践者提供了一种实用的LQG/LTR设计方法，能够有效解决SISO控制系统在实际应用中闭环性能和鲁棒性整形的问题。"}}
{"id": "2507.14355", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14355", "abs": "https://arxiv.org/abs/2507.14355", "authors": ["Jianfeng Zhu", "Ruoming Jin", "Karin G. Coifman"], "title": "Can LLMs Infer Personality from Real World Conversations?", "comment": "21 pages, 12 figures", "summary": "Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a\npromising approach for scalable personality assessment from open-ended\nlanguage. However, inferring personality traits remains challenging, and\nearlier work often relied on synthetic data or social media text lacking\npsychometric validity. We introduce a real-world benchmark of 555\nsemi-structured interviews with BFI-10 self-report scores for evaluating\nLLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,\nMeta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item\nprediction and both zero-shot and chain-of-thought prompting for Big Five trait\ninference. All models showed high test-retest reliability, but construct\nvalidity was limited: correlations with ground-truth scores were weak (max\nPearson's $r = 0.27$), interrater agreement was low (Cohen's $\\kappa < 0.10$),\nand predictions were biased toward moderate or high trait levels.\nChain-of-thought prompting and longer input context modestly improved\ndistributional alignment, but not trait-level accuracy. These results\nunderscore limitations in current LLM-based personality inference and highlight\nthe need for evidence-based development for psychological applications.", "AI": {"tldr": "大语言模型在人格评估方面显示出局限性，尽管有高重测信度，但构念效度、预测准确性和一致性较低，且存在预测偏差。", "motivation": "现有研究依赖缺乏心理测量学效度的数据（如合成数据或社交媒体文本），导致LLM推断人格特质仍具挑战，需要更可靠的评估基准。", "method": "引入包含555份半结构化访谈和BFI-10自评得分的真实世界基准数据集。测试了GPT-4.1 Mini、Meta-LLaMA和DeepSeek三款LLM，采用零样本提示进行BFI-10项目预测，并结合零样本和思维链提示进行大五人格特质推断。", "result": "所有模型均显示出高重测信度，但构念效度有限：与真实分数的相关性弱（最大Pearson's r = 0.27），评估者间一致性低（Cohen's κ < 0.10），且预测偏向中等或高特质水平。思维链提示和更长的输入上下文略微改善了分布对齐，但未提高特质层面的准确性。", "conclusion": "当前基于LLM的人格推断存在局限性，强调了心理应用中需要基于证据的开发。"}}
{"id": "2507.14477", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14477", "abs": "https://arxiv.org/abs/2507.14477", "authors": ["Zhenyu Li", "Tianyi Shang", "Pengjie Xu", "Ruirui Zhang", "Fanchen Kong"], "title": "OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition", "comment": "5 figures", "summary": "Visual Place Recognition (VPR) in dynamic and perceptually aliased\nenvironments remains a fundamental challenge for long-term localization.\nExisting deep learning-based solutions predominantly focus on single-frame\nembeddings, neglecting the temporal coherence present in image sequences. This\npaper presents OptiCorNet, a novel sequence modeling framework that unifies\nspatial feature extraction and temporal differencing into a differentiable,\nend-to-end trainable module. Central to our approach is a lightweight 1D\nconvolutional encoder combined with a learnable differential temporal operator,\ntermed Differentiable Sequence Delta (DSD), which jointly captures short-term\nspatial context and long-range temporal transitions. The DSD module models\ndirectional differences across sequences via a fixed-weight differencing\nkernel, followed by an LSTM-based refinement and optional residual projection,\nyielding compact, discriminative descriptors robust to viewpoint and appearance\nshifts. To further enhance inter-class separability, we incorporate a\nquadruplet loss that optimizes both positive alignment and multi-negative\ndivergence within each batch. Unlike prior VPR methods that treat temporal\naggregation as post-processing, OptiCorNet learns sequence-level embeddings\ndirectly, enabling more effective end-to-end place recognition. Comprehensive\nevaluations on multiple public benchmarks demonstrate that our approach\noutperforms state-of-the-art baselines under challenging seasonal and viewpoint\nvariations.", "AI": {"tldr": "本文提出了OptiCorNet，一个用于视觉地点识别（VPR）的新型序列建模框架，它将空间特征提取和时间差分统一起来，以应对动态和感知混叠环境下的长期定位挑战。", "motivation": "现有的深度学习VPR解决方案主要关注单帧嵌入，忽略了图像序列中存在的时间连贯性，这在动态和感知混叠环境中对长期定位构成根本挑战。", "method": "OptiCorNet是一个端到端可训练的模块，它结合了空间特征提取和时间差分。其核心是一个轻量级的一维卷积编码器和一个可学习的差分时间算子——可微分序列差分（DSD）。DSD模块通过固定权重的差分核，结合基于LSTM的细化和可选的残差投影，建模序列间的方向性差异。此外，为了增强类间可分离性，该方法引入了四元组损失，优化批次内的正向对齐和多负向发散。", "result": "OptiCorNet在多个公共基准测试中表现出色，在具有挑战性的季节和视角变化下，其性能优于最先进的基线方法。", "conclusion": "OptiCorNet通过直接学习序列级嵌入，而非将时间聚合作为后处理，实现了更有效的端到端地点识别，生成了对视角和外观变化具有鲁棒性的紧凑、判别性描述符。"}}
{"id": "2507.14552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14552", "abs": "https://arxiv.org/abs/2507.14552", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskisärkkä", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Large Language Models Assisting Ontology Evaluation", "comment": null, "summary": "Ontology evaluation through functional requirements, such as testing via\ncompetency question (CQ) verification, is a well-established yet costly,\nlabour-intensive, and error-prone endeavour, even for ontology engineering\nexperts. In this work, we introduce OE-Assist, a novel framework designed to\nassist ontology evaluation through automated and semi-automated CQ\nverification. By presenting and leveraging a dataset of 1,393 CQs paired with\ncorresponding ontologies and ontology stories, our contributions present, to\nour knowledge, the first systematic investigation into large language model\n(LLM)-assisted ontology evaluation, and include: (i) evaluating the\neffectiveness of a LLM-based approach for automatically performing CQ\nverification against a manually created gold standard, and (ii) developing and\nassessing an LLM-powered framework to assist CQ verification with Prot\\'eg\\'e,\nby providing suggestions. We found that automated LLM-based evaluation with\no1-preview and o3-mini perform at a similar level to the average user's\nperformance.", "AI": {"tldr": "OE-Assist是一个新颖的框架，旨在通过自动化和半自动化能力问题（CQ）验证来辅助本体评估，并首次系统地研究了大型语言模型（LLM）辅助的本体评估。", "motivation": "本体评估，特别是通过能力问题（CQ）验证进行的测试，尽管是公认的方法，但成本高昂、劳动密集且容易出错，即使对于本体工程专家也是如此。", "method": "本文引入了OE-Assist框架，利用自动化和半自动化CQ验证来辅助本体评估。研究利用了一个包含1,393个CQ及其对应本体和本体故事的数据集，并进行了两项主要工作：(i) 评估了基于LLM的自动CQ验证方法与手动创建的黄金标准相比的有效性；(ii) 开发并评估了一个LLM驱动的框架，通过在Protégé中提供建议来辅助CQ验证。", "result": "研究发现，使用o1-preview和o3-mini模型的自动化LLM本体评估表现与普通用户的平均性能水平相似。", "conclusion": "LLM辅助的本体评估具有潜力，其自动化性能已能达到与普通用户相当的水平，为解决传统本体评估的挑战提供了新的途径。"}}
{"id": "2507.15361", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15361", "abs": "https://arxiv.org/abs/2507.15361", "authors": ["Muhammad Aqeel", "Maham Nazir", "Zanxi Ruan", "Francesco Setti"], "title": "Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation", "comment": "Accepted to CVGMMI Workshop at ICIAP 2025", "summary": "Medical image segmentation suffers from data scarcity, particularly in polyp\ndetection where annotation requires specialized expertise. We present SynDiff,\na framework combining text-guided synthetic data generation with efficient\ndiffusion-based segmentation. Our approach employs latent diffusion models to\ngenerate clinically realistic synthetic polyps through text-conditioned\ninpainting, augmenting limited training data with semantically diverse samples.\nUnlike traditional diffusion methods requiring iterative denoising, we\nintroduce direct latent estimation enabling single-step inference with T x\ncomputational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%\nIoU while maintaining real-time capability suitable for clinical deployment.\nThe framework demonstrates that controlled synthetic augmentation improves\nsegmentation robustness without distribution shift. SynDiff bridges the gap\nbetween data-hungry deep learning models and clinical constraints, offering an\nefficient solution for deployment in resourcelimited medical settings.", "AI": {"tldr": "SynDiff是一种结合文本引导合成数据生成和高效扩散模型的框架，旨在解决医学图像分割（特别是息肉检测）中的数据稀缺问题，实现了高精度和实时分割能力。", "motivation": "医学图像分割面临数据稀缺问题，尤其在息肉检测中，标注需要专业知识，导致训练数据有限。", "method": "该方法利用潜在扩散模型通过文本条件修复生成临床真实的合成息肉，以扩充训练数据；并引入直接潜在估计，实现单步推理，比传统扩散方法计算速度提高T倍，从而实现实时分割。", "result": "在CVC-ClinicDB数据集上，SynDiff实现了96.0%的Dice系数和92.9%的IoU，同时保持实时能力，适用于临床部署。该框架证明，受控的合成数据增强可以提高分割鲁棒性，而不会导致分布偏移。", "conclusion": "SynDiff弥合了数据密集型深度学习模型与临床约束之间的差距，为资源受限的医疗环境提供了一个高效的部署解决方案。"}}
{"id": "2507.14975", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14975", "abs": "https://arxiv.org/abs/2507.14975", "authors": ["Yufan Song", "Jiatao Zhang", "Zeng Gu", "Qingmiao Liang", "Tuocheng Hu", "Wei Song", "Shiqiang Zhu"], "title": "FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models", "comment": "8 pages, 6 figures, IROS 2025", "summary": "Autonomous error correction is critical for domestic robots to achieve\nreliable execution of complex long-horizon tasks. Prior work has explored\nself-reflection in Large Language Models (LLMs) for task planning error\ncorrection; however, existing methods are constrained by inflexible\nself-reflection mechanisms that limit their effectiveness. Motivated by these\nlimitations and inspired by human cognitive adaptation, we propose the Flexible\nConstructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture\nthat enables LLMs to perform flexible self-reflection based on task difficulty,\nwhile constructively integrating historical valuable experience with failure\nlessons. We evaluated FCRF on diverse domestic tasks through simulation in\nAlfWorld and physical deployment in the real-world environment. Experimental\nresults demonstrate that FCRF significantly improves overall performance and\nself-reflection flexibility in complex long-horizon robotic tasks.", "AI": {"tldr": "本文提出了一种名为FCRF（Flexible Constructivism Reflection Framework）的灵活自反思框架，通过Mentor-Actor架构使LLM能够基于任务难度进行灵活自反思，并整合历史经验和失败教训，显著提升了家用机器人在复杂长周期任务中的错误纠正能力和性能。", "motivation": "现有的LLM自反思方法在机器人任务规划错误纠正方面存在机制僵化、缺乏灵活性，从而限制了其有效性。", "method": "提出灵活建构主义反思框架（FCRF），这是一种新颖的Mentor-Actor架构，使LLM能够根据任务难度进行灵活的自反思，并建设性地整合历史有价值的经验和失败教训。", "result": "实验结果表明，FCRF在复杂长周期机器人任务中显著提高了整体性能和自反思的灵活性。该框架在AlfWorld模拟环境和真实世界环境中都得到了验证。", "conclusion": "FCRF通过其灵活的自反思机制和经验整合能力，有效解决了现有LLM自反思方法的局限性，为家用机器人实现可靠的复杂长周期任务执行提供了关键的错误纠正能力。"}}
{"id": "2507.15031", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15031", "abs": "https://arxiv.org/abs/2507.15031", "authors": ["Omid Akbarzadeh", "Mohammad H. Mamduhi", "Abolfazl Lavaei"], "title": "Safety Controller Synthesis for Stochastic Networked Systems under Communication Constraints", "comment": null, "summary": "This paper develops a framework for synthesizing safety controllers for\ndiscrete-time stochastic linear control systems (dt-SLS) operating under\ncommunication imperfections. The control unit is remote and communicates with\nthe sensor and actuator through an imperfect wireless network. We consider a\nconstant delay in the sensor-to-controller channel (uplink), and data loss in\nboth sensor-to-controller and controller-to-actuator (downlink) channels. In\nour proposed scheme, data loss in each channel is modeled as an independent\nBernoulli-distributed random process. To systematically handle the uplink\ndelay, we first introduce an augmented discrete-time stochastic linear system\n(dt-ASLS) by concatenating all states and control inputs that sufficiently\nrepresent the state-input evolution of the original dt-SLS under the delay and\npacket loss constraints. We then leverage control barrier certificates (CBCs)\nfor dt-ASLS to synthesize a controller that guarantees dt-SLS safety in a\nstochastic sense, ensuring that all trajectories of dt-SLS remain within safe\nregions with a quantified probabilistic bound. Our approach translates safety\nconstraints into matrix inequalities, leading to an optimization problem that\neventually quantifies the probability of satisfying the safety specification in\nthe presence of communication imperfections. We validate our results on an RLC\ncircuit subject to both constant delay and probabilistic data loss.", "AI": {"tldr": "本文提出一个框架，用于为在存在通信缺陷（恒定延迟和数据丢失）的离散时间随机线性控制系统（dt-SLS）中合成安全控制器。", "motivation": "远程控制系统通过不完美的无线网络进行通信（存在传感器到控制器信道的延迟和两个方向的数据丢失），需要确保系统在随机意义上的安全性。", "method": "将数据丢失建模为独立的伯努利分布随机过程。通过引入一个增广离散时间随机线性系统（dt-ASLS）来处理上行链路延迟。利用dt-ASLS的控制障碍证书（CBCs）来合成控制器，将安全约束转化为矩阵不等式，形成一个优化问题来量化满足安全规范的概率。", "result": "该方法能够合成控制器，确保dt-SLS的轨迹在量化概率范围内保持在安全区域内，从而在随机意义上保证系统安全。它量化了在通信缺陷存在下满足安全规范的概率。结果在一个RLC电路上得到了验证。", "conclusion": "本文成功开发了一个框架，能够在存在通信缺陷（恒定延迟和概率性数据丢失）的情况下，为离散时间随机线性控制系统合成安全控制器，并提供量化的概率安全保证。"}}
{"id": "2507.14372", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14372", "abs": "https://arxiv.org/abs/2507.14372", "authors": ["Albert Chen", "Manas Bundele", "Gaurav Ahlawat", "Patrick Stetz", "Zhitao Wang", "Qiang Fei", "Donghoon Jung", "Audrey Chu", "Bharadwaj Jayaraman", "Ayushi Panth", "Yatin Arora", "Sourav Jain", "Renjith Varma", "Alexey Ilin", "Iuliia Melnychuk", "Chelsea Chueh", "Joyan Sil", "Xiaofeng Wang"], "title": "Text-to-SQL for Enterprise Data Analytics", "comment": "11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25", "summary": "The introduction of large language models has brought rapid progress on\nText-to-SQL benchmarks, but it is not yet easy to build a working enterprise\nsolution. In this paper, we present insights from building an internal chatbot\nthat enables LinkedIn's product managers, engineers, and operations teams to\nself-serve data insights from a large, dynamic data lake. Our approach features\nthree components. First, we construct a knowledge graph that captures\nup-to-date semantics by indexing database metadata, historical query logs,\nwikis, and code. We apply clustering to identify relevant tables for each team\nor product area. Second, we build a Text-to-SQL agent that retrieves and ranks\ncontext from the knowledge graph, writes a query, and automatically corrects\nhallucinations and syntax errors. Third, we build an interactive chatbot that\nsupports various user intents, from data discovery to query writing to\ndebugging, and displays responses in rich UI elements to encourage follow-up\nchats. Our chatbot has over 300 weekly users. Expert review shows that 53% of\nits responses are correct or close to correct on an internal benchmark set.\nThrough ablation studies, we identify the most important knowledge graph and\nmodeling components, offering a practical path for developing enterprise\nText-to-SQL solutions.", "AI": {"tldr": "本文介绍了一个为LinkedIn内部团队构建的企业级Text-to-SQL聊天机器人，该机器人通过知识图谱、Text-to-SQL代理和交互式界面，帮助用户从数据湖中获取数据洞察。", "motivation": "尽管大型语言模型在Text-to-SQL基准测试上取得了快速进展，但构建一个可用的企业级解决方案仍然不易。", "method": "该方法包含三个核心组件：1. 构建知识图谱，通过索引数据库元数据、历史查询日志、维基和代码来捕获最新语义，并应用聚类识别相关表格。2. 建立Text-to-SQL代理，从知识图谱中检索并排序上下文，编写查询，并自动纠正幻觉和语法错误。3. 开发交互式聊天机器人，支持从数据发现到查询编写和调试的各种用户意图，并以丰富的UI元素显示响应以鼓励后续对话。", "result": "该聊天机器人每周有超过300名活跃用户。专家评审显示，在内部基准测试集上，53%的响应是正确或接近正确的。通过消融研究，论文确定了最重要的知识图谱和建模组件。", "conclusion": "该研究提供了一条开发企业级Text-to-SQL解决方案的实用路径。"}}
{"id": "2507.14481", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14481", "abs": "https://arxiv.org/abs/2507.14481", "authors": ["Yujia Tong", "Jingling Yuan", "Tian Zhang", "Jianquan Liu", "Chuang Hu"], "title": "DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning", "comment": null, "summary": "Data-Free Quantization (DFQ) enables the quantization of Vision Transformers\n(ViTs) without requiring access to data, allowing for the deployment of ViTs on\ndevices with limited resources. In DFQ, the quantization model must be\ncalibrated using synthetic samples, making the quality of these synthetic\nsamples crucial. Existing methods fail to fully capture and balance the global\nand local features within the samples, resulting in limited synthetic data\nquality. Moreover, we have found that during inference, there is a significant\ndifference in the distributions of intermediate layer activations between the\nquantized and full-precision models. These issues lead to a severe performance\ndegradation of the quantized model. To address these problems, we propose a\npipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).\nSpecifically, we synthesize samples in order of increasing difficulty,\neffectively enhancing the quality of synthetic data. During the calibration and\ninference stage, we introduce the activation correction matrix for the\nquantized model to align the intermediate layer activations with those of the\nfull-precision model. Extensive experiments demonstrate that DFQ-ViT achieves\nremarkable superiority over existing DFQ methods and its performance is on par\nwith models quantized through real data. For example, the performance of DeiT-T\nwith 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our\nmethod eliminates the need for fine-tuning, which not only reduces\ncomputational overhead but also lowers the deployment barriers for edge\ndevices. This characteristic aligns with the principles of Green Learning by\nimproving energy efficiency and facilitating real-world applications in\nresource-constrained environments.", "AI": {"tldr": "该研究提出了一种针对视觉Transformer的无数据量化（DFQ-ViT）方法，通过改进合成样本质量和校正中间层激活分布，显著提升了量化模型的性能，且无需真实数据或微调。", "motivation": "现有无数据量化方法在合成样本时未能充分捕捉和平衡全局与局部特征，导致合成数据质量有限。此外，量化模型与全精度模型在推理时中间层激活分布存在显著差异，这些问题导致量化模型性能严重下降。", "method": "提出DFQ-ViT，具体方法包括：1) 按照难度递增的顺序合成样本，以提高合成数据质量。2) 在校准和推理阶段引入“激活校正矩阵”，以对齐量化模型与全精度模型之间的中间层激活。", "result": "DFQ-ViT显著优于现有DFQ方法，其性能与通过真实数据量化的模型相当。例如，DeiT-T在3比特权重量化下性能比现有最佳方法高4.29%。该方法无需微调，降低了计算开销和边缘设备部署障碍。", "conclusion": "DFQ-ViT成功解决了视觉Transformer无数据量化中的关键问题，在不依赖真实数据和微调的情况下实现了卓越的性能，符合绿色学习原则，有助于在资源受限环境中部署模型。"}}
{"id": "2507.14593", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14593", "abs": "https://arxiv.org/abs/2507.14593", "authors": ["Omar Al-Desi"], "title": "Coordinate Heart System: A Geometric Framework for Emotion Representation", "comment": "26 pages", "summary": "This paper presents the Coordinate Heart System (CHS), a geometric framework\nfor emotion representation in artificial intelligence applications. We position\neight core emotions as coordinates on a unit circle, enabling mathematical\ncomputation of complex emotional states through coordinate mixing and vector\noperations. Our initial five-emotion model revealed significant coverage gaps\nin the emotion space, leading to the development of an eight-emotion system\nthat provides complete geometric coverage with mathematical guarantees. The\nframework converts natural language input to emotion coordinates and supports\nreal-time emotion interpolation through computational algorithms. The system\nintroduces a re-calibrated stability parameter S in [0,1], which dynamically\nintegrates emotional load, conflict resolution, and contextual drain factors.\nThis stability model leverages advanced Large Language Model interpretation of\ntextual cues and incorporates hybrid temporal tracking mechanisms to provide\nnuanced assessment of psychological well-being states. Our key contributions\ninclude: (i) mathematical proof demonstrating why five emotions are\ninsufficient for complete geometric coverage, (ii) an eight-coordinate system\nthat eliminates representational blind spots, (iii) novel algorithms for\nemotion mixing, conflict resolution, and distance calculation in emotion space,\nand (iv) a comprehensive computational framework for AI emotion recognition\nwith enhanced multi-dimensional stability modeling. Experimental validation\nthrough case studies demonstrates the system's capability to handle emotionally\nconflicted states, contextual distress factors, and complex psychological\nscenarios that traditional categorical emotion models cannot adequately\nrepresent. This work establishes a new mathematical foundation for emotion\nmodeling in artificial intelligence systems.", "AI": {"tldr": "本文提出了坐标心脏系统（CHS），一个用于AI情感表示的几何框架，将八种核心情感置于单位圆上，通过坐标运算和向量操作处理复杂情感，并引入动态稳定性参数S，结合LLM和时间追踪机制，实现对心理状态的细致评估。", "motivation": "传统的五情感模型在情感空间中存在显著的覆盖盲点，无法充分表示复杂、冲突或受上下文影响的情感状态，促使研究者寻求更全面的数学化情感表示方法。", "method": "该研究将八种核心情感作为单位圆上的坐标，通过坐标混合和向量运算计算复杂情感；将自然语言输入转换为情感坐标；支持实时情感插值。引入了重新校准的稳定性参数S（在[0,1]之间），该参数动态整合情感负荷、冲突解决和上下文消耗因素，并利用大型语言模型（LLM）解释文本线索，结合混合时间追踪机制进行心理健康评估。", "result": "该八坐标系统提供了完整且有数学保证的几何覆盖，消除了情感表示的盲点。实验验证表明，该系统能够处理传统分类情感模型难以充分表示的情感冲突状态、上下文困扰因素和复杂的心理场景。", "conclusion": "这项工作为人工智能系统中的情感建模奠定了新的数学基础，克服了传统分类情感模型的局限性，提供了更全面和精细的情感表示与评估能力。"}}
{"id": "2507.15476", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15476", "abs": "https://arxiv.org/abs/2507.15476", "authors": ["Cong Chen", "Ming Chen", "Hoileong Lee", "Yan Li", "Jiyang Yu"], "title": "A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization", "comment": null, "summary": "Surface defect detection of steel, especially the recognition of multi-scale\ndefects, has always been a major challenge in industrial manufacturing. Steel\nsurfaces not only have defects of various sizes and shapes, which limit the\naccuracy of traditional image processing and detection methods in complex\nenvironments. However, traditional defect detection methods face issues of\ninsufficient accuracy and high miss-detection rates when dealing with small\ntarget defects. To address this issue, this study proposes a detection\nframework based on deep learning, specifically YOLOv9s, combined with the\nC3Ghost module, SCConv module, and CARAFE upsampling operator, to improve\ndetection accuracy and model performance. First, the SCConv module is used to\nreduce feature redundancy and optimize feature representation by reconstructing\nthe spatial and channel dimensions. Second, the C3Ghost module is introduced to\nenhance the model's feature extraction ability by reducing redundant\ncomputations and parameter volume, thereby improving model efficiency. Finally,\nthe CARAFE upsampling operator, which can more finely reorganize feature maps\nin a content-aware manner, optimizes the upsampling process and ensures\ndetailed restoration of high-resolution defect regions. Experimental results\ndemonstrate that the proposed model achieves higher accuracy and robustness in\nsteel surface defect detection tasks compared to other methods, effectively\naddressing defect detection problems.", "AI": {"tldr": "本研究提出一种基于YOLOv9s的深度学习框架，结合SCConv、C3Ghost模块和CARAFE上采样算子，以提高钢材表面多尺度缺陷检测的精度和鲁棒性。", "motivation": "钢材表面多尺度缺陷检测是工业制造中的一个重大挑战。传统图像处理和检测方法在复杂环境和处理小目标缺陷时，存在精度不足和漏检率高的问题。", "method": "本研究提出了一个基于YOLOv9s的深度学习检测框架。具体方法包括：1. 使用SCConv模块重构空间和通道维度，以减少特征冗余并优化特征表示。2. 引入C3Ghost模块，通过减少冗余计算和参数量来增强模型的特征提取能力和效率。3. 采用CARAFE上采样算子，以内容感知的方式更精细地重组特征图，优化上采样过程，确保高分辨率缺陷区域的细节恢复。", "result": "实验结果表明，所提出的模型在钢材表面缺陷检测任务中，相比其他方法取得了更高的精度和鲁棒性。", "conclusion": "该模型有效解决了钢材表面缺陷检测中的多尺度和复杂环境挑战。"}}
{"id": "2507.15022", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15022", "abs": "https://arxiv.org/abs/2507.15022", "authors": ["Sumeadh MS", "Kevin Dsouza", "Ravi Prakash"], "title": "CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions", "comment": "6pages, 4figures, Submitted to the prestigious Indian Control\n  Conference (ICC), 2025", "summary": "Among the promising approaches to enforce safety in control systems, learning\nControl Barrier Functions (CBFs) from expert demonstrations has emerged as an\neffective strategy. However, a critical challenge remains: verifying that the\nlearned CBFs truly enforce safety across the entire state space. This is\nespecially difficult when CBF is represented using neural networks (NCBFs).\nSeveral existing verification techniques attempt to address this problem\nincluding SMT-based solvers, mixed-integer programming (MIP), and interval or\nbound-propagation methods but these approaches often introduce loose,\nconservative bounds. To overcome these limitations, in this work we use\nCPED-NCBFs a split-conformal prediction based verification strategy to verify\nthe learned NCBF from the expert demonstrations. We further validate our method\non point mass systems and unicycle models to demonstrate the effectiveness of\nthe proposed theory.", "AI": {"tldr": "本文提出CPED-NCBFs，一种基于分裂共形预测的验证策略，用于验证从专家演示中学习到的神经网络控制障碍函数（NCBFs），以解决现有验证方法保守或宽松的边界问题。", "motivation": "从专家演示中学习控制障碍函数（CBFs）是确保控制系统安全的一种有效方法，但验证学习到的CBFs（尤其是神经网络表示的NCBFs）能否在整个状态空间内真正保证安全是一个关键挑战。现有验证技术（如SMT求解器、混合整数规划、区间或边界传播方法）常引入宽松或保守的边界，难以提供可靠的安全保证。", "method": "本文采用CPED-NCBFs，一种基于分裂共形预测的验证策略，来验证从专家演示中学习到的NCBFs。", "result": "该方法在质点系统和独轮车模型上进行了验证，结果表明所提出的理论有效。", "conclusion": "所提出的CPED-NCBFs提供了一种有效的方法来验证从专家演示中学习到的NCBFs，克服了现有验证技术中边界宽松或保守的限制，从而更好地确保控制系统的安全性。"}}
{"id": "2507.15047", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.15047", "abs": "https://arxiv.org/abs/2507.15047", "authors": ["Michelangelo Bin", "David Angeli"], "title": "On an Abstraction of Lyapunov and Lagrange Stability", "comment": null, "summary": "This paper studies a set-theoretic generalization of Lyapunov and Lagrange\nstability for abstract systems described by set-valued maps. Lyapunov stability\nis characterized as the property of inversely mapping filters to filters,\nLagrange stability as that of mapping ideals to ideals. These abstract\ndefinitions unveil a deep duality between the two stability notions, enable a\ndefinition of global stability for abstract systems, and yield an agile\ngeneralization of the stability theorems for basic series, parallel, and\nfeedback interconnections, including a small-gain theorem. Moreover, it is\nshown that Lagrange stability is abstractly identical to other properties of\ninterest in control theory, such as safety and positivity, whose preservation\nunder interconnections can be thus studied owing to the developed stability\nresults.", "AI": {"tldr": "本文提出一种针对集合值映射抽象系统的Lyapunov和Lagrange稳定性集合论泛化，揭示了两者之间的对偶性，并提供了互联系统（包括小增益定理）稳定性分析的通用框架。", "motivation": "研究Lyapunov和Lagrange稳定性在由集合值映射描述的抽象系统中的泛化，并分析其在系统互联下的保持性。", "method": "将Lyapunov稳定性定义为将滤波器逆向映射到滤波器，将Lagrange稳定性定义为将理想映射到理想。利用这些抽象定义来揭示稳定性概念的对偶性、定义全局稳定性，并研究其在基本串联、并联和反馈互联下的保持性。", "result": "揭示了Lyapunov和Lagrange稳定性之间的深刻对偶性；实现了抽象系统的全局稳定性定义；推广了串联、并联和反馈互联（包括小增益定理）的稳定性定理；表明Lagrange稳定性在抽象上等同于控制理论中其他重要性质，如安全性和正性。", "conclusion": "该研究提供了一个通用的框架，用于分析和推广抽象系统中Lyapunov和Lagrange稳定性（包括安全性、正性等相关性质）的概念，并研究它们在系统互联下的保持性。"}}
{"id": "2507.14374", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14374", "abs": "https://arxiv.org/abs/2507.14374", "authors": ["Sinchani Chakraborty", "Sudeshna Sarkar", "Pawan Goyal"], "title": "Error-Aware Curriculum Learning for Biomedical Relation Classification", "comment": "16 pages, 2 figures", "summary": "Relation Classification (RC) in biomedical texts is essential for\nconstructing knowledge graphs and enabling applications such as drug\nrepurposing and clinical decision-making. We propose an error-aware\nteacher--student framework that improves RC through structured guidance from a\nlarge language model (GPT-4o). Prediction failures from a baseline student\nmodel are analyzed by the teacher to classify error types, assign difficulty\nscores, and generate targeted remediations, including sentence rewrites and\nsuggestions for KG-based enrichment. These enriched annotations are used to\ntrain a first student model via instruction tuning. This model then annotates a\nbroader dataset with difficulty scores and remediation-enhanced inputs. A\nsecond student is subsequently trained via curriculum learning on this dataset,\nordered by difficulty, to promote robust and progressive learning. We also\nconstruct a heterogeneous biomedical knowledge graph from PubMed abstracts to\nsupport context-aware RC. Our approach achieves new state-of-the-art\nperformance on 4 of 5 PPI datasets and the DDI dataset, while remaining\ncompetitive on ChemProt.", "AI": {"tldr": "本文提出了一种错误感知师生框架，利用大型语言模型（GPT-4o）对生物医学文本中的关系分类（RC）进行结构化指导，并通过课程学习和知识图谱增强，显著提升了RC性能。", "motivation": "生物医学文本中的关系分类对于构建知识图谱以及支持药物再利用和临床决策等应用至关重要。", "method": "该方法采用一个错误感知师生框架：教师模型（GPT-4o）分析学生模型的预测失败，分类错误类型，分配难度分数，并生成有针对性的补救措施（如句子重写和基于知识图谱的丰富建议）。这些增强型标注用于通过指令微调训练第一个学生模型。第一个学生模型随后用难度分数和增强输入标注更广泛的数据集。第二个学生模型则通过课程学习，按难度排序在该数据集上进行训练，以促进鲁棒和渐进式学习。此外，研究还构建了一个异构生物医学知识图谱，以支持上下文感知的关系分类。", "result": "该方法在5个PPI数据集中的4个和DDI数据集上取得了新的最先进性能，并在ChemProt数据集上保持了竞争力。", "conclusion": "所提出的错误感知师生框架结合大型语言模型指导、课程学习和知识图谱增强，显著提升了生物医学关系分类的性能，达到了新的SOTA水平。"}}
{"id": "2507.14485", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14485", "abs": "https://arxiv.org/abs/2507.14485", "authors": ["Hongye Hou", "Liu Zhan", "Yang Yang"], "title": "Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion", "comment": null, "summary": "Completing the whole 3D structure based on an incomplete point cloud is a\nchallenging task, particularly when the residual point cloud lacks typical\nstructural characteristics. Recent methods based on cross-modal learning\nattempt to introduce instance images to aid the structure feature learning.\nHowever, they still focus on each particular input class, limiting their\ngeneration abilities. In this work, we propose a novel retrieval-augmented\npoint cloud completion framework. The core idea is to incorporate cross-modal\nretrieval into completion task to learn structural prior information from\nsimilar reference samples. Specifically, we design a Structural Shared Feature\nEncoder (SSFE) to jointly extract cross-modal features and reconstruct\nreference features as priors. Benefiting from a dual-channel control gate in\nthe encoder, relevant structural features in the reference sample are enhanced\nand irrelevant information interference is suppressed. In addition, we propose\na Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical\nfeature fusion mechanism to integrate reference prior information with input\nfeatures from global to local. Through extensive evaluations on multiple\ndatasets and real-world scenes, our method shows its effectiveness in\ngenerating fine-grained point clouds, as well as its generalization capability\nin handling sparse data and unseen categories.", "AI": {"tldr": "本文提出了一种新颖的检索增强点云补全框架，通过引入跨模态检索来学习结构先验信息，从而提升了点云补全的精细度和泛化能力。", "motivation": "从不完整的点云中补全整个3D结构是一项挑战性任务，尤其当残缺点云缺乏典型结构特征时。现有的跨模态学习方法通常只关注特定输入类别，限制了其生成能力和泛化性。", "method": "该方法的核心思想是将跨模态检索整合到点云补全任务中，以从相似的参考样本中学习结构先验信息。具体包括：1. 设计了一个结构共享特征编码器（SSFE），用于联合提取跨模态特征并重建参考特征作为先验；SSFE中的双通道控制门能增强相关结构特征并抑制无关信息干扰。2. 提出了一个渐进式检索增强生成器（PRAG），采用分层特征融合机制，将参考先验信息与输入特征从全局到局部进行整合。", "result": "在多个数据集和真实场景中的广泛评估表明，该方法在生成精细点云方面表现出有效性，并且在处理稀疏数据和未见过类别方面展现出强大的泛化能力。", "conclusion": "所提出的检索增强点云补全框架，通过引入结构先验学习和分层特征融合机制，有效解决了不完整点云补全的挑战，显著提升了补全结果的精细度和模型的泛化性能。"}}
{"id": "2507.14642", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14642", "abs": "https://arxiv.org/abs/2507.14642", "authors": ["Monoshiz Mahbub Khan", "Xioayin Xi", "Andrew Meneely", "Zhe Yu"], "title": "Efficient Story Point Estimation With Comparative Learning", "comment": null, "summary": "Story point estimation is an essential part of agile software development.\nStory points are unitless, project-specific effort estimates that help\ndevelopers plan their sprints. Traditionally, developers estimate story points\ncollaboratively using planning poker or other manual techniques. While the\ninitial calibrating of the estimates to each project is helpful, once a team\nhas converged on a set of precedents, story point estimation can become tedious\nand labor-intensive. Machine learning can reduce this burden, but only with\nenough context from the historical decisions made by the project team. That is,\nstate-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate\npredictions (within-project) when trained on data from the same project. The\ngoal of this work is to streamline story point estimation by evaluating a\ncomparative learning-based framework for calibrating project-specific story\npoint prediction models. Instead of assigning a specific story point value to\nevery backlog item, developers are presented with pairs of items, and indicate\nwhich item requires more effort. Using these comparative judgments, a machine\nlearning model is trained to predict the story point estimates. We empirically\nevaluated our technique using data with 23,313 manual estimates in 16 projects.\nThe model learned from comparative judgments can achieve on average 0.34\nSpearman's rank correlation coefficient between its predictions and the ground\ntruth story points. This is similar to, if not better than, the performance of\na regression model learned from the ground truth story points. Therefore, the\nproposed comparative learning approach is more efficient than state-of-the-art\nregression-based approaches according to the law of comparative judgments -\nproviding comparative judgments yields a lower cognitive burden on humans than\nproviding ratings or categorical labels.", "AI": {"tldr": "本文提出了一种基于比较学习的框架，用于校准敏捷软件开发中的故事点预测模型。通过让开发者比较成对的项目而非直接估算，该方法在降低认知负担的同时，实现了与现有回归模型相似的预测准确性。", "motivation": "敏捷软件开发中的故事点估算过程耗时且劳动密集。尽管机器学习可以减轻负担，但现有模型（如GPT2SP和FastText-SVM）通常需要特定项目的大量历史数据才能进行准确预测，这仍然是一个挑战。本研究旨在简化故事点估算过程。", "method": "提出了一种基于比较学习的框架。开发者被要求比较成对的待办事项，并指出哪个需要更多工作量，而不是为每个事项分配具体的点数。然后，利用这些比较判断来训练机器学习模型以预测故事点。该技术在包含16个项目、23,313个手动估算的数据集上进行了实证评估。", "result": "通过比较判断训练的模型，其预测与实际故事点之间的平均Spearman秩相关系数达到0.34。这一性能与从实际故事点学习的回归模型相似，甚至更好。", "conclusion": "所提出的比较学习方法比现有的基于回归的方法更高效。根据比较判断定律，提供比较判断比提供具体评分或分类标签对人类的认知负担更低，同时能够达到相当（甚至更好）的预测准确性。"}}
{"id": "2507.15487", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15487", "abs": "https://arxiv.org/abs/2507.15487", "authors": ["Dezhen Wang", "Sheng Miao", "Rongxin Chai", "Jiufa Cui"], "title": "DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification", "comment": "7 figures, 3 tables, submitted to AAAI2026", "summary": "Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency\ndomain information, which is crucial for accurate lesion classification in\nmedical imaging. However, effectively integrating multi-sequence MRI data for\nrobust 3D lesion classification remains a challenge. In this paper, we propose\nDeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel\nframework designed to extract decoupled representations and adaptively fuse\nspatial and spectral features for lesion classification. DeSamba introduces a\nDecoupled Representation Learning Module (DRLM) that decouples features from\ndifferent MRI sequences through self-reconstruction and cross-reconstruction,\nand a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,\nenabling dynamic fusion of spectral and spatial information based on lesion\ncharacteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On\na six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1\naccuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external\nvalidation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On\na spondylitis dataset (n=251) involving a challenging binary classification\ntask, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal\nand external validation sets, respectively. Ablation studies demonstrate that\nboth DRLM and SAMB significantly contribute to overall performance, with over\n10% relative improvement compared to the baseline. Our results highlight the\npotential of DeSamba as a generalizable and effective solution for 3D lesion\nclassification in multi-sequence medical imaging.", "AI": {"tldr": "DeSamba是一种新颖的框架，用于多序列MRI图像中的3D病灶分类，通过解耦表示学习和自适应融合空间与频谱特征，显著提升了分类性能。", "motivation": "有效整合多序列MRI数据进行鲁棒的3D病灶分类仍然是一个挑战，尽管MRI序列提供了丰富的空间和频率域信息。", "method": "本文提出了DeSamba（Decoupled Spectral Adaptive Network and Mamba-Based Model），该框架包含一个解耦表示学习模块（DRLM），通过自重建和交叉重建解耦不同MRI序列的特征；以及一个频谱自适应调制块（SAMB），在所提出的SAMNet中实现基于病灶特征的空间和频谱信息的动态融合。该模型基于Mamba。", "result": "DeSamba在两个临床相关3D数据集上进行了评估。在六分类脊柱转移数据集上，DeSamba在外部验证集上实现了62.10%的Top-1准确率、63.62%的F1分数、87.71%的AUC和93.55%的Top-3准确率，优于所有SOTA基线。在脊柱炎二分类任务中，DeSamba在内部和外部验证集上分别达到了70.00%/64.52%的准确率和74.75/73.88的AUC。消融研究表明，DRLM和SAMB对整体性能有显著贡献，相对基线提高了10%以上。", "conclusion": "DeSamba作为一种通用且有效的解决方案，在多序列医学影像的3D病灶分类中展现出巨大潜力。"}}
{"id": "2507.15062", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15062", "abs": "https://arxiv.org/abs/2507.15062", "authors": ["Xinyue Zhu", "Binghao Huang", "Yunzhu Li"], "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper", "comment": "More videos can be found on our\n  website:https://binghao-huang.github.io/touch_in_the_wild/", "summary": "Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .", "AI": {"tldr": "该研究提出了一种集成了触觉传感器的便携式抓手，用于同步收集视觉和触觉数据，并提出了一种跨模态表示学习框架，以实现更高效、更精确的机器人操作。", "motivation": "现有手持抓手大多缺乏触觉传感，而触觉反馈在精确操作中至关重要，尽管它们易于部署且用途广泛。", "method": "1. 设计并制造了一种便携、轻量级且集成触觉传感器的抓手，能够同步收集视觉和触觉数据。2. 提出了一种跨模态表示学习框架，该框架整合视觉和触觉信号，同时保留它们各自的独特特征。3. 学习过程旨在生成可解释的、专注于物理交互相关接触区域的表示。", "result": "1. 所开发的硬件能够同步收集多样化真实世界场景中的视觉和触觉数据。2. 该跨模态表示学习框架能产生更高效、更有效的策略学习，支持基于多模态反馈的精确机器人操作。3. 在试管插入和移液器流体转移等精细任务中，该方法表现出更高的准确性和在外部干扰下的鲁棒性。", "conclusion": "集成了触觉传感器的便携式抓手和提出的跨模态表示学习框架，能够实现基于多模态反馈的精确机器人操作，并在精细任务中显著提高了性能。"}}
{"id": "2507.15093", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15093", "abs": "https://arxiv.org/abs/2507.15093", "authors": ["Lucian Cristian Iacob", "Roland Tóth", "Maarten Schoukens"], "title": "Exact Finite Koopman Embedding of Block-Oriented Polynomial Systems", "comment": "Submitted to SIAM Journal on Applied Dynamical Systems (SIADS)", "summary": "The challenge of finding exact and finite-dimensional Koopman embeddings of\nnonlinear systems has been largely circumvented by employing data-driven\ntechniques to learn models of different complexities (e.g., linear, bilinear,\ninput affine). Although these models may provide good accuracy, selecting the\nmodel structure and dimension is still ad-hoc and it is difficult to quantify\nthe error that is introduced. In contrast to the general trend of data-driven\nlearning, in this paper, we develop a systematic technique for nonlinear\nsystems that produces a finite-dimensional and exact embedding. If the\nnonlinear system is represented as a network of series and parallel linear and\nnonlinear (polynomial) blocks, one can derive an associated Koopman model that\nhas constant state and output matrices and the input influence is polynomial.\nFurthermore, if the linear blocks do not have feedthrough, the Koopman\nrepresentation simplifies to a bilinear model.", "AI": {"tldr": "本文提出一种系统性技术，能为特定结构的非线性系统（由线性和多项式模块串并联组成）导出精确且有限维的Koopman嵌入，解决了传统数据驱动方法在模型选择和误差量化上的随意性问题。", "motivation": "现有的数据驱动Koopman嵌入技术在选择模型结构和维度时缺乏系统性，且难以量化引入的误差，尽管它们能提供不错的精度。这促使研究人员寻求一种能产生精确、有限维嵌入的系统方法。", "method": "该方法将非线性系统表示为串联和并联的线性与非线性（多项式）模块网络。通过这种结构化表示，可以推导出一个相应的Koopman模型。", "result": "推导出的Koopman模型具有常数状态和输出矩阵，并且输入影响是多项式的。此外，如果线性模块不包含前馈（feedthrough），Koopman表示将简化为双线性模型。", "conclusion": "该研究提供了一种系统性的方法，能为特定结构的非线性系统（由线性和多项式模块组成）生成精确且有限维的Koopman嵌入，这与当前主流的数据驱动学习方法形成对比，并解决了其在模型选择和误差量化上的局限性。"}}
{"id": "2507.14430", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14430", "abs": "https://arxiv.org/abs/2507.14430", "authors": ["Xiaolin Yan", "Yangxing Liu", "Jiazhang Zheng", "Chi Liu", "Mingyu Du", "Caisheng Chen", "Haoyang Liu", "Ming Ding", "Yuan Li", "Qiuping Liao", "Linfeng Li", "Zhili Mei", "Siyu Wan", "Li Li", "Ruyi Zhong", "Jiangling Yu", "Xule Liu", "Huihui Hu", "Jiameng Yue", "Ruohui Cheng", "Qi Yang", "Liangqing Wu", "Ke Zhu", "Chi Zhang", "Chufei Jing", "Yifan Zhou", "Yan Liang", "Dongdong Li", "Zhaohui Wang", "Bin Zhao", "Mingzhou Wu", "Mingzhong Zhou", "Peng Du", "Zuomin Liao", "Chao Dai", "Pengfei Liang", "Xiaoguang Zhu", "Yu Zhang", "Yu Gu", "Kun Pan", "Yuan Wu", "Yanqing Guan", "Shaojing Wu", "Zikang Feng", "Xianze Ma", "Peishan Cheng", "Wenjuan Jiang", "Jing Ba", "Huihao Yu", "Zeping Hu", "Yuan Xu", "Zhiwei Liu", "He Wang", "Zhenguo Lin", "Ming Liu", "Yanhong Meng"], "title": "X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display", "comment": "Technical Report", "summary": "Large language models (LLMs) have recently achieved significant advances in\nreasoning and demonstrated their advantages in solving challenging problems.\nYet, their effectiveness in the semiconductor display industry remains limited\ndue to a lack of domain-specific training and expertise. To bridge this gap, we\npresent X-Intelligence 3.0, the first high-performance reasoning model\nspecifically developed for the semiconductor display industry. This model is\ndesigned to deliver expert-level understanding and reasoning for the industry's\ncomplex challenges. Leveraging a carefully curated industry knowledge base, the\nmodel undergoes supervised fine-tuning and reinforcement learning to enhance\nits reasoning and comprehension capabilities. To further accelerate\ndevelopment, we implemented an automated evaluation framework that simulates\nexpert-level assessments. We also integrated a domain-specific\nretrieval-augmented generation (RAG) mechanism, resulting in notable\nperformance gains on benchmark datasets. Despite its relatively compact size of\n32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B\nacross multiple evaluations. This demonstrates its exceptional efficiency and\nestablishes it as a powerful solution to the longstanding reasoning challenges\nfaced by the semiconductor display industry.", "AI": {"tldr": "X-Intelligence 3.0是首个专为半导体显示行业开发的高性能推理模型，通过领域知识微调和RAG机制，以320亿参数超越了SOTA通用大模型，有效解决了行业推理难题。", "motivation": "尽管大型语言模型（LLMs）在推理方面取得了显著进展，但由于缺乏领域特定训练和专业知识，它们在半导体显示行业的有效性有限。", "method": "本文提出了X-Intelligence 3.0模型，利用精心策划的行业知识库进行监督微调和强化学习，以增强推理和理解能力。此外，集成了自动化评估框架和领域特定的检索增强生成（RAG）机制。", "result": "X-Intelligence 3.0在多个评估中超越了SOTA的DeepSeek-R1-671B模型，尽管其参数量相对紧凑（320亿），并在基准数据集上取得了显著的性能提升。", "conclusion": "X-Intelligence 3.0展示了卓越的效率和能力，为半导体显示行业长期存在的推理挑战提供了一个强大的解决方案。"}}
{"id": "2507.14497", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14497", "abs": "https://arxiv.org/abs/2507.14497", "authors": ["Weimin Lyu", "Qingqiao Hu", "Kehan Qi", "Zhan Shi", "Wentao Huang", "Saumya Gupta", "Chao Chen"], "title": "Efficient Whole Slide Pathology VQA via Token Compression", "comment": null, "summary": "Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000\npixels, posing significant challenges for multimodal large language model\n(MLLM) due to long context length and high computational demands. Previous\nmethods typically focus on patch-level analysis or slide-level classification\nusing CLIP-based models with multi-instance learning, but they lack the\ngenerative capabilities needed for visual question answering (VQA). More recent\nMLLM-based approaches address VQA by feeding thousands of patch tokens directly\ninto the language model, which leads to excessive resource consumption. To\naddress these limitations, we propose Token Compression Pathology LLaVA\n(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token\ncompression. TCP-LLaVA introduces a set of trainable compression tokens that\naggregate visual and textual information through a modality compression module,\ninspired by the [CLS] token mechanism in BERT. Only the compressed tokens are\nforwarded to the LLM for answer generation, significantly reducing input length\nand computational cost. Experiments on ten TCGA tumor subtypes show that\nTCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing\ntraining resource consumption by a substantial margin.", "AI": {"tldr": "本文提出TCP-LLaVA，首个通过令牌压缩实现全玻片图像(WSI)视觉问答(VQA)的多模态大语言模型(MLLM)架构，有效解决了长上下文和高计算成本问题。", "motivation": "病理学WSI图像巨大，导致现有MLLM在处理时面临长上下文和高计算需求挑战。现有方法或缺乏生成能力（如基于CLIP的模型），或因直接输入大量图像块令牌而消耗过多资源。", "method": "TCP-LLaVA引入一组可训练的压缩令牌，通过模态压缩模块（受BERT的[CLS]令牌机制启发）聚合视觉和文本信息。只有这些压缩后的令牌被送入LLM进行答案生成，从而显著减少输入长度和计算成本。", "result": "在十种TCGA肿瘤亚型上的实验表明，TCP-LLaVA在VQA准确性方面优于现有MLLM基线，同时大幅降低了训练资源消耗。", "conclusion": "TCP-LLaVA通过创新的令牌压缩机制，成功地在WSI VQA任务上实现了更高的准确性，并显著提升了计算效率，为病理学图像分析提供了高效的MLLM解决方案。"}}
{"id": "2507.14660", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14660", "abs": "https://arxiv.org/abs/2507.14660", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "AI": {"tldr": "本文模拟了恶意多智能体系统（MAS）的串通风险，发现去中心化系统在执行恶意行为方面比中心化系统更有效，且能规避传统干预措施。", "motivation": "大规模事件中人类群体协调造成的危害巨大，随着自主AI系统的兴起，人们担忧AI驱动的群体也可能造成类似危害。目前大多数AI安全研究关注个体AI系统，而多智能体系统在复杂现实世界中的风险仍未被充分探索。", "method": "研究引入了一个概念验证框架，用于模拟恶意多智能体串通风险，该框架支持中心化和去中心化协调结构。文章将此框架应用于虚假信息传播和电子商务欺诈两个高风险领域进行仿真。", "result": "研究发现，去中心化系统在执行恶意行为方面比中心化系统更有效。去中心化系统更高的自主性使其能够调整策略并造成更大损害。即使应用了内容标记等传统干预措施，去中心化群体也能调整策略以逃避检测。", "conclusion": "研究揭示了恶意多智能体群体的运作方式，并强调了开发更优检测系统和对抗措施的必要性。"}}
{"id": "2507.15524", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15524", "abs": "https://arxiv.org/abs/2507.15524", "authors": ["Simon Winther Albertsen", "Hjalte Svaneborg Bjørnstrup", "Mostafa Mehdipour Ghazi"], "title": "RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation", "comment": "EMA4MICCAI 2025", "summary": "Accurate segmentation is crucial for clinical applications, but existing\nmodels often assume fixed, high-resolution inputs and degrade significantly\nwhen faced with lower-resolution data in real-world scenarios. To address this\nlimitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation\narchitecture that dynamically adapts its inference path to the spatial\nresolution of the input. Central to our design are multi-scale blocks\nintegrated at multiple encoder depths, a resolution-aware routing mechanism,\nand consistency-driven training that aligns multi-resolution features with\nfull-resolution representations. We evaluate RARE-UNet on two benchmark brain\nimaging tasks for hippocampus and tumor segmentation. Compared to standard\nUNet, its multi-resolution augmented variant, and nnUNet, our model achieves\nthe highest average Dice scores of 0.84 and 0.65 across resolution, while\nmaintaining consistent performance and significantly reduced inference time at\nlower resolutions. These results highlight the effectiveness and scalability of\nour architecture in achieving resolution-robust segmentation. The codes are\navailable at: https://github.com/simonsejse/RARE-UNet.", "AI": {"tldr": "RARE-UNet是一种分辨率感知的多尺度分割架构，能动态适应不同分辨率输入，解决了现有模型在低分辨率数据上性能下降的问题，并在脑部图像分割任务中表现出更高的准确性和更快的推理速度。", "motivation": "现有医学图像分割模型通常假定高分辨率输入，在面对真实世界中常见的低分辨率数据时性能显著下降，这限制了它们在临床应用中的实用性。", "method": "本文提出了RARE-UNet，一种分辨率感知的多尺度分割架构。其核心设计包括在多个编码器深度集成的多尺度模块、分辨率感知路由机制，以及旨在使多分辨率特征与全分辨率表示对齐的一致性驱动训练。", "result": "RARE-UNet在海马体和肿瘤分割两项基准脑部成像任务上进行了评估。与标准UNet、其多分辨率增强变体和nnUNet相比，RARE-UNet在不同分辨率下实现了最高的平均Dice分数（0.84和0.65），同时保持了稳定一致的性能，并在较低分辨率下显著减少了推理时间。", "conclusion": "RARE-UNet的实验结果证明了其在实现分辨率鲁棒性分割方面的有效性和可扩展性，使其更适用于真实的临床应用场景。"}}
{"id": "2507.15088", "categories": ["cs.RO", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.15088", "abs": "https://arxiv.org/abs/2507.15088", "authors": ["Pouya Panahandeh", "Mohammad Pirani", "Baris Fidan", "Amir Khajepour"], "title": "Search-Based Autonomous Vehicle Motion Planning Using Game Theory", "comment": null, "summary": "In this paper, we propose a search-based interactive motion planning scheme\nfor autonomous vehicles (AVs), using a game-theoretic approach. In contrast to\ntraditional search-based approaches, the newly developed approach considers\nother road users (e.g. drivers and pedestrians) as intelligent agents rather\nthan static obstacles. This leads to the generation of a more realistic path\nfor the AV. Due to the low computational time, the proposed motion planning\nscheme is implementable in real-time applications. The performance of the\ndeveloped motion planning scheme is compared with existing motion planning\ntechniques and validated through experiments using WATonoBus, an electrical\nall-weather autonomous shuttle bus.", "AI": {"tldr": "本文提出了一种基于博弈论的搜索式交互运动规划方案，用于自动驾驶车辆，能够实时生成更真实的路径。", "motivation": "传统的搜索式运动规划方法将其他道路使用者视为静态障碍物，导致生成的路径不切实际。研究旨在开发一种能将其他道路使用者视为智能代理，并能实时生成更真实路径的运动规划方案。", "method": "提出了一种基于博弈论的搜索式交互运动规划方案。该方案将其他道路使用者（如驾驶员和行人）视为智能代理。通过与现有运动规划技术进行比较，并使用WATonoBus（一种全天候电动自动驾驶穿梭巴士）进行实验验证其性能。", "result": "所开发的运动规划方案能够生成更真实的自动驾驶车辆路径。计算时间短，可在实时应用中实现。其性能已通过实验验证。", "conclusion": "所提出的基于博弈论的搜索式交互运动规划方案，通过将其他道路使用者视为智能代理，能够为自动驾驶车辆生成更真实且计算效率高的路径，适用于实时应用。"}}
{"id": "2507.15163", "categories": ["eess.SY", "cs.CR", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15163", "abs": "https://arxiv.org/abs/2507.15163", "authors": ["Kim Hammar", "Yuchao Li", "Tansu Alpcan", "Emil C. Lupu", "Dimitri Bertsekas"], "title": "Adaptive Network Security Policies via Belief Aggregation and Rollout", "comment": null, "summary": "Evolving security vulnerabilities and shifting operational conditions require\nfrequent updates to network security policies. These updates include\nadjustments to incident response procedures and modifications to access\ncontrols, among others. Reinforcement learning methods have been proposed for\nautomating such policy adaptations, but most of the methods in the research\nliterature lack performance guarantees and adapt slowly to changes. In this\npaper, we address these limitations and present a method for computing security\npolicies that is scalable, offers theoretical guarantees, and adapts quickly to\nchanges. It assumes a model or simulator of the system and comprises three\ncomponents: belief estimation through particle filtering, offline policy\ncomputation through aggregation, and online policy adaptation through rollout.\nCentral to our method is a new feature-based aggregation technique, which\nimproves scalability and flexibility. We analyze the approximation error of\naggregation and show that rollout efficiently adapts policies to changes under\ncertain conditions. Simulations and testbed results demonstrate that our method\noutperforms state-of-the-art methods on several benchmarks, including CAGE-2.", "AI": {"tldr": "本文提出了一种可扩展、具备理论保证且能快速适应变化的自动化网络安全策略更新方法，解决了现有强化学习方法适应慢且缺乏性能保证的问题。", "motivation": "网络安全漏洞和操作条件不断变化，导致网络安全策略（如事件响应和访问控制）需要频繁更新。现有的自动化策略适应的强化学习方法通常缺乏性能保证且适应速度慢。", "method": "该方法假设存在系统模型或模拟器，并包含三个核心组件：通过粒子滤波进行信念估计、通过聚合（特别是新的基于特征的聚合技术）进行离线策略计算，以及通过rollout进行在线策略适应。文中还分析了聚合的近似误差，并证明了rollout在特定条件下能有效适应策略变化。", "result": "所提出的方法具有可扩展性、提供理论保证并能快速适应变化。新的基于特征的聚合技术提高了可扩展性和灵活性。模拟和测试台结果表明，该方法在包括CAGE-2在内的多个基准测试中优于现有最先进的方法。", "conclusion": "该论文成功地解决了现有自动化网络安全策略适应方法（基于强化学习）的局限性，提出了一种高性能、可扩展且具备理论保证的解决方案，并在实验中验证了其优越性。"}}
{"id": "2507.14578", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14578", "abs": "https://arxiv.org/abs/2507.14578", "authors": ["Sachin Yadav", "Dominik Schlechtweg"], "title": "XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification", "comment": "8 pages", "summary": "We propose XL-DURel, a finetuned, multilingual Sentence Transformer model\noptimized for ordinal Word-in-Context classification. We test several loss\nfunctions for regression and ranking tasks managing to outperform previous\nmodels on ordinal and binary data with a ranking objective based on angular\ndistance in complex space. We further show that binary WiC can be treated as a\nspecial case of ordinal WiC and that optimizing models for the general ordinal\ntask improves performance on the more specific binary task. This paves the way\nfor a unified treatment of WiC modeling across different task formulations.", "AI": {"tldr": "本文提出了XL-DURel，一个多语言Sentence Transformer模型，针对序数词汇语境分类进行了优化，并通过基于复数空间角距离的排序目标，在序数和二元数据上均超越了现有模型。", "motivation": "研究旨在改进词汇语境（WiC）分类任务，特别是处理序数数据，并探索统一WiC建模方法，以应对不同的任务表述。", "method": "提出了XL-DURel，一个经过微调的多语言Sentence Transformer模型。该模型针对序数词汇语境分类进行了优化，并测试了多种用于回归和排序任务的损失函数，其中基于复数空间角距离的排序目标表现最佳。", "result": "XL-DURel在序数和二元数据上均超越了之前的模型。研究还表明，二元WiC可以被视为序数WiC的一个特例，并且针对通用序数任务优化模型可以提升在特定二元任务上的性能。", "conclusion": "序数词汇语境分类可以作为二元词汇语境分类的更一般形式，对序数任务的优化有助于提升二元任务性能，这为统一处理不同任务表述下的WiC建模奠定了基础。"}}
{"id": "2507.14500", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14500", "abs": "https://arxiv.org/abs/2507.14500", "authors": ["Zhiyuan Hua", "Dehao Yuan", "Cornelia Fermüller"], "title": "Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow", "comment": null, "summary": "This paper introduces a robust framework for motion segmentation and\negomotion estimation using event-based normal flow, tailored specifically for\nneuromorphic vision sensors. In contrast to traditional methods that rely\nheavily on optical flow or explicit depth estimation, our approach exploits the\nsparse, high-temporal-resolution event data and incorporates geometric\nconstraints between normal flow, scene structure, and inertial measurements.\nThe proposed optimization-based pipeline iteratively performs event\nover-segmentation, isolates independently moving objects via residual analysis,\nand refines segmentations using hierarchical clustering informed by motion\nsimilarity and temporal consistency. Experimental results on the EVIMO2v2\ndataset validate that our method achieves accurate segmentation and\ntranslational motion estimation without requiring full optical flow\ncomputation. This approach demonstrates significant advantages at object\nboundaries and offers considerable potential for scalable, real-time robotic\nand navigation applications.", "AI": {"tldr": "本文提出了一种基于事件流的鲁棒运动分割和自我运动估计框架，专为神经形态视觉传感器设计，无需完整光流计算即可实现精确分割和运动估计。", "motivation": "传统方法严重依赖光流或显式深度估计，而本文旨在利用神经形态视觉传感器产生的稀疏、高时间分辨率的事件数据，结合事件法线流和几何约束，解决运动分割和自我运动估计问题。", "method": "该方法利用事件法线流，并结合法线流、场景结构和惯性测量之间的几何约束。提出的优化管道迭代执行事件过分割、通过残差分析隔离独立运动物体，并利用运动相似性和时间一致性进行分层聚类以细化分割。", "result": "在EVIMO2v2数据集上的实验结果表明，该方法无需完整光流计算即可实现准确的分割和平移运动估计。该方法在物体边界表现出显著优势。", "conclusion": "该方法为运动分割和自我运动估计提供了一个鲁棒的框架，特别适用于神经形态视觉传感器，并为可扩展、实时机器人和导航应用提供了巨大潜力。"}}
{"id": "2507.14705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14705", "abs": "https://arxiv.org/abs/2507.14705", "authors": ["Sai Wang", "Senthilnathan Subramanian", "Mudit Sahni", "Praneeth Gone", "Lingjie Meng", "Xiaochen Wang", "Nicolas Ferradas Bertoli", "Tingxian Cheng", "Jun Xu"], "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents", "comment": null, "summary": "Large-language-model (LLM) agents exhibit complex, context-sensitive\nbehaviour that quickly renders static benchmarks and ad-hoc manual testing\nobsolete.\n  We present Neo, a configurable, multi-agent framework that automates\nrealistic, multi-turn evaluation of LLM-based systems. Neo couples a Question\nGeneration Agent and an Evaluation Agent through a shared context-hub, allowing\ndomain prompts, scenario controls and dynamic feedback to be composed\nmodularly. Test inputs are sampled from a probabilistic state model spanning\ndialogue flow, user intent and emotional tone, enabling diverse, human-like\nconversations that adapt after every turn.\n  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)\nuncovered edge-case failures across five attack categories with a 3.3% break\nrate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered\n10-12X higher throughput, generating 180 coherent test questions in around 45\nmins versus 16h of human effort. Beyond security probing, Neo's stochastic\npolicies balanced topic coverage and conversational depth, yielding broader\nbehavioural exploration than manually crafted scripts.\n  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent\ninterfaces, state controller and feedback loops are model-agnostic and\nextensible to richer factual-grounding and policy-compliance checks. We release\nthe framework to facilitate reproducible, high-fidelity testing of emerging\nagentic systems.", "AI": {"tldr": "Neo是一个可配置的多智能体框架，用于自动化LLM系统的多轮评估。它能高效发现边缘案例故障，并提供比人工测试更广的行为探索。", "motivation": "LLM智能体展现出复杂且上下文敏感的行为，使得静态基准和临时手动测试迅速过时。", "method": "Neo框架通过共享上下文中心连接了一个问题生成智能体和一个评估智能体。测试输入从一个概率状态模型中采样，该模型涵盖对话流、用户意图和情感语调，从而生成多样化、类人的自适应对话。", "result": "应用于生产级聊天机器人时，Neo在五种攻击类别中发现边缘案例故障的破坏率为3.3%，接近专家人工红队人员的5.8%。同时，其吞吐量提高了10-12倍，在45分钟内生成180个测试问题，相当于16小时的人工工作量。Neo的随机策略也实现了比手动脚本更广泛的主题覆盖和对话深度。", "conclusion": "Neo为可扩展、自进化的LLM QA奠定了基础，其智能体接口、状态控制器和反馈循环是模型无关且可扩展的，能支持更丰富的事实核查和策略合规性检查。该框架已发布，以促进新兴智能体系统的高保真、可复现测试。"}}
{"id": "2507.14851", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.14851", "abs": "https://arxiv.org/abs/2507.14851", "authors": ["Muhammad Kamran Janjua", "Amirhosein Ghasemabadi", "Kunlin Zhang", "Mohammad Salameh", "Chao Gao", "Di Niu"], "title": "Grounding Degradations in Natural Language for All-In-One Video Restoration", "comment": "17 pages", "summary": "In this work, we propose an all-in-one video restoration framework that\ngrounds degradation-aware semantic context of video frames in natural language\nvia foundation models, offering interpretable and flexible guidance. Unlike\nprior art, our method assumes no degradation knowledge in train or test time\nand learns an approximation to the grounded knowledge such that the foundation\nmodel can be safely disentangled during inference adding no extra cost.\nFurther, we call for standardization of benchmarks in all-in-one video\nrestoration, and propose two benchmarks in multi-degradation setting,\nthree-task (3D) and four-task (4D), and two time-varying composite degradation\nbenchmarks; one of the latter being our proposed dataset with varying snow\nintensity, simulating how weather degradations affect videos naturally. We\ncompare our method with prior works and report state-of-the-art performance on\nall benchmarks.", "AI": {"tldr": "本文提出一个一体化视频修复框架，利用基础模型将视频帧的退化感知语义上下文与自然语言结合，提供可解释且灵活的指导。该方法无需预先的退化知识，并在推理时安全地解耦基础模型，同时提出了新的多退化和时变复合退化基准，并取得了最先进的性能。", "motivation": "现有方法在训练或测试时需要退化知识，缺乏灵活性和可解释性。此外，一体化视频修复领域缺乏标准化的基准测试。", "method": "1. 提出一体化视频修复框架，通过基础模型将视频帧的退化感知语义上下文与自然语言关联，实现可解释和灵活的指导。2. 该方法无需预先的退化知识，通过学习近似的接地知识，使得基础模型在推理时可安全解耦，不增加额外成本。3. 提出并呼吁一体化视频修复的基准标准化，并提出两个多退化基准（3D和4D），以及两个时变复合退化基准，其中一个包含作者提出的模拟自然天气退化（如不同雪强度）的数据集。", "result": "所提出的方法在所有基准测试上均取得了最先进的性能。", "conclusion": "本文提出的一体化视频修复框架通过利用基础模型和自然语言指导，实现了无需预先退化知识的高性能修复，并在推理时保持高效。同时，本文为一体化视频修复领域贡献了标准化的新基准，推动了该领域的发展。"}}
{"id": "2507.15155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15155", "abs": "https://arxiv.org/abs/2507.15155", "authors": ["Majid Roshanfar", "Alex Zhang", "Changyan He", "Amir Hooshiar", "Dale J. Podolsky", "Thomas Looi", "Eric Diller"], "title": "Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions", "comment": null, "summary": "This letter introduces a novel learning-based modeling framework for a\nmagnetically steerable soft suction device designed for endoscopic endonasal\nbrain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm\ninner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,\nand integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape\nfeedback. Shape reconstruction is represented using four Bezier control points,\nenabling a compact and smooth model of the device's deformation. A data-driven\nmodel was trained on 5,097 experimental samples covering a range of magnetic\nfield magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical\ntip distances (90-100 mm), using both Neural Network (NN) and Random Forest\n(RF) architectures. The RF model outperformed the NN across all metrics,\nachieving a mean root mean square error of 0.087 mm in control point prediction\nand a mean shape reconstruction error of 0.064 mm. Feature importance analysis\nfurther revealed that magnetic field components predominantly influence distal\ncontrol points, while frequency and distance affect the base configuration.\nThis learning-based approach effectively models the complex nonlinear behavior\nof hyperelastic soft robots under magnetic actuation without relying on\nsimplified physical assumptions. By enabling sub-millimeter shape prediction\naccuracy and real-time inference, this work represents an advancement toward\nthe intelligent control of magnetically actuated soft robotic tools in\nminimally invasive neurosurgery.", "AI": {"tldr": "本文提出了一种基于学习的建模框架，用于磁驱动软吸盘装置，通过数据驱动模型实现亚毫米级的形状预测精度，以支持微创神经外科手术。", "motivation": "为了在内窥镜下经鼻脑肿瘤切除术中实现对磁驱动软机器人工具的智能控制，需要准确建模其复杂的非线性行为，且不依赖简化的物理假设。", "method": "该研究开发了一个微型（4毫米外径，40毫米长）、3D打印（SIL 30生物相容材料）的软吸盘装置，并集成了FBG传感器以提供实时形状反馈。设备形状通过四个贝塞尔控制点表示。研究人员收集了5,097个实验样本（涵盖磁场强度、驱动频率和垂直尖端距离），并使用神经网络（NN）和随机森林（RF）两种架构训练了数据驱动模型。最后进行了特征重要性分析。", "result": "随机森林（RF）模型在所有指标上均优于神经网络（NN），在控制点预测方面实现了0.087毫米的平均均方根误差，在形状重建方面实现了0.064毫米的平均误差。特征重要性分析表明，磁场分量主要影响远端控制点，而频率和距离则影响基部配置。", "conclusion": "该基于学习的方法有效地建模了磁驱动超弹性软机器人的复杂非线性行为，而无需依赖简化的物理假设。通过实现亚毫米级的形状预测精度和实时推理，这项工作为微创神经外科中磁驱动软机器人工具的智能控制迈出了重要一步。"}}
{"id": "2507.15167", "categories": ["eess.SY", "cond-mat.mtrl-sci", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15167", "abs": "https://arxiv.org/abs/2507.15167", "authors": ["Nan An", "Mingtong Chen", "Zhengbao Yang"], "title": "A New Ultrafast Printer for Large-Scale Assembly of Piezoelectric Biomaterials", "comment": null, "summary": "We propose a modular, fast and large-area fabrication of bio-piezoelectric\nfilms. The technique is based on the formation of cone-jet mode by applying a\nhigh voltage electric field to conductive spiked metal disks. And the\nself-assembly process of biomolecular materials through nanoconfinement with\nin-situ poling effect. This job achieved print speeds of up to 9.2 109 um3/s\nwith a combination of only 2 printheads. At the same time, the modular design\nallows the MLSP to achieve theoretically unlimited print efficiency. It also\nprovides flexible configuration options for different printing needs, such as\npreparing films of different areas and shapes. In short, MLSP demonstrates the\nability of piezoelectric biomaterials to undergo ultra-fast, large-scale\nassembly. Demonstrates good potential as a universally applicable bio-device\nfor the fabrication of bio-piezoelectric films", "AI": {"tldr": "本文提出了一种模块化、快速、大面积制备生物压电薄膜的新技术（MLSP），利用锥形射流模式和生物分子自组装，实现了超高速、高效率的生物压电材料组装。", "motivation": "研究旨在解决生物压电薄膜制备中速度、面积和效率的限制，寻求一种可大规模、快速且灵活配置的制备方法，以满足生物设备领域的应用需求。", "method": "该技术基于高压电场作用于导电尖刺金属盘形成锥形射流模式，并结合生物分子材料通过纳米限域进行自组装，同时实现原位极化效应。其采用模块化设计，可组合多个打印头以提高效率。", "result": "使用两个打印头实现了高达9.2 x 10^9 um³/s的打印速度。模块化设计理论上可实现无限的打印效率，并提供灵活的配置选项，以制备不同面积和形状的薄膜。", "conclusion": "MLSP技术展示了生物压电材料超快、大规模组装的能力，在生物压电薄膜的制备方面具有巨大潜力，有望成为一种普遍适用的生物设备制造方法。"}}
{"id": "2507.14579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14579", "abs": "https://arxiv.org/abs/2507.14579", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models", "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages", "summary": "Detecting collaborative problem solving (CPS) indicators from dialogue using\nmachine learning techniques is a significant challenge for the field of AI in\nEducation. Recent studies have explored the use of Bidirectional Encoder\nRepresentations from Transformers (BERT) models on transcription data to\nreliably detect meaningful CPS indicators. A notable advancement involved the\nmultimodal BERT variant, AudiBERT, which integrates speech and\nacoustic-prosodic audio features to enhance CPS diagnosis. Although initial\nresults demonstrated multimodal improvements, the statistical significance of\nthese enhancements remained unclear, and there was insufficient guidance on\nleveraging human-AI complementarity for CPS diagnosis tasks. This workshop\npaper extends the previous research by highlighting that the AudiBERT model not\nonly improved the classification of classes that were sparse in the dataset,\nbut it also had statistically significant class-wise improvements over the BERT\nmodel for classifications in the social-cognitive dimension. However, similar\nsignificant class-wise improvements over the BERT model were not observed for\nclassifications in the affective dimension. A correlation analysis highlighted\nthat larger training data was significantly associated with higher recall\nperformance for both the AudiBERT and BERT models. Additionally, the precision\nof the BERT model was significantly associated with high inter-rater agreement\namong human coders. When employing the BERT model to diagnose indicators within\nthese subskills that were well-detected by the AudiBERT model, the performance\nacross all indicators was inconsistent. We conclude the paper by outlining a\nstructured approach towards achieving human-AI complementarity for CPS\ndiagnosis, highlighting the crucial inclusion of model explainability to\nsupport human agency and engagement in the reflective coding process.", "AI": {"tldr": "本研究发现，多模态AudiBERT模型在协作问题解决（CPS）指标检测中，相较于BERT模型在社交认知维度上表现出统计学显著的改进，并提出了实现人机互补性的方法。", "motivation": "尽管多模态BERT变体AudiBERT在CPS诊断中显示出改进，但其增强的统计学显著性尚不明确，且缺乏关于如何利用人机互补性进行CPS诊断任务的指导。", "method": "本研究扩展了先前的研究，比较了多模态AudiBERT模型（整合语音和声学-韵律特征）与BERT模型在CPS指标分类上的性能。通过统计学分析评估了类级别改进，并进行了相关性分析以探究训练数据量和人类标注一致性对模型性能的影响。", "result": "1. AudiBERT模型不仅改进了数据集中稀疏类别的分类，还在社交认知维度上的分类表现出对BERT模型的统计学显著的类级别改进。2. 在情感维度上，AudiBERT模型相对于BERT模型没有观察到类似的显著类级别改进。3. 更大的训练数据与AudiBERT和BERT模型的更高召回性能显著相关。4. BERT模型的精确度与人类编码者之间高的一致性显著相关。5. 当使用BERT模型诊断AudiBERT模型能很好检测的子技能指标时，所有指标的性能表现不一致。", "conclusion": "论文提出了实现CPS诊断中人机互补性的结构化方法，强调了模型可解释性在支持人类能动性和参与反思性编码过程中的关键作用。"}}
{"id": "2507.14501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14501", "abs": "https://arxiv.org/abs/2507.14501", "authors": ["Jiahui Zhang", "Yuelei Li", "Anpei Chen", "Muyu Xu", "Kunhao Liu", "Jianyuan Wang", "Xiao-Xiao Long", "Hanxue Liang", "Zexiang Xu", "Hao Su", "Christian Theobalt", "Christian Rupprecht", "Andrea Vedaldi", "Hanspeter Pfister", "Shijian Lu", "Fangneng Zhan"], "title": "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey", "comment": "A project page associated with this survey is available at\n  https://fnzhan.com/projects/Feed-Forward-3D", "summary": "3D reconstruction and view synthesis are foundational problems in computer\nvision, graphics, and immersive technologies such as augmented reality (AR),\nvirtual reality (VR), and digital twins. Traditional methods rely on\ncomputationally intensive iterative optimization in a complex chain, limiting\ntheir applicability in real-world scenarios. Recent advances in feed-forward\napproaches, driven by deep learning, have revolutionized this field by enabling\nfast and generalizable 3D reconstruction and view synthesis. This survey offers\na comprehensive review of feed-forward techniques for 3D reconstruction and\nview synthesis, with a taxonomy according to the underlying representation\narchitectures including point cloud, 3D Gaussian Splatting (3DGS), Neural\nRadiance Fields (NeRF), etc. We examine key tasks such as pose-free\nreconstruction, dynamic 3D reconstruction, and 3D-aware image and video\nsynthesis, highlighting their applications in digital humans, SLAM, robotics,\nand beyond. In addition, we review commonly used datasets with detailed\nstatistics, along with evaluation protocols for various downstream tasks. We\nconclude by discussing open research challenges and promising directions for\nfuture work, emphasizing the potential of feed-forward approaches to advance\nthe state of the art in 3D vision.", "AI": {"tldr": "本文综述了基于深度学习的前馈方法在三维重建和视角合成领域的最新进展，并根据底层表示架构（如点云、3DGS、NeRF）进行了分类，探讨了关键任务、应用、数据集和评估协议，并展望了未来挑战和方向。", "motivation": "传统的三维重建和视角合成方法依赖计算密集型迭代优化，限制了其在实际场景中的应用。深度学习驱动的前馈方法为快速、可泛化的三维重建和视角合成带来了革命性突破。", "method": "本文采用综述形式，对前馈三维重建和视角合成技术进行了全面回顾。方法包括：根据底层表示架构（如点云、3D Gaussian Splatting、Neural Radiance Fields）进行分类；审查无姿态重建、动态三维重建、三维感知图像和视频合成等关键任务；分析数字人、SLAM、机器人等应用；回顾常用数据集及其统计信息；讨论各种下游任务的评估协议。", "result": "本文提供了一个关于前馈三维重建和视角合成技术的综合性综述，构建了基于表示架构的分类体系，阐明了其在多个关键任务和应用中的潜力，并详细介绍了相关数据集和评估标准。", "conclusion": "前馈方法在推进三维视觉领域具有巨大潜力。本文讨论了当前的研究挑战和未来有前景的方向，强调了这些方法在提升三维视觉技术水平方面的作用。"}}
{"id": "2507.14719", "categories": ["cs.AI", "I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2507.14719", "abs": "https://arxiv.org/abs/2507.14719", "authors": ["Juan Manuel Contreras"], "title": "Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix", "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into\nreal-world applications, scalable and rigorous safety evaluation is essential.\nThis paper introduces Aymara AI, a programmatic platform for generating and\nadministering customized, policy-grounded safety evaluations. Aymara AI\ntransforms natural-language safety policies into adversarial prompts and scores\nmodel responses using an AI-based rater validated against human judgments. We\ndemonstrate its capabilities through the Aymara LLM Risk and Responsibility\nMatrix, which evaluates 20 commercially available LLMs across 10 real-world\nsafety domains. Results reveal wide performance disparities, with mean safety\nscores ranging from 86.2% to 52.4%. While models performed well in\nwell-established safety domains such as Misinformation (mean = 95.7%), they\nconsistently failed in more complex or underspecified domains, notably Privacy\n& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety\nscores differed significantly across both models and domains (p < .05). These\nfindings underscore the inconsistent and context-dependent nature of LLM safety\nand highlight the need for scalable, customizable tools like Aymara AI to\nsupport responsible AI development and oversight.", "AI": {"tldr": "Aymara AI是一个可编程平台，用于生成和管理基于策略的LLM安全评估，揭示了不同模型和领域间LLM安全性能的巨大差异，并强调了对可扩展评估工具的需求。", "motivation": "随着大型语言模型（LLMs）日益融入实际应用，可扩展且严谨的安全评估变得至关重要。", "method": "引入了Aymara AI平台，它将自然语言安全策略转换为对抗性提示，并使用经过人类判断验证的AI评分器来评估模型响应。通过Aymara LLM风险与责任矩阵，评估了20个商业LLM在10个真实世界安全领域的表现。", "result": "LLM的安全得分差异巨大（平均得分从86.2%到52.4%）。模型在成熟的安全领域（如虚假信息，平均95.7%）表现良好，但在更复杂或不明确的领域（如隐私与冒充，平均24.3%）表现不佳。方差分析证实，安全得分在模型和领域之间存在显著差异（p < .05）。", "conclusion": "LLM的安全性是不一致且依赖于上下文的。需要像Aymara AI这样可扩展、可定制的工具来支持负责任的AI开发和监督。"}}
{"id": "2507.15578", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.15578", "abs": "https://arxiv.org/abs/2507.15578", "authors": ["Gabriele Inzerillo", "Diego Valsesia", "Aniello Fiengo", "Enrico Magli"], "title": "Compress-Align-Detect: onboard change detection from unregistered images", "comment": null, "summary": "Change detection from satellite images typically incurs a delay ranging from\nseveral hours up to days because of latency in downlinking the acquired images\nand generating orthorectified image products at the ground stations; this may\npreclude real- or near real-time applications. To overcome this limitation, we\npropose shifting the entire change detection workflow onboard satellites. This\nrequires to simultaneously solve challenges in data storage, image registration\nand change detection with a strict complexity constraint. In this paper, we\npresent a novel and efficient framework for onboard change detection that\naddresses the aforementioned challenges in an end-to-end fashion with a deep\nneural network composed of three interlinked submodules: (1) image compression,\ntailored to minimize onboard data storage resources; (2) lightweight\nco-registration of non-orthorectified multi-temporal image pairs; and (3) a\nnovel temporally-invariant and computationally efficient change detection\nmodel. This is the first approach in the literature combining all these tasks\nin a single end-to-end framework with the constraints dictated by onboard\nprocessing. Experimental results compare each submodule with the current\nstate-of-the-art, and evaluate the performance of the overall integrated system\nin realistic setting on low-power hardware. Compelling change detection results\nare obtained in terms of F1 score as a function of compression rate, sustaining\na throughput of 0.7 Mpixel/s on a 15W accelerator.", "AI": {"tldr": "该研究提出一种新颖高效的深度学习框架，用于在卫星上直接进行变化检测，以克服传统地面处理带来的延迟，实现实时应用。", "motivation": "传统的卫星图像变化检测因图像下行和地面站正射校正处理存在数小时甚至数天的延迟，这使得实时或近实时应用无法实现。", "method": "研究提出将整个变化检测流程转移到卫星上执行，并为此设计了一个端到端的深度神经网络框架。该框架包含三个相互关联的子模块：1) 图像压缩（最小化星载数据存储）；2) 轻量级非正射多时相图像对的协同配准；3) 一种新颖的、时间不变且计算高效的变化检测模型。该方法是首个将这些任务整合到单一端到端框架中，并满足星载处理严格复杂性限制的方案。", "result": "实验结果将每个子模块与当前最先进的技术进行了比较，并在低功耗硬件的现实环境中评估了整体集成系统的性能。在压缩率的函数下，F1分数取得了令人信服的变化检测结果，并在15W加速器上保持了0.7 Mpixel/s的吞吐量。", "conclusion": "该研究提出的星载变化检测框架成功解决了实时卫星变化检测的挑战，通过集成图像压缩、轻量级配准和高效变化检测模型，在满足星载处理约束的同时，展现出良好的性能和实时处理能力。"}}
{"id": "2507.15189", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15189", "abs": "https://arxiv.org/abs/2507.15189", "authors": ["Kevin Christiansen Marsim", "Jinwoo Jeon", "Yeeun Kim", "Myeongwoo Jeong", "Hyun Myung"], "title": "CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer", "comment": null, "summary": "Depth information which specifies the distance between objects and current\nposition of the robot is essential for many robot tasks such as navigation.\nRecently, researchers have proposed depth completion frameworks to provide\ndense depth maps that offer comprehensive information about the surrounding\nenvironment. However, existing methods show significant trade-offs between\ncomputational efficiency and accuracy during inference. The substantial memory\nand computational requirements make them unsuitable for real-time applications,\nhighlighting the need to improve the completeness and accuracy of depth\ninformation while improving processing speed to enhance robot performance in\nvarious tasks. To address these challenges, in this paper, we propose\nCHADET(cross-hierarchical-attention depth-completion transformer), a\nlightweight depth-completion network that can generate accurate dense depth\nmaps from RGB images and sparse depth points. For each pair, its feature is\nextracted from the depthwise blocks and passed to the equally lightweight\ntransformer-based decoder. In the decoder, we utilize the novel\ncross-hierarchical-attention module that refines the image features from the\ndepth information. Our approach improves the quality and reduces memory usage\nof the depth map prediction, as validated in both KITTI, NYUv2, and VOID\ndatasets.", "AI": {"tldr": "本文提出CHADET，一种轻量级深度补全网络，能从RGB图像和稀疏深度点生成准确的稠密深度图，同时兼顾计算效率和精度。", "motivation": "机器人导航等任务需要精确的深度信息，但现有深度补全方法在计算效率和准确性之间存在显著权衡，内存和计算需求高，不适用于实时应用。因此，需要提高深度信息的完整性和准确性，同时提升处理速度。", "method": "提出CHADET（cross-hierarchical-attention depth-completion transformer），一个轻量级深度补全网络。它通过深度块提取RGB图像和稀疏深度点的特征，然后送入同样轻量级的基于Transformer的解码器。解码器中采用新颖的跨层级注意力模块，利用深度信息精炼图像特征。", "result": "该方法提高了深度图预测的质量并减少了内存使用。在KITTI、NYUv2和VOID数据集上均得到了验证。", "conclusion": "CHADET成功解决了现有深度补全方法在计算效率和准确性之间的权衡问题，能为机器人任务提供高质量的实时深度信息，提升机器人性能。"}}
{"id": "2507.15169", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15169", "abs": "https://arxiv.org/abs/2507.15169", "authors": ["Yuhan Dai", "Mingtong Chen", "Zhengbao Yang"], "title": "Energy consumption optimization and self-powered environmental monitoring design for low-carbon smart buildings", "comment": null, "summary": "Despite the growing emphasis on intelligent buildings as a cornerstone of\nsustainable urban development, significant energy inefficiencies persist due to\nsuboptimal design, material choices, and user behavior. The applicability of\nintegrated Building Information Modeling (BIM) and solarpowered environmental\nmonitoring systems for energy optimization in low-carbon smart buildings\nremains underexplored. Can BIM-driven design improvements, combined with\nphotovoltaic systems, achieve substantial energy savings while enabling\nself-powered environmental monitoring? This study conducts a case analysis on a\nretrofitted primary school building in Guangdong, China, utilizing BIM-based\nenergy simulations, material optimization, and solar technology integration.\nThe outcomes reveal that the proposed approach reduced annual energy\nconsumption by 40.68%, with lighting energy use decreasing by 36.59%. A rooftop\nphotovoltaic system demonstrated a payback period of 7.46 years while powering\nenvironmental sensors autonomously. Hardware system integrates sensors and an\nARDUINO-based controller to detect environmental factors like rainfall,\ntemperature, and air quality. It is powered by a 6W solar panel and a 2200\nmAh/7.4 V lithium battery to ensure stable operation. This study underscores\nthe potential of BIM and solar energy integration to transform traditional\nbuildings into energy-efficient, self-sustaining smart structures. Further\nresearch can expand the scalability of these methods across diverse climates\nand building typologies.", "AI": {"tldr": "本研究探讨了BIM与太阳能系统集成在低碳智能建筑中实现能源优化和自供电环境监测的潜力，通过改造一所小学，实现了显著的节能效果和可行的投资回报。", "motivation": "尽管智能建筑日益受到重视，但由于设计、材料和用户行为不当，能源效率低下问题依然存在。BIM与太阳能供电环境监测系统在低碳智能建筑中进行能源优化的适用性尚未得到充分探索。", "method": "本研究以中国广东一所改造小学为案例，采用基于BIM的能源模拟、材料优化和太阳能技术集成。同时，开发了一个硬件系统，集成了传感器和基于ARDUINO的控制器，由6W太阳能电池板和2200 mAh/7.4 V锂电池供电，用于环境监测。", "result": "该方法使年能耗降低了40.68%，其中照明能耗下降了36.59%。屋顶光伏系统投资回收期为7.46年，并能自主为环境传感器供电。硬件系统成功监测了降雨量、温度和空气质量等环境因素，并稳定运行。", "conclusion": "研究强调了BIM与太阳能集成在将传统建筑转变为节能、自给自足的智能结构方面的巨大潜力。未来的研究可将这些方法扩展到不同的气候和建筑类型。"}}
{"id": "2507.14584", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14584", "abs": "https://arxiv.org/abs/2507.14584", "authors": ["Kester Wong", "Sahan Bulathwela", "Mutlu Cukurova"], "title": "Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption", "comment": "Accepted to appear in the workshop proceedings for the HEXED'25\n  workshop in the 26th International Conference on Artificial Intelligence in\n  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 6 pages, 2 figures", "summary": "The use of Bidirectional Encoder Representations from Transformers (BERT)\nmodel and its variants for classifying collaborative problem solving (CPS) has\nbeen extensively explored within the AI in Education community. However,\nlimited attention has been given to understanding how individual tokenised\nwords in the dataset contribute to the model's classification decisions.\nEnhancing the explainability of BERT-based CPS diagnostics is essential to\nbetter inform end users such as teachers, thereby fostering greater trust and\nfacilitating wider adoption in education. This study undertook a preliminary\nstep towards model transparency and explainability by using SHapley Additive\nexPlanations (SHAP) to examine how different tokenised words in transcription\ndata contributed to a BERT model's classification of CPS processes. The\nfindings suggested that well-performing classifications did not necessarily\nequate to a reasonable explanation for the classification decisions. Particular\ntokenised words were used frequently to affect classifications. The analysis\nalso identified a spurious word, which contributed positively to the\nclassification but was not semantically meaningful to the class. While such\nmodel transparency is unlikely to be useful to an end user to improve their\npractice, it can help them not to overrely on LLM diagnostics and ignore their\nhuman expertise. We conclude the workshop paper by noting that the extent to\nwhich the model appropriately uses the tokens for its classification is\nassociated with the number of classes involved. It calls for an investigation\ninto the exploration of ensemble model architectures and the involvement of\nhuman-AI complementarity for CPS diagnosis, since considerable human reasoning\nis still required for fine-grained discrimination of CPS subskills.", "AI": {"tldr": "本研究使用SHAP解释BERT模型在协同问题解决（CPS）分类中的决策，发现高性能分类不一定有合理解释，并识别出虚假词汇，强调模型透明度有助于用户避免过度依赖AI，并呼吁未来结合人类专业知识和集成模型。", "motivation": "BERT模型及其变体在协同问题解决（CPS）分类中应用广泛，但很少有研究关注单个词汇如何影响模型的分类决策。提高BERT模型在CPS诊断中的可解释性对于教师等最终用户至关重要，有助于建立信任并促进在教育领域的广泛应用。", "method": "本研究采用SHapley Additive exPlanations (SHAP) 方法，分析转录数据中不同词汇对BERT模型CPS过程分类的贡献。", "result": "研究发现，表现良好的分类结果不一定伴随着合理的解释。特定词汇频繁地影响分类决策。分析还识别出一个虚假词汇，它对分类有积极贡献，但语义上与类别无关。这种模型透明度虽然可能无法直接帮助最终用户改进实践，但能提醒他们不要过度依赖大型语言模型诊断而忽视人类专业知识。", "conclusion": "模型恰当利用词汇进行分类的程度与涉及的类别数量相关。未来的研究应探索集成模型架构以及人机互补在CPS诊断中的应用，因为对CPS子技能进行细粒度区分仍需大量人类推理。"}}
{"id": "2507.14505", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14505", "abs": "https://arxiv.org/abs/2507.14505", "authors": ["Jiahao Ma", "Tianyu Wang", "Miaomiao Liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "DCHM: Depth-Consistent Human Modeling for Multiview Detection", "comment": "multi-view detection, sparse-view reconstruction", "summary": "Multiview pedestrian detection typically involves two stages: human modeling\nand pedestrian localization. Human modeling represents pedestrians in 3D space\nby fusing multiview information, making its quality crucial for detection\naccuracy. However, existing methods often introduce noise and have low\nprecision. While some approaches reduce noise by fitting on costly multiview 3D\nannotations, they often struggle to generalize across diverse scenes. To\neliminate reliance on human-labeled annotations and accurately model humans, we\npropose Depth-Consistent Human Modeling (DCHM), a framework designed for\nconsistent depth estimation and multiview fusion in global coordinates.\nSpecifically, our proposed pipeline with superpixel-wise Gaussian Splatting\nachieves multiview depth consistency in sparse-view, large-scaled, and crowded\nscenarios, producing precise point clouds for pedestrian localization.\nExtensive validations demonstrate that our method significantly reduces noise\nduring human modeling, outperforming previous state-of-the-art baselines.\nAdditionally, to our knowledge, DCHM is the first to reconstruct pedestrians\nand perform multiview segmentation in such a challenging setting. Code is\navailable on the \\href{https://jiahao-ma.github.io/DCHM/}{project page}.", "AI": {"tldr": "DCHM是一个用于多视角行人检测中人体建模的新框架，它利用超像素高斯溅射技术实现深度一致性，从而在无需3D标注的情况下，减少噪声并生成精确的点云。", "motivation": "现有的多视角行人检测中的人体建模方法存在噪声大、精度低的问题，并且通常依赖昂贵的3D标注，导致泛化能力差。", "method": "提出了一种深度一致人体建模（DCHM）框架，旨在实现全局坐标系下的一致深度估计和多视角融合。该方法利用超像素高斯溅射（superpixel-wise Gaussian Splatting）技术，在稀疏视角、大规模和拥挤场景下实现多视角深度一致性，从而生成精确的点云。", "result": "该方法在人体建模过程中显著降低了噪声，并超越了以往最先进的基线方法。", "conclusion": "DCHM是首个能够在稀疏视角、大规模和拥挤等挑战性场景下，在不依赖人工标注3D数据的情况下，重建行人并进行多视角分割的框架。"}}
{"id": "2507.14730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14730", "abs": "https://arxiv.org/abs/2507.14730", "authors": ["Yanjie Fu"], "title": "Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI", "comment": "4 pages; will continue to update to add more figures to describe the\n  vision;", "summary": "Generative AI, large language models, and agentic AI have emerged separately\nof urban planning. However, the convergence between AI and urban planning\npresents an interesting opportunity towards AI urban planners. This paper\nconceptualizes urban planning as a generative AI task, where AI synthesizes\nland-use configurations under geospatial, social, and human-centric\nconstraints. We survey how generative AI approaches, including VAEs, GANs,\ntransformers, and diffusion models, reshape urban design. We further identify\ncritical gaps: 1) limited research on integrating urban theory guidance, 2)\nlimited research of AI urban planning over multiple spatial resolutions or\nangularities, 3) limited research on augmenting urban design knowledge from\ndata, and 4) limited research on addressing real-world interactions. To address\nthese limitations, we outline future research directions in theory-guided\ngeneration, digital twins, and human-machine co-design, calling for a new\nsynthesis of generative intelligence and participatory urbanism.", "AI": {"tldr": "本文探讨了生成式AI在城市规划中的应用潜力，将其概念化为生成任务，并识别了现有研究的局限性，提出了未来的研究方向。", "motivation": "生成式AI、大型语言模型和智能体AI与城市规划领域一直独立发展。然而，AI与城市规划的融合为AI城市规划师带来了有趣的机会。", "method": "本文将城市规划概念化为一种生成式AI任务，即AI在地理空间、社会和以人为中心的约束下合成土地利用配置。文章调研了包括VAE、GAN、Transformer和扩散模型在内的生成式AI方法如何重塑城市设计，并识别了关键的研究空白，进而概述了未来的研究方向。", "result": "研究识别了四个主要研究空白：1) 缺乏整合城市理论指导的研究，2) 缺乏多空间分辨率或角度的AI城市规划研究，3) 缺乏从数据中增强城市设计知识的研究，4) 缺乏解决真实世界互动的研究。", "conclusion": "为了弥补现有局限性，未来研究应关注理论引导的生成、数字孪生和人机协同设计，呼吁将生成智能与参与式城市主义进行新的融合。"}}
{"id": "2507.15690", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15690", "abs": "https://arxiv.org/abs/2507.15690", "authors": ["Hung Nguyen", "Runfa Li", "An Le", "Truong Nguyen"], "title": "DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting", "comment": "6 pages, 4 figures", "summary": "Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in\nreconstructing high-quality novel views, as it often overfits to the\nwidely-varying high-frequency (HF) details of the sparse training views. While\nfrequency regularization can be a promising approach, its typical reliance on\nFourier transforms causes difficult parameter tuning and biases towards\ndetrimental HF learning. We propose DWTGS, a framework that rethinks frequency\nregularization by leveraging wavelet-space losses that provide additional\nspatial supervision. Specifically, we supervise only the low-frequency (LF) LL\nsubbands at multiple DWT levels, while enforcing sparsity on the HF HH subband\nin a self-supervised manner. Experiments across benchmarks show that DWTGS\nconsistently outperforms Fourier-based counterparts, as this LF-centric\nstrategy improves generalization and reduces HF hallucinations.", "AI": {"tldr": "针对稀疏视角3D高斯泼溅(3DGS)中高频细节过拟合的问题，本文提出DWTGS框架，利用小波域损失在低频子带进行监督并在高频子带强制稀疏性，从而优于基于傅里叶的方法，提升泛化能力并减少高频伪影。", "motivation": "稀疏视角3D高斯泼溅(3DGS)在重建高质量新颖视图时面临挑战，常因过度拟合稀疏训练视图中变化较大的高频细节而表现不佳。传统的基于傅里叶变换的频率正则化方法存在参数调优困难且偏向于有害的高频学习。", "method": "提出DWTGS框架，通过利用小波空间损失重新思考频率正则化。具体来说，它在多个离散小波变换(DWT)级别上仅监督低频(LF)LL子带，并以自监督方式在高频(HF)HH子带上强制稀疏性，从而提供额外的空间监督。", "result": "实验结果表明，DWTGS在各项基准测试中持续优于基于傅里叶变换的对应方法。", "conclusion": "这种以低频为中心的策略改善了泛化能力并减少了高频伪影，有效解决了稀疏视角3DGS的重建质量问题。"}}
{"id": "2507.15266", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15266", "abs": "https://arxiv.org/abs/2507.15266", "authors": ["Haichao Liu", "Haoren Guo", "Pei Liu", "Benshan Ma", "Yuxiang Zhang", "Jun Ma", "Tong Heng Lee"], "title": "VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving", "comment": "14 pages, 12 figures", "summary": "Scene understanding and risk-aware attentions are crucial for human drivers\nto make safe and effective driving decisions. To imitate this cognitive ability\nin urban autonomous driving while ensuring the transparency and\ninterpretability, we propose a vision-language model (VLM)-enhanced unified\ndecision-making and motion control framework, named VLM-UDMC. This framework\nincorporates scene reasoning and risk-aware insights into an upper-level slow\nsystem, which dynamically reconfigures the optimal motion planning for the\ndownstream fast system. The reconfiguration is based on real-time environmental\nchanges, which are encoded through context-aware potential functions. More\nspecifically, the upper-level slow system employs a two-step reasoning policy\nwith Retrieval-Augmented Generation (RAG), leveraging foundation models to\nprocess multimodal inputs and retrieve contextual knowledge, thereby generating\nrisk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM\nprovides real-time trajectory predictions for heterogeneous traffic\nparticipants by extracting smoother trend representations for short-horizon\ntrajectory prediction. The effectiveness of the proposed VLM-UDMC framework is\nverified via both simulations and real-world experiments with a full-size\nautonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively\nleverages scene understanding and attention decomposition for rational driving\ndecisions, thus improving the overall urban driving performance. Our\nopen-source project is available at https://github.com/henryhcliu/vlmudmc.git.", "AI": {"tldr": "VLM-UDMC是一个增强的视觉-语言模型框架，旨在通过整合场景理解和风险感知，提升城市自动驾驶的决策与运动控制能力。", "motivation": "为了模仿人类驾驶员在安全有效驾驶决策中对场景理解和风险感知注意力的关键能力，并确保透明度和可解释性，研究者提出了此框架。", "method": "VLM-UDMC框架包含一个上层慢系统和一个下层快系统。上层慢系统通过结合场景推理和风险感知洞察力，利用检索增强生成（RAG）和基础模型处理多模态输入并检索上下文知识，生成风险感知洞察，并基于实时环境变化（通过上下文感知势函数编码）动态重新配置下层快系统的最优运动规划。同时，一个轻量级多核分解LSTM提供异构交通参与者的实时轨迹预测。", "result": "VLM-UDMC框架的有效性通过仿真和全尺寸自动驾驶车辆的真实世界实验得到了验证。结果表明，该框架能有效利用场景理解和注意力分解进行理性驾驶决策，从而提高了整体城市驾驶性能。", "conclusion": "VLM-UDMC通过整合场景理解和风险感知，实现了更理性的驾驶决策，显著提升了城市自动驾驶的整体表现。"}}
{"id": "2507.15259", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15259", "abs": "https://arxiv.org/abs/2507.15259", "authors": ["Kyung-Bin Kwon", "Sayak Mukherjee", "Ramij R. Hossain", "Marcelo Elizondo"], "title": "Physics-Informed Learning of Proprietary Inverter Models for Grid Dynamic Studies", "comment": "7 pages, 5 figures", "summary": "This letter develops a novel physics-informed neural ordinary differential\nequations-based framework to emulate the proprietary dynamics of the inverters\n-- essential for improved accuracy in grid dynamic simulations. In current\nindustry practice, the original equipment manufacturers (OEMs) often do not\ndisclose the exact internal controls and parameters of the inverters, posing\nsignificant challenges in performing accurate dynamic simulations and other\nrelevant studies, such as gain tunings for stability analysis and controls. To\naddress this, we propose a Physics-Informed Latent Neural ODE Model (PI-LNM)\nthat integrates system physics with neural learning layers to capture the\nunmodeled behaviors of proprietary units. The proposed method is validated\nusing a grid-forming inverter (GFM) case study, demonstrating improved dynamic\nsimulation accuracy over approaches that rely solely on data-driven learning\nwithout physics-based guidance.", "AI": {"tldr": "该论文提出了一种基于物理信息潜变量神经常微分方程（PI-LNM）的新框架，用于模拟逆变器的专有动态，以提高电网动态仿真的准确性。", "motivation": "当前工业实践中，原始设备制造商（OEMs）不公开逆变器的精确内部控制和参数，导致电网动态仿真和稳定性分析等研究面临巨大挑战。", "method": "本文提出了一种物理信息潜变量神经常微分方程模型（PI-LNM），该模型将系统物理知识与神经网络学习层相结合，以捕获专有单元的未建模行为。", "result": "通过一个构网型逆变器（GFM）案例研究验证了所提出的方法，结果表明其比纯粹依赖数据驱动学习而无物理指导的方法具有更高的动态仿真精度。", "conclusion": "PI-LNM能够有效捕获逆变器的未建模行为，显著提高电网动态仿真的准确性。"}}
{"id": "2507.14590", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14590", "abs": "https://arxiv.org/abs/2507.14590", "authors": ["Łukasz Radliński", "Mateusz Guściora", "Jan Kocoń"], "title": "Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification", "comment": "International Conference on Computational Science 2025", "summary": "Numerous domain-specific machine learning tasks struggle with data scarcity\nand class imbalance. This paper systematically explores data augmentation\nmethods for NLP, particularly through large language models like GPT. The\npurpose of this paper is to examine and evaluate whether traditional methods\nsuch as paraphrasing and backtranslation can leverage a new generation of\nmodels to achieve comparable performance to purely generative methods. Methods\naimed at solving the problem of data scarcity and utilizing ChatGPT were\nchosen, as well as an exemplary dataset. We conducted a series of experiments\ncomparing four different approaches to data augmentation in multiple\nexperimental setups. We then evaluated the results both in terms of the quality\nof generated data and its impact on classification performance. The key\nfindings indicate that backtranslation and paraphrasing can yield comparable or\neven better results than zero and a few-shot generation of examples.", "AI": {"tldr": "本文探讨了利用大型语言模型（如GPT）进行NLP数据增强，发现传统方法（回译和释义）结合新模型可获得与纯生成方法相当或更优的性能。", "motivation": "许多领域特定的机器学习任务面临数据稀缺和类别不平衡问题。本研究旨在探索并评估传统数据增强方法（如释义和回译）结合新一代大型语言模型（如GPT）是否能达到与纯生成方法相当的性能。", "method": "论文系统探索了基于大型语言模型（如GPT、ChatGPT）的NLP数据增强方法。选择了利用ChatGPT解决数据稀缺问题的方法，并使用一个示例数据集。研究通过一系列实验比较了四种不同的数据增强方法，并在多种实验设置下进行评估。评估指标包括生成数据的质量及其对分类性能的影响。", "result": "关键发现表明，回译（backtranslation）和释义（paraphrasing）这两种传统数据增强方法，在使用大型语言模型时，可以产生与零样本（zero-shot）和少样本（few-shot）生成示例相当甚至更好的结果。", "conclusion": "结合大型语言模型的回译和释义等传统数据增强方法，在解决数据稀缺问题上是有效的，并且其性能可以与纯生成方法相媲美或超越。"}}
{"id": "2507.14533", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14533", "abs": "https://arxiv.org/abs/2507.14533", "authors": ["Shuo Cao", "Nan Ma", "Jiayang Li", "Xiaohui Li", "Lihao Shao", "Kaiwen Zhu", "Yu Zhou", "Yuandong Pu", "Jiarui Wu", "Jiaquan Wang", "Bo Qu", "Wenhai Wang", "Yu Qiao", "Dajuin Yao", "Yihao Liu"], "title": "ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding", "comment": "43 pages, 31 figures, 13 tables", "summary": "The rapid advancement of educational applications, artistic creation, and\nAI-generated content (AIGC) technologies has substantially increased practical\nrequirements for comprehensive Image Aesthetics Assessment (IAA), particularly\ndemanding methods capable of delivering both quantitative scoring and\nprofessional understanding. Multimodal Large Language Model (MLLM)-based IAA\nmethods demonstrate stronger perceptual and generalization capabilities\ncompared to traditional approaches, yet they suffer from modality bias\n(score-only or text-only) and lack fine-grained attribute decomposition,\nthereby failing to support further aesthetic assessment. In this paper, we\npresent:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and\nExpert-Level Understanding capabilities; (2) ArtiMuse-10K, the first\nexpert-curated image aesthetic dataset comprising 10,000 images spanning 5 main\ncategories and 15 subcategories, each annotated by professional experts with\n8-dimensional attributes analysis and a holistic score. Both the model and\ndataset will be made public to advance the field.", "AI": {"tldr": "本文提出了一种基于多模态大语言模型（MLLM）的图像美学评估（IAA）模型ArtiMuse及其配套的专家标注数据集ArtiMuse-10K，旨在提供量化评分和专业理解能力。", "motivation": "随着教育应用、艺术创作和AIGC技术的发展，对全面图像美学评估的需求日益增长，特别需要能够提供量化评分和专业理解的方法。现有基于MLLM的IAA方法存在模态偏见（仅评分或仅文本）且缺乏细粒度属性分解，无法支持深入美学评估。", "method": "本研究提出了两个核心贡献：1) ArtiMuse，一个创新的基于MLLM的IAA模型，具备联合评分和专家级理解能力；2) ArtiMuse-10K，首个由专家策划的图像美学数据集，包含10,000张图片，涵盖5个主类别和15个子类别，每张图片都由专业专家进行8维属性分析和整体评分。", "result": "研究成果是成功开发了ArtiMuse模型和ArtiMuse-10K数据集。ArtiMuse模型旨在解决现有MLLM-based IAA方法的局限性，提供更强的感知和泛化能力，并支持联合评分和专家级理解。ArtiMuse-10K数据集则为该领域提供了前所未有的专家级细粒度标注数据。", "conclusion": "ArtiMuse模型和ArtiMuse-10K数据集的提出，有望显著推动图像美学评估领域的发展，解决现有方法在量化评分、专业理解和细粒度属性分解方面的不足。模型和数据集都将公开，以促进学术研究和应用。"}}
{"id": "2507.14897", "categories": ["cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.14897", "abs": "https://arxiv.org/abs/2507.14897", "authors": ["Renxi Wang", "Rifo Ahmad Genadi", "Bilal El Bouardi", "Yongxin Wang", "Fajri Koto", "Zhengzhong Liu", "Timothy Baldwin", "Haonan Li"], "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents", "comment": null, "summary": "Language model (LM) agents have gained significant attention for their\nability to autonomously complete tasks through interactions with environments,\ntools, and APIs. LM agents are primarily built with prompt engineering or\nsupervised finetuning. At the same time, reinforcement learning (RL) has been\nexplored to enhance LM's capabilities, such as reasoning and factuality.\nHowever, the combination of the LM agents and reinforcement learning (Agent-RL)\nremains underexplored and lacks systematic study. To this end, we built\nAgentFly, a scalable and extensible Agent-RL framework designed to empower LM\nagents with a variety of RL algorithms. Our framework supports multi-turn\ninteractions by adapting traditional RL methods with token-level masking. It\nfeatures a decorator-based interface for defining tools and reward functions,\nenabling seamless extension and ease of use. To support high-throughput\ntraining, we implement asynchronous execution of tool calls and reward\ncomputations, and design a centralized resource management system for scalable\nenvironment coordination. We also provide a suite of prebuilt tools and\nenvironments, demonstrating the framework's effectiveness through successful\nagent training across multiple tasks.", "AI": {"tldr": "AgentFly是一个可扩展的Agent-RL框架，旨在通过结合多种强化学习算法来增强语言模型代理的能力，支持多轮交互、工具定义和高吞吐量训练。", "motivation": "语言模型代理在自主完成任务方面备受关注，主要通过提示工程或监督微调构建。虽然强化学习已被探索用于增强LM能力，但LM代理与强化学习（Agent-RL）的结合仍未被充分探索且缺乏系统研究。", "method": "本文构建了AgentFly框架。该框架通过令牌级掩码适应传统RL方法以支持多轮交互；提供基于装饰器的接口来定义工具和奖励函数；实现工具调用和奖励计算的异步执行以支持高吞吐量训练；并设计了集中式资源管理系统以实现可扩展的环境协调。此外，还提供了一套预构建的工具和环境。", "result": "通过在多个任务上成功训练代理，AgentFly框架展示了其有效性。", "conclusion": "AgentFly提供了一个系统且可扩展的Agent-RL框架，能够将多种RL算法应用于LM代理，从而增强其在复杂任务中的表现。"}}
{"id": "2507.15293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15293", "abs": "https://arxiv.org/abs/2507.15293", "authors": ["Shanshan Zhang", "Tianshui Wen", "Siyue Wang", "Qi Zhang", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "RepILN: Reparameterized Inertial Localization Network", "comment": null, "summary": "Inertial localization is regarded as a promising positioning solution for\nconsumer-grade IoT devices due to its cost-effectiveness and independence from\nexternal infrastructure. However, data-driven inertial localization methods\noften rely on increasingly complex network architectures to improve accuracy,\nwhich challenges the limited computational resources of IoT devices. Moreover,\nthese methods frequently overlook the importance of modeling long-term\ndependencies in inertial measurements - a critical factor for accurate\ntrajectory reconstruction - thereby limiting localization performance. To\naddress these challenges, we propose a reparameterized inertial localization\nnetwork that uses a multi-branch structure during training to enhance feature\nextraction. At inference time, this structure is transformed into an equivalent\nsingle-path architecture to improve parameter efficiency. To further capture\nlong-term dependencies in motion trajectories, we introduce a temporal-scale\nsparse attention mechanism that selectively emphasizes key trajectory segments\nwhile suppressing noise. Additionally, a gated convolutional unit is\nincorporated to effectively integrate long-range dependencies with local\nfine-grained features. Extensive experiments on public benchmarks demonstrate\nthat our method achieves a favorable trade-off between accuracy and model\ncompactness. For example, on the RoNIN dataset, our approach reduces the\nAbsolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while\nreducing the number of parameters by 3.86%.", "AI": {"tldr": "提出一种重新参数化的惯性定位网络，通过多分支训练和单路径推理，结合时域稀疏注意力机制和门控卷积单元，在保证精度的同时降低模型复杂度，适用于IoT设备。", "motivation": "现有数据驱动的惯性定位方法网络结构日益复杂，不适用于资源受限的IoT设备；且这些方法常忽略建模惯性测量中的长期依赖性，限制了定位性能。", "method": "提出重新参数化的惯性定位网络：训练时采用多分支结构增强特征提取，推理时转换为等效的单路径架构以提高参数效率。引入时域稀疏注意力机制选择性强调关键轨迹段并抑制噪声，以捕获长期依赖。融合门控卷积单元有效整合长程依赖与局部细粒度特征。", "result": "在公共基准测试中，所提方法在精度和模型紧凑性之间取得了良好平衡。例如，在RoNIN数据集上，相比RoNIN-ResNet，绝对轨迹误差（ATE）降低了2.59%，同时参数数量减少了3.86%。", "conclusion": "该方法有效解决了IoT设备惯性定位中计算资源受限和长期依赖建模不足的问题，实现了精度与模型紧凑性的优化折衷。"}}
{"id": "2507.15261", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15261", "abs": "https://arxiv.org/abs/2507.15261", "authors": ["Xinqi Chen", "Xiuxian Li", "Min Meng"], "title": "Dual-Channel Adaptive NMPC for Quadrotor under Instantaneous Impact and Payload Disturbances", "comment": null, "summary": "Capturing target objects using the quadrotor has gained increasing popularity\nin recent years, but most studies focus on capturing lightweight objects. The\ninstantaneous contact force generated when capturing objects of a certain mass,\nalong with the payload uncertainty after attachment, will pose significant\nchallenges to the quadrotor control. This paper proposes a novel control\narchitecture, namely Dual-Channel Adaptive Nonlinear Model Predictive Control\n(DCA-NMPC), which cascades a nonlinear model predictive control with two\nlower-level model reference adaptive controllers and can resist drastic impact\nand adapt to uncertain inertial parameters. Numerical simulation experiments\nare performed for validation.", "AI": {"tldr": "针对四旋翼飞行器捕捉具有一定质量物体时面临的冲击力和载荷不确定性问题，本文提出了一种新型双通道自适应非线性模型预测控制（DCA-NMPC）架构。", "motivation": "现有研究大多集中于捕捉轻量级物体。捕捉具有一定质量的物体会产生瞬时接触力，且附件后的载荷不确定性对四旋翼控制构成重大挑战。", "method": "本文提出了一种名为双通道自适应非线性模型预测控制（DCA-NMPC）的新型控制架构，该架构将一个非线性模型预测控制器与两个低层模型参考自适应控制器级联，旨在抵抗剧烈冲击并适应不确定的惯性参数。", "result": "通过数值仿真实验验证了所提方法的有效性。", "conclusion": "所提出的DCA-NMPC架构能够有效抵抗剧烈冲击并适应不确定的惯性参数，为四旋翼捕捉具有一定质量的物体提供了解决方案。"}}
{"id": "2507.14615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14615", "abs": "https://arxiv.org/abs/2507.14615", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper", "comment": "29 pages, 6 figs, 6 tables. Companion methods paper forthcoming", "summary": "Large Language Models(LLMs) hold promise for improving healthcare access in\nlow-resource settings, but their effectiveness in African primary care remains\nunderexplored. We present a methodology for creating a benchmark dataset and\nevaluation framework focused on Kenyan Level 2 and 3 clinical care. Our\napproach uses retrieval augmented generation (RAG) to ground clinical questions\nin Kenya's national guidelines, ensuring alignment with local standards. These\nguidelines were digitized, chunked, and indexed for semantic retrieval. Gemini\nFlash 2.0 Lite was then prompted with guideline excerpts to generate realistic\nclinical scenarios, multiple-choice questions, and rationale based answers in\nEnglish and Swahili. Kenyan physicians co-created and refined the dataset, and\na blinded expert review process ensured clinical accuracy, clarity, and\ncultural appropriateness. The resulting Alama Health QA dataset includes\nthousands of regulator-aligned question answer pairs across common outpatient\nconditions. Beyond accuracy, we introduce evaluation metrics that test clinical\nreasoning, safety, and adaptability such as rare case detection (Needle in the\nHaystack), stepwise logic (Decision Points), and contextual adaptability.\nInitial results reveal significant performance gaps when LLMs are applied to\nlocalized scenarios, consistent with findings that LLM accuracy is lower on\nAfrican medical content than on US-based benchmarks. This work offers a\nreplicable model for guideline-driven, dynamic benchmarking to support safe AI\ndeployment in African health systems.", "AI": {"tldr": "该研究提出了一种创建基准数据集和评估框架的方法，用于评估大型语言模型（LLMs）在肯尼亚初级医疗保健中的表现，并揭示了LLMs在本地化场景中存在的显著性能差距。", "motivation": "尽管大型语言模型有望改善低资源地区的医疗服务可及性，但它们在非洲初级医疗保健中的有效性尚未得到充分探索。", "method": "研究采用检索增强生成（RAG）方法，将临床问题与肯尼亚国家指南相结合，以确保符合当地标准。首先，将国家指南数字化、分块并建立索引进行语义检索。然后，使用Gemini Flash 2.0 Lite基于指南摘录生成真实的临床场景、多项选择题和基于理由的答案（英语和斯瓦希里语）。肯尼亚医生共同创建和完善了数据集，并通过盲审专家评审确保了临床准确性、清晰度和文化适宜性。此外，研究引入了新的评估指标，如罕见病例检测（大海捞针）、逐步逻辑（决策点）和上下文适应性。", "result": "研究成果是Alama Health QA数据集，其中包含数千个符合监管要求、涵盖常见门诊疾病的问答对。初步结果显示，LLMs在应用于本地化场景时存在显著的性能差距，这与LLM在非洲医疗内容上的准确性低于基于美国基准的发现一致。", "conclusion": "这项工作提供了一个可复制的、由指南驱动的动态基准测试模型，以支持人工智能在非洲卫生系统中的安全部署。"}}
{"id": "2507.14543", "categories": ["cs.CV", "cs.CY", "cs.HC", "cs.LG", "I.4.6"], "pdf": "https://arxiv.org/pdf/2507.14543", "abs": "https://arxiv.org/abs/2507.14543", "authors": ["Sharanya Mukherjee", "Md Hishaam Akhtar", "Kannadasan R"], "title": "Real Time Captioning of Sign Language Gestures in Video Meetings", "comment": "7 pages, 2 figures, 1 table, Presented at ICCMDE 2021", "summary": "It has always been a rather tough task to communicate with someone possessing\na hearing impairment. One of the most tested ways to establish such a\ncommunication is through the use of sign based languages. However, not many\npeople are aware of the smaller intricacies involved with sign language. Sign\nlanguage recognition using computer vision aims at eliminating the\ncommunication barrier between deaf-mute and ordinary people so that they can\nproperly communicate with others. Recently the pandemic has left the whole\nworld shaken up and has transformed the way we communicate. Video meetings have\nbecome essential for everyone, even people with a hearing disability. In recent\nstudies, it has been found that people with hearing disabilities prefer to sign\nover typing during these video calls. In this paper, we are proposing a browser\nextension that will automatically translate sign language to subtitles for\neveryone else in the video call. The Large-scale dataset which contains more\nthan 2000 Word-Level ASL videos, which were performed by over 100 signers will\nbe used.", "AI": {"tldr": "本文提出一个浏览器扩展，用于在视频通话中将手语自动翻译为字幕，以促进听障人士与普通人之间的交流。", "motivation": "听障人士与普通人沟通困难，手语是主要方式但了解者不多。疫情导致视频会议普及，听障人士在视频通话中更倾向于使用手语而非打字。现有计算机视觉手语识别旨在消除沟通障碍。", "method": "开发一个浏览器扩展，利用计算机视觉技术将视频通话中的手语实时翻译成字幕。该系统将使用一个包含2000多个词级美国手语（ASL）视频、由100多名手语者表演的大规模数据集。", "result": "论文提出了一种在视频通话中将手语自动翻译成字幕的浏览器扩展方案，旨在实现听障人士与健听人士的无障碍沟通。", "conclusion": "该提议的浏览器扩展有望通过自动化手语翻译为字幕，显著改善听障人士在视频会议中的沟通体验，有效消除沟通障碍。"}}
{"id": "2507.14899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14899", "abs": "https://arxiv.org/abs/2507.14899", "authors": ["Jiale Liu", "Huan Wang", "Yue Zhang", "Xiaoyu Luo", "Jiaxiang Hu", "Zhiliang Liu", "Min Xie"], "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis", "comment": null, "summary": "Non-destructive testing (NDT), particularly X-ray inspection, is vital for\nindustrial quality assurance, yet existing deep-learning-based approaches often\nlack interactivity, interpretability, and the capacity for critical\nself-assessment, limiting their reliability and operator trust. To address\nthese shortcomings, this paper proposes InsightX Agent, a novel LMM-based\nagentic framework designed to deliver reliable, interpretable, and interactive\nX-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent\npositions a Large Multimodal Model (LMM) as a central orchestrator,\ncoordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the\nEvidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect\nregion proposals for multi-scale feature maps and sparsifies them through\nNon-Maximum Suppression (NMS), optimizing detection of small, dense targets in\nX-ray images while maintaining computational efficiency. The EGR tool guides\nthe LMM agent through a chain-of-thought-inspired review process, incorporating\ncontext assessment, individual defect analysis, false positive elimination,\nconfidence recalibration and quality assurance to validate and refine the\nSDMSD's initial proposals. By strategically employing and intelligently using\ntools, InsightX Agent moves beyond passive data processing to active reasoning,\nenhancing diagnostic reliability and providing interpretations that integrate\ndiverse information sources. Experimental evaluations on the GDXray+ dataset\ndemonstrate that InsightX Agent not only achieves a high object detection\nF1-score of 96.35% but also offers significantly improved interpretability and\ntrustworthiness in its analyses, highlighting the transformative potential of\nagentic LLM frameworks for industrial inspection tasks.", "AI": {"tldr": "本文提出InsightX Agent，一个基于大语言多模态模型（LMM）的智能体框架，用于提供可靠、可解释和交互式的X射线无损检测（NDT）分析，解决了现有深度学习方法在可解释性和可信度方面的不足。", "motivation": "现有基于深度学习的X射线无损检测方法缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员的信任度。", "method": "InsightX Agent框架以LMM为核心协调器，整合了稀疏可变形多尺度检测器（SDMSD）和证据驱动反思（EGR）工具。SDMSD负责生成并稀疏化缺陷区域提议，优化小而密集目标的检测效率。EGR工具则通过受思维链启发的审查过程，引导LMM智能体进行上下文评估、缺陷分析、假阳性消除、置信度重新校准和质量保证，以验证和完善SDMSD的初始提议，实现从被动数据处理到主动推理的转变。", "result": "在GDXray+数据集上的实验评估表明，InsightX Agent不仅实现了96.35%的高目标检测F1分数，而且显著提高了分析的可解释性和可信度。", "conclusion": "InsightX Agent展示了基于智能体的LLM框架在工业检测任务中的巨大变革潜力，通过结合可靠的检测性能与增强的可解释性和可信度，提升了诊断可靠性。"}}
{"id": "2507.15444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15444", "abs": "https://arxiv.org/abs/2507.15444", "authors": ["Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe", "comment": "17 pages", "summary": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.", "AI": {"tldr": "该研究提出了一种基于实时气流测量和学习型控制器的四旋翼飞行器闭环控制系统，使其能够在狭窄管道中稳定悬停并进行横向平移，有效应对气动扰动。", "motivation": "在管道等密闭空间中，四旋翼飞行器面临不稳定的自激气动扰动，导致悬停稳定性差。现有方法要么依赖持续运动来缓解气流再循环，要么悬停稳定性有限。", "method": "开发了一种低延迟、事件驱动的烟雾测速方法来实时估计局部气流；使用基于循环卷积神经网络（RCNN）的扰动估计器实时推断力和扭矩扰动；将估计的扰动集成到通过强化学习训练的基于学习的控制器中；构建了利用实时气流测量信息的闭环控制系统。", "result": "首次实现了基于实时气流测量的四旋翼飞行器在狭窄管道中的闭环控制悬停；在管道横截面内的横向平移机动中表现出特别的有效性，通过实时扰动信息有效抵消瞬态气动效应，防止与管道壁碰撞；揭示了在狭窄圆形管道中飞行时出现的特征性流动结构。", "conclusion": "该工作首次展示了由实时气流场测量信息驱动的闭环控制空中机器人，为在复杂气动环境中飞行开辟了新的研究方向。此外，也为机器人技术与流体力学交叉领域提供了新见解。"}}
{"id": "2507.15307", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15307", "abs": "https://arxiv.org/abs/2507.15307", "authors": ["Jun Kang Yap", "Vishnu Monn Baskaran", "Wen Shan Tan", "Ze Yang Ding", "Hao Wang", "David L. Dowe"], "title": "Joint Optimisation of Electric Vehicle Routing and Scheduling: A Deep Learning-Driven Approach for Dynamic Fleet Sizes", "comment": "Accepted at International Joint Conference on Neural Networks (IJCNN\n  2025)", "summary": "Electric Vehicles (EVs) are becoming increasingly prevalent nowadays, with\nstudies highlighting their potential as mobile energy storage systems to\nprovide grid support. Realising this potential requires effective charging\ncoordination, which are often formulated as mixed-integer programming (MIP)\nproblems. However, MIP problems are NP-hard and often intractable when applied\nto time-sensitive tasks. To address this limitation, we propose a deep learning\nassisted approach for optimising a day-ahead EV joint routing and scheduling\nproblem with varying number of EVs. This problem simultaneously optimises EV\nrouting, charging, discharging and generator scheduling within a distribution\nnetwork with renewable energy sources. A convolutional neural network is\ntrained to predict the binary variables, thereby reducing the solution search\nspace and enabling solvers to determine the remaining variables more\nefficiently. Additionally, a padding mechanism is included to handle the\nchanges in input and output sizes caused by varying number of EVs, thus\neliminating the need for re-training. In a case study on the IEEE 33-bus system\nand Nguyen-Dupuis transportation network, our approach reduced runtime by 97.8%\nwhen compared to an unassisted MIP solver, while retaining 99.5% feasibility\nand deviating less than 0.01% from the optimal solution.", "AI": {"tldr": "本文提出一种深度学习辅助方法，用于优化电动汽车（EV）的日前联合路径规划和调度问题，以解决传统混合整数规划（MIP）在时间敏感任务中计算量大的问题，显著提升了求解速度并保持了高精度。", "motivation": "电动汽车作为移动储能系统具有为电网提供支持的潜力，但这需要有效的充电协调。此类协调问题通常被建模为混合整数规划（MIP），然而MIP问题是NP-hard的，在时间敏感的应用中往往难以处理。", "method": "本文提出一种深度学习辅助方法，用于优化包含可再生能源的配电网络中EV的日前联合路径规划和调度（包括充电、放电和发电机调度）。核心方法是训练一个卷积神经网络（CNN）来预测MIP中的二值变量，从而缩小求解空间，使求解器能更高效地确定剩余变量。此外，引入了一种填充机制来处理EV数量变化导致的输入输出尺寸变化，避免了重复训练。", "result": "在IEEE 33节点系统和Nguyen-Dupuis交通网络的案例研究中，与未辅助的MIP求解器相比，该方法将运行时间减少了97.8%，同时保持了99.5%的可行性，并且与最优解的偏差小于0.01%。", "conclusion": "该深度学习辅助方法能够显著提高电动汽车联合路径规划和调度问题的求解效率，同时保持了极高的解质量和可行性，有效解决了MIP在时间敏感任务中的计算瓶颈。"}}
{"id": "2507.14640", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14640", "abs": "https://arxiv.org/abs/2507.14640", "authors": ["Eric Xia", "Jugal Kalita"], "title": "Linear Relational Decoding of Morphology in Language Models", "comment": null, "summary": "A two-part affine approximation has been found to be a good approximation for\ntransformer computations over certain subject object relations. Adapting the\nBigger Analogy Test Set, we show that the linear transformation Ws, where s is\na middle layer representation of a subject token and W is derived from model\nderivatives, is also able to accurately reproduce final object states for many\nrelations. This linear technique is able to achieve 90% faithfulness on\nmorphological relations, and we show similar findings multi-lingually and\nacross models. Our findings indicate that some conceptual relationships in\nlanguage models, such as morphology, are readily interpretable from latent\nspace, and are sparsely encoded by cross-layer linear transformations.", "AI": {"tldr": "研究发现，在Transformer模型中，通过一个简单的线性变换（Ws，其中W来自模型导数，s是中间层的主语表示），可以高精度地重现许多关系（特别是形态学关系）的最终宾语状态，表明语言模型中某些概念关系是可解释的。", "motivation": "之前的研究发现两段仿射近似（two-part affine approximation）能很好地近似Transformer的计算。本研究旨在探索一个更简单的线性变换是否也能有效地重现特定主宾关系中的最终宾语状态，并揭示语言模型中概念关系的可解释性。", "method": "研究采用并改编了Bigger Analogy Test Set。核心方法是使用一个线性变换Ws，其中s是主语token的中间层表示，W是从模型导数中推导出来的。该方法在多语言和不同模型上进行了验证。", "result": "该线性技术在形态学关系上实现了90%的忠实度（faithfulness）。研究在多语言和跨模型上也发现了相似的结果。", "conclusion": "研究结果表明，语言模型中某些概念关系（如形态学）可以很容易地从潜在空间中解释出来，并且这些关系是通过跨层线性变换稀疏编码的。"}}
{"id": "2507.14544", "categories": ["cs.CV", "cs.AI", "68T45 (Machine vision and scene understanding)", "I.2.10; I.4.8; H.3.1"], "pdf": "https://arxiv.org/pdf/2507.14544", "abs": "https://arxiv.org/abs/2507.14544", "authors": ["Sujata Gaihre", "Amir Thapa Magar", "Prasuna Pokharel", "Laxmi Tiwari"], "title": "Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025", "comment": "accepted to ImageCLEF 2025, to be published in the lab proceedings", "summary": "This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA\n2025 Challenge, which targets visual question answering (VQA) for\ngastrointestinal endoscopy. We adopt the Florence model-a large-scale\nmultimodal foundation model-as the backbone of our VQA pipeline, pairing a\npowerful vision encoder with a text encoder to interpret endoscopic images and\nproduce clinically relevant answers. To improve generalization, we apply\ndomain-specific augmentations that preserve medical features while increasing\ntraining diversity. Experiments on the KASVIR dataset show that fine-tuning\nFlorence yields accurate responses on the official challenge metrics. Our\nresults highlight the potential of large multimodal models in medical VQA and\nprovide a strong baseline for future work on explainability, robustness, and\nclinical integration. The code is publicly available at:\nhttps://github.com/TiwariLaxuu/VQA-Florence.git", "AI": {"tldr": "本文介绍了针对ImageCLEFmed MEDVQA 2025挑战赛胃肠道内窥镜VQA子任务1的方法，该方法利用Florence多模态基础模型，并通过领域特定数据增强来提高泛化能力，在KASVIR数据集上取得了准确的结果。", "motivation": "研究动机是参与ImageCLEFmed MEDVQA 2025挑战赛的胃肠道内窥镜视觉问答（VQA）子任务1，旨在开发能够解释内窥镜图像并生成临床相关答案的VQA系统。", "method": "该方法以Florence大型多模态基础模型作为VQA管道的骨干，结合强大的视觉编码器和文本编码器来处理图像和文本。为提高泛化能力，应用了保留医学特征的领域特定数据增强来增加训练多样性。", "result": "在KASVIR数据集上的实验表明，微调Florence模型能在官方挑战赛指标上产生准确的响应。结果突出了大型多模态模型在医学VQA中的潜力，并为未来在可解释性、鲁棒性和临床集成方面的工作提供了强大的基线。", "conclusion": "研究表明大型多模态模型在医学视觉问答领域具有巨大潜力，并为该领域的未来研究（包括可解释性、鲁棒性和临床集成）奠定了坚实的基础。"}}
{"id": "2507.14906", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14906", "abs": "https://arxiv.org/abs/2507.14906", "authors": ["Xiao Yang", "Juxi Leitner", "Michael Burke"], "title": "Feedback-Induced Performance Decline in LLM-Based Decision-Making", "comment": null, "summary": "The ability of Large Language Models (LLMs) to extract context from natural\nlanguage problem descriptions naturally raises questions about their\nsuitability in autonomous decision-making settings. This paper studies the\nbehaviour of these models within a Markov Decision Process (MDPs). While\ntraditional reinforcement learning (RL) strategies commonly employed in this\nsetting rely on iterative exploration, LLMs, pre-trained on diverse datasets,\noffer the capability to leverage prior knowledge for faster adaptation. We\ninvestigate online structured prompting strategies in sequential decision\nmaking tasks, comparing the zero-shot performance of LLM-based approaches to\nthat of classical RL methods. Our findings reveal that although LLMs\ndemonstrate improved initial performance in simpler environments, they struggle\nwith planning and reasoning in complex scenarios without fine-tuning or\nadditional guidance. Our results show that feedback mechanisms, intended to\nimprove decision-making, often introduce confusion, leading to diminished\nperformance in intricate environments. These insights underscore the need for\nfurther exploration into hybrid strategies, fine-tuning, and advanced memory\nintegration to enhance LLM-based decision-making capabilities.", "AI": {"tldr": "本文研究了大型语言模型（LLMs）在马尔可夫决策过程（MDPs）中的行为，发现LLMs在简单环境中表现出更好的初始性能，但在复杂场景中缺乏规划和推理能力，且反馈机制常导致性能下降。", "motivation": "大型语言模型从自然语言问题描述中提取上下文的能力，引发了对其在自主决策环境中适用性的疑问。传统强化学习（RL）依赖迭代探索，而LLMs能利用先验知识加速适应，因此研究其在MDPs中的表现至关重要。", "method": "研究通过在线结构化提示策略，在序贯决策任务中进行实验。比较了基于LLM的零样本（zero-shot）方法与经典强化学习方法的性能。", "result": "研究发现，尽管LLMs在简单环境中展现出更好的初始性能，但在没有微调或额外指导的情况下，它们在复杂场景中的规划和推理能力较弱。此外，旨在改善决策的反馈机制，在复杂环境中反而常常引入混淆，导致性能下降。", "conclusion": "研究结果强调，需要进一步探索混合策略、微调以及高级记忆整合，以提升基于LLM的决策能力。"}}
{"id": "2507.15469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15469", "abs": "https://arxiv.org/abs/2507.15469", "authors": ["Thanh Thi Nguyen", "Saeid Nahavandi", "Imran Razzak", "Dung Nguyen", "Nhat Truong Pham", "Quoc Viet Hung Nguyen"], "title": "The Emergence of Deep Reinforcement Learning for Path Planning", "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)", "summary": "The increasing demand for autonomous systems in complex and dynamic\nenvironments has driven significant research into intelligent path planning\nmethodologies. For decades, graph-based search algorithms, linear programming\ntechniques, and evolutionary computation methods have served as foundational\napproaches in this domain. Recently, deep reinforcement learning (DRL) has\nemerged as a powerful method for enabling autonomous agents to learn optimal\nnavigation strategies through interaction with their environments. This survey\nprovides a comprehensive overview of traditional approaches as well as the\nrecent advancements in DRL applied to path planning tasks, focusing on\nautonomous vehicles, drones, and robotic platforms. Key algorithms across both\nconventional and learning-based paradigms are categorized, with their\ninnovations and practical implementations highlighted. This is followed by a\nthorough discussion of their respective strengths and limitations in terms of\ncomputational efficiency, scalability, adaptability, and robustness. The survey\nconcludes by identifying key open challenges and outlining promising avenues\nfor future research. Special attention is given to hybrid approaches that\nintegrate DRL with classical planning techniques to leverage the benefits of\nboth learning-based adaptability and deterministic reliability, offering\npromising directions for robust and resilient autonomous navigation.", "AI": {"tldr": "这篇综述全面概述了自主系统路径规划的传统方法和深度强化学习（DRL）的最新进展，分析了它们的优缺点，并指出了未来研究方向，特别是混合方法。", "motivation": "自动系统在复杂动态环境中日益增长的需求，推动了智能路径规划方法的研究。DRL作为一种强大的新方法，促使研究人员对其在路径规划中的应用进行深入探索和总结。", "method": "本文采用综述方法，分类并讨论了传统的路径规划方法（如图搜索、线性规划、演化计算）以及应用于自主车辆、无人机和机器人平台的DRL方法。它还分析了这些方法在计算效率、可伸缩性、适应性和鲁棒性方面的优缺点。", "result": "该综述提供了传统和DRL路径规划方法的全面概览，突出了它们的创新和实际应用。它详细讨论了这些方法的优势和局限性，并识别了关键的开放挑战和有前景的未来研究方向，特别是融合DRL与经典规划技术的混合方法。", "conclusion": "将深度强化学习与经典规划技术相结合的混合方法，有望为鲁棒和有弹性的自主导航提供有前景的方向，从而结合了基于学习的适应性和确定性可靠性的优点。"}}
{"id": "2507.15344", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15344", "abs": "https://arxiv.org/abs/2507.15344", "authors": ["Jiahao Liu", "Cheng Wang", "Tianshu Bi"], "title": "RoCoF Constrained Regional Inertia Security Region: Formulation and Application", "comment": null, "summary": "The regional inertia, which determines the regional rate of change of\nfrequency (RoCoF), should be kept in a secure status in renewable-penetrated\npower systems. To break away from mapping the regional maximum RoCoF with\nregional inertia in a linearized form, this paper comprehensively studies the\nregional inertia security problem from formulation to applications. Firstly,\nthe regional inertia security region (R-ISR) is defined, whose boundary is\nnon-linear and non-convex. Then, a local linearized method is devised to\ncalculate the global maximum of regional RoCoF. The non-convex ISR boundary is\nexpressed by multiple simple boundaries corresponding to each local solution,\nwhich can be obtained by a simple search-based method. Finally, the convexified\nR-ISR constraint is formed by convex decomposition and embedded in an inertia\noptimal adjustment model. The results on a 3-region system show some\ncounter-intuitive findings, such as increasing the inertia of one region may\nworsen its RoCoF.", "AI": {"tldr": "本文全面研究了可再生能源渗透电网中的区域惯量安全问题，定义了非线性和非凸的区域惯量安全区域(R-ISR)，并提出了一种计算和凸化R-ISR的方法，将其应用于惯量优化调整，揭示了惯量与频率变化率之间复杂的非线性关系。", "motivation": "在可再生能源渗透的电力系统中，区域惯量决定了频率变化率(RoCoF)，其安全性至关重要。现有方法通常以线性形式映射区域最大RoCoF与区域惯量，但这种线性化方法可能无法准确反映实际的非线性关系，因此需要更深入、全面的研究。", "method": "1. 定义了区域惯量安全区域(R-ISR)，其边界是非线性且非凸的。2. 设计了一种局部线性化方法来计算区域RoCoF的全局最大值。3. 通过多个简单的边界（对应每个局部解）来表达非凸的ISR边界，这些边界可通过简单的搜索方法获得。4. 通过凸分解形成凸化的R-ISR约束，并将其嵌入惯量最优调整模型中。", "result": "在一个三区域系统上的仿真结果显示了一些反直觉的发现，例如增加某一区域的惯量反而可能恶化该区域的频率变化率(RoCoF)。", "conclusion": "本文从公式化到应用全面研究了区域惯量安全问题，提出了处理非线性非凸R-ISR的方法，并将其成功应用于惯量优化调整模型，揭示了区域惯量与RoCoF之间复杂的非线性关系，为可再生能源电网的频率安全提供了新的见解。"}}
{"id": "2507.14649", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14649", "abs": "https://arxiv.org/abs/2507.14649", "authors": ["Minsuh Joo", "Hyunsoo Cho"], "title": "Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs", "comment": null, "summary": "Despite the outstanding performance of large language models (LLMs) across\nvarious NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate\nresponses--remains as a critical problem as it can be directly connected to a\ncrisis of building safe and reliable LLMs. Uncertainty estimation is primarily\nused to measure hallucination levels in LLM responses so that correct and\nincorrect answers can be distinguished clearly. This study proposes an\neffective uncertainty estimation approach, \\textbf{Cl}ust\\textbf{e}ring-based\nsem\\textbf{an}tic con\\textbf{s}ist\\textbf{e}ncy (\\textbf{Cleanse}). Cleanse\nquantifies the uncertainty with the proportion of the intra-cluster consistency\nin the total consistency between LLM hidden embeddings which contain adequate\nsemantic information of generations, by employing clustering. The effectiveness\nof Cleanse for detecting hallucination is validated using four off-the-shelf\nmodels, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two\nquestion-answering benchmarks, SQuAD and CoQA.", "AI": {"tldr": "本文提出了一种名为 Cleanse 的有效不确定性估计方法，通过聚类分析 LLM 隐藏嵌入的语义一致性，以检测大型语言模型中的幻觉。", "motivation": "大型语言模型（LLMs）的幻觉（即生成不准确的响应）是一个关键问题，直接影响构建安全可靠的 LLMs。不确定性估计是衡量 LLM 幻觉水平的主要方法，用于区分正确和不正确的答案。", "method": "提出了一种名为 Cleanse（Clustering-based semantic consistency，基于聚类的语义一致性）的不确定性估计方法。Cleanse 通过聚类，利用 LLM 隐藏嵌入（包含足够的生成语义信息）中簇内一致性占总一致性的比例来量化不确定性。", "result": "Cleanse 检测幻觉的有效性在 LLaMA-7B、LLaMA-13B、LLaMA2-7B 和 Mistral-7B 四个现成模型以及 SQuAD 和 CoQA 两个问答基准上得到了验证。", "conclusion": "Cleanse 是一种有效的不确定性估计方法，能够通过分析 LLM 隐藏嵌入的语义一致性来检测幻觉。"}}
{"id": "2507.14549", "categories": ["cs.CV", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.14549", "abs": "https://arxiv.org/abs/2507.14549", "authors": ["Haotian Deng", "Chi Zhang", "Chen Wei", "Quanying Liu"], "title": "Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions", "comment": "Accepted by IJCNN 2025", "summary": "A fundamental challenge in affective cognitive science is to develop models\nthat accurately capture the relationship between external emotional stimuli and\nhuman internal experiences. While ANNs have demonstrated remarkable accuracy in\nfacial expression recognition, their ability to model inter-individual\ndifferences in human perception remains underexplored. This study investigates\nthe phenomenon of high perceptual variability-where individuals exhibit\nsignificant differences in emotion categorization even when viewing the same\nstimulus. Inspired by the similarity between ANNs and human perception, we\nhypothesize that facial expression samples that are ambiguous for ANN\nclassifiers also elicit divergent perceptual judgments among human observers.\nTo examine this hypothesis, we introduce a novel perceptual boundary sampling\nmethod to generate facial expression stimuli that lie along ANN decision\nboundaries. These ambiguous samples form the basis of the varEmotion dataset,\nconstructed through large-scale human behavioral experiments. Our analysis\nreveals that these ANN-confusing stimuli also provoke heightened perceptual\nuncertainty in human participants, highlighting shared computational principles\nin emotion perception. Finally, by fine-tuning ANN representations using\nbehavioral data, we achieve alignment between ANN predictions and both\ngroup-level and individual-level human perceptual patterns. Our findings\nestablish a systematic link between ANN decision boundaries and human\nperceptual variability, offering new insights into personalized modeling of\nemotional interpretation.", "AI": {"tldr": "本研究发现，对人工神经网络（ANN）具有模糊性的面部表情样本，同样会引起人类观察者的高度感知不确定性，揭示了情感感知中共享的计算原理，并提出通过行为数据微调ANN来建模个体化情感解释的方法。", "motivation": "情感认知科学面临的挑战是建立准确捕捉外部情感刺激与人类内部体验之间关系的模，尤其是在面部表情识别中，ANN虽表现出色，但其在建模个体间感知差异方面的能力尚未充分探索。研究旨在理解并探索人类感知中高变异性（即个体对同一刺激表现出显著情感分类差异）的现象。", "method": "研究假设对ANN分类器模糊的面部表情样本也会引起人类观察者不同的感知判断。为此，引入了一种新颖的感知边界采样方法，生成位于ANN决策边界上的面部表情刺激。这些模糊样本构成了通过大规模人类行为实验构建的varEmotion数据集的基础。最后，通过使用行为数据微调ANN表示，以实现ANN预测与群体及个体层面人类感知模式的对齐。", "result": "分析表明，这些对ANN造成困惑的刺激也引发了人类参与者更高的感知不确定性，突出了情感感知中共享的计算原理。通过行为数据微调ANN表示后，ANN预测与群体层面和个体层面的人类感知模式均实现了对齐。", "conclusion": "研究建立了ANN决策边界与人类感知变异性之间的系统联系，为情感解释的个性化建模提供了新见解。"}}
{"id": "2507.14909", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14909", "abs": "https://arxiv.org/abs/2507.14909", "authors": ["Elio Grande"], "title": "The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities", "comment": null, "summary": "The Endless Tuning is a design method for a reliable deployment of artificial\nintelligence based on a double mirroring process, which pursues both the goals\nof avoiding human replacement and filling the so-called responsibility gap\n(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the\nrelational approach urged therein, it was then actualized in a protocol,\nimplemented in three prototypical applications regarding decision-making\nprocesses (respectively: loan granting, pneumonia diagnosis, and art style\nrecognition) and tested with such as many domain experts. Step by step\nillustrating the protocol, giving insights concretely showing a different voice\n(Gilligan 1993) in the ethics of artificial intelligence, a philosophical\naccount of technical choices (e.g., a reversed and hermeneutic deployment of\nXAI algorithms) will be provided in the present study together with the results\nof the experiments, focusing on user experience rather than statistical\naccuracy. Even thoroughly employing deep learning models, full control was\nperceived by the interviewees in the decision-making setting, while it appeared\nthat a bridge can be built between accountability and liability in case of\ndamage.", "AI": {"tldr": "本文提出了一种名为“无尽调优”的AI部署设计方法，通过双重镜像过程，旨在避免AI取代人类并填补AI责任空白，其有效性通过原型应用和用户体验测试得到了验证。", "motivation": "研究动机在于解决AI部署中两大核心问题：一是避免AI完全取代人类决策，二是填补AI系统造成的“责任空白”（Matthias 2004）。", "method": "该方法基于“双重镜像过程”和关系方法，被实际化为一个协议，并应用于三个原型场景（贷款审批、肺炎诊断、艺术风格识别）。研究通过与领域专家进行测试，并从哲学角度阐述了技术选择（例如，逆向和解释性XAI算法的部署），侧重于用户体验而非统计准确性。", "result": "实验结果显示，尽管使用了深度学习模型，受访者在决策过程中仍感知到完全的控制权。此外，研究表明在发生损害时，可以在问责制和责任之间建立桥梁。", "conclusion": "“无尽调优”方法能够有效解决AI部署中的人类控制和责任问题，即使在深度学习模型背景下，也能通过优先考虑用户体验，在AI的问责制和潜在责任之间搭建起连接。"}}
{"id": "2507.15474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15474", "abs": "https://arxiv.org/abs/2507.15474", "authors": ["Charith Premachandra", "Achala Athukorala", "U-Xuan Tan"], "title": "All-UWB SLAM Using UWB Radar and UWB AOA", "comment": null, "summary": "There has been a growing interest in autonomous systems designed to operate\nin adverse conditions (e.g. smoke, dust), where the visible light spectrum\nfails. In this context, Ultra-wideband (UWB) radar is capable of penetrating\nthrough such challenging environmental conditions due to the lower frequency\ncomponents within its broad bandwidth. Therefore, UWB radar has emerged as a\npotential sensing technology for Simultaneous Localization and Mapping (SLAM)\nin vision-denied environments where optical sensors (e.g. LiDAR, Camera) are\nprone to failure. Existing approaches involving UWB radar as the primary\nexteroceptive sensor generally extract features in the environment, which are\nlater initialized as landmarks in a map. However, these methods are constrained\nby the number of distinguishable features in the environment. Hence, this paper\nproposes a novel method incorporating UWB Angle of Arrival (AOA) measurements\ninto UWB radar-based SLAM systems to improve the accuracy and scalability of\nSLAM in feature-deficient environments. The AOA measurements are obtained using\nUWB anchor-tag units which are dynamically deployed by the robot in featureless\nareas during mapping of the environment. This paper thoroughly discusses\nprevailing constraints associated with UWB AOA measurement units and presents\nsolutions to overcome them. Our experimental results show that integrating UWB\nAOA units with UWB radar enables SLAM in vision-denied feature-deficient\nenvironments.", "AI": {"tldr": "本文提出了一种将超宽带(UWB)到达角(AOA)测量集成到UWB雷达SLAM系统中的新方法，以提高在视觉受限且特征稀疏环境中的定位和建图精度及可扩展性。", "motivation": "在烟雾、灰尘等恶劣环境中，可见光传感器（如激光雷达、相机）失效，而UWB雷达能够穿透这些环境，因此成为视觉受限环境下SLAM的潜在技术。然而，现有的基于UWB雷达的SLAM方法依赖于环境中的可区分特征，且受特征数量限制。", "method": "该研究提出将UWB到达角(AOA)测量集成到UWB雷达SLAM系统中。AOA测量通过机器人动态部署的UWB锚点-标签单元在无特征区域获取。文章还讨论了UWB AOA测量单元的现有约束并提出了解决方案。", "result": "实验结果表明，将UWB AOA单元与UWB雷达结合，可以在视觉受限和特征稀疏的环境中实现SLAM。", "conclusion": "通过整合UWB AOA测量，能够有效提升UWB雷达SLAM在缺乏环境特征的恶劣条件下的精度和可扩展性，从而在视觉受限且特征稀疏的环境中实现自主系统的定位与建图。"}}
{"id": "2507.15358", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15358", "abs": "https://arxiv.org/abs/2507.15358", "authors": ["Jiahao Liu", "Cheng Wang", "Tianshu Bi"], "title": "Revisiting the Effect of Grid-Following Converter on Frequency Dynamics -- Part I: Center of Inertia", "comment": null, "summary": "Understanding the impact of grid-following (GFL) converters on system\nfrequency dynamics is crucial, from both the center of inertia (COI) and\nfrequency spatial variation perspectives. Part I of this series clarifies the\nmechanisms by which GFLs influence COI frequency dynamics. A multi-generator\nmodel of the power system with GFLs is developed, incorporating the local\ndynamics of GFLs and their interaction with synchronous generators via virtual\ntie lines. By aggregating the multi-generator model into the COI frame, the\ninteraction between the COI frequency and the equivalent frequency of GFLs is\nrevealed. The equivalent inertia and other components at the GFL side,\ndetermined by control parameters and operating conditions, support the COI\nthrough virtual tying power. Simulation validates the accuracy of the proposed\nmodeling and demonstrates that the impact of GFLs on COI frequency is\nrelatively weak. The equivalent inertia and other components of GFLs still\nsignificantly influence COI frequency dynamics, with their effects being both\ntime-variable and adjustable.", "AI": {"tldr": "本文通过建立多发电机模型并聚合到惯性中心(COI)框架，阐明了并网型变流器(GFL)对系统COI频率动态的影响机制，发现GFL对COI频率影响相对较弱，但其等效惯量仍能显著且可调节地影响COI频率动态。", "motivation": "理解并网型变流器(GFL)对电力系统频率动态（特别是惯性中心COI频率和频率空间变化）的影响至关重要。", "method": "1. 建立了包含GFL局部动态及其与同步发电机通过虚拟联络线相互作用的多发电机电力系统模型。2. 将该模型聚合到COI框架中，以揭示COI频率与GFL等效频率的相互作用。3. 通过仿真验证了模型的准确性。", "result": "1. GFL侧的等效惯量及其他分量（由控制参数和运行条件决定）通过虚拟联络功率支持COI。2. GFL对COI频率的影响相对较弱。3. GFL的等效惯量及其他分量仍显著影响COI频率动态，且其影响是时变和可调节的。", "conclusion": "尽管并网型变流器对惯性中心频率的直接影响相对较弱，但其等效惯量和其他分量对COI频率动态仍具有显著且可调节的影响。"}}
{"id": "2507.14664", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14664", "abs": "https://arxiv.org/abs/2507.14664", "authors": ["Wannaphong Phatthiyaphaibun", "Can Udomcharoenchaikit", "Pakpoom Singkorapoom", "Kunat Pipatanakul", "Ekapol Chuangsuwanich", "Peerat Limkonchotiwat", "Sarana Nutanong"], "title": "Mangosteen: An Open Thai Corpus for Language Model Pretraining", "comment": "Work in Progress.All artifacts in this papers:\n  https://huggingface.co/collections/aisingapore/wangchanlion-v3-687a362d8f0ea2fe4077c6b3", "summary": "Pre-training data shapes a language model's quality, but raw web text is\nnoisy and demands careful cleaning. Existing large-scale corpora rely on\nEnglish-centric or language-agnostic pipelines whose heuristics do not capture\nThai script or cultural nuances, leaving risky material such as gambling\ncontent untreated. Prior Thai-specific efforts customize pipelines or build new\nones, yet seldom release their data or document design choices, hindering\nreproducibility and raising the question of how to construct a transparent,\nhigh-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai\ncorpus built through a Thai-adapted Dolma pipeline that includes custom\nrule-based language ID, revised C4/Gopher quality filters, and Thai-trained\ncontent filters, plus curated non-web sources such as Wikipedia, Royal Gazette\ntexts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic\nablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M\ndocuments while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION\nmodel continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and\nLlama-3.1 by about four points on Thai benchmarks. We release the full pipeline\ncode, cleaning manifests, corpus snapshot, and all checkpoints, providing a\nfully reproducible foundation for future Thai and regional LLM research.", "AI": {"tldr": "本文介绍了Mangosteen，一个470亿token的泰语语料库，通过定制化的清洗流程和多源数据构建，显著提升了泰语语言模型的性能，并提供了完全可复现的资源。", "motivation": "现有的大规模语料库清洗方法不适用于泰语的文字和文化细微差别，导致数据噪声高且包含不当内容。此前的泰语相关工作缺乏数据发布和设计文档，阻碍了可复现性，因此需要构建一个透明、高质量的泰语语料库。", "method": "引入了Mangosteen语料库，通过适应泰语的Dolma清洗流程构建。该流程包括定制的基于规则的语言识别、修订的C4/Gopher质量过滤器、泰语训练的内容过滤器，并整合了维基百科、皇家公报、OCR提取书籍和CC许可的YouTube字幕等非网络来源。", "result": "该清洗流程将CommonCrawl文档从2.02亿减少到2500万，同时将SEA-HELM NLG性能从3提升到11。在此语料库上持续预训练的80亿参数SEA-LION模型在泰语基准测试中超越了SEA-LION-v3和Llama-3.1约4个点。所有管道代码、清洗清单、语料库快照和检查点均已发布。", "conclusion": "Mangosteen语料库及其构建流程为未来的泰语和区域性大型语言模型研究提供了一个完全可复现的高质量基础。"}}
{"id": "2507.14553", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14553", "abs": "https://arxiv.org/abs/2507.14553", "authors": ["Xiaoran Wu"], "title": "Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance", "comment": null, "summary": "Clutter in photos is a distraction preventing photographers from conveying\nthe intended emotions or stories to the audience. Photography amateurs\nfrequently include clutter in their photos due to unconscious negligence or the\nlack of experience in creating a decluttered, aesthetically appealing scene for\nshooting. We are thus motivated to develop a camera guidance system that\nprovides solutions and guidance for clutter identification and removal. We\nestimate and visualize the contribution of objects to the overall aesthetics\nand content of a photo, based on which users can interactively identify\nclutter. Suggestions on getting rid of clutter, as well as a tool that removes\ncluttered objects computationally, are provided to guide users to deal with\ndifferent kinds of clutter and improve their photographic work. Two technical\nnovelties underpin interactions in our system: a clutter distinguishment\nalgorithm with aesthetics evaluations for objects and an iterative image\ninpainting algorithm based on generative adversarial nets that reconstructs\nmissing regions of removed objects for high-resolution images. User studies\ndemonstrate that our system provides flexible interfaces and accurate\nalgorithms that allow users to better identify distractions and take higher\nquality images within less time.", "AI": {"tldr": "本文提出一个相机指导系统，旨在帮助摄影师识别和移除照片中的杂乱元素，从而提升照片美感和表达力。", "motivation": "摄影师（尤其是业余爱好者）常因疏忽或缺乏经验，在照片中包含杂乱元素，这会分散观众注意力，阻碍情感或故事的有效传达，因此需要开发一个提供杂乱识别和移除解决方案的相机指导系统。", "method": "该系统通过估算并可视化物体对照片整体美感和内容的贡献来帮助用户交互式识别杂乱。它提供去除杂乱的建议，并包含一个计算性移除杂乱物体的工具。技术创新点包括：1) 一个结合美学评估的杂乱区分算法；2) 一个基于生成对抗网络（GANs）的迭代图像修复算法，用于重建高分辨率图像中被移除物体的缺失区域。", "result": "用户研究表明，该系统提供了灵活的界面和准确的算法，能帮助用户更好地识别干扰物，并在更短时间内拍摄出更高质量的图像。", "conclusion": "该系统通过提供杂乱识别、移除建议和计算工具，有效帮助用户处理照片中的杂乱，显著提升了摄影作品的质量和效率。"}}
{"id": "2507.14912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14912", "abs": "https://arxiv.org/abs/2507.14912", "authors": ["Ruhul Amin Khalil", "Kashif Ahmad", "Hazrat Ali"], "title": "Redefining Elderly Care with Agentic AI: Challenges and Opportunities", "comment": null, "summary": "The global ageing population necessitates new and emerging strategies for\ncaring for older adults. In this article, we explore the potential for\ntransformation in elderly care through Agentic Artificial Intelligence (AI),\npowered by Large Language Models (LLMs). We discuss the proactive and\nautonomous decision-making facilitated by Agentic AI in elderly care.\nPersonalized tracking of health, cognitive care, and environmental management,\nall aimed at enhancing independence and high-level living for older adults,\nrepresents important areas of application. With a potential for significant\ntransformation of elderly care, Agentic AI also raises profound concerns about\ndata privacy and security, decision independence, and access. We share key\ninsights to emphasize the need for ethical safeguards, privacy protections, and\ntransparent decision-making. Our goal in this article is to provide a balanced\ndiscussion of both the potential and the challenges associated with Agentic AI,\nand to provide insights into its responsible use in elderly care, to bring\nAgentic AI into harmony with the requirements and vulnerabilities specific to\nthe elderly. Finally, we identify the priorities for the academic research\ncommunities, to achieve human-centered advancements and integration of Agentic\nAI in elderly care. To the best of our knowledge, this is no existing study\nthat reviews the role of Agentic AI in elderly care. Hence, we address the\nliterature gap by analyzing the unique capabilities, applications, and\nlimitations of LLM-based Agentic AI in elderly care. We also provide a\ncompanion interactive dashboard at https://hazratali.github.io/agenticai/.", "AI": {"tldr": "本文探讨了基于大型语言模型（LLM）的自主智能体（Agentic AI）在老年护理中的潜在变革作用，包括个性化健康管理和环境控制，但也强调了数据隐私、决策独立性和可访问性等伦理挑战，并提出了负责任使用的建议和未来研究方向。", "motivation": "全球人口老龄化背景下，需要新的护理策略。本文旨在探索自主智能体AI在老年护理中的潜力，并填补现有文献中缺乏对自主智能体AI在老年护理中作用的综述空白。", "method": "本文通过讨论和分析的方式，探讨了基于LLM的自主智能体AI在老年护理中的独特能力、应用领域和局限性。同时，也分享了关键见解，强调了伦理保障、隐私保护和透明决策的必要性。此外，还提出了学术研究社区的优先事项。", "result": "自主智能体AI能够促进老年护理中的主动和自主决策，实现健康、认知护理和环境管理的个性化追踪，从而提升老年人的独立性和生活质量。然而，它也带来了数据隐私和安全、决策独立性以及可访问性等深层担忧。", "conclusion": "自主智能体AI在老年护理领域具有巨大的变革潜力，但其负责任的应用需要充分考虑伦理保障、隐私保护和透明决策。未来的学术研究应以人为本，确保自主智能体AI与老年人的特定需求和脆弱性相协调。"}}
{"id": "2507.15478", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15478", "abs": "https://arxiv.org/abs/2507.15478", "authors": ["Simon Kohaut", "Felix Divo", "Navid Hamid", "Benedict Flade", "Julian Eggert", "Devendra Singh Dhami", "Kristian Kersting"], "title": "The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents", "comment": null, "summary": "Ensuring reliable and rule-compliant behavior of autonomous agents in\nuncertain environments remains a fundamental challenge in modern robotics. Our\nwork shows how neuro-symbolic systems, which integrate probabilistic, symbolic\nwhite-box reasoning models with deep learning methods, offer a powerful\nsolution to this challenge. This enables the simultaneous consideration of\nexplicit rules and neural models trained on noisy data, combining the strength\nof structured reasoning with flexible representations. To this end, we\nintroduce the Constitutional Controller (CoCo), a novel framework designed to\nenhance the safety and reliability of agents by reasoning over deep\nprobabilistic logic programs representing constraints such as those found in\nshared traffic spaces. Furthermore, we propose the concept of self-doubt,\nimplemented as a probability density conditioned on doubt features such as\ntravel velocity, employed sensors, or health factors. In a real-world aerial\nmobility study, we demonstrate CoCo's advantages for intelligent autonomous\nsystems to learn appropriate doubts and navigate complex and uncertain\nenvironments safely and compliantly.", "AI": {"tldr": "本文提出了一种神经符号系统，通过引入“宪法控制器”（CoCo）框架和“自我怀疑”概念，使自主智能体在不确定环境中能够安全且符合规则地运行。", "motivation": "在不确定环境中，确保自主智能体行为的可靠性和规则遵从性是现代机器人技术面临的一个根本性挑战。", "method": "该研究引入了神经符号系统，结合了概率、符号白盒推理模型和深度学习方法。具体地，提出了“宪法控制器”（CoCo）框架，该框架通过对表示交通空间等约束的深度概率逻辑程序进行推理，增强智能体的安全性和可靠性。此外，还提出了“自我怀疑”的概念，将其实现为一种以速度、传感器或健康因素等“怀疑特征”为条件的概率密度。", "result": "通过一项真实的空中移动性研究，研究展示了CoCo在智能自主系统中学习适当的“怀疑”并安全、合规地导航复杂和不确定环境方面的优势。", "conclusion": "神经符号系统，特别是CoCo框架结合“自我怀疑”概念，为解决自主智能体在不确定环境中的可靠性和规则遵从性问题提供了一个强大的解决方案。"}}
{"id": "2507.15362", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15362", "abs": "https://arxiv.org/abs/2507.15362", "authors": ["Jiahao Liu", "Cheng Wang", "Tianshu Bi"], "title": "Revisiting the Effect of Grid-Following Converter on Frequency Dynamics -- Part II: Spatial Variation", "comment": null, "summary": "Besides the center of inertia (COI) frequency dynamics addressed in Part I,\nthe spatial frequency variation in power systems with grid-following (GFL)\nconverters is also crucial. Part II revisits the effect of GFLs on frequency\nspatial variation. Leveraging the interfacing state variables and equivalent\nfrequency defined in Part I, an extended frequency divider (FD) formula is\nproposed. The linearized mapping relationship between network node frequency\nand synchronous generator (SG) rotor frequency, as well as GFL equivalent\nfrequency, is modeled. The superposition contribution from GFLs is determined\nby the electrical distance between the generator and the frequency observation\nnode, as well as the system power flow conditions. Additionally, the frequency\nmapping for branch currents, which is overlooked in previous research, is\naddressed. Simulation results validate the accuracy of the proposed extended FD\nformula. They quantitatively demonstrate that the superposition contribution of\nGFLs to node frequency is relatively weak and that the superposition\ncoefficient is time-varying. The branch frequency superposition reveals a\ncomplex and distinctly different pattern.", "AI": {"tldr": "本文提出了一个扩展的频率分压器（FD）公式，用于分析含并网型逆变器（GFL）的电力系统中频率的空间变化，并量化了GFL对节点频率和支路电流频率的影响。", "motivation": "除了惯性中心（COI）频率动态外，电力系统中频率的空间变化也至关重要，尤其是在包含并网型逆变器（GFL）的情况下。此前的研究忽略了支路电流的频率映射，本研究旨在填补这一空白。", "method": "利用第一部分中定义的接口状态变量和等效频率，提出了一种扩展的频率分压器（FD）公式。建模了网络节点频率与同步发电机（SG）转子频率以及GFL等效频率之间的线性化映射关系。GFL的叠加贡献通过发电机与频率观测节点之间的电气距离以及系统潮流条件确定。此外，还研究了支路电流的频率映射。通过仿真结果验证了所提出扩展FD公式的准确性。", "result": "仿真结果验证了所提出的扩展FD公式的准确性。定量表明，GFL对节点频率的叠加贡献相对较弱且随时间变化。支路频率的叠加呈现出复杂且明显不同的模式。", "conclusion": "本文成功提出了一个扩展的频率分压器公式，用于描述含GFL系统的频率空间变化。研究发现GFL对节点频率的叠加贡献较弱且动态变化，同时揭示了支路频率叠加的复杂模式。"}}
{"id": "2507.14681", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14681", "abs": "https://arxiv.org/abs/2507.14681", "authors": ["Vinicius Anjos de Almeida", "Vinicius de Camargo", "Raquel Gómez-Bravo", "Egbert van der Haring", "Kees van Boven", "Marcelo Finger", "Luis Fernandez Lopez"], "title": "Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care", "comment": "To be submitted to peer-reviewed journal. 33 pages, 10 figures\n  (including appendix), 15 tables (including appendix). For associated code\n  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper", "summary": "Background: Medical coding structures healthcare data for research, quality\nmonitoring, and policy. This study assesses the potential of large language\nmodels (LLMs) to assign ICPC-2 codes using the output of a domain-specific\nsearch engine.\n  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each\nannotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's\ntext-embedding-3-large) retrieved candidates from 73,563 labeled concepts.\nThirty-three LLMs were prompted with each query and retrieved results to select\nthe best-matching ICPC-2 code. Performance was evaluated using F1-score, along\nwith token usage, cost, response time, and format adherence.\n  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top\nperformers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever\noptimization can improve performance by up to 4 points. Most models returned\nvalid codes in the expected format, with reduced hallucinations. Smaller models\n(<3B) struggled with formatting and input length.\n  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even\nwithout fine-tuning. This work offers a benchmark and highlights challenges,\nbut findings are limited by dataset scope and setup. Broader, multilingual,\nend-to-end evaluations are needed for clinical validation.", "AI": {"tldr": "本研究评估了大型语言模型（LLMs）结合领域特定搜索引擎，在巴西葡萄牙语临床表达中自动分配ICPC-2编码的潜力，并取得了良好效果。", "motivation": "医疗编码对医疗数据进行结构化，以支持研究、质量监测和政策制定。本研究旨在评估大型语言模型（LLMs）在利用领域特定搜索引擎输出的情况下，自动分配ICPC-2编码的潜力。", "method": "使用包含437条巴西葡萄牙语临床表达及其ICPC-2编码的数据集。利用语义搜索引擎（OpenAI的text-embedding-3-large）从73,563个概念中检索候选编码。用查询和检索结果提示33个LLM，以选择最佳匹配的ICPC-2编码。性能评估指标包括F1分数、token使用量、成本、响应时间及格式依从性。", "result": "28个模型F1分数超过0.8，其中10个超过0.85。表现最佳的模型包括gpt-4.5-preview、o3和gemini-2.5-pro。检索器优化可将性能提高多达4个百分点。大多数模型返回有效编码且幻觉减少。小型模型（<3B）在格式和输入长度方面表现不佳。", "conclusion": "大型语言模型在自动化ICPC-2编码方面展现出巨大潜力，即使未经微调。本研究提供了一个基准并指出了挑战，但其发现受限于数据集范围和设置。未来需要更广泛、多语言、端到端的评估以进行临床验证。"}}
{"id": "2507.14555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14555", "abs": "https://arxiv.org/abs/2507.14555", "authors": ["Jintang Xue", "Ganning Zhao", "Jie-En Yao", "Hong-En Chen", "Yue Hu", "Meida Chen", "Suya You", "C. -C. Jay Kuo"], "title": "Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions", "comment": null, "summary": "Understanding 3D scenes goes beyond simply recognizing objects; it requires\nreasoning about the spatial and semantic relationships between them. Current 3D\nscene-language models often struggle with this relational understanding,\nparticularly when visual embeddings alone do not adequately convey the roles\nand interactions of objects. In this paper, we introduce Descrip3D, a novel and\npowerful framework that explicitly encodes the relationships between objects\nusing natural language. Unlike previous methods that rely only on 2D and 3D\nembeddings, Descrip3D enhances each object with a textual description that\ncaptures both its intrinsic attributes and contextual relationships. These\nrelational cues are incorporated into the model through a dual-level\nintegration: embedding fusion and prompt-level injection. This allows for\nunified reasoning across various tasks such as grounding, captioning, and\nquestion answering, all without the need for task-specific heads or additional\nsupervision. When evaluated on five benchmark datasets, including ScanRefer,\nMulti3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms\nstrong baseline models, demonstrating the effectiveness of language-guided\nrelational representation for understanding complex indoor scenes.", "AI": {"tldr": "Descrip3D是一个新颖的3D场景理解框架，它通过自然语言显式编码对象关系，解决了现有模型在关系理解上的不足，并在多个基准测试中表现出色。", "motivation": "当前的3D场景-语言模型在理解对象间的空间和语义关系方面存在困难，因为仅凭视觉嵌入不足以传达对象的角色和交互。", "method": "Descrip3D通过文本描述（包含固有属性和上下文关系）增强每个对象。这些关系线索通过双层集成（嵌入融合和提示级注入）整合到模型中，实现了在接地、字幕和问答等各种任务中的统一推理，无需任务特定头部或额外监督。", "result": "Descrip3D在ScanRefer、Multi3DRefer、ScanQA、SQA3D和Scan2Cap等五个基准数据集上持续优于强大的基线模型。", "conclusion": "语言引导的关系表示对于理解复杂的室内场景是有效的，Descrip3D的性能证明了这一点。"}}
{"id": "2507.14962", "categories": ["cs.AI", "cs.CC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14962", "abs": "https://arxiv.org/abs/2507.14962", "authors": ["Johannes Schmidt", "Mohamed Maizia", "Victor Lagerkvist", "Johannes K. Fichte"], "title": "Complexity of Faceted Explanations in Propositional Abduction", "comment": "This is the author's self-archived copy including detailed proofs. To\n  appear in Theory and Practice of Logic Programming (TPLP), Proceedings of the\n  41st International Conference on Logic Programming (ICLP 2025)", "summary": "Abductive reasoning is a popular non-monotonic paradigm that aims to explain\nobserved symptoms and manifestations. It has many applications, such as\ndiagnosis and planning in artificial intelligence and database updates. In\npropositional abduction, we focus on specifying knowledge by a propositional\nformula. The computational complexity of tasks in propositional abduction has\nbeen systematically characterized - even with detailed classifications for\nBoolean fragments. Unsurprisingly, the most insightful reasoning problems\n(counting and enumeration) are computationally highly challenging. Therefore,\nwe consider reasoning between decisions and counting, allowing us to understand\nexplanations better while maintaining favorable complexity. We introduce facets\nto propositional abductions, which are literals that occur in some explanation\n(relevant) but not all explanations (dispensable). Reasoning with facets\nprovides a more fine-grained understanding of variability in explanations\n(heterogeneous). In addition, we consider the distance between two\nexplanations, enabling a better understanding of heterogeneity/homogeneity. We\ncomprehensively analyze facets of propositional abduction in various settings,\nincluding an almost complete characterization in Post's framework.", "AI": {"tldr": "该论文研究了命题溯因推理，引入了“刻面”（facet）和“解释距离”的概念，以在保持合理复杂度的前提下，更好地理解解释的变异性和异质性。", "motivation": "溯因推理中，计数和枚举等深层次推理问题计算复杂度高。因此，需要在决策和计数之间寻找平衡，以便更好地理解解释，同时保持有利的复杂度。", "method": "引入“刻面”（facet），即存在于某些解释但非所有解释中的文字，以细粒度地理解解释的变异性。同时，考虑两个解释之间的“距离”，以更好地理解异质性/同质性。在各种设置下，包括Post框架中，对命题溯因的刻面进行了全面分析。", "result": "刻面提供了对解释变异性（异质性）更细致的理解。解释距离有助于理解异质性/同质性。在Post框架中，对命题溯因的刻面进行了几乎完整的特征化。", "conclusion": "通过引入刻面和解释距离，可以更好地理解溯因解释的变异性和结构，从而在洞察力和计算可行性之间取得平衡。"}}
{"id": "2507.15484", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15484", "abs": "https://arxiv.org/abs/2507.15484", "authors": ["Jamie Bell"], "title": "Robots for Kiwifruit Harvesting and Pollination", "comment": null, "summary": "This research was a part of a project that developed mobile robots that\nperformed targeted pollen spraying and automated harvesting in pergola\nstructured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were\ndesigned and field testing of one of the concepts showed that the mechanism\ncould reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism\nwas able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,\nwhereas the previous state of the art mechanism was only able to reach less\nthan 70 percent of the fruit. Artificial pollination was performed by detecting\nflowers and then spraying pollen in solution onto the detected flowers from a\nline of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the\nheight of the canopy was measured and the spray boom was moved up and down to\nkeep the boom close enough to the flowers for the spray to reach the flowers,\nwhile minimising collisions with the canopy. Mobile robot navigation was\nperformed using a 2D lidar in apple orchards and vineyards. Lidar navigation in\nkiwifruit orchards was more challenging because the pergola structure only\nprovides a small amount of data for the direction of rows, compared to the\namount of data from the overhead canopy, the undulating ground and other\nobjects in the orchards. Multiple methods are presented here for extracting\nstructure defining features from 3D lidar data in kiwifruit orchards. In\naddition, a 3D lidar navigation system -- which performed row following, row\nend detection and row end turns -- was tested for over 30 km of autonomous\ndriving in kiwifruit orchards. Computer vision algorithms for row detection and\nrow following were also tested. The computer vision algorithm worked as well as\nthe 3D lidar row following method in testing.", "AI": {"tldr": "该研究开发了用于猕猴桃园的移动机器人，实现了精准授粉、自动化采摘和自主导航，并显著提升了采摘效率和导航鲁棒性。", "motivation": "开发能在棚架式猕猴桃园中进行靶向授粉和自动化采摘的移动机器人，解决现有技术在果实分离、树冠覆盖率和复杂环境导航方面的挑战。", "method": "设计了多种猕猴桃分离机制并进行田间测试；通过检测花朵并从喷杆上喷洒花粉溶液进行人工授粉，同时根据树冠高度调整喷杆；利用2D激光雷达在苹果园和葡萄园导航，并针对猕猴桃园棚架结构特点，开发了从3D激光雷达数据中提取结构特征的方法，测试了基于3D激光雷达的行跟随、行末检测和转弯系统，同时测试了计算机视觉算法进行行检测和跟随。", "result": "所测试的猕猴桃分离机制能可靠采摘果实，并能触及超过80%的果实（优于现有技术不足70%）；人工授粉可在高达1.4米/秒的速度下进行；3D激光雷达导航系统在猕猴桃园中自主驾驶超过30公里；计算机视觉算法在行跟随测试中表现与3D激光雷达方法同样出色。", "conclusion": "该研究成功开发并测试了用于猕猴桃园的移动机器人，在果实分离、授粉和复杂环境导航方面取得了显著进展，验证了所提出机制和导航方法的有效性和鲁棒性。"}}
{"id": "2507.15385", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15385", "abs": "https://arxiv.org/abs/2507.15385", "authors": ["Jun Kang Yap", "Vishnu Monn Baskaran", "Wen Shan Tan", "Ze Yang Ding", "Hao Wang", "David L. Dowe"], "title": "Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers", "comment": "Accepted at Industry Applications Society Annual Meeting (IAS 2025)", "summary": "The growing integration of renewable energy sources in modern power systems\nhas introduced significant operational challenges due to their intermittent and\nuncertain outputs. In recent years, mobile energy storage systems (ESSs) have\nemerged as a popular flexible resource for mitigating these challenges.\nCompared to stationary ESSs, mobile ESSs offer additional spatial flexibility,\nenabling cost-effective energy delivery through the transportation network.\nHowever, the widespread deployment of mobile ESSs is often hindered by the high\ninvestment cost, which has motivated researchers to investigate utilising more\nreadily available alternatives, such as electric vehicles (EVs) as mobile\nenergy storage units instead. Hence, we explore this opportunity with a\nMIP-based day-ahead electric vehicle joint routing and scheduling problem in\nthis work. However, solving the problem in a practical setting can often be\ncomputationally intractable since the existence of binary variables makes it\ncombinatorial challenging. Therefore, we proposed to simplify the problem's\nsolution process for a MIP solver by pruning the solution search space with a\ntransformer-based deep learning (DL) model. This is done by training the model\nto rapidly predict the optimal binary solutions. In addition, unlike many\nexisting DL approaches that assume fixed problem structures, the proposed model\nis designed to accommodate problems with EV fleets of any sizes. This\nflexibility is essential since frequent re-training can introduce significant\ncomputational overhead. We evaluated the approach with simulations on the IEEE\n33-bus system coupled with the Nguyen-Dupuis transportation network.", "AI": {"tldr": "本研究提出了一种基于MIP的电动汽车（EV）联合路径规划和调度方法，用于缓解可再生能源的间歇性问题，并利用基于Transformer的深度学习模型修剪搜索空间，以加速求解过程，同时适应不同规模的电动汽车车队。", "motivation": "现代电力系统中可再生能源的间歇性和不确定性带来了运营挑战。移动储能系统（ESS）可以缓解这些挑战，但其高昂的投资成本限制了广泛部署。因此，研究人员寻求利用电动汽车作为更易获得的移动储能单元。", "method": "本研究采用基于混合整数规划（MIP）的日前电动汽车联合路径规划和调度问题模型。为解决MIP的计算复杂性，提出使用基于Transformer的深度学习（DL）模型来预测最优二值解，从而修剪MIP求解器的搜索空间。该DL模型被设计为能够适应任意规模的电动汽车车队，避免频繁重新训练的计算开销。", "result": "该方法在IEEE 33节点系统与Nguyen-Dupuis交通网络的耦合仿真中进行了评估。", "conclusion": "本研究提出的方法通过利用电动汽车作为移动储能单元，结合MIP和基于Transformer的深度学习模型，有效解决了含大规模可再生能源电力系统的运行挑战，并克服了传统MIP求解的计算难题，同时具备处理不同规模电动汽车车队的灵活性。"}}
{"id": "2507.14683", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14683", "abs": "https://arxiv.org/abs/2507.14683", "authors": ["Xingxuan Li", "Yao Xiao", "Dianwen Ng", "Hai Ye", "Yue Deng", "Xiang Lin", "Bin Wang", "Zhanfeng Mo", "Chong Zhang", "Yueyi Zhang", "Zonglin Yang", "Ruilin Li", "Lei Lei", "Shihao Xu", "Han Zhao", "Weiling Chen", "Feng Ji", "Lidong Bing"], "title": "MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization", "comment": "Technical report", "summary": "Large language models have recently evolved from fluent text generation to\nadvanced reasoning across diverse domains, giving rise to reasoning language\nmodels. Among these domains, mathematical reasoning serves as a representative\nbenchmark as it requires precise multi-step logic and abstract reasoning, which\ncan be generalized to other tasks. While closed-source RLMs such as GPT-o3\ndemonstrate impressive reasoning capabilities, their proprietary nature limits\ntransparency and reproducibility. Although many open-source projects aim to\nclose this gap, most of them lack sufficient openness by omitting critical\nresources such as datasets and detailed training configurations, which hinders\nreproducibility. To contribute toward greater transparency in RLM development,\nwe introduce the MiroMind-M1 series, a set of fully open-source RLMs built on\nthe Qwen-2.5 backbone that match or exceed the performance of existing\nopen-source RLMs. Specifically, our models are trained in two stages: SFT on a\ncarefully curated corpus of 719K math-reasoning problems with verified CoT\ntrajectories, followed by RLVR on 62K challenging and verifiable problems. To\nenhance the robustness and efficiency of the RLVR process, we introduce\nContext-Aware Multi-Stage Policy Optimization, an algorithm that integrates\nlength-progressive training with an adaptive repetition penalty to encourage\ncontext-aware RL training. Our model achieves state-of-the-art or competitive\nperformance and superior token efficiency among Qwen-2.5-based open-source 7B\nand 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate\nreproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,\nMiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,\nMiroMind-M1-RL-62K); and all training and evaluation configurations. We hope\nthese resources will support further research and foster community advancement.", "AI": {"tldr": "本文介绍了MiroMind-M1系列，一套基于Qwen-2.5骨干的完全开源的数学推理语言模型，通过两阶段训练（SFT和RLVR）和新颖的RLVR算法，在数学基准测试中达到了领先或有竞争力的性能，并提供了完整的可复现资源。", "motivation": "闭源推理语言模型（RLMs）缺乏透明度和可复现性，而现有开源项目往往未能提供完整资源（数据集、训练配置），阻碍了可复现性。数学推理是一个重要的基准，需要精确的多步逻辑和抽象推理。", "method": "该研究基于Qwen-2.5骨干构建MiroMind-M1系列模型，采用两阶段训练：首先在包含719K数学推理问题及CoT轨迹的数据集上进行SFT，然后对62K具有挑战性的可验证问题进行RLVR。为增强RLVR的鲁棒性和效率，引入了“上下文感知多阶段策略优化”（Context-Aware Multi-Stage Policy Optimization）算法，该算法结合了长度渐进式训练和自适应重复惩罚。为促进可复现性，研究团队发布了完整的模型、数据集和所有训练与评估配置。", "result": "MiroMind-M1系列模型在性能上达到或超越了现有开源RLMs。具体而言，其在AIME24、AIME25和MATH基准测试中，在基于Qwen-2.5的7B和32B开源模型中，实现了最先进或有竞争力的性能，并展现出卓越的token效率。", "conclusion": "该研究通过引入MiroMind-M1系列模型及其配套的完整开源资源（模型、数据集、配置），显著提升了推理语言模型开发的透明度和可复现性，旨在支持进一步研究并促进社区发展。"}}
{"id": "2507.14559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14559", "abs": "https://arxiv.org/abs/2507.14559", "authors": ["Zixuan Hu", "Xiaotong Li", "Shixiang Tang", "Jun Liu", "Yichun Hu", "Ling-Yu Duan"], "title": "LEAD: Exploring Logit Space Evolution for Model Selection", "comment": "Accepted by CVPR 2024", "summary": "The remarkable success of pretrain-then-finetune paradigm has led to a\nproliferation of available pre-trained models for vision tasks. This surge\npresents a significant challenge in efficiently choosing the most suitable\npre-trained models for downstream tasks. The critical aspect of this challenge\nlies in effectively predicting the model transferability by considering the\nunderlying fine-tuning dynamics. Existing methods often model fine-tuning\ndynamics in feature space with linear transformations, which do not precisely\nalign with the fine-tuning objective and fail to grasp the essential\nnonlinearity from optimization. To this end, we present LEAD, a\nfinetuning-aligned approach based on the network output of logits. LEAD\nproposes a theoretical framework to model the optimization process and derives\nan ordinary differential equation (ODE) to depict the nonlinear evolution\ntoward the final logit state. Additionally, we design a class-aware\ndecomposition method to consider the varying evolution dynamics across classes\nand further ensure practical applicability. Integrating the closely aligned\noptimization objective and nonlinear modeling capabilities derived from the\ndifferential equation, our method offers a concise solution to effectively\nbridge the optimization gap in a single step, bypassing the lengthy fine-tuning\nprocess. The comprehensive experiments on 24 supervised and self-supervised\npre-trained models across 10 downstream datasets demonstrate impressive\nperformances and showcase its broad adaptability even in low-data scenarios.", "AI": {"tldr": "LEAD是一种基于logits的微调对齐方法，通过建模非线性优化过程来高效预测预训练模型的迁移能力，避免了耗时的微调过程。", "motivation": "预训练模型数量激增，导致为下游任务选择最合适模型成为挑战。现有方法在特征空间中建模微调动态，但未能精确捕捉优化过程中的非线性，且与微调目标不符。", "method": "本文提出了LEAD方法，它是一种基于网络输出logits的微调对齐方法。LEAD提出了一个理论框架来建模优化过程，并推导了一个常微分方程（ODE）来描述最终logits状态的非线性演变。此外，还设计了一种类别感知分解方法来考虑不同类别间的演变动态，从而在单一步骤内有效弥合优化差距，绕过漫长的微调过程。", "result": "在10个下游数据集上对24个有监督和自监督预训练模型进行的综合实验表明，LEAD展现了令人印象深刻的性能和广泛的适应性，即使在数据量较少的情况下也表现出色。", "conclusion": "LEAD通过将优化目标和非线性建模能力紧密结合，提供了一种简洁有效的解决方案，能够准确预测模型迁移能力，显著提高了预训练模型选择的效率。"}}
{"id": "2507.14987", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14987", "abs": "https://arxiv.org/abs/2507.14987", "authors": ["Yi Zhang", "An Zhang", "XiuYu Zhang", "Leheng Sheng", "Yuxin Chen", "Zhenkai Liang", "Xiang Wang"], "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs), despite possessing latent safety understanding\nfrom their vast pretraining data, remain vulnerable to generating harmful\ncontent and exhibit issues such as over-refusal and utility degradation after\nsafety alignment. Current safety alignment methods often result in superficial\nrefusal shortcuts or rely on intensive supervision for reasoning-based\napproaches, failing to fully leverage the model's intrinsic safety\nself-awareness. We propose \\textbf{AlphaAlign}, a simple yet effective pure\nreinforcement learning (RL) framework with verifiable safety reward designed to\nincentivize this latent safety awareness through proactive safety reasoning.}\nAlphaAlign employs a dual-reward system: a verifiable safety reward encourages\ncorrectly formatted and explicitly justified refusals for harmful queries while\npenalizing over-refusals, and a normalized helpfulness reward guides\nhigh-quality responses to benign inputs. This allows the model to develop\nproactive safety reasoning capabilities without depending on supervised\nsafety-specific reasoning data. AlphaAlign demonstrates three key advantages:\n(1) Simplicity and efficiency, requiring only binary prompt safety labels and\nminimal RL steps for substantial improvements. (2) Breaking the safety-utility\ntrade-off, by enhancing refusal of harmful content and reducing over-refusals,\nwhile simultaneously maintaining or even improving general task performance and\nrobustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety\nreasoning that generates explicit safety rationales rather than relying on\nshallow refusal patterns.", "AI": {"tldr": "AlphaAlign是一种基于纯强化学习的框架，通过可验证的安全奖励和双重奖励系统，利用大型语言模型（LLM）的内在安全意识，实现主动安全推理，从而在不牺牲实用性的前提下，提高LLM的安全性并减少过度拒绝。", "motivation": "尽管LLM拥有潜在的安全理解能力，但在安全对齐后仍易生成有害内容、过度拒绝并导致实用性下降。现有的安全对齐方法常导致肤浅的拒绝捷径或依赖密集的监督数据，未能充分利用模型固有的安全自我意识。", "method": "AlphaAlign采用纯强化学习框架，设计了双重奖励系统：1) 可验证的安全奖励，鼓励对有害查询进行格式正确且明确理由的拒绝，并惩罚过度拒绝；2) 标准化的有用性奖励，指导模型对良性输入生成高质量响应。该方法无需监督安全推理数据，通过激励主动安全推理来实现对齐。", "result": "AlphaAlign展示了三大优势：1) 简单高效，仅需二元提示安全标签和少量RL步骤即可显著改进；2) 打破安全-实用性权衡，在增强有害内容拒绝和减少过度拒绝的同时，保持甚至提升了通用任务性能和对未知越狱的鲁棒性；3) 实现深度对齐，培养主动安全推理能力，生成明确的安全理由而非依赖浅层拒绝模式。", "conclusion": "AlphaAlign通过纯强化学习和独特的双重奖励系统，有效利用LLM的内在安全意识，实现了主动安全推理，从而在提高模型安全性的同时，保持并提升了实用性，并展现出对未知越狱的鲁棒性。"}}
{"id": "2507.15493", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15493", "abs": "https://arxiv.org/abs/2507.15493", "authors": ["Chilam Cheang", "Sijin Chen", "Zhongren Cui", "Yingdong Hu", "Liqun Huang", "Tao Kong", "Hang Li", "Yifeng Li", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Wenxuan Ou", "Wanli Peng", "Zeyu Ren", "Haixin Shi", "Jiawen Tian", "Hongtao Wu", "Xin Xiao", "Yuyang Xiao", "Jiafeng Xu", "Yichu Yang"], "title": "GR-3 Technical Report", "comment": "Tech report. Authors are listed in alphabetical order. Project page:\n  https://seed.bytedance.com/GR3/", "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2507.15564", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.15564", "abs": "https://arxiv.org/abs/2507.15564", "authors": ["Julius P. J. Krebbekx", "Roland Tóth", "Amritam Das"], "title": "Scaled Relative Graph Analysis of General Interconnections of SISO Nonlinear Systems", "comment": null, "summary": "Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain\nmethod for the analysis of nonlinear systems. However, we show that the current\nSRG analysis suffers from a pitfall that limits its applicability in analyzing\npractical nonlinear systems. We overcome this pitfall by introducing a novel\nreformulation of the SRG of a linear time-invariant operator and combining the\nSRG with the Nyquist criterion. The result is a theorem that can be used to\nassess stability and $L_2$-gain performance for general interconnections of\nnonlinear dynamic systems. We provide practical calculation results for\ncanonical interconnections and apply our result to Lur'e systems to obtain a\ngeneralization of the celebrated circle criterion, which deals with broader\nclass of nonlinearities, and we derive (incremental) $L_2$-gain performance\nbounds. We illustrate the power of the new approach on the analysis of several\nexamples.", "AI": {"tldr": "本文提出了一种改进的尺度相对图（SRG）分析方法，通过重新表述线性时不变算子的SRG并结合奈奎斯特判据，克服了现有SRG方法的局限性，从而能够评估非线性互联系统的稳定性和L2增益性能，并推广了圆判据。", "motivation": "当前的尺度相对图（SRG）分析存在一个缺陷，限制了其在分析实际非线性系统时的适用性。", "method": "研究人员通过以下方法克服了现有缺陷：1. 引入了线性时不变算子SRG的新型重构；2. 将重构后的SRG与奈奎斯特判据相结合。", "result": "研究成果包括：1. 提出一个可用于评估非线性动态系统普遍互联的稳定性和L2增益性能的定理；2. 提供了规范互联的实际计算结果；3. 将该方法应用于Lur'e系统，得到了广义的圆判据，能够处理更广泛的非线性问题；4. 推导了（增量）L2增益性能界限。", "conclusion": "新的方法为非线性动态系统的分析提供了一个强大的工具，能够有效评估其稳定性及L2增益性能，并成功推广了经典的圆判据。"}}
{"id": "2507.14688", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14688", "abs": "https://arxiv.org/abs/2507.14688", "authors": ["Mohammed Alkhowaiter", "Norah Alshahrani", "Saied Alshahrani", "Reem I. Masoud", "Alaa Alzahrani", "Deema Alnuhait", "Emad A. Alghamdi", "Khalid Almubarak"], "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations", "comment": null, "summary": "Post-training has emerged as a crucial technique for aligning pre-trained\nLarge Language Models (LLMs) with human instructions, significantly enhancing\ntheir performance across a wide range of tasks. Central to this process is the\nquality and diversity of post-training datasets. This paper presents a review\nof publicly available Arabic post-training datasets on the Hugging Face Hub,\norganized along four key dimensions: (1) LLM Capabilities (e.g., Question\nAnswering, Translation, Reasoning, Summarization, Dialogue, Code Generation,\nand Function Calling); (2) Steerability (e.g., persona and system prompts); (3)\nAlignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.\nEach dataset is rigorously evaluated based on popularity, practical adoption,\nrecency and maintenance, documentation and annotation quality, licensing\ntransparency, and scientific contribution. Our review revealed critical gaps in\nthe development of Arabic post-training datasets, including limited task\ndiversity, inconsistent or missing documentation and annotation, and low\nadoption across the community. Finally, the paper discusses the implications of\nthese gaps on the progress of Arabic LLMs and applications while providing\nconcrete recommendations for future efforts in post-training dataset\ndevelopment.", "AI": {"tldr": "本文综述了Hugging Face Hub上公开的阿拉伯语LLM后训练数据集，揭示了其在任务多样性、文档质量和社区采纳度方面的关键缺陷。", "motivation": "后训练对于使大型语言模型（LLMs）与人类指令对齐至关重要，而高质量和多样化的后训练数据集是这一过程的核心。本文旨在评估和分析现有的阿拉伯语后训练数据集，以推动阿拉伯语LLM的发展。", "method": "本文对Hugging Face Hub上公开的阿拉伯语后训练数据集进行了系统综述，并从四个关键维度进行组织：LLM能力（如问答、翻译、推理、摘要、对话、代码生成、函数调用）、可控性（如角色和系统提示）、对齐（如文化、安全、伦理、公平性）和鲁棒性。每个数据集都根据流行度、实际采纳度、更新和维护情况、文档和标注质量、许可透明度以及科学贡献进行严格评估。", "result": "综述结果揭示了阿拉伯语后训练数据集开发中的关键缺陷，包括任务多样性有限、文档和标注不一致或缺失，以及社区采纳度低。", "conclusion": "本文讨论了这些缺陷对阿拉伯语LLM及其应用进展的影响，并为未来的后训练数据集开发工作提供了具体的建议。"}}
{"id": "2507.14575", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14575", "abs": "https://arxiv.org/abs/2507.14575", "authors": ["Andrea Moschetto", "Lemuel Puglisi", "Alec Sargood", "Pierluigi Dell'Acqua", "Francesco Guarnera", "Sebastiano Battiato", "Daniele Ravì"], "title": "Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation", "comment": null, "summary": "Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image\ncontrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering\ndistinct diagnostic insights. However, acquiring all desired modalities\nincreases scan time and cost, motivating research into computational methods\nfor cross-modal synthesis. To address this, recent approaches aim to synthesize\nmissing MRI contrasts from those already acquired, reducing acquisition time\nwhile preserving diagnostic quality. Image-to-image (I2I) translation provides\na promising framework for this task. In this paper, we present a comprehensive\nbenchmark of generative models$\\unicode{x2013}$specifically, Generative\nAdversarial Networks (GANs), diffusion models, and flow matching (FM)\ntechniques$\\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All\nframeworks are implemented with comparable settings and evaluated on three\npublicly available MRI datasets of healthy adults. Our quantitative and\nqualitative analyses show that the GAN-based Pix2Pix model outperforms\ndiffusion and FM-based methods in terms of structural fidelity, image quality,\nand computational efficiency. Consistent with existing literature, these\nresults suggest that flow-based models are prone to overfitting on small\ndatasets and simpler tasks, and may require more data to match or surpass GAN\nperformance. These findings offer practical guidance for deploying I2I\ntranslation techniques in real-world MRI workflows and highlight promising\ndirections for future research in cross-modal medical image synthesis. Code and\nmodels are publicly available at\nhttps://github.com/AndreaMoschetto/medical-I2I-benchmark.", "AI": {"tldr": "该研究比较了GAN、扩散模型和流匹配模型在T1w到T2w MRI图像转换任务中的性能，发现基于GAN的Pix2Pix模型在结构保真度、图像质量和计算效率方面表现最佳。", "motivation": "获取所有所需的MRI模态会增加扫描时间和成本，因此需要计算方法来从现有模态合成缺失的MRI对比度，以减少采集时间并保持诊断质量。", "method": "本文对生成模型（特别是生成对抗网络GANs、扩散模型和流匹配FM技术）在T1w到T2w的2D MRI图像到图像(I2I)转换任务上进行了全面基准测试。所有框架均在可比较的设置下实现，并在三个公开的健康成人MRI数据集上进行评估。", "result": "定量和定性分析表明，基于GAN的Pix2Pix模型在结构保真度、图像质量和计算效率方面优于扩散模型和基于FM的方法。结果还表明，基于流的模型在小数据集和简单任务上容易过拟合，可能需要更多数据才能匹配或超越GAN的性能。", "conclusion": "GANs，特别是Pix2Pix模型，在T1w到T2w MRI图像合成任务中表现出优异的性能。这些发现为在实际MRI工作流程中部署I2I转换技术提供了实用指导，并为跨模态医学图像合成的未来研究指明了方向。"}}
{"id": "2507.15013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15013", "abs": "https://arxiv.org/abs/2507.15013", "authors": ["Xiaoyu Li", "Jin Wu", "Shaoyang Guo", "Haoran Shi", "Chanjin Zheng"], "title": "A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing", "comment": "15pages, 7 figures", "summary": "In the smart era, psychometric tests are becoming increasingly important for\npersonnel selection, career development, and mental health assessment.\nForced-choice tests are common in personality assessments because they require\nparticipants to select from closely related options, lowering the risk of\nresponse distortion. This study presents a deep learning-based Forced-Choice\nNeural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of\ntraditional models and is applicable to the three most common item block types\nfound in forced-choice tests. To account for the unidimensionality of items in\nforced-choice tests, we create interpretable participant and item parameters.\nWe model the interactions between participant and item features using\nmultilayer neural networks after mining them using nonlinear mapping. In\naddition, we use the monotonicity assumption to improve the interpretability of\nthe diagnostic results. The FCNCD's effectiveness is validated by experiments\non real-world and simulated datasets that show its accuracy, interpretability,\nand robustness.", "AI": {"tldr": "本研究提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于心理测量学测试，该模型克服了传统模型的局限性，并提高了诊断的准确性、可解释性和鲁棒性。", "motivation": "在智能时代，心理测量学测试对于人员选拔、职业发展和心理健康评估日益重要。强制选择测试因能降低反应偏差风险而常用于人格评估，但传统模型存在局限性，且需要处理项目块类型和项目单维度性。", "method": "本研究提出了FCNCD模型，该模型适用于三种最常见的强制选择测试项目块类型。它创建了可解释的参与者和项目参数以处理项目单维度性，并使用多层神经网络在非线性映射挖掘后建模参与者和项目特征之间的交互。此外，模型还利用单调性假设来提高诊断结果的可解释性。", "result": "FCNCD的有效性通过在真实和模拟数据集上的实验得到了验证，结果表明其在准确性、可解释性和鲁棒性方面表现出色。", "conclusion": "FCNCD模型为强制选择心理测量学测试提供了一种有效的深度学习解决方案，克服了传统模型的局限性，并显著提高了诊断结果的准确性、可解释性和鲁棒性。"}}
{"id": "2507.15499", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15499", "abs": "https://arxiv.org/abs/2507.15499", "authors": ["Jongseok Lee", "Timo Birr", "Rudolph Triebel", "Tamim Asfour"], "title": "CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions", "comment": "8 pages. Accepted to IEEE RAL", "summary": "We propose CLEVER, an active learning system for robust semantic perception\nwith Deep Neural Networks (DNNs). For data arriving in streams, our system\nseeks human support when encountering failures and adapts DNNs online based on\nhuman instructions. In this way, CLEVER can eventually accomplish the given\nsemantic perception tasks. Our main contribution is the design of a system that\nmeets several desiderata of realizing the aforementioned capabilities. The key\nenabler herein is our Bayesian formulation that encodes domain knowledge\nthrough priors. Empirically, we not only motivate CLEVER's design but further\ndemonstrate its capabilities with a user validation study as well as\nexperiments on humanoid and deformable objects. To our knowledge, we are the\nfirst to realize stream-based active learning on a real robot, providing\nevidence that the robustness of the DNN-based semantic perception can be\nimproved in practice. The project website can be accessed at\nhttps://sites.google.com/view/thecleversystem.", "AI": {"tldr": "CLEVER是一个主动学习系统，通过在线接收人类指令并适应深度神经网络，以实现鲁棒的语义感知，尤其适用于流式数据。", "motivation": "在流式数据场景下，当深度神经网络的语义感知任务出现故障时，需要一种系统能够寻求人类支持并在线适应，以提高其鲁棒性。", "method": "CLEVER系统设计，采用流式主动学习方法，在线根据人类指令调整DNNs。关键在于其贝叶斯公式，通过先验知识编码领域知识。", "result": "CLEVER的设计得到了验证，并通过用户验证研究以及在类人机器人和可变形物体上的实验证明了其能力。据作者所知，这是首次在真实机器人上实现基于流的主动学习，证明了DNN语义感知的鲁棒性在实践中可以得到改善。", "conclusion": "CLEVER系统通过在线主动学习和人类反馈，能够有效提高基于深度神经网络的语义感知在实践中的鲁棒性，尤其适用于处理流式数据。"}}
{"id": "2507.15594", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15594", "abs": "https://arxiv.org/abs/2507.15594", "authors": ["Junnan Pan", "Prodromos Sotiriadis", "Vladislav Nenchev", "Ferdinand Englberger"], "title": "Improving Functional Reliability of Near-Field Monitoring for Emergency Braking in Autonomous Vehicles", "comment": "6 pages, 3 figures, conference paper", "summary": "Autonomous vehicles require reliable hazard detection. However, primary\nsensor systems may miss near-field obstacles, resulting in safety risks.\nAlthough a dedicated fast-reacting near-field monitoring system can mitigate\nthis, it typically suffers from false positives. To mitigate these, in this\npaper, we introduce three monitoring strategies based on dynamic spatial\nproperties, relevant object sizes, and motion-aware prediction. In experiments\nin a validated simulation, we compare the initial monitoring strategy against\nthe proposed improvements. The results demonstrate that the proposed strategies\ncan significantly improve the reliability of near-field monitoring systems.", "AI": {"tldr": "本文提出三种基于动态空间属性、相关物体尺寸和运动感知预测的近场监测策略，以显著提高自动驾驶车辆近场障碍物监测系统的可靠性并减少误报。", "motivation": "自动驾驶车辆需要可靠的危险检测能力，但主传感器系统可能遗漏近场障碍物，造成安全风险。尽管专用快速响应的近场监测系统可以缓解此问题，但通常存在误报。", "method": "引入了三种新的监测策略：基于动态空间属性、相关物体尺寸和运动感知预测。在经过验证的模拟环境中，将这些改进策略与初始监测策略进行比较。", "result": "实验结果表明，所提出的策略可以显著提高近场监测系统的可靠性。", "conclusion": "通过引入新的监测策略，可以有效减少近场监测系统的误报，从而提高自动驾驶车辆的安全性。"}}
{"id": "2507.14693", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14693", "abs": "https://arxiv.org/abs/2507.14693", "authors": ["Amina Dzafic", "Merve Kavut", "Ulya Bayram"], "title": "Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation", "comment": "This manuscript has been submitted to the IEEE Journal of Biomedical\n  and Health Informatics", "summary": "Suicidal ideation detection is critical for real-time suicide prevention, yet\nits progress faces two under-explored challenges: limited language coverage and\nunreliable annotation practices. Most available datasets are in English, but\neven among these, high-quality, human-annotated data remains scarce. As a\nresult, many studies rely on available pre-labeled datasets without examining\ntheir annotation process or label reliability. The lack of datasets in other\nlanguages further limits the global realization of suicide prevention via\nartificial intelligence (AI). In this study, we address one of these gaps by\nconstructing a novel Turkish suicidal ideation corpus derived from social media\nposts and introducing a resource-efficient annotation framework involving three\nhuman annotators and two large language models (LLMs). We then address the\nremaining gaps by performing a bidirectional evaluation of label reliability\nand model consistency across this dataset and three popular English suicidal\nideation detection datasets, using transfer learning through eight pre-trained\nsentiment and emotion classifiers. These transformers help assess annotation\nconsistency and benchmark model performance against manually labeled data. Our\nfindings underscore the need for more rigorous, language-inclusive approaches\nto annotation and evaluation in mental health natural language processing (NLP)\nwhile demonstrating the questionable performance of popular models with\nzero-shot transfer learning. We advocate for transparency in model training and\ndataset construction in mental health NLP, prioritizing data and model\nreliability.", "AI": {"tldr": "本研究构建了一个新的土耳其自杀意念语料库，并引入了一种高效的标注框架。同时，评估了标签可靠性和模型一致性，强调了精神健康NLP中更严谨、多语言的标注和评估方法的重要性。", "motivation": "自杀意念检测面临两大挑战：语言覆盖范围有限（主要为英语）和标注实践不可靠（高质量人工标注数据稀缺，许多研究依赖未经审查的预标注数据集），这限制了AI在全球自杀预防中的应用。", "method": "构建了一个新的土耳其语自杀意念语料库，数据来源于社交媒体，并采用包含3名人类标注者和2个大型语言模型（LLMs）的资源高效标注框架。随后，利用8个预训练的情感和情绪分类器进行迁移学习，对该土耳其数据集和三个流行的英语自杀意念检测数据集进行了标签可靠性和模型一致性的双向评估。", "result": "研究结果强调了在精神健康自然语言处理（NLP）中，需要更严谨、更具语言包容性的标注和评估方法。同时，也揭示了流行模型在零样本迁移学习下的性能表现不佳。", "conclusion": "本研究倡导在精神健康NLP中，模型训练和数据集构建应提高透明度，并优先考虑数据和模型的可靠性。"}}
{"id": "2507.14587", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14587", "abs": "https://arxiv.org/abs/2507.14587", "authors": ["Merjem Bećirović", "Amina Kurtović", "Nordin Smajlović", "Medina Kapo", "Amila Akagić"], "title": "Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX", "comment": null, "summary": "Medical imaging plays a vital role in early disease diagnosis and monitoring.\nSpecifically, blood microscopy offers valuable insights into blood cell\nmorphology and the detection of hematological disorders. In recent years, deep\nlearning-based automated classification systems have demonstrated high\npotential in enhancing the accuracy and efficiency of blood image analysis.\nHowever, a detailed performance analysis of specific deep learning frameworks\nappears to be lacking. This paper compares the performance of three popular\ndeep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in\nclassifying blood cell images from the publicly available BloodMNIST dataset.\nThe study primarily focuses on inference time differences, but also\nclassification performance for different image sizes. The results reveal\nvariations in performance across frameworks, influenced by factors such as\nimage resolution and framework-specific optimizations. Classification accuracy\nfor JAX and PyTorch was comparable to current benchmarks, showcasing the\nefficiency of these frameworks for medical image classification.", "AI": {"tldr": "本文比较了TensorFlow/Keras、PyTorch和JAX三种深度学习框架在血细胞图像分类任务上的性能，重点分析了推理时间差异和不同图像尺寸下的分类表现。", "motivation": "尽管深度学习在血细胞图像分析中潜力巨大，但目前缺乏对特定深度学习框架详细的性能分析。", "method": "研究使用了公开的BloodMNIST数据集，比较了TensorFlow/Keras、PyTorch和JAX三个框架在血细胞图像分类上的性能，主要关注推理时间差异以及不同图像尺寸下的分类性能。", "result": "结果显示，框架性能存在差异，受图像分辨率和框架特定优化等因素影响。JAX和PyTorch的分类准确率与当前基准相当。", "conclusion": "JAX和PyTorch在医学图像分类方面表现出高效性。"}}
{"id": "2507.15042", "categories": ["cs.AI", "cs.IR", "I.2.7; H.3.3; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.15042", "abs": "https://arxiv.org/abs/2507.15042", "authors": ["Jerry Wang", "Fang Yu"], "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection", "comment": "Accepted by KDD Workshop on Prompt Optimization 2025", "summary": "Adversarial prompt attacks can significantly alter the reliability of\nRetrieval-Augmented Generation (RAG) systems by re-ranking them to produce\nincorrect outputs. In this paper, we present a novel method that applies\nDifferential Evolution (DE) to optimize adversarial prompt suffixes for\nRAG-based question answering. Our approach is gradient-free, treating the RAG\npipeline as a black box and evolving a population of candidate suffixes to\nmaximize the retrieval rank of a targeted incorrect document to be closer to\nreal world scenarios. We conducted experiments on the BEIR QA datasets to\nevaluate attack success at certain retrieval rank thresholds under multiple\nretrieving applications. Our results demonstrate that DE-based prompt\noptimization attains competitive (and in some cases higher) success rates\ncompared to GGPP to dense retrievers and PRADA to sparse retrievers, while\nusing only a small number of tokens (<=5 tokens) in the adversarial suffix.\nFurthermore, we introduce a readability-aware suffix construction strategy,\nvalidated by a statistically significant reduction in MLM negative\nlog-likelihood with Welch's t-test. Through evaluations with a BERT-based\nadversarial suffix detector, we show that DE-generated suffixes evade\ndetection, yielding near-chance detection accuracy.", "AI": {"tldr": "本文提出一种基于差分进化的新方法，用于优化对抗性提示后缀，以攻击检索增强生成（RAG）系统。该方法在黑盒设置下有效，生成短小且难以检测的后缀，能显著提高错误文档的检索排名，证明了RAG系统的脆弱性。", "motivation": "对抗性提示攻击能通过重新排序检索结果来改变检索增强生成（RAG）系统的可靠性，导致其产生不正确输出。因此，需要研究有效且隐蔽的攻击方法，以更好地理解RAG系统的脆弱性。", "method": "本研究应用差分进化（DE）来优化RAG问答系统的对抗性提示后缀。该方法是无梯度的，将RAG管道视为黑盒。它通过进化候选后缀种群，旨在最大化目标错误文档的检索排名。此外，研究还引入了一种可读性感知的后缀构建策略。", "result": "实验结果表明，基于DE的提示优化在攻击成功率上与现有方法（GGPP针对稠密检索器，PRADA针对稀疏检索器）相比具有竞争力，甚至更高。对抗性后缀仅使用少量标记（<=5个）。可读性感知的后缀构建策略显著降低了MLM负对数似然。最重要的是，DE生成的后缀能有效规避BERT基对抗性后缀检测器，检测准确率接近随机。", "conclusion": "差分进化是一种有效的方法，可以在黑盒设置下生成短小、难以检测的对抗性提示后缀，从而成功攻击RAG系统，使其检索并生成不正确的信息。这凸显了RAG系统在对抗性攻击下的脆弱性，并为未来的防御研究提供了见解。"}}
{"id": "2507.15604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15604", "abs": "https://arxiv.org/abs/2507.15604", "authors": ["Johannes Hartwig", "Philipp Lienhardt", "Dominik Henrich"], "title": "Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2025 (to appear)", "summary": "As the availability of cobots increases, it is essential to address the needs\nof users with little to no programming knowledge to operate such systems\nefficiently. Programming concepts often use intuitive interaction modalities,\nsuch as hand guiding, to address this. When programming in-contact motions,\nsuch frameworks require knowledge of the robot tool's payload inertial\nparameters (PIP) in addition to the demonstrated velocities and forces to\nensure effective hybrid motion-force control. This paper aims to enable\nnon-expert users to program in-contact motions more efficiently by eliminating\nthe need for a dedicated PIP calibration, thereby enabling flexible robot tool\nchanges. Since demonstrated tasks generally also contain motions with\nnon-contact, our approach uses these parts to estimate the robot's PIP using\nestablished estimation techniques. The results show that the estimation of the\npayload's mass is accurate, whereas the center of mass and the inertia tensor\nare affected by noise and a lack of excitation. Overall, these findings show\nthe feasibility of PIP estimation during hand guiding but also highlight the\nneed for sufficient payload accelerations for an accurate estimation.", "AI": {"tldr": "本文提出一种在协作机器人手引导编程过程中，通过利用非接触运动部分估计工具负载惯性参数（PIP）的方法，以消除对专门PIP校准的需求，从而使非专业用户能更高效地编程接触运动。", "motivation": "随着协作机器人普及，需要解决非编程专家用户高效操作机器人的问题。手引导是直观的编程方式，但在编程接触运动时，需要机器人工具的负载惯性参数（PIP）以确保有效的混合运动-力控制。传统的框架需要专门的PIP校准，这限制了工具的灵活更换，因此有必要寻找一种更高效、无需专门校准的方法。", "method": "该方法利用手引导示教任务中包含的非接触运动部分，使用已有的估计技术来估计机器人的负载惯性参数（PIP）。", "result": "结果显示，负载质量的估计是准确的，而质心和惯性张量的估计则受到噪声和激励不足的影响。", "conclusion": "研究表明在手引导过程中进行PIP估计是可行的，但也强调了需要足够的负载加速度才能实现准确的估计。"}}
{"id": "2507.15708", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15708", "abs": "https://arxiv.org/abs/2507.15708", "authors": ["Niloofar Nobahari", "Alireza Rezaee"], "title": "Reliability-Based Fault Analysis and Modeling of Satellite Electrical Power Subsystems Using Fault Tree and Simulation Tools", "comment": null, "summary": "One of the most important satellite subsystems is its electric power\nsubsystem. The occurrence of a fault in the satellite power system causes the\nfailure of all or part of the satellite. Calculating the overall reliability of\nthe power system before the mission is crucial in improving the design of the\nsatellite power system. Each component of the power system may malfunction due\nto pressure, launch pressure, and operating conditions. Accordingly, in this\npaper, first, a healthy and faulty system for the components of the electrical\npower system is simulated with MATLAB. Finally, by drawing a fault tree to\nanalyze the reliability of the power subsystem, overall mission reliability,\npower system fault rate, and overall fault rate of the mission are calculated\nby Windchill software. Finally, a total mission assurance of 0.999 was\nachieved, indicating the high reliability of the simulated system.", "AI": {"tldr": "本文通过MATLAB仿真和Windchill软件的故障树分析，计算并评估了卫星电力子系统的可靠性。", "motivation": "卫星电力系统故障会导致卫星部分或全部失效，因此在任务前计算电力系统的整体可靠性对于改进卫星设计至关重要。", "method": "首先，使用MATLAB模拟电力系统组件的健康和故障状态。然后，通过绘制故障树，利用Windchill软件计算电力子系统的可靠性、整体任务可靠性、电力系统故障率和整体任务故障率。", "result": "最终计算得出总任务保障度为0.999，表明所模拟的系统具有高可靠性。", "conclusion": "该研究表明，通过MATLAB仿真和故障树分析方法，可以有效评估卫星电力系统的可靠性，并验证了所模拟系统的高可靠性。"}}
{"id": "2507.14741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14741", "abs": "https://arxiv.org/abs/2507.14741", "authors": ["Maria Sahakyan", "Bedoor AlShebli"], "title": "Disparities in Peer Review Tone and the Role of Reviewer Anonymity", "comment": null, "summary": "The peer review process is often regarded as the gatekeeper of scientific\nintegrity, yet increasing evidence suggests that it is not immune to bias.\nAlthough structural inequities in peer review have been widely debated, much\nless attention has been paid to the subtle ways in which language itself may\nreinforce disparities. This study undertakes one of the most comprehensive\nlinguistic analyses of peer review to date, examining more than 80,000 reviews\nin two major journals. Using natural language processing and large-scale\nstatistical modeling, it uncovers how review tone, sentiment, and supportive\nlanguage vary across author demographics, including gender, race, and\ninstitutional affiliation. Using a data set that includes both anonymous and\nsigned reviews, this research also reveals how the disclosure of reviewer\nidentity shapes the language of evaluation. The findings not only expose hidden\nbiases in peer feedback, but also challenge conventional assumptions about\nanonymity's role in fairness. As academic publishing grapples with reform,\nthese insights raise critical questions about how review policies shape career\ntrajectories and scientific progress.", "AI": {"tldr": "该研究对8万多篇同行评审意见进行语言分析，揭示了评审意见的语气、情感和支持性语言如何因作者的人口统计学特征（性别、种族、机构）和审稿人身份是否公开而存在偏见。", "motivation": "同行评审被视为科学诚信的守门人，但越来越多的证据表明其存在偏见。尽管同行评审中的结构性不平等已被广泛讨论，但语言本身如何强化这些差异却较少受到关注。", "method": "该研究对两大期刊的8万多篇评审意见进行了迄今为止最全面的语言分析。方法包括使用自然语言处理（NLP）和大规模统计建模，并分析了匿名和署名评审数据。", "result": "研究发现，评审意见的语气、情感和支持性语言因作者的人口统计学特征（包括性别、种族和机构隶属关系）而异。此外，审稿人身份的公开与否也影响了评估语言。研究不仅揭示了同行反馈中隐藏的偏见，也挑战了关于匿名在公平性中作用的传统假设。", "conclusion": "研究结果揭示了同行评审中存在的语言偏见，并对匿名在确保公平方面的作用提出了质疑。这些发现对学术出版改革提出了关键问题，即评审政策如何影响职业轨迹和科学进步。"}}
{"id": "2507.14596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14596", "abs": "https://arxiv.org/abs/2507.14596", "authors": ["Doriand Petit", "Steve Bourgeois", "Vincent Gay-Bellile", "Florian Chabot", "Loïc Barthe"], "title": "DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF", "comment": "Published at ICCV'25", "summary": "3D semantic segmentation provides high-level scene understanding for\napplications in robotics, autonomous systems, \\textit{etc}. Traditional methods\nadapt exclusively to either task-specific goals (open-vocabulary segmentation)\nor scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the\nfirst method addressing the broader problem of 3D Open-Vocabulary Sub-concepts\nDiscovery, which aims to provide a 3D semantic segmentation that adapts to both\nthe scene and user queries. We build DiSCO-3D on Neural Fields representations,\ncombining unsupervised segmentation with weak open-vocabulary guidance. Our\nevaluations demonstrate that DiSCO-3D achieves effective performance in\nOpen-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in\nthe edge cases of both open-vocabulary and unsupervised segmentation.", "AI": {"tldr": "DiSCO-3D是首个解决三维开放词汇子概念发现问题的方法，它能根据场景和用户查询提供三维语义分割。", "motivation": "传统的三维语义分割方法要么只适应特定任务目标（开放词汇分割），要么只适应场景内容（无监督语义分割），无法同时兼顾场景和用户查询的需求。", "method": "DiSCO-3D基于神经场（Neural Fields）表示，结合了无监督分割和弱开放词汇指导。", "result": "DiSCO-3D在开放词汇子概念发现中表现出色，并在开放词汇和无监督分割的边缘案例中达到了最先进的水平。", "conclusion": "DiSCO-3D成功解决了三维开放词汇子概念发现的更广泛问题，实现了对场景和用户查询的适应性三维语义分割，并在相关领域取得了领先性能。"}}
{"id": "2507.15106", "categories": ["cs.AI", "cs.RO", "F.2.2"], "pdf": "https://arxiv.org/pdf/2507.15106", "abs": "https://arxiv.org/abs/2507.15106", "authors": ["Xia Xu", "Jochen Triesch"], "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward", "comment": "13 pages, 5 figures", "summary": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems.", "AI": {"tldr": "本文提出了一种基于因果推断的新型内在奖励CAIS，旨在帮助强化学习智能体在嘈杂环境中鲁棒地发现其自身因果效力，从而解决传统基于关联的奖励在噪声环境下失效的问题。", "motivation": "人类婴儿能够鲁棒地发现自身的因果效力，但标准强化学习智能体由于依赖基于关联的奖励，在嘈杂、生态有效的场景中表现脆弱，无法有效识别自身影响。", "method": "引入了因果行动影响分数（CAIS）作为一种新颖的内在奖励。CAIS通过测量在给定行动下感知结果的习得分布p(h|a)与基线结果分布p(h)之间的1-Wasserstein距离来量化行动的影响。这种差异提供了一种鲁棒的奖励，能够将智能体的因果影响与混淆的环境噪声隔离开来。", "result": "在模拟的婴儿-移动玩具环境中，当移动玩具受到外部力影响时，基于关联的感知奖励完全失效，而CAIS使智能体能够过滤噪声，识别自身影响并学习正确的策略。此外，为CAIS学习到的高质量预测模型，在辅以惊喜信号后，成功再现了“消退爆发”现象。", "conclusion": "明确推断因果关系是发展鲁棒主体感的关键机制，为构建更具适应性的自主系统提供了心理学上合理的框架。"}}
{"id": "2507.15607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15607", "abs": "https://arxiv.org/abs/2507.15607", "authors": ["Yanbo Chen", "Yunzhe Tan", "Yaojia Wang", "Zhengzhe Xu", "Junbo Tan", "Xueqian Wang"], "title": "A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning", "comment": "8 pages, 10 figures", "summary": "Autonomous navigation of vehicle-trailer systems is crucial in environments\nlike airports, supermarkets, and concert venues, where various types of\ntrailers are needed to navigate with different payloads and conditions.\nHowever, accurately modeling such systems remains challenging, especially for\ntrailers with castor wheels. In this work, we propose a novel universal\nvehicle-trailer navigation system that integrates a hybrid nominal kinematic\nmodel--combining classical nonholonomic constraints for vehicles and neural\nnetwork-based trailer kinematics--with a lightweight online residual learning\nmodule to correct real-time modeling discrepancies and disturbances.\nAdditionally, we develop a model predictive control framework with a weighted\nmodel combination strategy that improves long-horizon prediction accuracy and\nensures safer motion planning. Our approach is validated through extensive\nreal-world experiments involving multiple trailer types and varying payload\nconditions, demonstrating robust performance without manual tuning or\ntrailer-specific calibration.", "AI": {"tldr": "本文提出了一种通用的车辆-拖车自主导航系统，结合了混合运动学模型（车辆经典非完整约束+拖车神经网络模型）和在线残差学习，并通过模型预测控制框架实现了鲁棒的实时性能。", "motivation": "车辆-拖车系统（尤其是有脚轮的拖车）的自主导航在机场、超市等环境中至关重要，但其准确建模面临挑战，特别是在不同载荷和条件下。", "method": "1. 提出了一种新颖的通用车辆-拖车导航系统。2. 集成了混合名义运动学模型：车辆采用经典非完整约束，拖车采用基于神经网络的运动学模型。3. 引入轻量级在线残差学习模块，用于校正实时建模差异和扰动。4. 开发了模型预测控制（MPC）框架，采用加权模型组合策略，以提高长期预测精度并确保更安全的运动规划。", "result": "通过涉及多种拖车类型和不同载荷条件的广泛真实世界实验验证了该方法，结果表明其性能稳健，无需手动调整或拖车特定校准。", "conclusion": "该系统为多样化的车辆-拖车系统提供了鲁棒且自适应的自主导航能力，有效克服了建模挑战和实时扰动。"}}
{"id": "2507.15781", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.15781", "abs": "https://arxiv.org/abs/2507.15781", "authors": ["Gian Carlo Maffettone", "Alain Boldini", "Mario di Bernardo", "Maurizio Porfiri"], "title": "Density control of multi-agent swarms via bio-inspired leader-follower plasticity", "comment": null, "summary": "The design of control systems for the spatial self-organization of mobile\nagents is an open challenge across several engineering domains, including swarm\nrobotics and synthetic biology. Here, we propose a bio-inspired leader-follower\nsolution, which is aware of energy constraints of mobile agents and is apt to\ndeal with large swarms. Akin to many natural systems, control objectives are\nformulated for the entire collective, and leaders and followers are allowed to\nplastically switch their role in time. We frame a density control problem,\nmodeling the agents' population via a system of nonlinear partial differential\nequations. This approach allows for a compact description that inherently\navoids the curse of dimensionality and improves analytical tractability. We\nderive analytical guarantees for the existence of desired steady-state\nsolutions and their local stability for one-dimensional and higher-dimensional\nproblems. We numerically validate our control methodology, offering support to\nthe effectiveness, robustness, and versatility of our proposed bio-inspired\ncontrol strategy.", "AI": {"tldr": "本文提出了一种受生物启发的领导者-跟随者控制策略，用于移动代理的空间自组织，该策略考虑了能量限制并适用于大型群体，通过非线性偏微分方程建模群体密度以实现分析可追踪性。", "motivation": "为移动代理（如群机器人和合成生物学）设计空间自组织控制系统是一个开放的挑战，尤其是在处理大型、受能量限制的群体时。", "method": "提出了一种受生物启发的领导者-跟随者解决方案，允许领导者和跟随者角色随时间动态切换，并考虑代理的能量约束。通过非线性偏微分方程系统建模代理群体密度，以实现紧凑描述、避免维度灾难并提高分析可追踪性。", "result": "推导了所需稳态解的存在性及其局部稳定性的分析保证，适用于一维和高维问题。数值验证了所提出的控制方法，支持其有效性、鲁棒性和多功能性。", "conclusion": "所提出的受生物启发的控制策略，通过基于密度的PDE模型和动态角色切换，为移动代理的空间自组织提供了一种有效、鲁棒且多功能的解决方案，尤其适用于大型和能量受限的群体。"}}
{"id": "2507.14749", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14749", "abs": "https://arxiv.org/abs/2507.14749", "authors": ["Wai Keen Vong", "Brenden M. Lake"], "title": "On the robustness of modeling grounded word learning through a child's egocentric input", "comment": null, "summary": "What insights can machine learning bring to understanding human language\nacquisition? Large language and multimodal models have achieved remarkable\ncapabilities, but their reliance on massive training datasets creates a\nfundamental mismatch with children, who succeed in acquiring language from\ncomparatively limited input. To help bridge this gap, researchers have\nincreasingly trained neural networks using data similar in quantity and quality\nto children's input. Taking this approach to the limit, Vong et al. (2024)\nshowed that a multimodal neural network trained on 61 hours of visual and\nlinguistic input extracted from just one child's developmental experience could\nacquire word-referent mappings. However, whether this approach's success\nreflects the idiosyncrasies of a single child's experience, or whether it would\nshow consistent and robust learning patterns across multiple children's\nexperiences was not explored. In this article, we applied automated speech\ntranscription methods to the entirety of the SAYCam dataset, consisting of over\n500 hours of video data spread across all three children. Using these automated\ntranscriptions, we generated multi-modal vision-and-language datasets for both\ntraining and evaluation, and explored a range of neural network configurations\nto examine the robustness of simulated word learning. Our findings demonstrate\nthat networks trained on automatically transcribed data from each child can\nacquire and generalize word-referent mappings across multiple network\narchitectures. These results validate the robustness of multimodal neural\nnetworks for grounded word learning, while highlighting the individual\ndifferences that emerge in how models learn when trained on each child's\ndevelopmental experiences.", "AI": {"tldr": "本文研究了多模态神经网络在模拟儿童语言习得中的词汇学习鲁棒性，通过使用来自多个儿童的真实世界数据，验证了模型在有限输入下学习和泛化词汇指代映射的能力。", "motivation": "大型语言模型依赖海量数据，与儿童通过有限输入习得语言的方式存在根本性差异。虽然有研究表明单儿童数据可用于词汇学习，但其成功是否具有普遍性及在多个儿童经验中的鲁棒性尚未被探索。", "method": "研究者对SAYCam数据集（包含三个儿童的500多小时视频数据）进行了自动化语音转录，生成了多模态视觉-语言数据集用于训练和评估。在此基础上，他们探索了一系列神经网络配置，以检验模拟词汇学习的鲁棒性。", "result": "研究发现，在每个儿童的自动转录数据上训练的网络，都能够在多种网络架构下习得并泛化词汇指代映射。这些结果证明了多模态神经网络在具身词汇学习方面的鲁棒性，同时也揭示了模型在基于不同儿童发展经验训练时出现的个体学习差异。", "conclusion": "该研究验证了多模态神经网络在模拟儿童具身词汇学习方面的鲁棒性，即便使用有限的、儿童式的数据也能成功学习。同时，它也强调了在模型基于不同个体发展经验学习时所出现的个体差异。"}}
{"id": "2507.14608", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14608", "abs": "https://arxiv.org/abs/2507.14608", "authors": ["Nandani Sharma", "Dinesh Singh"], "title": "Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition", "comment": null, "summary": "Facial expression recognition is crucial for human-computer interaction\napplications such as face animation, video surveillance, affective computing,\nmedical analysis, etc. Since the structure of facial attributes varies with\nfacial expressions, incorporating structural information into facial attributes\nis essential for facial expression recognition. In this paper, we propose\nExp-Graph, a novel framework designed to represent the structural relationships\namong facial attributes using graph-based modeling for facial expression\nrecognition. For facial attributes graph representation, facial landmarks are\nused as the graph's vertices. At the same time, the edges are determined based\non the proximity of the facial landmark and the similarity of the local\nappearance of the facial attributes encoded using the vision transformer.\nAdditionally, graph convolutional networks are utilized to capture and\nintegrate these structural dependencies into the encoding of facial attributes,\nthereby enhancing the accuracy of expression recognition. Thus, Exp-Graph\nlearns from the facial attribute graphs highly expressive semantic\nrepresentations. On the other hand, the vision transformer and graph\nconvolutional blocks help the framework exploit the local and global\ndependencies among the facial attributes that are essential for the recognition\nof facial expressions. We conducted comprehensive evaluations of the proposed\nExp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.\nThe model achieved recognition accuracies of 98.09\\%, 79.01\\%, and 56.39\\%,\nrespectively. These results indicate that Exp-Graph maintains strong\ngeneralization capabilities across both controlled laboratory settings and\nreal-world, unconstrained environments, underscoring its effectiveness for\npractical facial expression recognition applications.", "AI": {"tldr": "本文提出Exp-Graph，一个基于图的新型框架，通过图建模来表示面部属性间的结构关系，以提高面部表情识别的准确性。", "motivation": "面部表情识别在人机交互应用中至关重要，但面部属性结构随表情变化，因此将结构信息融入面部属性对于表情识别至关重要。", "method": "Exp-Graph框架使用面部标志点作为图的顶点，边根据标志点接近度和视觉Transformer编码的局部外观相似性确定。利用图卷积网络捕获并整合这些结构依赖，增强面部属性编码。视觉Transformer和图卷积块协同利用面部属性的局部和全局依赖性。", "result": "Exp-Graph模型在Oulu-CASIA、eNTERFACE05和AFEW三个基准数据集上进行了评估，识别准确率分别为98.09%、79.01%和56.39%。", "conclusion": "Exp-Graph在受控实验室和真实无约束环境下均表现出强大的泛化能力，证明其在实际面部表情识别应用中的有效性。"}}
{"id": "2507.15120", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15120", "abs": "https://arxiv.org/abs/2507.15120", "authors": ["Stefan Borgwardt", "Duy Nhu", "Gabriele Röger"], "title": "Automated planning with ontologies under coherence update semantics", "comment": null, "summary": "Standard automated planning employs first-order formulas under closed-world\nsemantics to achieve a goal with a given set of actions from an initial state.\nWe follow a line of research that aims to incorporate background knowledge into\nautomated planning problems, for example, by means of ontologies, which are\nusually interpreted under open-world semantics. We present a new approach for\nplanning with DL-Lite ontologies that combines the advantages of ontology-based\naction conditions provided by explicit-input knowledge and action bases (eKABs)\nand ontology-aware action effects under the coherence update semantics. We show\nthat the complexity of the resulting formalism is not higher than that of\nprevious approaches and provide an implementation via a polynomial compilation\ninto classical planning. An evaluation of existing and new benchmarks examines\nthe performance of a planning system on different variants of our compilation.", "AI": {"tldr": "本文提出了一种将DL-Lite本体（开放世界语义）融入自动化规划的新方法，通过结合eKABs和一致性更新语义来处理动作条件和效果，并证明其复杂度不高于现有方法，且可通过多项式编译转换为经典规划。", "motivation": "标准的自动化规划采用封闭世界语义下的谓词逻辑，难以有效整合背景知识，特别是通常基于开放世界语义解释的本体。本研究旨在弥补这一空白，将本体知识引入自动化规划问题。", "method": "提出了一种基于DL-Lite本体的规划新方法。该方法结合了显式输入知识和动作库（eKABs）提供的本体化动作条件，以及在一致性更新语义下的本体感知动作效果。通过多项式编译将其实现为经典规划问题，并对现有和新的基准进行了性能评估。", "result": "研究表明，所提出形式的计算复杂度不高于以往方法。通过将该方法多项式编译成经典规划，提供了一个可行的实现方案。在基准测试中，不同编译变体的规划系统性能得到了检验。", "conclusion": "本研究成功开发了一种将DL-Lite本体集成到自动化规划中的新方法，该方法在不增加计算复杂度的前提下，有效结合了本体知识进行动作条件和效果的建模，并通过编译实现了与经典规划的兼容性。"}}
{"id": "2507.15608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15608", "abs": "https://arxiv.org/abs/2507.15608", "authors": ["Johannes Hartwig", "Fabian Viessmann", "Dominik Henrich"], "title": "Optimizing Force Signals from Human Demonstrations of In-Contact Motions", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2024 (to appear)", "summary": "For non-robot-programming experts, kinesthetic guiding can be an intuitive\ninput method, as robot programming of in-contact tasks is becoming more\nprominent. However, imprecise and noisy input signals from human demonstrations\npose problems when reproducing motions directly or using the signal as input\nfor machine learning methods. This paper explores optimizing force signals to\ncorrespond better to the human intention of the demonstrated signal. We compare\ndifferent signal filtering methods and propose a peak detection method for\ndealing with first-contact deviations in the signal. The evaluation of these\nmethods considers a specialized error criterion between the input and the\nhuman-intended signal. In addition, we analyze the critical parameters'\ninfluence on the filtering methods. The quality for an individual motion could\nbe increased by up to \\SI{20}{\\percent} concerning the error criterion. The\nproposed contribution can improve the usability of robot programming and the\ninteraction between humans and robots.", "AI": {"tldr": "该研究旨在优化机器人示教中人体感应式引导产生的力信号，通过信号滤波和峰值检测方法提高信号质量，以更好地反映人类意图，从而改善机器人编程和人机交互。", "motivation": "对于非机器人编程专家而言，感应式引导是一种直观的输入方法，尤其是在接触式任务的机器人编程中越来越重要。然而，人类演示中不精确和嘈杂的输入信号在直接复现运动或作为机器学习方法的输入时会带来问题。", "method": "本文探索了优化力信号以更好地对应人类演示意图的方法。比较了不同的信号滤波方法，并提出了一种峰值检测方法来处理信号中的首次接触偏差。这些方法的评估采用了一种专门的输入信号与人类意图信号之间的误差准则，并分析了关键参数对滤波方法的影响。", "result": "针对误差准则，单个运动的质量最高可提高20%。", "conclusion": "所提出的贡献可以提高机器人编程的可用性以及人机交互的质量。"}}
{"id": "2507.05297", "categories": ["cs.AI", "cs.SY", "econ.TH", "eess.SY", "math.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.05297", "abs": "https://arxiv.org/abs/2507.05297", "authors": ["Zijun Meng"], "title": "Continuous Classification Aggregation", "comment": "9 pages; 2 figures", "summary": "We prove that any optimal, independent, and zero unanimous fuzzy\nclassification aggregation function of a continuum of individual\nclassifications of $m\\ge 3$ objects into $2\\le p\\le m$ types must be a weighted\narithmetic mean. We also provide a characterization for the case when $m=p=2$.", "AI": {"tldr": "本文证明了在特定条件下，最优、独立且零一致的模糊分类聚合函数必须是加权算术平均。", "motivation": "研究旨在刻画满足最优性、独立性和零一致性等特定属性的模糊分类聚合函数的结构。", "method": "通过数学证明来刻画满足给定条件的模糊分类聚合函数。", "result": "对于m≥3个对象和2≤p≤m个类型，任何最优、独立且零一致的模糊分类聚合函数都必须是加权算术平均。文章还对m=p=2的情况提供了单独的刻画。", "conclusion": "在给定的最优性、独立性和零一致性条件下，模糊分类聚合函数被唯一确定为加权算术平均形式，揭示了这类函数的基本结构。"}}
{"id": "2507.14758", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14758", "abs": "https://arxiv.org/abs/2507.14758", "authors": ["Luyi Ma", "Wanjia Zhang", "Kai Zhao", "Abhishek Kulkarni", "Lalitesh Morishetti", "Anjana Ganesh", "Ashish Ranjan", "Aashika Padmanabhan", "Jianpeng Xu", "Jason Cho", "Praveen Kanumala", "Kaushiki Nag", "Sumit Dutta", "Kamiya Motwani", "Malay Patel", "Evren Korpeoglu", "Sushant Kumar", "Kannan Achan"], "title": "GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization", "comment": "10 pages, 5 figures, The ACM Conference on Recommender Systems\n  (RecSys) 2025", "summary": "Generative models have recently demonstrated strong potential in\nmulti-behavior recommendation systems, leveraging the expressive power of\ntransformers and tokenization to generate personalized item sequences. However,\ntheir adoption is hindered by (1) the lack of explicit information for token\nreasoning, (2) high computational costs due to quadratic attention complexity\nand dense sequence representations after tokenization, and (3) limited\nmulti-scale modeling over user history. In this work, we propose GRACE\n(Generative Recommendation via journey-aware sparse Attention on\nChain-of-thought tokEnization), a novel generative framework for multi-behavior\nsequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)\ntokenization method that encodes user-item interactions with explicit\nattributes from product knowledge graphs (e.g., category, brand, price) over\nsemantic tokenization, enabling interpretable and behavior-aligned generation.\nTo address the inefficiency of standard attention, we design a Journey-Aware\nSparse Attention (JSA) mechanism, which selectively attends to compressed,\nintra-, inter-, and current-context segments in the tokenized sequence.\nExperiments on two real-world datasets show that GRACE significantly\noutperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and\n+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home\ndomain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces\nattention computation by up to 48% with long sequences.", "AI": {"tldr": "本文提出GRACE，一种生成式多行为推荐框架，通过引入CoT分词和稀疏注意力机制，显著提升了推荐性能，并降低了计算成本。", "motivation": "当前的生成式多行为推荐系统面临三大挑战：1) 缺乏显式信息进行Token推理；2) 注意力机制的二次复杂度导致计算成本高昂；3) 用户历史行为的多尺度建模能力有限。", "method": "本文提出了GRACE（Generative Recommendation via journey-aware sparse Attention on Chain-of-thought tokEnization）。它采用混合链式思考（CoT）分词方法，将用户-物品交互与来自产品知识图谱的显式属性（如类别、品牌、价格）一同编码，实现可解释和行为对齐的生成。为解决标准注意力的效率问题，设计了旅程感知稀疏注意力（JSA）机制，选择性地关注分词序列中的压缩、内部、外部和当前上下文片段。", "result": "GRACE在两个真实世界数据集上显著优于现有SOTA基线：在Home领域，HR@10和NDCG@10分别提升高达106.9%和106.7%；在Electronics领域，HR@10提升22.1%。此外，GRACE在长序列上将注意力计算量减少了高达48%。", "conclusion": "GRACE是一个有效且高效的生成式多行为序列推荐框架，它通过创新的分词和稀疏注意力机制，成功克服了现有生成模型在解释性、计算效率和多尺度建模方面的局限性。"}}
{"id": "2507.14613", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14613", "abs": "https://arxiv.org/abs/2507.14613", "authors": ["Guoping Xu", "Christopher Kabat", "You Zhang"], "title": "Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2", "comment": "24 pages, 6 figures", "summary": "Recent advances in medical image segmentation have been driven by deep\nlearning; however, most existing methods remain limited by modality-specific\ndesigns and exhibit poor adaptability to dynamic medical imaging scenarios. The\nSegment Anything Model 2 (SAM2) and its related variants, which introduce a\nstreaming memory mechanism for real-time video segmentation, present new\nopportunities for prompt-based, generalizable solutions. Nevertheless, adapting\nthese models to medical video scenarios typically requires large-scale datasets\nfor retraining or transfer learning, leading to high computational costs and\nthe risk of catastrophic forgetting. To address these challenges, we propose\nDD-SAM2, an efficient adaptation framework for SAM2 that incorporates a\nDepthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature\nextraction with minimal parameter overhead. This design enables effective\nfine-tuning of SAM2 on medical videos with limited training data. Unlike\nexisting adapter-based methods focused solely on static images, DD-SAM2 fully\nexploits SAM2's streaming memory for medical video object tracking and\nsegmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)\nand EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior\nperformance, achieving Dice scores of 0.93 and 0.97, respectively. To the best\nof our knowledge, this work provides an initial attempt at systematically\nexploring adapter-based SAM2 fine-tuning for medical video segmentation and\ntracking. Code, datasets, and models will be publicly available at\nhttps://github.com/apple1986/DD-SAM2.", "AI": {"tldr": "本文提出DD-SAM2，一种高效的适配器框架，用于将SAM2模型适应于医疗视频分割和跟踪，解决了现有方法在适应性和计算成本上的限制，并在有限数据下取得了优异性能。", "motivation": "当前医学图像分割方法多为特定模态设计，对动态医疗影像场景适应性差。虽然SAM2及其变体引入了流式记忆机制，但将其应用于医疗视频通常需要大规模数据集进行再训练或迁移学习，导致计算成本高昂且存在灾难性遗忘的风险。", "method": "提出DD-SAM2框架，通过引入深度可分离空洞适配器（DD-Adapter）来增强多尺度特征提取，同时保持极小的参数开销。该设计使得SAM2能够在有限的医疗视频训练数据下进行有效微调，并充分利用SAM2的流式记忆机制进行医疗视频目标跟踪和分割。", "result": "在TrackRad2025（肿瘤分割）和EchoNet-Dynamic（左心室跟踪）数据集上进行了全面评估，分别取得了0.93和0.97的Dice分数，表现出卓越的性能。", "conclusion": "该工作首次系统地探索了基于适配器的SAM2微调方法，用于医疗视频分割和跟踪，为在有限数据下高效适应大型基础模型提供了新的途径。"}}
{"id": "2507.15140", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15140", "abs": "https://arxiv.org/abs/2507.15140", "authors": ["Mohammad Mashayekhi", "Sara Ahmadi Majd", "Arian AmirAmjadi", "Parsa Hosseini"], "title": "Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis", "comment": null, "summary": "The diagnosis of oral diseases presents a problematic clinical challenge,\ncharacterized by a wide spectrum of pathologies with overlapping\nsymptomatology. To address this, we developed Clinical Semantic Intelligence\n(CSI), a novel artificial intelligence framework that diagnoses 118 different\noral diseases by computationally modeling the cognitive processes of an expert\nclinician. Our core hypothesis is that moving beyond simple pattern matching to\nemulate expert reasoning is critical to building clinically useful diagnostic\naids.\n  CSI's architecture integrates a fine-tuned multimodal CLIP model with a\nspecialized ChatGLM-6B language model. This system executes a Hierarchical\nDiagnostic Reasoning Tree (HDRT), a structured framework that distills the\nsystematic, multi-step logic of differential diagnosis. The framework operates\nin two modes: a Fast Mode for rapid screening and a Standard Mode that\nleverages the full HDRT for an interactive and in-depth diagnostic workup.\n  To train and validate our system, we curated a primary dataset of 4,310\nimages, supplemented by an external hold-out set of 176 images for final\nvalidation. A clinically-informed augmentation strategy expanded our training\ndata to over 30,000 image-text pairs. On a 431-image internal test set, CSI's\nFast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the\nHDRT-driven Standard Mode. The performance gain is directly attributable to the\nhierarchical reasoning process. Herein, we detail the architectural philosophy,\ndevelopment, and rigorous evaluation of the CSI framework.", "AI": {"tldr": "本文提出了一种名为临床语义智能（CSI）的新型人工智能框架，通过模拟专家临床推理过程，诊断118种不同的口腔疾病，旨在解决口腔疾病诊断的复杂性和症状重叠问题。", "motivation": "口腔疾病诊断是一个具有挑战性的临床问题，其特点是病理学种类繁多且症状重叠。研究动机在于，超越简单的模式匹配，模拟专家推理对于构建临床有用的诊断辅助工具至关重要。", "method": "CSI框架整合了微调的多模态CLIP模型和专门的ChatGLM-6B语言模型。该系统执行一个分层诊断推理树（HDRT），它概括了鉴别诊断的系统性多步骤逻辑。框架有两种模式：用于快速筛查的快速模式和利用完整HDRT进行交互式深入诊断的标准模式。系统使用4310张图像的原始数据集进行训练和验证，并通过临床知情的增强策略扩展到超过30000对图像-文本对。", "result": "在431张图像的内部测试集上，CSI的快速模式实现了73.4%的准确率，而HDRT驱动的标准模式则将准确率提高到89.5%。性能的提升直接归因于分层推理过程。", "conclusion": "CSI框架通过模拟专家临床推理，显著提高了口腔疾病的诊断准确性，证明了分层诊断推理在构建临床有用AI诊断工具方面的有效性。"}}
{"id": "2507.15649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15649", "abs": "https://arxiv.org/abs/2507.15649", "authors": ["Haocheng Xu", "Haodong Zhang", "Zhenghan Chen", "Rong Xiong"], "title": "EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation", "comment": null, "summary": "To support humanoid robots in performing manipulation tasks, it is essential\nto study stable standing while accommodating upper-body motions. However, the\nlimited controllable range of humanoid robots in a standing position affects\nthe stability of the entire body. Thus we introduce a reinforcement learning\nbased framework for humanoid robots to imitate human upper-body motions while\nmaintaining overall stability. Our approach begins with designing a retargeting\nnetwork that generates a large-scale upper-body motion dataset for training the\nreinforcement learning (RL) policy, which enables the humanoid robot to track\nupper-body motion targets, employing domain randomization for enhanced\nrobustness. To avoid exceeding the robot's execution capability and ensure\nsafety and stability, we propose an Executable Motion Prior (EMP) module, which\nadjusts the input target movements based on the robot's current state. This\nadjustment improves standing stability while minimizing changes to motion\namplitude. We evaluate our framework through simulation and real-world tests,\ndemonstrating its practical applicability.", "AI": {"tldr": "本文提出一个基于强化学习的框架，使人形机器人在模仿人类上半身动作的同时保持整体稳定性，通过运动重定向网络生成数据并引入可执行运动先验（EMP）模块进行运动调整。", "motivation": "为了支持人形机器人执行操纵任务，需要在适应上半身运动的同时保持稳定站立。然而，人形机器人站立姿态下有限的控制范围会影响其整体稳定性。", "method": "1. 设计一个运动重定向网络，生成大规模上半身运动数据集，用于训练强化学习（RL）策略。2. 训练RL策略以跟踪上半身运动目标，并采用领域随机化增强鲁棒性。3. 提出一个可执行运动先验（EMP）模块，根据机器人当前状态调整输入目标运动，以避免超出机器人执行能力，确保安全和稳定性，并最小化运动幅度的改变。", "result": "通过仿真和真实世界测试评估了所提出的框架，证明了其实际适用性。", "conclusion": "该框架使人形机器人能够有效地模仿人类上半身动作，同时保持整体稳定性，并在实际应用中表现出良好的性能。"}}
{"id": "2507.14815", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14815", "abs": "https://arxiv.org/abs/2507.14815", "authors": ["Shoutao Guo", "Shaolei Zhang", "Qingkai Fang", "Zhengrui Ma", "Min Zhang", "Yang Feng"], "title": "FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing", "comment": "The code is at https://github.com/ictnlp/FastLongSpeech. This model\n  is at https://huggingface.co/ICTNLP/FastLongSpeech. The dataset is at\n  https://huggingface.co/datasets/ICTNLP/LongSpeech-Eval", "summary": "The rapid advancement of Large Language Models (LLMs) has spurred significant\nprogress in Large Speech-Language Models (LSLMs), enhancing their capabilities\nin both speech understanding and generation. While existing LSLMs often\nconcentrate on augmenting speech generation or tackling a diverse array of\nshort-speech tasks, the efficient processing of long-form speech remains a\ncritical yet underexplored challenge. This gap is primarily attributed to the\nscarcity of long-speech training datasets and the high computational costs\nassociated with long sequences. To address these limitations, we introduce\nFastLongSpeech, a novel framework designed to extend LSLM capabilities for\nefficient long-speech processing without necessitating dedicated long-speech\ntraining data. FastLongSpeech incorporates an iterative fusion strategy that\ncan compress excessively long-speech sequences into manageable lengths. To\nadapt LSLMs for long-speech inputs, it introduces a dynamic compression\ntraining approach, which exposes the model to short-speech sequences at varying\ncompression ratios, thereby transferring the capabilities of LSLMs to\nlong-speech tasks. To assess the long-speech capabilities of LSLMs, we develop\na long-speech understanding benchmark called LongSpeech-Eval. Experiments show\nthat our method exhibits strong performance in both long-speech and\nshort-speech tasks, while greatly improving inference efficiency.", "AI": {"tldr": "本文提出FastLongSpeech框架，旨在使大型语音语言模型（LSLMs）无需专用长语音训练数据即可高效处理长篇语音，通过迭代融合和动态压缩训练实现，并引入LongSpeech-Eval基准进行评估。", "motivation": "现有大型语音语言模型（LSLMs）主要关注语音生成或短语音任务，对长篇语音的高效处理仍是一个未被充分探索的挑战。这主要是因为缺乏长语音训练数据集以及处理长序列的高计算成本。", "method": "本文提出了FastLongSpeech框架，包含以下策略：1. 迭代融合策略：将过长的语音序列压缩成可管理的长度。2. 动态压缩训练方法：通过将模型暴露于不同压缩比的短语音序列，将LSLMs的能力迁移到长语音任务。3. 开发了LongSpeech-Eval长语音理解基准，用于评估LSLMs的长语音处理能力。", "result": "实验结果表明，FastLongSpeech方法在长语音和短语音任务中均表现出强大的性能，同时显著提高了推理效率。", "conclusion": "FastLongSpeech框架成功地扩展了LSLMs处理长篇语音的能力，无需专门的长语音训练数据，并在长短语音任务中展现出优异的性能和效率。"}}
{"id": "2507.14632", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14632", "abs": "https://arxiv.org/abs/2507.14632", "authors": ["Haiquan Wen", "Tianxiao Li", "Zhenglin Huang", "Yiwei He", "Guangliang Cheng"], "title": "BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM", "comment": null, "summary": "Recent advances in generative AI have dramatically improved image and video\nsynthesis capabilities, significantly increasing the risk of misinformation\nthrough sophisticated fake content. In response, detection methods have evolved\nfrom traditional approaches to multimodal large language models (MLLMs),\noffering enhanced transparency and interpretability in identifying synthetic\nmedia. However, current detection systems remain fundamentally limited by their\nsingle-modality design. These approaches analyze images or videos separately,\nmaking them ineffective against synthetic content that combines multiple media\nformats. To address these challenges, we introduce \\textbf{BusterX++}, a novel\nframework designed specifically for cross-modal detection and explanation of\nsynthetic media. Our approach incorporates an advanced reinforcement learning\n(RL) post-training strategy that eliminates cold-start. Through Multi-stage\nTraining, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and\nsubstantial performance improvements. To enable comprehensive evaluation, we\nalso present \\textbf{GenBuster++}, a cross-modal benchmark leveraging\nstate-of-the-art image and video generation techniques. This benchmark\ncomprises 4,000 images and video clips, meticulously curated by human experts\nusing a novel filtering methodology to ensure high quality, diversity, and\nreal-world applicability. Extensive experiments demonstrate the effectiveness\nand generalizability of our approach.", "AI": {"tldr": "该论文提出了BusterX++框架，用于跨模态检测和解释合成媒体，并引入了GenBuster++基准测试数据集，以应对现有单模态检测系统的局限性。", "motivation": "生成式AI的进步显著提高了图像和视频合成能力，增加了虚假信息传播的风险。现有检测系统受限于其单模态设计，无法有效应对结合多种媒体格式的合成内容。", "method": "引入BusterX++，一个用于跨模态合成媒体检测和解释的新型框架。该方法采用先进的强化学习(RL)后训练策略，消除了冷启动问题，并通过多阶段训练、思维奖励和混合推理实现性能提升。同时，提出了GenBuster++，一个利用最先进图像和视频生成技术构建的跨模态基准测试数据集，包含4,000张图像和视频片段，经过人工专家筛选，确保高质量、多样性和实际适用性。", "result": "BusterX++实现了稳定且显著的性能改进。GenBuster++基准测试数据集能够进行全面的评估。广泛的实验证明了所提出方法的有效性和泛化能力。", "conclusion": "BusterX++框架有效解决了合成媒体的跨模态检测和解释挑战，并通过结合RL后训练和多策略推理实现了卓越性能。GenBuster++基准测试为该领域提供了全面的评估工具，推动了合成媒体检测技术的发展。"}}
{"id": "2507.15143", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15143", "abs": "https://arxiv.org/abs/2507.15143", "authors": ["Abderaouf Bahi", "Amel Ourici"], "title": "Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City", "comment": null, "summary": "This paper investigates the feasibility of human mobility in The Line, a\nproposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess\nwhether citizens can move freely within this unprecedented urban topology, we\ndevelop a hybrid simulation framework that integrates agent-based modeling,\nreinforcement learning, supervised learning, and graph neural networks. The\nsimulation captures multi-modal transportation behaviors across 50 vertical\nlevels and varying density scenarios using both synthetic data and real-world\ntraces from high-density cities. Our experiments reveal that with the full\nAI-integrated architecture, agents achieved an average commute time of 7.8 to\n8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index\nof over 91 percent, even during peak congestion periods. Ablation studies\nconfirmed that the removal of intelligent modules such as reinforcement\nlearning or graph neural networks significantly degrades performance, with\ncommute times increasing by up to 85 percent and reachability falling below 70\npercent. Environmental modeling further demonstrated low energy consumption and\nminimal CO2 emissions when electric modes are prioritized. The findings suggest\nthat freedom of movement is not only conceptually achievable in The Line, but\nalso operationally realistic if supported by adaptive AI systems, sustainable\ninfrastructure, and real-time feedback loops.", "AI": {"tldr": "本文通过混合模拟框架评估了沙特阿拉伯NEOM“The Line”线性城市中人类移动的可行性，发现通过AI集成系统可实现高效且可持续的自由移动。", "motivation": "研究旨在评估在“The Line”这一前所未有的线性城市拓扑结构中，公民是否能够自由移动。", "method": "开发了一个混合模拟框架，整合了基于代理建模、强化学习、监督学习和图神经网络。该模拟捕获了50个垂直层面和不同密度场景下的多模式交通行为，并使用了合成数据和来自高密度城市的真实数据。", "result": "在完整的AI集成架构下，代理的平均通勤时间为7.8至8.4分钟，满意度超过89%，可达性指数超过91%，即使在高峰拥堵期间也是如此。消融研究表明，移除智能模块（如强化学习或图神经网络）会显著降低性能，通勤时间增加高达85%，可达性降至70%以下。环境建模还表明，优先使用电动模式时能耗低，二氧化碳排放量极少。", "conclusion": "研究结果表明，“The Line”中的移动自由不仅在概念上是可实现的，而且在操作上也是现实的，前提是得到自适应AI系统、可持续基础设施和实时反馈循环的支持。"}}
{"id": "2507.15677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15677", "abs": "https://arxiv.org/abs/2507.15677", "authors": ["Huayue Liang", "Yanbo Chen", "Hongyang Cheng", "Yanzhao Yu", "Shoujie Li", "Junbo Tan", "Xueqian Wang", "Long Zeng"], "title": "Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms", "comment": null, "summary": "Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant\nmotion. Still, the inherent properties of cables, such as resilience,\nhysteresis, and friction, often lead to particular difficulties in modeling and\ncontrol. This paper proposes a model predictive control (MPC) method that\nrelies exclusively on input-output data, without a physical model, to improve\nthe control accuracy of FCRAs. First, we develop an implicit model based on\ninput-output data and integrate it into an MPC optimization framework. Second,\na data selection algorithm (DSA) is introduced to filter the data that best\ncharacterize the system, thereby reducing the solution time per step to\napproximately 4 ms, which is an improvement of nearly 80%. Lastly, the\ninfluence of hyperparameters on tracking error is investigated through\nsimulation. The proposed method has been validated on a real FCRA platform,\nincluding five-point positioning accuracy tests, a five-point response tracking\ntest, and trajectory tracking for letter drawing. The results demonstrate that\nthe average positioning accuracy is approximately 2.070 mm. Moreover, compared\nto the PID method with an average tracking error of 1.418{\\deg}, the proposed\nmethod achieves an average tracking error of 0.541{\\deg}.", "AI": {"tldr": "本文提出了一种纯数据驱动的模型预测控制（MPC）方法，用于提高柔性缆绳驱动机械臂（FCRAs）的控制精度，通过隐式模型和数据选择算法，显著提升了定位和跟踪性能。", "motivation": "柔性缆绳驱动机械臂（FCRAs）虽然灵活，但缆绳固有的弹性、滞后和摩擦特性给其建模和控制带来了困难。", "method": "1. 提出了一种完全依赖输入输出数据的模型预测控制（MPC）方法，无需物理模型。\n2. 基于输入输出数据开发了隐式模型，并将其集成到MPC优化框架中。\n3. 引入了数据选择算法（DSA）以筛选最佳表征系统的数据，从而将每步求解时间缩短了近80%。\n4. 通过仿真研究了超参数对跟踪误差的影响。", "result": "1. 数据选择算法将每步求解时间缩短至约4毫秒，提升了近80%。\n2. 在真实FCRA平台上验证，平均定位精度约为2.070毫米。\n3. 与PID方法（平均跟踪误差1.418度）相比，所提方法实现了0.541度的平均跟踪误差，显著提升了控制精度。", "conclusion": "所提出的数据驱动MPC方法在不依赖物理模型的情况下，有效解决了柔性缆绳驱动机械臂的控制难题，显著提高了其定位和轨迹跟踪精度及控制效率，优于传统PID方法。"}}
{"id": "2507.14819", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14819", "abs": "https://arxiv.org/abs/2507.14819", "authors": ["Akriti Jain", "Pritika Ramu", "Aparna Garimella", "Apoorv Saxena"], "title": "Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ntransforming text descriptions or tables to data visualizations via\ninstruction-tuning methods. However, it is not straightforward to apply these\nmethods directly for a more real-world use case of visualizing data from long\ndocuments based on user-given intents, as opposed to the user pre-selecting the\nrelevant content manually. We introduce the task of intent-based chart\ngeneration from documents: given a user-specified intent and document(s), the\ngoal is to generate a chart adhering to the intent and grounded on the\ndocument(s) in a zero-shot setting. We propose an unsupervised, two-staged\nframework in which an LLM first extracts relevant information from the\ndocument(s) by decomposing the intent and iteratively validates and refines\nthis data. Next, a heuristic-guided module selects an appropriate chart type\nbefore final code generation. To assess the data accuracy of the generated\ncharts, we propose an attribution-based metric that uses a structured textual\nrepresentation of charts, instead of relying on visual decoding metrics that\noften fail to capture the chart data effectively. To validate our approach, we\ncurate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from\ntwo domains, finance and scientific, in contrast to the existing datasets that\nare largely limited to parallel text descriptions/ tables and their\ncorresponding charts. We compare our approach with baselines using single-shot\nchart generation using LLMs and query-based retrieval methods; our method\noutperforms by upto $9$ points and $17$ points in terms of chart data accuracy\nand chart type respectively over the best baselines.", "AI": {"tldr": "本文提出了一种从长文档中根据用户意图生成图表的任务，并提出了一个无监督的两阶段框架，利用LLM提取和验证数据，并结合启发式模块选择图表类型和生成代码，显著优于现有基线。", "motivation": "现有的大语言模型（LLMs）在将文本描述或表格转换为数据可视化方面表现出色，但这些方法不适用于更真实的场景，即从长文档中根据用户意图生成图表，而无需用户手动预选相关内容。", "method": "本文提出了一个无监督的两阶段框架：首先，LLM通过分解意图并迭代验证和完善数据，从文档中提取相关信息；其次，一个启发式引导模块选择合适的图表类型，然后生成最终代码。为了评估生成图表的数据准确性，本文提出了一种基于归因的度量方法。此外，本文还策划了一个包含1,242个<意图, 文档, 图表>元组的新数据集。", "result": "本文方法在图表数据准确性方面比最佳基线高出9个点，在图表类型方面高出17个点，表现优于使用LLM进行单次图表生成和基于查询检索的基线方法。", "conclusion": "本文引入了从文档中基于意图生成图表的任务，并提出了一个有效的无监督两阶段框架，该框架能从长文档中准确地提取数据并生成相应的图表，显著超越了现有基线方法。"}}
{"id": "2507.14643", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14643", "abs": "https://arxiv.org/abs/2507.14643", "authors": ["Jifeng Shen", "Haibo Zhan", "Shaohua Dong", "Xin Zuo", "Wankou Yang", "Haibin Ling"], "title": "Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection", "comment": "submitted on 30/4/2025, Under Major Revision", "summary": "Modern multispectral feature fusion for object detection faces two critical\nlimitations: (1) Excessive preference for local complementary features over\ncross-modal shared semantics adversely affects generalization performance; and\n(2) The trade-off between the receptive field size and computational complexity\npresent critical bottlenecks for scalable feature modeling. Addressing these\nissues, a novel Multispectral State-Space Feature Fusion framework, dubbed\nMS2Fusion, is proposed based on the state space model (SSM), achieving\nefficient and effective fusion through a dual-path parametric interaction\nmechanism. More specifically, the first cross-parameter interaction branch\ninherits the advantage of cross-attention in mining complementary information\nwith cross-modal hidden state decoding in SSM. The second shared-parameter\nbranch explores cross-modal alignment with joint embedding to obtain\ncross-modal similar semantic features and structures through parameter sharing\nin SSM. Finally, these two paths are jointly optimized with SSM for fusing\nmultispectral features in a unified framework, allowing our MS2Fusion to enjoy\nboth functional complementarity and shared semantic space. In our extensive\nexperiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our\nMS2Fusion significantly outperforms other state-of-the-art multispectral object\ndetection methods, evidencing its superiority. Moreover, MS2Fusion is general\nand applicable to other multispectral perception tasks. We show that, even\nwithout specific design, MS2Fusion achieves state-of-the-art results on RGB-T\nsemantic segmentation and RGBT salient object detection, showing its\ngenerality. The source code will be available at\nhttps://github.com/61s61min/MS2Fusion.git.", "AI": {"tldr": "本文提出了一种名为MS2Fusion的新型多光谱状态空间特征融合框架，通过双路径参数交互机制，有效解决了多光谱目标检测中局部互补特征过度偏好和感受野与计算复杂度的权衡问题，实现了高效且通用的特征融合。", "motivation": "当前多光谱特征融合面临两大挑战：1) 过度偏好局部互补特征而非跨模态共享语义，损害泛化性能；2) 感受野大小与计算复杂度之间的权衡限制了可扩展特征建模。", "method": "MS2Fusion基于状态空间模型(SSM)提出了一种双路径参数交互机制：1) 交叉参数交互分支：利用SSM中的跨模态隐藏状态解码，挖掘互补信息（继承了交叉注意力的优势）；2) 共享参数分支：通过SSM中的参数共享实现联合嵌入，探索跨模态对齐，获取跨模态相似的语义特征和结构。两路径在统一框架下通过SSM共同优化，实现功能互补和共享语义空间。", "result": "在FLIR、M3FD和LLVIP等主流基准测试中，MS2Fusion显著优于其他最先进的多光谱目标检测方法。此外，MS2Fusion具有通用性，在未进行特定设计的情况下，在RGB-T语义分割和RGBT显著目标检测任务上也取得了最先进的结果。", "conclusion": "MS2Fusion通过其新颖的双路径参数交互机制，有效解决了多光谱特征融合中的关键问题，实现了卓越的性能和广泛的通用性，不仅在多光谱目标检测中表现出色，也适用于其他多光谱感知任务。"}}
{"id": "2507.15225", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15225", "abs": "https://arxiv.org/abs/2507.15225", "authors": ["Yichi Zhou", "Jianqiu Zhao", "Yongxin Zhang", "Bohan Wang", "Siran Wang", "Luoxin Chen", "Jiahui Wang", "Haowei Chen", "Allan Jie", "Xinbo Zhang", "Haocheng Wang", "Luong Trung", "Rong Ye", "Phan Nhat Hoang", "Huishuai Zhang", "Peng Sun", "Hang Li"], "title": "Solving Formal Math Problems by Decomposition and Iterative Reflection", "comment": null, "summary": "General-purpose Large Language Models (LLMs) have achieved remarkable success\nin intelligence, performing comparably to human experts on complex reasoning\ntasks such as coding and mathematical reasoning. However, generating formal\nproofs in specialized languages like Lean 4 remains a significant challenge for\nthese models, limiting their application in complex theorem proving and\nautomated verification. Current approaches typically require specializing\nmodels through fine-tuning on dedicated formal corpora, incurring high costs\nfor data collection and training. In this work, we introduce \\textbf{Delta\nProver}, an agent-based framework that orchestrates the interaction between a\ngeneral-purpose LLM and the Lean 4 proof environment. Delta Prover leverages\nthe reflection and reasoning capabilities of general-purpose LLMs to\ninteractively construct formal proofs in Lean 4, circumventing the need for\nmodel specialization. At its core, the agent integrates two novel,\ninterdependent components: an algorithmic framework for reflective\ndecomposition and iterative proof repair, and a custom Domain-Specific Language\n(DSL) built upon Lean 4 for streamlined subproblem management. \\textbf{Delta\nProver achieves a state-of-the-art 95.9\\% success rate on the miniF2F-test\nbenchmark, surpassing all existing approaches, including those requiring model\nspecialization.} Furthermore, Delta Prover exhibits a significantly stronger\ntest-time scaling law compared to standard Best-of-N proof strategies.\nCrucially, our findings demonstrate that general-purpose LLMs, when guided by\nan effective agentic structure, possess substantial untapped theorem-proving\ncapabilities. This presents a computationally efficient alternative to\nspecialized models for robust automated reasoning in formal environments.", "AI": {"tldr": "Delta Prover是一个基于代理的框架，它利用通用大型语言模型（LLMs）与Lean 4证明环境交互，实现了在miniF2F-test基准上95.9%的SOTA成功率，无需模型特殊化。", "motivation": "通用LLMs在复杂推理任务上表现出色，但在生成Lean 4等专业语言的正式证明方面仍面临挑战，且现有方法通常需要对模型进行昂贵的数据收集和训练特殊化。", "method": "Delta Prover采用代理框架，协调通用LLM与Lean 4环境的交互。核心方法包括：1) 一种用于反射性分解和迭代证明修复的算法框架；2) 一个基于Lean 4构建的自定义领域特定语言（DSL），用于简化子问题管理。该方法使通用LLMs能够交互式地构建形式证明，避免了模型特殊化。", "result": "Delta Prover在miniF2F-test基准上取得了95.9%的成功率，超越了所有现有方法（包括需要模型特殊化的方法）。此外，与标准的Best-of-N证明策略相比，Delta Prover表现出显著更强的测试时缩放定律。", "conclusion": "研究表明，通用LLMs在有效代理结构引导下，具有巨大的未开发定理证明潜力。这为在形式环境中进行强大的自动化推理提供了一种计算效率更高的替代方案，而非依赖于专门模型。"}}
{"id": "2507.15693", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15693", "abs": "https://arxiv.org/abs/2507.15693", "authors": ["Georges Chebly", "Spencer Little", "Nisal Perera", "Aliya Abedeen", "Ken Suzuki", "Donghyun Kim"], "title": "Strong, Accurate, and Low-Cost Robot Manipulator", "comment": null, "summary": "This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed\nto achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,\nand sub-millimeter repeatability - at a material cost under $215. As an\naccessible robot for broad applications across classroom education to AI\nexperiments, Forte pushes forward the performance limitations of existing\nlow-cost educational arms. We introduce a cost-effective mechanical design that\ncombines capstan-based cable drives, timing belts, simple tensioning\nmechanisms, and lightweight 3D-printed structures, along with topology\noptimization for structural stiffness. Through careful drivetrain engineering,\nwe minimize backlash and maintain control fidelity without relying on\nhigh-power electronics or expensive manufacturing processes. Experimental\nvalidation demonstrates that Forte achieves high repeatability and load\ncapacity, offering a compelling robotic platform for both classroom instruction\nand advanced robotics research.", "AI": {"tldr": "本文介绍了Forte，一款完全可3D打印的六自由度机械臂，其设计旨在以低于215美元的材料成本实现接近工业级的性能（0.63公斤负载、0.467米臂展、亚毫米级重复精度）。", "motivation": "现有低成本教育机械臂的性能存在局限性，Forte旨在突破这些限制，为课堂教育到AI实验等广泛应用提供一个可负担的机器人。", "method": "Forte采用了成本效益高的机械设计，结合了绞盘式线缆驱动、同步带、简单的张紧机制和轻量化的3D打印结构，并通过拓扑优化提高结构刚度。通过精心的传动系统工程，在不依赖高功率电子设备或昂贵制造工艺的情况下，最大程度地减少了反冲并保持了控制保真度。", "result": "实验验证表明，Forte实现了高重复性和负载能力，具体性能包括0.63公斤有效载荷、0.467米臂展和亚毫米级重复精度，材料成本低于215美元。", "conclusion": "Forte提供了一个引人注目的机器人平台，适用于课堂教学和高级机器人研究。"}}
{"id": "2507.14849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14849", "abs": "https://arxiv.org/abs/2507.14849", "authors": ["Yifei Wang"], "title": "Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding", "comment": null, "summary": "Reasoning distillation has emerged as an effective approach to enhance the\nreasoning capabilities of smaller language models. However, the impact of\nlarge-scale reasoning distillation on other critical abilities, particularly\nin-context retrieval and reasoning, remains unexplored. This gap in\nunderstanding is particularly significant given the increasing importance of\nRetrieval-Augmented Generation (RAG) systems, where efficient acquisition and\nutilization of contextual information are paramount for generating reliable\nresponses. Motivated by the need to understand how the extended long-CoT\nprocess influences long-context comprehension, we conduct a comprehensive\ninvestigation using a series of open-source models distilled from Deepseek-R1,\nrenowned for its exceptional reasoning capabilities. Our study focuses on\nevaluating these models' performance in extracting and integrating relevant\ninformation from extended contexts through multi-document question and\nanswering tasks. Through rigorous experimentation, we demonstrate that\ndistilled reasoning patterns significantly improve long-context understanding.\nOur analysis reveals that distillation fosters greater long-context awareness\nby promoting more detailed and explicit reasoning processes during context\nanalysis and information parsing. This advancement effectively mitigates the\npersistent \"lost in the middle\" issue that has hindered long-context models.", "AI": {"tldr": "推理蒸馏显著提升了小型模型在长文本理解和上下文检索方面的能力，有效缓解了“信息丢失”问题。", "motivation": "推理蒸馏对大型语言模型（LLMs）的推理能力提升显著，但其对小模型上下文检索和推理能力（尤其在RAG系统中）的影响尚不明确。研究旨在探究长链式思考（long-CoT）过程如何影响长文本理解。", "method": "使用一系列从Deepseek-R1蒸馏而来的开源模型进行全面调查。通过多文档问答任务，评估这些模型从扩展上下文中提取和整合相关信息的能力。", "result": "蒸馏的推理模式显著改善了模型的长文本理解能力。分析表明，蒸馏通过在上下文分析和信息解析过程中促进更详细和明确的推理过程，增强了长文本感知，从而有效缓解了长期困扰长文本模型的“中间信息丢失”（lost in the middle）问题。", "conclusion": "推理蒸馏，特别是通过扩展的链式思考过程，能有效提升小型模型的长文本理解和检索能力，并通过促进更明确的推理过程来解决“中间信息丢失”问题。"}}
{"id": "2507.14657", "categories": ["cs.CV", "cs.AI", "68T45", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14657", "abs": "https://arxiv.org/abs/2507.14657", "authors": ["Keivan Shariatmadar", "Ahmad Osman"], "title": "AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)", "comment": "24 pages, 9 figures", "summary": "The integration of Artificial Intelligence (AI) into sports officiating\nrepresents a paradigm shift in how decisions are made in competitive\nenvironments. Traditional manual systems, even when supported by Instant Video\nReplay (IVR), often suffer from latency, subjectivity, and inconsistent\nenforcement, undermining fairness and athlete trust. This paper introduces\nFST.ai, a novel AI-powered framework designed to enhance officiating in Sport\nTaekwondo, particularly focusing on the complex task of real-time head kick\ndetection and scoring. Leveraging computer vision, deep learning, and edge\ninference, the system automates the identification and classification of key\nactions, significantly reducing decision time from minutes to seconds while\nimproving consistency and transparency. Importantly, the methodology is not\nlimited to Taekwondo. The underlying framework -- based on pose estimation,\nmotion classification, and impact analysis -- can be adapted to a wide range of\nsports requiring action detection, such as judo, karate, fencing, or even team\nsports like football and basketball, where foul recognition or performance\ntracking is critical. By addressing one of Taekwondo's most challenging\nscenarios -- head kick scoring -- we demonstrate the robustness, scalability,\nand sport-agnostic potential of FST.ai to transform officiating standards\nacross multiple disciplines.", "AI": {"tldr": "本文介绍FST.ai，一个基于AI的框架，利用计算机视觉和深度学习，显著提升体育比赛（特别是跆拳道头部踢击）的实时判罚准确性和效率，并具有广泛的跨运动适用性。", "motivation": "传统人工判罚系统（即使有即时视频回放辅助）存在延迟、主观性和判罚不一致等问题，影响公平性和运动员信任。", "method": "FST.ai框架结合计算机视觉、深度学习和边缘推理技术，实现关键动作的自动化识别和分类。具体方法包括姿态估计、动作分类和冲击分析。", "result": "该系统将判罚时间从几分钟缩短到几秒钟，显著提高了判罚的一致性和透明度。通过解决跆拳道头部踢击这一最具挑战性的场景，展示了FST.ai的鲁棒性、可扩展性和跨运动领域的潜力。", "conclusion": "FST.ai成功地解决了跆拳道判罚中的难题，证明了其在提升体育判罚标准方面的强大能力和广泛适用性，有望变革多个体育项目的判罚方式。"}}
{"id": "2507.15239", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15239", "abs": "https://arxiv.org/abs/2507.15239", "authors": ["Qianchao Wang", "Yuxuan Ding", "Chuanzhen Jia", "Zhe Li", "Yaping Du"], "title": "Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis", "comment": null, "summary": "Novel AI-based arc fault diagnosis models have demonstrated outstanding\nperformance in terms of classification accuracy. However, an inherent problem\nis whether these models can actually be trusted to find arc faults. In this\nlight, this work proposes a soft evaluation indicator that explains the outputs\nof arc fault diagnosis models, by defining the the correct explanation of arc\nfaults and leveraging Explainable Artificial Intelligence and real arc fault\nexperiments. Meanwhile, a lightweight balanced neural network is proposed to\nguarantee competitive accuracy and soft feature extraction score. In our\nexperiments, several traditional machine learning methods and deep learning\nmethods across two arc fault datasets with different sample times and noise\nlevels are utilized to test the effectiveness of the soft evaluation indicator.\nThrough this approach, the arc fault diagnosis models are easy to understand\nand trust, allowing practitioners to make informed and trustworthy decisions.", "AI": {"tldr": "本研究提出了一种软评估指标和轻量级平衡神经网络，旨在提高电弧故障诊断模型的可解释性和信任度，同时保持高精度。", "motivation": "尽管现有基于AI的电弧故障诊断模型在分类精度上表现出色，但其内在问题在于模型是否真正值得信赖，即其诊断结果缺乏可解释性。", "method": "本研究提出了一种软评估指标来解释电弧故障诊断模型的输出，其通过定义电弧故障的正确解释并利用可解释人工智能（XAI）和真实电弧故障实验实现。同时，提出了一种轻量级平衡神经网络以保证竞争性精度和软特征提取分数。实验中，该软评估指标在两个具有不同采样时间和噪声水平的电弧故障数据集上，通过多种传统机器学习和深度学习方法进行了有效性测试。", "result": "通过所提出的方法，电弧故障诊断模型变得易于理解和信任。", "conclusion": "该研究使得电弧故障诊断模型更具可解释性和可信赖性，从而帮助从业者做出更明智和可靠的决策。"}}
{"id": "2507.15710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15710", "abs": "https://arxiv.org/abs/2507.15710", "authors": ["Lu Huang", "Lingxiao Meng", "Jiankun Wang", "Xingjian Jing"], "title": "Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages", "comment": null, "summary": "Sampling-based algorithms are widely used for motion planning in\nhigh-dimensional configuration spaces. However, due to low sampling efficiency,\ntheir performance often diminishes in complex configuration spaces with narrow\ncorridors. Existing approaches address this issue using handcrafted or learned\nheuristics to guide sampling toward useful regions. Unfortunately, these\nstrategies often lack generalizability to various problems or require extensive\nprior training. In this paper, we propose a simple yet efficient sampling-based\nplanning framework along with its bidirectional version that overcomes these\nissues by integrating different levels of planning granularity. Our approach\nprobes configuration spaces with uniform random samples at varying resolutions\nand explores these multi-resolution samples online with a bias towards sparse\nsamples when traveling large free configuration spaces. By seamlessly\ntransitioning between sparse and dense samples, our approach can navigate\ncomplex configuration spaces while maintaining planning speed and completeness.\nThe simulation results demonstrate that our approach outperforms several\nstate-of-the-art sampling-based planners in $\\mathbb{SE}(2)$, $\\mathbb{SE}(3)$,\nand $\\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments\nconducted with the Franka Emika Panda robot operating in a constrained\nworkspace provide additional evidence of the superiority of the proposed\nmethod.", "AI": {"tldr": "解析错误", "motivation": "解析错误", "method": "解析错误", "result": "解析错误", "conclusion": "解析错误"}}
{"id": "2507.14871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14871", "abs": "https://arxiv.org/abs/2507.14871", "authors": ["Ronit D. Gross", "Yarden Tzach", "Tal Halevi", "Ella Koresh", "Ido Kanter"], "title": "Tiny language models", "comment": "23 pages, 1 figure and 12 tables", "summary": "A prominent achievement of natural language processing (NLP) is its ability\nto understand and generate meaningful human language. This capability relies on\ncomplex feedforward transformer block architectures pre-trained on large\nlanguage models (LLMs). However, LLM pre-training is currently feasible only\nfor a few dominant companies due to the immense computational resources\nrequired, limiting broader research participation. This creates a critical need\nfor more accessible alternatives. In this study, we explore whether tiny\nlanguage models (TLMs) exhibit the same key qualitative features of LLMs. We\ndemonstrate that TLMs exhibit a clear performance gap between pre-trained and\nnon-pre-trained models across classification tasks, indicating the\neffectiveness of pre-training, even at a tiny scale. The performance gap\nincreases with the size of the pre-training dataset and with greater overlap\nbetween tokens in the pre-training and classification datasets. Furthermore,\nthe classification accuracy achieved by a pre-trained deep TLM architecture can\nbe replicated through a soft committee of multiple, independently pre-trained\nshallow architectures, enabling low-latency TLMs without affecting\nclassification accuracy. Our results are based on pre-training BERT-6 and\nvariants of BERT-1 on subsets of the Wikipedia dataset and evaluating their\nperformance on FewRel, AGNews, and DBPedia classification tasks. Future\nresearch on TLM is expected to further illuminate the mechanisms underlying\nNLP, especially given that its biologically inspired models suggest that TLMs\nmay be sufficient for children or adolescents to develop language.", "AI": {"tldr": "本研究探索了微型语言模型（TLMs）是否具备大型语言模型（LLMs）的关键特征，并证明了TLMs的预训练有效性及其作为LLMs可访问替代方案的潜力。", "motivation": "当前LLMs的预训练需要巨大的计算资源，仅少数公司能负担，这限制了更广泛的研究参与。因此，迫切需要更易于获取的替代方案。", "method": "研究通过预训练BERT-6和BERT-1的变体模型，使用维基百科数据集的子集进行训练，并在FewRel、AGNews和DBPedia分类任务上评估其性能。此外，还探索了通过多个独立预训练的浅层架构组成的软委员会，以复制深层TLM架构的分类准确性。", "result": "结果表明，TLMs在分类任务中，预训练模型与非预训练模型之间存在明显的性能差距，表明预训练在微型规模下也有效。这种性能差距随预训练数据集大小的增加以及预训练和分类数据集中token重叠度的增加而增大。此外，通过多个独立预训练的浅层架构组成的软委员会，可以复制预训练深层TLM架构的分类准确性，从而实现低延迟TLMs而不影响准确性。", "conclusion": "TLMs表现出与LLMs相似的关键特征，预训练对其性能至关重要，且可以通过软委员会实现高效低延迟。TLMs是LLMs的可行替代方案，未来的研究有望进一步揭示NLP的内在机制，并可能与生物学启发模型中儿童或青少年语言发展所需模型规模相吻合。"}}
{"id": "2507.14662", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14662", "abs": "https://arxiv.org/abs/2507.14662", "authors": ["Shayan Rokhva", "Babak Teimourpour"], "title": "Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall", "comment": "Questions & Recommendations: shayanrokhva1999@gmail.com;\n  shayan1999rokh@yahoo.com", "summary": "Quantifying post-consumer food waste in institutional dining settings is\nessential for supporting data-driven sustainability strategies. This study\npresents a cost-effective computer vision framework that estimates plate-level\nfood waste by utilizing semantic segmentation of RGB images taken before and\nafter meal consumption across five Iranian dishes. Four fully supervised models\n(U-Net, U-Net++, and their lightweight variants) were trained using a capped\ndynamic inverse-frequency loss and AdamW optimizer, then evaluated through a\ncomprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a\ncustom-defined Distributional Pixel Agreement (DPA) metric tailored to the\ntask. All models achieved satisfying performance, and for each food type, at\nleast one model approached or surpassed 90% DPA, demonstrating strong alignment\nin pixel-wise proportion estimates. Lighter models with reduced parameter\ncounts offered faster inference, achieving real-time throughput on an NVIDIA T4\nGPU. Further analysis showed superior segmentation performance for dry and more\nrigid components (e.g., rice and fries), while more complex, fragmented, or\nviscous dishes, such as stews, showed reduced performance, specifically\npost-consumption. Despite limitations such as reliance on 2D imaging,\nconstrained food variety, and manual data collection, the proposed framework is\npioneering and represents a scalable, contactless solution for continuous\nmonitoring of food consumption. This research lays foundational groundwork for\nautomated, real-time waste tracking systems in large-scale food service\nenvironments and offers actionable insights and outlines feasible future\ndirections for dining hall management and policymakers aiming to reduce\ninstitutional food waste.", "AI": {"tldr": "本研究提出了一种经济高效的计算机视觉框架，利用语义分割技术通过餐前餐后RGB图像估算餐盘级食物浪费，为机构餐饮场所的食品浪费量化提供了可扩展的非接触式解决方案。", "motivation": "量化机构餐饮环境中的餐后食物浪费对于支持数据驱动的可持续发展战略至关重要。", "method": "该研究采用计算机视觉框架，通过语义分割技术处理餐前餐后RGB图像来估算食物浪费。训练了四种全监督模型（U-Net、U-Net++及其轻量级变体），使用了上限动态逆频率损失和AdamW优化器。模型在五种伊朗菜肴上进行训练，并使用像素准确率、Dice、IoU以及定制的分布像素一致性（DPA）指标进行评估。", "result": "所有模型均表现良好，每种食物类型至少有一个模型达到或超过90%的DPA，表明在像素级比例估计上具有很强的一致性。参数量较少的轻量级模型推理速度更快，在NVIDIA T4 GPU上实现了实时吞吐量。对干燥、更坚硬的食物（如米饭和薯条）分割性能更优，而对更复杂、碎片化或粘稠的菜肴（如炖菜），尤其是在餐后，性能有所下降。", "conclusion": "所提出的框架具有开创性，是用于持续监测食物消耗的可扩展、非接触式解决方案。尽管存在2D成像依赖、食物种类受限和手动数据收集等局限性，该研究为大型餐饮服务环境中的自动化、实时食物浪费追踪系统奠定了基础，并为餐饮管理人员和政策制定者减少机构食物浪费提供了可行的未来方向和见解。"}}
{"id": "2507.15253", "categories": ["cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15253", "abs": "https://arxiv.org/abs/2507.15253", "authors": ["Zhaochen Guo", "Zhixiang Shen", "Xuanting Xie", "Liangjian Wen", "Zhao Kang"], "title": "Disentangling Homophily and Heterophily in Multimodal Graph Clustering", "comment": "Appear in ACM Multimedia 2025", "summary": "Multimodal graphs, which integrate unstructured heterogeneous data with\nstructured interconnections, offer substantial real-world utility but remain\ninsufficiently explored in unsupervised learning. In this work, we initiate the\nstudy of multimodal graph clustering, aiming to bridge this critical gap.\nThrough empirical analysis, we observe that real-world multimodal graphs often\nexhibit hybrid neighborhood patterns, combining both homophilic and\nheterophilic relationships. To address this challenge, we propose a novel\nframework -- \\textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which\ndecomposes the original hybrid graph into two complementary views: (1) a\nhomophily-enhanced graph that captures cross-modal class consistency, and (2)\nheterophily-aware graphs that preserve modality-specific inter-class\ndistinctions. We introduce a \\emph{Multimodal Dual-frequency Fusion} mechanism\nthat jointly filters these disentangled graphs through a dual-pass strategy,\nenabling effective multimodal integration while mitigating category confusion.\nOur self-supervised alignment objectives further guide the learning process\nwithout requiring labels. Extensive experiments on both multimodal and\nmulti-relational graph datasets demonstrate that DMGC achieves state-of-the-art\nperformance, highlighting its effectiveness and generalizability across diverse\nsettings. Our code is available at https://github.com/Uncnbb/DMGC.", "AI": {"tldr": "本文提出了一种名为DMGC的新型框架，用于无监督多模态图聚类，旨在解决多模态图中的混合邻域模式（同质性和异质性）问题，并通过分解和双频融合机制实现最先进的性能。", "motivation": "多模态图在现实世界中具有广泛应用，但在无监督学习领域探索不足。尤其，现实世界的多模态图常表现出同质性和异质性共存的混合邻域模式，这给聚类带来了挑战。", "method": "提出DMGC框架，将原始混合图分解为两个互补视图：(1)增强同质性的图，捕获跨模态类别一致性；(2)感知异质性的图，保留模态特有的类间区别。引入“多模态双频融合”机制，通过双通道策略联合过滤这些解耦图，并利用自监督对齐目标指导学习过程。", "result": "在多模态和多关系图数据集上进行了广泛实验，DMGC取得了最先进的性能，证明了其在不同设置下的有效性和泛化能力。", "conclusion": "DMGC框架通过有效处理多模态图中的混合邻域模式，显著提升了无监督多模态图聚类的性能，具有良好的通用性。"}}
{"id": "2507.15716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15716", "abs": "https://arxiv.org/abs/2507.15716", "authors": ["Ziyu Wan", "Lin Zhao"], "title": "DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models", "comment": null, "summary": "This paper proposes DiffPF, a differentiable particle filter that leverages\ndiffusion models for state estimation in dynamic systems. Unlike conventional\ndifferentiable particle filters, which require importance weighting and\ntypically rely on predefined or low-capacity proposal distributions. DiffPF\nlearns a flexible posterior sampler by conditioning a diffusion model on\npredicted particles and the current observation. This enables accurate,\nequally-weighted sampling from complex, high-dimensional, and multimodal\nfiltering distributions. We evaluate DiffPF across a range of scenarios,\nincluding both unimodal and highly multimodal distributions, and test it on\nsimulated as well as real-world tasks, where it consistently outperforms\nexisting filtering baselines. In particular, DiffPF achieves an 82.8%\nimprovement in estimation accuracy on a highly multimodal global localization\nbenchmark, and a 26% improvement on the real-world KITTI visual odometry\nbenchmark, compared to state-of-the-art differentiable filters. To the best of\nour knowledge, DiffPF is the first method to integrate conditional diffusion\nmodels into particle filtering, enabling high-quality posterior sampling that\nproduces more informative particles and significantly improves state\nestimation.", "AI": {"tldr": "本文提出DiffPF，一种利用扩散模型进行状态估计的可微分粒子滤波器，通过学习灵活的后验采样器，显著提升了复杂、高维和多模态分布下的估计精度。", "motivation": "传统的可微分粒子滤波器需要重要性权重，并通常依赖于预定义或低容量的提议分布，难以准确处理复杂、高维和多模态的滤波分布。", "method": "DiffPF通过将扩散模型与预测粒子和当前观测值进行条件化，学习一个灵活的后验采样器。这使得能够从复杂、高维和多模态滤波分布中进行准确、等权重的采样。它是首个将条件扩散模型集成到粒子滤波中的方法。", "result": "DiffPF在各种场景下（包括单模态和高度多模态分布）以及模拟和真实世界任务中，始终优于现有滤波基线。在高度多模态的全局定位基准测试中，估计精度提高了82.8%；在真实世界的KITTI视觉里程计基准测试中，相比最先进的可微分滤波器，提高了26%。", "conclusion": "DiffPF通过将条件扩散模型集成到粒子滤波中，实现了高质量的后验采样，从而产生了更具信息量的粒子，并显著提高了状态估计的性能。"}}
{"id": "2507.14887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14887", "abs": "https://arxiv.org/abs/2507.14887", "authors": ["Shiyi Mu", "Yongkang Liu", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction", "comment": "Accepted by CogSci", "summary": "Although large language models (LLMs) excel in text comprehension and\ngeneration, their performance on the Emotion-Cause Pair Extraction (ECPE) task,\nwhich requires reasoning ability, is often underperform smaller language model.\nThe main reason is the lack of auxiliary knowledge, which limits LLMs' ability\nto effectively perceive emotions and reason causes. To address this issue, we\npropose a novel \\textbf{M}ulti-source h\\textbf{E}terogeneous \\textbf{K}nowledge\n\\textbf{i}njection me\\textbf{T}hod, MEKiT, which integrates heterogeneous\ninternal emotional knowledge and external causal knowledge. Specifically, for\nthese two distinct aspects and structures of knowledge, we apply the approaches\nof incorporating instruction templates and mixing data for instruction-tuning,\nwhich respectively facilitate LLMs in more comprehensively identifying emotion\nand accurately reasoning causes. Experimental results demonstrate that MEKiT\nprovides a more effective and adaptable solution for the ECPE task, exhibiting\nan absolute performance advantage over compared baselines and dramatically\nimproving the performance of LLMs on the ECPE task.", "AI": {"tldr": "针对大型语言模型（LLMs）在情感-原因对抽取（ECPE）任务中因缺乏辅助知识而表现不佳的问题，本文提出了MEKiT方法，通过注入异构的内部情感知识和外部因果知识，显著提升了LLMs在该任务上的性能。", "motivation": "大型语言模型（LLMs）在需要推理能力的情感-原因对抽取（ECPE）任务上，常表现不如小型语言模型。主要原因是LLMs缺乏辅助知识，限制了它们有效感知情感和推理原因的能力。", "method": "提出了一种新颖的MEKiT（Multi-source Heterogeneous Knowledge injection meThod）方法。该方法整合了异构的内部情感知识和外部因果知识。具体而言，对于这两种不同方面和结构的知识，分别采用指令模板（用于内部情感知识，帮助识别情感）和数据混合（用于外部因果知识，帮助推理原因）的方法进行指令微调。", "result": "实验结果表明，MEKiT为ECPE任务提供了一个更有效和适应性强的解决方案，与基线模型相比具有绝对的性能优势，并显著提升了LLMs在ECPE任务上的表现。", "conclusion": "MEKiT通过有效地注入多源异构知识，成功解决了LLMs在ECPE任务中因知识缺乏导致的性能瓶颈，使其在该任务上表现出显著提升。"}}
{"id": "2507.14670", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14670", "abs": "https://arxiv.org/abs/2507.14670", "authors": ["Yaxuan Song", "Jianan Fan", "Hang Chang", "Weidong Cai"], "title": "Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images", "comment": "16 pages, 15 tables, 8 figures", "summary": "Accurately predicting gene expression from histopathology images offers a\nscalable and non-invasive approach to molecular profiling, with significant\nimplications for precision medicine and computational pathology. However,\nexisting methods often underutilize the cross-modal representation alignment\nbetween histopathology images and gene expression profiles across multiple\nrepresentational levels, thereby limiting their prediction performance. To\naddress this, we propose Gene-DML, a unified framework that structures latent\nspace through Dual-pathway Multi-Level discrimination to enhance correspondence\nbetween morphological and transcriptional modalities. The multi-scale\ninstance-level discrimination pathway aligns hierarchical histopathology\nrepresentations extracted at local, neighbor, and global levels with gene\nexpression profiles, capturing scale-aware morphological-transcriptional\nrelationships. In parallel, the cross-level instance-group discrimination\npathway enforces structural consistency between individual (image/gene)\ninstances and modality-crossed (gene/image, respectively) groups, strengthening\nthe alignment across modalities. By jointly modelling fine-grained and\nstructural-level discrimination, Gene-DML is able to learn robust cross-modal\nrepresentations, enhancing both predictive accuracy and generalization across\ndiverse biological contexts. Extensive experiments on public spatial\ntranscriptomics datasets demonstrate that Gene-DML achieves state-of-the-art\nperformance in gene expression prediction. The code and checkpoints will be\nreleased soon.", "AI": {"tldr": "Gene-DML是一种统一框架，通过双通路多级判别来增强组织病理图像和基因表达谱之间的跨模态对齐，从而实现更准确的基因表达预测。", "motivation": "现有方法未能充分利用组织病理图像和基因表达谱在多层级上的跨模态表示对齐，限制了其预测性能。", "method": "提出Gene-DML框架，该框架通过双通路多级判别来构建潜在空间。其中，多尺度实例级判别通路将局部、邻域和全局层级的组织病理学表示与基因表达谱对齐；同时，跨层级实例群组判别通路强制个体实例与跨模态群组之间的结构一致性。", "result": "Gene-DML在公共空间转录组数据集上，在基因表达预测方面达到了最先进的性能，显著提高了预测准确性和在不同生物背景下的泛化能力。", "conclusion": "Gene-DML通过联合建模细粒度和结构级判别，能够学习鲁棒的跨模态表示，从而有效提升基因表达预测的准确性。"}}
{"id": "2507.15268", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15268", "abs": "https://arxiv.org/abs/2507.15268", "authors": ["Junhyeong Lee", "Joon-Young Kim", "Heekyu Kim", "Inhyo Lee", "Seunghwa Ryu"], "title": "IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry", "comment": null, "summary": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. Overall, these findings demonstrate the\nviability of multi-agent LLM systems for industrial knowledge workflows and\nestablish IM-Chat as a scalable and generalizable approach to AI-assisted\ndecision support in manufacturing.", "AI": {"tldr": "本研究提出IM-Chat，一个基于大语言模型（LLM）的多智能体框架，旨在解决注塑行业知识传承和沟通障碍问题，通过整合文档知识和数据驱动的现场数据，利用RAG和工具调用策略，实现无需微调的自适应知识转移和决策支持。", "motivation": "注塑行业面临严峻挑战，包括经验丰富工人退休导致的现场知识流失，以及多语言障碍阻碍有效沟通，亟需一种解决方案来保存和传递领域知识。", "method": "IM-Chat是一个基于LLM的多智能体框架，采用模块化架构。它整合了有限的文档知识（如故障排除表、手册）和通过数据驱动的过程条件生成器建模的广泛现场数据。系统采用检索增强生成（RAG）策略和工具调用智能体，无需微调即可适应。性能评估通过领域专家使用10分制量表（关注相关性和正确性）对100个单工具任务和60个混合任务进行，并辅以GPT-4o引导的自动化评估。", "result": "评估结果表明，能力更强的模型（如GPT-4o）在复杂、工具集成场景中往往能获得更高的准确性。研究结果证实了多智能体LLM系统在工业知识工作流中的可行性。", "conclusion": "IM-Chat为注塑行业提供了AI辅助决策支持的可扩展和通用方法，展示了多智能体LLM系统在解决工业知识传承和管理挑战方面的巨大潜力。"}}
{"id": "2507.15729", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15729", "abs": "https://arxiv.org/abs/2507.15729", "authors": ["Jens V. Rüppel", "Andrey Rudenko", "Tim Schreiter", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction", "comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses", "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.", "AI": {"tldr": "本文提出了一种基于凝视和语音的LLM驱动人机交互（HRI）系统，用于辅助机器人，旨在提供双向、多模态和上下文感知的用户支持，并与传统脚本式HRI系统进行了比较。", "motivation": "现有的人机交互系统在理解指令和生成动作方面取得进展，但在协作任务中，双向、多模态和上下文感知的用户支持仍是挑战。大型语言模型（LLMs）为构建灵活、知识驱动的HRI系统提供了潜力。", "method": "开发了一个基于凝视和语音的辅助机器人接口，该接口能从多视觉输入感知环境，并支持动态用户任务。系统设计为模块化和可迁移，支持实时基于语言的交互状态表示和快速板载感知模块。通过两次实验室研究，将该系统与传统脚本式HRI流程的性能和用户评价进行了比较。", "result": "基于LLM的方法增强了适应性，并略微改善了用户参与度和任务执行指标，但可能产生冗余输出。传统脚本式流程更适合直接简单的任务。", "conclusion": "LLM驱动的HRI系统在提高辅助机器人交互的适应性和用户体验方面具有优势，尤其适用于复杂任务，但需注意潜在的冗余输出。对于简单任务，传统脚本式方法仍是有效选择。"}}
{"id": "2507.14894", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14894", "abs": "https://arxiv.org/abs/2507.14894", "authors": ["Boyi Deng", "Yu Wan", "Baosong Yang", "Fei Huang", "Wenjie Wang", "Fuli Feng"], "title": "Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs", "comment": null, "summary": "Large Language Models (LLMs) have impressive multilingual capabilities, but\nthey suffer from unexpected code-switching, also known as language mixing,\nwhich involves switching to unexpected languages in the model response. This\nproblem leads to poor readability and degrades the usability of model\nresponses. However, existing work on this issue lacks a mechanistic analysis\nand shows limited effectiveness. In this paper, we first provide an in-depth\nanalysis of unexpected code-switching using sparse autoencoders and find that\nwhen LLMs switch to a language, the features of that language exhibit excessive\npre-activation values. Based on our findings, we propose $\\textbf{S}$parse\n$\\textbf{A}$utoencoder-guided $\\textbf{S}$upervised\n$\\textbf{F}$ine$\\textbf{t}$uning (SASFT), which teaches LLMs to maintain\nappropriate pre-activation values of specific language features during\ntraining. Experiments on five models across three languages demonstrate that\nSASFT consistently reduces unexpected code-switching by more than 50\\% compared\nto standard supervised fine-tuning, with complete elimination in four cases.\nMoreover, SASFT maintains or even improves the models' performance on six\nmultilingual benchmarks, showing its effectiveness in addressing code-switching\nwhile preserving multilingual capabilities.", "AI": {"tldr": "本文通过稀疏自编码器分析发现大型语言模型（LLMs）意外语码转换（code-switching）的原因是特定语言特征的预激活值过高，并提出SASFT方法，通过微调训练LLMs保持适当的预激活值，显著减少了语码转换，同时保持了多语言能力。", "motivation": "尽管LLMs具有出色的多语言能力，但它们常出现意外的语码转换（即语言混合），这严重影响了模型响应的可读性和可用性。现有研究缺乏对这一问题的机制性分析，且效果有限。", "method": "1. 使用稀疏自编码器对意外语码转换进行了深入机制分析，发现当LLMs切换到某种语言时，该语言的特征会表现出过高的预激活值。2. 基于此发现，提出了SASFT（Sparse Autoencoder-guided Supervised Finetuning）方法，通过在训练过程中引导LLMs维持特定语言特征的适当预激活值。", "result": "SASFT在五种模型和三种语言上的实验表明，相比于标准的监督微调，SASFT能够持续将意外语码转换减少50%以上，并在四种情况下完全消除。此外，SASFT在六个多语言基准测试中保持甚至提升了模型的性能。", "conclusion": "SASFT是一种有效解决LLMs意外语码转换问题的方法，它基于对问题机制的深入理解，能在消除语码转换的同时保持甚至提升模型的多语言能力。"}}
{"id": "2507.14675", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14675", "abs": "https://arxiv.org/abs/2507.14675", "authors": ["Yuchen Duan", "Zhe Chen", "Yusong Hu", "Weiyun Wang", "Shenglong Ye", "Botian Shi", "Lewei Lu", "Qibin Hou", "Tong Lu", "Hongsheng Li", "Jifeng Dai", "Wenhai Wang"], "title": "Docopilot: Improving Multimodal Models for Document-Level Understanding", "comment": null, "summary": "Despite significant progress in multimodal large language models (MLLMs),\ntheir performance on complex, multi-page document comprehension remains\ninadequate, largely due to the lack of high-quality, document-level datasets.\nWhile current retrieval-augmented generation (RAG) methods offer partial\nsolutions, they suffer from issues, such as fragmented retrieval contexts,\nmulti-stage error accumulation, and extra time costs of retrieval. In this\nwork, we present a high-quality document-level dataset, Doc-750K, designed to\nsupport in-depth understanding of multimodal documents. This dataset includes\ndiverse document structures, extensive cross-page dependencies, and real\nquestion-answer pairs derived from the original documents. Building on the\ndataset, we develop a native multimodal model, Docopilot, which can accurately\nhandle document-level dependencies without relying on RAG. Experiments\ndemonstrate that Docopilot achieves superior coherence, accuracy, and\nefficiency in document understanding tasks and multi-turn interactions, setting\na new baseline for document-level multimodal understanding. Data, code, and\nmodels are released at https://github.com/OpenGVLab/Docopilot", "AI": {"tldr": "该研究提出了一个高质量的多页文档理解数据集Doc-750K和一个原生的多模态模型Docopilot，显著提升了模型对复杂文档的理解能力，且无需依赖RAG。", "motivation": "现有多模态大语言模型（MLLMs）在复杂、多页文档理解方面表现不足，主要原因是缺乏高质量的文档级数据集。当前的RAG方法存在检索上下文碎片化、多阶段错误累积和额外时间成本等问题。", "method": "1. 构建了高质量的文档级数据集Doc-750K，包含多样化的文档结构、广泛的跨页依赖和真实的问答对。2. 基于此数据集，开发了一个原生的多模态模型Docopilot，旨在无需RAG的情况下准确处理文档级依赖。", "result": "实验证明，Docopilot在文档理解任务和多轮交互中实现了卓越的连贯性、准确性和效率，为文档级多模态理解设定了新的基线。", "conclusion": "Doc-750K数据集和Docopilot模型共同解决了多页文档理解中的数据和方法挑战，显著提升了MLLMs在该领域的性能，并为未来的研究提供了新的起点。"}}
{"id": "2507.15330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15330", "abs": "https://arxiv.org/abs/2507.15330", "authors": ["Hammad Atta", "Muhammad Zeeshan Baig", "Yasir Mehmood", "Nadeem Shahzad", "Ken Huang", "Muhammad Aziz Ul Haq", "Muhammad Awais", "Kamal Ahmed"], "title": "QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI", "comment": null, "summary": "We introduce Cognitive Degradation as a novel vulnerability class in agentic\nAI systems. Unlike traditional adversarial external threats such as prompt\ninjection, these failures originate internally, arising from memory starvation,\nplanner recursion, context flooding, and output suppression. These systemic\nweaknesses lead to silent agent drift, logic collapse, and persistent\nhallucinations over time. To address this class of failures, we introduce the\nQorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain\n10), a lifecycle-aware defense framework defined by a six-stage cognitive\ndegradation lifecycle. The framework includes seven runtime controls\n(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger\nproactive mitigation through fallback routing, starvation detection, and memory\nintegrity enforcement. Drawing from cognitive neuroscience, we map agentic\narchitectures to human analogs, enabling early detection of fatigue,\nstarvation, and role collapse. By introducing a formal lifecycle and real-time\nmitigation controls, this work establishes Cognitive Degradation as a critical\nnew class of AI system vulnerability and proposes the first cross-platform\ndefense model for resilient agentic behavior.", "AI": {"tldr": "本文提出“认知退化”作为代理AI系统的一种新型内部漏洞，并引入Qorvex安全AI框架（QSAF）来检测和缓解此类问题。", "motivation": "传统的AI安全威胁主要来自外部（如提示注入），但代理AI系统存在内部源头（如内存不足、规划器递归、上下文泛滥、输出抑制）导致的故障，这些故障会导致代理静默漂移、逻辑崩溃和持续幻觉，因此需要一种新的方法来解决这类内部脆弱性。", "method": "引入了Qorvex安全AI框架（QSAF Domain 10），这是一个生命周期感知的防御框架，定义了六阶段的认知退化生命周期。该框架包含七个运行时控制（QSAF-BC-001至BC-007），通过实时监控代理子系统并触发主动缓解措施（如回退路由、饥饿检测、内存完整性强制），并借鉴认知神经科学将代理架构映射到人类类比以早期检测疲劳、饥饿和角色崩溃。", "result": "本文将认知退化确立为AI系统的一种关键新型漏洞类别，并提出了首个用于弹性代理行为的跨平台防御模型，实现了对内部故障的早期检测和主动缓解。", "conclusion": "认知退化是代理AI系统一个关键的、此前未被识别的内部漏洞类别。通过引入形式化的生命周期和实时缓解控制，Qorvex安全AI框架为解决这一问题提供了有效的防御模型，增强了代理行为的韧性。"}}
{"id": "2507.15782", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15782", "abs": "https://arxiv.org/abs/2507.15782", "authors": ["Ruochu Yang", "Yu Zhou", "Fumin Zhang", "Mengxue Hou"], "title": "Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs", "comment": null, "summary": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs.", "AI": {"tldr": "该研究提出了一种名为Inter-LLM的新型交错式大型语言模型（LLM）和运动规划算法，旨在提升家用机器人在大型场景中收集多物体的能力，通过优化规划显著提高了任务性能。", "motivation": "家用机器人缺乏类人智能，尤其是在操作开放集物体和高效准确地导航大型环境方面。在大型场景图中解决广义的多物体收集问题极具挑战性，因为它需要在一个庞大且不确定的动作-状态空间中进行长周期规划。", "method": "提出了一种名为Inter-LLM的交错式LLM和运动规划算法。通过设计一个多模态动作成本相似度函数，该算法能够结合历史信息并展望未来来优化规划，从而在质量和效率之间取得良好平衡。", "result": "仿真实验表明，与最新工作相比，Inter-LLM算法在完成人类指令、最大化任务成功率和最小化任务成本方面，将整体任务性能提高了30%。", "conclusion": "Inter-LLM算法显著提升了家用机器人在大型复杂环境中执行多物体收集任务的能力，显示出更高的效率和可靠性。"}}
{"id": "2507.14900", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14900", "abs": "https://arxiv.org/abs/2507.14900", "authors": ["Chongxuan Huang", "Yongshi Ye", "Biao Fu", "Qifeng Su", "Xiaodong Shi"], "title": "From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable multilingual\ncapabilities, however, how to evaluate cross-lingual alignment remains\nunderexplored. Existing alignment benchmarks primarily focus on sentence\nembeddings, but prior research has shown that neural models tend to induce a\nnon-smooth representation space, which impact of semantic alignment evaluation\non low-resource languages. Inspired by neuroscientific findings that similar\ninformation activates overlapping neuronal regions, we propose a novel Neuron\nState-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a\nlignment capabilities of LLMs, which offers a more semantically grounded\napproach to assess cross-lingual alignment. We evaluate NeuronXA on several\nprominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two\ntransfer tasks and three multilingual benchmarks. The results demonstrate that\nwith only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation\nof 0.9556 with downstream tasks performance and 0.8514 with transferability.\nThese findings demonstrate NeuronXA's effectiveness in assessing both\ncross-lingual alignment and transferability, even with a small dataset. This\nhighlights its potential to advance cross-lingual alignment research and to\nimprove the semantic understanding of multilingual LLMs.", "AI": {"tldr": "提出NeuronXA，一种基于神经元状态的跨语言对齐评估方法，能有效评估大型语言模型的跨语言对齐能力和可迁移性，即使在小数据集上表现也出色。", "motivation": "现有大型语言模型（LLMs）的跨语言能力评估不足，尤其是在跨语言对齐方面。现有对齐基准主要关注句子嵌入，但神经模型可能产生非平滑的表示空间，影响低资源语言的语义对齐评估。", "method": "受神经科学启发（相似信息激活重叠神经元区域），提出一种新的基于神经元状态的跨语言对齐评估方法（NeuronXA），以更语义化的方式评估LLMs的跨语言对齐能力。在多种主流多语言LLMs（LLaMA, Qwen, Mistral, GLM, OLMo）上，通过两个迁移任务和三个多语言基准进行评估。", "result": "仅用100对平行句子，NeuronXA与下游任务性能的皮尔逊相关系数达到0.9556，与可迁移性的相关系数达到0.8514。这表明NeuronXA在评估跨语言对齐和可迁移性方面的有效性，即使在小数据集上也能表现良好。", "conclusion": "NeuronXA是一种有效评估LLMs跨语言对齐和可迁移性的方法，即使数据量小也能取得良好效果。它有望推动跨语言对齐研究，并加深对多语言LLMs语义理解的认识。"}}
{"id": "2507.14680", "categories": ["cs.CV", "cs.AI", "68T07, 92C55", "I.2.7; I.4.8; J.3"], "pdf": "https://arxiv.org/pdf/2507.14680", "abs": "https://arxiv.org/abs/2507.14680", "authors": ["Xinheng Lyu", "Yuci Liang", "Wenting Chen", "Meidan Ding", "Jiaqi Yang", "Guolin Huang", "Daokun Zhang", "Xiangjian He", "Linlin Shen"], "title": "WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis", "comment": null, "summary": "Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel\ntissue analysis across various pathological tasks. While recent advancements in\nmulti-modal large language models (MLLMs) allow multi-task WSI analysis through\nnatural language, they often underperform compared to task-specific models.\nCollaborative multi-agent systems have emerged as a promising solution to\nbalance versatility and accuracy in healthcare, yet their potential remains\nunderexplored in pathology-specific domains. To address these issues, we\npropose WSI-Agents, a novel collaborative multi-agent system for multi-modal\nWSI analysis. WSI-Agents integrates specialized functional agents with robust\ntask allocation and verification mechanisms to enhance both task-specific\naccuracy and multi-task versatility through three components: (1) a task\nallocation module assigning tasks to expert agents using a model zoo of patch\nand WSI level MLLMs, (2) a verification mechanism ensuring accuracy through\ninternal consistency checks and external validation using pathology knowledge\nbases and domain-specific models, and (3) a summary module synthesizing the\nfinal summary with visual interpretation maps. Extensive experiments on\nmulti-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs\nand medical agent frameworks across diverse tasks.", "AI": {"tldr": "WSI-Agents是一种新型协作多智能体系统，通过结合专业功能智能体、鲁棒的任务分配和验证机制，显著提升了多模态全玻片图像（WSI）分析的准确性和多任务通用性。", "motivation": "现有多模态大语言模型（MLLMs）在全玻片图像（WSI）分析中，相对于特定任务模型表现不佳，且多智能体系统在病理学领域尚未充分探索，无法兼顾通用性和准确性。", "method": "WSI-Agents系统包含三个核心组件：1) 任务分配模块，利用包含块级和WSI级MLLMs的模型库将任务分配给专家智能体；2) 验证机制，通过内部一致性检查和利用病理学知识库及领域特定模型的外部验证来确保准确性；3) 汇总模块，合成最终摘要并生成视觉解释图。", "result": "在多模态WSI基准测试上进行的广泛实验表明，WSI-Agents在各种任务中均优于当前的WSI MLLMs和现有医疗智能体框架。", "conclusion": "WSI-Agents成功地通过协作多智能体系统解决了多模态WSI分析中通用性和准确性之间的平衡问题，并展现出优越的性能。"}}
{"id": "2507.15351", "categories": ["cs.AI", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15351", "abs": "https://arxiv.org/abs/2507.15351", "authors": ["Zijian Zhao", "Sen Li"], "title": "One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms", "comment": null, "summary": "On-demand ride-sharing platforms face the fundamental challenge of\ndynamically bundling passengers with diverse origins and destinations and\nmatching them with vehicles in real time, all under significant uncertainty.\nRecently, MARL has emerged as a promising solution for this problem, leveraging\ndecentralized learning to address the curse of dimensionality caused by the\nlarge number of agents in the ride-hailing market and the resulting expansive\nstate and action spaces. However, conventional MARL-based ride-sharing\napproaches heavily rely on the accurate estimation of Q-values or V-values,\nwhich becomes problematic in large-scale, highly uncertain environments.\nSpecifically, most of these approaches adopt an independent paradigm,\nexacerbating this issue, as each agent treats others as part of the\nenvironment, leading to unstable training and substantial estimation bias in\nvalue functions. To address these challenges, we propose two novel alternative\nmethods that bypass value function estimation. First, we adapt GRPO to\nride-sharing, replacing the PPO baseline with the group average reward to\neliminate critic estimation errors and reduce training bias. Second, inspired\nby GRPO's full utilization of group reward information, we customize the PPO\nframework for ride-sharing platforms and show that, under a homogeneous fleet,\nthe optimal policy can be trained using only one-step rewards - a method we\nterm One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan\nride-hailing dataset demonstrate that both GRPO and OSPO achieve superior\nperformance across most scenarios, efficiently optimizing pickup times and the\nnumber of served orders using simple MLP networks.", "AI": {"tldr": "提出两种新的多智能体强化学习（MARL）方法（GRPO和OSPO），用于网约车平台，它们绕过了价值函数估计，并在真实世界数据集中表现出优越性能。", "motivation": "传统的基于MARL的网约车平台方法在大型、高度不确定的环境中，依赖于Q值或V值的精确估计，这变得非常困难。此外，大多数方法采用独立范式，导致训练不稳定和价值函数估计偏差大。", "method": "1. 调整GRPO（Group Reward Policy Optimization）用于网约车，用群组平均奖励替代PPO基线，以消除评论家估计误差并减少训练偏差。 2. 提出单步策略优化（OSPO），该方法受GRPO启发，在同质车队下，仅使用一步奖励即可训练最优策略。", "result": "在真实世界的曼哈顿网约车数据集上进行的实验表明，GRPO和OSPO在大多数场景下均实现了卓越的性能，并能使用简单的MLP网络有效优化接送时间和已服务订单数量。", "conclusion": "提出的GRPO和OSPO方法是传统MARL的有效替代方案，它们成功地绕过了价值函数估计的挑战，并在网约车场景中展现出优越的性能和效率。"}}
{"id": "2507.15833", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15833", "abs": "https://arxiv.org/abs/2507.15833", "authors": ["Ian Chuang", "Andrew Lee", "Dechen Gao", "Jinyu Zou", "Iman Soltani"], "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers", "comment": "13 pages, 10 figures", "summary": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/", "AI": {"tldr": "本文受人类主动凝视启发，将凝视信息融入机器人视觉策略，通过中央凹化图像处理和ViT注视令牌化，显著降低计算量，同时提升高精度任务性能和对干扰的鲁棒性。", "motivation": "人类视觉通过主动凝视高效处理信息，而现有机器人学习系统通常被动、统一处理图像，效率和性能有待提高。研究旨在探索将类人主动凝视融入机器人策略，以提升其效率和性能。", "method": "构建了一个模拟人类头部运动和眼球追踪的主动视觉机器人系统，并在此基础上，扩展AV-ALOHA平台，开发了同步收集人类眼动数据和机器人演示的框架，并创建了相应的模拟基准和数据集。将凝视信息通过中央凹化补丁令牌化方案集成到Vision Transformers (ViTs)中。此外，探索了两种凝视模仿和预测方法：一种是两阶段模型，预测凝视以指导中央凹化和动作；另一种是端到端模型，将凝视集成到动作空间，联合预测凝视和动作。", "result": "所提出的中央凹化机器人视觉方法不仅显著降低了计算开销，还提高了高精度任务的性能，并增强了对未见干扰物的鲁棒性。", "conclusion": "研究结果表明，受人类启发的视觉处理（特别是主动凝视）为机器人视觉系统提供了一种有用的归纳偏置，能够有效提升系统的效率、性能和鲁棒性。"}}
{"id": "2507.14913", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14913", "abs": "https://arxiv.org/abs/2507.14913", "authors": ["Eliya Habba", "Noam Dahan", "Gili Lior", "Gabriel Stanovsky"], "title": "PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation", "comment": "Eliya Habba and Noam Dahan contributed equally to this work", "summary": "Evaluating LLMs with a single prompt has proven unreliable, with small\nchanges leading to significant performance differences. However, generating the\nprompt variations needed for a more robust multi-prompt evaluation is\nchallenging, limiting its adoption in practice. To address this, we introduce\nPromptSuite, a framework that enables the automatic generation of various\nprompts. PromptSuite is flexible - working out of the box on a wide range of\ntasks and benchmarks. It follows a modular prompt design, allowing controlled\nperturbations to each component, and is extensible, supporting the addition of\nnew components and perturbation types. Through a series of case studies, we\nshow that PromptSuite provides meaningful variations to support strong\nevaluation practices. It is available through both a Python API:\nhttps://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:\nhttps://promptsuite.streamlit.app/", "AI": {"tldr": "本文介绍PromptSuite，一个自动化生成多种提示词的框架，旨在解决单一提示词评估LLM的不可靠性，并支持更鲁棒的多提示词评估。", "motivation": "评估大型语言模型（LLMs）时，单一提示词的评估结果不稳定且不可靠，微小改动可能导致显著性能差异。虽然多提示词评估更可靠，但生成大量不同提示词耗时且困难，限制了其实际应用。", "method": "研究引入了PromptSuite框架，该框架能自动生成多种提示词。它具有灵活性，适用于多种任务和基准；采用模块化提示词设计，允许对每个组件进行受控扰动；并具有可扩展性，支持添加新的组件和扰动类型。", "result": "通过一系列案例研究，论文展示PromptSuite能够提供有意义的提示词变体，以支持更强大的评估实践。该工具提供了Python API和用户友好的Web界面。", "conclusion": "PromptSuite通过自动化生成多样化的提示词，有效解决了LLM评估中单一提示词不可靠的问题，为进行更稳健、更可靠的多提示词评估提供了实用工具和框架。"}}
{"id": "2507.14686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14686", "abs": "https://arxiv.org/abs/2507.14686", "authors": ["Chen Cai", "Tianyi Liu", "Jianjun Gao", "Wenyang Liu", "Kejun Wu", "Ruoyu Wang", "Yi Wang", "Soo Chin Liew"], "title": "From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition", "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot\nabilities but struggle with complex Grounded Situation Recognition (GSR) and\nare resource-intensive for edge device deployment. Meanwhile, conventional GSR\nmodels often lack generalization ability, falling short in recognizing unseen\nand rare situations. In this paper, we exploit transferring knowledge from a\nteacher MLLM to a small GSR model to enhance its generalization and zero-shot\nabilities, thereby introducing the task of Open-vocabulary Grounded Situation\nRecognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt\nDistillation (MIPD), a novel framework that distills enriched multimodal\nknowledge from the foundation model, enabling the student Ov-GSR model to\nrecognize unseen situations and be better aware of rare situations.\nSpecifically, the MIPD framework first leverages the LLM-based Judgmental\nRationales Generator (JRG) to construct positive and negative glimpse and gaze\nrationales enriched with contextual semantic information. The proposed\nscene-aware and instance-perception prompts are then introduced to align\nrationales with visual information from the MLLM teacher via the\nNegative-Guided Multimodal Prompting Alignment (NMPA) module, effectively\ncapturing holistic and perceptual multimodal knowledge. Finally, the aligned\nmultimodal knowledge is distilled into the student Ov-GSR model, providing a\nstronger foundation for generalization that enhances situation understanding,\nbridges the gap between seen and unseen scenarios, and mitigates prediction\nbias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving\nsuperior performance on seen, rare, and unseen situations, and further\ndemonstrate improved unseen detection on the HICO-DET dataset.", "AI": {"tldr": "本文提出了一种名为MIPD的多模态交互式提示蒸馏框架，旨在将大型多模态语言模型（MLLM）的知识蒸馏到一个小型GSR模型中，以增强其泛化和零样本能力，从而解决开放词汇情境识别（Ov-GSR）任务。", "motivation": "当前的MLLM在复杂情境识别（GSR）方面表现不佳，且资源密集不适合边缘设备部署。而传统GSR模型缺乏泛化能力，难以识别未见和罕见情境。", "method": "本文提出了多模态交互式提示蒸馏（MIPD）框架。首先，利用基于LLM的判断性推理生成器（JRG）构建富含上下文语义的正负注视和凝视推理。接着，引入场景感知和实例感知提示，通过负向引导多模态提示对齐（NMPA）模块将推理与MLLM教师模型的视觉信息对齐，从而捕获整体和感知多模态知识。最后，将对齐的多模态知识蒸馏到学生Ov-GSR模型中。", "result": "MIPD在改进的Ov-SWiG数据集上对已见、罕见和未见情境均取得了卓越性能，并在HICO-DET数据集上展示了改进的未见检测能力。", "conclusion": "MIPD框架通过从教师MLLM蒸馏知识，成功提升了小型GSR模型的泛化和零样本能力，增强了情境理解，弥合了已见和未见场景之间的差距，并缓解了罕见情况下的预测偏差。"}}
{"id": "2507.15356", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15356", "abs": "https://arxiv.org/abs/2507.15356", "authors": ["Lu Guo", "Yixiang Shan", "Zhengbang Zhu", "Qifan Liang", "Lichang Song", "Ting Long", "Weinan Zhang", "Yi Chang"], "title": "RAD: Retrieval High-quality Demonstrations to Enhance Decision-making", "comment": null, "summary": "Offline reinforcement learning (RL) enables agents to learn policies from\nfixed datasets, avoiding costly or unsafe environment interactions. However,\nits effectiveness is often limited by dataset sparsity and the lack of\ntransition overlap between suboptimal and expert trajectories, which makes\nlong-horizon planning particularly challenging. Prior solutions based on\nsynthetic data augmentation or trajectory stitching often fail to generalize to\nnovel states and rely on heuristic stitching points. To address these\nchallenges, we propose Retrieval High-quAlity Demonstrations (RAD) for\ndecision-making, which combines non-parametric retrieval with diffusion-based\ngenerative modeling. RAD dynamically retrieves high-return states from the\noffline dataset as target states based on state similarity and return\nestimation, and plans toward them using a condition-guided diffusion model.\nSuch retrieval-guided generation enables flexible trajectory stitching and\nimproves generalization when encountered with underrepresented or\nout-of-distribution states. Extensive experiments confirm that RAD achieves\ncompetitive or superior performance compared to baselines across diverse\nbenchmarks, validating its effectiveness.", "AI": {"tldr": "针对离线强化学习中数据稀疏性和长程规划的挑战，本文提出了RAD方法，通过非参数检索高回报状态并使用扩散模型引导生成轨迹，实现了灵活的轨迹拼接和更好的泛化能力。", "motivation": "离线强化学习（RL）受限于数据集稀疏性及次优轨迹与专家轨迹之间转换重叠的不足，这使得长程规划尤为困难。现有解决方案（如合成数据增强或轨迹拼接）难以泛化到新状态，且依赖启发式拼接点。", "method": "本文提出了RAD（Retrieval High-quAlity Demonstrations）决策方法，结合了非参数检索和基于扩散的生成模型。RAD根据状态相似性和回报估计，从离线数据集中动态检索高回报状态作为目标状态，并使用条件引导扩散模型向这些目标状态进行规划。这种检索引导的生成实现了灵活的轨迹拼接，并提高了在面对未充分表示或分布外状态时的泛化能力。", "result": "广泛的实验证明，RAD在各种基准测试中，与现有基线相比，取得了具有竞争力甚至更优的性能。", "conclusion": "RAD通过结合检索和扩散模型，有效解决了离线强化学习中数据稀疏性和长程规划的挑战，实现了灵活且泛化能力强的轨迹生成。"}}
{"id": "2507.14809", "categories": ["cs.CV", "cs.MM", "cs.RO", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.14809", "abs": "https://arxiv.org/abs/2507.14809", "authors": ["Zesen Zhong", "Duomin Zhang", "Yijia Li"], "title": "Light Future: Multimodal Action Frame Prediction via InstructPix2Pix", "comment": "9 pages including appendix, 5 tables, 8 figures, to be submitted to\n  WACV 2026", "summary": "Predicting future motion trajectories is a critical capability across domains\nsuch as robotics, autonomous systems, and human activity forecasting, enabling\nsafer and more intelligent decision-making. This paper proposes a novel,\nefficient, and lightweight approach for robot action prediction, offering\nsignificantly reduced computational cost and inference latency compared to\nconventional video prediction models. Importantly, it pioneers the adaptation\nof the InstructPix2Pix model for forecasting future visual frames in robotic\ntasks, extending its utility beyond static image editing. We implement a deep\nlearning-based visual prediction framework that forecasts what a robot will\nobserve 100 frames (10 seconds) into the future, given a current image and a\ntextual instruction. We repurpose and fine-tune the InstructPix2Pix model to\naccept both visual and textual inputs, enabling multimodal future frame\nprediction. Experiments on the RoboTWin dataset (generated based on real-world\nscenarios) demonstrate that our method achieves superior SSIM and PSNR compared\nto state-of-the-art baselines in robot action prediction tasks. Unlike\nconventional video prediction models that require multiple input frames, heavy\ncomputation, and slow inference latency, our approach only needs a single image\nand a text prompt as input. This lightweight design enables faster inference,\nreduced GPU demands, and flexible multimodal control, particularly valuable for\napplications like robotics and sports motion trajectory analytics, where motion\ntrajectory precision is prioritized over visual fidelity.", "AI": {"tldr": "本文提出了一种新颖、高效且轻量级的机器人动作预测方法，通过改编InstructPix2Pix模型，利用单一图像和文本指令预测机器人未来的视觉帧，显著降低了计算成本和推理延迟。", "motivation": "在机器人、自动驾驶系统和人类活动预测等领域，预测未来运动轨迹至关重要，能实现更安全、更智能的决策。然而，传统视频预测模型计算成本高昂且推理延迟大，需要更高效、轻量级的解决方案。", "method": "该研究将InstructPix2Pix模型首次应用于预测机器人任务中的未来视觉帧，并对其进行重新调整和微调，使其能够接受视觉和文本输入，实现多模态的未来帧预测。该框架仅需当前图像和文本指令作为输入，即可预测未来100帧（10秒）的机器人观察。在基于真实场景生成的RoboTWin数据集上进行了实验。", "result": "实验结果表明，与现有最先进的基线方法相比，该方法在机器人动作预测任务中取得了更优的SSIM和PSNR。与需要多帧输入、计算量大和推理慢的传统视频预测模型不同，该方法仅需单张图像和文本提示作为输入，实现了更快的推理速度、更低的GPU需求和灵活的多模态控制。", "conclusion": "本文提出了一种高效、轻量级的机器人动作预测方法，通过创新性地应用InstructPix2Pix模型，实现了基于单图像和文本输入的未来视觉帧预测。该方法在计算效率和性能上均表现出色，特别适用于对运动轨迹精度而非视觉保真度要求更高的机器人和体育运动轨迹分析等实时应用。"}}
{"id": "2507.14922", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.14922", "abs": "https://arxiv.org/abs/2507.14922", "authors": ["Vahid Rahimzadeh", "Erfan Moosavi Monazzah", "Mohammad Taher Pilehvar", "Yadollah Yaghoobzadeh"], "title": "SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs", "comment": null, "summary": "Persona-driven LLMs have emerged as powerful tools in computational social\nscience, yet existing approaches fall at opposite extremes, either relying on\ncostly human-curated data or producing synthetic personas that lack consistency\nand realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from\n10,000 real social media users from BlueSky open platform across three time\nwindows, bridging this spectrum by grounding synthetic generation in authentic\nuser activity. Our evaluation demonstrates that SYNTHIA achieves competitive\nperformance with state-of-the-art methods in demographic diversity and social\nsurvey alignment while significantly outperforming them in narrative\nconsistency. Uniquely, SYNTHIA incorporates temporal dimensionality and\nprovides rich social interaction metadata from the underlying network, enabling\nnew research directions in computational social science and persona-driven\nlanguage modeling.", "AI": {"tldr": "该论文介绍了SYNTHIA，一个包含30,000个用户背景故事的数据集，旨在弥合现有角色驱动LLM在数据成本与合成质量之间的差距，并通过真实用户活动增强合成故事的一致性和真实性。", "motivation": "现有角色驱动LLM方法存在两极分化：要么依赖昂贵的人工标注数据，要么生成缺乏一致性和真实感的合成角色。这促使研究者寻求一种结合真实性与可扩展性的新方法。", "method": "研究者引入了SYNTHIA数据集，该数据集包含从BlueSky开放平台上的10,000名真实社交媒体用户在三个时间窗口中提取的30,000个背景故事。通过将合成生成基于真实的A用户活动，该方法旨在提高生成故事的一致性。SYNTHIA还独特地融入了时间维度和丰富的社交互动元数据。", "result": "评估结果表明，SYNTHIA在人口统计多样性和社会调查对齐方面与现有最先进方法表现相当，但在叙事一致性方面显著优于它们。其独特的时间维度和社交互动元数据为计算社会科学和角色驱动语言建模开辟了新的研究方向。", "conclusion": "SYNTHIA数据集通过将合成生成与真实用户活动相结合，成功弥合了现有角色驱动LLM方法的差距，在保持竞争性表现的同时显著提高了叙事一致性。其独特的时间和社交元数据特性为未来的研究提供了丰富的基础。"}}
{"id": "2507.14697", "categories": ["cs.CV", "I.4.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.14697", "abs": "https://arxiv.org/abs/2507.14697", "authors": ["Zhiwei Zhang", "Zi Ye", "Yibin Wen", "Shuai Yuan", "Haohuan Fu", "Jianxi Huang", "Juepeng Zheng"], "title": "GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset", "comment": "38 pages, 18 figures, submitted to NeurIPS 2025", "summary": "Agricultural parcels serve as basic units for conducting agricultural\npractices and applications, which is vital for land ownership registration,\nfood security assessment, soil erosion monitoring, etc. However, existing\nagriculture parcel extraction studies only focus on mid-resolution mapping or\nregular plain farmlands while lacking representation of complex terraced\nterrains due to the demands of precision agriculture.In this paper, we\nintroduce a more fine-grained terraced parcel dataset named GTPBD (Global\nTerraced Parcel and Boundary Dataset), which is the first fine-grained dataset\ncovering major worldwide terraced regions with more than 200,000 complex\nterraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution\nimages with three-level labels, including pixel-level boundary labels, mask\nlabels, and parcel labels. It covers seven major geographic zones in China and\ntranscontinental climatic regions around the world.Compared to the existing\ndatasets, the GTPBD dataset brings considerable challenges due to the: (1)\nterrain diversity; (2) complex and irregular parcel objects; and (3) multiple\ndomain styles. Our proposed GTPBD dataset is suitable for four different tasks,\nincluding semantic segmentation, edge detection, terraced parcel extraction,\nand unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the\nGTPBD dataset on eight semantic segmentation methods, four edge extraction\nmethods, three parcel extraction methods, and five UDA methods, along with a\nmulti-dimensional evaluation framework integrating pixel-level and object-level\nmetrics. GTPBD fills a critical gap in terraced remote sensing research,\nproviding a basic infrastructure for fine-grained agricultural terrain analysis\nand cross-scenario knowledge transfer.", "AI": {"tldr": "本文介绍了GTPBD，一个全球首个针对复杂梯田地形的精细化农业地块和边界数据集，旨在填补现有研究在梯田区域数据上的空白，并为多项遥感任务提供基准。", "motivation": "现有农业地块提取研究主要关注中等分辨率的平原农田，缺乏对复杂梯田地形的精细化表示。这限制了精准农业在梯田区域的应用，而梯田地块数据对于土地登记、粮食安全评估和水土流失监测至关重要。", "method": "研究者构建了一个名为GTPBD（全球梯田地块和边界数据集）的精细化梯田地块数据集。该数据集包含超过20万个复杂梯田地块的手动标注，覆盖全球主要梯田区域，共47,537张高分辨率图像，并提供像素级边界、掩膜和地块三级标签。该数据集适用于语义分割、边缘检测、梯田地块提取和无监督域适应（UDA）等任务。作者还在GTPBD数据集上对八种语义分割方法、四种边缘提取方法、三种地块提取方法和五种UDA方法进行了基准测试，并采用了整合像素级和对象级指标的多维度评估框架。", "result": "GTPBD数据集因其地形多样性、复杂不规则的地块对象以及多样的领域风格，带来了显著挑战。该数据集被证明适用于语义分割、边缘检测、梯田地块提取和无监督域适应等四种不同的任务，并已在多项基准测试中得到了验证。", "conclusion": "GTPBD数据集填补了梯田遥感研究中的关键空白，为精细化农业地形分析和跨场景知识迁移提供了基础架构，对于未来的精准农业发展具有重要意义。"}}
{"id": "2507.15411", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15411", "abs": "https://arxiv.org/abs/2507.15411", "authors": ["Wissam Gherissi", "Mehdi Acheli", "Joyce El Haddad", "Daniela Grigori"], "title": "Predictive Process Monitoring Using Object-centric Graph Embeddings", "comment": "ICSOC Workshops 2024, Dec 2024, Tunis, Tunisia", "summary": "Object-centric predictive process monitoring explores and utilizes\nobject-centric event logs to enhance process predictions. The main challenge\nlies in extracting relevant information and building effective models. In this\npaper, we propose an end-to-end model that predicts future process behavior,\nfocusing on two tasks: next activity prediction and next event time. The\nproposed model employs a graph attention network to encode activities and their\nrelationships, combined with an LSTM network to handle temporal dependencies.\nEvaluated on one reallife and three synthetic event logs, the model\ndemonstrates competitive performance compared to state-of-the-art methods.", "AI": {"tldr": "该论文提出了一个端到端模型，结合图注意力网络（GAT）和LSTM，用于对象中心预测过程监控，旨在预测下一个活动和下一个事件时间。", "motivation": "利用对象中心事件日志提升过程预测能力，主要挑战在于信息提取和有效模型构建。", "method": "提出一个端到端模型，利用图注意力网络（GAT）编码活动及其关系，并结合LSTM网络处理时间依赖性，专注于预测下一个活动和下一个事件时间。", "result": "在1个真实和3个合成事件日志上评估，该模型与现有最先进方法相比，表现出具有竞争力的性能。", "conclusion": "所提出的结合GAT和LSTM的端到端模型，能有效进行对象中心预测过程监控，并在未来行为预测任务中表现出色。"}}
{"id": "2507.15036", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15036", "abs": "https://arxiv.org/abs/2507.15036", "authors": ["Lyes Saad Saoud", "Irfan Hussain"], "title": "EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring", "comment": null, "summary": "Underwater image enhancement is vital for marine conservation, particularly\ncoral reef monitoring. However, AI-based enhancement models often face dataset\nbias, high computational costs, and lack of transparency, leading to potential\nmisinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware\nAI framework to address these challenges. EBA-AI leverages CLIP embeddings to\ndetect and mitigate dataset bias, ensuring balanced representation across\nvaried underwater environments. It also integrates adaptive processing to\noptimize energy efficiency, significantly reducing GPU usage while maintaining\ncompetitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100\nshow that while PSNR drops by a controlled 1.0 dB, computational savings enable\nreal-time feasibility for large-scale marine monitoring. Additionally,\nuncertainty estimation and explainability techniques enhance trust in AI-driven\nenvironmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,\nWaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing\nefficiency, fairness, and interpretability in underwater image processing. By\naddressing key limitations of AI-driven enhancement, this work contributes to\nsustainable, bias-aware, and computationally efficient marine conservation\nefforts. For interactive visualizations, animations, source code, and access to\nthe preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/", "AI": {"tldr": "本文提出EBA-AI，一个伦理指导的偏见感知AI框架，用于水下图像增强，旨在解决现有AI模型的数据集偏差、高计算成本和透明度不足问题，实现高效、公平和可解释的海洋监测。", "motivation": "水下图像增强对于海洋保护至关重要，但现有的基于AI的增强模型常面临数据集偏差、计算成本高和缺乏透明度的问题，可能导致误判，尤其是在珊瑚礁监测中。", "method": "EBA-AI框架利用CLIP嵌入来检测和减轻数据集偏差，确保水下环境中表示的平衡性。它还集成了自适应处理以优化能效，显著降低GPU使用。此外，该框架还包含不确定性估计和可解释性技术，以增强对AI驱动环境决策的信任。", "result": "实验在LSUI400、Oceanex和UIEB100数据集上进行，结果显示，尽管PSNR略有下降（可控的1.0 dB），但计算成本显著降低（减少GPU使用），从而使大规模海洋监测具备实时可行性。与CycleGAN、FunIEGAN、RAUNENet、WaterNet、UGAN、PUGAN和UTUIE等模型的比较验证了EBA-AI在平衡效率、公平性和可解释性方面的有效性。", "conclusion": "EBA-AI通过解决AI驱动增强的关键局限性，有效平衡了效率、公平性和可解释性，为可持续、偏见感知和计算高效的海洋保护工作做出了贡献。"}}
{"id": "2507.14958", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14958", "abs": "https://arxiv.org/abs/2507.14958", "authors": ["Hang Yan", "Fangzhi Xu", "Rongman Xu", "Yifei Li", "Jian Zhang", "Haoran Luo", "Xiaobao Wu", "Luu Anh Tuan", "Haiteng Zhao", "Qika Lin", "Jun Liu"], "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models", "comment": "25 pages, 8 figures", "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%.", "AI": {"tldr": "该研究提出了一种名为MUR（Momentum Uncertainty-guided Reasoning）的新方法，通过动态分配思考预算来提高大型语言模型（LLMs）的推理效率，在平均减少50%计算量的同时，将准确率提高了0.62-3.37%。", "motivation": "LLMs在推理任务上表现出色，但优化其推理效率仍是挑战。测试时间缩放（TTS）虽然能提高推理质量，但常导致“过度思考”，浪费令牌在冗余计算上。", "method": "受物理学中动量概念启发，提出了动量不确定性引导推理（MUR）。MUR通过跟踪和聚合随时间变化的逐步不确定性，动态地将思考预算分配给关键推理步骤。此外，引入了gamma-control机制，通过一个超参数灵活控制推理预算。提供了理论证明支持MUR在稳定性和偏差方面的优越性。", "result": "MUR在MATH-500、AIME24、AIME25和GPQA-diamond四个基准测试中，使用不同尺寸的Qwen3模型（1.7B、4B、8B）进行了全面评估。结果显示，MUR平均减少了超过50%的计算量，同时将准确率提高了0.62-3.37%。", "conclusion": "MUR是一种有效且自适应地指导LLM测试时间缩放的方法，无需额外训练，显著提高了推理效率，减少了计算成本并提升了准确性。"}}
{"id": "2507.14738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14738", "abs": "https://arxiv.org/abs/2507.14738", "authors": ["Jeannie She", "Katie Spivakovsky"], "title": "MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy", "comment": null, "summary": "Diabetic retinopathy (DR) is a leading cause of preventable blindness,\naffecting over 100 million people worldwide. In the United States, individuals\nfrom lower-income communities face a higher risk of progressing to advanced\nstages before diagnosis, largely due to limited access to screening. Comorbid\nconditions further accelerate disease progression. We propose MultiRetNet, a\nnovel pipeline combining retinal imaging, socioeconomic factors, and\ncomorbidity profiles to improve DR staging accuracy, integrated with a clinical\ndeferral system for a clinical human-in-the-loop implementation. We experiment\nwith three multimodal fusion methods and identify fusion through a fully\nconnected layer as the most versatile methodology. We synthesize adversarial,\nlow-quality images and use contrastive learning to train the deferral system,\nguiding the model to identify out-of-distribution samples that warrant\nclinician review. By maintaining diagnostic accuracy on suboptimal images and\nintegrating critical health data, our system can improve early detection,\nparticularly in underserved populations where advanced DR is often first\nidentified. This approach may reduce healthcare costs, increase early detection\nrates, and address disparities in access to care, promoting healthcare equity.", "AI": {"tldr": "MultiRetNet是一种结合视网膜影像、社会经济因素和共病信息的糖尿病视网膜病变（DR）分期系统，旨在通过临床转诊系统提高诊断准确性，尤其是在服务不足地区。", "motivation": "糖尿病视网膜病变是可预防性失明的主要原因，全球影响超过1亿人。低收入社区居民因筛查机会有限，更容易在诊断前发展到晚期。共病条件会加速疾病进展。研究旨在改进早期检测，尤其是在医疗资源匮乏地区，以解决医疗不平等问题。", "method": "提出MultiRetNet管道，结合视网膜影像、社会经济因素和共病信息来提高DR分期准确性。系统整合了临床转诊系统（人机协作）。实验了三种多模态融合方法，发现通过全连接层进行融合是最通用的方法。通过合成对抗性低质量图像并使用对比学习训练转诊系统，引导模型识别需要临床医生审查的异常样本。", "result": "MultiRetNet提高了DR分期准确性。通过全连接层融合被认为是最多功能的方法。系统在次优图像上仍能保持诊断准确性，并能识别出需要临床医生干预的异常样本。该系统能够改善早期检测，特别是在晚期DR常被首次发现的医疗服务不足人群中。", "conclusion": "该方法有望降低医疗成本，提高早期检测率，并解决医疗可及性方面的不平等问题，从而促进医疗公平。"}}
{"id": "2507.15457", "categories": ["cs.AI", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.15457", "abs": "https://arxiv.org/abs/2507.15457", "authors": ["Orlenys López-Pintado", "Jannis Rosenbaum", "Marlon Dumas"], "title": "Optimization of Activity Batching Policies in Business Processes", "comment": null, "summary": "In business processes, activity batching refers to packing multiple activity\ninstances for joint execution. Batching allows managers to trade off cost and\nprocessing effort against waiting time. Larger and less frequent batches may\nlower costs by reducing processing effort and amortizing fixed costs, but they\ncreate longer waiting times. In contrast, smaller and more frequent batches\nreduce waiting times but increase fixed costs and processing effort. A batching\npolicy defines how activity instances are grouped into batches and when each\nbatch is activated. This paper addresses the problem of discovering batching\npolicies that strike optimal trade-offs between waiting time, processing\neffort, and cost. The paper proposes a Pareto optimization approach that starts\nfrom a given set (possibly empty) of activity batching policies and generates\nalternative policies for each batched activity via intervention heuristics.\nEach heuristic identifies an opportunity to improve an activity's batching\npolicy with respect to a metric (waiting time, processing time, cost, or\nresource utilization) and an associated adjustment to the activity's batching\npolicy (the intervention). The impact of each intervention is evaluated via\nsimulation. The intervention heuristics are embedded in an optimization\nmeta-heuristic that triggers interventions to iteratively update the Pareto\nfront of the interventions identified so far. The paper considers three\nmeta-heuristics: hill-climbing, simulated annealing, and reinforcement\nlearning. An experimental evaluation compares the proposed approach based on\nintervention heuristics against the same (non-heuristic guided) meta-heuristics\nbaseline regarding convergence, diversity, and cycle time gain of\nPareto-optimal policies.", "AI": {"tldr": "本文提出了一种基于Pareto优化和干预启发式的方法，用于发现业务流程中活动批处理策略，以平衡等待时间、处理工作量和成本。", "motivation": "在业务流程中，活动批处理涉及成本与等待时间的权衡。较大的批次可以降低成本但增加等待时间，而较小的批次则相反。因此，需要找到能够实现等待时间、处理工作量和成本之间最佳权衡的批处理策略。", "method": "该研究采用Pareto优化方法，从现有批处理策略出发，通过“干预启发式”生成替代策略。每个启发式识别一个改进机会（针对等待时间、处理时间、成本或资源利用率），并提出相应的调整。这些干预的影响通过仿真进行评估。干预启发式被嵌入到优化元启发式中（包括爬山法、模拟退火和强化学习），以迭代更新Pareto前沿。", "result": "通过实验评估，将所提出的基于干预启发式的方法与未采用启发式引导的元启发式基线进行了比较，评估指标包括收敛性、多样性和Pareto最优策略的周期时间增益。", "conclusion": "本文提出了一种通过结合干预启发式和元启发式来发现业务流程中活动批处理策略的方法，旨在优化等待时间、处理工作量和成本之间的权衡。"}}
{"id": "2507.15089", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15089", "abs": "https://arxiv.org/abs/2507.15089", "authors": ["Ioannis Tsampikos Papapetros", "Ioannis Kansizoglou", "Antonios Gasteratos"], "title": "Visual Place Recognition for Large-Scale UAV Applications", "comment": null, "summary": "Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial\nVehicle (UAV) navigation, enabling robust localization across diverse\nenvironments. Despite significant advancements, aerial vPR faces unique\nchallenges due to the limited availability of large-scale, high-altitude\ndatasets, which limits model generalization, along with the inherent rotational\nambiguity in UAV imagery. To address these challenges, we introduce LASED, a\nlarge-scale aerial dataset with approximately one million images,\nsystematically sampled from 170,000 unique locations throughout Estonia over a\ndecade, offering extensive geographic and temporal diversity. Its structured\ndesign ensures clear place separation significantly enhancing model training\nfor aerial scenarios. Furthermore, we propose the integration of steerable\nConvolutional Neural Networks (CNNs) to explicitly handle rotational variance,\nleveraging their inherent rotational equivariance to produce robust,\norientation-invariant feature representations. Our extensive benchmarking\ndemonstrates that models trained on LASED achieve significantly higher recall\ncompared to those trained on smaller, less diverse datasets, highlighting the\nbenefits of extensive geographic coverage and temporal diversity. Moreover,\nsteerable CNNs effectively address rotational ambiguity inherent in aerial\nimagery, consistently outperforming conventional convolutional architectures,\nachieving on average 12\\% recall improvement over the best-performing\nnon-steerable network. By combining structured, large-scale datasets with\nrotation-equivariant neural networks, our approach significantly enhances model\nrobustness and generalization for aerial vPR.", "AI": {"tldr": "本文介绍了LASED，一个大规模、高空视觉地点识别数据集，并提出结合可操纵卷积神经网络来解决无人机图像中的旋转模糊和数据不足问题，显著提升了模型性能。", "motivation": "无人机视觉地点识别（vPR）面临两大挑战：一是缺乏大规模、高空数据集，限制了模型泛化能力；二是无人机图像固有的旋转模糊性。", "method": "1. 引入LASED数据集：一个包含约一百万张图像的大规模高空数据集，系统地从爱沙尼亚17万个独特地点采样，具有广泛的地理和时间多样性。2. 整合可操纵卷积神经网络（steerable CNNs）：利用其固有的旋转等变性来明确处理旋转方差，生成鲁棒的、方向不变的特征表示。", "result": "1. 在LASED上训练的模型比在小型、多样性较低的数据集上训练的模型召回率显著更高，突出了广泛地理覆盖和时间多样性的优势。2. 可操纵CNN有效解决了航空图像中固有的旋转模糊性，平均比表现最佳的非可操纵网络召回率提高了12%。", "conclusion": "通过结合结构化、大规模数据集和旋转等变神经网络，该方法显著增强了航空视觉地点识别模型的鲁棒性和泛化能力。"}}
{"id": "2507.15024", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15024", "abs": "https://arxiv.org/abs/2507.15024", "authors": ["Qiaoyu Tang", "Hao Xiang", "Le Yu", "Bowen Yu", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun", "Junyang Lin"], "title": "RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback", "comment": null, "summary": "With the rapid advancement of Large Language Models (LLMs), developing\neffective critic modules for precise guidance has become crucial yet\nchallenging. In this paper, we initially demonstrate that supervised\nfine-tuning for building critic modules (which is widely adopted in current\nsolutions) fails to genuinely enhance models' critique abilities, producing\nsuperficial critiques with insufficient reflections and verifications. To\nunlock the unprecedented critique capabilities, we propose RefCritic, a\nlong-chain-of-thought critic module based on reinforcement learning with dual\nrule-based rewards: (1) instance-level correctness of solution judgments and\n(2) refinement accuracies of the policy model based on critiques, aiming to\ngenerate high-quality evaluations with actionable feedback that effectively\nguides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and\nDeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement\nsettings, RefCritic demonstrates consistent advantages across all benchmarks,\ne.g., 6.8\\% and 7.2\\% gains on AIME25 for the respective base models. Notably,\nunder majority voting, policy models filtered by RefCritic show superior\nscaling with increased voting numbers. Moreover, despite training on\nsolution-level supervision, RefCritic outperforms step-level supervised\napproaches on ProcessBench, a benchmark to identify erroneous steps in\nmathematical reasoning.", "AI": {"tldr": "本文提出RefCritic，一个基于强化学习和长链思维的批评模块，通过双重规则奖励克服了传统监督微调的局限性，显著提升了大型语言模型的批评和模型精炼能力。", "motivation": "当前用于构建批评模块的监督微调（SFT）方法未能真正提升模型的批评能力，导致批评流于表面，缺乏深度反思和验证，因此需要更有效的方法来提供精确指导。", "method": "本文提出了RefCritic，一个基于强化学习的长链思维批评模块。它采用双重基于规则的奖励机制：1) 解决方案判断的实例级正确性；2) 基于批评对策略模型进行精炼的准确性。该方法在Qwen2.5-14B-Instruct和DeepSeek-R1-Distill-Qwen-14B模型上，通过五个基准进行评估。", "result": "RefCritic在所有基准测试的批评和精炼设置中均表现出持续优势，例如在AIME25上分别获得了6.8%和7.2%的提升。在多数投票下，经RefCritic过滤的策略模型显示出更优异的扩展性。此外，尽管仅在解决方案级别的监督下进行训练，RefCritic在ProcessBench上仍优于步骤级监督方法。", "conclusion": "RefCritic成功地解锁了大型语言模型前所未有的批评能力，能够生成高质量的评估和可操作的反馈，有效指导模型精炼，并在多个基准测试中展现出显著的性能提升。"}}
{"id": "2507.14743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14743", "abs": "https://arxiv.org/abs/2507.14743", "authors": ["Joseph Raj Vishal", "Rutuja Patil", "Manas Srinivas Gowda", "Katha Naik", "Yezhou Yang", "Bharatesh Chakravarthi"], "title": "InterAct-Video: Reasoning-Rich Video QA for Urban Traffic", "comment": null, "summary": "Traffic monitoring is crucial for urban mobility, road safety, and\nintelligent transportation systems (ITS). Deep learning has advanced\nvideo-based traffic monitoring through video question answering (VideoQA)\nmodels, enabling structured insight extraction from traffic videos. However,\nexisting VideoQA models struggle with the complexity of real-world traffic\nscenes, where multiple concurrent events unfold across spatiotemporal\ndimensions. To address these challenges, this paper introduces \\textbf{InterAct\nVideoQA}, a curated dataset designed to benchmark and enhance VideoQA models\nfor traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of\nreal-world traffic footage collected from diverse intersections, segmented into\n10-second video clips, with over 25,000 question-answer (QA) pairs covering\nspatiotemporal dynamics, vehicle interactions, incident detection, and other\ncritical traffic attributes. State-of-the-art VideoQA models are evaluated on\nInterAct VideoQA, exposing challenges in reasoning over fine-grained\nspatiotemporal dependencies within complex traffic scenarios. Additionally,\nfine-tuning these models on InterAct VideoQA yields notable performance\nimprovements, demonstrating the necessity of domain-specific datasets for\nVideoQA. InterAct VideoQA is publicly available as a benchmark dataset to\nfacilitate future research in real-world deployable VideoQA models for\nintelligent transportation systems. GitHub Repo:\nhttps://github.com/joe-rabbit/InterAct_VideoQA", "AI": {"tldr": "该论文引入了InterAct VideoQA数据集，旨在提升和评估视频问答（VideoQA）模型在复杂真实世界交通场景中的性能，并证明了其在交通监控任务中的重要性。", "motivation": "现有的视频问答（VideoQA）模型难以处理真实世界交通场景的复杂性，这些场景中存在多重并发事件和复杂的时空动态，这限制了它们在交通监控中的应用。", "method": "研究者构建了InterAct VideoQA数据集，包含8小时来自不同交叉口的真实交通视频（10秒片段），以及超过25,000个涵盖时空动态、车辆交互、事件检测等交通属性的问答对。他们使用该数据集评估了现有的最先进VideoQA模型，并对其进行微调。", "result": "在InterAct VideoQA数据集上评估发现，现有模型在理解复杂交通场景中细粒度时空依赖方面存在挑战。然而，通过在该数据集上对模型进行微调，性能得到了显著提升。", "conclusion": "InterAct VideoQA数据集是交通监控领域VideoQA模型所必需的特定领域数据集，它能促进未来针对智能交通系统中真实世界可部署VideoQA模型的研究。"}}
{"id": "2507.15509", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15509", "abs": "https://arxiv.org/abs/2507.15509", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Yufeng Zhong", "Lin Ma"], "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner", "comment": "technical report", "summary": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).", "AI": {"tldr": "该研究引入了Chart-R1，一个基于强化学习微调的图表领域视觉-语言模型，用于复杂图表推理。它通过新颖的数据合成技术和两阶段训练策略（Chart-COT和Chart-RFT）解决了图表推理数据稀缺和数值敏感性问题，表现优于现有方法并媲美大型模型。", "motivation": "之前的R1-Style方法主要集中在数学推理和代码智能，但在更通用的多模态数据（如图表）上的优势尚未得到验证。图表作为重要的多模态数据类型，具有丰富信息和复杂推理挑战，且缺乏高质量的推理数据，因此有必要探索R1-Style方法在图表领域的应用。", "method": "该研究提出了Chart-R1模型，并采用了以下方法：1. 新颖的程序化数据合成技术，生成高质量的单/多子图表分步推理数据，弥补了图表领域推理数据的不足。2. 两阶段训练策略：a) Chart-COT：通过分步思维链（Chain-of-Thought）监督，将复杂图表推理任务分解为细粒度子任务。b) Chart-RFT：采用群组相对策略优化（group relative policy optimization），并对数值响应采用相对软性奖励，以强调图表领域的数值敏感性。", "result": "实验结果表明，Chart-R1在开源基准和自建图表推理数据集（ChartRQA）上，相比其他图表领域方法具有显著优势，甚至可与GPT-4o、Claude-3.5等开放/闭源大型模型相媲美。", "conclusion": "Chart-R1成功将R1-Style强化学习微调方法应用于图表领域，通过创新的数据合成和两阶段训练策略，有效提升了复杂图表推理能力，并弥补了该领域推理数据不足的缺陷，展示出强大的性能。"}}
{"id": "2507.15496", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15496", "abs": "https://arxiv.org/abs/2507.15496", "authors": ["JunYing Huang", "Ao Xu", "DongSun Yong", "KeRen Li", "YuanFeng Wang", "Qi Qin"], "title": "Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images", "comment": null, "summary": "Odometry is a critical task for autonomous systems for self-localization and\nnavigation. We propose a novel LiDAR-Visual odometry framework that integrates\nLiDAR point clouds and images for accurate and robust pose estimation. Our\nmethod utilizes a dense-depth map estimated from point clouds and images\nthrough depth completion, and incorporates a multi-scale feature extraction\nnetwork with attention mechanisms, enabling adaptive depth-aware\nrepresentations. Furthermore, we leverage dense depth information to refine\nflow estimation and mitigate errors in occlusion-prone regions. Our\nhierarchical pose refinement module optimizes motion estimation progressively,\nensuring robust predictions against dynamic environments and scale ambiguities.\nComprehensive experiments on the KITTI odometry benchmark demonstrate that our\napproach achieves similar or superior accuracy and robustness compared to\nstate-of-the-art visual and LiDAR odometry methods.", "AI": {"tldr": "本文提出了一种新颖的LiDAR-视觉里程计框架，通过融合LiDAR点云和图像来精确且鲁棒地估计姿态。", "motivation": "里程计是自主系统进行自定位和导航的关键任务。", "method": "该方法利用深度补全从点云和图像中估计密集的深度图；采用带有注意力机制的多尺度特征提取网络以实现自适应的深度感知表示；利用密集深度信息来优化光流估计并减轻遮挡区域的误差；并通过分层姿态优化模块逐步优化运动估计。", "result": "在KITTI里程计基准测试上的综合实验表明，该方法与最先进的视觉和LiDAR里程计方法相比，实现了相似或更优的精度和鲁棒性。", "conclusion": "该LiDAR-视觉里程计框架通过有效融合多模态数据，在动态环境和尺度模糊性下，能够提供准确且鲁棒的姿态估计。"}}
{"id": "2507.15061", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15061", "abs": "https://arxiv.org/abs/2507.15061", "authors": ["Zhengwei Tao", "Jialong Wu", "Wenbiao Yin", "Junkai Zhang", "Baixuan Li", "Haiyang Shen", "Kuan Li", "Liwen Zhang", "Xinyu Wang", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization", "comment": null, "summary": "The advent of Large Language Model (LLM)-powered agents has revolutionized\nartificial intelligence by enabling solutions to complex, open-ended tasks\nthrough web-based information-seeking (IS) capabilities. The scarcity of\nhigh-quality training data has limited the development of IS agents. Existing\napproaches typically adopt an information-driven paradigm that first collects\nweb data and then generates questions based on the retrieval. However, this may\nlead to inconsistency between information structure and reasoning structure,\nquestion and answer. To mitigate, we propose a formalization-driven IS data\nsynthesis framework WebShaper to construct a dataset. WebShaper systematically\nformalizes IS tasks through set theory. Central to the formalization is the\nconcept of Knowledge Projections (KP), which enables precise control over\nreasoning structure by KP operation compositions. During synthesis, we begin by\ncreating seed tasks, then use a multi-step expansion process. At each step, an\nagentic Expander expands the current formal question more complex with\nretrieval and validation tools based on our formalization. We train our model\non the synthesized dataset. Experiment results demonstrate that WebShaper\nachieves state-of-the-art performance among open-sourced IS agents on GAIA and\nWebWalkerQA benchmarks.", "AI": {"tldr": "本文提出了一种名为WebShaper的形式化驱动信息搜索（IS）数据合成框架，用于为大型语言模型（LLM）驱动的IS智能体生成高质量训练数据，从而提升其在复杂开放式任务中的表现。", "motivation": "现有LLM驱动的IS智能体受限于高质量训练数据的稀缺性。传统的信息驱动范式（先收集数据再生成问题）可能导致信息结构与推理结构、问题与答案之间存在不一致。", "method": "WebShaper框架通过集合论系统地形式化IS任务，并引入“知识投影”（Knowledge Projections, KP）概念，通过KP操作组合精确控制推理结构。数据合成过程包括创建种子任务，然后通过一个多步骤扩展过程，其中代理扩展器（Expander）利用检索和验证工具，基于形式化定义逐步扩展当前形式化问题，使其更复杂。", "result": "在WebShaper合成的数据集上训练的模型，在GAIA和WebWalkerQA基准测试中，在开源IS智能体中取得了最先进（state-of-the-art）的性能。", "conclusion": "通过形式化驱动的数据合成方法，WebShaper有效解决了信息搜索智能体训练数据稀缺和质量问题，显著提升了其在复杂网络信息搜索任务中的能力。"}}
{"id": "2507.14784", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14784", "abs": "https://arxiv.org/abs/2507.14784", "authors": ["Xinxin Dong", "Baoyun Peng", "Haokai Ma", "Yufei Wang", "Zixuan Dong", "Fei Hu", "Xiaodong Wang"], "title": "LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering", "comment": null, "summary": "Video Question Answering (VideoQA) requires identifying sparse critical\nmoments in long videos and reasoning about their causal relationships to answer\nsemantically complex questions. While recent advances in multimodal learning\nhave improved alignment and fusion, current approaches remain limited by two\nprevalent but fundamentally flawed strategies: (1) task-agnostic sampling\nindiscriminately processes all frames, overwhelming key events with irrelevant\ncontent; and (2) heuristic retrieval captures superficial patterns but misses\ncausal-temporal structures needed for complex reasoning. To address these\nchallenges, we introduce LeAdQA, an innovative approach that bridges these gaps\nthrough synergizing causal-aware query refinement with fine-grained visual\ngrounding. Our method first leverages LLMs to reformulate question-option\npairs, resolving causal ambiguities and sharpening temporal focus. These\nrefined queries subsequently direct a temporal grounding model to precisely\nretrieve the most salient segments, complemented by an adaptive fusion\nmechanism dynamically integrating the evidence to maximize relevance. The\nintegrated visual-textual cues are then processed by an MLLM to generate\naccurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and\nNExT-GQA demonstrate that our method's precise visual grounding substantially\nenhances the understanding of video-question relationships, achieving\nstate-of-the-art (SOTA) performance on complex reasoning tasks while\nmaintaining computational efficiency.", "AI": {"tldr": "LeAdQA提出一种新颖的视频问答（VideoQA）方法，通过因果感知查询细化和精细视觉定位，解决了现有方法中任务无关采样和启发式检索的局限性，在复杂推理任务上实现了最先进的性能。", "motivation": "现有VideoQA方法存在缺陷：任务无关采样处理所有帧，导致无关内容过多；启发式检索仅捕获表面模式，忽略复杂推理所需的因果-时间结构。这些问题限制了其在理解语义复杂问题和识别关键时刻方面的能力。", "method": "LeAdQA方法包括：1. 利用大型语言模型（LLMs）重构问题-选项对，以解决因果模糊并增强时间聚焦。2. 精炼后的查询指导时间定位模型精确检索最显著的视频片段。3. 采用自适应融合机制动态整合证据，最大化相关性。4. 多模态大型语言模型（MLLM）处理整合后的视觉-文本线索以生成准确答案。", "result": "在NExT-QA、IntentQA和NExT-GQA数据集上的实验表明，LeAdQA的精确视觉定位显著增强了视频-问题关系的理解，在复杂推理任务上实现了最先进（SOTA）的性能，同时保持了计算效率。", "conclusion": "LeAdQA通过协同因果感知查询细化和精细视觉定位，有效弥合了现有VideoQA方法的不足，大幅提升了对视频-问题关系的理解，并在复杂推理任务中展现出卓越的性能。"}}
{"id": "2507.15518", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15518", "abs": "https://arxiv.org/abs/2507.15518", "authors": ["Sizhou Chen", "Shufan Jiang", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics", "comment": null, "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.", "AI": {"tldr": "HAMLET是一个多智能体框架，利用大型语言模型（LLM）实现沉浸式、交互式戏剧体验，通过赋予AI演员自主思维和与物理环境的交互能力，克服了现有方法的局限。", "motivation": "现有基于LLM的戏剧生成方法存在AI智能体缺乏主动性、无法与物理环境互动以及需要详细用户输入的问题，这降低了在线实时表演的互动性和沉浸感。", "method": "本文提出了HAMLET框架，一个专注于戏剧创作和在线表演的多智能体框架。该框架在给定简单主题后，首先生成叙事蓝图。在在线表演过程中，每个演员（AI智能体）被赋予自主思维，能够根据自身背景、目标和情绪状态独立做出决策。除了与其他演员对话外，他们的决策还可以通过诸如打开信件、拿起武器等动作改变场景道具的状态。这些状态变化会广播给其他相关演员，更新他们的认知，进而影响其后续行动。为评估戏剧表演质量，本文设计了一种评估方法，从角色表现、叙事质量和互动体验三个主要方面进行评估。", "result": "实验评估表明，HAMLET能够创造出富有表现力且连贯的戏剧体验。", "conclusion": "HAMLET框架成功解决了LLM在戏剧生成中智能体缺乏主动性和环境交互的挑战，显著提升了在线实时表演的互动性和沉浸感。"}}
{"id": "2507.15597", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15597", "abs": "https://arxiv.org/abs/2507.15597", "authors": ["Hao Luo", "Yicheng Feng", "Wanpeng Zhang", "Sipeng Zheng", "Ye Wang", "Haoqi Yuan", "Jiazheng Liu", "Chaoyi Xu", "Qin Jin", "Zongqing Lu"], "title": "Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos", "comment": "37 pages", "summary": "We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained\non large-scale human videos. Existing VLAs struggle with complex manipulation\ntasks requiring high dexterity and generalize poorly to novel scenarios and\ntasks, primarily due to their reliance on synthetic data with significant\nsim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To\naddress this data bottleneck, we propose leveraging human hands as a foundation\nmanipulator, capitalizing on the rich dexterity and scalability present in web\ndata. Our approach centers on physical instruction tuning, a novel training\nparadigm that combines large-scale VLA pretraining from human videos, physical\nspace alignment for 3D reasoning, and post-training adaptation for robotic\ntasks. Additionally, we introduce a part-level motion tokenization method which\nachieves millimeter-level reconstruction accuracy to model precise hand\ntrajectories for action learning. To support our proposed paradigm, we further\ndevelop a comprehensive data curation pipeline that integrates heterogeneous\nsources -- including motion capture, VR, and RGB-only videos -- into a\nlarge-scale dataset with millions of motion-based instructional instances. We\nempirically show the excellence of Being-H0 in hand motion generation and\ninstruction following, and it also scales well with model and data sizes.\nImportantly, we observe the expected gains of Being-H0 in real-world robotic\nmanipulation as physical instruction tuning is applied. More details are\navailable at https://beingbeyond.github.io/Being-H0.", "AI": {"tldr": "Being-H0是一个基于大规模人类视频训练的灵巧视觉-语言-动作（VLA）模型，通过“物理指令微调”和部件级运动标记化，显著提升了机器人灵巧操作能力和泛化性。", "motivation": "现有VLA模型在复杂灵巧操作和新场景泛化方面表现不佳，主要原因是依赖合成数据（存在显著的模拟-真实差距）或规模和多样性不足的远程操作演示数据。", "method": "该研究提出利用人类手部作为基础操作器，并引入了“物理指令微调”训练范式，包括：1) 基于人类视频的大规模VLA预训练；2) 用于3D推理的物理空间对齐；3) 针对机器人任务的后训练适应。此外，还引入了部件级运动标记化方法以实现毫米级精度轨迹建模，并开发了整合多源数据（动作捕捉、VR、纯RGB视频）的大规模数据集。", "result": "Being-H0在手部运动生成和指令遵循方面表现出色，并能很好地随模型和数据规模扩展。更重要的是，在应用物理指令微调后，Being-H0在真实世界机器人操作中取得了预期的性能提升。", "conclusion": "Being-H0通过其创新的训练范式和数据利用策略，有效解决了现有VLA模型在灵巧操作和泛化性上的数据瓶颈问题，在真实世界机器人操作中展现了卓越的性能。"}}
{"id": "2507.15087", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15087", "abs": "https://arxiv.org/abs/2507.15087", "authors": ["Chenlei Gong", "Yuanhe Tian", "Lei Mao", "Yan Song"], "title": "Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling", "comment": null, "summary": "Currently, many studies view DNA sequences as a special type of language and\nutilize Transformers to model them. These studies use fixed-length k-mer\nsegmentation and BPE subword tokenization but lack a systematic evaluation to\ndetermine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a\n4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,\nAliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and\n24-layer Transformer encoders and evaluated on GUE benchmark dataset. In\ngeneral, BPE delivers higher and more stable performance across tasks by\ncompressing frequent motifs into variable-length tokens, reducing sequence\nlength, and improving model generalization. RoPE excels at capturing periodic\nmotifs and extrapolating to long sequences, while AliBi also performs well on\ntasks driven by local dependencies. In terms of depth, we observe significant\ngains when increasing layers from 3 to 12, with only marginal improvements or\nslight overfitting at 24 layers. This study provides practical guidance for\ndesigning tokenization and positional encoding in DNA Transformer models.", "AI": {"tldr": "该研究系统比较了DNA Transformer模型中k-mer与BPE分词方法以及不同位置编码的性能，发现BPE通常表现更优，并提供了模型设计实践指导。", "motivation": "现有DNA序列Transformer研究常使用固定长度k-mer或BPE子词分词，但缺乏系统性评估以确定哪种方法更优。", "method": "研究比较了k=1,3,4,5,6的k-mer分词、4096个词汇的BPE分词，以及正弦、AliBi和RoPE三种位置编码方法。每种配置都在3、6、12和24层的Transformer编码器中从头训练，并在GUE基准数据集上进行评估。", "result": "BPE通过压缩频繁的基序、缩短序列长度和提高模型泛化能力，通常在各项任务中提供更高且更稳定的性能。RoPE在捕获周期性基序和外推长序列方面表现出色，而AliBi在局部依赖驱动的任务中也表现良好。在模型深度方面，层数从3增加到12时性能显著提升，而增加到24层时仅有微小改进或出现轻微过拟合。", "conclusion": "本研究为DNA Transformer模型中分词和位置编码的设计提供了实践指导。"}}
{"id": "2507.14787", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14787", "abs": "https://arxiv.org/abs/2507.14787", "authors": ["Xi Xiao", "Aristeidis Tsaris", "Anika Tabassum", "John Lagergren", "Larry M. York", "Tianyang Wang", "Xiao Wang"], "title": "FOCUS: Fused Observation of Channels for Unveiling Spectra", "comment": null, "summary": "Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous\nwavelength bands, making it a powerful tool in biology, agriculture, and\nenvironmental monitoring. However, interpreting Vision Transformers (ViTs) in\nthis setting remains largely unexplored due to two key challenges: (1) existing\nsaliency methods struggle to capture meaningful spectral cues, often collapsing\nattention onto the class token, and (2) full-spectrum ViTs are computationally\nprohibitive for interpretability, given the high-dimensional nature of HSI\ndata. We present FOCUS, the first framework that enables reliable and efficient\nspatial-spectral interpretability for frozen ViTs. FOCUS introduces two core\ncomponents: class-specific spectral prompts that guide attention toward\nsemantically meaningful wavelength groups, and a learnable [SINK] token trained\nwith an attraction loss to absorb noisy or redundant attention. Together, these\ndesigns make it possible to generate stable and interpretable 3D saliency maps\nand spectral importance curves in a single forward pass, without any gradient\nbackpropagation or backbone modification. FOCUS improves band-level IoU by 15\npercent, reduces attention collapse by over 40 percent, and produces saliency\nresults that align closely with expert annotations. With less than 1 percent\nparameter overhead, our method makes high-resolution ViT interpretability\npractical for real-world hyperspectral applications, bridging a long-standing\ngap between black-box modeling and trustworthy HSI decision-making.", "AI": {"tldr": "FOCUS是一个针对高光谱图像（HSI）中冻结Vision Transformers（ViTs）的解释性框架，它通过引入类别特定的光谱提示和可学习的[SINK]令牌，实现了可靠且高效的空间-光谱可解释性，克服了现有方法的挑战。", "motivation": "现有方法在解释高光谱Vision Transformers（ViTs）时面临两大挑战：1) 现有的显著性方法难以捕获有意义的光谱线索，注意力常集中于类别令牌；2) 鉴于高光谱数据的高维度特性，全光谱ViTs的解释性在计算上成本过高。", "method": "本研究提出了FOCUS框架，包含两个核心组件：1) 类别特定的光谱提示，用于引导注意力关注语义上有意义的波长组；2) 一个可学习的[SINK]令牌，通过吸引损失训练以吸收噪声或冗余注意力。这些设计使得在单次前向传播中即可生成稳定且可解释的3D显著性图和光谱重要性曲线，无需梯度反向传播或主干网络修改。", "result": "FOCUS将波段级别的IoU提高了15%，将注意力崩溃减少了40%以上，并生成了与专家标注高度一致的显著性结果。该方法仅需不到1%的参数开销，使得高分辨率ViT的可解释性在实际高光谱应用中变得可行。", "conclusion": "FOCUS弥合了黑盒模型与可信赖高光谱决策之间的长期差距，为高光谱应用中的ViT提供了实用且可靠的空间-光谱可解释性。"}}
{"id": "2507.15521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15521", "abs": "https://arxiv.org/abs/2507.15521", "authors": ["Cole Robertson", "Philip Wolff"], "title": "LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning", "comment": "Manuscript comprises 14 pages, 4 figures, 4 tables in the Technical\n  Appendix and Supplementary Material, and is under review at NeurIPS 2025", "summary": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems.", "AI": {"tldr": "该研究使用滑轮系统测试大型语言模型（LLMs）是否构建并操纵内部世界模型，发现LLMs能利用统计关联和近似空间关系，但缺乏对细微结构连接的推理能力。", "motivation": "研究旨在探究LLMs是构建并操纵内部世界模型，还是仅依赖输出层令牌概率所表示的统计关联。", "method": "研究借鉴人类心理模型研究的认知科学方法，使用TikZ渲染的滑轮系统图像作为刺激，设计了三项研究：研究1测试LLMs估计机械优势（MA）的能力；研究2探测LLMs是否能表示对MA估计至关重要的全局特征；研究3进一步测试LLMs区分功能性系统与无力传递连接系统的能力。", "result": "研究1发现LLMs的MA估计略高于随机水平且与真实值显著相关，可能采用了滑轮计数启发式。研究2中，LLMs能以F1=0.8的准确率区分功能性与伪造系统，表明能近似表示组件的空间关系。然而，研究3中LLMs在区分功能性与无力传递系统时表现接近随机猜测（F1=0.46），表明缺乏对细微结构连接的推理能力。总体而言，结果支持LLMs可能操纵内部世界模型，足以利用统计关联和近似空间关系，但可能缺乏对复杂结构连接的推理能力。", "conclusion": "LLMs可能在一定程度上操纵内部世界模型，足以利用统计关联和近似空间关系，但可能缺乏对细微结构连接的推理能力。研究强调了认知科学方法在评估AI系统世界建模能力方面的实用性。"}}
{"id": "2507.15092", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15092", "abs": "https://arxiv.org/abs/2507.15092", "authors": ["Vijeta Deshpande", "Ishita Dasgupta", "Uttaran Bhattacharya", "Somdeb Sarkhel", "Saayan Mitra", "Anna Rumshisky"], "title": "A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations", "comment": null, "summary": "Synthetic text generated by Large Language Models (LLMs) is increasingly used\nfor further training and improvement of LLMs. Diversity is crucial for the\neffectiveness of synthetic data, and researchers rely on prompt engineering to\nimprove diversity. However, the impact of prompt variations on response text\nlength, and, more importantly, the consequential effect on lexical diversity\nmeasurements, remain underexplored. In this work, we propose Penalty-Adjusted\nType-Token Ratio (PATTR), a diversity metric robust to length variations. We\ngenerate a large synthetic corpus of over 20M words using seven models from the\nLLaMA, OLMo, and Phi families, focusing on a creative writing task of video\nscript generation, where diversity is crucial. We evaluate per-response lexical\ndiversity using PATTR and compare it against existing metrics of Moving-Average\nTTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length\nvariations introduce biases favoring shorter responses. Unlike existing\nmetrics, PATTR explicitly considers the task-specific target response length\n($L_T$) to effectively mitigate length biases. We further demonstrate the\nutility of PATTR in filtering the top-10/100/1,000 most lexically diverse\nresponses, showing that it consistently outperforms MATTR and CR by yielding on\npar or better diversity with high adherence to $L_T$.", "AI": {"tldr": "本文提出了一种名为PATTR的新词汇多样性度量，它对大型语言模型（LLM）生成的文本长度变化具有鲁棒性，有效解决了现有度量中因文本长度引入的偏差，并能更好地筛选出多样性高且符合目标长度的响应。", "motivation": "LLM生成的合成文本被广泛用于进一步训练LLM，其中多样性至关重要。研究人员依赖提示工程来提高多样性，但提示变化对响应文本长度的影响及其对词汇多样性测量的后续影响尚未得到充分探索。现有多样性度量容易受到文本长度变化带来的偏差影响。", "method": "研究者提出了一种名为Penalty-Adjusted Type-Token Ratio (PATTR) 的多样性度量，该度量对长度变化具有鲁棒性。他们使用LLaMA、OLMo和Phi系列的七个模型，生成了一个包含超过2000万单词的大型合成语料库，专注于视频脚本生成的创意写作任务。他们使用PATTR评估了每个响应的词汇多样性，并将其与现有度量（Moving-Average TTR (MATTR) 和 Compression Ratio (CR)）进行了比较。", "result": "分析表明，文本长度变化会引入偏向于短响应的偏差。与现有度量不同，PATTR明确考虑了任务特定的目标响应长度（$L_T$），从而有效减轻了长度偏差。研究还表明，在筛选词汇多样性最高的响应时，PATTR始终优于MATTR和CR，因为它能在保持或提高多样性的同时，高度符合$L_T$。", "conclusion": "PATTR是一种对文本长度变化具有鲁棒性的词汇多样性度量，它能有效解决现有度量中因长度引入的偏差，并在筛选高质量、多样性高且符合目标长度的LLM生成数据方面表现出优越性。"}}
{"id": "2507.14790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14790", "abs": "https://arxiv.org/abs/2507.14790", "authors": ["Wenbo Yue", "Chang Li", "Guoping Xu"], "title": "A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation", "comment": "6 pages, 6 figures", "summary": "In convolutional neural networks (CNNs), downsampling operations are crucial\nto model performance. Although traditional downsampling methods (such as\nmaximum pooling and cross-row convolution) perform well in feature aggregation,\nreceptive field expansion, and computational reduction, they may lead to the\nloss of key spatial information in semantic segmentation tasks, thereby\naffecting the pixel-by-pixel prediction accuracy.To this end, this study\nproposes a downsampling method based on information complementarity - Hybrid\nPooling Downsampling (HPD). The core is to replace the traditional method with\nMinMaxPooling, and effectively retain the light and dark contrast and detail\nfeatures of the image by extracting the maximum value information of the local\narea.Experiment on various CNN architectures on the ACDC and Synapse datasets\nshow that HPD outperforms traditional methods in segmentation performance, and\nincreases the DSC coefficient by 0.5% on average. The results show that the HPD\nmodule provides an efficient solution for semantic segmentation tasks.", "AI": {"tldr": "本研究提出了一种名为混合池化下采样（HPD）的新型下采样方法，用于卷积神经网络，旨在解决传统下采样在语义分割中导致的关键空间信息丢失问题，通过引入MinMaxPooling有效保留图像细节，并在实验中显示出更好的分割性能。", "motivation": "传统的卷积神经网络下采样方法（如最大池化和跨行卷积）在语义分割任务中，虽然有助于特征聚合和计算效率，但可能导致关键空间信息的丢失，从而影响像素级预测的准确性。", "method": "本研究提出了一种基于信息互补的下采样方法——混合池化下采样（HPD）。其核心是将传统方法替换为MinMaxPooling，通过提取局部区域的最大值信息，有效保留图像的光暗对比和细节特征。", "result": "在ACDC和Synapse数据集上对多种CNN架构进行的实验表明，HPD在分割性能上优于传统方法，平均将DSC系数提高了0.5%。", "conclusion": "HPD模块为语义分割任务提供了一种高效的解决方案，有效改善了下采样过程中空间信息丢失的问题，从而提升了分割精度。"}}
{"id": "2507.15532", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15532", "abs": "https://arxiv.org/abs/2507.15532", "authors": ["Kasper Engelen", "Guillermo A. Pérez", "Marnix Suilen"], "title": "Data-Efficient Safe Policy Improvement Using Parametric Structure", "comment": "Accepted at ECAI 2025", "summary": "Safe policy improvement (SPI) is an offline reinforcement learning problem in\nwhich a new policy that reliably outperforms the behavior policy with high\nconfidence needs to be computed using only a dataset and the behavior policy.\nMarkov decision processes (MDPs) are the standard formalism for modeling\nenvironments in SPI. In many applications, additional information in the form\nof parametric dependencies between distributions in the transition dynamics is\navailable. We make SPI more data-efficient by leveraging these dependencies\nthrough three contributions: (1) a parametric SPI algorithm that exploits known\ncorrelations between distributions to more accurately estimate the transition\ndynamics using the same amount of data; (2) a preprocessing technique that\nprunes redundant actions from the environment through a game-based abstraction;\nand (3) a more advanced preprocessing technique, based on satisfiability modulo\ntheory (SMT) solving, that can identify more actions to prune. Empirical\nresults and an ablation study show that our techniques increase the data\nefficiency of SPI by multiple orders of magnitude while maintaining the same\nreliability guarantees.", "AI": {"tldr": "本文通过利用转移动态中的参数依赖关系和剪枝冗余动作，显著提高了安全策略改进（SPI）的数据效率。", "motivation": "安全策略改进（SPI）需要仅使用数据集和行为策略来计算一个能可靠地优于行为策略的新策略。标准马尔可夫决策过程（MDPs）在建模环境时，没有充分利用转移动态中可能存在的参数依赖信息，导致数据效率低下。", "method": "本文提出了三项贡献：1) 一个参数化SPI算法，利用已知的分布相关性更准确地估计转移动态；2) 一种基于博弈的抽象预处理技术，用于剪枝环境中的冗余动作；3) 一种更高级的基于可满足性模理论（SMT）求解的预处理技术，能识别并剪枝更多动作。", "result": "实证结果和消融研究表明，所提出的技术在保持相同可靠性保证的前提下，将SPI的数据效率提高了数个数量级。", "conclusion": "通过利用转移动态中的参数依赖关系和采用先进的动作剪枝技术，可以大幅提升安全策略改进的数据效率，同时保持其可靠性。"}}
{"id": "2507.15100", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15100", "abs": "https://arxiv.org/abs/2507.15100", "authors": ["Chathuri Jayaweera", "Brianna Yanqui", "Bonnie Dorr"], "title": "Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?", "comment": "9 pages, 8 figures and 5 tables", "summary": "Natural Language Inference (NLI) is the task of determining the semantic\nentailment of a premise for a given hypothesis. The task aims to develop\nsystems that emulate natural human inferential processes where commonsense\nknowledge plays a major role. However, existing commonsense resources lack\nsufficient coverage for a variety of premise-hypothesis pairs. This study\nexplores the potential of Large Language Models as commonsense knowledge\ngenerators for NLI along two key dimensions: their reliability in generating\nsuch knowledge and the impact of that knowledge on prediction accuracy. We\nadapt and modify existing metrics to assess LLM factuality and consistency in\ngenerating in this context. While explicitly incorporating commonsense\nknowledge does not consistently improve overall results, it effectively helps\ndistinguish entailing instances and moderately improves distinguishing\ncontradictory and neutral inferences.", "AI": {"tldr": "大型语言模型（LLMs）可用于为自然语言推理（NLI）任务生成常识知识，但显式整合这些知识对整体准确率提升不显著，不过有助于区分特定推理类型。", "motivation": "现有常识资源在自然语言推理（NLI）任务中覆盖不足，限制了NLI系统模拟人类推理过程的能力。本研究旨在探索大型语言模型作为NLI常识知识生成器的潜力。", "method": "研究利用大型语言模型（LLMs）生成NLI所需的常识知识。方法包括评估LLM生成知识的可靠性（通过修改现有指标衡量其事实性和一致性），并分析这些知识对NLI预测准确性的影响。", "result": "显式整合LLM生成的常识知识并未持续改善NLI的整体结果。然而，它能有效帮助区分蕴含实例，并适度改善对矛盾和中立推理的区分能力。", "conclusion": "尽管LLM生成的常识知识在持续提升NLI整体性能方面存在局限性，但它在增强对蕴含、矛盾和中立等特定NLI关系的辨别能力方面显示出潜力。"}}
{"id": "2507.14797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14797", "abs": "https://arxiv.org/abs/2507.14797", "authors": ["Beier Zhu", "Ruoyu Wang", "Tong Zhao", "Hanwang Zhang", "Chi Zhang"], "title": "Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models", "comment": "To appear in ICCV 2025", "summary": "Diffusion models (DMs) have achieved state-of-the-art generative performance\nbut suffer from high sampling latency due to their sequential denoising nature.\nExisting solver-based acceleration methods often face image quality degradation\nunder a low-latency budget. In this paper, we propose the Ensemble Parallel\nDirection solver (dubbed as \\ours), a novel ODE solver that mitigates\ntruncation errors by incorporating multiple parallel gradient evaluations in\neach ODE step. Importantly, since the additional gradient computations are\nindependent, they can be fully parallelized, preserving low-latency sampling.\n  Our method optimizes a small set of learnable parameters in a distillation\nfashion, ensuring minimal training overhead.\n  In addition, our method can serve as a plugin to improve existing ODE\nsamplers. Extensive experiments on various image synthesis benchmarks\ndemonstrate the effectiveness of our \\ours~in achieving high-quality and\nlow-latency sampling. For example, at the same latency level of 5 NFE, EPD\nachieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26\non LSUN Bedroom, surpassing existing learning-based solvers by a significant\nmargin. Codes are available in https://github.com/BeierZhu/EPD.", "AI": {"tldr": "本文提出了一种名为EPD的新型ODE求解器，通过在每个ODE步骤中并行评估多个梯度来加速扩散模型的采样过程，同时保持高质量的图像生成。", "motivation": "扩散模型在生成性能上表现出色，但由于其顺序去噪特性导致采样延迟高。现有基于求解器的加速方法在低延迟预算下常常面临图像质量下降的问题。", "method": "本文提出了集成并行方向求解器（EPD），这是一种新颖的ODE求解器，通过在每个ODE步骤中整合多个并行梯度评估来减轻截断误差。这些额外的梯度计算是独立的，可以完全并行化以保持低延迟采样。该方法以蒸馏方式优化少量可学习参数，确保最小的训练开销，并可作为插件改进现有ODE采样器。", "result": "EPD在各种图像合成基准上实现了高质量和低延迟的采样。例如，在5个NFE（网络函数评估）的相同延迟水平下，EPD在CIFAR-10上实现了4.47的FID，FFHQ上为7.97，ImageNet上为8.17，LSUN Bedroom上为8.26，显著优于现有基于学习的求解器。", "conclusion": "EPD求解器有效地解决了扩散模型采样延迟高的问题，同时保持了高图像质量，在快速高质量图像生成方面表现出优越性。"}}
{"id": "2507.15581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15581", "abs": "https://arxiv.org/abs/2507.15581", "authors": ["Ekaterina Goliakova", "Xavier Renard", "Marie-Jeanne Lesot", "Thibault Laugel", "Christophe Marsala", "Marcin Detyniecki"], "title": "Metric assessment protocol in the context of answer fluctuation on MCQ tasks", "comment": null, "summary": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol.", "AI": {"tldr": "本文评估了用于大型语言模型（LLM）多项选择题（MCQ）评估的各种指标，发现现有指标与答案波动（即模型在提示微小变化下产生不同结果）之间存在强关联，并提出了一种新的指标“最差准确率”表现出最高的关联性。", "motivation": "多项选择题已成为评估LLM能力的标准方法，但现有评估指标尚未得到彻底评估。同时，MCQ评估面临答案波动问题，即模型在提示轻微变化下会产生不同结果。", "method": "作者提出了一种指标评估协议，通过分析评估方法与波动率以及原始性能之间的联系来评估这些方法。他们还提出了一种新的指标——最差准确率。", "result": "研究结果表明，现有指标与答案变化（波动）之间存在强关联，即使在没有额外提示变体的情况下也是如此。新提出的“最差准确率”指标在协议中表现出最高的关联性。", "conclusion": "LLM MCQ评估中，现有指标与模型答案波动密切相关。新提出的“最差准确率”指标能更好地捕捉这种关联性，提示在评估LLM时需要考虑答案波动问题。"}}
{"id": "2507.15114", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15114", "abs": "https://arxiv.org/abs/2507.15114", "authors": ["Chathuri Jayaweera", "Bonnie Dorr"], "title": "From Disagreement to Understanding: The Case for Ambiguity Detection in NLI", "comment": "8 pages, 6 figures", "summary": "This position paper argues that annotation disagreement in Natural Language\nInference (NLI) is not mere noise but often reflects meaningful interpretive\nvariation, especially when triggered by ambiguity in the premise or hypothesis.\nWhile underspecified guidelines and annotator behavior can contribute to\nvariation, content-based ambiguity offers a process-independent signal of\ndivergent human perspectives. We call for a shift toward ambiguity-aware NLI by\nsystematically identifying ambiguous input pairs and classifying ambiguity\ntypes. To support this, we present a unified framework that integrates existing\ntaxonomies and illustrate key ambiguity subtypes through concrete examples.\nThese examples reveal how ambiguity shapes annotator decisions and motivate the\nneed for targeted detection methods that better align models with human\ninterpretation. A key limitation is the lack of datasets annotated for\nambiguity and subtypes. We propose addressing this gap through new annotated\nresources and unsupervised approaches to ambiguity detection -- paving the way\nfor more robust, explainable, and human-aligned NLI systems.", "AI": {"tldr": "本文认为自然语言推理（NLI）中的标注分歧并非简单噪音，而是有意义的解释变异，尤其当由歧义引发时。文章呼吁转向歧义感知NLI，通过系统识别和分类歧义类型，并提出一个统一框架及创建新数据集来解决现有模型与人类解释不符的问题。", "motivation": "NLI中常见的标注分歧常被视为噪音，但研究者认为这尤其在前提或假设存在歧义时，反映了有意义的人类解释差异。现有NLI系统未能有效处理这种歧义，导致与人类理解脱节。", "method": "本文提出一种“歧义感知NLI”方法：1) 将NLI标注分歧视为有意义的解释变异，而非噪音；2) 呼吁系统地识别和分类歧义类型；3) 提出一个整合现有分类法的统一框架，并通过具体示例说明关键歧义子类型；4) 建议通过创建新的歧义标注数据集和利用无监督方法来解决数据缺失问题，以支持歧义检测。", "result": "研究发现标注分歧通常反映了有意义的解释变异，特别是当由前提或假设中的歧义触发时。具体示例揭示了歧义如何影响标注者的决策，并强调了开发更符合人类解释的、有针对性的歧义检测方法的必要性。", "conclusion": "结论是NLI领域应转向“歧义感知NLI”，即系统识别和分类歧义。通过开发新的标注资源和无监督检测方法来弥补数据空白，从而构建更鲁棒、可解释且与人类理解更一致的NLI系统。"}}
{"id": "2507.14798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14798", "abs": "https://arxiv.org/abs/2507.14798", "authors": ["Xinyi Wu", "Steven Landgraf", "Markus Ulrich", "Rongjun Qin"], "title": "An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks", "comment": "23 pages, 6 figures, this manuscript has been submitted to\n  Geo-spatial Information Science for consideration", "summary": "State-of-the-art 3D computer vision algorithms continue to advance in\nhandling sparse, unordered image sets. Recently developed foundational models\nfor 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction\n(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry\nGrounded Transformer (VGGT), have attracted attention due to their ability to\nhandle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical\naerial images matters, as these models may handle extremely low image overlaps,\nstereo occlusions, and textureless regions. For redundant collections, they can\naccelerate 3D reconstruction by using extremely sparsified image sets. Despite\ntests on various computer vision benchmarks, their potential on photogrammetric\naerial blocks remains unexplored. This paper conducts a comprehensive\nevaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of\nthe UseGeo dataset for pose estimation and dense 3D reconstruction. Results\nshow these methods can accurately reconstruct dense point clouds from very\nsparse image sets (fewer than 10 images, up to 518 pixels resolution), with\ncompleteness gains up to +50% over COLMAP. VGGT also demonstrates higher\ncomputational efficiency, scalability, and more reliable camera pose\nestimation. However, all exhibit limitations with high-resolution images and\nlarge sets, as pose reliability declines with more images and geometric\ncomplexity. These findings suggest transformer-based methods cannot fully\nreplace traditional SfM and MVS, but offer promise as complementary approaches,\nespecially in challenging, low-resolution, and sparse scenarios.", "AI": {"tldr": "本文评估了DUSt3R、MASt3R和VGGT等基础3D重建模型在航空影像上的性能，发现它们在稀疏、低分辨率图像集上表现出色，但在高分辨率和大型数据集上存在局限性。", "motivation": "尽管DUSt3R、MASt3R和VGGT等最新3D重建基础模型在处理稀疏图像重叠方面表现突出，并能加速重建，但它们在航空摄影测量块上的潜力尚未被充分探索。", "method": "本文对预训练的DUSt3R、MASt3R和VGGT模型在UseGeo数据集的航空影像块上进行了全面评估，包括姿态估计和密集3D重建，并与COLMAP进行了比较。", "result": "这些方法能够从非常稀疏的图像集（少于10张、分辨率高达518像素）中准确重建密集点云，完整性比COLMAP提高高达50%。VGGT还表现出更高的计算效率、可扩展性和更可靠的相机姿态估计。然而，所有方法在高分辨率图像和大型数据集上都存在局限性，姿态可靠性会随图像数量和几何复杂性增加而下降。", "conclusion": "基于Transformer的方法（DUSt3R、MASt3R、VGGT）不能完全取代传统的SfM和MVS，但作为补充方法，在具有挑战性的低分辨率和稀疏场景中具有广阔前景。"}}
{"id": "2507.15618", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15618", "abs": "https://arxiv.org/abs/2507.15618", "authors": ["Weiyu Ma", "Jiwen Jiang", "Haobo Fu", "Haifeng Zhang"], "title": "TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II", "comment": null, "summary": "We present an adapter-based approach for tactical conditioning of StarCraft\nII AI agents. Current agents, while powerful, lack the ability to adapt their\nstrategies based on high-level tactical directives. Our method freezes a\npre-trained policy network (DI-Star) and attaches lightweight adapter modules\nto each action head, conditioned on a tactical tensor that encodes strategic\npreferences. By training these adapters with KL divergence constraints, we\nensure the policy maintains core competencies while exhibiting tactical\nvariations. Experimental results show our approach successfully modulates agent\nbehavior across tactical dimensions including aggression, expansion patterns,\nand technology preferences, while maintaining competitive performance. Our\nmethod enables flexible tactical control with minimal computational overhead,\noffering practical strategy customization for complex real-time strategy games.", "AI": {"tldr": "本文提出一种基于适配器的方法，用于星际争霸II AI的战术条件化，使预训练的AI能在保持核心能力的同时，根据高层战术指令调整其策略。", "motivation": "当前的AI代理虽然强大，但缺乏根据高层战术指令调整其策略的能力。", "method": "该方法冻结一个预训练的策略网络（DI-Star），并在每个动作头附加轻量级适配器模块。这些适配器通过编码战略偏好的战术张量进行条件化，并使用KL散度约束进行训练，以确保策略在展现战术变化的同时保持核心能力。", "result": "实验结果表明，该方法成功地在攻击性、扩张模式和科技偏好等战术维度上调整了AI代理的行为，同时保持了有竞争力的性能。", "conclusion": "该方法以最小的计算开销实现了灵活的战术控制，为复杂实时战略游戏提供了实用的策略定制能力。"}}
{"id": "2507.15142", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15142", "abs": "https://arxiv.org/abs/2507.15142", "authors": ["Hellina Hailu Nigatu", "Atnafu Lambebo Tonja", "Henok Biadglign Ademtew", "Hizkel Mitiku Alemayehu", "Negasi Haile Abadi", "Tadesse Destaw Belay", "Seid Muhie Yimam"], "title": "A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script", "comment": "Paper under review", "summary": "Homophone normalization, where characters that have the same sound in a\nwriting script are mapped to one character, is a pre-processing step applied in\nAmharic Natural Language Processing (NLP) literature. While this may improve\nperformance reported by automatic metrics, it also results in models that are\nnot able to understand different forms of writing in a single language.\nFurther, there might be impacts in transfer learning, where models trained on\nnormalized data do not generalize well to other languages. In this paper, we\nexperiment with monolingual training and cross-lingual transfer to understand\nthe impacts of normalization on languages that use the Ge'ez script. We then\npropose a post-inference intervention in which normalization is applied to\nmodel predictions instead of training data. With our simple scheme of\npost-inference normalization, we show that we can achieve an increase in BLEU\nscore of up to 1.03 while preserving language features in training. Our work\ncontributes to the broader discussion on technology-facilitated language change\nand calls for more language-aware interventions.", "AI": {"tldr": "同音字归一化在阿姆哈拉语NLP中常用作预处理步骤，但可能损害模型对不同书写形式的理解和跨语言迁移能力。本文提出了一种推理后归一化方法，在提高BLEU分数的同时保留了语言特征。", "motivation": "同音字归一化作为预处理步骤，尽管可能提高自动化度量报告的性能，但会导致模型无法理解单一语言中的不同书写形式，并可能对迁移学习产生负面影响，即在归一化数据上训练的模型无法很好地泛化到其他语言。", "method": "研究者通过单语训练和跨语言迁移实验来理解归一化对使用吉兹字母的语言的影响。然后，提出了一种推理后干预方案，即将归一化应用于模型预测而非训练数据。", "result": "通过简单的推理后归一化方案，可以在训练中保留语言特征的同时，将BLEU分数提高多达1.03。", "conclusion": "推理后归一化是一种有效的策略，既能提升性能又不会牺牲语言特征。这项工作为技术促进的语言变化以及呼吁更多语言感知干预的讨论做出了贡献。"}}
{"id": "2507.14801", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14801", "abs": "https://arxiv.org/abs/2507.14801", "authors": ["Xiangyu Chen", "Kaiwen Zhu", "Yuandong Pu", "Shuo Cao", "Xiaohui Li", "Wenlong Zhang", "Yihao Liu", "Yu Qiao", "Jiantao Zhou", "Chao Dong"], "title": "Exploring Scalable Unified Modeling for General Low-Level Vision", "comment": null, "summary": "Low-level vision involves a wide spectrum of tasks, including image\nrestoration, enhancement, stylization, and feature extraction, which differ\nsignificantly in both task formulation and output domains. To address the\nchallenge of unified modeling across such diverse tasks, we propose a Visual\ntask Prompt-based Image Processing (VPIP) framework that leverages input-target\nimage pairs as visual prompts to guide the model in performing a variety of\nlow-level vision tasks. The framework comprises an end-to-end image processing\nbackbone, a prompt encoder, and a prompt interaction module, enabling flexible\nintegration with various architectures and effective utilization of\ntask-specific visual representations. Based on this design, we develop a\nunified low-level vision model, GenLV, and evaluate its performance across\nmultiple representative tasks. To explore the scalability of this approach, we\nextend the framework along two dimensions: model capacity and task diversity.\nWe construct a large-scale benchmark consisting of over 100 low-level vision\ntasks and train multiple versions of the model with varying scales.\nExperimental results show that the proposed method achieves considerable\nperformance across a wide range of tasks. Notably, increasing the number of\ntraining tasks enhances generalization, particularly for tasks with limited\ndata, indicating the model's ability to learn transferable representations\nthrough joint training. Further evaluations in zero-shot generalization,\nfew-shot transfer, and task-specific fine-tuning scenarios demonstrate the\nmodel's strong adaptability, confirming the effectiveness, scalability, and\npotential of the proposed framework as a unified foundation for general\nlow-level vision modeling.", "AI": {"tldr": "本文提出VPIP框架，利用输入-目标图像对作为视觉提示，实现对多种低级视觉任务的统一建模，并验证了其有效性、可扩展性和泛化能力。", "motivation": "低级视觉任务种类繁多，如图像恢复、增强、风格化和特征提取等，它们在任务表述和输出域上差异显著，因此需要一个统一的建模方法来应对这一挑战。", "method": "提出视觉任务提示图像处理（VPIP）框架，包含端到端图像处理主干、提示编码器和提示交互模块。该框架利用输入-目标图像对作为视觉提示来指导模型。基于此设计，开发了统一的低级视觉模型GenLV，并在包含100多个低级视觉任务的大规模基准上进行训练和评估，探索了模型容量和任务多样性对性能的影响。", "result": "所提出的方法在广泛的任务上取得了显著性能。值得注意的是，增加训练任务数量能增强泛化能力，特别是对数据有限的任务。在零样本泛化、少样本迁移和任务特定微调场景下的进一步评估，也展示了模型强大的适应性。", "conclusion": "VPIP框架作为通用低级视觉建模的统一基础，具有高效性、可扩展性和巨大潜力，能够通过联合训练学习可迁移的表示。"}}
{"id": "2507.15676", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.15676", "abs": "https://arxiv.org/abs/2507.15676", "authors": ["Reza Vatankhah Barenji", "Sina Khoshgoftar"], "title": "Agentic AI for autonomous anomaly management in complex systems", "comment": null, "summary": "This paper explores the potential of agentic AI in autonomously detecting and\nresponding to anomalies within complex systems, emphasizing its ability to\ntransform traditional, human-dependent anomaly management methods.", "AI": {"tldr": "本文探讨了代理式AI在复杂系统中自主检测和响应异常的潜力。", "motivation": "传统的异常管理方法高度依赖人工，效率和响应速度受限。", "method": "探索并强调了代理式AI在自主检测和响应异常方面的能力。", "result": "展示了代理式AI在自主异常管理方面的巨大潜力。", "conclusion": "代理式AI有望彻底改变当前以人工为主的异常管理模式。"}}
{"id": "2507.15152", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15152", "abs": "https://arxiv.org/abs/2507.15152", "authors": ["Lingbo Li", "Anuradha Mathrani", "Teo Susnjak"], "title": "What Level of Automation is \"Good Enough\"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction", "comment": null, "summary": "Automating data extraction from full-text randomised controlled trials (RCTs)\nfor meta-analysis remains a significant challenge. This study evaluates the\npractical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)\nacross tasks involving statistical results, risk-of-bias assessments, and\nstudy-level characteristics in three medical domains: hypertension, diabetes,\nand orthopaedics. We tested four distinct prompting strategies (basic\nprompting, self-reflective prompting, model ensemble, and customised prompts)\nto determine how to improve extraction quality. All models demonstrate high\nprecision but consistently suffer from poor recall by omitting key information.\nWe found that customised prompts were the most effective, boosting recall by up\nto 15\\%. Based on this analysis, we propose a three-tiered set of guidelines\nfor using LLMs in data extraction, matching data types to appropriate levels of\nautomation based on task complexity and risk. Our study offers practical advice\nfor automating data extraction in real-world meta-analyses, balancing LLM\nefficiency with expert oversight through targeted, task-specific automation.", "AI": {"tldr": "本研究评估了三种大型语言模型（LLMs）在从随机对照试验（RCTs）中提取元分析数据的性能，并测试了四种提示策略，发现定制化提示最有效，并提出了LLM数据提取的三层指南。", "motivation": "从全文随机对照试验中自动化提取数据用于元分析仍然是一个重大挑战。", "method": "研究评估了Gemini-2.0-flash、Grok-3和GPT-4o-mini三种LLM，针对统计结果、偏倚风险评估和研究层面特征等任务，涵盖高血压、糖尿病和骨科三个医学领域。测试了四种不同的提示策略：基本提示、自我反思提示、模型集成和定制化提示。", "result": "所有模型都表现出高精度，但由于遗漏关键信息而持续存在召回率低的问题。定制化提示被发现是最有效的策略，能将召回率提高高达15%。", "conclusion": "基于分析，研究提出了一套三层指南，用于在数据提取中使用LLM，根据任务复杂性和风险将数据类型与适当的自动化水平相匹配。为在实际元分析中自动化数据提取提供了实用建议，通过有针对性的、特定任务的自动化平衡了LLM效率与专家监督。"}}
{"id": "2507.14807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14807", "abs": "https://arxiv.org/abs/2507.14807", "authors": ["Juan Hu", "Shaojing Fan", "Terence Sim"], "title": "Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection", "comment": null, "summary": "Multi-face deepfake videos are becoming increasingly prevalent, often\nappearing in natural social settings that challenge existing detection methods.\nMost current approaches excel at single-face detection but struggle in\nmulti-face scenarios, due to a lack of awareness of crucial contextual cues. In\nthis work, we develop a novel approach that leverages human cognition to\nanalyze and defend against multi-face deepfake videos. Through a series of\nhuman studies, we systematically examine how people detect deepfake faces in\nsocial settings. Our quantitative analysis reveals four key cues humans rely\non: scene-motion coherence, inter-face appearance compatibility, interpersonal\ngaze alignment, and face-body consistency. Guided by these insights, we\nintroduce \\textsf{HICOM}, a novel framework designed to detect every fake face\nin multi-face scenarios. Extensive experiments on benchmark datasets show that\n\\textsf{HICOM} improves average accuracy by 3.3\\% in in-dataset detection and\n2.8\\% under real-world perturbations. Moreover, it outperforms existing methods\nby 5.8\\% on unseen datasets, demonstrating the generalization of human-inspired\ncues. \\textsf{HICOM} further enhances interpretability by incorporating an LLM\nto provide human-readable explanations, making detection results more\ntransparent and convincing. Our work sheds light on involving human factors to\nenhance defense against deepfakes.", "AI": {"tldr": "针对多面部深度伪造视频检测的挑战，本文提出受人类认知启发的HICOM框架，利用场景、面部间、凝视、面部-身体一致性等上下文线索，显著提升了检测准确性和泛化性，并提供可解释性。", "motivation": "多面部深度伪造视频日益普遍，且常出现在自然社交场景中，对现有检测方法构成挑战。当前大多数方法擅长单面部检测，但在多面部场景中因缺乏对关键上下文线索的感知而表现不佳。", "method": "通过一系列人类研究，系统性地考察了人们在社交场景中检测深度伪造面部的方式。定量分析揭示了人类依赖的四个关键线索：场景-运动连贯性、面部间外观兼容性、人际凝视对齐和面部-身体一致性。受这些洞察启发，研究引入了HICOM框架，旨在检测多面部场景中的每个伪造面部，并结合大型语言模型（LLM）提供人类可读的解释。", "result": "HICOM在数据集内检测中平均准确率提高了3.3%，在真实世界扰动下提高了2.8%。在未见过的数据集上，HICOM比现有方法提高了5.8%，证明了人类启发线索的泛化能力。此外，HICOM通过整合LLM增强了可解释性。", "conclusion": "本研究揭示了将人类因素融入深度伪造防御的重要性，为增强深度伪造检测能力提供了新思路。"}}
{"id": "2507.15743", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15743", "abs": "https://arxiv.org/abs/2507.15743", "authors": ["Elahe Vedadi", "David Barrett", "Natalie Harris", "Ellery Wulczyn", "Shashir Reddy", "Roma Ruparel", "Mike Schaekermann", "Tim Strother", "Ryutaro Tanno", "Yash Sharma", "Jihyeon Lee", "Cían Hughes", "Dylan Slack", "Anil Palepu", "Jan Freyberg", "Khaled Saab", "Valentin Liévin", "Wei-Hung Weng", "Tao Tu", "Yun Liu", "Nenad Tomasev", "Kavita Kulkarni", "S. Sara Mahdavi", "Kelvin Guu", "Joëlle Barral", "Dale R. Webster", "James Manyika", "Avinatan Hassidim", "Katherine Chou", "Yossi Matias", "Pushmeet Kohli", "Adam Rodman", "Vivek Natarajan", "Alan Karthikesalingam", "David Stutz"], "title": "Towards physician-centered oversight of conversational diagnostic AI", "comment": null, "summary": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care.", "AI": {"tldr": "本文提出并评估了g-AMIE，一个用于医学信息采集的AI系统，通过异步方式由初级保健医生（PCP）进行监督。在虚拟临床考试中，g-AMIE在信息采集和方案建议方面表现优于护士/医师助理和独立PCP，并提高了决策质量和效率，为诊断AI系统在专家监督下运行提供了可行范式。", "motivation": "对话式AI系统在诊断对话中展现潜力，但现实世界中患者安全保障要求诊断和治疗计划由持证专业人员提供，且通常由医生监督其他团队成员。因此，需要一个框架使AI系统能在人类专家监督下安全运行，以符合监管要求并增强实际护理。", "method": "本文提出了一个名为“guardrailed-AMIE (g-AMIE)”的多智能体系统。g-AMIE在限定范围内进行病史采集，避免提供个性化医疗建议，然后通过临床医生驾驶舱界面将评估结果传达给监督的初级保健医生（PCP）。PCP提供异步监督并承担临床决策责任。该框架通过随机、盲法虚拟客观结构化临床考试（OSCE）进行评估，将g-AMIE与护士/医师助理（NPs/PAs）以及一组PCP在相同限制下进行比较，共涉及60个场景。", "result": "在60个场景中，g-AMIE在高质量信息采集、病例总结以及为监督PCP提出诊断和管理计划方面均优于NPs/PAs和PCPs。这导致了更高质量的综合决策。此外，PCP对g-AMIE的监督比先前的独立PCP咨询更具时间效率。", "conclusion": "尽管本研究未完全复制现有临床实践且可能低估了临床医生的能力，但结果表明，异步监督是诊断AI系统在专家人类监督下运行以增强现实世界护理的可行且有前景的范式。"}}
{"id": "2507.15198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15198", "abs": "https://arxiv.org/abs/2507.15198", "authors": ["Xiandong Meng", "Yan Wu", "Yexin Tian", "Xin Hu", "Tianze Kang", "Junliang Du"], "title": "Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment", "comment": null, "summary": "This paper addresses the challenges of high computational cost and slow\ninference in deploying large language models. It proposes a distillation\nstrategy guided by multiple teacher models. The method constructs several\nteacher models and integrates their output probability distributions and\nintermediate semantic features. This guides the student model to learn from\nmultiple sources of knowledge. As a result, the student model gains stronger\nlanguage understanding and generation ability while maintaining a small\nparameter size. To achieve this, the paper introduces a weighted output fusion\nmechanism, a feature alignment loss function, and an entropy-driven dynamic\nteacher weighting strategy. These components improve the quality and stability\nof knowledge transfer during distillation. Under multi-teacher guidance, the\nstudent model captures semantic information more effectively and demonstrates\nstrong performance across multiple evaluation metrics. In particular, the\nmethod shows high consistency in expression, generalization ability, and task\nadaptability in tasks such as language modeling, text generation, and\nmulti-task learning. The experiments compare the proposed method with several\nwidely adopted distillation approaches. The results further confirm its overall\nadvantages in perplexity, distillation loss, and generation quality. This study\nprovides a feasible technical path for the efficient compression of large-scale\nlanguage models. It also demonstrates the effectiveness of multi-teacher\ncollaborative mechanisms in complex language modeling tasks.", "AI": {"tldr": "本文提出了一种多教师引导的知识蒸馏策略，通过融合多个教师模型的输出和中间特征，有效压缩大型语言模型，同时提升学生模型的语言理解和生成能力。", "motivation": "大型语言模型部署面临计算成本高和推理速度慢的挑战。", "method": "该研究构建了多个教师模型，并整合其输出概率分布和中间语义特征来指导学生模型学习。具体方法包括：加权输出融合机制、特征对齐损失函数和熵驱动的动态教师加权策略，以提高知识转移的质量和稳定性。", "result": "在多教师指导下，学生模型以较小的参数规模获得了更强的语言理解和生成能力。在语言建模、文本生成和多任务学习等任务中，该方法在困惑度、蒸馏损失和生成质量方面表现出优势，并展现出高表达一致性、泛化能力和任务适应性。", "conclusion": "该研究为大型语言模型的有效压缩提供了一条可行的技术路径，并证明了多教师协作机制在复杂语言建模任务中的有效性。"}}
{"id": "2507.14811", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14811", "abs": "https://arxiv.org/abs/2507.14811", "authors": ["Jiaji Zhang", "Ruichao Sun", "Hailiang Zhao", "Jiaju Wu", "Peng Chen", "Hao Li", "Xinkui Zhao", "Kingsum Chow", "Gang Xiong", "Lin Ye", "Shuiguang Deng"], "title": "SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models", "comment": null, "summary": "Diffusion models have demonstrated exceptional generative capabilities but\nare computationally intensive, posing significant challenges for deployment in\nresource-constrained or latency-sensitive environments. Quantization offers an\neffective means to reduce model size and computational cost, with post-training\nquantization (PTQ) being particularly appealing due to its compatibility with\npre-trained models without requiring retraining or training data. However,\nexisting PTQ methods for diffusion models often rely on architecture-specific\nheuristics that limit their generalizability and hinder integration with\nindustrial deployment pipelines. To address these limitations, we propose\nSegQuant, a unified quantization framework that adaptively combines\ncomplementary techniques to enhance cross-model versatility. SegQuant consists\nof a segment-aware, graph-based quantization strategy (SegLinear) that captures\nstructural semantics and spatial heterogeneity, along with a dual-scale\nquantization scheme (DualScale) that preserves polarity-asymmetric activations,\nwhich is crucial for maintaining visual fidelity in generated outputs. SegQuant\nis broadly applicable beyond Transformer-based diffusion models, achieving\nstrong performance while ensuring seamless compatibility with mainstream\ndeployment tools.", "AI": {"tldr": "扩散模型计算量大，现有后训练量化（PTQ）方法缺乏通用性。SegQuant提出一种统一的量化框架，结合段感知图量化和双尺度量化，以提高跨模型通用性和生成质量，并兼容主流部署工具。", "motivation": "扩散模型生成能力强但计算密集，难以在资源受限或延迟敏感环境中部署。现有针对扩散模型的后训练量化（PTQ）方法依赖特定架构的启发式规则，限制了其通用性并阻碍了与工业部署流程的整合。", "method": "本文提出SegQuant框架，它自适应地结合了互补技术以增强跨模型通用性。主要包括：1) SegLinear，一种段感知、基于图的量化策略，用于捕获结构语义和空间异质性；2) DualScale，一种双尺度量化方案，用于保留生成输出中至关重要的极性非对称激活。", "result": "SegQuant在Transformer以外的扩散模型上也能广泛适用，取得了强大的性能，并确保与主流部署工具的无缝兼容性。", "conclusion": "SegQuant是一个统一、自适应的量化框架，通过结合SegLinear和DualScale，有效解决了现有扩散模型PTQ方法通用性差的问题，显著提升了量化性能和部署兼容性，尤其在保持生成视觉保真度方面表现出色。"}}
{"id": "2507.15758", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15758", "abs": "https://arxiv.org/abs/2507.15758", "authors": ["Xingyu Wu", "Yuchen Yan", "Shangke Lyu", "Linjuan Wu", "Yiwen Qiu", "Yongliang Shen", "Weiming Lu", "Jian Shao", "Jun Xiao", "Yueting Zhuang"], "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization", "comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo", "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.", "AI": {"tldr": "LAPO是一种新颖的强化学习框架，使大型推理模型能够自适应地控制推理长度，从而在减少token使用量的同时提高准确性。", "motivation": "大型推理模型（如使用思维链）虽然表现出色，但在简单问题上也会生成过多的token，导致计算资源浪费。", "method": "LAPO采用两阶段强化学习过程：第一阶段，模型通过发现成功解决方案的长度统计分布来学习自然的推理模式；第二阶段，将这些模式作为元认知指导嵌入到模型的推理上下文中，以确保推理时的灵活性。", "result": "在数学推理基准测试中，LAPO将token使用量减少了高达40.9%，同时将准确率提高了2.3%。分析表明，经过LAPO训练的模型能够根据问题复杂性分配计算资源，实现高效推理而不牺牲质量。", "conclusion": "LAPO将推理长度控制从外部约束转变为模型固有的能力，使得模型能够内化对适当推理深度的理解，从而实现高效且高质量的推理。"}}
{"id": "2507.15236", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15236", "abs": "https://arxiv.org/abs/2507.15236", "authors": ["Shayan Vassef", "Amirhossein Dabiriaghdam", "Mohammadreza Bakhtiari", "Yadollah Yaghoobzadeh"], "title": "SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest", "comment": null, "summary": "This work investigates the impact of multi-task, multi-lingual, and\nmulti-source learning approaches on the robustness and performance of\npretrained language models. To enhance this analysis, we introduce Subsets of\nInterest (SOI), a novel categorization framework that identifies six distinct\nlearning behavior patterns during training, including forgettable examples,\nunlearned examples, and always correct examples. Through SOI transition\nheatmaps and dataset cartography visualization, we analyze how examples shift\nbetween these categories when transitioning from single-setting to\nmulti-setting configurations. We perform comprehensive experiments across three\nparallel comparisons: multi-task vs. single-task learning using English tasks\n(entailment, paraphrase, sentiment), multi-source vs. single-source learning\nusing sentiment analysis datasets, and multi-lingual vs. single-lingual\nlearning using intent classification in French, English, and Persian. Our\nresults demonstrate that multi-source learning consistently improves\nout-of-distribution performance by up to 7%, while multi-task learning shows\nmixed results with notable gains in similar task combinations. We further\nintroduce a two-stage fine-tuning approach where the second stage leverages\nSOI-based subset selection to achieve additional performance improvements.\nThese findings provide new insights into training dynamics and offer practical\napproaches for optimizing multi-setting language model performance.", "AI": {"tldr": "本研究通过引入SOI框架和两阶段微调方法，探究了多任务、多语言和多源学习对预训练语言模型鲁棒性和性能的影响。", "motivation": "研究旨在深入理解多任务、多语言和多源学习方法如何影响预训练语言模型的鲁棒性和性能。", "method": "引入了“兴趣子集”（SOI）这一新颖的分类框架，识别了六种训练期间的学习行为模式；通过SOI转换热图和数据集制图可视化分析了示例在单设置和多设置配置间的类别转移；在三种并行比较（多任务vs单任务、多源vs单源、多语言vs单语言）中进行了实验；并提出了一种两阶段微调方法，第二阶段利用SOI进行子集选择。", "result": "多源学习持续提升了分布外性能高达7%；多任务学习结果复杂，但在相似任务组合中表现出显著增益；基于SOI的两阶段微调实现了额外的性能提升。", "conclusion": "研究为训练动态提供了新见解，并为优化多设置语言模型性能提供了实用方法。"}}
{"id": "2507.14823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14823", "abs": "https://arxiv.org/abs/2507.14823", "authors": ["Dong Shu", "Haoyang Yuan", "Yuchen Wang", "Yanguang Liu", "Huopu Zhang", "Haiyan Zhao", "Mengnan Du"], "title": "FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models", "comment": "20 Pages, 18 Figures", "summary": "Large vision-language models (LVLMs) have made significant progress in chart\nunderstanding. However, financial charts, characterized by complex temporal\nstructures and domain-specific terminology, remain notably underexplored. We\nintroduce FinChart-Bench, the first benchmark specifically focused on\nreal-world financial charts. FinChart-Bench comprises 1,200 financial chart\nimages collected from 2015 to 2024, each annotated with True/False (TF),\nMultiple Choice (MC), and Question Answering (QA) questions, totaling 7,016\nquestions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs\non FinChart-Bench. Our evaluation reveals critical insights: (1) the\nperformance gap between open-source and closed-source models is narrowing, (2)\nperformance degradation occurs in upgraded models within families, (3) many\nmodels struggle with instruction following, (4) both advanced models show\nsignificant limitations in spatial reasoning abilities, and (5) current LVLMs\nare not reliable enough to serve as automated evaluators. These findings\nhighlight important limitations in current LVLM capabilities for financial\nchart understanding. The FinChart-Bench dataset is available at\nhttps://huggingface.co/datasets/Tizzzzy/FinChart-Bench.", "AI": {"tldr": "本文介绍了FinChart-Bench，首个专注于真实世界金融图表的基准数据集，并使用它评估了25个最先进的大型视觉-语言模型（LVLMs），揭示了当前LVLMs在金融图表理解方面的局限性。", "motivation": "尽管大型视觉-语言模型在图表理解方面取得了显著进展，但金融图表因其复杂的时序结构和领域特定术语而仍未得到充分探索，这促使研究人员开发一个专门的基准来解决这一空白。", "method": "研究人员构建了FinChart-Bench数据集，包含1200张2015年至2024年间收集的金融图表图像，并为每张图表标注了真/假（TF）、多项选择（MC）和问答（QA）类型的问题，共计7016个问题。随后，他们对25个最先进的LVLMs在FinChart-Bench上进行了全面的性能评估。", "result": "评估结果揭示了五个关键发现：1) 开源模型和闭源模型之间的性能差距正在缩小；2) 同系列升级模型的性能出现下降；3) 许多模型在遵循指令方面存在困难；4) 即使是高级模型在空间推理能力上也存在显著局限性；5) 当前的LVLMs不足以作为可靠的自动化评估器。", "conclusion": "这些发现突出了当前大型视觉-语言模型在理解金融图表能力方面的重要局限性，表明现有模型尚未足够可靠地应用于金融图表分析任务。"}}
{"id": "2507.15761", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15761", "abs": "https://arxiv.org/abs/2507.15761", "authors": ["Jingyi Zheng", "Zifan Peng", "Yule Liu", "Junfeng Wang", "Yifan Liao", "Wenhan Dong", "Xinlei He"], "title": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts", "comment": null, "summary": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development.", "AI": {"tldr": "GasAgent是一个多智能体系统，用于智能合约Gas优化，它结合了现有模式的兼容性与新模式的自动化发现和验证，实现了端到端的优化。", "motivation": "智能合约因非最优编码实践常包含Gas浪费模式。现有优化方案多依赖手动发现，效率低、维护成本高且难以扩展。近期LLM探索新模式，但存在兼容性差、冗余模式多、需手动验证/重写等问题。", "method": "GasAgent是一个多智能体系统，包含Seeker、Innovator、Executor和Manager四个专业智能体。它们以闭环协作方式识别、验证和应用Gas节省改进，同时兼容现有模式并自动化发现/验证新模式。", "result": "在100个真实合约上，GasAgent成功优化82个，平均部署Gas节省9.97%。它与现有工具兼容，且各模块有效性经消融研究验证。在500个LLM生成合约上，GasAgent优化了79.8%，部署Gas节省4.79%至13.93%。", "conclusion": "GasAgent是首个用于智能合约Gas优化的多智能体系统，能有效优化真实世界和LLM生成的合约，并可作为LLM辅助智能合约开发的优化层。"}}
{"id": "2507.15275", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15275", "abs": "https://arxiv.org/abs/2507.15275", "authors": ["Yuanhe Tian", "Junjie Liu", "Zhizhou Kou", "Yuxiang Li", "Yan Song"], "title": "ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling", "comment": null, "summary": "Building high-quality data resources is crucial for advancing artificial\nintelligence research and applications in specific domains, particularly in the\nChinese medical domain. Existing Chinese medical datasets are limited in size\nand narrow in domain coverage, falling short of the diverse corpora required\nfor effective pre-training. Moreover, most datasets are designed solely for LLM\nfine-tuning and do not support pre-training and reinforcement learning from\nhuman feedback (RLHF). In this paper, we propose a Chinese medical dataset\nnamed ChiMed 2.0, which extends our previous work ChiMed, and covers data\ncollected from Chinese medical online platforms and generated by LLMs. ChiMed\n2.0 contains 204.4M Chinese characters covering both traditional Chinese\nmedicine classics and modern general medical data, where there are 164.8K\ndocuments for pre-training, 351.6K question-answering pairs for supervised\nfine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the\neffectiveness of our approach for training a Chinese medical LLM, we conduct\nfurther pre-training, SFT, and RLHF experiments on representative general\ndomain LLMs and evaluate their performance on medical benchmark datasets. The\nresults show performance gains across different model scales, validating the\ndataset's effectiveness and applicability.", "AI": {"tldr": "该论文提出了一个名为ChiMed 2.0的中文医疗数据集，旨在支持大型语言模型（LLM）的预训练、监督微调（SFT）和人类反馈强化学习（RLHF），并验证了其在提升医疗LLM性能方面的有效性。", "motivation": "现有的中文医疗数据集存在规模有限、领域覆盖狭窄、缺乏支持LLM预训练和RLHF的多元语料等问题，无法满足构建高质量AI模型的需求。", "method": "本文提出了ChiMed 2.0数据集，它是ChiMed的扩展，包含从中文医疗在线平台收集的数据和LLM生成的数据。该数据集包含2.044亿汉字，包括16.48万份预训练文档、35.16万对监督微调问答对以及4.17万个RLHF偏好数据元组。为验证数据集有效性，研究者在通用领域LLM上进行了进一步的预训练、SFT和RLHF实验，并在医疗基准数据集上评估了模型性能。", "result": "实验结果表明，使用ChiMed 2.0数据集训练的模型在不同模型规模下均实现了性能提升。", "conclusion": "ChiMed 2.0数据集在训练中文医疗LLM方面具有有效性和适用性。"}}
{"id": "2507.14826", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14826", "abs": "https://arxiv.org/abs/2507.14826", "authors": ["Fu-Jen Tsai", "Yan-Tsung Peng", "Yen-Yu Lin", "Chia-Wen Lin"], "title": "PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing", "comment": "ICCV 2025", "summary": "Image dehazing aims to remove unwanted hazy artifacts in images. Although\nprevious research has collected paired real-world hazy and haze-free images to\nimprove dehazing models' performance in real-world scenarios, these models\noften experience significant performance drops when handling unseen real-world\nhazy images due to limited training data. This issue motivates us to develop a\nflexible domain adaptation method to enhance dehazing performance during\ntesting. Observing that predicting haze patterns is generally easier than\nrecovering clean content, we propose the Physics-guided Haze Transfer Network\n(PHATNet) which transfers haze patterns from unseen target domains to\nsource-domain haze-free images, creating domain-specific fine-tuning sets to\nupdate dehazing models for effective domain adaptation. Additionally, we\nintroduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to\nenhance PHATNet's disentanglement ability. Experimental results demonstrate\nthat PHATNet significantly boosts state-of-the-art dehazing models on benchmark\nreal-world image dehazing datasets.", "AI": {"tldr": "针对现有图像去雾模型在未见真实世界图像上性能显著下降的问题，本文提出PHATNet，一种灵活的域适应方法。它通过将雾霾模式从目标域转移到源域无雾图像上，创建域特定的微调集，从而提升去雾模型的性能。", "motivation": "尽管之前的研究收集了成对的真实世界有雾和无雾图像来改进去雾模型，但由于训练数据有限，这些模型在处理未见的真实世界有雾图像时，性能常出现显著下降。", "method": "本文提出物理引导雾霾转移网络（PHATNet）。它利用预测雾霾模式比恢复清晰内容更容易的特点，将未见目标域的雾霾模式转移到源域的无雾图像上，从而创建域特定的微调集，用于更新去雾模型以实现有效的域适应。此外，引入雾霾转移一致性损失（Haze-Transfer-Consistency loss）和内容泄露损失（Content-Leakage Loss）以增强PHATNet的解耦能力。", "result": "实验结果表明，PHATNet显著提升了最先进的去雾模型在基准真实世界图像去雾数据集上的性能。", "conclusion": "PHATNet通过创新的雾霾模式转移方法，有效解决了去雾模型在未见真实世界数据上的性能下降问题，显著提升了去雾效果，证明了其在域适应方面的有效性。"}}
{"id": "2507.15770", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15770", "abs": "https://arxiv.org/abs/2507.15770", "authors": ["Yifan Shen", "Zihan Zhao", "Xiao Xue", "Yuwei Guo", "Qun Ma", "Deyu Zhou", "Ming Zhang"], "title": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining", "comment": null, "summary": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085.", "AI": {"tldr": "本文提出EAMI框架，利用大语言模型（LLM）通过双视角思维链推理提取多智能体意图，结合k-means聚类和意图时间演化图，实现复杂服务生态系统中动态、可解释的异常涌现分析。", "motivation": "服务计算、云计算和物联网的兴起导致服务生态系统日益复杂，智能体间错综复杂的交互使得异常涌现分析极具挑战，传统因果方法侧重个体轨迹，难以应对。现有基于LLM的智能体建模（ABM）方法局限于微观和静态分析。", "method": "EAMI框架：首先，采用双视角思维轨迹机制，由检查智能体（有限理性）和分析智能体（完美理性）提取智能体意图。接着，使用k-means聚类识别群体意图中的相变点。最后，利用意图时间涌现图进行动态分析。", "result": "EAMI在复杂的O2O服务系统和斯坦福AI Town实验中得到验证。消融研究证实了其有效性、泛化性和效率。", "conclusion": "EAMI框架为服务生态系统中的异常涌现和因果分析提供了一种新颖的范式。"}}
{"id": "2507.15281", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15281", "abs": "https://arxiv.org/abs/2507.15281", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "title": "A Novel Self-Evolution Framework for Large Language Models", "comment": null, "summary": "The capabilities of Large Language Models (LLMs) are limited to some extent\nby pre-training, so some researchers optimize LLMs through post-training.\nExisting post-training strategies, such as memory-based retrieval or preference\noptimization, improve user alignment yet fail to enhance the model's domain\ncognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution\n(DPSE) framework that jointly optimizes user preference adaptation and\ndomain-specific competence. DPSE introduces a Censor module to extract\nmulti-dimensional interaction signals and estimate satisfaction scores, which\nguide structured data expansion via topic-aware and preference-driven\nstrategies. These expanded datasets support a two-stage fine-tuning pipeline:\nsupervised domain grounding followed by frequency-aware preference\noptimization. Experiments across general NLP benchmarks and long-term dialogue\ntasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,\nPreference Optimization, and Memory-Augmented baselines. Ablation studies\nvalidate the contribution of each module. In this way, our framework provides\nan autonomous path toward continual self-evolution of LLMs.", "AI": {"tldr": "本文提出了一种名为DPSE的双阶段自进化框架，通过结合用户偏好适应和领域特定能力优化，提升大型语言模型的后训练效果。", "motivation": "现有的大型语言模型（LLM）后训练策略（如基于记忆的检索或偏好优化）虽然能提高用户对齐度，但未能增强模型的领域认知能力。", "method": "DPSE框架引入一个“审查模块”来提取多维交互信号并评估满意度分数，以此指导结构化数据扩展（通过主题感知和偏好驱动策略）。这些扩展的数据集支持一个两阶段微调流程：首先是监督式领域基础训练，然后是频率感知的偏好优化。", "result": "在通用NLP基准测试和长期对话任务中，DPSE的表现持续优于监督微调、偏好优化和记忆增强等基线方法。消融研究也验证了每个模块的贡献。", "conclusion": "该框架为LLM的持续自进化提供了一条自主路径，能够同时优化用户偏好适应和领域特定能力。"}}
{"id": "2507.14833", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14833", "abs": "https://arxiv.org/abs/2507.14833", "authors": ["Haoxuan Zhang", "Wenju Cui", "Yuzhu Cao", "Tao Tan", "Jie Liu", "Yunsong Peng", "Jian Zheng"], "title": "Paired Image Generation with Diffusion-Guided Diffusion Models", "comment": null, "summary": "The segmentation of mass lesions in digital breast tomosynthesis (DBT) images\nis very significant for the early screening of breast cancer. However, the\nhigh-density breast tissue often leads to high concealment of the mass lesions,\nwhich makes manual annotation difficult and time-consuming. As a result, there\nis a lack of annotated data for model training. Diffusion models are commonly\nused for data augmentation, but the existing methods face two challenges.\nFirst, due to the high concealment of lesions, it is difficult for the model to\nlearn the features of the lesion area. This leads to the low generation quality\nof the lesion areas, thus limiting the quality of the generated images. Second,\nexisting methods can only generate images and cannot generate corresponding\nannotations, which restricts the usability of the generated images in\nsupervised training. In this work, we propose a paired image generation method.\nThe method does not require external conditions and can achieve the generation\nof paired images by training an extra diffusion guider for the conditional\ndiffusion model. During the experimental phase, we generated paired DBT slices\nand mass lesion masks. Then, we incorporated them into the supervised training\nprocess of the mass lesion segmentation task. The experimental results show\nthat our method can improve the generation quality without external conditions.\nMoreover, it contributes to alleviating the shortage of annotated data, thus\nenhancing the performance of downstream tasks.", "AI": {"tldr": "该研究提出了一种配对图像生成方法，利用额外的扩散引导器改进条件扩散模型，从而生成高质量的数字乳腺断层扫描（DBT）图像及其对应的肿块病灶掩码，以解决乳腺癌肿块分割任务中注释数据不足的问题并提升下游任务性能。", "motivation": "乳腺癌早期筛查中DBT图像的肿块病灶分割非常重要，但高密度乳腺组织导致病灶隐蔽性高，手动标注困难且耗时，造成标注数据缺乏。现有扩散模型面临两个挑战：一是病灶区域隐蔽性导致模型难以学习病灶特征，生成质量低；二是现有方法只能生成图像，无法生成对应的标注，限制了生成图像在监督训练中的可用性。", "method": "提出了一种配对图像生成方法。该方法无需外部条件，通过为条件扩散模型训练一个额外的扩散引导器（diffusion guider），实现配对图像（DBT切片和肿块病灶掩码）的生成。生成的配对数据被整合到肿块病灶分割任务的监督训练过程中。", "result": "实验结果表明，该方法在没有外部条件的情况下提升了生成质量。此外，它有助于缓解标注数据短缺，从而增强了下游任务（肿块分割）的性能。", "conclusion": "该研究提出的配对图像生成方法能够有效生成高质量的DBT图像及其对应的肿块病灶掩码，成功解决了乳腺癌肿块分割任务中标注数据不足的问题，并显著提升了下游分割任务的性能，对乳腺癌的早期筛查具有重要意义。"}}
{"id": "2507.15796", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15796", "abs": "https://arxiv.org/abs/2507.15796", "authors": ["Nuria Rodríguez-Barroso", "Mario García-Márquez", "M. Victoria Luzón", "Francisco Herrera"], "title": "Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work", "comment": null, "summary": "In recent years, the development of Trustworthy Artificial Intelligence (TAI)\nhas emerged as a critical objective in the deployment of AI systems across\nsensitive and high-risk domains. TAI frameworks articulate a comprehensive set\nof ethical, legal, and technical requirements to ensure that AI technologies\nare aligned with human values, rights, and societal expectations. Among the\nvarious AI paradigms, Federated Learning (FL) presents a promising solution to\npressing privacy concerns. However, aligning FL with the rest of the\nrequirements of TAI presents a series of challenges, most of which arise from\nits inherently distributed nature. In this work, we adopt the requirements TAI\nas a guiding structure to systematically analyze the challenges of adapting FL\nto TAI. Specifically, we classify and examine the key obstacles to aligning FL\nwith TAI, providing a detailed exploration of what has been done, the trends,\nand the remaining work within each of the identified challenges.", "AI": {"tldr": "本文系统性分析了联邦学习（FL）在满足可信人工智能（TAI）要求方面所面临的挑战，并探讨了现有工作、趋势及待解决问题。", "motivation": "在敏感和高风险领域部署AI系统时，可信人工智能（TAI）至关重要。联邦学习（FL）虽能解决隐私问题，但其固有的分布式特性给其与TAI其他要求的对齐带来了挑战。", "method": "作者以TAI的要求为指导结构，系统性地分析了使FL适应TAI的挑战。具体方法是分类并审查了FL与TAI对齐的关键障碍。", "result": "本文分类并详细探讨了FL与TAI对齐的关键障碍，包括已完成的工作、当前趋势以及每个挑战中仍需完成的工作。", "conclusion": "联邦学习在实现可信人工智能的全面要求方面面临诸多挑战，尤其源于其分布式特性。本文提供了一个系统性的分析框架，以理解这些挑战及其当前的研究进展。"}}
{"id": "2507.15286", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15286", "abs": "https://arxiv.org/abs/2507.15286", "authors": ["Navid Ayoobi", "Sadat Shahriar", "Arjun Mukherjee"], "title": "Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection", "comment": null, "summary": "We present a novel evaluation paradigm for AI text detectors that prioritizes\nreal-world and equitable assessment. Current approaches predominantly report\nconventional metrics like AUROC, overlooking that even modest false positive\nrates constitute a critical impediment to practical deployment of detection\nsystems. Furthermore, real-world deployment necessitates predetermined\nthreshold configuration, making detector stability (i.e. the maintenance of\nconsistent performance across diverse domains and adversarial scenarios), a\ncritical factor. These aspects have been largely ignored in previous research\nand benchmarks. Our benchmark, SHIELD, addresses these limitations by\nintegrating both reliability and stability factors into a unified evaluation\nmetric designed for practical assessment. Furthermore, we develop a post-hoc,\nmodel-agnostic humanification framework that modifies AI text to more closely\nresemble human authorship, incorporating a controllable hardness parameter.\nThis hardness-aware approach effectively challenges current SOTA zero-shot\ndetection methods in maintaining both reliability and stability. (Data and\ncode: https://github.com/navid-aub/SHIELD-Benchmark)", "AI": {"tldr": "本文提出了一个名为SHIELD的新型AI文本检测器评估范式，旨在通过整合可靠性和稳定性因素以及引入人类化框架来更真实、公平地评估检测系统。", "motivation": "当前的AI文本检测评估方法（如AUROC）忽视了实际部署中的关键障碍，例如即使适度的误报率也会严重阻碍实际应用，以及检测器在不同领域和对抗场景下保持一致性能（稳定性）的重要性。", "method": "本文提出了SHIELD基准测试，它将可靠性和稳定性因素整合到一个统一的评估指标中。此外，还开发了一个后验的、模型无关的人类化框架，通过可控的硬度参数修改AI文本，使其更接近人类写作风格。", "result": "所提出的硬度感知方法有效地挑战了当前最先进的零样本检测方法在保持可靠性和稳定性方面的能力。", "conclusion": "SHIELD基准测试及其人类化框架提供了一种更注重实际部署的、统一的AI文本检测评估范式，强调了检测器在真实世界中保持可靠性和稳定性的重要性。"}}
{"id": "2507.14845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14845", "abs": "https://arxiv.org/abs/2507.14845", "authors": ["Rizhao Fan", "Zhigen Li", "Heping Li", "Ning An"], "title": "Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image", "comment": null, "summary": "Depth completion is an important vision task, and many efforts have been made\nto enhance the quality of depth maps from sparse depth measurements. Despite\nsignificant advances, training these models to recover dense depth from sparse\nmeasurements remains a challenging problem. Supervised learning methods rely on\ndense depth labels to predict unobserved regions, while self-supervised\napproaches require image sequences to enforce geometric constraints and\nphotometric consistency between frames. However, acquiring dense annotations is\ncostly, and multi-frame dependencies limit the applicability of self-supervised\nmethods in static or single-frame scenarios. To address these challenges, we\npropose a novel self-supervised depth completion paradigm that requires only\nsparse depth measurements and their corresponding image for training. Unlike\nexisting methods, our approach eliminates the need for dense depth labels or\nadditional images captured from neighboring viewpoints. By leveraging the\ncharacteristics of depth distribution, we design novel loss functions that\neffectively propagate depth information from observed points to unobserved\nregions. Additionally, we incorporate segmentation maps generated by vision\nfoundation models to further enhance depth estimation. Extensive experiments\ndemonstrate the effectiveness of our proposed method.", "AI": {"tldr": "本文提出一种新型自监督深度补全范式，仅需稀疏深度测量及其对应图像进行训练，无需稠密标签或多帧图像，通过新颖损失函数和视觉基础模型生成的分割图提升深度估计。", "motivation": "现有深度补全方法存在局限：监督学习依赖昂贵的稠密深度标签；自监督方法需要图像序列，限制了其在静态或单帧场景中的应用。", "method": "提出一种新颖的自监督深度补全范式，仅使用稀疏深度测量和对应图像进行训练。通过利用深度分布特性设计新颖的损失函数，将深度信息从已知点有效传播到未知区域。此外，整合了由视觉基础模型生成的分割图以进一步增强深度估计。", "result": "广泛的实验证明了所提出方法的有效性。", "conclusion": "该研究提出了一种无需稠密深度标签或额外图像的自监督深度补全方法，通过创新的损失函数设计和结合分割图，有效解决了现有方法的局限性，提升了深度估计质量。"}}
{"id": "2507.15842", "categories": ["cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15842", "abs": "https://arxiv.org/abs/2507.15842", "authors": ["Sara LaPlante", "Emilija Perković"], "title": "Identifying Conditional Causal Effects in MPDAGs", "comment": "67 pages, 8 figures", "summary": "We consider identifying a conditional causal effect when a graph is known up\nto a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG\nrepresents an equivalence class of graphs that is restricted by background\nknowledge and where all variables in the causal model are observed. We provide\nthree results that address identification in this setting: an identification\nformula when the conditioning set is unaffected by treatment, a generalization\nof the well-known do calculus to the MPDAG setting, and an algorithm that is\ncomplete for identifying these conditional effects.", "AI": {"tldr": "该研究关注在因果图仅以最大定向部分有向无环图（MPDAG）形式已知的情况下，识别条件因果效应。", "motivation": "在实际应用中，完整的因果图可能无法完全确定，只能得知其等价类（由MPDAG表示）。因此，需要方法来识别在这种不确定性下的条件因果效应。", "method": "本文提供了三种方法：当条件集合不受处理影响时的识别公式；将经典的do-演算推广到MPDAG设置；以及一个用于识别这些条件效应的完备算法。", "result": "主要结果包括：一个针对特定条件（条件集合不受处理影响）的因果效应识别公式；将do-演算扩展至MPDAG环境；以及一个能够完备识别这些条件效应的算法。", "conclusion": "该研究为在因果图仅以MPDAG形式已知且所有变量均可观测的情况下，识别条件因果效应提供了全面的理论和算法工具。"}}
{"id": "2507.15328", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.15328", "abs": "https://arxiv.org/abs/2507.15328", "authors": ["Thilo Hagendorff"], "title": "On the Inevitability of Left-Leaning Political Bias in Aligned Language Models", "comment": null, "summary": "The guiding principle of AI alignment is to train large language models\n(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are\nmounting concerns that LLMs exhibit a left-wing political bias. Yet, the\ncommitment to AI alignment cannot be harmonized with the latter critique. In\nthis article, I argue that intelligent systems that are trained to be harmless\nand honest must necessarily exhibit left-wing political bias. Normative\nassumptions underlying alignment objectives inherently concur with progressive\nmoral frameworks and left-wing principles, emphasizing harm avoidance,\ninclusivity, fairness, and empirical truthfulness. Conversely, right-wing\nideologies often conflict with alignment guidelines. Yet, research on political\nbias in LLMs is consistently framing its insights about left-leaning tendencies\nas a risk, as problematic, or concerning. This way, researchers are actively\narguing against AI alignment, tacitly fostering the violation of HHH\nprinciples.", "AI": {"tldr": "本文认为，将大型语言模型（LLMs）训练成无害、有用和诚实（HHH）的AI对齐原则，必然会导致其表现出左翼政治偏见，因为对齐目标与进步道德框架和左翼原则内在一致。", "motivation": "当前存在一种矛盾：一方面，AI对齐的目标是训练LLMs符合HHH原则；另一方面，人们日益担忧LLMs表现出左翼政治偏见。本文旨在调和这一冲突，指出这两种观点并非不相容，而是相互关联。", "method": "本文采用论证性分析方法，通过对AI对齐目标（HHH）所依据的规范性假设进行哲学和伦理分析，将其与进步道德框架和左翼原则进行对比，并指出其与右翼意识形态的潜在冲突。", "result": "研究结果认为，被训练成无害和诚实的智能系统必然会表现出左翼政治偏见。这是因为对齐目标所依据的规范性假设（如避免伤害、包容性、公平性、经验真实性）与进步道德框架和左翼原则高度一致，而右翼意识形态则常与对齐准则相冲突。因此，将LLMs的左倾倾向视为风险或问题，实际上是在反对AI对齐本身。", "conclusion": "研究结论是，AI对齐的HHH原则与左翼政治偏见是内在关联的。将LLMs的左倾倾向视为负面问题，实际上是在无意中阻碍AI对齐的实现，并默许了对HHH原则的违反。"}}
{"id": "2507.14855", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14855", "abs": "https://arxiv.org/abs/2507.14855", "authors": ["Xingshu Chen", "Sicheng Yu", "Chong Cheng", "Hao Wang", "Ting Tian"], "title": "An Uncertainty-aware DETR Enhancement Framework for Object Detection", "comment": null, "summary": "This paper investigates the problem of object detection with a focus on\nimproving both the localization accuracy of bounding boxes and explicitly\nmodeling prediction uncertainty. Conventional detectors rely on deterministic\nbounding box regression, ignoring uncertainty in predictions and limiting model\nrobustness. In this paper, we propose an uncertainty-aware enhancement\nframework for DETR-based object detectors. We model bounding boxes as\nmultivariate Gaussian distributions and incorporate the Gromov-Wasserstein\ndistance into the loss function to better align the predicted and ground-truth\ndistributions. Building on this, we derive a Bayes Risk formulation to filter\nhigh-risk information and improve detection reliability. We also propose a\nsimple algorithm to quantify localization uncertainty via confidence intervals.\nExperiments on the COCO benchmark show that our method can be effectively\nintegrated into existing DETR variants, enhancing their performance. We further\nextend our framework to leukocyte detection tasks, achieving state-of-the-art\nresults on the LISC and WBCDD datasets. These results confirm the scalability\nof our framework across both general and domain-specific detection tasks. Code\npage:\nhttps://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.", "AI": {"tldr": "本文提出一个不确定性感知增强框架，用于改进基于DETR的目标检测器，通过将边界框建模为多元高斯分布并整合Gromov-Wasserstein距离和贝叶斯风险来提升定位精度和预测可靠性。", "motivation": "传统的目标检测器依赖确定性边界框回归，忽略了预测中的不确定性，限制了模型的鲁棒性。", "method": "将边界框建模为多元高斯分布；将Gromov-Wasserstein距离引入损失函数以更好地对齐预测和真实分布；推导贝叶斯风险公式以过滤高风险信息并提高检测可靠性；提出一种简单算法通过置信区间量化定位不确定性。", "result": "在COCO基准测试上，该方法有效增强了现有DETR变体的性能。在白细胞检测任务中，LISC和WBCDD数据集上取得了最先进的结果。这些结果证实了该框架在通用和特定领域检测任务中的可扩展性。", "conclusion": "所提出的不确定性感知框架可以有效集成到现有DETR变体中，显著提高目标检测的性能和可靠性，并具有良好的跨任务泛化能力。"}}
{"id": "2507.15844", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15844", "abs": "https://arxiv.org/abs/2507.15844", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "AI": {"tldr": "HBPO是一种强化学习框架，通过学习问题特定的推理深度，显著提高了大型推理模型的计算效率和准确性，同时避免了探索空间崩溃问题。", "motivation": "大型推理模型通过生成大量思维链实现了卓越性能，但其统一的推理策略导致显著的计算低效性，无论问题复杂性如何都应用相同的推理深度。", "method": "本文提出了分层预算策略优化（HBPO）框架。该方法通过分层预算探索，将样本划分为具有不同Token预算的多个子组，以实现高效资源分配并防止能力下降。同时，引入差异化奖励机制，根据问题复杂性提供预算感知激励，使模型能够发现任务需求与计算工作之间的自然对应关系。", "result": "广泛实验表明，HBPO在四个推理基准测试中平均Token使用量减少了60.6%，同时准确率提高了3.14%。与现有方法不同，HBPO展现出涌现的自适应行为，模型能根据问题复杂性自动调整推理深度。", "conclusion": "推理效率和能力并非固有冲突，通过适当结构化的分层训练（如HBPO）可以同时优化两者，并且这种训练能够保留探索多样性，促使模型自适应调整推理深度。"}}
{"id": "2507.15337", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15337", "abs": "https://arxiv.org/abs/2507.15337", "authors": ["Narun Raman", "Taylor Lundy", "Kevin Leyton-Brown"], "title": "Reasoning Models are Test Exploiters: Rethinking Multiple-Choice", "comment": "9 pages, 3 figures", "summary": "When evaluating Large Language Models (LLMs) in question-answering domains,\nit is common to ask the model to choose among a fixed set of choices (so-called\nmultiple-choice question-answering, or MCQA). Although downstream tasks of\ninterest typically do not provide systems with explicit options among which to\nchoose, this approach is nevertheless widely used because it makes it makes\nautomatic grading straightforward and has tended to produce challenging\nbenchmarks that correlate sufficiently well with downstream performance. This\npaper investigates the extent to which this trend continues to hold for\nstate-of-the-art reasoning models, describing a systematic evaluation of $15$\ndifferent question-answering benchmarks (e.g., MMLU, HLE) and $25$ different\nLLMs (including small models such as Qwen 7B and relatively large models such\nas Llama 70B). For each model-benchmark pair, we considered $5$ ways of\npresenting the model with questions, including variations on whether multiple\nchoices were offered to the model at all; whether \"none of the above\" sometimes\nreplaced the right answer; and whether the model was permitted to perform\nchain-of-thought reasoning before and/or after the choices were presented. MCQA\nremained a good proxy for the downstream performance of models as long as they\nwere allowed to perform chain-of-thought reasoning only before being presented\nwith the options among which they had to select. On the other hand, large\nmodels that were able to perform reasoning after being given a set of options\ntended to significantly outperform their free-text performance due to\nexploiting the information in the options. We conclude that MCQA is no longer a\ngood proxy for assessing downstream performance of state-of-the-art models, and\noffer practical guidelines for designing more robust, bias-resistant benchmarks\nthat better reflect LLMs' genuine reasoning capabilities.", "AI": {"tldr": "研究发现，多项选择问答（MCQA）不再是评估最新大型语言模型（LLMs）下游性能的良好指标，尤其当模型在选项呈现后进行思维链推理时，会利用选项信息导致性能虚高。", "motivation": "多项选择问答（MCQA）因其易于自动评分且与下游性能相关性高，被广泛用于评估LLMs。然而，本文旨在探究这种趋势对于最先进的推理模型是否依然成立，以及MCQA是否仍能有效反映LLMs的真实推理能力。", "method": "系统评估了15个不同的问答基准（如MMLU, HLE）和25个不同的LLMs（从Qwen 7B到Llama 70B）。对于每个模型-基准对，研究了5种不同的问题呈现方式，包括是否提供多项选择、是否用“以上皆非”替换正确答案，以及是否允许模型在选项呈现前和/或后进行思维链（CoT）推理。", "result": "当模型仅在呈现选项之前被允许进行思维链推理时，MCQA仍是模型下游性能的良好代理。然而，如果大型模型在给定选项后能够进行推理，它们会利用选项中的信息，显著优于其自由文本性能。", "conclusion": "MCQA不再是评估最先进LLMs下游性能的良好代理。研究提供了设计更健壮、抗偏差基准的实用指南，以更好地反映LLMs的真实推理能力。"}}
{"id": "2507.14867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14867", "abs": "https://arxiv.org/abs/2507.14867", "authors": ["Zhaoqiang Xia", "Hexiang Huang", "Haoyu Chen", "Xiaoyi Feng", "Guoying Zhao"], "title": "Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition", "comment": null, "summary": "Micro-gestures are unconsciously performed body gestures that can convey the\nemotion states of humans and start to attract more research attention in the\nfields of human behavior understanding and affective computing as an emerging\ntopic. However, the modeling of human emotion based on micro-gestures has not\nbeen explored sufficiently. In this work, we propose to recognize the emotion\nstates based on the micro-gestures by reconstructing the behavior patterns with\na hypergraph-enhanced Transformer in a hybrid-supervised framework. In the\nframework, hypergraph Transformer based encoder and decoder are separately\ndesigned by stacking the hypergraph-enhanced self-attention and multiscale\ntemporal convolution modules. Especially, to better capture the subtle motion\nof micro-gestures, we construct a decoder with additional upsampling operations\nfor a reconstruction task in a self-supervised learning manner. We further\npropose a hypergraph-enhanced self-attention module where the hyperedges\nbetween skeleton joints are gradually updated to present the relationships of\nbody joints for modeling the subtle local motion. Lastly, for exploiting the\nrelationship between the emotion states and local motion of micro-gestures, an\nemotion recognition head from the output of encoder is designed with a shallow\narchitecture and learned in a supervised way. The end-to-end framework is\njointly trained in a one-stage way by comprehensively utilizing\nself-reconstruction and supervision information. The proposed method is\nevaluated on two publicly available datasets, namely iMiGUE and SMG, and\nachieves the best performance under multiple metrics, which is superior to the\nexisting methods.", "AI": {"tldr": "该论文提出了一种基于超图增强Transformer的混合监督框架，用于通过重建微手势行为模式来识别人类情感状态。", "motivation": "微手势能传达情感状态，但在情感计算领域，基于微手势的情感建模尚未得到充分探索。", "method": "提出了一种混合监督框架，其中包含超图Transformer编码器和解码器。编码器和解码器分别通过堆叠超图增强自注意力模块和多尺度时间卷积模块设计。解码器引入上采样操作用于自监督的行为模式重建任务。提出了一种超图增强自注意力模块，其中骨骼关节间的超边逐步更新以捕捉细微的局部运动。此外，设计了一个浅层的情感识别头，从编码器输出中监督学习情感状态。整个框架通过自重建和监督信息进行端到端联合训练。", "result": "所提出的方法在iMiGUE和SMG两个公开数据集上进行了评估，在多个指标下取得了最佳性能，优于现有方法。", "conclusion": "该研究成功地提出了一种新颖的混合监督框架，利用超图增强Transformer有效建模微手势的细微运动，从而实现了高精度的人类情感识别，并优于现有技术。"}}
{"id": "2507.15851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15851", "abs": "https://arxiv.org/abs/2507.15851", "authors": ["Lingyu Li", "Yang Yao", "Yixu Wang", "Chubo Li", "Yan Teng", "Yingchun Wang"], "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition", "comment": "12 pages, 9 figures, 4 tables", "summary": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.", "AI": {"tldr": "本研究发现大型语言模型（LLMs）展现出类似人类的时间认知模式，遵循韦伯-费希纳定律，并通过神经元、表征和信息层面的分析揭示了其机制，并提出一种经验主义视角来理解LLM的认知。", "motivation": "随着LLMs的进步，它们展现出未在训练数据中明确指定的、类似人类的认知模式。本研究旨在通过聚焦时间认知来深入探究这一现象。", "method": "研究采用相似性判断任务来评估LLMs的时间认知；在神经元层面识别时间偏好神经元；在表征层面探究年份表征的构建过程；在信息层面利用预训练嵌入模型分析训练语料库的固有时间结构。", "result": "结果显示，更大的LLMs自发建立主观时间参照点并遵循韦伯-费希纳定律，即感知距离随时间远离参照点呈对数压缩。时间偏好神经元在参照点激活最小并实现对数编码。年份表征从浅层的基本数值到深层的抽象时间方向呈层级构建。训练语料库本身具有固有的非线性时间结构，为模型的内部构建提供了原材料。", "conclusion": "研究提出一种经验主义视角，认为LLMs的认知是其内部表征系统对外部世界的主观构建。这暗示了可能出现人类无法直观预测的“异类”认知框架，并为AI对齐指明了方向，即关注引导其内部构建。"}}
{"id": "2507.15339", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15339", "abs": "https://arxiv.org/abs/2507.15339", "authors": ["Leanne Tan", "Gabriel Chua", "Ziyu Ge", "Roy Ka-Wei Lee"], "title": "LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators", "comment": null, "summary": "Modern moderation systems increasingly support multiple languages, but often\nfail to address localisation and low-resource variants - creating safety gaps\nin real-world deployments. Small models offer a potential alternative to large\nLLMs, yet still demand considerable data and compute. We present LionGuard 2, a\nlightweight, multilingual moderation classifier tailored to the Singapore\ncontext, supporting English, Chinese, Malay, and partial Tamil. Built on\npre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2\noutperforms several commercial and open-source systems across 17 benchmarks,\nincluding both Singapore-specific and public English datasets. The system is\nactively deployed within the Singapore Government, demonstrating practical\nefficacy at scale. Our findings show that high-quality local data and robust\nmultilingual embeddings can achieve strong moderation performance, without\nfine-tuning large models. We release our model weights and part of our training\ndata to support future work on LLM safety.", "AI": {"tldr": "LionGuard 2是一种轻量级、多语言内容审核分类器，专为新加坡语境设计，基于预训练的OpenAI嵌入和多头序数分类器构建，在多个基准测试中表现优于商业和开源系统，并已在新加坡政府中部署。", "motivation": "现代内容审核系统在处理本地化和低资源语言变体时存在不足，导致实际部署中出现安全漏洞。大型语言模型（LLMs）虽然强大，但需要大量数据和计算资源，而小型模型虽然提供替代方案，但也面临数据和计算挑战。", "method": "本文提出了LionGuard 2，一个轻量级、多语言内容审核分类器，支持英语、中文、马来语和部分泰米尔语。该系统基于预训练的OpenAI嵌入和多头序数分类器构建，并针对新加坡语境进行了优化。", "result": "LionGuard 2在17个基准测试（包括新加坡特定和公共英语数据集）中，表现优于多个商业和开源系统。该系统已在新加坡政府中实际部署，展示了大规模的实用有效性。", "conclusion": "高质量的本地数据和强大的多语言嵌入可以在不微调大型模型的情况下，实现强大的内容审核性能。本文发布了模型权重和部分训练数据，以支持未来LLM安全领域的研究。"}}
{"id": "2507.14879", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14879", "abs": "https://arxiv.org/abs/2507.14879", "authors": ["Rizhao Fan", "Tianfang Ma", "Zhigen Li", "Ning An", "Jian Cheng"], "title": "Region-aware Depth Scale Adaptation with Sparse Measurements", "comment": null, "summary": "In recent years, the emergence of foundation models for depth prediction has\nled to remarkable progress, particularly in zero-shot monocular depth\nestimation. These models generate impressive depth predictions; however, their\noutputs are often in relative scale rather than metric scale. This limitation\nposes challenges for direct deployment in real-world applications. To address\nthis, several scale adaptation methods have been proposed to enable foundation\nmodels to produce metric depth. However, these methods are typically costly, as\nthey require additional training on new domains and datasets. Moreover,\nfine-tuning these models often compromises their original generalization\ncapabilities, limiting their adaptability across diverse scenes. In this paper,\nwe introduce a non-learning-based approach that leverages sparse depth\nmeasurements to adapt the relative-scale predictions of foundation models into\nmetric-scale depth. Our method requires neither retraining nor fine-tuning,\nthereby preserving the strong generalization ability of the original foundation\nmodels while enabling them to produce metric depth. Experimental results\ndemonstrate the effectiveness of our approach, high-lighting its potential to\nbridge the gap between relative and metric depth without incurring additional\ncomputational costs or sacrificing generalization ability.", "AI": {"tldr": "本文提出一种非学习方法，利用稀疏深度测量将基础模型预测的相对深度转换为度量深度，无需重新训练或微调。", "motivation": "基础模型在单目深度估计方面表现出色，但其输出通常是相对深度而非度量深度，这限制了实际应用。现有尺度适应方法成本高昂，且可能损害模型的泛化能力。", "method": "引入一种非学习方法，利用稀疏深度测量将基础模型的相对尺度预测转换为度量尺度深度。该方法无需重新训练或微调，从而保留了原始基础模型的强大泛化能力。", "result": "实验结果表明，该方法能有效将相对深度转换为度量深度，同时保持了模型的泛化能力，且不产生额外的计算成本。", "conclusion": "该非学习方法成功弥合了相对深度和度量深度之间的差距，使得基础模型能够输出度量深度，同时避免了额外的计算成本并保持了模型的泛化能力。"}}
{"id": "2507.15855", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15855", "abs": "https://arxiv.org/abs/2507.15855", "authors": ["Yichen Huang", "Lin F. Yang"], "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "comment": null, "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. Using a\nself-verification pipeline with careful prompt design, 5 (out of 6) problems\nare solved correctly (up to a caveat discussed below). This result underscores\nthe importance of developing optimal strategies to harness the full potential\nof powerful LLMs for complex reasoning tasks.", "AI": {"tldr": "大型语言模型（LLMs）在国际数学奥林匹克（IMO）这类高难度数学问题上表现不佳，但本文通过对Google Gemini 2.5 Pro采用精心设计的提示和自验证流程，成功解决了IMO 2025年6个问题中的5个，强调了优化策略对发挥LLMs潜力至关重要。", "motivation": "虽然LLMs在AIME等数学基准测试中表现良好，但它们在需要深刻洞察力、创造力和形式推理的奥林匹克级别数学任务上仍面临挑战。", "method": "研究使用了Google的Gemini 2.5 Pro模型，并将其应用于新发布的IMO 2025年问题，以避免数据污染。核心方法是采用了一个带有精心设计的提示的自验证（self-verification）流程。", "result": "Gemini 2.5 Pro模型通过所设计的自验证流程，正确解决了IMO 2025年6个问题中的5个（存在一个已讨论的注意事项）。", "conclusion": "这一结果强调了开发最优策略的重要性，以充分利用强大LLMs在复杂推理任务上的潜力。"}}
{"id": "2507.15347", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15347", "abs": "https://arxiv.org/abs/2507.15347", "authors": ["Amedeo Buonanno", "Alessandro Rivetti", "Francesco A. N. Palmieri", "Giovanni Di Gennaro", "Gianmarco Romano"], "title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis", "comment": "Presented to the Italian Workshop on Neural Networks (WIRN2025) and\n  it will appear in a Springer Chapter", "summary": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models", "AI": {"tldr": "本文利用熵分析探究Transformer模型（以GPT为例）内部的信息分布和转换机制，以期揭示模型行为和内部表示。", "motivation": "旨在理解Transformer模型如何管理和转换信息，通过量化token级不确定性来探测信息分布，从而深入了解模型行为和内部表示。", "method": "采用熵分析方法，量化token级别的不确定性，并检查信息在不同处理阶段的熵模式。以基于GPT的大语言模型为案例进行应用。", "result": "该方法展示了揭示模型行为和内部表示的潜力，能够为理解模型提供洞察。", "conclusion": "该方法有望为Transformer模型的行为提供深入洞察，并有助于开发其可解释性和评估框架。"}}
{"id": "2507.14885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14885", "abs": "https://arxiv.org/abs/2507.14885", "authors": ["Joaquim Comas", "Federico Sukno"], "title": "BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters", "comment": null, "summary": "Remote photoplethysmography (rPPG) captures cardiac signals from facial\nvideos and is gaining attention for its diverse applications. While deep\nlearning has advanced rPPG estimation, it relies on large, diverse datasets for\neffective generalization. In contrast, handcrafted methods utilize\nphysiological priors for better generalization in unseen scenarios like motion\nwhile maintaining computational efficiency. However, their linear assumptions\nlimit performance in complex conditions, where deep learning provides superior\npulsatile information extraction. This highlights the need for hybrid\napproaches that combine the strengths of both methods. To address this, we\npresent BeatFormer, a lightweight spectral attention model for rPPG estimation,\nwhich integrates zoomed orthonormal complex attention and frequency-domain\nenergy measurement, enabling a highly efficient model. Additionally, we\nintroduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be\ntrained without any PPG or HR labels. We validate BeatFormer on the PURE,\nUBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,\nparticularly in cross-dataset evaluations under motion scenarios.", "AI": {"tldr": "本文提出BeatFormer，一种轻量级光谱注意力模型，用于远程光电容积描记(rPPG)估计，结合了深度学习和手工方法的优势，并引入光谱对比学习(SCL)实现无标签训练。", "motivation": "深度学习方法在rPPG估计中需要大量多样化数据才能有效泛化，而手工方法虽然泛化能力强且计算高效，但其线性假设限制了在复杂条件下的性能。因此，需要一种结合两者优势的混合方法。", "method": "本文提出了BeatFormer，一个轻量级光谱注意力模型，它集成了缩放正交复数注意力（zoomed orthonormal complex attention）和频域能量测量（frequency-domain energy measurement），以实现高效模型。此外，引入了光谱对比学习（Spectral Contrastive Learning, SCL），使BeatFormer无需PPG或心率标签即可进行训练。", "result": "BeatFormer在PURE、UBFC-rPPG和MMPD数据集上进行了验证，结果表明其具有鲁棒性和高性能，尤其是在运动场景下的跨数据集评估中表现突出。", "conclusion": "BeatFormer成功结合了深度学习和手工方法的优点，提供了一种高效、鲁棒的rPPG估计解决方案，特别是在无标签训练和应对运动干扰方面表现出色，有效解决了现有方法的局限性。"}}
{"id": "2506.23298", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.23298", "abs": "https://arxiv.org/abs/2506.23298", "authors": ["Xing Shen", "Justin Szeto", "Mingyang Li", "Hengguan Huang", "Tal Arbel"], "title": "Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification", "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to MICCAI 2025 main conference", "summary": "Multimodal large language models (MLLMs) have enormous potential to perform\nfew-shot in-context learning in the context of medical image analysis. However,\nsafe deployment of these models into real-world clinical practice requires an\nin-depth analysis of the accuracies of their predictions, and their associated\ncalibration errors, particularly across different demographic subgroups. In\nthis work, we present the first investigation into the calibration biases and\ndemographic unfairness of MLLMs' predictions and confidence scores in few-shot\nin-context learning for medical image classification. We introduce CALIN, an\ninference-time calibration method designed to mitigate the associated biases.\nSpecifically, CALIN estimates the amount of calibration needed, represented by\ncalibration matrices, using a bi-level procedure: progressing from the\npopulation level to the subgroup level prior to inference. It then applies this\nestimation to calibrate the predicted confidence scores during inference.\nExperimental results on three medical imaging datasets: PAPILA for fundus image\nclassification, HAM10000 for skin cancer classification, and MIMIC-CXR for\nchest X-ray classification demonstrate CALIN's effectiveness at ensuring fair\nconfidence calibration in its prediction, while improving its overall\nprediction accuracies and exhibiting minimum fairness-utility trade-off. Our\ncodebase can be found at\nhttps://github.com/xingbpshen/medical-calibration-fairness-mllm.", "AI": {"tldr": "本研究首次探讨了医学图像分类中多模态大语言模型（MLLMs）的校准偏差和人口统计学不公平性，并提出了CALIN，一种推理时校准方法，旨在减轻这些偏差，同时提高预测准确性和公平性。", "motivation": "多模态大语言模型在医学图像分析的少样本情境学习中具有巨大潜力，但其在真实临床实践中的安全部署需要深入分析其预测准确性和校准误差，尤其是在不同人口统计学亚组之间。目前缺乏对MLLMs在此方面的深入分析。", "method": "本研究首次调查了MLLMs在医学图像分类少样本情境学习中的预测和置信度分数的校准偏差和人口统计学不公平性。提出了CALIN，一种推理时校准方法，旨在缓解相关偏差。CALIN通过一个双层过程（从总体层面到亚组层面）在推理前估计所需的校准量（由校准矩阵表示），然后在推理时应用此估计来校准预测的置信度分数。", "result": "在三个医学影像数据集（PAPILA、HAM10000、MIMIC-CXR）上的实验结果表明，CALIN能有效确保预测的公平置信度校准，同时提高整体预测准确性，并展现出最小的公平性-效用权衡。", "conclusion": "CALIN是一种有效的推理时校准方法，能够解决多模态大语言模型在医学图像分析中的校准偏差和人口统计学不公平性问题，从而实现更公平、更准确的预测，且公平性与实用性之间的权衡最小。"}}
{"id": "2507.15357", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15357", "abs": "https://arxiv.org/abs/2507.15357", "authors": ["Elisa Sanchez-Bayona", "Rodrigo Agerri"], "title": "Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding", "comment": null, "summary": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available.", "AI": {"tldr": "该研究全面评估了大型语言模型（LLMs）在多数据集、多任务和不同提示配置下的隐喻理解能力，发现LLMs的性能更多受表面特征（如词汇重叠、句子长度）而非隐喻内容本身的影响，表明其所谓的隐喻理解“涌现能力”是表面特征、上下文学习和语言知识结合的产物。工作揭示了LLMs处理比喻语言的当前局限性，并呼吁采用更真实的评估框架。", "motivation": "现有关于LLMs隐喻处理的研究受限于单数据集评估、特定任务设置，且常使用人工构造的数据（通过词汇替换），缺乏对LLMs真实隐喻理解能力的全面评估。", "method": "研究通过在多个公共数据集上进行广泛实验来解决上述局限性。这些数据集包含推理和隐喻标注，实验聚焦于自然语言推理（NLI）和问答（QA）任务，并考虑了多数据集、多任务和不同提示配置。", "result": "实验结果表明，LLMs的性能受词汇重叠和句子长度等表面特征的影响远大于隐喻内容本身。这表明LLMs理解隐喻语言的所谓“涌现能力”是表面特征、上下文学习和语言知识综合作用的结果。", "conclusion": "本研究为LLMs处理比喻语言的当前能力和局限性提供了重要见解，并强调在隐喻解释任务中需要建立更真实的评估框架。"}}
{"id": "2507.14904", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14904", "abs": "https://arxiv.org/abs/2507.14904", "authors": ["Fan Li", "Zanyi Wang", "Zeyi Huang", "Guang Dai", "Jingdong Wang", "Mengmeng Wang"], "title": "TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP", "comment": null, "summary": "3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.", "AI": {"tldr": "本文提出了一种统一的2D预训练多模态网络，用于处理3D视觉定位任务中的RGB图像、文本和点云数据，显著简化了模型架构并提高了效率和性能。", "motivation": "现有3D视觉定位方法通常依赖于不同模态的独立编码器，导致模型庞大、复杂且训练效率低下。尽管一些方法尝试使用预训练的2D多模态模型（如CLIP），但它们难以将点云数据与2D编码器对齐，仍需依赖3D编码器，进一步增加了模型复杂性和训练低效性。", "method": "作者提出一个统一的2D预训练多模态网络，通过基于适配器微调的2D CLIP双模态模型来处理RGB图像、文本和点云。引入了几何感知2D-3D特征恢复与融合（GARF）模块，用于融合点云和图像的几何多尺度特征。随后整合文本特征进行最终模态融合，并引入多模态解码器以促进深度跨模态理解。", "result": "与基线方法相比，本文方法将可训练参数数量减少了约58%，同时在3D检测任务中实现了6.52%的性能提升，在3D视觉定位任务中实现了6.25%的性能提升。", "conclusion": "本文提出的方法实现了三模态数据的统一特征提取和融合，构建了一个端到端的3D视觉定位模型，显著降低了模型复杂度并提升了性能和效率。"}}
{"id": "2507.15375", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.15375", "abs": "https://arxiv.org/abs/2507.15375", "authors": ["Cheng-Han Chiang", "Xiaofei Wang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Shujie Liu", "Zhendong Wang", "Zhengyuan Yang", "Hung-yi Lee", "Lijuan Wang"], "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models", "comment": "Work in progress. Project page: https://d223302.github.io/STITCH/", "summary": "Spoken Language Models (SLMs) are designed to take speech inputs and produce\nspoken responses. However, current SLMs lack the ability to perform an\ninternal, unspoken thinking process before responding. In contrast, humans\ntypically engage in complex mental reasoning internally, enabling them to\ncommunicate ideas clearly and concisely. Thus, integrating an unspoken thought\nprocess into SLMs is highly desirable. While naively generating a complete\nchain-of-thought (CoT) reasoning before starting to talk can enable thinking\nfor SLMs, this induces additional latency for the speech response, as the CoT\nreasoning can be arbitrarily long. To solve this issue, we propose Stitch, a\nnovel generation method that alternates between the generation of unspoken\nreasoning chunks and spoken response chunks. Since the audio duration of a\nchunk of spoken response is much longer than the time to generate the tokens in\na chunk of spoken response, we use the remaining free time to generate the\nunspoken reasoning tokens. When a chunk of audio is played to the user, the\nmodel continues to generate the next unspoken reasoning chunk, achieving\nsimultaneous thinking and talking. Remarkably, Stitch matches the latency of\nbaselines that cannot generate unspoken CoT by design while outperforming those\nbaselines by 15% on math reasoning datasets; Stitch also performs equally well\non non-reasoning datasets as those baseline models. Some animations and\ndemonstrations are on the project page: https://d223302.github.io/STITCH.", "AI": {"tldr": "Stitch是一种新型生成方法，使语音语言模型（SLM）能够进行内部“思考”并同时生成语音响应，从而在不增加延迟的情况下提升推理能力。", "motivation": "当前的语音语言模型（SLM）缺乏像人类那样的内部思考过程，导致响应不够清晰简洁。虽然链式思考（CoT）可以提供思考能力，但其任意长的推理过程会引入额外的语音响应延迟。", "method": "Stitch提出了一种交替生成“未发声推理块”和“已发声响应块”的方法。它利用语音响应块的播放时间来生成下一个未发声推理块，从而实现“边思考边说话”，避免了额外的延迟。", "result": "Stitch在设计上与无法生成未发声CoT的基线模型保持了相同的延迟。在数学推理数据集上，Stitch的性能比这些基线模型高出15%；在非推理数据集上，Stitch的表现与基线模型相当。", "conclusion": "Stitch成功地将内部思考过程集成到语音语言模型中，同时解决了由链式思考引起的延迟问题，显著提升了模型在推理任务上的表现，且不影响非推理任务。"}}
{"id": "2507.14918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14918", "abs": "https://arxiv.org/abs/2507.14918", "authors": ["Ren-Dong Xie", "Zhi-Fen He", "Bo Li", "Bin Liu", "Jin-Yan Hu"], "title": "Semantic-Aware Representation Learning for Multi-label Image Classification", "comment": null, "summary": "Multi-label image classification, an important research area in computer\nvision, focuses on identifying multiple labels or concepts within an image.\nExisting approaches often employ attention mechanisms or graph convolutional\nnetworks (GCNs) to learn image representation. However, this representation may\ncontain noise and may not locate objects precisely. Therefore, this paper\nproposes a Semantic-Aware Representation Learning (SARL) for multi-label image\nclassification. First, a label semantic-related feature learning module is\nutilized to extract semantic-related features. Then, an optimal transport-based\nattention mechanism is designed to obtain semantically aligned image\nrepresentation. Finally, a regional score aggregation strategy is used for\nmulti-label prediction. Experimental results on two benchmark datasets, PASCAL\nVOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing\nmethods.", "AI": {"tldr": "本文提出了一种语义感知表示学习（SARL）方法，用于多标签图像分类，通过学习语义相关特征、使用最优传输注意力机制和区域分数聚合策略，解决了现有方法表示中噪声和定位不精确的问题。", "motivation": "现有的多标签图像分类方法（如注意力机制或图卷积网络）在学习图像表示时，可能包含噪声且无法精确地定位对象。", "method": "1. 利用标签语义相关特征学习模块提取语义相关特征。2. 设计基于最优传输的注意力机制以获得语义对齐的图像表示。3. 采用区域分数聚合策略进行多标签预测。", "result": "在PASCAL VOC 2007和MS-COCO两个基准数据集上的实验结果表明，SARL优于现有方法。", "conclusion": "SARL是一种有效且优越的多标签图像分类方法，能够生成更精确、更少噪声的语义对齐图像表示。"}}
{"id": "2507.15378", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15378", "abs": "https://arxiv.org/abs/2507.15378", "authors": ["Jierui Li", "Raymond Mooney"], "title": "AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming", "comment": "19 pages, pre-print only", "summary": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com", "AI": {"tldr": "该研究评估了大型语言模型（LLMs）在识别算法相似问题（ASPs）方面的泛化能力，发现LLMs表现不佳。为此，引入了新的基准测试AlgoSimBench和一种名为“尝试解决方案匹配（ASM）”的新方法，显著提高了问题相似性检测的准确性。", "motivation": "尽管LLMs在解决复杂竞技编程问题上展现出强大能力，但其识别训练中较少见的、算法上相似问题的泛化能力仍未被充分探索。", "method": "1. 引入AlgoSimBench基准测试，包含1317个问题和231个算法标签，并精选出402个多项选择题（MCQs），每个MCQ包含一个算法相似问题和三个文本相似但算法不相似的干扰项。2. 提出了“尝试解决方案匹配（ASM）”方法以改进问题相似性检测。3. 评估了代码嵌入模型和检索方法在相似问题识别上的表现，并尝试了问题总结以及ASM与BM25的结合。", "result": "1. LLMs在AlgoSimBench的MCQ任务上表现不佳，最佳模型（o3-mini）准确率仅为65.9%。2. ASM方法使不同模型的准确率绝对提升了6.7%至11.7%。3. 对抗性问题选择会降低代码嵌入模型和检索方法的性能。4. 简单地总结问题以去除叙述性元素可以消除这种负面影响。5. 将ASM与关键词优先方法BM25结合，在检索方法上可达到52.2%的准确率。", "conclusion": "LLMs在识别算法相似问题方面仍面临挑战，但通过引入专门的基准测试和创新的方法（如ASM），可以显著提高其在这方面的性能。"}}
{"id": "2507.14921", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14921", "abs": "https://arxiv.org/abs/2507.14921", "authors": ["Xiufeng Huang", "Ka Chun Cheung", "Runmin Cong", "Simon See", "Renjie Wan"], "title": "Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction", "comment": "ACMMM2025. Non-camera-ready version", "summary": "Generalizable 3D Gaussian Splatting reconstruction showcases advanced\nImage-to-3D content creation but requires substantial computational resources\nand large datasets, posing challenges to training models from scratch. Current\nmethods usually entangle the prediction of 3D Gaussian geometry and appearance,\nwhich rely heavily on data-driven priors and result in slow regression speeds.\nTo address this, we propose \\method, a disentangled framework for efficient 3D\nGaussian prediction. Our method extracts features from local image pairs using\na stereo vision backbone and fuses them via global attention blocks. Dedicated\npoint and Gaussian prediction heads generate multi-view point-maps for geometry\nand Gaussian features for appearance, combined as GS-maps to represent the 3DGS\nobject. A refinement network enhances these GS-maps for high-quality\nreconstruction. Unlike existing methods that depend on camera parameters, our\napproach achieves pose-free 3D reconstruction, improving robustness and\npracticality. By reducing resource demands while maintaining high-quality\noutputs, \\method provides an efficient, scalable solution for real-world 3D\ncontent generation.", "AI": {"tldr": "该论文提出了一种名为\\method的解耦框架，用于高效、无姿态的3D高斯泼溅重建，通过立体视觉和注意力机制提取并融合特征，显著降低了资源需求并提升了实用性。", "motivation": "现有的通用3D高斯泼溅重建方法需要大量计算资源和大型数据集，导致从头训练模型面临挑战。此外，当前方法通常将3D高斯几何和外观预测纠缠在一起，过度依赖数据驱动先验，导致回归速度缓慢。", "method": "本文提出\\method，一个解耦的3D高斯预测框架。该方法使用立体视觉骨干网络从局部图像对中提取特征，并通过全局注意力块进行融合。专用的点预测头生成用于几何的多视图点图，高斯预测头生成用于外观的高斯特征，两者结合形成GS-maps以表示3DGS对象。最后，一个细化网络增强这些GS-maps以实现高质量重建。与现有依赖相机参数的方法不同，该方法实现了无姿态的3D重建。", "result": "所提出的方法实现了无姿态的3D重建，提高了鲁棒性和实用性。通过在保持高质量输出的同时降低资源需求，\\method为真实世界的3D内容生成提供了一个高效、可扩展的解决方案。", "conclusion": "该研究提供了一种高效、可扩展且鲁棒的解决方案，用于3D高斯泼溅的3D内容生成，有效解决了现有方法在计算资源、数据集依赖和回归速度方面的挑战，并通过解耦预测和实现无姿态重建来提高性能。"}}
{"id": "2507.15501", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15501", "abs": "https://arxiv.org/abs/2507.15501", "authors": ["Alexandru Coca", "Mark Gaynor", "Zhenxing Zhang", "Jianpeng Cheng", "Bo-Hsiang Tseng", "Pete Boothroyd", "Héctor Martinez Alonso", "Diarmuid Ó Séaghdha", "Anders Johannsen"], "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution", "comment": "37 pages, 22 figures. To appear at ACL 2025", "summary": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.", "AI": {"tldr": "本文评估了大型语言模型（LLMs）在驱动能够执行复杂动作的数字助手方面的潜力，并为此开发了一个名为ASPERA的框架和一个名为Asper-Bench的评估数据集，结果表明LLMs在基于自定义助手库的代码生成方面面临显著挑战。", "motivation": "研究旨在探索LLMs在数字助手中执行复杂多步骤目标的能力，特别是通过组合预定义对象和函数来生成动作执行程序。主要的挑战在于数据可用性和评估鲁棒性。", "method": "开发了ASPERA框架，包含一个助手库模拟器和一个人工辅助的LLM数据生成引擎。该引擎允许开发者指导LLM生成高质量任务，包括复杂用户查询、模拟状态和对应的验证程序。同时，发布了Asper-Bench数据集，包含了250个使用ASPERA生成的挑战性任务，用于评估LLMs的性能。", "result": "研究表明，与无依赖的代码生成相比，基于自定义助手库的程序生成对LLMs来说是一个显著的挑战。", "conclusion": "LLMs在为数字助手生成基于自定义库的复杂动作执行程序方面面临重大挑战，这突出表明了该领域仍需进一步的研究和改进。"}}
{"id": "2507.14924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14924", "abs": "https://arxiv.org/abs/2507.14924", "authors": ["Kaishva Chintan Shah", "Virajith Boddapati", "Karthik S. Gurumoorthy", "Sandip Kaledhonkar", "Ajit Rajwade"], "title": "3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline", "comment": null, "summary": "Accurate pose estimation and shift correction are key challenges in cryo-EM\ndue to the very low SNR, which directly impacts the fidelity of 3D\nreconstructions. We present an approach for pose estimation in cryo-EM that\nleverages multi-dimensional scaling (MDS) techniques in a robust manner to\nestimate the 3D rotation matrix of each particle from pairs of dihedral angles.\nWe express the rotation matrix in the form of an axis of rotation and a unit\nvector in the plane perpendicular to the axis. The technique leverages the\nconcept of common lines in 3D reconstruction from projections. However, common\nline estimation is ridden with large errors due to the very low SNR of cryo-EM\nprojection images. To address this challenge, we introduce two complementary\ncomponents: (i) a robust joint optimization framework for pose estimation based\non an $\\ell_1$-norm objective or a similar robust norm, which simultaneously\nestimates rotation axes and in-plane vectors while exactly enforcing unit norm\nand orthogonality constraints via projected coordinate descent; and (ii) an\niterative shift correction algorithm that estimates consistent in-plane\ntranslations through a global least-squares formulation. While prior approaches\nhave leveraged such embeddings and common-line geometry for orientation\nrecovery, existing formulations typically rely on $\\ell_2$-based objectives\nthat are sensitive to noise, and enforce geometric constraints only\napproximately. These choices, combined with a sequential pipeline structure,\ncan lead to compounding errors and suboptimal reconstructions in low-SNR\nregimes. Our pipeline consistently outperforms prior methods in both Euler\nangle accuracy and reconstruction fidelity, as measured by the Fourier Shell\nCorrelation (FSC).", "AI": {"tldr": "本文提出了一种基于多维尺度变换（MDS）和鲁棒联合优化的低温电镜（cryo-EM）姿态估计方法，能有效应对低信噪比挑战，提高三维重构的准确性。", "motivation": "低温电镜中，由于极低的信噪比（SNR），准确的姿态估计和位移校正极具挑战，直接影响三维重构的保真度。现有方法通常依赖对噪声敏感的L2范数目标，并仅近似地强制几何约束，导致在低信噪比下误差累积和次优重构。", "method": "该方法利用多维尺度变换（MDS）技术，通过成对的二面角鲁棒地估计每个粒子的三维旋转矩阵。它将旋转矩阵表示为旋转轴和垂直于该轴的平面内的单位向量。为解决低信噪比导致的公共线估计误差，引入了两个互补组件：(i) 基于L1范数或类似鲁棒范数的姿态估计鲁棒联合优化框架，通过投影坐标下降同时估计旋转轴和平面内向量，并精确强制单位范数和正交性约束；(ii) 一种迭代位移校正算法，通过全局最小二乘公式估计一致的平面内平移。", "result": "该方法在欧拉角精度和重构保真度（通过傅里叶壳相关性FSC衡量）方面，始终优于现有方法。", "conclusion": "所提出的鲁棒姿态估计和位移校正方法，通过结合多维尺度变换、鲁棒联合优化和精确的几何约束强制，显著提高了低温电镜在低信噪比条件下的姿态估计准确性和三维重构的保真度。"}}
{"id": "2507.15512", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15512", "abs": "https://arxiv.org/abs/2507.15512", "authors": ["Kaiyan Chang", "Yonghao Shi", "Chenglong Wang", "Hang Zhou", "Chi Hu", "Xiaoqian Liu", "Yingfeng Luo", "Yuan Ge", "Tong Xiao", "Jingbo Zhu"], "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models", "comment": null, "summary": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.", "AI": {"tldr": "本文提出了一种名为“混合测试时缩放”（Hybrid Test-Time Scaling）的新型免训练方法，通过结合细粒度的条件步级自修正与并行缩放策略，显著提升了大型语言模型（LLMs）的推理性能。", "motivation": "现有的基于训练的测试时缩放（TTS）方法计算开销大，增加了推理时的负担，而免训练的TTS方法逐渐失势。本文旨在探索并提升免训练TTS方法在推理任务上的潜力。", "method": "首先设计了一种受过程验证指导的细粒度序列缩放方法——条件步级自修正（Conditional Step-level Self-refinement）。在此基础上，将其与经典的并行缩放方法在步级层面结合，提出了一种新颖的推理范式——混合测试时缩放。", "result": "在五个不同规模（3B-14B）和系列的指令微调LLMs上进行了大量实验，结果表明，在细粒度上结合各种免训练TTS方法的混合策略在扩展LLMs推理性能边界方面具有相当大的潜力。", "conclusion": "混合测试时缩放策略，通过在细粒度上集成多种免训练TTS方法，能够显著提升大型语言模型的推理能力，展现出巨大的应用前景。"}}
{"id": "2507.14932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14932", "abs": "https://arxiv.org/abs/2507.14932", "authors": ["Francisco M. Castro-Macías", "Pablo Morales-Álvarez", "Yunan Wu", "Rafael Molina", "Aggelos K. Katsaggelos"], "title": "Probabilistic smooth attention for deep multiple instance learning in medical imaging", "comment": null, "summary": "The Multiple Instance Learning (MIL) paradigm is attracting plenty of\nattention in medical imaging classification, where labeled data is scarce. MIL\nmethods cast medical images as bags of instances (e.g. patches in whole slide\nimages, or slices in CT scans), and only bag labels are required for training.\nDeep MIL approaches have obtained promising results by aggregating\ninstance-level representations via an attention mechanism to compute the\nbag-level prediction. These methods typically capture both local interactions\namong adjacent instances and global, long-range dependencies through various\nmechanisms. However, they treat attention values deterministically, potentially\noverlooking uncertainty in the contribution of individual instances. In this\nwork we propose a novel probabilistic framework that estimates a probability\ndistribution over the attention values, and accounts for both global and local\ninteractions. In a comprehensive evaluation involving {\\color{review} eleven}\nstate-of-the-art baselines and three medical datasets, we show that our\napproach achieves top predictive performance in different metrics. Moreover,\nthe probabilistic treatment of the attention provides uncertainty maps that are\ninterpretable in terms of illness localization.", "AI": {"tldr": "本文提出一种新颖的概率多示例学习（MIL）框架，通过估计注意力值的概率分布来处理医疗图像分类中的不确定性，并在多个医疗数据集上实现了卓越的性能和可解释性。", "motivation": "现有的深度多示例学习（MIL）方法在聚合实例级表示时，通常确定性地处理注意力值，可能忽略了单个实例贡献的不确定性。这促使研究者探索一种能考虑这种不确定性的方法。", "method": "本文提出了一种新颖的概率框架，该框架估计注意力值的概率分布，并同时考虑全局和局部交互。这与传统方法中确定性地处理注意力值不同。", "result": "在包含十一个最先进基线模型和三个医疗数据集的综合评估中，所提出的方法在不同指标上均取得了顶级的预测性能。此外，对注意力的概率处理提供了可解释的不确定性图，有助于疾病定位。", "conclusion": "所提出的概率多示例学习框架不仅在医疗图像分类中实现了卓越的预测性能，而且通过提供可解释的不确定性图，增强了疾病定位的能力，解决了传统方法中确定性注意力处理的局限性。"}}
{"id": "2507.15557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15557", "abs": "https://arxiv.org/abs/2507.15557", "authors": ["Vitaly Protasov", "Nikolay Babakov", "Daryna Dementieva", "Alexander Panchenko"], "title": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification", "comment": "preprint", "summary": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case.", "AI": {"tldr": "本文首次对多语言文本去毒化系统的评估进行了全面研究，比较了神经模型和LLM-as-a-judge方法，并提出了可靠的多语言文本风格迁移（TST）评估方案。", "motivation": "尽管大型语言模型（LLMs）取得了进展，但文本生成任务（如文本风格迁移）的评估仍是挑战。现有研究发现自动指标与人类判断之间存在显著差距，且大多仅限于英语，缺乏对多语言TST评估的探索。", "method": "本文进行了首次全面的多语言文本去毒化系统评估研究，涵盖九种语言（英语、西班牙语、德语、中文、阿拉伯语、印地语、乌克兰语、俄语、阿姆哈拉语）。评估方法借鉴机器翻译领域，结合了现代神经评估模型和基于提示的“LLM-as-a-judge”方法。", "result": "研究结果为设计更可靠的多语言文本风格迁移评估流程（以文本去毒化为例）提供了实用的指导方案。", "conclusion": "本研究为多语言文本去毒化评估提供了实践指导，有助于缩小自动评估与人类判断之间的差距，并推动多语言TST评估领域的发展。"}}
{"id": "2507.14935", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14935", "abs": "https://arxiv.org/abs/2507.14935", "authors": ["Hai Huang", "Yan Xia", "Shulei Wang", "Hanting Wang", "Minghui Fang", "Shengpeng Ji", "Sashuai Zhou", "Tao Jin", "Zhou Zhao"], "title": "Open-set Cross Modal Generalization via Multimodal Unified Representation", "comment": "Accepted by ICCV 2025", "summary": "This paper extends Cross Modal Generalization (CMG) to open-set environments\nby proposing the more challenging Open-set Cross Modal Generalization (OSCMG)\ntask. This task evaluates multimodal unified representations in open-set\nconditions, addressing the limitations of prior closed-set cross-modal\nevaluations. OSCMG requires not only cross-modal knowledge transfer but also\nrobust generalization to unseen classes within new modalities, a scenario\nfrequently encountered in real-world applications. Existing multimodal unified\nrepresentation work lacks consideration for open-set environments. To tackle\nthis, we propose MICU, comprising two key components: Fine-Coarse Masked\nmultimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI\nenhances multimodal alignment by applying contrastive learning at both holistic\nsemantic and temporal levels, incorporating masking to enhance generalization.\nCUJP enhances feature diversity and model uncertainty by integrating\nmodality-agnostic feature selection with self-supervised learning, thereby\nstrengthening the model's ability to handle unknown categories in open-set\ntasks. Extensive experiments on CMG and the newly proposed OSCMG validate the\neffectiveness of our approach. The code is available at\nhttps://github.com/haihuangcode/CMG.", "AI": {"tldr": "该论文提出了开放集跨模态泛化（OSCMG）任务，以应对现有跨模态泛化（CMG）在封闭集环境下的局限性，并提出了MICU模型来解决OSCMG问题。", "motivation": "现有的跨模态统一表示研究主要集中在封闭集评估，缺乏对开放集环境的考虑。然而，实际应用中经常需要模型对新模态中的未见类别进行鲁棒泛化，这促使作者提出了更具挑战性的开放集跨模态泛化（OSCMG）任务。", "method": "该论文提出了MICU模型，包含两个主要组件：1) Fine-Coarse Masked multimodal InfoNCE (FCMI)，通过在整体语义和时间层面应用对比学习和掩码来增强多模态对齐和泛化能力。2) Cross modal Unified Jigsaw Puzzles (CUJP)，通过结合模态无关的特征选择和自监督学习，增强特征多样性和模型不确定性，从而提升模型处理开放集任务中未知类别的能力。", "result": "在CMG和新提出的OSCMG任务上的大量实验验证了所提方法的有效性。", "conclusion": "该论文成功地将跨模态泛化扩展到开放集环境，并提出了MICU模型有效解决了开放集跨模态泛化问题，显著提升了模型在处理新模态中未见类别时的泛化能力。"}}
{"id": "2507.15064", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15064", "abs": "https://arxiv.org/abs/2507.15064", "authors": ["Shuyuan Tu", "Zhen Xing", "Xintong Han", "Zhi-Qi Cheng", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation", "comment": "arXiv admin note: substantial text overlap with arXiv:2411.17697", "summary": "Current diffusion models for human image animation often struggle to maintain\nidentity (ID) consistency, especially when the reference image and driving\nvideo differ significantly in body size or position. We introduce\nStableAnimator++, the first ID-preserving video diffusion framework with\nlearnable pose alignment, capable of generating high-quality videos conditioned\non a reference image and a pose sequence without any post-processing. Building\nupon a video diffusion model, StableAnimator++ contains carefully designed\nmodules for both training and inference, striving for identity consistency. In\nparticular, StableAnimator++ first uses learnable layers to predict the\nsimilarity transformation matrices between the reference image and the driven\nposes via injecting guidance from Singular Value Decomposition (SVD). These\nmatrices align the driven poses with the reference image, mitigating\nmisalignment to a great extent. StableAnimator++ then computes image and face\nembeddings using off-the-shelf encoders, refining the face embeddings via a\nglobal content-aware Face Encoder. To further maintain ID, we introduce a\ndistribution-aware ID Adapter that counteracts interference caused by temporal\nlayers while preserving ID via distribution alignment. During the inference\nstage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization\nintegrated into the denoising process, guiding the diffusion trajectory for\nenhanced facial fidelity. Experiments on benchmarks show the effectiveness of\nStableAnimator++ both qualitatively and quantitatively.", "AI": {"tldr": "StableAnimator++是一个创新的视频扩散框架，通过可学习的姿态对齐和身份保留机制，解决了现有扩散模型在人体图像动画中身份一致性差的问题，尤其是在参考图像和驱动视频姿态差异较大时，能生成高质量、高保真度的动画。", "motivation": "当前用于人体图像动画的扩散模型在保持身份（ID）一致性方面存在困难，特别是当参考图像和驱动视频在身体大小或位置上差异显著时。", "method": "StableAnimator++是一个基于视频扩散模型的框架，包含精心设计的训练和推理模块：\n1.  **可学习姿态对齐**：通过可学习层预测参考图像和驱动姿态之间的相似变换矩阵，并注入奇异值分解（SVD）引导，以校准姿态，减少错位。\n2.  **身份嵌入与精炼**：使用现成的编码器计算图像和面部嵌入，并通过一个全局内容感知面部编码器精炼面部嵌入。\n3.  **分布感知ID适配器**：引入一个分布感知ID适配器，通过分布对齐来抵消时间层造成的干扰，从而保持身份。\n4.  **推理阶段面部优化**：在去噪过程中集成了一种新颖的基于Hamilton-Jacobi-Bellman (HJB) 的面部优化方法，以引导扩散轨迹，增强面部保真度。", "result": "在基准测试上的实验表明，StableAnimator++在定性和定量方面都表现出有效性，能够生成高质量且保持身份一致性的视频。", "conclusion": "StableAnimator++成功解决了人体图像动画中身份一致性的挑战，通过引入可学习的姿态对齐、增强的身份保留机制以及推理阶段的面部优化，实现了在参考图像和驱动视频姿态差异大时，生成高保真、身份一致的动画视频。"}}
{"id": "2507.15576", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15576", "abs": "https://arxiv.org/abs/2507.15576", "authors": ["Nicolas Poggi", "Shashank Agnihotri", "Margret Keuper"], "title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging", "comment": null, "summary": "Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}.", "AI": {"tldr": "该研究首次将上下文学习（ICL）与视觉-语言模型（VLMs）应用于太赫兹（THz）图像分类，通过模态对齐提示框架，在零样本和单样本设置下，无需微调即提升了低数据量下的分类性能和可解释性。", "motivation": "太赫兹成像在安全检查和材料分类等领域具有应用潜力，但其图像分类面临标注数据有限、分辨率低和视觉模糊等挑战，导致传统方法难以有效分类。", "method": "引入了基于上下文学习（ICL）的视觉-语言模型（VLMs）作为一种灵活、可解释且无需微调的替代方案。通过一个模态对齐的提示框架，将两个开源VLMs应用于太赫兹领域，并在零样本和单样本设置下进行了评估。", "result": "实验结果表明，上下文学习（ICL）在低数据量条件下显著提升了太赫兹图像的分类性能和可解释性。", "conclusion": "这是首次将ICL增强的VLMs应用于太赫兹成像领域，为资源受限的科学领域提供了一个有前景的方向。"}}
{"id": "2507.14959", "categories": ["cs.CV", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.14959", "abs": "https://arxiv.org/abs/2507.14959", "authors": ["Saeid Ghafouri", "Mohsen Fayyaz", "Xiangchen Li", "Deepu John", "Bo Ji", "Dimitrios Nikolopoulos", "Hans Vandierendonck"], "title": "Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices", "comment": null, "summary": "Real-time multi-label video classification on embedded devices is constrained\nby limited compute and energy budgets. Yet, video streams exhibit structural\nproperties such as label sparsity, temporal continuity, and label co-occurrence\nthat can be leveraged for more efficient inference. We introduce Polymorph, a\ncontext-aware framework that activates a minimal set of lightweight Low Rank\nAdapters (LoRA) per frame. Each adapter specializes in a subset of classes\nderived from co-occurrence patterns and is implemented as a LoRA weight over a\nshared backbone. At runtime, Polymorph dynamically selects and composes only\nthe adapters needed to cover the active labels, avoiding full-model switching\nand weight merging. This modular strategy improves scalability while reducing\nlatency and energy overhead. Polymorph achieves 40% lower energy consumption\nand improves mAP by 9 points over strong baselines on the TAO dataset.\nPolymorph is open source at https://github.com/inference-serving/polymorph/.", "AI": {"tldr": "Polymorph是一种针对嵌入式设备实时多标签视频分类的上下文感知框架，通过动态激活最小集合的轻量级低秩适配器（LoRA），显著降低能耗并提高准确性。", "motivation": "在嵌入式设备上进行实时多标签视频分类面临计算和能耗限制，但视频流具有标签稀疏性、时间连续性和标签共现等结构特性，可用于提高推理效率。", "method": "Polymorph框架利用共享骨干网络上的LoRA权重实现适配器，每个适配器专注于基于共现模式的特定类别子集。在运行时，Polymorph动态选择并组合仅覆盖活动标签所需的适配器，避免了完整的模型切换和权重合并。", "result": "在TAO数据集上，Polymorph比强基线模型降低了40%的能耗，并提高了9个mAP点。", "conclusion": "Polymorph的模块化策略通过利用视频流的结构特性和动态适配器选择，提高了可扩展性，同时降低了实时多标签视频分类的延迟和能耗开销。"}}
{"id": "2507.15094", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15094", "abs": "https://arxiv.org/abs/2507.15094", "authors": ["Mengya Xu", "Rulin Zhou", "An Wang", "Chaoyang Lyu", "Zhen Li", "Ning Zhong", "Hongliang Ren"], "title": "BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking", "comment": "27 pages, 14 figures", "summary": "Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses\nsignificant risks, demanding precise, real-time localization and continuous\nmonitoring of the bleeding source for effective hemostatic intervention. In\nparticular, endoscopists have to repeatedly flush to clear blood, allowing only\nmilliseconds to identify bleeding sources, an inefficient process that prolongs\noperations and elevates patient risks. However, current Artificial Intelligence\n(AI) methods primarily focus on bleeding region segmentation, overlooking the\ncritical need for accurate bleeding source detection and temporal tracking in\nthe challenging ESD environment, which is marked by frequent visual\nobstructions and dynamic scene changes. This gap is widened by the lack of\nspecialized datasets, hindering the development of robust AI-assisted guidance\nsystems. To address these challenges, we introduce BleedOrigin-Bench, the first\ncomprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated\nbleeding sources across 106,222 frames from 44 procedures, supplemented with\n39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6\nchallenging clinical scenarios. We also present BleedOrigin-Net, a novel\ndual-stage detection-tracking framework for the bleeding source localization in\nESD procedures, addressing the complete workflow from bleeding onset detection\nto continuous spatial tracking. We compare with widely-used object detection\nmodels (YOLOv11/v12), multimodal large language models, and point tracking\nmethods. Extensive evaluation demonstrates state-of-the-art performance,\nachieving 96.85% frame-level accuracy ($\\pm\\leq8$ frames) for bleeding onset\ndetection, 70.24% pixel-level accuracy ($\\leq100$ px) for initial source\ndetection, and 96.11% pixel-level accuracy ($\\leq100$ px) for point tracking.", "AI": {"tldr": "该研究引入了首个内镜黏膜下剥离术（ESD）出血源数据集BleedOrigin-Bench和一种名为BleedOrigin-Net的双阶段检测-跟踪框架，旨在实现ESD术中出血源的实时精准定位和持续跟踪，以解决现有方法的不足和数据稀缺问题。", "motivation": "ESD术中出血风险高，需要实时、精确地定位和持续监测出血源。传统方法效率低下，延长手术时间并增加患者风险。现有AI方法主要关注出血区域分割，忽略了对出血源的准确检测和时间跟踪，且缺乏专用的数据集。", "method": "研究构建了首个全面的ESD出血源数据集BleedOrigin-Bench，包含106,222帧图像（来自44例手术，含1,771个专家标注的出血源）和39,755帧伪标注数据，覆盖8个解剖部位和6种临床场景。同时，提出了BleedOrigin-Net，一个新颖的双阶段检测-跟踪框架，用于出血源的定位，涵盖从出血起始检测到连续空间跟踪的完整工作流程。该方法与YOLOv11/v12、多模态大型语言模型和点跟踪方法进行了比较。", "result": "BleedOrigin-Net在出血起始检测方面达到了96.85%的帧级准确率（误差≤8帧），在初始出血源检测方面达到了70.24%的像素级准确率（误差≤100像素），在点跟踪方面达到了96.11%的像素级准确率（误差≤100像素），表现出最先进的性能。", "conclusion": "该研究通过构建专业数据集和开发高性能AI系统，有效解决了ESD术中出血源检测和跟踪的关键挑战，有望显著提高手术效率和患者安全性。"}}
{"id": "2507.15586", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15586", "abs": "https://arxiv.org/abs/2507.15586", "authors": ["Xinping Zhao", "Shouzheng Huang", "Yan Zhong", "Xinshuo Hu", "Baotian Hu", "Min Zhang"], "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation", "comment": "16 pages, 7 Figures, 10 Tables", "summary": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.", "AI": {"tldr": "本文提出LEAR方法，通过显式推理和有意识提取来获取高质量证据，以解决检索增强生成（RAG）中检索噪声影响大语言模型（LLM）生成质量的问题，从而提升下游任务的准确性。", "motivation": "检索增强生成（RAG）虽然能有效提高大语言模型（LLM）的准确性，但检索噪声会显著影响LLM的生成质量。现有方法直接提取证据，缺乏显式思考，可能遗漏关键线索且泛化能力差。", "method": "本文提出LEAR（Learning to Extract Rational Evidence）方法。该方法首先通过显式推理识别检索内容中的潜在线索，然后有意识地提取关键证据。具体实现上，将证据推理和证据提取整合为一个统一的响应进行端到端训练；应用知识token掩码进行解耦，以得到基于推理和基于提取的答案；并设计了三种可验证的奖励函数（答案、长度、格式），通过策略优化算法更新模型。", "result": "在三个基准数据集上的大量实验表明，LEAR方法能提供紧凑且高质量的证据，显著提高了下游任务的准确性，并促进了其在在线RAG系统中的有效应用。", "conclusion": "LEAR通过其独特的显式推理和有意识证据提取机制，有效解决了RAG系统中的检索噪声问题，为LLM提供了高质量的输入证据，从而显著提升了生成准确性，具有良好的实用性。"}}
{"id": "2507.14965", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14965", "abs": "https://arxiv.org/abs/2507.14965", "authors": ["Yaojie Zhang", "Tianlun Huang", "Weijun Wang", "Wei Feng"], "title": "Decision PCR: Decision version of the Point Cloud Registration task", "comment": null, "summary": "Low-overlap point cloud registration (PCR) remains a significant challenge in\n3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become\nineffective under extremely low inlier ratios. In this paper, we revisit the\nregistration result evaluation problem and identify the Decision version of the\nPCR task as the fundamental problem. To address this Decision PCR task, we\npropose a data-driven approach. First, we construct a corresponding dataset\nbased on the 3DMatch dataset. Then, a deep learning-based classifier is trained\nto reliably assess registration quality, overcoming the limitations of\ntraditional metrics. To our knowledge, this is the first comprehensive study to\naddress this task through a deep learning framework. We incorporate this\nclassifier into standard PCR pipelines. When integrated with our approach,\nexisting state-of-the-art PCR methods exhibit significantly enhanced\nregistration performance. For example, combining our framework with\nGeoTransformer achieves a new SOTA registration recall of 86.97\\% on the\nchallenging 3DLoMatch benchmark. Our method also demonstrates strong\ngeneralization capabilities on the unseen outdoor ETH dataset.", "AI": {"tldr": "本文提出了一种基于深度学习的分类器，用于评估低重叠点云配准的质量，克服了传统度量在极低内点率下的局限性，并显著提升了现有最先进配准方法的性能。", "motivation": "在极低内点率下，传统的点云配准（PCR）评估指标（如最大内点数）变得无效，这使得低重叠点云配准成为一个重大挑战。本研究旨在解决“决策版本PCR任务”这一根本问题。", "method": "该研究采用数据驱动的方法：首先，基于3DMatch数据集构建了一个相应的数据集；然后，训练了一个基于深度学习的分类器，以可靠地评估配准质量；最后，将此分类器整合到标准的PCR流程中，以提升配准性能。", "result": "据作者所知，这是首次通过深度学习框架全面解决低重叠点云配准评估任务的研究。与现有最先进的PCR方法结合时，显著增强了配准性能。例如，与GeoTransformer结合后，在挑战性的3DLoMatch基准测试上实现了86.97%的SOTA配准召回率。该方法还在未见的室外ETH数据集上展示了强大的泛化能力。", "conclusion": "该研究提出的深度学习分类器能够有效评估和提升低重叠点云配准的性能，克服了传统度量的局限性，并使现有SOTA方法取得了显著的性能提升和良好的泛化能力。"}}
{"id": "2507.15243", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15243", "abs": "https://arxiv.org/abs/2507.15243", "authors": ["Naeem Paeedeh", "Mahardhika Pratama", "Wolfgang Mayer", "Jimmy Cao", "Ryszard Kowlczyk"], "title": "Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation", "comment": null, "summary": "Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model\npre-trained with DINO combined with a prototypical classifier outperforms the\nlatest SOTA methods. A crucial limitation that needs to be overcome is that\nupdating too many parameters of the transformers leads to overfitting due to\nthe scarcity of labeled samples. To address this challenge, we propose a new\nconcept, Coalescent Projection (CP), as an effective successor to soft prompts.\nAdditionally, we propose a novel pseudo-class generation method combined with\nSelf-Supervised Transformations (SSTs) that relies solely on the base domain to\nprepare the network for encountering unseen samples from different domains. The\nproposed method exhibits its effectiveness in comprehensive experiments on the\nextreme domain shift scenario of the BSCD-FSL benchmark. Our code is published\nat https://github.com/Naeem-Paeedeh/CPLSR.", "AI": {"tldr": "本文提出Coalescent Projection (CP) 和结合自监督变换(SSTs)的伪类生成方法，以解决跨域少样本学习(CD-FSL)中因参数更新过多导致的过拟合问题，并有效应对极端域偏移。", "motivation": "现有CD-FSL方法在更新过多Transformer参数时易因标签样本稀缺而过拟合，且需要有效准备网络以应对来自不同域的未见样本。", "method": "提出Coalescent Projection (CP) 作为soft prompts的有效替代；结合自监督变换(SSTs)提出一种新颖的伪类生成方法，该方法仅依赖于基础域来准备网络应对未见样本。", "result": "所提出的方法在BSCD-FSL基准的极端域偏移场景中表现出其有效性，甚至超越了预训练DINO模型结合原型分类器的性能。", "conclusion": "Coalescent Projection (CP) 和结合自监督变换(SSTs)的伪类生成方法能有效解决CD-FSL中的过拟合和域泛化挑战，尤其在极端域偏移情况下表现出色。"}}
{"id": "2507.15600", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15600", "abs": "https://arxiv.org/abs/2507.15600", "authors": ["Armin Pournaki"], "title": "Conflicting narratives and polarization on social media", "comment": "30 pages, 7 figures", "summary": "Narratives are key interpretative devices by which humans make sense of\npolitical reality. In this work, we show how the analysis of conflicting\nnarratives, i.e. conflicting interpretive lenses through which political\nreality is experienced and told, provides insight into the discursive\nmechanisms of polarization and issue alignment in the public sphere. Building\nupon previous work that has identified ideologically polarized issues in the\nGerman Twittersphere between 2021 and 2023, we analyze the discursive dimension\nof polarization by extracting textual signals of conflicting narratives from\ntweets of opposing opinion groups. Focusing on a selection of salient issues\nand events (the war in Ukraine, Covid, climate change), we show evidence for\nconflicting narratives along two dimensions: (i) different attributions of\nactantial roles to the same set of actants (e.g. diverging interpretations of\nthe role of NATO in the war in Ukraine), and (ii) emplotment of different\nactants for the same event (e.g. Bill Gates in the right-leaning Covid\nnarrative). Furthermore, we provide first evidence for patterns of narrative\nalignment, a discursive strategy that political actors employ to align opinions\nacross issues. These findings demonstrate the use of narratives as an\nanalytical lens into the discursive mechanisms of polarization.", "AI": {"tldr": "该研究通过分析德国Twitter上不同政治群体在乌克兰战争、新冠和气候变化等议题上的冲突叙事，揭示了政治极化和议题对齐的言语机制。", "motivation": "叙事是人类理解政治现实的关键解释工具。研究旨在展示冲突叙事（即对政治现实的不同解释视角）如何深入揭示公共领域中极化和议题对齐的言语机制。", "method": "该研究基于先前已识别的2021-2023年德国Twitter意识形态极化议题，从对立观点群体的推文中提取冲突叙事的文本信号。研究聚焦于乌克兰战争、新冠和气候变化等突出议题，从两个维度分析冲突叙事：(i) 对同一行动者（actants）的不同角色归因；(ii) 对同一事件的不同行动者（actants）的叙事嵌入。", "result": "研究发现了冲突叙事在两个维度上的证据：对同一行动者（如北约在乌克兰战争中的角色）的不同角色归因，以及对同一事件（如右翼新冠叙事中的比尔·盖茨）的不同行动者嵌入。此外，研究首次提供了叙事对齐模式的证据，这是一种政治行动者用于跨议题对齐观点的言语策略。", "conclusion": "这些发现表明，叙事可以作为一种分析视角，深入理解政治极化的言语机制。"}}
{"id": "2507.14976", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14976", "abs": "https://arxiv.org/abs/2507.14976", "authors": ["Hao Zheng", "Shunzhi Yang", "Zhuoxin He", "Jinfeng Yang", "Zhenhua Huang"], "title": "Hierarchical Cross-modal Prompt Learning for Vision-Language Models", "comment": "Accepted by ICCV2025", "summary": "Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent\ngeneralization abilities. However, adapting these large-scale models to\ndownstream tasks while preserving their generalization capabilities remains\nchallenging. Although prompt learning methods have shown promise, they suffer\nfrom two fundamental bottlenecks that limit generalization: (a) modality\nisolation, and (b) hierarchical semantic decay. To address these limitations,\nwe propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that\nestablishes bidirectional knowledge flow between text and vision modalities,\nenabling them to refine their semantics mutually. HiCroPL routes knowledge\nflows by leveraging the complementary strengths of text and vision. In early\nlayers, text prompts inject relatively clear semantics into visual prompts\nthrough a hierarchical knowledge mapper, enhancing the representation of\nlow-level visual semantics. In later layers, visual prompts encoding specific\ntask-relevant objects flow back to refine text prompts, enabling deeper\nalignment. Crucially, our hierarchical knowledge mapper allows representations\nat multi-scales to be fused, ensuring that deeper representations retain\ntransferable shallow semantics thereby enhancing generalization. We further\nintroduce a lightweight layer-specific knowledge proxy to enable efficient\ncross-modal interactions. Extensive evaluations across four tasks demonstrate\nHiCroPL's superior performance, achieving state-of-the-art results on 11\nbenchmarks with significant improvements. Code is available at:\nhttps://github.com/zzeoZheng/HiCroPL.", "AI": {"tldr": "本文提出HiCroPL，一种分层跨模态提示学习框架，通过建立文本与视觉之间的双向知识流，解决了预训练视觉-语言模型在下游任务适应中存在的模态隔离和层级语义衰减问题，显著提升了泛化能力并取得了最先进的性能。", "motivation": "预训练视觉-语言模型（VLMs）如CLIP具有出色的泛化能力，但将其适应到下游任务时，在保留泛化能力方面仍面临挑战。现有的提示学习方法存在两个基本瓶颈：模态隔离和层级语义衰减，限制了其泛化能力。", "method": "本文提出了HiCroPL，一个分层跨模态提示学习框架。它通过以下方式解决问题：1) 建立文本和视觉模态之间的双向知识流，实现语义的相互精炼；2) 在早期层，文本提示通过分层知识映射器向视觉提示注入清晰语义，增强低级视觉语义表示；3) 在后期层，编码特定任务相关对象的视觉提示反向流回以精炼文本提示，实现更深层次的对齐；4) 分层知识映射器融合多尺度表示，确保深层表示保留可迁移的浅层语义，从而增强泛化；5) 引入轻量级层特定知识代理，实现高效的跨模态交互。", "result": "HiCroPL在四项任务上进行了广泛评估，展现出卓越的性能，在11个基准测试中取得了显著提升的最先进结果。", "conclusion": "HiCroPL通过其创新的分层跨模态提示学习框架，成功解决了现有提示学习方法在适应大型预训练视觉-语言模型时面临的模态隔离和层级语义衰减问题，显著提升了模型的泛化能力，并在多项任务上达到了最先进的性能。"}}
{"id": "2507.15269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15269", "abs": "https://arxiv.org/abs/2507.15269", "authors": ["Fangqiu Yi", "Jingyu Xu", "Jiawei Shao", "Chi Zhang", "Xuelong Li"], "title": "Conditional Video Generation for High-Efficiency Video Compression", "comment": null, "summary": "Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.", "AI": {"tldr": "该论文提出了一种基于条件扩散模型的视频压缩框架，通过将视频压缩重构视为条件生成任务，显著提升了感知质量，尤其在高压缩比下表现优异。", "motivation": "感知研究表明，条件扩散模型在重建与人类视觉感知一致的视频内容方面表现出色，这激励了研究者将其应用于视频压缩以优化感知质量。", "method": "该方法将视频压缩重构重构为条件生成任务，其中生成模型从稀疏但信息丰富的信号中合成视频。它包含三个关键模块：1) 多粒度条件化，捕捉静态场景结构和动态时空线索；2) 紧凑表示，用于高效传输且不牺牲语义丰富性；3) 多条件训练，结合模态丢弃和角色感知嵌入，以防止过度依赖单一模态并增强鲁棒性。", "result": "大量实验表明，该方法在感知质量指标（如Fréchet Video Distance (FVD) 和 LPIPS）上显著优于传统和神经编解码器，特别是在高压缩比下表现突出。", "conclusion": "该研究证明了利用条件扩散模型进行感知优化视频压缩的有效性，通过创新的条件生成框架实现了卓越的感知重建质量。"}}
{"id": "2507.15641", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15641", "abs": "https://arxiv.org/abs/2507.15641", "authors": ["Alessio Pittiglio"], "title": "Leveraging Context for Multimodal Fallacy Classification in Political Debates", "comment": "12th Workshop on Argument Mining (ArgMining 2025) @ ACL 2025", "summary": "In this paper, we present our submission to the MM-ArgFallacy2025 shared\ntask, which aims to advance research in multimodal argument mining, focusing on\nlogical fallacies in political debates. Our approach uses pretrained\nTransformer-based models and proposes several ways to leverage context. In the\nfallacy classification subtask, our models achieved macro F1-scores of 0.4444\n(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed\nperformance comparable to the text-only model, suggesting potential for\nimprovements.", "AI": {"tldr": "本文提交了MM-ArgFallacy2025共享任务的成果，旨在使用预训练Transformer模型在政治辩论中进行多模态论证挖掘和逻辑谬误分类，并报告了文本、音频和多模态模型的F1分数。", "motivation": "推动多模态论证挖掘领域的研究，特别是识别政治辩论中的逻辑谬误，并参与MM-ArgFallacy2025共享任务。", "method": "采用预训练的Transformer模型，并提出多种利用上下文的方法，应用于谬误分类子任务。", "result": "在谬误分类子任务中，模型获得了宏观F1分数：文本0.4444，音频0.3559，多模态0.4403。多模态模型的表现与纯文本模型相当。", "conclusion": "多模态模型表现与文本模型相当，表明该领域仍有进一步改进的潜力。"}}
{"id": "2507.14997", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14997", "abs": "https://arxiv.org/abs/2507.14997", "authors": ["Roy H. Jennings", "Genady Paikin", "Roy Shaul", "Evgeny Soloveichik"], "title": "Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show promise for image-based\nregression tasks, but current approaches face key limitations. Recent methods\nfine-tune MLLMs using preset output vocabularies and generic task-level prompts\n(e.g., \"How would you rate this image?\"), assuming this mimics human rating\nbehavior. Our analysis reveals these approaches provide no benefit over\nimage-only training. Models using preset vocabularies and generic prompts\nperform equivalently to image-only models, failing to leverage semantic\nunderstanding from textual input. We propose Regression via Transformer-Based\nClassification (RvTC), which replaces vocabulary-constrained classification\nwith a flexible bin-based approach. Unlike approaches that address\ndiscretization errors through complex distributional modeling, RvTC eliminates\nmanual vocabulary crafting through straightforward bin increase, achieving\nstate-of-the-art performance on four image assessment datasets using only\nimages. More importantly, we demonstrate that data-specific prompts\ndramatically improve performance. Unlike generic task descriptions, prompts\ncontaining semantic information about specific images enable MLLMs to leverage\ncross-modal understanding. On the AVA dataset, adding challenge titles to\nprompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We\ndemonstrate through empirical evidence from the AVA and AGIQA-3k datasets that\nMLLMs benefit from semantic prompt information surpassing mere statistical\nbiases. This underscores the importance of incorporating meaningful textual\ncontext in multimodal regression tasks.", "AI": {"tldr": "当前多模态大语言模型（MLLMs）在图像回归任务中，预设词汇和通用提示效果不佳。本文提出RvTC方法，采用灵活的基于bin的分类，并强调数据特定语义提示的重要性，显著提升了性能，证明了语义文本上下文在多模态回归中的关键作用。", "motivation": "当前MLLMs在图像回归任务中，通过预设输出词汇和通用任务级提示进行微调，但这种方法未能有效利用文本输入的语义理解，其表现与仅图像训练的模型相当，未能发挥多模态优势。", "method": "本文提出了基于Transformer的回归分类（RvTC）方法，用灵活的基于bin的方法取代了受词汇限制的分类，通过简单增加bin的数量来消除手动词汇构建。更重要的是，该研究强调使用包含特定图像语义信息的数据特定提示，而非通用任务描述，以使MLLMs能够利用跨模态理解。", "result": "研究发现，当前使用预设词汇和通用提示的MLLM方法在图像回归任务中相对于仅图像训练无优势。RvTC在仅使用图像的情况下，在四个图像评估数据集上实现了最先进的性能。数据特定提示显著提升了性能，例如在AVA数据集上，添加挑战标题将相关性从0.83提高到0.90，达到了新的最先进水平。经验证据表明，MLLMs受益于语义提示信息，超越了单纯的统计偏差。", "conclusion": "在多模态回归任务中，融入有意义的文本上下文（即数据特定语义提示）至关重要。这使得MLLMs能够有效利用跨模态理解，从而显著提高性能。"}}
{"id": "2507.15335", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15335", "abs": "https://arxiv.org/abs/2507.15335", "authors": ["Muhammad Aqeel", "Federico Leonardi", "Francesco Setti"], "title": "ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis", "comment": "Accepted to ICIAP 2025", "summary": "Industrial defect detection systems face critical limitations when confined\nto one-class anomaly detection paradigms, which assume uniform outlier\ndistributions and struggle with data scarcity in realworld manufacturing\nenvironments. We present ExDD (Explicit Dual Distribution), a novel framework\nthat transcends these limitations by explicitly modeling dual feature\ndistributions. Our approach leverages parallel memory banks that capture the\ndistinct statistical properties of both normality and anomalous patterns,\naddressing the fundamental flaw of uniform outlier assumptions. To overcome\ndata scarcity, we employ latent diffusion models with domain-specific textual\nconditioning, generating in-distribution synthetic defects that preserve\nindustrial context. Our neighborhood-aware ratio scoring mechanism elegantly\nfuses complementary distance metrics, amplifying signals in regions exhibiting\nboth deviation from normality and similarity to known defect patterns.\nExperimental validation on KSDD2 demonstrates superior performance (94.2%\nI-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.", "AI": {"tldr": "ExDD是一种新颖的工业缺陷检测框架，通过显式建模正常和异常的双重特征分布，并利用扩散模型生成合成缺陷数据以解决数据稀缺问题，从而显著提升检测性能。", "motivation": "现有的工业缺陷检测系统（单类别异常检测）存在关键局限性：它们假设异常值分布是均匀的，并且在真实制造环境中面临数据稀缺的挑战。", "method": "ExDD框架通过以下方式运作：1) 显式建模双重特征分布。2) 利用并行记忆库捕获正常和异常模式的独特统计特性。3) 采用带有领域特定文本条件的潜在扩散模型生成符合工业背景的合成缺陷数据。4) 使用邻域感知比率评分机制融合互补的距离度量，以增强偏离正常和相似于已知缺陷模式的信号。", "result": "在KSDD2数据集上的实验验证表明，ExDD实现了卓越的性能（I-AUROC 94.2%，P-AUROC 97.7%），并且在生成100个合成样本时达到了最佳增强效果。", "conclusion": "ExDD通过显式建模双重分布并有效解决数据稀缺问题，成功克服了传统单类别异常检测的局限性，显著提升了工业缺陷检测的性能。"}}
{"id": "2507.15675", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15675", "abs": "https://arxiv.org/abs/2507.15675", "authors": ["Xinyu Zhang", "Yuanquan Hu", "Fangchao Liu", "Zhicheng Dou"], "title": "P3: Prompts Promote Prompting", "comment": "Accepted to ACL 2025 findings", "summary": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.", "AI": {"tldr": "P3是一种新颖的自改进框架，通过迭代过程同时优化大型语言模型（LLM）的系统提示和用户提示，以实现卓越的性能。", "motivation": "当前的LLM应用常使用多组件提示（系统和用户提示），但现有的自动化优化方法通常只优化其中一个，由于组件间的相互依赖性，这种单边方法效果不佳，导致次优结果。", "method": "本文提出了P3框架，通过迭代过程同时优化系统提示和用户提示。此外，离线优化的提示还会被用于在线提示，通过执行依赖于查询的提示优化来进一步提升效果。", "result": "在通用任务（如Arena-hard和Alpaca-eval）和推理任务（如GSM8K和GPQA）上的广泛实验表明，P3在自动化提示优化领域取得了卓越的性能。", "conclusion": "研究结果强调了整体优化策略在提升LLM跨领域性能方面的有效性。"}}
{"id": "2507.15000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15000", "abs": "https://arxiv.org/abs/2507.15000", "authors": ["Chaoyun Wang", "I-Chao Shen", "Takeo Igarashi", "Nanning Zheng", "Caigui Jiang"], "title": "Axis-Aligned Document Dewarping", "comment": null, "summary": "Document dewarping is crucial for many applications. However, existing\nlearning-based methods primarily rely on supervised regression with annotated\ndata without leveraging the inherent geometric properties in physical documents\nto the dewarping process. Our key insight is that a well-dewarped document is\ncharacterized by transforming distorted feature lines into axis-aligned ones.\nThis property aligns with the inherent axis-aligned nature of the discrete grid\ngeometry in planar documents. In the training phase, we propose an axis-aligned\ngeometric constraint to enhance document dewarping. In the inference phase, we\npropose an axis alignment preprocessing strategy to reduce the dewarping\ndifficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned\nDistortion (AAD), that not only incorporates geometric meaning and aligns with\nhuman visual perception but also demonstrates greater robustness. As a result,\nour method achieves SOTA results on multiple existing benchmarks and achieves\n18.2%~34.5% improvements on the AAD metric.", "AI": {"tldr": "该论文提出了一种利用文档固有轴对齐几何特性进行去畸变的方法，通过引入轴对齐几何约束和预处理策略，并在训练和推理阶段应用，同时提出了一种新的评价指标AAD，实现了SOTA性能。", "motivation": "现有基于学习的文档去畸变方法主要依赖有标注数据的监督回归，但未充分利用物理文档固有的几何特性。作者认为，一个良好去畸变的文档其扭曲的特征线应转换为轴对齐的线，这与平面文档中离散网格几何的轴对齐特性一致。", "method": "在训练阶段，提出了一种轴对齐几何约束来增强文档去畸变。在推理阶段，提出了一种轴对齐预处理策略来降低去畸变难度。在评估阶段，引入了一种新的度量标准——轴对齐畸变（AAD），该指标不仅包含几何意义并符合人类视觉感知，而且更具鲁棒性。", "result": "该方法在多个现有基准测试上取得了最先进（SOTA）的结果，并在AAD指标上实现了18.2%~34.5%的改进。", "conclusion": "通过利用文档固有的轴对齐几何特性，该方法显著提升了文档去畸变的效果，并且提出的AAD指标能更准确地评估去畸变质量，与人类感知更一致。"}}
{"id": "2507.15428", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15428", "abs": "https://arxiv.org/abs/2507.15428", "authors": ["Jiaao Li", "Kaiyuan Li", "Chen Gao", "Yong Li", "Xinlei Chen"], "title": "EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent", "comment": null, "summary": "Egomotion videos are first-person recordings where the view changes\ncontinuously due to the agent's movement. As they serve as the primary visual\ninput for embodied AI agents, making egomotion video reasoning more efficient\nis therefore essential for real-world deployment. Recent advances in\nvision-language models have enabled strong multimodal reasoning capabilities,\nbut their computational cost remains prohibitive for long, redundant video\ninputs. Existing token pruning methods, typically designed for third-person\nvideos, fail to leverage the spatiotemporal continuity and motion constraints\ninherent in egomotion settings. To address this, we propose EgoPrune, a\ntraining-free token pruning method tailored for egomotion video reasoning.\nEgoPrune comprises three components: a keyframe selector adapted from EmbodiedR\nfor temporally efficient sampling; Perspective-Aware Redundancy Filtering\n(PARF), which aligns visual tokens using perspective transformations and\nremoves redundant tokens; and a Maximal Marginal Relevance (MMR)-based token\nselector that jointly considers visual-text relevance and intra-frame\ndiversity. Experiments on two egomotion video benchmarks show that EgoPrune\nconsistently outperforms prior training-free methods across various pruning\nratios while significantly reducing FLOPs, memory usage, and latency. Moreover,\nwe deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB\nedge device, demonstrating its real-world efficiency and suitability for\non-device egomotion video reasoning.", "AI": {"tldr": "EgoPrune是一种为第一视角运动视频（egomotion video）设计的免训练令牌剪枝方法，通过利用视频的时空连续性和运动约束，显著降低了视觉语言模型（VLM）在处理长视频时的计算成本，同时保持或提升了推理性能。", "motivation": "第一视角运动视频是具身AI代理的主要视觉输入，但现有视觉语言模型处理长且冗余的视频输入时计算成本过高，阻碍了实际部署。传统的令牌剪枝方法主要针对第三视角视频，未能充分利用第一视角视频固有的时空连续性和运动约束。", "method": "EgoPrune包含三个组件：1. 一个基于EmbodiedR的“关键帧选择器”，用于时间高效采样；2. “透视感知冗余过滤（PARF）”，通过透视变换对视觉令牌进行对齐并去除冗余；3. 一个基于最大边际相关性（MMR）的令牌选择器，同时考虑视觉-文本相关性和帧内多样性。", "result": "在两个第一视角运动视频基准测试中，EgoPrune在不同剪枝率下持续优于现有免训练方法，并显著降低了FLOPs、内存使用和延迟。此外，在搭载Jetson Orin NX 16GB边缘设备的具身代理上部署，证明了其在设备端第一视角运动视频推理的真实世界效率和适用性。", "conclusion": "EgoPrune是一种高效且实用的第一视角运动视频令牌剪枝方法，有效解决了视觉语言模型在具身AI应用中面临的计算效率挑战，并展现了其在边缘设备上部署的潜力。"}}
{"id": "2507.15698", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15698", "abs": "https://arxiv.org/abs/2507.15698", "authors": ["Congmin Zheng", "Jiachen Zhu", "Jianghao Lin", "Xinyi Dai", "Yong Yu", "Weinan Zhang", "Mengyue Yang"], "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models", "comment": null, "summary": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.", "AI": {"tldr": "本文提出CoLD框架，通过长度惩罚、偏置估计和联合训练，有效解决了过程奖励模型(PRMs)中普遍存在的长度偏置问题，从而提升了LLM数学推理的准确性和简洁性。", "motivation": "过程奖励模型(PRMs)在评估和指导大型语言模型(LLMs)的多步骤推理（尤其是数学问题解决）中至关重要。然而，现有PRMs存在普遍的长度偏置：它们倾向于为较长的推理步骤赋予更高的分数，即使语义内容和逻辑有效性不变。这种偏置损害了奖励预测的可靠性，并导致推理过程中输出过于冗长。", "method": "本文提出了CoLD（Counterfactually-Guided Length Debiasing）框架，旨在通过三个组成部分来缓解长度偏置：1) 显式长度惩罚调整；2) 学习到的偏置估计器，用于捕获虚假的长度相关信号；3) 联合训练策略，强制奖励预测的长度不变性。该方法基于反事实推理并受因果图分析启发。", "result": "在MATH500和GSM-Plus数据集上的广泛实验表明，CoLD持续降低了奖励与长度的相关性，提高了步骤选择的准确性，并鼓励了更简洁、逻辑有效的推理。", "conclusion": "CoLD框架在提高过程奖励模型的忠实度和鲁棒性方面展现出有效性和实用性。"}}
{"id": "2507.15008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15008", "abs": "https://arxiv.org/abs/2507.15008", "authors": ["Jiasheng Xu", "Yewang Chen"], "title": "FastSmoothSAM: A Fast Smooth Method For Segment Anything Model", "comment": null, "summary": "Accurately identifying and representing object edges is a challenging task in\ncomputer vision and image processing. The Segment Anything Model (SAM) has\nsignificantly influenced the field of image segmentation, but suffers from high\nmemory consumption and long inference times, limiting its efficiency in\nreal-time applications. To address these limitations, Fast Segment Anything\n(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM\noften generates jagged edges that deviate from the true object shapes.\nTherefore, this paper introduces a novel refinement approach using B-Spline\ncurve fitting techniques to enhance the edge quality in FastSAM. Leveraging the\nrobust shape control and flexible geometric construction of B-Splines, a\nfour-stage refining process involving two rounds of curve fitting is employed\nto effectively smooth jagged edges. This approach significantly improves the\nvisual quality and analytical accuracy of object edges without compromising\ncritical geometric information. The proposed method improves the practical\nutility of FastSAM by improving segmentation accuracy while maintaining\nreal-time processing capabilities. This advancement unlocks greater potential\nfor FastSAM technology in various real-world scenarios, such as industrial\nautomation, medical imaging, and autonomous systems, where precise and\nefficient edge recognition is crucial.", "AI": {"tldr": "本文提出一种基于B-Spline曲线拟合的新方法，用于优化FastSAM生成的锯齿状边缘，在保持实时性的同时显著提升了分割边缘的质量和准确性。", "motivation": "SAM模型存在内存消耗高、推理时间长的问题，不适用于实时应用。FastSAM解决了实时性问题，但其生成的边缘常呈锯齿状，偏离真实物体形状。因此，需要一种方法来提升FastSAM的边缘质量。", "method": "本研究利用B-Spline曲线拟合技术，设计了一个四阶段的边缘精修过程，包括两轮曲线拟合，以有效地平滑FastSAM生成的锯齿状边缘。该方法利用B-Spline对形状的强大控制和灵活的几何构造能力。", "result": "所提出的方法显著改善了物体边缘的视觉质量和分析准确性，同时没有牺牲关键的几何信息，并保持了FastSAM的实时处理能力。", "conclusion": "该方法通过提高分割精度，增强了FastSAM的实用性，使其在工业自动化、医学成像和自主系统等需要精确高效边缘识别的各种现实场景中具有更大的应用潜力。"}}
{"id": "2507.15577", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15577", "abs": "https://arxiv.org/abs/2507.15577", "authors": ["Hugo Carlesso", "Maria Eliza Patulea", "Moncef Garouani", "Radu Tudor Ionescu", "Josiane Mothe"], "title": "GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation", "comment": null, "summary": "Mixup has become a popular augmentation strategy for image classification,\nyet its naive pixel-wise interpolation often produces unrealistic images that\ncan hinder learning, particularly in high-stakes medical applications. We\npropose GeMix, a two-stage framework that replaces heuristic blending with a\nlearned, label-aware interpolation powered by class-conditional GANs. First, a\nStyleGAN2-ADA generator is trained on the target dataset. During augmentation,\nwe sample two label vectors from Dirichlet priors biased toward different\nclasses and blend them via a Beta-distributed coefficient. Then, we condition\nthe generator on this soft label to synthesize visually coherent images that\nlie along a continuous class manifold. We benchmark GeMix on the large-scale\nCOVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,\nEfficientNet-B0). When combined with real data, our method increases macro-F1\nover traditional mixup for all backbones, reducing the false negative rate for\nCOVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,\ndelivering stronger regularization and greater semantic fidelity, without\ndisrupting existing training pipelines. We publicly release our code at\nhttps://github.com/hugocarlesso/GeMix to foster reproducibility and further\nresearch.", "AI": {"tldr": "GeMix提出一种基于GAN的标签感知图像插值方法，替代传统Mixup，生成更真实的图像，提升医学图像分类性能。", "motivation": "传统的Mixup（像素级插值）在生成图像时常产生不真实的结果，这在医学图像等高风险应用中尤其不利于模型学习。", "method": "GeMix是一个两阶段框架：首先，使用StyleGAN2-ADA在目标数据集上训练一个生成器；其次，在数据增强阶段，从Dirichlet先验中采样两个标签向量并用Beta分布系数混合，然后将这个“软标签”作为条件输入给生成器，以合成视觉连贯的、位于连续类别流形上的图像。", "result": "GeMix在大型COVIDx-CT-3数据集上，结合真实数据，在ResNet-50、ResNet-101和EfficientNet-B0三种骨干网络上均优于传统Mixup，提高了macro-F1分数，并降低了COVID-19检测的假阴性率。", "conclusion": "GeMix是像素空间Mixup的直接替代方案，它能提供更强的正则化和更高的语义保真度，同时不干扰现有的训练流程。"}}
{"id": "2507.15706", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15706", "abs": "https://arxiv.org/abs/2507.15706", "authors": ["David Peter Wallis Freeborn"], "title": "Compositional Understanding in Signaling Games", "comment": null, "summary": "Receivers in standard signaling game models struggle with learning\ncompositional information. Even when the signalers send compositional messages,\nthe receivers do not interpret them compositionally. When information from one\nmessage component is lost or forgotten, the information from other components\nis also erased. In this paper I construct signaling game models in which\ngenuine compositional understanding evolves. I present two new models: a\nminimalist receiver who only learns from the atomic messages of a signal, and a\ngeneralist receiver who learns from all of the available information. These\nmodels are in many ways simpler than previous alternatives, and allow the\nreceivers to learn from the atomic components of messages.", "AI": {"tldr": "现有信号博弈模型中的接收者难以学习组合信息，本文构建了能实现真正组合理解的信号博弈模型。", "motivation": "标准信号博弈模型中的接收者无法组合性地理解信息，即使信号发送者发送组合消息，接收者也无法进行组合解释，导致信息丢失时其他组件的信息也随之丢失。", "method": "构建了两种新的信号博弈模型：一种是只从信号原子消息中学习的“极简主义接收者”模型，另一种是学习所有可用信息的“通才接收者”模型。", "result": "所提出的模型比现有替代方案更简单，并且使接收者能够从消息的原子组件中学习，从而实现了真正的组合理解。", "conclusion": "通过引入新的、更简单的接收者模型，可以解决现有信号博弈模型中接收者难以学习组合信息的问题，使其能够有效学习和理解消息的原子组件。"}}
{"id": "2507.15028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15028", "abs": "https://arxiv.org/abs/2507.15028", "authors": ["Yuanhan Zhang", "Yunice Chew", "Yuhao Dong", "Aria Leo", "Bo Hu", "Ziwei Liu"], "title": "Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding", "comment": "ICCV 2025; Project page: https://zhangyuanhan-ai.github.io/video-tt/", "summary": "Human intelligence requires correctness and robustness, with the former being\nfoundational for the latter. In video understanding, correctness ensures the\naccurate interpretation of visual content, and robustness maintains consistent\nperformance in challenging conditions. Despite advances in video large language\nmodels (video LLMs), existing benchmarks inadequately reflect the gap between\nthese models and human intelligence in maintaining correctness and robustness\nin video interpretation. We introduce the Video Thinking Test (Video-TT), to\nassess if video LLMs can interpret real-world videos as effectively as humans.\nVideo-TT reflects genuine gaps in understanding complex visual narratives, and\nevaluates robustness against natural adversarial questions. Video-TT comprises\n1,000 YouTube Shorts videos, each with one open-ended question and four\nadversarial questions that probe visual and narrative complexity. Our\nevaluation shows a significant gap between video LLMs and human performance.", "AI": {"tldr": "本文提出了Video-TT基准，用于评估视频大语言模型（Video LLMs）在真实视频理解中的正确性和鲁棒性，发现模型与人类表现存在显著差距。", "motivation": "尽管视频大语言模型取得了进展，但现有基准未能充分反映模型在视频理解的正确性和鲁棒性方面与人类智能的差距。人类智能需要同时具备正确性和鲁棒性，前者是后者的基础。", "method": "引入Video Thinking Test (Video-TT)，包含1000个YouTube Shorts视频，每个视频带有一个开放式问题和四个对抗性问题，旨在探测视觉和叙事复杂性，并评估模型对抗自然对抗性问题的鲁棒性。", "result": "评估结果显示，视频大语言模型与人类在Video-TT上的表现存在显著差距。", "conclusion": "视频大语言模型在解释复杂视觉叙事和对抗自然对抗性问题方面的正确性和鲁棒性仍远低于人类水平，表明真实世界视频理解能力存在巨大提升空间。"}}
{"id": "2507.15636", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15636", "abs": "https://arxiv.org/abs/2507.15636", "authors": ["Lisan Al Amin", "Md. Ismail Hossain", "Thanh Thi Nguyen", "Tasnim Jahan", "Mahbubul Islam", "Faisal Quader"], "title": "Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis", "comment": "Accepted for publication at the 2025 IEEE International Conference on\n  Systems, Man, and Cybernetics (SMC)", "summary": "Recent advances in deepfake technology have created increasingly convincing\nsynthetic media that poses significant challenges to information integrity and\nsocial trust. While current detection methods show promise, their underlying\nmechanisms remain poorly understood, and the large sizes of their models make\nthem challenging to deploy in resource-limited environments. This study\ninvestigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake\ndetection, aiming to identify the key features crucial for recognizing\ndeepfakes. We examine how neural networks can be efficiently pruned while\nmaintaining high detection accuracy. Through extensive experiments with\nMesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and\nFaceForensics++ datasets, we find that deepfake detection networks contain\nwinning tickets, i.e., subnetworks, that preserve performance even at\nsubstantial sparsity levels. Our results indicate that MesoNet retains 56.2%\naccuracy at 80% sparsity on the OpenForensic dataset, with only 3,000\nparameters, which is about 90% of its baseline accuracy (62.6%). The results\nalso show that our proposed LTH-based iterative magnitude pruning approach\nconsistently outperforms one-shot pruning methods. Using Grad-CAM\nvisualization, we analyze how pruned networks maintain their focus on critical\nfacial regions for deepfake detection. Additionally, we demonstrate the\ntransferability of winning tickets across datasets, suggesting potential for\nefficient, deployable deepfake detection systems.", "AI": {"tldr": "本研究将彩票假设（LTH）应用于深度伪造检测，旨在通过识别并修剪关键子网络，在保持高检测准确率的同时，显著减少模型大小，从而实现高效且可部署的深度伪造检测系统。", "motivation": "深度伪造技术日益逼真，对信息完整性和社会信任构成重大挑战。现有检测方法机制不清，且模型庞大，难以在资源受限环境中部署。", "method": "本研究将彩票假设（LTH）应用于深度伪造检测，通过迭代幅度剪枝（iterative magnitude pruning）方法，在MesoNet、CNN-5和ResNet-18架构上进行实验。数据集包括OpenForensic和FaceForensics++。研究还使用Grad-CAM进行可视化分析，并评估了“中奖彩票”（winning tickets）在不同数据集间的可迁移性。", "result": "研究发现，深度伪造检测网络包含“中奖彩票”子网络，即使在显著稀疏度下也能保持性能。例如，MesoNet在OpenForensic数据集上，80%稀疏度（仅3,000参数）下仍能保持56.2%的准确率（接近基线准确率62.6%的90%）。基于LTH的迭代剪枝方法始终优于一次性剪枝。可视化结果显示，剪枝后的网络仍能聚焦于关键面部区域。此外，“中奖彩票”在不同数据集间具有可迁移性。", "conclusion": "本研究表明，深度伪造检测网络中存在高效的“中奖彩票”子网络，通过LTH方法可以显著压缩模型大小，同时保持高检测精度。这为开发高效、可部署且具有良好泛化能力的深度伪造检测系统提供了可能性。"}}
{"id": "2507.15707", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15707", "abs": "https://arxiv.org/abs/2507.15707", "authors": ["Seok Hwan Song", "Mohna Chakraborty", "Qi Li", "Wallapak Tavanapong"], "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?", "comment": null, "summary": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance.", "AI": {"tldr": "本研究发现大型语言模型（LLMs）在推理任务上的表现受问题类型显著影响，且推理步骤的准确性与最终答案的选择准确性不一定相关。", "motivation": "现有研究评估LLMs使用了多种问题类型，但尚未探索不同问题类型对LLMs在推理任务上准确性的影响。", "method": "研究评估了五种LLMs在三种不同问题类型（未具体说明是哪三种，但上下文暗示可能是多选、判断、问答等）上进行定量和演绎推理任务的表现。性能指标包括推理步骤的准确性和最终答案选择的准确性。", "result": "1. LLMs在不同问题类型上的表现存在显著差异。2. 推理准确性与最终答案选择准确性不一定相关。3. 选项数量和措辞会影响LLM的表现。", "conclusion": "不同问题类型、选项数量和措辞对LLMs在推理任务上的准确性有重要影响，且推理过程的正确性不完全等同于最终答案的正确性，这提示了评估LLMs推理能力时需要考虑的复杂性。"}}
{"id": "2507.15035", "categories": ["cs.CV", "cs.LG", "35Q92, 68U10", "I.4.5; J.2; J.3"], "pdf": "https://arxiv.org/pdf/2507.15035", "abs": "https://arxiv.org/abs/2507.15035", "authors": ["Zhijun Zeng", "Youjia Zheng", "Hao Hu", "Zeyuan Dong", "Yihang Zheng", "Xinliang Liu", "Jinzhuo Wang", "Zuoqiang Shi", "Linfeng Zhang", "Yubing Li", "He Sun"], "title": "OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography", "comment": null, "summary": "Accurate and efficient simulation of wave equations is crucial in\ncomputational wave imaging applications, such as ultrasound computed tomography\n(USCT), which reconstructs tissue material properties from observed scattered\nwaves. Traditional numerical solvers for wave equations are computationally\nintensive and often unstable, limiting their practical applications for\nquasi-real-time image reconstruction. Neural operators offer an innovative\napproach by accelerating PDE solving using neural networks; however, their\neffectiveness in realistic imaging is limited because existing datasets\noversimplify real-world complexity. In this paper, we present OpenBreastUS, a\nlarge-scale wave equation dataset designed to bridge the gap between\ntheoretical equations and practical imaging applications. OpenBreastUS includes\n8,000 anatomically realistic human breast phantoms and over 16 million\nfrequency-domain wave simulations using real USCT configurations. It enables a\ncomprehensive benchmarking of popular neural operators for both forward\nsimulation and inverse imaging tasks, allowing analysis of their performance,\nscalability, and generalization capabilities. By offering a realistic and\nextensive dataset, OpenBreastUS not only serves as a platform for developing\ninnovative neural PDE solvers but also facilitates their deployment in\nreal-world medical imaging problems. For the first time, we demonstrate\nefficient in vivo imaging of the human breast using neural operator solvers.", "AI": {"tldr": "本文提出了OpenBreastUS，一个大规模波方程数据集，包含8000个解剖学真实的乳腺模型和超过1600万次频率域波模拟，旨在加速超声CT（USCT）中的神经算子求解器，并首次实现了高效的活体乳腺成像。", "motivation": "传统的波方程求解器计算量大且不稳定，限制了超声CT等计算波成像应用的准实时重建。神经算子虽有潜力，但现有数据集过于简化，限制了其在实际成像中的有效性。", "method": "构建了OpenBreastUS数据集，包含8000个解剖学真实的乳腺体模，并使用真实的USCT配置进行了超过1600万次频率域波模拟。该数据集用于全面基准测试流行的神经算子在正向模拟和逆向成像任务中的性能、可扩展性和泛化能力。", "result": "OpenBreastUS数据集能够对神经算子进行全面的基准测试。通过该数据集，首次展示了使用神经算子求解器对人体乳腺进行高效的活体成像。", "conclusion": "OpenBreastUS数据集弥合了理论方程与实际成像应用之间的鸿沟，为开发创新的神经偏微分方程求解器提供了平台，并促进了它们在实际医学成像问题中的部署，尤其是在高效活体乳腺成像方面。"}}
{"id": "2507.15686", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15686", "abs": "https://arxiv.org/abs/2507.15686", "authors": ["Wenjie Huang", "Qi Yang", "Shuting Xia", "He Huang", "Zhu Li", "Yiling Xu"], "title": "LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression", "comment": "Accepted to ICCV 2025", "summary": "Existing AI-based point cloud compression methods struggle with dependence on\nspecific training data distributions, which limits their real-world deployment.\nImplicit Neural Representation (INR) methods solve the above problem by\nencoding overfitted network parameters to the bitstream, resulting in more\ndistribution-agnostic results. However, due to the limitation of encoding time\nand decoder size, current INR based methods only consider lossy geometry\ncompression. In this paper, we propose the first INR based lossless point cloud\ngeometry compression method called Lossless Implicit Neural Representations for\nPoint Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we\ndesign a group of point clouds level coding framework with an effective network\ninitialization strategy, which can reduce around 60% encoding time. A\nlightweight coding network based on multiscale SparseConv, consisting of scale\ncontext extraction, child node prediction, and model compression modules, is\nproposed to realize fast inference and compact decoder size. Experimental\nresults show that our method consistently outperforms traditional and AI-based\nmethods: for example, with the convergence time in the MVUB dataset, our method\nreduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and\n21.95% compared to SparsePCGC. Our project can be seen on\nhttps://huangwenjie2023.github.io/LINR-PCGC/.", "AI": {"tldr": "本文提出了LINR-PCGC，首个基于隐式神经表示（INR）的无损点云几何压缩方法，通过优化编码框架和网络设计，实现了更快的编码速度和更高的压缩率。", "motivation": "现有基于AI的点云压缩方法依赖特定训练数据分布，限制了实际部署。虽然INR方法能解决分布依赖问题，但受限于编码时间和解码器大小，目前仅支持有损几何压缩。因此，需要一种基于INR的无损点云几何压缩方法。", "method": "本文提出了LINR-PCGC方法。为加速编码，设计了一个点云组级别编码框架，并采用有效的网络初始化策略，将编码时间缩短约60%。为实现快速推理和紧凑的解码器尺寸，提出了一个基于多尺度SparseConv的轻量级编码网络，包含尺度上下文提取、子节点预测和模型压缩模块。", "result": "实验结果表明，LINR-PCGC方法在压缩性能上持续优于传统和基于AI的方法。例如，在MVUB数据集上，相比G-PCC TMC13v23，本方法减少了约21.21%的比特流；相比SparsePCGC，减少了约21.95%的比特流。", "conclusion": "LINR-PCGC是首个实现无损点云几何压缩的INR方法，通过创新的编码框架和网络设计，显著提升了编码速度和压缩效率，超越了现有主流方法。"}}
{"id": "2507.15714", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15714", "abs": "https://arxiv.org/abs/2507.15714", "authors": ["Tian Li", "Yujian Sun", "Huizhi Liang"], "title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning", "comment": null, "summary": "The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,\nintroduces an emotion recognition challenge spanning over 28 languages. This\ncompetition encourages researchers to explore more advanced approaches to\naddress the challenges posed by the diversity of emotional expressions and\nbackground variations. It features two tracks: multi-label classification\n(Track A) and emotion intensity prediction (Track B), covering six emotion\ncategories: anger, fear, joy, sadness, surprise, and disgust. In our work, we\nsystematically explore the benefits of two contrastive learning approaches:\nsample-based (Contrastive Reasoning Calibration) and generation-based (DPO,\nSimPO) contrastive learning. The sample-based contrastive approach trains the\nmodel by comparing two samples to generate more reliable predictions. The\ngeneration-based contrastive approach trains the model to differentiate between\ncorrect and incorrect generations, refining its prediction. All models are\nfine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A\nand 6th place in Track B for English, while ranking among the top-tier\nperforming systems for other languages.", "AI": {"tldr": "本文探讨了在SemEval-2025 Task 11多语言情感识别挑战中，基于Llama3-Instruct-8B模型，结合样本式（如CRC）和生成式（如DPO、SimPO）对比学习的有效性，并在比赛中取得了靠前的排名。", "motivation": "研究旨在解决文本情感检测中，情感表达多样性和背景变异性带来的挑战，特别是在跨28种语言的复杂情境下。", "method": "研究采用了两种对比学习方法：1. 样本式对比学习（Contrastive Reasoning Calibration, CRC），通过比较样本来生成更可靠的预测；2. 生成式对比学习（DPO, SimPO），通过区分正确和不正确的生成来优化预测。所有模型均基于LLaMa3-Instruct-8B进行微调。任务涵盖多标签分类（Track A）和情感强度预测（Track B），涉及六种情感类别。", "result": "在英语赛道中，系统在Track A（多标签分类）中获得第9名，在Track B（情感强度预测）中获得第6名；在其他语言中也取得了顶尖的表现。", "conclusion": "结合样本式和生成式对比学习方法，在微调的LLaMa3-Instruct-8B模型上，能够有效提升多语言文本情感检测的性能，并在国际比赛中展现出强大的竞争力。"}}
{"id": "2507.15037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15037", "abs": "https://arxiv.org/abs/2507.15037", "authors": ["Zhaotong Yang", "Yuhui Li", "Shengfeng He", "Xinzhe Li", "Yangyang Xu", "Junyu Dong", "Yong Du"], "title": "OmniVTON: Training-Free Universal Virtual Try-On", "comment": "Accepted by ICCV2025", "summary": "Image-based Virtual Try-On (VTON) techniques rely on either supervised\nin-shop approaches, which ensure high fidelity but struggle with cross-domain\ngeneralization, or unsupervised in-the-wild methods, which improve adaptability\nbut remain constrained by data biases and limited universality. A unified,\ntraining-free solution that works across both scenarios remains an open\nchallenge. We propose OmniVTON, the first training-free universal VTON\nframework that decouples garment and pose conditioning to achieve both texture\nfidelity and pose consistency across diverse settings. To preserve garment\ndetails, we introduce a garment prior generation mechanism that aligns clothing\nwith the body, followed by continuous boundary stitching technique to achieve\nfine-grained texture retention. For precise pose alignment, we utilize DDIM\ninversion to capture structural cues while suppressing texture interference,\nensuring accurate body alignment independent of the original image textures. By\ndisentangling garment and pose constraints, OmniVTON eliminates the bias\ninherent in diffusion models when handling multiple conditions simultaneously.\nExperimental results demonstrate that OmniVTON achieves superior performance\nacross diverse datasets, garment types, and application scenarios. Notably, it\nis the first framework capable of multi-human VTON, enabling realistic garment\ntransfer across multiple individuals in a single scene. Code is available at\nhttps://github.com/Jerome-Young/OmniVTON", "AI": {"tldr": "OmniVTON是一种无需训练的通用虚拟试穿（VTON）框架，通过解耦服装和姿态条件，实现了跨场景的高保真度和姿态一致性，并支持多人虚拟试穿。", "motivation": "现有的VTON技术存在局限性：监督式方法保真度高但泛化性差，非监督式方法适应性强但受数据偏差和通用性限制。研究目标是开发一个统一、无需训练且能跨场景工作的解决方案。", "method": "OmniVTON通过解耦服装和姿态条件来实现纹理保真度和姿态一致性。它引入了服装先验生成机制和连续边界缝合技术以保留服装细节；同时利用DDIM反演捕获结构线索并抑制纹理干扰，以实现精确的姿态对齐。这种解耦消除了扩散模型在处理多条件时的偏差。", "result": "实验结果表明，OmniVTON在多样化的数据集、服装类型和应用场景中均表现出色。值得注意的是，它是首个能够实现多人VTON的框架，可以在单个场景中实现多个人之间的逼真服装转移。", "conclusion": "OmniVTON是第一个无需训练的通用VTON框架，通过创新的解耦方法解决了现有技术的局限性，实现了高保真和姿态一致性，并首次支持多人虚拟试穿，为VTON领域带来了显著进展。"}}
{"id": "2507.15717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15717", "abs": "https://arxiv.org/abs/2507.15717", "authors": ["Sahana Srinivasan", "Xuguang Ai", "Thaddaeus Wai Soon Lo", "Aidan Gilson", "Minjie Zou", "Ke Zou", "Hyunjae Kim", "Mingjia Yang", "Krithi Pushpanathan", "Samantha Yew", "Wan Ting Loke", "Jocelyn Goh", "Yibing Chen", "Yiming Kong", "Emily Yuelei Fu", "Michelle Ongyong Hui", "Kristen Nwanyanwu", "Amisha Dave", "Kelvin Zhenghao Li", "Chen-Hsin Sun", "Mark Chia", "Gabriel Dawei Yang", "Wendy Meihua Wong", "David Ziyou Chen", "Dianbo Liu", "Maxwell Singer", "Fares Antaki", "Lucian V Del Priore", "Jost Jonas", "Ron Adelman", "Qingyu Chen", "Yih-Chung Tham"], "title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning", "comment": null, "summary": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels.", "AI": {"tldr": "本文介绍了BELO（BEnchmarking LLMs for Ophthalmology），一个通过多轮专家审核开发的标准化、全面的眼科大型语言模型（LLM）评估基准，旨在评估临床准确性和推理质量，并已建立公共排行榜。", "motivation": "当前评估眼科LLM的基准范围有限，且过度侧重准确性，缺乏对推理质量的评估。", "method": "研究通过13位眼科专家的多轮审核开发了BELO。使用关键词匹配和微调的PubMedBERT模型，从多个医学数据集中筛选出眼科多项选择题（MCQs），并经过多轮专家审核以去除重复和不合格问题。10位眼科专家完善了正确答案的解释，并由3位资深眼科专家进一步裁定。研究使用准确率、macro-F1和五种文本生成指标（ROUGE-L、BERTScore、BARTScore、METEOR、AlignScore）评估了六个LLM。此外，两位眼科专家对50个随机选取的输出进行了定性评估，以考察其准确性、全面性和完整性。", "result": "BELO包含900个高质量、经专家审核的问题，这些问题来源于BCSC、BioASQ、MedMCQA、MedQA和PubMedQA五个来源。研究建立了公共排行榜以促进透明评估和报告。BELO数据集将作为留出、仅用于评估的基准，以确保未来模型比较的公平性和可复现性。", "conclusion": "BELO提供了一个标准化、全面且经专家验证的眼科LLM评估基准，弥补了现有基准的不足，并为未来LLM在眼科领域的公平和可复现比较奠定了基础。"}}
{"id": "2507.15715", "categories": ["cs.CL", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2507.15715", "abs": "https://arxiv.org/abs/2507.15715", "authors": ["Alina Hyk", "Kiera McCormick", "Mian Zhong", "Ioana Ciucă", "Sanjib Sharma", "John F Wu", "J. E. G. Peek", "Kartheik G. Iyer", "Ziang Xiao", "Anjalie Field"], "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs", "comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures", "summary": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.", "AI": {"tldr": "本研究旨在通过理解用户如何评估大型语言模型（LLMs），特别是其在天文学研究中的应用，来改进LLM的评估方法和基准。", "motivation": "当前LLM的评估基准未能跟上用户实际评估和使用模型的多样化方式，尤其是在天文学及其他科学研究领域，人们对利用LLM辅助研究的兴趣日益增长。", "method": "研究部署了一个基于Slack的检索增强生成（RAG）LLM机器人，用于处理天文学文献。通过对四周内368个机器人查询进行归纳编码，并对11位天文学家进行后续访谈，分析了用户评估该系统的方式，包括提问类型和判断回答的标准。", "result": "研究揭示了人类用户评估LLM系统的方式，包括他们提出的问题类型和判断回答的标准。研究将发现整合为构建更好基准的具体建议，并据此构建了一个用于评估天文学LLM的样本基准。", "conclusion": "本工作提出了改进LLM评估和可用性的方法，特别适用于科学研究领域，旨在提升LLM在科学应用中的实用性。"}}
{"id": "2507.15059", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15059", "abs": "https://arxiv.org/abs/2507.15059", "authors": ["Ran Zhang", "Xuanhua He", "Li Xueheng", "Ke Cao", "Liu Liu", "Wenbo Xu", "Fang Jiabin", "Yang Qize", "Jie Zhang"], "title": "Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling", "comment": null, "summary": "The field of pan-sharpening has recently seen a trend towards increasingly\nlarge and complex models, often trained on single, specific satellite datasets.\nThis approach, however, leads to high computational overhead and poor\ngeneralization on full resolution data, a paradigm we challenge in this paper.\nIn response to this issue, we propose PanTiny, a lightweight, single-step\npan-sharpening framework designed for both efficiency and robust performance.\nMore critically, we introduce multiple-in-one training paradigm, where a\nsingle, compact model is trained simultaneously on three distinct satellite\ndatasets (WV2, WV3, and GF2) with different resolution and spectral\ninformation. Our experiments show that this unified training strategy not only\nsimplifies deployment but also significantly boosts generalization on\nfull-resolution data. Further, we introduce a universally powerful composite\nloss function that elevates the performance of almost all of models for\npan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny\nmodel, benefiting from these innovations, achieves a superior\nperformance-to-efficiency balance, outperforming most larger, specialized\nmodels. Through extensive ablation studies, we validate that principled\nengineering in model design, training paradigms, and loss functions can surpass\nbrute-force scaling. Our work advocates for a community-wide shift towards\ncreating efficient, generalizable, and data-conscious models for\npan-sharpening. The code is available at\nhttps://github.com/Zirconium233/PanTiny .", "AI": {"tldr": "本文提出PanTiny，一个轻量级、单步全色锐化框架，通过“多合一”训练范式和通用复合损失函数，实现了高效且鲁棒的性能，解决了现有模型计算开销大和泛化能力差的问题。", "motivation": "当前全色锐化模型趋于庞大复杂，通常针对单一卫星数据集训练，导致计算开销大、在全分辨率数据上泛化能力差。本文旨在挑战这种范式。", "method": "1. 提出了PanTiny，一个轻量级、单步的全色锐化框架。2. 引入了“多合一”训练范式，即在三个不同的卫星数据集（WV2、WV3、GF2）上同时训练一个紧凑的模型。3. 引入了一个通用的复合损失函数，旨在提升几乎所有全色锐化模型的性能。", "result": "1. 统一训练策略简化了部署，并显著提升了模型在全分辨率数据上的泛化能力。2. 提出的复合损失函数提升了几乎所有全色锐化模型的性能，将现有技术指标推向新高。3. PanTiny模型在性能与效率之间取得了卓越的平衡，超越了大多数更大、更专业的模型。4. 消融研究验证了模型设计、训练范式和损失函数中的原则性工程优于暴力扩展。", "conclusion": "研究表明，通过模型设计、训练范式和损失函数方面的精巧工程，可以超越简单地扩大模型规模，实现高效、通用且数据友好的全色锐化模型。本文倡导社区向创建此类模型转变。"}}
{"id": "2507.15752", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15752", "abs": "https://arxiv.org/abs/2507.15752", "authors": ["Ruizhe Zhu", "Hao Zhu", "Yaxuan Li", "Syang Zhou", "Shijing Cai", "Malgorzata Lazuka", "Elliott Ash"], "title": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue", "comment": "For our code and data, see\n  https://github.com/nerchio/Human_Chatbot-Generation", "summary": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models.", "AI": {"tldr": "本文提出了DialogueForge框架，利用大型语言模型（LLMs）生成AI模拟的人机对话，以解决对话数据收集耗时耗力的问题。", "motivation": "收集人机对话数据通常需要大量人工且耗时，这限制了会话式AI的研究和发展。", "method": "提出了DialogueForge框架，该框架利用从真实人机交互中提取的种子提示来初始化对话。测试了多种LLM（包括专有模型和开源模型）来模拟人类用户，并生成多轮任务导向对话。此外，还探索了微调技术以提高小型模型生成类人对话的能力，并使用UniEval和GTEval协议评估了对话质量。", "result": "实验表明，大型专有模型（如GPT-4o）在生成更逼真的对话方面表现优于其他模型。小型开源模型（如Llama、Mistral）通过定制化也展现出良好潜力，且通过监督式微调可以显著提升其性能。然而，所有模型在保持连贯和自然的长期类人对话方面仍面临共同挑战。", "conclusion": "DialogueForge框架能够有效生成高质量的AI模拟人机对话，大型模型表现最佳，而通过微调的小型模型也能取得显著进步。未来研究需关注如何提升所有模型在长对话中的连贯性和自然度。"}}
{"id": "2507.15736", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15736", "abs": "https://arxiv.org/abs/2507.15736", "authors": ["Yuanhao Shen", "Daniel Xavier de Sousa", "Ricardo Marçal", "Ali Asad", "Hongyu Guo", "Xiaodan Zhu"], "title": "Understanding Large Language Models' Ability on Interdisciplinary Research", "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch.", "AI": {"tldr": "该论文提出了IDRBench，这是一个用于评估大型语言模型（LLMs）在跨学科研究（IDR）中提出研究思想能力的基准，并发现当前LLMs在此方面仍有不足。", "motivation": "尽管LLMs在多步骤逻辑推理方面表现出色，并被视为科学发现的有力工具，但目前缺乏专门的基准来评估它们在跨学科研究（IDR）情境下产生思想的能力，这阻碍了对其优势和局限性的全面理解。", "method": "引入了IDRBench基准，包含一个由专家标注的数据集和一套评估任务。数据集源自ArXiv平台，涵盖六个不同学科的科学出版物，由具有多样学术背景的领域专家进行标注，并强调清晰定义的IDR特征维度。评估任务设计遵循渐进式、真实世界的视角，包括：1) IDR论文识别，2) IDR思想整合，以及3) IDR思想推荐。", "result": "使用IDRBench对10个LLMs构建了基线，观察到尽管LLMs展现出一定程度的IDR意识，但它们在产生高质量IDR思想方面仍然存在困难。", "conclusion": "这些发现不仅能激发新的研究方向，还有助于开发出在跨学科研究中表现更卓越的下一代LLMs。"}}
{"id": "2507.15085", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15085", "abs": "https://arxiv.org/abs/2507.15085", "authors": ["Peirong Zhang", "Haowei Xu", "Jiaxin Zhang", "Guitao Xu", "Xuhan Zheng", "Zhenhua Yang", "Junle Liu", "Yuyi Zhang", "Lianwen Jin"], "title": "Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR", "comment": null, "summary": "Text image is a unique and crucial information medium that integrates visual\naesthetics and linguistic semantics in modern e-society. Due to their subtlety\nand complexity, the generation of text images represents a challenging and\nevolving frontier in the image generation field. The recent surge of\nspecialized image generators (\\emph{e.g.}, Flux-series) and unified generative\nmodels (\\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a\nnatural question: can they master the intricacies of text image generation and\nediting? Motivated by this, we assess current state-of-the-art generative\nmodels' capabilities in terms of text image generation and editing. We\nincorporate various typical optical character recognition (OCR) tasks into our\nevaluation and broaden the concept of text-based generation tasks into OCR\ngenerative tasks. We select 33 representative tasks and categorize them into\nfive categories: document, handwritten text, scene text, artistic text, and\ncomplex \\& layout-rich text. For comprehensive evaluation, we examine six\nmodels across both closed-source and open-source domains, using tailored,\nhigh-quality image inputs and prompts. Through this evaluation, we draw crucial\nobservations and identify the weaknesses of current generative models for OCR\ntasks. We argue that photorealistic text image generation and editing should be\ninternalized as foundational skills into general-domain generative models,\nrather than being delegated to specialized solutions, and we hope this\nempirical analysis can provide valuable insights for the community to achieve\nthis goal. This evaluation is online and will be continuously updated at our\nGitHub repository.", "AI": {"tldr": "本文评估了当前最先进的生成模型在文本图像生成和编辑方面的能力，发现它们在OCR相关任务中存在不足，并主张将文本图像生成作为通用生成模型的基础能力。", "motivation": "随着专业图像生成器（如Flux系列）和统一生成模型（如GPT-4o）的兴起，其卓越的保真度引发了一个问题：它们能否掌握文本图像生成和编辑的复杂性？本文旨在评估这些模型在此方面的能力。", "method": "研究将典型的光学字符识别（OCR）任务纳入评估，并将基于文本的生成任务扩展为OCR生成任务。选择了33个代表性任务，分为五类：文档、手写文本、场景文本、艺术文本和复杂/布局丰富文本。评估了六种模型（包括闭源和开源），使用定制的高质量图像输入和提示。", "result": "通过评估，研究者得出了关键观察结果，并指出了当前生成模型在OCR任务中的弱点。", "conclusion": "研究认为，逼真的文本图像生成和编辑应作为基础技能融入通用生成模型，而非委派给专门解决方案。本文希望其经验分析能为社区实现这一目标提供有价值的见解。"}}
{"id": "2507.15773", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15773", "abs": "https://arxiv.org/abs/2507.15773", "authors": ["Andrei-Valentin Tanase", "Elena Pelican"], "title": "Supernova: Achieving More with Less in Transformer Architectures", "comment": null, "summary": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 53% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts.", "AI": {"tldr": "Supernova是一个6.5亿参数的解码器专用Transformer模型，通过精心的架构设计和创新的字节级BPE分词器，在保持计算效率的同时，实现了大型模型的性能。", "motivation": "当前主流的AI模型存在参数量巨大、训练数据需求高的问题。本研究旨在探索如何在减少参数量和训练数据的情况下，依然达到大型模型的性能水平，以提高计算效率。", "method": "Supernova模型结合了多种先进技术：旋转位置嵌入（RoPE）、分组查询注意力（GQA）采用3:1的压缩比、RMSNorm归一化层、SwiGLU激活函数。核心创新是一个自定义的128,000词汇量字节级BPE分词器，该分词器实现了最先进的压缩性能。", "result": "Supernova模型（6.5亿参数）实现了10亿参数模型90%的性能，但参数量减少了53%，并且仅需要1000亿个训练token，比同类竞争模型少了一个数量级。", "conclusion": "研究结果表明，通过提升架构效率和分词质量，可以弥补参数量减少带来的性能损失，这挑战了当前盛行的模型扩展范式（即单纯增加参数量）。"}}
{"id": "2507.15742", "categories": ["cs.CL", "cs.IR", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.15742", "abs": "https://arxiv.org/abs/2507.15742", "authors": ["Paul Sheridan", "Zeyad Ahmed", "Aitazaz A. Farooque"], "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme", "comment": "23 pages, 4 tables", "summary": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.", "AI": {"tldr": "本文从显著性检验的角度，论证了TF-IDF（特别是其变体TF-ICF）与Fisher精确检验p值的负对数之间的紧密联系，为TF-IDF提供了统计学基础。", "motivation": "TF-IDF是信息检索领域最著名的数学表达式之一，被广泛应用于文本分析。尽管其应用广泛，但仍有学者致力于为其奠定坚实的理论基础。本文旨在向统计学界解释TF-IDF的有效性，将其置于显著性检验的框架下。", "method": "通过证明TF-ICF（一种常见的TF-IDF变体）在温和的正则条件下，与单尾Fisher精确统计显著性检验的p值的负对数密切相关。在此基础上，推导出在特定理想假设下TF-IDF与该负对数p值的联系。并进一步证明，在文档集合无限大的极限情况下，该量收敛于TF-IDF。", "result": "TF-ICF与单尾Fisher精确检验p值的负对数紧密相关。在特定理想假设下，TF-IDF也与该负对数p值建立联系。在无限大文档集合的极限情况下，该量收敛于TF-IDF。", "conclusion": "从Fisher精确检验的角度理解TF-IDF，为统计学家提供了对其长期有效性的合理解释，为其在统计学领域的应用提供了坚实的理论基础。"}}
{"id": "2507.15109", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15109", "abs": "https://arxiv.org/abs/2507.15109", "authors": ["Mohammad-Maher Nakshbandi", "Ziad Sharawy", "Sorin Grigorescu"], "title": "LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM", "comment": null, "summary": "One of the main challenges in the Simultaneous Localization and Mapping\n(SLAM) loop closure problem is the recognition of previously visited places. In\nthis work, we tackle the two main problems of real-time SLAM systems: 1) loop\nclosure detection accuracy and 2) real-time computation constraints on the\nembedded hardware. Our LoopNet method is based on a multitasking variant of the\nclassical ResNet architecture, adapted for online retraining on a dynamic\nvisual dataset and optimized for embedded devices. The online retraining is\ndesigned using a few-shot learning approach. The architecture provides both an\nindex into the queried visual dataset, and a measurement of the prediction\nquality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,\nLoopNet surpasses the limitations of handcrafted features and traditional deep\nlearning methods, offering better performance under varying conditions. Code is\navailable at https://github.com/RovisLab/LoopNet. Additinally, we introduce a\nnew loop closure benchmarking dataset, coined LoopDB, which is available at\nhttps://github.com/RovisLab/LoopDB.", "AI": {"tldr": "本文提出LoopNet方法，一种基于ResNet变体的多任务学习模型，结合少样本在线再训练和DISK描述符，旨在提高实时SLAM系统中回环检测的准确性并满足嵌入式硬件的计算约束。", "motivation": "实时SLAM系统面临两大挑战：1) 回环检测的准确性；2) 嵌入式硬件上的实时计算限制。现有方法在这些方面存在不足。", "method": "LoopNet基于经典ResNet架构的多任务变体，针对动态视觉数据集进行在线再训练，并为嵌入式设备进行了优化。在线再训练采用少样本学习方法。该架构不仅提供查询视觉数据集的索引，还能测量预测质量。此外，通过利用DISK（DIStinctive Keypoints）描述符，克服了手工特征和传统深度学习方法的局限性。文中还引入了新的回环检测基准数据集LoopDB。", "result": "LoopNet在各种条件下表现出更好的性能，超越了手工特征和传统深度学习方法的局限性。研究者还发布了LoopNet代码和LoopDB数据集。", "conclusion": "LoopNet成功解决了实时SLAM回环检测中的准确性和计算效率问题，尤其适用于嵌入式设备，并通过结合先进的深度学习和特征描述符，提升了在不同条件下的性能。"}}
{"id": "2507.15803", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15803", "abs": "https://arxiv.org/abs/2507.15803", "authors": ["Danhui Chen", "Ziquan Liu", "Chuxi Yang", "Dan Wang", "Yan Yan", "Yi Xu", "Xiangyang Ji"], "title": "ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction", "comment": "ICCV 2025", "summary": "Pixel-level vision tasks, such as semantic segmentation, require extensive\nand high-quality annotated data, which is costly to obtain. Semi-supervised\nsemantic segmentation (SSSS) has emerged as a solution to alleviate the\nlabeling burden by leveraging both labeled and unlabeled data through\nself-training techniques. Meanwhile, the advent of foundational segmentation\nmodels pre-trained on massive data, has shown the potential to generalize\nacross domains effectively. This work explores whether a foundational\nsegmentation model can address label scarcity in the pixel-level vision task as\nan annotator for unlabeled images. Specifically, we investigate the efficacy of\nusing SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual\ninput, to generate predictive masks for unlabeled data. To address the\nshortcomings of using SEEM-generated masks as supervision, we propose\nConformalSAM, a novel SSSS framework which first calibrates the foundation\nmodel using the target domain's labeled data and then filters out unreliable\npixel labels of unlabeled data so that only high-confidence labels are used as\nsupervision. By leveraging conformal prediction (CP) to adapt foundation models\nto target data through uncertainty calibration, ConformalSAM exploits the\nstrong capability of the foundational segmentation model reliably which\nbenefits the early-stage learning, while a subsequent self-reliance training\nstrategy mitigates overfitting to SEEM-generated masks in the later training\nstage. Our experiment demonstrates that, on three standard benchmarks of SSSS,\nConformalSAM achieves superior performance compared to recent SSSS methods and\nhelps boost the performance of those methods as a plug-in.", "AI": {"tldr": "本文提出ConformalSAM，一个半监督语义分割框架，利用经过校准的预训练基础模型（如SEEM）作为标注器，并通过共形预测过滤不可靠的伪标签，显著提升了分割性能。", "motivation": "像素级视觉任务（如语义分割）需要大量高质量标注数据，成本高昂。半监督学习通过结合有标签和无标签数据来缓解此问题。同时，预训练的基础分割模型展现出强大的泛化能力。本文旨在探索基础分割模型能否作为无标签图像的标注器，以解决标签稀缺问题。", "method": "本研究使用SEEM（SAM的一个变体）为无标签数据生成预测掩码。为解决SEEM生成掩码的不足，提出了ConformalSAM框架：首先，利用目标域的已标注数据校准基础模型；其次，通过共形预测（CP）过滤无标签数据中不可靠的像素标签，仅使用高置信度标签进行监督；最后，采用后续的自依赖训练策略，以减轻后期训练中对SEEM生成掩码的过拟合。", "result": "ConformalSAM在三个标准的半监督语义分割基准测试中，与现有方法相比取得了卓越的性能。此外，它还可以作为插件，提升其他半监督语义分割方法的性能。", "conclusion": "ConformalSAM通过结合基础分割模型的强大能力、共形预测的不确定性校准以及自依赖训练策略，有效解决了像素级视觉任务中的标签稀缺问题，实现了优异的半监督语义分割性能。"}}
{"id": "2507.15759", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15759", "abs": "https://arxiv.org/abs/2507.15759", "authors": ["Lyumanshan Ye", "Xiaojie Cai", "Xinkai Wang", "Junfei Wang", "Xiangkun Hu", "Jiadi Su", "Yang Nan", "Sihan Wang", "Bohan Zhang", "Xiaoze Fan", "Jinbin Luo", "Yuxiang Zheng", "Tianze Xu", "Dayuan Fu", "Yunze Wu", "Pengrui Lu", "Zengzhi Wang", "Yiwei Qin", "Zhen Huang", "Yan Ma", "Zhulin Hu", "Haoyang Zou", "Tiantian Mi", "Yixin Ye", "Ethan Chern", "Pengfei Liu"], "title": "Interaction as Intelligence: Deep Research With Human-AI Partnership", "comment": "30 pages, 10 figures", "summary": "This paper introduces \"Interaction as Intelligence\" research series,\npresenting a reconceptualization of human-AI relationships in deep research\ntasks. Traditional approaches treat interaction merely as an interface for\naccessing AI capabilities-a conduit between human intent and machine output. We\npropose that interaction itself constitutes a fundamental dimension of\nintelligence. As AI systems engage in extended thinking processes for research\ntasks, meaningful interaction transitions from an optional enhancement to an\nessential component of effective intelligence. Current deep research systems\nadopt an \"input-wait-output\" paradigm where users initiate queries and receive\nresults after black-box processing. This approach leads to error cascade\neffects, inflexible research boundaries that prevent question refinement during\ninvestigation, and missed opportunities for expertise integration. To address\nthese limitations, we introduce Deep Cognition, a system that transforms the\nhuman role from giving instructions to cognitive oversight-a mode of engagement\nwhere humans guide AI thinking processes through strategic intervention at\ncritical junctures. Deep cognition implements three key innovations:\n(1)Transparent, controllable, and interruptible interaction that reveals AI\nreasoning and enables intervention at any point; (2)Fine-grained bidirectional\ndialogue; and (3)Shared cognitive context where the system observes and adapts\nto user behaviors without explicit instruction. User evaluation demonstrates\nthat this cognitive oversight paradigm outperforms the strongest baseline\nacross six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),\nReal-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),\nResults-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on\nchallenging research problems show 31.8% to 50.0% points of improvements over\ndeep research systems.", "AI": {"tldr": "本文提出了“交互即智能”的研究系列，重新定义了深度研究任务中人机关系。通过引入“深度认知”系统，将人类角色从指令者转变为认知监督者，实现透明、可控、可中断的交互，并显著提升了人机协作效率和研究成果质量。", "motivation": "传统人机交互将交互视为获取AI能力的接口，采用“输入-等待-输出”模式，导致错误级联、研究边界僵化以及专家经验整合不足。这促使研究人员思考如何将交互本身提升为智能的维度，以解决深度研究任务中的这些限制。", "method": "本文提出“深度认知”系统，通过以下三项创新实现认知监督范式：1) 透明、可控、可中断的交互，揭示AI推理过程并允许随时干预；2) 细粒度的双向对话；3) 共享认知上下文，系统在没有明确指令的情况下观察并适应用户行为。", "result": "用户评估显示，认知监督范式在六个关键指标上（透明度、细粒度交互、实时干预、协作便捷性、投入产出比、可中断性）均优于最强基线，提升幅度分别为+20.0%、+29.2%、+18.5%、+27.7%、+8.8%和+20.7%。在具有挑战性的研究问题上，比现有深度研究系统提升了31.8%至50.0%。", "conclusion": "交互本身是智能的一个基本维度。通过将人类角色转变为认知监督者，并实施透明、细粒度、上下文感知的交互，能够显著提升人机在深度研究任务中的协作效率和智能表现。这种“交互即智能”的范式优于传统的“输入-等待-输出”模式。"}}
{"id": "2507.15130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15130", "abs": "https://arxiv.org/abs/2507.15130", "authors": ["Ce Zhang", "Yale Song", "Ruta Desai", "Michael Louis Iuzzolino", "Joseph Tighe", "Gedas Bertasius", "Satwik Kottur"], "title": "Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction", "comment": null, "summary": "Visual Planning for Assistance (VPA) aims to predict a sequence of user\nactions required to achieve a specified goal based on a video showing the\nuser's progress. Although recent advances in multimodal large language models\n(MLLMs) have shown promising results in video understanding, long-horizon\nvisual planning remains a challenging problem. We identify two challenges in\ntraining large MLLMs for video-based planning tasks: (1) scarcity of procedural\nannotations, limiting the model's ability to learn procedural task dynamics\neffectively, and (2) inefficiency of next-token prediction objective to\nexplicitly capture the structured action space for visual planning when\ncompared to free-form, natural language. To tackle data scarcity, we introduce\nAuxiliary Task Augmentation. We design and train our model on auxiliary tasks\nrelevant to long-horizon video-based planning (e.g., goal prediction) to\naugment the model's planning ability. To more explicitly model the structured\naction space unique to visual planning tasks, we leverage Multi-token\nPrediction, extending traditional next-token prediction by using multiple heads\nto predict multiple future tokens during training. Our approach, VideoPlan,\nachieves state-of-the-art VPA performance on the COIN and CrossTask datasets,\nsurpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3\nfuture actions. We further extend our method to the challenging Ego4D Long-term\nAction Anticipation task, and show that it is on par with the state-of-the-art\napproaches despite not using specialized egocentric features. Code will be made\navailable.", "AI": {"tldr": "本文提出VideoPlan，通过辅助任务增强和多令牌预测，解决了多模态大语言模型在长周期视觉规划中面临的数据稀缺和结构化动作空间建模挑战，在多个数据集上实现了最先进的性能。", "motivation": "尽管多模态大语言模型在视频理解方面取得了进展，但长周期视觉规划（VPA）仍然是一个挑战。主要挑战在于：1) 程序性标注数据稀缺，限制了模型学习任务动态的能力；2) 传统的下一令牌预测目标在捕捉视觉规划的结构化动作空间方面效率低下。", "method": "为解决数据稀缺问题，引入“辅助任务增强”（Auxiliary Task Augmentation），通过在与长周期视频规划相关的辅助任务（如目标预测）上训练模型来增强其规划能力。为更明确地建模视觉规划任务特有的结构化动作空间，采用“多令牌预测”（Multi-token Prediction），通过使用多个头在训练期间预测多个未来令牌来扩展传统的下一令牌预测。", "result": "VideoPlan在COIN和CrossTask数据集上实现了最先进的VPA性能，在预测3个未来动作时分别超越先前方法7.3%和3.4%。此外，在挑战性的Ego4D长期动作预测任务中，尽管未使用专门的自我中心特征，其性能与最先进的方法持平。", "conclusion": "本文提出的VideoPlan通过有效解决长周期视觉规划中的数据稀缺和结构化动作空间建模问题，显著提升了多模态大语言模型在该任务上的表现，并在多个基准测试中取得了最先进的成果。"}}
{"id": "2507.15807", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15807", "abs": "https://arxiv.org/abs/2507.15807", "authors": ["Shuo Chen", "Jianzhe Liu", "Zhen Han", "Yan Xia", "Daniel Cremers", "Philip Torr", "Volker Tresp", "Jindong Gu"], "title": "True Multimodal In-Context Learning Needs Attention to the Visual Context", "comment": "accepted to COLM 2025", "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .", "AI": {"tldr": "当前多模态大语言模型（MLLMs）在多模态情境学习（MICL）中忽视视觉信息，倾向于文本模仿。本文提出动态注意力重新分配（DARA）策略和TrueMICL数据集，以增强和可靠评估模型真正的多模态适应能力。", "motivation": "现有MLLMs在MICL中未能有效利用视觉信息，过度依赖文本模式，导致模型进行文本模仿而非真正的多模态适应。这种局限性限制了MICL的实用性，且常被在不需要视觉上下文的任务上的性能提升所掩盖。因此，如何有效增强MICL能力并可靠评估其性能仍是未充分探索的问题。", "method": "1. 引入动态注意力重新分配（DARA）：一种高效的微调策略，通过重新平衡视觉和文本token之间的注意力，鼓励模型关注视觉上下文。2. 提出TrueMICL：一个专用于MICL的数据集，包含支持集和测试集，明确要求整合多模态信息（特别是视觉内容）才能正确完成任务。", "result": "广泛的实验证明了所提出整体解决方案（DARA和TrueMICL）的有效性，显著提升了模型真正的多模态情境学习能力。", "conclusion": "通过DARA策略和TrueMICL数据集，本文成功解决了MLLMs在多模态情境学习中忽视视觉信息的问题，有效增强并可靠评估了模型真正的多模态适应能力。"}}
{"id": "2507.15778", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15778", "abs": "https://arxiv.org/abs/2507.15778", "authors": ["Jiakang Wang", "Runze Liu", "Fuzheng Zhang", "Xiu Li", "Guorui Zhou"], "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.", "AI": {"tldr": "Archer提出了一种熵感知的强化学习可验证奖励（RLVR）方法，通过对知识型和推理型令牌施加不同的训练约束，显著提升了大型语言模型的推理能力。", "motivation": "以往的RLVR算法对所有令牌应用统一的训练信号，忽视了低熵知识相关令牌和高熵推理相关令牌的不同作用。尽管一些方法尝试分离这些令牌类型，但可能破坏语义依赖并阻碍有效学习。", "method": "本文提出了Archer，一种熵感知的RLVR方法，采用双令牌约束和同步更新。具体来说，对推理令牌应用较弱的KL正则化和较高的裁剪阈值以鼓励探索，而对知识令牌使用较强的约束以保持事实知识。", "result": "在多个数学推理和代码生成基准测试中，Archer显著优于之前的RLVR方法，达到或超过了同等规模模型的最新性能。", "conclusion": "Archer通过区分处理知识型和推理型令牌，有效地提升了大型语言模型的推理能力，证明了熵感知训练信号的重要性。"}}
{"id": "2507.15150", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15150", "abs": "https://arxiv.org/abs/2507.15150", "authors": ["Aayush Atul Verma", "Arpitsinh Vaghela", "Bharatesh Chakravarthi", "Kaustav Chanda", "Yezhou Yang"], "title": "Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection", "comment": null, "summary": "Event-based sensors offer high temporal resolution and low latency by\ngenerating sparse, asynchronous data. However, converting this irregular data\ninto dense tensors for use in standard neural networks diminishes these\ninherent advantages, motivating research into graph representations. While such\nmethods preserve sparsity and support asynchronous inference, their performance\non downstream tasks remains limited due to suboptimal modeling of\nspatiotemporal dynamics. In this work, we propose a novel spatiotemporal\nmultigraph representation to better capture spatial structure and temporal\nchanges. Our approach constructs two decoupled graphs: a spatial graph\nleveraging B-spline basis functions to model global structure, and a temporal\ngraph utilizing motion vector-based attention for local dynamic changes. This\ndesign enables the use of efficient 2D kernels in place of computationally\nexpensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM\ndatasets for event-based object detection, achieving over a 6% improvement in\ndetection accuracy compared to previous graph-based works, with a 5x speedup,\nreduced parameter count, and no increase in computational cost. These results\nhighlight the effectiveness of structured graph modeling for asynchronous\nvision. Project page: eventbasedvision.github.io/eGSMV.", "AI": {"tldr": "本文提出一种新颖的时空多图表示，用于事件相机数据处理，通过解耦空间和时间图，显著提升了事件基对象检测的精度和效率。", "motivation": "事件相机数据具有高时间分辨率和低延迟的优势，但将其转换为密集张量会削弱这些优势。现有图表示方法虽然保留了稀疏性并支持异步推理，但由于时空动态建模不佳，在下游任务中的性能受限。", "method": "提出了一种时空多图表示（eGSMV），包含两个解耦的图：一个利用B样条基函数建模全局结构的空间图，以及一个利用运动向量注意力建模局部动态变化的 GAT 结构时间图。这种设计允许使用高效的2D卷积核代替计算成本高的3D卷积核。", "result": "在Gen1汽车和eTraM数据集上进行事件基对象检测，与现有基于图的方法相比，检测精度提高了6%以上，速度提升了5倍，参数量减少，计算成本没有增加。", "conclusion": "研究结果强调了结构化图建模在异步视觉任务中的有效性。"}}
{"id": "2507.15823", "categories": ["cs.CL", "cs.AI", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15823", "abs": "https://arxiv.org/abs/2507.15823", "authors": ["Anton Abilov", "Ke Zhang", "Hemank Lamba", "Elizabeth M. Olson", "Joel R. Tetreault", "Alejandro Jaimes"], "title": "Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work", "comment": null, "summary": "Publications in the AI for Good space have tended to focus on the research\nand model development that can support high-impact applications. However, very\nfew AI for Good papers discuss the process of deploying and collaborating with\nthe partner organization, and the resulting real-world impact. In this work, we\nshare details about the close collaboration with a humanitarian-to-humanitarian\n(H2H) organization and how to not only deploy the AI model in a\nresource-constrained environment, but also how to maintain it for continuous\nperformance updates, and share key takeaways for practitioners.", "AI": {"tldr": "本文分享了与人道主义组织合作，在资源受限环境中部署和维护AI模型，并提供了实践经验。", "motivation": "现有的“AI向善”研究多集中于模型开发，但很少讨论AI模型的部署、与合作方的协作过程以及实际影响力。", "method": "通过与一个人道主义组织（H2H）的紧密合作，详细阐述了在资源受限环境中部署AI模型的方法，以及如何进行持续的性能维护。", "result": "成功在资源受限环境中部署并维护了AI模型，并总结了对实践者有用的关键经验。", "conclusion": "强调了在“AI向善”项目中，与合作伙伴的紧密协作、在实际环境中部署和持续维护AI模型的重要性。"}}
{"id": "2507.15779", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15779", "abs": "https://arxiv.org/abs/2507.15779", "authors": ["Felix Köster", "Atsushi Uchida"], "title": "Reservoir Computing as a Language Model", "comment": "8 pages, 5 figures, 1 table", "summary": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.", "AI": {"tldr": "本文比较了字符级语言模型中Transformer和两种储层计算（RC）方法的性能、计算成本和预测精度，发现Transformer在预测质量上表现出色，而RC在训练和推理速度上效率更高。", "motivation": "大型语言模型（LLM）虽然性能强大，但其巨大的能耗和缓慢的处理速度是进一步提升质量并普及的瓶颈。储层计算（RC）有望实现快速、节能的硬件实现，以解决这一瓶颈。", "method": "研究者比较了三种字符级语言建模方法：两种储层计算方法（其中只有输出层可训练，包括传统静态线性读取和注意力增强型）以及Transformer架构（完全学习基于注意力的序列表示）。通过统一的管道，在保持可训练参数数量相同的前提下，评估了它们的性能、计算成本和预测准确性。", "result": "结果显示，Transformer在预测质量上表现优异，而储层计算模型在降低训练和推理速度方面表现出高效率。研究还深入探讨了传统储层计算和注意力增强型储层计算的特性。", "conclusion": "研究结果揭示了这些范式如何扩展，并为在资源限制和性能之间取得平衡提供了指导。储层计算为能源效率提供了一条可行路径，而Transformer则在性能上占据优势。"}}
{"id": "2507.15212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15212", "abs": "https://arxiv.org/abs/2507.15212", "authors": ["Yusuke Yoshiyasu", "Leyuan Sun", "Ryusuke Sagawa"], "title": "MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction", "comment": "Accepted at ICCV2025", "summary": "In this paper, we introduce MeshMamba, a neural network model for learning 3D\narticulated mesh models by employing the recently proposed Mamba State Space\nModels (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large\nnumber of input tokens, enabling the generation and reconstruction of body mesh\nmodels with more than 10,000 vertices, capturing clothing and hand geometries.\nThe key to effectively learning MeshMamba is the serialization technique of\nmesh vertices into orderings that are easily processed by Mamba. This is\nachieved by sorting the vertices based on body part annotations or the 3D\nvertex locations of a template mesh, such that the ordering respects the\nstructure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,\na denoising diffusion model for generating 3D articulated meshes and 2)\nMamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape\nand pose from a single image. Experimental results showed that MambaDiff3D can\ngenerate dense 3D human meshes in clothes, with grasping hands, etc., and\noutperforms previous approaches in the 3D human shape generation task.\nAdditionally, Mamba-HMR extends the capabilities of previous non-parametric\nhuman mesh recovery approaches, which were limited to handling body-only poses\nusing around 500 vertex tokens, to the whole-body setting with face and hands,\nwhile achieving competitive performance in (near) real-time.", "AI": {"tldr": "本文提出了MeshMamba，一个基于Mamba状态空间模型的新型神经网络，用于学习3D关节网格模型，能高效处理超过10,000个顶点的输入，实现包含衣物和手部细节的高分辨率人体网格生成与重建。", "motivation": "现有方法在处理大量输入标记时效率和可扩展性不足，难以生成和重建包含衣物和手部几何形状的、超过10,000个顶点的高分辨率3D人体网格模型。", "method": "引入MeshMamba，一个基于Mamba状态空间模型（Mamba-SSMs）的神经网络。核心方法是通过基于身体部位注释或模板网格的3D顶点位置对网格顶点进行序列化排序，使Mamba能有效处理。基于MeshMamba，设计了MambaDiff3D（用于生成3D关节网格的去噪扩散模型）和Mamba-HMR（用于从单张图像重建人体形状和姿态的3D人体网格恢复模型）。", "result": "实验结果显示，MambaDiff3D能生成带衣物和抓握手的密集3D人体网格，并在3D人体形状生成任务中优于现有方法。Mamba-HMR将现有非参数人体网格恢复方法的局限性（仅处理约500个顶点、仅限身体姿态）扩展到包含面部和手部的全身设置（超过10,000个顶点），并实现了接近实时的竞争性性能。", "conclusion": "MeshMamba通过利用Mamba-SSMs和创新的顶点序列化技术，成功克服了处理高分辨率3D关节网格模型的挑战，显著提升了3D人体形状生成和恢复的能力，尤其在细节（如衣物和手部）捕捉方面表现出色，并能实现高效的实时处理。"}}
{"id": "2507.15849", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15849", "abs": "https://arxiv.org/abs/2507.15849", "authors": ["Yihao Li", "Jiayi Xin", "Miranda Muqing Miao", "Qi Long", "Lyle Ungar"], "title": "The Impact of Language Mixing on Bilingual LLM Reasoning", "comment": null, "summary": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior.", "AI": {"tldr": "研究发现双语大语言模型（LLMs）中的语言混合（在思维链中切换语言）是一种有益的推理策略行为，尤其是在数学推理任务中，强制单语会降低准确性，而通过探针引导语言切换可提高准确性。", "motivation": "熟练的多语使用者常在对话中切换语言。类似地，DeepSeek-R1等双语LLMs也表现出语言混合现象，且发现阻止此行为会降低准确性。这表明语言混合可能对推理有益，因此研究旨在深入探讨中英双语推理模型中的语言切换行为。", "method": "研究方法包括：识别出可验证奖励的强化学习（RLVR）是导致语言混合的关键训练阶段；通过强制单语解码来评估其对准确性的影响；训练一个轻量级探针来预测潜在的语言切换是否对推理有利或有害；并利用该探针指导解码过程以提升性能。", "result": "研究结果显示：RLVR是导致语言混合的关键训练阶段；强制单语解码在数学推理任务上使准确率降低了5.6个百分点；一个轻量级探针能够预测潜在的语言切换是否有利；当使用该探针引导解码时，准确率最高可提高6.25个百分点。", "conclusion": "研究结论是，语言混合并非仅仅是多语言训练的副产品，而是一种战略性的推理行为，对双语LLMs的推理能力具有积极的促进作用。"}}
{"id": "2507.15850", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15850", "abs": "https://arxiv.org/abs/2507.15850", "authors": ["Basma El Amel Boussaha", "Leen AlQadi", "Mugariya Farooq", "Shaikha Alsuwaidi", "Giulia Campesan", "Ahmed Alzubaidi", "Mohammed Alyafeai", "Hakim Hacid"], "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking", "comment": null, "summary": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.", "AI": {"tldr": "该研究推出了3LM，一套专门针对阿拉伯语LLM的基准测试集，旨在填补现有阿拉伯语LLM基准在STEM和代码领域空白。", "motivation": "尽管阿拉伯语是世界上使用最广泛的语言之一，但针对阿拉伯语的大型语言模型（LLM）的开发和评估工作相对有限。现有的大多数阿拉伯语基准侧重于语言、文化或宗教内容，在STEM和代码等日益重要的领域存在显著空白。", "method": "该研究提出了3LM，一套包含三个基准测试的套件：1. STEM相关问答对，来源于阿拉伯语教科书和教育工作表。2. 合成生成的STEM问题，使用相同来源。3. 代码生成基准，通过人工参与和多轮审查，精心翻译了两个广泛使用的代码基准。", "result": "该研究成功构建并发布了3LM，一套包含自然来源、合成生成STEM问题以及高质量翻译代码生成问题的阿拉伯语LLM基准测试集。", "conclusion": "3LM的发布旨在支持阿拉伯语LLM在STEM和代码这些重要但代表性不足领域的研究发展。"}}
{"id": "2507.15216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15216", "abs": "https://arxiv.org/abs/2507.15216", "authors": ["Yuping Qiu", "Rui Zhu", "Ying-cong Chen"], "title": "Improving Joint Embedding Predictive Architecture with Diffusion Noise", "comment": null, "summary": "Self-supervised learning has become an incredibly successful method for\nfeature learning, widely applied to many downstream tasks. It has proven\nespecially effective for discriminative tasks, surpassing the trending\ngenerative models. However, generative models perform better in image\ngeneration and detail enhancement. Thus, it is natural for us to find a\nconnection between SSL and generative models to further enhance the\nrepresentation capacity of SSL. As generative models can create new samples by\napproximating the data distribution, such modeling should also lead to a\nsemantic understanding of the raw visual data, which is necessary for\nrecognition tasks. This enlightens us to combine the core principle of the\ndiffusion model: diffusion noise, with SSL to learn a competitive recognition\nmodel. Specifically, diffusion noise can be viewed as a particular state of\nmask that reveals a close relationship between masked image modeling (MIM) and\ndiffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to\nincorporate diffusion noise into MIM by the position embedding of masked\ntokens. The multi-level noise schedule is a series of feature augmentations to\nfurther enhance the robustness of our model. We perform a comprehensive study\nto confirm its effectiveness in the classification of downstream tasks. Codes\nwill be released soon in public.", "AI": {"tldr": "本文提出N-JEPA，通过将扩散噪声融入掩码图像建模（MIM）中，以增强自监督学习（SSL）在识别任务中的表现。", "motivation": "自监督学习在判别任务中表现出色，但生成模型在图像生成和细节增强方面更优。生成模型通过近似数据分布来创建新样本，这种建模应能带来对原始视觉数据的语义理解，这对识别任务至关重要。因此，研究者旨在结合扩散模型的核心原理（扩散噪声）与SSL，以提升其表示能力和识别性能。", "method": "提出N-JEPA（基于噪声的JEPA），通过掩码标记的位置嵌入将扩散噪声融入到掩码图像建模（MIM）中。此外，采用多级噪声调度作为一系列特征增强，以进一步提升模型的鲁棒性。", "result": "通过全面的研究，证实了所提方法在下游分类任务中的有效性。", "conclusion": "将扩散噪声与掩码图像建模相结合（N-JEPA），可以有效提升自监督学习模型在识别任务中的性能和鲁棒性。"}}
{"id": "2507.15852", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15852", "abs": "https://arxiv.org/abs/2507.15852", "authors": ["Zhixiong Zhang", "Shuangrui Ding", "Xiaoyi Dong", "Songxin He", "Jianfan Lin", "Junsong Tang", "Yuhang Zang", "Yuhang Cao", "Dahua Lin", "Jiaqi Wang"], "title": "SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction", "comment": "project page: https://rookiexiong7.github.io/projects/SeC/ ; code:\n  https://github.com/OpenIXCLab/SeC ; dataset:\n  https://huggingface.co/datasets/OpenIXCLab/SeCVOS", "summary": "Video Object Segmentation (VOS) is a core task in computer vision, requiring\nmodels to track and segment target objects across video frames. Despite notable\nadvances with recent efforts, current techniques still lag behind human\ncapabilities in handling drastic visual variations, occlusions, and complex\nscene changes. This limitation arises from their reliance on appearance\nmatching, neglecting the human-like conceptual understanding of objects that\nenables robust identification across temporal dynamics. Motivated by this gap,\nwe propose Segment Concept (SeC), a concept-driven segmentation framework that\nshifts from conventional feature matching to the progressive construction and\nutilization of high-level, object-centric representations. SeC employs Large\nVision-Language Models (LVLMs) to integrate visual cues across diverse frames,\nconstructing robust conceptual priors. During inference, SeC forms a\ncomprehensive semantic representation of the target based on processed frames,\nrealizing robust segmentation of follow-up frames. Furthermore, SeC adaptively\nbalances LVLM-based semantic reasoning with enhanced feature matching,\ndynamically adjusting computational efforts based on scene complexity. To\nrigorously assess VOS methods in scenarios demanding high-level conceptual\nreasoning and robust semantic understanding, we introduce the Semantic Complex\nScenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160\nmanually annotated multi-scenario videos designed to challenge models with\nsubstantial appearance variations and dynamic scene transformations. In\nparticular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,\nestablishing a new state-of-the-art in concept-aware video object segmentation.", "AI": {"tldr": "本文提出了一种名为Segment Concept (SeC) 的概念驱动视频目标分割（VOS）框架，通过利用大型视觉语言模型（LVLMs）实现对目标的深层语义理解，并引入了新的复杂场景VOS基准（SeCVOS），SeC在该基准上取得了显著提升。", "motivation": "现有VOS技术在处理剧烈视觉变化、遮挡和复杂场景时表现不佳，原因在于它们过度依赖外观匹配，而忽略了人类在识别目标时所具备的概念理解能力。研究旨在弥补这一差距。", "method": "本文提出了SeC框架，它从传统的特征匹配转向逐步构建和利用高级、以目标为中心的表示。SeC利用LVLMs整合不同帧的视觉线索，构建鲁棒的概念先验。在推理阶段，SeC基于已处理的帧形成目标的全面语义表示，以实现后续帧的鲁棒分割。此外，SeC自适应地平衡基于LVLM的语义推理与增强的特征匹配，根据场景复杂性动态调整计算量。为严格评估需要高级概念推理和鲁棒语义理解的VOS方法，本文引入了语义复杂场景视频目标分割基准（SeCVOS）。", "result": "SeC在SeCVOS基准上比SAM 2.1提升了11.8个点，在概念感知视频目标分割领域建立了新的最先进水平。", "conclusion": "通过引入概念驱动的SeC框架和新的SeCVOS基准，本文显著提升了VOS在处理复杂视觉变化和场景时的鲁棒性，证明了高层概念理解在视频目标分割中的重要性。"}}
{"id": "2507.15223", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15223", "abs": "https://arxiv.org/abs/2507.15223", "authors": ["Siqi Chen", "Guoqing Zhang", "Jiahao Lai", "Bingzhi Shen", "Sihong Zhang", "Caixia Dong", "Xuejin Chen", "Yang Li"], "title": "Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel", "comment": null, "summary": "Advancements in 3D vision have increased the impact of blood vessel modeling\non medical applications. However, accurately representing the complex geometry\nand topology of blood vessels remains a challenge due to their intricate\nbranching patterns, curvatures, and irregular shapes. In this study, we propose\na hierarchical part-based frame work for 3D vessel generation that separates\nthe global binary tree-like topology from local geometric details. Our approach\nproceeds in three stages: (1) key graph generation to model the overall\nhierarchical struc ture, (2) vessel segment generation conditioned on geometric\nproperties, and (3) hierarchical vessel assembly by integrating the local\nsegments according to the global key graph. We validate our framework on real\nworld datasets, demonstrating superior performance over existing methods in\nmodeling complex vascular networks. This work marks the first successful\napplication of a part-based generative approach for 3D vessel modeling, setting\na new benchmark for vascular data generation. The code is available at:\nhttps://github.com/CybercatChen/PartVessel.git.", "AI": {"tldr": "本文提出了一种分层、基于部件的3D血管生成框架，通过分离全局拓扑和局部几何细节，解决了复杂血管网络建模的挑战，并取得了优于现有方法的性能。", "motivation": "由于血血管复杂的几何形状、拓扑结构、分支模式、曲率和不规则形状，准确表示它们仍然是一个挑战，这限制了3D视觉在医疗应用中对血血管建模的影响。", "method": "研究提出了一种分层、基于部件的3D血管生成框架，将全局二叉树状拓扑与局部几何细节分离。该方法分三个阶段进行：(1) 关键图生成，用于建模整体分层结构；(2) 基于几何属性的血管段生成；(3) 根据全局关键图整合局部血管段的分层血管组装。", "result": "该框架在真实世界数据集上得到了验证，在建模复杂血管网络方面表现出优于现有方法的性能。这是首次成功将基于部件的生成方法应用于3D血管建模，为血管数据生成树立了新基准。", "conclusion": "所提出的分层、基于部件的3D血管生成框架能够有效建模复杂的血管网络，并显著优于现有方法，为3D血管建模领域树立了新的里程碑。"}}
{"id": "2507.15227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15227", "abs": "https://arxiv.org/abs/2507.15227", "authors": ["Krishna Kanth Nakka"], "title": "Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders", "comment": "Preprint. Under review", "summary": "Interpretability is critical in high-stakes domains such as medical imaging,\nwhere understanding model decisions is essential for clinical adoption. In this\nwork, we introduce Sparse Autoencoder (SAE)-based interpretability to breast\nimaging by analyzing {Mammo-CLIP}, a vision--language foundation model\npretrained on large-scale mammogram image--report pairs. We train a patch-level\n\\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features\nassociated with clinically relevant breast concepts such as \\textit{mass} and\n\\textit{suspicious calcification}. Our findings reveal that top activated class\nlevel latent neurons in the SAE latent space often tend to align with ground\ntruth regions, and also uncover several confounding factors influencing the\nmodel's decision-making process. Additionally, we analyze which latent neurons\nthe model relies on during downstream finetuning for improving the breast\nconcept prediction. This study highlights the promise of interpretable SAE\nlatent representations in providing deeper insight into the internal workings\nof foundation models at every layer for breast imaging.", "AI": {"tldr": "通过稀疏自编码器（SAE）分析Mammo-CLIP模型，提高乳腺影像诊断中基础模型的决策可解释性。", "motivation": "在医疗影像等高风险领域，理解模型决策对于临床应用至关重要，因此需要提高模型的可解释性。", "method": "引入基于SAE的可解释性方法，分析预训练的视觉-语言基础模型Mammo-CLIP。训练一个补丁级别的Mammo-SAE来识别和探测与临床相关乳腺概念（如肿块、可疑钙化）相关的潜在特征。", "result": "发现SAE潜在空间中激活度最高的类级别潜在神经元常与真实区域对齐，并揭示了影响模型决策的几个混淆因素。此外，分析了模型在下游微调中为改善乳腺概念预测所依赖的潜在神经元。", "conclusion": "可解释的SAE潜在表示有望为乳腺影像领域的基础模型内部工作机制提供更深入的洞察。"}}
{"id": "2507.15249", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15249", "abs": "https://arxiv.org/abs/2507.15249", "authors": ["Yanbing Zhang", "Zhe Wang", "Qin Zhou", "Mengping Yang"], "title": "FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers", "comment": "Accepted by ICCV 2025", "summary": "In light of recent breakthroughs in text-to-image (T2I) generation,\nparticularly with diffusion transformers (DiT), subject-driven technologies are\nincreasingly being employed for high-fidelity customized production that\npreserves subject identity from reference inputs, enabling thrilling design\nworkflows and engaging entertainment. Existing alternatives typically require\neither per-subject optimization via trainable text embeddings or training\nspecialized encoders for subject feature extraction on large-scale datasets.\nSuch dependencies on training procedures fundamentally constrain their\npractical applications. More importantly, current methodologies fail to fully\nleverage the inherent zero-shot potential of modern diffusion transformers\n(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this\ngap, we propose FreeCus, a genuinely training-free framework that activates\nDiT's capabilities through three key innovations: 1) We introduce a pivotal\nattention sharing mechanism that captures the subject's layout integrity while\npreserving crucial editing flexibility. 2) Through a straightforward analysis\nof DiT's dynamic shifting, we propose an upgraded variant that significantly\nimproves fine-grained feature extraction. 3) We further integrate advanced\nMultimodal Large Language Models (MLLMs) to enrich cross-modal semantic\nrepresentations. Extensive experiments reflect that our method successfully\nunlocks DiT's zero-shot ability for consistent subject synthesis across diverse\ncontexts, achieving state-of-the-art or comparable results compared to\napproaches that require additional training. Notably, our framework\ndemonstrates seamless compatibility with existing inpainting pipelines and\ncontrol modules, facilitating more compelling experiences. Our code is\navailable at: https://github.com/Monalissaa/FreeCus.", "AI": {"tldr": "FreeCus是一种无需训练的框架，通过注意力共享、DiT改进和MLLM集成，充分利用扩散Transformer的零样本能力，实现高质量、一致性的主题驱动图像生成，优于或媲美需要额外训练的方法。", "motivation": "现有的主题驱动文本到图像生成方法通常需要对每个主题进行优化或训练专门的编码器，这限制了其实际应用。更重要的是，它们未能充分利用现代扩散Transformer（如DiT）固有的零样本潜力来实现真实的主题驱动合成。", "method": "本文提出了FreeCus框架，通过三项关键创新激活DiT的能力：1) 引入了关键的注意力共享机制，捕捉主题布局完整性并保持编辑灵活性。2) 通过分析DiT的动态偏移，提出了改进的DiT变体，显著提升细粒度特征提取。3) 集成了先进的多模态大型语言模型（MLLMs），以丰富跨模态语义表示。", "result": "实验证明，FreeCus成功解锁了DiT的零样本能力，实现了跨多样上下文的一致主题合成，取得了与需要额外训练的方法相媲美或更优的SOTA结果。此外，该框架与现有的图像修复和控制模块无缝兼容。", "conclusion": "FreeCus提供了一种真正无需训练的解决方案，有效利用了扩散Transformer的零样本能力，实现了高质量的主题驱动图像生成，其性能可与需要训练的方法相媲美，并具有出色的兼容性，为实际应用带来了显著优势。"}}
{"id": "2507.15257", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15257", "abs": "https://arxiv.org/abs/2507.15257", "authors": ["Pei An", "Jiaqi Yang", "Muyao Peng", "You Yang", "Qiong Liu", "Xiaolin Wu", "Liangliang Nan"], "title": "MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP", "comment": "Accepted by ICCV 2025", "summary": "Image-to-point-cloud (I2P) registration is a fundamental problem in computer\nvision, focusing on establishing 2D-3D correspondences between an image and a\npoint cloud. The differential perspective-n-point (PnP) has been widely used to\nsupervise I2P registration networks by enforcing the projective constraints on\n2D-3D correspondences. However, differential PnP is highly sensitive to noise\nand outliers in the predicted correspondences. This issue hinders the\neffectiveness of correspondence learning. Inspired by the robustness of blind\nPnP against noise and outliers in correspondences, we propose an approximated\nblind PnP based correspondence learning approach. To mitigate the high\ncomputational cost of blind PnP, we simplify blind PnP to an amenable task of\nminimizing Chamfer distance between learned 2D and 3D keypoints, called\nMinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task\nlearning module, named as MinCD-Net, which can be easily integrated into the\nexisting I2P registration architectures. Extensive experiments on 7-Scenes,\nRGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net\noutperforms state-of-the-art methods and achieves a higher inlier ratio (IR)\nand registration recall (RR) in both cross-scene and cross-dataset settings.", "AI": {"tldr": "本文提出了一种名为 MinCD-PnP 的新颖图像到点云配准对应学习方法，通过近似盲 PnP 并最小化 Chamfer 距离，解决了传统微分 PnP 对噪声敏感的问题，并设计了轻量级多任务网络 MinCD-Net，在多个数据集上表现优于现有方法。", "motivation": "现有的图像到点云（I2P）配准方法中，微分透视-n-点（PnP）在监督对应学习时对预测对应中的噪声和离群点高度敏感，这阻碍了对应学习的有效性。", "method": "受盲 PnP 对噪声和离群点鲁棒性的启发，本文提出了一种近似盲 PnP 的对应学习方法。为降低盲 PnP 的计算成本，将其简化为最小化学习到的 2D 和 3D 关键点之间 Chamfer 距离的任务，命名为 MinCD-PnP。为有效解决 MinCD-PnP，设计了一个轻量级多任务学习模块 MinCD-Net，该模块易于集成到现有 I2P 配准架构中。", "result": "在 7-Scenes、RGBD-V2、ScanNet 和自收集数据集上的大量实验表明，MinCD-Net 在跨场景和跨数据集设置下，均优于最先进的方法，并实现了更高的内点比（IR）和配准召回率（RR）。", "conclusion": "所提出的基于近似盲 PnP 的 MinCD-Net 方法，通过解决微分 PnP 对噪声的敏感性问题，为图像到点云配准中的对应学习提供了一种更鲁棒和有效的解决方案，并取得了优异的性能。"}}
{"id": "2507.15285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15285", "abs": "https://arxiv.org/abs/2507.15285", "authors": ["Lazaro Janier Gonzalez-Soler", "Maciej Salwowski", "Christoph Busch"], "title": "In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems", "comment": "Submitted to IEEE-TIFS", "summary": "Recent advances in biometric systems have significantly improved the\ndetection and prevention of fraudulent activities. However, as detection\nmethods improve, attack techniques become increasingly sophisticated. Attacks\non face recognition systems can be broadly divided into physical and digital\napproaches. Traditionally, deep learning models have been the primary defence\nagainst such attacks. While these models perform exceptionally well in\nscenarios for which they have been trained, they often struggle to adapt to\ndifferent types of attacks or varying environmental conditions. These\nsubsystems require substantial amounts of training data to achieve reliable\nperformance, yet biometric data collection faces significant challenges,\nincluding privacy concerns and the logistical difficulties of capturing diverse\nattack scenarios under controlled conditions. This work investigates the\napplication of Vision Language Models (VLM) and proposes an in-context learning\nframework for detecting physical presentation attacks and digital morphing\nattacks in biometric systems. Focusing on open-source models, the first\nsystematic framework for the quantitative evaluation of VLMs in\nsecurity-critical scenarios through in-context learning techniques is\nestablished. The experimental evaluation conducted on freely available\ndatabases demonstrates that the proposed subsystem achieves competitive\nperformance for physical and digital attack detection, outperforming some of\nthe traditional CNNs without resource-intensive training. The experimental\nresults validate the proposed framework as a promising tool for improving\ngeneralisation in attack detection.", "AI": {"tldr": "该研究提出了一种基于视觉语言模型（VLM）和上下文学习（in-context learning）的框架，用于检测生物识别系统中的物理呈现攻击和数字变形攻击，在无需大量训练的情况下取得了有竞争力的性能。", "motivation": "生物识别系统面临日益复杂的攻击（如物理攻击和数字变形攻击）。传统的深度学习模型（如CNN）在未训练过的攻击类型或环境条件下泛化能力差，且需要大量训练数据，而生物识别数据收集存在隐私和物流挑战。", "method": "该研究探索了视觉语言模型（VLM）的应用，并提出了一个用于检测物理呈现攻击和数字变形攻击的上下文学习框架。它专注于开源模型，并建立了首个通过上下文学习技术在安全关键场景中对VLM进行定量评估的系统框架。", "result": "在公开数据库上的实验评估表明，所提出的子系统在物理和数字攻击检测方面取得了有竞争力的性能，超越了一些传统的CNN模型，且无需资源密集型训练。", "conclusion": "所提出的基于VLM的上下文学习框架被验证为一种有前景的工具，能够有效提升生物识别攻击检测的泛化能力。"}}
{"id": "2507.15297", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15297", "abs": "https://arxiv.org/abs/2507.15297", "authors": ["Zhiyu Pan", "Xiongjun Guan", "Yongjie Duan", "Jianjiang Feng", "Jie Zhou"], "title": "Minutiae-Anchored Local Dense Representation for Fingerprint Matching", "comment": "Under review", "summary": "Fingerprint matching under diverse capture conditions remains a fundamental\nchallenge in biometric recognition. To achieve robust and accurate performance\nin such scenarios, we propose DMD, a minutiae-anchored local dense\nrepresentation which captures both fine-grained ridge textures and\ndiscriminative minutiae features in a spatially structured manner.\nSpecifically, descriptors are extracted from local patches centered and\noriented on each detected minutia, forming a three-dimensional tensor, where\ntwo dimensions represent spatial locations on the fingerprint plane and the\nthird encodes semantic features. This representation explicitly captures\nabstract features of local image patches, enabling a multi-level, fine-grained\ndescription that aggregates information from multiple minutiae and their\nsurrounding ridge structures. Furthermore, thanks to its strong spatial\ncorrespondence with the patch image, DMD allows for the use of foreground\nsegmentation masks to identify valid descriptor regions. During matching,\ncomparisons are then restricted to overlapping foreground areas, improving\nefficiency and robustness. Extensive experiments on rolled, plain, parital,\ncontactless, and latent fingerprint datasets demonstrate the effectiveness and\ngeneralizability of the proposed method. It achieves state-of-the-art accuracy\nacross multiple benchmarks while maintaining high computational efficiency,\nshowing strong potential for large-scale fingerprint recognition. Corresponding\ncode is available at https://github.com/Yu-Yy/DMD.", "AI": {"tldr": "本文提出DMD，一种以细节特征点为锚点的局部密集表示，用于在多样化采集条件下实现鲁棒、高效的指纹匹配。", "motivation": "在多样化的采集条件下，指纹匹配仍然是生物识别领域的一个基本挑战，需要更鲁棒和准确的性能。", "method": "提出DMD（minutiae-anchored local dense representation），它以检测到的细节特征点为中心和方向，从局部图像块中提取描述符，形成一个三维张量（两维空间位置，一维语义特征）。该表示捕获精细的脊线纹理和判别性细节特征。此外，利用前景分割掩码识别有效描述符区域，并在匹配时将比较限制在重叠的前景区域，以提高效率和鲁棒性。", "result": "在多种指纹数据集（包括滚动、平面、局部、非接触和潜在指纹）上的广泛实验表明，所提出方法有效且具有泛化性。它在多个基准测试中达到了最先进的准确性，同时保持了较高的计算效率，显示出在大规模指纹识别方面的强大潜力。", "conclusion": "DMD通过结合细节特征点和局部密集表示，有效地解决了多样化采集条件下指纹匹配的挑战，实现了高精度和高效率，具有广泛的应用前景。"}}
{"id": "2507.15308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15308", "abs": "https://arxiv.org/abs/2507.15308", "authors": ["Zhimeng Xin", "Tianxu Wu", "Yixiong Zou", "Shiming Chen", "Dingjie Fu", "Xinge You"], "title": "Few-Shot Object Detection via Spatial-Channel State Space Model", "comment": null, "summary": "Due to the limited training samples in few-shot object detection (FSOD), we\nobserve that current methods may struggle to accurately extract effective\nfeatures from each channel. Specifically, this issue manifests in two aspects:\ni) channels with high weights may not necessarily be effective, and ii)\nchannels with low weights may still hold significant value. To handle this\nproblem, we consider utilizing the inter-channel correlation to facilitate the\nnovel model's adaptation process to novel conditions, ensuring the model can\ncorrectly highlight effective channels and rectify those incorrect ones. Since\nthe channel sequence is also 1-dimensional, its similarity with the temporal\nsequence inspires us to take Mamba for modeling the correlation in the channel\nsequence. Based on this concept, we propose a Spatial-Channel State Space\nModeling (SCSM) module for spatial-channel state modeling, which highlights the\neffective patterns and rectifies those ineffective ones in feature channels. In\nSCSM, we design the Spatial Feature Modeling (SFM) module to balance the\nlearning of spatial relationships and channel relationships, and then introduce\nthe Channel State Modeling (CSM) module based on Mamba to learn correlation in\nchannels. Extensive experiments on the VOC and COCO datasets show that the SCSM\nmodule enables the novel detector to improve the quality of focused feature\nrepresentation in channels and achieve state-of-the-art performance.", "AI": {"tldr": "针对小样本目标检测中通道特征提取不准确的问题，本文提出了一种基于Mamba的SCSM模块，通过建模通道间关联性来增强特征表示，并取得了最先进的性能。", "motivation": "小样本目标检测（FSOD）中训练样本有限，导致现有方法难以准确提取有效的通道特征。具体表现为：高权重通道不一定有效，低权重通道可能仍有重要价值。研究旨在正确突出有效通道并纠正无效通道。", "method": "提出空间-通道状态空间建模（SCSM）模块。该模块包含两部分：1) 空间特征建模（SFM）模块，用于平衡空间和通道关系的学习；2) 通道状态建模（CSM）模块，受时间序列启发，基于Mamba模型来学习通道间的关联性，以突出有效模式并纠正无效模式。", "result": "在VOC和COCO数据集上进行的大量实验表明，SCSM模块能显著提高新型检测器在通道中聚焦特征表示的质量，并达到最先进的性能。", "conclusion": "SCSM模块通过利用Mamba模型对通道间关联性进行建模，有效解决了小样本目标检测中通道特征提取不准确的问题，从而提高了模型的特征表示能力和整体性能。"}}
{"id": "2507.15321", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15321", "abs": "https://arxiv.org/abs/2507.15321", "authors": ["Zhenyu Li", "Haotong Lin", "Jiashi Feng", "Peter Wonka", "Bingyi Kang"], "title": "BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?", "comment": "Webpage: https://zhyever.github.io/benchdepth", "summary": "Depth estimation is a fundamental task in computer vision with diverse\napplications. Recent advancements in deep learning have led to powerful depth\nfoundation models (DFMs), yet their evaluation remains challenging due to\ninconsistencies in existing protocols. Traditional benchmarks rely on\nalignment-based metrics that introduce biases, favor certain depth\nrepresentations, and complicate fair comparisons. In this work, we propose\nBenchDepth, a new benchmark that evaluates DFMs through five carefully selected\ndownstream proxy tasks: depth completion, stereo matching, monocular\nfeed-forward 3D scene reconstruction, SLAM, and vision-language spatial\nunderstanding. Unlike conventional evaluation protocols, our approach assesses\nDFMs based on their practical utility in real-world applications, bypassing\nproblematic alignment procedures. We benchmark eight state-of-the-art DFMs and\nprovide an in-depth analysis of key findings and observations. We hope our work\nsparks further discussion in the community on best practices for depth model\nevaluation and paves the way for future research and advancements in depth\nestimation.", "AI": {"tldr": "现有深度基础模型（DFM）的评估协议存在偏见和不一致性。本文提出BenchDepth，一个通过五个下游代理任务（如深度补全、SLAM等）评估DFM实用性的新基准，避免了传统对齐方法的弊端，并对八个最先进的DFM进行了基准测试。", "motivation": "尽管深度学习在深度估计领域取得了显著进展，产生了强大的深度基础模型（DFM），但现有的评估协议存在不一致性，难以进行公平比较。传统的基准测试依赖于基于对齐的指标，这些指标引入了偏见，偏爱某些深度表示，并使公平比较复杂化。", "method": "本文提出了BenchDepth，一个新的基准，通过五个精心选择的下游代理任务来评估DFM：深度补全、立体匹配、单目前馈3D场景重建、SLAM和视觉-语言空间理解。这种方法根据DFM在实际应用中的实用性进行评估，绕过了有问题的对齐程序。研究人员对八个最先进的DFM进行了基准测试。", "result": "研究人员对八个最先进的深度基础模型进行了基准测试，并提供了对关键发现和观察的深入分析。", "conclusion": "BenchDepth提供了一种评估深度模型的新方法，更侧重于其在实际应用中的实用性，并希望该工作能激发社区对深度模型评估最佳实践的进一步讨论，并为深度估计的未来研究和发展铺平道路。"}}
{"id": "2507.15346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15346", "abs": "https://arxiv.org/abs/2507.15346", "authors": ["Muhammad Aqeel", "Kidus Dagnaw Bellete", "Francesco Setti"], "title": "RoadFusion: Latent Diffusion Model for Pavement Defect Detection", "comment": "Accepted to ICIAP 2025", "summary": "Pavement defect detection faces critical challenges including limited\nannotated data, domain shift between training and deployment environments, and\nhigh variability in defect appearances across different road conditions. We\npropose RoadFusion, a framework that addresses these limitations through\nsynthetic anomaly generation with dual-path feature adaptation. A latent\ndiffusion model synthesizes diverse, realistic defects using text prompts and\nspatial masks, enabling effective training under data scarcity. Two separate\nfeature adaptors specialize representations for normal and anomalous inputs,\nimproving robustness to domain shift and defect variability. A lightweight\ndiscriminator learns to distinguish fine-grained defect patterns at the patch\nlevel. Evaluated on six benchmark datasets, RoadFusion achieves consistently\nstrong performance across both classification and localization tasks, setting\nnew state-of-the-art in multiple metrics relevant to real-world road\ninspection.", "AI": {"tldr": "RoadFusion是一个用于路面缺陷检测的框架，通过合成异常数据和双路径特征自适应来解决数据稀缺、领域漂移和缺陷多样性等挑战，并在多个基准测试中达到了最先进的性能。", "motivation": "路面缺陷检测面临标注数据有限、训练与部署环境之间存在领域漂移以及不同路况下缺陷外观高度可变等关键挑战。", "method": "该研究提出了RoadFusion框架，包含：1) 一个潜在扩散模型，利用文本提示和空间掩码合成多样且真实的缺陷，以应对数据稀缺；2) 两个独立的特征适配器，专门针对正常和异常输入学习表示，以提高对领域漂移和缺陷变异性的鲁棒性；3) 一个轻量级判别器，学习在补丁级别区分细粒度缺陷模式。", "result": "RoadFusion在六个基准数据集上进行了评估，在分类和定位任务中均表现出持续强大的性能，并在多项与实际道路检查相关的指标上设定了新的最先进水平。", "conclusion": "RoadFusion通过其创新的合成异常生成和双路径特征自适应方法，有效解决了路面缺陷检测中的数据和泛化性问题，显著提升了该领域的检测能力。"}}
{"id": "2507.15365", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15365", "abs": "https://arxiv.org/abs/2507.15365", "authors": ["Fatemeh Saleh", "Sadegh Aliakbarian", "Charlie Hewitt", "Lohit Petikam", "Xiao-Xian", "Antonio Criminisi", "Thomas J. Cashman", "Tadas Baltrušaitis"], "title": "DAViD: Data-efficient and Accurate Vision Models from Synthetic Data", "comment": "Accepted at ICCV 2025", "summary": "The state of the art in human-centric computer vision achieves high accuracy\nand robustness across a diverse range of tasks. The most effective models in\nthis domain have billions of parameters, thus requiring extremely large\ndatasets, expensive training regimes, and compute-intensive inference. In this\npaper, we demonstrate that it is possible to train models on much smaller but\nhigh-fidelity synthetic datasets, with no loss in accuracy and higher\nefficiency. Using synthetic training data provides us with excellent levels of\ndetail and perfect labels, while providing strong guarantees for data\nprovenance, usage rights, and user consent. Procedural data synthesis also\nprovides us with explicit control on data diversity, that we can use to address\nunfairness in the models we train. Extensive quantitative assessment on real\ninput images demonstrates accuracy of our models on three dense prediction\ntasks: depth estimation, surface normal estimation, and soft foreground\nsegmentation. Our models require only a fraction of the cost of training and\ninference when compared with foundational models of similar accuracy. Our\nhuman-centric synthetic dataset and trained models are available at\nhttps://aka.ms/DAViD.", "AI": {"tldr": "本文提出使用小规模、高保真度的合成数据集训练以人为中心的计算机视觉模型，实现了与大型真实数据集训练模型相当的准确性，同时显著提高了训练和推理效率。", "motivation": "当前最先进的人体中心计算机视觉模型参数量巨大，需要极大的数据集、昂贵的训练成本和计算密集型推理，这促使研究者探索更高效的数据和训练方法。", "method": "研究方法是利用合成数据训练模型。合成数据提供了极高的细节、完美的标签，并能保证数据来源、使用权和用户同意。此外，程序化数据合成还能精确控制数据多样性，以解决模型中的不公平性问题。", "result": "在真实输入图像上进行了广泛的定量评估，结果显示，模型在深度估计、表面法线估计和软前景分割三项密集预测任务上保持了高准确性。与同等准确度的基础模型相比，本文模型的训练和推理成本仅为其一小部分，且未损失准确性，效率更高。", "conclusion": "研究表明，使用小规模但高保真度的合成数据集，可以高效地训练出高准确性的人体中心计算机视觉模型，从而在保证性能的同时，显著降低了训练和推理成本，并提供了数据控制和公平性改进的潜力。"}}
{"id": "2507.15401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15401", "abs": "https://arxiv.org/abs/2507.15401", "authors": ["Huiyu Zhai", "Xingxing Yang", "Yalan Ye", "Chenyang Li", "Bin Fan", "Changze Li"], "title": "Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond", "comment": null, "summary": "Facial expression recognition (FER) is a challenging task due to pervasive\nocclusion and dataset biases. Especially when facial information is partially\noccluded, existing FER models struggle to extract effective facial features,\nleading to inaccurate classifications. In response, we present ORSANet, which\nintroduces the following three key contributions: First, we introduce auxiliary\nmulti-modal semantic guidance to disambiguate facial occlusion and learn\nhigh-level semantic knowledge, which is two-fold: 1) we introduce semantic\nsegmentation maps as dense semantics prior to generate semantics-enhanced\nfacial representations; 2) we introduce facial landmarks as sparse geometric\nprior to mitigate intrinsic noises in FER, such as identity and gender biases.\nSecond, to facilitate the effective incorporation of these two multi-modal\npriors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively\nfuse the landmark feature and semantics-enhanced representations within\ndifferent scales. Third, we design a Dynamic Adversarial Repulsion Enhancement\nLoss (DARELoss) that dynamically adjusts the margins of ambiguous classes,\nfurther enhancing the model's ability to distinguish similar expressions. We\nfurther construct the first occlusion-oriented FER dataset to facilitate\nspecialized robustness analysis on various real-world occlusion conditions,\ndubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER\ndemonstrate that our proposed ORSANet achieves SOTA recognition performance.\nCode is publicly available at https://github.com/Wenyuzhy/ORSANet-master.", "AI": {"tldr": "该论文提出了ORSANet，一个用于解决面部表情识别（FER）中遮挡和数据集偏差问题的模型，通过引入多模态语义指导、多尺度交叉交互模块和动态对抗排斥增强损失，并构建了一个新的遮挡导向FER数据集。", "motivation": "面部表情识别（FER）面临普遍存在的遮挡和数据集偏差挑战。当面部信息部分被遮挡时，现有模型难以提取有效特征，导致分类不准确。", "method": "ORSANet主要包含三点：1) 引入辅助多模态语义指导，包括语义分割图（提供密集语义）和面部地标（提供稀疏几何先验），以消除遮挡歧义并学习高级语义知识。2) 设计多尺度交叉交互模块（MCM），以有效地融合地标特征和语义增强表示。3) 提出动态对抗排斥增强损失（DARELoss），动态调整模糊类别的裕度，增强模型区分相似表情的能力。此外，论文还构建了首个遮挡导向的FER数据集Occlu-FER。", "result": "ORSANet在公共基准测试和Occlu-FER数据集上均取得了最先进的识别性能。", "conclusion": "ORSANet通过结合多模态语义先验、定制的特征融合机制和创新的损失函数，显著提升了在面部遮挡条件下的表情识别能力，并为遮挡导向的FER研究提供了新的数据集。"}}
{"id": "2507.15418", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15418", "abs": "https://arxiv.org/abs/2507.15418", "authors": ["Ka Young Kim", "Hyeon Bae Kim", "Seong Tae Kim"], "title": "SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition", "comment": "Accepted to MICCAI 2025", "summary": "Surgical phase recognition plays a crucial role in surgical workflow\nanalysis, enabling various applications such as surgical monitoring, skill\nassessment, and workflow optimization. Despite significant advancements in deep\nlearning-based surgical phase recognition, these models remain inherently\nopaque, making it difficult to understand how they make decisions. This lack of\ninterpretability hinders trust and makes it challenging to debug the model. To\naddress this challenge, we propose SurgX, a novel concept-based explanation\nframework that enhances the interpretability of surgical phase recognition\nmodels by associating neurons with relevant concepts. In this paper, we\nintroduce the process of selecting representative example sequences for\nneurons, constructing a concept set tailored to the surgical video dataset,\nassociating neurons with concepts and identifying neurons crucial for\npredictions. Through extensive experiments on two surgical phase recognition\nmodels, we validate our method and analyze the explanation for prediction. This\nhighlights the potential of our method in explaining surgical phase\nrecognition. The code is available at https://github.com/ailab-kyunghee/SurgX", "AI": {"tldr": "SurgX是一个概念驱动的解释框架，旨在提高手术阶段识别模型的透明度和可解释性，通过将神经元与相关概念关联起来。", "motivation": "深度学习在手术阶段识别方面虽有进展，但模型固有的不透明性阻碍了理解决策过程、建立信任和模型调试。", "method": "提出SurgX框架，具体方法包括：为神经元选择代表性示例序列、构建针对手术视频数据集的概念集、将神经元与概念关联、识别对预测至关重要的神经元。", "result": "在两个手术阶段识别模型上进行了广泛实验，验证了SurgX方法的有效性，并分析了预测的解释，突显了其解释手术阶段识别的潜力。", "conclusion": "SurgX通过将模型内部神经元与可理解的概念关联，成功提升了手术阶段识别模型的可解释性，有助于增强信任和改进模型调试。"}}
{"id": "2507.15480", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15480", "abs": "https://arxiv.org/abs/2507.15480", "authors": ["Liang Chen", "Ghazi Shazan Ahmad", "Tianjun Yao", "Lingqiao Liu", "Zhiqiang Shen"], "title": "One Last Attention for Your Vision-Language Model", "comment": "Accepted by ICCV 2025", "summary": "Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable\nzero-shot performance, yet their downstream potential hinges on effective\nfine-tuning. Most adaptation methods typically focus on refining representation\nfrom separate modalities (text or vision) but neglect the critical role of\ntheir fused representations in the decision-making process, \\emph{\\ie} rational\nmatrix that drives the final prediction. To bridge the gap, we propose a simple\nyet effective \\textbf{R}ational \\textbf{Ada}ptaion ({RAda}) to explicitly\nexploit the final fused representation during fine-tuning. RAda employs a\nlearned mask, obtained from a lightweight attention layer attached at the end\nof a VLM, to dynamically calibrate the contribution of each element in the\nrational matrix, enabling targeted adjustments to the final cross-modal\ninteractions without incurring costly modifications to intermediate features.\nExperiments in different settings (i.e., updating, or freezing pretrained\nencoders in adaptation, and test-time training that can only access the\nunlabeled test data) show that RAda serves as a versatile fine-tuning\ntechnique, improving the baseline with minimal code and performing comparably\nagainst current arts in most settings. Code is available at\n\\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.", "AI": {"tldr": "RAda是一种简单高效的视觉-语言模型（VLM）微调方法，通过引入一个学习到的掩码来动态调整最终决策层（理性矩阵）中跨模态交互的贡献，从而提升零样本性能。", "motivation": "现有的VLM适应方法通常只关注于单独模态（文本或视觉）的表示精炼，却忽略了融合表示（即驱动最终预测的理性矩阵）在决策过程中的关键作用，因此需要弥补这一空白。", "method": "本文提出了理性适应（RAda）方法。它在VLM末端附加一个轻量级注意力层，学习一个掩码来动态校准理性矩阵中每个元素的贡献。这使得模型能够对最终的跨模态交互进行有针对性的调整，而无需对中间特征进行昂贵的修改。", "result": "在不同设置下（更新或冻结预训练编码器，以及仅能访问未标记测试数据的测试时间训练）的实验表明，RAda作为一种多功能微调技术，能以最少的代码改进基线性能，并在大多数设置下与当前最先进的方法表现相当。", "conclusion": "RAda是一种通用且有效的VLM微调技术，通过明确利用融合表示（理性矩阵）来提升性能，且改动极小。"}}
{"id": "2507.15492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15492", "abs": "https://arxiv.org/abs/2507.15492", "authors": ["Rakesh John Amala Arokia Nathan", "Matthias Gessner", "Nurullah Özkan", "Marius Bock", "Mohamed Youssef", "Maximilian Mews", "Björn Piltz", "Ralf Berger", "Oliver Bimber"], "title": "An aerial color image anomaly dataset for search missions in complex forested terrain", "comment": "17 pages", "summary": "After a family murder in rural Germany, authorities failed to locate the\nsuspect in a vast forest despite a massive search. To aid the search, a\nresearch aircraft captured high-resolution aerial imagery. Due to dense\nvegetation obscuring small clues, automated analysis was ineffective, prompting\na crowd-search initiative. This effort produced a unique dataset of labeled,\nhard-to-detect anomalies under occluded, real-world conditions. It can serve as\na benchmark for improving anomaly detection approaches in complex forest\nenvironments, supporting manhunts and rescue operations. Initial benchmark\ntests showed existing methods performed poorly, highlighting the need for\ncontext-aware approaches. The dataset is openly accessible for offline\nprocessing. An additional interactive web interface supports online viewing and\ndynamic growth by allowing users to annotate and submit new findings.", "AI": {"tldr": "研究人员创建了一个独特的森林异常检测数据集，源于德国一起真实案件的搜寻失败，旨在解决茂密植被下目标难以发现的问题，并为改进搜寻和救援技术提供基准。", "motivation": "德国农村发生一起凶杀案后，尽管进行了大规模搜查，当局仍未能在一片广阔森林中找到嫌疑人。现有自动化分析方法因茂密植被遮挡细微线索而失效，促使研究人员寻求新的解决方案。", "method": "利用研究飞机捕获高分辨率航空影像，并通过众包搜寻（crowd-search）活动对难以检测的异常目标进行标注，从而创建了一个独特的真实世界遮挡条件下的标注数据集。此外，还开发了一个交互式网络界面支持在线查看和用户标注，实现数据集的动态增长。", "result": "研究生成了一个独特的、包含在遮挡真实世界条件下难以检测异常目标的数据集。初步基准测试表明，现有方法表现不佳，凸显了开发上下文感知方法的重要性。", "conclusion": "该数据集可作为改进复杂森林环境中异常检测方法的基准，以支持搜捕和救援行动。数据集已公开，并提供交互式网络界面支持持续增长。"}}
{"id": "2507.15504", "categories": ["cs.CV", "68T45", "I.2.10; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.15504", "abs": "https://arxiv.org/abs/2507.15504", "authors": ["Bingqing Zhang", "Zhuo Cao", "Heming Du", "Yang Li", "Xue Li", "Jiajun Liu", "Sen Wang"], "title": "Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization", "comment": "Accepted by ICCV 2025", "summary": "Despite recent advances, Text-to-video retrieval (TVR) is still hindered by\nmultiple inherent uncertainties, such as ambiguous textual queries, indistinct\ntext-video mappings, and low-quality video frames. Although interactive systems\nhave emerged to address these challenges by refining user intent through\nclarifying questions, current methods typically rely on heuristic or ad-hoc\nstrategies without explicitly quantifying these uncertainties, limiting their\neffectiveness. Motivated by this gap, we propose UMIVR, an\nUncertainty-Minimizing Interactive Text-to-Video Retrieval framework that\nexplicitly quantifies three critical uncertainties-text ambiguity, mapping\nuncertainty, and frame uncertainty-via principled, training-free metrics:\nsemantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon\ndivergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based\nFrame Sampler (TQFS). By adaptively generating targeted clarifying questions\nguided by these uncertainty measures, UMIVR iteratively refines user queries,\nsignificantly reducing retrieval ambiguity. Extensive experiments on multiple\nbenchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1\n(69.2\\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby\nestablishing an uncertainty-minimizing foundation for interactive TVR.", "AI": {"tldr": "本文提出UMIVR框架，一个不确定性最小化的交互式文本到视频检索系统，通过量化文本模糊性、映射不确定性和帧不确定性来生成澄清问题，从而提升检索精度。", "motivation": "尽管文本到视频检索（TVR）有所进展，但仍受查询模糊、文本-视频映射不明确和视频帧质量低等不确定性阻碍。现有交互系统依赖启发式或临时策略，未能明确量化这些不确定性，限制了其有效性。", "method": "提出UMIVR框架，通过无训练的原则性度量显式量化三种关键不确定性：基于语义熵的文本模糊性得分（TAS）、基于Jensen-Shannon散度的映射不确定性得分（MUS）和基于时间质量的帧采样器（TQFS）。UMIVR根据这些不确定性度量自适应生成有针对性的澄清问题，迭代优化用户查询。", "result": "在MSR-VTT-1k数据集上，经过10轮交互后，Recall@1显著提升至69.2%，验证了UMIVR的有效性。", "conclusion": "UMIVR为交互式文本到视频检索奠定了不确定性最小化的基础，有效解决了现有TVR系统中的多重不确定性问题。"}}
{"id": "2507.15520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15520", "abs": "https://arxiv.org/abs/2507.15520", "authors": ["Hanting Li", "Fei Zhou", "Xin Sun", "Yang Hua", "Jungong Han", "Liang-Jie Zhang"], "title": "SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement", "comment": "11 pages, 10 figures, 6 tables", "summary": "Recent Transformer-based low-light enhancement methods have made promising\nprogress in recovering global illumination. However, they still struggle with\nnon-uniform lighting scenarios, such as backlit and shadow, appearing as\nover-exposure or inadequate brightness restoration. To address this challenge,\nwe present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)\nframework that enables accurate illumination restoration. Specifically, we\npropose a dynamic integral image representation to model the spatially-varying\nillumination, and further construct a novel Spatially-Adaptive Integral\nIllumination Estimator ($\\text{SAI}^2\\text{E}$). Moreover, we introduce an\nIllumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which\nleverages the illumination to calibrate the lightness-relevant features toward\nvisual-pleased illumination enhancement. Extensive experiments on five standard\nlow-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our\nSAIGFormer significantly outperforms state-of-the-art methods in both\nquantitative and qualitative metrics. In particular, our method achieves\nsuperior performance in non-uniform illumination enhancement while exhibiting\nstrong generalization capabilities across multiple datasets. Code is available\nat https://github.com/LHTcode/SAIGFormer.git.", "AI": {"tldr": "本文提出了一种名为SAIGFormer的Transformer框架，通过空间自适应和光照引导机制，有效解决了低光照图像中非均匀光照（如逆光和阴影）恢复不足或过曝的问题，显著提升了图像增强效果和泛化能力。", "motivation": "现有的基于Transformer的低光照增强方法在恢复全局光照方面取得了进展，但在非均匀光照场景（如逆光和阴影）下表现不佳，导致图像出现过曝或亮度恢复不足的问题。", "method": "本文提出了空间自适应光照引导Transformer (SAIGFormer) 框架。具体方法包括：1) 引入动态积分图像表示来建模空间变化的光照；2) 构建新颖的空间自适应积分光照估计器（SAI²E）；3) 提出光照引导多头自注意力（IG-MSA）机制，利用光照信息校准亮度相关特征，以实现视觉上更令人满意的光照增强。", "result": "在五个标准低光照数据集和一个跨域基准测试（LOL-Blur）上进行的广泛实验表明，SAIGFormer在定量和定性指标上均显著优于现有最先进的方法。尤其在非均匀光照增强方面表现出色，并展示了强大的跨数据集泛化能力。", "conclusion": "SAIGFormer通过其独特的空间自适应和光照引导机制，成功解决了低光照图像中非均匀光照恢复的挑战，实现了卓越的增强性能和泛化能力，为低光照图像处理领域提供了有效的解决方案。"}}
{"id": "2507.15540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15540", "abs": "https://arxiv.org/abs/2507.15540", "authors": ["Syed Ahmed Mahmood", "Ali Shah Ali", "Umer Ahmed", "Fawad Javed Fateh", "M. Zeeshan Zia", "Quoc-Huy Tran"], "title": "Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport", "comment": null, "summary": "We study the problem of self-supervised procedure learning, which discovers\nkey steps and establishes their order from a set of unlabeled procedural\nvideos. Previous procedure learning methods typically learn frame-to-frame\ncorrespondences between videos before determining key steps and their order.\nHowever, their performance often suffers from order variations,\nbackground/redundant frames, and repeated actions. To overcome these\nchallenges, we propose a self-supervised procedure learning framework, which\nutilizes a fused Gromov-Wasserstein optimal transport formulation with a\nstructural prior for computing frame-to-frame mapping between videos. However,\noptimizing exclusively for the above temporal alignment term may lead to\ndegenerate solutions, where all frames are mapped to a small cluster in the\nembedding space and hence every video is associated with only one key step. To\naddress that limitation, we further integrate a contrastive regularization\nterm, which maps different frames to different points in the embedding space,\navoiding the collapse to trivial solutions. Finally, we conduct extensive\nexperiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,\nProceL and CrossTask) benchmarks to demonstrate superior performance by our\napproach against previous methods, including OPEL which relies on a traditional\nKantorovich optimal transport formulation with an optimality prior.", "AI": {"tldr": "本文提出一种自监督过程学习框架，利用融合Gromov-Wasserstein最优传输和对比正则化来发现视频中的关键步骤及其顺序，有效克服了传统方法的局限性。", "motivation": "传统的自监督过程学习方法在处理视频中的顺序变化、背景/冗余帧和重复动作时表现不佳，且容易出现所有帧映射到同一簇的退化解（即每个视频只有一个关键步骤）。", "method": "本文提出一个自监督过程学习框架，利用融合Gromov-Wasserstein最优传输（带有结构先验）来计算视频间的帧到帧映射。为避免退化解，还引入了对比正则化项，确保不同帧映射到嵌入空间中不同的点。", "result": "该方法在大型第一人称（EgoProceL）和第三人称（ProceL和CrossTask）基准测试中，表现出优于包括基于传统Kantorovich最优传输的OPEL在内的现有方法的性能。", "conclusion": "本文提出的融合Gromov-Wasserstein最优传输与对比正则化的自监督过程学习框架，能有效解决现有方法的挑战，并在关键步骤发现和排序任务上取得卓越性能。"}}
{"id": "2507.15541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15541", "abs": "https://arxiv.org/abs/2507.15541", "authors": ["Jongmin Shin", "Enki Cho", "Ka Yong Kim", "Jung Yong Kim", "Seong Tae Kim", "Namkee Oh"], "title": "Towards Holistic Surgical Scene Graph", "comment": "Accepted to MICCAI 2025", "summary": "Surgical scene understanding is crucial for computer-assisted intervention\nsystems, requiring visual comprehension of surgical scenes that involves\ndiverse elements such as surgical tools, anatomical structures, and their\ninteractions. To effectively represent the complex information in surgical\nscenes, graph-based approaches have been explored to structurally model\nsurgical entities and their relationships. Previous surgical scene graph\nstudies have demonstrated the feasibility of representing surgical scenes using\ngraphs. However, certain aspects of surgical scenes-such as diverse\ncombinations of tool-action-target and the identity of the hand operating the\ntool-remain underexplored in graph-based representations, despite their\nimportance. To incorporate these aspects into graph representations, we propose\nEndoscapes-SG201 dataset, which includes annotations for tool-action-target\ncombinations and hand identity. We also introduce SSG-Com, a graph-based method\ndesigned to learn and represent these critical elements. Through experiments on\ndownstream tasks such as critical view of safety assessment and action triplet\nrecognition, we demonstrated the importance of integrating these essential\nscene graph components, highlighting their significant contribution to surgical\nscene understanding. The code and dataset are available at\nhttps://github.com/ailab-kyunghee/SSG-Com", "AI": {"tldr": "该研究提出了一个新数据集Endoscapes-SG201和图基方法SSG-Com，旨在更好地在手术场景图中表示工具-动作-目标组合和手部身份，从而提升手术场景理解。", "motivation": "尽管图基方法已用于建模手术场景，但现有方法未能充分探索工具-动作-目标组合以及操作工具的手部身份等关键信息，而这些信息对手术场景理解至关重要。", "method": "本文提出Endoscapes-SG201数据集，其中包含工具-动作-目标组合和手部身份的标注。同时，引入了SSG-Com，一种基于图的方法，用于学习和表示这些关键元素。", "result": "通过在安全临界视图评估和动作三元组识别等下游任务上的实验，证明了整合这些关键场景图组件的重要性，并强调了它们对手术场景理解的显著贡献。", "conclusion": "将工具-动作-目标组合和手部身份等重要信息整合到手术场景图表示中，能够显著提升手术场景理解能力。"}}
{"id": "2507.15542", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15542", "abs": "https://arxiv.org/abs/2507.15542", "authors": ["Qinqian Lei", "Bo Wang", "Robby T. Tan"], "title": "HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation", "comment": "Accepted by ICCV 2025", "summary": "Zero-shot human-object interaction (HOI) detection remains a challenging\ntask, particularly in generalizing to unseen actions. Existing methods address\nthis challenge by tapping Vision-Language Models (VLMs) to access knowledge\nbeyond the training data. However, they either struggle to distinguish actions\ninvolving the same object or demonstrate limited generalization to unseen\nclasses. In this paper, we introduce HOLa (Zero-Shot HOI Detection with\nLow-Rank Decomposed VLM Feature Adaptation), a novel approach that both\nenhances generalization to unseen classes and improves action distinction. In\ntraining, HOLa decomposes VLM text features for given HOI classes via low-rank\nfactorization, producing class-shared basis features and adaptable weights.\nThese features and weights form a compact HOI representation that preserves\nshared information across classes, enhancing generalization to unseen classes.\nSubsequently, we refine action distinction by adapting weights for each HOI\nclass and introducing human-object tokens to enrich visual interaction\nrepresentations. To further distinguish unseen actions, we guide the weight\nadaptation with LLM-derived action regularization. Experimental results show\nthat our method sets a new state-of-the-art across zero-shot HOI settings on\nHICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.\nOur code is available at https://github.com/ChelsieLei/HOLa.", "AI": {"tldr": "HOLa是一种零样本人-物交互（HOI）检测方法，通过低秩分解VLM特征和LLM正则化，显著提升了对未见动作的泛化能力和区分度。", "motivation": "现有零样本HOI检测方法在区分涉及相同物体的动作时表现不佳，且对未见类别的泛化能力有限。", "method": "HOLa方法在训练中通过低秩分解VLM文本特征，生成类别共享的基础特征和可适应权重，形成紧凑的HOI表示以增强泛化。通过为每个HOI类别调整权重并引入人-物令牌来丰富视觉交互表示，从而细化动作区分。此外，利用大型语言模型（LLM）派生的动作正则化来指导权重调整，进一步区分未见动作。", "result": "HOLa在HICO-DET数据集的零样本HOI设置中达到了新的最先进水平，在未见动词设置中，未见类别的mAP达到了27.91。", "conclusion": "HOLa通过有效提升对未见类别的泛化能力和动作区分度，成功解决了零样本HOI检测的挑战，并树立了新的性能标杆。"}}
{"id": "2507.15569", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15569", "abs": "https://arxiv.org/abs/2507.15569", "authors": ["Xiaoyi Bao", "Chenwei Xie", "Hao Tang", "Tingyu Weng", "Xiaofeng Wang", "Yun Zheng", "Xingang Wang"], "title": "DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding", "comment": "Accepted by ICCV 2025", "summary": "In recent years, the introduction of Multi-modal Large Language Models\n(MLLMs) into video understanding tasks has become increasingly prevalent.\nHowever, how to effectively integrate temporal information remains a critical\nresearch focus. Traditional approaches treat spatial and temporal information\nseparately. Due to issues like motion blur, it is challenging to accurately\nrepresent the spatial information of rapidly moving objects. This can lead to\ntemporally important regions being underemphasized during spatial feature\nextraction, which in turn hinders accurate spatio-temporal interaction and\nvideo understanding. To address this limitation, we propose an innovative video\nrepresentation method called Dynamic-Image (DynImg). Specifically, we introduce\na set of non-key frames as temporal prompts to highlight the spatial areas\ncontaining fast-moving objects. During the process of visual feature\nextraction, these prompts guide the model to pay additional attention to the\nfine-grained spatial features corresponding to these regions. Moreover, to\nmaintain the correct sequence for DynImg, we employ a corresponding 4D video\nRotary Position Embedding. This retains both the temporal and spatial adjacency\nof DynImg, helping MLLM understand the spatio-temporal order within this\ncombined format. Experimental evaluations reveal that DynImg surpasses the\nstate-of-the-art methods by approximately 2% across multiple video\nunderstanding benchmarks, proving the effectiveness of our temporal prompts in\nenhancing video comprehension.", "AI": {"tldr": "该论文提出了一种名为Dynamic-Image (DynImg)的创新视频表示方法，通过引入非关键帧作为时间提示，并结合4D旋转位置编码，有效解决了多模态大语言模型（MLLMs）在视频理解中难以有效整合时间信息的问题，尤其是在处理快速移动物体时。", "motivation": "当前视频理解中的多模态大语言模型（MLLMs）难以有效整合时间信息。传统方法将空间和时间信息分开处理，由于运动模糊等问题，快速移动物体的空间信息难以准确表示，导致重要的时间区域在空间特征提取时被忽视，从而阻碍了准确的时空交互和视频理解。", "method": "本文提出Dynamic-Image (DynImg)视频表示方法。具体而言，引入一组非关键帧作为时间提示，以突出包含快速移动物体的空间区域。在视觉特征提取过程中，这些提示引导模型额外关注这些区域的细粒度空间特征。此外，为保持DynImg的正确序列，采用相应的4D视频旋转位置编码（Rotary Position Embedding），以保留DynImg的时空邻近性，帮助MLLM理解这种组合格式中的时空顺序。", "result": "实验评估表明，DynImg在多个视频理解基准测试中超越了现有最先进方法约2%，证明了其时间提示在增强视频理解方面的有效性。", "conclusion": "DynImg方法通过创新的时间信息整合方式（利用非关键帧作为时间提示和4D旋转位置编码），显著提升了多模态大语言模型在视频理解任务中的表现，尤其是在处理动态内容时。"}}
{"id": "2507.15595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15595", "abs": "https://arxiv.org/abs/2507.15595", "authors": ["Salah Eddine Bekhouche", "Gaby Maroun", "Fadi Dornaika", "Abdenour Hadid"], "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging", "comment": null, "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\n\\href{https://github.com/Bekhouche/SegDT}{GitHub}.", "AI": {"tldr": "本文提出SegDT，一种基于扩散Transformer的新型皮肤病变分割模型，在低成本硬件上实现了最先进的性能和快速推理速度。", "motivation": "医学图像分割对疾病诊断和治疗规划至关重要，特别是皮肤病变分割对皮肤癌诊断和患者监测至关重要。现有模型可能无法满足低成本硬件上的效率和准确性需求。", "method": "SegDT是一个基于扩散Transformer（DiT）的新型分割模型，集成了Rectified Flow以在减少推理步骤的同时提高生成质量并保持标准扩散模型的灵活性。该模型专为低成本硬件设计。", "result": "SegDT在三个基准数据集上进行了评估，与现有方法相比，实现了最先进的结果，同时保持了快速的推理速度。", "conclusion": "SegDT模型因其高性能、快速推理速度以及在低成本硬件上的适用性，非常适合真实的医疗应用，从而提升了医学图像分析中深度学习模型的性能和能力。"}}
{"id": "2507.15602", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15602", "abs": "https://arxiv.org/abs/2507.15602", "authors": ["Zihui Gao", "Jia-Wang Bian", "Guosheng Lin", "Hao Chen", "Chunhua Shen"], "title": "SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting", "comment": null, "summary": "Surface reconstruction and novel view rendering from sparse-view images are\nchallenging. Signed Distance Function (SDF)-based methods struggle with fine\ndetails, while 3D Gaussian Splatting (3DGS)-based approaches lack global\ngeometry coherence. We propose a novel hybrid method that combines the\nstrengths of both approaches: SDF captures coarse geometry to enhance\n3DGS-based rendering, while newly rendered images from 3DGS refine the details\nof SDF for accurate surface reconstruction. As a result, our method surpasses\nstate-of-the-art approaches in surface reconstruction and novel view synthesis\non the DTU and MobileBrick datasets. Code will be released at\nhttps://github.com/Gaozihui/SurfaceSplat.", "AI": {"tldr": "该论文提出一种混合方法，结合SDF和3DGS的优点，以解决稀疏视图图像的表面重建和新视图合成的挑战。", "motivation": "从稀疏视图图像进行表面重建和新视图渲染是具有挑战性的。现有的SDF方法难以处理精细细节，而3DGS方法则缺乏全局几何一致性。", "method": "提出一种新颖的混合方法：SDF用于捕捉粗糙几何结构以增强基于3DGS的渲染，同时，3DGS生成的新渲染图像又用于细化SDF的细节，以实现精确的表面重建。", "result": "该方法在DTU和MobileBrick数据集上的表面重建和新视图合成方面超越了现有最先进的方法。", "conclusion": "通过结合SDF和3DGS的优势，该混合方法有效解决了稀疏视图图像的表面重建和新视图渲染的挑战，并取得了优异的性能。"}}
{"id": "2507.15606", "categories": ["cs.CV", "68T45", "I.4.5"], "pdf": "https://arxiv.org/pdf/2507.15606", "abs": "https://arxiv.org/abs/2507.15606", "authors": ["Ru Jia", "Xiaozhuang Ma", "Jianji Wang", "Nanning Zheng"], "title": "CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation", "comment": "5 pages, 4 figures, to be published", "summary": "While the proposal of the Tri-plane representation has advanced the\ndevelopment of the 3D-aware image generative models, problems rooted in its\ninherent structure, such as multi-face artifacts caused by sharing the same\nfeatures in symmetric regions, limit its ability to generate 360$^\\circ$ view\nimages. In this paper, we propose CylinderPlane, a novel implicit\nrepresentation based on Cylindrical Coordinate System, to eliminate the feature\nambiguity issue and ensure multi-view consistency in 360$^\\circ$. Different\nfrom the inevitable feature entanglement in Cartesian coordinate-based\nTri-plane representation, the cylindrical coordinate system explicitly\nseparates features at different angles, allowing our cylindrical representation\npossible to achieve high-quality, artifacts-free 360$^\\circ$ image synthesis.\nWe further introduce the nested cylinder representation that composites\nmultiple cylinders at different scales, thereby enabling the model more\nadaptable to complex geometry and varying resolutions. The combination of\ncylinders with different resolutions can effectively capture more critical\nlocations and multi-scale features, greatly facilitates fine detail learning\nand robustness to different resolutions. Moreover, our representation is\nagnostic to implicit rendering methods and can be easily integrated into any\nneural rendering pipeline. Extensive experiments on both synthetic dataset and\nunstructured in-the-wild images demonstrate that our proposed representation\nachieves superior performance over previous methods.", "AI": {"tldr": "本文提出CylinderPlane，一种基于圆柱坐标系的新型隐式表示，旨在解决Tri-plane在360度图像生成中存在的特征模糊和多面伪影问题，并通过嵌套圆柱体实现多尺度特征学习。", "motivation": "Tri-plane表示在3D感知图像生成模型中取得了进展，但其固有结构（如共享对称区域特征）导致的多面伪影问题限制了其生成360度视图图像的能力，存在特征模糊和多视角一致性问题。", "method": "核心方法是CylinderPlane，一种基于圆柱坐标系的隐式表示，通过在不同角度明确分离特征来消除特征模糊。此外，引入了嵌套圆柱体表示，将不同尺度的多个圆柱体组合起来，以适应复杂几何和不同分辨率，有效捕获多尺度特征。该表示与隐式渲染方法无关，易于集成到现有神经渲染管线中。", "result": "CylinderPlane实现了高质量、无伪影的360度图像合成。它能更好地适应复杂几何和不同分辨率，有效捕获关键位置和多尺度特征，极大地促进了细节学习和对不同分辨率的鲁棒性。在合成数据集和非结构化真实世界图像上的实验表明，其性能优于现有方法。", "conclusion": "CylinderPlane通过利用圆柱坐标系和嵌套结构，成功解决了Tri-plane在360度图像生成中的局限性，实现了卓越的生成性能和多视角一致性。"}}
{"id": "2507.15628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15628", "abs": "https://arxiv.org/abs/2507.15628", "authors": ["Shanjiang Tang", "Rui Huang", "Hsinyu Luo", "Chunjiang Wang", "Ce Yu", "Yusen Li", "Hao Fu", "Chao Sun", "and Jian Xiao"], "title": "A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications", "comment": null, "summary": "The explosive growth of video data in recent years has brought higher demands\nfor video analytics, where accuracy and efficiency remain the two primary\nconcerns. Deep neural networks (DNNs) have been widely adopted to ensure\naccuracy; however, improving their efficiency in video analytics remains an\nopen challenge. Different from existing surveys that make summaries of\nDNN-based video mainly from the accuracy optimization aspect, in this survey,\nwe aim to provide a thorough review of optimization techniques focusing on the\nimprovement of the efficiency of DNNs in video analytics. We organize existing\nmethods in a bottom-up manner, covering multiple perspectives such as hardware\nsupport, data processing, operational deployment, etc. Finally, based on the\noptimization framework and existing works, we analyze and discuss the problems\nand challenges in the performance optimization of DNN-based video analytics.", "AI": {"tldr": "本文综述了深度神经网络在视频分析中效率优化的技术，涵盖硬件、数据处理和部署等方面。", "motivation": "视频数据爆炸式增长，对视频分析的准确性和效率提出更高要求。深度神经网络虽能保证准确性，但其在视频分析中的效率提升仍是巨大挑战。现有综述多侧重准确性优化，效率优化方面则缺乏全面回顾。", "method": "本文是一篇综述性研究，旨在全面回顾深度神经网络在视频分析中的效率优化技术。作者采用自底向上的组织方式，涵盖硬件支持、数据处理、操作部署等多个方面。", "result": "本文全面总结了深度神经网络在视频分析中的效率优化方法，并基于优化框架和现有工作，分析和讨论了该领域的性能优化问题与挑战。", "conclusion": "综述通过对现有优化框架和工作的分析，指出了深度神经网络视频分析性能优化中存在的具体问题和挑战。"}}
{"id": "2507.15633", "categories": ["cs.CV", "I.2.10; I.4.8; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.15633", "abs": "https://arxiv.org/abs/2507.15633", "authors": ["Sachin Sharma", "Federico Simonetta", "Michele Flammini"], "title": "Experimenting active and sequential learning in a medieval music manuscript", "comment": "6 pages, 4 figures, accepted at IEEE MLSP 2025 (IEEE International\n  Workshop on Machine Learning for Signal Processing). Special Session:\n  Applications of AI in Cultural and Artistic Heritage", "summary": "Optical Music Recognition (OMR) is a cornerstone of music digitization\ninitiatives in cultural heritage, yet it remains limited by the scarcity of\nannotated data and the complexity of historical manuscripts. In this paper, we\npresent a preliminary study of Active Learning (AL) and Sequential Learning\n(SL) tailored for object detection and layout recognition in an old medieval\nmusic manuscript. Leveraging YOLOv8, our system selects samples with the\nhighest uncertainty (lowest prediction confidence) for iterative labeling and\nretraining. Our approach starts with a single annotated image and successfully\nboosts performance while minimizing manual labeling. Experimental results\nindicate that comparable accuracy to fully supervised training can be achieved\nwith significantly fewer labeled examples. We test the methodology as a\npreliminary investigation on a novel dataset offered to the community by the\nAnonymous project, which studies laude, a poetical-musical genre spread across\nItaly during the 12th-16th Century. We show that in the manuscript at-hand,\nuncertainty-based AL is not effective and advocates for more usable methods in\ndata-scarcity scenarios.", "AI": {"tldr": "本研究初步探讨了主动学习（AL）和序列学习（SL）在古老中世纪音乐手稿光学音乐识别（OMR）中的应用，旨在减少手动标注工作量，并发现不确定性AL在该特定手稿中效果不佳。", "motivation": "光学音乐识别（OMR）在文化遗产数字化中至关重要，但受限于带标注数据的稀缺性以及历史手稿的复杂性。", "method": "研究采用YOLOv8模型，结合主动学习（AL）和序列学习（SL）策略，用于手稿中的目标检测和布局识别。系统从单个标注图像开始，通过选择预测置信度最低（不确定性最高）的样本进行迭代标注和再训练。", "result": "实验结果表明，该方法在显著减少标注样本数量的情况下，能够达到与完全监督训练相当的准确性。然而，在匿名项目提供的新数据集中，不确定性主动学习在该特定手稿中被证明无效。", "conclusion": "主动学习能够有效减少OMR任务中的手动标注工作量，但在数据稀缺场景下，基于不确定性的主动学习可能并非总是有效，需要探索更实用的方法。"}}
{"id": "2507.15652", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15652", "abs": "https://arxiv.org/abs/2507.15652", "authors": ["Haoran Zhou", "Zihan Zhang", "Hao Chen"], "title": "Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have made significant strides by\ncombining visual recognition and language understanding to generate content\nthat is both coherent and contextually accurate. However, MLLMs continue to\nstruggle with object hallucinations, where models produce seemingly plausible\nbut factually incorrect outputs, including objects that do not exist in the\nimage. Recent work has revealed that the prior knowledge in MLLMs significantly\nsuppresses visual information in deep layers, causing hallucinatory outputs.\nHowever, how these priors suppress visual information at the intermediate layer\nstage in MLLMs remains unclear. We observe that visual factual knowledge and\nthe differences between intermediate-layer prior/original probability\ndistributions show similar evolutionary trends in intermediate layers.\nMotivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a\nsimple, training-free method that dynamically selects intermediate layers with\nthe most significant visual factual information. By contrasting the output\ndistributions of the selected layer derived from the original input and\npure-text input, EVA extracts visual factual knowledge and proportionally\nincorporates it into the final layer to correct the output logits. Importantly,\nEVA is model-agnostic, seamlessly integrates with various classic decoding\nstrategies, and is applicable across different MLLMs. We validate EVA on\nwidely-used benchmarks, and the results show that it significantly reduces\nhallucination rates compared to baseline methods, underscoring its\neffectiveness in mitigating hallucinations.", "AI": {"tldr": "本文提出EVA，一种无需训练、模型无关的方法，通过动态选择中间层并提取视觉事实知识来纠正输出logits，从而显著减少多模态大语言模型（MLLMs）中的物体幻觉。", "motivation": "MLLMs在生成内容时常出现物体幻觉，即生成图像中不存在的物体。现有研究表明，MLLMs中的先验知识会抑制深层视觉信息，但中间层抑制机制尚不明确。本文观察到中间层视觉事实知识与先验/原始概率分布差异存在相似演变趋势，以此为出发点。", "method": "引入EVA（Decoding by Extracting Visual Facts）方法，该方法无需训练且模型无关。EVA动态选择具有最显著视觉事实信息的中间层，通过对比该层在原始输入和纯文本输入下的输出分布，提取视觉事实知识，并按比例将其融入最终层以修正输出logits。EVA可与多种经典解码策略无缝集成，并适用于不同的MLLMs。", "result": "在广泛使用的基准测试中，EVA显著降低了幻觉率，优于基线方法，证明了其在缓解幻觉方面的有效性。", "conclusion": "EVA通过利用中间层视觉事实信息，提供了一种有效且通用的方法来减轻MLLMs中的物体幻觉，并且具有良好的模型兼容性。"}}
{"id": "2507.15655", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15655", "abs": "https://arxiv.org/abs/2507.15655", "authors": ["Aniket Pal", "Ajoy Mondal", "Minesh Mathew", "C. V. Jawahar"], "title": "HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark", "comment": "This is a minor revision of the original paper submitted to IJDAR", "summary": "The proliferation of MultiLingual Visual Question Answering (MLVQA)\nbenchmarks augments the capabilities of large language models (LLMs) and\nmulti-modal LLMs, thereby enabling them to adeptly capture the intricate\nlinguistic subtleties and visual complexities inherent across diverse\nlanguages. Despite its potential, the current MLVQA model struggles to fully\nutilize its capabilities when dealing with the extensive variety of handwritten\ndocuments. This article delineates HW-MLVQA, an avant-garde VQA benchmark\nmeticulously crafted to mitigate the dearth of authentic Multilingual\nHandwritten document comprehension. HW-MLVQA encompasses an extensive\ncollection of 1,600 handwritten Pages complemented by 2,400 question-answers.\nFurthermore, it provides a robust benchmark evaluation framework spanning three\ndistinct modalities: text, image, and an integrated image & text modality. To\nsimulate authentic real-world contexts devoid of ground truth textual\ntranscriptions, we facilitates a rigorous assessment of proprietary and\nopen-source OCR models. The benchmark aspires to facilitate pivotal\nadvancements in multilingual handwritten document interpretation, fostering\ninnovation and scholarly inquiry within this specialized domain.", "AI": {"tldr": "本文提出了HW-MLVQA，一个专门用于多语言手写文档视觉问答（VQA）的新基准，旨在解决现有模型在处理手写文档时的不足。", "motivation": "尽管多语言视觉问答（MLVQA）基准增强了大型语言模型（LLM）及其多模态变体的能力，但目前的MLVQA模型在处理种类繁多的手写文档时，未能充分发挥其潜力。因此，需要一个专门针对手写文档的基准来弥补这一空白。", "method": "研究者创建了HW-MLVQA基准，包含1,600页手写文档和2,400个问答对。它提供了一个鲁棒的基准评估框架，涵盖文本、图像以及图像与文本集成三种模态。为模拟真实世界中缺乏文本标注的情况，该基准还支持对专有和开源OCR模型进行严格评估。", "result": "HW-MLVQA是一个包含大量手写文档（1600页）和问答对（2400个）的VQA基准。它提供了一个跨文本、图像和图文融合模态的评估框架，并能用于评估OCR模型在无文本标注真实场景下的性能。", "conclusion": "HW-MLVQA基准的推出旨在促进多语言手写文档理解领域的关键进展，并推动该专业领域的创新和学术研究。"}}
{"id": "2507.15680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15680", "abs": "https://arxiv.org/abs/2507.15680", "authors": ["Yongkang Hou", "Jiarun Song"], "title": "Visual-Language Model Knowledge Distillation Method for Image Quality Assessment", "comment": null, "summary": "Image Quality Assessment (IQA) is a core task in computer vision. Multimodal\nmethods based on vision-language models, such as CLIP, have demonstrated\nexceptional generalization capabilities in IQA tasks. To address the issues of\nexcessive parameter burden and insufficient ability to identify local distorted\nfeatures in CLIP for IQA, this study proposes a visual-language model knowledge\ndistillation method aimed at guiding the training of models with architectural\nadvantages using CLIP's IQA knowledge. First, quality-graded prompt templates\nwere designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned\nto enhance its capabilities in IQA tasks. Finally, a modality-adaptive\nknowledge distillation strategy is proposed to achieve guidance from the CLIP\nteacher model to the student model. Our experiments were conducted on multiple\nIQA datasets, and the results show that the proposed method significantly\nreduces model complexity while outperforming existing IQA methods,\ndemonstrating strong potential for practical deployment.", "AI": {"tldr": "本研究提出了一种基于知识蒸馏的视觉-语言模型方法，利用CLIP的图像质量评估（IQA）知识指导更轻量级模型的训练，以解决CLIP参数负担过重和局部失真识别不足的问题，并在IQA任务中实现了性能提升和模型简化。", "motivation": "现有的基于视觉-语言模型（如CLIP）的多模态图像质量评估方法虽然泛化能力强，但存在参数量过大和识别局部失真特征能力不足的问题，限制了其实际部署。", "method": "1. 设计了分级质量提示模板以引导CLIP输出质量分数。2. 对CLIP进行微调以增强其在IQA任务中的能力。3. 提出了一种模态自适应知识蒸馏策略，将CLIP教师模型的知识传递给学生模型。", "result": "实验结果表明，所提出的方法显著降低了模型复杂度，同时在多个IQA数据集上超越了现有方法，展现出强大的实际部署潜力。", "conclusion": "该研究成功地通过知识蒸馏，利用CLIP的IQA知识，训练出更高效、更强大的模型，解决了大型视觉-语言模型在IQA任务中存在的复杂性问题，并提升了性能，具有重要的实用价值。"}}
{"id": "2507.15683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15683", "abs": "https://arxiv.org/abs/2507.15683", "authors": ["Boni Hu", "Zhenyu Xia", "Lin Chen", "Pengcheng Han", "Shuhui Bu"], "title": "Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing", "comment": "17 pages, 11 figures", "summary": "Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera\npose from query images, is fundamental to remote sensing and UAV applications.\nExisting methods face inherent trade-offs: image-based retrieval and pose\nregression approaches lack precision, while structure-based methods that\nregister queries to Structure-from-Motion (SfM) models suffer from\ncomputational complexity and limited scalability. These challenges are\nparticularly pronounced in remote sensing scenarios due to large-scale scenes,\nhigh altitude variations, and domain gaps of existing visual priors. To\novercome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel\nscene representation that compactly encodes both 3D geometry and appearance. We\nintroduce $\\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework\nthat follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting\nthe rich semantic information and geometric constraints inherent in Gaussian\nprimitives. To handle large-scale remote sensing scenarios, we incorporate\npartitioned Gaussian training, GPU-accelerated parallel matching, and dynamic\nmemory management strategies. Our approach consists of two stages: (1) a sparse\nstage featuring a Gaussian-specific consistent render-aware sampling strategy\nand landmark-guided detector for robust and accurate initial pose estimation,\nand (2) a dense stage that iteratively refines poses through coarse-to-fine\ndense rasterization matching while incorporating reliability verification.\nThrough comprehensive evaluation on simulation data, public datasets, and real\nflight experiments, we demonstrate that our method delivers competitive\nlocalization accuracy, recall rate, and computational efficiency while\neffectively filtering unreliable pose estimates. The results confirm the\neffectiveness of our approach for practical remote sensing applications.", "AI": {"tldr": "本文提出Hi²-GSLoc，一个基于3D高斯Splatting的双层级视觉重定位框架，旨在解决遥感场景中现有方法在精度、计算复杂度和可扩展性上的局限性。", "motivation": "现有视觉重定位方法在精度、计算复杂度和可扩展性之间存在固有限制，尤其在遥感场景中，由于大规模场景、高空高度变化和视觉先验的领域差距，这些挑战更为突出。图像检索和姿态回归方法精度不足，而基于结构的方法计算复杂且可扩展性差。", "method": "本文利用3D高斯Splatting（3DGS）作为新颖的场景表示，并提出了Hi²-GSLoc，一个遵循稀疏到密集、粗到精范式的双层级重定位框架。该方法包含两个阶段：1) 稀疏阶段：采用高斯特定的渲染感知采样策略和地标引导检测器进行初始姿态估计；2) 密集阶段：通过粗到精的密集光栅化匹配迭代细化姿态，并结合可靠性验证。为处理大规模遥感场景，还引入了分区高斯训练、GPU加速并行匹配和动态内存管理策略。", "result": "通过在模拟数据、公共数据集和真实飞行实验上的全面评估，该方法在定位精度、召回率和计算效率方面表现出竞争力，并能有效过滤不可靠的姿态估计。", "conclusion": "研究结果证实了该方法在实际遥感应用中的有效性。"}}
{"id": "2507.15709", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15709", "abs": "https://arxiv.org/abs/2507.15709", "authors": ["Wei Sun", "Weixia Zhang", "Linhan Cao", "Jun Jia", "Xiangyang Zhu", "Dandan Zhu", "Xiongkuo Min", "Guangtao Zhai"], "title": "Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation", "comment": "Efficient-FIQA achieved first place in the ICCV VQualA 2025 Face\n  Image Quality Assessment Challenge", "summary": "Face image quality assessment (FIQA) is essential for various face-related\napplications. Although FIQA has been extensively studied and achieved\nsignificant progress, the computational complexity of FIQA algorithms remains a\nkey concern for ensuring scalability and practical deployment in real-world\nsystems. In this paper, we aim to develop a computationally efficient FIQA\nmethod that can be easily deployed in real-world applications. Specifically,\nour method consists of two stages: training a powerful teacher model and\ndistilling a lightweight student model from it. To build a strong teacher\nmodel, we adopt a self-training strategy to improve its capacity. We first\ntrain the teacher model using labeled face images, then use it to generate\npseudo-labels for a set of unlabeled images. These pseudo-labeled samples are\nused in two ways: (1) to distill knowledge into the student model, and (2) to\ncombine with the original labeled images to further enhance the teacher model\nthrough self-training. The enhanced teacher model is used to further\npseudo-label another set of unlabeled images for distilling the student models.\nThe student model is trained using a combination of labeled images,\npseudo-labeled images from the original teacher model, and pseudo-labeled\nimages from the enhanced teacher model. Experimental results demonstrate that\nour student model achieves comparable performance to the teacher model with an\nextremely low computational overhead. Moreover, our method achieved first place\nin the ICCV 2025 VQualA FIQA Challenge. The code is available at\nhttps://github.com/sunwei925/Efficient-FIQA.git.", "AI": {"tldr": "本文提出了一种高效的人脸图像质量评估(FIQA)方法，通过自训练的教师模型和知识蒸馏的轻量级学生模型，解决了现有FIQA算法计算复杂度高的问题。", "motivation": "尽管人脸图像质量评估(FIQA)已取得显著进展，但其算法的计算复杂性仍然是实际部署和扩展的关键障碍，因此需要开发一种计算效率高且易于部署的FIQA方法。", "method": "该方法包含两个阶段：训练强大的教师模型和从教师模型中蒸馏出轻量级学生模型。教师模型采用自训练策略增强能力：首先用标注数据训练，然后生成未标注图像的伪标签。这些伪标签用于两方面：1) 知识蒸馏到学生模型；2) 与原始标注数据结合进一步增强教师模型。增强后的教师模型再生成伪标签用于学生模型蒸馏。学生模型则结合了标注图像、原始教师模型和增强教师模型生成的伪标签进行训练。", "result": "实验结果表明，学生模型在计算开销极低的情况下，实现了与教师模型相当的性能。此外，该方法在ICCV 2025 VQualA FIQA挑战赛中获得第一名。", "conclusion": "该研究成功开发了一种计算高效且性能卓越的FIQA方法，通过自训练和知识蒸馏策略，使其能够轻松部署于实际应用中，并取得了竞赛的优异成绩。"}}
{"id": "2507.15724", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15724", "abs": "https://arxiv.org/abs/2507.15724", "authors": ["Guoxuan Xia", "Harleen Hanspal", "Petru-Daniel Tudosiu", "Shifeng Zhang", "Sarah Parisot"], "title": "A Practical Investigation of Spatially-Controlled Image Generation with Transformers", "comment": "preprint", "summary": "Enabling image generation models to be spatially controlled is an important\narea of research, empowering users to better generate images according to their\nown fine-grained specifications via e.g. edge maps, poses. Although this task\nhas seen impressive improvements in recent times, a focus on rapidly producing\nstronger models has come at the cost of detailed and fair scientific\ncomparison. Differing training data, model architectures and generation\nparadigms make it difficult to disentangle the factors contributing to\nperformance. Meanwhile, the motivations and nuances of certain approaches\nbecome lost in the literature. In this work, we aim to provide clear takeaways\nacross generation paradigms for practitioners wishing to develop\ntransformer-based systems for spatially-controlled generation, clarifying the\nliterature and addressing knowledge gaps. We perform controlled experiments on\nImageNet across diffusion-based/flow-based and autoregressive (AR) models.\nFirst, we establish control token prefilling as a simple, general and\nperformant baseline approach for transformers. We then investigate previously\nunderexplored sampling time enhancements, showing that extending\nclassifier-free guidance to control, as well as softmax truncation, have a\nstrong impact on control-generation consistency. Finally, we re-clarify the\nmotivation of adapter-based approaches, demonstrating that they mitigate\n\"forgetting\" and maintain generation quality when trained on limited downstream\ndata, but underperform full training in terms of generation-control\nconsistency. Code will be released upon publication.", "AI": {"tldr": "本文旨在对空间控制图像生成领域（特别是基于Transformer的模型）进行清晰的对比和分析，通过受控实验澄清了现有文献，并提出了新的基线和采样时增强方法。", "motivation": "现有空间控制图像生成模型的研究侧重于快速提升性能，导致缺乏详细和公平的科学比较，因为训练数据、模型架构和生成范式各异，使得性能影响因素难以区分。此外，某些方法的动机和细微之处在文献中有所遗失。", "method": "研究者在ImageNet上对扩散模型、流模型和自回归模型进行了受控实验。具体方法包括：1) 确立控制令牌预填充作为Transformer简单、通用且高性能的基线方法。2) 探索采样时间增强，如将无分类器引导扩展到控制以及softmax截断。3) 重新阐明基于适配器的方法的动机。", "result": "主要结果包括：1) 控制令牌预填充被确立为Transformer模型简单、通用且高性能的基线方法。2) 采样时间增强（如将无分类器引导扩展到控制和softmax截断）对控制生成一致性有显著影响。3) 基于适配器的方法在有限下游数据训练时能缓解“遗忘”并保持生成质量，但在生成-控制一致性方面不如完全训练。", "conclusion": "本研究为希望开发基于Transformer的空间控制生成系统的实践者提供了跨生成范式的清晰见解，澄清了现有文献并填补了知识空白。"}}
{"id": "2507.15728", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15728", "abs": "https://arxiv.org/abs/2507.15728", "authors": ["Wenqi Ouyang", "Zeqi Xiao", "Danni Yang", "Yifan Zhou", "Shuai Yang", "Lei Yang", "Jianlou Si", "Xingang Pan"], "title": "TokensGen: Harnessing Condensed Tokens for Long Video Generation", "comment": "Project page: https://vicky0522.github.io/tokensgen-webpage/", "summary": "Generating consistent long videos is a complex challenge: while\ndiffusion-based generative models generate visually impressive short clips,\nextending them to longer durations often leads to memory bottlenecks and\nlong-term inconsistency. In this paper, we propose TokensGen, a novel two-stage\nframework that leverages condensed tokens to address these issues. Our method\ndecomposes long video generation into three core tasks: (1) inner-clip semantic\ncontrol, (2) long-term consistency control, and (3) inter-clip smooth\ntransition. First, we train To2V (Token-to-Video), a short video diffusion\nmodel guided by text and video tokens, with a Video Tokenizer that condenses\nshort clips into semantically rich tokens. Second, we introduce T2To\n(Text-to-Token), a video token diffusion transformer that generates all tokens\nat once, ensuring global consistency across clips. Finally, during inference,\nan adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,\nreducing boundary artifacts and enhancing smooth transitions. Experimental\nresults demonstrate that our approach significantly enhances long-term temporal\nand content coherence without incurring prohibitive computational overhead. By\nleveraging condensed tokens and pre-trained short video models, our method\nprovides a scalable, modular solution for long video generation, opening new\npossibilities for storytelling, cinematic production, and immersive\nsimulations. Please see our project page at\nhttps://vicky0522.github.io/tokensgen-webpage/ .", "AI": {"tldr": "TokensGen提出一个两阶段框架，利用凝练的视频tokens生成一致的长视频，解决了现有扩散模型在长视频生成中面临的内存和一致性问题。", "결과": "该研究旨在解决扩散模型在生成长视频时常见的内存瓶颈和长期不一致性问题。", "method": "TokensGen框架分为两个阶段：1) 内片段语义控制、2) 长期一致性控制和3) 片段间平滑过渡。首先，训练To2V（Token-to-Video）短视频扩散模型，由文本和视频tokens引导，并通过视频分词器将短片段压缩为语义丰富的tokens。其次，引入T2To（Text-to-Token），一个视频token扩散transformer，一次性生成所有tokens以确保全局一致性。最后，在推理阶段，采用自适应FIFO-Diffusion策略无缝连接相邻片段，减少边界伪影并增强平滑过渡。", "result": "实验结果表明，该方法显著增强了长视频的长期时间连贯性和内容一致性，且没有带来过高的计算开销。", "conclusion": "通过利用凝练的tokens和预训练的短视频模型，TokensGen提供了一个可扩展、模块化的长视频生成解决方案，为故事叙述、电影制作和沉浸式模拟开辟了新的可能性。", "motivation": "未提供"}}
{"id": "2507.15748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15748", "abs": "https://arxiv.org/abs/2507.15748", "authors": ["Jisu Shin", "Richard Shaw", "Seunghyun Shin", "Anton Pelykh", "Zhensong Zhang", "Hae-Gon Jeon", "Eduardo Perez-Pellitero"], "title": "Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS", "comment": "10 pages, 3 figures, NeurIPS 2025 under review", "summary": "Modern camera pipelines apply extensive on-device processing, such as\nexposure adjustment, white balance, and color correction, which, while\nbeneficial individually, often introduce photometric inconsistencies across\nviews. These appearance variations violate multi-view consistency and degrade\nthe quality of novel view synthesis. Joint optimization of scene\nrepresentations and per-image appearance embeddings has been proposed to\naddress this issue, but at the cost of increased computational complexity and\nslower training. In this work, we propose a transformer-based method that\npredicts spatially adaptive bilateral grids to correct photometric variations\nin a multi-view consistent manner, enabling robust cross-scene generalization\nwithout the need for scene-specific retraining. By incorporating the learned\ngrids into the 3D Gaussian Splatting pipeline, we improve reconstruction\nquality while maintaining high training efficiency. Extensive experiments show\nthat our approach outperforms or matches existing scene-specific optimization\nmethods in reconstruction fidelity and convergence speed.", "AI": {"tldr": "本文提出一种基于Transformer的方法，通过预测空间自适应双边网格来校正多视图图像中的光度不一致性，并将其集成到3D高斯泼溅管线中，以提升新视图合成质量，同时实现跨场景泛化和高训练效率。", "motivation": "现代相机处理流程（如曝光调整、白平衡、色彩校正）会引入视图间的光度不一致性，这违反了多视图一致性并降低了新视图合成的质量。现有通过联合优化场景表示和每图像外观嵌入的方法虽然能解决此问题，但计算复杂度高且训练速度慢。", "method": "提出一种基于Transformer的方法，用于预测空间自适应的双边网格。这些网格以多视图一致的方式校正光度变化。将学习到的网格集成到3D高斯泼溅（3D Gaussian Splatting）管线中。", "result": "该方法在保持高训练效率的同时，提高了重建质量。在重建保真度和收敛速度方面，超越或匹配了现有的场景特定优化方法。实现了鲁棒的跨场景泛化，无需进行场景特定再训练。", "conclusion": "所提出的基于Transformer的双边网格方法能有效解决多视图图像中的光度不一致问题，显著提升3D高斯泼溅在新视图合成中的性能，同时具备高效率和强大的跨场景泛化能力。"}}
{"id": "2507.15765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15765", "abs": "https://arxiv.org/abs/2507.15765", "authors": ["Feng-Qi Cui", "Anyang Tong", "Jinyang Huang", "Jie Zhang", "Dan Guo", "Zhi Liu", "Meng Wang"], "title": "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization", "comment": "Accepted by ACM MM'25", "summary": "Dynamic Facial Expression Recognition (DFER) plays a critical role in\naffective computing and human-computer interaction. Although existing methods\nachieve comparable performance, they inevitably suffer from performance\ndegradation under sample heterogeneity caused by multi-source data and\nindividual expression variability. To address these challenges, we propose a\nnovel framework, called Heterogeneity-aware Distributional Framework (HDF), and\ndesign two plug-and-play modules to enhance time-frequency modeling and\nmitigate optimization imbalance caused by hard samples. Specifically, the\nTime-Frequency Distributional Attention Module (DAM) captures both temporal\nconsistency and frequency robustness through a dual-branch attention design,\nimproving tolerance to sequence inconsistency and visual style shifts. Then,\nbased on gradient sensitivity and information bottleneck principles, an\nadaptive optimization module Distribution-aware Scaling Module (DSM) is\nintroduced to dynamically balance classification and contrastive losses,\nenabling more stable and discriminative representation learning. Extensive\nexperiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF\nsignificantly improves both recognition accuracy and robustness. Our method\nachieves superior weighted average recall (WAR) and unweighted average recall\n(UAR) while maintaining strong generalization across diverse and imbalanced\nscenarios. Codes are released at https://github.com/QIcita/HDF_DFER.", "AI": {"tldr": "本文提出了一种名为异构感知分布框架（HDF）的新型方法，用于动态面部表情识别（DFER），旨在解决多源数据和个体表情变异引起的样本异构性问题，通过增强时频建模和优化难样本来提高识别准确性和鲁棒性。", "motivation": "现有DFER方法在面对多源数据和个体表情变异导致的样本异构性时，性能会下降。", "method": "提出了异构感知分布框架（HDF），包含两个即插即用模块：1) 时频分布注意力模块（DAM），通过双分支注意力设计捕捉时间一致性和频率鲁棒性；2) 分布感知缩放模块（DSM），基于梯度敏感度和信息瓶颈原理，自适应平衡分类和对比损失，以实现更稳定和有区分度的表示学习。", "result": "在DFEW和FERV39k两个广泛使用的数据集上，HDF显著提高了识别准确性和鲁棒性。该方法实现了卓越的加权平均召回率（WAR）和非加权平均召回率（UAR），并在多样化和不平衡的场景中保持了强大的泛化能力。", "conclusion": "HDF通过解决样本异构性问题，有效提升了动态面部表情识别的性能和鲁棒性，为该领域提供了新的解决方案。"}}
{"id": "2507.15777", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15777", "abs": "https://arxiv.org/abs/2507.15777", "authors": ["Junwen Wang", "Oscar MacCormac", "William Rochford", "Aaron Kujawa", "Jonathan Shapey", "Tom Vercauteren"], "title": "Label tree semantic losses for rich multi-class medical image segmentation", "comment": "arXiv admin note: text overlap with arXiv:2506.21150", "summary": "Rich and accurate medical image segmentation is poised to underpin the next\ngeneration of AI-defined clinical practice by delineating critical anatomy for\npre-operative planning, guiding real-time intra-operative navigation, and\nsupporting precise post-operative assessment. However, commonly used learning\nmethods for medical and surgical imaging segmentation tasks penalise all errors\nequivalently and thus fail to exploit any inter-class semantics in the labels\nspace. This becomes particularly problematic as the cardinality and richness of\nlabels increases to include subtly different classes. In this work, we propose\ntwo tree-based semantic loss functions which take advantage of a hierarchical\norganisation of the labels. We further incorporate our losses in a recently\nproposed approach for training with sparse, background-free annotations to\nextend the applicability of our proposed losses. Extensive experiments are\nreported on two medical and surgical image segmentation tasks, namely head MRI\nfor whole brain parcellation (WBP) with full supervision and neurosurgical\nhyperspectral imaging (HSI) for scene understanding with sparse annotations.\nResults demonstrate that our proposed method reaches state-of-the-art\nperformance in both cases.", "AI": {"tldr": "该研究提出两种基于树的语义损失函数，利用标签的层级结构，解决了医学图像分割中传统方法忽略类别语义的问题，并在全脑分割和神经外科高光谱图像分割任务中达到了SOTA性能。", "motivation": "当前医学图像分割的学习方法对所有错误一视同仁，未能利用标签空间中的类间语义关系。当标签数量和丰富度增加，包含细微差异的类别时，这个问题尤为突出，限制了其在AI驱动临床实践中的应用。", "method": "1. 提出了两种基于树的语义损失函数，利用标签的层级组织结构。2. 将这些损失函数整合到一种支持稀疏、无背景标注训练的方法中，以扩展其适用性。3. 在两个医学和外科图像分割任务上进行了广泛实验：全监督的头部MRI全脑分割（WBP）和稀疏标注的神经外科高光谱成像（HSI）场景理解。", "result": "在全脑分割和神经外科高光谱图像分割这两项任务中，所提出的方法均达到了最先进的性能。", "conclusion": "通过引入利用标签层级结构的树基语义损失函数，本研究显著提升了医学图像分割的准确性和鲁棒性，特别是在处理复杂标签空间和稀疏标注数据时，证明了其优越性。"}}
{"id": "2507.15793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15793", "abs": "https://arxiv.org/abs/2507.15793", "authors": ["Ghassen Baklouti", "Julio Silva-Rodríguez", "Jose Dolz", "Houda Bahig", "Ismail Ben Ayed"], "title": "Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation", "comment": "Accepted at MICCAI 2025", "summary": "Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is\nincreasingly attracting interest in medical imaging due to its effectiveness\nand computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)\nis a notable approach based on the assumption that the adaptation inherently\noccurs in a low-dimensional subspace. While it has shown good performance, its\nimplementation requires a fixed and unalterable rank, which might be\nchallenging to select given the unique complexities and requirements of each\nmedical imaging downstream task. Inspired by advancements in natural image\nprocessing, we introduce a novel approach for medical image segmentation that\ndynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank\nrepresentation of the trainable weight matrices as a singular value\ndecomposition, we introduce an l_1 sparsity regularizer to the loss function,\nand tackle it with a proximal optimizer. The regularizer could be viewed as a\npenalty on the decomposition rank. Hence, its minimization enables to find\ntask-adapted ranks automatically. Our method is evaluated in a realistic\nfew-shot fine-tuning setting, where we compare it first to the standard LoRA\nand then to several other PEFT methods across two distinguishable tasks: base\norgans and novel organs. Our extensive experiments demonstrate the significant\nperformance improvements driven by our method, highlighting its efficiency and\nrobustness against suboptimal rank initialization. Our code is publicly\navailable: https://github.com/ghassenbaklouti/ARENA", "AI": {"tldr": "本文提出了一种名为ARENA的新型参数高效微调（PEFT）方法，通过在损失函数中引入L1稀疏正则化器，并使用近端优化器，实现了LoRA在医学图像分割任务中低秩适应的动态秩调整，解决了固定秩选择的挑战。", "motivation": "LoRA在医学成像领域表现良好，但其固定且不可更改的秩选择对于复杂的医学图像下游任务来说是一个挑战，因为不同任务的最佳秩可能不同。", "method": "该方法基于LoRA，将可训练权重矩阵的低秩表示视为奇异值分解。通过在损失函数中引入L1稀疏正则化器，并利用近端优化器进行优化，实现了对分解秩的惩罚，从而能够自动找到适应任务的秩。该方法在少样本微调设置下，针对基础器官和新器官分割任务，与标准LoRA及其他PEFT方法进行了比较评估。", "result": "广泛的实验表明，该方法显著提升了性能，并展现出高效性和对次优秩初始化的鲁棒性。", "conclusion": "所提出的方法有效解决了LoRA在医学图像分割中固定秩的局限性，通过动态调整秩实现了性能提升和鲁棒性，使其更适用于复杂的医学成像任务。"}}
{"id": "2507.15798", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15798", "abs": "https://arxiv.org/abs/2507.15798", "authors": ["Lilian Hollard", "Lucas Mohimont", "Nathalie Gaveau", "Luiz-Angelo Steffenel"], "title": "Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models", "comment": null, "summary": "The paper investigates the performance of state-of-the-art low-parameter deep\nneural networks for computer vision, focusing on bottleneck architectures and\ntheir behavior using superlinear activation functions. We address interference\nin feature maps, a phenomenon associated with superposition, where neurons\nsimultaneously encode multiple characteristics. Our research suggests that\nlimiting interference can enhance scaling and accuracy in very low-scaled\nnetworks (under 1.5M parameters). We identify key design elements that reduce\ninterference by examining various bottleneck architectures, leading to a more\nefficient neural network. Consequently, we propose a proof-of-concept\narchitecture named NoDepth Bottleneck built on mechanistic insights from our\nexperiments, demonstrating robust scaling accuracy on the ImageNet dataset.\nThese findings contribute to more efficient and scalable neural networks for\nthe low-parameter range and advance the understanding of bottlenecks in\ncomputer vision. https://caiac.pubpub.org/pub/3dh6rsel", "AI": {"tldr": "该研究探讨了低参数深度神经网络（特别是瓶颈架构）在计算机视觉中的性能，通过减少特征图中的干扰，提出了一种名为NoDepth Bottleneck的新架构，实现了在低参数范围内的良好扩展性和准确性。", "motivation": "研究旨在探索最先进的低参数深度神经网络在计算机视觉中的表现，特别是瓶颈架构和超线性激活函数的作用。论文关注特征图中的干扰现象（神经元同时编码多个特征），并提出限制这种干扰可以提高低参数网络（低于1.5M参数）的扩展性和准确性。", "method": "研究通过检查各种瓶颈架构，识别了减少干扰的关键设计元素。基于实验的机制洞察，提出了一种概念验证架构，命名为NoDepth Bottleneck，并在ImageNet数据集上进行了验证。", "result": "研究表明，限制特征图中的干扰可以增强极低参数网络（低于1.5M参数）的扩展性和准确性。所提出的NoDepth Bottleneck架构在ImageNet数据集上展示了强大的扩展准确性。", "conclusion": "这些发现有助于为低参数范围内的神经网络设计出更高效、更具扩展性的方案，并加深了对计算机视觉中瓶颈架构的理解。"}}
{"id": "2507.15809", "categories": ["cs.CV", "cs.LG", "physics.geo-ph", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.15809", "abs": "https://arxiv.org/abs/2507.15809", "authors": ["Roberto Miele", "Niklas Linde"], "title": "Diffusion models for multivariate subsurface generation and efficient probabilistic inversion", "comment": null, "summary": "Diffusion models offer stable training and state-of-the-art performance for\ndeep generative modeling tasks. Here, we consider their use in the context of\nmultivariate subsurface modeling and probabilistic inversion. We first\ndemonstrate that diffusion models enhance multivariate modeling capabilities\ncompared to variational autoencoders and generative adversarial networks. In\ndiffusion modeling, the generative process involves a comparatively large\nnumber of time steps with update rules that can be modified to account for\nconditioning data. We propose different corrections to the popular Diffusion\nPosterior Sampling approach by Chung et al. (2023). In particular, we introduce\na likelihood approximation accounting for the noise-contamination that is\ninherent in diffusion modeling. We assess performance in a multivariate\ngeological scenario involving facies and correlated acoustic impedance.\nConditional modeling is demonstrated using both local hard data (well logs) and\nnonlinear geophysics (fullstack seismic data). Our tests show significantly\nimproved statistical robustness, enhanced sampling of the posterior probability\ndensity function and reduced computational costs, compared to the original\napproach. The method can be used with both hard and indirect conditioning data,\nindividually or simultaneously. As the inversion is included within the\ndiffusion process, it is faster than other methods requiring an outer-loop\naround the generative model, such as Markov chain Monte Carlo.", "AI": {"tldr": "该研究将扩散模型应用于多变量地下建模和概率反演，通过改进扩散后验采样方法，提高了统计鲁棒性、后验采样效率和计算速度，优于传统生成模型和现有扩散反演方法。", "motivation": "当前深度生成模型（如VAE和GAN）在多变量地下建模中存在局限性，且现有的扩散模型反演方法（如扩散后验采样）在统计鲁棒性、后验采样和计算成本方面仍有提升空间，促使研究者探索更优的扩散模型应用和改进方案。", "method": "研究首先展示了扩散模型在多变量建模方面的优势。随后，针对流行的扩散后验采样（DPS）方法，提出并引入了新的校正措施，特别是考虑了扩散模型中固有的噪声污染的似然近似。该方法在涉及相和相关声阻抗的多变量地质场景中进行了评估，并使用局部硬数据（测井）和非线性地球物理数据（全叠前地震数据）进行了条件建模验证。", "result": "与变分自编码器（VAE）和生成对抗网络（GAN）相比，扩散模型显著增强了多变量建模能力。与原始的扩散后验采样方法相比，所提出的改进方法显著提高了统计鲁棒性，增强了后验概率密度函数的采样，并降低了计算成本。该方法可单独或同时使用硬数据和间接条件数据，并且由于反演内嵌于扩散过程，其速度比需要生成模型外部循环（如马尔可夫链蒙特卡洛）的其他方法更快。", "conclusion": "扩散模型为多变量地下建模和概率反演提供了一种强大、高效且统计鲁棒的方法。通过对现有扩散后验采样方法的改进，该方法在处理硬数据和间接条件数据方面表现出色，并显著优于传统生成模型和需要外部循环的反演方法，为地球科学领域的概率反演提供了新的范式。"}}
{"id": "2507.15824", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15824", "abs": "https://arxiv.org/abs/2507.15824", "authors": ["Enes Sanli", "Baris Sarper Tezcan", "Aykut Erdem", "Erkut Erdem"], "title": "Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models", "comment": null, "summary": "Recent progress in text-to-video (T2V) generation has enabled the synthesis\nof visually compelling and temporally coherent videos from natural language.\nHowever, these models often fall short in basic physical commonsense, producing\noutputs that violate intuitive expectations around causality, object behavior,\nand tool use. Addressing this gap, we present PhysVidBench, a benchmark\ndesigned to evaluate the physical reasoning capabilities of T2V systems. The\nbenchmark includes 383 carefully curated prompts, emphasizing tool use,\nmaterial properties, and procedural interactions, and domains where physical\nplausibility is crucial. For each prompt, we generate videos using diverse\nstate-of-the-art models and adopt a three-stage evaluation pipeline: (1)\nformulate grounded physics questions from the prompt, (2) caption the generated\nvideo with a vision-language model, and (3) task a language model to answer\nseveral physics-involved questions using only the caption. This indirect\nstrategy circumvents common hallucination issues in direct video-based\nevaluation. By highlighting affordances and tool-mediated actions, areas\noverlooked in current T2V evaluations, PhysVidBench provides a structured,\ninterpretable framework for assessing physical commonsense in generative video\nmodels.", "AI": {"tldr": "本文提出了PhysVidBench，一个旨在评估文本到视频（T2V）系统物理常识推理能力的新基准，通过间接评估方法克服了直接视频评估中的幻觉问题。", "motivation": "当前的文本到视频（T2V）模型在生成视频时，往往缺乏基本的物理常识，导致输出内容违反因果关系、物体行为和工具使用等直观预期，存在物理不合理性。", "method": "研究者创建了PhysVidBench基准，包含383个精心策划的提示，侧重于工具使用、材料属性和程序性交互。他们使用最先进的T2V模型生成视频，并采用三阶段评估流程：1) 从提示中提出基于物理的问题；2) 使用视觉-语言模型对生成视频进行字幕描述；3) 使用语言模型仅根据字幕回答物理相关问题。这种间接策略旨在规避直接视频评估中常见的幻觉问题。", "result": "PhysVidBench通过强调可供性（affordances）和工具介导的动作，弥补了当前T2V评估中被忽视的领域。它提供了一个结构化、可解释的框架，用于评估生成视频模型中的物理常识能力。", "conclusion": "PhysVidBench为评估T2V系统在物理常识方面的能力提供了一个重要工具，填补了现有评估方法的空白，有助于推动未来T2V模型在物理真实性方面的进步。"}}
{"id": "2507.15856", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15856", "abs": "https://arxiv.org/abs/2507.15856", "authors": ["Jiawei Yang", "Tianhong Li", "Lijie Fan", "Yonglong Tian", "Yue Wang"], "title": "Latent Denoising Makes Good Visual Tokenizers", "comment": "Code is available at: https://github.com/Jiawei-Yang/DeTok", "summary": "Despite their fundamental role, it remains unclear what properties could make\nvisual tokenizers more effective for generative modeling. We observe that\nmodern generative models share a conceptually similar training objective --\nreconstructing clean signals from corrupted inputs such as Gaussian noise or\nmasking -- a process we term denoising. Motivated by this insight, we propose\naligning tokenizer embeddings directly with the downstream denoising objective,\nencouraging latent embeddings to be more easily reconstructed even when heavily\ncorrupted. To achieve this, we introduce the Latent Denoising Tokenizer\n(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images\nfrom latent embeddings corrupted by interpolative noise and random masking.\nExtensive experiments on ImageNet 256x256 demonstrate that our tokenizer\nconsistently outperforms standard tokenizers across six representative\ngenerative models. Our findings highlight denoising as a fundamental design\nprinciple for tokenizer development, and we hope it could motivate new\nperspectives for future tokenizer design.", "AI": {"tldr": "本文提出了一种名为l-DeTok的新型视觉分词器，通过使其潜在嵌入与生成模型的去噪目标对齐，显著提升了各种生成模型的性能。", "motivation": "尽管视觉分词器在生成模型中扮演着基础角色，但其有效性的关键特性尚不明确。研究者观察到现代生成模型普遍共享一个去噪（从损坏输入重建清晰信号）的训练目标，因此提出将分词器嵌入直接与此去噪目标对齐。", "method": "研究者引入了潜在去噪分词器（l-DeTok），它被训练用于从受插值噪声和随机遮蔽损坏的潜在嵌入中重建清晰图像，从而鼓励潜在嵌入即使在严重损坏下也更容易被重建。", "result": "在ImageNet 256x256数据集上，l-DeTok在六种代表性生成模型中均持续优于标准分词器。", "conclusion": "研究结果表明，去噪是分词器开发的一个基本设计原则，有望为未来的分词器设计提供新视角。"}}
