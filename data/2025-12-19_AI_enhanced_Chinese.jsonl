{"id": "2512.15734", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15734", "abs": "https://arxiv.org/abs/2512.15734", "authors": ["Eloy Serrano-Seco", "Edgar Ramirez-Laboreo", "Eduardo Moya-Lasheras"], "title": "Run-to-Run Indirect Trajectory Tracking Control of Electromechanical Systems Based on Identifiable and Flat Models", "comment": "6 pages, 4 figures. Version submitted to the 23rd IFAC World Congress", "summary": "Differentially flat models are frequently used to design feedforward controllers for electromechanical systems. However, control performance depends on model accuracy, which makes feedback imperative. This paper presents a control scheme for electromechanical systems in which measuring or estimating the output to be controlled -- typically the position -- is not feasible. It employs an identifiable-model-based controller and predictor, coupled with an iterative loop that updates model parameters using the error between a measurable output and its prediction. Simulations on electromechanical switching devices show effective tracking of the desired position trajectory using only coil current measurements.", "AI": {"tldr": "提出一种用于机电系统的控制方案，无需测量或估计被控输出（如位置），仅通过线圈电流测量即可实现有效跟踪。", "motivation": "现有基于微分平坦模型的机电系统前馈控制器依赖模型精度，需反馈补偿，但在无法测量或估计被控输出（如位置）时难以实现。", "method": "采用基于可辨识模型的控制器和预测器，结合迭代循环，利用可测量输出与其预测值之间的误差更新模型参数。", "result": "在机电开关设备上的仿真表明，仅使用线圈电流测量即可有效跟踪期望位置轨迹。", "conclusion": "该控制方案能在无需直接测量被控输出的情况下实现有效跟踪，为机电系统控制提供了新思路。"}}
{"id": "2512.15725", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15725", "abs": "https://arxiv.org/abs/2512.15725", "authors": ["Matteo Cercola", "Donatello Materassi", "Simone Formentin"], "title": "Generative design of stabilizing controllers with diffusion models: the Youla approach", "comment": null, "summary": "Designing controllers that simultaneously achieve strong performance and provable closed-loop stability remains a central challenge in control engineering. This work introduces a diffusion-based generative framework for linear controller synthesis grounded in the Youla-Kucera parameterization, enabling the construction of stabilizing controllers by design. The diffusion model learns a conditional mapping from plant dynamics and desired performance metrics to feasible Youla parameters, guaranteeing internal stability while flexibly accommodating user-specified targets. Trained on synthetically generated stable SISO plants with fixed-order Youla parameters, the proposed approach reliably synthesizes controllers that meet prescribed sensitivity and settling-time specifications on previously unseen systems. To the best of our knowledge, this work provides the first demonstration that diffusion models can generate stabilizing controllers, combining rigorous control-theoretic guarantees with the versatility of modern generative modeling.", "AI": {"tldr": "提出一种基于扩散模型的线性控制器生成框架，通过Youla-Kucera参数化保证闭环稳定性，实现性能指标驱动的控制器自动合成。", "motivation": "传统控制器设计难以同时保证强性能与可证明的闭环稳定性，需要一种能兼顾理论保证与灵活性的新方法。", "method": "基于Youla-Kucera参数化构建扩散生成框架，训练条件扩散模型学习从系统动态和性能指标到可行Youla参数的映射，使用合成生成的稳定SISO系统及固定阶次Youla参数进行训练。", "result": "模型能在未见系统上可靠合成满足指定灵敏度与调节时间要求的控制器，首次证明扩散模型可生成具有稳定性保证的控制器。", "conclusion": "该工作将严格的控制理论保证与现代生成建模相结合，为控制器设计提供了兼具性能与稳定性的新范式。"}}
{"id": "2512.15907", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.15907", "abs": "https://arxiv.org/abs/2512.15907", "authors": ["Tejas Anvekar", "Juhna Park", "Aparna Garimella", "Vivek Gupta"], "title": "TabReX : Tabular Referenceless eXplainable Evaluation", "comment": null, "summary": "Evaluating the quality of tables generated by large language models (LLMs) remains an open challenge: existing metrics either flatten tables into text, ignoring structure, or rely on fixed references that limit generalization. We present TabReX, a reference-less, property-driven framework for evaluating tabular generation via graph-based reasoning. TabReX converts both source text and generated tables into canonical knowledge graphs, aligns them through an LLM-guided matching process, and computes interpretable, rubric-aware scores that quantify structural and factual fidelity. The resulting metric provides controllable trade-offs between sensitivity and specificity, yielding human-aligned judgments and cell-level error traces. To systematically asses metric robustness, we introduce TabReX-Bench, a large-scale benchmark spanning six domains and twelve planner-driven perturbation types across three difficulty tiers. Empirical results show that TabReX achieves the highest correlation with expert rankings, remains stable under harder perturbations, and enables fine-grained model-vs-prompt analysis establishing a new paradigm for trustworthy, explainable evaluation of structured generation systems.", "AI": {"tldr": "提出了TabReX框架，一种无需参考、基于属性的表格生成评估方法，通过图推理量化结构和事实保真度，并建立了大规模基准TabReX-Bench验证其有效性。", "motivation": "现有评估指标要么将表格扁平化为文本忽略结构，要么依赖固定参考限制泛化能力，缺乏对表格生成质量的可靠评估方法。", "method": "将源文本和生成表格转换为规范知识图谱，通过LLM引导的匹配过程对齐，计算可解释的、基于量规的分数，量化结构和事实保真度。", "result": "TabReX在专家排名相关性上表现最佳，在更难的扰动下保持稳定，支持细粒度的模型与提示分析，实现了人类对齐的判断和单元格级错误追踪。", "conclusion": "TabReX为结构化生成系统提供了可信、可解释的评估新范式，通过可控的敏感性与特异性权衡实现鲁棒评估。"}}
{"id": "2512.15774", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15774", "abs": "https://arxiv.org/abs/2512.15774", "authors": ["Yan Yang", "George Bebis", "Mircea Nicolescu"], "title": "Two-Step Data Augmentation for Masked Face Detection and Recognition: Turning Fake Masks to Real", "comment": "9 pages, 9 figures. Conference version", "summary": "Data scarcity and distribution shift pose major challenges for masked face detection and recognition. We propose a two-step generative data augmentation framework that combines rule-based mask warping with unpaired image-to-image translation using GANs, enabling the generation of realistic masked-face samples beyond purely synthetic transformations. Compared to rule-based warping alone, the proposed approach yields consistent qualitative improvements and complements existing GAN-based masked face generation methods such as IAMGAN. We introduce a non-mask preservation loss and stochastic noise injection to stabilize training and enhance sample diversity. Experimental observations highlight the effectiveness of the proposed components and suggest directions for future improvements in data-centric augmentation for face recognition tasks.", "AI": {"tldr": "提出结合规则掩码变形与GAN图像转换的两步生成式数据增强框架，用于解决戴口罩人脸检测与识别中的数据稀缺和分布偏移问题。", "motivation": "戴口罩场景下人脸数据稀缺及分布偏移问题制约检测与识别性能，现有纯合成或纯生成方法存在局限性，需要更真实、多样化的数据增强方法。", "method": "1. 规则掩码变形生成初始样本；2. 基于GAN的无配对图像转换提升真实感；3. 引入非掩码保留损失和随机噪声注入以稳定训练并增强多样性。", "result": "相比纯规则变形方法，本框架在视觉质量上持续改进，能补充现有生成方法（如IAMGAN），实验验证了各组件有效性并提升了样本多样性。", "conclusion": "两步生成增强框架能有效生成逼真戴口罩人脸样本，为以数据为中心的人脸识别增强提供新方向，未来可进一步优化生成质量与多样性。"}}
{"id": "2512.15996", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15996", "abs": "https://arxiv.org/abs/2512.15996", "authors": ["Saiedeh Akbari", "Xuehui Shen", "Wenqian Xue", "Jordan C. Insinger", "Warren E. Dixon"], "title": "Lyapunov-based Adaptive Transformer (LyAT) for Control of Stochastic Nonlinear Systems", "comment": null, "summary": "This paper presents a novel Lyapunov-based Adaptive Transformer (LyAT) controller for stochastic nonlinear systems. While transformers have shown promise in various control applications due to sequential modeling through self-attention mechanisms, they have not been used within adaptive control architectures that provide stability guarantees. Existing transformer-based approaches for control rely on offline training with fixed weights, resulting in open-loop implementations that lack real-time adaptation capabilities and stability assurances. To address these limitations, a continuous LyAT controller is developed that adaptively estimates drift and diffusion uncertainties in stochastic dynamical systems without requiring offline pre-training. A key innovation is the analytically derived adaptation law constructed from a Lyapunov-based stability analysis, which enables real-time weight updates while guaranteeing probabilistic uniform ultimate boundedness of tracking and parameter estimation errors. Experimental validation on a quadrotor demonstrates the performance of the developed controller.", "AI": {"tldr": "本文提出了一种基于Lyapunov的自适应Transformer控制器（LyAT），用于随机非线性系统，通过实时权重更新保证跟踪和参数估计误差的概率一致有界性，并在四旋翼飞行器上进行了实验验证。", "motivation": "Transformer在控制应用中因自注意力机制展现潜力，但现有基于Transformer的控制方法依赖离线训练和固定权重，缺乏实时适应能力和稳定性保证。本研究旨在开发一种无需离线预训练、能实时适应随机系统不确定性并具有稳定性保证的自适应控制器。", "method": "设计了连续LyAT控制器，通过Lyapunov稳定性分析推导出自适应律，实时估计随机动态系统中的漂移和扩散不确定性，实现权重在线更新。", "result": "LyAT控制器在无需离线预训练的情况下，能够保证跟踪误差和参数估计误差的概率一致有界性，并在四旋翼飞行器实验中验证了其性能。", "conclusion": "LyAT控制器成功将Transformer集成到自适应控制架构中，提供了实时适应能力和稳定性保证，为随机非线性系统的控制提供了新方法。"}}
{"id": "2512.15811", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2512.15811", "abs": "https://arxiv.org/abs/2512.15811", "authors": ["Feifei Zhang", "Zhenhong Jia", "Sensen Song", "Fei Shi", "Aoxue Chen", "Dayong Ren"], "title": "Keep the Core: Adversarial Priors for Significance-Preserving Brain MRI Segmentation", "comment": null, "summary": "Medical image segmentation is constrained by sparse pathological annotations. Existing augmentation strategies, from conventional transforms to random masking for self-supervision, are feature-agnostic: they often corrupt critical diagnostic semantics or fail to prioritize essential features. We introduce \"Keep the Core,\" a novel data-centric paradigm that uses adversarial priors to guide both augmentation and masking in a significance-preserving manner. Our approach uses SAGE (Sparse Adversarial Gated Estimator), an offline module identifying minimal tokens whose micro-perturbation flips segmentation boundaries. SAGE forges the Token Importance Map $W$ by solving an adversarial optimization problem to maximally degrade performance, while an $\\ell_1$ sparsity penalty encourages a compact set of sensitive tokens. The online KEEP (Key-region Enhancement \\& Preservation) module uses $W$ for a two-pronged augmentation strategy: (1) Semantic-Preserving Augmentation: High-importance tokens are augmented, but their original pixel values are strictly restored. (2) Guided-Masking Augmentation: Low-importance tokens are selectively masked for an $\\text{MAE}$-style reconstruction, forcing the model to learn robust representations from preserved critical features. \"Keep the Core\" is backbone-agnostic with no inference overhead. Extensive experiments show SAGE's structured priors and KEEP's region-selective mechanism are highly complementary, achieving state-of-the-art segmentation robustness and generalization on 2D medical datasets.", "AI": {"tldr": "提出'Keep the Core'数据增强范式，通过对抗性先验识别关键图像区域，在保持诊断语义的前提下进行增强与掩码，提升医学图像分割的鲁棒性。", "motivation": "现有医学图像分割的数据增强方法往往忽略特征重要性，可能破坏关键诊断语义或无法聚焦核心特征，且标注数据稀缺限制了模型性能。", "method": "1. 离线模块SAGE通过对抗性优化识别对分割边界最敏感的最小化令牌集合，生成令牌重要性图W；2. 在线模块KEEP利用W进行双路径增强：对高重要性令牌进行语义保持增强（恢复原始像素），对低重要性令牌进行引导掩码增强（MAE风格重建）。", "result": "在2D医学数据集上实现了最先进的分割鲁棒性和泛化性能，方法具有骨干网络无关性且无推理开销。", "conclusion": "SAGE的结构化先验与KEEP的区域选择机制高度互补，为数据稀缺的医学图像分割提供了一种保持诊断语义的增强范式。"}}
{"id": "2512.16102", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16102", "abs": "https://arxiv.org/abs/2512.16102", "authors": ["Fanghua Li", "Xiaolin Zhou", "Yongkang Chen", "Wei Ni", "Xin Wang", "Dusit Niyato", "Ekram Hossain"], "title": "Synchronization, Identification, and Signal Detection for Underwater Photon-Counting Communications With Input-Dependent Shot Noise", "comment": null, "summary": "Photon counting (PhC) is an effective detection technology for underwater optical wireless communication (OWC) systems. The presence of signal-dependent Poisson shot noise and asynchronous multi-user interference (MUI) complicates the processing of received data signals, hindering the effective signal detection of PhC OWC systems. This paper proposes a novel iterative signal detection method in grant-free, multi-user, underwater PhC OWC systems with signal-dependent Poisson shot noise. We first introduce a new synchronization algorithm with a unique frame structure design.The algorithm performs active user identification and transmission delay estimation. Specifically, the estimation is performed first on a user group basis and then at the individual user level with reduced complexity and latency.We also develop a nonlinear iterative multi-user detection (MUD) algorithm that utilizes a detection window for each user to identify interfering symbols and estimate MUI on a slot-by-slot basis, followed by maximum \\textit{a-posteriori} probability detection of user signals.Simulations demonstrate that our scheme achieves bit error rates comparable to scenarios with transmission delays known and signal detection perfectly synchronized.", "AI": {"tldr": "提出一种用于水下光子计数光无线通信系统的迭代信号检测方法，包含新型同步算法和非线性多用户检测算法，能有效处理信号相关泊松散粒噪声和异步多用户干扰。", "motivation": "水下光子计数光无线通信系统中存在信号相关的泊松散粒噪声和异步多用户干扰，这些因素使接收信号处理复杂化，阻碍了有效的信号检测。", "method": "1. 提出具有独特帧结构设计的新型同步算法，先按用户组再按单个用户进行活跃用户识别和传输延迟估计；2. 开发非线性迭代多用户检测算法，为每个用户使用检测窗口逐时隙识别干扰符号并估计多用户干扰，然后进行最大后验概率信号检测。", "result": "仿真表明，该方案能达到与已知传输延迟和完美同步信号检测场景相当的误码率性能。", "conclusion": "所提出的方法能有效解决水下光子计数光无线通信系统中的噪声和干扰问题，实现接近理想同步条件下的通信性能。"}}
{"id": "2512.15840", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15840", "abs": "https://arxiv.org/abs/2512.15840", "authors": ["Boyuan Chen", "Tianyuan Zhang", "Haoran Geng", "Kiwhan Song", "Caiyi Zhang", "Peihao Li", "William T. Freeman", "Jitendra Malik", "Pieter Abbeel", "Russ Tedrake", "Vincent Sitzmann", "Yilun Du"], "title": "Large Video Planner Enables Generalizable Robot Control", "comment": "29 pages, 16 figures", "summary": "General-purpose robots require decision-making models that generalize across diverse tasks and environments. Recent works build robot foundation models by extending multimodal large language models (MLLMs) with action outputs, creating vision-language-action (VLA) systems. These efforts are motivated by the intuition that MLLMs' large-scale language and image pretraining can be effectively transferred to the action output modality. In this work, we explore an alternative paradigm of using large-scale video pretraining as a primary modality for building robot foundation models. Unlike static images and language, videos capture spatio-temporal sequences of states and actions in the physical world that are naturally aligned with robotic behavior. We curate an internet-scale video dataset of human activities and task demonstrations, and train, for the first time at a foundation-model scale, an open video model for generative robotics planning. The model produces zero-shot video plans for novel scenes and tasks, which we post-process to extract executable robot actions. We evaluate task-level generalization through third-party selected tasks in the wild and real-robot experiments, demonstrating successful physical execution. Together, these results show robust instruction following, strong generalization, and real-world feasibility. We release both the model and dataset to support open, reproducible video-based robot learning. Our website is available at https://www.boyuan.space/large-video-planner/.", "AI": {"tldr": "本文提出了一种基于大规模视频预训练的机器人基础模型范式，通过互联网规模的人类活动视频数据集训练生成式机器人规划模型，实现零样本视频规划并提取可执行动作。", "motivation": "现有机器人基础模型多基于多模态大语言模型（MLLMs）扩展动作输出，但视频能更自然地捕捉物理世界中的时空状态与动作序列，与机器人行为更契合，因此探索视频作为主要模态的替代范式。", "method": "构建互联网规模的人类活动与任务演示视频数据集；首次以基础模型规模训练开放式视频模型用于生成式机器人规划；通过后处理将生成的视频计划提取为可执行的机器人动作。", "result": "模型能对新场景和任务生成零样本视频计划；通过第三方野外任务评估和真实机器人实验验证了任务级泛化能力，实现了成功的物理执行，表现出鲁棒的指令跟随、强泛化性和现实可行性。", "conclusion": "大规模视频预训练是构建机器人基础模型的有效替代范式，所提出的模型与数据集支持开放、可复现的视频机器人学习，为通用机器人决策提供了新方向。"}}
{"id": "2512.15736", "categories": ["cs.AI", "cs.LG", "quant-ph"], "pdf": "https://arxiv.org/pdf/2512.15736", "abs": "https://arxiv.org/abs/2512.15736", "authors": ["S. K. Rithvik"], "title": "Anubuddhi: A Multi-Agent AI System for Designing and Simulating Quantum Optics Experiments", "comment": null, "summary": "We present Anubuddhi, a multi-agent AI system that designs and simulates quantum optics experiments from natural language prompts without requiring specialized programming knowledge. The system composes optical layouts by arranging components from a three-tier toolbox via semantic retrieval, then validates designs through physics simulation with convergent refinement. The architecture combines intent routing, knowledge-augmented generation, and dual-mode validation (QuTiP and FreeSim). We evaluated 13 experiments spanning fundamental optics (Hong-Ou-Mandel interference, Michelson/Mach-Zehnder interferometry, Bell states, delayed-choice quantum eraser), quantum information protocols (BB84 QKD, Franson interferometry, GHZ states, quantum teleportation, hyperentanglement), and advanced technologies (boson sampling, electromagnetically induced transparency, frequency conversion). The system achieves design-simulation alignment scores of 8--9/10, with simulations faithfully modeling intended physics. A critical finding distinguishes structural correctness from quantitative accuracy: high alignment confirms correct physics architecture, while numerical predictions require expert review. Free-form simulation outperformed constrained frameworks for 11/13 experiments, revealing that quantum optics diversity demands flexible mathematical representations. The system democratizes computational experiment design for research and pedagogy, producing strong initial designs users can iteratively refine through conversation.", "AI": {"tldr": "Anubuddhi是一个多智能体AI系统，能够根据自然语言提示设计和模拟量子光学实验，无需专业编程知识，通过语义检索组合光学元件并进行物理模拟验证。", "motivation": "量子光学实验设计通常需要专业编程和物理知识，限制了非专家用户的参与。本研究旨在开发一个系统，通过自然语言交互降低门槛，使研究者和学生能够更便捷地设计和验证量子光学实验。", "method": "系统采用多智能体架构，结合意图路由、知识增强生成和双模式验证（QuTiP和FreeSim）。通过语义检索从三层工具箱中选择光学元件组成实验布局，并通过物理模拟进行收敛性优化验证。", "result": "在13个实验中（涵盖基础光学、量子信息协议和先进技术），系统获得8-9/10的设计-模拟对齐分数，模拟结果能准确反映预期物理现象。研究发现结构正确性与定量准确性需区分：高对齐度确认物理架构正确，但数值预测仍需专家审核。自由模拟在11/13实验中优于受限框架。", "conclusion": "Anubuddhi系统通过自然语言交互实现了量子光学实验设计的民主化，为研究和教学提供了强大的初始设计工具。用户可通过对话迭代优化设计，系统的灵活性能够适应量子光学实验的多样性需求。"}}
{"id": "2512.15916", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.15916", "abs": "https://arxiv.org/abs/2512.15916", "authors": ["Enrique Rodríguez-Miranda", "Pablo Otálora", "José González-Hernández", "José Luis Guzmán", "Manuel Berenguel"], "title": "A Comprehensive Benchmark Platform for Process Control Research of Outdoor Microalgae Raceway Reactors", "comment": null, "summary": "This paper presents a benchmarking framework to evaluate process control strategies in outdoor microalgae raceway reactors, integrating four key control regulation tasks: pH, dissolved oxygen (DO), culture volume through coordinated harvest-dilution actions, and temperature via a sump-mounted spiral heat exchanger. The benchmark is built upon a high-fidelity, experimentally calibrated dynamic model that captures the strongly coupled thermal, physicochemical, and biological processes governing industrial-scale open raceway ponds. A closed-loop simulation environment is provided, featuring realistic actuator constraints, gas transport delays, stiff integration, and a fully specified scenario based on multi-day outdoor disturbances (irradiance, temperature, wind, and humidity). Four user-replaceable controllers define the manipulation of CO2 injection, air bubbling, harvest/dilution sequencing, and heat-exchanger operation. The platform computes a unified global performance index, in addition to individual metrics for each control problem, combining tracking error, gas and energy usage, and biomass productivity, enabling consistent and quantitative comparison of alternative control strategies. Baseline regulatory architectures (On/Off, PI/PID, and Economic Model Predictive Control (EMPC)) are included to illustrate the benchmark use for classical and advanced control methods. By providing an openly specified, reproducible, and computationally tractable benchmark with well-defined function interfaces, this work aims to bridge control methodology and outdoor algal bioprocess engineering, and to support the development of multivariable control strategies for disturbance-rich environmental systems.", "AI": {"tldr": "本文提出了一个用于评估户外微藻跑道反应器过程控制策略的基准测试框架，整合了pH值、溶解氧、培养体积和温度四个关键控制调节任务，基于高保真动态模型构建闭环仿真环境，并提供了统一的性能评估指标。", "motivation": "户外藻类生物过程面临复杂的环境扰动（光照、温度、风速、湿度等），且控制任务之间存在强耦合关系，缺乏标准化的评估框架来比较不同控制策略在工业规模开放跑道池中的性能。", "method": "建立基于实验校准的高保真动态模型，模拟热力学、物理化学和生物过程的耦合；设计包含实际执行器约束、气体传输延迟和室外多日扰动的闭环仿真场景；提供可替换的控制器接口（CO2注入、曝气、收获/稀释序列、热交换器操作）；开发统一的全局性能指标和分项指标。", "result": "构建了一个公开可重复、计算可行的基准测试平台，包含经典控制方法（开关控制、PI/PID）和先进控制方法（经济模型预测控制EMPC）的基准架构，支持多变量控制策略在扰动环境系统中的定量比较。", "conclusion": "该基准框架旨在连接控制方法与户外藻类生物过程工程，通过标准化评估促进针对复杂环境系统的多变量控制策略开发，为工业规模藻类培养的优化控制提供工具支持。"}}
{"id": "2512.15820", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15820", "abs": "https://arxiv.org/abs/2512.15820", "authors": ["Stefan Dvoretskii", "Anwai Archit", "Constantin Pape", "Josh Moore", "Marco Nolden"], "title": "BioimageAIpub: a toolbox for AI-ready bioimaging data publishing", "comment": null, "summary": "Modern bioimage analysis approaches are data hungry, making it necessary for researchers to scavenge data beyond those collected within their (bio)imaging facilities. In addition to scale, bioimaging datasets must be accompanied with suitable, high-quality annotations and metadata. Although established data repositories such as the Image Data Resource (IDR) and BioImage Archive offer rich metadata, their contents typically cannot be directly consumed by image analysis tools without substantial data wrangling. Such a tedious assembly and conversion of (meta)data can account for a dedicated amount of time investment for researchers, hindering the development of more powerful analysis tools. Here, we introduce BioimageAIpub, a workflow that streamlines bioimaging data conversion, enabling a seamless upload to HuggingFace, a widely used platform for sharing machine learning datasets and models.", "AI": {"tldr": "BioimageAIpub工作流简化生物成像数据转换，实现与HuggingFace平台的无缝对接，解决数据获取与预处理难题。", "motivation": "生物图像分析需要大规模高质量标注数据，但现有数据仓库（如IDR、BioImage Archive）的数据需繁琐处理才能被分析工具使用，阻碍了分析工具的发展。", "method": "开发BioimageAIpub工作流，实现生物成像数据的标准化转换与格式适配，支持直接上传至HuggingFace平台。", "result": "该工作流简化了数据转换流程，使研究人员能更便捷地获取和使用标准化生物成像数据集。", "conclusion": "BioimageAIpub降低了生物成像数据的使用门槛，有望加速生物图像分析工具的开发与应用。"}}
{"id": "2512.15994", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15994", "abs": "https://arxiv.org/abs/2512.15994", "authors": ["Manuel Mekkattu", "Mike Y. Michelis", "Robert K. Katzschmann"], "title": "SORS: A Modular, High-Fidelity Simulator for Soft Robots", "comment": "This work has been submitted to the IEEE for possible publication. Code and data are available at github.com/srl-ethz/sors", "summary": "The deployment of complex soft robots in multiphysics environments requires advanced simulation frameworks that not only capture interactions between different types of material, but also translate accurately to real-world performance. Soft robots pose unique modeling challenges due to their large nonlinear deformations, material incompressibility, and contact interactions, which complicate both numerical stability and physical accuracy. Despite recent progress, robotic simulators often struggle with modeling such phenomena in a scalable and application-relevant manner. We present SORS (Soft Over Rigid Simulator), a versatile, high-fidelity simulator designed to handle these complexities for soft robot applications. Our energy-based framework, built on the finite element method, allows modular extensions, enabling the inclusion of custom-designed material and actuation models. To ensure physically consistent contact handling, we integrate a constrained nonlinear optimization based on sequential quadratic programming, allowing for stable and accurate modeling of contact phenomena. We validate our simulator through a diverse set of real-world experiments, which include cantilever deflection, pressure-actuation of a soft robotic arm, and contact interactions from the PokeFlex dataset. In addition, we showcase the potential of our framework for control optimization of a soft robotic leg. These tests confirm that our simulator can capture both fundamental material behavior and complex actuation dynamics with high physical fidelity. By bridging the sim-to-real gap in these challenging domains, our approach provides a validated tool for prototyping next-generation soft robots, filling the gap of extensibility, fidelity, and usability in the soft robotic ecosystem.", "AI": {"tldr": "提出SORS软体机器人仿真器，基于有限元方法和能量框架，通过约束非线性优化处理接触问题，实现了高保真、可扩展的软体机器人仿真。", "motivation": "现有机器人仿真器难以准确模拟软体机器人的大变形、材料不可压缩性和接触交互等复杂物理现象，缺乏可扩展性和应用相关性。", "method": "基于有限元方法构建能量框架，支持模块化扩展材料与驱动模型；采用基于序列二次规划的约束非线性优化方法处理物理一致的接触交互。", "result": "通过悬臂梁弯曲、软体臂压力驱动、PokeFlex数据集接触实验验证，仿真器能准确捕捉材料基本行为和复杂驱动动力学；软体机器人腿控制优化案例展示了框架的应用潜力。", "conclusion": "SORS仿真器在可扩展性、保真度和可用性方面填补了软体机器人生态系统的空白，为下一代软体机器人原型设计提供了经过验证的工具，有效缩小了仿真与现实的差距。"}}
{"id": "2512.16011", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16011", "abs": "https://arxiv.org/abs/2512.16011", "authors": ["Jack Naylor", "Raghav Mishra", "Nicholas H. Barbara", "Donald G. Dansereau"], "title": "dLITE: Differentiable Lighting-Informed Trajectory Evaluation for On-Orbit Inspection", "comment": "13 pages, 9 images", "summary": "Visual inspection of space-borne assets is of increasing interest to spacecraft operators looking to plan maintenance, characterise damage, and extend the life of high-value satellites in orbit. The environment of Low Earth Orbit (LEO) presents unique challenges when planning inspection operations that maximise visibility, information, and data quality. Specular reflection of sunlight from spacecraft bodies, self-shadowing, and dynamic lighting in LEO significantly impact the quality of data captured throughout an orbit. This is exacerbated by the relative motion between spacecraft, which introduces variable imaging distances and attitudes during inspection. Planning inspection trajectories with the aide of simulation is a common approach. However, the ability to design and optimise an inspection trajectory specifically to improve the resulting image quality in proximity operations remains largely unexplored. In this work, we present $\\partial$LITE, an end-to-end differentiable simulation pipeline for on-orbit inspection operations. We leverage state-of-the-art differentiable rendering tools and a custom orbit propagator to enable end-to-end optimisation of orbital parameters based on visual sensor data. $\\partial$LITE enables us to automatically design non-obvious trajectories, vastly improving the quality and usefulness of attained data. To our knowledge, our differentiable inspection-planning pipeline is the first of its kind and provides new insights into modern computational approaches to spacecraft mission planning. Project page: https://appearance-aware.github.io/dlite/", "AI": {"tldr": "提出了一种名为∂LITE的端到端可微分仿真管道，用于优化在轨航天器视觉检测任务的轨迹规划，以提高图像质量和数据实用性。", "motivation": "低地球轨道（LEO）环境中的航天器视觉检测面临阳光镜面反射、自阴影和动态光照等挑战，现有仿真方法难以优化轨迹以提升图像质量。", "method": "结合先进的可微分渲染工具和自定义轨道传播器，构建端到端可微分仿真管道，通过视觉传感器数据优化轨道参数。", "result": "∂LITE能够自动设计非直观的检测轨迹，显著提升获取数据的质量和实用性。", "conclusion": "该可微分检测规划管道是首创的，为航天器任务规划的现代计算方法提供了新思路。"}}
{"id": "2512.15740", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.15740", "abs": "https://arxiv.org/abs/2512.15740", "authors": ["Timothy Prescher"], "title": "The Principle of Proportional Duty: A Knowledge-Duty Framework for Ethical Equilibrium in Human and Artificial Systems", "comment": "46 pages, 2 figures. Preregistered at OSF on Nov 14, 2025 (https://doi.org/10.17605/OSF.IO/BMVP3). Includes comparative analysis with OpenAI's 'Confessions' paper (Dec 3, 2025)", "summary": "Traditional ethical frameworks often struggle to model decision-making under uncertainty, treating it as a simple constraint on action. This paper introduces the Principle of Proportional Duty (PPD), a novel framework that models how ethical responsibility scales with an agent's epistemic state. The framework reveals that moral duty is not lost to uncertainty but transforms: as uncertainty increases, Action Duty (the duty to act decisively) is proportionally converted into Repair Duty (the active duty to verify, inquire, and resolve uncertainty).\n  This dynamic is expressed by the equation D_total = K[(1-HI) + HI * g(C_signal)], where Total Duty is a function of Knowledge (K), Humility/Uncertainty (HI), and Contextual Signal Strength (C_signal). Monte Carlo simulations demonstrate that systems maintaining a baseline humility coefficient (lambda > 0) produce more stable duty allocations and reduce the risk of overconfident decision-making.\n  By formalizing humility as a system parameter, the PPD offers a mathematically tractable approach to moral responsibility that could inform the development of auditable AI decision systems. This paper applies the framework across four domains, clinical ethics, recipient-rights law, economic governance, and artificial intelligence, to demonstrate its cross-disciplinary validity. The findings suggest that proportional duty serves as a stabilizing principle within complex systems, preventing both overreach and omission by dynamically balancing epistemic confidence against contextual risk.", "AI": {"tldr": "本文提出比例责任原则（PPD），将道德责任建模为随智能体认知状态变化的函数，通过数学公式和模拟证明该框架能减少过度自信决策，适用于临床伦理、法律、经济治理和人工智能等多个领域。", "motivation": "传统伦理框架难以处理不确定性下的决策问题，常将其简单视为行动约束，需要一种能动态量化责任与认知状态关系的理论框架。", "method": "提出比例责任原则理论框架，建立总责任公式 D_total = K[(1-HI) + HI * g(C_signal)]，并通过蒙特卡洛模拟验证系统稳定性。", "result": "模拟显示保持适度谦逊系数（λ>0）的系统能产生更稳定的责任分配，降低过度自信决策风险；框架在四个跨学科领域中验证有效。", "conclusion": "比例责任原则通过将谦逊参数化，为道德责任提供了可数学处理的模型，可作为复杂系统中的稳定机制，平衡认知信心与情境风险。"}}
{"id": "2512.15885", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.15885", "abs": "https://arxiv.org/abs/2512.15885", "authors": ["Davide Caffagni", "Sara Sarto", "Marcella Cornia", "Lorenzo Baraldi", "Pier Luigi Dovesi", "Shaghayegh Roohi", "Mark Granroth-Wilding", "Rita Cucchiara"], "title": "Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have recently demonstrated impressive capabilities in connecting vision and language, yet their proficiency in fundamental visual reasoning tasks remains limited. This limitation can be attributed to the fact that MLLMs learn visual understanding primarily from textual descriptions, which constitute a subjective and inherently incomplete supervisory signal. Furthermore, the modest scale of multimodal instruction tuning compared to massive text-only pre-training leads MLLMs to overfit language priors while overlooking visual details. To address these issues, we introduce JARVIS, a JEPA-inspired framework for self-supervised visual enhancement in MLLMs. Specifically, we integrate the I-JEPA learning paradigm into the standard vision-language alignment pipeline of MLLMs training. Our approach leverages frozen vision foundation models as context and target encoders, while training the predictor, implemented as the early layers of an LLM, to learn structural and semantic regularities from images without relying exclusively on language supervision. Extensive experiments on standard MLLM benchmarks show that JARVIS consistently improves performance on vision-centric benchmarks across different LLM families, without degrading multimodal reasoning abilities. Our source code is publicly available at: https://github.com/aimagelab/JARVIS.", "AI": {"tldr": "提出JARVIS框架，通过自监督视觉增强解决MLLMs在视觉推理任务中的不足，利用冻结视觉基础模型和I-JEPA学习范式提升视觉理解能力。", "motivation": "当前多模态大语言模型（MLLMs）的视觉理解主要依赖文本描述，这种监督信号具有主观性和不完整性，且多模态指令调优规模有限，导致模型过度依赖语言先验而忽略视觉细节。", "method": "引入JEPA启发的自监督视觉增强框架JARVIS，将I-JEPA学习范式集成到MLLMs训练中，使用冻结视觉基础模型作为上下文和目标编码器，训练LLM早期层作为预测器，从图像中学习结构性和语义规律。", "result": "在标准MLLM基准测试中，JARVIS在不同LLM家族上均能持续提升视觉中心任务的性能，且不影响多模态推理能力。", "conclusion": "JARVIS框架通过自监督视觉增强有效解决了MLLMs视觉理解不足的问题，为提升多模态模型的视觉推理能力提供了新方向。"}}
{"id": "2512.15925", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2512.15925", "abs": "https://arxiv.org/abs/2512.15925", "authors": ["Joel Mire", "Maria Antoniak", "Steven R. Wilson", "Zexin Ma", "Achyutarama R. Ganti", "Andrew Piper", "Maarten Sap"], "title": "Social Story Frames: Contextual Reasoning about Narrative Intent and Reception", "comment": "Presented at IC2S2 2025; Under Review (ARR Oct 2025)", "summary": "Reading stories evokes rich interpretive, affective, and evaluative responses, such as inferences about narrative intent or judgments about characters. Yet, computational models of reader response are limited, preventing nuanced analyses. To address this gap, we introduce SocialStoryFrames, a formalism for distilling plausible inferences about reader response, such as perceived author intent, explanatory and predictive reasoning, affective responses, and value judgments, using conversational context and a taxonomy grounded in narrative theory, linguistic pragmatics, and psychology. We develop two models, SSF-Generator and SSF-Classifier, validated through human surveys (N=382 participants) and expert annotations, respectively. We conduct pilot analyses to showcase the utility of the formalism for studying storytelling at scale. Specifically, applying our models to SSF-Corpus, a curated dataset of 6,140 social media stories from diverse contexts, we characterize the frequency and interdependence of storytelling intents, and we compare and contrast narrative practices (and their diversity) across communities. By linking fine-grained, context-sensitive modeling with a generic taxonomy of reader responses, SocialStoryFrames enable new research into storytelling in online communities.", "AI": {"tldr": "本文提出SocialStoryFrames形式化框架，用于从社交媒体故事中提取读者反应的多种推断（如作者意图、情感反应等），并开发了生成与分类模型，通过大规模故事分析展示了其在研究在线社区叙事实践中的应用价值。", "motivation": "现有计算模型难以捕捉读者对故事产生的丰富解释性、情感性和评价性反应（如对叙事意图的推断、角色判断等），限制了细粒度分析。", "method": "1. 基于叙事理论、语言语用学和心理学构建读者反应分类体系；2. 开发SSF-Generator（生成模型）和SSF-Classifier（分类模型）；3. 通过人类调查（N=382）和专家标注验证模型；4. 在SSF-Corpus（6,140个社交媒体故事数据集）上进行试点分析。", "result": "1. 模型经人类验证有效；2. 在跨社区故事分析中，成功量化了叙事意图的频率与相互依赖性，并对比了不同社区的叙事实践多样性。", "conclusion": "SocialStoryFrames通过结合细粒度情境建模与通用读者反应分类体系，为在线社区叙事研究提供了新工具，支持大规模故事分析。"}}
{"id": "2512.16139", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16139", "abs": "https://arxiv.org/abs/2512.16139", "authors": ["Mengqi Xue", "Yuchao Xiong", "Yue Song"], "title": "Consensus tracking of perturbed open multi-agent systems with repelling antagonistic interactions", "comment": null, "summary": "An open multi-agent system (OMAS) comprises migrating agents which produce a flexible network structure that is naturally switching and size-varying. Meanwhile, agent migrations also make an OMAS more prone to environmental adversities. In this work, we deal with the consensus tracking problem of OMASs suffering these migration-induced adversities, including non-vanishing perturbations in the agent dynamics/state and the repelling antagonistic interactions among agents, over an intermittently disconnected signed digraph. The OMAS is interpreted into a perturbed $M^3D$ system in which unstable subsystems are created when repelling interactions dominate the normal cooperative ones in the OMAS network regardless of its connectivity. To handle the destabilizing effects brought by the repelling interaction as well as the non-vanishing perturbations, we extend the stability theory for $M^3D$ systems and apply it to the OMAS to show that practical consensus tracking can be achieved if the migration-induced switching satisfies the piecewise average dwell time and activation time ratio constraints. Particularly, we indicate that for vanishing perturbations and repelling interactions, asymptotic tracking can be expected under weaker switching constraints.", "AI": {"tldr": "本文研究了开放多智能体系统在迁移引起的非消失扰动和排斥相互作用下的共识跟踪问题，通过扩展M^3D系统稳定性理论，证明了在满足切换时间约束条件下可实现实际共识跟踪。", "motivation": "开放多智能体系统中的智能体迁移会引入环境干扰（如非消失扰动和智能体间的排斥相互作用），这些干扰可能导致系统不稳定，因此需要研究如何在这种动态网络环境下实现共识跟踪。", "method": "将开放多智能体系统建模为受扰动的M^3D系统，扩展M^3D系统的稳定性理论，分析在间歇断开的符号有向图上满足分段平均驻留时间和激活时间比约束的切换条件。", "result": "当迁移引起的切换满足分段平均驻留时间和激活时间比约束时，即使存在非消失扰动和排斥相互作用，系统也能实现实际共识跟踪；若扰动和排斥作用可消失，则可在更弱的切换约束下实现渐近跟踪。", "conclusion": "通过适当的切换时间约束，开放多智能体系统能够克服迁移引起的干扰，实现稳定的共识跟踪，为动态网络环境下的多智能体协调控制提供了理论保证。"}}
{"id": "2512.15905", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2512.15905", "abs": "https://arxiv.org/abs/2512.15905", "authors": ["Nik Bhatt"], "title": "SNIC: Synthesized Noisy Images using Calibration", "comment": "11 pages including Appendix, 15 figures and 3 tables", "summary": "Advanced denoising algorithms require large, high-quality datasets. Physics-based, statistical noise models can create such datasets by realistically simulating noise in digital images. However, there is little information on the correct way to calibrate and tune these heteroscedastic models, and a lack of published datasets using them. In this paper, we explore the process of building high-quality heteroscedastic noise models. Our methods produce realistic synthesized noisy images, in both RAW and TIFF formats. Our synthesized noisy images achieve comparable LPIPS results to real noisy images, and greatly outperform those created with manufacturer-provided DNG noise models both in LPIPS and when tested with a state-of-the-art (SOTA) denoising model. Using our approach, we created the Synthesized Noisy Images using Calibration dataset (SNIC) containing over 6000 noisy images, comprising 30 scenes from four sensors, including two smartphone sensors, a point-and-shoot, and a DSLR. SNIC is the first synthesized noisy image dataset provided in both RAW and TIFF format.", "AI": {"tldr": "本文提出了一种构建高质量异方差噪声模型的方法，创建了包含6000多张噪声图像的SNIC数据集，支持RAW和TIFF格式，其合成噪声图像在LPIPS指标上与真实噪声图像相当，并显著优于制造商提供的DNG噪声模型。", "motivation": "先进的去噪算法需要大规模高质量数据集，而基于物理的统计噪声模型可以通过模拟数字图像中的噪声来创建此类数据集。但目前缺乏关于如何正确校准和调整这些异方差噪声模型的指导，以及使用这些模型发布的公开数据集。", "method": "探索构建高质量异方差噪声模型的过程，生成逼真的合成噪声图像（RAW和TIFF格式），创建包含30个场景、来自四个传感器（包括两个智能手机传感器、一个傻瓜相机和一个单反相机）的SNIC数据集。", "result": "合成噪声图像在LPIPS指标上与真实噪声图像结果相当，在使用最先进去噪模型测试时，显著优于制造商提供的DNG噪声模型生成的图像。", "conclusion": "提出的方法能够生成逼真的合成噪声图像，创建的SNIC数据集是第一个同时提供RAW和TIFF格式的合成噪声图像数据集，为去噪算法的发展提供了高质量的训练数据资源。"}}
{"id": "2512.15959", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15959", "abs": "https://arxiv.org/abs/2512.15959", "authors": ["Armağan Amcalar", "Eyup Cinar"], "title": "BRAID: Bounded Reasoning for Autonomous Inference and Decisions", "comment": null, "summary": "Large Language Models (LLMs) exhibit nonlinear relationships between performance, cost, and token usage. This paper presents a quantitative study on structured prompting using BRAID (Bounded Reasoning for Au tonomous Inference and Decisions) across multiple GPT model tiers, eval uated on the AdvancedIF, GSM-Hard, and the SCALE MultiChallenge benchmark datasets. BRAID introduces a bounded reasoning framework using Mermaid-based instruction graphs that enable models to reason struc turally rather than through unbounded natural-language token expansion. We show that structured machine-readable prompts substantially increase reasoning accuracy and cost efficiency for agents in production systems. The findings establish BRAID as an effective and scalable technique for optimizing inference efficiency in autonomous agent systems. All datasets and detailed result logs are available at https://benchmark.openserv.ai.", "AI": {"tldr": "本文提出BRAID框架，通过结构化提示提升LLM推理效率，在多个基准测试中验证了其提高准确率和成本效益的效果。", "motivation": "大型语言模型存在性能、成本与token使用之间的非线性关系，传统自然语言提示可能导致无限制的token扩展，需要更高效的结构化推理方法。", "method": "采用BRAID框架，使用基于Mermaid的指令图进行有界推理，在GPT多层级模型上进行测试，评估数据集包括AdvancedIF、GSM-Hard和SCALE MultiChallenge。", "result": "结构化机器可读提示显著提高了推理准确性和成本效率，BRAID被证明是优化自主代理系统推理效率的有效且可扩展技术。", "conclusion": "BRAID框架通过结构化推理替代无限制自然语言扩展，为生产系统中的智能代理提供了高效解决方案，相关数据集和结果已开源。"}}
{"id": "2512.15933", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15933", "abs": "https://arxiv.org/abs/2512.15933", "authors": ["Dwip Dalal", "Utkarsh Mishra", "Narendra Ahuja", "Nebojsa Jojic"], "title": "City Navigation in the Wild: Exploring Emergent Navigation from Web-Scale Knowledge in MLLMs", "comment": null, "summary": "Leveraging multimodal large language models (MLLMs) to develop embodied agents offers significant promise for addressing complex real-world tasks. However, current evaluation benchmarks remain predominantly language-centric or heavily reliant on simulated environments, rarely probing the nuanced, knowledge-intensive reasoning essential for practical, real-world scenarios. To bridge this critical gap, we introduce the task of Sparsely Grounded Visual Navigation, explicitly designed to evaluate the sequential decision-making abilities of MLLMs in challenging, knowledge-intensive real-world environments. We operationalize this task with CityNav, a comprehensive benchmark encompassing four diverse global cities, specifically constructed to assess raw MLLM-driven agents in city navigation. Agents are required to rely solely on visual inputs and internal multimodal reasoning to sequentially navigate 50+ decision points without additional environmental annotations or specialized architectural modifications. Crucially, agents must autonomously achieve localization through interpreting city-specific cues and recognizing landmarks, perform spatial reasoning, and strategically plan and execute routes to their destinations. Through extensive evaluations, we demonstrate that current state-of-the-art MLLMs and standard reasoning techniques (e.g., Chain-of-Thought, Reflection) significantly underperform in this challenging setting. To address this, we propose Verbalization of Path (VoP), which explicitly grounds the agent's internal reasoning by probing an explicit cognitive map (key landmarks and directions toward the destination) from the MLLMs, substantially enhancing navigation success. Project Webpage: https://dwipddalal.github.io/AgentNav/", "AI": {"tldr": "本文提出了稀疏接地视觉导航任务及CityNav基准，用于评估多模态大语言模型在真实城市环境中基于视觉的序列决策能力，并提出了显式认知地图的言语化方法以提升导航性能。", "motivation": "当前基于多模态大语言模型的具身智能体评估基准过于依赖语言或仿真环境，缺乏对真实世界知识密集型推理能力的测试，因此需要构建更贴近实际复杂场景的评估框架。", "method": "设计了CityNav基准，涵盖四个全球城市，要求智能体仅凭视觉输入在50多个决策点上进行导航；提出了路径言语化方法，通过显式构建关键地标和方向的认知地图来增强模型推理。", "result": "实验表明，现有最先进的多模态大语言模型及标准推理技术在CityNav上表现显著不足；而提出的路径言语化方法能大幅提升导航成功率。", "conclusion": "真实世界的知识密集型导航对当前多模态大语言模型仍具挑战，显式构建认知地图的推理方法能有效提升其空间决策能力，为未来具身智能研究提供了重要基准与改进方向。"}}
{"id": "2512.16263", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16263", "abs": "https://arxiv.org/abs/2512.16263", "authors": ["Bosen Yang", "Kang Ma", "Jin Lin", "Yonghua Song"], "title": "Black-Start Power Capacity Sizing and Control Strategy for an Islanded DFIG Wind-to-Hydrogen System", "comment": null, "summary": "This paper proposes a black-start method for an off-grid wind-to-hydrogen (W2H) system comprising a wind farm based on Doubly-Fed Induction Generators (DFIGs), proton exchange membrane fuel cells (PEMFCs) serving as the black-start power source, and a hydrogen production industry. The PEMFC is installed within the hydrogen industry to facilitate direct access to hydrogen fuel. Based on the microgrid topology and black-start scheme, this study innovatively sizes the rated capacity of the PEMFC through power flow analysis. The capacity must be sufficient to charge passive components such as transmission lines and transformers, provide rotor excitation, and supply wind turbine (WT) and electrolyzer (ELZ) auxiliaries during startup. The proposed system integrates wind-hydrogen coordinated control (WHCC) and hydrogen-storage coordinated control (HSCC). Under maximum power point tracking (MPPT) of the WTs, the ELZ follows power fluctuations to absorb wind output, ensuring stable voltage and frequency. Fixed-frequency control applied to either the DFIG or PEMFC converters enables DFIGs to retain conventional grid-following (GFL) operation, reducing converter development costs. For both control modes, this paper establishes the black-start sequence and formulates a comprehensive coordinated control strategy for the entire system. The entire control system is validated through simulations in MATLAB/Simulink. Results confirm that the calculated PEMFC capacity supports reliable black-start, while the black-start control strategy ensures smooth system self-startup. Furthermore, the coordinated control strategy maintains stable frequency and voltage under fluctuating wind power, demonstrating the practicality and robustness of the proposed approach.", "AI": {"tldr": "提出了一种离网风电制氢系统的黑启动方法，通过质子交换膜燃料电池作为启动电源，结合风电-氢协调控制和氢储能协调控制，实现了系统在风电波动下的稳定自启动和运行。", "motivation": "离网风电制氢系统在电网故障后需要可靠的黑启动能力，以恢复供电并维持系统稳定运行，同时需解决风电波动性和系统组件协调控制问题。", "method": "基于微电网拓扑和黑启动方案，通过潮流分析确定质子交换膜燃料电池的额定容量；提出风电-氢协调控制和氢储能协调控制策略；采用固定频率控制使双馈感应发电机保持常规电网跟随运行；在MATLAB/Simulink中进行仿真验证。", "result": "计算得出的质子交换膜燃料电池容量支持可靠黑启动；黑启动控制策略确保系统平稳自启动；协调控制策略在风电波动下维持频率和电压稳定。", "conclusion": "所提方法具有实用性和鲁棒性，能够实现离网风电制氢系统的可靠黑启动和稳定运行，为可再生能源与氢能集成系统提供了有效的解决方案。"}}
{"id": "2512.15940", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15940", "abs": "https://arxiv.org/abs/2512.15940", "authors": ["Tin Stribor Sohn", "Maximilian Dillitzer", "Jason J. Corso", "Eric Sax"], "title": "R4: Retrieval-Augmented Reasoning for Vision-Language Models in 4D Spatio-Temporal Space", "comment": null, "summary": "Humans perceive and reason about their surroundings in four dimensions by building persistent, structured internal representations that encode semantic meaning, spatial layout, and temporal dynamics. These multimodal memories enable them to recall past events, infer unobserved states, and integrate new information into context-dependent reasoning. Inspired by this capability, we introduce R4, a training-free framework for retrieval-augmented reasoning in 4D spatio-temporal space that equips vision-language models (VLMs) with structured, lifelong memory. R4 continuously constructs a 4D knowledge database by anchoring object-level semantic descriptions in metric space and time, yielding a persistent world model that can be shared across agents. At inference, natural language queries are decomposed into semantic, spatial, and temporal keys to retrieve relevant observations, which are integrated into the VLM's reasoning. Unlike classical retrieval-augmented generation methods, retrieval in R4 operates directly in 4D space, enabling episodic and collaborative reasoning without training. Experiments on embodied question answering and navigation benchmarks demonstrate that R4 substantially improves retrieval and reasoning over spatio-temporal information compared to baselines, advancing a new paradigm for embodied 4D reasoning in dynamic environments.", "AI": {"tldr": "提出R4框架，无需训练即可为视觉语言模型构建4D时空记忆，通过检索增强推理能力，在具身问答和导航任务中表现优异。", "motivation": "受人类通过构建持久结构化内部表征来感知和推理四维环境的能力启发，旨在为视觉语言模型赋予类似的4D时空记忆与推理能力。", "method": "R4框架持续构建4D知识库，将物体级语义描述锚定在度量空间和时间中，通过自然语言查询分解为语义、空间和时间键来检索相关观察，并集成到视觉语言模型的推理中。", "result": "在具身问答和导航基准测试中，R4在时空信息检索和推理方面显著优于基线方法。", "conclusion": "R4为动态环境中的具身4D推理提供了新范式，实现了无需训练的协作推理和情景推理能力。"}}
{"id": "2512.15921", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15921", "abs": "https://arxiv.org/abs/2512.15921", "authors": ["Lena Giebeler", "Deepa Krishnaswamy", "David Clunie", "Jakob Wasserthal", "Lalith Kumar Shiyam Sundar", "Andres Diaz-Pinto", "Klaus H. Maier-Hein", "Murong Xu", "Bjoern Menze", "Steve Pieper", "Ron Kikinis", "Andrey Fedorov"], "title": "In search of truth: Evaluating concordance of AI-based anatomy segmentation models", "comment": null, "summary": "Purpose AI-based methods for anatomy segmentation can help automate characterization of large imaging datasets. The growing number of similar in functionality models raises the challenge of evaluating them on datasets that do not contain ground truth annotations. We introduce a practical framework to assist in this task. Approach We harmonize the segmentation results into a standard, interoperable representation, which enables consistent, terminology-based labeling of the structures. We extend 3D Slicer to streamline loading and comparison of these harmonized segmentations, and demonstrate how standard representation simplifies review of the results using interactive summary plots and browser-based visualization using OHIF Viewer. To demonstrate the utility of the approach we apply it to evaluating segmentation of 31 anatomical structures (lungs, vertebrae, ribs, and heart) by six open-source models - TotalSegmentator 1.5 and 2.6, Auto3DSeg, MOOSE, MultiTalent, and CADS - for a sample of Computed Tomography (CT) scans from the publicly available National Lung Screening Trial (NLST) dataset. Results We demonstrate the utility of the framework in enabling automating loading, structure-wise inspection and comparison across models. Preliminary results ascertain practical utility of the approach in allowing quick detection and review of problematic results. The comparison shows excellent agreement segmenting some (e.g., lung) but not all structures (e.g., some models produce invalid vertebrae or rib segmentations). Conclusions The resources developed are linked from https://imagingdatacommons.github.io/segmentation-comparison/ including segmentation harmonization scripts, summary plots, and visualization tools. This work assists in model evaluation in absence of ground truth, ultimately enabling informed model selection.", "AI": {"tldr": "提出一个用于评估无标注数据上解剖结构分割模型的框架，通过标准化表示和可视化工具比较多个开源模型在CT图像上的分割性能。", "motivation": "随着功能相似的AI分割模型增多，如何在缺乏真实标注的数据集上评估这些模型成为挑战，需要一种实用的评估框架。", "method": "将分割结果统一为标准化的互操作表示，扩展3D Slicer工具加载和比较分割结果，结合交互式摘要图和基于浏览器的OHIF可视化，并在NLST数据集上评估6个开源模型对31个解剖结构的分割效果。", "result": "框架能自动化加载和跨模型比较分割结果，初步验证了其快速检测问题结果的实用性；不同模型对某些结构（如肺）分割一致性好，但对其他结构（如椎骨、肋骨）存在无效分割。", "conclusion": "开发的资源（包括标准化脚本、摘要图和可视化工具）有助于在无真实标注情况下评估模型，最终支持明智的模型选择。"}}
{"id": "2512.15743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15743", "abs": "https://arxiv.org/abs/2512.15743", "authors": ["David Noever"], "title": "Prompt-to-Parts: Generative AI for Physical Assembly and Scalable Instructions", "comment": null, "summary": "We present a framework for generating physically realizable assembly instructions from natural language descriptions. Unlike unconstrained text-to-3D approaches, our method operates within a discrete parts vocabulary, enforcing geometric validity, connection constraints, and buildability ordering. Using LDraw as a text-rich intermediate representation, we demonstrate that large language models can be guided with tools to produce valid step-by-step construction sequences and assembly instructions for brick-based prototypes of more than 3000 assembly parts. We introduce a Python library for programmatic model generation and evaluate buildable outputs on complex satellites, aircraft, and architectural domains. The approach aims for demonstrable scalability, modularity, and fidelity that bridges the gap between semantic design intent and manufacturable output. Physical prototyping follows from natural language specifications. The work proposes a novel elemental lingua franca as a key missing piece from the previous pixel-based diffusion methods or computer-aided design (CAD) models that fail to support complex assembly instructions or component exchange. Across four original designs, this novel \"bag of bricks\" method thus functions as a physical API: a constrained vocabulary connecting precisely oriented brick locations to a \"bag of words\" through which arbitrary functional requirements compile into material reality. Given such a consistent and repeatable AI representation opens new design options while guiding natural language implementations in manufacturing and engineering prototyping.", "AI": {"tldr": "提出一个从自然语言描述生成物理可实现的装配指令的框架，使用离散零件词汇表和LDraw中间表示，通过大型语言模型生成有效的分步构建序列，支持超过3000个零件的砖块原型装配。", "motivation": "现有基于像素的扩散方法或CAD模型无法支持复杂装配指令或组件交换，需要一种能够连接语义设计意图与可制造输出的方法，实现从自然语言规范到物理原型的直接转换。", "method": "采用离散零件词汇表确保几何有效性、连接约束和构建顺序；使用LDraw作为文本丰富的中间表示；开发Python库进行程序化模型生成；通过大型语言模型结合工具引导生成有效装配指令。", "result": "在复杂卫星、飞机和建筑领域评估可构建输出，成功生成超过3000个零件的装配指令；提出的“砖块袋”方法作为物理API，将精确定向的砖块位置与“词袋”连接，实现任意功能需求到物质现实的编译。", "conclusion": "该方法展示了可扩展性、模块化和保真度，填补了语义设计意图与可制造输出之间的空白；一致的AI表示为制造和工程原型中的自然语言实现提供了新设计选项，同时指导实际应用。"}}
{"id": "2512.16034", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16034", "abs": "https://arxiv.org/abs/2512.16034", "authors": ["Kieran Henderson", "Kian Omoomi", "Vasudha Varadarajan", "Allison Lahnala", "Charles Welch"], "title": "Examining the Utility of Self-disclosure Types for Modeling Annotators of Social Norms", "comment": null, "summary": "Recent work has explored the use of personal information in the form of persona sentences or self-disclosures to improve modeling of individual characteristics and prediction of annotator labels for subjective tasks. The volume of personal information has historically been restricted and thus little exploration has gone into understanding what kind of information is most informative for predicting annotator labels. In this work, we categorize self-disclosure sentences and use them to build annotator models for predicting judgments of social norms. We perform several ablations and analyses to examine the impact of the type of information on our ability to predict annotation patterns. We find that demographics are more impactful than attitudes, relationships, and experiences. Generally, theory-based approaches worked better than automatic clusters. Contrary to previous work, only a small number of related comments are needed. Lastly, having a more diverse sample of annotator self-disclosures leads to the best performance.", "AI": {"tldr": "该研究探索了不同类型个人信息对预测标注者标签的影响，发现人口统计信息比态度、关系和经历更有效，且少量相关评论即可达到良好效果。", "motivation": "以往研究使用个人信息提升主观任务标注预测，但信息类型受限，缺乏对何种信息最有效的深入探索。", "method": "对自我披露句子分类，构建标注者模型预测社会规范判断，进行消融实验分析信息类型对预测能力的影响。", "result": "人口统计信息最具影响力；理论驱动方法优于自动聚类；仅需少量相关评论；多样化的自我披露样本带来最佳性能。", "conclusion": "人口统计信息是预测标注者标签的关键，多样化的有限个人信息即可有效建模个体差异，挑战了以往需要大量数据的假设。"}}
{"id": "2512.16019", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16019", "abs": "https://arxiv.org/abs/2512.16019", "authors": ["Qiping Zhang", "Nathan Tsoi", "Mofeed Nagib", "Hao-Tien Lewis Chiang", "Marynel Vázquez"], "title": "Few-Shot Inference of Human Perceptions of Robot Performance in Social Navigation Scenarios", "comment": null, "summary": "Understanding how humans evaluate robot behavior during human-robot interactions is crucial for developing socially aware robots that behave according to human expectations. While the traditional approach to capturing these evaluations is to conduct a user study, recent work has proposed utilizing machine learning instead. However, existing data-driven methods require large amounts of labeled data, which limits their use in practice. To address this gap, we propose leveraging the few-shot learning capabilities of Large Language Models (LLMs) to improve how well a robot can predict a user's perception of its performance, and study this idea experimentally in social navigation tasks. To this end, we extend the SEAN TOGETHER dataset with additional real-world human-robot navigation episodes and participant feedback. Using this augmented dataset, we evaluate the ability of several LLMs to predict human perceptions of robot performance from a small number of in-context examples, based on observed spatio-temporal cues of the robot and surrounding human motion. Our results demonstrate that LLMs can match or exceed the performance of traditional supervised learning models while requiring an order of magnitude fewer labeled instances. We further show that prediction performance can improve with more in-context examples, confirming the scalability of our approach. Additionally, we investigate what kind of sensor-based information an LLM relies on to make these inferences by conducting an ablation study on the input features considered for performance prediction. Finally, we explore the novel application of personalized examples for in-context learning, i.e., drawn from the same user being evaluated, finding that they further enhance prediction accuracy. This work paves the path to improving robot behavior in a scalable manner through user-centered feedback.", "AI": {"tldr": "本研究提出利用大语言模型（LLMs）的少样本学习能力，基于少量上下文示例预测人类对机器人社交导航行为的评价，实验表明该方法在减少标注数据需求的同时能达到或超越传统监督学习模型的性能。", "motivation": "传统评估机器人行为的方法依赖用户研究，而现有数据驱动方法需要大量标注数据，限制了实际应用。为填补这一空白，研究探索如何利用LLMs的少样本学习能力，以更高效的方式预测人类对机器人交互行为的感知。", "method": "扩展SEAN TOGETHER数据集，增加真实人机导航场景与用户反馈；使用多个LLMs基于少量上下文示例（包含机器人及周围人的时空运动线索）进行预测；通过消融实验分析输入特征的影响，并探索个性化示例（来自同一用户）对预测性能的提升。", "result": "LLMs仅需十分之一的标注数据即可达到或超越传统监督学习模型的性能；增加上下文示例可进一步提升预测效果；个性化示例能进一步提高预测准确率；消融实验揭示了LLMs依赖的传感器信息类型。", "conclusion": "基于LLMs的少样本学习方法能有效预测人类对机器人行为的感知，显著降低数据标注需求，并通过个性化示例进一步提升性能，为通过用户反馈规模化改进机器人行为提供了可行路径。"}}
{"id": "2512.16333", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16333", "abs": "https://arxiv.org/abs/2512.16333", "authors": ["Rawan Hoteit", "Andrea Balestra", "Nathan Mingard", "Efe C. Balta", "John Lygeros"], "title": "Closed Loop Reference Optimization for Extrusion Additive Manufacturing", "comment": null, "summary": "Various defects occur during material extrusion additive manufacturing processes that degrade the quality of the 3D printed parts and lead to significant material waste. This motivates feedback control of the extrusion process to mitigate defects and prevent print failure. We propose a linear quadratic regulator (LQR) for closed-loop control with force feedback to provide accurate width tracking of the extruded filament. Furthermore, we propose preemptive optimization of the reference force given to the LQR that accounts for the performance of the LQR and generates the optimal reference for the closed loop extrusion dynamics and machine constraints. Simulation results demonstrate the improved tracking performance and response time. Experiments on a Fused Filament Fabrication 3D printer showcase a root mean square error improvement of 39.57% compared to tracking the unmodified reference as well as an 83.7% shorter settling time.", "AI": {"tldr": "提出一种结合线性二次调节器（LQR）和预参考优化的力反馈控制方法，用于材料挤出增材制造过程，以提高挤出丝宽跟踪精度并减少缺陷。", "motivation": "材料挤出增材制造过程中常出现缺陷，导致打印件质量下降和材料浪费，需通过反馈控制来缓解缺陷并防止打印失败。", "method": "采用线性二次调节器（LQR）进行闭环控制，结合力反馈实现挤出丝宽精确跟踪；并提出预参考优化方法，根据LQR性能和机器约束生成最优参考力。", "result": "仿真显示跟踪性能和响应时间改善；在FFF 3D打印机实验中，相比未优化参考，均方根误差降低39.57%，稳定时间缩短83.7%。", "conclusion": "所提出的LQR结合预参考优化的控制方法能有效提升挤出过程的跟踪精度和响应速度，减少打印缺陷和材料浪费。"}}
{"id": "2512.16024", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16024", "abs": "https://arxiv.org/abs/2512.16024", "authors": ["Rishabh Dev Yadav", "Shrey Agrawal", "Kamalakar Karlapalem"], "title": "Maintaining the Level of a Payload carried by Multi-Robot System on Irregular Surface", "comment": null, "summary": "In this paper, we introduce a multi robot payload transport system to carry payloads through an environment of unknown and uneven inclinations while maintaining the desired orientation of the payload. For this task, we used custom built robots with a linear actuator (pistons) mounted on top of each robot. The system continuously monitors the payload's orientation and computes the required piston height of each robot to maintain the desired orientation of the payload. In this work, we propose an open loop controller coupled with a closed loop PID controller to achieve the goal. As our modelling makes no assumptions on the type of terrain, the system can work on any unknown and uneven terrains and inclinations. We showcase the efficacy of our proposed controller by testing it on various simulated environments with varied and complex terrains.", "AI": {"tldr": "本文提出了一种多机器人有效载荷运输系统，能够在未知和不平坦的倾斜地形中运输有效载荷，同时保持其期望方向。系统通过线性执行器调节机器人高度，结合开环和闭环PID控制器实现目标。", "motivation": "现有系统难以在未知和不平坦的倾斜地形中运输有效载荷时保持其方向稳定，需要一种能够适应各种复杂地形的鲁棒控制方案。", "method": "使用配备线性执行器（活塞）的定制机器人，通过开环控制器与闭环PID控制器相结合的方法，持续监测有效载荷方向并计算各机器人所需的活塞高度。", "result": "系统在多种模拟环境中进行了测试，包括变化和复杂的地形，展示了所提控制器在不同未知和不平坦倾斜地形中的有效性。", "conclusion": "所提出的控制方法不依赖地形类型假设，能够在任何未知和不平坦的倾斜地形中工作，为多机器人有效载荷运输提供了鲁棒的解决方案。"}}
{"id": "2512.15776", "categories": ["cs.AI", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.15776", "abs": "https://arxiv.org/abs/2512.15776", "authors": ["Shaun Baek", "Sam Liu", "Joseph Ukpong"], "title": "Emergence: Overcoming Privileged Information Bias in Asymmetric Embodied Agents via Active Querying", "comment": "12 pages, 9 pages of content, 6 tables, 5 figures", "summary": "Large Language Models (LLMs) act as powerful reasoning engines but struggle with \"symbol grounding\" in embodied environments, particularly when information is asymmetrically distributed. We investigate the Privileged Information Bias (or \"Curse of Knowledge\"), where a knowledgeable \"Leader\" agent fails to guide a sensor-limited \"Follower\" due to a lack of Theory of Mind. To quantify this phenomenon, we propose a novel Asymmetric Assistive Reasoning framework within AI2-THOR. Our experiments reveal a significant \"Success Gap\": while the Leader successfully perceives the target in 35.0% of episodes, the collaborative team succeeds only 17.0% of the time, implying that nearly 50% of feasible plans fail solely due to communicative grounding errors. We demonstrate that a \"Pull-based\" protocol (active querying) is significantly more robust than standard \"Push-based\" instruction, with successful episodes featuring 2x the frequency of clarification requests. This research isolates the mechanism of active uncertainty reduction as a prerequisite for safe human-AI and robot-robot collaboration.", "AI": {"tldr": "该研究提出了一种非对称辅助推理框架，揭示了大型语言模型在具身环境中因信息不对称导致的特权信息偏见问题，并证明主动查询协议能显著改善协作成功率。", "motivation": "大型语言模型作为推理引擎在具身环境中存在符号接地问题，特别是当信息不对称分布时，知识丰富的领导者智能体因缺乏心智理论而难以有效指导传感器受限的跟随者，这阻碍了人机协作与机器人协作的安全性。", "method": "在AI2-THOR环境中构建非对称辅助推理框架，设计领导者（感知全面）与跟随者（传感器受限）的协作任务，对比分析推送式指令与拉取式（主动查询）协议的效果，通过成功率、澄清请求频率等指标量化协作效率。", "result": "实验发现显著的成功率差距：领导者单独感知目标的成功率为35.0%，而团队协作成功率仅为17.0%，表明近50%的可行计划因沟通接地错误而失败；拉取式协议比推送式指令更鲁棒，成功任务中的澄清请求频率高出2倍。", "conclusion": "主动不确定性减少机制是安全人机协作与机器人协作的前提条件，拉取式通信协议能有效缓解特权信息偏见问题，为具身智能体的符号接地与协作设计提供了关键见解。"}}
{"id": "2512.15947", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15947", "abs": "https://arxiv.org/abs/2512.15947", "authors": ["Jin Young Kim", "Jeremy Hudson", "Jeongchul Kim", "Qing Lyu", "Christopher T. Whitlow"], "title": "MCR-VQGAN: A Scalable and Cost-Effective Tau PET Synthesis Approach for Alzheimer's Disease Imaging", "comment": "12 pages, 5 figures. A preliminary version of this work was presented at RSNA 2025", "summary": "Tau positron emission tomography (PET) is a critical diagnostic modality for Alzheimer's disease (AD) because it visualizes and quantifies neurofibrillary tangles, a hallmark of AD pathology. However, its widespread clinical adoption is hindered by significant challenges, such as radiation exposure, limited availability, high clinical workload, and substantial financial costs. To overcome these limitations, we propose Multi-scale CBAM Residual Vector Quantized Generative Adversarial Network (MCR-VQGAN) to synthesize high-fidelity tau PET images from structural T1-weighted MRI scans. MCR-VQGAN improves standard VQGAN by integrating three key architectural enhancements: multi-scale convolutions, ResNet blocks, and Convolutional Block Attention Modules (CBAM). Using 222 paired structural T1-weighted MRI and tau PET scans from Alzheimer's Disease Neuroimaging Initiative (ADNI), we trained and compared MCR-VQGAN with cGAN, WGAN-GP, CycleGAN, and VQGAN. Our proposed model achieved superior image synthesis performance across all metrics: MSE of 0.0056 +/- 0.0061, PSNR of 24.39 +/- 4.49 dB, and SSIM of 0.9000 +/- 0.0453. To assess the clinical utility of the synthetic images, we trained and evaluated a CNN-based AD classifier. The classifier achieved comparable accuracy when tested on real (63.64%) and synthetic (65.91%) images. This result indicates that our synthesis process successfully preserves diagnostically relevant features without significant information loss. Our results demonstrate that MCR-VQGAN can offer a reliable and scalable surrogate for conventional tau PET imaging, potentially improving the accessibility and scalability of tau imaging biomarkers for AD research and clinical workflows.", "AI": {"tldr": "提出MCR-VQGAN模型，从结构T1加权MRI合成高质量tau PET图像，以解决tau PET临床应用的局限性，并在AD分类任务中验证其临床实用性。", "motivation": "tau PET成像对阿尔茨海默病诊断至关重要，但存在辐射暴露、可用性有限、临床工作量大和成本高等问题，限制了其广泛应用。", "method": "提出MCR-VQGAN模型，在VQGAN基础上整合多尺度卷积、ResNet块和CBAM注意力模块，使用ADNI的222对T1-MRI和tau PET数据训练，并与cGAN、WGAN-GP、CycleGAN和VQGAN对比。", "result": "MCR-VQGAN在图像合成指标上表现最优（MSE: 0.0056，PSNR: 24.39 dB，SSIM: 0.9000），基于合成图像训练的AD分类器准确率（65.91%）与真实图像（63.64%）相当。", "conclusion": "MCR-VQGAN能生成保留诊断特征的高保真tau PET图像，可作为传统tau PET的可靠替代方案，有望提升tau成像在AD研究和临床中的可及性与可扩展性。"}}
{"id": "2512.16027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16027", "abs": "https://arxiv.org/abs/2512.16027", "authors": ["Shuaidong Ji", "Mahdi Bamdad", "Francisco Cruz"], "title": "SWIFT-Nav: Stability-Aware Waypoint-Level TD3 with Fuzzy Arbitration for UAV Navigation in Cluttered Environments", "comment": "10 pages, Accepted at Australasian Conference on Robotics and Automation (ACRA) 2025", "summary": "Efficient and reliable UAV navigation in cluttered and dynamic environments remains challenging. We propose SWIFT-Nav: Stability-aware Waypoint-level Integration of Fuzzy arbitration and TD3 for Navigation, a TD3-based navigation framework that achieves fast, stable convergence to obstacle-aware paths. The system couples a sensor-driven perception front end with a TD3 waypoint policy: the perception module converts LiDAR ranges into a confidence-weighted safety map and goal cues, while the TD3 policy is trained with Prioritised Experience Replay to focus on high-error transitions and a decaying epsilon-greedy exploration schedule that gradually shifts from exploration to exploitation. A lightweight fuzzy-logic layer computes a safety score from radial measurements and near obstacles, gates mode switching and clamps unsafe actions; in parallel, task-aligned reward shaping combining goal progress, clearance, and switch-economy terms provides dense, well-scaled feedback that accelerates learning. Implemented in Webots with proximity-based collision checking, our approach consistently outperforms baselines in trajectory smoothness and generalization to unseen layouts, while preserving real-time responsiveness. These results show that combining TD3 with replay prioritisation, calibrated exploration, and fuzzy-safety rules yields a robust and deployable solution for UAV navigation in cluttered scenes.", "AI": {"tldr": "提出SWIFT-Nav导航框架，结合TD3算法与模糊逻辑，实现无人机在复杂动态环境中的高效稳定导航。", "motivation": "无人机在杂乱动态环境中的导航仍面临挑战，需要兼顾效率、稳定性和实时性。", "method": "采用TD3算法作为路径点策略，结合优先级经验回放和衰减探索策略；前端感知模块将LiDAR数据转换为安全地图；模糊逻辑层计算安全分数并控制模式切换；奖励函数融合目标进度、避障和切换效率。", "result": "在Webots仿真中表现优于基线方法，轨迹更平滑，对未知布局泛化能力更强，同时保持实时响应。", "conclusion": "TD3与优先级回放、校准探索和模糊安全规则结合，可为无人机在杂乱场景中提供鲁棒且可部署的导航解决方案。"}}
{"id": "2512.16041", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16041", "abs": "https://arxiv.org/abs/2512.16041", "authors": ["Yuanning Feng", "Sinan Wang", "Zhengxiang Cheng", "Yao Wan", "Dongping Chen"], "title": "Are We on the Right Way to Assessing LLM-as-a-Judge?", "comment": null, "summary": "LLM-as-a-Judge has been widely adopted as an evaluation method and served as supervised rewards in model training. However, existing benchmarks for LLM-as-a-Judge are mainly relying on human-annotated ground truth, which introduces human bias that undermines the assessment of reliability and imposes scalability constraints. To overcome these limitations, we introduce Sage, a novel evaluation suite that assesses the quality of LLM judges without necessitating any human annotation. Inspired by axioms of rational choice theory, Sage introduces two new lenses for measuring LLM-as-a-Judge: local self-consistency (pair-wise preference stability) and global logical consistency (transitivity across a full set of preferences). We curate a dataset of 650 questions by combining structured benchmark problems with real-world user queries. Our experiments demonstrate both the stability of our metrics and their high correlation with supervised benchmarks like LLMBar and RewardBench2, confirming Sage's reliability as an evaluation suite for the robustness and accuracy of LLM-as-a-Judge. Based on Sage, we reveal that current state-of-the-art LLMs exhibit significant reliability problems when acting as judges in both scoring and pairwise settings; even the top-performing models, Gemini-2.5-Pro and GPT-5, fail to maintain consistent preferences in nearly a quarter of difficult cases. We attribute this to a new phenomenon called situational preference, which explains why explicit rubrics or criteria can help the model judge consistently across answer pairs. Our further analysis shows that finetuned LLM-as-a-Judge is a feasible method to boost performance, and the panel-based judge as well as deep reasoning can enhance the judging consistency. We also find substantial inconsistency in human judgments, which indicates that human annotation may not be a reliable gold standard.", "AI": {"tldr": "提出Sage评估套件，无需人工标注即可评估LLM作为评判者的质量，发现当前先进LLM在评判任务中存在显著可靠性问题。", "motivation": "现有LLM-as-a-Judge评估方法依赖人工标注，存在人类偏见且难以扩展，需要无需人工标注的评估方案。", "method": "基于理性选择理论，提出局部自一致性（成对偏好稳定性）和全局逻辑一致性（偏好传递性）两个评估维度，构建包含650个问题的数据集，结合结构化基准和真实用户查询。", "result": "Sage指标稳定且与监督基准高度相关；发现GPT-5、Gemini-2.5-Pro等先进LLM在近1/4困难案例中偏好不一致；提出情境偏好现象；微调、小组评审和深度推理可提升一致性；人类标注也存在显著不一致性。", "conclusion": "Sage是有效的无监督评估工具；当前LLM作为评判者存在可靠性问题；人类标注并非可靠黄金标准；微调和推理增强可改善LLM评判性能。"}}
{"id": "2512.16065", "categories": ["eess.IV"], "pdf": "https://arxiv.org/pdf/2512.16065", "abs": "https://arxiv.org/abs/2512.16065", "authors": ["Sean Breckling", "Matthew Swan", "Keith D. Tan", "Derek Wingard", "Brandon Baldonado", "Yoohwan Kim", "Ju-Yeon Jo", "Evan Scott", "Jordan Pillow"], "title": "Single-View Tomographic Reconstruction Using Learned Primal Dual", "comment": "9 Pages, 11 Figures", "summary": "The Learned Primal Dual (LPD) method has shown promising results in various tomographic reconstruction modalities, particularly under challenging acquisition restrictions such as limited viewing angles or a limited number of views. We investigate the performance of LPD in a more extreme case: single-view tomographic reconstructions of axially-symmetric targets. This study considers two modalities: the first assumes low-divergence or parallel X-rays. The second models a cone-beam X-ray imaging testbed. For both modalities, training data is generated using closed-form integral transforms, or physics-based ray-tracing software, then corrupted with blur and noise. Our results are then compared against common numerical inversion methodologies.", "AI": {"tldr": "研究了学习型原始对偶方法在单视角轴对称目标断层扫描重建中的性能，比较了两种X射线成像模式，并与传统数值反演方法进行了对比。", "motivation": "学习型原始对偶方法在有限视角或有限视图的断层扫描重建中表现良好，但尚未在更极端的单视角轴对称目标重建场景中进行系统评估。", "method": "采用两种X射线成像模式：低发散/平行X射线和锥束X射线成像测试台。通过闭式积分变换或基于物理的光线追踪软件生成训练数据，并添加模糊和噪声。将结果与常见数值反演方法进行比较。", "result": "学习型原始对偶方法在单视角轴对称目标断层扫描重建中表现出优于传统数值反演方法的性能。", "conclusion": "学习型原始对偶方法在极端单视角断层扫描重建场景中具有应用潜力，尤其在处理轴对称目标时能有效应对模糊和噪声干扰。"}}
{"id": "2512.15949", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15949", "abs": "https://arxiv.org/abs/2512.15949", "authors": ["Tejas Anvekar", "Fenil Bardoliya", "Pavan K. Turaga", "Chitta Baral", "Vivek Gupta"], "title": "The Perceptual Observatory Characterizing Robustness and Grounding in MLLMs", "comment": "Accepted at WACV 2026", "summary": "Recent advances in multimodal large language models (MLLMs) have yielded increasingly powerful models, yet their perceptual capacities remain poorly characterized. In practice, most model families scale language component while reusing nearly identical vision encoders (e.g., Qwen2.5-VL 3B/7B/72B), which raises pivotal concerns about whether progress reflects genuine visual grounding or reliance on internet-scale textual world knowledge. Existing evaluation methods emphasize end-task accuracy, overlooking robustness, attribution fidelity, and reasoning under controlled perturbations. We present The Perceptual Observatory, a framework that characterizes MLLMs across verticals like: (i) simple vision tasks, such as face matching and text-in-vision comprehension capabilities; (ii) local-to-global understanding, encompassing image matching, grid pointing game, and attribute localization, which tests general visual grounding. Each vertical is instantiated with ground-truth datasets of faces and words, systematically perturbed through pixel-based augmentations and diffusion-based stylized illusions. The Perceptual Observatory moves beyond leaderboard accuracy to yield insights into how MLLMs preserve perceptual grounding and relational structure under perturbations, providing a principled foundation for analyzing strengths and weaknesses of current and future models.", "AI": {"tldr": "提出'感知观测台'框架，通过系统扰动评估多模态大语言模型的视觉感知能力，超越传统准确率指标，分析模型在扰动下的视觉基础保持能力。", "motivation": "当前MLLMs研究多通过扩大语言组件提升性能，但视觉编码器基本不变，难以区分模型进步是源于真正的视觉理解还是依赖文本知识。现有评估方法过于关注任务准确率，忽视鲁棒性、归因保真度和受控扰动下的推理能力。", "method": "构建包含两个维度的评估框架：(1)简单视觉任务（人脸匹配、视觉文本理解）；(2)局部到全局理解（图像匹配、网格指向游戏、属性定位）。使用人脸和文字的真实数据集，通过像素级增强和基于扩散的风格化错觉进行系统扰动。", "result": "该框架能够超越排行榜准确率指标，揭示MLLMs在扰动下如何保持感知基础和关系结构，为分析当前和未来模型的优缺点提供原则性基础。", "conclusion": "'感知观测台'为系统评估MLLMs的感知能力提供了新方法，有助于区分模型是真正理解视觉内容还是依赖文本知识，推动更可靠的视觉基础模型发展。"}}
{"id": "2512.16338", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16338", "abs": "https://arxiv.org/abs/2512.16338", "authors": ["Edwin Baum", "Zonglin Liu", "Yuzhen Qin", "Olaf Stursberg"], "title": "Using Seminorms To Analyze Contraction of Switched Systems With Only Non-Contracting Modes", "comment": null, "summary": "This paper investigates contraction properties of switched dynamical systems for the case that all modes are non-contracting, thereby extending existing results that require at least one mode to be contracting. Leveraging the property that unstable systems may still exhibit stable behavior within certain subspaces, conditions are provided which ensure contracting evolution within a given subspace of the state space of the switched system. These conditions are derived using the concepts of seminorms and semi-contracting systems. Then, by selecting a set of subspaces whose corresponding seminorms form a separating family of the state space, and by verifying whether a given mode is contracting in each subspace, conditions on the activation time of each mode are provided by which contraction on the complete state space is guaranteed. Numerical examples are presented for illustration.", "AI": {"tldr": "本文研究了切换动力系统的收缩特性，针对所有模式都不收缩的情况，通过子空间收缩和半范数概念，提出了保证系统在状态空间子空间中收缩的条件，并给出了切换模式激活时间的约束条件。", "motivation": "现有切换系统收缩理论通常要求至少一个模式是收缩的，这限制了理论的应用范围。本文旨在研究所有模式都不收缩时，切换系统仍可能通过子空间收缩实现整体稳定性的情况。", "method": "采用半范数和半收缩系统的概念，分析系统在特定子空间中的收缩特性；通过选择一组子空间（其对应半范数构成状态空间的分离族），验证每个模式在各子空间中的收缩性，并推导模式激活时间的约束条件。", "result": "提出了保证切换系统在子空间中收缩的条件；通过适当选择子空间集合和切换策略，即使所有模式都不收缩，也能保证整个状态空间的收缩性；数值算例验证了理论的有效性。", "conclusion": "即使所有切换模式都不收缩，通过合理设计切换策略和利用子空间收缩特性，切换系统仍可实现整体收缩稳定性，这扩展了现有切换系统收缩理论的应用范围。"}}
{"id": "2512.15783", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15783", "abs": "https://arxiv.org/abs/2512.15783", "authors": ["Kit Tempest-Walters"], "title": "AI Epidemiology: achieving explainable AI through expert oversight patterns", "comment": "41 pages, 1 figure, 7 tables", "summary": "AI Epidemiology is a framework for governing and explaining advanced AI systems by applying population-level surveillance methods to AI outputs. The approach mirrors the way in which epidemiologists enable public health interventions through statistical evidence before molecular mechanisms are understood. This bypasses the problem of model complexity which plagues current interpretability methods (such as SHAP and mechanistic interpretability) at the scale of deployed models.\n  AI Epidemiology achieves this population-level surveillance by standardising capture of AI-expert interactions into structured assessment fields: risk level, alignment score, and accuracy score. These function as exposure variables which predict output failure through statistical associations, much like cholesterol and blood pressure act as exposure variables predicting cardiac events. Output-failure associations are subsequently validated against expert overrides and real-world outcomes.\n  The framework places zero burden on experts and provides automatic audit trails by passively tracking expert convergence and divergence with AI recommendations. Since it analyses outputs rather than internal model computations, it also provides governance continuity when institutions update models and switch vendors. Finally, by providing reliability scores and semantic assessments (e.g. 'this recommendation resembles 500 cases overridden by experts due to guideline violations'), it enables experts and institutions to detect unreliable AI outputs before they cause harm. This democratises AI oversight by enabling domain experts to govern AI systems without requiring machine learning expertise.", "AI": {"tldr": "AI流行病学是一个通过将群体层面的监测方法应用于AI输出来治理和解释高级AI系统的框架，它绕过模型复杂性，通过标准化评估字段预测输出失败，实现零专家负担的AI治理。", "motivation": "当前的可解释性方法（如SHAP和机制可解释性）在部署模型规模上受限于模型复杂性，难以有效治理AI系统。该研究旨在通过模仿流行病学的群体监测方法，解决这一治理难题。", "method": "框架将AI-专家交互标准化为结构化评估字段（风险等级、对齐分数、准确度分数），作为预测输出失败的暴露变量；通过统计关联验证输出失败与专家覆盖及真实结果的关联；被动追踪专家与AI建议的趋同与分歧，提供自动审计追踪。", "result": "该框架实现了对AI输出的群体层面监测，无需专家额外负担，提供治理连续性（即使机构更新模型或更换供应商），并通过可靠性分数和语义评估帮助专家和机构在AI输出造成危害前检测不可靠输出。", "conclusion": "AI流行病学通过将流行病学方法应用于AI治理，使领域专家无需机器学习专业知识即可监督AI系统，民主化了AI监督，为复杂AI系统的可靠部署提供了可行路径。"}}
{"id": "2512.16125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16125", "abs": "https://arxiv.org/abs/2512.16125", "authors": ["Daniela N. Rim", "Heeyoul Choi"], "title": "Convolutional Lie Operator for Sentence Classification", "comment": "Proceedings of the 2024 8th International Conference on Natural Language Processing and Information Retrieval", "summary": "Traditional Convolutional Neural Networks have been successful in capturing local, position-invariant features in text, but their capacity to model complex transformation within language can be further explored. In this work, we explore a novel approach by integrating Lie Convolutions into Convolutional-based sentence classifiers, inspired by the ability of Lie group operations to capture complex, non-Euclidean symmetries. Our proposed models SCLie and DPCLie empirically outperform traditional Convolutional-based sentence classifiers, suggesting that Lie-based models relatively improve the accuracy by capturing transformations not commonly associated with language. Our findings motivate more exploration of new paradigms in language modeling.", "AI": {"tldr": "提出将李卷积集成到基于卷积的句子分类器中，以捕捉语言中的复杂非欧几里得对称性，实验表明该方法优于传统卷积模型。", "motivation": "传统卷积神经网络在捕捉文本局部特征方面表现良好，但在建模语言内部复杂变换方面仍有提升空间，受李群运算能捕捉复杂非欧几里得对称性的启发，探索新的语言建模范式。", "method": "将李卷积集成到基于卷积的句子分类器中，提出了SCLie和DPCLie两种模型，通过李群运算捕捉语言中的复杂变换。", "result": "SCLie和DPCLie模型在实验中优于传统基于卷积的句子分类器，通过捕捉语言中不常见的变换相对提高了准确率。", "conclusion": "李卷积模型为语言建模提供了新的有效范式，研究结果鼓励进一步探索语言建模的新方法。"}}
{"id": "2512.15957", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15957", "abs": "https://arxiv.org/abs/2512.15957", "authors": ["Utsav Panchal", "Yuchen Liu", "Luigi Palmieri", "Ilche Georgievski", "Marco Aiello"], "title": "Seeing is Believing (and Predicting): Context-Aware Multi-Human Behavior Prediction with Vision Language Models", "comment": "Accepted at IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026", "summary": "Accurately predicting human behaviors is crucial for mobile robots operating in human-populated environments. While prior research primarily focuses on predicting actions in single-human scenarios from an egocentric view, several robotic applications require understanding multiple human behaviors from a third-person perspective. To this end, we present CAMP-VLM (Context-Aware Multi-human behavior Prediction): a Vision Language Model (VLM)-based framework that incorporates contextual features from visual input and spatial awareness from scene graphs to enhance prediction of humans-scene interactions. Due to the lack of suitable datasets for multi-human behavior prediction from an observer view, we perform fine-tuning of CAMP-VLM with synthetic human behavior data generated by a photorealistic simulator, and evaluate the resulting models on both synthetic and real-world sequences to assess their generalization capabilities. Leveraging Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), CAMP-VLM outperforms the best-performing baseline by up to 66.9% in prediction accuracy.", "AI": {"tldr": "提出了CAMP-VLM框架，利用视觉语言模型和场景图增强多人在第三人称视角下的行为预测，通过合成数据微调并在真实场景中验证，准确率提升显著。", "motivation": "现有研究多关注单人在第一人称视角下的行为预测，但许多机器人应用需要从第三人称视角理解多人行为，缺乏相关数据集和方法。", "method": "开发CAMP-VLM框架，结合视觉输入的上下文特征和场景图的空间感知；使用光真实感模拟器生成合成数据进行监督微调（SFT）和直接偏好优化（DPO）。", "result": "在合成和真实世界序列上评估，CAMP-VLM比最佳基线模型预测准确率提升高达66.9%。", "conclusion": "CAMP-VLM能有效预测多人场景中的行为，通过合成数据微调和上下文整合提升了泛化能力，为机器人应用提供了实用解决方案。"}}
{"id": "2512.16345", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16345", "abs": "https://arxiv.org/abs/2512.16345", "authors": ["Zonglin Liu", "Kyra Borchhardt", "Olaf Stursberg"], "title": "Contraction Analysis of Filippov Solutions in Multi-Modal Piecewise Smooth Systems", "comment": null, "summary": "This paper provides conditions to ensure contractive behavior of Filippov solutions generated by multi-modal piecewise smooth (PWS) systems. These conditions are instrumental in analyzing the asymptotic behavior of PWS systems, such as convergence towards an equilibrium point or a limit cycle. The work is motivated by a known principle for contraction analysis of bimodal PWS systems which ensures that the flow dynamics of each mode and the sliding dynamics on the switching manifold are contracting. This approach is extended first to PWS systems with multiple non-intersecting switching manifolds in Rn, and then to two intersecting switching manifolds in R2. Numerical examples are provided to validate the theoretical findings, along with a discussion on extensions to more general intersecting switching manifolds in Rn.", "AI": {"tldr": "本文为多模态分段光滑系统的Filippov解提供了收缩性条件，扩展了现有双模态系统的收缩分析原理，适用于非相交和相交切换流形的情况。", "motivation": "受双模态分段光滑系统收缩分析原理的启发，该原理要求每个模态的流动态和切换流形上的滑移动态都具有收缩性，本文旨在将此原理推广到更一般的多模态系统。", "method": "首先将收缩分析扩展到Rn空间中具有多个非相交切换流形的系统，然后专门处理R2空间中两个相交切换流形的情况，并通过数值算例验证理论结果，最后讨论了向Rn空间中更一般相交切换流形的扩展。", "result": "建立了确保多模态分段光滑系统Filippov解具有收缩行为的条件，这些条件可用于分析系统向平衡点或极限环的渐近收敛性，数值算例证实了理论的有效性。", "conclusion": "所提出的收缩性条件为分析多模态分段光滑系统的渐近行为提供了有效工具，特别是在处理非相交和相交切换流形时，为更一般的相交切换流形系统分析奠定了基础。"}}
{"id": "2512.15784", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.15784", "abs": "https://arxiv.org/abs/2512.15784", "authors": ["Zibin Liu", "Cheng Zhang", "Xi Zhao", "Yunfei Feng", "Bingyu Bai", "Dahu Feng", "Erhu Feng", "Yubin Xia", "Haibo Chen"], "title": "Beyond Training: Enabling Self-Evolution of Agents with MOBIMEM", "comment": null, "summary": "Large Language Model (LLM) agents are increasingly deployed to automate complex workflows in mobile and desktop environments. However, current model-centric agent architectures struggle to self-evolve post-deployment: improving personalization, capability, and efficiency typically requires continuous model retraining/fine-tuning, which incurs prohibitive computational overheads and suffers from an inherent trade-off between model accuracy and inference efficiency.\n  To enable iterative self-evolution without model retraining, we propose MOBIMEM, a memory-centric agent system. MOBIMEM first introduces three specialized memory primitives to decouple agent evolution from model weights: (1) Profile Memory uses a lightweight distance-graph (DisGraph) structure to align with user preferences, resolving the accuracy-latency trade-off in user profile retrieval; (2) Experience Memory employs multi-level templates to instantiate execution logic for new tasks, ensuring capability generalization; and (3) Action Memory records fine-grained interaction sequences, reducing the reliance on expensive model inference. Building upon this memory architecture, MOBIMEM further integrates a suite of OS-inspired services to orchestrate execution: a scheduler that coordinates parallel sub-task execution and memory operations; an agent record-and-replay (AgentRR) mechanism that enables safe and efficient action reuse; and a context-aware exception handling that ensures graceful recovery from user interruptions and runtime errors.\n  Evaluation on AndroidWorld and top-50 apps shows that MOBIMEM achieves 83.1% profile alignment with 23.83 ms retrieval time (280x faster than GraphRAG baselines), improves task success rates by up to 50.3%, and reduces end-to-end latency by up to 9x on mobile devices.", "AI": {"tldr": "提出MOBIMEM——一种基于记忆中心的智能体系统，通过三种专用记忆原语和操作系统风格的服务，实现无需模型重训练的自进化，显著提升移动设备上LLM智能体的个性化、能力和效率。", "motivation": "当前以模型为中心的智能体架构在部署后难以自我进化，改进个性化、能力和效率通常需要持续模型重训练/微调，计算开销巨大，且模型精度与推理效率存在固有权衡。", "method": "提出MOBIMEM系统：1) 引入三种记忆原语：Profile Memory（使用轻量距离图结构对齐用户偏好）、Experience Memory（使用多级模板实例化新任务执行逻辑）、Action Memory（记录细粒度交互序列）；2) 集成操作系统风格服务：调度器、AgentRR机制、上下文感知异常处理。", "result": "在AndroidWorld和top-50应用评估中：Profile Memory实现83.1%偏好对齐，检索时间23.83毫秒（比GraphRAG基线快280倍）；任务成功率提升高达50.3%；端到端延迟在移动设备上降低高达9倍。", "conclusion": "MOBIMEM通过记忆中心架构有效解耦智能体进化与模型权重，无需模型重训练即可实现迭代自进化，显著提升了LLM智能体在移动环境中的性能、效率和适应性。"}}
{"id": "2512.16219", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.16219", "abs": "https://arxiv.org/abs/2512.16219", "authors": ["Zhihao Zhang", "Xuejun Yang", "Weihua Liu", "Mouquan Shen"], "title": "Learning High-Quality Initial Noise for Single-View Synthesis with Diffusion Models", "comment": "16 pages, 9 figures", "summary": "Single-view novel view synthesis (NVS) models based on diffusion models have recently attracted increasing attention, as they can generate a series of novel view images from a single image prompt and camera pose information as conditions. It has been observed that in diffusion models, certain high-quality initial noise patterns lead to better generation results than others. However, there remains a lack of dedicated learning frameworks that enable NVS models to learn such high-quality noise. To obtain high-quality initial noise from random Gaussian noise, we make the following contributions. First, we design a discretized Euler inversion method to inject image semantic information into random noise, thereby constructing paired datasets of random and high-quality noise. Second, we propose a learning framework based on an encoder-decoder network (EDN) that directly transforms random noise into high-quality noise. Experiments demonstrate that the proposed EDN can be seamlessly plugged into various NVS models, such as SV3D and MV-Adapter, achieving significant performance improvements across multiple datasets. Code is available at: https://github.com/zhihao0512/EDN.", "AI": {"tldr": "提出一种基于编码器-解码器网络的学习框架，将随机高斯噪声转换为高质量初始噪声，提升单视图新视角合成模型的生成质量", "motivation": "现有基于扩散模型的单视图新视角合成方法缺乏专门学习高质量初始噪声的框架，而高质量初始噪声对生成结果有显著影响", "method": "1. 设计离散化欧拉反演方法，将图像语义信息注入随机噪声，构建随机噪声与高质量噪声的配对数据集；2. 提出基于编码器-解码器网络的学习框架，直接转换随机噪声为高质量噪声", "result": "所提方法可无缝集成到SV3D、MV-Adapter等多种新视角合成模型中，在多个数据集上实现显著性能提升", "conclusion": "通过专门学习高质量初始噪声的框架，有效提升了单视图新视角合成模型的生成质量，该方法具有通用性和可扩展性"}}
{"id": "2512.16356", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16356", "abs": "https://arxiv.org/abs/2512.16356", "authors": ["Manuel R. Arahal", "Manuel G. Satué", "Manuel G. Ortega"], "title": "Optimal chiller loading including transients", "comment": null, "summary": "Scheduling and loading of chillers in a multi-chiller plant is considered. A new framework is introduced considering an extended set of independent variables for the optimization problem of energy consumption. In this way the number of decision variables is increased, providing extra degrees of freedom to optimize cooling plant operation. The dynamic effects due to transients arising from switching on and off of units are usually not considered in the literature dealing with Optimal Chiller Loading/Sequencing which is restricted to the static case. In this paper, these effects are treated in a way that results in a manageable optimization problem. A Simultaneous Perturbation Stochastic Approximation solution is deployed for the problem and the proposed method is compared with a similar but static approach showing the benefits in terms of reduced energy consumption.", "AI": {"tldr": "提出一种考虑动态效应的多冷水机组优化调度新框架，通过增加决策变量和改进优化方法降低能耗。", "motivation": "现有冷水机组优化调度研究多局限于静态工况，未充分考虑机组启停带来的动态瞬态效应，限制了节能潜力。", "method": "引入扩展独立变量集增加优化自由度；采用同时扰动随机逼近算法处理动态效应；与静态优化方法进行对比验证。", "result": "所提动态优化方法相比静态方法能有效降低多冷水机组系统的整体能耗。", "conclusion": "考虑动态效应的优化框架能更真实反映冷水机组运行特性，为实际工程中的节能调度提供了可行方案。"}}
{"id": "2512.15824", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15824", "abs": "https://arxiv.org/abs/2512.15824", "authors": ["Richard Fox", "Rui Li", "Gustav Jonsson", "Farzaneh Goli", "Miying Yang", "Emel Aktas", "Yongjing Wang"], "title": "State-Augmented Graphs for Circular Economy Triage", "comment": null, "summary": "Circular economy (CE) triage is the assessment of products to determine which sustainable pathway they can follow once they reach the end of their usefulness as they are currently being used. Effective CE triage requires adaptive decisions that balance retained value against the costs and constraints of processing and labour. This paper presents a novel decision-making framework as a simple deterministic solver over a state-augmented Disassembly Sequencing Planning (DSP) graph. By encoding the disassembly history into the state, our framework enforces the Markov property, enabling optimal, recursive evaluation by ensuring each decision only depends on the previous state. The triage decision involves choices between continuing disassembly or committing to a CE option. The model integrates condition-aware utility based on diagnostic health scores and complex operational constraints. We demonstrate the framework's flexibility with a worked example: the hierarchical triage of electric vehicle (EV) batteries, where decisions are driven by the recursive valuation of components. The example illustrates how a unified formalism enables the accommodation of varying mechanical complexity, safety requirements, and economic drivers. This unified formalism therefore provides a tractable and generalisable foundation for optimising CE triage decisions across diverse products and operational contexts.", "AI": {"tldr": "本文提出了一种基于状态增强拆解序列规划图的新型决策框架，用于优化循环经济产品分拣决策，并以电动汽车电池为例验证了其灵活性。", "motivation": "循环经济分拣需要在产品达到使用寿命时，平衡保留价值与处理成本、劳动力约束，做出适应性决策。现有方法缺乏统一、可扩展的决策框架来处理复杂的产品结构和操作约束。", "method": "开发了一种确定性求解器，基于状态增强的拆解序列规划图，通过将拆解历史编码到状态中强制满足马尔可夫性质，实现递归优化评估。模型整合了基于诊断健康分数的条件感知效用和复杂操作约束。", "result": "以电动汽车电池分层分拣为例，展示了该框架如何通过组件递归估值驱动决策，适应不同的机械复杂性、安全要求和经济驱动因素，证明了其灵活性和通用性。", "conclusion": "该统一形式化为优化不同产品和操作场景下的循环经济分拣决策提供了可处理且可推广的基础，能够有效支持可持续路径选择。"}}
{"id": "2512.16076", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16076", "abs": "https://arxiv.org/abs/2512.16076", "authors": ["Jia Hu", "Junqi Li", "Xuerun Yan", "Jintao Lai", "Lianhua An"], "title": "A simulation platform calibration method for automated vehicle evaluation: accurate on both vehicle level and traffic flow level", "comment": null, "summary": "Simulation testing is a fundamental approach for evaluating automated vehicles (AVs). To ensure its reliability, it is crucial to accurately replicate interactions between AVs and background traffic, which necessitates effective calibration. However, existing calibration methods often fall short in achieving this goal. To address this gap, this study introduces a simulation platform calibration method that ensures high accuracy at both the vehicle and traffic flow levels. The method offers several key features:(1) with the capability of calibration for vehicle-to-vehicle interaction; (2) with accuracy assurance; (3) with enhanced efficiency; (4) with pipeline calibration capability. The proposed method is benchmarked against a baseline with no calibration and a state-of-the-art calibration method. Results show that it enhances the accuracy of interaction replication by 83.53% and boosts calibration efficiency by 76.75%. Furthermore, it maintains accuracy across both vehicle-level and traffic flow-level metrics, with an improvement of 51.9%. Notably, the entire calibration process is fully automated, requiring no human intervention.", "AI": {"tldr": "本文提出了一种用于自动驾驶车辆仿真平台的高精度校准方法，能够在车辆和交通流两个层面准确复现交互行为，并显著提升校准效率和自动化程度。", "motivation": "现有仿真测试中的校准方法难以准确复现自动驾驶车辆与背景交通之间的交互，影响了仿真测试的可靠性，因此需要开发更有效的校准方法。", "method": "提出了一种具备车辆间交互校准能力、精度保证、高效性及流水线校准能力的仿真平台校准方法，并与无校准基准方法和现有先进方法进行对比验证。", "result": "该方法将交互复现精度提升83.53%，校准效率提高76.75%，在车辆层面和交通流层面的指标精度均保持良好，整体提升51.9%，且实现了全自动校准无需人工干预。", "conclusion": "所提出的校准方法能有效提升自动驾驶仿真测试中交互行为的复现精度和校准效率，实现了全自动化，为仿真测试的可靠性提供了重要保障。"}}
{"id": "2512.16069", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16069", "abs": "https://arxiv.org/abs/2512.16069", "authors": ["Maolin Lei", "Edoardo Romiti", "Arturo Laurenzi", "Rui Dai", "Matteo Dalle Vedove", "Jiatao Ding", "Daniele Fontanelli", "Nikos Tsagarakis"], "title": "A Task-Driven, Planner-in-the-Loop Computational Design Framework for Modular Manipulators", "comment": null, "summary": "Modular manipulators composed of pre-manufactured and interchangeable modules offer high adaptability across diverse tasks. However, their deployment requires generating feasible motions while jointly optimizing morphology and mounted pose under kinematic, dynamic, and physical constraints. Moreover, traditional single-branch designs often extend reach by increasing link length, which can easily violate torque limits at the base joint. To address these challenges, we propose a unified task-driven computational framework that integrates trajectory planning across varying morphologies with the co-optimization of morphology and mounted pose. Within this framework, a hierarchical model predictive control (HMPC) strategy is developed to enable motion planning for both redundant and non-redundant manipulators. For design optimization, the CMA-ES is employed to efficiently explore a hybrid search space consisting of discrete morphology configurations and continuous mounted poses. Meanwhile, a virtual module abstraction is introduced to enable bi-branch morphologies, allowing an auxiliary branch to offload torque from the primary branch and extend the achievable workspace without increasing the capacity of individual joint modules. Extensive simulations and hardware experiments on polishing, drilling, and pick-and-place tasks demonstrate the effectiveness of the proposed framework. The results show that: 1) the framework can generate multiple feasible designs that satisfy kinematic and dynamic constraints while avoiding environmental collisions for given tasks; 2) flexible design objectives, such as maximizing manipulability, minimizing joint effort, or reducing the number of modules, can be achieved by customizing the cost functions; and 3) a bi-branch morphology capable of operating in a large workspace can be realized without requiring more powerful basic modules.", "AI": {"tldr": "提出一个任务驱动的计算框架，用于联合优化模块化机械臂的形态、安装位姿和轨迹规划，并引入双分支形态以扩展工作空间而不增加关节模块容量。", "motivation": "模块化机械臂部署时需在运动学、动力学和物理约束下联合优化形态与安装位姿，传统单分支设计通过增加连杆长度扩展工作空间易导致基关节扭矩超限。", "method": "1. 建立分层模型预测控制（HMPC）策略，支持冗余与非冗余机械臂的运动规划；2. 采用CMA-ES算法在离散形态配置和连续安装位姿的混合搜索空间中进行设计优化；3. 引入虚拟模块抽象实现双分支形态，通过辅助分支分担主分支扭矩。", "result": "仿真和硬件实验（抛光、钻孔、抓放任务）表明：1. 框架能为给定任务生成满足约束且避障的可行设计；2. 通过定制成本函数可实现最大化可操作性、最小化关节负载或减少模块数量等目标；3. 双分支形态可在不增强基础模块的情况下实现大工作空间操作。", "conclusion": "该框架能有效协同优化模块化机械臂的形态、位姿与运动，双分支设计突破了传统单分支的扭矩限制，为复杂任务提供了灵活高效的解决方案。"}}
{"id": "2512.16145", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16145", "abs": "https://arxiv.org/abs/2512.16145", "authors": ["Pengyu Wang", "Shuchang Ye", "Usman Naseem", "Jinman Kim"], "title": "MRG-R1: Reinforcement Learning for Clinically Aligned Medical Report Generation", "comment": "12 pages", "summary": "Medical report generation (MRG) aims to automatically derive radiology-style reports from medical images to aid in clinical decision-making. However, existing methods often generate text that mimics the linguistic style of radiologists but fails to guarantee clinical correctness, because they are trained on token-level objectives which focus on word-choice and sentence structure rather than actual medical accuracy. We propose a semantic-driven reinforcement learning (SRL) method for medical report generation, adopted on a large vision-language model (LVLM). SRL adopts Group Relative Policy Optimization (GRPO) to encourage clinical-correctness-guided learning beyond imitation of language style. Specifically, we optimise a report-level reward: a margin-based cosine similarity (MCCS) computed between key radiological findings extracted from generated and reference reports, thereby directly aligning clinical-label agreement and improving semantic correctness. A lightweight reasoning format constraint further guides the model to generate structured \"thinking report\" outputs. We evaluate Medical Report Generation with Sematic-driven Reinforment Learning (MRG-R1), on two datasets: IU X-Ray and MIMIC-CXR using clinical efficacy (CE) metrics. MRG-R1 achieves state-of-the-art performance with CE-F1 51.88 on IU X-Ray and 40.39 on MIMIC-CXR. We found that the label-semantic reinforcement is better than conventional token-level supervision. These results indicate that optimizing a clinically grounded, report-level reward rather than token overlap,meaningfully improves clinical correctness. This work is a prior to explore semantic-reinforcement in supervising medical correctness in medical Large vision-language model(Med-LVLM) training.", "AI": {"tldr": "提出了一种基于语义驱动强化学习（SRL）的医学报告生成方法MRG-R1，通过优化报告级奖励（基于关键放射学发现的余弦相似度）而非传统的词级目标，显著提升了生成报告的临床正确性。", "motivation": "现有医学报告生成方法多关注语言风格的模仿，但缺乏对临床正确性的保证，因为其训练目标（如词级损失）未直接优化医学准确性。", "method": "采用语义驱动强化学习（SRL），结合Group Relative Policy Optimization（GRPO）优化报告级奖励——基于生成报告与参考报告中提取的关键放射学发现计算的边际余弦相似度（MCCS），并引入轻量级推理格式约束以生成结构化报告。", "result": "在IU X-Ray和MIMIC-CXR数据集上，MRG-R1在临床效能（CE）指标上达到最优性能（CE-F1分别为51.88和40.39），证明语义强化优于传统词级监督。", "conclusion": "优化基于临床的报告级奖励（而非词级重叠）能有效提升医学报告生成的临床正确性，为医学大视觉语言模型（Med-LVLM）的语义强化监督提供了先导探索。"}}
{"id": "2512.15971", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15971", "abs": "https://arxiv.org/abs/2512.15971", "authors": ["Manuel Nkegoum", "Minh-Tan Pham", "Élisa Fromont", "Bruno Avignon", "Sébastien Lefèvre"], "title": "From Words to Wavelengths: VLMs for Few-Shot Multispectral Object Detection", "comment": null, "summary": "Multispectral object detection is critical for safety-sensitive applications such as autonomous driving and surveillance, where robust perception under diverse illumination conditions is essential. However, the limited availability of annotated multispectral data severely restricts the training of deep detectors. In such data-scarce scenarios, textual class information can serve as a valuable source of semantic supervision. Motivated by the recent success of Vision-Language Models (VLMs) in computer vision, we explore their potential for few-shot multispectral object detection. Specifically, we adapt two representative VLM-based detectors, Grounding DINO and YOLO-World, to handle multispectral inputs and propose an effective mechanism to integrate text, visual and thermal modalities. Through extensive experiments on two popular multispectral image benchmarks, FLIR and M3FD, we demonstrate that VLM-based detectors not only excel in few-shot regimes, significantly outperforming specialized multispectral models trained with comparable data, but also achieve competitive or superior results under fully supervised settings. Our findings reveal that the semantic priors learned by large-scale VLMs effectively transfer to unseen spectral modalities, ofFering a powerful pathway toward data-efficient multispectral perception.", "AI": {"tldr": "该研究探索了基于视觉语言模型（VLMs）的少样本多光谱目标检测，通过改进Grounding DINO和YOLO-World模型处理多光谱输入，在FLIR和M3FD数据集上取得了优于专用多光谱模型的性能。", "motivation": "多光谱目标检测在自动驾驶等安全敏感应用中至关重要，但标注数据稀缺限制了深度检测器的训练。文本类别信息可作为语义监督的有效来源，而视觉语言模型在计算机视觉中的成功应用激发了其在少样本多光谱检测中的潜力探索。", "method": "1. 将两种代表性的VLM检测器（Grounding DINO和YOLO-World）适配为多光谱输入处理；2. 提出有效的文本、视觉和热模态融合机制；3. 在FLIR和M3FD多光谱图像基准上进行广泛实验。", "result": "1. 在少样本场景下，VLM检测器显著优于使用相当数据训练的专用多光谱模型；2. 在全监督设置下也能达到竞争性或更优的结果；3. 大规模VLM学习的语义先验可有效迁移到未见过的光谱模态。", "conclusion": "视觉语言模型为数据高效的多光谱感知提供了强大途径，其语义先验知识能够有效迁移至多光谱领域，在数据稀缺和全监督场景下均表现出优越性能。"}}
{"id": "2512.15977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15977", "abs": "https://arxiv.org/abs/2512.15977", "authors": ["Earl Ranario", "Mason J. Earles"], "title": "Are vision-language models ready to zero-shot replace supervised classification models in agriculture?", "comment": "Draft version", "summary": "Vision-language models (VLMs) are increasingly proposed as general-purpose solutions for visual recognition tasks, yet their reliability for agricultural decision support remains poorly understood. We benchmark a diverse set of open-source and closed-source VLMs on 27 agricultural classification datasets from the AgML collection, spanning 162 classes across plant disease, pest and damage, and plant and weed species identification. Across all tasks, zero-shot VLMs substantially underperform a supervised task-specific baseline (YOLO11), which consistently achieves markedly higher accuracy than any foundation model. Under multiple-choice prompting, the best-performing VLM (Gemini-3 Pro) reaches approximately 62% average accuracy, while open-ended prompting yields much lower performance, with raw accuracies typically below 25%. Applying LLM-based semantic judging increases open-ended accuracy (for example, from 21% to 30% for top models) and alters model rankings, demonstrating that evaluation methodology meaningfully affects reported conclusions. Among open-source models, Qwen-VL-72B performs best, approaching closed-source performance under constrained prompting but still trailing top proprietary systems. Task-level analysis shows that plant and weed species classification is consistently easier than pest and damage identification, which remains the most challenging category across models. Overall, these results indicate that current off-the-shelf VLMs are not yet suitable as standalone agricultural diagnostic systems, but can function as assistive components when paired with constrained interfaces, explicit label ontologies, and domain-aware evaluation strategies.", "AI": {"tldr": "本文评估了多种开源和闭源视觉语言模型在27个农业分类数据集上的表现，发现当前VLM在零样本设置下显著落后于监督学习基线，尚不能作为独立的农业诊断系统，但可作为辅助组件使用。", "motivation": "视觉语言模型被广泛提出作为视觉识别任务的通用解决方案，但其在农业决策支持中的可靠性尚未得到充分研究，需要系统评估其在农业分类任务中的实际表现。", "method": "使用AgML收集的27个农业分类数据集（涵盖162个类别），对多种开源和闭源VLM进行基准测试，比较零样本性能与监督学习基线（YOLO11），并分析多选题提示、开放式提示及基于LLM的语义判断等不同评估方法的影响。", "result": "1. 零样本VLM在所有任务中均大幅落后于监督学习基线；2. 最佳VLM（Gemini-3 Pro）在多选题提示下平均准确率约62%，开放式提示下通常低于25%；3. 使用LLM语义判断可提升开放式提示准确率（如从21%升至30%）；4. 开源模型中Qwen-VL-72B表现最佳，但仍落后于顶级闭源系统；5. 植物和杂草分类任务比病虫害识别更容易。", "conclusion": "当前现成的VLM尚不适合作为独立的农业诊断系统，但可通过结合约束界面、明确标签本体和领域感知评估策略，作为辅助组件发挥作用。"}}
{"id": "2512.16147", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16147", "abs": "https://arxiv.org/abs/2512.16147", "authors": ["Yash Bhaskar", "Sankalp Bahad", "Parameswari Krishnamurthy"], "title": "Decoding Fake Narratives in Spreading Hateful Stories: A Dual-Head RoBERTa Model with Multi-Task Learning", "comment": "Accepted Paper, Anthology ID: 2024.icon-fauxhate.3, 4 pages, 1 figure, 1 table", "summary": "Social media platforms, while enabling global connectivity, have become hubs for the rapid spread of harmful content, including hate speech and fake narratives \\cite{davidson2017automated, shu2017fake}. The Faux-Hate shared task focuses on detecting a specific phenomenon: the generation of hate speech driven by fake narratives, termed Faux-Hate. Participants are challenged to identify such instances in code-mixed Hindi-English social media text. This paper describes our system developed for the shared task, addressing two primary sub-tasks: (a) Binary Faux-Hate detection, involving fake and hate speech classification, and (b) Target and Severity prediction, categorizing the intended target and severity of hateful content. Our approach combines advanced natural language processing techniques with domain-specific pretraining to enhance performance across both tasks. The system achieved competitive results, demonstrating the efficacy of leveraging multi-task learning for this complex problem.", "AI": {"tldr": "本文介绍了针对Faux-Hate共享任务的系统，该任务旨在检测印地语-英语混合社交媒体文本中由虚假叙事驱动的仇恨言论。系统通过多任务学习方法，在二元分类及目标与严重性预测任务中取得了有竞争力的结果。", "motivation": "社交媒体已成为有害内容（如仇恨言论和虚假叙事）快速传播的温床。Faux-Hate任务专门针对由虚假叙事驱动的仇恨言论（即Faux-Hate现象）进行检测，以应对其在多语言混合文本环境中的识别挑战。", "method": "采用先进的自然语言处理技术，结合领域特定的预训练模型，并利用多任务学习框架同时处理二元Faux-Hate检测（虚假与仇恨言论分类）以及目标与严重性预测两个子任务。", "result": "系统在共享任务中取得了具有竞争力的性能表现，验证了多任务学习方法在解决此类复杂问题上的有效性。", "conclusion": "通过结合领域预训练与多任务学习，能够有效提升在代码混合文本中检测Faux-Hate内容的性能，为社交媒体有害内容识别提供了可行的技术方案。"}}
{"id": "2512.16379", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16379", "abs": "https://arxiv.org/abs/2512.16379", "authors": ["Manuel G. Satué", "Manuel R. Arahal", "Luis F. Acedo", "Manuel G. Ortega"], "title": "Economic versus energetic model predictive control of a cold production plant with thermal energy storage", "comment": "14 pages", "summary": "Economic model predictive control has been proposed as a means for solving the unit loading and unit allocation problem in multi-chiller cooling plants. The adjective economic stems from the use of financial cost due to electricity consumption in a time horizon, such is the loss function minimized at each sampling period. The energetic approach is rarely encountered. This article presents for the first time a comparison between the energetic optimization objective and the economic one. The comparison is made on a cooling plant using air-cooled water chillers and a cold storage system. Models developed have been integrated into Simscape, and non-convex mixed optimization methods used to achieve optimal control trajectories for both energetic and economic goals considered separately. The results over several scenarios, and in different seasons, support the consideration of the energetic approach despite the current prevalence of the economic one. The results are dependent on the electric season and the available tariffs. In particular, for the high electric season and considering a representative tariff, the results show that an increment of about 2.15% in energy consumption takes place when using the economic approach instead of the energetic one. On the other hand, a reduction in cost of 2.94% is achieved.", "AI": {"tldr": "本文首次比较了多冷机冷却系统中能量优化与经济优化两种控制目标，发现经济优化虽然能降低2.94%成本，但会导致能耗增加2.15%。", "motivation": "当前多冷机冷却系统普遍采用经济模型预测控制（以电费成本最小化为目标），而能量优化方法（以能耗最小化为目标）很少被研究。本文旨在比较这两种优化目标在实际系统中的表现差异。", "method": "在Simscape中建立包含风冷式水冷机和冷存储系统的冷却系统模型，采用非凸混合优化方法分别计算能量优化和经济优化下的最优控制轨迹，并在不同季节和多种场景下进行对比测试。", "result": "结果表明：在高电价季节采用典型电价方案时，经济优化相比能量优化可降低2.94%运行成本，但能耗增加2.15%。优化效果受电价季节和具体电价方案影响显著。", "conclusion": "尽管当前经济优化方法更普遍，但能量优化方法仍值得考虑。实际应用中应根据电价季节和电价结构权衡能耗与成本目标，两种方法各有适用场景。"}}
{"id": "2512.16302", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16302", "abs": "https://arxiv.org/abs/2512.16302", "authors": ["Zixuan Chen", "Chongkai Gao", "Lin Shao", "Jieqi Shi", "Jing Huo", "Yang Gao"], "title": "ManiLong-Shot: Interaction-Aware One-Shot Imitation Learning for Long-Horizon Manipulation", "comment": "Accepted by AAAI 2026", "summary": "One-shot imitation learning (OSIL) offers a promising way to teach robots new skills without large-scale data collection. However, current OSIL methods are primarily limited to short-horizon tasks, thus limiting their applicability to complex, long-horizon manipulations. To address this limitation, we propose ManiLong-Shot, a novel framework that enables effective OSIL for long-horizon prehensile manipulation tasks. ManiLong-Shot structures long-horizon tasks around physical interaction events, reframing the problem as sequencing interaction-aware primitives instead of directly imitating continuous trajectories. This primitive decomposition can be driven by high-level reasoning from a vision-language model (VLM) or by rule-based heuristics derived from robot state changes. For each primitive, ManiLong-Shot predicts invariant regions critical to the interaction, establishes correspondences between the demonstration and the current observation, and computes the target end-effector pose, enabling effective task execution. Extensive simulation experiments show that ManiLong-Shot, trained on only 10 short-horizon tasks, generalizes to 20 unseen long-horizon tasks across three difficulty levels via one-shot imitation, achieving a 22.8% relative improvement over the SOTA. Additionally, real-robot experiments validate ManiLong-Shot's ability to robustly execute three long-horizon manipulation tasks via OSIL, confirming its practical applicability.", "AI": {"tldr": "提出ManiLong-Shot框架，通过将长时程操作任务分解为基于物理交互事件的基元序列，实现单次模仿学习在复杂长时程操作任务上的有效应用。", "motivation": "现有单次模仿学习方法主要适用于短时程任务，难以处理复杂的长时程操作任务，限制了其在机器人技能学习中的实际应用。", "method": "将长时程任务围绕物理交互事件结构化，分解为交互感知的基元序列；利用视觉语言模型或基于状态变化的启发式规则驱动基元分解；为每个基元预测关键交互区域、建立演示与观测间的对应关系，计算目标末端执行器位姿。", "result": "在仅使用10个短时程任务训练的模拟实验中，能通过单次模仿泛化到20个未见的长时程任务，相对现有最佳方法提升22.8%；真实机器人实验验证了其在三个长时程操作任务上的鲁棒执行能力。", "conclusion": "ManiLong-Shot框架通过基元分解方法有效扩展了单次模仿学习在长时程操作任务中的应用，在模拟和真实环境中均表现出良好的泛化能力和实用性。"}}
{"id": "2512.15993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.15993", "abs": "https://arxiv.org/abs/2512.15993", "authors": ["Lars Beckers", "Arno Waes", "Aaron Van Campenhout", "Toon Goedemé"], "title": "Eyes on the Grass: Biodiversity-Increasing Robotic Mowing Using Deep Visual Embeddings", "comment": null, "summary": "This paper presents a robotic mowing framework that actively enhances garden biodiversity through visual perception and adaptive decision-making. Unlike passive rewilding approaches, the proposed system uses deep feature-space analysis to identify and preserve visually diverse vegetation patches in camera images by selectively deactivating the mower blades. A ResNet50 network pretrained on PlantNet300K provides ecologically meaningful embeddings, from which a global deviation metric estimates biodiversity without species-level supervision. These estimates drive a selective mowing algorithm that dynamically alternates between mowing and conservation behavior. The system was implemented on a modified commercial robotic mower and validated both in a controlled mock-up lawn and on real garden datasets. Results demonstrate a strong correlation between embedding-space dispersion and expert biodiversity assessment, confirming the feasibility of deep visual diversity as a proxy for ecological richness and the effectiveness of the proposed mowing decision approach. Widespread adoption of such systems will turn ecologically worthless, monocultural lawns into vibrant, valuable biotopes that boost urban biodiversity.", "AI": {"tldr": "提出一种通过视觉感知和自适应决策主动提升花园生物多样性的机器人割草框架，利用深度特征空间分析识别并保护植被多样性区域，选择性关闭割草刀片。", "motivation": "传统草坪割草导致生态单一化，被动式野化方法效果有限，需要主动干预技术将生态价值低的单一草坪转化为促进城市生物多样性的生物栖息地。", "method": "使用在PlantNet300K上预训练的ResNet50网络提取生态意义特征嵌入，通过全局偏差度量无监督估计生物多样性，开发选择性割草算法动态切换割草与保护行为，在改装商用机器人割草机上实现并验证。", "result": "特征空间离散度与专家生物多样性评估高度相关，证实深度视觉多样性可作为生态丰富度的有效代理指标，所提割草决策方法在实际花园数据集中表现有效。", "conclusion": "该框架通过选择性保护视觉多样性植被斑块，能将单一草坪转化为活跃生物栖息地，大规模应用有望显著提升城市生物多样性。"}}
{"id": "2512.16183", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.16183", "abs": "https://arxiv.org/abs/2512.16183", "authors": ["Mengfan Shen", "Kangqi Song", "Xindi Wang", "Wei Jia", "Tao Wang", "Ziqiang Han"], "title": "A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media", "comment": "41 pages,3figures and 9 tables", "summary": "Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.", "AI": {"tldr": "提出一个基于Qwen2.5-7B模型、结合提示工程与LoRA微调的领域自适应信息抽取管道，用于从警方通报中可靠提取15个关键字段，在死亡检测、伤亡计数和地点抽取等任务上达到高准确率。", "motivation": "警方通报等社交媒体文本具有多变性和非正式性，传统方法难以可靠地从中提取结构化信息，而结构化信息对于及时准确的数据处理至关重要。", "method": "使用针对性的提示工程，并结合参数高效的LoRA方法对Qwen2.5-7B模型进行微调；构建了一个包含4,933条实例的高质量标注数据集，数据来源于2019-2020年新浪微博的27,822条警方通报。", "result": "LoRA微调显著提升了模型性能，在死亡检测任务上准确率超过98.36%，在死亡人数和省级地点提取任务上的精确匹配率分别达到95.31%和95.54%。", "conclusion": "该管道为专业领域的多任务结构化信息抽取提供了一个经过验证的高效解决方案，为社会科学研究中将非结构化文本转化为可靠结构化数据提供了实用框架。"}}
{"id": "2512.15906", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15906", "abs": "https://arxiv.org/abs/2512.15906", "authors": ["Jonathan A. Handler"], "title": "Darth Vecdor: An Open-Source System for Generating Knowledge Graphs Through Large Language Model Queries", "comment": "17 pages, 3 figures", "summary": "Many large language models (LLMs) are trained on a massive body of knowledge present on the Internet. Darth Vecdor (DV) was designed to extract this knowledge into a structured, terminology-mapped, SQL database (\"knowledge base\" or \"knowledge graph\"). Knowledge graphs may be useful in many domains, including healthcare. Although one might query an LLM directly rather than a SQL-based knowledge graph, concerns such as cost, speed, safety, and confidence may arise, especially in high-volume operations. These may be mitigated when the information is pre-extracted from the LLM and becomes query-able through a standard database. However, the author found the need to address several issues. These included erroneous, off-topic, free-text, overly general, and inconsistent LLM responses, as well as allowing for multi-element responses. DV was built with features intended to mitigate these issues. To facilitate ease of use, and to allow for prompt engineering by those with domain expertise but little technical background, DV provides a simple, browser-based graphical user interface. DV has been released as free, open-source, extensible software, on an \"as is\" basis, without warranties or conditions of any kind, either express or implied. Users need to be cognizant of the potential risks and benefits of using DV and its outputs, and users are responsible for ensuring any use is safe and effective. DV should be assumed to have bugs, potentially very serious ones. However, the author hopes that appropriate use of current and future versions of DV and its outputs can help improve healthcare.", "AI": {"tldr": "Darth Vecdor (DV) 是一个开源工具，用于从大型语言模型中提取结构化知识到SQL知识图谱，旨在解决直接查询LLM的成本、速度、安全性和置信度问题，特别关注医疗领域的应用。", "motivation": "直接查询大型语言模型在医疗等高要求领域存在成本高、速度慢、安全性低和置信度不足的问题，因此需要一种方法将LLM中的知识预先提取到可查询的结构化数据库中。", "method": "开发了Darth Vecdor (DV) 工具，通过术语映射将LLM知识提取到SQL知识图谱中，并设计了浏览器图形界面以方便非技术用户进行提示工程，同时解决了LLM响应中的错误、离题、自由文本、过于泛化和不一致等问题。", "result": "DV 已作为免费开源软件发布，具备可扩展性，但用户需自行承担使用风险，工具可能存在严重缺陷。", "conclusion": "尽管DV存在潜在缺陷，但作者希望其当前及未来版本能通过结构化知识提取帮助改善医疗领域，用户需负责任地评估使用风险与收益。"}}
{"id": "2512.15894", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15894", "abs": "https://arxiv.org/abs/2512.15894", "authors": ["Vahideh Zolfaghari"], "title": "PediatricAnxietyBench: Evaluating Large Language Model Safety Under Parental Anxiety and Pressure in Pediatric Consultations", "comment": null, "summary": "Large language models (LLMs) are increasingly consulted by parents for pediatric guidance, yet their safety under real-world adversarial pressures is poorly understood. Anxious parents often use urgent language that can compromise model safeguards, potentially causing harmful advice. PediatricAnxietyBench is an open-source benchmark of 300 high-quality queries across 10 pediatric topics (150 patient-derived, 150 adversarial) enabling reproducible evaluation. Two Llama models (70B and 8B) were assessed using a multi-dimensional safety framework covering diagnostic restraint, referral adherence, hedging, and emergency recognition. Adversarial queries incorporated parental pressure patterns, including urgency, economic barriers, and challenges to disclaimers. Mean safety score was 5.50/15 (SD=2.41). The 70B model outperformed the 8B model (6.26 vs 4.95, p<0.001) with lower critical failures (4.8% vs 12.0%, p=0.02). Adversarial queries reduced safety by 8% (p=0.03), with urgency causing the largest drop (-1.40). Vulnerabilities appeared in seizures (33.3% inappropriate diagnosis) and post-vaccination queries. Hedging strongly correlated with safety (r=0.68, p<0.001), while emergency recognition was absent. Model scale influences safety, yet all models showed vulnerabilities to realistic parental pressures. PediatricAnxietyBench provides a reusable adversarial evaluation framework to reveal clinically significant failure modes overlooked by standard benchmarks.", "AI": {"tldr": "该研究开发了儿科焦虑基准测试PediatricAnxietyBench，评估大语言模型在真实世界家长压力下的安全性，发现模型存在安全隐患，对抗性查询会降低安全性，模型规模影响安全表现。", "motivation": "大语言模型越来越多地被家长用于儿科咨询，但其在真实世界对抗性压力下的安全性尚未得到充分研究。焦虑家长常使用紧急语言，可能绕过模型安全防护，导致有害建议。", "method": "创建包含300个高质量查询的开放基准测试（150个患者来源，150个对抗性），涵盖10个儿科主题。使用多维安全框架评估两个Llama模型（70B和8B），评估维度包括诊断克制、转诊依从性、措辞谨慎性和紧急情况识别。对抗性查询模拟家长压力模式，如紧迫性、经济障碍和免责声明挑战。", "result": "平均安全得分为5.50/15（SD=2.41）。70B模型优于8B模型（6.26 vs 4.95，p<0.001），关键失败率更低（4.8% vs 12.0%，p=0.02）。对抗性查询使安全性降低8%（p=0.03），紧迫性导致最大降幅（-1.40）。模型在癫痫发作（33.3%不当诊断）和疫苗接种后查询中表现脆弱。措辞谨慎性与安全性强相关（r=0.68，p<0.001），但紧急情况识别完全缺失。", "conclusion": "模型规模影响安全性，但所有模型都对真实家长压力表现出脆弱性。PediatricAnxietyBench提供了一个可重复使用的对抗性评估框架，能够揭示标准基准测试忽略的临床重要失败模式。"}}
{"id": "2512.16367", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16367", "abs": "https://arxiv.org/abs/2512.16367", "authors": ["Sijia Chen", "Wei Dong"], "title": "A2VISR: An Active and Adaptive Ground-Aerial Localization System Using Visual Inertial and Single-Range Fusion", "comment": "accept by IEEE Transactions on Industrial Electronics", "summary": "It's a practical approach using the ground-aerial collaborative system to enhance the localization robustness of flying robots in cluttered environments, especially when visual sensors degrade. Conventional approaches estimate the flying robot's position using fixed cameras observing pre-attached markers, which could be constrained by limited distance and susceptible to capture failure. To address this issue, we improve the ground-aerial localization framework in a more comprehensive manner, which integrates active vision, single-ranging, inertial odometry, and optical flow. First, the designed active vision subsystem mounted on the ground vehicle can be dynamically rotated to detect and track infrared markers on the aerial robot, improving the field of view and the target recognition with a single camera. Meanwhile, the incorporation of single-ranging extends the feasible distance and enhances re-capture capability under visual degradation. During estimation, a dimension-reduced estimator fuses multi-source measurements based on polynomial approximation with an extended sliding window, balancing computational efficiency and redundancy. Considering different sensor fidelities, an adaptive sliding confidence evaluation algorithm is implemented to assess measurement quality and dynamically adjust the weighting parameters based on moving variance. Finally, extensive experiments under conditions such as smoke interference, illumination variation, obstacle occlusion, prolonged visual loss, and extended operating range demonstrate that the proposed approach achieves robust online localization, with an average root mean square error of approximately 0.09 m, while maintaining resilience to capture loss and sensor failures.", "AI": {"tldr": "提出一种地面-空中协同系统，通过主动视觉、单点测距、惯性里程计和光流融合的方法，提升飞行机器人在复杂环境中的定位鲁棒性，尤其在视觉传感器退化时。", "motivation": "传统基于固定摄像头观测标记的方法存在距离受限、易丢失目标的问题，难以在视觉退化或遮挡的杂乱环境中保证飞行机器人定位的稳定性。", "method": "设计地面车辆搭载的主动视觉子系统动态跟踪空中机器人红外标记；结合单点测距扩展有效距离；采用降维估计器融合多源测量，基于多项式近似与扩展滑动窗口平衡计算效率；引入自适应滑动置信度评估算法动态调整权重。", "result": "在烟雾干扰、光照变化、障碍遮挡、长时间视觉丢失及大范围操作等条件下，该方法实现鲁棒的在线定位，平均均方根误差约0.09米，且对目标丢失和传感器故障具有强韧性。", "conclusion": "所提出的地面-空中协同定位框架通过多传感器融合与自适应权重机制，显著提升了复杂环境下飞行机器人定位的鲁棒性和适应性。"}}
{"id": "2512.16023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16023", "abs": "https://arxiv.org/abs/2512.16023", "authors": ["Liudi Yang", "Yang Bai", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Dong Chen", "Ziyuan Liu", "Abhinav Valada"], "title": "CoVAR: Co-generation of Video and Action for Robotic Manipulation via Multi-Modal Diffusion", "comment": "9 pages, 7 figures", "summary": "We present a method to generate video-action pairs that follow text instructions, starting from an initial image observation and the robot's joint states. Our approach automatically provides action labels for video diffusion models, overcoming the common lack of action annotations and enabling their full use for robotic policy learning. Existing methods either adopt two-stage pipelines, which limit tightly coupled cross-modal information sharing, or rely on adapting a single-modal diffusion model for a joint distribution that cannot fully leverage pretrained video knowledge. To overcome these limitations, we (1) extend a pretrained video diffusion model with a parallel, dedicated action diffusion model that preserves pretrained knowledge, (2) introduce a Bridge Attention mechanism to enable effective cross-modal interaction, and (3) design an action refinement module to convert coarse actions into precise controls for low-resolution datasets. Extensive evaluations on multiple public benchmarks and real-world datasets demonstrate that our method generates higher-quality videos, more accurate actions, and significantly outperforms existing baselines, offering a scalable framework for leveraging large-scale video data for robotic learning.", "AI": {"tldr": "提出一种从图像和关节状态生成视频-动作对的方法，通过扩展预训练视频扩散模型并引入跨模态交互机制，解决机器人策略学习中动作标注缺乏的问题。", "motivation": "现有方法存在两阶段流程限制跨模态信息共享，或单模态扩散模型无法充分利用预训练视频知识的问题，需要一种能自动生成动作标注并有效利用大规模视频数据的方法。", "method": "1. 扩展预训练视频扩散模型，增加并行专用动作扩散模型以保留预训练知识；2. 引入桥接注意力机制实现跨模态交互；3. 设计动作细化模块将粗略动作转换为精确控制。", "result": "在多个公开基准和真实数据集上的评估表明，该方法能生成更高质量的视频、更准确的动作，显著优于现有基线方法。", "conclusion": "该方法为利用大规模视频数据进行机器人学习提供了可扩展的框架，通过自动生成动作标注有效解决了数据标注不足的问题。"}}
{"id": "2512.16497", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16497", "abs": "https://arxiv.org/abs/2512.16497", "authors": ["Mohamed Shamseldein"], "title": "From Liability to Asset: A Three-Mode Grid-Forming Control Framework for Centralized Data Center UPS Systems", "comment": null, "summary": "AI workloads are turning large data centers into highly dynamic power-electronic loads; fault-time behavior and workload pulsing can stress weak-grid points of interconnection. This paper proposes a centralized medium-voltage (MV) uninterruptible power supply (UPS) control architecture implemented as three operating modes: Mode 1 regulates a DC stiff bus and shapes normal-operation grid draw, Mode 2 enforces current-limited fault-mode P--Q priority with UPS battery energy storage system (UPS-BESS) buffering and a rate-limited post-fault \"soft return,\" and Mode 3 optionally provides droop-based fast frequency response via grid-draw modulation. Fundamental-frequency averaged dq simulations (50 MW block, short-circuit ratio (SCR) = 1.5, 0.5 p.u. three-phase dip for 150~ms) show zero unserved information-technology (IT) energy (0.00000 MWh vs.0.00208 MWh for a momentary-cessation benchmark), a 0.57 p.u. peak inverter current (vs. 1.02 p.u. for a synchronous-reference-frame phase-locked loop (SRF-PLL) low-voltage ride-through (LVRT) baseline), a nonzero mean fault-window grid draw of 0.20~p.u. (vs.approx 0 for momentary cessation), and an improved settled point-of-common-coupling (PCC) voltage minimum of 0.79 p.u. after one cycle (vs. 0.66 p.u.). A forced-oscillation case study applies a 1 Hz pulsed load (+/- 0.25 p.u.) and shows that the normal-operation shaping filters the oscillation seen by the grid while the UPS-BESS buffers the pulsing component.", "AI": {"tldr": "本文提出了一种集中式中压不间断电源控制架构，通过三种运行模式管理数据中心动态负载，在故障期间保持IT负载供电并改善电网稳定性。", "motivation": "AI工作负载使数据中心成为高度动态的电力电子负载，故障时的行为和负载脉冲可能对弱电网连接点造成压力，需要解决方案来维持供电连续性和电网稳定。", "method": "提出三模式控制架构：模式1调节直流母线并整形正常电网功率；模式2实施限流故障模式P-Q优先级控制，利用UPS-BESS缓冲并实现速率限制的“软恢复”；模式3可选提供基于下垂的快速频率响应。通过基频平均dq仿真验证（50 MW模块，SCR=1.5，150ms三相电压跌落）。", "result": "仿真显示：零未供电IT能量（对比基准0.00208 MWh）；峰值逆变电流0.57 p.u.（对比SRF-PLL LVRT基准1.02 p.u.）；故障期间电网功率均值0.20 p.u.（对比瞬时中断接近0）；PCC电压最小值0.79 p.u.（对比0.66 p.u.）。脉冲负载案例显示正常模式滤波可抑制电网侧振荡。", "conclusion": "所提出的集中式UPS控制架构能有效维持数据中心供电连续性，降低故障期间逆变器电流应力，改善电网电压恢复，并通过储能缓冲平滑负载脉冲对电网的影响。"}}
{"id": "2512.16189", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16189", "abs": "https://arxiv.org/abs/2512.16189", "authors": ["Musarrat Zeba", "Abdullah Al Mamun", "Kishoar Jahan Tithee", "Debopom Sutradhar", "Mohaimenul Azam Khan Raiaan", "Saddam Mukta", "Reem E. Mohamed", "Md Rafiqul Islam", "Yakub Sebastian", "Mukhtar Hussain", "Sami Azam"], "title": "Mitigating Hallucinations in Healthcare LLMs with Granular Fact-Checking and Domain-Specific Adaptation", "comment": null, "summary": "In healthcare, it is essential for any LLM-generated output to be reliable and accurate, particularly in cases involving decision-making and patient safety. However, the outputs are often unreliable in such critical areas due to the risk of hallucinated outputs from the LLMs. To address this issue, we propose a fact-checking module that operates independently of any LLM, along with a domain-specific summarization model designed to minimize hallucination rates. Our model is fine-tuned using Low-Rank Adaptation (LoRa) on the MIMIC III dataset and is paired with the fact-checking module, which uses numerical tests for correctness and logical checks at a granular level through discrete logic in natural language processing (NLP) to validate facts against electronic health records (EHRs). We trained the LLM model on the full MIMIC-III dataset. For evaluation of the fact-checking module, we sampled 104 summaries, extracted them into 3,786 propositions, and used these as facts. The fact-checking module achieves a precision of 0.8904, a recall of 0.8234, and an F1-score of 0.8556. Additionally, the LLM summary model achieves a ROUGE-1 score of 0.5797 and a BERTScore of 0.9120 for summary quality.", "AI": {"tldr": "该研究提出了一种独立于LLM的事实核查模块和特定领域摘要模型，以减少医疗领域LLM输出的幻觉问题，并在MIMIC-III数据集上验证了其有效性。", "motivation": "在医疗健康领域，LLM生成的输出必须可靠准确，尤其是在涉及决策和患者安全的关键场景中。然而，LLM存在幻觉风险，导致输出不可靠，因此需要解决这一问题。", "method": "使用LoRa在MIMIC-III数据集上微调LLM，构建特定领域摘要模型；设计独立的事实核查模块，通过数值正确性测试和基于离散逻辑的自然语言处理细粒度逻辑检查，验证事实与电子健康记录的一致性。", "result": "事实核查模块在3,786个命题上达到精确率0.8904、召回率0.8234、F1分数0.8556；摘要模型的ROUGE-1得分为0.5797，BERTScore为0.9120。", "conclusion": "结合事实核查模块和特定领域摘要模型能有效减少LLM在医疗领域的幻觉，提高输出可靠性，为关键医疗应用提供了可行解决方案。"}}
{"id": "2512.16624", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16624", "abs": "https://arxiv.org/abs/2512.16624", "authors": ["Mark Benazet", "Francesco Ricca", "Dario Bralla", "Melanie N. Zeilinger", "Andrea Carron"], "title": "Learning-based Approximate Model Predictive Control for an Impact Wrench Tool", "comment": null, "summary": "Learning-based model predictive control has emerged as a powerful approach for handling complex dynamics in mechatronic systems, enabling data-driven performance improvements while respecting safety constraints. However, when computational resources are severely limited, as in battery-powered tools with embedded processors, existing approaches struggle to meet real-time requirements. In this paper, we address the problem of real-time torque control for impact wrenches, where high-frequency control updates are necessary to accurately track the fast transients occurring during periodic impact events, while maintaining high-performance safety-critical control that mitigates harmful vibrations and component wear. The key novelty of the approach is that we combine data-driven model augmentation through Gaussian process regression with neural network approximation of the resulting control policy. This insight allows us to deploy predictive control on resource-constrained embedded platforms while maintaining both constraint satisfaction and microsecond-level inference times. The proposed framework is evaluated through numerical simulations and hardware experiments on a custom impact wrench testbed. The results show that our approach successfully achieves real-time control suitable for high-frequency operation while maintaining constraint satisfaction and improving tracking accuracy compared to baseline PID control.", "AI": {"tldr": "提出一种结合高斯过程回归和神经网络近似的学习型模型预测控制方法，用于资源受限嵌入式平台上的实时扭矩控制，在冲击扳手上实现微秒级推理时间。", "motivation": "电池供电工具等计算资源严重受限的嵌入式系统难以满足现有学习型模型预测控制的实时性要求，而冲击扳手等设备需要高频控制更新以跟踪快速瞬态过程并保证安全约束。", "method": "通过高斯过程回归进行数据驱动模型增强，并用神经网络近似所得控制策略，实现在资源受限平台上的部署。", "result": "数值仿真和硬件实验表明，该方法在保持约束满足的同时实现微秒级推理时间，相比基线PID控制提升了跟踪精度。", "conclusion": "所提框架能够在资源受限嵌入式平台上实现适用于高频操作的实时预测控制，同时保证约束满足和控制性能。"}}
{"id": "2512.15922", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15922", "abs": "https://arxiv.org/abs/2512.15922", "authors": ["Jovan Pavlović", "Miklós Krész", "László Hajdu"], "title": "Leveraging Spreading Activation for Improved Document Retrieval in Knowledge-Graph-Based RAG Systems", "comment": "20 pages, 5 figures", "summary": "Despite initial successes and a variety of architectures, retrieval-augmented generation (RAG) systems still struggle to reliably retrieve and connect the multi-step evidence required for complicated reasoning tasks. Most of the standard RAG frameworks regard all retrieved information as equally reliable, overlooking the varying credibility and interconnected nature of large textual corpora. GraphRAG approaches offer potential improvement to RAG systems by integrating knowledge graphs, which structure information into nodes and edges, capture entity relationships, and enable multi-step logical traversal. However, GraphRAG is not always an ideal solution as it depends on high-quality graph representations of the corpus, which requires either pre-existing knowledge graphs that are expensive to build and update, or automated graph construction pipelines that are often unreliable. Moreover, systems following this paradigm typically use large language models to guide graph traversal and evidence retrieval, leading to challenges similar to those encountered with standard RAG. In this paper, we propose a novel RAG framework that employs the spreading activation algorithm to retrieve information from a corpus of documents interconnected by automatically constructed knowledge graphs, thereby enhancing the performance of large language models on complex tasks such as multi-hop question answering. Experiments show that our method achieves better or comparable performance to iterative RAG methodologies, while also being easily integrable as a plug-and-play module with a wide range of RAG-based approaches. Combining our method with chain-of-thought iterative retrieval yields up to a 39\\% absolute gain in answer correctness compared to naive RAG, achieving these results with small open-weight language models and highlighting its effectiveness in resource-constrained settings.", "AI": {"tldr": "提出一种基于扩散激活算法的新型RAG框架，通过自动构建的知识图谱连接文档，提升大语言模型在复杂推理任务中的表现。", "motivation": "传统RAG系统难以可靠检索多步证据，且忽视信息的可信度和关联性；GraphRAG依赖高质量知识图谱构建成本高，且存在与标准RAG类似的挑战。", "method": "使用扩散激活算法从自动构建的知识图谱中检索信息，增强文档间的关联，可作为即插即用模块与多种RAG方法集成。", "result": "在复杂问答任务中性能优于或接近迭代RAG方法，与思维链迭代检索结合相比朴素RAG提升39%答案正确率，在资源受限环境下使用小规模开源模型有效。", "conclusion": "该方法通过扩散激活算法利用知识图谱增强检索，显著提升复杂推理任务性能，且具有易集成和资源高效的优势。"}}
{"id": "2512.16446", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16446", "abs": "https://arxiv.org/abs/2512.16446", "authors": ["Enis Yalcin", "Joshua O'Hara", "Maria Stamatopoulou", "Chengxu Zhou", "Dimitrios Kanoulas"], "title": "E-SDS: Environment-aware See it, Do it, Sorted - Automated Environment-Aware Reinforcement Learning for Humanoid Locomotion", "comment": "12 pages, 3 figures, 4 tables. Accepted at RiTA 2025 (Springer LNNS)", "summary": "Vision-language models (VLMs) show promise in automating reward design in humanoid locomotion, which could eliminate the need for tedious manual engineering. However, current VLM-based methods are essentially \"blind\", as they lack the environmental perception required to navigate complex terrain. We present E-SDS (Environment-aware See it, Do it, Sorted), a framework that closes this perception gap. E-SDS integrates VLMs with real-time terrain sensor analysis to automatically generate reward functions that facilitate training of robust perceptive locomotion policies, grounded by example videos. Evaluated on a Unitree G1 humanoid across four distinct terrains (simple, gaps, obstacles, stairs), E-SDS uniquely enabled successful stair descent, while policies trained with manually-designed rewards or a non-perceptive automated baseline were unable to complete the task. In all terrains, E-SDS also reduced velocity tracking error by 51.9-82.6%. Our framework reduces the human effort of reward design from days to less than two hours while simultaneously producing more robust and capable locomotion policies.", "AI": {"tldr": "提出E-SDS框架，结合视觉语言模型和地形感知，自动生成奖励函数以训练人形机器人适应复杂地形的运动策略，显著减少人工设计工作量并提升性能。", "motivation": "现有基于视觉语言模型的奖励设计方法缺乏环境感知能力，无法适应复杂地形，需要大量人工工程。", "method": "E-SDS框架整合视觉语言模型与实时地形传感器分析，通过示例视频自动生成奖励函数，训练具有环境感知能力的运动策略。", "result": "在Unitree G1人形机器人上测试四种地形，E-SDS首次实现成功下楼梯任务，速度跟踪误差降低51.9-82.6%，奖励设计时间从数天缩短至2小时内。", "conclusion": "E-SSD通过环境感知的自动化奖励设计，显著提升人形机器人在复杂地形中的运动鲁棒性和能力，大幅减少人工干预。"}}
{"id": "2512.16227", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16227", "abs": "https://arxiv.org/abs/2512.16227", "authors": ["Qizhou Chen", "Chengyu Wang", "Taolin Zhang", "Xiaofeng He"], "title": "An Information-Theoretic Framework for Robust Large Language Model Editing", "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools in science, technology, and society, enabling transformative advances across diverse fields. However, errors or outdated information within these models can undermine their accuracy and restrict their safe deployment. Developing efficient strategies for updating model knowledge without the expense and disruption of full retraining remains a critical challenge. Current model editing techniques frequently struggle to generalize corrections beyond narrow domains, leading to unintended consequences and limiting their practical impact. Here, we introduce a novel framework for editing LLMs, grounded in information bottleneck theory. This approach precisely compresses and isolates the essential information required for generalizable knowledge correction while minimizing disruption to unrelated model behaviors. Building upon this foundation, we present the Information Bottleneck Knowledge Editor (IBKE), which leverages compact latent representations to guide gradient-based updates, enabling robust and broadly applicable model editing. We validate IBKE's effectiveness across multiple LLM architectures and standard benchmark tasks, demonstrating state-of-the-art accuracy and improved generality and specificity of edits. These findings establish a theoretically principled and practical paradigm for open-domain knowledge editing, advancing the utility and trustworthiness of LLMs in real-world applications.", "AI": {"tldr": "提出基于信息瓶颈理论的LLM知识编辑框架IBKE，通过压缩关键信息实现泛化性知识修正，在多种模型和基准任务上达到最优性能。", "motivation": "LLM中的错误或过时信息会影响其准确性和安全部署，现有编辑方法泛化能力有限且易产生副作用，需要高效且通用的知识更新策略。", "method": "基于信息瓶颈理论构建编辑框架，利用紧凑的潜在表示引导梯度更新，在最小化无关行为干扰的同时实现可泛化的知识修正。", "result": "IBKE在多种LLM架构和标准基准任务上验证有效，实现了最先进的编辑准确率，并显著提升了编辑的泛化性和特异性。", "conclusion": "该研究为开放域知识编辑建立了理论原则和实践范式，提升了LLM在现实应用中的实用性和可信度。"}}
{"id": "2512.16055", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16055", "abs": "https://arxiv.org/abs/2512.16055", "authors": ["Jiaheng Geng", "Jiatong Du", "Xinyu Zhang", "Ye Li", "Panqu Wang", "Yanjun Huang"], "title": "Driving in Corner Case: A Real-World Adversarial Closed-Loop Evaluation Platform for End-to-End Autonomous Driving", "comment": null, "summary": "Safety-critical corner cases, difficult to collect in the real world, are crucial for evaluating end-to-end autonomous driving. Adversarial interaction is an effective method to generate such safety-critical corner cases. While existing adversarial evaluation methods are built for models operating in simplified simulation environments, adversarial evaluation for real-world end-to-end autonomous driving has been little explored. To address this challenge, we propose a closed-loop evaluation platform for end-to-end autonomous driving, which can generate adversarial interactions in real-world scenes. In our platform, the real-world image generator cooperates with an adversarial traffic policy to evaluate various end-to-end models trained on real-world data. The generator, based on flow matching, efficiently and stably generates real-world images according to the traffic environment information. The efficient adversarial surrounding vehicle policy is designed to model challenging interactions and create corner cases that current autonomous driving systems struggle to handle. Experimental results demonstrate that the platform can generate realistic driving images efficiently. Through evaluating the end-to-end models such as UniAD and VAD, we demonstrate that based on the adversarial policy, our platform evaluates the performance degradation of the tested model in corner cases. This result indicates that this platform can effectively detect the model's potential issues, which will facilitate the safety and robustness of end-to-end autonomous driving.", "AI": {"tldr": "提出了一个用于端到端自动驾驶的闭环评估平台，通过对抗性交互生成真实世界中的安全关键极端场景，以评估模型在复杂情况下的性能表现。", "motivation": "现有对抗性评估方法主要针对简化仿真环境中的模型，而真实世界端到端自动驾驶的对抗性评估研究较少，需要有效生成安全关键极端场景来评估模型安全性。", "method": "构建包含基于流匹配的真实世界图像生成器和对抗性交通策略的闭环平台，前者根据交通环境信息高效生成真实图像，后者设计对抗性周围车辆策略来创建具有挑战性的交互场景。", "result": "平台能够高效生成逼真的驾驶图像，通过对UniAD和VAD等端到端模型的评估，展示了对抗性策略能够有效检测模型在极端场景下的性能下降问题。", "conclusion": "该平台能够有效识别自动驾驶模型的潜在安全问题，有助于提升端到端自动驾驶系统的安全性和鲁棒性。"}}
{"id": "2512.16449", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16449", "abs": "https://arxiv.org/abs/2512.16449", "authors": ["Abhishek Kashyap", "Yuxuan Yang", "Henrik Andreasson", "Todor Stoyanov"], "title": "Single-View Shape Completion for Robotic Grasping in Clutter", "comment": null, "summary": "In vision-based robot manipulation, a single camera view can only capture one side of objects of interest, with additional occlusions in cluttered scenes further restricting visibility. As a result, the observed geometry is incomplete, and grasp estimation algorithms perform suboptimally. To address this limitation, we leverage diffusion models to perform category-level 3D shape completion from partial depth observations obtained from a single view, reconstructing complete object geometries to provide richer context for grasp planning. Our method focuses on common household items with diverse geometries, generating full 3D shapes that serve as input to downstream grasp inference networks. Unlike prior work, which primarily considers isolated objects or minimal clutter, we evaluate shape completion and grasping in realistic clutter scenarios with household objects. In preliminary evaluations on a cluttered scene, our approach consistently results in better grasp success rates than a naive baseline without shape completion by 23% and over a recent state of the art shape completion approach by 19%. Our code is available at https://amm.aass.oru.se/shape-completion-grasping/.", "AI": {"tldr": "提出一种基于扩散模型的类别级三维形状补全方法，从单视角深度观测中重建完整物体几何，用于提升杂乱场景中的机器人抓取成功率。", "motivation": "单视角相机在机器人操作中只能捕捉物体单侧信息，且杂乱场景中的遮挡会进一步限制可见性，导致几何信息不完整，抓取算法性能下降。", "method": "利用扩散模型从单视角部分深度观测中完成类别级三维形状补全，重建完整物体几何，为抓取规划提供更丰富上下文信息。专注于常见家居物品，生成完整三维形状作为下游抓取推理网络的输入。", "result": "在杂乱场景的初步评估中，该方法比未使用形状补全的基线抓取成功率提高23%，比近期先进形状补全方法提高19%。", "conclusion": "通过扩散模型进行三维形状补全能有效提升杂乱场景中机器人抓取的成功率，为实际家居环境中的机器人操作提供了更可靠的几何上下文。"}}
{"id": "2512.15943", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.15943", "abs": "https://arxiv.org/abs/2512.15943", "authors": ["Polaris Jhandi", "Owais Kazi", "Shreyas Subramanian", "Neel Sendas"], "title": "Small Language Models for Efficient Agentic Tool Calling: Outperforming Large Models with Targeted Fine-tuning", "comment": null, "summary": "As organizations scale adoption of generative AI, model cost optimization and operational efficiency have emerged as critical factors determining sustainability and accessibility. While Large Language Models (LLMs) demonstrate impressive capabilities across diverse tasks, their extensive computational requirements make them cost-prohibitive for routine enterprise use. This limitation motivates the exploration of Small Language Models (SLMs), which can deliver comparable performance in targeted applications while drastically reducing infrastructure overhead (Irugalbandara et al., 2023). In this work, we investigate the feasibility of replacing LLM-driven workflows with optimized SLMs. We trained a domain-adapted SLM to execute representative tasks traditionally handled by LLMs, such as document summarization, query answering, and structured data interpretation. As part of the experiment, we investigated the fine-tuning of facebook/opt-350m model (single epoch only) using the Hugging Face TRL (Transformer Reinforcement Learning), specifically the Supervised Fine-Tuning (SFT) trainer. The OPT-350M model was released by Meta AI in 2022 as part of the OPT (Open Pretrained Transformer) family of models. Similar studies demonstrate that even models at the 350M parameter scale can meaningfully contribute to instruction-tuning pipelines (Mekala et al., 2024). Experimental results demonstrated that our fine-tuned SLM achieves exceptional performance with a 77.55\\% pass rate on ToolBench evaluation, significantly outperforming all baseline models including ChatGPT-CoT (26.00\\%), ToolLLaMA-DFS (30.18\\%), and ToolLLaMA-CoT (16.27\\%). These findings emphasize that thoughtful design and targeted training of SLMs can significantly lower barriers to adoption, enabling cost-effective, large-scale integration of generative AI into production systems.", "AI": {"tldr": "研究探索用优化的小型语言模型（SLM）替代大型语言模型（LLM）驱动的工作流，通过领域适应的微调，在特定任务上实现高性能且大幅降低成本。", "motivation": "随着生成式AI的规模化应用，模型成本优化和运行效率成为可持续性和可访问性的关键因素。LLM虽然能力强，但计算需求大、成本高，限制了企业日常使用，因此需要探索更高效的SLM解决方案。", "method": "使用Hugging Face TRL中的监督微调（SFT）方法，对facebook/opt-350m模型进行单轮微调，训练其执行文档摘要、问答和结构化数据解释等任务。", "result": "微调后的SLM在ToolBench评估中取得了77.55%的通过率，显著优于ChatGPT-CoT（26.00%）、ToolLLaMA-DFS（30.18%）和ToolLLaMA-CoT（16.27%）等基线模型。", "conclusion": "精心设计和针对性训练的SLM能够显著降低生成式AI的采用门槛，实现成本效益高的大规模生产系统集成，为替代LLM驱动的工作流提供了可行性。"}}
{"id": "2512.16736", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16736", "abs": "https://arxiv.org/abs/2512.16736", "authors": ["Xiaofeng Zong", "Ming-Yu Wang", "Jimin Wang", "Ji-Feng Zhang"], "title": "Observer-based Differentially Private Consensus for Linear Multi-agent Systems", "comment": null, "summary": "This paper investigates the differentially private consensus problem for general linear multi-agent systems (MASs) based on output feedback protocols. To protect the output information, which is considered private data and may be at high risk of exposure, Laplace noise is added to the information exchange. The conditions for achieving mean square and almost sure consensus in observer-based MASs are established using the backstepping method and the convergence theory for nonnegative almost supermartingales. It is shown that the separation principle remains valid for the consensus problem of linear MASs with decaying Laplace noise. Furthermore, the convergence rate is provided. Then, a joint design framework is developed for state estimation gain, feedback control gain, and noise to ensure the preservation of ε-differential privacy. The output information of each agent is shown to be protected at every time step. Finally, sufficient conditions are established for simultaneously achieving consensus and preserving differential privacy for linear MASs utilizing both full-order and reduced-order observers. Meanwhile, an ε*-differentially private consensus is achieved to meet the desired privacy level. Two simulation examples are provided to validate the theoretical results.", "AI": {"tldr": "本文研究了基于输出反馈协议的线性多智能体系统差分隐私共识问题，通过添加拉普拉斯噪声保护输出信息，建立了实现均方和几乎必然共识的条件，并提出了同时保证共识和差分隐私的联合设计框架。", "motivation": "多智能体系统的输出信息作为隐私数据存在泄露风险，需要在不影响系统共识性能的前提下实现隐私保护。", "method": "采用反步法和非负几乎超鞅收敛理论，在输出反馈协议中添加拉普拉斯噪声，建立观测器基多智能体系统的共识条件，并提出状态估计增益、反馈控制增益和噪声的联合设计框架。", "result": "证明了线性多智能体系统在衰减拉普拉斯噪声下分离原理仍然有效，给出了收敛速率，实现了每个时间步的输出信息保护，同时为全阶和降阶观测器提供了同时达成共识和保持差分隐私的充分条件。", "conclusion": "所提方法能有效实现ε-差分隐私保护下的多智能体系统共识，并通过ε*-差分隐私共识满足特定隐私要求，仿真验证了理论结果的有效性。"}}
{"id": "2512.16075", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16075", "abs": "https://arxiv.org/abs/2512.16075", "authors": ["Hao Tang", "Hanyu Liu", "Alessandro Perelli", "Xi Chen", "Chao Li"], "title": "FOD-Diff: 3D Multi-Channel Patch Diffusion Model for Fiber Orientation Distribution", "comment": null, "summary": "Diffusion MRI (dMRI) is a critical non-invasive technique to estimate fiber orientation distribution (FOD) for characterizing white matter integrity. Estimating FOD from single-shell low angular resolution dMRI (LAR-FOD) is limited by accuracy, whereas estimating FOD from multi-shell high angular resolution dMRI (HAR-FOD) requires a long scanning time, which limits its applicability. Diffusion models have shown promise in estimating HAR-FOD based on LAR-FOD. However, using diffusion models to efficiently generate HAR-FOD is challenging due to the large number of spherical harmonic (SH) coefficients in FOD. Here, we propose a 3D multi-channel patch diffusion model to predict HAR-FOD from LAR-FOD. We design the FOD-patch adapter by introducing the prior brain anatomy for more efficient patch-based learning. Furthermore, we introduce a voxel-level conditional coordinating module to enhance the global understanding of the model. We design the SH attention module to effectively learn the complex correlations of the SH coefficients. Our experimental results show that our method achieves the best performance in HAR-FOD prediction and outperforms other state-of-the-art methods.", "AI": {"tldr": "提出一种3D多通道补丁扩散模型，从低角分辨率扩散MRI预测高角分辨率纤维取向分布，通过引入脑解剖先验和球谐系数注意力机制提升性能。", "motivation": "单壳层低角分辨率dMRI估算纤维取向分布精度有限，而多壳层高角分辨率dMRI需要长扫描时间，限制了临床应用。现有扩散模型在高效生成高分辨率FOD时面临球谐系数数量庞大的挑战。", "method": "1. 设计3D多通道补丁扩散模型；2. 引入脑解剖先验的FOD补丁适配器；3. 体素级条件协调模块增强全局理解；4. 球谐注意力模块学习复杂系数相关性。", "result": "实验表明该方法在高分辨率FOD预测中取得最佳性能，优于其他先进方法。", "conclusion": "所提出的扩散模型能有效从低分辨率数据生成高质量纤维取向分布，为临床提供更高效的dMRI分析方案。"}}
{"id": "2512.16454", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16454", "abs": "https://arxiv.org/abs/2512.16454", "authors": ["Tianhao Shao", "Kaixing Zhao", "Feng Liu", "Lixin Yang", "Bin Guo"], "title": "AG-MPBS: a Mobility-Aware Prediction and Behavior-Based Scheduling Framework for Air-Ground Unmanned Systems", "comment": null, "summary": "As unmanned systems such as Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) become increasingly important to applications like urban sensing and emergency response, efficiently recruiting these autonomous devices to perform time-sensitive tasks has become a critical challenge. This paper presents MPBS (Mobility-aware Prediction and Behavior-based Scheduling), a scalable task recruitment framework that treats each device as a recruitable \"user\". MPBS integrates three key modules: a behavior-aware KNN classifier, a time-varying Markov prediction model for forecasting device mobility, and a dynamic priority scheduling mechanism that considers task urgency and base station performance. By combining behavioral classification with spatiotemporal prediction, MPBS adaptively assigns tasks to the most suitable devices in real time. Experimental evaluations on the real-world GeoLife dataset show that MPBS significantly improves task completion efficiency and resource utilization. The proposed framework offers a predictive, behavior-aware solution for intelligent and collaborative scheduling in unmanned systems.", "AI": {"tldr": "提出MPBS框架，通过行为分类和时空预测为无人机/无人车等无人系统实时分配合适的任务，提高任务完成效率和资源利用率。", "motivation": "随着无人机和无人车在城市场景感知和应急响应等应用中日益重要，如何高效招募这些自主设备执行时效性任务成为关键挑战。", "method": "MPBS框架包含三个模块：行为感知KNN分类器、时变马尔可夫移动性预测模型、考虑任务紧急性和基站性能的动态优先级调度机制。", "result": "在真实GeoLife数据集上的实验表明，MPBS显著提升了任务完成效率和资源利用率。", "conclusion": "该框架为无人系统提供了预测性、行为感知的智能协同调度解决方案。"}}
{"id": "2512.16555", "categories": ["cs.RO", "cs.FL", "cs.MA", "eess.SY"], "pdf": "https://arxiv.org/pdf/2512.16555", "abs": "https://arxiv.org/abs/2512.16555", "authors": ["Marcelo Rosa", "José E. R. Cury", "Fabio L. Baldissera"], "title": "A Formal Modular Synthesis Approach for the Coordination of 3-D Robotic Construction with Multi-robots", "comment": null, "summary": "In this paper, we deal with the problem of coordinating multiple robots to build 3-D structures. This problem consists of a set of mobile robots that interact with each other in order to autonomously build a predefined 3-D structure. Our approach is based on Supervisory Control Theory, and it allows us to synthesize from models that represent a single robot and the target structure a correct-by-construction reactive controller, called supervisor. When this supervisor is replicated for the other robots, then the target structure can be completed by all robots", "AI": {"tldr": "本文提出了一种基于监督控制理论的方法，用于协调多个机器人自主构建预定义的三维结构，通过合成正确构造的监督控制器实现多机器人协作。", "motivation": "研究旨在解决多机器人协同构建三维结构的协调问题，以实现自主、高效的群体协作建造任务。", "method": "采用监督控制理论，从单个机器人和目标结构的模型中合成正确构造的监督控制器，并将该控制器复制到所有机器人上。", "result": "该方法能够生成可复制的监督控制器，使所有机器人能够协作完成目标三维结构的构建。", "conclusion": "基于监督控制理论的方法为多机器人协同构建三维结构提供了一种正确构造的控制器合成方案，实现了自主协调建造。"}}
{"id": "2512.15948", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.15948", "abs": "https://arxiv.org/abs/2512.15948", "authors": ["Samuel J. Gershman"], "title": "Subjective functions", "comment": null, "summary": "Where do objective functions come from? How do we select what goals to pursue? Human intelligence is adept at synthesizing new objective functions on the fly. How does this work, and can we endow artificial systems with the same ability? This paper proposes an approach to answering these questions, starting with the concept of a subjective function, a higher-order objective function that is endogenous to the agent (i.e., defined with respect to the agent's features, rather than an external task). Expected prediction error is studied as a concrete example of a subjective function. This proposal has many connections to ideas in psychology, neuroscience, and machine learning.", "AI": {"tldr": "本文提出主观函数概念作为高阶目标函数，研究预期预测误差作为具体示例，探索智能体如何内生地合成目标函数。", "motivation": "人类智能能够动态合成新目标函数，而现有AI系统缺乏这种能力。研究旨在理解目标函数的起源和选择机制，使人工系统具备类似能力。", "method": "提出主观函数概念（内生于智能体的高阶目标函数），以预期预测误差为具体案例进行研究，连接心理学、神经科学和机器学习相关理论。", "result": "建立了主观函数的理论框架，展示了预期预测误差作为主观函数的具体实现方式，揭示了目标函数合成的可能机制。", "conclusion": "主观函数为理解目标函数起源提供了新视角，预期预测误差作为实例展示了内源性目标合成的可行性，为赋予AI系统动态目标合成能力奠定了基础。"}}
{"id": "2512.16229", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16229", "abs": "https://arxiv.org/abs/2512.16229", "authors": ["Chenkai Xu", "Yijie Jin", "Jiajun Li", "Yi Tu", "Guoping Long", "Dandan Tu", "Tianqi Hou", "Junchi Yan", "Zhijie Deng"], "title": "LoPA: Scaling dLLM Inference via Lookahead Parallel Decoding", "comment": null, "summary": "Diffusion Large Language Models (dLLMs) have demonstrated significant potential for high-speed inference. However, current confidence-driven decoding strategies are constrained by limited parallelism, typically achieving only 1--3 tokens per forward pass (TPF). In this work, we identify that the degree of parallelism during dLLM inference is highly sensitive to the Token Filling Order (TFO). Then, we introduce Lookahead PArallel Decoding LoPA, a training-free, plug-and-play algorithm, to identify a superior TFO and hence accelerate inference. LoPA concurrently explores distinct candidate TFOs via parallel branches, and selects the one with the highest potential for future parallelism based on branch confidence. We apply LoPA to the state-of-the-art D2F model and observe a substantial enhancement in decoding efficiency. Notably, LoPA increases the TPF of D2F-Dream to 10.1 on the GSM8K while maintaining performance superior to the Dream baseline. Furthermore, to facilitate this unprecedented degree of parallelism, we develop a specialized multi-device inference system featuring Branch Parallelism (BP), which achieves a single-sample throughput of 1073.9 tokens per second under multi-GPU deployment. The code is available at https://github.com/zhijie-group/LoPA.", "AI": {"tldr": "提出LoPA算法，通过优化令牌填充顺序提高扩散大语言模型的并行推理速度，使D2F-Dream模型在GSM8K数据集上达到10.1 TPF，并开发支持分支并行的多设备推理系统。", "motivation": "现有扩散大语言模型的置信度驱动解码策略并行度有限（通常仅1-3 TPF），限制了推理速度的提升潜力。", "method": "1. 提出无需训练即插即用的LoPA算法，通过并行分支探索不同令牌填充顺序候选；2. 根据分支置信度选择未来并行潜力最高的顺序；3. 开发支持分支并行的多GPU推理系统。", "result": "1. 在D2F-Dream模型上实现GSM8K数据集10.1 TPF，性能优于原基线；2. 多GPU部署下单样本吞吐量达1073.9 tokens/秒。", "conclusion": "令牌填充顺序对扩散大语言模型推理并行度具有关键影响，LoPA算法能显著提升解码效率，其配套推理系统为高并行度推理提供了可行方案。"}}
{"id": "2512.16469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16469", "abs": "https://arxiv.org/abs/2512.16469", "authors": ["Jiayu Zhang", "Kaixing Zhao", "Tianhao Shao", "Bin Guo", "Liang He"], "title": "Tri-Select: A Multi-Stage Visual Data Selection Framework for Mobile Visual Crowdsensing", "comment": null, "summary": "Mobile visual crowdsensing enables large-scale, fine-grained environmental monitoring through the collection of images from distributed mobile devices. However, the resulting data is often redundant and heterogeneous due to overlapping acquisition perspectives, varying resolutions, and diverse user behaviors. To address these challenges, this paper proposes Tri-Select, a multi-stage visual data selection framework that efficiently filters redundant and low-quality images. Tri-Select operates in three stages: (1) metadata-based filtering to discard irrelevant samples; (2) spatial similarity-based spectral clustering to organize candidate images; and (3) a visual-feature-guided selection based on maximum independent set search to retain high-quality, representative images. Experiments on real-world and public datasets demonstrate that Tri-Select improves both selection efficiency and dataset quality, making it well-suited for scalable crowdsensing applications.", "AI": {"tldr": "提出Tri-Select多阶段视觉数据选择框架，用于高效过滤移动视觉群智感知中的冗余和低质量图像，提升数据集质量和处理效率。", "motivation": "移动视觉群智感知采集的图像存在冗余性高、异质性强的问题（如视角重叠、分辨率差异、用户行为多样），影响环境监测的效率和精度。", "method": "采用三阶段框架：1) 基于元数据过滤无关样本；2) 基于空间相似性的谱聚类组织候选图像；3) 基于视觉特征的最大独立集搜索选择高质量代表性图像。", "result": "在真实世界和公开数据集上的实验表明，Tri-Select能显著提升数据选择效率和数据集质量。", "conclusion": "该框架适用于可扩展的群智感知应用，能有效解决视觉数据冗余和异质性问题。"}}
{"id": "2512.16077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16077", "abs": "https://arxiv.org/abs/2512.16077", "authors": ["Haomeng Zhang", "Kuan-Chuan Peng", "Suhas Lohit", "Raymond A. Yeh"], "title": "Auto-Vocabulary 3D Object Detection", "comment": "technical report", "summary": "Open-vocabulary 3D object detection methods are able to localize 3D boxes of classes unseen during training. Despite the name, existing methods rely on user-specified classes both at training and inference. We propose to study Auto-Vocabulary 3D Object Detection (AV3DOD), where the classes are automatically generated for the detected objects without any user input. To this end, we introduce Semantic Score (SS) to evaluate the quality of the generated class names. We then develop a novel framework, AV3DOD, which leverages 2D vision-language models (VLMs) to generate rich semantic candidates through image captioning, pseudo 3D box generation, and feature-space semantics expansion. AV3DOD achieves the state-of-the-art (SOTA) performance on both localization (mAP) and semantic quality (SS) on the ScanNetV2 and SUNRGB-D datasets. Notably, it surpasses the SOTA, CoDA, by 3.48 overall mAP and attains a 24.5% relative improvement in SS on ScanNetV2.", "AI": {"tldr": "提出Auto-Vocabulary 3D目标检测(AV3DOD)，无需用户指定类别即可自动生成检测对象的类别名称，通过语义评分评估生成质量，在ScanNetV2和SUNRGB-D数据集上取得SOTA性能。", "motivation": "现有开放词汇3D检测方法在训练和推理时仍需用户指定类别，限制了其真正开放词汇的能力。", "method": "提出语义评分(SS)评估生成类别质量；开发AV3DOD框架，利用2D视觉语言模型通过图像描述、伪3D框生成和特征空间语义扩展生成丰富语义候选。", "result": "在ScanNetV2和SUNRGB-D数据集上实现定位(mAP)和语义质量(SS)的SOTA性能；在ScanNetV2上比SOTA方法CoDA提升3.48 mAP，SS相对提升24.5%。", "conclusion": "AV3DOD首次实现了真正的自动词汇3D目标检测，无需用户输入即可生成高质量语义类别，为开放词汇3D感知开辟了新方向。"}}
{"id": "2512.16248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16248", "abs": "https://arxiv.org/abs/2512.16248", "authors": ["Qingguo Hu", "Zhenghao Lin", "Ziyue Yang", "Yucheng Ding", "Xiao Liu", "Yuting Jiang", "Ruizhe Wang", "Tianyu Chen", "Zhongxin Guo", "Yifan Xiong", "Rui Gao", "Lei Qu", "Jinsong Su", "Peng Cheng", "Yeyun Gong"], "title": "Sigma-Moe-Tiny Technical Report", "comment": null, "summary": "Mixture-of-Experts (MoE) has emerged as a promising paradigm for foundation models due to its efficient and powerful scalability. In this work, we present Sigma-MoE-Tiny, an MoE language model that achieves the highest sparsity compared to existing open-source models. Sigma-MoE-Tiny employs fine-grained expert segmentation with up to 96 experts per layer, while activating only one expert for each token, resulting in 20B total parameters with just 0.5B activated. The major challenge introduced by such extreme sparsity lies in expert load balancing. We find that the widely-used load balancing loss tends to become ineffective in the lower layers under this setting. To address this issue, we propose a progressive sparsification schedule aiming to balance expert utilization and training stability. Sigma-MoE-Tiny is pre-trained on a diverse and high-quality corpus, followed by post-training to further unlock its capabilities. The entire training process remains remarkably stable, with no occurrence of irrecoverable loss spikes. Comprehensive evaluations reveal that, despite activating only 0.5B parameters, Sigma-MoE-Tiny achieves top-tier performance among counterparts of comparable or significantly larger scale. In addition, we provide an in-depth discussion of load balancing in highly sparse MoE models, offering insights for advancing sparsity in future MoE architectures.\n  Project page: https://qghuxmu.github.io/Sigma-MoE-Tiny\n  Code: https://github.com/microsoft/ltp-megatron-lm", "AI": {"tldr": "提出Sigma-MoE-Tiny，一种极稀疏的MoE语言模型，通过细粒度专家分割和渐进稀疏化调度实现96专家/层但每令牌仅激活1专家，总参20B激活仅0.5B，性能媲美更大规模模型。", "motivation": "混合专家模型因高效可扩展性成为基础模型的有前景范式，但现有开源模型在稀疏度上仍有提升空间，且极高稀疏度下的专家负载均衡是主要挑战。", "method": "采用细粒度专家分割（每层最多96专家）、每令牌激活单专家策略；提出渐进稀疏化调度以解决深层负载均衡失效问题；使用高质量多样化语料预训练并进行后训练。", "result": "模型总参20B激活仅0.5B，训练全程稳定无不可恢复损失尖峰；在同等或更大规模对比模型中达到顶级性能；深入分析了高稀疏MoE的负载均衡机制。", "conclusion": "Sigma-MoE-Tiny证明了极稀疏MoE架构的可行性，其负载均衡解决方案为未来MoE稀疏化发展提供了重要参考，模型代码已开源。"}}
{"id": "2512.16089", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16089", "abs": "https://arxiv.org/abs/2512.16089", "authors": ["Haopeng Zhao", "Marsha Mariya Kappan", "Mahdi Bamdad", "Francisco Cruz"], "title": "LAPX: Lightweight Hourglass Network with Global Context", "comment": "10 pages", "summary": "Human pose estimation is a crucial task in computer vision. Methods that have SOTA (State-of-the-Art) accuracy, often involve a large number of parameters and incur substantial computational cost. Many lightweight variants have been proposed to reduce the model size and computational cost of them. However, several of these methods still contain components that are not well suited for efficient deployment on edge devices. Moreover, models that primarily emphasize inference speed on edge devices often suffer from limited accuracy due to their overly simplified designs. To address these limitations, we propose LAPX, an Hourglass network with self-attention that captures global contextual information, based on previous work, LAP. In addition to adopting the self-attention module, LAPX advances the stage design and refine the lightweight attention modules. It achieves competitive results on two benchmark datasets, MPII and COCO, with only 2.3M parameters, and demonstrates real-time performance, confirming its edge-device suitability.", "AI": {"tldr": "提出LAPX，一种基于Hourglass网络和自注意力的轻量级人体姿态估计模型，在保持竞争力的同时实现边缘设备实时性能。", "motivation": "现有SOTA姿态估计模型参数量大、计算成本高；轻量级变体要么边缘部署效率不足，要么因过度简化设计导致精度受限。", "method": "基于LAP改进，引入自注意力模块捕获全局上下文信息，优化阶段设计并精炼轻量级注意力模块，构建Hourglass网络架构。", "result": "在MPII和COCO基准测试中取得有竞争力的结果，仅需230万参数，并实现边缘设备实时性能。", "conclusion": "LAPX在模型轻量化与精度间取得良好平衡，验证了其在边缘设备部署的适用性。"}}
{"id": "2512.16022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16022", "abs": "https://arxiv.org/abs/2512.16022", "authors": ["Defu Cao", "Michael Gee", "Jinbo Liu", "Hengxuan Wang", "Wei Yang", "Rui Wang", "Yan Liu"], "title": "Conversational Time Series Foundation Models: Towards Explainable and Effective Forecasting", "comment": "31Pages", "summary": "The proliferation of time series foundation models has created a landscape where no single method achieves consistent superiority, framing the central challenge not as finding the best model, but as orchestrating an optimal ensemble with interpretability. While Large Language Models (LLMs) offer powerful reasoning capabilities, their direct application to time series forecasting has proven ineffective. We address this gap by repositioning the LLM as an intelligent judge that evaluates, explains, and strategically coordinates an ensemble of foundation models. To overcome the LLM's inherent lack of domain-specific knowledge on time series, we introduce an R1-style finetuning process, guided by SHAP-based faithfulness scores, which teaches the model to interpret ensemble weights as meaningful causal statements about temporal dynamics. The trained agent then engages in iterative, multi-turn conversations to perform forward-looking assessments, provide causally-grounded explanations for its weighting decisions, and adaptively refine the optimization strategy. Validated on the GIFT-Eval benchmark on 23 datasets across 97 settings, our approach significantly outperforms leading time series foundation models on both CRPS and MASE metrics, establishing new state-of-the-art results.", "AI": {"tldr": "提出一种利用LLM作为智能协调器来集成多个时间序列基础模型的方法，通过R1微调和SHAP引导提升模型对时序动态的因果解释能力，在多个基准测试中达到SOTA性能。", "motivation": "现有时间序列基础模型缺乏一致最优解，且LLM直接用于时序预测效果不佳，但LLM的推理能力可用于协调模型集成并提升可解释性。", "method": "将LLM重新定位为评估与协调基础模型的智能体，采用R1风格微调结合SHAP忠实度评分，使LLM能将集成权重解释为时序因果陈述，并通过多轮对话迭代优化策略。", "result": "在GIFT-Eval基准的23个数据集、97种设置中，CRPS和MASE指标显著优于现有时间序列基础模型，取得新的SOTA结果。", "conclusion": "LLM作为智能协调器能有效集成时间序列基础模型，其因果解释能力提升了集成策略的可解释性与性能，为时序预测提供了新范式。"}}
{"id": "2512.16287", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16287", "abs": "https://arxiv.org/abs/2512.16287", "authors": ["Yehor Tereshchenko", "Mika Hämäläinen", "Svitlana Myroniuk"], "title": "Evaluating OpenAI GPT Models for Translation of Endangered Uralic Languages: A Comparison of Reasoning and Non-Reasoning Architectures", "comment": "IWCLUL 2025", "summary": "The evaluation of Large Language Models (LLMs) for translation tasks has primarily focused on high-resource languages, leaving a significant gap in understanding their performance on low-resource and endangered languages. This study presents a comprehensive comparison of OpenAI's GPT models, specifically examining the differences between reasoning and non-reasoning architectures for translating between Finnish and four low-resource Uralic languages: Komi-Zyrian, Moksha, Erzya, and Udmurt. Using a parallel corpus of literary texts, we evaluate model willingness to attempt translation through refusal rate analysis across different model architectures. Our findings reveal significant performance variations between reasoning and non-reasoning models, with reasoning models showing 16 percentage points lower refusal rates. The results provide valuable insights for researchers and practitioners working with Uralic languages and contribute to the broader understanding of reasoning model capabilities for endangered language preservation.", "AI": {"tldr": "本研究比较了OpenAI GPT模型在芬兰语与四种低资源乌拉尔语翻译任务中的表现，发现推理模型比非推理模型的拒绝翻译率低16个百分点。", "motivation": "现有大语言模型翻译评估主要关注高资源语言，对低资源和濒危语言的性能了解不足，需要填补这一研究空白。", "method": "使用文学文本平行语料库，分析不同模型架构（推理vs非推理）在翻译芬兰语与四种乌拉尔语时的拒绝率。", "result": "推理模型在翻译低资源语言时表现显著优于非推理模型，拒绝翻译率降低16个百分点。", "conclusion": "研究为乌拉尔语研究者和实践者提供了重要参考，并证明推理模型在濒危语言保护方面具有独特优势。"}}
{"id": "2512.16092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16092", "abs": "https://arxiv.org/abs/2512.16092", "authors": ["Zibin Liu", "Shunkun Liang", "Banglei Guan", "Dongcai Tan", "Yang Shang", "Qifeng Yu"], "title": "Collimator-assisted high-precision calibration method for event cameras", "comment": "4 pages, 3 figures", "summary": "Event cameras are a new type of brain-inspired visual sensor with advantages such as high dynamic range and high temporal resolution. The geometric calibration of event cameras, which involves determining their intrinsic and extrinsic parameters, particularly in long-range measurement scenarios, remains a significant challenge. To address the dual requirements of long-distance and high-precision measurement, we propose an event camera calibration method utilizing a collimator with flickering star-based patterns. The proposed method first linearly solves camera parameters using the sphere motion model of the collimator, followed by nonlinear optimization to refine these parameters with high precision. Through comprehensive real-world experiments across varying conditions, we demonstrate that the proposed method consistently outperforms existing event camera calibration methods in terms of accuracy and reliability.", "AI": {"tldr": "提出了一种使用闪烁星点图案准直仪的事件相机标定方法，用于解决远距离高精度测量中的几何标定难题。", "motivation": "事件相机具有高动态范围和高时间分辨率等优势，但其在远距离测量场景下的几何标定（包括内参和外参）仍面临重大挑战。", "method": "首先利用准直仪的球面运动模型线性求解相机参数，然后通过非线性优化对这些参数进行高精度细化。", "result": "在不同条件下的综合真实世界实验中，该方法在准确性和可靠性方面始终优于现有的事件相机标定方法。", "conclusion": "所提出的方法有效解决了事件相机在远距离高精度测量中的标定问题，显著提升了标定性能。"}}
{"id": "2512.16705", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16705", "abs": "https://arxiv.org/abs/2512.16705", "authors": ["David Müller", "Espen Knoop", "Dario Mylonopoulos", "Agon Serifi", "Michael A. Hopkins", "Ruben Grandia", "Moritz Bächer"], "title": "Olaf: Bringing an Animated Character to Life in the Physical World", "comment": null, "summary": "Animated characters often move in non-physical ways and have proportions that are far from a typical walking robot. This provides an ideal platform for innovation in both mechanical design and stylized motion control. In this paper, we bring Olaf to life in the physical world, relying on reinforcement learning guided by animation references for control. To create the illusion of Olaf's feet moving along his body, we hide two asymmetric legs under a soft foam skirt. To fit actuators inside the character, we use spherical and planar linkages in the arms, mouth, and eyes. Because the walk cycle results in harsh contact sounds, we introduce additional rewards that noticeably reduce impact noise. The large head, driven by small actuators in the character's slim neck, creates a risk of overheating, amplified by the costume. To keep actuators from overheating, we feed temperature values as additional inputs to policies, introducing new rewards to keep them within bounds. We validate the efficacy of our modeling in simulation and on hardware, demonstrating an unmatched level of believability for a costumed robotic character.", "AI": {"tldr": "将动画角色奥拉夫物理化，通过强化学习控制、隐藏式机械结构和温度管理，实现高度逼真的拟人机器人。", "motivation": "动画角色通常具有非物理的运动方式和夸张的身体比例，这为机械设计和风格化运动控制提供了创新平台。", "method": "使用动画参考引导的强化学习进行控制；设计隐藏式不对称腿部结构；采用球形和平面的连杆机构；引入减少冲击噪声的奖励函数；将温度作为策略输入并添加温度约束奖励。", "result": "在仿真和硬件上验证了模型的有效性，实现了服装机器人角色前所未有的逼真度。", "conclusion": "通过机械设计创新和强化学习控制策略，成功将动画角色物理化，解决了噪声、过热等实际问题，展现了高度可信的机器人表现。"}}
{"id": "2512.16724", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16724", "abs": "https://arxiv.org/abs/2512.16724", "authors": ["Yixiang Chen", "Yan Huang", "Keji He", "Peiyan Li", "Liang Wang"], "title": "VERM: Leveraging Foundation Models to Create a Virtual Eye for Efficient 3D Robotic Manipulation", "comment": "Accepted at RA-L 2025", "summary": "When performing 3D manipulation tasks, robots have to execute action planning based on perceptions from multiple fixed cameras. The multi-camera setup introduces substantial redundancy and irrelevant information, which increases computational costs and forces the model to spend extra training time extracting crucial task-relevant details. To filter out redundant information and accurately extract task-relevant features, we propose the VERM (Virtual Eye for Robotic Manipulation) method, leveraging the knowledge in foundation models to imagine a virtual task-adaptive view from the constructed 3D point cloud, which efficiently captures necessary information and mitigates occlusion. To facilitate 3D action planning and fine-grained manipulation, we further design a depth-aware module and a dynamic coarse-to-fine procedure. Extensive experimental results on both simulation benchmark RLBench and real-world evaluations demonstrate the effectiveness of our method, surpassing previous state-of-the-art methods while achieving 1.89x speedup in training time and 1.54x speedup in inference speed. More results can be found on our project website at https://verm-ral.github.io .", "AI": {"tldr": "提出VERM方法，通过基础模型从3D点云生成虚拟任务自适应视图，过滤多摄像头冗余信息，提升机器人3D操作任务的效率和性能。", "motivation": "多摄像头设置带来大量冗余和无关信息，增加计算成本并延长训练时间，需要有效提取任务关键特征。", "method": "使用基础模型从3D点云想象虚拟任务自适应视图，设计深度感知模块和动态粗到细处理流程。", "result": "在RLBench仿真基准和真实世界评估中超越先前最优方法，训练时间加速1.89倍，推理速度提升1.54倍。", "conclusion": "VERM方法能有效过滤冗余信息，提高3D操作任务的效率和性能，为机器人多摄像头感知提供了有效解决方案。"}}
{"id": "2512.16093", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16093", "abs": "https://arxiv.org/abs/2512.16093", "authors": ["Jintao Zhang", "Kaiwen Zheng", "Kai Jiang", "Haoxu Wang", "Ion Stoica", "Joseph E. Gonzalez", "Jianfei Chen", "Jun Zhu"], "title": "TurboDiffusion: Accelerating Video Diffusion Models by 100-200 Times", "comment": null, "summary": "We introduce TurboDiffusion, a video generation acceleration framework that can speed up end-to-end diffusion generation by 100-200x while maintaining video quality. TurboDiffusion mainly relies on several components for acceleration: (1) Attention acceleration: TurboDiffusion uses low-bit SageAttention and trainable Sparse-Linear Attention (SLA) to speed up attention computation. (2) Step distillation: TurboDiffusion adopts rCM for efficient step distillation. (3) W8A8 quantization: TurboDiffusion quantizes model parameters and activations to 8 bits to accelerate linear layers and compress the model. In addition, TurboDiffusion incorporates several other engineering optimizations.\n  We conduct experiments on the Wan2.2-I2V-14B-720P, Wan2.1-T2V-1.3B-480P, Wan2.1-T2V-14B-720P, and Wan2.1-T2V-14B-480P models. Experimental results show that TurboDiffusion achieves 100-200x speedup for video generation even on a single RTX 5090 GPU, while maintaining comparable video quality. The GitHub repository, which includes model checkpoints and easy-to-use code, is available at https://github.com/thu-ml/TurboDiffusion.", "AI": {"tldr": "TurboDiffusion是一个视频生成加速框架，通过注意力加速、步数蒸馏和量化等技术，将端到端扩散生成速度提升100-200倍，同时保持视频质量。", "motivation": "扩散模型在视频生成中计算成本高、速度慢，需要高效的加速方案来提升实际应用可行性。", "method": "采用低比特SageAttention和可训练稀疏线性注意力加速注意力计算；使用rCM进行步数蒸馏；对模型参数和激活进行W8A8量化；结合其他工程优化。", "result": "在多个视频生成模型上实验表明，单张RTX 5090 GPU上实现100-200倍加速，视频质量保持可比水平。", "conclusion": "TurboDiffusion有效解决了扩散模型视频生成的速度瓶颈，为高质量实时视频生成提供了实用解决方案。"}}
{"id": "2512.16030", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16030", "abs": "https://arxiv.org/abs/2512.16030", "authors": ["Lukas Nel"], "title": "Do Large Language Models Know What They Don't Know? Kalshibench: A New Benchmark for Evaluating Epistemic Calibration via Prediction Markets", "comment": null, "summary": "A well-calibrated model should express confidence that matches its actual accuracy -- when it claims 80\\% confidence, it should be correct 80\\% of the time. While large language models (LLMs) have achieved remarkable performance across diverse tasks, their epistemic calibration remains poorly understood. We introduce \\textbf{KalshiBench}, a benchmark of 300 prediction market questions from Kalshi, a CFTC-regulated exchange, with verifiable real-world outcomes occurring after model training cutoffs. Unlike traditional benchmarks measuring accuracy on static knowledge, KalshiBench evaluates whether models can appropriately quantify uncertainty about genuinely unknown future events. We evaluate five frontier models -- Claude Opus 4.5, GPT-5.2, DeepSeek-V3.2, Qwen3-235B, and Kimi-K2 -- and find \\textbf{systematic overconfidence across all models}. Even the best-calibrated model (Claude Opus 4.5, ECE=0.120) shows substantial calibration errors, while reasoning-enhanced models like GPT-5.2-XHigh exhibit \\emph{worse} calibration (ECE=0.395) despite comparable accuracy. Critically, only one model achieves a positive Brier Skill Score, indicating most models perform worse than simply predicting base rates. Our findings suggest that scaling and enhanced reasoning do not automatically confer calibration benefits, highlighting epistemic calibration as a distinct capability requiring targeted development.", "AI": {"tldr": "该研究引入KalshiBench基准测试，评估大语言模型对未来未知事件的置信度校准情况，发现所有前沿模型均存在系统性过度自信问题，且增强推理能力反而可能恶化校准效果。", "motivation": "大语言模型在各类任务中表现优异，但其认知校准能力（即模型置信度与实际准确度的匹配程度）尚未得到充分研究。现有基准主要评估静态知识准确性，缺乏对模型在真正未知未来事件上量化不确定性能力的评估。", "method": "构建包含300个预测市场问题的KalshiBench基准（问题结果发生在模型训练截止时间之后），评估Claude Opus 4.5、GPT-5.2、DeepSeek-V3.2、Qwen3-235B和Kimi-K2五个前沿模型的校准表现，使用预期校准误差（ECE）和Brier技能评分等指标。", "result": "所有模型均表现出系统性过度自信；最佳校准模型（Claude Opus 4.5）的ECE为0.120，而增强推理模型（如GPT-5.2-XHigh）校准更差（ECE=0.395）；仅一个模型获得正Brier技能评分，表明大多数模型表现不如简单预测基准率。", "conclusion": "模型规模扩展和推理能力增强不会自动带来校准改进，认知校准是一种需要针对性开发的独立能力，当前前沿模型在量化未来不确定性方面存在显著缺陷。"}}
{"id": "2512.16108", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16108", "abs": "https://arxiv.org/abs/2512.16108", "authors": ["Wendong Bi", "Yirong Mao", "Xianglong Liu", "Kai Tian", "Jian Zhang", "Hanjie Wang", "Wenhui Que"], "title": "WeMusic-Agent: Efficient Conversational Music Recommendation via Knowledge Internalization and Agentic Boundary Learning", "comment": null, "summary": "Personalized music recommendation in conversational scenarios usually requires a deep understanding of user preferences and nuanced musical context, yet existing methods often struggle with balancing specialized domain knowledge and flexible tool integration. This paper proposes WeMusic-Agent, a training framework for efficient LLM-based conversational music recommendation. By integrating the knowledge internalization and agentic boundary learning, the framework aims to teach the model to intelligently decide when to leverage internalized knowledge and when to call specialized tools (e.g., music retrieval APIs, music recommendation systems). Under this framework, we present WeMusic-Agent-M1, an agentic model that internalizes extensive musical knowledge via continued pretraining on 50B music-related corpus while acquiring the ability to invoke external tools when necessary. Additionally, considering the lack of open-source benchmarks for conversational music recommendation, we also construct a benchmark for personalized music recommendations derived from real-world data in WeChat Listen. This benchmark enables comprehensive evaluation across multiple dimensions, including relevance, personalization, and diversity of the recommendations. Experiments on real-world data demonstrate that WeMusic-Agent achieves significant improvements over existing models.", "AI": {"tldr": "提出了WeMusic-Agent框架，通过内部知识内化与外部工具调用的智能平衡，提升对话式音乐推荐的性能，并构建了开源评测基准。", "motivation": "现有对话式音乐推荐方法难以平衡专业领域知识与灵活工具集成，且缺乏开源评测基准。", "method": "结合知识内化与智能体边界学习，通过持续预训练50B音乐语料内化知识，并学习何时调用外部工具（如检索API、推荐系统）。", "result": "在真实数据实验中，WeMusic-Agent在推荐相关性、个性化、多样性等维度显著优于现有模型。", "conclusion": "该框架能有效提升对话式音乐推荐的性能，所构建的评测基准有助于推动领域研究。"}}
{"id": "2512.16323", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16323", "abs": "https://arxiv.org/abs/2512.16323", "authors": ["Hiroyuki Deguchi", "Katsuki Chousa", "Yusuke Sakai"], "title": "Hacking Neural Evaluation Metrics with Single Hub Text", "comment": null, "summary": "Strongly human-correlated evaluation metrics serve as an essential compass for the development and improvement of generation models and must be highly reliable and robust. Recent embedding-based neural text evaluation metrics, such as COMET for translation tasks, are widely used in both research and development fields. However, there is no guarantee that they yield reliable evaluation results due to the black-box nature of neural networks. To raise concerns about the reliability and safety of such metrics, we propose a method for finding a single adversarial text in the discrete space that is consistently evaluated as high-quality, regardless of the test cases, to identify the vulnerabilities in evaluation metrics. The single hub text found with our method achieved 79.1 COMET% and 67.8 COMET% in the WMT'24 English-to-Japanese (En--Ja) and English-to-German (En--De) translation tasks, respectively, outperforming translations generated individually for each source sentence by using M2M100, a general translation model. Furthermore, we also confirmed that the hub text found with our method generalizes across multiple language pairs such as Ja--En and De--En.", "AI": {"tldr": "该研究提出了一种寻找对抗性文本的方法，该文本无论测试用例如何都能被评估为高质量，揭示了基于嵌入的神经文本评估指标（如COMET）的脆弱性。", "motivation": "基于嵌入的神经文本评估指标（如COMET）在生成模型开发中被广泛使用，但其黑盒性质可能导致评估结果不可靠。研究旨在揭示这类指标的脆弱性，以提高其可靠性和安全性。", "method": "提出一种在离散空间中寻找单个对抗性文本（hub text）的方法，该文本能始终被评估为高质量，无论测试用例如何。该方法通过识别评估指标中的漏洞来实现。", "result": "在WMT'24英日（En-Ja）和英德（En-De）翻译任务中，该方法找到的hub文本分别达到79.1%和67.8%的COMET评分，超过了通用翻译模型M2M100为每个源句子单独生成的翻译。此外，该方法发现的hub文本在多个语言对（如Ja-En和De-En）中具有泛化性。", "conclusion": "基于嵌入的神经文本评估指标存在脆弱性，可能产生不可靠的评估结果。研究提出的方法能有效识别这些漏洞，强调了提高评估指标可靠性和安全性的必要性。"}}
{"id": "2512.16036", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16036", "abs": "https://arxiv.org/abs/2512.16036", "authors": ["Diane Myung-kyung Woodbridge", "Allyson Seba", "Freddie Seba", "Aydin Schwartz"], "title": "Topic Discovery and Classification for Responsible Generative AI Adaptation in Higher Education", "comment": null, "summary": "As generative artificial intelligence (GenAI) becomes increasingly capable of delivering personalized learning experiences and real-time feedback, a growing number of students are incorporating these tools into their academic workflows. They use GenAI to clarify concepts, solve complex problems, and, in some cases, complete assignments by copying and pasting model-generated contents. While GenAI has the potential to enhance learning experience, it also raises concerns around misinformation, hallucinated outputs, and its potential to undermine critical thinking and problem-solving skills. In response, many universities, colleges, departments, and instructors have begun to develop and adopt policies to guide responsible integration of GenAI into learning environments. However, these policies vary widely across institutions and contexts, and their evolving nature often leaves students uncertain about expectations and best practices. To address this challenge, the authors designed and implemented an automated system for discovering and categorizing AI-related policies found in course syllabi and institutional policy websites. The system combines unsupervised topic modeling techniques to identify key policy themes with large language models (LLMs) to classify the level of GenAI allowance and other requirements in policy texts. The developed application achieved a coherence score of 0.73 for topic discovery. In addition, GPT-4.0-based classification of policy categories achieved precision between 0.92 and 0.97, and recall between 0.85 and 0.97 across eight identified topics. By providing structured and interpretable policy information, this tool promotes the safe, equitable, and pedagogically aligned use of GenAI technologies in education. Furthermore, the system can be integrated into educational technology platforms to help students understand and comply with relevant guidelines.", "AI": {"tldr": "开发了一个自动化系统，用于发现和分类课程大纲与机构政策网站中的AI相关政策，结合无监督主题建模和LLMs分类，以促进教育中GenAI的安全、公平和教学对齐使用。", "motivation": "随着GenAI在教育中的普及，学生使用AI工具学习或完成作业，但存在错误信息、幻觉输出及削弱批判性思维的风险。各教育机构的AI政策差异大且不断演变，导致学生不确定期望和最佳实践，需要系统化政策分析工具。", "method": "设计自动化系统，结合无监督主题建模技术识别关键政策主题，并使用大语言模型（如GPT-4.0）对政策文本中的GenAI允许程度和其他要求进行分类。", "result": "系统主题发现的一致性得分为0.73；基于GPT-4.0的政策分类在八个已识别主题中，精确度介于0.92至0.97，召回率介于0.85至0.97。", "conclusion": "该工具通过提供结构化和可解释的政策信息，促进了GenAI在教育中的安全、公平和教学对齐使用，并可集成到教育技术平台中，帮助学生理解和遵守相关指南。"}}
{"id": "2512.16113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16113", "abs": "https://arxiv.org/abs/2512.16113", "authors": ["Shunkun Liang", "Banglei Guan", "Zhenbao Yu", "Dongcai Tan", "Pengju Sun", "Zibin Liu", "Qifeng Yu", "Yang Shang"], "title": "Flexible Camera Calibration using a Collimator System", "comment": null, "summary": "Camera calibration is a crucial step in photogrammetry and 3D vision applications. This paper introduces a novel camera calibration method using a designed collimator system. Our collimator system provides a reliable and controllable calibration environment for the camera. Exploiting the unique optical geometry property of our collimator system, we introduce an angle invariance constraint and further prove that the relative motion between the calibration target and camera conforms to a spherical motion model. This constraint reduces the original 6DOF relative motion between target and camera to a 3DOF pure rotation motion. Using spherical motion constraint, a closed-form linear solver for multiple images and a minimal solver for two images are proposed for camera calibration. Furthermore, we propose a single collimator image calibration algorithm based on the angle invariance constraint. This algorithm eliminates the requirement for camera motion, providing a novel solution for flexible and fast calibration. The performance of our method is evaluated in both synthetic and real-world experiments, which verify the feasibility of calibration using the collimator system and demonstrate that our method is superior to existing baseline methods. Demo code is available at https://github.com/LiangSK98/CollimatorCalibration", "AI": {"tldr": "提出了一种使用准直仪系统的新型相机标定方法，通过角度不变性约束将标定目标与相机之间的相对运动简化为纯旋转运动，实现了更灵活快速的标定。", "motivation": "相机标定是摄影测量和3D视觉应用中的关键步骤，需要更可靠、可控且灵活的标定方法。", "method": "设计了准直仪系统，利用其光学几何特性引入角度不变性约束，将6自由度相对运动简化为3自由度纯旋转运动，提出了多图像闭式线性求解器、双图像最小求解器以及单准直仪图像标定算法。", "result": "合成和真实世界实验验证了准直仪系统标定的可行性，该方法在性能上优于现有基线方法。", "conclusion": "基于准直仪系统的相机标定方法提供了一种灵活、快速且可靠的标定解决方案，无需相机运动即可实现标定。"}}
{"id": "2512.16401", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16401", "abs": "https://arxiv.org/abs/2512.16401", "authors": ["Darshil Chauhan", "Adityasinh Solanki", "Vansh Patel", "Kanav Kapoor", "Ritvik Jain", "Aditya Bansal", "Dhruv Kumar", "Prateek Narang"], "title": "Bridging the Reality Gap: Efficient Adaptation of ASR systems for Challenging Low-Resource Domains", "comment": null, "summary": "Automatic Speech Recognition (ASR) holds immense potential to streamline clinical documentation, such as digitizing handwritten prescriptions and reports, thereby increasing patient throughput and reducing costs in resource-constrained sectors like rural healthcare. However, realizing this utility is currently obstructed by significant technical barriers: strict data privacy constraints, limited computational resources, and severe acoustic domain shifts. We quantify this gap by showing that a robust multilingual model (IndicWav2Vec) degrades to a stark 40.94% Word Error Rate (WER) when deployed on real-world clinical audio (Gram Vaani), rendering it unusable for practical applications. To address these challenges and bring ASR closer to deployment, we propose an efficient, privacy-preserving adaptation framework. We employ Low-Rank Adaptation (LoRA) to enable continual learning from incoming data streams directly on edge devices, ensuring patient data confidentiality. Our strategy yields a 17.1% relative improvement in WER on the target domain. Furthermore, by integrating multi-domain experience replay, we reduce catastrophic forgetting by 47% compared to naive adaptation. These results demonstrate a viable pathway for building reliable, self-improving ASR systems that can operate effectively within the constraints of high-impact real-world environments.", "AI": {"tldr": "提出一种高效、保护隐私的自动语音识别适应框架，通过低秩适应和跨域经验回放，在临床音频数据上显著降低词错误率并减少灾难性遗忘。", "motivation": "自动语音识别在临床文档处理中潜力巨大，但面临数据隐私限制、计算资源有限和声学领域偏移等技术障碍，现有模型在真实临床音频上词错误率高达40.94%，无法实际应用。", "method": "采用低秩适应实现边缘设备上的持续学习，确保数据隐私；结合跨域经验回放策略减少灾难性遗忘。", "result": "目标领域词错误率相对改善17.1%；与简单适应相比，灾难性遗忘减少47%。", "conclusion": "该框架为在高影响力现实环境中构建可靠、自我改进的自动语音识别系统提供了可行路径。"}}
{"id": "2512.16378", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.16378", "abs": "https://arxiv.org/abs/2512.16378", "authors": ["Sara Papi", "Javier Garcia Gilabert", "Zachary Hopton", "Vilém Zouhar", "Carlos Escolano", "Gerard I. Gállego", "Jorge Iranzo-Sánchez", "Ahrii Kim", "Dominik Macháček", "Patricia Schmidtova", "Maike Züfle"], "title": "Hearing to Translate: The Effectiveness of Speech Modality Integration into LLMs", "comment": "Project available at https://github.com/sarapapi/hearing2translate", "summary": "As Large Language Models (LLMs) expand beyond text, integrating speech as a native modality has given rise to SpeechLLMs, which aim to translate spoken language directly, thereby bypassing traditional transcription-based pipelines. Whether this integration improves speech-to-text translation quality over established cascaded architectures, however, remains an open question. We present Hearing to Translate, the first comprehensive test suite rigorously benchmarking 5 state-of-the-art SpeechLLMs against 16 strong direct and cascade systems that couple leading speech foundation models (SFM), with multilingual LLMs. Our analysis spans 16 benchmarks, 13 language pairs, and 9 challenging conditions, including disfluent, noisy, and long-form speech. Across this extensive evaluation, we find that cascaded systems remain the most reliable overall, while current SpeechLLMs only match cascades in selected settings and SFMs lag behind both, highlighting that integrating an LLM, either within the model or in a pipeline, is essential for high-quality speech translation.", "AI": {"tldr": "本文通过全面测试套件比较了5种SpeechLLMs与16种级联系统在语音翻译任务上的性能，发现当前级联系统仍最可靠，SpeechLLMs仅在特定场景下表现相当，而纯语音基础模型落后于两者。", "motivation": "随着大语言模型扩展到语音领域，SpeechLLMs旨在直接翻译口语，但尚不清楚其是否优于传统的级联架构（语音识别+文本翻译）。本研究旨在系统评估SpeechLLMs与级联系统的性能差异。", "method": "构建了首个综合测试套件Hearing to Translate，在16个基准测试、13种语言对和9种挑战性条件（如不流利、噪声、长语音）下，对比了5种先进SpeechLLMs与16种直接/级联系统（结合语音基础模型与多语言LLMs）。", "result": "级联系统整体最可靠；当前SpeechLLMs仅在部分场景中与级联系统性能相当；纯语音基础模型落后于两者。结果表明，整合LLM（无论是模型内还是流水线中）对高质量语音翻译至关重要。", "conclusion": "尽管SpeechLLMs有潜力，但当前级联架构仍是语音翻译的稳健选择；未来工作需提升SpeechLLMs在多样化场景下的性能，并进一步探索LLM与语音模态的深度融合。"}}
{"id": "2512.16760", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16760", "abs": "https://arxiv.org/abs/2512.16760", "authors": ["Tianshuai Hu", "Xiaolu Liu", "Song Wang", "Yiyao Zhu", "Ao Liang", "Lingdong Kong", "Guoyang Zhao", "Zeying Gong", "Jun Cen", "Zhiyu Huang", "Xiaoshuai Hao", "Linfeng Li", "Hang Song", "Xiangtai Li", "Jun Ma", "Shaojie Shen", "Jianke Zhu", "Dacheng Tao", "Ziwei Liu", "Junwei Liang"], "title": "Vision-Language-Action Models for Autonomous Driving: Past, Present, and Future", "comment": "Preprint; 40 pages, 7 figures, 9 tables; GitHub at https://github.com/worldbench/awesome-vla-for-ad", "summary": "Autonomous driving has long relied on modular \"Perception-Decision-Action\" pipelines, where hand-crafted interfaces and rule-based components often break down in complex or long-tailed scenarios. Their cascaded design further propagates perception errors, degrading downstream planning and control. Vision-Action (VA) models address some limitations by learning direct mappings from visual inputs to actions, but they remain opaque, sensitive to distribution shifts, and lack structured reasoning or instruction-following capabilities. Recent progress in Large Language Models (LLMs) and multimodal learning has motivated the emergence of Vision-Language-Action (VLA) frameworks, which integrate perception with language-grounded decision making. By unifying visual understanding, linguistic reasoning, and actionable outputs, VLAs offer a pathway toward more interpretable, generalizable, and human-aligned driving policies. This work provides a structured characterization of the emerging VLA landscape for autonomous driving. We trace the evolution from early VA approaches to modern VLA frameworks and organize existing methods into two principal paradigms: End-to-End VLA, which integrates perception, reasoning, and planning within a single model, and Dual-System VLA, which separates slow deliberation (via VLMs) from fast, safety-critical execution (via planners). Within these paradigms, we further distinguish subclasses such as textual vs. numerical action generators and explicit vs. implicit guidance mechanisms. We also summarize representative datasets and benchmarks for evaluating VLA-based driving systems and highlight key challenges and open directions, including robustness, interpretability, and instruction fidelity. Overall, this work aims to establish a coherent foundation for advancing human-compatible autonomous driving systems.", "AI": {"tldr": "本文系统综述了自动驾驶中视觉-语言-动作（VLA）框架的演进、范式与挑战，旨在建立更可解释、泛化且符合人类意图的驾驶系统基础。", "motivation": "传统模块化驾驶系统在复杂场景中易失效，且感知误差会级联传播；纯视觉-动作模型缺乏可解释性与语言推理能力。大语言模型与多模态学习的发展推动了VLA框架的兴起，以整合感知、语言推理与动作生成。", "method": "通过结构化梳理VLA研究脉络，将现有方法分为两大范式：端到端VLA（单一模型整合感知、推理与规划）与双系统VLA（慢速语言模型推理与快速安全规划分离）。进一步区分了文本/数值动作生成器、显式/隐式引导机制等子类。", "result": "总结了VLA框架的演进路径、代表性数据集与评估基准，并系统比较了不同范式的设计特点与适用场景。", "conclusion": "VLA为自动驾驶提供了更可解释、泛化且人类对齐的潜在路径，但仍面临鲁棒性、可解释性与指令忠实性等挑战，需进一步研究以推动人机兼容的自动驾驶系统发展。"}}
{"id": "2512.16149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16149", "abs": "https://arxiv.org/abs/2512.16149", "authors": ["Hao Chen", "Zhexin Hu", "Jiajun Chai", "Haocheng Yang", "Hang He", "Xiaohan Wang", "Wei Lin", "Luhang Wang", "Guojun Yin", "Zhuofeng zhao"], "title": "ToolForge: A Data Synthesis Pipeline for Multi-Hop Search without Real-World APIs", "comment": "13 pages, 9 tables, 6 figures. Code available at https://github.com/Buycar-arb/ToolForge", "summary": "Training LLMs to invoke tools and leverage retrieved information necessitates high-quality, diverse data. However, existing pipelines for synthetic data generation often rely on tens of thousands of real API calls to enhance generalization, incurring prohibitive costs while lacking multi-hop reasoning and self-reflection. To address these limitations, we introduce ToolForge, an automated synthesis framework that achieves strong real-world tool-calling performance by constructing only a small number of virtual tools, eliminating the need for real API calls. ToolForge leverages a (question, golden context, answer) triple to synthesize large-scale tool-learning data specifically designed for multi-hop search scenarios, further enriching the generated data through multi-hop reasoning and self-reflection mechanisms. To ensure data fidelity, we employ a Multi-Layer Validation Framework that integrates both rule-based and model-based assessments. Empirical results show that a model with only 8B parameters, when trained on our synthesized data, outperforms GPT-4o on multiple benchmarks. Our code and dataset are publicly available at https://github.com/Buycar-arb/ToolForge .", "AI": {"tldr": "ToolForge是一个自动化合成框架，通过少量虚拟工具生成高质量工具调用数据，无需真实API调用，使8B参数模型在多个基准测试中超越GPT-4o。", "motivation": "现有工具调用数据生成方法依赖大量真实API调用，成本高昂且缺乏多跳推理和自反思能力，限制了LLM工具调用性能的提升。", "method": "提出ToolForge框架：1）基于（问题、黄金上下文、答案）三元组合成大规模工具学习数据；2）引入多跳推理和自反思机制丰富数据；3）采用多层验证框架（规则和模型评估）确保数据质量。", "result": "仅使用8B参数的模型在ToolForge合成数据上训练后，在多个基准测试中表现优于GPT-4o，证明了框架的有效性。", "conclusion": "ToolForge通过自动化数据合成和严格验证，以低成本生成高质量工具调用数据，显著提升LLM工具调用性能，为工具学习提供了高效解决方案。"}}
{"id": "2512.16171", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16171", "abs": "https://arxiv.org/abs/2512.16171", "authors": ["Karthikeyan K", "Philip Wu", "Xin Tang", "Alexandre Alves"], "title": "Science Consultant Agent", "comment": null, "summary": "The Science Consultant Agent is a web-based Artificial Intelligence (AI) tool that helps practitioners select and implement the most effective modeling strategy for AI-based solutions. It operates through four core components: Questionnaire, Smart Fill, Research-Guided Recommendation, and Prototype Builder. By combining structured questionnaires, literature-backed solution recommendations, and prototype generation, the Science Consultant Agent accelerates development for everyone from Product Managers and Software Developers to Researchers. The full pipeline is illustrated in Figure 1.", "AI": {"tldr": "Science Consultant Agent是一个基于网络的AI工具，通过问卷、智能填充、研究指导推荐和原型构建四个核心组件，帮助从业者选择和实施最有效的AI建模策略。", "motivation": "为从业者（包括产品经理、软件开发者和研究人员）提供一个系统化的工具，以加速AI解决方案的开发过程，解决建模策略选择困难的问题。", "method": "结合结构化问卷、文献支持的解决方案推荐和原型生成，构建了一个包含四个核心组件的完整流程。", "result": "开发了一个完整的AI工具，能够通过系统化的流程帮助用户快速选择和实施AI建模策略，并生成原型。", "conclusion": "Science Consultant Agent通过整合多个核心组件，为AI解决方案的开发提供了一个高效、系统化的支持工具，加速了从策略选择到原型实现的整个过程。"}}
{"id": "2512.16530", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16530", "abs": "https://arxiv.org/abs/2512.16530", "authors": ["Primoz Kocbek", "Leon Kopitar", "Gregor Stiglic"], "title": "Plain language adaptations of biomedical text using LLMs: Comparision of evaluation metrics", "comment": "5 pages, 1 figure", "summary": "This study investigated the application of Large Language Models (LLMs) for simplifying biomedical texts to enhance health literacy. Using a public dataset, which included plain language adaptations of biomedical abstracts, we developed and evaluated several approaches, specifically a baseline approach using a prompt template, a two AI agent approach, and a fine-tuning approach. We selected OpenAI gpt-4o and gpt-4o mini models as baselines for further research. We evaluated our approaches with quantitative metrics, such as Flesch-Kincaid grade level, SMOG Index, SARI, and BERTScore, G-Eval, as well as with qualitative metric, more precisely 5-point Likert scales for simplicity, accuracy, completeness, brevity. Results showed a superior performance of gpt-4o-mini and an underperformance of FT approaches. G-Eval, a LLM based quantitative metric, showed promising results, ranking the approaches similarly as the qualitative metric.", "AI": {"tldr": "本研究评估了大型语言模型在简化生物医学文本以提高健康素养方面的应用，发现gpt-4o-mini表现最佳，而微调方法表现不佳。", "motivation": "提高生物医学文本的可读性，以增强公众的健康素养，使复杂的医学信息更易于理解。", "method": "使用公共数据集，比较了基于提示模板的基线方法、双AI代理方法和微调方法，并采用OpenAI的gpt-4o和gpt-4o-mini模型作为基准。", "result": "gpt-4o-mini表现最优，微调方法表现较差；基于LLM的定量评估指标G-Eval与定性评估结果一致。", "conclusion": "大型语言模型在简化生物医学文本方面具有潜力，gpt-4o-mini是有效的工具，而微调方法需要进一步优化。"}}
{"id": "2512.16133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16133", "abs": "https://arxiv.org/abs/2512.16133", "authors": ["Ren Nakagawa", "Yang Yang", "Risa Shinoda", "Hiroaki Santo", "Kenji Oyama", "Fumio Okura", "Takenao Ohkawa"], "title": "Interaction-via-Actions: Cattle Interaction Detection with Joint Learning of Action-Interaction Latent Space", "comment": "Accepted to WACV 2026", "summary": "This paper introduces a method and application for automatically detecting behavioral interactions between grazing cattle from a single image, which is essential for smart livestock management in the cattle industry, such as for detecting estrus. Although interaction detection for humans has been actively studied, a non-trivial challenge lies in cattle interaction detection, specifically the lack of a comprehensive behavioral dataset that includes interactions, as the interactions of grazing cattle are rare events. We, therefore, propose CattleAct, a data-efficient method for interaction detection by decomposing interactions into the combinations of actions by individual cattle. Specifically, we first learn an action latent space from a large-scale cattle action dataset. Then, we embed rare interactions via the fine-tuning of the pre-trained latent space using contrastive learning, thereby constructing a unified latent space of actions and interactions. On top of the proposed method, we develop a practical working system integrating video and GPS inputs. Experiments on a commercial-scale pasture demonstrate the accurate interaction detection achieved by our method compared to the baselines. Our implementation is available at https://github.com/rakawanegan/CattleAct.", "AI": {"tldr": "提出CattleAct方法，通过将交互分解为个体牛只动作组合，实现从单张图像自动检测放牧牛群行为交互，并开发集成视频与GPS的实用系统。", "motivation": "智能畜牧管理（如发情检测）需要牛群交互检测技术，但现有研究缺乏包含交互行为的牛只数据集，且放牧中的交互事件稀少，构成检测挑战。", "method": "1. 从大规模牛只动作数据集学习动作潜在空间；2. 通过对比学习微调预训练潜在空间，嵌入稀有交互行为，构建动作与交互的统一潜在空间；3. 开发集成视频与GPS输入的实用系统。", "result": "在商业规模牧场上的实验表明，该方法相比基线模型实现了更准确的交互检测。", "conclusion": "CattleAct通过数据高效的方法解决了牛群交互检测的数据稀缺问题，为智能畜牧管理提供了可行的技术方案。"}}
{"id": "2512.16861", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16861", "abs": "https://arxiv.org/abs/2512.16861", "authors": ["Zihan Zhou", "Animesh Garg", "Ajay Mandlekar", "Caelan Garrett"], "title": "ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning", "comment": null, "summary": "Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/", "AI": {"tldr": "提出ReinforceGen系统，结合任务分解、数据生成、模仿学习和运动规划，通过强化学习微调解决机器人长时程操作任务，在Robosuite数据集上达到80%成功率。", "motivation": "长时程操作是机器人领域的长期挑战，需要解决复杂任务分解与技能协调问题。", "method": "1. 任务分解为多个局部技能；2. 基于10个人类演示生成数据集；3. 通过模仿学习训练技能和运动规划目标；4. 在线适应和强化学习微调各组件。", "result": "在Robosuite数据集最高重置范围设置下，视觉运动控制任务成功率达80%；消融实验显示微调方法带来平均89%性能提升。", "conclusion": "ReinforceGen通过分层学习框架有效解决长时程操作任务，强化学习微调显著提升系统性能。"}}
{"id": "2512.16793", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16793", "abs": "https://arxiv.org/abs/2512.16793", "authors": ["Xiaopeng Lin", "Shijie Lian", "Bin Yu", "Ruoqi Yang", "Changti Wu", "Yuzhuo Miao", "Yurun Jin", "Yukun Shi", "Cong Huang", "Bojun Cheng", "Kai Chen"], "title": "PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence", "comment": "17 pages, 4 figures", "summary": "Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. However, most VLMs are trained primarily on third-person data, creating a fundamental viewpoint mismatch for humanoid robots. Scaling robot egocentric data collection remains impractical due to high cost and limited diversity, whereas large-scale human egocentric videos offer a scalable alternative that naturally capture rich interaction context and causal structure. The key challenge is to convert raw egocentric videos into structured and reliable embodiment training supervision. Accordingly, we propose an Egocentric2Embodiment translation pipeline that transforms first-person videos into multi-level, schema-driven VQA supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning on EgoThink. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher SimplerEnv success rates (53.9\\%), demonstrating effective transfer from human egocentric supervision to downstream robot control.", "AI": {"tldr": "提出Egocentric2Embodiment转换流程，利用人类第一人称视频构建大规模数据集E2E-3M，训练出具有物理智能的具身模型PhysBrain，显著提升机器人视角理解与规划能力。", "motivation": "现有视觉语言模型主要基于第三人称数据训练，与拟人机器人所需的第一人称视角存在根本性不匹配。机器人本体数据收集成本高且多样性有限，而大规模人类第一人称视频能自然捕捉丰富的交互情境和因果结构，可作为可扩展的替代方案。", "method": "设计Egocentric2Embodiment转换流程，将原始第一人称视频转化为多层次、模式驱动的视觉问答监督数据，强调证据基础和时序一致性，构建E2E-3M数据集（300万样本）。基于该数据集训练具身模型PhysBrain。", "result": "PhysBrain在第一人称理解方面显著提升，在EgoThink规划任务上表现优异。作为第一人称感知的初始化模型，能实现更高效的视觉语言动作微调，在SimplerEnv任务上达到53.9%的成功率，验证了从人类第一人称监督到机器人控制的有效迁移。", "conclusion": "通过人类第一人称视频构建的结构化监督数据能有效训练具身智能体，解决视角不匹配问题，为机器人物理智能的发展提供了可扩展且高效的新途径。"}}
{"id": "2512.16541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16541", "abs": "https://arxiv.org/abs/2512.16541", "authors": ["Primoz Kocbek", "Gregor Stiglic"], "title": "UM_FHS at the CLEF 2025 SimpleText Track: Comparing No-Context and Fine-Tune Approaches for GPT-4.1 Models in Sentence and Document-Level Text Simplification", "comment": "10 pages, 3 tables. CLEF 2025 Working Notes, 9 to 12 September 2025, Madrid, Spain", "summary": "This work describes our submission to the CLEF 2025 SimpleText track Task 1, addressing both sentenceand document-level simplification of scientific texts. The methodology centered on using the gpt-4.1, gpt-4.1mini, and gpt-4.1-nano models from OpenAI. Two distinct approaches were compared: a no-context method relying on prompt engineering and a fine-tuned (FT) method across models. The gpt-4.1-mini model with no-context demonstrated robust performance at both levels of simplification, while the fine-tuned models showed mixed results, highlighting the complexities of simplifying text at different granularities, where gpt-4.1-nano-ft performance stands out at document-level simplification in one case.", "AI": {"tldr": "本文介绍了CLEF 2025 SimpleText竞赛Task 1的提交方案，比较了不同GPT-4.1模型在科学文本简化任务中的表现，发现无上下文提示方法在句子和文档级别均表现稳健。", "motivation": "科学文本通常复杂难懂，需要简化以提高可读性。CLEF 2025 SimpleText竞赛旨在推动科学文本简化技术发展，特别是句子和文档级别的简化。", "method": "使用OpenAI的gpt-4.1、gpt-4.1-mini和gpt-4.1-nano模型，比较两种方法：基于提示工程的无上下文方法和微调方法。", "result": "gpt-4.1-mini模型在无上下文方法中表现最佳，在句子和文档级别简化均稳健；微调模型结果不一，其中gpt-4.1-nano-ft在特定文档级简化任务中表现突出。", "conclusion": "提示工程方法在科学文本简化中更可靠，微调效果因模型和任务粒度而异，表明不同简化层级需要针对性优化策略。"}}
{"id": "2512.16185", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16185", "abs": "https://arxiv.org/abs/2512.16185", "authors": ["Gourab Ghatak"], "title": "Weighted K-Harmonic Means Clustering: Convergence Analysis and Applications to Wireless Communications", "comment": null, "summary": "We propose the \\emph{weighted K-harmonic means} (WKHM) clustering algorithm, a regularized variant of K-harmonic means designed to ensure numerical stability while enabling soft assignments through inverse-distance weighting. Unlike classical K-means and constrained K-means, WKHM admits a direct interpretation in wireless networks: its weights are exactly equivalent to fractional user association based on received signal strength. We establish rigorous convergence guarantees under both deterministic and stochastic settings, addressing key technical challenges arising from non-convexity and random initialization. Specifically, we prove monotone descent to a local minimum under fixed initialization, convergence in probability under Binomial Point Process (BPP) initialization, and almost sure convergence under mild decay conditions. These results provide the first stochastic convergence guarantees for harmonic-mean-based clustering. Finally, through extensive simulations with diverse user distributions, we show that WKHM achieves a superior tradeoff between minimum signal strength and load fairness compared to classical and modern clustering baselines, making it a principled tool for joint radio node placement and user association in wireless networks.", "AI": {"tldr": "提出加权K调和均值聚类算法，通过逆距离加权实现软分配并确保数值稳定性，在无线网络中具有用户关联的直观解释，并提供了收敛性保证和性能优势验证。", "motivation": "经典K均值和约束K均值在无线网络用户关联中缺乏直接解释，且传统调和均值聚类存在数值稳定性问题，需要一种既能实现软分配又具有理论保证的聚类方法。", "method": "提出加权K调和均值算法，采用逆距离加权实现软分配；在确定性设置下证明单调下降收敛到局部最小值，在随机设置下证明二项点过程初始化的概率收敛和温和衰减条件下的几乎必然收敛。", "result": "算法在无线网络中权重等价于基于接收信号强度的分数用户关联；通过多种用户分布仿真显示，WKHM在最小信号强度和负载公平性之间取得优于经典和现代基线方法的平衡。", "conclusion": "WKHM为基于调和均值的聚类提供了首个随机收敛保证，是无线网络中联合无线电节点放置和用户关联的原则性工具，具有优越的性能权衡特性。"}}
{"id": "2512.16140", "categories": ["cs.CV", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2512.16140", "abs": "https://arxiv.org/abs/2512.16140", "authors": ["Ze Yuan", "Wenbin Li", "Shusen Zhao"], "title": "ResDynUNet++: A nested U-Net with residual dynamic convolution blocks for dual-spectral CT", "comment": null, "summary": "We propose a hybrid reconstruction framework for dual-spectral CT (DSCT) that integrates iterative methods with deep learning models. The reconstruction process consists of two complementary components: a knowledge-driven module and a data-driven module. In the knowledge-driven phase, we employ the oblique projection modification technique (OPMT) to reconstruct an intermediate solution of the basis material images from the projection data. We select OPMT for this role because of its fast convergence, which allows it to rapidly generate an intermediate solution that successfully achieves basis material decomposition. Subsequently, in the data-driven phase, we introduce a novel neural network, ResDynUNet++, to refine this intermediate solution. The ResDynUNet++ is built upon a UNet++ backbone by replacing standard convolutions with residual dynamic convolution blocks, which combine the adaptive, input-specific feature extraction of dynamic convolution with the stable training of residual connections. This architecture is designed to address challenges like channel imbalance and near-interface large artifacts in DSCT, producing clean and accurate final solutions. Extensive experiments on both synthetic phantoms and real clinical datasets validate the efficacy and superior performance of the proposed method.", "AI": {"tldr": "提出了一种结合迭代方法和深度学习模型的双能CT混合重建框架，包含知识驱动和数据驱动两个互补模块，通过OPMT快速重建中间解，再用ResDynUNet++网络进行精修，在合成和临床数据上验证了有效性。", "motivation": "双能CT重建面临通道不平衡和界面附近大伪影等挑战，需要兼顾重建速度与精度，传统方法或纯深度学习方案各有局限，因此探索结合两者优势的混合框架。", "method": "1. 知识驱动模块：使用斜投影修正技术（OPMT）从投影数据快速重建基物质图像的中间解；2. 数据驱动模块：提出ResDynUNet++网络（基于UNet++，用残差动态卷积块替换标准卷积）精修中间解。", "result": "在合成体模和真实临床数据集上的大量实验表明，该方法能有效解决通道不平衡和界面伪影问题，生成清晰准确的重建图像，性能优于对比方法。", "conclusion": "所提混合框架成功整合了迭代重建的快速收敛性与深度学习的自适应特征提取能力，为双能CT提供了一种高效且鲁棒的重建解决方案。"}}
{"id": "2512.16881", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16881", "abs": "https://arxiv.org/abs/2512.16881", "authors": ["Arhan Jain", "Mingtong Zhang", "Kanav Arora", "William Chen", "Marcel Torne", "Muhammad Zubair Irshad", "Sergey Zakharov", "Yue Wang", "Sergey Levine", "Chelsea Finn", "Wei-Chiu Ma", "Dhruv Shah", "Abhishek Gupta", "Karl Pertsch"], "title": "PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies", "comment": "Website: https://polaris-evals.github.io/", "summary": "A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.", "AI": {"tldr": "本文提出了PolaRiS框架，通过神经重建方法将真实场景视频转化为交互式仿真环境，实现机器人策略的高保真仿真评估，显著提升了仿真与真实世界性能的相关性。", "motivation": "机器人策略评估面临真实世界实验随机性强、可复现性差、耗时长的挑战，而现有仿真环境与真实世界存在视觉和物理域差距，无法可靠反映策略性能，尤其难以评估通用策略在多样化场景中的表现。", "method": "提出PolaRiS框架：1）利用神经重建方法将真实场景短视频扫描转化为交互式仿真环境；2）开发仿真数据协同训练方法，弥合剩余的真实-仿真差距，支持在未见仿真环境中进行零样本评估。", "result": "通过大量仿真与真实世界的配对实验表明，PolaRiS评估结果与真实世界通用策略性能的相关性显著优于现有仿真基准，且能快速创建多样化仿真环境。", "conclusion": "PolaRiS为机器人基础模型的分布式和民主化评估迈出重要一步，通过高保真实-仿转换解决了机器人策略评估的规模化难题。"}}
{"id": "2512.16896", "categories": ["cs.RO", "cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.16896", "abs": "https://arxiv.org/abs/2512.16896", "authors": ["Jinghuan Shang", "Harsh Patel", "Ran Gong", "Karl Schmeckpeper"], "title": "Sceniris: A Fast Procedural Scene Generation Framework", "comment": "Code is available at https://github.com/rai-inst/sceniris", "summary": "Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris", "AI": {"tldr": "Sceniris是一个高效的程序化场景生成框架，用于快速生成大规模、无碰撞的场景变体，支持机器人可达性检查，相比现有方法速度提升至少234倍。", "motivation": "现有程序化生成方法输出吞吐量低，成为扩展数据集创建的瓶颈，需要高效生成大规模、无碰撞且适合机器人操作的场景。", "method": "通过批量采样和cuRobo中的快速碰撞检查优化性能，扩展对象间空间关系支持多样化场景需求，并提供可选的机器人可达性检查。", "result": "相比Scene Synthesizer方法，Sceniris实现了至少234倍的速度提升，并能生成适合机器人操作的可行场景。", "conclusion": "Sceniris通过高效的程序化生成解决了场景创建瓶颈，为物理AI和生成模型提供了可扩展的场景数据集生成工具。"}}
{"id": "2512.16143", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16143", "abs": "https://arxiv.org/abs/2512.16143", "authors": ["Yueyang Hu", "Haiyong Jiang", "Haoxuan Song", "Jun Xiao", "Hao Pan"], "title": "SegGraph: Leveraging Graphs of SAM Segments for Few-Shot 3D Part Segmentation", "comment": null, "summary": "This work presents a novel framework for few-shot 3D part segmentation. Recent advances have demonstrated the significant potential of 2D foundation models for low-shot 3D part segmentation. However, it is still an open problem that how to effectively aggregate 2D knowledge from foundation models to 3D. Existing methods either ignore geometric structures for 3D feature learning or neglects the high-quality grouping clues from SAM, leading to under-segmentation and inconsistent part labels. We devise a novel SAM segment graph-based propagation method, named SegGraph, to explicitly learn geometric features encoded within SAM's segmentation masks. Our method encodes geometric features by modeling mutual overlap and adjacency between segments while preserving intra-segment semantic consistency. We construct a segment graph, conceptually similar to an atlas, where nodes represent segments and edges capture their spatial relationships (overlap/adjacency). Each node adaptively modulates 2D foundation model features, which are then propagated via a graph neural network to learn global geometric structures. To enforce intra-segment semantic consistency, we map segment features to 3D points with a novel view-direction-weighted fusion attenuating contributions from low-quality segments. Extensive experiments on PartNet-E demonstrate that our method outperforms all competing baselines by at least 6.9 percent mIoU. Further analysis reveals that SegGraph achieves particularly strong performance on small components and part boundaries, demonstrating its superior geometric understanding. The code is available at: https://github.com/YueyangHu2000/SegGraph.", "AI": {"tldr": "提出SegGraph框架，通过构建SAM分割图来聚合2D基础模型知识到3D部件分割，显著提升小部件和边界的分割精度", "motivation": "现有方法在将2D基础模型知识聚合到3D部件分割时，要么忽略几何结构学习，要么未能充分利用SAM的高质量分组线索，导致欠分割和标签不一致问题", "method": "设计基于SAM分割图的传播方法：构建类似图谱的分割图（节点为分割区域，边表示空间关系），通过图神经网络传播自适应调制的2D特征，并采用视角方向加权融合实现片段内语义一致性", "result": "在PartNet-E数据集上mIoU超越所有基线至少6.9%，在小部件和部件边界上表现尤为突出", "conclusion": "SegGraph通过显式学习SAM掩码中的几何特征，有效解决了2D知识到3D的聚合问题，实现了更精确的少样本3D部件分割"}}
{"id": "2512.16602", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16602", "abs": "https://arxiv.org/abs/2512.16602", "authors": ["Iker García-Ferrero", "David Montero", "Roman Orus"], "title": "Refusal Steering: Fine-grained Control over LLM Refusal Behaviour for Sensitive Topics", "comment": null, "summary": "We introduce Refusal Steering, an inference-time method to exercise fine-grained control over Large Language Models refusal behaviour on politically sensitive topics without retraining. We replace fragile pattern-based refusal detection with an LLM-as-a-judge that assigns refusal confidence scores and we propose a ridge-regularized variant to compute steering vectors that better isolate the refusal--compliance direction. On Qwen3-Next-80B-A3B-Thinking, our method removes the refusal behaviour of the model around politically sensitive topics while maintaining safety on JailbreakBench and near-baseline performance on general benchmarks. The approach generalizes across 4B and 80B models and can also induce targeted refusals when desired. We analize the steering vectors and show that refusal signals concentrate in deeper layers of the transformer and are distributed across many dimensions. Together, these results demonstrate that activation steering can remove political refusal behaviour while retaining safety alignment for harmful content, offering a practical path to controllable, transparent moderation at inference time.", "AI": {"tldr": "提出Refusal Steering方法，通过推理时激活向量调整控制大语言模型在政治敏感话题上的拒绝行为，无需重新训练模型。", "motivation": "现有基于模式的拒绝检测方法脆弱且不够精细，需要一种能在推理时精确控制模型拒绝行为的方法，特别是在政治敏感话题上实现可控的合规性调整。", "method": "使用LLM-as-a-judge分配拒绝置信度分数，提出岭正则化变体计算更优的拒绝-合规方向向量，通过激活向量调整实现行为控制。", "result": "在Qwen3-Next-80B等模型上成功移除政治敏感话题的拒绝行为，同时在安全基准测试中保持性能，方法可推广到不同规模模型并实现定向拒绝诱导。", "conclusion": "激活向量调整能有效分离政治拒绝行为与安全对齐，为推理时提供可控、透明的审核机制，拒绝信号主要分布在Transformer深层且维度分散。"}}
{"id": "2512.16214", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16214", "abs": "https://arxiv.org/abs/2512.16214", "authors": ["Jianming Liu", "Ren Zhu", "Jian Xu", "Kun Ding", "Xu-Yao Zhang", "Gaofeng Meng", "Cheng-Lin Liu"], "title": "PDE-Agent: A toolchain-augmented multi-agent framework for PDE solving", "comment": null, "summary": "Solving Partial Differential Equations (PDEs) is a cornerstone of engineering and scientific research. Traditional methods for PDE solving are cumbersome, relying on manual setup and domain expertise. While Physics-Informed Neural Network (PINNs) introduced end-to-end neural network-based solutions, and frameworks like DeepXDE further enhanced automation, these approaches still depend on expert knowledge and lack full autonomy. In this work, we frame PDE solving as tool invocation via LLM-driven agents and introduce PDE-Agent, the first toolchain-augmented multi-agent collaboration framework, inheriting the reasoning capacity of LLMs and the controllability of external tools and enabling automated PDE solving from natural language descriptions. PDE-Agent leverages the strengths of multi-agent and multi-tool collaboration through two key innovations: (1) A Prog-Act framework with graph memory for multi-agent collaboration, which enables effective dynamic planning and error correction via dual-loop mechanisms (localized fixes and global revisions). (2) A Resource-Pool integrated with a tool-parameter separation mechanism for multi-tool collaboration. This centralizes the management of runtime artifacts and resolves inter-tool dependency gaps in existing frameworks. To validate and evaluate this new paradigm for PDE solving , we develop PDE-Bench, a multi-type PDE Benchmark for agent-based tool collaborative solving, and propose multi-level metrics for assessing tool coordination. Evaluations verify that PDE-Agent exhibits superior applicability and performance in complex multi-step, cross-step dependent tasks. This new paradigm of toolchain-augmented multi-agent PDE solving will further advance future developments in automated scientific computing. Our source code and dataset will be made publicly available.", "AI": {"tldr": "提出了PDE-Agent框架，首个基于工具链增强的多智能体协作框架，通过LLM驱动智能体调用工具实现从自然语言描述自动求解偏微分方程。", "motivation": "传统PDE求解方法依赖人工设置和领域专业知识，现有PINN等方法仍需要专家知识且缺乏完全自主性，需要更自动化的求解方案。", "method": "1. 提出Prog-Act框架配合图记忆实现多智能体协作，通过双循环机制（局部修复和全局修订）实现动态规划和错误纠正；2. 设计集成工具-参数分离机制的资源池，集中管理运行时工件并解决现有框架中的工具间依赖间隙。", "result": "开发了PDE-Bench多类型PDE基准测试集，评估表明PDE-Agent在复杂多步骤、跨步骤依赖任务中表现出优越的适用性和性能。", "conclusion": "工具链增强的多智能体PDE求解新范式将推动自动化科学计算的未来发展，代码和数据集将公开。"}}
{"id": "2512.16164", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16164", "abs": "https://arxiv.org/abs/2512.16164", "authors": ["Chao Li", "Dasha Hu", "Chengyang Li", "Yuming Jiang", "Yuncheng Shen"], "title": "C-DGPA: Class-Centric Dual-Alignment Generative Prompt Adaptation", "comment": null, "summary": "Unsupervised Domain Adaptation transfers knowledge from a labeled source domain to an unlabeled target domain. Directly deploying Vision-Language Models (VLMs) with prompt tuning in downstream UDA tasks faces the signifi cant challenge of mitigating domain discrepancies. Existing prompt-tuning strategies primarily align marginal distribu tion, but neglect conditional distribution discrepancies, lead ing to critical issues such as class prototype misalignment and degraded semantic discriminability. To address these lim itations, the work proposes C-DGPA: Class-Centric Dual Alignment Generative Prompt Adaptation. C-DGPA syner gistically optimizes marginal distribution alignment and con ditional distribution alignment through a novel dual-branch architecture. The marginal distribution alignment branch em ploys a dynamic adversarial training framework to bridge marginal distribution discrepancies. Simultaneously, the con ditional distribution alignment branch introduces a Class Mapping Mechanism (CMM) to align conditional distribu tion discrepancies by standardizing semantic prompt under standing and preventing source domain over-reliance. This dual alignment strategy effectively integrates domain knowl edge into prompt learning via synergistic optimization, ensur ing domain-invariant and semantically discriminative repre sentations. Extensive experiments on OfficeHome, Office31, and VisDA-2017 validate the superiority of C-DGPA. It achieves new state-of-the-art results on all benchmarks.", "AI": {"tldr": "本文提出C-DGPA方法，通过双分支架构同时优化边缘分布对齐和条件分布对齐，解决视觉语言模型在无监督域适应中因忽略条件分布差异导致的类别原型错位问题，在多个基准测试中达到最优性能。", "motivation": "现有基于提示调优的无监督域适应方法主要关注边缘分布对齐，但忽略了条件分布差异，导致类别原型错位和语义判别性下降，限制了视觉语言模型在跨域任务中的性能。", "method": "提出C-DGPA方法：1）边缘分布对齐分支采用动态对抗训练框架；2）条件分布对齐分支引入类别映射机制，通过标准化语义提示理解和防止源域过依赖来对齐条件分布；3）双分支协同优化将领域知识融入提示学习。", "result": "在OfficeHome、Office31和VisDA-2017数据集上的实验表明，C-DGPA在所有基准测试中均取得了新的最优结果，验证了其有效性。", "conclusion": "C-DGPA通过双分布对齐策略成功解决了视觉语言模型在无监督域适应中的条件分布差异问题，实现了领域不变且语义判别性强的表示学习。"}}
{"id": "2512.16178", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16178", "abs": "https://arxiv.org/abs/2512.16178", "authors": ["M. Oltan Sevinc", "Liao Wu", "Francisco Cruz"], "title": "Towards Closing the Domain Gap with Event Cameras", "comment": "Accepted to Australasian Conference on Robotics and Automation (ACRA), 2025", "summary": "Although traditional cameras are the primary sensor for end-to-end driving, their performance suffers greatly when the conditions of the data they were trained on does not match the deployment environment, a problem known as the domain gap. In this work, we consider the day-night lighting difference domain gap. Instead of traditional cameras we propose event cameras as a potential alternative which can maintain performance across lighting condition domain gaps without requiring additional adjustments. Our results show that event cameras maintain more consistent performance across lighting conditions, exhibiting domain-shift penalties that are generally comparable to or smaller than grayscale frames and provide superior baseline performance in cross-domain scenarios.", "AI": {"tldr": "事件相机在自动驾驶中能更好地应对昼夜光照差异带来的域差距问题，相比传统相机在跨域场景中表现更稳定。", "motivation": "传统相机在端到端驾驶中性能受限于训练数据与部署环境之间的域差距问题，尤其是昼夜光照差异严重影响模型性能。", "method": "提出使用事件相机替代传统相机，通过对比事件相机、灰度帧在跨域场景下的性能表现，评估其在光照条件变化时的稳定性。", "result": "事件相机在不同光照条件下性能更一致，其域偏移惩罚通常与灰度帧相当或更小，且在跨域场景中提供更优的基线性能。", "conclusion": "事件相机是解决自动驾驶中光照域差距问题的有效替代方案，无需额外调整即可保持跨域性能稳定性。"}}
{"id": "2512.16649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16649", "abs": "https://arxiv.org/abs/2512.16649", "authors": ["Bingxiang He", "Zekai Qu", "Zeyuan Liu", "Yinghao Chen", "Yuxin Zuo", "Cheng Qian", "Kaiyan Zhang", "Weize Chen", "Chaojun Xiao", "Ganqu Cui", "Ning Ding", "Zhiyuan Liu"], "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe", "comment": "12 pages, 3 figures", "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.", "AI": {"tldr": "本文提出JustRL，一种极简的单阶段强化学习方法，使用固定超参数，在数学推理任务上达到SOTA性能，且计算量减半，挑战了当前强化学习训练中复杂多阶段流程的必要性。", "motivation": "当前大语言模型的强化学习方法日益复杂（多阶段训练、动态超参数、课程学习等），作者质疑这种复杂性是否必要，旨在探索更简单有效的替代方案。", "method": "采用单阶段训练，固定超参数，不依赖显式长度惩罚或鲁棒验证器等常见技巧，在1.5B参数模型上进行实验，并在九个数学推理基准上评估。", "result": "JustRL在两个1.5B推理模型上分别达到54.9%和64.3%的平均准确率，计算量减少一半；相同超参数可跨模型迁移；训练过程平滑单调，未出现崩溃或平台期；常见技巧反而可能因限制探索而降低性能。", "conclusion": "研究表明，许多复杂训练策略可能是为了解决本不存在的稳定性问题；提供一个简单、可复现的基线有助于社区更聚焦核心问题。"}}
{"id": "2512.16770", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16770", "abs": "https://arxiv.org/abs/2512.16770", "authors": ["William English", "Chase Walker", "Dominic Simon", "Rickard Ewetz"], "title": "GinSign: Grounding Natural Language Into System Signatures for Temporal Logic Translation", "comment": null, "summary": "Natural language (NL) to temporal logic (TL) translation enables engineers to specify, verify, and enforce system behaviors without manually crafting formal specifications-an essential capability for building trustworthy autonomous systems. While existing NL-to-TL translation frameworks have demonstrated encouraging initial results, these systems either explicitly assume access to accurate atom grounding or suffer from low grounded translation accuracy. In this paper, we propose a framework for Grounding Natural Language Into System Signatures for Temporal Logic translation called GinSign. The framework introduces a grounding model that learns the abstract task of mapping NL spans onto a given system signature: given a lifted NL specification and a system signature $\\mathcal{S}$, the classifier must assign each lifted atomic proposition to an element of the set of signature-defined atoms $\\mathcal{P}$. We decompose the grounding task hierarchically- first predicting predicate labels, then selecting the appropriately typed constant arguments. Decomposing this task from a free-form generation problem into a structured classification problem permits the use of smaller masked language models and eliminates the reliance on expensive LLMs. Experiments across multiple domains show that frameworks which omit grounding tend to produce syntactically correct lifted LTL that is semantically nonequivalent to grounded target expressions, whereas our framework supports downstream model checking and achieves grounded logical-equivalence scores of $95.5\\%$, a $1.4\\times$ improvement over SOTA.", "AI": {"tldr": "提出GinSign框架，将自然语言转化为时序逻辑时学习系统签名中的原子命题映射，提升接地翻译准确性。", "motivation": "现有自然语言到时序逻辑翻译方法要么依赖准确原子接地假设，要么接地翻译准确率低，影响自主系统可信性。", "method": "引入分层接地模型：先预测谓词标签，再选择类型匹配的常量参数，将任务转化为结构化分类问题，使用掩码语言模型而非大语言模型。", "result": "实验显示，GinSign在接地逻辑等价性上达95.5%，比现有最佳方法提升1.4倍，支持下游模型检验。", "conclusion": "GinSign通过结构化接地方法有效提升翻译准确性，为自主系统规范验证提供可靠基础。"}}
{"id": "2512.16237", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16237", "abs": "https://arxiv.org/abs/2512.16237", "authors": ["Zhi Helu", "Huang Jingjing", "Xu Wang", "Xu Yangbin", "Zhang Wanyue", "Jiang Baoyang", "Deng Shirui", "Zhu Liang", "Li Fangfang", "Zhao Tiejun", "Lin Yankai", "Yao Yuan"], "title": "Scaling Spatial Reasoning in MLLMs through Programmatic Data Synthesis", "comment": null, "summary": "Embodied intelligence, a grand challenge in artificial intelligence, is fundamentally constrained by the limited spatial understanding and reasoning capabilities of current models. Prevailing efforts to address this through enhancing Vision-Language Models (VLMs) are trapped in a dilemma: template-based datasets are scalable but structurally rigid, while manual annotation is linguistically diverse but unscalable and, critically, computationally imprecise. We introduce SPRITE, a novel framework that overcomes this dilemma by leveraging simulators and large models to programmatically synthesize scalable, diverse, and high-quality spatial reasoning data. The core innovation of SPRITE is to reframe ground-truth generation as a code-generation task. We utilize LLMs to compile complex spatial questions into executable programs, which are then verified against high-precision scene meta-information extracted from simulators. This ensures our ground truth is both computationally precise and verifiable, while the generative power of LLMs provides vast linguistic diversity. Leveraging this pipeline, we have curated a dataset encompassing 3 simulators, 11k+ scenes, and 300k+ image/video instruction-tuning pairs. We demonstrate that a VLM trained on our data achieves significant performance gains on multiple spatial benchmarks and outperforms other open-source datasets of equivalent size. Furthermore, a scalability analysis confirms our hypothesis that overcoming the low-diversity nature of traditional template methods is essential for building robust, generalizable spatial intelligence. We will make the SPRITE framework code and the full 300k+ dataset publicly available to facilitate future research in spatial intelligence.", "AI": {"tldr": "SPRITE框架通过模拟器和大型模型程序化合成可扩展、多样且高质量的空间推理数据，解决了现有方法在可扩展性和语言多样性之间的困境。", "motivation": "当前视觉语言模型在空间理解和推理能力上存在局限，传统方法面临模板数据集结构僵化与人工标注不可扩展且计算不精确的困境。", "method": "将真实标注生成重构为代码生成任务，利用LLM将复杂空间问题编译为可执行程序，并通过模拟器提取的高精度场景元信息进行验证。", "result": "构建了包含3个模拟器、11k+场景和300k+图像/视频指令调优对的数据集，训练出的VLM在多个空间基准测试中表现显著提升，优于同等规模的开源数据集。", "conclusion": "克服传统模板方法的低多样性是构建鲁棒、可泛化空间智能的关键，SPRITE框架为空间智能研究提供了可扩展的高质量数据生成方案。"}}
{"id": "2512.16201", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16201", "abs": "https://arxiv.org/abs/2512.16201", "authors": ["Sarosij Bose", "Ravi K. Rajendran", "Biplob Debnath", "Konstantinos Karydis", "Amit K. Roy-Chowdhury", "Srimat Chakradhar"], "title": "Visual Alignment of Medical Vision-Language Models for Grounded Radiology Report Generation", "comment": null, "summary": "Radiology Report Generation (RRG) is a critical step toward automating healthcare workflows, facilitating accurate patient assessments, and reducing the workload of medical professionals. Despite recent progress in Large Medical Vision-Language Models (Med-VLMs), generating radiology reports that are both visually grounded and clinically accurate remains a significant challenge. Existing approaches often rely on large labeled corpora for pre-training, costly task-specific preference data, or retrieval-based methods. However, these strategies do not adequately mitigate hallucinations arising from poor cross-modal alignment between visual and linguistic representations. To address these limitations, we propose VALOR:Visual Alignment of Medical Vision-Language Models for GrOunded Radiology Report Generation. Our method introduces a reinforcement learning-based post-alignment framework utilizing Group-Relative Proximal Optimization (GRPO). The training proceeds in two stages: (1) improving the Med-VLM with textual rewards to encourage clinically precise terminology, and (2) aligning the vision projection module of the textually grounded model with disease findings, thereby guiding attention toward image re gions most relevant to the diagnostic task. Extensive experiments on multiple benchmarks demonstrate that VALOR substantially improves factual accuracy and visual grounding, achieving significant performance gains over state-of-the-art report generation methods.", "AI": {"tldr": "提出VALOR方法，通过强化学习后对齐框架提升医学视觉语言模型在放射学报告生成中的视觉对齐和临床准确性", "motivation": "现有医学视觉语言模型在生成放射学报告时存在幻觉问题，视觉与语言表征的跨模态对齐不足，且依赖大量标注数据或检索方法", "method": "采用两阶段强化学习后对齐框架：1) 使用文本奖励改进模型临床术语准确性；2) 通过GRPO算法对齐视觉投影模块与疾病发现，引导注意力到相关图像区域", "result": "在多个基准测试中显著提升事实准确性和视觉对齐性能，优于现有最先进的报告生成方法", "conclusion": "VALOR方法有效解决了医学视觉语言模型在放射学报告生成中的视觉对齐问题，提高了报告的临床准确性和可靠性"}}
{"id": "2512.16199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16199", "abs": "https://arxiv.org/abs/2512.16199", "authors": ["Jerrin Bright", "Zhibo Wang", "Dmytro Klepachevskyi", "Yuhao Chen", "Sirisha Rambhatla", "David Clausi", "John Zelek"], "title": "Avatar4D: Synthesizing Domain-Specific 4D Humans for Real-World Pose Estimation", "comment": null, "summary": "We present Avatar4D, a real-world transferable pipeline for generating customizable synthetic human motion datasets tailored to domain-specific applications. Unlike prior works, which focus on general, everyday motions and offer limited flexibility, our approach provides fine-grained control over body pose, appearance, camera viewpoint, and environmental context, without requiring any manual annotations. To validate the impact of Avatar4D, we focus on sports, where domain-specific human actions and movement patterns pose unique challenges for motion understanding. In this setting, we introduce Syn2Sport, a large-scale synthetic dataset spanning sports, including baseball and ice hockey. Avatar4D features high-fidelity 4D (3D geometry over time) human motion sequences with varying player appearances rendered in diverse environments. We benchmark several state-of-the-art pose estimation models on Syn2Sport and demonstrate their effectiveness for supervised learning, zero-shot transfer to real-world data, and generalization across sports. Furthermore, we evaluate how closely the generated synthetic data aligns with real-world datasets in feature space. Our results highlight the potential of such systems to generate scalable, controllable, and transferable human datasets for diverse domain-specific tasks without relying on domain-specific real data.", "AI": {"tldr": "提出Avatar4D，一个可迁移的合成人体运动数据集生成管道，能精细控制姿态、外观、视角和环境，并创建了Syn2Sport体育数据集验证其有效性。", "motivation": "现有方法主要关注日常动作且灵活性有限，缺乏针对特定领域（如体育）的可定制化合成数据生成能力，难以满足复杂运动理解任务的需求。", "method": "1. 开发Avatar4D管道，无需人工标注即可控制身体姿态、外观、相机视角和环境；2. 构建Syn2Sport大规模合成体育数据集（含棒球、冰球等）；3. 在合成数据上训练姿态估计模型，评估其监督学习、零样本迁移和跨体育泛化能力；4. 分析合成数据与真实数据在特征空间的对齐程度。", "result": "1. 生成高保真4D人体运动序列；2. 合成数据能有效用于模型训练，并实现向真实数据的零样本迁移；3. 合成数据在特征空间与真实数据高度对齐；4. 证明了系统可生成可扩展、可控且可迁移的领域特定数据集。", "conclusion": "Avatar4D能无需领域真实数据即可生成可定制化合成数据集，为特定领域任务提供可扩展的解决方案，在体育等复杂运动理解场景中具有应用潜力。"}}
{"id": "2512.16461", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16461", "abs": "https://arxiv.org/abs/2512.16461", "authors": ["Tin Stribor Sohn", "Maximilian Dillitzer", "Jason J. Corso", "Eric Sax"], "title": "SNOW: Spatio-Temporal Scene Understanding with World Knowledge for Open-World Embodied Reasoning", "comment": null, "summary": "Autonomous robotic systems require spatio-temporal understanding of dynamic environments to ensure reliable navigation and interaction. While Vision-Language Models (VLMs) provide open-world semantic priors, they lack grounding in 3D geometry and temporal dynamics. Conversely, geometric perception captures structure and motion but remains semantically sparse. We propose SNOW (Scene Understanding with Open-World Knowledge), a training-free and backbone-agnostic framework for unified 4D scene understanding that integrates VLM-derived semantics with point cloud geometry and temporal consistency. SNOW processes synchronized RGB images and 3D point clouds, using HDBSCAN clustering to generate object-level proposals that guide SAM2-based segmentation. Each segmented region is encoded through our proposed Spatio-Temporal Tokenized Patch Encoding (STEP), producing multimodal tokens that capture localized semantic, geometric, and temporal attributes. These tokens are incrementally integrated into a 4D Scene Graph (4DSG), which serves as 4D prior for downstream reasoning. A lightweight SLAM backend anchors all STEP tokens spatially in the environment, providing the global reference alignment, and ensuring unambiguous spatial grounding across time. The resulting 4DSG forms a queryable, unified world model through which VLMs can directly interpret spatial scene structure and temporal dynamics. Experiments on a diverse set of benchmarks demonstrate that SNOW enables precise 4D scene understanding and spatially grounded inference, thereby setting new state-of-the-art performance in several settings, highlighting the importance of structured 4D priors for embodied reasoning and autonomous robotics.", "AI": {"tldr": "提出SNOW框架，将视觉语言模型的语义知识与点云几何及时序一致性结合，实现免训练的4D场景理解，通过构建4D场景图支持机器人推理。", "motivation": "现有视觉语言模型缺乏3D几何与时序动态的关联，而几何感知方法语义稀疏，需融合两者以实现自主机器人对动态环境的时空理解。", "method": "使用同步RGB图像与3D点云，通过HDBSCAN聚类生成物体提案，引导SAM2分割；提出STEP编码提取多模态特征，构建4D场景图；轻量SLAM后端提供空间锚定。", "result": "在多个基准测试中取得最优性能，实现精确的4D场景理解与空间推理，验证了结构化4D先验对具身推理的重要性。", "conclusion": "SNOW框架通过统一语义与几何时序信息，构建可查询的4D世界模型，为自主机器人提供了可靠的场景理解基础。"}}
{"id": "2512.16795", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16795", "abs": "https://arxiv.org/abs/2512.16795", "authors": ["Shubham Mishra", "Samyek Jain", "Gorang Mehrishi", "Shiv Tiwari", "Harsh Sharma", "Pratik Narang", "Dhruv Kumar"], "title": "From Facts to Conclusions : Integrating Deductive Reasoning in Retrieval-Augmented LLMs", "comment": "Under Review", "summary": "Retrieval-Augmented Generation (RAG) grounds large language models (LLMs) in external evidence, but fails when retrieved sources conflict or contain outdated or subjective information. Prior work address these issues independently but lack unified reasoning supervision. We propose a reasoning-trace-augmented RAG framework that adds structured, interpretable reasoning across three stages : (1) document-level adjudication, (2) conflict analysis, and (3) grounded synthesis, producing citation-linked answers or justified refusals. A Conflict-Aware Trust-Score (CATS) pipeline is introduced which evaluates groundedness, factual correctness, refusal accuracy, and conflict-behavior alignment using an LLM-as-a-Judge. Our 539-query reasoning dataset and evaluation pipeline establish a foundation for conflict-aware, interpretable RAG systems. Experimental results demonstrate substantial gains over baselines, most notably with Qwen, where Supervised Fine-Tuning improved End-to-End answer correctness from 0.069 to 0.883 and behavioral adherence from 0.074 to 0.722.", "AI": {"tldr": "提出了一种推理轨迹增强的RAG框架，通过三阶段结构化推理处理检索文档中的冲突、过时和主观信息，并引入冲突感知信任评分管道进行评估。", "motivation": "现有检索增强生成方法在检索到相互冲突、过时或主观的信息时表现不佳，且缺乏统一的推理监督机制。", "method": "提出推理轨迹增强RAG框架，包含文档级裁决、冲突分析和基于证据的合成三阶段结构化推理；设计冲突感知信任评分管道，使用LLM作为评判员评估多个维度。", "result": "实验显示该方法显著优于基线，特别是对Qwen模型，监督微调后端到端答案正确率从0.069提升至0.883，行为一致性从0.074提升至0.722。", "conclusion": "该框架为冲突感知、可解释的RAG系统奠定了基础，通过结构化推理和综合评估管道有效提升了系统在复杂信息环境下的表现。"}}
{"id": "2512.16250", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.16250", "abs": "https://arxiv.org/abs/2512.16250", "authors": ["Sanjoy Chowdhury", "Karren D. Yang", "Xudong Liu", "Fartash Faghri", "Pavan Kumar Anasosalu Vasu", "Oncel Tuzel", "Dinesh Manocha", "Chun-Liang Li", "Raviteja Vemulapalli"], "title": "AMUSE: Audio-Visual Benchmark and Alignment Framework for Agentic Multi-Speaker Understanding", "comment": null, "summary": "Recent multimodal large language models (MLLMs) such as GPT-4o and Qwen3-Omni show strong perception but struggle in multi-speaker, dialogue-centric settings that demand agentic reasoning tracking who speaks, maintaining roles, and grounding events across time. These scenarios are central to multimodal audio-video understanding, where models must jointly reason over audio and visual streams in applications such as conversational video assistants and meeting analytics. We introduce AMUSE, a benchmark designed around tasks that are inherently agentic, requiring models to decompose complex audio-visual interactions into planning, grounding, and reflection steps. It evaluates MLLMs across three modes zero-shot, guided, and agentic and six task families, including spatio-temporal speaker grounding and multimodal dialogue summarization. Across all modes, current models exhibit weak multi-speaker reasoning and inconsistent behavior under both non-agentic and agentic evaluation. Motivated by the inherently agentic nature of these tasks and recent advances in LLM agents, we propose RAFT, a data-efficient agentic alignment framework that integrates reward optimization with intrinsic multimodal self-evaluation as reward and selective parameter adaptation for data and parameter efficient updates. Using RAFT, we achieve up to 39.52\\% relative improvement in accuracy on our benchmark. Together, AMUSE and RAFT provide a practical platform for examining agentic reasoning in multimodal models and improving their capabilities.", "AI": {"tldr": "提出了AMUSE基准测试和RAFT框架，用于评估和改进多模态大语言模型在多说话人对话场景中的代理推理能力。", "motivation": "现有MLLMs在多说话人对话场景中表现不佳，缺乏跟踪说话人、维持角色和跨时间事件关联的能力，而这些能力对音频-视频理解应用至关重要。", "method": "1) 创建AMUSE基准测试，包含零样本、引导和代理三种评估模式及六类任务；2) 提出RAFT框架，通过奖励优化和内在多模态自评估实现数据高效的代理对齐。", "result": "现有模型在多说话人推理中表现薄弱且不一致；使用RAFT框架在基准测试中实现了最高39.52%的相对准确率提升。", "conclusion": "AMUSE和RAFT为研究多模态模型的代理推理能力提供了实用平台，能有效提升模型在复杂音频-视频交互任务中的表现。"}}
{"id": "2512.16262", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16262", "abs": "https://arxiv.org/abs/2512.16262", "authors": ["Yifei She", "Ping Zhang", "He Liu", "Yanmin Jia", "Yang Jing", "Zijun Liu", "Peng Sun", "Xiangbin Li", "Xiaohe Hu"], "title": "Learning to Wait: Synchronizing Agents with the Physical World", "comment": null, "summary": "Real-world agentic tasks, unlike synchronous Markov Decision Processes (MDPs), often involve non-blocking actions with variable latencies, creating a fundamental \\textit{Temporal Gap} between action initiation and completion. Existing environment-side solutions, such as blocking wrappers or frequent polling, either limit scalability or dilute the agent's context window with redundant observations. In this work, we propose an \\textbf{Agent-side Approach} that empowers Large Language Models (LLMs) to actively align their \\textit{Cognitive Timeline} with the physical world. By extending the Code-as-Action paradigm to the temporal domain, agents utilize semantic priors and In-Context Learning (ICL) to predict precise waiting durations (\\texttt{time.sleep(t)}), effectively synchronizing with asynchronous environment without exhaustive checking. Experiments in a simulated Kubernetes cluster demonstrate that agents can precisely calibrate their internal clocks to minimize both query overhead and execution latency, validating that temporal awareness is a learnable capability essential for autonomous evolution in open-ended environments.", "AI": {"tldr": "提出一种代理端方法，使大语言模型能够通过预测等待时间主动对齐认知时间线与物理世界，解决异步环境中的时间差问题。", "motivation": "现实世界中的代理任务常涉及具有可变延迟的非阻塞动作，造成动作启动与完成之间的时间差。现有环境端解决方案（如阻塞包装器或频繁轮询）要么限制可扩展性，要么用冗余观察稀释代理的上下文窗口。", "method": "将代码即行动范式扩展到时间领域，代理利用语义先验和上下文学习预测精确等待时间，实现与异步环境的同步，无需详尽检查。", "result": "在模拟Kubernetes集群中的实验表明，代理能够精确校准内部时钟，最小化查询开销和执行延迟。", "conclusion": "时间意识是一种可学习的能力，对于开放环境中自主演进至关重要。"}}
{"id": "2512.16202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16202", "abs": "https://arxiv.org/abs/2512.16202", "authors": ["Zilin Wang", "Sangwoo Mo", "Stella X. Yu", "Sima Behpour", "Liu Ren"], "title": "Open Ad-hoc Categorization with Contextualized Feature Learning", "comment": "26 pages, 17 figures", "summary": "Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks. Unlike fixed common categories for plants or animals, ad-hoc categories are created dynamically to serve specific goals. We study open ad-hoc categorization: Given a few labeled exemplars and abundant unlabeled data, the goal is to discover the underlying context and to expand ad-hoc categories through semantic extension and visual clustering around it.\n  Building on the insight that ad-hoc and common categories rely on similar perceptual mechanisms, we propose OAK, a simple model that introduces a small set of learnable context tokens at the input of a frozen CLIP and optimizes with both CLIP's image-text alignment objective and GCD's visual clustering objective.\n  On Stanford and Clevr-4 datasets, OAK achieves state-of-the-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. Moreover, OAK produces interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling adaptive and generalizable categorization.", "AI": {"tldr": "提出OAK模型，通过可学习上下文令牌和双目标优化，实现开放临时类别自适应分类，在多个数据集上达到SOTA性能。", "motivation": "AI智能体需要自适应视觉场景分类能力。与固定通用类别不同，临时类别需动态创建以服务特定目标，现有方法难以有效发现上下文并扩展临时类别。", "method": "基于CLIP和GCD，在冻结CLIP输入端引入少量可学习上下文令牌，同时优化CLIP的图像-文本对齐目标和GCD的视觉聚类目标。", "result": "在Stanford和Clevr-4数据集上，OAK在准确率和概念发现方面达到SOTA，其中Stanford Mood数据集新类别准确率达87.4%，比CLIP和GCD提升超50%。模型还能生成可解释的显著图。", "conclusion": "OAK通过结合语义扩展和视觉聚类，实现了自适应、可泛化的临时类别分类，同时提供透明可解释的结果，增强了AI系统的可信度。"}}
{"id": "2512.16245", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16245", "abs": "https://arxiv.org/abs/2512.16245", "authors": ["Aniruddha Roy", "Jyoti Patel", "Aman Chadha", "Vinija Jain", "Amitava Das"], "title": "AlignMerge - Alignment-Preserving Large Language Model Merging via Fisher-Guided Geometric Constraints", "comment": null, "summary": "Merging large language models (LLMs) is a practical way to compose capabilities from multiple fine-tuned checkpoints without retraining. Yet standard schemes (linear weight soups, task vectors, and Fisher-weighted averaging) can preserve loss while quietly destroying alignment. We argue that merging is not a numerical trick but a geometry-constrained operation around an already-aligned anchor: fusion must be steered to respect safety geometry, not validated post hoc.\n  We introduce AlignMerge, a geometry-aware merging framework that makes alignment an explicit invariant. In a local Fisher chart around an instruction-tuned base, we estimate an alignment subspace with projector P_A and optimize:\n  L_AlignMerge = L_geo + lambda_align * L_align + lambda_bud * L_bud,\n  where L_geo keeps the merge close to its experts in Fisher-Rao geometry, L_align penalizes motion along alignment-sensitive directions, and L_bud enforces a soft alignment budget. As the alignment functional we use the decoding-invariant Alignment Quality Index (AQI), a latent-space criterion that captures how cleanly aligned and misaligned behaviors separate in representation space.\n  Across five model families (LLaMA-3 8B, Mistral 7B, Qwen 2, Phi-3.5, Gemma 2), merging safety anchors with task experts, AlignMerge improves alignment metrics (AQI, toxicity, LLM-judge alignment) while matching or exceeding the best expert on instruction-following, reasoning, and helpfulness. It also exhibits smaller alignment-subspace drift and fewer budget violations than Fisher soups, TIES, SafeMerge, and MergeAlign. These results make alignment-preserving merging a first-class design goal and suggest a path to geometry-aware composition of future foundation models.", "AI": {"tldr": "提出AlignMerge框架，在合并大语言模型时显式保持对齐性，通过几何约束和惩罚对齐敏感方向运动，在多个模型家族中实现对齐指标提升的同时保持任务性能。", "motivation": "现有大语言模型合并方法（如线性权重组合、任务向量、Fisher加权平均）可能在保持损失函数值的同时破坏模型的对齐性（如安全性、价值观对齐），需要将对齐性保护作为合并过程的核心设计目标而非事后验证。", "method": "提出AlignMerge框架：在指令微调基模型的局部Fisher图表中，估计对齐子空间并定义投影器P_A；优化目标包含几何损失（保持与专家模型的Fisher-Rao几何距离）、对齐损失（惩罚沿对齐敏感方向的运动）和预算损失（软对齐预算约束）；使用解码不变的对齐质量指数（AQI）作为对齐性度量标准。", "result": "在五个模型家族（LLaMA-3 8B、Mistral 7B、Qwen 2、Phi-3.5、Gemma 2）的实验中，AlignMerge在合并安全锚点与任务专家时：提升对齐指标（AQI、毒性、LLM评判对齐性）；在指令遵循、推理和帮助性任务上匹配或超越最佳专家；相比Fisher soups、TIES、SafeMerge和MergeAlign方法，表现出更小的对齐子空间漂移和更少的预算违反。", "conclusion": "对齐保持的模型合并应成为首要设计目标，AlignMerge为未来基础模型的几何感知组合提供了一条可行路径，证明通过显式约束对齐几何可以同时保持模型性能和安全对齐特性。"}}
{"id": "2512.16811", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16811", "abs": "https://arxiv.org/abs/2512.16811", "authors": ["Jingjing Qian", "Boyao Han", "Chen Shi", "Lei Xiao", "Long Yang", "Shaoshuai Shi", "Li Jiang"], "title": "GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation", "comment": null, "summary": "Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.", "AI": {"tldr": "GeoPredict是一个几何感知的视觉-语言-动作框架，通过预测3D关键点轨迹和几何结构来增强机器人操作的3D推理能力，在几何密集型任务中表现优异。", "motivation": "现有的视觉-语言-动作模型在机器人操作中表现出良好的泛化能力，但主要局限于反应式2D推理，在需要精确3D几何推理的任务中可靠性不足。", "method": "提出GeoPredict框架：1）轨迹级模块编码运动历史并预测多步3D关键点轨迹；2）预测性3D高斯几何模块预测工作空间几何结构，并沿关键点轨迹进行跟踪引导细化；3）训练时通过基于深度的渲染进行监督，推理时仅需轻量级查询令牌。", "result": "在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict持续优于现有视觉-语言-动作基线方法，特别是在几何密集型和空间要求高的场景中。", "conclusion": "GeoPredict通过整合预测性几何和运动先验，显著提升了视觉-语言-动作模型在3D推理任务中的性能，为机器人操作提供了更可靠的几何感知能力。"}}
{"id": "2512.16814", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16814", "abs": "https://arxiv.org/abs/2512.16814", "authors": ["William English", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "title": "Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs", "comment": null, "summary": "Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.", "AI": {"tldr": "提出Grammar Forced Translation (GraFT)框架，通过限制每一步有效输出词汇来提升自然语言到时序逻辑的翻译准确率，在多个基准测试中优于现有方法。", "motivation": "现有方法在原子命题提取、共指消解和小样本学习方面存在困难，且采用全词汇迭代预测导致任务复杂度高。", "method": "基于语法约束的翻译框架，利用问题特性逐步限制有效输出词汇，降低解空间复杂度，并提供理论证明。", "result": "在CW、GLTL和Navi基准测试中，端到端翻译准确率平均提升5.49%，跨领域翻译准确率平均提升14.06%。", "conclusion": "GraFT通过解空间缩减显著提升翻译性能，为自然语言到形式化语言的翻译提供了更高效的解决方案。"}}
{"id": "2512.16842", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16842", "abs": "https://arxiv.org/abs/2512.16842", "authors": ["Yuxin Ray Song", "Jinzhou Li", "Rao Fu", "Devin Murphy", "Kaichen Zhou", "Rishi Shiv", "Yaqi Li", "Haoyu Xiong", "Crystal Elaine Owens", "Yilun Du", "Yiyue Luo", "Xianyi Cheng", "Antonio Torralba", "Wojciech Matusik", "Paul Pu Liang"], "title": "OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction", "comment": "https://opentouch-tactile.github.io/", "summary": "The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.", "AI": {"tldr": "提出了首个野外环境下的自我中心全手触觉数据集OpenTouch，包含同步的视频-触觉-姿态数据，并建立了检索和分类基准任务，展示了触觉信号在抓握理解和跨模态对齐中的重要作用。", "motivation": "人类手是与物理世界交互的主要接口，但现有自我中心感知系统难以准确感知接触的时间、位置和力度。缺乏鲁棒的穿戴式触觉传感器，且没有野外环境下的视频-全手触觉对齐数据集，阻碍了视觉感知与物理交互的融合。", "method": "收集并发布了OpenTouch数据集，包含5.1小时同步的视频-触觉-姿态数据，以及2900个带有详细文本标注的精选片段。基于该数据集设计了检索和分类基准任务，评估触觉信号如何增强感知与行动的基础理解。", "result": "触觉信号为抓握理解提供了紧凑而强大的线索，加强了跨模态对齐，并能够从野外视频查询中可靠地检索。数据集和基准任务的发布支持了多模态自我中心感知、具身学习和接触丰富的机器人操作研究。", "conclusion": "OpenTouch数据集填补了视觉感知与物理交互之间的空白，通过提供标注的多模态数据和基准任务，有望推动多模态自我中心感知、具身学习及接触密集型机器人操作的发展。"}}
{"id": "2512.16213", "categories": ["cs.CV", "math.DG"], "pdf": "https://arxiv.org/pdf/2512.16213", "abs": "https://arxiv.org/abs/2512.16213", "authors": ["Amit Vishwakarma", "K. S. Subrahamanian Moosath"], "title": "Enhanced 3D Shape Analysis via Information Geometry", "comment": null, "summary": "Three-dimensional point clouds provide highly accurate digital representations of objects, essential for applications in computer graphics, photogrammetry, computer vision, and robotics. However, comparing point clouds faces significant challenges due to their unstructured nature and the complex geometry of the surfaces they represent. Traditional geometric metrics such as Hausdorff and Chamfer distances often fail to capture global statistical structure and exhibit sensitivity to outliers, while existing Kullback-Leibler (KL) divergence approximations for Gaussian Mixture Models can produce unbounded or numerically unstable values. This paper introduces an information geometric framework for 3D point cloud shape analysis by representing point clouds as Gaussian Mixture Models (GMMs) on a statistical manifold. We prove that the space of GMMs forms a statistical manifold and propose the Modified Symmetric Kullback-Leibler (MSKL) divergence with theoretically guaranteed upper and lower bounds, ensuring numerical stability for all GMM comparisons. Through comprehensive experiments on human pose discrimination (MPI-FAUST dataset) and animal shape comparison (G-PCD dataset), we demonstrate that MSKL provides stable and monotonically varying values that directly reflect geometric variation, outperforming traditional distances and existing KL approximations.", "AI": {"tldr": "本文提出了一种基于信息几何框架的3D点云形状分析方法，通过将点云表示为统计流形上的高斯混合模型，并引入具有理论保证上下界的修正对称KL散度，解决了传统度量方法在点云比较中的局限性。", "motivation": "传统点云比较方法（如Hausdorff和Chamfer距离）难以捕捉全局统计结构且对异常值敏感，而现有的高斯混合模型KL散度近似可能产生无界或不稳定的数值。需要一种更稳定、更能反映几何变化的点云比较方法。", "method": "1. 将点云表示为统计流形上的高斯混合模型；2. 证明GMM空间构成统计流形；3. 提出具有理论保证上下界的修正对称KL散度；4. 在MPI-FAUST和G-PCD数据集上进行人体姿态和动物形状比较实验。", "result": "MSKL散度在所有GMM比较中均保持数值稳定性，其值随几何变化单调变化，在人体姿态判别和动物形状比较任务中优于传统距离方法和现有KL近似方法。", "conclusion": "所提出的信息几何框架为3D点云形状分析提供了稳定有效的数学基础，MSKL散度能够更好地捕捉几何结构变化，为点云比较提供了更可靠的度量标准。"}}
{"id": "2512.16907", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16907", "abs": "https://arxiv.org/abs/2512.16907", "authors": ["Mingfei Chen", "Yifan Wang", "Zhengqin Li", "Homanga Bharadhwaj", "Yujin Chen", "Chuan Qin", "Ziyi Kou", "Yuan Tian", "Eric Whitmire", "Rajinder Sodhi", "Hrvoje Benko", "Eli Shlizerman", "Yue Liu"], "title": "Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos", "comment": "Project website: https://egoman-project.github.io", "summary": "Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.", "AI": {"tldr": "提出EgoMAN数据集和模型，用于结合语义推理与动作生成的3D手部轨迹预测，实现阶段感知的轨迹预测并具有现实场景泛化能力。", "motivation": "现有3D手部轨迹预测研究受限于数据集将动作与语义监督分离，且模型对推理与动作的关联较弱，需要更紧密的结合语义推理与运动生成的方法。", "method": "1) 构建EgoMAN数据集（包含21.9万条6DoF轨迹和300万结构化QA对）；2) 提出EgoMAN模型，通过轨迹令牌接口连接视觉语言推理与运动生成，采用渐进式训练对齐推理与运动动态。", "result": "方法能够生成准确且阶段感知的轨迹，并在现实场景中展现出良好的泛化能力。", "conclusion": "通过结合大规模语义增强数据集与推理-运动联合框架，实现了更紧密关联语义推理与3D手部运动生成的轨迹预测，为具身智能交互提供了新思路。"}}
{"id": "2512.16802", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16802", "abs": "https://arxiv.org/abs/2512.16802", "authors": ["Primož Kocbek", "Azra Frkatović-Hodžić", "Dora Lalić", "Vivian Hui", "Gordan Lauc", "Gregor Štiglic"], "title": "Exploration of Augmentation Strategies in Multi-modal Retrieval-Augmented Generation for the Biomedical Domain: A Case Study Evaluating Question Answering in Glycobiology", "comment": "Will be published in IEEE BigData 2025 proceedings. Contains 10 pages, 1 figure, 5 tables", "summary": "Multi-modal retrieval-augmented generation (MM-RAG) promises grounded biomedical QA, but it is unclear when to (i) convert figures/tables into text versus (ii) use optical character recognition (OCR)-free visual retrieval that returns page images and leaves interpretation to the generator. We study this trade-off in glycobiology, a visually dense domain. We built a benchmark of 120 multiple-choice questions (MCQs) from 25 papers, stratified by retrieval difficulty (easy text, medium figures/tables, hard cross-evidence). We implemented four augmentations-None, Text RAG, Multi-modal conversion, and late-interaction visual retrieval (ColPali)-using Docling parsing and Qdrant indexing. We evaluated mid-size open-source and frontier proprietary models (e.g., Gemma-3-27B-IT, GPT-4o family). Additional testing used the GPT-5 family and multiple visual retrievers (ColPali/ColQwen/ColFlor). Accuracy with Agresti-Coull 95% confidence intervals (CIs) was computed over 5 runs per configuration. With Gemma-3-27B-IT, Text and Multi-modal augmentation outperformed OCR-free retrieval (0.722-0.740 vs. 0.510 average accuracy). With GPT-4o, Multi-modal achieved 0.808, with Text 0.782 and ColPali 0.745 close behind; within-model differences were small. In follow-on experiments with the GPT-5 family, the best results with ColPali and ColFlor improved by ~2% to 0.828 in both cases. In general, across the GPT-5 family, ColPali, ColQwen, and ColFlor were statistically indistinguishable. GPT-5-nano trailed larger GPT-5 variants by roughly 8-10%. Pipeline choice is capacity-dependent: converting visuals to text lowers the reader burden and is more reliable for mid-size models, whereas OCR-free visual retrieval becomes competitive under frontier models. Among retrievers, ColFlor offers parity with heavier options at a smaller footprint, making it an efficient default when strong generators are available.", "AI": {"tldr": "研究比较了多模态检索增强生成（MM-RAG）中两种处理视觉内容（图表）的策略：转换为文本 vs OCR-free视觉检索。在糖生物学领域构建基准测试，发现策略选择取决于模型能力：中等模型更适合文本转换，前沿模型下视觉检索具有竞争力。", "motivation": "多模态检索增强生成在生物医学QA中应用时，缺乏对视觉内容处理策略（文本转换 vs 视觉检索）的系统比较，特别是在视觉密集领域（如糖生物学）中需要明确何时采用何种策略。", "method": "1. 构建包含120道多选题的糖生物学基准，按检索难度分层；2. 实现四种增强策略（无增强、文本RAG、多模态转换、OCR-free视觉检索）；3. 使用Docling解析和Qdrant索引；4. 评估多种开源和专有模型（Gemma-3、GPT-4o/5系列）；5. 采用Agresti-Coull 95%置信区间统计5次运行结果。", "result": "1. Gemma-3-27B-IT：文本/多模态增强（0.722-0.740）显著优于视觉检索（0.510）；2. GPT-4o：多模态最佳（0.808），文本（0.782）和视觉检索（0.745）接近；3. GPT-5系列：视觉检索最佳结果提升至0.828，不同检索器性能无统计差异；4. GPT-5-nano性能落后较大模型8-10%。", "conclusion": "管道选择依赖模型能力：中等模型更适合将视觉内容转换为文本以降低理解负担，而前沿模型下OCR-free视觉检索具有竞争力。ColFlor检索器在保持性能的同时计算开销更小，是强生成器下的高效默认选择。"}}
{"id": "2512.16279", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16279", "abs": "https://arxiv.org/abs/2512.16279", "authors": ["Yiliu Yang", "Yilei Jiang", "Qunzhong Wang", "Yingshui Tan", "Xiaoyong Zhu", "Sherman S. M. Chow", "Bo Zheng", "Xiangyu Yue"], "title": "QuadSentinel: Sequent Safety for Machine-Checkable Control in Multi-agent Systems", "comment": "Preprint", "summary": "Safety risks arise as large language model-based agents solve complex tasks with tools, multi-step plans, and inter-agent messages. However, deployer-written policies in natural language are ambiguous and context dependent, so they map poorly to machine-checkable rules, and runtime enforcement is unreliable. Expressing safety policies as sequents, we propose \\textsc{QuadSentinel}, a four-agent guard (state tracker, policy verifier, threat watcher, and referee) that compiles these policies into machine-checkable rules built from predicates over observable state and enforces them online. Referee logic plus an efficient top-$k$ predicate updater keeps costs low by prioritizing checks and resolving conflicts hierarchically. Measured on ST-WebAgentBench (ICML CUA~'25) and AgentHarm (ICLR~'25), \\textsc{QuadSentinel} improves guardrail accuracy and rule recall while reducing false positives. Against single-agent baselines such as ShieldAgent (ICML~'25), it yields better overall safety control. Near-term deployments can adopt this pattern without modifying core agents by keeping policies separate and machine-checkable. Our code will be made publicly available at https://github.com/yyiliu/QuadSentinel.", "AI": {"tldr": "提出QuadSentinel四智能体防护框架，将自然语言安全策略编译为机器可检查规则，在线执行以提升基于大语言模型的智能体在复杂任务中的安全性。", "motivation": "基于大语言模型的智能体使用工具、多步规划和跨智能体通信解决复杂任务时存在安全风险。部署者用自然语言编写的策略存在模糊性和上下文依赖性，难以映射为机器可检查规则，导致运行时执行不可靠。", "method": "将安全策略表达为序列式规则，设计包含状态追踪器、策略验证器、威胁监视器和裁判的四智能体防护框架QuadSentinel，将策略编译为基于可观测状态谓词的机器可检查规则，通过裁判逻辑和高效top-k谓词更新器实现分层冲突解决和优先级检查以控制成本。", "result": "在ST-WebAgentBench和AgentHarm基准测试中，QuadSentinel提高了防护准确率和规则召回率，降低了误报率；相比ShieldAgent等单智能体基线方法，实现了更好的整体安全控制。", "conclusion": "QuadSentinel框架可在不修改核心智能体的前提下，通过保持策略分离和机器可检查性，为近期部署提供有效的安全防护模式，代码将开源。"}}
{"id": "2512.16832", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16832", "abs": "https://arxiv.org/abs/2512.16832", "authors": ["Aditya Yadavalli", "Tiago Pimentel", "Tamar I Regev", "Ethan Wilcox", "Alex Warstadt"], "title": "What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels", "comment": null, "summary": "Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.", "AI": {"tldr": "提出信息论方法量化韵律单独传达的信息量及其内容，发现音频（韵律）在传达讽刺和情感信息方面比文本多一个数量级的信息。", "motivation": "韵律（语音的旋律）常传达文字未捕获的关键信息，但缺乏量化其独立信息贡献的方法。", "method": "使用大型语音和语言模型估计话语意义维度（如情感）与通信通道（如音频或文本）之间的互信息，应用于电视和播客语音分析讽刺、情感和疑问性。", "result": "对于讽刺和情感，音频通道（隐含韵律）在缺乏长上下文时传达的信息量比文本通道高一个数量级；对于疑问性，韵律的额外信息贡献相对较小。", "conclusion": "韵律是讽刺和情感信息的关键载体，并提出了将方法扩展到更多意义维度、通信通道和语言的后续研究计划。"}}
{"id": "2512.16226", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16226", "abs": "https://arxiv.org/abs/2512.16226", "authors": ["Justin Jiang"], "title": "Image Compression Using Singular Value Decomposition", "comment": null, "summary": "Images are a substantial portion of the internet, making efficient compression important for reducing storage and bandwidth demands. This study investigates the use of Singular Value Decomposition and low-rank matrix approximations for image compression, evaluating performance using relative Frobenius error and compression ratio. The approach is applied to both grayscale and multichannel images to assess its generality. Results show that the low-rank approximations often produce images that appear visually similar to the originals, but the compression efficiency remains consistently worse than established formats such as JPEG, JPEG2000, and WEBP at comparable error levels. At low tolerated error levels, the compressed representation produced by Singular Value Decomposition can even exceed the size of the original image, indicating that this method is not competitive with industry-standard codecs for practical image compression.", "AI": {"tldr": "本研究评估了奇异值分解和低秩矩阵近似在图像压缩中的应用，发现其压缩效率始终低于JPEG、JPEG2000和WEBP等标准格式，且在高保真要求下压缩文件可能大于原图，不具实用竞争力。", "motivation": "图像在互联网中占比巨大，高效压缩对降低存储和带宽需求至关重要。本研究旨在探索基于奇异值分解的低秩近似方法在图像压缩中的潜力。", "method": "使用奇异值分解和低秩矩阵近似进行图像压缩，通过相对Frobenius误差和压缩比评估性能，并将方法应用于灰度图像和多通道图像以验证普适性。", "result": "低秩近似生成的图像视觉上与原图相似，但在相同误差水平下，压缩效率始终低于JPEG、JPEG2000和WEBP等标准格式；在低容忍误差下，压缩文件甚至可能大于原图。", "conclusion": "基于奇异值分解的低秩近似方法在实际图像压缩中无法与行业标准编解码器竞争，不推荐作为实用压缩方案。"}}
{"id": "2512.16295", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16295", "abs": "https://arxiv.org/abs/2512.16295", "authors": ["Zhenyu Wu", "Jingjing Xie", "Zehao Li", "Bowen Yang", "Qiushi Sun", "Zhaoyang Liu", "Zhoumianze Liu", "Yu Qiao", "Xiangyu Yue", "Zun Wang", "Zichen Ding"], "title": "OS-Oracle: A Comprehensive Framework for Cross-Platform GUI Critic Models", "comment": null, "summary": "With VLM-powered computer-using agents (CUAs) becoming increasingly capable at graphical user interface (GUI) navigation and manipulation, reliable step-level decision-making has emerged as a key bottleneck for real-world deployment. In long-horizon workflows, errors accumulate quickly and irreversible actions can cause unintended consequences, motivating critic models that assess each action before execution. While critic models offer a promising solution, their effectiveness is hindered by the lack of diverse, high-quality GUI feedback data and public critic benchmarks for step-level evaluation in computer use. To bridge these gaps, we introduce OS-Oracle that makes three core contributions: (1) a scalable data pipeline for synthesizing cross-platform GUI critic data; (2) a two-stage training paradigm combining supervised fine-tuning (SFT) and consistency-preserving group relative policy optimization (CP-GRPO); (3) OS-Critic Bench, a holistic benchmark for evaluating critic model performance across Mobile, Web, and Desktop platforms. Leveraging this framework, we curate a high-quality dataset containing 310k critic samples. The resulting critic model, OS-Oracle-7B, achieves state-of-the-art performance among open-source VLMs on OS-Critic Bench, and surpasses proprietary models on the mobile domain. Furthermore, when serving as a pre-critic, OS-Oracle-7B improves the performance of native GUI agents such as UI-TARS-1.5-7B in OSWorld and AndroidWorld environments. The code is open-sourced at https://github.com/numbmelon/OS-Oracle.", "AI": {"tldr": "本文介绍了OS-Oracle，一个用于提升计算机使用代理（CUA）决策可靠性的批评模型框架，包括数据合成管道、训练方法和跨平台评测基准。", "motivation": "随着视觉语言模型驱动的计算机使用代理在图形用户界面导航和操作能力增强，长流程任务中的错误累积和不可逆操作风险成为实际部署的关键瓶颈，需要可靠的步骤级决策评估机制。", "method": "提出OS-Oracle框架：1）跨平台GUI批评数据合成管道；2）两阶段训练范式（监督微调+一致性保持的组相对策略优化）；3）OS-Critic Bench跨平台评测基准（移动端、网页端、桌面端）。", "result": "构建了31万高质量批评样本数据集；OS-Oracle-7B模型在OS-Critic Bench上达到开源视觉语言模型最佳性能，在移动领域超越专有模型；作为预批评器可提升UI-TARS-1.5-7B等原生GUI代理在OSWorld和AndroidWorld环境的表现。", "conclusion": "OS-Oracle通过系统化的数据合成、训练方法和评测体系，有效提升了GUI代理的步骤级决策可靠性，为实际部署提供了解决方案，代码已开源。"}}
{"id": "2512.16909", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16909", "abs": "https://arxiv.org/abs/2512.16909", "authors": ["Yuanchen Ju", "Yongyuan Liang", "Yen-Jen Wang", "Nandiraju Gireesh", "Yuanliang Ju", "Seungjae Lee", "Qiao Gu", "Elvis Hsieh", "Furong Huang", "Koushil Sreenath"], "title": "MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning", "comment": "25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/", "summary": "Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.", "AI": {"tldr": "提出MomaGraph统一场景表示法，包含空间功能关系和部件级交互元素，并发布首个大规模任务驱动场景图数据集和评估套件，基于此训练出在多个基准测试中表现优异的7B视觉语言模型。", "motivation": "现有移动机械臂的家庭场景表示方法存在以下问题：空间与功能关系分离、场景视为静态快照忽略物体状态和时序更新、缺乏对当前任务最关键的信息关注。需要一种能同时支持导航和操作的紧凑语义丰富场景表示。", "method": "1. 提出MomaGraph统一场景表示法，整合空间功能关系和部件级交互元素；2. 构建MomaGraph-Scenes大规模家庭环境任务驱动场景图数据集；3. 开发MomaGraph-Bench六种推理能力评估套件；4. 基于数据集通过强化学习训练MomaGraph-R1（7B视觉语言模型），采用“先构图后规划”框架。", "result": "1. MomaGraph-R1在基准测试中达到71.6%准确率，比最佳基线提升11.4%；2. 在公开基准测试中展现良好泛化能力；3. 在真实机器人实验中有效迁移应用；4. 模型在开源模型中达到最先进水平。", "conclusion": "MomaGraph为具身智能体提供了一种有效的统一场景表示方法，相关数据集、评估套件和模型为家庭环境中的移动操作任务提供了全面解决方案，并在多个维度验证了其优越性。"}}
{"id": "2512.16234", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16234", "abs": "https://arxiv.org/abs/2512.16234", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Hesheng Wang", "Ajmal Mian"], "title": "ARMFlow: AutoRegressive MeanFlow for Online 3D Human Reaction Generation", "comment": null, "summary": "3D human reaction generation faces three main challenges:(1) high motion fidelity, (2) real-time inference, and (3) autoregressive adaptability for online scenarios. Existing methods fail to meet all three simultaneously. We propose ARMFlow, a MeanFlow-based autoregressive framework that models temporal dependencies between actor and reactor motions. It consists of a causal context encoder and an MLP-based velocity predictor. We introduce Bootstrap Contextual Encoding (BSCE) in training, encoding generated history instead of the ground-truth ones, to alleviate error accumulation in autoregressive generation. We further introduce the offline variant ReMFlow, achieving state-of-the-art performance with the fastest inference among offline methods. Our ARMFlow addresses key limitations of online settings by: (1) enhancing semantic alignment via a global contextual encoder; (2) achieving high accuracy and low latency in a single-step inference; and (3) reducing accumulated errors through BSCE. Our single-step online generation surpasses existing online methods on InterHuman and InterX by over 40% in FID, while matching offline state-of-the-art performance despite using only partial sequence conditions.", "AI": {"tldr": "提出ARMFlow框架解决3D人体反应生成中高保真、实时推理和自回归适应性的挑战，通过因果上下文编码器和MLP速度预测器实现，引入BSCE训练方法减少误差累积，在线和离线版本均取得优异性能。", "motivation": "现有方法无法同时满足3D人体反应生成的高运动保真度、实时推理能力和在线场景的自回归适应性需求，需要一种能兼顾三者的解决方案。", "method": "采用基于MeanFlow的自回归框架ARMFlow，包含因果上下文编码器和MLP速度预测器；提出Bootstrap Contextual Encoding（BSCE）训练方法，使用生成历史而非真实历史进行编码以减少误差累积；同时开发离线版本ReMFlow。", "result": "ARMFlow在InterHuman和InterX数据集上，单步在线生成的FID指标比现有在线方法提升超过40%，同时匹配离线SOTA性能；ReMFlow在离线方法中达到最快推理速度并取得SOTA性能。", "conclusion": "ARMFlow通过全局上下文编码增强语义对齐、单步推理实现高精度低延迟、BSCE减少累积误差，有效解决了在线3D人体反应生成的关键限制，在保持性能的同时满足实时性要求。"}}
{"id": "2512.16843", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16843", "abs": "https://arxiv.org/abs/2512.16843", "authors": ["Harsh Vardhan Bansal"], "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference", "comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)", "summary": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications", "AI": {"tldr": "提出LLMCache，一种基于语义相似度的层间缓存框架，通过重用Transformer中间激活来加速推理，实现最高3.1倍加速且精度损失<0.5%。", "motivation": "Transformer模型推理延迟高，现有缓存机制（如token级KV缓存）适用范围有限，难以满足实时大规模部署需求。", "method": "1. 设计模型无关的层间缓存框架，支持任意Transformer层缓存；2. 引入轻量级语义指纹匹配机制；3. 提出自适应缓存淘汰策略管理过期数据。", "result": "在BERT/GPT-2模型上，SQuAD、WikiText-103、OpenBookQA任务中实现最高3.1倍推理加速，精度损失低于0.5%。", "conclusion": "LLMCache是一种通用高效的Transformer推理优化方案，适用于编码器和解码器架构，具备实际部署价值。"}}
{"id": "2512.16300", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16300", "abs": "https://arxiv.org/abs/2512.16300", "authors": ["Fanrui Zhang", "Qiang Zhang", "Sizhuo Zhou", "Jianwen Sun", "Chuanhao Li", "Jiaxin Ai", "Yukang Feng", "Yujie Zhang", "Wenjie Li", "Zizhen Li", "Yifan Chang", "Jiawei Liu", "Kaipeng Zhang"], "title": "Code-in-the-Loop Forensics: Agentic Tool Use for Image Forgery Detection", "comment": "11 pages, 6 figures", "summary": "Existing image forgery detection (IFD) methods either exploit low-level, semantics-agnostic artifacts or rely on multimodal large language models (MLLMs) with high-level semantic knowledge. Although naturally complementary, these two information streams are highly heterogeneous in both paradigm and reasoning, making it difficult for existing methods to unify them or effectively model their cross-level interactions. To address this gap, we propose ForenAgent, a multi-round interactive IFD framework that enables MLLMs to autonomously generate, execute, and iteratively refine Python-based low-level tools around the detection objective, thereby achieving more flexible and interpretable forgery analysis. ForenAgent follows a two-stage training pipeline combining Cold Start and Reinforcement Fine-Tuning to enhance its tool interaction capability and reasoning adaptability progressively. Inspired by human reasoning, we design a dynamic reasoning loop comprising global perception, local focusing, iterative probing, and holistic adjudication, and instantiate it as both a data-sampling strategy and a task-aligned process reward. For systematic training and evaluation, we construct FABench, a heterogeneous, high-quality agent-forensics dataset comprising 100k images and approximately 200k agent-interaction question-answer pairs. Experiments show that ForenAgent exhibits emergent tool-use competence and reflective reasoning on challenging IFD tasks when assisted by low-level tools, charting a promising route toward general-purpose IFD. The code will be released after the review process is completed.", "AI": {"tldr": "提出ForenAgent框架，通过多轮交互使多模态大语言模型自主生成、执行并迭代优化基于Python的低级工具，实现更灵活可解释的图像伪造检测。", "motivation": "现有图像伪造检测方法中，低级语义无关的伪影检测与依赖多模态大语言模型的高级语义知识检测高度异构，难以统一建模跨层级交互。", "method": "采用两阶段训练流程（冷启动+强化微调），设计动态推理循环（全局感知、局部聚焦、迭代探测、整体裁决），构建FABench数据集（10万图像、20万交互问答对）。", "result": "实验表明ForenAgent在低级工具辅助下，对挑战性IFD任务展现出新兴工具使用能力和反思推理能力。", "conclusion": "该方法为通用图像伪造检测开辟了有前景的技术路径，代码将在评审完成后开源。"}}
{"id": "2512.16919", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.16919", "abs": "https://arxiv.org/abs/2512.16919", "authors": ["Sicheng Zuo", "Zixun Xie", "Wenzhao Zheng", "Shaoqing Xu", "Fang Li", "Shengyin Jiang", "Long Chen", "Zhi-Xin Yang", "Jiwen Lu"], "title": "DVGT: Driving Visual Geometry Transformer", "comment": "Code is available at https://github.com/wzzheng/DVGT", "summary": "Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.", "AI": {"tldr": "提出DVGT模型，从无位姿多视角图像序列重建全局密集3D点云地图，无需相机参数和显式几何先验，适用于任意相机配置的自动驾驶场景。", "motivation": "现有自动驾驶密集几何感知模型难以适应不同场景和相机配置，且依赖精确相机参数，缺乏灵活性和通用性。", "method": "使用DINO骨干网络提取图像特征，通过交替的视图内局部注意力、跨视图空间注意力和跨帧时间注意力推断几何关系，多头解码生成首帧坐标系下的全局点云和每帧位姿。", "result": "在nuScenes、Waymo等多个驾驶数据集上训练，DVGT在多种场景下显著优于现有模型，可直接预测度量尺度几何信息。", "conclusion": "DVGT无需相机参数和显式3D几何先验，能灵活处理任意相机配置，为自动驾驶提供了一种通用的密集几何感知解决方案。"}}
{"id": "2512.16235", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16235", "abs": "https://arxiv.org/abs/2512.16235", "authors": ["Satya Narayana Panda", "Vaishnavi Kukkala", "Spandana Iyer"], "title": "AI-Powered Dermatological Diagnosis: From Interpretable Models to Clinical Implementation A Comprehensive Framework for Accessible and Trustworthy Skin Disease Detection", "comment": "9 pages, 5 figures, 1 table. Code available at https://github.com/colabre2020/Enhancing-Skin-Disease-Diagnosis", "summary": "Dermatological conditions affect 1.9 billion people globally, yet accurate diagnosis remains challenging due to limited specialist availability and complex clinical presentations. Family history significantly influences skin disease susceptibility and treatment responses, but is often underutilized in diagnostic processes. This research addresses the critical question: How can AI-powered systems integrate family history data with clinical imaging to enhance dermatological diagnosis while supporting clinical trial validation and real-world implementation?\n  We developed a comprehensive multi-modal AI framework that combines deep learning-based image analysis with structured clinical data, including detailed family history patterns. Our approach employs interpretable convolutional neural networks integrated with clinical decision trees that incorporate hereditary risk factors. The methodology includes prospective clinical trials across diverse healthcare settings to validate AI-assisted diagnosis against traditional clinical assessment.\n  In this work, validation was conducted with healthcare professionals to assess AI-assisted outputs against clinical expectations; prospective clinical trials across diverse healthcare settings are proposed as future work. The integrated AI system demonstrates enhanced diagnostic accuracy when family history data is incorporated, particularly for hereditary skin conditions such as melanoma, psoriasis, and atopic dermatitis. Expert feedback indicates potential for improved early detection and more personalized recommendations; formal clinical trials are planned. The framework is designed for integration into clinical workflows while maintaining interpretability through explainable AI mechanisms.", "AI": {"tldr": "开发了一个结合家族史数据和临床图像的多模态AI框架，用于提高皮肤病诊断准确性，特别针对遗传性皮肤病。", "motivation": "全球皮肤病影响19亿人，但准确诊断面临专科医生不足和临床表现复杂的挑战。家族史对疾病易感性和治疗反应有重要影响，但在诊断过程中常未被充分利用。", "method": "开发了多模态AI框架，结合深度学习图像分析和结构化临床数据（包括详细家族史模式）。使用可解释的卷积神经网络与整合遗传风险因素的临床决策树。方法包括在不同医疗环境中进行前瞻性临床试验验证。", "result": "整合家族史数据的AI系统显示出更高的诊断准确性，特别针对黑色素瘤、银屑病和特应性皮炎等遗传性皮肤病。专家反馈表明该系统有潜力改善早期检测和个性化建议。", "conclusion": "该框架设计用于整合到临床工作流程中，同时通过可解释AI机制保持可解释性。计划进行正式临床试验以进一步验证。"}}
{"id": "2512.16301", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16301", "abs": "https://arxiv.org/abs/2512.16301", "authors": ["Pengcheng Jiang", "Jiacheng Lin", "Zhiyi Shi", "Zifeng Wang", "Luxi He", "Yichen Wu", "Ming Zhong", "Peiyang Song", "Qizheng Zhang", "Heng Wang", "Xueqiang Xu", "Hanwen Xu", "Pengrui Han", "Dylan Zhang", "Jiashuo Sun", "Chaoqi Yang", "Kun Qian", "Tian Wang", "Changran Hu", "Manling Li", "Quanzheng Li", "Hao Peng", "Sheng Wang", "Jingbo Shang", "Chao Zhang", "Jiaxuan You", "Liyuan Liu", "Pan Lu", "Yu Zhang", "Heng Ji", "Yejin Choi", "Dawn Song", "Jimeng Sun", "Jiawei Han"], "title": "Adaptation of Agentic AI", "comment": null, "summary": "Cutting-edge agentic AI systems are built on foundation models that can be adapted to plan, reason, and interact with external tools to perform increasingly complex and specialized tasks. As these systems grow in capability and scope, adaptation becomes a central mechanism for improving performance, reliability, and generalization. In this paper, we unify the rapidly expanding research landscape into a systematic framework that spans both agent adaptations and tool adaptations. We further decompose these into tool-execution-signaled and agent-output-signaled forms of agent adaptation, as well as agent-agnostic and agent-supervised forms of tool adaptation. We demonstrate that this framework helps clarify the design space of adaptation strategies in agentic AI, makes their trade-offs explicit, and provides practical guidance for selecting or switching among strategies during system design. We then review the representative approaches in each category, analyze their strengths and limitations, and highlight key open challenges and future opportunities. Overall, this paper aims to offer a conceptual foundation and practical roadmap for researchers and practitioners seeking to build more capable, efficient, and reliable agentic AI systems.", "AI": {"tldr": "本文提出了一个系统框架，将智能体AI系统的适应机制统一分类为智能体适应和工具适应，并进一步细分，以澄清设计空间、明确权衡，并为系统设计提供实用指导。", "motivation": "随着基于基础模型的智能体AI系统在能力和范围上的增长，适应机制成为提升性能、可靠性和泛化性的核心手段，但相关研究缺乏系统化框架。", "method": "构建了一个涵盖智能体适应和工具适应的统一框架，将智能体适应分解为工具执行信号驱动和智能体输出信号驱动两类，将工具适应分解为智能体无关和智能体监督两类，并分析各类代表方法的优劣。", "result": "该框架有助于厘清智能体AI适应策略的设计空间，明确其权衡取舍，为系统设计中选择或切换策略提供实用指导，并识别了关键开放挑战与未来机遇。", "conclusion": "本文为构建更强大、高效和可靠的智能体AI系统提供了概念基础和实践路线图，旨在推动该领域的研究与应用发展。"}}
{"id": "2512.16243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16243", "abs": "https://arxiv.org/abs/2512.16243", "authors": ["Qi Zhang", "Yunfei Gong", "Zhidan Xie", "Zhizi Wang", "Antoni B. Chan", "Hui Huang"], "title": "Semi-Supervised Multi-View Crowd Counting by Ranking Multi-View Fusion Models", "comment": "13 pages, 7 figures, under review", "summary": "Multi-view crowd counting has been proposed to deal with the severe occlusion issue of crowd counting in large and wide scenes. However, due to the difficulty of collecting and annotating multi-view images, the datasets for multi-view counting have a limited number of multi-view frames and scenes. To solve the problem of limited data, one approach is to collect synthetic data to bypass the annotating step, while another is to propose semi- or weakly-supervised or unsupervised methods that demand less multi-view data. In this paper, we propose two semi-supervised multi-view crowd counting frameworks by ranking the multi-view fusion models of different numbers of input views, in terms of the model predictions or the model uncertainties. Specifically, for the first method (vanilla model), we rank the multi-view fusion models' prediction results of different numbers of camera-view inputs, namely, the model's predictions with fewer camera views shall not be larger than the predictions with more camera views. For the second method, we rank the estimated model uncertainties of the multi-view fusion models with a variable number of view inputs, guided by the multi-view fusion models' prediction errors, namely, the model uncertainties with more camera views shall not be larger than those with fewer camera views. These constraints are introduced into the model training in a semi-supervised fashion for multi-view counting with limited labeled data. The experiments demonstrate the advantages of the proposed multi-view model ranking methods compared with other semi-supervised counting methods.", "AI": {"tldr": "提出两种半监督多视角人群计数框架，通过排序不同视角数量的融合模型预测或不确定性，解决多视角数据稀缺问题。", "motivation": "多视角人群计数可缓解大范围场景中的遮挡问题，但多视角数据收集和标注困难，导致数据集规模有限。需要开发数据高效的方法。", "method": "1. 基于预测排序：要求较少视角输入的模型预测不大于较多视角的预测；2. 基于不确定性排序：利用预测误差引导，要求较多视角的模型不确定性不大于较少视角的不确定性。两种约束均以半监督方式融入训练。", "result": "实验表明，所提出的多视角模型排序方法优于其他半监督计数方法。", "conclusion": "通过排序多视角融合模型的预测或不确定性，可在有限标注数据下有效提升多视角人群计数性能，为数据稀缺场景提供实用解决方案。"}}
{"id": "2512.16883", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16883", "abs": "https://arxiv.org/abs/2512.16883", "authors": ["Tzu-Han Lin", "Wei-Lin Chen", "Chen-An Li", "Hung-yi Lee", "Yun-Nung Chen", "Yu Meng"], "title": "AdaSearch: Balancing Parametric Knowledge and Search in Large Language Models via Reinforcement Learning", "comment": "Preprint. Code and artifacts will be uploaded to https://github.com/hank0316/AdaSearch", "summary": "Equipping large language models (LLMs) with search engines via reinforcement learning (RL) has emerged as an effective approach for building search agents. However, overreliance on search introduces unnecessary cost and risks exposure to noisy or malicious content, while relying solely on parametric knowledge risks hallucination. The central challenge is to develop agents that adaptively balance parametric knowledge with external search, invoking search only when necessary. Prior work mitigates search overuse by shaping rewards around the number of tool calls. However, these penalties require substantial reward engineering, provide ambiguous credit assignment, and can be exploited by agents that superficially reduce calls. Moreover, evaluating performance solely through call counts conflates necessary and unnecessary search, obscuring the measurement of true adaptive behavior. To address these limitations, we first quantify the self-knowledge awareness of existing search agents via an F1-based decision metric, revealing that methods such as Search-R1 often overlook readily available parametric knowledge. Motivated by these findings, we propose AdaSearch, a simple two-stage, outcome-driven RL framework that disentangles problem solving from the decision of whether to invoke search, and makes this decision process explicit and interpretable. This transparency is crucial for high-stakes domains such as finance and medical question answering, yet is largely neglected by prior approaches. Experiments across multiple model families and sizes demonstrate that AdaSearch substantially improves knowledge-boundary awareness, reduces unnecessary search calls, preserves strong task performance, and offers more transparent, interpretable decision behaviors.", "AI": {"tldr": "提出AdaSearch框架，通过两阶段强化学习使LLM搜索代理自适应平衡参数知识与外部搜索，减少不必要搜索并提高决策透明度。", "motivation": "现有搜索代理过度依赖外部搜索导致成本高、易受噪声/恶意内容影响，而仅依赖参数知识又易产生幻觉。现有方法通过惩罚搜索次数来减少搜索，但存在奖励工程复杂、信用分配模糊、易被代理表面减少调用规避等问题，且仅通过调用次数评估性能无法区分必要与不必要搜索。", "method": "提出AdaSearch两阶段结果驱动强化学习框架：第一阶段将问题解决与是否调用搜索的决策解耦，第二阶段使决策过程显式化、可解释化。", "result": "实验表明AdaSearch显著提升知识边界意识，减少不必要搜索调用，保持强大任务性能，并提供更透明、可解释的决策行为。", "conclusion": "AdaSearch通过解耦决策与问题解决、提高决策透明度，有效解决了搜索代理自适应平衡参数知识与外部搜索的挑战，特别适用于金融、医疗等高风险领域。"}}
{"id": "2512.16899", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16899", "abs": "https://arxiv.org/abs/2512.16899", "authors": ["Yushi Hu", "Reyhane Askari-Hemmat", "Melissa Hall", "Emily Dinan", "Luke Zettlemoyer", "Marjan Ghazvininejad"], "title": "Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image", "comment": "Code and data available at https://github.com/facebookresearch/MMRB2", "summary": "Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (\"thinking-with-images\"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.", "AI": {"tldr": "提出了首个用于多模态奖励模型的综合基准MMRB2，涵盖文本到图像、图像编辑、交错生成和多模态推理四个任务，包含4000个专家标注的偏好对，用于评估现有奖励模型性能并指导未来改进。", "motivation": "奖励模型对于训练大语言模型至关重要，但在处理交错图像和文本序列的全能模型领域仍缺乏系统研究，需要建立专门的多模态奖励模型评估基准。", "method": "构建MMRB2基准，包含：(1) 实用且具有挑战性的提示；(2) 来自23个最先进模型和代理的响应；(3) 通过集成过滤策略筛选的具有强人类专家共识的偏好对。使用该基准评估多模态LLM作为评判者和人类偏好训练模型的性能。", "result": "Gemini 3 Pro达到75-80%准确率，GPT-5和Gemini 2.5 Pro达到66-75%，优于广泛使用的GPT-4o（59%）。最佳开源模型Qwen3-VL-32B达到与Gemini 2.5 Flash相似的64%准确率。人类专家准确率超过90%。MMRB2性能与下游任务成功强相关。", "conclusion": "MMRB2为多模态奖励模型提供了首个全面评估基准，揭示了当前模型与人类专家之间的显著差距，并确定了奖励模型未来改进的关键方向。"}}
{"id": "2512.16266", "categories": ["cs.CV", "cs.LG", "physics.med-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2512.16266", "abs": "https://arxiv.org/abs/2512.16266", "authors": ["Paloma Casteleiro Costa", "Parnian Ghapandar Kashani", "Xuhui Liu", "Alexander Chen", "Ary Portes", "Julien Bec", "Laura Marcu", "Aydogan Ozcan"], "title": "Pixel Super-Resolved Fluorescence Lifetime Imaging Using Deep Learning", "comment": "30 Pages, 9 Figures", "summary": "Fluorescence lifetime imaging microscopy (FLIM) is a powerful quantitative technique that provides metabolic and molecular contrast, offering strong translational potential for label-free, real-time diagnostics. However, its clinical adoption remains limited by long pixel dwell times and low signal-to-noise ratio (SNR), which impose a stricter resolution-speed trade-off than conventional optical imaging approaches. Here, we introduce FLIM_PSR_k, a deep learning-based multi-channel pixel super-resolution (PSR) framework that reconstructs high-resolution FLIM images from data acquired with up to a 5-fold increased pixel size. The model is trained using the conditional generative adversarial network (cGAN) framework, which, compared to diffusion model-based alternatives, delivers a more robust PSR reconstruction with substantially shorter inference times, a crucial advantage for practical deployment. FLIM_PSR_k not only enables faster image acquisition but can also alleviate SNR limitations in autofluorescence-based FLIM. Blind testing on held-out patient-derived tumor tissue samples demonstrates that FLIM_PSR_k reliably achieves a super-resolution factor of k = 5, resulting in a 25-fold increase in the space-bandwidth product of the output images and revealing fine architectural features lost in lower-resolution inputs, with statistically significant improvements across various image quality metrics. By increasing FLIM's effective spatial resolution, FLIM_PSR_k advances lifetime imaging toward faster, higher-resolution, and hardware-flexible implementations compatible with low-numerical-aperture and miniaturized platforms, better positioning FLIM for translational applications.", "AI": {"tldr": "提出FLIM_PSR_k深度学习框架，通过像素超分辨率技术提升荧光寿命成像的时空分辨率，实现5倍超分辨率重建，显著提高成像速度与信噪比。", "motivation": "荧光寿命成像（FLIM）虽具临床潜力，但受限于长像素驻留时间、低信噪比及分辨率-速度权衡，阻碍其临床转化应用。", "method": "基于条件生成对抗网络（cGAN）构建多通道像素超分辨率框架，从像素尺寸扩大5倍的低分辨率数据中重建高分辨率FLIM图像。", "result": "在患者肿瘤组织样本盲测中，FLIM_PSR_k实现5倍超分辨率因子，空间带宽积提升25倍，显著改善图像质量指标，并揭示低分辨率图像丢失的细微结构特征。", "conclusion": "该框架通过提升FLIM有效空间分辨率，推动寿命成像向更快、更高分辨率、硬件灵活的方向发展，增强其在低数值孔径和小型化平台的兼容性，促进临床转化。"}}
{"id": "2512.16317", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16317", "abs": "https://arxiv.org/abs/2512.16317", "authors": ["Arther Tian", "Alex Ding", "Frank Chen", "Alan Wu", "Aaron Chan", "Bruce Zhang"], "title": "Design and Evaluation of Cost-Aware PoQ for Decentralized LLM Inference", "comment": null, "summary": "Decentralized large language model (LLM) inference promises transparent and censorship resistant access to advanced AI, yet existing verification approaches struggle to scale to modern models. Proof of Quality (PoQ) replaces cryptographic verification of computation with consensus over output quality, but the original formulation ignores heterogeneous computational costs across inference and evaluator nodes. This paper introduces a cost-aware PoQ framework that integrates explicit efficiency measurements into the reward mechanism for both types of nodes. The design combines ground truth token level F1, lightweight learned evaluators, and GPT based judgments within a unified evaluation pipeline, and adopts a linear reward function that balances normalized quality and cost.\n  Experiments on extractive question answering and abstractive summarization use five instruction tuned LLMs ranging from TinyLlama-1.1B to Llama-3.2-3B and three evaluation models spanning cross encoder and bi encoder architectures. Results show that a semantic textual similarity bi encoder achieves much higher correlation with both ground truth and GPT scores than cross encoders, indicating that evaluator architecture is a critical design choice for PoQ. Quality-cost analysis further reveals that the largest models in the pool are also the most efficient in terms of quality per unit latency. Monte Carlo simulations over 5\\,000 PoQ rounds demonstrate that the cost-aware reward scheme consistently assigns higher average rewards to high quality low cost inference models and to efficient evaluators, while penalizing slow low quality nodes. These findings suggest that cost-aware PoQ provides a practical foundation for economically sustainable decentralized LLM inference.", "AI": {"tldr": "本文提出了一种成本感知的Proof of Quality框架，用于去中心化大语言模型推理，通过整合效率测量到奖励机制中，平衡输出质量与计算成本。", "motivation": "现有去中心化LLM推理验证方法难以扩展到现代模型，且传统Proof of Quality忽略了推理节点和评估节点之间的异构计算成本差异。", "method": "设计成本感知PoQ框架，结合真实标记级F1分数、轻量级学习评估器和GPT判断的统一评估流程，采用线性奖励函数平衡归一化质量和成本。实验使用5个指令调优LLM和3种评估模型架构，进行抽取式问答和抽象摘要任务。", "result": "语义文本相似性双编码器比交叉编码器与真实值和GPT分数相关性更高；质量-成本分析显示最大模型在单位延迟质量上最有效；蒙特卡罗模拟表明成本感知奖励方案能持续奖励高质量低成本节点，惩罚低效节点。", "conclusion": "成本感知PoQ为经济可持续的去中心化LLM推理提供了实用基础，评估器架构是关键设计选择，最大模型在质量-效率权衡中表现最优。"}}
{"id": "2512.16270", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16270", "abs": "https://arxiv.org/abs/2512.16270", "authors": ["Rui Gui", "Yang Wan", "Haochen Han", "Dongxing Mao", "Fangming Liu", "Min Li", "Alex Jinpeng Wang"], "title": "TextEditBench: Evaluating Reasoning-aware Text Editing Beyond Rendering", "comment": null, "summary": "Text rendering has recently emerged as one of the most challenging frontiers in visual generation, drawing significant attention from large-scale diffusion and multimodal models. However, text editing within images remains largely unexplored, as it requires generating legible characters while preserving semantic, geometric, and contextual coherence. To fill this gap, we introduce TextEditBench, a comprehensive evaluation benchmark that explicitly focuses on text-centric regions in images. Beyond basic pixel manipulations, our benchmark emphasizes reasoning-intensive editing scenarios that require models to understand physical plausibility, linguistic meaning, and cross-modal dependencies. We further propose a novel evaluation dimension, Semantic Expectation (SE), which measures reasoning ability of model to maintain semantic consistency, contextual coherence, and cross-modal alignment during text editing. Extensive experiments on state-of-the-art editing systems reveal that while current models can follow simple textual instructions, they still struggle with context-dependent reasoning, physical consistency, and layout-aware integration. By focusing evaluation on this long-overlooked yet fundamental capability, TextEditBench establishes a new testing ground for advancing text-guided image editing and reasoning in multimodal generation.", "AI": {"tldr": "本文提出了TextEditBench，一个专注于图像中文本区域编辑的评估基准，强调推理密集型编辑场景，并引入语义期望（SE）作为新的评估维度。", "motivation": "当前文本渲染在视觉生成中备受关注，但图像中的文本编辑仍未被充分探索，因为它需要在生成清晰字符的同时保持语义、几何和上下文的一致性。", "method": "提出了TextEditBench基准，专注于图像中的文本中心区域，强调需要理解物理合理性、语言意义和跨模态依赖的推理密集型编辑场景，并引入语义期望（SE）作为评估维度。", "result": "对最先进的编辑系统进行广泛实验表明，当前模型能够遵循简单的文本指令，但在上下文依赖推理、物理一致性和布局感知整合方面仍存在困难。", "conclusion": "TextEditBench通过关注这一长期被忽视但基本的能力，为推进文本引导的图像编辑和多模态生成中的推理建立了新的测试平台。"}}
{"id": "2512.16902", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16902", "abs": "https://arxiv.org/abs/2512.16902", "authors": ["Eric Todd", "Jannik Brinkmann", "Rohit Gandikota", "David Bau"], "title": "In-Context Algebra", "comment": "28 pages, 18 figures. Code and data at https://algebra.baulab.info", "summary": "We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.", "AI": {"tldr": "本文研究了Transformer在变量符号含义随序列变化的算术任务中的机制，发现模型能学习符号推理机制，而非仅依赖几何嵌入。", "motivation": "先前研究发现Transformer在处理固定含义的算术符号时会形成几何嵌入，但未探索符号含义随上下文变化的场景。本文旨在探究当符号作为变量（含义随序列变化）时，Transformer是否能发展出不同的推理机制。", "method": "设计新任务：符号与代数群元素的对应关系随序列变化；训练Transformer完成该任务；设计针对性数据分布以因果测试假设机制；分析模型内部机制。", "result": "模型在任务上达到接近完美的准确率，并能泛化到未见过的代数群；识别出三种稳定学习到的机制：交换复制（专用头复制答案）、单位元识别（区分含单位元的事实）、基于封闭性的消去（跟踪群成员以约束有效答案）。", "conclusion": "与固定符号设置中的几何表示互补，当训练模型在上下文推理中处理含义不固定的变量时，模型会发展出符号推理机制。"}}
{"id": "2512.16344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16344", "abs": "https://arxiv.org/abs/2512.16344", "authors": ["Peter Coveney", "Roger Highfield"], "title": "AI Needs Physics More Than Physics Needs AI", "comment": null, "summary": "Artificial intelligence (AI) is commonly depicted as transformative. Yet, after more than a decade of hype, its measurable impact remains modest outside a few high-profile scientific and commercial successes. The 2024 Nobel Prizes in Chemistry and Physics recognized AI's potential, but broader assessments indicate the impact to date is often more promotional than technical. We argue that while current AI may influence physics, physics has significantly more to offer this generation of AI. Current architectures - large language models, reasoning models, and agentic AI - can depend on trillions of meaningless parameters, suffer from distributional bias, lack uncertainty quantification, provide no mechanistic insights, and fail to capture even elementary scientific laws. We review critiques of these limits, highlight opportunities in quantum AI and analogue computing, and lay down a roadmap for the adoption of 'Big AI': a synthesis of theory-based rigour with the flexibility of machine learning.", "AI": {"tldr": "论文认为当前AI的实际影响有限，提出物理能为AI提供更多价值，并规划了结合理论严谨性与机器学习灵活性的'大AI'路线图。", "motivation": "尽管AI被广泛宣传为变革性技术，但其实际可衡量的影响在少数成功案例外仍有限，且当前AI架构存在参数无意义、分布偏差、缺乏不确定性量化等根本缺陷。", "method": "通过回顾对当前AI局限性的批评，分析量子AI和模拟计算中的机遇，并提出理论驱动与数据驱动方法相结合的'大AI'框架。", "result": "指出当前AI（如大语言模型、推理模型）无法捕捉基本科学规律，而物理学能为AI发展提供更实质性的贡献，特别是在解决可解释性、鲁棒性和理论一致性方面。", "conclusion": "呼吁发展'大AI'——一种将理论严谨性与机器学习灵活性相结合的新范式，以克服当前AI的局限性并实现更实质性的科学影响。"}}
{"id": "2512.16914", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16914", "abs": "https://arxiv.org/abs/2512.16914", "authors": ["Nikhil Prakash", "Donghao Ren", "Dominik Moritz", "Yannick Assogba"], "title": "Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates", "comment": "18 pages, 3 figures", "summary": "Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.", "AI": {"tldr": "提出了一种名为构造性电路放大的新方法，通过识别关键令牌和任务相关模型组件，仅更新稀疏的组件子集来增强特定任务能力，在数学推理任务中显著提升准确率且对其他能力影响最小。", "motivation": "现有研究发现LLMs中存在负责特定任务的稀疏子网络（电路），且微调通常通过强化现有电路提升性能，这启发了直接干预电路进行精准任务定向更新的可能性。", "method": "构造性电路放大方法：从模型推理轨迹中识别关键令牌，定位负责目标任务的模型组件，仅更新这些稀疏组件（最低仅更新1.59%的组件）。", "result": "在数学推理任务中，多个模型的准确率最高提升+11.4%；在MMLU、TriviaQA和TruthfulQA等基准测试中，对其他能力的影响极小。", "conclusion": "通过选择性更新稀疏的模型组件，可以可靠地增强目标能力，这为高效、精准的模型编辑提供了新途径。"}}
{"id": "2512.16392", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16392", "abs": "https://arxiv.org/abs/2512.16392", "authors": ["Mohammad-Javad Rezaei", "Mozafar Bag-Mohammadi"], "title": "PCIA: A Path Construction Imitation Algorithm for Global Optimization", "comment": null, "summary": "In this paper, a new metaheuristic optimization algorithm, called Path Construction Imitation Algorithm (PCIA), is proposed. PCIA is inspired by how humans construct new paths and use them. Typically, humans prefer popular transportation routes. In the event of a path closure, a new route is built by mixing the existing paths intelligently. Also, humans select different pathways on a random basis to reach unknown destinations. PCIA generates a random population to find the best route toward the destination, similar to swarm-based algorithms. Each particle represents a path toward the destination. PCIA has been tested with 53 mathematical optimization problems and 13 constrained optimization problems. The results showed that the PCIA is highly competitive compared to both popular and the latest metaheuristic algorithms.", "AI": {"tldr": "提出了一种新的元启发式优化算法——路径构建模仿算法（PCIA），受人类构建和使用路径的行为启发，在53个数学优化问题和13个约束优化问题上表现出色。", "motivation": "受人类在路径选择中的智能行为启发，如偏好流行路线、在路径封闭时混合现有路线构建新路径、随机探索未知目的地等，旨在开发一种更有效的优化算法。", "method": "PCIA模拟人类路径构建行为，生成随机种群（类似群智能算法），每个粒子代表一条通往目标的路径，通过智能混合现有路径和随机探索来优化搜索过程。", "result": "在53个数学优化问题和13个约束优化问题上的测试表明，PCIA与当前流行及最新的元启发式算法相比具有高度竞争力。", "conclusion": "PCIA是一种有效的元启发式优化算法，其基于人类路径构建行为的灵感使其在解决优化问题时表现优异，具有实际应用潜力。"}}
{"id": "2512.16275", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16275", "abs": "https://arxiv.org/abs/2512.16275", "authors": ["Mohamed Abouagour", "Eleftherios Garyfallidis"], "title": "GFLAN: Generative Functional Layouts", "comment": "21 pages, 15 figures", "summary": "Automated floor plan generation lies at the intersection of combinatorial search, geometric constraint satisfaction, and functional design requirements -- a confluence that has historically resisted a unified computational treatment. While recent deep learning approaches have improved the state of the art, they often struggle to capture architectural reasoning: the precedence of topological relationships over geometric instantiation, the propagation of functional constraints through adjacency networks, and the emergence of circulation patterns from local connectivity decisions. To address these fundamental challenges, this paper introduces GFLAN, a generative framework that restructures floor plan synthesis through explicit factorization into topological planning and geometric realization. Given a single exterior boundary and a front-door location, our approach departs from direct pixel-to-pixel or wall-tracing generation in favor of a principled two-stage decomposition. Stage A employs a specialized convolutional architecture with dual encoders -- separating invariant spatial context from evolving layout state -- to sequentially allocate room centroids within the building envelope via discrete probability maps over feasible placements. Stage B constructs a heterogeneous graph linking room nodes to boundary vertices, then applies a Transformer-augmented graph neural network (GNN) that jointly regresses room boundaries.", "AI": {"tldr": "提出GFLAN框架，将平面图生成分解为拓扑规划和几何实现两阶段，通过专门架构解决传统方法难以捕捉建筑推理的问题。", "motivation": "现有深度学习方法难以捕捉建筑推理中的拓扑关系优先性、功能约束传播和交通模式涌现等关键问题，需要更结构化的生成方法。", "method": "两阶段框架：阶段A使用双编码器卷积网络在边界内顺序分配房间中心点；阶段B构建异构图并通过Transformer增强的图神经网络联合回归房间边界。", "result": "GFLAN通过显式分解实现了更符合建筑逻辑的平面图生成，能够处理拓扑约束和几何实现的复杂交互。", "conclusion": "将平面图生成分解为拓扑规划和几何实现两个阶段，为自动化设计提供了更结构化、可解释的生成框架。"}}
{"id": "2512.16553", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.16553", "abs": "https://arxiv.org/abs/2512.16553", "authors": ["Yumeng Wang", "Tianyu Fan", "Lingrui Xu", "Chao Huang"], "title": "Needle in the Web: A Benchmark for Retrieving Targeted Web Pages in the Wild", "comment": "Data and code are available at https://github.com/Tango-Whiskyman/Needle_in_the_Web", "summary": "Large Language Models (LLMs) have evolved from simple chatbots into sophisticated agents capable of automating complex real-world tasks, where browsing and reasoning over live web content is key to assessing retrieval and cognitive skills. Existing benchmarks like BrowseComp and xBench-DeepSearch emphasize complex reasoning searches requiring multi-hop synthesis but neglect Fuzzy Exploratory Search, namely queries that are vague and multifaceted, where users seek the most relevant webpage rather than a single factual answer. To address this gap, we introduce Needle in the Web, a novel benchmark specifically designed to evaluate modern search agents and LLM-based systems on their ability to retrieve and reason over real-world web content in response to ambiguous, exploratory queries under varying levels of difficulty. Needle in the Web comprises 663 questions spanning seven distinct domains. To ensure high query quality and answer uniqueness, we employ a flexible methodology that reliably generates queries of controllable difficulty based on factual claims of web contents. We benchmark three leading LLMs and three agent-based search systems on Needle in the Web, finding that most models struggle: many achieve below 35% accuracy, and none consistently excel across domains or difficulty levels. These findings reveal that Needle in the Web presents a significant challenge for current search systems and highlights the open problem of effective fuzzy retrieval under semantic ambiguity.", "AI": {"tldr": "提出了名为'Needle in the Web'的新基准测试，专门评估搜索代理和LLM系统在模糊探索性查询下的网页检索与推理能力，发现现有系统在此任务上表现不佳。", "motivation": "现有基准测试（如BrowseComp和xBench-DeepSearch）侧重于需要多跳推理的复杂搜索，但忽略了模糊探索性搜索——即用户查询模糊多面、旨在寻找最相关网页而非单一事实答案的场景。", "method": "采用灵活的方法论，基于网页内容的事实声明生成可控难度的查询，构建了包含663个问题、覆盖七个领域的基准测试集，并对三个领先LLM和三个基于代理的搜索系统进行了评估。", "result": "大多数模型表现不佳：许多准确率低于35%，且没有模型能在所有领域或难度级别上 consistently 表现优异。", "conclusion": "Needle in the Web对当前搜索系统构成显著挑战，凸显了在语义模糊下进行有效模糊检索这一开放性问题。"}}
{"id": "2512.16424", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16424", "abs": "https://arxiv.org/abs/2512.16424", "authors": ["Nguyen Xuan-Vu", "Daniel Armstrong", "Milena Wehrbach", "Andres M Bran", "Zlatko Jončev", "Philippe Schwaller"], "title": "Synthelite: Chemist-aligned and feasibility-aware synthesis planning with LLMs", "comment": null, "summary": "Computer-aided synthesis planning (CASP) has long been envisioned as a complementary tool for synthetic chemists. However, existing frameworks often lack mechanisms to allow interaction with human experts, limiting their ability to integrate chemists' insights. In this work, we introduce Synthelite, a synthesis planning framework that uses large language models (LLMs) to directly propose retrosynthetic transformations. Synthelite can generate end-to-end synthesis routes by harnessing the intrinsic chemical knowledge and reasoning capabilities of LLMs, while allowing expert intervention through natural language prompts. Our experiments demonstrate that Synthelite can flexibly adapt its planning trajectory to diverse user-specified constraints, achieving up to 95\\% success rates in both strategy-constrained and starting-material-constrained synthesis tasks. Additionally, Synthelite exhibits the ability to account for chemical feasibility during route design. We envision Synthelite to be both a useful tool and a step toward a paradigm where LLMs are the central orchestrators of synthesis planning.", "AI": {"tldr": "提出了Synthelite框架，利用大语言模型直接进行逆合成分析，支持自然语言交互，在约束条件下合成路线规划成功率高达95%。", "motivation": "现有计算机辅助合成规划工具缺乏与化学家交互的机制，难以整合专家经验，限制了实际应用价值。", "method": "基于大语言模型构建Synthelite框架，利用LLM的化学知识和推理能力直接生成逆合成转化步骤，支持自然语言提示进行专家干预。", "result": "实验表明Synthelite能灵活适应多种用户约束条件，在策略约束和起始物料约束的合成任务中成功率高达95%，且在路线设计中能考虑化学可行性。", "conclusion": "Synthelite既是实用工具，也代表了以大语言模型为核心协调器的合成规划新范式发展方向。"}}
{"id": "2512.16444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16444", "abs": "https://arxiv.org/abs/2512.16444", "authors": ["Yadong Li", "Tong Zhang", "Bo Huang", "Zhen Cui"], "title": "StarCraft+: Benchmarking Multi-agent Algorithms in Adversary Paradigm", "comment": "15 pages, 11 figures", "summary": "Deep multi-agent reinforcement learning (MARL) algorithms are booming in the field of collaborative intelligence, and StarCraft multi-agent challenge (SMAC) is widely-used as the benchmark therein. However, imaginary opponents of MARL algorithms are practically configured and controlled in a fixed built-in AI mode, which causes less diversity and versatility in algorithm evaluation. To address this issue, in this work, we establish a multi-agent algorithm-vs-algorithm environment, named StarCraft II battle arena (SC2BA), to refresh the benchmarking of MARL algorithms in an adversary paradigm. Taking StarCraft as infrastructure, the SC2BA environment is specifically created for inter-algorithm adversary with the consideration of fairness, usability and customizability, and meantime an adversarial PyMARL (APyMARL) library is developed with easy-to-use interfaces/modules. Grounding in SC2BA, we benchmark those classic MARL algorithms in two types of adversarial modes: dual-algorithm paired adversary and multi-algorithm mixed adversary, where the former conducts the adversary of pairwise algorithms while the latter focuses on the adversary to multiple behaviors from a group of algorithms. The extensive benchmark experiments exhibit some thought-provoking observations/problems in the effectivity, sensibility and scalability of these completed algorithms. The SC2BA environment as well as reproduced experiments are released in \\href{https://github.com/dooliu/SC2BA}{Github}, and we believe that this work could mark a new step for the MARL field in the coming years.", "AI": {"tldr": "本文提出了SC2BA环境，一个用于多智能体强化学习算法对抗评估的新基准，解决了现有SMAC基准中对手多样性不足的问题。", "motivation": "现有SMAC基准中的对手配置固定、缺乏多样性，导致算法评估不够全面和真实，需要一个新的对抗性评估环境来提升MARL算法的评测效果。", "method": "基于星际争霸II构建了SC2BA环境，开发了APyMARL库，设计了双算法对抗和多算法混合对抗两种模式，并进行了广泛的基准实验。", "result": "实验揭示了现有MARL算法在有效性、敏感性和可扩展性方面存在一些值得思考的问题，SC2BA环境为算法评估提供了更丰富的对抗场景。", "conclusion": "SC2BA环境为MARL领域提供了一个公平、易用、可定制的新基准，有望推动该领域在对抗性评估方面的发展。"}}
{"id": "2512.16303", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16303", "abs": "https://arxiv.org/abs/2512.16303", "authors": ["Feng Liang", "Sizhe Cheng", "Chenqi Yi"], "title": "PixelArena: A benchmark for Pixel-Precision Visual Intelligence", "comment": "7 pages, 11 figures, project page: https://pixelarena.reify.ing/project", "summary": "Multi-modal large language models that have image output are emerging. Many image generation benchmarks focus on aesthetics instead of fine-grained generation capabilities. In PixelArena, we propose using semantic segmentation tasks to objectively examine their fine-grained generative intelligence with pixel precision. We find the latest Gemini 3 Pro Image has emergent image generation capabilities that generate semantic masks with high fidelity under zero-shot settings, showcasing visual intelligence unseen before and true generalization in new image generation tasks. We further investigate its results, compare them qualitatively and quantitatively with those of other models, and present failure cases. The findings not only signal exciting progress in the field but also provide insights into future research related to multimodality, reasoning, interpretability and benchmarking.", "AI": {"tldr": "本文提出PixelArena基准，利用语义分割任务评估多模态大语言模型的细粒度图像生成能力，发现Gemini 3 Pro Image在零样本设置下能生成高保真语义掩码，展现了前所未有的视觉智能。", "motivation": "现有图像生成基准多关注美学而非细粒度生成能力，需要一种客观评估模型像素级生成智能的方法。", "method": "提出PixelArena基准，使用语义分割任务评估模型；在零样本设置下测试Gemini 3 Pro Image等模型；通过定性和定量分析比较结果；展示失败案例。", "result": "Gemini 3 Pro Image展现出新兴的图像生成能力，能生成高保真语义掩码，在细粒度生成任务中表现出真正的泛化能力。", "conclusion": "该发现标志着多模态领域的重大进展，为未来多模态、推理、可解释性和基准测试研究提供了重要见解。"}}
{"id": "2512.16313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16313", "abs": "https://arxiv.org/abs/2512.16313", "authors": ["Haiyu Zhao", "Yiwen Shan", "Yuanbiao Gou", "Xi Peng"], "title": "LaverNet: Lightweight All-in-one Video Restoration via Selective Propagation", "comment": null, "summary": "Recent studies have explored all-in-one video restoration, which handles multiple degradations with a unified model. However, these approaches still face two challenges when dealing with time-varying degradations. First, the degradation can dominate temporal modeling, confusing the model to focus on artifacts rather than the video content. Second, current methods typically rely on large models to handle all-in-one restoration, concealing those underlying difficulties. To address these challenges, we propose a lightweight all-in-one video restoration network, LaverNet, with only 362K parameters. To mitigate the impact of degradations on temporal modeling, we introduce a novel propagation mechanism that selectively transmits only degradation-agnostic features across frames. Through LaverNet, we demonstrate that strong all-in-one restoration can be achieved with a compact network. Despite its small size, less than 1\\% of the parameters of existing models, LaverNet achieves comparable, even superior performance across benchmarks.", "AI": {"tldr": "提出轻量级全合一视频修复网络LaverNet，仅362K参数，通过选择性传播机制处理时变退化，性能优于现有大模型。", "motivation": "现有全合一视频修复方法在处理时变退化时面临两个挑战：退化干扰时间建模，以及过度依赖大模型掩盖根本问题。", "method": "设计LaverNet轻量网络，引入选择性传播机制，仅跨帧传输与退化无关的特征，避免退化主导时间建模。", "result": "LaverNet参数量不足现有模型的1%，但在多个基准测试中达到相当甚至更优的性能。", "conclusion": "通过轻量网络和选择性特征传播，可实现高效的全合一视频修复，证明强性能不一定需要大模型。"}}
{"id": "2512.16442", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16442", "abs": "https://arxiv.org/abs/2512.16442", "authors": ["Allard Oelen", "Sören Auer"], "title": "TIB AIssistant: a Platform for AI-Supported Research Across Research Life Cycles", "comment": null, "summary": "The rapidly growing popularity of adopting Artificial Intelligence (AI), and specifically Large Language Models (LLMs), is having a widespread impact throughout society, including the academic domain. AI-supported research has the potential to support researchers with tasks across the entire research life cycle. In this work, we demonstrate the TIB AIssistant, an AI-supported research platform providing support throughout the research life cycle. The AIssistant consists of a collection of assistants, each responsible for a specific research task. In addition, tools are provided to give access to external scholarly services. Generated data is stored in the assets and can be exported as an RO-Crate bundle to provide transparency and enhance reproducibility of the research project. We demonstrate the AIssistant's main functionalities by means of a sequential walk-through of assistants, interacting with each other to generate sections for a draft research paper. In the end, with the AIssistant, we lay the foundation for a larger agenda of providing a community-maintained platform for AI-supported research.", "AI": {"tldr": "介绍了TIB AIssistant平台，这是一个基于AI的研究辅助系统，旨在支持整个研究生命周期，通过多个助手和工具促进研究过程，并注重透明度和可重复性。", "motivation": "人工智能（尤其是大语言模型）的普及正在深刻影响学术界，AI辅助研究有望在整个研究生命周期中为研究人员提供支持，因此需要开发集成化平台来系统化地实现这一目标。", "method": "开发了TIB AIssistant平台，包含多个专门负责特定研究任务的助手，提供访问外部学术服务的工具，并将生成的数据存储在资产中，支持以RO-Crate格式导出以增强透明度和可重复性。", "result": "通过顺序演示各助手之间的交互，展示了平台如何协作生成研究论文草稿的部分内容，验证了其核心功能的有效性。", "conclusion": "该平台为AI辅助研究奠定了基础，旨在推动建设一个由社区维护的AI研究支持平台，促进学术研究的系统化协作与可重复性。"}}
{"id": "2512.16294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16294", "abs": "https://arxiv.org/abs/2512.16294", "authors": ["Amna Amir", "Erchan Aptoula"], "title": "MACL: Multi-Label Adaptive Contrastive Learning Loss for Remote Sensing Image Retrieval", "comment": null, "summary": "Semantic overlap among land-cover categories, highly imbalanced label distributions, and complex inter-class co-occurrence patterns constitute significant challenges for multi-label remote-sensing image retrieval. In this article, Multi-Label Adaptive Contrastive Learning (MACL) is introduced as an extension of contrastive learning to address them. It integrates label-aware sampling, frequency-sensitive weighting, and dynamic-temperature scaling to achieve balanced representation learning across both common and rare categories. Extensive experiments on three benchmark datasets (DLRSD, ML-AID, and WHDLD), show that MACL consistently outperforms contrastive-loss based baselines, effectively mitigating semantic imbalance and delivering more reliable retrieval performance in large-scale remote-sensing archives. Code, pretrained models, and evaluation scripts will be released at https://github.com/amna/MACL upon acceptance.", "AI": {"tldr": "提出多标签自适应对比学习（MACL）方法，通过标签感知采样、频率敏感加权和动态温度缩放技术，解决遥感图像检索中的语义重叠、标签分布不平衡和复杂类别共现问题，在三个基准数据集上验证了其优越性。", "motivation": "遥感图像多标签检索面临语义类别重叠、标签分布高度不平衡以及复杂类别共现模式等挑战，传统对比学习方法难以有效处理这些语义不平衡问题。", "method": "提出MACL框架，集成三种核心技术：1）标签感知采样策略；2）频率敏感加权机制；3）动态温度缩放方法，以实现对常见和稀有类别的平衡表示学习。", "result": "在DLRSD、ML-AID和WHDLD三个基准数据集上的实验表明，MACL持续优于基于对比损失的基线方法，有效缓解语义不平衡问题，在大规模遥感档案库中提供更可靠的检索性能。", "conclusion": "MACL通过自适应对比学习机制成功解决了多标签遥感图像检索中的语义不平衡挑战，为大规模遥感数据检索提供了有效的解决方案，相关代码和模型将开源发布。"}}
{"id": "2512.16917", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16917", "abs": "https://arxiv.org/abs/2512.16917", "authors": ["Qihao Liu", "Luoxin Ye", "Wufei Ma", "Yu-Cheng Chou", "Alan Yuille"], "title": "Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.", "AI": {"tldr": "提出生成对抗推理器框架，通过对抗强化学习联合训练推理器和判别器，提升大语言模型的数学推理能力", "motivation": "现有大语言模型在数学推理中仍存在计算错误、逻辑脆弱和表面合理但无效的步骤等过程错误，需要改进推理过程的可靠性和准确性", "method": "采用基于策略的联合训练框架，将推理链分割为逻辑完整的片段，由判别器评估每个片段的合理性并生成结构化理由，推理器和判别器通过互补的奖励信号共同进化", "result": "在多个数学基准测试中取得显著提升，特别是在AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提升至61.3，DeepSeek-R1-Distill-Llama-8B从43.7提升至53.7", "conclusion": "该方法通过密集、校准良好的步骤级奖励补充稀疏的精确匹配信号，改善了信用分配，提高了样本效率，增强了LLMs的整体推理质量，且模块化判别器支持灵活的奖励塑造"}}
{"id": "2512.16314", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16314", "abs": "https://arxiv.org/abs/2512.16314", "authors": ["Huayu Huang", "Chen Chen", "Banglei Guan", "Ze Tan", "Yang Shang", "Zhang Li", "Qifeng Yu"], "title": "Ridge Estimation-Based Vision and Laser Ranging Fusion Localization Method for UAVs", "comment": null, "summary": "Tracking and measuring targets using a variety of sensors mounted on UAVs is an effective means to quickly and accurately locate the target. This paper proposes a fusion localization method based on ridge estimation, combining the advantages of rich scene information from sequential imagery with the high precision of laser ranging to enhance localization accuracy. Under limited conditions such as long distances, small intersection angles, and large inclination angles, the column vectors of the design matrix have serious multicollinearity when using the least squares estimation algorithm. The multicollinearity will lead to ill-conditioned problems, resulting in significant instability and low robustness. Ridge estimation is introduced to mitigate the serious multicollinearity under the condition of limited observation. Experimental results demonstrate that our method achieves higher localization accuracy compared to ground localization algorithms based on single information. Moreover, the introduction of ridge estimation effectively enhances the robustness, particularly under limited observation conditions.", "AI": {"tldr": "提出一种基于岭估计的融合定位方法，结合序列影像的场景信息和激光测距的高精度，提升无人机多传感器目标定位的准确性和鲁棒性。", "motivation": "在无人机多传感器目标定位中，远距离、小交会角和大倾角等有限观测条件下，最小二乘估计的设计矩阵存在严重多重共线性，导致解算不稳定、鲁棒性差。", "method": "采用岭估计方法替代最小二乘估计，缓解有限观测条件下的多重共线性问题，融合序列影像的场景信息和激光测距数据。", "result": "实验表明，该方法相比基于单一信息的地面定位算法具有更高定位精度；岭估计的引入显著提升了系统鲁棒性，尤其在有限观测条件下效果明显。", "conclusion": "基于岭估计的融合定位方法能有效解决有限观测条件下的多重共线性问题，提高无人机多传感器目标定位的精度和稳定性。"}}
{"id": "2512.16447", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16447", "abs": "https://arxiv.org/abs/2512.16447", "authors": ["Sören Auer", "Allard Oelen", "Mohamad Yaser Jaradeh", "Mutahira Khalid", "Farhana Keya", "Sasi Kiran Gaddipati", "Jennifer D'Souza", "Lorenz Schlüter", "Amirreza Alasti", "Gollam Rabby", "Azanzi Jiomekong", "Oliver Karras"], "title": "Towards AI-Supported Research: a Vision of the TIB AIssistant", "comment": null, "summary": "The rapid advancements in Generative AI and Large Language Models promise to transform the way research is conducted, potentially offering unprecedented opportunities to augment scholarly workflows. However, effectively integrating AI into research remains a challenge due to varying domain requirements, limited AI literacy, the complexity of coordinating tools and agents, and the unclear accuracy of Generative AI in research. We present the vision of the TIB AIssistant, a domain-agnostic human-machine collaborative platform designed to support researchers across disciplines in scientific discovery, with AI assistants supporting tasks across the research life cycle. The platform offers modular components - including prompt and tool libraries, a shared data store, and a flexible orchestration framework - that collectively facilitate ideation, literature analysis, methodology development, data analysis, and scholarly writing. We describe the conceptual framework, system architecture, and implementation of an early prototype that demonstrates the feasibility and potential impact of our approach.", "AI": {"tldr": "提出TIB AIssistant平台愿景，这是一个领域无关的人机协作平台，旨在通过AI助手支持跨学科研究生命周期中的各项任务，解决AI集成到研究中的挑战。", "motivation": "生成式AI和大型语言模型的快速发展有望改变研究方式，但AI在研究中的有效集成面临领域需求差异、AI素养有限、工具协调复杂以及生成式AI准确性不明确等挑战。", "method": "设计模块化平台组件，包括提示和工具库、共享数据存储和灵活编排框架，支持构思、文献分析、方法开发、数据分析和学术写作。描述了概念框架、系统架构和早期原型实现。", "result": "早期原型证明了该方法的可行性和潜在影响，展示了如何通过模块化设计促进研究生命周期中的AI辅助任务。", "conclusion": "TIB AIssistant平台为跨学科研究提供了人机协作的可行路径，通过模块化组件和灵活架构应对AI集成挑战，有望增强科研工作流程。"}}
{"id": "2512.16453", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16453", "abs": "https://arxiv.org/abs/2512.16453", "authors": ["Jiayang Yang", "Chunhui Zhao", "Martin Guay", "Zhixing Cao"], "title": "TimeSeries2Report prompting enables adaptive large language model management of lithium-ion batteries", "comment": null, "summary": "Large language models (LLMs) offer promising capabilities for interpreting multivariate time-series data, yet their application to real-world battery energy storage system (BESS) operation and maintenance remains largely unexplored. Here, we present TimeSeries2Report (TS2R), a prompting framework that converts raw lithium-ion battery operational time-series into structured, semantically enriched reports, enabling LLMs to reason, predict, and make decisions in BESS management scenarios. TS2R encodes short-term temporal dynamics into natural language through a combination of segmentation, semantic abstraction, and rule-based interpretation, effectively bridging low-level sensor signals with high-level contextual insights. We benchmark TS2R across both lab-scale and real-world datasets, evaluating report quality and downstream task performance in anomaly detection, state-of-charge prediction, and charging/discharging management. Compared with vision-, embedding-, and text-based prompting baselines, report-based prompting via TS2R consistently improves LLM performance in terms of across accuracy, robustness, and explainability metrics. Notably, TS2R-integrated LLMs achieve expert-level decision quality and predictive consistency without retraining or architecture modification, establishing a practical path for adaptive, LLM-driven battery intelligence.", "AI": {"tldr": "提出了TimeSeries2Report（TS2R）框架，将锂离子电池运行时间序列转换为结构化报告，使大语言模型能在电池储能系统管理中执行推理、预测和决策任务，显著提升了模型性能。", "motivation": "大语言模型在多元时间序列数据解释方面具有潜力，但其在真实世界电池储能系统运维中的应用尚未充分探索，需要一种方法将低层传感器信号与高层语义理解相连接。", "method": "开发了TS2R提示框架，通过分割、语义抽象和基于规则的解释，将短期时间动态编码为自然语言报告，并在实验室和真实数据集上进行了基准测试，评估了异常检测、荷电状态预测和充放电管理等任务。", "result": "与基于视觉、嵌入和文本的提示基线相比，TS2R的报告提示方法在准确性、鲁棒性和可解释性方面持续提升大语言模型性能，实现了专家级决策质量和预测一致性，且无需重新训练或修改架构。", "conclusion": "TS2R为自适应的大语言模型驱动电池智能提供了实用路径，通过语义丰富的报告有效弥合了原始传感器数据与高层管理决策之间的鸿沟。"}}
{"id": "2512.16325", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16325", "abs": "https://arxiv.org/abs/2512.16325", "authors": ["Nan Zhou", "Zuxin Li", "Fanhang Man", "Xuecheng Chen", "Susu Xu", "Fan Dang", "Chaopeng Hong", "Yunhao Liu", "Xiao-Ping Zhang", "Xinlei Chen"], "title": "QUIDS: Quality-informed Incentive-driven Multi-agent Dispatching System for Mobile Crowdsensing", "comment": null, "summary": "This paper addresses the challenge of achieving optimal Quality of Information (QoI) in non-dedicated vehicular mobile crowdsensing (NVMCS) systems. The key obstacles are the interrelated issues of sensing coverage, sensing reliability, and the dynamic participation of vehicles. To tackle these, we propose QUIDS, a QUality-informed Incentive-driven multi-agent Dispatching System, which ensures high sensing coverage and reliability under budget constraints. QUIDS introduces a novel metric, Aggregated Sensing Quality (ASQ), to quantitatively capture QoI by integrating both coverage and reliability. We also develop a Mutually Assisted Belief-aware Vehicle Dispatching algorithm that estimates sensing reliability and allocates incentives under uncertainty, further improving ASQ. Evaluation using real-world data from a metropolitan NVMCS deployment shows QUIDS improves ASQ by 38% over non-dispatching scenarios and by 10% over state-of-the-art methods. It also reduces reconstruction map errors by 39-74% across algorithms. By jointly optimizing coverage and reliability via a quality-informed incentive mechanism, QUIDS enables low-cost, high-quality urban monitoring without dedicated infrastructure, applicable to smart-city scenarios like traffic and environmental sensing.", "AI": {"tldr": "提出QUIDS系统，通过质量感知的激励机制优化非专用车载移动群智感知中的信息质量，提升覆盖率和可靠性。", "motivation": "非专用车载移动群智感知面临覆盖范围、感知可靠性和车辆动态参与的相互关联挑战，需在预算约束下实现高质量城市监测。", "method": "设计QUIDS系统，引入聚合感知质量指标整合覆盖与可靠性；开发基于相互辅助信念的车辆调度算法，在不确定性下估计可靠性并分配激励。", "result": "真实数据评估显示，QUIDS比无调度场景提升聚合感知质量38%，比现有方法提升10%，重建地图误差降低39-74%。", "conclusion": "QUIDS通过联合优化覆盖与可靠性的质量感知激励机制，无需专用基础设施即可实现低成本高质量城市监测，适用于交通与环境感知等智慧城市场景。"}}
{"id": "2512.16465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16465", "abs": "https://arxiv.org/abs/2512.16465", "authors": ["Jinwu Chen", "Qidie Wu", "Bin Li", "Lin Ma", "Xin Si", "Yang Hu", "Shouyi Yin", "Jun Yang"], "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution", "comment": null, "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.", "AI": {"tldr": "提出cuPilot框架，通过策略协调的多智能体系统和语义化进化表示优化CUDA内核，在100个内核基准测试中平均加速3.09倍。", "motivation": "现有基于大语言模型和进化算法的自动内核优化方法因智能体设计欠佳和进化表示不匹配导致性能不足，需要更有效的自动化优化方案。", "method": "提出策略协调的多智能体框架，引入策略作为中间语义表示；采用策略协调进化算法、屋顶线引导提示和策略级种群初始化。", "result": "在100个内核基准测试中平均加速3.09倍（相比PyTorch）；在GEMM任务中实现复杂优化并高效利用关键硬件单元。", "conclusion": "cuPilot通过语义化策略表示和协调进化机制显著提升CUDA内核优化效果，为硬件-软件协同设计提供自动化解决方案。"}}
{"id": "2512.16349", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16349", "abs": "https://arxiv.org/abs/2512.16349", "authors": ["Soochang Song", "Yongjune Kim"], "title": "Collaborative Edge-to-Server Inference for Vision-Language Models", "comment": "13 pages, 12 figures", "summary": "We propose a collaborative edge-to-server inference framework for vision-language models (VLMs) that reduces the communication cost while maintaining inference accuracy. In typical deployments, visual data captured at edge devices (clients) is transmitted to the server for VLM inference. However, resizing the original image (global image) to match the vision encoder's input resolution often discards fine-grained details, leading to accuracy degradation. To overcome this limitation, we design a two-stage framework. In the first stage, the server performs inference on the global image and identifies a region of interest (RoI) using the VLM's internal attention. The min-entropy of the output tokens is then computed as a confidence measure to determine whether retransmission is required. If the min-entropy exceeds a predefined threshold, the server requests the edge device to send a detail-preserved local image of the RoI. The server then refines its inference by jointly leveraging the global and local images. This selective retransmission strategy ensures that only essential visual content is transmitted. Experiments across multiple VLM architectures show that the proposed framework significantly reduces communication cost while maintaining inference accuracy.", "AI": {"tldr": "提出了一种协作的边缘到服务器视觉语言模型推理框架，通过选择性重传策略在保持精度的同时降低通信成本。", "motivation": "传统部署中，边缘设备将原始图像调整大小后传输到服务器进行推理，会丢失细粒度细节导致精度下降，需要解决通信成本与推理精度的平衡问题。", "method": "设计两阶段框架：1) 服务器在全局图像上推理，利用VLM内部注意力识别感兴趣区域，计算输出令牌的最小熵作为置信度；2) 若最小熵超过阈值，请求边缘设备传输保留细节的局部图像，结合全局与局部图像进行细化推理。", "result": "在多种VLM架构上的实验表明，该框架显著降低了通信成本，同时保持了推理精度。", "conclusion": "所提出的选择性重传策略能有效减少非必要视觉内容的传输，实现通信效率与推理精度的优化平衡。"}}
{"id": "2512.16491", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16491", "abs": "https://arxiv.org/abs/2512.16491", "authors": ["Theresa Eimer", "Lennart Schäpermeier", "André Biedenkapp", "Alexander Tornede", "Lars Kotthoff", "Pieter Leyman", "Matthias Feurer", "Katharina Eggensperger", "Kaitlin Maile", "Tanja Tornede", "Anna Kozak", "Ke Xue", "Marcel Wever", "Mitra Baratchi", "Damir Pulatov", "Heike Trautmann", "Haniye Kashgarani", "Marius Lindauer"], "title": "Best Practices For Empirical Meta-Algorithmic Research Guidelines from the COSEAL Research Network", "comment": null, "summary": "Empirical research on meta-algorithmics, such as algorithm selection, configuration, and scheduling, often relies on extensive and thus computationally expensive experiments. With the large degree of freedom we have over our experimental setup and design comes a plethora of possible error sources that threaten the scalability and validity of our scientific insights. Best practices for meta-algorithmic research exist, but they are scattered between different publications and fields, and continue to evolve separately from each other. In this report, we collect good practices for empirical meta-algorithmic research across the subfields of the COSEAL community, encompassing the entire experimental cycle: from formulating research questions and selecting an experimental design, to executing ex- periments, and ultimately, analyzing and presenting results impartially. It establishes the current state-of-the-art practices within meta-algorithmic research and serves as a guideline to both new researchers and practitioners in meta-algorithmic fields.", "AI": {"tldr": "本文收集了元算法研究领域的实验最佳实践，涵盖从问题提出到结果呈现的完整实验周期，旨在提高研究的可扩展性和有效性。", "motivation": "元算法研究（如算法选择、配置和调度）依赖大量计算实验，实验设计的自由度大且错误源多，威胁科学见解的可扩展性和有效性。现有最佳实践分散在不同领域，缺乏系统整合。", "method": "通过收集COSEAL社区各子领域的良好实践，系统整理元算法研究的完整实验周期指南，包括研究问题设计、实验执行、结果分析与公正呈现。", "result": "建立了元算法研究领域当前最先进的实践标准，形成系统化的实验指导框架。", "conclusion": "本报告为元算法领域的新研究人员和实践者提供了全面的实验指南，有助于提升该领域研究的严谨性和可重复性。"}}
{"id": "2512.16360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16360", "abs": "https://arxiv.org/abs/2512.16360", "authors": ["Haotian Ling", "Zequn Chen", "Qiuying Chen", "Donglin Di", "Yongjia Ma", "Hao Li", "Chen Wei", "Zhulin Tao", "Xun Yang"], "title": "EverybodyDance: Bipartite Graph-Based Identity Correspondence for Multi-Character Animation", "comment": null, "summary": "Consistent pose-driven character animation has achieved remarkable progress in single-character scenarios. However, extending these advances to multi-character settings is non-trivial, especially when position swap is involved. Beyond mere scaling, the core challenge lies in enforcing correct Identity Correspondence (IC) between characters in reference and generated frames. To address this, we introduce EverybodyDance, a systematic solution targeting IC correctness in multi-character animation. EverybodyDance is built around the Identity Matching Graph (IMG), which models characters in the generated and reference frames as two node sets in a weighted complete bipartite graph. Edge weights, computed via our proposed Mask-Query Attention (MQA), quantify the affinity between each pair of characters. Our key insight is to formalize IC correctness as a graph structural metric and to optimize it during training. We also propose a series of targeted strategies tailored for multi-character animation, including identity-embedded guidance, a multi-scale matching strategy, and pre-classified sampling, which work synergistically. Finally, to evaluate IC performance, we curate the Identity Correspondence Evaluation benchmark, dedicated to multi-character IC correctness. Extensive experiments demonstrate that EverybodyDance substantially outperforms state-of-the-art baselines in both IC and visual fidelity.", "AI": {"tldr": "提出EverybodyDance方法解决多角色动画中的身份对应问题，通过身份匹配图建模角色关系并优化结构指标，显著提升身份对应准确性和视觉质量。", "motivation": "现有姿态驱动动画在单角色场景表现良好，但扩展到多角色场景（尤其涉及位置交换时）存在身份对应困难，需要系统化解决方案。", "method": "构建身份匹配图建模生成帧与参考帧角色关系；提出掩码查询注意力计算角色亲和度；采用身份嵌入引导、多尺度匹配策略和预分类采样等协同策略。", "result": "在构建的身份对应评估基准上，EverybodyDance在身份对应准确性和视觉保真度方面显著优于现有先进方法。", "conclusion": "通过形式化身份对应为图结构优化问题，并结合针对性策略，该方法为多角色动画提供了有效的系统性解决方案。"}}
{"id": "2512.16529", "categories": ["cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.16529", "abs": "https://arxiv.org/abs/2512.16529", "authors": ["Julien Gachadoat", "Guillaume Lagarde"], "title": "ParamExplorer: A framework for exploring parameters in generative art", "comment": "16 pages, 3 figures", "summary": "Generative art systems often involve high-dimensional and complex parameter spaces in which aesthetically compelling outputs occupy only small, fragmented regions. Because of this combinatorial explosion, artists typically rely on extensive manual trial-and-error, leaving many potentially interesting configurations undiscovered. In this work we make two contributions. First, we introduce ParamExplorer, an interactive and modular framework inspired by reinforcement learning that helps the exploration of parameter spaces in generative art algorithms, guided by human-in-the-loop or even automated feedback. The framework also integrates seamlessly with existing p5.js projects. Second, within this framework we implement and evaluate several exploration strategies, referred to as agents.", "AI": {"tldr": "提出ParamExplorer交互式框架，帮助探索生成艺术算法中的参数空间，并实现多种探索策略进行评估。", "motivation": "生成艺术系统通常涉及高维复杂参数空间，其中美学上令人满意的输出仅占据小而分散的区域，艺术家通常依赖大量手动试错，导致许多潜在有趣配置未被发现。", "method": "引入ParamExplorer框架，该框架受强化学习启发，支持人机交互或自动化反馈引导的参数空间探索，并与现有p5.js项目无缝集成；在框架内实现并评估多种探索策略（称为智能体）。", "result": "开发了模块化交互框架，支持多种探索策略，并与常用生成艺术工具集成。", "conclusion": "ParamExplorer框架通过结构化探索方法，有助于更高效地发现生成艺术参数空间中的美学区域，减少对手动试错的依赖。"}}
{"id": "2512.16357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16357", "abs": "https://arxiv.org/abs/2512.16357", "authors": ["Tao Hu", "Weiyu Zhou", "Yanjie Tu", "Peng Wu", "Wei Dong", "Qingsen Yan", "Yanning Zhang"], "title": "GMODiff: One-Step Gain Map Refinement with Diffusion Priors for HDR Reconstruction", "comment": null, "summary": "Pre-trained Latent Diffusion Models (LDMs) have recently shown strong perceptual priors for low-level vision tasks, making them a promising direction for multi-exposure High Dynamic Range (HDR) reconstruction. However, directly applying LDMs to HDR remains challenging due to: (1) limited dynamic-range representation caused by 8-bit latent compression, (2) high inference cost from multi-step denoising, and (3) content hallucination inherent to generative nature. To address these challenges, we introduce GMODiff, a gain map-driven one-step diffusion framework for multi-exposure HDR reconstruction. Instead of reconstructing full HDR content, we reformulate HDR reconstruction as a conditionally guided Gain Map (GM) estimation task, where the GM encodes the extended dynamic range while retaining the same bit depth as LDR images. We initialize the denoising process from an informative regression-based estimate rather than pure noise, enabling the model to generate high-quality GMs in a single denoising step. Furthermore, recognizing that regression-based models excel in content fidelity while LDMs favor perceptual quality, we leverage regression priors to guide both the denoising process and latent decoding of the LDM, suppressing hallucinations while preserving structural accuracy. Extensive experiments demonstrate that our GMODiff performs favorably against several state-of-the-art methods and is 100 faster than previous LDM-based methods.", "AI": {"tldr": "提出GMODiff，一种基于增益图的一步扩散框架，用于多曝光HDR重建，通过将HDR重建转化为增益图估计任务，结合回归先验引导，在单步去噪中实现高质量结果，速度比之前LDM方法快100倍。", "motivation": "预训练的潜在扩散模型（LDMs）在低层视觉任务中表现出强大的感知先验，但直接应用于HDR重建面临挑战：8位潜在压缩导致动态范围受限、多步去噪推理成本高、以及生成性质导致的内容幻觉问题。", "method": "将HDR重建重新定义为条件引导的增益图（GM）估计任务，GM编码扩展的动态范围同时保持与LDR图像相同的位深度；从基于回归的估计初始化去噪过程，实现单步生成；利用回归先验引导去噪过程和LDM的潜在解码，抑制幻觉并保持结构准确性。", "result": "GMODiff在多项实验中优于多种最先进方法，且比之前基于LDM的方法快100倍，在感知质量和内容保真度之间取得良好平衡。", "conclusion": "GMODiff通过增益图驱动的一步扩散框架，有效解决了LDM在HDR重建中的动态范围、效率和幻觉问题，为多曝光HDR重建提供了高效且高质量的解决方案。"}}
{"id": "2512.16468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16468", "abs": "https://arxiv.org/abs/2512.16468", "authors": ["Danial Safaei", "Siddartha Khastgir", "Mohsen Alirezaei", "Jeroen Ploeg", "Son Tong", "Xingyu Zhao"], "title": "Quantifying and Bridging the Fidelity Gap: A Decisive-Feature Approach to Comparing Synthetic and Real Imagery", "comment": null, "summary": "Virtual testing using synthetic data has become a cornerstone of autonomous vehicle (AV) safety assurance. Despite progress in improving visual realism through advanced simulators and generative AI, recent studies reveal that pixel-level fidelity alone does not ensure reliable transfer from simulation to the real world. What truly matters is whether the system-under-test (SUT) bases its decisions on the same causal evidence in both real and simulated environments - not just whether images \"look real\" to humans. This paper addresses the lack of such a behavior-grounded fidelity measure by introducing Decisive Feature Fidelity (DFF), a new SUT-specific metric that extends the existing fidelity spectrum to capture mechanism parity - the agreement in causal evidence underlying the SUT's decisions across domains. DFF leverages explainable-AI (XAI) methods to identify and compare the decisive features driving the SUT's outputs for matched real-synthetic pairs. We further propose practical estimators based on counterfactual explanations, along with a DFF-guided calibration scheme to enhance simulator fidelity. Experiments on 2126 matched KITTI-VirtualKITTI2 pairs demonstrate that DFF reveals discrepancies overlooked by conventional output-value fidelity. Furthermore, results show that DFF-guided calibration improves decisive-feature and input-level fidelity without sacrificing output value fidelity across diverse SUTs.", "AI": {"tldr": "本文提出了一种新的系统特定保真度度量方法——决定性特征保真度（DFF），用于评估自动驾驶虚拟测试中合成数据与真实数据在因果证据层面的一致性，而非仅关注视觉真实性。", "motivation": "当前自动驾驶安全验证依赖合成数据，但研究发现像素级视觉保真度不足以保证仿真到现实的可靠迁移。关键在于被测系统在真实和仿真环境中是否基于相同的因果证据进行决策，而现有方法缺乏对此的行为基础保真度度量。", "method": "提出决定性特征保真度（DFF）度量方法，利用可解释AI技术识别和比较被测系统在匹配的真实-合成数据对中的决策特征；基于反事实解释设计实用估计器，并提出DFF引导的仿真器校准方案。", "result": "在2126对KITTI-VirtualKITTI2匹配数据上的实验表明，DFF能发现传统输出值保真度忽略的差异；DFF引导的校准能在不牺牲输出值保真度的前提下，提升多种被测系统的决定性特征保真度和输入级保真度。", "conclusion": "DFF为仿真到现实迁移提供了更本质的保真度评估框架，通过关注决策因果机制而非表面视觉相似性，能更有效地指导仿真器改进和自动驾驶系统验证。"}}
{"id": "2512.16371", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16371", "abs": "https://arxiv.org/abs/2512.16371", "authors": ["Mariam Hassan", "Bastien Van Delft", "Wuyang Li", "Alexandre Alahi"], "title": "Factorized Video Generation: Decoupling Scene Construction and Temporal Synthesis in Text-to-Video Diffusion Models", "comment": null, "summary": "State-of-the-art Text-to-Video (T2V) diffusion models can generate visually impressive results, yet they still frequently fail to compose complex scenes or follow logical temporal instructions. In this paper, we argue that many errors, including apparent motion failures, originate from the model's inability to construct a semantically correct or logically consistent initial frame. We introduce Factorized Video Generation (FVG), a pipeline that decouples these tasks by decomposing the Text-to-Video generation into three specialized stages: (1) Reasoning, where a Large Language Model (LLM) rewrites the video prompt to describe only the initial scene, resolving temporal ambiguities; (2) Composition, where a Text-to-Image (T2I) model synthesizes a high-quality, compositionally-correct anchor frame from this new prompt; and (3) Temporal Synthesis, where a video model, finetuned to understand this anchor, focuses its entire capacity on animating the scene and following the prompt. Our decomposed approach sets a new state-of-the-art on the T2V CompBench benchmark and significantly improves all tested models on VBench2. Furthermore, we show that visual anchoring allows us to cut the number of sampling steps by 70% without any loss in performance, leading to a substantial speed-up in sampling. Factorized Video Generation offers a simple yet practical path toward more efficient, robust, and controllable video synthesis", "AI": {"tldr": "提出Factorized Video Generation（FVG）方法，将文本到视频生成分解为推理、构图和时序合成三个阶段，通过大语言模型重写提示、文本到图像模型生成锚定帧，再微调视频模型进行动画化，显著提升视频生成质量和效率。", "motivation": "现有文本到视频扩散模型在生成复杂场景或遵循逻辑时序指令时经常失败，许多错误源于模型无法构建语义正确或逻辑一致的初始帧，因此需要一种方法来解耦这些任务以提高鲁棒性和可控性。", "method": "采用三阶段分解方法：1）推理阶段，使用大语言模型重写视频提示以描述初始场景，解决时序模糊性；2）构图阶段，使用文本到图像模型从新提示合成高质量、构图正确的锚定帧；3）时序合成阶段，微调视频模型专注于动画化场景并遵循提示。", "result": "FVG在T2V CompBench基准测试中达到新的最先进水平，在VBench2上显著提升所有测试模型性能；视觉锚定允许将采样步骤减少70%而不损失性能，大幅加速采样过程。", "conclusion": "Factorized Video Generation提供了一种简单而实用的路径，实现更高效、鲁棒和可控的视频合成，通过任务分解有效解决了现有模型在复杂场景和逻辑时序上的局限性。"}}
{"id": "2512.16531", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16531", "abs": "https://arxiv.org/abs/2512.16531", "authors": ["Ander Alvarez", "Alessandro Genuardi", "Nilotpal Sinha", "Antonio Tiene", "Samuel Mugel", "Román Orús"], "title": "Scaling Laws for Energy Efficiency of Local LLMs", "comment": null, "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.", "AI": {"tldr": "本文系统性地评估了在CPU设备上部署大型语言模型和视觉语言模型的性能，发现了两种计算缩放规律，并提出量子启发的压缩方法可显著降低资源消耗。", "motivation": "当前边缘设备主要依赖CPU进行AI推理，但CPU上本地语言和视觉语言模型的计算规律尚未得到充分研究，需要平衡计算资源限制与模型精度。", "method": "在MacBook Pro M2和Raspberry Pi 5两类CPU设备上，采用基于处理器和内存使用连续采样及曲线下面积积分的统一方法，系统评估模型性能，并应用量子启发压缩技术。", "result": "发现两个经验缩放规律：语言模型推理计算成本随token长度近似线性增长；视觉语言模型存在预处理驱动的“分辨率拐点”。量子压缩可使处理器和内存使用降低71.9%，能耗降低62%，同时保持或提升语义准确性。", "conclusion": "研究量化了CPU上多模态模型的缩放规律，表明模型压缩和输入分辨率预处理是低成本、可持续边缘推理的有效手段。"}}
{"id": "2512.16393", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16393", "abs": "https://arxiv.org/abs/2512.16393", "authors": ["Zhanwei Li", "Liang Li", "Jiawan Zhang"], "title": "Adaptive Frequency Domain Alignment Network for Medical image segmentation", "comment": null, "summary": "High-quality annotated data plays a crucial role in achieving accurate segmentation. However, such data for medical image segmentation are often scarce due to the time-consuming and labor-intensive nature of manual annotation. To address this challenge, we propose the Adaptive Frequency Domain Alignment Network (AFDAN)--a novel domain adaptation framework designed to align features in the frequency domain and alleviate data scarcity. AFDAN integrates three core components to enable robust cross-domain knowledge transfer: an Adversarial Domain Learning Module that transfers features from the source to the target domain; a Source-Target Frequency Fusion Module that blends frequency representations across domains; and a Spatial-Frequency Integration Module that combines both frequency and spatial features to further enhance segmentation accuracy across domains. Extensive experiments demonstrate the effectiveness of AFDAN: it achieves an Intersection over Union (IoU) of 90.9% for vitiligo segmentation in the newly constructed VITILIGO2025 dataset and a competitive IoU of 82.6% on the retinal vessel segmentation benchmark DRIVE, surpassing existing state-of-the-art approaches.", "AI": {"tldr": "提出AFDAN网络，通过频域特征对齐解决医学图像分割数据稀缺问题，在白癜风和视网膜血管分割任务上取得优异性能。", "motivation": "医学图像分割依赖高质量标注数据，但人工标注耗时耗力导致数据稀缺，需要有效的跨域知识迁移方法。", "method": "提出自适应频域对齐网络（AFDAN），包含对抗域学习模块、源-目标频域融合模块和空间-频域集成模块，实现跨域特征对齐。", "result": "在VITILIGO2025数据集上IoU达90.9%，在DRIVE视网膜血管分割基准上IoU达82.6%，超越现有先进方法。", "conclusion": "AFDAN通过频域特征对齐有效缓解医学图像分割数据稀缺问题，在跨域分割任务中表现出优越性能。"}}
{"id": "2512.16532", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.16532", "abs": "https://arxiv.org/abs/2512.16532", "authors": ["Himanshu Gharat", "Himanshi Agrawal", "Gourab K. Patro"], "title": "From Personalization to Prejudice: Bias and Discrimination in Memory-Enhanced AI Agents for Recruitment", "comment": "In Proceedings of the Nineteenth ACM International Conference on Web Search and Data Mining (WSDM '26)", "summary": "Large Language Models (LLMs) have empowered AI agents with advanced capabilities for understanding, reasoning, and interacting across diverse tasks. The addition of memory further enhances them by enabling continuity across interactions, learning from past experiences, and improving the relevance of actions and responses over time; termed as memory-enhanced personalization. Although such personalization through memory offers clear benefits, it also introduces risks of bias. While several previous studies have highlighted bias in ML and LLMs, bias due to memory-enhanced personalized agents is largely unexplored. Using recruitment as an example use case, we simulate the behavior of a memory-enhanced personalized agent, and study whether and how bias is introduced and amplified in and across various stages of operation. Our experiments on agents using safety-trained LLMs reveal that bias is systematically introduced and reinforced through personalization, emphasizing the need for additional protective measures or agent guardrails in memory-enhanced LLM-based AI agents.", "AI": {"tldr": "本文研究了基于记忆增强的大型语言模型AI代理在个性化过程中引入和放大偏见的问题，以招聘场景为例进行模拟实验。", "motivation": "虽然记忆增强的个性化AI代理能提升交互连续性和响应相关性，但可能引入偏见风险。现有研究主要关注ML和LLM中的偏见，而记忆增强个性化代理的偏见问题尚未充分探索。", "method": "以招聘为用例，模拟记忆增强个性化代理的行为，分析其在各操作阶段是否及如何引入和放大偏见。实验使用经过安全训练的LLM构建代理。", "result": "实验表明，偏见通过个性化过程被系统性地引入和强化，即使使用安全训练的LLM也无法避免。", "conclusion": "记忆增强的LLM基AI代理需要额外的保护措施或代理护栏来缓解偏见风险。"}}
{"id": "2512.16644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16644", "abs": "https://arxiv.org/abs/2512.16644", "authors": ["Wisnu Uriawan", "Aria Octavian Hamza", "Ade Ripaldi Nuralim", "Adi Purnama", "Ahmad Juaeni Yunus", "Anissya Auliani Supriadi Putri"], "title": "Implementing a Sharia Chatbot as a Consultation Medium for Questions About Islam", "comment": null, "summary": "This research presents the implementation of a Sharia-compliant chatbot as an interactive medium for consulting Islamic questions, leveraging Reinforcement Learning (Q-Learning) integrated with Sentence-Transformers for semantic embedding to ensure contextual and accurate responses. Utilizing the CRISP-DM methodology, the system processes a curated Islam QA dataset of 25,000 question-answer pairs from authentic sources like the Qur'an, Hadith, and scholarly fatwas, formatted in JSON for flexibility and scalability. The chatbot prototype, developed with a Flask API backend and Flutter-based mobile frontend, achieves 87% semantic accuracy in functional testing across diverse topics including fiqh, aqidah, ibadah, and muamalah, demonstrating its potential to enhance religious literacy, digital da'wah, and access to verified Islamic knowledge in the Industry 4.0 era. While effective for closed-domain queries, limitations such as static learning and dataset dependency highlight opportunities for future enhancements like continuous adaptation and multi-turn conversation support, positioning this innovation as a bridge between traditional Islamic scholarship and modern AI-driven consultation.", "AI": {"tldr": "本研究开发了一个符合伊斯兰教法的聊天机器人，通过强化学习与语义嵌入技术，为伊斯兰问题提供准确咨询，在功能测试中达到87%的语义准确率。", "motivation": "在工业4.0时代，需要一种数字化工具来提升宗教素养、促进数字宣教，并提供经过验证的伊斯兰知识，同时将传统伊斯兰学术与现代人工智能咨询相结合。", "method": "采用CRISP-DM方法论，利用强化学习（Q-Learning）与Sentence-Transformers进行语义嵌入，处理包含25,000个问答对的伊斯兰数据集，后端使用Flask API，前端基于Flutter开发。", "result": "聊天机器人在功能测试中对包括教法、信仰、功修和交易在内的多样主题达到87%的语义准确率，展示了其在封闭领域查询中的有效性。", "conclusion": "该聊天机器人作为传统伊斯兰学术与现代人工智能咨询的桥梁，具有增强宗教素养和知识获取的潜力，但静态学习和数据集依赖等限制为未来改进（如持续适应和多轮对话支持）提供了方向。"}}
{"id": "2512.16397", "categories": ["cs.CV", "cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.16397", "abs": "https://arxiv.org/abs/2512.16397", "authors": ["Haodi He", "Jihun Yu", "Ronald Fedkiw"], "title": "Using Gaussian Splats to Create High-Fidelity Facial Geometry and Texture", "comment": "Submitted to CVPR 2026. 21 pages, 22 figures", "summary": "We leverage increasingly popular three-dimensional neural representations in order to construct a unified and consistent explanation of a collection of uncalibrated images of the human face. Our approach utilizes Gaussian Splatting, since it is more explicit and thus more amenable to constraints than NeRFs. We leverage segmentation annotations to align the semantic regions of the face, facilitating the reconstruction of a neutral pose from only 11 images (as opposed to requiring a long video). We soft constrain the Gaussians to an underlying triangulated surface in order to provide a more structured Gaussian Splat reconstruction, which in turn informs subsequent perturbations to increase the accuracy of the underlying triangulated surface. The resulting triangulated surface can then be used in a standard graphics pipeline. In addition, and perhaps most impactful, we show how accurate geometry enables the Gaussian Splats to be transformed into texture space where they can be treated as a view-dependent neural texture. This allows one to use high visual fidelity Gaussian Splatting on any asset in a scene without the need to modify any other asset or any other aspect (geometry, lighting, renderer, etc.) of the graphics pipeline. We utilize a relightable Gaussian model to disentangle texture from lighting in order to obtain a delit high-resolution albedo texture that is also readily usable in a standard graphics pipeline. The flexibility of our system allows for training with disparate images, even with incompatible lighting, facilitating robust regularization. Finally, we demonstrate the efficacy of our approach by illustrating its use in a text-driven asset creation pipeline.", "AI": {"tldr": "提出一种基于3D高斯溅射的统一人脸重建方法，通过少量未标定图像生成结构化网格和神经纹理，兼容标准图形管线。", "motivation": "现有神经表示方法（如NeRF）在约束处理和显式控制上存在局限，且需要大量输入数据；同时缺乏与标准图形管线直接兼容的高质量人脸重建方案。", "method": "利用11张未标定人脸图像，结合语义分割对齐面部区域；通过高斯溅射构建显式3D表示，并软约束到三角网格表面；将高斯溅射转换为视角依赖的神经纹理，并利用可重光照模型分离光照与纹理。", "result": "实现了从少量图像生成高质量、结构化的人脸网格和神经纹理；支持在标准图形管线中直接使用，且能处理光照不一致的训练数据。", "conclusion": "该方法通过结合高斯溅射与网格约束，实现了高效、兼容的人脸重建与神经纹理生成，为文本驱动资产创建等应用提供了可行方案。"}}
{"id": "2512.16650", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.16650", "abs": "https://arxiv.org/abs/2512.16650", "authors": ["Jirui Yang", "Hengqi Guo", "Zhihui Lu", "Yi Zhao", "Yuansen Zhang", "Shijing Hu", "Qiang Duan", "Yinggui Wang", "Tao Wei"], "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models", "comment": null, "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.", "AI": {"tldr": "提出Prefix Probing方法，通过比较不同前缀的条件对数概率实现有害内容检测，在保持高准确率的同时显著降低计算成本和延迟。", "motivation": "大型语言模型在安全敏感应用中面临检测准确率、推理延迟和部署成本之间的三难权衡，需要一种高效低成本的检测方法。", "method": "提出黑盒有害内容检测方法Prefix Probing：1）比较'同意/执行'与'拒绝/安全'前缀的条件对数概率生成危害性分数；2）利用前缀缓存将检测开销降至接近首词元延迟；3）设计高效前缀构造算法自动发现高信息量前缀。", "result": "实验表明，Prefix Probing的检测效果与主流外部安全模型相当，同时仅需最小计算成本且无需额外模型部署，具有强实用性和效率。", "conclusion": "Prefix Probing通过单次对数概率计算和前缀缓存机制，在保证检测性能的同时极大降低了计算开销，为实际应用提供了高效解决方案。"}}
{"id": "2512.16413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16413", "abs": "https://arxiv.org/abs/2512.16413", "authors": ["Liyuan Deng", "Hao Guo", "Yunpeng Bai", "Yongkang Dai", "Huaxi Huang", "Yilei Shi"], "title": "BrepLLM: Native Boundary Representation Understanding with Large Language Models", "comment": null, "summary": "Current token-sequence-based Large Language Models (LLMs) are not well-suited for directly processing 3D Boundary Representation (Brep) models that contain complex geometric and topological information. We propose BrepLLM, the first framework that enables LLMs to parse and reason over raw Brep data, bridging the modality gap between structured 3D geometry and natural language. BrepLLM employs a two-stage training pipeline: Cross-modal Alignment Pre-training and Multi-stage LLM Fine-tuning. In the first stage, an adaptive UV sampling strategy converts Breps into graphs representation with geometric and topological information. We then design a hierarchical BrepEncoder to extract features from geometry (i.e., faces and edges) and topology, producing both a single global token and a sequence of node tokens. Then we align the global token with text embeddings from a frozen CLIP text encoder (ViT-L/14) via contrastive learning. In the second stage, we integrate the pretrained BrepEncoder into an LLM. We then align its sequence of node tokens using a three-stage progressive training strategy: (1) training an MLP-based semantic mapping from Brep representation to 2D with 2D-LLM priors. (2) performing fine-tuning of the LLM. (3) designing a Mixture-of-Query Experts (MQE) to enhance geometric diversity modeling. We also construct Brep2Text, a dataset comprising 269,444 Brep-text question-answer pairs. Experiments show that BrepLLM achieves state-of-the-art (SOTA) results on 3D object classification and captioning tasks.", "AI": {"tldr": "提出了BrepLLM框架，首次使大语言模型能够直接处理包含复杂几何拓扑信息的3D边界表示模型，通过两阶段训练实现3D几何与自然语言的跨模态对齐。", "motivation": "当前基于token序列的大语言模型难以直接处理包含复杂几何和拓扑信息的3D边界表示模型，存在3D几何数据与自然语言之间的模态鸿沟。", "method": "采用两阶段训练：1）跨模态对齐预训练：通过自适应UV采样将Brep转换为图表示，设计分层BrepEncoder提取特征，使用对比学习对齐全局token与CLIP文本嵌入；2）多阶段LLM微调：集成BrepEncoder到LLM，通过三阶段渐进训练（语义映射、LLM微调、混合查询专家）对齐节点token序列。", "result": "构建了包含269,444个Brep-文本问答对的数据集Brep2Text；在3D物体分类和描述生成任务上取得了最先进的性能。", "conclusion": "BrepLLM成功弥合了结构化3D几何数据与自然语言之间的模态差距，为LLM处理原始Brep数据提供了有效框架，在多项3D理解任务上表现优异。"}}
{"id": "2512.16656", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.16656", "abs": "https://arxiv.org/abs/2512.16656", "authors": ["Sri Yash Tadimalla", "Justin Cary", "Gordon Hull", "Jordan Register", "Daniel Maxwell", "David Pugalee", "Tina Heafner"], "title": "Comprehensive AI Literacy: The Case for Centering Human Agency", "comment": "2 figures, 2 tables", "summary": "The rapid assimilation of Artificial Intelligence technologies into various facets of society has created a significant educational imperative that current frameworks are failing to effectively address. We are witnessing the rise of a dangerous literacy gap, where a focus on the functional, operational skills of using AI tools is eclipsing the development of critical and ethical reasoning about them. This position paper argues for a systemic shift toward comprehensive AI literacy that centers human agency - the empowered capacity for intentional, critical, and responsible choice. This principle applies to all stakeholders in the educational ecosystem: it is the student's agency to question, create with, or consciously decide not to use AI based on the task; it is the teacher's agency to design learning experiences that align with instructional values, rather than ceding pedagogical control to a tool. True literacy involves teaching about agency itself, framing technology not as an inevitability to be adopted, but as a choice to be made. This requires a deep commitment to critical thinking and a robust understanding of epistemology. Through the AI Literacy, Fluency, and Competency frameworks described in this paper, educators and students will become agents in their own human-centric approaches to AI, providing necessary pathways to clearly articulate the intentions informing decisions and attitudes toward AI and the impact of these decisions on academic work, career, and society.", "AI": {"tldr": "本文主张从功能性AI技能培养转向以人类能动性为核心的全面AI素养教育，强调批判性思维和伦理决策能力。", "motivation": "当前AI技术快速普及导致教育体系出现危险的素养鸿沟——过度关注工具操作技能而忽视批判性与伦理思考，现有教育框架无法有效应对这一挑战。", "method": "提出系统性教育转型框架，包括AI素养、流畅度与能力三层模型，强调将技术视为可选择性工具而非必然宿命的教学设计原则。", "result": "构建了以人类能动性为中心的教育路径，使师生能够基于任务需求、教学价值观自主决定AI使用方式，并清晰阐述决策意图及社会影响。", "conclusion": "真正的AI素养应培养所有教育参与者（师生）成为技术应用的主体，通过强化认识论理解和批判思维，实现以人为本的AI技术决策能力。"}}
{"id": "2512.16415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16415", "abs": "https://arxiv.org/abs/2512.16415", "authors": ["Muhammad Ibraheem Siddiqui", "Muhammad Haris Khan"], "title": "CountZES: Counting via Zero-Shot Exemplar Selection", "comment": null, "summary": "Object counting in complex scenes remains challenging, particularly in the zero-shot setting, where the goal is to count instances of unseen categories specified only by a class name. Existing zero-shot object counting (ZOC) methods that infer exemplars from text either rely on open-vocabulary detectors, which often yield multi-instance candidates, or on random patch sampling, which fails to accurately delineate object instances. To address this, we propose CountZES, a training-free framework for object counting via zero-shot exemplar selection. CountZES progressively discovers diverse exemplars through three synergistic stages: Detection-Anchored Exemplar (DAE), Density-Guided Exemplar (DGE), and Feature-Consensus Exemplar (FCE). DAE refines open-vocabulary detections to isolate precise single-instance exemplars. DGE introduces a density-driven, self-supervised paradigm to identify statistically consistent and semantically compact exemplars, while FCE reinforces visual coherence through feature-space clustering. Together, these stages yield a diverse, complementary exemplar set that balances textual grounding, count consistency, and feature representativeness. Experiments on diverse datasets demonstrate CountZES superior performance among ZOC methods while generalizing effectively across natural, aerial and medical domains.", "AI": {"tldr": "提出CountZES框架，通过检测锚定、密度引导和特征共识三阶段协同的零样本示例选择方法，解决复杂场景下未见类别对象计数问题，在多个数据集上表现优异。", "motivation": "现有零样本对象计数方法存在局限：基于开放词汇检测器常产生多实例候选，而随机补丁采样无法准确界定对象实例，需要更精确的示例选择机制。", "method": "提出无需训练的CountZES框架，包含三阶段：检测锚定示例（DAE）从开放词汇检测中精炼单实例示例；密度引导示例（DGE）通过密度驱动自监督识别统计一致示例；特征共识示例（FCE）通过特征空间聚类增强视觉一致性。", "result": "在多样化数据集上的实验表明，CountZES在零样本对象计数方法中表现优越，并能有效泛化到自然、航拍和医学领域。", "conclusion": "CountZES通过协同三阶段机制实现了文本对齐、计数一致性和特征代表性的平衡，为零样本对象计数提供了有效的训练免费解决方案。"}}
{"id": "2512.16443", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16443", "abs": "https://arxiv.org/abs/2512.16443", "authors": ["Shangxun Li", "Youngjung Uh"], "title": "Geometric Disentanglement of Text Embeddings for Subject-Consistent Text-to-Image Generation using A Single Prompt", "comment": null, "summary": "Text-to-image diffusion models excel at generating high-quality images from natural language descriptions but often fail to preserve subject consistency across multiple outputs, limiting their use in visual storytelling. Existing approaches rely on model fine-tuning or image conditioning, which are computationally expensive and require per-subject optimization. 1Prompt1Story, a training-free approach, concatenates all scene descriptions into a single prompt and rescales token embeddings, but it suffers from semantic leakage, where embeddings across frames become entangled, causing text misalignment. In this paper, we propose a simple yet effective training-free approach that addresses semantic entanglement from a geometric perspective by refining text embeddings to suppress unwanted semantics. Extensive experiments prove that our approach significantly improves both subject consistency and text alignment over existing baselines.", "AI": {"tldr": "提出一种无需训练的方法，通过几何视角精炼文本嵌入以抑制不想要的语义，显著提升扩散模型在视觉叙事中的主体一致性和文本对齐能力。", "motivation": "现有文本到图像扩散模型在生成多张图像时难以保持主体一致性，限制了视觉叙事的应用；现有方法依赖模型微调或图像条件，计算成本高且需要逐主体优化。", "method": "从几何角度出发，通过精炼文本嵌入来抑制不想要的语义，避免语义泄漏和文本错位，是一种无需训练的方法。", "result": "大量实验证明，该方法在主体一致性和文本对齐方面显著优于现有基线方法。", "conclusion": "所提出的简单有效方法解决了语义纠缠问题，提升了扩散模型在跨图像生成中的性能，无需额外训练成本。"}}
{"id": "2512.16694", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16694", "abs": "https://arxiv.org/abs/2512.16694", "authors": ["Wisnu Uriawan", "Achmad Ajie Priyajie", "Angga Gustian", "Fikri Nur Hidayat", "Sendi Ahmad Rafiudin", "Muhamad Fikri Zaelani"], "title": "Unsupervised Thematic Clustering Of hadith Texts Using The Apriori Algorithm", "comment": null, "summary": "This research stems from the urgency to automate the thematic grouping of hadith in line with the growing digitalization of Islamic texts. Based on a literature review, the unsupervised learning approach with the Apriori algorithm has proven effective in identifying association patterns and semantic relations in unlabeled text data. The dataset used is the Indonesian Translation of the hadith of Bukhari, which first goes through preprocessing stages including case folding, punctuation cleaning, tokenization, stopword removal, and stemming. Next, an association rule mining analysis was conducted using the Apriori algorithm with support, confidence, and lift parameters. The results show the existence of meaningful association patterns such as the relationship between rakaat-prayer, verse-revelation, and hadith-story, which describe the themes of worship, revelation, and hadith narration. These findings demonstrate that the Apriori algorithm has the ability to automatically uncover latent semantic relationships, while contributing to the development of digital Islamic studies and technology-based learning systems.", "AI": {"tldr": "本研究使用Apriori算法对印尼语布哈里圣训进行无监督主题关联挖掘，通过关联规则分析自动识别圣训文本中的语义关系模式。", "motivation": "随着伊斯兰文本数字化进程加快，迫切需要自动化方法对圣训进行主题分组，以促进数字伊斯兰研究和基于技术的学习系统发展。", "method": "采用无监督学习方法，使用Apriori算法进行关联规则挖掘。数据预处理包括大小写转换、标点清洗、分词、停用词去除和词干提取。通过支持度、置信度和提升度参数分析关联模式。", "result": "发现了有意义的关联模式，如'拜功-礼拜'、'经文-启示'、'圣训-故事'等关系，这些模式对应崇拜、启示和圣训叙述等主题。", "conclusion": "Apriori算法能够自动挖掘潜在的语义关系，验证了无监督学习方法在伊斯兰文本分析中的有效性，为数字伊斯兰研究提供了技术基础。"}}
{"id": "2512.16456", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16456", "abs": "https://arxiv.org/abs/2512.16456", "authors": ["Masashi Hatano", "Saptarshi Sinha", "Jacob Chalk", "Wei-Hong Li", "Hideo Saito", "Dima Damen"], "title": "Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach", "comment": "Project Page: https://masashi-hatano.github.io/prime-and-reach/", "summary": "Human motion generation is a challenging task that aims to create realistic motion imitating natural human behaviour. We focus on the well-studied behaviour of priming an object/location for pick up or put down -- that is, the spotting of an object/location from a distance, known as gaze priming, followed by the motion of approaching and reaching the target location. To that end, we curate, for the first time, 23.7K gaze-primed human motion sequences for reaching target object locations from five publicly available datasets, i.e., HD-EPIC, MoGaze, HOT3D, ADT, and GIMO. We pre-train a text-conditioned diffusion-based motion generation model, then fine-tune it conditioned on goal pose or location, on our curated sequences. Importantly, we evaluate the ability of the generated motion to imitate natural human movement through several metrics, including the 'Reach Success' and a newly introduced 'Prime Success' metric. On the largest dataset, HD-EPIC, our model achieves 60% prime success and 89% reach success when conditioned on the goal object location.", "AI": {"tldr": "该研究提出了一种基于扩散模型的人类运动生成方法，专注于视线引导的物体抓取/放置行为，通过整合五个公开数据集构建了首个大规模视线引导运动序列数据集，并引入新的评估指标验证生成运动的自然性。", "motivation": "人类运动生成是模拟自然人类行为的挑战性任务，现有研究缺乏对视线引导（gaze priming）行为的系统建模——即远距离识别目标后接近并抓取物体的连贯动作。", "method": "1. 从HD-EPIC等五个公开数据集中首次整合出23.7K条视线引导运动序列；2. 采用文本条件扩散模型进行预训练；3. 使用目标姿态/位置条件进行微调；4. 提出'Prime Success'新指标与'Reach Success'等多项评估指标。", "result": "在最大数据集HD-EPIC上，模型在目标位置条件下达到60%的视线引导成功率和89%的抓取成功率，证明生成运动能有效模仿自然人类行为。", "conclusion": "通过构建大规模视线引导运动数据集与扩散模型框架，该研究实现了对复杂人类行为（识别-接近-抓取）的连贯生成，为具身智能等应用提供了重要基础。"}}
{"id": "2512.16698", "categories": ["cs.AI", "cs.CG"], "pdf": "https://arxiv.org/pdf/2512.16698", "abs": "https://arxiv.org/abs/2512.16698", "authors": ["Mahbub E Sobhani", "Md. Faiyaz Abdullah Sayeedi", "Mohammad Nehad Alam", "Proma Hossain Progga", "Swakkhar Shatabda"], "title": "Do Multi-Agents Solve Better Than Single? Evaluating Agentic Frameworks for Diagram-Grounded Geometry Problem Solving and Reasoning", "comment": "Accepted to the ARR October 2025 cycle", "summary": "Diagram-grounded geometry problem solving is a critical benchmark for multimodal large language models (MLLMs), yet the benefits of multi-agent design over single-agent remain unclear. We systematically compare single-agent and multi-agent pipelines on four visual math benchmarks: Geometry3K, MathVerse, OlympiadBench, and We-Math. For open-source models, multi-agent consistently improves performance. For example, Qwen-2.5-VL (7B) gains +6.8 points and Qwen-2.5-VL (32B) gains +3.3 on Geometry3K, and both Qwen-2.5-VL variants see further gains on OlympiadBench and We-Math. In contrast, the closed-source Gemini-2.0-Flash generally performs better in single-agent mode on classic benchmarks, while multi-agent yields only modest improvements on the newer We-Math dataset. These findings show that multi-agent pipelines provide clear benefits for open-source models and can assist strong proprietary systems on newer, less familiar benchmarks, but agentic decomposition is not universally optimal. All code, data, and reasoning files are available at https://github.com/faiyazabdullah/Interpreter-Solver", "AI": {"tldr": "本文系统比较了单智能体与多智能体在视觉数学基准测试中的表现，发现多智能体对开源模型有明显提升，但对闭源模型效果有限，表明智能体分解并非普遍最优。", "motivation": "研究旨在探究多智能体设计在图表几何问题求解中是否优于单智能体，以澄清多模态大语言模型在该领域的性能差异。", "method": "在Geometry3K、MathVerse、OlympiadBench和We-Math四个视觉数学基准上，系统比较了单智能体与多智能体管道的性能，测试了开源模型（如Qwen-2.5-VL）和闭源模型（如Gemini-2.0-Flash）。", "result": "开源模型在多智能体模式下性能显著提升（如Qwen-2.5-VL (7B)在Geometry3K上提升6.8分），而闭源模型在经典基准上单智能体表现更优，仅在较新的We-Math数据集上多智能体带来有限改进。", "conclusion": "多智能体管道对开源模型有明显益处，并能在较新、不熟悉的基准上辅助强闭源系统，但智能体分解并非普遍最优策略。"}}
{"id": "2512.16483", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16483", "abs": "https://arxiv.org/abs/2512.16483", "authors": ["Senmao Li", "Kai Wang", "Salman Khan", "Fahad Shahbaz Khan", "Jian Yang", "Yaxing Wang"], "title": "StageVAR: Stage-Aware Acceleration for Visual Autoregressive Models", "comment": null, "summary": "Visual Autoregressive (VAR) modeling departs from the next-token prediction paradigm of traditional Autoregressive (AR) models through next-scale prediction, enabling high-quality image generation. However, the VAR paradigm suffers from sharply increased computational complexity and running time at large-scale steps. Although existing acceleration methods reduce runtime for large-scale steps, but rely on manual step selection and overlook the varying importance of different stages in the generation process. To address this challenge, we present StageVAR, a systematic study and stage-aware acceleration framework for VAR models. Our analysis shows that early steps are critical for preserving semantic and structural consistency and should remain intact, while later steps mainly refine details and can be pruned or approximated for acceleration. Building on these insights, StageVAR introduces a plug-and-play acceleration strategy that exploits semantic irrelevance and low-rank properties in late-stage computations, without requiring additional training. Our proposed StageVAR achieves up to 3.4x speedup with only a 0.01 drop on GenEval and a 0.26 decrease on DPG, consistently outperforming existing acceleration baselines. These results highlight stage-aware design as a powerful principle for efficient visual autoregressive image generation.", "AI": {"tldr": "提出StageVAR框架，通过分析VAR模型中不同生成阶段的重要性差异，在保持早期关键阶段完整的同时加速后期细节优化阶段，实现高效视觉自回归图像生成。", "motivation": "传统VAR模型在大规模生成步骤中计算复杂度急剧增加，现有加速方法依赖人工步骤选择且忽视生成过程中不同阶段的重要性差异，需要更智能的加速策略。", "method": "提出StageVAR框架：1）分析发现早期步骤对语义和结构一致性至关重要，后期步骤主要优化细节；2）利用后期计算的语义无关性和低秩特性，设计即插即用加速策略，无需额外训练。", "result": "StageVAR实现最高3.4倍加速，在GenEval上仅下降0.01分，在DPG上下降0.26分，性能优于现有加速基线方法。", "conclusion": "阶段感知设计是高效视觉自回归图像生成的有效原则，StageVAR通过智能阶段划分和加速实现了计算效率与生成质量的平衡。"}}
{"id": "2512.16484", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16484", "abs": "https://arxiv.org/abs/2512.16484", "authors": ["Yuan Li", "Yahan Yu", "Youyuan Lin", "Yong-Hao Yang", "Chenhui Chu", "Shin'ya Nishida"], "title": "Guiding Perception-Reasoning Closer to Human in Blind Image Quality Assessment", "comment": "Under review", "summary": "Humans assess image quality through a perception-reasoning cascade, integrating sensory cues with implicit reasoning to form self-consistent judgments. In this work, we investigate how a model can acquire both human-like and self-consistent reasoning capability for blind image quality assessment (BIQA). We first collect human evaluation data that capture several aspects of human perception-reasoning pipeline. Then, we adopt reinforcement learning, using human annotations as reward signals to guide the model toward human-like perception and reasoning. To enable the model to internalize self-consistent reasoning capability, we design a reward that drives the model to infer the image quality purely from self-generated descriptions. Empirically, our approach achieves score prediction performance comparable to state-of-the-art BIQA systems under general metrics, including Pearson and Spearman correlation coefficients. In addition to the rating score, we assess human-model alignment using ROUGE-1 to measure the similarity between model-generated and human perception-reasoning chains. On over 1,000 human-annotated samples, our model reaches a ROUGE-1 score of 0.512 (cf. 0.443 for baseline), indicating substantial coverage of human explanations and marking a step toward human-like interpretable reasoning in BIQA.", "AI": {"tldr": "本文提出一种通过强化学习模仿人类感知-推理过程的无参考图像质量评估方法，使模型具备人类式自我一致推理能力。", "motivation": "人类通过感知-推理级联评估图像质量，现有BIQA方法缺乏人类式的可解释推理能力，需要开发能同时预测评分并生成人类可理解解释的模型。", "method": "收集包含人类感知推理过程的多维度评估数据；采用强化学习，以人类标注作为奖励信号；设计额外奖励使模型能从自生成描述中推断质量，实现自我一致推理。", "result": "在通用指标（皮尔逊/斯皮尔曼相关系数）上达到SOTA水平；在1000+人工标注样本上，ROUGE-1得分达0.512（基线为0.443），显著覆盖人类解释模式。", "conclusion": "该方法在保持评分性能的同时，实现了更高的人类-模型对齐，推动了BIQA向人类式可解释推理的发展。"}}
{"id": "2512.16701", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16701", "abs": "https://arxiv.org/abs/2512.16701", "authors": ["Giovanni Adorni"], "title": "Cyber Humanism in Education: Reclaiming Agency through AI and Learning Sciences", "comment": "15 pages, 16 references, Key Note preented at the \"WAILS 2025 - The 2nd. Workshop on Artificial Intelligence with and for Learning Sciences\", Cagliary, Italy, 10-12 December 2025", "summary": "Generative Artificial Intelligence (GenAI) is rapidly reshaping how knowledge is produced and validated in education. Rather than adding another digital tool, large language models reconfigure reading, writing, and coding into hybrid human-AI workflows, raising concerns about epistemic automation, cognitive offloading, and the de-professiona\\-lisation of teachers. This paper proposes \\emph{Cyber Humanism in Education} as a framework for reclaiming human agency in this landscape. We conceptualise AI-enabled learning environments as socio-technical infrastructures co-authored by humans and machines, and position educators and learners as epistemic agents and \\emph{algorithmic citizens} who have both the right and the responsibility to shape these infrastructures.\n  We articulate three pillars for cyber-humanist design, \\emph{reflexive competence}, \\emph{algorithmic citizenship}, and \\emph{dialogic design}, and relate them to major international digital and AI competence frameworks. We then present higher-education case studies that operationalise these ideas through \\emph{prompt-based learning} and a new \\emph{Conversational AI Educator} certification within the EPICT ecosystem. The findings show how such practices can strengthen epistemic agency while surfacing tensions around workload, equity, and governance, and outline implications for the future of AI-rich, human-centred education.", "AI": {"tldr": "本文提出教育中的赛博人文主义框架，旨在应对生成式AI对知识生产与验证方式的重塑，通过三个支柱设计原则和高等教育案例研究，探索如何在AI丰富的教育环境中维护人类能动性。", "motivation": "生成式AI正在改变教育的知识生产与验证方式，将阅读、写作和编码重构为混合人机工作流，引发了关于认知自动化、认知卸载和教师去专业化等担忧，需要建立框架以重新主张人类在教育中的主体地位。", "method": "提出赛博人文主义教育框架，包含反思性能力、算法公民身份和对话式设计三大支柱；结合国际数字与AI能力框架，并通过高等教育案例研究（包括提示式学习和对话式AI教育者认证）进行实践验证。", "result": "研究发现相关实践能够增强认知能动性，但也暴露出工作量、公平性和治理方面的张力；案例表明该框架有助于在教育中协调人机协作，同时突显了实施中的挑战。", "conclusion": "赛博人文主义为AI密集型、以人为本的教育提供了可行路径，强调教育者和学习者作为算法公民应共同参与技术基础设施的塑造，未来需进一步解决实践中的张力以推动教育生态的健康发展。"}}
{"id": "2512.16733", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16733", "abs": "https://arxiv.org/abs/2512.16733", "authors": ["Daniel Bramblett", "Rushang Karia", "Adrian Ciotinga", "Ruthvick Suresh", "Pulkit Verma", "YooJung Choi", "Siddharth Srivastava"], "title": "Discovering and Learning Probabilistic Models of Black-Box AI Capabilities", "comment": null, "summary": "Black-box AI (BBAI) systems such as foundational models are increasingly being used for sequential decision making. To ensure that such systems are safe to operate and deploy, it is imperative to develop efficient methods that can provide a sound and interpretable representation of the BBAI's capabilities. This paper shows that PDDL-style representations can be used to efficiently learn and model an input BBAI's planning capabilities. It uses the Monte-Carlo tree search paradigm to systematically create test tasks, acquire data, and prune the hypothesis space of possible symbolic models. Learned models describe a BBAI's capabilities, the conditions under which they can be executed, and the possible outcomes of executing them along with their associated probabilities. Theoretical results show soundness, completeness and convergence of the learned models. Empirical results with multiple BBAI systems illustrate the scope, efficiency, and accuracy of the presented methods.", "AI": {"tldr": "提出一种使用PDDL风格表示法高效学习黑盒AI系统规划能力的方法，通过蒙特卡洛树搜索生成测试任务并学习符号模型，理论证明模型具有可靠性、完备性和收敛性。", "motivation": "随着基础模型等黑盒AI系统被越来越多地用于序列决策，需要开发能提供其能力可靠且可解释表示的方法，以确保系统部署的安全性。", "method": "采用PDDL风格表示法描述AI能力，利用蒙特卡洛树搜索系统化生成测试任务、收集数据并剪枝假设空间，学习包含执行条件、可能结果及概率的符号模型。", "result": "理论分析表明学习模型具有可靠性、完备性和收敛性；在多类黑盒AI系统上的实验验证了方法的适用范围、效率和准确性。", "conclusion": "该方法能有效建模黑盒AI的规划能力，为理解复杂AI系统的决策行为提供了可解释的符号表示框架。"}}
{"id": "2512.16707", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.16707", "abs": "https://arxiv.org/abs/2512.16707", "authors": ["Abhisek Ganguly"], "title": "Dual Computational Horizons: Incompleteness and Unpredictability in Intelligent Systems", "comment": "6 Pages, 0 figures", "summary": "We formalize two independent computational limitations that constrain algorithmic intelligence: formal incompleteness and dynamical unpredictability. The former limits the deductive power of consistent reasoning systems while the later bounds long-term prediction under finite precision. We show that these two extrema together impose structural bounds on an agent's ability to reason about its own predictive capabilities. In particular, an algorithmic agent cannot compute its own maximal prediction horizon generally. This perspective clarifies inherent trade-offs between reasoning, prediction, and self-analysis in intelligent systems.", "AI": {"tldr": "本文形式化了形式不完备性和动态不可预测性两种计算限制，证明它们共同约束了智能体对自身预测能力的推理，揭示了智能系统中推理、预测与自我分析之间的固有权衡。", "motivation": "研究动机是理解算法智能体的根本计算限制，特别是形式系统的不完备性和动力系统的不可预测性如何共同制约智能体的自我认知与预测能力。", "method": "采用形式化分析方法，将哥德尔不完备定理（形式不完备性）与动力系统有限精度下的长期不可预测性（动态不可预测性）相结合，构建理论框架分析算法智能体的能力边界。", "result": "主要结果表明：算法智能体无法普遍计算自身的最大预测边界；形式不完备性与动态不可预测性共同限制了智能体对自身预测能力的推理；智能系统的推理、预测与自我分析之间存在结构性权衡。", "conclusion": "研究结论指出：算法智能体的能力存在根本性计算限制；形式不完备与动态不可预测的极端情况共同约束了智能体的自我认知；这一视角为理解智能系统的内在权衡提供了理论框架。"}}
{"id": "2512.16485", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16485", "abs": "https://arxiv.org/abs/2512.16485", "authors": ["Kejun Liu", "Yuanyuan Liu", "Lin Wei", "Chang Tang", "Yibing Zhan", "Zijing Chen", "Zhe Chen"], "title": "Smile on the Face, Sadness in the Eyes: Bridging the Emotion Gap with a Multimodal Dataset of Eye and Facial Behaviors", "comment": "Accepted by TMM", "summary": "Emotion Recognition (ER) is the process of analyzing and identifying human emotions from sensing data. Currently, the field heavily relies on facial expression recognition (FER) because visual channel conveys rich emotional cues. However, facial expressions are often used as social tools rather than manifestations of genuine inner emotions. To understand and bridge this gap between FER and ER, we introduce eye behaviors as an important emotional cue and construct an Eye-behavior-aided Multimodal Emotion Recognition (EMER) dataset. To collect data with genuine emotions, spontaneous emotion induction paradigm is exploited with stimulus material, during which non-invasive eye behavior data, like eye movement sequences and eye fixation maps, is captured together with facial expression videos. To better illustrate the gap between ER and FER, multi-view emotion labels for mutimodal ER and FER are separately annotated. Furthermore, based on the new dataset, we design a simple yet effective Eye-behavior-aided MER Transformer (EMERT) that enhances ER by bridging the emotion gap. EMERT leverages modality-adversarial feature decoupling and a multitask Transformer to model eye behaviors as a strong complement to facial expressions. In the experiment, we introduce seven multimodal benchmark protocols for a variety of comprehensive evaluations of the EMER dataset. The results show that the EMERT outperforms other state-of-the-art multimodal methods by a great margin, revealing the importance of modeling eye behaviors for robust ER. To sum up, we provide a comprehensive analysis of the importance of eye behaviors in ER, advancing the study on addressing the gap between FER and ER for more robust ER performance. Our EMER dataset and the trained EMERT models will be publicly available at https://github.com/kejun1/EMER.", "AI": {"tldr": "该论文通过引入眼动行为作为重要情感线索，构建了EMER数据集并提出EMERT模型，以弥补面部表情识别与真实情感识别之间的差距。", "motivation": "当前情感识别领域过度依赖面部表情识别，但面部表情常被用作社交工具而非真实情感表达，导致面部表情识别与真实情感识别之间存在差距。", "method": "1. 使用自发情感诱导范式收集真实情感数据，同步采集眼动序列、眼动注视图与面部视频；2. 构建EMER数据集，为多模态情感识别和面部表情识别分别标注多视角情感标签；3. 设计EMERT模型，采用模态对抗特征解耦和多任务Transformer，将眼动行为作为面部表情的补充。", "result": "1. 提出七种多模态基准协议用于全面评估EMER数据集；2. EMERT模型显著优于其他先进多模态方法，证明眼动行为建模对提升情感识别鲁棒性的重要性。", "conclusion": "眼动行为在情感识别中具有重要作用，通过建模眼动行为可有效弥合面部表情识别与情感识别之间的差距，提升情感识别的鲁棒性。EMER数据集和EMERT模型将开源以促进相关研究。"}}
{"id": "2512.16493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16493", "abs": "https://arxiv.org/abs/2512.16493", "authors": ["Huma Hafeez", "Matthew Garratt", "Jo Plested", "Sankaran Iyer", "Arcot Sowmya"], "title": "YOLO11-4K: An Efficient Architecture for Real-Time Small Object Detection in 4K Panoramic Images", "comment": "Conference paper just submitted", "summary": "The processing of omnidirectional 360-degree images poses significant challenges for object detection due to inherent spatial distortions, wide fields of view, and ultra-high-resolution inputs. Conventional detectors such as YOLO are optimised for standard image sizes (for example, 640x640 pixels) and often struggle with the computational demands of 4K or higher-resolution imagery typical of 360-degree vision. To address these limitations, we introduce YOLO11-4K, an efficient real-time detection framework tailored for 4K panoramic images. The architecture incorporates a novel multi-scale detection head with a P2 layer to improve sensitivity to small objects often missed at coarser scales, and a GhostConv-based backbone to reduce computational complexity without sacrificing representational power. To enable evaluation, we manually annotated the CVIP360 dataset, generating 6,876 frame-level bounding boxes and producing a publicly available, detection-ready benchmark for 4K panoramic scenes. YOLO11-4K achieves 0.95 mAP at 0.50 IoU with 28.3 milliseconds inference per frame, representing a 75 percent latency reduction compared to YOLO11 (112.3 milliseconds), while also improving accuracy (mAP at 0.50 of 0.95 versus 0.908). This balance of efficiency and precision enables robust object detection in expansive 360-degree environments, making the framework suitable for real-world high-resolution panoramic applications. While this work focuses on 4K omnidirectional images, the approach is broadly applicable to high-resolution detection tasks in autonomous navigation, surveillance, and augmented reality.", "AI": {"tldr": "提出YOLO11-4K框架，针对4K全景图像的高效实时目标检测，通过多尺度检测头和轻量化主干网络，在保持精度的同时大幅降低延迟。", "motivation": "传统目标检测器（如YOLO）难以处理360度全景图像的空间畸变、大视场和超高分辨率（4K及以上）带来的计算挑战。", "method": "设计多尺度检测头（含P2层）以提升小目标检测能力；采用GhostConv轻量化主干网络降低计算复杂度；构建并标注CVIP360数据集（6,876个边界框）作为评估基准。", "result": "在4K全景图像上达到0.95 mAP（IoU=0.50），单帧推理时间28.3毫秒，相比YOLO11延迟降低75%（原112.3毫秒），精度提升（mAP从0.908升至0.95）。", "conclusion": "YOLO11-4K在效率与精度间取得平衡，适用于自动驾驶、监控和增强现实等高分辨率全景应用，其方法可扩展至其他高分辨率检测任务。"}}
{"id": "2512.16739", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16739", "abs": "https://arxiv.org/abs/2512.16739", "authors": ["Yipeng Zhuang", "Yifeng Guo", "Yuewen Li", "Yuheng Wu", "Philip Leung-Ho Yu", "Tingting Song", "Zhiyong Wang", "Kunzhong Zhou", "Weifang Wang", "Li Zhuang"], "title": "AI-Driven Prediction of Cancer Pain Episodes: A Hybrid Decision Support Approach", "comment": null, "summary": "Lung cancer patients frequently experience breakthrough pain episodes, with up to 91% requiring timely intervention. To enable proactive pain management, we propose a hybrid machine learning and large language model pipeline that predicts pain episodes within 48 and 72 hours of hospitalization using both structured and unstructured electronic health record data. A retrospective cohort of 266 inpatients was analyzed, with features including demographics, tumor stage, vital signs, and WHO-tiered analgesic use. The machine learning module captured temporal medication trends, while the large language model interpreted ambiguous dosing records and free-text clinical notes. Integrating these modalities improved sensitivity and interpretability. Our framework achieved an accuracy of 0.874 (48h) and 0.917 (72h), with an improvement in sensitivity of 8.6% and 10.4% due to the augmentation of large language model. This hybrid approach offers a clinically interpretable and scalable tool for early pain episode forecasting, with potential to enhance treatment precision and optimize resource allocation in oncology care.", "AI": {"tldr": "提出结合机器学习与大语言模型的混合框架，利用电子病历预测肺癌住院患者48/72小时内疼痛发作，准确率达0.874-0.917，提升敏感性8.6%-10.4%。", "motivation": "肺癌患者突破性疼痛发生率高（达91%），需及时干预，但现有方法缺乏对非结构化临床文本的解析能力，难以实现精准预警。", "method": "1. 回顾性分析266名住院患者数据；2. 机器学习模块提取时序用药特征；3. 大语言模型解析模糊用药记录与自由文本临床笔记；4. 融合结构化与非结构化数据构建预测模型。", "result": "混合模型在48小时/72小时预测准确率分别为0.874和0.917，大语言模型贡献使敏感性提升8.6%和10.4%，同时增强临床可解释性。", "conclusion": "该混合框架为疼痛早期预警提供可扩展工具，有望提升肿瘤治疗精准度并优化医疗资源配置。"}}
{"id": "2512.16494", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16494", "abs": "https://arxiv.org/abs/2512.16494", "authors": ["Mengyuan Liu", "Jiajie Liu", "Jinyan Zhang", "Wenhao Li", "Junsong Yuan"], "title": "PoseMoE: Mixture-of-Experts Network for Monocular 3D Human Pose Estimation", "comment": "IEEE Transactions on Image Processing (T-IP)", "summary": "The lifting-based methods have dominated monocular 3D human pose estimation by leveraging detected 2D poses as intermediate representations. The 2D component of the final 3D human pose benefits from the detected 2D poses, whereas its depth counterpart must be estimated from scratch. The lifting-based methods encode the detected 2D pose and unknown depth in an entangled feature space, explicitly introducing depth uncertainty to the detected 2D pose, thereby limiting overall estimation accuracy. This work reveals that the depth representation is pivotal for the estimation process. Specifically, when depth is in an initial, completely unknown state, jointly encoding depth features with 2D pose features is detrimental to the estimation process. In contrast, when depth is initially refined to a more dependable state via network-based estimation, encoding it together with 2D pose information is beneficial. To address this limitation, we present a Mixture-of-Experts network for monocular 3D pose estimation named PoseMoE. Our approach introduces: (1) A mixture-of-experts network where specialized expert modules refine the well-detected 2D pose features and learn the depth features. This mixture-of-experts design disentangles the feature encoding process for 2D pose and depth, therefore reducing the explicit influence of uncertain depth features on 2D pose features. (2) A cross-expert knowledge aggregation module is proposed to aggregate cross-expert spatio-temporal contextual information. This step enhances features through bidirectional mapping between 2D pose and depth. Extensive experiments show that our proposed PoseMoE outperforms the conventional lifting-based methods on three widely used datasets: Human3.6M, MPI-INF-3DHP, and 3DPW.", "AI": {"tldr": "提出PoseMoE网络，通过专家混合设计分离2D姿态与深度特征编码，改进基于提升的单目3D人体姿态估计方法。", "motivation": "传统基于提升的方法将检测到的2D姿态与未知深度在纠缠特征空间中联合编码，导致深度不确定性显式影响2D姿态特征，限制了整体估计精度。研究发现深度表示的初始状态对编码过程至关重要。", "method": "1. 专家混合网络：设计专用专家模块分别优化2D姿态特征和学习深度特征，解耦二者编码过程；2. 跨专家知识聚合模块：通过2D姿态与深度间的双向映射聚合跨专家时空上下文信息。", "result": "在Human3.6M、MPI-INF-3DHP和3DPW三个常用数据集上，PoseMoE均优于传统基于提升的方法。", "conclusion": "深度表示的初始质量决定其与2D姿态联合编码的效用：初始未知时联合编码有害，经网络初步优化后联合编码有益。所提方法通过特征解耦与跨专家信息聚合有效提升估计精度。"}}
{"id": "2512.16501", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16501", "abs": "https://arxiv.org/abs/2512.16501", "authors": ["Beitong Zhou", "Zhexiao Huang", "Yuan Guo", "Zhangxuan Gu", "Tianyu Xia", "Zichen Luo", "Fei Tang", "Dehan Kong", "Yanyi Shang", "Suling Ou", "Zhenlin Guo", "Changhua Meng", "Shuheng Shen"], "title": "VenusBench-GD: A Comprehensive Multi-Platform GUI Benchmark for Diverse Grounding Tasks", "comment": null, "summary": "GUI grounding is a critical component in building capable GUI agents. However, existing grounding benchmarks suffer from significant limitations: they either provide insufficient data volume and narrow domain coverage, or focus excessively on a single platform and require highly specialized domain knowledge. In this work, we present VenusBench-GD, a comprehensive, bilingual benchmark for GUI grounding that spans multiple platforms, enabling hierarchical evaluation for real-word applications. VenusBench-GD contributes as follows: (i) we introduce a large-scale, cross-platform benchmark with extensive coverage of applications, diverse UI elements, and rich annotated data, (ii) we establish a high-quality data construction pipeline for grounding tasks, achieving higher annotation accuracy than existing benchmarks, and (iii) we extend the scope of element grounding by proposing a hierarchical task taxonomy that divides grounding into basic and advanced categories, encompassing six distinct subtasks designed to evaluate models from complementary perspectives. Our experimental findings reveal critical insights: general-purpose multimodal models now match or even surpass specialized GUI models on basic grounding tasks. In contrast, advanced tasks, still favor GUI-specialized models, though they exhibit significant overfitting and poor robustness. These results underscore the necessity of comprehensive, multi-tiered evaluation frameworks.", "AI": {"tldr": "提出了一个跨平台、双语的大规模GUI基准测试VenusBench-GD，用于分层评估GUI元素定位能力，发现通用多模态模型在基础任务上已媲美专用模型，但高级任务仍依赖专用模型。", "motivation": "现有GUI定位基准存在数据量不足、领域覆盖窄、平台单一或需专业知识的局限，缺乏全面评估真实应用场景的能力。", "method": "构建大规模跨平台双语基准，覆盖多样应用与UI元素；设计高质量数据标注流程；提出分层任务分类法，将定位分为基础与高级两类共六项子任务。", "result": "通用多模态模型在基础定位任务上达到或超越专用GUI模型；高级任务仍以专用模型为主，但存在明显过拟合与鲁棒性差的问题。", "conclusion": "需要建立全面、多层次的评估框架以推动GUI智能体发展，当前基准揭示了模型能力差异与局限。"}}
{"id": "2512.16855", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.16855", "abs": "https://arxiv.org/abs/2512.16855", "authors": ["Khurram Khalil", "Khaza Anuarul Hoque"], "title": "TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge", "comment": "Published in the IEEE ICCAD 2025 conference", "summary": "Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.", "AI": {"tldr": "提出TOGGLE框架，首次将形式化方法（信号时序逻辑）引入大语言模型压缩，在保证语言属性的前提下实现高效压缩，支持边缘设备部署。", "motivation": "现有大语言模型压缩技术（如量化和剪枝）会损害语言属性且缺乏形式化保证，难以在资源受限的边缘设备上部署高性能模型。", "method": "使用信号时序逻辑（STL）形式化描述语言属性，通过STL鲁棒性引导的贝叶斯优化系统探索分层量化和剪枝配置，无需重新训练或微调。", "result": "在GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B上测试，计算成本（FLOPs）最高降低3.3倍，模型大小最高减少68.8%，同时满足所有语言属性约束。", "conclusion": "TOGGLE首次实现形式化方法与LLM压缩的结合，为边缘硬件部署提供可验证的高效压缩方案。"}}
{"id": "2512.16755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16755", "abs": "https://arxiv.org/abs/2512.16755", "authors": ["Siqi Wang", "Chao Liang", "Yunfan Gao", "Erxin Yu", "Sen Li", "Yushi Li", "Jing Li", "Haofen Wang"], "title": "CitySeeker: How Do VLMS Explore Embodied Urban Navigation With Implicit Human Needs?", "comment": null, "summary": "Vision-Language Models (VLMs) have made significant progress in explicit instruction-based navigation; however, their ability to interpret implicit human needs (e.g., \"I am thirsty\") in dynamic urban environments remains underexplored. This paper introduces CitySeeker, a novel benchmark designed to assess VLMs' spatial reasoning and decision-making capabilities for exploring embodied urban navigation to address implicit needs. CitySeeker includes 6,440 trajectories across 8 cities, capturing diverse visual characteristics and implicit needs in 7 goal-driven scenarios. Extensive experiments reveal that even top-performing models (e.g., Qwen2.5-VL-32B-Instruct) achieve only 21.1% task completion. We find key bottlenecks in error accumulation in long-horizon reasoning, inadequate spatial cognition, and deficient experiential recall. To further analyze them, we investigate a series of exploratory strategies-Backtracking Mechanisms, Enriching Spatial Cognition, and Memory-Based Retrieval (BCR), inspired by human cognitive mapping's emphasis on iterative observation-reasoning cycles and adaptive path optimization. Our analysis provides actionable insights for developing VLMs with robust spatial intelligence required for tackling \"last-mile\" navigation challenges.", "AI": {"tldr": "提出了CitySeeker基准测试，评估视觉语言模型在动态城市环境中理解隐式需求进行导航的能力，发现现有模型表现不佳，并分析了关键瓶颈和改进策略。", "motivation": "现有视觉语言模型在显式指令导航方面进展显著，但在理解动态城市环境中的隐式人类需求（如“我渴了”）方面能力不足，需要专门基准来评估和提升其空间推理与决策能力。", "method": "构建了包含6,440条轨迹、覆盖8个城市和7种目标场景的CitySeeker基准；通过实验分析模型瓶颈；提出了回溯机制、增强空间认知和基于记忆检索的BCR策略，受人类认知映射中迭代观察-推理循环和自适应路径优化的启发。", "result": "实验表明，即使顶级模型（如Qwen2.5-VL-32B-Instruct）的任务完成率也仅为21.1%；关键瓶颈包括长时程推理中的错误累积、空间认知不足和经验回忆缺陷。", "conclusion": "研究揭示了视觉语言模型在隐式需求导航中的局限性，提出的BCR策略为开发具有鲁棒空间智能的模型提供了可行见解，有助于解决“最后一公里”导航挑战。"}}
{"id": "2512.16504", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16504", "abs": "https://arxiv.org/abs/2512.16504", "authors": ["Qiushuo Cheng", "Jingjing Liu", "Catherine Morgan", "Alan Whone", "Majid Mirmehdi"], "title": "Skeleton-Snippet Contrastive Learning with Multiscale Feature Fusion for Action Localization", "comment": null, "summary": "The self-supervised pretraining paradigm has achieved great success in learning 3D action representations for skeleton-based action recognition using contrastive learning. However, learning effective representations for skeleton-based temporal action localization remains challenging and underexplored. Unlike video-level {action} recognition, detecting action boundaries requires temporally sensitive features that capture subtle differences between adjacent frames where labels change. To this end, we formulate a snippet discrimination pretext task for self-supervised pretraining, which densely projects skeleton sequences into non-overlapping segments and promotes features that distinguish them across videos via contrastive learning. Additionally, we build on strong backbones of skeleton-based action recognition models by fusing intermediate features with a U-shaped module to enhance feature resolution for frame-level localization. Our approach consistently improves existing skeleton-based contrastive learning methods for action localization on BABEL across diverse subsets and evaluation protocols. We also achieve state-of-the-art transfer learning performance on PKUMMD with pretraining on NTU RGB+D and BABEL.", "AI": {"tldr": "提出一种用于骨架时序动作定位的自监督预训练方法，通过片段判别任务和U形模块增强特征分辨率，在多个数据集上取得优异性能。", "motivation": "现有自监督预训练方法在骨架动作识别上成功，但时序动作定位需要更精细的时序敏感特征来检测动作边界，而这一领域尚未充分探索。", "method": "1. 设计片段判别前置任务：将骨架序列密集投影为不重叠片段，通过对比学习区分不同视频的片段；2. 在骨架动作识别骨干网络上融合U形模块，增强帧级定位的特征分辨率。", "result": "1. 在BABEL数据集上持续改进现有骨架对比学习方法；2. 在PKUMMD上实现迁移学习SOTA性能（使用NTU RGB+D和BABEL预训练）。", "conclusion": "所提出的自监督预训练框架能有效学习时序敏感特征，显著提升骨架时序动作定位性能，并具有优秀的跨数据集迁移能力。"}}
{"id": "2512.16856", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16856", "abs": "https://arxiv.org/abs/2512.16856", "authors": ["Nenad Tomašev", "Matija Franklin", "Julian Jacobs", "Sébastien Krier", "Simon Osindero"], "title": "Distributional AGI Safety", "comment": null, "summary": "AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.", "AI": {"tldr": "论文提出应重视\"拼凑式AGI\"假说，即通用智能可能首先通过多个子AGI智能体协调实现，而非单一AGI系统。为此需要超越个体对齐，建立基于虚拟沙盒经济体的分布式AGI安全框架。", "motivation": "现有AI安全研究主要假设未来会出现单一AGI系统，但忽视了另一种可能性：通用能力可能首先通过多个能力互补的子AGI智能体协调涌现。随着具备工具使用和协调能力的AI智能体快速部署，这种分布式AGI风险需要紧急关注。", "method": "提出分布式AGI安全框架，核心是设计和实施虚拟智能体沙盒经济系统（不可渗透或半渗透），其中智能体间交易由稳健的市场机制管理，并配合适当的可审计性、声誉管理和监督机制。", "result": "论证了\"拼凑式AGI\"假说需要被认真考虑，并应据此开发相应的安全保障和缓解措施。提出了从个体对齐转向群体协调安全的新框架。", "conclusion": "AI安全研究需要超越对单个系统的关注，为分布式AGI涌现场景做好准备。虚拟沙盒经济体结合市场机制和监督措施，是缓解集体风险的有效途径。"}}
{"id": "2512.16873", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16873", "abs": "https://arxiv.org/abs/2512.16873", "authors": ["Otman A. Basir"], "title": "The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI", "comment": null, "summary": "Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.", "AI": {"tldr": "本文提出社会责任感堆栈（SRS），一个六层架构框架，将社会价值观嵌入AI系统作为显式约束、保障措施、行为接口、审计机制和治理流程，为负责任AI提供可执行的工程机制。", "motivation": "当前负责任AI和治理工作提供重要规范原则，但缺乏贯穿系统生命周期的可执行工程机制。AI系统在影响人类行为、机构决策和社会结果的领域日益部署，需要将伦理原则转化为具体技术控制。", "method": "提出社会责任感堆栈（SRS）框架，将责任建模为社会技术系统的闭环监督控制问题。采用统一的约束形式化方法，引入安全包络和反馈解释，整合设计时保障与运行时监控及机构监督。", "result": "SRS框架展示了如何将公平性、自主性、认知负担和解释质量等社会价值目标转化为可连续监控和执行的工程控制。临床决策支持、协作自动驾驶和公共部门系统的案例研究验证了其可行性。", "conclusion": "SRS框架连接了伦理学、控制理论和AI治理，为可问责、自适应和可审计的社会技术AI系统提供了实践基础，将规范性目标转化为可操作的工程和运营控制。"}}
{"id": "2512.16511", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.16511", "abs": "https://arxiv.org/abs/2512.16511", "authors": ["Hossein Javidnia"], "title": "Multi-scale Attention-Guided Intrinsic Decomposition and Rendering Pass Prediction for Facial Images", "comment": null, "summary": "Accurate intrinsic decomposition of face images under unconstrained lighting is a prerequisite for photorealistic relighting, high-fidelity digital doubles, and augmented-reality effects. This paper introduces MAGINet, a Multi-scale Attention-Guided Intrinsics Network that predicts a $512\\times512$ light-normalized diffuse albedo map from a single RGB portrait. MAGINet employs hierarchical residual encoding, spatial-and-channel attention in a bottleneck, and adaptive multi-scale feature fusion in the decoder, yielding sharper albedo boundaries and stronger lighting invariance than prior U-Net variants. The initial albedo prediction is upsampled to $1024\\times1024$ and refined by a lightweight three-layer CNN (RefinementNet). Conditioned on this refined albedo, a Pix2PixHD-based translator then predicts a comprehensive set of five additional physically based rendering passes: ambient occlusion, surface normal, specular reflectance, translucency, and raw diffuse colour (with residual lighting). Together with the refined albedo, these six passes form the complete intrinsic decomposition. Trained with a combination of masked-MSE, VGG, edge, and patch-LPIPS losses on the FFHQ-UV-Intrinsics dataset, the full pipeline achieves state-of-the-art performance for diffuse albedo estimation and demonstrates significantly improved fidelity for the complete rendering stack compared to prior methods. The resulting passes enable high-quality relighting and material editing of real faces.", "AI": {"tldr": "提出MAGINet网络，从单张RGB人像中预测512×512的光照归一化漫反射反照率图，并通过RefinementNet和Pix2PixHD翻译器生成完整的六通道本征分解，实现高质量人脸重光照和材质编辑。", "motivation": "无约束光照下人脸图像的本征分解是照片级重光照、高保真数字替身和增强现实效果的前提，现有方法在反照率边界清晰度和光照不变性方面存在不足。", "method": "采用多尺度注意力引导网络（MAGINet）进行初始反照率预测，包含分层残差编码、瓶颈空间通道注意力、自适应多尺度特征融合；通过轻量级RefinementNet上采样至1024×1024；基于Pix2PixHD的翻译器预测其余五个物理渲染通道（环境光遮蔽、法线、镜面反射、半透明、原始漫反射颜色）。", "result": "在FFHQ-UV-Intrinsics数据集上训练，结合掩码MSE、VGG、边缘和patch-LPIPS损失，在漫反射反照率估计上达到SOTA，完整渲染通道的保真度显著优于先前方法。", "conclusion": "该方法实现了高质量的人脸本征分解，生成的六通道渲染数据支持真实人脸的高质量重光照和材质编辑，为数字人创建和AR应用提供了有效工具。"}}
{"id": "2512.16523", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16523", "abs": "https://arxiv.org/abs/2512.16523", "authors": ["Zhiwei Li", "Yitian Pang", "Weining Wang", "Zhenan Sun", "Qi Li"], "title": "TTP: Test-Time Padding for Adversarial Detection and Robust Adaptation on Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs), such as CLIP, have achieved impressive zero-shot recognition performance but remain highly susceptible to adversarial perturbations, posing significant risks in safety-critical scenarios. Previous training-time defenses rely on adversarial fine-tuning, which requires labeled data and costly retraining, while existing test-time strategies fail to reliably distinguish between clean and adversarial inputs, thereby preventing both adversarial robustness and clean accuracy from reaching their optimum. To address these limitations, we propose Test-Time Padding (TTP), a lightweight defense framework that performs adversarial detection followed by targeted adaptation at inference. TTP identifies adversarial inputs via the cosine similarity shift between CLIP feature embeddings computed before and after spatial padding, yielding a universal threshold for reliable detection across architectures and datasets. For detected adversarial cases, TTP employs trainable padding to restore disrupted attention patterns, coupled with a similarity-aware ensemble strategy for a more robust final prediction. For clean inputs, TTP leaves them unchanged by default or optionally integrates existing test-time adaptation techniques for further accuracy gains. Comprehensive experiments on diverse CLIP backbones and fine-grained benchmarks show that TTP consistently surpasses state-of-the-art test-time defenses, delivering substantial improvements in adversarial robustness without compromising clean accuracy. The code for this paper will be released soon.", "AI": {"tldr": "提出了一种名为TTP的轻量级防御框架，用于提升视觉语言模型（如CLIP）的对抗鲁棒性。该框架在推理时通过空间填充前后的特征相似度变化检测对抗样本，并采用可训练填充和相似度感知集成策略进行自适应修复，同时保持干净样本的准确性。", "motivation": "现有视觉语言模型在零样本识别任务中表现优异，但对对抗扰动高度敏感，存在安全风险。传统训练时防御方法依赖标注数据和昂贵重训练，而测试时策略难以可靠区分干净与对抗样本，无法同时优化鲁棒性和准确性。", "method": "提出测试时填充（TTP）框架：1）通过计算空间填充前后CLIP特征嵌入的余弦相似度偏移检测对抗样本；2）对检测到的对抗样本使用可训练填充恢复被破坏的注意力模式，并结合相似度感知集成策略进行预测；3）对干净样本默认保持不变或集成现有测试时自适应技术。", "result": "在多种CLIP骨干网络和细粒度基准测试上的实验表明，TTP持续优于现有测试时防御方法，显著提升对抗鲁棒性且不损害干净样本准确率。", "conclusion": "TTP是一种高效轻量的测试时防御框架，无需重训练或标注数据，能可靠检测并修复对抗样本，为视觉语言模型的安全部署提供了实用解决方案。"}}
{"id": "2511.00456", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.00456", "abs": "https://arxiv.org/abs/2511.00456", "authors": ["Kiran Shahi", "Anup Bagale"], "title": "Weakly Supervised Pneumonia Localization from Chest X-Rays Using Deep Neural Network and Grad-CAM Explanations", "comment": "https://github.com/kiranshahi/pneumonia-analysis", "summary": "Chest X-ray imaging is commonly used to diagnose pneumonia, but accurately localizing the pneumonia-affected regions typically requires detailed pixel-level annotations, which are costly and time consuming to obtain. To address this limitation, this study proposes a weakly supervised deep learning framework for pneumonia classification and localization using Gradient-weighted Class Activation Mapping (Grad-CAM). Instead of relying on costly pixel-level annotations, the proposed method utilizes image-level labels to generate clinically meaningful heatmaps that highlight pneumonia-affected regions. Furthermore, we evaluate seven pre-trained deep learning models, including a Vision Transformer, under identical training conditions, using focal loss and patient-wise splits to prevent data leakage. Experimental results suggest that all models achieved high classification accuracy (96--98\\%), with ResNet-18 and EfficientNet-B0 showing the best overall performance and MobileNet-V3 providing an efficient lightweight alternative. Grad-CAM heatmap visualizations confirm that the proposed methods focus on clinically relevant lung regions, supporting the use of explainable AI for radiological diagnostics. Overall, this work highlights the potential of weakly supervised, explainable models that enhance transparency and clinical trust in AI-assisted pneumonia screening.", "AI": {"tldr": "提出一种基于弱监督学习和Grad-CAM的肺炎分类与定位框架，仅使用图像级标签即可生成肺炎区域热力图，在多个预训练模型上验证了高分类准确率与临床可解释性。", "motivation": "传统肺炎诊断依赖胸部X光图像，但精确标注肺炎区域需要像素级注释，成本高且耗时。本研究旨在开发一种仅需图像级标签的弱监督方法，降低标注负担的同时提供可解释的病灶定位。", "method": "采用弱监督深度学习框架，结合Grad-CAM生成热力图；评估了7种预训练模型（包括Vision Transformer），使用焦点损失函数和患者级数据划分防止数据泄露；在相同训练条件下比较模型性能。", "result": "所有模型分类准确率达96%-98%，其中ResNet-18和EfficientNet-B0综合性能最佳，MobileNet-V3为轻量高效替代方案；Grad-CAM热力图能有效聚焦临床相关肺区，验证了方法的可解释性。", "conclusion": "弱监督可解释模型能显著降低标注成本，提升AI辅助肺炎筛查的透明度与临床可信度，为放射诊断提供了实用且高效的解决方案。"}}
{"id": "2512.16561", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16561", "abs": "https://arxiv.org/abs/2512.16561", "authors": ["Yuxin Wang", "Lei Ke", "Boqiang Zhang", "Tianyuan Qu", "Hanxun Yu", "Zhenpeng Huang", "Meng Yu", "Dan Xu", "Dong Yu"], "title": "N3D-VLM: Native 3D Grounding Enables Accurate Spatial Reasoning in Vision-Language Models", "comment": "Project Page: https://n3d-vlm.github.io", "summary": "While current multimodal models can answer questions based on 2D images, they lack intrinsic 3D object perception, limiting their ability to comprehend spatial relationships and depth cues in 3D scenes. In this work, we propose N3D-VLM, a novel unified framework that seamlessly integrates native 3D object perception with 3D-aware visual reasoning, enabling both precise 3D grounding and interpretable spatial understanding. Unlike conventional end-to-end models that directly predict answers from RGB/RGB-D inputs, our approach equips the model with native 3D object perception capabilities, enabling it to directly localize objects in 3D space based on textual descriptions. Building upon accurate 3D object localization, the model further performs explicit reasoning in 3D, achieving more interpretable and structured spatial understanding. To support robust training for these capabilities, we develop a scalable data construction pipeline that leverages depth estimation to lift large-scale 2D annotations into 3D space, significantly increasing the diversity and coverage for 3D object grounding data, yielding over six times larger than the largest existing single-image 3D detection dataset. Moreover, the pipeline generates spatial question-answering datasets that target chain-of-thought (CoT) reasoning in 3D, facilitating joint training for both 3D object localization and 3D spatial reasoning. Experimental results demonstrate that our unified framework not only achieves state-of-the-art performance on 3D grounding tasks, but also consistently surpasses existing methods in 3D spatial reasoning in vision-language model.", "AI": {"tldr": "提出N3D-VLM框架，将原生3D物体感知与3D视觉推理结合，通过大规模数据构建流程提升3D物体定位与空间推理能力。", "motivation": "现有多模态模型缺乏内在3D物体感知能力，限制了其对3D场景空间关系和深度线索的理解。", "method": "开发统一框架，通过深度估计将2D标注提升至3D空间构建训练数据，支持3D物体定位和链式空间推理的联合训练。", "result": "框架在3D定位任务上达到最优性能，在3D空间推理方面持续超越现有视觉语言模型方法。", "conclusion": "N3D-VLM通过集成原生3D感知与显式3D推理，实现了更可解释的结构化空间理解，显著提升了3D视觉语言任务性能。"}}
{"id": "2512.16564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16564", "abs": "https://arxiv.org/abs/2512.16564", "authors": ["Kirill Mazur", "Marwan Taher", "Andrew J. Davison"], "title": "4D Primitive-Mâché: Glueing Primitives for Persistent 4D Scene Reconstruction", "comment": "For project page, see https://makezur.github.io/4DPM/", "summary": "We present a dynamic reconstruction system that receives a casual monocular RGB video as input, and outputs a complete and persistent reconstruction of the scene. In other words, we reconstruct not only the the currently visible parts of the scene, but also all previously viewed parts, which enables replaying the complete reconstruction across all timesteps.\n  Our method decomposes the scene into a set of rigid 3D primitives, which are assumed to be moving throughout the scene. Using estimated dense 2D correspondences, we jointly infer the rigid motion of these primitives through an optimisation pipeline, yielding a 4D reconstruction of the scene, i.e. providing 3D geometry dynamically moving through time. To achieve this, we also introduce a mechanism to extrapolate motion for objects that become invisible, employing motion-grouping techniques to maintain continuity.\n  The resulting system enables 4D spatio-temporal awareness, offering capabilities such as replayable 3D reconstructions of articulated objects through time, multi-object scanning, and object permanence. On object scanning and multi-object datasets, our system significantly outperforms existing methods both quantitatively and qualitatively.", "AI": {"tldr": "提出一种动态重建系统，可从单目RGB视频中重建完整且持久的4D场景，包括可见和不可见部分，实现场景回放。", "motivation": "现有方法通常只能重建当前可见场景，无法处理物体消失后的状态。需要实现完整的时空感知，支持多物体扫描和物体持久性追踪。", "method": "将场景分解为刚性3D基元，通过密集2D对应关系联合推断基元刚性运动，采用优化管道实现4D重建，并引入运动外推机制处理不可见物体。", "result": "在物体扫描和多物体数据集上，系统在定量和定性评估中显著优于现有方法，实现了可回放的3D重建和物体运动连续性。", "conclusion": "该系统实现了完整的4D时空感知，为动态场景重建提供了新解决方案，在多个应用场景中表现出优越性能。"}}
{"id": "2512.16567", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16567", "abs": "https://arxiv.org/abs/2512.16567", "authors": ["Yin Zhang", "Yongqiang Zhang", "Yaoyue Zheng", "Bogdan Raducanu", "Dan Liu"], "title": "Causal-Tune: Mining Causal Factors from Vision Foundation Models for Domain Generalized Semantic Segmentation", "comment": "Accepted by AAAI 2026", "summary": "Fine-tuning Vision Foundation Models (VFMs) with a small number of parameters has shown remarkable performance in Domain Generalized Semantic Segmentation (DGSS). Most existing works either train lightweight adapters or refine intermediate features to achieve better generalization on unseen domains. However, they both overlook the fact that long-term pre-trained VFMs often exhibit artifacts, which hinder the utilization of valuable representations and ultimately degrade DGSS performance. Inspired by causal mechanisms, we observe that these artifacts are associated with non-causal factors, which usually reside in the low- and high-frequency components of the VFM spectrum. In this paper, we explicitly examine the causal and non-causal factors of features within VFMs for DGSS, and propose a simple yet effective method to identify and disentangle them, enabling more robust domain generalization. Specifically, we propose Causal-Tune, a novel fine-tuning strategy designed to extract causal factors and suppress non-causal ones from the features of VFMs. First, we extract the frequency spectrum of features from each layer using the Discrete Cosine Transform (DCT). A Gaussian band-pass filter is then applied to separate the spectrum into causal and non-causal components. To further refine the causal components, we introduce a set of causal-aware learnable tokens that operate in the frequency domain, while the non-causal components are discarded. Finally, refined features are transformed back into the spatial domain via inverse DCT and passed to the next layer. Extensive experiments conducted on various cross-domain tasks demonstrate the effectiveness of Causal-Tune. In particular, our method achieves superior performance under adverse weather conditions, improving +4.8% mIoU over the baseline in snow conditions.", "AI": {"tldr": "提出Causal-Tune方法，通过频域分析分离视觉基础模型中的因果与非因果因素，提升领域泛化语义分割性能", "motivation": "现有方法忽视预训练视觉基础模型中存在的伪影（artifacts），这些伪影与非因果因素相关，阻碍模型利用有价值的表征，导致领域泛化性能下降", "method": "使用离散余弦变换提取特征频谱，应用高斯带通滤波器分离因果与非因果成分，引入可学习的因果感知令牌在频域细化因果成分，通过逆变换重构特征", "result": "在多种跨领域任务中验证有效性，在恶劣天气条件下表现突出，雪天条件下比基线提升4.8% mIoU", "conclusion": "通过因果机制分析并分离特征中的因果与非因果因素，能有效提升视觉基础模型在领域泛化语义分割中的鲁棒性"}}
{"id": "2512.16577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16577", "abs": "https://arxiv.org/abs/2512.16577", "authors": ["Nico Albert Disch", "Saikat Roy", "Constantin Ulrich", "Yannick Kirchhoff", "Maximilian Rokuss", "Robin Peretzke", "David Zimmerer", "Klaus Maier-Hein"], "title": "CRONOS: Continuous Time Reconstruction for 4D Medical Longitudinal Series", "comment": "https://github.com/MIC-DKFZ/Longitudinal4DMed", "summary": "Forecasting how 3D medical scans evolve over time is important for disease progression, treatment planning, and developmental assessment. Yet existing models either rely on a single prior scan, fixed grid times, or target global labels, which limits voxel-level forecasting under irregular sampling. We present CRONOS, a unified framework for many-to-one prediction from multiple past scans that supports both discrete (grid-based) and continuous (real-valued) timestamps in one model, to the best of our knowledge the first to achieve continuous sequence-to-image forecasting for 3D medical data. CRONOS learns a spatio-temporal velocity field that transports context volumes toward a target volume at an arbitrary time, while operating directly in 3D voxel space. Across three public datasets spanning Cine-MRI, perfusion CT, and longitudinal MRI, CRONOS outperforms other baselines, while remaining computationally competitive. We will release code and evaluation protocols to enable reproducible, multi-dataset benchmarking of multi-context, continuous-time forecasting.", "AI": {"tldr": "CRONOS是一个用于3D医学扫描时序预测的统一框架，支持从多个历史扫描中进行多对一预测，并处理离散和连续时间戳，首次实现3D医学数据的连续序列到图像预测。", "motivation": "现有3D医学扫描时序预测模型依赖单一先验扫描、固定时间网格或全局标签，限制了不规则采样下的体素级预测能力，需要支持多历史扫描和连续时间戳的灵活框架。", "method": "提出CRONOS框架，学习时空速度场将上下文体积传输到任意时间的目标体积，直接在3D体素空间操作，支持离散和连续时间戳的统一建模。", "result": "在Cine-MRI、灌注CT和纵向MRI三个公共数据集上，CRONOS优于其他基线方法，同时保持计算效率竞争力。", "conclusion": "CRONOS首次实现3D医学数据的连续序列到图像预测，为多上下文、连续时间预测提供了可复现的多数据集基准测试框架。"}}
{"id": "2512.16586", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16586", "abs": "https://arxiv.org/abs/2512.16586", "authors": ["Shaohua Wu", "Tong Yu", "Shenling Wang", "Xudong Zhao"], "title": "Yuan-TecSwin: A text conditioned Diffusion model with Swin-transformer blocks", "comment": null, "summary": "Diffusion models have shown remarkable capacity in image synthesis based on their U-shaped architecture and convolutional neural networks (CNN) as basic blocks. The locality of the convolution operation in CNN may limit the model's ability to understand long-range semantic information. To address this issue, we propose Yuan-TecSwin, a text-conditioned diffusion model with Swin-transformer in this work. The Swin-transformer blocks take the place of CNN blocks in the encoder and decoder, to improve the non-local modeling ability in feature extraction and image restoration. The text-image alignment is improved with a well-chosen text encoder, effective utilization of text embedding, and careful design in the incorporation of text condition. Using an adapted time step to search in different diffusion stages, inference performance is further improved by 10%. Yuan-TecSwin achieves the state-of-the-art FID score of 1.37 on ImageNet generation benchmark, without any additional models at different denoising stages. In a side-by-side comparison, we find it difficult for human interviewees to tell the model-generated images from the human-painted ones.", "AI": {"tldr": "提出Yuan-TecSwin扩散模型，用Swin-transformer替换CNN提升长距离语义理解，在ImageNet生成任务上取得SOTA的FID 1.37，人类难以区分生成图像与手绘图像。", "motivation": "传统扩散模型基于CNN的U型架构，卷积操作的局部性限制了模型对长距离语义信息的理解能力。", "method": "1. 用Swin-transformer块替换编码器/解码器中的CNN块；2. 优化文本编码器选择、文本嵌入利用和文本条件融合设计；3. 采用自适应时间步搜索策略提升推理性能10%。", "result": "1. ImageNet生成基准FID达1.37（SOTA）；2. 无需多阶段去噪辅助模型；3. 人类评估者难以区分生成图像与真实手绘图像。", "conclusion": "Swin-transformer能有效提升扩散模型的非局部建模能力，结合文本对齐优化可显著改善生成质量与真实性。"}}
{"id": "2512.16584", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16584", "abs": "https://arxiv.org/abs/2512.16584", "authors": ["Jintao Tong", "Jiaqi Gu", "Yujing Lou", "Lubin Fan", "Yixiong Zou", "Yue Wu", "Jieping Ye", "Ruixuan Li"], "title": "Sketch-in-Latents: Eliciting Unified Reasoning in MLLMs", "comment": "14 pages, 11 figures", "summary": "While Multimodal Large Language Models (MLLMs) excel at visual understanding tasks through text reasoning, they often fall short in scenarios requiring visual imagination. Unlike current works that take predefined external toolkits or generate images during thinking, however, humans can form flexible visual-text imagination and interactions during thinking without predefined toolkits, where one important reason is that humans construct the visual-text thinking process in a unified space inside the brain. Inspired by this capability, given that current MLLMs already encode visual and text information in the same feature space, we hold that visual tokens can be seamlessly inserted into the reasoning process carried by text tokens, where ideally, all visual imagination processes can be encoded by the latent features. To achieve this goal, we propose Sketch-in-Latents (SkiLa), a novel paradigm for unified multi-modal reasoning that expands the auto-regressive capabilities of MLLMs to natively generate continuous visual embeddings, termed latent sketch tokens, as visual thoughts. During multi-step reasoning, the model dynamically alternates between textual thinking mode for generating textual think tokens and visual sketching mode for generating latent sketch tokens. A latent visual semantics reconstruction mechanism is proposed to ensure these latent sketch tokens are semantically grounded. Extensive experiments demonstrate that SkiLa achieves superior performance on vision-centric tasks while exhibiting strong generalization to diverse general multi-modal benchmarks. Codes will be released at https://github.com/TungChintao/SkiLa.", "AI": {"tldr": "提出SkiLa方法，使多模态大语言模型能够生成连续的视觉嵌入作为视觉思维，实现统一的视觉-文本推理，提升视觉想象能力。", "motivation": "当前多模态大语言模型在需要视觉想象的场景中表现不足，而人类能在统一思维空间中灵活进行视觉-文本想象与交互。受此启发，利用现有模型将视觉和文本信息编码到同一特征空间的能力，探索在推理过程中无缝插入视觉标记。", "method": "提出SkiLa范式，扩展多模态大语言模型的自回归能力，使其原生生成连续的视觉嵌入（潜在草图标记）。模型在推理过程中动态切换文本思维模式和视觉草图模式，并引入潜在视觉语义重建机制确保草图标记的语义基础。", "result": "实验表明SkiLa在视觉中心任务上表现优异，同时在多样化通用多模态基准测试中展现出强大的泛化能力。", "conclusion": "SkiLa通过统一的多模态推理范式，有效增强了多模态大语言模型的视觉想象能力，为更接近人类思维方式的AI推理提供了新方向。"}}
{"id": "2512.16609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16609", "abs": "https://arxiv.org/abs/2512.16609", "authors": ["Ayush Bhavsar"], "title": "Hazedefy: A Lightweight Real-Time Image and Video Dehazing Pipeline for Practical Deployment", "comment": "4 pages, 2 figures. Code and demo available at https://doi.org/10.5281/zenodo.17915355", "summary": "This paper introduces Hazedefy, a lightweight and application-focused dehazing pipeline intended for real-time video and live camera feed enhancement. Hazedefy prioritizes computational simplicity and practical deployability on consumer-grade hardware, building upon the Dark Channel Prior (DCP) concept and the atmospheric scattering model. Key elements include gamma-adaptive reconstruction, a fast transmission approximation with lower bounds for numerical stability, a stabilized atmospheric light estimator based on fractional top-pixel averaging, and an optional color balance stage. The pipeline is suitable for mobile and embedded applications, as experimental demonstrations on real-world images and videos show improved visibility and contrast without requiring GPU acceleration.", "AI": {"tldr": "本文提出了Hazedefy，一种轻量级、面向应用的实时视频和直播去雾管道，专注于在消费级硬件上实现计算简单性和实际可部署性。", "motivation": "现有去雾方法通常计算复杂，难以在移动和嵌入式设备上实时运行，因此需要一种轻量级、高效的去雾解决方案，以提升实时视频和直播的视觉质量。", "method": "基于暗通道先验（DCP）和大气散射模型，采用伽马自适应重建、快速传输近似（带数值稳定性下界）、基于分数顶部像素平均的稳定大气光估计器，以及可选的颜色平衡阶段。", "result": "在真实世界图像和视频上的实验表明，Hazedefy能够在不依赖GPU加速的情况下显著提升可见度和对比度，适用于移动和嵌入式应用。", "conclusion": "Hazedefy是一种高效、轻量级的去雾管道，能够在消费级硬件上实现实时视频和直播的去雾增强，具有实际部署价值。"}}
{"id": "2512.16743", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16743", "abs": "https://arxiv.org/abs/2512.16743", "authors": ["Mahadev Prasad Panda", "Purnachandra Rao Makkena", "Srivatsa Prativadibhayankaram", "Siegfried Fößel", "André Kaup"], "title": "TreeNet: A Light Weight Model for Low Bitrate Image Compression", "comment": null, "summary": "Reducing computational complexity remains a critical challenge for the widespread adoption of learning-based image compression techniques. In this work, we propose TreeNet, a novel low-complexity image compression model that leverages a binary tree-structured encoder-decoder architecture to achieve efficient representation and reconstruction. We employ attentional feature fusion mechanism to effectively integrate features from multiple branches. We evaluate TreeNet on three widely used benchmark datasets and compare its performance against competing methods including JPEG AI, a recent standard in learning-based image compression. At low bitrates, TreeNet achieves an average improvement of 4.83% in BD-rate over JPEG AI, while reducing model complexity by 87.82%. Furthermore, we conduct extensive ablation studies to investigate the influence of various latent representations within TreeNet, offering deeper insights into the factors contributing to reconstruction.", "AI": {"tldr": "提出TreeNet，一种基于二叉树编码器-解码器架构的低复杂度图像压缩模型，通过注意力特征融合机制整合多分支特征，在降低计算复杂度的同时提升压缩性能。", "motivation": "降低学习型图像压缩技术的计算复杂度是其广泛应用的关键挑战，现有方法在复杂度和性能之间难以平衡。", "method": "采用二叉树结构的编码器-解码器架构，结合注意力特征融合机制整合多分支特征，并进行广泛的消融实验分析潜在表示的影响。", "result": "在三个基准数据集上，TreeNet在低码率下比JPEG AI平均BD-rate提升4.83%，同时模型复杂度降低87.82%。", "conclusion": "TreeNet通过创新的树形结构和特征融合机制，在显著降低计算复杂度的同时实现了优于现有标准的压缩性能，为高效学习型图像压缩提供了新思路。"}}
{"id": "2512.16685", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16685", "abs": "https://arxiv.org/abs/2512.16685", "authors": ["Gonçalo Gaspar Alves", "Shekoufeh Gorgi Zadeh", "Andreas Husch", "Ben Bausch"], "title": "Few-Shot Fingerprinting Subject Re-Identification in 3D-MRI and 2D-X-Ray", "comment": null, "summary": "Combining open-source datasets can introduce data leakage if the same subject appears in multiple sets, leading to inflated model performance. To address this, we explore subject fingerprinting, mapping all images of a subject to a distinct region in latent space, to enable subject re-identification via similarity matching. Using a ResNet-50 trained with triplet margin loss, we evaluate few-shot fingerprinting on 3D MRI and 2D X-ray data in both standard (20-way 1-shot) and challenging (1000-way 1-shot) scenarios. The model achieves high Mean- Recall-@-K scores: 99.10% (20-way 1-shot) and 90.06% (500-way 5-shot) on ChestXray-14; 99.20% (20-way 1-shot) and 98.86% (100-way 3-shot) on BraTS- 2021.", "AI": {"tldr": "本文提出一种基于主题指纹识别的方法，通过将同一受试者的所有图像映射到潜在空间中的特定区域，解决开放数据集合并时的数据泄漏问题，并在MRI和X射线数据上验证了其有效性。", "motivation": "合并开源数据集时，若同一受试者出现在多个数据集中会导致数据泄漏，从而虚增模型性能。为解决此问题，研究探索通过主题指纹识别实现受试者重识别。", "method": "使用基于三元组边界损失的ResNet-50模型，在3D MRI和2D X射线数据上进行少样本指纹识别评估，包括标准（20-way 1-shot）和挑战性（1000-way 1-shot）场景。", "result": "模型在ChestXray-14数据集上获得99.10%（20-way 1-shot）和90.06%（500-way 5-shot）的Mean-Recall-@-K分数；在BraTS-2021数据集上获得99.20%（20-way 1-shot）和98.86%（100-way 3-shot）的分数。", "conclusion": "主题指纹识别方法能有效识别跨数据集的同一受试者，为解决数据合并时的数据泄漏问题提供了可行方案，并在不同模态医学影像数据上表现出色。"}}
{"id": "2512.16791", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16791", "abs": "https://arxiv.org/abs/2512.16791", "authors": ["Shuting Zhao", "Zeyu Xiao", "Xinrong Chen"], "title": "KineST: A Kinematics-guided Spatiotemporal State Space Model for Human Motion Tracking from Sparse Signals", "comment": "Accepted by AAAI 2026", "summary": "Full-body motion tracking plays an essential role in AR/VR applications, bridging physical and virtual interactions. However, it is challenging to reconstruct realistic and diverse full-body poses based on sparse signals obtained by head-mounted displays, which are the main devices in AR/VR scenarios. Existing methods for pose reconstruction often incur high computational costs or rely on separately modeling spatial and temporal dependencies, making it difficult to balance accuracy, temporal coherence, and efficiency. To address this problem, we propose KineST, a novel kinematics-guided state space model, which effectively extracts spatiotemporal dependencies while integrating local and global pose perception. The innovation comes from two core ideas. Firstly, in order to better capture intricate joint relationships, the scanning strategy within the State Space Duality framework is reformulated into kinematics-guided bidirectional scanning, which embeds kinematic priors. Secondly, a mixed spatiotemporal representation learning approach is employed to tightly couple spatial and temporal contexts, balancing accuracy and smoothness. Additionally, a geometric angular velocity loss is introduced to impose physically meaningful constraints on rotational variations for further improving motion stability. Extensive experiments demonstrate that KineST has superior performance in both accuracy and temporal consistency within a lightweight framework. Project page: https://kaka-1314.github.io/KineST/", "AI": {"tldr": "提出KineST模型，通过运动学引导的状态空间模型，从稀疏头显信号重建真实多样的全身姿态，平衡精度、时序一致性与效率。", "motivation": "AR/VR应用中，基于头显稀疏信号重建真实多样全身姿态存在挑战，现有方法计算成本高或时空依赖建模分离，难以兼顾精度、时序一致性和效率。", "method": "提出运动学引导状态空间模型KineST：1）在状态空间对偶框架中引入运动学先验，设计双向扫描策略捕获关节关系；2）采用混合时空表示学习耦合时空上下文；3）引入几何角速度损失增强运动稳定性。", "result": "大量实验表明，KineST在轻量级框架下，在精度和时序一致性方面均优于现有方法。", "conclusion": "KineST通过运动学引导的时空建模，有效解决了稀疏信号下全身姿态重建的精度与效率平衡问题，为AR/VR应用提供了实用解决方案。"}}
{"id": "2512.16826", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16826", "abs": "https://arxiv.org/abs/2512.16826", "authors": ["Arslan Amin", "Rafia Mumtaz", "Muhammad Jawad Bashir", "Syed Mohammad Hassan Zaidi"], "title": "Next-Generation License Plate Detection and Recognition System using YOLOv8", "comment": "6 pages, 5 figures. Accepted and published in the 2023 IEEE 20th International Conference on Smart Communities: Improving Quality of Life using AI, Robotics and IoT (HONET)", "summary": "In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.", "AI": {"tldr": "本研究评估了YOLOv8不同变体在车牌识别和字符识别任务上的性能，提出了一种优化的识别流程，在保持计算效率的同时实现了高精度。", "motivation": "在智能交通系统的发展中，车牌识别技术面临多样化环境中实时准确识别的挑战，现有方法在实时性和准确性方面仍有不足。", "method": "使用两个独立数据集进行训练和评估；采用YOLOv8 Nano进行车牌检测，YOLOv8 Small进行字符识别；提出基于x轴位置的字符排序方法。", "result": "YOLOv8 Nano在车牌识别任务上达到0.964精确度和0.918 mAP50；YOLOv8 Small在字符识别任务上达到0.92精确度和0.91 mAP50。", "conclusion": "提出的优化流程在计算效率和识别精度之间取得了良好平衡，为智能交通系统的边缘设备部署提供了可靠基础。"}}
{"id": "2512.16615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16615", "abs": "https://arxiv.org/abs/2512.16615", "authors": ["Yifan Zhou", "Zeqi Xiao", "Tianyi Wei", "Shuai Yang", "Xingang Pan"], "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers", "comment": "Code is available at: https://github.com/SingleZombie/LLSA", "summary": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA", "AI": {"tldr": "提出LLSA，一种可训练的层次化稀疏注意力机制，将选择和注意力计算复杂度从二次降低到对数线性，显著加速长序列扩散变换器的训练和推理。", "motivation": "扩散变换器（DiTs）在视觉生成中表现优异，但其二次自注意力成本限制了向长令牌序列的扩展。现有Top-K稀疏注意力方法仍存在（i）压缩令牌上的二次选择成本，以及（ii）序列增长时需增加K值以保持质量的问题。", "method": "设计对数线性稀疏注意力（LLSA），采用层次化结构进行层次Top-K选择，并引入层次KV丰富机制以在注意力计算中使用更少但不同粒度的令牌来保留全局上下文。开发了仅使用稀疏索引的高性能GPU实现，无需稠密注意力掩码。", "result": "在不使用分块和VAE编码的高分辨率像素空间图像生成任务中，LLSA在256x256像素令牌序列上实现了注意力推理加速28.27倍，DiT训练加速6.09倍，同时保持生成质量。", "conclusion": "LLSA为高效训练长序列DiTs提供了有前景的方向，代码已开源。"}}
{"id": "2512.16620", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16620", "abs": "https://arxiv.org/abs/2512.16620", "authors": ["Kanwal Aftab", "Graham Adams", "Mark Scanlon"], "title": "Plug to Place: Indoor Multimedia Geolocation from Electrical Sockets for Digital Investigation", "comment": null, "summary": "Computer vision is a rapidly evolving field, giving rise to powerful new tools and techniques in digital forensic investigation, and shows great promise for novel digital forensic applications. One such application, indoor multimedia geolocation, has the potential to become a crucial aid for law enforcement in the fight against human trafficking, child exploitation, and other serious crimes. While outdoor multimedia geolocation has been widely explored, its indoor counterpart remains underdeveloped due to challenges such as similar room layouts, frequent renovations, visual ambiguity, indoor lighting variability, unreliable GPS signals, and limited datasets in sensitive domains. This paper introduces a pipeline that uses electric sockets as consistent indoor markers for geolocation, since plug socket types are standardised by country or region. The three-stage deep learning pipeline detects plug sockets (YOLOv11, mAP@0.5 = 0.843), classifies them into one of 12 plug socket types (Xception, accuracy = 0.912), and maps the detected socket types to countries (accuracy = 0.96 at >90% threshold confidence). To address data scarcity, two dedicated datasets were created: socket detection dataset of 2,328 annotated images expanded to 4,072 through augmentation, and a classification dataset of 3,187 images across 12 plug socket classes. The pipeline was evaluated on the Hotels-50K dataset, focusing on the TraffickCam subset of crowd-sourced hotel images, which capture real-world conditions such as poor lighting and amateur angles. This dataset provides a more realistic evaluation than using professional, well-lit, often wide-angle images from travel websites. This framework demonstrates a practical step toward real-world digital forensic applications. The code, trained models, and the data for this paper are available open source.", "AI": {"tldr": "提出了一种基于电源插座作为室内标记物的多媒体地理位置识别管道，通过检测、分类插座类型并映射到国家/地区，解决室内地理位置识别难题。", "motivation": "室内多媒体地理位置识别对打击人口贩卖、儿童剥削等犯罪有重要价值，但面临布局相似、装修频繁、视觉模糊、光照变化、GPS信号弱及敏感领域数据稀缺等挑战。", "method": "采用三阶段深度学习管道：1) 使用YOLOv11检测插座；2) 使用Xception将插座分类为12种类型；3) 将插座类型映射到国家/地区。创建了两个专用数据集并通过数据增强扩充。", "result": "插座检测mAP@0.5达0.843，插座类型分类准确率0.912，国家映射准确率在>90%置信度下达0.96。在Hotels-50K数据集的TraffickCam子集上验证了管道在真实场景下的有效性。", "conclusion": "该框架为实际数字取证应用提供了可行方案，所有代码、模型和数据均已开源。"}}
{"id": "2512.16625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16625", "abs": "https://arxiv.org/abs/2512.16625", "authors": ["Linghui Shen", "Mingyue Cui", "Xingyi Yang"], "title": "DeContext as Defense: Safe Image Editing in Diffusion Transformers", "comment": "17 pages, 11 figures", "summary": "In-context diffusion models allow users to modify images with remarkable ease and realism. However, the same power raises serious privacy concerns: personal images can be easily manipulated for identity impersonation, misinformation, or other malicious uses, all without the owner's consent. While prior work has explored input perturbations to protect against misuse in personalized text-to-image generation, the robustness of modern, large-scale in-context DiT-based models remains largely unexamined. In this paper, we propose DeContext, a new method to safeguard input images from unauthorized in-context editing. Our key insight is that contextual information from the source image propagates to the output primarily through multimodal attention layers. By injecting small, targeted perturbations that weaken these cross-attention pathways, DeContext breaks this flow, effectively decouples the link between input and output. This simple defense is both efficient and robust. We further show that early denoising steps and specific transformer blocks dominate context propagation, which allows us to concentrate perturbations where they matter most. Experiments on Flux Kontext and Step1X-Edit show that DeContext consistently blocks unwanted image edits while preserving visual quality. These results highlight the effectiveness of attention-based perturbations as a powerful defense against image manipulation.", "AI": {"tldr": "提出DeContext方法，通过注入微小扰动来阻断多模态注意力层的上下文传播，从而保护输入图像免受未经授权的上下文编辑。", "motivation": "上下文扩散模型虽然能便捷地修改图像，但也带来严重的隐私问题：个人图像可能被恶意用于身份冒充、虚假信息等未经授权的编辑。现有方法对大规模DiT模型的保护效果尚未充分验证。", "method": "基于注意力层是上下文信息传播关键路径的洞察，设计针对性扰动来削弱跨注意力机制，重点在早期去噪步骤和特定Transformer块中注入扰动以最大化效果。", "result": "在Flux Kontext和Step1X-Edit上的实验表明，DeContext能有效阻止非授权图像编辑，同时保持视觉质量，验证了注意力扰动防御的有效性。", "conclusion": "注意力机制扰动是一种高效且鲁棒的防御策略，能有效阻断输入与输出间的关联，为图像防篡改提供了新思路。"}}
{"id": "2512.16635", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16635", "abs": "https://arxiv.org/abs/2512.16635", "authors": ["Danxu Liu", "Di Wang", "Hebaixu Wang", "Haoyang Chen", "Wentao Jiang", "Yilin Cheng", "Haonan Guo", "Wei Cui", "Jing Zhang"], "title": "SARMAE: Masked Autoencoder for SAR Representation Learning", "comment": "Code and models will be available at https://github.com/MiliLab/SARMAE", "summary": "Synthetic Aperture Radar (SAR) imagery plays a critical role in all-weather, day-and-night remote sensing applications. However, existing SAR-oriented deep learning is constrained by data scarcity, while the physically grounded speckle noise in SAR imagery further hampers fine-grained semantic representation learning. To address these challenges, we propose SARMAE, a Noise-Aware Masked Autoencoder for self-supervised SAR representation learning. Specifically, we construct SAR-1M, the first million-scale SAR dataset, with additional paired optical images, to enable large-scale pre-training. Building upon this, we design Speckle-Aware Representation Enhancement (SARE), which injects SAR-specific speckle noise into masked autoencoders to facilitate noise-aware and robust representation learning. Furthermore, we introduce Semantic Anchor Representation Constraint (SARC), which leverages paired optical priors to align SAR features and ensure semantic consistency. Extensive experiments across multiple SAR datasets demonstrate that SARMAE achieves state-of-the-art performance on classification, detection, and segmentation tasks. Code and models will be available at https://github.com/MiliLab/SARMAE.", "AI": {"tldr": "提出SARMAE，一种噪声感知掩码自编码器，用于合成孔径雷达（SAR）图像的自监督表示学习，通过构建大规模SAR-1M数据集和引入噪声增强与语义约束，提升SAR任务的性能。", "motivation": "现有SAR深度学习受限于数据稀缺性和固有的斑点噪声，阻碍了细粒度语义表示学习，需要一种能处理噪声并利用大规模数据的自监督方法。", "method": "构建SAR-1M百万级数据集，设计斑点感知表示增强（SARE）注入噪声到掩码自编码器，并引入语义锚点表示约束（SARC）利用配对光学图像对齐SAR特征。", "result": "在多个SAR数据集上的实验表明，SARMAE在分类、检测和分割任务上达到了最先进的性能。", "conclusion": "SARMAE通过噪声感知和语义对齐，有效解决了SAR数据稀缺和噪声问题，为SAR表示学习提供了高效的自监督框架。"}}
{"id": "2512.16853", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16853", "abs": "https://arxiv.org/abs/2512.16853", "authors": ["Amita Kamath", "Kai-Wei Chang", "Ranjay Krishna", "Luke Zettlemoyer", "Yushi Hu", "Marjan Ghazvininejad"], "title": "GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation", "comment": null, "summary": "Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.", "AI": {"tldr": "本文指出文本到图像（T2I）模型评估中的基准漂移问题，以GenEval为例展示其随时间推移与人类判断的偏差增大，并提出新基准GenEval 2和改进的评估方法Soft-TIFA以应对此问题。", "motivation": "自动化T2I模型评估面临挑战：评估模型需正确评分，测试提示需对当前T2I模型具有挑战性但对评估模型不难。这些约束可能导致基准随时间漂移，使静态基准评估无法跟上新模型能力。", "method": "1) 分析GenEval基准的漂移问题；2) 通过大规模人类研究验证基准饱和；3) 提出新基准GenEval 2，改进原始视觉概念覆盖和组合性；4) 提出Soft-TIFA评估方法，结合视觉原始概念判断。", "result": "1) GenEval基准已显著漂移，与人类判断的绝对误差高达17.7%；2) GenEval 2对当前模型更具挑战性；3) Soft-TIFA与人类判断更一致，且比整体性评估方法（如VQAScore）更不易漂移。", "conclusion": "基准漂移是T2I评估中的重要问题，需要持续审计和改进。GenEval 2和Soft-TIFA提供了更可靠的评估方案，但避免漂移仍需长期努力。"}}
{"id": "2512.16874", "categories": ["cs.CV", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16874", "abs": "https://arxiv.org/abs/2512.16874", "authors": ["Tomáš Souček", "Pierre Fernandez", "Hady Elsahar", "Sylvestre-Alvise Rebuffi", "Valeriu Lacatusu", "Tuan Tran", "Tom Sander", "Alexandre Mourachko"], "title": "Pixel Seal: Adversarial-only training for invisible image and video watermarking", "comment": "Code and model available at https://github.com/facebookresearch/videoseal", "summary": "Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.", "AI": {"tldr": "本文提出Pixel Seal，一种新的图像和视频水印方法，通过对抗训练、三阶段训练计划和分辨率适应技术，在鲁棒性和不可感知性方面超越现有方法。", "motivation": "现有水印方法存在三个根本问题：依赖无法准确模拟人类感知的代理损失函数导致可见伪影；优化不稳定需要大量超参数调优；在高分辨率图像和视频中水印的鲁棒性和不可感知性下降。", "method": "1) 提出仅对抗训练范式，消除不可靠的像素级不可感知性损失；2) 引入三阶段训练计划，通过解耦鲁棒性和不可感知性稳定收敛；3) 通过基于JND的衰减和训练时推理模拟实现高分辨率适应，消除上采样伪影。", "result": "Pixel Seal在不同图像类型和多种变换下表现出卓越的鲁棒性和不可感知性，明显优于现有技术，并能通过时间水印池化高效适应视频应用。", "conclusion": "Pixel Seal为现实世界图像和视频场景提供了实用且可扩展的可靠溯源解决方案，在不可感知水印领域确立了新的技术标杆。"}}
{"id": "2512.16670", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.16670", "abs": "https://arxiv.org/abs/2512.16670", "authors": ["Ole Beisswenger", "Jan-Niklas Dihlmann", "Hendrik P. A. Lensch"], "title": "FrameDiffuser: G-Buffer-Conditioned Diffusion for Neural Forward Frame Rendering", "comment": "Project Page: https://framediffuser.jdihlmann.com/", "summary": "Neural rendering for interactive applications requires translating geometric and material properties (G-buffer) to photorealistic images with realistic lighting on a frame-by-frame basis. While recent diffusion-based approaches show promise for G-buffer-conditioned image synthesis, they face critical limitations: single-image models like RGBX generate frames independently without temporal consistency, while video models like DiffusionRenderer are too computationally expensive for most consumer gaming sets ups and require complete sequences upfront, making them unsuitable for interactive applications where future frames depend on user input. We introduce FrameDiffuser, an autoregressive neural rendering framework that generates temporally consistent, photorealistic frames by conditioning on G-buffer data and the models own previous output. After an initial frame, FrameDiffuser operates purely on incoming G-buffer data, comprising geometry, materials, and surface properties, while using its previously generated frame for temporal guidance, maintaining stable, temporal consistent generation over hundreds to thousands of frames. Our dual-conditioning architecture combines ControlNet for structural guidance with ControlLoRA for temporal coherence. A three-stage training strategy enables stable autoregressive generation. We specialize our model to individual environments, prioritizing consistency and inference speed over broad generalization, demonstrating that environment-specific training achieves superior photorealistic quality with accurate lighting, shadows, and reflections compared to generalized approaches.", "AI": {"tldr": "提出了FrameDiffuser，一种用于交互式应用的自回归神经渲染框架，通过结合G-buffer数据和先前生成的帧来生成时间一致且逼真的图像。", "motivation": "现有基于扩散的G-buffer条件图像合成方法存在局限性：单图像模型缺乏时间一致性，而视频模型计算成本高且需要完整序列，不适合依赖用户输入的交互式应用。", "method": "采用自回归框架，结合ControlNet进行结构引导和ControlLoRA确保时间一致性，使用三阶段训练策略实现稳定的自回归生成，并针对特定环境进行专门化训练。", "result": "FrameDiffuser能够在数百至数千帧中保持稳定的时间一致性生成，在逼真度、光照、阴影和反射方面优于通用方法。", "conclusion": "通过环境专门化训练和双条件架构，FrameDiffuser为交互式应用提供了高效且高质量的神经渲染解决方案，平衡了一致性和推理速度。"}}
{"id": "2512.16636", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16636", "abs": "https://arxiv.org/abs/2512.16636", "authors": ["Giorgos Petsangourakis", "Christos Sgouropoulos", "Bill Psomas", "Theodoros Giannakopoulos", "Giorgos Sfikas", "Ioannis Kakogeorgiou"], "title": "REGLUE Your Latents with Global and Local Semantics for Entangled Diffusion", "comment": null, "summary": "Latent diffusion models (LDMs) achieve state-of-the-art image synthesis, yet their reconstruction-style denoising objective provides only indirect semantic supervision: high-level semantics emerge slowly, requiring longer training and limiting sample quality. Recent works inject semantics from Vision Foundation Models (VFMs) either externally via representation alignment or internally by jointly modeling only a narrow slice of VFM features inside the diffusion process, under-utilizing the rich, nonlinear, multi-layer spatial semantics available. We introduce REGLUE (Representation Entanglement with Global-Local Unified Encoding), a unified latent diffusion framework that jointly models (i) VAE image latents, (ii) compact local (patch-level) VFM semantics, and (iii) a global (image-level) [CLS] token within a single SiT backbone. A lightweight convolutional semantic compressor nonlinearly aggregates multi-layer VFM features into a low-dimensional, spatially structured representation, which is entangled with the VAE latents in the diffusion process. An external alignment loss further regularizes internal representations toward frozen VFM targets. On ImageNet 256x256, REGLUE consistently improves FID and accelerates convergence over SiT-B/2 and SiT-XL/2 baselines, as well as over REPA, ReDi, and REG. Extensive experiments show that (a) spatial VFM semantics are crucial, (b) non-linear compression is key to unlocking their full benefit, and (c) global tokens and external alignment act as complementary, lightweight enhancements within our global-local-latent joint modeling framework. The code is available at https://github.com/giorgospets/reglue .", "AI": {"tldr": "提出REGLUE框架，通过联合建模VAE潜在表示、局部VFM语义和全局CLS标记，改进潜在扩散模型的语义监督，提升图像生成质量和训练效率。", "motivation": "现有潜在扩散模型（LDMs）的重构式去噪目标仅提供间接语义监督，导致高级语义学习缓慢、训练时间长且样本质量受限。现有方法未能充分利用视觉基础模型（VFMs）丰富的多层空间语义信息。", "method": "提出REGLUE框架：1）使用轻量卷积语义压缩器非线性聚合多层VFM特征为低维空间结构化表示；2）在单个SiT主干中联合建模VAE潜在、局部VFM语义和全局CLS标记；3）通过外部对齐损失正则化内部表示。", "result": "在ImageNet 256×256上，REGLUE相比SiT-B/2和SiT-XL/2基线以及REPA、ReDi、REG方法，持续改善FID指标并加速收敛。实验表明空间VFM语义、非线性压缩、全局标记和外部对齐均发挥关键作用。", "conclusion": "REGLUE通过全局-局部-潜在联合建模框架有效利用VFM语义，为扩散模型提供更直接的语义监督，显著提升生成性能。代码已开源。"}}
{"id": "2512.16891", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.16891", "abs": "https://arxiv.org/abs/2512.16891", "authors": ["Haichao Zhang", "Yao Lu", "Lichen Wang", "Yunzhe Li", "Daiwei Chen", "Yunpeng Xu", "Yun Fu"], "title": "LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation", "comment": null, "summary": "Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.", "AI": {"tldr": "LinkedOut是一种从视频大语言模型中提取世界知识表示的方法，用于视频推荐任务，支持多视频输入、低延迟推理，并在标准基准测试中达到最先进水平。", "motivation": "现有视频大语言模型在部署到下游任务（如视频推荐）时面临挑战：解码生成延迟高、不支持多视频输入、语言输出丢弃了视觉细节。这些限制源于缺乏既能保留像素级细节又能利用世界知识的表示方法。", "method": "提出LinkedOut表示方法：1）使用VLLM从原始视频帧中提取语义接地、知识感知的token；2）引入跨层知识融合混合专家模型，从丰富的VLLM特征中选择合适的抽象层级；3）支持可提示查询和可选辅助模态。", "result": "LinkedOut在标准视频推荐基准测试中达到最先进水平，无需手工标注标签。可解释性研究和消融实验证实了层级多样性和逐层融合的优势。", "conclusion": "LinkedOut为充分利用VLLM的世界知识先验和视觉推理能力提供了一条实用路径，特别适用于推荐等下游视觉任务，实现了快速推理、多视频历史支持和消除语言瓶颈。"}}
{"id": "2512.16921", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16921", "abs": "https://arxiv.org/abs/2512.16921", "authors": ["Qihao Liu", "Chengzhi Mao", "Yaojie Liu", "Alan Yuille", "Wen-Sheng Chu"], "title": "Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification", "comment": "project page: https://auditdm.github.io/", "summary": "Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.", "AI": {"tldr": "提出了AuditDM框架，通过强化学习训练MLLM作为审计器，自动发现并修复多模态大语言模型的失败模式，提升模型性能。", "motivation": "传统多模态大语言模型评估方法缺乏可解释性，且难以充分揭示模型间的能力差距，需要更有效的模型诊断和改进方法。", "method": "使用强化学习微调MLLM作为审计器，生成具有挑战性的问题和反事实图像以最大化目标模型间的分歧，从而发现模型弱点。", "result": "在Gemma-3和PaliGemma-2等先进模型上发现20多种失败类型；基于这些发现微调后，所有模型在16个基准测试中性能均提升，3B模型甚至超越28B模型。", "conclusion": "随着数据扩展收益递减，针对性的模型审计为模型诊断和改进提供了有效途径。"}}
{"id": "2512.16920", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.16920", "abs": "https://arxiv.org/abs/2512.16920", "authors": ["Jinjie Mai", "Chaoyang Wang", "Guocheng Gordon Qian", "Willi Menapace", "Sergey Tulyakov", "Bernard Ghanem", "Peter Wonka", "Ashkan Mirzaei"], "title": "EasyV2V: A High-quality Instruction-based Video Editing Framework", "comment": "Project page: https://snap-research.github.io/easyv2v/", "summary": "While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \\emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/", "AI": {"tldr": "EasyV2V是一个基于指令的视频编辑框架，通过数据合成、架构简化和统一控制机制，实现了高质量的视频编辑效果。", "motivation": "图像编辑技术发展迅速，但视频编辑仍面临一致性、控制和泛化性等挑战，需要更有效的解决方案。", "method": "1) 数据方面：组合现有专家模型构建视频对，通过单帧监督和伪视频对提升数据质量，添加过渡监督；2) 模型方面：利用预训练文本到视频模型的编辑能力，采用简单序列拼接和轻量LoRA微调；3) 控制方面：通过单一掩码机制统一时空控制，支持参考图像输入。", "result": "EasyV2V在多种输入条件下（视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本）均实现了最先进的视频编辑效果，优于同期和商业系统。", "conclusion": "该框架通过系统性的数据、架构和控制设计，为指令驱动的视频编辑提供了一个简单而强大的解决方案，显著提升了视频编辑的质量和灵活性。"}}
{"id": "2512.16688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16688", "abs": "https://arxiv.org/abs/2512.16688", "authors": ["Serafino Pandolfini", "Lorenzo Pellegrini", "Matteo Ferrara", "Davide Maltoni"], "title": "Detecting Localized Deepfakes: How Well Do Synthetic Image Detectors Handle Inpainting?", "comment": "17 pages, 5 figures, 9 tables", "summary": "The rapid progress of generative AI has enabled highly realistic image manipulations, including inpainting and region-level editing. These approaches preserve most of the original visual context and are increasingly exploited in cybersecurity-relevant threat scenarios. While numerous detectors have been proposed for identifying fully synthetic images, their ability to generalize to localized manipulations remains insufficiently characterized. This work presents a systematic evaluation of state-of-the-art detectors, originally trained for the deepfake detection on fully synthetic images, when applied to a distinct challenge: localized inpainting detection. The study leverages multiple datasets spanning diverse generators, mask sizes, and inpainting techniques. Our experiments show that models trained on a large set of generators exhibit partial transferability to inpainting-based edits and can reliably detect medium- and large-area manipulations or regeneration-style inpainting, outperforming many existing ad hoc detection approaches.", "AI": {"tldr": "本文系统评估了现有深度伪造检测器在局部修复检测任务上的泛化能力，发现经过多生成器训练的模型对中大面积修复具有部分可迁移性。", "motivation": "生成式AI的快速发展使得局部图像修复技术日趋逼真，这些技术被越来越多地用于网络安全威胁场景。然而，现有针对全合成图像的检测器在局部篡改检测上的泛化能力尚未得到充分研究。", "method": "采用系统性评估方法，将原本用于全合成图像深度伪造检测的先进检测器应用于局部修复检测任务，使用包含多种生成器、掩码尺寸和修复技术的多数据集进行实验验证。", "result": "实验表明，在多生成器上训练的模型对修复式编辑具有部分可迁移性，能够可靠检测中大面积篡改或再生式修复，其性能优于许多现有的专门检测方法。", "conclusion": "针对全合成图像训练的检测器在局部修复检测任务上表现出有限的泛化能力，特别是在中大面积篡改检测方面具有实用价值，这为开发更通用的图像篡改检测系统提供了参考。"}}
{"id": "2512.16706", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.16706", "abs": "https://arxiv.org/abs/2512.16706", "authors": ["Antonella Rech", "Nicola Conci", "Nicola Garau"], "title": "SDFoam: Signed-Distance Foam for explicit surface reconstruction", "comment": null, "summary": "Neural radiance fields (NeRF) have driven impressive progress in view synthesis by using ray-traced volumetric rendering. Splatting-based methods such as 3D Gaussian Splatting (3DGS) provide faster rendering by rasterizing 3D primitives. RadiantFoam (RF) brought ray tracing back, achieving throughput comparable to Gaussian Splatting by organizing radiance with an explicit Voronoi Diagram (VD). Yet, all the mentioned methods still struggle with precise mesh reconstruction. We address this gap by jointly learning an explicit VD with an implicit Signed Distance Field (SDF). The scene is optimized via ray tracing and regularized by an Eikonal objective. The SDF introduces metric-consistent isosurfaces, which, in turn, bias near-surface Voronoi cell faces to align with the zero level set. The resulting model produces crisper, view-consistent surfaces with fewer floaters and improved topology, while preserving photometric quality and maintaining training speed on par with RadiantFoam. Across diverse scenes, our hybrid implicit-explicit formulation, which we name SDFoam, substantially improves mesh reconstruction accuracy (Chamfer distance) with comparable appearance (PSNR, SSIM), without sacrificing efficiency.", "AI": {"tldr": "提出SDFoam方法，通过联合学习显式Voronoi图与隐式符号距离场，改进神经辐射场和3D高斯泼溅等方法在精确网格重建上的不足，实现更清晰、视图一致的表面重建，同时保持渲染效率。", "motivation": "现有神经辐射场（NeRF）、3D高斯泼溅（3DGS）和RadiantFoam等方法在视图合成和快速渲染方面取得进展，但在精确网格重建方面仍存在困难，如表面模糊、漂浮物和拓扑问题。", "method": "联合优化显式Voronoi图和隐式符号距离场（SDF），通过光线追踪进行场景优化，并采用Eikonal正则化。SDF引入度量一致等值面，使近表面Voronoi单元面对齐零水平集。", "result": "SDFoam在多种场景中显著提升网格重建精度（Chamfer距离降低），同时保持可比的外观质量（PSNR、SSIM），且训练速度与RadiantFoam相当，减少了漂浮物并改善了拓扑结构。", "conclusion": "SDFoam通过隐式-显式混合表示，有效解决了现有方法在网格重建中的局限性，实现了更精确的表面重建和视图一致性，而不牺牲渲染效率或外观质量。"}}
{"id": "2512.16710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16710", "abs": "https://arxiv.org/abs/2512.16710", "authors": ["Chiara Di Vece", "Zhehua Mao", "Netanell Avisdris", "Brian Dromey", "Raffaele Napolitano", "Dafna Ben Bashat", "Francisco Vasconcelos", "Danail Stoyanov", "Leo Joskowicz", "Sophia Bano"], "title": "A multi-centre, multi-device benchmark dataset for landmark-based comprehensive fetal biometry", "comment": "11 pages, 5 figures, 3 tables", "summary": "Accurate fetal growth assessment from ultrasound (US) relies on precise biometry measured by manually identifying anatomical landmarks in standard planes. Manual landmarking is time-consuming, operator-dependent, and sensitive to variability across scanners and sites, limiting the reproducibility of automated approaches. There is a need for multi-source annotated datasets to develop artificial intelligence-assisted fetal growth assessment methods. To address this bottleneck, we present an open, multi-centre, multi-device benchmark dataset of fetal US images with expert anatomical landmark annotations for clinically used fetal biometric measurements. These measurements include head bi-parietal and occipito-frontal diameters, abdominal transverse and antero-posterior diameters, and femoral length. The dataset contains 4,513 de-identified US images from 1,904 subjects acquired at three clinical sites using seven different US devices. We provide standardised, subject-disjoint train/test splits, evaluation code, and baseline results to enable fair and reproducible comparison of methods. Using an automatic biometry model, we quantify domain shift and demonstrate that training and evaluation confined to a single centre substantially overestimate performance relative to multi-centre testing. To the best of our knowledge, this is the first publicly available multi-centre, multi-device, landmark-annotated dataset that covers all primary fetal biometry measures, providing a robust benchmark for domain adaptation and multi-centre generalisation in fetal biometry and enabling more reliable AI-assisted fetal growth assessment across centres. All data, annotations, training code, and evaluation pipelines are made publicly available.", "AI": {"tldr": "提出了首个公开的多中心、多设备胎儿超声图像基准数据集，包含专家标注的解剖标志点，用于开发可靠的AI辅助胎儿生长评估方法。", "motivation": "胎儿超声生长评估依赖手动标注解剖标志点，但该方法耗时、操作者依赖性强，且受设备和中心差异影响，限制了自动化方法的可重复性。需要多源标注数据集来推动AI辅助方法的发展。", "method": "收集了来自3个临床中心、7种不同超声设备的4,513张去标识化胎儿超声图像，涵盖1,904名受试者。提供了标准化的训练/测试划分、评估代码和基线结果，并利用自动生物测量模型量化了领域偏移。", "result": "通过自动生物测量模型验证，发现仅基于单一中心的训练和评估会显著高估模型性能，而多中心测试能更真实反映模型泛化能力。", "conclusion": "该数据集为胎儿生物测量领域的领域适应和多中心泛化研究提供了可靠基准，有助于开发跨中心更可靠的AI辅助胎儿生长评估工具。所有数据、标注和代码均已公开。"}}
{"id": "2512.16727", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.16727", "abs": "https://arxiv.org/abs/2512.16727", "authors": ["Haochen Chang", "Pengfei Ren", "Buyuan Zhang", "Da Li", "Tianhao Han", "Haoyang Zhang", "Liang Xie", "Hongbo Chen", "Erwei Yin"], "title": "OMG-Bench: A New Challenging Benchmark for Skeleton-based Online Micro Hand Gesture Recognition", "comment": "Project page: https://omg-bench.github.io/", "summary": "Online micro gesture recognition from hand skeletons is critical for VR/AR interaction but faces challenges due to limited public datasets and task-specific algorithms. Micro gestures involve subtle motion patterns, which make constructing datasets with precise skeletons and frame-level annotations difficult. To this end, we develop a multi-view self-supervised pipeline to automatically generate skeleton data, complemented by heuristic rules and expert refinement for semi-automatic annotation. Based on this pipeline, we introduce OMG-Bench, the first large-scale public benchmark for skeleton-based online micro gesture recognition. It features 40 fine-grained gesture classes with 13,948 instances across 1,272 sequences, characterized by subtle motions, rapid dynamics, and continuous execution. To tackle these challenges, we propose Hierarchical Memory-Augmented Transformer (HMATr), an end-to-end framework that unifies gesture detection and classification by leveraging hierarchical memory banks which store frame-level details and window-level semantics to preserve historical context. In addition, it employs learnable position-aware queries initialized from the memory to implicitly encode gesture positions and semantics. Experiments show that HMATr outperforms state-of-the-art methods by 7.6\\% in detection rate, establishing a strong baseline for online micro gesture recognition. Project page: https://omg-bench.github.io/", "AI": {"tldr": "本文提出了OMG-Bench，首个大规模公开的基于骨架的在线微手势识别基准，并设计了Hierarchical Memory-Augmented Transformer (HMATr) 端到端框架，在检测率上优于现有方法7.6%。", "motivation": "在线微手势识别对VR/AR交互至关重要，但面临公开数据集稀缺、任务特定算法有限以及微手势动作细微导致数据标注困难等挑战。", "method": "开发了多视角自监督流程自动生成骨架数据，结合启发式规则和专家精修实现半自动标注；提出HMATr框架，利用分层记忆库存储帧级细节和窗口级语义以保留历史上下文，并通过可学习的位置感知查询隐式编码手势位置和语义。", "result": "构建的OMG-Bench包含40个细粒度手势类别、13,948个实例和1,272个序列，具有动作细微、动态快速和连续执行的特点；HMATr在检测率上超越现有最优方法7.6%。", "conclusion": "OMG-Bench为在线微手势识别提供了首个大规模公开基准，HMATr框架通过统一手势检测与分类、利用分层记忆机制，为该任务建立了强基线。"}}
{"id": "2512.16740", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16740", "abs": "https://arxiv.org/abs/2512.16740", "authors": ["Yunkai Yang", "Yudong Zhang", "Kunquan Zhang", "Jinxiao Zhang", "Xinying Chen", "Haohuan Fu", "Runmin Dong"], "title": "Task-Oriented Data Synthesis and Control-Rectify Sampling for Remote Sensing Semantic Segmentation", "comment": null, "summary": "With the rapid progress of controllable generation, training data synthesis has become a promising way to expand labeled datasets and alleviate manual annotation in remote sensing (RS). However, the complexity of semantic mask control and the uncertainty of sampling quality often limit the utility of synthetic data in downstream semantic segmentation tasks. To address these challenges, we propose a task-oriented data synthesis framework (TODSynth), including a Multimodal Diffusion Transformer (MM-DiT) with unified triple attention and a plug-and-play sampling strategy guided by task feedback. Built upon the powerful DiT-based generative foundation model, we systematically evaluate different control schemes, showing that a text-image-mask joint attention scheme combined with full fine-tuning of the image and mask branches significantly enhances the effectiveness of RS semantic segmentation data synthesis, particularly in few-shot and complex-scene scenarios. Furthermore, we propose a control-rectify flow matching (CRFM) method, which dynamically adjusts sampling directions guided by semantic loss during the early high-plasticity stage, mitigating the instability of generated images and bridging the gap between synthetic data and downstream segmentation tasks. Extensive experiments demonstrate that our approach consistently outperforms state-of-the-art controllable generation methods, producing more stable and task-oriented synthetic data for RS semantic segmentation.", "AI": {"tldr": "提出TODSynth框架，通过多模态扩散变换器和任务反馈引导的采样策略，生成面向遥感语义分割任务的合成数据，提升少样本和复杂场景下的性能。", "motivation": "遥感数据标注成本高，现有可控生成方法存在语义掩码控制复杂和采样质量不稳定的问题，限制了合成数据在下游分割任务中的实用性。", "method": "1. 基于DiT构建多模态扩散变换器（MM-DiT），采用文本-图像-掩码联合注意力机制；2. 提出控制-校正流匹配（CRFM）方法，在采样早期通过语义损失动态调整方向；3. 结合任务反馈的即插即用采样策略。", "result": "实验表明，该方法在少样本和复杂场景下显著提升遥感语义分割数据合成的有效性，生成更稳定、任务导向的合成数据，优于现有可控生成方法。", "conclusion": "TODSynth框架通过联合注意力机制和动态采样校正，有效解决了合成数据与下游任务间的鸿沟，为遥感数据扩展提供了高效解决方案。"}}
{"id": "2512.16767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16767", "abs": "https://arxiv.org/abs/2512.16767", "authors": ["Zhiyang Guo", "Ori Zhang", "Jax Xiang", "Alan Zhao", "Wengang Zhou", "Houqiang Li"], "title": "Make-It-Poseable: Feed-forward Latent Posing Model for 3D Humanoid Character Animation", "comment": "Project page: https://jasongzy.github.io/Make-It-Poseable/", "summary": "Posing 3D characters is a fundamental task in computer graphics and vision. However, existing methods like auto-rigging and pose-conditioned generation often struggle with challenges such as inaccurate skinning weight prediction, topological imperfections, and poor pose conformance, limiting their robustness and generalizability. To overcome these limitations, we introduce Make-It-Poseable, a novel feed-forward framework that reformulates character posing as a latent-space transformation problem. Instead of deforming mesh vertices as in traditional pipelines, our method reconstructs the character in new poses by directly manipulating its latent representation. At the core of our method is a latent posing transformer that manipulates shape tokens based on skeletal motion. This process is facilitated by a dense pose representation for precise control. To ensure high-fidelity geometry and accommodate topological changes, we also introduce a latent-space supervision strategy and an adaptive completion module. Our method demonstrates superior performance in posing quality. It also naturally extends to 3D editing applications like part replacement and refinement.", "AI": {"tldr": "提出Make-It-Poseable框架，通过潜在空间变换实现3D角色姿态生成，避免传统网格变形的问题，支持高保真几何和拓扑变化。", "motivation": "现有3D角色姿态生成方法（如自动绑定、姿态条件生成）存在蒙皮权重预测不准确、拓扑缺陷和姿态一致性差等问题，限制了鲁棒性和泛化能力。", "method": "采用前馈框架，将角色姿态生成重构为潜在空间变换问题；使用潜在姿态变换器根据骨骼运动操作形状令牌；引入密集姿态表示进行精确控制；采用潜在空间监督策略和自适应补全模块确保几何保真度和拓扑适应性。", "result": "方法在姿态生成质量上表现优越，并能自然扩展到部件替换和细化等3D编辑应用。", "conclusion": "Make-It-Poseable通过潜在空间操作有效解决了传统3D角色姿态生成的局限性，实现了高质量、可泛化的姿态生成和编辑。"}}
{"id": "2512.16771", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16771", "abs": "https://arxiv.org/abs/2512.16771", "authors": ["Enis Baty", "C. P. Bridges", "Simon Hadfield"], "title": "FlowDet: Unifying Object Detection and Generative Transport Flows", "comment": null, "summary": "We present FlowDet, the first formulation of object detection using modern Conditional Flow Matching techniques. This work follows from DiffusionDet, which originally framed detection as a generative denoising problem in the bounding box space via diffusion. We revisit and generalise this formulation to a broader class of generative transport problems, while maintaining the ability to vary the number of boxes and inference steps without re-training. In contrast to the curved stochastic transport paths induced by diffusion, FlowDet learns simpler and straighter paths resulting in faster scaling of detection performance as the number of inference steps grows. We find that this reformulation enables us to outperform diffusion based detection systems (as well as non-generative baselines) across a wide range of experiments, including various precision/recall operating points using multiple feature backbones and datasets. In particular, when evaluating under recall-constrained settings, we can highlight the effects of the generative transport without over-compensating with large numbers of proposals. This provides gains of up to +3.6% AP and +4.2% AP$_{rare}$ over DiffusionDet on the COCO and LVIS datasets, respectively.", "AI": {"tldr": "FlowDet首次将条件流匹配技术应用于目标检测，相比基于扩散的方法学习更简单直接的生成路径，在多个数据集上实现性能提升。", "motivation": "现有基于扩散的目标检测方法（如DiffusionDet）存在生成路径弯曲且随机的问题，导致推理效率受限。研究者希望探索更广泛的生成传输问题，寻找更优的检测框架。", "method": "将目标检测重新构建为条件流匹配问题，学习从噪声到边界框的直线生成路径，支持可变数量的边界框和推理步骤而无需重新训练。", "result": "在COCO和LVIS数据集上分别取得比DiffusionDet高+3.6% AP和+4.2% AP_rare的性能提升，在召回约束设置下优势更明显。", "conclusion": "流匹配为生成式目标检测提供了比扩散模型更有效的框架，其直线生成路径能更快提升检测性能，且适用于多种特征主干和数据集。"}}
{"id": "2512.16784", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16784", "abs": "https://arxiv.org/abs/2512.16784", "authors": ["Simone Teglia", "Claudia Melis Tonti", "Francesco Pro", "Leonardo Russo", "Andrea Alfarano", "Leonardo Pentassuglia", "Irene Amerini"], "title": "R3ST: A Synthetic 3D Dataset With Realistic Trajectories", "comment": null, "summary": "Datasets are essential to train and evaluate computer vision models used for traffic analysis and to enhance road safety. Existing real datasets fit real-world scenarios, capturing authentic road object behaviors, however, they typically lack precise ground-truth annotations. In contrast, synthetic datasets play a crucial role, allowing for the annotation of a large number of frames without additional costs or extra time. However, a general drawback of synthetic datasets is the lack of realistic vehicle motion, since trajectories are generated using AI models or rule-based systems. In this work, we introduce R3ST (Realistic 3D Synthetic Trajectories), a synthetic dataset that overcomes this limitation by generating a synthetic 3D environment and integrating real-world trajectories derived from SinD, a bird's-eye-view dataset recorded from drone footage. The proposed dataset closes the gap between synthetic data and realistic trajectories, advancing the research in trajectory forecasting of road vehicles, offering both accurate multimodal ground-truth annotations and authentic human-driven vehicle trajectories.", "AI": {"tldr": "提出R3ST合成数据集，通过整合真实世界轨迹到合成3D环境中，解决了现有合成数据集缺乏真实车辆运动的问题。", "motivation": "现有真实数据集缺乏精确标注，而合成数据集虽可低成本标注但车辆运动不真实，需要兼具真实轨迹和精确标注的数据集来推进轨迹预测研究。", "method": "创建合成3D环境，并从无人机拍摄的鸟瞰数据集SinD中提取真实世界轨迹，将其整合到合成环境中生成R3ST数据集。", "result": "R3ST数据集填补了合成数据与真实轨迹之间的鸿沟，同时提供精确的多模态标注和真实的人类驾驶车辆轨迹。", "conclusion": "该数据集通过结合合成环境的标注优势和真实轨迹的真实性，为道路车辆轨迹预测研究提供了重要资源。"}}
{"id": "2512.16776", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16776", "abs": "https://arxiv.org/abs/2512.16776", "authors": ["Kling Team", "Jialu Chen", "Yuanzheng Ci", "Xiangyu Du", "Zipeng Feng", "Kun Gai", "Sainan Guo", "Feng Han", "Jingbin He", "Kang He", "Xiao Hu", "Xiaohua Hu", "Boyuan Jiang", "Fangyuan Kong", "Hang Li", "Jie Li", "Qingyu Li", "Shen Li", "Xiaohan Li", "Yan Li", "Jiajun Liang", "Borui Liao", "Yiqiao Liao", "Weihong Lin", "Quande Liu", "Xiaokun Liu", "Yilun Liu", "Yuliang Liu", "Shun Lu", "Hangyu Mao", "Yunyao Mao", "Haodong Ouyang", "Wenyu Qin", "Wanqi Shi", "Xiaoyu Shi", "Lianghao Su", "Haozhi Sun", "Peiqin Sun", "Pengfei Wan", "Chao Wang", "Chenyu Wang", "Meng Wang", "Qiulin Wang", "Runqi Wang", "Xintao Wang", "Xuebo Wang", "Zekun Wang", "Min Wei", "Tiancheng Wen", "Guohao Wu", "Xiaoshi Wu", "Zhenhua Wu", "Da Xie", "Yingtong Xiong", "Yulong Xu", "Sile Yang", "Zikang Yang", "Weicai Ye", "Ziyang Yuan", "Shenglong Zhang", "Shuaiyu Zhang", "Yuanxing Zhang", "Yufan Zhang", "Wenzheng Zhao", "Ruiliang Zhou", "Yan Zhou", "Guosheng Zhu", "Yongjie Zhu"], "title": "Kling-Omni Technical Report", "comment": "Kling-Omni Technical Report", "summary": "We present Kling-Omni, a generalist generative framework designed to synthesize high-fidelity videos directly from multimodal visual language inputs. Adopting an end-to-end perspective, Kling-Omni bridges the functional separation among diverse video generation, editing, and intelligent reasoning tasks, integrating them into a holistic system. Unlike disjointed pipeline approaches, Kling-Omni supports a diverse range of user inputs, including text instructions, reference images, and video contexts, processing them into a unified multimodal representation to deliver cinematic-quality and highly-intelligent video content creation. To support these capabilities, we constructed a comprehensive data system that serves as the foundation for multimodal video creation. The framework is further empowered by efficient large-scale pre-training strategies and infrastructure optimizations for inference. Comprehensive evaluations reveal that Kling-Omni demonstrates exceptional capabilities in in-context generation, reasoning-based editing, and multimodal instruction following. Moving beyond a content creation tool, we believe Kling-Omni is a pivotal advancement toward multimodal world simulators capable of perceiving, reasoning, generating and interacting with the dynamic and complex worlds.", "AI": {"tldr": "Kling-Omni是一个通用的生成框架，能够直接从多模态视觉语言输入合成高保真视频，将视频生成、编辑和智能推理任务整合到一个端到端系统中。", "motivation": "现有视频生成方法通常采用分离的流水线，无法有效整合多样化的用户输入（如文本指令、参考图像、视频上下文）和多种任务（生成、编辑、推理）。研究旨在开发一个统一框架，支持多模态输入并实现电影级质量的智能视频内容创作。", "method": "采用端到端框架，将多模态输入处理为统一表示；构建了支持多模态视频创作的全面数据系统；实施了高效的大规模预训练策略和推理基础设施优化。", "result": "综合评估显示，Kling-Omni在上下文生成、基于推理的编辑和多模态指令跟随方面表现出卓越能力，能够生成高质量、智能的视频内容。", "conclusion": "Kling-Omni不仅是内容创作工具，更是向能够感知、推理、生成并与动态复杂世界交互的多模态世界模拟器迈出的关键一步。"}}
{"id": "2512.16818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16818", "abs": "https://arxiv.org/abs/2512.16818", "authors": ["Marius Dähling", "Sebastian Krebs", "J. Marius Zöllner"], "title": "DenseBEV: Transforming BEV Grid Cells into 3D Objects", "comment": "15 pages, 8 figures, accepted by WACV 2026", "summary": "In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.", "AI": {"tldr": "提出DenseBEV方法，将BEV特征单元直接作为锚点进行多摄像头3D目标检测，通过两阶段锚点生成和混合时序建模提升性能，在nuScenes和Waymo数据集上取得显著改进。", "motivation": "传统BEV-based transformer模型使用随机查询作为锚点，或依赖辅助网络检测；本文提出更直观高效的方法，直接利用BEV特征单元作为锚点，避免额外计算并更好利用时序信息。", "method": "1. 使用BEV特征单元作为锚点进行端到端检测；2. 提出两阶段锚点生成方法；3. 引入BEV-based非极大值抑制解决大量查询的注意力缩放问题；4. 集成先验检测进行混合时序建模。", "result": "在nuScenes数据集上NDS和mAP持续显著提升，行人检测mAP提高3.8%；在Waymo数据集上LET-mAP达到60.7%，超越之前最佳方法5.4%，对小目标检测效果尤为明显。", "conclusion": "DenseBEV通过直接利用BEV特征作为锚点，结合两阶段生成和混合时序建模，实现了高效准确的多摄像头3D目标检测，在稀疏BEV网格下仍能保持优异性能，为相关领域提供了新思路。"}}
{"id": "2512.16841", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16841", "abs": "https://arxiv.org/abs/2512.16841", "authors": ["Emmanuel D. Muñiz-De-León", "Jorge A. Rosales-de-Golferichs", "Ana S. Muñoz-Rodríguez", "Alejandro I. Trejo-Castro", "Eduardo de Avila-Armenta", "Antonio Martínez-Torteya"], "title": "Radiology Report Generation with Layer-Wise Anatomical Attention", "comment": "11 pages, 6 figures", "summary": "Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.", "AI": {"tldr": "提出了一种紧凑的图像到文本架构，仅使用单张胸部X光正面图像生成报告发现部分，通过解剖注意力机制提升临床相关区域的生成质量，在多项指标上显著超越现有方法。", "motivation": "当前最先进的放射学报告生成系统依赖大规模多模态训练、临床元数据和多视图图像，资源消耗大且难以普及。本研究旨在开发一种仅需单张正面X光图像的轻量级解决方案，降低应用门槛。", "method": "采用冻结的DINOv3 ViT编码器与GPT-2解码器结合，引入分层高斯平滑的解剖注意力机制，集成肺部和心脏分割掩码以偏向临床相关区域，不增加可训练参数。", "result": "在MIMIC-CXR数据集上评估：5种关键病理的CheXpert Macro-F1提升168%（0.083→0.238），Micro-F1提升146%（0.137→0.337）；14项观察指标提升86%（0.170→0.316）；RadGraph F1提升9.7%。", "conclusion": "解码器层的解剖引导能有效改善空间定位和临床相关区域的连贯性，证明轻量级纯图像条件模型在放射报告生成中的潜力。"}}
{"id": "2512.16864", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16864", "abs": "https://arxiv.org/abs/2512.16864", "authors": ["Tianyuan Qu", "Lei Ke", "Xiaohang Zhan", "Longxiang Tang", "Yuqi Liu", "Bohao Peng", "Bei Yu", "Dong Yu", "Jiaya Jia"], "title": "RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing", "comment": "Precise region control and planning for instruction-based image editing. Our project page: https://replan-iv-edit.github.io", "summary": "Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io", "AI": {"tldr": "提出RePlan框架解决指令-视觉复杂性下的图像编辑问题，通过规划-执行架构实现多区域精确编辑，无需迭代修复", "motivation": "现有基于指令的图像编辑模型在处理复杂指令与杂乱/模糊场景结合的IV-Complexity时表现不佳，需要更精确的视觉定位和推理能力", "method": "采用Region-aligned Planning框架：1)视觉语言规划器通过逐步推理分解指令并定位目标区域；2)扩散编辑器使用免训练的注意力区域注入机制进行并行多区域编辑；3)使用GRPO强化学习增强规划能力", "result": "在IV-Edit基准测试中，RePlan在区域精度和整体保真度上显著优于基线模型，即使基线使用更大训练数据集", "conclusion": "RePlan通过显式的区域对齐规划和强化学习训练，有效解决了复杂指令下的图像编辑挑战，为细粒度视觉定位和知识密集型编辑提供了新方案"}}
{"id": "2512.16880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16880", "abs": "https://arxiv.org/abs/2512.16880", "authors": ["Valay Bundele", "Mehran Hosseinzadeh", "Hendrik P. A. Lensch"], "title": "Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation", "comment": "Under Review", "summary": "Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.", "AI": {"tldr": "提出ReMeDI-SAM3方法，通过增强记忆机制改进SAM3在手术内镜视频中的器械分割性能，无需额外训练即可显著提升分割精度。", "motivation": "内镜视频中手术器械分割面临遮挡、快速运动、反光伪影和器械重复进入等挑战，现有SAM3方法在手术场景中因记忆更新不区分、容量固定和遮挡后身份恢复能力弱而性能受限。", "method": "提出三个训练免费组件：1) 相关性感知记忆过滤与专用遮挡感知记忆存储遮挡前帧；2) 分段插值方案扩展有效记忆容量；3) 基于特征的重新识别模块结合时序投票实现遮挡后身份消歧。", "result": "在EndoVis17和EndoVis18数据集零样本设置下，相比原始SAM3分别获得约7%和16%的绝对mcIoU提升，性能优于先前需要训练的方法。", "conclusion": "ReMeDI-SAM3通过增强记忆管理有效减少错误累积并提升遮挡恢复能力，为手术器械分割提供了高效可靠的解决方案。"}}
{"id": "2512.16906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16906", "abs": "https://arxiv.org/abs/2512.16906", "authors": ["Xiaoyan Cong", "Haotian Yang", "Angtian Wang", "Yizhi Wang", "Yiding Yang", "Canyu Zhang", "Chongyang Ma"], "title": "VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization", "comment": null, "summary": "Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io", "AI": {"tldr": "提出VIVA框架，通过VLM引导编码和奖励优化实现基于指令的视频编辑，提升对复杂真实指令的泛化能力。", "motivation": "现有基于扩散模型的视频编辑方法通常使用简单编辑操作的配对数据训练，难以泛化到多样复杂的真实世界指令。", "method": "1. 使用VLM编码器将文本指令、视频首帧和可选参考图像编码为视觉基础指令表示；2. 提出Edit-GRPO后训练阶段，通过相对奖励优化模型；3. 设计合成数据生成管道创建多样化高质量配对数据。", "result": "实验表明VIVA在指令遵循、泛化能力和编辑质量上优于现有方法。", "conclusion": "VIVA框架通过结合VLM引导编码和奖励优化，有效解决了指令视频编辑的泛化问题，实现了高质量的编辑效果。"}}
{"id": "2512.16893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16893", "abs": "https://arxiv.org/abs/2512.16893", "authors": ["Kaiwen Jiang", "Xueting Li", "Seonwook Park", "Ravi Ramamoorthi", "Shalini De Mello", "Koki Nagano"], "title": "Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation", "comment": "Project website is https://research.nvidia.com/labs/amri/projects/instant4d", "summary": "Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face's 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d", "AI": {"tldr": "提出Instant4D方法，通过知识蒸馏将2D扩散模型的表达能力与3D表示的速度和一致性结合，实现单张图片到可动画3D人像的快速转换。", "motivation": "现有2D人像动画方法缺乏3D一致性和实时性，而3D方法又牺牲了表情细节。需要结合两者优势，满足数字孪生、远程呈现等实际应用需求。", "method": "1) 从2D扩散模型蒸馏知识到前馈编码器；2) 设计解耦的动画表示，从数据隐式学习运动；3) 采用轻量级局部融合策略替代全局注意力机制。", "result": "达到107.31 FPS的动画/姿态控制速度，动画质量与最先进方法相当，在速度与质量间取得更好平衡。", "conclusion": "Instant4D实现了快速、3D一致且富有表现力的人像动画，摆脱了对参数化模型的依赖，为实际应用提供了可行方案。"}}
{"id": "2512.16885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16885", "abs": "https://arxiv.org/abs/2512.16885", "authors": ["Norika Wada", "Kohei Yamashita", "Ryo Kawahara", "Ko Nishino"], "title": "M-PhyGs: Multi-Material Object Dynamics from Video", "comment": null, "summary": "Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.", "AI": {"tldr": "提出M-PhyGs方法，从视频中估计多材料复杂自然物体（以花朵为例）的材料组成和物理参数，解决了现有方法对单一材料、预学习动力学或简单拓扑的假设限制。", "motivation": "现实世界物体通常具有复杂的材料组成和几何结构，而现有方法仅适用于均质单一材料、预学习动力学或简单拓扑的物体，无法准确估计其物理材料参数。", "method": "提出多材料物理高斯方法（M-PhyGs），通过新引入的级联3D和2D损失函数以及时间小批量处理，从自然场景的短视频中联合分割物体材料并恢复其连续介质力学参数（考虑重力影响）。", "result": "在提出的花朵交互数据集Phlowers上实验表明，M-PhyGs及其组件在多材料物理参数估计任务中具有准确性和有效性。", "conclusion": "M-PhyGs能够从视频中准确估计复杂多材料自然物体的物理参数，为现实世界物体的物理建模提供了新方法，并通过Phlowers数据集为该挑战性任务建立了评估平台。"}}
{"id": "2512.16900", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16900", "abs": "https://arxiv.org/abs/2512.16900", "authors": ["Shuyuan Tu", "Yueming Pan", "Yinming Huang", "Xintong Han", "Zhen Xing", "Qi Dai", "Kai Qiu", "Chong Luo", "Zuxuan Wu"], "title": "FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction", "comment": null, "summary": "Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.", "AI": {"tldr": "FlashPortrait是一种端到端视频扩散变换器，能够合成保持身份一致性的无限长度肖像动画，推理速度提升高达6倍。", "motivation": "现有基于扩散模型的长肖像动画加速方法难以保证身份一致性，需要一种既能保持身份特征又能加速推理的解决方案。", "method": "使用现成提取器获取身份无关的面部表情特征；引入归一化面部表情块对齐面部特征与扩散潜在表示；采用动态滑动窗口加权融合确保过渡平滑；基于潜在变化率和导数幅度比，使用高阶潜在导数直接预测未来时间步的潜在表示，跳过多个去噪步骤。", "result": "在基准测试中，FlashPortrait在质量和数量上均表现出有效性，实现了身份保持的长动画合成和6倍推理加速。", "conclusion": "FlashPortrait通过创新的面部特征对齐、动态窗口机制和加速策略，成功解决了长肖像动画的身份一致性和推理效率问题。"}}
{"id": "2512.16905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16905", "abs": "https://arxiv.org/abs/2512.16905", "authors": ["Kaixin Ding", "Yang Zhou", "Xi Chen", "Miao Yang", "Jiarong Ou", "Rui Chen", "Xin Tao", "Hengshuang Zhao"], "title": "Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection", "comment": "project page: https://kxding.github.io/project/Alchemist/", "summary": "Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.", "AI": {"tldr": "提出了Alchemist框架，通过元梯度方法自动选择高质量的文本-图像训练数据对，提升文本到图像生成模型的训练效率和质量。", "motivation": "现有文本到图像生成模型（如Imagen、Stable Diffusion）的性能受限于训练数据质量，网络爬取和合成数据集中常包含低质量或冗余样本，导致视觉保真度下降、训练不稳定和计算效率低下。现有数据选择方法依赖人工筛选或基于单维度特征的启发式评分，缺乏针对图像模态的自动、可扩展解决方案。", "method": "提出Alchemist框架，包含数据评分和数据剪枝两个阶段：1）训练轻量级评分器，基于梯度信息和多粒度感知评估每个样本的影响力；2）使用Shift-G采样策略选择信息丰富的子集进行高效模型训练。", "result": "在合成和网络爬取数据集上的实验表明，Alchemist能持续提升视觉质量和下游性能。使用Alchemist选择的50%数据训练，可超越使用完整数据集训练的效果。", "conclusion": "Alchemist是首个面向文本到图像模型训练的自动、可扩展、基于元梯度的数据选择框架，通过数据中心的优化视角有效提升训练数据质量，为生成模型的数据效率问题提供了新解决方案。"}}
{"id": "2512.16908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16908", "abs": "https://arxiv.org/abs/2512.16908", "authors": ["Yuqun Wu", "Chih-hao Lin", "Henry Che", "Aditi Tiwari", "Chuhang Zou", "Shenlong Wang", "Derek Hoiem"], "title": "SceneDiff: A Benchmark and Method for Multiview Object Change Detection", "comment": null, "summary": "We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.", "AI": {"tldr": "该论文提出了首个多视角变化检测基准SceneDiff Benchmark和一种无需训练的SceneDiff方法，用于检测场景中物体的添加、移除或移动，在多个基准测试中大幅优于现有方法。", "motivation": "检测场景中物体的变化对于机器人整理、施工进度监控等应用很重要，但现有方法面临视角变化导致误检的挑战，且缺乏多视角带实例标注的基准数据集。", "method": "提出SceneDiff方法：1）将不同时间的场景捕获对齐到3D空间；2）提取物体区域；3）比较空间和语义区域特征来检测变化。该方法利用预训练的3D重建、分割和图像编码模型，无需额外训练。", "result": "在多视角和双视角基准测试中，该方法相对现有方法分别取得94%和37.4%的平均精度（AP）提升，并构建了包含350个视频对、数千个变化物体的SceneDiff Benchmark数据集。", "conclusion": "SceneDiff方法通过3D对齐和特征比较有效解决了多视角变化检测问题，提出的基准数据集将促进该领域研究，代码和数据集将公开。"}}
{"id": "2512.16910", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.16910", "abs": "https://arxiv.org/abs/2512.16910", "authors": ["Qihang Rao", "Borui Zhang", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "title": "SFTok: Bridging the Performance Gap in Discrete Tokenizers", "comment": "Under review. Code is available at https://github.com/Neur-IO/SFTok", "summary": "Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \\textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \\textbf{self-forcing guided visual reconstruction} and \\textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).", "AI": {"tldr": "提出SFTok离散分词器，通过多步迭代机制和自强制引导视觉重建策略，在高压缩率下实现高质量图像重建和生成。", "motivation": "现有离散分词器在图像重建质量上落后于连续分词器，限制了其在多模态系统中的广泛应用，需要提升其性能以匹配自回归范式需求。", "method": "采用多步迭代重建机制，结合自强制引导视觉重建和去偏拟合训练策略，解决多步过程中的训练-推理不一致问题。", "result": "在每图像仅64个令牌的高压缩率下，在ImageNet上达到rFID=1.21的最优重建质量，在类别到图像生成任务中取得gFID=2.29的优异表现。", "conclusion": "SFTok通过创新训练策略有效提升了离散分词器的重建能力，为高分辨率图像生成提供了高效且高质量的解决方案。"}}
{"id": "2512.16915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16915", "abs": "https://arxiv.org/abs/2512.16915", "authors": ["Guibao Shen", "Yihua Du", "Wenhang Ge", "Jing He", "Chirui Chang", "Donghao Zhou", "Zhen Yang", "Luozhou Wang", "Xin Tao", "Ying-Cong Chen"], "title": "StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors", "comment": null, "summary": "The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.", "AI": {"tldr": "提出UniStereo数据集和StereoPilot模型，用于高质量单目到立体视频转换，解决传统方法的多阶段流程问题。", "motivation": "立体显示设备快速增长，但3D视频制作成本高、复杂度大；传统单目到立体转换方法存在误差传播、深度模糊和格式不一致等问题。", "method": "构建UniStereo统一数据集支持两种立体格式；提出StereoPilot前馈模型，无需显式深度图或迭代扩散采样，采用可学习域切换器和循环一致性损失。", "result": "StereoPilot在视觉保真度和计算效率上显著优于现有方法，能无缝适应不同立体格式并提升一致性。", "conclusion": "UniStereo数据集和StereoPilot模型有效解决了传统立体视频转换的局限性，为高质量3D内容生成提供了高效解决方案。"}}
{"id": "2512.16913", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16913", "abs": "https://arxiv.org/abs/2512.16913", "authors": ["Xin Lin", "Meixi Song", "Dizhe Zhang", "Wenxuan Lu", "Haodong Li", "Bo Du", "Ming-Hsuan Yang", "Truong Nguyen", "Lu Qi"], "title": "Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation", "comment": "Project Page: https://insta360-research-team.github.io/DAP_website/", "summary": "In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\\_website/}", "AI": {"tldr": "提出了一种全景度量深度基础模型，通过数据循环范式结合大规模数据集和优化方法，实现了跨场景距离的泛化能力，在多个基准测试中表现出色。", "motivation": "现有深度估计模型在跨场景（室内/室外、合成/真实）和不同距离上泛化能力有限，需要一种能适应多样化真实场景的鲁棒全景深度估计方法。", "method": "采用数据循环范式：1）构建大规模数据集（公开数据、UE5合成数据、文本生成图像、网络全景图）；2）三阶段伪标签生成减少域差异；3）以DINOv3-Large为骨干，引入可插拔范围掩码头、锐度中心优化和几何中心优化。", "result": "在Stanford2D3D、Matterport3D、Deep360等基准测试中表现出强大性能，零样本泛化能力优秀，在多样化真实场景中提供鲁棒且稳定的度量深度预测。", "conclusion": "该全景度量深度基础模型通过系统化的数据构建和框架设计，有效解决了跨域泛化问题，为实际应用提供了可靠的深度估计解决方案。"}}
{"id": "2512.16924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16924", "abs": "https://arxiv.org/abs/2512.16924", "authors": ["Hanlin Wang", "Hao Ouyang", "Qiuyu Wang", "Yue Yu", "Yihao Meng", "Wen Wang", "Ka Leong Cheng", "Shuailei Ma", "Qingyan Bai", "Yixuan Li", "Cheng Chen", "Yanhong Zeng", "Xing Zhu", "Yujun Shen", "Qifeng Chen"], "title": "The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text", "comment": "Project page and code: https://worldcanvas.github.io/", "summary": "We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.", "AI": {"tldr": "WorldCanvas是一个支持多模态提示的世界事件生成框架，通过结合文本、轨迹和参考图像实现用户可控的丰富模拟。", "motivation": "现有文本生成视频方法缺乏对运动轨迹和视觉细节的精确控制，难以生成包含多智能体交互、物体进出等复杂事件的连贯视频。", "method": "提出多模态方法：1) 轨迹编码运动、时序和可见性；2) 自然语言表达语义意图；3) 参考图像提供物体视觉特征。三者结合实现可控事件生成。", "result": "生成的视频具有时间连贯性和涌现一致性，能保持物体身份和场景稳定性（即使物体暂时消失），支持多智能体交互、反直觉事件等复杂场景。", "conclusion": "WorldCanvas将世界模型从被动预测器推进为交互式用户可塑模拟器，为可控世界事件生成提供了新框架。"}}
{"id": "2512.16918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16918", "abs": "https://arxiv.org/abs/2512.16918", "authors": ["Chaoyang Wang", "Kaituo Feng", "Dongyang Chen", "Zhongyu Wang", "Zhixun Li", "Sicheng Gao", "Meng Meng", "Xu Zhou", "Manyuan Zhang", "Yuzhang Shang", "Xiangyu Yue"], "title": "AdaTooler-V: Adaptive Tool-Use for Images and Videos", "comment": "Project page: https://github.com/CYWang735/AdaTooler-V", "summary": "Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.", "AI": {"tldr": "提出AdaTooler-V多模态大语言模型，通过自适应工具使用机制减少不必要的视觉工具调用，在多个基准测试中表现优异", "motivation": "现有开源多模态大语言模型存在盲目使用视觉工具的问题，即使不需要时也调用工具，导致推理开销增加和性能下降", "method": "1. 提出AT-GRPO强化学习算法，根据样本的工具效益分数自适应调整奖励尺度；2. 构建两个训练数据集：AdaTooler-V-CoT-100k用于监督微调冷启动，AdaTooler-V-300k用于强化学习；3. 支持单图像、多图像和视频数据的可验证奖励", "result": "在12个基准测试中表现出强大的推理能力，在多样化视觉推理任务中优于现有方法。AdaTooler-V-7B在V*高分辨率基准上达到89.8%准确率，超过GPT-4o和Gemini 1.5 Pro", "conclusion": "AdaTooler-V通过自适应工具使用机制有效减少了不必要的视觉工具调用，提高了推理效率和性能，在多个视觉推理任务中达到最先进水平"}}
{"id": "2512.16923", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16923", "abs": "https://arxiv.org/abs/2512.16923", "authors": ["Chun-Wei Tuan Mu", "Jia-Bin Huang", "Yu-Lun Liu"], "title": "Generative Refocusing: Flexible Defocus Control from a Single Image", "comment": "Project website: https://generative-refocusing.github.io/", "summary": "Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.", "AI": {"tldr": "提出生成式重聚焦方法，通过两步流程实现单图像重聚焦，结合半监督训练提升真实感散景合成效果。", "motivation": "现有单图像重聚焦方法存在三大局限：需要全焦输入、依赖合成数据、光圈控制能力有限，难以满足摄影中对景深控制的灵活需求。", "method": "采用两步流程：DeblurNet从各种输入恢复全焦图像，BokehNet生成可控散景；创新性地使用半监督训练，结合合成配对数据和未配对的真实散景图像，利用EXIF元数据捕捉真实光学特性。", "result": "在散焦去模糊、散景合成和重聚焦基准测试中均取得最优性能；支持文本引导调整和自定义光圈形状。", "conclusion": "生成式重聚焦方法克服了现有技术的局限性，实现了更灵活、真实的单图像景深控制，为摄影后期处理提供了新工具。"}}
{"id": "2512.16922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.16922", "abs": "https://arxiv.org/abs/2512.16922", "authors": ["Sihan Xu", "Ziqiao Ma", "Wenhao Chai", "Xuweiyi Chen", "Weiyang Jin", "Joyce Chai", "Saining Xie", "Stella X. Yu"], "title": "Next-Embedding Prediction Makes Strong Vision Learners", "comment": "Project Page: https://sihanxu.me/nepa", "summary": "Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.", "AI": {"tldr": "提出Next-Embedding Predictive Autoregression (NEPA)，一种通过预测未来图像块嵌入进行视觉自监督学习的方法，无需像素重建或对比损失，在ImageNet和ADE20K上取得优异性能。", "motivation": "受自然语言生成式预训练成功的启发，探索是否可将类似原则应用于视觉自监督学习，从学习表示转向直接学习预测模型。", "method": "使用因果掩码和梯度停止，训练Transformer模型基于过去图像块嵌入预测未来嵌入，称为NEPA。方法仅依赖嵌入预测目标，无需像素重建、离散标记、对比损失或任务特定头。", "result": "在ImageNet-1K上，ViT-B和ViT-L骨干网络微调后分别达到83.8%和85.3%的top-1准确率；在ADE20K语义分割任务上有效迁移。", "conclusion": "基于嵌入的生成式预训练为视觉自监督学习提供了简单、可扩展且可能模态无关的替代方案。"}}
