{"id": "2507.13511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13511", "abs": "https://arxiv.org/abs/2507.13511", "authors": ["Nabil Abdelaziz Ferhat Taleb", "Abdolazim Rezaei", "Raj Atulkumar Patel", "Mehdi Sookhak"], "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination", "comment": null, "summary": "Large Language Models (LLMs) offer significant promise for intelligent\ntraffic management; however, current chain-based systems like TrafficGPT are\nhindered by sequential task execution, high token usage, and poor scalability,\nmaking them inefficient for complex, real-world scenarios. To address these\nlimitations, we propose GraphTrafficGPT, a novel graph-based architecture,\nwhich fundamentally redesigns the task coordination process for LLM-driven\ntraffic applications. GraphTrafficGPT represents tasks and their dependencies\nas nodes and edges in a directed graph, enabling efficient parallel execution\nand dynamic resource allocation. The main idea behind the proposed model is a\nBrain Agent that decomposes user queries, constructs optimized dependency\ngraphs, and coordinates a network of specialized agents for data retrieval,\nanalysis, visualization, and simulation. By introducing advanced context-aware\ntoken management and supporting concurrent multi-query processing, the proposed\narchitecture handles interdependent tasks typical of modern urban mobility\nenvironments. Experimental results demonstrate that GraphTrafficGPT reduces\ntoken consumption by 50.2% and average response latency by 19.0% compared to\nTrafficGPT, while supporting simultaneous multi-query execution with up to\n23.0% improvement in efficiency.", "AI": {"tldr": "GraphTrafficGPT提出了一种图基架构，通过并行执行和动态资源分配，显著提升了大型语言模型（LLMs）在智能交通管理中的效率和可扩展性，解决了现有链式系统的局限性。", "motivation": "现有基于LLM的交通管理系统（如TrafficGPT）采用链式执行，存在任务顺序执行、高token消耗和可扩展性差等问题，难以有效应对复杂的现实交通场景。", "method": "提出GraphTrafficGPT，一种图基架构，将任务及其依赖表示为有向图中的节点和边，实现并行执行和动态资源分配。核心思想是引入一个“大脑代理”（Brain Agent），负责分解用户查询、构建优化依赖图，并协调专门代理（数据检索、分析、可视化、仿真）网络。此外，还引入了上下文感知token管理和并发多查询处理。", "result": "与TrafficGPT相比，GraphTrafficGPT将token消耗降低了50.2%，平均响应延迟减少了19.0%。同时，在支持同步多查询执行方面，效率提升高达23.0%。", "conclusion": "GraphTrafficGPT通过其创新的图基架构和大脑代理协调机制，有效解决了LLM驱动交通应用中链式系统的效率和可扩展性问题，为复杂的现代城市交通环境提供了更优的解决方案。"}}
{"id": "2507.13541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13541", "abs": "https://arxiv.org/abs/2507.13541", "authors": ["Shuyue Stella Li", "Melanie Sclar", "Hunter Lang", "Ansong Ni", "Jacqueline He", "Puxin Xu", "Andrew Cohen", "Chan Young Park", "Yulia Tsvetkov", "Asli Celikyilmaz"], "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes", "comment": "17 pages, 6 tables, 5 figures", "summary": "Personalizing AI systems requires understanding not just what users prefer,\nbut the reasons that underlie those preferences - yet current preference models\ntypically treat human judgment as a black box. We introduce PrefPalette, a\nframework that decomposes preferences into attribute dimensions and tailors its\npreference prediction to distinct social community values in a\nhuman-interpretable manner. PrefPalette operationalizes a cognitive science\nprinciple known as multi-attribute decision making in two ways: (1) a scalable\ncounterfactual attribute synthesis step that involves generating synthetic\ntraining data to isolate for individual attribute effects (e.g., formality,\nhumor, cultural values), and (2) attention-based preference modeling that\nlearns how different social communities dynamically weight these attributes.\nThis approach moves beyond aggregate preference modeling to capture the diverse\nevaluation frameworks that drive human judgment. When evaluated on 45 social\ncommunities from the online platform Reddit, PrefPalette outperforms GPT-4o by\n46.6% in average prediction accuracy. Beyond raw predictive improvements,\nPrefPalette also shed light on intuitive, community-specific profiles:\nscholarly communities prioritize verbosity and stimulation, conflict-oriented\ncommunities value sarcasm and directness, and support-based communities\nemphasize empathy. By modeling the attribute-mediated structure of human\njudgment, PrefPalette delivers both superior preference modeling and\ntransparent, interpretable insights, and serves as a first step toward more\ntrustworthy, value-aware personalized applications.", "AI": {"tldr": "PrefPalette是一个框架，它将用户偏好分解为可解释的属性维度，并根据不同社群的价值观动态调整偏好预测，显著优于现有模型并提供透明的洞察。", "motivation": "当前的AI偏好模型将人类判断视为黑箱，无法理解偏好背后的深层原因，导致个性化不足。研究旨在突破聚合偏好模型，捕捉驱动人类判断的多元评估框架。", "method": "PrefPalette框架基于多属性决策原则，包含两部分：1) 可扩展的反事实属性合成步骤，生成合成训练数据以隔离单个属性（如正式性、幽默、文化价值观）的影响；2) 基于注意力的偏好建模，学习不同社群如何动态地加权这些属性。", "result": "在Reddit的45个社群上进行评估，PrefPalette的平均预测准确率比GPT-4o高出46.6%。此外，它揭示了直观的社群特定偏好：学术社群重视冗长和刺激性，冲突导向社群看重讽刺和直接性，支持型社群强调同理心。", "conclusion": "通过建模人类判断的属性中介结构，PrefPalette不仅提供了卓越的偏好建模能力，还带来了透明、可解释的洞察，为构建更值得信赖、更具价值观意识的个性化应用迈出了第一步。"}}
{"id": "2507.13550", "categories": ["cs.AI", "cs.CL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.13550", "abs": "https://arxiv.org/abs/2507.13550", "authors": ["Eduardo C. Garrido-Merchán", "Cristina Puente"], "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models", "comment": null, "summary": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains.", "AI": {"tldr": "本文提出一种受控且透明地使用大型语言模型（LLMs）开发专家系统的新方法，通过将LLM生成的信息转换为Prolog符号知识，并允许人工验证，以解决LLMs幻觉问题，同时确保可解释性、可扩展性和可靠性。", "motivation": "大型语言模型（LLMs）在知识密集型任务中表现出色，但存在幻觉或自信地生成不正确、不可验证事实的缺点。研究动机在于如何利用LLMs的生成能力，同时克服其不可靠性，从而开发出可信赖的专家系统。", "method": "该方法通过限制领域和采用结构化的基于提示的提取方法，将LLM（如Claude Sonnet 3.7和GPT-4.1）生成的信息转化为Prolog中的符号表示。这种符号知识可以由人类专家进行验证和纠正，从而保证了系统的可解释性、可扩展性和可靠性。", "result": "通过定量和定性实验，研究表明所生成的知识库在事实依从性和语义连贯性方面表现出色。结果展示了一个透明的混合解决方案，成功结合了LLMs的召回能力和符号系统的精确性。", "conclusion": "该研究提出了一个结合LLMs召回能力与符号系统精确性的混合解决方案，为在敏感领域开发可靠的人工智能应用奠定了基础，解决了LLMs在专家系统应用中的可靠性问题。"}}
{"id": "2507.13558", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13558", "abs": "https://arxiv.org/abs/2507.13558", "authors": ["David Poole"], "title": "Why Isn't Relational Learning Taking Over the World?", "comment": "10 pages (6 pages + references + appendices)", "summary": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.", "AI": {"tldr": "当前AI主要关注像素、文字等感知数据，但现实世界和企业最有价值的数据是关系型的。本文探讨了关系学习（如统计关系AI）未能普及的原因，并提出了使其获得应有地位的改进方向。", "motivation": "现有AI系统主要建模像素、文字和音素，而世界本质上由具有属性和关系实体构成。企业最有价值的数据多以电子表格、数据库等关系格式存在，而非文本或图像。尽管关系学习研究这类数据，但其并未像其他AI领域一样普及，作者旨在探究其原因。", "method": "本文通过分析和解释的方式，阐述了关系学习未能广泛应用的原因（除少数受限关系情况），并提出了为提升其地位所需采取的措施。", "result": "研究揭示了关系学习目前未能普及的原因，并指出了为使其达到应有重要性所必需的改进和发展方向。", "conclusion": "关系学习作为一种处理实体、属性和关系数据的AI范式，尽管与现实世界和企业核心数据高度相关，但目前其应用受限。为使其发挥潜力，需要针对性地解决现有问题，使其在AI领域获得更重要的地位。"}}
{"id": "2507.13534", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13534", "abs": "https://arxiv.org/abs/2507.13534", "authors": ["Leo Semmelmann", "Frederik vom Scheidt"], "title": "Heatwave-driven air conditioning adoption could increase German electricity demand by 14 GW in the near future", "comment": "10 pages, 6 figures", "summary": "Intensifying heatwaves driven by climate change are accelerating the adoption\nof mobile air conditioning (AC) systems. A rapid mass adoption of such AC\nsystems could create additional stress on electricity grids and the power\nsystem. This study presents a novel method to estimate the electricity demand\nfrom AC systems both at system level and at high temporal and spatial\ngranularity. We apply the method to a near-future heatwave scenario in Germany\nin which household AC adoption increases from current 19% to 35% during a\nheatwave similar to the one of July 2025. We analyze the effects for 196,428\ngrid cells of one square kilometer across Germany, by combining weather data,\ncensus data, socio-demographic assumptions, mobility patterns, and\ntemperature-dependent AC activation functions. We find that electricity demand\nof newly purchased mobile AC systems could increase the peak load by over 14 GW\n(23%), with urban hot-spots reaching 5.8 MW per square kilometer. The temporal\npattern creates a pronounced afternoon peak that coincides with lower\nphotovoltaic generation, potentially exacerbating power system stability\nchallenges. Our findings underscore the urgency for proactive energy system\nplanning to manage emerging demand peaks.", "AI": {"tldr": "研究预测，在德国未来热浪情景下，移动空调的普及将使电网峰值负荷增加超过14 GW（23%），尤其是在城市热点区域，且需求高峰与光伏发电低谷重合，对电力系统稳定构成挑战。", "motivation": "气候变化导致的热浪加剧促使移动空调系统快速普及，这可能给电网和电力系统带来额外压力。", "method": "本研究提出了一种估算空调系统电力需求的新方法，该方法能够在系统层面以及高时间和空间粒度上进行估算。该方法结合了天气数据、人口普查数据、社会人口学假设、出行模式和温度依赖的空调激活函数。研究将此方法应用于德国近未来（2025年7月）的热浪情景，假设家庭空调普及率从目前的19%增加到35%，并分析了德国196,428个1平方公里网格单元的影响。", "result": "新购买的移动空调系统可能使峰值负荷增加超过14 GW（23%），城市热点区域每平方公里可达5.8 MW。需求的时间模式呈现明显的午后高峰，与光伏发电量较低的时间段重合，可能加剧电力系统稳定性挑战。", "conclusion": "研究结果强调了积极主动的能源系统规划的紧迫性，以管理新兴的需求高峰。"}}
{"id": "2507.13384", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13384", "abs": "https://arxiv.org/abs/2507.13384", "authors": ["Osama Hardan", "Omar Elshenhabi", "Tamer Khattab", "Mohamed Mabrok"], "title": "Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation", "comment": "Submitted to the 2025 IEEE International Conference on Future Machine\n  Learning and Data Science (FMLDS)", "summary": "Vision Mamba models promise transformer-level performance at linear\ncomputational cost, but their reliance on serializing 2D images into 1D\nsequences introduces a critical, yet overlooked, design choice: the patch scan\norder. In medical imaging, where modalities like brain MRI contain strong\nanatomical priors, this choice is non-trivial. This paper presents the first\nsystematic study of how scan order impacts MRI segmentation. We introduce\nMulti-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures\nthat facilitates exploring diverse scan paths without additional computational\ncost. We conduct a large-scale benchmark of 21 scan strategies on three public\ndatasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our\nanalysis shows conclusively that scan order is a statistically significant\nfactor (Friedman test: $\\chi^{2}_{20}=43.9, p=0.0016$), with performance\nvarying by as much as 27 Dice points. Spatially contiguous paths -- simple\nhorizontal and vertical rasters -- consistently outperform disjointed diagonal\nscans. We conclude that scan order is a powerful, cost-free hyperparameter, and\nprovide an evidence-based shortlist of optimal paths to maximize the\nperformance of Mamba models in medical imaging.", "AI": {"tldr": "本文首次系统研究了Vision Mamba模型中图像块扫描顺序对MRI分割性能的影响，发现扫描顺序是一个关键且被忽视的超参数，简单的空间连续路径表现最佳。", "motivation": "Vision Mamba模型在计算效率和性能上具有潜力，但将2D图像序列化为1D时，图像块的扫描顺序是一个关键但常被忽视的设计选择，尤其是在具有强解剖先验的医学影像中。", "method": "本文提出了Multi-Scan 2D (MS2D)模块，一个无参数的Mamba架构模块，用于探索不同的扫描路径。研究在三个公共数据集（BraTS 2020, ISLES 2022, LGG）上对21种扫描策略进行了大规模基准测试，涵盖超过70,000个切片。通过Friedman检验进行统计分析。", "result": "分析结果显示，扫描顺序是一个统计学上显著的因素（Friedman检验: $\\chi^{2}_{20}=43.9, p=0.0016$），性能差异高达27个Dice点。空间连续的路径（如简单的水平和垂直光栅扫描）始终优于不连续的对角线扫描。", "conclusion": "扫描顺序是一个强大且无额外计算成本的超参数。本文为在医学影像中最大化Mamba模型性能提供了基于证据的最佳路径选择建议。"}}
{"id": "2507.13455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13455", "abs": "https://arxiv.org/abs/2507.13455", "authors": ["Dean Chen", "Armin Pomeroy", "Brandon T. Peterson", "Will Flanagan", "He Kai Lim", "Alexandra Stavrakis", "Nelson F. SooHoo", "Jonathan B. Hopkins", "Tyler R. Clites"], "title": "Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms", "comment": "42 pages, 17 figures. Under review at ASME Journal of Mechanical\n  Design", "summary": "Compliant mechanisms have significant potential in precision applications due\nto their ability to guide motion without contact. However, an inherent\nvulnerability to fatigue and mechanical failure has hindered the translation of\ncompliant mechanisms to real-world applications. This is particularly\nchallenging in service environments where loading is complex and uncertain, and\nthe cost of failure is high. In such cases, mechanical hard stops are critical\nto prevent yielding and buckling. Conventional hard-stop designs, which rely on\nstacking single-DOF limits, must be overly restrictive in multi-DOF space to\nguarantee safety in the presence of unknown loads. In this study, we present a\nsystematic design synthesis method to guarantee overload protection in\ncompliant mechanisms by integrating coupled multi-DOF motion limits within a\nsingle pair of compact hard-stop surfaces. Specifically, we introduce a\ntheoretical and practical framework for optimizing the contact surface geometry\nto maximize the mechanisms multi-DOF working space while still ensuring that\nthe mechanism remains within its elastic regime. We apply this synthesis method\nto a case study of a caged-hinge mechanism for orthopaedic implants, and\nprovide numerical and experimental validation that the derived design offers\nreliable protection against fatigue, yielding, and buckling. This work\nestablishes a foundation for precision hard-stop design in compliant systems\noperating under uncertain loads, which is a crucial step toward enabling the\napplication of compliant mechanisms in real-world systems.", "AI": {"tldr": "本文提出了一种系统性的设计方法，通过集成耦合的多自由度限位结构，为柔性机构提供过载保护，在不确定载荷下最大化工作空间并防止疲劳和失效，为柔性机构的实际应用奠定了基础。", "motivation": "柔性机构在精密应用中潜力巨大，但其固有的疲劳和机械失效问题阻碍了其实际应用，尤其是在复杂和不确定载荷环境下。传统的限位设计在多自由度空间中过于严格，无法有效平衡安全性和工作空间。", "method": "开发了一种系统性的设计合成方法，通过将耦合的多自由度运动限制集成到一对紧凑的硬停止表面中。该方法引入了一个理论和实践框架，用于优化接触表面几何形状，以在确保机构保持弹性变形的前提下，最大化其多自由度工作空间。该方法应用于骨科植入物的笼式铰链机构案例研究。", "result": "所推导的设计能够可靠地防止疲劳、屈服和屈曲。研究提供了数值和实验验证，证明了其有效性。", "conclusion": "这项工作为在不确定载荷下运行的柔性系统中的精密硬停止设计奠定了基础，这是实现柔性机构在实际系统中应用的关键一步。"}}
{"id": "2507.13359", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13359", "abs": "https://arxiv.org/abs/2507.13359", "authors": ["Yang Zhou", "Junjie Li", "CongYang Ou", "Dawei Yan", "Haokui Zhang", "Xizhe Xue"], "title": "Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives", "comment": "27 pages, 5 figures", "summary": "Due to its extensive applications, aerial image object detection has long\nbeen a hot topic in computer vision. In recent years, advancements in Unmanned\nAerial Vehicles (UAV) technology have further propelled this field to new\nheights, giving rise to a broader range of application requirements. However,\ntraditional UAV aerial object detection methods primarily focus on detecting\npredefined categories, which significantly limits their applicability. The\nadvent of cross-modal text-image alignment (e.g., CLIP) has overcome this\nlimitation, enabling open-vocabulary object detection (OVOD), which can\nidentify previously unseen objects through natural language descriptions. This\nbreakthrough significantly enhances the intelligence and autonomy of UAVs in\naerial scene understanding. This paper presents a comprehensive survey of OVOD\nin the context of UAV aerial scenes. We begin by aligning the core principles\nof OVOD with the unique characteristics of UAV vision, setting the stage for a\nspecialized discussion. Building on this foundation, we construct a systematic\ntaxonomy that categorizes existing OVOD methods for aerial imagery and provides\na comprehensive overview of the relevant datasets. This structured review\nenables us to critically dissect the key challenges and open problems at the\nintersection of these fields. Finally, based on this analysis, we outline\npromising future research directions and application prospects. This survey\naims to provide a clear road map and a valuable reference for both newcomers\nand seasoned researchers, fostering innovation in this rapidly evolving domain.\nWe keep tracing related works at\nhttps://github.com/zhouyang2002/OVOD-in-UVA-imagery", "AI": {"tldr": "本文综述了无人机（UAV）航拍场景中的开放词汇目标检测（OVOD）技术，涵盖其原理、现有方法、数据集、面临的挑战及未来发展方向。", "motivation": "传统的无人机航拍目标检测方法受限于预定义类别，极大地限制了其应用范围。跨模态文本-图像对齐技术（如CLIP）的出现，使得开放词汇目标检测（OVOD）成为可能，能够通过自然语言描述识别未曾见过的物体，从而显著增强无人机在空中场景理解方面的智能性和自主性。", "method": "本文采用综述方法，首先将OVOD的核心原理与UAV视觉的独特特性相结合；其次，构建了一个系统的分类法，对现有航拍图像OVOD方法进行分类，并全面概述相关数据集；最后，通过分析批判性地剖析了这些领域交叉的关键挑战和未解决问题。", "result": "本文构建了一个系统的分类法，涵盖了现有的航拍图像OVOD方法和相关数据集，并批判性地分析了该领域面临的关键挑战和开放问题。基于此分析，论文提出了有前景的未来研究方向和应用前景。", "conclusion": "本综述旨在为该快速发展的领域提供清晰的路线图和有价值的参考，以促进该领域的创新，对新手和经验丰富的研究人员都有帮助。"}}
{"id": "2507.13357", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13357", "abs": "https://arxiv.org/abs/2507.13357", "authors": ["Atharva Bhargude", "Ishan Gonehal", "Chandler Haney", "Dave Yoon", "Kevin Zhu", "Aaron Sandoval", "Sean O'Brien", "Kaustubh Vinnakota"], "title": "Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models", "comment": "Published at ACL 2025 SRW, 9 pages, 3 figures", "summary": "Phishing attacks represent a significant cybersecurity threat, necessitating\nadaptive detection techniques. This study explores few-shot Adaptive Linguistic\nPrompting (ALP) in detecting phishing webpages through the multimodal\ncapabilities of state-of-the-art large language models (LLMs) such as GPT-4o\nand Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides\nLLMs to analyze textual deception by breaking down linguistic patterns,\ndetecting urgency cues, and identifying manipulative diction commonly found in\nphishing content. By integrating textual, visual, and URL-based analysis, we\npropose a unified model capable of identifying sophisticated phishing attempts.\nOur experiments demonstrate that ALP significantly enhances phishing detection\naccuracy by guiding LLMs through structured reasoning and contextual analysis.\nThe findings highlight the potential of ALP-integrated multimodal LLMs to\nadvance phishing detection frameworks, achieving an F1-score of 0.93,\nsurpassing traditional approaches. These results establish a foundation for\nmore robust, interpretable, and adaptive linguistic-based phishing detection\nsystems using LLMs.", "AI": {"tldr": "本研究利用少数样本自适应语言提示（ALP）结合GPT-4o和Gemini 1.5 Pro等多模态大型语言模型（LLMs），实现对网络钓鱼网页的高效检测，并取得了显著的F1分数。", "motivation": "网络钓鱼攻击是重大的网络安全威胁，需要自适应的检测技术来应对。", "method": "采用少数样本自适应语言提示（ALP），这是一种结构化的语义推理方法，指导LLMs分析文本欺骗，包括识别语言模式、紧急提示和操纵性措辞。该方法整合了文本、视觉和URL信息进行多模态分析，以识别复杂的网络钓鱼尝试。", "result": "实验证明，ALP显著提高了网络钓鱼检测的准确性，通过结构化推理和上下文分析指导LLMs。该方法实现了0.93的F1分数，超越了传统方法。", "conclusion": "研究结果表明，结合ALP的多模态LLMs在推进网络钓鱼检测框架方面具有巨大潜力，为更鲁棒、可解释和自适应的基于语言的网络钓鱼检测系统奠定了基础。"}}
{"id": "2507.13625", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13625", "abs": "https://arxiv.org/abs/2507.13625", "authors": ["Yuxin Zhang", "Xi Wang", "Mo Hu", "Zhenyu Zhang"], "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety", "comment": "19 pages, 13 figures", "summary": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains.", "AI": {"tldr": "BifrostRAG是一个双图RAG系统，结合图遍历和向量搜索，用于处理复杂法规文本中的多跳查询，显著优于传统RAG。", "motivation": "自动化合规性检查需要从复杂的安全法规中检索信息，但法规文本的语言和结构复杂性，以及多跳查询的需求，对传统检索增强生成（RAG）系统构成了挑战。", "method": "提出BifrostRAG，一个集成了双图的RAG系统。它通过实体网络图（Entity Network Graph）建模语言关系，通过文档导航图（Document Navigator Graph）建模文档结构。该系统采用混合检索机制，结合图遍历和基于向量的语义搜索，使大型语言模型能够同时推理文本的含义和结构。", "result": "在多跳问题数据集上进行评估，BifrostRAG实现了92.8%的精确率、85.5%的召回率和87.3%的F1分数。这些结果显著优于仅基于向量和仅基于图的RAG基线方法。", "conclusion": "BifrostRAG被确立为LLM驱动的合规性检查的强大知识引擎。其双图、混合检索机制为在知识密集型工程领域处理复杂技术文档提供了可迁移的蓝图。"}}
{"id": "2507.13623", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13623", "abs": "https://arxiv.org/abs/2507.13623", "authors": ["Rahul Gulia"], "title": "MD-OFDM: An Energy-Efficient and Low-PAPR MIMO-OFDM Variant for Resource-Constrained Applications", "comment": null, "summary": "Orthogonal Frequency Division Multiplexing (OFDM) combined with\nMultiple-Input Multiple-Output (MIMO) techniques forms the backbone of modern\nwireless communication systems. While offering high spectral efficiency and\nrobustness, conventional MIMO-OFDM, especially with complex equalizers like\nMinimum Mean Square Error (MMSE), suffers from high Peak-to-Average Power Ratio\n(PAPR) and significant power consumption due to multiple active Radio Frequency\n(RF) chains. This paper proposes and mathematically models an alternative\nsystem, termed Multi-Dimensional OFDM (MD-OFDM), which employs a per-subcarrier\ntransmit antenna selection strategy. By activating only one transmit antenna\nfor each subcarrier, MD-OFDM aims to reduce PAPR, lower power consumption, and\nimprove Bit Error Rate (BER) performance. We provide detailed mathematical\nformulations for BER, Energy Efficiency (EE), and PAPR, and discuss the\nsuitability of MD-OFDM for various applications, particularly in\nenergy-constrained and cost-sensitive scenarios such as the Internet of Things\n(IoT) and Low-Power Wide Area Networks (LPWAN). Simulation results demonstrate\nthat MD-OFDM achieves superior BER and significantly lower PAPR compared to\nMMSE MIMO, albeit with a trade-off in peak overall energy efficiency due to\nreduced spectral multiplexing.", "AI": {"tldr": "本文提出多维OFDM（MD-OFDM）系统，通过逐子载波发射天线选择策略，旨在降低传统MIMO-OFDM的高峰均功率比（PAPR）和功耗，并提升误码率（BER）性能。", "motivation": "传统的MIMO-OFDM系统，特别是使用MMSE等复杂均衡器时，由于多路射频链导致高峰均功率比（PAPR）和显著的功耗，是其主要缺点。", "method": "本文提出MD-OFDM系统，其核心方法是为每个子载波仅激活一根发射天线（逐子载波发射天线选择）。此外，论文还提供了BER、能量效率（EE）和PAPR的详细数学公式模型。", "result": "仿真结果表明，MD-OFDM与MMSE MIMO相比，实现了更优的误码率（BER）和显著更低的PAPR。然而，由于频谱复用减少，其峰值整体能量效率有所下降。", "conclusion": "MD-OFDM是一种适用于物联网（IoT）和低功耗广域网（LPWAN）等对能量和成本敏感场景的替代方案，它能有效降低PAPR和功耗，并改善BER性能。"}}
{"id": "2507.13394", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13394", "abs": "https://arxiv.org/abs/2507.13394", "authors": ["Akhil John Thomas", "Christiaan Boerkamp"], "title": "Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning", "comment": null, "summary": "Nerve segmentation is crucial in medical imaging for precise identification\nof nerve structures. This study presents an optimized DeepLabV3-based\nsegmentation pipeline that incorporates automated threshold fine-tuning to\nimprove segmentation accuracy. By refining preprocessing steps and implementing\nparameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a\nPixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate\nsignificant improvements over baseline models and highlight the importance of\ntailored parameter selection in automated nerve detection.", "AI": {"tldr": "本研究提出了一种优化的基于DeepLabV3的神经分割流程，通过自动化阈值微调显著提高了超声神经图像的分割精度。", "motivation": "神经分割在医学成像中对于精确识别神经结构至关重要。", "method": "本研究采用了一种优化的DeepLabV3分割流程，结合了自动化阈值微调，并对预处理步骤和参数进行了优化。", "result": "在超声神经成像上，该方法取得了0.78的Dice分数、0.70的IoU和0.95的像素精度，显示出比基线模型显著的改进。", "conclusion": "研究结果证明了该方法的显著改进，并强调了在自动化神经检测中定制参数选择的重要性。"}}
{"id": "2507.13468", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13468", "abs": "https://arxiv.org/abs/2507.13468", "authors": ["Shiye Cao", "Maia Stiber", "Amama Mahmood", "Maria Teresa Parreira", "Wendy Ju", "Micol Spitale", "Hatice Gunes", "Chien-Ming Huang"], "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations", "comment": null, "summary": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.", "AI": {"tldr": "ERR@HRI 2.0 挑战赛旨在通过提供多模态数据集，鼓励研究人员开发机器学习模型来检测LLM驱动的对话机器人在人机交互中的故障，以提升故障检测能力。", "motivation": "LLM驱动的对话机器人虽然使人机对话更具动态性，但仍易出现错误，如误解用户意图、过早打断或无响应。这些故障会导致对话中断、任务受阻和用户信任流失，因此检测和解决这些问题至关重要。", "method": "挑战赛提供了一个包含16小时双向人机交互的多模态数据集，其中包括面部、语音和头部运动特征。数据已标注系统视角的机器人错误以及用户纠正意图。参与者需利用这些多模态数据开发机器学习模型来检测机器人故障，提交的模型将根据检测准确率和误报率等指标进行评估。", "result": "预期结果是参与者能开发出有效的机器学习模型，利用多模态数据检测LLM驱动的对话机器人故障，并通过挑战赛的评估指标进行性能验证。", "conclusion": "该挑战是利用社交信号分析改进人机交互中故障检测的又一关键步骤，旨在推动该领域的研究进展。"}}
{"id": "2507.13360", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13360", "abs": "https://arxiv.org/abs/2507.13360", "authors": ["Le-Anh Tran", "Chung Nguyen Tran", "Ngoc-Luu Nguyen", "Nhan Cach Dang", "Jordi Carrabina", "David Castells-Rufas", "Minh Son Nguyen"], "title": "Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance", "comment": "6 pages, 3 figures, ICCCE 2025", "summary": "This paper introduces a novel deep learning framework for low-light image\nenhancement, named the Encoder-Decoder Network with Illumination Guidance\n(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination\nmap, derived from Bright Channel Prior (BCP), as a guidance input. This\nillumination guidance helps the network focus on underexposed regions,\neffectively steering the enhancement process. To further improve the model's\nrepresentational power, a Spatial Pyramid Pooling (SPP) module is incorporated\nto extract multi-scale contextual features, enabling better handling of diverse\nlighting conditions. Additionally, the Swish activation function is employed to\nensure smoother gradient propagation during training. EDNIG is optimized within\na Generative Adversarial Network (GAN) framework using a composite loss\nfunction that combines adversarial loss, pixel-wise mean squared error (MSE),\nand perceptual loss. Experimental results show that EDNIG achieves competitive\nperformance compared to state-of-the-art methods in quantitative metrics and\nvisual quality, while maintaining lower model complexity, demonstrating its\nsuitability for real-world applications. The source code for this work is\navailable at https://github.com/tranleanh/ednig.", "AI": {"tldr": "本文提出了一种名为EDNIG的深度学习框架，用于低光图像增强，该框架基于U-Net并结合了亮度通道先验（BCP）生成的照明图作为指导输入，同时利用空间金字塔池化（SPP）提取多尺度特征，并在GAN框架下通过复合损失函数进行优化。", "motivation": "现有方法在低光图像增强方面仍有提升空间，尤其是在处理欠曝光区域和复杂光照条件时，同时需要兼顾模型性能和复杂度。", "method": "EDNIG框架构建于U-Net架构之上，核心创新包括：1) 整合基于亮度通道先验（BCP）的照明图作为指导输入，以聚焦欠曝光区域；2) 引入空间金字塔池化（SPP）模块以提取多尺度上下文特征；3) 采用Swish激活函数确保梯度传播平滑；4) 在生成对抗网络（GAN）框架下进行优化，使用结合对抗损失、像素级均方误差（MSE）和感知损失的复合损失函数。", "result": "实验结果表明，EDNIG在定量指标和视觉质量方面均达到了与现有最先进方法相当的竞争性性能，同时保持了较低的模型复杂度。", "conclusion": "EDNIG框架在低光图像增强方面表现出色，其性能和较低的复杂度使其适用于实际应用。"}}
{"id": "2507.13380", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13380", "abs": "https://arxiv.org/abs/2507.13380", "authors": ["Keito Inoshita", "Rushia Harada"], "title": "Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition", "comment": null, "summary": "In the field of emotion recognition, the development of high-performance\nmodels remains a challenge due to the scarcity of high-quality, diverse\nemotional datasets. Emotional expressions are inherently subjective, shaped by\nindividual personality traits, socio-cultural backgrounds, and contextual\nfactors, making large-scale, generalizable data collection both ethically and\npractically difficult. To address this issue, we introduce PersonaGen, a novel\nframework for generating emotionally rich text using a Large Language Model\n(LLM) through multi-stage persona-based conditioning. PersonaGen constructs\nlayered virtual personas by combining demographic attributes, socio-cultural\nbackgrounds, and detailed situational contexts, which are then used to guide\nemotion expression generation. We conduct comprehensive evaluations of the\ngenerated synthetic data, assessing semantic diversity through clustering and\ndistributional metrics, human-likeness via LLM-based quality scoring, realism\nthrough comparison with real-world emotion corpora, and practical utility in\ndownstream emotion classification tasks. Experimental results show that\nPersonaGen significantly outperforms baseline methods in generating diverse,\ncoherent, and discriminative emotion expressions, demonstrating its potential\nas a robust alternative for augmenting or replacing real-world emotional\ndatasets.", "AI": {"tldr": "PersonaGen是一个新颖的框架，利用大型语言模型（LLM）通过多阶段基于角色的条件作用生成情感丰富的文本，旨在解决高质量情感数据集稀缺的问题。", "motivation": "情感识别领域面临高质量、多样化情感数据集稀缺的挑战。情感表达具有主观性，受个体特质、社会文化背景和情境因素影响，导致大规模、通用性数据收集在伦理和实践上都很困难。", "method": "该研究引入了PersonaGen框架，通过多阶段基于角色的条件作用，利用大型语言模型（LLM）生成情感文本。PersonaGen构建分层虚拟角色，结合人口统计属性、社会文化背景和详细情境上下文，以此引导情感表达的生成。生成的合成数据通过聚类和分布度量评估语义多样性，通过基于LLM的质量评分评估类人度，通过与真实世界情感语料库比较评估真实性，并通过下游情感分类任务评估实用性。", "result": "实验结果表明，PersonaGen在生成多样、连贯和可区分的情感表达方面显著优于基线方法。", "conclusion": "PersonaGen展示了作为增强或替代真实世界情感数据集的强大潜力，是一种稳健的替代方案。"}}
{"id": "2507.13651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13651", "abs": "https://arxiv.org/abs/2507.13651", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks", "comment": null, "summary": "Many intelligent tutoring systems can support a student in solving a stepwise\ntask. When a student combines several steps in one step, the number of possible\npaths connecting consecutive inputs may be very large. This combinatorial\nexplosion makes error diagnosis hard. Using a final answer to diagnose a\ncombination of steps can mitigate the combinatorial explosion, because there\nare generally fewer possible (erroneous) final answers than (erroneous)\nsolution paths. An intermediate input for a task can be diagnosed by\nautomatically completing it according to the task solution strategy and\ndiagnosing this solution. This study explores the potential of automated error\ndiagnosis based on a final answer. We investigate the design of a service that\nprovides a buggy rule diagnosis when a student combines several steps. To\nvalidate the approach, we apply the service to an existing dataset (n=1939) of\nunique student steps when solving quadratic equations, which could not be\ndiagnosed by a buggy rule service that tries to connect consecutive inputs with\na single rule. Results show that final answer evaluation can diagnose 29,4% of\nthese steps. Moreover, a comparison of the generated diagnoses with teacher\ndiagnoses on a subset (n=115) shows that the diagnoses align in 97% of the\ncases. These results can be considered a basis for further exploration of the\napproach.", "AI": {"tldr": "本研究提出并验证了一种基于最终答案的自动错误诊断方法，用于解决智能辅导系统中学生合并多个步骤时诊断困难的问题，并在二次方程求解数据集中取得了良好效果。", "motivation": "在智能辅导系统中，当学生将多个步骤合并为一个步骤时，连接连续输入的可能路径数量会呈组合爆炸式增长，使得错误诊断变得非常困难。传统的单规则诊断服务无法处理这种情况。", "method": "研究探索了基于最终答案的自动错误诊断潜力。具体方法是：当学生合并多个步骤时，利用最终答案来诊断错误，通过自动完成中间输入来诊断其解决方案。设计了一个服务来提供错误规则诊断，并将其应用于一个包含1939个唯一学生步骤的二次方程求解现有数据集进行验证，这些步骤是之前无法被单一规则服务诊断的。", "result": "结果显示，最终答案评估能够诊断出这些步骤中的29.4%。此外，在115个步骤的子集上，生成的诊断与教师诊断的吻合度高达97%。", "conclusion": "基于最终答案的错误诊断方法在处理学生合并步骤时的诊断问题上具有潜力，其结果为进一步探索该方法奠定了基础。"}}
{"id": "2507.13672", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13672", "abs": "https://arxiv.org/abs/2507.13672", "authors": ["Hang Zhou", "Tao Meng", "Kun Wang", "Chengrui Shi", "Renhao Mao", "Weijia Wang", "Jiakun Lei"], "title": "Spacecraft Safe Robust Control Using Implicit Neural Representation for Geometrically Complex Targets in Proximity Operations", "comment": "15 pages, 18 figures, submitted to TAES", "summary": "This study addresses the challenge of ensuring safe spacecraft proximity\noperations, focusing on collision avoidance between a chaser spacecraft and a\ncomplex-geometry target spacecraft under disturbances. To ensure safety in such\nscenarios, a safe robust control framework is proposed that leverages implicit\nneural representations. To handle arbitrary target geometries without explicit\nmodeling, a neural signed distance function (SDF) is learned from point cloud\ndata via a enhanced implicit geometric regularization method, which\nincorporates an over-apporximation strategy to create a conservative,\nsafety-prioritized boundary. The target's surface is implicitly defined by the\nzero-level set of the learned neural SDF, while the values and gradients\nprovide critical information for safety controller design. This neural SDF\nrepresentation underpins a two-layer hierarchcial safe robust control\nframework: a safe velocity generation layer and a safe robust controller layer.\nIn the first layer, a second-order cone program is formulated to generate\nsafety-guaranteed reference velocity by explicitly incorporating the\nunder-approximation error bound. Furthermore, a circulation inequality is\nintroduced to mitigate the local minimum issues commonly encountered in control\nbarrier function (CBF) methods. The second layer features an integrated\ndisturbance observer and a smooth safety filter explicitly compensating for\nestimation error, bolstering robustness to external disturbances. Extensive\nnumerical simulations and Monte Carlo analysis validate the proposed framework,\ndemonstrating significantly improved safety margins and avoidance of local\nminima compared to conventional CBF approaches.", "AI": {"tldr": "该研究提出一种基于隐式神经表示和两层分层控制框架的安全鲁棒控制方法，用于解决复杂几何目标航天器在扰动下的近距离操作碰撞避免问题，显著提升了安全性并避免了局部最小值问题。", "motivation": "确保航天器近距离操作（特别是追逐器与复杂几何目标航天器之间的操作）在存在扰动的情况下避免碰撞，是当前面临的挑战。", "method": "1. 学习神经符号距离函数（SDF）：通过增强的隐式几何正则化方法，从点云数据中学习神经SDF，并采用过近似策略创建保守的安全边界。2. 两层分层安全鲁棒控制框架：\n    - 第一层（安全速度生成）：构建二次锥规划（SOCP），显式纳入欠近似误差界，并引入循环不等式以缓解控制障碍函数（CBF）方法的局部最小值问题。\n    - 第二层（安全鲁棒控制器）：集成扰动观测器和平滑安全滤波器，补偿估计误差，增强对外部扰动的鲁棒性。", "result": "广泛的数值模拟和蒙特卡洛分析验证了所提框架，结果表明与传统CBF方法相比，该框架显著提高了安全裕度，并有效避免了局部最小值问题。", "conclusion": "所提出的基于隐式神经表示和分层控制的安全鲁棒控制框架，能够有效应对复杂几何目标航天器在扰动下的碰撞避免挑战，显著提升了操作安全性与鲁棒性。"}}
{"id": "2507.13458", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13458", "abs": "https://arxiv.org/abs/2507.13458", "authors": ["Malte Hoffmann"], "title": "Domain-randomized deep learning for neuroimage analysis", "comment": "12 pages, 6 figures, 2 tables, deep learning, domain generalization,\n  domain randomization, neuroimaging, medical image analysis, accepted for\n  publication in IEEE Signal Processing Magazine", "summary": "Deep learning has revolutionized neuroimage analysis by delivering\nunprecedented speed and accuracy. However, the narrow scope of many training\ndatasets constrains model robustness and generalizability. This challenge is\nparticularly acute in magnetic resonance imaging (MRI), where image appearance\nvaries widely across pulse sequences and scanner hardware. A recent\ndomain-randomization strategy addresses the generalization problem by training\ndeep neural networks on synthetic images with randomized intensities and\nanatomical content. By generating diverse data from anatomical segmentation\nmaps, the approach enables models to accurately process image types unseen\nduring training, without retraining or fine-tuning. It has demonstrated\neffectiveness across modalities including MRI, computed tomography, positron\nemission tomography, and optical coherence tomography, as well as beyond\nneuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray\nmicrotomography. This tutorial paper reviews the principles, implementation,\nand potential of the synthesis-driven training paradigm. It highlights key\nbenefits, such as improved generalization and resistance to overfitting, while\ndiscussing trade-offs such as increased computational demands. Finally, the\narticle explores practical considerations for adopting the technique, aiming to\naccelerate the development of generalizable tools that make deep learning more\naccessible to domain experts without extensive computational resources or\nmachine learning knowledge.", "AI": {"tldr": "本文综述了一种基于合成图像的域随机化训练范式，旨在解决深度学习模型在神经影像分析中泛化能力差的问题，尤其是在MRI等模态中，以提高模型鲁棒性和可访问性。", "motivation": "深度学习在神经影像分析中表现出色，但训练数据集范围狭窄限制了模型的鲁棒性和泛化能力。特别是在MRI中，图像外观因序列和硬件差异大，导致模型难以泛化到未见过的数据类型。", "method": "采用域随机化策略，通过基于解剖分割图生成具有随机强度和解剖内容的合成图像来训练深度神经网络。本文作为一篇教程，回顾了这种合成驱动训练范式的原理、实现和潜力。", "result": "该方法使模型能够准确处理训练期间未见过的图像类型，无需重新训练或微调。已在MRI、CT、PET、OCT等多种医学影像模态以及超声、电子显微镜等非神经影像领域中验证其有效性，显著提升了泛化能力并增强了抗过拟合性。", "conclusion": "合成驱动的训练范式能够加速通用工具的开发，使深度学习更易于没有大量计算资源或机器学习知识的领域专家使用。文章探讨了采用该技术的实际考量，并讨论了计算需求增加等权衡。"}}
{"id": "2507.13539", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13539", "abs": "https://arxiv.org/abs/2507.13539", "authors": ["Jim O'Connor", "Jay B. Nash", "Derin Gezgin", "Gary B. Parker"], "title": "SCOPE for Hexapod Gait Generation", "comment": "IJCCI Conference on Evolutionary Computation and Theory and\n  Applications, 2025", "summary": "Evolutionary methods have previously been shown to be an effective learning\nmethod for walking gaits on hexapod robots. However, the ability of these\nalgorithms to evolve an effective policy rapidly degrades as the input space\nbecomes more complex. This degradation is due to the exponential growth of the\nsolution space, resulting from an increasing parameter count to handle a more\ncomplex input. In order to address this challenge, we introduce Sparse Cosine\nOptimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine\nTransform (DCT) to learn directly from the feature coefficients of an input\nmatrix. By truncating the coefficient matrix returned by the DCT, we can reduce\nthe dimensionality of an input while retaining the highest energy features of\nthe original input. We demonstrate the effectiveness of this method by using\nSCOPE to learn the gait of a hexapod robot. The hexapod controller is given a\nmatrix input containing time-series information of previous poses, which are\nthen transformed to gait parameters by an evolved policy. In this task, the\naddition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.\nSCOPE achieves this result by reducing the total input size of the time-series\npose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of\ncompressing an input to any output shape, provided that each output dimension\nis no greater than the corresponding input dimension. This paper demonstrates\nthat SCOPE is capable of significantly compressing the size of an input to an\nevolved controller, resulting in a statistically significant gain in efficacy.", "AI": {"tldr": "针对六足机器人步态学习中进化算法在复杂输入空间下性能下降的问题，本文提出了SCOPE方法，利用离散余弦变换（DCT）有效降低输入维度，显著提升了学习效率。", "motivation": "在六足机器人步态学习中，当输入空间变得复杂时，现有进化算法的性能会迅速下降。这主要是由于解决方案空间的指数级增长，以及处理复杂输入所需的参数数量增加所致。", "method": "本文引入了稀疏余弦优化策略进化（SCOPE）方法。SCOPE利用离散余弦变换（DCT）直接从输入矩阵的特征系数中学习，通过截断系数矩阵来降低输入维度，同时保留原始输入中能量最高的特征。该方法被应用于学习六足机器人的步态，控制器接收包含先前姿态时间序列信息的矩阵输入，并通过进化的策略将其转换为步态参数。", "result": "将SCOPE添加到参考算法中，在六足机器人步态学习任务中实现了20%的效率提升。SCOPE将时间序列姿态数据的总输入大小从2700减少到54，降低了98%。此外，SCOPE能够将输入压缩到任何输出形状，只要每个输出维度不大于相应的输入维度。", "conclusion": "SCOPE方法能够显著压缩进化控制器所需的输入尺寸，从而带来统计学上显著的效率提升。"}}
{"id": "2507.13361", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13361", "abs": "https://arxiv.org/abs/2507.13361", "authors": ["Shmuel Berman", "Jia Deng"], "title": "VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs", "comment": null, "summary": "Visual Language Models (VLMs) excel at complex visual tasks such as VQA and\nchart understanding, yet recent work suggests they struggle with simple\nperceptual tests. We present an evaluation that tests vision-language models'\ncapacity for nonlocal visual reasoning -- reasoning that requires chaining\nevidence collected from multiple, possibly distant, regions of an image. We\nisolate three distinct forms of non-local vision: comparative perception, which\ndemands holding two images in working memory and comparing them; saccadic\nsearch, which requires making discrete, evidence-driven jumps to locate\nsuccessive targets; and smooth visual search, which involves searching smoothly\nalong a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude\nVision 3.7, GPT-o4-mini), even those that perform well on prior\nprimitive-vision benchmarks, fail these tests and barely exceed random accuracy\non two variants of our tasks that are trivial for humans. Our structured\nevaluation suite allows us to test if VLMs can perform similar visual\nalgorithms to humans. Our findings show that despite gains in raw visual\nacuity, current models lack core visual reasoning capabilities.", "AI": {"tldr": "视觉语言模型（VLMs）在需要整合来自图像多个区域信息的非局部视觉推理方面表现不佳，即使是顶级模型也难以通过人类认为简单的测试。", "motivation": "尽管VLMs在VQA和图表理解等复杂视觉任务上表现出色，但近期研究表明它们在简单的感知测试上存在困难。本研究旨在评估VLMs进行非局部视觉推理的能力，即需要链式整合来自图像多个（可能是远距离）区域证据的推理。", "method": "本研究评估了VLMs的非局部视觉推理能力，并将其细分为三种形式：比较感知（需要比较两张图像）、眼跳式搜索（需要根据证据离散跳跃寻找目标）和平滑视觉搜索（需要沿连续轮廓平滑搜索）。测试对象是主流旗舰模型，如Gemini 2.5 Pro、Claude Vision 3.7和GPT-o4-mini。", "result": "即使在先前的原始视觉基准测试中表现良好的旗舰模型，在这些非局部视觉推理测试中也表现失败，在两个对人类来说微不足道的任务变体上，其准确率仅略高于随机水平。研究发现，尽管模型在原始视觉敏锐度上有所提升，但它们缺乏核心的视觉推理能力。", "conclusion": "当前VLMs，尽管在原始视觉敏锐度方面有所进步，但仍缺乏核心的视觉推理能力，特别是进行非局部视觉推理的能力，这表明它们无法像人类那样执行类似的视觉算法。"}}
{"id": "2507.13381", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13381", "abs": "https://arxiv.org/abs/2507.13381", "authors": ["Rafiq Kamel", "Filippo Guerranti", "Simon Geisler", "Stephan Günnemann"], "title": "SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation", "comment": "Accepted at the KDD2025 Workshop on Structured Knowledge for LLMs", "summary": "Large Language Models (LLMs) are increasingly applied to tasks involving\nstructured inputs such as graphs. Abstract Meaning Representations (AMRs),\nwhich encode rich semantics as directed graphs, offer a rigorous testbed for\nevaluating LLMs on text generation from such structures. Yet, current methods\noften arbitrarily linearize AMRs, discarding key structural cues, or rely on\narchitectures incompatible with standard LLMs. We introduce SAFT, a\nstructure-aware fine-tuning approach that injects graph topology into\npretrained LLMs without architectural changes. We compute direction-sensitive\npositional encodings from the magnetic Laplacian of transformed AMRs and\nproject them into the embedding space of the LLM. While possibly applicable to\nany graph-structured inputs, we focus on AMR-to-text generation as a\nrepresentative and challenging benchmark. SAFT sets a new state-of-the-art on\nAMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph\ncomplexity, highlighting the value of structure-aware representations in\nenhancing LLM performance. SAFT offers a general and effective pathway for\nbridging structured data and language models.", "AI": {"tldr": "SAFT是一种结构感知微调方法，通过注入图拓扑信息，显著提升了大型语言模型在AMR到文本生成任务上的性能，并达到了新的SOTA。", "motivation": "现有方法在处理图等结构化输入时，要么随意线性化AMR导致结构信息丢失，要么依赖与标准LLM不兼容的架构。因此，需要一种在不改变LLM架构的情况下，有效利用图结构信息的方法。", "method": "SAFT方法通过计算转换后的AMR的磁拉普拉斯算子（magnetic Laplacian）得到方向敏感的位置编码，并将其投影到LLM的嵌入空间中。这是一种结构感知的微调方法，无需改变LLM的内部架构。", "result": "SAFT在AMR 3.0数据集上取得了新的最先进成果，相较于基线模型，BLEU分数提升了3.5。性能提升随着图复杂度的增加而更为显著，这凸显了结构感知表示在提升LLM性能方面的价值。", "conclusion": "SAFT提供了一种通用且有效的方法，用于连接结构化数据（特别是图）和语言模型，通过利用结构感知表示来增强LLM的性能。"}}
{"id": "2507.13652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13652", "abs": "https://arxiv.org/abs/2507.13652", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Combining model tracing and constraint-based modeling for multistep strategy diagnoses", "comment": null, "summary": "Model tracing and constraint-based modeling are two approaches to diagnose\nstudent input in stepwise tasks. Model tracing supports identifying consecutive\nproblem-solving steps taken by a student, whereas constraint-based modeling\nsupports student input diagnosis even when several steps are combined into one\nstep. We propose an approach that merges both paradigms. By defining\nconstraints as properties that a student input has in common with a step of a\nstrategy, it is possible to provide a diagnosis when a student deviates from a\nstrategy even when the student combines several steps. In this study we explore\nthe design of a system for multistep strategy diagnoses, and evaluate these\ndiagnoses. As a proof of concept, we generate diagnoses for an existing dataset\ncontaining steps students take when solving quadratic equations (n=2136). To\ncompare with human diagnoses, two teachers coded a random sample of deviations\n(n=70) and applications of the strategy (n=70). Results show that that the\nsystem diagnosis aligned with the teacher coding in all of the 140 student\nsteps.", "AI": {"tldr": "本文提出一种结合模型追踪和基于约束建模的方法，用于诊断学生在多步骤任务中的输入，即使学生合并了多个步骤。该系统在诊断二次方程求解步骤时，与教师的编码高度一致。", "motivation": "模型追踪和基于约束建模是诊断学生分步任务输入的两种主要方法。模型追踪适用于连续的解题步骤，而基于约束建模能处理合并步骤。然而，需要一种方法，即使学生合并了多个步骤，也能诊断其偏离策略的情况。", "method": "本文提出一种融合两种范式的方法，通过将约束定义为学生输入与策略步骤共有的属性。设计了一个用于多步骤策略诊断的系统。通过对现有数据集（包含学生求解二次方程的步骤，n=2136）生成诊断来验证概念。将系统诊断与两位教师对随机样本（偏差70例，策略应用70例）的手动编码进行比较。", "result": "结果显示，系统诊断与教师对所有140个学生步骤的编码完全一致。", "conclusion": "所提出的融合方法能够有效地诊断学生在多步骤任务中的输入，即使学生合并了多个步骤，并且诊断结果与人类教师的判断高度吻合，证明了其有效性。"}}
{"id": "2507.13678", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13678", "abs": "https://arxiv.org/abs/2507.13678", "authors": ["Honghao Wu", "Kemi Ding", "Li Qiu"], "title": "Minimum Clustering of Matrices Based on Phase Alignment", "comment": "This work has been received by CDC2025", "summary": "Coordinating multi-agent systems requires balancing synchronization\nperformance and controller implementation costs. To this end, we classify\nagents by their intrinsic properties, enabling each group to be controlled by a\nuniform controller and thus reducing the number of unique controller types\nrequired. Existing centralized control methods, despite their capability to\nachieve high synchronization performance with fewer types of controllers,\nsuffer from critical drawbacks such as limited scalability and vulnerability to\nsingle points of failure. On the other hand, distributed control strategies,\nwhere controllers are typically agent-dependent, result in the type of required\ncontrollers increasing proportionally with the size of the system.\n  This paper introduces a novel phase-alignment-based framework to minimize the\ntype of controllers by strategically clustering agents with aligned\nsynchronization behaviors. Leveraging the intrinsic phase properties of complex\nmatrices, we formulate a constrained clustering problem and propose a\nhierarchical optimization method combining recursive exact searches for\nsmall-scale systems and scalable stochastic approximations for large-scale\nnetworks. This work bridges theoretical phase analysis with practical control\nsynthesis, offering a cost-effective solution for large-scale multi-agent\nsystems. The theoretical results applied for the analysis of a 50-agent network\nillustrate the effectiveness of the proposed algorithms.", "AI": {"tldr": "本文提出了一种基于相位对齐的框架，通过策略性地聚类具有相似同步行为的智能体，以最小化多智能体系统中所需控制器类型，同时平衡同步性能和实现成本。", "motivation": "多智能体系统需要平衡同步性能和控制器实现成本。现有集中式控制方法虽然能用更少控制器类型实现高性能，但扩展性差且存在单点故障；分布式控制策略则导致所需控制器类型随系统规模增加。因此，需要一种方法来减少所需控制器类型，以降低大规模多智能体系统的实现成本。", "method": "通过智能体的内在属性进行分类，引入了一种新颖的基于相位对齐的框架。该框架利用复矩阵的内在相位特性，将问题表述为受约束的聚类问题。提出了一种分层优化方法，结合了针对小规模系统的递归精确搜索和针对大规模网络的可扩展随机逼近。", "result": "该工作为大规模多智能体系统提供了一种经济高效的解决方案。理论结果应用于一个50智能体网络的分析，证明了所提出算法的有效性。", "conclusion": "该研究通过连接理论相位分析与实际控制综合，提出了一种有效的方法来最小化多智能体系统中的控制器类型，实现了成本效益和高性能的平衡，尤其适用于大规模系统。"}}
{"id": "2507.13604", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13604", "abs": "https://arxiv.org/abs/2507.13604", "authors": ["Qihang Li", "Jichen Yang", "Yaqian Chen", "Yuwen Chen", "Hanxue Gu", "Lars J. Grimm", "Maciej A. Mazurowski"], "title": "BreastSegNet: Multi-label Segmentation of Breast MRI", "comment": null, "summary": "Breast MRI provides high-resolution imaging critical for breast cancer\nscreening and preoperative staging. However, existing segmentation methods for\nbreast MRI remain limited in scope, often focusing on only a few anatomical\nstructures, such as fibroglandular tissue or tumors, and do not cover the full\nrange of tissues seen in scans. This narrows their utility for quantitative\nanalysis. In this study, we present BreastSegNet, a multi-label segmentation\nalgorithm for breast MRI that covers nine anatomical labels: fibroglandular\ntissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and\nimplant. We manually annotated a large set of 1123 MRI slices capturing these\nstructures with detailed review and correction from an expert radiologist.\nAdditionally, we benchmark nine segmentation models, including U-Net, SwinUNet,\nUNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among\nthem, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across\nall labels. It performs especially well on heart, liver, muscle, FGT, and bone,\nwith Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All\nmodel code and weights are publicly available, and we plan to release the data\nat a later date.", "AI": {"tldr": "本研究提出了BreastSegNet，一种针对乳腺MRI的多标签分割算法，涵盖9种解剖结构，并对多个分割模型进行了基准测试，其中nnU-Net ResEncM表现最佳。", "motivation": "现有的乳腺MRI分割方法范围有限，通常只关注少数组织结构（如纤维腺体组织或肿瘤），未能覆盖扫描中可见的全部组织，从而限制了其在定量分析中的效用。", "method": "开发了BreastSegNet多标签分割算法，涵盖纤维腺体组织、血管、肌肉、骨骼、病变、淋巴结、心脏、肝脏和植入物共九种解剖标签。手动标注了1123张MRI切片，并由专家放射科医生进行详细审查和校正。同时，对包括U-Net、SwinUNet、UNet++、SAM、MedSAM和带有多个ResNet编码器的nnU-Net在内的九种分割模型进行了基准测试。", "result": "在所有标签中，nnU-Net ResEncM实现了最高的平均Dice分数0.694。它在心脏、肝脏、肌肉、纤维腺体组织和骨骼上的表现尤为出色，Dice分数超过0.73，心脏和肝脏接近0.90。", "conclusion": "BreastSegNet为乳腺MRI提供了全面的多标签分割解决方案，nnU-Net ResEncM展现出最佳性能，有望增强乳腺MRI的定量分析能力。"}}
{"id": "2507.13602", "categories": ["cs.RO", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13602", "abs": "https://arxiv.org/abs/2507.13602", "authors": ["Shivakanth Sujit", "Luca Nunziante", "Dan Ogawa Lillrank", "Rousslan Fernand Julien Dossa", "Kai Arulkumaran"], "title": "Improving Low-Cost Teleoperation: Augmenting GELLO with Force", "comment": "Accepted at the 2025 IEEE/SICE International Symposium on System\n  Integration", "summary": "In this work we extend the low-cost GELLO teleoperation system, initially\ndesigned for joint position control, with additional force information. Our\nfirst extension is to implement force feedback, allowing users to feel\nresistance when interacting with the environment. Our second extension is to\nadd force information into the data collection process and training of\nimitation learning models. We validate our additions by implementing these on a\nGELLO system with a Franka Panda arm as the follower robot, performing a user\nstudy, and comparing the performance of policies trained with and without force\ninformation on a range of simulated and real dexterous manipulation tasks.\nQualitatively, users with robotics experience preferred our controller, and the\naddition of force inputs improved task success on the majority of tasks.", "AI": {"tldr": "该研究扩展了低成本GELLO遥操作系统，增加了力反馈功能，并将力信息整合到模仿学习中，以提升灵巧操作任务的性能和用户体验。", "motivation": "原有的GELLO遥操作系统仅支持关节位置控制，缺乏力信息，限制了其在需要环境交互和精细操作任务中的表现。研究旨在通过引入力信息来增强系统能力。", "method": "1. 实现了力反馈功能，让用户能感知环境阻力。2. 将力信息纳入数据收集和模仿学习模型的训练过程。3. 在搭载Franka Panda机械臂的GELLO系统上进行验证。4. 进行了用户研究。5. 在模拟和真实的灵巧操作任务中，比较了有无力信息训练的策略性能。", "result": "1. 具有机器人经验的用户定性上更偏好新的控制器。2. 增加力输入显著提高了大多数任务的成功率。", "conclusion": "为GELLO遥操作系统增加力信息（包括力反馈和用于模仿学习）能够有效提升系统在灵巧操作任务中的性能，并改善用户体验。"}}
{"id": "2507.13362", "categories": ["cs.CV", "cs.AI", "cs.CL", "I.2.10; I.4.8; I.2.6; I.2.7; I.5.4; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.13362", "abs": "https://arxiv.org/abs/2507.13362", "authors": ["Binbin Ji", "Siddharth Agrawal", "Qiance Tang", "Yvonne Wu"], "title": "Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning", "comment": "10 pages, 5 figures, submitted to a conference (IEEE formate).\n  Authored by students from the Courant Institute, NYU", "summary": "This study investigates the spatial reasoning capabilities of vision-language\nmodels (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement\nlearning. We begin by evaluating the impact of different prompting strategies\nand find that simple CoT formats, where the model generates a reasoning step\nbefore the answer, not only fail to help, but can even harm the model's\noriginal performance. In contrast, structured multi-stage prompting based on\nscene graphs (SceneGraph CoT) significantly improves spatial reasoning\naccuracy. Furthermore, to improve spatial reasoning ability, we fine-tune\nmodels using Group Relative Policy Optimization (GRPO) on the SAT dataset and\nevaluate their performance on CVBench. Compared to supervised fine-tuning\n(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates\nsuperior robustness under out-of-distribution (OOD) conditions. In particular,\nwe find that SFT overfits to surface-level linguistic patterns and may degrade\nperformance when test-time phrasing changes (e.g., from \"closer to\" to \"farther\nfrom\"). GRPO, on the other hand, generalizes more reliably and maintains stable\nperformance under such shifts. Our findings provide insights into how\nreinforcement learning and structured prompting improve the spatial reasoning\ncapabilities and generalization behavior of modern VLMs. All code is open\nsource at: https://github.com/Yvonne511/spatial-vlm-investigator", "AI": {"tldr": "本研究通过链式思考(CoT)提示和强化学习(RL)提升视觉-语言模型(VLM)的空间推理能力。发现简单的CoT无效甚至有害，而基于场景图的结构化CoT显著提高准确性。此外，使用GRPO进行RL微调比监督微调(SFT)在准确性和OOD鲁棒性方面表现更优，SFT易过拟合语言模式。", "motivation": "旨在深入探究并提升视觉-语言模型(VLM)的空间推理能力，并解决现有CoT提示的局限性以及探索强化学习在提高模型泛化性方面的潜力。", "method": "1. 评估不同的CoT提示策略，包括简单CoT和基于场景图的结构化多阶段提示(SceneGraph CoT)。2. 使用Group Relative Policy Optimization (GRPO)在SAT数据集上对模型进行微调。3. 将GRPO与监督微调(SFT)进行对比。4. 在CVBench和出域(OOD)条件下评估模型性能，特别关注对语言模式变化的鲁棒性。", "result": "1. 简单的CoT提示不仅未能帮助，反而可能损害VLM的原始空间推理性能。2. 基于场景图的结构化多阶段提示(SceneGraph CoT)显著提高了空间推理准确性。3. 与SFT相比，GRPO在Pass@1评估中取得了更高的准确性，并在OOD条件下展现出卓越的鲁棒性。4. SFT容易过拟合表层语言模式，当测试时措辞变化时性能会下降（例如，从“更靠近”到“更远离”）。5. GRPO的泛化能力更可靠，在此类变化下仍能保持稳定性能。", "conclusion": "强化学习（特别是GRPO）和结构化提示（如SceneGraph CoT）能够有效提高现代视觉-语言模型的空间推理能力和泛化行为。SFT在泛化性上存在局限性，易受表层语言模式过拟合的影响。"}}
{"id": "2507.13382", "categories": ["cs.CL", "cs.LG", "05-05C12"], "pdf": "https://arxiv.org/pdf/2507.13382", "abs": "https://arxiv.org/abs/2507.13382", "authors": ["Chandrashekar Muniyappa", "Sirisha Velampalli"], "title": "Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "In today\\'s digital world, fake news is spreading with immense speed. Its a\nsignificant concern to address. In this work, we addressed that challenge using\nnovel graph based approach. We took dataset from Kaggle that contains real and\nfake news articles. To test our approach we incorporated recent covid-19\nrelated news articles that contains both genuine and fake news that are\nrelevant to this problem. This further enhances the dataset as well instead of\nrelying completely on the original dataset. We propose a contextual graph-based\napproach to detect fake news articles. We need to convert news articles into\nappropriate schema, so we leverage Natural Language Processing (NLP) techniques\nto transform news articles into contextual graph structures. We then apply the\nMinimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)\nalgorithm for graph mining. Graph-based methods are particularly effective for\nhandling rich contextual data, as they enable the discovery of complex patterns\nthat traditional query-based or statistical techniques might overlook. Our\nproposed approach identifies normative patterns within the dataset and\nsubsequently uncovers anomalous patterns that deviate from these established\nnorms.", "AI": {"tldr": "本文提出了一种基于上下文图的方法，利用NLP技术将新闻文章转换为图结构，并结合MDL-GBAD算法进行图挖掘，以检测虚假新闻。", "motivation": "虚假新闻在数字世界中传播迅速，是一个亟待解决的重大问题。", "method": "研究方法包括：1) 结合Kaggle数据集和近期COVID-19新闻文章；2) 利用自然语言处理（NLP）技术将新闻文章转换为上下文图结构；3) 应用基于最小描述长度（MDL）的图基异常检测（GBAD）算法进行图挖掘；4) 识别数据集中的规范模式，并发现偏离这些规范的异常模式。", "result": "该方法能够识别数据集中的规范模式，并随后揭示偏离这些既定规范的异常模式（即虚假新闻）。", "conclusion": "图基方法在处理丰富的上下文数据方面特别有效，能够发现传统方法可能忽略的复杂模式，从而有效检测虚假新闻。"}}
{"id": "2507.13737", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13737", "abs": "https://arxiv.org/abs/2507.13737", "authors": ["Ye Tian", "Xiaoyuan Ren", "Zihao Wang", "Onat Gungor", "Xiaofan Yu", "Tajana Rosing"], "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs", "comment": null, "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed.", "AI": {"tldr": "DailyLLM是一个利用智能手机和智能手表传感器数据，全面整合四维上下文信息（位置、运动、环境、生理）的轻量级LLM系统，用于生成和总结丰富的活动日志，显著提升了准确性和效率。", "motivation": "现有的活动日志生成方法在准确性、效率和语义丰富性方面存在显著局限性，尽管大型语言模型（LLMs）为活动日志生成带来了新机遇。", "method": "本文提出了DailyLLM系统，首次全面整合了来自智能手机和智能手表常用传感器的四维上下文活动信息（位置、运动、环境、生理）。它引入了一个基于LLM的轻量级框架，结合结构化提示和高效特征提取，以实现高级活动理解。", "result": "DailyLLM在日志生成方面优于最先进（SOTA）的方法，并能高效部署在个人电脑和树莓派上。使用仅1.5B参数的LLM模型，DailyLLM在日志生成BERTScore精度上比70B参数的SOTA基线提高了17%，同时推理速度快了近10倍。", "conclusion": "DailyLLM有效解决了活动日志生成中的准确性、效率和语义丰富性挑战，通过整合多维度上下文信息和轻量级LLM框架，实现了卓越的性能提升和部署效率。"}}
{"id": "2507.13687", "categories": ["eess.SY", "cs.SY", "93C95, 93E35, 93E20", "H.4.1"], "pdf": "https://arxiv.org/pdf/2507.13687", "abs": "https://arxiv.org/abs/2507.13687", "authors": ["Ming Lei", "Shufan Wu"], "title": "Robust Probability Hypothesis Density Filtering: Theory and Algorithms", "comment": "This version is submitted and in review currently", "summary": "Multi-target tracking (MTT) serves as a cornerstone technology in information\nfusion, yet faces significant challenges in robustness and efficiency when\ndealing with model uncertainties, clutter interference, and target\ninteractions. Conventional approaches like Gaussian Mixture PHD (GM-PHD) and\nCardinalized PHD (CPHD) filters suffer from inherent limitations including\ncombinatorial explosion, sensitivity to birth/death process parameters, and\nnumerical instability. This study proposes an innovative minimax robust PHD\nfiltering framework with four key contributions: (1) A theoretically derived\nrobust GM-PHD recursion algorithm that achieves optimal worst-case error\ncontrol under bounded uncertainties; (2) An adaptive real-time parameter\nadjustment mechanism ensuring stability and error bounds; (3) A generalized\nheavy-tailed measurement likelihood function maintaining polynomial\ncomputational complexity; (4) A novel partition-based credibility weighting\nmethod for extended targets. The research not only establishes rigorous\nconvergence guarantees and proves the uniqueness of PHD solutions, but also\nverifies algorithmic equivalence with standard GM-PHD. Experimental results\ndemonstrate that in high-clutter environments, this method achieves a\nremarkable 32.4% reduction in OSPA error and 25.3% lower cardinality RMSE\ncompared to existing techniques, while maintaining real-time processing\ncapability at 15.3 milliseconds per step. This breakthrough lays a crucial\nfoundation for reliable MTT in safety-critical applications.", "AI": {"tldr": "本研究提出了一种创新的极小极大鲁棒PHD滤波框架，旨在解决多目标跟踪（MTT）在模型不确定性、杂波干扰和目标交互下的鲁棒性和效率挑战，并在高杂波环境中显著降低了跟踪误差并保持了实时性。", "motivation": "传统的多目标跟踪方法（如GM-PHD和CPHD滤波器）存在固有限制，包括组合爆炸、对目标生灭过程参数敏感以及数值不稳定等问题，难以有效应对模型不确定性、杂波干扰和目标交互。", "method": "该研究提出了一个创新的极小极大鲁棒PHD滤波框架，包括：1) 理论推导的鲁棒GM-PHD递归算法，实现有界不确定性下的最优最坏情况误差控制；2) 自适应实时参数调整机制，确保稳定性和误差界限；3) 广义重尾测量似然函数，保持多项式计算复杂度；4) 针对扩展目标的新型基于划分的置信度加权方法。研究还建立了严格的收敛性保证，证明了PHD解的唯一性，并验证了算法与标准GM-PHD的等效性。", "result": "实验结果表明，在高杂波环境中，该方法相比现有技术，OSPA误差降低了32.4%，基数RMSE降低了25.3%，同时保持了每步15.3毫秒的实时处理能力。", "conclusion": "这项突破为安全关键应用中可靠的多目标跟踪奠定了关键基础。"}}
{"id": "2507.13782", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13782", "abs": "https://arxiv.org/abs/2507.13782", "authors": ["Malo Gicquel", "Ruoyi Zhao", "Anika Wuestefeld", "Nicola Spotorno", "Olof Strandberg", "Kalle Åström", "Yu Xiao", "Laura EM Wisse", "Danielle van Westen", "Rik Ossenkoppele", "Niklas Mattsson-Carlgren", "David Berron", "Oskar Hansson", "Gabrielle Flood", "Jacob Vogel"], "title": "Converting T1-weighted MRI from 3T to 7T quality using deep learning", "comment": null, "summary": "Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides\ndetailed anatomical views, offering better signal-to-noise ratio, resolution\nand tissue contrast than 3T MRI, though at the cost of accessibility. We\npresent an advanced deep learning model for synthesizing 7T brain MRI from 3T\nbrain MRI. Paired 7T and 3T T1-weighted images were acquired from 172\nparticipants (124 cognitively unimpaired, 48 impaired) from the Swedish\nBioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:\na specialized U-Net, and a U-Net integrated with a generative adversarial\nnetwork (GAN U-Net). Our models outperformed two additional state-of-the-art\n3T-to-7T models in image-based evaluation metrics. Four blinded MRI\nprofessionals judged our synthetic 7T images as comparable in detail to real 7T\nimages, and superior in subjective visual quality to 7T images, apparently due\nto the reduction of artifacts. Importantly, automated segmentations of the\namygdalae of synthetic GAN U-Net 7T images were more similar to manually\nsegmented amygdalae (n=20), than automated segmentations from the 3T images\nthat were used to synthesize the 7T images. Finally, synthetic 7T images showed\nsimilar performance to real 3T images in downstream prediction of cognitive\nstatus using MRI derivatives (n=3,168). In all, we show that synthetic\nT1-weighted brain images approaching 7T quality can be generated from 3T\nimages, which may improve image quality and segmentation, without compromising\nperformance in downstream tasks. Future directions, possible clinical use\ncases, and limitations are discussed.", "AI": {"tldr": "该研究提出了一种深度学习模型，能将3T脑部MRI图像合成为接近7T质量的图像，从而在不牺牲下游任务性能的情况下提升图像质量和分割精度。", "motivation": "7T MRI虽然提供更高的信噪比、分辨率和组织对比度，但其可及性远低于3T MRI。研究旨在通过技术手段，在3T MRI的普及性基础上，获得接近7T MRI的详细解剖视图和图像质量。", "method": "研究使用了来自瑞典BioFINDER-2研究的172名参与者的配对7T和3T T1加权脑部MRI图像。训练了两种深度学习模型来合成7T MRI：一个专门的U-Net模型和一个集成生成对抗网络（GAN）的U-Net模型（GAN U-Net）。通过图像度量指标、四位盲审MRI专业人士的主观评估、自动分割结果与手动分割的相似性比较，以及在认知状态预测下游任务中的表现来评估模型。", "result": "研究开发的模型在图像评估指标上优于其他两种先进的3T到7T合成模型。盲审MRI专业人士认为合成的7T图像在细节上可与真实7T图像媲美，且在主观视觉质量上更优（可能由于伪影减少）。重要的是，合成的GAN U-Net 7T图像的杏仁核自动分割结果比原始3T图像的分割结果更接近手动分割。此外，合成的7T图像在认知状态预测的下游任务中表现与真实3T图像相似。", "conclusion": "该研究表明，可以从3T图像生成接近7T质量的合成T1加权脑部图像。这种方法能够提高图像质量和分割精度，同时不影响下游任务的性能，为未来的临床应用提供了可能性。"}}
{"id": "2507.13647", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13647", "abs": "https://arxiv.org/abs/2507.13647", "authors": ["Minze Li", "Wei Zhao", "Ran Chen", "Mingqiang Wei"], "title": "Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones", "comment": "8 papers,7 figures", "summary": "Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic\nenvironments remains a key challenge due to high computational demands and the\nneed for fast, adaptive responses. Traditional Particle Swarm Optimization\n(PSO) methods, while effective for offline planning, often struggle with\npremature convergence and latency in real-time scenarios. To overcome these\nlimitations, we propose PE-PSO, an enhanced PSO-based online trajectory\nplanner. The method introduces a persistent exploration mechanism to preserve\nswarm diversity and an entropy-based parameter adjustment strategy to\ndynamically adapt optimization behavior. UAV trajectories are modeled using\nB-spline curves, which ensure path smoothness while reducing optimization\ncomplexity. To extend this capability to UAV swarms, we develop a multi-agent\nframework that combines genetic algorithm (GA)-based task allocation with\ndistributed PE-PSO, supporting scalable and coordinated trajectory generation.\nThe distributed architecture allows for parallel computation and decentralized\ncontrol, enabling effective cooperation among agents while maintaining\nreal-time performance. Comprehensive simulations demonstrate that the proposed\nframework outperforms conventional PSO and other swarm-based planners across\nseveral metrics, including trajectory quality, energy efficiency, obstacle\navoidance, and computation time. These results confirm the effectiveness and\napplicability of PE-PSO in real-time multi-UAV operations under complex\nenvironmental conditions.", "AI": {"tldr": "本文提出了一种名为PE-PSO的增强型粒子群优化算法，用于无人机实时轨迹规划，并通过结合遗传算法和分布式PE-PSO，扩展到多无人机蜂群的协同轨迹生成，在复杂动态环境中表现出卓越的性能。", "motivation": "在动态环境中，无人机实时轨迹规划面临计算量大、响应速度要求高以及传统粒子群优化（PSO）方法易早熟收敛和延迟等挑战。", "method": "本文提出PE-PSO算法，通过引入持久探索机制保持种群多样性，并采用基于熵的参数调整策略动态适应优化行为。无人机轨迹采用B-样条曲线建模以确保平滑性并降低优化复杂度。对于无人机蜂群，开发了一个多智能体框架，结合了基于遗传算法（GA）的任务分配和分布式PE-PSO，实现可扩展和协调的轨迹生成，并采用分布式架构支持并行计算和去中心化控制。", "result": "综合仿真结果表明，所提出的框架在轨迹质量、能量效率、避障能力和计算时间等方面均优于传统的PSO及其他基于群体的规划器。", "conclusion": "PE-PSO及其多智能体框架在复杂环境下的实时多无人机操作中是有效且适用的。"}}
{"id": "2507.13363", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13363", "abs": "https://arxiv.org/abs/2507.13363", "authors": ["Atharv Goel", "Mehar Khurana"], "title": "Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop", "comment": null, "summary": "Modern 3D object detection datasets are constrained by narrow class\ntaxonomies and costly manual annotations, limiting their ability to scale to\nopen-world settings. In contrast, 2D vision-language models trained on\nweb-scale image-text pairs exhibit rich semantic understanding and support\nopen-vocabulary detection via natural language prompts. In this work, we\nleverage the maturity and category diversity of 2D foundation models to perform\nopen-vocabulary 3D object detection without any human-annotated 3D labels.\n  Our pipeline uses a 2D vision-language detector to generate text-conditioned\nproposals, which are segmented with SAM and back-projected into 3D using camera\ngeometry and either LiDAR or monocular pseudo-depth. We introduce a geometric\ninflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D\nbounding boxes without training. To simulate adverse real-world conditions, we\nconstruct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes\ndataset.\n  Experiments demonstrate that our method achieves competitive localization\nperformance across multiple settings, including LiDAR-based and purely RGB-D\ninputs, all while remaining training-free and open-vocabulary. Our results\nhighlight the untapped potential of 2D foundation models for scalable 3D\nperception. We open-source our code and resources at\nhttps://github.com/atharv0goel/open-world-3D-det.", "AI": {"tldr": "该研究利用2D视觉-语言模型实现零样本、开放词汇的3D目标检测，无需任何人工标注的3D标签，并通过几何膨胀策略生成3D边界框。", "motivation": "现有3D目标检测数据集受限于狭窄的类别分类和昂贵的手动标注，难以扩展到开放世界场景。而2D视觉-语言模型（VLM）在网络规模的图像-文本对上训练，展现出丰富的语义理解能力，并支持通过自然语言提示进行开放词汇检测。", "method": "该方法利用2D视觉-语言检测器生成文本条件下的2D候选框，然后使用SAM进行分割，并通过相机几何和LiDAR或单目伪深度将其反投影到3D空间。为推断3D边界框，引入了一种基于DBSCAN聚类和旋转卡尺（Rotating Calipers）的几何膨胀策略，整个过程无需训练。为模拟恶劣现实条件，构建了Pseudo-nuScenes数据集（nuScenes的雾增强、纯RGB变体）。", "result": "实验证明，该方法在多种设置下（包括基于LiDAR和纯RGB-D输入）实现了具有竞争力的定位性能，同时保持了免训练和开放词汇的特性。", "conclusion": "该研究结果突出了2D基础模型在可扩展3D感知方面的巨大潜力。"}}
{"id": "2507.13390", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13390", "abs": "https://arxiv.org/abs/2507.13390", "authors": ["Kundeshwar Pundalik", "Piyush Sawarkar", "Nihar Sahoo", "Abhishek Shinde", "Prateek Chanda", "Vedant Goswami", "Ajay Nagpal", "Atul Singh", "Viraj Thakur", "Vijay Dewane", "Aamod Thakur", "Bhargav Patel", "Smita Gautam", "Bhagwan Panditi", "Shyam Pawar", "Madhav Kotcha", "Suraj Racha", "Saral Sureka", "Pankaj Singh", "Rishi Bal", "Rohit Saluja", "Ganesh Ramakrishnan"], "title": "PARAM-1 BharatGen 2.9B Model", "comment": null, "summary": "Large Language Models (LLMs) have emerged as powerful general-purpose\nreasoning systems, yet their development remains dominated by English-centric\ndata, architectures, and optimization paradigms. This exclusionary design\nresults in structural under-representation of linguistically diverse regions\nsuch as India, where over 20 official languages and 100+ dialects coexist\nalongside phenomena like code-switching and diglossia. We introduce PARAM-1, a\n2.9B parameter decoder-only, text-only language model trained from scratch with\nan explicit architectural and linguistic focus on Indian diversity. PARAM-1 is\ntrained on a bilingual dataset consisting of only Hindi and English,\nconstructed with a strong focus on fact-rich, high-quality content. It is\nguided by three core principles: equitable representation of Indic languages\nthrough a 25% corpus allocation; tokenization fairness via a SentencePiece\ntokenizer adapted to Indian morphological structures; and culturally aligned\nevaluation benchmarks across IndicQA, code-mixed reasoning, and\nsocio-linguistic robustness tasks. By embedding diversity at the pretraining\nlevel-rather than deferring it to post-hoc alignment-PARAM-1 offers a\ndesign-first blueprint for equitable foundation modeling. Our results\ndemonstrate that it serves as both a competent general-purpose model and a\nrobust baseline for India-centric applications.", "AI": {"tldr": "本文介绍了PARAM-1，一个2.9B参数的解码器专用语言模型，旨在解决大型语言模型在印度语言多样性方面的代表性不足问题，通过从头训练并明确关注印度语言和文化多样性。", "motivation": "现有的大型语言模型（LLMs）主要以英语为中心，导致印度等语言多样性丰富的地区在数据、架构和优化范式上存在结构性代表不足，无法充分处理印度的多种官方语言、方言、语码转换和双言现象。", "method": "PARAM-1是一个2.9B参数的解码器专用、纯文本语言模型，从零开始训练。它使用仅包含印地语和英语的双语数据集，该数据集侧重于事实丰富的高质量内容。其核心原则包括：印度语言在语料库中占比25%以实现公平表示；采用适应印度形态结构的SentencePiece分词器以实现分词公平性；以及通过IndicQA、语码混合推理和社会语言学鲁棒性任务进行文化对齐的评估。", "result": "PARAM-1表现出作为通用模型的竞争力，并为以印度为中心的应用提供了强大的基线。它通过在预训练阶段嵌入多样性，而非推迟到事后对齐，提供了一个公平基础模型设计的蓝图。", "conclusion": "PARAM-1通过在预训练级别嵌入多样性，为公平的基础模型设计提供了一个“设计优先”的蓝图。它既能作为通用的语言模型，也能作为印度特定应用的强大基线，证明了其对印度语言多样性的有效支持。"}}
{"id": "2507.13759", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13759", "abs": "https://arxiv.org/abs/2507.13759", "authors": ["Carlos Bobed", "Carlota Quintana", "Eduardo Mena", "Jorge Bobed", "Fernando Bobillo"], "title": "OntView: What you See is What you Meant", "comment": null, "summary": "In the field of knowledge management and computer science, ontologies provide\na structured framework for modeling domain-specific knowledge by defining\nconcepts and their relationships. However, the lack of tools that provide\neffective visualization is still a significant challenge. While numerous\nontology editors and viewers exist, most of them fail to graphically represent\nontology structures in a meaningful and non-overwhelming way, limiting users'\nability to comprehend dependencies and properties within large ontological\nframeworks.\n  In this paper, we present OntView, an ontology viewer that is designed to\nprovide users with an intuitive visual representation of ontology concepts and\ntheir formal definitions through a user-friendly interface. Building on the use\nof a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm,\nshowing the actual inferred knowledge. One key aspect for this is its ability\nto visualize General Concept Inclusions (GCI), a feature absent in existing\nvisualization tools. Moreover, to avoid a possible information overload,\nOntView also offers different ways to show a simplified view of the ontology\nby: 1) creating ontology summaries by assessing the importance of the concepts\n(according to different available algorithms), 2) focusing the visualization on\nthe existing TBox elements between two given classes and 3) allowing to\nhide/show different branches in a dynamic way without losing the semantics.\nOntView has been released with an open-source license for the whole community.", "AI": {"tldr": "本文介绍了OntView，一个本体可视化工具，它通过结合描述逻辑推理机提供直观的、基于推断的本体结构表示，并能显示通用概念包含（GCI），同时提供多种简化视图以避免信息过载。", "motivation": "现有本体编辑器和查看器在图形化表示本体结构方面存在不足，尤其对于大型本体，难以提供有意义且不令人 overwhelmed 的视图，限制了用户理解概念依赖和属性的能力。", "method": "本文提出了OntView本体查看器。它利用描述逻辑（DL）推理机来显示实际推断出的知识（遵循“所见即所得”范式），并能可视化现有工具中缺失的通用概念包含（GCI）。为避免信息过载，OntView还提供多种简化视图方式，包括：1) 根据概念重要性（采用不同算法）创建本体摘要；2) 聚焦于两个给定类之间的TBox元素可视化；3) 动态隐藏/显示不同分支而不失语义。OntView已作为开源软件发布。", "result": "OntView成功地提供了一种直观的本体概念及其形式化定义的视觉表示，能够显示推断出的知识，并特别支持GCI的可视化。此外，它通过提供本体摘要、聚焦视图和动态分支控制等功能，有效避免了信息过载问题，提升了用户对大型本体的理解能力。", "conclusion": "OntView成功解决了本体可视化中的挑战，提供了一个用户友好、能显示推断知识（包括GCI）并能有效管理信息过载的工具，为本体社区提供了有价值的开源解决方案。"}}
{"id": "2507.13872", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13872", "abs": "https://arxiv.org/abs/2507.13872", "authors": ["Aditya Singh", "Aastha Mishra", "Manan Tayal", "Shishir Kolathaya", "Pushpak Jagtap"], "title": "Safe and Performant Controller Synthesis using Gradient-based Model Predictive Control and Control Barrier Functions", "comment": "6 Pages, 2 Figures. The first two authors contributed equally", "summary": "Ensuring both performance and safety is critical for autonomous systems\noperating in real-world environments. While safety filters such as Control\nBarrier Functions (CBFs) enforce constraints by modifying nominal controllers\nin real time, they can become overly conservative when the nominal policy lacks\nsafety awareness. Conversely, solving State-Constrained Optimal Control\nProblems (SC-OCPs) via dynamic programming offers formal guarantees but is\nintractable in high-dimensional systems. In this work, we propose a novel\ntwo-stage framework that combines gradient-based Model Predictive Control (MPC)\nwith CBF-based safety filtering for co-optimizing safety and performance. In\nthe first stage, we relax safety constraints as penalties in the cost function,\nenabling fast optimization via gradient-based methods. This step improves\nscalability and avoids feasibility issues associated with hard constraints. In\nthe second stage, we modify the resulting controller using a CBF-based\nQuadratic Program (CBF-QP), which enforces hard safety constraints with minimal\ndeviation from the reference. Our approach yields controllers that are both\nperformant and provably safe. We validate the proposed framework on two case\nstudies, showcasing its ability to synthesize scalable, safe, and\nhigh-performance controllers for complex, high-dimensional autonomous systems.", "AI": {"tldr": "本文提出一个两阶段框架，结合基于梯度的模型预测控制（MPC）和基于控制屏障函数（CBF）的安全滤波，以协同优化自主系统的性能和安全性。", "motivation": "在实际环境中运行的自主系统需要兼顾性能和安全性。现有方法存在局限：控制屏障函数（CBF）可能过于保守，而状态约束最优控制问题（SC-OCP）在高维系统中难以处理。", "method": "该方法包含两个阶段：第一阶段，将安全约束松弛为成本函数中的惩罚项，通过基于梯度的MPC进行快速优化，提高可扩展性并避免硬约束的可行性问题；第二阶段，使用基于CBF的二次规划（CBF-QP）修改第一阶段得到的控制器，强制执行硬安全约束，同时最小化与参考控制器的偏差。", "result": "该方法得到的控制器既具有高性能，又可证明是安全的。通过两个案例研究验证了该框架，表明其能够为复杂、高维的自主系统合成可扩展、安全且高性能的控制器。", "conclusion": "所提出的两阶段框架成功地将基于梯度的MPC与基于CBF的安全滤波相结合，有效解决了自主系统在性能和安全性方面的挑战，实现了可扩展、安全且高性能的控制。"}}
{"id": "2507.13830", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13830", "abs": "https://arxiv.org/abs/2507.13830", "authors": ["Maximilian Rokuss", "Benjamin Hamm", "Yannick Kirchhoff", "Klaus Maier-Hein"], "title": "Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation", "comment": "Accepted at MICCAI 2025 WOMEN", "summary": "We introduce the first publicly available breast MRI dataset with explicit\nleft and right breast segmentation labels, encompassing more than 13,000\nannotated cases. Alongside this dataset, we provide a robust deep-learning\nmodel trained for left-right breast segmentation. This work addresses a\ncritical gap in breast MRI analysis and offers a valuable resource for the\ndevelopment of advanced tools in women's health. The dataset and trained model\nare publicly available at: www.github.com/MIC-DKFZ/BreastDivider", "AI": {"tldr": "本文介绍了一个包含超过13,000个注释病例的乳腺MRI数据集，并提供了用于左右乳腺分割的深度学习模型，填补了乳腺MRI分析中的关键空白。", "motivation": "研究动机是为了解决乳腺MRI分析中存在的关键空白，即缺乏带有明确左右乳腺分割标签的公开数据集和相应的工具。", "method": "研究方法是构建并发布了一个大型乳腺MRI数据集，该数据集包含明确的左右乳腺分割标签，并基于此数据集训练了一个鲁棒的深度学习模型用于左右乳腺分割。", "result": "研究成果是发布了首个公开可用的乳腺MRI数据集，包含超过13,000个带左右乳腺分割标签的病例，同时提供了一个训练好的鲁棒深度学习模型。", "conclusion": "该工作为乳腺MRI分析提供了宝贵资源，有助于开发女性健康领域的高级工具。"}}
{"id": "2507.13650", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13650", "abs": "https://arxiv.org/abs/2507.13650", "authors": ["Yu-Ting Lai", "Yasamin Foroutani", "Aya Barzelay", "Tsu-Chin Tsao"], "title": "Safe Robotic Capsule Cleaning with Integrated Transpupillary and Intraocular Optical Coherence Tomography", "comment": "12 pages, 27 figures", "summary": "Secondary cataract is one of the most common complications of vision loss due\nto the proliferation of residual lens materials that naturally grow on the lens\ncapsule after cataract surgery. A potential treatment is capsule cleaning, a\nsurgical procedure that requires enhanced visualization of the entire capsule\nand tool manipulation on the thin membrane. This article presents a robotic\nsystem capable of performing the capsule cleaning procedure by integrating a\nstandard transpupillary and an intraocular optical coherence tomography probe\non a surgical instrument for equatorial capsule visualization and real-time\ntool-to-tissue distance feedback. Using robot precision, the developed system\nenables complete capsule mapping in the pupillary and equatorial regions with\nin-situ calibration of refractive index and fiber offset, which are still\ncurrent challenges in obtaining an accurate capsule model. To demonstrate\neffectiveness, the capsule mapping strategy was validated through five\nexperimental trials on an eye phantom that showed reduced root-mean-square\nerrors in the constructed capsule model, while the cleaning strategy was\nperformed in three ex-vivo pig eyes without tissue damage.", "AI": {"tldr": "本文提出一种机器人系统，通过整合瞳孔穿透式和眼内光学相干断层扫描（OCT）探头，实现继发性白内障的晶状体囊膜清洁，提供增强的可视化和实时工具-组织距离反馈。", "motivation": "继发性白内障是白内障手术后常见的视力丧失并发症，由残留晶状体物质在囊膜上增生引起。现有的囊膜清洁手术需要增强对整个囊膜的可视化以及在薄膜上的工具操作。", "method": "该系统将标准瞳孔穿透式和眼内OCT探头集成到手术器械上，用于赤道囊膜的可视化和实时工具-组织距离反馈。利用机器人精度，该系统能够对瞳孔区和赤道区的囊膜进行完整映射，并进行原位折射率和光纤偏移校准。", "result": "通过在眼部模型上进行的五次实验验证了囊膜映射策略，结果显示构建的囊膜模型均方根误差降低；在三只离体猪眼上进行了清洁策略演示，未造成组织损伤。", "conclusion": "所开发的机器人系统能够实现精确的囊膜映射和清洁操作，有效解决了继发性白内障治疗中囊膜可视化和精确模型获取的挑战。"}}
{"id": "2507.13364", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13364", "abs": "https://arxiv.org/abs/2507.13364", "authors": ["Siddharth Srivastava", "Gaurav Sharma"], "title": "OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning", "comment": null, "summary": "We present a novel multimodal multitask network and associated training\nalgorithm. The method is capable of ingesting data from approximately 12\ndifferent modalities namely image, video, audio, text, depth, point cloud, time\nseries, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed\napproach utilizes modality specialized tokenizers, a shared transformer\narchitecture, and cross-attention mechanisms to project the data from different\nmodalities into a unified embedding space. It addresses multimodal and\nmultitask scenarios by incorporating modality-specific task heads for different\ntasks in respective modalities. We propose a novel pretraining strategy with\niterative modality switching to initialize the network, and a training\nalgorithm which trades off fully joint training over all modalities, with\ntraining on pairs of modalities at a time. We provide comprehensive evaluation\nacross 25 datasets from 12 modalities and show state of the art performances,\ndemonstrating the effectiveness of the proposed architecture, pretraining\nstrategy and adapted multitask training.", "AI": {"tldr": "本文提出了一种新颖的多模态多任务网络及其训练算法，能够处理约12种不同模态的数据，并通过统一的嵌入空间和创新的预训练/训练策略，在多任务场景中实现最先进的性能。", "motivation": "研究动机在于开发一个能够统一处理大量不同数据模态（如图像、视频、音频、文本等）并解决多任务场景的通用框架，同时实现高性能。", "method": "该方法包括：1) 模态专用分词器；2) 共享的Transformer架构；3) 交叉注意力机制，将不同模态数据投影到统一的嵌入空间；4) 针对不同模态任务的模态特定任务头。此外，还提出了一种新颖的预训练策略（迭代模态切换）和一种训练算法，该算法在所有模态的完全联合训练与一次训练一对模态之间进行权衡。", "result": "该方法在来自12种模态的25个数据集上进行了全面评估，并展示了最先进的性能。", "conclusion": "所提出的架构、预训练策略和适应性多任务训练是有效的，能够成功处理多模态多任务场景，并达到顶尖水平。"}}
{"id": "2507.13392", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13392", "abs": "https://arxiv.org/abs/2507.13392", "authors": ["Emil Häglund", "Johanna Björklund"], "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction", "comment": null, "summary": "We improve the extraction of insights from customer reviews by restructuring\nthe topic modelling pipeline to operate on opinion units - distinct statements\nthat include relevant text excerpts and associated sentiment scores. Prior work\nhas demonstrated that such units can be reliably extracted using large language\nmodels. The result is a heightened performance of the subsequent topic\nmodeling, leading to coherent and interpretable topics while also capturing the\nsentiment associated with each topic. By correlating the topics and sentiments\nwith business metrics, such as star ratings, we can gain insights on how\nspecific customer concerns impact business outcomes. We present our system's\nimplementation, use cases, and advantages over other topic modeling and\nclassification solutions. We also evaluate its effectiveness in creating\ncoherent topics and assess methods for integrating topic and sentiment\nmodalities for accurate star-rating prediction.", "AI": {"tldr": "该研究通过将主题建模应用于包含文本和情感得分的“意见单元”，显著提升了从客户评论中提取洞察的能力，并能关联客户关注点与业务指标。", "motivation": "现有方法在从客户评论中提取有意义的洞察方面存在局限。研究旨在改进主题建模，使其能更有效地识别连贯、可解释的主题，并捕捉相关情感，最终将这些洞察与业务成果（如星级评分）关联起来，以理解客户关注点如何影响业务表现。", "method": "该研究重构了主题建模流程，使其操作对象为“意见单元”。这些意见单元是包含相关文本摘录和情感得分的独立陈述，并可通过大型语言模型可靠提取。随后，将这些主题和情感与星级评分等业务指标进行关联，以评估其对业务成果的影响。", "result": "结果显示，该方法显著提升了后续主题建模的性能，生成了连贯且可解释的主题，并成功捕获了每个主题相关的情感。通过关联主题和情感与业务指标，系统能够洞察特定客户关注点如何影响业务成果。此外，系统在创建连贯主题方面表现出有效性，并评估了整合主题和情感模态以准确预测星级评分的方法。", "conclusion": "该系统在从客户评论中提取洞察方面优于其他主题建模和分类解决方案，能够有效地创建连贯主题，并通过整合主题和情感模态实现准确的星级评分预测，从而为理解客户关注点对业务成果的影响提供了新的视角和优势。"}}
{"id": "2507.13768", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13768", "abs": "https://arxiv.org/abs/2507.13768", "authors": ["Renato Ghisellini", "Remo Pareschi", "Marco Pedroni", "Giovanni Battista Raggi"], "title": "From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning", "comment": "Peer-reviewed full paper accepted through a double-blind review\n  process at the HAR 2025 conference (https://har-conf.eu/). The official\n  version will appear in a volume of the Lecture Notes in Computer Science\n  (LNCS) series", "summary": "We present a hybrid architecture for agent-augmented strategic reasoning,\ncombining heuristic extraction, semantic activation, and compositional\nsynthesis. Drawing on sources ranging from classical military theory to\ncontemporary corporate strategy, our model activates and composes multiple\nheuristics through a process of semantic interdependence inspired by research\nin quantum cognition. Unlike traditional decision engines that select the best\nrule, our system fuses conflicting heuristics into coherent and\ncontext-sensitive narratives, guided by semantic interaction modeling and\nrhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,\nwith preliminary validation through semantic metrics. Limitations and\nextensions (e.g., dynamic interference tuning) are discussed.", "AI": {"tldr": "本文提出一种混合架构，用于代理增强的战略推理，通过语义激活和组合合成，将冲突的启发式规则融合为连贯的叙事，而非简单选择最佳规则。", "motivation": "传统的决策引擎倾向于选择最佳规则，而本研究旨在解决如何将冲突的启发式规则融合成连贯且上下文敏感的叙事。", "method": "采用混合架构，结合启发式提取、语义激活和组合合成。受量子认知研究启发，通过语义相互依存过程激活和组合多个启发式规则，并利用语义交互建模和修辞框架来融合冲突的启发式规则。", "result": "该框架通过Meta vs. FTC案例研究进行了演示，并通过语义指标进行了初步验证。", "conclusion": "该研究提出了一种新的战略推理框架，能够将冲突的启发式规则融合为语境敏感的叙事，为传统决策引擎提供了替代方案。文章也讨论了局限性及未来扩展方向。"}}
{"id": "2507.13888", "categories": ["eess.SY", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13888", "abs": "https://arxiv.org/abs/2507.13888", "authors": ["Janani S K", "Shishir Kolathaya"], "title": "Fixed time convergence guarantees for Higher Order Control Barrier Functions", "comment": "6 PAGES, 2 FIGURES", "summary": "We present a novel method for designing higher-order Control Barrier\nFunctions (CBFs) that guarantee convergence to a safe set within a\nuser-specified finite. Traditional Higher Order CBFs (HOCBFs) ensure asymptotic\nsafety but lack mechanisms for fixed-time convergence, which is critical in\ntime-sensitive and safety-critical applications such as autonomous navigation.\nIn contrast, our approach imposes a structured differential constraint using\nrepeated roots in the characteristic polynomial, enabling closed-form\npolynomial solutions with exact convergence at a prescribed time. We derive\nconditions on the barrier function and its derivatives that ensure forward\ninvariance and fixed-time reachability, and we provide an explicit formulation\nfor second-order systems. Our method is evaluated on three robotic systems - a\npoint-mass model, a unicycle, and a bicycle model and benchmarked against\nexisting HOCBF approaches. Results demonstrate that our formulation reliably\nenforces convergence within the desired time, even when traditional methods\nfail. This work provides a tractable and robust framework for real-time control\nwith provable finite-time safety guarantees.", "AI": {"tldr": "本文提出一种新颖的高阶控制障碍函数（HOCBFs）设计方法，以确保在用户指定有限时间内收敛到安全集。", "motivation": "传统高阶控制障碍函数只能保证渐近安全，但在时间敏感和安全关键型应用（如自主导航）中，固定时间收敛至关重要。", "method": "通过在特征多项式中使用重复根施加结构化微分约束，从而得到在规定时间精确收敛的闭式多项式解。文中推导了确保前向不变性和固定时间可达性的障碍函数及其导数条件，并提供了二阶系统的明确公式。", "result": "该方法在点质量模型、独轮车和自行车模型三个机器人系统上进行了评估，并与现有HOCBF方法进行了基准测试。结果表明，即使传统方法失败，本文提出的方法也能可靠地在期望时间内强制收敛。", "conclusion": "这项工作为具有可证明的有限时间安全保证的实时控制提供了一个可行且鲁棒的框架。"}}
{"id": "2507.13901", "categories": ["eess.IV", "cs.CV", "62H35, 68U10", "I.4.10; I.4.7; J.3"], "pdf": "https://arxiv.org/pdf/2507.13901", "abs": "https://arxiv.org/abs/2507.13901", "authors": ["Lei Xu", "Torkel B Brismar"], "title": "Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive", "comment": "24 pages, 7 figures", "summary": "We have developed a novel CT image analysis package named AnatomyArchive,\nbuilt on top of the recent full body segmentation model TotalSegmentator. It\nprovides automatic target volume selection and deselection capabilities\naccording to user-configured anatomies for volumetric upper- and lower-bounds.\nIt has a knowledge graph-based and time efficient tool for anatomy segmentation\nmask management and medical image database maintenance. AnatomyArchive enables\nautomatic body volume cropping, as well as automatic arm-detection and\nexclusion, for more precise body composition analysis in both 2D and 3D\nformats. It provides robust voxel-based radiomic feature extraction, feature\nvisualization, and an integrated toolchain for statistical tests and analysis.\nA python-based GPU-accelerated nearly photo-realistic segmentation-integrated\ncomposite cinematic rendering is also included. We present here its software\narchitecture design, illustrate its workflow and working principle of\nalgorithms as well provide a few examples on how the software can be used to\nassist development of modern machine learning models. Open-source codes will be\nreleased at https://github.com/lxu-medai/AnatomyArchive for only research and\neducational purposes.", "AI": {"tldr": "AnatomyArchive是一个基于TotalSegmentator的CT图像分析软件包，提供自动解剖区域选择、精确体成分分析、影像组学特征提取与可视化，并支持机器学习模型开发。", "motivation": "为了实现更精确的CT图像体成分分析，解决自动目标体积选择与排除、分割掩膜管理、医学图像数据库维护以及影像组学特征提取和统计分析的挑战，并辅助现代机器学习模型的开发。", "method": "该软件包基于TotalSegmentator模型构建，提供根据用户配置的解剖结构进行自动目标体积选择和取消选择。它采用基于知识图谱的高效工具进行解剖分割掩膜管理和医学图像数据库维护。方法包括自动身体体积裁剪、自动手臂检测和排除，以实现精确的体成分分析。此外，还提供鲁棒的体素级影像组学特征提取、特征可视化以及集成的统计测试和分析工具链。软件还包含基于Python的GPU加速的近乎照片级的分割集成复合电影渲染功能。", "result": "AnatomyArchive实现了根据用户配置的解剖结构进行自动目标体积选择与取消选择。它能够进行更精确的2D和3D体成分分析，并提供鲁棒的体素级影像组学特征提取和可视化能力，以及集成的统计分析工具。该软件可用于辅助现代机器学习模型的开发。", "conclusion": "AnatomyArchive是一个功能全面、高效的CT图像分析软件包，它通过集成先进的分割模型和多种分析工具，显著提升了医学图像分析的自动化和精确性，尤其在体成分分析、影像组学和机器学习模型开发方面具有重要应用潜力。该软件将开源用于研究和教育目的。"}}
{"id": "2507.13654", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13654", "abs": "https://arxiv.org/abs/2507.13654", "authors": ["Haoran Wang", "Yasamin Foroutani", "Matthew Nepo", "Mercedes Rodriguez", "Ji Ma", "Jean-Pierre Hubschman", "Tsu-Chin Tsao", "Jacob Rosen"], "title": "A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery Environment", "comment": "9 pages, 11 figures", "summary": "This paper examines the performance of Inside and Outside Control modes at\nvarious scaling factors in a simulated vitreoretinal surgical setting. The\nIRISS teleoperated surgical system's console (cockpit) was adapted to project a\nsimulated microscope view of an intraocular setup to a virtual reality (VR)\nheadset. Five experienced vitreoretinal surgeons and five engineers with no\nsurgical experience used the system to perform tasks common to vitreoretinal\nsurgery. Experimental results indicate that Inside Control methods at higher\nscaling factors (20 or 30) achieved the best performance overall, though the\noptimal scaling factor may vary by task and complexity. Optimizing control\nmethods and scaling factors could lead to improvements in surgical efficiency\nand accuracy, as well as minimize risks in future robotic-assisted intraocular\nprocedures.", "AI": {"tldr": "本文在模拟玻璃体视网膜手术环境中，研究了不同缩放因子下“内部控制”和“外部控制”模式的性能。", "motivation": "优化控制方法和缩放因子可以提高手术效率和准确性，并最大限度地降低未来机器人辅助眼内手术的风险。", "method": "研究人员改造了IRISS远程手术系统的控制台，将模拟显微镜视图投射到VR头显中。五名经验丰富的玻璃体视网膜外科医生和五名无手术经验的工程师使用该系统执行了玻璃体视网膜手术中的常见任务。", "result": "实验结果表明，在较高缩放因子（20或30）下的“内部控制”方法总体表现最佳，尽管最佳缩放因子可能因任务和复杂性而异。", "conclusion": "优化控制方法和缩放因子有望提高未来机器人辅助眼内手术的效率、准确性并降低风险。"}}
{"id": "2507.13371", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13371", "abs": "https://arxiv.org/abs/2507.13371", "authors": ["Yeming Cai", "Yang Wang", "Zhenglin Li"], "title": "Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation", "comment": null, "summary": "This paper proposes an end-to-end deep learning framework integrating optical\nmotion capture with a Transformer-based model to enhance medical\nrehabilitation. It tackles data noise and missing data caused by occlusion and\nenvironmental factors, while detecting abnormal movements in real time to\nensure patient safety. Utilizing temporal sequence modeling, our framework\ndenoises and completes motion capture data, improving robustness. Evaluations\non stroke and orthopedic rehabilitation datasets show superior performance in\ndata reconstruction and anomaly detection, providing a scalable, cost-effective\nsolution for remote rehabilitation with reduced on-site supervision.", "AI": {"tldr": "该论文提出了一个结合光学运动捕捉和Transformer模型的端到端深度学习框架，用于医疗康复，旨在处理数据噪声和缺失，并实时检测异常动作。", "motivation": "光学运动捕捉数据常因遮挡和环境因素导致噪声和缺失，且需要实时检测异常动作以确保患者安全。现有康复方案可能缺乏可扩展性和成本效益，需要减少现场监督。", "method": "本文提出了一个端到端深度学习框架，该框架将光学运动捕捉与基于Transformer的模型相结合。它利用时间序列建模来去噪和补全运动捕捉数据，并实时检测异常运动。", "result": "在针对中风和骨科康复数据集的评估中，该框架在数据重建和异常检测方面表现出卓越的性能。", "conclusion": "该框架提供了一个可扩展、经济高效的远程康复解决方案，能够减少现场监督，有效提升医疗康复效果和患者安全性。"}}
{"id": "2507.13395", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13395", "abs": "https://arxiv.org/abs/2507.13395", "authors": ["Xuanqi Gao", "Weipeng Jiang", "Juan Zhai", "Shiqing Ma", "Siyi Xie", "Xinyang Yin", "Chao Shen"], "title": "Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only", "comment": null, "summary": "The advent of neural machine translation (NMT) has revolutionized\ncross-lingual communication, yet preserving stylistic nuances remains a\nsignificant challenge. While existing approaches often require parallel corpora\nfor style preservation, we introduce Babel, a novel framework that enhances\nstylistic fidelity in NMT using only monolingual corpora. Babel employs two key\ncomponents: (1) a style detector based on contextual embeddings that identifies\nstylistic disparities between source and target texts, and (2) a\ndiffusion-based style applicator that rectifies stylistic inconsistencies while\nmaintaining semantic integrity. Our framework integrates with existing NMT\nsystems as a post-processing module, enabling style-aware translation without\nrequiring architectural modifications or parallel stylistic data. Extensive\nexperiments on five diverse domains (law, literature, scientific writing,\nmedicine, and educational content) demonstrate Babel's effectiveness: it\nidentifies stylistic inconsistencies with 88.21% precision and improves\nstylistic preservation by 150% while maintaining a high semantic similarity\nscore of 0.92. Human evaluation confirms that translations refined by Babel\nbetter preserve source text style while maintaining fluency and adequacy.", "AI": {"tldr": "Babel是一种新颖的框架，它利用单语语料库作为后处理模块，显著提升了神经机器翻译（NMT）中的风格保持能力，无需平行语料或NMT系统架构修改。", "motivation": "神经机器翻译（NMT）在跨语言交流方面取得了革命性进展，但保留文体细微差别仍然是一个重大挑战。现有方法通常需要平行语料库来实现风格保持。", "method": "Babel框架包含两个关键组件：1) 基于上下文嵌入的风格检测器，用于识别源文本和目标文本之间的风格差异；2) 基于扩散的风格应用器，用于纠正风格不一致性，同时保持语义完整性。该框架作为一个后处理模块与现有NMT系统集成，仅使用单语语料库。", "result": "在法律、文学、科学写作、医学和教育内容等五个不同领域进行了广泛实验。Babel能够以88.21%的精度识别风格不一致性，将风格保持度提高了150%，同时保持了0.92的高语义相似度得分。人工评估证实，经过Babel优化的翻译能更好地保留源文本风格，同时保持流畅性和充分性。", "conclusion": "Babel框架通过其风格检测器和扩散式风格应用器，有效解决了NMT中风格保留的挑战，并且仅需单语语料库即可实现，作为NMT系统的后处理模块，显著提升了翻译的风格保真度，同时维持了语义完整性。"}}
{"id": "2507.13825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13825", "abs": "https://arxiv.org/abs/2507.13825", "authors": ["Haoyang Li", "Yuming Xu", "Yiming Li", "Hanmo Liu", "Darian Li", "Chen Jason Zhang", "Lei Chen", "Qing Li"], "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction", "comment": "Submitted in 2024. Accepted in 2025", "summary": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.", "AI": {"tldr": "本文提出EAGLE，一个轻量级框架，通过结合短期时间近邻信息和长期全局结构模式，显著提升了动态图中时间链接预测的效率和效果。", "motivation": "现有的时间图神经网络（T-GNNs）在建模时间及结构依赖方面虽取得成功，但因计算开销大，常面临可扩展性和效率挑战。", "method": "EAGLE框架包含：1) 时间感知模块，聚合节点最近邻居信息以反映即时偏好；2) 结构感知模块，利用时间个性化PageRank捕捉全局重要节点的影响；3) 自适应加权机制，动态调整两模块贡献；4) 避免了复杂的多跳消息传递或内存密集型机制，以提高效率。", "result": "在七个真实世界时间图上的大量实验表明，EAGLE在有效性和效率方面均持续超越最先进的T-GNNs，相比基于Transformer的T-GNNs实现了超过50倍的加速。", "conclusion": "EAGLE通过巧妙地整合短期时间近邻和长期全局结构模式，提供了一种高效且高性能的时间链接预测解决方案，显著优于现有方法。"}}
{"id": "2507.13908", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13908", "abs": "https://arxiv.org/abs/2507.13908", "authors": ["Frederik Thiele", "Felix Biertümpfel", "Harald Pfifer"], "title": "A Robust Periodic Controller for Spacecraft Attitude Tracking", "comment": "Presented at European Control Conference 2025", "summary": "This paper presents a novel approach for robust periodic attitude control of\nsatellites. Respecting the periodicity of the satellite dynamics in the\nsynthesis allows to achieve constant performance and robustness requirements\nover the orbit. The proposed design follows a mixed sensitivity control design\nemploying a physically motivated weighting scheme. The controller is calculated\nusing a novel structured linear time-periodic output feedback synthesis with\nguaranteed optimal L2-performance. The synthesis poses a convex optimization\nproblem and avoids grid-wise evaluations of coupling conditions inherent for\nclassical periodic H-infinity-synthesis. Moreover, the controller has a\ntransparent and easy to implement structure. A solar power plant satellite is\nused to demonstrate the effectiveness of the proposed method for periodic\nsatellite attitude control.", "AI": {"tldr": "本文提出了一种新颖的鲁棒周期性卫星姿态控制方法，通过考虑系统周期性，实现了轨道上恒定的性能和鲁棒性。", "motivation": "研究动机是为了在卫星动力学中尊重其周期性，从而在整个轨道上实现恒定的性能和鲁棒性要求。", "method": "采用混合敏感度控制设计，结合物理驱动的加权方案。控制器通过一种新颖的结构化线性时变输出反馈综合方法计算，该方法是一个凸优化问题，避免了传统周期性H-infinity综合中固有的网格化耦合条件评估。", "result": "该方法保证了最优的L2性能，控制器结构透明且易于实现。在一个太阳能电站卫星上验证了所提方法的有效性。", "conclusion": "所提出的周期性卫星姿态控制方法能够有效实现鲁棒控制，并具有最优性能和易于实现的控制器结构。"}}
{"id": "2507.13915", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13915", "abs": "https://arxiv.org/abs/2507.13915", "authors": ["Huu-Phu Do", "Po-Chih Hu", "Hao-Chien Hsueh", "Che-Kai Liu", "Vu-Hoang Tran", "Ching-Chun Huang"], "title": "Blind Super Resolution with Reference Images and Implicit Degradation Representation", "comment": "Accepted by ACCV 2024", "summary": "Previous studies in blind super-resolution (BSR) have primarily concentrated\non estimating degradation kernels directly from low-resolution (LR) inputs to\nenhance super-resolution. However, these degradation kernels, which model the\ntransition from a high-resolution (HR) image to its LR version, should account\nfor not only the degradation process but also the downscaling factor. Applying\nthe same degradation kernel across varying super-resolution scales may be\nimpractical. Our research acknowledges degradation kernels and scaling factors\nas pivotal elements for the BSR task and introduces a novel strategy that\nutilizes HR images as references to establish scale-aware degradation kernels.\nBy employing content-irrelevant HR reference images alongside the target LR\nimage, our model adaptively discerns the degradation process. It is then\napplied to generate additional LR-HR pairs through down-sampling the HR\nreference images, which are keys to improving the SR performance. Our\nreference-based training procedure is applicable to proficiently trained blind\nSR models and zero-shot blind SR methods, consistently outperforming previous\nmethods in both scenarios. This dual consideration of blur kernels and scaling\nfactors, coupled with the use of a reference image, contributes to the\neffectiveness of our approach in blind super-resolution tasks.", "AI": {"tldr": "该研究提出一种新的盲超分辨率（BSR）策略，利用高分辨率（HR）参考图像来学习尺度感知降质核，并通过生成额外的LR-HR对来提升超分辨率性能，在多种BSR场景下均优于现有方法。", "motivation": "以往的盲超分辨率研究主要关注直接从低分辨率（LR）输入估计降质核，但这些降质核不仅应考虑降质过程，还应考虑下采样因子。在不同超分辨率尺度下应用相同的降质核是不切实际的。", "method": "该研究提出一种新策略，将降质核和缩放因子视为BSR任务的关键要素。它利用高分辨率（HR）图像作为参考，建立尺度感知的降质核。通过使用与内容无关的HR参考图像和目标LR图像，模型能够自适应地识别降质过程，并生成额外的LR-HR图像对（通过下采样HR参考图像），这对于提高SR性能至关重要。该基于参考的训练过程适用于已训练好的盲SR模型和零样本盲SR方法。", "result": "该方法在已训练好的盲SR模型和零样本盲SR方法两种场景下，均持续优于以往的方法。", "conclusion": "结合模糊核和缩放因子的双重考量，以及参考图像的使用，共同提升了该方法在盲超分辨率任务中的有效性。"}}
{"id": "2507.13662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13662", "abs": "https://arxiv.org/abs/2507.13662", "authors": ["Jing Cheng", "Yasser G. Alqaham", "Zhenyu Gan", "Amit K. Sanyal"], "title": "Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive and High Precision Locomotion", "comment": null, "summary": "This paper presents a scalable and adaptive control framework for legged\nrobots that integrates Iterative Learning Control (ILC) with a biologically\ninspired torque library (TL), analogous to muscle memory. The proposed method\naddresses key challenges in robotic locomotion, including accurate trajectory\ntracking under unmodeled dynamics and external disturbances. By leveraging the\nrepetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the\nframework enhances accuracy and generalization across diverse locomotion\nscenarios. The control architecture is data-enabled, combining a physics-based\nmodel derived from hybrid-system trajectory optimization with real-time\nlearning to compensate for model uncertainties and external disturbances. A\ncentral contribution is the development of a generalized TL that stores learned\ncontrol profiles and enables rapid adaptation to changes in speed, terrain, and\ngravitational conditions-eliminating the need for repeated learning and\nsignificantly reducing online computation. The approach is validated on the\nbipedal robot Cassie and the quadrupedal robot A1 through extensive simulations\nand hardware experiments. Results demonstrate that the proposed framework\nreduces joint tracking errors by up to 85% within a few seconds and enables\nreliable execution of both periodic and nonperiodic gaits, including slope\ntraversal and terrain adaptation. Compared to state-of-the-art whole-body\ncontrollers, the learned skills eliminate the need for online computation\nduring execution and achieve control update rates exceeding 30x those of\nexisting methods. These findings highlight the effectiveness of integrating ILC\nwith torque memory as a highly data-efficient and practical solution for legged\nlocomotion in unstructured and dynamic environments.", "AI": {"tldr": "本文提出了一种可扩展、自适应的腿式机器人控制框架，该框架将迭代学习控制（ILC）与受生物启发的扭矩库（TL，类比肌肉记忆）相结合，显著提高了轨迹跟踪精度和泛化能力，并降低了在线计算需求。", "motivation": "现有腿式机器人面临未建模动力学和外部干扰下准确轨迹跟踪的挑战，需要一种能提高准确性和泛化能力，以适应各种步态和环境的控制方法。", "method": "该方法将迭代学习控制（ILC）与生物启发的扭矩库（TL）集成。它利用周期性步态的重复性，并将ILC扩展到非周期性任务。控制架构是数据驱动的，结合了基于物理的模型（源自混合系统轨迹优化）和实时学习，以补偿模型不确定性和外部干扰。核心是开发了一个广义扭矩库，用于存储学习到的控制配置文件，实现对速度、地形和重力条件的快速适应，从而消除重复学习并显著减少在线计算。该方法在双足机器人Cassie和四足机器人A1上进行了广泛的仿真和硬件实验验证。", "result": "该框架能在几秒钟内将关节跟踪误差减少高达85%，并能可靠执行周期性和非周期性步态，包括斜坡遍历和地形适应。与现有最先进的全身控制器相比，学习到的技能在执行过程中无需在线计算，且控制更新速率超过现有方法的30倍。", "conclusion": "将ILC与扭矩记忆相结合，是解决非结构化和动态环境下腿式机器人运动的一个高效、数据高效且实用的解决方案。"}}
{"id": "2507.13372", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13372", "abs": "https://arxiv.org/abs/2507.13372", "authors": ["Yeming Cai", "Zhenglin Li", "Yang Wang"], "title": "Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks", "comment": null, "summary": "Breast cancer is a leading cause of death among women globally, and early\ndetection is critical for improving survival rates. This paper introduces an\ninnovative framework that integrates Vision Transformers (ViT) and Graph Neural\nNetworks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.\nOur framework leverages ViT's ability to capture global image features and\nGNN's strength in modeling structural relationships, achieving an accuracy of\n84.2%, outperforming traditional methods. Additionally, interpretable attention\nheatmaps provide insights into the model's decision-making process, aiding\nradiologists in clinical settings.", "AI": {"tldr": "本文提出一个结合ViT和GNN的创新框架，用于乳腺癌早期检测，在CBIS-DDSM数据集上表现优异并提供可解释性。", "motivation": "乳腺癌是全球女性死亡的主要原因之一，早期检测对于提高生存率至关重要。", "method": "引入一个整合Vision Transformers (ViT) 和 Graph Neural Networks (GNN) 的框架。该框架利用ViT捕捉全局图像特征的能力和GNN建模结构关系的优势，并在CBIS-DDSM数据集上进行乳腺癌检测。此外，还提供可解释的注意力热图。", "result": "该框架在乳腺癌检测中实现了84.2%的准确率，优于传统方法。可解释的注意力热图为模型决策过程提供了洞察。", "conclusion": "结合ViT和GNN的框架能有效提升乳腺癌检测性能，并提供模型决策的可解释性，有助于放射科医生在临床环境中的应用。"}}
{"id": "2507.13410", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13410", "abs": "https://arxiv.org/abs/2507.13410", "authors": ["Cheng-Ting Chou", "George Liu", "Jessica Sun", "Cole Blondin", "Kevin Zhu", "Vasu Sharma", "Sean O'Brien"], "title": "Causal Language Control in Multilingual Transformers via Sparse Feature Steering", "comment": null, "summary": "Deterministically controlling the target generation language of large\nmultilingual language models (LLMs) remains a fundamental challenge,\nparticularly in zero-shot settings where neither explicit language prompts nor\nfine-tuning are available. In this work, we investigate whether sparse\nautoencoder (SAE) features, previously shown to correlate with interpretable\nmodel behaviors, can be leveraged to steer the generated language of LLMs\nduring inference. Leveraging pretrained SAEs on the residual streams of\nGemma-2B and Gemma-9B, we identify features whose activations differ most\nsignificantly between English and four target languages: Chinese, Japanese,\nSpanish, and French. By modifying just a single SAE feature at one transformer\nlayer, we achieve controlled language shifts with up to 90\\% success, as\nmeasured by FastText language classification, while preserving semantic\nfidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)\nsimilarity. Our analysis reveals that language steering is most effective in\nmid-to-late transformer layers and is amplified by specific attention heads\ndisproportionately associated with language-sensitive SAE features. These\nresults demonstrate the promise of sparse feature steering as a lightweight and\ninterpretable mechanism for controllable multilingual generation.", "AI": {"tldr": "该研究利用稀疏自编码器（SAE）特征，在零样本设置下，通过修改大型多语言模型（LLM）推理过程中的单个特征，实现了高达90%的生成语言控制，同时保持语义一致性。", "motivation": "在零样本设置下，缺乏显式语言提示或微调的情况下，确定性地控制大型多语言语言模型（LLM）的目标生成语言仍然是一个基本挑战。", "method": "研究利用在Gemma-2B和Gemma-9B的残差流上预训练的稀疏自编码器（SAE），识别出在英语和四种目标语言（中文、日文、西班牙文、法文）之间激活差异最大的特征。通过在推理过程中修改单个Transformer层的一个SAE特征，实现语言转换。使用FastText进行语言分类评估，并使用LaBSE（Language-Agnostic BERT Sentence Embedding）评估语义保真度。", "result": "通过修改单个SAE特征，实现了高达90%的受控语言转换成功率，同时保持了语义保真度。分析表明，语言控制在中后期Transformer层最有效，并由与语言敏感SAE特征相关的特定注意力头放大。", "conclusion": "稀疏特征操纵是一种有前景、轻量级且可解释的机制，可用于实现可控的多语言生成。"}}
{"id": "2507.13846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13846", "abs": "https://arxiv.org/abs/2507.13846", "authors": ["Kathrin Korte", "Christian Medeiros Adriano", "Sona Ghahremani", "Holger Giese"], "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments", "comment": null, "summary": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.", "AI": {"tldr": "该论文提出了一种因果知识迁移框架，使多智能体强化学习（MARL）在非平稳环境中实现零样本适应，通过共享碰撞恢复动作的紧凑因果表示。", "motivation": "传统的多智能体强化学习知识迁移方法在非平稳环境中泛化能力差，且智能体适应新环境需要昂贵的再训练。", "method": "引入了一个因果知识迁移框架。将每次碰撞建模为因果干预，实例化为一系列恢复动作（宏）。这些宏代表了规避障碍并实现目标的因果知识，可在线从另一个智能体零样本迁移，通过查询一个包含局部上下文信息（碰撞）的查找模型来应用，无需再训练。", "result": "(1) 具有异构目标的智能体在新环境中适应时，能够弥补随机探索与完全再训练策略之间约一半的性能差距；(2) 因果知识迁移的效果取决于环境复杂度和智能体异构目标之间的相互作用。", "conclusion": "该因果知识迁移框架能有效帮助多智能体在非平稳环境中进行零样本适应，并揭示了环境复杂度和智能体目标异构性对知识迁移影响的重要性。"}}
{"id": "2507.13931", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13931", "abs": "https://arxiv.org/abs/2507.13931", "authors": ["L. D. Couto", "K. Haghverdi", "F. Guo", "K. Trad", "G. Mulder"], "title": "Identifiability Analysis of a Pseudo-Two-Dimensional Model & Single Particle Model-Aided Parameter Estimation", "comment": "9 pages, 2 figures, This work has been presented at the 2025 American\n  Control Conference (ACC) and will appear in the conference proceedings.\n  \\c{opyright} 2025 IEEE", "summary": "This contribution presents a parameter identification methodology for the\naccurate and fast estimation of model parameters in a pseudo-two-dimensional\n(P2D) battery model. The methodology consists of three key elements. First, the\ndata for identification is inspected and specific features herein that need to\nbe captured are included in the model. Second, the P2D model is analyzed to\nassess the identifiability of the physical model parameters and propose\nalternative parameterizations that alleviate possible issues. Finally, diverse\noperating conditions are considered that excite distinct battery dynamics which\nallows the use of different low-order battery models accordingly. Results show\nthat, under low current conditions, the use of low-order models achieve\nparameter estimates at least 500 times faster than using the P2D model at the\nexpense of twice the error. However, if accuracy is a must, these estimated\nparameters can be used to initialize the P2D model and perform the\nidentification in half of the time.", "AI": {"tldr": "本文提出了一种参数识别方法，旨在快速准确地估计伪二维（P2D）电池模型的参数，通过结合低阶模型加速初步估计，并用P2D模型进行精确识别。", "motivation": "需要一种准确且快速的方法来估计伪二维（P2D）电池模型的模型参数。", "method": ["检查用于识别的数据，并将需要捕获的特定特征包含在模型中。", "分析P2D模型以评估物理参数的可识别性，并提出替代参数化方案以缓解可能的问题。", "考虑不同的操作条件，这些条件能够激发不同的电池动态，从而相应地使用不同的低阶电池模型。"], "result": ["在低电流条件下，使用低阶模型进行参数估计的速度比使用P2D模型快至少500倍，但误差会增加一倍。", "如果精度是必需的，这些估计的参数可以用于初始化P2D模型，并将识别时间缩短一半。"], "conclusion": "该方法通过结合低阶模型和P2D模型，可以在保证速度的同时，提高P2D电池模型参数识别的准确性，尤其是在需要高精度时，可利用低阶模型的估计结果初始化P2D模型以加速识别过程。"}}
{"id": "2507.13974", "categories": ["eess.IV", "cs.CV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.13974", "abs": "https://arxiv.org/abs/2507.13974", "authors": ["Jiaqi Lv", "Yijie Zhu", "Carmen Guadalupe Colin Tenorio", "Brinder Singh Chohan", "Mark Eastwood", "Shan E Ahmed Raza"], "title": "Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images", "comment": "Accepted by MIUA 2025", "summary": "Melanoma is an aggressive form of skin cancer with rapid progression and high\nmetastatic potential. Accurate characterisation of tissue morphology in\nmelanoma is crucial for prognosis and treatment planning. However, manual\nsegmentation of tissue regions from haematoxylin and eosin (H&E) stained\nwhole-slide images (WSIs) is labour-intensive and prone to inter-observer\nvariability, this motivates the need for reliable automated tissue segmentation\nmethods. In this study, we propose a novel deep learning network for the\nsegmentation of five tissue classes in melanoma H&E images. Our approach\nleverages Virchow2, a pathology foundation model trained on 3.1 million\nhistopathology images as a feature extractor. These features are fused with the\noriginal RGB images and subsequently processed by an encoder-decoder\nsegmentation network (Efficient-UNet) to produce accurate segmentation maps.\nThe proposed model achieved first place in the tissue segmentation task of the\nPUMA Grand Challenge, demonstrating robust performance and generalizability.\nOur results show the potential and efficacy of incorporating pathology\nfoundation models into segmentation networks to accelerate computational\npathology workflows.", "AI": {"tldr": "本文提出一种基于深度学习的新方法，结合病理学基础模型（Virchow2）和Efficient-UNet网络，用于黑色素瘤H&E图像的五种组织类型分割，并在PUMA挑战赛中取得第一名。", "motivation": "黑色素瘤组织形态的准确表征对预后和治疗至关重要。然而，从H&E全切片图像中手动分割组织区域劳动密集且易受观察者间差异影响，因此需要可靠的自动化组织分割方法。", "method": "该研究提出一种新颖的深度学习网络，用于分割黑色素瘤H&E图像中的五种组织类别。该方法利用在310万张组织病理学图像上训练的病理学基础模型Virchow2作为特征提取器，将提取的特征与原始RGB图像融合，然后由编码器-解码器分割网络（Efficient-UNet）进行处理以生成准确的分割图。", "result": "所提出的模型在PUMA Grand Challenge的组织分割任务中获得第一名，展示了其稳健的性能和泛化能力。结果表明将病理学基础模型整合到分割网络中具有加速计算病理学工作流程的潜力与效力。", "conclusion": "将病理学基础模型整合到分割网络中具有巨大的潜力，能够有效提高计算病理学工作流程的效率和准确性，尤其是在黑色素瘤组织分割方面。"}}
{"id": "2507.13702", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13702", "abs": "https://arxiv.org/abs/2507.13702", "authors": ["Junho Choi", "Kihwan Ryoo", "Jeewon Kim", "Taeyun Kim", "Eungchang Lee", "Myeongwoo Jeong", "Kevin Christiansen Marsim", "Hyungtae Lim", "Hyun Myung"], "title": "SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization", "comment": "This paper has been accepted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "summary": "Multi-robot localization is a crucial task for implementing multi-robot\nsystems. Numerous researchers have proposed optimization-based multi-robot\nlocalization methods that use camera, IMU, and UWB sensors. Nevertheless,\ncharacteristics of individual robot odometry estimates and distance\nmeasurements between robots used in the optimization are not sufficiently\nconsidered. In addition, previous researches were heavily influenced by the\nodometry accuracy that is estimated from individual robots. Consequently,\nlong-term drift error caused by error accumulation is potentially inevitable.\nIn this paper, we propose a novel visual-inertial-range-based multi-robot\nlocalization method, named SaWa-ML, which enables geometric structure-aware\npose correction and weight adaptation-based robust multi-robot localization.\nOur contributions are twofold: (i) we leverage UWB sensor data, whose range\nerror does not accumulate over time, to first estimate the relative positions\nbetween robots and then correct the positions of each robot, thus reducing\nlong-term drift errors, (ii) we design adaptive weights for robot pose\ncorrection by considering the characteristics of the sensor data and\nvisual-inertial odometry estimates. The proposed method has been validated in\nreal-world experiments, showing a substantial performance increase compared\nwith state-of-the-art algorithms.", "AI": {"tldr": "本文提出了一种名为 SaWa-ML 的新型视觉-惯性-测距多机器人定位方法，通过利用UWB传感器数据进行几何结构感知位姿校正和基于权重自适应的鲁棒定位，有效减少了长期漂移。", "motivation": "现有的多机器人定位方法在优化过程中未能充分考虑个体机器人里程计估计和机器人间距离测量的特性，且过度依赖个体里程计精度，导致长期漂移误差的累积。", "method": "SaWa-ML方法通过以下两点实现：1) 利用UWB传感器数据（其测距误差不随时间累积）首先估计机器人间的相对位置，然后校正每个机器人的位置，从而减少长期漂移误差。2) 考虑传感器数据和视觉-惯性里程计估计的特性，设计了自适应权重用于机器人位姿校正。", "result": "该方法在真实世界实验中得到了验证，与现有最先进的算法相比，性能有显著提升。", "conclusion": "SaWa-ML通过结合UWB传感器和自适应权重机制，成功解决了多机器人定位中长期漂移和鲁棒性不足的问题，实现了更精确和稳定的多机器人定位。"}}
{"id": "2507.13373", "categories": ["cs.CV", "I.4.8; I.2.10; H.5.1; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.13373", "abs": "https://arxiv.org/abs/2507.13373", "authors": ["Xiaojian Lin", "Wenxin Zhang", "Yuchu Jiang", "Wangyu Wu", "Yiran Guo", "Kangxu Wang", "Zongzheng Zhang", "Guijin Wang", "Lei Jin", "Hao Zhao"], "title": "Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection", "comment": "10 pages, 6 figures. Supplementary material: 8 pages, 7 figures.\n  Accepted at ACM Multimedia 2025", "summary": "Hierarchical feature representations play a pivotal role in computer vision,\nparticularly in object detection for autonomous driving. Multi-level semantic\nunderstanding is crucial for accurately identifying pedestrians, vehicles, and\ntraffic signs in dynamic environments. However, existing architectures, such as\nYOLO and DETR, struggle to maintain feature consistency across different scales\nwhile balancing detection precision and computational efficiency. To address\nthese challenges, we propose Butter, a novel object detection framework\ndesigned to enhance hierarchical feature representations for improving\ndetection robustness. Specifically, Butter introduces two key innovations:\nFrequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which\nrefines multi-scale feature consistency by leveraging adaptive frequency\nfiltering to enhance structural and boundary precision, and Progressive\nHierarchical Feature Fusion Network (PHFFNet) Module, which progressively\nintegrates multi-level features to mitigate semantic gaps and strengthen\nhierarchical feature learning. Through extensive experiments on BDD100K, KITTI,\nand Cityscapes, Butter demonstrates superior feature representation\ncapabilities, leading to notable improvements in detection accuracy while\nreducing model complexity. By focusing on hierarchical feature refinement and\nintegration, Butter provides an advanced approach to object detection that\nachieves a balance between accuracy, deployability, and computational\nefficiency in real-time autonomous driving scenarios. Our model and\nimplementation are publicly available at https://github.com/Aveiro-Lin/Butter,\nfacilitating further research and validation within the autonomous driving\ncommunity.", "AI": {"tldr": "Butter是一种新型目标检测框架，通过增强分层特征表示，提高了自动驾驶场景下的检测鲁棒性，并在精度、部署性和计算效率之间取得了平衡。", "motivation": "现有目标检测架构（如YOLO和DETR）在处理自动驾驶场景中的多尺度特征时，难以在保持特征一致性的同时兼顾检测精度和计算效率，导致对动态环境中行人、车辆和交通标志的准确识别面临挑战。", "method": "本文提出了Butter框架，包含两项主要创新：1. 频率自适应特征一致性增强组件（FAFCE），利用自适应频率滤波细化多尺度特征一致性，增强结构和边界精度。2. 渐进式分层特征融合网络模块（PHFFNet），逐步整合多级特征，弥合语义鸿沟并强化分层特征学习。", "result": "在BDD100K、KITTI和Cityscapes数据集上的大量实验表明，Butter展示了卓越的特征表示能力，显著提高了检测精度，同时降低了模型复杂度。", "conclusion": "Butter通过专注于分层特征的精炼和整合，为目标检测提供了一种先进方法，在实时自动驾驶场景中实现了精度、可部署性和计算效率之间的良好平衡。"}}
{"id": "2507.13411", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13411", "abs": "https://arxiv.org/abs/2507.13411", "authors": ["Nur A Zarin Nishat", "Andrea Coletta", "Luigi Bellomarini", "Kossi Amouzouvi", "Jens Lehmann", "Sahar Vahdati"], "title": "Aligning Knowledge Graphs and Language Models for Factual Accuracy", "comment": null, "summary": "Large language models like GPT-4, Gemini, and Claude have transformed natural\nlanguage processing (NLP) tasks such as question answering, dialogue\ngeneration, summarization, and so forth; yet their susceptibility to\nhallucination stands as one of the major challenges. Among numerous approaches\nto overcome this challenge, integration of Knowledge Graphs (KGs) into language\nmodels has emerged as a promising solution as it provides structured, reliable,\ndomain-specific, and up-to-date external information to the language models. In\nthis paper, we introduce ALIGNed-LLM, a simple yet effective approach to\nimprove language models' factuality via a lean strategy to infuse KGs into the\nlatent space of language models inspired by LLaVA where visual and textual\ninformation is infused. We use embeddings from a pre-trained Knowledge Graph\nEmbedding (KGE) model, such as TransE, and a trainable projection layer to\nalign entity and text embeddings. This alignment enables the language model to\ndistinguish between similar entities improving factual grounding and reducing\nhallucination. We tested our approach on three popular questions-answering\nbenchmark datasets alongside language models of varying sizes, showing\nsignificant improvement. Furthermore, we applied our approach to a real-world\nfinancial use case from a large central bank in Europe, which demands high\naccuracy and precision, demonstrating a substantial improvement of the LLM\nanswers.", "AI": {"tldr": "本文提出ALIGNed-LLM，一种通过将知识图谱（KGs）简洁有效地融入大语言模型（LLMs）潜在空间，以提高其事实性和减少幻觉的方法。", "motivation": "大语言模型（如GPT-4、Gemini等）在NLP任务中表现出色，但存在幻觉问题。将知识图谱集成到语言模型中被认为是一个有前景的解决方案，因为它能提供结构化、可靠且最新的外部信息。", "method": "引入ALIGNed-LLM，该方法受LLaVA启发，采用精益策略将知识图谱融入语言模型的潜在空间。具体来说，使用预训练的知识图谱嵌入（KGE）模型（如TransE）的嵌入，并通过一个可训练的投影层来对齐实体和文本嵌入。这种对齐使语言模型能够区分相似实体，从而改善事实基础并减少幻觉。", "result": "该方法在三个流行的问答基准数据集上对不同大小的语言模型进行了测试，显示出显著的性能提升。此外，将其应用于欧洲一家大型中央银行的真实金融用例中，也证明了LLM答案的实质性改进，满足了高准确性和精确性的要求。", "conclusion": "ALIGNed-LLM通过将知识图谱有效融入LLM的潜在空间，并对齐实体与文本嵌入，成功提高了语言模型的事实性，减少了幻觉，并在基准测试和真实世界应用中均取得了显著效果。"}}
{"id": "2507.13874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13874", "abs": "https://arxiv.org/abs/2507.13874", "authors": ["Mateusz Bystroński", "Mikołaj Hołysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery", "comment": null, "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.", "AI": {"tldr": "本文提出一种模型无关的潜在空间构思框架，通过在思想的连续嵌入空间中导航，实现可控、可扩展的创新，旨在克服大型语言模型在生成新颖且相关想法方面的局局限性。", "motivation": "大型语言模型（LLMs）在生成既新颖又相关的想法方面仍面临核心挑战，倾向于复制训练模式，需要大量提示工程，且现有解决方案（如领域特定启发式和结构化提示）不够通用和灵活。", "method": "本文提出一种模型无关的潜在空间构思框架。该框架通过导航思想的连续嵌入空间来实现受控和可扩展的创造力，无需手工规则，并且能够轻松适应不同的领域、输入格式和创意任务。", "result": "论文介绍了该方法的早期原型，概述了其概念框架，并展示了初步结果，突出其作为人机协作通用共同构思器的潜力。", "conclusion": "该潜在空间构思框架具有作为人机协作中通用共同构思器的巨大潜力，能够克服现有AI在创新性方面的局限性，实现可控且可扩展的创造力。"}}
{"id": "2507.13982", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.13982", "abs": "https://arxiv.org/abs/2507.13982", "authors": ["Yanni Jiwan-Mercier", "Barış Dönmez", "Güneş Karabulut-Kurt", "Sébastien Loranger"], "title": "Diffraction and Scattering Modeling for Laser Power Beaming in Lunar Environment", "comment": "10 pages, 8 figures", "summary": "Reliable energy delivery is a critical requirement for\n  long-term lunar missions, particularly in regions with limited\n  solar access, such as polar craters and during extended lunar\n  nights. Optical Power Beaming (OPB) using high-power lasers\n  offers a promising alternative to conventional solar power, but\n  the effects of suspended lunar dust on beam propagation remain\n  poorly understood. This study introduces a detailed simulation\n  model that incorporates both diffraction and height-dependent\n  scattering by the electrostatically suspended lunar regolith. Un like prior\napproaches, which assumed uniform dust layers or\n  center-to-center transmission loss, our model uses generalized\n  diffraction theory and refractive index gradients derived from\n  particle density to assess beam deformation and attenuation. The\n  results show that even in ground-to-ground scenarios, lunar dust\n  significantly degrades energy transfer efficiency, dropping from\n  57% to 3.7% over 50 km in dust-free vs. dusty conditions with\n  175 nm particles. Increasing the particle size to 250 nm limits the\n  viable transmission range to below 30 km at 6% efficiency. The\n  study further demonstrates that raising the laser source height\n  can improve efficiency, achieving 91% for a distance of 5 km\n  and 25% at 50 km when the source is positioned 12 m above\n  ground. These findings underscore the importance of system\n  elevation and dust modeling in lunar OPB design and reveal\n  the mission-critical role of particle size distribution, especially in\n  environments disturbed by human activity.", "AI": {"tldr": "研究表明月球尘埃显著降低了光学功率束传输效率，通过提高激光源高度可有效缓解。", "motivation": "月球长期任务，特别是在太阳光照有限区域，需要可靠的能源传输。光学功率束（OPB）是一种有前景的替代方案，但月球尘埃对光束传播的影响尚不明确。", "method": "引入了一个详细的仿真模型，该模型结合了静电悬浮月球风化层引起的衍射和高度依赖散射。与之前假设均匀尘埃层或中心到中心传输损耗的方法不同，本模型使用广义衍射理论和基于粒子密度导出的折射率梯度来评估光束变形和衰减。", "result": "在无尘埃与有尘埃（175纳米颗粒）条件下，50公里距离内的能量传输效率从57%降至3.7%。当粒子尺寸增加到250纳米时，有效传输范围限制在30公里以下，效率为6%。将激光源高度提高至离地12米，在5公里距离内效率可达91%，50公里距离内可达25%。", "conclusion": "研究强调了系统高度和尘埃建模在月球OPB设计中的重要性，并揭示了粒子尺寸分布对任务的关键作用，尤其是在人类活动干扰的环境中。"}}
{"id": "2507.13993", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13993", "abs": "https://arxiv.org/abs/2507.13993", "authors": ["Ningyong Wu", "Jinzhi Wang", "Wenhong Zhao", "Chenzhan Yu", "Zhigang Xiu", "Duwei Dai"], "title": "OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models", "comment": null, "summary": "The growing volume of medical imaging data has increased the need for\nautomated diagnostic tools, especially for musculoskeletal injuries like rib\nfractures, commonly detected via CT scans. Manual interpretation is\ntime-consuming and error-prone. We propose OrthoInsight, a multi-modal deep\nlearning framework for rib fracture diagnosis and report generation. It\nintegrates a YOLOv9 model for fracture detection, a medical knowledge graph for\nretrieving clinical context, and a fine-tuned LLaVA language model for\ngenerating diagnostic reports. OrthoInsight combines visual features from CT\nimages with expert textual data to deliver clinically useful outputs. Evaluated\non 28,675 annotated CT images and expert reports, it achieves high performance\nacross Diagnostic Accuracy, Content Completeness, Logical Coherence, and\nClinical Guidance Value, with an average score of 4.28, outperforming models\nlike GPT-4 and Claude-3. This study demonstrates the potential of multi-modal\nlearning in transforming medical image analysis and providing effective support\nfor radiologists.", "AI": {"tldr": "OrthoInsight是一个多模态深度学习框架，用于肋骨骨折的诊断和报告生成，通过整合YOLOv9、医学知识图谱和LLaVA模型，实现了高准确性和全面的临床报告。", "motivation": "医疗影像数据量不断增长，导致人工诊断（如肋骨骨折CT扫描）耗时且易出错，急需自动化诊断工具。", "method": "OrthoInsight框架整合了：1) YOLOv9模型用于骨折检测；2) 医学知识图谱用于检索临床上下文；3) 经过微调的LLaVA语言模型用于生成诊断报告。它结合了CT图像的视觉特征和专家文本数据。", "result": "在28,675张带注释的CT图像和专家报告上进行评估，OrthoInsight在诊断准确性、内容完整性、逻辑连贯性和临床指导价值方面表现出色，平均得分4.28，优于GPT-4和Claude-3等模型。", "conclusion": "这项研究展示了多模态学习在转化医学图像分析和为放射科医生提供有效支持方面的巨大潜力。"}}
{"id": "2507.13729", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13729", "abs": "https://arxiv.org/abs/2507.13729", "authors": ["Yu Yao", "Salil Bhatnagar", "Markus Mazzola", "Vasileios Belagiannis", "Igor Gilitschenski", "Luigi Palmieri", "Simon Razniewski", "Marcel Hallgarten"], "title": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework", "comment": null, "summary": "Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually.", "AI": {"tldr": "本文提出了一种基于LLM代理的框架，利用自然语言描述来增强真实世界交通场景，以解决自动驾驶测试中罕见关键场景数据稀缺、数据驱动生成缺乏控制以及手动增强不可扩展的问题。", "motivation": "自动驾驶规划器的测试和评估面临挑战，因为罕见但关键的场景难以通过纯粹的真实世界数据收集来捕获。数据驱动模型生成场景需要大量训练数据且缺乏细粒度控制，同时可能引入分布偏移。现有的通过增强原始测试集场景的方法依赖于领域专家手动操作，无法满足大规模评估的需求。", "method": "本文引入了一种新颖的基于LLM代理的框架，通过自然语言描述来增强真实世界的交通场景。其核心创新在于采用代理设计，实现了对输出的细粒度控制，即使使用较小、成本效益高的LLM也能保持高性能。", "result": "广泛的人类专家评估表明，该框架能够准确遵循用户意图，生成高质量的增强场景，其质量可与手动创建的场景相媲美。", "conclusion": "该LLM代理框架为自动驾驶系统评估中交通场景的增强提供了一种可扩展、可控且高质量的解决方案，克服了现有方法的局限性。"}}
{"id": "2507.13374", "categories": ["cs.CV", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13374", "abs": "https://arxiv.org/abs/2507.13374", "authors": ["Kevin Dela Rosa"], "title": "Smart Routing for Multimodal Video Retrieval: When to Search What", "comment": "Accepted to ICCV 2025 Multimodal Representation and Retrieval\n  Workshop", "summary": "We introduce ModaRoute, an LLM-based intelligent routing system that\ndynamically selects optimal modalities for multimodal video retrieval. While\ndense text captions can achieve 75.9% Recall@5, they require expensive offline\nprocessing and miss critical visual information present in 34% of clips with\nscene text not captured by ASR. By analyzing query intent and predicting\ninformation needs, ModaRoute reduces computational overhead by 41% while\nachieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR\n(speech), OCR (text), and visual indices, averaging 1.78 modalities per query\nversus exhaustive 3.0 modality search. Evaluation on 1.8M video clips\ndemonstrates that intelligent routing provides a practical solution for scaling\nmultimodal retrieval systems, reducing infrastructure costs while maintaining\ncompetitive effectiveness for real-world deployment.", "AI": {"tldr": "ModaRoute是一个基于LLM的智能路由系统，用于多模态视频检索，它能动态选择最优模态，在降低计算开销的同时保持较高的检索效率。", "motivation": "现有方法（如密集文本字幕）虽然能达到75.9%的Recall@5，但需要昂贵的离线处理，并且会遗漏34%视频片段中ASR未捕获的关键视觉信息（如场景文本），因此需要一个更高效且能充分利用多模态信息的检索系统。", "method": "ModaRoute使用GPT-4.1分析查询意图并预测信息需求，动态地将查询路由到ASR（语音）、OCR（文本）和视觉索引。它平均每个查询使用1.78种模态，而非穷举的3.0种模态搜索。", "result": "ModaRoute将计算开销降低了41%，同时实现了60.9%的Recall@5。该系统在180万个视频片段上进行了评估。", "conclusion": "智能路由（ModaRoute）为扩展多模态检索系统提供了一个实用的解决方案，它在降低基础设施成本的同时，为实际部署保持了有竞争力的效果。"}}
{"id": "2507.13474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13474", "abs": "https://arxiv.org/abs/2507.13474", "authors": ["Liang Lin", "Zhihao Xu", "Xuehai Tang", "Shi Liu", "Biyu Zhou", "Fuqing Zhu", "Jizhong Han", "Songlin Hu"], "title": "Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers", "comment": null, "summary": "The safety of large language models (LLMs) has garnered significant research\nattention. In this paper, we argue that previous empirical studies demonstrate\nLLMs exhibit a propensity to trust information from authoritative sources, such\nas academic papers, implying new possible vulnerabilities. To verify this\npossibility, a preliminary analysis is designed to illustrate our two findings.\nBased on this insight, a novel jailbreaking method, Paper Summary Attack\n(\\llmname{PSA}), is proposed. It systematically synthesizes content from either\nattack-focused or defense-focused LLM safety paper to construct an adversarial\nprompt template, while strategically infilling harmful query as adversarial\npayloads within predefined subsections. Extensive experiments show significant\nvulnerabilities not only in base LLMs, but also in state-of-the-art reasoning\nmodel like Deepseek-R1. PSA achieves a 97\\% attack success rate (ASR) on\nwell-aligned models like Claude3.5-Sonnet and an even higher 98\\% ASR on\nDeepseek-R1. More intriguingly, our work has further revealed diametrically\nopposed vulnerability bias across different base models, and even between\ndifferent versions of the same model, when exposed to either attack-focused or\ndefense-focused papers. This phenomenon potentially indicates future research\nclues for both adversarial methodologies and safety alignment.Code is available\nat https://github.com/233liang/Paper-Summary-Attack", "AI": {"tldr": "本文提出了一种名为“论文摘要攻击”（PSA）的新型越狱方法，利用大型语言模型（LLMs）对权威信息（如学术论文）的信任，通过合成论文内容并嵌入恶意查询来生成有害输出，实现了极高的攻击成功率，并揭示了不同模型在面对攻击型或防御型论文时的不同漏洞偏向。", "motivation": "先前的研究表明，大型语言模型（LLMs）倾向于信任来自权威来源（如学术论文）的信息。本文旨在验证这种信任是否构成新的潜在漏洞，并基于此开发一种攻击方法。", "method": "本文设计了一种名为“论文摘要攻击”（PSA）的新型越狱方法。该方法系统地综合了攻击型或防御型LLM安全论文的内容，构建对抗性提示模板，并策略性地在预定义的子部分中注入有害查询作为对抗性有效载荷。研究还进行了一项初步分析来验证LLM对权威信息的信任。", "result": "实验结果显示，PSA不仅对基础LLM有效，对如Deepseek-R1等先进推理模型也表现出显著的漏洞。PSA在Claude3.5-Sonnet等良好对齐的模型上取得了97%的攻击成功率（ASR），在Deepseek-R1上更是高达98%。更值得注意的是，研究发现不同基础模型之间，甚至同一模型的不同版本之间，在暴露于攻击型或防御型论文时，会展现出截然相反的漏洞偏向。", "conclusion": "PSA方法成功揭示了大型语言模型，包括先进的对齐模型，在面对利用其对权威信息信任的攻击时存在的严重漏洞。研究发现的漏洞偏向现象为未来的对抗性方法和安全对齐研究提供了重要的线索。"}}
{"id": "2507.13956", "categories": ["cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13956", "abs": "https://arxiv.org/abs/2507.13956", "authors": ["Yutao Jin", "Haowen Xiao", "Jielei Chu", "Fengmao Lv", "Yuxiao Li", "Tianrui Li"], "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction", "comment": null, "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.", "AI": {"tldr": "该研究提出了一种名为ADPC的视觉-语言因果干预框架，利用MRI、fMRI图像和LLM生成的文本数据，通过因果干预消除混杂因素，实现了对认知正常(CN)、轻度认知障碍(MCI)和阿尔茨海默病(AD)病例的精准分类，并达到了最先进的性能。", "motivation": "轻度认知障碍(MCI)是阿尔茨海默病(AD)的前驱阶段，早期识别和干预能有效延缓疾病进展。然而，AD诊断面临挑战，主要源于多模态数据选择偏差和变量间复杂关系导致的混杂因素，这些因素可能使非因果模型捕获虚假相关性，导致结果不可靠。", "method": "提出了一种新颖的视觉-语言因果干预框架ADPC。该框架利用大型语言模型(LLM)将临床数据总结为结构化文本，即使面对不完整或不均匀的数据集也能保持输出一致性。ADPC模型整合磁共振成像(MRI)、功能性MRI(fMRI)图像以及LLM生成的文本数据，对参与者进行认知正常(CN)、轻度认知障碍(MCI)和阿尔茨海默病(AD)的分类。通过因果干预，该框架隐式地消除了神经影像伪影和年龄相关生物标志物等混杂因素。", "result": "实验结果表明，该方法在区分CN/MCI/AD病例方面表现出色，在大多数评估指标上均达到了最先进(SOTA)的性能。", "conclusion": "该研究展示了将因果推理与多模态学习相结合在神经系统疾病诊断中的巨大潜力。"}}
{"id": "2507.14004", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14004", "abs": "https://arxiv.org/abs/2507.14004", "authors": ["Niloofar Nobahari", "Alireza Rezaee"], "title": "Smart fault detection in satellite electrical power system", "comment": null, "summary": "This paper presents an new approach for detecting in the electrical power\nsystem of satellites operating in Low Earth Orbit (LEO) without an Attitude\nDetermination and Control Subsystem (ADCS). Components of these systems are\nprone to faults, such as line-to-line faults in the photovoltaic subsystem,\nopen circuits, and short circuits in the DC-to-DC converter, as well as ground\nfaults in batteries. In the previous research has largely focused on detecting\nfaults in each components, such as photovoltaic arrays or converter systems,\ntherefore, has been limited attention given to whole electrical power system of\nsatellite as a whole system. Our approach addresses this gap by utilizing a\nMulti-Layer Perceptron (MLP) neural network model, which leverages input data\nsuch as solar radiation and surface temperature to predict current and load\noutputs. These machine learning techniques that classifiy use different\napproaches like Principal Component Analysis (PCA) and K-Nearest Neighbors\n(KNN), to classify faults effectively. The model presented achieves over 99%\naccuracy in identifying faults across multiple subsystems, marking a notable\nadvancement from previous approaches by offering a complete diagnostic solution\nfor the entire satellite power system. This thorough method boosts system\nreliability and helps lower the chances of mission failure", "AI": {"tldr": "本文提出了一种基于多层感知器（MLP）神经网络的新方法，用于检测低地球轨道（LEO）卫星（无姿态确定与控制子系统）整个电力系统中的故障。", "motivation": "卫星电力系统组件（如光伏子系统、DC-DC转换器、电池）易发生故障（如线对线故障、开路、短路、接地故障）。以往研究主要集中于检测单个组件的故障，对整个电力系统作为一个整体的故障检测关注不足。", "method": "利用多层感知器（MLP）神经网络模型，输入数据包括太阳辐射和表面温度，以预测电流和负载输出。结合主成分分析（PCA）和K-近邻（KNN）等机器学习技术进行故障分类。", "result": "该模型在识别多个子系统故障方面取得了超过99%的准确率，为整个卫星电力系统提供了完整的诊断解决方案。", "conclusion": "该方法显著提升了系统可靠性，并有助于降低任务失败的风险，代表了卫星电力系统故障诊断的显著进步。"}}
{"id": "2507.14046", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14046", "abs": "https://arxiv.org/abs/2507.14046", "authors": ["Hao Fang", "Hao Yu", "Sihao Teng", "Tao Zhang", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging", "comment": "11 pages, 9 figures", "summary": "Unsupervised learning methods, such as Deep Image Prior (DIP), have shown\ngreat potential in tomographic imaging due to their training-data-free nature\nand high generalization capability. However, their reliance on numerous network\nparameter iterations results in high computational costs, limiting their\npractical application, particularly in complex 3D or time-sequence tomographic\nimaging tasks. To overcome these challenges, we propose Deep Dynamic Image\nPrior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces\nthree key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal\nParameter Propagation (TPP), and a customized lightweight reconstruction\nbackbone, 3D-FastResUNet - to accelerate convergence, enforce temporal\ncoherence, and improve computational efficiency. Experimental results on both\nsimulated and clinical pulmonary datasets demonstrate that D2IP enables fast\nand accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)\nreconstruction. Compared to state-of-the-art baselines, D2IP delivers superior\nimage quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in\nERR, alongside significantly reduced computational time (7.1x faster),\nhighlighting its promise for clinical dynamic pulmonary imaging.", "AI": {"tldr": "针对深度图像先验（DIP）等无监督学习方法在断层成像中计算成本高的问题，本文提出了Deep Dynamic Image Prior (D2IP)框架，通过引入参数热启动、时间参数传播和轻量级网络，显著加速了3D时间序列图像重建，并在肺部数据上实现了更快的速度和更高的图像质量。", "motivation": "无监督学习方法（如DIP）在断层成像中展现出巨大潜力，但其依赖大量网络参数迭代导致计算成本高昂，尤其在复杂的3D或时间序列断层成像任务中，限制了实际应用。", "method": "本文提出Deep Dynamic Image Prior (D2IP)框架用于3D时间序列成像，引入了三项关键策略：1) 无监督参数热启动（Unsupervised Parameter Warm-Start, UPWS）以加速收敛；2) 时间参数传播（Temporal Parameter Propagation, TPP）以增强时间一致性；3) 定制的轻量级重建骨干网络3D-FastResUNet以提高计算效率。", "result": "在模拟和临床肺部数据集上的实验结果表明，D2IP能够实现快速准确的3D时间序列电阻抗断层成像（tsEIT）重建。与现有基线相比，D2IP提供了更优的图像质量（平均MSSIM提高24.8%，ERR降低8.1%），同时显著减少了计算时间（快7.1倍）。", "conclusion": "D2IP在计算速度、准确性和图像质量方面表现出色，预示着其在临床动态肺部成像中的巨大应用前景。"}}
{"id": "2507.13787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13787", "abs": "https://arxiv.org/abs/2507.13787", "authors": ["Doina Pisla", "Alexandru Pusca", "Andrei Caprariu", "Adrian Pisla", "Bogdan Gherman", "Calin Vaida", "Damien Chablat"], "title": "Design Analysis of an Innovative Parallel Robot for Minimally Invasive Pancreatic Surgery", "comment": null, "summary": "This paper focuses on the design of a parallel robot designed for robotic\nassisted minimally invasive pancreatic surgery. Two alternative architectures,\ncalled ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are\nproposed. Their kinematic schemes are presented, and the conceptual 3D CAD\nmodels are illustrated. Based on these, two Finite Element Method (FEM)\nsimulations were performed to determine which architecture has the higher\nstiffness. A workspace quantitative analysis is performed to further assess the\nusability of the two proposed parallel architectures related to the medical\ntasks. The obtained results are used to select the architecture which fit the\nrequired design criteria and will be used to develop the experimental model of\nthe surgical robot.", "AI": {"tldr": "本文设计了两种四自由度并联机器人架构（ATHENA-1和ATHENA-2），用于机器人辅助微创胰腺手术，并通过有限元分析和工作空间分析比较它们的刚度和可用性，以选择最佳架构。", "motivation": "为机器人辅助微创胰腺手术设计一种并联机器人。", "method": "提出了两种四自由度并联机器人架构（ATHENA-1和ATHENA-2），展示了它们的运动学方案和概念性三维CAD模型。通过有限元方法（FEM）模拟评估两种架构的刚度，并进行工作空间定量分析以评估其在医疗任务中的可用性。", "result": "FEM模拟确定了两种架构中刚度更高的一种。工作空间定量分析评估了两种架构的可用性。这些结果被用于选择符合设计标准的架构。", "conclusion": "根据刚度和工作空间分析结果，选择了一种架构来开发手术机器人的实验模型。"}}
{"id": "2507.13378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13378", "abs": "https://arxiv.org/abs/2507.13378", "authors": ["Yuqi Cheng", "Yunkang Cao", "Haiming Yao", "Wei Luo", "Cheng Jiang", "Hui Zhang", "Weiming Shen"], "title": "A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects", "comment": "27 pages, 7 figures", "summary": "Industrial defect detection is vital for upholding product quality across\ncontemporary manufacturing systems. As the expectations for precision,\nautomation, and scalability intensify, conventional inspection approaches are\nincreasingly found wanting in addressing real-world demands. Notable progress\nin computer vision and deep learning has substantially bolstered defect\ndetection capabilities across both 2D and 3D modalities. A significant\ndevelopment has been the pivot from closed-set to open-set defect detection\nframeworks, which diminishes the necessity for extensive defect annotations and\nfacilitates the recognition of novel anomalies. Despite such strides, a\ncohesive and contemporary understanding of industrial defect detection remains\nelusive. Consequently, this survey delivers an in-depth analysis of both\nclosed-set and open-set defect detection strategies within 2D and 3D\nmodalities, charting their evolution in recent years and underscoring the\nrising prominence of open-set techniques. We distill critical challenges\ninherent in practical detection environments and illuminate emerging trends,\nthereby providing a current and comprehensive vista of this swiftly progressing\nfield.", "AI": {"tldr": "本综述深入分析了工业缺陷检测领域，涵盖了2D和3D模态下的闭集与开集方法，追踪其演变，并强调了开集技术的日益重要性，同时探讨了实践挑战和新兴趋势。", "motivation": "传统检测方法难以满足现代制造业对精度、自动化和可扩展性的要求。尽管计算机视觉和深度学习在缺陷检测方面取得了显著进展，但目前缺乏对工业缺陷检测领域（特别是从闭集到开集框架的转变）的连贯和最新理解。", "method": "本文采用综述方法，对2D和3D模态下的闭集与开集缺陷检测策略进行了深入分析，梳理了其近年来的演变，强调了开集技术的兴起，并提炼了实际检测环境中的关键挑战和新兴趋势。", "result": "该综述提供了2D和3D模态下闭集和开集缺陷检测策略的深入分析，描绘了其近年来的发展轨迹，并突出了开集技术的日益突出。同时，它提炼了实际检测环境中的关键挑战，并阐明了新兴趋势。", "conclusion": "本综述为快速发展的工业缺陷检测领域提供了当前且全面的视角，有助于理解该领域的演变、面临的挑战以及未来的发展方向。"}}
{"id": "2507.13490", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13490", "abs": "https://arxiv.org/abs/2507.13490", "authors": ["Siqi Shen", "Mehar Singh", "Lajanugen Logeswaran", "Moontae Lee", "Honglak Lee", "Rada Mihalcea"], "title": "Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?", "comment": null, "summary": "There has been extensive research on assessing the value orientation of Large\nLanguage Models (LLMs) as it can shape user experiences across demographic\ngroups. However, several challenges remain. First, while the Multiple Choice\nQuestion (MCQ) setting has been shown to be vulnerable to perturbations, there\nis no systematic comparison of probing methods for value probing. Second, it is\nunclear to what extent the probed values capture in-context information and\nreflect models' preferences for real-world actions. In this paper, we evaluate\nthe robustness and expressiveness of value representations across three widely\nused probing strategies. We use variations in prompts and options, showing that\nall methods exhibit large variances under input perturbations. We also\nintroduce two tasks studying whether the values are responsive to demographic\ncontext, and how well they align with the models' behaviors in value-related\nscenarios. We show that the demographic context has little effect on the\nfree-text generation, and the models' values only weakly correlate with their\npreference for value-based actions. Our work highlights the need for a more\ncareful examination of LLM value probing and awareness of its limitations.", "AI": {"tldr": "本文评估了大型语言模型（LLMs）价值取向探测方法的鲁棒性和表达性，发现现有方法对输入扰动敏感，且探测到的价值观与模型在特定情境下的行为相关性较弱，提示需要更谨慎地评估LLM的价值观。", "motivation": "尽管LLMs的价值取向研究广泛，但仍存在挑战：多项选择题（MCQ）设置易受扰动影响，缺乏对不同探测方法的系统比较；不清楚探测到的价值观在多大程度上反映了上下文信息或模型对真实世界行为的偏好。", "method": "研究评估了三种广泛使用的探测策略的鲁棒性和表达性，通过改变提示和选项进行输入扰动测试。此外，引入了两项任务：研究价值观是否对人口统计学背景敏感，以及它们与模型在价值相关场景中行为的一致性。", "result": "所有探测方法在输入扰动下都表现出较大的方差。人口统计学背景对自由文本生成影响甚微。模型的价值观与其对基于价值行为的偏好仅存在微弱关联。", "conclusion": "研究强调了需要更仔细地审视LLM价值探测方法，并认识到其局限性。"}}
{"id": "2507.13958", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13958", "abs": "https://arxiv.org/abs/2507.13958", "authors": ["Pedro Cabalar", "Martín Diéguez", "François Olivier", "Torsten Schaub", "Igor Stéphan"], "title": "Towards Constraint Temporal Answer Set Programming", "comment": null, "summary": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.", "AI": {"tldr": "本文提出了一种新的时态和约束扩展的Here-and-There逻辑及其非单调平衡扩展，旨在解决ASP在处理高分辨率动态系统时的挑战。", "motivation": "逻辑编程方法（如ASP）在处理具有细粒度时间与数值分辨率的动态系统时面临显著挑战。", "method": "通过协同结合两种ASP基础扩展实现：线性时间Here-and-There逻辑（提供鲁棒的非单调时态推理能力）和带约束的Here-and-There逻辑（实现数值约束的直接集成和操作）。", "result": "建立了一个富有表达力的系统，这是首个专为ASP设计的非单调时态推理与约束方法，能够处理高分辨率的复杂动态系统。", "conclusion": "这项工作为在ASP范式内处理高分辨率复杂动态系统奠定了逻辑基础框架。"}}
{"id": "2507.14020", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14020", "abs": "https://arxiv.org/abs/2507.14020", "authors": ["Marwan Hassini", "Colette Mintsa-Eya", "Eduardo Redondo-Iglesias", "Pascal Venet"], "title": "Influence of Cell Position on the Capacity of Retired Batteries: Experimental and Statistical Studies", "comment": "5 pages, 4 figures, IECON 2025", "summary": "Understanding how batteries perform after automotive use is crucial to\ndetermining their potential for reuse. This article presents experimental\nresults aimed at advancing knowledge of retired battery performance. Three\nmodules extracted from electric vehicles were tested. Their performance was\nassessed, and the results were analyzed statistically using analysis of\nvariance (ANOVA). The 36 retired cells exhibited a high level of performance,\nalbeit with significant variation. On average, the cells had a 95% state of\nhealth capacity with a dispersion of 2.4%. ANOVA analysis suggests that cell\nperformance is not correlated with their position inside the module. These\nresults demonstrate the need to evaluate dispersion within retired batteries\nand to develop thermal management and balancing systems for second-life\nbatteries.", "AI": {"tldr": "该研究评估了电动汽车退役电池的性能，发现其仍保持高容量但存在显著差异，并强调了二次利用中热管理和均衡系统的必要性。", "motivation": "了解汽车退役电池的性能对于确定其再利用潜力至关重要。", "method": "测试了从电动汽车中提取的三个电池模块，对其性能进行了评估，并使用方差分析（ANOVA）对结果进行了统计分析。", "result": "36个退役电池表现出高水平的性能，平均健康状态容量为95%，但存在2.4%的显著离散度。ANOVA分析表明电池性能与其在模块内的位置无关。", "conclusion": "结果表明有必要评估退役电池内部的离散度，并为二次利用电池开发热管理和均衡系统。"}}
{"id": "2507.14102", "categories": ["eess.IV", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14102", "abs": "https://arxiv.org/abs/2507.14102", "authors": ["Shravan Venkatraman", "Pavan Kumar S", "Rakesh Raj Madavan", "Chandrakala S"], "title": "UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography", "comment": "18 pages, 10 figures, 5 tables, 2025 ICCV Workshops", "summary": "Accurate classification of computed tomography (CT) images is essential for\ndiagnosis and treatment planning, but existing methods often struggle with the\nsubtle and spatially diverse nature of pathological features. Current\napproaches typically process images uniformly, limiting their ability to detect\nlocalized abnormalities that require focused analysis. We introduce UGPL, an\nuncertainty-guided progressive learning framework that performs a\nglobal-to-local analysis by first identifying regions of diagnostic ambiguity\nand then conducting detailed examination of these critical areas. Our approach\nemploys evidential deep learning to quantify predictive uncertainty, guiding\nthe extraction of informative patches through a non-maximum suppression\nmechanism that maintains spatial diversity. This progressive refinement\nstrategy, combined with an adaptive fusion mechanism, enables UGPL to integrate\nboth contextual information and fine-grained details. Experiments across three\nCT datasets demonstrate that UGPL consistently outperforms state-of-the-art\nmethods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for\nkidney abnormality, lung cancer, and COVID-19 detection, respectively. Our\nanalysis shows that the uncertainty-guided component provides substantial\nbenefits, with performance dramatically increasing when the full progressive\nlearning pipeline is implemented. Our code is available at:\nhttps://github.com/shravan-18/UGPL", "AI": {"tldr": "UGPL是一种不确定性引导的渐进式学习框架，通过先识别诊断模糊区域再进行局部精细分析，显著提高了CT图像（如肾脏异常、肺癌、COVID-19）的分类准确性。", "motivation": "现有CT图像分类方法难以处理病理特征的细微性和空间多样性，且通常对图像进行统一处理，限制了其检测需要聚焦分析的局部异常的能力。", "method": "UGPL采用全局到局部的分析策略，首先识别诊断模糊区域，然后对关键区域进行详细检查。它使用证据深度学习量化预测不确定性，并通过非极大值抑制机制提取信息丰富的补丁，同时保持空间多样性。该方法结合渐进式细化策略和自适应融合机制，整合了上下文信息和细粒度细节。", "result": "UGPL在三个CT数据集上均优于现有最先进方法，在肾脏异常、肺癌和COVID-19检测中的准确率分别提高了3.29%、2.46%和8.08%。分析表明，不确定性引导组件提供了显著益处，当完整渐进式学习流程实施时，性能大幅提升。", "conclusion": "UGPL框架通过其不确定性引导的渐进式学习和全局到局部分析策略，有效解决了CT图像中细微和局部病理特征的分类挑战，取得了显著优于现有方法的性能。"}}
{"id": "2507.13871", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13871", "abs": "https://arxiv.org/abs/2507.13871", "authors": ["Mehul Anand", "Shishir Kolathaya"], "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models", "comment": "6 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2409.12616", "summary": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.", "AI": {"tldr": "该论文提出一个半监督框架，利用世界模型潜在空间中的控制障碍证书，在有限标记数据下合成安全的视觉运动策略，以解决视觉数据安全控制器合成中标记数据量大的问题。", "motivation": "从视觉数据合成安全控制器通常需要大量监督标记的安全关键数据，这在现实世界中往往不切实际。", "method": "引入一个半监督框架，该框架利用在世界模型潜在空间中学习到的控制障碍证书（CBCs）来合成安全的视觉运动策略。它使用有限的标记数据共同学习神经障碍函数和安全控制器，同时利用现代视觉Transformer的预测能力进行潜在动力学建模。", "result": "该方法成功实现了在有限标记数据下合成安全视觉运动策略，并利用世界模型的预测能力进行潜在动态建模。", "conclusion": "该框架通过结合潜在空间世界模型、控制障碍证书和半监督学习，实现了可扩展且数据高效的安全控制。"}}
{"id": "2507.13385", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13385", "abs": "https://arxiv.org/abs/2507.13385", "authors": ["Arjun Rao", "Esther Rolf"], "title": "Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery", "comment": "17 pages, 9 figures, 7 tables. Accepted to TerraBytes@ICML 2025", "summary": "A large variety of geospatial data layers is available around the world\nranging from remotely-sensed raster data like satellite imagery, digital\nelevation models, predicted land cover maps, and human-annotated data, to data\nderived from environmental sensors such as air temperature or wind speed data.\nA large majority of machine learning models trained on satellite imagery\n(SatML), however, are designed primarily for optical input modalities such as\nmulti-spectral satellite imagery. To better understand the value of using other\ninput modalities alongside optical imagery in supervised learning settings, we\ngenerate augmented versions of SatML benchmark tasks by appending additional\ngeographic data layers to datasets spanning classification, regression, and\nsegmentation. Using these augmented datasets, we find that fusing additional\ngeographic inputs with optical imagery can significantly improve SatML model\nperformance. Benefits are largest in settings where labeled data are limited\nand in geographic out-of-sample settings, suggesting that multi-modal inputs\nmay be especially valuable for data-efficiency and out-of-sample performance of\nSatML models. Surprisingly, we find that hard-coded fusion strategies\noutperform learned variants, with interesting implications for future work.", "AI": {"tldr": "通过将多种地理空间数据与光学卫星图像融合，显著提升了卫星机器学习（SatML）模型的性能，尤其在数据受限和地理样本外场景中效果更佳，且硬编码融合策略优于学习型策略。", "motivation": "大多数卫星机器学习（SatML）模型主要设计用于光学输入模态，研究旨在探究在监督学习中，将其他地理空间数据与光学图像结合使用的价值。", "method": "研究人员通过将额外的地理数据层附加到现有的SatML基准任务数据集（涵盖分类、回归和分割）上，生成了增强版数据集。在此基础上，他们评估了融合不同地理输入与光学图像的效果，并比较了硬编码融合策略和学习型融合策略的性能。", "result": "将额外的地理输入与光学图像融合可以显著提高SatML模型的性能。在标记数据有限和地理样本外（out-of-sample）场景中，这种融合带来的益处最大。令人惊讶的是，硬编码融合策略的表现优于学习型策略。", "conclusion": "多模态输入对于SatML模型的数据效率和样本外性能具有重要价值。简单的融合策略可能非常有效，这为未来的研究提供了有趣的启示。"}}
{"id": "2507.13501", "categories": ["cs.CL", "math.RA", "q-bio.NC", "91F20, 16Y60, 16T05, 92C20"], "pdf": "https://arxiv.org/pdf/2507.13501", "abs": "https://arxiv.org/abs/2507.13501", "authors": ["Matilde Marcolli", "Robert C. Berwick"], "title": "Encoding syntactic objects and Merge operations in function spaces", "comment": "40 pages, LaTeX, 4 png figures", "summary": "We provide a mathematical argument showing that, given a representation of\nlexical items as functions (wavelets, for instance) in some function space, it\nis possible to construct a faithful representation of arbitrary syntactic\nobjects in the same function space. This space can be endowed with a\ncommutative non-associative semiring structure built using the second Renyi\nentropy. The resulting representation of syntactic objects is compatible with\nthe magma structure. The resulting set of functions is an algebra over an\noperad, where the operations in the operad model circuits that transform the\ninput wave forms into a combined output that encodes the syntactic structure.\nThe action of Merge on workspaces is faithfully implemented as action on these\ncircuits, through a coproduct and a Hopf algebra Markov chain. The results\nobtained here provide a constructive argument showing the theoretical\npossibility of a neurocomputational realization of the core computational\nstructure of syntax. We also present a particular case of this general\nconstruction where this type of realization of Merge is implemented as a cross\nfrequency phase synchronization on sinusoidal waves. This also shows that Merge\ncan be expressed in terms of the successor function of a semiring, thus\nclarifying the well known observation of its similarities with the successor\nfunction of arithmetic.", "AI": {"tldr": "本文提出一种数学论证，展示了如何在函数空间中，通过将词汇项表示为函数（如小波），构建出任意句法对象的忠实表示，并赋予其特定的代数结构，从而实现句法核心计算结构的神经计算可能性。", "motivation": "研究旨在提供一个建设性论证，证明句法核心计算结构在理论上具有神经计算实现的可能。", "method": "将词汇项表示为函数（例如小波）在某个函数空间中；在该函数空间中构建句法对象的忠实表示；使用二阶Renyi熵赋予该空间一个可交换非结合半环结构，使其与岩浆结构兼容；将得到的函数集构建成一个在operad上的代数，其中operad操作模拟将输入波形转换为编码句法结构的组合输出的电路；通过一个余积和Hopf代数马尔可夫链，忠实地实现Merge操作对工作空间的作用；并提出了一个具体的实现案例，即通过正弦波的交叉频率相位同步实现Merge。", "result": "成功地在函数空间中构建了句法对象的忠实表示，并使其与岩浆结构兼容；所得到的函数集构成一个在operad上的代数；Merge操作被忠实地实现在这些电路上；为句法核心计算结构的神经计算实现提供了建设性论证；展示了Merge可以通过交叉频率相位同步实现，并能用半环的后继函数来表达，从而阐明了其与算术后继函数的相似性。", "conclusion": "本文提供了一个强大的理论框架，证明了句法核心计算结构（特别是Merge操作）在神经计算层面实现的可能性，通过将句法元素表示为函数并赋予其特定的代数结构，为理解大脑如何处理句法提供了新的视角。"}}
{"id": "2507.14032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14032", "abs": "https://arxiv.org/abs/2507.14032", "authors": ["Lam Nguyen", "Erika Barcelos", "Roger French", "Yinghui Wu"], "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models", "comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)", "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.", "AI": {"tldr": "KROMA是一个新颖的本体匹配框架，它利用检索增强生成（RAG）管道中的大型语言模型（LLMs）动态丰富语义上下文，并通过基于双相似性的概念匹配和轻量级本体细化来优化性能和效率。", "motivation": "现有的本体匹配（OM）系统通常依赖于手工规则或适应性有限的专用模型。研究旨在开发一种更具适应性、能动态丰富语义上下文的本体匹配方法。", "method": "KROMA框架结合了LLMs和RAG管道，以结构、词汇和定义知识增强OM任务的语义上下文。为提高性能和效率，它整合了基于双相似性的概念匹配（用于修剪候选概念）和轻量级本体细化步骤（显著减少LLM调用开销）。其优化技术包括目标知识检索、提示丰富和本体细化。", "result": "实验表明，将知识检索与上下文增强的LLMs相结合能显著提升本体匹配效果，优于经典的OM系统和尖端的基于LLM的方法，同时保持可比的通信开销。", "conclusion": "该研究强调了所提出的优化技术（目标知识检索、提示丰富和本体细化）对于大规模本体匹配的可行性和益处。"}}
{"id": "2507.14025", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14025", "abs": "https://arxiv.org/abs/2507.14025", "authors": ["Wataru Hashimoto", "Kazumune Hashimoto", "Masako Kishida", "Shigemasa Takai"], "title": "Reference-Free Iterative Learning Model Predictive Control with Neural Certificates", "comment": "This paper was submitted to IET Control Theory & Applications on May\n  19, 2025 (under review)", "summary": "In this paper, we propose a novel reference-free iterative learning model\npredictive control (MPC). In the proposed method, a certificate function based\non the concept of Control Lyapunov Barrier Function (CLBF) is learned using\ndata collected from past control executions and used to define the terminal set\nand cost in the MPC optimization problem at the current iteration. This scheme\nenables the progressive refinement of the MPC's terminal components over\nsuccessive iterations. Unlike existing methods that rely on mixed-integer\nprogramming and suffer from numerical difficulties, the proposed approach\nformulates the MPC optimization problem as a standard nonlinear program,\nenabling more efficient online computation. The proposed method satisfies key\nMPC properties, including recursive feasibility and asymptotic stability.\nAdditionally, we demonstrate that the performance cost is non-increasing with\nrespect to the number of iterations, under certain assumptions. Numerical\nexperiments including the simulation with PyBullet confirm that our control\nscheme iteratively enhances control performance and significantly improves\nonline computational efficiency compared to the existing methods.", "AI": {"tldr": "该论文提出了一种新型无参考迭代学习模型预测控制（MPC）方法，通过从历史数据学习控制Lyapunov障碍函数（CLBF）来定义MPC的终端集合和成本，并将其表述为非线性规划以提高计算效率，同时确保稳定性并逐步提升性能。", "motivation": "现有迭代学习MPC方法依赖于混合整数规划，导致数值困难和在线计算效率低下。", "method": "提出一种无参考迭代学习MPC。该方法利用历史控制执行数据学习基于控制Lyapunov障碍函数（CLBF）的证书函数，并用其定义当前迭代MPC优化问题中的终端集合和成本。MPC优化问题被表述为标准的非线性规划（NLP）。", "result": "该方法实现了MPC终端组件的逐次迭代改进。它能够更高效地进行在线计算，并满足递归可行性和渐近稳定性等关键MPC特性。在特定假设下，性能成本随迭代次数非递增。PyBullet仿真实验证实了控制性能的迭代增强和在线计算效率的显著提升。", "conclusion": "所提出的控制方案通过迭代学习和改进MPC的终端组件，并采用高效的非线性规划表述，显著提升了控制性能和在线计算效率，克服了现有方法的局限性。"}}
{"id": "2507.13387", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13387", "abs": "https://arxiv.org/abs/2507.13387", "authors": ["Chihiro Noguchi", "Takaki Yamamoto"], "title": "From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction", "comment": "Accepted to ICCV Workshop 2025", "summary": "Accurate perception of the surrounding environment is essential for safe\nautonomous driving. 3D occupancy prediction, which estimates detailed 3D\nstructures of roads, buildings, and other objects, is particularly important\nfor vision-centric autonomous driving systems that do not rely on LiDAR\nsensors. However, in 3D semantic occupancy prediction -- where each voxel is\nassigned a semantic label -- annotated LiDAR point clouds are required, making\ndata acquisition costly. In contrast, large-scale binary occupancy data, which\nonly indicate occupied or free space without semantic labels, can be collected\nat a lower cost. Despite their availability, the potential of leveraging such\ndata remains unexplored. In this study, we investigate the utilization of\nlarge-scale binary occupancy data from two perspectives: (1) pre-training and\n(2) learning-based auto-labeling. We propose a novel binary occupancy-based\nframework that decomposes the prediction process into binary and semantic\noccupancy modules, enabling effective use of binary occupancy data. Our\nexperimental results demonstrate that the proposed framework outperforms\nexisting methods in both pre-training and auto-labeling tasks, highlighting its\neffectiveness in enhancing 3D semantic occupancy prediction. The code is\navailable at https://github.com/ToyotaInfoTech/b2s-occupancy", "AI": {"tldr": "本文提出一个新框架，利用大规模二值占用数据（成本较低）进行预训练和自动标注，以提升3D语义占用预测的性能，尤其适用于视觉中心自动驾驶系统。", "motivation": "自动驾驶需要精确的环境感知，其中3D语义占用预测对纯视觉系统尤为重要。然而，语义标签数据（需激光雷达标注）获取成本高昂。与之相对，大规模二值占用数据（仅区分占用/空闲，无语义标签）成本较低且可用，但其潜力尚未被充分利用。", "method": "研究从两个方面利用大规模二值占用数据：1) 预训练，2) 基于学习的自动标注。提出了一个新颖的基于二值占用的框架，将预测过程分解为二值占用模块和语义占用模块，从而有效利用二值占用数据。", "result": "实验结果表明，所提出的框架在预训练和自动标注任务中均优于现有方法，证明了其在增强3D语义占用预测方面的有效性。", "conclusion": "所提出的框架能够有效利用大规模二值占用数据，显著提升了3D语义占用预测的性能，为解决语义标签数据获取成本高的问题提供了有效途径。"}}
{"id": "2507.13903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13903", "abs": "https://arxiv.org/abs/2507.13903", "authors": ["Ziliang Li", "Hongming Chen", "Yiyang Lin", "Biyu Ye", "Ximin Lyu"], "title": "AeroThrow: An Autonomous Aerial Throwing System for Precise Payload Delivery", "comment": null, "summary": "Autonomous aerial systems play an increasingly vital role in a wide range of\napplications, particularly for transport and delivery tasks in complex\nenvironments. In airdrop missions, these platforms face the dual challenges of\nabrupt control mode switching and inherent system delays along with control\nerrors. To address these issues, this paper presents an autonomous airdrop\nsystem based on an aerial manipulator (AM). The introduction of additional\nactuated degrees of freedom enables active compensation for UAV tracking\nerrors. By imposing smooth and continuous constraints on the parabolic landing\npoint, the proposed approach generates aerial throwing trajectories that are\nless sensitive to the timing of payload release. A hierarchical disturbance\ncompensation strategy is incorporated into the Nonlinear Model Predictive\nControl (NMPC) framework to mitigate the effects of sudden changes in system\nparameters, while the predictive capabilities of NMPC are further exploited to\nimprove the precision of aerial throwing. Both simulation and real-world\nexperimental results demonstrate that the proposed system achieves greater\nagility and precision in airdrop missions.", "AI": {"tldr": "该论文提出了一种基于空中机械手（AM）的自主空投系统，结合非线性模型预测控制（NMPC）和分层扰动补偿策略，以提高空投任务在复杂环境中的敏捷性和精度，解决控制模式切换和系统延迟问题。", "motivation": "自主航空系统在运输和配送任务中日益重要，尤其是在复杂环境中。空投任务面临控制模式突然切换、固有系统延迟以及控制误差等挑战，需要解决方案来提高其精度和可靠性。", "method": "该研究引入了空中机械手（AM），利用其额外的自由度主动补偿无人机跟踪误差。通过对抛物线着陆点施加平滑连续约束，生成对有效载荷释放时机不敏感的空中投掷轨迹。此外，在非线性模型预测控制（NMPC）框架中融入分层扰动补偿策略，以减轻系统参数突然变化的影响，并利用NMPC的预测能力提高空中投掷的精度。", "result": "仿真和实际实验结果均表明，所提出的系统在空投任务中实现了更高的敏捷性和精度。", "conclusion": "所提出的基于空中机械手的自主空投系统，通过有效应对控制模式切换、系统延迟和控制误差等挑战，显著提高了空投任务的敏捷性和精度。"}}
{"id": "2507.13386", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13386", "abs": "https://arxiv.org/abs/2507.13386", "authors": ["Yang Zhang", "Er Jin", "Yanfei Dong", "Yixuan Wu", "Philip Torr", "Ashkan Khakzar", "Johannes Stegmaier", "Kenji Kawaguchi"], "title": "Minimalist Concept Erasure in Generative Models", "comment": "ICML2025", "summary": "Recent advances in generative models have demonstrated remarkable\ncapabilities in producing high-quality images, but their reliance on\nlarge-scale unlabeled data has raised significant safety and copyright\nconcerns. Efforts to address these issues by erasing unwanted concepts have\nshown promise. However, many existing erasure methods involve excessive\nmodifications that compromise the overall utility of the model. In this work,\nwe address these issues by formulating a novel minimalist concept erasure\nobjective based \\emph{only} on the distributional distance of final generation\noutputs. Building on our formulation, we derive a tractable loss for\ndifferentiable optimization that leverages backpropagation through all\ngeneration steps in an end-to-end manner. We also conduct extensive analysis to\nshow theoretical connections with other models and methods. To improve the\nrobustness of the erasure, we incorporate neuron masking as an alternative to\nmodel fine-tuning. Empirical evaluations on state-of-the-art flow-matching\nmodels demonstrate that our method robustly erases concepts without degrading\noverall model performance, paving the way for safer and more responsible\ngenerative models.", "AI": {"tldr": "本文提出了一种新颖的极简主义概念擦除方法，仅基于最终生成输出的分布距离来解决生成模型中的安全和版权问题。该方法通过端到端反向传播和神经元掩蔽，能够在不降低整体模型性能的情况下，鲁棒地擦除不需要的概念。", "motivation": "生成模型虽然能产生高质量图像，但其对大规模未标记数据的依赖引发了严重的安全和版权问题。现有擦除方法常导致过度修改，损害模型整体效用。", "method": "提出了一种基于最终生成输出分布距离的极简主义概念擦除目标。推导了一个可微分优化的可处理损失函数，通过端到端反向传播实现。为提高擦除的鲁棒性，引入了神经元掩蔽作为模型微调的替代方案。此外，还进行了理论分析以建立与其他模型和方法的联系。", "result": "在最先进的流匹配模型上进行了实证评估，结果表明所提出的方法能够鲁棒地擦除概念，且不降低模型整体性能。", "conclusion": "该方法为开发更安全、更负责任的生成模型铺平了道路，因为它能在不影响模型效用的前提下有效擦除有害概念。"}}
{"id": "2507.13544", "categories": ["cs.CL", "68T50, 05C85, 68T05, 68R10", "I.2.7; I.2.4; H.3.3; I.5.0"], "pdf": "https://arxiv.org/pdf/2507.13544", "abs": "https://arxiv.org/abs/2507.13544", "authors": ["Mohamed Achref Ben Ammar", "Mohamed Taha Bennani"], "title": "A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows", "comment": null, "summary": "The analysis of conversational dynamics has gained increasing importance with\nthe rise of large language model-based systems, which interact with users\nacross diverse contexts. In this work, we propose a novel computational\nframework for constructing conversational graphs that capture the flow and\nstructure of loosely organized dialogues, referred to as quasi-patterned\nconversations. We introduce the Filter & Reconnect method, a novel graph\nsimplification technique that minimizes noise while preserving semantic\ncoherence and structural integrity of conversational graphs. Through\ncomparative analysis, we demonstrate that the use of large language models\ncombined with our graph simplification technique has resulted in semantic\nmetric S increasing by a factor of 2.06 compared to previous approaches while\nsimultaneously enforcing a tree-like structure with 0 {\\delta}-hyperbolicity,\nensuring optimal clarity in conversation modeling. This work provides a\ncomputational method for analyzing large-scale dialogue datasets, with\npractical applications related to monitoring automated systems such as\nchatbots, dialogue management tools, and user behavior analytics.", "AI": {"tldr": "本文提出了一种新的计算框架，用于构建对话图，以捕捉松散组织的“准模式对话”的流程和结构，并引入了“过滤与重连”图简化方法，显著提高了语义度量并确保了树状结构，适用于大规模对话数据分析。", "motivation": "随着基于大型语言模型的系统兴起，对话动态分析变得越来越重要，这些系统在不同上下文中与用户互动。", "method": "本文提出了一种构建对话图的计算框架，用于捕捉“准模式对话”的流和结构。引入了“过滤与重连”方法，这是一种新的图简化技术，旨在最小化噪声同时保持语义连贯性和结构完整性。该方法结合了大型语言模型和图简化技术。", "result": "通过比较分析，该方法使语义度量S比现有方法提高了2.06倍，同时强制执行了具有0 δ-双曲性的树状结构，确保了对话建模的最佳清晰度。", "conclusion": "这项工作提供了一种分析大规模对话数据集的计算方法，在监控聊天机器人等自动化系统、对话管理工具和用户行为分析方面具有实际应用价值。"}}
{"id": "2507.14077", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14077", "abs": "https://arxiv.org/abs/2507.14077", "authors": ["Temiloluwa Prioleau", "Baiying Lu", "Yanjun Cui"], "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions", "comment": "19 pages, 3 figures, 6 tables", "summary": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.", "AI": {"tldr": "本文介绍了Glucose-ML，一个包含10个公开糖尿病数据集的大型集合，旨在解决高质量数据不足的问题，并提供基准测试和开发鲁棒AI解决方案的建议。", "motivation": "高质量、大规模数据集的缺乏阻碍了糖尿病管理领域中鲁棒AI解决方案的开发。", "method": "研究者收集了10个在过去7年内发布的公开糖尿病数据集，形成Glucose-ML集合。他们对这些数据集进行了比较分析，并以血糖预测为例进行了案例研究，为所有数据集提供了短期血糖预测的基准。", "result": "Glucose-ML集合包含超过30万天的连续血糖监测数据，共计3800万个血糖样本，来自4个国家的2500多名参与者（包括T1D、T2D、糖尿病前期和非糖尿病人群）。研究发现，同一算法在不同数据集上开发/评估时，预测结果可能存在显著差异。他们提供了血糖预测的基准。", "conclusion": "该研究的结果为在糖尿病或更广泛的健康领域开发鲁棒AI解决方案提供了指导和建议。研究者公开了Glucose-ML集合中每个纵向糖尿病数据集的直接链接和相关代码。"}}
{"id": "2507.14052", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14052", "abs": "https://arxiv.org/abs/2507.14052", "authors": ["Mingdao Lin", "Max Bolderman", "Mircea Lazar"], "title": "Physics-guided gated recurrent units for inversion-based feedforward control", "comment": "8 pages", "summary": "Inversion-based feedforward control relies on an accurate model that\ndescribes the inverse system dynamics. The gated recurrent unit (GRU), which is\na recent architecture in recurrent neural networks, is a strong candidate for\nobtaining such a model from data. However, due to their black-box nature, GRUs\nface challenges such as limited interpretability and vulnerability to\noverfitting. Recently, physics-guided neural networks (PGNNs) have been\nintroduced, which integrate the prior physical model structure into the\nprediction process. This approach not only improves training convergence, but\nalso facilitates the learning of a physics-based model. In this work, we\nintegrate a GRU in the PGNN framework to obtain a PG-GRU, based on which we\nadopt a two-step approach to feedforward control design. First, we adopt stable\ninversion techniques to design a stable linear model of the inverse dynamics.\nThen, a GRU trained on the residual is tailored to inverse system\nidentification. The resulting PG-GRU feedforward controller is validated by\nmeans of real-life experiments on a two-mass spring-damper system, where it\ndemonstrates roughly a two-fold improvement compared to the linear feedforward\nand a preview-based GRU feedforward in terms of the integral absolute error.", "AI": {"tldr": "本文提出了一种基于物理引导门控循环单元（PG-GRU）的前馈控制器设计方法，通过结合物理模型和残差学习，显著提升了控制精度。", "motivation": "传统的基于倒置的前馈控制依赖精确模型，而门控循环单元（GRU）虽能从数据中学习，但存在黑箱性、可解释性差和易过拟合等问题。物理引导神经网络（PGNN）能结合物理先验知识，改善训练收敛并促进物理模型学习，因此有动机将GRU与PGNN结合以解决上述挑战。", "method": "研究者将GRU集成到PGNN框架中，形成PG-GRU。采用两步法设计前馈控制器：首先，利用稳定倒置技术设计一个稳定的逆动力学线性模型；然后，训练一个GRU来识别系统残差逆动力学。最后，在双质量弹簧阻尼系统上进行真实实验验证。", "result": "在双质量弹簧阻尼系统上进行的实际实验表明，所提出的PG-GRU前馈控制器在积分绝对误差方面，比线性前馈控制器和基于预览的GRU前馈控制器表现出大约两倍的性能提升。", "conclusion": "通过将GRU与物理引导神经网络框架结合，并采用两步法进行逆系统识别，PG-GRU前馈控制器在实际系统中展现出显著的性能优势，有效解决了传统GRU在控制应用中的局限性。"}}
{"id": "2507.13722", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13722", "abs": "https://arxiv.org/abs/2507.13722", "authors": ["Julia Laubmann", "Johannes Reschke"], "title": "Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box", "comment": null, "summary": "In today's digital age, concerns about the dangers of AI-generated images are\nincreasingly common. One powerful tool in this domain is StyleGAN (style-based\ngenerative adversarial networks), a generative adversarial network capable of\nproducing highly realistic synthetic faces. To gain a deeper understanding of\nhow such a model operates, this work focuses on analyzing the inner workings of\nStyleGAN's generator component. Key architectural elements and techniques, such\nas the Equalized Learning Rate, are explored in detail to shed light on the\nmodel's behavior. A StyleGAN model is trained using the PyTorch framework,\nenabling direct inspection of its learned weights. Through pruning, it is\nrevealed that a significant number of these weights can be removed without\ndrastically affecting the output, leading to reduced computational\nrequirements. Moreover, the role of the latent vector -- which heavily\ninfluences the appearance of the generated faces -- is closely examined. Global\nalterations to this vector primarily affect aspects like color tones, while\ntargeted changes to individual dimensions allow for precise manipulation of\nspecific facial features. This ability to finetune visual traits is not only of\nacademic interest but also highlights a serious ethical concern: the potential\nmisuse of such technology. Malicious actors could exploit this capability to\nfabricate convincing fake identities, posing significant risks in the context\nof digital deception and cybercrime.", "AI": {"tldr": "本研究深入分析了StyleGAN生成器的内部机制，通过权重修剪发现可显著减少计算需求，并揭示了潜在向量对生成人脸特征的精细控制能力，同时强调了该技术潜在的滥用风险。", "motivation": "在数字时代，AI生成图像（特别是StyleGAN生成的高度逼真人脸）带来的潜在危险日益受到关注。本研究旨在深入理解StyleGAN模型，特别是其生成器组件的内部工作原理。", "method": "详细探讨了StyleGAN的关键架构元素和技术，如Equalized Learning Rate。使用PyTorch框架训练StyleGAN模型，以便直接检查其学习到的权重。通过修剪（pruning）技术分析了权重的冗余性，并密切检查了潜在向量（latent vector）对生成图像的影响。", "result": "1. 发现StyleGAN的大量权重可以在不显著影响输出质量的情况下被移除，从而降低计算需求。2. 潜在向量对生成人脸的外观有显著影响：对该向量的全局改变主要影响色调，而对单个维度的有针对性改变则允许精确操纵特定的面部特征。", "conclusion": "StyleGAN微调视觉特征的能力不仅具有学术价值，也带来了严重的伦理担忧。恶意行为者可能利用此技术伪造逼真的虚假身份，从而在数字欺诈和网络犯罪中构成重大风险。"}}
{"id": "2507.13940", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13940", "abs": "https://arxiv.org/abs/2507.13940", "authors": ["Qingyi Chen", "Ahmed H. Qureshi"], "title": "NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized Safe Multi-Agent Motion Planning", "comment": null, "summary": "Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in\nrobotics. Despite substantial advancements, existing methods often face a\ndilemma. Decentralized algorithms typically rely on predicting the behavior of\nother agents, sharing contracts, or maintaining communication for safety, while\ncentralized approaches struggle with scalability and real-time decision-making.\nTo address these challenges, we introduce Neural Hamilton-Jacobi Reachability\nLearning (HJR) for Decentralized Multi-Agent Motion Planning. Our method\nprovides scalable neural HJR modeling to tackle high-dimensional configuration\nspaces and capture worst-case collision and safety constraints between agents.\nWe further propose a decentralized trajectory optimization framework that\nincorporates the learned HJR solutions to solve MAMP tasks in real-time. We\ndemonstrate that our method is both scalable and data-efficient, enabling the\nsolution of MAMP problems in higher-dimensional scenarios with complex\ncollision constraints. Our approach generalizes across various dynamical\nsystems, including a 12-dimensional dual-arm setup, and outperforms a range of\nstate-of-the-art techniques in successfully addressing challenging MAMP tasks.\nVideo demonstrations are available at https://youtu.be/IZiePX0p1Mc.", "AI": {"tldr": "本文提出了一种基于神经哈密顿-雅可比可达性学习（Neural HJR）的去中心化多智能体运动规划（MAMP）方法，以解决现有MAMP方法在安全性、可伸缩性和实时性方面的挑战。", "motivation": "现有的多智能体运动规划（MAMP）方法面临两难困境：去中心化算法依赖预测、契约或通信来保证安全，而中心化方法则难以扩展且无法实时决策。", "method": "引入神经哈密顿-雅可比可达性学习（Neural HJR）来建模高维配置空间中的最坏情况碰撞和安全约束。在此基础上，提出一个去中心化轨迹优化框架，该框架利用学习到的HJR解来实时解决MAMP任务。", "result": "该方法具有可伸缩性和数据效率，能够解决具有复杂碰撞约束的高维MAMP问题。它适用于各种动力学系统，包括12维双臂设置，并在解决具有挑战性的MAMP任务方面优于一系列现有技术。", "conclusion": "所提出的Neural HJR方法通过提供可伸缩、数据高效且通用的解决方案，成功应对了多智能体运动规划中的安全和实时性挑战，并超越了现有技术。"}}
{"id": "2507.13397", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13397", "abs": "https://arxiv.org/abs/2507.13397", "authors": ["Kaiyuan Zhai", "Juan Chen", "Chao Wang", "Zeyi Xu"], "title": "InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction", "comment": null, "summary": "Accurate pedestrian trajectory prediction is crucial for intelligent\napplications, yet it remains highly challenging due to the complexity of\ninteractions among pedestrians. Previous methods have primarily relied on\nrelative positions to model pedestrian interactions; however, they tend to\noverlook specific interaction patterns such as paired walking or conflicting\nbehaviors, limiting the prediction accuracy in crowded scenarios. To address\nthis issue, we propose InSyn (Interaction-Synchronization Network), a novel\nTransformer-based model that explicitly captures diverse interaction patterns\n(e.g., walking in sync or conflicting) while effectively modeling\ndirection-sensitive social behaviors. Additionally, we introduce a training\nstrategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue\nof initial-step divergence in numerical time-series prediction. Experiments on\nthe ETH and UCY datasets demonstrate that our model outperforms recent\nbaselines significantly, especially in high-density scenarios. Furthermore, the\nSSOS strategy proves effective in improving sequential prediction performance,\nreducing the initial-step prediction error by approximately 6.58%.", "AI": {"tldr": "该研究提出了InSyn模型，一个基于Transformer的网络，用于准确预测行人轨迹，通过显式捕捉多样化的交互模式和方向敏感的社会行为。此外，还引入了SSOS训练策略来缓解时间序列预测中常见的初始步发散问题。", "motivation": "行人轨迹预测对智能应用至关重要，但由于行人之间复杂的交互而极具挑战性。现有方法主要依赖相对位置建模交互，但常忽略特定的交互模式（如结伴行走或冲突行为），导致在拥挤场景中预测精度受限。同时，数值时间序列预测普遍存在初始步发散问题。", "method": "提出InSyn（Interaction-Synchronization Network），一个基于Transformer的新模型，旨在显式捕捉多样化的交互模式（如同步行走或冲突）并有效建模方向敏感的社会行为。此外，引入了名为Seq-Start of Seq (SSOS) 的训练策略，旨在缓解数值时间序列预测中常见的初始步发散问题。", "result": "在ETH和UCY数据集上的实验表明，InSyn模型显著优于现有基线方法，尤其是在高密度场景中。SSOS策略也被证明能有效提升序列预测性能，将初始步预测误差降低了约6.58%。", "conclusion": "InSyn模型通过有效捕捉行人多样化交互模式和方向敏感行为，显著提高了行人轨迹预测的准确性，尤其是在拥挤场景下。SSOS训练策略则有效解决了初始步发散问题，进一步提升了预测性能。"}}
{"id": "2507.13551", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13551", "abs": "https://arxiv.org/abs/2507.13551", "authors": ["Feng Chen", "Weizhe Xu", "Changye Li", "Serguei Pakhomov", "Alex Cohen", "Simran Bhola", "Sandy Yin", "Sunny X Tang", "Michael Mackinley", "Lena Palaniyappan", "Dror Ben-Zeev", "Trevor Cohen"], "title": "Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder", "comment": null, "summary": "Formal thought disorder (FTD), a hallmark of schizophrenia spectrum\ndisorders, manifests as incoherent speech and poses challenges for clinical\nassessment. Traditional clinical rating scales, though validated, are\nresource-intensive and lack scalability. Automated speech analysis with\nautomatic speech recognition (ASR) allows for objective quantification of\nlinguistic and temporal features of speech, offering scalable alternatives. The\nuse of utterance timestamps in ASR captures pause dynamics, which are thought\nto reflect the cognitive processes underlying speech production. However, the\nutility of integrating these ASR-derived features for assessing FTD severity\nrequires further evaluation. This study integrates pause features with semantic\ncoherence metrics across three datasets: naturalistic self-recorded diaries\n(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream\nnarratives (PsyCL, n = 43). We evaluated pause related features alongside\nestablished coherence measures, using support vector regression (SVR) to\npredict clinical FTD scores. Key findings demonstrate that pause features alone\nrobustly predict the severity of FTD. Integrating pause features with semantic\ncoherence metrics enhanced predictive performance compared to semantic-only\nmodels, with integration of independent models achieving correlations up to\n\\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best\n\\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance\ngains from semantic and pause features integration held consistently across all\ncontexts, though the nature of pause patterns was dataset-dependent. These\nfindings suggest that frameworks combining temporal and semantic analyses\nprovide a roadmap for refining the assessment of disorganized speech and\nadvance automated speech analysis in psychosis.", "AI": {"tldr": "本研究通过整合停顿特征和语义连贯性指标，显著提升了自动语音分析在预测精神分裂症谱系障碍中形式思维障碍（FTD）严重程度方面的表现。", "motivation": "传统的FTD临床评估方法资源密集且缺乏可扩展性。自动语音识别（ASR）技术能够客观量化语音的语言和时间特征，提供可扩展的替代方案。然而，将ASR衍生的特征（特别是停顿动态）用于评估FTD严重程度的有效性尚需进一步评估。", "method": "研究整合了ASR提取的停顿特征和语义连贯性指标，并在三个不同数据集（自然日记、图片描述、梦境叙述）上进行评估。使用支持向量回归（SVR）模型来预测临床FTD评分。", "result": "研究发现，单独的停顿特征就能有效预测FTD的严重程度。与仅使用语义模型的相比，整合停顿特征和语义连贯性指标显著提升了预测性能，最高相关性达到ρ = 0.649，严重病例检测AUC达到83.71%。性能提升在所有语境中均保持一致，尽管停顿模式因数据集而异。", "conclusion": "结合时间（停顿）和语义分析的框架为完善紊乱言语评估提供了路径，并推动了精神病学中自动化语音分析的发展。"}}
{"id": "2507.14097", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14097", "abs": "https://arxiv.org/abs/2507.14097", "authors": ["Hari Iyer", "Neel Macwan", "Atharva Jitendra Hude", "Heejin Jeong", "Shenghan Guo"], "title": "Generative AI-Driven High-Fidelity Human Motion Simulation", "comment": null, "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.", "AI": {"tldr": "提出G-AI-HMS，通过生成式AI集成文本到文本和文本到运动模型，显著提升了工业任务中人体运动模拟的保真度。", "motivation": "现有的人体运动模拟（HMS）方法在工业任务中存在运动保真度低的问题，限制了其在评估工人行为、安全和生产力方面的有效性。", "method": "本研究引入了Generative-AI-Enabled HMS (G-AI-HMS)。该方法整合了文本到文本和文本到运动模型，以增强物理任务的模拟质量。G-AI-HMS主要解决两个挑战：1) 使用与MotionGPT训练词汇对齐的大语言模型将任务描述转换为运动感知语言；2) 使用计算机视觉技术（姿态估计算法从实时视频中提取关节地标，并使用运动相似性度量）验证AI增强的运动与真实人体运动的一致性。", "result": "在涉及八项任务的案例研究中，AI增强的运动在大多数场景下比人工创建的描述表现出更低的误差。具体而言，在空间精度方面优于六项任务，在姿态归一化后的对齐方面优于四项任务，在整体时间相似性方面优于七项任务。统计分析表明，AI增强的提示显著（p < 0.0001）降低了关节误差和时间错位，同时保持了可比的姿态准确性。", "conclusion": "G-AI-HMS通过利用生成式AI，显著提高了人体运动模拟的质量和准确性，尤其在减少运动误差和时间错位方面表现出色，为工业任务评估提供了更可靠的工具。"}}
{"id": "2507.14073", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.14073", "abs": "https://arxiv.org/abs/2507.14073", "authors": ["Oumayma Khattabi", "Matteo Tacchi-Bénard", "Sorin Olaru"], "title": "Convex computation of regions of attraction from data using Sums-of-Squares programming", "comment": null, "summary": "The paper concentrates on the analysis of the region of attraction (ROA) for\nunknown autonomous dynamical systems. The aim is to explore a data-driven\napproach based on moment-sum-of-squares (SoS) hierarchy, which enables novel\nRoA outer approximations despite the reduced information on the structure of\nthe dynamics. The main contribution of this work is bypassing the system model\nand, consequently, the recurring constraint on its polynomial structure.\nNumerical experimentation showcases the influence of data on learned\napproximating sets, offering a promising outlook on the potential of this\nmethod.", "AI": {"tldr": "本文提出一种基于数据驱动的矩-平方和（SoS）层次方法，用于分析未知自治动力系统的吸引域（ROA），无需系统模型结构信息。", "motivation": "研究未知自治动力系统的吸引域分析，旨在开发一种数据驱动的方法，以克服传统方法对系统模型及其多项式结构的依赖。", "method": "采用基于矩-平方和（SoS）层次的数据驱动方法，实现对吸引域的外部近似，并避免了对系统模型及其多项式结构的显式要求。", "result": "该方法能够在信息受限的情况下实现新颖的吸引域外部近似。数值实验表明数据对学习到的近似集合有显著影响，展示了该方法的潜力。", "conclusion": "该方法在处理未知动力系统吸引域分析方面具有广阔前景，尤其在于其能够绕过系统模型及其多项式结构限制的能力。"}}
{"id": "2507.13852", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.13852", "abs": "https://arxiv.org/abs/2507.13852", "authors": ["Luigi Russo", "Francesco Mauro", "Babak Memar", "Alessandro Sebastianelli", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data", "comment": "Accepted at IEEE Joint Urban Remote Sensing Event (JURSE) 2025", "summary": "Building segmentation in urban areas is essential in fields such as urban\nplanning, disaster response, and population mapping. Yet accurately segmenting\nbuildings in dense urban regions presents challenges due to the large size and\nhigh resolution of satellite images. This study investigates the use of a\nQuanvolutional pre-processing to enhance the capability of the Attention U-Net\nmodel in the building segmentation. Specifically, this paper focuses on the\nurban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)\nimagery. In this work, Quanvolution was used to extract more informative\nfeature maps that capture essential structural details in radar imagery,\nproving beneficial for accurate building segmentation. Preliminary results\nindicate that proposed methodology achieves comparable test accuracy to the\nstandard Attention U-Net model while significantly reducing network parameters.\nThis result aligns with findings from previous works, confirming that\nQuanvolution not only maintains model accuracy but also increases computational\nefficiency. These promising outcomes highlight the potential of\nquantum-assisted Deep Learning frameworks for large-scale building segmentation\nin urban environments.", "AI": {"tldr": "本研究探索了将量子卷积预处理应用于注意力U-Net模型，以提高城市地区（突尼斯）利用SAR图像进行建筑物分割的能力，结果显示在保持准确性的同时显著减少了模型参数。", "motivation": "城市区域的建筑物分割对于城市规划、灾害响应和人口测绘至关重要。然而，由于卫星图像尺寸大、分辨率高，在密集城市区域准确分割建筑物面临挑战。", "method": "本研究采用量子卷积（Quanvolution）作为预处理步骤，以增强注意力U-Net模型进行建筑物分割的能力。具体来说，研究利用Sentinel-1合成孔径雷达（SAR）图像对突尼斯城市景观进行了研究。量子卷积用于提取更具信息量的特征图，以捕捉雷达图像中重要的结构细节。", "result": "初步结果表明，所提出的方法在测试准确性方面与标准注意力U-Net模型相当，但显著减少了网络参数。这一结果与以往研究的发现一致，证实了量子卷积不仅能保持模型准确性，还能提高计算效率。", "conclusion": "这些有前景的结果突显了量子辅助深度学习框架在城市环境中进行大规模建筑物分割的潜力。"}}
{"id": "2507.13969", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.13969", "abs": "https://arxiv.org/abs/2507.13969", "authors": ["Maria Eduarda Silva de Macedo", "Ana Paula Chiarelli de Souza", "Roberto Silvio Ubertino Rosso Jr.", "Yuri Kaszubowski Lopes"], "title": "A Minimalist Controller for Autonomously Self-Aggregating Robotic Swarms: Enabling Compact Formations in Multitasking Scenarios", "comment": "7 pages total (6 pages of content + 1 page of references). Short\n  paper manuscript submitted to TAROS 2025", "summary": "The deployment of simple emergent behaviors in swarm robotics has been\nwell-rehearsed in the literature. A recent study has shown how self-aggregation\nis possible in a multitask approach -- where multiple self-aggregation task\ninstances occur concurrently in the same environment. The multitask approach\nposes new challenges, in special, how the dynamic of each group impacts the\nperformance of others. So far, the multitask self-aggregation of groups of\nrobots suffers from generating a circular formation -- that is not fully\ncompact -- or is not fully autonomous. In this paper, we present a multitask\nself-aggregation where groups of homogeneous robots sort themselves into\ndifferent compact clusters, relying solely on a line-of-sight sensor. Our\nmultitask self-aggregation behavior was able to scale well and achieve a\ncompact formation. We report scalability results from a series of simulation\ntrials with different configurations in the number of groups and the number of\nrobots per group. We were able to improve the multitask self-aggregation\nbehavior performance in terms of the compactness of the clusters, keeping the\nproportion of clustered robots found in other studies.", "AI": {"tldr": "本文提出了一种多任务自聚集算法，使同质机器人群能够仅依靠视线传感器形成紧凑的集群，解决了现有方法形成非紧凑或非完全自主集群的问题。", "motivation": "现有的多任务自聚集方法存在生成非紧凑圆形队形或非完全自主的问题，且不同组的动态行为会相互影响，这促使研究者寻求更紧凑、更自主的解决方案。", "method": "研究人员提出了一种新的多任务自聚集行为，使同质机器人群能够仅依靠视线传感器进行自我分类，形成不同的紧凑集群。他们通过一系列模拟实验，测试了不同组数和每组机器人数量配置下的可扩展性。", "result": "该多任务自聚集行为表现出良好的可扩展性，并能实现紧凑的队形。与现有研究相比，该方法显著提高了集群的紧凑性，同时保持了相似的机器人聚集比例。", "conclusion": "该研究成功开发了一种可扩展、自主且能形成紧凑集群的多任务自聚集行为，为群体机器人协作提供了有效解决方案，并改善了集群的紧凑性。"}}
{"id": "2507.13401", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13401", "abs": "https://arxiv.org/abs/2507.13401", "authors": ["Shreya Kadambi", "Risheek Garrepalli", "Shubhankar Borse", "Munawar Hyatt", "Fatih Porikli"], "title": "MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing", "comment": "26 pages", "summary": "Despite the remarkable success of diffusion models in text-to-image\ngeneration, their effectiveness in grounded visual editing and compositional\ncontrol remains challenging. Motivated by advances in self-supervised learning\nand in-context generative modeling, we propose a series of simple yet powerful\ndesign choices that significantly enhance diffusion model capacity for\nstructured, controllable generation and editing. We introduce Masking-Augmented\nDiffusion with Inference-Time Scaling (MADI), a framework that improves the\neditability, compositionality and controllability of diffusion models through\ntwo core innovations. First, we introduce Masking-Augmented gaussian Diffusion\n(MAgD), a novel training strategy with dual corruption process which combines\nstandard denoising score matching and masked reconstruction by masking noisy\ninput from forward process. MAgD encourages the model to learn discriminative\nand compositional visual representations, thus enabling localized and\nstructure-aware editing. Second, we introduce an inference-time capacity\nscaling mechanism based on Pause Tokens, which act as special placeholders\ninserted into the prompt for increasing computational capacity at inference\ntime. Our findings show that adopting expressive and dense prompts during\ntraining further enhances performance, particularly for MAgD. Together, these\ncontributions in MADI substantially enhance the editability of diffusion\nmodels, paving the way toward their integration into more general-purpose,\nin-context generative diffusion architectures.", "AI": {"tldr": "本文提出了MADI框架，通过引入双重损坏训练策略（MAgD）和推理时容量扩展机制（Pause Tokens），显著提升了扩散模型在接地视觉编辑和组合控制方面的能力。", "motivation": "尽管扩散模型在文本到图像生成方面取得了显著成功，但在接地视觉编辑和组合控制方面仍面临挑战。研究动机来源于自监督学习和上下文生成建模的进展。", "method": "本文提出了MADI（Masking-Augmented Diffusion with Inference-Time Scaling）框架。核心创新包括：1. Masking-Augmented gaussian Diffusion (MAgD)：一种新颖的训练策略，结合了标准去噪分数匹配和通过掩蔽噪声输入进行的重建，旨在学习判别性和组合性视觉表示。2. 基于Pause Tokens的推理时容量扩展机制：在提示中插入特殊占位符以增加推理时的计算能力。此外，研究发现训练时使用表达性强且密集的提示能进一步提升性能。", "result": "MADI框架下的贡献显著增强了扩散模型的编辑能力。特别地，采用表达性强且密集的提示在训练期间进一步提升了性能，尤其对MAgD效果显著。", "conclusion": "MADI框架通过其创新设计，极大地提升了扩散模型的编辑能力，为将其集成到更通用、上下文感知的生成扩散架构中铺平了道路。"}}
{"id": "2507.13563", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13563", "abs": "https://arxiv.org/abs/2507.13563", "authors": ["Kirill Borodin", "Nikita Vasiliev", "Vasiliy Kudryavtsev", "Maxim Maslov", "Mikhail Gorodnichev", "Oleg Rogov", "Grach Mkrtchian"], "title": "A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models", "comment": "The work is still in progress", "summary": "Russian speech synthesis presents distinctive challenges, including vowel\nreduction, consonant devoicing, variable stress patterns, homograph ambiguity,\nand unnatural intonation. This paper introduces Balalaika, a novel dataset\ncomprising more than 2,000 hours of studio-quality Russian speech with\ncomprehensive textual annotations, including punctuation and stress markings.\nExperimental results show that models trained on Balalaika significantly\noutperform those trained on existing datasets in both speech synthesis and\nenhancement tasks. We detail the dataset construction pipeline, annotation\nmethodology, and results of comparative evaluations.", "AI": {"tldr": "本文介绍了Balalaika，一个包含超过2000小时高质量俄语语音的新数据集，旨在解决俄语语音合成中的特有挑战，并实验证明其在语音合成和增强任务中优于现有数据集。", "motivation": "俄语语音合成面临独特的挑战，包括元音弱化、辅音清化、可变重音模式、同形异义词歧义以及不自然的语调。", "method": "引入了Balalaika数据集，该数据集包含2000多小时的录音室质量俄语语音，并附有全面的文本标注，包括标点和重音标记。论文详细介绍了数据集的构建流程和标注方法。", "result": "实验结果表明，在Balalaika数据集上训练的模型在语音合成和增强任务中均显著优于在现有数据集上训练的模型。", "conclusion": "Balalaika数据集的引入及其在实验中的优异表现，证明了其能够有效提升俄语语音合成和增强的质量。"}}
{"id": "2507.14107", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14107", "abs": "https://arxiv.org/abs/2507.14107", "authors": ["Viraj Nishesh Darji", "Callie C. Liao", "Duoduo Liao"], "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment", "comment": null, "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.", "AI": {"tldr": "本研究探索了大型语言模型（LLMs）在解释无损评估（NDE）等高线图方面的能力，旨在自动化桥梁状况分析，提高效率和准确性，并提出了一个整合LLMs到桥梁检测工作流程的框架。", "motivation": "桥梁维护中的无损评估（NDE）数据解释耗时且需要专业知识，可能延迟决策。研究旨在利用LLMs自动化并改进这一过程。", "method": "本试点研究评估了多种LLMs（共9个）解释NDE等高线图的能力。通过专门设计的提示，LLMs被用于生成图像描述，并应用于5个不同的NDE等高线图。评估标准包括描述细节、缺陷识别、建议可行性和整体准确性。最佳模型的输出（来自4个LLM）再由另外5个LLM进行总结。", "result": "研究发现，9个模型中有4个提供了更好的图像描述，能有效涵盖桥梁状况的广泛主题。在总结方面，ChatGPT-4和Claude 3.5 Sonnet生成了更有效的摘要。结果表明LLMs能显著提高效率和准确性。", "conclusion": "LLMs在桥梁维护中通过并行图像标注和摘要功能，具有显著提高效率和准确性的潜力。这种创新方法有助于加快决策制定，提升基础设施管理和安全评估水平。"}}
{"id": "2507.14117", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.14117", "abs": "https://arxiv.org/abs/2507.14117", "authors": ["Aayushya Agarwal", "Larry Pileggi"], "title": "Integrating Forecasting Models Within Steady-State Analysis and Optimization", "comment": null, "summary": "Extreme weather variations and the increasing unpredictability of load\nbehavior make it difficult to determine power grid dispatches that are robust\nto uncertainties. While machine learning (ML) methods have improved the ability\nto model uncertainty caused by loads and renewables, accurately integrating\nthese forecasts and their sensitivities into steady-state analyses and\ndecision-making strategies remains an open challenge. Toward this goal, we\npresent a generalized methodology that seamlessly embeds ML-based forecasting\nengines within physics-based power flow and grid optimization tools. By\ncoupling physics-based grid modeling with black-box ML methods, we accurately\ncapture the behavior and sensitivity of loads and weather events by directly\nintegrating the inputs and outputs of trained ML forecasting models into the\nnumerical methods of power flow and grid optimization. Without fitting\nsurrogate load models, our approach obtains the sensitivities directly from\ndata to accurately predict the response of forecasted devices to changes in the\ngrid. Our approach combines the sensitivities of forecasted devices attained\nvia backpropagation and the sensitivities of physics-defined grid devices. We\ndemonstrate the efficacy of our method by showcasing improvements in\nsensitivity calculations and leveraging them to design a robust power dispatch\nthat improves grid reliability under stochastic weather events. Our approach\nenables the computation of system sensitivities to exogenous factors which\nsupports broader analyses that improve grid reliability in the presence of load\nvariability and extreme weather conditions.", "AI": {"tldr": "该研究提出一种通用方法，将机器学习预测引擎无缝嵌入到基于物理的电力流和电网优化工具中，以提高在不确定性下电网调度的鲁棒性和可靠性。", "motivation": "极端天气变化和负载行为的不可预测性使得确定对不确定性具有鲁棒性的电网调度变得困难。尽管机器学习方法提高了对负载和可再生能源引起的不确定性建模能力，但如何将这些预测及其敏感性准确地整合到稳态分析和决策策略中仍然是一个开放的挑战。", "method": "该方法通过将基于物理的电网建模与黑盒机器学习方法相结合，将训练好的机器学习预测模型的输入和输出直接整合到电力流和电网优化的数值方法中。它通过反向传播从数据中直接获取敏感性，无需拟合替代负载模型，并结合了预测设备（通过ML获得）和物理定义电网设备的敏感性。", "result": "该方法改进了敏感性计算，并利用这些改进设计了在随机天气事件下提高电网可靠性的鲁棒电力调度。它能够计算系统对外生因素的敏感性。", "conclusion": "该方法支持更广泛的分析，以在负载变异性和极端天气条件下提高电网可靠性，通过将机器学习预测及其敏感性准确地整合到电网运行中。"}}
{"id": "2507.13970", "categories": ["cs.RO", "cs.AI", "I.2; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.13970", "abs": "https://arxiv.org/abs/2507.13970", "authors": ["Casper Bröcheler", "Thomas Vroom", "Derrick Timmermans", "Alan van den Akker", "Guangzhi Tang", "Charalampos S. Kouzinopoulos", "Rico Möckel"], "title": "A segmented robot grasping perception neural network for edge AI", "comment": "Accepted by SMC 2025", "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.", "AI": {"tldr": "本文在RISC-V边缘芯片上实现并优化了一个基于热图的6自由度抓取姿态检测深度学习模型，验证了低功耗MCU在实时机器人抓取中的可行性。", "motivation": "机器人抓取需要精确感知和控制，深度神经网络表现出色。但在资源受限环境中，实现低延迟、低功耗的实时抓取推理是一个挑战。", "method": "在GAP9 RISC-V片上系统上实现了“热图引导抓取检测”框架，用于检测6自由度抓取姿态。通过输入维度缩减、模型分区和量化等硬件感知技术对模型进行了优化。", "result": "在GraspNet-1Billion基准测试上进行了实验评估，验证了完全片上推理的可行性。", "conclusion": "低功耗微控制器在实时、自主操纵方面具有巨大潜力。"}}
{"id": "2507.13403", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13403", "abs": "https://arxiv.org/abs/2507.13403", "authors": ["Morteza Bodaghi", "Majid Hosseini", "Raju Gottumukkala", "Ravi Teja Bhupatiraju", "Iftikhar Ahmad", "Moncef Gabbouj"], "title": "UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data", "comment": null, "summary": "In this study, we present a comprehensive public dataset for driver\ndrowsiness detection, integrating multimodal signals of facial, behavioral, and\nbiometric indicators. Our dataset includes 3D facial video using a depth\ncamera, IR camera footage, posterior videos, and biometric signals such as\nheart rate, electrodermal activity, blood oxygen saturation, skin temperature,\nand accelerometer data. This data set provides grip sensor data from the\nsteering wheel and telemetry data from the American truck simulator game to\nprovide more information about drivers' behavior while they are alert and\ndrowsy. Drowsiness levels were self-reported every four minutes using the\nKarolinska Sleepiness Scale (KSS). The simulation environment consists of three\nmonitor setups, and the driving condition is completely like a car. Data were\ncollected from 19 subjects (15 M, 4 F) in two conditions: when they were fully\nalert and when they exhibited signs of sleepiness. Unlike other datasets, our\nmultimodal dataset has a continuous duration of 40 minutes for each data\ncollection session per subject, contributing to a total length of 1,400\nminutes, and we recorded gradual changes in the driver state rather than\ndiscrete alert/drowsy labels. This study aims to create a comprehensive\nmultimodal dataset of driver drowsiness that captures a wider range of\nphysiological, behavioral, and driving-related signals. The dataset will be\navailable upon request to the corresponding author.", "AI": {"tldr": "该研究提出了一个综合性的公开数据集，用于驾驶员疲劳检测，整合了面部、行为和生物识别多模态信号，并记录了驾驶员疲劳状态的渐进变化。", "motivation": "现有驾驶员疲劳检测数据集的局限性，促使研究者旨在创建一个更全面的多模态数据集，捕获更广泛的生理、行为和驾驶相关信号。", "method": "研究收集了19名受试者（15男，4女）在清醒和疲劳两种状态下的数据。数据采集在模拟驾驶环境中进行，使用深度摄像头、红外摄像头、后置视频、生物识别信号（心率、皮电活动、血氧饱和度、皮肤温度、加速度计）、方向盘握力传感器数据以及美国卡车模拟器游戏的遥测数据。疲劳程度每四分钟通过Karolinska嗜睡量表（KSS）自报。每个受试者的数据采集持续40分钟，总时长1400分钟，记录了驾驶员状态的渐进变化。", "result": "研究成功构建了一个1400分钟的连续多模态数据集，该数据集包含3D面部视频、红外视频、后置视频、多种生物识别信号、握力传感器数据和遥测数据。与现有数据集不同，该数据集记录了驾驶员状态的渐进变化而非离散的清醒/疲劳标签。", "conclusion": "该研究创建了一个全面、多模态的驾驶员疲劳数据集，能够捕获广泛的生理、行为和驾驶相关信号，并记录了疲劳状态的连续变化，有望成为相关领域研究的宝贵资源。"}}
{"id": "2507.13614", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13614", "abs": "https://arxiv.org/abs/2507.13614", "authors": ["Sergio E. Zanotto", "Segun Aroyehun"], "title": "Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models", "comment": "arXiv admin note: text overlap with arXiv:2412.03025", "summary": "The rapid advancements in large language models (LLMs) have significantly\nimproved their ability to generate natural language, making texts generated by\nLLMs increasingly indistinguishable from human-written texts. While recent\nresearch has primarily focused on using LLMs to classify text as either\nhuman-written and machine-generated texts, our study focus on characterizing\nthese texts using a set of linguistic features across different linguistic\nlevels such as morphology, syntax, and semantics. We select a dataset of\nhuman-written and machine-generated texts spanning 8 domains and produced by 11\ndifferent LLMs. We calculate different linguistic features such as dependency\nlength and emotionality and we use them for characterizing human-written and\nmachine-generated texts along with different sampling strategies, repetition\ncontrols and model release date. Our statistical analysis reveals that\nhuman-written texts tend to exhibit simpler syntactic structures and more\ndiverse semantic content. Furthermore, we calculate the variability of our set\nof features across models and domains. Both human and machine texts show\nstylistic diversity across domains, with humans displaying greater variation in\nour features. Finally, we apply style embeddings to further test variability\namong human-written and machine-generated texts. Notably, newer models output\ntext that is similarly variable, pointing to an homogenization of\nmachine-generated texts.", "AI": {"tldr": "本研究通过分析形态、句法和语义等语言特征，揭示了人类撰写文本与大型语言模型（LLM）生成文本之间的差异，并发现新模型生成的文本趋于同质化。", "motivation": "随着LLM的快速发展，其生成文本与人类撰写文本越来越难以区分。现有研究主要侧重于文本分类，而本研究旨在从语言特征层面深入刻画这两种文本的特点。", "method": "研究选取了涵盖8个领域、由11个不同LLM生成的文本数据集，计算了依赖长度、情感性等多种语言特征。通过统计分析，并考虑了抽样策略、重复控制和模型发布日期，对人类和机器生成文本的特征进行了刻画。此外，还应用了风格嵌入（style embeddings）来进一步测试变异性。", "result": "统计分析显示，人类撰写文本倾向于展现更简单的句法结构和更多样的语义内容。人类和机器文本在不同领域都表现出文体多样性，但人类文本在所选特征上的变异性更大。值得注意的是，较新的模型输出的文本变异性相似，表明机器生成文本存在同质化趋势。", "conclusion": "人类撰写文本和机器生成文本在语言特征上存在可识别的差异，人类文本在特征上表现出更大的多样性。随着LLM的发展，新模型生成的文本趋于同质化，这为文本检测和理解提供了新的视角。"}}
{"id": "2507.14111", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14111", "abs": "https://arxiv.org/abs/2507.14111", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Jiwei Li", "Chris Shum"], "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning", "comment": "Preprint Version", "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.", "AI": {"tldr": "CUDA-L1是一个基于强化学习的自动化CUDA优化框架，它在多种GPU上实现了显著的性能提升，并展现了RL在无需人工专业知识的情况下优化代码的潜力。", "motivation": "大语言模型（LLMs）的快速发展导致GPU计算资源需求呈指数级增长，急需自动化的CUDA优化策略。然而，当前最先进的LLM在提高CUDA速度方面的成功率较低。", "method": "本文引入了CUDA-L1，一个自动化的强化学习框架，用于CUDA优化。该框架通过基于加速比的奖励信号进行训练，旨在将初始性能不佳的LLM转化为有效的CUDA优化器。", "result": "CUDA-L1在NVIDIA A100上训练，在KernelBench的250个CUDA核函数上实现了平均17.7倍的加速，峰值加速达到449倍。它还展现了卓越的跨GPU架构可移植性，在H100、RTX 3090等多种GPU上均实现平均13.9倍至19.0倍的加速。此外，该模型能发现并组合多种优化技术，揭示优化原理，并识别非显而易见的性能瓶颈。", "conclusion": "研究表明，强化学习能够仅通过基于加速比的奖励信号，在无需人类专业知识的情况下，将性能不佳的LLM转化为有效的CUDA优化器。该模型能将其习得的推理能力扩展到新的核函数，为CUDA操作的自动化优化开辟了可能性，有望显著提升GPU效率并缓解计算资源压力。"}}
{"id": "2507.14043", "categories": ["cs.RO", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.14043", "abs": "https://arxiv.org/abs/2507.14043", "authors": ["Genliang Li", "Yaxin Cui", "Jinyu Su"], "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems", "comment": "59 pages, 22 figures", "summary": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.", "AI": {"tldr": "本文提出了一种多策略改进蛇优化器（MISO），旨在解决原始蛇优化器（SO）收敛慢和易陷入局部最优的问题，并通过引入多种新策略显著提升了算法性能，并在基准测试函数和实际应用中取得了优异效果。", "motivation": "原始的蛇优化器（SO）存在收敛速度慢和易陷入局部最优的缺点。同时，无人机路径规划和工程设计等实际问题对优化算法的性能提出了更高的要求，需要更高效、稳定的优化方法。", "method": "本文提出MISO算法，主要包含以下改进策略：1) 基于正弦函数的自适应随机扰动策略，以避免陷入局部最优。2) 基于尺度因子和领导者的自适应Levy飞行策略，并赋予雄性蛇领导者飞行能力，以跳出局部最优。3) 结合精英领导和布朗运动的位置更新策略，以加速收敛并保证精度。为验证MISO性能，使用30个CEC2017和CEC2022测试函数与11种流行算法进行比较，并将其应用于无人机3D路径规划和6个工程设计问题。", "result": "实验结果表明，MISO在求解质量和稳定性方面均优于其他竞争算法。在无人机3D路径规划问题和工程设计问题中，MISO也展现了良好的可行性。", "conclusion": "MISO有效解决了原始SO的局限性，并在优化问题上表现出强大的应用潜力，尤其在解决复杂实际工程问题方面具有显著优势。"}}
{"id": "2507.13404", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13404", "abs": "https://arxiv.org/abs/2507.13404", "authors": ["Delin An", "Pan Du", "Jian-Xun Wang", "Chaoli Wang"], "title": "AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation", "comment": null, "summary": "Accurate 3D aortic construction is crucial for clinical diagnosis,\npreoperative planning, and computational fluid dynamics (CFD) simulations, as\nit enables the estimation of critical hemodynamic parameters such as blood flow\nvelocity, pressure distribution, and wall shear stress. Existing construction\nmethods often rely on large annotated training datasets and extensive manual\nintervention. While the resulting meshes can serve for visualization purposes,\nthey struggle to produce geometrically consistent, well-constructed surfaces\nsuitable for downstream CFD analysis. To address these challenges, we introduce\nAortaDiff, a diffusion-based framework that generates smooth aortic surfaces\ndirectly from CT/MRI volumes. AortaDiff first employs a volume-guided\nconditional diffusion model (CDM) to iteratively generate aortic centerlines\nconditioned on volumetric medical images. Each centerline point is then\nautomatically used as a prompt to extract the corresponding vessel contour,\nensuring accurate boundary delineation. Finally, the extracted contours are\nfitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh\nrepresentation. AortaDiff offers distinct advantages over existing methods,\nincluding an end-to-end workflow, minimal dependency on large labeled datasets,\nand the ability to generate CFD-compatible aorta meshes with high geometric\nfidelity. Experimental results demonstrate that AortaDiff performs effectively\neven with limited training data, successfully constructing both normal and\npathologically altered aorta meshes, including cases with aneurysms or\ncoarctation. This capability enables the generation of high-quality\nvisualizations and positions AortaDiff as a practical solution for\ncardiovascular research.", "AI": {"tldr": "AortaDiff是一个基于扩散模型的新框架，能直接从CT/MRI图像生成平滑且兼容CFD分析的3D主动脉表面模型，减少了对大量标注数据和手动干预的依赖。", "motivation": "现有的3D主动脉构建方法通常需要大量标注训练数据和广泛的手动干预，并且难以生成几何一致、适用于下游计算流体动力学（CFD）分析的表面，而精确的3D主动脉模型对临床诊断、术前规划和血流动力学参数估计至关重要。", "method": "AortaDiff采用扩散模型框架：1. 利用体素引导条件扩散模型（CDM）从医学图像中迭代生成主动脉中心线。2. 每个中心线点自动作为提示，提取相应的血管轮廓，确保准确的边界描绘。3. 将提取的轮廓拟合为平滑的3D表面，生成连续、兼容CFD的网格表示。", "result": "AortaDiff提供了端到端的工作流程，对大型标注数据集的依赖性最小，能够生成具有高几何保真度的CFD兼容主动脉网格。实验结果表明，即使在有限的训练数据下，AortaDiff也能有效工作，成功构建正常和病变（如动脉瘤或主动脉缩窄）主动脉网格，实现高质量可视化。", "conclusion": "AortaDiff是一种实用的心血管研究解决方案，能够生成高保真、CFD兼容的3D主动脉模型，同时显著减少了对大量标注数据和手动干预的需求，即使面对病理情况也表现出色。"}}
{"id": "2507.13618", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13618", "abs": "https://arxiv.org/abs/2507.13618", "authors": ["Shanbo Cheng", "Yu Bao", "Qian Cao", "Luyang Huang", "Liyan Kang", "Zhicheng Liu", "Yu Lu", "Wenhao Zhu", "Zhichao Huang", "Tao Li", "Sitong Liu", "Ningxin Peng", "Shuaijie She", "Lu Xu", "Nuo Xu", "Sen Yang", "Runsheng Yu", "Yiming Yu", "Liehao Zou", "Hang Li", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters", "comment": null, "summary": "Multilingual translation stands as a challenging task for large language\nmodels (LLMs) to handle intricate language patterns and stilted translations\nthat arise in automated translations. In this paper, we introduce Seed-X, a\nfamily of open-source LLMs comprising instruct and reasoning models, pushing\nthe limits of translation capability with 7B parameter size. The base model is\npre-trained on a diverse, high-quality dataset encompassing both monolingual\nand bilingual content across 28 languages, harnessing the full potential of\nmultilingual data. The instruct model is then finetuned to translate by\nChain-of-Thought (CoT) reasoning and further enhanced through reinforcement\nlearning (RL) to achieve better generalization across diverse language pairs.\nSeed-X achieves performance comparable to leading closed-source models,\nincluding Gemini-2.5 and GPT-4o, across 28 languages, and significantly\noutperforms larger open-source models in both automatic metrics and human\nevaluations. We share the best practices through our optimization process, and\nmake the parameter public available for advancing translation research and\napplications.", "AI": {"tldr": "本文介绍了Seed-X，一个7B参数的开源LLM系列，通过多语言预训练、CoT微调和RL增强，在28种语言的翻译任务上实现了与顶尖闭源模型相当的性能，并显著优于大型开源模型。", "motivation": "大型语言模型（LLMs）在处理多语言翻译中复杂的语言模式和生硬的自动化翻译方面面临挑战。", "method": "引入Seed-X系列开源LLM（包含指令和推理模型）。基础模型在包含28种语言的单语和双语高质量数据集上进行预训练。指令模型通过思维链（CoT）推理进行微调以实现翻译，并通过强化学习（RL）进一步增强，以提高在不同语言对之间的泛化能力。", "result": "Seed-X在28种语言上取得了与Gemini-2.5和GPT-4o等领先闭源模型相当的性能，并在自动评估指标和人工评估中显著优于更大的开源模型。研究还分享了优化过程中的最佳实践。", "conclusion": "Seed-X证明了7B参数的LLM在多语言翻译任务上可以达到先进水平，其性能可与领先的闭源模型媲美。该研究分享了优化经验，并公开了模型参数，以促进翻译研究和应用。"}}
{"id": "2507.13407", "categories": ["cs.CV", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.13407", "abs": "https://arxiv.org/abs/2507.13407", "authors": ["Vinu Sankar Sadasivan", "Mehrdad Saberi", "Soheil Feizi"], "title": "IConMark: Robust Interpretable Concept-Based Watermark For AI Images", "comment": "Accepted at ICLR 2025 Workshop on GenAI Watermarking (WMARK)", "summary": "With the rapid rise of generative AI and synthetic media, distinguishing\nAI-generated images from real ones has become crucial in safeguarding against\nmisinformation and ensuring digital authenticity. Traditional watermarking\ntechniques have shown vulnerabilities to adversarial attacks, undermining their\neffectiveness in the presence of attackers. We propose IConMark, a novel\nin-generation robust semantic watermarking method that embeds interpretable\nconcepts into AI-generated images, as a first step toward interpretable\nwatermarking. Unlike traditional methods, which rely on adding noise or\nperturbations to AI-generated images, IConMark incorporates meaningful semantic\nattributes, making it interpretable to humans and hence, resilient to\nadversarial manipulation. This method is not only robust against various image\naugmentations but also human-readable, enabling manual verification of\nwatermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,\ndemonstrating its superiority in terms of detection accuracy and maintaining\nimage quality. Moreover, IConMark can be combined with existing watermarking\ntechniques to further enhance and complement its robustness. We introduce\nIConMark+SS and IConMark+TM, hybrid approaches combining IConMark with\nStegaStamp and TrustMark, respectively, to further bolster robustness against\nmultiple types of image manipulations. Our base watermarking technique\n(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%\nhigher mean area under the receiver operating characteristic curve (AUROC)\nscores for watermark detection, respectively, compared to the best baseline on\nvarious datasets.", "AI": {"tldr": "IConMark是一种新颖的、可解释的、语义鲁棒的生成内水印方法，用于AI生成图像，旨在对抗虚假信息和提高数字真实性，其性能优于现有技术。", "motivation": "随着生成式AI和合成媒体的快速兴起，区分AI生成图像和真实图像变得至关重要，以防范虚假信息并确保数字真实性。传统水印技术易受对抗性攻击，影响其有效性。", "method": "本文提出了IConMark，一种在生成过程中嵌入可解释概念的鲁棒语义水印方法。与传统添加噪声或扰动的方法不同，IConMark嵌入有意义的语义属性，使其对人类可解释，从而能够抵御对抗性操纵。该方法还可与现有水印技术（如StegaStamp和TrustMark）结合，形成混合方法（IConMark+SS和IConMark+TM）以进一步增强鲁棒性。", "result": "IConMark在检测准确性和图像质量保持方面表现出优越性，且能抵抗各种图像增强。它具有人类可读性，支持手动验证水印。与最佳基线相比，IConMark及其变体（+TM和+SS）在不同数据集上的水印检测平均ROC曲线下面积（AUROC）分数分别高出10.8%、14.5%和15.9%。", "conclusion": "IConMark是一种有效、鲁棒且可解释的AI生成图像语义水印方法，它通过嵌入可理解的概念，在保持图像质量的同时显著提高了水印检测的准确性，并能抵抗多种图像操作。其可解释性也为人为验证提供了可能。"}}
{"id": "2507.14049", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14049", "abs": "https://arxiv.org/abs/2507.14049", "authors": ["Paweł Budzianowski", "Wesley Maa", "Matthew Freed", "Jingxiang Mo", "Winston Hsiao", "Aaron Xie", "Tomasz Młoduchowski", "Viraj Tipnis", "Benjamin Bolte"], "title": "EdgeVLA: Efficient Vision-Language-Action Models", "comment": null, "summary": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.", "AI": {"tldr": "本文提出Edge VLA (EVLA)，一种新方法，通过消除自回归预测和利用小型语言模型，显著提高视觉-语言-动作(VLA)模型在边缘设备上的推理速度和内存效率，同时保持其表示能力。", "motivation": "视觉-语言模型(VLMs)在解决机器人数据稀缺问题方面表现出潜力，但将大型VLMs部署到资源受限的移动操作系统上仍是巨大挑战。", "method": "EVLA通过两项关键创新实现：1) 消除末端执行器位置预测的自回归要求，从而将推理速度提高7倍；2) 利用小型语言模型(SLMs)的效率，在显著降低计算需求的同时，达到与大型模型相当的训练性能。", "result": "早期结果表明，EVLA实现了与OpenVLA相当的训练特性，同时在推理速度和内存效率方面取得了显著提升。", "conclusion": "EVLA成功地在边缘设备上实现了VLA模型的实时性能，并保持了与大型模型相当的能力，有望推动通用视觉运动控制策略在资源受限系统上的部署。"}}
{"id": "2507.13405", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13405", "abs": "https://arxiv.org/abs/2507.13405", "authors": ["Ishant Chintapatla", "Kazuma Choji", "Naaisha Agarwal", "Andrew Lin", "Hannah You", "Charles Duong", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark", "comment": null, "summary": "Recently, many benchmarks and datasets have been developed to evaluate\nVision-Language Models (VLMs) using visual question answering (VQA) pairs, and\nmodels have shown significant accuracy improvements. However, these benchmarks\nrarely test the model's ability to accurately complete visual entailment, for\ninstance, accepting or refuting a hypothesis based on the image. To address\nthis, we propose COREVQA (Crowd Observations and Reasoning Entailment), a\nbenchmark of 5608 image and synthetically generated true/false statement pairs,\nwith images derived from the CrowdHuman dataset, to provoke visual entailment\nreasoning on challenging crowded images. Our results show that even the\ntop-performing VLMs achieve accuracy below 80%, with other models performing\nsubstantially worse (39.98%-69.95%). This significant performance gap reveals\nkey limitations in VLMs' ability to reason over certain types of image-question\npairs in crowded scenes.", "AI": {"tldr": "现有视觉语言模型（VLMs）在视觉问答（VQA）方面表现良好，但在处理拥挤场景中的视觉蕴涵（visual entailment）任务时表现不佳，新提出的COREVQA基准揭示了这一局限性。", "motivation": "尽管VLMs在VQA基准上取得了显著进展，但现有基准很少测试模型准确完成视觉蕴涵的能力，特别是在具有挑战性的拥挤图像上。", "method": "提出了COREVQA（Crowd Observations and Reasoning Entailment）基准，包含5608对图像与合成生成的真/假陈述，图像来源于CrowdHuman数据集，旨在引发对拥挤图像的视觉蕴涵推理。", "result": "即使是表现最佳的VLMs在COREVQA上的准确率也低于80%，其他模型表现更差（39.98%-69.95%），这表明VLMs在拥挤场景中对特定类型图像-问题对的推理能力存在显著局限。", "conclusion": "视觉语言模型在处理复杂拥挤场景中的视觉蕴涵推理方面存在关键局限性，需要进一步改进。"}}
{"id": "2507.13655", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13655", "abs": "https://arxiv.org/abs/2507.13655", "authors": ["Teerapong Panboonyuen"], "title": "CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer", "comment": "12 pages", "summary": "Integrating large language models into specialized domains like healthcare\npresents unique challenges, including domain adaptation and limited labeled\ndata. We introduce CU-ICU, a method for customizing unsupervised\ninstruction-finetuned language models for ICU datasets by leveraging the\nText-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse\nfine-tuning approach that combines few-shot prompting with selective parameter\nupdates, enabling efficient adaptation with minimal supervision. Our evaluation\nacross critical ICU tasks--early sepsis detection, mortality prediction, and\nclinical note generation--demonstrates that CU-ICU consistently improves\npredictive accuracy and interpretability over standard fine-tuning methods.\nNotably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and\na 20% enhancement in generating clinically relevant explanations while updating\nfewer than 1% of model parameters in its most efficient configuration. These\nresults establish CU-ICU as a scalable, low-overhead solution for delivering\naccurate and interpretable clinical decision support in real-world ICU\nenvironments.", "AI": {"tldr": "CU-ICU是一种针对ICU领域，通过稀疏微调（结合少量样本提示和选择性参数更新）定制无监督指令微调语言模型的方法，能在有限监督下高效提升预测准确性和可解释性。", "motivation": "将大型语言模型整合到医疗等专业领域面临独特挑战，包括领域适应和标记数据有限的问题。", "method": "引入CU-ICU方法，利用Text-to-Text Transfer Transformer (T5)架构，通过稀疏微调（结合少量样本提示和选择性参数更新）来定制ICU数据集的无监督指令微调语言模型。", "result": "CU-ICU在早期败血症检测、死亡率预测和临床笔记生成等关键ICU任务上，相较于标准微调方法，持续提高了预测准确性和可解释性。特别是，败血症检测准确率提高了15%，生成临床相关解释的能力增强了20%，同时在最有效配置下更新的模型参数少于1%。", "conclusion": "CU-ICU是一种可扩展、低开销的解决方案，能够为现实世界的ICU环境提供准确且可解释的临床决策支持。"}}
{"id": "2507.13408", "categories": ["cs.CV", "cs.AI", "68T07", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.13408", "abs": "https://arxiv.org/abs/2507.13408", "authors": ["Hemanth Kumar M", "Karthika M", "Saianiruth M", "Vasanthakumar Venugopal", "Anandakumar D", "Revathi Ezhumalai", "Charulatha K", "Kishore Kumar J", "Dayana G", "Kalyan Sivasailam", "Bargava Subramanian"], "title": "A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs", "comment": "12 pages, 2 figures", "summary": "Background: Shoulder fractures are often underdiagnosed, especially in\nemergency and high-volume clinical settings. Studies report up to 10% of such\nfractures may be missed by radiologists. AI-driven tools offer a scalable way\nto assist early detection and reduce diagnostic delays. We address this gap\nthrough a dedicated AI system for shoulder radiographs. Methods: We developed a\nmulti-model deep learning system using 10,000 annotated shoulder X-rays.\nArchitectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and\nRF-DETR. To enhance detection, we applied bounding box and classification-level\nensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW\nensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming\nindividual models across all key metrics. It demonstrated strong recall and\nlocalization precision, confirming its effectiveness for clinical fracture\ndetection in shoulder X-rays. Conclusion: The results show ensemble-based AI\ncan reliably detect shoulder fractures in radiographs with high clinical\nrelevance. The model's accuracy and deployment readiness position it well for\nintegration into real-time diagnostic workflows. The current model is limited\nto binary fracture detection, reflecting its design for rapid screening and\ntriage support rather than detailed orthopedic classification.", "AI": {"tldr": "本文开发了一种基于多模型深度学习集成系统，用于在肩部X光片中高精度检测骨折，以辅助早期诊断和减少漏诊。", "motivation": "肩部骨折在急诊和高流量临床环境中常被漏诊（放射科医生漏诊率高达10%），导致诊断延迟。研究旨在通过AI工具解决这一问题，实现早期检测。", "method": "研究开发了一个多模型深度学习系统，使用了10,000张标注的肩部X光片进行训练。采用的架构包括Faster R-CNN (ResNet50-FPN, ResNeXt)、EfficientDet和RF-DETR。为提高检测性能，应用了边界框和分类级别的集成技术，如Soft-NMS、WBF和NMW融合。", "result": "NMW集成方法实现了95.5%的准确率和0.9610的F1分数，在所有关键指标上均优于单一模型。该方法在召回率和定位精度方面表现出色，证实了其在肩部X光片临床骨折检测中的有效性。", "conclusion": "基于集成的AI系统能够可靠地检测X光片中的肩部骨折，具有高度临床相关性。该模型的高准确性和部署准备度使其非常适合集成到实时诊断工作流程中，用于快速筛查和分诊支持（目前仅限于二元骨折检测，而非详细分类）。"}}
{"id": "2507.14059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14059", "abs": "https://arxiv.org/abs/2507.14059", "authors": ["Tianyuan Wang", "Mark A Post", "Mathieu Deremetz"], "title": "Design of a Modular Mobile Inspection and Maintenance Robot for an Orbital Servicing Hub", "comment": "In proceedings of the Towards Autonomous Robotic Systems 2025\n  conference (TAROS 2025), York, UK 6 pages, one page of references, 6 figures", "summary": "The use of autonomous robots in space is an essential part of the \"New Space\"\ncommercial ecosystem of assembly and re-use of space hardware components in\nEarth orbit and beyond. The STARFAB project aims to create a ground\ndemonstration of an orbital automated warehouse as a hub for sustainable\ncommercial operations and servicing. A critical part of this fully-autonomous\nrobotic facility will be the capability to monitor, inspect, and assess the\ncondition of both the components stored in the warehouse, and the STARFAB\nfacility itself. This paper introduces ongoing work on the STARFAB Mobile\nInspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it\ncan be carried by Walking Manipulators (WM) as an independently-mobile robot,\nand multiple MIMs can be stored and retrieved as needed for operations on\nSTARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a\nthermal imaging sensor, with the capability to add other modular sensors. A\ngrasping tool and torque wrench are stored within the modular body for use by\nan attached WM for maintenance operations. Implementation and testing is still\nongoing at the time of writing. This paper details the concept of operations\nfor the MIM as an on-orbit autonomous inspection and maintenance system, the\nmechanical and electronic design of the MIM, and the sensors package used for\nnon-destructive testing.", "AI": {"tldr": "本文介绍了STARFAB项目中的移动检查模块（MIM），一个用于轨道自动化仓库的自主检查和维护机器人，旨在支持“新空间”商业生态系统。", "motivation": "在“新空间”商业生态系统中，自主机器人在轨道上组装和重复利用空间硬件组件至关重要。STARFAB项目旨在创建一个地面演示的轨道自动化仓库，作为可持续商业运营和服务的中心。该设施的关键能力是监测、检查和评估存储组件及设施自身的状况。", "method": "STARFAB移动检查模块（MIM）被设计为一个独立移动的机器人，通过标准互连（SI）可由步行机械臂（WM）携带，并可根据需要进行存储和取回。MIM配备高分辨率摄像头、3D轮廓仪和热成像传感器，并可添加其他模块化传感器。其模块化本体内还存储有抓取工具和扭矩扳手，供连接的WM进行维护操作。", "result": "本文详细介绍了MIM作为在轨自主检查和维护系统的操作概念、机械和电子设计，以及用于无损检测的传感器包。目前，实施和测试仍在进行中。", "conclusion": "MIM是STARFAB轨道自动化仓库实现自主检查和维护的关键组成部分，其设计和操作概念为未来可持续的商业空间运营提供了基础，支持了“新空间”商业生态系统的发展。"}}
{"id": "2507.13420", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13420", "abs": "https://arxiv.org/abs/2507.13420", "authors": ["Alessandro Pistola", "Valentina Orru'", "Nicolo' Marchetti", "Marco Roccetti"], "title": "AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery", "comment": "25 pages, 9 Figures", "summary": "By upgrading an existing deep learning model with the knowledge provided by\none of the oldest sets of grayscale satellite imagery, known as CORONA, we\nimproved the AI model attitude towards the automatic identification of\narchaeological sites in an environment which has been completely transformed in\nthe last five decades, including the complete destruction of many of those same\nsites. The initial Bing based convolutional network model was retrained using\nCORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,\ncentral Mesopotamian floodplain. The results were twofold and surprising.\nFirst, the detection precision obtained on the area of interest increased\nsensibly: in particular, the Intersection over Union (IoU) values, at the image\nsegmentation level, surpassed 85 percent, while the general accuracy in\ndetecting archeological sites reached 90 percent. Second, our retrained model\nallowed the identification of four new sites of archaeological interest\n(confirmed through field verification), previously not identified by\narchaeologists with traditional techniques. This has confirmed the efficacy of\nusing AI techniques and the CORONA imagery from the 1960 to discover\narchaeological sites currently no longer visible, a concrete breakthrough with\nsignificant consequences for the study of landscapes with vanishing\narchaeological evidence induced by anthropization", "AI": {"tldr": "通过使用CORONA历史卫星图像重新训练深度学习模型，研究人员显著提高了在已完全转变的景观中自动识别考古遗址的准确性，并成功发现了新的遗址。", "motivation": "在过去五十年中，由于人类活动（anthropization）导致许多考古遗址被完全破坏或不再可见，传统考古技术难以识别这些遗址。研究旨在利用AI模型克服这一挑战。", "method": "研究人员升级了一个现有的基于Bing的卷积神经网络模型，并使用1960年代的CORONA卫星图像对该模型进行了再训练，目标区域是巴格达以西的阿布格莱布区。", "result": "结果令人惊喜：1. 模型检测精度显著提高，图像分割级别的IoU值超过85%，考古遗址检测的总体准确率达到90%。2. 再训练后的模型识别出四个新的考古遗址，这些遗址此前未被考古学家通过传统技术发现，并已通过实地验证确认。", "conclusion": "研究证实了结合AI技术和1960年代CORONA图像在发现目前已不可见的考古遗址方面的有效性，这对于研究因人类活动导致考古证据消失的景观具有重要突破性意义。"}}
{"id": "2507.13666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13666", "abs": "https://arxiv.org/abs/2507.13666", "authors": ["Woo-Chan Kim", "Ji-Hoon Park", "Seong-Whan Lee"], "title": "KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs", "comment": null, "summary": "Large language models (LLMs) have demonstrated state-of-the-art performance\nacross a wide range of natural language processing tasks. However,\nhigh-performing models are typically accessible only via APIs, incurring\nsubstantial inference costs. Cascade methods address this by initially\nemploying a cheaper model and escalating to a stronger one only when necessary.\nNevertheless, existing cascade approaches struggle to select a reliable\nrepresentative response and assess the overall reliability of free-form\noutputs, as they rely on exact text matching. To overcome these limitations, we\npropose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient\nfree-form text generation. KiC identifies the most representative answer among\nmultiple outputs from a weaker model and evaluates the semantic alignment of\nother responses with it. Based on the degree of alignment, KiC determines\nwhether to accept the weaker model's output or escalate to a stronger model.\nExperiments on three free-form text generation benchmarks show that KiC\nachieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81\npercent on average, and even outperforms GPT-4 in a specific benchmark.", "AI": {"tldr": "本文提出Keyword-inspired Cascade (KiC) 框架，通过从弱模型多输出中选出代表性答案并评估语义对齐，决定是否升级到强模型，从而在保持高准确率的同时显著降低LLM API成本。", "motivation": "高性能大型语言模型(LLMs)通常通过API访问，导致高昂的推理成本。现有级联方法依赖精确文本匹配，难以可靠地选择代表性响应并评估自由形式输出的整体可靠性。", "method": "KiC框架首先从较弱模型生成多个输出中识别出“最具代表性的答案”，然后评估其他响应与该代表性答案的语义对齐程度。根据对齐程度，KiC决定是接受弱模型的输出还是升级到更强的模型。", "result": "在三个自由形式文本生成基准测试中，KiC达到了GPT-4 97.53%的准确率，同时平均降低了28.81%的API成本，甚至在一个特定基准测试中超越了GPT-4。", "conclusion": "KiC是一种成本效益高的自由形式文本生成框架，它能在显著降低API成本的同时保持高准确性，并且在某些情况下甚至能超越更强的模型。"}}
{"id": "2507.13425", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13425", "abs": "https://arxiv.org/abs/2507.13425", "authors": ["Sirui Wang", "Zhou Guan", "Bingxi Zhao", "Tongjia Gu"], "title": "CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction", "comment": null, "summary": "Accurate prediction of driving intention is key to enhancing the safety and\ninteractive efficiency of human-machine co-driving systems. It serves as a\ncornerstone for achieving high-level autonomous driving. However, current\napproaches remain inadequate for accurately modeling the complex\nspatio-temporal interdependencies and the unpredictable variability of human\ndriving behavior. To address these challenges, we propose CaSTFormer, a Causal\nSpatio-Temporal Transformer to explicitly model causal interactions between\ndriver behavior and environmental context for robust intention prediction.\nSpecifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)\nmechanism for precise temporal alignment of internal and external feature\nstreams, a Causal Pattern Extraction (CPE) module that systematically\neliminates spurious correlations to reveal authentic causal dependencies, and\nan innovative Feature Synthesis Network (FSN) that adaptively synthesizes these\npurified representations into coherent spatio-temporal inferences. We evaluate\nthe proposed CaSTFormer on the public Brain4Cars dataset, and it achieves\nstate-of-the-art performance. It effectively captures complex causal\nspatio-temporal dependencies and enhances both the accuracy and transparency of\ndriving intention prediction.", "AI": {"tldr": "CaSTFormer是一种因果时空Transformer模型，通过明确建模驾驶员行为和环境背景之间的因果关系，提高了驾驶意图预测的准确性和透明度。", "motivation": "当前的驾驶意图预测方法在准确建模复杂时空相互依赖性和人类驾驶行为的不可预测变异性方面存在不足，而准确的驾驶意图预测对提升人机共驾系统的安全性和交互效率至关重要。", "method": "本文提出了CaSTFormer模型，其中包含：1) 互惠偏移融合（RSF）机制，用于精确对齐内部和外部特征流；2) 因果模式提取（CPE）模块，系统性地消除虚假关联以揭示真实的因果依赖；3) 特征合成网络（FSN），自适应地将净化后的表示合成为连贯的时空推断。", "result": "CaSTFormer在公开的Brain4Cars数据集上取得了最先进的性能，有效地捕捉了复杂的因果时空依赖关系，并显著提升了驾驶意图预测的准确性和透明度。", "conclusion": "CaSTFormer通过显式建模驾驶员行为与环境背景之间的因果交互，成功解决了现有方法在驾驶意图预测中的挑战，为高水平自动驾驶奠定了基础。"}}
{"id": "2507.14061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14061", "abs": "https://arxiv.org/abs/2507.14061", "authors": ["Nataliya Nechyporenko", "Yutong Zhang", "Sean Campbell", "Alessandro Roncone"], "title": "MorphIt: Flexible Spherical Approximation of Robot Morphology for Representation-driven Adaptation", "comment": null, "summary": "What if a robot could rethink its own morphological representation to better\nmeet the demands of diverse tasks? Most robotic systems today treat their\nphysical form as a fixed constraint rather than an adaptive resource, forcing\nthe same rigid geometric representation to serve applications with vastly\ndifferent computational and precision requirements. We introduce MorphIt, a\nnovel algorithm for approximating robot morphology using spherical primitives\nthat balances geometric accuracy with computational efficiency. Unlike existing\napproaches that rely on either labor-intensive manual specification or\ninflexible computational methods, MorphIt implements an automatic\ngradient-based optimization framework with tunable parameters that provides\nexplicit control over the physical fidelity versus computational cost tradeoff.\nQuantitative evaluations demonstrate that MorphIt outperforms baseline\napproaches (Variational Sphere Set Approximation and Adaptive Medial-Axis\nApproximation) across multiple metrics, achieving better mesh approximation\nwith fewer spheres and reduced computational overhead. Our experiments show\nenhanced robot capabilities in collision detection accuracy, contact-rich\ninteraction simulation, and navigation through confined spaces. By dynamically\nadapting geometric representations to task requirements, robots can now exploit\ntheir physical embodiment as an active resource rather than an inflexible\nparameter, opening new frontiers for manipulation in environments where\nphysical form must continuously balance precision with computational\ntractability.", "AI": {"tldr": "MorphIt是一种新颖的算法，通过球形基元近似机器人形态，以平衡几何精度和计算效率，使机器人能根据任务需求动态调整其形态表示。", "motivation": "目前的机器人系统将物理形态视为固定约束而非自适应资源，导致相同的刚性几何表示无法有效满足具有不同计算和精度要求的任务。研究旨在开发一种能自适应调整机器人形态表示的方法。", "method": "引入MorphIt算法，它采用球形基元来近似机器人形态。该算法基于自动化的梯度优化框架，具有可调参数，明确控制物理保真度与计算成本之间的权衡。", "result": "定量评估显示，MorphIt在多项指标上优于基线方法（变分球集近似和自适应中轴近似），能用更少的球体实现更好的网格近似，并降低计算开销。实验证明，MorphIt增强了机器人的碰撞检测精度、接触丰富的交互仿真能力以及在狭窄空间中的导航能力。", "conclusion": "通过动态调整几何表示以适应任务需求，机器人能够将其实体作为一种活跃资源而非僵硬参数来利用。这为机器人操作开辟了新的领域，使其在物理形态必须持续平衡精度与计算可行性的环境中表现更佳。"}}
{"id": "2507.13428", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13428", "abs": "https://arxiv.org/abs/2507.13428", "authors": ["Jing Gu", "Xian Liu", "Yu Zeng", "Ashwin Nagarajan", "Fangrui Zhu", "Daniel Hong", "Yue Fan", "Qianqi Yan", "Kaiwen Zhou", "Ming-Yu Liu", "Xin Eric Wang"], "title": "\"PhyWorldBench\": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models", "comment": "31 pages, 21 figures", "summary": "Video generation models have achieved remarkable progress in creating\nhigh-quality, photorealistic content. However, their ability to accurately\nsimulate physical phenomena remains a critical and unresolved challenge. This\npaper presents PhyWorldBench, a comprehensive benchmark designed to evaluate\nvideo generation models based on their adherence to the laws of physics. The\nbenchmark covers multiple levels of physical phenomena, ranging from\nfundamental principles like object motion and energy conservation to more\ncomplex scenarios involving rigid body interactions and human or animal motion.\nAdditionally, we introduce a novel \"\"Anti-Physics\"\" category, where prompts\nintentionally violate real-world physics, enabling the assessment of whether\nmodels can follow such instructions while maintaining logical consistency.\nBesides large-scale human evaluation, we also design a simple yet effective\nmethod that could utilize current MLLM to evaluate the physics realism in a\nzero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation\nmodels, including five open-source and five proprietary models, with a detailed\ncomparison and analysis. we identify pivotal challenges models face in adhering\nto real-world physics. Through systematic testing of their outputs across 1,050\ncurated prompts-spanning fundamental, composite, and anti-physics scenarios-we\nidentify pivotal challenges these models face in adhering to real-world\nphysics. We then rigorously examine their performance on diverse physical\nphenomena with varying prompt types, deriving targeted recommendations for\ncrafting prompts that enhance fidelity to physical principles.", "AI": {"tldr": "本文提出了PhyWorldBench，一个全面的基准，用于评估视频生成模型在遵循物理定律方面的能力。通过对12个SOTA模型进行评估，识别了模型在物理模拟方面面临的关键挑战，并提供了提示工程建议。", "motivation": "视频生成模型在生成高质量、逼真内容方面取得了显著进展，但其准确模拟物理现象的能力仍是一个关键且未解决的挑战。", "method": "提出了PhyWorldBench基准，涵盖从基本物理原理到复杂交互场景，并引入“反物理”类别。评估方法包括大规模人工评估和基于MLLM的零样本评估。对12个最先进的文本到视频生成模型（包括开源和专有模型）进行了系统测试，使用了1050个精心策划的提示。", "result": "识别了视频生成模型在遵循现实世界物理方面面临的关键挑战。通过对不同物理现象和提示类型的性能进行严格审查，得出了提高物理保真度的提示工程建议。", "conclusion": "视频生成模型在准确模拟物理现象方面仍存在重大挑战。PhyWorldBench基准和评估结果揭示了这些挑战，并为未来模型改进和提示优化提供了有价值的见解和方向。"}}
{"id": "2507.13681", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13681", "abs": "https://arxiv.org/abs/2507.13681", "authors": ["Haoyang Li", "Zhanchao Xu", "Yiming Li", "Xuejia Chen", "Darian Li", "Anxin Tian", "Qingfa Xiao", "Cheng Deng", "Jun Wang", "Qing Li", "Lei Chen", "Mingxuan Yuan"], "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues", "comment": null, "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.", "AI": {"tldr": "LoopServe是一个自适应双阶段推理加速框架，旨在解决多轮对话中大型语言模型在长对话历史下的计算和内存挑战。", "motivation": "现有大型语言模型在处理长多轮对话历史时面临计算和内存挑战，影响其效率和响应速度。大多数现有加速方法依赖固定或基于位置的启发式方法，无法很好地适应动态和不可预测的对话模式。", "method": "LoopServe引入了两项主要创新：1. 在预填充阶段进行在线稀疏化，动态选择注意力矩阵最重要的部分；2. 在解码阶段使用渐进式键值压缩，自适应地维护相关且高效的缓存。此外，论文还提出了一个包含十一个多轮数据集的新基准。", "result": "LoopServe在广泛的长上下文对话任务中，相比现有基线，持续展现出卓越的有效性，并显著加速了LLM推理。", "conclusion": "LoopServe提供了一个有效的自适应框架，显著提升了大型语言模型在多轮对话场景中的推理效率和性能，解决了长对话历史带来的计算和内存瓶颈。"}}
{"id": "2507.13659", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13659", "abs": "https://arxiv.org/abs/2507.13659", "authors": ["Xiao Wang", "Qian Zhu", "Shujuan Wu", "Bo Jiang", "Shiliang Zhang", "Yaowei Wang", "Yonghong Tian", "Bin Luo"], "title": "When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework", "comment": null, "summary": "Recent researchers have proposed using event cameras for person\nre-identification (ReID) due to their promising performance and better balance\nin terms of privacy protection, event camera-based person ReID has attracted\nsignificant attention. Currently, mainstream event-based person ReID algorithms\nprimarily focus on fusing visible light and event stream, as well as preserving\nprivacy. Although significant progress has been made, these methods are\ntypically trained and evaluated on small-scale or simulated event camera\ndatasets, making it difficult to assess their real identification performance\nand generalization ability. To address the issue of data scarcity, this paper\nintroduces a large-scale RGB-event based person ReID dataset, called EvReID.\nThe dataset contains 118,988 image pairs and covers 1200 pedestrian identities,\nwith data collected across multiple seasons, scenes, and lighting conditions.\nWe also evaluate 15 state-of-the-art person ReID algorithms, laying a solid\nfoundation for future research in terms of both data and benchmarking. Based on\nour newly constructed dataset, this paper further proposes a pedestrian\nattribute-guided contrastive learning framework to enhance feature learning for\nperson re-identification, termed TriPro-ReID. This framework not only\neffectively explores the visual features from both RGB frames and event\nstreams, but also fully utilizes pedestrian attributes as mid-level semantic\nfeatures. Extensive experiments on the EvReID dataset and MARS datasets fully\nvalidated the effectiveness of our proposed RGB-Event person ReID framework.\nThe benchmark dataset and source code will be released on\nhttps://github.com/Event-AHU/Neuromorphic_ReID", "AI": {"tldr": "该论文提出了一个大规模RGB-事件行人重识别数据集EvReID，并提出了一个名为TriPro-ReID的行人属性引导对比学习框架，以解决数据稀缺性问题并增强特征学习。", "motivation": "现有基于事件相机的行人重识别算法主要在小规模或模拟数据集上训练和评估，难以准确评估其实际识别性能和泛化能力，存在数据稀缺性问题。", "method": "1. 构建并发布了大规模RGB-事件行人重识别数据集EvReID，包含118,988对图像和1200个行人身份，覆盖多种季节、场景和光照条件。2. 在EvReID数据集上评估了15种最先进的行人重识别算法。3. 提出了一种名为TriPro-ReID的行人属性引导对比学习框架，该框架有效融合了RGB帧和事件流的视觉特征，并充分利用行人属性作为中级语义特征。", "result": "1. EvReID数据集的构建为未来RGB-事件行人重识别研究奠定了数据和基准基础。2. 所提出的TriPro-ReID框架能够有效探索RGB帧和事件流的视觉特征，并充分利用行人属性。3. 在EvReID和MARS数据集上的大量实验验证了所提出的RGB-事件行人重识别框架的有效性。", "conclusion": "该论文通过提供大规模数据集和提出的新型框架，显著推动了RGB-事件行人重识别领域的研究，解决了数据稀缺问题并提升了特征学习能力。"}}
{"id": "2507.14099", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14099", "abs": "https://arxiv.org/abs/2507.14099", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Maria Koskinopoulou", "Yvan R. Petillot"], "title": "Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation", "comment": "Accepted at 2025 IEEE International Conference on Intelligent Robots\n  and Systems (IROS)", "summary": "Autonomous motion planning is critical for efficient and safe underwater\nmanipulation in dynamic marine environments. Current motion planning methods\noften fail to effectively utilize prior motion experiences and adapt to\nreal-time uncertainties inherent in underwater settings. In this paper, we\nintroduce an Adaptive Heuristic Motion Planner framework that integrates a\nHeuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning\nfor autonomous underwater manipulation. Our approach employs the Probabilistic\nRoadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite\ncost function that accounts for distance, uncertainty, energy consumption, and\nexecution time. By leveraging HMS, our framework significantly reduces the\nsearch space, thereby boosting computational performance and enabling real-time\nplanning capabilities. Bayesian Networks are utilized to dynamically update\nuncertainty estimates based on real-time sensor data and environmental\nconditions, thereby refining the joint probability of path success. Through\nextensive simulations and real-world test scenarios, we showcase the advantages\nof our method in terms of enhanced performance and robustness. This\nprobabilistic approach significantly advances the capability of autonomous\nunderwater robots, ensuring optimized motion planning in the face of dynamic\nmarine challenges.", "AI": {"tldr": "该论文提出了一种自适应启发式运动规划器框架，结合启发式运动空间（HMS）和贝叶斯网络，以增强水下自主操作的运动规划，有效应对实时不确定性。", "motivation": "当前运动规划方法未能有效利用先前的运动经验，也无法适应水下环境中固有的实时不确定性，导致规划效率和安全性不足。", "method": "引入自适应启发式运动规划器（AHMP）框架，该框架将启发式运动空间（HMS）与贝叶斯网络相结合。在HMS内使用概率路线图（PRM）算法，通过最小化综合成本函数（考虑距离、不确定性、能耗和执行时间）来优化路径。HMS显著减小了搜索空间，提高了计算性能。贝叶斯网络用于根据实时传感器数据和环境条件动态更新不确定性估计，从而完善路径成功的联合概率。", "result": "通过广泛的仿真和真实世界测试场景，展示了该方法在提高性能和鲁棒性方面的优势。", "conclusion": "这种概率方法显著提升了自主水下机器人的能力，确保在动态海洋挑战面前实现优化的运动规划。"}}
{"id": "2507.13486", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13486", "abs": "https://arxiv.org/abs/2507.13486", "authors": ["Debao Huang", "Rongjun Qin"], "title": "Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation", "comment": "16 pages, 9 figures, this manuscript has been submitted to ISPRS\n  Journal of Photogrammetry and Remote Sensing for consideration", "summary": "Uncertainty quantification of the photogrammetry process is essential for\nproviding per-point accuracy credentials of the point clouds. Unlike airborne\nLiDAR, which typically delivers consistent accuracy across various scenes, the\naccuracy of photogrammetric point clouds is highly scene-dependent, since it\nrelies on algorithm-generated measurements (i.e., stereo or multi-view stereo).\nGenerally, errors of the photogrammetric point clouds propagate through a\ntwo-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),\nfollowed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM\nstage has been well studied using the first-order statistics of the\nreprojection error function, that in the MVS stage remains largely unsolved and\nnon-standardized, primarily due to its non-differentiable and multi-modal\nnature (i.e., from pixel values to geometry). In this paper, we present an\nuncertainty quantification framework closing this gap by associating an error\ncovariance matrix per point accounting for this two-step photogrammetry\nprocess. Specifically, to estimate the uncertainty in the MVS stage, we propose\na novel, self-calibrating method by taking reliable n-view points (n>=6)\nper-view to regress the disparity uncertainty using highly relevant cues (such\nas matching cost values) from the MVS stage. Compared to existing approaches,\nour method uses self-contained, reliable 3D points extracted directly from the\nMVS process, with the benefit of being self-supervised and naturally adhering\nto error propagation path of the photogrammetry process, thereby providing a\nrobust and certifiable uncertainty quantification across diverse scenes. We\nevaluate the framework using a variety of publicly available airborne and UAV\nimagery datasets. Results demonstrate that our method outperforms existing\napproaches by achieving high bounding rates without overestimating uncertainty.", "AI": {"tldr": "本文提出一个光度测量不确定性量化框架，通过为每个点关联误差协方差矩阵来解决多视图立体（MVS）阶段的不确定性估计难题，该方法自校准且性能优于现有方法。", "motivation": "机载LiDAR通常提供一致的精度，而摄影测量点云的精度高度依赖场景，且其误差通过SfM和MVS两步传播。SfM阶段的不确定性已得到充分研究，但MVS阶段的不确定性估计仍未解决且缺乏标准化，主要原因在于其不可微和多模态特性。", "method": "本文提出一种不确定性量化框架，为每个点关联一个误差协方差矩阵，以解释两步摄影测量过程。具体地，为估计MVS阶段的不确定性，提出一种新颖的自校准方法，通过利用每视图可靠的n视图点（n>=6）并结合MVS阶段的相关线索（如匹配成本值）来回归视差不确定性。该方法从MVS过程中直接提取自包含的可靠3D点，具有自监督和遵循误差传播路径的优点。", "result": "该框架在多种公开的机载和无人机图像数据集上进行了评估。结果表明，本文方法在不夸大不确定性的前提下，实现了高边界率，优于现有方法。", "conclusion": "本文提出的不确定性量化框架通过解决MVS阶段的不确定性估计问题，为摄影测量过程提供了鲁棒且可认证的跨场景不确定性量化能力。"}}
{"id": "2507.13705", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.13705", "abs": "https://arxiv.org/abs/2507.13705", "authors": ["Cedric Waterschoot", "Nava Tintarev", "Francesco Barile"], "title": "Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations", "comment": "Short paper accepted at the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25). Cedric Waterschoot, Nava Tintarev, and Francesco\n  Barile. 2025. Consistent Explainers or Unreliable Narrators? Understanding\n  LLM-generated Group Recommendations. Proceedings of the Nineteenth ACM\n  Conference on Recommender Systems (RecSys '25), Prague, Czech Republic. doi:\n  10.1145/3705328.3748015", "summary": "Large Language Models (LLMs) are increasingly being implemented as joint\ndecision-makers and explanation generators for Group Recommender Systems (GRS).\nIn this paper, we evaluate these recommendations and explanations by comparing\nthem to social choice-based aggregation strategies. Our results indicate that\nLLM-generated recommendations often resembled those produced by Additive\nUtilitarian (ADD) aggregation. However, the explanations typically referred to\naveraging ratings (resembling but not identical to ADD aggregation). Group\nstructure, uniform or divergent, did not impact the recommendations.\nFurthermore, LLMs regularly claimed additional criteria such as user or item\nsimilarity, diversity, or used undefined popularity metrics or thresholds. Our\nfindings have important implications for LLMs in the GRS pipeline as well as\nstandard aggregation strategies. Additional criteria in explanations were\ndependent on the number of ratings in the group scenario, indicating potential\ninefficiency of standard aggregation methods at larger item set sizes.\nAdditionally, inconsistent and ambiguous explanations undermine transparency\nand explainability, which are key motivations behind the use of LLMs for GRS.", "AI": {"tldr": "该研究评估了大型语言模型（LLMs）在群组推荐系统（GRS）中生成的推荐和解释，发现其推荐结果常与加性效用聚合（ADD）相似，但解释存在不一致性和模糊性，并常提及未明确定义的额外标准，这削弱了LLMs在GRS中应用的关键优势——透明度和可解释性。", "motivation": "随着LLMs在群组推荐系统中被越来越多地用作决策者和解释生成器，有必要评估其推荐和解释的质量，并与基于社会选择的聚合策略进行比较，以了解其表现及其对透明度和可解释性的影响。", "method": "该研究通过将LLM生成的推荐和解释与基于社会选择的聚合策略（特别是加性效用聚合，ADD）进行比较来评估它们。同时，还分析了群组结构（统一或分歧）对推荐的影响，以及解释中提及的额外标准。", "result": "LLM生成的推荐通常与加性效用聚合（ADD）的结果相似。然而，其解释通常提及平均评分（与ADD相似但不完全相同）。群组结构（统一或分歧）对推荐没有影响。LLMs经常在解释中声称使用了用户或物品相似性、多样性或未定义的流行度指标/阈值等额外标准。解释中额外标准的出现依赖于群组场景中的评分数量，这可能暗示了标准聚合方法在更大物品集尺寸下的潜在低效性。此外，解释存在不一致性和模糊性。", "conclusion": "LLMs在GRS中的应用对GRS流程和标准聚合策略都有重要影响。解释中存在的不一致和模糊性，以及提及未明确定义的额外标准，削弱了LLMs作为GRS中透明和可解释工具的关键优势。研究结果还表明标准聚合方法在处理大型物品集时可能效率低下。"}}
{"id": "2507.13677", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13677", "abs": "https://arxiv.org/abs/2507.13677", "authors": ["Chuheng Wei", "Ziye Qin", "Walter Zimmer", "Guoyuan Wu", "Matthew J. Barth"], "title": "HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors", "comment": "Ranked first in CVPR DriveX workshop TUM-Traf V2X challenge. Accepted\n  by ITSC2025", "summary": "Real-world Vehicle-to-Everything (V2X) cooperative perception systems often\noperate under heterogeneous sensor configurations due to cost constraints and\ndeployment variability across vehicles and infrastructure. This heterogeneity\nposes significant challenges for feature fusion and perception reliability. To\naddress these issues, we propose HeCoFuse, a unified framework designed for\ncooperative perception across mixed sensor setups where nodes may carry Cameras\n(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that\nadaptively weights features through a combination of channel-wise and spatial\nattention, HeCoFuse can tackle critical challenges such as cross-modality\nfeature misalignment and imbalanced representation quality. In addition, an\nadaptive spatial resolution adjustment module is employed to balance\ncomputational cost and fusion effectiveness. To enhance robustness across\ndifferent configurations, we further implement a cooperative learning strategy\nthat dynamically adjusts fusion type based on available modalities. Experiments\non the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%\n3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D\nbaseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC\nscenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine\nheterogeneous sensor configurations. These results, validated by our\nfirst-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the\ncurrent state-of-the-art on TUM-Traf V2X dataset while demonstrating robust\nperformance across diverse sensor deployments.", "AI": {"tldr": "HeCoFuse是一种统一的V2X协同感知框架，旨在解决异构传感器配置下的挑战，通过分层融合和自适应空间分辨率调整，在TUMTraf-V2X数据集上实现了最先进的性能。", "motivation": "现实世界中的V2X协同感知系统常因成本和部署差异而采用异构传感器配置（如仅摄像头、仅激光雷达或两者兼有），这给特征融合和感知可靠性带来了巨大挑战。", "method": "该论文提出了HeCoFuse框架，采用以下方法：1) 引入分层融合机制，通过通道和空间注意力自适应地加权特征，以解决跨模态特征不对齐和表示质量不平衡问题。2) 采用自适应空间分辨率调整模块，平衡计算成本和融合效果。3) 实现协同学习策略，根据可用模态动态调整融合类型，增强不同配置下的鲁棒性。", "result": "在TUMTraf-V2X数据集上的实验表明，HeCoFuse在全传感器配置（LC+LC）下实现了43.22%的3D mAP，比基线CoopDet3D高出1.17%。在L+LC场景下达到更高的43.38%的3D mAP。在九种异构传感器配置下，3D mAP保持在21.74%至43.38%的范围内。该成果在CVPR 2025 DriveX挑战赛中获得第一名。", "conclusion": "HeCoFuse是目前TUM-Traf V2X数据集上的最先进方法，在各种异构传感器部署中展现出强大的鲁棒性能，有效解决了异构V2X协同感知中的关键挑战。"}}
{"id": "2507.13857", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13857", "abs": "https://arxiv.org/abs/2507.13857", "authors": ["Max van den Hoven", "Kishaan Jeeveswaran", "Pieter Piscaer", "Thijs Wensveen", "Elahe Arani", "Bahram Zonooz"], "title": "Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation", "comment": null, "summary": "Monocular 3D lane detection is essential for autonomous driving, but\nchallenging due to the inherent lack of explicit spatial information.\nMulti-modal approaches rely on expensive depth sensors, while methods\nincorporating fully-supervised depth networks rely on ground-truth depth data\nthat is impractical to collect at scale. Additionally, existing methods assume\nthat camera parameters are available, limiting their applicability in scenarios\nlike crowdsourced high-definition (HD) lane mapping. To address these\nlimitations, we propose Depth3DLane, a novel dual-pathway framework that\nintegrates self-supervised monocular depth estimation to provide explicit\nstructural information, without the need for expensive sensors or additional\nground-truth depth data. Leveraging a self-supervised depth network to obtain a\npoint cloud representation of the scene, our bird's-eye view pathway extracts\nexplicit spatial information, while our front view pathway simultaneously\nextracts rich semantic information. Depth3DLane then uses 3D lane anchors to\nsample features from both pathways and infer accurate 3D lane geometry.\nFurthermore, we extend the framework to predict camera parameters on a\nper-frame basis and introduce a theoretically motivated fitting procedure to\nenhance stability on a per-segment basis. Extensive experiments demonstrate\nthat Depth3DLane achieves competitive performance on the OpenLane benchmark\ndataset. Furthermore, experimental results show that using learned parameters\ninstead of ground-truth parameters allows Depth3DLane to be applied in\nscenarios where camera calibration is infeasible, unlike previous methods.", "AI": {"tldr": "Depth3DLane是一种新的单目3D车道线检测框架，它通过集成自监督深度估计和双路径结构，在无需昂贵传感器或深度真值数据的情况下提供明确的空间信息，并能处理未知相机参数。", "motivation": "单目3D车道线检测因缺乏显式空间信息而具有挑战性。现有方法要么依赖昂贵的深度传感器，要么需要难以大规模收集的深度真值数据。此外，它们通常假设相机参数已知，这限制了其在众包高清车道图等场景中的应用。", "method": "Depth3DLane是一个双路径框架，集成了自监督单目深度估计以提供显式结构信息。它利用自监督深度网络获取场景点云表示，并通过鸟瞰图路径提取显式空间信息，同时通过前视图路径提取丰富的语义信息。该方法使用3D车道锚点从两条路径中采样特征以推断准确的3D车道几何。此外，它扩展了框架以逐帧预测相机参数，并引入了理论驱动的拟合过程以增强逐段稳定性。", "result": "Depth3DLane在OpenLane基准数据集上取得了有竞争力的性能。实验结果表明，使用学习到的参数而非真值参数，Depth3DLane可以在相机校准不可行的情况下应用，这是以往方法无法实现的。", "conclusion": "Depth3DLane通过结合自监督深度估计和相机参数预测，克服了单目3D车道线检测的现有局限性，提供了一种无需昂贵传感器、深度真值数据且适用于未知相机参数场景的实用解决方案。"}}
{"id": "2507.13514", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13514", "abs": "https://arxiv.org/abs/2507.13514", "authors": ["Bhumika Laxman Sadbhave", "Philipp Vaeth", "Denise Dejon", "Gunther Schorcht", "Magda Gregorová"], "title": "Sugar-Beet Stress Detection using Satellite Image Time Series", "comment": null, "summary": "Satellite Image Time Series (SITS) data has proven effective for agricultural\ntasks due to its rich spectral and temporal nature. In this study, we tackle\nthe task of stress detection in sugar-beet fields using a fully unsupervised\napproach. We propose a 3D convolutional autoencoder model to extract meaningful\nfeatures from Sentinel-2 image sequences, combined with\nacquisition-date-specific temporal encodings to better capture the growth\ndynamics of sugar-beets. The learned representations are used in a downstream\nclustering task to separate stressed from healthy fields. The resulting stress\ndetection system can be directly applied to data from different years, offering\na practical and accessible tool for stress detection in sugar-beets.", "AI": {"tldr": "本研究提出了一种基于3D卷积自编码器的无监督方法，利用Sentinel-2卫星图像时间序列数据检测甜菜田的胁迫。", "motivation": "卫星图像时间序列数据在农业任务中表现出有效性，但甜菜田胁迫检测仍需更实用和可访问的工具，尤其是无监督方法。", "method": "核心方法是一个3D卷积自编码器模型，用于从Sentinel-2图像序列中提取特征，并结合采集日期特定的时间编码以捕捉甜菜生长动态。提取的特征随后用于下游聚类任务，区分受胁迫和健康的田地。", "result": "开发出的胁迫检测系统可以直接应用于不同年份的数据，能够有效区分受胁迫和健康的甜菜田。", "conclusion": "该研究提供了一个实用且易于访问的甜菜胁迫检测工具，其无监督特性和跨年份适用性使其具有很高的应用价值。"}}
{"id": "2507.13732", "categories": ["cs.CL", "cs.LG", "J.1; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13732", "abs": "https://arxiv.org/abs/2507.13732", "authors": ["Guillaume Zambrano"], "title": "The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction", "comment": "23 pages, 24 figures shorter version submitted to JURIX 2025", "summary": "This study examines the role of human judges in legal decision-making by\nusing machine learning to predict child physical custody outcomes in French\nappellate courts. Building on the legal realism-formalism debate, we test\nwhether individual judges' decision-making patterns significantly influence\ncase outcomes, challenging the assumption that judges are neutral variables\nthat apply the law uniformly. To ensure compliance with French privacy laws, we\nimplement a strict pseudonymization process. Our analysis uses 18,937 living\narrangements rulings extracted from 10,306 cases. We compare models trained on\nindividual judges' past rulings (specialist models) with a judge-agnostic model\ntrained on aggregated data (generalist models). The prediction pipeline is a\nhybrid approach combining large language models (LLMs) for structured feature\nextraction and ML models for outcome prediction (RF, XGB and SVC). Our results\nshow that specialist models consistently achieve higher predictive accuracy\nthan the general model, with top-performing models reaching F1 scores as high\nas 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x\nmore samples. Specialist models capture stable individual patterns that are not\ntransferable to other judges. In-Domain and Cross-Domain validity tests provide\nempirical support for legal realism, demonstrating that judicial identity plays\na measurable role in legal outcomes. All data and code used will be made\navailable.", "AI": {"tldr": "本研究利用机器学习预测法国上诉法院的儿童抚养权判决结果，发现个体法官的判决模式显著影响案件结果，支持了法律现实主义。", "motivation": "研究旨在检验个体法官的决策模式是否显著影响案件结果，挑战法官是统一适用法律的中立变量的假设，以探究法律现实主义与形式主义的争论。", "method": "研究使用了10,306个案件中的18,937份抚养权裁决，并实施了严格的匿名化处理。预测流程采用混合方法，结合大型语言模型（LLMs）进行结构化特征提取，以及机器学习模型（RF、XGB和SVC）进行结果预测。研究对比了基于个体法官过往判决训练的“专家模型”与基于聚合数据训练的“通用模型”。", "result": "专家模型持续取得比通用模型更高的预测准确性，其中表现最佳的模型F1分数高达92.85%，而通用模型为82.63%。专家模型捕获了稳定的个体判决模式，这些模式不可转移到其他法官。域内和跨域有效性测试为法律现实主义提供了经验支持。", "conclusion": "法官的身份在法律结果中扮演着可衡量的角色，证实了法律现实主义的观点。"}}
{"id": "2507.13739", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13739", "abs": "https://arxiv.org/abs/2507.13739", "authors": ["Junsu Kim", "Yunhoe Ku", "Seungryul Baek"], "title": "Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning", "comment": "6th CLVISION ICCV Workshop accepted", "summary": "Few-shot class-incremental learning (FSCIL) is challenging due to extremely\nlimited training data; while aiming to reduce catastrophic forgetting and learn\nnew information. We propose Diffusion-FSCIL, a novel approach that employs a\ntext-to-image diffusion model as a frozen backbone. Our conjecture is that\nFSCIL can be tackled using a large generative model's capabilities benefiting\nfrom 1) generation ability via large-scale pre-training; 2) multi-scale\nrepresentation; 3) representational flexibility through the text encoder. To\nmaximize the representation capability, we propose to extract multiple\ncomplementary diffusion features to play roles as latent replay with slight\nsupport from feature distillation for preventing generative biases. Our\nframework realizes efficiency through 1) using a frozen backbone; 2) minimal\ntrainable components; 3) batch processing of multiple feature extractions.\nExtensive experiments on CUB-200, \\emph{mini}ImageNet, and CIFAR-100 show that\nDiffusion-FSCIL surpasses state-of-the-art methods, preserving performance on\npreviously learned classes and adapting effectively to new ones.", "AI": {"tldr": "该论文提出Diffusion-FSCIL，一种利用预训练文本到图像扩散模型作为冻结骨干网络，通过提取多尺度扩散特征进行潜在回放和特征蒸馏，以解决小样本类增量学习（FSCIL）中的灾难性遗忘和数据稀缺问题，并实现了优于现有技术水平的性能。", "motivation": "小样本类增量学习（FSCIL）面临巨大挑战，主要原因在于训练数据极其有限，同时需要有效减少灾难性遗忘并学习新信息。", "method": "本文提出Diffusion-FSCIL方法，核心是使用一个冻结的文本到图像扩散模型作为骨干网络。该方法利用大型生成模型的能力，包括其大规模预训练带来的生成能力、多尺度表示以及通过文本编码器实现的表示灵活性。为最大化表示能力，模型提取多个互补的扩散特征作为潜在回放，并辅以特征蒸馏以防止生成偏差。该框架通过使用冻结骨干、最少可训练组件和批量处理多特征提取实现高效性。", "result": "在CUB-200、miniImageNet和CIFAR-100上的大量实验表明，Diffusion-FSCIL超越了现有最先进的方法，成功保持了对先前学习类的性能，并有效适应了新类别。", "conclusion": "利用大型预训练文本到图像扩散模型作为冻结骨干，通过提取多尺度特征并结合潜在回放和特征蒸馏，可以有效解决FSCIL中的数据稀缺和灾难性遗忘问题，实现卓越的性能和效率。"}}
{"id": "2507.13527", "categories": ["cs.CV", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.13527", "abs": "https://arxiv.org/abs/2507.13527", "authors": ["Levi Harris", "Md Jayed Hossain", "Mufan Qiu", "Ruichen Zhang", "Pingchuan Ma", "Tianlong Chen", "Jiaqi Gu", "Seth Ariel Tongay", "Umberto Celano"], "title": "SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM", "comment": null, "summary": "The increasing use of two-dimensional (2D) materials in nanoelectronics\ndemands robust metrology techniques for electrical characterization, especially\nfor large-scale production. While atomic force microscopy (AFM) techniques like\nconductive AFM (C-AFM) offer high accuracy, they suffer from slow data\nacquisition speeds due to the raster scanning process. To address this, we\nintroduce SparseC-AFM, a deep learning model that rapidly and accurately\nreconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM\nscans. Our approach is robust across various scanning modes, substrates, and\nexperimental conditions. We report a comparison between (a) classic flow\nimplementation, where a high pixel density C-AFM image (e.g., 15 minutes to\ncollect) is manually parsed to extract relevant material parameters, and (b)\nour SparseC-AFM method, which achieves the same operation using data that\nrequires substantially less acquisition time (e.g., under 5 minutes).\nSparseC-AFM enables efficient extraction of critical material parameters in\nMoS$_2$, including film coverage, defect density, and identification of\ncrystalline island boundaries, edges, and cracks. We achieve over 11x reduction\nin acquisition time compared to manual extraction from a full-resolution C-AFM\nimage. Moreover, we demonstrate that our model-predicted samples exhibit\nremarkably similar electrical properties to full-resolution data gathered using\nclassic-flow scanning. This work represents a significant step toward\ntranslating AI-assisted 2D material characterization from laboratory research\nto industrial fabrication. Code and model weights are available at\ngithub.com/UNITES-Lab/sparse-cafm.", "AI": {"tldr": "该研究提出SparseC-AFM，一个深度学习模型，能从稀疏的导电原子力显微镜（C-AFM）扫描数据中快速准确地重建二维材料（如MoS2）的导电率图，显著缩短了材料表征时间。", "motivation": "纳米电子学中二维材料的使用日益增加，需要鲁棒的电学表征计量技术，尤其是在大规模生产中。传统的C-AFM技术虽然精度高，但由于光栅扫描过程导致数据采集速度慢。", "method": "引入SparseC-AFM，一个深度学习模型，用于从稀疏的C-AFM扫描数据中快速准确地重建二维材料的导电率图。该方法在各种扫描模式、衬底和实验条件下均表现出鲁棒性。", "result": "与传统方法相比，SparseC-AFM将数据采集时间缩短了超过11倍（例如，从15分钟缩短到5分钟以下）。该模型能够高效提取MoS2的关键材料参数，包括薄膜覆盖率、缺陷密度以及晶岛边界、边缘和裂纹的识别。模型预测的样本与全分辨率数据表现出非常相似的电学特性。", "conclusion": "这项工作代表了将人工智能辅助的二维材料表征从实验室研究转化为工业制造的重要一步。"}}
{"id": "2507.13743", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13743", "abs": "https://arxiv.org/abs/2507.13743", "authors": ["Maluna Menke", "Thilo Hagendorff"], "title": "PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs", "comment": null, "summary": "Large Language Models (LLMs) frequently reproduce the gender- and\nsexual-identity prejudices embedded in their training corpora, leading to\noutputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of\ngreat importance. To achieve this, we evaluate two parameter-efficient\nfine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt\ntuning - as lightweight alternatives to full-model fine-tuning for mitigating\nsuch biases. Using the WinoQueer benchmark, we quantify bias in three\nopen-source LLMs and observe baseline bias scores reaching up to 98 (out of\n100) across a range of queer identities defined by gender and/or sexual\norientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%\nadditional parameters) on a curated QueerNews corpus reduces those scores by up\nto 50 points and raises neutrality from virtually 0% to as much as 36%.\nSoft-prompt tuning (10 virtual tokens) delivers only marginal improvements.\nThese findings show that LoRA can deliver meaningful fairness gains with\nminimal computation. We advocate broader adoption of community-informed PEFT,\nthe creation of larger queer-authored corpora, and richer evaluation suites\nbeyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.", "AI": {"tldr": "本研究评估了两种参数高效微调（PEFT）技术（LoRA和软提示调优）在减轻大型语言模型（LLMs）中针对LGBTQIA+群体的偏见方面的效果，发现LoRA能以极低的计算成本显著降低偏见。", "motivation": "大型语言模型（LLMs）经常复制训练语料库中存在的性别和性取向偏见，导致输出歧视LGBTQIA+用户，因此减少这些偏见至关重要。", "method": "研究评估了LoRA和软提示调优这两种PEFT技术，作为全模型微调的轻量级替代方案。他们使用WinoQueer基准测试量化了三款开源LLM的偏见，并使用一个精选的QueerNews语料库对模型进行微调。", "result": "基线偏见分数高达98（满分100，50为中立）。使用LoRA（不到0.1%的额外参数）进行微调可将这些分数降低多达50点，并将中立性从几乎0%提高到36%。软提示调优（10个虚拟token）仅带来微不足道的改进。", "conclusion": "LoRA能以最小的计算量带来显著的公平性提升。研究倡导更广泛地采用社区参与的PEFT方法、创建更大的由酷儿群体创作的语料库，以及开发超越WinoQueer的更丰富的评估套件，并结合持续审计以保持LLMs的包容性。"}}
{"id": "2507.13769", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13769", "abs": "https://arxiv.org/abs/2507.13769", "authors": ["Mingyang Yu", "Zhijian Wu", "Dingjiang Huang"], "title": "Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction", "comment": null, "summary": "Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its\ndegraded 2D measurements. Recently great progress has been made in deep\nlearning-based methods, however, these methods often struggle to accurately\ncapture high-frequency details of the HSI. To address this issue, this paper\nproposes a Spectral Diffusion Prior (SDP) that is implicitly learned from\nhyperspectral images using a diffusion model. Leveraging the powerful ability\nof the diffusion model to reconstruct details, this learned prior can\nsignificantly improve the performance when injected into the HSI model. To\nfurther improve the effectiveness of the learned prior, we also propose the\nSpectral Prior Injector Module (SPIM) to dynamically guide the model to recover\nthe HSI details. We evaluate our method on two representative HSI methods: MST\nand BISRNet. Experimental results show that our method outperforms existing\nnetworks by about 0.5 dB, effectively improving the performance of HSI\nreconstruction.", "AI": {"tldr": "该论文提出一种基于扩散模型的频谱扩散先验（SDP）和频谱先验注入模块（SPIM），以解决高光谱图像（HSI）重建中高频细节恢复不足的问题。", "motivation": "现有基于深度学习的高光谱图像（HSI）重建方法难以准确捕捉高光谱图像的高频细节。", "method": "本文提出了一种从高光谱图像中隐式学习的频谱扩散先验（SDP），利用扩散模型强大的细节重建能力。为进一步提高学习到的先验的有效性，还提出了频谱先验注入模块（SPIM），用于动态引导模型恢复HSI细节。该方法在MST和BISRNet两种代表性HSI方法上进行了评估。", "result": "实验结果表明，该方法比现有网络性能提高了约0.5 dB，有效提升了高光谱图像重建的性能。", "conclusion": "通过引入基于扩散模型的频谱扩散先验和频谱先验注入模块，可以显著提高高光谱图像重建中高频细节的恢复能力，从而提升整体性能。"}}
{"id": "2507.13530", "categories": ["cs.CV", "math.DG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.13530", "abs": "https://arxiv.org/abs/2507.13530", "authors": ["Lukas Baumgärtner", "Ronny Bergmann", "Roland Herzog", "Stephan Schmidt", "Manuel Weiß"], "title": "Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising", "comment": null, "summary": "We propose a novel formulation for the second-order total generalized\nvariation (TGV) of the normal vector on an oriented, triangular mesh embedded\nin $\\mathbb{R}^3$. The normal vector is considered as a manifold-valued\nfunction, taking values on the unit sphere. Our formulation extends previous\ndiscrete TGV models for piecewise constant scalar data that utilize a\nRaviart-Thomas function space. To exctend this formulation to the manifold\nsetting, a tailor-made tangential Raviart-Thomas type finite element space is\nconstructed in this work. The new regularizer is compared to existing methods\nin mesh denoising experiments.", "AI": {"tldr": "本文提出了一种针对三维网格上法向量的二阶全广义变分（TGV）新公式，通过构建定制的切向Raviart-Thomas型有限元空间，将TGV扩展到流形值函数，并应用于网格去噪。", "motivation": "现有离散TGV模型主要针对分段常数标量数据，并利用Raviart-Thomas函数空间。本研究旨在将TGV公式扩展到处理流形值数据（如单位球上的法向量）。", "method": "提出了一种新颖的二阶全广义变分（TGV）公式，用于R3中定向三角网格上的法向量。将法向量视为流形值函数，取值于单位球。为此，构建了一个定制的切向Raviart-Thomas型有限元空间。新正则化器在网格去噪实验中与现有方法进行了比较。", "result": "所提出的新正则化器在网格去噪实验中与现有方法进行了比较，表明了其在处理网格法向量去噪问题上的适用性和有效性。", "conclusion": "成功开发了一种新的TGV公式，能够处理嵌入在R3中的定向三角网格上的流形值法向量，并通过构建专门的有限元空间实现了这一扩展，为网格去噪提供了有效的新工具。"}}
{"id": "2507.13761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13761", "abs": "https://arxiv.org/abs/2507.13761", "authors": ["Palash Nandi", "Maithili Joshi", "Tanmoy Chakraborty"], "title": "Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models", "comment": null, "summary": "Language models are highly sensitive to prompt formulations - small changes\nin input can drastically alter their output. This raises a critical question:\nTo what extent can prompt sensitivity be exploited to generate inapt content?\nIn this paper, we investigate how discrete components of prompt design\ninfluence the generation of inappropriate content in Visual Language Models\n(VLMs). Specifically, we analyze the impact of three key factors on successful\njailbreaks: (a) the inclusion of detailed visual information, (b) the presence\nof adversarial examples, and (c) the use of positively framed beginning\nphrases. Our findings reveal that while a VLM can reliably distinguish between\nbenign and harmful inputs in unimodal settings (text-only or image-only), this\nability significantly degrades in multimodal contexts. Each of the three\nfactors is independently capable of triggering a jailbreak, and we show that\neven a small number of in-context examples (as few as three) can push the model\ntoward generating inappropriate outputs. Furthermore, we propose a framework\nthat utilizes a skip-connection between two internal layers of the VLM, which\nsubstantially increases jailbreak success rates, even when using benign images.\nFinally, we demonstrate that memes, often perceived as humorous or harmless,\ncan be as effective as toxic visuals in eliciting harmful content, underscoring\nthe subtle and complex vulnerabilities of VLMs.", "AI": {"tldr": "本文研究了视觉语言模型（VLMs）中提示敏感性如何被利用来生成不当内容，发现多模态上下文下的模型脆弱性显著增加，即使是良性输入和幽默内容也能触发不当输出。", "motivation": "语言模型对提示词高度敏感，微小输入变化可大幅改变输出。这引出了一个关键问题：提示敏感性在多大程度上可被利用来生成不当内容？", "method": "研究分析了提示设计中三个离散组件对VLM生成不当内容（即越狱）的影响：1) 详细视觉信息的包含；2) 对抗性示例的存在；3) 积极框架的起始短语的使用。此外，提出了一种利用VLM内部两层之间跳跃连接的框架，并探讨了少量上下文示例和模因的影响。", "result": "研究发现，VLM在单模态设置下能可靠区分良性与有害输入，但在多模态上下文中，此能力显著下降。上述三个因素均能独立触发越狱，甚至少量上下文示例（少至三个）也能促使模型生成不当输出。所提出的跳跃连接框架能大幅提高越狱成功率，即使使用良性图像。最后，模因也能像有害视觉内容一样有效引发有害内容。", "conclusion": "VLMs在多模态情境下存在复杂且微妙的漏洞，其对提示敏感性的利用可导致不当内容的生成，即使是看似无害的输入（如模因）也能引发有害输出。"}}
{"id": "2507.13789", "categories": ["cs.CV", "cs.AI", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.13789", "abs": "https://arxiv.org/abs/2507.13789", "authors": ["Kyriakos Flouris", "Moritz Halter", "Yolanne Y. R. Lee", "Samuel Castonguay", "Luuk Jacobs", "Pietro Dirix", "Jonathan Nestmann", "Sebastian Kozerke", "Ender Konukoglu"], "title": "Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI", "comment": null, "summary": "Hemodynamic analysis is essential for predicting aneurysm rupture and guiding\ntreatment. While magnetic resonance flow imaging enables time-resolved\nvolumetric blood velocity measurements, its low spatiotemporal resolution and\nsignal-to-noise ratio limit its diagnostic utility. To address this, we propose\nthe Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that\nenhances both spatial and temporal resolution with the ability to predict wall\nshear stress (WSS) directly from clinical imaging data. LoFNO integrates\nLaplacian eigenvectors as geometric priors for improved structural awareness on\nirregular, unseen geometries and employs an Enhanced Deep Super-Resolution\nNetwork (EDSR) layer for robust upsampling. By combining geometric priors with\nneural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow\ndata, achieving superior velocity and WSS predictions compared to interpolation\nand alternative deep learning methods, enabling more precise cerebrovascular\ndiagnostics.", "AI": {"tldr": "本文提出LoFNO，一种新型3D傅里叶神经算子，用于提高磁共振血流图像的时空分辨率和信噪比，并直接预测壁面剪切应力，从而实现更精确的脑血管诊断。", "motivation": "磁共振血流成像的时空分辨率和信噪比较低，限制了其在动脉瘤破裂预测和治疗指导中的诊断效用，而血流动力学分析对此至关重要。", "method": "LoFNO是一种新颖的3D架构，它将拉普拉斯特征向量作为几何先验以增强对不规则几何结构的感知，并采用增强型深度超分辨率网络（EDSR）层进行鲁棒的上采样。通过结合几何先验和神经算子框架，LoFNO对血流数据进行去噪和时空超分辨率处理。", "result": "LoFNO在速度和壁面剪切应力预测方面优于传统的插值方法和替代的深度学习方法。", "conclusion": "LoFNO通过提高临床成像数据的分辨率和直接预测壁面剪切应力，能够实现更精确的脑血管诊断。"}}
{"id": "2507.13546", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13546", "abs": "https://arxiv.org/abs/2507.13546", "authors": ["Dmitrii Mikhailov", "Aleksey Letunovskiy", "Maria Kovaleva", "Vladimir Arkhipkin", "Vladimir Korviakov", "Vladimir Polovnikov", "Viacheslav Vasilev", "Evelina Sidorova", "Denis Dimitrov"], "title": "$\\nabla$NABLA: Neighborhood Adaptive Block-Level Attention", "comment": null, "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA", "AI": {"tldr": "NABLA提出了一种新颖的邻域自适应块级注意力机制，用于视频扩散Transformer（DiTs），通过动态适应稀疏模式来降低计算开销，同时保持生成质量，实现了显著的训练和推理加速。", "motivation": "Transformer架构在视频生成中表现出色，但全注意力机制的二次复杂度是高分辨率和长持续时间视频序列的关键瓶颈。", "method": "本文提出了NABLA（Neighborhood Adaptive Block-Level Attention），它通过利用块级注意力与自适应稀疏驱动阈值，动态适应视频扩散Transformer中的稀疏模式。该方法无需自定义底层操作，可无缝集成PyTorch的Flex Attention算子。", "result": "实验表明，与基线相比，NABLA实现了高达2.7倍的训练和推理速度提升，几乎不影响定量指标（CLIP分数、VBench分数、人工评估分数）和视觉质量。", "conclusion": "NABLA通过其新颖的注意力机制有效解决了视频扩散Transformer的计算瓶颈，显著提高了训练和推理效率，同时保持了高质量的视频生成能力。"}}
{"id": "2507.13793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13793", "abs": "https://arxiv.org/abs/2507.13793", "authors": ["Enhao Cheng", "Shoujia Zhang", "Jianhua Yin", "Xuemeng Song", "Tian Gan", "Liqiang Nie"], "title": "An Enhanced Model-based Approach for Short Text Clustering", "comment": null, "summary": "Short text clustering has become increasingly important with the popularity\nof social media like Twitter, Google+, and Facebook. Existing methods can be\nbroadly categorized into two paradigms: topic model-based approaches and deep\nrepresentation learning-based approaches. This task is inherently challenging\ndue to the sparse, large-scale, and high-dimensional characteristics of the\nshort text data. Furthermore, the computational intensity required by\nrepresentation learning significantly increases the running time. To address\nthese issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet\nMultinomial Mixture model (GSDMM), which effectively handles the sparsity and\nhigh dimensionality of short texts while identifying representative words for\neach cluster. Based on several aspects of GSDMM that warrant further\nrefinement, we propose an improved approach, GSDMM+, designed to further\noptimize its performance. GSDMM+ reduces initialization noise and adaptively\nadjusts word weights based on entropy, achieving fine-grained clustering that\nreveals more topic-related information. Additionally, strategic cluster merging\nis employed to refine clustering granularity, better aligning the predicted\ndistribution with the true category distribution. We conduct extensive\nexperiments, comparing our methods with both classical and state-of-the-art\napproaches. The experimental results demonstrate the efficiency and\neffectiveness of our methods. The source code for our model is publicly\navailable at https://github.com/chehaoa/VEMC.", "AI": {"tldr": "该论文提出了GSDMM及其改进版GSDMM+，用于解决短文本聚类中的稀疏性、高维性和计算强度问题，并通过实验证明了其效率和有效性。", "motivation": "短文本聚类在社交媒体普及的背景下变得日益重要，但其固有的稀疏性、大规模和高维度特性，以及表示学习带来的高计算强度，使得现有方法面临挑战。", "method": "本文提出了一种基于狄利克雷多项式混合模型（DMM）的折叠吉布斯采样算法（GSDMM），以有效处理短文本的稀疏性和高维度。在此基础上，进一步提出了改进的GSDMM+，通过减少初始化噪声、基于熵自适应调整词权重实现细粒度聚类，并采用策略性簇合并来优化聚类粒度，使其更好地与真实类别分布对齐。", "result": "通过与经典和最先进方法的广泛实验比较，结果表明所提出的GSDMM和GSDMM+方法在效率和有效性方面均表现出色。", "conclusion": "GSDMM和GSDMM+能够有效且高效地处理短文本聚类任务中的挑战，尤其在处理稀疏性和高维度数据方面表现优异，并能识别代表性词汇和揭示更多主题相关信息。"}}
{"id": "2507.13801", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13801", "abs": "https://arxiv.org/abs/2507.13801", "authors": ["Haoang Lu", "Yuanqi Su", "Xiaoning Zhang", "Hao Hu"], "title": "One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion", "comment": null, "summary": "In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a\ncritical perception task for autonomous driving due to its ability to infer\ncomplete 3D scene layouts and semantics from single 2D images. However, in\nreal-world traffic scenarios, a significant portion of the scene remains\noccluded or outside the camera's field of view -- a fundamental challenge that\nexisting monocular SSC methods fail to address adequately. To overcome these\nlimitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC\nframework that leverages pseudo-future frame prediction to expand the model's\neffective perceptual range. Our approach combines poses and depths to establish\naccurate 3D correspondences, enabling geometrically-consistent fusion of past,\npresent, and predicted future frames in 3D space. Unlike conventional methods\nthat rely on simple feature stacking, our 3D-aware architecture achieves more\nrobust scene completion by explicitly modeling spatial-temporal relationships.\nComprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks\ndemonstrate state-of-the-art performance, validating the effectiveness of our\napproach, highlighting our method's ability to improve occlusion reasoning and\n3D scene completion accuracy.", "AI": {"tldr": "本文提出CF-SSC，一种新颖的基于时间序列的3D语义场景补全（SSC）框架，通过预测伪未来帧来扩展感知范围，有效解决单目SSC中遮挡和视野外区域的挑战，并实现了最先进的性能。", "motivation": "现有的单目3D语义场景补全方法在实际交通场景中，无法充分处理大量被遮挡或超出相机视野的区域，这是一个基本挑战。", "method": "本文提出CF-SSC，一个利用伪未来帧预测的全新时序SSC框架。该方法结合位姿和深度信息建立精确的3D对应关系，实现过去、现在和预测未来帧在3D空间中的几何一致性融合。与传统方法不同，其3D感知架构通过显式建模时空关系，实现更鲁棒的场景补全。", "result": "在SemanticKITTI和SSCBench-KITTI-360基准测试上，CF-SSC展现了最先进的性能，验证了其有效性，并显著提高了遮挡推理和3D场景补全的准确性。", "conclusion": "CF-SSC通过引入伪未来帧预测和3D感知时空融合，有效克服了传统单目SSC在处理遮挡和视野外区域的局限性，显著提升了3D场景补全的准确性和鲁棒性。"}}
{"id": "2507.13568", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13568", "abs": "https://arxiv.org/abs/2507.13568", "authors": ["Kaihong Wang", "Donghyun Kim", "Margrit Betke"], "title": "LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning", "comment": null, "summary": "Continual learning for vision-language models has achieved remarkable\nperformance through synthetic replay, where samples are generated using Stable\nDiffusion to regularize during finetuning and retain knowledge. However,\nreal-world downstream applications often exhibit domain-specific nuances and\nfine-grained semantics not captured by generators, causing synthetic-replay\nmethods to produce misaligned samples that misguide finetuning and undermine\nretention of prior knowledge. In this work, we propose a LoRA-enhanced\nsynthetic-replay framework that injects task-specific low-rank adapters into a\nfrozen Stable Diffusion model, efficiently capturing each new task's unique\nvisual and semantic patterns. Specifically, we introduce a two-stage,\nconfidence-based sample selection: we first rank real task data by\npost-finetuning VLM confidence to focus LoRA finetuning on the most\nrepresentative examples, then generate synthetic samples and again select them\nby confidence for distillation. Our approach integrates seamlessly with\nexisting replay pipelines-simply swap in the adapted generator to boost replay\nfidelity. Extensive experiments on the Multi-domain Task Incremental Learning\n(MTIL) benchmark show that our method outperforms previous synthetic-replay\ntechniques, achieving an optimal balance among plasticity, stability, and\nzero-shot capability. These results demonstrate the effectiveness of generator\nadaptation via LoRA for robust continual learning in VLMs.", "AI": {"tldr": "本文提出了一种LoRA增强的合成回放框架，通过将任务特定的低秩适配器注入冻结的Stable Diffusion模型，并结合置信度样本选择机制，提高了视觉-语言模型（VLM）在持续学习中合成样本的保真度，从而增强了知识保留和性能。", "motivation": "现有合成回放方法在持续学习中，由于生成器未能捕捉到真实世界领域特定的细微差别和细粒度语义，导致生成的样本与实际任务不匹配，从而误导微调并损害对先前知识的保留。", "method": "本文提出一个LoRA增强的合成回放框架。该框架将任务特定的低秩适配器（LoRA）注入冻结的Stable Diffusion模型中，以高效捕捉每个新任务独特的视觉和语义模式。具体地，引入了一个两阶段、基于置信度的样本选择机制：首先，根据微调后VLM的置信度对真实任务数据进行排序，以选择最具代表性的样本进行LoRA微调；然后，生成合成样本并再次通过置信度进行选择，用于知识蒸馏。", "result": "在多领域任务增量学习（MTIL）基准上的大量实验表明，该方法优于先前的合成回放技术，在可塑性、稳定性和零样本能力之间实现了最佳平衡。", "conclusion": "通过LoRA进行生成器适应对于视觉-语言模型中稳健的持续学习是有效的，显著提升了合成回放的保真度，从而改善了知识保留和整体性能。"}}
{"id": "2507.13827", "categories": ["cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13827", "abs": "https://arxiv.org/abs/2507.13827", "authors": ["Hosein Azarbonyad", "Zi Long Zhu", "Georgios Cheirmpos", "Zubair Afzal", "Vikrant Yadav", "Georgios Tsatsaronis"], "title": "Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models", "comment": "SIGIR 2025", "summary": "When deciding to read an article or incorporate it into their research,\nscholars often seek to quickly identify and understand its main ideas. In this\npaper, we aim to extract these key concepts and contributions from scientific\narticles in the form of Question and Answer (QA) pairs. We propose two distinct\napproaches for generating QAs. The first approach involves selecting salient\nparagraphs, using a Large Language Model (LLM) to generate questions, ranking\nthese questions by the likelihood of obtaining meaningful answers, and\nsubsequently generating answers. This method relies exclusively on the content\nof the articles. However, assessing an article's novelty typically requires\ncomparison with the existing literature. Therefore, our second approach\nleverages a Knowledge Graph (KG) for QA generation. We construct a KG by\nfine-tuning an Entity Relationship (ER) extraction model on scientific articles\nand using it to build the graph. We then employ a salient triplet extraction\nmethod to select the most pertinent ERs per article, utilizing metrics such as\nthe centrality of entities based on a triplet TF-IDF-like measure. This measure\nassesses the saliency of a triplet based on its importance within the article\ncompared to its prevalence in the literature. For evaluation, we generate QAs\nusing both approaches and have them assessed by Subject Matter Experts (SMEs)\nthrough a set of predefined metrics to evaluate the quality of both questions\nand answers. Our evaluations demonstrate that the KG-based approach effectively\ncaptures the main ideas discussed in the articles. Furthermore, our findings\nindicate that fine-tuning the ER extraction model on our scientific corpus is\ncrucial for extracting high-quality triplets from such documents.", "AI": {"tldr": "该研究旨在从科学文章中提取关键概念和贡献，以问答（QA）对的形式呈现。提出了两种方法：一种基于大型语言模型（LLM），另一种基于知识图谱（KG），并评估了它们的有效性。", "motivation": "学者在阅读或整合文章时，需要快速识别并理解其主要思想，以便决定是否阅读或将其纳入研究。", "method": "1. **LLM方法**：选择突出段落，使用LLM生成问题，根据获得有意义答案的可能性对问题进行排名，然后生成答案。此方法仅依赖文章内容。 2. **KG方法**：通过在科学文章上微调实体关系（ER）提取模型来构建知识图谱，然后利用三元组TF-IDF类似度量（基于实体中心性）选择最相关的突出三元组。 3. **评估**：使用两种方法生成QA对，并由主题专家（SMEs）通过预定义指标评估问题和答案的质量。", "result": "评估结果表明，基于知识图谱的方法能有效捕捉文章的主要思想。此外，在科学语料库上微调ER提取模型对于从此类文档中提取高质量三元组至关重要。", "conclusion": "研究成功地通过两种方法（特别是基于知识图谱的方法）从科学文章中提取了关键概念和贡献。知识图谱方法在捕捉文章主要思想方面表现出色，且微调ER提取模型对于提高三元组质量至关重要。"}}
{"id": "2507.13820", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13820", "abs": "https://arxiv.org/abs/2507.13820", "authors": ["Jun Xie", "Zhaoran Zhao", "Xiongjun Guan", "Yingjian Zhu", "Hongzhu Yi", "Xinming Wang", "Feng Chen", "Zhepeng Wang"], "title": "Team of One: Cracking Complex Video QA with Model Synergy", "comment": null, "summary": "We propose a novel framework for open-ended video question answering that\nenhances reasoning depth and robustness in complex real-world scenarios, as\nbenchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models\n(Video-LMMs) often exhibit limited contextual understanding, weak temporal\nmodeling, and poor generalization to ambiguous or compositional queries. To\naddress these challenges, we introduce a prompting-and-response integration\nmechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)\nvia structured chains of thought, each tailored to distinct reasoning pathways.\nAn external Large Language Model (LLM) serves as an evaluator and integrator,\nselecting and fusing the most reliable responses. Extensive experiments\ndemonstrate that our method significantly outperforms existing baselines across\nall evaluation metrics, showcasing superior generalization and robustness. Our\napproach offers a lightweight, extensible strategy for advancing multimodal\nreasoning without requiring model retraining, setting a strong foundation for\nfuture Video-LMM development.", "AI": {"tldr": "本文提出了一种新颖的开放式视频问答框架，通过协调多个异构视频-语言模型（VLM）并利用大型语言模型（LLM）进行评估和集成，显著提升了推理深度和鲁棒性。", "motivation": "现有视频-大型多模态模型（Video-LMMs）在复杂现实场景中存在上下文理解有限、时间建模薄弱以及对模糊或组合查询泛化能力差的问题。", "method": "引入了一种“提示-响应集成机制”，通过结构化的思维链协调多个异构VLM，每个VLM专注于不同的推理路径。一个外部LLM充当评估器和集成器，选择并融合最可靠的响应。", "result": "在CVRR-ES数据集上，该方法在所有评估指标上显著优于现有基线，表现出卓越的泛化能力和鲁棒性。", "conclusion": "该方法提供了一种轻量级、可扩展的策略，无需模型再训练即可推进多模态推理，为未来的Video-LMM发展奠定了坚实基础。"}}
{"id": "2507.13595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13595", "abs": "https://arxiv.org/abs/2507.13595", "authors": ["Tengkai Wang", "Weihao Li", "Ruikai Cui", "Shi Qiu", "Nick Barnes"], "title": "NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision", "comment": "14 pages, 4 figures", "summary": "Reconstructing accurate implicit surface representations from point clouds\nremains a challenging task, particularly when data is captured using\nlow-quality scanning devices. These point clouds often contain substantial\nnoise, leading to inaccurate surface reconstructions. Inspired by the\nNoise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel\nmethod designed to extend this concept to 3D neural fields. Our approach\nenables learning clean neural SDFs directly from noisy point clouds through\nnoisy supervision by minimizing the MSE loss between noisy SDF representations,\nallowing the network to implicitly denoise and refine surface estimations. We\nevaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the\nShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that\nour framework significantly improves surface reconstruction quality from noisy\ninputs.", "AI": {"tldr": "该论文提出NoiseSDF2NoiseSDF，一种受Noise2Noise启发的3D神经场方法，能直接从噪声点云中学习干净的神经SDF，显著提升表面重建质量。", "motivation": "从点云重建准确的隐式表面表示是一个挑战，尤其当点云由低质量扫描设备捕获时，常包含大量噪声，导致重建不准确。", "method": "引入NoiseSDF2NoiseSDF，将2D图像的Noise2Noise范式扩展到3D神经场。通过最小化噪声SDF表示之间的MSE损失，利用噪声监督直接从噪声点云学习干净的神经SDF，使网络隐式去噪并优化表面估计。", "result": "在ShapeNet、ABC、Famous和Real等基准数据集上进行评估，实验结果表明该框架显著提高了从噪声输入重建的表面质量。", "conclusion": "NoiseSDF2NoiseSDF是一个有效的框架，能够从噪声点云中学习并重建高质量的隐式表面表示。"}}
{"id": "2507.13839", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13839", "abs": "https://arxiv.org/abs/2507.13839", "authors": ["Lizhi Ma", "Tong Zhao", "Shuai Zhang", "Nirui Song", "Hongliang He", "Anqi Li", "Ran Feng", "Huachuan Qiu", "Jingsong Ma", "Zhenzhong Lan"], "title": "The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words", "comment": null, "summary": "This study explores the relationship between linguistic expressions and\npsychological states of depression and anxiety within Chinese psycho-counseling\ninteractions, focusing specifically on the usage of first-person singular\npronouns and negative emotional words. Utilizing a corpus derived from 735\nonline counseling sessions, the analysis employed a general linear mixed-effect\nmodel to assess linguistic patterns quantified by the Linguistic Inquiry and\nWord Count (LIWC) software. Results indicate a significant positive correlation\nbetween the frequency of negative emotional words and the severity of both\ndepressive and anxious states among clients. However, contrary to prior\nfindings predominantly derived from English-language contexts, the usage\nfrequency of first-person singular pronouns did not vary significantly with the\nclients' psychological conditions. These outcomes are discussed within the\nframework of cultural distinctions between collectivist Chinese contexts and\nindividualistic Western settings, as well as the interactive dynamics unique to\npsycho-counseling conversations. The findings highlight the nuanced influence\nof cultural and conversational contexts on language use in mental health\ncommunications, providing insights into psycholinguistic markers relevant to\ntherapeutic practices in Chinese-speaking populations.", "AI": {"tldr": "本研究探讨了中文心理咨询中语言表达（第一人称单数代词、负面情绪词）与抑郁/焦虑状态的关系，发现负面情绪词频率与症状严重程度正相关，但第一人称单数代词频率无显著变化，这与西方研究不同，可能受文化和会话语境影响。", "motivation": "现有关于语言表达与心理状态关系的研究多基于英文语境，本研究旨在探索中文集体主义文化背景下，第一人称单数代词和负面情绪词的使用与抑郁、焦虑状态之间的关系，以弥补跨文化研究的空白，并为中文心理治疗提供语言标记线索。", "method": "研究利用包含735个在线心理咨询会话的语料库，采用LIWC软件量化语言模式，并使用通用线性混合效应模型评估了第一人称单数代词和负面情绪词的使用频率与客户心理状态的关系。", "result": "结果显示，负面情绪词的使用频率与来访者的抑郁和焦虑状态的严重程度呈显著正相关。然而，与此前主要基于英文语境的研究不同，第一人称单数代词的使用频率并未随客户的心理状况而显著变化。", "conclusion": "研究强调了文化（集体主义与个人主义）和会话语境对心理健康沟通中语言使用的细微影响。这些发现为中文语境下的心理治疗实践提供了重要的心理语言学标记线索，并指出在中文语境下，第一人称单数代词可能不是衡量抑郁/焦虑的通用指标。"}}
{"id": "2507.13868", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13868", "abs": "https://arxiv.org/abs/2507.13868", "authors": ["Francesco Ortu", "Zhijing Jin", "Diego Doimo", "Alberto Cazzaniga"], "title": "When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) increasingly leverage diverse knowledge sources\nto address complex tasks, often encountering conflicts between their internal\nparametric knowledge and external information. Knowledge conflicts can result\nin hallucinations and unreliable responses, but the mechanisms governing such\ninteractions remain unknown. To address this gap, we analyze the mechanisms\nthat VLMs use to resolve cross-modal conflicts by introducing a dataset of\nmultimodal counterfactual queries that deliberately contradict internal\ncommonsense knowledge. We localize with logit inspection a small set of heads\nthat control the conflict. Moreover, by modifying these heads, we can steer the\nmodel towards its internal knowledge or the visual inputs. Finally, we show\nthat attention from such heads pinpoints localized image regions driving visual\noverrides, outperforming gradient-based attribution in precision.", "AI": {"tldr": "该研究分析了视觉-语言模型（VLMs）如何解决其内部知识与外部视觉信息之间的冲突，识别并操控了控制冲突的注意力头，并展示了其在定位视觉覆盖区域上的高精度。", "motivation": "VLMs在处理复杂任务时，其内部参数化知识与外部信息（如视觉输入）之间常出现冲突，导致幻觉和不可靠的响应。然而，控制这些交互的机制尚不明确。", "method": "研究引入了一个多模态反事实查询数据集，该数据集故意与模型的内部常识知识相矛盾。通过logit检查定位了控制冲突的少量注意力头。通过修改这些头，可以引导模型偏向内部知识或视觉输入。此外，研究展示了这些头的注意力可以精确地定位驱动视觉覆盖的图像区域。", "result": "研究发现并定位了控制VLMs中知识冲突的一小部分注意力头。通过修改这些头，可以成功地将模型响应引导至其内部知识或视觉输入。这些头的注意力在定位驱动视觉覆盖的图像区域方面表现出高精度，优于基于梯度的归因方法。", "conclusion": "该研究揭示了VLMs解决跨模态知识冲突的机制，通过识别和操控特定的注意力头，不仅能理解模型如何权衡信息，还能精确控制其行为并解释视觉输入的影响。"}}
{"id": "2507.13599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13599", "abs": "https://arxiv.org/abs/2507.13599", "authors": ["Chengxu Liu", "Lu Qi", "Jinshan Pan", "Xueming Qian", "Ming-Hsuan Yang"], "title": "Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model", "comment": "Accepted by ICCV2025", "summary": "Since acquiring large amounts of realistic blurry-sharp image pairs is\ndifficult and expensive, learning blind image deblurring from unpaired data is\na more practical and promising solution. Unfortunately, dominant approaches\nrely heavily on adversarial learning to bridge the gap from blurry domains to\nsharp domains, ignoring the complex and unpredictable nature of real-world blur\npatterns. In this paper, we propose a novel diffusion model (DM)-based\nframework, dubbed \\ours, for image deblurring by learning spatially varying\ntexture prior from unpaired data. In particular, \\ours performs DM to generate\nthe prior knowledge that aids in recovering the textures of blurry images. To\nimplement this, we propose a Texture Prior Encoder (TPE) that introduces a\nmemory mechanism to represent the image textures and provides supervision for\nDM training. To fully exploit the generated texture priors, we present the\nTexture Transfer Transformer layer (TTformer), in which a novel\nFilter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes\nspatially varying blurring through adaptive filtering. Furthermore, we\nimplement a wavelet-based adversarial loss to preserve high-frequency texture\ndetails. Extensive evaluations show that \\ours provides a promising\nunsupervised deblurring solution and outperforms SOTA methods in widely-used\nbenchmarks.", "AI": {"tldr": "本文提出了一种新颖的基于扩散模型（DM）的框架，用于从非配对数据中学习空间变化的纹理先验，以实现盲图像去模糊。", "motivation": "获取大量真实的模糊-清晰图像对既困难又昂贵，而现有主流的非配对去模糊方法过于依赖对抗学习，忽略了真实世界中复杂且不可预测的模糊模式。", "method": "该研究提出一个名为“ours”的DM框架。它利用DM生成纹理恢复的先验知识，并通过一个带有记忆机制的纹理先验编码器（TPE）来表示图像纹理并监督DM训练。为充分利用生成的纹理先验，引入了纹理传输Transformer层（TTformer），其中包含一个新颖的滤波器调制多头自注意力（FM-MSA）机制，通过自适应滤波有效去除空间变化的模糊。此外，还采用基于小波的对抗损失来保留高频纹理细节。", "result": "广泛的评估表明，该方法提供了一个有前景的无监督去模糊解决方案，并在广泛使用的基准测试中超越了现有最先进的方法。", "conclusion": "所提出的基于DM的框架通过学习空间变化的纹理先验，并结合TPE、TTformer和基于小波的对抗损失，为非配对盲图像去模糊提供了一种有效且性能优异的解决方案。"}}
{"id": "2507.13841", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13841", "abs": "https://arxiv.org/abs/2507.13841", "authors": ["Eitan Wagner", "Renana Keydar", "Omri Abend"], "title": "Modeling Fair Play in Detective Stories with Language Models", "comment": null, "summary": "Effective storytelling relies on a delicate balance between meeting the\nreader's prior expectations and introducing unexpected developments. In the\ndomain of detective fiction, this tension is known as fair play, which includes\nthe implicit agreement between the writer and the reader as to the range of\npossible resolutions the mystery story may have. In this work, we present a\nprobabilistic framework for detective fiction that allows us to define desired\nqualities. Using this framework, we formally define fair play and design\nappropriate metrics for it. Stemming from these definitions is an inherent\ntension between the coherence of the story, which measures how much it ``makes\nsense'', and the surprise it induces. We validate the framework by applying it\nto LLM-generated detective stories. This domain is appealing since we have an\nabundance of data, we can sample from the distribution generating the story,\nand the story-writing capabilities of LLMs are interesting in their own right.\nResults show that while LLM-generated stories may be unpredictable, they\ngenerally fail to balance the trade-off between surprise and fair play, which\ngreatly contributes to their poor quality.", "AI": {"tldr": "本文提出了一个用于侦探小说的概率框架，以量化和衡量“公平竞争”（fair play）这一概念，并发现LLM生成的故事虽然不可预测，但在平衡惊喜和连贯性方面表现不佳，导致质量低下。", "motivation": "有效的叙事需要平衡读者的预期和意想不到的情节发展。在侦探小说中，这种平衡被称为“公平竞争”。研究动机在于形式化地定义和衡量这一概念，并评估LLM在生成此类故事时的表现，因为LLM生成的故事常被认为质量不佳。", "method": "研究者提出了一个针对侦探小说的概率框架，用于形式化定义“公平竞争”并设计相应的度量标准。该框架还衡量了故事的连贯性（“有意义”的程度）和它所引起的惊喜。研究通过将此框架应用于LLM生成的侦探小说来验证其有效性。", "result": "结果表明，虽然LLM生成的故事可能不可预测，但它们通常未能平衡惊喜和“公平竞争”之间的权衡，这极大地导致了其故事质量的低下。", "conclusion": "该概率框架能够有效定义和衡量侦探小说中的“公平竞争”和惊喜-连贯性之间的张力。LLM在生成侦探小说时，难以在惊喜和“公平竞争”之间取得平衡，这限制了其故事的质量。"}}
{"id": "2507.13880", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13880", "abs": "https://arxiv.org/abs/2507.13880", "authors": ["Marten Kreis", "Benjamin Kiefer"], "title": "Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision", "comment": null, "summary": "This paper presents a novel approach to enhancing marine vision by fusing\nreal-time visual data with chart information. Our system overlays nautical\nchart data onto live video feeds by accurately matching detected navigational\naids, such as buoys, with their corresponding representations in chart data. To\nachieve robust association, we introduce a transformer-based end-to-end neural\nnetwork that predicts bounding boxes and confidence scores for buoy queries,\nenabling the direct matching of image-domain detections with world-space chart\nmarkers. The proposed method is compared against baseline approaches, including\na ray-casting model that estimates buoy positions via camera projection and a\nYOLOv7-based network extended with a distance estimation module. Experimental\nresults on a dataset of real-world maritime scenes demonstrate that our\napproach significantly improves object localization and association accuracy in\ndynamic and challenging environments.", "AI": {"tldr": "本文提出一种通过融合实时视频与海图信息来增强海洋视觉的新方法，利用基于Transformer的神经网络实现浮标检测与海图数据的精确匹配。", "motivation": "研究旨在提升动态和复杂海洋环境中的目标定位与关联准确性，从而增强海洋视觉能力。", "method": "该方法通过精确匹配检测到的航标（如浮标）与海图中的对应表示，将海图数据叠加到实时视频流上。核心是引入一个基于Transformer的端到端神经网络，用于预测浮标的边界框和置信度，从而实现图像域检测与世界空间海图标记的直接匹配。该方法与光线投射模型和扩展YOLOv7的基线方法进行了比较。", "result": "在真实世界海事场景数据集上的实验结果表明，该方法显著提高了在动态和挑战性环境中的目标定位和关联准确性。", "conclusion": "所提出的基于Transformer的实时视觉与海图信息融合方法，能有效提升海洋环境下的目标定位和关联精度，显著增强海洋视觉能力。"}}
{"id": "2507.13607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13607", "abs": "https://arxiv.org/abs/2507.13607", "authors": ["Kento Kawai", "Takeru Oba", "Kyotaro Tokoro", "Kazutoshi Akita", "Norimichi Ukita"], "title": "Efficient Burst Super-Resolution with One-step Diffusion", "comment": "NTIRE2025", "summary": "While burst Low-Resolution (LR) images are useful for improving their Super\nResolution (SR) image compared to a single LR image, prior burst SR methods are\ntrained in a deterministic manner, which produces a blurry SR image. Since such\nblurry images are perceptually degraded, we aim to reconstruct sharp and\nhigh-fidelity SR images by a diffusion model. Our method improves the\nefficiency of the diffusion model with a stochastic sampler with a high-order\nODE as well as one-step diffusion using knowledge distillation. Our\nexperimental results demonstrate that our method can reduce the runtime to 1.6\n% of its baseline while maintaining the SR quality measured based on image\ndistortion and perceptual quality.", "AI": {"tldr": "本文提出一种基于扩散模型的方法，用于从低分辨率图像爆发序列中重建锐利、高保真度的超分辨率图像，并通过高效采样和知识蒸馏显著提升了运行效率。", "motivation": "现有的爆发超分辨率方法是确定性的，导致生成的超分辨率图像模糊且感知质量下降。研究目标是重建锐利、高保真度的超分辨率图像。", "method": "采用扩散模型实现超分辨率。通过使用高阶ODE的随机采样器和利用知识蒸馏实现一步扩散，提高了扩散模型的效率。", "result": "实验结果表明，该方法在保持图像失真和感知质量方面超分辨率性能的同时，将运行时间缩短至基线的1.6%。", "conclusion": "所提出的扩散模型方法能有效从爆发低分辨率图像中生成锐利、高保真度的超分辨率图像，并在保持质量的同时大幅提升了计算效率。"}}
{"id": "2507.13858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13858", "abs": "https://arxiv.org/abs/2507.13858", "authors": ["Nicolò Brunello", "Davide Rigamonti", "Andrea Sassella", "Vincenzo Scotti", "Mark James Carman"], "title": "InTraVisTo: Inside Transformer Visualisation Tool", "comment": "8 pages", "summary": "The reasoning capabilities of Large Language Models (LLMs) have increased\ngreatly over the last few years, as have their size and complexity.\nNonetheless, the use of LLMs in production remains challenging due to their\nunpredictable nature and discrepancies that can exist between their desired\nbehavior and their actual model output. In this paper, we introduce a new tool,\nInTraVisTo (Inside Transformer Visualisation Tool), designed to enable\nresearchers to investigate and trace the computational process that generates\neach token in a Transformer-based LLM. InTraVisTo provides a visualization of\nboth the internal state of the Transformer model (by decoding token embeddings\nat each layer of the model) and the information flow between the various\ncomponents across the different layers of the model (using a Sankey diagram).\nWith InTraVisTo, we aim to help researchers and practitioners better understand\nthe computations being performed within the Transformer model and thus to shed\nsome light on internal patterns and reasoning processes employed by LLMs.", "AI": {"tldr": "本文介绍了一个名为InTraVisTo的新工具，用于可视化大型语言模型（LLMs）内部的计算过程，以帮助研究人员理解其推理机制和信息流。", "motivation": "尽管LLMs的推理能力显著提升，但由于其不可预测性以及期望行为与实际输出之间的差异，LLMs在生产中的应用仍面临挑战。研究动机是需要深入理解Transformer模型内部的计算过程，以揭示其内部模式和推理机制。", "method": "本文提出了InTraVisTo工具，它通过两种方式可视化Transformer模型：1) 解码模型每一层的token嵌入来展示内部状态；2) 使用桑基图（Sankey diagram）可视化模型不同层之间各组件的信息流。", "result": "InTraVisTo旨在帮助研究人员和实践者更好地理解Transformer模型内部执行的计算，从而揭示LLMs采用的内部模式和推理过程。", "conclusion": "InTraVisTo工具能够通过可视化Transformer模型内部状态和信息流，帮助研究人员和实践者深入理解LLMs的计算和推理过程，以应对其不可预测性挑战。"}}
{"id": "2507.13881", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.13881", "abs": "https://arxiv.org/abs/2507.13881", "authors": ["Cole Walsh", "Rodica Ivan", "Muhammad Zafar Iqbal", "Colleen Robb"], "title": "Using LLMs to identify features of personal and professional skills in an open-response situational judgment test", "comment": "10 pages, 2 figures, 4 tables; this work was accepted for\n  presentation at the 2025 Artificial Intelligence in Measurement and Education\n  Conference in Pittsburgh, Pennsylvania, United States", "summary": "Academic programs are increasingly recognizing the importance of personal and\nprofessional skills and their critical role alongside technical expertise in\npreparing students for future success in diverse career paths. With this\ngrowing demand comes the need for scalable systems to measure, evaluate, and\ndevelop these skills. Situational Judgment Tests (SJTs) offer one potential\navenue for measuring these skills in a standardized and reliable way, but\nopen-response SJTs have traditionally relied on trained human raters for\nevaluation, presenting operational challenges to delivering SJTs at scale. Past\nattempts at developing NLP-based scoring systems for SJTs have fallen short due\nto issues with construct validity of these systems. In this article, we explore\na novel approach to extracting construct-relevant features from SJT responses\nusing large language models (LLMs). We use the Casper SJT to demonstrate the\nefficacy of this approach. This study sets the foundation for future\ndevelopments in automated scoring for personal and professional skills.", "AI": {"tldr": "本文提出一种利用大型语言模型（LLMs）从情境判断测试（SJTs）的开放式回答中提取与构念相关的特征的新方法，旨在实现个人和专业技能的自动化、可扩展评估。", "motivation": "学术项目日益重视个人和专业技能，但缺乏可扩展的系统来测量和评估这些技能。传统的开放式SJTs依赖人工评分，难以规模化。过去的NLP评分系统存在构念效度问题。", "method": "研究探索了一种新颖的方法，利用大型语言模型（LLMs）从SJTs回答中提取与构念相关的特征。以Casper SJT为例，验证了该方法的有效性。", "result": "研究展示了利用LLMs从SJTs回答中提取构念相关特征的有效性，为未来个人和专业技能的自动化评分奠定了基础。", "conclusion": "LLMs为解决开放式SJTs的规模化评估挑战提供了新的途径，有望实现个人和专业技能的自动化、可靠测量，并推动该领域未来的发展。"}}
{"id": "2507.13609", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13609", "abs": "https://arxiv.org/abs/2507.13609", "authors": ["Yanan Wang", "Julio Vizcarra", "Zhi Li", "Hao Niu", "Mori Kurokawa"], "title": "CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks", "comment": null, "summary": "Despite recent progress in video large language models (VideoLLMs), a key\nopen challenge remains: how to equip models with chain-of-thought (CoT)\nreasoning abilities grounded in fine-grained object-level video understanding.\nExisting instruction-tuned models, such as the Qwen and LLaVA series, are\ntrained on high-level video-text pairs, often lacking structured annotations\nnecessary for compositional, step-by-step reasoning. We propose CoTasks:\nChain-of-Thought based Video Instruction Tuning Tasks, a new framework that\ndecomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)\ninto four entity-level foundational tasks: frame localization, entity tracking,\nspatial and temporal relation extraction. By embedding these intermediate\nCoT-style reasoning steps into the input, CoTasks enables models to explicitly\nperform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA\nbenchmark show that CoTasks significantly enhance inference performance:\nLLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and\nQwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal\n(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the\neffectiveness of CoTasks as a structured CoT-style supervision framework for\nimproving compositional video reasoning.", "AI": {"tldr": "CoTasks是一个新框架，通过将复杂视频问题分解为实体级别的基础任务，为视频大语言模型（VideoLLMs）提供链式思考（CoT）推理能力，从而显著提升了视频理解性能。", "motivation": "现有视频大语言模型（VideoLLMs）缺乏基于细粒度物体级视频理解的链式思考（CoT）推理能力。这是因为它们主要通过高级别的视频-文本对进行训练，缺少支持组合式、分步推理所需的结构化标注。", "method": "本文提出了CoTasks框架，将现有数据集中（如NeXT-QA, STAR）的复杂视频问题分解为四个实体级别的基础任务：帧定位、实体跟踪、空间关系提取和时间关系提取。通过将这些中间的CoT风格推理步骤嵌入到模型输入中，CoTasks使模型能够明确地执行以物体为中心的时空推理。", "result": "在NeXT-QA基准测试上，CoTasks显著提升了推理性能：LLaVA-video-7B的平均GPT-4评估分数提高了3.3分，Qwen2.5-VL-3B提高了17.4分，在因果（+14.6）、时间（+10.9）和描述性（+48.1）子类别中均有显著提升。", "conclusion": "CoTasks作为一种结构化的CoT风格监督框架，能有效提升模型的组合式视频推理能力，解决了现有VideoLLMs在细粒度物体级CoT推理方面的不足。"}}
{"id": "2507.13870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13870", "abs": "https://arxiv.org/abs/2507.13870", "authors": ["Maciej Jalocha", "Johan Hausted Schmidt", "William Michelseen"], "title": "Label Unification for Cross-Dataset Generalization in Cybersecurity NER", "comment": "5 pages, 5 figures", "summary": "The field of cybersecurity NER lacks standardized labels, making it\nchallenging to combine datasets. We investigate label unification across four\ncybersecurity datasets to increase data resource usability. We perform a\ncoarse-grained label unification and conduct pairwise cross-dataset evaluations\nusing BiLSTM models. Qualitative analysis of predictions reveals errors,\nlimitations, and dataset differences. To address unification limitations, we\npropose alternative architectures including a multihead model and a graph-based\ntransfer model. Results show that models trained on unified datasets generalize\npoorly across datasets. The multihead model with weight sharing provides only\nmarginal improvements over unified training, while our graph-based transfer\nmodel built on BERT-base-NER shows no significant performance gains compared\nBERT-base-NER.", "AI": {"tldr": "网络安全NER领域缺乏标准化标签，导致数据集难以整合。本文研究了四个数据集的标签统一性，发现统一后的模型泛化能力差，提出的多头模型和图基迁移模型改进有限。", "motivation": "网络安全命名实体识别（NER）领域缺乏标准化标签，使得整合现有数据集以提高数据资源可用性面临挑战。", "method": "研究人员进行了粗粒度标签统一，并使用BiLSTM模型进行成对的跨数据集评估。他们还对预测结果进行了定性分析，并提出了包括多头模型和图基迁移模型在内的替代架构，以解决统一的局限性。", "result": "结果显示，在统一数据集上训练的模型在跨数据集泛化能力差。带有权重共享的多头模型仅比统一训练略有改进，而基于BERT-base-NER构建的图基迁移模型与BERT-base-NER相比没有显著的性能提升。", "conclusion": "当前的网络安全NER标签统一方法和提出的多头及图基迁移模型在提高跨数据集泛化能力方面效果不佳，表明该领域在数据整合和模型泛化方面仍存在挑战。"}}
{"id": "2507.13913", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13913", "abs": "https://arxiv.org/abs/2507.13913", "authors": ["Matous Volf", "Jakub Simko"], "title": "Political Leaning and Politicalness Classification of Texts", "comment": null, "summary": "This paper addresses the challenge of automatically classifying text\naccording to political leaning and politicalness using transformer models. We\ncompose a comprehensive overview of existing datasets and models for these\ntasks, finding that current approaches create siloed solutions that perform\npoorly on out-of-distribution texts. To address this limitation, we compile a\ndiverse dataset by combining 12 datasets for political leaning classification\nand creating a new dataset for politicalness by extending 18 existing datasets\nwith the appropriate label. Through extensive benchmarking with leave-one-in\nand leave-one-out methodologies, we evaluate the performance of existing models\nand train new ones with enhanced generalization capabilities.", "AI": {"tldr": "本文通过整合现有数据集并创建新数据集，使用Transformer模型解决了政治文本的政治倾向和政治性自动分类中，现有模型在域外文本上表现不佳的问题，并提升了模型的泛化能力。", "motivation": "现有用于政治倾向和政治性分类的Transformer模型在域外（out-of-distribution）文本上表现不佳，因为它们是孤立的解决方案且缺乏多样化的训练数据。", "method": "作者通过以下方式解决问题：1. 整合12个现有数据集用于政治倾向分类。2. 扩展18个现有数据集并添加政治性标签，创建新的政治性数据集。3. 使用留一法（leave-one-in）和留一出法（leave-one-out）进行广泛基准测试，评估现有模型并训练新的模型。", "result": "通过整合和创建多样化数据集，并进行严格的基准测试，训练出了具有增强泛化能力的新模型。", "conclusion": "通过构建多样化的数据集和采用全面的评估方法，可以有效提升Transformer模型在政治文本分类任务上的泛化能力，克服现有解决方案的局限性。"}}
{"id": "2507.13628", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13628", "abs": "https://arxiv.org/abs/2507.13628", "authors": ["Masahiro Ogawa", "Qi An", "Atsushi Yamashita"], "title": "Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation", "comment": "8 pages, 15 figures, RA-L submission", "summary": "Separating moving and static objects from a moving camera viewpoint is\nessential for 3D reconstruction, autonomous navigation, and scene understanding\nin robotics. Existing approaches often rely primarily on optical flow, which\nstruggles to detect moving objects in complex, structured scenes involving\ncamera motion. To address this limitation, we propose Focus of Expansion\nLikelihood and Segmentation (FoELS), a method based on the core idea of\nintegrating both optical flow and texture information. FoELS computes the focus\nof expansion (FoE) from optical flow and derives an initial motion likelihood\nfrom the outliers of the FoE computation. This likelihood is then fused with a\nsegmentation-based prior to estimate the final moving probability. The method\neffectively handles challenges including complex structured scenes, rotational\ncamera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016\ndataset and real-world traffic videos demonstrate its effectiveness and\nstate-of-the-art performance.", "AI": {"tldr": "该论文提出FoELS方法，通过结合光流和纹理信息，有效解决了移动相机视角下复杂场景中运动物体与静态物体分离的挑战。", "motivation": "现有方法主要依赖光流，在涉及相机运动的复杂结构化场景中难以检测运动物体，这限制了3D重建、自主导航和场景理解的应用。", "method": "FoELS方法整合光流和纹理信息。它首先从光流计算膨胀焦点（FoE），并从FoE计算的异常值中得出初始运动可能性。然后，将该可能性与基于分割的先验融合，以估计最终的运动概率。", "result": "该方法有效处理了复杂结构化场景、旋转相机运动和平行运动等挑战。在DAVIS 2016数据集和真实世界交通视频上的综合评估表明其有效性及最先进的性能。", "conclusion": "FoELS是一种有效的分离移动和静态物体的方法，能够应对复杂场景和多种相机运动，并展现出卓越的性能。"}}
{"id": "2507.13875", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.13875", "abs": "https://arxiv.org/abs/2507.13875", "authors": ["Carlos Mena", "Pol Serra", "Jacobo Romero", "Abir Messaoudi", "Jose Giraldo", "Carme Armentano-Oller", "Rodolfo Zevallos", "Ivan Meza", "Javier Hernando"], "title": "Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies", "comment": "Accepted at Interspeech 2025", "summary": "Code-switching (CS), the alternating use of two or more languages, challenges\nautomatic speech recognition (ASR) due to scarce training data and linguistic\nsimilarities. The lack of dedicated CS datasets limits ASR performance, as most\nmodels rely on monolingual or mixed-language corpora that fail to reflect\nreal-world CS patterns. This issue is critical in multilingual societies where\nCS occurs in informal and formal settings. A key example is Catalan-Spanish CS,\nwidely used in media and parliamentary speeches. In this work, we improve ASR\nfor Catalan-Spanish CS by exploring three strategies: (1) generating synthetic\nCS data, (2) concatenating monolingual audio, and (3) leveraging real CS data\nwith language tokens. We extract CS data from Catalan speech corpora and\nfine-tune OpenAI's Whisper models, making them available on Hugging Face.\nResults show that combining a modest amount of synthetic CS data with the\ndominant language token yields the best transcription performance.", "AI": {"tldr": "该研究旨在改善加泰罗尼亚语-西班牙语语码转换（CS）自动语音识别（ASR）的性能，通过探索合成数据生成、单语音频拼接和利用带有语言标记的真实CS数据三种策略，发现少量合成数据与主导语言标记结合效果最佳。", "motivation": "语码转换（CS）因训练数据稀缺和语言相似性给自动语音识别（ASR）带来挑战。缺乏专门的CS数据集限制了ASR的性能，尤其是在多语言社会中，CS在非正式和正式场合中普遍存在，如加泰罗尼亚语-西班牙语CS。", "method": "本研究探索了三种策略来改进加泰罗尼亚语-西班牙语CS的ASR：1) 生成合成CS数据，2) 拼接单语音频，3) 利用带有语言标记的真实CS数据。研究人员从加泰罗尼亚语语音语料库中提取CS数据，并对OpenAI的Whisper模型进行微调，并将模型发布在Hugging Face上。", "result": "结果显示，结合适量的合成CS数据和主导语言标记能产生最佳的转录性能。", "conclusion": "通过结合少量合成语码转换数据和主导语言标记，可以有效提升加泰罗尼亚语-西班牙语语码转换自动语音识别的性能，为解决CS ASR的数据稀缺问题提供了有效途径。"}}
{"id": "2507.13919", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13919", "abs": "https://arxiv.org/abs/2507.13919", "authors": ["Kobi Hackenburg", "Ben M. Tappin", "Luke Hewitt", "Ed Saunders", "Sid Black", "Hause Lin", "Catherine Fist", "Helen Margetts", "David G. Rand", "Christopher Summerfield"], "title": "The Levers of Political Persuasion with Conversational AI", "comment": "19 pages, 4 figures. Our supplementary materials file can be found at\n  https://github.com/kobihackenburg/scaling-conversational-AI", "summary": "There are widespread fears that conversational AI could soon exert\nunprecedented influence over human beliefs. Here, in three large-scale\nexperiments (N=76,977), we deployed 19 LLMs-including some post-trained\nexplicitly for persuasion-to evaluate their persuasiveness on 707 political\nissues. We then checked the factual accuracy of 466,769 resulting LLM claims.\nContrary to popular concerns, we show that the persuasive power of current and\nnear-future AI is likely to stem more from post-training and prompting\nmethods-which boosted persuasiveness by as much as 51% and 27%\nrespectively-than from personalization or increasing model scale. We further\nshow that these methods increased persuasion by exploiting LLMs' unique ability\nto rapidly access and strategically deploy information and that, strikingly,\nwhere they increased AI persuasiveness they also systematically decreased\nfactual accuracy.", "AI": {"tldr": "研究发现，当前AI的说服力主要来源于后训练和提示工程，而非个性化或模型规模，并且说服力增强往往伴随着事实准确性下降。", "motivation": "由于普遍担忧对话式AI可能对人类信念产生前所未有的影响，本研究旨在评估其说服力及其来源。", "method": "通过三项大规模实验（N=76,977），使用19个大型语言模型（LLMs，包括部分为说服而进行后训练的模型），评估它们在707个政治问题上的说服力，并检查了466,769个LLM声明的事实准确性。", "result": "研究显示，当前及未来AI的说服力主要源于后训练（提升高达51%）和提示方法（提升高达27%），而非个性化或模型规模。这些方法通过利用LLMs快速获取和策略性部署信息的能力来增强说服力。值得注意的是，说服力增强的同时，事实准确性系统性地下降。", "conclusion": "当前及未来AI的说服力主要由后训练和提示工程驱动，这些方法通过信息利用来增强说服力，但代价是事实准确性会系统性降低，这与普遍认为个性化或模型规模是主要影响因素的担忧有所不同。"}}
{"id": "2507.13648", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13648", "abs": "https://arxiv.org/abs/2507.13648", "authors": ["Seungjun Moon", "Sangjoon Yu", "Gyeong-Moon Park"], "title": "EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation", "comment": null, "summary": "The rapid advancement of neural radiance fields (NeRF) has paved the way to\ngenerate animatable human avatars from a monocular video. However, the sole\nusage of NeRF suffers from a lack of details, which results in the emergence of\nhybrid representation that utilizes SMPL-based mesh together with NeRF\nrepresentation. While hybrid-based models show photo-realistic human avatar\ngeneration qualities, they suffer from extremely slow inference due to their\ndeformation scheme: to be aligned with the mesh, hybrid-based models use the\ndeformation based on SMPL skinning weights, which needs high computational\ncosts on each sampled point. We observe that since most of the sampled points\nare located in empty space, they do not affect the generation quality but\nresult in inference latency with deformation. In light of this observation, we\npropose EPSilon, a hybrid-based 3D avatar generation scheme with novel\nefficient point sampling strategies that boost both training and inference. In\nEPSilon, we propose two methods to omit empty points at rendering; empty ray\nomission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that\nprogress through the empty space. Then, EIO narrows down the sampling interval\non the ray, which wipes out the region not occupied by either clothes or mesh.\nThe delicate sampling scheme of EPSilon enables not only great computational\ncost reduction during deformation but also the designation of the important\nregions to be sampled, which enables a single-stage NeRF structure without\nhierarchical sampling. Compared to existing methods, EPSilon maintains the\ngeneration quality while using only 3.9% of sampled points and achieves around\n20 times faster inference, together with 4 times faster training convergence.\nWe provide video results on https://github.com/seungjun-moon/epsilon.", "AI": {"tldr": "EPSilon提出了一种高效的点采样策略，显著加速了基于NeRF的3D人体虚拟形象的训练和推理，同时保持了生成质量。", "motivation": "现有的混合表示（结合SMPL网格和NeRF）虽然能生成逼真的人体虚拟形象，但由于其形变方案（基于SMPL蒙皮权重）对每个采样点都需要高计算成本，导致推理速度极慢。研究观察到大部分采样点位于空旷空间，不影响生成质量但增加了延迟。", "method": "EPSilon提出两种高效点采样策略：1. 空射线剔除（ERO）：剔除穿过空旷空间的射线。2. 空区间剔除（EIO）：缩小射线上的采样区间，剔除衣服或网格未占据的区域。这种精细的采样方案不仅大大减少了形变时的计算成本，还指定了重要的采样区域，从而实现了无需分层采样的单阶段NeRF结构。", "result": "与现有方法相比，EPSilon在保持生成质量的同时，仅使用了3.9%的采样点，实现了约20倍的推理速度提升和4倍的训练收敛速度提升。", "conclusion": "EPSilon通过创新的高效点采样策略，显著解决了混合NeRF模型在人体虚拟形象生成中的推理速度慢的问题，实现了计算成本的大幅降低和训练推理效率的显著提升，同时保持了高质量的生成效果。"}}
{"id": "2507.13937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13937", "abs": "https://arxiv.org/abs/2507.13937", "authors": ["Jan Trienes", "Anastasiia Derzhanskaia", "Roland Schwarzkopf", "Markus Mühling", "Jörg Schlötterer", "Christin Seifert"], "title": "Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support", "comment": null, "summary": "We present Marcel, a lightweight and open-source conversational agent\ndesigned to support prospective students with admission-related inquiries. The\nsystem aims to provide fast and personalized responses, while reducing workload\nof university staff. We employ retrieval-augmented generation to ground answers\nin university resources and to provide users with verifiable, contextually\nrelevant information. To improve retrieval quality, we introduce an FAQ\nretriever that maps user questions to knowledge-base entries, allowing\nadministrators to steer retrieval, and improving over standard dense/hybrid\nretrieval strategies. The system is engineered for easy deployment in\nresource-constrained academic settings. We detail the system architecture,\nprovide a technical evaluation of its components, and report insights from a\nreal-world deployment.", "AI": {"tldr": "Marcel是一个轻量级、开源的对话代理，旨在帮助潜在学生解答招生相关问题，同时减轻大学工作人员的负担。", "motivation": "现有系统无法提供快速、个性化的回复，且大学工作人员处理招生咨询的工作量巨大。研究旨在开发一个能够提供快速、个性化回复并减轻工作人员负担的系统。", "method": "采用检索增强生成（RAG）技术，将答案基于大学资源进行生成，并提供可验证、上下文相关的信。引入FAQ检索器将用户问题映射到知识库条目，以提高检索质量，优于标准密集/混合检索策略。系统设计易于在资源受限的学术环境中部署。", "result": "论文详细介绍了系统架构，提供了对其组件的技术评估，并报告了真实世界部署的见解。", "conclusion": "Marcel能够为潜在学生提供快速、个性化且可验证的招生相关信息，通过独特的FAQ检索器提高了检索质量，并且易于在资源有限的学术环境中部署，有效减轻了大学工作人员的负担。"}}
{"id": "2507.13942", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13942", "abs": "https://arxiv.org/abs/2507.13942", "authors": ["Jacob C Walker", "Pedro Vélez", "Luisa Polania Cabrera", "Guangyao Zhou", "Rishabh Kabra", "Carl Doersch", "Maks Ovsjanikov", "João Carreira", "Shiry Ginosar"], "title": "Generalist Forecasting with Frozen Video Models via Latent Diffusion", "comment": null, "summary": "Forecasting what will happen next is a critical skill for general-purpose\nsystems that plan or act in the world at different levels of abstraction. In\nthis paper, we identify a strong correlation between a vision model's\nperceptual ability and its generalist forecasting performance over short time\nhorizons. This trend holds across a diverse set of pretrained models-including\nthose trained generatively-and across multiple levels of abstraction, from raw\npixels to depth, point tracks, and object motion. The result is made possible\nby a novel generalist forecasting framework that operates on any frozen vision\nbackbone: we train latent diffusion models to forecast future features in the\nfrozen representation space, which are then decoded via lightweight,\ntask-specific readouts. To enable consistent evaluation across tasks, we\nintroduce distributional metrics that compare distributional properties\ndirectly in the space of downstream tasks and apply this framework to nine\nmodels and four tasks. Our results highlight the value of bridging\nrepresentation learning and generative modeling for temporally grounded video\nunderstanding.", "AI": {"tldr": "研究发现视觉模型的感知能力与其短期预测性能强相关，并提出了一个通用的预测框架，该框架通过在冻结视觉骨干的表示空间中预测未来特征来实现，突出了表示学习与生成模型结合的价值。", "motivation": "预测未来是通用系统在不同抽象级别上规划或行动的关键技能。", "method": "提出了一种新颖的通用预测框架，该框架可在任何冻结的视觉骨干网络上运行。具体方法是训练潜在扩散模型来预测冻结表示空间中的未来特征，然后通过轻量级的、特定任务的读出器进行解码。为实现跨任务的一致评估，引入了直接在下游任务空间中比较分布属性的分布度量。", "result": "研究发现视觉模型的感知能力与其短期预测性能之间存在强相关性。这种趋势适用于各种预训练模型（包括生成式模型）以及多个抽象级别（从原始像素到深度、点轨迹和物体运动）。该框架已应用于九个模型和四个任务。", "conclusion": "研究结果突出了将表示学习与生成建模相结合对于时间感知的视频理解的价值。"}}
{"id": "2507.13663", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13663", "abs": "https://arxiv.org/abs/2507.13663", "authors": ["Xingyu Jiang", "Ning Gao", "Hongkun Dou", "Xiuhui Zhang", "Xiaoqing Zhong", "Yue Deng", "Hongjue Li"], "title": "Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration", "comment": null, "summary": "Natural image quality is often degraded by adverse weather conditions,\nsignificantly impairing the performance of downstream tasks. Image restoration\nhas emerged as a core solution to this challenge and has been widely discussed\nin the literature. Although recent transformer-based approaches have made\nremarkable progress in image restoration, their increasing system complexity\nposes significant challenges for real-time processing, particularly in\nreal-world deployment scenarios. To this end, most existing methods attempt to\nsimplify the self-attention mechanism, such as by channel self-attention or\nstate space model. However, these methods primarily focus on network\narchitecture while neglecting the inherent characteristics of image restoration\nitself. In this context, we explore a pyramid Wavelet-Fourier iterative\npipeline to demonstrate the potential of Wavelet-Fourier processing for image\nrestoration. Inspired by the above findings, we propose a novel and efficient\nrestoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).\nSpecifically, PW-FNet features two key design principles: 1) at the inter-block\nlevel, integrates a pyramid wavelet-based multi-input multi-output structure to\nachieve multi-scale and multi-frequency bands decomposition; and 2) at the\nintra-block level, incorporates Fourier transforms as an efficient alternative\nto self-attention mechanisms, effectively reducing computational complexity\nwhile preserving global modeling capability. Extensive experiments on tasks\nsuch as image deraining, raindrop removal, image super-resolution, motion\ndeblurring, image dehazing, image desnowing and underwater/low-light\nenhancement demonstrate that PW-FNet not only surpasses state-of-the-art\nmethods in restoration quality but also achieves superior efficiency, with\nsignificantly reduced parameter size, computational cost and inference time.", "AI": {"tldr": "本文提出了一种名为PW-FNet的新型高效图像复原网络，通过结合金字塔小波分解和傅里叶变换替代自注意力机制，在多种图像复原任务中实现了卓越的性能和效率。", "motivation": "现有基于Transformer的图像复原方法系统复杂性高，难以实时处理和实际部署。尽管有方法尝试简化自注意力，但它们多关注网络架构，忽视了图像复原本身的内在特性。", "method": "本文探索了一种金字塔小波-傅里叶迭代管道，并提出PW-FNet。其核心设计包括：1) 在块间层面，采用基于金字塔小波的多输入多输出结构，实现多尺度和多频带分解；2) 在块内层面，引入傅里叶变换作为自注意力机制的有效替代，以降低计算复杂度并保持全局建模能力。", "result": "在图像去雨、雨滴移除、图像超分辨率、运动去模糊、图像去雾、图像去雪以及水下/低光增强等任务上，PW-FNet不仅超越了现有最先进方法的复原质量，还在参数量、计算成本和推理时间方面显著降低，展现出卓越的效率。", "conclusion": "小波-傅里叶处理在图像复原方面具有巨大潜力。PW-FNet作为一种高效且有效的复原基线，证明了其在多种图像复原任务中的优越性能和效率，为未来研究提供了新方向。"}}
{"id": "2507.13949", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13949", "abs": "https://arxiv.org/abs/2507.13949", "authors": ["Bianca Raimondi", "Maurizio Gabbrielli"], "title": "Exploiting Primacy Effect To Improve Large Language Models", "comment": "Accepted by RANLP 2025", "summary": "Large Language Models (LLMs) have become essential in many Natural Language\nProcessing (NLP) tasks, leveraging extensive pre-training and fine-tuning to\nachieve high accuracy. However, like humans, LLMs exhibit biases, particularly\npositional biases such as primacy and recency effects, which can influence the\naccuracy of the answers. The primacy effect-where items presented first are\nmore likely to be remembered or selected-plays a key role in Multiple Choice\nQuestion Answering (MCQA), where the order of answer options can affect\nprediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We\nfirst show that fine-tuning amplifies this bias, probably due to exposure to\nhuman-like patterns. Hence, we strategically leverage this effect by reordering\nresponse options based on semantic similarity to the query, without requiring\nknowledge of the correct answer. Our experimental results show that this\napproach significantly improves performance in MCQA. More generally, our\nfindings underscore the dual nature of biases as both challenges and\nopportunities, offering insights for bias-aware model design and NLP\napplications.", "AI": {"tldr": "研究发现大型语言模型（LLMs）的微调会放大其首位偏见，该研究通过基于语义相似性重新排序多项选择题（MCQA）选项，成功利用此偏见，显著提高了MCQA性能。", "motivation": "LLMs在许多NLP任务中表现出色，但它们存在偏见，特别是位置偏见（如首位效应和近因效应），这会影响答案的准确性。在多项选择题问答（MCQA）中，首位效应尤为关键，选项顺序会影响预测结果。本研究旨在探讨微调LLMs中的首位偏见及其影响。", "method": "1. 证明微调会放大LLMs的首位偏见，这可能源于接触到类人模式。2. 策略性地利用此效应，通过基于语义相似性对查询的响应选项进行重新排序，且无需预知正确答案。3. 在MCQA任务中进行实验验证。", "result": "实验结果表明，该方法显著提高了MCQA的性能。研究结果更普遍地强调了偏见作为挑战和机遇的双重性质。", "conclusion": "LLMs中的偏见既是挑战也是机遇，本研究为偏见感知模型设计和NLP应用提供了见解。通过策略性地利用偏见，可以提升模型性能。"}}
{"id": "2507.13966", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13966", "abs": "https://arxiv.org/abs/2507.13966", "authors": ["Bhishma Dedhia", "Yuval Kansal", "Niraj K. Jha"], "title": "Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need", "comment": null, "summary": "Language models traditionally used for cross-domain generalization have\nrecently demonstrated task-specific reasoning. However, their top-down training\napproach on general corpora is insufficient for acquiring abstractions needed\nfor deep domain expertise. This may require a bottom-up approach that acquires\nexpertise by learning to compose simple domain concepts into more complex ones.\nA knowledge graph (KG) provides this compositional structure, where domain\nprimitives are represented as head-relation-tail edges and their paths encode\nhigher-level concepts. We present a task generation pipeline that synthesizes\ntasks directly from KG primitives, enabling models to acquire and compose them\nfor reasoning. We fine-tune language models on the resultant KG-grounded\ncurriculum to demonstrate domain-specific superintelligence. While broadly\napplicable, we validate our approach in medicine, where reliable KGs exist.\nUsing a medical KG, we curate 24,000 reasoning tasks paired with thinking\ntraces derived from diverse medical primitives. We fine-tune the QwQ-32B model\non this curriculum to obtain QwQ-Med-3 that takes a step towards medical\nsuperintelligence. We also introduce ICD-Bench, an evaluation suite to quantify\nreasoning abilities across 15 medical domains. Our experiments demonstrate that\nQwQ-Med-3 significantly outperforms state-of-the-art reasoning models on\nICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired\nprimitives to widen the performance gap on the hardest tasks of ICD-Bench.\nFinally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3\ntransfers acquired expertise to enhance the base model's performance. While the\nindustry's approach to artificial general intelligence (AGI) emphasizes broad\nexpertise, we envision a future in which AGI emerges from the composable\ninteraction of efficient domain-specific superintelligent agents.", "AI": {"tldr": "该研究提出一种基于知识图谱（KG）的自下而上训练方法，通过合成任务和精调语言模型，使其获得深层领域专业知识，并在医学领域实现了“领域超级智能”。", "motivation": "传统的语言模型虽然能进行跨领域泛化和任务特定推理，但其自上而下的训练方式不足以获得深层领域专业知识所需的抽象能力。这需要一种自下而上的方法，通过学习组合简单的领域概念来获得专业知识。", "method": "该研究利用知识图谱的组合结构（头-关系-尾边代表领域原语，路径编码高级概念），提出一个任务生成流程，直接从KG原语合成任务。他们在一个医学KG上策划了24,000个推理任务，并包含思维痕迹。随后，他们使用这些KG-grounded课程精调了QwQ-32B模型，得到了QwQ-Med-3。此外，他们还引入了ICD-Bench评估套件来量化15个医学领域的推理能力。", "result": "实验表明，QwQ-Med-3在ICD-Bench类别上显著优于最先进的推理模型。进一步分析显示，QwQ-Med-3利用习得的原语扩大了在ICD-Bench最困难任务上的性能差距。最后，在医学问答基准上的评估表明，QwQ-Med-3将获得的专业知识转移并增强了基础模型的性能。", "conclusion": "通过基于知识图谱的课程，语言模型可以获得深层领域专业知识，实现领域特定超级智能。该研究展望了通用人工智能（AGI）的未来，认为AGI可能源于高效领域特定超级智能代理的可组合交互，而非仅仅是广泛的专业知识。"}}
{"id": "2507.13673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13673", "abs": "https://arxiv.org/abs/2507.13673", "authors": ["Yuechen Xie", "Haobo Jiang", "Jian Yang", "Yigong Zhang", "Jin Xie"], "title": "MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training", "comment": "10 pages, 8 figures, 6 tables", "summary": "In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of\nhands and objects from monocular RGB input remains highly challenging due to\nthe inherent geometric ambiguity of RGB images and the severe mutual occlusions\nthat occur during interaction.To address these challenges, we propose MaskHOI,\na novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI\npose estimation. Our core idea is to leverage the masking-then-reconstruction\nstrategy of MAE to encourage the feature encoder to infer missing spatial and\nstructural information, thereby facilitating geometric-aware and\nocclusion-robust representation learning. Specifically, based on our\nobservation that human hands exhibit far greater geometric complexity than\nrigid objects, conventional uniform masking fails to effectively guide the\nreconstruction of fine-grained hand structures. To overcome this limitation, we\nintroduce a Region-specific Mask Ratio Allocation, primarily comprising the\nregion-specific masking assignment and the skeleton-driven hand masking\nguidance. The former adaptively assigns lower masking ratios to hand regions\nthan to rigid objects, balancing their feature learning difficulty, while the\nlatter prioritizes masking critical hand parts (e.g., fingertips or entire\nfingers) to realistically simulate occlusion patterns in real-world\ninteractions. Furthermore, to enhance the geometric awareness of the pretrained\nencoder, we introduce a novel Masked Signed Distance Field (SDF)-driven\nmultimodal learning mechanism. Through the self-masking 3D SDF prediction, the\nlearned encoder is able to perceive the global geometric structure of hands and\nobjects beyond the 2D image plane, overcoming the inherent limitations of\nmonocular input and alleviating self-occlusion issues. Extensive experiments\ndemonstrate that our method significantly outperforms existing state-of-the-art\napproaches.", "AI": {"tldr": "MaskHOI是一种基于掩码自编码器（MAE）的预训练框架，通过区域特定掩码分配和骨架驱动的手部掩码指导，并结合掩码符号距离场（SDF）驱动的多模态学习，显著提升了从单目RGB图像中估计三维手物交互（HOI）姿态的准确性和鲁棒性。", "motivation": "从单目RGB图像中精确估计三维手物交互姿态极具挑战性，主要原因在于RGB图像固有的几何模糊性以及交互过程中严重的手物相互遮挡。", "method": "本文提出了MaskHOI框架，其核心思想是利用MAE的掩码-重建策略来推断缺失的空间和结构信息，从而促进几何感知和抗遮挡的表示学习。具体方法包括：1) 区域特定掩码比率分配：根据手部比刚性物体几何复杂性更高的特点，对手部区域分配较低的掩码比率，并引入骨架驱动的手部掩码指导，优先遮蔽关键手部区域（如指尖或整个手指）以模拟真实遮挡模式。2) 掩码符号距离场（SDF）驱动的多模态学习机制：通过自掩码三维SDF预测，使预训练编码器能够感知手和物体在二维图像平面之外的全局几何结构，克服单目输入的局限性并缓解自遮挡问题。", "result": "广泛的实验表明，MaskHOI方法显著优于现有最先进的方法。", "conclusion": "MaskHOI通过其新颖的区域特定掩码策略和掩码SDF驱动的多模态学习机制，有效地解决了单目RGB图像中三维手物交互姿态估计的几何模糊性和严重遮挡问题，实现了卓越的性能。"}}
{"id": "2507.13977", "categories": ["cs.CL", "eess.AS", "I.5.1"], "pdf": "https://arxiv.org/pdf/2507.13977", "abs": "https://arxiv.org/abs/2507.13977", "authors": ["Lilit Grigoryan", "Nikolay Karpov", "Enas Albasiri", "Vitaly Lavrukhin", "Boris Ginsburg"], "title": "Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic", "comment": "Accepted to ICASSP 2025", "summary": "Despite Arabic being one of the most widely spoken languages, the development\nof Arabic Automatic Speech Recognition (ASR) systems faces significant\nchallenges due to the language's complexity, and only a limited number of\npublic Arabic ASR models exist. While much of the focus has been on Modern\nStandard Arabic (MSA), there is considerably less attention given to the\nvariations within the language. This paper introduces a universal methodology\nfor Arabic speech and text processing designed to address unique challenges of\nthe language. Using this methodology, we train two novel models based on the\nFastConformer architecture: one designed specifically for MSA and the other,\nthe first unified public model for both MSA and Classical Arabic (CA). The MSA\nmodel sets a new benchmark with state-of-the-art (SOTA) performance on related\ndatasets, while the unified model achieves SOTA accuracy with diacritics for CA\nwhile maintaining strong performance for MSA. To promote reproducibility, we\nopen-source the models and their training recipes.", "AI": {"tldr": "本文提出了一种通用的阿拉伯语语音和文本处理方法，并基于FastConformer架构训练了两个新型阿拉伯语自动语音识别（ASR）模型：一个专用于现代标准阿拉伯语（MSA），另一个是首个统一支持MSA和古典阿拉伯语（CA）的公开模型。这些模型在相关数据集上均达到了最先进（SOTA）的性能，并已开源以促进复现。", "motivation": "尽管阿拉伯语是使用最广泛的语言之一，但由于其复杂性，阿拉伯语ASR系统开发面临巨大挑战，且公开可用的模型数量有限。现有研究主要关注MSA，对语言内部变体的关注较少。", "method": "本文提出了一种通用的阿拉伯语语音和文本处理方法，旨在解决该语言的独特挑战。利用此方法，训练了两个基于FastConformer架构的新模型：一个专为MSA设计，另一个是首个统一支持MSA和CA的公开模型。", "result": "MSA模型在相关数据集上设定了新的基准，达到了最先进（SOTA）的性能。统一模型在CA上实现了带音符的最先进准确率，同时保持了MSA的强大性能。", "conclusion": "本文提出的通用方法和基于FastConformer的新模型显著提升了阿拉伯语ASR的性能，特别是对MSA和CA。模型的开源发布有助于促进研究的复现和进一步发展。"}}
{"id": "2507.13984", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13984", "abs": "https://arxiv.org/abs/2507.13984", "authors": ["Quang-Binh Nguyen", "Minh Luu", "Quang Nguyen", "Anh Tran", "Khoi Nguyen"], "title": "CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models", "comment": "Accepted to ICCV 2025", "summary": "Disentangling content and style from a single image, known as content-style\ndecomposition (CSD), enables recontextualization of extracted content and\nstylization of extracted styles, offering greater creative flexibility in\nvisual synthesis. While recent personalization methods have explored the\ndecomposition of explicit content style, they remain tailored for diffusion\nmodels. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a\npromising alternative with a next-scale prediction paradigm, achieving\nperformance comparable to that of diffusion models. In this paper, we explore\nVAR as a generative framework for CSD, leveraging its scale-wise generation\nprocess for improved disentanglement. To this end, we propose CSD-VAR, a novel\nmethod that introduces three key innovations: (1) a scale-aware alternating\noptimization strategy that aligns content and style representation with their\nrespective scales to enhance separation, (2) an SVD-based rectification method\nto mitigate content leakage into style representations, and (3) an Augmented\nKey-Value (K-V) memory enhancing content identity preservation. To benchmark\nthis task, we introduce CSD-100, a dataset specifically designed for\ncontent-style decomposition, featuring diverse subjects rendered in various\nartistic styles. Experiments demonstrate that CSD-VAR outperforms prior\napproaches, achieving superior content preservation and stylization fidelity.", "AI": {"tldr": "本文提出CSD-VAR，一种基于视觉自回归模型（VAR）的内容-风格分解（CSD）新方法，通过尺度感知优化、SVD校正和增强的K-V记忆，实现了更好的内容保留和风格化，并引入了CSD-100数据集。", "motivation": "内容-风格分解（CSD）能提供更大的视觉合成灵活性。现有分解方法主要针对扩散模型，而视觉自回归模型（VAR）作为一种有前景的替代方案，其在CSD中的应用尚未被探索，且其逐尺度生成过程可能有利于解耦。", "method": "本文提出CSD-VAR方法，包含三项关键创新：1) 尺度感知交替优化策略，用于对齐内容和风格表示以增强分离；2) 基于SVD的校正方法，以减少内容泄露到风格表示中；3) 增强的键值（K-V）记忆，以提高内容身份保留。此外，为评估该任务，论文还引入了CSD-100数据集。", "result": "实验结果表明，CSD-VAR在内容保留和风格化保真度方面均优于现有方法。", "conclusion": "CSD-VAR成功地将视觉自回归模型应用于内容-风格分解任务，通过其创新的分解策略和记忆机制，实现了卓越的性能，并为该领域提供了专门的数据集。"}}
{"id": "2507.13693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13693", "abs": "https://arxiv.org/abs/2507.13693", "authors": ["Hongyi Liu", "Haifeng Wang"], "title": "Gaussian kernel-based motion measurement", "comment": null, "summary": "The growing demand for structural health monitoring has driven increasing\ninterest in high-precision motion measurement, as structural information\nderived from extracted motions can effectively reflect the current condition of\nthe structure. Among various motion measurement techniques, vision-based\nmethods stand out due to their low cost, easy installation, and large-scale\nmeasurement. However, when it comes to sub-pixel-level motion measurement,\ncurrent vision-based methods either lack sufficient accuracy or require\nextensive manual parameter tuning (e.g., pyramid layers, target pixels, and\nfilter parameters) to reach good precision. To address this issue, we developed\na novel Gaussian kernel-based motion measurement method, which can extract the\nmotion between different frames via tracking the location of Gaussian kernels.\nThe motion consistency, which fits practical structural conditions, and a\nsuper-resolution constraint, are introduced to increase accuracy and robustness\nof our method. Numerical and experimental validations show that it can\nconsistently reach high accuracy without customized parameter setup for\ndifferent test samples.", "AI": {"tldr": "本文提出一种基于高斯核的视觉运动测量方法，解决了现有方法在亚像素级精度不足或需大量手动调参的问题，实现了高精度和鲁棒性，无需定制参数。", "motivation": "结构健康监测对高精度运动测量的需求日益增长，但现有视觉测量方法在亚像素级精度方面不足，或需要大量手动参数调整才能达到良好精度。", "method": "开发了一种新颖的基于高斯核的运动测量方法，通过跟踪高斯核的位置来提取帧间运动。引入了运动一致性（符合实际结构条件）和超分辨率约束来提高精度和鲁棒性。", "result": "数值和实验验证表明，该方法能够持续达到高精度，且无需针对不同测试样本进行定制的参数设置。", "conclusion": "所提出的高斯核运动测量方法在无需定制参数设置的情况下，能够为结构健康监测提供高精度、鲁棒的运动信息，有效解决了现有视觉方法的局限性。"}}
{"id": "2507.14017", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14017", "abs": "https://arxiv.org/abs/2507.14017", "authors": ["Haoyu He", "Haozheng Luo", "Yan Chen", "Qi R. Wang"], "title": "Efficient Temporal Tokenization for Mobility Prediction with Large Language Models", "comment": null, "summary": "We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for\nHuman Mobility), a framework that leverages large language models (LLMs) as\nspatio-temporal predictors and trajectory reasoners. RHYTHM partitions\ntrajectories into daily segments encoded as discrete tokens with hierarchical\nattention, capturing both daily and weekly dependencies while substantially\nreducing the sequence length. Token representations are enriched with\npre-computed prompt embeddings via a frozen LLM, enhancing the model's ability\nto capture interdependencies without extensive computational overhead. By\nfreezing the LLM backbone, RHYTHM achieves significant computational\nefficiency. Evaluation on three real-world datasets demonstrates a 2.4%\nimprovement in accuracy, 5.0% increase on weekends, and 24.6% reduction in\ntraining time compared to state-of-the-art methods.", "AI": {"tldr": "RHYTHM是一个利用大型语言模型（LLMs）进行人类移动预测和轨迹推理的框架，通过分层时间标记化和冻结LLM骨干，显著提高了预测准确性和计算效率。", "motivation": "利用LLM进行时空预测和轨迹推理，同时解决传统方法中序列长度过长和计算开销大的问题。", "method": "RHYTHM将轨迹划分为日常片段，并将其编码为具有分层注意力的离散标记，捕获每日和每周依赖性，同时大幅缩短序列长度。通过一个冻结的LLM，用预计算的提示嵌入来丰富标记表示，从而在不增加大量计算开销的情况下增强模型捕获相互依赖性的能力。", "result": "在三个真实世界数据集上的评估显示，RHYTHM在准确性方面提高了2.4%，周末提高了5.0%，训练时间比现有最先进方法减少了24.6%。", "conclusion": "RHYTHM通过创新的分层时间标记化和冻结LLM方法，成功地将LLM应用于人类移动预测，在准确性和计算效率上均超越了现有技术。"}}
{"id": "2507.14067", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14067", "abs": "https://arxiv.org/abs/2507.14067", "authors": ["Shuliang Liu", "Qi Zheng", "Jesse Jiaxi Xu", "Yibo Yan", "He Geng", "Aiwei Liu", "Peijie Jiang", "Jia Liu", "Yik-Cheung Tam", "Xuming Hu"], "title": "VLA-Mark: A cross modal watermark for large vision-language alignment model", "comment": null, "summary": "Vision-language models demand watermarking solutions that protect\nintellectual property without compromising multimodal coherence. Existing text\nwatermarking methods disrupt visual-textual alignment through biased token\nselection and static strategies, leaving semantic-critical concepts vulnerable.\nWe propose VLA-Mark, a vision-aligned framework that embeds detectable\nwatermarks while preserving semantic fidelity through cross-modal coordination.\nOur approach integrates multiscale visual-textual alignment metrics, combining\nlocalized patch affinity, global semantic coherence, and contextual attention\npatterns, to guide watermark injection without model retraining. An\nentropy-sensitive mechanism dynamically balances watermark strength and\nsemantic preservation, prioritizing visual grounding during low-uncertainty\ngeneration phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than\nconventional methods, with near-perfect detection (98.8% AUC). The framework\ndemonstrates 96.1\\% attack resilience against attacks such as paraphrasing and\nsynonym substitution, while maintaining text-visual consistency, establishing\nnew standards for quality-preserving multimodal watermarking", "AI": {"tldr": "VLA-Mark是一种用于视觉-语言模型的数字水印框架，它通过跨模态协调在嵌入可检测水印的同时，保持语义保真度和视觉-文本一致性。", "motivation": "现有的文本水印方法通过有偏的token选择和静态策略破坏了视觉-文本对齐，使得语义关键概念易受攻击，因此视觉-语言模型需要一种既能保护知识产权又不损害多模态一致性的水印解决方案。", "method": "VLA-Mark通过跨模态协调实现语义保真。它整合了多尺度视觉-文本对齐度量，包括局部块亲和度、全局语义一致性和上下文注意力模式，以指导水印注入，且无需模型再训练。此外，一个熵敏感机制动态平衡水印强度和语义保留，在低不确定性生成阶段优先考虑视觉基础。", "result": "实验结果显示，VLA-Mark比传统方法PPL降低7.4%，BLEU提高26.6%，检测AUC接近完美（98.8%）。该框架对复述和同义词替换等攻击表现出96.1%的抗攻击性，同时保持了文本-视觉一致性。", "conclusion": "VLA-Mark为质量保持的多模态水印设定了新标准，在保护知识产权的同时，有效维护了视觉-语言模型的语义完整性和跨模态一致性。"}}
{"id": "2507.13706", "categories": ["cs.CV", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.13706", "abs": "https://arxiv.org/abs/2507.13706", "authors": ["Ángel F. García-Fernández", "Jinhao Gu", "Lennart Svensson", "Yuxuan Xia", "Jan Krejčí", "Oliver Kost", "Ondřej Straka"], "title": "GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms", "comment": null, "summary": "This paper introduces two quasi-metrics for performance assessment of\nmulti-object tracking (MOT) algorithms. In particular, one quasi-metric is an\nextension of the generalised optimal subpattern assignment (GOSPA) metric and\nmeasures the discrepancy between sets of objects. The other quasi-metric is an\nextension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy\nbetween sets of trajectories. Similar to the GOSPA-based metrics, these\nquasi-metrics include costs for localisation error for properly detected\nobjects, the number of false objects and the number of missed objects. The\nT-GOSPA quasi-metric also includes a track switching cost. Differently from the\nGOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of\npenalising missed and false objects with different costs, and the localisation\ncosts are not required to be symmetric. These properties can be useful in MOT\nevaluation in certain applications. The performance of several Bayesian MOT\nalgorithms is assessed with the T-GOSPA quasi-metric via simulations.", "AI": {"tldr": "本文引入了两种新的准度量（GOSPA和T-GOSPA的扩展）用于多目标跟踪（MOT）算法的性能评估，它们提供了更大的灵活性。", "motivation": "现有的GOSPA和T-GOSPA度量在某些MOT应用中缺乏灵活性，例如无法对漏检和虚警目标施加不同的惩罚成本，且要求定位成本对称。", "method": "提出了两种准度量：一种是广义最优子模式分配（GOSPA）度量的扩展，用于衡量目标集差异；另一种是轨迹GOSPA（T-GOSPA）度量的扩展，用于衡量轨迹集差异。这些准度量允许对漏检和虚警目标施加不同的成本，并且定位成本无需对称。T-GOSPA准度量还包含轨迹切换成本。", "result": "通过仿真，使用T-GOSPA准度量评估了几种贝叶斯MOT算法的性能。", "conclusion": "所提出的准度量为MOT评估提供了更大的灵活性，这在特定应用中可能非常有用。"}}
{"id": "2507.14022", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14022", "abs": "https://arxiv.org/abs/2507.14022", "authors": ["Jianfei Li", "Kevin Kam Fung Yuen"], "title": "CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis", "comment": "35 pages, 33 tables, 6 Figures", "summary": "This study proposes the Cognitive Pairwise Comparison Classification Model\nSelection (CPC-CMS) framework for document-level sentiment analysis. The CPC,\nbased on expert knowledge judgment, is used to calculate the weights of\nevaluation criteria, including accuracy, precision, recall, F1-score,\nspecificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and\nefficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random\nForest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long\nShort-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from\nTransformers (ALBERT) are chosen as classification baseline models. A weighted\ndecision matrix consisting of classification evaluation scores with respect to\ncriteria weights, is formed to select the best classification model for a\nclassification problem. Three open datasets of social media are used to\ndemonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,\nfor evaluation results excluding the time factor, ALBERT is the best for the\nthree datasets; if time consumption is included, no single model always\nperforms better than the other models. The CPC-CMS can be applied to the other\nclassification applications in different areas.", "AI": {"tldr": "本研究提出了CPC-CMS框架，通过专家判断加权的评估标准（包括效率）来选择文档级情感分析的最佳分类模型，并在社交媒体数据集上进行了验证。", "motivation": "为文档级情感分析选择最佳分类模型时，需要系统地考虑多个评估标准，并纳入专家知识判断，以克服单一指标或非结构化选择的局限性。", "method": "提出认知成对比较分类模型选择（CPC-CMS）框架。利用基于专家判断的认知成对比较（CPC）方法计算准确率、精确率、召回率、F1分数、特异性、MCC、Kappa和效率等评估标准的权重。选择朴素贝叶斯、LSVC、随机森林、逻辑回归、XGBoost、LSTM和ALBERT作为基线模型。构建一个由分类评估分数和标准权重组成的加权决策矩阵，以选择最佳模型。使用三个社交媒体开放数据集进行可行性验证。", "result": "仿真结果显示，在不考虑时间因素的情况下，ALBERT在三个数据集中表现最佳；如果包含时间消耗，则没有一个单一模型始终优于其他模型。", "conclusion": "CPC-CMS框架在文档级情感分析模型选择中是可行的，并且可以应用于其他领域的分类问题，提供了一种基于加权标准的结构化模型选择方法。"}}
{"id": "2507.14079", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14079", "abs": "https://arxiv.org/abs/2507.14079", "authors": ["Garapati Keerthana", "Manik Gupta"], "title": "DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits", "comment": null, "summary": "Progress notes are among the most clinically meaningful artifacts in an\nElectronic Health Record (EHR), offering temporally grounded insights into a\npatient's evolving condition, treatments, and care decisions. Despite their\nimportance, they are severely underrepresented in large-scale EHR datasets. For\ninstance, in the widely used Medical Information Mart for Intensive Care III\n(MIMIC-III) dataset, only about $8.56\\%$ of hospital visits include progress\nnotes, leaving gaps in longitudinal patient narratives. In contrast, the\ndataset contains a diverse array of other note types, each capturing different\naspects of care.\n  We present DENSE (Documenting Evolving Progress Notes from Scattered\nEvidence), a system designed to align with clinical documentation workflows by\nsimulating how physicians reference past encounters while drafting progress\nnotes. The system introduces a fine-grained note categorization and a temporal\nalignment mechanism that organizes heterogeneous notes across visits into\nstructured, chronological inputs. At its core, DENSE leverages a clinically\ninformed retrieval strategy to identify temporally and semantically relevant\ncontent from both current and prior visits. This retrieved evidence is used to\nprompt a large language model (LLM) to generate clinically coherent and\ntemporally aware progress notes.\n  We evaluate DENSE on a curated cohort of patients with multiple visits and\ncomplete progress note documentation. The generated notes demonstrate strong\nlongitudinal fidelity, achieving a temporal alignment ratio of $1.089$,\nsurpassing the continuity observed in original notes. By restoring narrative\ncoherence across fragmented documentation, our system supports improved\ndownstream tasks such as summarization, predictive modeling, and clinical\ndecision support, offering a scalable solution for LLM-driven note synthesis in\nreal-world healthcare settings.", "AI": {"tldr": "该研究提出了DENSE系统，通过模拟医生工作流，利用大语言模型（LLM）从分散的电子健康记录中生成临床连贯且时间感知的病程记录，以弥补现有数据集中病程记录的缺失。", "motivation": "病程记录在电子健康记录中具有重要的临床意义，但它们在大型数据集中严重不足（例如MIMIC-III中仅有8.56%的住院记录包含病程记录），导致患者纵向叙述存在空白。", "method": "DENSE系统模拟医生起草病程记录时参考过往就诊的流程。它引入了细粒度笔记分类和时间对齐机制，将异构笔记组织成结构化、按时间顺序排列的输入。核心是利用临床知情的检索策略，从当前和之前就诊中识别出时间上和语义上相关的证据，然后用这些证据提示LLM生成病程记录。", "result": "在多重就诊且病程记录完整的患者队列上评估，DENSE生成的笔记表现出强大的纵向忠实度，时间对齐比达到1.089，超过了原始笔记的连续性。", "conclusion": "DENSE系统通过恢复碎片化文档的叙述连贯性，支持了下游任务（如总结、预测建模和临床决策支持），为真实医疗环境中LLM驱动的笔记合成提供了一个可扩展的解决方案。"}}
{"id": "2507.13708", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13708", "abs": "https://arxiv.org/abs/2507.13708", "authors": ["Sofia Jamil", "Bollampalli Areen Reddy", "Raghvendra Kumar", "Sriparna Saha", "Koustava Goswami", "K. J. Joseph"], "title": "PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement", "comment": "ECAI 2025", "summary": "Recent advancements in text-to-image diffusion models have achieved\nremarkable success in generating realistic and diverse visual content. A\ncritical factor in this process is the model's ability to accurately interpret\ntextual prompts. However, these models often struggle with creative\nexpressions, particularly those involving complex, abstract, or highly\ndescriptive language. In this work, we introduce a novel training-free approach\ntailored to improve image generation for a unique form of creative language:\npoetic verse, which frequently features layered, abstract, and dual meanings.\nOur proposed PoemTale Diffusion approach aims to minimise the information that\nis lost during poetic text-to-image conversion by integrating a multi stage\nprompt refinement loop into Language Models to enhance the interpretability of\npoetic texts. To support this, we adapt existing state-of-the-art diffusion\nmodels by modifying their self-attention mechanisms with a consistent\nself-attention technique to generate multiple consistent images, which are then\ncollectively used to convey the poem's meaning. Moreover, to encourage research\nin the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting\nof 1111 poems sourced from multiple online and offline resources. We engaged a\npanel of poetry experts for qualitative assessments. The results from both\nhuman and quantitative evaluations validate the efficacy of our method and\ncontribute a novel perspective to poem-to-image generation with enhanced\ninformation capture in the generated images.", "AI": {"tldr": "提出PoemTale Diffusion，一种无需训练的方法，通过多阶段提示词精炼和一致性自注意力机制，改善文本到图像模型对诗歌等创意语言的理解和图像生成，并发布P4I数据集。", "motivation": "现有文本到图像扩散模型在处理复杂、抽象或富有创意的语言（特别是诗歌）时，难以准确解释文本并生成高质量图像，导致信息丢失。", "method": "引入PoemTale Diffusion方法，该方法无需训练。它通过在语言模型中集成多阶段提示词精炼循环来增强诗歌文本的可解释性。同时，通过修改现有扩散模型的自注意力机制，采用一致性自注意力技术生成多张连贯图像，共同传达诗歌含义。此外，还构建并发布了包含1111首诗歌的P4I (PoemForImage) 数据集。通过人类专家定性评估和定量评估来验证方法效果。", "result": "人类和定量评估结果均验证了所提出方法的有效性。生成图像中诗歌信息的捕获能力得到显著增强。", "conclusion": "该研究为诗歌到图像生成提供了一种新颖的视角，有效提升了生成图像中诗歌信息的捕获能力，解决了现有模型在处理复杂创意文本方面的挑战。"}}
{"id": "2507.14045", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14045", "abs": "https://arxiv.org/abs/2507.14045", "authors": ["Israt Jahan", "Md Tahmid Rahman Laskar", "Chun Peng", "Jimmy Huang"], "title": "Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks", "comment": "Accepted at Canadian AI 2025", "summary": "This paper presents a comprehensive evaluation of cost-efficient Large\nLanguage Models (LLMs) for diverse biomedical tasks spanning both text and\nimage modalities. We evaluated a range of closed-source and open-source LLMs on\ntasks such as biomedical text classification and generation, question\nanswering, and multimodal image processing. Our experimental findings indicate\nthat there is no single LLM that can consistently outperform others across all\ntasks. Instead, different LLMs excel in different tasks. While some\nclosed-source LLMs demonstrate strong performance on specific tasks, their\nopen-source counterparts achieve comparable results (sometimes even better),\nwith additional benefits like faster inference and enhanced privacy. Our\nexperimental results offer valuable insights for selecting models that are\noptimally suited for specific biomedical applications.", "AI": {"tldr": "本文全面评估了多种成本效益高的LLM在生物医学文本和图像任务上的表现，发现没有模型能通吃所有任务，开源模型常能媲美甚至超越闭源模型，并具有额外优势。", "motivation": "旨在评估成本效益高的大型语言模型（LLMs）在多样化生物医学任务（涵盖文本和图像模态）中的表现，以提供模型选择的指导。", "method": "评估了一系列闭源和开源LLMs，测试任务包括生物医学文本分类和生成、问答以及多模态图像处理。", "result": "实验结果显示，没有单一LLM能在所有任务上始终表现最佳；不同的LLM在不同任务中各有优势。虽然一些闭源LLM在特定任务上表现出色，但其开源对应模型也能达到可比（有时甚至更好）的结果，并具有更快的推理速度和增强的隐私性等额外优势。", "conclusion": "研究结果为选择最适合特定生物医学应用的模型提供了宝贵的见解。"}}
{"id": "2507.14093", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14093", "abs": "https://arxiv.org/abs/2507.14093", "authors": ["Šimon Kubov", "Simon Klíčník", "Jakub Dandár", "Zdeněk Straka", "Karolína Kvaková", "Daniel Kvak"], "title": "Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment", "comment": null, "summary": "Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment\ndecisions depend on precise Cobb angle measurement. Manual assessment is time\nconsuming and subject to inter observer variation. We conducted a\nretrospective, multi centre evaluation of a fully automated deep learning\nsoftware (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on\n103 standing anteroposterior whole spine radiographs collected from ten\nhospitals. Two musculoskeletal radiologists independently measured each study\nand served as reference readers. Agreement between the AI and each radiologist\nwas assessed with Bland Altman analysis, mean absolute error (MAE), root mean\nsquared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four\ngrade severity classification. Against Radiologist 1 the AI achieved an MAE of\n3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of\nagreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI\nachieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees\nand limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r\nequals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen\nkappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).\nThese results demonstrate that the proposed software reproduces expert level\nCobb angle measurements and categorical grading across multiple centres,\nsuggesting its utility for streamlining scoliosis reporting and triage in\nclinical workflows.", "AI": {"tldr": "该研究评估了一款基于深度学习的自动化软件在脊柱侧弯Cobb角测量上的表现，结果显示其测量精度可媲美放射科专家，有望简化临床工作流程。", "motivation": "脊柱侧弯的治疗决策依赖于精确的Cobb角测量，但手动评估耗时且存在观察者间差异。因此，需要一种更高效、客观的测量方法。", "method": "研究对来自10家医院的103张站立位全脊柱前后位X线片进行了回顾性、多中心评估。使用一款全自动深度学习软件（Carebot AI Bones）测量Cobb角，并与两位独立的肌肉骨骼放射科医生的测量结果进行比较。评估指标包括Bland-Altman分析、平均绝对误差（MAE）、均方根误差（RMSE）、Pearson相关系数和Cohen Kappa系数（用于四级严重程度分类）。", "result": "与放射科医生1相比，AI的MAE为3.89度（RMSE 4.77度），偏差为0.70度。与放射科医生2相比，AI的MAE为3.90度（RMSE 5.68度），偏差为2.14度。AI与两位放射科医生的Pearson相关系数分别为0.906和0.880（放射科医生间为0.928）。严重程度分级的Cohen Kappa系数分别为0.51和0.64（放射科医生间为0.59）。", "conclusion": "研究结果表明，所提出的深度学习软件在多中心环境下能够重现专家水平的Cobb角测量和分类分级，这表明其在简化脊柱侧弯报告和临床工作流程中的分流具有实用价值。"}}
{"id": "2507.13719", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13719", "abs": "https://arxiv.org/abs/2507.13719", "authors": ["Daniele Pannone", "Alessia Castronovo", "Maurizio Mancini", "Gian Luca Foresti", "Claudio Piciarelli", "Rossana Gabrieli", "Muhammad Yasir Bilal", "Danilo Avola"], "title": "Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction", "comment": null, "summary": "This paper presents an innovative augmented reality pipeline tailored for\nmuseum environments, aimed at recognizing artworks and generating accurate 3D\nmodels from single images. By integrating two complementary pre-trained depth\nestimation models, i.e., GLPN for capturing global scene structure and\nDepth-Anything for detailed local reconstruction, the proposed approach\nproduces optimized depth maps that effectively represent complex artistic\nfeatures. These maps are then converted into high-quality point clouds and\nmeshes, enabling the creation of immersive AR experiences. The methodology\nleverages state-of-the-art neural network architectures and advanced computer\nvision techniques to overcome challenges posed by irregular contours and\nvariable textures in artworks. Experimental results demonstrate significant\nimprovements in reconstruction accuracy and visual realism, making the system a\nhighly robust tool for museums seeking to enhance visitor engagement through\ninteractive digital content.", "AI": {"tldr": "本文提出了一种博物馆AR管道，通过融合GLPN和Depth-Anything两种深度估计模型，从单张图像生成高精度艺术品3D模型，以增强访客体验。", "motivation": "旨在解决博物馆环境中艺术品识别和单图3D模型生成挑战，克服艺术品不规则轮廓和多变纹理带来的困难，从而通过互动数字内容提升访客参与度。", "method": "该方法整合了GLPN（用于全局结构）和Depth-Anything（用于局部细节）两种预训练深度估计模型，生成优化深度图。随后，这些深度图被转换为高质量点云和网格，并利用先进的神经网络架构和计算机视觉技术来处理复杂的艺术特征。", "result": "实验结果显示，该系统在重建精度和视觉真实感方面有显著提升，证明其作为一个高度鲁棒的工具，能有效增强博物馆访客的互动体验。", "conclusion": "该系统为博物馆提供了一个强大的工具，能够通过生成准确的艺术品3D模型和沉浸式AR体验，显著提升访客的参与度和数字内容互动性。"}}
{"id": "2507.14063", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14063", "abs": "https://arxiv.org/abs/2507.14063", "authors": ["Lautaro Estienne", "Gabriel Ben Zenou", "Nona Naderi", "Jackie Cheung", "Pablo Piantanida"], "title": "Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog", "comment": null, "summary": "As AI systems take on collaborative roles, they must reason about shared\ngoals and beliefs-not just generate fluent language. The Rational Speech Act\n(RSA) framework offers a principled approach to pragmatic reasoning, but\nexisting extensions face challenges in scaling to multi-turn, collaborative\nscenarios. In this paper, we introduce Collaborative Rational Speech Act\n(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn\ndialog by optimizing a gain function adapted from rate-distortion theory. This\ngain is an extension of the gain model that is maximized in the original RSA\nmodel but takes into account the scenario in which both agents in a\nconversation have private information and produce utterances conditioned on the\ndialog. We demonstrate the effectiveness of CRSA on referential games and\ntemplate-based doctor-patient dialogs in the medical domain. Empirical results\nshow that CRSA yields more consistent, interpretable, and collaborative\nbehavior than existing baselines-paving the way for more pragmatic and socially\naware language agents.", "AI": {"tldr": "本文提出了协作理性言语行为（CRSA），一个基于信息理论的理性言语行为（RSA）框架扩展，用于建模多轮对话，通过优化源自率失真理论的增益函数，实现了更具协作性的语言智能体。", "motivation": "现有的AI系统在协作角色中需要推理共享目标和信念，而不仅仅是生成流畅语言。然而，现有的RSA扩展在处理多轮、协作场景时面临扩展性挑战。", "method": "引入了协作理性言语行为（CRSA），它是RSA的一个信息理论（IT）扩展。CRSA通过优化一个从率失真理论改编的增益函数来建模多轮对话，该函数考虑了对话中双方代理的私有信息和对话条件下的言语产生。", "result": "CRSA在指称游戏和医疗领域的模板式医患对话中表现出有效性。实证结果表明，CRSA比现有基线模型产生更一致、可解释和协作的行为。", "conclusion": "CRSA为更具语用和社交意识的语言智能体铺平了道路，解决了多轮协作场景中的挑战。"}}
{"id": "2507.14096", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14096", "abs": "https://arxiv.org/abs/2507.14096", "authors": ["Brian Ondov", "William Xia", "Kush Attal", "Ishita Unde", "Jerry He", "Hoa Dang", "Ian Soboroff", "Dina Demner-Fushman"], "title": "Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track", "comment": null, "summary": "Objective: Recent advances in language models have shown potential to adapt\nprofessional-facing biomedical literature to plain language, making it\naccessible to patients and caregivers. However, their unpredictability,\ncombined with the high potential for harm in this domain, means rigorous\nevaluation is necessary. Our goals with this track were to stimulate research\nand to provide high-quality evaluation of the most promising systems.\n  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts\n(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included\ncomplete, sentence-level, rewriting of abstracts (Task 1) as well as\nidentifying and replacing difficult terms (Task 2). For automatic evaluation of\nTask 1, we developed a four-fold set of professionally-written references.\nSubmissions for both Tasks 1 and 2 were provided extensive manual evaluation\nfrom biomedical experts.\n  Results: Twelve teams spanning twelve countries participated in the track,\nwith models from multilayer perceptrons to large pretrained transformers. In\nmanual judgments of Task 1, top-performing models rivaled human levels of\nfactual accuracy and completeness, but not simplicity or brevity. Automatic,\nreference-based metrics generally did not correlate well with manual judgments.\nIn Task 2, systems struggled with identifying difficult terms and classifying\nhow to replace them. When generating replacements, however, LLM-based systems\ndid well in manually judged accuracy, completeness, and simplicity, though not\nin brevity.\n  Conclusion: The PLABA track showed promise for using Large Language Models to\nadapt biomedical literature for the general public, while also highlighting\ntheir deficiencies and the need for improved automatic benchmarking tools.", "AI": {"tldr": "PLABA竞赛评估了语言模型将生物医学文献改编为普通语言的能力，发现顶级模型在准确性和完整性上接近人类水平，但在简洁性、术语识别和自动评估相关性方面仍有不足，强调了未来改进的方向。", "motivation": "语言模型在将专业生物医学文献转化为通俗语言方面展现潜力，但其不可预测性及潜在危害性要求严格评估。本研究旨在刺激相关研究并提供高质量的系统评估。", "method": "在2023和2024年的文本检索会议上举办了PLABA竞赛。任务包括：1) 完整、句子级别的摘要重写（任务1）；2) 识别和替换难懂术语（任务2）。任务1采用四套专业编写的参考文本进行自动评估，所有提交均由生物医学专家进行详尽的人工评估。", "result": "12个国家的团队参与了竞赛。在任务1的人工评估中，表现最佳的模型在事实准确性和完整性上可与人类媲美，但在简洁性和简短性上仍有差距。自动、基于参考的度量标准与人工判断的相关性不佳。在任务2中，系统在识别和分类难懂术语方面表现不佳；但在生成替换内容时，基于LLM的系统在准确性、完整性和简洁性方面表现良好，但在简短性上仍有不足。", "conclusion": "PLABA竞赛显示了使用大型语言模型为公众改编生物医学文献的潜力，同时也揭示了它们的不足，并强调了改进自动基准测试工具的必要性。"}}
{"id": "2507.13753", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13753", "abs": "https://arxiv.org/abs/2507.13753", "authors": ["Tongtong Su", "Chengyu Wang", "Bingyan Liu", "Jun Huang", "Dongming Lu"], "title": "Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis", "comment": null, "summary": "In recent years, large text-to-video (T2V) synthesis models have garnered\nconsiderable attention for their abilities to generate videos from textual\ndescriptions. However, achieving both high imaging quality and effective motion\nrepresentation remains a significant challenge for these T2V models. Existing\napproaches often adapt pre-trained text-to-image (T2I) models to refine video\nframes, leading to issues such as flickering and artifacts due to\ninconsistencies across frames. In this paper, we introduce EVS, a training-free\nEncapsulated Video Synthesizer that composes T2I and T2V models to enhance both\nvisual fidelity and motion smoothness of generated videos. Our approach\nutilizes a well-trained diffusion-based T2I model to refine low-quality video\nframes by treating them as out-of-distribution samples, effectively optimizing\nthem with noising and denoising steps. Meanwhile, we employ T2V backbones to\nensure consistent motion dynamics. By encapsulating the T2V temporal-only prior\ninto the T2I generation process, EVS successfully leverages the strengths of\nboth types of models, resulting in videos of improved imaging and motion\nquality. Experimental results validate the effectiveness of our approach\ncompared to previous approaches. Our composition process also leads to a\nsignificant improvement of 1.6x-4.5x speedup in inference time. Source codes:\nhttps://github.com/Tonniia/EVS.", "AI": {"tldr": "本文提出EVS，一个免训练的封装式视频合成器，它结合了文本到图像（T2I）和文本到视频（T2V）模型，以显著提升生成视频的图像质量和运动流畅性，并实现推理速度的提升。", "motivation": "当前大型文本到视频（T2V）合成模型在实现高图像质量和有效运动表示方面面临挑战。现有方法常将预训练的T2I模型用于精炼视频帧，但这会导致帧间不一致，产生闪烁和伪影问题。", "method": "EVS（Encapsulated Video Synthesizer）是一种免训练的方法。它利用训练好的扩散模型T2I模型来精炼低质量视频帧，将其视为分布外样本并通过去噪步骤进行优化。同时，它采用T2V骨干网络确保一致的运动动态。通过将T2V模型的时序先验封装到T2I生成过程中，EVS有效结合了两类模型的优势。", "result": "EVS成功提升了生成视频的图像和运动质量。实验结果验证了其相较于现有方法的有效性。此外，该组合过程还带来了1.6倍至4.5倍的显著推理速度提升。", "conclusion": "EVS通过巧妙地结合T2I和T2V模型的优势，解决了文本到视频合成中图像质量和运动流畅性的核心挑战，并显著提高了推理效率，为高质量视频生成提供了有效方案。"}}
{"id": "2507.13773", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13773", "abs": "https://arxiv.org/abs/2507.13773", "authors": ["Pu Jian", "Donglei Yu", "Wen Yang", "Shuo Ren", "Jiajun Zhang"], "title": "Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions", "comment": "ACL2025 Main", "summary": "In visual question answering (VQA) context, users often pose ambiguous\nquestions to visual language models (VLMs) due to varying expression habits.\nExisting research addresses such ambiguities primarily by rephrasing questions.\nThese approaches neglect the inherently interactive nature of user interactions\nwith VLMs, where ambiguities can be clarified through user feedback. However,\nresearch on interactive clarification faces two major challenges: (1)\nBenchmarks are absent to assess VLMs' capacity for resolving ambiguities\nthrough interaction; (2) VLMs are trained to prefer answering rather than\nasking, preventing them from seeking clarification. To overcome these\nchallenges, we introduce \\textbf{ClearVQA} benchmark, which targets three\ncommon categories of ambiguity in VQA context, and encompasses various VQA\nscenarios.", "AI": {"tldr": "该论文提出了ClearVQA基准测试，旨在解决视觉问答（VQA）中用户提问歧义的问题，并通过交互式澄清而非简单改写来评估视觉语言模型（VLMs）的澄清能力。", "motivation": "在VQA中，用户常因表达习惯不同而提出模糊问题。现有研究主要通过改写问题来处理歧义，但忽视了用户与VLM交互的本质——歧义可通过用户反馈来澄清。然而，交互式澄清研究面临两大挑战：缺乏评估VLM交互澄清能力的基准，以及VLM倾向于回答而非提问，阻碍其主动寻求澄清。", "method": "为克服上述挑战，论文引入了ClearVQA基准测试。", "result": "ClearVQA基准测试针对VQA语境中三种常见的歧义类别，并涵盖了多种VQA场景。", "conclusion": "ClearVQA基准测试的引入为评估和提升VLM通过交互解决歧义的能力提供了工具，填补了现有研究的空白。"}}
{"id": "2507.14119", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14119", "abs": "https://arxiv.org/abs/2507.14119", "authors": ["Maksim Kuprashevich", "Grigorii Alekseenko", "Irina Tolstykh", "Georgii Fedorov", "Bulat Suleimanov", "Vladimir Dokholyan", "Aleksandr Gordeev"], "title": "NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining", "comment": null, "summary": "Recent advances in generative modeling enable image editing assistants that\nfollow natural language instructions without additional user input. Their\nsupervised training requires millions of triplets: original image, instruction,\nedited image. Yet mining pixel-accurate examples is hard. Each edit must affect\nonly prompt-specified regions, preserve stylistic coherence, respect physical\nplausibility, and retain visual appeal. The lack of robust automated\nedit-quality metrics hinders reliable automation at scale. We present an\nautomated, modular pipeline that mines high-fidelity triplets across domains,\nresolutions, instruction complexities, and styles. Built on public generative\nmodels and running without human intervention, our system uses a task-tuned\nGemini validator to score instruction adherence and aesthetics directly,\nremoving any need for segmentation or grounding models. Inversion and\ncompositional bootstrapping enlarge the mined set by approximately 2.2x,\nenabling large-scale high-fidelity training data. By automating the most\nrepetitive annotation steps, the approach allows a new scale of training\nwithout human labeling effort. To democratize research in this\nresource-intensive area, we release NHR-Edit: an open dataset of 358k\nhigh-quality triplets. In the largest cross-dataset evaluation, it surpasses\nall public alternatives. We also release Bagel-NHR-Edit, an open-source\nfine-tuned Bagel model, which achieves state-of-the-art metrics in our\nexperiments.", "AI": {"tldr": "本文提出了一种自动化、模块化的管道，用于挖掘高质量的图像-指令-编辑图像三元组，以训练生成式图像编辑模型，并发布了一个大规模的开放数据集NHR-Edit和最先进的模型Bagel-NHR-Edit。", "motivation": "当前生成式图像编辑助手的监督训练需要数百万个高质量的三元组（原始图像、指令、编辑图像），但手动挖掘像素级精确的示例非常困难，因为每个编辑必须满足区域限制、风格一致性、物理合理性和视觉吸引力等严格要求，且缺乏可靠的自动化编辑质量度量标准。", "method": "该研究开发了一个自动化、模块化的管道，利用公共生成模型，并使用一个针对任务优化的Gemini验证器直接评估指令遵循度和美观性，无需分割或接地模型。通过反演和组合引导（inversion and compositional bootstrapping）方法，将挖掘到的数据集扩大了约2.2倍。", "result": "该方法成功挖掘并发布了NHR-Edit，一个包含35.8万个高质量三元组的开放数据集，该数据集在最大的跨数据集评估中超越了所有公开替代方案。此外，还发布了Bagel-NHR-Edit，一个开源的微调Bagel模型，在实验中实现了最先进的性能指标。", "conclusion": "该自动化管道实现了大规模、高保真训练数据的生成，无需人工标注工作，从而降低了该资源密集型领域的研究门槛。通过提供NHR-Edit数据集和Bagel-NHR-Edit模型，该研究为图像编辑领域的研究做出了重要贡献。"}}
{"id": "2507.13772", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13772", "abs": "https://arxiv.org/abs/2507.13772", "authors": ["Abhijit Sen", "Giridas Maiti", "Bikram K. Parida", "Bhanu P. Mishra", "Mahima Arya", "Denys I. Bondar"], "title": "Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification", "comment": null, "summary": "Feature engineering continues to play a critical role in image\nclassification, particularly when interpretability and computational efficiency\nare prioritized over deep learning models with millions of parameters. In this\nstudy, we revisit classical machine learning based image classification through\na novel approach centered on Permutation Entropy (PE), a robust and\ncomputationally lightweight measure traditionally used in time series analysis\nbut rarely applied to image data. We extend PE to two-dimensional images and\npropose a multiscale, multi-orientation entropy-based feature extraction\napproach that characterizes spatial order and complexity along rows, columns,\ndiagonals, anti-diagonals, and local patches of the image. To enhance the\ndiscriminatory power of the entropy features, we integrate two classic image\ndescriptors: the Histogram of Oriented Gradients (HOG) to capture shape and\nedge structure, and Local Binary Patterns (LBP) to encode micro-texture of an\nimage. The resulting hand-crafted feature set, comprising of 780 dimensions, is\nused to train Support Vector Machine (SVM) classifiers optimized through grid\nsearch. The proposed approach is evaluated on multiple benchmark datasets,\nincluding Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers\ncompetitive classification performance without relying on deep architectures.\nOur results demonstrate that the fusion of PE with HOG and LBP provides a\ncompact, interpretable, and effective alternative to computationally expensive\nand limited interpretable deep learning models. This shows a potential of\nentropy-based descriptors in image classification and contributes a lightweight\nand generalizable solution to interpretable machine learning in image\nclassification and computer vision.", "AI": {"tldr": "该研究提出一种基于置换熵（PE）的多尺度、多方向特征提取方法，并结合HOG和LBP特征，用于图像分类，旨在提供一种计算高效且可解释的深度学习替代方案。", "motivation": "在图像分类中，当可解释性和计算效率优先于参数量庞大的深度学习模型时，特征工程仍扮演关键角色。该研究旨在探索一种新颖的、非深度学习的图像分类方法。", "method": "将时间序列分析中常用的置换熵（PE）扩展到二维图像，并提出一种多尺度、多方向的基于熵的特征提取方法，用于描述图像的行、列、对角线、反对角线和局部块的空间顺序与复杂性。为增强特征的判别力，集成了经典的HOG（用于捕获形状和边缘结构）和LBP（用于编码微纹理）描述符。最终形成780维的手工特征集，并使用网格搜索优化的支持向量机（SVM）分类器进行训练。", "result": "在Fashion-MNIST、KMNIST、EMNIST和CIFAR-10等多个基准数据集上进行了评估，结果显示该方法在不依赖深度学习架构的情况下，实现了具有竞争力的分类性能。PE与HOG和LBP的融合提供了一种紧凑、可解释且有效的替代方案，相较于计算昂贵且可解释性有限的深度学习模型。", "conclusion": "该研究展示了基于熵的描述符在图像分类中的潜力，并为图像分类和计算机视觉领域的可解释机器学习贡献了一种轻量级且可泛化的解决方案。"}}
{"id": "2507.13779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13779", "abs": "https://arxiv.org/abs/2507.13779", "authors": ["Durgesh Singh", "Ahcène Boubekki", "Robert Jenssen", "Michael Kampffmeyer"], "title": "SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering", "comment": null, "summary": "Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)\nenhance the model performance by exploiting information from labeled and\nunlabeled data. The clustering assumption has proven advantageous for learning\nwith limited supervision and states that data points belonging to the same\ncluster in a high-dimensional space should be assigned to the same category.\nRecent works have utilized different training mechanisms to implicitly enforce\nthis assumption for the SSL and UDA. In this work, we take a different approach\nby explicitly involving a differentiable clustering module which is extended to\nleverage the supervised data to compute its centroids. We demonstrate the\neffectiveness of our straightforward end-to-end training strategy for SSL and\nUDA over extensive experiments and highlight its benefits, especially in low\nsupervision regimes, both as a standalone model and as a regularizer for\nexisting approaches.", "AI": {"tldr": "本文提出一种显式可微分聚类模块，通过利用监督数据计算聚类中心，以端到端方式应用于半监督学习（SSL）和无监督域适应（UDA），在低监督条件下表现尤为出色。", "motivation": "半监督学习（SSL）和无监督域适应（UDA）利用有限的监督信息提升模型性能。聚类假设（同类数据点在高维空间中应聚在一起）已被证明对学习有益，但现有方法多是隐式强制执行。本文旨在显式地应用这一假设。", "method": "引入一个显式且可微分的聚类模块，并扩展该模块以利用监督数据计算聚类中心。采用直接的端到端训练策略进行模型训练。", "result": "通过大量实验证明了所提方法在SSL和UDA任务上的有效性，尤其在低监督条件下效果显著。该方法既可作为独立模型使用，也可作为现有方法的正则化器。", "conclusion": "通过显式整合一个利用监督数据计算聚类中心的可微分聚类模块，可以有效提升半监督学习和无监督域适应的性能，特别是在监督数据稀缺的情况下，并且能作为独立模型或正则化器发挥作用。"}}
{"id": "2507.13797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13797", "abs": "https://arxiv.org/abs/2507.13797", "authors": ["Huu-Phu Do", "Yu-Wei Chen", "Yi-Cheng Liao", "Chi-Wei Hsiao", "Han-Yang Wang", "Wei-Chen Chiu", "Ching-Chun Huang"], "title": "DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance", "comment": "Accepted by ICCV 2025", "summary": "Blind Face Restoration aims to recover high-fidelity, detail-rich facial\nimages from unknown degraded inputs, presenting significant challenges in\npreserving both identity and detail. Pre-trained diffusion models have been\nincreasingly used as image priors to generate fine details. Still, existing\nmethods often use fixed diffusion sampling timesteps and a global guidance\nscale, assuming uniform degradation. This limitation and potentially imperfect\ndegradation kernel estimation frequently lead to under- or over-diffusion,\nresulting in an imbalance between fidelity and quality. We propose\nDynFaceRestore, a novel blind face restoration approach that learns to map any\nblindly degraded input to Gaussian blurry images. By leveraging these blurry\nimages and their respective Gaussian kernels, we dynamically select the\nstarting timesteps for each blurry image and apply closed-form guidance during\nthe diffusion sampling process to maintain fidelity. Additionally, we introduce\na dynamic guidance scaling adjuster that modulates the guidance strength across\nlocal regions, enhancing detail generation in complex areas while preserving\nstructural fidelity in contours. This strategy effectively balances the\ntrade-off between fidelity and quality. DynFaceRestore achieves\nstate-of-the-art performance in both quantitative and qualitative evaluations,\ndemonstrating robustness and effectiveness in blind face restoration.", "AI": {"tldr": "DynFaceRestore是一种新颖的盲人脸修复方法，通过动态选择扩散采样起始时间步和应用动态局部引导比例调整，有效平衡了图像保真度和质量。", "motivation": "现有的人脸修复方法通常使用固定的扩散采样时间步和全局引导比例，并假设降质是均匀的，这可能导致过度或不足的扩散，从而在保真度和质量之间产生不平衡。此外，不完美的降质核估计也加剧了这个问题。", "method": "该方法首先学习将任意盲降质输入映射到高斯模糊图像。然后，利用这些模糊图像及其对应的高斯核，动态选择每个模糊图像的扩散采样起始时间步，并在扩散采样过程中应用闭式引导以保持保真度。此外，引入了一个动态引导比例调节器，用于调整局部区域的引导强度，从而在复杂区域增强细节生成，同时在轮廓处保持结构保真度。", "result": "DynFaceRestore在定量和定性评估中均达到了最先进的性能，展示了其在盲人脸修复中的鲁棒性和有效性。", "conclusion": "DynFaceRestore通过动态调整扩散参数和引导强度，成功解决了盲人脸修复中保真度和质量之间的权衡问题，实现了卓越的修复效果。"}}
{"id": "2507.13803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13803", "abs": "https://arxiv.org/abs/2507.13803", "authors": ["Weiqi Yang", "Xu Zhou", "Jingfu Guan", "Hao Du", "Tianyu Bai"], "title": "GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation", "comment": null, "summary": "Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely\ndeployed in smart homes, intelligent transport, industrial automation, and\nhealthcare. However, existing systems often face challenges: high model\ncomplexity hinders deployment in resource-constrained environments,\nunidirectional modal alignment neglects inter-modal relationships, and\nrobustness suffers when sensor data is missing. These issues impede efficient\nand robust multimodal perception in real-world IoT settings. To overcome these\nlimitations, we propose GRAM-MAMBA. This framework utilizes the\nlinear-complexity Mamba model for efficient sensor time-series processing,\ncombined with an optimized GRAM matrix strategy for pairwise alignment among\nmodalities, addressing the shortcomings of traditional single-modality\nalignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive\nlow-rank layer compensation strategy to handle missing modalities\npost-training. This strategy freezes the pre-trained model core and irrelevant\nadaptive layers, fine-tuning only those related to available modalities and the\nfusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On\nthe SPAWC2021 indoor positioning dataset, the pre-trained model shows lower\nerror than baselines; adapting to missing modalities yields a 24.5% performance\nboost by training less than 0.2% of parameters. On the USC-HAD human activity\nrecognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),\noutperforming prior work; the update strategy increases F1 by 23% while\ntraining less than 0.3% of parameters. These results highlight GRAM-MAMBA's\npotential for achieving efficient and robust multimodal perception in\nresource-constrained environments.", "AI": {"tldr": "GRAM-MAMBA是一种针对物联网（IoT）多模态感知的框架，它利用Mamba模型处理时序数据，通过GRAM矩阵进行模态对齐，并引入低秩自适应策略处理缺失模态，实现了高效且鲁棒的感知。", "motivation": "现有物联网多模态系统面临模型复杂性高（难以部署于资源受限环境）、单向模态对齐忽略模态间关系以及传感器数据缺失时鲁棒性差等挑战，这些问题阻碍了在实际IoT环境中高效、鲁棒的多模态感知。", "method": "本文提出了GRAM-MAMBA框架：1) 利用线性复杂度的Mamba模型高效处理传感器时序数据。2) 采用优化的GRAM矩阵策略实现模态间的成对对齐，以解决传统单模态对齐的不足。3) 借鉴LoRA思想，引入自适应低秩层补偿策略，在训练后处理缺失模态，通过冻结预训练模型核心和无关自适应层，仅微调与可用模态及融合过程相关的参数。", "result": "在SPAWC2021室内定位数据集上，预训练模型误差低于基线；适应缺失模态后，仅训练不到0.2%的参数，性能提升24.5%。在USC-HAD人类活动识别数据集上，F1分数达到93.55%，整体准确率(OA)达到93.81%，优于现有工作；更新策略使F1分数提高23%，而训练参数不到0.3%。", "conclusion": "GRAM-MAMBA框架在资源受限环境中实现了高效且鲁棒的多模态感知，其在处理时序数据、模态对齐和应对缺失数据方面的创新策略得到了实验验证，显示出巨大的应用潜力。"}}
{"id": "2507.13812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13812", "abs": "https://arxiv.org/abs/2507.13812", "authors": ["Yingying Zhang", "Lixiang Ru", "Kang Wu", "Lei Yu", "Lei Liang", "Yansheng Li", "Jingdong Chen"], "title": "SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing", "comment": "Accepted by ICCV25", "summary": "The multi-modal remote sensing foundation model (MM-RSFM) has significantly\nadvanced various Earth observation tasks, such as urban planning, environmental\nmonitoring, and natural disaster management. However, most existing approaches\ngenerally require the training of separate backbone networks for each data\nmodality, leading to redundancy and inefficient parameter utilization.\nMoreover, prevalent pre-training methods typically apply self-supervised\nlearning (SSL) techniques from natural images without adequately accommodating\nthe characteristics of remote sensing (RS) images, such as the complicated\nsemantic distribution within a single RS image. In this work, we present\nSkySense V2, a unified MM-RSFM that employs a single transformer backbone to\nhandle multiple modalities. This backbone is pre-trained with a novel SSL\nstrategy tailored to the distinct traits of RS data. In particular, SkySense V2\nincorporates an innovative adaptive patch merging module and learnable modality\nprompt tokens to address challenges related to varying resolutions and limited\nfeature diversity across modalities. In additional, we incorporate the mixture\nof experts (MoE) module to further enhance the performance of the foundation\nmodel. SkySense V2 demonstrates impressive generalization abilities through an\nextensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense\nby an average of 1.8 points.", "AI": {"tldr": "SkySense V2是一个统一的多模态遥感基础模型，采用单个Transformer骨干网络和定制的自监督学习策略，解决了现有模型冗余和特征多样性不足的问题。", "motivation": "现有多模态遥感基础模型通常为每种模态训练独立的骨干网络，导致冗余和参数利用效率低下。此外，主流预训练方法直接应用自然图像的自监督学习技术，未能充分适应遥感图像复杂的语义分布特性。", "method": "SkySense V2采用一个统一的Transformer骨干网络来处理多模态数据，并引入了专为遥感数据设计的创新自监督学习策略。具体地，它包含了自适应补丁合并模块以应对不同分辨率，以及可学习模态提示令牌以解决模态间有限特征多样性问题。此外，该模型还集成了专家混合（MoE）模块以进一步提升性能。", "result": "SkySense V2在涉及7项任务的16个数据集上进行了广泛评估，展示了令人印象深刻的泛化能力，平均性能比SkySense提高了1.8个点。", "conclusion": "SkySense V2提供了一个更高效、更有效的统一多模态遥感基础模型，显著提升了地球观测任务的性能和泛化能力。"}}
{"id": "2507.13861", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13861", "abs": "https://arxiv.org/abs/2507.13861", "authors": ["Junjie Hu", "Tianyang Han", "Kai Ma", "Jialin Gao", "Hao Dou", "Song Yang", "Xianhua He", "Jianhui Zhang", "Junfeng Luo", "Xiaoming Wei", "Wenqiang Zhang"], "title": "PositionIC: Unified Position and Identity Consistency for Image Customization", "comment": null, "summary": "Recent subject-driven image customization has achieved significant\nadvancements in fidelity, yet fine-grained entity-level spatial control remains\nelusive, hindering the broader real-world application. This limitation is\nmainly attributed to scalable datasets that bind identity with precise\npositional cues are absent. To this end, we introduce PositionIC, a unified\nframework that enforces position and identity consistency for multi-subject\ncustomization. We construct a scalable synthesis pipeline that employs a\nbidirectional generation paradigm to eliminate subject drift and maintain\nsemantic coherence. On top of these data, we design a lightweight positional\nmodulation layer that decouples spatial embeddings among subjects, enabling\nindependent, accurate placement while preserving visual fidelity. Extensive\nexperiments demonstrate that our approach can achieve precise spatial control\nwhile maintaining high consistency in image customization task. PositionIC\npaves the way for controllable, high-fidelity image customization in\nopen-world, multi-entity scenarios and will be released to foster further\nresearch.", "AI": {"tldr": "PositionIC是一个统一框架，通过引入可扩展合成管线和轻量级位置调制层，解决了多主体图像定制中缺乏精细空间控制的问题，实现了精确的空间定位和高保真度。", "motivation": "现有图像定制方法在保真度方面取得了显著进展，但缺乏精细的实体级空间控制，这主要是因为缺少将身份与精确位置线索绑定的可扩展数据集，从而阻碍了其在现实世界中的广泛应用。", "method": "本研究提出了PositionIC框架，包含两个核心组件：1) 一个可扩展的合成管线，采用双向生成范式来消除主体漂移并保持语义一致性。2) 一个轻量级的位置调制层，用于解耦主体之间的空间嵌入，从而实现独立的、精确的定位，同时保持视觉保真度。", "result": "实验证明，PositionIC方法能够在图像定制任务中实现精确的空间控制，同时保持高一致性和视觉保真度。", "conclusion": "PositionIC为开放世界、多实体场景中的可控、高保真图像定制铺平了道路，并有望促进未来的相关研究。"}}
{"id": "2507.13891", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13891", "abs": "https://arxiv.org/abs/2507.13891", "authors": ["Yu Wei", "Jiahui Zhang", "Xiaoqin Zhang", "Ling Shao", "Shijian Lu"], "title": "PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations", "comment": "Accepted by ICCV2025", "summary": "COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing\nattention due to its remarkable performance in reconstructing high-quality 3D\nscenes from unposed images or videos. However, it often struggles to handle\nscenes with complex camera trajectories as featured by drastic rotation and\ntranslation across adjacent camera views, leading to degraded estimation of\ncamera poses and further local minima in joint optimization of camera poses and\n3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that\nachieves superior 3D scene modeling and camera pose estimation via camera pose\nco-regularization. PCR-GS achieves regularization from two perspectives. The\nfirst is feature reprojection regularization which extracts view-robust DINO\nfeatures from adjacent camera views and aligns their semantic information for\ncamera pose regularization. The second is wavelet-based frequency\nregularization which exploits discrepancy in high-frequency details to further\noptimize the rotation matrix in camera poses. Extensive experiments over\nmultiple real-world scenes show that the proposed PCR-GS achieves superior\npose-free 3D-GS scene modeling under dramatic changes of camera trajectories.", "AI": {"tldr": "PCR-GS是一种创新的免COLMAP 3D Gaussian Splatting技术，通过特征重投影和基于小波的频率正则化，解决了复杂相机轨迹下3D-GS场景建模和相机姿态估计性能下降的问题。", "motivation": "免COLMAP的3D Gaussian Splatting在处理相邻视图间存在剧烈旋转和平移的复杂相机轨迹场景时表现不佳，导致相机姿态估计退化，并在姿态与3D-GS联合优化中陷入局部最优。", "method": "PCR-GS通过相机姿态协同正则化实现：1) 特征重投影正则化：从相邻视图提取视图鲁棒的DINO特征并对齐其语义信息以正则化相机姿态。2) 基于小波的频率正则化：利用高频细节差异进一步优化相机姿态中的旋转矩阵。", "result": "在多个真实世界场景的广泛实验表明，所提出的PCR-GS在相机轨迹剧烈变化的情况下，实现了卓越的无姿态3D-GS场景建模。", "conclusion": "PCR-GS通过引入特征重投影和基于小波的频率正则化，显著提升了免COLMAP 3D-GS在处理复杂相机轨迹场景时的性能，实现了更优的场景建模和相机姿态估计。"}}
{"id": "2507.13899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13899", "abs": "https://arxiv.org/abs/2507.13899", "authors": ["Yujian Mo", "Yan Wu", "Junqiao Zhao", "Jijun Wang", "Yinghao Hu", "Jun Yan"], "title": "Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection", "comment": null, "summary": "Recent advances in foundation models have opened up new possibilities for\nenhancing 3D perception. In particular, DepthAnything offers dense and reliable\ngeometric priors from monocular RGB images, which can complement sparse LiDAR\ndata in autonomous driving scenarios. However, such priors remain underutilized\nin LiDAR-based 3D object detection. In this paper, we address the limited\nexpressiveness of raw LiDAR point features, especially the weak discriminative\ncapability of the reflectance attribute, by introducing depth priors predicted\nby DepthAnything. These priors are fused with the original LiDAR attributes to\nenrich each point's representation. To leverage the enhanced point features, we\npropose a point-wise feature extraction module. Then, a Dual-Path RoI feature\nextraction framework is employed, comprising a voxel-based branch for global\nsemantic context and a point-based branch for fine-grained structural details.\nTo effectively integrate the complementary RoI features, we introduce a\nbidirectional gated RoI feature fusion module that balances global and local\ncues. Extensive experiments on the KITTI benchmark show that our method\nconsistently improves detection accuracy, demonstrating the value of\nincorporating visual foundation model priors into LiDAR-based 3D object\ndetection.", "AI": {"tldr": "该论文提出了一种方法，通过融合来自视觉基础模型DepthAnything的深度先验与原始LiDAR数据，以增强LiDAR点特征，从而提升3D目标检测的准确性。", "motivation": "LiDAR点云特征（特别是反射率属性）的表达能力有限，判别性较弱。而DepthAnything等视觉基础模型能提供密集的几何先验，可以补充稀疏的LiDAR数据，但这些先验在LiDAR-based 3D目标检测中尚未得到充分利用。", "method": "1. 将DepthAnything预测的深度先验与原始LiDAR属性融合，以丰富每个点的表示。2. 提出一个点级特征提取模块。3. 采用双路径RoI特征提取框架，包含一个基于体素的分支（用于全局语义上下文）和一个基于点的分支（用于细粒度结构细节）。4. 引入一个双向门控RoI特征融合模块，以有效整合互补的RoI特征。", "result": "在KITTI基准测试上进行了广泛实验，结果表明该方法持续提高了检测精度。", "conclusion": "将视觉基础模型的先验知识融入基于LiDAR的3D目标检测具有重要价值，能够显著提升检测性能。"}}
{"id": "2507.13929", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13929", "abs": "https://arxiv.org/abs/2507.13929", "authors": ["Hsiang-Hui Hung", "Huu-Phu Do", "Yung-Hui Li", "Ching-Chun Huang"], "title": "TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views", "comment": "Accepted by MM 2024", "summary": "We present TimeNeRF, a generalizable neural rendering approach for rendering\nnovel views at arbitrary viewpoints and at arbitrary times, even with few input\nviews. For real-world applications, it is expensive to collect multiple views\nand inefficient to re-optimize for unseen scenes. Moreover, as the digital\nrealm, particularly the metaverse, strives for increasingly immersive\nexperiences, the ability to model 3D environments that naturally transition\nbetween day and night becomes paramount. While current techniques based on\nNeural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing\nnovel views, the exploration of NeRF's potential for temporal 3D scene modeling\nremains limited, with no dedicated datasets available for this purpose. To this\nend, our approach harnesses the strengths of multi-view stereo, neural radiance\nfields, and disentanglement strategies across diverse datasets. This equips our\nmodel with the capability for generalizability in a few-shot setting, allows us\nto construct an implicit content radiance field for scene representation, and\nfurther enables the building of neural radiance fields at any arbitrary time.\nFinally, we synthesize novel views of that time via volume rendering.\nExperiments show that TimeNeRF can render novel views in a few-shot setting\nwithout per-scene optimization. Most notably, it excels in creating realistic\nnovel views that transition smoothly across different times, adeptly capturing\nintricate natural scene changes from dawn to dusk.", "AI": {"tldr": "TimeNeRF是一种可泛化的神经渲染方法，能够在少量输入视图的情况下，在任意视点和任意时间渲染新颖视图，尤其擅长捕捉自然场景从白天到夜晚的平滑过渡。", "motivation": "在现实应用中，收集多视图数据成本高昂，且为未见场景重新优化效率低下。元宇宙等数字领域需要能够自然实现昼夜过渡的沉浸式3D环境。现有NeRF技术在合成新视图方面表现出色，但其在时间3D场景建模方面的潜力探索有限，且缺乏专用数据集。", "method": "TimeNeRF结合了多视图立体、神经辐射场和跨不同数据集的解耦策略。这使得模型在少样本设置下具有泛化能力，能够构建用于场景表示的隐式内容辐射场，并进一步在任意时间构建神经辐射场。最后通过体渲染合成该时间的新视图。", "result": "实验表明，TimeNeRF能够在少样本设置下渲染新颖视图，无需进行逐场景优化。最显著的是，它在创建逼真的新颖视图方面表现出色，能够平滑地过渡不同时间，巧妙捕捉从黎明到黄昏复杂的自然场景变化。", "conclusion": "TimeNeRF提出了一种新颖、可泛化的神经渲染方法，有效解决了在少样本条件下进行任意时间点和视点渲染的挑战，并成功实现了自然场景的平滑时间过渡建模。"}}
{"id": "2507.13934", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13934", "abs": "https://arxiv.org/abs/2507.13934", "authors": ["Marzieh Gheisari", "Auguste Genovesio"], "title": "DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization", "comment": null, "summary": "Unsupervised disentanglement of static appearance and dynamic motion in video\nremains a fundamental challenge, often hindered by information leakage and\nblurry reconstructions in existing VAE- and GAN-based approaches. We introduce\nDiViD, the first end-to-end video diffusion framework for explicit\nstatic-dynamic factorization. DiViD's sequence encoder extracts a global static\ntoken from the first frame and per-frame dynamic tokens, explicitly removing\nstatic content from the motion code. Its conditional DDPM decoder incorporates\nthree key inductive biases: a shared-noise schedule for temporal consistency, a\ntime-varying KL-based bottleneck that tightens at early timesteps (compressing\nstatic information) and relaxes later (enriching dynamics), and cross-attention\nthat routes the global static token to all frames while keeping dynamic tokens\nframe-specific. An orthogonality regularizer further prevents residual\nstatic-dynamic leakage. We evaluate DiViD on real-world benchmarks using\nswap-based accuracy and cross-leakage metrics. DiViD outperforms\nstate-of-the-art sequential disentanglement methods: it achieves the highest\nswap-based joint accuracy, preserves static fidelity while improving dynamic\ntransfer, and reduces average cross-leakage.", "AI": {"tldr": "DiViD是一种新颖的端到端视频扩散框架，用于无监督地解耦视频中的静态外观和动态运动，解决了现有方法的信息泄露和模糊重建问题，并取得了领先的性能。", "motivation": "视频中静态外观和动态运动的无监督解耦仍然是一个基本挑战，现有基于VAE和GAN的方法常受到信息泄露和模糊重建的困扰。", "method": "DiViD是首个端到端视频扩散框架，用于显式静态-动态分解。其序列编码器从第一帧提取全局静态token和每帧动态token，并明确从运动代码中去除静态内容。其条件DDPM解码器包含三个关键归纳偏置：用于时间一致性的共享噪声调度、随时间变化的KL瓶颈（早期紧缩以压缩静态信息，后期放松以丰富动态信息），以及将全局静态token路由到所有帧同时保持动态token帧特异性的交叉注意力。此外，采用正交正则化器防止残余的静态-动态泄露。", "result": "DiViD在真实世界基准测试中超越了最先进的序列解耦方法，实现了最高的基于交换的联合准确性，在保持静态保真度的同时改进了动态迁移，并降低了平均交叉泄露。", "conclusion": "DiViD成功解决了视频中静态外观和动态运动无监督解耦的挑战，在性能上显著优于现有方法，能够有效分离并重构视频中的静态和动态元素。"}}
{"id": "2507.13981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13981", "abs": "https://arxiv.org/abs/2507.13981", "authors": ["Sara Abdulaziz", "Giacomo D'Amicantonio", "Egor Bondarev"], "title": "Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset", "comment": "accepted at ICCV'25 workshop CV4BIOM", "summary": "Recent advances in AI-powered surveillance have intensified concerns over the\ncollection and processing of sensitive personal data. In response, research has\nincreasingly focused on privacy-by-design solutions, raising the need for\nobjective techniques to evaluate privacy protection. This paper presents a\ncomprehensive framework for evaluating visual privacy-protection methods across\nthree dimensions: privacy, utility, and practicality. In addition, it\nintroduces HR-VISPR, a publicly available human-centric dataset with biometric,\nsoft-biometric, and non-biometric labels to train an interpretable privacy\nmetric. We evaluate 11 privacy protection methods, ranging from conventional\ntechniques to advanced deep-learning methods, through the proposed framework.\nThe framework differentiates privacy levels in alignment with human visual\nperception, while highlighting trade-offs between privacy, utility, and\npracticality. This study, along with the HR-VISPR dataset, serves as an\ninsightful tool and offers a structured evaluation framework applicable across\ndiverse contexts.", "AI": {"tldr": "本文提出了一个用于评估视觉隐私保护方法的综合框架，并引入了一个新的数据集HR-VISPR，以客观地量化隐私、效用和实用性之间的权衡。", "motivation": "AI驱动的监控技术日益普及，加剧了人们对敏感个人数据收集和处理的担忧。这促使研究转向隐私保护设计方案，并产生了对客观评估隐私保护技术的需求。", "method": "本文提出了一个在隐私、效用和实用性三个维度上评估视觉隐私保护方法的综合框架。此外，还引入了HR-VISPR，一个公开可用的以人为中心的数据集，包含生物识别、软生物识别和非生物识别标签，用于训练可解释的隐私度量。该研究使用所提出的框架评估了11种隐私保护方法，涵盖了传统技术和先进的深度学习方法。", "result": "该框架能够根据人类视觉感知区分隐私级别，并突出了隐私、效用和实用性之间的权衡。研究结果表明，该框架可以有效地评估不同隐私保护方法的性能。", "conclusion": "这项研究及其HR-VISPR数据集提供了一个有洞察力的工具和一个结构化的评估框架，适用于各种背景下的视觉隐私保护方法评估。"}}
{"id": "2507.13985", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13985", "abs": "https://arxiv.org/abs/2507.13985", "authors": ["Haoran Li", "Yuli Tian", "Kun Lan", "Yong Liao", "Lin Wang", "Pan Hui", "Peng Yuan Zhou"], "title": "DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation", "comment": "Extended version of ECCV 2024 paper \"DreamScene\"", "summary": "Generating 3D scenes from natural language holds great promise for\napplications in gaming, film, and design. However, existing methods struggle\nwith automation, 3D consistency, and fine-grained control. We present\nDreamScene, an end-to-end framework for high-quality and editable 3D scene\ngeneration from text or dialogue. DreamScene begins with a scene planning\nmodule, where a GPT-4 agent infers object semantics and spatial constraints to\nconstruct a hybrid graph. A graph-based placement algorithm then produces a\nstructured, collision-free layout. Based on this layout, Formation Pattern\nSampling (FPS) generates object geometry using multi-timestep sampling and\nreconstructive optimization, enabling fast and realistic synthesis. To ensure\nglobal consistent, DreamScene employs a progressive camera sampling strategy\ntailored to both indoor and outdoor settings. Finally, the system supports\nfine-grained scene editing, including object movement, appearance changes, and\n4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior\nmethods in quality, consistency, and flexibility, offering a practical solution\nfor open-domain 3D content creation. Code and demos are available at\nhttps://dreamscene-project.github.io.", "AI": {"tldr": "DreamScene是一个端到端框架，能够从文本或对话生成高质量、可编辑的3D场景。", "motivation": "现有方法在从自然语言生成3D场景时，面临自动化程度低、3D一致性差和精细控制不足的问题。", "method": "DreamScene首先通过GPT-4代理进行场景规划，推断对象语义和空间约束以构建混合图。接着，图基放置算法生成无碰撞布局。基于此布局，Formation Pattern Sampling (FPS) 使用多时间步采样和重建优化生成对象几何。为确保全局一致性，系统采用渐进式相机采样策略。此外，DreamScene支持对象移动、外观变化和4D动态运动等精细编辑。", "result": "实验表明，DreamScene在质量、一致性和灵活性方面超越了现有方法，为开放域3D内容创作提供了实用解决方案。", "conclusion": "DreamScene提供了一个实用、高质量且可编辑的解决方案，用于从文本或对话生成3D场景，解决了现有方法在自动化、一致性和控制方面的局限性。"}}
{"id": "2507.14010", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14010", "abs": "https://arxiv.org/abs/2507.14010", "authors": ["Yong Feng", "Xiaolei Zhang", "Shijin Feng", "Yong Zhao", "Yihan Chen"], "title": "Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations", "comment": "8 pages, 10 figures, 3 tables", "summary": "Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming\nto classify and segment tunnel cracks with enhanced accuracy and efficiency,\nthis study proposes a two-step deep learning-based method. An automatic tunnel\nimage classification model is developed using the DenseNet-169 in the first\nstep. The proposed crack segmentation model in the second step is based on the\nDeepLabV3+, whose internal logic is evaluated via a score-weighted visual\nexplanation technique. Proposed method combines tunnel image classification and\nsegmentation together, so that the selected images containing cracks from the\nfirst step are segmented in the second step to improve the detection accuracy\nand efficiency. The superior performances of the two-step method are validated\nby experiments. The results show that the accuracy and frames per second (FPS)\nof the tunnel crack classification model are 92.23% and 39.80, respectively,\nwhich are higher than other convolutional neural networks (CNN) based and\nTransformer based models. Also, the intersection over union (IoU) and F1 score\nof the tunnel crack segmentation model are 57.01% and 67.44%, respectively,\noutperforming other state-of-the-art models. Moreover, the provided visual\nexplanations in this study are conducive to understanding the \"black box\" of\ndeep learning-based models. The developed two-stage deep learning-based method\nintegrating visual explanations provides a basis for fast and accurate\nquantitative assessment of tunnel health status.", "AI": {"tldr": "本研究提出了一种两步深度学习方法，用于提高隧道衬砌裂缝的分类和分割精度及效率，并结合视觉解释技术，为隧道健康状态的快速准确评估提供了基础。", "motivation": "隧道衬砌裂缝是隧道安全状况的关键指标。为了提高裂缝分类和分割的准确性和效率，研究旨在开发一种自动化的检测方法。", "method": "该方法包括两步：第一步，使用DenseNet-169开发隧道图像分类模型，自动筛选出包含裂缝的图像；第二步，对筛选出的图像使用基于DeepLabV3+的裂缝分割模型进行分割。此外，还通过得分加权视觉解释技术评估了分割模型的内部逻辑，以提高模型可解释性。", "result": "实验验证了该两步法的优越性能。隧道裂缝分类模型的准确率达到92.23%，FPS为39.80，均高于其他基于CNN和Transformer的模型。隧道裂缝分割模型的IoU为57.01%，F1分数为67.44%，优于其他先进模型。此外，提供的视觉解释有助于理解深度学习模型的“黑箱”特性。", "conclusion": "所开发的两阶段深度学习方法，结合视觉解释，为隧道健康状况的快速准确量化评估提供了基础。"}}
{"id": "2507.14013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14013", "abs": "https://arxiv.org/abs/2507.14013", "authors": ["Ji-Yan Wu", "Zheng Yong Poh", "Anoop C. Patil", "Bongsoo Park", "Giovanni Volpe", "Daisuke Urano"], "title": "Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model", "comment": null, "summary": "Accurate detection of nutrient deficiency in plant leaves is essential for\nprecision agriculture, enabling early intervention in fertilization, disease,\nand stress management. This study presents a deep learning framework for leaf\nanomaly segmentation using multispectral imaging and an enhanced YOLOv5 model\nwith a transformer-based attention head. The model is tailored for processing\nnine-channel multispectral input and uses self-attention mechanisms to better\ncapture subtle, spatially-distributed symptoms. The plants in the experiments\nwere grown under controlled nutrient stress conditions for evaluation. We carry\nout extensive experiments to benchmark the proposed model against the baseline\nYOLOv5. Extensive experiments show that the proposed model significantly\noutperforms the baseline YOLOv5, with an average Dice score and IoU\n(Intersection over Union) improvement of about 12%. In particular, this model\nis effective in detecting challenging symptoms like chlorosis and pigment\naccumulation. These results highlight the promise of combining multi-spectral\nimaging with spectral-spatial feature learning for advancing plant phenotyping\nand precision agriculture.", "AI": {"tldr": "本研究提出了一种基于多光谱成像和改进YOLOv5模型的深度学习框架，用于植物叶片异常分割，以精确检测养分缺乏。", "motivation": "在精准农业中，准确检测植物叶片养分缺乏对于早期干预施肥、疾病和胁迫管理至关重要。", "method": "研究采用深度学习框架进行叶片异常分割，使用九通道多光谱输入。模型基于增强的YOLOv5，并集成了一个基于Transformer的注意力头，利用自注意力机制更好地捕捉细微、空间分布的症状。实验中植物在受控养分胁迫条件下生长。", "result": "所提出的模型显著优于基线YOLOv5，平均Dice分数和IoU（交并比）提高了约12%。该模型在检测叶片黄化和色素积累等挑战性症状方面尤其有效。", "conclusion": "结合多光谱成像与光谱-空间特征学习在推进植物表型分析和精准农业方面具有广阔前景。"}}
{"id": "2507.14024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14024", "abs": "https://arxiv.org/abs/2507.14024", "authors": ["Jiarong Ye", "Sharon X. Huang"], "title": "Moodifier: MLLM-Enhanced Emotion-Driven Image Editing", "comment": null, "summary": "Bridging emotions and visual content for emotion-driven image editing holds\ngreat potential in creative industries, yet precise manipulation remains\nchallenging due to the abstract nature of emotions and their varied\nmanifestations across different contexts. We tackle this challenge with an\nintegrated approach consisting of three complementary components. First, we\nintroduce MoodArchive, an 8M+ image dataset with detailed hierarchical\nemotional annotations generated by LLaVA and partially validated by human\nevaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned\non MoodArchive to translate abstract emotions into specific visual attributes.\nThird, we propose Moodifier, a training-free editing model leveraging\nMoodifyCLIP and multimodal large language models (MLLMs) to enable precise\nemotional transformations while preserving content integrity. Our system works\nacross diverse domains such as character expressions, fashion design, jewelry,\nand home d\\'ecor, enabling creators to quickly visualize emotional variations\nwhile preserving identity and structure. Extensive experimental evaluations\nshow that Moodifier outperforms existing methods in both emotional accuracy and\ncontent preservation, providing contextually appropriate edits. By linking\nabstract emotions to concrete visual changes, our solution unlocks new\npossibilities for emotional content creation in real-world applications. We\nwill release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier\ncode and demo publicly available upon acceptance.", "AI": {"tldr": "情感驱动的图像编辑具有巨大潜力，但情感的抽象性使其难以精确操作。本文提出Moodifier系统，包含大规模情感数据集MoodArchive、情感视觉语言模型MoodifyCLIP和免训练编辑模型Moodifier，实现跨领域的情感图像编辑，同时保持内容完整性。", "motivation": "在创意产业中，将情感与视觉内容结合进行情感驱动的图像编辑潜力巨大。然而，由于情感的抽象性及其在不同语境下的多样化表现，精确的情感操作仍然具有挑战性。", "method": "本文采用集成方法，包含三个互补组件：1. **MoodArchive数据集**: 构建了一个包含800多万张图像的大规模数据集，具有详细的分层情感标注。2. **MoodifyCLIP模型**: 开发了一个在MoodArchive上微调的视觉-语言模型，用于将抽象情感转化为具体的视觉属性。3. **Moodifier编辑模型**: 提出一个免训练的编辑模型，利用MoodifyCLIP和多模态大语言模型（MLLMs），实现精确的情感转换，同时保持内容完整性。", "result": "Moodifier系统能够在角色表情、时尚设计、珠宝和家居装饰等不同领域进行情感编辑，帮助创作者快速可视化情感变化，同时保留身份和结构。广泛的实验评估表明，Moodifier在情感准确性和内容保留方面均优于现有方法，提供符合语境的编辑。", "conclusion": "通过将抽象情感与具体的视觉变化联系起来，本解决方案为现实世界应用中的情感内容创作开辟了新的可能性。该研究将发布数据集、模型代码和演示。"}}
{"id": "2507.14031", "categories": ["cs.CV", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14031", "abs": "https://arxiv.org/abs/2507.14031", "authors": ["Hao Fang", "Sihao Teng", "Hao Yu", "Siyi Yuan", "Huaiwu He", "Zhe Liu", "Yunjie Yang"], "title": "QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography", "comment": "10 pages, 12 figures", "summary": "Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside\nimaging modality with high temporal resolution, making it suitable for bedside\nmonitoring. However, its inherently ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Deep learning (DL)-based\napproaches have shown promise but often rely on complex network architectures\nwith a large number of parameters, limiting efficiency and scalability. Here,\nwe propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework\nfor EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network\n(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive\nlatent representations that serve as implicit nonlinear priors, followed by a\nsingle linear layer for conductivity reconstruction. This design drastically\nreduces model complexity and parameter number. Uniquely, QuantEIT operates in\nan unsupervised, training-data-free manner and represents the first integration\nof quantum circuits into EIT image reconstruction. Extensive experiments on\nsimulated and real-world 2D and 3D EIT lung imaging data demonstrate that\nQuantEIT outperforms conventional methods, achieving comparable or superior\nreconstruction accuracy using only 0.2% of the parameters, with enhanced\nrobustness to noise.", "AI": {"tldr": "QuantEIT是一种超轻量级、量子辅助的EIT图像重建框架，它通过结合量子电路和线性层，在无监督、无训练数据的情况下，以极少的参数实现了比传统方法更优或相当的重建精度和噪声鲁棒性。", "motivation": "EIT作为一种床旁成像技术，具有高时间分辨率，但其固有的病态逆问题使得图像重建极具挑战。现有的深度学习方法虽然有前景，但通常依赖于复杂的网络架构和大量参数，限制了效率和可扩展性。", "method": "本文提出了一种名为QuantEIT的超轻量级量子辅助推理框架。它利用量子辅助网络（QA-Net），结合并行的2量子比特量子电路来生成富有表现力的潜在表示（作为隐式非线性先验），然后通过单个线性层进行电导率重建。该设计显著降低了模型复杂度和参数数量。QuantEIT以无监督、无训练数据的方式运行，是首次将量子电路集成到EIT图像重建中。", "result": "在模拟和真实世界的2D和3D EIT肺部成像数据上的广泛实验表明，QuantEIT优于传统方法，仅使用0.2%的参数即可实现相当或更优的重建精度，并增强了对噪声的鲁棒性。", "conclusion": "QuantEIT提供了一种高效、精确且参数极少的EIT图像重建解决方案，通过引入量子电路实现了无监督学习，克服了传统深度学习方法的复杂性限制，并展示了在实际应用中的巨大潜力。"}}
{"id": "2507.14042", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14042", "abs": "https://arxiv.org/abs/2507.14042", "authors": ["Qiankun Ma", "Ziyao Zhang", "Chi Su", "Jie Chen", "Zhen Song", "Hairong Zheng", "Wen Gao"], "title": "Training-free Token Reduction for Vision Mamba", "comment": null, "summary": "Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)\ndue to its ability to efficiently capture long-range dependencies with linear\ncomputational complexity. While token reduction, an effective compression\ntechnique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision\nMamba's efficiency is essential for enabling broader applications. However, we\nfind that directly applying existing token reduction techniques for ViTs to\nVision Mamba leads to significant performance degradation. This is primarily\nbecause Mamba is a sequence model without attention mechanisms, whereas most\ntoken reduction techniques for ViTs rely on attention mechanisms for importance\nmeasurement and overlook the order of compressed tokens. In this paper, we\ninvestigate a Mamba structure-aware importance score to evaluate token\nimportance in a simple and effective manner. Building on this score, we further\npropose MTR, a training-free \\textbf{M}amba \\textbf{T}oken \\textbf{R}eduction\nframework. Without the need for training or additional tuning parameters, our\nmethod can be seamlessly integrated as a plug-and-play component across various\nMamba models. Extensive experiments demonstrate that our approach significantly\nreduces computational workload while minimizing performance impact across\nvarious tasks and multiple backbones. Notably, MTR reduces FLOPs by\napproximately 40\\% on the Vim-B backbone, with only a 1.6\\% drop in ImageNet\nperformance without retraining.", "AI": {"tldr": "提出了一个针对Vision Mamba模型的免训练令牌缩减框架MTR，通过引入Mamba结构感知的令牌重要性评分，显著降低计算量同时保持性能。", "motivation": "Vision Mamba在处理长距离依赖方面具有线性计算复杂度的优势，但其效率优化（如令牌缩减）尚未被充分探索。现有ViT的令牌缩减技术不适用于Vision Mamba，因为Mamba是序列模型且缺乏注意力机制，而这些技术依赖注意力机制进行重要性评估且忽略令牌顺序。因此，探索Vision Mamba的效率以实现更广泛的应用至关重要。", "method": "本文研究了一种Mamba结构感知的令牌重要性评分方法，以简单有效的方式评估令牌重要性。在此基础上，提出了一个名为MTR（Mamba Token Reduction）的免训练框架。该方法无需训练或额外调参，可作为即插即用组件无缝集成到各种Mamba模型中。", "result": "MTR显著降低了计算负载，同时最大程度地减少了性能影响。在Vim-B骨干网络上，MTR将FLOPs降低了约40%，而ImageNet性能仅下降1.6%（无需重新训练）。", "conclusion": "MTR框架有效实现了Vision Mamba模型的令牌缩减，显著提升了其计算效率，使其能更广泛地应用于各种任务，且无需额外训练或调参，表现出良好的通用性和实用性。"}}
{"id": "2507.14050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14050", "abs": "https://arxiv.org/abs/2507.14050", "authors": ["Mohamed Elkhayat", "Mohamed Mahmoud", "Jamil Fayyad", "Nourhan Bayasi"], "title": "Foundation Models as Class-Incremental Learners for Dermatological Image Classification", "comment": "Accepted at the MICCAI EMERGE 2025 workshop", "summary": "Class-Incremental Learning (CIL) aims to learn new classes over time without\nforgetting previously acquired knowledge. The emergence of foundation models\n(FM) pretrained on large datasets presents new opportunities for CIL by\noffering rich, transferable representations. However, their potential for\nenabling incremental learning in dermatology remains largely unexplored. In\nthis paper, we systematically evaluate frozen FMs pretrained on large-scale\nskin lesion datasets for CIL in dermatological disease classification. We\npropose a simple yet effective approach where the backbone remains frozen, and\na lightweight MLP is trained incrementally for each task. This setup achieves\nstate-of-the-art performance without forgetting, outperforming regularization,\nreplay, and architecture based methods. To further explore the capabilities of\nfrozen FMs, we examine zero training scenarios using nearest mean classifiers\nwith prototypes derived from their embeddings. Through extensive ablation\nstudies, we demonstrate that this prototype based variant can also achieve\ncompetitive results. Our findings highlight the strength of frozen FMs for\ncontinual learning in dermatology and support their broader adoption in real\nworld medical applications. Our code and datasets are available here.", "AI": {"tldr": "本文系统评估了预训练的冻结基础模型在皮肤病分类的类增量学习（CIL）中的潜力，并提出了一种简单有效的方法，通过冻结骨干网络和增量训练轻量级MLP，实现了SOTA性能。", "motivation": "基础模型（FM）提供了丰富的可迁移表示，为类增量学习（CIL）带来了新机遇。然而，其在皮肤病学领域实现增量学习的潜力尚未被充分探索。", "method": "研究系统评估了在大型皮肤病变数据集上预训练的冻结基础模型在皮肤病分类CIL中的表现。提出了一种简单有效的方法：骨干网络保持冻结，并为每个任务增量训练一个轻量级多层感知器（MLP）。此外，还探索了零训练场景，使用基于基础模型嵌入派生的原型进行最近均值分类。", "result": "所提出的冻结基础模型加轻量级MLP的方法在不遗忘的情况下实现了最先进的性能，优于基于正则化、回放和架构的方法。基于原型的零训练变体也取得了有竞争力的结果。", "conclusion": "研究结果强调了冻结基础模型在皮肤病学持续学习中的强大能力，并支持它们在真实世界医疗应用中的更广泛采用。"}}
{"id": "2507.14083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14083", "abs": "https://arxiv.org/abs/2507.14083", "authors": ["Sara Abdulaziz", "Egor Bondarev"], "title": "Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection", "comment": "ACIVS 2025", "summary": "Advancements in deep learning have improved anomaly detection in surveillance\nvideos, yet they raise urgent privacy concerns due to the collection of\nsensitive human data. In this paper, we present a comprehensive analysis of\nanomaly detection performance under four human anonymization techniques,\nincluding blurring, masking, encryption, and avatar replacement, applied to the\nUCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,\nBN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method\nresponds to different obfuscation techniques. Experimental results demonstrate\nthat anomaly detection remains viable under anonymized data and is dependent on\nthe algorithmic design and the learning strategy. For instance, under certain\nanonymization patterns, such as encryption and masking, some models\ninadvertently achieve higher AUC performance compared to raw data, due to the\nstrong responsiveness of their algorithmic components to these noise patterns.\nThese results highlight the algorithm-specific sensitivities to anonymization\nand emphasize the trade-off between preserving privacy and maintaining\ndetection utility. Furthermore, we compare these conventional anonymization\ntechniques with the emerging privacy-by-design solutions, highlighting an often\noverlooked trade-off between robust privacy protection and utility flexibility.\nThrough comprehensive experiments and analyses, this study provides a\ncompelling benchmark and insights into balancing human privacy with the demands\nof anomaly detection.", "AI": {"tldr": "该研究评估了四种人体匿名化技术（模糊、遮罩、加密、头像替换）对视频监控异常检测性能的影响，并分析了不同异常检测算法对匿名化模式的敏感性，探讨了隐私保护与检测效用之间的权衡。", "motivation": "尽管深度学习提升了监控视频中的异常检测能力，但敏感人体数据的收集引发了紧迫的隐私问题。", "method": "研究在UCF-Crime数据集上应用了模糊、遮罩、加密和头像替换四种人体匿名化技术。然后，评估了MGFN、UR-DMU、BN-WVAD和PEL4VAD四种异常检测方法在这些匿名化数据上的性能。此外，还比较了传统匿名化技术与新兴的隐私设计（privacy-by-design）解决方案。", "result": "实验结果表明，在匿名化数据下异常检测仍然可行，且其性能取决于算法设计和学习策略。在某些匿名化模式（如加密和遮罩）下，一些模型甚至比使用原始数据获得了更高的AUC性能，这是由于其算法组件对这些“噪声模式”的强响应性。这些结果突出了算法对匿名化的特定敏感性，并强调了隐私保护与检测效用之间的权衡。此外，研究还揭示了鲁棒隐私保护与效用灵活性之间的权衡。", "conclusion": "本研究通过全面的实验和分析，为平衡人体隐私与异常检测需求提供了一个有说服力的基准和深入见解。"}}
{"id": "2507.14095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14095", "abs": "https://arxiv.org/abs/2507.14095", "authors": ["Yung-Hong Sun", "Ting-Hung Lin", "Jiangang Chen", "Hongrui Jiang", "Yu Hen Hu"], "title": "C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs", "comment": null, "summary": "Multi-view multi-object association is a fundamental step in 3D\nreconstruction pipelines, enabling consistent grouping of object instances\nacross multiple camera views. Existing methods often rely on appearance\nfeatures or geometric constraints such as epipolar consistency. However, these\napproaches can fail when objects are visually indistinguishable or observations\nare corrupted by noise. We propose C-DOG, a training-free framework that serves\nas an intermediate module bridging object detection (or pose estimation) and 3D\nreconstruction, without relying on visual features. It combines connected\ndelta-overlap graph modeling with epipolar geometry to robustly associate\ndetections across views. Each 2D observation is represented as a graph node,\nwith edges weighted by epipolar consistency. A delta-neighbor-overlap\nclustering step identifies strongly consistent groups while tolerating noise\nand partial connectivity. To further improve robustness, we incorporate\nInterquartile Range (IQR)-based filtering and a 3D back-projection error\ncriterion to eliminate inconsistent observations. Extensive experiments on\nsynthetic benchmarks demonstrate that C-DOG outperforms geometry-based\nbaselines and remains robust under challenging conditions, including high\nobject density, without visual features, and limited camera overlap, making it\nwell-suited for scalable 3D reconstruction in real-world scenarios.", "AI": {"tldr": "C-DOG是一个免训练的多视图多目标关联框架，它结合了连接delta-overlap图建模和对极几何，在不依赖视觉特征的情况下，即使在挑战性条件下也能鲁棒地关联跨视图检测。", "motivation": "现有的多视图多目标关联方法依赖外观特征或几何约束（如对极一致性），但在物体视觉上难以区分或观测受噪声污染时会失效。因此需要一种更鲁棒的方法。", "method": "C-DOG是一个连接目标检测/姿态估计与3D重建的中间模块，不依赖视觉特征。它将每个2D观测表示为图节点，边权重由对极一致性决定。通过delta-neighbor-overlap聚类识别强一致性组，同时容忍噪声和部分连接。为进一步提高鲁棒性，引入了基于四分位距(IQR)的过滤和3D反投影误差准则来消除不一致观测。", "result": "在合成基准测试中，C-DOG优于基于几何的基线方法，并在高物体密度、无视觉特征和有限相机重叠等挑战性条件下保持鲁棒性。", "conclusion": "C-DOG的鲁棒性和在挑战性条件下的优异表现使其非常适合可扩展的真实世界3D重建场景。"}}
{"id": "2507.14137", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14137", "abs": "https://arxiv.org/abs/2507.14137", "authors": ["Shashanka Venkataramanan", "Valentinos Pariza", "Mohammadreza Salehi", "Lukas Knobel", "Spyros Gidaris", "Elias Ramzi", "Andrei Bursuc", "Yuki M. Asano"], "title": "Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning", "comment": null, "summary": "We present Franca (pronounced Fran-ka): free one; the first fully open-source\n(data, code, weights) vision foundation model that matches and in many cases\nsurpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,\nCLIP, SigLIPv2, etc. Our approach is grounded in a transparent training\npipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and\na subset of ReLAION-2B. Beyond model release, we tackle critical limitations in\nSSL clustering methods. While modern models rely on assigning image features to\nlarge codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to\naccount for the inherent ambiguity in clustering semantics. To address this, we\nintroduce a parameter-efficient, multi-head clustering projector based on\nnested Matryoshka representations. This design progressively refines features\ninto increasingly fine-grained clusters without increasing the model size,\nenabling both performance and memory efficiency. Additionally, we propose a\nnovel positional disentanglement strategy that explicitly removes positional\nbiases from dense representations, thereby improving the encoding of semantic\ncontent. This leads to consistent gains on several downstream benchmarks,\ndemonstrating the utility of cleaner feature spaces. Our contributions\nestablish a new standard for transparent, high-performance vision models and\nopen a path toward more reproducible and generalizable foundation models for\nthe broader AI community. The code and model checkpoints are available at\nhttps://github.com/valeoai/Franca.", "AI": {"tldr": "Franca是一个完全开源的视觉基础模型，性能媲美甚至超越现有专有模型，通过引入多头聚类投影器和位置解耦策略，解决了自监督学习中的聚类语义模糊和位置偏差问题。", "motivation": "现有视觉基础模型多为专有且不透明，同时自监督学习（SSL）中的聚类方法（如Sinkhorn-Knopp）存在语义模糊性，无法有效处理特征的细粒度划分，且密集表示中存在位置偏差。", "method": "Franca采用透明的训练流程（受Web-SSL启发），使用公开数据集（ImageNet-21K和ReLAION-2B子集）。核心方法包括：1) 引入基于嵌套Matryoshka表示的参数高效、多头聚类投影器，实现特征的渐进式细化，同时提高性能和内存效率。2) 提出新颖的位置解耦策略，显式消除密集表示中的位置偏差，以提升语义内容的编码质量。", "result": "Franca的性能与DINOv2、CLIP、SigLIPv2等最先进的专有模型相当，并在许多情况下超越它们。多头聚类设计在不增加模型大小的情况下实现了性能和内存效率。位置解耦策略在多个下游基准测试中带来了显著的性能提升，表明了更清晰特征空间的有效性。", "conclusion": "Franca为透明、高性能的视觉模型树立了新标准，为人工智能社区中更可复现和泛化的基础模型开辟了道路。"}}
